2007,A General Boosting Method and its Application to Learning Ranking Functions for Web Search,We present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems. Our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm. More importantly  this general framework enables us to use a standard regression base learner such as decision trees for fitting any loss function. We illustrate an application of the proposed method in learning ranking functions for Web search by combining both preference data and labeled data for training. We present experimental results for Web search using data from a commercial search engine that show significant improvements of our proposed methods over some existing methods.,A General Boosting Method and its Application to

Learning Ranking Functions for Web Search

Zhaohui Zhengy Hongyuan Zha? Tong Zhangy Olivier Chapelley Keke Cheny Gordon Suny

yYahoo! Inc.

701 First Avene

Sunnyvale  CA 94089

fzhaohui tzhang chap kchen gzsung@yahoo-inc.com

?College of Computing

Georgia Institute of Technology

Atlanta  GA 30032

zha@cc.gatech.edu

Abstract

We present a general boosting method extending functional gradient boosting to
optimize complex loss functions that are encountered in many machine learning
problems. Our approach is based on optimization of quadratic upper bounds of the
loss functions which allows us to present a rigorous convergence analysis of the
algorithm. More importantly  this general framework enables us to use a standard
regression base learner such as single regression tree for £tting any loss function.
We illustrate an application of the proposed method in learning ranking functions
for Web search by combining both preference data and labeled data for training.
We present experimental results for Web search using data from a commercial
search engine that show signi£cant improvements of our proposed methods over
some existing methods.

1 Introduction

There has been much interest in developing machine learning methods involving complex loss func-
tions beyond those used in regression and classi£cation problems [13]. Many methods have been
proposed dealing with a wide range of problems including ranking problems  learning conditional
random £elds and other structured learning problems [1  3  4  5  6  7  11  13]. In this paper we
propose a boosting framework that can handle a wide variety of complex loss functions. The pro-
posed method uses a regression black box to optimize a general loss function based on quadratic
upper bounds  and it also allows us to present a rigorous convergence analysis of the method. Our
approach extends the gradient boosting approach proposed in [8] but can handle substantially more
complex loss functions arising from a variety of machine learning problems.

As an interesting and important application of the general boosting framework we apply it to the
problem of learning ranking functions for Web search. Speci£cally  we want to rank a set of docu-
ments according to their relevance to a given query. We adopt the following framework: we extract
a set of features x for each query-document pair  and learn a function h(x) so that we can rank the
documents using the values h(x)  say x with larger h(x) values are ranked higher. We call such
a function h(x) a ranking function. In Web search  we can identify two types of training data for
learning a ranking function: 1) preference data indicating a document is more relevant than another
with respect to a query [11  12]; and 2) labeled data where documents are assigned ordinal labels
representing degree of relevancy. In general  we will have both preference data and labeled data for

1

training a ranking function for Web search  leading to a complex loss function that can be handled
by our proposed general boosting method which we now describe.

2 A General Boosting Method

We consider the following general optimization problem:

(1)
where h denotes a prediction function which we are interested in learning from the data  H is a pre-
chosen function class  and R(h) is a risk functional with respect to h. We consider the following
form of the risk functional R:

h2HR(h);

^h = arg min

n

R(h) =

1
n

Xi=1

`i(h(xi;1);¢¢¢ ; h(xi;mi ); yi);

(2)

where `i(h1; : : : ; hmi ; y) is a loss function with respect to the £rst m i arguments h1; : : : ; hmi.
For example  each function `i can be a single variable function (mi = 1) such as in regression:
`i(h; y) = (h ¡ y)2; or a two-variable function (mi = 2)  such as those in ranking based on pair-
wise comparisons: `i(h1; h2; y) = max(0; 1¡ y(h1 ¡ h2))2  where y 2 f§1g indicates whether h1
is preferred to h2 or not; or it can be a multi-variable function as used in some structured prediction
problems: `i(h1; : : : ; hmi ; y) = supz –(y; z) + ˆ(h; z) ¡ ˆ(h; y)  where – is a loss function [13].
Assume we do not have a general solver for the optimization problem (1)  but we have a learning
algorithm A which we refer to as regression weak learner. Given any set of data points X =
[x1; : : : ; xk]  with corresponding target values R = [r1; : : : ; rk]  weights W = [w1; : : : ; wk]  and
tolerance † > 0  the regression weak learner A produces a function ^g = A(W; X; R; †) 2 C such
that

k

k

Xj=1

wj(^g(xj) ¡ rj)2 • min

g2C

Xj=1

wj(g(xj) ¡ rj)2 + †:

(3)

Our goal is to use this weak learner A to solve the original optimization problem (1). Here H =
span(C)  i.e.  h 2 H can be expressed as h(x) = Pj ajhj(x) with hj 2 C.

Friedman [8] proposed a solution when the loss function in (2) can be expressed as

R(h) =

n

Xi=1

`i(h(xi));

(4)

which he named as gradient boosting. The idea is to estimate the gradient r`i(h(xi)) using regres-
sion at each step with uniform weighting  and update. However  there is no convergence proof.

Following his work  we consider an extension that is more principly motivated  for which a conver-
gence analysis can be obtained. We £rst rewrite (2) in the more general form:

R(h) = R(h(x1); : : : ; h(xN ));

(5)

where N • P mi.1 Note that R depends on h only through the function values h(xi) and from
now on we identify the function h with the vector [h(xi)]. Also the function R is considered to be a
function of N variables.
Our main observation is that for twice differentiable risk functional R  at each tentative solution hk 
we can expand R(h) around hk using Taylor expansion as
R(hk + g) = R(hk) + rR(hk)T g +

gTr2R(h0)g;

1
2

where h0 lies between hk and hk + g. The right hand side is almost quadratic  and we can then
replace it by a quadratic upper-bound

R(hk + g) • Rk(g) = R(hk) + rR(hk)T g +

1
2

gT W g;

(6)

1We consider that all xi are different  but some of the xi;mi in (2) might have been identical  hence the

inequality.

2

where W is a diagonal matrix upper bounding the Hessian between hk and hk + g. If we de£ne

rj = ¡[rR(hk)]j=wj  then 8g 2 C;Pj wj(g(xj) ¡ rj)2 is equal to the above quadratic form
(up to a constant). So g can be found by calling the regression weak learner A. Since at each
step we try to minimize an upper bound Rk of R  if we let the minimum be gk  it is clear that
R(hk + gk) • Rk(gk) • R(hk). This means that by optimizing with respect to the problem Rk
that can be handled by A  we also make progress with respect to optimizing R. The algorithm based
on this idea is listed in Algorithm 1 for the loss function in (5).

Convergence analysis of this algorithm can be established using the idea summarized above; see
details in appendix. However  in partice  instead of the quadratic upper bound (which has a theo-
retical garantee easier to derive)  one may also consider minimizing an approximation to the Taylor
expansion  which would be closer to a Newton type method.

Algorithm 1 Greedy Algorithm with Quadratic Approximation

Input: X = [x‘]‘=1;:::;N
let h0 = 0
for k = 0; 1; 2; : : :

let W = [w‘]‘=1;:::;N   with either
w‘ = @2R=@hk(x‘)2 or
W global diagonal upper bound on the Hessian
let R = [r‘]‘=1;:::;N   where r‘ = w¡1
‘ @R=@hk(x‘)
pick †k ‚ 0
let gk = A(W; X; R; †k)
pick step-size sk ‚ 0  typically by line search on R
let hk+1 = hk + skgk
end

% Newton-type method with diagonal Hessian
% Upper-bound minimization

The main conceptual difference between our view and that of Friedman is that he views regression
as a “reasonable” approximation to the £rst order gradient rR  while our work views it as a natural
consequence of second order approximation of the objective function (in which the quadratic term
serve as an upper bound of the Hessian either locally or globally). This leads to algorithmic differ-
ence. In our approach  a good choice of the second order upper bound (leading to tighter bound)
may require non-uniform weights W . This is inline with earlier boosting work in which sample-
reweighting was a central idea. In our framework  the reweighting naturally occurs when we choose
a tight second order approximation. Different reweighting can affect the rate of convergence in our
analysis. The other main difference with Friedman is that he only considered objective functions of
the form (4); we propose a natural extension to the ones of the form (5).

3 Learning Ranking Functions

We now apply Algorithm 1 to the problem of learning ranking functions. We use preference data as
well as labeled data for training the ranking function. For preference data  we use x ´ y to mean
that x is preferred over y or x should be ranked higher than y  where x and y are the feature vectors
for corresponding items to be ranked. We denote the set of available preferences as S = fxi ´
yi; i = 1; : : : ; Ng: In addition to the preference data  there are also labeled data  L = f(zi; li); i =
1; : : : ; ng; where zi is the feature of an item and li is the corresponding numerically coded label.2
We formulate the ranking problem as computing a ranking function h 2 H  such that h satis£es as
much as possible the set of preferences  i.e.  h(xi) ‚ h(yi)  if xi ´ yi; i = 1; : : : ; N; while at the
same time h(zi) matches the label li in a sense to be detailed below.

2Some may argue that  absolute relevance judgments can also be converted to relative relevance judgments.
For example  for a query  suppose we have three documents d1; d2 and d3 labeled as perfect  good  and bad 
respectively. We can obtain the following relative relevance judgments: d1 is preferred over d2  d1 is preferred
over d3 and d2 is preferred over d3. However  it is often the case in Web search that for many queries there
only exist documents with a single label and for such kind of queries  no preference data can be constructed.

3

THE OBJECTIVE FUNCTION. We use the following objective function to measure the empirical risk
of a ranking function h 

R(h) =

w
2

N

Xi=1

(maxf0; h(yi) ¡ h(xi) + ¿g)2 +

1 ¡ w

2

n

Xi=1

(li ¡ h(zi))2:

The objective function consists of two parts: 1) for the preference data part  we introduce a margin
parameter ¿ and would like to enforce that h(xi) ‚ h(yi) + ¿ ; if not  the difference is quadratically
penalized; and 2) for the labeled data part  we simply minimize the squared errors. The parameter
w is the relative weight for the preference data and could typically be found by cross-validation.
The optimization problem we seek to solve is h⁄ = argmin h2H R(h); where H is some given
function class. Note that R depends only on the values h(xi); h(yi); h(zi) and we can optimize it
using the general boosting framework discussed in section 2.
QUADRATIC APPROXIMATION. To this end consider the quadratic approximation (6) for R(h).
For simplicity let us assume that each feature vector xi  yi and zi only appears in S and L once 
otherwise we need to compute appropriately formed averages. We consider

h(xi); h(yi);

i = 1; : : : ; N;

h(zi);

i = 1; : : : ; n

as the unknowns  and compute the gradient of R(h) with respect to those unknowns. The com-
ponents of the negative gradient corresponding to h(zi) is just li ¡ h(zi): The components of the
negative gradient corresponding to h(xi) and h(yi)  respectively  are

maxf0; h(yi) ¡ h(xi) + ¿g; ¡ maxf0; h(yi) ¡ h(xi) + ¿g:

Both of the above equal to zero when h(xi)¡h(yi) ‚ ¿ . For the second-order term  it can be readily
veri£ed that the Hessian of R(h) is block-diagonal with 2-by-2 blocks corresponding to h(x i) and
h(yi) and 1-by-1 blocks for h(zi). In particular  if we evaluate the Hessian at h  the 2-by-2 block
equals to

• 1 ¡1
¡1

1 ‚ ;

• 0

0

0

0 ‚ ;

for xi ´ yi with h(xi) ¡ h(yi) < ¿ and h(xi) ¡ h(yi) ‚ ¿   respectively. We can upper bound the
£rst matrix by the diagonal matrix diag(2; 2) leading to a quadratic upper bound. We summarize
the above derivations in the following algorithm.

Algorithm 2 Boosted Ranking using Successive Quadratic Approximation (QBRank)

Start with an initial guess h0  for m = 1; 2; : : :  
1) we construct a training set for £tting gm(x) by adding the following for each hxi; yii 2 S 
(xi; maxf0; hm¡1(yi) ¡ hm¡1(xi) + ¿g); (yi;¡ maxf0; hm¡1(yi) ¡ hm¡1(xi) + ¿g);
and
f(zi; li ¡ hm¡1(zi));
The £tting of gm(x) is done by using a base regressor with the above training set; We weigh
the above preference data by w and the labeled data by 1 ¡ w respectively.
2) forming hm = hm¡1 + ·smgm(x);
where sm is found by line search to minimize the objective function. · is a shrinkage factor.

i = 1; : : : ; ng:

The shrinkage factor · by default is 1  but Friedman [8] reported better results (coming from better
regularization) by taking · < 1. In general  we choose · and w by cross-validation. ¿ could be the
degree of preference if that information is available  e.g.  the absolute grade difference between each
prefernce if it is converted from labeled data. Otherwise  we simply set it to be 1.0. When there is
no preference data and the weak regression learner produces a regression tree  QBrank is identical
to Gradient Boosting Trees (GBT) as proposed in [8].
REMARK. An xi can appear multiple times in Step 1)  in this case we use the average gradient
values as the target value for each distinct xi.

4

4 Experiment Results

We carried out several experiments illustrating the properties and effectiveness of QBrank using
combined preference data and labeled data in the context of learning ranking functions for Web
search [3]. We also compared its performance with QBrank using preference data only and several
existing algorithms such as Gradient Boosting Trees [8] and RankSVM [11  12]. RankSVM is a
preference learning method which learns pair-wise preferences based on SVM approach.

DATA COLLECTION. We £rst describe how the data used in the experiments are collected. For
each query-document pair we extracted a set of features to form a feature vector. which consists of
three parts  x = [xQ; xD; xQD]; where 1) the query-feature vector xQ comprises features dependent
on the query q only and have constant values across all the documents d in the document set  for
example  the number of terms in the query  whether or not the query is a person name  etc.; 2)
the document-feature vector xD comprises features dependent on the document d only and have
constant values across all the queries q in the query set  for example  the number of inbound links
pointing to the document  the amount of anchor-texts in bytes for the document  and the language
identity of the document  etc.; and 3) the query-document feature vector xQD which comprises
features dependent on the relation of the query q with respect to the document d  for example  the
number of times each term in the query q appears in the document d  the number of times each term
in the query q appears in the anchor-texts of the document d  etc.
We sampled a set of queries from the query logs of a commercial search engine and generated
a certain number of query-document pairs for each of the queries. A £ve-level numerical grade
(0; 1; 2; 3; 4) is assigned to each query-document pair based on the degree of relevance. In total
we have 4 898 queries and 105 243 query-document pairs. We split the data into three subsets as
follows: 1) we extract all the queries which have documents with a single label. The set of feature
vectors and the corresponding labels form training set L1  which contains around 2000 queries
giving rise to 20 000 query-document pairs. (Some single-labeled data are from editorial database 
where each query has a few ideal results with the same label. Other are bad ranking cases submitted
internally and all the documents for a query are labeled as bad. As we will see those type of single-
labeled data are very useful for learning ranking functions); and 2) we then randomly split the
remaining data by queries  and construct a training set L2 containing about 1300 queries and 40 000
query-document pairs and a test set L3 with about 1400 queries and 44 000 query-document pairs.
We use L2 or L3 to generate a set of preference data as follows: given a query q and two documents
dx and dy. Let the feature vectors for (q; dx) and (q; dy) be x and y  respectively. If dx has a higher
grade than dy  we include the preference x ´ y while if dy has a higher grade than dx  we include
the preference y ´ x. For each query  we consider all pairs of documents within the search results
for that query except those with equal grades. This way  we generate around 500 000 preference
pairs in total. We denote the preference data as P2 and P3 corresponding to L2 and L3  respectively.
EVALUATION METRICS. The output of QBrank is a ranking function h which is used to rank the
documents x according to h(x). Therefore  document x is ranked higher than y by the ranking
function h if h(x) > h(y)  and we call this the predicted preference. We propose the following two
metrics to evaluate the performance of a ranking function with respect to a given set of preferences
which we considered as the true preferences.

1) Precision at K%: for two documents x and y (with respect to the same query)  it is reasonable to
assume that it is easy to compare x and y if jh(x) ¡ h(y)j is large  and x and y should have about
the same rank if h(x) is close to h(y). Base on this  we sort all the document pairs hx; yi according
to jh(x) ¡ h(y)j. We call precision at K%  the fraction of non-contradicting pairs in the top K% of
the sorted list. Precision at 100% can be considered as an overall performance measure of a ranking
function.

2) Discounted Cumulative Gain (DCG): DCG has been widely used to assess relevance in the context
of search engines [10]. For a ranked list of N documents (N is set to be 5 in our experiments)  we
i=1 Gi= log2 (i + 1)  where Gi represents the
weights assigned to the label of the document at position i. Higher degree of relevance corresponds
to higher value of the weight.

use the following variation of DCG  DCGN = PN

PARAMETERS. There are three parameters in QBrank: ¿   ·  and w. In our experiments  ¿ is the
absolute grade difference between each pair hxi; yii. We set · to be 0.05  and w to be 0.5 in our

5

Table 1: Precision at K% for QBrank  GBT  and RankSVM

QBrank
%K
0.9446
10%
0.903
20%
0.8611
30%
0.8246
40%
0.7938
50%
0.7673
60%
0.7435
70%
0.7218
80%
90%
0.7015
100% 0.6834

GBT RankSVM
0.8524
0.8152
0.7839
0.7578
0.7357
0.7151
0.6957
0.6779
0.6615
0.6465

0.9328
0.8939
0.8557
0.8199
0.7899
0.7637
0.7399
0.7176
0.6977
0.6803

experiments. For a fair comparsion  we used single regression tree with 20 leaf nodes as the base
regressor of both GBT and QBrank in our experiments. · and number of leaf nodes were tuned for
GBT through cross validation. We did not retune them for QBrank.

EXPERIMENTS AND RESULTS. We are interested in the following questions: 1) How does GBT
using labeled data L2 compare with QBrank or RankSVM using the preference data extracted from
the same labeled data: P2? and 2) Is it useful to include single-labeled data L1 in GBT and QBrank?
To this end  we considered the following six experiments for comparison: 1) GBT using L1  2) GBT
using L2  3) GBT using L1 [ L2  4) RankSVM using P2  5) QBrank using P2  and 6) QBrank using
P2 [ L1.
Table 1 presents the precision at K% on data P3 for the ranking function learned from GBT with
labeled training data L2  and QBrank and RankSVM with the corresponding preference data P2.
This shows that QBrank outperforms both GBT and RankSVM with respect to the precision at K%
metric.
The DCG-5 for RankSVM using P2 is 6.181 while that for the other £ve methods are shown in
Figure 1  from which we can see it is useful to include single-labeled data in GBT training. In case
of preference learning  no preference pairs could be extracted from single labeled data. Therefore 
existing methods such as RankSVM  RankNet and RankBoost that are formulated for preference
data only can not take advantage of such data. The QBrank framework can combine preference data
and labeled data in a natural way. From Figure 1  we can see QBrank using combined preference
data and labeled data outperforms both QBrank and RankSVM using preference data only  which
indicates that singled labeled data are also useful to QBrank training. Another observation is that
GBT using labeled data is signi£cantly worse than QBrank using preference data extracted from the
same labeled data3. The clear convergence trend of QBrank is also demonstrated in Figure 1. Notice
that  we excluded all tied data (pairs of documents with the same grades) when converting preference
data from the absolute relevance judgments  which can be signi£cant information loss  for example
of x1 > x2  and x3 > x4.
If we know x2 ties with x3  then we can have the whole ranking
x1 > fx2; x3g > x4. Including tied data could further improve performance of both GBrank and
QBrank.

5 Conclusions and Future Work

We proposed a general boosting method for optimizing complex loss functions. We also applied
the general framework to the problem of learning ranking functions. Experimental results using a
commercial search engine data show that our approach leads to signi£cant improvements. In future
work  1) we will add regularization to the preference part in the objective function; 2) we plan
to apply our general boosting method to other structured learning problems; and 3) we will also
explore other applications where both preference and labeled data are available for training ranking
functions.

3a 1% dcg gain is considered sign£cant on this data set for commercial search engines.

6

6.9

6.85

6.8

6.75

6.7

6.65

5
-
G
C
D

6.6

50

DCG-5 v. Iterations

100

150

200

250

300

350

400

Iterations (Number of trees)

GBT using L2

GBT using L1

GBT using L1+L2

QBrank using P2

QBrank using P2+L1

Figure 1: DCG v. Iterations. Notice that DCG for RankSVM using P2 is 6.181.

Appendix: Convergence results

n P‘ w‘g(x‘)2.

We introduce a few de£nitions.
De£nition 1 C is scale-invariant if 8g 2 C and ﬁ 2 R  ﬁg 2 C.
De£nition 2 kgkW;X = q 1
De£nition 3 Let h 2 span(C)  then khkW;X = inf nPj jﬁjj : h = Pj ﬁjgj=kgjkW;X ; gj 2 Co.
De£nition 4 Let R(h) be a function of h  an global upper bound M of its Hessian with respect to
[W; X] satisfy: 8h; ﬂ and g: R(h + ﬂg) • R(h) + ﬂrR(h)T g + ﬂ 2
Although we only consider global upper bounds  it is easy to see that results with respect to local
upper bounds can also be established.

2 Mkgk2

W;X.

R(hk+1) • R(„h)+

max(0; R(0)¡R(„h))+inf

i =2)  then

k„hkW;X

k„hkW;X + ak

Theorem 1 Consider Algorithm 1  where R is a convex function of h. Let M be an upper bound of
the Hessian of R. Assume that C is scale-invariant. Let „h 2 span(C). Let „sk = skkgkkW;X be the
normalized step-size  aj = Pj

i=0 „si  and bj = Pi‚j(„sip2†i + M „s2
j •(b0 ¡ bj+1)
+ (bj+1 ¡ bk+1)‚ :
k + „skp†k) < 1  then limk!1 R(hk) =
If we choose „sk ‚ 0 such that Pk „sk = 1 and Pk(„s2
inf „h2span(C) R(„h)  and the rate of convergence compared to any target „h 2 span(C) only depends
on k„hkW;X  and the sequences fajg and fbjg.
The proof is a systematic application of the idea outlined earlier and will be detailed in a sep-
arate publication.
In particu-

In practice  one often set the step size to be a small constant.

lar  for for some £xed s > 0  we can choose p2†i • M s2=2  and skkgkkW;X = s2 when
R(hk + „sk ~gk) • R(hk) („sk = 0 otherwise). Theorem 1 gives the following bound when
k ‚ qk„hkW;X max(0; R(0) ¡ R(„h))=M s¡3 

k„hkW;X + aj
k„hkW;X + ak

R(hk+1) • R(„h) + 2sqmax(0; R(0) ¡ R(„h))k„hkW;X M + M s4:

7

The convergence results show that in order to have a risk not much worse than any target function
„h 2 span(C)  the approximation function hk does not need to be very complex when the complexity
is measured by its 1-norm. It is also important to see that quantities appearing in the generalization
analysis do not depend on the number of samples. These results imply that statistically  Algorithm 1
(with small step-size) has an implicit regularization effect that prevents the procedure from over£ting
the data. Standard empirical process techniques can then be applied to obtain generalization bounds
for Algorithm 1.

References

[1] BALCAN N.  BEYGELZIMER A.  LANGFORD J.  AND SORKIN G. Robust Reductions from Ranking to

Classi£cation  manuscript  2007.

[2] BERTSEKAS D. Nonlinear programming. Athena Scienti£c  second edition  1999.
[3] BURGES  C.  SHAKED  T.  RENSHAW  E.  LAZIER  A.  DEEDS  M.  HAMILTON  N.  AND HULLEN-
DER  G. Learning to rank using gradient descent. Proc. of Intl. Conf. on Machine Learning (ICML)
(2005).

[4] DIETTERICH  T. G.  ASHENFELTER  A.  BULATOV  Y. Training Conditional Random Fields via Gradi-

ent Tree Boosting Proc. of Intl. Conf. on Machine Learning (ICML) (2004).

[5] CLEMENCON S.  LUGOSI G.  AND VAYATIS N. Ranking and scoring using empirical risk minimization.

Proc. of COLT (2005).

[6] COHEN  W. W.  SCHAPIRE  R. E.  AND SINGER  Y. Learning to order things. Journal of Arti£cial

Intelligence Research  Neural Computation  13  14431472 (1999).

[7] FREUND  Y.  IYER  R.  SCHAPIRE  R. E.  AND SINGER  Y. An ef£cient boosting algorithm for com-

bining preferences. Journal of Machine Learning Research 4 (2003)  933–969.

[8] FRIEDMAN  J. H. Greedy function approximation: A gradient boosting machine. Annals of Statistics 29 

5 (2001)  1189–1232.

[9] HERBRICH  R.  GRAEPEL  T.  AND OBERMAYER  K. Large margin rank boundaries for ordinal regres-

sion. 115–132.

[10] JARVELIN  K.  AND KEKALAINEN  J. Ir evaluation methods for retrieving highly relevant documents.

Proc. of ACM SIGIR Conference (2000).

[11] JOACHIMS  T. Optimizing search engines using clickthrough data. Proc. of ACM SIGKDD Conference

(2002).

[12] JOACHIMS  T.  GRANKA  L.  PAN  B.  AND GAY  G. Accurately interpreting clickthough data as

implicit feedback. Proc. of ACM SIGIR Conference (2005).

[13] TSOCHANTARIDIS  I.  JOACHIMS  T.  HOFMANN  T.  AND ALTUN  Y. Large margin methods for
structured and interdependent output variables. Journal of Machine Learning Research  6:1453–1484 
2005.

8

,James Newling
François Fleuret