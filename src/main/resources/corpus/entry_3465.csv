2010,Minimum Average Cost Clustering,A number of objective functions in clustering problems can be described with submodular functions. In this paper  we introduce the minimum average cost criterion  and show that the theory of intersecting submodular functions can be used for clustering with submodular objective functions. The proposed algorithm does not require the number of clusters in advance  and it will be determined by the property of a given set of data points. The minimum average cost clustering problem is parameterized with a real variable  and surprisingly  we show that all information about optimal clusterings for all parameters can be computed in polynomial time in total. Additionally  we evaluate the performance of the proposed algorithm through computational experiments.,Minimum Average Cost Clustering

Kiyohito Nagano

Institute of Industrial Science
University of Tokyo  Japan

The Institute of Scientiﬁc and Industrial Research

Yoshinobu Kawahara

Osaka University  Japan

nagano@sat.t.u-tokyo.ac.jp

kawahara@ar.sanken.osaka-u.ac.jp

Satoru Iwata

Research Institute for Mathematical Sciences

Kyoto University  Japan

iwata@kurims.kyoto-u.ac.jp

Abstract

A number of objective functions in clustering problems can be described with
submodular functions.
In this paper  we introduce the minimum average cost
criterion  and show that the theory of intersecting submodular functions can be
used for clustering with submodular objective functions. The proposed algorithm
does not require the number of clusters in advance  and it will be determined by
the property of a given set of data points. The minimum average cost clustering
problem is parameterized with a real variable  and surprisingly  we show that all
information about optimal clusterings for all parameters can be computed in poly-
nomial time in total. Additionally  we evaluate the performance of the proposed
algorithm through computational experiments.

1

Introduction

A clustering of a ﬁnite set V of data points is a partition of V into subsets (called clusters) such
that data points in the same cluster are similar to each other. Basically  a clustering problem asks
for a partition P of V such that the intra-cluster similarity is maximized and/or the inter-cluster
similarity is minimized. The clustering of data is one of the most fundamental unsupervised learning
problems. We use a criterion function deﬁned on all partitions of V   and the clustering problem
becomes that of ﬁnding a partition of V that minimizes the clustering cost under some constraints.
Suppose that the inhomogeneity of subsets of the data set is measured by a nonnegative set function
f : 2V → R with f (∅) = 0  where 2V denotes the set of all subsets of V   and the clustering cost
of a partition P = {S1  S2  . . .   Sk} is deﬁned by f [P] = f (S1) + · · · + f (Sk). A number of set
functions that represent the inhomogeneity  including cut functions of graphs and entropy functions 
are known to be submodular [3  4]. Throughout of this paper  we suppose that f is submodular  that
is  f (S) + f (T ) ≥ f (S ∪ T ) + f (S ∩ T ) for all S  T ⊆ V . A submodular function is known to be
a discrete counterpart of a convex function  and in recent years  its importance has been recognized
in the ﬁeld of machine learning.
For any given integer k with 1 ≤ k ≤ n  where n is the number of points in V   a partition P of
V is called a k-partition if there are exactly k nonempty elements in P  and is called an optimal
k-clustering if P is a k-partition that minimizes the cost f [P] among all k-partitions. A problem of
ﬁnding an optimal k-clustering is widely studied in combinatorial optimization and various ﬁelds 
and it is recognized as a natural formulation of a clustering problem [8  9  10]. Even if f is a cut
function of a graph  which is submodular and symmetric  that is  f (V − S) = f (S) for all S ⊆ V  
this problem is known to be NP-hard unless k can be regarded as a constant [5]. Zhao et al. [17]
and Narasimhan et al. [10] dealt with the case when f is submodular and symmetric. Zhao et al.
[17] gave a 2(1−1/k)-approximation algorithm using Queyranne’s symmetric submodular function
minimization algorithm [13]. Narasimhan et al. [10] showed that Queyranne’s algorithm can be used

1

for clustering problems with some speciﬁc natural criteria. For a general submodular function and
a small constant k  constant factor approximation algorithms for optimal k-clusterings are designed
in [12  18]. In addition  balanced clustering problems with submodular costs are studied in [8  9].
Generally speaking  it is difﬁcult to ﬁnd an optimal k-clustering for any given k because the opti-
mization problem is NP-hard even for simple special cases. Furthermore  the number of clusters
has to be determined in advance  regardless of the property of the data points  or an additional com-
putation is required to ﬁnd a proper number of clusters via some method like cross-validation. In
this paper  we introduce a new clustering criterion to resolve the above shortcomings of previous
approaches [10]. In the minimum average cost (MAC) clustering problem we consider  the objec-
tive function is the average cost of a partition P which combines the clustering cost f [P] and the
number of clusters |P|. Now the number of clusters is not pre-determined  but it will be determined
automatically by solving the combinatorial optimization problem. We argue that the MAC clustering
problem represents a natural clustering criterion. In this paper  we show that the Dilworth truncation
of an intersecting submodular function [2] (see also Chapter II of Fujishige [4] and Chapter 48 of
Schrijver [14]) can be used to solve the clustering problem exactly and efﬁciently. To the best of
our knowledge  this is the ﬁrst time that the theory of intersecting submodular functions is used for
clustering. The MAC clustering problem can be parameterized with a real-valued parameter β ≥ 0 
and the problem with respect to β asks for a partition P of V that minimizes the average cost under
a constraint |P| > β. The main contribution of this paper is a polynomial time algorithm that solves
the MAC clustering problem exactly for any given parameter β. This result is in stark contrast to the
NP-hardness of the optimal k-clustering problems. Even more surprisingly  our algorithm computes
all information about MAC clusterings for all parameters in polynomial time in total.
In the case where f is a cut function of a graph  there are some related works. If f is a cut function
and β = 1  the optimal value of the MAC clustering problem coincides with the strength of a graph
[1]. In addition  the computation of the principal sequence of partitions of a graph [7] is a special
case of the parametrized MAC clustering problem in an implicit way.

This paper is organized as follows. In Section 2  we formulate the minimum average cost clustering
problem  and show a structure property of minimum average cost clusterings.
In Section 3  we
propose a framework of our algorithm for the minimum average cost clustering problem. In Section
4  we explain the basic results on the theory of intersecting submodular functions  and describe the
Dilworth truncation algorithm which is used in Section 3 as a subroutine. Finally  we show the result
of computational experiments in Section 5  and give concluding remarks in Section 6.

2 Minimum Average Cost Clustering

In this section  we give a deﬁnition of minimum average cost clusterings. After that  we show
a structure property of them. Let V be a given set of n data points  and let f : 2V → R be a
nonnegative submodular function with f (∅) = 0  which is not necessarily symmetric. For each
subset S ⊆ V   the value f (S) represents the inhomogeneity of data points in S. For a partition
P = {S1  . . .   Sk}  the clustering cost is deﬁned by f [P] = f (S1) + · · · + f (Sk). We will
introduce the minimum average cost criterion in order to make consideration of both the clustering
cost f [P] and the number of clusters |P|.

2.1 Deﬁnition

Consider a k-partition P of V with k > 1  and compare P with a trivial partition {V } of V . Then 
the number of clusters has increased by k − 1 and the clustering cost has increased by f [P] + c 
where c is a constant. Therefore  it is natural to deﬁne an average cost of P by f [P]/(|P| − 1).
Suppose that P ∗ is a partition of V that minimizes the average cost among all partitions P of V with
|P| > 1. Remark that the number of clusters of P ∗ is determined not by us  but by the property of
the given data set. Therefore  it may be said that P ∗ is a natural clustering.
More generally  using a parameter β ∈ [0  n) = {τ ∈ R : 0 ≤ τ < n}  we deﬁne an extended
average cost by f [P]/(|P| − β). For any parameter β ∈ [0  n)  we consider the minimum average
cost (MAC) clustering problem

λ(β) := min

P

{f [P]/(|P| − β) : P is a partition of V   |P| > β} .

(1)

2

Let us say that a partition P is a β-MAC clustering if P is optimal for the problem (1) with respect
to β ∈ [0  n). Naturally  the case where β = 1 is fundamental. Furthermore  we can expect ﬁner
clusterings for relatively large parameters. The problem (1) and the optimal k-clustering problem
[10] are closely related.
Proposition 1. Let P be a β-MAC clustering for some β ∈ [0  n)  and set k := |P|. Then we have
f [P] ≤ f [Q] for any k-partition Q of V . In other words  P is an optimal k-clustering.
Proof. By deﬁnition  we have k > β and f [P]/(k − β) ≤ f [Q]/(k − β) for any k-partition Q.

We will show that all information about β-MAC clusterings for all parameters β can be computed in
polynomial time in total. Our algorithm requires the help of the theory of intersecting submodular
functions [4  14]. Proposition 1 says that if there exists a β-MAC clustering P satisfying |P| = k 
then we obtain an optimal k-clustering. Note that this fact is consistent with the NP-hardness of the
optimal k-clustering problem because the information about MAC clusterings just gives a portion of
the information about optimal k-clusterings (k = 1  . . .   n).

2.2 Structure property

We will investigate the structure of all β-MAC clusterings. Denote by R+ the set of nonnegative
real values. Let us choose a parameter β ∈ [0  n). If P is a partition of V satisfying |P| ≤ β  we
have −βλ ≤ −|P|λ ≤ f [P] − |P|λ for all λ ∈ R+. Hence the minimum average cost λ(β) deﬁned
in (1) is represented as

λ(β) = max{λ ∈ R+ : λ ≤ f [P]/(|P| − β) for all partition P of V with |P| > β}

= max{λ ∈ R+ : −βλ ≤ f [P] − |P|λ for all partition P of V }
= max{λ ∈ R+ : −βλ ≤ h(λ)} 

where h : R+ → R is deﬁned by

h(λ) = min

P

{f [P] − |P|λ : P is a partition of V } (λ ≥ 0).

(2)

(3)

The function h does not depend on the parameter β. For λ ≥ 0  we say that a partition P determines
h at λ if f [P] − |P|λ = h(λ). Apparently  the minimization problem (3) is difﬁcult to solve for any
given λ ≥ 0. This point will be discussed in Section 4 in detail.
Let us examine properties of the function h. For each partition P of V   deﬁne a linear function
hP : R+ → R as hP (λ) = f [P] − |P|λ. Since h is the minimum of these linear functions  h is
a piecewise-linear concave function on R+. The function h is illustrated in Figure 1 by the thick
curve. We have h(0) = f (V ) because f [{V }] ≤ f [P] for any partition P of V . Moreover  it is easy
to see that the set of singletons {{1}  {2}  . . .   {n}} determines h at a sufﬁciently large λ. In view
of (2)  the minimum average cost λ(β) can be obtained by solving the equation −βλ = h(λ) (see
also Figure 1). In addition  a β-MAC clustering can be characterized as follows.
Lemma 2. Given a parameter β ∈ [0  n)  let P be a partition of V such that |P| > β and
h(λ(β)) = f [P] − |P|λ(β). Then P is a β-MAC clustering.
Proof. Since −βλ(β) = h(λ(β)) = f [P] − |P|λ(β)  we have λ(β) = f [P]/(|P| − β). For
any partition Q of V with |Q| > β  we have −βλ(β) ≤ f [Q] − |Q|λ(β)  and thus λ(β) ≤
f [Q]/(|Q| − β). Therefore  P is a β-MAC clustering.

h (λ)

f (V )

0

hP (λ) = f [P] - |P| λ 

h (λ)

Ps1

Ps2

λ

− β λ 

0

h (λ)

0

Ps3

Ps4

λ

Ps5

λ (0)

I1

I2

I3

I4 λ
B1
B2
B3

λ ( β )

Figure 1: The function h

(a)

(b)

Figure 2: The structure of h

Now  we will present a structure property of the MAC problem (1). Suppose that the slopes of h
take values −s1 > −s2 > · · · > −sd. As {s1  s2  . . .   sd} ⊆ {1  . . .   n}  we have d ≤ n. The

3

interval R+ is split into d subintervals R1 = [0  λ1)  R2 = [λ1  λ2)  . . .   Rd = [λd−1  +∞)
such that  for each j = 1  . . .   d  the function h is linear and its slope is −sj on Rj. Let
Ps1  Ps2  . . .   Psd be partitions of V such that  for each j = 1  . . .   d  the partition Psj deter-
mines h at all λ ∈ Rj (see Figure 2 (a)). In particular  sd = n and the last partition Psd is the set
of singletons {{1}  {2}  . . .   {n}}. Observe that the range I of the minimum average costs λ(β) is
I = [λ(0)  +∞). Suppose that j∗ is an index such that λ(0) ∈ Rj∗. Let d∗ = d − j∗ + 1  and let
j = λj+j∗−1 and s∗
j = sj+j∗−1 for each j = 1  . . .   d∗. The interval I is split into d∗ subintervals
λ∗
d∗−1  +∞). Accordingly  the domain of β is split
I1 = [λ(0)  λ∗
into d∗ subintervals B1 = [0  β1)  B2 = [β1  β2)  . . .   Bd = [βd∗−1  n)  where βj = −h(λ∗
j )/λ∗
j
for each j = 1  . . .   d∗ − 1. Figure 2 (b) illustrates these two sets of subintervals {I1  . . .   Id∗} and
{B1  . . .   Bd∗}. By Lemma 2  we directly obtain the structure property of the MAC problem (1):
Lemma 3. Let j ∈ {1  . . .   d∗}. For any β ∈ Bj  the partition Ps∗
Lemma 3 implies that if we can ﬁnd the collection {Ps1  Ps2   . . .   Psd}  then the MAC problem
(1) will be completely solved. In the subsequent sections  we will give an algorithm that computes
the collection {Ps1   Ps2  . . .   Psd} in polynomial time in total.

is a β-MAC clustering.

2)  . . .   Id∗ = [λ∗

1)  I2 = [λ∗

1  λ∗

j

3 The clustering algorithm

In this section  we present a framework of a polynomial time algorithm that ﬁnds the collection
{Ps1   Ps2   . . .   Psd} deﬁned in §2.2. That is  our algorithm computes all the breakpoints of the
piecewise linear concave function h deﬁned in (3). By Lemma 3  we can immediately construct a
polynomial time algorithm that solves the MAC problem (1) completely.

The proposed algorithm uses the following procedure FINDPARTITION  which will be described in
Section 4 precisely.

Procedure FINDPARTITION(λ): For any given λ ≥ 0  this procedure computes the value
h(λ) and ﬁnds a partition P of V that determines h at λ.

We will use SFM(n) to denote the time required to minimize a general submodular function deﬁned
on 2V   where n = |V |. Submodular function minimization can be solved in polynomial time
(see [6]). Although the minimization problem (3) is apparently hard  we show that the procedure
FINDPARTITION can be designed to run in polynomial time.
Lemma 4. For any λ ≥ 0  the procedure FINDPARTITION(λ) runs in O(n · SFM(n)).

The proof of Lemma 4  which will be given in §4  utilizes the Dilworth truncation of an intersecting
submodular function [4  14].
Let us call a partition P of V supporting if there exists λ ≥ 0 such that h(λ) = hP (λ). By deﬁni-
tion  each Psj is supporting. In addition  for any λ ≥ 0  FINDPARTITION(λ) returns a supporting
partition of V . Set Q1 := {V } and Qn := {{1}  {2}  . . .   {n}}. Q1 is a supporting partition of V
because h(0) = f [{V }] = hQ1(0)  and Qn is also supporting because Qn = Psd. For a supporting
partition P of V   if |P| = sj for some j ∈ {1  . . .   d}  then we can put Psj = P. For integers
1 ≤ k < ℓ ≤ n  deﬁne R(k  ℓ) = {λ ∈ R+ : −k ≥ ∂+h(λ)  and ∂−h(λ) ≥ −ℓ}  where ∂+h
and ∂−h are the right and left derivatives of h  respectively  and we set ∂−h(0) = 0. Observe that
R(k  ℓ) is an interval in R+. All breakpoints of h are included in R(1  n) = R+.
Suppose that we are given two supporting partitions Qk and Qℓ such that |Qk| = k  |Qℓ| = ℓ
and k < ℓ. We describe the algorithm SPLIT(Qk  Qℓ)  which computes the information about all
breakpoints of h on the interval R(k  ℓ). This algorithm is a recursive one. First of all  the algorithm
SPLIT decides whether “k = sj and ℓ = sj+1 for some j ∈ {1  . . .   d − 1}” or not. Besides  if
the decision is negative  the algorithm ﬁnds a supporting partition Qm such that |Qm| = m and
k < m < ℓ. If the decision is positive  there is exactly one breakpoint on the interior of R(k  ℓ) 
which can be given by Qk and Qℓ. Now we show how to execute these operations. For two linear
functions hQk (λ) and hQℓ (λ)  the equality hQk (λ) = hQℓ(λ) holds at λ = (f [Qℓ]−f [Qk])/(ℓ−k).
Set h = hQk (λ) = (ℓf [Qk] − kf [Qℓ])/(ℓ − k). Clearly  we have h(λ) ≤ h. The algorithm SPLIT
performs the procedure FINDPARTITION(λ). Consider the case where h(λ) = h (see Figure 3 (a)).
Then algorithm gives an afﬁrmative answer  returns Qk and Qℓ  and stops. Next  consider the case
where h(λ) < h (see Figure 3 (b)). Then the algorithm gives a negative answer  and the partition

4

P returned by FINDPARTITION is supporting and satisﬁes k < |P| < ℓ. We set m = |P| and
Qm = P. Finally  the algorithm performs SPLIT(Qk  Qm) and SPLIT(Qm  Qℓ).

h (λ)

0

hQℓ

(λ)

(λ  h)

λ

h (λ)

hQℓ

(λ)

(λ  h)
λ

hQk

(λ)

(a)

0

hQk

(λ)

(b)

Figure 3: Two different situations in SPLIT(Qk  Qℓ)

The algorithm SPLIT can be summarized as follows.

Algorithm SPLIT(Qk  Qℓ)
Input :
Output : The information about all breakpoints of h on the interval R(k  ℓ).

Supporting partitions of V   Qk and Qℓ such that |Qk| = k  |Qℓ| = ℓ and k < ℓ.

1:

2:
3:

Set λ := (f [Qℓ] − f [Qk])/(ℓ − k)  and set h := (ℓf [Qk] − kf [Qℓ])/(ℓ − k). By performing
FINDPARTITION(λ)  compute h(λ) and a partition P of V that determines h(λ).
If h(λ) = h (positive case)  return Qk and Qℓ  and stop.
If h(λ) < h (negative case)  set m := |P|  Qm := P  and perform SPLIT(Qk  Qm) and
SPLIT(Qm  Qℓ).

By performing the algorithm SPLIT(Q1  Qn)  where Q1 := {V } and Qn := {{1}  {2}  . . .   {n}} 
the information of all breakpoints of h is obtained. Therefore  the collection {Ps1  Ps2  . . .   Psd}
deﬁned in §2.2 can be obtained. Let us show that this algorithm runs in polynomial time.
Theorem 5. The collection {Ps1  Ps2  . . .   Psd} can be computed in O(n2 · SFM(n)) time. In
other words  the information of all breakpoints of h can be computed in O(n2 · SFM(n)) time.

Proof. By Lemma 4  it sufﬁces to show that the number of calls of the procedure FINDPARTITION
in the execution of SPLIT(Q1  Qn) is O(n). In the algorithm  after one call of FINDPARTITION 
(i) we can obtain the information about one breakpoint of h  or (ii) a new supporting partition Qm
can be obtained. Clearly  the number of breakpoints of h is at most n. Throughout the execution
of SPLIT(Q1  Qn)  the algorithm computes a supporting k-partition at most once for each k ∈
{1  . . .   n}. Therefore  FINDPARTITION is called at most 2n times in total.

The main theorem of this paper directly follows from Lemma 3 and Theorem 5.
Theorem 6. All information of optimal solutions to the minimum average cost clustering problem
(1) for all parameters β ∈ [0  n) can be computed in O(n2 · SFM(n)) time in total.

4 Finding a partition

In the clustering algorithm of Section 3  we iteratively call the procedure FINDPARTITION  which
computes h(λ) deﬁned in (3) and a partition P that determines h(λ) for any given λ ≥ 0. In this
section  we will see that the procedure FINDPARTITION can be implemented to run in polynomial
time with the aid of the Dilworth truncation of an intersecting submodular function [2]  and give a
proof of Lemma 4. The Dilworth truncation algorithm is sketched in the proof of Theorem 48.4 of
Schrijver [14]  and the algorithm described in §4.2 is based on that algorithm.

4.1 The Dilworth truncation of an intersecting submodular function

We start with deﬁnitions of an intersecting submodular function and the Dilworth truncation. Subsets
S  T ⊆ V are intersecting if S ∩ T 6= ∅  S \ T 6= ∅  and T \ S 6= ∅. A set function g : 2V → R
is intersecting submodular if g(S) + g(T ) ≥ g(S ∪ T ) + g(S ∩ T ) for all intersecting subsets
S  T ⊆ V . Clearly  the fully submodular function1 f is also intersecting submodular. For any λ ≥ 0 

1To emphasize the difference between submodular and intersecting submodular functions  in what follows

we refer to a submodular function as a fully submodular function.

5

deﬁne fλ : 2V → R as follows: fλ(S) = 0 if S = ∅  and fλ(S) = f (S) − λ otherwise. It is easy to
see that fλ is an intersecting submodular function.
For a fully submodular function f with f (∅) = 0  consider a polyhedron P(f ) = {x ∈ Rn : x(S) ≤
f (S)  ∅ 6= ∀S ⊆ V }  where x(S) = Pi∈S xi. The polyhedron P(f ) is called a submodular
polyhedron. In the same manner  for an intersecting submodular function g with g(∅) = 0  deﬁne
P(g) = {x ∈ Rn : x(S) ≤ g(S)  ∅ 6= ∀S ⊆ V }. As for P(f )  for each nonempty subset S ⊆ V  
there exists a vector x ∈ P(f ) such that x(S) = f (S) by the validity of the greedy algorithm of
Edmonds [3]. On the other hand  the polyhedron P(g) does not necessarily satisfy such a property.
Alternatively  the following property is known.

Theorem 7 (Refer to Theorems 2.5  2.6 of [4]). Given an intersecting submodular function g :
2V → R with g(∅) = 0  there exists a fully submodular function bg : 2V → R such that bg(∅) = 0
and P(bg) = P(g). Furthermore  the function bg can be represented as

bg(S) = min{PS∈P g(S) : P is a partition of S}.

(4)

The function bg in Theorem 7 is called the Dilworth truncation of g. If g is fully submodular  for
each S ⊆ V   {S} is an optimal solution to the RHS of (4) and we have bg(S) = g(S). For a general
intersecting submodular function g  however  the computation of bg(S) is a nontrivial task.
Let us see a small example. Suppose that a fully submodular function f : 2{1  2} → R satisﬁes
f (∅) = 0  f ({1}) = 12  f ({2}) = 8  and f ({1  2}) = 19. Set λ = 2. There is no vector x ∈ P(fλ)
such that x({1  2}) = fλ({1  2}). The Dilworth truncation bfλ : 2V → R deﬁned by (4) satisﬁes
bfλ(S) = fλ(S) for S ∈ {∅  {1}  {2}}  and bfλ({1  2}) = fλ({1}) + fλ({2}) = 16. Observe that
bfλ is fully submodular and P( bfλ) = P(fλ). Figure 4 illustrates these polyhedra.

19

8

0

x2

x2

x2

17

6

0

P(fλ)

x1

10

16

6

0

P( bfλ)

x1

10

x2
6

x0
0

10 e1

x1
10 x1

x2
6

x0
0

6 e2

x2

x1
10 x1

P(f )

x1

12

Figure 4: Polyhedra P(f )  P(fλ)  and P( bfλ)

Figure 5: The greedy algorithm [3]

4.2 Algorithm that ﬁnds a partition

Let us ﬁx λ ≥ 0  and describe FINDPARTITION(λ). In view of equations (3)  (4) and the deﬁnition
of bfλ  we obtain h(λ) = bfλ(V ) using the Dilworth truncation of fλ. We ask for a partition P of V
satisfying bfλ(V ) = fλ[P] (= PT ∈P fλ(T )) because such a partition P of V determines h at λ.
We know that bfλ : 2V → R is submodular  but bfλ(S) = min{fλ[P] : P is a partition of S} cannot
be obtained directly for each S ⊆ V . To evaluate bfλ(V )  we will use the greedy algorithm of
Edmonds [3]. Denote the set of all extreme points of P( bfλ) ⊆ Rn by ex(P( bfλ)). In the example
of §4.1  we have ex(P( bfλ)) = {(10  6)}. We set x
0 ≤ y for all
i := −M for each i ∈ V   where M = λ + Pj∈V {|f ({j})| +
y ∈ ex(P( bfλ)). For example  set x0
|f (V ) − f (V − {j})|}. For each i ∈ V   let ei denote the i-th unit vector in Rn.

0 ∈ Rn in such a way that x

Let L = (i1  . . .   in) be any ordering of V   and let V ℓ = {i1  . . .   iℓ} for each ℓ = 1  . . .   n.
Now we describe the framework of the greedy algorithm [3]. In the ℓ-th iteration (ℓ = 1  . . .   n) 
ℓ−1 + αℓ · eiℓ. Finally  the
we compute αℓ := max{α : x
algorithm returns z := x
n. Figure 5 illustrates this process. By the following property  we can use
the greedy algorithm to evaluate the value h(λ) = bfλ(V ).
Theorem 8 ([3]). For each ℓ = 1  . . .   n  we have bfλ(V ℓ) = xℓ(V ℓ) = z(V ℓ).

ℓ−1 + α · eiℓ ∈ P( bfλ)} and set x

ℓ := x

6

Let us see that the greedy algorithm with bfλ can be implemented to run in polynomial time. We
discuss how to compute αℓ in each iteration. Since xℓ−1 ∈ P( bfλ) and P( bfλ) = P(fλ)  we have
ℓ−1 + α · eiℓ ∈ P(fλ)} = max{α : xℓ−1(S) + α ≤ fλ(S)  iℓ ∈ ∀S ⊆ V }

αℓ = max{α : x

= min{f (S) − xℓ−1(S) − λ : iℓ ∈ ∀S ⊆ V }
= min{f (S) − xℓ−1(S) − λ : iℓ ∈ ∀S ⊆ V ℓ} 

(5)

where the last equality holds because of the choice of the initial vector x
i for
all i ∈ V − V ℓ). Hence  the value αℓ can be computed by minimizing a fully submodular function.
It follows from Theorem 8 that the value h(λ) = bfλ(V ) can be computed in O(n · SFM(n)) time.
In addition to the value h(λ)  a partition P of V such that f [P] − λ|P| = h(λ) is also required. For
this purpose  we modify the above greedy algorithm  and obtain the procedure FINDPARTITION.

0 (remark that xℓ−1

i = x0

A nonnegative real value λ ≥ 0.

Procedure FINDPARTITION(λ)
Input :
Output : A real value hλ and a partition Pλ of V .
1:
2:

Set P 0 := ∅.
For each ℓ = 1  . . .   n  do:

Compute αℓ = min{f (S) − xℓ−1(S) − λ : iℓ ∈ ∀S ⊆ V ℓ};
Find a subset T ℓ such that iℓ ∈ T ℓ ⊆ V ℓ and f (T ℓ) − xℓ−1(T ℓ) − λ = αℓ;
Set x
P ℓ := {U ℓ} ∪ {S : S ∈ P ℓ−1  T ℓ ∩ S = ∅}.

ℓ−1 + αℓ · eiℓ  set U ℓ := T ℓ ∪ [ ∪{S : S ∈ P ℓ−1  T ℓ ∩ S 6= ∅}]  and set

ℓ := x

3: Return hλ := z(V ) and Pλ := P n.

iℓ

T ℓ

P ℓ

U ℓ

P ℓ−1

Basically  this procedure FINDPARTITION(λ) is the
same algorithm as the above greedy algorithm. But
now  we compute P ℓ in each iteration. Figure 6 shows
the computation of P ℓ in the ℓ-th iteration of the pro-
cedure FINDPARTITION(λ). For each ℓ = 1  . . .   n 
P ℓ is a partition of V ℓ = {i1  . . .   iℓ}. Thus  Pλ is a
partition of V .
Let x be a vector in P(fλ). We say that a subset S ⊆ V is x-tight (with respect to fλ) if fλ(S) =
x(S). By the intersecting submodularity of fλ  if S and T are intersecting and both S and T are
x-tight  then S ∪ T is also x-tight. Using this property  we obtain the following property.
Lemma 9. For each ℓ = 1  . . .   n  we have bfλ(V ℓ) = xℓ(V ℓ) = fλ[P ℓ].
Proof. (Sketch) For each ℓ = 1  . . .   n  observe that T ℓ is x
tion that any cluster in P ℓ is x
PS∈P ℓ xℓ(S) = xℓ(V ℓ). Moreover  the equality bfλ(V ℓ) = xℓ(V ℓ) follows from Theorem 8.
The procedure FINDPARTITION(λ) returns hλ ∈ R and Pλ. By Theorem 8  we have hλ = h(λ)  and
by Lemma 9  we have bfλ(V ) = fλ[Pλ]  and thus the partition Pλ of V determines h(λ). Clearly 
the procedure runs in O(n · SFM(n)) time. So  in the end  we completed the proof of Lemma 4.

ℓ-tight. Thus  we can show by induc-
ℓ-tight for each ℓ = 1  . . .   n. Thus  fλ[P ℓ] = PS∈P ℓ fλ(S) =

Figure 6: Computation of P ℓ

5 Experimental results

5.1

Illustrative example

We ﬁrst illustrate the proposed algorithm using two artiﬁcial datasets depicted in Figure 7. The above
dataset is generated from four Gaussians with unit variance (whose centers are located at (3 3)  (3 -
3)  (-3 3) and (-3 -3)  respectively)  and the below one consists of three cycles with different radii
with a line. The numbers of samples in these examples are 100 and 310  respectively. Figure 7
shows the typical examples of partitions calculated through Algorithm SPLIT given in Section 3.
Now the function f is a cut function of a complete graph and the weight of each edge of that graph
is determined by the Gaussian similarity function [15]. The values of λ above the ﬁgures are the

7

λ = 0.19

λ = 0.54

λ = 5.21

(cid:25)

(cid:23)

(cid:21)

(cid:19)

(cid:827)(cid:21)

(cid:827)(cid:23)

(cid:827)(cid:25)

(cid:25)

(cid:23)

(cid:21)

(cid:19)

(cid:827)(cid:21)

(cid:827)(cid:23)

(cid:827)(cid:25)

(cid:25)

(cid:23)

(cid:21)

(cid:19)

(cid:827)(cid:21)

(cid:827)(cid:23)

(cid:827)(cid:25)

(cid:827)(cid:25)

(cid:827)(cid:23)

(cid:827)(cid:21)

(cid:19)

(cid:21)

(cid:23)

(cid:25)

(cid:827)(cid:25)

(cid:827)(cid:23)

(cid:827)(cid:21)

(cid:19)

(cid:21)

(cid:23)

(cid:25)

(cid:827)(cid:25)

(cid:827)(cid:23)

(cid:827)(cid:21)

(cid:19)

(cid:21)

(cid:23)

(cid:25)

λ = 0.87

λ = 3.22

λ = 4.90

(cid:19)(cid:17)(cid:22)

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:20)

(cid:19)

(cid:827)(cid:19)(cid:17)(cid:20)

(cid:827)(cid:19)(cid:17)(cid:21)

(cid:827)(cid:19)(cid:17)(cid:22)

(cid:19)(cid:17)(cid:22)

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:20)

(cid:19)

(cid:827)(cid:19)(cid:17)(cid:20)

(cid:827)(cid:19)(cid:17)(cid:21)

(cid:827)(cid:19)(cid:17)(cid:22)

(cid:19)(cid:17)(cid:22)

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:20)

(cid:19)

(cid:827)(cid:19)(cid:17)(cid:20)

(cid:827)(cid:19)(cid:17)(cid:21)

(cid:827)(cid:19)(cid:17)(cid:22)

(cid:827)(cid:19)(cid:17)(cid:22)

(cid:827)(cid:19)(cid:17)(cid:21)

(cid:827)(cid:19)(cid:17)(cid:20)

(cid:19)

(cid:19)(cid:17)(cid:20)

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:22)

(cid:827)(cid:19)(cid:17)(cid:22)

(cid:827)(cid:19)(cid:17)(cid:21)

(cid:827)(cid:19)(cid:17)(cid:20)

(cid:19)

(cid:19)(cid:17)(cid:20)

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:22)

(cid:827)(cid:19)(cid:17)(cid:22)

(cid:827)(cid:19)(cid:17)(cid:21)

(cid:827)(cid:19)(cid:17)(cid:20)

(cid:19)

(cid:19)(cid:17)(cid:20)

(cid:19)(cid:17)(cid:21)

(cid:19)(cid:17)(cid:22)

Figure 7: Illustrative examples with datasets from four Gaussians (above) and three circles (below).

ones identiﬁed as breakpoints. Note several partitions other than shown in the ﬁgures were obtained
through one execution of Algorithm SPLIT. As can be seen  the algorithm produced several different
sizes of clusters with inclusive relations.

5.2 Empirical comparison

Next  in this subsection  we empirically compare the performance of the algorithm with the ex-
isting algorithms using several synthetic and real world datasets from the UCI repository. The
compared algorithms are k-means method  spectral-clustering method with normalized-cut [11] and
maximum-margin clustering [16]  and we used cut functions as the objective functions for the MAC
clustering algorithm. The three UCI datasets used in this experiment are ’Glass’  ’Iris’ and ’Libras’
which respectively consist of 214  150 and 360 samples  respectively. For the existing algorithms 
the number of clusters was selected through 5-fold cross-validation (again note that our algorithm
needs no such hyper-parameter tuning). Table 1 shows the clustering accuracy when applying the
algorithms to two artiﬁcial (stated in Subsection 5.1 and three UCI datasets. For our algorithm  the
results with the best performance between among several partitions are shown. As can be seen  our
algorithm seems to be competitive with the existing leading algorithms for these datasets.

k-means
normalized cut
maximum margin
minimum average

Gaussian

1.0
0.88
0.99
0.99

Circle
0.88
0.86
1.0
1.0

Iris
0.79
0.84
0.96
0.99

Libras Glass
0.93
0.85
0.93
0.87
0.97
0.90
0.97
0.97

Table 1: Clustering accuracy for the proposed and existing algorithms.

6 Concluding remarks

We have introduced the new concept  the minimum average cost clustering problem. We have shown
that the set of minimum average cost clusterings has a compact representation  and if the clustering
cost is given by a submodular function  we have proposed a polynomial time algorithm that compute
all information about minimum average cost clusterings. This result contrasts sharply with the NP-
hardness of the optimal k-clustering problem [5]. The present paper reinforced the importance of
the theory of intersecting submodular functions from the viewpoint of clustering.

Acknowledgments

This work is supported in part by JSPS Global COE program “Computationism as a Foundation for
the Sciences”  KAKENHI (20310088  22700007  and 22700147)  and JST PRESTO program. We
would also like to thank Takuro Fukunaga for his helpful comments.

8

References

[1] W. H. Cunningham: Optimal attack and reinforcement of a network. Journal of the ACM 32

(1985)  pp. 549–561.

[2] R. P. Dilworth: Dependence relations in a semimodular lattice. Duke Mathematical Journal 

11 (1944)  pp. 575–587.

[3] J. Edmonds: Submodular functions  matroids  and certain polyhedra. CombinatorialStructures
and Their Applications  R. Guy  H. Hanani  N. Sauer  and J. Sch¨onheim  eds.  Gordon and
Breach  1970  pp. 69–87.

[4] S. Fujishige: Submodular Functions and Optimization (Second Edition). Elsevier  Amsterdam 

2005.

[5] O. Goldschmidt and D. S. Hochbaum: A polynomial algorithm for the k-cut problem for ﬁxed

k  Mathematics of Operations Research  19 (1994)  pp. 24–37.

[6] S. Iwata: Submodular function minimization. Mathematical Programming  112 (2008)  pp.

45–64.

[7] V. Kolmogorov: A faster algorithm for computing the principal sequence of partitions of a

graph. Algorithmica 56  pp. 394-412.

[8] Y. Kawahara  K. Nagano  and Y. Okamoto: Submodular fractional programming for balanced

clustering. Pattern Recognition Letters  to appear.

[9] M. Narasimhan and J. Bilmes: Local search for balanced submodular clusterings. In Proceed-
ings of the 12th International Joint Conference on Artiﬁcial Intelligence (IJCAI 2007)  pp.
981–986.

[10] M. Narasimhan  N. Jojic  and J. Bilmes: Q-clustering. In Advances in Neural Information

Processing Systems  18 (2006)  pp. 979–986. Cambridge  MA: MIT Press.

[11] A. Y. Ng  M. I. Jordan  and Y. Weiss. On spectral clustering: Analysis and an algorithm.

Advances in neural information processing systems  2:849–856  2002.

[12] K. Okumoto  T. Fukunaga  and H. Nagamochi: Divide-and-conquer algorithms for partitioning
hypergraphs and submodular systems. In Proceedings of the 20th International Symposium on
Algorithms and Computation (ISAAC 2009)  LNCS 5878  2009  pp. 55–64.

[13] M. Queyranne: Minimizing symmetric submodular functions  Mathematical Programming  82

(1998)  pp. 3–12.

[14] A. Schrijver: CombinatorialOptimization—PolyhedraandEfﬁciency. Springer-Verlag  2003.
[15] U. von Luxburg: Tutorial on spectral clustering. Statistics and Computing 17 (2007)  pp. 395–

416.

[16] L. Xu  J. Neufeld  B. Larson  and D. Schuurmans. Maximum margin clustering. Advances in

neural information processing systems  17:1537–1544  2005.

[17] L. Zhao  H. Nagamochi  and T. Ibaraki: Approximating the minimum k-way cut in a graph via

minimum 3-way cuts. Journal of Combinatorial Optimization  5 (2001)  pp. 397–410.

[18] L. Zhao  H. Nagamochi  and T. Ibaraki: A uniﬁed framework for approximating multiway
partition problems. In Proceedings of the 12th International Symposium on Algorithms and
Computation (ISAAC 2001)  LNCS 2223  2001  pp. 682–694.

9

,Jerry Zhu
Damien Garreau
Rémi Lajugie
Sylvain Arlot
Francis Bach
Steven Cheng-Xian Li
Benjamin Marlin