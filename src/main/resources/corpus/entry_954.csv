2015,Closed-form Estimators for High-dimensional Generalized Linear Models,We propose a class of closed-form estimators for GLMs under high-dimensional sampling regimes. Our class of estimators is based on deriving closed-form variants of the vanilla unregularized MLE but which are (a) well-defined even under high-dimensional settings  and (b) available in closed-form. We then perform thresholding operations on this MLE variant to obtain our class of estimators. We derive a unified statistical analysis of our class of estimators  and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection  that surprisingly match those of the more complex regularized GLM MLEs  even while our closed-form estimators are computationally much simpler. We derive instantiations of our class of closed-form estimators  as well as corollaries of our general theorem  for the special cases of logistic  exponential and Poisson regression models. We corroborate the surprising statistical and computational performance of our class of estimators via extensive simulations.,Closed-form Estimators for High-dimensional

Generalized Linear Models

Eunho Yang

IBM T.J. Watson Research Center

eunhyang@us.ibm.com

Aur´elie C. Lozano

IBM T.J. Watson Research Center

aclozano@us.ibm.com

Pradeep Ravikumar

University of Texas at Austin

pradeepr@cs.utexas.edu

Abstract

We propose a class of closed-form estimators for GLMs under high-dimensional
sampling regimes. Our class of estimators is based on deriving closed-form vari-
ants of the vanilla unregularized MLE but which are (a) well-deﬁned even under
high-dimensional settings  and (b) available in closed-form. We then perform
thresholding operations on this MLE variant to obtain our class of estimators. We
derive a uniﬁed statistical analysis of our class of estimators  and show that it en-
joys strong statistical guarantees in both parameter error as well as variable selec-
tion  that surprisingly match those of the more complex regularized GLM MLEs 
even while our closed-form estimators are computationally much simpler. We de-
rive instantiations of our class of closed-form estimators  as well as corollaries
of our general theorem  for the special cases of logistic  exponential and Poisson
regression models. We corroborate the surprising statistical and computational
performance of our class of estimators via extensive simulations.

1

Introduction

We consider the estimation of generalized linear models (GLMs) [1]  under high-dimensional set-
tings where the number of variables p may greatly exceed the number of observations n. GLMs are
a very general class of statistical models for the conditional distribution of a response variable given
a covariate vector  where the form of the conditional distribution is speciﬁed by any exponential
family distribution. Popular instances of GLMs include logistic regression  which is widely used
for binary classiﬁcation  as well as Poisson regression  which together with logistic regression  is
widely used in key tasks in genomics  such as classifying the status of patients based on genotype
data [2] and identifying genes that are predictive of survival [3]  among others. Recently  GLMs
have also been used as a key tool in the construction of graphical models [4]. Overall  GLMs have
proven very useful in many modern applications involving prediction with high-dimensional data.
Accordingly  an important problem is the estimation of such GLMs under high-dimensional sam-
pling regimes. Under such sampling regimes  it is now well-known that consistent estimators can-
not be obtained unless low-dimensional structural constraints are imposed upon the underlying re-
gression model parameter vector. Popular structural constraints include that of sparsity  which en-
courages parameter vectors supported with very few non-zero entries  group-sparse constraints  and
low-rank structure with matrix-structured parameters  among others. Several lines of work have
focused on consistent estimators for such structurally constrained high-dimensional GLMs. A pop-
ular instance  for the case of sparsity-structured GLMs  is the `1 regularized maximum likelihood
estimator (MLE)  which has been shown to have strong theoretical guarantees  ranging from risk

1

consistency [5]  consistency in the `1 and `2-norm [6  7  8]  and model selection consistency [9].
Another popular instance is the `1/`q (for q  2) regularized MLE for group-sparse-structured lo-
gistic regression  for which prediction consistency has been established [10]. All of these estimators
solve general non-linear convex programs involving non-smooth components due to regularization.
While a strong line of research has developed computationally efﬁcient optimization methods for
solving these programs  these methods are iterative and their computational complexity scales poly-
nomially with the number of variables and samples [10  11  12  13]  making them expensive for very
large-scale problems.
A key reason for the popularity of these iterative methods is that while the number of iterations
are some function of the required accuracy  each iteration itself consists of a small ﬁnite number
of steps  and can thus scale to very large problems. But what if we could construct estimators
that overall require only a very small ﬁnite number of steps  akin to a single iteration of popular
iterative optimization methods? The computational gains of such an approach would require that
the steps themselves be suitably constrained  and moreover that the steps could be suitably proﬁled
and optimized (e.g. efﬁcient linear algebra routines implemented in BLAS libraries)  a systematic
study of which we defer to future work. We are motivated on the other hand by the simplicity of
such a potential class of “closed-form” estimators.
In this paper  we thus address the following question: “Is it possible to obtain closed-form estimators
for GLMs under high-dimensional settings  that nonetheless have the sharp convergence rates of the
regularized convex programs and other estimators noted above?” This question was ﬁrst considered
for linear regression models [14]  and was answered in the afﬁrmative. Our goal is to see whether
a positive response can be provided for the more complex statistical model class of GLMs as well.
In this paper we focus speciﬁcally on the class of sparse-structured GLMs  though our framework
should extend to more general structures as well.
As an inkling of why closed-form estimators for high-dimensional GLMs is much trickier than that
for high-dimensional linear models is that under small-sample settings  linear regression models do
have a statistically efﬁcient closed-form estimator — the ordinary least-squares (OLS) estimator 
which also serves as the MLE under Gaussian noise. For GLMs on the other hand  even under
small-sample settings  we do not yet have statistically efﬁcient closed-form estimators. A classical
algorithm to solve for the MLE of logistic regression models for instance is the iteratively reweighted
least squares (IRLS) algorithm  which as its name suggests  is iterative and not available in closed-
form. Indeed  as we show in the sequel  developing our class of estimators for GLMs requires far
more advanced mathematical machinery (moment polytopes  and projections onto an interior subset
of these polytopes for instance) than the linear regression case.
Our starting point to devise a closed-form estimator for GLMs is to nonetheless revisit this classical
unregularized MLE estimator for GLMs from a statistical viewpoint  and investigate the reasons
why the estimator fails or is even ill-deﬁned in the high-dimensional setting. These insights enable
us to propose variants of the MLE that are not only well-deﬁned but can also be easily computed
in analytic-form. We provide a uniﬁed statistical analysis for our class of closed-form GLM es-
timators  and instantiate our theoretical results for the speciﬁc cases of logistic  exponential  and
Poisson regressions. Surprisingly  our results indicate that our estimators have comparable statisti-
cal guarantees to the regularized MLEs  in terms of both variable selection and parameter estimation
error  which we also corroborate via extensive simulations (which surprisingly even show a slight
statistical performance edge for our closed-form estimators). Moreover  our closed-form estimators
are much simpler and competitive computationally  as is corroborated by our extensive simulations.
With respect to the conditions we impose on the GLM models  we require that the population covari-
ance matrix of our covariates be weakly sparse  which is a different condition than those typically
imposed for regularized MLE estimators; we discuss this further in Section 3.2. Overall  we hope
our simple class of statistically as well as computationally efﬁcient closed-form estimators for GLMs
would open up the use of GLMs in large-scale machine learning applications even to lay users on the
one hand  and on the other hand  encourage the development of new classes of “simple” estimators
with strong statistical guarantees extending the initial proposals in this paper.

2

2 Setup

We consider the class of generalized linear models (GLMs)  where a response variable y 2Y  
conditioned on a covariate vector x 2 Rp  follows an exponential family distribution:

(1)

P(y|x; ✓⇤) = exp⇢ h(y) + yh✓⇤  xi  Ah✓⇤  xi

c()



2

where  2 R > 0 is ﬁxed and known scale parameter  ✓⇤ 2 Rp is the GLM parameter of interest 
and A(h✓⇤  xi) is the log-partition function or the log-normalization constant of the distribution. Our
goal is to estimate the GLM parameter ✓⇤ given n i.i.d. samples(x(i)  y(i)) n
i=1. By properties of
exponential families  the conditional moment of the response given the covariates can be written as
µ(h✓⇤  xi) ⌘ E(y|x; ✓⇤) = A0(h✓⇤  xi).
Examples. Popular instances of (1) include the standard linear regression model  the logistic re-
gression model  and the Poisson regression model  among others.
In the case of the linear re-
gression model  we have a response variable y 2 R  with the conditional distribution P(y|x  ✓⇤):
expny2/2+yh✓⇤ xih✓⇤ xi2/2
o  where the log-partition function (or log-normalization constant)
A(a) of (1) in this speciﬁc case is given by A(a) = a2/2. Another popular GLM instance is
the logistic regression model P(y|x  ✓⇤)  for a categorical output variable y 2 Y ⌘ {1  1} 
expyh✓⇤  xi  log⇥ exp(h✓⇤  xi) + exp(h✓⇤  xi)⇤ where the log-partition function A(a) =
log exp(a) + exp(a). The exponential regression model P(y|x  ✓⇤) in turn is given by:
expyh✓⇤  xi + log  h✓⇤  xi . Here  the domain of response variable Y = R+ is the set
of non-negative real numbers (it is typically used to model time intervals between events for in-
stance)  and the log-partition function A(a) =  log(a). Our ﬁnal example is the Poisson regres-
sion model  P(y|x  ✓⇤): exp log(y!) + yh✓⇤  xi  exph✓⇤  xi where the response variable is
count-valued with domain Y⌘{ 0  1  2  ...}  and with log-partition function A(a) = exp(a).
Any exponential family distribution can be used to derive a canonical GLM regression model (1)
of a response y conditioned on covariates x  by setting the canonical parameter of the exponential
family distribution to h✓⇤  xi. For the parameterization to be valid  the conditional density should be
normalizable  so that Ah✓⇤  xi < +1.
High-dimensional Estimation Suppose that we are given n covariate vectors  x(i) 2 Rp  drawn
from some distribution  and corresponding response variables  y(i) 2Y   drawn from the
i.i.d.
distribution P(y|x(i) ✓ ⇤) in (1). A key goal in statistical estimation is to estimate the parameters
✓⇤ 2 Rp  given just the samples(x(i)  y(i)) n
i=1. Such estimation becomes particularly challenging
in a high-dimensional regime  where the dimension of covariate vector p is potentially even larger
than the number of samples n. In such high-dimensional regimes  it is well understood that structural
constraints on ✓⇤ are necessary in order to ﬁnd consistent estimators. In this paper  we focus on the
structural constraint of element-wise sparsity  so that the number of non-zero elements in ✓⇤ is less
than or equal to some value k much smaller than p: k✓⇤k0  k.
Estimators: Regularized Convex Programs The `1 norm is known to encourage the esti-
mation of such sparse-structured parameters ✓⇤. Accordingly  a popular class of M-estimators
for sparse-structured GLM parameters is the `1 regularized maximum log-likelihood estimator
for (1). Given n samples (x(i)  y(i)) n
i=1 from P(y|x  ✓⇤)  the `1 regularized MLEs can be
i=1 y(i)x(i)↵ + 1
written as: minimize ✓ ⌦✓  1
i=1 Ah✓  x(i)i + nk✓k1 . For nota-
nPn
nPn
tional simplicity  we collate the n observations in vector and matrix forms where we overload
the notation y 2 Rn to denote the vector of n responses so that i-th element of y  yi  is
y(i)  and X 2 Rn⇥p to denote the design matrix whose i-th row is [x(i)]>. With this no-
tation we can rewrite optimization problem characterizing the `1-regularized MLE simply as
n 1>A(X✓ ) + nk✓k1 where we overload the notation A(·) for an
minimize ✓  1
input vector ⌘ 2 Rn to denote A(⌘) ⌘A(⌘1)  A(⌘2)  . . .   A(⌘n)>  and 1 ⌘ (1  . . .   1)> 2 Rn.

n ✓>X>y + 1

3

3 Closed-form Estimators for High-dimensional GLMs

The goal of this paper is to derive a general class of closed-form estimators for high-dimensional
GLMs  in contrast to solving huge  non-differentiable `1 regularized optimization problems. Before
introducing our class of such closed-form estimators  we ﬁrst introduce some notation.
For any u 2 Rp  we use [S(u)]i = sign(ui) max(|ui|   0) to denote the element-wise soft-
thresholding operator  with thresholding parameter . For any given matrix M 2 Rp⇥p  we denote
by T⌫(M ) : Rp⇥p 7! Rp⇥p a family of matrix thresholding operators that are deﬁned point-wise 
so that they can be written as [T⌫(M )]ij := ⇢⌫(Mij)  for any scalar thresholding operator ⇢⌫(·)
that satisﬁes the following conditions: for any input a 2 R  (a) |⇢⌫(a)|| a|  (b) |⇢⌫(a)| = 0 for
|a| ⌫  and (c) |⇢⌫(a)  a| ⌫. The standard soft-thresholding and hard-thresholding operators
are both pointwise operators that satisfy these properties. See [15] for further discussion of such
pointwise matrix thresholding operators.
For any ⌘ 2 Rn  we let rA(⌘) denote the element-wise gradients: rA(⌘) ⌘A0(⌘1)  A0(⌘2)  . . .  
A0(⌘n)>. We assume that the exponential family underlying the GLM is minimal  so that this map
is invertible  and so that for any µ 2 Rn in the range of rA(·)  we can denote [rA]1(µ) as an
element-wise inverse map of rA(·):(A0)1(µ1)  (A0)1(µ2)  . . .   (A0)1(µn)>.
Consider the response moment polytope M := {µ : µ = Ep[y]  for some distribution p over
y 2Y}   and let Mo denote the interior of M. Our closed-form estimator will use a carefully
selected subset
(2)
Denote the projection of a response variable y 2Y onto this subset as ⇧ ¯M(y) = arg minµ2 ¯M |y 
µ|  where the subset M is selected so that the projection step is always well-deﬁned  and the mini-
mum exists. Given a vector y 2Y n  we denote the vector of element-wise projections of entries in
y as ⇧ ¯M(y) so that:
(3)
As the conditions underlying our theorem will make clear  we will need the operator [rA]1(·)
deﬁned above to be both well-deﬁned and Lipschitz in the subset M of the interior of the response
moment polytope. In later sections  we will show how to carefully construct such a subset M for
different GLM models.
We now have the machinery to describe our class of closed-form estimators:

[⇧ ¯M(y)]i := ⇧ ¯M(yi).

M✓M o.

b✓Elem = Sn hT⌫⇣ X>X

n ⌘i1 X>[rA]1⇧ ¯M(y)

n

!  

where the various mathematical terms were deﬁned above.
It can be immediately seen that the
estimator is available in closed-form. In a later section  we will see instantiations of this class of
estimators for various speciﬁc GLM models  and where we will see that these estimators take very
simple forms. Before doing so  we ﬁrst describe some insights that led to our particular construction
of the high-dimensional GLM estimator above.

3.1

Insights Behind Construction of Our Closed-Form Estimator

We

ﬁrst

classical

the
revisit
n ✓>X>y + 1

arg min✓ 1

2
unique minimum in general  especially under high-dimensional sample settings where p > n.
Nonetheless  it is instructive to study why this unregularized MLE is either ill-suited or even
ill-deﬁned under high-dimensional settings. The stationary condition of unregularized MLE
optimization problem can be written as:

n 1>A(X✓ ) . Note that this optimization problem does not have a

unregularized MLE

GLMs:

for

b✓

(4)

(5)

clarify below.

There are two main caveats to solving for a uniqueb✓ satisfying this stationary condition  which we

X>y = X>rA(Xb✓).

4

(Mapping to mean parameters)

here  since the sample covariance matrix (X>X)/n would then be rank-deﬁcient  and hence not

In a high dimensional sampling regime where p  n  (5) can
be seen to reduce to y = rA(Xb✓) (so long as X T has rank n). This then suggests solving for
Xb✓ = [rA]1(y)  where we recall the deﬁnition of the operator rA(·) in terms of element-wise
operations involving A0(·). The caveat however is that A0(·) is only onto the interior Mo of the
response moment polytope [16]  so that [A0(·)]1 is well-deﬁned only when given µ 2M o. When
entries of the sample response vector y however lie outside of Mo  as will typically be the case and
which we will illustrate for multiple instances of GLM models in later sections  the inverse mapping
would not be well-deﬁned. We thus ﬁrst project the sample response vector y onto M✓M o
to obtain ⇧ ¯M(y) as deﬁned in (3). Armed with this approximation  we then consider the more
amenable ⇧ ¯M(y) ⇡ rA(Xb✓)  instead of the original stationary condition in (5).
(Sample covariance) We thus now have the approximate characterization of the MLE as Xb✓ ⇡
[rA]1⇧ ¯M(y). This then suggests solving for an approximate MLE b✓ via least squares as
b✓ = [X>X]1X>[rA]1⇧ ¯M(y). The high-dimensional regime with p > n poses a caveat
invertible. Our approach is to then use a thresholded sample covariance matrix T⌫ X>X
n  deﬁned
n  is consistent with respect to the spectral norm with
that thresholded sample covariance T⌫ X>X
n ⌘  under some mild conditions detailed in our
convergence rateT⌫ X>X

in the previous subsection instead  which can be shown to be invertible and consistent to the popu-
lation covariance matrix ⌃ with high probability [15  17]. In particular  recent work [15] has shown

main theorem. Plugging in this thresholded sample covariance matrix  to get an approximate least
squares solution for the GLM parameters ✓  and then performing soft-thresholding precisely yields
our closed-form estimator in (4).
Our class of closed-form estimators in (4) can thus be viewed as surgical approximations to the MLE
so that it is well-deﬁned in high-dimensional settings  as well as being available in closed-form. But
would such an approximation actually yield rigorous consistency guarantees? Surprisingly  as we
show in the next section  not only is our class of estimators consistent  but in our corollaries we
show that the statistical guarantees are comparable to those of the state of the art iterative ways like
regularized MLEs.
We note that our class of closed-form estimators in (4) can also be written in an equivalent form that
is more amenable to analysis:
minimize

n  ⌃op  O⇣c0q log p

(6)

k✓k1

✓

s. t ✓ hT⌫⇣ X>X

n ⌘i1 X>[rA]1⇧ ¯M(y)

n

 n.

1

The equivalence between (4) and (6) easily follows from the fact that the optimization problem (6)
is decomposable into independent element-wise sub-problems  and each sub-problem corresponds
to soft-thresholding. It can be seen that this form is also amenable to extending the framework in
this paper to structures beyond sparsity  by substituting in alternative regularizers. Due to space
constraints  the computational complexity is discussed in detail in the Appendix.

3.2 Statistical Guarantees

In this subsection  we provide an uniﬁed statistical analysis for the class of estimators (4) under the
following standard conditions  namely sparse ✓⇤ and sub-Gaussian design X:
(C1) The parameter ✓⇤ in (1) is exactly sparse with k non-zero elements indexed by the support
set S  so that ✓⇤Sc = 0.
(C2) Each row of the design matrix X 2 Rn⇥p is i.i.d. sampled from a zero-mean distribution
with covariance matrix ⌃ such that for any v 2 Rp  the variable hv  Xii is sub-Gaussian with
parameter at most ukvk2 for every row of X  Xi.
Our next assumption is on the covariance matrix of the covariate random vector:
(C3) The covariance matrix ⌃ of X satisﬁes that for all w 2 Rp  k⌃wk1  ` kwk1 with
ﬁxed constant ` > 0. Moreover  ⌃ is approximately sparse  along the lines of [17]: for some

5

We also introduce some notations used in the following theorem. Under the condition (C2)  we

j=1 |⌃ij|q  c0. If q = 0  then this condition will be equivalent with ⌃ being sparse.

positive constant D  ⌃ii  D for all diagonal entries  and moreover  for some 0  q < 1 and c0 
maxiPp
have that with high probability  |h✓⇤  x(i)i|  2uk✓⇤k2plog n for all samples  i = 1  . . .   n. Let
⌧⇤ := 2uk✓⇤k2plog n. We then let M0 be the subset of M such that
M0 :=nµ : µ = A0(↵)   where ↵ 2 [⌧⇤ ⌧ ⇤]o .
a2M0[ ¯M|(A1)0(a)| ` A.
↵2[⌧⇤ ⌧⇤]|A00(↵)| u A   max

We also deﬁne u A and ` A on the upper bounds of A00(·) and (A1)0(·)  respectively:

max

(7)

(8)

2

n .

1q log p0 where c1 is a constant related only on ⌧ and maxi ⌃ii 

Armed with these conditions and notations  we derive our main theorem:
Theorem 1. Consider any generalized linear model in (1) where all the conditions (C1)  (C2) and
(C3) hold. Now  suppose that we solve the estimation problem (4) setting the thresholding parameter
⌫ = C1q log p0
n where C1 := 16(maxj ⌃jj)p10⌧ for any constant ⌧> 2  and p0 := max{n  p}.
Furthermore  suppose also that we set the constraint bound n as C2q log p0
n + E where C2 :=
`⇣u` Ap2u A + C1k✓⇤k1⌘ and where E depends on the approximation error induced by the
` q log p0
projection (3)  and is deﬁned as: E := maxi=1 ... n⇣y(i) ⇥⇧ ¯M(y)⇤i⌘ 4u` A
(A) Then  as long as n > 2c1c0
`  2
any optimal solutionb✓ of (4) is guaranteed to be consistent:
n + E◆.
b✓  ✓⇤1  2✓C2q log p0
n + E◆ b✓  ✓⇤2  4pk✓C2q log p0
n + E◆ b✓  ✓⇤1  8k✓C2q log p0
(B) Moreover  the support set of the estimateb✓ correctly excludes all true zero values of ✓⇤. More-
over  when mins2S |✓⇤s| 3n  it correctly includes all non-zero true supports of ✓⇤ 
with probability at least 1  cp0c0 for some universal constants c  c0 > 0 depending on ⌧ and u.
Remark 1. While our class of closed-form estimators and analyses consider sparse-structured pa-
rameters  these can be seamlessly extended to more general structures (such as group sparsity and
low rank)  using appropriate thresholding functions.
Remark 2. The condition (C3) required in Theorem 1 is different from (and possibly stronger)
than the restricted strong convexity [8] required for `2 error bound of `1 regularized MLE. A key
facet of our analysis with our Condition (C3) however is that it provides much simpler and clearer
identifying constants in our non-asymptotic error bounds. Deriving constant factors in the analysis
of the `1-regularized MLE on the other hand  with its restricted strong convexity condition  involves
many probabilistic statements  and is non-trivial  as shown in [8].
Another key facet of our analysis in Theorem 1 is that it also provides an `1 error bound  and
guarantees the sparsistency of our closed-form estimator. For `1 regularized MLEs  this requires a
separate sparsistency analysis. In the case of the simplest standard linear regression models  [18]
showed that the incoherence condition of |||⌃ScS⌃1
SS|||1 < 1 is required for sparsistency  where
||| · |||1 is the maximum of absolute row sum. As discussed in [18]  instances of such incoherent
covariance matrices ⌃ include the identity  and Toeplitz matrices: these matrices can be seen to
also satisfy our condition (C3). On the other hand  not all matrices that satisfy our condition (C3)
need satisfy the stringent incoherence condition in turn. For example  consider ⌃ where ⌃SS =
0.95I3 + 0.0513⇥3 for a matrix 1 of ones  ⌃SSc is all zeros but the last column is 0.413⇥1  and
⌃ScSc = I(p3)⇥(p3). Then  this positive deﬁnite ⌃ can be seen to satisfy our Condition (C3) 
since each row has only 4 non-zeros. However  |||⌃ScS⌃1
SS|||1 is equal to 1.0909 and larger than 1 
and consequently  the incoherence condition required for the Lasso will not be satisﬁed. We defer
relaxing our condition (C3) further as well as a deeper investigation of all the above conditions to
future work.

6

Remark 3. The constant C2 in the statement depends on k✓⇤k1  which in the worst case where
only k✓⇤k2 is bounded  may scale with pk. On the other hand  our theorem does not require an
explicit sample complexity condition that n be larger than some function on k  while the analysis
of `1-regularized MLEs do additionally require that n  c k log p for some constant c.
In our
experiments  we verify that our closed-form estimators outperform the `1-regularized MLEs even
when k is fairly large (for instant  when (n  p  k) = (5000  104  1000)).

In order to apply Theorem 1 to a speciﬁc instance of GLMs  we need to specify the quantities in (8) 
as well as carefully construct a subset M of the interior of the response moment polytope. In case
of the simplest linear models described in Section 2  we have the identity mapping µ = A0(⌘) = ⌘.
The inequalities in (8) can thus be seen to be satisﬁed with ` A = u A = 1 . Moreover  we can set
M := Mo = R so that ⇧ ¯M(y) = y  and trivially recover the previous results in [14] as a special
case. In the following sections  we will derive the consequences of our framework for the complex
instances of logistic and Poisson regression models  which are also important members in GLMs.

4 Key Corollaries

In order to derive corollaries of our main Theorem 1  we need to specify the response polytope
subsets M M0 in (2) and (7) respectively  as well as bound the two quantities ` A and u A in (8).
Logistic regression models. The exponential family log-partition function of logistic regression
models described in Section 2 can be seen to be A(⌘) = log⇥ exp(⌘) + exp(⌘)⇤. Consequently 
its double derivative A00(⌘) = 4 exp(2⌘)
(exp(2⌘)+1)2  1 for any ⌘  so that (8) holds with u A = 1.
The response moment polytope for the binary response variable y 2 Y ⌘ {1  1} is the inter-
val M = [1  1]  so that its interior is given by Mo = (1  1). For the subset of the interior 
we deﬁne M = [1 + ✏  1  ✏]  for some 0 <✏< 1. At the same time  the forward mapping
is given by A0(⌘) = exp(2⌘)  1)/(exp(2⌘) + 1)  and hence M0 becomes [ a1
a+1 ] where
1µ  and
a := n
given M and M0  it can be seen that (A0)1(µ) is Lipschitz for M[M 0 with constant less than
  1/✏o in (8). Note that with this setting of the subset M  we have
` A := maxn 1
that maxi=1 ... n(y(i) ⇥⇧ ¯M(y)⇤i) = ✏  and moreover  ⇧ ¯M(yi) = yi(1  ✏)  which we will use in

4uk✓⇤k2
plog n . The inverse mapping of logistic models is given by (A0)1(µ) = 1

2 log 1+µ

the corollary below.

a+1   a1

2 + 1
2 n

4uk✓⇤k2
plog n

Poisson regression models. Another important instance of GLMs is the Poisson regression model 
that is becoming increasingly more relevant in modern big-data settings with varied multivariate
count data. For the Poisson regression model case  the double derivative of A(·) is not uniformly
upper bounded: A00(u) = exp(u). Denoting ⌧⇤ := 2uk✓⇤k2plog n  we then have that for any
↵ in [⌧⇤ ⌧ ⇤]  A00(↵)  exp2uk✓⇤k2plog n = n2uk✓⇤k2/plog n  so that (8) is satisﬁed with
u A = n2uk✓⇤k2/plog n. The response moment polytope for the count-valued response variable
y 2Y⌘{ 0  1  . . .} is given by M = [0 1)  so that its interior is given by Mo = (0 1). For the
subset of the interior  we deﬁne M = [✏ 1) for some ✏ s.t. 0 <✏< 1. The forward mapping in this
2uk✓⇤k2
plog n . The
case is simply given by A0(⌘) = exp(⌘)  and M0 in (7) becomes [a1  a] where a is n
inverse mapping for the Poisson regression model then is given by (A0)1(µ) = log(µ)  which can
2uk✓⇤k2
plog n   1/✏} in (8). With this setting
be seen to be Lipschitz for M with constant ` A = max{n
of M  it can be seen that the projection operator is given by ⇧ ¯M(yi) = I(yi = 0)✏ + I(yi 6= 0)yi.
Now  we are ready to recover the error bounds  as a corollary of Theorem 1  for logistic regression
and Poisson models when condition (C2) holds:
Corollary 1. Consider any logistic regression model or a Poisson regression model where all con-
ditions in Theorem 1 hold. Suppose that we solve our closed-form estimation problem (4)  setting
n(1/2c0/plog n) +

n   and the constraint bound n = 2

cplog p0

n  where c and c0 are some constants depending only on u  k✓⇤k2 and ✏. Then the

the thresholding parameter ⌫ = C1q log p0
C1k✓⇤k1q log p0

`

7

Table 1: Comparisons on simulated datasets when parameters are tuned to minimize `2 error on
independent validation sets.

(n  p  k)

METHOD

TP

FP

`2 ERROR

TIME

(n  p  k)

METHOD

TP

FP

`2 ERROR

TIME

(n = 2000  `1 MLE1
`1 MLE2
p = 5000 
`1 MLE3
k = 10)
ELEM
(n = 4000  `1 MLE1
`1 MLE2
p = 5000 
`1 MLE3
k = 10)
ELEM
(n = 5000  `1 MLE1
`1 MLE2
p = 104 
`1 MLE3
k = 100)
ELEM

1
1
1

0.1094
0.0873
0.1000
0.9900 0.0184
0.1626
0.1327
0.1112
0.0069
0.1301
0.1695
0.2001
0.9975 0.3622

1
1
1
1
1
1
1

4.5450
4.0721
3.4846
2.7375
4.2132
3.6569
2.9681
2.6213
18.9079
18.5567
18.2351
16.4148

63.9
133.1
348.3
26.5
155.5
296.8
829.3
40.2
500.1
983.8
2353.3
151.8

(n = 8000 
p = 104 
k = 100)

`1 MLE1
(n = 5000 
p = 104 
`1 MLE2
k = 1000) `1 MLE3
ELEM
`1 MLE1
`1 MLE2
`1 MLE3
ELEM
`1 MLE1
(n = 8000 
p = 104 
`1 MLE2
k = 1000) `1 MLE3
ELEM

0.7990
0.7935
0.7965
0.8295

1
1
1
1

1
1
1

0.1904
0.2181
0.2364
0.9450 0.0359
0.7965
0.7900
0.7865
0.7015 0.5103

1
1
1

65.1895
65.1165
65.1024
63.2359
18.6186
18.1806
17.6762
11.9881
65.0714
64.9650
64.8857
61.0532

520.7
1005.8
2560.1
152.1
810.6
1586.2
3568.9
221.1
809.5
1652.8
4196.6
219.4

4

cplog p0

cplog p0

n(1/2c0/plog n)

optimal solutionb✓ of (4) is guaranteed to be consistent:
+ C1k✓⇤k1r log p0
`✓
n ◆  
cplog p0
b✓  ✓⇤1 
+ C1k✓⇤k1r log p0
+ C1k✓⇤k1r log p0
✓
n ◆  
n ◆  
b✓  ✓⇤1 
n(1/2c0/plog n)
with probability at least 1  c1p0c01 for some universal constants c1  c01 > 0 and p0 := max{n  p}.
n(1/2c0/plog n) + C1k✓⇤k1q log p0
`
Moreover  when mins2S |✓⇤s| 6
Remarkably  the rates in Corollary 1 are asymptotically comparable to those for the `1-regularized
MLE (see for instance Theorem 4.2 and Corollary 4.4 in [7]). In Appendix A  we place slightly
more stringent condition than (C2) and guarantee error bounds with faster convergence rates.

n  b✓ is sparsistent.

b✓  ✓⇤2 
` ✓

n(1/2c0/plog n)

16k

cplog p0

8pk
`

5 Experiments

We corroborate the performance of our elementary estimators on simulated data over varied regimes
of sample size n  number of covariates p  and sparsity size k. We consider two popular instances
of GLMs  logistic and Poisson regression models. We compare against standard `1 regularized
MLE estimators with iteration bounds of 50  100  and 500  denoted by `1 MLE1  `1 MLE2 and `1
MLE3 respectively. We construct the n ⇥ p design matrices X by sampling the rows independently
from N (0  ⌃) where ⌃i j = 0.5|ij|. For each simulation  the entries of the true model coefﬁcient
vector ✓⇤ are set to be 0 everywhere  except for a randomly chosen subset of k coefﬁcients  which
are chosen independently and uniformly in the interval (1  3). We report results averaged over 100
independent trials. Noting that our theoretical results were not sensitive to the setting of ✏ in ⇧ ¯M(y) 
we simply report the results when ✏ = 104 across all experiments.
While our theorem speciﬁed an optimal setting of the regularization parameter n and ⌫  this optimal
setting depended on unknown model parameters. Thus  as is standard with high-dimensional regu-

larized estimators  we set tuning parameters n = cplog p/n and ⌫ = c0plog p/n by a holdout-

validated fashion; ﬁnding a parameter that minimizes the `2 error on an independent validation set.
Detailed experimental setup is described in the appendix.
Table 1 summarizes the performances of `1 MLE using 3 different stopping criteria and Elem-GLM.
Besides `2 errors  the target tuning metric  we also provide the true and false positives for the support
set recovery task on the new test set where the best tuning parameters are used. The computation
times in second indicate the overall training computation time summing over the whole parameter
tuning process. As we can see from our experiments  with respect to both statistical and compu-
tational performance our closed form estimators are quite competitive compared to the classical `1
regularized MLE estimators and in certain case outperform them. Note that `1 MLE1 stops pre-
maturely after only 50 iterations  so that training computation time is sometimes comparable to
closed-form estimator. However  its statistical performance measured by `2 is much inferior to other
`1 MLEs with more iterations as well as Elem-GLM estimator. Due to the space limit  ROC curves 
results for other settings of p and more experiments on real datasets are presented in the appendix.

8

References
[1] P. McCullagh and J.A. Nelder. Generalized linear models. Monographs on statistics and applied proba-

bility 37. Chapman and Hall/CRC  1989.

[2] G. E. Hoffman  B. A. Logsdon  and J. G. Mezey. Puma: A uniﬁed framework for penalized multiple

regression analysis of gwas data. Plos computational Biology  2013.

[3] D. Witten and R. Tibshirani. Survival analysis with high-dimensional covariates. Stat Methods Med Res. 

19:29–51  2010.

[4] E. Yang  P. Ravikumar  G. I. Allen  and Z. Liu. Graphical models via generalized linear models. In Neur.

Info. Proc. Sys. (NIPS)  25  2012.

[5] S. Van de Geer. High-dimensional generalized linear models and the lasso. Annals of Statistics  36(2):

614–645  2008.

[6] F. Bach. Self-concordant analysis for logistic regression. Electron. J. Stat.  4:384–414  2010.
[7] S. M. Kakade  O. Shamir  K. Sridharan  and A. Tewari. Learning exponential families in high-dimensions:

Strong convexity and sparsity. In Inter. Conf. on AI and Statistics (AISTATS)  2010.

[8] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional

analysis of M-estimators with decomposable regularizers. Arxiv preprint arXiv:1010.2731v1  2010.

[9] F. Bunea. Honest variable selection in linear and logistic regression models via l1 and l1 + l2 penalization.

Electron. J. Stat.  2:1153–1194  2008.

[10] L. Meier  S. Van de Geer  and P. B¨uhlmann. The group lasso for logistic regression. Journal of the Royal

Statistical Society  Series B  70:53–71  2008.

[11] Y. Kim  J. Kim  and Y. Kim. Blockwise sparse regression. Statistica Sinica  16:375–390  2006.
[12] J. Friedman  T. Hastie  and R. Tibshirani. Regularization paths for generalized linear models via coordi-

nate descent. Journal of Statistical Software  33(1):1–22  2010.

[13] K. Koh  S. J. Kim  and S. Boyd. An interior-point method for large-scale `1-regularized logistic regres-

sion. Jour. Mach. Learning Res.  3:1519–1555  2007.

[14] E. Yang  A. C. Lozano  and P. Ravikumar. Elementary estimators for high-dimensional linear regression.

In International Conference on Machine learning (ICML)  31  2014.

[15] A. J. Rothman  E. Levina  and J. Zhu. Generalized thresholding of large covariance matrices. Journal of

the American Statistical Association (Theory and Methods)  104:177–186  2009.

[16] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families and variational inference.

Foundations and Trends in Machine Learning  1(1–2):1—305  December 2008.

[17] P. J. Bickel and E. Levina. Covariance regularization by thresholding. Annals of Statistics  36(6):2577–

2604  2008.

[18] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained

quadratic programming (Lasso). IEEE Trans. Information Theory  55:2183–2202  May 2009.

[19] Daniel A. Spielman and Shang-Hua Teng. Solving sparse  symmetric  diagonally-dominant linear systems
in time 0(m1.31). In 44th Symposium on Foundations of Computer Science (FOCS 2003)  11-14 October
2003  Cambridge  MA  USA  Proceedings  pages 416–427  2003.

[20] Michael B. Cohen  Rasmus Kyng  Gary L. Miller  Jakub W. Pachocki  Richard Peng  Anup B. Rao  and
Shen Chen Xu. Solving sdd linear systems in nearly mlog1/2n time. In Proceedings of the 46th Annual
ACM Symposium on Theory of Computing  STOC ’14  pages 343–352. ACM  2014.

[21] Daniel A. Spielman and Shang-Hua Teng. Nearly linear time algorithms for preconditioning and solving
symmetric  diagonally dominant linear systems. SIAM J. Matrix Analysis Applications  35(3):835–885 
2014.

[22] P. Ravikumar  M. J. Wainwright  G. Raskutti  and B. Yu. High-dimensional covariance estimation by
minimizing `1-penalized log-determinant divergence. Electronic Journal of Statistics  5:935–980  2011.
[23] E. Yang  A. C. Lozano  and P. Ravikumar. Elementary estimators for sparse covariance matrices and other

structured moments. In International Conference on Machine learning (ICML)  31  2014.

[24] E. Yang  A. C. Lozano  and P. Ravikumar. Elementary estimators for graphical models. In Neur. Info.

Proc. Sys. (NIPS)  27  2014.

9

,Eunho Yang
Aurelie Lozano
Pradeep Ravikumar