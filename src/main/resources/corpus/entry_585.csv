2017,Approximation Algorithms for $\ell_0$-Low Rank Approximation,We study the $\ell_0$-Low Rank Approximation Problem  where the goal is     given an $m \times n$ matrix $A$  to output a rank-$k$ matrix $A'$ for which   $\|A'-A\|_0$ is minimized.    Here  for a matrix $B$  $\|B\|_0$ denotes the number of its non-zero entries.    This NP-hard variant of low rank approximation is natural for problems    with no underlying metric  and its goal is to minimize the number of disagreeing   data positions.      We provide approximation algorithms which significantly improve the running time    and approximation factor of previous work.    For $k > 1$  we show how to find  in poly$(mn)$ time for every $k$     a rank $O(k \log(n/k))$ matrix $A'$ for which $\|A'-A\|_0 \leq O(k^2 \log(n/k)) \OPT$.    To the best of our knowledge  this is the first algorithm with provable guarantees    for the $\ell_0$-Low Rank Approximation Problem for $k > 1$     even for bicriteria algorithms.       For the well-studied case when $k = 1$  we give a $(2+\epsilon)$-approximation    in {\it sublinear time}  which is impossible for other variants of low rank    approximation such as for the  Frobenius norm.    We strengthen this for the well-studied case of binary matrices to obtain    a $(1+O(\psi))$-approximation in sublinear time     where $\psi = \OPT/\nnz{A}$.   For small $\psi$  our approximation factor is $1+o(1)$.,Approximation Algorithms for
(cid:96)0-Low Rank Approximation

Karl Bringmann1

Pavel Kolev1∗

David P. Woodruff2

kbringma@mpi-inf.mpg.de

pkolev@mpi-inf.mpg.de

dwoodruf@cs.cmu.edu

1 Max Planck Institute for Informatics  Saarland Informatics Campus  Saarbrücken  Germany

2 Department of Computer Science  Carnegie Mellon University

Abstract

We study the (cid:96)0-Low Rank Approximation Problem  where the goal is  given an
m × n matrix A  to output a rank-k matrix A(cid:48) for which (cid:107)A(cid:48) − A(cid:107)0 is minimized.
Here  for a matrix B  (cid:107)B(cid:107)0 denotes the number of its non-zero entries. This NP-
hard variant of low rank approximation is natural for problems with no underlying
metric  and its goal is to minimize the number of disagreeing data positions.
We provide approximation algorithms which signiﬁcantly improve the running
time and approximation factor of previous work. For k > 1  we show how to
ﬁnd  in poly(mn) time for every k  a rank O(k log(n/k)) matrix A(cid:48) for which
(cid:107)A(cid:48) − A(cid:107)0 ≤ O(k2 log(n/k)) OPT. To the best of our knowledge  this is the ﬁrst
algorithm with provable guarantees for the (cid:96)0-Low Rank Approximation Problem
for k > 1  even for bicriteria algorithms.
For the well-studied case when k = 1  we give a (2+)-approximation in sublinear
time  which is impossible for other variants of low rank approximation such as for
the Frobenius norm. We strengthen this for the well-studied case of binary matrices
to obtain a (1 + O(ψ))-approximation in sublinear time  where ψ = OPT /(cid:107)A(cid:107)0.
For small ψ  our approximation factor is 1 + o(1).

Introduction

1
Low rank approximation of an m × n matrix A is an extremely well-studied problem  where the
goal is to replace the matrix A with a rank-k matrix A(cid:48) which well-approximates A  in the sense that
(cid:107)A − A(cid:48)(cid:107) is small under some measure (cid:107) · (cid:107). Since any rank-k matrix A(cid:48) can be written as U · V  
where U is m × k and V is k × n  this allows for a signiﬁcant parameter reduction. Namely  instead
of storing A  which has mn entries  one can store U and V   which have only (m + n)k entries in total.
Moreover  when computing Ax  one can ﬁrst compute V x and then U (V x)  which takes (m + n)k
instead of mn time. We refer the reader to several surveys [19  24  40] for references to the many
results on low rank approximation.
We focus on approximation algorithms for the low-rank approximation problem  i.e. we seek to
output a rank-k matrix A(cid:48) for which (cid:107)A− A(cid:48)(cid:107) ≤ α(cid:107)A− Ak(cid:107)  where Ak = argminrank(B)=k(cid:107)A− B(cid:107)
is the best rank-k approximation to A  and the approximation ratio α is as small as possible. One of
i j)1/2  for
which the optimal rank-k approximation can be obtained via the singular value decomposition (SVD).
Using randomization and approximation  one can compute an α = 1 + -approximation  for any
 > 0  in time much faster than the min(mn2  mn2) time required for computing the SVD  namely 
in O((cid:107)A(cid:107)0 + n · poly(k/)) time [9  26  29]  where (cid:107)A(cid:107)0 denotes the number of non-zero entries
∗This work has been funded by the Cluster of Excellence “Multimodal Computing and Interaction” within

the most widely studied error measures is the Frobenius norm (cid:107)A(cid:107)F = ((cid:80)m

the Excellence Initiative of the German Federal Government.

(cid:80)n

i=1

j=1 A2

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

of A. For the Frobenius norm (cid:107)A(cid:107)0 time is also a lower bound  as any algorithm that does not read
nearly all entries of A might not read a very large entry  and therefore cannot achieve a relative error
approximation.
The rank-k matrix Ak obtained by computing the SVD is also optimal with respect to any rotationally
invariant norm  such as the operator and Schatten-p norms. Thus  such norms can also be solved
exactly in polynomial time. Recently  however  there has been considerable interest [10  3  32]
in obtaining low rank approximations for NP-hard error measures such as the entrywise (cid:96)p-norm

i j |Ai j|p(cid:1)1/p  where p ≥ 1 is a real number. Note that for p < 1  this is not a norm 

though it is still a well-deﬁned quantity. For p = ∞  this corresponds to the max-norm or Chebyshev
norm. It is known that one can achieve a poly(k log(mn))-approximation in poly(mn) time for the
low-rank approximation problem with entrywise (cid:96)p-norm for every p ≥ 1 [36  8].

(cid:107)A(cid:107)p =(cid:0)(cid:80)

1.1

(cid:96)0-Low Rank Approximation

i j) = 1 if Ai j (cid:54)= A(cid:48)

i j)  where δ(Ai j (cid:54)= A(cid:48)

(cid:80)
i j δ(Ai j (cid:54)= A(cid:48)

A natural variant of low rank approximation which the results above do not cover is that of (cid:96)0-low
rank approximation  where the measure (cid:107)A(cid:107)0 is the number of non-zero entries. In other words  we
i j (cid:54)= Ai j is as small as possible.
seek a rank-k matrix A(cid:48) for which the number of entries (i  j) with A(cid:48)
Letting OPT = minrank(B)=k
i j and 0
otherwise  we would like to output a rank-k matrix A(cid:48) for which there are at most α OPT entries
i j (cid:54)= Ai j. Approximation algorithms for this problem are essential since solving the
(i  j) with A(cid:48)
problem exactly is NP-hard [12  14]  even when k = 1 and A is a binary matrix.
The (cid:96)0-low rank approximation problem is quite natural for problems with no underlying metric  and
its goal is to minimize the number of disagreeing data positions with a low rank matrix. Indeed  this
error measure directly answers the following question: if we are allowed to ignore some data - outliers
or anomalies - what is the best low-rank model we can get? One well-studied case is when A is binary 
but A(cid:48) and its factors U and V need not necessarily be binary. This is called unconstrained Binary
Matrix Factorization in [18]  which has applications to association rule mining [20]  biclustering
structure identiﬁcation [42  43]  pattern discovery for gene expression [34]  digits reconstruction [25] 
mining high-dimensional discrete-attribute data [21  22]  market based clustering [23]  and document
clustering [43]. There is also a body of work on Boolean Matrix Factorization which restricts the
factors to also be binary  which is referred to as constrained Binary Matrix Factorization in [18]. This
is motivated in applications such as classifying text documents and there is a large body of work on
this  see  e.g. [28  31].
The (cid:96)0-low rank approximation problem coincides with a number of problems in different areas. It
exactly coincides with the famous matrix rigidity problem over the reals  which asks for the minimal
number OPT of entries of A that need to be changed in order to obtain a matrix of rank at most k.
The matrix rigidity problem is well-studied in complexity theory [15  16  39] and parameterized
complexity [13]. These works are not directly relevant here as they do not provide approximation
algorithms. There are also other variants of (cid:96)0-low rank approximation  corresponding to cases such
as when A is binary  A(cid:48) = U V is required to have binary factors U and V   and multiplication is
either performed over a binary ﬁeld [41  17  12  30]  or corresponds to an OR of ANDs. The latter is
known as the Boolean model [4  12  27  33  35  38]. These different notions of inner products lead to
very different algorithms and results for the (cid:96)0-low rank approximation problem. However  all these
models coincide in the special and important case in which A is binary and k = 1. This case was
studied in [20  34  18]  as their algorithm for k = 1 forms the basis for their successful heuristic for
general k  e.g. the PROXIMUS technique [20].
Another related problem is robust PCA [6]  in which there is an underlying matrix A that can
be written as a low rank matrix L plus a sparse matrix S [7]. Candès et al. [7] argue that both
components are of arbitrary magnitude  and we do not know the locations of the non-zeros in S nor
how many there are. Moreover  grossly corrupted observations are common in image processing 
web data analysis  and bioinformatics where some measurements are arbitrarily corrupted due to
occlusions  malicious tampering  or sensor failures. Speciﬁc scenarios include video surveillance 
face recognition  latent semantic indexing  and ranking of movies  books  etc. [7]. These problems
have the common theme of being an arbitrary magnitude sparse perturbation to a low rank matrix
with no natural underlying metric  and so the (cid:96)0-error measure (which is just the Hamming distance 
or number of disagreements) is appropriate. In order to solve robust PCA in practice  Candès et al. [7]

2

relaxed the (cid:96)0-error measure to the (cid:96)1-norm. Understanding theoretical guarantees for solving the
original (cid:96)0-problem is of fundamental importance  and we study this problem in this paper.
Finally  interpreting 00 as 0  the (cid:96)0-low rank approximation problem coincides with the aforemen-
tioned notion of entrywise (cid:96)p-approximation when p = 0. It is not hard to see that previous work [8]
for general p ≥ 1 fails to give any approximation factor for p = 0. Indeed  critical to their analysis is
the scale-invariance property of a norm  which does not hold for p = 0 since (cid:96)0 is not a norm.

1.2 Our Results

into(cid:80)

We provide approximation algorithms for the (cid:96)0-low rank approximation problem which signiﬁcantly
improve the running time or approximation factor of previous work. In some cases our algorithms
even run in sublinear time  i.e.  faster than reading all non-zero entries of the matrix. This is provably
impossible for other measures such as the Frobenius norm and more generally  any (cid:96)p-norm for p > 0.
For k > 1  our approximation algorithms are  to the best of our knowledge  the ﬁrst with provable
guarantees for this problem.
First  for k = 1  we signiﬁcantly improve the polynomial running time of previous (2 + )-
approximations for this problem. The best previous algorithm due to Jiang et al. [18] was based on
the observation that there exists a column u of A spanning a 2-approximation. Therefore  solving the
problem minv (cid:107)A− uv(cid:107)0 for each column u of A yields a 2-approximation  where for a matrix B the
measure (cid:107)B(cid:107)0 counts the number of non-zero entries. The problem minv (cid:107)A − uv(cid:107)0 decomposes
i mini (cid:107)A: i − viu(cid:107)0  where A: i is the i-th column of A  and vi the i-th entry of vector v.
The optimal vi is the mode of the ratios Ai j/uj  where j ranges over indices in {1  2  . . .   m} with
uj (cid:54)= 0. As a result  one can ﬁnd a rank-1 matrix uvT providing a 2-approximation in O((cid:107)A(cid:107)0n)
time  which was the best known running time. Somewhat surprisingly  we show that one can
achieve sublinear time for solving this problem. Namely  we obtain a (2 + )-approximation in
(m + n) poly(−1ψ−1 log(mn)) time  for any  > 0  where ψ = OPT /(cid:107)A(cid:107)0. This signiﬁcantly
improves upon the earlier O((cid:107)A(cid:107)0n) time for not too small  and ψ. Our result should be contrasted
to Frobenius norm low rank approximation  for which Ω((cid:107)A(cid:107)0) time is required even for k = 1  as
otherwise one might miss a very large entry in A. Since (cid:96)0-low rank approximation is insensitive to
the magnitude of entries of A  we bypass this general impossibility result.
Next  still considering the case of k = 1  we show that if the matrix A is binary  a well-studied
case coinciding with the abovementioned GF (2) and Boolean models  we obtain an approximation
algorithm parameterized in terms of the ratio ψ = OPT /(cid:107)A(cid:107)0  showing it is possible in time
(m + n)ψ−1 poly(log(mn)) to obtain a (1 + O(ψ))-approximation. Note that our algorithm is again
sublinear  unlike all algorithms in previous work. Moreover  when A is itself very well approximated
by a low rank matrix  then ψ may actually be sub-constant  and we obtain a signiﬁcantly better
(1 + o(1))-approximation than the previous best known 2-approximations. Thus  we simultaneously
improve the running time and approximation factor. We also show that the running time of our
algorithm is optimal up to poly(log(mn)) factors by proving that any (1 + O(ψ))-approximation
succeeding with constant probability must read Ω((m + n)ψ−1) entries of A in the worst case.
Finally  for arbitrary k > 1  we ﬁrst give an impractical algorithm that runs in time nO(k) and achieves
an α = poly(k)-approximation. To the best of our knowledge this is the ﬁrst approximation algorithm
for the (cid:96)0-low rank approximation problem with any non-trivial approximation factor. To make our
algorithm practical  we reduce the running time to poly(mn)  with an exponent independent of k  if
we allow for a bicriteria solution. In particular  we allow the algorithm to output a matrix A(cid:48) of some-
what larger rank O(k log(n/k))  for which (cid:107)A − A(cid:48)(cid:107)0 ≤ O(k2 log(n/k)) minrank(B)=k (cid:107)A − B(cid:107)0.
Although we do not obtain rank exactly k  many of the motivations for ﬁnding a low rank approxima-
tion  such as reducing the number of parameters and fast matrix-vector product  still hold if the output
rank is O(k log(n/k)). We are not aware of any alternative algorithms which achieve poly(mn) time
and any provable approximation factor  even for bicriteria solutions.

2 Preliminaries

For an matrix A ∈ Am×n with entries Ai j  we write Ai : for its i-th row and A: j for its j-th column.

3

Input Formats We always assume that we have random access to the entries of the given matrix A 
i.e. we can read any entry Ai j in constant time. For our sublinear time algorithms we need more
efﬁcient access to the matrix  speciﬁcally the following two variants:
(1) We say that we are given A with column adjacency arrays if we are given arrays B1  . . .   Bn and
lengths (cid:96)1  . . .   (cid:96)n such that for any 1 ≤ k ≤ (cid:96)j the pair Bj[k] = (i  Ai j) stores the row i containing
the k-th nonzero entry in column j as well as that entry Ai j. This is a standard representation of
matrices used in many applications. Note that given only these adjacency arrays B1  . . .   Bn  in order
to access any entry Ai j we can perform a binary search over Bj  and hence random access to any
matrix entry is in time O(log n). Moreover  we assume to have random access to matrix entries in
constant time  and note that this is optimistic by at most a factor O(log n).
(2) We say that we are given matrix A with row and column sums if we can access the numbers
i Ai j for j ∈ [n] in constant time (and  as always  access any entry Ai j
in constant time). Notice that storing the row and column sums takes O(n + m) space  and thus
while this might not be standard information it is very cheap to store.
We show that the ﬁrst access type even allows to sample from the set of nonzero entries uniformly in
constant time.
Lemma 1. Given a matrix A ∈ Rm×n with column adjacency arrays  after O(n) time preprocessing
we can sample a uniformly random nonzero entry (i  j) from A in time O(1).

(cid:80)
j Ai j for i ∈ [m] and(cid:80)

The proof of this lemma  as well as most other proofs in this extended abstract  can be found in the
full version of the paper.

3 Algorithms for Real (cid:96)0-rank-k
Given a matrix A ∈ Rm×n  the (cid:96)0-rank-k problem asks to ﬁnd a matrix A(cid:48) with rank k such that the
difference between A and A(cid:48) measured in (cid:96)0 norm is minimized. We denote the optimum value by

OPT(k) def

= min

rank(A(cid:48))=k

(cid:107)A − A(cid:48)(cid:107)0 =

min

U∈Rm×k  V ∈Rk×n

(cid:107)A − U V (cid:107)0 .

(1)

In this section  we establish several new results on the (cid:96)0-rank-k problem. In Subsection 3.1  we prove
a structural lemma that shows the existence of k columns which provide a (k + 1)-approximation
to OPT(k)  and we also give an Ω(k)-approximation lower bound for any algorithm that selects k
columns from the input matrix A. In Subsection 3.2  we give an approximation algorithm that runs in
poly(nk  m) time and achieves an O(k2)-approximation. To the best of our knowledge  this is the
ﬁrst algorithm with provable non-trivial approximation guarantees. In Subsection 3.3  we design a
practical algorithm that runs in poly(n  m) time with an exponent independent of k  if we allow for a
bicriteria solution.

3.1 Structural Results

We give a new structural result for (cid:96)0 showing that any matrix A contains k columns which provide a
(k + 1)-approximation for the (cid:96)0-rank-k problem (1).
Lemma 2. Let A ∈ Rm×n be a matrix and k ∈ [n]. There is a subset J (k) ⊂ [n] of size k and a
matrix Z ∈ Rk×n such that (cid:107)A − A: J (k) Z(cid:107)0 ≤ (k + 1)OPT(k).

= ∅. We split the value OPT(k) into OPT(S(0)  R(0))

Proof. Let Q(0) be the set of columns j with U V: j = 0  and let R(0) def
T (0) def
OPT(S(0)  Q(0))
Suppose OPT(S(0)  R(0)) ≥ |S(0)||R(0)|/(k + 1). Then  for any subset J (k) it follows that
minZ(cid:107)A − AS(0) J (k)Z(cid:107)0 ≤ |S(0)||R(0)| + (cid:107)AS(0) Q(0)(cid:107)0 ≤ (k + 1)OPT(k). Otherwise  there is a

= [n] 
= (cid:107)AS(0) R(0) − U VS(0) R(0)(cid:107)0 and

def

= (cid:107)AS(0) Q(0) − U VS(0) Q(0)(cid:107)0 = (cid:107)AS(0) Q(0)(cid:107)0.

= [n] \ Q(0). Let S(0) def

column i(1) such that(cid:13)(cid:13)AS(0) i(1) − (U V )S(0) i(1)

(cid:13)(cid:13)0 ≤ OPT(S(0)  R(0))/|R(0)| ≤ OPT(k)/|R(0)|.

def

4

|S((cid:96)+1)| ≥ m ·(cid:81)(cid:96)

Let T (1) be the set of indices on which (U V )S(0) i(1) and AS(0) i(1) disagree  and similarly S(1) def
=
S(0)\T (1) on which they agree. Then we have |T (1)| ≤ OPT(k)/|R(0)|. Hence  in the submatrix
T (1) × R(0) the total error is at most |T (1)| · |R(0)| ≤ OPT(k). Let R(1)  D(1) be a partitioning of
R(0) such that AS(1) j is linearly dependent on AS(1) i(1) iff j ∈ D(1). Then by selecting column
A: i(1) the incurred cost on matrix S(1) × D(1) is zero. For the remaining submatrix S((cid:96)) × R((cid:96))  we
perform a recursive call of the algorithm.
We make at most k recursive calls  on instances S((cid:96)) × R((cid:96)) for (cid:96) ∈ {0  . . .   k − 1}.
In the
(cid:96)th iteration  either OPT(S((cid:96))  R((cid:96))) ≥ |S((cid:96))||R((cid:96))|/(k + 1 − (cid:96)) and we are done  or there is a
column i((cid:96)+1) which partitions S((cid:96)) into S((cid:96)+1)  T ((cid:96)+1) and R((cid:96)) into R((cid:96)+1)  D((cid:96)+1) such that
k+1 · m and for every j ∈ D((cid:96)) the column AS((cid:96)+1) j belongs

i=0(1 − 1
k+1−i ) = k−(cid:96)
to the span of {AS((cid:96)+1) i(t)}(cid:96)+1
t=1.
Suppose we performed k recursive calls. We show now that the incurred cost in submatrix S(k)×R(k)
is at most OPT(S(k)  R(k)) ≤ OPT(k). By construction  the sub-columns {AS(k) i}i∈I (k) are lin-
early independent  where I (k) = {i(1)  . . .   i(k)} is the set of the selected columns  and AS(k) I (k) =
(U V )S(k) I (k). Since rank(AS(k) I (k)) = k  it follows that rank(US(k) :) = k  rank(V: I (k) ) = k
and the matrix V: I (k) ∈ Rk×k is invertible. Hence  for matrix Z = (V: I (k) )−1V: Rk we have
OPT(S(k)  R(k)) = (cid:107)ASk Rk − ASk I k Z(cid:107)0.
The statement follows by noting that the recursive calls accumulate a total cost of at most k · OPT(k)
in the submatrices T ((cid:96)+1) × R((cid:96)) for (cid:96) ∈ {0  1  . . .   k − 1}  as well as cost at most OPT(k) in
submatrix S(k) × R(k).

We also show that any algorithm that selects k columns of a matrix A incurs at least an Ω(k)-
approximation for the (cid:96)0-rank-k problem.
Lemma 3. Let k ≤ n/2. Suppose A = (Gk×n; In×n) ∈ R(n+k)×n is a matrix composed of a
Gaussian random matrix G ∈ Rk×n with Gi j ∼ N (0  1) and identity matrix In×n. Then for any
subset J (k) ⊂ [n] of size k  we have minZ∈Rk×n(cid:107)A − A: J (k) Z(cid:107)0 = Ω(k) · OPT(k).

3.2 Basic Algorithm

We give an impractical algorithm that runs in poly(nk  m) time and achieves an O(k2)-approximation.
To the best of our knowledge this is the ﬁrst approximation algorithm for the (cid:96)0-rank-k problem with
non-trivial approximation guarantees.
Theorem 4. Given A ∈ Rm×n and k ∈ [n] we can compute in O(nk+1m2kω+1) time a set of k
indices J (k) ⊂ [n] and a matrix Z ∈ Rk×n such that (cid:107)A − A: J (k) Z(cid:107)0 ≤ O(k2) · OPT(k).
Our result relies on a subroutine by Berman and Karpinski [5] (attributed also to Kannan in that
paper) which given a matrix U and a vector b approximates minx (cid:107)U x − b(cid:107)0 in polynomial time.
Speciﬁcally  we invoke in our algorithm the following variant of this result established by Alon 
Panigrahy  and Yekhanin [2].
Theorem 5. [2] There is an algorithm that given A ∈ Rm×k and b ∈ Rm outputs in O(m2kω+1)
time a vector z ∈ Rk such that w.h.p. (cid:107)Az − b(cid:107)0 ≤ k · minx (cid:107)Ax − b(cid:107)0.
3.3 Bicriteria Algorithm

Our main contribution in this section is to design a practical algorithm that runs in poly(n  m) time
with an exponent independent of k  if we allow for a bicriteria solution.
Theorem 6. Given A ∈ Rm×n and k ∈ [1  n]  there is an algorithm that in expected poly(m  n)
time outputs a subset of indices J ⊂ [n] with |J| = O(k log(n/k)) and a matrix Z ∈ R|J|×n such
that (cid:107)A − A: J Z(cid:107)0 ≤ O(k2 log(n/k)) · OPT(k).
The structure of the proof follows a recent approximation algorithm [8  Algorithm 3] for the (cid:96)p-low
rank approximation problem  for any p ≥ 1. We note that the analysis of [8  Theorem 7] is missing an

5

O(log1/p n) approximation factor  and naïvely provides an O(k log1/p n)-approximation rather than
the stated O(k)-approximation. Further  it might be possible to obtain an efﬁcient algorithm yielding
an O(k2 log k)-approximation for Theorem 6 using unpublished techniques in [37]; we leave the
study of obtaining the optimal approximation factor to future work.
There are two critical differences with the proof of [8  Theorem 7]. We cannot use the earlier [8 
Theorem 3] which shows that any matrix A contains k columns which provide an O(k)-approximation
for the (cid:96)p-low rank approximation problem  since that proof requires p ≥ 1 and critically uses
scale-invariance  which does not hold for p = 0. Our combinatorial argument in Lemma 2 seems
fundamentally different than the maximum volume submatrix argument in [8] for p ≥ 1.
Second  unlike for (cid:96)p-regression for p ≥ 1  the (cid:96)0-regression problem minx (cid:107)U x− b(cid:107)0 given a matrix
U and vector b is not efﬁciently solvable since it corresponds to a nearest codeword problem  which
is NP-hard [1]. Thus  we resort to an approximation algorithm for (cid:96)0-regression  based on ideas for
solving the nearest codeword problem in [2  5].
Note that OPT(k) ≤ (cid:107)A(cid:107)0. Since there are only mn + 1 possibilities of OPT(k)  we can assume
we know OPT(k) and we can run the Algorithm 1 below for each such possibility  obtaining a
rank-O(k log n) solution  and then outputting the solution found with the smallest cost. This can
be further optimized by forming instead O(log(mn)) guesses of OPT(k). One of these guesses is
within a factor of 2 from the true value of OPT(k)  and we note that the following argument only
needs to know OPT(k) up to a factor of 2.
We start by deﬁning the notion of approximate coverage  which is different than the corresponding
notion in [8] for p ≥ 1  due to the fact that (cid:96)0-regression cannot be efﬁciently solved. Consequently 
approximate coverage for p = 0 cannot be efﬁciently tested. Let Q ⊆ [n] and M = A: Q be an
m × |Q| submatrix of A. We say that a column M: i is (S  Q)-approximately covered by a submatrix
M: S of M  if |S| = 2k and minx (cid:107)M: Sx − M: i(cid:107)0 ≤ 100(k+1)OPT(k)
Lemma 7. (Similar to [8  Lemma 6]  but using Lemma 2) Let Q ⊆ [n] and M = A: Q be a submatrix
of A. Suppose we select a subset R of 2k uniformly random columns of M. Then with probability at
least 1/3  at least a 1/10 fraction of the columns of M are (R  Q)-approximately covered.

|Q|

.

M

|Q|

def

def

|Q|

≤ (2k+1)OPT(k)

= R ∪ {i} and η

Proof. To show this  as in [8]  consider a uniformly random column index i not in the set R. Let
= minrank(B)=k(cid:107)M: T − B(cid:107)0. Since T is a uniformly random subset of 2k + 1
T
columns of M  ET η ≤ (2k+1)OPT(k)
. Let E1 be the event η ≤ 10(2k+1)OPT(k)
.
Then  by a Markov bound  Pr[E1] ≥ 9/10.
Fix a conﬁguration T = R ∪ {i} and let L(T ) ⊂ T be the subset guaranteed by Lemma 2 such
that |L(T )| = k and minX(cid:107)M: L(T )X − M: T(cid:107)0 ≤ (k + 1) minrank(B)=k(cid:107)M: T − B(cid:107)0. Notice that
2k+1 minX(cid:107)M: L(T )X − M: T(cid:107)0  and thus by the law of total
Ei
probability we have ET
2k+1 .
Let E2 denote the event that minx (cid:107)M: Lx− M: i(cid:107)0 ≤ 10(k+1)η

(cid:2)minx(cid:107)M: L(T )x − M: i(cid:107)0 | T(cid:3) = 1
(cid:3) ≤ (k+1)η
Further  as in [8]  let E3 be the event that i /∈ L. Observe that there are(cid:0)k+1
R(cid:48) ⊂ T such that |R(cid:48)| = 2k and L ⊂ R(cid:48). Since there are(cid:0)2k+1

(cid:1) ways to choose a subset
(cid:1) ways to choose R(cid:48)  it follows that

(cid:2)minx (cid:107)M: L(T )x − M: i(cid:107)0

. By a Markov bound  Pr[E2] ≥ 9/10.

2k+1 > 1/2. Hence  by the law of total probability  we have Pr[E3] > 1/2.

Pr[L ⊂ R | T ] = k+1
As in [8]  Pr[E1 ∧ E2 ∧ E3] > 2/5  and conditioned on E1 ∧ E2 ∧ E3  minx (cid:107)M: Rx − M: i(cid:107)0 ≤
minx (cid:107)M: Lx − M: i(cid:107)0 ≤ 10(k+1)η
  where the ﬁrst inequality uses that L is a
subset of R given E3  and so the regression cost cannot decrease  while the second inequality uses the
occurrence of E2 and the ﬁnal inequality uses the occurrence of E1.
As in [8]  if Zi is an indicator random variable indicating whether i is approximately covered
5 . By a Markov bound 
3. Thus  probability at least 1/3  at least a 1/10 fraction of the columns of

by R  and Z = (cid:80)

i∈Q Zi  then ER[Z] ≥ 2|Q|

2k+1 ≤ 100(k+1)OPT(k)

and ER[|Q| − Z] ≤ 3|Q|

Pr[|Q| − Z ≥ 9|Q|
M are (R  Q)-approximately covered.

10 ] ≤ 2

|Q|

2k+1

2k

|Q|

k

5

6

Algorithm 1 Selecting O(k log(n/k)) columns of A.
Require: An integer k  and a matrix A.
Ensure: O(k log(n/k)) columns of A
APPROXIMATELYSELECTCOLUMNS (k  A):
if number of columns of A ≤ 2k then

return all the columns of A

Let R be a set of 2k uniformly random columns of A

until at least (1/10)-fraction columns of A are nearly approximately covered
Let AR be the columns of A not nearly approximately covered by R
return R ∪ APPROXIMATELYSELECTCOLUMNS(k  AR)

else

repeat

end if

Given Lemma 7  we are ready to prove Theorem 6. As noted above  a key difference with the
corresponding [8  Algorithm 3] for (cid:96)p and p ≥ 1  is that we cannot efﬁciently test if a column i is
approximately covered by a set R. We will instead again make use of Theorem 5.

|Q|

Proof of Theorem 6. The computation of matrix Z force us to relax the notion of (R  Q)-
approximately covered to the notion of (R  Q)-nearly-approximately covered as follows: we say
that a column M: i is (R  Q)-nearly-approximately covered if  the algorithm in Theorem 5 returns a
vector z such that (cid:107)M: Rz − M: i(cid:107)0 ≤ 100(k+1)2OPT(k)
. By the guarantee of Theorem 5  if M: i is
(R  Q)-approximately covered then it is also w.h.p. (R  Q)-nearly-approximately covered.
Suppose Algorithm 1 makes t iterations and let A: ∪t
i=1Ri and Z be the resulting solution. We bound
now its cost. Let B0 = [n]  and consider the i-th iteration of Algorithm 1. We denote by Ri a set of 2k
uniformly random columns of Bi−1  by Gi a set of columns that is (Ri  Bi−1)-nearly-approximately
covered  and by Bi = Bi−1\{Gi ∪ Ri} a set of the remaining columns. By construction  |Gi| ≥
10|Bi−1|. Since Algorithm 1 terminates when Bt+1 ≤ 2k 
|Bi−1|/10 and |Bi| ≤ 9
we have 2k < |Bt| < (1 − 1
10 )tn  and thus the number of iterations t ≤ 10 log(n/2k). By
|Gi|
|Bi−1| ≤ t ≤ 10 log n
2k .
(cid:107)A: Riz(j) − A: j(cid:107)0 ≤

construction  |Gi| = (1 − αi)|Bi−1| for some αi ≤ 9/10  and so(cid:80)t
(cid:80)
(cid:1) · OPT(k).
(cid:80)t
Since minx(j)(cid:107)A: Rix(j) − A: j(cid:107)0 ≤ 100(k+1)2OPT(k)

10|Bi−1| − 2k < 9

  we have(cid:80)t
k · minx(j)(cid:107)A: Rix(j) − A: j(cid:107)0 ≤ O(cid:0)k2 · log n

|Bi−1|

i=1
j∈Gi

(cid:80)

By Lemma 7  the expected number of iterations of selecting a set Ri such that |Gi| ≥ 1/10|Bi−1|
is O(1). Since the number of recursive calls t is bounded by O(log(n/k))  it follows by a Markov
bound that Algorithm 1 chooses O(k log(n/k)) columns in total. Since the approximation algorithm
of Theorem 5 runs in polynomial time  our entire algorithm has expected polynomial time.

i=1

j∈Gi

i=1

2k

4 Algorithm for Real (cid:96)0-rank-1
Given a matrix A ∈ Rm×n  the (cid:96)0-rank-1 problem asks to ﬁnd a matrix A(cid:48) with rank 1 such that the
difference between A and A(cid:48) measured in (cid:96)0 norm is minimized. We denote the optimum value by

OPT(1) def

= min

rank(A(cid:48))=1

(cid:107)A − A(cid:48)(cid:107)0 =

min

u∈Rm  v∈Rn

(cid:107)A − uvT(cid:107)0.

(2)

In the trivial case when OPT(1) = 0  there is an optimal algorithm that runs in time O((cid:107)A(cid:107)0) and
ﬁnds the exact rank-1 decomposition uvT of a matrix A. In this work  we focus on the case when
OPT(1) ≥ 1. We show that Algorithm 2 yields a (2 + )-approximation factor and runs in nearly
linear time in (cid:107)A(cid:107)0  for any constant  > 0. Furthermore  a variant of our algorithm even runs in
sublinear time  if (cid:107)A(cid:107)0 is large and ψ
= OPT(1)/(cid:107)A(cid:107)0 is not too small. In particular  we obtain
time o((cid:107)A(cid:107)0) when OPT(1) ≥ (−1 log(mn))4 and (cid:107)A(cid:107)0 ≥ n(−1 log(mn))4.

def

7

Algorithm 2
Input: A ∈ Rm×n and  ∈ (0  0.1).
1. Partition the columns of A into weight-classes S = {S(0)  . . .   S(log n+1)} such that S(0) contains
all columns j with (cid:107)A: j(cid:107)0 = 0 and S(i) contains all columns j with 2i−1 ≤ (cid:107)A: j(cid:107)0 < 2i.
2. For each weight-class S(i) do:

2.1 Sample a set C (i) of Θ(−2 log n) elements uniformly at random from S(i).

2.2 Find a vector z(j) ∈ Rn such that (cid:107)A− A: j[z(j)]T(cid:107)0 ≤(cid:0)1 + 

each column A: j ∈ C (i).
3. Compute a (1 + 
Return: the pair (A: j  z(j)) corresponding to the minimal value Yj.

15 )-approximation Yj of (cid:107)A − A: j[z(j)]T(cid:107)0 for every j ∈(cid:83)

(cid:1) minv (cid:107)A− A: jvT(cid:107)0  for

i∈[|S|] C (i).

15

2

2

(cid:17)

runs w.h.p.

(cid:9)(cid:1) log2 n

(cid:16)(cid:0) n log m
2 + min(cid:8)(cid:107)A(cid:107)0  n + ψ−1 log n

Theorem 8. There is an algorithm that  given A ∈ Rm×n with column adja-
cency arrays and OPT(1) ≥ 1  and given  ∈ (0  0.1] 
in time
and outputs a column A: j and a vector z that
O
satisfy w.h.p. (cid:107)A − A: jzT(cid:107)0 ≤ (2 + )OPT(1). The algorithm also computes a value Y satisfying
w.h.p. (1 − )OPT(1) ≤ Y ≤ (2 + 2)OPT(1).
The only steps for which the implementation details are not immediate are Steps 2.2 and 3. We will
discuss them in Sections 4.1 and 4.2  respectively. Note that the algorithm from Theorem 8 selects a
column A: j and then ﬁnds a good vector z such that the product A: jzT approximates A. We show
that the approximation guarantee 2 +  is essentially tight for algorithms following this pattern.
Lemma 9. There exist a matrix A ∈ Rn×n such that minz(cid:107)A − A: jzT(cid:107)0 ≥ 2(1 − 1/n)OPT(1) 
for every column A: j.

4.1 Implementing Step 2.2

The Step 2.2 of Algorithm 2 uses the following sublinear procedure  given in Algorithm 3.
Lemma 10. Given A ∈ Rm×n  u ∈ Rm and  ∈ (0  1) we can compute in O(−2n log m) time a
vector z ∈ Rn such that w.h.p. (cid:107)A: i − ziu(cid:107)0 ≤ (1 + ) minvi(cid:107)A: i − viu(cid:107)0 for every i ∈ [n].

Algorithm 3
Input: A ∈ Rm×n  u ∈ Rm and  ∈ (0  1).
def
Let Z
= supp(u)  and p
1. Select each index i ∈ N with probability p and let S be the resulting set.
2. Compute a vector z ∈ Rn such that zj = arg minr∈R(cid:107)AS j − r · uS(cid:107)0 for all j ∈ [n].
Return: vector z.

= Θ(−2 log m)  N
def

= Z/|N|.

def

4.2

Implementing Step 3

for every j ∈(cid:83)

In Step 3 of Algorithm 2 we want to compute a (1 + 

15 )-approximation Yj of (cid:107)A − A: j[z(j)]T(cid:107)0
i∈[|S|] C (i). We present two solutions  an exact algorithm (see Lemma 11) and a
sublinear time sampling-based algorithm (see Lemma 13).
Lemma 11. Suppose A  B ∈ Rm×n are represented by column adjacency arrays. Then  we can
compute in O((cid:107)A(cid:107)0 + n) time the measure (cid:107)A − B(cid:107)0.
For our second  sampling-based implementation of Step 3  we make use of an algorithm by Dagum
et al. [11] for estimating the expected value of a random variable. We note that the runtime of their
algorithm is a random variable  the magnitude of which is bounded w.h.p. within a certain range.
= E[X] > 0. Let
def
Theorem 12. [11] Let X be a random variable taking values in [0  1] with µ
0 <   δ < 1 and ρX = max{Var[X]  µ}. There is an algorithm with sample access to X that
computes an estimator ˜µ in time t such that for a universal constant c we have:
i) Pr[(1 − )µ ≤ ˜µ ≤ (1 + )µ] ≥ 1 − δ 

ii) Pr[t ≥ c −2 log(1/δ)ρX /µ2] ≤ δ.

and

8

We state now our key technical insight  on which we build upon our sublinear algorithm.
Lemma 13. There is an algorithm that  given A  B ∈ Rm×n with column adjacency arrays and
(cid:107)A − B(cid:107)0 ≥ 1  and given  > 0  computes an estimator Z that satisﬁes w.h.p. (1 − )(cid:107)A − B(cid:107)0 ≤
Z ≤ (1 + )(cid:107)A − B(cid:107)0. The algorithm runs w.h.p. in time O(n + −2 (cid:107)A(cid:107)0+(cid:107)B(cid:107)0
(cid:107)A−B(cid:107)0

log n}).

We present now our main result in this section.
Theorem 14. There is an algorithm that  given A ∈ Rm×n with column adjacency arrays and
OPT(1) ≥ 1  and given j ∈ [n]  v ∈ Rm and  ∈ (0  1)  outputs an estimator Y that satisﬁes
w.h.p. (1 − )(cid:107)A − A: jvT(cid:107)0 ≤ Y ≤ (1 + )(cid:107)A − A: jvT(cid:107)0. The algorithm runs w.h.p. in time
O(min{(cid:107)A(cid:107)0  n + −2ψ−1 log n})  where ψ = OPT(1)/(cid:107)A(cid:107)0.

sampled column j ∈(cid:83)

0≤i≤log n+1 C (i).

To implement Step 3 of Algorithm 2  we simply apply Theorem 14 with A   and v = z(j) to each

5 Algorithms for Boolean (cid:96)0-rank-1

Our goal is to compute an approximate solution of the Boolean (cid:96)0-rank-1 problem  deﬁned by:

OPT = OPTA

def
=

min

u∈{0 1}m  v∈{0 1}n

(cid:107)A − uvT(cid:107)0  where A ∈ {0  1}m×n.

(3)

In practice  approximating a matrix A by a rank-1 matrix uvT makes most sense if A is close to being
rank-1. Hence  the above optimization problem is most relevant when OPT (cid:28) (cid:107)A(cid:107)0. In this section 
we focus on the case OPT/(cid:107)A(cid:107)0 ≤ φ for sufﬁciently small φ > 0. We prove the following.
Theorem 15. Given A ∈ {0  1}m×n with row and column sums  and given φ ∈ (0  1/80] with
OPT/(cid:107)A(cid:107)0 ≤ φ  we can compute vectors ˜u  ˜v with (cid:107)A − ˜u˜vT(cid:107)0 ≤ (1 + 5φ)OPT + 37φ2(cid:107)A(cid:107)0 in
time O(min{(cid:107)A(cid:107)0 + m + n  φ−1(m + n) log(mn)}).
In combination with Theorem 8 we obtain the following.
Theorem 16. Given A ∈ {0  1}m×n with column adjacency arrays and with row and column sums 
for ψ = OPT/(cid:107)A(cid:107)0 we can compute vectors ˜u  ˜v with (cid:107)A − ˜u˜vT(cid:107)0 ≤ (1 + 500ψ)OPT in time
w.h.p. O(min{(cid:107)A(cid:107)0 + m + n  ψ−1(m + n)} · log3(mn)).
A variant of the algorithm from Theorem 15 can also be used to solve the Boolean (cid:96)0-rank-1 problem
exactly. This yields the following theorem  which in particular shows that the problem is in polynomial

time when OPT ≤ O(cid:0)(cid:112)(cid:107)A(cid:107)0 log(mn)(cid:1).

Theorem 17. Given a matrix A ∈ {0  1}m×n  if OPTA/(cid:107)A(cid:107)0 ≤ 1/240 then we can exactly solve
the Boolean (cid:96)0-rank-1 problem in time 2O(OPT/

(cid:107)A(cid:107)0) · poly(mn).

√

6 Lower Bounds for Boolean (cid:96)0-rank-1

We give now a lower bound of Ω(n/φ) on the number of samples of any 1 + O(φ)-approximation
algorithm for the Boolean (cid:96)0-rank-1 problem  where OPT/(cid:107)A(cid:107)0 ≤ φ as before.
Theorem 18. Let C ≥ 1. Given an n × n Boolean matrix A with column adjacency arrays and

with row and column sums  and given(cid:112)log(n)/n (cid:28) φ ≤ 1/100C such that OPTA/(cid:107)A(cid:107)0 ≤ φ 

computing a (1 + Cφ)-approximation of OPTA requires to read Ω(n/φ) entries of A.

The technical core of our argument is the following lemma.
Lemma 19. Let φ ∈ (0  1/2). Let X1  . . .   Xk be Boolean random variables with expectations
p1  . . .   pk  where pi ∈ {1/2 − φ  1/2 + φ} for each i. Let A be an algorithm which can adaptively
obtain any number of samples of each random variable  and which outputs bits bi for every i ∈ [1 : k].
Suppose that with probability at least 0.95 over the joint probability space of A and the random
samples  A outputs for at least a 0.95 fraction of all i that bi = 1 if pi = 1/2 + φ and bi = 0
otherwise. Then  with probability at least 0.05  A makes Ω(k/φ2) samples in total.

9

References
[1] Michael Alekhnovich. More on average case vs approximation complexity. Computational

Complexity  20(4):755–786  2011.

[2] Noga Alon  Rina Panigrahy  and Sergey Yekhanin. Deterministic approximation algorithms
for the nearest codeword problem. In Approximation  Randomization  and Combinatorial Opti-
mization. Algorithms and Techniques  12th International Workshop  APPROX 2009  and 13th
International Workshop  RANDOM 2009  Berkeley  CA  USA  August 21-23  2009. Proceedings 
pages 339–351  2009.

[3] Sanjeev Arora  Rong Ge  Ravi Kannan  and Ankur Moitra. Computing a nonnegative matrix

factorization - provably. SIAM J. Comput.  45(4):1582–1611  2016.

[4] Radim Belohlávek and Vilém Vychodil. Discovery of optimal factors in binary data via a novel

method of matrix decomposition. J. Comput. Syst. Sci.  76(1):3–20  2010.

[5] Piotr Berman and Marek Karpinski. Approximating minimum unsatisﬁability of linear equations.
In Proceedings of the Thirteenth Annual ACM-SIAM Symposium on Discrete Algorithms 
January 6-8  2002  San Francisco  CA  USA.  pages 514–516  2002.

[6] Emmanuel J Candès  Xiaodong Li  Yi Ma  and John Wright. Robust principal component

analysis? Journal of the ACM (JACM)  58(3):11  2011.

[7] Emmanuel J. Candès  Xiaodong Li  Yi Ma  and John Wright. Robust principal component

analysis? J. ACM  58(3):11:1–11:37  June 2011.

[8] Flavio Chierichetti  Sreenivas Gollapudi  Ravi Kumar  Silvio Lattanzi  Rina Panigrahy  and
David P. Woodruff. Algorithms for $\ell_p$ low-rank approximation. In Proceedings of the
34th International Conference on Machine Learning  ICML 2017  Sydney  NSW  Australia  6-11
August 2017  pages 806–814  2017.

[9] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input
sparsity time. In Symposium on Theory of Computing Conference  STOC’13  Palo Alto  CA 
USA  June 1-4  2013  pages 81–90  2013.

[10] Kenneth L. Clarkson and David P. Woodruff. Input sparsity and hardness for robust subspace
approximation. In IEEE 56th Annual Symposium on Foundations of Computer Science  FOCS
2015  Berkeley  CA  USA  17-20 October  2015  pages 310–329  2015.

[11] Paul Dagum  Richard M. Karp  Michael Luby  and Sheldon M. Ross. An optimal algorithm for

monte carlo estimation. SIAM J. Comput.  29(5):1484–1496  2000.

[12] C. Dan  K. Arnsfelt Hansen  H. Jiang  L. Wang  and Y. Zhou. Low Rank Approximation of

Binary Matrices: Column Subset Selection and Generalizations. ArXiv e-prints  2015.

[13] Fedor V. Fomin  Daniel Lokshtanov  S. M. Meesum  Saket Saurabh  and Meirav Zehavi. Matrix
rigidity: Matrix theory from the viewpoint of parameterized complexity. In STACS. Springer 
March 2017.

[14] Nicolas Gillis and Stephen A. Vavasis. On the complexity of robust PCA and (cid:96)1-norm low-rank

matrix approximation. CoRR  abs/1509.09236  2015.

[15] D. Grigoriev. Using the notions of separability and independence for proving the lower bounds
on the circuit complexity (in russian). Notes of the Leningrad branch of the Steklov Mathematical
Institute  Nauka  1976.

[16] D. Grigoriev. Using the notions of separability and independence for proving the lower bounds

on the circuit complexity. Journal of Soviet Math.  14(5):1450–1456  1980.

[17] Harold W. Gutch  Peter Gruber  Arie Yeredor  and Fabian J. Theis. ICA over ﬁnite ﬁelds -

separability and algorithms. Signal Processing  92(8):1796–1808  2012.

10

[18] Peng Jiang  Jiming Peng  Michael Heath  and Rui Yang. A clustering approach to constrained
binary matrix factorization. In Data Mining and Knowledge Discovery for Big Data  pages
281–303. Springer  2014.

[19] Ravi Kannan and Santosh Vempala. Spectral algorithms. Foundations and Trends in Theoretical

Computer Science  4(3-4):157–288  2009.

[20] Mehmet Koyutürk and Ananth Grama. Proximus: a framework for analyzing very high dimen-
sional discrete-attributed datasets. In Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining  pages 147–156. ACM  2003.

[21] Mehmet Koyutürk  Ananth Grama  and Naren Ramakrishnan. Compression  clustering  and
pattern discovery in very high-dimensional discrete-attribute data sets. IEEE Trans. Knowl.
Data Eng.  17(4):447–461  2005.

[22] Mehmet Koyutürk  Ananth Grama  and Naren Ramakrishnan. Nonorthogonal decomposition
of binary matrices for bounded-error data compression and analysis. ACM Transactions on
Mathematical Software (TOMS)  32(1):33–69  2006.

[23] Tao Li. A general model for clustering binary data. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge discovery in data mining  pages 188–197.
ACM  2005.

[24] Michael W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends

in Machine Learning  3(2):123–224  2011.

[25] Edward Meeds  Zoubin Ghahramani  Radford M. Neal  and Sam T. Roweis. Modeling dyadic
data with binary latent factors. In Advances in Neural Information Processing Systems 19 
Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems 
Vancouver  British Columbia  Canada  December 4-7  2006  pages 977–984  2006.

[26] Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-
sparsity time and applications to robust linear regression. In Symposium on Theory of Computing
Conference  STOC’13  Palo Alto  CA  USA  June 1-4  2013  pages 91–100  2013.

[27] Pauli Miettinen  Taneli Mielikäinen  Aristides Gionis  Gautam Das  and Heikki Mannila. The

discrete basis problem. IEEE Trans. Knowl. Data Eng.  20(10):1348–1362  2008.

[28] Pauli Miettinen and Jilles Vreeken. MDL4BMF: minimum description length for boolean

matrix factorization. TKDD  8(4):18:1–18:31  2014.

[29] Jelani Nelson and Huy L. Nguyen. OSNAP: faster numerical linear algebra algorithms via
sparser subspace embeddings. In 54th Annual IEEE Symposium on Foundations of Computer
Science  FOCS 2013  26-29 October  2013  Berkeley  CA  USA  pages 117–126  2013.

[30] A. Painsky  S. Rosset  and M. Feder. Generalized Independent Component Analysis Over Finite

Alphabets. ArXiv e-prints  2015.

[31] S. Ravanbakhsh  B. Poczos  and R. Greiner. Boolean Matrix Factorization and Noisy Completion

via Message Passing. ArXiv e-prints  2015.

[32] Ilya P. Razenshteyn  Zhao Song  and David P. Woodruff. Weighted low rank approximations
with provable guarantees. In Proceedings of the 48th Annual ACM SIGACT Symposium on
Theory of Computing  STOC 2016  Cambridge  MA  USA  June 18-21  2016  pages 250–263 
2016.

[33] Jouni K. Seppänen  Ella Bingham  and Heikki Mannila. A simple algorithm for topic identiﬁca-
tion in 0-1 data. In Knowledge Discovery in Databases: PKDD 2003  7th European Conference
on Principles and Practice of Knowledge Discovery in Databases  Cavtat-Dubrovnik  Croatia 
September 22-26  2003  Proceedings  pages 423–434  2003.

[34] Bao-Hong Shen  Shuiwang Ji  and Jieping Ye. Mining discrete patterns via binary matrix
factorization. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining  Paris  France  June 28 - July 1  2009  pages 757–766  2009.

11

[35] Tomás Singliar and Milos Hauskrecht. Noisy-or component analysis and its application to link

analysis. Journal of Machine Learning Research  7:2189–2213  2006.

[36] Zhao Song  David P. Woodruff  and Peilin Zhong. Low rank approximation with entrywise

(cid:96)1-norm error. CoRR  abs/1611.00898  2016.

[37] Zhao Song  David P. Woodruff  and Peilin Zhong. Entrywise low rank approximation of general

functions  2018. Manuscript.

[38] Jaideep Vaidya  Vijayalakshmi Atluri  and Qi Guo. The role mining problem: ﬁnding a minimal
descriptive set of roles. In 12th ACM Symposium on Access Control Models and Technologies 
SACMAT 2007  Sophia Antipolis  France  June 20-22  2007  Proceedings  pages 175–184  2007.

[39] Leslie G. Valiant. Graph-theoretic arguments in low-level complexity.

In Mathematical
Foundations of Computer Science 1977  6th Symposium  Tatranska Lomnica  Czechoslovakia 
September 5-9  1977  Proceedings  pages 162–176  1977.

[40] David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in

Theoretical Computer Science  10(1-2):1–157  2014.

[41] Arie Yeredor. Independent component analysis over galois ﬁelds of prime order. IEEE Trans.

Information Theory  57(8):5342–5359  2011.

[42] Zhong-Yuan Zhang  Tao Li  Chris Ding  Xian-Wen Ren  and Xiang-Sun Zhang. Binary matrix
factorization for analyzing gene expression data. Data Mining and Knowledge Discovery 
20(1):28–52  2010.

[43] Zhongyuan Zhang  Tao Li  Chris Ding  and Xiangsun Zhang. Binary matrix factorization with
applications. In Data Mining  2007. ICDM 2007. Seventh IEEE International Conference on 
pages 391–400. IEEE  2007.

12

,Karl Bringmann
Pavel Kolev
David Woodruff
Jie Song
Yixin Chen
Xinchao Wang
Chengchao Shen
Mingli Song