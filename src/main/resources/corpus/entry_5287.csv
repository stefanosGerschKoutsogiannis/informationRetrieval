2017,Streaming Weak Submodularity: Interpreting Neural Networks on the Fly,In many machine learning applications  it is important to explain the predictions of a black-box classifier. For example  why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014]  we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function. This is the first such theoretical guarantee for this general class of functions  and we also show that no such algorithm exists for a worst case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster than the state-of-the-art LIME framework of Ribeiro et al. [2016].,Streaming Weak Submodularity:

Interpreting Neural Networks on the Fly

Ethan R. Elenberg

Department of Electrical
and Computer Engineering

The University of Texas at Austin

elenberg@utexas.edu

Alexandros G. Dimakis
Department of Electrical
and Computer Engineering

The University of Texas at Austin
dimakis@austin.utexas.edu

Moran Feldman

Department of Mathematics

and Computer Science
Open University of Israel
moranfe@openu.ac.il

Amin Karbasi

Department of Electrical Engineering

Department of Computer Science

Yale University

amin.karbasi@yale.edu

Abstract

In many machine learning applications  it is important to explain the predictions
of a black-box classiﬁer. For example  why does a deep neural network assign
an image to a particular class? We cast interpretability of black-box classiﬁers
as a combinatorial maximization problem and propose an efﬁcient streaming
algorithm to solve it subject to cardinality constraints. By extending ideas from
Badanidiyuru et al. [2014]  we provide a constant factor approximation guarantee
for our algorithm in the case of random stream order and a weakly submodular
objective function. This is the ﬁrst such theoretical guarantee for this general class
of functions  and we also show that no such algorithm exists for a worst case stream
order. Our algorithm obtains similar explanations of Inception V3 predictions 10
times faster than the state-of-the-art LIME framework of Ribeiro et al. [2016].

Introduction

1
Consider the following combinatorial optimization problem. Given a ground set N of N elements
and a set function f : 2N 7! R0  ﬁnd the set S of size k which maximizes f (S). This formulation
is at the heart of many machine learning applications such as sparse regression  data summarization 
facility location  and graphical model inference. Although the problem is intractable in general  if
f is assumed to be submodular then many approximation algorithms have been shown to perform
provably within a constant factor from the best solution.
Some disadvantages of the standard greedy algorithm of Nemhauser et al. [1978] for this problem are
that it requires repeated access to each data element and a large total number of function evaluations.
This is undesirable in many large-scale machine learning tasks where the entire dataset cannot ﬁt in
main memory  or when a single function evaluation is time consuming. In our main application  each
function evaluation corresponds to inference on a large neural network and can take a few seconds.
In contrast  streaming algorithms make a small number of passes (often only one) over the data and
have sublinear space complexity  and thus  are ideal for tasks of the above kind.
Recent ideas  algorithms  and techniques from submodular set function theory have been used to
derive similar results in much more general settings. For example  Elenberg et al. [2016a] used
the concept of weak submodularity to derive approximation and parameter recovery guarantees for

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

nonlinear sparse regression. Thus  a natural question is whether recent results on streaming algorithms
for maximizing submodular functions [Badanidiyuru et al.  2014  Buchbinder et al.  2015  Chekuri
et al.  2015] extend to the weakly submodular setting.
This paper answers the above question by providing the ﬁrst analysis of a streaming algorithm
for any class of approximately submodular functions. We use key algorithmic components of
SIEVE-STREAMING [Badanidiyuru et al.  2014]  namely greedy thresholding and binary search 
combined with a novel analysis to prove a constant factor approximation for -weakly submodular
functions (deﬁned in Section 3). Speciﬁcally  our contributions are as follows.

• An impossibility result showing that  even for 0.5-weakly submodular objectives  no rando-
mized streaming algorithm which uses o(N ) memory can have a constant approximation
ratio when the ground set elements arrive in a worst case order.

• STREAK: a greedy  deterministic streaming algorithm for maximizing -weakly submodular
functions which uses O("1k log k) memory and has an approximation ratio of (1  ") 
2 ·
(3  e/2  2p2  e/2) when the ground set elements arrive in a random order.
• An experimental evaluation of our algorithm in two applications: nonlinear sparse regres-
sion using pairwise products of features and interpretability of black-box neural network
classiﬁers.

The above theoretical impossibility result is quite surprising since it stands in sharp contrast to known
streaming algorithms for submodular objectives achieving a constant approximation ratio even for
worst case stream order.
One advantage of our approach is that  while our approximation guarantees are in terms of   our
algorithm STREAK runs without requiring prior knowledge about the value of . This is important
since the weak submodularity parameter  is hard to compute  especially in streaming applications 
as a single element can alter  drastically.
We use our streaming algorithm for neural network interpretability on Inception V3 [Szegedy et al. 
2016]. For that purpose  we deﬁne a new set function maximization problem similar to LIME [Ribeiro
et al.  2016] and apply our framework to approximately maximize this function. Experimentally 
we ﬁnd that our interpretability method produces explanations of similar quality as LIME  but runs
approximately 10 times faster.

2 Related Work

Monotone submodular set function maximization has been well studied  starting with the classical
analysis of greedy forward selection subject to a matroid constraint [Nemhauser et al.  1978  Fisher
et al.  1978]. For the special case of a uniform matroid constraint  the greedy algorithm achieves
an approximation ratio of 1  1/e [Fisher et al.  1978]  and a more involved algorithm obtains this
ratio also for general matroid constraints [C˘alinescu et al.  2011]. In general  no polynomial-time
algorithm can have a better approximation ratio even for a uniform matroid constraint [Nemhauser
and Wolsey  1978  Feige  1998]. However  it is possible to improve upon this bound when the data
obeys some additional guarantees [Conforti and Cornuéjols  1984  Vondrák  2010  Sviridenko et al. 
2015]. For maximizing nonnegative  not necessarily monotone  submodular functions subject to
a general matroid constraint  the state-of-the-art randomized algorithm achieves an approximation
ratio of 0.385 [Buchbinder and Feldman  2016b]. Moreover  for uniform matroids there is also
a deterministic algorithm achieving a slightly worse approximation ratio of 1/e [Buchbinder and
Feldman  2016a]. The reader is referred to Bach [2013] and Krause and Golovin [2014] for surveys
on submodular function theory.
A recent line of work aims to develop new algorithms for optimizing submodular functions suit-
able for large-scale machine learning applications. Algorithmic advances of this kind include
STOCHASTIC-GREEDY [Mirzasoleiman et al.  2015]  SIEVE-STREAMING [Badanidiyuru et al. 
2014]  and several distributed approaches [Mirzasoleiman et al.  2013  Barbosa et al.  2015  2016  Pan
et al.  2014  Khanna et al.  2017b]. Our algorithm extends ideas found in SIEVE-STREAMING and
uses a different analysis to handle more general functions. Additionally  submodular set functions
have been used to prove guarantees for online and active learning problems [Hoi et al.  2006  Wei
et al.  2015  Buchbinder et al.  2015]. Speciﬁcally  in the online setting corresponding to our setting

2

(i.e.  maximizing a monotone function subject to a cardinality constraint)  Chan et al. [2017] achieve
a competitive ratio of about 0.3178 when the function is submodular.
The concept of weak submodularity was introduced in Krause and Cevher [2010]  Das and Kempe
[2011]  where it was applied to the speciﬁc problem of feature selection in linear regression. Their
main results state that if the data covariance matrix is not too correlated (using either incoherence or
restricted eigenvalue assumptions)  then maximizing the goodness of ﬁt f (S) = R2
S as a function of
the feature set S is weakly submodular. This leads to constant factor approximation guarantees for
several greedy algorithms. Weak submodularity was connected with Restricted Strong Convexity
in Elenberg et al. [2016a b]. This showed that the same assumptions which imply the success of
regularization also lead to guarantees on greedy algorithms. This framework was later used for
additional algorithms and applications [Khanna et al.  2017a b]. Other approximate versions of
submodularity were used for greedy selection problems in Horel and Singer [2016]  Hassidim and
Singer [2017]  Altschuler et al. [2016]  Bian et al. [2017]. To the best of our knowledge  this is the
ﬁrst analysis of streaming algorithms for approximately submodular set functions.
Increased interest in interpretable machine learning models has led to extensive study of sparse
feature selection methods. For example  Bahmani et al. [2013] consider greedy algorithms for logistic
regression  and Yang et al. [2016] solve a more general problem using `1 regularization. Recently 
Ribeiro et al. [2016] developed a framework called LIME for interpreting black-box neural networks 
and Sundararajan et al. [2017] proposed a method that requires access to the network’s gradients with
respect to its inputs. We compare our algorithm to variations of LIME in Section 6.2.

3 Preliminaries

First we establish some deﬁnitions and notation. Sets are denoted with capital letters  and all big O
notation is assumed to be scaling with respect to N (the number of elements in the input stream).
Given a set function f  we often use the discrete derivative f (B | A)   f (A [ B)  f (A). f is
monotone if f (B | A)  0 8A  B and nonnegative if f (A)  0 8A. Using this notation one can
deﬁne weakly submodular functions based on the following ratio.
Deﬁnition 3.1 (Weak Submodularity  adapted from Das and Kempe [2011]). A monotone nonnegative
set function f : 2N 7! R0 is called -weakly submodular for an integer r if

  r   min
L S✓N :

 

|L| |S\L|rPj2S\L f (j | L)

f (S | L)

where the ratio is considered to be equal to 1 when its numerator and denominator are both 0.

This generalizes submodular functions by relaxing the diminishing returns property of discrete
derivatives. It is easy to show that f is submodular if and only if |N| = 1.
Deﬁnition 3.2 (Approximation Ratio). A streaming maximization algorithm ALG which returns
a set S has approximation ratio R 2 [0  1] if E[f (S)]  R · f (OP T )  where OP T is the optimal
solution and the expectation is over the random decisions of the algorithm and the randomness of the
input stream order (when it is random).
Formally our problem is as follows. Assume that elements from a ground set N arrive in a stream at
either random or worst case order. The goal is then to design a one pass streaming algorithm that
given oracle access to a nonnegative set function f : 2N 7! R0 maintains at most o(N ) elements in
memory and returns a set S of size at most k approximating

f (T )  

max
|T|k

up to an approximation ratio R(k). Ideally  this approximation ratio should be as large as possible 
and we also want it to be a function of k and nothing else. In particular  we want it to be independent
of k and N.
To simplify notation  we use  in place of k in the rest of the paper. Additionally  proofs for all our
theoretical results are deferred to the Supplementary Material.

3

4

Impossibility Result

To prove our negative result showing that no streaming algorithm for our problem has a constant
approximation ratio against a worst case stream order  we ﬁrst need to construct a weakly submodular
set function fk. Later we use it to construct a bad instance for any given streaming algorithm.
Fix some k  1  and consider the ground set Nk = {ui  vi}k
for every subset S ✓N k

i=1. For ease of notation  let us deﬁne

u(S) = |S \{ ui}k
Now we deﬁne the following set function:

i=1|  

v(S) = |S \{ vi}k

i=1| .

fk(S) = min{2 · u(S) + 1  2 · v(S)}8

S ✓N k .

Lemma 4.1. fk is nonnegative  monotone and 0.5-weakly submodular for the integer |Nk|.
Since |Nk| = 2k  the maximum value of fk is fk(Nk) = 2· v(Nk) = 2k. We now extend the ground
set of fk by adding to it an arbitrary large number d of dummy elements which do not affect fk at all.
Clearly  this does not affect the properties of fk proved in Lemma 4.1. However  the introduction
of dummy elements allows us to assume that k is an arbitrary small value compared to N  which is
necessary for the proof of the next theorem. In a nutshell  this proof is based on the observation that
the elements of {ui}k
i=1 are indistinguishable from the dummy elements as long as no element of
i=1 has arrived yet.
{vi}k
Theorem 4.2. For every constant c 2 (0  1] there is a large enough k such that no randomized
streaming algorithm that uses o(N ) memory to solve max|S|2k fk(S) has an approximation ratio
of c for a worst case stream order.

We note that fk has strong properties.
In particular  Lemma 4.1 implies that it is 0.5-weakly
submodular for every 0  r |N| .
In contrast  the algorithm we show later assumes weak
submodularity only for the cardinality constraint k. Thus  the above theorem implies that worst
case stream order precludes a constant approximation ratio even for functions with much stronger
properties compared to what is necessary for getting a constant approximation ratio when the order is
random.
The proof of Theorem 4.2 relies critically on the fact that each element is seen exactly once. In
other words  once the algorithm decides to discard an element from its memory  this element is gone
forever  which is a standard assumption for streaming algorithms. Thus  the theorem does not apply
to algorithms that use multiple passes over N   or non-streaming algorithms that use o(N ) writable
memory  and their analysis remains an interesting open problem.

5 Streaming Algorithms

In this section we give a deterministic streaming algorithm for our problem which works in a model
in which the stream contains the elements of N in a random order. We ﬁrst describe in Section 5.1
such a streaming algorithm assuming access to a value ⌧ which approximates a · f (OP T )  where a
is a shorthand for a = (p2  e/2  1)/2. Then  in Section 5.2 we explain how this assumption
can be removed to obtain STREAK and bound its approximation ratio  space complexity  and running
time.

5.1 Algorithm with access to ⌧

Consider Algorithm 1.
In addition to the input instance  this algorithm gets a parameter ⌧ 2
[0  a · f (OP T )]. One should think of ⌧ as close to a · f (OP T )  although the following analysis
of the algorithm does not rely on it. We provide an outline of the proof  but defer the technical details
to the Supplementary Material.
Theorem 5.1. The expected value of the set produced by Algorithm 1 is at least

3  e/2  2p2  e/2

2

⌧
a ·

= ⌧ · (p2  e/2  1) .

4

Algorithm 1 THRESHOLD GREEDY(f  k  ⌧ )

Let S ?.
while there are more elements do

Let u be the next element.
if |S| < k and f (u | S)  ⌧ /k then
Update S S [{ u}.
end if
end while
return: S

Algorithm 2 STREAK(f  k  ")

Let m 0  and let I be an (originally empty) collection of instances of Algorithm 1.
while there are more elements do

Update m f (u) and um u.

Let u be the next element.
if f (u)  m then
end if
Update I so that it contains an instance of Algorithm 1 with ⌧ = x for every x 2{ (1  ")i | i 2
Z and (1  ")m/(9k2)  (1  ")i  mk}  as explained in Section 5.2.
Pass u to all instances of Algorithm 1 in I.

end while
return: the best set among all the outputs of the instances of Algorithm 1 in I and the singleton
set {um}.

Proof (Sketch). Let E be the event that f (S) <⌧   where S is the output produced by Algorithm 1.
Clearly f (S)  ⌧ whenever E does not occur  and thus  it is possible to lower bound the expected
value of f (S) using E as follows.
Observation 5.2. Let S denote the output of Algorithm 1  then E[f (S)]  (1  Pr[E]) · ⌧.
The lower bound given by Observation 5.2 is decreasing in Pr[E]. Proposition 5.4 provides another
lower bound for E[f (S)] which increases with Pr[E]. An important ingredient of the proof of this
proposition is the next observation  which implies that the solution produced by Algorithm 1 is always
of size smaller than k when E happens.
Observation 5.3. If at some point Algorithm 1 has a set S of size k  then f (S)  ⌧.
The proof of Proposition 5.4 is based on the above observation and on the observation that the random
arrival order implies that every time that an element of OP T arrives in the stream we may assume it
is a random element out of all the OP T elements that did not arrive yet.
Proposition 5.4. For the set S produced by Algorithm 1 

1

E[f (S)] 

2 ·⇣ · [Pr[E]  e/2] · f (OP T )  2⌧⌘ .
The theorem now follows by showing that for every possible value of Pr[E] the guarantee of the
theorem is implied by either Observation 5.2 or Proposition 5.4. Speciﬁcally  the former happens
when Pr[E]  2  p2  e/2 and the later when Pr[E]  2  p2  e/2.
5.2 Algorithm without access to ⌧

In this section we explain how to get an algorithm which does not depend on ⌧. Instead  STREAK
(Algorithm 2) receives an accuracy parameter " 2 (0  1). Then  it uses " to run several instances of
Algorithm 1 stored in a collection denoted by I. The algorithm maintains two variables throughout its
execution: m is the maximum value of a singleton set corresponding to an element that the algorithm
already observed  and um references an arbitrary element satisfying f (um) = m.

5

The collection I is updated as follows after each element arrival. If previously I contained an instance
of Algorithm 1 with a given value for ⌧  and it no longer should contain such an instance  then the
instance is simply removed. In contrast  if I did not contain an instance of Algorithm 1 with a given
value for ⌧  and it should now contain such an instance  then a new instance with this value for ⌧ is
created. Finally  if I contained an instance of Algorithm 1 with a given value for ⌧  and it should
continue to contain such an instance  then this instance remains in I as is.
Theorem 5.5. The approximation ratio of STREAK is at least

3  e/2  2p2  e/2

.

(1  ") ·

2

The proof of Theorem 5.5 shows that in the ﬁnal collection I there is an instance of Algorithm 1
whose ⌧ provides a good approximation for a · f (OP T )  and thus  this instance of Algorithm 1
should (up to some technical details) produce a good output set in accordance with Theorem 5.1.
It remains to analyze the space complexity and running time of STREAK. We concentrate on bounding
the number of elements STREAK keeps in its memory at any given time  as this amount dominates
the space complexity as long as we assume that the space necessary to keep an element is at least as
large as the space necessary to keep each one of the numbers used by the algorithm.
Theorem 5.6. The space complexity of STREAK is O("1k log k) elements.
The running time of Algorithm 1 is O(N f ) where  abusing notation  f is the running time of a single
oracle evaluation of f. Therefore  the running time of STREAK is O(N f"1 log k) since it uses at
every given time only O("1 log k) instances of the former algorithm. Given multiple threads  this
can be improved to O(N f + "1 log k) by running the O("1 log k) instances of Algorithm 1 in
parallel.

6 Experiments

We evaluate the performance of our streaming algorithm on two sparse feature selection applications.1
Features are passed to all algorithms in a random order to match the setting of Section 5.

(a) Performance

(b) Cost

Figure 1: Logistic Regression  Phishing dataset with pairwise feature products. Our algorithm is
comparable to LOCALSEARCH in both log likelihood and generalization accuracy  with much lower
running time and number of model ﬁts in most cases. Results averaged over 40 iterations  error bars
show 1 standard deviation.
6.1 Sparse Regression with Pairwise Features

In this experiment  a sparse logistic regression is ﬁt on 2000 training and 2000 test observations from
the Phishing dataset [Lichman  2013]. This setup is known to be weakly submodular under mild data
assumptions [Elenberg et al.  2016a]. First  the categorical features are one-hot encoded  increasing

1Code for these experiments is available at https://github.com/eelenberg/streak.

6

RandomStreak(0.75)Streak(0.1)LocalSearch0200400600LogLikelihoodRandomStreak(0.75)Streak(0.1)LocalSearch0.700.750.800.850.900.951.00GeneralizationAccuracyk=20k=40k=80RandomStreak(0.75)Streak(0.1)LocalSearch050001000015000RunningTime(s)RandomStreak(0.75)Streak(0.1)LocalSearch0100000200000300000400000OracleEvaluationsk=20k=40k=80(a) Sparse Regression

(b) Interpretability

Figure 2:
2(a): Logistic Regression  Phishing dataset with pairwise feature products  k = 80
features. By varying the parameter "  our algorithm captures a time-accuracy tradeoff between
RANDOMSUBSET and LOCALSEARCH. Results averaged over 40 iterations  standard deviation
shown with error bars. 2(b): Running times of interpretability algorithms on the Inception V3
network  N = 30  k = 5. Streaming maximization runs 10 times faster than the LIME framework.
Results averaged over 40 total iterations using 8 example explanations  error bars show 1 standard
deviation.

the feature dimension to 68. Then  all pairwise products are added for a total of N = 4692 features.
To reduce computational cost  feature products are generated and added to the stream on-the-ﬂy as
needed. We compare with 2 other algorithms. RANDOMSUBSET selects the ﬁrst k features from
the random stream. LOCALSEARCH ﬁrst ﬁlls a buffer with the ﬁrst k features  and then swaps each
incoming feature with the feature from the buffer which yields the largest nonnegative improvement.
Figure 1(a) shows both the ﬁnal log likelihood and the generalization accuracy for RANDOMSUBSET 
LOCALSEARCH  and our STREAK algorithm for " = {0.75  0.1} and k = {20  40  80}. As expected 
the RANDOMSUBSET algorithm has much larger variation since its performance depends highly on
the random stream order. It also performs signiﬁcantly worse than LOCALSEARCH for both metrics 
whereas STREAK is comparable for most parameter choices. Figure 1(b) shows two measures of
computational cost: running time and the number of oracle evaluations (regression ﬁts). We note
STREAK scales better as k increases; for example  STREAK with k = 80 and " = 0.1 (" = 0.75)
runs in about 70% (5%) of the time it takes to run LOCALSEARCH with k = 40. Interestingly  our
speedups are more substantial with respect to running time. In some cases STREAK actually ﬁts
more regressions than LOCALSEARCH  but still manages to be faster. We attribute this to the fact
that nearly all of LOCALSEARCH’s regressions involve k features  which are slower than many of
the small regressions called by STREAK.
Figure 2(a) shows the ﬁnal log likelihood versus running time for k = 80 and " 2 [0.05  0.75]. By
varying the precision "  we achieve a gradual tradeoff between speed and performance. This shows
that STREAK can reduce the running time by over an order of magnitude with minimal impact on the
ﬁnal log likelihood.

6.2 Black-Box Interpretability

Our next application is interpreting the predictions of black-box machine learning models. Speciﬁcally 
we begin with the Inception V3 deep neural network [Szegedy et al.  2016] trained on ImageNet. We
use this network for the task of classifying 5 types of ﬂowers via transfer learning. This is done by
adding a ﬁnal softmax layer and retraining the network.
We compare our approach to the LIME framework [Ribeiro et al.  2016] for developing sparse 
interpretable explanations. The ﬁnal step of LIME is to ﬁt a k-sparse linear regression in the space of
interpretable features. Here  the features are superpixels determined by the SLIC image segmentation
algorithm [Achanta et al.  2012] (regions from any other segmentation would also sufﬁce). The
number of superpixels is bounded by N = 30. After a feature selection step  a ﬁnal regression is
performed on only the selected features. The following feature selection methods are supplied by

7

100101102103104105RunningTime(s)500550600650700LogLikelihoodRandomStreak(0.75)Streak(0.5)Streak(0.2)Streak(0.1)Streak(0.05)LocalSearchLIME+MaxWtsLIME+FSLIME+LassoStreak05001000150020002500RunningTime(s)LIME: 1. Highest Weights: ﬁts a full regression and keep the k features with largest coefﬁcients. 2.
Forward Selection: standard greedy forward selection. 3. Lasso: `1 regularization.
We introduce a novel method for black-box interpretability that is similar to but simpler than LIME.
As before  we segment an image into N superpixels. Then  for a subset S of those regions we can
create a new image that contains only these regions and feed this into the black-box classiﬁer. For a
given model M  an input image I  and a label L1 we ask for an explanation: why did model M label
image I with label L1. We propose the following solution to this problem. Consider the set function
f (S) giving the likelihood that image I(S) has label L1. We approximately solve

f (S)  

max
|S|k

using STREAK. Intuitively  we are limiting the number of superpixels to k so that the output will
include only the most important superpixels  and thus  will represent an interpretable explanation. In
our experiments we set k = 5.
Note that the set function f (S) depends on the black-box classiﬁer and is neither monotone nor
submodular in general. Still  we ﬁnd that the greedy maximization algorithm produces very good
explanations for the ﬂower classiﬁer as shown in Figure 3 and the additional experiments in the
Supplementary Material. Figure 2(b) shows that our algorithm is much faster than the LIME approach.
This is primarily because LIME relies on generating and classifying a large set of randomly perturbed
example images.

7 Conclusions

We propose STREAK  the ﬁrst streaming algorithm for maximizing weakly submodular functions 
and prove that it achieves a constant factor approximation assuming a random stream order. This
is useful when the set function is not submodular and  additionally  takes a long time to evaluate or
has a very large ground set. Conversely  we show that under a worst case stream order no algorithm
with memory sublinear in the ground set size has a constant factor approximation. We formulate
interpretability of black-box neural networks as set function maximization  and show that STREAK
provides interpretable explanations faster than previous approaches. We also show experimentally
that STREAK trades off accuracy and running time in nonlinear sparse regression.
One interesting direction for future work is to tighten the bounds of Theorems 5.1 and 5.5  which
are nontrivial but somewhat loose. For example  there is a gap between the theoretical guarantee
of the state-of-the-art algorithm for submodular functions and our bound for  = 1. However  as
our algorithm performs the same computation as that state-of-the-art algorithm when the function
is submodular  this gap is solely an analysis issue. Hence  the real theoretical performance of our
algorithm is better than what we have been able to prove in Section 5.

8 Acknowledgments

This research has been supported by NSF Grants CCF 1344364  1407278  1422549  1618689  ARO
YIP W911NF-14-1-0258  ISF Grant 1357/16  Google Faculty Research Award  and DARPA Young
Faculty Award (D16AP00046).

8

(a)

(b)

(c)

(d)

Figure 3: Comparison of interpretability algorithms for the Inception V3 deep neural network. We
have used transfer learning to extract features from Inception and train a ﬂower classiﬁer. In these
four input images the ﬂower types were correctly classiﬁed (from (a) to (d): rose  sunﬂower  daisy 
and daisy). We ask the question of interpretability: why did this model classify this image as rose.
We are using our framework (and the recent prior work LIME [Ribeiro et al.  2016]) to see which
parts of the image the neural network is looking at for these classiﬁcation tasks. As can be seen
STREAK correctly identiﬁes the ﬂower parts of the images while some LIME variations do not. More
importantly  STREAK is creating subsampled images on-the-ﬂy  and hence  runs approximately 10
times faster. Since interpretability tasks perform multiple calls to the black-box model  the running
times can be quite signiﬁcant.

9

References
Radhakrishna Achanta  Appu Shaji  Kevin Smith  Aurelien Lucchi  Pascal Fua  and Sabine Süsstrunk.
SLIC Superpixels Compared to State-of-the-art Superpixel Methods. IEEE Transactions on Pattern
Analysis and Machine Intelligence  34(11):2274–2282  2012.

Jason Altschuler  Aditya Bhaskara  Gang (Thomas) Fu  Vahab Mirrokni  Afshin Rostamizadeh 
and Morteza Zadimoghaddam. Greedy Column Subset Selection: New Bounds and Distributed
Algorithms. In ICML  pages 2539–2548  2016.

Francis R. Bach. Learning with Submodular Functions: A Convex Optimization Perspective. Foun-

dations and Trends in Machine Learning  6  2013.

Ashwinkumar Badanidiyuru  Baharan Mirzasoleiman  Amin Karbasi  and Andreas Krause. Streaming
Submodular Maximization: Massive Data Summarization on the Fly. In KDD  pages 671–680 
2014.

Sohail Bahmani  Bhiksha Raj  and Petros T. Boufounos. Greedy Sparsity-Constrained Optimization.

Journal of Machine Learning Research  14:807–841  2013.

Rafael da Ponte Barbosa  Alina Ene  Huy L. Nguyen  and Justin Ward. The Power of Randomization:
Distributed Submodular Maximization on Massive Datasets. In ICML  pages 1236–1244  2015.
Rafael da Ponte Barbosa  Alina Ene  Huy L. Nguyen  and Justin Ward. A New Framework for

Distributed Submodular Maximization. In FOCS  pages 645–654  2016.

Andrew An Bian  Baharan Mirzasoleiman  Joachim M. Buhmann  and Andreas Krause. Guaranteed
Non-convex Optimization: Submodular Maximization over Continuous Domains. In AISTATS 
pages 111–120  2017.

Niv Buchbinder and Moran Feldman. Deterministic Algorithms for Submodular Maximization

Problems. In SODA  pages 392–403  2016a.

Niv Buchbinder and Moran Feldman. Constrained Submodular Maximization via a Non-symmetric

Technique. CoRR  abs/1611.03253  2016b. URL http://arxiv.org/abs/1611.03253.

Niv Buchbinder  Moran Feldman  and Roy Schwartz. Online Submodular Maximization with

Preemption. In SODA  pages 1202–1216  2015.

Gruia C˘alinescu  Chandra Chekuri  Martin Pál  and Jan Vondrák. Maximizing a Monotone Submodu-

lar Function Subject to a Matroid Constraint. SIAM J. Comput.  40(6):1740–1766  2011.

T-H. Hubert Chan  Zhiyi Huang  Shaofeng H.-C. Jiang  Ning Kang  and Zhihao Gavin Tang. Online
Submodular Maximization with Free Disposal: Randomization Beats 1/4 for Partition Matroids. In
SODA  pages 1204–1223  2017.

Chandra Chekuri  Shalmoli Gupta  and Kent Quanrud. Streaming Algorithms for Submodular

Function Maximization. In ICALP  pages 318–330  2015.

Michele Conforti and Gérard Cornuéjols. Submodular set functions  matroids and the greedy
algorithm: Tight worst-case bounds and some generalizations of the Rado-Edmonds theorem.
Discrete Applied Mathematics  7(3):251–274  March 1984.

Abhimanyu Das and David Kempe. Submodular meets Spectral: Greedy Algorithms for Subset

Selection  Sparse Approximation and Dictionary Selection. In ICML  pages 1057–1064  2011.

Ethan R. Elenberg  Rajiv Khanna  Alexandros G. Dimakis  and Sahand Negahban. Restricted
Strong Convexity Implies Weak Submodularity. CoRR  abs/1612.00804  2016a. URL http:
//arxiv.org/abs/1612.00804.

Ethan R. Elenberg  Rajiv Khanna  Alexandros G. Dimakis  and Sahand Negahban. Restricted Strong
Convexity Implies Weak Submodularity. In NIPS Workshop on Learning in High Dimensions with
Structure  2016b.

Uriel Feige. A Threshold of ln n for Approximating Set Cover. Journal of the ACM (JACM)  45(4):

634–652  1998.

10

Marshall L. Fisher  George L. Nemhauser  and Laurence A. Wolsey. An analysis of approximations
for maximizing submodular set functions–II.
In M. L. Balinski and A. J. Hoffman  editors 
Polyhedral Combinatorics: Dedicated to the memory of D.R. Fulkerson  pages 73–87. Springer
Berlin Heidelberg  Berlin  Heidelberg  1978.

Avinatan Hassidim and Yaron Singer. Submodular Optimization Under Noise. In COLT  pages

1069–1122  2017.

Steven C. H. Hoi  Rong Jin  Jianke Zhu  and Michael R. Lyu. Batch Mode Active Learning and its

Application to Medical Image Classiﬁcation. In ICML  pages 417–424  2006.

Thibaut Horel and Yaron Singer. Maximization of Approximately Submodular Functions. In NIPS 

2016.

Rajiv Khanna  Ethan R. Elenberg  Alexandros G. Dimakis  Joydeep Ghosh  and Sahand Negahban.
On Approximation Guarantees for Greedy Low Rank Optimization. In ICML  pages 1837–1846 
2017a.

Rajiv Khanna  Ethan R. Elenberg  Alexandros G. Dimakis  Sahand Negahban  and Joydeep Ghosh.
Scalable Greedy Support Selection via Weak Submodularity. In AISTATS  pages 1560–1568 
2017b.

Andreas Krause and Volkan Cevher. Submodular Dictionary Selection for Sparse Representation. In

ICML  pages 567–574  2010.

Andreas Krause and Daniel Golovin. Submodular Function Maximization. Tractability: Practical

Approaches to Hard Problems  3:71–104  2014.

Moshe Lichman. UCI machine learning repository  2013. URL http://archive.ics.uci.edu/

ml.

Baharan Mirzasoleiman  Amin Karbasi  Rik Sarkar  and Andreas Krause. Distributed Submodular
Maximization: Identifying Representative Elements in Massive Data. NIPS  pages 2049–2057 
2013.

Baharan Mirzasoleiman  Ashwinkumar Badanidiyuru  Amin Karbasi  Jan Vondrák  and Andreas

Krause. Lazier Than Lazy Greedy. In AAAI  pages 1812–1818  2015.

George L. Nemhauser and Laurence A. Wolsey. Best Algorithms for Approximating the Maximum

of a Submodular Set Function. Math. Oper. Res.  3(3):177–188  August 1978.

George L. Nemhauser  Laurence A. Wolsey  and Marshall L. Fisher. An analysis of approximations
for maximizing submodular set functions–I. Mathematical Programming  14(1):265–294  1978.
Xinghao Pan  Stefanie Jegelka  Joseph E. Gonzalez  Joseph K. Bradley  and Michael I. Jordan.

Parallel Double Greedy Submodular Maximization. In NIPS  pages 118–126  2014.

Marco Tulio Ribeiro  Sameer Singh  and Carlos Guestrin. “Why Should I Trust You?” Explaining

the Predictions of Any Classiﬁer. In KDD  pages 1135–1144  2016.

Mukund Sundararajan  Ankur Taly  and Qiqi Yan. Axiomatic Attribution for Deep Networks. In

ICML  pages 3319–3328  2017.

Maxim Sviridenko  Jan Vondrák  and Justin Ward. Optimal approximation for submodular and

supermodular optimization with bounded curvature. In SODA  pages 1134–1148  2015.

Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Rethinking

the Inception Architecture for Computer Vision. In CVPR  pages 2818–2826  2016.

Jan Vondrák. Submodularity and curvature: the optimal algorithm. RIMS Kôkyûroku Bessatsu B23 

pages 253–266  2010.

Kai Wei  Iyer Rishabh  and Jeff Bilmes. Submodularity in Data Subset Selection and Active Learning.

ICML  pages 1954–1963  2015.

Zhuoran Yang  Zhaoran Wang  Han Liu  Yonina C. Eldar  and Tong Zhang. Sparse Nonlinear

Regression: Parameter Estimation and Asymptotic Inference. ICML  pages 2472–2481  2016.

11

,Ethan Elenberg
Alexandros Dimakis
Moran Feldman
Amin Karbasi