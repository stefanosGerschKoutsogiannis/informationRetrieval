2015,Structured Estimation with Atomic Norms: General Bounds and Applications,For structured estimation problems with atomic norms  recent advances in the literature express sample complexity and estimation error bounds in terms of certain geometric measures  in particular Gaussian width of the unit norm ball  Gaussian width of a spherical cap induced by a tangent cone  and a restricted norm compatibility constant. However  given an atomic norm  bounding these geometric measures can be difficult. In this paper  we present general upper bounds for such geometric measures  which only require simple information of the atomic norm under consideration  and we establish tightness of these bounds by providing the corresponding lower bounds. We show applications of our analysis to certain atomic norms  especially k-support norm  for which existing result is incomplete.,Structured Estimation with Atomic Norms:

General Bounds and Applications

Sheng Chen

Arindam Banerjee

Dept. of Computer Science & Engg.  University of Minnesota  Twin Cities

{shengc banerjee}@cs.umn.edu

Abstract

For structured estimation problems with atomic norms  recent advances in the lit-
erature express sample complexity and estimation error bounds in terms of certain
geometric measures  in particular Gaussian width of the unit norm ball  Gaussian
width of a spherical cap induced by a tangent cone  and a restricted norm com-
patibility constant. However  given an atomic norm  bounding these geometric
measures can be difﬁcult. In this paper  we present general upper bounds for such
geometric measures  which only require simple information of the atomic nor-
m under consideration  and we establish tightness of these bounds by providing
the corresponding lower bounds. We show applications of our analysis to certain
atomic norms  especially k-support norm  for which existing result is incomplete.

1

Introduction

Accurate recovery of structured sparse signal/parameter vectors from noisy linear measurements has
been extensively studied in the ﬁeld of compressed sensing  statistics  etc. The goal is to recover
a high-dimensional signal (parameter) θ∗ ∈ Rp which is sparse (only has a few nonzero entries) 
possibly with additional structure such as group sparsity. Typically one assume linear models  y =
Xθ∗ + ω  in which X ∈ Rn×p is the design matrix consisting of n samples  y ∈ Rn is the observed
response vector  and ω ∈ Rn is an unknown noise vector. By leveraging the sparsity of θ∗  previous
work has shown that certain L1-norm based estimators [22  7  8] can ﬁnd a good approximation of
θ∗ using sample size n (cid:28) p. Recent work has extended the notion of unstructured sparsity to other
structures in θ∗ which can be captured or approximated by some norm R(·) [10  18  3  11  6  19]
other than L1  e.g.  (non)overlapping group sparsity with L1/L2 norm [24  15]  etc. In general  two
broad classes of estimators are considered in recovery analysis: (i) Lasso-type estimators [22  18  3] 
which solve the regularized optimization problem

ˆθλn = argmin
θ∈Rp

1
2n

(cid:107)Xθ − y(cid:107)2

2 + λnR(θ)  

(1)

and (ii) Dantzig-type estimators [7  11  6]  which solve the constrained problem

R(θ) s.t. R∗(XT (Xθ − y)) ≤ λn  

ˆθλn = argmin
θ∈Rp

(2)
where R∗(·) is the dual norm of R(·). Variants of these estimators exist [10  19  23]  but the recovery
analysis proceeds along similar lines as these two classes of estimators.
To establish recovery guarantees  [18] focused on Lasso-type estimators and R(·) from the class of
decomposable norm  e.g.  L1  non-overlapping L1/L2 norm. The upper bound for the estimation
error (cid:107) ˆθλn−θ∗(cid:107)2 for any decomposable norm is characterized in terms of three geometric measures:
(i) a dual norm bound  as an upper bound for R∗(XT ω)  (ii) sample complexity  the minimal sample
size needed for a certain restricted eigenvalue (RE) condition to be true [4  18]  and (iii) a restricted

1

norm compatibility constant between R(·) and L2 norms [18  3]. The non-asymptotic estimation
√
error bound typically has the form (cid:107) ˆθλn − θ∗||2 ≤ c/
n  where c depends on a product of dual
norm bound and restricted norm compatibility  whereas the sample complexity characterizes the
minimum number of samples after which the error bound starts to be valid. In recent work  [3]
extended the analysis of Lasso-type estimator for decomposable norm to any norm  and gave a more
succinct characterization of the dual norm bound for R∗(XT ω) and the sample complexity for the
RE condition in terms of Gaussian widths [14  10  20  1] of suitable sets where  for any set A ∈ Rp 
the Gaussian width is deﬁned as

w(A) = E sup
u∈A

(cid:104)u  g(cid:105)  

(3)

√

R(u)
(cid:107)u(cid:107)2

(see Section 2 for details).

TR(θ∗) = cone{u ∈ Rp | R(θ∗ + u) ≤ R(θ∗)} .

where g is a standard Gaussian random vector. For Dantzig-type estimators  [11  6] obtained similar
extensions. To be speciﬁc  assume entries in X and ω are i.i.d. normal  and deﬁne the tangent cone 
(4)
nw(ΩR)) where ΩR = {u ∈
Then one can get (high-probability) upper bound for R∗(XT ω) as O(
Rp|R(u) ≤ 1} is the unit norm ball  and the RE condition is satisﬁed with O(w2(TR(θ∗) ∩ Sp−1))
samples  in which Sp−1 is the unit sphere. For convenience  we denote by CR(θ∗) the spherical
cap TR(θ∗) ∩ Sp−1 throughout the paper. Further  the restricted norm compatibility is given by
ΨR(θ∗) = supu∈TR(θ∗)
Thus  for any given norm  it sufﬁces to get a characterization of (i) w(ΩR)  the width of the u-
nit norm ball  (ii) w(CR(θ∗))  the width of the spherical cap induced by the tangent cone TR(θ∗) 
and (iii) ΨR(θ∗)  the restricted norm compatibility in the tangent cone. For the special case of L1
norm  accurate characterization of all three measures exist [10  18]. However  for more general
norms  the literature is rather limited. For w(ΩR)  the characterization is often reduced to com-
parison with either w(CR(θ∗)) [3] or known results on other norm balls [13]. While w(CR(θ∗))
has been investigated for certain decomposable norms [10  9  1]  little is known about general non-
decomposable norms. One general approach for upper bounding w(CR(θ∗)) is via the statistical
dimension [10  19  1]  which computes the expected squared distance between a Gaussian random
vector and the polar cone of TR(θ∗). To specify the polar  one need full information of the sub-
differential ∂R(θ∗)  which could be difﬁcult to obtain for non-decomposable norms. A notable
bound for (overlapping) L1/L2 norms is presented in [21]  which yields tight bounds for mildly
non-overlapping cases  but is loose for highly overlapping ones. For ΨR(θ∗)  the restricted norm
compatibility  results are only available for decomposable norms [18  3].
In this paper  we present a general set of bounds for the width w(ΩR) of the norm ball  the width
w(CR(θ∗)) of the spherical cap  and the restricted norm compatibility ΨR(θ∗). For the analysis 
we consider the class of atomic norms that are invariant under sign-changes  i.e.  the norm of a
vector stays unchanged if any entry changes only by ﬂipping its sign. The class is quite general  and
covers most of the popular norms used in practical applications  e.g.  L1 norm  ordered weighted
L1 (OWL) norm [5] and k-support norm [2]. Speciﬁcally we show that sharp bounds on w(ΩR)
can be obtained using simple calculation based on a decomposition inequality from [16]. To upper
bound w(CR(θ∗)) and ΨR(θ∗)  instead of a full speciﬁcation of TR(θ∗)  we only require some
information regarding the subgradient of R(θ∗)  which is often readily accessible. The key insight
is that bounding statistical dimension often ends up computing the expected distance from Gaussian
vector to a single point rather than to the whole polar cone  thus the full information on ∂R(θ∗) is
unnecessary. In addition  we derive the corresponding lower bounds to show the tightness of our
results. As examples  we illustrate the bounds for L1 and OWL norms [5]. Finally  we give sharp
bounds for the recently proposed k-support norm [2]  for which existing analysis is incomplete.
The rest of the paper is organized as follows: we ﬁrst review the relevant background for Dantzig-
type estimator and atomic norm in Section 2. In Section 3  we introduce the general bounds for the
geometric measures. In Section 4  we discuss the tightness of our bounds. Section 5 is dedicated to
the example of k-support norm  and we conclude in Section 6.

2 Background

In this section  we brieﬂy review the recovery guarantee for the generalized Dantzig selector in (2)
and the basics on atomic norms. The following lemma  originally [11  Theorem 1]  provides an error
bound for (cid:107) ˆθλn − θ∗(cid:107)2. Related results have appeared for other estimators [18  10  19  3  23].

2

Lemma 1 Assume that y = Xθ∗ + ω  where entries of X and ω are i.i.d. copies of standard
Gaussian random variable. If λn ≥ c1
nw(ΩR) and n > c2w2(TR(θ∗) ∩ Sp−1) = w2(CR(θ∗))
for some constant c1  c2 > 1  with high probability  the estimate ˆθλn given by (2) satisﬁes

√

(cid:19)

(cid:107) ˆθλn − θ∗(cid:107)2 ≤ O

ΨR(θ∗)

w(ΩR)√
n

.

(5)

(cid:18)

(cid:88)

(cid:41)

(cid:40)(cid:88)

In this Lemma  there are three geometric measures—w(ΩR)  w(CR(θ∗)) and ΨR(θ∗)—which need
to be determined for speciﬁc R(·) and θ∗. In this work  we focus on general atomic norms R(·).
Given a set of atomic vectors A ⊂ Rp  the corresponding atomic norm of any θ ∈ Rp is given by

(cid:107)θ(cid:107)A = inf

caa  ca ≥ 0 ∀ a ∈ A

a∈A

a∈A

ca : θ =

(6)
In order for (cid:107) · (cid:107)A to be a valid norm  atomic vectors in A has to span Rp  and a ∈ A iff −a ∈ A.
The unit ball of atomic norm (cid:107) · (cid:107)A is given by ΩA = conv(A). In addition  we assume that the
atomic set A contains v(cid:12) a for any v ∈ {±1}p if a belongs to A  where (cid:12) denotes the elementwise
(Hadamard) product for vectors. This assumption guarantees that both (cid:107) · (cid:107)A and its dual norm
are invariant under sign-changes  which is satisﬁed by many widely used norms  such as L1 norm 
OWL norm [5] and k-support norm [2]. For the rest of the paper  we will use ΩA  TA(θ∗)  CA(θ∗)
and ΨA(θ∗) with A replaced by appropriate subscript for speciﬁc norms. For any vector u and
coordinate set S  we deﬁne uS by zeroing out all the coordinates outside S.

3 General Analysis for Atomic Norms

In this section  we present detailed analysis of the general bounds for the geometric measures 
w(ΩA)  w(CA(θ∗)) and ΨA(θ∗). In general  knowing the atomic set A is sufﬁcient for bound-
ing w(ΩA). For w(CA(θ∗)) and ΨA(θ∗)  we only need a single subgradient of (cid:107)θ∗(cid:107)A and some
simple additional calculations.

3.1 Gaussian width of unit norm ball
Although the atomic set A may contain uncountably many vectors  we assume that A can be de-
composed as a union of M “simple” sets  A = A1 ∪ A2 ∪ . . . ∪ AM . By “simple ” we mean the
Gaussian width of each Ai is easy to compute/bound. Such a decomposition assumption is often
satisﬁed by commonly used atomic norms  e.g.  L1  L1/L2  OWL  k-support norm. The Gaussian
width of the unit norm ball of (cid:107) · (cid:107)A can be easily obtained using the following lemma  which is
essentially the Lemma 2 in [16]. Related results appear in [16].
Lemma 2 Let M > 4  A1 ···  AM ⊂ Rp  and A = ∪mAm. The Gaussian width of unit norm
ball of (cid:107) · (cid:107)A satisﬁes

w(ΩA) = w(conv(A)) = w(A) ≤ max
1≤m≤M

w(Am) + 2 sup
z∈A

(cid:107)z(cid:107)2

(7)

(cid:112)log M

Next we illustrate application of this result to bounding the width of the unit norm ball of L1 and
OWL norm.
Example 1.1 (L1 norm): Recall that the L1 norm can be viewed as the atomic norm induced by the
set AL1 = {±ei
i=1 is the canonical basis of Rp. Since the Gaussian
width of a singleton is 0  if we treat A as the union of individual {+ei} and {−ei}  we have

: 1 ≤ i ≤ p}  where {ei}p

w(ΩL1 ) ≤ 0 + 2(cid:112)log 2p = O((cid:112)log p) .

(OWL) norm [13  25  5] deﬁned as (cid:107)θ(cid:107)owl =(cid:80)p

(8)
Example 1.2 (OWL norm): A recent variant of L1 norm is the so-called ordered weighted L1
i   where w1 ≥ w2 ≥ . . . ≥ wp ≥ 0 are
pre-speciﬁed ordered weights  and |θ|↓ is the permutation of |θ| with entries sorted in decreasing
order. In [25]  the OWL norm is proved to be an atomic norm with atomic set
Aowl =

u ∈ Rp : uS c = 0  uS =

  v ∈ {±1}p

i=1 wi|θ|↓

(cid:91)

(cid:91)

(cid:91)

Ai =

(cid:40)

(cid:41)

. (9)

vS(cid:80)i

j=1 wj

1≤i≤p

1≤i≤p

| supp(S)|=i

3

(cid:115)

(cid:115)

(cid:1) atomic vectors.
We ﬁrst apply Lemma 2 to each set Ai  and note that each Ai contains 2i(cid:0)p
(cid:17) ≤ 2
(cid:16) p
(cid:16) p
(cid:18)√
(cid:112)log p = O

where ¯w is the average of w1  . . .   wp. Then we apply the lemma again to Aowl and obtain

(cid:18)p
(cid:19)
2i(cid:80)i
(cid:112)2 + log p +

w(Ai) ≤ 0 + 2

(cid:114)
(cid:19)

((cid:80)i

j=1 wj)2

(cid:114)

j=1 wj

2 + log

2 + log

log 2i

≤

¯w

i

i

i

i

i

w(Ωowl) = w(Aowl) ≤ 2
¯w

log p
¯w

 

2
¯w

(cid:17)

 

(10)

which matches the result in [13].

3.2 Gaussian width of the intersection of tangent cone and unit sphere
In this subsection  we consider the computation of general w(CA(θ∗)). Using the deﬁnition of dual
norm  we can write (cid:107)θ∗(cid:107)A as (cid:107)θ∗(cid:107)A = sup(cid:107)u(cid:107)∗
A≤1(cid:104)u  θ∗(cid:105)  where (cid:107) · (cid:107)∗
A denotes the dual norm of
(cid:107) · (cid:107)A. The u∗ for which (cid:104)u∗  θ∗(cid:105) = (cid:107)θ∗(cid:107)A  is a subgradient of (cid:107)θ∗(cid:107)A. One can obtain u∗ by
simply solving the so-called polar operator [26] for the dual norm (cid:107) · (cid:107)∗
A 

u∗ ∈ argmax
A≤1

(cid:107)u(cid:107)∗

(cid:104)u  θ∗(cid:105) .

(11)

Based on polar operator  we start with the Lemma 3  which plays a key role in our analysis.
Lemma 3 Let u∗ be a solution to the polar operator (11)  and deﬁne the weighted L1 semi-norm

(cid:107) · (cid:107)u∗ as (cid:107)v(cid:107)u∗ =(cid:80)p

i=1 |u∗

i | · |vi|. Then the following relation holds

TA(θ∗) ⊆ Tu∗ (θ∗)  
where Tu∗ (θ∗) = cone{v ∈ Rp | (cid:107)θ∗ + v(cid:107)u∗ ≤ (cid:107)θ∗(cid:107)u∗}.
The proof of this lemma is in supplementary material. Note that the solution to (11) may not be
unique. A good criterion for choosing u∗ is to avoid zeros in u∗  as any u∗
i = 0 will lead to the
unboundedness of unit ball of (cid:107)·(cid:107)u∗  which could potentially increase the size of Tu∗ (θ∗). Next we
present the upper bound for w(CA(θ∗)).
Theorem 4 Suppose that u∗ is one of the solutions to (11)  and deﬁne the following sets 

i (cid:54)= 0} 

R = {i | u∗

i (cid:54)= 0  θ∗

i = 0} .



(cid:113)

i = 0} 

Q = {i | u∗

i (cid:54)= 0  θ∗
The Gaussian width w(CA(θ∗)) is upper bounded by
if R is empty

S = {i | u∗

√

p  

s log(cid:0) p−m

(cid:1)  

w(CA(θ∗)) ≤

 

(12)

if R is nonempty

s

min

max

κ2

m + 3

i | and κmax = maxi∈S |u∗
i |.

2 s + 2κ2
where m = |Q|  s = |S|  κmin = mini∈R |u∗
Proof: By Lemma 3  we have w(CA(θ∗)) ≤ w(Tu∗ (θ∗) ∩ Sp−1) (cid:44) w(Cu∗ (θ∗)). Hence we can
focus on bounding w(Cu∗ (θ∗)). We ﬁrst analyze the structure of v that satisﬁes (cid:107)θ∗ + v(cid:107)u∗ ≤
i = 0}  the corresponding entries vi’s can be arbitrary since
(cid:107)θ∗(cid:107)u∗. For the coordinates Q = {i | u∗
it does not affect the value of (cid:107)θ∗ + v(cid:107)u∗. Thus all possible vQ form a m-dimensional subspace 
where m = |Q|. For S ∪ R = {i | u∗
i (cid:54)= 0}  we deﬁne ˜θ = θ∗
S∪R and ˜v = vS∪R  and ˜v needs to
satisfy

(cid:107)˜v + ˜θ(cid:107)u∗ ≤ (cid:107) ˜θ(cid:107)u∗  

which is similar to the L1-norm tangent cone except that coordinates are weighted by |u∗|. Therefore
we use the techniques for proving the Proposition 3.10 in [10]. Based on the structure of v  The
normal cone at θ∗ for Tu∗ (θ∗) is given by

N (θ∗) = {z : (cid:104)z  v(cid:105) ≤ 0 ∀v s.t. (cid:107)v + θ∗(cid:107)u∗ ≤ (cid:107)θ∗(cid:107)u∗}

= {z : zi = 0 for i ∈ Q  zi = |u∗

i |sign(˜θi)t for i ∈ S  |zi| ≤ |u∗

i |t for i ∈ R  for any t ≥ 0} .

4

Given a standard Gaussian random vector g  using the relation between Gaussian width and statisti-
cal dimension (Proposition 2.4 and 10.2 in [1])  we have

w2(Cu∗ (θ∗)) ≤ E[

(cid:107)z − g(cid:107)2

zS∪R∈N (θ∗)

inf

inf

j∈S

i∈Q

g2
i +

2] = E[

(cid:88)

(cid:88)

(cid:88)
z∈N (θ∗)
j∈S
j|sign(˜θj)t − gj)2 +
(|u∗
(cid:88)
j|2 + |S| + E[
|u∗
(cid:88)
(cid:88)

(cid:32)(cid:90) +∞

|u∗
j|2 + |S| +

k|t
|zk|≤|u∗

k∈R
2√
2π
2√
2π

|u∗
j|2 + |S| +

1
|u∗
k|t

k|t
|u∗

k∈R

exp

inf

k∈R

inf

z∈N (θ∗)
= |Q| + E[

≤ |Q| + t2(cid:88)
≤ |Q| + t2(cid:88)
≤ |Q| + t2(cid:88)

j∈S

j∈S

j∈S

k∈R
(zk − gk)2]

(cid:88)

(zj − gj)2 +

(zk − gk)2]

(cid:88)

k∈R
(zk − gk)2]

(cid:33)

−g2
2

k

)dgk

(cid:18)

(gk − |u∗
−|u∗

k|t)2 exp(
(cid:19)
k|2t2
2

(∗) .

The details for the derivation above can be found in Appendix C of [10]. If R is empty  by taking
t = 0  we have

(∗) ≤ |Q| + t2(cid:88)

j∈S

|u∗
j|2 + |S| = |Q| + |S| = p .

If R is nonempty  we denote κmin = mini∈R |u∗

(cid:114)

(cid:16)|S∪R|

(cid:17)

|S|

1

κmin

2 log

  we obtain

2|R| exp
√

(cid:16)− κ2
(cid:17) ≤ |Q| +

2πκmint
2κ2
κ2

mint2
2

i | and κmax = maxi∈S |u∗
(cid:17)

(cid:18) 2κ2
(cid:19)
(cid:18)|S ∪ R|

κ2

= |Q| + |S|

max

log

i |. Taking t =
(cid:19)

(cid:18)|S ∪ R|

(cid:19)

+ 1

|S|

min
3
2

+

|S| .

|S| log

max

min

|S|

(∗) ≤ |Q| + |S|(κ2

(cid:114)

maxt2 + 1) +
|R||S|

(cid:16)|S∪R|

π log

|S|

+

|S ∪ R|

Substituting |Q| = m  |S| = s and |S ∪R| = p− m into the last inequality completes the proof.

Suppose that θ∗ is a s-sparse vector. We illustrate the above bound on the Gaussian width of the
spherical cap using L1 norm and OWL norm as examples.
Example 2.1 (L1 norm): The dual norm of L1 is L∞ norm  and its easy to verify that u∗ =
[1  1  . . .   1]T ∈ Rp is a solution to (11). Applying Theorem 4 to u∗  we have

(cid:16) p
s + s log
s
Example 2.2 (OWL norm): For OWL  its dual norm is given by (cid:107)u(cid:107)∗
owl = maxb∈Aowl(cid:104)b  u(cid:105).
W.l.o.g. we assume θ∗ = |θ∗|↓  and a solution to (11) is given by u∗ = [w1  . . .   ws  ˜w  ˜w  . . .   ˜w]T  
in which ˜w is the average of ws+1  . . .   wp. If all wi’s are nonzero  the Gaussian width satisﬁes

w(CL1(θ∗)) ≤

(cid:114) 3

(cid:18)(cid:114)

(cid:17)(cid:19)

(cid:16) p

s + 2s log

(cid:17)

= O

2

s

.

(cid:114)

w(Cowl(θ∗)) ≤

3
2

s +

2w2
1
˜w2 s log

(cid:16) p

(cid:17)

s

.

3.3 Restricted norm compatibility
The next theorem gives general upper bounds for the restricted norm compatibility ΨA(θ∗).
Theorem 5 Assume that (cid:107)u(cid:107)A ≤ max{β1(cid:107)u(cid:107)1  β2(cid:107)u(cid:107)2} for all u ∈ Rp. Under the setting of
Theorem 4  the restricted norm compatibility ΨA(θ∗) is upper bounded by

ΨA(θ∗) ≤

(cid:110)

if R is empty
β2  β1

ΦQ + max

(cid:40) Φ  
(cid:107)u(cid:107)A(cid:107)u(cid:107)2

where Φ = supu∈Rp

and ΦQ = supsupp(u)⊆Q

(cid:111)

s

(cid:16)

(cid:17)√
1 + κmax
κmin
(cid:107)u(cid:107)A(cid:107)u(cid:107)2

.

5

if R is nonempty

 

 

(13)

Proof: As analyzed in the proof of Theorem 4  vQ for v ∈ Tu∗ (θ∗) can be arbitrary  and the
vS∪R = vQc satisﬁes

Qc(cid:107)u∗ =⇒ (cid:88)
j| ≤(cid:88)
(cid:88)

|vj||u∗

i∈S

i∈S

(cid:88)

j| ≤(cid:88)

|vj||u∗

|θ∗
i + vi||u∗
|θ∗
i ||u∗

i | +
i | =⇒ κmin(cid:107)vR(cid:107)1 ≤ κmax(cid:107)vS(cid:107)1

i ||u∗
|θ∗
i |

j∈R

i∈S

(cid:107)vQc + θ∗

=⇒ (cid:88)

Qc(cid:107)u∗ ≤ (cid:107)θ∗
i | +
i | − |vi|)|u∗

(|θ∗

i∈S

j∈R
If R is empty  by Lemma 3  we obtain

ΨA(θ∗) ≤ Ψu∗ (θ∗) (cid:44) sup

v∈Tu∗ (θ∗)

(cid:107)v(cid:107)A
(cid:107)v(cid:107)2

≤ sup
v∈Rp

(cid:107)v(cid:107)A
(cid:107)v(cid:107)2

= Φ .

If R is nonempty  we have
ΨA(θ∗) ≤ Ψu∗ (θ∗) ≤ sup

v∈Tu∗ (θ∗)
(cid:107)v(cid:107)A
(cid:107)v(cid:107)2

+

≤ sup

supp(v)⊆Q

(cid:107)vQ(cid:107)A + (cid:107)vQc(cid:107)A

≤

(cid:107)v(cid:107)2

sup

supp(v(cid:48))⊆Qc
R(cid:107)1≤κmax(cid:107)v(cid:48)
β(1 + κmax
κmin
(cid:107)v(cid:48)(cid:107)2

S(cid:107)1
)(cid:107)v(cid:48)(cid:107)1

κmin(cid:107)v(cid:48)

sup

supp(v(cid:48))⊆S

≤ ΦQ + max{β2 

in which the last inequality in the ﬁrst line uses the property of Tu∗ (θ∗).

sup

supp(v)⊆Q  supp(v(cid:48))⊆Qc
κmin(cid:107)v(cid:48)
R(cid:107)1≤κmax(cid:107)v(cid:48)
S(cid:107)1
max{β1(cid:107)v(cid:48)(cid:107)1  β2(cid:107)v(cid:48)(cid:107)2}

(cid:107)v(cid:48)(cid:107)2

} ≤ ΦQ + max{β2  β1

(cid:107)v(cid:107)A + (cid:107)v(cid:48)(cid:107)A

(cid:107)v + v(cid:48)(cid:107)2

(cid:18)

1 +

κmax
κmin

(cid:19)√

s}  

Remark: We call Φ the unrestricted norm compatibility  and ΦQ the subspace norm compatibility 
both of which are often easier to compute than ΨA(θ∗). The β1 and β2 in the assumption of (cid:107) · (cid:107)A
can have multiple choices  and one has the ﬂexibility to choose the one that yields the tightest bound.
Example 3.1 (L1 norm): To apply the Theorem 5 to L1 norm  we can choose β1 = 1 and β2 = 0.
We recall the u∗ for L1 norm  whose Q is empty while R is nonempty. So we have for s-sparse θ∗

ΨL1 (θ∗) ≤ 0 + max
(cid:110)

Example 3.2 (OWL norm): For OWL  note that (cid:107) · (cid:107)owl ≤ w1(cid:107) · (cid:107)1. Hence we choose β1 = w1
and β2 = 0. As a result  we similarly have for s-sparse θ∗
w1
˜w

Ψowl(θ∗) ≤ 0 + max

0  w1

(cid:16)

1 +

√

s .

˜w

s

(cid:26)

(cid:18)

0 

1 +

1
1

s

(cid:19)√
(cid:17)√

√
= 2

(cid:27)
(cid:111) ≤ 2w2

1

s .

4 Tightness of the General Bounds

So far we have shown that the geometric measures can be upper bounded for general atomic norms.
One might wonder how tight the bounds in Section 3 are for these measures. For w(ΩA)  as the
result from [16] depends on the decomposition of A for the ease of computation  it might be tricky
to discuss its tightness in general. Hence we will focus on the other two  w(CA(θ∗)) and ΨA(θ∗).
To characterize the tightness  we need to compare the lower bounds of w(CA(θ∗)) and ΨA(θ∗) 
with their upper bounds determined by u∗. While there can be multiple u∗  it is easy to see that any
convex combination of them is also a solution to (11). Therefore we can always ﬁnd a u∗ that has
the largest support  i.e.  supp(u(cid:48)) ⊆ supp(u∗) for any other solution u(cid:48). We will use such u∗ to
generate the lower bounds. First we need the following lemma for the cone TA(θ∗).
Lemma 6 Consider a solution u∗ to (11)  which satisﬁes supp(u(cid:48)) ⊆ supp(u∗) for any other
solution u(cid:48). Under the setting of notations in Theorem 4  we deﬁne an additional set of coordinates
P = {i | u∗

i = 0}. Then the tangent cone TA(θ∗) satisﬁes

i = 0  θ∗

(14)
where ⊕ denotes the direct (Minkowski) sum operation  cl(·) denotes the closure  T1 = {v ∈
Rp | vi = 0 for i /∈ P} is a |P|-dimensional subspace  and T2 = {v ∈ Rp | sign(vi) =
−sign(θ∗

i ) for i ∈ supp(θ∗)  vi = 0 for i /∈ supp(θ∗)} is a | supp(θ∗)|-dimensional orthant.

T1 ⊕ T2 ⊆ cl(TA(θ∗))  

6

The proof of Lemma 6 is given in supplementary material. The following theorem gives us the lower
bound for w(CA(θ∗)) and ΨA(θ∗).
Theorem 7 Under the setting of Theorem 4 and Lemma 6  the following lower bounds hold 

(15)
(16)
Proof: To lower bound w(CA(θ∗))  we use Lemma 6 and the relation between Gaussian width and
statistical dimension (Proposition 10.2 in [1]) 

ΨA(θ∗) ≥ ΦQ∪S .

m + s)  

√
w(CA(θ∗)) ≥ O(

inf

z∈NT1⊕T2 (θ∗)

(cid:107)z − g(cid:107)2

2] − 1 (∗)  

w(TA(θ∗)) ≥ w(T1 ⊕ T2 ∩ Sp−1) ≥(cid:114)E[
(cid:115)
(cid:88)

(cid:88)

where the normal cone NT1⊕T2(θ∗) of T1 ⊕ T2 is given by NT1⊕T2 (θ∗) = {z : zi = 0 for i ∈
P  sign(zi) = sign(θ∗
(∗) =

(cid:114)
i ) for i ∈ supp(θ∗)}. Hence we have

| supp(θ∗)|

− 1 = O(

|P| +

m + s)  

E[

√

I{gj θ∗

j <0}] − 1 =

g2
i +

g2
j

2

i∈P

j∈supp(θ∗)

where the last equality follows the fact that P ∪ supp(θ∗) = Q ∪ S. This completes proof of (15).
To prove (16)  we again use Lemma 6 and the fact P ∪ supp(θ∗) = Q ∪ S. Noting that (cid:107) · (cid:107)A is
invariant under sign-changes  we get

ΨA(θ∗) = sup

v∈TA(θ∗)

(cid:107)v(cid:107)A
(cid:107)v(cid:107)2

≥ sup

v∈T1⊕T2

(cid:107)v(cid:107)A
(cid:107)v(cid:107)2

=

sup

supp(v)⊆P∪supp(θ∗)

(cid:107)v(cid:107)A
(cid:107)v(cid:107)2

= ΦQ∪S .

Remark: We compare the lower bounds (15) (16) with the upper bounds (12) (13). If R is empty 
m + s = p  and the lower bounds actually match the upper bounds up to a constant factor for both
w(CA(θ∗)) and ΨA(θ∗). If R is nonempty  the lower and upper bounds of w(CA(θ∗)) differ by a
)  which can be small in practice. For ΨA(θ∗)  as ΦQ∪S ≥ ΦQ 
multiplicative factor 2κ2
√
κ2
s) term in upper bound  since the assumption on (cid:107) · (cid:107)A
we usually have at most an additive O(
often holds with a constant β1 and β2 = 0 for most norms.

log( p−m

max

min

s

5 Application to the k-Support Norm

In this section  we apply our general results on geometric measures to a non-trivial example  k-
support norm [2]  which has been proved effective for sparse recovery [11  17  12]. The k-support
norm can be viewed as an atomic norm  for which A = {a ∈ Rp | (cid:107)a(cid:107)0 ≤ k  (cid:107)a(cid:107)2 ≤ 1}. The
k-support norm can be explicitly expressed as an inﬁmum convolution given by

(cid:110)(cid:88)

(cid:107)ui(cid:107)2

(cid:12)(cid:12)(cid:12) (cid:107)ui(cid:107)0 ≤ k

(cid:111)

(cid:107)θ(cid:107)sp

k = inf(cid:80)

i ui=θ

i

 

(17)

and its dual norm is the so-called 2-k symmetric gauge norm deﬁned as

(cid:107)θ(cid:107)sp∗

k = (cid:107)θ(cid:107)(k) = (cid:107)|θ|↓

1:k(cid:107)2  

(cid:115)

Lemma 2  we know the Gaussian width of the unit ball of k-support norm

(18)
It is straightforward to see that the dual norm is simply the L2 norm of the largest k entries in |θ|.
Suppose that all the sets of coordinates with cardinality k can be listed as S1 S2  . . .  S(p
k). Then A
can be written as A = A1 ∪ . . . ∪ A(p
k)  where each Ai = {a ∈ Rp | supp(a) ⊆ Si  (cid:107)a(cid:107)2 ≤ 1}.
2 ≤ √
k. Using
(cid:19)

It is not difﬁcult to see that w(Ai) = E(cid:2)supa∈Ai(cid:104)a  g(cid:105)(cid:3) = E(cid:107)gSi(cid:107)2 ≤(cid:112)E(cid:107)gSi(cid:107)2
(cid:17)
(cid:16) p
(19)
+ k = O
k
which matches that in [11]. Now we turn to the calculation of w(Csp
k (θ∗)) and Ψsp
k (θ∗). As we have
seen in the general analysis  the solution u∗ to the polar operator (11) is important in characterizing
the two quantities. We ﬁrst present a simple procedure in Algorithm 1 for solving the polar operator
for (cid:107) · (cid:107)sp∗
k . The time complexity is only O(p log p + k). This procedure can be utilized to compute
the k-support norm  or be applied to estimation with (cid:107) · (cid:107)sp∗
using generalized conditional gradient
method [26]  which requires solving the polar operator in each iteration.

(cid:18)(cid:114)

(cid:18)p

(cid:16) p

k ) ≤

(cid:114)

w(Ωsp

(cid:19)

k + 2

k + 2

(cid:17)

k log

k log

+ k

√

√

≤

log

k

k

k

 

7

k

Algorithm 1 Solving polar operator for (cid:107) · (cid:107)sp∗
Input: θ∗ ∈ Rp  positive integer k
solution u∗ to the polar operator (11)
Output:
1: z = |θ∗|↓  t = 0
2: for i = 1 to k do
3:

γ1 = (cid:107)z1:i−1(cid:107)2  γ2 = (cid:107)zi:p(cid:107)1  d = k − i + 1  β =
if γ2

2α + βγ2  u∗ = [w  β1]T

2α + βγ2 > t and β < wi−1 then
t = γ2

4:
5:
end if
6:
7: end for
8: change the sign and order of u∗ to conform with θ∗
9: return u∗

1

1

γ2√

1 d2   α =

γ2
2 d+γ2

√

2

γ1
1−β2d

  w = z1:i−1
2α

(1 is (p − i + 1)-dimensional vector with all ones)

Theorem 8 For a given θ∗  Algorithm 1 returns a solution to polar operator (11) for (cid:107) · (cid:107)sp∗
k .
The proof of this theorem is provided in supplementary material. Now we consider w(Csp
k (θ∗))
k (θ∗) for s-sparse θ∗ (here s-sparse θ∗ means | supp(θ∗)| = s) in three scenarios: (i) over-
and Ψsp
speciﬁed k  where s < k  (ii) exactly speciﬁed k  where s = k  and (iii) under-speciﬁed k  where
s > k. The bounds are given in Theorem 9  and the proof is also in supplementary material.
Theorem 9 For given s-sparse θ∗ ∈ Rp  the Gaussian width w(Csp
compatibility Ψsp

k (θ∗)) and the restricted norm



(cid:113) 2p

√

k   if s < k
2(1 + θ∗
θ∗

max

min

(cid:113) 2s

(1 + κmax
κmin

)

)   if s = k

 

k   if s > k
(20)



√

p   if s < k

k (θ∗) for a speciﬁed k are given by
(cid:114)
(cid:113) 3

(cid:1)   if s = k
(cid:1)   if s > k

s log(cid:0) p
s log(cid:0) p

2 s + 2θ∗2
θ∗2

2 s + 2κ2

max

max

min

3

s

s

κ2

min

w(Csp

k (θ∗)) ≤

  Ψsp

k (θ∗) ≤

where θ∗

max = maxi∈supp(θ∗) |θ∗

i | and θ∗

min = mini∈supp(θ∗) |θ∗
i |.
k (θ∗) is unknown and the bound on w(Csp

k (θ∗)) given in [11] is loose  as
Remark: Previously Ψsp
it used the result in [21]. Based on Theorem 9  we note that the choice of k can affect the recovery
guarantees. Over-speciﬁed k leads to a direct dependence on the dimensionality p for w(Csp
k (θ∗))
k (θ∗)  resulting in a weak error bound. The bounds are sharp for exactly speciﬁed or under-
(cid:33)
and Ψsp
speciﬁed k. Thus  it is better to under-specify k in practice. where the estimation error satiﬁes

(cid:32)(cid:114)

(cid:107) ˆθλn − θ∗(cid:107)2 ≤ O

s + s log (p/k)

n

(21)

6 Conclusions

In this work  we study the problem of structured estimation with general atomic norms that are
invariant under sign-changes. Based on Dantzig-type estimators  we provide the general bounds
for the geometric measures. In terms of w(ΩA)  instead of comparison with other results or direct
calculation  we demonstrate a third way to compute it based on decomposition of atomic set A.
For w(CA(θ∗)) and ΨA(θ∗)  we derive general upper bounds  which only require the knowledge
of a single subgradient of (cid:107)θ∗(cid:107)A. We also show that these upper bounds are close to the lower
bounds  which makes them practical in general. To illustrate our results  we discuss the application
to k-support norm in details and shed light on the choice of k in practice.

Acknowledgements
The research was supported by NSF grants IIS-1447566  IIS-1422557  CCF-1451986  CNS-
1314560  IIS-0953274  IIS-1029711  and by NASA grant NNX12AQ39A.

8

References
[1] D. Amelunxen  M. Lotz  M. B. McCoy  and J. A. Tropp. Living on the edge: Phase transitions in convex

programs with random data. Inform. Inference  3(3):224–294  2014.

[2] A. Argyriou  R. Foygel  and N. Srebro. Sparse prediction with the k-support norm. In Advances in Neural

Information Processing Systems (NIPS)  2012.

[3] A. Banerjee  S. Chen  F. Fazayeli  and V. Sivakumar. Estimation with norm regularization. In Advances

in Neural Information Processing Systems (NIPS)  2014.

[4] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. The

Annals of Statistics  37(4):1705–1732  2009.

[5] M. Bogdan  E. van den Berg  W. Su  and E. Candes. Statistical estimation and testing via the sorted L1

norm. arXiv:1310.1969  2013.

[6] T. T. Cai  T. Liang  and A. Rakhlin. Geometrizing Local Rates of Convergence for High-Dimensional

Linear Inverse Problems. arXiv:1404.4408  2014.

[7] E. Candes and T Tao. The Dantzig selector: statistical estimation when p is much larger than n. The

Annals of Statistics  35(6):2313–2351  2007.

[8] E. J. Cand`es  J. K. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate measure-

ments. Communications on Pure and Applied Mathematics  59(8):1207–1223  2006.

[9] E. J. Cands and B. Recht. Simple bounds for recovering low-complexity models. Math. Program.  141(1-

2):577–589  2013.

[10] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear inverse

problems. Foundations of Computational Mathematics  12(6):805–849  2012.

[11] S. Chatterjee  S. Chen  and A. Banerjee. Generalized dantzig selector: Application to the k-support norm.

In Advances in Neural Information Processing Systems (NIPS)  2014.

[12] S. Chen and A. Banerjee. One-bit compressed sensing with the k-support norm. In International Confer-

ence on Artiﬁcial Intelligence and Statistics (AISTATS)  2015.

[13] M. A. T. Figueiredo and R. D. Nowak. Sparse estimation with strongly correlated variables using ordered

weighted l1 regularization. arXiv:1409.4005  2014.

[14] Y. Gordon. Some inequalities for gaussian processes and applications. Israel Journal of Mathematics 

50(4):265–289  1985.

[15] L. Jacob  G. Obozinski  and J.-P. Vert. Group lasso with overlap and graph lasso.

Conference on Machine Learning (ICML)  2009.

In International

[16] A. Maurer  M. Pontil  and B. Romera-Paredes. An Inequality with Applications to Structured Sparsity

and Multitask Dictionary Learning. In Conference on Learning Theory (COLT)  2014.

[17] A. M. McDonald  M. Pontil  and D. Stamos. Spectral k-support norm regularization. In Advances in

Neural Information Processing Systems (NIPS)  2014.

[18] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for the analysis of

regularized M-estimators. Statistical Science  27(4):538–557  2012.

[19] S. Oymak  C. Thrampoulidis  and B. Hassibi. The Squared-Error of Generalized Lasso: A Precise Anal-

ysis. arXiv:1311.0830  2013.

[20] Y. Plan and R. Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A convex

programming approach. IEEE Transactions on Information Theory  59(1):482–494  2013.

[21] N. Rao  B. Recht  and R. Nowak. Universal Measurement Bounds for Structured Sparse Signal Recovery.

In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2012.

[22] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society 

Series B  58(1):267–288  1996.

[23] J. A. Tropp. Convex recovery of a structured signal from independent random linear measurements. In

Sampling Theory  a Renaissance. 2015.

[24] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society  Series B  68:49–67  2006.

[25] X. Zeng and M. A. T. Figueiredo. The Ordered Weighted (cid:96)1 Norm: Atomic Formulation  Projections  and

Algorithms. arXiv:1409.4271  2014.

[26] X. Zhang  Y. Yu  and D. Schuurmans. Polar operators for structured sparse estimation. In Advances in

Neural Information Processing Systems (NIPS)  2013.

9

,Sheng Chen
Arindam Banerjee
Murat Erdogdu
Lee Dicker
Mohsen Bayati