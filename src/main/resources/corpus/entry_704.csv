2019,BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning,We develop BatchBALD  a tractable approximation to the mutual information between a batch of points and model parameters  which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. BatchBALD is a greedy linear-time $1 - \nicefrac{1}{e}$-approximate algorithm amenable to dynamic programming and efficient caching. We compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points  sometimes performing worse than randomly acquiring data. We finish by showing that  using BatchBALD to consider dependencies within an acquisition batch  we achieve new state of the art performance on standard benchmarks  providing substantial data efficiency improvements in batch acquisition.,BatchBALD: Eﬃcient and Diverse Batch Acquisition

for Deep Bayesian Active Learning

Andreas Kirsch∗

Joost van Amersfoort∗

Yarin Gal

OATML

Department of Computer Science

University of Oxford

{andreas.kirsch  joost.van.amersfoort  yarin}@cs.ox.ac.uk

Abstract

We develop BatchBALD  a tractable approximation to the mutual information
between a batch of points and model parameters  which we use as an acquisition
function to select multiple informative points jointly for the task of deep Bayesian
active learning. BatchBALD is a greedy linear-time 1 − 1
e -approximate algorithm
amenable to dynamic programming and eﬃcient caching. We compare BatchBALD
to the commonly used approach for batch data acquisition and ﬁnd that the current
approach acquires similar and redundant points  sometimes performing worse
than randomly acquiring data. We ﬁnish by showing that  using BatchBALD to
consider dependencies within an acquisition batch  we achieve new state of the
art performance on standard benchmarks  providing substantial data eﬃciency
improvements in batch acquisition.

1

Introduction

A key problem in deep learning is data eﬃciency. While excellent performance can be obtained
with modern tools  these are often data-hungry  rendering the deployment of deep learning in the
real-world challenging for many tasks. Active learning (AL) [7] is a powerful technique for attaining
data eﬃciency. Instead of a-priori collecting and labelling a large dataset  which often comes at a
signiﬁcant expense  in AL we iteratively acquire labels from an expert only for the most informative
data points from a pool of available unlabelled data. After each acquisition step  the newly labelled
points are added to the training set  and the model is retrained. This process is repeated until a suitable
level of accuracy is achieved. The goal of AL is to minimise the amount of data that needs to be
labelled. AL has already made real-world impact in manufacturing [34]  robotics [5]  recommender
systems [1]  medical imaging [18]  and NLP [31]  motivating the need for pushing AL even further.
In AL  the informativeness of new points is assessed by an acquisition function. There are a number
of intuitive choices  such as model uncertainty and mutual information  and  in this paper  we focus
on BALD [19]  which has proven itself in the context of deep learning [13  30  20]. BALD is based
on mutual information and scores points based on how well their label would inform us about the true
model parameter distribution. In deep learning models [16  32]  we generally treat the parameters
as point estimates instead of distributions. However  Bayesian neural networks have become a
powerful alternative to traditional neural networks and do provide a distribution over their parameters.
Improvements in approximate inference [4  12] have enabled their usage for high dimensional data
such as images and in conjunction with BALD for Bayesian AL of images [13].
In practical AL applications  instead of single data points  batches of data points are acquired
during each acquisition step to reduce the number of times the model is retrained and expert-time is

∗joint ﬁrst authors

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Idealised acquisitions of BALD and
BatchBALD. If a dataset were to contain many
(near) replicas for each data point  then BALD
would select all replicas of a single informative
data point at the expense of other informative data
points  wasting data eﬃciency.

Figure 2: Performance on Repeated MNIST with
acquisition size 10. See section 4.1 for further
details. BatchBALD outperforms BALD while
BALD performs worse than random acquisition
due to the replications in the dataset.

requested. Model retraining becomes a computational bottleneck for larger models while expert time
is expensive: consider  for example  the eﬀort that goes into commissioning a medical specialist to
label a single MRI scan  then waiting until the model is retrained  and then commissioning a new
medical specialist to label the next MRI scan  and the extra amount of time this takes.
In Gal et al. [13]  batch acquisition  i.e. the acquisition of multiple points  takes the top b points
with the highest BALD acquisition score. This naive approach leads to acquiring points that are
individually very informative  but not necessarily so jointly. See ﬁgure 1 for such a batch acquisition
of BALD in which it performs poorly whereas scoring points jointly ("BatchBALD") can ﬁnd batches
of informative data points. Figure 2 shows how a dataset consisting of repeated MNIST digits (with
added Gaussian noise) leads BALD to perform worse than random acquisition while BatchBALD
sustains good performance.
Naively ﬁnding the best batch to acquire requires enumerating all possible subsets within the available
data  which is intractable as the number of potential subsets grows exponentially with the acquisition
size b and the size of available points to choose from. Instead  we develop a greedy algorithm that
selects a batch in linear time  and show that it is at worst a 1− 1/e approximation to the optimal choice
for our acquisition function. We provide an open-source implementation2.
The main contributions of this work are:

1. BatchBALD  a data-eﬃcient active learning method that acquires sets of high-dimensional

image data  leading to improved data eﬃciency and reduced total run time  section 3.1;

2. a greedy algorithm to select a batch of points eﬃciently  section 3.2; and
3. an estimator for the acquisition function that scales to larger acquisition sizes and to datasets

with many classes  section 3.3.

2 Background

2.1 Problem Setting
The Bayesian active learning setup consists of an unlabelled dataset Dpool  the current training set
Dtrain  a Bayesian model M with model parameters ωωω ∼ p(ωωω | Dtrain)  and output predictions p(y |
x  ωωω Dtrain) for data point x and prediction y ∈ {1  ...  c} in the classiﬁcation case. The conditioning of
ωωω on Dtrain expresses that the model has been trained with Dtrain. Furthermore  an oracle can provide
us with the correct label ˜y for a data point in the unlabelled pool x ∈ Dpool. The goal is to obtain a
certain level of prediction accuracy with the least amount of oracle queries. At each acquisition step 

2https://github.com/BlackHC/BatchBALD

2

BatchBALDBALD(cid:110)

(cid:111)

(cid:111)

a batch of data points
is selected using an acquisition function a which scores a candidate
batch of unlabelled data points {x1  ...  xb} ⊆ Dpool using the current model parameters p(ωωω | Dtrain):
(1)

a(cid:0){x1  ...  xb}   p(ωωω | Dtrain)(cid:1).

= arg max

b

(cid:110)

x∗
1  ...  x∗
x∗
1  ...  x∗

b

{x1 ... xb}⊆Dpool

2.2 BALD

(cid:2)H(y | x  ωωω Dtrain)(cid:3) .

BALD (Bayesian Active Learning by Disagreement) [19] uses an acquisition function that estimates
the mutual information between the model predictions and the model parameters. Intuitively  it
captures how strongly the model predictions for a given data point and the model parameters are
coupled  implying that ﬁnding out about the true label of data points with high mutual information
would also inform us about the true model parameters. Originally introduced outside the context of
deep learning  the only requirement on the model is that it is Bayesian. BALD is deﬁned as:

I(y ; ωωω | x Dtrain) = H(y | x Dtrain) − Ep(ωωω|Dtrain)

(2)
Looking at the two terms in equation (2)  for the mutual information to be high  the left term has to
be high and the right term low. The left term is the entropy of the model prediction  which is high
when the model’s prediction is uncertain. The right term is an expectation of the entropy of the model
prediction over the posterior of the model parameters and is low when the model is overall certain
for each draw of model parameters from the posterior. Both can only happen when the model has
many possible ways to explain the data  which means that the posterior draws are disagreeing among
themselves.
BALD was originally intended for acquiring individual data points and immediately retraining the
model. This becomes a bottleneck in deep learning  where retraining takes a substantial amount of
time. Applications of BALD [12  20] usually acquire the top b. This can be expressed as summing
over individual scores:

(cid:0){x1  ...  xb}   p(ωωω | Dtrain)(cid:1) =

b(cid:88)

aBALD

I(yi ; ωωω | xi Dtrain) 

(3)

and ﬁnding the optimal batch for this acquisition function using a greedy algorithm  which reduces to
picking the top b highest-scoring data points.

i=1

2.3 Bayesian Neural Networks (BNN)

In this paper we focus on BNNs as our Bayesian model because they scale well to high dimensional
inputs  such as images. Compared to regular neural networks  BNNs maintain a distribution over
their weights instead of point estimates. Performing exact inference in BNNs is intractable for any
reasonably sized model  so we resort to using a variational approximation. Similar to Gal et al. [13] 
we use MC dropout [12]  which is easy to implement  scales well to large models and datasets  and is
straightforward to optimise.

3 Methods

3.1 BatchBALD

aBatchBALD

We propose BatchBALD as an extension of BALD whereby we jointly score points by estimating the
mutual information between a joint of multiple data points and the model parameters:3

(cid:0){x1  ...  xb}   p(ωωω | Dtrain)(cid:1) = I(y1  ...  yb ; ωωω | x1  ...  xb Dtrain).

(4)
This builds on the insight that independent selection of a batch of data points leads to data ineﬃciency
as correlations between data points in an acquisition batch are not taken into account.
To understand how to compute the mutual information between a set of points and the model
parameters  we express x1  ...  xb  and y1  ...  yb through joint random variables x1:b and y1:b in a
product probability space and use the deﬁnition of the mutual information for two random variables:
(5)
3We use the notation I(x  y ; z | c) to denote the mutual information between the joint of the random variables

I(y1:b ; ωωω | x1:b Dtrain) = H(y1:b | x1:b Dtrain) − Ep(ωωω|Dtrain) H(y1:b | x1:b  ωωω Dtrain).

x  y and the random variable z conditioned on c.

3

(cid:88)

i

I(yi ; ωωω | xi Dtrain) =

(cid:88)

i

µ*(yi ∩ ωωω)

I(y1  ...  yb ; ωωω| x1  ...  xb Dtrain) = µ*

(cid:91)

i



yi ∩ ωωω

(a) BALD

(b) BatchBALD

Figure 3: Intuition behind BALD and BatchBALD using I-diagrams [36]. BALD overestimates the
joint mutual information. BatchBALD  however  takes the overlap between variables into account and
will strive to acquire a better cover of ωωω. Areas contributing to the respective score are shown in grey 
and areas that are double-counted in dark grey.

Algorithm 1: Greedy BatchBALD 1 − 1/e-approximate algorithm
Input: acquisition size b  unlabelled dataset Dpool  model parameters p(ωωω | Dtrain)
1 A0 ← ∅
2 for n ← 1 to b do
foreach x ∈ Dpool \ An−1 do sx ← aBatchBALD
xn ← arg max
sx
x∈Dpool\An−1
An ← An−1 ∪ {xn}

(cid:0)An−1 ∪ {x}   p(ωωω | Dtrain)(cid:1)

3
4

5
6 end
Output: acquisition batch An = {x1  ...  xb}

diverse ones under maximisation.

Intuitively  the mutual information between two random variables can be seen as the intersection
of their information content. In fact  Yeung [36] shows that a signed measure µ∗ can be deﬁned for
discrete random variables x  y  such that I(x ; y) = µ*(x ∩ y)  H(x  y) = µ*(x ∪ y)  Ep(y) H(x | y) =
µ*(x \ y)  and so on  where we identify random variables with their counterparts in information space 
and conveniently drop conditioning on Dtrain and xi.
i µ*(yi ∩ ωωω)  which double
counts overlaps between the yi. Naively extending BALD to the mutual information between y1  ...  yb

Using this  BALD can be viewed as the sum of individual intersections(cid:80)
and ωωω  which is equivalent to µ*(cid:0)(cid:84)
BatchBALD  on the other hand  takes overlaps into account by computing µ*(cid:0)(cid:83)
(cid:91)

I(y1  ...  yb ; ωωω | x1  ...  xb Dtrain) = H(y1:b | x1:b Dtrain) − Ep(ωωω|Dtrain) H(y1:b | x1:b  ωωω Dtrain)
= µ*
(7)
This is depicted in ﬁgure 3 and also motivates that aBatchBALD ≤ aBALD  which we prove in appendix
B.1. For acquisition size 1  BatchBALD and BALD are equivalent.

i yi ∩ ωωω(cid:1)  would lead to selecting similar data points instead of
i yi ∩ ωωω(cid:1) and is more

likely to acquire a more diverse cover under maximisation:

 = µ*

(cid:91)

 − µ*

(cid:91)

i

yi \ ωωω

yi

i



yi ∩ ωωω

i

(6)

3.2 Greedy approximation algorithm for BatchBALD

To avoid the combinatorial explosion that arises from jointly scoring subsets of points  we introduce a
greedy approximation for computing BatchBALD  depicted in algorithm 1. In appendix A  we prove
that aBatchBALD is submodular  which means the greedy algorithm is 1 − 1/e-approximate [8  24  25].

4

n(cid:88)

i=1

k

n(cid:88)

k(cid:88)

i=1

j=1

(9)

Ep(ωωω)

H(yi | ˆωωω j).

(cid:2)H(yi | ωωω)(cid:3) ≈ 1
(cid:2)p(y | ωωω)(cid:3)  and  using sampled ˆωωω j  we compute the
(cid:2)− log p(y1  ...  yn)(cid:3)
(cid:2)p(y1  ...  yn | ωωω)(cid:3)(cid:105)
(cid:104)− log Ep(ωωω)
1
 log
 .
k(cid:88)
k(cid:88)

p(ˆy1:n | ˆωωω j)

p(ˆy1:n | ˆωωω j)

(10)
(11)

(12)

k

H(y1  ...  yn) = Ep(y1 ... yn)

= Ep(ωωω) Ep(y1 ... yn|ωωω)

≈ −(cid:88)

1

k

ˆy1:n

j=1

In appendix B.2  we show that  under idealised conditions  when using BatchBALD and a ﬁxed ﬁnal
|Dtrain|  the active learning loop itself can be seen as a greedy 1 − 1/e-approximation algorithm  and
that an active learning loop with BatchBALD and acquisition size larger than 1 is bounded by an an
active learning loop with individual acquisitions  that is BALD/BatchBALD with acquisition size 1 
which is the ideal case.

3.3 Computing aBatchBALD
For brevity  we leave out conditioning on x1  ...  xn  and Dtrain  and p(ωωω) denotes p(ωωω | Dtrain) in this
section. aBatchBALD is then written as:

(cid:0){x1  ...  xn}   p(ωωω)(cid:1) = H(y1  ...  yn) − Ep(ωωω)

(8)
Because the yi are independent when conditioned on ωωω  computing the right term of equation (8)
is simpliﬁed as the conditional joint entropy decomposes into a sum. We can approximate the
expectation using a Monte-Carlo estimator with k samples from our model parameter distribution
ˆωωω j ∼ p(ωωω):

(cid:2)H(y1  ...  yn | ωωω)(cid:3) .

aBatchBALD

(cid:2)H(y1  ...  yn | ωωω)(cid:3) =

Ep(ωωω)

Computing the left term of equation (8) is diﬃcult because the unconditioned joint probability does
not factorise. Applying the equality p(y) = Ep(ωωω)
entropy by summing over all possible conﬁgurations ˆy1:n of y1:n:

j=1

(cid:32)1

k

(cid:33)

k(cid:88)

j=1

1
k

k(cid:88)

j=1

3.4 Eﬃcient implementation
In each iteration of the algorithm  x1  ...  xn−1 stay ﬁxed while xn varies over Dpool \ An−1. We can
reduce the required computations by factorizing p(y1:n | ωωω) into p(y1:n−1 | ωωω) p(yn | ωωω). We store
p(ˆy1:n−1 | ˆωωω j) in a matrix ˆP1:n−1 of shape cn−1 × k and p(yn | ˆωωω j) in a matrix ˆPn of shape c × k. The

j=1 p(ˆy1:n | ˆωωω j) in (12) can be then be turned into a matrix product:

sum(cid:80)k

p(ˆy1:n | ˆωωω j) =

1
k

p(ˆy1:n−1 | ˆωωω j) p(ˆyn | ˆωωω j) =

ˆP1:n−1 ˆPT
n

.

ˆy1:n−1 ˆyn

(13)

This can be further sped up by using batch matrix multiplication to compute the joint entropy for
diﬀerent xn. ˆP1:n−1 only has to be computed once  and we can recursively compute ˆP1:n using ˆP1:n−1
and ˆPn  which allows us to sample p(y | ˆωωω j) for each x ∈ Dpool only once at the beginning of the
algorithm.
For larger acquisition sizes  we use m MC samples of y1:n−1 as enumerating all possible conﬁgurations
becomes infeasible. See appendix C for details.
Monte-Carlo sampling bounds the time complexity of the full BatchBALD algorithm to O(bc ·
min{cb  m} · |Dpool| · k) compared to O(cb · |Dpool|b · k) for naively ﬁnding the exact optimal batch and
O((b + k) · |Dpool|) for BALD4.

4 Experiments

In our experiments  we start by showing how a naive application of the BALD algorithm to an image
dataset can lead to poor results in a dataset with many (near) duplicate data points  and show that
BatchBALD solves this problem in a grounded way while obtaining favourable results (ﬁgure 2).

4b is the acquisition size  c is the number of classes  k is the number of MC dropout samples  and m is the

number of sampled conﬁgurations of y1:n−1.

5

(a) BALD

(b) BatchBALD

Figure 4: Performance on MNIST for increasing acquisition sizes. BALD’s performance drops
drastically as the acquisition size increases. BatchBALD maintains strong performance even with
increasing acquisition size.

We then illustrate BatchBALD’s eﬀectiveness on standard AL datasets: MNIST and EMNIST.
EMNIST [6] is an extension of MNIST that also includes letters  for a total of 47 classes  and has a
twice as large training set. See appendix F for examples of the dataset. We show that BatchBALD
provides a substantial performance improvement in these scenarios  too  and has more diverse
acquisitions. Finally  we look at BatchBALD in the setting of transfer learning  where we ﬁnetune a
large pretrained model on a more diﬃcult dataset called CINIC-10 [9]  which is a combination of
CIFAR-10 and downscaled ImageNet.
In our experiments  we repeatedly go through active learning loops. One active learning loop consists
of training the model on the available labelled data and subsequently acquiring new data points using
a chosen acquisition function. As the labelled dataset is small in the beginning  it is important to
avoid overﬁtting. We do this by using early stopping after 3 epochs of declining accuracy on the
validation set. We pick the model with the highest validation accuracy. Throughout our experiments 
we use the Adam [22] optimiser with learning rate 0.001 and betas 0.9/0.999. All our results report
the median of 6 trials  with lower and upper quartiles. We use these quartiles to draw the ﬁlled error
bars on our ﬁgures.
We reinitialize the model after each acquisition  similar to Gal et al. [13]. We found this helps
the model improve even when very small batches are acquired. It also decorrelates subsequent
acquisitions as ﬁnal model performance is dependent on a particular initialization [10].
When computing p(y| x  ωωω Dtrain)  it is important to keep the dropout masks in MC dropout consistent
while sampling from the model. This is necessary to capture dependencies between the inputs for
BatchBALD  and it makes the scores for diﬀerent points more comparable by removing this source
of noise. We do not keep the masks ﬁxed when computing BALD scores because its performance
usually beneﬁts from the added noise. We also do not need to keep these masks ﬁxed for training and
evaluating the model.
In all our experiments  we either compute joint entropies exactly by enumerating all conﬁgurations 
or we estimate them using 10 000 MC samples  picking whichever method is faster. In practice  we
compute joint entropies exactly for roughly the ﬁrst 4 data points in an acquisition batch and use MC
sampling thereafter.

4.1 Repeated MNIST

As demonstrated in the introduction  naively applying BALD to a dataset that contains many (near)
replicated data points leads to poor performance. We show how this manifests in practice by taking
the MNIST dataset and replicating each data point in the training set two times (obtaining a training
set that is three times larger than the original). After normalising the dataset  we add isotropic
Gaussian noise with a standard deviation of 0.1 to simulate slight diﬀerences between the duplicated
data points in the training set. All results are obtained using an acquisition size of 10 and 10 MC

6

Figure 5: Performance on MNIST. BatchBALD
outperforms BALD with acquisition size 10 and
performs close to the optimum of acquisition size
1.

Figure 6: Relative total time on MNIST. Normal-
ized to training BatchBALD with acquisition size
10 to 95% accuracy. The stars mark when 95%
accuracy is reached for each method.

Table 1: Number of required data points on MNIST until 90% and 95% accuracy are reached. 25%- 
50%- and 75%-quartiles for the number of required data points when available.

90% accuracy
BatchBALD 70 / 90 / 110
BALD 6
BALD [13]

120 / 120 / 170
145

95% accuracy
190 / 200 / 230
250 / 250 / >300
335

dropout samples. The initial dataset was constructed by taking a balanced set of 20 data points5  two
of each class (similar to [13]).
Our model consists of two blocks of [convolution  dropout  max-pooling  relu]  with 32 and 64 5x5
convolution ﬁlters. These blocks are followed by a two-layer MLP that includes dropout between the
layers and has 128 and 10 hidden units. The dropout probability is 0.5 in all three locations. This
architecture achieves 99% accuracy with 10 MC dropout samples during test time on the full MNIST
dataset.
The results can be seen in ﬁgure 2. In this illustrative scenario  BALD performs poorly  and even
randomly acquiring points performs better. However  BatchBALD is able to cope with the replication
perfectly. In appendix D  we look at varying the repetition number and show that as we increase
the number of repetitions BALD gradually performs worse. In appendix E  we also compare with
Variation Ratios [11]  and Mean STD [21] which perform on par with random acquisition.

4.2 MNIST

For the second experiment  we follow the setup of Gal et al. [13] and perform AL on the MNIST
dataset using 100 MC dropout samples. We use the same model architecture and initial dataset as
described in section 4.1. Due to diﬀerences in model architecture  hyper parameters and model
retraining  we signiﬁcantly outperform the original results in Gal et al. [13] as shown in table 1.
We ﬁrst look at BALD for increasing acquisition size in ﬁgure 4a. As we increase the acquisition size
from the ideal of acquiring points individually and fully retraining after each points (acquisition size
1) to 40  there is a substantial performance drop.
BatchBALD  in ﬁgure 4b  is able to maintain performance when doubling the acquisition size from 5
to 10. Performance drops only slightly at 40  possibly due to estimator noise.
The results for acquisition size 10 for both BALD and BatchBALD are compared in ﬁgure 5.
BatchBALD outperforms BALD. Indeed  BatchBALD with acquisition size 10 performs close to the
ideal with acquisition size 1. The total run time of training these three models until 95% accuracy is

5These initial data points were chosen by running BALD 6 times with the initial dataset picked randomly and

choosing the set of the median model. They were subsequently held ﬁxed.

6reimplementation using reported experimental setup

7

Figure 7: Performance on EMNIST. BatchBALD
consistently outperforms both random acquisition
and BALD while BALD is unable to beat random
acquisition.

Figure 8: Entropy of acquired class labels over ac-
quisition steps on EMNIST. BatchBALD steadily
acquires a more diverse set of data points.

visualized in ﬁgure 6  where we see that BatchBALD with acquisition size 10 is much faster than
BALD with acquisition size 1  and only marginally slower than BALD with acquisition size 10.

4.3 EMNIST

In this experiment  we show that BatchBALD also provides a signiﬁcant improvement when we
consider the more diﬃcult EMNIST dataset [6] in the Balanced setup  which consists of 47 classes 
comprising letters and digits. The training set consists of 112 800 28x28 images balanced by class  of
which the last 18 800 images constitute the validation set. We do not use an initial dataset and instead
perform the initial acquisition step with the randomly initialized model. We use 10 MC dropout
samples.
We use a similar model architecture as before  but with added capacity. Three blocks of [convolution 
dropout  max-pooling  relu]  with 32  64 and 128 3x3 convolution ﬁlters  and 2x2 max pooling. These
blocks are followed by a two-layer MLP with 512 and 47 hidden units  with again a dropout layer in
between. We use dropout probability 0.5 throughout the model.
The results for acquisition size 5 can be seen in ﬁgure 7. BatchBALD outperforms both random
acquisition and BALD while BALD is unable to beat random acquisition. Figure 8 gives some insight
into why BatchBALD performs better than BALD. The entropy of the categorical distribution of
acquired class labels is consistently higher  meaning that BatchBALD acquires a more diverse set
of data points. In ﬁgure 15  the classes on the x-axis are sorted by number of data points that were
acquired of that class. We see that BALD undersamples classes while BatchBALD is more consistent.

4.4 CINIC-10

CINIC-10 is an interesting dataset because it is large
(270k data points) and its data comes from two diﬀer-
ent sources: CIFAR-10 and ImageNet. To get strong
performance on the test set it is important to obtain
data from both sets. Instead of training a very deep
model from scratch on a small dataset  we opt to run
this experiment in a transfer learning setting  where
we use a pretrained model and acquire data only to
ﬁnetune the original model. This is common prac-
tice and suitable in cases where data is abound for
an auxiliary domain  but is expensive to label for the
domain of interest.
For the CINIC-10 experiment  we use 160k training
samples for the unlabelled pool  20k validation sam-
ples  and the other 90k as test samples. We use an

8

Figure 9: Performance on CINIC-10. Batch-
BALD outperforms BALD from 500 acquired
samples onwards.

ImageNet pretrained VGG-16  provided by PyTorch [26]  with a dropout layer before a 512 hidden
unit (instead of 4096) fully connected layer. We use 50 MC dropout samples  acquisition size 10 and
repeat the experiment for 6 trials. The results are in ﬁgure 9  with the 59% mark reached at 1170 for
BatchBALD and 1330 for BALD (median).

5 Related work

AL is closely related to Bayesian Optimisation (BO)  which is concerned with ﬁnding the global
optimum of a function [33]  with the fewest number of function evaluations. This is generally done
using a Gaussian Process. A common problem in BO is the lack of parallelism  with usually a single
worker being responsible for function evaluations. In real-world settings  there are usually many
such workers available and making optimal use of them is an open problem [14  2] with some work
exploring mutual information for optimising a multi-objective problem [17].
Maintaining diversity when acquiring a batch of data has also been attempted using constrained
optimisation [15] and in Gaussian Mixture Models [3]. In AL of molecular data  the lack of diversity
in batches of data points acquired using the BALD objective has been noted by Janz et al. [20]  who
propose to resolve it by limiting the number of MC dropout samples and relying on noisy estimates.
A related approach to AL is semi-supervised learning (also sometimes referred to as weakly-
supervised)  in which the labelled data is commonly assumed to be ﬁxed and the unlabelled data is
used for unsupervised learning [23  27]. Wang et al. [35]  Sener and Savarese [29]  Samarth Sinha
[28] explore combining it with AL.

6 Scope and limitations

Unbalanced datasets BALD and BatchBALD do not work well when the test set is unbalanced as
they aim to learn well about all classes and do not follow the density of the dataset. However  if the
test set is balanced  but the training set is not  we expect BatchBALD to perform well.
Unlabelled data BatchBALD does not take into account any information from the unlabelled
dataset. However  BatchBALD uses the underlying Bayesian model for estimating uncertainty for
unlabelled data points  and semi-supervised learning could improve these estimates by providing
more information about the underlying structure of the feature space. We leave a semi-supervised
extension of BatchBALD to future work.
Noisy estimator A signiﬁcant amount of noise is introduced by MC-dropout’s variational approxi-
mation to training BNNs. Sampling of the joint entropies introduces additional noise. The quality of
larger acquisition batches would be improved by reducing this noise.

7 Conclusion

We have introduced a new batch acquisition function  BatchBALD  for Deep Bayesian Active
Learning  and a greedy algorithm that selects good candidate batches compared to the intractable
optimal solution. Acquisitions show increased diversity of data points and improved performance
over BALD and other methods.
While our method comes with additional computational cost during acquisition  BatchBALD is able
to signiﬁcantly reduce the number of data points that need to be labelled and the number of times
the model has to be retrained  potentially saving considerable costs and ﬁlling an important gap in
practical Deep Bayesian Active Learning.

9

Acknowledgements

The authors want to thank Binxin (Robin) Ru for helpful references to submodularity and the
appropriate proofs. We would also like to thank the rest of OATML for their feedback at several
stages of the project. AK is supported by the UK EPSRC CDT in Autonomous Intelligent Machines
and Systems (grant reference EP/L015897/1). JvA is grateful for funding by the EPSRC (grant
reference EP/N509711/1) and Google-DeepMind. Funding for computational resources was provided
by the Allan Turing Institute and Google.

Author contributions

AK derived the original estimator  proved submodularity and bounds  implemented BatchBALD
eﬃciently  and ran the experiments. JvA developed the narrative and experimental design  advised on
debugging  structured the paper into its current form  and pushed it forward at diﬃcult times. JvA
and AK wrote the paper jointly.

10

References
[1] Gediminas Adomavicius and Alexander Tuzhilin. Toward the next generation of recommender
IEEE Transactions on

systems: A survey of the state-of-the-art and possible extensions.
Knowledge & Data Engineering  2005.

[2] Ahsan S Alvi  Binxin Ru  Jan Calliess  Stephen J Roberts  and Michael A Osborne. Asyn-
chronous batch Bayesian optimisation with improved local penalisation. arXiv preprint
arXiv:1901.10452  2019.

[3] Javad Azimi  Alan Fern  Xiaoli Zhang-Fern  Glencora Borradaile  and Brent Heeringa. Batch

active learning via coordinated matching. arXiv preprint arXiv:1206.6458  2012.

[4] Charles Blundell  Julien Cornebise  Koray Kavukcuoglu  and Daan Wierstra. Weight uncertainty
in neural network. In Proceedings of the 32nd International Conference on Machine Learning 
Proceedings of Machine Learning Research  pages 1613–1622  2015.

[5] Sylvain Calinon  Florent Guenter  and Aude Billard. On learning  representing  and generalizing
a task in a humanoid robot. IEEE Transactions on Systems  Man  and Cybernetics  Part B
(Cybernetics)  37(2):286–298  2007.

[6] Gregory Cohen  Saeed Afshar  Jonathan Tapson  and André van Schaik. Emnist: Extending
In 2017 International Joint Conference on Neural Networks

mnist to handwritten letters.
(IJCNN)  pages 2921–2926. IEEE  2017.

[7] David A Cohn  Zoubin Ghahramani  and Michael I Jordan. Active learning with statistical

models. Journal of artiﬁcial intelligence research  4:129–145  1996.

[8] Nguyen Viet Cuong  Wee Sun Lee  Nan Ye  Kian Ming A Chai  and Hai Leong Chieu. Active
learning for probabilistic hypotheses using the maximum gibbs error criterion. In Advances in
Neural Information Processing Systems  pages 1457–1465  2013.

[9] Luke N Darlow  Elliot J Crowley  Antreas Antoniou  and Amos J Storkey. Cinic-10 is not

imagenet or cifar-10. arXiv preprint arXiv:1810.03505  2018.

[10] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse  trainable

neural networks. In International Conference on Learning Representations  2019.

[11] Linton C Freeman. Elementary applied statistics: for students in behavioral science. John

Wiley & Sons  1965.

[12] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning. In international conference on machine learning  pages
1050–1059  2016.

[13] Yarin Gal  Riashat Islam  and Zoubin Ghahramani. Deep Bayesian active learning with image
data. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 1183–1192. JMLR. org  2017.

[14] Javier González  Zhenwen Dai  Philipp Hennig  and Neil Lawrence. Batch Bayesian optimiza-

tion via local penalization. In Artiﬁcial Intelligence and Statistics  pages 648–657  2016.

[15] Yuhong Guo and Dale Schuurmans. Discriminative batch mode active learning. In Advances in

neural information processing systems  pages 593–600  2008.

[16] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[17] Daniel Hernández-Lobato  Jose Hernandez-Lobato  Amar Shah  and Ryan Adams. Predictive
entropy search for multi-objective Bayesian optimization. In International Conference on
Machine Learning  pages 1492–1501  2016.

11

[18] Steven CH Hoi  Rong Jin  Jianke Zhu  and Michael R Lyu. Batch mode active learning and its
application to medical image classiﬁcation. In Proceedings of the 23rd international conference
on Machine learning  pages 417–424. ACM  2006.

[19] Neil Houlsby  Ferenc Huszár  Zoubin Ghahramani  and Máté Lengyel. Bayesian active learning

for classiﬁcation and preference learning. arXiv preprint arXiv:1112.5745  2011.

[20] David Janz  Jos van der Westhuizen  and José Miguel Hernández-Lobato. Actively learning

what makes a discrete sequence valid. arXiv preprint arXiv:1708.04465  2017.

[21] Alex Kendall  Vijay Badrinarayanan  and Roberto Cipolla. Bayesian segnet: Model uncertainty
in deep convolutional encoder-decoder architectures for scene understanding. arXiv preprint
arXiv:1511.02680  2015.

[22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[23] Durk P Kingma  Shakir Mohamed  Danilo Jimenez Rezende  and Max Welling. Semi-supervised
learning with deep generative models. In Advances in neural information processing systems 
pages 3581–3589  2014.

[24] Andreas Krause  Ajit Singh  and Carlos Guestrin. Near-optimal sensor placements in Gaussian
processes: Theory  eﬃcient algorithms and empirical studies. Journal of Machine Learning
Research  9(Feb):235–284  2008.

[25] George L Nemhauser  Laurence A Wolsey  and Marshall L Fisher. An analysis of approximations
for maximizing submodular set functions—i. Mathematical programming  14(1):265–294 
1978.

[26] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic diﬀerentiation in
PyTorch. In NIPS Autodiﬀ Workshop  2017.

[27] Antti Rasmus  Mathias Berglund  Mikko Honkala  Harri Valpola  and Tapani Raiko. Semi-
supervised learning with ladder networks. In Advances in neural information processing systems 
pages 3546–3554  2015.

[28] Trevor Darrell Samarth Sinha  Sayna Ebrahimi. Variational adversarial active learning. arXiv

preprint arXiv:1904.00370  2019.

[29] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set

approach. In International Conference on Learning Representations  2018.

[30] Yanyao Shen  Hyokun Yun  Zachary C. Lipton  Yakov Kronrod  and Animashree Anandkumar.
Deep active learning for named entity recognition. In International Conference on Learning
Representations  2018.

[31] Aditya Siddhant and Zachary C Lipton. Deep Bayesian active learning for natural language
processing: Results of a large-scale empirical study. arXiv preprint arXiv:1808.05697  2018.
[32] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image

recognition. In International Conference on Learning Representations  2015.

[33] Jasper Snoek  Hugo Larochelle  and Ryan P Adams. Practical Bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems  pages 2951–2959 
2012.

[34] Simon Tong. Active learning: theory and applications  volume 1. Stanford University USA 

2001.

[35] Keze Wang  Dongyu Zhang  Ya Li  Ruimao Zhang  and Liang Lin. Cost-eﬀective active learning
for deep image classiﬁcation. IEEE Transactions on Circuits and Systems for Video Technology 
27(12):2591–2600  2017.

[36] Raymond W Yeung. A new outlook on shannon’s information measures. IEEE transactions on

information theory  37(3):466–474  1991.

12

,Andreas Kirsch
Joost van Amersfoort
Yarin Gal