2019,Procrastinating with Confidence: Near-Optimal  Anytime  Adaptive Algorithm Configuration,Algorithm configuration methods optimize the performance of a parameterized heuristic algorithm on a given distribution of problem instances. Recent work introduced an algorithm configuration procedure (``Structured Procrastination'') that provably achieves near optimal performance with high probability and with nearly minimal runtime in the worst case. It also offers an anytime property: it keeps tightening its optimality guarantees the longer it is run. Unfortunately  Structured Procrastination is not adaptive to characteristics of the parameterized algorithm: it treats every input like the worst case. Follow-up work (``LeapsAndBounds'') achieves adaptivity but trades away the anytime property. This paper introduces a new algorithm  ``Structured Procrastination with Confidence''  that preserves the near-optimality and anytime properties of Structured Procrastination while adding adaptivity. In particular  the new algorithm will perform dramatically faster in settings where many algorithm configurations perform poorly. We show empirically both that such settings arise frequently in practice and that the anytime property is useful for finding good configurations quickly.,Procrastinating with Conﬁdence: Near-Optimal 

Anytime  Adaptive Algorithm Conﬁguration

Robert Kleinberg

Department of Computer Science

Cornell University

rdk@cs.cornell.edu

Brendan Lucier
Microsoft Research

brlucier@microsoft.com

Kevin Leyton-Brown

Department of Computer Science
University of British Columbia

kevinlb@cs.ubc.ca

Devon Graham

Department of Computer Science
University of British Columbia

drgraham@cs.ubc.ca

Abstract

Algorithm conﬁguration methods optimize the performance of a parameterized
heuristic algorithm on a given distribution of problem instances. Recent work
introduced an algorithm conﬁguration procedure (“Structured Procrastination”)
that provably achieves near optimal performance with high probability and with
nearly minimal runtime in the worst case. It also offers an anytime property: it keeps
tightening its optimality guarantees the longer it is run. Unfortunately  Structured
Procrastination is not adaptive to characteristics of the parameterized algorithm:
it treats every input like the worst case. Follow-up work (“LeapsAndBounds”)
achieves adaptivity but trades away the anytime property. This paper introduces a
new algorithm  “Structured Procrastination with Conﬁdence”  that preserves the
near-optimality and anytime properties of Structured Procrastination while adding
adaptivity. In particular  the new algorithm will perform dramatically faster in
settings where many algorithm conﬁgurations perform poorly. We show empirically
both that such settings arise frequently in practice and that the anytime property is
useful for ﬁnding good conﬁgurations quickly.

1

Introduction

Algorithm conﬁguration is the task of searching a space of conﬁgurations of a given algorithm
(typically represented as joint assignments to a set of algorithm parameters) in order to ﬁnd a single
conﬁguration that optimizes a performance objective on a given distribution of inputs. In this paper 
we focus exclusively on the objective of minimizing average runtime. Considerable progress has
recently been made on solving this problem in practice via general-purpose  heuristic techniques such
as ParamILS (Hutter et al.  2007  2009)  GGA (Ans´otegui et al.  2009  2015)  irace (Birattari et al. 
2002; L´opez-Ib´a˜nez et al.  2011) and SMAC (Hutter et al.  2011a b). Notably  in the context of this
paper  all these methods are adaptive: they surpass their worst-case performance when presented
with “easier” search problems.
Recently  algorithm conﬁguration has also begun to attract theoretical analysis. While there is a
large body of less-closely related work that we survey in Section 1.3  the ﬁrst nontrivial worst-case
performance guarantees for general algorithm conﬁguration with an average runtime minimization
objective were achieved by a recently introduced algorithm called Structured Procrastination (SP)
(Kleinberg et al.  2017). This work considered a worst-case setting in which an adversary causes every
deterministic choice to play out as poorly as possible  but where observations of random variables are

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

unbiased samples. It is straightforward to argue that  in this setting  any ﬁxed  deterministic heuristic
for searching the space of conﬁgurations can be extremely unhelpful. The work therefore focuses
on obtaining candidate conﬁgurations via random sampling (rather than  e.g.  following gradients or
taking the advice of a response surface model). Besides its use of heuristics  SMAC also devotes half
its runtime to random sampling. Any method based on random sampling will eventually encounter
the optimal conﬁguration; the crucial question is the amount of time that this will take. The key result
of Kleinberg et al. (2017) is that SP is guaranteed to ﬁnd a near-optimal conﬁguration with high
probability  with worst-case running time that nearly matches a lower bound on what is possible and
that asymptotically dominates that of existing alternatives such as SMAC.
Unfortunately  there is a ﬂy in the ointment: SP turns out to be impractical in many cases  taking an
extremely long time to run even on inputs that existing methods ﬁnd easy. At the root  the issue is
that SP treats every instance like the worst case  in which it is necessary to achieve a ﬁne-grained
understanding of every conﬁguration’s runtime in order to distinguish between them. For example  if
every conﬁguration is very similar but most are not quite "-optimal  subtle performance differences
must be identiﬁed. SP thus runs every conﬁguration enough times that with high probability the
conﬁguration’s runtime can accurately be estimated to within a 1 + " factor.

1.1 LEAPSANDBOUNDS and CAPSANDRUNS
Weisz et al. (2018b) introduced a new algorithm  LEAPSANDBOUNDS (LB)  that improves upon
Structured Procrastination in several ways. First  LB improves upon SP’s worst-case performance 
matching its information-theoretic lower bound on running time by eliminating a log factor. Second 
LB does not require the user to specify a runtime cap that they would never be willing to exceed on any
run  replacing this term in the analysis with the runtime of the optimal conﬁguration  which is typically
much smaller. Third  and most relevant to our work here  LB includes an adaptive mechanism  which
takes advantage of the fact that when a conﬁguration exhibits low variance across instances  its
performance can be estimated accurately with a smaller number of samples. However  the easiest
algorithm conﬁguration problems are probably those in which a few conﬁgurations are much faster
on average than all other conﬁgurations. (Empirically  many algorithm conﬁguration instances exhibit
just such non-worst-case behaviour; see our empirical investigation in the Supplementary Materials.)
In such cases  it is clearly unnecessary to obtain high-precision estimates of each bad conﬁguration’s
runtime; instead  we only need to separate these conﬁgurations’ runtimes from that of the best
alternative. LB offers no explicit mechanism for doing this. LB also has a key disadvantage when
compared to SP: it is not anytime  but instead must be given ﬁxed values of " and . Because LB is
adaptive  there is no way for a user to anticipate the amount of time that will be required to prove
("  )-optimality  forcing a tradeoff between the risks of wasting available compute resources and of
having to terminate LB before it returns an answer.
CAPSANDRUNS (CR) is a reﬁnement of LB that was developed concurrently with the current paper;
it has not been formally published  but was presented at an ICML 2018 workshop (Weisz et al.  2018a).
CR maintains all of the beneﬁts of LB  and furthermore introduces a second adaptive mechanism that
does exploit variation in conﬁgurations’ mean runtimes. Like LB  it is not anytime.

1.2 Our Contributions
Our main contribution is a reﬁned version of SP that maintains the anytime property while aiming
to observe only as many samples as necessary to separate the runtime of each conﬁguration from
that of the best alternative. We call it “Structured Procrastination with Conﬁdence” (SPC). SPC
differs from SP in that it maintains a novel form of lower conﬁdence bound as an indicator of the
quality of a particular conﬁguration  while SP simply uses that conﬁguration’s sample mean. The
consequence is that SPC spends much less time running poorly performing conﬁgurations  as other
conﬁgurations quickly appear better and receive more attention. We initialize each lower bound with a
trivial value: each conﬁguration’s runtime is bounded below by the fastest possible runtime  0. SPC
then repeatedly evaluates the conﬁguration that has the most promising lower bound.1 We perform
1While both SPC and CR use conﬁdence bounds to guide search  they take different approaches. Rather
than rejecting conﬁgurations whose lower bounds get too large  SPC focuses on conﬁgurations with small lower
bounds. By allocating a greater proportion of total runtime to such promising conﬁgurations we both improve
the bounds for conﬁgurations about which we are more uncertain and allot more resources to conﬁgurations
with relatively low mean runtimes about which we are more conﬁdent.

2

these runs by “capping” (censoring) runs at progressively doubling multiples of 0. If a run does
not complete  SPC “procrastinates”  deferring it until it has exhausted all runs with shorter captimes.
Eventually  SPC observes enough completed runs of some conﬁguration to obtain a nontrivial upper
bound on its runtime. At this point  it is able to start drawing high-probability conclusions that other
conﬁgurations are worse.
Our paper is focused on a theoretical analysis of SPC. We show that it identiﬁes an approximately
optimal conﬁguration using running time that is nearly the best possible in the worst case; however 
so does SP. The key difference  and the subject of our main theorem  is that SPC also exhibits
near-minimal runtime beyond the worst case  in the following sense. Deﬁne an ("  )-suboptimal
conﬁguration to be one whose average runtime exceeds that of the optimal conﬁguration by a factor
of more than 1 + "  even when the suboptimal conﬁguration’s runs are capped so that a  fraction of
them fail to ﬁnish within the time limit. A straightforward information-theoretic argument shows that
in order to verify that a conﬁguration is ("  )-suboptimal it is sufﬁcient—and may also be necessary 
in the worst case—to run it for O("2 · 1 · OPT) time. The running time of SPC matches (up to
logarithmic factors) the running time of a hypothetical “optimality veriﬁcation procedure” that knows
the identity of the optimal conﬁguration  and for each suboptimal conﬁguration i knows a pair ("i  i)
such that i is ("i  i)-suboptimal and the product "2
SPC is anytime in the sense that it ﬁrst identiﬁes an ("  )-optimal conﬁguration for large values of
" and  and then continues to reﬁne these values as long as it is allowed to run. This is helpful for
users who have difﬁculty setting these parameters up front  as already discussed. SPC’s strategy for
progressing iteratively through smaller and smaller values of " and  also has another advantage: it is
actually faster than starting with the “ﬁnal” values of " and  and applying them to each conﬁguration.
This is because extremely weak conﬁgurations can be dismissed cheaply based on large ("  ) values 
instead of taking more samples to estimate their runtimes more ﬁnely.

is as small as possible.

i

· 1

i

1.3 Other Related Work

There is a large body of related work in the multi-armed bandits literature  which does not attack quite
the same problem but does similarly leverage the “optimism in the face of uncertainty” paradigm
and many tools of analysis (Lai & Robbins  1985; Auer et al.  2002; Bubeck et al.  2012). We do
not survey this work in detail as we have little to add to the extensive discussion by Kleinberg et al.
(2017)  but we brieﬂy identify some dominant threads in that work. Perhaps the greatest contact
between the communities has occurred in the sphere of hyperparameter optimization (Bergstra et al. 
2011; Thornton et al.  2013; Li et al.  2016) and in the literature on bandits with correlated arms
that scale to large experimental design settings (Kleinberg  2006; Kleinberg et al.  2008; Chaudhuri
et al.  2009; Bubeck et al.  2011; Srinivas et al.  2012; Cesa-Bianchi & Lugosi  2012; Munos  2014;
Shahriari et al.  2016). In most of this literature  all arms have the same  ﬁxed cost; others (Guha
& Munagala  2007; Tran-Thanh et al.  2012; Badanidiyuru et al.  2013) consider a model where
costs are variable but always paid in full. (Conversely  in algorithm conﬁguration we can stop runs
that exceed a captime  yielding a potentially censored sample at bounded cost.) Some inﬂuential
departures from this paradigm include Kandasamy et al. (2016)  Ganchev et al. (2010)  and most
notably Li et al. (2016); reasons why these methods are nevertheless inappropriate for use in the
algorithm conﬁguration setting are discussed at length by Kleinberg et al. (2017).
Recent work has examined the learning-theoretic foundations of algorithm conﬁguration  inspired
in part by an inﬂuential paper of Gupta & Roughgarden (2017) that framed algorithm conﬁguration
and algorithm selection in terms of learning theory. This vein of work has not aimed at a general-
purpose algorithm conﬁguration procedure  as we do here  but has rather sought sample-efﬁcient 
special-purpose algorithms for particular classes of problems  including combinatorial partitioning
problems (clustering  max-cut  etc) (Balcan et al.  2017)  branching strategies in tree search (Balcan
et al.  2018b)  and various algorithm selection problems (Balcan et al.  2018a). Nevertheless  this
vein of work takes a perspective similar to our own and demonstrates that algorithm conﬁguration has
moved decisively from being solely the province of heuristic methods to being a topic for rigorous
theoretical study.

3

2 Model

We deﬁne an algorithm conﬁguration problem by the 4-tuple (N    R  0)  where these elements are
deﬁned as follows. N is a family of (potentially randomized) algorithms  which we call conﬁgurations
to suggest that a single piece of code instantiates each algorithm under a different parameter setting.
We do not assume that different conﬁgurations exhibit any sort of performance correlations  and can
so capture the case of n distinct algorithms by imagining a “master algorithm” with a single  n-valued
categorical parameter. Parameters are allowed to take continuous values: |N| can be uncountable.
We typically use i to index conﬁgurations.  is a probability distribution over input instances. When
the instance distribution is given implicitly by a ﬁnite benchmark set  let  be the uniform distribution
over this set. We typically use j to index (input instance  random seed) pairs  to which we will
hereafter refer simply as instances. R(i  j) is the execution time when conﬁguration i 2 N is run on
input instance j. Given some value of ✓> 0  we deﬁne R(i  j  ✓) = min{R(i  j) ✓ }  the runtime
capped at ✓. 0 > 0 is a constant such that R(i  j)  0 for all conﬁgurations i and inputs j.
For any timeout threshold ✓  let R✓(i) = Ej⇠[R(i  j  ✓)] denote the average ✓-capped running time
of conﬁguration i  over input distribution . Fixing some running time ¯ = 20 that we will never
be willing to exceed  the quantity R¯(i) corresponds to the expected running time of conﬁguration i
and will be denoted simply by R(i). We will write OP T = mini R(i). Given ✏> 0  a goal is to ﬁnd
i⇤ 2 N such that R(i⇤)  (1 + ✏)OP T . We also consider a relaxed objective  where the running
time of i⇤ is capped at some threshold value ✓ for some small fraction of (instance  seed) pairs .
Deﬁnition 2.1. A conﬁguration i⇤ is (✏  )-optimal if there exists some threshold ✓ such that R✓(i⇤) 
(1 + ✏)OP T   and Prj⇠R(i⇤  j) >✓  . Otherwise  we say i⇤ is (✏  )-suboptimal.

3 Structured Procrastination with Conﬁdence

In this section we present and analyze our algorithm conﬁguration procedure  which is based on the
“Structured Procrastination” principle introduced in Kleinberg et al. (2017). We call the procedure
SPC (Structured Procrastination with Conﬁdence) because  compared with the original Structured
Procrastination algorithm  the main innovation is that instead of approximating the running time of

each conﬁguration by taking eO(1/"2) samples for some "  it approximates it using a lower conﬁdence

bound that becomes progressively tighter as the number of samples increases. We focus on the case
where N  the set of all conﬁgurations  is ﬁnite and can be iterated over explicitly. Our main result for
this case is given as Theorem 3.4. In Section 4 we extend SPC to handle large or inﬁnite spaces of
conﬁgurations where full enumeration is impossible or impractical.

3.1 Description of the algorithm

The algorithm is best described in terms of two components: a “thread pool” of subroutines called
conﬁguration testers  each tasked with testing one particular conﬁguration  and a scheduler that
controls the allocation of time to the different conﬁguration testers. Because the algorithm is structured
in this way  it lends itself well to parallelization  but in this section we will present and analyze it as a
sequential algorithm.
Each conﬁguration tester provides  at all times  a lower conﬁdence bound (LCB) on the average
running time of its conﬁguration. The rule for computing the LCB will be speciﬁed below; it is
designed so that (with probability tending to 1 as time goes on) the LCB is less than or equal to the true
average running time. The scheduler runs a main loop whose iterations are numbered t = 1  2  . . ..
In each iteration t  it polls all of the conﬁguration testers for their LCBs  selects the one with the
minimum LCB  and passes control to that conﬁguration tester. The loop iteration ends when the tester
passes control back to the scheduler. SPC is an anytime algorithm  so the scheduler’s main loop is
inﬁnite; if it is prompted to return a candidate conﬁguration at any time  the algorithm will poll each
conﬁguration tester for its “score” (described below) and then output the conﬁguration whose tester
reported the maximum score.
The way each conﬁguration tester i operates is best visualized as follows. There is an inﬁnite stream
of i.i.d. random instances j1  j2  . . . that the tester processes. Each of them is either completed 
pending (meaning we ran the conﬁguration on that instance at least once  but it timed out before
completing)  or inactive. An instance that is completed or pending will be called active. Conﬁguration

4

// Main loop. Run until interrupted.

// GetLCB() returns LCB as described in the text.

Algorithm 1: Structured Procrastination w/ Conﬁdence

require :Set N of n algorithm conﬁgurations
require :Lower bound on runtime  0
// Initializations
1 t := 0
2 for i 2 N do

Ci := new Conﬁguration Tester for i
Ci.Initialize()

5 repeat
i := arg mini2N Ci.GetLCB()
6
Ci.ExecuteStep()
7
8 until anytime search is interrupted
9 return i⇤ = arg maxi2N {Ci.GetNumActive()}
// Conﬁguration Testing Controller.
10 Class ConfigurationTester()

tester i maintains state variables ✓i and ri such that the following invariants are satisﬁed at all times:
(1) the ﬁrst ri instances in the stream are active and the rest are inactive; (2) the number of pending
instances is at most q = q(ri  t) = 50 log(t log ri); (3) every pending instance has been attempted
with timeout ✓i  and no instance has been attempted with timeout greater than 2✓i. To maintain
these invariants  conﬁguration tester i maintains a queue of pending instances  each with a timeout
parameter representing the timeout threshold to be used the next time the conﬁguration attempts
to solve the instance. When the scheduler passes control to conﬁguration tester i  it either runs the
pending instance at the head of its queue (if the queue has q(ri  t) elements) or it selects an inactive
instance from the head of the i.i.d. stream and runs it with timeout threshold ✓i. In both cases  if the
run exceeds its timeout  it is reinserted into the back of the queue with the timeout threshold doubled.
At any time  if conﬁguration tester
i is asked to return a score (for the
purpose of selecting a candidate opti-
mal conﬁguration) it simply outputs
ri  the number of active instances.
The logic justifying this choice of
score function is that the scheduler
devotes more time to promising con-
ﬁgurations than to those that appear
suboptimal; furthermore  better con-
ﬁgurations run faster on average and
so complete a greater number of runs.
This dual tendency of near-optimal
conﬁguration testers to be allocated
a greater amount of running time and
to complete a greater number of runs
per unit time makes the number of
active instances a strong indicator of
the quality of a conﬁguration  as we
formalize in the analysis.
We must ﬁnally specify how conﬁg-
uration tester i computes its lower
conﬁdence bound on R(i); see Fig-
ure 1 for an illustration. Recall that
the conﬁguration tester has a state
variable ✓i and that for every ac-
tive instance j  the value R(i  j  ✓i)
is already known because i has ei-
ther completed instance j  or it has
attempted instance j with timeout
threshold ✓i. Given some iteration of
the algorithm  deﬁne G to be the em-
pirical cumulative distribution func-
tion (CDF) of R(i  j  ✓i) as j ranges
over all the active instances. A natu-
ral estimation of R✓i(i) would be the
expectation of this empirical distribu-
0 (1  G(x))dx. Our lower
bound will be the expectation of a
modiﬁed CDF  found by scaling G
non-uniformly toward 1. To formally
describe the modiﬁcation we require
some deﬁnitions. Here and through-
out this paper  we use the notation
log(·) to denote the base-2 logarithm
and ln(·) to denote the natural loga-
rithm. Let ✏(k  r  t) =q 9·2k ln(kt)
.

require :Sequence j1  j2  . . . of instances
require :Global iteration counter  t
Procedure Initialize()
r := 0  ✓ := 0  q = 1
Q := empty double-ended queue

if RUN(i  j` ✓ ) terminates in time ⌧  ✓ then
else

Ri`✓ := ⌧

Remove (`  ✓0) from head of Q
✓ := ✓0

t := t + 1
if |Q| < q then
r := r + 1
` := r

Ri`✓ := ✓
Insert (`  2✓) at tail of Q

q := d25 log(t log r)e

Procedure GetNumActive()

return r

3
4

11
12
13

14
15
16
17
18
19
20
21

22
23
24
25
26

27

28
29

Procedure ExecuteStep()

// Replenish queue

tion R 1

else

r

5

For 0 < p < 1 let

(p  r  t) =(

p

1+✏(blog(1/p)c r t)
0

if ✏(blog(1/p)c  r  t)  1/2
otherwise.

(1)

Given a function G : [0 1) ! [0  1]  we let
L(G  r  t) =Z 1

0

(1  G(x)  r  t) dx.

The conﬁguration tester’s lower conﬁdence bound is L(Gi  ri  t)  where t is the current iteration  ri
is the number of active instances  and Gi is the empirical CDF of R(i  j  ✓i).
To interpret this deﬁnition and Equation (1)  think of p as the value of 1  G(x) for some x  and
(p  r  t)  p as a scaled-down version of p. The scaling factor we use  (1 + ✏(k  r  t))  depends on
the value of p; speciﬁcally  it increases with k = blog(1/p)c. In other words  we scale G(x) more
aggressively as G(x) gets closer to 1. If p is too small as a function of r and t  then we give up
on scaling it and instead set it all the way to (p  r  t) = 0. To see this  note that for k such that
2k  p < 21k  if k is large enough then we will have that ✏(k  r  t) > 1/2 so the second case of
Equation (1) applies.
We also note that L(Gi  ri  t) can be explicitly computed. Observe that Gi(x) is actually a step
function with at most ri steps and that Gi(x) = 1 for x >✓ i  so the integral deﬁning L(Gi  ri  t)
is actually a ﬁnite sum that can be computed in O(ri) time  given a sorted list of the elements of
{R(i  j  ✓i) | j active}. Example 3.1 illustrates the gains SPC can offer over SP.
Example 3.1. Suppose that there are two conﬁgurations: one that takes 100ms on every input and
another that takes 1000ms. With 0 = 1ms  ✏ = 0.01  and ⇣ = 0.1  SP will set the initial queue size
of each conﬁguration to be at least2 7500  because the queue size is initialized with a value that is at
least 12✏2 ln(3n/⇣). It will run each conﬁguration 7500 times with a timeout of 1ms  then it will
run each of them 7500 times with a timeout of 2ms  then 4ms  and so on  until it reaches 128ms. At
that point it exceeds 100ms  so the ﬁrst conﬁguration will solve all instances in its queue. However 
for the ﬁrst 2· 7500· (1 + 2 + 4 +··· + 64) = 1.9⇥ 106 milliseconds of running the algorithm—more
than half an hour—essentially nothing happens: SP obtains no evidence of the superiority of the ﬁrst
conﬁguration.
In contrast  SPC maintians more modest queue sizes  and thus runs each conﬁguration on fewer
instances before running them with a timeout of 128ms  at which point it can distinguish between
the two. During the ﬁrst 5000 iterations of SPC  the size of each conﬁguration’s instance queue is
at most 400. This is because ri  t  and t  5000  so qi  25 log(5000 log(5000)) < 400. Further 
observe that 5000 iterations is sufﬁcient for SPC to attempt to run both conﬁgurations on some
instance with a cutoff of 128ms  since each conﬁguration will ﬁrst run at most 400 instances with
cutoff 1ms  then at most 400 instances with cutoff 2ms  and so on. Continuing up to 64ms  for both
conﬁgurations  takes a total of 2 · log(64) · 400 = 4800 < 5000 iterations. Thus  it takes at most
2· 400· (1 + 2 + 4 + ... + 64) = 101  600 milliseconds (less than two minutes) before SPC runs each
conﬁguration on some instance with cutoff time 128ms. We see that SPC requires signiﬁcantly less
time—in this example  almost a factor of 20 less—to reach the point where it can distinguish between
the two conﬁgurations.

Justiﬁcation of lower conﬁdence bound

3.2
In this section we will show that for any conﬁguration i and any iteration t  with probability 1 
O(t5/4) the inequality L(Gi  ri  t)  R(i) holds. Let Fi denote the cumulative distribution function
of the running time of conﬁguration i. Then R(i) = R 1
0 1  Fi(x) dx  so in order to prove that
L(Gi  ri  t)  R(i) with high probability it sufﬁces to prove that  with high probability  for all x
the inequality (1  Gi(x)  ri  t)  1  Fi(x) holds. To do so we will apply a multiplicative error
estimate from empirical process theory due to Wellner (1978). This error estimate can be used to
derive the following error bound in our setting.
Lemma 3.2. Let x1  . . .   xn be independent random samples from a distribution with cumulative
distribution function F   and G their empirical CDF. For 0  b  1  x  0  and 0  "  1/2
2The exact queue size depends on the number of active instances  but this bound sufﬁces for our example.

6

Figure1:Anillustrationofhowwecomputethelowerboundonaconﬁgu-ration’saverageruntime.Thedistribu-tionofagivenconﬁguration’struerun-timeisF(x);theempiricalCDF G(x) constitutesobservationssampledfromF(x)andcensoredat✓.Theconﬁgura-tion’sexpectedruntime thequantitywewanttoestimate isthe(blue)shadedregionabovecurveF(x).Ourhigh-probabilitylowerboundonthisquantityisthe(green)areaaboveG(x) scaledtowards1asdescribedinEquation(1).Runtime0✓Probabilityofsolvinganinstance01/23/47/81F(x)G(x)deﬁnetheeventsE1(b x)={1G(x)b}andE2(✏ x)=1G(x)1+">1F(x) .ThenwehavePr(9xs.t.E1(b x)andE2(✏ x))exp(14"2nb).TojustifytheuseofL(Gi ri t)asalowerconﬁdenceboundonR(i) weapplyLemma3.2withb=2k n=rand"="(k r t).Withtheseparameters 14"2nb=94ln(kt) hencethelemmaimpliesthefollowingforallk r t:Pr9xs.t.E1(2k x)andE2("(k r t) x)(kt)9/4.(2)TheinequalityisusedinthefollowingpropositiontoshowthatL(Gi ri t)isalowerboundonR(i)withhighprobability.Lemma3.3.Foreachconﬁgurationtester i andeachloopiterationt Pr(9xs.t.(1Gi(x) ri t)>1Fi(x))=O(t5/4).(3)ConsequentlyPr(L(Gi ri t)>R(i))=O(t5/4).3.3RunningtimeanalysisSinceSPCspendslesstimerunningbadconﬁgurations weareabletoshowanimprovedruntimeboundoverSP.Supposethatiis(" )-suboptimal.Weboundtheexpectedamountoftimedevotedtorunningiduringtheﬁrsttloopiterations.WeshowthatthisquantityisO("21log(tlog(1/))).Summingover(" )-suboptimalconﬁgurationsyieldsourmainresult whichisthatAlgorithm1isextremelyunlikelytoreturnan(✏ )-suboptimalconﬁgurationonceitsruntimeexceedstheaverageruntimeofthebestconﬁgurationbyagivenfactor.WriteB(t " )="21log(tlog(1/)).Theorem3.4.Fix"andandletSbethesetof(" )-optimalconﬁgurations.Foreachi62Ssupposethatiis("i i)-suboptimal with"i"andi.ThenifthetimespentrunningSPCis⌦✓R(i⇤)✓|S|·B(t " )+Xi62SB(t "i i)◆◆ wherei⇤denotesanoptimalconﬁguration thenSPCwillreturnan(" )-optimalconﬁgurationwhenitisterminated withhighprobabilityint.RatherthanhavinganadditiveO(✏21)termforeachofnconﬁgurationsconsidered(asisthecasewithSP) theboundinTheorem3.4hasatermoftheformO(✏2i1i) foreachconﬁgurationithatisnot(✏ )-optimal where✏2i1iisassmallaspossible.Thiscanbeasigniﬁcantimprovementincaseswheremanyconﬁgurationsbeingconsideredarefarfrombeing(✏ )-optimal.ToproveTheorem3.4 wewillmakeuseofthefollowinglemma whichboundsthetimespentrunningconﬁgurationiintermsofitslowerconﬁdenceboundandnumberofactiveinstances.Lemma3.5.Atanytime iftheconﬁgurationtesterforconﬁgurationihasriactiveinstancesandlowerconﬁdenceboundLi thenthetotalamountofrunningtimethathasbeenspentrunningconﬁgurationiisatmost9riLi.Theintuitionisthatbecauseexecutiontimeoutsaresuccessivelydoubled thetotaltimespentrunningonagiveninputinstancejisnotmuchmorethanthetimeofthemostrecentexecutiononj.Butif7i

i 1

i

0 (1  Gi(x)) dx.

log(t log(1/i))) where i⇤ denotes an optimal conﬁguration.

we take an average over all active j  the total time spent on the most recent runs is precisely r times
the average runtime under the empirical CDF. The result then follows from the following lemma 
Lemma 3.6  which shows that Li is at least a constant times this empirical average runtime.
Lemma 3.6. At any iteration t  if the conﬁguration tester for conﬁguration i has ri active instances
3R ✓i
and Gi is the empirical CDF for R(i  j  ✓i)  then L(Gi  ri  t)  2
Given Lemma 3.5  it sufﬁces to argue that a sufﬁciently suboptimal conﬁguration will have few active
instances. This is captured by the following lemma.
Lemma 3.7. If conﬁguration i is ("i  i)-suboptimal then at any iteration t  the expected number
of active instances for conﬁguration tester i is bounded by O("2
log(t log(1/i))) and the
expected amount of time spent running conﬁguration i on those instances is bounded by O(R(i⇤) ·
i 1
"2
Intuitively  Lemma 3.7 follows because in order for the algorithm to select a suboptimal conﬁguration
i  it must be that the lower bound for i is less than the lower bound for an optimal conﬁguration.
Since the lower bounds are valid with high probability  this can only happen if the lower bound
for conﬁguration i is not yet very tight. Indeed  it must be signiﬁcantly less than R(i) for some
threshold  with Prj(R(i  j) > )  i. However  the lower bound cannot remain this loose for
long: once the threshold ✓ gets large enough relative to   and we take sufﬁciently many samples as a
function of ✏i and i  standard concentration bounds will imply that the empirical CDF (and hence our
lower bound) will approximate the true runtime distribution over the range [0  ]. Once this happens 
the lower bound will exceed the average runtime of the optimal distribution  and conﬁguration i will
stop receiving time from the scheduler.
Lemma 3.7 also gives us a way of determining ✏ and  from an empirical run of SPC. If SPC returns
conﬁguration i at time t  then by Lemma 3.7 i will not be (✏  )-suboptimal for any ✏ and  for which
ri =⌦( ✏21 log(t log(1/)))  where ri is the number of active instances for i at termination time.
Thus  given a choice of ✏ and the value of ri at termination  one can solve to determine a  for which
i is guaranteed to be (✏  )-optimal. See Appendix E for further details.
Given Lemma 3.7  Theorem 3.4 follows from a straightforward counting argument; see Appendix B.

4 Handling Many Conﬁgurations

Algorithm 1 assumes a ﬁxed set N of n possible conﬁgurations. In practice  these conﬁgurations are
often determined by the settings of dozens or even hundreds of parameters  some of which might
have continuous domains. In these cases  it is not practical for the search procedure to take time
proportional to the number of all possible conﬁgurations. However  like Structured Procrastination 
the SPC procedure can be modiﬁed to handle such cases. What follows is a brief discussion; due to
space constraints  the details are provided in the supplementary material.
The ﬁrst idea is to sample a set ˆN of n conﬁgurations from the large (or inﬁnite) pool  and run
Algorithm 1 on the sampled set. This yields an (✏  )-optimality guarantee with respect to the best
conﬁguration in ˆN. Assuming the samples are representative  this corresponds to the top (1/n)’th
quantile of runtimes over all conﬁgurations. We can then imagine running instances of SPC in
parallel with successively doubled sample sizes  appropriately weighted  so that we make progress on
estimating the top (1/2k)’th quantile simultaneously for each k. This ultimately leads to an extension
of Theorem 3.4 in which  for any > 0  one obtains a conﬁguration that is (✏  )-optimal with respect
to OPT  the top -quantile of conﬁguration runtimes. This method is anytime  and the time required
for a given ✏    and  is (up to log factors) OPT · 1
 times the expected minimum time needed to
determine whether a randomly chosen conﬁguration is (✏  )-suboptimal relative to OPT.

5 Experimental Results

We experiment3 with SPC on the benchmark set of runtimes generated by Weisz et al. (2018b) for
testing LEAPSANDBOUNDS. This data consists of pre-computed runtimes for 972 conﬁgurations

3Code to reproduce experiments is available at https://github.com/drgrhm/alg_config

8

Figure 2: Mean runtimes for
solutions returned by SPC af-
ter various amounts of compute
time (blue line)  and for those
returned by LB for different ✏  
pairs (red points). For LB  each
point represents a different ✏  
combination. Its size represents
the value of ✏  and its color in-
tensity represents the value of .
SPC is able to ﬁnd a good solu-
tion relatively quickly. Different
✏   pairs can lead to drastically
different runtimes  while still re-
turning the same conﬁguration.
The x-axis is in log scale.

of the minisat (Sorensson & Een  2005) SAT solver on 20118 SAT instances generated using
CNFuzzDD4. A key difference between SPC and LB is the former’s anytime guarantee: unlike with
LB  users need not choose values of ✏ or  in advance. Our experiments investigate the impact of
this property. To avoid conﬂating the results with effects due to restarts and their interaction with the
multiplier of ✓  all the times we considered were for the non-resuming simulated environment.
Figure 2 compares the solutions returned by SPC after various amounts of CPU compute time with
those of LB and SP for different ✏   pairs chosen from a grid with ✏ 2 [0.1  0.9] and  2 [0.1  0.5].
The x-axis measures CPU time in days  and the y-axis shows the expected runtime of the solution
returned (capping at the dataset’s max cap of 900s). The blue line shows the result of SPC over time.
The red points show the result of LB for different ✏   pairs  and the green points show this result for
SP. The size of each point is proportional to ✏  while the color is proportional to .
We draw two main conclusions from Figure 2. First  SPC was able to ﬁnd a reasonable solution after
a much smaller amount of compute time than LB. After only about 10 CPU days  SPC identiﬁed a
conﬁguration that was in the top 1% of all conﬁgurations in terms of max-capped runtime  while runs
of LB took at least 100 CPU days for every ✏   combination we considered. Second  choosing a
good ✏   combination for LB was not easy. One might expect that big  dark points would appear at
shorter runtimes  while smaller  lighter ones would appear at higher runtimes. However  this was not
the case. Instead  we see that different ✏   pairs led to drastically different total runtimes  often while
still returning the same conﬁguration. Conversely  SPC lets the user completely avoid this problem.
It settles on a fairly good conﬁguration after about 100 CPU days. If the user has a few hundred more
CPU days to spare  they can continue to run SPC and eventually obtain the best solution reached by
LB  and then to the dataset’s true optimal value after about 525 CPU days. However  even at this time
scale many ✏   pairs led to worse conﬁgurations being returned by LB than SPC.

6 Conclusion

We have presented Structured Procrastination with Conﬁdence  an approximately optimal procedure
for algorithm conﬁguration. SPC is an anytime algorithm that uses a novel lower conﬁdence bound
to select conﬁgurations to explore  rather than a sample mean. As a result  SPC adapts to problem
instances in which it is easier to discard poorly-performing conﬁgurations. We are thus able to show
an improved runtime bound for SPC over SP  while maintaining the anytime property of SP.
We compare SPC to other conﬁguration procedures on a simple benchmark set of SAT solver runtimes 
and show that SPC’s anytime property can be helpful in ﬁnding good conﬁgurations  especially early
on in the search process. However  a more comprehensive empirical investigation is needed  in
particular in the setting of many conﬁgurations. Such large-scale experiments will be a signiﬁcant
engineering challenge  and we leave this avenue to future work.

4http://fmv.jku.at/cnfuzzdd/

9

References
Ans´otegui  C.  Sellmann  M.  and Tierney  K. A gender-based genetic algorithm for automatic
conﬁguration of algorithms. In Principles and Practice of Constraint Programming (CP)  pp.
142–157  2009.

Ans´otegui  C.  Malitsky  Y.  Sellmann  M.  and Tierney  K. Model-based genetic algorithms for
algorithm conﬁguration. In International Joint Conference on Artiﬁcial Intelligence (IJCAI)  pp.
733–739  2015.

Auer  P.  Cesa-Bianchi  N.  and Fischer  P. Finite-time analysis of the multiarmed bandit problem.

Machine learning  47(2-3):235–256  2002.

Badanidiyuru  A.  Kleinberg  R.  and Slivkins  A. Bandits with knapsacks.

Computer Science (FOCS)  pp. 207–216  2013.

In Foundations of

Balcan  M.  Dick  T.  and Vitercik  E. Dispersion for data-driven algorithm design  online learning 

and private optimization. In Proc. IEEE FOCS  pp. 603–614  2018a.

Balcan  M.-F.  Nagarajan  V.  Vitercik  E.  and White  C. Learning-theoretic foundations of algorithm
conﬁguration for combinatorial partitioning problems. In Conference on Learning Theory  pp.
213–274  2017.

Balcan  M.-F.  Dick  T.  Sandholm  T.  and Vitercik  E. Learning to branch. International Conference

on Machine Learning  2018b.

Bergstra  J. S.  Bardenet  R.  Bengio  Y.  and K´egl  B. Algorithms for hyper-parameter optimization.

In Advances in Neural Information Processing Systems (NIPS)  pp. 2546–2554  2011.

Birattari  M.  Sttzle  T.  Paquete  L.  and Varrentrapp  K. A racing algorithm for conﬁguring
metaheuristics. In Genetic and Evolutionary Computation Conference (GECCO)  pp. 11–18  2002.
Bubeck  S.  Munos  R.  Stoltz  G.  and Szepesv´ari  C. X-armed bandits. Journal of Machine Learning

Research  12(May):1655–1695  2011.

Bubeck  S.  Cesa-Bianchi  N.  et al. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

Cesa-Bianchi  N. and Lugosi  G. Combinatorial bandits. Journal of Computer and System Sciences 

78(5):1404–1422  2012.

Chaudhuri  K.  Freund  Y.  and Hsu  D. J. A parameter-free hedging algorithm. In Advances in

Neural Information Processing Systems (NIPS)  pp. 297–305  2009.

Ganchev  K.  Nevmyvaka  Y.  Kearns  M.  and Vaughan  J. W. Censored exploration and the dark

pool problem. Communications of the ACM  53(5):99–107  2010.

Guha  S. and Munagala  K. Approximation algorithms for budgeted learning problems. In ACM

Symposium on Theory of Computing (STOC)  pp. 104–113  2007.

Gupta  R. and Roughgarden  T. A PAC approach to application-speciﬁc algorithm selection. SIAM

Journal on Computing  46(3):992–1017  2017.

Hutter  F.  Hoos  H.  and St¨utzle  T. Automatic algorithm conﬁguration based on local search. In

AAAI Conference on Artiﬁcial Intelligence  pp. 1152–1157  2007.

Hutter  F.  Hoos  H.  Leyton-Brown  K.  and St¨utzle  T. ParamILS: An automatic algorithm conﬁgura-

tion framework. Journal of Artiﬁcial Intelligence Research  36:267–306  2009.

Hutter  F.  Hoos  H.  and Leyton-Brown  K. Bayesian optimization with censored response data.
In NIPS workshop on Bayesian Optimization  Sequential Experimental Design  and Bandits
(BayesOpt’11)  2011a.

Hutter  F.  Hoos  H.  and Leyton-Brown  K. Sequential model-based optimization for general
algorithm conﬁguration. In Conference on Learning and Intelligent Optimization (LION)  pp.
507–523  2011b.

10

Hutter  F.  Xu  L.  Hoos  H. H.  and Leyton-Brown  K. Algorithm runtime prediction: Methods &

evaluation. Artiﬁcial Intelligence  206:79–111  2014.

Kandasamy  K.  Dasarathy  G.  Poczos  B.  and Schneider  J. The multi-ﬁdelity multi-armed bandit.

In Advances in Neural Information Processing Systems (NIPS)  pp. 1777–1785  2016.

Kleinberg  R. Anytime algorithms for multi-armed bandit problems. In ACM-SIAM Symposium on

Discrete Algorithms (SODA)  pp. 928–936  2006.

Kleinberg  R.  Slivkins  A.  and Upfal  E. Multi-armed bandits in metric spaces. In ACM Symposium

on Theory of Computing  pp. 681–690  2008.

Kleinberg  R.  Leyton-Brown  K.  and Lucier  B. Efﬁciency through procrastination: Approximately
optimal algorithm conﬁguration with runtime guarantees. In Proceedings of the 26th International
Joint Conference on Artiﬁcial Intelligence (IJCAI)  2017.

Lai  T. L. and Robbins  H. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied

Mathematics  6(1):4–22  1985.

Li  L.  Jamieson  K.  DeSalvo  G.  Rostamizadeh  A.  and Talwalkar  A. Hyperband: A novel
bandit-based approach to hyperparameter optimization. arXiv preprint arXiv:1603.06560  2016.
L´opez-Ib´a˜nez  M.  Dubois-Lacoste  J.  St¨utzle  T.  and Birattari  M. The irace package  iterated race
for automatic algorithm conﬁguration. Technical report  IRIDIA  Universit´e Libre de Bruxelles 
2011. URL http://iridia.ulb.ac.be/IridiaTrSeries/IridiaTr2011-004.pdf.

Munos  R. From bandits to Monte-Carlo tree search: The optimistic principle applied to optimization

and planning. Foundations and Trends in Machine Learning  7(1):1–129  2014.

Shahriari  B.  Swersky  K.  Wang  Z.  Adams  R. P.  and de Freitas  N. Taking the human out of the

loop: A review of Bayesian optimization. Proceedings of the IEEE  104(1):148–175  2016.

Sorensson  N. and Een  N. Minisat v1. 13-a sat solver with conﬂict-clause minimization. SAT  2005

(53):1–2  2005.

Srinivas  N.  Krause  A.  Kakade  S. M.  and Seeger  M. W. Information-theoretic regret bounds for
Gaussian process optimization in the bandit setting. IEEE Transactions on Information Theory  58
(5):3250–3265  2012.

Thornton  C.  Hutter  F.  Hoos  H.  and Leyton-Brown  K. Auto-WEKA: Combined selection and
hyperparameter optimization of classiﬁcation algorithms. In Conference on Knowledge Discovery
and Data mining (KDD)  pp. 847–855  2013.

Tran-Thanh  L.  Chapman  A.  Rogers  A.  and Jennings  N. R. Knapsack based optimal policies for

budget–limited multi–armed bandits. In AAAI Conference on Artiﬁcial Intelligence  2012.

Weisz  G.  Gy¨ogy  A.  and Szepesv´ari  C. CAPSANDRUNS: An improved method for approximately

optimal algorithm conﬁguration. ICML 2018 AutoML Workshop  2018a.

Weisz  G.  Gy¨ogy  A.  and Szepesv´ari  C. LEAPSANDBOUNDS: A method for approximately optimal
algorithm conﬁguration. In International Conference on Machine Learning  pp. 5254–5262  2018b.
Wellner  J. A. Limit theorems for the ratio of the empirical distribution function to the true distribution

function. Z. Wahrscheinlichkeitstheorie verw. Gebeite  45:73–88  1978.

11

,Robert Kleinberg
Kevin Leyton-Brown
Brendan Lucier
Devon Graham