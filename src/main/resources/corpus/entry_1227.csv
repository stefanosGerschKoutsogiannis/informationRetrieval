2019,Correlation Clustering with Adaptive Similarity Queries,In correlation clustering  we are given $n$ objects together with a binary similarity score between each pair of them.
The goal is to partition the objects into clusters so to minimise the disagreements with the scores.
In this work we investigate correlation clustering as an active learning problem: each similarity score can be learned by making a query  and the goal is to minimise both the disagreements and the total number of queries.
On the one hand  we describe simple active learning algorithms  which provably achieve an almost optimal trade-off while giving cluster recovery guarantees  and we test them on different datasets.
On the other hand  we prove information-theoretical bounds on the number of queries necessary to guarantee a prescribed disagreement bound.
These results give a rich characterization of the trade-off between queries and clustering error.,Correlation Clustering

with Adaptive Similarity Queries

Marco Bressan

Department of Computer Science

University of Rome Sapienza

Andrea Paudice

Department of Computer Science

Università degli Studi di Milano & IIT

Nicolò Cesa-Bianchi

Department of Computer Science & DSRC

Università degli Studi di Milano

Fabio Vitale

Department of Computer Science

University of Lille & Inria

Abstract

In correlation clustering  we are given n objects together with a binary similarity
score between each pair of them. The goal is to partition the objects into clusters
so to minimise the disagreements with the scores. In this work we investigate
correlation clustering as an active learning problem: each similarity score can be
learned by making a query  and the goal is to minimise both the disagreements
and the total number of queries. On the one hand  we describe simple active
learning algorithms  which provably achieve an almost optimal trade-off while
giving cluster recovery guarantees  and we test them on different datasets. On the
other hand  we prove information-theoretical bounds on the number of queries
necessary to guarantee a prescribed disagreement bound. These results give a rich
characterization of the trade-off between queries and clustering error.

1

Introduction

Clustering is a central problem in unsupervised learning. A clustering problem is typically represented
by a set of elements together with a notion of similarity (or dissimilarity) between them. When the
elements are points in a metric space  dissimilarity can be measured via a distance function. In more
general settings  when the elements to be clustered are members of an abstract set V   similarity is
deﬁned by an arbitrary symmetric function σ deﬁned on pairs of distinct elements in V . Correlation
Clustering (CC) [4] is a well-known special case where σ is a {−1  +1}-valued function establishing
whether any two distinct elements of V are similar or not. The objective of CC is to cluster the points
in V so to maximize the correlation with σ. More precisely  CC seeks a clustering minimizing the
number of errors  where an error is given by any pair of elements having similarity −1 and belonging
to the same cluster  or having similarity +1 and belonging to different clusters. Importantly  there
are no a priori limitations on the number of clusters or their sizes: all partitions of V   including
the trivial ones  are valid. Given V and σ  the error achieved by an optimal clustering is known as
the Correlation Clustering index  denoted by OPT. A convenient way of representing σ is through
a graph G = (V  E) where {u  v} ∈ E iff σ(u  v) = +1. Note that OPT = 0 is equivalent to a
perfectly clusterable graph (i.e.  G is the union of disjoint cliques). Since its introduction  CC has
attracted a lot of interest in the machine learning community  and has found numerous applications in
entity resolution [16]  image analysis [18]  and social media analysis [25]. Known problems in data
integration [14] and biology [5] can be cast into the framework of CC [26].
From a machine learning viewpoint  we are interested in settings when the similarity function σ is
not available beforehand  and the algorithm must learn σ by querying for its value on pairs of objects.
This setting is motivated by scenarios in which the similarity information is costly to obtain. For

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

example  in entity resolution  disambiguating between two entities may require invoking the user’s
help. Similarly  deciding if two documents are similar may require a complex computation  and
possibly the interaction with human experts. In these active learning settings  the learner’s goal is to
trade the clustering error against the number of queries. Hence  the fundamental question is: how
many queries are needed to achieve a speciﬁed clustering error? Or  in other terms  how close can we
get to OPT  under a prescribed query budget Q?

1.1 Our Contributions

In this work we characterize the trade-off between the number Q of queries and the clustering error
on n points. The table below here summarizes our bounds in the context of previous work. Running
time and upper/lower bounds on the expected clustering error are expressed in terms of the number of
queries Q  and all our upper bounds assume Q = Ω(n) while our lower bounds assume Q = O(n2).

Running time
Q + LP solver + rounding
Q
Exponential
Exponential (OPT = 0)
Unrestricted (OPT = 0)
Unrestricted (OPT (cid:29) 0)

Q(cid:1)

Expected clustering error
√

3OPT + O(n3/Q)
√

3(ln n + 1)OPT + O(cid:0)n5/2/
Q(cid:1)
OPT + O(cid:0)n5/2/
(cid:101)O(cid:0)n3/Q(cid:1)
Q(cid:1)
Ω(cid:0)n2/
OPT + Ω(cid:0)n3/Q(cid:1)

√

Reference

[7]

Theorem 1 (see also [6])

Theorem 7
Theorem 7
Theorem 8
Theorem 9

2

Our ﬁrst set of contributions is algorithmic. We take inspiration from an existing greedy algorithm 
KwikCluster [2]  that has expected error 3OPT but a vacuous O(n2) worst-case bound on the number
of queries. We propose a variant of KwikCluster  called ACC  for which we prove several desirable
properties. First  ACC achieves expected clustering error 3OPT + O(n3/Q)  where Q = Ω(n) is

becomes exactly equivalent to KwikCluster. Second  ACC recovers adversarially perturbed latent
clusters. More precisely  if the input contains a cluster C obtained from a clique by adversarially
perturbing a fraction ε of its edges (internal to the clique or leaving the clique)  then ACC returns a

a deterministic bound on the number of queries. In particular  if ACC is run with Q =(cid:0)n
(cid:1)  then it
cluster (cid:98)C such that E(cid:2)|C ⊕ (cid:98)C|(cid:3) = O(cid:0)ε|C| + n2/Q(cid:1)  where ⊕ denotes symmetric difference. This
For example  when OPT = 0 and there are Ω(cid:0)n3/Q(cid:1) similar pairs  the expected number of queries

means that ACC recovers almost completely all perturbed clusters that are large enough to be “seen”
with Q queries. We also show  under stronger assumptions  that via independent executions of ACC
one can recover exactly all large clusters with high probability. Third  we show a variant of ACC 
called ACCESS (for Early Stopping Strategy)  that makes signiﬁcantly less queries on some graphs.

made by ACCESS is only the square root of the queries made by ACC. In exchange  ACCESS
makes at most Q queries in expectation rather than deterministically.
Our second set of contributions is a nearly complete information-theoretic characterization of the
query vs. clustering error trade-off (thus  ignoring computational efﬁciency). Using VC theory 
we prove that for all Q = Ω(n) the strategy of minimizing disagreements on a random subset of

approximation algorithm  too. The catch is that the approximation algorithm cannot ask the similarity
of arbitrary pairs  but only of pairs included in the random sample of edges. The best known
approximation factor in this case is 3(ln n + 1) [15]  which gives a clustering error bound of

Q(cid:1)  which
pairs achieves  with high probability  clustering error bounded by OPT + O(cid:0)n5/2/
reduces to (cid:101)O(cid:0)n3/Q(cid:1) when OPT = 0. The VC theory approach can be applied to any efﬁcient
3(ln n + 1)OPT + O(cid:0)n5/2/
Q(cid:1) with high probability. This was already observed in [7] albeit in a
least OPT + Ω(cid:0)n3/Q(cid:1). In particular  for Q = Θ(n2) any algorithm still suffers an additive error of
the special case OPT = 0  we show a lower bound Ω(cid:0)n2/

slightly different context.
We complement our upper bounds by developing two information-theoretic lower bounds; these
lower bounds apply to any algorithm issuing Q = O(n2) queries  possibly chosen in an adaptive
way. For the general case  we show that any algorithm must suffer an expected clustering error of at

order n  and for Q = Ω(n) our algorithm ACC is essentially optimal in its additive error term. For

Q(cid:1).

√

√

√

Finally  we evaluate our algorithms empirically on real-world and synthetic datasets.

2

2 Related work

Minimizing the correlation clustering error is APX-hard [9]  and the best efﬁcient algorithm found
so far achieves 2.06 OPT [10]. This almost matches the best possible approximation factor 2
achievable via the natural LP relaxation of the problem [9]. A very simple and elegant algorithm for
approximating CC is KwikCluster [2]. At each round  KwikCluster draws a random pivot πr from V  
queries the similarities between πr and every other node in V   and creates a cluster C containing πr
and all points u such that σ(πr  u) = +1. The algorithm then recursively invokes itself on V \ C. On
any instance of CC  KwikCluster achieves an expected error bounded by 3OPT. However  it is easy
to see that KwikCluster makes Θ(n2) queries in the worst case (e.g.  if σ is the constant function
−1). Our algorithms can be seen as a parsimonious version of KwikCluster whose goal is reducing
the number of queries.
The work closest to ours is [6]. Their algorithm runs KwikCluster on a random subset of 1/(2ε)
nodes and stores the set Π of resulting pivots. Then  each node v ∈ V \ Π is assigned to the cluster
identiﬁed by the pivot π ∈ Π with smallest index and such that σ(v  π) = +1. If no such pivot is
found  then v becomes a singleton cluster. According to [6  Lemma 4.1]  the expected clustering

error for this variant is 3OPT + O(cid:0)εn2(cid:1)  which can be compared to our bound for ACC by setting

Q = n/ε. On the other hand our algorithms are much simpler and signiﬁcantly easier to analyze.
This allows us to prove a set of additional properties  such as cluster recovery and instance-dependent
query bounds. It is unclear whether these results are obtainable with the techniques of [6].
Another line of work attempts to circumvent computational hardness by using the more powerful
same-cluster queries (SCQ). A same-cluster query tells whether any two given nodes are clustered
together according to an optimal clustering or not. In [3] SCQs are used to design a FPTAS for a
variant of CC with bounded number of clusters. In [23] SCQs are used to design algorithms for
solving CC optimally by giving bounds on Q which depend on OPT. Unlike our setting  both works

(cid:1) similarities are known in advance. The work [21] considers the case in which there is

assume all(cid:0)n

2

a latent clustering with OPT = 0. The algorithm can issue SCQs  however the oracle is noisy: each
query is answered incorrectly with some probability  and the noise is persistent (repeated queries give
the same noisy answer). The above setting is closely related to the stochastic block model (SBM) 
which is a well-studied model for cluster recovery [1  19  22]. However  few works investigate SBMs
with pairwise queries [12]. Our setting is strictly harder because our oracle has a budget of OPT
adversarially incorrect answers.
A different model is edge classiﬁcation. Here the algorithm is given a graph G with hidden binary
labels on the edges. The task is to predict the sign of all edges by querying as few labels as
possible [7  11  13]. As before  the oracle can have a budget OPT of incorrect answers  or a latent
clustering with OPT = 0 is assumed and the oracle’s answers are affected by persistent noise. Unlike
correlation clustering  in edge classiﬁcation the algorithm is not constrained to predict in agreement
with a partition of the nodes. On the other hand  the algorithm cannot query arbitrary pairs of nodes
in V   but only those that form an edge in G.

Preliminaries and notation. We denote by V ≡ {1  . . .   n} the set of input nodes  by E ≡(cid:0)V
(cid:1)
The cost ∆C of C is(cid:12)(cid:12)ΓC(cid:12)(cid:12). The correlation clustering index is OPT = minC ∆C  where the minimum

the set of all pairs {u  v} of distincts nodes in V   and by σ : E → {−1  +1} the binary similarity
function. A clustering C is a partition of V in disjoint clusters Ci : i = 1  . . .   k. Given C and σ  the
set ΓC of mistaken edges contains all pairs {u  v} such that σ(u  v) = −1 and u  v belong to same
cluster of C and all pairs {u  v} such that σ(u  v) = +1 and u  v belong to different clusters of C.
is over all clusterings C. We often view V  σ as a graph G = (V  E) where {u  v} ∈ E is an edge
if and only if σ(u  v) = +1. In this case  for any subset U ⊆ V we let G[U ] be the subgraph of G
induced by U  and for any v ∈ V we let Nv be the neighbor set of v.
A triangle is any unordered triple T = {u  v  w} ⊆ V . We denote by e = {u  w} a generic triangle
edge; we write e ⊂ T and v ∈ T \ e. We say T is a bad triangle if the labels σ(u  v)  σ(u  w)  σ(v  w)
are {+  + −} (the order is irrelevant). We denote by T the set of all bad triangles in V . It is easy to
see that the number of edge-disjoint bad triangles is a lower bound on OPT.
Due to space limitations  here most of our results are stated without proof  or with a concise proof
sketch; the full proofs can be found in the supplementary material.

2

3

3 The ACC algorithm

We introduce our active learning algorithm ACC (Active Correlation Clustering).

Algorithm 1 ACC with query rate f
Parameters: residual node set Vr  round index r
1: if |Vr| = 0 then RETURN
2: if |Vr| = 1 then output singleton cluster Vr and RETURN
3: if r > (cid:100)f (|V1| − 1)(cid:101) then RETURN
4: Draw pivot πr u.a.r. from Vr
5: Cr ← {πr}
6: Draw a random subset Sr of (cid:100)f (|Vr| − 1)(cid:101) nodes from Vr \ {πr}
7: for each u ∈ Sr do query σ(πr  u)
8: if ∃ u ∈ Sr such that σ(πr  u) = +1 then
9:
10:
11: Output cluster Cr
12: ACC(Vr \ Cr  r + 1)

Query all remaining pairs (πr  u) for u ∈ Vr \(cid:0){πr} ∪ Sr

Cr ← Cr ∪ {u : σ(πr  u) = +1}

(cid:1)

(cid:46) Create new cluster and add the pivot to it

(cid:46) Check if there is at least a positive edge

(cid:46) Populate cluster based on queries

(cid:46) Recursive call on the remaining nodes

ACC has the same recursive structure as KwikCluster. First  it starts with the full instance V1 = V .
Then  for each round r = 1  2  . . . it selects a random pivot πr ∈ Vr  queries the similarities between
πr and a subset of Vr  removes πr and possibly other points from Vr  and proceeds on the remaining
residual subset Vr+1. However  while KwikCluster queries σ(πr  u) for all u ∈ Vr \ {πr}  ACC
queries only (cid:100)f (nr)(cid:101) ≤ nr other nodes u (lines 6–7)  where nr = |Vr|− 1. Thus  while KwikCluster
always ﬁnds all positive labels involving the pivot πr  ACC can ﬁnd them or not  with a probability
that depends on f. The function f is called query rate function and dictates the tradeoff between the
clustering cost ∆ and the number of queries Q  as we prove below. Now  if any of the aforementioned
(cid:100)f (nr)(cid:101) queries returns a positive label (line 8)  then all the labels between πr and the remaining
u ∈ Vr are queried and the algorithm operates as KwikCluster until the end of the recursive call;
otherwise  the pivot becomes a singleton cluster which is removed from the set of nodes. Another
important difference is that ACC deterministically stops after at most (cid:100)f (n)(cid:101) recursive calls (line 1) 
declaring all remaining points as singleton clusters. The intuition is that with good probability the
clusters not found within (cid:100)f (n)(cid:101) rounds are small enough to be safely disregarded. Since the choice
of f is delicate  we avoid trivialities by assuming f is positive and smooth enough. Formally:

Deﬁnition 1. f : N → R is a query rate function if f (1) = 1  and f (n) ≤ f (n + 1) ≤(cid:0)1 + 1

(cid:1)f (n)

n

for all n ∈ N. This implies f (n+k)

n+k ≤ f (n)

n for all k ≥ 1.

We can now state formally our bounds for ACC.
Theorem 1. For any query rate function f and any labeling σ on n nodes  the expected cost E[∆A]
of the clustering output by ACC satisﬁes

E[∆A] ≤ 3OPT +

2e − 1
2(e − 1)

n2
f (n)

+

n
e

.

The number of queries made by ACC is deterministically bounded as Q ≤ n(cid:100)f (n)(cid:101). In the special
case f (n) = n for all n ∈ N  ACC reduces to KwikCluster and achieves E[∆A] ≤ 3OPT with
Q ≤ n2.
Note that Theorem 1 gives an upper bound on the error achievable when using Q queries: since
Q = nf (n)  the expected error is at most 3OPT + O(n3/Q). Furthermore  as one expects  if the
learner is allowed to ask for all edge signs  then the exact bound of KwikCluster is recovered (note
that the ﬁrst formula in Theorem 1 clearly does not take into account the special case when f (n) = n 
which is considered in the last part of the statement).
Proof sketch. Look at a generic round r  and consider a pair of points {u  w} ∈ Vr. The essence is
that ACC can misclassify {u  w} in one of two ways. First  if σ(u  w) = −1  ACC can choose as

4

pivot πr a node v such that σ(v  u) = σ(v  w) = +1. In this case  if the condition on line 8 holds 
then ACC will cluster v together with u and w  thus mistaking {u  w}. If instead σ(u  w) = +1 
then ACC could mistake {u  w} by pivoting on a node v such that σ(v  u) = +1 and σ(v  w) = −1 
and clustering together only v and u. Crucially  both cases imply the existence of a bad triangle
T = {u  w  v}. We charge each such mistake to exactly one bad triangle T   so that no triangle is
charged twice. The expected number of mistakes can then be bound by 3OPT using the packing
argument of [2] for KwikCluster. Second  if σ(u  w) = +1 then ACC could choose one of them  say
u  as pivot πr  and assign it to a singleton cluster. This means the condition on line 8 fails. We can
then bound the number of such mistakes as follows. Suppose πr has cn/f (n) positive labels towards
Vr for some c ≥ 0. Loosely speaking  we show that the check of line 8 fails with probability e−c 

in which case cn/f (n) mistakes are added. In expectation  this gives cne−c/f (n) = O(cid:0)n/f (n)(cid:1)
mistakes. Over all f (n) ≤ n rounds  this gives an overall O(cid:0)n2/f (n)(cid:1). (The actual proof has to take

into account that all the quantities involved here are not constants  but random variables).

3.1 ACC with Early Stopping Strategy

We can reﬁne our algorithm ACC so that  in some cases  it takes advantage of the structure of
the input to reduce signiﬁcantly the expected number of queries. To this end we see the input as
a graph G with edges corresponding to positive labels (see above). Suppose then G contains a
sufﬁciently small number O(n2/f (n)) of edges. Since ACC performs up to (cid:100)f (n)(cid:101) rounds  it could
make Q = Θ(f (n)2) queries. However  with just (cid:100)f (n)(cid:101) queries one could detect that G contains
O(n2/f (n)) edges  and immediately return the trivial clustering formed by all singletons. The
expected error would obviously be at most OPT + O(n2/f (n))  i.e. the same of Theorem 1. More
generally  at each round r with (cid:100)f (nr)(cid:101) queries one can check if the residual graph contains at least
n2/f (n) edges; if the test fails  declaring all nodes in Vr as singletons gives expected additional error
O(n2/f (n)). The resulting algorithm is a variant of ACC that we call ACCESS (ACC with Early
Stopping Strategy). The pseudocode can be found in the supplementary material.
First  we show ACCESS gives guarantees virtually identical to ACC (only  with Q in expectation).
Formally:
Theorem 2. For any query rate function f and any labeling σ on n nodes  the expected cost E[∆A]
of the clustering output by ACCESS satisﬁes

E[∆A] ≤ 3OPT + 2

n2
f (n)

+

n
e

.

Moreover  the expected number of queries performed by ACCESS is E[Q] ≤ n((cid:100)f (n)(cid:101) + 4).
Theorem 2 reassures us that ACCESS is no worse than ACC. In fact  if most edges of G belong to
relatively large clusters (namely  all but O(n2/f (n)) edges)  then we can show ACCESS uses much
fewer queries than ACC (in a nutshell  ACCESS quickly ﬁnds all large clusters and then quits). The
following theorem captures the essence. For simplicity we assume OPT = 0  i.e. G is a disjoint
union of cliques.
Theorem 3. Suppose OPT = 0 so G is a union of disjoint cliques. Let C1  . . .   C(cid:96) be the cliques of
j=1 |ECj| = Ω(n2/f (n))  and

G in nondecreasing order of size. Let i(cid:48) be the smallest i such that(cid:80)i
let h(n) = |Ci(cid:48)|. Then ACCESS makes in expectation E[Q] = O(cid:0)n2 lg(n)/h(n)(cid:1) queries.

As an example  say f (n) =
n and G contains n1/3 cliques of n2/3 nodes each. Then for ACC The-
orem 1 gives Q ≤ nf (n) = O(n3/2)  while for ACCESS Theorem 3 gives E[Q] = O(n4/3 lg(n)).

√

4 Cluster recovery
In the previous section we gave bounds on E[∆]  the expected total cost of the clustering. However 
in applications such as community detection and alike  the primary objective is recovering accurately
the latent clusters of the graph  the sets of nodes that are “close” to cliques. This is usually referred to

as cluster recovery. For this problem  an algorithm that outputs a good approximation (cid:98)C of every

latent cluster C is preferable to an algorithm that minimizes E[∆] globally. In this section we show
that ACC natively outputs clusters that are close to the latent clusters in the graph  thus acting as a

5

2

2

(cid:12)(cid:12) ≥ (1 − ε)(cid:0)|C|

cluster recovery tool. We also show that  for a certain type of latent clusters  one can amplify the
accuracy of ACC via independent executions and recover all clusters exactly with high probability.
To capture the notion of “latent cluster”  we introduce the concept of (1 − ε)-knit set. As usual  we
view V  σ as a graph G = (V  E) with e ∈ E iff σ(e) = +1. Let EC be the edges in the subgraph
induced by C ⊆ V and cut(C  C) be the edges between C and C = V \ C.

(cid:1) and(cid:12)(cid:12)cut(C  C)(cid:12)(cid:12) ≤ ε(cid:0)|C|
Deﬁnition 2. A subset C ⊆ V is (1 − ε)-knit if(cid:12)(cid:12)EC
(cid:1).
Suppose now we have a cluster (cid:98)C as “estimate” of C. We quantify the distance between C and (cid:98)C as
the cardinality of their symmetric difference (cid:12)(cid:12)(cid:98)C ⊕ C(cid:12)(cid:12) =(cid:12)(cid:12)(cid:98)C \ C(cid:12)(cid:12) +(cid:12)(cid:12)C \ (cid:98)C(cid:12)(cid:12). The goal is to obtain 
for each (1 − ε)-knit set C in the graph  a cluster (cid:98)C with |(cid:98)C ⊕ C| = O(ε|C|) for some small ε. We
then ACC will miss C entirely. But  for |C| = Ω(n/f (n))  we can prove E[|(cid:98)C ⊕ C|] = O(ε|C|).

prove ACC does exactly this. Clearly  we must accept that if C is too small  i.e. |C| = o(n/f (n)) 
We point out that the property of being (1 − ε)-knit is rather weak for an algorithm  like ACC  that is
completely oblivious to the global topology of the cluster — all what ACC tries to do is to blindly
cluster together all the neighbors of the current pivot. In fact  consider a set C formed by two disjoint
cliques of equal size. This set would be close to 1/2-knit  and yet ACC would never produce a single

cluster (cid:98)C corresponding to C. Things can only worsen if we consider also the edges in cut(C  C) 

which can lead ACC to assign the nodes of C to several different clusters when pivoting on C. Hence
it is not obvious that a (1 − ε)-knit set C can be efﬁciently recovered by ACC.
Note that this task can be seen as an adversarial cluster recovery problem. Initially  we start with a
disjoint union of cliques  so that OPT = 0. Then  an adversary ﬂips the signs of some of the edges
of the graph. The goal is to retrieve every original clique that has not been perturbed excessively.
Note that we put no restriction on how the adversary can ﬂip edges; therefore  this adversarial setting
subsumes constrained adversaries. For example  it subsumes the high-probability regime of the
stochastic block model [17] where edges are ﬂipped according to some distribution.
We can now state our main cluster recovery bound for ACC.

Theorem 4. For every C ⊆ V that is (1− ε)-knit  ACC outputs a cluster (cid:98)C such that E(cid:2)|C ⊕(cid:98)C|(cid:3) ≤
3ε|C| + min(cid:8) 2n
The min in the bound captures two different regimes: when f (n) is very close to n  then E(cid:2)|C⊕(cid:98)C|(cid:3) =

(cid:1)|C|(cid:9) + |C|e−|C|f (n)/5n.

O(ε|C|) independently of the size of C  but when f (n) (cid:28) n we need |C| = Ω(n/f (n))  i.e.  |C|
must be large enough to be found by ACC.

f (n)  (cid:0)1 − f (n)

n

4.1 Exact cluster recovery via ampliﬁcation

For certain latent clusters  one can get recovery guarantees signiﬁcantly stronger than the ones given
natively by ACC (see Theorem 4). We start by introducing strongly (1 − ε)-knit sets (also known as
quasi-cliques). Recall that Nv is the neighbor set of v in the graph G induced by the positive labels.
Deﬁnition 3. A subset C ⊆ V is strongly (1 − ε)-knit if  for every v ∈ C  we have Nv ⊆ C and
|Nv| ≥ (1 − ε)(|C| − 1).
We remark that ACC alone does not give better guarantees on strongly (1 − ε)-knit subsets than on
(1 − ε)-knit subsets. Suppose for example that |Nv| = (1 − ε)(|C| − 1) for all v ∈ C. Then C is

strongly (1 − ε)-knit  and yet when pivoting on any v ∈ C ACC will inevitably produce a cluster (cid:98)C
with |(cid:98)C ⊕ C| ≥ ε|C|  since the pivot has edges to less than (1 − ε)|C| other nodes of C.
C is found. Recall that V = [n]. Then  we deﬁne the id of a cluster (cid:98)C as the smallest node of (cid:98)C.
The min-tagging rule is the following: when forming (cid:98)C  use its id to tag all of its nodes. Therefore 
if u(cid:98)C = min{u ∈ (cid:98)C} is the id of (cid:98)C  we will set id(v) = u(cid:98)C for every v ∈ (cid:98)C. Consider now the

To bypass this limitation  we run ACC several times to amplify the probability that every node in

following algorithm  called ACR (Ampliﬁed Cluster Recovery). First  ACR performs K independent
runs of ACC on input V   using the min-tagging rule on each run. In this way  for each v ∈ V we
obtain K tags id1(v)  . . .   idK(v)  one for each run. Thereafter  for each v ∈ V we select the tag that
v has received most often  breaking ties arbitrarily. Finally  nodes with the same tag are clustered

6

together. One can prove that  with high probability  this clustering contains all strongly (1 − ε)-knit
sets. In other words  ACR with high probability recovers all such latent clusters exactly. Formally 
we prove:
Theorem 5. Let ε ≤ 1
with probability at least 1 − p: for every strongly (1 − ε)-knit C with |C| > 10 n

10 and ﬁx p > 0. If ACR is run with K = 48 ln n

p   then the following holds
f (n)   the algorithm

outputs a cluster (cid:98)C such that (cid:98)C = C.

It is not immediately clear that one can extend this result by relaxing the notion of strongly (1−ε)-knit
set so to allow for edges between C and the rest of the graph. We just notice that  in that case  every
node v ∈ C could have a neighbor xv ∈ V \ C that is smaller than every node of C. In this case 
when pivoting on v ACC would tag v with x rather than with uC  disrupting ACR.

5 A fully additive scheme

2

2

ε ln 1

ε2 queries. When OPT = 0  Q = n

In this section  we introduce a(n inefﬁcient) fully additive approximation algorithm achieving cost
OPT + n2ε in high probability using order of n
ε sufﬁces.
Our algorithm combines uniform sampling with empirical risk minimization and is analyzed using
VC theory.
First  note that CC can be formulated as an agnostic binary classiﬁcation problem with binary
classiﬁers hC : E → {−1  +1} associated with each clustering C of V (recall that E denotes the
set of all pairs {u  v} of distinct elements u  v ∈ V )  and we assume hC(u  v) = +1 iff u and v
belong to the same cluster of C. Let Hn be the set of all such hC. The risk of a classiﬁer hC with
respect to the uniform distribution over E is P(hC(e) (cid:54)= σ(e)) where e is drawn u.a.r. from E. It is

(cid:1).
easy to see that the risk of any classiﬁer hC is directly related to ∆C  P(cid:0)hC(e) (cid:54)= σ(e)(cid:1) = ∆C(cid:14)(cid:0)n
Hence  in particular  OPT =(cid:0)n
P(cid:0)h(e) (cid:54)= σ(e)(cid:1). Now  it is well known —see  e.g.  [24 
procedure: query O(cid:0)d/ε2(cid:1) edges drawn u.a.r. from E  where d is the VC dimension of Hn  and ﬁnd
risk  then O(cid:0)(d/ε) ln(1/ε)(cid:1) random queries sufﬁce. A trivial upper bound on the VC dimension of
Hn is log2 |Hn| = O(cid:0)n ln n). The next result gives the exact value.

Theorem 6.8]— that we can minimize the risk to within an additive term of ε using the following
the clustering C such that hC makes the fewest mistakes on the sample. If there is h∗ ∈ Hn with zero

(cid:1) minh∈Hn

Theorem 6. The VC dimension of the class Hn of all partitions of n elements is n − 1.
Proof. Let d be the VC dimension of Hn. We view an instance of CC as the complete graph Kn with
edges labelled by σ. Let T be any spanning tree of Kn. For any labeling σ  we can ﬁnd a clustering
C of V such that hC perfectly classiﬁes the edges of T : simply remove the edges with label −1 in T
and consider the clusters formed by the resulting connected components. Hence d ≥ n − 1 because
any spanning tree has exactly n − 1 edges. On the other hand  any set of n edges must contain at
least a cycle. It is easy to see that no clustering C makes hC consistent with the labeling σ that gives
positive labels to all edges in the cycle but one. Hence d < n.

An immediate consequence of the above is the following.
Theorem 7. There exists a randomized algorithm A that  for all 0 < ε < 1  ﬁnds a clustering C

satisfying ∆C ≤ OPT +O(cid:0)n2ε(cid:1) with high probability while using Q = O(cid:0) n
OPT = 0  then Q = O(cid:0) n

(cid:1) queries. Moreover  if
(cid:1) queries are enough to ﬁnd a clustering C satisfying ∆C = O(cid:0)n2ε(cid:1).

ε2

ε ln 1

ε

6 Lower bounds

In this section we give two lower bounds on the expected clustering error of any (possibly randomized)
algorithm. The ﬁrst bound holds for OPT = 0  and applies to algorithms using a deterministically
bounded number of queries. This bound is based on a construction from [8  Lemma 11] and related
to kernel-based learning.
Theorem 8. For any ε > 0 such that 1
learning algorithm asking fewer than 1
n ≥ 16

ε is an even integer  and for every (possibly randomized)
50ε2 queries with probability 1  there exists a labeling σ on

ε nodes such that OPT = 0 and the expected cost of the algorithm is at least n2ε
8 .

ε ln 1

7

Theorem 9. Choose any function ε = ε(n) such that Ω(cid:0) 1

Our second bound relaxed the assumption on OPT. It uses essentially the same construction of [6 
Lemma 6.1]  giving asymptotically the same guarantees. However  the bound of [6] applies only to
a very restricted class of algorithms: namely  those where the number qv of queries involving any
speciﬁc node v ∈ V is deterministically bounded. This rules out a vast class of algorithms  including
KwikCluster  ACC  and ACCESS  where the number of queries involving a node is a function of the
random choices of the algorithm. Our lower bound is instead fully general: it holds unconditionally
for any randomized algorithm  with no restriction on what or how many pairs of points are queried.
ε ∈ N. For every (possibly
randomized) learning algorithm and any n0 > 0 there exists a labeling σ on n ≥ n0 nodes such
that the algorithm has expected error E[∆] ≥ OPT + n2ε
80 whenever its expected number of queries
satisﬁes E[Q] < n
In fact  the bound of Theorem 9 can be put in a more general form: for any constant c ≥ 1  the
expected error is at least c · OPT + A(c) where A(c) = Ω(n2ε) is an additive term with constant
factors depending on c (see the proof). Thus  our algorithms ACC and ACCESS are essentially
optimal in the sense that  for c = 3  they guarantee an optimal additive error up to constant factors.

(cid:1) ≤ ε ≤ 1

2 and 1

80 ε .

n

7 Experiments

We verify experimentally the tradeoff between clustering cost and number of queries of ACC  using
six datasets from [21  20]. Four datasets come from real-world data  and two are synthetic; all of them
provide a ground-truth partitioning of some set V of nodes. Here we show results for one real-world
dataset (cora  with |V |=1879 and 191 clusters) and one synthetic dataset (skew  with |V |=900 and
30 clusters). Results for the remaining datasets are similar and can be found in the supplementary
material. Since the original datasets have OPT = 0  we derived perturbed versions where OPT > 0

as follows. First  for each η ∈ {0  0.1  0.5  1} we let p = η|E|/(cid:0)n

(cid:1) where |E| is the number of edges

(positive labels) in the dataset (so η is the expected number of ﬂipped edges measured as a multiple
of |E|). Then  we ﬂipped the label of each pair of nodes independently with probability p. Obviously
for p = 0 we have the original dataset.
For every dataset and its perturbed versions we then proceeded as follows. For α = 0  0.05  ...  0.95  1 
we set the query rate function to f (x) = xα. Then we ran 20 independent executions of ACC  and
computed the average number of queries µQ and average clustering cost µ∆. The variance was
often negligible  but is reported in the full plots in the supplementary material. The tradeoff between
µ∆ and µQ is depicted in Figure 1  where the circular marker highlights the case f (x) = x  i.e.
KwikCluster.

2

(a) skew.

(b) cora.

Figure 1: Performance of ACC.

The clustering cost clearly drops as the number of queries increases. This drop is particularly marked
on cora  where ACC achieves a clustering cost close to that of KwikCluster using an order of
magnitude fewer queries. It is also worth noting that  for the case OPT = 0  the measured clustering
cost achieved by ACC is 2 to 3 times lower than the theoretical bound of ≈ 3.8n3/Q given by
Theorem 1.

8

0.00.51.01.52.02.53.0Q1e40.00.20.40.60.81.01.21e4η=0η=0.1η=0.5η=101234567Q1e40.00.20.40.60.81.01e5η=0η=0.1η=0.5η=1Acknowledgements

The authors gratefully acknowledge partial support by the Google Focused Award “Algorithms and
Learning for AI” (ALL4AI). Marco Bressan and Fabio Vitale are also supported in part by the ERC
Starting Grant DMAP 680153 and by the “Dipartimenti di Eccellenza 2018-2022” grant awarded to
the Department of Computer Science of the Sapienza University of Rome. Nicolò Cesa-Bianchi is
also supported by the MIUR PRIN grant Algorithms  Games  and Digital Markets (ALGADIMAR).

References
[1] Emmanuel Abbe and Colin Sandon. Community detection in general stochastic block models:
In Proc. of IEEE FOCS  pages

Fundamental limits and efﬁcient algorithms for recovery.
670–688  2015.

[2] Nir Ailon  Moses Charikar  and Alantha Newman. Aggregating inconsistent information:

Ranking and clustering. J. ACM  55(5):23:1–23:27  2008.

[3] Nir Ailon  Anup Bhattacharya  and Ragesh Jaiswal. Approximate correlation clustering using

same-cluster queries. In Proc. of LATIN  pages 14–27  2018.

[4] Nikhil Bansal  Avrim Blum  and Shuchi Chawla. Correlation clustering. Machine Learning  56

(1-3):89–113  2004.

[5] Amir Ben-Dor  Ron Shamir  and Zohar Yakhini. Clustering gene expression patterns. Journal

of Computational Biology  6(3-4):281–297  1999.

[6] Francesco Bonchi  David García-Soriano  and Konstantin Kutzkov. Local correlation clustering.

CoRR  abs/1312.5105  2013.

[7] Nicolò Cesa-Bianchi  Claudio Gentile  Fabio Vitale  and Giovanni Zappella. A correlation
clustering approach to link classiﬁcation in signed networks. In Proc. of COLT  pages 34.1–
34.20  2012.

[8] Nicolò Cesa-Bianchi  Yishay Mansour  and Ohad Shamir. On the complexity of learning with

kernels. In Proc. of COLT  pages 297–325  2015.

[9] Moses Charikar  Venkatesan Guruswami  and Anthony Wirth. Clustering with qualitative

information. Journal of Computer and System Sciences  71(3):360–383  2005.

[10] Shuchi Chawla  Konstantin Makarychev  Tselil Schramm  and Grigory Yaroslavtsev. Near
optimal LP rounding algorithm for correlation clustering on complete and complete k-partite
graphs. In Proc. of ACM STOC  pages 219–228  2015.

[11] Yudong Chen  Ali Jalali  Sujay Sanghavi  and Huan Xu. Clustering partially observed graphs
via convex optimization. The Journal of Machine Learning Research  15(1):2213–2238  2014.

[12] Yuxin Chen  Govinda Kamath  Changho Suh  and David Tse. Community recovery in graphs

with locality. In Proc. of ICML  pages 689–698  2016.

[13] Kai-Yang Chiang  Cho-Jui Hsieh  Nagarajan Natarajan  Inderjit S Dhillon  and Ambuj Tewari.
Prediction and clustering in signed networks: a local to global perspective. The Journal of
Machine Learning Research  15(1):1177–1213  2014.

[14] William W Cohen and Jacob Richman. Learning to match and cluster large high-dimensional

data sets for data integration. In Proc. of ACM KDD  pages 475–480  2002.

[15] Erik D Demaine  Dotan Emanuel  Amos Fiat  and Nicole Immorlica. Correlation clustering in

general weighted graphs. Theoretical Computer Science  361(2-3):172–187  2006.

[16] Lise Getoor and Ashwin Machanavajjhala. Entity resolution: theory  practice & open challenges.

Proc. of the VLDB Endowment  5(12):2018–2019  2012.

[17] Paul W. Holland  Kathryn Blackmond Laskey  and Samuel Leinhardt. Stochastic blockmodels:

First steps. Social Networks  5(2):109 – 137  1983.

9

[18] Sungwoong Kim  Sebastian Nowozin  Pushmeet Kohli  and Chang D Yoo. Higher-order
correlation clustering for image segmentation. In Proc. of NeurIPS  pages 1530–1538  2011.

[19] Laurent Massoulié. Community detection thresholds and the weak ramanujan property. In Proc.

of ACM STOC  pages 694–703. ACM  2014.

[20] Arya Mazumdar and Barna Saha. Query complexity of clustering with side information. In

Proc. of NeurIPS  pages 4682–4693  2017.

[21] Arya Mazumdar and Barna Saha. Clustering with noisy queries. In Proc. of NeurIPS  pages

5788–5799  2017.

[22] Elchanan Mossel  Joe Neeman  and Allan Sly. A proof of the block model threshold conjecture.

Combinatorica  38(3):665–708  2018.

[23] Barna Saha and Sanjay Subramanian. Correlation clustering with same-cluster queries bounded

by optimal cost. In Proc. of ESA  pages 81:1–81:17  2019.

[24] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to

Algorithms. Cambridge University Press  New York  NY  USA  2014.

[25] Jiliang Tang  Yi Chang  Charu Aggarwal  and Huan Liu. A survey of signed network mining in

social media. ACM Computing Surveys (CSUR)  49(3):42  2016.

[26] Anthony Wirth. Correlation Clustering. In Claude Sammut and Geoffrey I Webb  editors 

Encyclopedia of machine learning and data mining  pages 227–231. Springer US  2010.

10

,Marco Bressan
Nicolò Cesa-Bianchi
Andrea Paudice
Fabio Vitale