2010,Boosting Classifier Cascades,The problem of optimal and automatic design of a detector cascade is considered. A novel mathematical model is introduced for a cascaded detector. This model is analytically tractable  leads to recursive computation  and accounts for both classification and complexity. A boosting algorithm  FCBoost  is proposed for fully automated cascade design. It exploits the new cascade model  minimizes a Lagrangian cost that accounts for both classification risk and complexity. It searches the space of cascade configurations to automatically determine the optimal number of stages and their predictors  and is compatible with bootstrapping of negative examples and cost sensitive learning. Experiments show that the resulting cascades have state-of-the-art performance in various computer vision problems.,Boosting Classiﬁer Cascades

Mohammad J. Saberian

Statistical Visual Computing Laboratory 

University of California  San Diego

La Jolla  CA 92039

saberian@ucsd.edu

Abstract

Nuno Vasconcelos

Statistical Visual Computing Laboratory 

University of California  San Diego

La Jolla  CA 92039
nuno@ucsd.edu

The problem of optimal and automatic design of a detector cascade is considered.
A novel mathematical model is introduced for a cascaded detector. This model is
analytically tractable  leads to recursive computation  and accounts for both clas-
siﬁcation and complexity. A boosting algorithm  FCBoost  is proposed for fully
automated cascade design. It exploits the new cascade model  minimizes a La-
grangian cost that accounts for both classiﬁcation risk and complexity. It searches
the space of cascade conﬁgurations to automatically determine the optimal num-
ber of stages and their predictors  and is compatible with bootstrapping of neg-
ative examples and cost sensitive learning. Experiments show that the resulting
cascades have state-of-the-art performance in various computer vision problems.

1

Introduction

There are many applications where a classiﬁer must be designed under computational constraints.
One problem where such constraints are extreme is that of object detection in computer vision.
To accomplish tasks such as face detection  the classiﬁer must process thousands of examples per
image  extracted from all possible image locations and scales  at a rate of several images per second.
This problem has been the focus of substantial attention since the introduction of the detector cascade
architecture by Viola and Jones (VJ) in [13]. This architecture was used to design the ﬁrst real time
face detector with state-of-the-art classiﬁcation accuracy. The detector has  since  been deployed
in many practical applications of broad interest  e.g. face detection on low-complexity platforms
such as cameras or cell phones. The outstanding performance of the VJ detector is the result of 1) a
cascade of simple to complex classiﬁers that reject most non-faces with a few machine operations 
2) learning with a combination of boosting and Haar features of extremely low complexity  and 3)
use of bootstrapping to efﬁciently deal with the extremely large class of non-face examples.

While the resulting detector is fast and accurate  the process of designing a cascade is not.
In
particular  VJ did not address the problem of how to automatically determine the optimal cascade
conﬁguration  e.g. the numbers of cascade stages and weak learners per stage  or even how to design
individual stages so as to guarantee optimality of the cascade as a whole. In result  extensive manual
supervision is required to design cascades with good speed/accuracy trade off. This includes trial-
and-error tuning of the false positive/detection rate of each stage  and of the cascade conﬁguration.
In practice  the design of a good cascade can take up several weeks. This has motivated a number
of enhancements to the VJ training procedure  which can be organized into three main areas: 1)
enhancement of the boosting algorithms used in cascade design  e.g. cost-sensitive variations of
boosting [12  4  8]  ﬂoat Boost [5] or KLBoost [6]  2) post processing of a learned cascade  by ad-
justing stage thresholds  to improve performance [7]  and 3) specialized cascade architectures which
simplify the learning process  e.g. the embedded cascade (ChainBoost) of [15]  where each stage
contains all weak learners of previous stages. These enhancements do not address the fundamental
limitations of the VJ design  namely how to guarantee overall cascade optimality.

1

L

R

 

0.8

0.6

0.4

0.2

0

AdaBoost
ChainBoost

10

20

Iterations

30

40

50

L

0.8

0.7

0.6

0.5

0

AdaBoost
ChainBoost

10

20

Iterations

30

40

50

Figure 1: Plots of RL (left) and L (right) for detectors designed with AdaBoost and ChainBoost.

More recently  various works have attempted to address this problem [9  8  1  14  10]. However  the
proposed algorithms still rely on sequential learning of cascade stages  which is suboptimal  some-
times require manual supervision  do not search over cascade conﬁgurations  and frequently lack
a precise mathematical model for the cascade. In this work  we address these problems  through
two main contributions. The ﬁrst is a mathematical model for a detector cascade  which is an-
alytically tractable  accounts for both classiﬁcation and complexity  and is amenable to recursive
computation. The second is a boosting algorithm  FCBoost  that exploits this model to solve the
cascade learning problem. FCBoost solves a Lagrangian optimization problem  where the classiﬁ-
cation risk is minimized under complexity constraints. The risk is that of the entire cascade  which
is learned holistically  rather than through sequential stage design  and FCBoost determines the opti-
mal cascade conﬁguration automatically. It is also compatible with bootstrapping and cost sensitive
boosting extensions  enabling efﬁcient sampling of negative examples and explicit control of the
false positive/detection rate trade off. An extensive experimental evaluation  covering the problems
of face  car  and pedestrian detection demonstrates its superiority over previous approaches.

2 Problem Deﬁnition

A binary classiﬁer h(x) maps an example x into a class label y ∈ {−1  1} according to h(x) =
sign[f (x)]  where f (x) is a continuous-valued predictor. Optimal classiﬁers minimize a risk

RL(f ) = EX Y {L[y  f (x)]} ≃

L[yi  f (xi)]

(1)

1

|St|Xi

1

|St|Xi

where St = {(x1  y1)  . . .   (xn  yn)} is a set of training examples  yi ∈ {1  −1} the class label of
example xi  and L[y  f (x)] a loss function. Commonly used losses are upper bounds on the zero-one
loss  whose risk is the probability of classiﬁcation error. Hence  RL is a measure of classiﬁcation
accuracy. For applications with computational constraints  optimal classiﬁer design must also take
into consideration the classiﬁcation complexity. This is achieved by deﬁning a computational risk

RC(f ) = EX Y {LC [y  C(f (x))]} ≃

LC[yi  C(f (xi))]

(2)

where C(f (x)) is the complexity of evaluating f (x)  and LC[y  C(f (x))] a loss function that encodes
the cost of this operation. In most detection problems  targets are rare events and contribute little to
the overall complexity. In this case  which we assume throughout this work  LC[1  C(f (x))] = 0
and LC[−1  C(f (x))] is denoted LC[C(f (x))]. The computational risk is thus

RC(f ) ≈

1
|S−

t | Xxi∈S−

t

LC[C(f (xi))].

(3)

whereS−
t contains the negative examples of St. Usually  more accurate classiﬁers are more complex.
For example in boosting  where the decision rule is a combination of weak rules  a ﬁner approxima-
tion of the classiﬁcation boundary (smaller error) requires more weak learners and computation.

Optimal classiﬁer design under complexity constraints is a problem of constrained optimization 
which can be solved with Lagrangian methods. These minimize a Lagrangian

L(f ; St) =

1

|St| Xxi∈St

L[yi  f (xi)] +

2

η
|S−

t | Xxi∈S−

t

LC[C(f (xi))] 

(4)

where η is a Lagrange multiplier  which controls the trade-off between error rate and complexity.
Figure 1 illustrates this trade-off  by plotting the evolution of RL and L as a function of the boosting
iteration  for the AdaBoost algorithm [2]. While the risk always decreases with the addition of weak
learners  this is not true for the Lagrangian. After a small number of iterations  the gain in accuracy
does not justify the increase in classiﬁer complexity. The design of classiﬁers under complexity
constraints has been addressed through the introduction of detector cascades. A detector cascade
H(x) implements a sequence of binary decisions hi(x)  i = 1 . . . m. An example x is declared a
target (y = 1) if and only if it is declared a target by all stages of H  i.e. hi(x) = 1  ∀i. Otherwise 
the example is rejected. For applications where the majority of examples can be rejected after a
small number of cascade stages  the average classiﬁcation time is very small. However  the problem
of designing an optimal detector cascade is still poorly understood. A popular approach  known as
ChainBoost or embedded cascade [15]  is to 1) use standard boosting algorithms to design a detector 
and 2) insert a rejection point after each weak learner. This is simple to implement  and creates
a cascade with as many stages as weak learners. However  the introduction of the intermediate
rejection points  a posteriori of detector design  sacriﬁces the risk-optimality of the detector. This
is illustrated in Figure 1  where the evolution of RL and L are also plotted for ChainBoost. In this
example  L is monotonically decreasing  i.e. the addition of weak learners no longer carries a large
complexity penalty. This is due to the fact that most negative examples are rejected in the earliest
cascade stages. On the other hand  the classiﬁcation risk is more than double that of the original
boosted detector. It is not known how close ChainBoost is to optimal  in the sense of (4).

3 Classiﬁer cascades

In this work  we seek the design of cascades that are provably optimal under (4). We start by
introducing a mathematical model for a detector cascade.

3.1 Cascade predictor

Let H(x) = {h1(x)  . . .   hm(x)} be a cascade of m detectors hi(x) = sgn[fi(x)]. To develop
some intuition  we start with a two-stage cascade  m = 2. The cascade implements the decision rule
(5)

H(F)(x) = sgn[F(x)]

where

F(x) = F(f1  f2)(x) =(cid:26) f1(x)

f2(x)

= f1u(−f1) + u(f1)f2

if f1(x) < 0
if f1(x) ≥ 0

(6)

(7)

is denoted the cascade predictor  u(.) is the step function and we omit the dependence on x for
notational simplicity. This equation can be extended to a cascade of m stages  by replacing the
predictor of the second stage  when m = 2  with the predictor of the remaining cascade  when m is
larger. Letting Fj = F(fj  . . .   fm) be the cascade predictor for the cascade composed of stages j
to m

F = F1 = f1u(−f1) + u(f1)F2.

(8)

More generally  the following recursion holds

(9)
with initial condition Fm = fm. In Appendix A  it is shown that combining (8) and (9) recursively
leads to

Fk = fku(−fk) + u(fk)Fk+1

F = T1 m + T2 mfm

= T1 k + T2 kfku(−fk) + T2 kFk+1u(fk)  k < m.

(10)
(11)

with initial conditions T1 0 = 0  T2 0 = 1 and

T1 k+1 = T1 k + fku(−fk) T2 k 

(12)
Since T1 k  T2 k  and Fk+1 do not depend on fk  (10) and (11) make explicit the dependence of the
cascade predictor  F  on the predictor of the kth stage.

T2 k+1 = T2 k u(fk).

3

3.2 Differentiable approximation

Letting F(fk + ǫg) = F(f1  ..  fk + ǫg  ..fm)  the design of boosting algorithms requires the eval-
uation of both F(fk + ǫg)  and the functional derivative of F with respect to each fk  along any
direction g

These are straightforward for the last stage since  from (10) 

< δF(fk)  g >=

d
dǫ

.

F(fk + ǫg)(cid:12)(cid:12)(cid:12)(cid:12)ǫ=0

F(fm + ǫg) = am + ǫbmg  < δF(fm)  g >= bmg 

(13)

where

(15)

(16)

(17)

(18)

(14)
In general  however  the right-hand side of (11) is non-differentiable  due to the u(.) functions. A
differentiable approximation is possible by adopting the classic sigmoidal approximation u(x) ≈
tanh(σx)+1

am = T1 m + T2 mfm = F(fm) 

  where σ is a relaxation parameter. Using this approximation in (11) 

bm = T2 m.

2

F = F(fk) = T1 k + T2 kfk(1 − u(fk)) + T2 kFk+1u(fk)

It follows that

≈ T1 k + T2 kfk +

1
2

T2 k[Fk+1 − fk][tanh(σfk) + 1].

< δF(fk)  g > = bkg

1
2

F(fk + ǫg) can also be simpliﬁed by resorting to a ﬁrst order Taylor series expansion around fk

T2 k(cid:8)[1 − tanh(σfk)] + σ[Fk+1 − fk][1 − tanh2(σfk)](cid:9) .

bk =

F(fk + ǫg) ≈ ak + ǫbkg

ak = F(fk) = T1 k + T2 k(cid:26)fk +

1
2

[Fk+1 − fk][tanh(σfk) + 1](cid:27) .

(19)

(20)

3.3 Cascade complexity

In Appendix B  a similar analysis is performed for the computational complexity. Denoting by C(fk)
the complexity of evaluating fk  it is shown that

with initial conditions C(Fm+1) = 0  P1 1 = 0  P2 1 = 1 and

C(F) = P1 k + P2 kC(fk) + P2 ku(fk)C(Fk+1).

P1 k+1 = P1 k + C(fk) P2 k

P2 k+1 = P2 k u(fk).

(21)

(22)

This makes explicit the dependence of the cascade complexity on the complexity of the kth stage.

In practice  fk = Pl clgl for gl ∈ U  where U is a set of functions of approximately identical

complexity. For example  the set of projections into Haar features  in which C(fk) is proportional to
the number of features gl. In general  fk has three components. The ﬁrst is a predictor that is also
used in a previous cascade stage  e.g. fk(x) = fk−1(x) + cg(x) for an embedded cascade. In this
case  fk−1(x) has already been evaluated in stage k − 1 and is available with no computational cost.
The second is the set O(fk) of features that have been used in some stage j ≤ k. These features are
also available and require minimal computation (multiplication by the weight cl and addition to the
running sum). The third is the set N (fk) of features that have not been used in any stage j ≤ k. The
overall computation is

(23)
where λ < 1 is the ratio of computation required to evaluate a used vs. new feature. For Haar
wavelets  λ ≈ 1
20 . It follows that updating the predictor of the kth stage increases its complexity to
(24)

C(fk) = |N (fk)| + λ|O(fk)| 

C(fk + ǫg) =(cid:26) C(fk) + λ

C(fk) + 1

if g ∈ O(fk)
if g ∈ N (fk) 

and the complexity of the cascade to

C(F(fk + ǫg)) = P1 k + P2 kC(fk + ǫg) + P2 ku(fk + ǫg)C(Fk+1)

with

= αk + γkC(fk + ǫg) + βku(fk + ǫg)

αk = P1 k

γk = P2 k βk = P2 kC(Fk+1).

(25)
(26)

(27)

4

3.4 Neutral predictors

The models of (10)  (11) and (21) will be used for the design of optimal cascades. Another observa-
tion that we will exploit is that

H[F(f1  . . .   fm  fm)] = H[F(f1  . . .   fm)].

This implies that repeating the last stage of a cascade does not change its decision rule. For this
reason n(x) = fm(x) is referred to as the neutral predictor of a cascade of m stages.

4 Boosting classiﬁer cascades

In this section  we introduce a boosting algorithm for cascade design.

4.1 Boosting

Boosting algorithms combine weak learners to produce a complex decision boundary. Boost-
ing iterations are gradient descent steps towards the predictor f (x) of minimum risk for the loss
L[y  f (x)] = e−yf (x) [3]. Given a set U of weak learners  the functional derivative of RL along the
direction of weak leaner g is

< δRL(f )  g > =

1

|St|Xi (cid:20) d

dǫ

e−yi(f (xi)+ǫg(xi))(cid:21)ǫ=0

= −

yiwig(xi) 

(28)

1

|St|Xi

where wi = e−yif (xi) is the weight of xi. Hence  the best update is

Letting I(x) be the indicator function  the optimal step size along the selected direction  g∗(x)  is

g∗(x) = arg max
g∈U

< −δRL(f )  g > .

(29)

c∗ = arg min

c∈RXi

e−yi(f (xi)+cg∗(xi)) =

1
2

log Pi wiI(yi = g∗(xi))
Pi wiI(yi 6= g∗(xi))

The predictor is updated into f (x) = f (x) + c∗g∗(x) and the procedure iterated.

.

(30)

4.2 Cascade risk minimization

To derive a boosting algorithm for
L[y  F(f1  . . .   fm)(x)] = e−yF (f1 ... fm)(x)  and minimize the cascade risk

the design of detector cascades  we adopt

the loss

RL(F) = EX Y {e−yF (f1 ... fm)} ≈

e−yiF (f1 ... fm)(xi).

1

|St|Xi

Using (13) and (19) 

< δRL(F(fk))  g >=

1

|St|Xi (cid:20) d

dǫ

e−yi[ak(xi)+ǫbk(xi)g(xi)](cid:21)ǫ=0

= −

1

|St|Xi

yiwk

i bk

i g(xi) (31)

where wk
descent direction and step size for the kth stage are then

i = e−yiak(xi)  bk

i = bk(xi) and ak  bk are given by (14)  (18)  and (20). The optimal

g∗
k = arg max
g∈U

< −δRL(F(fk))  g >

c∗
k = arg min

c∈RXi

i e−yibk
wk

i cg∗

k(xi).

(32)

(33)

i are not constant  there is no closed form for c∗

In general  because the bk
k  and a line search must
be used. Note that  since ak(xi) = F(fk)(xi)  the weighting mechanism is identical to that of
boosting  i.e. points are reweighed according to how well they are classiﬁed by the current cascade.
Given the optimal c∗  g∗ for all stages  the impact of each update in the overall cascade risk  RL  is
evaluated and the stage of largest impact is updated.

5

4.3 Adding a new stage

Searching for the optimal cascade conﬁguration requires support for the addition of new stages 
whenever necessary. This is accomplished by including a neutral predictor as the last stage of the
cascade. If adding a weak learner to the neutral stage reduces the risk further than the corresponding
addition to any other stage  a new stage (containing the neutral predictor plus the weak learner) is
created. Since this new stage includes the last stage of the previous cascade  the process mimics the
design of an embedded cascade. However  there are no restrictions that a new stage should be added
at each boosting iteration  or consist of a single weak learner.

4.4

Incorporating complexity constraints

Joint optimization of speed and accuracy  requires the minimization of the Lagrangian of (4). This
requires the computation of the functional derivatives

< δRC(F(fk))  g >=

1
|S−

t |Xi

ys

i (cid:26) d

dǫ

LC[C(F(fk + ǫg)(xi)](cid:27)ǫ=0

(34)

i = I(yi = −1). Similarly to boosting  which upper bounds the zero-one loss u(−yf ) by
where ys
the exponential loss e−yf   we rely on a loss that upper-bounds the true complexity. This upper-bound
is a combination of a boosting-style bound u(f + ǫg) ≤ ef +ǫg  and the bound C(f + ǫg) ≤ C(f )+1 
which follows from (24). Using (26) 

LC[C(F(fk + ǫg)(xi)] = LC[αk + γkC(fk + ǫg) + βku(fk + ǫg)]

= αk + γk(C(fk) + 1) + βkefk+ǫg

and  since(cid:8) d

dǫ LC[C(F(fk + ǫg))](cid:9)ǫ=0 = βkefk g 

< δRC(F(fk))  g > =

1
|S−

t |Xi

ys
i ψk

i βk

i g(xi)

i = βk(xi) and ψk

i = efk(xi). The derivative of (4) with respect to the kth stage predictor is

with βk
then

< δL(F(fk))  g > = < δRL(F(fk))  g > +η < δRC(F(fk))  g >

= Xi (cid:18)−

i bk
i

yiwk
|St|

+ η

i ψk
ys
|S−

i βk
i

t | (cid:19) g(xi)

g∗
k = arg max
g∈U

< −δL(F(fk))  g >

(35)
(36)

(37)

(38)

(39)

(40)

(41)

i = e−yiak(xi) and ak and bk given by (14)  (18)  and (20). Given a set of weak learners U 

with wk
the optimal descent direction and step size for the kth stage are then

c∗
k = arg min

i e−yibk
wk

i cg∗

k(xi) +

c∈R( 1

|St|Xi

η
|S−

t |Xi

ys
i ψk

i βk

i ecg∗

k(xi)) .

k 1  c∗

A pair (g∗
k 1) is found among the set O(fk) and another among the set U − O(fk) . The one
that most reduces (4) is selected as the best update for the kth stage and the stage with the largest
impact is updated. This gradient descent procedure is denoted Fast Cascade Boosting (FCBoost).

5 Extensions

FCBoost supports a number of extensions that we brieﬂy discuss in this section.

5.1 Cost Sensitive Boosting

As is the case for AdaBoost  it is possible to use cost sensitive risks in FCBoost. For exam-
ple  the risk of CS-AdaBoost: RL(f ) = EX Y {yce−yf (x)} [12] or Asym-AdaBoost: RL(f ) =
EX Y {e−ycyf (x)} [8]  where yc = CI(y = −1) + (1 − C)I(y = 1) and C is a cost factor.

6

Train Set

Data Set

Face
Car

Pedestrian

pos
9 000
1 000
1 000

neg
9 000
10 000
10 000

Test Set
neg
832
2 000
2 000

pos
832
100
200

L

R

0.36

0.32

0.28

0.24

0

10

RC

20

30

Figure 2: Left: data set characteristics. Right: Trade-off between the error (RL) and complexity (RC) com-
ponents of the risk as η changes in (4).

Table 1: Performance of various classiﬁers on the face  car  and pedestrian test sets.

Method
AdaBoost
ChainBoost

FCBoost (η = 0.02)

Face
RC
50
2.65
4.93

RL
0.20
0.45
0.30

L
1.20
0.50
0.40

RL
0.22
0.65
0.44

Car
RC
50
2.40
5.38

Pedestrian

L
1.22
0.70
0.55

RL
0.35
.052
0.46

RC
50
3.34
4.23

L
1.35
0.59
0.54

5.2 Bootstrapping

Bootstrapping is a procedure to augment the training set  by using false positives of the current
classiﬁer as the training set for the following [11]. This improves performance  but is feasible only
when the bootstrapping procedure does not affect previously rejected examples. Otherwise  the
classiﬁer will forget the previous negatives while learning from the new ones. Since FCBoost learns
all cascade stages simultaneously  and any stage can change after bootstrapping  this condition is
violated. To overcome the problem  rather than replacing all negative examples with false positives 
only a random subset is replaced. The negatives that remain in the training set prevent the classiﬁer
from forgetting about the previous iterations. This method is used to update the training set whenever
the false positive rate of the cascade being learned reaches 50%.

6 Evaluation

Several experiments were performed to evaluate the performance of FCBoost  using face  car  and
pedestrian recognition data sets  from computer vision. In all cases  Haar wavelet features were used
as weak learners. Figure 2 summarizes the data sets.
Effect of η: We started by measuring the impact of η  see (4)  on the accuracy and complexity of
FCBoost cascades. Figure 2 plots the accuracy component of the risk  RL  as a function of the
complexity component  RC  on the face data set  for cascades trained with different η. The leftmost
point corresponds to η = 0.05  and the rightmost to η = 0. As expected  as η decreases the cascade
has lower error and higher complexity. In the remaining experiments we used η = 0.02.
Cascade comparison: Figure 3 (a) repeats the plots of the Lagrangian of the risk shown in Fig-
ure 1  for classiﬁers trained with 50 boosting iterations  on the face data. In addition to AdaBoost
and ChainBoost  it presents the curves of FCBoost with (η = 0.02) and without (η = 0) com-
plexity constraints. Note that  in the latter case  performance is in between those of AdaBoost and
ChainBoost. This reﬂects the fact that FCBoost (η = 0) does produce a cascade  but this cascade
has worse accuracy/complexity trade-off than that of ChainBoost. On the other hand  the inclusion
of complexity constraints  FCBoost (η = 0.02)  produces a cascade with the best trade-off. These
results are conﬁrmed by Table 1  which compares classiﬁers trained on all data sets. In all cases  Ad-
aBoost detectors have the lowest error  but at a tremendous computational cost. On the other hand 
ChainBoost cascades are always the fastest  at the cost of the highest classiﬁcation error. Finally 
FCBoost (η = 0.02) achieves the best accuracy/complexity trade-off: its cascade has the lowest risk
Lagrangian L. It is close to ten times faster than the AdaBoost detector  and has half of the increase
in classiﬁcation error (with respect to AdaBoost) of the ChainBoost cascade. Based on these results 
FCBoost (η = 0.02) was used in the last experiment.

7

0.8

0.7

L

0.6

0.5

0.4

0

FCBoost h =0
FCBoost h =0.02
AdaBoost
ChainBoost

94

90

85

e
t
a
R
 
n
o
i
t
c
e
t
e
D

Viola & Jones
ChainBoost
FloatBoost
WaldBoost
FCBoost

10

20

Iterations

(a)

30

40

50

80

0

25

50

75

100

125

150

Number of False Positives

(b)

Figure 3: a) Lagrangian of the risk for classiﬁers trained with various boosting algorithms. b) ROC of various
detector cascades on the MIT-CMU data set.

Table 2: Comparison of the speed of different detectors.

Method VJ [13]
Evals

8

FloatBoost [5] ChainBoost [15] WaldBoost [9]

18.9

18.1

10.84

[8]
15.45

FCBoost

7.2

Face detection: We ﬁnish with a face detector designed with FCBoost (η = 0.02)  bootstrapping 
and 130K Haar features. To make the detector cost-sensitive  we used CS-AdaBoost with C = 0.99.
Figure 3 b) compares the resulting ROC to those of VJ [13]  ChainBoost [15]  FloatBoost [5] and
WaldBoost [9]. Table 2 presents a similar comparison for the detector speed (average number of
features evaluated per patch). Note the superior performance of the FCBoost cascade in terms of
both accuracy and speed. To the best of our knowledge  this is the fastest face detector reported to
date.

A Recursive form of cascade predictor

Applying (9) recursively to (8)

F = f1u(−f1) + u(f1)F2

= f1u(−f1) + u(f1) [f2u(−f2) + u(f2)F3]
= f1u(−f1) + f2u(f1)u(−f2) + u(f1)u(f2) [f3u(−f3) + u(f3)F4]

k−1

(42)
(43)
(44)

=

fiu(−fi)Yj<i

Xi=1
where T1 k = Pk−1
i=1 fiu(−fi)Qj<i u(fj) and T2 k = Qj<k u(fj) satisfy the recursions of (12).

Combining (46) and (9) then leads to (11). (10) follows from (46) and the initial condition Fm = fm.

u(fj) + Fk Yj<k

= T1 k + T2 kFk

u(fj)

(46)

(45)

B Recursive form of cascade complexity

Let C(fk) be the complexity of evaluating fk. Then

C(F) = C(f1) + u(f1)C(F2)

= C(f1) + u(f1)[C(f2) + u(f2)C(F3)]

k−1

=

Xi=1

C(fi)Yj<i

u(fj) + C(Fk)Yj<k

= P1 k + P2 kC(Fk)

u(fj)

with

P1 k+1 = P1 k + C(fk) P2 k

P2 k+1 = P2 k u(fk)

(47)
(48)

(49)

(50)

(51)

and initial conditions P1 1 = 0  P2 1 = 1. The relationship of (47) is a special case of

(52)
with initial conditions C(Fm) = C(fm) and C(Fm+1) = 0. Combining (52) with (50) leads to (21).

C(Fk) = C(fk) + u(fk)C(Fk+1)

8

References

[1] S. C. Brubaker  M. D. Mullin  and J. M. Rehg. On the design of cascades of boosted ensembles

for face detection. International Journal of Computer Vision  77:65–86  2008.

[2] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting  1997.

[3] J. Friedman  T. Hastie  and R. Tibshirani. Additive logistic regression: a statistical view of

boosting. Annals of Statistics  28:2000  1998.

[4] X. Hou  C.-L. Liu  and T. Tan. Learning boosted asymmetric classiﬁers for object detection.

In IEEE Conference on Computer Vision and Pattern Recognition   pages 330–338  2006.

[5] S. Z. Li and Z. Zhang. Floatboost learning and statistical face detection.

Pattern Analysis and Machine Intelligence  26(9):1112–1123  2004.

IEEE Trans. on

[6] C. Liu and H.-Y. Shum;. Kullback-leibler boosting. In IEEE Conference on Computer Vision

and Pattern Recognition  pages 587–594  2003.

[7] H. Luo. Optimization design of cascaded classiﬁers. In IEEE Conference on Computer Vision

and Pattern Recognition   pages 480–485  2005.

[8] H. Masnadi-Shirazi and N. Vasconcelos. High detection-rate cascades for real-time object
detection. In IEEE International Conference on Computer Vision  volume 2  pages 1–6  2007.
[9] J. Sochman and J. Matas. Waldboost - learning for time constrained sequential detection. In

IEEE Conference on Computer Vision and Pattern Recognition  pages 150–157  2005.

[10] J. Sun  J. M. Rehg  and A. Bobick. Automatic cascade training with perturbation bias. IEEE

Conference on Computer Vision and Pattern Recognition  2:276–283  2004.

[11] K. K. Sung and T. Poggio. Example based learning for view-based human face detection. IEEE

Trans. on Pattern Analysis and Machine Intelligence  20:39–51  1998.

[12] P. Viola and M. Jones. Fast and robust classiﬁcation using asymmetric adaboost and a detector

cascade. In Advances in Neural Information Processing System  pages 1311–1318  2001.

[13] P. Viola and M. Jones. Robust real-time object detection. International Journal of Computer

Vision  57(2):137–154  2004.

[14] J. Wu  S. Brubaker  M. D. Mullin  and J. M. Rehg. Fast asymmetric learning for cascade face

detection. IEEE Trans. on Pattern Analysis and Machine Intelligence  3:369–382  2008.

[15] R. Xiao  L. Zhu  and H.-J. Zhang. Boosting chain learning for object detection.

International Conference on Computer Vision  pages 709–715  2003.

In IEEE

9

,Min Xiao
Yuhong Guo