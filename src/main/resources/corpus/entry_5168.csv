2013,Spectral methods for neural characterization using generalized quadratic models,We describe a set of fast  tractable methods for characterizing neural responses to high-dimensional sensory stimuli using a model we refer to as the generalized quadratic model (GQM). The GQM consists of a low-rank quadratic form followed by a point nonlinearity and exponential-family noise. The quadratic form characterizes the neuron's stimulus selectivity in terms of a set linear receptive fields followed by a quadratic combination rule  and the invertible nonlinearity maps this output to the desired response range. Special cases of the GQM include the 2nd-order Volterra model (Marmarelis and Marmarelis 1978  Koh and Powers 1985) and the elliptical Linear-Nonlinear-Poisson model (Park and Pillow 2011). Here we show that for canonical form" GQMs  spectral decomposition of the first two response-weighted moments yields approximate maximum-likelihood estimators via a quantity called the expected log-likelihood. The resulting theory generalizes moment-based estimators such as the spike-triggered covariance  and  in the Gaussian noise case  provides closed-form estimators under a large class of non-Gaussian stimulus distributions. We show that these estimators are fast and provide highly accurate estimates with far lower computational cost than full maximum likelihood. Moreover  the GQM provides a natural framework for combining multi-dimensional stimulus sensitivity and spike-history dependencies within a single model. We show applications to both analog and spiking data using intracellular recordings of V1 membrane potential and extracellular recordings of retinal spike trains.",Spectral methods for neural characterization using

generalized quadratic models

Il Memming Park∗123  Evan Archer∗13  Nicholas Priebe14  & Jonathan W. Pillow123

1. Center for Perceptual Systems  2. Dept. of Psychology 

3. Division of Statistics & Scientiﬁc Computation  4. Section of Neurobiology 

{memming@austin.  earcher@  nicholas@  pillow@mail.} utexas.edu

The University of Texas at Austin

Abstract

We describe a set of fast  tractable methods for characterizing neural responses
to high-dimensional sensory stimuli using a model we refer to as the generalized
quadratic model (GQM). The GQM consists of a low-rank quadratic function fol-
lowed by a point nonlinearity and exponential-family noise. The quadratic func-
tion characterizes the neuron’s stimulus selectivity in terms of a set linear receptive
ﬁelds followed by a quadratic combination rule  and the invertible nonlinearity
maps this output to the desired response range. Special cases of the GQM include
the 2nd-order Volterra model [1  2] and the elliptical Linear-Nonlinear-Poisson
model [3]. Here we show that for “canonical form” GQMs  spectral decomposi-
tion of the ﬁrst two response-weighted moments yields approximate maximum-
likelihood estimators via a quantity called the expected log-likelihood. The result-
ing theory generalizes moment-based estimators such as the spike-triggered co-
variance  and  in the Gaussian noise case  provides closed-form estimators under a
large class of non-Gaussian stimulus distributions. We show that these estimators
are fast and provide highly accurate estimates with far lower computational cost
than full maximum likelihood. Moreover  the GQM provides a natural framework
for combining multi-dimensional stimulus sensitivity and spike-history dependen-
cies within a single model. We show applications to both analog and spiking data
using intracellular recordings of V1 membrane potential and extracellular record-
ings of retinal spike trains.

1

Introduction

Although sensory stimuli are high-dimensional  sensory neurons are typically sensitive to only a
small number of stimulus features. Linear dimensionality-reduction methods seek to identify these
features in terms of a subspace spanned by a small number of spatiotemporal ﬁlters. These ﬁlters 
which describe how the stimulus is integrated over space and time  can be considered the ﬁrst stage in
a “cascade” model of neural responses. In the well-known linear-nonlinear-Poisson (LNP) cascade
model  ﬁlter outputs are combined via a nonlinear function to produce an instantaneous spike rate 
which generates spikes via an inhomogeneous Poisson process [4  5].
The most popular methods for dimensionality reduction with spike train data involve the ﬁrst two
moments of the spike-triggered stimulus distribution: (1) the spike-triggered average (STA) [7–9];
and (2) major and minor eigenvectors of spike-triggered covariance (STC) matrix [10  11]1. STC
analysis can be described as a spectral method because the estimate is obtained by eigenvector

∗ These authors contributed equally.
1Related moment-based estimators have also appeared in the statistics literature under the names “inverse
regression” and “sufﬁcient dimensionality reduction”  although the connection to STA and STC analysis does
not appear to have been noted previously [12  13].

1

Figure 1: Schematic of generalized quadratic model (GQM) for analog or spike train data.

decomposition of an appropriately deﬁned matrix. Compared to likelihood-based methods  spectral
methods are generally computationally efﬁcient and devoid of (non-global) local optima.
Recently  Park and Pillow [3] described a connection between STA/STC analysis and maximum
likelihood estimators based on a quantity called the expected log-likelihood (EL). The EL results
from replacing the nonlinear term in the log-likelihood and with its expectation over the stimulus
distribution. When the stimulus is Gaussian  the EL depends only on moments (mean spike rate 
STA  STC  and stimulus mean and covariance) and leads to a closed-form spectral estimate for LNP
ﬁlters  which has STC analysis as a special case. More recently  Ramirez and Paninski derived EL-
based estimators for the linear Gaussian model and proposed fast EL-based inference methods for
generalized linear models (GLMs) [14].
Here  we show that the EL framework can be extended to a more general class that we refer to
as the generalized quadratic model (GQM). The GQM represents a straightforward extension of
the generalized linear model GLM [15  16] wherein the linear predictor is replaced by a quadratic
function (Fig. 1). For Gaussian and Poisson GQMs  we derive computationally efﬁcient EL-based
estimators that apply to a variety of non-Gaussian stimulus distributions; this substantially extends
previous work on the conditions of validity for moment-based estimators [7 17–19]. In the Gaussian
case  the EL-based estimator has a closed form solution that relies only on the ﬁrst two response-
weighted moments and the ﬁrst four stimulus moments.
In the Poisson case  GQMs provide a
natural synthesis of models that have multiple ﬁlters (i.e.  where the response depends on multiple
projections of the stimulus) and dependencies on spike history. We show that spectral estimates of a
low-dimensional feature space are nearly as accurate as maximum likelihood estimates (for GQMs
without spike-history)  and demonstrate the applicability of GQMs for both analog and spiking data.

2 Generalized Quadratic Models

We begin by brieﬂy reviewing of the class of models known as GLMs  which includes the single-
ﬁlter LNP model  and the Wiener model from the systems identiﬁcation literature. A GLM has three
basic components: a linear stimulus ﬁlter  an invertible nonlinearity (or “inverse link” function) 
and an exponential-family noise model. The GLM describes the conditional response y to a vector
stimulus x as:

(1)
where w is the ﬁlter  f is the nonlinearity  and P(λ) denotes a noise distribution function with
mean λ. From the standpoint of dimensionality reduction  the GLM makes the strong modeling
assumption that response y depends upon x only via its one-dimensional projection onto w.
At the other end of the modeling spectrum sits the very general “multiple ﬁlter” linear-nonlinear
(LN) cascade model  which posits that the response depends on a p-dimensional projection of
the stimulus  represented by a bank of ﬁlters {wi}p
i=1  and combined via some arbitrary multi-
dimensional function f : Rp → R:

y|x ∼ P(f (w(cid:62)x)) 

(2)
Spike-triggered covariance analysis and related methods provide low-cost estimates of the ﬁlters
{wi} under Poisson or Bernoulli noise models  but only under restrictive conditions on the stimulus

p x)).

y|x ∼ P(f (w(cid:62)

1 x  . . .   w(cid:62)

2

responseanalogspikesorstimulus...nonlinearfunctionGeneralized Quadratic Modelnoisequadraticlinearfiltersrecurrent filtersdistribution (e.g.  elliptical symmetry) and some weak conditions on f [17  19]. Semi-parametric
estimators like “maximally informative dimensions” (MID) eliminate these restrictions [20]  but do
not practically scale beyond two or three ﬁlters without additional modeling assumptions [21].
The generalized quadratic model (GQM) provides a tractable middle ground between the GLM and
general multi-ﬁlter LN models. The GQM allows for multi-dimensional stimulus dependence  yet
restricts the nonlinearity to be a transformed quadratic function [22–25]. The GQM can be written:
(3)
where Q(x) = x(cid:62)Cx + b(cid:62)x + a denotes a quadratic function of x  governed by a (possibly low-
rank) symmetric matrix C  a vector b  and a scalar a. Note that the GQM may be regarded as a
GLM in the space of quadratically transformed stimuli [6]  although this approach does not allow
Q(x) to be parametrized directly in terms of a projection onto a small number of linear ﬁlters.
In the following  we show that the elliptical-LNP model [3] is a GQM with Poisson noise  and make
a detailed study of canonical GQMs with Gaussian noise. We show that the maximum-EL estimates
for C  b  and a have similar forms for both Gaussian and Poisson GQMs  and that the eigenspectrum
of C provides accurate estimates of a neuron’s low-dimensional feature space. Finally  we show that
the GQM provides a natural framework for combining multi-dimensional stimulus sensitivity with
dependencies on spike train history or other response covariates.

y|x ∼ P(f (Q(x))) 

3 Estimation with expected log-likelihoods

The expected log-likelihood is a quantity that approximates log-likelihood but can be computed very
efﬁciently using moments. It exists for any GQM or GLM with “canonical” nonlinearity (or link
function). The canonical nonlinearity for an exponential-family noise distribution has the special
property that it allows the log-likelihood to be written as the sum of two terms: a term that depends
linearly on the responses {yi}  and a second (nonlinear) term that depends only on the stimuli
{xi} and parameters θ. The expected log-likelihood (EL) results from replacing the nonlinear term
with its expectation over the stimulus distribution P (x)  which in neurophysiology settings is often
known a priori to the experimenter. Maximizing the EL results in maximum expected log-likelihood
(MEL) estimators that have very low computational cost while achieving nearly the accuracy of
full maximum likelihood (ML) estimators. Spectral decompositions derived from the EL provide
estimators that generalize STA/STC analysis. In the following  we derive MEL estimators for three
special cases—two for the Gaussian noise model  and one for the Poisson noise model.

3.1 Gaussian GQMs

Gaussian noise provides a natural model for analog neural response variables like membrane poten-
tial or ﬂuorescence. The canonical nonlinearity for Gaussian noise is the identity function  f (x) = x.
The the canonical-form Gaussian GQM can therefore be written: y|x ∼ N (Q(x)  σ2). Given a
dataset {xi  yi}N
L = − 1
2σ2

i=1  the log-likelihood per sample is:
(Q(xi) − yi)2 = − 1
2σ2

(cid:0)−2Q(xi)yi + Q(xi)2(cid:1) + const

1
N

1
N

(cid:88)
(cid:32)
−2(cid:0)Tr(CΛ) + µ(cid:62)b + a¯y(cid:1) +

i

(cid:33)

= − 1
2σ2

Q(xi)2

+ const 

(cid:80)

where σ2 is the noise variance  const is a parameter-independent constant  ¯y = 1
i yi is the mean
N
response  and µ and Λ denote cross-correlation statistics that we will refer to (in a slight abuse of
terminology) as the response triggered average and response-triggered covariance:

N(cid:88)

i=1

µ =

1
N

yixi (“RTA”)

Λ =

1
N

yixixi

(cid:62) (“RTC”).2

The expected log-likelihood results from replacing the troublesome nonlinear term 1
i Q(xi)2
N
by its expectation over the stimulus distribution. This is justiﬁed by the law of large numbers  which

2When responses yi are spike counts  these correspond to the STA and STC.

3

(cid:88)
(cid:88)

i

i

1
N

N(cid:88)

i=1

(4)

(5)

(cid:80)

(cid:80)

(6)

asserts that 1
N
this leads to the per-sample expected log-likelihood [3  14]  which is deﬁned:

i Q(xi)2 converges to EP (x)[Q(x)2] asymptotically. Leaving off the const term 

˜L = − 1

(cid:0)−2(cid:0)Tr(CΛ) + µ(cid:62)b + a¯y(cid:1) + E[Q(x)2](cid:1) .
E[Q(x)2] = 2 Tr(cid:8)(CΣ)2(cid:9) + Tr(bT Σb) + (Tr(CΣ) + a)2.

2σ2

Gaussian stimuli
If the stimuli are drawn from a Gaussian distribution  x ∼ N (0  Σ)  then we have (from [26]):

(7)
The EL is concave in the parameters a  b  C  so we can obtain the MEL estimates by ﬁnding the
stationary point:
˜L = − 1
˜L = − 1
˜L = − 1
2σ2

2σ2 (−2¯y + 2 (Tr(CΣ) + a)) = 0 =⇒ amel = ¯y − Tr(CmelΣ))
2σ2 (−2µ + 2Σb) = 0 =⇒
(cid:0)−2Λ +(cid:0)4ΣCΣ + 2¯yΣ(cid:1)(cid:1) = 0 =⇒ Cmel =

(cid:0)Σ−1ΛΣ−1 − ¯yΣ−1(cid:1) (10)

bmel = Σ−1µ

∂
∂a
∂
∂b
∂
∂C

(8)

(9)

1
2

Note that this coincides with the moment-based estimate for the 2nd-order Volterra model [2].
Axis-symmetric stimuli
More generally  we can derive the MEL estimator for stimuli with arbitrary axis-symmetric dis-
tributions with ﬁnite 4th-order moments. Axis-symmetric distributions exhibit invariance under
reﬂections around each axis  that is  P (x1  . . .   xd) = P (ρ1x1  . . .   ρdxd) for any ρi ∈ {−1  1}.
The class of axis-symmetric distributions subsumes both radially symmetric and independent prod-
uct distributions. However  axis symmetry is a strictly weaker condition; signiﬁcantly  marginals
need not be identically distributed.
To simplify derivation of the MEL estimator for axis-symmetric stimuli  we take the derivative of
Q(x) with respect to (a  b  C) before taking the expectation. Derivatives with respect to model
parameters are given by ∂E[Q(x)2]

. For each θi  we solve the equation 

= E(cid:104)

(cid:105)
∂(cid:0)Tr(CΛ) + µ(cid:62)b + a¯y(cid:1)

2Q(x) ∂Q(x)
∂θi

∂θi

(cid:21)

(cid:20)

∂θi

+ 2E

Q(x)

∂Q(x)

∂θi

= 0.

∂ ˜L
∂θi

= −2

From derivatives w.r.t. a  b  and C  respectively  we obtain conditions for the MEL estimates:

¯y = E [Q(x)] = a + b(cid:62)E[x] + Tr(CE[xx(cid:62)])
µ = E [Q(x)x] = aE[x] + b(cid:62)E[xx(cid:62)] +

Λ = E(cid:2)Q(x)xx(cid:62)(cid:3) = aE[xx(cid:62)] +

(cid:88)

i j

biE[xixx(cid:62)] +

(cid:88)

CijE[xixjx]

(cid:88)

i

i j

CijE[xixjxx(cid:62)]

(cid:3).
Mij = E(cid:2)x2
(cid:88)

i x2
j

ij

where the subindices within the sums are for components. Due to axis symmetry  E[x]  E[xixjxk]
and E[xix3
j ] are all zero for distinct indices. Thus  the MEL estimates for a and b are identical to the
Gaussian case given above. If we further assume that the stimulus is whitened so that E[xx(cid:62)] = I 
sufﬁcient stimulus statistics are the 4th order even moments  which we represent with the matrix

In general  when the marginals are not identical but the joint distribution is axis-symmetric 

CijE[xixjxx(cid:62)] =

Cii diag(x2

i x2

1 ···   x2

i x2

d) +

CijMijeie(cid:62)

j

(11)

where 1 is a vector of 1’s  ei is the standard basis  and ◦ denotes the Hadamard product. We can solve
these sets of linear equations for the diagonal terms and off-diagonal terms separately obtaining 

= diag(1(cid:62)(I ◦ C)M ) + C ◦ M ◦ (11(cid:62) − I).

(cid:88)

i(cid:54)=j

(cid:88)

i

(cid:40) Λij

 

4

[Cmel]ij =

2Mij

Ω(M − 11(cid:62))−1 

i (cid:54)= j
i = j

(12)

Figure 2: Maximum expected log-likelihood (MEL) estimators for a Gaussian GQM under different
assumptions about the stimulus distribution. (left) Axis-symmetric stimulus distribution in 2D. The
horizontal axis is a (symmetric) mixture of Gaussian  and the vertical axis is a uniform distribution.
Red dots indicate samples from the distribution. (right) Response prediction based on various ˆC
estimated using eq. 10  eq. 14  and eq. 12. Performance is evaluated on a cross-validation test set
with no noise for each C  and we see a huge loss in performance as a result of incorrect assumption
about the stimulus distribution.

where Ω = diag(1(cid:62)(I ◦ Λ) − ¯y1(cid:62)).
For the special case when the marginal distributions are identical  we note that

E[x(cid:62)Cx(xx(cid:62))] = µ22 Tr(C)I + (µ4 − µ22)C ◦ I + 2µ22C ◦ (11(cid:62) − I)

(13)

2] = M1 2 and µ4 = E[x4

1x2

1] = M1 1. This gives the simpliﬁed formula (also

where µ22 = E[x2
given in [27]):

(cid:40) Λij

 
2µ22
Λii−¯y
µ4−µ22

[Cmel]ij =

i (cid:54)= j
i = j

 

(14)

When the stimulus is not Gaussian or the marginals not identical  the estimates obtained from
(eq. 10) and (eq. 14) are not consistent. In this case  the general axis-symmetric estimate (eq. 12)
gives much better performance  as we illustrate with a simulated example in Fig. 2.

3.2 Poisson GQM

Poisson noise provides a natural model for discrete events like spike counts  and extends easily to
point process models for spike trains. The canonical nonlinearity for Poisson noise is exponential 
f (x) = exp(x)  so the canonical-form Poisson GQM is: y|x ∼ Poiss(exp(Q(x))).
Ignoring
irrelevant constants  the log-likelihood per sample is

L = 1

N

yi log(exp(Q(xi))) − 1

exp(Q(xi))

(cid:88)

i

= Tr(CΛ) + µ(cid:62)b + a¯y − 1

N

exp(Q(xi)) 

(15)

where ¯y  µ and Λ denote mean response  STA  and STC  as given above (eq. 5). We obtain the
EL for a Poisson GQM by replacing the term 1
N
P (x). Under a zero-mean Gaussian stimulus distribution with covariance Σ  the closed-form MEL
estimates are (from [3]):

(cid:80) exp(Q(xi)) by its expectation with respect to

i

µ 

Λ + 1

bmel =

(16)
¯y 2µµ(cid:62) is invertible. Note that the MEL estimator combines information
where we assume that Λ + 1
from µ and Λ  unlike standard STA and STC-based estimates  which maximize EL only when either
b or C is zero (respectively). Park and Pillow 2011 used Poisson EL in conjunction with a log-prior
to obtain approximate Bayesian estimates  an approach referred to as Bayesian STC [3].

Cmel = 1
2

Λ + 1

(cid:16)

¯y 2µµ(cid:62)(cid:17)−1

(cid:18)

(cid:16)

Σ−1 − ¯y

¯y 2µµ(cid:62)(cid:17)−1(cid:19)

 

(cid:88)

i

N

(cid:88)

5

timeneural responsetrueresponseiid axis-sym [r2 = 0.894]general axis-sym [r2 = 0.99]Gaussian [r2 = 0.424]2D stimulus distributionassumed stimulus distributionFigure 3: Rank-1 quadratic
ﬁlter
reconstruction perfor-
mance. Both rank-1 models
were optimized using conju-
gate gradient descent. (Left)
l1 distance from the ground
truth ﬁlter. (Right) Computa-
tion time for the optimization.

Mixture-of-Gaussians stimuli
Results for Gaussian stimuli extend naturally to mixtures of Gaussians  which can be used to approx-
imate arbitrary stimulus distributions. The EL for mixture-of-Gaussian stimuli can be computed
i αjN (µj  Σj) with

simply via the linearity of expectation. For stimuli drawn from a mixture(cid:80)
mixing weights(cid:80)

j αj = 1  the EL is

˜L = Tr(CΛ) + µ(cid:62)b + a¯y −(cid:88)

αjEN (µj  Σj )[eQ(x)] 

i

(cid:16)

(17)

(18)

(cid:17)

.

(b+2Cµj )

where the Gaussian expectation terms are given by

EN (µj  Σj )[eQ(x)] =

1

|I−2CΣj| 1

2

e

a+µ(cid:62)

j Cµj +b(cid:62)µj + 1

2 (b+2Cµj )(cid:62)(Σ

−1
j −2C)

−1

Although the MEL estimator does not have a closed analytic form in this case  the EL can be ef-
ﬁciently optimized numerically  as it still depends on the responses only via the spike-triggered
moments ¯y  µ and Λ  and on the stimuli only via the mean  covariance  and mixing weight of each
Gaussian.

4 Spectral estimation for low-dimensional models

4.1 Low-rank parameterization

We have so far focused upon MEL estimators for the parameters a  b  and C. These results have a
natural mapping to dimensionality reduction methods. Under the GQM  a low-dimensional stimulus
dependence is equivalent to having a low-rank C. If C = BB(cid:62) for some d× p matrix B  we have a
p-ﬁlter model (or p + 1 ﬁlter model if the linear term b is not spanned by the columns of B). We can
obtain spectral estimates of a low-dimensional GQM by performing an eigenvector decomposition
of Cmel and selecting the eigenvectors corresponding to the largest p eigenvalues. The eigenvectors
of Cmel also make natural initializers for maximization of the full GQM likelihood.
In Fig. 3  we show the results of three different methods for recovering a simulated rank-1 GQM
with Poisson noise: (1) the largest eigenvector of Cmel  (2) numerically maximizing the expected
log-likelihood for a rank-1 GQM (i.e.  with C parametrized as a rank-1 matrix)  and (3) maximizing
the (full) likelihood for a rank-1 GQM. Although the difference in performance between expected
and full GQM log-likelihood is negligible  there is a drastic difference in optimization time com-
plexity between the full and expected log-likelihood. The expected log-likelihood only requires
computation of the sufﬁcient statistics  while the full ML estimate requires a full pass through the
dataset for each evaluation of the log-likelihood. Thus  the expected log-likelihood offers a fast yet
accurate estimate for C. In the following section we show that  asymptotically  the eigenvectors of
Cmel span the “correct” (in an appropriate sense) low-dimensional subspace.

4.2 Consistency of subspace estimates
If the conditional probability y|x = y|β(cid:62)x for a matrix β  the neural feature space is spanned by the
columns of β. As a generalization of STC  we introduce moment-based dimensionality reduction

6

10410510010-1(cid:31)lter estimation error104105012optimization timesecondsL1 error# of samples# of samplesMELE (1st eigenvector) rank−1 MELErank−1 MLFigure 4: GQM ﬁt and prediction for intracellular recording in cat V1 with a trinary noise stimulus.
(A) On top  estimated linear (b) and quadratic (w1 and w2) ﬁlters for the GQM  lagged by 20ms. On
bottom  the empirical marginal nonlinearities along each dimension (black) and model prediction
(red). (B) Cross-validated model prediction (red) and n = 94 recordings with repeats of identical
stimulus (light grey) along with their mean (black). Reported performance metric (r2 = 0.55) is for
prediction of the mean response.

2 ΛΣ− 1

2 µ and eigenvectors of Σ− 1

techniques that recover (portions of) β and show the relationship of these techniques to the MEL
estimators of GQM.
We propose to use Σ− 1
2 (whose eigenvalues are signiﬁcantly
smaller or larger than 1) as the feature space basis. When the response is binary  this coincides
with the traditional STA/STC analysis  which is provably consistent only in the case of stimuli
drawn from a spherically symmetric (for STA) or independent Gaussian distribution (for STC) [5].
Below  we argue that this procedure can identify the subspace when y has mean f (β(cid:62)x) with ﬁnite
variance  f is some function  and the stimulus distribution is zero-mean with white covariance  i.e. 
E[x] = 0 and E[xxT ] = I.

First  note that by the law of large numbers  Λ → E[y xxT ] = E(cid:2)yE[xxT|y](cid:3) . Let Ψ = ββT be a

projection operator to the feature space  and Ψ⊥ = I − Ψ be the perpendicular space. We follow the
discussion in [12  13] regarding the related “sliced regression” literature. Recalling that E[X] = 0 
we can exploit the independence of Ψ⊥x and y to ﬁnd 

E(cid:2)xx(cid:62)|y = ξ(cid:3) = E(cid:2)(Ψ + Ψ⊥)xx(cid:62)(Ψ + Ψ⊥)|y = ξ(cid:3)

= ΨE(cid:2)xx(cid:62)|y = ξ(cid:3) Ψ + Ψ⊥E(cid:2)xx(cid:62)(cid:3) Ψ⊥ = ΨE(cid:2)xx(cid:62)|y = ξ(cid:3) Ψ + Ψ⊥

thus  E(cid:2)yxx(cid:62)(cid:3) = ΨE(cid:2)yxx(cid:62)(cid:3) Ψ + E[y]Ψ⊥ and therefore the eigenvectors of E(cid:2)yxx(cid:62)(cid:3) whose
GQM  the eigenvectors of E(cid:2)yxx(cid:62)(cid:3) are closely related to the expected log-likelihood estimators we

eigenvalues signiﬁcantly differ from E[y] span a subspace of the range of Ψ. Effective estimation
of the subspace depends critically on both the stimulus distribution and the form of f. Under the

derived earlier. Indeed  those eigenvectors of eq. 10  eq. 12 and eq. 16 whose associated eigenvalues
differ signiﬁcantly from zero span precisely the same space.

5 Results

5.1

Intracellular membrane potential

We ﬁt a Gaussian GQM to intracellular recordings of membrane potential from a neuron in cat V1 
using a 2D spatiotemporal “ﬂickering bars” stimulus aligned with the cell’s preferred orientation
(Fig. 4). The recorded time-series is a continuous signal  so the Gaussian GQM provides an appro-
priate noise model. The recorded voltage was median-ﬁltered (to remove spikes) and down-sampled
to a 10 ms sample rate. We ﬁt the GQM to a 21.6 minute recording of responses to non-repeating
trinary noise stimulus . We validated the model using responses to 94 repeats of a 1 second frozen
noise stimulus. Panel (B) of Fig. 4 illustrates the GQM prediction on cross-validation data.
Although the cell was classiﬁed as “simple”  meaning that its response is predominately linear  the
GQM ﬁt reveals two quadratic ﬁlters that also inﬂuence the membrane potential response. The GQM
captures a substantial percentage of the variance in the mean response  systematically outperforming
the GLM in terms of r2 (GQM:55% vs. GLM:50%).

7

−101−2024mVbx−10100.40.8mVw1x−10100.4mVw2xtime (10ms/frame)linear filter b space (0.70 deg/bar)quadratic filter w1space (0.70 deg/bar)quadratic filter w2space (0.70 deg/bar)meanmodel (r2 = 0.55)AB0.20.40.60.81−60−55−50−45time (s)Membrane Potential (mV)Prediction Performance (test data) Figure 5: (left) GLM and GQM ﬁlters ﬁt to spike responses of a retinal ganglion cell stimulated
with a 120 Hz binary full ﬁeld noise stimulus [28]. The GLM has only linear stimulus and spike
history ﬁlters (top left) while the GQM contains all four ﬁlters. Each plot shows the exponentiated
ﬁlter  so the ordinate has units of gain  and ﬁlters interact multiplicatively. Quadratic ﬁlter outputs are
squared and then subtracted from other inputs  giving them a suppressive effect on spiking (although
(right) Cross-validated rate prediction averaged over 167
quadratic excitation is also possible).
repeated trials.

5.2 Retinal ganglion spike train
The Poisson GLM provides a popular model for neural spike trains due to its ability to incorporate
dependencies on spike history (e.g.  refractoriness  bursting  and adaptation). These dependencies
cannot be captured by models with inhomogeneous Poisson output like the multi-ﬁlter LNP model
(which is also implicit in information-theoretic methods like MID [21]). The GLM achieves this
by incorporating a one-dimensional linear projection of spike history as an input to the model. In
general  however  a spike train may exhibit dependencies on more than one linear projection of spike
history.
The GQM extends the GLM by allowing multiple stimulus ﬁlters and multiple spike-history ﬁlters.
It can therefore capture multi-dimensional stimulus sensitivity (e.g.  as found in complex cells) and
produce dynamic spike patterns unachievable by GLMs. We ﬁt a Poisson GQM with a quadratic
history ﬁlter to data recorded from a retinal ganglion cell driven by a full-ﬁeld white noise stimu-
lus [28]. For ease of comparison  we ﬁt a Poisson GLM  then added quadratic stimulus and history
ﬁlters  initialized using a spectral decomposition of the MEL estimate (eq. 16) and then optimized by
numerical ascent of the full log-likelihood. Both quadratic ﬁlters (which enter with negative sign) 
have a suppressive effect on spiking (Fig. 5). The quadratic stimulus ﬁlter induces strong suppres-
sion at a delay of 5 frames  while the quadratic spike history ﬁlter induces strong suppression during
a 50 ms window after a spike.

6 Conclusion
The GQM provides a ﬂexible class of probabilistic models that generalizes the GLM  the 2nd-
order Volterra model  the Wiener model  and the elliptical-LNP model [3]. Unlike the GLM  the
GQM allows multiple stimulus and history ﬁlters and yet remains tractable for likelihood-based
inference. We have derived expected log-likelihood estimators in a general form that reveals a deep
connection between likelihood-based and moment-based inference methods. We have shown that
GQM performs well on neural data  both for discrete (spiking) and analog (voltage) data. Although
we have discussed the GQM in the context of neural systems  but we believe it (and EL-based
inference methods) will ﬁnd applications in other areas such as signal processing and psychophysics.

Acknowledgments
We thank the L. Paninski and A. Ramirez for helpful discussions and V. J. Uzzell and E. J. Chichilnisky for
retinal data. This work was supported by Sloan Research Fellowship (JP)  McKnight Scholar’s Award (JP) 
NSF CAREER Award IIS-1150186 (JP)  NIH EY019288 (NP)  and Pew Charitable Trust (NP).

8

0102030400.20.61time (stimulus frames)012010020011.41.8time (ms)0.20.61spike historystimulus filtergaingainlinearquadratic1 secrate prediction (test data)GQM ( )GLM ( )References

[1] P. Z. Marmarelis and V. Marmarelis. Analysis of physiological systems: the white-noise approach. Plenum

Press  New York  1978.

[2] Taiho Koh and E. Powers. Second-order volterra ﬁltering and its application to nonlinear system identiﬁ-

cation. IEEE Transactions on Acoustics  Speech  and Signal Processing  33(6):1445–1455  1985.

[3] Il Memming Park and Jonathan W. Pillow. Bayesian spike-triggered covariance analysis. Advances in

Neural Information Processing Systems 24  pp 1692–1700  2011.

[4] E. P. Simoncelli  J. W. Pillow  L. Paninski  and O. Schwartz. Characterization of neural responses with
stochastic stimuli. The Cognitive Neurosciences  III  chapter 23  pp 327–338. MIT Press  Cambridge 
MA  October 2004.

[5] L. Paninski. Maximum likelihood estimation of cascade point-process neural encoding models. Network:

Computation in Neural Systems  15:243–262  2004.

[6] S. Gerwinn  J. H. Macke  M. Seeger  and M. Bethge. Bayesian inference for spiking neuron models with

a sparsity prior. Advances in Neural Information Processing Systems  pp 529–536  2008.

[7] J. Bussgang. Crosscorrelation functions of amplitude-distorted gaussian signals. RLE Technical Reports 

216  1952.

[8] E. deBoer and P. Kuyper. Triggered correlation. IEEE Transact. Biomed. Eng.  15  pp 169–179  1968.
[9] E. J. Chichilnisky. A simple white noise analysis of neuronal light responses. Network: Computation in

Neural Systems  12:199–213  2001.

[10] R. R. de Ruyter van Steveninck and W. Bialek. Real-time performance of a movement-senstivive neuron
in the blowﬂy visual system: coding and information transmission in short spike sequences. Proc. R. Soc.
Lond. B  234:379–414  1988.

[11] O. Schwartz  J. W. Pillow  N. C. Rust  and E. P. Simoncelli. Spike-triggered neural characterization. J.

Vision  6(4):484–507  7 2006.

[12] RD Cook and S. Weisberg. Comment on ”sliced inverse regression for dimension reduction” by k.-c. li.

Journal of the American Statistical Association  86:328–332  1991.

[13] Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American Statistical

Association  86(414):316–327  1991.

[14] Alexandro D. Ramirez and Liam Paninski. Fast inference in generalized linear models via expected

log-likelihoods. Journal of Computational Neuroscience  pp 1–20  2013.

[15] W. Truccolo  U. T. Eden  M. R. Fellows  J. P. Donoghue  and E. N. Brown. A point process framework
for relating neural spiking activity to spiking history  neural ensemble and extrinsic covariate effects. J.
Neurophysiol  93(2):1074–1089  2005.

[16] J. W. Pillow  J. Shlens  L. Paninski  A. Sher  A. M. Litke  and E. P. Chichilnisky  E. J. Simoncelli. Spatio-
temporal correlations and visual signaling in a complete neuronal population. Nature  454:995–999 
2008.

[17] L. Paninski. Convergence properties of some spike-triggered analysis techniques. Network: Computation

in Neural Systems  14:437–464  2003.

[18] J. W. Pillow and E. P. Simoncelli. Dimensionality reduction in neural models: An information-theoretic

generalization of spike-triggered average and covariance analysis. J. Vision  6(4):414–428  4 2006.

[19] In´es Samengo and Tim Gollisch. Spike-triggered covariance: geometric proof  symmetry properties  and

extension beyond gaussian stimuli. Journal of Computational Neuroscience  34(1):137–161  2013.

[20] Tatyana Sharpee  Nicole C. Rust  and William Bialek. Analyzing neural responses to natural signals:

maximally informative dimensions. Neural Comput  16(2):223–250  Feb 2004.

[21] R. S. Williamson  M. Sahani  and J. W. Pillow. Equating information-theoretic and likelihood-based

methods for neural dimensionality reduction. arXiv:1308.3542 [q-bio.NC]  2013.

[22] J. D. Fitzgerald  R. J. Rowekamp  L. C. Sincich  and T. O. Sharpee. Second order dimensionality reduction

using minimum and maximum mutual information models. PLoS Comput Biol  7(10):e1002249  2011.

[23] K. Rajan and W. Bialek. Maximally informative ”stimulus energies” in the analysis of neural responses

to natural signals. arXiv:1201.0321v1 [q-bio.NC]  2012.

[24] James M. McFarland  Yuwei Cui  and Daniel A. Butts. Inferring nonlinear neuronal computation based

on physiologically plausible inputs. PLoS Comput Biol  9(7):e1003143+  July 2013.

[25] L. Theis  A. M. Chagas  D. Arnstein  C. Schwarz  and M. Bethge. Beyond glms: A generative mixture

modeling approach to neural system identiﬁcation. PLoS Computational Biology  Nov 2013. in press.

[26] A. M. Mathai and S. B. Provost. Quadratic forms in random variables: theory and applications. M.

Dekker  1992.

[27] Y. S. Cho and E. J. Powers. Estimation of quadratically nonlinear systems with an i.i.d. input. [Pro-
ceedings] ICASSP 91: 1991 International Conference on Acoustics  Speech  and Signal Processing pp
3117–3120 vol.5. IEEE  1991.

[28] V. J. Uzzell and E. J. Chichilnisky. Precision of spike trains in primate retinal ganglion cells. Journal of

Neurophysiology  92:780–789  2004.

9

,Il Memming Park
Evan Archer
Nicholas Priebe
Jonathan Pillow