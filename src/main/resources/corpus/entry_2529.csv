2016,Identification and Overidentification of Linear Structural Equation Models,In this paper  we address the problems of identifying linear structural equation models and discovering the constraints they imply. We first extend the half-trek criterion to cover a broader class of models and apply our extension to finding testable constraints implied by the model. We then show that any semi-Markovian linear model can be recursively decomposed into simpler sub-models  resulting in improved identification and constraint discovery power. Finally  we show that  unlike the existing methods developed for linear models  the resulting method subsumes the identification and constraint discovery algorithms for non-parametric models.,Identiﬁcation and Overidentiﬁcation of

Linear Structural Equation Models

Bryant Chen

University of California  Los Angeles

Computer Science Department

Los Angeles  CA  90095-1596  USA

Abstract

In this paper  we address the problems of identifying linear structural equation
models and discovering the constraints they imply. We ﬁrst extend the half-trek
criterion to cover a broader class of models and apply our extension to ﬁnding
testable constraints implied by the model. We then show that any semi-Markovian
linear model can be recursively decomposed into simpler sub-models  resulting
in improved identiﬁcation and constraint discovery power. Finally  we show that 
unlike the existing methods developed for linear models  the resulting method
subsumes the identiﬁcation and constraint discovery algorithms for non-parametric
models.

1

Introduction

Many researchers  particularly in economics  psychology  and the social sciences  use linear structural
equation models (SEMs) to describe the causal and statistical relationships between a set of variables 
predict the effects of interventions and policies  and to estimate parameters of interest. When modeling
using linear SEMs  researchers typically specify the causal structure (i.e. exclusion restrictions and
independence restrictions between error terms) from domain knowledge  leaving the structural
coefﬁcients (representing the strength of the causal relationships) as free parameters to be estimated
from data. If these coefﬁcients are known  then total effects  direct effects  and counterfactuals
can be computed from them directly (Balke and Pearl  1994). However  in some cases  the causal
assumptions embedded in the model are not enough to uniquely determine one or more coefﬁcients
from the probability distribution  and therefore  cannot be estimated using data. In such cases  we say
that the coefﬁcient is not identiﬁed or not identiﬁable1.
In other cases  a coefﬁcient may be overidentiﬁed in addition to being identiﬁed  meaning that there
are at least two minimal sets of logically independent assumptions in the model that are sufﬁcient
for identifying a coefﬁcient  and the identiﬁed expressions for the coefﬁcient are distinct functions
of the covariance matrix (Pearl  2004). As a result  the model imposes a testable constraint on the
probability distribution that the two (or more) identiﬁed expressions for the coefﬁcient are equal.
As compact and transparent representations of the model’s structure  causal graphs provide a con-
venient tool to aid in the identiﬁcation of coefﬁcients. First utilized as a causal inference tool by
Wright (1921)  graphs have more recently been applied to identify causal effects in non-parametric
causal models (Pearl  2009) and enabled the development of causal effect identiﬁcation algorithms
that are complete for non-parametric models (Huang and Valtorta  2006; Shpitser and Pearl  2006).
These algorithms can be applied to the identiﬁcation of coefﬁcients in linear SEMs by identifying
non-parametric direct effects  which are closely related to structural coefﬁcients (Tian  2005; Chen
and Pearl  2014). Algorithms designed speciﬁcally for the identiﬁcation of linear SEMs were de-

1We will also use the term “identiﬁed" with respect to individual variables and the model as a whole.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

veloped by Brito and Pearl (2002)  Brito (2004)  Tian (2005  2007  2009)  Foygel et al. (2012)  and
Chen et al. (2014).
Graphs have also proven to be valuable tools in the discovery of testable implications. It is well
known that conditional independence relationships can be easily read from the causal graph using d-
separation (Pearl  2009)  and Kang and Tian (2009) gave a procedure for linear SEMs that enumerates
a set of conditional independences that imply all others. In non-parametric models without latent
variables or correlated error terms  these conditional independence constraints represent all of the
testable implications of the model (Pearl  2009). In models with latent variables and/or correlated
error terms  there may be additional constraints implied by the model. These non-independence
constraints  often called Verma constraints  were ﬁrst noted by Verma and Pearl (1990)  and Tian
and Pearl (2002b) and Shpitser and Pearl (2008) developed graphical algorithms for systematically
discovering such constraints in non-parametric models. In the case of linear models  Chen et al. (2014)
applied their aforementioned identiﬁcation method to the discovery of overidentifying constraints 
which in some cases are equivalent to the non-parametric constraints enumerated in Tian and Pearl
(2002b) and Shpitser and Pearl (2008).
Surprisingly  naively applying algorithms designed for non-parametric models to linear models
enables the identiﬁcation of coefﬁcients and constraints that the aforementioned methods developed
for linear models are unable to  despite utilizing the additional assumption of linearity. In this paper 
we ﬁrst extend the half-trek identiﬁcation method of Foygel et al. (2012) and apply it to the discovery
of half-trek constraints  which generalize the overidentifying constraints given in Chen et al. (2014).
Our extensions can be applied to Markovian  semi-Markovian  and non-Markovian models. We then
demonstrate how recursive c-component decomposition  which was ﬁrst utilized in identiﬁcation
algorithms developed for non-parametric models (Tian  2002; Huang and Valtorta  2006; Shpitser
and Pearl  2006)  can be incorporated into our linear identiﬁcation and constraint discovery methods
for Markovian and semi-Markovian models. We show that doing so allows the identiﬁcation of
additional models and constraints. Further  we will demonstrate that  unlike existing algorithms  our
method subsumes the aforementioned identiﬁcation and constraint discovery methods developed for
non-parametric models when applied to linear SEMs.

2 Preliminaries

A linear structural equation model consists of a set of equations of the form  X = ΛX +   where
X = [x1  ...  xn]t is a vector containing the model variables  Λ is a matrix containing the coefﬁcients
of the model  which convey the strength of the causal relationships  and  = [1  ...  n]t is a vector
of error terms  which represents omitted or latent variables. The matrix Λ contains zeroes on the
diagonal  and Λij = 0 whenever xi is not a cause of xj. The error terms are normally distributed
random variables and induce the probability distribution over the model variables. The covariance
matrix of X will be denoted by Σ and the covariance matrix over the error terms    by Ω.
An instantiation of a model M is an assignment of values to the model parameters (i.e. Λ and the
non-zero elements of Ω). For a given instantiation mi  let Σ(mi) denote the covariance matrix
implied by the model and λk(mi) be the value of coefﬁcient λk.
Deﬁnition 1. A coefﬁcient  λk  is identiﬁed if for any two instantiations of the model  mi and mj 
we have λk(mi) = λk(mj) whenever Σ(mi) = Σ(mj).

In other words  λk is identiﬁed if it can be uniquely determined from the covariance matrix  Σ. Now 
we deﬁne when a structural coefﬁcient  λk  is overidentiﬁed.
Deﬁnition 2. (Pearl  2004) A coefﬁcient  λk is overidentiﬁed if there are two or more distinct sets of
logically independent assumptions in M such that

(i) each set is sufﬁcient for deriving λk as a function of Σ  λk = f (Σ) 
(ii) each set induces a distinct function λk = f (Σ)  and
(iii) each assumption set is minimal  that is  no proper subset of those assumptions is sufﬁcient for

the derivation of λk.

The causal graph or path diagram of an SEM is a graph  G = (V  D  B)  where V are vertices or
nodes  D directed edges  and B bidirected edges. The vertices represent model variables. Directed

2

eges represent the direction of causality  and for each coefﬁcient Λij (cid:54)= 0  an edge is drawn from
xi to xj. Each directed edge  therefore  is associated with a coefﬁcient in the SEM  which we
will often refer to as its structural coefﬁcient. The error terms  i  are not represented in the graph.
However  a bidirected edge between two variables indicates that their corresponding error terms
may be statistically dependent while the lack of a bidirected edge indicates that the error terms are
independent. When the causal graph is acyclic without bidirected edges  then we say that the model
is Markovian. Graphs with bidirected edges are non-Markovian  while acyclic graphs with bidirected
edges are additionally called semi-Markovian.
We will use standard graph terminology with P a(y) denoting the parents of y  Anc(y) denoting
the ancestors of y  De(y) denoting the descendants of y  and Sib(y) denoting the siblings of y  the
variables that are connected to y via a bidirected edge. He(E) denotes the heads of a set of directed
edges  E  while T a(E) denotes the tails. Additionally  for a node v  the set of edges for which
He(E) = v is denoted Inc(v). Lastly  we will utilize d-separation (Pearl  2009).
Lastly  we establish a couple preliminary deﬁnitions around half-treks. These deﬁnitions and
illustrative examples can also be found in Foygel et al. (2012) and Chen et al. (2014).
Deﬁnition 3. (Foygel et al.  2012) A half-trek  π  from x to y is a path from x to y that either begins
with a bidirected arc and then continues with directed edges towards y or is simply a directed path
from x to y.

We will denote the set of nodes that are reachable by half-trek from v htr(v).
Deﬁnition 4. (Foygel et al.  2012) For any half-trek  π  let Right(π) be the set of vertices in π that
have an outgoing directed edge in π (as opposed to bidirected edge) union the last node in the trek. In
other words  if the trek is a directed path then every node in the path is a member of Right(π). If the
trek begins with a bidirected edge then every node other than the ﬁrst node is a member of Right(π).
Deﬁnition 5. (Foygel et al.  2012) A system of half-treks  π1  ...  πn  has no sided intersection if for
all πi  πj ∈ {π1  ...  πn} such that πi (cid:54)= πj  Right(πi)∩Right(πj)= ∅.
Deﬁnition 6. (Chen et al.  2014) For an arbitrary variable  v  let P a1  P a2  ...  P ak be the unique
partition of Pa(v) such that any two parents are placed in the same subset  P ai  whenever they are
connected by an unblocked path (given the empty set). A connected edge set with head v is a set of
directed edges from P ai to v for some i ∈ {1  2  ...  k}.

3 General Half-Trek Criterion

The half-trek criterion is a graphical condition that can be used to determine the identiﬁability of
recursive and non-recursive linear models (Foygel et al.  2012). Foygel et al. (2012) use the half-trek
criterion to identify the model variables one at a time  where each identiﬁed variable may be able
to aid in the identiﬁcation of other variables. If any variable is not identiﬁable using the half-trek
criterion  then their algorithm returns that the model is not HTC-identiﬁable. Otherwise the algorithm
returns that the model is identiﬁable. Their algorithm subsumes the earlier methods of Brito and Pearl
(2002) and Brito (2004). In this section  we extend the half-trek criterion to allow the identiﬁcation
of arbitrary subsets of edges belonging to a variable. As a result  our algorithm can be utilized to
identify as many coefﬁcients as possible  even when the model is not identiﬁed. Additionally  this
extension improves our ability to identify entire models  as we will show.
Deﬁnition 7. (General Half-Trek Criterion) Let E be a set of directed edges sharing a single head y.
A set of variables Z satisﬁes the general half-trek criterion with respect to E  if
(i) |Z| = |E| 
(ii) Z ∩ (y ∪ Sib(y)) = ∅ 
(iii) There is a system of half-treks with no sided intersection from Z to T a(E)  and
(iv) (P a(y) \ T a(E)) ∩ htr(Z) = ∅.

A set of directed edges  E  sharing a head y is identiﬁable if there exists a set  ZE  that satisﬁes
the general half-trek criterion (g-HTC) with respect to E  and ZE consists only of “allowed” nodes.
Intuitively  a node z is allowed if Ezy is identiﬁed or empty  where Ezy ⊆ Inc(z) is the set of edges

3

Figure 1: The above model is identiﬁed using the g-HTC but not the HTC.

belonging to z that lie on half-treks from y to z or lie on unblocked paths (given the empty set)
between z and P a(y) \ T a(E).2 The following deﬁnition formalizes this notion.
Deﬁnition 8. A node  z  is g-HT allowed (or simply allowed) for directed edges E with head y if
Ezy = ∅ or there exists sequences of sets of nodes  (Z1  ...Zk)  and sets of edges  (E1  ...  Ek)  with
Ezy ⊆ E1 ∪ ... ∪ Ek such that
(i) Zi satisﬁes the g-HTC with respect to Ei for all i ∈ {1  ...  k} 
(ii) EZ1y1 = ∅  where yi = He(Ei) for all i ∈ {1  ...  k}  and
(iii) EZiyi ⊆ (E1 ∪ ... ∪ Ei−1) for all i ∈ {1  ...k}.
When a set of allowed nodes  ZE  satisﬁes the g-HTC for a set of edges E  then we will say that ZE
is a g-HT admissible set for E.
Theorem 1. If a g-HT admissible set for directed edges Ey with head y exists then Ey is g-HT
identiﬁable. Further  let ZEy = {z1  ...  zk} be a g-HT admissible set for Ey  T a(Ey) = {p1  ...  pk} 
and Σ be the covariance matrix of the model variables. Deﬁne A as

and b as

Aij =

bi =

(cid:26)[(I − Λ)T Σ]zipj   Eziy (cid:54)= ∅
(cid:26)[(I − Λ)T Σ]ziy  Eziy (cid:54)= ∅

Eziy = ∅

Σzipj  

Eziy = ∅

(1)

(2)

Then A is an invertible matrix and A · ΛT a(Ey) y = b.

Σziy 

Proof. See Appendix for proofs of all theorems and lemmas.

The g-HTC impoves upon the HTC because subsets of a variable’s coefﬁcients may be identiﬁable
even when the variable is not. By identifying subsets of a variable’s coefﬁcients  we not only allow
the identiﬁcation of as many coefﬁcients as possible in unidentiﬁed models  but we also are able to
identify additional models as a whole. For example  Figure 1 is not identiﬁable using the HTC. In
order to identify Y   Z2 needs to be identiﬁed ﬁrst as it is the only variable with a half-trek to X2
without being a sibling of Y . However  to identify Z2  either Y or W1 needs to be identiﬁed. Finally
to identify W1  Y needs to be identiﬁed. This cycle implies that the model is not HTC-identiﬁable.
It is  however  g-HTC identiﬁable since the g-HTC allows d to be identiﬁed independently of f 
using {Z1} as a g-HT admissible set  which in turn allows {Y } to be a g-HT admissible set for W1’s
coefﬁcient  a.
Finding a g-HT admissible set for directed edges  E  with head  y  from a set of allowed nodes  AE 
can be accomplished by utilizing the max-ﬂow algorithm described in Chen et al. (2014)3  which we
call MaxFlow(G  E  AE). This algorithm returns a maximal set of allowed nodes that satisﬁes (ii) -
(iv) of the g-HTC.

. In
In some cases  there may be no g-HT admissible set for E
other cases  there may be no g-HT admissible set of variables for a set of edges E but there may be a

but there may be one for E ⊂ E

(cid:48)

(cid:48)

2We will continue to use the EZy notation and allow Z to be a set of nodes.
3Brito (2004) utilized a similar max-ﬂow construction in his identiﬁcation algorithm.

4

(a)

(b)

(c)

(d)

Figure 2: (a) The graph is not identiﬁed using the g-HTC and cannot be decomposed (b) After
removing V6 we are able to decompose the graph (c) Graph for c-component  {V2  V3  V5} (d) Graph
for c-component  {V1  V4}

(cid:48)

with E ⊂ E

(cid:48)

g-HT admissible set of variables for E
. As a result  if a HT-admissible set does not
exist for Ey  where Ey = Inc(y) for some node y  we may have to check whether such a set exists
for all possible subsets of Ey in order to identify as many coefﬁcients in Ey as possible. This process
can be somewhat simpliﬁed by noting that if E is a connected edge set with no g-HT admissible set 
then there is no superset E
An algorithm that utilizes the g-HTC and Theorem 1 to identify as many coefﬁcients as possible in
recursive or non-recursive linear SEMs is given in the Appendix. Since we may need to check the
identiﬁability of all subsets of a node’s edges  the algorithm’s complexity is polynomial time if the
degree of each node is bounded.

with a g-HT admissible set.

(cid:48)

4 Generalizing Overidentifying Constraints

Chen et al. (2014) discovered overidentifying constraints by ﬁnding two HT-admissible sets for
a given connected edge set. When two such sets exist  we obtain two distinct expressions for the
identiﬁed coefﬁcients  and equating the two expressions gives the overidentifying constraint. However 
we may be able to obtain constraints even when |ZE| < |E| and E is not identiﬁed. The algorithm 
MaxFlow  returns a maximal set  ZE  for which the equations  A · ΛT a(E) y = b  are linearly
independent  regardless of whether |ZE| = |E| and E is identiﬁed or not. Therefore  if we are able
to ﬁnd an allowed node w that satisﬁes the conditions below  then the equation aw · ΛT a(E) y = bw
will be a linear combination of the equations  A · ΛT a(E) y = b.
Theorem 2. Let ZE be a set of maximal size that satisﬁes conditions (ii)-(iv) of the g-HTC for a
set of edges  E  with head y. If there exists a node w such that there exists a half-trek from w to
Ta(E)  w /∈ (y ∪ Sib(y))  and w is g-HT allowed for E  then we obtain the equality constraint 
awA−1
We will call these generalized overidentifying constraints  half-trek constraints or HT-constraints. An
algorithm that identiﬁes coefﬁcients and ﬁnds HT-constraints for a recursive or non-recursive linear
SEM is given in the Appendix.

rightb = bw  where A−1

right is the right inverse of A.

5 Decomposition

Tian showed that the identiﬁcation problem could be simpliﬁed in semi-Markovian linear structural
equation models by decomposing the model into sub-models according to their c-components (Tian 
2005). Each coefﬁcient is identiﬁable if and only if it is identiﬁable in the sub-model to which it
belongs (Tian  2005). In this section  we show that the c-component decomposition can be applied
recursively to the model after marginalizing certain variables. This idea was ﬁrst used to identify
interventional distributions in non-parameteric models by Tian (2002) and Tian and Pearl (2002a)
and adapting this technique for linear models will allow us to identify models that the g-HTC  even
coupled with (non-recursive) c-component decomposition  is unable to identify. Further  it ensures the
identiﬁcation of all coefﬁcients identiﬁable using methods developed for non-parametric models–a
guarantee that none of the existing methods developed for linear models satisfy.

5

(cid:48)

(cid:48)

v6

yields two c-components  as shown in Figure 2b.

induces the distribution(cid:82)

The graph in Figure 2a consists of a single c-component  and we are unable to decompose it.
As a result  we are able to identify a but no other coefﬁcients using the g-HTC. Moreover 
E[v5|do(v6  v4  v3  v2  v1)] is identiﬁed using identiﬁcation methods developed for non-
f = ∂
∂v4
parametric models (e.g. do-calculus) but not the g-HTC or other methods developed for linear
models.
However  if we remove v6 from the analysis  then the resulting model can be decomposed. Let M be
be a model that is
the model depicted in Figure 2a  P (v) be the distribution induced by M  and M
identical to M except the equation for v6 is removed. M
P (V )dv6  and
(cid:48)
its associated graph G
(cid:48)
Now  decomposing G
according to these c-components yields the sub-models depicted by Figures 2c
and 2d. Both of these sub-models are identiﬁable using the half-trek criterion. Thus  all coefﬁcients
other than h have been shown to be identiﬁable. Returning to the graph prior to removal  depicted in
Figure 2a  we are now able to identify h because both v4 and v5 are now allowed nodes for h  and the
model is identiﬁed4.
As a result  we can improve our identiﬁcation and constraint-discovery algorithm by recursively
decomposing  using the g-HTC and Theorem 2  and removing descendant sets5. Note  however 
that we must consider every descendant set for removal. It is possible that removing D1 will allow
identiﬁcation of a coefﬁcient but removing a superset D2 with D1 ⊂ D2 will not. Additionally  it is
possible that removing D2 will allow identiﬁcation but removing a subset D1 will not.
After recursively decomposing the graph  if some of the removed variables were unidentiﬁed  we
may be able to identify them by returning to the original graph prior to removal since we may have a
larger set of allowed nodes. For example  we were able to identify h in Figure 2a by “un-removing"
v6 after the other coefﬁcients were identiﬁed. In some cases  however  we may need to again
recursively decompose and remove descendant sets. As a result  in order to fully exploit the powers
of decomposition and the g-HTC  we must repeat the recursive decomposition process on the original
model until all marginalized nodes are identiﬁed or no new coefﬁcients are identiﬁed in an iteration.
Clearly  recursive decomposition also aids in the discovery of HT-constraints in the same way that it
aids in the identiﬁcation of coefﬁcients using the g-HTC. However  note that recursive decomposition
may also introduce additional d-separation constraints. Prior to decomposition  if a node Z is d-
separated from a node V then we trivially obtain the constraint that ΣZV = 0. However  in some
cases  Z may become d-separated from V after decomposition. In this case  the independence
constraint on the covariance matrix of the decomposed c-component corresponds to a non-conditional
independence constraint in the original joint distribution P (V ). It is for this reason that we output
independence constraints in Algorithm 2 (see Appendix).
For example  consider the graph depicted in Figure 3a. Theorem 2 does not yield any constraints for
the edges of V7. However  after decomposing the graph we obtain the c-component for {V2  V5  V7} 
shown in Figure 3b. In this graph  V1 is d-separated from V7 yielding a non-independence constraint
in the original model.
We can systematically identify coefﬁcients and HT-constraints using recursive c-component decom-
position by repeating the following steps for the model’s graph G until the model has been identiﬁed
or no new coefﬁcients are identiﬁed in an iteration:
(i) Decompose the graph into c-components  {Si}
(ii) For each c-component  utilize the g-HTC and Theorems 1 and 2 to identify coefﬁcients and ﬁnd

HT-constraints

(iii) For each descendant set  marginalize the descendant set and repeat steps (i)-(iii) until all

variables have been marginalized

sets to identify h using Theorem 1 since their coefﬁcients have been identiﬁed.

4While v4 and v5 are technically not allowed according to Deﬁnition 8  they can be used in g-HT admissible
5Only removing descendant sets have the ability to break up components. For example  removing {v2} from
Figure 2a does not break the c-component because removing v2 would relegate its inﬂuence to the error term of
its child  v3. As a result  the graph of the resulting model would include a bidirected arc between v3 and v6  and
we would still have a single c-component.

6

Figure 3: (a) V1 cannot be d-separated from V7 (b) V1 is d-separated from V7 in the graph of the
c-component  {V2  V5  V7}

(a)

(b)

If a coefﬁcient α can be identiﬁed using the above method (see also Algorithm 3 in the Appendix 
which utilizes recursive decomposition to identify coefﬁcients and output HT-constraints)  then we
will say that α is g-HTC identiﬁable.
We now show that any direct effect identiﬁable using non-parametric methods is also g-HTC identiﬁ-
able.
Theorem 3. Let M be a linear SEM with variables V . Let M
be a non-parametric SEM with
identical structure to M. If the direct effect of x on y for x  y ∈ V is identiﬁed in M
then the
coefﬁcient Λxy in M is g-HTC identiﬁable and can be identiﬁed using Algorithm 3 (see Appendix).

(cid:48)

(cid:48)

6 Non-Parametric Verma Constraints

Tian and Pearl (2002b) and Shpitser and Pearl (2008) provided algorithms for discovering Verma
constraints in recursive  non-parametric models. In this section  we will show that the constraints
obtained by the above method and Algorithm 3 (see Appendix) subsume the constraints discovered
by both methods when applied to linear models. First  we will show that the constraints identiﬁed in
(Tian and Pearl  2002b)  which we call Q-constraints  are subsumed by HT-constraints. Second  we
will show that the constraints given by Shpitser and Pearl (2008)  called dormant independences  are 
in fact  equivalent to the constraints given by Tian and Pearl (2002b) for linear models. As a result 
both dormant independences and Q-constraints are subsumed by HT-constraints.

6.1 Q-Constraints

We refer to the constraints enumerated in (Tian and Pearl  2002b) as Q-constraints since they are
discovered by identifying Q-factors  which are deﬁned below.
Deﬁnition 9. For any subset  S ⊆ V   the Q-factor  QS  is given by

(cid:90)

(cid:89)

QS =

S

i|Vi∈S

P (vi|pai  i)P (S)dS 

(3)

where S contains the error terms of the variables in S.

A Q-factor  QS  is identiﬁable whenever S is a c-component (Tian and Pearl  2002a).
Lemma 1.
component  V (i) = {v1  ...  vi}  and V (0) = ∅.

(Tian and Pearl  2002a) Let {v1  ...  vn} be sorted topologically  S be a c-
Then QS can be computed as QS =

(cid:81){i|vi∈S} P (vi|V (i−1))  j = 1  ...  k.

For example  consider again Figure 2b. We have that Q1 = P (v1)P (v4|v3  v2  v1) and Q2 =
P (v2|v1)P (v3|v2  v1)P (v5|v4  v3  v2  v1).
A Q-factor can also be identiﬁed by marginalizing out descendant sets (Tian and Pearl  2002a).
Suppose that QS is identiﬁed and D is a descendant set in GS  then

QSi\D =

QSi.

(4)

If the marginalization over D yields additional c-components in the marginalized graph  then we can
again compute each of them from QS\D (Tian and Pearl  2002b).

D

7

(cid:88)

Figure 4: The above graph induces the Verma constraint  Q[v4] is not a function of v1  and equivalently 
v4 ⊥ v1|do(v3).

Tian’s method recursively computes the Q-factors associated with c-components  marginalizes
descendant sets in the graph for the computed Q-factor  and again computes Q-factors associated
with c-components in the marginalized graph. The Q-constraint is obtained in the following way.
The deﬁnition of a Q-factor  QS  given by Equation 3 is a function of P a(S) only. However  the
equivalent expression given by Lemma 1 and Equation 4 may be functions of additional variables.
in Figure 4  {v2  v4} is a c-component so we can identify Qv2v4 =
For example 
P (v4|v3  v2  v1)P (v2|v1). The decomposition also makes v2 a leaf node in Gv2v4. As a result 
P (v4|v3  v2  v1)P (v2|v1)dv2. Since v1 is not a parent of v4 in Gv4  we

we can identify Qv4 =(cid:82)
have that Qv4 =(cid:82)

P (v4|v3  v2  v1)P (v2|v1)dv2 ⊥ v1.

Theorem 4. Any Q-constraint  QS ⊥ Z  in a linear SEM  has an equivalent set of HT-constraints
that can be discovered using Algorithm 3 (see Appendix).

v2

v2

6.2 Dormant Independences

Dormant independences have a natural interpretation as independence and conditional independence
constraints within identiﬁable interventional distributions (Shpitser and Pearl  2008). For example 
in Figure 4  the distribution after intervention on v3 can be represented graphically by removing
the edge from v2 to v3 since v3 is no longer a function of v2 but is instead a constant. In the
resulting graph  v4 is d-separated from v1 implying that v4 is independent of v1 in the distribution 
P (v4  v2  v1|do(v3)). In other words  P (v4|do(v3)  v1) = P (v4|do(v3)). Now  it is not hard to show
P (v4|v3  v2  v1)P (v2|v1) and we obtain the
P (v4|v3  v2  v1)P (v2|v1) is not a function of v1  which is exactly the Q-constraint

that P (v4|v1  do(v3)) is identiﬁable and equal to(cid:80)
constraint that(cid:80)

v2
we obtained above.
It turns out that dormant independences among singletons and Q-constraints are equivalent  as stated
by the following lemma.
Lemma 2. Any dormant independence  x|=y|w  do(Z)  with x and y singletons has an equivalent
Q-constraint and vice versa.

v2

Since pairwise independence implies independence in normal distributions  Lemma 2 and Theorem 4
imply the following theorem.
Theorem 5. Any dormant independence among sets  x|=y|W  do(Z)  in a linear SEM  has an
equivalent set of HT-constraints that can be discovered by incorporating recursive c-component
decomposition with Algorithm 3 (see Appendix).

7 Conclusion

In this paper  we extend the half-trek criterion (Foygel et al.  2012) and generalize the notion of
overidentiﬁcation to discover constraints using the generalized half-trek criterion  even when the
coefﬁcients are not identiﬁed. We then incorporate recursive c-component decomposition and show
that the resulting identiﬁcation method is able to identify more models and constraints than the
existing linear and non-parameteric algorithms.
Finally  we note that while we were preparing this manuscript for submission  Drton and Weihs
(2016) independently introduced a similar idea to the recursive decomposition discussed in this paper 
which they called ancestor decomposition. While ancestor decomposition is more efﬁcient  recursive
decomposition is more general in that it enables the identiﬁcation of a larger set of coefﬁcients.

8

8 Acknowledgments

I would like to thank Jin Tian and Judea Pearl for helpful comments and discussions. This research
was supported in parts by grants from NSF #IIS-1302448 and #IIS-1527490 and ONR #N00014-13-
1-0153 and #N00014-13-1-0153.

References
BALKE  A. and PEARL  J. (1994). Probabilistic evaluation of counterfactual queries. In Proceedings of the

Twelfth National Conference on Artiﬁcial Intelligence  vol. I. MIT Press  Menlo Park  CA  230–237.

BRITO  C. (2004). Graphical methods for identiﬁcation in structural equation models. Ph.D. thesis  Computer

Science Department  University of California  Los Angeles  CA.
URL {$<$http://ftp.cs.ucla.edu/pub/stat_ser/r314.pdf$>$}

BRITO  C. and PEARL  J. (2002). Generalized instrumental variables. In Uncertainty in Artiﬁcial Intelligence 
Proceedings of the Eighteenth Conference (A. Darwiche and N. Friedman  eds.). Morgan Kaufmann  San
Francisco  85–93.

CHEN  B. and PEARL  J. (2014). Graphical tools for linear structural equation modeling. Tech. Rep. R-432 
<http://ftp.cs.ucla.edu/pub/stat_ser/r432.pdf>  Department of Computer Science  University of California 
Los Angeles  CA. Forthcoming  Psychometrika.

CHEN  B.  TIAN  J. and PEARL  J. (2014). Testable implications of linear structual equation models. In
Proceedings of the Twenty-eighth AAAI Conference on Artiﬁcial Intelligence (C. E. Brodley and P. Stone 
eds.). AAAI Press  Palo  CA. <http://ftp.cs.ucla.edu/pub/stat_ser/r428-reprint.pdf>.

DRTON  M. and WEIHS  L. (2016). Generic identiﬁability of linear structural equation models by ancestor

decomposition. Scandinavian Journal of Statistics n/a–n/a10.1111/sjos.12227.
URL http://dx.doi.org/10.1111/sjos.12227

FOYGEL  R.  DRAISMA  J. and DRTON  M. (2012). Half-trek criterion for generic identiﬁability of linear

structural equation models. The Annals of Statistics 40 1682–1713.

HUANG  Y. and VALTORTA  M. (2006). Pearl’s calculus of intervention is complete. In Proceedings of the
Twenty-Second Conference on Uncertainty in Artiﬁcial Intelligence (R. Dechter and T. Richardson  eds.).
AUAI Press  Corvallis  OR  217–224.

KANG  C. and TIAN  J. (2009). Markov properties for linear causal models with correlated errors. The Journal

of Machine Learning Research 10 41–70.

PEARL  J. (2004). Robustness of causal claims. In Proceedings of the Twentieth Conference Uncertainty in

Artiﬁcial Intelligence (M. Chickering and J. Halpern  eds.). AUAI Press  Arlington  VA  446–453.

PEARL  J. (2009). Causality: Models  Reasoning  and Inference. 2nd ed. Cambridge University Press  New

York.

SHPITSER  I. and PEARL  J. (2006). Identiﬁcation of conditional interventional distributions. In Proceedings of
the Twenty-Second Conference on Uncertainty in Artiﬁcial Intelligence (R. Dechter and T. Richardson  eds.).
AUAI Press  Corvallis  OR  437–444.

SHPITSER  I. and PEARL  J. (2008). Dormant independence. In Proceedings of the Twenty-Third Conference

on Artiﬁcial Intelligence. AAAI Press  Menlo Park  CA  1081–1087.

TIAN  J. (2002). Studies in Causal Reasoning and Learning. Ph.D. thesis  Computer Science Department 

University of California  Los Angeles  CA.

TIAN  J. (2005). Identifying direct causal effects in linear models. In Proceedings of the National Conference
on Artiﬁcial Intelligence  vol. 20. Menlo Park  CA; Cambridge  MA; London; AAAI Press; MIT Press; 1999.

TIAN  J. (2007). A criterion for parameter identiﬁcation in structural equation models. In Proceedings of the
Twenty-Third Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-07). AUAI Press 
Corvallis  Oregon.

TIAN  J. (2009). Parameter identiﬁcation in a class of linear structural equation models. In Proceedings of the

Twenty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-09).

9

TIAN  J. and PEARL  J. (2002a). A general identiﬁcation condition for causal effects. In Proceedings of the
Eighteenth National Conference on Artiﬁcial Intelligence. AAAI Press/The MIT Press  Menlo Park  CA 
567–573.

TIAN  J. and PEARL  J. (2002b). On the testable implications of causal models with hidden variables. In Pro-
ceedings of the Eighteenth Conference on Uncertainty in Artiﬁcial Intelligence (A. Darwiche and N. Friedman 
eds.). Morgan Kaufmann  San Francisco  CA  519–527.

VERMA  T. and PEARL  J. (1990). Equivalence and synthesis of causal models. In Proceedings of the Sixth
Conference on Uncertainty in Artiﬁcial Intelligence. Cambridge  MA. Also in P. Bonissone  M. Henrion 
L.N. Kanal and J.F. Lemmer (Eds.)  Uncertainty in Artiﬁcial Intelligence 6  Elsevier Science Publishers  B.V. 
255–268  1991.

WRIGHT  S. (1921). Correlation and causation. Journal of Agricultural Research 20 557–585.

10

,Bryant Chen