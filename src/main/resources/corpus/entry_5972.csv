2019,You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle,Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing. However  recent works have shown that deep networks can be vulnerable to adversarial perturbations which raised a serious robustness issue of deep networks. Adversarial training  typically formulated as a robust optimization problem  is an effective way of improving the robustness of deep networks. A major drawback of existing adversarial training algorithms is the computational overhead of the generation of adversarial examples  typically far greater than that of the network training. This leads to unbearable overall computational cost of adversarial training. In this paper  we show that adversarial training can be cast as a discrete time differential game. Through analyzing the Pontryagin’s Maximum Principle (PMP) of the problem  we observe that the adversary update is only coupled with the parameters of the first layer of the network. This inspires us to restrict most of the forward and back propagation within the first layer of the network during adversary updates. This effectively reduces the total number of full forward and backward propagation to only one for each group of adversary updates. Therefore  we refer to this algorithm YOPO (\textbf{Y}ou \textbf{O}nly \textbf{P}ropagate  \textbf{O}nce). Numerical experiments demonstrate that YOPO can achieve comparable defense accuracy with \textbf{approximately 1/5 $\sim$ 1/4 GPU time} of the projected gradient descent (PGD) algorithm~\cite{kurakin2016adversarial}.,You Only Propagate Once: Accelerating Adversarial

Training via Maximal Principle

Dinghuai Zhang∗  Tianyuan Zhang∗

Peking University

{zhangdinghuai  1600012888}@pku.edu.cn

Yiping Lu∗

Stanford University
yplu@stanford.edu

Zhanxing Zhu†

School of Mathematical Sciences  Peking University

Center for Data Science  Peking University

Beijing Institute of Big Data Research

zhanxing.zhu@pku.edu.cn

Beijing International Center for Mathematical Research  Peking University

Bin Dong†

Center for Data Science  Peking University

Beijing Institute of Big Data Research

dongbin@math.pku.edu.cn

Abstract

Deep learning achieves state-of-the-art results in many tasks in computer vision
and natural language processing. However  recent works have shown that deep
networks can be vulnerable to adversarial perturbations  which raised a serious
robustness issue of deep networks. Adversarial training  typically formulated as a
robust optimization problem  is an effective way of improving the robustness of
deep networks. A major drawback of existing adversarial training algorithms is
the computational overhead of the generation of adversarial examples  typically
far greater than that of the network training. This leads to the unbearable overall
computational cost of adversarial training. In this paper  we show that adversarial
training can be cast as a discrete time differential game. Through analyzing the
Pontryagin’s Maximum Principle (PMP) of the problem  we observe that the
adversary update is only coupled with the parameters of the ﬁrst layer of the
network. This inspires us to restrict most of the forward and back propagation
within the ﬁrst layer of the network during adversary updates. This effectively
reduces the total number of full forward and backward propagation to only one
for each group of adversary updates. Therefore  we refer to this algorithm YOPO
(You Only Propagate Once). Numerical experiments demonstrate that YOPO can
achieve comparable defense accuracy with approximately 1/5 ∼ 1/4 GPU time
of the projected gradient descent (PGD) algorithm .3

∗Equal Contribution
†Corresponding Authors
3Our codes are available at https://github.com/a1600012888/YOPO-You-Only-Propagate-Once

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1

Introduction

Deep neural networks achieve state-of-the-art performance on many tasks [4 8 16 21 25 44]. However 
recent works show that deep networks are often sensitive to adversarial perturbations [27 35 49]  i.e. 
changing the input in a way imperceptible to humans while causing the neural network to output
an incorrect prediction. This poses signiﬁcant concerns when applying deep neural networks to
safety-critical problems such as autonomous driving and medical domains. To effectively defend
the adversarial attacks  [26] proposed adversarial training  which can be formulated as a robust
optimization [38]:

min

θ

E(x y)∼D max
(cid:107)η(cid:107)≤

(cid:96)(θ; x + η  y) 

(1)

where θ is the network parameter  η is the adversarial perturbation  and (x  y) is a pair of data
and label drawn from a certain distribution D. The magnitude of the adversarial perturbation η is
restricted by  > 0. For a given pair (x  y)  we refer to the value of the inner maximization of (1)  i.e.
max(cid:107)η(cid:107)≤ (cid:96)(θ; x + η  y)  as the adversarial loss which depends on (x  y).
A major issue of the current adversarial training methods is their signiﬁcantly high computational cost.
In adversarial training  we need to solve the inner loop  which is to obtain the "optimal" adversarial
attack to the input in every iteration. Such "optimal" adversary is usually obtained using multi-step
gradient decent  and thus the total time for learning a model using standard adversarial training
method is much more than that using the standard training. Considering applying 40 inner iterations
of projected gradient descent (PGD [15]) to obtain the adversarial examples  the computation cost of
solving the problem (1) is about 40 times that of a regular training.
The main objective of this paper is to re-
duce the computational burden of adver-
sarial training by limiting the number of
forward and backward propagation with-
out hurting the performance of the trained
network. In this paper  we exploit the struc-
tures that the min-max objectives is encoun-
tered with deep neural networks. To achieve
this  we formulate the adversarial training
problem(1) as a differential game. After-
wards we can derive the Pontryagin’s Max-
imum Principle (PMP) of the problem.
From the PMP  we discover a key fact that
the adversarial perturbation is only coupled
with the weights of the ﬁrst layer. This
motivates us to propose a novel adversarial
training strategy by decoupling the adver-
sary update from the training of the network
parameters. This effectively reduces the to-
tal number of full forward and backward
propagation to only one for each group of
adversary updates  signiﬁcantly lowering
the overall computation cost without ham-
pering performance of the trained network.
We name this new adversarial training algorithm as YOPO (You Only Propagate Once). Our nu-
merical experiments show that YOPO achieves approximately 4 ∼5 times speedup over the original
PGD adversarial training with comparable accuracy on MNIST/CIFAR10. Furthermore  we apply
our algorithm to a recent proposed min max optimization objective "TRADES" [46] and achieve better
clean and robust accuracy within half of the time TRADES need.

Figure 1: Our proposed YOPO expolits the structure
of neural network. To alleviate the heavy computation
cost  YOPO focuses the calculation of the adversary at
the ﬁrst layer.

1.1 Related Works

Adversarial Defense. To improve the robustness of neural networks to adversarial examples  many
defense strategies and models have been proposed  such as adversarial training [26]  orthogonal reg-
ularization [6 22]  Bayesian method [45]  TRADES [46]  rejecting adversarial examples [43]  Jacobian

2

Adversary updaterAdversary updaterBlack boxPrevious WorkYOPOHeavy gradient calculationregularization [14 29]  generative model based defense [12 33]  pixel defense [24 31]  ordinary differential
equation (ODE) viewpoint [47]  ensemble via an intriguing stochastic differential equation perspec-
tive [39]  and feature denoising [34 42]  etc. Among all these approaches  adversarial training and its
variants tend to be most effective since it largely avoids the the obfuscated gradient problem [2].
Therefore  in this paper  we choose adversarial training to achieve model robustness.

Neural ODEs. Recent works have built up the relationship between ordinary differential equations
and neural networks [5 10 23 32 37 40 48]. Observing that each residual block of ResNet can be written
as un+1 = un + ∆tf (un)  one step of forward Euler method approximating the ODE ut = f (u).
Thus [19 41] proposed an optimal control framework for deep learning and [5 19 20] utilize the adjoint
equation and the maximal principle to train a neural network.

Decouple Training. Training neural networks requires forward and backward propagation in
a sequential manner. Different ways have been proposed to decouple the sequential process by
parallelization. This includes ADMM [36]  synthetic gradients [13]  delayed gradient [11]  lifted ma-
chines [1 9 18]. Our work can also be understood as a decoupling method based on a splitting technique.
However  we do not attempt to decouple the gradient w.r.t. network parameters but the adversary
update instead.

1.2 Contribution

• To the best of our knowledge  it is the ﬁrst attempt to design NN–speciﬁc algorithm for
adversarial defense. To achieve this  we recast the adversarial training problem as a discrete
time differential game. From optimal control theory  we derive the an optimality condition 
i.e. the Pontryagin’s Maximum Principle  for the differential game.

• Through PMP  we observe that the adversarial perturbation is only coupled with the ﬁrst
layer of neural networks. The PMP motivates a new adversarial training algorithm  YOPO.
We split the adversary computation and weight updating and the adversary computation is
focused on the ﬁrst layer. Relations between YOPO and original PGD are discussed.
• We ﬁnally achieve about 4∼ 5 times speed up than the original PGD training with compa-
rable results on MNIST/CIFAR10. Combining YOPO with TRADES [46]  we achieve both
higher clean and robust accuracy within less than half of the time TRADES need.

1.3 Organization

This paper is organized as follows. In Section 2  we formulate the robust optimization for neural
network adversarial training as a differential game and propose the gradient based YOPO. In Section 3 
we derive the PMP of the differential game  study the relationship between the PMP and the back-
propagation based gradient descent methods  and propose a general version of YOPO. Finally  all the
experimental details and results are given in Section 4.

2 Differential Game Formulation and Gradient Based YOPO

2.1 The Optimal Control Perspective and Differential Game

Inspired by the link between deep learning and optimal control [20]  we formulate the robust optimiza-
tion (1) as a differential game [7]. A two-player  zero-sum differential game is a game where each
player controls a dynamics  and one tries to maximize  the other to minimize  a payoff functional.
In the context of adversarial training  one player is the neural network  which controls the weights
of the network to ﬁt the label  while the other is the adversary that is dedicated to producing a false
prediction by modifying the input.
The robust optimization problem (1) can be written as a differential game as follows 

N(cid:88)

1
N

N(cid:88)

T−1(cid:88)

1
N

min

θ

max

J(θ  η) :=

(cid:107)ηi(cid:107)∞≤
subject to xi 1 = f0(xi 0 + ηi  θ0)  i = 1  2 ···   N
xi t+1 = ft(xi t  θt)  t = 1  2 ···   T − 1

t=0

i=1

i=1

(cid:96)i(xi T ) +

Rt(xi t; θt)

(2)

3

Here  the dynamics {ft(xt  θt)  t = 0  1  . . .   T − 1} represent a deep neural network  T denote the
number of layers  θt ∈ Θt denotes the parameters in layer t (denote θ = {θt}t ∈ Θ)  the function
ft : Rdt × Θt → Rdt+1 is a nonlinear transformation for one layer of neural network where dt is
the dimension of the t th feature map and {xi 0  i = 1  . . .   N} is the training dataset. The variable
η = (η1 ···   ηN ) is the adversarial perturbation and we constrain it in an ∞-ball. Function (cid:96)i is a
data ﬁtting loss function and Rt is the regularization weights θt such as the L2-norm. By casting
the problem of adversarial training as a differential game (2)  we regard θ and η as two competing
players  each trying to minimize/maximize the loss function J(θ  η) respectively.

2.2 Gradient Based YOPO

The Pontryagin’s Maximum Principle (PMP) is a fundamental tool in optimal control that character-
izes optimal solutions of the corresponding control problem [7]. PMP is a rather general framework
that inspires a variety of optimization algorithms. In this paper  we will derive the PMP of the
differential game (2)  which motivates the proposed YOPO in its most general form. However  to
better illustrate the essential idea of YOPO and to better address its relations with existing methods
such as PGD  we present a special case of YOPO in this section based on gradient descent/ascent.
We postpone the introduction of PMP and the general version of YOPO to Section 3.
Let us ﬁrst rewrite the original robust optimization problem (1) (in a mini-batch form) as

B(cid:88)

i=1

min

θ

max
(cid:107)ηi(cid:107)≤

(cid:96)(g˜θ(f0(xi + ηi  θ0))  yi) 

T−1 ◦ f θT −2

T−2 ◦ ··· f θ1

where f0 denotes the ﬁrst layer  g˜θ = f θT −1
1 denotes the network without the ﬁrst
layer and B is the batch size. Here ˜θ is deﬁned as {θ1 ···   θT−1}. For simplicity we omit the
regularization term Rt.
The simplest way to solve the problem is to perform gradient ascent on the input data and gradient
descent on the weights of the neural network as shown below. Such alternating optimization algorithm
is essentially the popular PGD adversarial training [26]. We summarize the PGD-r (for each update on
θ) as follows  i.e. performing r iterations of gradient ascent for inner maximization.

• For s = 0  1  . . .   r − 1  perform

ηs+1
i = ηs

i + α1∇ηi(cid:96)(g˜θ(f0(xi + ηs

i   θ0))  yi)  i = 1 ···   B 

where by the chain rule 
∇ηi(cid:96)(g˜θ(f0(xi + ηs

i   θ0))  yi) =∇g ˜θ
∇f0

(cid:0)(cid:96)(g˜θ(f0(xi + ηs
(cid:0)g˜θ(f0(xi + ηs

i   θ0))  yi)(cid:1)·
i   θ0))(cid:1) · ∇ηif0(xi + ηs
(cid:33)

i   θ0).

• Perform the SGD weight update (momentum SGD can also be used here)

θ ← θ − α2∇θ

(cid:96)(g˜θ(f0(xi + ηm

i   θ0))  yi)

(cid:32) B(cid:88)

i=1

Note that this method conducts r sweeps of forward and backward propagation for each update of θ.
This is the main reason why adversarial training using PGD-type algorithms can be very slow.
To reduce the total number of forward and backward propagation  we introduce a slack variable

(cid:0)(cid:96)(g˜θ(f0(xi + ηi  θ0))  yi)(cid:1) · ∇f0

(cid:0)g˜θ(f0(xi + ηi  θ0))(cid:1)

p = ∇g ˜θ

and freeze it as a constant within the inner loop of the adversary update. The modiﬁed algorithm is
given below and we shall refer to it as YOPO-m-n.

4

– Let ηj+1 0

i

= ηj n

.

i

• Calculate the weight update

(cid:32) B(cid:88)

m(cid:88)

∇θ

U =

  θ0)  i = 1 ···   B

(cid:33)

• Initialize {η1 0

– Calculate the slack variable p

i } for each input xi. For j = 1  2 ···   m
(cid:16)

(cid:16)

(cid:96)(g˜θ(f0(xi + ηj 0

i

  θ0))  yi)

(cid:17) · ∇f0

(cid:17)

  θ0))

 

g˜θ(f0(xi + ηj 0

i

p = ∇g ˜θ

– Update the adversary for s = 0  1  . . .   n − 1 for ﬁxed p

ηj s+1
i

= ηj s

i + α1p · ∇ηif0(xi + ηj s

i

(cid:96)(g˜θ(f0(xi + ηj n

i

  θ0))  yi)

j=1

i=1

and update the weight θ ← θ − α2U. (Momentum SGD can also be used here.)

Intuitively  YOPO freezes the values of the derivatives of the network at level 1  2 . . .   T − 1 during
the s-loop of the adversary updates. Figure 2 shows the conceptual comprison between YOPO and
PGD. YOPO-m-n accesses the data m × n times while only requires m full forward and backward
propagation. PGD-r  on the other hand  propagates the data r times for r full forward and backward
propagation. As one can see that  YOPO-m-n has the ﬂexibility of increasing n and reducing m to
achieve approximately the same level of attack but with much less computation cost. For example 
suppose one applies PGD-10 (i.e. 10 steps of gradient ascent for solving the inner maximization) to
calculate the adversary. An alternative approach is using YOPO-5-2 which also accesses the data 10
times but the total number of full forward propagation is only 5. Empirically  YOPO-m-n achieves
comparable results only requiring setting m × n a litter larger than r.
Another beneﬁt of YOPO is that we take full advantage of every forward and backward propagation
i   j = 1 ···   m − 1 are not wasted like
to update the weights  i.e. the intermediate perturbation ηj
PGD-r. This allows us to perform multiple updates per iteration  which potentially drives YOPO
to converge faster in terms of the number of epochs. Combining the two factors together  YOPO
signiﬁcantly could accelerate the standard PGD adversarial training.
We would like to point out a concurrent paper [30] that is related to YOPO. Their proposed method 
called "Free-m"  also can signiﬁcantly speed up adversarial training. In fact  Free-m is essentially
YOPO-m-1  except that YOPO-m-1 delays the weight update after the whole mini-batch is processed
in order for a proper usage of momentum 4.

3 The Pontryagin’s Maximum Principle for Adversarial Training

In this section  we present the PMP of the discrete time differential game (2). From the PMP  we
can observe that the adversary update and its associated back-propagation process can be decoupled.
Furthermore  back-propagation based gradient descent can be understood as an iterative algorithm
solving the PMP and with that the version of YOPO presented in the previous section can be viewed
as an algorithm solving the PMP. However  the PMP facilitates a much wider class of algorithms than
gradient descent algorithms [19]. Therefore  we will present a general version of YOPO based on the
PMP for the discrete differential game.

3.1 PMP

Pontryagin type of maximal principle [3 28] provides necessary conditions for optimality with a
layer-wise maximization requirement on the Hamiltonian function. For each layer t ∈ [T ] :=

4Momentum should be accumulated between mini-batches other than different adversarial examples from

one mini-batch  otherwise overﬁtting will become a serious problem.

5

Figure 2: Pipeline of YOPO-m-n described in Algorithm 1. The yellow and olive blocks represent
feature maps while the orange blocks represent the gradients of the loss w.r.t. feature maps of each
layer.

{0  1  . . .   T − 1}  we deﬁne the Hamiltonian function Ht : Rdt × Rdt+1 × Θt → R as

Ht(x  p  θt) = p · ft(x  θt) − 1
B

Rt(x  θt).

(PMP for adversarial

The PMP for continuous time differential game has been well studied in the literature [7]. Here  we
present the PMP for our discrete time differential game (2).
Theorem 1.
is twice continuous differentiable 
ft(·  θ)  Rt(·  θ) are twice continuously differentiable with respect to x; ft(·  θ)  Rt(·  θ) together
with their x partial derivatives are uniformly bounded in t and θ  and the sets {ft(x  θ) : θ ∈ Θt}
and {Rt(x  θ) : θ ∈ Θt} are convex for every t and x ∈ Rdt. Denote θ∗ as the solution of the
i t : t ∈ [T ]} such that the following holds
problem (2)  then there exists co-state processes p∗
for all t ∈ [T ] and i ∈ [B]:

training) Assume (cid:96)i

i := {p∗

i t+1 = ∇pHt(x∗
x∗
i t = ∇xHt(x∗
p∗
i t  p∗

i t  p∗
i t+1  θ∗
t ) 
i t+1  θ∗
t ) 

i 0 = xi 0 + η∗
x∗
i T )

∇(cid:96)i(x∗

i

i T = − 1
p∗
B

(4)
0 ∈ Θ0 and the optimal adversarial perturbation

At the same time  the parameters of the ﬁrst layer θ∗
η∗
i satisfy

B(cid:88)

0) ≥ B(cid:88)

H0(x∗

i 0 + ηi  p∗

i 1  θ∗

i=1

i=1

0) ≥ B(cid:88)

(3)

(5)

(6)

(7)

i   p∗

H0(x∗

i 1  θ∗

i 0 + η∗

H0(x∗
i 1  θ0) 
∀θ0 ∈ Θ0 (cid:107)ηi(cid:107)∞ ≤ 
t ∈ Θt  t ∈ [T ] maximize the Hamiltonian functions

i 0 + η∗

i   p∗

i=1

and the parameters of the other layers θ∗

B(cid:88)

t ) ≥ B(cid:88)

Ht(x∗

i t  p∗

i t+1  θ∗

Ht(x∗

i t  p∗

i t+1  θt)  ∀θt ∈ Θt

i=1

i=1

Proof. Proof is in the supplementary materials.

From the theorem  we can observe that the adversary η is only coupled with the parameters of the
ﬁrst layer θ0. This key observation inspires the design of YOPO.

6

𝒑𝒔𝒕𝒙𝒔𝒕YOPO Outer IterationYOPO Inner Iterationcopy𝒑𝒔𝟏𝒑𝒔𝒕𝒙𝒔𝒕PGD Adv. TrainIterationFor r times𝒑𝒔𝟏For m timesFor n times3.2 PMP and Back-Propagation Based Gradient Descent

The classical back-propagation based gradient descent algorithm [17] can be viewed as an algorithm
attempting to solve the PMP. Without loss of generality  we can let the regularization term R = 0 
since we can simply add an extra dynamic wt to evaluate the regularization term R  i.e.

wt+1 = wt + Rt(xt  θt)  w0 = 0.

We append w to x to study the dynamics of a new (dt + 1)-dimension vector and change ft(x  θt)
to (ft(x  θt)  w + Rt(x  θt)). The relationship between the PMP and the back-propagation based
gradient descent method was ﬁrst observed by Li et al. [19]. They showed that the forward dynamical
system Eq.(3) is the same as the neural network forward propagation. The backward dynamical
system Eq.(4) is the back-propagation  which is formally described by the following lemma.
Lemma 1.
t = ∇xHt(x∗
p∗

t+1)T ·−∇xt+1((cid:96)(xT )) = −∇xt((cid:96)(xT )).

t )T pt+1 = (∇xt x∗

t ) = ∇xf (x∗

t   p∗

t+1  θ∗

t   θ∗

To solve the maximization of the Hamiltonian  a simple way is the gradient ascent:

B(cid:88)

θ1
t = θ0

t + α · ∇θ

Ht(xθ0

i t  pθ0

i t+1  θ0

t ).

(8)

Theorem 2. The update (8) is equivalent to gradient descent method for training networks [19 20].

i=1

3.3 YOPO from PMP’s View Point

Based on the relationship between back-propagation and the Pontryagin’s Maximum Principle  in this
section  we provide a new understanding of YOPO  i.e. solving the PMP for the differential game.
Observing that  in the PMP  the adversary η is only coupled with the weight of the ﬁrst layer θ0. Thus
we can update the adversary via minimizing the Hamiltonian function instead of directly attacking
the loss function  described in Algorithm 1.
For YOPO-m-n  to approximate the exact minimization of the Hamiltonian  we perform n times
gradient descent to update the adversary. Furthermore  in order to make the calculation of the
adversary more accurate  we iteratively pass one data point m times. Besides  the network weights
are optimized via performing the gradient ascent to Hamiltonian  resulting in the gradient-based
YOPO proposed in Section 2.2.

4 Experiments

4.1 YOPO for Adversarial Training

To demonstrate the effectiveness of YOPO  we conduct experiments on MNIST and CIFAR10. We
ﬁnd that models trained with YOPO have comparable performance with that of the PGD adversarial
training  but with a much fewer computational cost. We also compare our method with a concurrent
method "For Free" [30]  and the result shows that our algorithm can achieve comparable performance
with around 2/3 GPU time of their ofﬁcial implementation.

MNIST We achieve comparable results with the best in [5] within 250 seconds  while it takes
PGD-40 more than 1250s to reach the same level. The accuracy-time curve is shown in Figuire 3(a).
Quantitative results can be seen in supplementary materials. Naively reducing the backprop times of
PGD-40 to PGD-10 will harm the robustness  as can be seen in supplementary materials.
CIFAR10.
[26] performs a 7-step PGD to generate adversary while training. As a comparison  we
test YOPO-3-5 and YOPO-5-3 with a step size of 2/255. Quantitative results can be seen in Table 1
and supplementary materials.
Under PreAct-Res18  for YOPO-5-3  it achieves comparable robust accuracy with [26] with around
half computation for every epoch. The accuracy-time curve is shown in Figuire 3(b).The quantitative
results can be seen in supplementary materials.

7

Algorithm 1 YOPO (You Only Propagate Once)

Randomly initialize the network parameters or using a pre-trained network.
repeat

Randomly select a mini-batch B = {(x1  y1) ···   (xB  yB)} from training set.
Initialize ηi  i = 1  2 ···   B by sampling from a uniform distribution between [-  ]
for j = 1 to m do
xi 0 = xi + ηj
for t = 0 to T − 1 do

i   i = 1  2 ···   B

xi t+1 = ∇pHt(xi t  pi t+1  θt)  i = 1  2 ···   B

end for
pi T = − 1
for t = T − 1 to 0 do

B∇(cid:96)(x∗

i T )  i = 1  2 ···   B

pi t = ∇xHt(xi t  pi t+1  θt)  i = 1  2 ···   B
end for
i = arg minηi H0(xi 0 + ηi  pi 0  θ0)  i = 1  2 ···   B
ηj

end for
for t = T − 1 to 1 do

end for
θ0 = arg maxθ0
until Convergence

1
m

(cid:80)B
(cid:80)m

(cid:80)B

θt = arg maxθt

i=1 Ht(xi t  pi t+1  θt)

k=1

i=1 H0(xi 0 + ηj

i   pi 1  θ0)

(a) "Samll CNN" [46] Result on MNIST

(b) PreAct-Res18 Results on CIFAR10

Figure 3: Performance w.r.t. training time

As for Wide ResNet34  YOPO-5-3 still achieves similar acceleration against PGD-10  as shown in
Table 1. We also test PGD-3/5 to show that naively reducing backward times for this minmax prob-
lem [26] cannot produce comparable results within the same computation time as YOPO. Meanwhile 
YOPO-3-5 can achieve more aggressive speed-up with only a slight drop in robustness.

4.2 YOPO for TRADES

TRADES [46] formulated a new min-max objective function of adversarial defense and achieves the
state-of-the-art adversarial defense results. The experiment details are in supplementary material  and
quantitative results are demonstrated in Table 2.

Training Methods
TRADES-10 [46]

TRADES-YOPO-3-4 (Ours)
TRADES-YOPO-2-5 (Ours)

Clean Data

PGD-20 Attack CW Attack

Training Time (mins)

86.14%
87.82%
88.15%

44.50%
46.13%
42.48%

58.40%
59.48%
59.25%

633
259
218

Table 2: Results of training PreAct-Res18 for CIFAR10 with TRADES objective

8

5timesfaster4timesfasterTraining Methods Clean Data

PGD-20 Attack

Training Time (mins)

Natural train
PGD-3 [26]
PGD-5 [26]
PGD-10 [26]
Free-8 [30]1

233
1134
1574
2713
667
299
YOPO-3-5 (Ours)
476
YOPO-5-3 (Ours)
1 Code from https://github.com/ashafahi/free_adv_train.

Table 1: Results of Wide ResNet34 for CIFAR10.

95.03%
90.07%
89.65%
87.30%
86.29%
87.27%
86.70%

0.00%
39.18%
43.85%
47.04%
47.00%
43.04%
47.98%

5 Conclusion

In this work  we have developed an efﬁcient strategy for accelerating adversarial training. We recast
the adversarial training of deep neural networks as a discrete time differential game and derive a
Pontryagin’s Maximum Principle (PMP) for it. Based on this maximum principle  we discover
that the adversary is only coupled with the weights of the ﬁrst layer. This motivates us to split the
adversary updates from the back-propagation gradient calculation. The proposed algorithm  called
YOPO  avoids computing full forward and backward propagation for too many times  thus effectively
reducing the computational time as supported by our experiments.

Acknowledgement

We thank Di He and Long Chen for beneﬁcial discussion. Zhanxing Zhu is supported in part by
National Natural Science Foundation of China (No.61806009)  Beijing Natural Science Foundation
(No. 4184090) and Beijing Academy of Artiﬁcial Intelligence (BAAI). Bin Dong is supported in part
by Beijing Natural Science Foundation (No. Z180001) and Beijing Academy of Artiﬁcial Intelligence
(BAAI). Dinghuai Zhang is supported by the Elite Undergraduate Training Program of Applied Math
of the School of Mathematical Sciences at Peking University.

9

References
[1] Armin Askari  Geoffrey Negiar  Rajiv Sambharya  and Laurent El Ghaoui. Lifted neural

networks. arXiv preprint arXiv:1805.01532  2018.

[2] Anish Athalye  Nicholas Carlini  and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420 
2018.

[3] Vladimir Grigor’evich Boltyanskii  Revaz Valer’yanovich Gamkrelidze  and Lev Semenovich
Pontryagin. The theory of optimal processes. i. the maximum principle. Technical report  TRW
SPACE TECHNOLOGY LABS LOS ANGELES CALIF  1960.

[4] Peng Cao  Yilun Xu  Yuqing Kong  and Yizhou Wang. Max-mig: an information theoretic

approach for joint learning from crowds. arXiv preprint arXiv:1905.13436  2019.

[5] Tian Qi Chen  Yulia Rubanova  Jesse Bettencourt  and David K Duvenaud. Neural ordinary
differential equations. In Advances in Neural Information Processing Systems  pages 6572–6583 
2018.

[6] Moustapha Cisse  Piotr Bojanowski  Edouard Grave  Yann Dauphin  and Nicolas Usunier.
Parseval networks: Improving robustness to adversarial examples. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70  pages 854–863. JMLR. org  2017.

[7] Lawrence C Evans. An introduction to mathematical optimal control theory. Lecture Notes 

University of California  Department of Mathematics  Berkeley  2005.

[8] Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep learning. MIT press  2016.

[9] Fangda Gu  Armin Askari  and Laurent El Ghaoui. Fenchel lifted networks: A lagrange

relaxation of neural network training. arXiv preprint arXiv:1811.08039  2018.

[10] Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems 

34(1):014004  2017.

[11] Zhouyuan Huo  Bin Gu  Qian Yang  and Heng Huang. Decoupled parallel backpropagation

with convergence guarantee. arXiv preprint arXiv:1804.10574  2018.

[12] Andrew Ilyas  Ajil Jalal  Eirini Asteri  Constantinos Daskalakis  and Alexandros G Dimakis.
The robust manifold defense: Adversarial training using generative models. arXiv preprint
arXiv:1712.09196  2017.

[13] Max Jaderberg  Wojciech Marian Czarnecki  Simon Osindero  Oriol Vinyals  Alex Graves 
David Silver  and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages
1627–1635. JMLR. org  2017.

[14] Daniel Jakubovitz and Raja Giryes. Improving dnn robustness to adversarial attacks using
In Proceedings of the European Conference on Computer Vision

jacobian regularization.
(ECCV)  pages 514–529  2018.

[15] Alexey Kurakin  Ian Goodfellow  and Samy Bengio. Adversarial machine learning at scale.

arXiv preprint arXiv:1611.01236  2016.

[16] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. nature  521(7553):436 

2015.

[17] Yann LeCun  D Touresky  G Hinton  and T Sejnowski. A theoretical framework for back-
propagation. In Proceedings of the 1988 connectionist models summer school  volume 1  pages
21–28. CMU  Pittsburgh  Pa: Morgan Kaufmann  1988.

[18] Jia Li  Cong Fang  and Zhouchen Lin. Lifted proximal operator machines. arXiv preprint

arXiv:1811.01501  2018.

10

[19] Qianxiao Li  Long Chen  Cheng Tai  and E Weinan. Maximum principle based algorithms for

deep learning. The Journal of Machine Learning Research  18(1):5998–6026  2017.

[20] Qianxiao Li and Shuji Hao. An optimal control approach to deep learning and applications to
discrete-weight neural networks. In Jennifer Dy and Andreas Krause  editors  Proceedings of
the 35th International Conference on Machine Learning  volume 80 of Proceedings of Machine
Learning Research  pages 2985–2994  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018.
PMLR.

[21] Xuechen Li  Denny Wu  Lester Mackey  and Murat A Erdogdu. Stochastic runge-kutta acceler-

ates langevin monte carlo and beyond. arXiv preprint arXiv:1906.07868  2019.

[22] Ji Lin  Chuang Gan  and Song Han. Defensive quantization: When efﬁciency meets robustness.

In International Conference on Learning Representations  2019.

[23] Yiping Lu  Aoxiao Zhong  Quanzheng Li  and Bin Dong. Beyond ﬁnite layer neural net-
works: Bridging deep architectures and numerical differential equations. arXiv preprint
arXiv:1710.10121  2017.

[24] Tiange Luo  Tianle Cai  Mengxiao Zhang  Siyu Chen  and Liwei Wang. RANDOM MASK:

Towards robust convolutional neural networks  2019.

[25] Pingchuan Ma  Yunsheng Tian  Zherong Pan  Bo Ren  and Dinesh Manocha. Fluid directed
rigid body control using deep reinforcement learning. ACM Transactions on Graphics (TOG) 
37(4):96  2018.

[26] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations  2018.

[27] Seyed-Mohsen Moosavi-Dezfooli  Alhussein Fawzi  and Pascal Frossard. Deepfool: a simple
and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 2574–2582  2016.

[28] Lev Semenovich Pontryagin. Mathematical theory of optimal processes. CRC  1987.

[29] Haifeng Qian and Mark N Wegman. L2-nonexpansive neural networks. arXiv preprint

arXiv:1802.07896  2018.

[30] Ali Shafahi  Mahyar Najibi  Amin Ghiasi  Xu Zeng  John Dickerson  Christoph Studer  Larry
S. Davis  Gavin Taylor  and Tom Goldstein. Adversarial training for free! arXiv preprint
arXiv:1904.12843  2019.

[31] Yang Song  Taesup Kim  Sebastian Nowozin  Stefano Ermon  and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. arXiv
preprint arXiv:1710.10766  2017.

[32] Sho Sonoda and Noboru Murata. Transport analysis of inﬁnitely deep neural network. The

Journal of Machine Learning Research  20(1):31–82  2019.

[33] Ke Sun  Zhanxing Zhu  and Zhouchen Lin. Enhancing the robustness of deep neural networks

by boundary conditional gan. arXiv preprint arXiv:1902.11029  2019.

[34] Jan Svoboda  Jonathan Masci  Federico Monti  Michael Bronstein  and Leonidas Guibas.
Peernets: Exploiting peer wisdom against adversarial attacks. In International Conference on
Learning Representations  2019.

[35] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfel-
low  and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 
2013.

[36] Gavin Taylor  Ryan Burmeister  Zheng Xu  Bharat Singh  Ankit Patel  and Tom Goldstein.
Training neural networks without gradients: A scalable admm approach. In International
conference on machine learning  pages 2722–2731  2016.

11

[37] Matthew Thorpe and Yves van Gennip. Deep limits of residual neural networks. arXiv preprint

arXiv:1810.11741  2018.

[38] Abraham Wald. Contributions to the theory of statistical estimation and testing hypotheses. The

Annals of Mathematical Statistics  10(4):299–326  1939.

[39] Bao Wang  Binjie Yuan  Zuoqiang Shi  and Stanley J Osher. Enresnet: Resnet ensemble via the

feynman-kac formalism. arXiv preprint arXiv:1811.10745  2018.

[40] E Weinan. A proposal on machine learning via dynamical systems. Communications in

Mathematics and Statistics  5(1):1–11  2017.

[41] E Weinan  Jiequn Han  and Qianxiao Li. A mean-ﬁeld optimal control formulation of deep

learning. Research in the Mathematical Sciences  6(1):10  2019.

[42] Cihang Xie  Yuxin Wu  Laurens van der Maaten  Alan Yuille  and Kaiming He. Feature

denoising for improving adversarial robustness. arXiv preprint arXiv:1812.03411  2018.

[43] Weilin Xu  David Evans  and Yanjun Qi. Feature squeezing: Detecting adversarial examples in

deep neural networks. arXiv preprint arXiv:1704.01155  2017.

[44] Yilun Xu  Peng Cao  Yuqing Kong  and Yizhou Wang. L_dmi: An information-theoretic

noise-robust loss function. arXiv preprint arXiv:1909.03388  2019.

[45] Nanyang Ye and Zhanxing Zhu. Bayesian adversarial learning. In Advances in Neural Informa-

tion Processing Systems  pages 6892–6901  2018.

[46] Hongyang Zhang  Yaodong Yu  Jiantao Jiao  Eric P Xing  Laurent El Ghaoui  and Michael I
Jordan. Theoretically principled trade-off between robustness and accuracy. arXiv preprint
arXiv:1901.08573  2019.

[47] Jingfeng Zhang  Bo Han  Laura Wynter  Kian Hsiang Low  and Mohan Kankanhalli. Towards

robust resnet: A small step but a giant leap. arXiv preprint arXiv:1902.10887  2019.

[48] Xiaoshuai Zhang  Yiping Lu  Jiaying Liu  and Bin Dong. Dynamically unfolding recurrent
restorer: A moving endpoint control method for image restoration. In International Conference
on Learning Representations  2019.

[49] Daniel Zügner  Amir Akbarnejad  and Stephan Günnemann. Adversarial attacks on neural
networks for graph data. In Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining  pages 2847–2856. ACM  2018.

12

,Maren Mahsereci
Philipp Hennig
Zequn Jie
Xiaodan Liang
Jiashi Feng
Xiaojie Jin
Wen Lu
Shuicheng Yan
El Mahdi El Mhamdi
Hadrien Hendrikx
Alexandre Maurer
Yixi Xu
Xiao Wang
Dinghuai Zhang
Tianyuan Zhang
Yiping Lu
Zhanxing Zhu
Bin Dong