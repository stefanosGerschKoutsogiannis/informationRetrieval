2013,A Comparative Framework for Preconditioned Lasso Algorithms,The Lasso is a cornerstone of modern multivariate data analysis  yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of \emph{Preconditioned Lasso} algorithms that pre-multiply $X$ and $y$ by matrices $P_X$  $P_y$ prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difficult because the performance of all of these methods depends critically on an auxiliary penalty parameter $\lambda$. In this paper we propose an agnostic  theoretical framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose $\lambda$. We apply our framework to three Preconditioned Lasso instances and highlight when they will outperform the Lasso. Additionally  our theory offers insights into the fragilities of these algorithms to which we provide partial solutions.,A Comparative Framework for
Preconditioned Lasso Algorithms

Fabian L. Wauthier
Statistics and WTCHG
University of Oxford

flw@stats.ox.ac.uk

Nebojsa Jojic

Microsoft Research  Redmond
jojic@microsoft.com

Michael I. Jordan

Computer Science Division

University of California  Berkeley
jordan@cs.berkeley.edu

Abstract

The Lasso is a cornerstone of modern multivariate data analysis  yet its perfor-
mance suffers in the common situation in which covariates are correlated. This
limitation has led to a growing number of Preconditioned Lasso algorithms that
pre-multiply X and y by matrices PX  Py prior to running the standard Lasso. A
direct comparison of these and similar Lasso-style algorithms to the original Lasso
is difﬁcult because the performance of all of these methods depends critically on
an auxiliary penalty parameter λ. In this paper we propose an agnostic framework
for comparing Preconditioned Lasso algorithms to the Lasso without having to
choose λ. We apply our framework to three Preconditioned Lasso instances and
highlight cases when they will outperform the Lasso. Additionally  our theory
reveals fragilities of these algorithms to which we provide partial solutions.

1

Introduction

Variable selection is a core inferential problem in a multitude of statistical analyses. Confronted with
a large number of (potentially) predictive variables  the goal is to select a small subset of variables
that can be used to construct a parsimonious model. Variable selection is especially relevant in linear
observation models of the form

y = Xβ∗ + w with w ∼ N (0  σ2In×n) 

(1)
where X is an n × p matrix of features or predictors  β∗ is an unknown p-dimensional regression
parameter  and w is a noise vector. In high-dimensional settings where n (cid:28) p  ordinary least squares
is generally inappropriate. Assuming that β∗ is sparse (i.e.  the support set S(β∗) (cid:44) {i|β∗i (cid:54)= 0} has
cardinality k < n)  a mainstay algorithm for such settings is the Lasso [10]:
2 + λ||β||1 .

||y − Xβ||2

(2)

Lasso: ˆβ = argminβ∈Rp

1
2n

For a particular choice of λ  the variable selection properties of the Lasso can be analyzed by quan-
tifying how well the estimated support S( ˆβ) approximates the true support S(β∗). More careful
analyses focus instead on recovering the signed support S±(β∗) 

(cid:40) +1 if β∗i > 0

−1
0

if β∗i < 0
o.w.

S±(β∗i ) (cid:44)

.

(3)

Theoretical developments during the last decade have shed light onto the support recovery proper-
ties of the Lasso and highlighted practical difﬁculties when the columns of X are correlated. These
developments have led to various conditions on X for support recovery  such as the mutual incoher-
ence or the irrepresentable condition [1  3  8  12  13].

1

In recent years  several modiﬁcations of the standard Lasso have been proposed to improve its
support recovery properties [2  7  14  15]. In this paper we focus on a class of “Preconditioned
Lasso” algorithms [5  6  9] that pre-multiply X and y by suitable matrices PX and Py to yield
¯X = PX X  ¯y = Pyy  prior to running Lasso. Thus  the general strategy of these methods is

(cid:12)(cid:12)(cid:12)(cid:12)¯y − ¯Xβ(cid:12)(cid:12)(cid:12)(cid:12)2

2 + ¯λ||β||1 .

Preconditioned Lasso: ˆ¯β = argminβ∈Rp

1
2n

(4)

Although this class of algorithms often compares favorably to the Lasso in practice  our theoretical
understanding of them is at present still fairly poor. Huang and Jojic [5]  for example  consider
only empirical evaluations  while both Jia and Rohe [6] and Paul et al. [9] consider asymptotic
consistency under various assumptions. Important and necessary as they are  consistency results do
not provide insight into the relative performance of Preconditioned Lasso variants for ﬁnite data sets.
In this paper we provide a new theoretical basis for making such comparisons. Although the focus
of the paper is on problems of the form of Eq. (4)  we note that the core ideas can also be applied to
algorithms that right-multiply X and/or y with some matrices (e.g.  [4  11]).
For particular instances of X  β∗  we want to discover whether a given Preconditioned Lasso al-
gorithm following Eq. (4) improves or degrades signed support recovery relative to the standard
Lasso of Eq. (2). A major roadblock to a one-to-one comparison are the auxiliary penalty param-
eters  λ  ¯λ  which trade off the (cid:96)1 penalty to the quadratic objective in both Eq. (2) and Eq. (4).
A correct choice of penalty parameter is essential for signed support recovery: If it is too small 
the algorithm behaves like ordinary least squares; if it is too large  the estimated support may be
empty. Unfortunately  in all but the simplest cases  pre-multiplying data X  y by matrices PX  Py
changes the relative geometry of the (cid:96)1 penalty contours to the elliptical objective contours in a
nontrivial way. Suppose we wanted to compare the Lasso to the Preconditioned Lasso by choosing
for each λ in Eq. (2) a suitable  matching ¯λ in Eq. (4). For a fair comparison  the resulting map-
ping would have to capture the change of relative geometry induced by preconditioning of X  y 
i.e. ¯λ = f (λ  X  y  PX   Py). It seems difﬁcult to theoretically characterize such a mapping. Fur-
thermore  it seems unlikely that a comparative framework could be built by independently choosing
“ideal” penalty parameters λ  ¯λ: Meinshausen and B¨uhlmann [8]  for example  demonstrate that a
seemingly reasonable oracle estimator of λ will not lead to consistent support recovery in the Lasso.
In the Preconditioned Lasso literature this problem is commonly sidestepped either by resorting
to asymptotic comparisons [6  9]  empirically comparing regularization paths [5]  or using model-
selection techniques which aim to choose reasonably “good” matching penalty parameters [6]. We
deem these approaches to be unsatisfactory—asymptotic and empirical analyses provide limited in-
sight  and model selection strategies add a layer of complexity that may lead to unfair comparisons.
It is our view that all of these approaches place unnecessary emphasis on particular choices of
penalty parameter. In this paper we propose an alternative strategy that instead compares the Lasso to
the Preconditioned Lasso by comparing data-dependent upper and lower penalty parameter bounds.
Speciﬁcally  we give bounds (λu  λl) on λ so that the Lasso in Eq. (2) is guaranteed to recover the
signed support iff λl < λ < λu. Consequently  if λl > λu signed support recovery is not possible.
The Preconditioned Lasso in Eq. (4) uses data ¯X = PX X  ¯y = Pyy and will thus induce new
bounds (¯λu  ¯λl) on ¯λ. The comparison of Lasso and Preconditioned Lasso on an instance X  β∗
then proceeds by suitably comparing the bounds on λ and ¯λ. The advantage of this approach is that
the upper and lower bounds are easy to compute  even though a general mapping between speciﬁc
penalty parameters cannot be readily derived.
To demonstrate the effectiveness of our framework  we use it to analyze three Preconditioned Lasso
algorithms [5  6  9]. Using our framework we make several contributions: (1) We conﬁrm intuitions
about advantages and disadvantages of the algorithms proposed in [5  9]; (2) We show that for an
SVD-based construction of n × p matrices X  the algorithm in [6] changes the bounds determin-
istically; (3) We show that in the context of our framework  this SVD-based construction can be
thought of as a limit point of a Gaussian construction.
The paper is organized as follows. In Section 2 we will discuss three recent instances of Eq. (4). We
outline our comparative framework in Section 3 and highlight some immediate consequences for [5]
and [9] on general matrices X in Section 4. More detailed comparisons can be made by considering
a generative model for X. In Section 5 we introduce such a model based on a block-wise SVD of X
and then analyze [6] for speciﬁc instances of this generative model. Finally  we show that in terms
of signed support recovery  this generative model can be thought of as a limit point of a Gaussian

2

construction. Section 6 concludes with some ﬁnal thoughts. The proofs of all lemmas and theorems
are in the supplementary material.

2 Preconditioned Lasso Algorithms

Our interest lies in the class of Preconditioned Lasso algorithms that is summarized by Eq. (4).
Extensions to related algorithms  such as [4  11] will follow readily. In this section we focus on three
recent Preconditioned Lasso examples and instantiate the matrices PX   Py appropriately. Detailed
derivations can be found in the supplementary material. For later reference  we will denote each
algorithm by the author initials.
Huang and Jojic [5] (HJ). Huang and Jojic proposed Correlation Sifting [5]  which  although
not presented as a preconditioning algorithm  can be rewritten as one. Let the SVD of X be X =
U DV (cid:62). Given an algorithm parameter q  let UA be the set of q smallest left singular vectors of X1.
Then HJ amounts to setting
(5)

PX = Py = UAU(cid:62)
A .

Paul et al. [9] (PBHT). An earlier instance of the preconditioning idea was put forward by Paul
et al. [9]. For some algorithm parameter q  let A be the q column indices of X with largest absolute
correlation to y  (i.e.  where |X(cid:62)j y|/||Xj||2 is largest). Deﬁne UA to be the q largest left singular
vectors of XA. With this  PBHT can be expressed as setting
(6)

PX = In×n

Py = UAU(cid:62)
A .

Jia and Rohe [6] (JR).
whitening the matrix X. If X = U DV (cid:62) is full rank  then JR deﬁnes2

Jia and Rohe [6] propose a preconditioning method that amounts to

PX = Py = U(cid:0)DD(cid:62)(cid:1)−1/2

U(cid:62).

If n < p then ¯X ¯X(cid:62) = PX XX(cid:62)P (cid:62)X ∝ In×n and if n > p then ¯X(cid:62) ¯X = X(cid:62)P (cid:62)X PX X ∝ Ip×p.
Both HJ and PBHT estimate a basis UA for a q-dimensional subspace onto which they project y
and/or X. However  since the methods differ substantially in their assumptions  the estimators differ
also. Empirical results in [5] and [9] suggest that the respective assumptions are useful in a variety of
situations. In contrast  JR reweights the column space directions U and requires no extra parameter
q to be estimated.

(7)

3 Comparative Framework

In this section we propose a new comparative approach for Preconditioned Lasso algorithms which
avoids choosing particular penalty parameters λ  ¯λ. We ﬁrst derive upper and lower bounds for λ
and ¯λ respectively so that signed support recovery can be guaranteed iff λ and ¯λ satisfy the bounds.
We then compare estimators by comparing the resulting bounds.

3.1 Conditions for signed support recovery

Before proceeding  we make some deﬁnitions motivated by Wainwright [12]. Suppose that the
support set of β∗ is S (cid:44) S(β∗)  with |S| = k. To simplify notation  we will assume throughout that
S = {1  . . .   k} so that the corresponding off-support set is Sc = {1  . . .   p}\S  with |Sc| = p − k.
Denote by Xj column j of X and by XA the submatrix of X consisting of columns indexed by set
A. Deﬁne the following variables: For all j ∈ Sc and i ∈ S  let

(cid:0)In×n − XS(X(cid:62)S XS)−1X(cid:62)S
(cid:18) 1

(cid:19)−1

(cid:1) w

n

X(cid:62)S XS

n

X(cid:62)S

w
n

.

(8)

(9)

µj = X(cid:62)j XS(X(cid:62)S XS)−1sgn(β∗S)

ηj = X(cid:62)j

γi = e(cid:62)i

X(cid:62)S XS

sgn(β∗S)

i = e(cid:62)i

(cid:18) 1

n

(cid:19)−1

1The choice of smallest singular vectors is considered for matrices X with sharply decaying spectrum.
2We note that Jia and Rohe [6] let D be square  so that it can be directly inverted. If X is not full rank  the

pseudo-inverse of D can be used.

3

(a) Signed support recovery around λl.

(b) Signed support recovery around λu.

Figure 1: Empirical evaluation of the penalty parameter bounds of Lemma 1. For each of 500
synthetic Lasso problems (n = 300  p = 1000  k = 10) we computed λl  λu as per Lemma 1.
Then we ran Lasso using penalty parameters f λl in Figure (a) and f λu in Figure (b)  where the
factor f = 0.5  . . .   1.5. The ﬁgures show the empirical probability of signed support recovery as a
function of the factor f for both λl and λu. As expected  the probabilities change sharply at f = 1.

For the traditional Lasso of Eq. (2)  results in (for example) Wainwright [12] connect settings of λ
with instances of X  β∗  w to certify whether or not Lasso will recover the signed support. We invert
these results and  for particular instances of X  β∗  w  derive bounds on λ so that signed support
recovery is guaranteed if and only if the bounds are satisﬁed. Speciﬁcally  we prove the following
Lemma in the supplementary material.
Lemma 1. Suppose that X(cid:62)S XS is invertible  |µj| < 1 ∀j ∈ Sc  and sgn(β∗i )γi > 0 ∀i ∈ S. Then
the Lasso has a unique solution ˆβ which recovers the signed support (i.e.  S±( ˆβ) = S±(β∗)) if and
only if λl < λ < λu  where

(cid:12)(cid:12)(cid:12)(cid:12) β∗i + i

(cid:12)(cid:12)(cid:12)(cid:12)+

 

(10)

λl = max
j∈Sc

ηj

(2(cid:74)ηj > 0(cid:75) − 1) − µj

λu = min
i∈S

γi

(cid:74)·(cid:75) denotes the indicator function and | · |+ = max(0 ·) denotes the hinge function. On the other

hand  if X(cid:62)S XS is not invertible  then the signed support cannot in general be recovered.
Lemma 1 recapitulates well-worn intuitions about when the Lasso has difﬁculty recovering the
signed support. For instance  assuming that w has symmetric distribution with mean 0  if 1 − |µj|
is small (i.e.  the irrepresentable condition almost fails to hold)  then λl will tend to be large. In
extreme cases we might have λl > λu so that signed support recovery is impossible. Figure 1 em-
pirically validates the bounds of Lemma 1 by estimating probabilities of signed support recovery for
a range of penalty parameters on synthetic Lasso problems.

3.2 Comparisons

In this paper we propose to compare a preconditioning algorithm to the traditional Lasso by compar-
ing the penalty parameter bounds produced by Lemma 1. As highlighted in Eq. 4  the precondition-
ing framework runs Lasso on modiﬁed variables ¯X = PX X  ¯y = Pyy. For the purpose of applying
Lemma 1  these transformations induce a new noise vector

¯w = ¯y − ¯Xβ∗ = Py (Xβ∗ + w) − PX Xβ∗.

(11)
Note that if PX = Py then ¯w = Pyw. Provided the conditions of Lemma 1 hold for ¯X  β∗ we can
deﬁne updated variables ¯µj  ¯γi  ¯ηj  ¯i from which the bounds ¯λu  ¯λl on the penalty parameter ¯λ can
be derived. In order for our comparison to be scale-invariant  we will compare algorithms by ratios
of resulting penalty parameter bounds. That is  we deem a Preconditioned Lasso algorithm to be
more effective than the traditional Lasso if ¯λu/¯λl > λu/λl. Intuitively  the upper bound ¯λu is then
disproportionately larger than ¯λl relative to λu and λl  which in principle allows easier tuning of ¯λ3.
We will later encounter the special case ¯λu (cid:54)= 0  ¯λl = 0 in which case we deﬁne ¯λu/¯λl (cid:44) ∞ to
indicate that the preconditioned problem is very easy. If ¯λu/¯λl < 1 then signed support recovery is
in general impossible. Finally  to match this intuition  we deﬁne ¯λu/¯λl (cid:44) 0 if ¯λu = ¯λl = 0.

3Other functions of λl  λu and ¯λl  ¯λu could also be considered. However  we ﬁnd the ratio to be a particu-

larly intuitive measure.

4

0.511.500.20.40.60.81P(S±(ˆβ)=S±(β∗))f  0.511.500.20.40.60.81P(S±(ˆβ)=S±(β∗))f  4 General Comparisons

We begin our comparisons with some immediate consequences of Lemma 1 for HJ and PBHT. In
order to highlight the utility of the proposed framework  we focus in this section on special cases of
PX   Py. The framework can of course also be applied to general matrices PX   Py. As we will see 
both HJ and PBHT have the potential to improve signed support recovery relative to the traditional
Lasso  provided the matrices PX   Py are suitably estimated. The following notation will be used
during our comparisons: We will write ¯A (cid:22) A to indicate that random variable A stochastically
dominates ¯A  that is  ∀t P( ¯A ≥ t) ≤ P(A ≥ t). We also let US be a minimal basis for the column

space of the submatrix XS  and deﬁne span(US) =(cid:8)x(cid:12)(cid:12)∃c ∈ Rk s.t. x = USc(cid:9) ⊆ Rn. Finally  we

let USc be a minimal basis for the orthogonal complement of span(US).
Consequences for HJ. Recall from Section 2 that HJ uses PX = Py = UAU(cid:62)
A
column basis estimated from X. We have the following theorem:
Theorem 1. Suppose that the conditions of Lemma 1 are met for a ﬁxed instance of X  β∗.
span(US) ⊆ span(UA)  then after preconditioning using HJ the conditions continue to hold  and
(12)

  where UA is a
If

λu
λl

(cid:22) ¯λu
¯λl

 

where the stochasticity on both sides is due to independent noise vectors w. On the other hand  if
X(cid:62)S P (cid:62)X PX XS is not invertible  then HJ cannot in general recover the signed support.
We brieﬂy sketch the proof of Theorem 1. If span(US) ⊆ span(UA) then plugging in the deﬁnition
of PX into ¯µj  ¯γi  ¯ηj  ¯i  one can derive the following
(13)

¯γi = γi

(14)
If span(UA) = span(US)  then it is easy to see that ¯ηj = 0. Notice that because ¯µj and ¯γi are un-
changed  if the conditions of Lemma 1 hold for the original Lasso problem (i.e.  X(cid:62)S XS is invertible 
|µj| < 1 ∀j ∈ Sc and sgn(β∗i )γi > 0 ∀i ∈ S)  they will continue to hold for the preconditioned
problem. Suppose then that the conditions set forth in Lemma 1 are met. With some additional work
one can show that

¯i = i.

A

¯µj = µj
¯ηj = X(cid:62)j

(cid:0)In×n − USU(cid:62)S

(cid:1) UAU(cid:62)

w
n

¯λu = min
i∈S

= λu

¯λl = max
j∈Sc

¯ηj

(2(cid:74)¯ηj > 0(cid:75) − 1) − ¯µj

(cid:22) λl.

(15)

The result then follows by showing that ¯λl  λl are both independent of ¯λu = λu. Note that if
span(UA) = span(US)  then ¯λl = 0 and so ¯λu/¯λl (cid:44) ∞.
In the more common case when
span(US) (cid:54)⊆ span(UA) the performance of the Lasso depends on how misaligned UA and US are. In
extreme cases  X(cid:62)S P (cid:62)X PX XS is singular and so signed support recovery is not in general possible.
Consequences for PBHT. Recall from Section 2 that PBHT uses PX = In×n  Py = UAU(cid:62)
 
A
where UA is a column basis estimated from X. We have the following theorem.
Theorem 2. Suppose that the conditions of Lemma 1 are met for a ﬁxed instance of X  β∗.
If
span(US) ⊆ span(UA)  after preconditioning using PBHT the conditions continue to hold  and

(cid:12)(cid:12)(cid:12)(cid:12) β∗i + ¯i

¯γi

(cid:12)(cid:12)(cid:12)(cid:12)+

λu
λl

(cid:22) ¯λu
¯λl

 

(16)

where the stochasticity on both sides is due to independent noise vectors w. On the other hand  if
span(USc ) = span(UA)  then PBHT cannot recover the signed support.
As before  we sketch the proof to build some intuition. Because PBHT does not set PX = Py as HJ
does  there is no danger of X(cid:62)S P (cid:62)X PX XS becoming singular. On the other hand  this complicates
−
the form of the induced noise vector ¯w. Plugging PX and Py into Eq. (11)  we ﬁnd ¯w = (UAU(cid:62)
A
A w. However  even though the noise has a more complicated form  derivations
In×n)Xβ∗ + UAU(cid:62)
in the supplementary material show that if span(US) ⊆ span(UA)  then
(17)

¯γi = γi

¯µj = µj
¯ηj = X(cid:62)j

(cid:0)In×n − USU(cid:62)S

(cid:1) UAU(cid:62)

A

w
n

¯i = i.

(18)

5

(a) Empirical validation of Theorems 1 and 2.

(b) Evaluation of JR on Gaussian ensembles.

Figure 2: Experimental evaluations. Figure (a) shows empirical c.d.f.’s of penalty parameter bounds
ratios estimated from 1000 variable selection problems. Each problem consists of Gaussians X and
w  and β∗  with n = 100  p = 300  k = 5. The blue curve shows the c.d.f. for λu/λl estimated on
  where span(US) ⊆
the original data (Lasso). Then we projected the data using PX = Py = UAU(cid:62)
A
span(UA) but dim(UA) = dim(span(UA)) is variable (see legend)  and estimated the resulting
c.d.f. for the updated bounds ratio ¯λu/¯λl. As predicted by Theorems 1 and 2  λu/λl (cid:22) ¯λu/¯λl. In
Figure (b) the blue curve shows the scale factor (p − k)/(n + pκ2 − k) predicted by Theorem 3 for
1 − (n/p). The red curve plots the corresponding
problems constructed from Eq. (19) for κ = f
factor estimated from the Gaussian construction in Eq. (25) (n = 100  m = 2000  p = 200  k = 5)
using the same ΣS  ΣSc as in Theorem 3  averaged over 50 problem instances and with error bars
for one standard deviation. As in Theorem 3  the factor is approximately 1 if f = 1.

(cid:112)

As with HJ  if span(UA) = span(US)  then ¯ηj = 0. Because ¯µj and ¯γi are again unchanged 
the conditions of Lemma 1 will continue to hold for the preconditioned problem if they hold for
the original Lasso problem. With the previous equalities established  the remainder of the proof
is identical to that of Theorem 1. The fact that the above ¯µj  ¯ηj  ¯γi  ¯i are identical to those of HJ
depends crucially on the fact that span(US) ⊆ span(UA). In general the values will differ because
PBHT sets PX = In×n  but HJ does not.
On the other hand  if span(US) (cid:54)⊆ span(UA) then the distribution of ¯i depends on how misaligned
UA and US are. In the extreme case when span(USc ) = span(UA)  one can show that ¯i = −β∗i  
which results in ¯λu = 0  ¯λl (cid:22) λl. Because P(¯λl ≥ 0) = 1  signed support recovery is not possible.

Remarks. Our theoretical analyses show that both HJ and PBHT can indeed lead to improved
signed support recovery relative to the Lasso on ﬁnite datasets. To underline our ﬁndings  we em-
pirically validate Theorems 1 and 2 in Figure 2(a)  where we plot estimated c.d.f.’s for penalty
parameter bounds ratios of Lasso and Preconditioned Lasso for various subspaces UA. Our theo-
rems focussed on speciﬁc settings of PX   Py and ignored others. In general  the gains of HJ and
PBHT over Lasso depend on how much the decoy signals in XSc are suppressed and how much of
the true signal due to XS is preserved. Further comparison of HJ and PBHT must thus analyze how
the subspaces span(UA) are estimated in the context of the assumptions made in [5] and [9]. A ﬁnal
note concerns the dimension of the subspace span(UA). Both HJ and PBHT were proposed with the
implicit goal of ﬁnding a basis UA that has the same span as US. This of course requires estimating
|S| = k by q  which adds another layer of complexity to these algorithms. Theorems 1 and 2 sug-
gest that underestimating k can be more detrimental to signed support recovery than overestimating
it. By overestimating q > k  we can trade off milder improvement when span(US) ⊆ span(UA)
against poor behavior should we have span(US) (cid:54)⊆ span(UA).

5 Model-Based Comparisons

In the previous section we used Lemma 1 in conjunction with assumptions on UA to make statements
about HJ and PBHT. Of course  the quality of the estimated UA depends on the speciﬁc instances
X  β∗  w  which hinders a general analysis. Similarly  a direct application of Lemma 1 to JR yields
bounds that exhibit strong X dependence.
It is possible to crystallize prototypical examples by
specializing X and w to come from a generative model. In this section we brieﬂy present this model
and will show the resulting penalty parameter bounds for JR.

6

01000200030004000500000.20.40.60.81tP(λu/λl<t)  Lasso55352515100.20.40.60.811.21.400.511.522.5f¯λu/¯λlλu/λl  Orthogonal dataGaussian data5.1 Generative model for X

As discussed in Section 2  many preconditioning algorithms can be phrased as truncating or
reweighting column subspaces associated with X [5  6  9]. This suggests that a natural generative
model for X can be formulated in terms of the SVD of submatrices of X.
Assume p − k > n and let ΣS  ΣSc be ﬁxed-spectrum matrices of dimension n × k and n ×
p − k respectively. We will assume throughout this paper that the top left “diagonal” entries of
ΣS  ΣSc are positive and the remainder is zero. Furthermore  we let U  VS  VSc be orthonormal
bases of dimension n × n  k × k and p − k × p − k respectively. We assume that these bases are
chosen uniformly at random from the corresponding Stiefel manifold. As before and without loss of
generality  suppose S = {1  . . .   k}. Then we let the Lasso problem be

y = Xβ∗ + w with X = U(cid:2)ΣSV (cid:62)S   ΣScV (cid:62)Sc

(19)
To ensure that the column norms of X are controlled  we compute the spectra ΣS  ΣSc by normal-
izing spectra ˆΣS and ˆΣSc with arbitrary positive elements on the diagonal. Speciﬁcally  we let

(cid:3) w ∼ N (0  σ2In×n) 
(cid:112)

√

(20)

ΣS =

ˆΣS
|| ˆΣS||F

kn

ΣSc =

ˆΣSc
|| ˆΣSc||F

(p − k)n.

We verify in the supplementary material that with these assumptions the squared column norms of
X are in expectation n (provided the orthonormal bases are chosen uniformly at random).
Intuition. Note that any matrix X can be decomposed using a block-wise SVD as

X = [XS  XSc] = U(cid:2)ΣSV (cid:62)S   T ΣScV (cid:62)Sc

(cid:3)  

(21)
with orthonormal bases U  T  VS  VSc. Our model in Eq. (19) is only a minor restriction of this
model  where we set T = In×n. To develop more intuition  let us temporarily set VS = Ik×k 
VSc = Ip−k×p−k. Then X = [XS  XSc] = U [ΣS  ΣSc] and we see that up to scaling XS equals
the ﬁrst k columns of XSc. The difﬁculty for Lasso thus lies in correctly selecting the columns in
XS  which are highly correlated with the ﬁrst few columns in XSc.

5.2 Piecewise constant spectra

For notational clarity we will now focus on a special case of the above model. To begin  we develop
some notation. In previous sections we used US to denote a basis for the column space of XS. We
will continue to use this notation  and let US contain the ﬁrst k columns of U. Accordingly  we
denote the last n − k columns of U by USc. We let the diagonal elements of ΣS  ˆΣS  ΣSc  ˆΣSc
be identiﬁed by their column indices. That is  the diagonal entries σS c of ΣS and ˆσS c of ˆΣS
are indexed by c ∈ {1  . . .   k}; the diagonal entries σSc c of ΣSc and ˆσSc c of ˆΣSc are indexed
by c ∈ {1  . . .   n}. Each of the diagonal entries in ΣS  ΣSc is associated with a column of U.
The set of diagonal entries of ΣS and ΣSc associated with US is σ(S) = {1  . . .   k} and the set
of diagonal entries in ΣSc associated with USc is σ(Sc) = {1  . . .   n}\σ(S). We will construct
spectrum matrices ΣS  ΣSc that are piecewise constant on their diagonals. For some κ ≥ 0  we let
ˆσS i = 1  ˆσSc i = κ ∀i ∈ σ(S) and ˆσSc j = 1 ∀j ∈ σ(Sc).

Consequences for JR. Recall that for JR  if X = U DV (cid:62)  then PX = Py = U(cid:0)DD(cid:62)(cid:1)−1/2

We have the following theorem.
Theorem 3. Assume the Lasso problem was generated according to the generative model of
Eq. (19) with ∀i ∈ σ(S)  ˆσS i = 1  ˆσSc i = κ and ∀j ∈ σ(Sc)  ˆσSc j = 1 and that
k(p − k − 1). Then the conditions of Lemma 1 hold before and after precondi-
κ <
tioning using JR. Moreover 

n − k/

(cid:112)

U(cid:62).

√

¯λu
¯λl

=

(p − k)

n + pκ2 − k

λu
λl

.

(22)

In other words  JR deterministically scales the ratio of penalty parameter bounds. The proof idea
is as follows. It is easy to see that X(cid:62)S XS is always invertible. Furthermore  one can show that if

7

√

n − k/

(cid:112)
k(p − k − 1)  we have |µj| < 1 ∀j ∈ Sc and sgn(β∗i )γi > 0 ∀i ∈ S. Thus  by our
κ <
assumptions  the preconditions of Lemma 1 are satisﬁed for the original Lasso problem. Plugging in
the deﬁnitions of ΣS  ΣSc into Eq. (19) we ﬁnd that the SVD becomes X = U DV (cid:62)  where U is the
same column basis as in Eq. (19)  and the diagonal elements of D are determined by κ. Substituting
this into the deﬁnitions of ¯µj  ¯γi  ¯ηj  ¯i  we have that after preconditioning using JR

(cid:18)

(cid:19)

n(p − k)κ2
kκ2 + n − k

¯µj = µj

¯γi =

n +

(kκ2 + n − k)

n(p − k)

¯ηj =

ηj

¯i = i.

γi

(23)

(24)

Thus  if the conditions of Lemma 1 hold for X  β∗  they will continue to hold after precondition-

ing using JR. Furthermore  notice that (2(cid:74)¯ηj > 0(cid:75) − 1) − ¯µj = (2(cid:74)ηj > 0(cid:75) − 1) − µj. Applying
(cid:112)
Lemma 1 then gives the new ratio ¯λu/¯λl as claimed. According to Theorem 3 the ratio ¯λu/¯λl will
1 − (n/p) then PX = Py ∝ In×n and
be larger than λu/λl iff κ <
so JR coincides with standard Lasso.

1 − (n/p). Indeed  if κ =

(cid:112)

5.3 Extension to Gaussian ensembles

1√
n

(cid:3) wm ∼ N(cid:16)

W m(cid:2)ΣSV (cid:62)S   ΣSc V (cid:62)Sc

The construction in Eq. (19) uses an orthonormal matrix U as the column basis of X. At ﬁrst
sight this may appear to be restrictive. However  as we show in the supplementary material  one
can construct Lasso problems using a Gaussian basis W m which lead to penalty parameter bounds
ratios that converge in distribution to those of the Lasso problem in Eq. (19). For some ﬁxed β∗  VS 
VSc  ΣS and ΣSc  generate two independent problems: One using Eq. (19)  and one according to
ym = X mβ∗ + wm with X m =
  (25)
where W m is an m × n standard Gaussian ensemble. Note that an X so constructed is low rank if
n < p. The latter generative model bears some resemblance to Gaussian models considered in Paul
et al. [9] (Eq. (7)) and Jia and Rohe [6] (Proposition 2). Note that while the problem in Eq. (19) uses
n observations with noise variance σ2  Eq. (25) has m observations with noise variance σ2m/n.
The increased variance is necessary because the matrix W m has expected column length m  while
columns in U are of length 1. We will think of n as ﬁxed and will let m → ∞. Let the penalty
parameter bounds ratio induced by the problem in Eq. (19) be λu/λl and that induced by Eq. (25)
be λm
Theorem 4. Let VS  VSc  ΣS  ΣSc and β∗ be ﬁxed. If the conditions of Lemma 1 hold for X  β∗ 
then for m large enough they will hold for X m  β∗. Furthermore  as m → ∞

. Then we have the following result.

0  σ2 m
n

Im×m

u /λm
l

(cid:17)

where the stochasticity on the left is due to W m  wm and on the right is due to w.

λm
u
λm
l

d→ λu
λl

 

(26)

Thus  with respect to the bounds ratio λu/λl  the construction of Eq. (19) can be thought of as the
limiting construction of Gaussian Lasso problems in Eq. (25) for large m. As such  we believe
that Eq. (19) is a useful proxy for less restrictive generative models. Indeed  as the experiment in
Figure 2(b) shows  Theorem 3 can be used to predict the scaling factor for penalty parameter bounds

(cid:1) / (λu/λl)) with good accuracy even for Gaussian ensembles.

ratios (i.e. (cid:0)¯λu/¯λl

6 Conclusions

This paper proposes a new framework for comparing Preconditioned Lasso algorithms to the stan-
dard Lasso which skirts the difﬁculty of choosing penalty parameters. By eliminating this parameter
from consideration  ﬁnite data comparisons can be greatly simpliﬁed  avoiding the use of model
selection strategies. To demonstrate the framework’s usefulness  we applied it to a number of Pre-
conditioned Lasso algorithms and in the process conﬁrmed intuitions and revealed fragilities and
mitigation strategies. Additionally  we presented an SVD-based generative model for Lasso prob-
lems that can be thought of as the limit point of a less restrictive Gaussian model. We believe this
work to be a ﬁrst step towards a comprehensive theory for evaluating and comparing Lasso-style
algorithms and believe that the strategy can be extended to comparing other penalized likelihood
methods on ﬁnite datasets.

8

References
[1] D.L. Donoho  M. Elad  and V.N. Temlyakov. Stable recovery of sparse overcomplete repre-
sentations in the presence of noise. Information Theory  IEEE Transactions on  52(1):6–18 
2006.

[2] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle proper-

ties. Journal of the American Statistical Association  96:1348–1360  2001.

[3] J.J. Fuchs. Recovery of exact sparse representations in the presence of bounded noise. Infor-

mation Theory  IEEE Transactions on  51(10):3601–3608  2005.

[4] H.-C. Huang  N.-J. Hsu  D.M. Theobald  and F.J. Breidt. Spatial Lasso with applications to GIS

model selection. Journal of Computational and Graphical Statistics  19(4):963–983  2010.

[5] J.C. Huang and N. Jojic. Variable selection through Correlation Sifting.

In V. Bafna and
S.C. Sahinalp  editors  RECOMB  volume 6577 of Lecture Notes in Computer Science  pages
106–123. Springer  2011.

[6] J. Jia and K. Rohe. “Preconditioning” to comply with the irrepresentable condition. 2012.
[7] N. Meinshausen. Lasso with relaxation. Technical Report 129  Eidgen¨ossische Technische

Hochschule  Z¨urich  2005.

[8] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with the

Lasso. Annals of Statistics  34(3):1436–1462  2006.

[9] D. Paul  E. Bair  T. Hastie  and R. Tibshirani. “Preconditioning” for feature selection and

regression in high-dimensional problems. Annals of Statistics  36(4):1595–1618  2008.

[10] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society  Series B  58(1):267–288  1994.

[11] R.J. Tibshirani. The solution path of the Generalized Lasso. Stanford University  2011.
[12] M.J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using
IEEE Transactions on Information Theory 

(cid:96)1-constrained quadratic programming (Lasso).
55(5):2183–2202  2009.

[13] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning

Research  7:2541–2563  2006.

[14] H. Zou. The Adaptive Lasso and its oracle properties. Journal of the American Statistical

Association  101:1418–1429  2006.

[15] H. Zou and T. Hastie. Regularization and variable selection via the Elastic Net. Journal of the

Royal Statistical Society  Series B  67:301–320  2005.

9

,Fabian Wauthier
Nebojsa Jojic
Michael Jordan
Daniel Hernández-lobato
Viktoriia Sharmanska
Kristian Kersting
Christoph Lampert
Novi Quadrianto
Wei Shen
KAI ZHAO
Yilu Guo
Alan Yuille