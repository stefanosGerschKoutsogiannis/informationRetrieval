2017,Few-Shot Learning Through an Information Retrieval  Lens,Few-shot learning refers to understanding new concepts from only a few examples. We propose an information retrieval-inspired approach for this problem that is motivated by the increased importance of maximally leveraging all the available information in this low-data regime. We define a training objective that aims to extract as much information as possible from each training batch by effectively optimizing over all relative orderings of the batch points simultaneously. In particular  we view each batch point as a `query' that ranks the remaining ones based on its predicted relevance to them and we define a model within the framework of structured prediction to optimize mean Average Precision over these rankings. Our method achieves impressive results on the standard few-shot classification benchmarks while is also capable of few-shot retrieval.,Few-Shot Learning Through an Information

Retrieval Lens

Eleni Triantaﬁllou
University of Toronto

Vector Institute

Richard Zemel

University of Toronto

Vector Institute

Raquel Urtasun

University of Toronto

Vector Institute

Uber ATG

Abstract

Few-shot learning refers to understanding new concepts from only a few examples.
We propose an information retrieval-inspired approach for this problem that is
motivated by the increased importance of maximally leveraging all the available
information in this low-data regime. We deﬁne a training objective that aims to
extract as much information as possible from each training batch by effectively
optimizing over all relative orderings of the batch points simultaneously. In partic-
ular  we view each batch point as a ‘query’ that ranks the remaining ones based
on its predicted relevance to them and we deﬁne a model within the framework
of structured prediction to optimize mean Average Precision over these rankings.
Our method achieves impressive results on the standard few-shot classiﬁcation
benchmarks while is also capable of few-shot retrieval.

1

Introduction

Recently  the problem of learning new concepts from only a few labelled examples  referred to
as few-shot learning  has received considerable attention [1  2]. More concretely  K-shot N-way
classiﬁcation is the task of classifying a data point into one of N classes  when only K examples
of each class are available to inform this decision. This is a challenging setting that necessitates
different approaches from the ones commonly employed when the labelled data of each new concept
is abundant. Indeed  many recent success stories of machine learning methods rely on large datasets
and suffer from overﬁtting in the face of insufﬁcient data. It is however not realistic nor preferred to
always expect many examples for learning a new class or concept  rendering few-shot learning an
important problem to address.
We propose a model for this problem that aims to extract as much information as possible from each
training batch  a capability that is of increased importance when the available data for learning each
class is scarce. Towards this goal  we formulate few-shot learning in information retrieval terms: each
point acts as a ‘query’ that ranks the remaining ones based on its predicted relevance to them. We are
then faced with the choice of a ranking loss function and a computational framework for optimization.
We choose to work within the framework of structured prediction and we optimize mean Average
Precision (mAP) using a standard Structural SVM (SSVM) [3]  as well as a Direct Loss Minimization
(DLM) [4] approach. We argue that the objective of mAP is especially suited for the low-data regime
of interest since it allows us to fully exploit each batch by simultaneously optimizing over all relative
orderings of the batch points. Figure 1 provides an illustration of this training objective.
Our contribution is therefore to adopt an information retrieval perspective on the problem of few-shot
learning; we posit that a model is prepared for the sparse-labels setting by being trained in a manner
31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Best viewed in color. Illustration of our training objective. Assume a batch of 6 points: G1 
G2 and G3 of class "green"  Y1 and Y2 of "yellow"  and another point. We show in columns 1-5
the predicted rankings for queries G1  G2  G3  Y1 and Y2  respectively. Our learning objective is to
move the 6 points in positions that simultaneously maximize the Average Precision (AP) of the 5
rankings. For example  the AP of G1’s ranking would be optimal if G2 and G3 had received the two
highest ranks  and so on.

that fully exploits the information in each batch. We also introduce a new form of a few-shot learning
task  ‘few-shot retrieval’  where given a ‘query’ image and a pool of candidates all coming from
previously-unseen classes  the task is to ‘retrieve’ all relevant (identically labelled) candidates for the
query. We achieve competitive with the state-of-the-art results on the standard few-shot classiﬁcation
benchmarks and show superiority over a strong baseline in the proposed few-shot retrieval problem.

2 Related Work

Our approach to few-shot learning heavily relies on learning an informative similarity metric  a goal
that has been extensively studied in the area of metric learning. This can be thought of as learning
a mapping of objects into a space where their relative positions are indicative of their similarity
relationships. We refer the reader to a survey of metric learning [5] and merely touch upon a few
representative methods here.
Neighborhood Component Analysis (NCA) [6] learns a metric aiming at high performance in nearest
neirhbour classiﬁcation. Large Margin Nearest Neighbor (LMNN) [7] refers to another approach for
nearest neighbor classiﬁcation which constructs triplets and employs a contrastive loss to move the
‘anchor’ of each triplet closer to the similarly-labelled point and farther from the dissimilar one by at
least a predeﬁned margin.
More recently  various methods have emerged that harness the power of neural networks for metric
learning. These methods vary in terms of loss functions but have in common a mechanism for the
parallel and identically-parameterized embedding of the points that will inform the loss function.
Siamese and triplet networks are commonly-used variants of this family that operate on pairs and
triplets  respectively. Example applications include signature veriﬁcation [8] and face veriﬁcation
[9  10]. NCA and LMNN have also been extended to their deep variants [11] and [12]  respectively.
These methods often employ hard-negative mining strategies for selecting informative constraints
for training [10  13]. A drawback of siamese and triplet networks is that they are local  in the sense
that their loss function concerns pairs or triplets of training examples  guiding the learning process
to optimize the desired relative positions of only two or three examples at a time. The myopia of
these local methods introduces drawbacks that are reﬂected in their embedding spaces. [14] propose
a method to address this by using higher-order information.
We also learn a similarity metric in this work  but our approach is speciﬁcally tailored for few-shot
learning. Other metric learning approaches for few-shot learning include [15  1  16  17]. [15] employs
a deep convolutional neural network that is trained to correctly predict pairwise similarities. Attentive
Recurrent Comparators [16] also perform pairwise comparisons but form the representation of the
pair through a sequence of glimpses at the two points that comprise it via a recurrent neural network.
We note that these pairwise approaches do not offer a natural mechanism to solve K-shot N-way tasks
for K > 1 and focus on one-shot learning  whereas our method tackles the more general few-shot
learning problem. Matching Networks [1] aim to ‘match’ the training setup to the evaluation trials of
K-shot N-way classiﬁcation: they divide each sampled training ‘episode’ into disjoint support and
query sets and backpropagate the classiﬁcation error of each query point conditioned on the support
set. Prototypical Networks [17] also perform episodic training  and use the simple yet effective
mechanism of representing each class by the mean of its examples in the support set  constructing a

2

‘prototype’ in this way that each query example will be compared with. Our approach can be thought
of as constructing all such query/support sets within each batch in order to fully exploit it.
Another family of methods for few-shot learning is based on meta-learning. Some representative
work in this category includes [2  18]. These approaches present models that learn how to use the
support set in order to update the parameters of a learner model in such a way that it can generalize to
the query set. Meta-Learner LSTM [2] learns an initialization for learners that can solve new tasks 
whereas Model-Agnostic Meta-Learner (MAML) [18] learns an update step that a learner can take to
be successfully adapted to a new task. Finally  [19] presents a method that uses an external memory
module that can be integrated into models for remembering rarely occurring events in a life-long
learning setting. They also demonstrate competitive results on few-shot classiﬁcation.

3 Background

3.1 Mean Average Precision (mAP)
Consider a batch B of points: X = {x1  x2  . . .   xN} and denote by cj the class label of the point
xj. Let Relx1 = {xj ∈ B : c1 == cj} be the set of points that are relevant to x1  determined in a
binary fashion according to class membership. Let Ox1 denote the ranking based on the predicted
similarity between x1 and the remaining points in B so that Ox1 [j] stores x1’s jth most similar point.
Precision at j in the ranking Ox1  denoted by P rec@jx1 is the proportion of points that are relevant
to x1 within the j highest-ranked ones. The Average Precision (AP) of this ranking is then computed
by averaging the precisions at j over all positions j in Ox1 that store relevant points.

P rec@jx1
|Relx1|

where P rec@jx1 =

|{k ≤ j : Ox1[k] ∈ Relx1}|

j

(cid:88)

AP x1 =

j∈{1 ... |B−1|:
Ox1 [j]∈Relx1}

Finally  mean Average Precision (mAP) calculates the mean AP across batch points.

(cid:88)

mAP =

1
|B|

AP xi

i∈{1 ...B}

3.2 Structural Support Vector Machine (SSVM)

Structured prediction refers to a family of tasks with inter-dependent structured output variables
such as trees  graphs  and sequences  to name just a few [3]. Our proposed learning objective
that involves producing a ranking over a set of candidates also falls into this category so we adopt
structured prediction as our computational framework. SSVM [3] is an efﬁcient method for these
tasks with the advantage of being tunable to custom task loss functions. More concretely  let X
and Y denote the spaces of inputs and structured outputs  respectively. Assume a scoring function
F (x  y; w) depending on some weights w  and a task loss L(yGT  ˆy) incurred when predicting ˆy
when the groundtruth is yGT. The margin-rescaled SSVM optimizes an upper bound of the task loss
formulated as:

min

w

ˆy∈Y {L(yGT  ˆy) − F (x  yGT; w) + F (x  ˆy; w)}]
E[max
∇wL(y) = ∇wF (X   yhinge  w) − ∇wF (X   yGT  w)

{F (X   ˆy  w) + L(yGT  ˆy)}

The loss gradient can then be computed as:

with yhinge = arg max

ˆy∈Y

3.3 Direct Loss Minimization (DLM)

(1)

[4] proposed a method that directly optimizes the task loss of interest instead of an upper bound of it.
In particular  they provide a perceptron-like weight update rule that they prove corresponds to the
gradient of the task loss. [20] present a theorem that equips us with the corresponding weight update
rule for the task loss in the case of nonlinear models  where the scoring function is parameterized by
a neural network. Since we make use of their theorem  we include it below for completeness.
Let D = {(x  y)} be a dataset composed of input x ∈ X and output y ∈ Y pairs. Let F (X   y  w) be
a scoring function which depends on the input  the output and some parameters w ∈ RA.

3

Theorem 1 (General Loss Gradient Theorem from [20]). When given a ﬁnite set Y  a scoring function
F (X   y  w)  a data distribution  as well as a task-loss L(y  ˆy)  then  under some mild regularity
conditions  the direct loss gradient has the following form:

with:

∇wL(y  yw) = ± lim
→0

1


(∇wF (X   ydirect  w) − ∇wF (X   yw  w))

(2)

yw = arg max

ˆy∈Y

F (X   ˆy  w) and ydirect = arg max
ˆy∈Y

{F (X   ˆy  w) ± L(y  ˆy)}

This theorem presents us with two options for the gradient update  henceforth the positive and negative
update  obtained by choosing the + or − of the ± respectively. [4] and [20] provide an intuitive view
for each one. In the case of the positive update  ydirect can be thought of as the ‘worst’ solution since
it corresponds to the output value that achieves high score while producing high task loss. In this
case  the positive update encourages the model to move away from the bad solution ydirect. On the
other hand  when performing the negative update  ydirect represents the ‘best’ solution: one that does
well both in terms of the scoring function and the task loss. The model is hence encouraged in this
case to adjust its weights towards the direction of the gradient of this best solution’s score.
In a nutshell  this theorem provides us with the weight update rule for the optimization of a custom
task loss  provided that we deﬁne a scoring function and procedures for performing standard and
loss-augmented inference.

3.4 Relationship between DLM and SSVM

As also noted in [4]  the positive update of direct loss minimization strongly resembles that of the
margin-rescaled structural SVM [3] which also yields a loss-informed weight update rule. This
gradient computation differs from that of the direct loss minimization approach only in that  while
SSVM considers the score of the ground-truth F (X   yGT  w)  direct loss minimization considers the
score of the current prediction F (X   yw  w). The computation of yhinge strongly resembles that of
ydirect in the positive update. Indeed SSVM’s training procedure also encourages the model to move
away from weights that produce the ‘worst’ solution yhinge.

3.5 Optimizing for Average Precision (AP)

In the following section we adapt and extend a method for optimizing AP [20].
Given a query point  the task is to rank N points x = (x1  . . .   xN ) with respect to their relevance
to the query  where a point is relevant if it belongs to the same class as the query and irrelevant
otherwise. Let P and N be the sets of ‘positive’ (i.e. relevant) and ‘negative’ (i.e. irrelevant) points
respectively. The output ranking is represented as yij pairs where ∀i  j  yij = 1 if i is ranked higher
than j and yij = −1 otherwise  and ∀i  yii = 0. Deﬁne y = (. . .   yij  . . . ) to be the collection of all
such pairwise rankings.
The scoring function that [20] used is borrowed from [21] and [22]:

F (x  y  w) =

1

|P||N|

yij(ϕ(xi  w) − ϕ(xj  w))

(cid:88)

i∈P j∈N

where ϕ(xi  w) can be interpreted as the learned similarity between xi and the query.
[20] devise a dynamic programming algorithm to perform loss-augmented inference in this setting
which we make use of but we omit for brevity.

4 Few-Shot Learning by Optimizing mAP

In this section  we present our approach for few-shot learning that optimizes mAP. We extend the
work of [20] that optimizes for AP in order to account for all possible choices of query among the
batch points. This is not a straightforward extension as it requires ensuring that optimizing the AP of
one query’s ranking does not harm the AP of another query’s ranking.
In what follows we deﬁne a mathematical framework for this problem and we show that we can treat
each query independently without sacriﬁcing correctness  therefore allowing to efﬁciently in parallel

4

learn to optimize all relative orderings within each batch. We then demonstrate how we can use the
frameworks of SSVM and DLM for optimization of mAP  producing two variants of our method
henceforth referred to as mAP-SSVM and mAP-DLM  respectively.
Setup: Let B be a batch of points: B = {x1  x2  . . .   xN} belonging to C different classes. Each
class c ∈ {1  2  . . .  C} deﬁnes the positive set P c containing the points that belong to c and the
negative set N c containing the rest of the points. We denote by ci the class label of the ith point.
We represent the output rankings as a collection of yi
higher than j in i’s ranking  yi
convenience we combine these comparisons for each query i in yi = (. . .   yi
Let f (x  w) be the embedding function  parameterized by a neural network and ϕ(x1  x2  w) the
cosine similarity of points x1 and x2 in the embedding space given by w:

kj = 1 if k is ranked
kj = −1 if j is ranked higher than k in i’s ranking. For

kj variables where yi

kk = 0 and yi

kj  . . . ).

ϕ(x1  x2  w) =

f (x1  w) · f (x2  w)
|f (x1  w)||f (x2  w)|

ϕ(xi  xj  w) is typically referred in the literature as the score of a siamese network.
We consider for each query i  the function F i(X   yi  w):
kj(ϕ(xi  xk  w) − ϕ(xi  xj  w))
(cid:88)
yi

We then compose the scoring function by summing over all queries: F (X   y  w) =

F i(X   yi  w) =

|P ci||N ci|

(cid:88)

(cid:88)

k∈P ci\i

j∈N ci

1

F i(X   yi  w)

i∈B

Further  for each query i ∈ B  we let pi = rank(yi) ∈ {0  1}|P ci|+|N ci| be a vector obtained by
sorting the yi
g = 1 if g is relevant for query i
and pi

g = −1 otherwise. Then the AP loss for the ranking induced by some query i is deﬁned as:

kj’s ∀k ∈ P ci \ i  j ∈ N ci  such that for a point g (cid:54)= i  pi

AP (pi  ˆpi) = 1 − 1
Li
|P ci|

P rec@j

(cid:88)

j: ˆpi

j =1

where P rec@j is the percentage of relevant points among the top-ranked j and pi and ˆpi denote the
ground-truth and predicted binary relevance vectors for query i  respectively. We deﬁne the mAP loss
to be the average AP loss over all query points.
Inference: We proof-sketch in the supplementary material that inference can be performed efﬁciently
in parallel as we can decompose the problem of optimizing the orderings induced by the different
queries to optimizing each ordering separately. Speciﬁcally  for a query i of class c the computation
of the yi
k(cid:48)j(cid:48)’s for
some other query i(cid:48) (cid:54)= i. We are thus able to optimize the ordering induced by each query point
independently of those induced by the other queries. For query i  positive point k and negative point
= arg maxyi F i(X   yi  w) and can be computed as
j  the solution of standard inference is yi
follows

kj’s  ∀k ∈ P c \ i  j ∈ N c can happen independently of the computation of the yi(cid:48)

wkj

yi
wkj

=

−1  otherwise

(cid:26)1  if ϕ(xi  xk  w) − ϕ(xi  xj  w) > 0
(cid:8)F i(X   ˆyi  w) ± Li(yi  ˆyi)(cid:9)
(cid:88)

ˆyi

(3)

(4)

Loss-augmented inference for query i is deﬁned as

yi
direct = arg max

(cid:88)

F (X   yw  w) =

and can be performed via a run of the dynamic programming algorithm of [20]. We can then combine
the results of all the independent inferences to compute the overall scoring function
F i(X   yi

(5)
Finally  we deﬁne the ground-truth output value yGT . For any query i and distinct points m  n (cid:54)= i
we set yi
= 0
otherwise.

= −1 if n ∈ P ci and m ∈ N ci and yi

= 1 if m ∈ P ci and n ∈ N ci  yi

and F (X   ydirect  w) =

F i(X   yi

direct  w)

w  w)

i∈B

i∈B

GTmn

GTmn

GTmn

5

Algorithm 1 Few-Shot Learning by Optimizing mAP

Input: A batch of points X = {x1  . . .   xN} of C different classes and ∀c ∈ {1  . . .  C} the sets P c and N c.

Initialize w
if using mAP-SSVM then

Set yi

GT = ONES(|P ci| |N ci|)  ∀i = 1  . . .   N

end if
repeat

if using mAP-DLM then

Standard inference: Compute yi

w  ∀i = 1  . . .   N as in Equation 3

direct  ∀i = 1  . . .   N via the DP algorithm of [20] as in Equation 4.

end if
Loss-augmented inference: Compute yi
In the case of mAP-SSVM  always use the positive update option and set  = 1
Compute F (X   ydirect  w) as in Equation 5
if using mAP-DLM then
Compute F (X   yw  w) as in Equation 5
Compute the gradient ∇wL(y  yw) as in Equation 2
Compute F (X   yGT   w) as in Equation 6
Compute the gradient ∇wL(y  yw) as in Equation 1 (using ydirect in the place of yhinge)

else if using mAP-SSVM then

end if
Perform the weight update rule with stepsize η: w ← w − η∇wL(y  yw)

until stopping criteria

We note that by construction of our scoring function deﬁned above  we will only have to compute
kj’s where k and i belong to the same class ci and j is a point from another class. Because of this  we
yi
GT = ones(|P ci| |N ci|).
set the yi
The overall score of the ground truth is then

GT for each query i to be an appropriately-sized matrix of ones: yi

F (X   yGT   w) =

F i(X   yi

GT   w)

(6)

(cid:88)

i∈B

Optimizing mAP via SSVM and DLM We have now deﬁned all the necessary components to
compute the gradient update as speciﬁed by the General Loss Gradient Theorem of [20] in equation 2
or as deﬁned by the Structural SVM in equation 1. For clarity  Algorithm 1 describes this process 
outlining the two variants of our approach for few-shot learning  namely mAP-DLM and mAP-SSVM.

5 Evaluation

In what follows  we describe our training setup  the few-shot learning tasks of interest  the datasets we
use  and our experimental results. Through our experiments  we aim to evaluate the few-shot retrieval
ability of our method and additionally to compare our model to competing approaches for few-shot
classiﬁcation. For this  we have updated our tables to include very recent work that is published
concurrently with ours in order to provide the reader with a complete view of the state-of-the-art on
few-shot learning. Finally  we also aim to investigate experimentally our model’s aptness for learning
from little data via its training objective that is designed to fully exploit each training batch.
Controlling the inﬂuence of loss-augmented inference on the loss gradient We found empirically
that for the positive update of mAP-DLM and for mAP-SSVM  it is beneﬁcial to introduce a
hyperparamter α that controls the contribution of the loss-augmented F (X   ydirect  w) relative to that
of F (X   yw  w) in the case of mAP-DLM  or F (X   yGT   w) in the case of mAP-SSVM. The updated
rules that we use in practice for training mAP-DLM and mAP-SSVM  respectively  are shown below 
where α is a hyperparamter.

∇wL(y  yw) = ± lim
→0

1


(α∇wF (X   ydirect  w) − ∇wF (X   yw  w))

and

∇wL(y) = α∇wF (X   ydirect  w) − ∇wF (X   yyGT   w)

We refer the reader to the supplementary material for more details concerning this hyperparameter.

6

Siamese
Matching Networks [1]
Prototypical Networks [17]
MAML [18]
ConvNet w/ Memory [19]
mAP-SSVM (ours)
mAP-DLM (ours)

Classiﬁcation

1-shot

5-shot

5-way
98.8
98.1
98.8
98.7
98.4
98.6
98.8

20-way

5-way

20-way

95.5
93.8
96.0
95.8
95.0
95.2
95.4

-

98.9
99.7
99.9
99.6
99.6
99.6

-

98.5
98.9
98.9
98.6
98.6
98.6

Retrieval
1-shot

5-way
98.6

20-way

95.7

98.6
98.7

95.7
95.8

-
-
-
-

-
-
-
-

Table 1: Few-shot learning results on Omniglot (averaged over 1000 test episodes). We report accuracy for the
classiﬁcation and mAP for the retrieval tasks.

Few-shot Classiﬁcation and Retrieval Tasks Each K-shot N-way classiﬁcation ‘episode’ is con-
structed as follows: N evaluation classes and 20 images from each one are selected uniformly at
random from the test set. For each class  K out of the 20 images are randomly chosen to act as the
‘representatives’ of that class. The remaining 20 − K images of each class are then to be classiﬁed
among the N classes. This poses a total of (20 − K)N classiﬁcation problems. Following the
standard procedure  we repeat this process 1000 times when testing on Omniglot and 600 times for
mini-ImageNet in order to compute the results reported in tables 1 and 2.
We also designed a similar one-shot N-way retrieval task  where to form each episode we select N
classes at random and 10 images per class  yielding a pool of 10N images. Each of these 10N images
acts as a query and ranks all remaining (10N - 1) images. The goal is to retrieve all 9 relevant images
before any of the (10N - 10) irrelevant ones. We measure the performance on this task using mAP.
Note that since this is a new task  there are no publicly available results for the competing few-shot
learning methods.
Our Algorithm for K-shot N-way classiﬁcation Our model classiﬁes image x into class c =
arg maxi AP i(x)  where AP i(x) denotes the average precision of the ordering that image x assigns
to the pool of all KN representatives assuming that the ground truth class for image x is i. This
means that when computing AP i(x)  the K representatives of class i will have a binary relevance of
1 while the K(N − 1) representatives of the other classes will have a binary relevance of 0. Note that
in the one-shot learning case where K = 1 this amounts to classifying x into the class whose (single)
representative is most similar to x according to the model’s learned similarity metric.
We note that the siamese model does not naturally offer a procedure for exploiting all K representatives
of each class when making the classiﬁcation decision for some reference. Therefore we omit few-shot
learning results for siamese when K > 1 and examine this model only in the one-shot case.
Training details We use the same embedding architecture for all of our models for both Omniglot and
mini-ImageNet. This architecture mimics that of [1] and consists of 4 identical blocks stacked upon
each other. Each of these blocks consists of a 3x3 convolution with 64 ﬁlters  batch normalization
[23]  a ReLU activation  and 2x2 max-pooling. We resize the Omniglot images to 28x28  and the
mini-ImageNet images to 3x84x84  therefore producing a 64-dimensional feature vector for each
Omniglot image and a 1600-dimensional one for each mini-ImageNet image. We use ADAM [24]
for training all models. We refer the reader to the supplementary for more details.
Omniglot The Omniglot dataset [25] is designed for testing few-shot learning methods. This dataset
consists of 1623 characters from 50 different alphabets  with each character drawn by 20 different
drawers. Following [1]  we use 1200 characters as training classes and the remaining 423 for
evaluation while we also augment the dataset with random rotations by multiples of 90 degrees. The
results for this dataset are shown in Table 1. Both mAP-SSVM and mAP-DLM are trained with
α = 10  and for mAP-DLM the positive update was used. We used |B| = 128 and N = 16 for our
models and the siamese. Overall  we observe that many methods perform very similarly on few-shot
classiﬁcation on this dataset  ours being among the top-performing ones. Further  we perform equally
well or better than the siamese network in few-shot retrieval. We’d like to emphasize that the siamese
network is a tough baseline to beat  as can be seen from its performance in the classiﬁcation tasks
where it outperforms recent few-shot learning methods.
mini-ImageNet mini-ImageNet refers to a subset of the ILSVRC-12 dataset [26] that was used as
a benchmark for testing few-shot learning approaches in [1]. This dataset contains 60 000 84x84
color images and constitutes a signiﬁcantly more challenging benchmark than Omniglot. In order to

7

Classiﬁcation

5-way

1-shot

5-shot

Retrieval

5-way
1-shot

20-way
1-shot

Baseline Nearset Neighbors*
Matching Networks* [1]
Matching Networks FCE* [1]
Meta-Learner LSTM* [2]
Prototypical Networks [17]
MAML [18]
Siamese
mAP-SSVM (ours)
mAP-DLM (ours)

41.08 ± 0.70 % 51.04 ± 0.65 %
43.40 ± 0.78 % 51.09 ± 0.71 %
43.56 ± 0.84 % 55.31 ± 0.73 %
43.44 ± 0.77 % 60.60 ± 0.71 %
49.42 ± 0.78% 68.20 ± 0.66 %
48.70 ± 1.84 % 63.11 ± 0.92 %
48.42 ± 0.79 %
51.24 ± 0.57 % 22.66 ± 0.13 %
50.32 ± 0.80 % 63.94 ± 0.72 % 52.85 ± 0.56 % 23.87 ± 0.14 %
50.28 ± 0.80 % 63.70 ± 0.70 % 52.96 ± 0.55 % 23.68 ± 0.13 %

-
-
-
-
-
-

-
-
-
-
-
-

-

Table 2: Few-shot learning results on miniImageNet (averaged over 600 test episodes and reported with 95%
conﬁdence intervals). We report accuracy for the classiﬁcation and mAP for the retrieval tasks. *Results reported
by [2].

compare our method with the state-of-the-art on this benchmark  we adapt the splits introduced in [2]
which contain a total of 100 classes out of which 64 are used for training  16 for validation and 20 for
testing. We train our models on the training set and use the validation set for monitoring performance.
Table 2 reports the performance of our method and recent competing approaches on this benchmark.
As for Omniglot  the results of both versions of our method are obtained with α = 10  and with the
positive update in the case of mAP-DLM. We used |B| = 128 and N = 8 for our models and the
siamese. We also borrow the baseline reported in [2] for this task which corresponds to performing
nearest-neighbors on top of the learned embeddings. Our method yields impressive results here 
outperforming recent approaches tailored for few-shot learning either via deep-metric learning such
as Matching Networks [1] or via meta-learning such as Meta-Learner LSTM [2] and MAML [18] in
few-shot classiﬁcation. We set the new state-of-the-art for 1-shot 5-way classiﬁcation. Further  our
models are superior than the strong baseline of the siamese network in the few-shot retrieval tasks.
CUB We also experimented on the Caltech-UCSD Birds (CUB) 200-2011 dataset [27]  where we
outperform the siamese network as well. More details can be found in the supplementary.
Learning Efﬁciency We examine our method’s learning efﬁciency via comparison with a siamese
network. For fair comparison of these models  we create the training batches in a way that enforces
that they have the same amount of information available for each update: each training batch B
is formed by sampling N classes uniformly at random and |B| examples from these classes. The
siamese network is then trained on all possible pairs from these sampled points. Figure 2 displays the
performance of our model and the siamese on different metrics on Omniglot and mini-ImageNet. The
ﬁrst two rows show the performance of our two variants and the siamese in the few-shot classiﬁcation
(left) and few-shot retrieval (right) tasks  for various levels of difﬁculty as regulated by the different
values of N. The ﬁrst row corresponds to Omniglot and the second to mini-ImageNet. We observe
that even when both methods converge to comparable accuracy or mAP values  our method learns
faster  especially when the ‘way’ of the evaluation task is larger  making the problem harder.
In the third row in Figure 2  we examine the few-shot learning performance of our model and the
all-pairs siamese that were trained with N = 8 but with different |B|. We note that for a given N 
larger batch size implies larger ‘shot’. For example  for N = 8  |B| = 64 results to on average 8
examples of each class in each batch (8-shot) whereas |B| = 16 results to on average 2-shot. We
observe that especially when the ‘shot’ is smaller  there is a clear advantage in using our method
over the all-pairs siamese. Therefore it indeed appears to be the case that the fewer examples we are
given per class  the more we can beneﬁt from our structured objective that simultaneously optimizes
all relative orderings. Further  mAP-DLM can reach higher performance overall with smaller batch
sizes (thus smaller ‘shot’) than the siamese  indicating that our method’s training objective is indeed
efﬁciently exploiting the batch examples and showing promise in learning from less data.
Discussion It is interesting to compare experimentally methods that have pursued different paths
in addressing the challenge of few-shot learning. In particular  the methods we compare against
each other in our tables include deep metric learning approaches such as ours  the siamese network 
Prototypical Networks and Matching Networks  as well as meta-learning methods such as Meta-
Learner LSTM [2] and MAML [18]. Further  [19] has a metric-learning ﬂavor but employs external
memory as a vehicle for remembering representations of rarely-observed classes. The experimental

8

Figure 2: Few-shot learning performance (on unseen validation classes). Each point represents the average
performance across 100 sampled episodes. Top row: Omniglot. Second and third rows: mini-ImageNet.

results suggest that there is no clear winner category and all these directions are worth exploring
further.
Overall  our model performs on par with the state-of-the-art results on the classiﬁcation benchmarks 
while also offering the capability of few-shot retrieval where it exhibits superiority over a strong
baseline. Regarding the comparison between mAP-DLM and mAP-SSVM  we remark that they
mostly perform similarly to each other on the benchmarks considered. We have not observed in this
case a signiﬁcant win for directly optimizing the loss of interest  offered by mAP-DLM  as opposed
to minimizing an upper bound of it.

6 Conclusion

We have presented an approach for few-shot learning that strives to fully exploit the available
information of the training batches  a skill that is utterly important in the low-data regime of few-shot
learning. We have proposed to achieve this via deﬁning an information-retrieval based training
objective that simultaneously optimizes all relative orderings of the points in each training batch.
We experimentally support our claims for learning efﬁciency and present promising results on two
standard few-shot learning datasets. An interesting future direction is to not only reason about how to
best exploit the information within each batch  but additionally about how to create training batches
in order to best leverage the information in the training set. Furthermore  we leave it as future work to
explore alternative information retrieval metrics  instead of mAP  as training objectives for few-shot
learning (e.g. ROC curve  discounted cumulative gain etc).

9

References
[1] Oriol Vinyals  Charles Blundell  Tim Lillicrap  Daan Wierstra  et al. Matching networks for one shot

learning. In Advances in Neural Information Processing Systems  pages 3630–3638  2016.

[2] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International

Conference on Learning Representations  volume 1  page 6  2017.

[3] Ioannis Tsochantaridis  Thorsten Joachims  Thomas Hofmann  and Yasemin Altun. Large margin methods
for structured and interdependent output variables. Journal of machine learning research  6(Sep):1453–
1484  2005.

[4] Tamir Hazan  Joseph Keshet  and David A McAllester. Direct loss minimization for structured prediction.

In Advances in Neural Information Processing Systems  pages 1594–1602  2010.

[5] Aurélien Bellet  Amaury Habrard  and Marc Sebban. A survey on metric learning for feature vectors and

structured data. arXiv preprint arXiv:1306.6709  2013.

[6] Jacob Goldberger  Sam Roweis  Geoff Hinton  and Ruslan Salakhutdinov. Neighbourhood components

analysis. In Advances in Neural Information Processing Systems  page 513–520  2005.

[7] Kilian Q Weinberger  John Blitzer  and Lawrence K Saul. Distance metric learning for large margin nearest
neighbor classiﬁcation. In Advances in neural information processing systems  pages 1473–1480  2005.

[8] Jane Bromley  James W Bentz  Léon Bottou  Isabelle Guyon  Yann LeCun  Cliff Moore  Eduard Säckinger 
and Roopak Shah. Signature veriﬁcation using a “siamese” time delay neural network. International
Journal of Pattern Recognition and Artiﬁcial Intelligence  7(04):669–688  1993.

[9] Sumit Chopra  Raia Hadsell  and Yann LeCun. Learning a similarity metric discriminatively  with
application to face veriﬁcation. In 2005 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR’05)  volume 1  pages 539–546. IEEE  2005.

[10] Florian Schroff  Dmitry Kalenichenko  and James Philbin. Facenet: A uniﬁed embedding for face
recognition and clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 815–823  2015.

[11] Ruslan Salakhutdinov and Geoffrey E Hinton. Learning a nonlinear embedding by preserving class

neighbourhood structure. In AISTATS  volume 11  2007.

[12] Renqiang Min  David A Stanley  Zineng Yuan  Anthony Bonner  and Zhaolei Zhang. A deep non-linear
In Data Mining  2009. ICDM’09. Ninth IEEE

feature mapping for large-margin knn classiﬁcation.
International Conference on  pages 357–366. IEEE  2009.

[13] Hyun Oh Song  Yu Xiang  Stefanie Jegelka  and Silvio Savarese. Deep metric learning via lifted structured
feature embedding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 4004–4012  2016.

[14] Hyun Oh Song  Stefanie Jegelka  Vivek Rathod  and Kevin Murphy. Learnable structured clustering

framework for deep metric learning. arXiv preprint arXiv:1612.01213  2016.

[15] Gregory Koch. Siamese neural networks for one-shot image recognition. PhD thesis  University of Toronto 

2015.

[16] Pranav Shyam  Shubham Gupta  and Ambedkar Dukkipati. Attentive recurrent comparators. arXiv preprint

arXiv:1703.00767  2017.

[17] Jake Snell  Kevin Swersky  and Richard S Zemel. Prototypical networks for few-shot learning. arXiv

preprint arXiv:1703.05175  2017.

[18] Chelsea Finn  Pieter Abbeel  and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep

networks. arXiv preprint arXiv:1703.03400  2017.

[19] Łukasz Kaiser  Oﬁr Nachum  Aurko Roy  and Samy Bengio. Learning to remember rare events. arXiv

preprint arXiv:1703.03129  2017.

[20] Yang Song  Alexander G Schwing  Richard S Zemel  and Raquel Urtasun. Training deep neural networks
via direct loss minimization. In Proceedings of The 33rd International Conference on Machine Learning 
pages 2169–2177  2016.

10

[21] Yisong Yue  Thomas Finley  Filip Radlinski  and Thorsten Joachims. A support vector method for
optimizing average precision. In Proceedings of the 30th annual international ACM SIGIR conference on
Research and development in information retrieval  pages 271–278. ACM  2007.

[22] Pritish Mohapatra  CV Jawahar  and M Pawan Kumar. Efﬁcient optimization for average precision svm. In

Advances in Neural Information Processing Systems  pages 2312–2320  2014.

[23] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. arXiv preprint arXiv:1502.03167  2015.

[24] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[25] Brenden M Lake  Ruslan Salakhutdinov  Jason Gross  and Joshua B Tenenbaum. One shot learning of

simple visual concepts. In CogSci  volume 172  page 2  2011.

[26] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng Huang 
Andrej Karpathy  Aditya Khosla  Michael Bernstein  et al. Imagenet large scale visual recognition challenge.
International Journal of Computer Vision  115(3):211–252  2015.

[27] Catherine Wah  Steve Branson  Peter Welinder  Pietro Perona  and Serge Belongie. The caltech-ucsd

birds-200-2011 dataset. 2011.

11

,Eleni Triantafillou
Richard Zemel
Raquel Urtasun
Dylan Foster
Akshay Krishnamurthy