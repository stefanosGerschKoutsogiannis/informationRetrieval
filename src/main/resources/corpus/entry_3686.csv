2010,Near-Optimal Bayesian Active Learning with Noisy Observations,We tackle the fundamental problem of Bayesian active learning with noise  where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution. In the case of noise-free observations  a greedy algorithm called generalized binary search (GBS) is known to perform near-optimally. We show that if the observations are noisy  perhaps surprisingly  GBS can perform very poorly. We develop EC2  a novel  greedy active learning algorithm and prove that it is competitive with the optimal policy  thus obtaining the first competitiveness guarantees for Bayesian active learning with noisy observations. Our bounds rely on a recently discovered diminishing returns property called adaptive submodularity  generalizing the classical notion of submodular set functions to adaptive policies. Our results hold even if the tests have non–uniform cost and their noise is correlated. We also propose EffECXtive  a particularly fast approximation of EC2  and evaluate it on a Bayesian experimental design problem involving human subjects  intended to tease apart competing economic theories of how people make decisions under uncertainty.,Near–Optimal Bayesian Active Learning

with Noisy Observations

Daniel Golovin

Caltech

Andreas Krause

Caltech

Debajyoti Ray

Caltech

Abstract

We tackle the fundamental problem of Bayesian active learning with noise  where
we need to adaptively select from a number of expensive tests in order to identify
an unknown hypothesis sampled from a known prior distribution. In the case of
noise–free observations  a greedy algorithm called generalized binary search (GBS)
is known to perform near–optimally. We show that if the observations are noisy 
perhaps surprisingly  GBS can perform very poorly. We develop EC2  a novel 
greedy active learning algorithm and prove that it is competitive with the optimal
policy  thus obtaining the ﬁrst competitiveness guarantees for Bayesian active learn-
ing with noisy observations. Our bounds rely on a recently discovered diminishing
returns property called adaptive submodularity  generalizing the classical notion
of submodular set functions to adaptive policies. Our results hold even if the tests
have non–uniform cost and their noise is correlated. We also propose EFFECX-
TIVE  a particularly fast approximation of EC2  and evaluate it on a Bayesian
experimental design problem involving human subjects  intended to tease apart
competing economic theories of how people make decisions under uncertainty.

Introduction

1
How should we perform experiments to determine the most accurate scientiﬁc theory among com-
peting candidates  or choose among expensive medical procedures to accurately determine a patient’s
condition  or select which labels to obtain in order to determine the hypothesis that minimizes general-
ization error? In all these applications  we have to sequentially select among a set of noisy  expensive
observations (outcomes of experiments  medical tests  expert labels) in order to determine which hy-
pothesis (theory  diagnosis  classiﬁer) is most accurate. This fundamental problem has been studied in
a number of areas  including statistics [17]  decision theory [13]  machine learning [19  7] and others.
One way to formalize such active learning problems is Bayesian experimental design [6]  where one
assumes a prior on the hypotheses  as well as probabilistic assumptions on the outcomes of tests. The
goal then is to determine the correct hypothesis while minimizing the cost of the experimentation. Un-
fortunately  ﬁnding this optimal policy is not just NP-hard  but also hard to approximate [5]. Several
heuristic approaches have been proposed that perform well in some applications  but do not carry theo-
retical guarantees (e.g.  [18]). In the case where observations are noise-free1  a simple algorithm  gen-
eralized binary search2(GBS) run on a modiﬁed prior  is guaranteed to be competitive with the optimal
policy; the expected number of queries is a factor of O(log n) (where n is the number of hypotheses)
more than that of the optimal policy [15]  which matches lower bounds up to constant factors [5].
The important case of noisy observations  however  as present in most applications  is much less
well understood. While there are some recent positive results in understanding the label complexity
of noisy active learning [19  1]  to our knowledge  so far there are no algorithms that are provably
competitive with the optimal sequential policy  except in very restricted settings [16]. In this paper  we

1This case is known as the Optimal Decision Tree (ODT) problem.
2GBS greedily selects tests to maximize  in expectation over the test outcomes  the prior probability mass of
eliminated hypotheses (i.e.  those with zero posterior probability  computed w.r.t. the observed test outcomes).

1

introduce a general formulation of Bayesian active learning with noisy observations that we call the
Equivalence Class Determination problem. We show that  perhaps surprisingly  generalized binary
search performs poorly in this setting  as do greedily (myopically) maximizing the information gain
(measured w.r.t. the distribution on equivalence classes) or the decision-theoretic value of information.
This motivates us to introduce a novel active learning criterion  and use it to develop a greedy active
learning algorithm called the Equivalence Class Edge Cutting algorithm (EC2)  whose expected cost
is competitive to that of the optimal policy. Our key insight is that our new objective function satisﬁes
adaptive submodularity [9]  a natural diminishing returns property that generalizes the classical notion
of submodularity to adaptive policies. Our results also allow us to relax the common assumption
that the outcomes of the tests are conditionally independent given the true hypothesis. We also
develop the Efficient Edge Cutting approXimate objective algorithm (EFFECXTIVE)  an efﬁcient
approximation to EC2  and evaluate it on a Bayesian experimental design problem intended to tease
apart competing theories on how people make decisions under uncertainty  including Expected Value
[22]  Prospect Theory [14]  Mean-Variance-Skewness [12] and Constant Relative Risk Aversion [20].
In our experiments  EFFECXTIVE typically outperforms existing experimental design criteria such as
information gain  uncertainty sampling  GBS  and decision-theoretic value of information. Our results
from human subject experiments further reveal that EFFECXTIVE can be used as a real-time tool
to classify people according to the economic theory that best describes their behaviour in ﬁnancial
decision-making  and reveal some interesting heterogeneity in the population.

2 Bayesian Active Learning in the Noiseless Case
In the Bayesian active learning problem  we would like to distinguish among a given set of hypotheses
H = {h1  . . .   hn} by performing tests from a set T = {1  . . .   N} of possible tests. Running test
t incurs a cost of c(t) and produces an outcome from a ﬁnite set of outcomes X = {1  2  . . .   (cid:96)}.
We let H denote the random variable which equals the true hypothesis  and model the outcome of
each test t by a random variable Xt taking values in X . We denote the observed outcome of test
t by xt. We further suppose we have a prior distribution P modeling our assumptions on the joint
probability P (H  X1  . . .   XN ) over the hypotheses and test outcomes. In the noiseless case  we
assume that the outcome of each test is deterministic given the true hypothesis  i.e.  for each h ∈ H 
P (X1  . . .   XN | H = h) is a deterministic distribution. Thus  each hypothesis h is associated
with a particular vector of test outcomes. We assume  w.l.o.g.  that no two hypotheses lead to the
same outcomes for all tests. Thus  if we perform all tests  we can uniquely determine the true
hypothesis. However in most applications we will wish to avoid performing every possible test  as
this is prohibitively expensive. Our goal is to ﬁnd an adaptive policy for running tests that allows us
to determine the value of H while minimizing the cost of the tests performed. Formally  a policy π
(also called a conditional plan) is a partial mapping π from partial observation vectors xA to tests 
specifying which test to run next (or whether we should stop testing) for any observation vector xA.
Hereby  xA ∈ X A is a vector of outcomes indexed by a set of tests A ⊆ T that we have performed
so far 3 (e.g.  the set of labeled examples in active learning  or outcomes of a set of medical tests that
we ran). After having made observations xA  we can rule out inconsistent hypotheses. We denote
the set of hypotheses consistent with event Λ (often called the version space associated with Λ) by
V(Λ) := {h ∈ H : P (h | Λ) > 0}. We call a policy feasible if it is guaranteed to uniquely determine
the correct hypothesis. That is  upon termination with observation xA  it must hold that |V(xA)| = 1.
We can deﬁne the expected cost of a policy π by

(cid:88)

h

c(π) :=

P (h)c(T (π  h))

where T (π  h) ⊆ T is the set of tests run by policy π in case H = h. Our goal is to ﬁnd a feasible
policy π∗ of minimum expected cost  i.e. 

π∗ = arg min{c(π) : π is feasible}

(2.1)
A policy π can be naturally represented as a decision tree T π  and thus problem (2.1) is often called
the Optimal Decision Tree (ODT) problem.
Unfortunately  obtaining an approximate policy π for which c(π) ≤ c(π∗) · o(log(n)) is NP-hard [5].
Hence  various heuristics are employed to solve the Optimal Decision Tree problem and its variants.
Two of the most popular heuristics are to select tests greedily to maximize the information gain (IG)

3Formally we also require that (xt)t∈B ∈ dom(π) and A ⊆ B  implies (xt)t∈A ∈ dom(π) (c.f.  [9]).

2

conditioned on previous test outcomes  and generalized binary search (GBS). Both heuristics are
greedy  and after having made observations xA will select

t∗ = arg max

t∈T

∆Alg (t| xA) /c(t) 

and ∆GBS (t| xA) := P (V(xA)) −(cid:80)

where Alg ∈ {IG  GBS}. Here  ∆IG (t| xA) := H (XT | xA) − Ext∼Xt|xA [H (XT |xA  xt)] is the
marginal information gain measured with respect to the Shannon entropy H (X) := Ex[− log2 P (x)] 
x∈X P (Xt = x | xA)P (V(xA  Xt = x)) is the expected
reduction in version space probability mass. Thus  both heuristics greedily chooses the test that
maximizes the beneﬁt-cost ratio  measured with respect to their particular beneﬁt functions. They
stop after running a set of tests A such that |V(xA)| = 1  i.e.  once the true hypothesis has been
uniquely determined.
It turns out that for the (noiseless) Optimal Decision Tree problem  these two heuristics are equivalent
[23]  as can be proved using the chain rule of entropy.
Interestingly  despite its myopic nature
GBS has been shown [15  7  11  9] to obtain near-optimal expected cost: the strongest known bound
is c(πGBS) ≤ c(π∗) (ln(1/pmin) + 1) where pmin := minh∈H P (h). Let xS(h) be the unique
vector xS ∈ X S such that P (xS | h) = 1. The result above is proved by exploiting the fact
that fGBS(S  h) := 1 − P (V(xS(h))) + P (h) is adaptive submodular and strongly adaptively
monotone [9]. Call xA a subvector of xB if A ⊆ B and P (xB | xA) > 0. In this case we write
xA ≺ xB. A function f : 2T × H is called adaptive submodular w.r.t. a distribution P   if for any
xA ≺ xB and any test t it holds that ∆ (t| xA) ≥ ∆ (t| xB)  where

∆ (t| xA) := EH [f (A ∪ {t}   H) − f (A  H) | xA] .

Thus  f is adaptive submodular if the expected marginal beneﬁts ∆ (t| xA) of adding a new test t can
only decrease as we gather more observations. f is called strongly adaptively monotone w.r.t. P if 
informally  “observations never hurt” with respect to the expected reward. Formally  for all A  all
t /∈ A  and all x ∈ X we require EH [f (A  H) | xA] ≤ EH [f (A ∪ {t}   H) | xA  Xt = x] .
The performance guarantee for GBS follows from the following general result about the greedy
algorithm for adaptive submodular functions (applied with Q = 1 and η = pmin):
Theorem 1 (Theorem 10 of [9] with α = 1). Suppose f : 2T × H → R≥0 is adaptive submodular
and strongly adaptively monotone with respect to P and there exists Q such that f (T   h) = Q for all
h. Let η be any value such that f (S  h) > Q− η implies f (S  h) = Q for all sets S and hypotheses h.
Then for self–certifying instances the adaptive greedy policy π satisﬁes c(π) ≤ c(π∗)
.

(cid:16) Q

(cid:16)

(cid:17)

(cid:17)

ln

η

+ 1

The technical requirement that instances be self–certifying means that the policy will have proof
that it has obtained the maximum possible objective value  Q  immediately upon doing so. It is
not difﬁcult to show that this is the case with the instances we consider in this paper. We refer the
interested reader to [9] for more detail.
In the following sections  we will use the concept of adaptive submodularity to provide the ﬁrst
approximation guarantees for Bayesian active learning with noisy observations.

3 The Equivalence Class Determination Problem and the EC2 Algorithm
We now wish to consider the Bayesian active learning problem where tests can have noisy outcomes.
Our general strategy is to reduce the problem of noisy observations to the noiseless setting. To gain
intuition  consider a simple model where tests have binary outcomes  and we know that the outcome
of exactly one test  chosen uniformly at random unbeknown to us  is ﬂipped. If any pair of hypotheses
h (cid:54)= h(cid:48) differs by the outcome of at least three tests  we can still uniquely determine the correct
hypothesis after running all tests. In this case we can reduce the noisy active learning problem to
the noiseless setting by  for each hypothesis  creating N “noisy” copies  each obtained by ﬂipping
the outcome of one of the N tests. The modiﬁed prior P (cid:48) would then assign mass P (cid:48)(h(cid:48)) = P (h)/N
to each noisy copy h(cid:48) of h. The conditional distribution P (cid:48)(XT | h(cid:48)) is still deterministic (obtained
by ﬂipping the outcome of one of the tests). Thus  each hypothesis hi in the original problem is
now associated with a set Hi of hypotheses in the modiﬁed problem instance. However  instead of
selecting tests to determine which noisy copy has been realized  we only care which set Hi is realized.

3

of m equivalence classes {H1  . . .  Hm} so that H =(cid:85)m

The Equivalence Class Determination problem (ECD). More generally  we introduce the
Equivalence Class Determination problem4  where our set of hypotheses H is partitioned into a set
i=1 Hi  and the goal is to determine which
class Hi the true hypothesis lies in. Formally  upon termination with observations xA we require
that V(xA) ⊆ Hi for some i. As with the ODT problem  the goal is to minimize the expected cost
of the tests  where the expectation is taken over the true hypothesis sampled from P . In §4  we will
show how the Equivalence Class Determination problem arises naturally from Bayesian experimental
design problems in probabilistic models.
Given the fact that GBS performs near-optimally on the Optimal Decision Tree problem  a natural ap-
proach to solving ECD would be to run GBS until the termination condition is met. Unfortunately  and
perhaps surprisingly  GBS can perform very poorly on the ECD problem. Consider an instance with
a uniform prior over n hypotheses  h1  . . .   hn  and two equivalence classes H1 := {hi : 1 ≤ i < n}
and H2 := {hn}. There are tests T = {1  . . .   n} such that hi(t) = 1[i = t]  all of unit cost. Hereby 
1[Λ] is the indicator variable for event Λ. In this case  the optimal policy only needs to select test
n  however GBS may select tests 1  2  . . .   n in order until running test t  where H = ht is the true
hypothesis. Given our uniform prior  it takes n/2 tests in expectation until this happens  so that GBS
pays  in expectation  n/2 times the optimal expected cost in this instance.
The poor performance of GBS in this instance may be attributed to its lack of consideration for the
equivalence classes. Another natural heuristic would be to run the greedy information gain policy 
only with the entropy measured with respect to the probability distribution on equivalence classes
rather than hypotheses. Call this policy πIG. It is clearly aware of the equivalence classes  as it
adaptively and myopically selects tests to reduce the uncertainty of the realized class  measured w.r.t.
the Shannon entropy. However  we can show there are instances in which it pays Ω(n/ log(n)) times
the optimal cost  even under a uniform prior. See the long version of this paper [10] for details.

The EC2 algorithm. The reason why GBS fails is because reducing the version space mass does
not necessarily facilitate differentiation among the classes Hi. The reason why πIG fails is that there
are complementarities among tests; a set of tests can be far better than the sum of its parts. Thus  we
would like to optimize an objective function that encourages differentiation among classes  but lacks
complementarities. We adopt a very elegant idea from Dasgupta [8]  and deﬁne weighted edges be-
tween hypotheses that we aim to distinguish between. However  instead of introducing edges between
arbitrary pairs of hypotheses (as done in [8])  we only introduce edges between hypotheses in different
classes. Tests will allow us to cut edges inconsistent with their outcomes  and we aim to eliminate
all inconsistent edges while minimizing the expected cost incurred. We now formalize this intuition.
Speciﬁcally  we deﬁne a set of edges E = ∪1≤i<j≤m {{h  h(cid:48)} : h ∈ Hi  h(cid:48) ∈ Hj}  consisting of all
(unordered) pairs of hypotheses belonging to distinct classes. These are the edges that must be cut  by
which we mean for any edge {h  h(cid:48)} ∈ E  at least one hypothesis in {h  h(cid:48)} must be ruled out (i.e. 
eliminated from the version space). Hence  a test t run under true hypothesis h is said to cut edges
Et (h) := {{h(cid:48)  h(cid:48)(cid:48)} : h(cid:48)(t) (cid:54)= h(t) or h(cid:48)(cid:48)(t) (cid:54)= h(t)}. See Fig. 1(a) for an illustration. We deﬁne a
weight function w : E → R≥0 by w({h  h(cid:48)}) := P (h) · P (h(cid:48)). We extend the weight function to an
e∈E(cid:48) w(e). The

additive (modular) function on sets of edges in the natural manner  i.e.  w(E(cid:48)) :=(cid:80)

objective fEC that we will greedily maximize is then deﬁned as the weight of the edges cut (EC):

fEC(A  h) := w

Et (h)

(3.1)

(cid:17)

(cid:16)(cid:91)

t∈A

The key insight that allows us to prove approximation guarantees for fEC is that fEC shares the same
beneﬁcial properties that make fGBS amenable to efﬁcient greedy optimization. The proof of this
fact  as stated in Proposition 2  can be found in the long version of this paper [10].
Proposition 2. The objective fEC is strongly adaptively monotone and adaptively submodular.

Based on the objective fEC  we can calculate the marginal beneﬁts for test t upon observations xA as

∆EC (t| xA) := EH [fEC(A ∪ {t}   H) − fEC(A  H) | xA] .
adaptive policy πEC that 

after observing xA 

We
t∗ ∈ arg maxt ∆EC (t| xA) /c(t)  the EC2 algorithm (for equivalence class edge cutting).

greedily selects

call

the

test

4Bellala et al. simultaneously studied ECD [2]  and  like us  used it to model active learning with noise [3].
They developed an extension of GBS for ECD. We defer a detailed comparison of our approaches to future work.

4

(a) The Equivalence Class Determination problem

(b) Error model

Figure 1: (a) An instance of Equivalence Class Determination with binary test outcomes  shown with the set of
edges that must be cut  and depicting the effects of test i under different outcomes. (b) The graphical model
underlying our error model.

Equivalence Class Determination. Hereby  Q = w(E) = 1 −(cid:80)

Note that these instances are self–certifying  because we obtain maximum objective value if and
only if the version space lies within an equivalence class  and the policy can certify this condition
when it holds. So we can apply Theorem 1 to show EC2 obtains a ln(Q/η) + 1 approximation to
i(P (h ∈ Hi))2 ≤ 1 is the total
weight of all edges that need to be cut  and η = mine∈E w(e) ≥ p2
min is a bound on the minimum
weight among all edges. We have the following result:
Theorem 3. Suppose P (h) is rational for all h ∈ H. For the adaptive greedy policy πEC imple-
mented by EC2 it holds that

c(πEC) ≤ (2 ln(1/pmin) + 1)c(π∗) 

where pmin := minh∈H P (h) is the minimum prior probability of any hypothesis  and π∗ is the
optimal policy for the Equivalence Class Determination problem.

In the case of unit cost tests  we can apply a technique of Kosaraju et al. [15]  originally developed
for the GBS algorithm  to improve the approximation guarantee to O(log n) by applying EC2 with a
modiﬁed prior distribution. We defer details to the full version of this paper.

4 Bayesian Active Learning with Noise and the EFFECXTIVE Algorithm
We now address the case of noisy observations  using ideas from §3. With noisy observations  the
conditional distribution P (X1  . . .   XN | h) is no longer deterministic. We model the noise using an
additional random variable Θ. Fig. 1(b) depicts the underlying graphical model. The vector of test
outcomes xT is assumed to be an arbitrary  deterministic function xT : H × supp(Θ) → X N ; hence
XT | h is distributed as xT (h  Θh) where Θh is distributed as P (θ | h). For example  there might be
up to s = | supp(Θ)| ways any particular disease could manifest itself  with different patients with
the same disease suffering from different symptoms.
In cases where it is always possible to identify the true hypothesis  i.e.  xT (h  θ) (cid:54)= xT (h(cid:48)  θ(cid:48))
for all h (cid:54)= h(cid:48) and all θ  θ(cid:48) ∈ supp(Θ)  we can reduce the problem to Equivalence Class De-
termination with hypotheses {xT (h  θ) : h ∈ H  θ ∈ supp(Θ)} and equivalence classes Hi
:=
{xT (hi  θ) : θ ∈ supp(Θ)} for all i. Then Theorem 3 immediately yields that the approximation
factor of EC2 is at most 2 ln (1/ minh θ P (h  θ)) + 1  where the minimum is taken over all (h  θ) in
the support of P . In the unit cost case  running EC2 with a modiﬁed prior `a la Kosaraju et al. [15]
allows us to obtain an O(log |H| + log | supp(Θ)|) approximation factor. Note this model allows us
to incorporate noise with complex correlations.
However  a major challenge when dealing with noisy observations is that it is not always possible
to distinguish distinct hypotheses. Even after we have run all tests  there will generally still be
uncertainty about the true hypothesis  i.e.  the posterior distribution P (H | xT ) obtained using
Bayes’ rule may still assign non-zero probability to more than one hypothesis. If so  uniquely
determining the true hypothesis is not possible. Instead  we imagine that there is a set D of possible
decisions we may make after (adaptively) selecting a set of tests to perform and we must choose
one (e.g.  we must decide how to treat the medical patient  which scientiﬁc theory to adopt  or which
classiﬁer to use  given our observations). Thus our goal is to gather data to make effective decisions
[13]. Formally  for any decision d ∈ D we take  and each realized hypothesis h  we incur some loss
(cid:96)(d  h). Decision theory recommends  after observing xA  to choose the decision d∗ that minimizes
the risk  i.e.  the expected loss  namely d∗ ∈ arg mind

EH [(cid:96)(d  H) | xA].

5

.  .  .A natural goal in Bayesian active learning is thus to adaptively pick observations  until we are
guaranteed to make the same decision (and thus incur the same expected loss) that we would have
made had we run all tests. Thus  we can reduce the noisy Bayesian active learning problem to
the ECD problem by deﬁning the equivalence classes over all test outcomes that lead to the same
minimum risk decision. Hence  for each decision d ∈ D  we deﬁne

Hd := {xT : d = arg min

EH [(cid:96)(d(cid:48)  H) | xT ]}.

d(cid:48)

information (VoI): ∆VoI (t| xA)

(4.1)
If multiple decisions minimize the risk for a particular xT   we break ties arbitrarily. Identifying the
best decision d ∈ D then amounts to identifying which equivalence class Hd contains the realized
vector of outcomes  which is an instance of ECD.
One common approach to this problem is to myopically pick tests maximizing the
:= mind EH [(cid:96)(d  H) | xA] −
decision-theoretic value of
Ext∼Xt|xA [mind EH [(cid:96)(d  H) | xA  xt]]. The VoI of a test t is the expected reduction in the expected
loss of the best decision due to the observation of xt. However  we can show there are instances in
which such a policy pays Ω(n/ log(n)) times the optimal cost  even under a uniform prior on (h  θ)
and with | supp(Θ)| = 2. See the long version of this paper [10] for details. In contrast  on such
instances EC2 obtains an O(log n) approximation. More generally  we have the following result
for EC2 as an immediate consequence of Theorem 3.
Theorem 4. Fix hypotheses H  tests T with costs c(t) and outcomes in X   decision set D  and
loss function (cid:96). Fix a prior P (H  Θ) and a function xT : H × supp(Θ) → X N which deﬁne the
probabilistic noise model. Let c(π) denote the expected cost of π incurs to ﬁnd the best decision  i.e. 
to identify which equivalence class Hd the outcome vector xT belongs to. Let π∗ denote the policy
minimizing c(·)  and let πEC denote the adaptive policy implemented by EC2. Then it holds that

(cid:18)

(cid:18) 1

(cid:19)

(cid:19)

+ 1

c(π∗) 

c(πEC) ≤

p(cid:48)
min := minh∈H {P (h  θ) : P (h  θ) > 0}.

2 ln

min

where p(cid:48)

If all tests have unit cost  by using a modiﬁed prior [15] the approximation factor can be improved to
O (log |H| + log | supp(Θ)|) as in the case of Theorem 3.

The EFFECXTIVE algorithm. For some noise models  Θ may have exponentially–large support. In
this case reducing Bayesian active learning with noise to Equivalence Class Determination results in
instances with exponentially-large equivalence classes. This makes running EC2 on them challenging 
since explicitly keeping track of the equivalence classes is impractical. To overcome this challenge 
we develop EFFECXTIVE  a particularly efﬁcient algorithm which approximates EC2.
For clarity  we only consider the 0−1 loss  i.e.  our goal is to ﬁnd the most likely hypothesis (MAP
estimate) given all the data xT   namely h∗(xT ) := arg maxh P (h | xT ). Recall deﬁnition (4.1)  and
consider the weight of edges between distinct equivalence classes Hi and Hj:
w(Hi×Hj) =
= P (XT ∈ Hi)P (XT ∈ Hj).
In general  P (XT ∈ Hi) can be estimated to arbitrary accuracy using a rejection sampling approach
with bounded sample complexity. We defer details to the full version of the paper. Here  we focus on
the case where  upon observing all tests  the hypothesis is uniquely determined  i.e.  P (H | xT ) is
deterministic for all xT in the support of P . In this case  it holds that P (XT ∈ Hi) = P (H = hi).

P (xT )P (x(cid:48)
T ∈Hj

(cid:17)(cid:16) (cid:88)

(cid:16) (cid:88)

(cid:88)

xT ∈Hi x(cid:48)

T ∈Hj
x(cid:48)

P (xT )

P (x(cid:48)

xT ∈Hi

T ) =

(cid:17)

T )

Thus  the total weight is(cid:88)
(cid:104)(cid:88)

∆Eff (t| xA) :=

i(cid:54)=j

w(Hi × Hj) =

P (Xt = x | xA)

This insight motivates us to use the objective function

(cid:16)(cid:88)

i

P (hi)

(cid:17)2 −(cid:88)
P (hi)2 = 1 −(cid:88)
P (hi | xA  Xt = x)2(cid:17)(cid:105) −(cid:88)
(cid:16)(cid:88)

i

i

i

i

P (hi)2.

P (hi | xA)2 

the weight of a distribution 1 −(cid:80)

x

which is the expected reduction in weight from the prior to the posterior distribution. Note that
i P (hi)2 is a monotonically increasing function of the R´enyi

6

2 log(cid:80)

entropy (of order 2)  which is − 1
i P (hi)2. Thus the objective ∆Eff can be interpreted as a
(non-standard) information gain in terms of the (exponentiated) R´enyi entropy. In our experiments 
we show that this criterion performs well in comparison to existing experimental design criteria 
including the classical Shannon information gain. Computing ∆Eff (t| xA) requires us to perform one
inference task for each outcome x of Xt  and O(n) computations to calculate the weight for each
outcome. We call the algorithm that greedily optimizes ∆Eff the EFFECXTIVE algorithm (since it
uses an Efﬁcient Edge Cutting approXimate objective)  and present pseudocode in Algorithm 1.

Input: Set of hypotheses H; Set of tests T ; prior distribution P ; function f.
beginA ← ∅;

while ∃h (cid:54)= h(cid:48) : P (h | xA) > 0 and P (h(cid:48) | xA) > 0 do

(cid:104)(cid:80)
xP (Xt = x | xA)(cid:0)(cid:80)

iP (hi | xA  Xt = x)2(cid:1)(cid:105) −(cid:80)

foreach t ∈ T \ A do

Select t∗ ∈ arg maxt ∆Eff (t| xA) /c(t); Set A ← A ∪ {t∗} and observe outcome xt∗;

end
Algorithm 1: The EFFECXTIVE algorithm using the Efﬁcient Edge Cutting approXimate objective.

∆Eff (t| xA) :=

i P (hi | xA)2;

UP T (L) =(cid:80)

i f ((cid:96)i)w(pi) for nonlinear functions f ((cid:96)i) = (cid:96)ρ

ity posited is UCRRA(L) =(cid:80)

/(1 − a) if a (cid:54)= 1  and UCRRA(L) =(cid:80)

5 Experiments
Several economic theories make claims to explain how people make decisions when the payoffs
are uncertain. Here we use human subject experiments to compare four key theories proposed in
literature. The uncertainty of the payoff in a given situation is represented by a lottery L  which is
simply a random variable with a range of payoffs L := {(cid:96)1  . . .   (cid:96)k}. For our purposes  a payoff is
an integer denoting how many dollars you receive (or lose  if the payoff is negative). Fix lottery
L  and let pi := P [L = (cid:96)i]. The four theories posit distinct utility functions  with agents prefer-
ring larger utility lotteries. Three of the theories have associated parameters. The Expected Value
theory [22] posits simply UEV (L) = E [L]  and has no parameters. Prospect theory [14] posits
i   if (cid:96)i ≥ 0 and f ((cid:96)i) = −λ(−(cid:96)i)ρ  if
(cid:96)i < 0  and w(pi) = e−(log(1/pi))α [21]. The parameters ΘP T = {ρ  λ  α} represent risk aversion 
loss aversion and probability weighing factor respectively. For portfolio optimization problems  ﬁnan-
cial economists have used value functions that give weights to different moments of the lottery [12]:
UM V S(L) = wµµ − wσσ + wνν  where ΘM V S = {wµ  wσ  wν} are the weights for the mean  stan-
dard deviation and standardized skewness of the lottery respectively. In Constant Relative Risk Aver-
sion theory [20]  there is a parameter ΘCRRA = a representing the level of risk aversion  and the util-
i pi log((cid:96)i)  if a = 1.
The goal is to adaptively select a sequence of tests to present to a human subject in order to
distinguish which of the four theories best explains the subject’s responses. Here a test t is a pair
of lotteries  (Lt
2). Based on the theory that represents behaviour  one of the lotteries would be
preferred to the other  denoted by a binary response xt ∈ {1  2}. The possible payoffs were ﬁxed to
L = {−10  0  10} (in dollars)  and the distribution (p1  p2  p3) over the payoffs was varied  where
pi ∈ {0.01  0.99} ∪ {0.1  0.2  . . .   0.9}. By considering all non-identical pairs of such lotteries  we
obtained the set of possible tests.
We compare six algorithms: EFFECXTIVE  greedily maximizing Information Gain (IG)  Value of
Information (VOI)  Uncertainty Sampling5 (US)  Generalized Binary Search (GBS)  and tests selected
at Random. We evaluated the ability of the algorithms to recover the true model based on simulated
responses. We chose parameter values for the theories such that they made distinct predictions and
were consistent with the values proposed in literature [14]. We drew 1000 samples of the true model
and ﬁxed the parameters of the model to some canonical values  ΘP T = {0.9  2.2  0.9}  ΘM V S =
{0.8  0.25  0.25}  ΘCRRA = 1. Responses were generated using a softmax function  with the
probability of response xt = 1 given by P (xt = 1) = 1/(1 + eU (Lt
1)). Fig. 2(a) shows the
performance of the 6 methods  in terms of the accuracy of recovering the true model with the number
of tests. We ﬁnd that US  GBS and VOI perform signiﬁcantly worse than Random in the presence
of noise. EFFECXTIVE outperforms InfoGain signiﬁcantly  which outperforms Random.

i pi(cid:96)1−a

i

2)−U (Lt

1  Lt

5Uncertainty sampling greedily selects the test whose outcome distribution has maximum Shannon entropy.

7

(a) Fixed parameters

(b) With parameter uncertainty

(c) Human subject data

Figure 2: (a) Accuracy of identifying the true model with ﬁxed parameters  (b) Accuracy using a grid of
parameters  incorporating uncertainty in their values  (c) Experimental results: 11 subjects were classiﬁed into
the theories that described their behavior best. We plot probability of classiﬁed type.

We also considered uncertainty in the values of the parameters  by setting ρ from 0.85-0.95  λ from
2.1-2.3  α from 0.9-1; wµ from 0.8-1.0  wσ from 0.2-0.3  wν from 0.2-0.3; and a from 0.9-1.0  all
with 3 values per parameter. We generated 500 random samples by ﬁrst randomly sampling a model
and then randomly sampling parameter values. EFFECXTIVE and InfoGain outperformed Random
signiﬁcantly  Fig. 2(b)  although InfoGain did marginally better among the two. The increased
parameter range potentially poses model identiﬁability issues  and violates some of the assumptions
behind EFFECXTIVE  decreasing its performance to the level of InfoGain.
After obtaining informed consent according to a protocol approved by the Institutional Review Board
of Caltech  we tested 11 human subjects to determine which model ﬁt their behaviour best. Laboratory
experiments have been used previously to distinguish economic theories  [4]  and here we used a
real-time  dynamically optimized experiment that required fewer tests. Subjects were presented 30
tests using EFFECXTIVE. To incentivise the subjects  one of these tests was picked at random  and
subjects received payment based the outcome of their chosen lottery. The behavior of most subjects
(7 out of 10) was best described by EV. This is not unexpected given the high quantitative abilities of
the subjects. We also found heterogeneity in classiﬁcation: One subject got classiﬁed as MVS  as
identiﬁed by violations of stochastic dominance in the last few choices. 2 subjects were best described
by prospect theory since they exhibited a high degree of loss aversion and risk aversion. One subject
was also classiﬁed as a CRRA-type (log-utility maximizer). Figure 2(c) shows the probability of
the classiﬁed model with number of tests. Although we need a larger sample to make signiﬁcant
claims of the validity of different economic theories  our preliminary results indicate that subject
types can be identiﬁed and there is heterogeneity in the population. They also serve as an example of
the beneﬁts of using real-time dynamic experimental design to collect data on human behavior.

6 Conclusions
In this paper  we considered the problem of adaptively selecting which noisy tests to perform in order
to identify an unknown hypothesis sampled from a known prior distribution. We studied the Equiva-
lence Class Determination problem as a means to reduce the case of noisy observations to the classic 
noiseless case. We introduced EC2  an adaptive greedy algorithm that is guaranteed to choose the
same hypothesis as if it had observed the outcome of all tests  and incurs near-minimal expected cost
among all policies with this guarantee. This is in contrast to popular heuristics that are greedy w.r.t.
version space mass reduction  information gain or value of information  all of which we show can be
very far from optimal. EC2 works by greedily optimizing an objective tailored to differentiate between
sets of observations that lead to different decisions. Our bounds rely on the fact that this objective func-
tion is adaptive submodular. We also develop EFFECXTIVE  a practical algorithm based on EC2  that
can be applied to arbitrary probabilistic models in which efﬁcient exact inference is possible. We apply
EFFECXTIVE to a Bayesian experimental design problem  and our results indicate its effectiveness in
comparison to existing algorithms. We believe that our results provide an interesting direction towards
providing a theoretical foundation for practical active learning and experimental design problems.

Acknowledgments. This research was partially supported by ONR grant N00014-09-1-1044  NSF grant
CNS-0932392  NSF grant IIS-0953413  a gift by Microsoft Corporation  an Okawa Foundation Research Grant 
and by the Caltech Center for the Mathematics of Information.

8

0510152025300.20.30.40.50.60.70.80.91Number of testsVOIEffECXtiveGBSRandomUncertaintySamplingInfoGain0510152025300.20.30.40.50.60.70.80.91Number of testsVOIEffECXtiveInfoGainRandomGBSUncertaintySampling05101520253000.10.20.30.40.50.60.70.80.91Prob. of Classified TypeNumber of testsMVS  n=1CRRA  n=1EV  n=7PT  n=2References
[1] N. Balcan  A. Beygelzimer  and J. Langford. Agnostic active learning. In ICML  2006.
[2] Gowtham Bellala  Suresh Bhavnani  and Clayton Scott. Extensions of generalized binary search to group
identiﬁcation and exponential costs. In Advances in Neural Information Processing Systems (NIPS)  2010.
[3] Gowtham Bellala  Suresh K. Bhavnani  and Clayton D. Scott. Group-based query learning for rapid

diagnosis in time-critical situations. CoRR  abs/0911.4511  2009.

[4] Colin F. Camerer. An experimental test of several generalized utility theories. The Journal of Risk and

Uncertainty  2(1):61–104  1989.

[5] V. T. Chakaravarthy  V. Pandit  S. Roy  P. Awasthi  and M. Mohania. Decision trees for entity identiﬁcation:
Approximation algorithms and hardness results. In In Proceedings of the ACM- SIGMOD Symposium on
Principles of Database Systems  2007.

[6] K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science  10(3):273–304 

Aug. 1995.

[7] Sanjoy Dasgupta. Analysis of a greedy active learning strategy. In NIPS  2004.
[8] Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. In Y. Weiss  B. Sch¨olkopf 
and J. Platt  editors  Advances in Neural Information Processing Systems 18  pages 235–242. MIT Press 
Cambridge  MA  2006.

[9] Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active learning

and stochastic optimization. CoRR  abs/1003.3967v3  2010.

[10] Daniel Golovin  Andreas Krause  and Debajyoti Ray. Near-optimal Bayesian active learning with noisy

observations. CoRR  abs/1010.3091  2010.

[11] Andrew Guillory and Jeff Bilmes. Average-case active learning with costs. In The 20th International

Conference on Algorithmic Learning Theory  University of Porto  Portugal  October 2009.

[12] Giora Hanoch and Haim Levy. Efﬁcient portfolio selection with quadratic and cubic utility. The Journal of

Business  43(2):181–189  1970.

[13] R. A. Howard. Information value theory. In IEEE Transactions on Systems Science and Cybernetics

(SSC-2)  1966.

[14] D. Kahneman and A. Tversky. Prospect theory: An analysis of decision under risk. Econometrica 

47(2):263–292  1979.

[15] S. Rao Kosaraju  Teresa M. Przytycka  and Ryan S. Borgstrom. On an optimal split tree problem. In WADS
’99: Proceedings of the 6th International Workshop on Algorithms and Data Structures  pages 157–168 
London  UK  1999. Springer-Verlag.

[16] Andreas Krause and Carlos Guestrin. Optimal value of information in graphical models. Journal of

Artiﬁcial Intelligence Research (JAIR)  35:557–591  2009.

[17] D. V. Lindley. On a measure of the information provided by an experiment. Annals of Mathematical

Statistics  27:986–1005  1956.

[18] D. MacKay. Information-based objective functions for active data selection. Neural Computation  4(4):590–

604  1992.

[19] Rob Nowak. Noisy generalized binary search. In Y. Bengio  D. Schuurmans  J. Lafferty  C. K. I. Williams 
and A. Culotta  editors  Advances in Neural Information Processing Systems 22  pages 1366–1374. 2009.

[20] John W. Pratt. Risk aversion in the small and in the large. Econometrica  32(1):122–136  1964.
[21] D. Prelec. The probablity weighting function. Econometrica  66(3):497–527  1998.
[22] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behaviour. Princeton

University Press  1947.

[23] Alice X. Zheng  Irina Rish  and Alina Beygelzimer. Efﬁcient test selection in active diagnosis via entropy
approximation. In UAI ’05  Proceedings of the 21st Conference in Uncertainty in Artiﬁcial Intelligence 
2005.

9

,Qian Wang
Jiaxing Zhang
Sen Song
Zheng Zhang
Guruprasad Raghavan
Matt Thomson