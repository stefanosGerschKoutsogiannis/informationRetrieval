2019,Pure Exploration with Multiple Correct Answers,We determine the sample complexity of pure exploration bandit problems with multiple good answers. We derive a lower bound using a new game equilibrium argument. We show how continuity and convexity properties of single-answer problems ensure that the existing Track-and-Stop algorithm has asymptotically optimal sample complexity. However  that convexity is lost when going to the multiple-answer setting. We present a new algorithm which extends Track-and-Stop to the multiple-answer case and has asymptotic sample complexity matching the lower bound.,Pure Exploration with Multiple Correct Answers

Rémy Degenne

Centrum Wiskunde & Informatica
Science Park 123  Amsterdam  NL

remy.degenne@cwi.nl

Wouter M. Koolen

Centrum Wiskunde & Informatica
Science Park 123  Amsterdam  NL

wmkoolen@cwi.nl

Abstract

We determine the sample complexity of pure exploration bandit problems with
multiple good answers. We derive a lower bound using a new game equilibrium
argument. We show how continuity and convexity properties of single-answer
problems ensure that the existing Track-and-Stop algorithm has asymptotically
optimal sample complexity. However  that convexity is lost when going to the
multiple-answer setting. We present a new algorithm which extends Track-and-
Stop to the multiple-answer case and has asymptotic sample complexity matching
the lower bound.

1

Introduction

In pure exploration aka active testing problems the learning system interacts with its environment
by sequentially performing experiments to quickly and reliably identify the answer to a particular
pre-speciﬁed question. Practical applications range from simple queries for cost-constrained physical
regimes  i.e. clinical drug testing  to complex queries in structured environments bottlenecked
by computation  i.e. simulation-based planning. The theory of pure exploration is studied in the
multi-armed bandit framework. The scientiﬁc challenge is to develop tools for characterising the
sample complexity of new pure exploration problems  and methodologies for developing (matching)
algorithms. With the aim of understanding power and limits of existing methodology  we study an
extended problem formulation where each instance may have multiple correct answers. We ﬁnd
that multiple-answer problems present a phase transition in complexity  and require a change in our
thinking about algorithms.
The existing methodology for developing asymptotically instance-optimal algorithms  Track-and-
Stop by Garivier and Kaufmann [2016]  exploits the so-called oracle weights. These probability
distributions on arms naturally arise in sample complexity lower bounds  and dictate the optimal
sampling proportions for an “oracle” algorithm that needs to be sample efﬁcient only on exactly the
current problem instance. The main idea is to track the oracle weights computed at a converging
estimate of the instance. The analysis of Track-and-Stop requires continuity of the oracle weights as a
function of the bandit model. For the core Best Arm Identiﬁcation problem  discontinuity only occurs
at degenerate instances where the sample complexity explodes. So this assumption seemed harmless.

Our contribution We show that the oracle weights may be non-unique  even for single-answer
problems  and hence need to be regarded as a set-valued mapping. We show this mapping is always
(upper hemi-)continuous. We show that each instance maps to a convex set for single-answer
problems  and this allows us to extend the Track-and-Stop methodology to all such problems. At
instances with non-singleton set-valued oracle weights more care is needed: of the two classical
tracking schemes “C” converges to the convex set  while “D” may fail entirely.
We show that for multiple-answer problems convexity is violated. There are instances where two
distinct oracle weights are optimal  while no mixture is. Unmodiﬁed tracking converges in law

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(experimentally) to a distribution on the full convex hull  and suffers as a result. We propose a “sticky”
modiﬁcation to stabilise the approach  and show that now it converges to only the corners. We prove
that Sticky Track-and-Stop is asymptotically optimal.

Related work Multi-armed bandits have been the subject of intense study in their role as a model
for medical testing and reinforcement learning. For the objective of reward maximisation [Berry and
Fristedt  1985  Lai and Robbins  1985  Auer et al.  2002  Bubeck and Cesa-Bianchi  2012] the main
challenge is balancing exploration and exploitation. The ﬁeld of pure exploration (active testing)
focuses on generalisation vs sample complexity  in the ﬁxed conﬁdence  ﬁxed budget and simple regret
scalarisations. It took off in machine learning with the multiple-answer problem of (  δ)-Best Arm
Identiﬁcation (BAI) [Even-Dar et al.  2002]. Early results focused on worst-case sample complexity
guarantees in sub-Gaussian bandits. Successful approaches include Upper and Lower conﬁdence
bounds [Bubeck et al.  2011  Kalyanakrishnan et al.  2012  Gabillon et al.  2012  Kaufmann and
Kalyanakrishnan  2013  Jamieson et al.  2014]  Racing or Successive Rejects/Eliminations [Maron
and Moore  1997  Even-Dar et al.  2006  Audibert et al.  2010  Kaufmann and Kalyanakrishnan  2013 
Karnin et al.  2013].
Fundamental information-theoretic barriers [Castro  2014  Kaufmann et al.  2016  Garivier and
Kaufmann  2016] for each speciﬁc problem instance reﬁned the worst-case picture  and sparked
the development of instance-optimal algorithms for single-answer problems based on Track-and-
Stop [Garivier and Kaufmann  2016] and Thompson Sampling [Russo  2016]. For multiple-answer
problems the elegant KL-contraction-based lower bound is not sharp  and new techniques were
developed by Garivier and Kaufmann [2019].
Recent years also saw a surge of interest in pure exploration with complex queries and structured
environments. Kalyanakrishnan and Stone [2010] identify the top-M set  Locatelli et al. [2016]
the arm closest to a threshold  and Chen et al. [2014]  Gabillon et al. [2016] the optimiser over an
arbitrary combinatorial class. For arms arranged in a matrix Katariya et al. [2017] study BAI under a
rank-one assumption  while Zhou et al. [2017] seek to identify a Nash equilibrium. For arms arranged
in a minimax tree there is signiﬁcant interest in ﬁnding the optimal move at the root Teraoka et al.
[2014]  Garivier et al. [2016]  Huang et al. [2017]  Kaufmann and Koolen [2017]  Kaufmann et al.
[2018]  as a theoretical model for studying Monte Carlo Tree search (which is a planning sub-module
of many advanced reinforcement learning systems).

2 Notations

We work in a given one-parameter one-dimensional canonical exponential family  with mean pa-
rameter in an open interval O ⊆ R. We denote by d(µ  λ) the KL divergence from the distribution
with mean µ to that with mean λ. A K-armed bandit model is identiﬁed by its vector µ ∈ OK of
mean parameters. We denote by M ⊆ OK the set of possible mean parameters in a speciﬁc bandit
problem. Excluding parts of OK from M may be used to encode a known structure of the problem.
We assume that there is a ﬁnite domain I of answers  and that the correct answer for each bandit
model is speciﬁed by a set-valued function i∗ : M → 2I.
Example 1. As our running example  we will use the Any Low Arm multiple-answer problem. Given
a threshold γ ∈ O  the goal is return the index k of any arm with µk < γ if such an arm exists  or to
return “no” otherwise. Formally  we have possible answers I = [K] ∪ {no}  and correct answers

We exclude the case mink µk = γ from M (as such instances require inﬁnite sample complexity).
Additional examples of multiple-answer identiﬁcation problems are visualised in Table 1 in Ap-
pendix B.
Algorithms. A learning strategy is parametrised by a stopping rule τδ ∈ N depending on a
parameter δ ∈ [0  1]  a sampling rule A1  A2  . . . ∈ [K]  and a recommendation rule ˆı ∈ I. When a
learning strategy meets a bandit model µ  they interactively generate a history A1  X1  . . .   Aτ   Xτ   ˆı 
where Xt ∼ µAt. We allow the possibility of non-termination τδ = ∞  in which case there is no
recommendation ˆı. At stage t ∈ N  we denote by Nt = (Nt 1  . . .   Nt K) the number of samples (or
“pulls”) of the arms  and by ˆµt the vector of empirical means of the samples of each arm.

(cid:26){k | µk ≤ γ}

{no}

i∗(µ) =

if mink µk < γ 
if mink µk > γ.

2

(cid:88)

k

(cid:0)τδ < ∞ and ˆı ∈ i∗(µ)(cid:1) ≥ 1 − δ. We call a given strategy δ-correct if it is δ-correct for

Conﬁdence and sample complexity. For conﬁdence parameter δ ∈ (0  1)  we say that a strategy
is δ-correct (or δ-PAC) for bandit model µ if it recommends a correct answer with high probability 
i.e. Pµ
every µ ∈ M. We measure the statistical efﬁciency of a strategy on a bandit model µ by its sample
complexity Eµ[τδ]. We are interested in δ-correct algorithms minimizing sample complexity.
Divergences. For any answer i ∈ I  we deﬁne the alternative to i  denoted ¬i  to be the set of
bandit models on which answer i is incorrect  i.e.

¬i := {µ ∈ M|i /∈ i∗(µ)} .

We deﬁne the (w-weighted) divergence from µ ∈ M to λ ∈ M or Λ ⊆ M by

D(w  µ  λ) =

wkd(µk  λk)

D(w  µ  Λ) = inf
λ∈Λ

D(µ  Λ) = sup
w∈(cid:52)K

D(w  µ  Λ)

D(µ) = max
i∈I

D(w  µ  λ)
D(µ ¬i)

Note that D(w  µ  Λ) = 0 whenever µ ∈ Λ. We denote by iF (µ) the argmax (set of maximisers) of
(cid:83)
i (cid:55)→ D(µ ¬i)  and we call iF (µ) the oracle answer(s) at µ. We write w∗(µ ¬i) for the maximisers
of w (cid:55)→ D(w  µ ¬i)  and call these the oracle weights for answer i at µ. We write w∗(µ) =
i∈iF (µ) w∗(µ ¬i) for the set of oracle weights among all oracle answers. We include expressions
for the divergence when i∗ is generated by half-spaces  minima and spheres in Appendix H.
Example 1 (Continued). Consider an Any Low Arm instance µ with mink µk < γ. For any arm i ∈
i∗(µ)  we have D(w  µ ¬i) = wid(µi  γ) and D(µ ¬i) = d(µi  γ). Hence D(µ) = d(mini µi  γ) 
and iF (µ) = argmini µi. On the other hand  when mink µk > γ  we have i∗(µ) = {no}  so that
D(w  µ ¬no) = mink wkd(µk  γ) and D(µ ¬no) = D(µ) = 1
The function iF (µ) = {i ∈ I : D(µ ¬i) = D(µ)} is set valued  as is w∗. They are singletons with
continuous value over some connected subsets of M  and are multi-valued on common boudaries of
two or more such sets. Both iF and w∗ can be thought of as having single values  unless µ sits on
such a boundary  in which case we will prove that they are equal to the union (or convex hull of the
union) of their values in the neighbouring regions.

(cid:46)(cid:80)K

k=1

1

d(µk γ) .

3 Lower Bound

We show a lower bound for any algorithm for multiple-answer problems. Our lower bound extends
the single-answer result of Garivier and Kaufmann [2016]. We are further inspired by Garivier
and Kaufmann [2019]  who analyse the -BAI problem. They prove lower bounds for algorithms
with a sampling rule independent of δ  imposing the further restriction that either K = 2 or that
the algorithm ensures that Nt k/t converges as t → ∞. The new ingredient in this section is a
game-theoretic equilibrium argument  which allows us to analyse any δ-correct algorithm in any
multiple answer problem. Our main lower bound is the following.
Theorem 1. Any δ-correct algorithm veriﬁes

Eµ[τδ]
log(1/δ)

lim inf
δ→0

≥ T ∗(µ) := D(µ)−1 where D(µ) = max
i∈i∗(µ)

max
w∈(cid:52)K

inf
λ∈¬i

for any multiple answer instance µ with sub-Gaussian arm distributions.

K(cid:88)

k=1

wkd(µk  λk)

The proof is in Appendix C  where we also discuss how the convenient sub-Gaussian assumption can
be relaxed. We would like to point out one salient feature here. To show sample complexity lower
bounds at µ  one needs to ﬁnd problems that are hard to distinguish from it statistically  yet require a
different answer. We obtain these problems by means of a minimax result.
Lemma 2. For any answer i ∈ I  the divergence from µ to ¬i equals
Eλ∼P [d(µk  λk)] .

D(µ ¬i) = inf
P

max
k∈[K]

where the inﬁmum ranges over probability distributions on ¬i supported on (at most) K points.

3

The proof of Theorem 1 then challenges any algorithm for µ by obtaining a witness P for D(µ) =
maxi D(µ ¬i) from Lemma 2  sampling a model λ ∼ P  and showing that if the algorithm stops
early  it must make a mistake w.h.p. on at least one model from the support. The equilibrium property
of P allows us to control a certain likelihood ratio martingale regardless of the sampling strategy
employed by the algorithm.
We discuss the novel aspect of Theorem 1 and its lessons for the design of optimal algorithms. First
of all  for single-answer instances |i∗(µ)|=1 we recover the known asymptotic lower bound [Garivier
and Kaufmann  2016  Remark 2]. For multiple-answer instances the bound says the following.
The optimal sample complexity hinges on the oracle answers iF (µ). That is  for if ∈ iF (µ)  the
complexity of problem µ is governed by the difﬁculty of discriminating µ from the set of models on
which answer if is incorrect.
Is the bound tight? We argue yes. Consider the following oracle strategy  which is speciﬁcally
designed to be very good at µ. First  it computes a pair (i  w) witnessing the two outer maxima in
D(µ). The algorithm samples according to w. It stops when it can statistically discriminate ˆµt from
¬i  and outputs ˆı = i. This algorithm will indeed have expected sample complexity equal to D(µ)−1
at µ  and it will be correct.
The above oracle viewpoint presents an idea for designing algorithms  following Garivier and
Kaufmann [2016] and Chen et al. [2017]. Perform a lower-order amount of forced exploration of all
arms to ensure ˆµt → µ. Then at each time point compute the empirical mean vector ˆµt and oracle
weights wt ∈ w∗( ˆµt). Then sample according to wt. This approach is successful for single-answer
bandits with unique and continuous oracle weights. We argue in Section 4.3 below that it extends to
points of discontinuity by exploiting upper hemicontinuity and convexity of w∗.
For multiple-answer bandits  we argue that the set of maximisers w∗(µ) is no longer convex when
iF (µ) is not a singleton. It can then happen that ˆµt → µ  while at the same time w∗( ˆµt) keeps
oscillating. If the algorithm tracks w∗( ˆµt)  its sampling proportions will end up in the convex hull of
w∗(µ). However  as w∗(µ) is not convex itself  these proportions will not be optimal. We present
empirical evidence for that effect in Appendix D. The lesson here is that the oracle needs to pick an
answer and “stick with it”. This will be the basis of our algorithm design in Section 5.

4 Properties of the Optimal Allocation Sets

The Track-and-Stop sampling strategy aims at ensuring that the sampling proportions converge to
oracle weights. In the case of a singleton-valued oracle weights set w∗(µ) for single answer problems 
that convergence was proven in [Garivier and Kaufmann  2016]. We study properties of that set
with the double aim of extending Track-and-Stop to points µ where w∗(µ) is not a singleton and of
highlighting what properties hold only for the single-answer case  but not in general.

4.1 Continuity
We ﬁrst prove continuity properties of D and w∗. We show how the convergence of ˆµt to µ translates
into properties of the divergences from ˆµt to the alternative sets.
For a set B  let S(B) = 2B \ {∅} be the set of all non-empty subsets of B.
Deﬁnition 3 (Upper hemicontinuity). A set-valued function Γ : A → S(B) is upper hemicontinuous
at a ∈ A if for any open neighbourhood V of Γ(a) there exists a neighbourhood U of a such that for
all x ∈ U  Γ(x) is a subset of V .
Theorem 4. For all i ∈ I 

1. the function (w  µ) (cid:55)→ D(w  µ ¬i) is continuous on (cid:52)K × M 
2. µ (cid:55)→ D(µ ¬i) and µ (cid:55)→ D(µ) are continuous on M 
3. µ (cid:55)→ w∗(µ ¬i)  µ (cid:55)→ w∗(µ) and µ (cid:55)→ iF (µ) are upper hemicontinuous on M with

non-empty and compact values 

The proof is in Appendix F. It uses Berge’s maximum theorem and a modiﬁcation thereof due to
[Feinberg et al.  2014]. Related continuity results using this type of arguments  but restricted to
single-valued functions  appeared for the regret minimization problem in [Combes et al.  2017].

4

4.2 Convexity

Next we establish convexity.
Proposition 5. For each i ∈ I  for all µ ∈ M the set w∗(µ ¬i) is convex.
This is a consequence of the concavity of w (cid:55)→ D(w  µ ¬i) (which is an inﬁmum of linear functions).
In single-answer problems  we obtain that the oracle weights set w∗(µ) is convex everywhere. This
is however not the case in general for multiple-answer problems  as illustrated by the next example.
Example 1 (Continued). Consider a K = 2-arm Any Low Arm instance µ with µ1 < γ and
µ2 < γ  so that both answers 1 and 2 are correct. Recall that D(µ) = maxk=1 2 d(µk  γ). Now for
µ1 < µ2 < γ  w∗(µ) = {(1  0)} and symmetrically for µ2 < µ1 < γ  w∗(µ) = {(0  1)}. However 
for µ1 = µ2 < γ  w∗(µ) = {(1  0)  (0  1)}  which is not convex. Playing intermediate weights w =
(α  1 − α) results in strictly sub-optimal D(µ  w) = max{α  1 − α} d(µ  γ) < d(µ  γ) = D(µ).
This example also illustrates the upper hemicontinuity of w∗(µ): since µ of the form (µ  µ) is the
limit of a sequence (µt)t∈N with µt 1<µt 2  we obtain that {(1  0)} ⊆ w∗(µ). Similarly  using a
sequence with µt 1>µt 2  {(0  1)} ⊆ w∗(µ).
The example scales up to K arms  and shows that the sample complexity guarantee for vanilla TaS
(Theorem 9) may exceeds by a factor K the optimal complexity  which is matched by our new method
(Theorem 11).

4.3 Consequences for Track-and-Stop
The original analysis of Track-and-Stop excludes the mean vectors µ ∈ M for which w∗(µ) is not
a singleton. We show that the upper hemicontinuity and convexity properties of w∗(µ) allow us
to extend that analysis to all µ with a single oracle answer (in particular all single-answer bandit
problems)  at least for one of the two Track-and-Stop variants. Indeed  that algorithm was introduced
with two possible subroutines  dubbed C-tracking and D-tracking [Garivier and Kaufmann  2016].
Both variants compute oracle weights wt at the point ˆµt  but the arm pulled differs.
K = {w ∈ (cid:52)K : ∀k ∈ [K]  wk ≥ εt}  where
C-tracking: compute the projection wεt

εt > 0. Pull the arm with index kt = arg mink∈[K] Nt k −(cid:80)t
D-tracking: if there is an arm j with Nt j ≤ √
kt = arg mink∈[K] Nt k − twt k .
The proof of the optimal sample complexity of Track-and-Stop for C-tracking remains essentially
unchanged but we replace Proposition 9 of [Garivier and Kaufmann  2016] by the following lemma 
proved in Appendix G.3.
Lemma 6. Let a sequence ( ˆµt)t∈N verify limt→+∞ ˆµt = µ . For all t ≥ 0  let wt ∈ w∗( ˆµt) be
arbitrary oracle weights for ˆµt . If w∗(µ) is convex  then

t − K/2  then pull kt = j. Otherwise  pull the arm

t of wt on (cid:52)εt

s=1 wεs
s k.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

t

t(cid:88)

s=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∞

lim
t→+∞ inf

w∈w∗(µ)

ws − w

= 0 .

The average of oracle weights for ˆµt converges to the set of oracle weights for µ. C-tracking then
ensures that the proportion of pulls Nt/t is close to that average by Lemma 7 of [Garivier and
Kaufmann  2016]  hence Nt/t gets close to oracle weights.
Theorem 7. For all µ ∈ M such that iF (µ) is a singleton (in particular all single-answer problems) 
Track-and-Stop with C-tracking is δ-correct with asymptotically optimal sample complexity.

Proof in Appendix G.6. We encourage the reader to ﬁrst proceed to Section 5  since the proof
considers the result as a special case of the multiple-answers setting.
Remark 8. If w∗(µ) is not a singleton  Track-and-Stop using D-tracking may fail to converge to
w∗(µ)  even when it is convex.
While we do not prove that D-tracking fails to converge to w∗(µ) on a speciﬁc example of a
bandit  we provide empirical evidence in Appendix E. The reason for the failure of D-tracking

5

Dt

i∗(µ)

ˆµt

{1}
{2}
{no}
{1  2}

Ct

ˆµt

< <

(a) Stopping rule: does the conservative conﬁdence
region Dt exclude the alternative ¬i to any answer i?

(b) Sampling rule: ﬁnd least (in sticky order) ora-
cle answer in the aggressive conﬁdence region Ct.
Track its oracle weights at ˆµt.

Figure 1: Sticky Track-and-Stop: The two main ideas  illustrated on the Any Low Arm problem.

is that it does not in general converge to the convex hull of the points it tracks. Suppose that
wt = w(1) = (1/2  1/2  0) for t odd and wt = w(2) = (1/2  0  1/2) for t even. Then D-tracking
veriﬁes limt→+∞ Nt/t = (1/3  1/3  1/3). This limit is outside of the convex hull of {w(1)  w(2)}.

5 Algorithms for the Multiple-Answers Setting

We can prove for Track-and-Stop the following suboptimal upper bound on the sample complexity 
based on the fact that it ensures convergence of Nt/t to the convex hull of the oracle weight set.
Theorem 9. Let conv(A) be the convex hull of a set A. For all µ ∈ M in a multi-answer problem 
Track-and-Stop with C-tracking is δ-correct and veriﬁes

Eµ[τδ]
log(1/δ)

lim
δ→0

≤

max

w∈conv(w∗(µ))

1

D(w  µ)

.

5.1 Sticky Track-and-Stop
The cases of multiple-answers problems for which Track-and-Stop is inadequate are µ ∈ M with
iF (µ) of cardinality greater than 1. When convexity does not hold  w∗(µ) is the union of the convex
sets (w∗(µ ¬i))i∈iF (µ). If an algorithm can a priori select if ∈ iF (µ) and track allocations wt in
w∗( ˆµt ¬if )  then using Track-and-Stop on that restricted problem will ensure that Nt/t converges to
the oracle weights. Our proposed algorithm  Sticky Track-and-Stop  which we display in Algorithm 1 
uses a conﬁdence region around the current estimate ˆµt to determine what i ∈ I can be the oracle
answer for µ. It selects one of these answers according to an arbitrary total order on I and does not
change it (sticks to it) until no point in the conﬁdence region has the chosen answer in its set of oracle
answers.

µ(cid:48)∈Ct

Compute It =(cid:83)

Algorithm 1 Sticky Track-and-Stop.
Input: δ > 0  strict total order on I. Set t = 1   ˆµ0 = 0   N0 = 0 .
while not stopped do
Let Ct = {µ(cid:48) ∈ M : D(Nt−1  ˆµt−1  µ(cid:48)) ≤ log(f (t − 1))} .
iF (µ(cid:48)) .
Pick the ﬁrst alternative it ∈ It in the order on I.
Compute wt ∈ w∗( ˆµt−1 ¬it).
Pull an arm at according to the C-tracking rule and receive Xt ∼ νat .
Set Nt = Nt−1 + eat and ˆµt = ˆµt−1 + 1
Nt at
Let Dt = {µ(cid:48) ∈ M : D(Nt  ˆµt  µ(cid:48)) ≤ β(t  δ)} .
if there exists i ∈ I such that Dt ∩ ¬i = ∅ then
end
t ← t + 1 .

(Xt − ˆµt−1 at)eat .

end

Theorem 10. For β(t  δ) = log(Ct2/δ)  with C such that C ≥ e(cid:80)+∞

stop and return i.

Sticky Track-and-Stop is δ-correct.

// small conf. reg.

// large conf. reg.

t=1 ( e

K )K (log2(Ct2) log(t))K

t2

 

6

That result is a consequence of Proposition 12 of [Garivier and Kaufmann  2016].

5.2 Sample Complexity
Theorem 11. Sticky Track-and-Stop is asymptotically optimal  i.e. it veriﬁes for all µ ∈ M 

Eµ[τδ]
log(1/δ)

lim
δ→0

→ 1

D(µ)

.

Let iµ = min iF (µ) in the arbitrary order on answers. For ε  ξ > 0  we deﬁne C∗
value of D(w(cid:48)  µ(cid:48) ¬iµ) for w(cid:48) and µ(cid:48) in ε and ξ-neighbourhoods of w∗(µ) and µ.

ε ξ(µ)  the minimal

C∗
ε ξ(µ) =

inf

µ(cid:48):(cid:107)µ(cid:48)−µ(cid:107)∞≤ξ

w(cid:48):infw∈w∗ (µ ¬iµ) (cid:107)w(cid:48)−w(cid:107)∞≤3ε

D(w(cid:48)  µ(cid:48) ¬iµ) .

Our proof strategy is to show that under a concentration event deﬁned below  for t big enough 
( ˆµt  Nt/t) belongs to that (ξ  ε) neighbourhood of (µ  w∗(µ ¬iµ)). From that fact  we obtain
D(Nt  ˆµt ¬iµ) ≥ tC∗
ε ξ(µ). Furthermore  if the algorithm does not stop at stage t  we also get
an upper bound on D(Nt  ˆµt ¬iµ) from the stopping condition. We obtain an upper bound on
ε ξ(µ). By continuity of (w  µ) (cid:55)→ D(w  µ ¬iµ) (from
the stopping time  function of δ and C∗
Theorem 4)  we have limε→0 ξ→0 C∗

ε ξ(µ) = D(µ ¬iµ) = D(µ).

Two concentration events. Let ET =(cid:84)T

√

t=h(T ){µ ∈ Ct} be the event that the small conﬁdence
region contains the true parameter vector µ for t ≥ h(T ). The function h : N → R  positive 
increasing and going to +∞  makes sure that each event {µ ∈ Ct} appears in ﬁnitely many ET  
which will be essential in the concentration results. We will eventually use h(T ) =
In order to deﬁne the second event  we ﬁrst highlight a consequence of Theorem 4.
Corollary 12. For all ε > 0  for all µ ∈ M  for all i ∈ I  there exists ξ > 0 such that
(cid:107)µ(cid:48) − µ(cid:107)∞ ≤ ξ ⇒ ∀w(cid:48) ∈ w∗(µ(cid:48) ¬i) ∃w ∈ w∗(µ ¬i)  (cid:107)w(cid:48) − w(cid:107)∞ ≤ ε .
Let E(cid:48)
t=h(T ){(cid:107) ˆµt − µ(cid:107)∞ ≤ ξ} be the event that the empirical parameter vector is close to µ 
where ξ is chosen as in the previous corollary for i = iµ. The analysis of Sticky Track-and-Stop
T and E(cid:48)
consists of two parts: ﬁrst show that E c
c happen rarely enough to lead only to a ﬁnite term in
Eµ[τδ]; then show than under ET ∩ E(cid:48)
T there is an upper bound on τδ.
Lemma 13. Suppose that there exists T0 such that for T ≥ T0  ET ∩ E(cid:48)
+∞(cid:88)
Pµ(E(cid:48)

T ⊂ {τδ ≤ T}. Then

T =(cid:84)T

Eµ[τδ] ≤ T0 +

+∞(cid:88)

Pµ(E c

c) .

(1)

T .

T

T ) +

T

Proof. Since τδ is a non-negative integer-valued random variable  Eµ[τδ] =(cid:80)+∞

T =T0

T =T0

T ∪ E(cid:48)

For T ≥ T0  Pµ{τδ > T} ≤ Pµ(E c
The sums depending on the events ET and E(cid:48)
Lemma 14. For h(T ) =

c) ≤ Pµ(E c
T in (1) are ﬁnite for well chosen h(T ) and C(t).
Pµ(E(cid:48)

√
T and f (t) = exp(β(t  1/t5)) = Ct10 in the deﬁnition of the conﬁdence
Pµ(E c

region Ct  the sum(cid:80)+∞

T ) +(cid:80)+∞

T ) + Pµ(E(cid:48)

c) is ﬁnite.

c).

T =0

Pµ{τδ > T}.

T

T =T0

T

T =T0

T

The proof of the Lemma can be found in Appendix G.1. The remainder of the proof is concerned
with ﬁnding a suitable T0. First  we show that if ˆµt and Nt/t are in an (ξ  ε) neighbourhood of µ
and w∗(µ ¬iµ)  then such an upper bound T0 on τδ can be obtained.
Lemma 15. Let t1 be an integer and suppose that for all t ≥ t1  D(Nt  ˆµt ¬iµ) ≥ tC∗
Tβ = inf{t : t > β(t  δ)/C∗
Proof. Take t ≥ t1.
D(Nt  ˆµt ¬iµ)/C∗
then τδ ≤ t. We obtain that τδ ≤ max(t1  inf{t : t > β(t  δ)/C∗

ε ξ(µ)}. Then τδ ≤ max(t1  Tβ).
If τδ > t then by hypothesis and the stopping condition  t ≤
ε ξ(µ)

ε ξ(µ) . Conversely  for t ≥ t1  if t > β(t  δ)/C∗

ε ξ(µ) ≤ β(t  δ)/C∗

ε ξ(µ). Let

ε ξ(µ)}).

7

√

t + K 2 − 2K by Lemma 34 [Garivier and Kaufmann  2016  Lemma 7].

The oracle answer it becomes constant. Due to the forced exploration present in the C-tracking
procedure  the conﬁdence region Ct shrinks. After some time  when concentration holds  the set of
possible oracle answers It becomes constant over t and equal to iF (µ).
Lemma 16. If an algorithm guaranties that for all k ∈ [K] and all t ≥ 1  Nt k ≥ n(t) > 0
with limt→+∞ n(t)/ log(f (t)) = +∞  then there exists T∆ such that under the event ET   for
t ≥ max(h(T )  T∆)  It = iF (µ) and min It = iµ = min iF (µ).
Proof in Appendix G.4. Note that Lemma 16 depends only on the amount of forced exploration and
not on other details of the algorithm. Any algorithm using C-tracking veriﬁes the hypothesis with
n(t) =
Convergence to the neighbourhood of (µ  w∗(µ ¬iµ)). Once it = iµ  we fall back to tracking
points from a convex set of oracle weights. The estimate ˆµt and Nt/t both converge  to µ and to the
set w∗(µ ¬iµ). The Lemma below is proved in Appendix G.5.
Lemma 17. Let T∆ be deﬁned as in Lemma 16. For T such that h(T ) ≥ T∆  it holds that on ET ∩E(cid:48)
Sticky Track-and-Stop with C-Tracking veriﬁes
∀t ≥ h(T )  (cid:107) ˆµt − µ(cid:107)∞ ≤ ξ  
and ∀t ≥ 4

K 2
ε2 + 3
Remainder of the proof. Suppose that the event ET ∩E(cid:48)
T holds. Let T∆ be deﬁned as in Lemma 16
and T be such that h(T ) ≥ T∆. Let η(T ) = 4K 2/ε2 + 3h(T )/ε. For all t ≥ η(T ) we have
D(Nt  ˆµt ¬iµ) ≥ tC∗
ε ξ(µ) by Lemma 17. For h(T ) bigger than some Tη we have η(T ) ≤ T .
We suppose h(T ) ≥ max(T∆  Tη). We apply Lemma 15 with t1 = η(T ). We obtain that τδ ≤
max(η(T )  Tβ) ≤ max(T  Tβ). Conclusion: for T ≥ T0 = max(h−1(T∆)  h−1(Tη)  Tβ)  under
the concentration event  τδ ≤ T and we can apply Lemma 13.
ε ξ(µ). Taking ε → 0 (hence ξ → 0 as well)  we obtain
Note that limδ→0
C∗
D(µ) . We proved Theorem 11.
limδ→0

− w(cid:107)∞ ≤ 3ε .

Eµ[τδ]
log(1/δ) =

T0

log(1/δ) =

w∈w∗(µ ¬iµ)

h(T )

 

ε

inf

(cid:107) Nt
t

T

1

1

limε→0 C∗

ε ξ(µ) = 1

6 Conclusion

We characterized the complexity of multiple-answers pure exploration bandit problems  showing a
lower bound and exhibiting an algorithm with asymptotically matching sample complexity on all
such problems. That study could be extended in several interesting directions and we now list a few.
• The computational complexity of Track-and-Stop is an important issue: it would be desirable to
design a pure exploration algorithm with optimal sample complexity which does not need to solve a
min-max problem at each step. Furthermore  the same would need to be done for the sticky selection
of an answer for the multiple-answers setting.
• Both lower bounds and upper bounds in this paper are asymptotic. In the upper bound case  only
the forced exploration rounds are considered when evaluating the convergence of ˆµt to µ  giving rise
to potentially sub-optimal lower order terms. A ﬁnite time analysis with reasonably small o(log(1/δ))
terms for an optimal algorithm is desirable. In addition  while selecting one of the oracle answers
to stick to has no asymptotic cost  it could have a lower order effect on the sample complexity and
appear in a reﬁned lower bound.
• Current tools in the theory of Brownian motion are insufﬁcient to characterise the asymptotic
distribution of proportions induced by tracking  even for two arms. Without tracking the Arcsine law
arises  so this slightly more challenging problem holds the promise of similarly elegant results.
• Finally  the multiple answer pure exploration setting can be extended in various ways. Making
I continuous leads to regression problems. The parametric assumption that the arms are in one-
parameter exponential families could also be relaxed.

8

References
J.-Y. Audibert  S. Bubeck  and R. Munos. Best Arm Identiﬁcation in Multi-armed Bandits. In

Proceedings of the 23rd Conference on Learning Theory  2010.

P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning  47(2):235–256  2002.

D. A. Berry and B. Fristedt. Bandit Problems. Sequential allocation of experiments. Chapman and

Hall  1985.

D. Blackwell and M. A. Girshick. Theory of games and statistical decisions. Wiley  1954.

S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  New York  NY 

USA  2004. ISBN 0521833787.

S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit

problems. Fondations and Trends in Machine Learning  5(1):1–122  2012.

S. Bubeck  R. Munos  and G. Stoltz. Pure Exploration in Finitely Armed and Continuous Armed

Bandits. Theoretical Computer Science 412  1832-1852  412:1832–1852  2011.

R. M. Castro. Adaptive sensing performance lower bounds for sparse signal detection and support

estimation. Bernoulli  20(4):2217–2246  11 2014.

L. Chen  A. Gupta  J. Li  M. Qiao  and R. Wang. Nearly optimal sampling algorithms for combinatorial
pure exploration. In Proceedings of the 2017 Conference on Learning Theory  volume 65 of
Proceedings of Machine Learning Research  pages 482–534. PMLR  July 2017.

S. Chen  T. Lin  I. King  M. Lyu  and W. Chen. Combinatorial Pure Exploration of Multi-Armed

Bandits. In Advances in Neural Information Processing Systems  2014.

R. Combes  S. Magureanu  and A. Proutiere. Minimal exploration in structured stochastic bandits. In

Advances in Neural Information Processing Systems  pages 1763–1771  2017.

E. Even-Dar  S. Mannor  and Y. Mansour. PAC bounds for multi-armed bandit and Markov decision
processes. In Computational Learning Theory  15th Annual Conference on Computational Learning
Theory  COLT 2002  Sydney  Australia  July 8-10  2002  Proceedings  volume 2375 of Lecture
Notes in Computer Science  pages 255–270. Springer  2002. ISBN 3-540-43836-X.

E. Even-Dar  S. Mannor  and Y. Mansour. Action Elimination and Stopping Conditions for the Multi-
Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning Research  7:
1079–1105  2006.

E. A. Feinberg  P. O. Kasyanov  and M. Voorneveld. Berge’s maximum theorem for noncompact

image sets. Journal of Mathematical Analysis and Applications  413(2):1040–1046  2014.

V. Gabillon  M. Ghavamzadeh  and A. Lazaric. Best Arm Identiﬁcation: A Uniﬁed Approach to
Fixed Budget and Fixed Conﬁdence. In Advances in Neural Information Processing Systems  2012.

V. Gabillon  A. Lazaric  M. Ghavamzadeh  R. Ortner  and P. L. Bartlett. Improved learning complexity
in combinatorial pure exploration bandits. In Proceedings of the 19th International Conference on
Artiﬁcial Intelligence and Statistics  AISTATS 2016  Cadiz  Spain  May 9-11  2016  volume 51 of
JMLR Workshop and Conference Proceedings  pages 1004–1012. JMLR.org  2016.

A. Garivier and E. Kaufmann. Optimal best arm identiﬁcation with ﬁxed conﬁdence. In Proceedings

of the 29th Conference On Learning Theory (COLT)  2016.

A. Garivier and E. Kaufmann. Non-asymptotic sequential tests for overlapping hypotheses and

application to near optimal arm identiﬁcation in bandit models. In arXiv 1905.03495  2019.

A. Garivier  E. Kaufmann  and W. M. Koolen. Maximin action identiﬁcation: A new bandit framework
for games. In Proceedings of the 29th Annual Conference on Learning Theory (COLT)  pages
1028 – 1050  June 2016.

9

R. Huang  M. M. Ajallooeian  C. Szepesvári  and M. Müller. Structured best arm identiﬁcation with
ﬁxed conﬁdence. In International Conference on Algorithmic Learning Theory  ALT 2017  15-17
October 2017  Kyoto University  Kyoto  Japan  volume 76 of Proceedings of Machine Learning
Research  pages 593–616. PMLR  2017.

K. Jamieson  M. Malloy  R. Nowak  and S. Bubeck. lil’UCB: an Optimal Exploration Algorithm for

Multi-Armed Bandits. In Proceedings of the 27th Conference on Learning Theory  2014.

S. Kalyanakrishnan and P. Stone. Efﬁcient Selection in Multiple Bandit Arms: Theory and Practice.

In International Conference on Machine Learning (ICML)  2010.

S. Kalyanakrishnan  A. Tewari  P. Auer  and P. Stone. PAC subset selection in stochastic multi-armed

bandits. In International Conference on Machine Learning (ICML)  2012.

Z. Karnin  T. Koren  and O. Somekh. Almost optimal Exploration in multi-armed bandits.

International Conference on Machine Learning (ICML)  2013.

In

S. Katariya  B. Kveton  C. Szepesvári  C. Vernade  and Z. Wen. Stochastic rank-1 bandits. In
Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics  AISTATS
2017  20-22 April 2017  Fort Lauderdale  FL  USA  volume 54 of Proceedings of Machine Learning
Research  pages 392–401. PMLR  2017.

E. Kaufmann and S. Kalyanakrishnan.

Proceeding of the 26th Conference On Learning Theory.  2013.

Information complexity in bandit subset selection.

In

E. Kaufmann and W. M. Koolen. Monte-Carlo tree search by best arm identiﬁcation. In Advances in

Neural Information Processing Systems (NeurIPS) 30  pages 4904–4913  Dec. 2017.

E. Kaufmann  O. Cappé  and A. Garivier. On the Complexity of Best Arm Identiﬁcation in Multi-

Armed Bandit Models. Journal of Machine Learning Research  17(1):1–42  2016.

E. Kaufmann  W. M. Koolen  and A. Garivier. Sequential test for the lowest mean: From Thompson
to Murphy sampling. In Advances in Neural Information Processing Systems (NeurIPS) 31  pages
6333–6343. Curran Associates  Inc.  Dec. 2018.

T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied

Mathematics  6(1):4–22  1985.

A. Locatelli  M. Gutzeit  and A. Carpentier. An optimal algorithm for the thresholding bandit problem.
In Proceedings of the 33nd International Conference on Machine Learning  ICML 2016  New York
City  NY  USA  June 19-24  2016  volume 48 of JMLR Workshop and Conference Proceedings 
pages 1690–1698. JMLR.org  2016.

S. Magureanu  R. Combes  and A. Proutière. Lipschitz bandits: Regret lower bound and optimal
algorithms. In Proceedings of The 27th Conference on Learning Theory  COLT 2014  Barcelona 
Spain  June 13-15  2014  volume 35 of JMLR Workshop and Conference Proceedings  pages
975–999. JMLR.org  2014.

O. Maron and A. Moore. The Racing algorithm: Model selection for Lazy learners. Artiﬁcial

Intelligence Review  11(1-5):113–131  1997.

D. Russo. Simple Bayesian algorithms for best arm identiﬁcation. CoRR  abs/1602.08448  2016.

K. Teraoka  K. Hatano  and E. Takimoto. Efﬁcient sampling method for Monte Carlo tree search

problem. IEICE Transactions  97-D(3):392–398  2014.

Y. Zhou  J. Li  and J. Zhu. Identify the Nash equilibrium in static games with random payoffs. In
Proceedings of the 34th International Conference on Machine Learning  volume 70 of Proceedings
of Machine Learning Research  pages 4160–4169  International Convention Centre  Sydney 
Australia  Aug. 2017. PMLR.

10

,Rémy Degenne
Wouter Koolen