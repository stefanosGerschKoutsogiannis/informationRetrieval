2013,Bayesian inference for low rank spatiotemporal neural receptive fields,The receptive field (RF) of a sensory neuron describes how the   neuron integrates sensory stimuli over time and space. In typical   experiments with naturalistic or flickering spatiotemporal stimuli    RFs are very high-dimensional  due to the large number of   coefficients needed to specify an integration profile across time   and space.  Estimating these coefficients from small amounts of data   poses a variety of challenging statistical and computational   problems.  Here we address these challenges by developing Bayesian   reduced rank regression methods for RF estimation. This corresponds   to modeling the RF as a sum of several space-time separable (i.e.    rank-1) filters  which proves accurate even for neurons with   strongly oriented space-time RFs.  This approach substantially   reduces the number of parameters needed to specify the RF  from   1K-100K down to mere 100s in the examples we consider  and confers   substantial benefits in statistical power and computational   efficiency.  In particular  we introduce a novel prior over low-rank   RFs using the restriction of a matrix normal prior to the manifold   of low-rank matrices. We then use a localized'' prior over row and   column covariances to obtain sparse  smooth  localized estimates of   the spatial and temporal RF components.  We develop two methods for   inference in the resulting hierarchical model: (1) a fully Bayesian   method using blocked-Gibbs sampling; and (2) a fast  approximate   method that employs alternating coordinate ascent of the conditional   marginal likelihood.  We develop these methods under Gaussian and   Poisson noise models  and show that low-rank estimates substantially   outperform full rank estimates in accuracy and speed using neural   data from retina and V1.",Bayesian inference for low rank spatiotemporal

neural receptive ﬁelds

Mijung Park

Electrical and Computer Engineering

The University of Texas at Austin
mjpark@mail.utexas.edu

Jonathan W. Pillow

Center for Perceptual Systems

The University of Texas at Austin
pillow@mail.utexas.edu

Abstract

The receptive ﬁeld (RF) of a sensory neuron describes how the neuron integrates
sensory stimuli over time and space. In typical experiments with naturalistic or
ﬂickering spatiotemporal stimuli  RFs are very high-dimensional  due to the large
number of coefﬁcients needed to specify an integration proﬁle across time and
space. Estimating these coefﬁcients from small amounts of data poses a vari-
ety of challenging statistical and computational problems. Here we address these
challenges by developing Bayesian reduced rank regression methods for RF esti-
mation. This corresponds to modeling the RF as a sum of space-time separable
(i.e.  rank-1) ﬁlters. This approach substantially reduces the number of parameters
needed to specify the RF  from 1K-10K down to mere 100s in the examples we
consider  and confers substantial beneﬁts in statistical power and computational
efﬁciency. We introduce a novel prior over low-rank RFs using the restriction of
a matrix normal prior to the manifold of low-rank matrices  and use “localized”
row and column covariances to obtain sparse  smooth  localized estimates of the
spatial and temporal RF components. We develop two methods for inference in
the resulting hierarchical model: (1) a fully Bayesian method using blocked-Gibbs
sampling; and (2) a fast  approximate method that employs alternating ascent of
conditional marginal likelihoods. We develop these methods for Gaussian and
Poisson noise models  and show that low-rank estimates substantially outperform
full rank estimates using neural data from retina and V1.

1

Introduction

A neuron’s linear receptive ﬁeld (RF) is a ﬁlter that maps high-dimensional sensory stimuli to a
one-dimensional variable underlying the neuron’s spike rate. In white noise or reverse-correlation
experiments  the dimensionality of the RF is determined by the number of stimulus elements in
the spatiotemporal window inﬂuencing a neuron’s probability of spiking. For a stimulus movie with
nx×ny pixels per frame  the RF has nxnynt coefﬁcients  where nt is the (experimenter-determined)
number of movie frames in the neuron’s temporal integration window. In typical neurophysiology
experiments  this can result in RFs with hundreds to thousands of parameters  meaning we can think
of the RF as a vector in a very high dimensional space.
In high dimensional settings  traditional RF estimators like the whitened spike-triggered average
(STA) exhibit large errors  particularly with naturalistic or correlated stimuli. A substantial liter-
ature has therefore focused on methods for regularizing RF estimates to improve accuracy in the
face of limited experimental data. The Bayesian approach to regularization involves specifying a
prior distribution that assigns higher probability to RFs with particular kinds of structure. Popular
methods have involved priors to impose smallness  sparsity  smoothness  and localized structure in
RF coefﬁcients[1  2  3  4  5].

1

Here we develop a novel regularization method to exploit the fact that neural RFs can be modeled
as a low-rank matrices (or tensors). This approach is justiﬁed by the observation that RFs can be
well described by summing a small number of space-time separable ﬁlters [6  7  8  9]. Moreover 
it can substantially reduce the number of RF parameters: a rank p receptive ﬁeld in nxnynt di-
mensions requires only p(nxny + nt − 1) parameters  since a single space-time separable ﬁlter has
nxny spatial coefﬁcients and nt − 1 temporal coefﬁcients (i.e.  for a temporal unit vector). When
p (cid:28) min(nxny  nt)  as commonly occurs in experimental settings  this parametrization yields con-
siderable savings.
In the statistics literature  the problem of estimating a low-rank matrix of regression coefﬁcients is
known as reduced rank regression [10  11]. This problem has received considerable attention in
the econometrics literature  but Bayesian formulations have tended to focus on non-informative or
minimally informative priors [12]. Here we formulate a novel prior for reduced rank regression using
a restriction of the matrix normal distribution [13] to the manifold of low-rank matrices. This results
in a marginally Gaussian prior over RF coefﬁcients  which puts it on equal footing with “ridge” 
AR1  and other Gaussian priors. Moreover  under a linear-Gaussian response model  the posterior
over RF rows and columns are conditionally Gaussian  leading to fast and efﬁcient sampling-based
inference methods. We use a “localized” form for the row and and column covariances in the matrix
normal prior  which have hyperparameters governing smoothness and locality of RF components
in space and time [5]. In addition to fully Bayesian sampling-based inference  we develop a fast
approximate inference method using coordinate ascent of the conditional marginal likelihoods for
temporal (column) and spatial (row) hyperparameters. We apply this method under linear-Gaussian
and linear-nonlinear-Poisson encoding models  and show that the latter gives the best performance
on neural data.
The paper is organized as follows. In Sec. 2  we describe the low-rank RF model with localized
priors. In Sec. 3  we describe a fully Bayesian inference method using the blocked-Gibbs sampling
with interleaved Metroplis Hastings steps. In Sec. 4  we introduce a fast method for approximate
inference using conditional empirical Bayesian hyperparameter estimates. In Sec. 5  we extend our
estimator to the linear-nonlinear Poisson encoding model. Finally  in Sec. 6  we show applications
to simulated and real neural datasets from retina and V1.

2 Hierarchical low-rank receptive ﬁeld model

2.1 Response model (likelihood)

We begin by deﬁning two probabilistic encoding models that will provide likelihood functions for
RF inference. Let yi denote the number of spikes that occur in response to a (dt × dx) matrix stimu-
lus Xi  where dt and dx denote the number of temporal and spatial elements in the RF  respectively.
Let K denote the neuron’s (dt × dx) matrix receptive ﬁeld.
We will consider  ﬁrst  a linear Gaussian encoding model:

(1)
where xi = vec(Xi) and k = vec(K) denote the vectorized stimulus and vectorized RF  respec-
tively  γ is the variance of the response noise  and b is a bias term. Second  we will consider a
linear-nonlinear-Poisson (LNP) encoding model

i k + b  γ) 

yi|Xi ∼ N (x(cid:62)

(2)
where g denotes the nonlinearity. Examples of g include exponential and soft rectifying function 
log(exp(·) + 1)  both of which give rise to a concave log-likelihood [14].

i k + b)).

yi|Xi  ∼ Poiss(g(x(cid:62)

2.2 Prior for low rank receptive ﬁeld

We can represent an RF of rank p using the factorization
K = KtK(cid:62)
x  

(3)
where the columns of the matrix Kt ∈ Rdt×p contain temporal ﬁlters and the columns of the matrix
Kx ∈ Rdx×p contain spatial ﬁlters.

2

Z exp(cid:0)− 1

We deﬁne a prior over rank-p matrices using a restriction of the matrix normal distribution
MN (0  Cx  Ct). The prior can be written:
p(K|Ct  Cx) = 1

(4)
where the normalizer Z involves integration over the space of rank-p matrices  which has no known
closed-form expression. The prior is controlled by a “column” covariance matrix Ct ∈ Rdt×dt and
“row” covariance matrix Cx ∈ Rdx×dx  which govern the temporal and spatial RF components 
respectively.
If we express K in factorized form (eq. 3)  we can rewrite the prior

t K](cid:1)  

x K(cid:62)C−1

2Tr[C−1

p(K|Ct  Cx) = 1

Z exp

x C−1

x Kx)(K(cid:62)

t C−1

.

(5)

(cid:16) − 1
2Tr(cid:2)(K(cid:62)

t Kt)(cid:3)(cid:17)

This formulation makes it clear that we have conditionally Gaussian priors on Kt and Kx  that is:

kt|kx  Cx  Ct ∼ N (0  A−1
kx|kt  Ct  Cx ∼ N (0  A−1

x ⊗ Ct) 
t ⊗ Cx) 

t Kt.

x C−1

t C−1

x Kx and At = K(cid:62)

(6)
where ⊗ denotes Kronecker product  and kt = vec(Kt) ∈ Rpdt×1  kx = vec(Kx) ∈ Rpdx×1  and
where we deﬁne Ax = K(cid:62)
We deﬁne Ct and Cx have a parametric form controlled by hyperparameters θt and θx  respectively.
This form is adopted from the “automatic locality determination” (ALD) prior introduced in [5]. In
the ALD prior  the covariance matrix encodes the tendency for RFs to be localized in both space-time
and spatiotemporal frequency.
For the spatial covariance matrix Cx  the hyperparameters are θx = {ρ  µs  µf   Φs  Φf}  where ρ is
a scalar determining the overall scale of the covariance; µs and µf are length-D vectors specifying
the center location of the RF support in space and spatial frequency  respectively (where D is the
number of spatial dimensions  e.g.  “D=2” for standard 2D visual pixel stimuli). The positive deﬁnite
matrices Φs and Φf are D × D determine the size of the local region of RF support in space and
spatial frequency  respectively [15]. In the temporal covariance matrix Ct  the hyperparameters θt 
which are directly are analogous to θx  determine the localized RF structure in time and temporal
frequency.
Finally  we place a zero-mean Gaussian prior on the (scalar) bias term: b ∼ N (0  σ2
b ).

3 Posterior inference using Markov Chain Monte Carlo
For a complete dataset D = {X  y}  where X ∈ Rn×(dtdx) is a design matrix  and y is a vector of
responses  our goal is to infer the joint posterior over K and b 
p(D|K  b)p(K|θt  θx)p(b|σ2

p(K  b|D) ∝

(cid:90) (cid:90)

b )p(θt  θx  σ2

b dθtdθx.

b )dσ2

(7)

b   θt  γ  b  kt)  we then sample θx and kx similarly.

We develop an efﬁcient Markov chain Monte Carlo (MCMC) sampling method using blocked-Gibbs
sampling. Blocked-Gibbs sampling is possible since the closed-form conditional priors in eq. 6
and the Gaussian likelihood yields closed-form “conditional marginal likelihood” for θt|(kx  θx  D)
and θx|(kt  θt  D)  respectively1. The blocked-Gibbs ﬁrst samples (σ2
b   θt  γ) from the condi-
tional evidence and simultaneously sample kt from the conditional posterior. Given the samples
of (σ2
For sampling from the conditional evidence  we use the Metropolis Hastings (MH) algorithm to
sample the low dimensional space of hyperparameters. For sampling (b  kt) and kx  we use the
closed-form formula (will be introduced shortly) for the mean of the conditional posterior. The
details of our algorithm are as follows.
Step 1 Given (i-1)th samples of (kx  θx)  we draw ith samples (b  kt  θt  σ2
  γ(i)|k(i−1)
p(b(i)  k(i)
t
x
|θ(i)
  σ2
b

 D) 
1In this section and Sec.4  we ﬁx the likelihood to Gaussian (eq. 1). An extension to the Poisson likelihood

b   γ) from
 D)
  θ(i−1)
  γ(i)  k(i−1)

 D) = p(θ(i)

t
p(b(i)  k(i)
t

  γ(i)|k(i−1)

  θ(i−1)

  θ(i−1)

  θ(i)
t

  σ2
b

  σ2
b

(i)

x

x

x

x

(i)

x

(i)

t

model (eq. 2) will be described in Sec.5.

3

which is divided into two parts2:

• We sample (θt  σ2

b   γ) from the conditional posterior given by

(cid:90)
(cid:90)

p(θt  σ2

b   γ|kx  θx D) ∝ p(θt  σ2
∝ p(θt  σ2
t ]T   M(cid:48)

b   γ)

b   γ)

p(D|b  kt  kx  γ)p(b  kt|kx  θx  θt)dbdkt 
N (D|M(cid:48)

xwt  γI)N (wt|0  Cwt)dwt 

(8)

where wt is a vector of [b kT
x is concatenation of a vector of ones and the matrix
Mx  which is generated by projecting each stimulus Xi onto Kx and then stacking it in
each row  meaning that the i-th row of Mx is [vec(XiKx)](cid:62)  and Cwt is a block diagonal
x ⊗ Ct. Using the standard formula for a product of
matrix whose diagonal is σ2
two Gaussians  we obtain the closed form conditional evidence:
2 µ(cid:62)

b   γ  kx  θx) ≈

b and A−1

t µt − 1

p(D|θt  σ2

2γ y(cid:62)y

t Λ−1

(cid:104) 1

(cid:105)

exp

(9)

|2πΛt| 1
2|2πCwt| 1

2

2

|2πγI| 1

where the mean and covariance of conditional posterior over wt given kx are given by

µt = 1

γ ΛtM(cid:48)T

x y 

and Λt = (C−1

wt

+ 1

γ M(cid:48)T

x Mx)−1.

(10)

We use the MH algorithm to search over the low dimensional hyperparameter space  with
the conditional evidence (eq. 9) as the target distribution  under a uniform hyperprior on
(θt  σ2

b   γ).

• We sample (b  kt) from the conditional posterior given in eq. 10.

Step 2 Given the ith samples of (b  kt  θt  σ2

p(k(i)

x   θ(i)

x |b(i)  k(i)

t

  σ2
b

(i)

  θ(i)
t

which is divided into two parts:

  γ(i) D) = p(θ(i)

b   γ)  we draw ith samples (kx  θx) from
  γ(i) D) 
  θ(i)
  σ2
t
b

x |b(i)  k(i)
x |θ(i)
p(k(i)

  σ2
b
x   b(i)  k(i)

  θ(i)
t

(i)

(i)

t

t

  γ(i) D) 

• We sample θx from the conditional posterior given by

p(θx|b  kt  θt  σ2

b   γ D) ∝ p(θx)
∝ p(θx)

p(D|b  kt  kx  γ)p(kx|kt  θt  θx)dkx 
N (D|Mtkx + b1  γI)N (kx|0  A−1

(11)
t ⊗ Cx)dkx 

(cid:90)
(cid:90)

where the matrix Mt is generated by projecting each stimulus Xi onto Kt and then stacking
it in each row  meaning that the i-th row of Mt is [vec([X(cid:62)
i Kt])](cid:62). Using the standard
(cid:105)
formula for a product of two Gaussians  we obtain the closed form conditional evidence:
2γ (y − b1)T (y − b1)

p(D|θx  kt  b) =

x µx − 1

x Λ−1

(cid:104) 1

2 µ(cid:62)

exp

 

|2πΛx| 1
2|2π(A−1

2

|2πγI| 1

t ⊗ Cx)| 1

2

where the mean and covariance of conditional posterior over kx given (b  kt) are given by

µx = 1

γ ΛxM(cid:62)

t (y − b1) 

and Λx = (At ⊗ C−1

x + 1

γ M(cid:62)

t Mt)−1.

(12)

As in Step 1  with a uniform hyperprior on θx  the conditional evidence is the target distri-
bution in the MH algorithm.

• We sample kx from the conditional posterior given in eq. 12.

A summary of this algorithm is given in Algorithm 1.

2We omit the sample index  the superscript i and (i-1)  for notational cleanness.

4

Algorithm 1 fully Bayesian low-rank RF inference using blocked-Gibbs sampling
Given data D  conditioned on samples for other variables  iterate the following:
b   θt  γ) from the conditional evidence for (θt  σ2

1. Sample for (b  kt  σ2

conditional posterior over (b  kt) (in eq. 10).

b   γ) (in eq. 8) and the

2. Sample for (kx  θx) from the conditional evidence for θx (in eq. 11) and the conditional

posterior over kx (in eq. 12).

Until convergence.

4 Approximate algorithm for fast posterior inference

Here we develop an alternative  approximate algorithm for fast posterior inference. Instead of in-
tegrating over hyperparameters  we attempt to ﬁnd point estimates that maximize the conditional
marginal likelihood. This resembles empirical Bayesian inference  where the hyperparameters are
set by maximizing the full marginal likelihood. In our model  the evidence has no closed form; how-
ever  the conditional evidence for (θt  σ2
b   γ) given (kx  θx) and the conditional evidence for θx given
b   γ) are given in closed form (in eq. 8 and eq. 11). Thus  we alternate (1) maximizing the
(b  kt  θt  σ2
b   γ) and ﬁnding the MAP estimates of (b  kt)  and (2) maximizing
conditional evidence to set (θt  σ2
the conditional evidence to set θx and ﬁnding the MAP estimates of kx  that is 

ˆθt  ˆγ  ˆσ2

b = arg max
b  γ
ˆb  ˆkt = arg max

θt σ2

b kt

ˆθx = arg max

θx

ˆkx = arg max

kx

b   γ  ˆkx  ˆθx) 

b   ˆkx  ˆθx D) 

p(D|θt  σ2
p(b  kt|ˆθt  ˆγ  ˆσ2
p(D|θx  ˆb  ˆkt  ˆθt  ˆγ  ˆσ2
b ) 
b  D).
p(kx|ˆθx  ˆb  ˆkt  ˆθt  ˆγ  ˆσ2

(13)

(14)

(15)

(16)

The approximate algorithm works well if the conditional evidence is tightly concentrated around its
maximum. Note that if the hyperparameters are ﬁxed  the iterative updates of (b  kt) and kx given
above amount to alternating coordinate ascent of the posterior over (b  K).

5 Extension to Poisson likelihood

When the likelihood is non-Gaussian  blocked-Gibbs sampling is not tractable  because we do not
have a closed form expression for conditional evidence. Here  we introduce a fast  approximate
inference algorithm for the low-rank RF model under the LNP likelihood. The basic steps are the
same as those in the approximate algorithm (Sec.4). However  we make a Gaussian approximation to
the conditional posterior over (b  kt) given kx via the Laplace approximation. We then approximate
the conditional evidence for (θt  σ2
b ) given kx at the posterior mode of (b  kt) given kx. The details
are as follows.
The conditional evidence for θt given kx is

p(D|θt  σ2

b   kx  θx) ∝

Poiss(y|g(M(cid:48)

xwt))N (wt|0  Cwt)dwt

(17)

(cid:90)

The integrand is proportional to the conditional posterior over wt given kx  which we approximate
to a Gaussian distribution via Laplace approximation

p(wt|θt  σ2

b   kx D) ≈ N ( ˆwt  Σt) 

(18)
where ˆwt is the conditional MAP estimate of wt obtained by numerically maximizing the log-
conditional posterior for wt (e.g.  using Newton’s method. See Appendix A) 
2 w(cid:62)

(19)
and Σt is the covariance of the conditional posterior obtained by the second derivative of the log-
conditional posterior around its mode Σ−1
  where the Hessian of the negative log-
likelihood is denoted by Ht = − ∂2

b   kx D) = y(cid:62) log(g(M(cid:48)

t = Ht + C−1

xwt)) − g(M(cid:48)

log p(wt|θt  σ2

log p(D|wt  M(cid:48)
x).

xwt) − 1

t C−1

wt

wt + c 

wt

∂w2
t

5

Figure 1: Simulated data. Data generated from the linear Gaussian response model with a rank-2 RF
(16 by 64 pixels: 1024 parameters for full-rank model; 160 for rank-2 model). A. True rank-2 RF
(left). Estimates obtained by ML  full-rank ALD  low-rank approximate method  and blocked-Gibbs
sampling  using 250 samples (top)  and using 2000 samples (bottom)  respectively. B. Average mean
squared error of the RF estimate by each method (average over 10 independent repetitions).

Under the Gaussian posterior (eq. 18)  the log conditional evidence (log of eq. 17) at the posterior
mode wt = ˆwt is simply
log p(D|θt  σ2

b   kx) ≈ log p(D| ˆwt  M(cid:48)

2 log |CwtΣ−1

ˆwt − 1

| 

x) − 1

2 ˆw(cid:62)

t C−1

wt

t

which we maximize to set θt and σ2
posterior for kx and the conditional evidence for θx given (b  kt). (See Appendix B).

b . Due to space limit  we omit the derivations for the conditional

6 Results

6.1 Simulations

We ﬁrst tested the performance of the blocked-Gibbs sampling and the fast approximate algorithm
on a simulated Gaussian neuron with a rank-2 RF of 16 temporal bins and 64 spatial pixels shown in
Fig. 1A. We compared these methods with the maximum likelihood estimate and the full-rank ALD
estimate. Fig. 1 shows that the low-rank RF estimates obtained by the blocked-Gibbs sampling
and the approximate algorithm perform similarly  and achieve lower mean squared error than the
full-rank RF estimates.

Figure 2: Simulated data. Data generated from the linear-nonlinear Poisson (LNP) response model
with a rank-2 RF (shown in Fig. 1A) and “softrect” nonlinearity. A. Estimates obtained by ML  full-
rank ALD  low-rank approximate method under the linear Gaussian model  and the methods under
the LNP model  using 250 (top) and 2000 (bottom) samples  respectively. B. Average mean squared
error of the RF estimate (from 10 independent repetitions). The low-rank RF estimates under the
LNP model perform better than those under the linear Gaussian model.

We then tested the performance of the above methods on a simulated linear-nonlinear Poisson (LNP)
neuron with the same RF and the softrect nonlinearity. We estimated the RF using each method
under the linear Gaussian model as well as under the LNP model. Fig. 2 shows that the low-rank RF

6

MSE# training data164116timespace    250samplesAB   2000samplesfull-ranktrue kMLlow-rank fast250500100020000.0030.010.112MLlow-rank Gibbsfull-ranklow-rank (Gibbs)low-rank (fast) MSE# training data    250samplesAB   2000samplesfull-rankMLlow-rank MLfull-rank low-rank 2505001000200000.511.52ML full-rank low-rank full-rankMLlow-rank linear GaussianLinear Nonlinear PoissonGaussianLNPFigure 3: Comparison of low-rank RF estimates for V1 simple cells (using white noise ﬂickering
bars stimuli [16]). A: Relative likelihood per test stimulus (left) and low-rank RF estimates for
three different ranks (right). Relative likelihood is the ratio of the test likelihood of rank-1 STA to
that of other estimates. Using 1 minutes of training data  the rank-2 RF estimates obtained by the
blocked-Gibbs sampling and the approximate method achieve the highest test likelihood (estimates
are shown in the top row)  while rank-1 STA achieves the highest test likelihood  since more noise is
added to the low-rank STA as the rank increases (estimates are shown in the bottom row). Relative
likelihood under full rank ALD is 2.25. B: Similar plot for another V1 simple cell. The rank-4
estimates obtained by the blocked-Gibbs sampling and the approximate method achieve the highest
test likelihood for this cell. Relative likelihood under full rank ALD is 2.17.

estimates perform better than full-rank estimates regardless of the model  and that the low-rank RF
estimates under the LNP model achieved the lowest MSE.

6.2 Application to neural data

We applied our methods to estimate the RFs of V1 simple cells and retinal ganglion cells (RGCs).
The details of data collection are described in [16  9]. We performed 10-fold cross-validation using
1 minute of training and 2 minutes of test data. In Fig. 3 and Fig. 4  we show the average test
likelihood as a function of RF rank under the linear Gaussian model. We also show the low-rank
RF estimates obtained by our methods as well as the low-rank STA. The low-rank STA (rank-p) is
i   where di is the i-th singular value  ui and vi are the i-th left
and right singular vectors  respectively. If the stimulus distribution is non-Gaussian  the low-rank
STA will have larger bias than the low-rank ALD estimate.

computed as ˆKST A p =(cid:80)p

i diuiv(cid:62)

Figure 4: Comparison of low-rank
RF estimates for retinal data (using
binary white noise stimuli [9]). The
RF consists of 10 by 10 spatial pixels
and 25 temporal bins (2500 RF coef-
ﬁcients). A: Relative likelihood per
test stimulus (left)  top three left sin-
gular vectors (middle) and right sin-
gular vectors (right) of estimated RF
for an off-RGC cell. The sampling-
based RF estimate beneﬁts from a
rank-3 representation  making use
of three distinct spatial and tempo-
ral components  whereas the perfor-
mance of the low-rank STA degrades
above rank 1. Relative likelihood
under full rank ALD is 1.0146. B:
Similar plot for on-RGC cell. Rel-
ative likelihood under full rank ALD
is 1.006. Both estimates perform best
with rank 1.

7

116124 low-rank (fast) timespaceABrank-1rank-2rank-4V1 simple cell #2V1 simple cell #1low-rank STA low-rank (fast) rank-1rank-2rank-4112space124timeper stimulusrelative likelihoodper stimulusrelative likelihoodlow-rank  (Gibbs)low-rank  (Gibbs)2.250.67124rank3low-rank STA2.500.67124rank30.91BRGC on-cell1240.91ARGC o(cid:31)-cell low-rank per stimulusranklow-rank STA1st2nd3rd 1st2nd3rdtemporal extent0253spatial extent low-rank (fast) low-rank STAspatial extent1101100251st2ndtemporal extent3rd110110 (fast)relative likelihoodper stimulusrelative likelihoodlow-rank (Gibbs)low-rank (Gibbs)124rank3Figure 5: RF estimates for a V1 simple cell. (Data from [16]). A: RF estimates obtained by ML
(left) and low-rank blocked-Gibbs sampling under the linear Gaussian model (middle)  and low-rank
approximate algorithm under the LNP model (right)  for two different amounts of training data (30
sec. and 2 min.). The RF consists of 16 temporal and 16 spatial dimensions (256 RF coefﬁcients).
B: Average prediction (on spike count) error across 10-subset of available data. The low-rank RF
estimates under the LNP model achieved the lowest prediction error among all other methods. C:
Runtime of each method. The low-rank approximate algorithms took less than 10 sec.  while the
full-rank inference methods took 10 to 100 times longer.

Finally  we applied our methods to estimate the RF of a V1 simple cell with four different amounts
of training data (0.25  0.5 1  and 2 minutes) and computed the prediction error of each estimate
under the linear Gaussian and the LNP models. In Fig. 5  we show the estimates using 30 sec. and 2
min. of training data. We computed the test likelihood of each estimate to set the RF rank and found
that the rank-2 RF estimates achieved the highest test likelihood. In terms of the average prediction
error  the low-rank RF estimates obtained by our fast approximate algorithm achieved the lowest
error  while the runtime of the algorithm was signiﬁcantly lower than full-rank inference methods.

7 Conclusion

We have described a new hierarchical model for low-rank RFs. We introduced a novel prior for
low-rank matrices based on a restricted matrix normal distribution  which has the feature of pre-
serving a marginally Gaussian prior over the regression coefﬁcients. We used a “localized” form to
deﬁne row and column covariance matrices in the matrix normal prior  which allows the model to
ﬂexibly learn smooth and sparse structure in RF spatial and temporal components. We developed
two inference methods: an exact one based on MCMC with blocked-Gibbs sampling and an approx-
imate one based on alternating evidence optimization. We applied the model to neural data using
both Gaussian and Poisson noise models  and found that the Poisson (or LNP) model performed
best despite the increased reliance on approximate inference. Overall  we found that low-rank esti-
mates achieved higher prediction accuracy with signiﬁcantly lower computation time compared to
full-rank estimates.
We believe our localized  low-rank RF model will be especially useful in high-dimensional settings 
particularly in cases where the stimulus covariance matrix does not ﬁt in memory. In future work  we
will develop fully Bayesian inference methods for low-rank RFs under the LNP noise model  which
will allow us to quantify the accuracy of our approximate method. Secondly  we will examine
methods for inferring the RF rank  so that the number of space-time separable components can be
determined automatically from the data.

Acknowledgments

We thank N. C. Rust and J. A. Movshon for V1 data  and E. J. Chichilnisky  J. Shlens  A. .M. Litke 
and A. Sher for retinal data. This work was supported by a Sloan Research Fellowship  McKnight
Scholar’s Award  and NSF CAREER Award IIS-1150186.

8

0.50.2512100101102103runtime (sec)# minutes of training dataprediction error# minutes of training data116116    rank-2      (LNP)    rank -2 (Gaussian)        ML(Gaussian)timespace30 sec.2 min.BAML full-rank rank-2(fast) full-rank rank-20.250.5120.180.20.220.24rank-2(Gibbs)CGaussianLNPReferences
[1] F. Theunissen  S. David  N. Singh  A. Hsu  W. Vinje  and J. Gallant. Estimating spatio-temporal receptive
ﬁelds of auditory and visual neurons from their responses to natural stimuli. Network: Computation in
Neural Systems  12:289–316  2001.

[2] D. Smyth  B. Willmore  G. Baker  I. Thompson  and D. Tolhurst. The receptive-ﬁeld organization of
simple cells in primary visual cortex of ferrets under natural scene stimulation. Journal of Neuroscience 
23:4746–4759  2003.

[3] M. Sahani and J. Linden. Evidence optimization techniques for estimating stimulus-response functions.

NIPS  15  2003.

[4] S.V. David and J.L. Gallant. Predicting neuronal responses during natural vision. Network: Computation

in Neural Systems  16(2):239–260  2005.

[5] M. Park and J. W. Pillow. Receptive ﬁeld inference with localized priors. PLoS Comput Biol 

7(10):e1002219  2011.

[6] Jennifer F. Linden  Robert C. Liu  Maneesh Sahani  Christoph E. Schreiner  and Michael M. Merzenich.
Spectrotemporal structure of receptive ﬁelds in areas ai and aaf of mouse auditory cortex. Journal of
Neurophysiology  90(4):2660–2675  2003.

[7] Anqi Qiu  Christoph E. Schreiner  and Monty A. Escab. Gabor analysis of auditory midbrain receptive

ﬁelds: Spectro-temporal and binaural composition. Journal of Neurophysiology  90(1):456–476  2003.

[8] J. W. Pillow and E. P. Simoncelli. Dimensionality reduction in neural models: An information-theoretic
generalization of spike-triggered average and covariance analysis. Journal of Vision  6(4):414–428  4
2006.

[9] J. W. Pillow  J. Shlens  L. Paninski  A. Sher  A. M. Litke  and E. P. Chichilnisky  E. J. Simoncelli. Spatio-
temporal correlations and visual signaling in a complete neuronal population. Nature  454:995–999 
2008.

[10] A.J. Izenman. Reduced-rank regression for the multivariate linear model. Journal of multivariate analysis 

5(2):248–264  1975.

[11] Gregory C Reinsel and Rajabather Palani Velu. Multivariate reduced-rank regression: theory and appli-

cations. Springer New York  1998.

[12] John Geweke. Bayesian reduced rank regression in econometrics. Journal of Econometrics  75(1):121 –

146  1996.

[13] A.P. Dawid. Some matrix-variate distribution theory: notational considerations and a bayesian applica-

tion. Biometrika  68(1):265  1981.

[14] L. Paninski. Maximum likelihood estimation of cascade point-process neural encoding models. Network:

Computation in Neural Systems  15:243–262  2004.

[15] M. Park and J. W. Pillow. Bayesian active learning with localized priors for fast receptive ﬁeld character-

ization. In NIPS  pages 2357–2365  2012.

[16] N. C. Rust  Schwartz O.  J. A. Movshon  and Simoncelli E.P. Spatiotemporal elements of macaque v1

receptive ﬁelds. Neuron  46(6):945–956  2005.

9

,Mijung Park
Jonathan Pillow
Pranjal Awasthi
Andrej Risteski