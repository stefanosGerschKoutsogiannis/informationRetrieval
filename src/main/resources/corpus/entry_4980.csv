2013,Sensor Selection in High-Dimensional Gaussian Trees with Nuisances,We consider the sensor selection problem on multivariate Gaussian distributions where only a \emph{subset} of latent variables is of inferential interest.  For pairs of vertices connected by a unique path in the graph  we show that there exist decompositions of nonlocal mutual information into local information measures that can be computed efficiently from the output of message passing algorithms.  We integrate these decompositions into a computationally efficient greedy selector where the computational expense of quantification can be distributed across nodes in the network.  Experimental results demonstrate the comparative efficiency of our algorithms for sensor selection in high-dimensional distributions. We additionally derive an online-computable performance bound based on augmentations of the relevant latent variable set that  when such a valid augmentation exists  is applicable for \emph{any} distribution with nuisances.,Sensor Selection in High-Dimensional

Gaussian Trees with Nuisances

Daniel Levine

MIT LIDS

dlevine@mit.edu

Jonathan P. How

MIT LIDS

jhow@mit.edu

Abstract

We consider the sensor selection problem on multivariate Gaussian distributions
where only a subset of latent variables is of inferential interest. For pairs of ver-
tices connected by a unique path in the graph  we show that there exist decom-
positions of nonlocal mutual information into local information measures that can
be computed efﬁciently from the output of message passing algorithms. We inte-
grate these decompositions into a computationally efﬁcient greedy selector where
the computational expense of quantiﬁcation can be distributed across nodes in the
network. Experimental results demonstrate the comparative efﬁciency of our al-
gorithms for sensor selection in high-dimensional distributions. We additionally
derive an online-computable performance bound based on augmentations of the
relevant latent variable set that  when such a valid augmentation exists  is applica-
ble for any distribution with nuisances.

1

Introduction

This paper addresses the problem of focused active inference: selecting a subset of observable ran-
dom variables that is maximally informative with respect to a speciﬁed subset of latent random
variables. The subset selection problem is motivated by the desire to reduce the overall cost of
inference while providing greater inferential accuracy. For example  in the context of sensor net-
works  control of the data acquisition process can lead to lower energy expenses in terms of sensing 
computation  and communication [1  2].
In many inferential problems  the objective is to reduce uncertainty in only a subset of the unknown
quantities  which are related to each other and to observations through a joint probability distribution
that includes auxiliary variables called nuisances. On their own  nuisances are not of any extrinsic
importance to the uncertainty reduction task and merely serve as intermediaries when describing
statistical relationships  as encoded with the joint distribution  between variables. The structure in
the joint can be represented parsimoniously with a probabilistic graphical model  often leading to
efﬁcient inference algorithms [3  4  5]. However  marginalization of nuisance variables is potentially
expensive and can mar the very sparsity of the graphical model that permitted efﬁcient inference.
Therefore  we seek methods for selecting informative subsets of observations in graphical models
that retain nuisance variables.
Two primary issues arise from the inclusion of nuisance variables in the problem. Observation
random variables and relevant latent variables may be nonadjacent in the graphical model due to
the interposition of nuisances between them  requiring the development of information measures
that extend beyond adjacency (alternatively  locality) in the graph. More generally  the absence of
certain conditional independencies  particularly between observations conditioned on the relevant
latent variable set  means that one cannot directly apply the performance bounds associated with
submodularity [6  7  8].

1

In an effort to pave the way for analyzing focused active inference on the class of general distribu-
tions  this paper speciﬁcally examines multivariate Gaussian distributions – which exhibit a number
of properties amenable to analysis – and later specializes to Gaussian trees. This paper presents
a decomposition of pairwise nonlocal mutual information (MI) measures on Gaussian graphs that
permits efﬁcient information valuation  e.g.  to be used in a greedy selection. Both the valuation
and subsequent selection may be distributed over nodes in the network  which can be of beneﬁt for
high-dimensional distributions and/or large-scale distributed sensor networks. It is also shown how
an augmentation to the relevant set can lead to an online-computable performance bound for general
distributions with nuisances.
The nonlocal MI decomposition extensively exploits properties of Gaussian distributions  Markov
random ﬁelds  and Gaussian belief propagation (GaBP)  which are reviewed in Section 2. The formal
problem statement of focused active inference is stated in Section 3  along with an example that
contrasts focused and unfocused selection. Section 4 presents pairwise nonlocal MI decompositions
for scalar and vectoral Gaussian Markov random ﬁelds. Section 5 shows how to integrate pairwise
nonlocal MI into a distributed greedy selection algorithm for the focused active inference problem;
this algorithm is benchmarked in Section 6. A performance bound applicable to any focused selector
is presented in Section 7.

2 Preliminaries

2.1 Markov Random Fields (MRFs)
Let G = (V E) be a Markov random ﬁeld (MRF) with vertex set V and edge set E. Let u and
v be vertices of the graph G. A u-v path is a ﬁnite sequence of adjacent vertices  starting with
vertex u and terminating at vertex v  that does not repeat any vertex. Let PG(u  v) denote the set
of all paths between distinct u and v in G. If |PG(u  v)| > 0  then u and v are graph connected. If
|PG(u  v)| = 1  then there is a unique path between u and v  and denote the sole element of PG(u  v)
by ¯Pu:v.
If |PG(u  v)| = 1 for all u  v ∈ V  then G is a tree. If |PG(u  v)| ≤ 1 for all u  v ∈ V  then G is a
forest  i.e.  a disjoint union of trees. A chain is a simple tree with diameter equal to the number of
nodes. A chain is said to be embedded in graph G if the nodes in the chain comprise a unique path
in G.
For MRFs  the global Markov property relates connectivity in the graph to implied conditional
independencies. If D ⊆ V  then GD = (D ED) is the subgraph induced by D  with ED = E ∩
(D × D). For disjoint subsets A  B  C ⊂ V  let G\B be the subgraph induced by V \ B. The global
Markov property holds that xA ⊥⊥ xC | xB iff |PG\B (i  j)| = 0 for all i ∈ A and j ∈ C.

2.2 Gaussian Distributions in Information Form
Consider a random vector x distributed according to a multivariate Gaussian distribution N (µ  Λ)
with mean µ and (symmetric  positive deﬁnite) covariance Λ > 0. One could equivalently consider
the information form x ∼ N −1(h  J) with precision matrix J = Λ−1 > 0 and potential vector
h = Jµ  for which px(x) ∝ exp{− 1
One can marginalize out or condition on a subset of random variables by considering a partition of
x into two subvectors  x1 and x2  such that

(cid:21)(cid:19)

(cid:20)J11 J12

JT
12 J22

 

.

2 xT Jx + hT x}.
(cid:19)

(cid:18)(cid:18)h1

∼ N −1

(cid:19)

h2

(cid:18)x1

x2

x =

22 h2 and J(cid:48)

In the information form  the marginal distribution over x1 is px1(·) = N −1(·; h(cid:48)
1)  where
1 = h1 − J12J−1
h(cid:48)
12  the latter being the Schur complement of
J22. Conditioning on a particular realization x2 of the random subvector x2 induces the conditional
distribution px1|x2 (x1|x2) = N −1(x1; h(cid:48)
1|2 = h1 − J12x2  and J11 is exactly the
upper-left block submatrix of J. (Note that the conditional precision matrix is independent of the
value of the realized x2.)

1 = J11 − J12J−1

1|2  J11)  where h(cid:48)

1  J(cid:48)

22 JT

2

If x ∼ N −1(h  J)  where h ∈ Rn and J ∈ Rn×n  then the (differential) entropy of x is [9]

H(x) = − 1
2

log ((2πe)n · det(J)) .

(1)

Likewise  for nonempty A ⊆ {1  . . .   n}  and (possibly empty) B ⊆ {1  . . .   n}\ A  let J(cid:48)
precision matrix parameterizing pxA|xB . The conditional entropy of xA ∈ Rd given xB is

A|B be the

H(xA|xB) = − 1
2
The mutual information between xA and xB is

log((2πe)d · det(J(cid:48)

A|B)).

(cid:32) det(J(cid:48)

(cid:33)

 

(2)

(3)

I(xA; xB) = H(xA) + H(xB) − H(xA  xB) =

1
2

log

det(J(cid:48)

{A B})
A) det(J(cid:48)
B)

which generally requires O(n3) operations to compute via Schur complement.

2.3 Gaussian MRFs (GMRFs)
If x ∼ N −1(h  J)  the conditional independence structure of px(·) can be represented with a Gaus-
sian MRF (GMRF) G = (V E)  where E is determined by the sparsity pattern of J and the pairwise
Markov property: {i  j} ∈ E iff Jij (cid:54)= 0.
In a scalar GMRF  V indexes scalar components of x.
In a vectoral GMRF  V indexes disjoint
subvectors of x  each of potentially different dimension. The block submatrix Jii can be thought of
as specifying the sparsity pattern of the scalar micro-network within the vectoral macro-node i ∈ V.

2.4 Gaussian Belief Propagation (GaBP)

If x can be partitioned into n subvectors of dimension at most d  and the resulting graph is tree-
i  i ∈ V can be computed by Gaussian belief propa-
shaped  then all marginal precision matrices J(cid:48)
gation (GaBP) [10] in O(n · d3). For such trees  one can also compute all edge marginal precision
matrices J(cid:48)
In light of (3)  pairwise MI quantities between adjacent nodes i and j may be expressed as

{i j}  {i  j} ∈ E  with the same asymptotic complexity of O(n · d3).

I(xi; xj) = H(xi) + H(xj) − H(xi  xj) 
ln det(J(cid:48)

ln det(J(cid:48)

i) − 1
2

= − 1
2

j) +

ln det(J(cid:48)

{i j}) 

1
2

{i  j} ∈ E 

(4)

i.e.  purely in terms of node and edge marginal precision matrices. Thus  GaBP provides a way of
computing all local pairwise MI quantities in O(n · d3).
Note that Gaussian trees comprise an important class of distributions that subsumes Gaussian hidden
Markov models (HMMs)  and GaBP on trees is a generalization of the Kalman ﬁltering/smoothing
algorithms that operate on HMMs. Moreover  the graphical inference community appears to best un-
derstand the convergence of message passing algorithms for continuous distributions on subclasses
of multivariate Gaussians (e.g.  tree-shaped [10]  walk-summable [11]  and feedback-separable [12]
models  among others).

3 Problem Statement
Let px(·) = N −1(·; h  J) be represented by GMRF G = (V E)  and consider a partition of V into
the subsets of latent nodes U and observable nodes S  with R ⊆ U denoting the subset of relevant
latent variables (i.e.  those to be inferred). Given a cost function c : 2S → R≥0 over subsets of
observations  and a budget β ∈ R≥0  the focused active inference problem is

maximizeA⊆S
s.t.

I(xR; xA)
c(A) ≤ β.

(5)

3

The focused active inference problem in (5) is distinguished from the unfocused active inference
problem

maximizeA⊆S
s.t.

I(xU ; xA)
c(A) ≤ β 

(6)
which considers the entirety of the latent state U ⊇ R to be of interest. Both problems are known to
be NP-hard [13  14].
By the chain rule and nonnegativity of MI  I(xU ; xA) = I(xR; xA) + I(xU\R; xA | xR) ≥
I(xR; xA)  for any A ⊆ S. Therefore  maximizing unfocused MI does not imply maximizing
focused MI. Focused active inference must be posed as a separate problem to avoid the situa-
tion where the observation selector becomes ﬁxated on inferring nuisance variables as a result of
I(xU\R; xA | xR) being included implicitly in the valuation. In fact  an unfocused selector can
perform arbitrarily poorly with respect to a focused metric  as the following example illustrates.

Example 1. Consider a scalar GMRF over a four-node chain (Figure 1a)  whereby J13 = J14 =
J24 = 0 by the pairwise Markov property  with R = {2}  S = {1  4}  c(A) = |A| (i.e.  unit-cost ob-
servations)  and β = 1. The optimal unfocused decision rule A∗
(U F ) = argmaxa∈{1 4} I(x2  x3; xa)
can be shown  by conditional independence and positive deﬁniteness of J  to reduce to

|J34|

A∗
(U F )={4}

(cid:82)

(U F )={1}
A∗

|J12| 

independent of J23  which parameterizes the edge potential between nodes 2 and 3. Conversely  the
optimal focused decision rule A∗

(F ) = argmaxa∈{1 4} I(x2; xa) can be shown to be

|J23| · 1{J 2

34−J 2

34−J 2

12J 2

12≥0}

A∗
(F )={4}

(cid:82)

A∗
(F )={1}

(1 − J 2
J 2
34

34)J 2
12

 

(cid:115)

where 1{·} is the indicator function  which evaluates to 1 when its argument is true and 0 otherwise.
The loss associated with optimizing the “wrong” information measure is demonstrated in Figure 1b.
The reason for this loss is that as |J23| → 0+  the information that node 3 can convey about node 2
also approaches zero  although the unfocused decision rule is oblivious to this fact.

x1

x2

x3

x4

(a) Graphical model.

(b) Policy comparison.

Figure 1: (a) Graphical model for the four-node chain example. (b) Unfocused vs. focused policy
comparison. There exists a range of values for |J23| such that the unfocused and focused policies
coincide; however  as |J23| → 0+  the unfocused policy approaches complete performance loss with
respect to the focused measure.

4

00.10.20.30.40.50.600.10.20.30.40.50.60.70.80.91|J23|I(xR;xA)  [nats]Score vs. |J23| (with J212 = 0.3  J234 = 0.5)  Unfocused PolicyFocused Policy1

˜G1

2

. . .

k

˜G2

˜Gk
(a) Unique path with sidegraphs.

(b) Vectoral graph with thin edges.

Figure 2: (a) Example of a nontree graph G with a unique path ¯P1:k between nodes 1 and k. The
“sidegraph” attached to each node i ∈ ¯P1:k is labeled as ˜Gi. (b) Example of a vectoral graph with
thin edges  with internal (scalar) structure depicted.

4 Nonlocal MI Decomposition

1

For GMRFs with n nodes indexing d-dimensional random subvectors  I(xR; xA) can be computed
exactly in O((nd)3) via Schur complements/inversions on the precision matrix J. However  cer-
tain graph structures permit the computation via belief propagation of all local pairwise MI terms
I(xi; xj)  for adjacent nodes i  j ∈ V in O(n · d3) – a substantial savings for large networks. This
section describes a transformation of nonlocal MI between uniquely path-connected nodes that per-
mits a decomposition into the sum of transformed local MI quantities  i.e.  those relating adjacent
nodes in the graph. Furthermore  the local MI terms can be transformed in constant time  yielding
an O(n · d3) for computing any pairwise nonlocal MI quantity coinciding with a unique path.
Deﬁnition 1 (Warped MI). For disjoint subsets A  B  C ⊆ V 
the warped mutual
mation measure W : 2V × 2V × 2V → (−∞  0]
2 log (1 − exp{−2I(xA; xB|xC)}).
For convenience  let W (i; j|C) (cid:44) W ({i};{j}|C) for i  j ∈ V.
Remark 2. For i  j ∈ V indexing scalar nodes  the warped MI of Deﬁnition 1 reduces to W (i; j) =
log |ρij|  where ρij ∈ [−1  1] is the correlation coefﬁcient between scalar r.v.s xi and xj. The
measure log |ρij| has long been known to the graphical model learning community as an “additive
tree distance” [15  16]  and our decomposition for vectoral graphs is a novel application for sensor
selection problems. To the best of the authors’ knowledge  the only other distribution class with
established additive distances are tree-shaped symmetric discrete distributions [16]  which require a
very limiting parameterization of the potentials functions deﬁned over edges in the factorization of
the joint distribution.
Proposition 3 (Scalar Nonlocal MI Decomposition). For any GMRF G = (V E) where V indexes
scalar random variables  if |PG(u  v)| = 1 for distinct vertices u  v ∈ V  then for any C ⊆ V \
{u  v}  I(xu; xv|xC) can be decomposed as

infor-
is deﬁned such that W (A; B|C) (cid:44)

(cid:88)

W (u; v|C) =

W (i; j|C) 

(7)

{i j}∈ ¯Eu:v

where ¯Eu:v is the set of edges joining consecutive nodes of ¯Pu:v  the unique path between u and v
and sole element of PG(u  v).
(Proofs of this and subsequent propositions can be found in the supplementary material.)
Remark 4. Proposition 3 requires only that the path between vertices u and v be unique. If G is
a tree  this is obviously satisﬁed. However  the result holds on any graph for which: the subgraph
induced by ¯Pu:v is a chain; and every i ∈ ¯Pu:v separates N (i) \ ¯Pu:v from ¯Pu:v \ {i}  where
N (i) (cid:44) {j : {i  j} ∈ E} is the neighbor set of i. See Figure 2a for an example of a nontree graph
with a unique path.
Deﬁnition 5 (Thin Edges). An edge {i  j} ∈ E of GMRF G = (V E; J) is thin if the corresponding
submatrix Jij has exactly one nonzero scalar component. (See Figure 2b.)
For vectoral problems  each node may contain a subnetwork of arbitrarily connected scalar random
variables (see Figure 2b). Under the assumption of thin edges (Deﬁnition 5)  a unique path between
nodes u and v must enter interstitial nodes through one scalar r.v. and leave through one scalar

5

r.v. Therefore  let ζi(u  v|C) ∈ (−∞  0] denote the warped MI between the enter and exit r.v.s of
interstitial vectoral node i on ¯Pu:v  with conditioning set C ⊆ V \ {u  v}.1 Note that ζi(u  v|C) can
be computed online in O(d3) via local marginalization given J(cid:48)
Proposition 6 (Vectoral Nonlocal MI Decomposition). For any GMRF G = (V E) where V indexes
random vectors of dimension at most d and the edges in E are thin  if |PG(u  v)| = 1 for distinct
vertices u  v ∈ V  then for any C ⊆ V \ {u  v}  I(xu; xv|xC) can be decomposed as

i|C  which is an output of GaBP.

(cid:88)

W (u; v|C) =

W (i; j|C) +

{i j}∈ ¯Eu:v

5

(Distributed) Focused Greedy Selection

(cid:88)

ζi(u  v|C).

i∈ ¯Pu:v\{u v}

(8)

The nonlocal MI decompositions of Section 4 can be used to efﬁciently solve the focused greedy
selection problem  which at each iteration  given the subset A ⊂ S of previously selected observable
random variables  is

argmax

{y∈S\A : c(y)≤β−c(A)}

I(xR; xy | xA).

To proceed  ﬁrst consider the singleton case R = {r} for r ∈ U. Running GaBP on the graph
G conditioned on A and subsequently computing all terms W (i; j|A) ∀{i  j} ∈ E incurs a com-
putational cost of O(n · d3). Once GaBP has converged  node r authors an “r-message” with the
value 0. Each neighbor i ∈ N (r) receives that message with value modiﬁed by W (r; i|A); there
is no ζ term because there are no interstitial nodes between r and its neighbors. Subsequently 
each i ∈ N (r) messages its neighbors j ∈ N (i) \ {r}  modifying the value of its r-message by
W (i; j|A) + ζi(r  j|A)  the latter term being computed online in O(d3) from J(cid:48)
i|A  itself an output
of GaBP.2 Then j messages N (j) \ {i}  and so on down to the leaves of the tree. Since there are at
most n−1 edges in a forest  the total cost of dissemination is still O(n·d3)  after which all nodes y in
the same component as r will have received an r-message whose value on arrival is W (r; y|A)  from
which I(xr; xy|A) can be computed in constant time. Thus  for |R| = 1  all scores I(xR; xy|xA)
for y ∈ S \ A can collectively be computed at each iteration of the greedy algorithm in O(n · d3).
Now consider |R| > 1. Let R = (r1  . . .   r|R|) be an ordering of the elements of R  and let Rk
(cid:80)|R|
be the ﬁrst k elements of R. Then  by the chain rule of mutual information  I(xR; xy | xA) =
k=1 I(xrk ; xy | xA∪Rk−1)  y ∈ S \ A  where each term in the sum is a pairwise (potentially
nonlocal) MI evaluation. The implication is that one can run |R| separate instances of GaBP  each
using a different conditioning set A ∪ Rk−1  to compute “node and edge weights” (W and ζ terms)
cost of a greedy update is then O(cid:0)|R| · nd3(cid:1).
for the r-message passing scheme outlined above. The chain rule suggests one should then sum the
unwarped r-scores of these |R| instances to yield the scores I(xR; xy|xA) for y ∈ S \ A. The total

One of the beneﬁts of the focused greedy selection algorithm is its amenability to parallelization.
All quantities needed to form the W and ζ terms are derived from GaBP  which is parallelizable and
guaranteed to converge on trees in at most diam(G) iterations [10]. Parallelization reallocates the
expense of quantiﬁcation across networked computational resources  often leading to faster solution
times and enabling larger problem instantiations than are otherwise permissible. However  full paral-
lelization  wherein each node i ∈ V is viewed as separate computing resource  incurs a multiplicative
overhead of O(diam(G)) due to each i having to send |N (i)| messages diam(G) times  yielding lo-
cal communication costs of O(diam(G)|N (i)|·d3) and overall complexity of O(diam(G)·|R|·nd3).
This overhead can be alleviated by instead assigning to every computational resource a connected
subgraph of G.

1As node i may have additional neighbors that are not on the u-v path  using the notation ζi(u  v|C) is
a convenient way to implicitly specify the enter/exit scalar r.v.s associated with the path. Any unique path
subsuming u-v  or any unique path subsumed in u-v for which i is interstitial  will have equivalent ζi terms.
2If i is in the conditioning set  its outgoing message can be set to be −∞  so that the nodes it blocks
from reaching r see an apparent information score of 0. Alternatively  i could simply choose not to transmit
r-messages to its neighbors.

6

It should also be noted that if the quantiﬁcation is instead performed using serial BP – which can be
conceptualized as choosing an arbitrary root  collecting messages from the leaves up to the root  and
disseminating messages back down again – a factor of 2 savings can be achieved for R2  . . .   R|R|
by noting that in moving between instances k and k + 1  only rk is added to the conditioning set.
Therefore  by reassigning rk as the root for the BP instance associated with rk+1 (i.e.  A ∪ Rk as
the conditioning set)  only the second half of the message passing schedule (disseminating messages
from the root to the leaves) is necessary. We subsequently refer to this trick as “caching.”

6 Experiments

To benchmark the runtime performance of the algorithm in Section 5  we implemented its serial
GaBP variant in Java  with and without the caching trick described above.
We compare our algorithm with greedy selectors that use matrix inversion (with cubic complexity)
to compute nonlocal mutual information measures. Let Sfeas := {y ∈ S \ A : c(y) ≤ β −
c(A)}. At each iteration of the greedy selector  the blocked inversion-based quantiﬁer computes ﬁrst
R|A∪y ∀y ∈
J(cid:48)
R∪Sfeas|A (entailing a block marginalization of nuisances)  from which J(cid:48)
Sfeas  are computed. Then I(xR; xy | xA) ∀y ∈ Sfeas  are computed via a variant of (3). The na¨ıve
inversion-based quantiﬁer computes I(xR; xy | xA) ∀y ∈ Sfeas  “from scratch” by using separate
Schur complements of J submatrices and not storing intermediate results. The inversion-based
quantiﬁers were implemented in Java using the Colt sparse matrix libraries [17].

R|A and J(cid:48)

Figure 3: Performance of GaBP-based and inversion-based quantiﬁers used in greedy selectors.
For each n  the mean of the runtimes over 20 random scalar problem instances is displayed. Our
BP-Quant algorithm of Section 5 empirically has approximately linear complexity; caching re-
duces the mean runtime by a factor of approximately 2.

Figure 3 shows the comparative mean runtime performance of each of the quantiﬁers for scalar
networks of size n  where the mean is taken over the 20 problem instances proposed for each value
of n. Each problem instance consists of a randomly generated  symmetric  positive-deﬁnite  tree-
shaped precision matrix J  along with a randomly labeled S (such that  arbitrarily  |S| = 0.3|V|)
and R (such that |R| = 5)  as well as randomly selected budget and heterogeneous costs deﬁned
over S. Note that all selectors return the same greedy selection; we are concerned with how the
decompositions proposed in this paper aid in the computational performance. In the ﬁgure  it is
clear that the GaBP-based quantiﬁcation algorithms of Section 5 vastly outperform both inversion-
based methods; for relatively small n  the solution times for the inversion-based methods became
prohibitively long. Conversely  the behavior of the BP-based quantiﬁers empirically conﬁrms the
asymptotic O(n) complexity of our method for scalar networks.

7

0200400600800100012001400160018002000101102103104105106n (Network Size)Mean Runtime [ms]Greedy Selection Total Runtimes for Quantification Algorithms  BP−Quant−CacheBP−Quant−NoCacheInv−Quant−BlockInv−Quant−Naive7 Performance Bounds
Due to the presence of nuisances in the model  even if the subgraph induced by S is completely
disconnected  it is not always the case that the nodes in S are conditionally independent when
conditioned on only the relevant latent set R. Lack of conditional independence means one cannot
guarantee submodularity of the information measure  as per [6]. Our approach will be to augment
R such that submodularity is guaranteed and relate the performance bound to this augmented set.
Let ˆR be any subset such that R ⊂ ˆR ⊆ U and such that nodes in S are conditionally independent
conditioned on ˆR. Then  by Corollary 4 of [6]  I(x ˆR; xA) is submodular and nondecreasing on S.
Additionally  for the case of unit-cost observations (i.e.  c(A) = |A| for all A ⊆ S)  a greedily
selected subset Ag

β( ˆR) of cardinality β satisﬁes the performance bound
I( ˆR;Ag

max

β( ˆR)) ≥

{A⊆S:|A|≤β} I( ˆR;A)
{A⊆S:|A|≤β}[I(R;A) + I( ˆR \ R;A|R)]
{A⊆S:|A|≤β} I(R;A) 

max

max

(9)

(10)

(11)

where (9) is due to [6]  (10) to the chain rule of MI  and (11) to the nonnegativity of MI. The
following proposition follows immediately from (11).
Proposition 7. For any set ˆR such that R ⊂ ˆR ⊆ U and nodes in S are conditionally independent
conditioned on ˆR  provided I( ˆR;Ag
β( ˆR)) > 0  an online-computable performance bound for any
¯A ⊆ S in the original focused problem with relevant set R and unit-cost observations is

I(R; ¯A) ≥

1 − 1
e

{A⊆S:|A|≤β}I(R;A).

max

(12)

(cid:19)
(cid:19)
(cid:19)

(cid:18)
(cid:18)
(cid:18)

1 − 1
e
1 − 1
e
1 − 1
e

=

≥

(cid:34)
(cid:124)

(cid:35)(cid:18)

I(R; ¯A)

I( ˆR;Ag

(cid:123)(cid:122)
β( ˆR))
(cid:44) δR( ¯A  ˆR)

(cid:19)
(cid:125)

Proposition 7 can be used at runtime to determine what percentage δR( ¯A  ˆR) of the optimal ob-
jective is guaranteed  for any focused selector  despite the lack of conditional independence of S
conditioned on R. In order to compute the bound  a greedy heuristic running on a separate  surro-
gate problem with ˆR as the relevant set is required. Finding an ˆR ⊃ R providing the tightest bound
is an area of future research.

8 Conclusion

In this paper  we have considered the sensor selection problem on multivariate Gaussian distributions
that  in order to preserve a parsimonious representation  contain nuisances. For pairs of nodes con-
nected in the graph by a unique path  there exist decompositions of nonlocal mutual information into
local MI measures that can be computed efﬁciently from the output of message passing algorithms.
For tree-shaped models  we have presented a greedy selector where the computational expense of
quantiﬁcation can be distributed across nodes in the network. Despite deﬁciency in conditional in-
dependence of observations  we have derived an online-computable performance bound based on
an augmentation of the relevant set. Future work will consider extensions of the MI decomposition
to graphs with nonunique paths and/or non-Gaussian distributions  as well as extend the analysis of
augmented relevant sets to derive tighter performance bounds.

Acknowledgments

The authors thank John W. Fisher III  Myung Jin Choi  and Matthew Johnson for helpful discussions
during the preparation of this paper. This work was supported by DARPA Mathematics of Sensing 
Exploitation and Execution (MSEE).

8

References
[1] C. M. Kreucher  A. O. Hero  and K. D. Kastella. An information-based approach to sensor
management in large dynamic networks. Proc. IEEE  Special Issue on Modeling  Identiﬁcia-
tion  & Control of Large-Scale Dynamical Systems  95(5):978–999  May 2007.

[2] H.-L. Choi and J. P. How. Continuous trajectory planning of mobile sensors for informative

forecasting. Automatica  46(8):1266–1275  2010.

[3] V. Chandrasekaran  N. Srebro  and P. Harsha. Complexity of inference in graphical models. In

Proc. Uncertainty in Artiﬁcial Intelligence  2008.

[4] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT

Press  2009.

[5] F. R. Kschischang  B. J. Frey  and H.-A. Loeliger. Factor graphs and the sum-product algo-

rithm. IEEE Transactions on Information Theory  47(2):498–519  Feb 2001.

[6] A. Krause and C. Guestrin. Near-optimal nonmyopic value of information in graphical models.

In Proc. Uncertainty in Artiﬁcial Intelligence (UAI)  2005.

[7] G. Nemhauser  L. Wolsey  and M. Fisher. An analysis of approximations for maximizing

submodular set functions. Mathematical Programming  14:489–498  1978.

[8] J. L. Williams  J. W. Fisher III  and A. S. Willsky. Performance guarantees for information
In M. Meila and X. Shen  editors  Proc. Eleventh Int. Conf. on

theoretic active inference.
Artiﬁcial Intelligence and Statistics  pages 616–623  2007.

[9] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley  2nd ed. edition  2006.
[10] Y. Weiss and W. T. Freeman. Correctness of belief propagation in Gaussian graphical models

of arbitrary topology. Neural Computation  13(10):2173–2200  2001.

[11] D. M. Malioutov  J. K. Johnson  and A. S. Willsky. Walk-sums and belief propagation in

Gaussian graphical models. Journal of Machine Learning Research  7:2031–2064  2006.

[12] Y. Liu  V. Chandrasekaran  A. Anandkumar  and A. S. Willsky. Feedback message passing for
inference in gaussian graphical models. IEEE Transactions on Signal Processing  60(8):4135–
4150  Aug 2012.

[13] C. Ko  J. Lee  and M. Queyranne. An exact algorithm for maximum entropy sampling. Oper-

ations Research  43:684–691  1995.

[14] A. Krause and C. Guestrin. Optimal value of information in graphical models. Journal of

Artiﬁcial Intelligence Research  35:557–591  2009.

[15] P. L. Erd˝os  M. A. Steel  L. A. Sz´ekely  and T. J. Warnow. A few logs sufﬁce to build (almost)

all trees: Part ii. Theoretical Computer Science  221:77–118  1999.

[16] M. J. Choi  V. Y. F. Tan  A. Anandkumar  and A. S. Willsky. Learning latent tree graphical

models. Journal of Machine Learning Research  12:1771–1812  May 2011.

[17] CERN - European Organization for Nuclear Research. Colt  1999.

9

,Daniel Levine
Jonathan How