2012,Volume Regularization for Binary Classification,We introduce a large-volume box classification for binary   prediction  which maintains a subset of weight vectors  and   specifically axis-aligned boxes. Our learning algorithm seeks for a   box of large volume that contains ``simple'' weight vectors which   most of are accurate on the training set. Two versions of the   learning process are cast as convex optimization problems  and it   is shown how to solve them efficiently.  The formulation yields a   natural PAC-Bayesian performance bound and it is shown to minimize a   quantity directly aligned with it. The algorithm outperforms SVM and   the recently proposed AROW algorithm on a majority of $30$ NLP   datasets and binarized USPS optical character recognition datasets.,Volume Regularization for Binary Classiﬁcation

Koby Crammer

Department of Electrical Enginering

The Technion - Israel Institute of Technology

Haifa  32000 Israel

koby@ee.technion.ac.il

Tal Wagner∗

Faculty of Mathematics and Computer Science

Weizmann Institute of Science

Rehovot  76100  Israel

tal.wagner@gmail.com

Abstract

We introduce a large-volume box classiﬁcation for binary prediction  which main-
tains a subset of weight vectors  and speciﬁcally axis-aligned boxes. Our learning
algorithm seeks for a box of large volume that contains “simple” weight vectors
which most of are accurate on the training set. Two versions of the learning pro-
cess are cast as convex optimization problems  and it is shown how to solve them
efﬁciently. The formulation yields a natural PAC-Bayesian performance bound
and it is shown to minimize a quantity directly aligned with it. The algorithm out-
performs SVM and the recently proposed AROW algorithm on a majority of 30
NLP datasets and binarized USPS optical character recognition datasets.

1

Introduction

Linear models are widely used for a variety of tasks including classiﬁcation and regression. Support
vectors machines [3  22] (SVMs) are considered a primary method to efﬁciently build linear clas-
siﬁers from data  yielding state-of-the-art performance. SVMs and many other methods are often
easy to implement and efﬁcient  yet return only a single weight-vector with no additional informa-
tion about alternative models nor about conﬁdence in prediction.
An alternative approach is taken by Bayesian methods [21  13]. The primary object is a (posterior)
distribution over models that is updated using Bayes rule. Unfortunately  the posterior is very com-
plicated even for simple models  such as Bayesian logistic regression [15]  and it is not known how
to perform the update analytically  and approximations are required.
In this work we integrate the advantages of both approaches. We propose to model uncertainty
over weight-vectors by maintaining a (simple) set of possible weight-vectors  rather than a single
weight-vector. Learning is motivated from principles of discriminative learning rather than Bayes’
rule  and it is optimizing a combination of an hand-crafted regularization term and the empirical
loss. Speciﬁcally  our algorithm maintains an axis-aligned box  which only requires double number
of parameters than maintaing a single weight-vector  a dominating model for many tasks.
We use a similar conceptual reasoning as used in Bayes point machines (BPM) [13]. Both ap-
proaches maintain a set of possible weights  which can be thought of as a posterior. BPMs use the
version space  the set of all consistent weight vectors  which is a convex polyhedron. Since the size
of the polyhedron’s representation grows with the number of training examples  BPMs approximate
the polyhedron with a single weight-vector  the Bayes point. Our algorithms model the set as a box 
with a representation that is ﬁxed in the size of the input  and ﬁnd an optimal prediction box.
We cast learning as a convex optimization problem and propose methods to solve it efﬁciently. We
further provide generalization bounds using PAC-Bayesian theory  and show that our algorithm is

∗The research was performed while TW was a student at the Technion.

1

minimizing a quantity directly related to the generalization bound. We give two formulations or
versions of the algorithm  one that is closely related to the bound  while the other is smooth.
We experiment with 30 binary text categorization datasets from various tasks: sentiment classiﬁca-
tion  predicting domain of product-review  assigning topics to news items  tagging spam emails  and
classifying posts to news-groups. The results indicate that our algorithms outperform SVM and the
recently proposed AROW [4] algorithm  which was shown to be the state-of-the-art in numerous
NLP tasks. Additional support for the superiority and robustness of our algorithms  especially in
high-noise setting  is provided using experiments with 45 pairs of binarized USPS OCR problems.
Notation: Given a vector x ∈ Rd  we denote its kth element by xk ∈ R  and by |x| ∈ Rd the
vector with component-wise absolute value of its elements  |x| = (|x1|  . . .  |xd|).

2 Large-Volume Box Classiﬁers
Standard linear classiﬁcation learning algorithms maintain and return a single weight vector w(cid:63) ∈
Rd used to predict the label of any test point. We study a generalization of these algorithms where
hypotheses are uncertainty (sub)sets of weight vectors w. Such a hypothesis can be seen as a ran-
domized linear classiﬁer or a voting process. To classify an instance x  a parameter vector w is
drawn according to the hypothesis and predicts the label sign(w · x). Herbrich et.al. [13  12]  ar-
gued in a similar context that such a randomization yields a more robust solution. PAC-Bayesian
analysis and its generalization bounds give additional justiﬁcation to this approach (see Sec 5).
The uncertainty subsets we study are axis aligned boxes parametrized with two vectors u  v ∈
Rd where we assume  uk ≤ vk for all k = 1 . . . d.
In words  u is the vertex with the lowest
coordinates  and v is the vertex with the largest coordinates. The projection of the box onto the
k-axis yields the interval [uk  vk]. The set of weight vectors contained in the box is denoted by 
Q = {w : uk ≤ wk ≤ vk for k = 1 . . . d} . Given an instance x to be classiﬁed  a Gibbs classiﬁer
samples a weight vector uniformly in random from the box w ∈ Q and returns sign(w · x). A
deterministic alternative we use in practice is to employ the center of mass deﬁned by µ = 1
2 (u + v)
and return sign(µ · x). For linear classiﬁers  the majority prediction with Gibbs sampling coincides
2 (v − u).
with predicting using the center of mass. We also deﬁne the uncertainty intervals σ = 1
Intuitively  the uncertainty in the weight associated with the kth feature is σk. Clearly  v = µ + σ
and u = µ − σ.

3 Learning as Optimization
Given a labeled sample S = {(xi  yi)}n
i=1  a common practice in learning linear models w is to
perform structural risk minimization (SRM) [25] that picks a weight-vector that is both “simple” (eg
small norm) and performs well on the training set. Learning is cast as an optimization problem 

(cid:88)

i

w(cid:63) = arg min
w

1
n

(cid:96)(w  (xi  yi)) + D R(w) .

(1)

The ﬁrst term is the empirical loss evaluated on the training set with some loss function (cid:96)(w  (x  y)) 
and the second term is a regularization that penalizes weight-vectors according to their complexity.
The parameter D > 0 is a tradeoff parameter.
Learning with uncertainty sets invites us to balance three desires rather than two as when learning
a single weight-vector. The ﬁrst two desires are generalizations of the structural risk minimiza-
tion principle [25] to boxes: we prefer boxes containing weight-vectors that attain both low loss
(cid:96)(w  (xi  yi)) and are “simple” (eg small norm). This alone though is not enough  as if the loss and
regularization functions are strictly convex then the optimal box would in fact be a single weight-
vector. The third desire is thus to prefer boxes with large volume. Intuitively  if during training
an algorithm ﬁnds a box with large volume  such that all weight-vectors belonging to it attain low
training error and are simple  we expect the classiﬁer based on the center of mass to be robust to
noise or ﬂuctuations. This will be formally stated in the analysis described in Sec 5. We formalize
this requirement by adding a term that is inversely proportional to the volume of the box Q.
We take a worst-case approach  and deﬁne the loss of the box Q given an example (x  y) denoted
by (cid:96)(Q  (x  y)) to be the loss of the worst member w ∈ Q. Similarly  we deﬁne the complexity
of the box Q to be the complexity of the most complex member of the box w ∈ Q  formally 
(cid:96)(Q  (x  y)) = supw∈Q (cid:96)(w  (x  y)) and R(Q) = supw∈Q R(w).

2

Putting it all together  we replace (1) with 

(cid:32)

(cid:33)

 

(cid:96)(w  (xi  yi)) + DR(w)

(cid:88)

i

1
m

(2)

Q(cid:63) = arg minQ∈Q

sup
w∈Q

where Q is a set of boxes with some minimal volume. In other words  the algorithm is seeking for
a set of alternative weight-vectors all of which are performing well on the training data. We expect
this formulation to be robust  as a box is evaluated with its worst performing member.
We modify the problem by removing the constraint Q ∈ Q and adding an equivalent penalty term to
the objective  namely the log-volume of the box. We use the log-volume function for three reasons.
First  it is a common barrier function in optimization [26]  and in our case it keeps the box from
actually shrinking to a zero volume box. Second  this choice is supported by the analysis below 
and third  it is additive in the dimension of the data d  like all other quantities of the objective.
Additionally  we bound the supremum over w with a sum of supremum operators. To conclude  we
cast the learning problem as the following optimization problem over boxes 
(cid:96)(w  (xi  yi)) − C log volQ + D sup
w∈Q

(cid:88)

R(w)  

arg minQ

sup
w∈Q

1
m

(3)

i

where C  D > 0 are two trade-off parameters used to balance the three goals.
(In the analysis
below it will be shown that D can be also interpreted as a Langrange multiplier of a constrained
optimization problem.) We further develop the last equation by making additional assumptions over
the loss function and the regularization. We assume that the loss is a monotonically decreasing
function of the product y(x(cid:62)w)  often called the margin (or the signed margin). This is a property
of many popular loss functions for binary classiﬁcation  including the hinge-loss and its square used
by SVMs [3  22]  exp-loss used by boosting [9]  logistic-regression [11] and the Huber-loss [14].
Under this assumption we compute analytically the ﬁrst term of the objective (3).

(cid:96)(y(cid:0)x(cid:62)w(cid:1)) then supw∈Q (cid:96)(w  (xi  yi)) = (cid:96)(y(x(cid:62)µ) − |x|σ).

the loss function is monotonically decreasing in the margin  (cid:96)(w  (x  y)) =

From the monotonicity of (cid:96)(·) we have supw∈Q (cid:96)(cid:0)y(x(cid:62)w)(cid:1) = (cid:96)(cid:0)inf w∈Q y(x(cid:62)w)(cid:1).

Lemma 1 If

Proof:
Computing the inﬁmum we get 
w∈Q y(x(cid:62)w) =
d(cid:88)

(cid:26) uk

inf

inf

=

(yxk)

wk∈[uk vk] for k=1...d

(yxk) ≥ 0
(yxk) < 0

vk

d(cid:88)

k=1

d(cid:88)

k=1

d(cid:88)

k=1

(yxk)wk =

inf

wk∈[uk vk]

(yxk)wk

=

(yxk) (µk − sign(yxk)σk) = y(x(cid:62)µ) − |x|σ  

k=1

using u = µ − σ and v = u + σ as stated above.
The lemma highlights the need to constrain the volume to be strictly larger than zero: due to mono-
tonicity and the fact that σ ≥ 0 (component wise) we have (cid:96)(y(x(cid:62)µ) − |x|σ) ≥ (cid:96)(y(x(cid:62)µ))  so
the loss is always minimized when we set σ = 0. We next turn to analyse the third term of (3) with
the following lemma.
Lemma 2 (1) Assuming R(w) is convex  then supw∈Q R(w) is attained on vertices of the box Q.
(2) Additionally  if R(w) is strictly convex then the supremum is attained only on vertices.
non-negative elements and(cid:80)
Proof: We use the fact that every point in the box can be represented as a convex combination
box. Convexity of R(·) yields  R(w) ≤ (cid:80)
of the vertices. Formally  given a point in the box w ∈ Q  there exists a vector α ∈ R2d with
t αtzt where zt are the vertices of the
t αtR(zt) ≤ maxt {R(zt)} . Thus  if w attains the
supremum supw∈Q R(w) then so does at least one vertex. Additionally  if R(w) is a strictly convex
function  then the ﬁrst inequality in the last equation is a strict inequality  and thus a non-vertex
(cid:80)
cannot attain the supremum.
Common regularization functions are deﬁned as sums over individual features  that is R(w) =

t αt = 1 such that w = (cid:80)

k r(wk). In this case the supremum is attained on each coordinate independently as follows.

3

Corollary 3 Assuming R(w) is a sum of scalar-convex functions(cid:80)

k r(wk)  we have 

max{r(uk)  r(vk)}

=

max{r(µk − σk)  r(µk + σk)} .

k

k

(cid:88)

R(w) =

sup
w∈Q

(cid:88)

The corollary follows from the lemma since a supremum of a scalar-function over a box is equivalent
to taking the supremum over the box projected to a single coordinate. Finally  the volume of a box
k (2σk) =

is given by a product of the length of its axes  that is  vol (Q) = (cid:81)
2d(cid:81)

k (vk − uk) = (cid:81)

k σk .

To summarize  the learning problem of the large-volume box algorithm is cast by solving the fol-
lowing minimization problem  in terms of the center µ and the size (or dimensions) σ 

m(cid:88)

(cid:96)(cid:0)yi(x(cid:62)

i µ) − |xi|σ(cid:1) −C

(cid:88)

(cid:88)

1
m

max{r(µk − σk)  r(µk + σk)}   (4)
min
σ≥0 µ
where (cid:96)(·) is a monotonically decreasing function  r(·) is a convex function  and C  D > 0 are two
trade-off parameters used to balance our three desires. We denote by

log σk +D

i=1

k

k

(cid:96)

2

(cid:18) 1

1
min
v≥u
m
− C

zi + = yixi + |xi| ∈ Rd   zi − = yixi − |xi| ∈ Rd .

(cid:0)v(cid:62)(zi −) + u(cid:62)(zi +)(cid:1)(cid:19)

(5)
The kth element of zi + (zi −) is twice the kth element of |xi| if the sign of the kth element of xi
agrees (disagrees) with yi  and zero otherwise.
This problem can equivalently be written in terms of the two “extreme” vertices u and v as follows 

m(cid:88)
(cid:88)
i (v+u)−|xi|(v−u) = v(cid:62)(zi −)+u(cid:62)(zi +) . Note  if the loss function (cid:96)(·)
by using the relation yix(cid:62)
is convex  then both formulations (4) and (6) of the learning problem are convex in their arguments 
as each is a sum of convex functions of linear combination of the arguments  and a maximum of
convex functions is convex.
(cid:80)
We conclude this section with an additional alternative formulation  which for convenience  we
it with a smooth term  by changing the max to a sum  yielding(cid:80)
present in the notation of (6). Although the above problem is convex  the regularization term
k max{r(vk)  r(uk)} is not smooth because of the max operator. In this alternative  we replace
k r(vk) + r(uk) = R(u) +R(v).

max{r(vk)  r(uk)}  

log (vk − uk)+D

(cid:88)

(6)

i=1

k

k

The problem then becomes 

m(cid:88)

(cid:18) 1

(cid:0)v(cid:62)(zi −) + u(cid:62)(zi +)(cid:1)(cid:19)

(cid:88)

1
m

− C

log (vk − uk) + D (R(u) + R(v)) .

(cid:96)

2

i=1

min
v≥u
The two alternatives are related via the following chain of inequalities  0.5 max{r(vk)  r(uk)} ≤
0.5 (r(vk) + r(uk)) ≤ max{r(vk)  r(uk)} ≤ r(vk) + r(uk) . In other words  given either one
of the problems (6) or (7)  we can lower and upper bound it with the other problem with a proper
choice of trade-off parameter D. We call the two versions BoW for box-of-weights algorithm  and
refer to them as BoW-M(ax) and BoW-S(um)  respectively.

k

(7)

4 Optimization Algorithm

We now present an algorithm to solve (6) for the special case r(x) = x2. The algorithm is a based
on COMID [8] and its convergence analysis follows directly from the analysis of COMID  which
is omitted due to lack of space. The algorithm works in iterations. On each iteration a (stochastic)
gradient decent step is performed  followed by a regularization-optimization step. Formally  the
algorithm picks a random example i and updates 

(cid:17)

for α = (cid:96)(cid:48)(cid:18) 1

(cid:0)v(cid:62)(zi −) + u(cid:62)(zi +)(cid:1)(cid:19)

.

(˜u  ˜v) ← (u  v) − α

η
2

zi +   zi −

(cid:16)

2

4

The algorithm then solves the following regularization-oriented optimization problem 

min
u v

1
2

(cid:107)u − ˜u(cid:107)2 +

1
2

(cid:107)v − ˜v(cid:107)2 − C

(cid:88)

k

(cid:88)

log (vk − uk) + D

max(cid:8)v2

(cid:9) .
(v − ˜v)2 − C log (v − u) + D max(cid:8)v2  u2(cid:9) .

k  u2
k

k

The objective of the last problem decomposes over individual pairs uk  vk so we reduce the opti-
mization to d independent problems  each deﬁned over 2 scalars u and v (omitting index k) 

(u − ˜u)2 +

1
2

1
2

min
u v

F (u  v) =

(8)
We denote the half-plane H = {(u  v) ∈ R2 : v > u} and partition it into three subsets: G1 =
{(u  v) ∈ H : v > −u}  G2 = {(u  v) ∈ H : v < −u}  and the line L = {(u  v) ∈ R2 : v = −u}.
The following lemma describes the optimal solution of (8).

Lemma 4 Exactly one of the items below holds and describe the optimal solution of (8).

1. If there exists (u  v) ∈ G1 such that v is a root of f (v) = αv2 + βv + γ and u =
˜u − 2Dv + (˜v − v) where α = 2(1 + D)(1 + 2D)  β = −˜u(1 + 2D) − ˜v(3 + 4D)  and
γ = ˜v + ˜u˜v − C  then it is a global minimum of F .

2. If there exists (u  v) ∈ G2 such that u is a root of f (u) = αu2 + βu + γ and v =
˜v − 2Du + (˜u − u) where α = 2(1 + D)(1 + 2D)  β = −˜v(1 + 2D) − ˜u(3 + 4D) and
γ = ˜u + ˜v˜u − C  then it is a global minimum of F . Furthermore  such point and a point
described in 1 cannot exist simultaneously.

3. If no points as described in 1 nor 2 exist  then the global minimum of F is (u −u) such

that u is a root of f (u) = αu2 + βu + C where α = 2 + 2D  β = ˜v − ˜u  γ = −C.

F(cid:12)(cid:12)G1

Proof sketch: By deﬁnition  the function F is smooth and convex on G1. The condition in 1 is
equivalent to satisfying ∇F (u  v) = 0  and therefore any point that satisﬁes it  is a minimum of
. A similar argument applies to G2 with 2. The convexity of F on the entire set H yields
that any such point is also a global minimum of F   and that if no such point exists then F attains
a global minimum on L (which is derived in 3). The latter is sure to exist since limv→0 F|L =
limv→∞ F|L = ∞. The algebraic derivation is omitted due to lack of space.
Similarly  we develop the update for solving (7). Here after the gradient step we need to solve the
2 (v − ˜v)2 − C log (v − u) +
following problem per coordinate k  minu v F (u  v) = 1

D(cid:0)v2 + u2(cid:1) . The following lemma characterizes the optimal solution.

2 (u − ˜u)2 + 1

Lemma 5 The optimal solution (u  v) ∈ {(u  v) ∈ R2 : v − u > 0} of the last problem is such
that u is a root of the polynomial f (u) = αu2 + βu + γ where α = 2 + 2D + 6D + 8D2 β =
−(˜v + 2D˜v + ˜u + 6D˜u)− 2˜u γ = ˜u2 + ˜u˜v− 4C − 2CD and v = (˜v + ˜u − u(1 + 2D)) /(1 + 2D).

Its proof is similar to the proof of Lemma 4  but simpler and omitted due to lack of space.

5 Analysis

PAC-Bayesian bounds were introduced by McAllester [19]  were further reﬁned later (e.g. [17  23]) 
and applied to analyze SVMs [18]. They often have been shown to be quite tight.
We ﬁrst introduce some notation needed for the discussion of these bounds. Let ¯(cid:96)(w  (x  y)) denote
the zero-one loss  that is ¯(cid:96)(w  (x  y)) = 1 if sign(w· x) (cid:54)= y and ¯(cid:96)(w  (x  y)) = 0 otherwise. Let D
be a distribution over the labeled examples (x  y)  and denote by ¯(cid:96)(w D) the expected zero-one loss
of a linear classiﬁer characterized by its weight vector w: ¯(cid:96)(w D) = Pr(x y)∼D[sign(w·x) (cid:54)= y] =
E(x y)∼D[¯(cid:96)(w  (x  y))] . We abuse notation  and denote by ¯(cid:96)(w  S) the expected loss ¯(cid:96)(w DS) for
the empirical distribution DS of a sample S.
PAC-Bayesian analysis states generalization bounds in terms of two distributions - prior and pos-
terior - over all hypotheses (i.e. over weight-vectors w). Below  we identify a compact set with a
uniform distribution over the set  and in particular we identify a box Q with a uniform distribution

5

over all weight vectors it contains (and zero mass otherwise). Similarly  we identify any compact
body P with a uniform distribution over its elements. In other words  we refer to the prior P and
the posterior Q both as two uniform distributions and as their support (which are subsets). We
also denote by (cid:96)(Q D) the expectation of (cid:96)(w D) over weight vectors w drawn according to the
distribution Q. We quote Cor. 2.2 of Germain et.al. [10] 
Corollary 6 ([10]) : For any distribution D  for any set H of weight-vectors  for any distribution
P of support H  for any δ ∈ (0  1]  and any positive number γ the following statement holds with
probability ≥ 1 − δ over samples S of size n 

(cid:40)

(cid:34)

(cid:32)

¯(cid:96)(Q D) ≤

1

1 − e−γ

1 − exp

−

γ · ¯(cid:96)(Q  S) +

DKL (Q(cid:107)P) +

1
n

1
n

ln

(cid:19)(cid:33)(cid:35)(cid:41)

(cid:18) 1

δ

.

(9)

The corollary states that the expected number of mistakes over examples drawn according to some
ﬁxed and unknown distribution D over inputs  and over weight-vectors drawn from the box Q uni-
formly  is bounded by the right term  which is a monotonic function of the following sum 

1
nγ
For uniform distributions we have the following 

¯(cid:96)(Q  S) +

DKL (Q(cid:107)P) .

Additionally  we bound the empirical training error 

¯(cid:96)(Q  S) =

1
n

1
volQ

¯(cid:96)(w  (xi  yi)) dw ≤ 1
n

w∈Q

(cid:96)

i

DKL (Q(cid:107)P) =

(cid:90)

n(cid:88)

i

(cid:26) log vol(P)

∞

otherwise

vol(Q) Q ⊆ P
(cid:88)

(10)

(11)

 

(12)

.

(cid:18)
w∈Q yi(x(cid:62)

inf

i w)

(cid:19)

where the equality is the deﬁnition of ¯(cid:96)(Q  S)  and the inequality follows by choosing a loss function
(cid:96)(·) which upper bounds the zero-one loss (e.g. Hinge loss)  by bounding an expectation with the
supremum value  and from Lemma 1.
We get that to minimize the generalization bound of (9) we can minimize a bound on (10) which is
obtained by substituting (11) and (12) in (10). Omitting constants we get 

(cid:18)
w∈Q yiw(cid:62)xi

inf

(cid:19)

(cid:96)

(cid:88)

i

minQ

1
n

− 1
nγ

log volQ s.t. Q ⊆ P .

(13)

k max{v2

k} ≤ R2 .

k  u2

Next  we set P to be a ball of radius R about the origin  and  as in Sec 2  we set Q as a box
parametrized with the vectors u and v. We use the following lemma  of which proof is omitted due
to lack of space 
Lemma 7 If P is a ball of radius R about the origin and Q is a box parametrized using u and v 

we have Q ⊆ P ⇔(cid:80)
i=1 (cid:96)(cid:0) 1
(cid:80)m
(cid:80)
(cid:80)
of
k log (vk − uk)
k max{r(vk)  r(uk)} ≤ R2 . To solve the last problem we write its Lagrangian 
(cid:88)

(cid:0)v(cid:62) (zi −) + u(cid:62) (zi +)(cid:1)(cid:1) − 1
(cid:0)v(cid:62)(zi −) + u(cid:62)(zi +)(cid:1)(cid:19)
(cid:18) 1
m(cid:88)
(cid:88)

plugging Lemma 7 and Lemma 1 in (13) we get
the

is monotonically

the
generalization

following prob-
loss 
to

Finally 
lem  which
1
minv≥u
n

2
max{r(vk)  r(uk)} − ηR2  

k

(14)

log (vk − uk)

− 1
nγ

subject

max

η

min
v≥u

related

+ η

to

a

bound

1
n

i=1

nγ

(cid:96)

2

k

where η is the Lagrange multiplier ensuring the constraint. Comparing (14)  whose objective is used
in the PAC-Bayesian bound  and our learning algorithm in (6)  we observe that the three terms in
both objectives are the same by setting C = 1
nγ and identifying the optimal value of the Lagrange

6

Figure 1: Fraction of error on text classiﬁcation datasets of BoW-M and BoW-S vs SVM (two left plots); and
BoW-M and BoW-S vs AROW (two right plots). Markers above the line indicate superior BoW performance.

multipler with the trade-off constant η = D. In fact  each value of the radius R yields a unique
optimal value of the Lagrange multiplier η. Thus  we can interpret the role of the constant D as
setting implicitly the effective radius of the prior ball P.
Few comments are in order. First  the KL-divergence between distributions is minimized more
effectively if both P and Q are of the same form  e.g. both P and Q are boxes. However  we chose
Q to be a box  as it has a nice interpretation of uncertainty over features  and P to be a ball  as
it decomposes (as opposed to an (cid:96)∞ ball)  which allows simpler optimization algorithms. Second 
as noted above  BoW-S is indeed smoother than BoW-M  yet  from (14) it follows that the latter is
better motivated from the PAC-Bayesian bound  as we want Q ⊆ P. Third  the bound is small if
the volume of the box Q is large  which motivates seeking for large-volume boxes  whose members
perform well.

6 Empirical Evaluation

We evaluated BoW-M and BoW-S on NLP tasks experimenting with all the 12 datasets used by
Dredze et al [6] (sentiment classiﬁcation in 6 Amazon domains  3 pairs of 20 newsgroups and 3
pairs of Reuters (RCV1)). We deﬁned an additional task from the 6 Amazon domains (book  dvd 
music  video  electronics  kitchen). Given reviews from two domains  the goal is to identify the do-
main identity. We used all 6×5/2 = 15 unordered pairs of domains. Additionally  we selected 3 users
from task A of the 2006 ECML/PKDD Discovery Challenge spam data set. The goal is to classify
an email as either a spam or a not-spam. This yielded a total of 30 datasets. For each problem we
selected about 2  000 instances and represented them with vectors of uni/bi-grams counts. Feature
extraction followed a previous protocol [6  2]. Each dataset was randomly divided for 10-fold cross
validation. We also experimented with USPS OCR data which we binarized into 45 all-pairs prob-
lems  maintaing the standard split into training and test sets. Given an image of one of two digits 
the goal is to detect which of the two digits is shown in the image.
We implemented BoW-M and BoW-S both with Hinge loss and Huber loss. The performance of the
latter was slightly worse than the former  thus we report results only for the Hinge loss. We also tried
AdaGrad [7] but surprisingly it did not work as well as COMID. We compared BoW with support
vector machines (SVM) [3] and AROW [4] which was shown to outperform many algorithms on
NLP tasks. (Other algorithms we evaluated  including maximum-entropy and SGD with Huber-
loss  performed worse than either of these two algorithms and thus are omitted.) It is not clear at
this point how to incorporate Mercer kernels into BoW  and thus we are restricted to evaluate all
algorithms on data that can be classiﬁed well with linear models.
Classiﬁers parameters (C for SVM  r for AROW and C  D for BoW) were tuned for each task on
a single additional randomized run over the data splitting it into 80%  used for training  and the
remaining 20% of examples were used to choose the parameters. Results are reported for NLP tasks
as the average error over the 10 folds per problem   while for USPS the standard test sets are used.
The mean error for 30 NLP tasks over 10 folds of BoW-M and BoW-S vs SVM is summarized in the
two left panels of Fig. 1. Markers above the line indicate superior BoW performance. Clearly  both
BoW versions outperform SVM obtaining lower test error on most (26) datasets and higher only on
few (at most 3). The right two panels compare the performance of both BoW versions with AROW.
Here the trend remains yet with a smaller gap  BoW-M outperforms AROW in 20 datasets  and is
outperformed in 9  while BoW-S outperforms AROW in 19 datasets and outperformed in 12. Note 
AROW was previously shown [4] to have superior performance on text data over other algorithms.

7

10−110010110−1100101error (%) BoW−Merror (%) SVM10−110010110−1100101error (%) BoW−Serror (%) SVM10−110010110−1100101error (%) BoW−Merror (%) AROW10−110010110−1100101error (%) BoW−Serror (%) AROWFigure 2: No. of USPS 1-vs-1 datasets (out of 45) for which one algorithm is better than the other (see legend)
shown for four levels of label noise during training: 0%  10%  20% and 30% (left to right). Higher values
indicate better performance.

The results of the experiments with USPS are summarized in Fig. 2. Each panel shows the number of
datasets (out of 45) for which one algorithm outperforms another algorithm  for four levels of label
noise (i.e. probability of ﬂipping the correct label) during training: 0%  10%  20% and 30%.The
four pairs compared are BoW vs SVM (two left panels  BoW-M most left) and BoW vs AROW
(two right panels  BoW-S most right). A left bar higher than a middle bar (in each group in each
panel) indicates superior BoW performance. With no label noise (left group in each panel) SVM
outperforms both BoW algorithms (e.g. SVM attains lower test error than BoW-S on 20 datasets
and higher on 12 datasets  with a tie in 13 datasets). The average test error of SVM is 1.81  AROW
is 1.98 and BoW-S is 1.97. When the level of noise increases both BoW algorithms outperform
AROW and SVM. With maximal level of 30% label noise  the average test error is 16.1% for SVM 
14.8 for AROW  and 6.1% for BoW-S. BoW-M achieves lower test error on 27 datasets (compared
both with SVM and AROW)  while BoW-S achieves lower test error than SVM on 38 datasets and
than AROW on 40 datasets. Interestingly  while  in general  BoW-M achieved lower test error than
BoW-S on the NLP problems  the situation is reversed in the USPS data where BoW-S achieves in
general lower test error.

7 Related Work
There is much previous work on a related topic of incorporating additional constraints  using prior
knowledge of the problem. Shivaswamy and Jebara [24] use a geometric motivation to modify
SVMs. Their effort and other related works  ﬁrst deduce some additional knowledge about the
problem [16  20  1]  and keep it ﬁxed while learning. In contrast  our method learns together the
classiﬁer and some additional information.
Another line of research is about algorithms that are maintaining a Gaussian distribution over
weights  as opposed to uniform distribution as in our case  either AROW [4] in the online setting
and its predecessors  or Gaussian Margin Machines (GMMs) [5] in the batch setting. Our motiva-
tion is similar to the motivation behind GMMs  yet it is different in few important aspects. (1) BoW
maintains only 2d parameters  while GMM employs d + d(d + 1)/2 as it maintains a full covariance
matrix. (2) As a consequence  GMMs are not feasible to run on data with more than hundreds of
features  which is further supported by the fact that GMMs were evaluated only on data of dimen-
sion 64 [5]. (3) We use directly a specialized PAC-Bayes bound for convex loss functions [10] while
the analysis of GMMs uses a bound designed for the 0 − 1 loss which is then further bounded. (4)
The optimization problem of both versions of BoW is convex  while the optimization problem of
GMMs is not convex  and it is only approximated with a convex problem. (5) Therefore  we can and
do employ COMID [8] which is theoretically justiﬁed and fast in practice  while GMMs are trained
using another technique with no convergence (to local minima) guarantees. (6) Conceptually  BoW
maintains a compact set (box) while the set of possible weights for GMM is not compact. This
allows us to extend our work to other types of sets (in progress)  while its not clear how to extend
the GMMs approach from Gaussian distributions to other objects.

8 Conclusion
We extend the commonly used linear classiﬁers to subsets of the class of possible classiﬁers  or
in other words uniform distributions over weight vectors. Our learning algorithm is based on a
worst-case margin minimization principle  and it beneﬁts from strong theoretical guarantees based
on tight PAC-Bayesian bounds. The empirical evaluation presented shows that our method performs
favourably with respect to SVMs and AROW  and is more robust in the presence of label noise. We

8

0%10%20%30%010203040No winsTraining Label Error  BoW−MSVMTIE0%10%20%30%010203040No winsTraining Label Error  BoW−SSVMTIE0%10%20%30%010203040No winsTraining Label Error  BoW−MAROWTIE0%10%20%30%010203040No winsTraining Label Error  BoW−SAROWTIEplan to study the integration of kernels  extend our framework for various shapes and problems  and
develop specialized large scale algorithms.
Acknowledgments: The paper was partially supported by an Israeli Science Foundation grant ISF-
1567/10 and by a Google research award.

References
[1] J. Bi and T. Zhang. Support vector classiﬁcation with input data uncertainty. In NIPS  2004.
[2] J. Blitzer  M. Dredze  and F. Pereira. Biographies  bollywood  boom-boxes and blenders:

Domain adaptation for sentiment classiﬁcation. In ACL  2007.

[3] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning  20(3):273–297 

September 1995.

[4] K. Crammer  A. Kulesza  and M. Dredze. Adaptive regularization of weighted vectors.

NIPS  2009.

In

[5] K. Crammer  M. Mohri  and F. Pereira. Gaussian margin machines. In AISTATS  2009.
[6] M. Dredze  K. Crammer  and F. Pereira. Conﬁdence-weighted linear classiﬁcation. In ICML 

2008.

[7] J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and

stochastic optimization. In COLT  2010.

[8] J. Duchi  S. Shalev-Shwartz  Y. Singer  and A. Tewari. Composite objective mirror descent. In

COLT  pages 250–264  2010.

[9] Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. In Euro-COLT  pages 23–37  1995.

[10] P. Germain  A. Lacasse  F. Laviolette  and M. Marchand. Pac-bayesian learning of linear

classiﬁers. In ICML  2009.

[11] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning: Data Mining 

Inference  and Prediction. Springer  2001.

[12] R. Herbrich  T. Graepel  and C. Campbell. Robust Bayes point machines. In ESANN 2000 

pages 49–54  2000.

[13] R. Herbrich  T. Graepel  and C. Campbell. Bayes point machines. JMLR  1:245–279  2001.
[14] P.J. Huber. Robust estimation of a location parameter. Annals of Statistics  53:73101  1964.
[15] T. Jaakkola and M. Jordan. A variational approach to Bayesian logistic regression models and

their extensions. In Workshop on Artiﬁcial Intelligence and Statistics  1997.

[16] G. Lanckriet  L. Ghaoui  C. Bhattacharyya  and M. Jordan. A robust minimax approach to

classiﬁcation. JMLR  3:555–582  2002.

[17] J. Langford and M. Seeger. Bounds for averaging classiﬁers. Technical report  CMU-CS-01-

102  2002.

[18] J. Langford and J. Shawe-Taylor. PAC-bayes and margins. In NIPS  2002.
[19] D. McAllester. PAC-Bayesian model averaging. In COLT  1999.
[20] J. Nath  C. Bhattacharyya  and M. Murty. Clustering based large margin classiﬁcation: A

scalable approach using SOCP formulation. In KDD  2006.

[21] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.

Morgan Kaufmann  1988.

[22] B. Sch¨olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines  Regulariza-

tion  Optimization and Beyond. MIT Press  2002.

[23] M. Seeger. PAC-Bayesian generalization bounds for gaussian processes. JMLR  3:233–269 

2002.

[24] P. Shivaswamy and T. Jebara. Ellipsoidal kernel machines. In AISTATS  2007.
[25] V. N. Vapnik. Statistical Learning Theory. Wiley  1998.
[26] M.H. Wright. The interior-point revolution in optimization: history  recent developments  and

lasting consequences. Bull. Amer. Math. Soc.  42:39–56  2005.

9

,Yiwen Guo
Chao Zhang
Changshui Zhang
Yurong Chen