2017,Variational Inference for Gaussian Process Models with Linear Complexity,Large-scale Gaussian process inference has long faced practical challenges due to time and space complexity that is superlinear in dataset size. While sparse variational Gaussian process models are capable of learning from large-scale data  standard strategies for sparsifying the model can prevent the approximation of complex functions. In this work  we propose a novel variational Gaussian process model that decouples the representation of mean and covariance functions in reproducing kernel Hilbert space. We show that this new parametrization generalizes previous models. Furthermore  it yields a variational inference problem that can be solved by stochastic gradient ascent with time and space complexity that is only linear in the number of mean function parameters  regardless of the choice of kernels  likelihoods  and inducing points. This strategy makes the adoption of large-scale expressive Gaussian process models possible. We run several experiments on regression tasks and show that this decoupled approach greatly outperforms previous sparse variational Gaussian process inference procedures.,Variational Inference for Gaussian Process Models

with Linear Complexity

Ching-An Cheng

Byron Boots

Institute for Robotics and Intelligent Machines

Institute for Robotics and Intelligent Machines

Georgia Institute of Technology

Atlanta  GA 30332

cacheng@gatech.edu

Georgia Institute of Technology

Atlanta  GA 30332

bboots@cc.gatech.edu

Abstract

Large-scale Gaussian process inference has long faced practical challenges due
to time and space complexity that is superlinear in dataset size. While sparse
variational Gaussian process models are capable of learning from large-scale
data  standard strategies for sparsifying the model can prevent the approximation
of complex functions. In this work  we propose a novel variational Gaussian
process model that decouples the representation of mean and covariance functions
in reproducing kernel Hilbert space. We show that this new parametrization
generalizes previous models. Furthermore  it yields a variational inference problem
that can be solved by stochastic gradient ascent with time and space complexity that
is only linear in the number of mean function parameters  regardless of the choice of
kernels  likelihoods  and inducing points. This strategy makes the adoption of large-
scale expressive Gaussian process models possible. We run several experiments
on regression tasks and show that this decoupled approach greatly outperforms
previous sparse variational Gaussian process inference procedures.

1

Introduction

Gaussian process (GP) inference is a popular nonparametric framework for reasoning about functions
under uncertainty. However  the expressiveness of GPs comes at a price: solving (approximate)
inference for a GP with N data instances has time and space complexities in Θ(N 3) and Θ(N 2) 
respectively. Therefore  GPs have traditionally been viewed as a tool for problems with small- or
medium-sized datasets
Recently  the concept of inducing points has been used to scale GPs to larger datasets. The idea is to
summarize a full GP model with statistics on a sparse set of M (cid:28) N ﬁctitious observations [18  24].
By representing a GP with these inducing points  the time and the space complexities are reduced to
O(N M 2 + M 3) and O(N M + M 2)  respectively. To further process datasets that are too large to ﬁt
into memory  stochastic approximations have been proposed for regression [10] and classiﬁcation [11].
These methods have similar complexity bounds  but with N replaced by the size of a mini-batch Nm.
Despite the success of sparse models  the scalability issues of GP inference are far from resolved.
The major obstruction is that the cubic complexity in M in the aforementioned upper-bound is also
a lower-bound  which results from the inversion of an M-by-M covariance matrix deﬁned on the
inducing points. As a consequence  these models can only afford to use a small set of M basis
functions  limiting the expressiveness of GPs for prediction.
In this work  we show that superlinear complexity is not completely necessary. Inspired by the
reproducing kernel Hilbert space (RKHS) representation of GPs [2]  we propose a generalized
variational GP model  called DGPs (Decoupled Gaussian Processes)  which decouples the bases

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

a  B

SGA
SNGA
SMA
CG
CG

α β

SGA
SGA
SMA
CG
CG

θ

SGA
SGA
SGA
CG
CG

SVDGP
SVI
iVSGPR
VSGPR
GPR

α = β N (cid:54)= M Time
FALSE
TRUE
TRUE
TRUE
TRUE

TRUE
TRUE
TRUE
TRUE
FALSE

Space

O(DN Mα + N M 2
β + M 3
O(DN M + N M 2 + M 3)
O(DN M + N M 2 + M 3)
O(DN M + N M 2 + M 3)
O(DN 2 + N 3)

β ) O(N Mα + M 2
β )
O(N M + M 2)
O(N M + M 2)
O(N M + M 2)
O(N 2)

Table 1: Comparison between SVDGP and variational GPR algorithms: SVI [10]  iVSGPR [2] 
VSGPR [24]  and GPR [19]  where N is the number of observations/the size of a mini-batch  M  Mα 
Mβ are the number of basis functions  and D is the input dimension. Here it is assumed Mα ≥ Mβ
1.

(a) M = 10

(b) Mα = 100  Mβ = 10

(c) M = 100

Figure 1: Comparison between models with shared and decoupled basis. (a)(c) denote the models
with shared basis of size M. (b) denotes the model of decoupled basis with size (Mα  Mβ). In each
ﬁgure  the red line denotes the ground truth; the blue circles denote the observations; the black line
and the gray area denote the mean and variance in prediction  respectively.

β + M 3

for the mean and the covariance functions. Speciﬁcally  let Mα and Mβ be the numbers of basis
functions used to model the mean and the covariance functions  respectively. Assume Mα ≥ Mβ.
We show  when DGPs are used as a variational posterior [24]  the associated variational inference
problem can be solved by stochastic gradient ascent with space complexity O(NmMα + M 2
β ) and
β )  where D is the input dimension. We name this
time complexity O(DNmMα + NmM 2
algorithm SVDGP. As a result  we can choose Mα (cid:29) Mβ  which allows us to keep the time and space
complexity similar to previous methods (by choosing Mβ = M) while greatly increasing accuracy.
To the best of our knowledge  this is the ﬁrst variational GP algorithm that admits linear complexity
in Mα  without any assumption on the choice of kernel and likelihood.
While we design SVDGP for general likelihoods  in this paper we study its effectiveness in Gaussian
process regression (GPR) tasks. We consider this is without loss of generality  as most of the
sparse variational GPR algorithms in the literature can be modiﬁed to handle general likelihoods
by introducing additional approximations (e.g. in Hensman et al. [11] and Sheth et al. [22]). Our
experimental results show that SVDGP signiﬁcantly outperforms the existing techniques  achieving
higher variational lower bounds and lower prediction errors when evaluated on held-out test sets.

1.1 Related Work

Our framework is based on the variational inference problem proposed by Titsias [24]  which treats
the inducing points as variational parameters to allow direct approximation of the true posterior.
This is in contrast to Seeger et al. [21]  Snelson and Ghahramani [23]  Quiñonero-Candela and
Rasmussen [18]  and Lázaro-Gredilla et al. [15]  which all use inducing points as hyper-parameters
of a degenerate prior. While both approaches have the same time and space complexity  the latter
additionally introduces a large set of unregularized hyper-parameters and  therefore  is more likely to
suffer from over-ﬁtting [1].
In Table 1  we compare SVDGP with recent GPR algorithms in terms of the assumptions made and the
time and space complexity. Each algorithm can be viewed as a special way to solve the maximization
of the variational lower bound (5)  presented in Section 3.2. Our algorithm SVDGP generalizes the
previous approaches to allow the basis functions for the mean and the covariance to be decoupled  so
an approximate solution can be found by stochastic gradient ascent in linear complexity.

1The ﬁrst three columns show the algorithms to update the parameters: SGA/SNGA/SMA denotes stochastic
gradient/natural gradient/mirror ascent  and CG denotes batch nonlinear conjugate gradient ascent. The 4th and
the 5th columns indicate whether the bases for mean and covariance are strictly shared  and whether a variational
posterior can be used. The last two columns list the time and space complexity.

2

To illustrate the idea  we consider a toy GPR example in Figure 1. The dataset contains 500 noisy
observations of a sinc function. Given the same training data  we conduct experiments with three
different GP models. Figure 1 (a)(c) show the results of the traditional coupled basis  which can be
solved by any of the variational algorithms listed in Table 1  and Figure 1 (b) shows the result using the
decoupled approach SVDGP. The sizes of basis and observations are selected to emulate a large dataset
scenario. We can observe SVDGP achieves a nice trade-off between prediction performance and
complexity: it achieves almost the same accuracy in prediction as the full-scale model in Figure 1(c)
and preserves the overall shape of the predictive variance.
In addition to the sparse algorithms above  some recent attempts aim to revive the non-parametric
property of GPs by structured covariance functions. For example  Wilson and Nickisch [27] proposes
to space the inducing points on a multidimensional lattice  so the time and space complexities of
using a product kernel becomes O(N + DM 1+1/D) and O(N + DM 1+2/D)  respectively. However 
because M = cD  where c is the number of grid points per dimension  the overall complexity is
exponential in D and infeasible for high-dimensional data. Another interesting approach by Hensman
et al. [12] combines variational inference [24] and a sparse spectral approximation [15]. By equally
spacing inducing points on the spectrum  they show the covariance matrix on the inducing points have
diagonal plus low-rank structure. With MCMC  the algorithm can achieve complexity O(DN M ).
However  the proposed structure in [12] does not help to reduce the complexity when an approximate
Gaussian posterior is favored or when the kernel hyper-parameters need to be updated.
Other kernel methods with linear complexity have been proposed using functional gradient descent
[14  5]. However  because these methods use a model strictly the same size as the entire dataset  they
fail to estimate the predictive covariance  which requires Ω(N 2) space complexity. Moreover  they
cannot learn hyper-parameters online. The latter drawback also applies to greedy algorithms based
on rank-one updates  e.g. the algorithm of Csató and Opper [4].
In contrast to these previous methods  our algorithm applies to all choices of inducing points 
likelihoods  and kernels  and we allow both variational parameters and hyper-parameters to adapt
online as more data are encountered.

2 Preliminaries

In this section  we brieﬂy review the inference for GPs and the variational framework proposed
by Titsias [24]. For now  we will focus on GPR for simplicity of exposition. We will discuss the case
of general likelihoods in the next section when we introduce our framework  DGPs.

Inference for GPs

2.1
Let f : X → R be a latent function deﬁned on a compact domain X ⊂ RD. Here we assume a priori
that f is distributed according to a Gaussian process GP(m  k). That is  ∀x  x(cid:48) ∈ X   E[f (x)] = m(x)
and C[f (x)  f (x(cid:48))] = k(x  x(cid:48)). In short  we write f ∼ GP(m  k).
A GP probabilistic model is composed of a likelihood p(y|f (x)) and a GP prior GP(m  k); in GPR 
the likelihood is assumed to be Gaussian i.e. p(y|f (x)) = N (y|f (x)  σ2) with variance σ2. Usually 
the likelihood and the GP prior are parameterized by some hyper-parameters  which we summarize
as θ. This includes  for example  the variance σ2 and the parameters implicitly involved in deﬁning
k(x  x(cid:48)). For notational convenience  and without loss of generality  we assume m(x) = 0 in the
prior distribution and omit explicitly writing the dependence of distributions on θ.
Assume we are given a dataset D = {(xn  yn)}N
n=1  in which xn ∈ X and yn ∼ p(y|f (xn)). Let2
n=1. Inference for GPs involves solving for the posterior pθ∗ (f (x)|y)
X = {xn}N
for any new input x ∈ X   where θ∗ = arg maxθ log pθ(y). For example in GPR  because the
likelihood is Gaussian  the predictive posterior is also Gaussian with mean and covariance

n=1 and y = (yn)N

m|y(x) = kx X (KX + σ2I)−1y 

k|y(x  x(cid:48)) = kx x(cid:48) − kx X (KX + σ2I)−1kX x(cid:48) 

and the hyper-parameter θ∗ can be found by nonlinear conjugate gradient ascent [19]

max

θ

log pθ(y) = max

θ

log N (y|0  KX + σ2I) 

2In notation  we use boldface to distinguish ﬁnite-dimensional vectors (lower-case) and matrices (upper-case)

that are used in computation from scalar and abstract mathematical objects.

3

(1)

(2)

where k· ·  k· · and K· · denote the covariances between the sets in the subscript.3 One can show that
these two functions  m|y(x) and k|y(x  x(cid:48))  deﬁne a valid GP. Therefore  given observations y  we
say f ∼ GP(m|y  k|y).
Although theoretically GPs are non-parametric and can model any function as N → ∞  in practice
this is difﬁcult. As the inference has time complexity Ω(N 3) and space complexity Ω(N 2)  applying
vanilla GPs to large datasets is infeasible.

2.2 Variational Inference with Sparse GPs

To scale GPs to large datasets  Titsias [24] introduced a scheme to compactly approximate the true
posterior with a sparse GP  GP( ˆm|y  ˆk|y)  deﬁned by the statistics on M (cid:28) N function values:
{Lmf (˜xm)}M
m=1  where Lm is a bounded linear operator4 and ˜xm ∈ X . Lmf (·) is called an
inducing function and ˜xm an inducing point. Common choices of Lm include the identity map (as
used originally by Titsias [24]) and integrals to achieve better approximation or to consider multi-
domain information [26  7  3]. Intuitively  we can think of {Lmf (˜xm)}M
m=1 as a set of potentially
indirect observations that capture salient information about the unknown function f.
m=1 and let fX ∈ RN
Titsias [24] solves for GP( ˆm|y  ˆk|y) by variational inference. Let ˜X = {˜xm}M
and f ˜X ∈ RM be the (inducing) function values deﬁned on X and ˜X  respectively. Let p(f ˜X ) be
the prior given by GP(m  k) and deﬁne q(f ˜X ) = N (f ˜X| ˜m  ˜S) to be its variational posterior  where
˜m ∈ RM and ˜S ∈ RM×M are the mean and the covariance of the approximate posterior of f ˜X.
Titsias [24] proposes to use q(fX   f ˜X ) = p(fX|f ˜X )q(f ˜X ) as the variational posterior to approximate
p(fX   f ˜X|y) and to solve for q(f ˜X ) together with the hyper-parameter θ through
p(y|fX )p(fX|f ˜X )p(f ˜X )

(cid:90)

max

θ  ˜X  ˜m ˜S

Lθ( ˜X  ˜m  ˜S) = max

θ  ˜X  ˜m ˜S

q(fX   f ˜X ) log

q(fX   f ˜X )

(3)

dfX df ˜X  
f ˜X   KX − ˆKX )

where Lθ is a variational lower bound of log pθ(y)  p(fX|f ˜X ) = N (fX|KX  ˜X K−1
is the conditional probability given in GP(m  k)  and ˆKX = KX  ˜X K−1
At ﬁrst glance  the speciﬁc choice of variational posterior q(fX   f ˜X ) seems heuristic. However 
although parameterized ﬁnitely  it resembles a full-ﬂedged GP GP( ˆm|y  ˆk|y):

K ˜X X.

˜X

˜X

ˆm|y(x) = kx  ˜X K−1

˜X

˜m 

ˆk|y(x  x(cid:48)) = kx x(cid:48) + kx  ˜X K−1

˜X

K−1
˜X

k ˜X x(cid:48).

(4)

(cid:16)˜S − K ˜X

(cid:17)

This result is further studied in Matthews et al. [16] and Cheng and Boots [2]  where it is shown that
(3) is indeed minimizing a proper KL-divergence between Gaussian processes/measures.
By comparing (2) and (3)  one can show that the time and the space complexities now reduce
to O(DN M + M 2N + M 3) and O(M 2 + M N )  respectively  due to the low-rank structure of
ˆK ˜X [24]. To further reduce complexity  stochastic optimization  such as stochastic natural ascent
[10] or stochastic mirror descent [2] can be applied. In this case  N in the above asymptotic bounds
would be replaced by the size of a mini-batch Nm. The above results can be modiﬁed to consider
general likelihoods as in [22  11].

3 Variational Inference with Decoupled Gaussian Processes

Despite the success of sparse GPs  the scalability issues of GPs persist. Although parameterizing a GP
with inducing points/functions enables learning from large datasets  it also restricts the expressiveness
of the model. As the time and the space complexities still scale in Ω(M 3) and Ω(M 2)  we cannot
learn or use a complex model with large M.
In this work  we show that these two complexity bounds  which have long accompanied GP models 
are not strictly necessary  but are due to the tangled representation canonically used in the GP

3If the two sets are the same  only one is listed.
4Here we use the notation Lmf loosely for the compactness of writing. Rigorously  Lm is a bounded linear

operator acting on m and k  not necessarily on all sample paths f.

4

literature. To elucidate this  we adopt the dual representation of Cheng and Boots [2]  which treats
GPs as linear operators in RKHS. But  unlike Cheng and Boots [2]  we show how to decouple the
basis representation of mean and covariance functions of a GP and derive a new variational problem 
which can be viewed as a generalization of (3). We show that this problem—with arbitrary likelihoods
and kernels—can be solved by stochastic gradient ascent with linear complexity in Mα  the number
of parameters used to specify the mean function for prediction.
In the following  we ﬁrst review the results in [2]. We next introduce the decoupled representation 
DGPs  and its variational inference problem. Finally  we present SVDGP and discuss the case with
general likelihoods.

x µ and k(x  x(cid:48)) = φT

3.1 Gaussian Processes as Gaussian Measures
Let an RKHS H be a Hilbert space of functions with the reproducing property: ∀x ∈ X   ∃φx ∈ H
such that ∀f ∈ H  f (x) = φT
x f.5 A Gaussian process GP(m  k) is equivalent to a Gaussian
measure ν on Banach space B which possesses an RKHS H [2]:6 there is a mean functional µ ∈ H
and a bounded positive semi-deﬁnite linear operator Σ : H → H  such that for any x  x(cid:48) ∈ X  
∃φx  φx(cid:48) ∈ H  we can write m(x) = φT
x Σφx(cid:48). The triple (B  ν H) is known
as an abstract Wiener space [9  6]  in which H is also called the Cameron-Martin space. Here the
restriction that µ  Σ are RKHS objects is necessary  so the variational inference problem in the next
section can be well-deﬁned.
We call this the dual representation of a GP in RKHS H (the mean function m and the covariance
function k are realized as linear operators µ and Σ deﬁned in H). With abuse of notation  we write
N (f|µ  Σ) in short. This notation does not mean a GP has a Gaussian distribution in H  nor does it
imply that the sample paths from GP(m  k) are necessarily in H. Precisely  B contains the sample
paths of GP(m  k) and H is dense in B. In most applications of GP models  B is the Banach space
of continuous function C(X ;Y) and H is the span of the covariance function. As a special case  if
H is ﬁnite-dimensional  B and H coincide and ν becomes equivalent to a Gaussian distribution in a
Euclidean space.
x φx(cid:48) and φx : X → H
In relation to our previous notation in Section 2.1: suppose k(x  x(cid:48)) = φT
is a feature map to some Hilbert space H. Then we have assumed a priori that GP(m  k) =
(cid:80)dim H
N (f|0  I) is a normal Gaussian measure; that is GP(m  k) samples functions f in the form f (x) =
l=1 φl(x)T l  where l ∼ N (0  1) are independent. Note if dimH = ∞  with probability one
f is not in H  but fortunately H is large enough for us to approximate the sampled functions. In
particular  it can be shown that the posterior GP(m|y  k|y) in GPR has a dual RKHS representation
in the same RKHS as the prior GP [2].

3.2 Variational Inference in Gaussian Measures

(cid:90)

Lθ(q(f )) = max

q(f ) θ

pθ(y|f )p(f )

Cheng and Boots [2] proposes a dual formulation of (3) in terms of Gaussian measures7:

q(f ) log

max
q(f ) θ

(5)
where q(f ) = N (f|˜µ  ˜Σ) is a variational Gaussian measure and p(f ) = N (f|0  I) is a normal prior.
Its connection to the inducing points/functions in (3) can be summarized as follows [2  3]: Deﬁne
m=1 amψ˜xm  where ψ˜xm ∈ H is deﬁned such that

a linear operator Ψ ˜X : RM → H as a (cid:55)→ (cid:80)M

µ = E[Lmf (˜xm)]. Then (3) and (5) are equivalent  if q(f ) has a subspace parametrization 

df = max
q(f ) θ

q(f )

Eq[log pθ(y|f )] − KL[q||p] 

ψT
˜xm

(6)
with a ∈ RM and A ∈ RM×M satisfying ˜m = K ˜X a  and ˜S = K ˜X + K ˜X AK ˜X. In other words 
the variational inference algorithms in the literature are all using a variational Gaussian measure in
which ˜µ and ˜Σ are parametrized by the same basis {ψ˜xm|˜xm ∈ ˜X}M
i=1.

˜Σ = I + Ψ ˜X AΨT
˜X  

˜µ = Ψ ˜X a 

L : H → H  even if H is inﬁnite-dimensional.

x f for (cid:104)f  φx(cid:105)H  and f T Lg for (cid:104)f  Lg(cid:105)H  where f  g ∈ H and
5To simplify the notation  we write φT
6Such H w.l.o.g. can be identiﬁed as the natural RKHS of the covariance function of a zero-mean prior GP.
7 We assume q(f ) is absolutely continuous wrt p(f )  which is true as p(f ) is non-degenerate. The integral

denotes the expectation of log pθ(y|f ) + log p(f )

q(f ) over q(f )  and q(f )

p(f ) denotes the Radon-Nikodym derivative.

5

Compared with (3)  the formulation in (5) is neater: it follows the deﬁnition of the very basic
variational inference problem. This is not surprising  since GPs can be viewed as Bayesian linear
models in an inﬁnite-dimensional space. Moreover  in (5) all hyper-parameters are isolated in the
likelihood pθ(y|f )  because the prior is ﬁxed as a normal Gaussian measure.

3.3 Disentangling the GP Representation with DGPs

˜µ = Ψαa 

˜Σ = (I + ΨβBΨT

While Cheng and Boots [2] treat (5) as an equivalent form of (3)  here we show that it is a generaliza-
tion. By further inspecting (5)  it is apparent that sharing the basis Ψ ˜X between ˜µ and ˜Σ in (6) is not
strictly necessary  since (5) seeks to optimize two linear operators  ˜µ and ˜Σ. With this in mind  we
propose a new parametrization that decouples the bases for ˜µ and ˜Σ:
β )−1

(7)
where Ψα : RMα → H and Ψβ : RMβ → H denote linear operators deﬁned similarly to Ψ ˜X and
B (cid:23) 0 ∈ RMβ×Mβ . Compared with (6)  here we parametrize ˜Σ through its inversion with B so the
condition that ˜Σ (cid:23) 0 can be easily realized as B (cid:23) 0. This form agrees with the posterior covariance
in GPR [2] and will give a posterior that is strictly less uncertain than the prior. Note the choice of
decoupled parametrization is not unique. In particular  the bases can be partially shared  or (a  B)
can be further parametrized (e.g. B can be parametrized using the canonical form in (4)) to improve
the numerical convergence rate. Please refer to Appendix A for a discussion.8
The decoupled subspace parametrization (7) corresponds to a DGP  GP( ˆmα|y  ˆkβ|y)  with mean and
covariance functions as 9

(cid:0)B−1 + Kβ

(cid:1)−1

kβ x(cid:48).

(8)

ˆmα|y(x) = kx αa 

ˆkβ|y(x  x(cid:48)) = kx x(cid:48) − kx β

While the structure of (8) looks similar to (4)  directly replacing the basis ˜X in (4) with α and β is
not trivial. Because the equations in (4) are derived from the traditional viewpoint of GPs as statistics
on function values  the original optimization problem (3) is not deﬁned if α (cid:54)= β and therefore  it is
not clear how to learn a decoupled representation traditionally. Conversely  by using the dual RKHS
representation  the objective function to learn (8) follows naturally from (5)  as we will show next.

3.4

SVDGP: Algorithm and Analysis

Substituting the decoupled subspace parametrization (7) into the variational inference problem in (5)
results in a numerical optimization problem: maxq(f ) θ Eq[log pθ(y|f )] − KL[q||p] with

KL[q||p] =

Eq[log pθ(y|f )] =

1
2

N(cid:88)

aT Kαa +

log |I + KβB| +

1
2

Eq(f (xn))[log pθ(yn|f (xn))]

tr(cid:0)Kβ(B−1 + Kβ)−1(cid:1)

−1
2

(9)

(10)

n=1

where each expectation is over a scalar Gaussian q(f (xn)) given by (8) as functions of (a  α) and
(B  β). Our objective function contains [11] as a special case  which assumes α = β = ˜X. In
addition  we note that Hensman et al. [11] indirectly parametrize the posterior by ˜m and ˜S = LLT  
whereas we parametrize directly by (6) with a for scalability and B = LLT for better stability (which
always reduces the uncertainty in the posterior compared with the prior).
We notice that (a  α) and (B  β) are completely decoupled in (9) and potentially combined again in
(10). In particular  if pθ(yn|f (xn)) is Gaussian as in GPR  we have an additional decoupling  i.e.
Lθ(a  B  α  β) = Fθ(a  α)+Gθ(B  β) for some Fθ(a  α) and Gθ(B  β). Intuitively  the optimization
8Appendix A is partially based on a discussion with Hugh Salimbeni at the NIPS conference. Here we adopt
the fully decoupled  directly parametrized form in (7) to demonstrate the idea. We leave the full comparison of
different decoupled parametrizations in future work.
9In practice  we can parametrize B = LLT with Cholesky factor L ∈ RMβ×Mβ so the problem is

unconstrained. The required terms in (8) and later in (9) can be stably computed as (cid:0)B−1 + Kβ

(cid:1)−1 =

LH−1LT and log |I + KβB| = log |H|  where H = I + LT KβL.

6

Algorithm 1 Online Learning with DGPs
Parameters: Mα  Mβ  Nm  N∆
Input: M(a  B  α  β  θ)   D
1: θ0 ← initializeHyperparameters( sampleMinibatch(D  Nm) )
2: for t = 1 . . . T do
3: Dt ← sampleMinibatch(D  Nm)
4: M.addBasis(Dt  N∆  Mα  Mβ)
5: M.updateModel(Dt  t)
6: end for

over (a  α) aims to minimize the ﬁtting-error  and the optimization over (B  β) aims to memorize the
samples encountered so far; the mean and the covariance functions only interact indirectly through
the optimization of the hyper-parameter θ.
One salient feature of SVDGP is that it tends to overestimate  rather than underestimate  the variance 
when we select Mβ ≤ Mα. This is inherited from the non-degeneracy property of the variational
framework [24] and can be seen in the toy example in Figure 1. In the extreme case when Mβ = 0 
we can see the covariance in (8) becomes the same as the prior; moreover  the objective function
of SVDGP becomes similar to kernel methods (exactly the same as kernel ridge regression  when the
likelihood is Gaussian). The additional inclusion of expected log-likelihoods here allows SVDGP
to learn the hyper-parameters in a uniﬁed framework  as its objective function can be viewed as
minimizing a generalization upper-bound in PAC-Bayes learning [8].
SVDGP solves the above optimization problem by stochastic gradient ascent. Here we purposefully
ignore speciﬁc details of pθ(y|f ) to emphasize that SVDGP can be applied to general likelihoods as it
only requires unbiased ﬁrst-order information  which e.g. can be found in [22]. In addition to having
a more adaptive representation  the main beneﬁt of SVDGP is that the computation of an unbiased
gradient requires only linear complexity in Mα  as shown below (see Appendix Bfor details).
KL-Divergence Assume |α| = O(DMα) and |β| = O(DMβ). By (9)  One can show
∇aKL[q||p] = Kαa and ∇BKL[q||p] = 1
2 (I+KβB)−1KβBKβ(I+BKβ)−1. Therefore  the time
complexity to compute ∇aKL[q||p] can be reduced to O(NmMα) if we sample over the columns
of Kα with a mini-batch of size Nm. By contrast  the time complexity to compute ∇BKL[q||p]
β ) and cannot be further reduced  regardless of the parametrization of B.10 The
will always be Θ(M 3
gradient with respect to α and β can be derived similarly and have time complexity O(DNmMα)
and O(DM 2
Expected Log-Likelihood Let ˆm(a  α) ∈ RN and ˆs(B  β) ∈ RN be the vectors of the mean and
covariance of scalar Gaussian q(f (xn)) for n ∈ {1  . . .   N}. As (10) is a sum over N terms  by
sampling with a mini-batch of size Nm  an unbiased gradient of (10) with respect to (θ  ˆm  ˆs) can
be computed in O(Nm). To compute the full gradient with respect to (a  B  α  β)  we compute
the derivative of ˆm and ˆs with respect to (a  B  α  β) and then apply chain rule. These steps take
O(DNmMα) and O(DNmMβ + NmM 2
The above analysis shows that the curse of dimensionality in GPs originates in the covariance function.
For space complexity  the decoupled parametrization (7) requires memory in O(NmMα + M 2
β );
for time complexity  an unbiased gradient with respect to (a  α) can be computed in O(DNmMα) 
but that with respect to (B  β) has time complexity Ω(DNmMβ + NmM 2
β ). This motivates
choosing Mβ = O(M ) and Mα in O(M 2
β )  which maintains the same complexity as
previous variational techniques but greatly improves the prediction performance.

β ) for (a  α) and (B  β)  respectively.

β ) or O(M 3

β + M 3

β )  respectively.

β + M 3

β + M 3

4 Experimental Results

We compare our new algorithm  SVDGP  with the state-of-the-art incremental algorithms for sparse
variational GPR  SVI [10] and iVSGPR [2]  as well as the classical GPR and the batch algorithm VS-
GPR [24]. As discussed in Section 1.1  these methods can be viewed as different ways to optimize (5).
Therefore  in addition to the normalized mean square error (nMSE) [19] in prediction  we report

10Due to Kβ  the complexity would remain as O(M 3

β ) even if B is constrained to be diagonal.

7

KUKA1 - Variational Lower Bound (105)
SVDGP
VSGPR
1.262
0.195

0.472
0.265

0.649
0.201

0.391
0.076

iVSGPR

SVI

KUKA1 - Prediction Error (nMSE)

SVDGP
0.037
0.013

SVI

0.169
0.025

iVSGPR

VSGPR

GPR

0.128
0.033

0.139
0.026

0.231
0.045

mean
std

GPR

-5.335
7.777

GPR

MUJOCO1 - Variational Lower Bound (105)
SVDGP
6.007
0.673

2.822
0.871

4.543
0.898

2.178
0.692

iVSGPR

VSGPR

SVI

-10312.727
22679.778

mean
std

MUJOCO1 - Prediction Error (nMSE)
VSGPR

iVSGPR

SVI

0.163
0.053

0.099
0.026

0.118
0.016

SVDGP
0.072
0.013

GPR

0.213
0.061

mean
std

mean
std

Table 2: Experimental results of KUKA1 and MUJOCO1 after 2 000 iterations.

the performance in the variational lower bound (VLB) (5)  which also captures the quality of the
predictive variance and hyper-parameter learning.11 These two metrics are evaluated on held-out test
sets in all of our experimental domains.
Algorithm 1 summarizes the online learning procedure used by all stochastic algorithms 12 where
each learner has to optimize all the parameters on-the-ﬂy using i.i.d. data. The hyper-parameters are
ﬁrst initialized heuristically by median trick using the ﬁrst mini-batch. We incrementally build up the
variational posterior by including N∆ ≤ Nm observations in each mini-batch as the initialization of
new variational basis functions. Then all the hyper-parameters and the variational parameters are
updated online. These steps are repeated for T iterations.
For all the algorithms  we assume the prior covariance is deﬁned by the SE-ARD kernel [19] and
we use the generalized SE-ARD kernel [2] as the inducing functions in the variational posterior (see
Appendix C for details). We note that all algorithms in comparison use the same kernel and optimize
both the variational parameters (including inducing points) and the hyperparameters.
√
In particular  we implement SGA by ADAM [13] (with default parameters β1 = 0.9 and β2 = 0.999).
t)−1  where
The step-size for each stochastic algorithms is scheduled according to γt = γ0(1 + 0.1
γ0 ∈ {10−1  10−2  10−3} is selected manually for each algorithm to maximize the improvement
in objective function after the ﬁrst 100 iterations. We test each stochastic algorithm for T = 2000
iterations with mini-batches of size Nm = 1024 and the increment size N∆ = 128. Finally  the
model sizes used in the experiments are listed as follows: Mα = 1282 and Mβ = 128 for SVDGP;
M = 1024 for SVI; M = 256 for iVSGPR; M = 1024  N = 4096 for VSGPR; N = 1024 for GP.
These settings share similar order of time complexity in our current Matlab implementation.

4.1 Datasets

Inverse Dynamics of KUKA Robotic Arm This dataset records the inverse dynamics of a KUKA
arm performing rhythmic motions at various speeds [17]. The original dataset consists of two parts:
KUKA1 and KUKA2  each of which have 17 560 ofﬂine data and 180 360 online data with 28 attributes
and 7 outputs. In the experiment  we mix the online and the ofﬂine data and then split 90% as training
data (178 128 instances) and 10% testing data (19 792 instances) to satisfy the i.i.d. assumption.

Walking MuJoCo MuJoCo (Multi-Joint dynamics with Contact) is a physics engine for research
in robotics  graphics  and animation  created by [25]. In this experiment  we gather 1 000 walking
trajectories by running TRPO [20]. In each time frame  the MuJoCo transition dynamics have a
23-dimensional input and a 17-dimensional output. We consider two regression problems to predict
9 of the 17 outputs from the input13: MUJOCO1 which maps the input of the current frame (23
dimensions) to the output  and MUJOCO2 which maps the inputs of the current and the previous
frames (46 dimensions) to the output. In each problem  we randomly select 90% of the data as
training data (842 745 instances) and 10% as test data (93 608 instances).

4.2 Results

We summarize part of the experimental results in Table 2 in terms of nMSE in prediction and VLB.
While each output is treated independently during learning  Table 2 present the mean and the standard

11The exact marginal likelihood is computationally infeasible to evaluate for our large model.
12The algorithms differs only in whether the bases are shared and how the model is updated (see Table 1).
13Because of the structure of MuJoCo dynamics  the rest 8 outputs can be trivially known from the input.

8

(a) Sample Complexity

(b) Time Complexity

Figure 2: An example of online learning results (the 9th output of MUJOCO1 dataset). The blue  red 
and yellow lines denote SVDGP  SVI  and iVSGPR  respectively.

deviation over all the outputs as the selected metrics are normalized. For the complete experimental
results  please refer to Appendix D.
We observe that SVDGP consistently outperforms the other approaches with much higher VLBs and
much lower prediction errors; SVDGP also has smaller standard deviation. These results validate our
initial hypothesis that adopting a large set of basis functions for the mean can help when modeling
complicated functions. iVSGPR has the next best result after SVDGP  despite using a basis size of
256  much smaller than that of 1 024 in SVI  VSGPR  and GPR. Similar to SVDGP  iVSGPR also
generalizes better than the batch algorithms VSGPR and GPR  which only have access to a smaller set
of training data and are more prone to over-ﬁtting. By contrast  the performance of SVI is surprisingly
worse than VSGPR. We conjecture this might be due to the fact that the hyper-parameters and the
inducing points/functions are only crudely initialized in online learning. We additionally ﬁnd that the
stability of SVI is more sensitive to the choice of step size than other methods. This might explain
why in [10  2] batch data was used to initialize the hyper-parameters and the learning rate to update
the hyper-parameters was selected to be much smaller than that for stochastic natural gradient ascent.
To further investigate the properties of different stochastic approximations  we show the change of
VLB and the prediction error over iterations and time in Figure 2. Overall  whereas iVSGPR and SVI
share similar convergence rate  the behavior of SVDGP is different. We see that iVSGPR converges the
fastest  both in time and sample complexity. Afterwards  SVDGP starts to descend faster and surpass
the other two methods. From Figure 2  we can also observe that although SVI has similar convergence
to iVSGPR  it slows down earlier and therefore achieves a worse result. These phenomenon are
observed in multiple experiments.

5 Conclusion

We propose a novel  fully-differentiable framework  Decoupled Gaussian Processes DGPs  for large-
scale GP problems. By decoupling the representation  we derive a variational inference problem
that can be solved with stochastic gradients with linear time and space complexity. Compared with
existing algorithms  SVDGP can adopt a much larger set of basis functions to predict more accurately.
Empirically  SVDGP signiﬁcantly outperforms state-of-the-arts variational sparse GPR algorithms in
multiple regression tasks. These encouraging experimental results motivate further application of
SVDGP to end-to-end learning with neural networks in large-scale  complex real world problems.

Acknowledgments

This work was supported in part by NSF NRI award 1637758. The authors additionally thank the
reviewers and Hugh Salimbeni for productive discussion which improved the quality of the paper.

References
[1] Matthias Bauer  Mark van der Wilk  and Carl Edward Rasmussen. Understanding probabilistic
sparse Gaussian process approximations. In Advances in Neural Information Processing Systems 

9

pages 1525–1533  2016.

[2] Ching-An Cheng and Byron Boots. Incremental variational sparse Gaussian process regression.

In Advances in Neural Information Processing Systems  pages 4403–4411  2016.

[3] Ching-An Cheng and Han-Pang Huang. Learn the Lagrangian: A vector-valued RKHS approach
to identifying Lagrangian systems. IEEE Transactions on Cybernetics  46(12):3247–3258 
2016.

[4] Lehel Csató and Manfred Opper. Sparse representation for Gaussian process models. Advances

in Neural Information Processing Systems  pages 444–450  2001.

[5] Bo Dai  Bo Xie  Niao He  Yingyu Liang  Anant Raj  Maria-Florina F Balcan  and Le Song.
Scalable kernel methods via doubly stochastic gradients. In Advances in Neural Information
Processing Systems  pages 3041–3049  2014.

[6] Nathaniel Eldredge. Analysis and probability on inﬁnite-dimensional spaces. arXiv preprint

arXiv:1607.03591  2016.

[7] Anibal Figueiras-Vidal and Miguel Lázaro-gredilla. Inter-domain Gaussian processes for sparse
inference using inducing features. In Advances in Neural Information Processing Systems 
pages 1087–1095  2009.

[8] Pascal Germain  Francis Bach  Alexandre Lacoste  and Simon Lacoste-Julien. Pac-bayesian
theory meets bayesian inference. In Advances in Neural Information Processing Systems  pages
1884–1892  2016.

[9] Leonard Gross. Abstract wiener spaces. In Proceedings of the Fifth Berkeley Symposium on
Mathematical Statistics and Probability  Volume 2: Contributions to Probability Theory  Part 1 
pages 31–42. University of California Press  1967.

[10] James Hensman  Nicolo Fusi  and Neil D. Lawrence. Gaussian processes for big data. arXiv

preprint arXiv:1309.6835  2013.

[11] James Hensman  Alexander G. de G. Matthews  and Zoubin Ghahramani. Scalable variational
Gaussian process classiﬁcation. In International Conference on Artiﬁcial Intelligence and
Statistics  2015.

[12] James Hensman  Nicolas Durrande  and Arno Solin. Variational Fourier features for Gaussian

processes. arXiv preprint arXiv:1611.06740  2016.

[13] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[14] Jyrki Kivinen  Alexander J Smola  and Robert C Williamson. Online learning with kernels.

IEEE transactions on signal processing  52(8):2165–2176  2004.

[15] Miguel Lázaro-Gredilla  Joaquin Quiñonero-Candela  Carl Edward Rasmussen  and Aníbal R.
Figueiras-Vidal. Sparse spectrum Gaussian process regression. Journal of Machine Learning
Research  11(Jun):1865–1881  2010.

[16] Alexander G. de G. Matthews  James Hensman  Richard E. Turner  and Zoubin Ghahramani. On
sparse variational methods and the Kullback-Leibler divergence between stochastic processes. In
Proceedings of the Nineteenth International Conference on Artiﬁcial Intelligence and Statistics 
2016.

[17] Franziska Meier  Philipp Hennig  and Stefan Schaal. Incremental local Gaussian regression. In

Advances in Neural Information Processing Systems  pages 972–980  2014.

[18] Joaquin Quiñonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approxi-
mate Gaussian process regression. The Journal of Machine Learning Research  6:1939–1959 
2005.

[19] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine

learning. 2006.

[20] John Schulman  Sergey Levine  Pieter Abbeel  Michael I. Jordan  and Philipp Moritz. Trust
region policy optimization. In Proceedings of the 32nd International Conference on Machine
Learning  pages 1889–1897  2015.

[21] Matthias Seeger  Christopher Williams  and Neil Lawrence. Fast forward selection to speed
In Artiﬁcial Intelligence and Statistics 9  number

up sparse Gaussian process regression.
EPFL-CONF-161318  2003.

10

[22] Rishit Sheth  Yuyang Wang  and Roni Khardon. Sparse variational inference for generalized
GP models. In Proceedings of the 32nd International Conference on Machine Learning  pages
1302–1311  2015.

[23] Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In

Advances in Neural Information Processing Systems  pages 1257–1264  2005.

[24] Michalis K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In

International Conference on Artiﬁcial Intelligence and Statistics  pages 567–574  2009.

[25] Emanuel Todorov  Tom Erez  and Yuval Tassa. Mujoco: A physics engine for model-based
In IEEE/RSJ International Conference on Intelligent Robots and Systems  pages

control.
5026–5033. IEEE  2012.

[26] Christian Walder  Kwang In Kim  and Bernhard Schölkopf. Sparse multiscale Gaussian process
regression. In Proceedings of the 25th international conference on Machine learning  pages
1112–1119. ACM  2008.

[27] Andrew Wilson and Hannes Nickisch. Kernel interpolation for scalable structured Gaussian
In Proceedings of the 32nd International Conference on Machine

processes (KISS-GP).
Learning  pages 1775–1784  2015.

11

,Ching-An Cheng
Byron Boots
Richard Zhang
Cedric Josz
Somayeh Sojoudi
Javad Lavaei