2019,Nonparametric Regressive Point Processes Based on Conditional Gaussian Processes,Real-world event sequences consist of complex mixtures of different types of events occurring in time. An event may depend on past events of the same type  as well as  the other types. Point processes define a general class of models for event sequences. ``Regressive point processes'' refer to point processes that directly model the dependency between an event and any past event  an example of which is a Hawkes process. In this work  we propose and develop a new nonparametric regressive point process model based on Gaussian processes. We show that our model can represent better many commonly observed real-world event sequences and capture the dependencies between events that are difficult to model using existing nonparametric Hawkes process variants. We demonstrate the improved predictive performance of our model against state-of-the-art baselines on multiple synthetic and real-world datasets.,Nonparametric Regressive Point Processes Based on

Conditional Gaussian Processes

Department of Computer Science

Department of Computer Science

Milos Hauskrecht

University of Pittsburgh
Pittsburgh  PA 15213
milos@pitt.edu

Siqi Liu

University of Pittsburgh
Pittsburgh  PA 15213

siqiliu@cs.pitt.edu

Abstract

Real-world event sequences consist of complex mixtures of different types of
events occurring in time. An event may depend on past events of the same type 
as well as  the other types. Point processes deﬁne a general class of models for
event sequences. “Regressive point processes” refer to point processes that directly
model the dependency between an event and any past event  an example of which
is a Hawkes process. In this work  we propose and develop a new nonparametric
regressive point process model based on Gaussian processes. We show that our
model can represent better many commonly observed real-world event sequences
and capture the dependencies between events that are difﬁcult to model using
existing nonparametric Hawkes process variants. We demonstrate the improved
predictive performance of our model against state-of-the-art baselines on multiple
synthetic and real-world datasets.

1

Introduction

Event sequences consist of timestamps of events occurring over a period of time. They arise commonly
in our everyday life. Examples are spread of news in a social network  buying and selling actions in
a stock market  occurrences of earthquakes in a region  administration of medications for a patient 
and many others. Because of their wide applicability  event sequences and their models have become
popular in machine learning research.
Point processes [Daley and Vere-Jones  2003  2007] can model event sequences by representing
events as points in the one-dimensional space: time. In general  a point process deﬁnes a probabilistic
distribution of points in a space. For a (temporal) point process  the distribution is uniquely determined
by its intensity function  which deﬁnes the rate of event occurring at any instant.
Two main types of point process models have been developed independently over years. One type is
what we call “regressive point processes”  where the dependencies of the intensity function on the
past events are directly modeled. Hawkes processes [Hawkes  1971] are the most studied and used
class of regressive point processes (e.g.  [Zhou et al.  2013a] [Zhou et al.  2013b] [Bacry and Muzy 
2014] [Lee et al.  2016] [Wang et al.  2016] [Xu et al.  2016] [Eichler et al.  2017]). A beneﬁt of the
regressive point processes is that they are easy to apply and interpret. They can be learned on a set
of sequences and then applied on another unseen set of sequences. Since the inﬂuence of each past
event on the intensity is explicitly modeled  it is easy to see how different types of events inﬂuence
each other over time.
Another type is what we call “latent-state point processes”  where the dependencies of the intensity
on the past events are indirectly modeled through a latent state. Based on the past latent state  we can
infer the future latent state and thereby predict future events. The most studied class of latent-state
point processes are Gaussian-process-modulated point processes (e.g.  [Adams et al.  2009] [Lasko 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

2014] [Rao and Teh  2011] [Gunter et al.  2014] [Lloyd et al.  2015] [Lloyd et al.  2016] [Ding et al. 
2018] [Kim  2018]). They use some transformation of a Gaussian process (GP) as the prior for the
intensity function  which provides a probabilistic distribution over possible intensity functions and
acts as the latent state. Then the posterior of the intensity function can be inferred from the data. The
main beneﬁt of GP-modulated point processes is that they provide a principled way to ﬂexibly model
the intensity functions. However  a signiﬁcant drawback is that they are harder to apply  compared
with Hawkes processes  due to the need of inferring a separate latent state for each sequence. To
make inference on any new sequence  long enough history of the sequence must be available. It is
impossible to learn a model from a set of sequences and apply it to other unseen sequences (i.e. cold
start).
In this work  we propose a new nonparametric model  GP regressive point process (GPRPP)  com-
bining the advantages of the above two models: the ﬂexibility of GP-modulated point processes
and the applicability of Hawkes processes. (A discussion of related work is in the supplementary
material.) Similar to Hawkes processes  our model directly captures the dependencies of the intensity
function on the past events. However  unlike Hawkes processes  the dependencies are modeled
nonparametrically through a GP. Meanwhile  different from GP-modulated point processes  the input
of our GP is not deﬁned by the “absolute” time relative to each sequence  but by the collection of
“relative” times from past events of different types. This deﬁnes a latent state independent of speciﬁc
sequences  and therefore can be learned from and applied to different sequences. Figure 1 illustrates
the differences between GPRPP and the previous models.
To better model the dependencies of the intensity function on the past events  we propose a conditional
GP model for GPRPP. It relies on a set of points introduced in the input space of the GP to capture
the dependencies independent of speciﬁc sequences. These points  although bearing a similarity to
the inducing points in sparse GPs [Quiñonero-Candela and Rasmussen  2005  Titsias  2009]  function
quite differently  because instead of marginalizing them out  we condition on them for inference.

(a) Hawkes process

(b) GP-modulated point process

(c) GP regressive point process

Figure 1: Illustrations of different point process models. The ﬁrst three rows are a multivariate event
sequence consisting of events (stems) of three types (u) on the timeline. The vertical line t marks the
current time. The last row is the estimated conditional intensity function (CIF) λ3(·) for event type
u = 3. (a) In a Hawkes process  the CIF depends on the past events through the triggering kernels
φuiuj (Eq. 1). (b) In a GP-modulated point process  the entire (transformed) CIF on the speciﬁc
sequence is a function with a GP prior. (c) In a GPRPP  the (transformed) CIF depends on the past
events through a function with a GP prior.

2 Preliminary: point processes
The training data consist of multiple sequences D = {yc}|D|
c=1. Each yc is a sequence of (time 
label) pairs yc = {(ti  ui)}|yc|
i=1  representing the time and the type of each event  where ti ∈ R≥0
and ui = 1  . . .   U. A (temporal) point process has a conditional intensity function (CIF) λ(t) =
as the instantaneous rate of events at time t given the history Ht up to t 
limdt→0+
where N (·) counts the number of events in an interval. For example  for a Hawkes process  the CIF
of event type ui is deﬁned as

E[N ([t t+dt))|Ht]

dt

(cid:88)

λui (t) = µui +

φuiuj (t − tj)

(1)

where φuiuj is a function of time that characterizes the inﬂuence of past events of type uj on type ui.
It is called a triggering kernel in previous works. Meanwhile  µui deﬁnes the baseline intensity.

tj <t

2

Given the CIF  the probability density of the data D is

|D|(cid:89)

p(D) =

|D|(cid:89)

|yc|(cid:89)

p(yc) =

λui(ti) exp

c=1

c=1

i=1

(cid:32)

−

(cid:90) T c

U(cid:88)

tc
0

u=1

(cid:33)

λu(t)dt

(2)

0 and T c are the start and end time of yc. Without loss of generality  we assume tc

where tc
0 = 0 from
now on. We note that the density of the data factorizes over the individual sequences yc (Eq. 2)  while
the density of each sequence factorizes over the event types (Eq. 3):

U(cid:89)

|yc|(cid:89)

u=1

i=1

(cid:32)

−

(cid:90) T c

tc
0

(cid:33) (cid:44) U(cid:89)

u=1

p(yc) =

λu(ti)δ(ui u) exp

λu(t)dt

pu(yc)

(3)

where δ(x  y) = 1 when x = y  and 0 otherwise. In this way  a point process can be viewed as a set
of sub-models  one for each type of events. The total likelihood is the product of the likelihood of all
the sub-models. For a Hawkes process  each sub-model is similar to a regression model  where the
predictors are the elapsed times since the past events of all types  transformed through the triggering
kernels.

3 GP regressive point processes

U (t))2

λ˜u(t) = f (x(t))2 = f (t − s1

Inspired by the regressive view of Hawkes processes  we propose a new model based on Gaussian
processes (GPs). Since the density factorizes over sequences (Eq. 2)  we describe the method for one
sequence yc in the following. To avoid cluttering  we use y to denote the sequence. Since the density
of each sequence factorizes over event types (Eq. 3)  we describe the method for p˜u(y) of one type
˜u (target type) and call the events of type ˜u the target events. The same method can be repetitively
applied to all pu(y)  u = 1  . . .   U. However  we stress that even for one target type ˜u  p˜u(y) can
depend on all types of events in the history through the CIF (Eq. 1 and 4).
In our model  for each target type ˜u  the CIF λ˜u(t) is a transformation of a function f drawn from
a GP with mean µ and covariance function K  f ∼ GP(µ  K). We note that f  µ and K can be
different for λu(t) of a different type u. The input of f consists of the elapsed times since the last Q
events of all the types. That is  f : X → R  where X ⊆ RD≥0  D = U × Q  and U is the size of the
label set. To convert it to a valid CIF  we use the square transformation
1(t)  . . .   t − sQ

(4)
u(t) is the time of the q-th (from last) event of type u before time t  which could be undeﬁned
where sq
when no such event exists. The input of the GP is x(t) = (t − sq
u=1 q=1 ∈ X . That is  x
depends on the current time t and the last Q events of all types. The d-th dimension of x(t) is
xd(t) = t − sq
u(t)  corresponding to the time elapsed since the q-th (from last) event of type u. In
fact  Q does not have to be the same for each type u  i.e.  we can have a different Qu for each type u 
but for notational simplicity  we use Q as if it were the same for all types. We note that the CIF of the
model directly depends on the past events and call the model a GP regressive point process (GPRPP).
The square transformation ensures nonnegativity of λ and enables closed-form evaluation of the
integrals in the likelihood as shown in the next section. It was originally proposed in [Lloyd et al. 
2015] for GP-modulated point processes for event sequences without types and then exploited in
later works (e.g  [Lloyd et al.  2016  Ding et al.  2018]). Compared with these works  where the GP
is a function of the single “absolute” time  in our model  the GP is a function of multiple “relative”
times since past events  which keeps changing as new events happen. This makes the inference much
harder  and new efﬁcient algorithms to evaluate the integrals are developed in this work.
A key challenge in the model deﬁnition is to deal with undeﬁned inputs. That is  inevitably at some
time t (e.g.  at the very beginning of a sequence)  the q-th (from last) event of type u may not exist.
Inspired by Hawkes processes  we come up with a novel kernel for the GP. We start by augmenting
each input xd(t) with an additional indicator I [xd(t)] to indicate whether the q-th (from last) event of
type u exists  i.e.  whether sq
u(t) is undeﬁned (there
are less than q events of type u in the past)  we can set a dummy value for xd(t) (e.g.  xd(t) = ∞) 
and I [xd(t)] = 0. Otherwise  xd(t) = t − sq
u(t) as before  and I [xd(t)] = 1. The dimensionality of

u(t) (correspondingly xd(t)) is deﬁned. When sq

u(t))U Q

3

the input essentially becomes 2D  but for notational simplicity  the indicators are implicit in x(t).
We deﬁne the kernel as

K(x(t)  x(cid:48)(t(cid:48))) =

I [xd(t)] I [x(cid:48)

(cid:123)(cid:122)

K1

(cid:125)
d(t(cid:48))]

γd exp

(cid:124)

− (xd(t) − x(cid:48)

d(t(cid:48)))2

2αd

(cid:123)(cid:122)

K2

(5)

D(cid:88)

d=1

(cid:124)

(cid:18)

(cid:19)
(cid:125)

where γd  αd > 0. This is essentially a sum of D kernels  each of which is a product of two kernels
K1 and K2. K2 is the squared-exponential kernel on the value of xd(t). K1 is the inner product
on the indicator I [xd(t)]. We use the squared-exponential kernel  because it is widely used and has
closed-form evaluations of ψ and Ψ as shown in the next section  but it can be replaced by any kernel
with the latter property.
Remark 1. The two inputs of the kernel have different notations for x and t  indicating they can
come from different sequences at different absolute times. The kernel is actually isolated from the
absolute time t in the individual sequences  since it only depends on the value of x(·) at t. This is
very different from previous GP-based models (e.g.  [Lloyd et al.  2015  2016  Ding et al.  2018]) 
where the inputs of the kernel always come from the same sequence  and the kernel depends on the
absolute time t in the sequence.

We make the following two assumptions to justify the deﬁnition of our model. A proof of Theorem 1
is in the supplementary material.
Assumption 1. For each type u of events  they have time-limited inﬂuences on the target events.
That is  there is a time limit  ∆Tu < ∞  for each type u of event  such that for any event of type u
occurring at su  λ˜u(t) may depend on su only if 0 < t − su ≤ ∆Tu.
Assumption 2. For each type u of events  there exists Mu : R → Z such that for any bounded time
interval I = [tbeg  tend)  |I| = tend − tbeg < ∞  the number of events Nu(I) ≤ Mu(|I|) < ∞.
Theorem 1. Given that assumption 1 and 2 hold  there exists Q < ∞ such that λ˜u(t) depends on at
most the last Q events of any type at any time t.

4 Conditional GPRPP

In this section  we propose a conditional GP model for GPRPP and call it conditional GP regressive
point process (CGPRPP). The input of the GP is deﬁned in the previous section and denoted as
x = x(t). When t is not important or clear from the context  we just denote the input as x.
Previously  different forms of sparse GPs based on inducing variables have been proposed to improve
the efﬁciency of GPs (e.g.  [Quiñonero-Candela and Rasmussen  2005  Titsias  2009]). Typically  a
set of inducing points are introduced in the input space  and the inducing variables corresponding to
the points are marginalized out for learning and inference. Our idea is similar  but the difference is
that we condition on the inducing variables  which correspond to the values of the CIF given different
situations of the history. Therefore  we call these points conditional points.
Let Z ∈ X M be a sequence of M conditional points in the input space. We will explain how to
pick these points later. Given any input x ∈ X   the output of the GP is fx = f (x) ∈ R. Let
fZ = f (Z) ∈ RM . We deﬁne µx and µZ as the prior mean µ of the appropriate dimensions for x
and Z respectively. Let Kxx(cid:48) = K(x  x(cid:48)) as deﬁned in Eq. 5 for any inputs x and x(cid:48). If x and x(cid:48)
are vectors  Kxx(cid:48) is the Gram matrix of the corresponding size. Then p(fZ) = N (µZ  KZZ)  and
p(fx|fZ) = N (µx|Z  σ2

x|Z)  where

µx|Z = µx + KxZK−1

x|Z = Kxx − KxZK−1
σ2

ZZKZx

From Eq. 3 and 4  the conditional density of the sequence y given fx is

ln p˜u(y|fx) =

δ(un  ˜u) ln f (x(tn))2 −

f (x(t))2dt

where N = |y| is the total number of all types of events in y.
Remark 2. It is worth noting that the conditional points in Z are independent of any speciﬁc sequence
y  since they are points in X .

(cid:90) T

0

(6)

(7)

ZZ(fZ − µZ) 
N(cid:88)

n=1

4

Assuming we observe fZ = mZ  we can maximize the conditional density p˜u(y|mZ) to learn the
hyper-parameters of the model:

(cid:90)

ln p˜u(y|mZ) = ln

p˜u(y|fx)p(fx|mZ)dfx

where p(fx|mZ) is deﬁned by Eq. 6 with fZ = mZ. Because there is a correspondence between
f (x(t)) and the CIF λ(t)  mZ essentially corresponds to different values of the CIF given different
situations of the history  determined by different z ∈ Z. Even given the exact same history  the CIF
may still be stochastic. Therefore  we allow noise in fZ  which is a generalization of the noiseless
case  so fZ = mZ + Z  where p(Z) = N (0  S). Then we can marginalize out Z by integrating
w.r.t. p(z). In the end  we maximize

ln p˜u(y|mZ) = ln

p˜u(y|fx)p(fx|mZ  Z)p(Z)dfxdZ = ln

p˜u(y|fx)p(fx|mZ)dfx

(8)

(cid:90)(cid:90)

(cid:90)

where p(fx|mZ  Z) is deﬁned by Eq. 6  and

(cid:90)

p(fx|mZ) =

p(fx|mZ  Z)p(Z)dZ = N (˜µx  ˜σ2
x)

(cid:90)
N(cid:88)

n=1

has a closed-form solution
˜µx = µx + KxZK−1
(9)
Remark 3. Because we condition on the pseudo-observations mZ  they can move freely when we
optimize Eq. 8  and their values are determined by ﬁtting to the training data. Intuitively  they act as
key points of the CIF  which are supposed to capture the key information regarding the entire CIF.

x = Kxx − KxZK−1
˜σ2

ZZKZx + KxZK−1

ZZ(mZ − µZ) 

ZZSK−1

ZZKZx.

Eq. 8 is hard to maximize directly. Instead  we derive a lower bound using Jensen’s inequality and
maximize the lower bound

ln

p˜u(y|fx)p(fx|mZ)dfx = ln E [p˜u(y|fx)] ≥ E [ln p˜u(y|fx)]

where the expectation is w.r.t. p(fx|mZ)  and from Eq. 7
E [ln p˜u(y|fx)] =

δ(un  ˜u)E(cid:2)ln f (x(tn))2(cid:3) − N +1(cid:88)
(cid:19)

E(cid:2)ln f (x(tn))2(cid:3) = − ˜G

(cid:18)

n=1

− ˜µ2
x
2˜σ2
x

(cid:90) tn

tn−1

where we deﬁne t0 = 0 and tN +1 = T to be the start and end time of the sequence y.
From [Lloyd et al.  2015]  we have

(cid:16)E [f (x(t))]2 + Var [f (x(t))]
(cid:17)
(cid:19)
(cid:18) ˜σ2

− C

(11)
where C ≈ 0.57721566 is the Euler-Mascheroni constant  ˜G is deﬁned via the conﬂuent hypergeo-
metric function  and ˜µ2
Meanwhile 

x = Var [fx] can be computed as in Eq. 9.

x = E [fx]2   ˜σ2

+ ln

x
2

(10)

dt

(cid:90) tn
(cid:90) tn

tn−1

tn−1

E [fx]2 dt =(tn − tn−1)µ2

x + 2µxψT

n K−1
ZZΨnK−1

ZZ(mZ − µZ)
ZZ(mZ − µZ) 

γdI [xd(t)] dt − Tr(cid:0)K−1

ZZΨn

(cid:1) + Tr(cid:0)K−1

+ (mZ − µZ)T K−1

(cid:90) tn

D(cid:88)

d=1

tn−1

Var [fx] dt =

(cid:1) .

(12)

(13)

ZZSK−1

ZZΨn

The deﬁnitions of ψ and Ψ are complex and included in the supplementary material. We note that the
computation of them is the bottleneck of the learning algorithm. A straightforward algorithm would
cost O(M 2N D2). However  by merging computation related to the same event type  we can reduce
the complexity to O(M 2N DQ). By setting each conditional point active on only one dimension 
we can reduce the complexity to O(M 2N Q). If we further set Q˜u > 1 only for the target type and
Qu = 1 for u (cid:54)= ˜u  and assume N˜uQ˜u = O(N )  where Nu is the number of points of type u  the
complexity can be reduced to O(M 2N )  which is what we adopt in the experiments. The details and
a proof are in the supplementary material.

5

Learning We also add an independent noise kernel σ2I to the existing kernel (Eq. 5)  which results
in a new term in the integral of the variance (Eq. 13). For learning the model  we maximize the lower
bound (Eq. 10) w.r.t. the set of hyper-parameters Θ = {µ  α  γ  σ  mZ  S}.
We assume that mZ provides sufﬁcient information for the inference on the test data D∗.
Assumption 3. Conditioned on mZ  the test data D∗ is independent from the training data D 
p(D∗|D  mZ) = p(D∗|mZ).

Inference For inference of the test likelihood  to compare with non-Bayesian models  we use a
point estimate of the CIF  instead of model averaging. We use the optimal hyper-parameters Θ∗
learned from the training data D to estimate the mean CIF

˜u(t) = E [λ˜u(t)|Θ∗] = E [f (x(t))|Θ∗]2 + Var [f (x(t))|Θ∗]
λ∗

(14)
on the test data D∗  where the conditional mean and variance are deﬁned in Eq. 9. Then we use the
mean CIF as our prediction to compute the likelihood p(D∗|λ∗) on the test data. For prediction of
event times  a sampling-based algorithm is derived in the supplementary material.

Conditional point placement Due to the high dimensionality of the input space of f  it is preferable
that the conditional points are placed beforehand and ﬁxed in the learning procedure. Based on the
additive form of our kernel  we place the conditional points independently on each dimension. Each
conditional point will be active on only one dimension. In our experiments  for simplicity  we put the
conditional points regularly on each dimension within a region. If prior knowledge is available  it can
be used to determine the region; otherwise  we can use the following heuristics. The left bound of the
region is usually 0. The right bound can be set to the maximum (or some quantile) of the time span
between two (Q = 1) or more (Q > 1) consecutive points of the same type  since beyond that  the
conditional points will have limited effects.

5 Experiments

We compare our method with two state-of-the-art nonparametric Hawkes process variants. HP-GS
[Xu et al.  2016] is a nonparametric Hawkes process using a set of (Gaussian) basis functions to
approximate the triggering kernels  with sparse and group lasso regularization. For each experiment 
we tune its tuning parameters αS and αG in a wide range {10−2  10−1  . . .   104} as in the original
work using cross-validation based on the likelihood. In all the experiments  the bandwidth of the
Gaussian kernels is set to be optimal  that is the inverse of the cut-off frequency  based on the positions
of the kernels. The cut-off frequency ω0 = πM/T   where M is the number of kernels and T is the
right bound on the kernels. HP-LS [Eichler et al.  2017] is another nonparametric Hawkes process.
This method allows very ﬂexible triggering kernels to be estimated by discretizing the kernels and
solving a least-square problem. Its parameters are set in accordance with the other methods for each
experiment. For our method  to improve efﬁciency  when we set Q > 1  we only set it for the target
type and keep Q = 1 for the others  as discussed in the supplementary material. We tie the parameters
for different dimensions q = 1  . . .   Q of the same type u.

5.1 Synthetic datasets

First we generate two synthetic datasets representing two distinctive types of event sequences using
the thinning algorithm [Ogata  1981]. The ﬁrst dataset is generated through a renewal process.
The baseline intensity is µ. When there is a new event  the intensity temporarily becomes A(1 −
sin(2πt/τ ))  for a limited time t ∈ (0  τ ) after the event. Each new event will reset the intensity. We
set µ = 0.1  A = 0.1  τ = 20.
The second dataset is generated through a Hawkes process. The baseline intensity is µ. The
triggering kernel is A exp(−(t − b)2/σ2)  i.e.  a Gaussian kernel. Different from the renewal
process  each new event will add a new Gaussian kernel on top of the existing intensity. We set
µ = 0.1  A = 0.1  b = 10  σ2 = 4.
For each dataset we generate 200 sequences of length of 100 time units each. Each dataset is split into
100 training sequences and 100 testing sequences. For the ﬁrst dataset  we set Q = 1 and conditional
points at 0  5  . . .   15 for CGPRPP. For HP-GS  the kernels are also placed at 0  5  . . .   15. For HP-LS 

6

we set h = 1  k = 20. For the second dataset  we use the same settings for all the methods  except
that we vary Q = 1  5  10  20  40 for CGPRPP to see the effect of adding more regression terms.
We visualize the inﬂuence from a past event of a speciﬁc type to the target events as the changes
in the intensity of the target type over time since that event. For Hawkes processes  it is similar to
plotting the triggering kernels  except that the triggering kernels are added on top of the baseline
intensity  so we can compare the intensity after an event with the baseline intensity. For CGPRPP  it
is equivalent to simulating an event at time 0 and plotting the changes in the intensity as time elapses.
The true inﬂuence functions are in the ﬁrst column in Figure 2  followed by the inferred inﬂuence
functions for each method. For the ﬁrst dataset  HP-GS cannot learn the inﬂuence function  because
its limitation in the dependencies of the CIF on the past events. Although the triggering kernels
are nonparametric  the baseline intensity and the triggering kernels are additive in the CIF. This
limitation is quite common in nonparametric Hawkes processes (e.g.  [Zhang et al.  2018  Donnet
et al.  2018  Zhou et al.  2013b]). HP-LS is more ﬂexible and learns a better inﬂuence function  but
the discretization tends to make the function noisy. CGPRPP almost completely recovers the true
inﬂuence function. We note that this inﬂuence represents an inhibition followed by an excitation 
which is common in practice such as neural spike trains [Eichler et al.  2017]. However  most Hawkes
process variants can only model either excitations or inhibitions  but not a mix of both at the same
time. In contrast  CGPRPP models the whole CIF as a nonparametric function of the past events and
therefore can model these more complex dependencies.
For the second dataset  HP-GS is a perfect match for the data  so unsurprisingly it recovers the
inﬂuence function very well. HP-LS also learns the inﬂuence reasonably well  although still suffering
from discretization. Interestingly  CGPRPP with Q = 40 (similar for Q = 10  20) learns an inﬂuence
function very close to HP-GS.
The test log-likelihood on the synthetic datasets are shown in Table 1. CGPRPP performs the best on
the ﬁrst dataset  while HP-GS and CGPRPP perform similarly on the second dataset  with CGPRPP
being marginally better. The likelihood results are concordant with how well the models recovered
the inﬂuence function. For the second dataset  we show the performance of CGPRPP selected with
the training likelihood (Q = 40). A comparison of GPRPP based on variational sparse GP [Lloyd
et al.  2015] and conditional GP  and the effect of varying Q on the performance of CGPRPP are in
the supplementary material.

(a) Ground truth

(b) HP-GS

(c) HP-LS

(d) CGPRPP

Figure 2: Inﬂuences from past events on the ﬁrst (top) and second (bottom) synthetic datasets. Solid
lines are the CIFs after an event. Dashed lines are the baseline intensities. The ground truth is in the
ﬁrst column  followed by the result of each method.

7

Time elapsed01020Influence00.050.10.150.2Time elapsed01020Influence0.070.0720.0740.0760.078Time elapsed01020Influence0.040.060.080.10.120.14Time elapsed01020Influence00.050.10.15Time elapsed01020Influence0.10.150.2Time elapsed01020Influence0.10.120.140.160.18Time elapsed01020Influence0.150.20.25Time elapsed01020Influence0.10.120.140.16Table 1: Test log-likelihood on synthetic
datasets.
Data HP-GS HP-LS CGPRPP
1
2

-2455
-4071

-2770
-4161

-2671
-4074

Table 2: Test log-likelihood on IPTV dataset.
CGPRPP
-1.479e+05
-1.502e+05
-1.608e+05

Month HP-GS
1
2
3

HP-LS
-1.779e+05
-1.825e+05
-1.928e+05

-1.477e+05
-1.509e+05
-1.608e+05

5.2

IPTV dataset

The IPTV dataset consists of TV viewing records of users over 11 months [Luo et al.  2014  Xu et al. 
2016]. Each sequence consists of times and types of the TV programs viewed by a user. Events in
this dataset are generally very bursty  i.e.  one event tends to trigger a group of events of the same
type happening in a relatively short amount of time  while the distance between these burst groups
are relatively large. This is a distinctive characteristic of data generated by Hawkes processes  so we
expect the Hawkes process baselines to perform well. To the best of our knowledge  HP-GS has the
best performance on this dataset  but our goal is to conﬁrm whether CGPRPP can also ﬁt the data
well and achieve similar or better performance.
The data are extracted from THAP1 [Xu and Zha  2017]  which contain 302 users in total. For
efﬁciency  we randomly sample 200 users and use 100 for training and the others for testing. All
the models are trained on 1 month and tested on the following 3 months. More details are in the
supplementary material. For HP-GS  we put the kernels at every 20 minutes from 0 up to 24 hours 
since the length of most TV programs is about 20 to 40 minutes [Xu et al.  2016]. For HP-LS  we
train multiple models with h = 1.25  5  20 minutes and k = (24 ∗ 60 + 20)/h correspondingly.
For CGPRPP  the conditional points are also placed at every 20 minutes up to 24 hours. We set
Q = 5  10  20 and select Q based on the training likelihood.
Table 2 shows the test log-likelihood of the models on different months. This is the total log-likelihood
of all types of events. For HP-LS  we show the best test log-likelihood across different h and k. As
expected  HP-GS performs the best  conﬁrming the bursty characteristic of the data. However  HP-LS
does not perform well. A problem of HP-LS is that the discretization tends to make the inﬂuence
function noisy and fail to generalize well. CGPRPP has a competitive performance close to HP-GS 
showing its capability to model bursty events.

5.3 MIMIC datasets

To show the ﬂexibility of CGPRPP in modeling other complex event patterns than the bursty patterns
as in many previously used datasets similar to the IPTV dataset  we derive multiple new event
sequence datasets from MIMIC III [Johnson et al.  2016] consisting of lab tests ordered to patients in
a hospital. Lab orders tend to have more complex dependencies such as a complex mix of multiple
inhibitions and excitations over time (e.g.  see Figure 3).
Since there are labs that tend to occur together  we group them into several lab classes. We extract
20 different datasets targeting the most frequent 20 classes. Each dataset consists of 10 different lab
classes  one of which is the target we try to predict  while the others are the predictors. We sample
200 admissions (sequences) randomly from each dataset  where 100 admissions are used for training
and the others for testing. More details are in the supplementary material.
For HP-GS  we put the kernels at 0  8  . . .   48 hours. We also test a different version of the method 
HP-GS-A  using the adaptive basis-function-selection algorithm in [Xu et al.  2016] to place the
kernels. For HP-LS  we train multiple models with h = 0.5  2  8 hours and k = (48 + 8)/h
correspondingly. For CGPRPP  the conditional points are also placed at 0  8  . . .   48 hours. We set
Q = 1  10 and select Q based on the training likelihood. As a reference  we also test against a model
based on deep neural networks  the neural self-modulating multivariate point process (NSMMPP)
[Mei and Eisner  2017]. The number of hidden units is selected from 64  128  . . .   1024 as in the
original work through a validation set (80/20 split from the full training set).

1https://github.com/HongtengXu/Hawkes-Process-Toolkit

8

The test log-likelihood of the models is shown in Table 3. Each column is a different dataset with a
different target lab class. They are ordered from the most frequent (355) to the least frequent (18)
based on their occurrences. For HP-LS  we show the best test log-likelihood across different h and k
on each dataset. CGPRPP achieves the best or close to the best performance on all datasets except
class 550 and 18. On class 355  60  151  113  and 140  CGPRPP outperforms the second best by a
large margin. In some cases (e.g.  class 550) CGPRPP with a different Q actually has a much better
result  although not being selected. We also conducted time prediction experiments to predict the
time for each target event. The results also show the advantage of CGPRPP. The full likelihood and
time prediction results are in the supplementary material.
As an example  we plot the inﬂuence functions for class 355 from past events of the same type in
Figure 3. HP-GS learns a smooth inﬂuence function with excitations around 24 and 48 hours. This
corresponds to the fact that right after a lab being ordered  it might need to be repeated after one or
two days. In contrast  HP-LS (h = 0.5 with the best test likelihood) learns a much noisier pattern
due to discretization  which is harder to interpret. Compared with HP-GS  CGPRPP learns not only
similar excitations around 24 and 48 hours  but also a strong inhibition after each excitation  showing
a more ﬂexible ﬁt to the data.

355
Dataset
HP-GS
-3668
HP-GS-A -3947
HP-LS
-6510
-3664
NSMMPP
-3249
CGPRPP
294
Dataset
-1011
HP-GS
HP-GS-A -1054
-1308
HP-LS
-941.2
NSMMPP
CGPRPP
-993.6

60
-4673
-5051
-7299
-4660
-4246
17
-3783
-3807
-5339
-3758
-3808

3
-3721
-3733
-5722
-3737
-3759

150
-3238
-3537
-4894
-3377
-3100

95
-4064
-4390
-5712
-3982
-3933
80
-3388
-3772
-5365
-3903
-3402

354
-4344
-4792
-7185
-4409
-4225
1
-3220
-3291
-3772
-3228
-3234

151
-3338
-3574
-5323
-3763
-3093
53
-1913
-2138
-2963
-1916
-1900

394
-3098
-3251
-4945
-3268
-3010

Table 3: Test log-likelihood on MIMIC lab order datasets.
550
-1053
-1064
-1744
-1039
-1175

368
-3366
-3711
-5625
-3309
-3378

113
-4656
-5049
-7143
-4539
-4276
8
-1633
-1667
-3142
-1786
-1694

140
-3206
-3475
-4625
-3244
-2942
18
-1596
-1678
-3085
-1532
-1648

7
-2502
-2533
-3514
-2626
-2512

(a) HP-GS

(b) HP-LS

(c) CGPRPP

Figure 3: Inﬂuences from past events of the same type as the target class 355 on the MIMIC dataset.

6 Conclusion

In this work  we proposed a new nonparametric method for modeling dependencies between events in
event sequences using Gaussian processes. Similar to Hawkes processes and different from previous
GP-modulated point processes  the proposed model can be learned on a sample of sequences and
then applied to other unseen sequences. However  we showed that the proposed model is more
ﬂexible than state-of-the-art nonparametric Hawkes process variants. It can learn the dependencies
between events that are common in practice but difﬁcult for the Hawkes process variants to represent 
e.g.  a mix of inhibitions and excitations after an event. Our method showed competitive or better
performance on different datasets compared with the Hawkes process variants.

9

Time elapsed050Influence0.020.030.04Time elapsed050Influence00.050.10.15Time elapsed050Influence00.050.1Acknowledgement

This work was supported by NIH grant R01-GM088224. Siqi Liu was also supported by CS50 Merit
Pre-doctoral Fellowship from the Department of Computer Science  University of Pittsburgh. The
content of this paper is solely the responsibility of the authors and does not necessarily represent the
ofﬁcial views of the NIH.

References
Daryl J. Daley and David Vere-Jones. An Introduction to the Theory of Point Processes: Volume I:

Elementary Theory and Methods. Springer  New York  2003.

Daryl J. Daley and David Vere-Jones. An Introduction to the Theory of Point Processes: Volume II:

General Theory and Structure. Springer Science & Business Media  2007.

Alan G. Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika 

58(1):83–90  1971. ISSN 0006-3444. doi: 10.2307/2334319.

Ke Zhou  Hongyuan Zha  and Le Song. Learning social infectivity in sparse low-rank networks using
multi-dimensional Hawkes processes. In Artiﬁcial Intelligence and Statistics  pages 641–649 
2013a.

Ke Zhou  Hongyuan Zha  and Le Song. Learning triggering kernels for multi-dimensional Hawkes

processes. In International Conference on Machine Learning  pages 1301–1309  2013b.

Emmanuel Bacry and Jean-Francois Muzy. Second order statistics characterization of Hawkes

processes and non-parametric estimation. arXiv:1401.0903 [physics  q-ﬁn  stat]  January 2014.

Young Lee  Kar Wai Lim  and Cheng Soon Ong. Hawkes processes with stochastic excitations. In

International Conference on Machine Learning  pages 79–88  2016.

Yichen Wang  Bo Xie  Nan Du  and Le Song. Isotonic Hawkes processes. In International Conference

on Machine Learning  pages 2226–2234  2016.

Hongteng Xu  Mehrdad Farajtabar  and Hongyuan Zha. Learning Granger causality for Hawkes

processes. In International Conference on Machine Learning  pages 1717–1726  2016.

Michael Eichler  Rainer Dahlhaus  and Johannes Dueck. Graphical modeling for multivariate Hawkes
processes with nonparametric link functions. Journal of Time Series Analysis  38(2):225–242 
March 2017. ISSN 01439782. doi: 10.1111/jtsa.12213.

Ryan Prescott Adams  Iain Murray  and David JC MacKay. Tractable nonparametric Bayesian
inference in Poisson processes with Gaussian process intensities. In Proceedings of the 26th
Annual International Conference on Machine Learning  pages 9–16. ACM  2009.

Thomas A. Lasko. Efﬁcient inference of Gaussian-process-modulated renewal processes with
application to medical event data. In Proceedings of the Thirtieth Conference on Uncertainty in
Artiﬁcial Intelligence  UAI’14  pages 469–476  Arlington  Virginia  United States  2014. AUAI
Press. ISBN 978-0-9749039-1-0.

Vinayak Rao and Yee W. Teh. Gaussian process modulated renewal processes. In Advances in Neural

Information Processing Systems  pages 2474–2482  2011.

Tom Gunter  Chris Lloyd  Michael A. Osborne  and Stephen J. Roberts. Efﬁcient Bayesian nonpara-
metric modelling of structured point processes. In Proceedings of the Thirtieth Conference on
Uncertainty in Artiﬁcial Intelligence  UAI’14  pages 310–319  Arlington  Virginia  United States 
2014. AUAI Press. ISBN 978-0-9749039-1-0.

Chris Lloyd  Tom Gunter  Michael Osborne  and Stephen Roberts. Variational inference for Gaussian
process modulated Poisson processes. In International Conference on Machine Learning  pages
1814–1822  2015.

Chris Lloyd  Tom Gunter  Michael Osborne  Stephen Roberts  and Tom Nickson. Latent point process

allocation. In Artiﬁcial Intelligence and Statistics  pages 389–397  May 2016.

10

Hongyi Ding  Mohammad Khan  Issei Sato  and Masashi Sugiyama. Bayesian nonparametric
Poisson-process allocation for time-sequence modeling. In International Conference on Artiﬁcial
Intelligence and Statistics  pages 1108–1116  2018.

Minyoung Kim. Markov modulated Gaussian Cox processes for semi-stationary intensity modeling
of events data. In International Conference on Machine Learning  pages 2640–2648  July 2018.

Joaquin Quiñonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate
Gaussian process regression. Journal of Machine Learning Research  6:1939–1959  December
2005. ISSN 1532-4435.

Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Artiﬁcial

Intelligence and Statistics  pages 567–574  2009.

Y. Ogata. On Lewis’ simulation method for point processes. IEEE Transactions on Information

Theory  27(1):23–31  January 1981. ISSN 0018-9448. doi: 10.1109/TIT.1981.1056305.

Rui Zhang  Christian Walder  Marian-Andrei Rizoiu  and Lexing Xie. Efﬁcient non-parametric

Bayesian Hawkes processes. arXiv:1810.03730 [cs  stat]  October 2018.

Sophie Donnet  Vincent Rivoirard  and Judith Rousseau. Nonparametric Bayesian estimation of

multivariate Hawkes processes. arXiv:1802.05975 [math  stat]  February 2018.

D. Luo  H. Xu  H. Zha  J. Du  R. Xie  X. Yang  and W. Zhang. You are what you watch and
when you watch: Inferring household structures from IPTV viewing data. IEEE Transactions on
Broadcasting  60(1):61–72  March 2014. ISSN 0018-9316. doi: 10.1109/TBC.2013.2295894.

Hongteng Xu and Hongyuan Zha. THAP: A matlab toolkit for learning with Hawkes processes.

arXiv:1708.09252 [cs  stat]  August 2017.

Alistair E. W. Johnson  Tom J. Pollard  Lu Shen  Li-wei H. Lehman  Mengling Feng  Mohammad
Ghassemi  Benjamin Moody  Peter Szolovits  Leo Anthony Celi  and Roger G. Mark. MIMIC-III 
a freely accessible critical care database. Scientiﬁc Data  3:160035  May 2016. ISSN 2052-4463.
doi: 10.1038/sdata.2016.35.

Hongyuan Mei and Jason M. Eisner. The neural Hawkes process: A neurally self-modulating
multivariate point process. In Advances in Neural Information Processing Systems  pages 6757–
6767  2017.

11

,Siqi Liu
Milos Hauskrecht