2017,Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding,Neural time-series data contain a wide variety of prototypical signal waveforms (atoms) that are of significant importance in clinical and cognitive research. One of the goals for analyzing such data is hence to extract such `shift-invariant' atoms. Even though some success has been reported with existing algorithms  they are limited in applicability due to their heuristic nature. Moreover  they are often vulnerable to artifacts and impulsive noise  which are typically present in raw neural recordings. In this study  we address these issues and propose a novel probabilistic convolutional sparse coding (CSC) model for learning shift-invariant atoms from raw neural signals containing potentially severe artifacts. In the core of our model  which we call $\alpha$CSC  lies a family of heavy-tailed distributions called $\alpha$-stable distributions. We develop a novel  computationally efficient Monte Carlo expectation-maximization algorithm for inference. The maximization step boils down to a weighted CSC problem  for which we  develop a computationally efficient optimization algorithm. Our results show that the proposed algorithm achieves state-of-the-art convergence speeds. Besides  $\alpha$CSC is significantly more robust to artifacts when compared to three competing algorithms: it can extract spike bursts  oscillations  and even reveal more subtle phenomena such as cross-frequency coupling when applied to noisy neural time series.,Learning the Morphology of Brain Signals Using

Alpha-Stable Convolutional Sparse Coding

Mainak Jas1  Tom Dupré La Tour1  Umut ¸Sim¸sekli1  Alexandre Gramfort1 2

1: LTCI  Telecom ParisTech  Université Paris-Saclay  Paris  France

2: INRIA  Université Paris-Saclay  Saclay  France

Abstract

Neural time-series data contain a wide variety of prototypical signal waveforms
(atoms) that are of signiﬁcant importance in clinical and cognitive research. One of
the goals for analyzing such data is hence to extract such ‘shift-invariant’ atoms.
Even though some success has been reported with existing algorithms  they are
limited in applicability due to their heuristic nature. Moreover  they are often
vulnerable to artifacts and impulsive noise  which are typically present in raw
neural recordings. In this study  we address these issues and propose a novel
probabilistic convolutional sparse coding (CSC) model for learning shift-invariant
atoms from raw neural signals containing potentially severe artifacts. In the core of
our model  which we call αCSC  lies a family of heavy-tailed distributions called
α-stable distributions. We develop a novel  computationally efﬁcient Monte Carlo
expectation-maximization algorithm for inference. The maximization step boils
down to a weighted CSC problem  for which we develop a computationally efﬁcient
optimization algorithm. Our results show that the proposed algorithm achieves
state-of-the-art convergence speeds. Besides  αCSC is signiﬁcantly more robust to
artifacts when compared to three competing algorithms: it can extract spike bursts 
oscillations  and even reveal more subtle phenomena such as cross-frequency
coupling when applied to noisy neural time series.

1

Introduction

Neural time series data  either non-invasive such as electroencephalograhy (EEG) or invasive such as
electrocorticography (ECoG) and local ﬁeld potentials (LFP)  are fundamental to modern experimental
neuroscience. Such recordings contain a wide variety of ‘prototypical signals’ that range from beta
rhythms (12–30 Hz) in motor imagery tasks and alpha oscillations (8–12 Hz) involved in attention
mechanisms  to spindles in sleep studies  and the classical P300 event related potential  a biomarker for
surprise. These prototypical waveforms are considered critical in clinical and cognitive research [1] 
thereby motivating the development of computational tools for learning such signals from data.
Despite the underlying complexity in the morphology of neural signals  the majority of the computa-
tional tools in the community are based on representing the signals with rather simple  predeﬁned
bases  such as the Fourier or wavelet bases [2]. While such bases lead to computationally efﬁcient
algorithms  they often fall short at capturing the precise morphology of signal waveforms  as demon-
strated by a number of recent studies [3  4]. An example of such a failure is the disambiguation of
the alpha rhythm from the mu rhythm [5]  both of which have a component around 10 Hz but with
different morphologies that cannot be captured by Fourier- or wavelet-based representations.
Recently  there have been several attempts for extracting more realistic and precise morphologies
directly from unﬁltered electrophysiology signals  via dictionary learning approaches [6–9]. These
methods all aim to extract certain shift-invariant prototypical waveforms (called ‘atoms’ in this
context) to better capture the temporal structure of the signals. As opposed to using generic bases

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

that have predeﬁned shapes  such as the Fourier or the wavelet bases  these atoms provide a more
meaningful representation of the data and are not restricted to narrow frequency bands.
In this line of research  Jost et al. [6] proposed the MoTIF algorithm  which uses an iterative strategy
based on generalized eigenvalue decompositions  where the atoms are assumed to be orthogonal to
each other and learnt one by one in a greedy way. More recently  the ‘sliding window matching’
(SWM) algorithm [9] was proposed for learning time-varying atoms by using a correlation-based
approach that aims to identify the recurring patterns. Even though some success has been reported
with these algorithms  they have several limitations: SWM uses a slow stochastic search inspired by
simulated annealing and MoTIF poorly handles correlated atoms  simultaneously activated  or having
varying amplitudes; some cases which often occur in practical applications.
A natural way to cast the problem of learning a dictionary of shift-invariant atoms into an optimization
problem is a convolutional sparse coding (CSC) approach [10]. This approach has gained popularity
in computer vision [11–15]  biomedical imaging [16] and audio signal processing [10  17]  due to its
ability to obtain compact representations of the signals and to incorporate the temporal structure of
the signals via convolution. In the neuroscience context  Barthélemy et al. [18] used an extension
of the K-SVD algorithm using convolutions on EEG data. In a similar spirit  Brockmeier and
Príncipe [7] used the matching pursuit algorithm combined with a rather heuristic dictionary update 
which is similar to the MoTIF algorithm. In a very recent study  Hitziger et al. [8] proposed the
AWL algorithm  which presents a mathematically more principled CSC approach for modeling
neural signals. Yet  as opposed to classical CSC approaches  the AWL algorithm imposes additional
combinatorial constraints  which limit its scope to certain data that contain spike-like atoms. Also 
since these constraints increase the complexity of the optimization problem  the authors had to resort
to dataset-speciﬁc initializations and many heuristics in their inference procedure.
While the current state-of-the-art CSC methods have a strong potential for modeling neural signals 
they might also be limited as they consider an (cid:96)2 reconstruction error  which corresponds to assuming
an additive Gaussian noise distribution. While this assumption could be reasonable for several signal
processing tasks  it turns out to be very restrictive for neural signals  which often contain heavy noise
bursts and have low signal-to-noise ratio.
In this study  we aim to address the aforementioned concerns and propose a novel probabilistic
CSC model called αCSC  which is better-suited for neural signals. αCSC is based on a family
of heavy-tailed distributions called α-stable distributions [19] whose rich structure covers a broad
range of noise distributions. The heavy-tailed nature of the α-stable distributions renders our model
robust to impulsive observations. We develop a Monte Carlo expectation maximization (MCEM)
algorithm for inference  with a weighted CSC model for the maximization step. We propose efﬁcient
optimization strategies that are speciﬁcally designed for neural time series. We illustrate the beneﬁts
of the proposed approach on both synthetic and real datasets.

2 Preliminaries

Notation: For a vector v ∈ Rn we denote the (cid:96)p norm by (cid:107)v(cid:107)p = ((cid:80)

i |vi|p)1/p. The convolution of
two vectors v1 ∈ RN and v2 ∈ RM is denoted by v1 ∗ v2 ∈ RN +M−1. We denote by x the observed
signals  d the temporal atoms  and z the sparse vector of activations. The symbols U  E  N   S denote
the univariate uniform  exponential  Gaussian  and α-stable distributions  respectively.
Convolutional sparse coding: The CSC problem formulation adopted in this work follows the Shift
Invariant Sparse Coding (SISC) model from [10]. It is deﬁned as follows:

N(cid:88)

(cid:16) 1

2

n=1

(cid:107)xn − K(cid:88)

min
d z

K(cid:88)

(cid:17)

dk ∗ zk

n(cid:107)2

2 + λ

(cid:107)zk

n(cid:107)1

s.t. (cid:107)dk(cid:107)2

2 ≤ 1 and zk

n ≥ 0 ∀n  k  

 

(1)

k=1

k=1

where xn ∈ RT denotes one of the N observed segments of signals  also referred to as a trials in
this paper. We denote by T as the length of a trial  and K the number of atoms. The aim in this
model is to approximate the signals xn by the convolution of certain atoms and their respective
activations  which are sparse. Here  dk ∈ RL denotes the kth atom of the dictionary d ≡ {dk}k  and
n ∈ RT−L+1
zk
The objective function (1) has two terms  an (cid:96)2 data ﬁtting term that corresponds to assuming an
additive Gaussian noise model  and a regularization term that promotes sparsity with an (cid:96)1 norm. The

denotes the activation of the kth atom in the nth trial. We denote by z ≡ {zk

n}n k.

+

2

(a)

(b)

Figure 1: (a) PDFs of α-stable distributions. (b) Illustration of two trials from the striatal LFP data 
which contain severe artifacts. The artifacts are illustrated with dashed rectangles.

regularization parameter is called λ > 0. Two constraints are also imposed. First  we ensure that dk
lies within the unit sphere  which prevents the scale ambiguity between d and z. Second  a positivity
constraint on z is imposed to be able to obtain physically meaningful activations and to avoid sign
ambiguities between d and z. This positivity constraint is not present in the original SISC model [10].
α-Stable distributions: The α-stable distributions have become increasingly popular in modeling
signals that might incur large variations [20–24] and have a particular importance in statistics since
they appear as the limiting distributions in the generalized central limit theorem [19]. They are
characterized by four parameters: α  β  σ  and µ: (i) α ∈ (0  2] is the characteristic exponent and
determines the tail thickness of the distribution: the distribution will be heavier-tailed as α gets
smaller. (ii) β ∈ [−1  1] is the skewness parameter. If β = 0  the distribution is symmetric. (iii)
σ ∈ (0 ∞) is the scale parameter and measures the spread of the random variable around its mode
(similar to the standard deviation of a Gaussian distribution). Finally  (iv) µ ∈ (−∞ ∞) is the
location parameter (for α > 1  it is simply the mean).
The probability density function of an α-stable distribution cannot be written in closed-form except
for certain special cases; however  the characteristic function can be written as follows:

x ∼ S(α  β  σ  µ) ⇐⇒ E[exp(iωx)] = exp(−|σω|α [1 + i sign(ω)βψα(ω)] + iµω)  

√−1. As an important
where ψα(ω) = log |ω| for α = 1  ψα(ω) = tan(πα/2) for α (cid:54)= 1  and i =
special case of the α-stable distributions  we obtain the Gaussian distribution when α = 2 and β = 0 
i.e. S(2  0  σ  µ) = N (µ  2σ2). In Fig. 1(a)  we illustrate the (approximately computed) probability
density functions (PDF) of the α-stable distribution for different values of α and β. The distribution
becomes heavier-tailed as we decrease α  whereas the tails vanish quickly when α = 2.
The moments of the α-stable distributions can only be deﬁned up to the order α  i.e. E[|x|p] < ∞
if and only if p < α  which implies the distribution has inﬁnite variance when α < 2. Further-
more  despite the fact that the PDFs of α-stable distributions do not admit an analytical form  it is
straightforward to draw random samples from them [25].

3 Alpha-Stable Convolutional Sparse Coding

3.1 The Model

From a probabilistic perspective  the CSC problem can be also formulated as a maximum a-posteriori
(MAP) estimation problem on the following probabilistic generative model:

ˆxn (cid:44) K(cid:88)

n t ∼ E(λ) 
zk

xn t|z  d ∼ N (ˆxn t  1)  where 

dk ∗ zk
n .

(2)

k=1

n t denotes the tth element of zk

n. We use the same notations for xn t and ˆxn t. It is easy to
Here  zk
verify that the MAP estimate for this probabilistic model  i.e. maxd z log p(d  z|x)  is identical to the
original optimization problem deﬁned in (1)1.
It has been long known that  due to their light-tailed nature  Gaussian models often fail at handling
noisy high amplitude observations or outliers [26]. As a result  the ‘vanilla’ CSC model turns out
to be highly sensitive to outliers and impulsive noise that frequently occur in electrophysiological

1Note that the positivity constraint on the activations is equivalent to an exponential prior for the regularization

term rather than the more common Laplacian prior.

3

-30-20-100102030x10-510-410-310-210-1100p(x)α=2.0  β=0α=1.9  β=0α=1.8  β=0α=0.9  β=15001000150020002500Time(t)-10010Trial1(xn t)5001000150020002500Time(t)-505Trial2(xn t)recordings  as illustrated in Fig. 1(b). Possible origins of such artifacts are movement  muscle
contractions  ocular blinks or electrode contact losses.
In this study  we aim at developing a probabilistic CSC model that would be capable of modeling
challenging electrophysiological signals. We propose an extension of the original CSC model deﬁned
in (2) by replacing the light-tailed Gaussian likelihood (corresponding to the (cid:96)2 reconstruction loss
in (1)) with heavy-tailed α-stable distributions. We deﬁne the proposed probabilistic model (αCSC)
as follows:
(3)
where S denotes the α-stable distribution. While still being able to capture the temporal structure
of the observed signals via convolution  the proposed model has a richer structure and would allow
large variations and outliers  thanks to the heavy-tailed α-stable distributions. Note that the vanilla
CSC deﬁned in (2) appears as a special case of αCSC  as the α-stable distribution coincides with the
Gaussian distribution when α = 2.

√
xn t|z  d ∼ S(α  0  1/

n t ∼ E(λ) 
zk

2  ˆxn t)  

3.2 Maximum A-Posteriori Inference

(cid:88)

(cid:16)

(d(cid:63)  z(cid:63)) = arg max

d z

n t

(cid:17)

(cid:88)

k

Given the observed signals x  we are interested in the MAP estimates  deﬁned as follows:

log p(xn t|d  z) +

log p(zk

n t)

.

(4)

(cid:17)

xn t|z  d  φ ∼ N(cid:16)

(cid:17)

1
2

n t ∼ E(λ)  φn t ∼ S(cid:16) α

As opposed to the Gaussian case  unfortunately  this optimization problem is not amenable to classical
optimization tools  since the PDF of the α-stable distributions does not admit an analytical expression.
As a remedy  we use the product property of the symmetric α-stable densities [19  27] and re-express
the αCSC model as conditionally Gaussian. It leads to:

πα
4

 

 

2

zk

φn t

ˆxn t 

)2/α  0

  1  2(cos

(5)
where φ is called the impulse variable that is drawn from a positive α-stable distribution (i.e. β = 1) 
whose PDF is illustrated in Fig. 1(a). It can be shown that both formulations of the αCSC model are
identical by marginalizing the joint distribution p(x  d  z  φ) over φ [19  Proposition 1.3.1].
The impulsive structure of the αCSC model becomes more prominent in this formulation: the
variances of the Gaussian observations are modulated by stable random variables with inﬁnite
variance  where the impulsiveness depends on the value of α. It is also worth noting that when α = 2 
φn t becomes deterministic and we can again verify that αCSC coincides with the vanilla CSC.
The conditionally Gaussian structure of the augmented model has a crucial practical implication: if
the impulse variable φ were to be known  then the MAP estimation problem over d and z in this
model would turn into a ‘weighted’ CSC problem  which is a much easier task compared to the
original problem. In order to be able to exploit this property  we propose an expectation-maximization
(EM) algorithm  which iteratively maximizes a lower bound of the log-posterior log p(d  z|x)  and
algorithmically boils down to computing the following steps in an iterative manner:

(6)

E-Step:

B(i)(d  z) = E [log p(x  φ  z|d)]p(φ|x z(i) d(i))  
(d(i+1)  z(i+1)) = arg maxd z B(i)(d  z).

M-Step:

(7)
where E[f (x)]q(x) denotes the expectation of a function f under the distribution q  i denotes the
iterations  and B(i) is a lower bound to log p(d  z|x) and it is tight at the current iterates z(i)  d(i).
The E-Step: In the ﬁrst step of our algorithm  we need to compute the EM lower bound B that has
the following form:

B(i)(d  z) =+ − N(cid:88)

(cid:113)
(cid:16)(cid:107)

n (cid:12) (xn − K(cid:88)

w(i)

dk ∗ zk

n)(cid:107)2

(cid:107)zk

n(cid:107)1

n=1

2 + λ

(8)
where =+ denotes equality up to additive constants  (cid:12) denotes the Hadamard (element-wise) product 
and the square-root operator is also deﬁned element-wise. Here  w(i)
+ are the weights that are
(cid:44) E [1/φn t]p(φ|x z(i) d(i)). As the variables φn t are expected to be large
deﬁned as follows: w(i)
n t
when ˆxn t cannot explain the observation xn t – typically due to a corruption or a high noise – the
weights will accordingly suppress the importance of the particular point xn t. Therefore  the overall
approach will be more robust to corrupted data than the Gaussian models where all weights would be
deterministic and equal to 0.5.

n ∈ RT

k=1

k=1

 

K(cid:88)

(cid:17)

4

j=1 1/φ(i j)

n t   where φ(i j)

(1/J)(cid:80)J

/* E-step: */
for j = 1 to J do

Unfortunately  the weights w(i) cannot be
therefore we need
computed analytically 
to resort to approximate methods.
In this
study  we develop a Markov chain Monte
Carlo (MCMC) method to approximately
compute the weights  where we approxi-
mate the intractable expectations with a ﬁnite
n t ≈
sample average  given as follows: w(i)
n t are some
samples that are ideally drawn from the pos-
terior distribution p(φ|x  z(i)  d(i)). Unfor-
tunately  directly drawing samples from the
posterior distribution of φ is not tractable ei-
ther  and therefore  we develop a Metropolis-
Hastings algorithm [28]  that asymptotically
generates samples from the target distribution
p(φ|·) in two steps. In the j-th iteration of this
algorithm  we ﬁrst draw a random sample for each n and t from the prior distribution (cf. (5))  i.e. 
n t ∼ p(φn t). We then compute an acceptance probability for each φ(cid:48)
φ(cid:48)
n t that is deﬁned as follows:

Algorithm 1 α-stable Convolutional Sparse Coding
Require: Regularization: λ ∈ R+  Num. atoms:
K  Atom length: L  Num. iterations: I   J  M
1: for i = 1 to I do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12: end for
13: return w(I)  d(I)  z(I)

end for
w(i)
/* M-step: */
for m = 1 to M do

z(i) = L-BFGS-B on (10)
d(i) = L-BFGS-B on the dual of (11)

n t via MCMC (9)
j=1 1/φ(i j)

n t ≈ (1/J)(cid:80)J

Draw φ(i j)

end for

n t

acc(φ(i j)

n t → φ(cid:48)

n t) (cid:44) min

1  p(xn t|d(i)  z(i)  φ(cid:48)

n t)/p(xn t|d(i)  z(i)  φ(i j)
n t )

(9)

(cid:110)

(cid:111)

n t → φ(cid:48)
n t = φ(i)

where j denotes the iteration number of the MCMC algorithm. Finally  we draw a uniform random
number un t ∼ U([0  1]) for each n and t. If un t < acc(φ(i)
n t)  we accept the sample and set
n t = φ(cid:48)
φ(i+1)
n t; otherwise we reject the sample and set φ(i+1)
n t. This procedure forms a Markov
chain that leaves the target distribution p(φ|·) invariant  where under mild ergodicity conditions  it
can be shown that the ﬁnite-sample averages converge to their true values when J goes to inﬁnity
[29]. More detailed explanation of this procedure is given in the supplementary document.
The M-Step: Given the weights wn that are estimated during the E-step  the objective of the M-
step (7) is to solve a weighted CSC problem  which is much easier when compared to our original
problem. This objective function is not jointly convex in d and z  yet it is convex if one ﬁx either d
or z. Here  similarly to the vanilla CSC approaches [9  10]  we develop a block coordinate descent
strategy  where we solve the problem in (7) for either d or z  by keeping respectively z and d ﬁxed.
We ﬁrst focus on solving the problem for z while keeping d ﬁxed  given as follows:

N(cid:88)

(cid:16)(cid:107)√

wn (cid:12) (xn − K(cid:88)

min

z

(cid:88)

(cid:17)

n=1

k=1

k

Dk ¯zk

n)(cid:107)2

2 + λ

(cid:107)zk

n(cid:107)1

s.t. zk

n ≥ 0 ∀n  k .

(10)

(cid:44) [(zk

n)(cid:62)  0··· 0](cid:62) ∈ RT

n as the inner product of the zero-padded activations
Here  we expressed the convolution of dk and zk
+  with a Toeplitz matrix Dk ∈ RT×T   that is constructed from dk. The
¯zk
n
matrices Dk are never constructed in practice  and all operations are carried out using convolutions.
This problem can be solved by various constrained optimization algorithms. Here  we choose the
quasi-Newton L-BFGS-B algorithm [30] with a box constraint: 0 ≤ zk
n t ≤ ∞. This approach
only requires the simple computation of the gradient of the objective function with respect to z (cf.
supplementary material). Note that  since each trial is independent from each other  we can solve this
problem for each zn in parallel.
We then solve the problem for the atoms d while keeping z ﬁxed. This optimization problem turns out
to be a constrained weighted least-squares problem. In the non-weighted case  this problem can be
solved either in the time domain or in the Fourier domain [10–12]. The Fourier transform simpliﬁes
the convolutions that appear in least-squares problem  but it also induces several difﬁculties  such
as that the atom dk have to be in a ﬁnite support L  an important issue ignored in the seminal work
of [10] and addressed with an ADMM solver in[11  12]. In the weighted case  it is not clear how to
solve this problem in the Fourier domain. We thus perform all the computations in the time domain.
Following the traditional ﬁlter identiﬁcation approach [31]  we need to embed the one-dimensional
n i+j−L+1 if L − 1 ≤
signals zk

n into a matrix of delayed signals Z k

n ∈ RT×L  where (Z k

n)i j = zk

5

(a) K = 10  L = 32.

(b) Time to reach a relative precision of 0.01.

Figure 2: Comparison of state-of-the-art methods with our approach. (a) Convergence plot with the
objective function relative to the obtained minimum  as a function of computational time. (b) Time
taken to reach a relative precision of 10−2  for different settings of K and L.

i + j < T and 0 elsewhere. Equation (1) then becomes:

N(cid:88)

min

d

wn (cid:12) (xn − K(cid:88)

(cid:107)√

ndk)(cid:107)2
Z k
2 

s.t. (cid:107)dk(cid:107)2

2 ≤ 1 .

(11)

n=1

k=1

Due to the constraint  we must resort to an iterative approach. The options are to use (accelerated)
projected gradient methods such as FISTA [32] applied to (11)  or to solve a dual problem as done
in [10]. The dual is also a smooth constraint problem yet with a simpler positivity box constraint
(cf. supplementary material). The dual can therefore be optimized with L-BFGS-B. Using such a
quasi-Newton solver turned out to be more efﬁcient than any accelerated ﬁrst order method in either
the primal or the dual (cf. benchmarks in supplementary material).
Our entire EM approach can be summarized in the Algorithm 1. Note that during the alternating
minimization  thanks to convexity we can warm start the d update and the z update using the solution
from the previous update. This signiﬁcantly speeds up the convergence of the L-BFGS-B algorithm 
particularly in the later iterations of the overall algorithm.

4 Experiments

In order to evaluate our approach  we conduct several experiments on both synthetic and real
data. First  we show that our proposed optimization scheme for the M-step provides signiﬁcant
improvements in terms of convergence speed over the state-of-the-art CSC methods. Then  we provide
empirical evidence that our algorithm is more robust to artifacts and outliers than three competing
CSC methods [6  7  12]. Finally  we consider LFP data  where we illustrate that our algorithm can
reveal interesting properties in electrophysiological signals without supervision  even in the presence
of severe artifacts. The source code is publicly available at https://alphacsc.github.io/.
Synthetic simulation setup: In our synthetic data experiments  we simulate N trials of length T by
ﬁrst generating K zero mean and unit norm atoms of length L. The activation instants are integers

drawn from a uniform distribution in(cid:74)0  T − L(cid:75). The amplitude of the activations are drawn from a

uniform distribution in [0  1]. Atoms are activated only once per trial and are allowed to overlap. The
activations are then convolved with the generated atoms and summed up as in (1).
M-step performance: In our ﬁrst set of synthetic experiments  we illustrate the beneﬁts of our
M-step optimization approach over state-of-the-art CSC solvers. We set N = 100  T = 2000 and
λ = 1  and use different values for K and L. To be comparable  we set α = 2 and add Gaussian
noise to the synthesized signals  where the standard deviation is set to 0.01. In this setting  we
have wn t = 1/2 for all n  t  which reduces the problem to a standard CSC setup. We monitor the
convergence of ADMM-based methods by Heide et al. [11] and Wohlberg [12] against our M-step
algorithm  using both a single-threaded and a parallel version for the z-update. As the problem is
non-convex  even if two algorithms start from the same point  they are not guaranteed to reach the
same local minimum2. Hence  for a fair comparison  we use a multiple restart strategy with averaging
across 24 random seeds.

2Note that the M-step can be viewed as a biconvex problem  for which global convergence guarantees can be
shown under certain assumptions [33  34]. However  we have observed that it is required to use multiple restarts
even for vanilla CSC  implying that these assumptions are not satisﬁed in this particular problem.

6

020004000Time (s)103102101100101(objective - best) / bestHeide et al (2015)Wohlberg (2016)M-step M-step - 4 parallelK = 2  L = 32K = 2  L = 128K = 10  L = 320100020003000400050006000Time (s)Heide et al (2015)Wohlberg (2016)M-step M-step - 4 parallel(a) No corruption.

Figure 3: Simulation to compare state-of-the-art methods against αCSC.

(b) 10% corruption.

(c) 20% corruption

During our experiments we have observed that the ADMM-based methods do not guarantee the
feasibility of the iterates. In other words  the norms of the estimated atoms might be greater than 1
during the iterations. To keep the algorithms comparable  when computing the objective value  we
project the atoms to the unit ball and scale the activations accordingly. To be strictly comparable 
we also imposed a positivity constraint on these algorithms. This is easily done by modifying the
soft-thresholding operator to be a rectiﬁed linear function. In the benchmarks  all algorithms use a
single thread  except “M-step - 4 parallel” which uses 4 threads during the z update.
In Fig. 2  we illustrate the convergence behaviors of the different methods. Note that the y-axis is the
precision relative to the objective value obtained upon convergence. In other words  each curve is
relative to its own local minimum (see supplementary document for details). In the right subplot  we
show how long it takes for the algorithms to reach a relative precision of 0.01 for different settings
(cf. supplementary material for more benchmarks). Our method consistently performs better and the
difference is even more striking for more challenging setups. This speed improvement on the M-step
is crucial for us as this step will be repeatedly executed.
Robustness to corrupted data: In our second synthetic data experiment  we illustrate the robustness
of αCSC in the presence of corrupted observations. In order to simulate the likely presence of high
amplitude artifacts  one way would be to directly simulate the generative model in (3). However 
this would give us an unfair advantage  since αCSC is speciﬁcally designed for such data. Here 
we take an alternative approach  where we corrupt a randomly chosen fraction of the trials (10% or
20%) with strong Gaussian noise of standard deviation 0.1  i.e. one order of magnitude higher than
in a regular trial. We used a regularization parameter of λ = 0.1. In these experiments  by CSC we
refer to αCSC with α = 2  that resembles using only the M-step of our algorithm with deterministic
weights wn t = 1/2 for all n  t. We used a simpler setup where we set N = 100  T = 512  and
L = 64. We used K = 2 atoms  as shown in dashed lines in Fig. 3.
For αCSC  we set the number of outer iterations I = 5  the number of iterations of the M-step to
M = 50  and the number of iterations of the MCMC algorithm to J = 10. We discard the ﬁrst 5
samples of the MCMC algorithm as burn-in. To enable a fair comparison  we run the standard CSC
algorithm for I × M iterations  i.e. the total number of M-step iterations in αCSC. We also compared
αCSC against competing state-of-art methods previously applied to neural time series: Brockmeier
and Príncipe [7] and MoTIF [6]. Starting from multiple random initializations  the estimated atoms
with the smallest (cid:96)2 distance with the true atoms are shown in Fig. 3.
In the artifact-free scenario  all algorithms perform equally well  except for MoTIF that suffers
from the presence of activations with varying amplitudes. This is because it aligns the data using
correlations before performing the eigenvalue decomposition  without taking into account the strength
of activations in each trial. The performance of Brockmeier and Príncipe [7] and CSC degrades as

4:

Figure
Atoms
learnt by αCSC on
LFP data containing
epileptiform spikes
with α = 2.

(a) LFP spike data from [8]

(b) Estimated atoms

7

Brockmeier et al.Atom 1Atom 2G. TruthMoTIFCSCαCSCBrockmeier et al.MoTIFCSCαCSCBrockmeier et al.MoTIFCSCαCSC9.09.510.010.511.011.512.0Time (s)8006004002000200400µ V0.00.10.20.3Time (s)0.10.0(a) Atoms learnt by: CSC (clean data)  CSC (full data)  αCSC (full data)

(b) Comodulogram.

Figure 5: (a) Three atoms learnt from a rodent striatal LFP channel  using CSC on cleaned data  and
both CSC and αCSC on the full data. The atoms capture the cross-frequency coupling of the data
(dashed rectangle). (b) Comodulogram presents the cross-frequency coupling intensity computed
between pairs of frequency bands on the entire cleaned signal  following [37].

the level of corruption increases. On the other hand  αCSC is clearly more robust to the increasing
level of corruption and recovers reasonable atoms even when 20% of the trials are corrupted.
Results on LFP data In our last set of experiments  we consider real neural data from two different
datasets. We ﬁrst applied αCSC on an LFP dataset previously used in [8] and containing epileptiform
spikes as shown in Fig. 4(a). The data was recorded in the rat cortex  and is free of artifact. Therefore 
we used the standard CSC with our optimization scheme  (i.e. αCSC with α = 2). As a standard
preprocessing procedure  we applied a high-pass ﬁlter at 1 Hz in order to remove drifts in the signal 
and then applied a tapered cosine window to down-weight the samples near the edges. We set λ = 6 
N = 300  T = 2500  L = 350  and K = 3. The recovered atoms by our algorithm are shown in
Fig. 4(b). We can observe that the estimated atoms resemble the spikes in Fig. 4(a). These results
show that  without using any heuristics  our approach can recover similar atoms to the ones reported
in [8]  even though it does not make any assumptions on the shapes of the waveforms  or initializes
the atoms with template spikes in order to ease the optimization.
The second dataset is an LFP channel in a rodent striatum from [35]. We segmented the data into 70
trials of length 2500 samples  windowed each trial with a tapered cosine function  and detrended the
data with a high-pass ﬁlter at 1 Hz. We set λ = 10  initialized the weights wn to the inverse of the
variance of the trial xn. Atoms are in all experiments initialized with Gaussian white noise.
As opposed to the ﬁrst LFP dataset  this dataset contains strong artifacts  as shown in Fig. 1(b). In
order to be able to illustrate the potential of CSC on this data  we ﬁrst manually identiﬁed and removed
the trials that were corrupted by artifacts. In Fig. 5(a)  we illustrate the estimated atoms with CSC on
the manually-cleaned data. We observe that the estimated atoms correspond to canonical waveforms
found in the signal. In particular  the high frequency oscillations around 80 Hz are modulated in
amplitude by the low-frequency oscillation around 3 Hz  a phenomenon known as cross-frequency
coupling (CFC) [36]. We can observe this by computing a comodulogram [37] on the entire signal
(Fig. 5(b)). This measures the correlation between the amplitude of the high frequency band and the
phase of the low frequency band.
Even though CSC is able to provide these excellent results on the cleaned data set  its performance
heavily relies on the manual removal of the artifacts. Finally  we repeated the previous experiment on
the full data  without removing the artifacts and compared CSC with αCSC  where we set α = 1.2.
The results are shown in the middle and the right sub-ﬁgures of Fig. 5(a). It can be observed that in
the presence of strong artifacts  CSC is not able to recover the atoms anymore. On the contrary  we
observe that αCSC can still recover atoms as observed in the artifact-free regime. In particular  the
cross-frequency coupling phenomenon is still visible.

5 Conclusion
We address the present need in the neuroscience community to better capture the complex morphology
of brain waves. Our approach is based on a probabilistic formulation of a CSC model. We propose
an inference strategy based on MCEM to deal efﬁciently with heavy tailed noise and take into
account the polarity of neural activations with a positivity constraint. Our problem formulation
allows the use of fast quasi-Newton methods that outperform previously proposed state-of-the-art
ADMM-based algorithms  even when not making use of our parallel implementation. Results on LFP
data demonstrate that such algorithms can be robust to the presence of transient artifacts in data and
reveal insights on neural time-series without supervision.

8

0.00.20.40.6Time (s)0.100.050.000.050.100.150.00.20.40.6Time (s)0.00.20.40.6Time (s)2.55.07.510.0Low frequency255075100125150175High frequency0.000 0.001 0.002 0.003 0.004 0.005 6 Acknowledgement

The work was supported by the French National Research Agency grants ANR-14-NEUC-0002-01 
ANR-13-CORD-0008-02  and ANR-16-CE23-0014 (FBIMATRIX)  as well as the ERC Starting
Grant SLAB ERC-YStG-676943.

References
[1] S. R. Cole and B. Voytek. Brain oscillations and the importance of waveform shape. Trends

Cogn. Sci.  2017.

[2] M. X. Cohen. Analyzing neural time series data: Theory and practice. MIT Press  2014. ISBN

9780262319560.

[3] S. R. Jones. When brain rhythms aren’t ‘rhythmic’: implication for their mechanisms and

meaning. Curr. Opin. Neurobiol.  40:72–80  2016.

[4] A. Mazaheri and O. Jensen. Asymmetric amplitude modulations of brain oscillations generate

slow evoked responses. The Journal of Neuroscience  28(31):7781–7787  2008.

[5] R. Hari and A. Puce. MEG-EEG Primer. Oxford University Press  2017.

[6] P. Jost  P. Vandergheynst  S. Lesage  and R. Gribonval. MoTIF: an efﬁcient algorithm for
learning translation invariant dictionaries. In Acoustics  Speech and Signal Processing  ICASSP 
volume 5. IEEE  2006.

[7] A. J. Brockmeier and J. C. Príncipe. Learning recurrent waveforms within EEGs.

Transactions on Biomedical Engineering  63(1):43–54  2016.

IEEE

[8] S. Hitziger  M. Clerc  S. Saillet  C. Benar  and T. Papadopoulo. Adaptive Waveform Learning:
A Framework for Modeling Variability in Neurophysiological Signals. IEEE Transactions on
Signal Processing  2017.

[9] B. Gips  A. Bahramisharif  E. Lowet  M. Roberts  P. de Weerd  O. Jensen  and J. van der Eerden.
Discovering recurring patterns in electrophysiological recordings. J. Neurosci. Methods  275:
66–79  2017.

[10] R. Grosse  R. Raina  H. Kwong  and A. Y. Ng. Shift-invariant sparse coding for audio classiﬁ-
cation. In 23rd Conference on Uncertainty in Artiﬁcial Intelligence  UAI’07  pages 149–158.
AUAI Press  2007. ISBN 0-9749039-3-0.

[11] F. Heide  W. Heidrich  and G. Wetzstein. Fast and ﬂexible convolutional sparse coding. In

Computer Vision and Pattern Recognition (CVPR)  pages 5135–5143. IEEE  2015.

[12] B. Wohlberg. Efﬁcient algorithms for convolutional sparse representations. Image Processing 

IEEE Transactions on  25(1):301–315  2016.

[13] M. D. Zeiler  D. Krishnan  G.W. Taylor  and R. Fergus. Deconvolutional networks. In Computer

Vision and Pattern Recognition (CVPR)  pages 2528–2535. IEEE  2010.

[14] M. Šorel and F. Šroubek. Fast convolutional sparse coding using matrix inversion lemma.

Digital Signal Processing  2016.

[15] K. Kavukcuoglu  P. Sermanet  Y-L. Boureau  K. Gregor  M. Mathieu  and Y. Cun. Learning
convolutional feature hierarchies for visual recognition. In Advances in Neural Information
Processing Systems (NIPS)  pages 1090–1098  2010.

[16] M. Pachitariu  A. M Packer  N. Pettit  H. Dalgleish  M. Hausser  and M. Sahani. Extracting
regions of interest from biological images with convolutional sparse block coding. In Advances
in Neural Information Processing Systems (NIPS)  pages 1745–1753  2013.

[17] B. Mailhé  S. Lesage  R. Gribonval  F. Bimbot  and P. Vandergheynst. Shift-invariant dictionary
learning for sparse representations: extending K-SVD. In 16th Eur. Signal Process. Conf.  pages
1–5. IEEE  2008.

9

[18] Q. Barthélemy  C. Gouy-Pailler  Y. Isaac  A. Souloumiac  A. Larue  and J. I. Mars. Multivariate

temporal dictionary learning for EEG. J. Neurosci. Methods  215(1):19–28  2013.

[19] G. Samorodnitsky and M. S. Taqqu. Stable non-Gaussian random processes: stochastic models

with inﬁnite variance  volume 1. CRC press  1994.

[20] E. E. Kuruoglu. Signal processing in α-stable noise environments: a least Lp-norm approach.

PhD thesis  University of Cambridge  1999.

[21] B. B. Mandelbrot. Fractals and scaling in ﬁnance: Discontinuity  concentration  risk. Selecta

volume E. Springer Science & Business Media  2013.

[22] U. ¸Sim¸sekli  A. Liutkus  and A. T. Cemgil. Alpha-stable matrix factorization. IEEE SPL  22

(12):2289–2293  2015.

[23] Y. Wang  Y. Qi  Y. Wang  Z. Lei  X. Zheng  and G. Pan. Delving into α-stable distribution in
noise suppression for seizure detection from scalp EEG. J. Neural. Eng.  13(5):056009  2016.

[24] S. Leglaive  U. ¸Sim¸sekli  A. Liutkus  R. Badeau  and G. Richard. Alpha-stable multichannel

audio source separation. In ICASSP  pages 576–580  2017.

[25] J. M. Chambers  C. L. Mallows  and B. W. Stuck. A method for simulating stable random

variables. Journal of the american statistical association  71(354):340–344  1976.

[26] P. J. Huber. Robust Statistics. Wiley  1981.

[27] S. Godsill and E. Kuruoglu. Bayesian inference for time series with heavy-tailed symmetric
α-stable noise processes. Proc. Applications of heavy tailed distributions in economics  eng.
and stat.  1999.

[28] S. Chib and E. Greenberg. Understanding the Metropolis-Hastings algorithm. The American

Statistician  49(4):327–335  1995.

[29] J.S. Liu. Monte Carlo strategies in scientiﬁc computing. Springer  2008.

[30] R. H. Byrd  P. Lu  J. Nocedal  and C. Zhu. A limited memory algorithm for bound constrained

optimization. SIAM Journal on Scientiﬁc Computing  16(5):1190–1208  1995.

[31] E. Moulines  P. Duhamel  J-F. Cardoso  and S. Mayrargue. Subspace methods for the blind
IEEE Transactions on signal processing  43(2):

identiﬁcation of multichannel FIR ﬁlters.
516–525  1995.

[32] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM journal on imaging sciences  2(1):183–202  2009.

[33] Alekh Agarwal  Animashree Anandkumar  Prateek Jain  Praneeth Netrapalli  and Rashish
Tandon. Learning sparsely used overcomplete dictionaries. In Conference on Learning Theory 
pages 123–137  2014.

[34] Jochen Gorski  Frank Pfeuffer  and Kathrin Klamroth. Biconvex sets and optimization with
biconvex functions: a survey and extensions. Mathematical Methods of Operations Research 
66(3):373–407  2007.

[35] G. Dallérac  M. Graupner  J. Knippenberg  R. C. R. Martinez  T. F. Tavares  L. Tallot  N. El Mas-
sioui  A. Verschueren  S. Höhn  J.B. Bertolus  et al. Updating temporal expectancy of an
aversive event engages striatal plasticity under amygdala control. Nature Communications  8:
13920  2017.

[36] O. Jensen and L. L. Colgin. Cross-frequency coupling between neuronal oscillations. Trends in

cognitive sciences  11(7):267–269  2007.

[37] A. BL. Tort  R. Komorowski  H. Eichenbaum  and N. Kopell. Measuring phase-amplitude
coupling between neuronal oscillations of different frequencies. J. Neurophysiol.  104(2):
1195–1210  2010.

10

,Lionel Ott
Linsey Pang
Fabio Ramos
Sanjay Chawla
Mainak Jas
Tom Dupré la Tour
Umut Simsekli
Alexandre Gramfort
Sid Reddy
Anca Dragan
Sergey Levine
Shibani Santurkar
Andrew Ilyas
Dimitris Tsipras
Logan Engstrom
Brandon Tran
Aleksander Madry