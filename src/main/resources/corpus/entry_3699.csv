2014,Near-optimal Reinforcement Learning in Factored MDPs,Any reinforcement learning algorithm that applies to all Markov decision processes (MDPs) will suffer $\Omega(\sqrt{SAT})$ regret on some MDP  where $T$ is the elapsed time and $S$ and $A$ are the cardinalities of the state and action spaces. This implies $T = \Omega(SA)$ time to guarantee a near-optimal policy. In many settings of practical interest  due to the curse of dimensionality  $S$ and $A$ can be so enormous that this learning time is unacceptable. We establish that  if the system is known to be a \emph{factored} MDP  it is possible to achieve regret that scales polynomially in the number of \emph{parameters} encoding the factored MDP  which may be exponentially smaller than $S$ or $A$. We provide two algorithms that satisfy near-optimal regret bounds in this context: posterior sampling reinforcement learning (PSRL) and an upper confidence bound algorithm (UCRL-Factored).,Near-optimal Reinforcement Learning

in Factored MDPs

Ian Osband

Stanford University

iosband@stanford.edu

Benjamin Van Roy
Stanford University
bvr@stanford.edu

Abstract

Any reinforcement learning algorithm that applies to all Markov decision
processes (MDPs) will suer (ÔSAT) regret on some MDP  where T is
the elapsed time and S and A are the cardinalities of the state and action
spaces. This implies T =( SA) time to guarantee a near-optimal policy.
In many settings of practical interest  due to the curse of dimensionality 
S and A can be so enormous that this learning time is unacceptable. We
establish that  if the system is known to be a factored MDP  it is possible
to achieve regret that scales polynomially in the number of parameters
encoding the factored MDP  which may be exponentially smaller than S
or A. We provide two algorithms that satisfy near-optimal regret bounds
in this context: posterior sampling reinforcement learning (PSRL) and an
upper conﬁdence bound algorithm (UCRL-Factored).

1 Introduction
We consider a reinforcement learning agent that takes sequential actions within an uncertain
environment with an aim to maximize cumulative reward [1]. We model the environment
as a Markov decision process (MDP) whose dynamics are not fully known to the agent.
The agent can learn to improve future performance by exploring poorly-understood states
and actions  but might improve its short-term rewards through a policy which exploits its
existing knowledge. Ecient reinforcement learning balances exploration with exploitation
to earn high cumulative reward.
The vast majority of ecient reinforcement learning has focused upon the tabula rasa setting 
where little prior knowledge is available about the environment beyond its state and action
spaces. In this setting several algorithms have been designed to attain sample complexity
polynomial in the number of states S and actions A [2  3]. Stronger bounds on regret 
the dierence between an agent’s cumulative reward and that of the optimal controller 
have also been developed. The strongest results of this kind establish ˜O(SÔAT) regret for
particular algorithms [4  5  6] which is close to the lower bound (ÔSAT) [4]. However  in
many setting of interest  due to the curse of dimensionality  S and A can be so enormous
that even this level of regret is unacceptable.
In many practical problems the agent will have some prior understanding of the environment
beyond tabula rasa. For example  in a large production line with m machines in sequence
each with K possible states  we may know that over a single time-step each machine can
only be inﬂuenced by its direct neighbors. Such simple observations can reduce the dimen-
sionality of the learning problem exponentially  but cannot easily be exploited by a tabula
rasa algorithm. Factored MDPs (FMDPs) [7]  whose transitions can be represented by a
dynamic Bayesian network (DBN) [8]  are one eective way to represent these structured
MDPs compactly.

1

Several algorithms have been developed that exploit the known DBN structure to achieve
sample complexity polynomial in the parameters of the FMDP  which may be exponentially
smaller than S or A [9  10  11]. However  these polynomial bounds include several high order
terms. We present two algorithms  UCRL-Factored and PSRL  with the ﬁrst near-optimal
regret bounds for factored MDPs. UCRL-Factored is an optimistic algorithm that modiﬁes
the conﬁdence sets of UCRL2 [4] to take advantage of the network structure. PSRL is
motivated by the old heuristic of Thompson sampling [12] and has been previously shown
to be ecient in non-factored MDPs [13  6]. These algorithms are descibed fully in Section
6.
Both algorithms make use of approximate FMDP planner in internal steps. However  even
where an FMDP can be represented concisely  solving for the optimal policy may take
exponentially long in the most general case [14]. Our focus in this paper is upon the
statistical aspect of the learning problem and like earlier discussions we do not specify which
computational methods are used [10]. Our results serve as a reduction of the reinforcement
learning problem to ﬁnding an approximate solution for a given FMDP. In many cases of
interest  eective approximate planning methods for FMDPs do exist. Investigating and
extending these methods are an ongoing subject of research [15  16  17  18].

2 Problem formulation
We consider the problem of learning to optimize a random ﬁnite horizon MDP M =
(S A  RM   P M  · ﬂ ) in repeated ﬁnite episodes of interaction. S is the state space  A is the
action space  RM(s  a) is the reward distibution over R in state s with action a  P M(·|s  a)
is the transition probability over S from state s with action a  · is the time horizon  and
ﬂ the initial state distribution. We deﬁne the MDP and all other random variables we will
consider with respect to a probability space ( F  P).
A deterministic policy µ is a function mapping each state s œS and i = 1  . . .  · to an action
a œA . For each MDP M = (S A  RM   P M  · ﬂ ) and policy µ  we deﬁne a value function

V M

µ i(s) := EM µSU
·ÿj=i

R

M(sj  aj)---si = sTV  

M(s  a) denotes the expected reward realized when action a is selected while in
where R
state s  and the subscripts of the expectation operator indicate that aj = µ(sj  j)  and
sj+1 ≥ P M(·|sj  aj) for j = i  . . .   ·. A policy µ is optimal for the MDP M if V M
µ i(s) =
µÕ i(s) for all s œS and i = 1  . . .  · . We will associate with each MDP M a policy
maxµÕ V M
µM that is optimal for M.
The reinforcement learning agent interacts with the MDP over episodes that begin at times
tk = (k ≠ 1)· + 1  k = 1  2  . . .. At each time t  the agent selects an action at  observes
a scalar reward rt  and then transitions to st+1. Let Ht = (s1  a1  r1  . . .   st≠1  at≠1  rt≠1)
denote the history of observations made prior to time t. A reinforcement learning algorithm
is a deterministic sequence {ﬁk|k = 1  2  . . .} of functions  each mapping Htk to a probability
distribution ﬁk(Htk) over policies which the agent will employ during the kth episode. We
deﬁne the regret incurred by a reinforcement learning algorithm ﬁ up to time T to be:

where k denotes regret over the kth episode  deﬁned with respect to the MDP Mú by

with µú = µMú and µk ≥ ﬁk(Htk). Note that regret is not deterministic since it can
depend on the random MDP Mú  the algorithm’s internal random sampling and  through
the history Htk  on previous random transitions and random rewards. We will assess and
compare algorithm performance in terms of regret and its expectation.

Regret(T ﬁ  M ú) :=

k 

ÁT /· Ëÿk=1
µú 1(s) ≠ V Mú

µk 1(s))

k :=ÿS

ﬂ(s)(V Mú

2

3 Factored MDPs
Intuitively a factored MDP is an MDP whose rewards and transitions exhibit some condi-
tional independence structure. To formalize this deﬁnition we must introduce some more
notation common to the literature [11].
Deﬁnition 1 (Scope operation for factored sets X = X1 ◊ .. ◊X n).
For any subset of indices Z ™{ 1  2  ..  n} let us deﬁne the scope set X[Z] := oiœZ Xi. Further 
for any x œX deﬁne the scope variable x[Z] œX [Z] to be the value of the variables xi œX i
with indices i œ Z. For singleton sets Z we will write x[i] for x[{i}] in the natural way.
Let PX  Y be the set of functions mapping elements of a ﬁnite set X to probability mass
functions over a ﬁnite set Y. P C ‡
X  R will denote the set of functions mapping elements of a
ﬁnite set X to ‡-sub-Gaussian probability measures over (R B(R)) with mean bounded in
[0  C]. For reinforcement learning we will write X for S◊A and consider factored reward
and factored transition functions which are drawn from within these families.
Deﬁnition 2 ( Factored reward functions R œR™P C ‡
X  R).
The reward function class R is factored over S◊A = X = X1 ◊ .. ◊X n with scopes Z1  ..Zl
if and only if  for all R œR   x œX there exist functions {Ri œP C ‡

i=1 such that 

X[Zi] R}l

E[r] =

lÿi=1

E#ri$

i=1 ri with each ri ≥ Ri(x[Zi]) and individually observed.

for r ≥ R(x) is equal toql
Deﬁnition 3 ( Factored transition functions P œP™P X  S ).
The transition function class P is factored over S◊A = X = X1 ◊ .. ◊X n and S =
S1 ◊ ..◊S m with scopes Z1  ..Zm if and only if  for all P œP   x œX   s œS there exist some
{Pi œP X[Zi] Si}m

i=1 such that 

P(s|x) =

mŸi=1

Pi3s[i]---- x[Zi]4

A factored MDP (FMDP) is then deﬁned to be an MDP with both factored rewards and
factored transitions. Writing X = S◊A a FMDP is fully characterized by the tuple
i=1; ·; ﬂ" 

where ZR
i are the scopes for the reward and transition functions respectively in
{1  ..  n} for Xi. We assume that the size of all scopes |Zi|Æ ’ π n and factors |Xi|Æ K so
that the domains of Ri and Pi are of size at most K’.

M =!{Si}m

i=1; {Pi}m

i=1; {ZR
i }l

i=1; {Xi}n

i=1; {Ri}l

i=1; {ZP

i and ZP

i }m

If „ is the distribution of Mú and  is the span of the optimal value function then we can
bound the regret of PSRL:

4 Results
Our ﬁrst result shows that we can bound the expected regret of PSRL.
Theorem 1 (Expected regret for PSRL in factored MDPs).
i=1; {Xi}n

Let Mú be factored with graph structure G = !{Si}m
E#Regret(T ﬁ PS
·   Mú)$Æ
+4 + E[]31 + 4

lÿi=1;5·C |X[ZR
T ≠ 44 mÿj=1;5·|X[ZP

i ]| + 12‡Ò|X[ZR
j ]| + 12Ò|X[ZP

i ]|T log!4l|X[ZR
j ]||Sj|T log!4m|X[ZP

i }m

i=1; ·".
i ]|kT"< + 2ÔT
j ]|kT"< (1)

i=1; {ZR
i }l

i=1; {ZP

We have a similar result for UCRL-Factored that holds with high probability.

3

  Mú)Æ

·

Regret(T ﬁ UC

·

Theorem 2 (High probability regret for UCRL-Factored in factored MDPs).
i=1; {ZP

i }m
D is the diameter of Mú  then for any Mú can bound the regret of UCRL-Factored:

i=1; {ZR
i }l

i=1; {Xi}n

i ]|T log!12l|X[ZR
j ]||Sj|T log!12m|X[ZP

Let Mú be factored with graph structure G =!{Si}m
i=1; ·". If
lÿi=1;5·C |X[ZR
i ]|kT /”"< + 2ÔT
i ]| + 12‡Ò|X[ZR
mÿj=1;5·|X[ZP
j ]|kT /”"<(2)
j ]| + 12Ò|X[ZP
+CD2T log(6/”) + CD
with probability at least 1 ≠ ”
Both algorithms give bounds ˜O1qm
j ]||Sj|T2 where  is a measure of MDP
j=1Ò|X[ZP
connectedness: expected span E[] for PSRL and scaled diameter CD for UCRL-Factored.
The span of an MDP is the maximum dierence in value of any two states under the optimal
µú 1(s)≠V Mú
policy (Mú) := maxs sÕœS{V Mú
µú 1(sÕ)}. The diameter of an MDP is the maximum
number of expected timesteps to get between any two states D(Mú) = maxs”=sÕ minµ T µ
sæsÕ.
PSRL’s bounds are tighter since (M) Æ CD(M) and may be exponentially smaller.
However  UCRL-Factored has stronger probabilistic guarantees than PSRL since its bounds
hold with high probability for any MDP Mú not just in expectation. There is an optimistic
algorithm REGAL [5] which formally replaces the UCRL2 D with  and retains the high
probability guarantees. An analogous extension to REGAL-Factored is possible  however 
no practical implementation of that algorithm exists even with an FMDP planner.
The algebra in Theorems 1 and 2 can be overwhelming. For clarity  we present a symmetric
problem instance for which we can produce a cleaner single-term upper bound. Let Q be
shorthand for the simple graph structure with l + 1 = m  C = ‡ = 1  |Si| = |Xi| = K and
i | = |ZP
|ZR
Corollary 1 (Clean bounds for PSRL in a symmetric problem).
If „ is the distribution of Mú with structure Q then we can bound the regret of PSRL:
Corollary 2 (Clean bounds for UCRL-Factored in a symmetric problem).
For any MDP Mú with structure Q we can bound the regret of UCRL-Factored:
with probability at least 1 ≠ ”.
Both algorithms satisfy bounds of ˜O(·mÔJKT) which is exponentially tighter than can be
obtained by any Q-naive algorithm. For a factored MDP with m independent components
with S states and A actions the bound ˜O(mSÔAT) is close to the lower bound (mÔSAT)
and so the bound is near optimal. The corollaries follow directly from Theorems 1 and 2 as
shown in Appendix B.

j | = ’ for i = 1  ..  l and j = 1  ..  m  we will write J = K’.
·   Mú)$ Æ 15m·JKT log(2mJT)
  Mú) Æ 15m·JKT log(12mJT /”)

E#Regret(T ﬁ PS

Regret(T ﬁ UC

(3)

(4)

5 Conﬁdence sets
Our analysis will rely upon the construction of conﬁdence sets based around the empirical
estimates for the underlying reward and transition functions. The conﬁdence sets are con-
structed to contain the true MDP with high probability. This technique is common to the
literature  but we will exploit the additional graph structure G to sharpen the bounds.
Consider a family of functions F™M X  (Y Y) which takes x œX to a probability distribu-
tion over (Y  Y). We will write MX  Y unless we wish to stress a particular ‡-algebra.
Deﬁnition 4 (Set widths).
Let X be a ﬁnite set  and let (Y  Y) be a measurable space. The width of a set FœM X  Y
at x œX with respect to a norm Î·Î is

wF(x) := sup
f  fœF

Î(f ≠ f)(x)Î

4

Our conﬁdence set sequence {Ft ™F : t œ N} is initialized with a set F. We adapt our
conﬁdence set to the observations yt œY which are drawn from the true function fú œF
at measurement points xt œX so that yt ≥ fú(xt). Each conﬁdence set is then centered
around an empirical estimate ˆft œM X  Y at time t  deﬁned by

ˆft(x) = 1

nt(x) ÿ·<t:x· =x

”y·  

Ft = Ft(Î·Î   xt≠1

1

where nt(x) is the number of time x appears in (x1  ..  xt≠1) and ”yt is the probability mass
function over Y that assigns all probability to the outcome yt.
Our sequence of conﬁdence sets depends on our choice of norm Î·Î and a non-decreasing
sequence {dt : t œ N}. For each t  the conﬁdence set is deﬁned by:

  dt) :=If œF ---- Î(f ≠ ˆft)(xi)Î ÆÛ dt

nt(xi) ’i = 1  ..  t ≠ 1J .

1

Where xt≠1
is shorthand for (x1  ..  xt≠1) and we interpret nt(xi) = 0 as a null constraint.
The following result shows that we can bound the sum of conﬁdence widths through time.
Theorem 3 (Bounding the sum of widths).
For all ﬁnite sets X  measurable spaces (Y  Y)  function classes F™M X  Y with uniformly
bounded widths wF(x) Æ CF ’x œX and non-decreasing sequences {dt : t œ N}:

Lÿk=1

·ÿi=1

wFk(xtk+i) Æ 4!·CF|X| + 1" + 42dT|X|T

(5)

Proof. The proof follows from elementary counting arguments on nt(x) and the pigeonhole
principle. A full derivation is given in Appendix A.

6 Algorithms
With our notation established  we are now able to introduce our algorithms for ecient
learning in Factored MDPs. PSRL and UCRL-Factored proceed in episodes of ﬁxed policies.
At the start of the kth episode they produce a candidate MDP Mk and then proceed with the
policy which is optimal for Mk. In PSRL  Mk is generated by a sample from the posterior
for Mú  whereas UCRL-Factored chooses Mk optimistically from the conﬁdence set Mk.
Both algorithms require prior knowledge of the graphical structure G and an approximate
planner for FMDPs. We will write (M  ‘) for a planner which returns ‘-optimal policy
for M. We will write ˜(M ‘ ) for a planner which returns an ‘-optimal policy for most
optimistic realization from a family of MDPs M. Given  it is possible to obtain ˜ through
extended value iteration  although this might become computationally intractable [4].
PSRL remains identical to earlier treatment [13  6] provided G is encoded in the prior
„. UCRL-Factored is a modiﬁcation to UCRL2 that can exploit the graph and episodic
t ) as shorthand for these conﬁdence sets
t (dPj
structure of . We write Ri
t(|E[·]|  xt≠1
[ZP
j ]  dPj
t ) generated from initial sets Ri1 =
Ri
j ] Sj.
P C ‡
X[ZR
We should note that UCRL2 was designed to obtain regret bounds even in MDPs without
episodic reset. This is accomplished by imposing artiﬁcial episodes which end whenever
the number of visits to a state-action pair is doubled [4].
It is simple to extend UCRL-
Factored’s guarantees to this setting using this same strategy. This will not work for PSRL
since our current analysis requires that the episode length is independent of the sampled
MDP. Nevertheless  there has been good empirical performance using this method for MDPs
without episodic reset in simulation [6].

t(dRi
t ) and P j
t(Î·Î 1  xt≠1
t ) and P i

1

1

i ]  dRi
[ZR
1 = PX[ZP
i ] R and P j

5

Algorithm 1
PSRL (Posterior Sampling)
1: Input: Prior „ encoding G  t = 1
2: for episodes k = 1  2  .. do
sample Mk ≥ „(·|Ht)
3:
compute µk =( Mk ·/k)
4:
for timesteps j = 1  .. · do
5:
6:
7:
8:
9:
10: end for

sample and apply at = µk(st  j)
observe rt and sm
t+1
t = t + 1

end for

dRi
dPj

Algorithm 2
UCRL-Factored (Optimism)
1: Input: Graph structure G  conﬁdence ”  t = 1
2: for episodes k = 1  2  .. do
i ]|k/”" for i = 1  ..  l
t = 4‡2 log!4l|X[Z R
3:
j ]|k/”" for j = 1  ..  m
t = 4|Sj| log!4m|X[Z P
4:
t ) ’i  j}
t )  Pj œP j
5: Mk = {M |G  Ri œR i
compute µk = ˜(Mk ·/k)
6:
7:
8:
9:
10:
11:
12: end for

sample and apply at = µk(st  u)
observe r1
t+1  ..  sm
t+1
t = t + 1

for timesteps u = 1  .. · do

t and s1

end for

t (dPj

t   ..  rl

t(dRi

7 Analysis
For our common analysis of PSRL and UCRL-Factored we will let ˜Mk refer generally to
either the sampled MDP used in PSRL or the optimistic MDP chosen from Mk with
associated policy ˜µk). We introduce the Bellman operator T M
µ   which for any MDP
M = (S A  RM   P M  · ﬂ )  stationary policy µ : SæA and value function V : Sæ R 
is deﬁned by

µ V (s) := R
T M

M(s  µ(s)) +ÿsÕœS

P M(sÕ|s  µ(s))V (sÕ).

µ i and T M

˜µk i. We will also write xk i := (stk+i  µk(stk+i)).

This returns the expected value of state s where we follow the policy µ under the laws of
M  for one time step. We will streamline our discussion of P M   RM   V M
µ by simply
writing ú in place of Mú or µú and k in place of ˜Mk or ˜µk where appropriate; for example
V úk i := V Mú
We now break down the regret by adding and subtracting the imagined near optimal reward
of policy ˜µK  which is known to the agent. For clarity of analysis we consider only the case
of ﬂ(sÕ) = 1{sÕ = s} but this changes nothing for our consideration of ﬁnite S.
k 1(s)4

(6)
k 1 relates the optimal rewards of the MDP Mú to those near optimal for ˜Mk. We
V úú 1 ≠ V k
can bound this dierence by the planning accuracy 1/k for PSRL in expectation  since
Mú and Mk are equal in law  and for UCRL-Factored in high probability by optimism.
We decompose the ﬁrst term through repeated application of dynamic programming:

k 1(s) ≠ V úk 1(s)4 +3V ú

ú 1(s) ≠ V úk 1(s) =3V k

ú 1(s) ≠ V k

k = V ú

k i+1(stk+i) +

!V k
k 1 ≠ V úk 1" (stk+1) =

·ÿi=1!T k
Where dtk+i :=qsœSÓP ú(s|xk i)(V úk i+1 ≠ V k
tingale dierence bounded by k  the span of V k
to say that k Æ CD [4] and apply the Azuma-Hoeding inequality to say that:

·ÿi=1
k i ≠T úk i" V k
k i+1)(s)Ô ≠ (V úk i+1 ≠ V k
dtk+i > CD2T log(2/”)B Æ ”

PA mÿk=1

·ÿi=1

k i+1)(stk+i) is a mar-
k i. For UCRL-Factored we can use optimism

dtk+1.

(8)

(7)

The remaining term is the one step Bellman error of the imagined MDP ˜Mk. Crucially this
term only depends on states and actions xk i which are actually observed. We can now use

6

the H¨older inequality to bound

·ÿi=1!T k

k i ≠T úk i" V k

k i+1(stk+i) Æ

k(xk i)≠Rú(xk i)|+1

2kÎP k(·|xk i)≠P ú(·|xk i)Î1 (9)

·ÿi=1 |R

7.1 Factorization decomposition
We aim to exploit the graphical structure G to create more ecient conﬁdence sets Mk. It is
clear from (9) that we may upper bound the deviations of Rú  R
k factor-by-factor using the
triangle inequality. Our next result  Lemma 1  shows we can also do this for the transition
functions P ú and P k. This is the key result that allows us to build conﬁdence sets around
each factor P új rather than P ú as a whole.
Lemma 1 (Bounding factored deviations).
Let the transition function class P™P X  S be factored over X = X1 ◊ .. ◊X n and S =
S1 ◊ .. ◊S m with scopes Z1  ..Zm. Then  for any P  ˜P œP we may bound their L1 distance
by the sum of the dierences of their factorizations:

ÎP(x) ≠ ˜P(x)Î1 Æ

mÿi=1 ÎPi(x[Zi]) ≠ ˜Pi(x[Zi])Î1

Proof. We begin with the simple claim that for any –1 – 2 — 1 — 2 œ (0  1]:

|–1–2 ≠ —1—2| = –2----–1 ≠

—1—2

–2 ----

Æ –23|–1 ≠ —1| +----—1 ≠

Æ –2 |–1 ≠ —1| + —1 |–2 ≠ —2|

–2 ----4

—1—2

This result also holds for any –1 – 2 — 1 — 2 œ [0  1]  where 0 can be veriﬁed case by case.
We now consider the probability distributions p  ˜p over {1  ..  d1} and q  ˜q over {1  ..  d2}. We
let Q = pqT   ˜Q = ˜p˜qT be the joint probability distribution over {1  ..  d1}◊{ 1  ..  d2}. Using
the claim above we bound the L1 deviation ÎQ ≠ ˜QÎ1 by the deviations of their factors:

ÎQ ≠ ˜QÎ1 =

d2ÿj=1 |piqj ≠ ˜pi˜qj|
d2ÿj=1
qj|pi ≠ ˜pi| + ˜pi|qj ≠ ˜qj|

d1ÿi=1
d1ÿi=1
Æ
= Îp ≠ ˜pÎ1 + Îq ≠ ˜qÎ1

We conclude the proof by applying this m times to the factored transitions P and ˜P.
7.2 Concentration guarantees for Mk
We now want to show that the true MDP lies within Mk with high probability. Note that
posterior sampling will also allow us to then say that the sampled Mk is within Mk with
high probability too. In order to show this  we ﬁrst present a concentration result for the
L1 deviation of empirical probabilities.
Lemma 2 (L1 bounds for the empirical transition function).
For all ﬁnite sets X  ﬁnite sets Y  function classes P™P X  Y then for any x œX   ‘> 0 the
deviation the true distribution P ú to the empirical estimate after t samples ˆPt is bounded:

P1ÎP ú(x) ≠ ˆPt(x)Î1 Ø ‘2 Æ exp3|Y| log(2) ≠

nt(x)‘2

2 4

7

”Õ" ) Æ ”Õ. We
j ]|k2). Now using a union bound

nt(x) log! 2

Proof. This is a relaxation of the result proved by Weissman [19].

Lemma 2 ensures that for any x œX P(ÎP új (x) ≠ ˆPj t(x)Î1 ØÒ 2|Sj|
tk = 2|Si| log(2/”Õk j) with ”Õk j = ”/(2m|X[ZP
then deﬁne dPj
tk ) ’k œ N  j = 1  ..  m) Ø 1 ≠ ”.
we conclude P(P új œP j
Lemma 3 (Tail bounds for sub ‡-gaussian random variables).
If {‘i} are all independent and sub ‡-gaussian then ’— Ø 0:
‘i| >—B Æ exp3log(2) ≠
PA 1

2‡24

t (dPj

n—2

n|

nÿi=1
P3Mú œM k ’k œ N4 Ø 1 ≠ 2”

i œR i

t(dRi

A similar argument now ensures that P1Rú

tk ) ’k œ N  i = 1  ..  l2 Ø 1 ≠ ”  and so

(10)

7.3 Regret bounds
We now have all the necessary intermediate results to complete our proof. We begin with
the analysis of PSRL. Using equation (10) and the fact that Mú  Mk are equal in law by
posterior sampling  we can say that P(Mú  Mk œM k’k œ N) Ø 1 ≠ 4”. The contributions
k=1· /k Æ 2ÔT. From here we take
from regret in planning function  are bounded byqm
equation (9)  Lemma 1 and Theorem 3 to say that for any ”> 0:
i ]|T<
E#Regret(T ﬁ PS
j ]|T<
T |X[ZP
Let A = {Mú  Mk œM k}  since k Ø 0 and by posterior sampling E[k] = E[] for all k:
1 ≠ 4”4 E[].
E[k|A] Æ P(A)≠1E[] Æ31 ≠
Plugging in dRi
T and setting ” = 1/T completes the proof of Theorem 1. The analysis
of UCRL-Factored and Theorem 2 follows similarly from (8) and (10). Corollaries 1 and 2
follow from substituting the structure Q and upper bounding the constant and logarithmic
terms. This is presented in detail in Appendix B.

i ]| + 1) + 4Ò2dRi
T |X[ZR
j ]| + 1) + 4Ò2dPj
k2 ≠ 4”4 E[] Æ31 + 4”

·   Mú)$ Æ 4”T + 2ÔT +
k=1 .. L!E[k|Mk  Mú œM k]" ◊

lÿi=1;4(·C |X[ZR
mÿj=1;4(·|X[ZP

E[] =31 + 4”

k24≠1

T and dPj

+ sup

4”

8 Conclusion
We present the ﬁrst algorithms with near-optimal regret bounds in factored MDPs. Many
practical problems for reinforcement learning will have extremely large state and action
spaces  this allows us to obtain meaningful performance guarantees even in previously in-
tractably large systems. However  our analysis leaves several important questions unad-
dressed. First  we assume access to an approximate FMDP planner that may be compu-
tationally prohibitive in practice. Second  we assume that the graph structure is known a
priori but there are other algorithms that seek to learn this from experience [20  21]. Finally 
we might consider dimensionality reduction in large MDPs more generally  where either the
rewards  transitions or optimal value function are known to belong in some function class
F to obtain bounds that depend on the dimensionality of F.
Acknowledgments
Osband is supported by Stanford Graduate Fellowships courtesy of PACCAR inc. This work
was supported in part by Award CMMI-0968707 from the National Science Foundation.

8

References
[1] Apostolos Burnetas and Michael Katehakis. Optimal adaptive policies for Markov decision

processes. Mathematics of Operations Research  22(1):222–255  1997.

[2] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.

Machine Learning  49(2-3):209–232  2002.

[3] Ronen Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for
near-optimal reinforcement learning. The Journal of Machine Learning Research  3:213–231 
2003.

[4] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. The Journal of Machine Learning Research  99:1563–1600  2010.

[5] Peter Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement
learning in weakly communicating MDPs. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artiﬁcial Intelligence  pages 35–42. AUAI Press  2009.

[6] Ian Osband  Daniel Russo  and Benjamin Van Roy. (More) Ecient Reinforcement Learning

via Posterior Sampling. Advances in Neural Information Processing Systems  2013.

[7] Craig Boutilier  Richard Dearden  and Mois´es Goldszmidt. Stochastic dynamic programming

with factored representations. Artiﬁcial Intelligence  121(1):49–107  2000.

[8] Zoubin Ghahramani. Learning dynamic bayesian networks. In Adaptive processing of sequences

and data structures  pages 168–197. Springer  1998.

[9] Alexander Strehl. Model-based reinforcement learning in factored-state MDPs. In Approximate
Dynamic Programming and Reinforcement Learning  2007. ADPRL 2007. IEEE International
Symposium on  pages 103–110. IEEE  2007.

[10] Michael Kearns and Daphne Koller. Ecient reinforcement learning in factored MDPs. In

IJCAI  volume 16  pages 740–747  1999.

[11] Istv´an Szita and Andr´as L˝orincz. Optimistic initialization and greediness lead to polynomial
time learning in factored MDPs. In Proceedings of the 26th Annual International Conference
on Machine Learning  pages 1001–1008. ACM  2009.

[12] William Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika  25(3/4):285–294  1933.

[13] Malcom Strens. A Bayesian framework for reinforcement learning. In Proceedings of the 17th

International Conference on Machine Learning  pages 943–950  2000.

[14] Carlos Guestrin  Daphne Koller  Ronald Parr  and Shobha Venkataraman. Ecient solution

algorithms for factored MDPs. J. Artif. Intell. Res.(JAIR)  19:399–468  2003.

[15] Daphne Koller and Ronald Parr. Policy iteration for factored MDPs. In Proceedings of the Six-
teenth conference on Uncertainty in artiﬁcial intelligence  pages 326–334. Morgan Kaufmann
Publishers Inc.  2000.

[16] Carlos Guestrin  Daphne Koller  and Ronald Parr. Max-norm projections for factored MDPs.

In IJCAI  volume 1  pages 673–682  2001.

[17] Karina Valdivia Delgado  Scott Sanner  and Leliane Nunes De Barros. Ecient solutions to
factored MDPs with imprecise transition probabilities. Artiﬁcial Intelligence  175(9):1498–
1527  2011.

[18] Scott Sanner and Craig Boutilier. Approximate linear programming for ﬁrst-order MDPs.

arXiv preprint arXiv:1207.1415  2012.

[19] Tsachy Weissman  Erik Ordentlich  Gadiel Seroussi  Sergio Verdu  and Marcelo J Weinberger.
Inequalities for the L1 deviation of the empirical distribution. Hewlett-Packard Labs  Tech.
Rep  2003.

[20] Alexander Strehl  Carlos Diuk  and Michael Littman. Ecient structure learning in factored-

state MDPs. In AAAI  volume 7  pages 645–650  2007.

[21] Carlos Diuk  Lihong Li  and Bethany R Leer. The adaptive k-meteorologists problem and its
application to structure learning and feature selection in reinforcement learning. In Proceedings
of the 26th Annual International Conference on Machine Learning  pages 249–256. ACM  2009.

9

,Shiau Hong Lim
Huan Xu
Shie Mannor
Ian Osband
Benjamin Van Roy
Weifeng Chen
Zhao Fu
Dawei Yang
Jia Deng