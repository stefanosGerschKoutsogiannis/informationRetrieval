2018,Q-learning with Nearest Neighbors,We consider model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel  when only a single sample path under an arbitrary policy of the system is available.  We consider the Nearest Neighbor Q-Learning (NNQL) algorithm to learn the optimal Q function using nearest neighbor regression method. As the main contribution  we provide tight finite sample analysis of the convergence rate. In particular  for MDPs with a $d$-dimensional state space and the discounted factor $\gamma \in (0 1)$  given an arbitrary sample path with ``covering time'' $L$  we establish that the algorithm is guaranteed to output an $\varepsilon$-accurate estimate of the optimal Q-function using  $\Ot(L/(\varepsilon^3(1-\gamma)^7))$ samples. For instance  for a well-behaved MDP  the covering time of the sample path under the purely random policy scales as $\Ot(1/\varepsilon^d) $ so the sample complexity scales as $\Ot(1/\varepsilon^{d+3}).$ Indeed  we establish a lower bound that argues that the dependence of $ \Omegat(1/\varepsilon^{d+2})$ is necessary.,Q-learning with Nearest Neighbors

Massachusetts Institute of Technology

Massachusetts Institute of Technology

Qiaomin Xie ⇤

qxie@mit.edu

Devavrat Shah ⇤

devavrat@mit.edu

Abstract

We consider model-free reinforcement learning for inﬁnite-horizon discounted
Markov Decision Processes (MDPs) with a continuous state space and unknown
transition kernel  when only a single sample path under an arbitrary policy of
the system is available. We consider the Nearest Neighbor Q-Learning (NNQL)
algorithm to learn the optimal Q function using nearest neighbor regression method.
As the main contribution  we provide tight ﬁnite sample analysis of the convergence
rate. In particular  for MDPs with a d-dimensional state space and the discounted
factor  2 (0  1)  given an arbitrary sample path with “covering time” L  we
establish that the algorithm is guaranteed to output an "-accurate estimate of the

behaved MDP  the covering time of the sample path under the purely random policy

optimal Q-function using eOL/("3(1  )7) samples. For instance  for a well-
scales as eO1/"d  so the sample complexity scales as eO1/"d+3. Indeed  we
establish a lower bound that argues that the dependence ofe⌦1/"d+2 is necessary.

1

Introduction

Markov Decision Processes (MDPs) are natural models for a wide variety of sequential decision-
making problems. It is well-known that the optimal control problem in MDPs can be solved  in
principle  by standard algorithms such as value and policy iterations. These algorithms  however  are
often not directly applicable to many practical MDP problems for several reasons. First  they do not
scale computationally as their complexity grows quickly with the size of the state space and especially
for continuous state space. Second  in problems with complicated dynamics  the transition kernel of
the underlying MDP is often unknown  or an accurate model thereof is lacking. To circumvent these
difﬁculties  many model-free Reinforcement Learning (RL) algorithms have been proposed  in which
one estimates the relevant quantities of the MDPs (e.g.  the value functions or the optimal policies)
from observed data generated by simulating the MDP.
A popular model-free Reinforcement Learning (RL) algorithm is the so called Q-learning [47]  which
directly learns the optimal action-value function (or Q function) from the observations of the system
trajectories. A major advantage of Q-learning is that it can be implemented in an online  incremental
fashion  in the sense that Q-learning can be run as data is being sequentially collected from the system
operated/simulated under some policy  and continuously reﬁnes its estimates as new observations
become available. The behaviors of standard Q-learning in ﬁnite state-action problems have by now
been reasonably understood; in particular  both asymptotic and ﬁnite-sample convergence guarantees
have been established [43  22  41  18].
In this paper  we consider the general setting with continuous state spaces. For such problems 
existing algorithms typically make use of a parametric function approximation method  such as a
linear approximation [27]  to learn a compact representation of the action-value function. In many of

⇤Both authors are afﬁliated with Laboratory for Information and Decision Systems (LIDS). DS is with the

Department of EECS as well as Statistics and Data Science Center at MIT.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

the recently popularized applications of Q-learning  much more expressive function approximation
method such as deep neural networks have been utilized. Such approaches have enjoyed recent
empirical success in game playing and robotics problems [38  29  14]. Parametric approaches typically
require careful selection of approximation method and parametrization (e.g.  the architecture of neural
networks). Further  rigorous convergence guarantees of Q-learning with deep neural networks are
relatively less understood. In comparison  non-parametric approaches are  by design  more ﬂexible
and versatile. However  in the context of model-free RL with continuous state spaces  the convergence
behaviors and ﬁnite-sample analysis of non-parametric approaches are less understood.
Summary of results.
In this work  we consider a natural combination of the Q-learning with
Kernel-based nearest neighbor regression for continuous state-space MDP problems  denoted as
Nearest-Neighbor based Q-Learning (NNQL). As the main result  we provide ﬁnite sample analysis
of NNQL for a single  arbitrary sequence of data for any inﬁnite-horizon discounted-reward MDPs
with continuous state space. In particular  we show that the algorithm outputs an "-accurate (with
respect to supremum norm) estimate of the optimal Q-function with high probability using a number
of observations that depends polynomially on "  the model parameters and the “cover time” of the
sequence of the data or trajectory of the data utilized. For example  if the data was sampled per a
completely random policy  then our generic bound suggests that the number of samples would scale

bound stating that for any policy to learn optimal Q function within " approximation  the number of

as eO(1/"d+3) where d is the dimension of the state space. We establish effectively matching lower
samples required must scale ase⌦(1/"d+2). In that sense  our policy is nearly optimal.

Our analysis consists of viewing our algorithm as a special case of a general biased stochastic
approximation procedure  for which we establish non-asymptotic convergence guarantees. Key to our
analysis is a careful characterization of the bias effect induced by nearest-neighbor approximation
of the population Bellman operator  as well as the statistical estimation error due to the variance of
ﬁnite  dependent samples. Speciﬁcally  the resulting Bellman nearest neighbor operator allows us
to connect the update rule of NNQL to a class of stochastic approximation algorithms  which have
biased noisy updates. Note that traditional results from stochastic approximation rely on unbiased
updates and asymptotic analysis [35  43]. A key step in our analysis involves decomposing the update
into two sub-updates  which bears some similarity to the technique used by [22]. Our results make
improvement in characterizing the ﬁnite-sample convergence rates of the two sub-updates.
In summary  the salient features of our work are
• Unknown system dynamics: We assume that the transition kernel and reward function of the
MDP is unknown. Consequently  we cannot exactly evaluate the expectation required in standard
dynamic programming algorithms (e.g.  value/policy iteration). Instead  we consider a sample-
based approach which learns the optimal value functions/policies by directly observing data
generated by the MDP.

• Single sample path: We are given a single  sequential samples obtained from the MDP operated
under an arbitrary policy. This in particular means that the observations used for learning are
dependent. Existing work often studies the easier settings where samples can be generated at will;
that is  one can sample any number of (independent) transitions from any given state  or reset
the system to any initial state. For example  Parallel Sampling in [23]. We do not assume such
capabilities  but instead deal with the realistic  challenging setting with a single path.

• Online computation: We assume that data arrives sequentially rather than all at once. Estimates
are updated in an online fashion upon observing each new sample. Moreover  as in standard
Q-learning  our approach does not store old data. In particular  our approach differs from other
batch methods  which need to wait for all data to be received before starting computation  and
require multiple passes over the data. Therefore  our approach is space efﬁcient  and hence can
handle the data-rich scenario with a large  increasing number of samples.

• Non-asymptotic  near optimal guarantees: We characterize the ﬁnite-sample convergence rate
of our algorithm; that is  how many samples are needed to achieve a given accuracy for estimating
the optimal value function. Our analysis is nearly tight in that we establish a lower bound that
nearly matches our generic upper bound specialized to setting when data is generated per random
policy or more generally any policy with random exploration component to it.

While there is a large and growing literature on Reinforcement Learning for MDPs  to the best of our
knowledge  ours is the ﬁrst result on Q-learning that simultaneously has all of the above four features.

2

Table 1: Summary of relevant work. See Appendix A for details.

Speciﬁc work

[10]  [36]  [37]
[43]  [22]  [41]
[20]  [3]  [18]
[23]
[42] [28]
[33]  [32]
[19]
[44]
[12]
[8]
[9]
[30]
[1]
Our work

Method

Continuous
state space

Unknown

transition Kernel

Single

sample path

Finite-state approximation

Q-learning
Q-learning
Q-learning
Q-learning

Kernel-based approximation

Value/Policy iteration

Parameterized TD-learning
Parameterized TD-learning
Parameterized TD-learning

Non-parametric LP
Fitted value iteration
Fitted policy iteration

Q-learning

Yes
No
No
No
Yes
Yes
No
No
No
No
No
Yes
Yes
Yes

No
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes

No
Yes
Yes
No
Yes
No
No
Yes
No
Yes
No
No
Yes
Yes

guarantees

Online Non-asymptotic
update
Yes
Yes
Yes
Yes
Yes
No
No
Yes
Yes
Yes
No
No
No
Yes

Yes
No
Yes
Yes
No
No
Yes
No
Yes
Yes
Yes
Yes
Yes
Yes

We summarize comparison with relevant prior works in Table 1. Detailed discussion can be found in
Appendix A.
2 Setup
In this section  we introduce necessary notations  deﬁnitions for the framework of Markov Decision
Processes that will be used throughout the paper. We also precisely deﬁne the question of interest.

Notation. For a metric space E endowed with metric ⇢  we denote by C(E) the set of all bounded and
measurable functions on E. For each f 2 C(E)  let kfk1 := supx2E|f (x)| be the supremum norm 
which turns C(E) into a Banach space B. Let Lip(E  M ) denote the set of Lipschitz continuous
functions on E with Lipschitz bound M  i.e. 

Lip(E  M ) = {f 2 C(E) | |f (x)  f (y)| M⇢ (x  y)  8x  y 2 E} .

The indicator function is denoted by 1{·}. For each integer k  0  let [k]   {1  2  . . .   k}.
Markov Decision Process. We consider a general setting where an agent interacts with a stochastic
environment. This interaction is modeled as a discrete-time discounted Markov decision process
(MDP). An MDP is described by a ﬁve-tuple (X  A  p  r  )  where X and A are the state space and
action space  respectively. We shall utilize t 2 N to denote time. Let xt 2X be state at time t. At
time t  the action chosen is denoted as at 2A . Then the state evolution is Markovian as per some
transition probability kernel with density p (with respect to the Lebesgue measure  on X ). That is 
(1)

Pr(xt+1 2 B|xt = x  at = a) =ZB

p(y|x  a)(dy)

for any measurable set B 2X . The one-stage reward earned at time t is a random variable Rt
with expectation E[Rt|xt = x  at = a] = r(x  a)  where r : X⇥A! R is the expected reward
function. Finally   2 (0  1) is the discount factor and the overall reward of interest isP1t=0 tRt
The goal is to maximize the expected value of this reward. Here we consider a distance function
⇢ : X⇥X! R+ so that (X  ⇢ ) forms a metric space. For the ease of exposition  we use Z for the
joint state-action space X⇥A .
We start with the following standard assumptions on the MDP:
Assumption 1 (MDP Regularity). We assume that: (A1.) The continuous state space X is a compact
subset of Rd; (A2.) A is a ﬁnite set of cardinality |A|; (A3.) The one-stage reward Rt is non-
negative and uniformly bounded by Rmax  i.e.  0  Rt  Rmax almost surely. For each a 2A  
r(·  a) 2 Lip(X   Mr) for some Mr > 0. (A4.) The transition probability kernel p satisﬁes
where the function Wp(·) satisﬁesRX

|p(y|x  a)  p(y|x0  a)| Wp(y)⇢ (x  x0)  
Wp(y)(dy)  Mp.

8a 2A  8x  x0  y 2X  

3

The ﬁrst two assumptions state that the state space is compact and the action space is ﬁnite. The third
and forth stipulate that the reward and transition kernel are Lipschitz continuous (as a function of the
current state). Our Lipschitz assumptions are identical to (or less restricted than) those used in the
work of [36]  [11]  and [17]. In general  this type of Lipschitz continuity assumptions are standard in
the literature on MDPs with continuous state spaces; see  e.g.  the work of [15  16]  and [6].
A Markov policy ⇡(·|x) gives the probability of performing action a 2A given the current state
x. A deterministic policy assigns each state a unique action. The value function for each state x
under policy ⇡  denoted by V ⇡(x)  is deﬁned as the expected discounted sum of rewards received
following the policy ⇡ from initial state x  i.e.  V ⇡(x) = E⇡ [P1t=0 tRt|x0 = x]. The action-value
function Q⇡ under policy ⇡ is deﬁned by Q⇡(x  a) = r(x  a) + Ry p(y|x  a)V ⇡(y)(dy). The

number Q⇡(x  a) is called the Q-value of the pair (x  a)  which is the return of initially performing
action a at state s and then following policy ⇡. Deﬁne

   1/(1  )

and

Vmax   Rmax.

Since all the rewards are bounded by Rmax  it is easy to see that the value function of every policy
is bounded by Vmax [18  40]. The goal is to ﬁnd an optimal policy ⇡⇤ that maximizes the value
from any start state. The optimal value function V ⇤is deﬁned as V ⇤(x) = V ⇡⇤(x) = sup⇡ V ⇡(x) 
8x 2X . The optimal action-value function is deﬁned as Q⇤(x  a) = Q⇡⇤(x  a) = sup⇡ Q⇡(x  a).
The Bellman optimality operator F is deﬁned as
(F Q)(x  a) = r(x  a) + Emax
It is well known that F is a contraction with factor  on the Banach space C(Z) [7  Chap. 1]. The
optimal action-value function Q⇤ is the unique solution of the Bellman’s equation Q = F Q in
C(X⇥A ). In fact  under our setting  it can be show that Q⇤ is bounded and Lipschitz. This is stated
below and established in Appendix B.
Lemma 1. Under Assumption 1  the function Q⇤ satisﬁes that kQ⇤k1  Vmax and that Q⇤(·  a) 2
Lip(X   Mr + VmaxMp) for each a 2A .
3 Reinforcement Learning Using Nearest Neighbors

Q(x0  b) | x  a = r(x  a) + ZX

p(y|x  a) max
b2A

Q(y  b)(dy).

b2A

In this section  we present the nearest-neighbor-based reinforcement learning algorithm. The al-
gorithm is based on constructing a ﬁnite-state discretization of the original MDP  and combining
Q-learning with nearest neighbor regression to estimate the Q-values over the discretized state space 
which is then interpolated and extended to the original continuous state space. In what follows  we
shall ﬁrst describe several building blocks for the algorithm in Sections 3.1–3.4  and then summarize
the algorithm in Section 3.5.
3.1 State Space Discretization
Let h > 0 be a pre-speciﬁed scalar parameter. Since the state space X is compact  one can ﬁnd a
ﬁnite set Xh   {ci}Nh

i=1 of points in X such that

min
i2[Nh]

⇢(x  ci) < h  8x 2X .

The ﬁnite grid Xh is called an h-net of X   and its cardinality n ⌘ Nh can be chosen to be the
h-covering number of the metric space (X  ⇢ ). Deﬁne Zh = Xh ⇥A . Throughout this paper  we
denote by Bi the ball centered at ci with radius h; that is  Bi   {x 2X : ⇢ (x  ci)  h} .
3.2 Nearest Neighbor Regression
Suppose that we are given estimated Q-values for the ﬁnite subset of states Xh = {ci}n
i=1  denoted
by q = {q(ci  a)  ci 2X h  a 2A} . For each state-action pair (x  a) 2X⇥A   we can predict its
Q-value via a regression method. We focus on nonparametric regression operators that can be written
as nearest neighbors averaging in terms of the data q of the form

(2)
8x 2X   a 2A  
where K(x  ci)  0 is a weighting kernel function satisfyingPn
i=1 K(x  ci) = 1 8x 2X . Equa-
tion (2) deﬁnes the so-called Nearest Neighbor (NN) operator NN  which maps the space C(Xh ⇥A )

i=1K(x  ci)q(ci  a) 

(NNq)(x  a) =Pn

4

into the set of all bounded function over X⇥A . Intuitively  in (2) one assesses the Q-value of (x  a)
by looking at the training data where the action a has been applied  and by averaging their values. It
can be easily checked that the operator NN is non-expansive in the following sense:

kNNq  NNq0k1  kq  q0k1  

8q  q0 2 C(Xh ⇥A ).

This property will be crucially used for establishing our results. K is assumed to satisfy

(3)

8x 2X   y 2X h 

K(x  y) = 0 if ⇢(x  y)  h 

(4)
where h is the discretization parameter deﬁned in Section 3.1.2 This means that the values of states
located in the neighborhood of x are more inﬂuential in the averaging procedure (2). There are many
possible choices for K. In Section C we describe three representative choices that correspond to
k-Nearest Neighbor Regression  Fixed-Radius Near Neighbor Regression and Kernel Regression.
3.3 A Joint Bellman-NN Operator
Now  we deﬁne the joint Bellman-NN (Nearest Neighbor) operator. As will become clear subse-
quently  it is this operator that the algorithm aims to approximate  and hence it plays a crucial role in
the subsequent analysis.
For a function q : Zh ! R  we denote by ˜Q   (NNq) the nearest-neighbor average extension of q
to Z; that is 
The joint Bellman-NN operator G on R|Zh| is deﬁned by composing the original Bellman operator F
with the NN operator NN and then restricting to Zh; that is  for each (ci  a) 2Z h 
(Gq)(ci  a)   (F NNq)(ci  a) = (F ˜Q)(ci  a) = r(ci  a) + Emax

(NNq)(x0  b) | ci  a .

˜Q(x  a) = ( NNq)(x  a) 

8(x  a) 2Z .

b2A

(5)

It can be shown that G is a contraction operator with modulus  mapping R|Zh| to itself  thus
admitting a unique ﬁxed point  denoted by q⇤h; see Appendix E.2.
3.4 Covering Time of Discretized MDP
As detailed in Section 3.5 to follow  our algorithm uses data generated by an abritrary policy ⇡ for
the purpose of learning. The goal of our approach is to estimate the Q-values of every state. For there
to be any hope to learn something about the value of a given state  this state (or its neighbors) must
be visited at least once. Therefore  to study the convergence rate of the algorithm  we need a way to
quantify how often ⇡ samples from different regions of the state-action space Z = X⇥A .
Following the approach taken by [18] and [3]  we introduce the notion of the covering time of MDP
under a policy ⇡. This notion is particularly suitable for our setting as our algorithm is based on
asynchronous Q-learning (that is  we are given a single  sequential trajectory of the MDP  where at
each time step one state-action pair is observed and updated)  and the policy ⇡ may be non-stationary.
In our continuous state space setting  the covering time is deﬁned with respect to the discretized space
Zh  as follows:
Deﬁnition 1 (Covering time of discretized MDP). For each 1  i  n = Nh and a 2A   a ball-
action pair (Bi  a) is said to be visited at time t if xt 2B i and at = a. The discretized state-action
space Zh is covered by the policy ⇡ if all the ball-action pairs are visited at least once under the
policy ⇡. Deﬁne ⌧⇡ h(x  t)  the covering time of the MDP under the policy ⇡  as the minimum number
of steps required to visit all ball-action pairs starting from state x 2X at time-step t  0. Formally 
⌧⇡ h(x  t) is deﬁned as
minns  0 : xt = x  8i Nh  a2A   9ti a2 [t  t+s]  such that xti a 2 Bi and ati a = a  under ⇡o 
with notation that minimum over empty set is 1.
We shall assume that there exists a policy ⇡ with bounded expected cover time  which guarantees
that  asymptotically  all the ball-action pairs are visited inﬁnitely many times under the policy ⇡.

2This assumption is not absolutely necessary  but is imposed to simplify subsequent analysis. In general  our

results hold as long as K(x  y) decays sufﬁciently fast with the distance ⇢(x  y).

5

Assumption 2. There exists an integer Lh < 1 such that E[⌧⇡ h(x  t)]  Lh  8x 2X   t > 0. Here
the expectation is deﬁned with respect to randomness introduced by Markov kernel of MDP as well
as the policy ⇡.

In general  the covering time can be large in the worst case. In fact  even with a ﬁnite state space 
it is easy to ﬁnd examples where the covering time is exponential in the number of states for every
policy. For instance  consider an MDP with states 1  2  . . .   N  where at any state i  the chain is reset
to state 1 with probability 1/2 regardless of the action taken. Then  every policy takes exponential
time to reach state N starting from state 1  leading to an exponential covering time.
To avoid the such bad cases  some additional assumptions are needed to ensure that the MDP is
well-behaved. For such MDPs  there are a variety of polices that have a small covering time. Below
we focus on a class of MDPs satisfying a form of the uniform ergodic assumptions  and show that the
standard "-greedy policy (which includes the purely random policy as special case by setting " = 1)
has a small covering time. This is done in the following two Propositions. Proofs can be found in
Appendix D.
Proposition 1. Suppose that the MDP satisﬁes the following: there exists a probability measure ⌫
on X   a number '> 0 and an integer m  1 such that for all x 2X   all t  0 and all policies µ 
(6)
Let ⌫min   mini2[n] ⌫(Bi)  where we recall that n ⌘ Nh = |Xh| is the cardinality of the
discretized state space. Then the expected covering time of "-greedy is upper bounded by
Lh = O⇣ m|A|
Proposition 2. Suppose that the MDP satisﬁes the following: there exists a probability measure ⌫ on
X   a number '> 0 and an integer m  1 such that for all x 2X   all t  0  there exists a sequence
of actions ˆa(x) = (ˆa1  . . .   ˆam) 2A m 

Prµ (xm+t 2·| xt = x)  '⌫(·).

log(n|A|)⌘.

"'⌫min

"m+1'⌫min

Pr (xm+t 2·| xt = x  at = ˆa1  . . .   at+m1 = ˆam)  '⌫(·).

(7)
Let ⌫min   mini2[n] ⌫(Bi)  where we recall that n ⌘ Nh = |Xh| is the cardinality of the
discretized state space. Then the expected covering time of "-greedy is upper bounded by
Lh = O⇣ m|A|m+1

log(n|A|)⌘.

3.5 Q-learning using Nearest Neighbor
We describe the nearest-neighbor Q-learning (NNQL) policy. Like Q-learning  it is a model-free
policy for solving MDP. Unlike standard Q-learning  it is (relatively) efﬁcient to implement as it
does not require learning the Q function over entire space X⇥A . Instead  we utilize the nearest
neighbor regressed Q function using the learned Q values restricted to Zh. The policy assumes access
to an existing policy ⇡ (which is sometimes called the “exploration policy”  and need not have any
optimality properties) that is used to sample data points for learning.
The pseudo-code of NNQL is described in Policy 1. At each time step t  action at is performed
from state Yt as per the given (potentially non-optimal) policy ⇡  and the next state Yt+1 is generated
according to p(·|Yt  at). Note that the sequence of observed states (Yt) take continuous values in the
state space X .
The policy runs over iteration with each iteration lasting for a number of time steps. Let k denote
iteration count  Tk denote time when iteration k starts for k 2 N.
Initially  k = 0  T0 = 0 
and for t 2 [Tk  Tk+1)  the policy is in iteration k. The iteration is updated from k to k + 1
when starting with t = Tk  all ball-action (Bi  a) pairs have been visited at least once. That is 
Tk+1 = Tk + ⌧⇡ h(YTk   Tk). In the policy description  the counter Nk(ci  a) records how many times
the ball-action pair (Bi  a) has been visited from the beginning of iteration k till the current time t;
that is  Nk(ci  a) =Pt
1{Ys 2B i  as = a}. By deﬁnition  the iteration k ends at the ﬁrst time
step for which min(ci a) Nk(ci  a) > 0.
During each iteration  the policy keeps track of the Q-function over the ﬁnite set Zh. Speciﬁcally  let
qk denote the approximate Q-values on Zh within iteration k. The policy also maintains Gkqk(ci  at) 
which is a biased empirical estimate of the joint Bellman-NN operator G applied to the estimates qk.

s=Tk

6

Policy 1 Nearest-Neighbor Q-learning
Input: Exploration policy ⇡  discount factor   number of steps T   bandwidth parameter h  and
initial state Y0.
Construct discretized state space Xh; initialize t = k = 0 ↵ 0 = 1  q0 ⌘ 0;
Foreach (ci  a) 2Z h  set N0(ci  a) = 0; end
repeat

Draw action at ⇠ ⇡(·|Yt) and observe reward Rt; generate the next state Yt+1 ⇠ p(·|Yt  at);
Foreach i such that Yt 2B i do

1

Nk(ci at)+1 ;

⌘N =
if Nk(ci  at) > 0 then

(Gkqk)(ci  at) = (1  ⌘N )(Gkqk)(ci  at) + ⌘NRt +  maxb2A(NNqk)(Yt+1  b);

else (Gkqk)(ci  at) = Rt +  maxb2A(NNqk)(Yt+1  b);
end
Nk(ci  at) = Nk(ci  at) + 1
end
if min(ci a)2Zh Nk(ci  a) > 0 then

qk+1(ci  a) = (1  ↵k)qk(ci  a) + ↵k(Gkqk)(ci  a);

Foreach (ci  a) 2Z h do
end
k = k + 1; ↵k = 
Foreach (ci  a) 2Z h do Nk(ci  a) = 0; end

+k ;

end
t = t + 1;

until t  T ;
return ˆq = qk

At each time step t 2 [Tk  Tk+1) within iteration k  if the current state Yt falls in the ball Bi  then the
corresponding value (Gkqk)(ci  at) is updated as

(Gkqk)(ci  at) = (1  ⌘N )(Gkqk)(ci  at) + ⌘N⇣Rt +  max

b2A

(NNqk)(Yt+1  b)⌘ 

(8)

1

where ⌘N =
Nk(ci at)+1. We notice that the above update rule computes  in an incremental fashion 
an estimate of the joint Bellman-NN operator G applied to the current qk for each discretized state-
action pair (ci  a)  using observations Yt that fall into the neighborhood Bi of ci. This nearest-neighbor
approximation causes the estimate to be biased.
At the end of iteration k  i.e.  at time step t = Tk+1  1  a new qk+1 is generated as follows: for each
(ci  a) 2Z h 

qk+1(ci  a) = (1  ↵k)qk(ci  a) + ↵k(Gkqk)(ci  a).

(9)
At a high level  this update is similar to standard Q-learning updates — the Q-values are updated
by taking a weighted average of qk  the previous estimate  and Gkqk  an one-step application of
the Bellman operator estimated using newly observed data. There are two main differences from
standard Q-learning: 1) the Q-value of each (ci  a) is estimated using all observations that lie in its
neighborhood — a key ingredient of our approach; 2) we wait until all ball-action pairs are visited to
update their Q-values  all at once.
Given the output ˆq of Policy 1  we obtain an approximate Q-value for each (continuous) state-action
pair (x  a) 2Z via the nearest-neighbor average operation  i.e.  QT
h (x  a) = ( NN ˆq) (x  a); here the
superscript T emphasizes that the algorithm is run for T time steps with a sample size of T .

4 Main Results

As a main result of this paper  we obtain ﬁnite-sample analysis of NNQL policy. Speciﬁcally  we ﬁnd
that the NNQL policy converges to an "-accurate estimate of the optimal Q⇤ with time T that has
polynomial dependence on the model parameters. The proof can be found in Appendix E.

7

"2

T = C0

Lh⇤V 3
max4
"3

◆ log✓ Nh⇤ |A| V 2
h⇤  Q⇤1  ".

Theorem 1. Suppose that Assumptions 1 and 2 hold. With notation  = 1/(1  ) and C =
Mr + VmaxMp  for a given " 2 (0  4Vmax)  deﬁne h⇤ ⌘ h⇤(") = "
4C . Let Nh⇤ be the h⇤-
covering number of the metric space (X  ⇢ ). For a universal constant C0 > 0  after at most
◆
max4

log✓ 2
steps  with probability at least 1    we haveQT

The theorem provides sufﬁcient conditions for NNQL to achieve " accuracy (in sup norm) for
estimating the optimal action-value function Q⇤. The conditions involve the bandwidth parameter
h⇤ and the number of time steps T   both of which depend polynomially on the relevant problem
parameters. Here an important parameter is the covering number Nh⇤: it provides a measure of
the “complexity” of the state space X   replacing the role of the cardinality |X| in the context of
discrete state spaces. For instance  for a unit volume ball in Rd  the corresponding covering number
Nh⇤ scales as O(1/h⇤)d (cf. Proposition 4.2.12 in [46]). We take note of several remarks on the

implications of the theorem.
Sample complexity: The number of time steps T   which also equals the number of samples needed 
scales linearly with the covering time Lh⇤ of the underlying policy ⇡ to sample data for the given
MDP. Note that Lh⇤ depends implicitly on the complexities of the state and action space as measured
by Nh⇤ and |A|. In the best scenario  Lh⇤  and hence T as well  is linear in Nh⇤ ⇥|A| (up to
logarithmic factors)  in which case we achieve (near) optimal linear sample complexity. The sample
complexity T also depends polynomially on the desired accuracy "1 and the effective horizon
 = 1/(1  ) of the discounted MDP — optimizing the exponents of the polynomial dependence
remains interesting future work.
Space complexity: The space complexity of NNQL is O(Nh⇤ ⇥|A| )  which is necessary for storing
the values of qk. Note that NNQL is a truly online algorithm  as each data point (Yt  at) is accessed
only once upon observation and then discarded; no storage of them is needed.
Computational complexity: In terms of computational complexity  the algorithm needs to compute
the NN operator NN and maximization over A in each time step  as well as to update the values of qk
for all ci 2X h⇤ and a 2A in each iteration. Therefore  the worst-case computational complexity per
time step is O(Nh⇤ ⇥|A| )  with an overall complexity of O(T ⇥ Nh⇤ ⇥|A| ). The computation can
be potentially sped up by using more efﬁcient data structures and algorithms for ﬁnding (approximate)
nearest neighbors  such as k-d trees [5]  random projection trees [13]  Locality Sensitive Hashing [21]
and boundary trees [26].
Choice of h⇤: NNQL requires as input a user-speciﬁed parameter h  which determines the discretiza-
tion granularity of the state space as well as the bandwidth of the (kernel) nearest neighbor regression.
Theorem 1 provides a desired value h⇤ = "/4C  where we recall that C is the Lipschitz parameter
of the optimal action-value function Q⇤ (see Lemma 1). Therefore  we need to use a small h⇤ if we
demand a small error "  or if Q⇤ ﬂuctuates a lot with a large C.

4.1 Special Cases and Lower Bounds
Theorem 1  combined with Proposition 1  immediately yield the following bound that quantify the
number of samples required to obtain an "-optimal action-value function with high probability  if the
sample path is generated per the uniformly random policy. The proof is given in Appendix F.
Corollary 1. Suppose that Assumptions 1 and 2 hold  with X = [0  1]d. Assume that the MDP
satisﬁes the following: there exists a uniform probability measure ⌫ over X   a number '> 0 and an
integer m  1 such that for all x 2X   all t  0 and all policies µ  Prµ (xm+t 2·| xt = x)  '⌫(·).
After at most

T = 

1

"d+3 log3✓ 1
"◆

with probability at least 1  .

steps  where  ⌘ (|A|  d   m ) is a number independent of " and   we haveQT
Corollary 1 states that the sample complexity of NNQL scales as eO 1

effectively necessary by establishing a lower bound on any algorithm under any sampling policy!
The proof of Theorem 2 can be found in Appendix G.

h⇤  Q⇤1  "
"d+3. We will show that this is

8

Theorem 2. For any reinforcement learning algorithm ˆQT and any number  2 (0  1)  there exists
an MDP problem and some number T > 0 such that

where C > 0 is a constant. Consequently  for any reinforcement learning algorithm ˆQT and any
sufﬁciently small "> 0  there exists an MDP problem such that in order to achieve

2+d   

for all T  T 

T ◆ 1
Pr ˆQT  Q⇤1  C✓ log T
Prh ˆQT  Q⇤1
T  C0d✓ 1
"◆2+d

<"i  1   
log✓ 1
"◆  

one must have

where C0 > 0 is a constant.

5 Conclusions

In this paper  we considered the reinforcement learning problem for inﬁnite-horizon discounted
MDPs with a continuous state space. We focused on a reinforcement learning algorithm NNQL
that is based on kernelized nearest neighbor regression. We established nearly tight ﬁnite-sample
convergence guarantees showing that NNQL can accurately estimate optimal Q function using nearly
optimal number of samples. In particular  our results state that the sample  space and computational
complexities of NNQL scale polynomially (sometimes linearly) with the covering number of the
state space  which is continuous and has uncountably inﬁnite cardinality.
In this work  the sample complexity analysis with respect to the accuracy parameter is nearly optimal.
But its dependence on the other problem parameters is not optimized. This will be an important
direction for future work. It is also interesting to generalize approach to the setting of MDP beyond
inﬁnite horizon discounted problems  such as ﬁnite horizon or average-cost problems. Another
possible direction for future work is to combine NNQL with a smart exploration policy  which may
further improve the performance of NNQL. It would also be of much interest to investigate whether
our approach  speciﬁcally the idea of using nearest neighbor regression  can be extended to handle
inﬁnite or even continuous action spaces.

Acknowledgment

This work was supported in parts by NSF projects NeTs-1523546  TRIPODS-1740751  and CMMI-
1462158.

References
[1] A. Antos  C. Szepesvári  and R. Munos. Learning near-optimal policies with Bellman-residual
minimization based ﬁtted policy iteration and a single sample path. Machine Learning  71(1):89–
129  2008.

[2] M. G. Azar  R. Munos  M. Ghavamzadeh  and H. J. Kappen. Reinforcement learning with a

near optimal rate of convergence. Technical Report  2011.

[3] M. G. Azar  R. Munos  M. Ghavamzadeh  and H. J. Kappen. Speedy Q-learning. In NIPS  2011.

[4] A. Barreto  D. Precup  and J. Pineau. Practical kernel-based reinforcement learning. The

Journal of Machine Learning Research  17(1):2372–2441  2016.

[5] J. L. Bentley. Multidimensional binary search trees in database applications. IEEE Transactions

on Software Engineering  (4):333–340  1979.

[6] D. Bertsekas. Convergence of discretization procedures in dynamic programming.

Transactions on Automatic Control  20(3):415–419  1975.

IEEE

9

[7] D. P. Bertsekas. Dynamic programming and optimal control  volume II. Athena Scientiﬁc 

Belmont  MA  3rd edition  2007.

[8] Jalaj Bhandari  Daniel Russo  and Raghav Singal. A ﬁnite time analysis of temporal difference
learning with linear function approximation.
In Sébastien Bubeck  Vianney Perchet  and
Philippe Rigollet  editors  Proceedings of the 31st Conference On Learning Theory  volume 75
of Proceedings of Machine Learning Research  pages 1691–1692. PMLR  06–09 Jul 2018.

[9] N. Bhat  V. F. Farias  and C. C. Moallemi. Non-parametric approximate dynamic programming

via the kernel method. In NIPS  2012.

[10] C.-S. Chow and J. N. Tsitsiklis. The complexity of dynamic programming. Journal of Complex-

ity  5(4):466–488  1989.

[11] C.-S. Chow and J. N. Tsitsiklis. An optimal one-way multigrid algorithm for discrete-time

stochastic control. IEEE Transactions on Automatic Control  36(8):898–914  1991.

[12] Gal Dalal  Balázs Szörényi  Gugan Thoppe  and Shie Mannor. Finite sample analysis for TD(0)

with linear function approximation. arXiv preprint arXiv:1704.01161  2017.

[13] S. Dasgupta and Y. Freund. Random projection trees and low dimensional manifolds. In
Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing  pages 537–546.
ACM  2008.

[14] Y. Duan  X. Chen  R. Houthooft  J. Schulman  and P. Abbeel. Benchmarking deep reinforcement
learning for continuous control. In International Conference on Machine Learning  pages 1329–
1338  2016.

[15] F. Dufour and T. Prieto-Rumeau. Approximation of Markov decision processes with general

state space. Journal of Mathematical Analysis and applications  388(2):1254–1267  2012.

[16] F. Dufour and T. Prieto-Rumeau. Finite linear programming approximations of constrained
discounted Markov decision processes. SIAM Journal on Control and Optimization  51(2):1298–
1324  2013.

[17] F. Dufour and T. Prieto-Rumeau. Approximation of average cost Markov decision processes
using empirical distributions and concentration inequalities. Stochastics: An International
Journal of Probability and Stochastic Processes  87(2):273–307  2015.

[18] E. Even-Dar and Y. Mansour. Learning rates for Q-learning. JMLR  5  December 2004.

[19] W. B. Haskell  R. Jain  and D. Kalathil. Empirical dynamic programming. Mathematics of

Operations Research  41(2)  2016.

[20] H. V. Hasselt. Double Q-learning. In NIPS. 2010.

[21] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing 
pages 604–613. ACM  1998.

[22] T. Jaakkola  M. I. Jordan  and S. P. Singh. On the convergence of stochastic iterative dynamic

programming algorithms. Neural Comput.  6(6)  1994.

[23] M. Kearns and S. Singh. Finite-sample convergence rates for Q-learning and indirect algorithms.

In NIPS  1999.

[24] S. H. Lim and G. DeJong. Towards ﬁnite-sample convergence of direct reinforcement learn-
ing. In Proceedings of the 16th European Conference on Machine Learning  pages 230–241.
Springer-Verlag  2005.

[25] Bo Liu  Ji Liu  Mohammad Ghavamzadeh  Sridhar Mahadevan  and Marek Petrik. Finite-sample
analysis of proximal gradient TD algorithms. In Proceedings of the Thirty-First Conference on
Uncertainty in Artiﬁcial Intelligence  pages 504–513. AUAI Press  2015.

10

[26] C. Mathy  N. Derbinsky  J. Bento  J. Rosenthal  and J. S. Yedidia. The boundary forest algorithm
for online supervised and unsupervised learning. In Twenty-Ninth AAAI Conference on Artiﬁcial
Intelligence  pages 2864–2870  2015.

[27] F. S. Melo  S. P. Meyn  and M. I. Ribeiro. An analysis of reinforcement learning with function
approximation. In Proceedings of the 25th international conference on Machine learning  pages
664–671. ACM  2008.

[28] Francisco S Melo and M Isabel Ribeiro. Q-learning with linear function approximation. In
International Conference on Computational Learning Theory  pages 308–322. Springer  2007.
[29] V. Mnih  K. Kavukcuoglu  D. Silver  A. A. Rusu  J. Veness  M. G. Bellemare  A. Graves 
M. Riedmiller  A. K Fidjeland  and G. Ostrovski. Human-level control through deep reinforce-
ment learning. Nature  518(7540):529–533  2015.

[30] R. Munos and C. Szepesvári. Finite-time bounds for ﬁtted value iteration. Journal of Machine

Learning Research  9(May):815–857  2008.

[31] E. A. Nadaraya. On estimating regression. Theory of Probability & Its Applications  9(1):141–

142  1964.

[32] D. Ormoneit and P. Glynn. Kernel-based reinforcement learning in average-cost problems.

IEEE Trans. Automatic Control  47(10)  2002.

[33] D. Ormoneit and ´S. Sen. Kernel-based reinforcement learning. Mach. Learning  49(2-3)  2002.
[34] Jason Pazis and Ronald Parr. PAC optimal exploration in continuous space Markov decision
processes. In Proceedings of the Twenty-Seventh AAAI Conference on Artiﬁcial Intelligence 
pages 774–781. AAAI Press  2013.

[35] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical

Statistics  pages 400–407  1951.

[36] J. Rust. Using randomization to break the curse of dimensionality. Econometrica  65(3)  1997.
[37] N. Saldi  S. Yuksel  and T. Linder. On the asymptotic optimality of ﬁnite approximations to

markov decision processes with borel spaces. Math. of Operations Research  42(4)  2017.

[38] D. Silver  A. Huang  C. J. Maddison  A. Guez  L. Sifre  G. Van Den Driessche  J. Schrittwieser 
I. Antonoglou  V. Panneershelvam  and M. Lanctot. Mastering the game of go with deep neural
networks and tree search. Nature  529(7587):484–489  2016.

[39] Charles J. Stone. Optimal global rates of convergence for nonparametric regression. The Annals

of Statistics  pages 1040–1053  1982.

[40] A. L Strehl  L. Li  E. Wiewiora  J. Langford  and M. L. Littman. PAC model-free reinforcement

learning. In ICML  2006.

[41] C. Szepesvári. The asymptotic convergence-rate of Q-learning. In NIPS  1997.
[42] C. Szepesvári and W. D. Smart. Interpolation-based Q-learning. In Proceedings of the Twenty-

First International Conference on Machine learning  page 100. ACM  2004.

[43] J. N. Tsitsiklis. Asynchronous stochastic approximation and Q-learning. Mach. Learning  16(3) 

1994.

[44] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function

approximation. IEEE Trans. Automatic Control  42(5)  1997.

[45] Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics.

Springer  2009.

[46] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data

Science. Cambridge University Press  2017.

[47] C. J. C. H. Watkins and P. Dayan. Q-learning. Mach. learning  8(3-4)  1992.
[48] G. S. Watson. Smooth regression analysis. Sankhy¯a: The Indian Journal of Statistics  Series A 

pages 359–372  1964.

11

,Chris Maddison
Daniel Tarlow
Tom Minka
Christoph Dann
Emma Brunskill
Devavrat Shah
Qiaomin Xie