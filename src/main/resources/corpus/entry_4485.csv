2016,Proximal Stochastic Methods for Nonsmooth Nonconvex Finite-Sum Optimization,We analyze stochastic algorithms for optimizing nonconvex  nonsmooth finite-sum problems  where the nonsmooth part is convex.  Surprisingly  unlike the smooth case  our knowledge of this fundamental problem is very limited. For example  it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue  we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore  using a variant of these algorithms  we obtain provably faster convergence than batch proximal gradient descent. Our results are based on the recent variance reduction techniques for convex optimization but with a novel analysis for handling nonconvex and nonsmooth functions. We also prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions  which subsumes several recent works.,Proximal Stochastic Methods for Nonsmooth

Nonconvex Finite-Sum Optimization

Sashank J. Reddi

Carnegie Mellon University
sjakkamr@cs.cmu.edu

Barnabás Póczos

Carnegie Mellon University
bapoczos@cs.cmu.edu

Suvrit Sra

Massachusetts Institute of Technology

suvrit@mit.edu

Alexander J. Smola

Carnegie Mellon University

alex@smola.org

Abstract

We analyze stochastic algorithms for optimizing nonconvex  nonsmooth ﬁnite-sum
problems  where the nonsmooth part is convex. Surprisingly  unlike the smooth
case  our knowledge of this fundamental problem is very limited. For example 
it is not known whether the proximal stochastic gradient method with constant
minibatch converges to a stationary point. To tackle this issue  we develop fast
stochastic algorithms that provably converge to a stationary point for constant
minibatches. Furthermore  using a variant of these algorithms  we obtain provably
faster convergence than batch proximal gradient descent. Our results are based
on the recent variance reduction techniques for convex optimization but with a
novel analysis for handling nonconvex and nonsmooth functions. We also prove
global linear convergence rate for an interesting subclass of nonsmooth nonconvex
functions  which subsumes several recent works.

Introduction

1
We study nonconvex  nonsmooth  ﬁnite-sum optimization problems of the form

F (x) := f (x) + h(x)  where f (x) :=

1
n

min
x2Rd

nXi=1

fi(x) 

(1)

: Rd ! R is smooth (possibly nonconvex) for all i 2{ 1  . . .   n}   [n]  while
and each fi
h : Rd ! R is nonsmooth but convex and relatively simple.
Such ﬁnite-sum optimization problems are fundamental to machine learning when performing regu-
larized empirical risk minimization. While there has been extensive research in solving nonsmooth
convex ﬁnite-sum problems (i.e.  each fi is convex for i 2 [n]) [4  16  31]  our understanding of their
nonsmooth nonconvex counterparts is surprisingly limited. We hope to amend this situation (at least
partially)  given the widespread importance of nonconvexity throughout machine learning.
A popular approach to handle nonsmoothness in convex problems is via proximal operators [14  25] 
but as we will soon see  this approach does not work so easily for the nonconvex problem (1).
Nevertheless  recall that proper closed convex function h  the proximal operator is deﬁned as

prox⌘h(x) := argmin

y2Rd ⇣h(y) + 1

2⌘ky  xk2⌘  

for ⌘> 0.

(2)

The power of proximal operators lies in how they generalize projections: e.g.  if h is the indicator
function IC(x) of a closed convex set C  then proxIC (x) ⌘ projC(x) ⌘ argminy2C ky  xk.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

xt+1 = prox⌘th✓xt 

⌘t

|It|Xi2It rfi(xt)◆  

t = 0  1  . . .  

(4)

Throughout this paper  we assume that the proximal operator of h is easy to compute. This is true
for many applications in machine learning and statistics including `1 regularization  box-constraints 
simplex constraints  among others [2  18].
Similar to other algorithms  we also assume access to a proximal oracle (PO) that takes a point
x 2 Rd and returns the output of (2). In addition to the number of PO calls  to describe our complexity
nPi fi  an IFO
results we use the incremental ﬁrst-order oracle (IFO) model.1 For a function f = 1
takes an index i 2 [n] and a point x 2 Rd  and returns the pair (fi(x) rfi(x)).
A standard (batch) method for solving (1) is the proximal-gradient method (PROXGD) [13]  ﬁrst
studied for (batch) nonconvex problems in [5]. This method performs the following iteration:

xt+1 = prox⌘h(xt  ⌘rf (xt)) 

t = 0  1  . . .  

(3)

where ⌘> 0 is a step size. The following convergence rate for PROXGD was proved recently.
Theorem (Informal). [7]: The number of IFO and PO calls made by the proximal gradient method (3)
to reach ✏ close to a stationary point is O(n/✏) and O(1/✏)  respectively.

We refer the reader to [7] for details. The key point to note here is that the IFO complexity of (3) is
O(n/✏). This is due to the fact that a full gradient rf needs to be computed at each iteration (3) 
which requires n IFO calls. When n is large  this high cost per iteration is prohibitive. A more
practical approach is offered by proximal stochastic gradient (PROXSGD)  which performs the
iteration

where It (referred to as minibatch) is a randomly chosen set (with replacement) from [n] and ⌘t is a
step size. Non-asymptotic convergence of PROXSGD was also shown recently  as noted below.
Theorem (Informal). [7]: The number of IFO and PO calls made by PROXSGD  i.e.  iteration (4)  to
reach ✏ close to a stationary point is O(1/✏2) and O(1/✏) respectively. For achieving this convergence 
we impose batch sizes |It| that increase and step sizes ⌘t that decrease with 1/✏.
Notice that the PO complexity of PROXSGD is similar to PROXGD  but its IFO complexity is
independent of n; though  this beneﬁt comes at the cost of an extra 1/✏ factor. Furthermore  the step
size must decrease with 1/✏ (or alternatively decay with the number of iterations of the algorithm).
The same two aspects are also seen for convex stochastic gradient  in both the smooth and proximal
versions. However  in the nonconvex setting there is a key third and more important aspect: the
minibatch size |It| increases with 1/✏.
To understand this aspect  consider the case where |It| is a constant (independent of both n and ✏) 
typically the choice used in practice. In this case  the above convergence result no longer holds and
it is not clear if PROXSGD even converges to a stationary point at all! To clarify  a decreasing step
size ⌘t trivially ensures convergence as t ! 1  but the limiting point is not necessarily stationary.
On the other hand  increasing |It| with 1/✏ can easily lead to |It| n for reasonably small ✏  which
effectively reduces the algorithm to (batch) PROXGD.
This dismal news does not apply to the convex setting  where PROXSGD is known to converge (in
expectation) to an optimal solution using constant minibatch sizes |It|. Furthermore  this problem
does not afﬂict smooth nonconvex problems (h ⌘ 0)  where convergence with constant minibatches
is known [6  21  22]. Thus  there is a fundamental gap in our understanding of stochastic methods
for nonsmooth nonconvex problems. Given the ubiquity of nonconvex models in machine learning 
bridging this gap is important. We do so by analyzing stochastic proximal methods with guaranteed
convergence for constant minibatches  and faster convergence with minibatches independent of 1/✏.
Main Contributions
We state our main contributions below and list the key complexity results in Table 1.
• We analyze nonconvex proximal versions of the recently proposed stochastic algorithms SVRG
and SAGA [4  8  31]  hereafter referred to as PROXSVRG and PROXSAGA  respectively. We show
convergence of these algorithms with constant minibatches. To the best of our knowledge  this is
the ﬁrst work to present non-asymptotic convergence rates for stochastic methods that apply to
nonsmooth nonconvex problems with constant (hence more realistic) minibatches.
1Introduced in [1] to study lower bounds of deterministic algorithms for convex ﬁnite-sum problems.

2

• We show that by carefully choosing the minibatch size (to be sublinearly dependent on n but still
independent of 1/✏)  we can achieve provably faster convergence than both proximal gradient and
proximal stochastic gradient. We are not aware of any earlier results on stochastic methods for the
general nonsmooth nonconvex problem that have faster convergence than proximal gradient.

• We study a nonconvex subclass of (1) based on the proximal extension of Polyak-Łojasiewicz
inequality [9]. We show linear convergence of PROXSVRG and PROXSAGA to the optimal solution
for this subclass. This includes the recent results proved in [27  32] as special cases. Ours is the
ﬁrst stochastic method with provable global linear convergence for this subclass of problems.

1.1 Related Work

The literature on ﬁnite-sum problems is vast; so we summarize only a few closely related works.
Convex instances of (1) have been long studied [3  15] and are fairly well-understood. Remarkable
recent progress for smooth convex instances of (1) is the creation of variance reduced (VR) stochastic
methods [4  8  26  28]. Nonsmooth proximal VR stochastic algorithms are studied in [4  31]
where faster convergence rates for both strongly convex and non-strongly convex cases are proved.
Asynchronous VR frameworks are developed in [20]; lower-bounds are studied in [1  10].
In contrast  nonconvex instances of (1) are much less understood. Stochastic gradient for smooth
nonconvex problems is analyzed in [6]  and only very recently  convergence results for VR stochastic
methods for smooth nonconvex problems were obtained in [21  22]. In [11]  the authors consider a VR
nonconvex setting different from ours  namely  where the loss is (essentially strongly) convex but hard
thresholding is used. We build upon [21  22]  and focus on handling nonsmooth convex regularizers
(h 6⌘ 0 in (1)).2 Incremental proximal gradient methods for this class were also considered in [30]
but only asymptotic convergence was shown. The ﬁrst analysis of a projection version of nonconvex
SVRG is due to [29]  who considers the special problem of PCA. Perhaps  the closest to our work
is [7]  where convergence of minibatch nonconvex PROXSGD method is studied. However  typical
to the stochastic gradient method  the convergence is slow; moreover  no convergence for constant
minibatches is provided.

2 Preliminaries
We assume that the function h(x) in (1) is lower semi-continuous (lsc) and convex. Furthermore  we
also assume that its domain dom(h) = {x 2 Rd|h(x) < +1} is closed. We say f is L-smooth if
there is a constant L such that

krf (x)  rf (y)k  Lkx  yk 

8 x  y 2 Rd.

Throughout  we assume that the functions fi in (1) are L-smooth  so that krfi(x)  rfi(y)k 
Lkx  yk for all i 2 [n]. Such an assumption is typical in the analysis of ﬁrst-order methods.
One crucial aspect of the analysis for nonsmooth nonconvex problems is the convergence criterion.
For convex problems  typically the optimality gap F (x)  F (x⇤) is used as a criterion.
It is
unreasonable to use such a criterion for general nonconvex problems due to their intractability. For
smooth nonconvex problems (i.e.  h ⌘ 0)  it is typical to measure stationarity  e.g.  using krFk. This
cannot be used for nonsmooth problems  but a ﬁtting alternative is the gradient mapping3 [17]:

G⌘(x) := 1

⌘ [x  prox⌘h(x  ⌘rf (x))].

(5)
When h ⌘ 0 this mapping reduces to G⌘(x) = rf (x) = rF (x)  the gradient of function F at x.
We analyze our algorithms using the gradient mapping (5) as described more precisely below.
Deﬁnition 1. A point x output by stochastic iterative algorithm for solving (1) is called an ✏-accurate
solution  if E[kG⌘(x)k2]  ✏ for some ⌘> 0.
Our goal is to obtain efﬁcient algorithms for achieving an ✏-accurate solution  where efﬁciency is
measured using IFO and PO complexity as functions of 1/✏ and n.

2More recently  the authors have also developed VR Frank-Wolfe methods for handling constrained problems

that do not admit easy projection operators [24].

3This mapping has also been used in the analysis of nonconvex proximal methods in [6  7  30].

3

Algorithm

IFO

PO

IFO (PL)

PROXSGD
PROXGD

O1/✏2

O (n/✏)

O (1/✏)
O (1/✏)

O1/✏2

O (n log(1/✏))

PO (PL)

O (1/✏)

O ( log(1/✏))

PROXSVRG O(n + (n2/3/✏)) O(1/✏) O((n + n2/3) log(1/✏)) O( log(1/✏))

PROXSAGA O(n + (n2/3/✏)) O(1/✏) O((n + n2/3) log(1/✏)) O( log(1/✏))

Constant
minibatch?

?

p
p

Table 1: Table comparing the best IFO and PO complexity of different algorithms discussed in the paper.
The complexity is measured in terms of the number of oracle calls required to achieve an ✏-accurate solution.
The IFO (PL) and PO (PL) represents the IFO and PO complexity of PL functions (see Section 4 for a formal
deﬁnition). The results marked in red are the contributions of this paper. In the table  “constant minibatch”
indicates whether stochastic algorithm converges using a constant minibatch size. To the best of our knowledge 
it is not known if PROXSGD converges on using constant minibatches for nonconvex nonsmooth optimization.
Also  we are not aware of any speciﬁc convergence results for PROXSGD in the context of PL functions.

3 Algorithms

We focus on two algorithms: (a) proximal SVRG (PROXSVRG) and (b) proximal SAGA (PROXSAGA).

3.1 Nonconvex Proximal SVRG
We ﬁrst consider a variant of PROXSVRG [31]; pseudocode of this variant is stated in Algorithm 1.
When F is strongly convex  SVRG attains linear convergence rate as opposed to sublinear convergence
of SGD [8]. Note that  while SVRG is typically stated with b = 1  we use its minibatch variant with
batch size b. The speciﬁc reasons for using such a variant will become clear during the analysis.
While some other algorithms have been proposed for reducing the variance in the stochastic gradients 
SVRG is particularly attractive because of its low memory requirement; it requires just O(d) extra
memory in comparison to SGD for storing the average gradient (gs in Algorithm 1)  while algorithms
like SAG and SAGA incur O(nd) storage cost. In addition to its strong theoretical results  SVRG is
known to outperform SGD empirically while being more robust to selection of step size. For convex
problems  PROXSVRG is known to inherit these advantages of SVRG [31].
We now present our analysis of nonconvex PROXSVRG  starting with a result for batch size b = 1.
Theorem 1. Let b = 1 in Algorithm 1. Let ⌘ = 1/(3Ln)  m = n and T be a multiple of m. Then the
output xa of Algorithm 1 satisﬁes the following bound:

E[kG⌘(xa)k2] 

where x⇤ is an optimal solution of (1).

18Ln2

3n  2✓ F (x0)  F (x⇤)

◆  

T

Theorem 1 shows that PROXSVRG converges for constant minibatches of size b = 1. This result
is in strong contrast to PROXSGD whose convergence with constant minibatches is still unknown.
However  the result delivered by Theorem 1 is not stronger than that of PROXGD. The following
corollary to Theorem 1 highlights this point.
Corollary 1. To obtain an ✏-accurate solution  with b = 1 and parameters from Theorem 1  the IFO
and PO complexities of Algorithm 1 are O(n/✏) and O(n/✏)  respectively.

Corollary 1 follows upon noting that each inner iteration (Step 7) of Algorithm 1 has an effective
IFO complexity of O(1) since m = n. This IFO complexity includes the IFO calls for calculating
the average gradient at the end of each epoch. Furthermore  each inner iteration also invokes the
proximal oracle  whereby the PO complexity is also O(n/✏). While the IFO complexity of constant
minibatch PROXSVRG is same as PROXGD  we see that its PO complexity is much worse. This
is due to the fact that n IFO calls correspond to one PO call in PROXGD  while one IFO call in
PROXSVRG corresponds to one PO call. Consequently  we do not gain any theoretical advantage by
using constant minibatch PROXSVRG over PROXGD.

4

m = x0 2 Rd  epoch length m  step sizes ⌘> 0  S = dT /me  minibatch size b
xs+1
0 = xs
nPn
m
gs+1 = 1
i=1 rfi(˜xs)
for t = 0 to m  1 do
bPit2It

Algorithm 1: Nonconvex PROXSVRGx0  T  m  b ⌘
1: Input: ˜x0 = x0
2: for s = 0 to S  1 do
3:
4:
5:
6:
7:
8:
9:
10:
11: end for
12: Output: Iterate xa chosen uniformly at random from {{xs+1

(rfit (xs+1
t  ⌘v s+1

t=0 }S1
}m1
s=0 .

m

Uniformly randomly pick It ⇢{ 1  . . .   n} (with replacement) such that |It| = b
vs+1
t = 1
xs+1
t+1 = prox⌘h(xs+1
end for
˜xs+1 = xs+1

)  rfit (˜xs)) + gs+1
)

t

t

t

The key question is therefore: can we modify the algorithm to obtain better theoretical guarantees?
To answer this question  we prove the following main convergence result. For ease of theoretical
exposition  we assume n2/3 to be an integer. This is only for convenience in stating our theoretical
results and all the results in the paper hold for the general case.
Theorem 2. Suppose b = n2/3 in Algorithm 1. Let ⌘ = 1/(3L)  m = bn1/3c and T be a multiple of
m. Then for the output xa of Algorithm 1  we have:

E[kG⌘(xa)k2] 

18L(F (x0)  F (x⇤))

T

 

where x⇤ is an optimal solution to (1).

Rewriting Theorem 2 in terms of the IFO and PO complexity  we obtain the following corollary.
Corollary 2. Let b = n2/3 and set parameters as in Theorem 2. Then  to obtain an ✏-accurate
solution the IFO and PO complexities of Algorithm 1 are O(n + n2/3/✏) and O(1/✏)  respectively.

The above corollary is due to the following observations. From Theorem 2  it can be seen that the
total number of inner iterations (across all epochs) of Algorithm 1 to obtain an ✏-accurate solution
is O(1/✏). Since each inner iteration of Algorithm 2 involves a call to the PO  we obtain a PO
complexity of O(1/✏). Further  since b = n2/3 IFO calls are made at each inner iteration  we obtain
a net IFO complexity of O(n2/3/✏). Adding the IFO calls for the calculation of the average gradient
(and noting that T is a multiple of m)  as well as noting that S  1  we obtain a total cost of
O(n + n2/3/✏). A noteworthy aspect of Corollary 2 is that its PO complexity matches PROXGD  but
its IFO complexity is signiﬁcantly decreased to O(n + n2/3/✏) as opposed to O(n/✏) in PROXGD.

3.2 Nonconvex Proximal SAGA

In the previous section  we investigated PROXSVRG for solving (1). Note that PROXSVRG is not a
fully “incremental" algorithm since it requires calculation of the full gradient once per epoch. An
alternative to PROXSVRG is the algorithm proposed in [4] (popularly referred to as SAGA). We build
upon the work of [4] to develop PROXSAGA  a nonconvex proximal variant of SAGA.
The pseudocode for PROXSAGA is presented in Algorithm 2. The key difference between Algorithm 1
and 2 is that PROXSAGA  unlike PROXSVRG  avoids computation of the full gradient. Instead  it
maintains an average gradient vector gt  which changes at each iteration (refer to [20]). However 
such a strategy entails additional storage costs. In particular  for implementing Algorithm 2  we must
i=1  which in general can cost O(nd) in storage. Nevertheless  in some
store the gradients {rfi(↵t
scenarios common to machine learning (see [4])  one can reduce the storage requirements to O(n).
Whenever such an implementation of PROXSAGA is possible  it can perform similar to or even better
than PROXSVRG [4]; hence  in addition to theoretical interest  it is of signiﬁcant practical value.
We remark that PROXSAGA in Algorithm 2 differs slightly from [4]. In particular  it uses minibatches
where two sets It  Jt are sampled at each iteration as opposed to one in [4]. This is mainly for the
ease of theoretical analysis.

i)}n

5

i = x0 for i 2 [n]  step size ⌘> 0  minibatch size b

Algorithm 2: Nonconvex PROXSAGAx0  T  b ⌘
1: Input: x0 2 Rd  ↵0
nPn
2: g0 = 1
i=1 rfi(↵0
i )
3: for t = 0 to T  1 do
Uniformly randomly pick sets It  Jt from [n] (with replacement) such that |It| = |Jt| = b
4:
bPit2It
5:
(rfit (xt)  rfit (↵t
vt = 1
6:
xt+1 = prox⌘h(xt  ⌘v t)
j = xt for j 2 Jt and ↵t+1
7:
↵t+1
j = ↵t
nPjt2Jt
8:
(rfjt (↵t
gt+1 = gt  1
9: end for
10: Output: Iterate xa chosen uniformly random from {xt}T1
t=0 .

j for j /2 Jt
jt )  rfjt (↵t+1
jt ))

it )) + gt

We prove that as in the convex case  nonconvex PROXSVRG and PROXSAGA share similar theoretical
guarantees. In particular  our ﬁrst result for PROXSAGA is a counterpart to Theorem 1 for PROXSVRG.
Theorem 3. Suppose b = 1 in Algorithm 2. Let ⌘ = 1/(5Ln). Then for the output xa of Algorithm 2
after T iterations  the following stationarity bound holds:

E[kG⌘(xa)k2] 

where x⇤ is an optimal solution of (1).

50Ln2
5n  2

F (x0)  F (x⇤)

T

 

Theorem 3 immediately leads to the following corollary.
Corollary 3. The IFO and PO complexity of Algorithm 3 for b = 1 and parameters speciﬁed in
Theorem 3 to obtain an ✏-accurate solution are O(n/✏) and O(n/✏) respectively.

Similar to Theorem 2 for PROXSVRG  we obtain the following main result for PROXSAGA.
Theorem 4. Suppose b = n2/3 in Algorithm 2. Let ⌘ = 1/(5L). Then for the output xa of
Algorithm 2 after T iterations  the following holds:

E[kG⌘(xa)k2] 
where x⇤ is an optimal solution of Problem (1).

50L(F (x0)  F (x⇤))

 

3T

Rewriting this result in terms of IFO and PO access  we obtain the following important corollary.
Corollary 4. Let b = n2/3 and set parameters as in Theorem 4. Then  to obtain an ✏-accurate
solution the IFO and PO complexities of Algorithm 2 are O(n + n2/3/✏) and O(1/✏)  respectively.

The above result is due to Theorem 4 and because each iteration of PROXSAGA requires O(n2/3)
IFO calls. The number of PO calls is only O(1/✏)  since make one PO call for every n2/3 IFO calls.
Discussion: It is important to note the role of minibatches in Corollaries 2 and 4. Minibatches are
typically used for reducing variance and promoting parallelism in stochastic methods. But unlike
previous works  we use minibatches as a theoretical tool to improve convergence rates of both
nonconvex PROXSVRG and PROXSAGA. In particular  by carefully selecting the minibatch size  we
can improve the IFO complexity of the algorithms described in the paper from O(n/✏) (similar to
PROXGD) to O(n2/3/✏) (matching the smooth nonconvex case). Furthermore  the PO complexity is
also improved in a similar manner by using the minibatch size mentioned in Theorems 2 and 4. 4

4 Extensions

We discuss some extensions of our approach in this section. Our ﬁrst extension is to provide
convergence analysis for a subclass of nonconvex functions that satisfy a speciﬁc growth condition
popularly known as the Polyak-Łojasiewicz (PL) inequality. In the context of gradient descent 

4We refer the readers to the full version [23] for a more general convergence analysis of the algorithms.

6

PL-SVRG:(x0  K  T  m  ⌘)
for k = 1 to K do

PL-SAGA:(x0  K  T  m  ⌘)
for k = 1 to K do

xk = ProxSVRG(xk1  T  m  b  ⌘) ;

xk = ProxSAGA(xk1  T  b ⌘ ) ;

end
Output: xK

end
Output: xK

Figure 1: PROXSVRG and PROXSAGA variants for PL functions.

this inequality was proposed by Polyak in 1963 [19]  who showed global linear convergence of
gradient descent for functions that satisfy the PL inequality. Recently  in [9] the PL inequality was
generalized to nonsmooth functions and used for proving linear convergence of proximal gradient.
The generalization presented in [9] considers functions F (x) = f (x)+h(x) that satisfy the following:

µ(F (x)  F (x⇤)) 

1
2

Dh(x  µ)  where µ > 0

(6)

and Dh(x  µ) := 2µ miny⇥hrf (x)  y  xi +

µ

2ky  xk2 + h(y)  h(x)⇤.

An F that satisﬁes (6) is called a µ-PL function.
When h ⌘ 0  condition (6) reduces to the usual PL inequality. The class of µ-PL functions includes
several other classes as special cases. It subsumes strongly convex functions  covers fi(x) = g(a>i x)
with only g being strongly convex  and includes functions that satisfy a optimal strong convexity
property [12]. Note that the µ-PL functions also subsume the recently studied special case where fi’s
are nonconvex but their sum f is strongly convex. Hence  it encapsulates the problems of [27  32].
The algorithms in Figure 1 provide variants of PROXSVRG and PROXSAGA adapted to optimize
µ-PL functions. We show the following global linear convergence result of PL-SVRG and PL-SAGA
in Figure 1 for PL functions. For simplicity  we assume  = (L/µ) > n1/3. When f is strongly
convex   is referred to as the condition number  in which case > n 1/3 corresponds to the high
condition number regime.
Theorem 5. Suppose F is a µ-PL function. Let b = n2/3  ⌘ = 1/5L  m = bn1/3c and T = d30e.
Then for the output xK of PL-SVRG and PL-SAGA (in Figure 1)  the following holds:

E[F (xK)  F (x⇤)] 

[F (x0)  F (x⇤)]

 

2K

where x⇤ is an optimal solution of (1).
The following corollary on IFO and PO complexity of PL-SVRG and PL-SAGA is immediate.
Corollary 5. When F is a µ-PL function  then the IFO and PO complexities of PL-SVRG and
PL-SAGA with the parameters speciﬁed in Theorem 5 to obtain an ✏-accurate solution are O((n +
n2/3) log(1/✏)) and O( log(1/✏))  respectively.
Note that proximal gradient also has global linear convergence for PL functions  as recently shown
in [9]. However  its IFO complexity is O(n log(1/✏))  which is much worse than that of PL-SVRG
and PL-SAGA (Corollary 5).
Other extensions: While we state our results for speciﬁc minibatch sizes  a more general convergence
analysis is provided for any minibatch size b  n2/3 (Theorems 6 and 7 in the Appendix). Moreover 
our results can be easily generalized to the case where non-uniform sampling is used in Algorithm 1
and Algorithm 2. This is useful when the functions fi have different Lipschitz constants.

5 Experiments
We present our empirical results in this section. For our experiments  we study the problem of
non-negative principal component analysis (NN-PCA). More speciﬁcally  for a given set of samples
{zi}n

i=1  we solve the following optimization problem:

(7)

min

kxk1  x0

1
2

x> nXi=1

ziz>i ! x.

7

10-5

10-10

)
^x
(
f
!

)
x
(
f

10-15

0

SGD
SAGA
SVRG

10-5

10-10

)
^x
(
f
!

)
x
(
f

SGD
SAGA
SVRG

10-5

10-10

)
^x
(
f
!

)
x
(
f

SGD
SAGA
SVRG

10-5

10-10

)
^x
(
f
!

)
x
(
f

SGD
SAGA
SVRG

5

10

# grad/n

15

10-15

0

5

10

# grad/n

15

10-15

0

5

10

# grad/n

15

10-15

0

5

10

# grad/n

15

Figure 2: Non-negative principal component analysis. Performance of PROXSGD  PROXSVRG and
PROXSAGA on ’rcv1’ (left)  ’a9a’(left-center)  ’mnist’ (right-center) and ’aloi’ (right) datasets. Here 
the y-axis is the function suboptimality i.e.  f (x) f (ˆx) where ˆx represents the best solution obtained
by running gradient descent for long time and with multiple restarts.

t

The problem of NN-PCA is  in general  NP-hard. This variant of the standard PCA problem can
be written in the form (1) with fi(x) = (x>zi)2 for all i 2 [n] and h(x) = IC(x) where C is the
convex set {x 2 Rd|kxk  1  x  0}. In our experiments  we compare PROXSGD with nonconvex
PROXSVRG and PROXSAGA. The choice of step size is important to PROXSGD. The step size of
PROXSGD is set using the popular t-inverse step size choice of ⌘t = ⌘0(1 + ⌘0bt/nc)1 where
⌘0 ⌘ 0 > 0. For PROXSVRG and PROXSAGA  motivated by the theoretical analysis  we use a ﬁxed
step size. The parameters of the step size in each of these methods are chosen so that the method gives
the best performance on the objective value. In our experiments  we include the value ⌘0 = 0  which
corresponds to PROXSGD with ﬁxed step size. For PROXSVRG  we use the epoch length m = n.
We use standard machine learning datasets in LIBSVM for all our experiments 5. The samples from
each of these datasets are normalized i.e. kzik = 1 for all i 2 [n]. Each of these methods is initialized
by running PROXSGD for n iterations. Such an initialization serves two purposes: (a) it provides a
reasonably good initial point  typically beneﬁcial for variance reduction techniques [4  26]. (b) it
provides a heuristic for calculating the initial average gradient g0 [26]. In our experiments  we use
b = 1 in order to demonstrate the performance of the algorithms with constant minibatches.
We report the objective function value for the datasets. In particular  we report the suboptimality in
objective function i.e.  f (xs+1
)  f (ˆx) (for PROXSVRG) and f (xt)  f (ˆx) (for PROXSAGA). Here
ˆx refers to the solution obtained by running proximal gradient descent for a large number of iterations
and multiple random initializations. For all the algorithms  we compare the aforementioned criteria
against for the number of effective passes through the dataset i.e.  IFO complexity divided by n. For
PROXSVRG  this includes the cost of calculating the full gradient at the end of each epoch.
Figure 2 shows the performance of the algorithms on NN-PCA problem (see Section D of the Ap-
pendix for more experiments). It can be seen that the objective value for PROXSVRG and PROXSAGA
is much lower compared to PROXSGD  suggesting faster convergence for these algorithms. We
observed a signiﬁcant gain consistently across all the datasets. Moreover  the selection of step size
was much simpler for PROXSVRG and PROXSAGA than that for PROXSGD. We did not observe any
signiﬁcant difference in the performance of PROXSVRG and PROXSAGA for this particular task.
6 Final Discussion
In this paper  we presented fast stochastic methods for nonsmooth nonconvex optimization. In
particular  by employing variance reduction techniques  we show that one can design methods that
can provably perform better than PROXSGD and proximal gradient descent. Furthermore  in contrast
to PROXSGD  the resulting approaches have provable convergence to a stationary point with constant
minibatches; thus  bridging a fundamental gap in our knowledge of nonsmooth nonconvex problems.
We proved that with a careful selection of minibatch size  it is possible to theoretically show superior
performance to proximal gradient descent. Our empirical results provide evidence for a similar
conclusion even with constant minibatches. Thus  we conclude with an important open problem of
developing stochastic methods with provably better performance than proximal gradient descent with
constant minibatch size.
Acknowledgment: SS acknowledges support of NSF grant: IIS-1409802.

5The

can

be
libsvmtools/datasets.

datasets

downloaded

from https://www.csie.ntu.edu.tw/~cjlin/

8

References
[1] A. Agarwal and L. Bottou. A lower bound for the optimization of ﬁnite sums. arXiv:1410.0723  2014.
[2] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Convex optimization with sparsity-inducing norms. In

S. Sra  S. Nowozin  and S. J. Wright  editors  Optimization for Machine Learning. MIT Press  2011.

[3] Léon Bottou. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nımes  91(8)  1991.
[4] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with

support for non-strongly convex composite objectives. In NIPS 27  pages 1646–1654. 2014.

[5] Masao Fukushima and Hisashi Mine. A generalized proximal point algorithm for certain non-convex

minimization problems. International Journal of Systems Science  12(8):989–1000  1981.

[6] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic

programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[7] Saeed Ghadimi  Guanghui Lan  and Hongchao Zhang. Mini-batch stochastic approximation methods for

nonconvex stochastic composite optimization. Mathematical Programming  155(1-2):267–305  2014.

[8] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

In NIPS 26  pages 315–323. 2013.

[9] Hamed Karimi  Julie Nutini  and Mark W. Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. In Machine Learning and Knowledge Discovery in
Databases - European Conference  ECML PKDD 2016  pages 795–811  2016.

[10] G. Lan and Y. Zhou. An optimal randomized incremental gradient method. arXiv:1507.02000  2015.
[11] Xingguo Li  Tuo Zhao  Raman Arora  Han Liu  and Jarvis Haupt. Stochastic variance reduced optimization

for nonconvex sparse learning. In ICML  2016. arXiv:1605.02711.

[12] Ji Liu and Stephen J. Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence

properties. SIAM Journal on Optimization  25(1):351–376  January 2015.

[13] Hisashi Mine and Masao Fukushima. A minimization method for the sum of a convex function and a
continuously differentiable function. Journal of Optimization Theory and Applications  33(1):9–23  1981.
[14] J. J. Moreau. Fonctions convexes duales et points proximaux dans un espace hilbertien. C. R. Acad. Sci.

Paris Sér. A Math.  255:2897–2899  1962.

[15] Arkadi Nemirovski and D Yudin. Problem Complexity and Method Efﬁciency in Optimization. John Wiley

[16] Yu Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM

Journal on Optimization  22(2):341–362  2012.

[17] Yurii Nesterov. Introductory Lectures On Convex Optimization: A Basic Course. Springer  2003.
[18] N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Optimization  1(3):127–239 

and Sons  1983.

2014.

[19] B.T. Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics

and Mathematical Physics  3(4):864–878  January 1963.

[20] Sashank Reddi  Ahmed Hefny  Suvrit Sra  Barnabas Poczos  and Alex J Smola. On variance reduction in

stochastic gradient descent and its asynchronous variants. In NIPS 28  pages 2629–2637  2015.

[21] Sashank J. Reddi  Ahmed Hefny  Suvrit Sra  Barnabás Póczos  and Alexander J. Smola. Stochastic variance
reduction for nonconvex optimization. In Proceedings of the 33nd International Conference on Machine
Learning  ICML 2016  New York City  NY  USA  June 19-24  2016  pages 314–323  2016.

[22] Sashank J. Reddi  Suvrit Sra  Barnabás Póczos  and Alexander J. Smola. Fast incremental method for

nonconvex optimization. CoRR  abs/1603.06159  2016.

[23] Sashank J. Reddi  Suvrit Sra  Barnabás Póczos  and Alexander J. Smola. Fast stochastic methods for

nonsmooth nonconvex optimization. CoRR  abs/1605.06900  2016.

[24] Sashank J. Reddi  Suvrit Sra  Barnabás Póczos  and Alexander J. Smola. Stochastic frank-wolfe methods for
nonconvex optimization. In 54th Annual Allerton Conference on Communication  Control  and Computing 
Allerton 2016  2016.

[25] R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on control and

[26] Mark W. Schmidt  Nicolas Le Roux  and Francis R. Bach. Minimizing Finite Sums with the Stochastic

optimization  14(5):877–898  1976.

Average Gradient. arXiv:1309.2388  2013.

[27] Shai Shalev-Shwartz. SDCA without duality. CoRR  abs/1502.06177  2015.
[28] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss. The

Journal of Machine Learning Research  14(1):567–599  2013.

[29] Ohad Shamir.

A stochastic PCA and SVD algorithm with an exponential convergence rate.

arXiv:1409.2848  2014.

[30] Suvrit Sra. Scalable nonconvex inexact proximal splitting. In NIPS  pages 530–538  2012.
[31] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction.

SIAM Journal on Optimization  24(4):2057–2075  2014.

[32] Zeyuan Allen Zhu and Yang Yuan. Improved svrg for non-strongly-convex or sum-of-non-convex objectives.

CoRR  abs/1506.01972  2015.

9

,Navid Zolghadr
Gabor Bartok
Russell Greiner
András György
Csaba Szepesvari
Nisheeth Srivastava
Ed Vul
Paul Schrater
Sashank J. Reddi
Suvrit Sra
Barnabas Poczos
Alexander Smola
Zhengyang Shen
Francois-Xavier Vialard
Marc Niethammer