2016,Improving PAC Exploration Using the Median Of Means,We present the first application of the median of means in a PAC exploration algorithm for MDPs. Using the median of means allows us to significantly reduce the dependence of our bounds on the range of values that the value function can take  while introducing a dependence on the (potentially much smaller) variance of the Bellman operator. Additionally  our algorithm is the first algorithm with PAC bounds that can be applied to MDPs with unbounded rewards.,Improving PAC Exploration
Using the Median of Means

Laboratory for Information and Decision Systems

Department of Computer Science

Ronald Parr

Duke University

Durham  NC 27708
parr@cs.duke.edu

Jason Pazis

Massachusetts Institute of Technology

Cambridge  MA 02139  USA

jpazis@mit.edu

Jonathan P. How

Aerospace Controls Laboratory

Department of Aeronautics and Astronautics

Massachusetts Institute of Technology

Cambridge  MA 02139  USA

jhow@mit.edu

Abstract

We present the ﬁrst application of the median of means in a PAC exploration
algorithm for MDPs. Using the median of means allows us to signiﬁcantly reduce
the dependence of our bounds on the range of values that the value function can
take  while introducing a dependence on the (potentially much smaller) variance of
the Bellman operator. Additionally  our algorithm is the ﬁrst algorithm with PAC
bounds that can be applied to MDPs with unbounded rewards.

1

Introduction

As the reinforcement learning community has shifted its focus from heuristic methods to methods
that have performance guarantees  PAC exploration algorithms have received signiﬁcant attention.
Thus far  even the best published PAC exploration bounds are too pessimistic to be useful in prac-
tical applications. Even worse  lower bound results [14  7] indicate that there is little room for
improvement.
While these lower bounds prove that there exist pathological examples for which PAC exploration
can be prohibitively expensive  they leave the door open for the existence of “well-behaved” classes
of problems in which exploration can be performed at a signiﬁcantly lower cost. The challenge of
course is to identify classes of problems that are general enough to include problems of real-world
interest  while at the same time restricted enough to have a meaningfully lower cost of exploration
than pathological instances.
The approach presented in this paper exploits the fact that while the square of the maximum value
that the value function can take (Q2
max) is typically quite large  the variance of the Bellman operator
is rather small in many domains of practical interest. For example  this is true in many control tasks:
It is not very often that an action takes the system to the best possible state with 50% probability and
to the worst possible state with 50% probability.
Most PAC exploration algorithms take an average over samples. By contrast  the algorithm presented
in this paper splits samples into sets  takes the average over each set  and returns the median of the
averages. This seemingly simple trick (known as the median trick [1])  allows us to derive sample
complexity bounds that depend on the variance of the Bellman operator rather than Q2
max. Addi-

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

max
4

tionally  our algorithm (Median-PAC) is the ﬁrst reinforcement learning algorithm with theoretical
guarantees that allows for unbounded rewards.1
Not only does Median-PAC offer signiﬁcant sample complexity savings in the case when the variance
of the Bellman operator is low  but even in the worst case (the variance of the Bellman operator is
bounded above by Q2
) our bounds match the best  published PAC bounds. Note that Median-PAC
does not require the variance of the Bellman operator to be known in advance. Our bounds show that
there is an inverse relationship between the (possibly unknown) variance of the Bellman operator
and Median-PAC’s performance. This is to the best of our knowledge not only the ﬁrst application
of the median of means in PAC exploration  but also the ﬁrst application of the median of means in
reinforcement learning in general.
Contrary to recent work which has exploited variance in Markov decision processes to improve PAC
bounds [7  3]  Median-PAC makes no assumptions about the number of possible next-states from
every state-action (it does not even require the number of possible next states to be ﬁnite)  and as a
result it is easily extensible to the continuous state  concurrent MDP  and delayed update settings [12].

2 Background  notation  and deﬁnitions
In the following  important symbols and terms will appear in bold when ﬁrst introduced. Let X be
the domain of x. Throughout this paper  888x will serve as a shorthand for 8x 2X . In the following
s  ¯s  ˜s  s0 are used to denote various states  and a  ¯a  ˜a  a0
a  ¯a  ˜a  a0 are used to denote actions.
a  ¯a  ˜a  a0
s  ¯s  ˜s  s0
s  ¯s  ˜s  s0
A Markov Decision Process (MDP) [13] is a 5-tuple (S A  P  R  )  where SSS is the state space
of the process  AAA is the action space2  PPP is a Markovian transition modelp(s0|s  a) denotes the
probability of a transition to state s0 when taking action a in state s  RRR is a reward function
R(s  a  s0) is the reward for taking action a in state s and transitioning to state s0  and  2 [0  1)
is a discount factor for future rewards. A deterministic policy ⇡⇡⇡ is a mapping ⇡ : S 7! A from
states to actions; ⇡(s) denotes the action choice in state s. The value V ⇡(s)
V ⇡(s) of state s under
V ⇡(s)
policy ⇡ is deﬁned as the expected  accumulated  discounted reward when the process begins
in state s and all decisions are made according to policy ⇡. There exists an optimal policy ⇡⇤⇡⇤⇡⇤
for choosing actions which yields the optimal value function V ⇤(s)  deﬁned recursively via the
Bellman optimality equation V ⇤(s)
V ⇤(s)
the value Q⇡(s  a)
Q⇡(s  a) of a state-action (s  a) under policy ⇡ is deﬁned as the expected  accumulated 
Q⇡(s  a)
discounted reward when the process begins in state s by taking action a and all decisions thereafter
are made according to policy ⇡. The Bellman optimality equation for Q becomes Q⇤(s  a)
Q⇤(s  a)
Q⇤(s  a) =

V ⇤(s) = maxa {Ps0 p(s0|s  a) (R(s  a  s0) + V ⇤(s0))}. Similarly 
Ps0 p(s0|s  a) (R(s  a  s0) +  maxa0{Q⇤(s0  a0)}). For a ﬁxed policy ⇡ the Bellman operator for Q
B⇡Q(s  a) =Ps0 p(s0|s  a)⇣R(s  a  s0) + Q(s0 ⇡ (s0))⌘. In reinforcement learning

is deﬁned as B⇡Q(s  a)
B⇡Q(s  a)
(RL) [15]  a learner interacts with a stochastic process modeled as an MDP and typically observes the
state and immediate reward at every step; however  the transition model P and reward function R are
not known. The goal is to learn a near optimal policy using experience collected through interaction
with the process. At each step of interaction  the learner observes the current state s  chooses an
action a  and observes the reward received r  and resulting next state s0  essentially sampling the
transition model and reward function of the process. Thus experience comes in the form of (s  a  r  s0)
samples.
We assume that all value functions Q live in a complete metric space.
Deﬁnition 2.1. QmaxQmaxQmax denotes an upper bound on the expected  accumulated  discounted reward
from any state-action under any policy.

We require that Qmin  the minimum expected  accumulated  discounted reward from any state-action
under any policy is bounded  and in order to simplify notation we also assume without loss of

1Even though domains with truly unbounded rewards are not common  many domains exist for which
infrequent events with extremely high (winning the lottery) or extremely low (nuclear power-plant meltdown)
rewards exist. Algorithms whose sample complexity scales with the highest magnitude event are not well suited
to such domains.

2For simplicity of exposition we assume that the same set of actions is available at every state. Our results

readily extend to the case where the action set can differ from state to state.

2

generality that it is bounded below by 0. If Qmin < 0  this assumption is easy to satisfy in all MDPs
for which Qmin is bounded by simply shifting the reward space by (  1)Qmin.
There have been many deﬁnitions of sample complexity in RL. In this paper we will be using the
following [12]:
Deﬁnition 2.2. Let (s1  s2  s3  . . . ) be the random path generated on some execution of ⇡  where ⇡
is an arbitrarily complex  possibly non-stationary  possibly history dependent policy (such as the
policy followed by an exploration algorithm). Let ✏ be a positive constant  T the (possibly inﬁnite)
set of time steps for which V ⇡(st) < V ⇤(st)  ✏  and deﬁne3

✏e(t) =V ⇤(st)  V ⇡(st)  ✏  8 t 2 T.
✏e(t) =0  8 t /2 T.

The Total Cost of Exploration (TCE) is deﬁned as the undiscounted inﬁnite sumP1t=0 ✏e(t).

“Number of suboptimal steps” bounds follow as a simple corollary of TCE bounds.
We will be using the following deﬁnition of efﬁcient PAC exploration [14]:
Deﬁnition 2.3. An algorithm is said to be efﬁcient PAC-MDP (Probably Approximately Correct in
Markov Decision Processes) if  for any ✏> 0 and 0 << 1  its sample complexity  its per-timestep
computational complexity  and its space complexity  are less than some polynomial in the relevant
quantities (S  A  1

1

1 )  with probability at least 1  .

✏   1
  

3 The median of means

n

2+✏2 and P (X  µ  ✏)  2

n . From Cantelli’s inequality we have that P (X0  µ  ✏)  2
2+n✏2 . Solving for n we have that we need at most n = (1)2

Before we present Median-PAC we will demonstrate the usefulness of the median of means with a
simple example. Suppose we are given n independent samples from a random variable X and we
want to estimate its mean. The types of guarantees that we can provide about how close that estimate
will be to the expectation  will depend on what knowledge we have about the variable  and on the
method we use to compute the estimate. The main question of interest in our work is how many
samples are needed until our estimate is ✏-close to the expectation with probability at least 1  .
Let the expectation of X be E[X] = µ and its variance var[X] = 2. Cantelli’s inequality tells
us that: P (X  µ  ✏)  2
2+✏2 . Let Xi be a random variable
describing the value of the i-th sample  and deﬁne X0 = X1+X2+···+Xn
. We have that E[X0] =
µ and var[X0] = 2
2+n✏2 and
✏2 = O⇣ 2
✏2⌘
P (X0  µ  ✏)  2
samples until our estimate is ✏-close to the expectation with probability at least 1  . In RL  it is
common to apply a union bound over the entire state-action space in order to prove uniformly good
approximation. This means that  has to be small enough that even when multiplied with the number
of state-actions  it yields an acceptably low probability of failure. The most signiﬁcant drawback of
the bound above is that it grows very quickly as  becomes smaller. Without further assumptions one
can show that the bound above is tight for the average estimator.
If we know that X can only take values in a bounded range a  X  b  Hoeffding’s inequality
tells us that P (X0  µ  ✏)  e 2n✏2
(ba)2 . Solving for n we have
that n = (ba)2 ln 1
samples sufﬁce to guarantee that our estimate is ✏-close to the expectation with
probability at least 1  . Hoeffding’s inequality yields a much better bound with respect to   but
introduces a quadratic dependence on the range of values that the variable can take. For long planning
horizons (discount factor close to 1) and/or large reward magnitudes  the range of possible Q-values
can be very large  much larger than the variance of individual state-actions.
We can get the best of both worlds by using a more sophisticated estimator. Instead of taking the
average over n samples  we will split them into km = n✏2
✏2 samples each 4 compute the
3Note that V ⇡(st) denotes the expected  discounted  accumulated reward of the arbitrarily complex policy ⇡

(ba)2 and P (X0  µ  ✏)  e 2n✏2

42 sets of 42

2✏2



from state st at time t  rather than the expectation of some stationary snapshot of ⇡.

4The number of samples per set was chosen so as to minimize the constants in the ﬁnal bound.

3

average over each set  and then take the median of the averages. From Cantelli’s inequality we have
that with probability at least 4
5  each one of the sets will not underestimate  or overestimate the mean
µ by more than ✏. Let f be the function that counts the number of sets that underestimate the
mean by more than ✏  and f + the function that counts the number of sets that overestimate the mean

10 )2
2( 3km
km

by more than ✏. From McDiarmid’s inequality [9] we have that Pf  km
Pf +  km
samples
sufﬁce to guarantee that our estimate is ✏-close to the expectation with probability at least 1  .
The median of means offers logarithmic dependence on 1
   independence from the range of values
that the variables in question can take (even allowing for them to be inﬁnite)  and can be computed
efﬁciently. The median of means estimator only requires a ﬁnite variance and the existence of a mean.
No assumptions (including boundedness) are made on higher moments.

2   e
⇡ 22.222 ln( 1

. Solving for n we have that n =

2   e

9 2 ln( 1
 )

and

 )

200

✏2

✏2

10 )2
2( 3km
km

4 Median PAC exploration

samples in u(s  a))

acceptable error ✏a.

Perform action a = arg max˜a ˜Q(s  ˜a)
Receive reward r  and transition to state s0.
if |u(s  a)| < k then

Algorithm 1 Median-PAC
1: Inputs: start state s  discount factor   max number of samples k  number of sets km  and
2: Initialize sample sets unew(s  a) = ;  u(s  a) = ; 8 (s  a). (|u(s  a)| denotes the number of
3: Set ✏b = ✏apk  and initialize value function ˜Q(s  a) = Qmax 8 (s  a).
4: loop
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
end if
16:
17: end loop
18: function ˜B ˜Q(s  a)
19:
20:
21:

end if
while max(s a)( ˜B ˜Q(s  a)  ˜Q(s  a)) >✏ a or max(s a)( ˜Q(s  a)  ˜B ˜Q(s  a)) >✏ a do
end while

Add (s  a  r  s0) to unew(s  a).
if |unew(s  a)| > |u(s  a)| and |unew(s  a)| = 2ikm  where i  0 is an integer then

Set ˜Q(s  a) = ˜B ˜Q(s  a) 8 (s  a).

u(s  a) = unew(s  a)
unew(s  a) = ;

if |u(s  a)| km then
Let (s  a  ri  s0i) be the i-th sample in u(s  a).
for j = 1 to km do
g(j) =Pj |u(s a)|
return min⇢Qmax 

i=1+(j1) |u(s a)|

✏bp|u(s a)|

end for

km

km ri +  max¯a ˜Q(s0i  ¯a)!

+ kmmedian{g(1) ...g(km)}

|u(s a)|



22:

23:
24:

else

25:
26:
27:
28: end function

end if

return Qmax.

Algorithm 1 has three parameters that can be set by the user:

lead to increased sample complexity but better approximation.

• k is the maximum number of samples per state-action. As we will show  higher values for k
• ✏a is an “acceptable error” term. Since Median-PAC is based on value iteration (lines 13
through 15) we specify a threshold after which value iteration should terminate. Value

4

iteration is suspended when the max-norm of the difference between Bellman backups is no
larger than ✏a.

• Due to the stochasticity of Markov decision processes  Median-PAC is only guaranteed
to achieve a particular approximation quality with some probability. km offers a trade-
off between approximation quality and the probability that this approximation quality is
achieved. For a ﬁxed k smaller values of km offer potentially improved approximation
quality  while larger values offer a higher probability of success. For simplicity of exposition

our analysis requires that k = 2ikm for some integer i. If km & 50

9 ln

the probability of failure is bounded above by .

4 log2

4Q2
max
✏2
a


|SA|2

'

Like most modern PAC exploration algorithms  Median-PAC is based on the principle of optimism
in the face of uncertainty. At every step  the algorithm selects an action greedily based on the
current estimate of the Q-value function ˜Q. The value function is optimistically initialized to Qmax 
the highest value that any state-action can take. If k is set appropriately (see theorem 5.4)  the
value function is guaranteed to remain approximately optimistic (approximately represent the most
optimistic world consistent with the algorithm’s observations) with high probability.
We would like to draw the reader’s attention to two aspects of Median-PAC  both in the way Bellman
backups are computed: 1) Instead of taking a simple average over sample values  Median-PAC divides
them into km sets  computes the mean over each set  and takes the median of means. 2) Instead of
using all the samples available for every state-action  Median-PAC uses samples in batches of a power
of 2 times km (line 9). The reasoning behind the ﬁrst choice follows from the discussion above: using
the median of means will allow us to show that Median-PAC’s complexity scales with the variance of
the Bellman operator (see deﬁnition 5.1) rather than Q2
max. The reasoning behind using samples in
batches of increasing powers of 2 is more subtle. A key requirement in the analysis of our algorithm
is that samples belonging to the same state-action are independent. While the outcome of sample i
does not provide information about the outcome of sample j if i < j (from the Markov property)  the
fact that j samples exist can reveal information about the outcome of i. If the ﬁrst i samples led to a
severe underestimation of the value of the state-action in question  it is likely that j samples would
never have been collected. The fact that they did gives us some information about the outcome of the
ﬁrst i samples. Using samples in batches  and discarding the old batch when a new batch becomes
available  ensures that the outcomes of samples within each batch are independent from one another.

5 Analysis
Deﬁnition 5.1.  is the minimal constant satisfying

8(s  a  ⇡

˜Q  ˜Q) sXs0

p(s0|s  a)⇣R(s  a  s0) +  ˜Q(s0 ⇡ ˜Q(s0))  B⇡ ˜Q ˜Q(s  a)⌘2

  

where 8 ˜Q refers to any value function produced by Median-PAC  rather than any conceivable
value function (similarly ⇡ ˜Q refers to any greedy policy over ˜Q followed during the execution of
Median-PAC rather than any conceivable policy).

In the following we will call 222 the variance of the Bellman operator. Note that the variance of the
Bellman operator is not the same as the variance  or stochasticity in the transition model of an MDP.
A state-action can be highly stochastic (lead to many possible next states)  yet if all the states it
transitions to have similar values  the variance of its Bellman operator will be small.
From Lemmas 5.2  5.3  and theorem 5.4 below  we have that Median-PAC is efﬁcient PAC-MDP.
Lemma 5.2. The space complexity of algorithm 1 is O (k|S||A|).
Proof. Follows directly from the fact that at most k samples are stored per state-action.
Lemma 5.3. The per step computational complexity of algorithm 1 is bounded above by

O✓ k|S||A|2

1  

ln

Qmax

✏a ◆ .

5

Proof. The proof of this lemma is deferred to the appendix.

Theorem 5.4 below is the main theorem of this paper. It decomposes errors into the following three
sources:

1. ✏a is the error caused by the fact that we are only ﬁnding an ✏a-approximation  rather than
the true ﬁxed point of the approximate Bellman operator ˜B  and the fact that we are using
only a ﬁnite set of samples (at most k) to compute the median of the means  thus we only
have an estimate.

2. ✏u is the error caused by underestimating the variance of the MDP. When k is too small
and Median-PAC fails to be optimistic  ✏u will be non-zero. ✏u is a measure of how far
Median-PAC is from being optimistic (follow the greedy policy over the value function of
the most optimistic world consistent with its observations).

3. Finally  ✏e(t) is the error caused by the fact that at time t there may exist state-actions that

do not yet have k samples.

Theorem 5.4. Let (s1  s2  s3  . . . ) be the random path generated on some execution of
Median-PAC  and ˜⇡˜⇡˜⇡ be the (non-stationary) policy followed by Median-PAC. Let ✏u =

max{0  p4km  ✏apk}  and ✏a be deﬁned as in algorithm 1. If km =& 50
✏a  ✏bpk
at least 1    for all t

1 ln (1)Qmax

km|SA|+1

  2d 1

e2 ln

log2


2k
km

✏a

4 log2

9 ln

4Q2
max
✏2
a


|SA|2

' 

< 1  and k = 2ikm for some integer i  then with probability

2✏u + 5✏a
1  

+ ✏e(t) 

V ⇤(st)  V ˜⇡(st) 
✏e(t) < c0✓✓2km + log2
1Xt=0
(|SA| + 1)⇣1 + log2l 1
1 s 2l 1

2k

km◆ Qmax + ✏ak✓8 +
m⌘l 1
1 ln (1)Qmax
m2 ln
1 ln (1)Qmax
km|SA|+1

log2


2k
km

✏a

✏a

c0 =

1 ln (1)Qmax

✏a

m

.

(1)

(2)

8

p2◆◆  

If k = 2ikm where i is the smallest integer such that 2i  42
probability at least 1    for all t

✏2
a

  and ✏0 = (1  )✏a  then with

where

and

where5

V ⇤(st)  V ˜⇡(st)  ✏0 + ✏e(t) 
✏e(t) ⇡ ˜O✓✓

✏0(1  )2 +

1  ◆|SA|◆ .

Qmax

2

1Xt=0

(3)

(4)

Note that the probability of success holds for all timesteps simultaneously  andP1t=0 ✏e(t) is an

undiscounted inﬁnite sum.

Proof. The detailed proof of this theorem is deferred to the appendix. Here we provide a proof
sketch:
The non-stationary policy of the algorithm can be broken up into ﬁxed policy (and ﬁxed approximate
value function) segments. The ﬁrst step in proving theorem 5.4 is to show that the Bellman error of
each state-action at a particular ﬁxed approximate value function segment is acceptable with respect
to the number of samples currently available for that state-action with high probability. We use
Cantelli’s and McDiarmid’s inequalities to prove this point. This is where the median of means

5f (n) = ˜O(g(n)) is a shorthand for f (n) = O(g(n) logc g(n)) for some constant c.

6

becomes useful  and the main difference between our work and earlier work. We then combine the
result from the median of means  the fact that there are only a small number of possible policy and
approximate value function changes that can happen during the lifetime of the algorithm  and the
union bound  to prove that the Bellman error of all state-actions during all timesteps is acceptable
with high probability. We subsequently prove that due to the optimistic nature of Median-PAC  at
every time-step it will either perform well  or learn something new about the environment with high
probability. Since there is only a ﬁnite number of things it can learn  the total cost of exploration for
Median-PAC will be small with high probability.

✏1

A typical “number of suboptimal steps” sample complexity bound follows as a simple corollary of

theorem 5.4. If the total cost of exploration isP1t=0 ✏e(t) for an ✏0-optimal policy  there can be no
more than P1t=0 ✏e(t)

steps that are more than (✏0 + ✏1)-suboptimal.

Note that the sample complexity of Median-PAC depends log-linearly on Qmax  which can be ﬁnite
even if Rmax is inﬁnite. Consider for example an MDP for which the reward at every state-action
follows a Gaussian distribution (for discrete MDPs this example requires rewards to be stochastic 
while for continuous MDPs rewards can be a deterministic function of state-action-nextstate since
there can be an inﬁnite number of possible nextstates for every state-action). If the mean of the reward
for every state-action is bounded above by c  Qmax is bounded above by c
1   even though Rmax is
inﬁnite.
As we can see from theorem 5.4  apart from being the ﬁrst PAC exploration algorithm that can be
applied to MDPs with unbounded rewards  Median-PAC offers signiﬁcant advantages over the current
state of the art for MDPs with bounded rewards. Until recently  the algorithm with the best known
sample complexity for the discrete state-action setting was MORMAX  an algorithm by Szita and
Szepesvári [16]. Theorem 5.4 offers an improvement of
(1)2 even in the worst case  and trades
a factor of Q2
max for a (potentially much smaller) factor of 2. A recent algorithm by Pazis and
Parr [12] currently offers the best known bounds for PAC exploration without additional assumptions
on the number of states that each action can transition to. Compared to that work we trade a factor of
Q2

max for a factor of 2.

1

5.1 Using Median-PAC when  is not known

In many practical situations  will not be known. Instead the user will have a ﬁxed exploration cost
budget  a desired maximum probability of failure   and a desired maximum error ✏a. Given  we

can solve for the number of sets as km =& 50
equation 2 except for k are known  and we can solve for k. When the sampling budget is large enough
such that k  42km

  then ✏u in equation 1 will be zero. Otherwise ✏u = p4km  ✏apk.

'  at which point all variables in

4Q2
max
✏2
a


|SA|2

9 ln

4 log2

✏2
a

5.2 Beyond the discrete state-action setting

Recent work has extended PAC exploration to the continuous state [11] concurrent exploration [4] and
delayed update [12] settings. The goal in the concurrent exploration setting is to explore in multiple
identical or similar MDPs and incur low aggregate exploration cost over all MDPs. For a concurrent
algorithm to offer an improvement over non-concurrent exploration  the aggregate cost must be lower
than the cost of non-concurrent exploration times the number of tasks. The delayed update setting
takes into account the fact that in real world domains  reaching a ﬁxed point after collecting a new
sample can take longer that the time between actions. Contrary to other work that has exploited
the variance of MDPs to improve bounds on PAC exploration [7  3] our analysis does not make
assumptions about the number of possible next states from a given action. As such  Median-PAC
and its bounds are easily extensible to the continuous state  concurrent exploration  delayed update
setting. Replacing the average over samples in an approximation unit with the median of means over
samples in an approximation unit in the algorithm of Pazis and Parr [12]  improves their bounds
(which are the best published bounds for PAC exploration in these settings) by (Rmax + Qmax)2
while introducing a factor of 2.

7

6 Experimental evaluation

We compared Median-PAC against the algorithm of Pazis and Parr [12] on a simple 5 by 5 gridworld
(see appendix for more details). The agent has four actions: move one square up  down  left  or right.
All actions have a 1% probability of self-transition with a reward of 100. Otherwise the agent moves
in the chosen direction and receives a reward of 0  unless its action causes it to land on the top-right
corner  in which case it receives a reward of 1. The world wraps around and the agent always starts at
the center. The optimal policy for this domain is to take the shortest path to the top-right corner if at a
state other than the top-right corner  and take any action while at the top-right corner.
While the probability of any individual sample being a self-transition is small  unless the number of
samples per state-action is very large  the probability that there will exist at least one state-action
with signiﬁcantly more than 1
100 sampled self-transitions is high. As a result  the naive average
algorithm frequently produced a policy that maximized the probability of encountering state-actions
100 sampled self-transitions. By contrast  it is far less likely that there will exist
with more than 1
a state-action for which at least half of the sets used by the median of means have more than 1
100
sampled self-transitions. Median-PAC was able to consistently ﬁnd the optimal policy.

7 Related Work

Maillard  Mann  and Mannor [8] present the distribution norm  a measure of hardness of an MDP.
Similarly to our deﬁnition of the variance of the Bellman operator  the distribution norm does not
directly depend on the stochasticity of the underlying transition model. It would be interesting to see
if the distribution norm (or a similar concept) can be used to improve PAC exploration bounds for
“easy” MDPs.
While to the best our knowledge our work is the ﬁrst in PAC exploration for MDPs that introduces a
measure of hardness for MDPs (the variance of the Bellman operator)  measures of hardness have
been previously used in regret analysis [6]. Such measures include the diameter of an MDP [6]  the
one way diameter [2]  as well as the span [2]. These measures express how hard it is to reach any
state of an MDP from any other state. A major advantage of sample complexity over regret is that
ﬁnite diameter is not required to prove PAC bounds. Nevertheless  if introducing a requirement for a
ﬁnite diameter could offer drastically improved PAC bounds  it may be worth the trade-off for certain
classes of problems. Note that variance and diameter of an MDP appear to be orthogonal. One can
construct examples of arbitrary diameter and then manipulate the variance by changing the reward
function and/or discount factor.
Another measure of hardness which was recently introduced in regret analysis is the Eluder dimension.
Osband and Van Roy [10] show that if an MDP can be parameterized within some known function
class  regret bounds that scale with the dimensionality  rather than cardinality of the underlying MDP
can be obtained. Like the diameter  the Eluder dimension appears to be orthogonal to the variance of
the Bellman operator  potentially allowing for the two concepts to be combined.
Lattimore and Hutter [7] have presented an algorithm that can match the best known lower bounds
for PAC exploration up to logarithmic factors for the case of discrete MDPs where every state-action
can transition to at most two next states.
To the best of our knowledge there has been no work in learning with unbounded rewards. Harrison [5]
has examined the feasibility of planning with unbounded rewards.

Acknowledgments

We would like to thank Emma Brunskill  Tor Lattimore  and Christoph Dann for spotting an error
in an earlier version of this paper  as well as the anonymous reviewers for helpful comments and
suggestions. This material is based upon work supported in part by The Boeing Company  by
ONR MURI Grant N000141110688  and by the National Science Foundation under Grant No. IIS-
1218931. Opinions  ﬁndings  conclusions or recommendations herein are those of the authors and not
necessarily those of the NSF.

8

References
[1] N Alon  Y Matias  and M Szegedy. The space complexity of approximating the frequency
moments. Journal of Computer and System Sciences - JCSS (special issue of selected papers
from STOC’96)  58:137–147  1999.

[2] Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement
learning in weakly communicating MDPs. In Proceedings of the 25th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI2009)  pages 35–42  June 2009.

[3] Christoph Dann and Emma Brunskill. Sample complexity of episodic ﬁxed-horizon reinforce-

ment learning. Advances in Neural Information Processing Systems  2015.

[4] Zhaohan Guo and Emma Brunskill. Concurrent PAC RL. In AAAI Conference on Artiﬁcial

Intelligence  pages 2624–2630  2015.

[5] J. Michael Harrison. Discrete dynamic programming with unbounded rewards. The Annals of

Mathematical Statistics  43(2):636–644  04 1972.

[6] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11:1563–1600  August 2010.

[7] Tor Lattimore and Marcus Hutter. PAC bounds for discounted MDPs. In Proceedings of the
23th International Conference on Algorithmic Learning Theory  volume 7568 of Lecture Notes
in Computer Science  pages 320–334. Springer Berlin / Heidelberg  2012.

[8] Odalric-Ambrym Maillard  Timothy A Mann  and Shie Mannor. How hard is my MDP?” the
distribution-norm to the rescue”. In Advances in Neural Information Processing Systems 27 
page 1835–1843. 2014.

[9] C. McDiarmid. On the method of bounded differences. In Surveys in Combinatorics  number 141
in London Mathematical Society Lecture Note Series  pages 148–188. Cambridge University
Press  August 1989.

[10] Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder
dimension. In Z. Ghahramani  M. Welling  C. Cortes  N. D. Lawrence  and K. Q. Weinberger 
editors  Advances in Neural Information Processing Systems 27  pages 1466–1474. 2014.

[11] Jason Pazis and Ronald Parr. PAC optimal exploration in continuous space Markov decision

processes. In AAAI Conference on Artiﬁcial Intelligence  pages 774–781  July 2013.

[12] Jason Pazis and Ronald Parr. Efﬁcient PAC-optimal exploration in concurrent  continuous state

MDPs with delayed updates. In AAAI Conference on Artiﬁcial Intelligence  February 2016.

[13] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

Wiley-Interscience  April 1994.

[14] Alexander L. Strehl  Lihong Li  and Michael L. Littman. Reinforcement learning in ﬁnite
MDPs: PAC analysis. Journal of Machine Learning Research  10:2413–2444  December 2009.
[15] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. The MIT Press 

Cambridge  Massachusetts  1998.

[16] Istvan Szita and Csaba Szepesvári. Model-based reinforcement learning with nearly tight
exploration complexity bounds. In International Conference on Machine Learning  pages
1031–1038  2010.

9

,Roger Frigola
Yutian Chen
Carl Edward Rasmussen
Jason Pazis
Ronald Parr
Jonathan How