2019,Integrating Bayesian and Discriminative Sparse Kernel Machines for  Multi-class Active Learning,We propose a novel active learning (AL) model that integrates Bayesian and discriminative kernel machines for fast and accurate multi-class data sampling. By joining a sparse Bayesian model and a maximum margin machine under a unified kernel machine committee (KMC)  the proposed model is able to identify a small number of data samples that best represent the overall data space while accurately capturing the decision boundaries. The integration is conducted using the  maximum entropy discrimination framework  resulting in a joint objective function that contains generalized entropy as a regularizer. Such a property allows the proposed AL model to choose data samples that more effectively handle non-separable classification problems. Parameter learning is achieved through a principled optimization framework that leverages  convex duality and sparse structure of KMC to efficiently optimize the joint objective function. Key model parameters are used to design a novel sampling function  to choose data samples that can simultaneously improve multiple decision boundaries  making it an effective sampler for problems with a large number of classes. Experiments conducted over both synthetic and real data and comparison with competitive AL methods demonstrate the effectiveness of the proposed model.,Integrating Bayesian and Discriminative Sparse
Kernel Machines for Multi-class Active Learning

Rochester Institute of Technology

Rochester Institute of Technology

Weishi Shi

ws7586@rit.edu

Qi Yu

qi.yu@rit.edu

Abstract

We propose a novel active learning (AL) model that integrates Bayesian and
discriminative kernel machines for fast and accurate multi-class data sampling.
By joining a sparse Bayesian model and a maximum margin machine under a
uniﬁed kernel machine committee (KMC)  the proposed model is able to identify
a small number of data samples that best represent the overall data space while
accurately capturing the decision boundaries. The integration is conducted using
the maximum entropy discrimination framework  resulting in a joint objective
function that contains generalized entropy as a regularizer. Such a property allows
the proposed AL model to choose data samples that more effectively handle
non-separable classiﬁcation problems. Parameter learning is achieved through
a principled optimization framework that leverages convex duality and sparse
structure of KMC to efﬁciently optimize the joint objective function. Key model
parameters are used to design a novel sampling function to choose data samples that
can simultaneously improve multiple decision boundaries  making it an effective
sampler for problems with a large number of classes. Experiments conducted
over both synthetic and real data and comparison with competitive AL methods
demonstrate the effectiveness of the proposed model.

Introduction

1
While more labeled data tends to improve the performance of supervised learning  labeling a large
number of data samples is labor intensive and time consuming. Furthermore  obtaining accurate
labels may be highly challenging for many specialized domains  such as medicine and biology  where
expert knowledge is required for understanding and extracting the underlying semantics of data.
Active Learning (AL) provides a promising direction to use a small subset of labeled data samples to
train high-quality supervised learning models in a cost-effective way. Consequently  AL has been
successfully applied to various applications [1  2  3].
A large number of AL models have been developed for different types of supervised learning models.
However  the design of the data sampling strategy is usually limited by the learning models  which
are not designed speciﬁcally for AL purpose. For example  max-margin based classiﬁers  such as
support vector machines (SVMs)  are widely used for sampling purpose in AL. However  as they are
essentially designed for the classiﬁcation task  using them directly for sampling might lead to a slow
convergence. Figure 1a illustrates such behavior in existing models. Assume that the two middle
clusters contain 80% of data samples. Hence  it is highly likely that the initially labeled samples are
from these two clusters  which give the initial decision boundary as shown by the dashed line. Then 
samples in the middle clusters will continue to be sampled as they are close to the current decision
boundary. This will cause a very slow convergence to the true decision boundary shown as the solid
line. Furthermore  since the model performance over iterations stays roughly the same  it may cause
AL to terminate. This is undesirable  because the true decision boundary is never discovered. Such
behavior is intrinsic to the classiﬁer  which primarily focuses on exploiting the current decision
boundary rather than exploring the entire data distribution for more effective data sampling.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) Convergence Issue of AL

(b) Distribution of SVs

(c) Distribution of RVs

Figure 1: (a) Undesired convergence behavior of AL; (b) Distribution of SVs; (c) Distribution of RVs

To address the undesired convergence behavior of existing AL models  we propose a novel a kernel
machine committee (KMC) based model that integrates Bayesian and discriminative sparse kernel
machines for multi-class active learning. The KMC model naturally extends the sampling space from
around the current decision boundaries to other critical areas through the representative data samples
identiﬁed by the sparse Bayesian model. More speciﬁcally  the proposed KMC sampler incorporates
a relevant vector machine (RVM)  which is a Bayesian sparse kernel technique  to identify data
samples (referred as relevance vectors  or RVs) that capture the overall data distribution. By further
augmenting the SVM with a RVM  the KMC model is able to choose data samples that provide a good
coverage of the entire data space (by maximizing the data likelihood) while giving special attention
to the critical areas for accurate classiﬁcation (by maximizing the margins of decision boundaries).
Figures 1b and 1c demonstrate the complementary distribution of RVs and SVs (support vectors)
and how they cover different critical areas in the data space  where most SVs are located near to the
decision boundary while most RVs are in the densely distributed areas of the two classes. There are
also much less RVs than SVs  implying that RVM is even a sparser model than SVM [4]. The sparse
nature of both RVM and SVM makes their combination an ideal choice for AL.
In essence  the KMC joins a sparse Bayesian model (RVM) with a maximum margin machine (SVM)
to choose data samples that meet two key properties simultaneously: (1) providing a good ﬁt of
the overall data distribution  and (2) accurately capturing the decision boundaries. We propose to
use the maximum entropy discrimination (MED) framework [5  6] to seamlessly integrate these
two distinctive properties into one joint objective function to train the KMC for multi-class data
sampling in AL. Furthermore  the objective function can be equivalently expressed as combination of
a likelihood term with the generalized entropy [7]. This deeper connection implies that the KMC
model is able to choose data samples that are most instrumental to tackle the difﬁcult non-separable
classiﬁcation problems (as shown in our experiments). In contrast  the SVM based models need
a much large number of SVs (hence more labeled data) to accurately capture the more complex
decision boundaries. Our main contribution is threefold: (i) a novel kernel machine committee (KMC)
model that seamlessly uniﬁes Bayesian and discriminative sparse kernel machines for effective data
sampling; (ii) a principled optimization framework by leveraging convex duality and sparse structure
of KMC to efﬁciently optimize the joint objective function; and (iii) a novel sampling function that
combines key model parameters to choose data samples that can simultaneously improve multiple
decision boundaries  making it an effective sampler for problems with a large number of classes.
2 Related Work
Uncertainty sampling is one of the most commonly used sampling method for AL  where the
informativeness of the unlabeled data point is determined by its distance to the decision boundaries [8 
9]. In order to better accommodate multi-class problems  Culotta and McCallum [10] propose a
sampling method based on the predictive probability over the sample pool where the data point with
the smallest probability of its predicted class is sampled. However  this method prefers trivial data
samples with evenly distributed predictive probabilities. To address this  a Best-versus-Second Best
(BvSB) model is proposed to choose the data sample whose probabilities of the most and second
most classes are closest to each other [10]. But BvSB only focuses on the two most probable classes
while the probability distribution of other classes is ignored. As a result  this method is less effective
with more classes. Entropy-based methods obtain a complete view of uncertainty over all classes to
conduct effective AL sampling. But the lack of training samples in the beginning of the AL impedes
the accurate estimation of entropy. In fact  all sampling methods that rely on the probability output of

2

True decision boundaryExploreExploitInitial decision boundary−4−3−2−10123Feature 1−2−1012345Feature 2SVM with 149 vectorstrain_postrain_negsupport vecs0.000.150.300.450.600.750.901.05−4−3−2−10123Feature 1 2 1012345Feature 2RVM with 30 vectorstrain_postrain_negrelevant vecs0.000.150.300.450.600.750.901.05SVM should be dealt with caution as the probabilities are estimated by ﬁtting an additional logistic
regression model over SVM scores. Thus  the estimated probability might not reﬂect the true behavior
of the SVM as a discriminative model [11].
Compared to discriminative models (e.g.  SVMs)  generative models can be more naturally used for
multi-class AL. Roy et al. propose a sampling method based on Naive Bayes using the expectation
of future classiﬁcation error as the sampling criterion [12]. Kottke et al. propose a multi-class
probabilistic AL model (McPAL) according to the expectation of the classiﬁcation error from clusters
of unlabeled data [13]. Both methods are computationally intensive  making them hard to be applied
to real-time AL. Furthermore  since the learning objective (e.g. maximum likelihood) of generative
models are not speciﬁcally designed for discrimination [14]  the model performance might be less
competitive when AL is complete as compared to the discriminative models.
There are also existing models that utilize the properties of the data space or the trained model
for effective data sampling. A convex-hull based AL model is developed to avoid sampling less
informative data points that are close to the current support vectors [15]. A similar strategy is adopted
in [16]  where data samples with furthest distance to its closest relevance vectors are sampled. The
QUIRE model combines the clustering structure of unlabeled data with the class assignments of the
labeled data  allowing it to choose samples both informative and representative [17]. However  this
model is designed for binary problems. Different from all existing works  the proposed KMC model
leverages the complementary behavior of Bayesian and discriminative sparse kernel machines and
systematic integrates them for effective data sampling for multi-class AL.
3 Kernel Machine Committee based Active Learning
Let X = {x1  ...  xM} denote a training set with M data samples and y = {y1  ...  yM} be their
corresponding labels. Let’s consider the binary-class case where ∀yi ∈ y  yi ∈ {−1  +1} and the
multi-class problems can be achieved via the one-versus-the-rest strategy. The conditional distribution
of label yi is given by p(yi = 1|w  xi) = σ(wT φ(xi))  where σ is the logistic sigmoid function 
w is the coefﬁcient  and φ(xi) is feature vector of xi. For RVM  we set φj(xi) = k(xi  xj) with
k(· ·) being a kernel function. We further place a prior over the coefﬁcient w with hyperparameter
j ). Having a separate hyperparameter
αi for each coefﬁcient wi will ensure model sparsity through automatic relevance determination
(ARD) [4]. In particular  during the parameter learning process  some of the αi will be driven
to inﬁnity  which has the effect of making the corresponding wi approach zero. As a result  the
associated data sample xi will be excluded from the RV set. The optimal α can be determined
through evidence approximation  which maximizes the log marginal likelihood of observed data

α = (α1  ...  αM )  given by p(w|α) = (cid:81)

j N (wj|0  α−1

given by ln p(y|X  α) = ln(cid:82)(cid:81)

i [p(yi|xi  w)] p(w|α)dw.

Since the likelihood term p(yi|xi  w) is a logistic function  which is non-conjugate to the Gaussian
prior p(w|α)  the integration can not be straightforwardly performed. By applying Jensen’s inequality
to the log function  we can obtain a lower bound of the log likelihood given by ln p(y|X  α) ≥
Eq(w)[ln p(y|X  w)] − KL(q(w)||p(w|α))  where q(w) is a variational distribution and the second
term is the KL divergence between q(w) and prior distribution p(w|α). This change makes it
possible to put parameter learning in RVM into the MED framework [5]  which allows us to further
integrate a set of margin-based constraints

min

q(w) α ξ

KL(q(w)||p(w|α)) − Eq(w)[ln p(y|X  w)] + C

ξi

(1)

(cid:88)

(cid:90)

i

q(w)dw = 1

subject to ∀i : Eq(w)[yif (w  xi)] ≥ −ξi 

ξi ≥ 0 

p(yi=−1|w xi) = wT φ(xi) ∀i  yi = −1; f (w  xi) = ln p(yi=−1|w xi)

where ξ’s are slack variables and f (w  xi) is a cost function  deﬁned as f (w  xi) =
ln p(yi=1|w xi)
p(yi=1|w xi) = −wT φ(xi) ∀i  yi = 1 
which ensures that a linear cost is introduced only for misclassiﬁed data samples.
Directly optimizing (1) is still challenging due to the likelihood term that follows a logistic function.
We make further approximation by using an exponential quadratic function to lower bound the logistic
function [18]: σ(z) ≥ σ(γ) exp{(z − γ)/2 − λ(γ)(z2 − γ2)}  where λ(γ) = 1
2 ). This
approximation allows us to derive a lower bound of the likelihood term in (1).

2γ (σ(γ) − 1

3

Lemma 1. For ∀γ ∈ RM   ∀y ∈ {−1  +1}M   there exists a lower bound of the likelihood of the
logistic regression function that has an exponential quadratic functional form and satisﬁes:

σ(γi) exp

(wT φ(xi)yi − γi) − λ(γi)([wT φ(xi)]2 − γ2
i )

= h(w  γ)

(2)

p(y|w  X) ≥ M(cid:89)

i=1

where γ = (γ1  ...  γm)T .

(cid:26) 1

2

(cid:27) .

(cid:88)
(cid:90)

i

Proof. By leveraging the symmetry of the sigmoid function  we have

p(yi = 1|w  xi) = σ(wT φ(xi))
p(yi = −1|w  xi) = 1 − σ(wT φ(xi)) = σ(−wT φ(xi))

(3)
(4)
Using σ(z) ≥ σ(γ) exp{(z − γ)/2 − λ(γ)(z2 − γ2)}  the conditional likelihood of yi is given by:
p(yi|w) = σ(yiwT φ(xi) ≥ σ(γi) exp

(wT φ(xi)yi − γi) − λ(γi)([yiwT φ(xi)]2 − γ2
i )

(cid:26) 1

(cid:27)

(5)

Substitute for y2

2
i = 1 and lemma 1 is proved.

Replacing the likelihood with h(w  γ) in (1)  the ﬁnal objective function of KMC is given by

Objective (KMC): min

q(w) γ α ξ

KL(q(w)||p(w|α)) − Eq(w)[ln h(w  γ)] + C

ξi

(6)

subject to

∀i : Eq(w)[yif (w  xi)] > −ξi 

ξi ≥ 0 

q(w)dw = 1

The ﬁrst term is a regularizer of the variational distribution q(w). The use of an ARD prior imposes
the sparsity of w  which guarantees the sparsity of the KMC. The second term approximates the
negative log likelihood of the observed data and the last term brings in the maximum margin-based
constraints. It is also worth to note that the expectation Eq(w)[f (w  xi)] is taken over q(w)  which
demonstrates the interplay of the Bayesian RVM model and the maximum margin SVM model. The
integrated objective allows the KMC model to identify a small number of data samples  referred to
as KMC vectors  which can describe the observed data well while accurately capturing the decision
boundaries at the same time. In addition  by combining the ﬁrst and third terms  they form a special
case of generalized entropy [7]. Therefore  the KMC objective function can also be interpreted
as minimizing the negative log likelihood with the generalized entropy as a regularizer. Such
a formulation implies that it can more effectively handle non-separable classiﬁcation problems 
beneﬁting from the generalized entropy (as conﬁrmed through our experiments). To extend to K
classes  we adopt the one-versus-the-rest strategy and then apply a softmax transformation  which
gives rise to the posterior probability of the k-th class: p(Ck|x) = e
j φ(x)].
The conjugacy introduced by the lower bound function is essential for efﬁcient KMC parameter
learning. First  it guarantees a Gaussian form of q(w) and other parameters expressed by the moments
of q(w). This allows us to develop an iterative algorithm to efﬁciently optimize q(w). Second  we
can leverage convex duality and the sparse structure to efﬁciently solve for Lagrangian multipliers.
3.1 Parameter Learning in KMC
In order to optimize the KMC objective function in (6) and learn the key model parameters  we ﬁrst
derive the Lagrangian function by introducing dual variables ui ≥ 0 and v for each inequality and
equality constraints:

k φ(x)]/(cid:80)K

j=1 e

E[wT

E[wT

L(ξ  α  γ) = KL(q(w)||p(w|α)) − Eq(w)[ln h(w  γ)]

ui(Eq(w)[yi(f (w  xi))] + ξi) + v(

q(w)dw − 1)

(7)

(cid:90)

(cid:88)

ξi −(cid:88)

i

i

+ C

We start by solving q(w). By setting ∂L
quadratic form  we have: q(w) = N (mq  Sq) with

∂q(w) = 0 and recognizing that q(w) takes an exponential

λ(γi)φ(xi)φ(xi)T   A = diag(α)

(8)

(cid:88)

i

mq = Sq[

(cid:88)

i

(yi(ui +

)φ(xi)]  S−1

q = A + 2

1
2

4

We now solve the Lagrangian multipliers. By substituting (8) back to (7)  we obtain the dual problem:
(9)

− ln Z(u) 

∀i  ui ≥ 0

ui = C 

max

u

subject to (cid:88)

i

where u = (u1  ...  uM )T and Z(u) is the normalization factor that ensures that q(w) integrates to 1.
In particular  we have

ln Z(u) = ln(2π)

M

2 +

ln σ(γi) + ln|Sq| 1

2 +

1
2

zT Sqz +

(λ(γi)γ2

γi)

(10)

i − 1
2

(cid:88)

i

(cid:88)

i

i yi(ui + 1

2 )φ(xi). By removing terms irrelevant to u  the dual problem is given by

Dual (KMC): min
u

1
2

zT Sqz subject to :

ui = C  ui ≥ 0

(11)

where z =(cid:80)

(cid:88)

i

The dual problem is essentially a constrained quadratic programming problem of u  which can be
solved using a standard QP solver. However  using the ARD prior ensures that the KMC problem has
a nice sparse structure  as shown in the following theorem.
Theorem 1. Using an ARD prior  the covariance matrix Sq of variational distribution q(w) has a
sparse structure. In particular  for |αi| → ∞ and |αj| → ∞  Sq(i  j) → 0 as Sq(i  j) ∝ 1/|αj|.
Proof. First  reformulate S−1
as a matrix form: A + 2ΦT ΛΦ  where Φ = (φ(x1)  ...  φ(xM )T and
Λ = diag(λ(γ1)  ...  λ(γM )). Given the deﬁnition of φ(xi)  Φ = ΦT . Applying Woodbury identity
to S−1

  we have

q

q

where A−1 = diag(α−1
1   ...  α−1
second term in (12). In particular 

α−1
Similarly  we have ΦA−1 = (cid:0)α−1

A−1Φ =

1

Sq = A−1 + A−1Φ(Λ−1 + ΦA−1Φ)−1ΦA−1

(12)
M ) is a diagonal matrix (hence already sparse). So we focus on the

1 )

 =

 α−1

 φ(xT

M φ(xM )(cid:1). It can be shown that a signiﬁcant pro-

α−1
M φ(xT

...
φ(xT

1 φ(xT
1 )



(13)

M )

M )

...

...

α−1
1 φ(x1)  ...  α−1

M

portion of α’s approach ∞ due to the ARD prior [19]. We then apply the Woodbury identity to
the term (Λ−1 + ΦA−1Φ)−1 in (12) and assume that |αi| → ∞  it is straightforward to show that
(Λ−1 + ΦA−1Φ)−1 ≈ A. Using this fact and (13)  we can show Sq(i  j) ∝ 1/|αj| and hence
Sq(i  j) → 0 for |αj| → ∞.

Our empirical evaluation over both synthetic and real data shows that a high percentage (e.g.  > 80%)
of α’s are driven to ∞ during the optimization process. Therefore  Sq is indeed highly sparse. Thus 
the problem can be solved much more efﬁciently by quadratic solvers boosted by sparse input such
as MOSEK [20].
Next  we solve γ by set ∂L
∂γi

= 0 and obtain the update rule of γi as:

i = φ(xi)T Eq(w)[wwT ]φ(xi) = φ(xi)T [Sq + mqmT
γ2
The derivation of the update rule of α can beneﬁt from the following result.
Lemma 2. Let p1(x) ∼ N (x|m1  S1) and p2(x) ∼ N (x|m2  S2) then the KL(p1||p2) is given by:
(15)

(cid:21)
2 (m1 − m2)

(m1 − m2)T S−1

KL(p1||p2) =

1
2
Substituting q(w) for p1 and p(w|α) for p2 in (15)  we have

2 S1] +

q ]φ(xi)

(14)

(cid:20)

1
2

ln

|S2|
|S1| − M + Tr[S−1
(cid:88)

ln α−1

i + ln|Sq| − M + Tr[S−1

p Sq] + mT

q S−1

p mq)

KL(q(w)||p(w|α)) =

(16)

1
2

(

where S−1

p = A. Solving for ∂L
∂αi

i

= 0 while making use of (16)  we obtain the update rule for αi:

αi =

1

Sq(ii) + (mq(i))2

(17)

where Sq(ii) denotes the i-th diagonal entry of Sq.

5

3.2 KMC-based Multi-class Data Sampling
We develop a novel two-phase KMC-based sampling process to achieve many-class sampling.
The proposed KMC model is used for different purposes in each phase: predicting the posterior
probabilities of different classes in initial sampling and KMC vector discovery for ﬁnal sampling.
More speciﬁcally  in the initial sampling phase  a pre-trained KMC model using the initial labeled
pool is used to make an prediction of all the data samples in the unlabeled pool. The top-S samples
will be selected according to their entropy deﬁned over the posterior probabilities of different classes.
In essence  these samples confuse the current KMC model the most and thus have the greatest
potential to improve the model if being labeled. Different from existing AL approaches that directly
send these samples for human labeling  the proposed process proceeds by including these samples
along with their predicted labels to retrain the KMC model. The goal is to further select data samples
identiﬁed as KMC vectors  which can contribute to improving the decision boundaries while properly
exploring the data space to avoid slow convergence of AL.
We propose a multi-class sampling function to measure the overall improvement that a sample can
bring to all the classes. In particular  when solving the KMC objective in (6) for the k-class  we obtain
an optimal α(k)
  we obtain two quantities
s(k)
that are referred to as sparsity and quality  respectively  where sparsity measures the
i
overlap of data sample xi with other samples and quality measures xi’s contribution to reducing the
i → ∞
error between the model output and the actual targets. The optimization process will set α(k)
  which makes the corresponding wi → 0. In this case  xi will not contribute to
if q(k)
2 − s(k)
the k-class (and is not included as a KMC vector). Otherwise  α(k)
).
Intuitively  if xi is not too close to other data samples and effective to reduce the classiﬁcation error 
its corresponding α(k)
i will take a small positive value. By considering all K classes  we use the
following function for multi-class sampling:
x∗ = arg min

for each xi. Similar to RVM [4]  when optimizing each α(k)

i
and q(k)

is set to s(k)

(cid:88)

/(q(k)

i

(18)

< s(k)

i

Eq(w)[w(k)

i

]/α(k)

i

2

2

i

i

i

i

i

i

i

k

i

i

In essence  the multi-class sampling function aggregates the impact of w(k)
that is directly used to
compute the posterior probability and the contribution of α(k)
that gives preference to non-redundant
samples that can help reduce the classiﬁcation error. Finally  by combining the contribution to all
classes  it will choose a data sample that can beneﬁt a large number of classes  making it effective and
efﬁcient when many classes are involved. The details are summarized in the supplementary materials
and the source code is available at [21].
4 Experiments
We conduct extensive experiments to evaluate the proposed KMC AL model. We ﬁrst investigate and
verify some important model properties by using synthetic data and through comparison with SVM
and RVM  which helps demonstrate the potential of using KMC for AL. We then apply the model to
multiple real-world datasets from diverse domains. Comparison with state-of-the-art multi-class AL
models will establish the advantage of using KMC in real-world AL applications. For KMC  unless
otherwise speciﬁed  parameter C is set to 10−2 and the convergence threshold is set to 10−3.

4.1 Synthetic Data
We draw 500 2-D data samples from a moon-shape distribution and use 70 % of the data for training
and 30 % for testing. In Figures 2 and 3  we visualize the learned vectors of the compare models
at different noise levels. The results help verify important properties of the proposed KMC model 
as described in our theoretical ﬁndings. First  the selected KMC vectors sufﬁciently explore critical
areas to cover the entire data distribution while giving adequate attention to the decision boundaries.
In contrast  SVM overly focuses on the decision boundaries by using a large number of SVs while
RVM under explores the decision areas by using a very small number of RVs and hence suffers
from a relatively low model accuracy. This result veriﬁes the desired behavior of KMC vectors
that are discovered through optimizing the joint objective function (6). Second  KMC maintains a
very high sparsity level at around 90% in both cases. This veriﬁes our theoretical result as stated in
Theorem 1. These two important properties clearly establish the potential of using KMC for effective
data sampling in AL. Finally  as we add more Gaussian noises to make the data less separable  KMC

6

Figure 2: Moon-shape distribution with 30% Gaussian noises

Figure 3: Moon-shape distribution with 60% Gaussian noises

Dataset
Yeast
Reuters
Penstroke
Derm 1
Derm 2
Auto-drive

8

5227
500
1391
1554
48

Table 1: Description of Datasets

#Inst #Attr #Classes Class Distr. Domain
Biology
1484
News
10788
Image
1144
Medical
800
868
Medical
Automobile
58509

Skewed
Skewed
Even
Even
Even
Even

10
75
26
50
30
11

shows its robustness by maintaining the highest accuracy. Furthermore  the sparsity of KMC remains
stable unlike SVM with an exploding number of SVs. This veriﬁes the impact of the generalized
entropy regularizer in the KMC objective function (6).
4.2 Real Data
We choose 6 datasets from different domains  as summarized in Table 1  to evaluate the proposed
KMC based multi-class sampling model.
• Yeast uses biological features to predict cellular localization sites of proteins.
• Reuters uses the content of Reuters News to conduct text classiﬁcation.
• Penstroke contains handwritten English letters from writers with different writing styles.
• Derm I&II contain physicians’ verbal narrations of dermatological images with different diseases.
• Auto-drive uses different sensor readings to predict failures of a running automobile.
Experimental setting and comparison methods:
It is typical for an AL model to start with limited
labeled training samples. For datasets with an even class distribution (except for Auto-drive)  we
randomly select one data sample per class to form the initial training set for AL. For Auto-drive  given
its large size  we use 20 labeled samples per class. For unevenly distributed datasets  we randomly
sample 1% of the data from each class to form the initial set. For comparison  random sampling
(Random) is a commonly used baseline approach. We also include Entropy-based sampling (Entr) that
selects the data sample with the maximum entropy of the predicted class distribution. Furthermore 
we also compare the KMC AL model with three state-of-the-art multi-class AL models that have
been discussed in the related work section  including Best-vs-Second-Best sampling (BvSB) [10] 
multi-class Probabilistic Active Learning (McPAL) [13]  and multi-class convex hull based sampling
(MC-CH) [15]. The reported test accuracy is averaged over three runs. Two important parameters of
KMC  the large margin coefﬁcient C and the initial sample size S  are set to 1 and 40 respectively
when compared with other AL models.
AL performance comparison:
Figure 4 compares the AL results from KMC and the other four
competitive models along with the baseline random sampling. In most cases  the KMC model shows

7

−1012Feature 1−1.0−0.50.00.51.01.5Feature 2RVM with 17 vectors Test accuracy:0.89train_postrain_negrelevant vecs0.000.150.300.450.600.750.901.05−1012Feature 1−1.0−0.50.00.51.01.5Feature 2SVM with 123 vectors Test accuracy:0.94train_postrain_negsupport vecs0.000.150.300.450.600.750.901.05−1012Feature 1−1.0−0.50.00.51.01.5Feature 2KMC with 23 vectors Test accuracy:0.95train_postrain_negKMC vecs0.000.150.300.450.600.750.901.05−2−10123Feature 1−1.5−1.0−0.50.00.51.01.52.0Feature 2RVM with 19 vectors Test accuracy:0.78train_postrain_negrelevant vecs0.000.150.300.450.600.750.901.05−2−10123Feature 1−1.5−1.0−0.50.00.51.01.52.0Feature 2SVM with 192 vectors Test accuracy:0.8train_postrain_negsupport vecs0.000.150.300.450.600.750.90−2−10123Feature 1−1.5−1.0−0.50.00.51.01.52.0Feature 2KMC with 40 vectors Test accuracy:0.8train_postrain_negKMC vecs0.000.150.300.450.600.750.901.05Figure 4: AL Performance Comparison

a fast convergence of AL and better model accuracy. In four datasets  including Yeast  Reuters  Derm
I (tied with MC-CH)  and Derm II  KMC demonstrates a very clear advantages in the convergence
speed of AL. In the other two datasets (i.e.  Penstroke and Auto-drive)  KMC achieves comparable
performance with the best competitive model in the early stage of AL but converges to a high model
accuracy in the end. The excellent AL performance of KMC beneﬁts from the ability of the KMC
vectors that can effectively explore the entire data distribution while accurately capturing the decision
boundaries. The sparse structure of KMC further ensures that only very limited labeled data samples
are needed to train highly accurate models. Furthermore  the fast convergence also attributes to
effectiveness of the multi-class sampling function (18) that chooses data samples to simultaneously
improve multiple classes  which is further veriﬁed in later experiments.

Impact of model parameters: We study the impact of two parameters  including the large margin
coefﬁcient C and the initial sample size S that may affect the model performance. Since a data
sample with ξi > 0 will be wrongly classiﬁed  a large C will lead to a variational distribution q(w)
with more discriminative strength. In contrast  a smaller C makes the model more tolerant of wrongly
classiﬁed cases (and also more robust to noises). In practice  KMC performs quite stable over a wide
range of C values (i.e.  0.001 to 1) for all the datasets. As for the initial sample size S  Figure 5
shows the AL curves of KMC with S set to 5 20  and 40  respectively. It can be seen that at the early
stage of AL  a larger S creates an advantage by allowing the model to explore more confusing data
samples when the model is not accurate enough to conﬁrm the confusion. For most datasets  such
advantage reduces as the model becomes more accurate along the AL process. The only exception
is Auto-drive  where a larger initial sample size still shows a good advantage toward the end of the
500 AL iterations. In fact  given the large size of this dataset  after sampling 500 data samples  all
the labeled data is only comprised of around 1% of the entire dataset. In addition  this dataset is
highly noisy  so the AL model may not fully converge yet by using such limited labeled data  which
corresponds to a relative early stage of AL. Therefore  the advantage of using a larger sample size is
still signiﬁcant as in the early stages of other datasets.

Effectiveness of multi-class sampling:
In this set of experiments  we further verify the effective-
ness of the multi-class sampling function by demonstrating that the selected data samples have the
potential to beneﬁt multiple classes. Figure 6 visualizes data samples with high sampling scores in
the Penstroke dataset from the ﬁrst 100 AL interactions. For each character  we show the true label
along with the most confusing labels (shown in the parenthesis) based on the predictive distribution
of KMC. The AL iteration is shown at the bottom. It can be seen that the sampling function prefers
samples that is confusing w.r.t. multiple classes. In other words  once the sample is labeled  it has
the potential to improve multiple decision boundaries. This exhibits the behavior of more effective
exploitation  leading to fast convergence in AL. We also observe that characters in similar appearance
tend not to be sampled repeatedly over a large number of AL iterations. Instead  samples from each
class are selected in a round-robin manner  which shows the behavior of effective exploration. The
good balance between exploitation and exploration explains fast and accurate sampling behavior of
the KMC based multi-class sampling function.

8

050100150200250300350400450500Number of active iterations0.250.300.350.400.450.500.550.60Model accuracyYeast datasetRandomBvSBEntrMCPALMC-CHKMC050100150200250300350400450500Number of active iterations0.50.60.70.80.9Model accuracyReuters datasetRandomBvSBEntrMC-CHKMC050100150200250300350400450500Number of active iterations0.000.050.100.150.200.250.300.35Model accuracyPenstroke datasetRandomBvSBEntrMCPALMC-CHKMC050100150200250300350400450500Number of active iterations0.00.20.40.60.81.0Model accuracyDermatology_1 datasetRandomBvSBEntrMCPALMC-CHKMC050100150200250300350400450500Number of active iterations0.00.20.40.60.81.0Model accuracyDermatology_2 datasetRandomBvSBEntrMCPALMC-CHKMC050100150200250300350400450500Number of active iterations0.200.250.300.35Model accuracyAuto-dirve datasetRandomBvSBEntrMCPALMC-CHKMCFigure 5: Impact of the Initial Sample Size S

Figure 6: Representative AL Samples that Beneﬁt Multiple Classes

5 Conclusion
We propose a novel kernel machine committee that combines Bayesian and discriminative sparse
kernel machines for multi-class AL. These two kernel machines with distinct properties are seamlessly
uniﬁed using the maximum entropy discrimination framework in a principled way that allows the
resultant model to choose data samples ideal for AL purpose. The sparse structure of the KMC
minimizes the size of the selected data samples for labeling and also ensures efﬁcient parameter
learning to support fast model training for real-time AL. A novel multi-class sampling function is
designed that combines key model parameters to choose data samples most effective to improve the
decision boundaries of multiple classes  leading to faster AL convergence in multi-class problems.
Extensive experiments conducted over both synthetic and real data help verify our theoretical results
and clearly justify the effectiveness of the proposed KMC based AL model.

Acknowledgement
This research was supported in part by an NSF IIS award IIS-1814450 and an ONR award N00014-
18-1-2875. The views and conclusions contained in this paper are those of the authors and should not
be interpreted as representing any funding agency.

References
[1] Bishan Yang  Jian-Tao Sun  Tengjiao Wang  and Zheng Chen. Effective multi-label active learn-
ing for text classiﬁcation. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining  pages 917–926. ACM  2009.

[2] Oisin Mac Aodha  Neill DF Campbell  Jan Kautz  and Gabriel J Brostow. Hierarchical subquery
evaluation for active learning on a graph. In Proceedings of the IEEE conference on computer
vision and pattern recognition  pages 564–571  2014.

9

050100150200250300350400450500Number of active iterations0.300.350.400.450.500.550.60Model accuracyYeast datasetS=5S=20S=40050100150200250300350400450500Number of active iterations0.550.600.650.700.750.800.850.90Model accuracyReuters datasetS=5S=20S=40050100150200250300350400450500Number of active iterations0.050.100.150.200.250.300.35Model accuracyPenstroke datasetS=5S=20S=40050100150200250300350400450500Number of active iterations−0.20.00.20.40.60.81.0Model accuracyDermatology_1 datasetS=5S=20S=40050100150200250300350400450500Number of active iterations0.00.20.40.60.81.0Model accuracyDermatology_2 datasetS=5S=20S=40050100150200250300350400450500Number of active iterations0.220.240.260.280.300.320.340.36Model accuracyAuto-drive datasetS=5S=20S=40AL iteration:3True label:f (j p g t)AL iteration:23True label:f (z p r)AL iteration:9True label:r (b w h)AL iteration:39True label:r (v y l h)AL iteration:13True label:t (a k e)AL iteration:26True label:o (u g k)AL iteration:17True label:d (l e a)AL iteration:45True label:d (a e p u)AL iteration:24True label:h (l b)AL iteration:53True label:h (a u q l c)AL iteration:87True label:h (a k n)AL iteration:32True label:i (j l f e)AL iteration:51True label:i (g c)AL iteration:34True label:u (c l)AL iteration:55True label:u (a q l g)AL iteration:98True label:u (w a q)[3] Meng Wang and Xian-Sheng Hua. Active learning in multimedia annotation and retrieval: A

survey. ACM Transactions on Intelligent Systems and Technology (TIST)  2(2):10  2011.

[4] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and

Statistics). Springer-Verlag  Berlin  Heidelberg  2006.

[5] Tommi Jaakkola  Marina Meila  and Tony Jebara. Maximum entropy discrimination.

Advances in neural information processing systems  pages 470–476  2000.

In

[6] Jun Zhu and Eric P Xing. Maximum entropy discrimination markov networks. Journal of

Machine Learning Research  10(Nov):2531–2569  2009.

[7] Miroslav Dudík  Steven J Phillips  and Robert E Schapire. Maximum entropy density estimation
with generalized regularization and an application to species distribution modeling. Journal of
Machine Learning Research  8(Jun):1217–1260  2007.

[8] Simon Tong and Edward Chang. Support vector machine active learning for image retrieval. In
Proceedings of the ninth ACM international conference on Multimedia  pages 107–118. ACM 
2001.

[9] Michael I Mandel  Graham E Poliner  and Daniel PW Ellis. Support vector machine active

learning for music retrieval. Multimedia systems  12(1):3–13  2006.

[10] Ajay J Joshi  Fatih Porikli  and Nikolaos Papanikolopoulos. Multi-class active learning for

image classiﬁcation. In CVPR  pages 2372–2379. IEEE  2009.

[11] Tong Zhang et al. Statistical behavior and consistency of classiﬁcation methods based on convex

risk minimization. The Annals of Statistics  32(1):56–85  2004.

[12] Nicholas Roy and Andrew McCallum. Toward optimal active learning through monte carlo

estimation of error reduction. ICML  Williamstown  pages 441–448  2001.

[13] Daniel Kottke  Georg Krempl  Dominik Lang  Johannes Teschner  and Myra Spiliopoulou.

Multi-class probabilistic active learning. In ECAI  pages 586–594  2016.

[14] Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural

networks  10(5):988–999  1999.

[15] Weishi Shi and Qi Yu. An efﬁcient many-class active learning framework for knowledge-rich
domains. In 2018 IEEE International Conference on Data Mining (ICDM)  pages 1230–1235.
IEEE  2018.

[16] Catarina Silva and Bernardete Ribeiro. Combining active learning and relevance vector machines
for text classiﬁcation. In Sixth International Conference on Machine Learning and Applications
(ICMLA 2007)  pages 130–135. IEEE  2007.

[17] Sheng-Jun Huang  Rong Jin  and Zhi-Hua Zhou. Active learning by querying informative and
representative examples. In Advances in neural information processing systems  pages 892–900 
2010.

[18] Tommi S Jaakkola and Michael I Jordan. Bayesian parameter estimation via variational methods.

Statistics and Computing  10(1):25–37  2000.

[19] Anita C Faul and Michael E Tipping. Analysis of sparse bayesian learning. In Advances in

neural information processing systems  pages 383–389  2002.

[20] MOSEK ApS. MOSEK Optimizer API for Python 8.1.0.80  2019.

[21] Weishi Shi and Qi Yu. Source Code and Data. https://drive.google.com/drive/
[Online; ac-

folders/1kk50iDvgR8PdpB8lbt32rqn9KGTDFB4n?usp=sharing  2019.
cessed 12-October-2019].

[22] M Andersen  Joachim Dahl  and Lieven Vandenberghe. Cvxopt: A python package for convex

optimization. abel. ee. ucla. edu/cvxopt  2013.

10

,Sheeraz Ahmad
He Huang
Angela Yu
Andrew Wilson
Zhiting Hu
Russ Salakhutdinov
Eric Xing
Weishi Shi
Qi Yu