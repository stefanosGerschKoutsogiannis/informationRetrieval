2018,Inequity aversion improves cooperation in intertemporal social dilemmas,Groups of humans are often able to find ways to cooperate with one another in complex  temporally extended social dilemmas. Models based on behavioral economics are only able to explain this phenomenon for unrealistic stateless matrix games. Recently  multi-agent reinforcement learning has been applied to generalize social dilemma problems to temporally and spatially extended Markov games. However  this has not yet generated an agent that learns to cooperate in social dilemmas as humans do. A key insight is that many  but not all  human individuals have inequity averse social preferences. This promotes a particular resolution of the matrix game social dilemma wherein inequity-averse individuals are personally pro-social and punish defectors. Here we extend this idea to Markov games and show that it promotes cooperation in several types of sequential social dilemma  via a profitable interaction with policy learnability. In particular  we find that inequity aversion improves temporal credit assignment for the important class of intertemporal social dilemmas. These results help explain how large-scale cooperation may emerge and persist.,Inequity aversion improves cooperation in

intertemporal social dilemmas

Edward Hughes∗  Joel Z. Leibo∗  Matthew Phillips  Karl Tuyls  Edgar Dueñez-Guzman 
Antonio García Castañeda  Iain Dunning  Tina Zhu  Kevin McKee  Raphael Koster 

Heather Roff  Thore Graepel

DeepMind  London  United Kingdom

{edwardhughes  jzl  karltuyls  duenez  antoniogc  idunning  tinazhu 

kevinrmckee  rkoster  hroff  thore}@google.com 

matthew.phillips.12@ucl.ac.uk

Abstract

Groups of humans are often able to ﬁnd ways to cooperate with one another
in complex  temporally extended social dilemmas. Models based on behavioral
economics are only able to explain this phenomenon for unrealistic stateless matrix
games. Recently  multi-agent reinforcement learning has been applied to generalize
social dilemma problems to temporally and spatially extended Markov games.
However  this has not yet generated an agent that learns to cooperate in social
dilemmas as humans do. A key insight is that many  but not all  human individuals
have inequity averse social preferences. This promotes a particular resolution of
the matrix game social dilemma wherein inequity-averse individuals are personally
pro-social and punish defectors. Here we extend this idea to Markov games and
show that it promotes cooperation in several types of sequential social dilemma 
via a proﬁtable interaction with policy learnability. In particular  we ﬁnd that
inequity aversion improves temporal credit assignment for the important class
of intertemporal social dilemmas. These results help explain how large-scale
cooperation may emerge and persist.

Introduction

1
In intertemporal social dilemmas  there is a tradeoff between short-term individual incentives and
long-term collective interest. Humans face such dilemmas when contributing to a collective food
storage during the summer in preparation for a harsh winter  organizing annual maintenance of
irrigation systems  or sustainably sharing a local ﬁshery. Classical models of human behavior based
on rational choice theory predict that cooperation in these situations is impossible [1  2]. This poses
a puzzle since humans evidently do ﬁnd ways to cooperate in many everyday intertemporal social
dilemmas  as documented by decades of ﬁeldwork [3  4] and laboratory experiments [5  6]. Providing
an empirically grounded explanation of how individual behavior gives rise to societal cooperation is
seen as a core goal in several subﬁelds of the social sciences and evolutionary biology [7  8  9].
[10  11] proposed inﬂuential models based on behavioral game theory. However  these models have
limited applicability since they only generate predictions when the problem can be cast as a matrix
game (see e.g. [12  13]). Here we consider a more realistic video-game setting  like those introduced
in the behavioral research of [14  15  16]. In this environment  agents do not simply choose to
cooperate or defect like they do in matrix games. Rather they must learn policies to implement their
strategic decisions  and must do so while coping with the non-stationarity arising from other agents
learning simultaneously. Several papers used multi-agent reinforcement learning [17  18  19] and

∗Equal contribution.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

planning [20  21  22  23] to generate cooperation in this setting. However  this approach has not yet
demonstrated robust cooperation in games with more than two players  which is often observed in
human behavioral experiments. Moreover naïvely optimizing group reward is also ineffective  due to
the lazy agent problem [24].†
It is difﬁcult for both natural and artiﬁcial agents to ﬁnd cooperative solutions to intertemporal social
dilemmas for the following reasons:

1. Collective action – individuals must learn and coordinate policies at a group level to avoid

falling into socially deﬁcient equilibria.

2. Temporal credit assignment – rational defection in the short-term must become associated

with long-term negative consequences.

Many different research traditions  including economics  evolutionary biology  sociology  psychology 
and political philosophy have all converged on the idea that fairness norms are involved in resolving
social dilemmas [25  26  27  28  29  30  31]. In one well-known model  agents are assumed to have
inequity-averse preferences [10]. They balance their selﬁsh desire for individual rewards against a
need to keep deviations between their own rewards and the rewards of others as small as possible.
Inequity-averse individuals are able to solve social dilemmas by resisting the temptation to pull ahead
of others or—if punishment is possible—by punishing and discouraging free-riding. The inequity
aversion model has been successfully applied to explain human behavior in a variety of laboratory
economic games  such as the ultimatum game  the dictator game  the gift exchange game  market
games  the trust game and public goods [32  33].‡
In this research  we generalize the inequity aversion model to Markov games  and show that it
resolves intertemporal social dilemmas. Crucial to our analysis will be the distinction between
disadvantageous inequity aversion (negative reward received by individuals who underperform
relative to others) and advantageous inequity aversion (negative reward received by individuals who
overperform relative to others). Colloquially  these may be thought of as reductionist models of envy
(disadvantageous inequity aversion) and guilt (advantageous inequity aversion) respectively [36]. We
hypothesise that these directly address the two challenges set out above in the following way.
Inequity aversion mitigates the problem of collective action by changing the effective payoff structure
experienced by agents through both a direct and an indirect mechanism. In the direct mechanism 
defectors experience advantageous inequity aversion  diminishing the marginal beneﬁt of defection
over cooperation. The indirect mechanism arises when cooperating agents are disadvantageous-
inequity averse. This motivates them to punish defectors by sanctioning them  reducing the payoff
incentive for free-riding. Since agents must learn a defecting strategy via exploration  initially
cooperative agents are deterred from switching strategies if the payoff bonus does not outweigh the
cost of inefﬁciently executing the defecting strategy while learning.
Inequity aversion also ameliorates the temporal credit assignment problem. Learning the association
between short-term actions and long-term consequences is a high-variance and error-prone process 
both for animals [37] and reinforcement learning algorithms [38]. Inequity aversion short-circuits
the need for such long-term temporal credit assignment by acting as an “early warning system” for
intertemporal social dilemmas. As before  both a direct and an indirect mechanism are at work.
With the direct mechanism  advantageous-inequity-averse defectors receive negative rewards in the
short-term  since the beneﬁts of defection are delivered on that timescale. The indirect mechanism
operates because cooperators experience disadvantageous inequity aversion at precisely the time
when other agents defect. This leads cooperators to punish defectors on a short-term timescale.
Both systems have the effect of operant conditioning [39]  incentivizing agents that cannot resolve
long-term uncertainty to act in the lasting interest of the group.

2 Reinforcement learning in sequential social dilemmas
2.1 Partially observable Markov games
We consider multi-agent reinforcement learning in partially-observable general-sum Markov games
[40  41]. In each game state  agents take actions based on a partial observation of the state space and

†For more detail on the motivations for our research program  see the supplementary information.
‡For alternative theories of the other-regarding preferences that may underlie human cooperative behavior in

economic games  see [34  35].

2

Figure 1: Screenshots from (A) the Cleanup game  (B) the Harvest game  (C) the Dictate apples
game  and (D) the Take apples and Give apples games. The size of the agent-centered observation
window is also shown in (B). The same size observation was used in all experiments.

receive an individual reward. Agents must learn through experience an appropriate behavior policy
while interacting with one another. We formalize this as follows.
Consider an N-player partially observable Markov game M deﬁned on a ﬁnite set of states S. The
observation function O : S × {1  . . .   N} → Rd speciﬁes each player’s d-dimensional view on
the state space. From each state  players may take actions from the set A1  . . .  AN (one for each
player). As a result of their joint action a1  . . .   aN ∈ A1  . . .  AN the state changes following
the stochastic transition function T : S × A1 × ··· × AN → ∆(S) (where ∆(S) denotes the set
of discrete probability distributions over S). Write Oi = {oi | s ∈ S  oi = O(s  i)} to indicate
the observation space of player i. Each player receives an individual extrinsic reward deﬁned as
ri : S × A1 × ··· × AN → R for player i.§
Each agent learns  independently through its own experience of the environment  a behavior policy
πi : Oi → ∆(Ai) (written π(ai|oi)) based on its own observation oi = O(s  i) and extrinsic reward
ri(s  a1  . . .   aN ). For the sake of simplicity we will write (cid:126)a = (a1  . . .   aN )  (cid:126)o = (o1  . . .   oN )
and (cid:126)π(.|(cid:126)o) = (π1(.|o1)  . . .   πN (.|oN )). Each agent’s goal is to maximize a long term γ-discounted
payoff deﬁned as follows:

(cid:35)

(cid:34) ∞(cid:88)

(cid:126)π(s0) = E
V i

γtri(st  (cid:126)at)|(cid:126)at ∼ (cid:126)πt  st+1 ∼ T (st  (cid:126)at)

.

(1)

t=0

2.2 Learning agents
We deploy asynchronous advantage actor-critic (A3C) as the learning algorithm for our agents [42].
A3C maintains both value (critic) and policy (actor) estimates using a deep neural network. The
policy is updated according to the policy gradient method  using a value estimate as a baseline to
reduce variance. Gradients are generated asynchronously by 24 independent copies of each agent 
playing simultaneously in distinct instantiations of the environment. Explicitly  the gradients are
∇θ log π(at|st; θ)A(st  at; θ  θv)  where A(st  at; θ  θv) is the advantage function  estimated via
i=0 γiut+i + γkV (st+k; θv) − V (st; θv) where ut+i is the subjective reward. In
section 3.1 we decompose this into an extrinsic reward from the environment and an intrinsic reward
that deﬁnes the agent’s inequity-aversion.

k-step backups (cid:80)k−1

Intertemporal social dilemmas

2.3
An intertemporal social dilemma is a temporally extended multi-agent game in which individual
short-term optimal strategies lead to poor long-term outcomes for the group. To deﬁne this term
§In our games  N = 5  d = 15 × 15 × 3 and |Ai| ranges from 8 to 10  with actions comprising movement 

rotation and ﬁring.

3

Figure 2: The public goods game (Cleanup) and the commons game (Harvest) are social dilemmas.
(A) shows the Schelling diagram for Cleanup. (B) shows the Schelling diagram for Harvest. The
dotted line shows the overall average return were the individual to choose defection.

precisely  we employ a formalization of empirical game theoretic analysis [43  44]. Our deﬁnition
is consistent with that of [17]. However  since that work was limited to the 2-player case  it relied
on the empirical payoff matrix to represent the relative values of cooperation and defection. This
quantity is unwieldy for N > 2 since it becomes a tensor. Therefore we base our deﬁnition on a
different representation of the N-player game. Explicitly  a Schelling diagram [45  18] depicts the
relative payoffs for a single cooperator or defector given a ﬁxed number of other cooperators. Thus
Schelling diagrams are a natural and convenient generalization of payoff matrices to multi-agent
settings. Game-theoretic properties like Nash equilibria are readily visible in Schelling diagrams; see
[45] for additional details and intuition.
An N-player sequential social dilemma is a tuple (M  Π = Πc (cid:116) Πd) of a Markov game and
two disjoint sets of policies  said to implement cooperation and defection respectively  satisfying
the following properties. Consider the strategy proﬁle (π1
d with
(cid:96) + m = N. We shall denote the average payoff for the cooperating policies by Rc((cid:96)) and for the
defecting policies by Rd((cid:96)). A Schelling diagram plots the curves Rc((cid:96) + 1) and Rd((cid:96)). Intuitively 
the diagram displays the two possible payoffs to the N th player given that (cid:96) of the remaining players
elect to cooperate and the rest defect. We say that (M  Π) is a sequential social dilemma iff the
following hold:

d ) ∈ Π(cid:96)

c × Πm

c   . . .   π(cid:96)

c  π1

d  . . .   πm

1. Mutual cooperation is preferred over mutual defection: Rc(N ) > Rd(0).
2. Mutual cooperation is preferred to being exploited by defectors: Rc(N ) > Rc(0).
3. Either the fear property  the greed property  or both:

• Fear: mutual defection is preferred to being exploited. Rd(i) > Rc(i) for sufﬁciently
• Greed: exploiting a cooperator is preferred to mutual cooperation. Rd(i) > Rc(i) for

small i.

sufﬁciently large i.

We show that the matrix games Stag Hunt  Chicken and Prisoner’s Dilemma satisfy these properties
in Supplementary Fig. 1.
A sequential social dilemma is intertemporal if the choice to defect is optimal in the short-term. More
precisely  consider an individual i and an arbitrary set of policies for the rest of the group. Given a
k ∈ Π with maximum return in the next k steps
starting state  for all k sufﬁciently small  the policy πi
is a defecting policy. There is thus a tension between short-term personal gain and long-term group
utility.

2.4 Examples
[46] divides all multi-person social dilemmas into two broad categories:

1. Public goods dilemmas  in which an individual must pay a personal cost in order to provide

a resource that is shared by all.

2. Commons dilemmas  in which an individual is tempted by a personal beneﬁt  depleting a

resource that is shared by all.

4

Figure 3: Advantageous inequity aversion facilitates cooperation in the Cleanup game. (A) compares
the collective return achieved by A3C and advantageous inequity averse agents  (B) shows contribu-
tions to the public good  and (C) shows equality over the course of training. (D-F) demonstrate that
disadvantageous inequity aversion does not promote greater cooperation in the Cleanup game.

We consider two dilemmas in this paper  one of the public goods type and one of the commons
type. Each was implemented as a partially observable Markov game on a 2D grid. Both are
also intertemporal social dilemmas because individually selﬁsh actions produce immediate beneﬁts
while their impacts on the collective develop over a longer time horizon. The availability of costly
punishment is of critical importance in human sequential social dilemmas [47  48] and is therefore an
action in the environments presented here.¶
In the Cleanup game  the aim is to collect apples from a ﬁeld. Each apple provides a reward of 1.
The spawning of apples is controlled by a geographically separate aquifer that supplies water and
nutrients. Over time  this aquifer ﬁlls up with waste  lowering the respawn rate of apples linearly. For
sufﬁciently high waste levels  no apples can spawn. At the start of each episode  the environment
resets with waste just beyond this saturation point. To cause apples to spawn  agents must clean some
of the waste.
Here we have a dilemma. Provided that some agents contribute to the public good by cleaning up the
aquifer  it is individually more rewarding to stay in the apple ﬁeld. However  if all players defect 
then no-one gets any reward. A successful group must balance the temptation to free-ride with the
provision of the public good. Cooperative agents must make a positive commitment to group-level
well-being to solve the task.
The goal of the Harvest game is to collect apples. Each apple provides a reward of 1. The apple
regrowth rate varies across the map  dependent on the spatial conﬁguration of uncollected apples: the
more nearby apples  the higher the local regrowth rate. If all apples in a local area are harvested then
none ever grow back. After 1000 steps the episode ends  at which point the game resets to an initial
state.
The dilemma is as follows. The short-term interests of each individual leads toward harvesting
as rapidly as possible. However  the long-term interests of the group as a whole are advanced if
individuals refrain from doing so  especially when many agents are in the same local region. Such
situations are precarious because the more harvesting agents there are  the greater the chance of
permanently depleting the local resources. Cooperators must abstain from a personal beneﬁt for the
good of the group.(cid:107)

timeout beam was used.

¶In both games  players can ﬁne each other using a punishment beam. This contrasts with [18]  in which a
(cid:107)Precise details of the ecological dynamics may be found in the supplementary information.

5

2.5 Validating the environments
We would like to demonstrate that these environments are social dilemmas by plotting Schelling dia-
grams. In complex  spatially and temporally extended Markov games  it is not feasible to analytically
determine cooperating and defecting policies. Instead  we must study the environment empirically.
One method employs reinforcement learning to train such policies. We enforce cooperation or
defection by making appropriate modiﬁcations to the environment  as follows.
In Harvest  we enforce cooperation by modifying the environment to prevent some agents from
gathering apples in low-density areas. In Cleanup  we enforce free-riding by removing the ability
of some agents to clean up waste. We also add a small group reward signal to encourage the
remaining agents to cooperate. The resulting empirical Schelling diagrams in Figure 2 prove that our
environments are indeed social dilemmas.

Figure 4: Inequity aversion promotes cooperation in the Harvest game. When all 5 agents have
advantageous inequity aversion  there is a small improvement over A3C in the three social outcome
metrics: (A) collective return  (B) apple consumption  and (C) sustainability. Disadvantageous
inequity aversion provides a much larger improvement over A3C  and works even when only 1
out of 5 agents are inequity averse. (D) shows collective return  (E) apple consumption  and (F)
sustainability.

3 The model
We ﬁrst introduce the inequity aversion model of [10]. It is directly applicable only to stateless games.
We then extend their model to sequential or multi-state problems  making use of deep reinforcement
learning.

Inequity aversion

3.1
The [10] utility function is as follows. Let r1  . . .   rN be the extrinsic payoffs achieved by each of N
players. Each agent receives a utility
Ui(ri  . . . rN ) = ri − αi
N − 1

max (rj − ri  0) − βi

max (ri − rj  0)  

(2)

(cid:88)

j(cid:54)=i

(cid:88)

j(cid:54)=i

N − 1

where the additional terms may be interpreted as intrinsic payoffs  in the language of [49].
The parameter αi controls an agent’s aversion to disadvantageous inequity. A larger value for αi
implies a larger utility loss when other agents achieve rewards greater than one’s own. Likewise  the
parameter βi controls an agent’s aversion to advantageous inequity  utility lost when performing
better than others. [10] argue that α > β. That is  most people are loss averse in social comparisons.
There is some empirical support for this prediction [50]  though the evidence is mixed [51  52]. In a
sweep over values for α and β  we found our strongest results for α = 5 and β = 0.05.

6

Figure 5: Inequity aversion promotes cooperation by improving temporal credit assignment. (A)
shows collective return for delayed advantageous inequity aversion in the Cleanup game. (B) shows
apple consumption for delayed disadvantageous inequity aversion in the Harvest game.

Inequity aversion in sequential dilemmas

3.2
Experimental work in behavioral economics suggests that some proportion of natural human pop-
ulations are inequity averse [8]. However  as a computational model  inequity aversion has only
been expounded for the matrix game setting. Equation (2) can be directly applied only to stateless
games [53  54]. In this section we extend this model of inequity aversion to the temporally extended
Markov game case.
The main problem in re-deﬁning the social preference of equation (2) for Markov games is that the
rewards of different players may occur on different timesteps. Thus the key step in extending (2) to
this case is to introduce per-player temporal smoothing of the reward traces.
Let ri(s  a) denote the reward obtained by the i-th player when it takes action a from state s. For
convenience  we also sometimes write it with a time index: rt
i := ri(st  at). We deﬁne the subjective
reward ui(s  a) received by the i-th player when it takes action a from state s to be

ui(st

i  at

i) = ri(st

i  at

i) − αi
N − 1

max(et

j(st

j  at

j) − et

i(st

i  at

i)  0)

(3)

(cid:88)
(cid:88)

j(cid:54)=i

− βi

j(st

max(et

N − 1
j for the agents j = 1  . . .   N are updated at each timestep t

j)  0)  

j(cid:54)=i

j  at

i  at

j(st

i) − et

where the temporal smoothed rewards et
according to

(4)
where γ is the discount factor and λ is a hyperparameter. This is analogous to the mathematical
formalism used for eligibility traces [55]. Furthermore  we allow agents to observe the smoothed
reward of every player on each timestep.

) + rt

et
j(st

j  at

j  at

j(st

j)  

j

j

j

j) = γλet−1

(st−1

  at−1

4 Results
We show that advantageous inequity aversion is able to resolve certain intertemporal social dilem-
mas without resorting to punishment by providing a temporally correct intrinsic reward. For this
mechanism to be effective  the population must have sufﬁciently many advantageous-inequity-averse
individuals. By contrast disadvantageous-inequity-averse agents can drive mutual cooperation even
in small numbers. They achieve this by punishing defectors at a time concomitant with their offences.
In addition  we ﬁnd that advantageous inequity aversion is particularly effective for resolving public
goods dilemmas  whereas disadvantageous inequity aversion is more powerful for addressing com-
mons dilemmas. Our baseline A3C agent fails to ﬁnd socially beneﬁcial outcomes in either category
of game. We deﬁne the metrics used to quantify our results in the supplementary information.

4.1 Advantageous inequity aversion promotes cooperation
Advantageous-inequity-averse agents are better than A3C at maintaining cooperation in both public
goods and commons games. This effect is particularly pronounced in the Cleanup game (Figure 3).

7

Here groups of 5 advantageous-inequity-averse agents ﬁnd solutions in which 2 consistently clean
large amounts of waste  producing a large collective return.∗∗ We clarify the effect of advantageous
inequity aversion on the intertemporal nature of the problem by delaying the delivery of the intrinsic
reward signal. Figure 5 suggests that improving temporal credit assignment is an important function
of inequity aversion since delaying the time at which the intrinsic reward signal is delivered removes
its beneﬁcial effect.

4.2 Disadvantageous inequity aversion promotes cooperation
Disadvantageous-inequity-averse agents are better than A3C at maintaining cooperation via pun-
ishment in commons games (Figure 4). In particular  a single disadvantageous-averse agent can
ﬁne defectors  generating a sustainable outcome.†† In Figure 5  we see that the disadvantageous-
inequity-aversion signal must be temporally aligned with over-consumption for effective policing
to arise. Hence  it is plausible that inequity aversion bridges the temporal gap between short-term
incentives and long-term outcomes. Disadvantageous inequity aversion has no such positive impact
in the Cleanup game  for reasons that we discuss in section 5.

5 Discussion
In the Cleanup game  advantageous inequity aversion is an unambiguous feedback signal: it en-
courages agents to contribute to the public good. In the direct pathway  trial and error will quickly
discover that the fastest way to diminish the negative rewards arising from advantageous inequity
aversion is to clean up waste  since doing so creates more apples for others to consume. However the
indirect mechanism of disadvantageous inequity aversion and punishment lacks this property; while
punishment may help exploration of new policies  it does not directly increase the attractiveness of
waste cleaning.
The Harvest game requires passive abstention rather than active provision. In this setting  advan-
tageous inequity aversion provides a noisy signal for sustainable behaviour. This is because it is
sensitive to the precise apple conﬁguration in the environment  which changes rapidly over time.
Hence advantageous inequity aversion does not greatly aid the exploration of policy space. Pun-
ishment  on the other hand  operates as a valuable shaping reward for learning  dis-incentivizing
overconsumption at precisely the correct time and place.
In the Harvest game  disadvantageous inequity aversion generates cooperation in a grossly inefﬁ-
cient manner: huge amounts of collective resource are lost to ﬁnes (compare Figures 4D and 4E).
This parallels human behavior in laboratory matrix games  e.g. [56  57]. In the Cleanup game 
advantageous-inequity averse agents resolve the social dilemma without such losses  but must com-
prise a large proportion of the population to be successful. This mirrors the cultural modulation of
advantageous inequity aversion in humans [58]. Evolution is hypothesized to have favored fairness
as a mechanism for continued human cooperation [59]. It remains to be seen whether emergent
inequity-aversion can be obtained by evolving reinforcement learning agents.
We conclude by putting our approach in the context of prior work. Since our mechanism does not
require explicitly training cooperating and defecting agents or modelling their behaviour  it scales
more easily to complex environments and large populations of agents. However  our method has
several limitations. Firstly  our guilty agents are quite exploitable  as evidenced by the necessity of a
homogeneous guilty population to achieve cooperation. Secondly  our agents use outcomes rather
than predictions to inform their policies. This is known to be a problem in environments with high
stochasticity [22]. Finally  the heterogeneity of the population is an additional hyperparameter in our
model. Clearly  one must set this appropriately  particularly in games with asymmetric outcomes. It
is likely that a hybrid approach will be required to solve these challenging issues at scale.

∗∗For a video of this behavior  visit https://youtu.be/N8BUzzFx7uQ.
††For a video of this behavior  visit https://youtu.be/tz3ZpTTmxTk.

8

References
[1] M. Olson  The logic of collective action. Harvard University Press  1965.
[2] G. Hardin  “The tragedy of the commons ” Science  vol. 162  no. 3859  pp. 1243–1248  1968.
[3] E. Ostrom  Governing the Commons: The Evolution of Institutions for Collective Action.

Cambridge University Press  1990.

[4] T. Dietz  E. Ostrom  and P. C. Stern  “The struggle to govern the commons ” science  vol. 302 

no. 5652  pp. 1907–1912  2003.

[5] E. Ostrom  J. Walker  and R. Gardner  “Covenants with and without a sword: Self-governance

is possible. ” American political science Review  vol. 86  no. 02  pp. 404–417  1992.

[6] E. Fehr and S. Gächter  “Altruistic punishment in humans ” Nature  vol. 415  no. 6868  p. 137 

2002.

[7] E. Ostrom  “A behavioral approach to the rational choice theory of collective action: Presidential
address  american political science association  1997 ” American political science review  vol. 92 
no. 1  pp. 1–22  1998.

[8] E. Fehr and H. Gintis  “Human motivation and social cooperation: Experimental and analytical

foundations ” Annu. Rev. Sociol.  vol. 33  pp. 43–64  2007.

[9] D. G. Rand and M. A. Nowak  “Human cooperation ” Trends in cognitive sciences  vol. 17 

no. 8  pp. 413–425  2013.

[10] E. Fehr and K. M. Schmidt  “A theory of fairness  competition  and cooperation ” The quarterly

journal of economics  vol. 114  no. 3  pp. 817–868  1999.

[11] A. Falk and U. Fischbacher  “A theory of reciprocity ” Games and economic behavior  vol. 54 

no. 2  pp. 293–315  2006.

[12] T. W. Sandholm and R. H. Crites  “Multiagent reinforcement learning in the iterated prisoner’s

dilemma ” Biosystems  vol. 37  no. 1-2  pp. 147–166  1996.

[13] E. Munoz de Cote  A. Lazaric  and M. Restelli  “Learning to cooperate in multi-agent social
dilemmas ” in 5th International Joint Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2006)  pp. 783–790  2006.

[14] M. A. Janssen  R. Holahan  A. Lee  and E. Ostrom  “Lab experiments for the study of social-

ecological systems ” Science  vol. 328  no. 5978  pp. 613–617  2010.

[15] M. Janssen  “Introducing ecological dynamics into common-pool resource experiments ” Ecol-

ogy and Society  vol. 15  no. 2  2010.

[16] M. Janssen  “The role of information in governing the commons: experimental results ” Ecology

and Society  vol. 18  no. 4  2013.

[17] J. Z. Leibo  V. Zambaldi  M. Lanctot  J. Marecki  and T. Graepel  “Multi-agent Reinforcement
Learning in Sequential Social Dilemmas ” in Proceedings of the 16th International Conference
on Autonomous Agents and Multiagent Systems (AA-MAS 2017)  (Sao Paulo  Brazil)  2017.

[18] J. Perolat  J. Z. Leibo  V. Zambaldi  C. Beattie  K. Tuyls  and T. Graepel  “A multi-agent
reinforcement learning model of common-pool resource appropriation ” in Advances in Neural
Information Processing Systems (NIPS)  (Long Beach  CA)  2017.

[19] J. N. Foerster  R. Y. Chen  M. Al-Shedivat  S. Whiteson  P. Abbeel  and I. Mordatch  “Learning

with opponent-learning awareness ” arXiv preprint arXiv:1709.04326  2017.

[20] A. Lerer and A. Peysakhovich  “Maintaining cooperation in complex social dilemmas using

deep reinforcement learning ” arXiv preprint arXiv:1707.01068  2017.

[21] A. Peysakhovich and A. Lerer  “Prosocial learning agents solve generalized stag hunts better

than selﬁsh ones ” arXiv preprint arXiv:1709.02865  2017.

[22] A. Peysakhovich and A. Lerer  “Consequentialist conditional cooperation in social dilemmas

with imperfect information ” CoRR  vol. abs/1710.06975  2017.

[23] M. Kleiman-Weiner  M. K. Ho  J. L. Austerweil  M. L. Littman  and J. B. Tenenbaum  “Co-
ordinate to cooperate or compete: Abstract goals and joint intentions in social interaction ” in
CogSci  2016.

9

[24] P. Sunehag  G. Lever  A. Gruslys  W. M. Czarnecki  V. F. Zambaldi  M. Jaderberg  M. Lanc-
tot  N. Sonnerat  J. Z. Leibo  K. Tuyls  and T. Graepel  “Value-decomposition networks for
cooperative multi-agent learning ” CoRR  vol. abs/1706.05296  2017.

[25] J. J. Rousseau  Discourse on the Origin of Inequality. Marc-Michel Rey  1755.
[26] H. L. A. Hart  “Are there any natural rights? ” The Philosophical Review  vol. 64  no. 2 

pp. 175–191  1955.

[27] J. Rawls  “Justice as fairness ” The philosophical review  vol. 67  no. 2  pp. 164–194  1958.
[28] G. Klosko  “The principle of fairness and political obligation ” Ethics  vol. 97  no. 2  pp. 353–

362  1987.

[29] B. S. Frey and I. Bohnet  “Institutions Affect Fairness: Experimental Investigations ” Journal of

Institutional and Theoretical Economics  vol. 151  pp. 286–303  June 1995.

[30] C. Bicchieri and A. Chavez  “Behaving as expected: Public information and fairness norms ” J.

Behav. Decis. Making  vol. 23  pp. 161–178  Apr. 2010.

[31] J. Henrich  J. Ensminger  R. McElreath  A. Barr  C. Barrett  et al.  “Markets  Religion  Commu-
nity Size  and the Evolution of Fairness and Punishment ” Science  vol. 327  pp. 1480–1484 
Mar. 2010.

[32] R. Gibbons  A primer in game theory. Harvester Wheatsheaf  1992.
[33] C. Eckel and H. Gintis  “Blaming the messenger: Notes on the current state of experimental
economics ” Journal of Economic Behavior & Organization  vol. 73  no. 1  pp. 109–119  2010.
[34] G. Charness and M. Rabin  “Understanding social preferences with simple tests ” The Quarterly

Journal of Economics  vol. 117  no. 3  pp. 817–869  2002.

[35] D. Engelmann and M. Strobel  “Inequality aversion  efﬁciency  and maximin preferences in
simple distribution experiments ” American economic review  vol. 94  no. 4  pp. 857–869  2004.

[36] C. Camerer  Behavioral Game Theory: Experiments in Strategic Interaction. 01 2011.
[37] G. R. Grice  “The relation of secondary reinforcement to delayed reward in visual discrimination

learning. ” Journal of Experimental Psychology  vol. 38  no. 1  pp. 1–16  1948.

[38] M. J. Kearns and S. P. Singh  “Bias-variance error bounds for temporal difference updates ” in
Proceedings of the Thirteenth Annual Conference on Computational Learning Theory  COLT
’00  (San Francisco  CA  USA)  pp. 142–147  Morgan Kaufmann Publishers Inc.  2000.

[39] B. F. Skinner  The Behavior of Organisms; An Experimental Analysis. D. Appleton-Century

Company  1938.

[40] L. S. Shapley  “Stochastic Games ” In Proc. of the National Academy of Sciences of the United

States of America  1953.

[41] M. L. Littman  “Markov games as a framework for multi-agent reinforcement learning ” in
Proceedings of the 11th International Conference on Machine Learning (ICML)  pp. 157–163 
1994.

[42] V. Mnih  A. P. Badia  M. Mirza  A. Graves  T. P. Lillicrap  T. Harley  D. Silver  and
K. Kavukcuoglu  “Asynchronous methods for deep reinforcement learning ” in Proceedings of
the 33nd International Conference on Machine Learning  ICML 2016  New York City  NY  USA 
June 19-24  2016  pp. 1928–1937  2016.

[43] W. E. Walsh  R. Das  G. Tesauro  and J. O. Kephart  “Analyzing complex strategic interactions
in multi-agent systems ” in AAAI-02 Workshop on Game-Theoretic and Decision-Theoretic
Agents  pp. 109–118  2002.

[44] M. P. Wellman  “Methods for empirical game-theoretic analysis ” in Proceedings of the national
conference on artiﬁcial intelligence  vol. 21  p. 1552  Menlo Park  CA; Cambridge  MA;
London; AAAI Press; MIT Press; 1999  2006.

[45] T. C. Schelling  “Hockey helmets  concealed weapons  and daylight saving: A study of binary
choices with externalities ” The Journal of Conﬂict Resolution  vol. 17  no. 3  pp. 381–428 
1973.

[46] P. Kollock  “Social dilemmas: The anatomy of cooperation ” Annual review of sociology  vol. 24 

no. 1  pp. 183–214  1998.

10

[47] P. Oliver  “Rewards and punishments as selective incentives for collective action: theoretical

investigations ” American journal of sociology  vol. 85  no. 6  pp. 1356–1375  1980.

[48] Ö. Gürerk  B. Irlenbusch  and B. Rockenbach  “The competitive advantage of sanctioning

institutions ” Science  vol. 312  no. 5770  pp. 108–111  2006.

[49] N. Chentanez  A. G. Barto  and S. P. Singh  “Intrinsically motivated reinforcement learning ” in

Advances in neural information processing systems  pp. 1281–1288  2005.

[50] G. F. Loewenstein  L. Thompson  and M. H. Bazerman  “Social utility and decision making in
interpersonal contexts. ” Journal of Personality and Social psychology  vol. 57  no. 3  p. 426 
1989.

[51] C. Bellemare  S. Kröger  and A. Van Soest  “Measuring inequity aversion in a heterogeneous
population using experimental decisions and subjective probabilities ” Econometrica  vol. 76 
no. 4  pp. 815–839  2008.

[52] E. I. Hoppe and P. W. Schmitz  “Contracting under incomplete information and social prefer-
ences: An experimental study ” The Review of Economic Studies  vol. 80  no. 4  pp. 1516–1544 
2013.

[53] K. Verbeeck  J. Parent  and A. Nowé  “Homo egualis reinforcement learning agents for load
balancing ” in Innovative Concepts for Agent-Based Systems  First International Workshop on
Radical Agent Concepts  WRAC 2002  McLean  VA  USA  January 16-18  2002  Revised Papers 
pp. 81–91  2002.

[54] S. de Jong and K. Tuyls  “Human-inspired computational fairness ” Autonomous Agents and

Multi-Agent Systems  vol. 22  no. 1  pp. 103–126  2011.

[55] R. S. Sutton and A. G. Barto  Reinforcement Learning: An Introduction. Cambridge  MA  USA:

MIT Press  1st ed.  1998.

[56] T. Yamagishi  “The provision of a sanctioning system as a public good ” vol. 51  pp. 110–116 

07 1986.

[57] E. Fehr and S. Gachter  “Cooperation and punishment in public goods experiments ” American

Economic Review  vol. 90  pp. 980–994  September 2000.

[58] R. P. Blake  K. Mcauliffe  J. Corbit  T. Callaghan  O. Barry  A. Bowie  L. Kleutsch  K. Kramer 
E. Ross  H. Vongsachang  R. Wrangham  and F. Warneken  “The ontogeny of fairness in seven
societies ” vol. 528  11 2015.

[59] S. F. Brosnan and F. B. M. de Waal  “Evolution of responses to (un)fairness ” Science (New

York  N.Y.)  vol. 346  no. 6207  p. 1251776  2014.

11

,Vasilis Syrgkanis
Alekh Agarwal
Haipeng Luo
Robert Schapire
Brian Dolhansky
Jeff Bilmes
Ben London
Edward Hughes
Joel Leibo
Matthew Phillips
Karl Tuyls
Edgar Dueñez-Guzman
Antonio García Castañeda
Iain Dunning
Tina Zhu
Kevin McKee
Raphael Koster
Heather Roff
Thore Graepel
Juyeon Heo
Sunghwan Joo
Taesup Moon