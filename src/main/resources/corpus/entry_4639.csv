2019,Regret Minimization for Reinforcement Learning with Vectorial Feedback and Complex Objectives,We consider an agent who is involved in an online Markov decision process  and receives a vector of outcomes every round. The agent aims to simultaneously  optimize multiple objectives associated with the multi-dimensional outcomes. Due to state transitions  it is challenging to balance the vectorial outcomes for achieving  near-optimality. In particular  contrary to the single objective case  stationary policies are generally sub-optimal. We propose a no-regret algorithm based on the  Frank-Wolfe algorithm (Frank and Wolfe 1956)  UCRL2 (Jaksch et al. 2010)  as well as a crucial and novel gradient threshold procedure. The procedure involves carefully delaying gradient updates  and returns a non-stationary policy that diversifies the outcomes for optimizing the objectives.,Regret Minimization for Reinforcement Learning
with Vectorial Feedback and Complex Objectives

Department of Industrial Systems Engineering and Management

Wang Chi Cheung

National University of Singapore

isecwc@nus.edu.sg

Abstract

We consider an agent who is involved in an online Markov Decision Process  and
receives a vector of outcomes every round. The agent optimizes an aggregate
reward function on the multi-dimensional outcomes. Due to state transitions  it
is challenging to balance the contribution from each dimension for achieving
near-optimality. Contrary to the single objective case  stationary policies are
generally sub-optimal. We propose a no-regret algorithm based on the Frank-Wolfe
algorithm (Frank and Wolfe 1956  Agrawal and Devanur 2014)   UCRL2 (Jaksch
et al. 2010)  as well as a crucial and novel Gradient Threshold Procedure (GTP).
GTP involves carefully delaying gradient updates  and returns a non-stationary
policy that diversiﬁes the outcomes for optimizing the aggregate reward.

1

Introduction

Markov Decision Processes (MDPs) model sequential optimization problems with changes in the
state of the underlying environment. At each time  an agent performs an action  contingent upon the
current state. Inﬂuenced by the present state and action  the agent transits to another state and receives
some form of feedback. Typically  the feedback is a scalar reward  and the agent aims to maximize
the total reward. Nevertheless  in many settings  the feedback is a vector of multiple outcomes  and
the agent’s goal depend on each of these outcomes. Moreover  the underlying MDP model is usually
not known to the agent  and is to be learned on-the-ﬂy. Motivated by these situations  we consider the
Complex-Objective Online MDP (CO-OMDP) problem  which maximizes an aggregate function on
the average vectorial outcome.
Solving the CO-OMDP problem requires overcoming the following subtle challenges. To maximize
the aggregate function  an agent has to balance the contributions from the outcomes’ different
components by alternating among different actions  which are generally associated with different
states. Consequently  the agent has to traverse the state space  which could require visiting sub-
optimal states that do not contribute to the maximization of the aggregate function. Altogether  the
maximization can be hindered by undesirable state transitions  which is worsened by the agent’s
model uncertainty.
We overcome the mentioned challenges by proposing TFW-UCRL2  a near-optimal online algorithm
for the CO-OMDP problem. The algorithm is built upon the Frank-Wolfe algorithm (FW) [21  2] 
UCRL2 [28]  as well as our novel Gradient Threshold Procedure (GTP). FW balances the objectives
by scalarizing the outcomes  and UCRL2 solves scalarized online MDP problems under model
uncertainty. However  FW and UCRL2 are not enough for overcoming the challenges in balancing the
outcomes while avoiding sub-optimal states. GTP overcomes the challenges by judiciously delaying
the gradient updates in FW. The procedure approximately maintains the balancing effect by FW  while
limits the visits to sub-optimal states by switching among different stationary policies adaptively and
infrequently  despite the model uncertainty.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Related Literature. The CO-OMDP problem is closely related to the Bandits with global concave
Rewards problem (BwR) [2] and the Scalar-Obejctive Online MDPs problem (SO-OMDP) [28].
BwR concerns maximizing aggregate functions on vectorial feedback in stochastic bandit settings.
BwR is ﬁrst studied by [2]  who solve BwR by a synergy of online convex optimization and upper
conﬁdence bound (UCB) algorithms. Subsequently  BwR is studied under various stochastic bandit
settings and reward functions [4  17  14]. BwR is closely related to Bandit with Knapsacks problem
(BwK)  which models stochastic bandit problems with resource constraints. BwK is ﬁrst studied
by [10]  and is subsequently studied in various settings [2  11  3  20]. The models for BwR and
BwK assume i.i.d. outcomes across time  while in an MDP setting the outcome distribution changes
endogenously according to the state transition.
The adversarial BwK problem is recently studied in [27]. Among other results  they show that no
online algorithm can achieve an expected value of Ω(1/ log T ) times the ofﬂine optimum  where T is
the number of time steps. Our positive results on CO-OMDPs walk a ﬁne line between the negative
results for adversarial BwK and the positive results for stochastic BwR and BwK. Online optimization
problems with global reward functions are studied in adversarial settings with full feedback [19  9].
The SO-OMDP problem is ﬁrst studied by [8  28] for communicating MDPs. Subsequently  the
problem is studied under the more general cases of weakly communicating MDPs [13  23] and
non-communicating MDPs [22]. Posterior sampling algorithms for the SO-OMDP problem are
proposed and analyzed [5  37]. The SO-OMDP problem is also studied under certain mixing time
assumptions on all stationary policies [36]. The SO-OMDP problem assumes scalar rewards  but do
not incorporate multi-objective optimization. For a review on MDPs  please consult [38  15].
Reinforcement Learning (RL) with vectorial feedback and aggregate functions are studied in the
discounted-reward setting [24  6  12  43  1  25  42  29  30  33] and the average-reward [6  32  41  40]
setting. We study the latter with an online model under model uncertainty. Our work shows non-
asymptotic convergence to the optimum  which differs from [6  32] who show asymptotic convergence.
Tarbouriech and Lazaric [41] study an online model for state space exploration  and achieve a non-
asymptotic convergence to the optimum. In addition to the choice of the aggregate functions  our work
differs from [41] in two aspects. First  the transition kernel is assumed to be known in [41]  whereas
the kernel is not known in our model. Second  the reachability assumption of unichain MDPs is
made in [41]  while we make the much weaker assumption of communicating MDPs. More recently 
online MDPs with adversarially chosen aggregate functions are studied by [40]. The model in [40] is
episodic  where the state is reset to a ﬁxed state at the end of an episode (involving a ﬁxed number
of steps). In contrast  our setting does not involve any state reset. In [40] the aggregate function is
applied only on the trajectory in each episode  whereas in our setting the aggregate function is applied
on the trajectory across the whole horizon. Finally  we point out that a substantial generalization of
the current paper has been put forth in [18].
In the discounted reward setting  multi-objective optimization are studied in [24  6  12  43]. Many
recent works study the discounted-reward setting with resource constraints [1  42  29  33]. Numerous
recent research works focus on state space exploration problems [25  30] in the discounted-reward
setting. Constrained MDPs are reviewed in [6]  and multi-objective RL is surveyed in [39  31].

2 Problem Deﬁnition of CO-OMDP
A CO-OMDP instance is speciﬁed as (S  s1 A  p V  g). The set S is a ﬁnite state space  and s1 ∈ S
is the starting state. The collection A = {As}s∈S contains a ﬁnite set of actions As for each state s.
We say (s  a) is a state-action pair iff s ∈ S  a ∈ As. The collections p = {p(·|s  a)}s∈S a∈As is the
transition kernel  and the collection V = {V(s  a)}s∈S a∈As governs the vectorial outcomes. When
the agent chooses action a ∈ As at state s  her subsequent state s(cid:48) is distributed as p(·|s  a) ∈ ∆S.
She receives a stochastic vectorial outcome V (s  a) ∈ [0  1]K  distributed as V(s  a)  and has
k=1. We emphasize that s(cid:48)  V1(s  a)  . . .   VK(s  a) can be
mean E[V (s  a)] = v(s  a) = (vk(s  a))K
arbitrarily correlated. We focus on the following reward function g : [0  1]K → R≥0  which is
parameterized by L0 ∈ R≥0  L1  . . .   LK ∈ R  and a convex compact set U ⊆ [0  1]K:

g(w) :=

·

1
K

Lkwk − L0
2

min
u∈U

(wk − uk)2

.

(1)

(cid:34) K(cid:88)

k=1

(cid:41)(cid:35)

(cid:40) K(cid:88)

k=1

The function g is concave (see Appendix B.1)  and is to be maximized.

2

Dynamics. An agent  who faces an CO-OMDP instance M = (S  s1 A  p V  g)  starts at state
s1 ∈ S. At time t  three events happen. First  the agent observes his current state st. Second 
she takes an action at ∈ Ast. Third  she transits to another state st+1 ∼ p(·|st  at)  and receives
the vectorial outocme Vt(st  at) ∼ V(st  at). Both st+1 and Vt(st  at) are observed by the agent.
The whole dynamics result in a controlled Markov process {st  at  Vt(st  at)}∞
t=1. Conditioned on
(st  at)  the random variable pair (st+1  Vt(st  at)) is independent of Ht−1.
In the second event  the choice of at is based on a non-anticipatory policy. The choice only depends
on the current state st and the previous observations Ht−1 := {sq  aq  Vq(sq  aq)}t−1
q=1. When at only
depends on st  but not on Ht−1  the corresponding non-anticipatory policy is said to be stationary.
Objective. The CO-OMDP instance M is latent. While the agent knows S  s1 A  g  she does not
know v  p. To state the objective  deﬁne ¯V1:t := 1
q=1 Vq(sq  aq). For any horizon T not known a
t
priori  the agent aims to maximize g( ¯V1:T )  by selecting actions a1  . . .   aT with a non-anticipatory
policy. Denote ¯V1:T k as the k-component of the time average vector ¯V1:T . CO-OMDPs capture the
following problems:

Multi-Objective Optimization. Consider maximizing the scalar function(cid:80)K

(cid:80)t

s∈S (s −(cid:80)T

squared error(cid:80)

k=1 Lk ¯V1:T k  while
trying to meet the Key Performance Index (KPI) requirement ¯V1:T k ≥ ρk for each k ∈ {1  . . .   K}.
k=1 ∈ [0  1]K comprises the pre-determined KPI targets for the K objectives
The vector ρ = (ρk)K
{ ¯V1:T k}K
k=1. The task can be modelled as a CO-OMDP problem  by setting L0 ≥ 0  and U = {w :
wk ≥ ρk ∀1 ≤ k ≤ K}. By putting ρk = 1 and Lk = 0 for each k  any maximizer of g( ¯V1:T ) is
Pareto-optimal for the simultaneous maximization of ¯V1:T 1  . . .   ¯V1:T K. The Pareto optimality still
holds when we replace the inequality wk ≥ 1 with wk ≥ ρUB
k that bounds the average
¯V1:T k for any policy from above.
State Space Exploration. Consider visiting each state s with empirical frequency as close as possible
to a target frequency s in T time steps  where  = {s}s∈S ∈ ∆S. The task can be phrased as a
CO-OMDP problem. For each state-action pair (s  a)  we deﬁne V (s  a) ∈ {0  1}S as the standard
basis vector for s in RS  with value 1 at the s-coordinate and value 0 at the others. In addition  set
L0 = 1  L1 = . . . = LK = 0  U = {}. Maximizing g( ¯V1:T ) is equivalent to minimizing the mean
t=1 1st=s/T )2. To generalize  we can consider visiting certain subsets
(not necessarily disjoint or covering) of S with some target frequencies.
Finally  when we specialize the CO-OMDP problem with L0 = 0  we recover the SO-OMDP problem
[28]. If we specialize with S = {s}  we recover the BwR problem [2] with reward function g.
Reachability of M. To learn the latent model  the agent has to travel among states. For any s  s(cid:48) ∈ S
and any stationary policy π  we deﬁne the travel time from s to s(cid:48) under π as the random variable
Λ(s(cid:48)|π  s) := min{t : st+1 = s(cid:48)  s1 = s  sτ +1 ∼ p(·|sτ   π(sτ )) ∀τ}. We assume the following:
Assumption 2.1. The latent CO-OMDP instance M is communicating  that is  the quantity D :=
maxs s(cid:48)∈S minstationary π E[Λ(s(cid:48)|π  s)] is ﬁnite. We call D the diameter of M.
The same reachability assumption is made in [28]. Since the instance M is latent  the corresponding
diameter D is also not known to the agent. Assumption 2.1 is weaker than the unichain assumption
[6  32  41]  where every stationary policy induces a single recurrent class on S.
Ofﬂine Benchmark and Regret. To measure the effectiveness of a policy  we rephrase the agent’s
objective as the minimization of regret: Reg(T ) := opt(PM) − g( ¯V1:T ). The ofﬂine benchmark
opt(PM) is the optimum of the convex optimization problem (P(g))  which serves as a ﬂuid relaxation
[38  6] to the CO-OMDP problem.

k   for any ρUB

 (cid:88)

s∈S a∈As



v(s  a)x(s  a)

(cid:88)

(PM): max

g

x

s.t. (cid:88)
(cid:88)

a∈As

s∈S a∈As
x(s  a) ≥ 0

x(s  a) =

s(cid:48)∈S a(cid:48)∈As(cid:48)

x(s  a) = 1

p(s|s(cid:48)  a(cid:48))x(s(cid:48)  a(cid:48)) ∀s ∈ S

∀s ∈ S  a ∈ As

3

(2a)

(2b)

(2c)

In (PM)  the variables {x(s  a)}s a form a probability distribution over the state-action pairs. The
set of constraints (2a) requires the rates of transiting into and out of each state s to be equal.
We aim to design a non-anticipatory policy with an anytime regret bound Reg(T ) = O(1/T α) for
some α > 0. That is  for all δ > 0  there exist constants c  C (which only depend on K  S  A  g  δ) 
so that the policy satisﬁes Reg(T ) ≤ c/T α for all T ≥ C with probability at least 1 − δ. Achieving
Reg(T ) = O(1/T α) for some α > 0 implies achieving near-optimality  since opt(PM) differs from
the expected optimum only by an additive error of O( ¯LD/T )  by a similar reasoning to [28] (see
[18] for details).

3 Challenges of CO-OMDP  and Algorithm TFW-UCRL2

We ﬁrst discuss some unique challenges in the CO-OMDP  then present and discuss TFW-UCRL2 in
Algorithm 1. Finally  we present the regret bound for TFW-UCRL2.
Challenges. We begin by describing some unique challenges in CO-OMDP hinted in the Introduction.
Consider the three instances in Fig 1. An arc from state s to s(cid:48) represents action a with p(s(cid:48)|s  a) = 1 
and is labelled with its outcome V (s  a)  which is deterministic. Let’s focus on Figs 1a  1b. The
common objective requires balancing the 2-dimensional outcomes by visiting the left loop (ll) and
the right loop (rl) with frequency 0.5 each. In Fig 1a  the agent incurs a O(1/T ) regret by choosing
ll once  then rl once  then ll once  and so on.

(cid:0)1
(cid:1)

(cid:0)0
(cid:1)
Figure 1: Instances  with opt. actions bolded. Insts (1a  1b) have g(w) = −(cid:80)2

(b) CO-OMDP

(cid:0)1
(cid:1)

(a) BwR

(cid:0)0

s0

(cid:1)

s0

1

1

0

s1

k=1(wk − 0.5)2/2.

(c) SO-OMDP

(cid:0)0
(cid:0)0

(cid:1)
(cid:1)

(cid:0)0
(cid:0)0

0

(cid:1)
(cid:1)

0

0

0

0

s1

0

s2

1

s2

s0

1

0

1

0

However  if the agent visits ll once  then rl once  then ll once  and so on in Fig 1b  she suffers
Reg(T ) = Ω(1). Indeed  she spends two third of the time at the actions with the ‘sub-optimal’ state
s0  resulting in ¯V1:T ≈ (1/6  1/6)(cid:62) for large T . While the agent should visit each loop multiple times
before going to state s0 and then another loop  the length of stay at each loop is not a priori clear.
Our Gradient Threshold Procedure (GTP) provides a principled way for determining these lengths 
and GTP generalizes to other communicating MDPs. Finally  such a subtlety in state transitions does
not occur in Fig 1c or generally in communicating SO-OMDP instances  where the agent achieves
near-optimality by remaining in a single recurrent class.
TFW-UCRL2 runs in episodes. Episode m starts at the beginning of time τ (m) and ends at the end
of time τ (m + 1) − 1. During episode m  the agent follows a certain stationary policy ˜πm. The start
times {τ (m)}∞
m=1 are decided adaptively. We maintain conﬁdence regions
m = {H v
H v

m(s  a)}s a on the latent v  p across episodes  by ﬁrst deﬁning

m=1 and policies {˜πm}∞

m(s  a)}s a  H p

m = {H p
τ (m)−1(cid:88)

t=1

τ (m)−1(cid:88)

τ (m)−1(cid:88)

4

Nm(s  a) =

1(st at)=(s a)  N +

The estimates and conﬁdence regions for v are:

m(s  a) = max{1  Nm(s  a)}.
(cid:32)(cid:115)

1

ˆvm(s  a) :=

m(s  a) :=(cid:8)¯v ∈ [0  1]K : |¯vk − ˆvm k(s  a)| ≤ radv

Vt(st  at)1(st at)=(s a) 

m k(s  a) = ˜O

m k(s  a) ∀k ∈ [K](cid:9) .

m(s  a)

radv

N +

t=1

H v

ˆvm k(s  a)
N +
m(s  a)

The estimates and conﬁdence regions for p are:

1

ˆpm(s(cid:48)|s  a) :=

m(s  a) :=(cid:8)¯p ∈ ∆S : |¯p(s(cid:48)) − ˆpm(s(cid:48)|s  a)| ≤ radp

1(st at st+1)=(s a s(cid:48))  radp

m(s  a)

N +

t=1

H p

(cid:32)(cid:115)
m(s(cid:48)|s  a) ∀s(cid:48) ∈ S(cid:9) .

m(s(cid:48)|s  a) = ˜O

(cid:33)

ˆpm(s(cid:48)|s  a)
N +
m(s  a)

(cid:33)

(3)

 

(4)

 

(5)

1
K

m k(s  a)  radp

m(s(cid:48)|s  a) in Appendix B.2. We now explain
We provide the complete expressions of radv
the three vital components of TFW-UCRL2: (i) Frank-Wolfe (FW) [21]  which has been adapted in
related research on BwR [2  14] and exploration problems in MDPs [25  41]  (ii) Extended Value
Iteration (EVI) [28]  (iii) our crucial and novel Gradient Threshold Procedure (GTP).
Frank Wolfe (FW) [21] provides a way to balance the vectorial outcome at each time step t. We
denote (cid:107) · (cid:107)2 as the Euclidean norm  and deﬁne ΠU (w) = argminu∈U(cid:107)u − w(cid:107)2. At time t  FW
scalarizes the outcome in eqn (6) with the gradient

∇g( ¯V1:t−1) =

(cid:2)(L1  . . .   LK)(cid:62) − L0( ¯V1:t−1 − ΠU ( ¯V1:t−1))(cid:3) .
1  L1  . . .   LK = 0  U = {}. The s-component of ∇g( ¯V1:t−1) is (s −(cid:80)t−1

To gain intuitions  consider State Space Exploration with target frequency   where L0 =
q=1 1sq=s/(t − 1))/K 
which encourages visiting state s when its empirical frequency is below the target s. Sim-
ilarly  for Multi-Objective Optimization with KPI target ρ  the k-component of ∇g( ¯V1:t−1) is
(Lk + L0 max{ρk − ¯V1:t−1 k  0})/K. The agent is motivated to focus on the kth objective when
¯V1:t−1 k ≤ ρk.
Extended Value Iteration (EVI) [28] solves for an optimistic stationary policy for an SO-OMDP
problem  when v  p are not known. We extract EVI from [28] in Appendix B.3. Ideally  at the start
of each episode m  the agent wishes to compute the optimal policy under the scalarized reward
∇g( ¯V1:τ (m)−1)(cid:62)v and transition kernel p. Since v  p are uncertain  the agent uses EVI [28] to
compute the stationary policy ˜πm in (7)  which is optimal for the optimistic choices of ˜vm ∈ H v
and ˜pm ∈ H p
m
m. By optimistic choices ˜vm  ˜pm  we mean that the resulting single objective MDP
with scalar rewards ˜rm = {˜rm(s  a)}s a  ˜rm(s  a) = ∇g( ¯V1:τ (m)−1)(cid:62)˜vm(s  a) and transition kernel
˜p has the highest long term average reward  among all ¯vm ∈ H v
m. The last argument

1/(cid:112)τ (m) of EVI is an additive error term allowed for EVI. By [28]  EVI converges to a stationary

m  ¯pm ∈ H p

policy ˜πm in ﬁnite time when H p

m contains the transition kernel for a communicating MDP.

Algorithm 1 TFW-UCRL2 on g
1: Inputs: Parameter δ ∈ (0  1)  gradient threshold Q ≥ 0 (default Q = ¯L/
2: Initialize t = 1
3: for Episode m = 1  2  . . . do
4:
5:
6:

Set τ (m) = t  and initialize N +
Compute the conﬁdence regions H v
Compute the optimistic reward ˜rm = {˜rm(s  a)}s∈S a∈As:

m  H p

m(s  a) according to Eq (3) for each s ∈ S  a ∈ As.

m respectively for v  p  according to Eqs (4  5).

√

K)  initial state s1.

˜rm(s  a) =

Compute a (1/(cid:112)τ (m))-optimal optimistic policy ˜πm:

max
¯v(s a)∈H v

m(s a)

7:

∇g( ¯V1:τ (m)−1)(cid:62)¯v(s  a).

˜πm ← EVI(˜rm  H p

m; 1/(cid:112)τ (m)).

(6)

(7)

Initialize νm(s  a) = 0 for each s  a  θref = θτ (m) = ∇g( ¯V1:(τ (m)−1))  Ψ = 0.
while Ψ ≤ Q and νm(st  ˜πm(st)) < N +

m(st  ˜πm(st)) do

Choose action at = ˜πm(st).
Observe the outcomes Vt(st  at) and the next state st+1.
Compute gradient θt+1 = ∇g( ¯Vt).
Update Ψ ← Ψ + (cid:107)θt+1 − θref(cid:107)2.
Update νm(st  at) ← νm(st  at) + 1.
Update t ← t + 1.

8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end for

end while

(cid:46) Frank-Wolfe

The Gradient Threshold Procedure (GTP) maintains FW’s balancing effect on the vectorial out-
comes  while overcoming the challenges in avoiding sub-optimal actions. GTP maintains a distance
measure Ψ on the gradients generated by FW during each episode  and starts the next episode if the
measure Ψ exceeds a threshold Q. A small Q makes the agent alternate among different stationary

5

policies frequently and balances the outcomes  while a large Q facilitates learning and avoids visiting
sub-optimal states. A properly tuned Q paths the way to solve the CO-OMDP problem.
A direct combination of FW and EVI corresponds to TFW-UCRL2 with Q = 0  which silences
GTP and incurs Reg(T ) = Ω(1) on the instance in Fig 1b. Let’s assume start state to be s0  the
complete knowledge of v  p  and consistent tie breaking. The agent would go to s2  take ll once 
then go to s1  take rl once  then back to s2 and take ll once  and so on (the same dynamics as in
Challenges). Indeed  under the pure effect of FW  the agent is obsessed with balancing the outcomes.
Once ¯V1:t−1 1 > ¯V1:t−1 2  the scalarized reward for (s2  ll) is higher than that for (s1  rl)  and she
travels to s2. Similarly  once ¯V1:t−1 1 ≤ ¯V1:t−1 2  she travels to s1. In this process  she is oblivious to
the fact that constantly alternating between ll  rl penalizes her objective by constantly visiting s0.
In contrast  applying TFW-UCRL2 with 0 < Q < ∞ leads us to near-optimality. For example  with
√
√
Q = ¯L/
K  the agent follows this interesting trajectory: Suppose the agent is at s0 at time t. If
√
¯V1:t−1 1 > ¯V1:t−1 2  then she would travel to s2  take ll for Θ(
Qt) times  then head back to s0.
Qt) times  then head back to s0. Altogether  for
Otherwise  she would travel to s1  take rl for Θ(

every t we have ¯V1:t−1 1  ¯V1:t−1 2 = 0.5 ± O((cid:112)Q/t)  and the agent only visits s0 for O((cid:112)t/Q)

T ).

times  leading to the anytime regret bound Reg(T ) = O((
Finally  in another extreme case of Q = ∞  in fact we have Reg(T ) = Ω(1/SA log T ). Indeed 
the condition Ψ ≤ Q is always satisﬁed. By applying [28]  the agent alternates among ll  rl only
O(SA log T ) times in T time steps. This leads to an imbalance in the outcomes  since the agent
could stay at a loop for Ω(T /SA log T ) time  and results in Reg(T ) = Ω(1/SA log T ).
Main Results. We establish regret bounds for TFW-UCRL2. Denote S := |S|  A := 1
s∈S |As| 
so SA is the number of state-action pairs. Denote Γ := maxs∈S a∈As (cid:107)p(·|s  a)(cid:107)0  which is the
maximum number of states from which a state-action pair can transit to. We employ the ˜O(·) notation 
which hides additive terms which scales with log(T /δ)/T as well as multiplicative log(T /δ) factors.
Theorem 3.1. Consider TFW-UCRL2 with gradient threshold Q > 0  applied on a communicating
CO-OMDP instance M with diameter D. With probability 1 − O(δ)  we have anytime regret bound

(cid:80)

S

Q +(cid:112)1/Q)/

√

√

(cid:16)(cid:104)(cid:112)L0Q +

(cid:112)

L0 ¯LD/(cid:112)KQ

(cid:105)

K 1/4(cid:46)√

T

(cid:17)

Reg(T ) = ˜O

(cid:16) ¯L(D + 1)

√

+ ˜O
√

√
ΓSA/

(cid:46)√

(cid:17)

ΓSA

T

.

√

√

T ).

K gives Reg(T ) = ˜O( ¯L(D + 1)

In particular  setting Q = ¯L/
Let’s focus on the ﬁrst ˜O(·) term in the bound. The ˜O(
√
Q) term represents the regret due to
the delay in gradient updates by GTP. The ˜O(1/
Q) term represents the regret due to (a) the
interference of GTP with the learning of v  p  (b) the switches among stationary policies  which
could require visiting sub-optimal states. The second ˜O(·) term is the regret due to the simultaneous
exploration-exploitation by EVI.
By specializing L0 = 0  L1  . . .   LK = 1  TFW-UCRL2 incurs Reg(T ) = ˜O(D
T )
on SO-OMDP  which essentially matches [28]1. By specializing S = {s}  TFW-UCRL2 incurs
√
Reg(T ) = ˜O( ¯L
T )  which matches [2] on BwR on g. While our regret bounds match [28  2]
in those special cases  the design and analysis of TFW-UCRL2 require novel ideas that depart from
[28  2]. We design the novel GTP for handling state transitions. In the upcoming analysis  we show
that GTP is streamlined so that it achieves our regret bounds  without excessively interfering the
balancing by FW and the learning by EVI. Finally  note that TFW-UCRL2 is a non-stationary policy
that diversiﬁes across different stationary policies across time. Interestingly  non-stationary policy is
necessary achieving near-optimality  even when the model parameters are unchanging:
Claim 3.2. Every stationary policy incurs an Ω(1) anytime regret on the instance in Fig 1b.

√
ΓSA/

√

√

A/

The Claim  proved in Appendix B.4  illustrates a profound difference between communicating
CO-OMDPs and unichain CO-OMDPs  see Appendix B.4.
Max E[g( ¯V1:T )] vs max g(E[ ¯V1:T ]). Our objective is to maximize g( ¯V1:T ) (also leads to max
E[g( ¯V1:T )])  which crucially different from maximizing g(E[ ¯V1:T ]). Now  for any policy  it holds that
ΓS  by

1Jaksch et al. [28] achieve the regret bound ˜O(DS

T ). The factor of S is improved to

√
A/

√

√

applying an empirical Bernstein inequality[7] instead of the Hoeffding inequality  as used in [23].

6

E[g( ¯V1:T )] ≤ g(E[ ¯V1:T ]) ≤ opt(PM) + O( ¯LD/T ). The second inequality (formally proved in [18])
is demonstrated by showing that  for any policy  the empirical frequency of visiting each state-action
pair is “nearly” a feasible solution to (PM)  with the O( ¯LD/T ) term capturing the error due to the
near feasibility. Under TFW-UCRL2  we know that E[g( ¯V1:T )] tends to opt(PM) as T grows. Hence
we also have g(E[ ¯V1:T ]) tending to opt(PM) as T grows.
Nevertheless  the converse is not true. Consider the instance in Fig 1b again  where the starting state
is s0. Consider the following policy: At the start  the agent transits to either s1 or s2 with probability
1/2. After that  the agent loops at that state indeﬁnitely. It is clear that Pr[ ¯V1:T = (1 − 1/T  0)(cid:62)] =
Pr[ ¯V1:T = (0  1 − 1/T )(cid:62)] = 1/2. On the one hand  we have g(E[ ¯V1:T ]) = −1/(8T 2)  which tends
to opt(PM) = 0 as T → ∞. On the other hand  we have E[g( ¯V1:T )] = −1/8 + O(1/T )  which
does not tend to opt(PM) = 0 as T → ∞. Altogether  solving max g(E[ ¯V1:T ]) to near-optimality
does not lead to the near-optimality for max E[g( ¯V1:T )].
To this end  it is worth mentioning that the related works in the discounted settings [24  6  12  43 
t=1 γtVt(st  at)])  where ¯g is a certain non-linear
function and γ ∈ (0  1) is the discounted factor. We envision that our technique could be useful for

1  25  42  29  30  33] focus on maximizing ¯g(E[(cid:80)∞
maximizing E[¯g((cid:80)∞

t=1 γtVt(st  at))] instead of maximizing ¯g(E[(cid:80)∞

t=1 γtVt(st  at)]).

Generalizations. While Theorem 3.1 concerns the specialized aggregate function (1)  Cheung [18]
recently generalizes the algorithmic framework to any Lipschitz continuous and smooth function.
By adapting to the online mirror descent algorithm [34]  Cheung [18] proposes another algorithm
that results in O(1/T 3) regret (We hide the dependence on M) for Lipschitz continuous concave
aggregate functions that are not necessarily smooth.

4 Analysis of TFW-UCRL2
In this Section  we prove Theorem 3.1. To start  we consider events E v E p and Lemma 4.1  which is
proved in Appendix C.1. The shorthand ∀m  s  a means ‘for all m ∈ N  s ∈ S  a ∈ As’.
m(s  a) ∀m  s  a} .

E p := {p(·|s  a) ∈ H p

E v := {v(s  a) ∈ H v

m(s  a) ∀m  s  a}  

Lemma 4.1. It holds that P[E v] ≥ 1 − δ/2  P[E p] ≥ 1 − δ/2.

41]. Deﬁne v∗ :=(cid:80)

We decompose Reg(T ) with the analytical tools on FW [21  16]  which is also adapted in [2  14  25 

s a v(s  a)x∗(s  a)  where x∗ is an optimal solution of (PM). We have

(cid:107) ¯V1:t − ¯V1:t−1(cid:107)2

2

(8)

g( ¯V1:t) ≥ g( ¯V1:t−1) + ∇g( ¯V1:t−1)(cid:62)[ ¯V1:t − ¯V1:t−1] − L0
K
∇g( ¯V1:t−1)(cid:62)[Vt(st  at) − ¯V1:t−1] − L0
∇g( ¯V1:t−1)(cid:62)[v∗ − ¯V1:t−1] +
1
t

(cid:2)opt(PM) − g( ¯V1:t−1)(cid:3) +

=g( ¯V1:t−1) +
≥g( ¯V1:t−1) +
≥g( ¯V1:t−1) +

1
t
1
t
1
t

Kt2(cid:107)Vt(st  at) − ¯V1:t−1(cid:107)2

2

∇g( ¯V1:t−1)(cid:62)[Vt(st  at) − v∗] − L0
1
t2
t
∇g( ¯V1:t−1)(cid:62)[Vt(st  at) − v∗] − L0
t2 .

(9)
Step (8) is by the property that g is (2L0/K)-smooth w.r.t. (cid:107)·(cid:107)2 on the domain [0  1]K (see Appendix
B.1). Rearranging (9) gives

t · Reg(t) ≤ (t − 1) · Reg(t − 1) +

+ ∇g( ¯V1:t−1)(cid:62)[v∗ − Vt(st  at)].

Apply (10) recursively for t = T  . . .   1  we obtain (recall that θt = ∇g( ¯V1:t−1)):

Reg(T ) ≤ 2L0 log T

T

+

1
T

t [v∗ − Vt(st  at)].
θ(cid:62)

L0
t

T(cid:88)

t=1

(10)

(11)

The main analysis is now on the second term in (11)  which requires novel technical analysis regarding
the dynamics of the gradient threshold procedure. We start with the following bound.

7

Proposition 4.2. Consider an execution of TFW-UCRL2 on a communicating instance with diameter
D. For each T ∈ N  suppose that there is a deterministic constant M (T ) s.t. Pr[m(T ) ≤ M (T )] = 1.
Conditioned on events E v E p  with probability at least 1 − O(δ) we have

(cid:16)

√
(Q

(cid:17)

(cid:16) ¯L(D + 1)

√

(cid:17)

K + ¯LD)M (T )

+ ˜O

ΓSAT

.

t [v∗ − Vt(st  at)] = ˜O
θ(cid:62)

T(cid:88)

t=1

m  H p

Proposition 4.2 bounds two sources of error: (i) the error due to GTP  (ii) the estimation errors
associated with H v
m and EVI. Error (ii) can be upper bounded by the machinery in [28]. Error (i)
concerns the following discrepancy. For each time t in episode m  the action at = ˜πm(st) is chosen
based on policy ˜πm  which involves the scalarization by θτ (m). However  ideally the action at time t
should balance the current vectorial outcomes by the scalarization with θt. The Proposition bounds
error (i) by charging the discrepancy to the threshold Q and the upper bound M (T ). To complete the
proof of bounding Reg(T )  we establish a bound M (T ) small enough to achieve Theorem 3.1.
Lemma 4.3. Consider an execution of TFW-UCRL2 with gradient threshold Q > 0. With certainty 
for every T ∈ N we have m(T ) ≤ M (T ) = ˜O(

√
L0T /(

(cid:113)

KQ)).

The Lemma bounds the error of GTP in balancing the outcomes. Under FW  the gradients change at
a rate O(1/t)  which is slow enough for the agent to judiciously delay the gradient updates without
sacriﬁcing their balancing effect too much. This opens the door to avoid visiting sub-optimal states
too frequently.
Sketch Proof of Lemma 4.3. First  observe that {1  . . .   m(T )} is the union of

MΨ(T ) := {m ∈ N : τ (m) ≤ T   episode m + 1 is started due to Ψ ≥ Q}  
Mν(T ) := {m ∈ N : τ (m) ≤ T   episode m + 1 is started due to

m(st  ˜πm(st)) for some t ≥ τ (m)(cid:9)  

νm(st  ˜πm(st)) ≥ N +

To prove the Lemma  it sufﬁces to show that:
√
|MΨ(T )| ≤ MΨ(T ) := 1 + (
|Mν(T )| ≤ Mν(T ) := SA(1 + log2 T ).

(12)
(13)
The bound (13) follows from [28]. Thus  we focus on showing bound (12). Let’s express MΨ(T ) =
{m1  m2  . . .   mnΨ}  where m1 < m2 < . . . < mnΨ. We also deﬁne m0 = 0. We focus on an
episode index mj with j ≥ 1  and consider for each t ∈ {τ (mj) + 1  . . .   τ (mj + 1)} the difference
(cid:107)θt − θτ (mj )(cid:107)2. In the following  we argue that the gradients under FW changes slowly:

√
2L0T /(

KQ/2L0) + 4

KQ) 

(cid:113)

(cid:107)θt − θτ (mj )(cid:107)2 = (cid:107)∇g(V1:t−1) − ∇g(V1:τ (mj )−1)(cid:107)2
≤ L0
K

τ (mj )−1(cid:88)

Vq(sq  aq) −

τ (mj) − 1

t−1(cid:88)

1

q=1

Vq(sq  aq)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

Vq(sq  aq) −

1

τ (mj) − 1

τ (mj )−1(cid:88)

q=1

Vq(sq  aq)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

=

q=1

L0
K

t − 1
· t − τ (mj)
·
t − 1
· t − τ (mj)
t − 1

t−1(cid:88)
· t − τ (mj)
Since mj ∈ MΨ(T )  we know that(cid:80)τ (mj +1)

t − τ (mj)
≤ 2L0√
K

≤ 2L0√
K

q=τ (mj )

1

.

(τ (mj + 1) − τ (mj−1 + 1))2

τ (mj−1 + 1)

τ (mj)
t=τ (mj ) (cid:107)θt − θτ (mj )(cid:107)2 > Q  which means
t − τ (mj)
τ (mj)

≥ (τ (mj + 1) − τ (mj))2

τ (mj +1)(cid:88)

τ (mj)

≥

t=τ (mj )

√

KQ
2L0

>

  (14)

Inequality (14) says that  since gradients change slowly  the time indexes {τ (mj + 1)}nΨ
j=1 have to be
√
far apart. Thus  nΨ can be bounded from above. Indeed  by some technical arguments (see Appendix
C.2)  inequality (14) turns out to imply τ (m(cid:100)Q(cid:48)(cid:101)+j +1) ≥ Q(cid:48)(j−1)2/16  where Q(cid:48) =
KQ/(2L0).
With j = nΨ − 1  we get Q(cid:48)(nΨ − 2)2/16 ≤ τ (m(cid:100)Q(cid:48)(cid:101)+nΨ−1 + 1) ≤ T   leading to (12).
Combining the bound (11)  Proposition 4.2 and Lemma 4.3  we have proved Theorem 3.1.

8

5 Numerical Experiments

We empirically evaluate TFW-UCRL2 on State Space Exploration on 3 instances: Small  Medium 
Large. These instances are detailed in Appendix A.1. In Fig 2a  TFW-UCRL2 is simulated on each
√
instance and each Q for 25 times. In Figs 2b  2c  TFW-UCRL2 is simulated on each instance and
Q = ¯L/
K for 25 times. Each curve plots the averages across the 25 trials  and each error bar
quantiﬁes the ± standard deviation error region. Fig 2a depicts Reg(105) under different Qs. While
√
extremely small or large Q leads to a large regret  TFW-UCRL2 seems robust in the middle range of
Q. Our default Q = ¯L/
K (green dot) is motivated by our analysis  and it does not optimize the
empirical performance. Tuning Q online is an interesting research direction.

(a) Reg(105) under different Q

(b) Reg(T ) as T grows

(c) Simultaneous Convergence

Figure 2: Simulation Results on State Space Exploration

Fig 2b demonstrates the trend of Reg(T ) as T grows  in log-log scales. The performance of TFW-
UCRL2 is in contrast with the random policy (in yellow)  which samples an action uniformly at
random at every state. The Reg(T ) under TFW-UCRL2 converges to 0 as T grows  while the Reg(T )
under the random policy is constant. The slight wiggling in the plots for TFW-UCRL2 is due to GTP 
which could deteriorate the objective in the short term but still leads to near-optimality eventually.
Fig. 2c highlights the simultaneous convergence of each objective to its target on the Large instance.
The instance involves a star graph with a center state and 12 branch states. The objectives are to visit
the center state with frequency 0 (Obj 1) and to visit each branch state with frequency 1/12 = 0.83
(Objs 2  . . . 13). These target frequencies (in dashed black) are not realizable  and we plot (in dotted
s a v(s  a)x∗(s  a)  where x∗ is an optimal solution of
(PM). Along with the complete plot in Fig 4 in Appendix A.2  we see that the outputs {V1:T k}13
by TFW-UCRL2 (in solid blue) simultaneously converge to all the 13 target frequencies.

cyan) the frequencies indicated by v∗ =(cid:80)

k=1

References
[1] J. Achiam  D. Held  A. Tamar  and P. Abbeel. Constrained policy optimization. In Proceedings
of the 34th International Conference on Machine Learning - Volume 70  ICML’17  pages 22–31.
JMLR.org  2017.

[2] S. Agrawal and N. R. Devanur. Bandits with concave rewards and convex knapsacks. In ACM

Conference on Economics and Computation  2014.

[3] S. Agrawal and N. R. Devanur. Linear contextual bandits with knapsacks. In Advances in Neural
Information Processing Systems 29: Annual Conference on Neural Information Processing
Systems 2016  December 5-10  2016  Barcelona  Spain  pages 3450–3458  2016.

[4] S. Agrawal  N. R. Devanur  and L. Li. An efﬁcient algorithm for contextual bandits with
knapsacks  and an extension to concave objectives. In Proceedings of the 29th Conference on
Learning Theory  COLT 2016  New York  USA  June 23-26  2016  pages 4–18  2016.

[5] S. Agrawal and R. Jia. Optimistic posterior sampling for reinforcement learning: worst-case
regret bounds. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan 
and R. Garnett  editors  Advances in Neural Information Processing Systems 30  pages 1184–
1194. Curran Associates  Inc.  2017.

[6] E. Altman. Constrained Markov Decision Processes. Chapman and Hall  1999.

9

10−210−1100101102Q10−4Reg(105)SmallMediumLarge103104T10−410−310−2Reg(T)TFW-UCRL2 SmallTFW-UCRL2 MediumTFW-UCRL2 LargeRandom SmallRandom MediumRandom Large0.00.2Obj 1 by algoObj 1 by PMTarget for Obj 11031040.000.05Obj 2 by algoObj 2 by PMTarget for Obj 2[7] J. Audibert  R. Munos  and C. Szepesvári. Exploration-exploitation tradeoff using variance

estimates in multi-armed bandits. Theor. Comput. Sci.  410(19):1876–1902  2009.

[8] P. Auer and R. Ortner. Logarithmic online regret bounds for undiscounted reinforcement
learning. In Advances in Neural Information Processing Systems  pages 49–56. MIT Press 
2006.

[9] Y. Azar  U. Felge  M. Feldman  and M. Tennenholtz. Sequential decision making with vector
outcomes. In Proceedings of the 5th Conference on Innovations in Theoretical Computer
Science  ITCS ’14  pages 195–206  New York  NY  USA  2014. ACM.

[10] A. Badanidiyuru  R. Kleinberg  and A. Slivkins. Bandits with knapsacks. In Proceedings of the
2013 IEEE 54th Annual Symposium on Foundations of Computer Science  FOCS ’13  pages
207–216. IEEE Computer Society  2013.

[11] A. Badanidiyuru  J. Langford  and A. Slivkins. Resourceful contextual bandits. In Proceedings
of The 27th Conference on Learning Theory  COLT 2014  Barcelona  Spain  June 13-15  2014 
pages 1109–1134  2014.

[12] L. Barrett and S. Narayanan. Learning all optimal policies with multiple criteria. In Proceedings
of the 25th International Conference on Machine Learning  ICML ’08  pages 41–47  New York 
NY  USA  2008. ACM.

[13] P. L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement
In UAI 2009  Proceedings of the Twenty-Fifth
learning in weakly communicating mdps.
Conference on Uncertainty in Artiﬁcial Intelligence  Montreal  QC  Canada  June 18-21  2009 
pages 35–42  2009.

[14] Q. Berthet and V. Perchet. Fast rates for bandit optimization with upper-conﬁdence frank-wolfe.

In Advances in Neural Information Processing Systems 30  pages 2225–2234. 2017.

[15] D. P. Bertsekas. Dynamic programming and optimal control  3rd Edition. Athena Scientiﬁc 

2005.

[16] S. Bubeck. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn. 

8(3-4):231–357  Nov. 2015.

[17] R. Busa-Fekete  B. Szörényi  P. Weng  and S. Mannor. Multi-objective bandits: Optimizing
the generalized Gini index. In D. Precup and Y. W. Teh  editors  Proceedings of the 34th
International Conference on Machine Learning  volume 70 of Proceedings of Machine Learning
Research  pages 625–634  International Convention Centre  Sydney  Australia  06–11 Aug 2017.
PMLR.

[18] W. C. Cheung. Exploration-exploitation trade-off in reinforcement learning on online markov

decision processes with global concave rewards. CoRR  abs/1905.06466  2019.

[19] E. Even-Dar  R. Kleinberg  S. Mannor  and Y. Mansour. Online learning with global cost

functions. In 22nd Annual Conference on Learning Theory  COLT  2009.

[20] K. J. Ferreira  D. Simchi-Levi  and H. Wang. Online network revenue management using

thompson sampling. Operations Research  66(6):1586–1602  2018.

[21] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics

Quarterly  3(1-2):95–110  March - June 1956.

[22] R. Fruit  M. Pirotta  and A. Lazaric. Near optimal exploration-exploitation in non-
communicating markov decision processes. In S. Bengio  H. Wallach  H. Larochelle  K. Grau-
man  N. Cesa-Bianchi  and R. Garnett  editors  Advances in Neural Information Processing
Systems 31  pages 2998–3008. Curran Associates  Inc.  2018.

[23] R. Fruit  M. Pirotta  A. Lazaric  and R. Ortner. Efﬁcient bias-span-constrained exploration-
exploitation in reinforcement learning. In J. Dy and A. Krause  editors  Proceedings of the
35th International Conference on Machine Learning  volume 80 of Proceedings of Machine
Learning Research  pages 1578–1586  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018.
PMLR.

10

[24] Z. Gábor  Z. Kalmár  and C. Szepesvári. Multi-criteria reinforcement learning. In Proceedings
of the Fifteenth International Conference on Machine Learning  ICML ’98  pages 197–205  San
Francisco  CA  USA  1998. Morgan Kaufmann Publishers Inc.

[25] E. Hazan  S. M. Kakade  K. Singh  and A. V. Soest. Provably efﬁcient maximum entropy

exploration. CoRR  2018.

[26] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the

American statistical association  58(301):13–30  1963.

[27] N. Immorlica  K. A. Sankararaman  R. E. Schapire  and A. Slivkins. Adversarial bandits with

knapsacks. CoRR  2018.

[28] T. Jaksch  R. Ortner  and P. Auer. Near-optimal regret bounds for reinforcement learning. J.

Mach. Learn. Res.  11:1563–1600  Aug. 2010.

[29] H. Le  C. Voloshin  and Y. Yue. Batch policy learning under constraints. In K. Chaudhuri
and R. Salakhutdinov  editors  Proceedings of the 36th International Conference on Machine
Learning  volume 97 of Proceedings of Machine Learning Research  pages 3703–3712  Long
Beach  California  USA  09–15 Jun 2019. PMLR.

[30] L. Lee  B. Eysenbach  E. Parisotto  E. P. Xing  S. Levine  and R. Salakhutdinov. Efﬁcient

exploration via state marginal matching. CoRR  abs/1906.05274  2019.

[31] C. Liu  X. Xu  and D. Hu. Multiobjective reinforcement learning: A comprehensive overview.
IEEE Transactions on Systems  Man  and Cybernetics: Systems  45(3):385–398  March 2015.
[32] S. Mannor and N. Shimkin. A geometric approach to multi-criterion reinforcement learning. J.

Mach. Learn. Res.  5:325–360  Dec. 2004.

[33] S. Miryooseﬁ  K. Brantley  H. D. III  M. Dudík  and R. E. Schapire. Reinforcement learning

with convex constraints. To appear in NeurIPS 2019  2019.

[34] A. Nemirovski and D. B. Yudin. Problem complexity and method efﬁciency in optimization.

Wiley Interscience Series in discrete mathematics  1983.

[35] J. R. Norris. Markov chains. Cambridge series in statistical and probabilistic mathematics.

Cambridge University Press  1998.

[36] R. Ortner. Regret bounds for reinforcement learning via markov chain concentration. CoRR 

2018.

[37] Y. Ouyang  M. Gagrani  A. Nayyar  and R. Jain. Learning unknown markov decision processes:
A thompson sampling approach. In Advances in Neural Information Processing Systems 30 
pages 1333–1342. 2017.

[38] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John

Wiley & Sons  Inc.  New York  NY  USA  1st edition  1994.

[39] D. M. Roijers  P. Vamplew  S. Whiteson  and R. Dazeley. A survey of multi-objective sequential

decision-making. J. Artif. Int. Res.  48(1):67–113  Oct. 2013.

[40] A. Rosenberg and Y. Mansour. Online convex optimization in adversarial Markov decision
processes. In K. Chaudhuri and R. Salakhutdinov  editors  Proceedings of the 36th International
Conference on Machine Learning  volume 97 of Proceedings of Machine Learning Research 
pages 5478–5486  Long Beach  California  USA  09–15 Jun 2019. PMLR.

[41] J. Tarbouriech and A. Lazaric. Active exploration in markov decision processes. In K. Chaud-
huri and M. Sugiyama  editors  Proceedings of Machine Learning Research  volume 89 of
Proceedings of Machine Learning Research  pages 974–982. PMLR  16–18 Apr 2019.

[42] C. Tessler  D. J. Mankowitz  and S. Mannor. Reward constrained policy optimization. In

International Conference on Learning Representations  2019.

[43] K. Van Moffaert and A. Nowé. Multi-objective reinforcement learning using sets of pareto

dominating policies. J. Mach. Learn. Res.  15(1):3483–3512  Jan. 2014.

11

,Wang Chi Cheung