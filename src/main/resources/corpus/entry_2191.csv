2016,GAP Safe Screening Rules for Sparse-Group Lasso,For statistical learning in high dimension  sparse regularizations have proven useful to boost both computational and statistical efficiency. In some contexts  it is natural to handle more refined structures than pure sparsity  such as for instance group sparsity. Sparse-Group Lasso has recently been introduced in the context of linear regression to enforce sparsity both at the feature and at the group level. We propose the first (provably) safe screening rules for Sparse-Group Lasso  i.e.  rules that allow to discard early in the solver features/groups that are inactive at optimal solution. Thanks to efficient dual gap computations relying on the geometric properties of $\epsilon$-norm  safe screening rules for Sparse-Group Lasso lead to significant gains in term of computing time for our coordinate descent implementation.,GAP Safe Screening Rules for Sparse-Group Lasso

Eugene Ndiaye  Olivier Fercoq  Alexandre Gramfort  Joseph Salmon

LTCI  CNRS  Télécom ParisTech

Université Paris-Saclay

75013 Paris  France

first.last@telecom-paristech.fr

Abstract

For statistical learning in high dimension  sparse regularizations have proven useful
to boost both computational and statistical efﬁciency. In some contexts  it is natural
to handle more reﬁned structures than pure sparsity  such as for instance group
sparsity. Sparse-Group Lasso has recently been introduced in the context of linear
regression to enforce sparsity both at the feature and at the group level. We propose
the ﬁrst (provably) safe screening rules for Sparse-Group Lasso  i.e.  rules that allow
to discard early in the solver features/groups that are inactive at optimal solution.
Thanks to efﬁcient dual gap computations relying on the geometric properties of
-norm  safe screening rules for Sparse-Group Lasso lead to signiﬁcant gains in
term of computing time for our coordinate descent implementation.

1

Introduction

Sparsity is a critical property for the success of regression methods  especially in high dimension.
Often  group (or block) sparsity is helpful when a known group structure needs to be enforced. This
is for instance the case in multi-task learning [1] or multinomial logistic regression [5  Chapter 3]. In
the multi-task setting  the group structure appears natural since one aims at jointly recovering signals
whose supports are shared. In this context  sparsity and group sparsity are generally obtained by
adding a regularization term to the data-ﬁtting: (cid:96)1 norm for sparsity and (cid:96)1 2 norm for group sparsity.
Along with recent works on hierarchical regularization [12  17] have focused on a speciﬁc case:
the Sparse-Group Lasso. This method is the solution of a (convex) optimization program with a
regularization term that is a convex combination of the two aforementioned norms  enforcing sparsity
and group sparsity at the same time.
With such advanced regularizations  the computational burden can be particularly heavy in high
dimension. Yet  it can be signiﬁcantly reduced if one can exploit the known sparsity of the solution
in the optimization. Following the seminal paper on “safe screening rules” [9]  many contributions
have investigated such strategies [21  20  3]. These so called safe screening rules compute some
tests on dual feasible points to eliminate primal variables whose coefﬁcients are guaranteed to be
zero in the exact solution. Still  the computation of a dual feasible point can be challenging when
the regularization is more complex than (cid:96)1 or (cid:96)1 2 norms. This is the case for the Sparse-Group
Lasso as it is not straightforward to characterize if a dual point is feasible or not [20]. Here  we
propose an efﬁcient computation of the associated dual norm. It is all the more crucial since the naive
implementation computes the Sparse-Group Lasso dual norm with a quadratic complexity w.r.t the
groups dimensions.
We propose here efﬁcient safe screening rules for the Sparse-Group Lasso that combine sequential
rules (i.e.  rules that perform screening thanks to solutions obtained for a previously processed tuning
parameter) and dynamic rules (i.e.  rules that perform screening as the algorithm proceeds) in a
uniﬁed way. We elaborate on GAP safe rules  a strategy relying on dual gap computations introduced

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

for the Lasso [10] and to more general learning tasks in [15]. Note that alternative (unsafe) screening
rules  for instance the “strong rules” [19]  have been applied to the Lasso and its simple variants.
Our contributions are two fold here. First  we introduce the ﬁrst safe screening rules for this problem 
other alleged safe rules [20] for Sparse-Group Lasso were in fact not safe  as explained in detail in
[15]  and could lead to non-convergent implementation. Second  we link the Sparse-Group Lasso
penalties to the -norm in [6]. This allows to provide a new algorithm to efﬁciently compute the
required dual norms  adapting an algorithm introduced in [7]. We incorporate our proposed GAP Safe
rules in a block coordinate descent algorithm and show its practical efﬁciency in climate prediction
tasks. Another strategy leveraging dual gap computations and active sets has recently been proposed
under the name Blitz [13]. It could naturally beneﬁt from our fast dual norm evaluations in this
context.
Notation For any integer d P N  we denote by rds the set t1  . . .   du. The standard Euclidean norm
is written (cid:107)¨(cid:107)  the (cid:96)1 norm (cid:107)¨(cid:107)1  the (cid:96)8 norm (cid:107)¨(cid:107)8  and the transpose of a matrix Q is denoted by
QJ. We also denote ptq` “ maxp0  tq. Our observation vector is y P Rn and the design matrix
X “ rX1  . . .   Xps P Rnˆp has p features  stored column-wise. We consider problems where the
vector of parameters β “ pβ1  . . .   βpqJ admits a natural group structure. A group of features is a
subset g Ă rps and ng is its cardinality. The set of groups is denoted by G and we focus only on
non-overlapping groups that form a partition of rps. We denote by βg the vector in Rng which is the
restriction of β to the indexes in g. We write rβgsj the j-th coordinate of βg. We also use the notation
Xg P Rnˆng for the sub-matrix of X assembled from the columns with indexes j P g; similarly
rXgsj is the j-th column of rXgs.
For any norm Ω  BΩ refers to the corresponding unit ball  and B (resp. B8) stands for the Euclidean
(resp. (cid:96)8) unit ball. The soft-thresholding operator (at level τ ě 0)  Sτ   is deﬁned for any x P Rd
by rSτpxqsj “ signpxjqp|xj| ´ τq`  while the group soft-thresholding (at level τ) is S gp
τ pxq “
p1 ´ τ{(cid:107)x(cid:107)q`x. Denoting ΠC the projection on a closed convex set C  this yields Sτ “ Id´ΠτB8.
The sub-differential of a convex function f : Rd Ñ R at x is deﬁned by Bfpxq “ tz P Rd : @y P
Rd  fpxq ´ fpyq ě zJpx ´ yqu. We recall that the sub-differential B(cid:107)¨(cid:107)1 of the (cid:96)1 norm is signp¨q 
deﬁned element-wise by @j P rds  signpxqj “

"
tsignpxjqu  
r´1  1s 

if xj ‰ 0 
"
if xj “ 0.
tx{(cid:107)x(cid:107)u  
B 
ř

Note that the sub-differential B(cid:107)¨(cid:107) of the Euclidean norm is B(cid:107)¨(cid:107)pxq “
For any norm Ω on Rd  ΩD is the dual norm of Ω  and is deﬁned for any x P Rd by ΩDpxq “
maxvPBΩ vJx  e.g.  (cid:107)¨(cid:107)D
1 “ (cid:107)¨(cid:107)8 and (cid:107)¨(cid:107)D “ (cid:107)¨(cid:107). We only focus on the Sparse-Group Lasso
norm  so we assume that Ω “ Ωτ w  where Ωτ wpβq :“ τ(cid:107)β(cid:107)1 ` p1 ´ τq
gPG wg(cid:107)βg(cid:107)  for
τ P r0  1s  w “ pwgqgPG with wg ě 0 for all g P G. The case where wg “ 0 for some g P G together
with τ “ 0 is excluded (Ωτ w is not a norm in such a case).

if x ‰ 0 
if x “ 0.

2 Sparse-Group Lasso regression
For λ ą 0 and τ P r0  1s  the Sparse-Group Lasso estimator denoted by ˆβpλ Ωq is deﬁned as a
minimizer of the primal objective Pλ Ω deﬁned by:

ˆβpλ Ωq P arg min
βPRp

1
2

(cid:107)y ´ Xβ(cid:107)2 ` λΩpβq :“ Pλ Ωpβq.

A dual formulation (see [4  Th. 3.3.5]) of (1) is given by
(cid:107)y(cid:107)2 ´ λ2
2

ˆθpλ Ωq “ arg max
θP∆X Ω

1
2

(cid:13)(cid:13)(cid:13)θ ´ y

λ

(cid:13)(cid:13)(cid:13)2

:“ Dλpθq 

(1)

(2)

where ∆X Ω “ tθ P Rn : ΩDpXJθq ď 1u. The parameter λ ą 0 controls the trade-off between
data-ﬁtting and sparsity  and τ controls the trade-off between features sparsity and group sparsity. In
particular one recovers the Lasso [18] if τ “ 1  and the Group-Lasso [22] if τ “ 0.

2

For the primal problem  Fermat’s rule (cf. Appendix for details) reads:

λˆθpλ Ωq “ y ´ X ˆβpλ Ωq
XJ ˆθpλ Ωq P BΩp ˆβpλ Ωqq

(link-equation)  
(sub-differential inclusion).

(3)
(4)
Remark 1 (Dual uniqueness). The dual solution ˆθpλ Ωq is unique  while the primal solution ˆβpλ Ωq
might not be. Indeed  the dual formulation (2) is equivalent to ˆθpλ Ωq “ arg minθP∆X Ω (cid:107)θ ´ y{λ(cid:107) 
so ˆθpλ Ωq “ Π∆X Ωpy{λq is the projection of y{λ over the dual feasible set ∆X Ω.
Remark 2 (Critical parameter: λmax). There is a critical value λmax such that 0 is a primal solution
of (1) for all λ ě λmax. Indeed  the Fermat’s rule states 0 P arg minβPRp(cid:107)y´ Xβ(cid:107)2{2` λΩpβqðñ
0 P tXJyu ` λBΩp0qðñΩDpXJyq ď λ. Hence  the critical parameter is given by: λmax :“
ΩDpXJyq. Note that evaluating λmax highly relies on the ability to (efﬁciently) compute the dual
norm ΩD.

3 GAP safe rule for the Sparse-Group Lasso

The safe rule we propose here is an extension to the Sparse-Group Lasso of the GAP safe rules
introduced for Lasso and Group-Lasso [10  15]. For the Sparse-Group Lasso  the geometry of the
dual feasible set ∆X Ω is more complex (an illustration is given in Fig. 1). Hence  computing a dual
feasible point is more intricate. As seen in Section 3.2  the computation of a dual feasible point
strongly relies on the ability to evaluate the dual norm ΩD. This crucial evaluation is discussed in
Section 4. We ﬁrst detail how GAP safe screening rules can be obtained for the Sparse-Group Lasso.

3.1 Description of the screening rules

Safe screening rules exploit the known sparsity of the solutions of problems such as (1). They discard
inactive features/groups whose coefﬁcients are guaranteed to be zero for optimal solutions. Then  a
signiﬁcant reduction in computing time can be obtained ignoring “irrelevant” features/groups. The
Sparse-Group Lasso beneﬁts from two levels of screening: the safe rules can detect both group-wise
zeros in the vector ˆβpλ Ωq and coordinate-wise zeros in the remaining groups.
To obtain useful screening rules one needs a safe region  i.e.  a set containing the optimal dual
solution ˆθpλ Ωq. Following [9]  when we choose a ball Bpθc  rq with radius r and centered at θc as a
safe region  we call it a safe sphere. A safe sphere is all the more useful that r is small and θc close to
ˆθpλ Ωq. The safe rules for the Sparse-Group Lasso work as follows: for any group g in G and any safe
sphere Bpθc  rq

Group level safe screening rule:

Feature level safe screening rule:

max

θPBpθc rq

(cid:13)(cid:13)SτpXJ

g θq(cid:13)(cid:13) ă p1 ´ τqwg ñ ˆβpλ Ωq
θPBpθc rq|XJ

j θ| ă τ ñ ˆβ

g
pλ Ωq
j

@j P g  max

“ 0 
“ 0.

(5)

(6)

(7)

(8)

This means that provided one the last two test is true  the corresponding group or feature can be
(safely) discarded. For screening variables  we rely on the following upper-bounds:
Proposition 1. For all group g P G and j P g 
θPBpθc rq|XJ
#(cid:13)(cid:13)SτpXJ
p(cid:13)(cid:13)XJ

g θcq(cid:13)(cid:13) ` r (cid:107)Xg(cid:107)
(cid:13)(cid:13)8 ` r (cid:107)Xg(cid:107) ´ τq`

g θq(cid:13)(cid:13) ď Tg :“

(cid:13)(cid:13)8 ą τ 

(cid:13)(cid:13)SτpXJ

if (cid:13)(cid:13)XJ

j θc| ` r (cid:107)Xj(cid:107) .

g θc
otherwise.

j θ| ď |XJ

max

θPBpθc rq

max

g θc

and

Assume now that one has found a safe sphere Bpθc  rq (their creation is deferred to Section 3.2)  then
the safe screening rules given by (5) and (6) read:
Theorem 1 (Safe rules for the Sparse-Group Lasso). Using Tg deﬁned in (8)  we can state the
following safe screening rules:
“ 0 
Group level safe screening:
“ 0.
Feature level safe screening:

if Tg ă p1 ´ τqwg 
if |XJ

@g P G 
@g P G @j P g 

j θc| ` r (cid:107)Xj(cid:107) ă τ 

then ˆβpλ Ωq
g
pλ Ωq
then ˆβ
j

3

(a) Lasso dual ball BΩD for
ΩDpθq “ (cid:107)θ(cid:107)8.

a

(b) Group-Lasso dual ball BΩD for
2 |θ3|q.
ΩDpθq “ maxp

1 ` θ2
θ2

(cid:32)
(
(c) Sparse-Group Lasso dual ball
θ : @g P G (cid:107)Sτpθgq(cid:107) ď
BΩD “
p1 ´ τqwg
.

Figure 1: Lasso  Group-Lasso and Sparse-Group Lasso dual unit balls BΩD “ tθ : ΩDpθq ď 1u  for
the case of G “ tt1  2u t3uu (i.e.  g1 “ t1  2u  g2 “ t3u)  n “ p “ 3  wg1 “ wg2 “ 1 and τ “ 1{2.

The screening rules can detect which coordinates or group of coordinates can be safely set to zero.
This allows to remove the corresponding features from the design matrix X during the optimization
process. While standard algorithms solve (1) scanning all variables  only active ones  i.e.  non
screened-out variables (using the terminology from Section 3.3) need to be considered with safe
screening strategies. This leads to signiﬁcant computational speed-ups  especially with a coordinate
descent algorithm for which it is natural to ignore features (see Algorithm 2  in Appendix G).

3.2 GAP safe sphere

We now show how to compute the safe sphere radius and center using the duality gap.

3.2.1 Computation of the radius
With a dual feasible point θ P ∆X Ω and a primal vector β P Rp at hand  let us construct a safe sphere
centered on θ  with radius obtained thanks to dual gap computations.
Theorem 2 (Safe radius). For any θ P ∆X Ω and β P Rp  one has ˆθpλ Ωq P B pθ  rλ Ωpβ  θqq   for

c

rλ Ωpβ  θq “

2pPλ Ωpβq ´ Dλpθqq

λ2

 

i.e.  the aforementioned ball is a safe region for the Sparse-Group Lasso problem.

Proof. The result holds thanks to strong concavity of the dual objective  cf. Appendix C.

3.2.2 Computation of the center

In GAP safe screening rules  the screening test relies crucially on the ability to compute a vector
that belongs to the dual feasible set ∆X Ω. The geometry of this set is illustrated in Figure 1.
Following [3]  we leverage the primal/dual link-equation (3) to construct a dual point based on a
current approximation β of ˆβpλ Ωq. When β “ βλ1
is obtained as an approximation for a previous
value of λ1 ‰ λ we call such a strategy sequential screening. When β “ βk is the primal value at
iteration k obtained by an iterative algorithm  we call this dynamical screening. Starting from a
residual ρ “ y ´ Xβ  one can create a dual feasible point by choosing 1:

θ “

ρ

maxpλ  ΩDpXJρqq .

(9)
We refer to the sets Bpθ  rλ Ωpβ  θqq as GAP safe spheres. Note that the generalization to any smooth
data ﬁtting term would be straightforward see [15].s
Remark 3. Recall that λ ě λmax yields ˆβpλ Ωq “ 0  in which case ρ :“ y ´ X ˆβpλ Ωq “ y is the
optimal residual and y{λmax is the dual solution. Thus  as for getting λmax “ ΩDpXJyq  the scaling
computation in (9) requires a dual norm evaluation.

1We have used a simpler scaling w.r.t. [2] choice’s (without noticing much difference in practice): θ “ sρ

”

´

where s “ min

max

ρJy
λ(cid:107)ρ(cid:107)2  

´1

ΩDpXJρq

¯

ı

 

1

ΩDpXJρq

.

4

Algorithm 1 Computation of Λpx  α  Rq.

Input:
x “ px1  . . .   xdqJ P Rd  α P r0  1s  R ě 0
Output: Λpx  α  Rq
if α “ 0 and R “ 0 then
else if α “ 0 and R ‰ 0 then
else if R “ 0 then
else

Λpx  α  Rq “ 8
Λpx  α  Rq “ (cid:107)x(cid:107){R
!
Λpx  α  Rq “ (cid:107)x(cid:107)8{α
Get I :“
i P rds : |xi| ą α(cid:107)x(cid:107)8
α`R
nI :“ CardpIq
Sort xp1q ě xp2q ě ¨¨¨ ě xpnIq

)

3.3 Convergence of the active set

S0 “ xp0q  S
for k P rnI ´ 1s do

0 “ x2p0q  a0 “ 0
p2q
k “ S
p2q

Sk “ Sk´1 ` xpkq; S
p2q
ak`1 “ S
´ 2 Sk
k
x2pk`1q
α2 P rak  ak`1r then
if R2
j0 “ k ` 1
break

k´1 ` x2pkq
p2q
xpk`1q ` k ` 1

if α2j0 ´ R2 “ 0 then
Λpx  α  Rq “ S2
j0
2αSj0

c
else
Λpx  α  Rq “ αSj0´

´S

p2q
α2S2
j0
j0
α2j0´R2

pα2j0´R2q

The next proposition states that the sequence of dual feasible points obtained from (9) converges to the
dual solution ˆθpλ Ωq if pβkqkPN converges to an optimal primal solution ˆβpλ Ωq (proof in Appendix).
It guarantees that the GAP safe spheres Bpθk  rλ Ωpβk  θkqq are converging safe regions in the sense
introduced by [10]  since by strong duality limkÑ8 rλ Ωpβk  θkq “ 0.
Proposition 2. If limkÑ8 βk “ ˆβpλ Ωq  then limkÑ8 θk “ ˆθpλ Ωq.
ď
For any safe region R  i.e.  a set containing ˆθpλ Ωq  we deﬁne two levels of active sets  one for the
group level and one for the feature level:
AgppRq :“ tg P G  max
θPR

tj P g : max

θPR |XJ

!
If one considers sequence of converging regions  then the next proposition (whose proof in Appendix)
states that we can identify in ﬁnite time the optimal active sets deﬁned as follows:
j P g : |XJ

!
g P G :

ˆθpλ Ωq| ě τ

Egp :“

.

j

j θ| ě τu.
)

g θq(cid:13)(cid:13) ě p1 ´ τqwgu  AftpRq :“
(cid:13)(cid:13)SτpXJ
ď
(cid:13)(cid:13)(cid:13)SτpXJ
ˆθpλ Ωqq(cid:13)(cid:13)(cid:13) “ p1 ´ τqwg

  Eft :“

)

g

gPEgp

gPAgppRq

Proposition 3. Let pRkqkPN be a sequence of safe regions whose diameters converge to 0. Then 
kÑ8AgppRkq “ Egp and lim
lim
4 Properties of the Sparse-Group Lasso

kÑ8AftpRkq “ Eft.

To apply our safe rule  we need to be able to evaluate the dual norm ΩD efﬁciently. We describe such
as step hereafter along with some useful properties of the norm Ω. Such evaluations are performed
multiple times during the algorithm  motivating the derivation of an efﬁcient algorithm  as presented
in Algorithm 1.

4.1 Connections with -norms
ř
Here  we establish a link between the Sparse-Group Lasso norm Ω and the -norm (denoted (cid:107)¨(cid:107))
introduced in [6]. For any  P r0  1s and x P Rd  (cid:107)x(cid:107) is deﬁned as the unique nonnegative solution
i“1p|xi| ´ p1 ´ qνq2` “ pνq2  ((cid:107)x(cid:107)0 :“ (cid:107)x(cid:107)8). Using soft-thresholding  this
ν of the equation
i“1 Sp1´qνpxiq2 “ (cid:107)Sp1´qνpxq(cid:107)2 “ pνq2. Moreover  the
is equivalent to solve in ν the equation
 “ (cid:107)y(cid:107)D ` p1 ´ q(cid:107)y(cid:107)D8 “ (cid:107)y(cid:107) ` p1 ´ q(cid:107)y(cid:107)1. Now
dual norm of the -norm is given by2: (cid:107)y(cid:107)D
we can express the Sparse-Group Lasso norm Ω in term of the dual -norm and derive some basic
properties.

ř

d

d

2see [7  Eq. (42)] or Appendix

5

Proposition 4. For all groups g in G  let us introduce g :“ p1´τqwg
τ`p1´τqwg
Lasso norm satisﬁes the following properties: for any β and ξ in Rp

ÿ
(cid:32)
pτ ` p1 ´ τqwgq(cid:107)βg(cid:107)D
gPG
ξ P Rp : @g P G (cid:107)Sτpξgq(cid:107) ď p1 ´ τqwg

and

g

 

(

Ωpβq “
BΩD “

. Then  the Sparse-Group

(cid:107)ξg(cid:107)g

τ ` p1 ´ τqwg

 

(10)

ΩDpξq “ max
gPG

.

(11)
The sub-differential at β reads BΩpβq “ tz P Rp : @g P G  zg P τB(cid:107)¨(cid:107)1pβgq ` p1 ´ τqwgB(cid:107)¨(cid:107)pβgqu .
We obtain from the characterization of the unit dual ball (11) that for the Sparse-Group Lasso  any
dual feasible point θ P ∆X Ω veriﬁes: @g P G  XJ
From the dual norm formulation (10)  a vector θ P Rn is feasible if and only if ΩDpXJθq ď 1 
i.e.  @g P G (cid:107)XJ
g θ(cid:107)g ď τ ` p1 ´ τqwg. Hence we deduce from (11) a new characterization of the
dual feasible set: ∆X Ω “

g θ P p1 ´ τqwgB ` τB8.
(

(cid:32)
θ P Rn : @g P G (cid:107)XJ

g θ(cid:107)g ď τ ` p1 ´ τqwg

.

4.2 Efﬁcient computation of the dual norm

The following proposition shows how to compute the dual norm of the Sparse-Group Lasso (and the
-norm). This is turned into an efﬁcient procedure in Algorithm 1 (see the Appendix for details).
Proposition 5. For α P r0  1s  R ě 0 and x P Rd  the equation
i“1 Sναpxiq2 “ pνRq2 has
a unique solution ν :“ Λpx  α  Rq P R`  that can be computed in Opd log dq operations in the
worst case. With nI “ Cardti P rds : |xi| ą α(cid:107)x(cid:107)8{pα ` Rqu  the complexity of Algorithm 1 is
nI ` nI logpnIq  which is comparable to the ambient dimension d.
Thanks to Remark 2  we can explicit the critical parameter λmax for the Sparse-Group Lasso that is

d

ř

and get a dual feasible point (9)  since ΩDpXJρq “ maxgPG ΛpXJ

λmax “ max
gPG

g y  1 ´ g  gq
ΛpXJ
τ ` p1 ´ τqwg

“ ΩDpXJyq 

(12)
g ρ  1 ´ g  gq{pτ ` p1 ´ τqwgq.

5

Implementation

In this section we provide details on how to solve the Sparse-Group Lasso primal problem  and how
we apply the GAP safe screening rules. We focus on the block coordinate iterative soft-thresholding
algorithm (ISTA-BC); see [16]. This algorithm requires a block-wise Lipschitz gradient condition
on the data ﬁtting term fpβq “ (cid:107)y ´ Xβ(cid:107)2{2. For our problem (1)  one can show that for all
2 (where (cid:107)¨(cid:107)2 is the spectral norm of a matrix) is a suitable block-wise
group g in G  Lg “ (cid:107)Xg(cid:107)2
Lipschitz constant. We deﬁne the block coordinate descent algorithm according to the Majorization-
`
`
˘
Minimization principle: at each iteration l  we choose (e.g.  cyclically) a group g and the next
˘˘
`
g “ arg minβgPRng(cid:107)βg ´
g1 “ βl
iterate βl`1 is deﬁned such that βl`1
λ{Lg  where we denote for all g in G  αg :“
τ(cid:107)βg(cid:107)1 `p1´ τqwg(cid:107)βg(cid:107)
g ´ ∇gfpβlq{Lg
βl
λ{Lg. This can be simpliﬁed to βl`1
g “ S gpp1´τqωgαg
g ´ ∇gfpβlq{Lg
Sτ αg
. The expensive
βl
computation of the dual gap is not performed at each pass over the data  but only every f ce pass (in
practice f ce “ 10 in all our experiments). A pseudo code is given in Appendix G.

g1 if g1 ‰ g and otherwise βl`1

(cid:107)2{2`

˘

`

6 Experiments

In this section we present our experiments and illustrate the numerical beneﬁt of screening rules for
the Sparse-Group Lasso.

6.1 Experimental settings and methods compared

We have run our ISTA-BC algorithm 3 to obtain the Sparse-Group Lasso estimator for a non-increasing
sequence of T regularization parameters pλtqtPrT´1s deﬁned as follows: λt :“ λmax10´δpt´1q{pT´1q.

3The source code can be found in https://github.com/EugeneNdiaye/GAPSAFE_SGL.

6

Figure 2: Experiments on a synthetic dataset (ρ “ 0.5  γ1 “ 10  γ2 “ 4  τ “ 0.2).
(a) Proportion of active variables  i.e.  variables not safely eliminated  as a function of parameters pλtq
and the number of iterations K. More red  means more variables eliminated and better screening. (b)
Time to reach convergence w.r.t the accuracy on the duality gap  using various screening strategies.

By default  we choose δ “ 3 and T “ 100  following the standard practice when running cross-
wg “ ?
validation using sparse models (see R glmnet package [11]). The weights are always chosen as

ng (as in [17]).

We also provide a natural extension of the previous safe rules [9  21  3] to the Sparse-Group
Lasso for comparisons (please refer to Appendix D for more details). The static safe region
[9] is given by B py{λ (cid:107)y{λmax ´ y{λ(cid:107)q. The corresponding dynamic safe region [3]) is given by
B py{λ (cid:107)θk ´ y{λ(cid:107)q  where pθkqkPN is a sequence of dual feasible points obtained by dual scaling;
cf. Equation (9). The DST3  is an improvement of the preceding safe region  see [21  3]  that we
adapted to the Sparse-Group Lasso. The GAP safe sequential rules corresponds to using only
GAP Safe spheres whose centers are the (last) dual point output by the solver for a former value
of λ in the path. The GAP safe rules corresponds to performing our strategy both sequentially and
dynamically. Presenting the sequential rule allows to measure the beneﬁts due to sequential rules and
to the dynamic rules.
We now demonstrate the efﬁciency of our method in both synthetic (Fig. (2)) and real datasets
(Fig. 6.2). For comparison  we report computation times to reach convergence up to a certain
tolerance on the duality gap for all the safe rules considered.
Synthetic dataset: We use a common framework [19  20] based on the model y “ Xβ ` 0.01ε
where ε „ Np0  Idnq  X P Rnˆp follows a multivariate normal distribution such that @pi  jq P
rps2  corrpXi  Xjq “ ρ|i´j|. We ﬁx n “ 100 and break randomly p “ 10000 in 1000 groups of
size 10 and select γ1 groups to be active and the others are set to zero. In each selected groups  γ2
coordinates are drawn with rβgsj “ signpξq ˆ U for U is uniform in r0.5  10sq  ξ uniform in r´1  1s.
Real dataset: NCEP/NCAR Reanalysis 1 [14] The dataset contains monthly means of climate data
measurements spread across the globe in a grid of 2.5˝ ˆ 2.5˝ resolutions (longitude and latitude
144ˆ73) from 1948{1{1 to 2015{10{31 . Each grid point constitutes a group of 7 predictive variables
(Air Temperature  Precipitable water  Relative humidity  Pressure  Sea Level Pressure  Horizontal
Wind Speed and Vertical Wind Speed) whose concatenation across time constitutes our design matrix
X P R814ˆ73577. Such data have therefore a natural group structure.
In our experiments  we considered as target variable y P R814  the values of Air Temperature in a
neighborhood of Dakar. Seasonality and trend are ﬁrst removed  as usually done in climate analysis
for bias reduction in the regression estimates. Similar data has been used in [8]  showing that the
Sparse-Group Lasso estimator is well suited for prediction in climatology. Indeed  thanks to the
sparsity structure  the estimates delineate via their support some predictive regions at the group level 
as well as predictive features via coordinate-wise screening.
We choose τ in the set t0  0.1  . . .   0.9  1u by splitting in 50% the observations and run a training-test
validation procedure. For each value of τ  we require a duality gap of 10´8 on the training part

7

Figure 3: Experiments on NCEP/NCAR Reanalysis
1 pn “ 814  p “ 73577q: (a) Prediction error for the
Sparse-Group Lasso path with 100 values of λ and
11 values of τ (best : τ‹ “ 0.4). (b) Time to reach
convergence controlled by duality gap (for whole path
pλtqtPrTs with δ “ 2.5 and τ‹ “ 0.4). (c) Active groups
to predict Air Temperature in a neighborhood of Dakar
(in blue). Cross validation was run over 100 values for
λ’s and 11 for τ’s. At each location  the highest absolute
value among the seven coefﬁcients is displayed.

and pick the best one in term of prediction accuracy on the test part. The result is displayed in
Figure 6.2.(a). We ﬁxed δ “ 2.5 for the computational time benchmark in Figure 6.2.(b)

6.2 Performance of the screening rules

In all our experiments  we observe that our proposed GAP Safe rule outperforms the other rules
in term of computation time. On Figure 2.(c)  we can see that we need 65s to reach convergence
whereas others rules need up to 212s at a precision of 10´8. A similar performance is observed on
the real dataset (Figure 6.2) where we obtain up to a 5x speed up over the other rules. The key reason
behind this performance gain is the convergence of the GAP Safe regions toward the dual optimal
point as well as the efﬁcient strategy to compute the screening rule. As shown in the results presented
on Figure 2  our method still manages to screen out variables when λ is small. It corresponds to low
regularizations which lead to less sparse solutions but need to be explored during cross-validation.
In the climate experiments  the support map in Figure 6.2.(c) shows that the most important coefﬁ-
cients are distributed in the vicinity of the target region (in agreement with our intuition). Nevertheless 
some active variables with small coefﬁcients remain and cannot be screened out.
Note that we do not compare our method to the TLFre [20]  since this sequential rule requires the
exact knowledge of the dual optimal solution which is not available in practice. As a consequence 
one may discard active variables which can prevent the algorithm from converging as shown in [15].

7 Conclusion

The recent GAP safe rules introduced have shown great improvements  for a wide range of regularized
regression  in the reduction of computing time  especially in high dimension. To apply such GAP
safe rules to the Sparse-Group Lasso  we have proposed a new description of the dual feasible set
by establishing connections between the Sparse-Group Lasso norm and -norms. This geometrical
connection has helped providing an efﬁcient algorithm to compute the dual norm and dual feasible
points  bottlenecks for applying the GAP Safe rules. Extending GAP safe rules on general hierarchical
regularizations  is a possible direction for future research.
Acknowledgments: this work was supported by the ANR THALAMEEG ANR-14-NEUC-0002-
01  the NIH R01 MH106174  by ERC Starting Grant SLAB ERC-YStG-676943 and by the Chair
Machine Learning for Big Data at Télécom ParisTech.

8

a)b)References
[1] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Machine Learning 

73(3):243–272  2008.

[2] A. Bonnefoy  V. Emiya  L. Ralaivola  and R. Gribonval. A dynamic screening principle for the lasso. In

EUSIPCO  2014.

[3] A. Bonnefoy  V. Emiya  L. Ralaivola  and R. Gribonval. Dynamic Screening: Accelerating First-Order

Algorithms for the Lasso and Group-Lasso. IEEE Trans. Signal Process.  63(19):20  2015.

[4] J. M. Borwein and A. S. Lewis. Convex analysis and nonlinear optimization. Springer  New York  second

edition  2006.

[5] P. Bühlmann and S. van de Geer. Statistics for high-dimensional data. Springer Series in Statistics.

Springer  Heidelberg  2011. Methods  theory and applications.

[6] O. Burdakov. A new vector norm for nonlinear curve ﬁtting and some other optimization problems. 33.
Int. Wiss. Kolloq. Fortragsreihe "Mathematische Optimierung | Theorie und Anwendungen"  pages 15–17 
1988.

[7] O. Burdakov and B. Merkulov. On a new norm for data ﬁtting and optimization problems. Linköping

University  Linköping  Sweden  Tech. Rep. LiTH-MAT  2001.

[8] S. Chatterjee  K. Steinhaeuser  A. Banerjee  S. Chatterjee  and A. Ganguly. Sparse group lasso: Consistency

and climate applications. In SIAM International Conference on Data Mining  pages 47–58  2012.

[9] L. El Ghaoui  V. Viallon  and T. Rabbani. Safe feature elimination in sparse supervised learning. J. Paciﬁc

Optim.  8(4):667–698  2012.

[10] O. Fercoq  A. Gramfort  and J. Salmon. Mind the duality gap: safer rules for the lasso. In ICML  pages

333–342  2015.

[11] J. Friedman  T. Hastie  H. Höﬂing  and R. Tibshirani. Pathwise coordinate optimization. Ann. Appl. Stat. 

1(2):302–332  2007.

[12] R. Jenatton  J. Mairal  G. Obozinski  and F. Bach. Proximal methods for hierarchical sparse coding. J.

Mach. Learn. Res.  12:2297–2334  2011.

[13] T. B. Johnson and C. Guestrin. Blitz: A principled meta-algorithm for scaling sparse optimization. In

ICML  pages 1171–1179  2015.

[14] E. Kalnay  M. Kanamitsu  R. Kistler  W. Collins  D. Deaven  L. Gandin  M. Iredell  S. Saha  G. White 
J. Woollen  et al. The NCEP/NCAR 40-year reanalysis project. Bulletin of the American meteorological
Society  77(3):437–471  1996.

[15] E. Ndiaye  O. Fercoq  A. Gramfort  and J. Salmon. GAP safe screening rules for sparse multi-task and

multi-class models. NIPS  pages 811–819  2015.

[16] Z. Qin  K. Scheinberg  and D. Goldfarb. Efﬁcient block-coordinate descent algorithms for the group lasso.

Mathematical Programming Computation  5(2):143–169  2013.

[17] N. Simon  J. Friedman  T. Hastie  and R. Tibshirani. A sparse-group lasso. J. Comput. Graph. Statist. 

22(2):231–245  2013.

[18] R. Tibshirani. Regression shrinkage and selection via the lasso. JRSSB  58(1):267–288  1996.

[19] R. Tibshirani  J. Bien  J. Friedman  T. Hastie  N. Simon  J. Taylor  and R. J. Tibshirani. Strong rules for

discarding predictors in lasso-type problems. JRSSB  74(2):245–266  2012.

[20] J. Wang and J. Ye. Two-layer feature reduction for sparse-group lasso via decomposition of convex sets.

arXiv preprint arXiv:1410.4210  2014.

[21] Z. J. Xiang  H. Xu  and P. J. Ramadge. Learning sparse representations of high dimensional data on large

scale dictionaries. In NIPS  pages 900–908  2011.

[22] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. JRSSB 

68(1):49–67  2006.

9

,Tapani Raiko
Yao Li
Kyunghyun Cho
Yoshua Bengio
Eugene Ndiaye
Olivier Fercoq
Alexandre Gramfort
Joseph Salmon