2019,High-Dimensional Optimization in Adaptive Random Subspaces,We propose a new randomized optimization method for high-dimensional problems which can be seen as a generalization of coordinate descent to random subspaces. We show that an adaptive sampling strategy for the random subspace significantly outperforms the oblivious sampling method  which is the common choice in the recent literature. The adaptive subspace can be efficiently generated by a correlated random matrix ensemble whose statistics mimic the input data. We prove that the improvement in the relative error of the solution can be tightly characterized in terms of the spectrum of the data matrix  and provide probabilistic upper-bounds. We then illustrate the consequences of our theory with data matrices of different spectral decay. Extensive experimental results show that the proposed approach offers significant speed ups in machine learning problems including logistic regression  kernel classification with random convolution layers and shallow neural networks with rectified linear units. Our analysis is based on convex analysis and Fenchel duality  and establishes connections to sketching and randomized matrix decompositions.,High-Dimensional Optimization in Adaptive Random

Subspaces

Department of Electrical Engineering

Department of Electrical Engineering

Mert Pilanci

Stanford University

Jonathan Lacotte

Stanford University

lacotte@stanford.edu

Marco Pavone

Department of Aeronautics &Astronautics

Stanford University

Abstract

We propose a new randomized optimization method for high-dimensional problems
which can be seen as a generalization of coordinate descent to random subspaces.
We show that an adaptive sampling strategy for the random subspace signiﬁcantly
outperforms the oblivious sampling method  which is the common choice in the
recent literature. The adaptive subspace can be efﬁciently generated by a correlated
random matrix ensemble whose statistics mimic the input data. We prove that
the improvement in the relative error of the solution can be tightly characterized
in terms of the spectrum of the data matrix  and provide probabilistic upper-
bounds. We then illustrate the consequences of our theory with data matrices of
different spectral decay. Extensive experimental results show that the proposed
approach offers signiﬁcant speed ups in machine learning problems including
logistic regression  kernel classiﬁcation with random convolution layers and shallow
neural networks with rectiﬁed linear units. Our analysis is based on convex analysis
and Fenchel duality  and establishes connections to sketching and randomized
matrix decomposition.

1

Introduction

Random Fourier features  Nystrom method and sketching techniques have been successful in large
scale machine learning problems. The common practice is to employ oblivious sampling or sketching
matrices  which are typically randomized and ﬁxed ahead of the time. However  it is not clear
whether one can do better by adapting the sketching matrices to data. In this paper  we show that
adaptive sketching matrices can signiﬁcantly improve the approximation quality. We characterize the
approximation error on the optimal solution in terms of the smoothness of the function  and spectral
properties of the data matrix.
Many machine learning problems end up being high dimensional optimization problems  which
typically follow from forming the kernel matrix of a large dataset or mapping the data trough a high
dimensional feature map  such as random Fourier features [20] or convolutional neural networks [13].
Such high dimensional representations induce higher computational and memory complexities  and
result in slower training of the models. Random projections are a classical way of performing
dimensionality reduction  and are widely used in many algorithmic contexts [25]. Nevertheless  only
recently these methods have captured great attention as an effective way of performing dimensionality
reduction in convex optimization. In the context of solving a linear system Ax = b and least-squares
optimization  the authors of [14] propose a randomized iterative method with linear convergence rate 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

2

which  at each iteration  performs a proximal update x(k+1) = argminx∈T (cid:107)x − x(k)(cid:107)
2  where the
next iterate x(k+1) is restricted to lie within an afﬁne subspace T = x(k) + range(A(cid:62)S)  and S is a
n×m dimension-reduction matrix with m (cid:54) min{n  d}. In the context of kernel ridge regression  the
authors of [31] propose to approximate the n-dimensional kernel matrix by sketching its columns to a
∈ Rm  they show how to reconstruct an approximation(cid:101)x ∈ Rn of the high dimensional
lower m-dimensional subspace  chosen uniformly at random. From the low dimensional kernel ridge
solution α∗
spectral properties of the kernel matrix K –  the estimate(cid:101)x retains some statistical properties of x∗ 
solution x∗
∈ Rn. Provided that the sketching dimension m is large enough – as measured by the
e.g.  minimaxity. Similarly  in the broader context of classiﬁcation through convex loss functions 
the authors of [32  33] propose to project the d-dimensional features of a given data matrix A to
∈ Rm  their algorithm returns an estimate(cid:101)x ∈ Rd of the optimal
a lower m-dimensional subspace  chosen independently of the data. After computing the optimal
∈ Rd. Even though they provide formal guarantees on the estimation error (cid:107)(cid:101)x − x∗
low-dimensional classiﬁer α∗
classiﬁer x∗
(cid:107)2 
their results rely on several restrictive assumptions  that is  the data matrix A must be low rank  or  the
classiﬁer x∗ must lie in the span of the top few left singular vectors of A. Further  random subspace
optimization has also been explored for large-scale trust region problems [27]  also using a subspace
chosen uniformly at random. Our proposed approach draws connections with the Gaussian Kaczmarz
method proposed in [14] and the kernel sketching method in [31]. Differently  we are interested in
smooth  convex optimization problems with ridge regularization. In contrast to [32  33]  we do not
make any assumption on the optimal solution x∗.
Our work relates to the considerable amount of literature on randomized approximations of high
dimensional kernel matrices K. The typical approach consists of building a low-rank factorization of
the matrix K  using a random subset of its columns [28  23  9  17]. The so-called Nystrom method
has proven to be effective empirically [30]  and many research efforts have been devoted to improving
and analyzing the performance of its many variants (e.g.  uniform column sub-sampling  leverage-
score based sampling)  especially in the context of kernel ridge regression [1  2]. In a related vein 
sketching methods have been proposed to reduce the space complexity of storing high-dimensional
data matrices [19  31]  by projecting their rows to a randomly chosen lower dimensional subspace.
Our theoretical ﬁndings build on known results for low-rank factorization of positive semi-deﬁnite
(p.s.d.) matrices [15  5  12  29]  and show intimate connections with kernel matrices sketching [31].
Lastly  our problem setting also draws connections with compressed sensing [7] where the goal is
to recover a high dimensional structured signal from a small number of randomized  and usually
oblivious  measurements.

1.1 Contributions

In this work  we propose a novel randomized subspace optimization method with strong solution
approximation guarantees which outperform oblivious sampling methods. We derive probabilistic
bounds on the error of approximation for general convex functions. We show that our method provides
a signiﬁcant improvement over the oblivious version  and theoretically quantify this observation as
function of the spectral properties of the data matrix. We also introduce an iterative version of our
method  which converges to the optimal solution by iterative reﬁnement.

1.2 An overview of our results
Let f : Rn → R be a convex and µ-strongly smooth function  i.e.  ∇2f (w) (cid:22) µIn for all w ∈ Rn 
and A ∈ Rn×d a high-dimensional matrix. We are interested in solving the primal problem

x∗ = argmin
x∈Rd

λ

2(cid:107)x(cid:107)2
2  

(1)
Given a random matrix S ∈ Rd×m with m (cid:28) d  we consider instead the sketched primal problem
(2)

α(cid:62)S(cid:62)Sα  

f (ASα) +

f (Ax) +

α∗

∈ argmin
α∈Rm

λ
2

where we effectively restrict the optimization domain to a lower m-dimensional subspace. In this
work  we explore the following questions: How can we estimate the original solution x∗ given the
sketched solution α∗? Is a uniformly random subspace the optimal choice  e.g.  S ∼ Gaussian i.i.d.?

2

(cid:107)2

(cid:107)2/(cid:107)x∗

Or  can we come up with an adaptive sampling distribution that is related to the matrix A  which
yields stronger guarantees?
by(cid:101)x = −λ−1A(cid:62)
By Fenchel duality analysis  we exhibit a natural candidate for an approximate solution to x∗  given
matrix of the form S = A(cid:62)(cid:101)S where (cid:101)S is typically Gaussian i.i.d.  the relative error satisﬁes a high-
∇f (ASα∗). Our main result (Section 2) establishes that  for an adaptive sketching
probability guarantee of the form (cid:107)(cid:101)x − x∗
(cid:54) ε  with ε < 1. Our error bound ε depends on
the smoothness parameter µ  the regularization parameter λ  the shape of the domain of the Fenchel
conjugate f∗ and the spectral decay of the matrix A. Further  we show that this error can be explicitly
the adaptive matrix S = A(cid:62)(cid:101)S provides much stronger guarantees than oblivious sketching  where
controlled in terms of the singular values of A  and we derive concrete bounds for several standard
spectral proﬁles  which arise in data analysis and machine learning. In particular  we show that using
a higher precision estimate(cid:101)x(T ) that satisﬁes (cid:107)(cid:101)x(T ) − x∗
we specialize our formal guarantees and empirical evaluations (Section 5) to Gaussian matrices (cid:101)S 
broader class of matrices (cid:101)S  such as Rademacher matrices  sub-sampled randomized Fourier (SRFT)

S is independent of A. Then  we take advantage of the error contraction (i.e.  ε < 1)  and extend
our adaptive sketching scheme to an iterative version (Section 3)  which  after T iterations  returns
(cid:54) εT . Throughout this work 

which is a standard choice and yields the tightest error bounds. However  our approach extends to a

or Hadamard (SRHT) transforms  and column sub-sampling matrices. Thus  it provides a general
framework for random subspace optimization with strong solution guarantees.

(cid:107)2/(cid:107)x∗

(cid:107)2

2 Convex optimization in adaptive random subspaces
We introduce the Fenchel conjugate of f  deﬁned as f∗(z) := supw∈Rn
the relative error (cid:107)(cid:101)x − x∗
convex and its domain domf∗ := {z ∈ Rn | f∗(z) < +∞} is a closed  convex set. Our control of
(cid:107)2 is closely tied to controlling a distance between the respective
solutions of the dual problems of (1) and (2). The proof of the next two Propositions follow from
standard convex analysis arguments [22]  and are deferred to Appendix C.
Proposition 1 (Fenchel Duality). Under the previous assumptions on f  it holds that

(cid:8)w(cid:62)z − f (w)(cid:9)  which is

(cid:107)2/(cid:107)x∗

min

f (Ax) +

x

2 = max

z −f∗(z) −

1

2λ(cid:107)A(cid:62)z(cid:107)
2
2 .

There exist an unique primal solution x∗ and an unique dual solution z∗. Further  we have Ax∗
∈
∂f∗(z∗)  z∗ = ∇f (Ax∗) and x∗ = − 1
Proposition 2 (Fenchel Duality on Sketched Program). Strong duality holds for the sketched program

λ

2(cid:107)x(cid:107)2
λ A(cid:62)z∗.

min

α

f (ASα) +

λ

2(cid:107)Sα(cid:107)2

2 = max

y −f∗(y) −

1

2λ(cid:107)PSA(cid:62)y(cid:107)
2
2  

(cid:32)

(cid:33) 1

2

where PS = S(S(cid:62)S)†S(cid:62) is the orthogonal projector onto the range of S. There exist a sketched
primal solution α∗ and an unique sketched dual solution y∗. Further  for any solution α∗  it holds
that ASα∗
We deﬁne the following deterministic functional Zf which depends on f∗  the data matrix A and the
sketching matrix S  and plays an important role in controlling the approximation error 

∈ ∂f∗(y∗) and y∗ = ∇f (ASα∗).

sup

S A(cid:62)∆

∆∈(domf∗−z∗)

 

(3)

Zf ≡ Zf (A  S) =

∆(cid:62)AP ⊥
(cid:107)∆(cid:107)2
S = I − PS is the orthogonal projector onto range(S)

∇f (Ax∗) suggests the point (cid:101)x = −λ−1A(cid:62)
⊥. The relationship x∗ =
where P ⊥
∇f (ASα∗) as a candidate for approximat-
−λ−1A(cid:62)
ing x∗. The Fenchel dual programs of (1) and (2) only differ in their quadratic regularization term 
that (cid:107)(cid:101)x − x∗
2 and (cid:107)PSA(cid:62)y(cid:107)
(cid:107)A(cid:62)z(cid:107)
S A(cid:62)(z − y)(cid:107)2. As it holds
(cid:107)2 = λ−1(cid:107)A(cid:62)(z∗
(cid:107)2 can be controlled in
terms of the spectral norm (cid:107)P ⊥
(cid:107)2  or more sharply  in terms of the quantity Zf   which satis-
ﬁes Zf (cid:54) (cid:107)P ⊥
S A(cid:62)
(cid:107)2. We formalize this statement in our next result  which proof is deferred to
Appendix B.1.

− y∗)(cid:107)2  we show that the error (cid:107)(cid:101)x − x∗
2  which difference is tied to the quantity (cid:107)P ⊥

S A(cid:62)

2

2

2

3

2λ

(cid:54)

(cid:107)2

(cid:107)2  

f   we have

Zf(cid:107)x∗

which further implies

(cid:107)(cid:101)x − x∗
(cid:107)(cid:101)x − x∗

Theorem 1 (Deterministic bound). Let α∗ be any minimizer of the sketched program (2). Then 
under the condition λ (cid:62) 2µZ 2

(cid:114) µ
(cid:114) µ
2λ(cid:107)P ⊥
(cid:107)2(cid:107)x∗
For an adaptive sketching matrix S = A(cid:62)(cid:101)S  we rewrite (cid:107)P ⊥
2 = (cid:107)K − K(cid:101)S((cid:101)S(cid:62)K(cid:101)S)†(cid:101)S(cid:62)K(cid:107)2 
S A(cid:62)
for randomized low-rank matrix factorization in the form K(cid:101)S((cid:101)S(cid:62)K(cid:101)S)†(cid:101)S(cid:62)K of p.s.d. matrices
of matrices (cid:101)S. For conciseness  we specialize our next result to adaptive Gaussian sketching  i.e. 
(cid:101)S Gaussian i.i.d. Given a target rank k (cid:62) 2  we introduce a measure of the spectral tail of A as

where K = AA(cid:62) is p.s.d. Combining our deterministic bound (5) with known results [15  12  5]

K  we can give guarantees with high probability (w.h.p.) on the relative error for various types

S A(cid:62)

(cid:107)2 .

(cid:107)2

(4)

(5)

2

(cid:107)

(cid:54)

(cid:16)

(cid:80)ρ

(cid:17) 1

0R2

k + 1
σ2
k

j=k+1 σ2
j

2   where ρ is the rank of the matrix A and σ1 (cid:62) σ2 (cid:62) . . . (cid:62) σρ
Rk(A) =
its singular values. The proof of the next result follows from a combination of Theorem 1 and
Corollary 10.9 in [15]  and is deferred to Appendix B.2.
Corollary 1 (High-probability bound). Given k (cid:54) min(n  d)/2 and a sketching dimension m = 2k 
λ (cid:62) 2µc2

let S = A(cid:62)(cid:101)S  with (cid:101)S ∈ Rn×m Gaussian i.i.d. Then  for some universal constant c0 (cid:54) 36  provided

(cid:114) µ
k(A)  it holds with probability at least 1 − 12e−k that
Rk(A)(cid:107)x∗
(cid:16) ∆(cid:62)AP ⊥
(cid:17) 1
(cid:107)2 .
S A(cid:62)∆
Remark 1. The quantity Zf := sup∆∈(domf∗−z∗)
2 is the eigenvalue of the matrix
(cid:107)∆(cid:107)2
P ⊥
− z∗) ∩ S n−1  where S n−1 is the unit sphere
S A(cid:62)  restricted to the spherical cap K := (domf∗
in dimension n. Thus  depending on the geometry of K  the deterministic bound (4) might be much
tighter than (5)  and yield a probabilistic bound better than (6). The investigation of such a result is
left for future work.

(cid:107)(cid:101)x − x∗

(cid:54) c0

(cid:107)2

(6)

2λ

2

2.1 Theoretical predictions as a function of spectral decay

We study the theoretical predictions given by (5) on the relative error  for different spectral decays
of A and sketching methods  in particular  adaptive Gaussian sketching versus oblivious Gaussian
k the eigenvalues of AA(cid:62).
sketching and leverage score column sub-sampling [12]. We denote νk = σ2
For conciseness  we absorb µ into the eigenvalues by setting νk ≡ µνk and µ ≡ 1. This re-scaling
leaves the right-hand side of the bound (5) unchanged  and does not affect the analysis below. Then 
we assume that ν1 = O(1)  λ ∈ (νρ  ν1)  and λ → 0 as n → +∞. These assumptions are standard
in empirical risk minimization and kernel regression methods [11]  which we focus on in Sections 4
and 5. We consider three decaying schemes of practical interest. The matrix A has either a ﬁnite-rank
ρ  a κ-exponential decay where νj ∼ e−κj and κ > 0  or  a β-polynomial decay where νk ∼ j−2β
and β > 1/2. Among other examples  these decays are characteristic of various standard kernel
functions  such as the polynomial  Gaussian and ﬁrst-order Sobolev kernels [3]. Given a precision
ε > 0 and a conﬁdence level η ∈ (0  1)  we denote by mA (resp. mO  mS) a sufﬁcient dimension for
the relative error. That is  with probability at least 1 − η  it holds that (cid:107)(cid:101)x − x∗
which adaptive (resp. oblivious  leverage score) sketching yields the following (ε  η)-guarantee on
We determine mA from our probabilistic regret bound (6). For mS  using our deterministic regret
A(cid:62)(cid:101)S
when (cid:101)S is a leverage score column sub-sampling matrix. To the best of our knowledge  the tightest
bound (5)  it then sufﬁces to bound the spectral norm (cid:107)P ⊥
in terms of the eigenvalues νk 
provide an upper bound on the relative error (cid:107)(cid:101)x−x∗
bound has been given by [12] (see Lemma 5). For mO  we leverage results from [32]. The authors
(cid:107)2  when S is Gaussian i.i.d. with variance
d. It should be noted that their sketched solution α∗ is slightly different from ours. They solve
α∗ = argmin f (ASα) + (2λ)−1(cid:107)α(cid:107)2
2  whereas we do include the matrix S in the regularization

(cid:107)2 (cid:54) ε.

(cid:107)2/(cid:107)x∗

(cid:107)2/(cid:107)x∗

A(cid:62)

(cid:107)2

1

4

(cid:107)2
2.

2 ≈ (cid:107)α∗
(cid:107)2

term. One might wonder which regularizer works best when S is Gaussian i.i.d. Through extensive
numerical simulations  we observed a strongly similar performance. Further  standard Gaussian
concentration results yields that (cid:107)Sα∗
Our theoretical ﬁndings are summarized in Table 1  and we give the mathematical details of our
derivations in Appendix D. For the sake of clarity  we provide in Table 1 lower bounds on the
adaptive Gaussian sketching provides stronger guarantees on the relative error (cid:107)(cid:101)x − x∗
predicted values mO and mS  and  thus  lower bounds on the ratios mO/mA and mS/mA. Overall 
(cid:107)2/(cid:107)x∗
(cid:107)2.
Table 1: Sketching dimensions for a (ε  η)-guarantee on the relative error (cid:107)(cid:101)x − x∗
(cid:107)2/(cid:107)x∗
(cid:107)2.
(cid:16) 12
(cid:17)
(cid:16) 2d
(cid:17)
(cid:19)2∧ β
(cid:16) 1
(cid:19)−1+2∧ β

(cid:16) 12
κ−1 log(cid:0) 1
(cid:1) + log
(cid:16) 2d
κ−1ε−2 log(cid:0) 1
(cid:1) log
(cid:17)
(cid:16) 1
(cid:1) log
κ−1 log(cid:0) 1
(cid:16)
(cid:16) 1
(cid:17)
  κ−1 log(cid:0) 1

2β ε
log
ε1/β−2 log(2d/η)
− 1

Leverage score (mS)
Lower bound on mO
mA

(β > 1/2)
λ−1/2β ε−1/β + log

(cid:16) 12
(cid:17)
(cid:16) 2ρ
(cid:17)
(cid:16) 4ρ

ρ + 1 + log
(ρ + 1)ε−2 log

2β ε−2 log
− 1
β

ε−2+h log 2d  ∀h > 0

Oblivious Gaussian (mO)

ρ-rank matrix
(ρ (cid:28) n ∧ d)

Adaptive Gaussian (mA)

κ-exponential decay

β-polynomial decay

ε−2 log ρ

η

β−1

(ρ + 1) log

η

− 1
β

λ

2β ε

(cid:17)

η

λε

λ

(cid:17)

η

η

(cid:17)
(cid:17)

η

η

− 1

λ

− 1

λ

(cid:18)

(κ > 0)

λε

η

η

β−1

Lower bound on mS
mA

log ρ

min

log

η

(cid:18)

(cid:1)(cid:17)

λε

We illustrate numerically our predictions for adaptive Gaussian sketching versus oblivious Gaussian
regression  with f (Ax) = n−1(cid:80)n
sketching. With n = 1000 and d = 2000  we generate matrices Aexp and Apoly  with spectral
decay satisfying respectively νj ∼ ne−0.1j and νj ∼ nj−2. First  we perform binary logistic
i x) where (cid:96)yi(z) = yi log(1+e−z)+(1−yi) log(1+ez) 
relative error (cid:107)(cid:101)x−x∗
y ∈ {0  1}n and ai is the i-th row of A. For the polynomial (resp. exponential) decay  we expect the
(2n)−1(cid:80)n
(cid:107)2 to follow w.h.p. a decay proportional to m−1 (resp. e−0.05m). Figure 1
conﬁrms those predictions. We repeat the same experiments with a second loss function  f (Ax) =
i x)yi. The latter is a convex relaxation of the penalty 1
2 for
ﬁtting a shallow neural network with a ReLU non-linearity. Again  Figure 1 conﬁrms our predictions 
and we observe that the adaptive method performs much better than the oblivious sketch.

(cid:107)2/(cid:107)x∗
+−2(a(cid:62)

2(cid:107)(Ax)+−y(cid:107)2

i=1 (cid:96)yi(a(cid:62)

i=1(a(cid:62)

i x)2

Figure 1: Relative error versus sketching dimension m ∈ {2k | 3 (cid:54) k (cid:54) 10} of adaptive Gaussian
sketching (red) and oblivious Gaussian sketching (green)  for the ReLU and logistic models  and the
exponential and polynomial decays. We use λ = 10−4 for all simulations. Results are averaged over
10 trials. Bar plots show (twice) the empirical standard deviations.

3 Algorithms for adaptive subspace sketching

3.1 Numerical conditioning and generic algorithm

A standard quantity to characterize the capacity of a convex program to be solved efﬁciently is its
condition number [6]  which  for the primal (1) and (adaptive) sketched program (2)  are given by

(cid:0)A(cid:62)

∇2f (Ax)A(cid:1)

λ + supx σ1
λ + inf x σd (A(cid:62)∇2f (Ax)A)

κ =

 

κS =

supα σ1

inf α σd

The latter can be signiﬁcantly larger than κ  up to κS ≈ κ
variable overcomes this issue. With AS † = AS(S(cid:62)S)− 1

(cid:16)(cid:101)S(cid:62)A(λI + A(cid:62)
(cid:17)
∇2f (AA(cid:62)(cid:101)Sα)A)A(cid:62)(cid:101)S
(cid:16)(cid:101)S(cid:62)A(λI + A(cid:62)∇2f (AA(cid:62)(cid:101)Sα)A)A(cid:62)(cid:101)S
(cid:17) .
σ1((cid:101)S(cid:62)AA(cid:62)(cid:101)S)
σm((cid:101)S(cid:62)AA(cid:62)(cid:101)S) (cid:29) κ. A simple change of

2   we solve instead the optimization problem

5

2528(a)ReLU–polynomial10−210005001000(b)ReLU–exponential10−410−21002528(c)Logistic–polynomial10−410−210005001000(d)Logistic–exponential10−510−310−1λ

f (AS †α†) +

2(cid:107)α†(cid:107)2
2.

α∗
† = argmin
α†∈Rm
∇f (AS †α∗

It holds that (cid:101)x = −λ−1A(cid:62)
†). The additional complexity induced by this change of
variables comes from computing the (square-root) pseudo-inverse of S(cid:62)S  which requires O(m3)
ﬂops via a singular value decomposition. When m is small  this additional computation is negligible
and numerically stable  and the re-scaled sketched program (7) is actually better conditioned that the
original primal program (1)  as stated in the next result that we prove in Appendix C.3.
Proposition 3. Under adaptive sketching  the condition number κ† of the re-scaled sketched pro-
gram (7) satisﬁes κ† (cid:54) κ with probability 1.

(7)

Algorithm 1: Generic algorithm for adaptive sketching.

Input :Data matrix A ∈ Rn×d  random matrix (cid:101)S ∈ Rn×m and parameter λ > 0.
1 Compute the sketching matrix S = A(cid:62)(cid:101)S  and  the sketched matrix AS = AS.
2 Compute the re-scaling matrix R =(cid:0)S(cid:62)S(cid:1)− 1
(cid:17)
3 Solve the convex optimization problem (7)  and return(cid:101)x = − 1

AS †α∗
†

λ A(cid:62)

∇f

(cid:16)

.

2   and the re-scaled sketched matrix AS † = ASR.

2(cid:107)SQα(cid:107)2

∇f (ASQα∗
Q).

3.2 Error contraction and almost exact recovery of the optimal solution

We observed a drastic practical performance improvement between solving the sketched program as
formulated in (2) and its well-conditioned version (7).
If the chosen sketch dimension m is itself prohibitively large for computing the matrix Q = (S(cid:62)S)− 1
2  
one might consider a pre-conditioning matrix Q  which is faster to compute  and such that the
matrix SQ is well-conditioned. Typically  one might compute a matrix Q based on an approximate
singular value decomposition of the matrix S(cid:62)S. Then  one solves the optimization problem
α∗
Q = argminα∈Rm f (ASQα) + λ

2. Provided that Q is invertible  it holds that(cid:101)x satisﬁes

(cid:107)2 w.h.p.  and  with ε < 1 provided
that λ is large enough. Here  we extend Algorithm 1 to an iterative version which takes advantage of

(cid:101)x = −λ−1A(cid:62)
The estimate(cid:101)x satisﬁes a guarantee of the form (cid:107)(cid:101)x− x∗
this error contraction  and which is relevant when a high-precision estimate(cid:101)x is needed.
Input :Data matrix A ∈ Rn×d  random matrix (cid:101)S ∈ Rn×m  iterations number T   parameter λ > 0.
1 Compute the sketched matrix AS † as in Algorithm 1. Set(cid:101)x(0) = 0.
Compute a(t) = A(cid:101)x(t−1)  and  b(t) =(cid:0)S(cid:62)S(cid:1)− 1
2 S(cid:62)(cid:101)x(t−1).

Algorithm 2: Iterative adaptive sketching

(cid:107)2 (cid:54) ε(cid:107)x∗

2 for t = 1  2  . . .   T do
3
4

Solve the following convex optimization problem

f (AS †α† + a(t)) +

λ
2(cid:107)α† + b(t)(cid:107)2
2 .

α(t)† = argmin
α†∈Rm
λ A(cid:62)

5 end

Update the solution by(cid:101)x(t) = − 1
6 Return the last iterate(cid:101)x(T ).
matrix AS † has to be computed only once  at the beginning of the procedure. The output(cid:101)x(T )

A key advantage is that  at each iteration  the same sketching matrix S is used. Thus  the sketched

∇f (AS †α(t)† + a(t)).

satisﬁes the following recovery property  which empirical beneﬁts are illustrated in Figure 2.
Theorem 2. After T iterations of Algorithm 2  provided that λ (cid:62) 2µZ 2

f   it holds that

(8)

(cid:107)(cid:101)x(T ) − x∗

(cid:107)2 (cid:54)

(cid:32) µZ 2

(cid:33) T

2

f
2λ

6

(cid:107)x∗

(cid:107)2 .

(9)

Further  if S = A(cid:62)(cid:101)S where (cid:101)S ∈ Rn×m with i.i.d. Gaussian entries and m = 2k for some target
k(A)  the approximate solution(cid:101)x(T ) satisﬁes with probability at least 1 − 12e−k 

rank k (cid:62) 2  then  for some universal constant c0 (cid:54) 36  after T iterations of Algorithm 2  provided
that λ (cid:62) 2c2

0µR2

(cid:18) c2

(cid:107)(cid:101)x(T ) − x∗

(cid:107)2 (cid:54)

(cid:19) T

2

k(A)

0µR2
2λ

(cid:107)x∗

(cid:107)2 .

(10)

Figure 2: Relative error versus sketching dimension m ∈ {2k | 3 (cid:54) k (cid:54) 10} of adaptive Gaussian
sketching for (a) the iterative method (Algorithm 2) and (b) the power method (see Remark 2). We use
the MNIST dataset with images mapped through 10000-dimensional random Fourier features [20]
for even-vs-odd classiﬁcation using binary logistic loss  and  λ = 10−5. Results are averaged over 20
trials. Bar plots show (twice) the empirical standard deviations.

Remark 2. An immediate extension of Algorithms 1 and 2 consists in using the power method [15].
Given q ∈ N  one uses the sketching matrix S = (A(cid:62)A)
approximation error (cid:107)AA(cid:62)

A(cid:62)(cid:101)S. The larger q  the smaller the
(cid:107)2 (see Corollary 10.10 in [15]). Of pratical interest
are data matrices A with a spectral proﬁle starting with a fast decay  and then becoming ﬂat. This
happens typically for A of the form A = A + W   where A has a fast decay and W is a noise matrix
with  for instance  independent subgaussian rows [26]. Our results easily extend to this setting and
we illustrate its empirical beneﬁts in Figure 2.

− AS(S(cid:62)S)†S(cid:62)A(cid:62)

q

4 Application to empirical risk minimization and kernel methods

By the representer theorem  the primal program (1) can be re-formulated as

where K = AA(cid:62). Clearly  it holds that x∗ = A(cid:62)w∗. Given a matrix (cid:101)S with i.i.d. Gaussian entries 

∈ argmin
w∈Rn

f (Kw) +

we consider the sketched version of the kernelized primal program (11) 

(11)

w(cid:62)Kw  

λ
2

w∗

f (K(cid:101)Sα) +

α(cid:62)(cid:101)S(cid:62)K(cid:101)Sα .

λ
2

α∗

(12)

∈ argmin
α∈Rm

The sketched program (12) is exactly our adaptive Gaussian sketched program (2). Thus  setting (cid:101)w =
−λ−1∇f (K(cid:101)Sα∗)  it holds that(cid:101)x = A(cid:62)(cid:101)w. Since the relative error (cid:107)(cid:101)x − x∗
by the decay of the eigenvalues of K  so does the relative error (cid:107)A(cid:62)((cid:101)w − w∗)(cid:107)2/(cid:107)A(cid:62)w∗
(cid:107)2/(cid:107)x∗
(cid:107)2 is controlled
(cid:16)
(cid:17)
(cid:107)2. More
2(cid:101)S
generally  the latter statements are still true if K is any positive semi-deﬁnite matrix  and  if we
replace A(cid:62) by any square-root matrix of K. Here  we denote Zf ≡ Zf
(see Eq. (3)).
Theorem 3. Let K ∈ Rn×n be any positive semi-deﬁnite matrix. Let w∗ be any minimizer of the
solution (cid:101)w = − 1
kernel program (11) and α∗ be any minimizer of its sketched version (12). Deﬁne the approximate

f   then it holds that

2   K 1

K 1

λ∇f (K(cid:101)Sα∗). If λ (cid:62) 2µZ 2
2 ((cid:101)w − w∗)(cid:107)2

(cid:107)K

1

(cid:54)

Zf(cid:107)K

1

2 w∗

(cid:107)2 .

(13)

2λ

(cid:114) µ

7

242628210(b)Powermethod10−1100q=0q=1q=2242628210(a)Iterativemethod10−210−1100T=1T=2T=3For a positive deﬁnite kernel k : Rd × Rd → R and a data matrix A = [a1  . . .   an](cid:62)
∈ Rn×d 
let K be the empirical kernel matrix  with Kij = k(ai  aj). Let ϕ(·) ∈ RD be a random feature
map [20  8]  such as random Fourier features or a random convolutional neural net. We are interested
in the computational complexities of forming the sketched versions of the primal (1)  the kernel
primal (11) and the primal (1) with ϕ(A) instead of A. We compare the complexities of adaptive and
oblivious sketching and uniform column sub-sampling. Table 2 shows that all three methods have
similar complexities for computing AS and ϕ(A)S. Adaptive sketching exhibits an additional factor

practice  the latter is negligible compared to the cost of forming ϕ(A) which  for instance  corresponds
to a forward pass over the whole dataset in the case of a convolutional neural network. On the other
hand  uniform column sub-sampling is signiﬁcantly faster in order to form the sketched kernel matrix

2 that comes from computing the correlated sketching matrices S = A(cid:62)(cid:101)S and S = ϕ(A)(cid:62)(cid:101)S. In
K(cid:101)S  which relates to the well-known computational advantages of kernel Nystrom methods [30].
Table 2: Complexity of forming the sketched programs  given A ∈ Rn×d. We denote dk the number
of ﬂops to evaluate the kernel product k(a  a(cid:48))  and  dϕ the number of ﬂops for a forward-pass ϕ(a).
Note that these complexities could be reduced through parallelization.

K(cid:101)S

-

O (dknm)

O (2mdn) O (dϕn) + O (2mDn) O(cid:0)dkn2(cid:1) + O(cid:0)mn2(cid:1)

ϕ(A)S

AS

Adaptive sketching
Oblivious sketching

O (mdn)
Uniform column sub-sampling O (mdn)

O (dϕn) + O (mDn)
O (dϕn) + O (mDn)

5 Numerical evaluation of adaptive Gaussian sketching

(cid:107)2

2

−γ(cid:107)a − a(cid:48)

(cid:1) [20  18]. For MNIST and

We evaluate Algorithm 1 on MNIST and CIFAR10. First  we aim to show that the sketching
dimension can be considerably smaller than the original dimension while retaining (almost) the same
test classiﬁcation accuracy. Second  we aim to get signiﬁcant speed-ups in achieving a high-accuracy
classiﬁer. To solve the primal program (1)  we use two standard algorithms  stochastic gradient
descent (SGD) with (best) ﬁxed step size and stochastic variance reduction gradient (SVRG) [16]
with (best) ﬁxed step size and frequency update of the gradient correction. To solve the adaptive
sketched program (2)  we use SGD  SVRG and the sub-sampled Newton method [4  10] – which we
refer to as Sketch-SGD  Sketch-SVRG and Sketch-Newton. The latter is well-suited to the sketched
program  as the low-dimensional Hessian matrix can be quickly inverted at each iteration. For both
datasets  we use 50000 training and 10000 testing images. We transform each image using a random

Fourier feature map ϕ(·) ∈ RD  i.e.  (cid:104)ϕ(a)  ϕ(a(cid:48))(cid:105) ≈ exp(cid:0)
CIFAR10  we choose respectively D = 10000 and γ = 0.02  and  D = 60000 and γ = 0.002  so
that the primal is respectively 10000-dimensional and 60000-dimensional. Then  we train a classiﬁer
via a sequence of binary logistic regressions – which allow for efﬁcient computation of the Hessian
First  we evaluate the test classiﬁcation error of (cid:101)x. We solve to optimality the primal and
and implementation of the Sketch-Newton algorithm –  using a one-vs-all procedure.
sketched programs for values of λ ∈ {10−4  5 · 10−5  10−5  5 · 10−6} and sketching dimensions
m ∈ {64  128  256  512  1024}. In Table 3 are reported the results  which are averaged over 20 trials
for MNIST and 10 trials for CIFAR10  and  empirical variances are reported in Appendix A. Overall 
the adaptive sketched program yields a high-accuracy classiﬁer for most couples (λ  m). Further  we
match the best primal classiﬁer with values of m as small as 256 for MNIST and 512 for CIFAR10 
which respectively corresponds to a dimension reduction by a factor ≈ 40 and ≈ 120. These results
additionally suggest that adaptive Gaussian sketching introduces an implicit regularization effect 
which might be related to the beneﬁts of spectral cutoff estimators. For instance  on CIFAR10  using
λ = 10−5 and m = 512  we obtain an improvement in test accuracy by more than 2% compared to
value of m increases  the test classiﬁcation error of(cid:101)x increases to that of x∗  until matching it.
x∗. Further  over some sketching dimension threshold under which the performance is bad  as the
(Nystrom method) for which S = A(cid:62)(cid:101)S with(cid:101)S a column sub-sampling matrix. As reported in Table 4 

Further  we evaluate the test classiﬁcation error of two sketching baselines  that is  oblivious Gaussian
sketching for which the matrix S has i.i.d. Gaussian entries  and  adaptive column sub-sampling

adaptive Gaussian sketching performs better for a wide range of values of sketching size m and
regularization parameter λ.

8

Table 3: Test classiﬁcation error of adaptive Gaussian sketching on MNIST and CIFAR10 datasets.

λ
10−4
5 · 10−5
10−5
5 · 10−6

x∗
MNIST
5.4
4.6
2.8
2.5

(cid:101)x64

(cid:101)x128

(cid:101)x256

(cid:101)x512

(cid:101)x1024

4.8
4.1
8.1
11.8

4.8
3.8
3.4
4.9

5.2
4.0
2.4
2.8

5.3
4.3
2.5
2.6

5.4
4.5
2.8
2.4

(cid:101)x64

(cid:101)x128

(cid:101)x256

(cid:101)x512

(cid:101)x1024

-

52.1
60.1
63.6

-

50.5
54.5
59.8

-

50.6
47.7
51.9

-

50.8
45.9
47.7

-

51.0
46.2
45.8

x∗

CIFAR
-

51.6
48.2
47.6

Table 4: Test classiﬁcation error on MNIST and CIFAR10. "AG": Adaptive Gaussian sketch  "Ob":
Oblivious Gaussian sketch  "N": Nystrom method.

(cid:101)xAG
(cid:101)xAG

256

(cid:101)xAG
(cid:101)xAG

(cid:101)xOb
(cid:101)xOb

256

(cid:101)xOb
(cid:101)xOb

256

x∗
MNIST
256
4.6 % 4.0 % 4.5 % 25.2 % 8.5% 5.0 %
2.5%

1024
4.6
2.8% 2.4% 30.1% 9.4% 3.0% 2.7%

1024

1024

256

CIFAR

x∗
1024
51.6 % 50.6% 51.0% 88.2% 70.5% 55.8% 53.1%
47.6% 51.9% 45.8% 88.9% 80.1% 57.2% 55.8%

1024

1024

256

(cid:101)xN
(cid:101)xN

(cid:101)xN
(cid:101)xN

λ

5 · 10−5
5 · 10−6

λ

5 · 10−5
5 · 10−6

Then  we compare the test classiﬁcation error versus wall-clock time of the optimization algorithms
mentioned above. Figure 3 shows results for some values of m and λ. We observe some speed-ups
on the 10000-dimensional MNIST problem  in particular for Sketch-SGD and for Sketch-SVRG  for
which computing the gradient correction is relatively fast. Such speed-ups are even more signiﬁcant
on the 60000-dimensional CIFAR10 problem  especially for Sketch-Newton. A few iterations of

Sketch-Newton sufﬁce to almost reach the minimum(cid:101)x  with a per-iteration time which is relatively

small thanks to dimensionality reduction. Hence  it is more than 10 times faster to reach the best test
accuracy using the sketched program. In addition to random Fourier features mapping  we carry out
another set of experiments with the CIFAR10 dataset  in which we pre-process the images. That is 
similarly to [24  21]  we map each image through a random convolutional layer. Then  we kernelize
these processed images using a Gaussian kernel with γ = 2·10−5. Using our implementation  the best
test accuracy of the kernel primal program (11) we obtained is 73.1%. Sketch-SGD  Sketch-SVRG
and Sketch-Newton – applied to the sketched kernel program (12) – match this test accuracy  with
signiﬁcant speed-ups  as reported in Figure 3.

Figure 3: Test classiﬁcation error (percentage) versus wall-clock time (seconds).

Acknowledgements

This work was partially supported by the National Science Foundation under grant IIS-1838179 and
Ofﬁce of Naval Research  ONR YIP Program  under contract N00014-17-1-2433.

9

050100(a)MNIST m=512 λ=10−5.2510SGDSVRGSketch-SVRGSketch-NewtonSketch-SGD102103(b)CIFAR m=256 λ=10−5.485060SGDSketch-SVRGSketch-NewtonSketch-SGD102103(c)CIFAR(randomlayer) m=1024 λ=5.10−6.273040SGDSketch-SVRGSketch-NewtonSketch-SGDReferences
[1] A. Alaoui and M. W. Mahoney. Fast randomized kernel ridge regression with statistical

guarantees. In Advances in Neural Information Processing Systems  pages 775–783  2015.

[2] F. Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference on Learning

Theory  pages 185–209  2013.

[3] A. Berlinet and C. Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and

statistics. Kluwer Adademic  2004.

[4] R. Bollapragada  R. H. Byrd  and J. Nocedal. Exact and inexact subsampled newton methods

for optimization. IMA Journal of Numerical Analysis  39(2):545–578  2018.

[5] C. Boutsidis and A. Gittens. Improved matrix algorithms via the subsampled randomized
hadamard transform. SIAM Journal on Matrix Analysis and Applications  34(3):1301–1340 
2013.

[6] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press  2004.

[7] E. J. Candes  J. K. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate
measurements. Communications on Pure and Applied Mathematics: A Journal Issued by the
Courant Institute of Mathematical Sciences  59(8):1207–1223  2006.

[8] A. Coates and A. Y. Ng. Learning feature representations with k-means. In Neural networks:

Tricks of the trade  pages 561–580. Springer  2012.

[9] P. Drineas and M. W. Mahoney. On the nyström method for approximating a gram matrix for
improved kernel-based learning. journal of machine learning research  6(Dec):2153–2175 
2005.

[10] M. A. Erdogdu and A. Montanari. Convergence rates of sub-sampled newton methods. arXiv

preprint arXiv:1508.02810  2015.

[11] J. Friedman  T. Hastie  and R. Tibshirani. The elements of statistical learning  volume 1.

[12] A. Gittens and M. W. Mahoney. Revisiting the nyström method for improved large-scale

machine learning. The Journal of Machine Learning Research  17(1):3977–4041  2016.

[13] I. Goodfellow  Y. Bengio  and A. Courville. Deep learning. MIT press  2016.

[14] R. M. Gower and P. Richtárik. Randomized iterative methods for linear systems. SIAM Journal

on Matrix Analysis and Applications  36(4):1660–1690  2015.

[15] N. Halko  P.-G. Martinsson  and J. A. Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM review  53(2):217–288 
2011.

[16] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[17] S. Kumar  M. Mohri  and A. Talwalkar. Sampling methods for the nyström method. Journal of

Machine Learning Research  13(Apr):981–1006  2012.

[18] F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel 
P. Prettenhofer  R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher 
M. Perrot  and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research  12:2825–2830  2011.

[19] M. Pilanci and M. J. Wainwright. Randomized sketches of convex programs with sharp

guarantees. IEEE Transactions on Information Theory  61(9):5096–5115  2015.

10

[20] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in

neural information processing systems  pages 1177–1184  2008.

[21] B. Recht  R. Roelofs  L. Schmidt  and V. Shankar. Do cifar-10 classiﬁers generalize to cifar-10?

arXiv preprint arXiv:1806.00451  2018.

[22] R. T. Rockafellar. Convex analysis. Princeton university press  2015.

[23] A. J. Smola and B. Schölkopf. A tutorial on support vector regression. Statistics and computing 

14(3):199–222  2004.

[24] S. Tu  R. Roelofs  S. Venkataraman  and B. Recht. Large scale kernel learning using block

coordinate descent. arXiv preprint arXiv:1602.05310  2016.

[25] S. S. Vempala. The random projection method  volume 65. American Mathematical Soc.  2005.

[26] R. Vershynin. High-dimensional probability: An introduction with applications in data science 

volume 47. Cambridge University Press  2018.

[27] K. Vu  P.-L. Poirion  C. D’Ambrosio  and L. Liberti. Random projections for trust region

subproblems. arXiv preprint arXiv:1706.02730  2017.

[28] C. K. Williams and M. Seeger. Using the nyström method to speed up kernel machines. In

Advances in neural information processing systems  pages 682–688  2001.

[29] R. Witten and E. Candes. Randomized algorithms for low-rank matrix factorizations: sharp

performance bounds. Algorithmica  72(1):264–281  2015.

[30] T. Yang  Y.-F. Li  M. Mahdavi  R. Jin  and Z.-H. Zhou. Nyström method vs random fourier
features: A theoretical and empirical comparison. In Advances in neural information processing
systems  pages 476–484  2012.

[31] Y. Yang  M. Pilanci  M. J. Wainwright  et al. Randomized sketches for kernels: Fast and optimal

nonparametric regression. The Annals of Statistics  45(3):991–1023  2017.

[32] L. Zhang  M. Mahdavi  R. Jin  T. Yang  and S. Zhu. Recovering the optimal solution by dual

random projection. In Conference on Learning Theory  pages 135–157  2013.

[33] L. Zhang  M. Mahdavi  R. Jin  T. Yang  and S. Zhu. Random projections for classiﬁcation: A

recovery approach. IEEE Transactions on Information Theory  60(11):7300–7316  2014.

11

,Jonathan Lacotte
Mert Pilanci
Marco Pavone