2010,A New Probabilistic Model for Rank Aggregation,This paper is concerned with rank aggregation  which aims to combine multiple input rankings to get a better ranking. A popular approach to rank aggregation is based on probabilistic models on permutations  e.g.  the Luce model and the Mallows model. However  these models have their limitations in either poor expressiveness or high computational complexity. To avoid these limitations  in this paper  we propose a new model  which is defined with a coset-permutation distance  and models the generation of a permutation as a stagewise process. We refer to the new model as coset-permutation distance based stagewise (CPS) model. The CPS model has rich expressiveness and can therefore be used in versatile applications  because many different permutation distances can be used to induce the coset-permutation distance. The complexity of the CPS model is low because of the stagewise decomposition of the permutation probability and the efficient computation of most coset-permutation distances. We apply the CPS model to supervised rank aggregation  derive the learning and inference algorithms  and empirically study their effectiveness and efficiency. Experiments on public datasets show that the derived algorithms based on the CPS model can achieve state-of-the-art ranking accuracy  and are much more efficient than previous algorithms.,A New Probabilistic Model for Rank Aggregation

Tao Qin

Microsoft Research Asia

taoqin@microsoft.com

Xiubo Geng

Chinese Academy of Sciences
xiubogeng@gmail.com

Tie-Yan Liu

Microsoft Research Asia

tyliu@microsoft.com

Abstract

This paper is concerned with rank aggregation  which aims to combine multiple
input rankings to get a better ranking. A popular approach to rank aggregation
is based on probabilistic models on permutations  e.g.  the Luce model and the
Mallows model. However  these models have their limitations in either poor ex-
pressiveness or high computational complexity. To avoid these limitations  in this
paper  we propose a new model  which is deﬁned with a coset-permutation dis-
tance  and models the generation of a permutation as a stagewise process. We re-
fer to the new model as coset-permutation distance based stagewise (CPS) model.
The CPS model has rich expressiveness and can therefore be used in versatile ap-
plications  because many different permutation distances can be used to induce
the coset-permutation distance. The complexity of the CPS model is low because
of the stagewise decomposition of the permutation probability and the efﬁcient
computation of most coset-permutation distances. We apply the CPS model to su-
pervised rank aggregation  derive the learning and inference algorithms  and em-
pirically study their effectiveness and efﬁciency. Experiments on public datasets
show that the derived algorithms based on the CPS model can achieve state-of-
the-art ranking accuracy  and are much more efﬁcient than previous algorithms.

1 Introduction

Rank aggregation aims at combining multiple rankings of objects to generate a better ranking. It is
the key problem in many applications. For example  in meta search [1]  when users issue a query 
the query is sent to several search engines and the rankings given by them are aggregated to generate
more comprehensive ranking results.
Given the underlying correspondence between ranking and permutation  probabilistic models on
permutations  originated in statistics [19  5  4]  have been widely applied to solve the problems of
rank aggregation. Among different models  the Mallows model [15  6] and the Luce model [14  18]
are the most popular ones.
The Mallows model is a distance-based model  which deﬁnes the probability of a permutation ac-
cording to its distance to a location permutation. Due to many applicable permutation distances  the
Mallows model has very rich expressiveness  and therefore can be potentially used in many different
applications. Its weakness lies in the high computational complexity. In many cases  it requires a
time complexity of O(n!) to compute the probability of a single permutation of n objects. This is
clearly intractable when we need to rank a large number of objects in real applications.
The Luce model is a stagewise model  which decomposes the process of generating a permutation of
n objects into n sequential stages. At the k-th stage  an object is selected and assigned to position k

1

according to a probability based on the scores of the unassigned objects. The product of the selection
probabilities at all the stages deﬁnes the probability of the permutation. The Luce model is highly
efﬁcient (with a polynomial time complexity) due to the decomposition. The expressiveness of the
Luce model  however  is limited because it is deﬁned on the scores of individual objects and cannot
leverage versatile distance measures between permutations.
In this paper  we propose a new probabilistic model on permutations  which inherits the advantages
of both the Luce model and the Mallows model and avoids their limitations. We refer to the model
as coset-permutation distance based stagewise (CPS) model. Different from the Mallows model 
the CPS model is a stagewise model. It decomposes the generative process of a permutation (cid:25) into
sequential stages  which makes the efﬁcient computation possible. At the k-th stage  an object is
selected and assigned to position k with a certain probability. Different from the Luce model  the
CPS model deﬁnes the selection probability based on the distance between a location permutation
(cid:27) and the right coset of (cid:25) (referred to as coset-permutation distance) at each stage. In this sense 
it is also a distance-based model. Because many different permutation distances can be used to
induce the coset-permutation distance  the CPS model also has rich expressiveness. Furthermore 
the coset-permutation distances induced by many popular permutation distances can be computed
with polynomial time complexity  which further ensures the efﬁciency of the CPS model.
We then apply the CPS model to supervised rank aggregation and derive corresponding algorithms
for learning and inference of the model. Experiments on public datasets show that the CPS model
based algorithms can achieve state-of-the-art ranking accuracy  and are much more efﬁcient than
baseline methods based on previous probabilistic models.

2 Background

2.1 Rank Aggregation

There are mainly two kinds of rank aggregation  i.e.  score-based rank aggregation [17  16] and
order-based rank aggregation [2  7  3]. In the former  objects in the input rankings are associated
with scores  while in the latter  only the order information of these objects is available. In this work 
we focus on the order-based rank aggregation  because it is more popular in real applications [7]  and
score-based rank aggregation can be easily converted to order-based rank aggregation by ignoring
the additional score information [7].
Early methods for rank aggregation are heuristic based. For example  BordaCount [2  7] and median
rank aggregation [8] are simply based on average rank positions or the number of pairwise wins. In
the recent literature  probabilistic models on permutations  such as the Mallows model and the Luce
model  have been introduced to solve the problem of rank aggregation. Previous studies have shown
that the probabilistic model based algorithms can outperform the heuristic methods in many settings.
For example  the Mallows model has been shown very effective in both supervised rank aggregation
and unsupervised rank aggregation  and the effectiveness of the Luce model has been demonstrated
in the context of unsupervised rank aggregation. In the next subsection  we will describe these two
models in more detail.

2.2 Probabilistic Models on Permutations

In order to better illustrate the probabilistic models on permutations  we ﬁrst introduce some con-
cepts and notations.
Let {1; 2; : : : ; n} be a set of objects to be ranked. A ranking/permutation1 (cid:25) is a bijection from
{1; 2; : : : ; n} to itself. We use (cid:25)(i) to denote the position given to object i and (cid:25)
(cid:0)1(i) to denote the
(cid:0)1 as vectors whose i-th component is (cid:25)(i)
object assigned to position i. We usually write (cid:25) and (cid:25)
(cid:0)1(i)  respectively. We also use the bracket alternative notation to represent a permutation 
and (cid:25)
i.e.  (cid:25) = ⟨(cid:25)
The collection of all permutations of n objects forms a non-abelian group under composition  called
the symmetric group of order n  denoted as Sn. Let Sn(cid:0)k denote the subgroup of Sn consisting of

(cid:0)1(2); : : : ; (cid:25)

(cid:0)1(1); (cid:25)

(cid:0)1(n)⟩.

1We will interchangeably use the two terms in the paper.

2

all permutations whose ﬁrst k positions are ﬁxed:

Sn(cid:0)k = {(cid:25) ∈ Sn|(cid:25)(i) = i;∀i = 1; : : : ; k}:

(1)
The right coset Sn(cid:0)k(cid:25) = {(cid:27)(cid:25)|(cid:27) ∈ Sn(cid:0)k} is a subset of permutations whose top-k objects are
exactly the same as in (cid:25). In other words 

Sn(cid:0)k(cid:25) = {(cid:27)|(cid:27) ∈ Sn; (cid:27)

(cid:0)1(i) = (cid:25)

(cid:0)1(i);∀i = 1; : : : ; k}:

We also use Sn(cid:0)k(⟨i1; i2; : : : ; ik⟩) to denote the right coset with object i1 in position 1  i2 in position
2  : : :   and ik in position k.
The Mallows model is a distance based probabilistic model on permutations. It uses a permutation
distance d on the symmetric group Sn to deﬁne the probability of a permutation:

(2)

(3)

P ((cid:25)|(cid:18); (cid:27)) =

1

exp(−(cid:18)d((cid:25); (cid:27)));

Z((cid:18); (cid:27))

n

n
i=1

n
i=1

(cid:25)2Sn

∑

∑

Z((cid:18); (cid:27)) =

where (cid:27) ∈ Sn is the location permutation  (cid:18) ∈ R is a dispersion parameter  and

∑
exp(−(cid:18)d((cid:25); (cid:27))):
∑
∑
There are many well-deﬁned metrics to measure the distance between two permutations  such as
i=1((cid:25)(i) − (cid:27)(i))2  Spearman’s footrule df ((cid:25); (cid:27)) =
Spearman’s rank correlation dr((cid:25); (cid:27)) =
|(cid:25)(i) − (cid:27)(i)|  and Kendall’s tau dt((cid:25); (cid:27)) =
j>i 1f(cid:25)(cid:27)(cid:0)1(i)>(cid:25)(cid:27)(cid:0)1(j)g  where 1fxg = 1
if x is true and 0 otherwise. One can (and sometimes should) choose different distances for different
applications. In this regard  the Mallows model has rich expressiveness.
Note that there are n! permutations in Sn. The computation of Z((cid:18); (cid:27)) involves the sum of n! items.
Although for some speciﬁc distances (such as dt)  there exist efﬁcient ways for parameter estimation
in the Mallows model  for many other distances (such as dr and df )  there is no known efﬁcient
method to compute Z((cid:18); (cid:27)) and one has to pay for the high computational complexity of O(n!) [9].
This has greatly limited the application of the Mallows model in real problems. Usually  one has to
employ sampling methods such as MCMC to reduce the complexity [12  11]. This  however  will
affect the effectiveness of the model.
The Luce model is a stagewise probabilistic model on permutations.
It assumes that there is a
(hidden) score !i; i = 1; : : : ; n  for each individual object i. To generate a permutation (cid:25)  ﬁrstly
exp(!(cid:25)(cid:0)1(1))
i=1 exp(!(cid:25)(cid:0)1(i)); secondly the object
the object (cid:25)
(cid:0)1(2) is assigned to position 2 with probability
exp(!(cid:25)(cid:0)1(2))
i=2 exp(!(cid:25)(cid:0)1(i)); the assignment is continued
(cid:25)
until a complete permutation is formed. In this way  we obtain the permutation probability of (cid:25) as
follows 

(cid:0)1(1) is assigned to position 1 with probability

∑

n

∑

n

n∏

∑

P ((cid:25)) =

i=1

exp(!(cid:25)(cid:0)1(i))
n
j=i exp(!(cid:25)(cid:0)1(j))

:

(4)

The computation of permutation probability in the Luce model is very efﬁcient  as shown above.
Actually the corresponding complexity is in the polynomial order of the number of objects. This is a
clear advantage over the Mallows model. However  the Luce model is deﬁned as a speciﬁc function
of the scores of the objects  and therefore cannot make use of versatile permutation distances. As a
result  its expressiveness is not as rich as the Mallows model  which may limit its applications.

3 A New Probabilistic Model

As discussed in the above section  both the Mallows and the Luce model have certain advantages and
limitations. In this section  we propose a new probabilistic model on permutations  which can inherit
their advantages and avoid their limitations. We call this model the coset-permutation distance based
stagewise (CPS) model.

3

3.1 The CPS Model

∑

As indicated by the name  the CPS model is deﬁned on the basis of the so-called coset-permutation
distance. A coset-permutation distance is induced from a permutation distance  as shown in the
following deﬁnition.
Deﬁnition 1. Given a permutation distance d  the coset-permutation distance ^d from a coset Sn(cid:0)k(cid:25)
to a target permutation (cid:27) is deﬁned as the average distance between the permutations in the coset
and the target permutation:

^d(Sn(cid:0)k(cid:25); (cid:27)) =

1

|Sn(cid:0)k(cid:25)|

(cid:28)2Sn(cid:0)k(cid:25)

d((cid:28); (cid:27));

(5)

where |Sn(cid:0)k(cid:25)| is the number of permutations in set Sn(cid:0)k(cid:25).
It is easy to verify that if the permutation distance d is right invariant  then the induced coset-
permutation distance ^d is also right invariant.
With the concept of coset-permutation distance  given a dispersion parameter (cid:18) ∈ R and a location
permutation (cid:27) ∈ Sn  we can deﬁne the CPS model as follows. Speciﬁcally  the generative process
of a permutation (cid:25) of n objects is decomposed into n sequential stages. As an initialization  all the
objects are placed in a working set. At the k-th stage  the task is to select the k-th object in the
original permutation (cid:25) out of the working set. The probability of this selection is deﬁned with the
coset-permutation distance between the right coset Sn(cid:0)k(cid:25) and the location permutation (cid:27):

exp(−(cid:18) ^d(Sn(cid:0)k(cid:25); (cid:27)))

∑
j=k exp(−(cid:18) ^d(Sn(cid:0)k((cid:25); k; j); (cid:27)))
(cid:0)1(j) in the top k positions respectively.

n

;

(6)

(cid:0)1(k − 1)  and (cid:25)

where Sn(cid:0)k((cid:25); k; j) denotes the right coset including all the permutations that rank objects
(cid:0)1(1); : : : ; (cid:25)
(cid:25)
From Eq. (6)  we can see that the closer the coset Sn(cid:0)k(cid:25) is to the location permutation (cid:27)  the larger
the selection probability is. Considering all the n stages  we will obtain the overall probability of
generating (cid:25)  which is shown in the following deﬁnition.
Deﬁnition 2. The CPS model deﬁnes the probability of a permutation (cid:25) conditioned on a dispersion
parameter (cid:18) and a location permutation (cid:27) as:

P ((cid:25)j(cid:18); (cid:27)) =

exp((cid:0)(cid:18) ^d(Sn(cid:0)k(cid:25); (cid:27)))

n

j=k exp((cid:0)(cid:18) ^d(Sn(cid:0)k((cid:25); k; j); (cid:27)))

;

(7)

n∏

k=1

∑

P ((cid:25)|(cid:18); (cid:27)) = 1.

where Sn(cid:0)k((cid:25); k; j) is deﬁned in the sentence after Eq. (6).
∑
It is easy to verify that the probabilities P ((cid:25)|(cid:18); (cid:27)); (cid:25) ∈ Sn deﬁned in the CPS model naturally
form a distribution over Sn. That is  for each (cid:25) ∈ Sn  we always have P ((cid:25)|(cid:18); (cid:27)) ≥ 0  and
(cid:25)2Sn
In rank aggregation  one usually needs to combine multiple input rankings. To deal with this sce-
nario  we further extend the CPS model  following the methodology used in [12].

(cid:0)∑
(cid:0)∑
e
n
j=i e
where (cid:18)= {(cid:18)1; : : : ; (cid:18)M} and (cid:27)= {(cid:27)1; : : : ; (cid:27)M}.
The CPS model deﬁned as above can be computed in a highly efﬁcient manner  as discussed in the
following subsection.

m=1 (cid:18)m ^d(Sn(cid:0)i(cid:25);(cid:27)m)
m=1 (cid:18)m ^d(Sn(cid:0)i((cid:25);i;j);(cid:27)m)

P ((cid:25)|(cid:18); (cid:27)) =

n∏

∑

(8)

M

M

i=1

;

3.2 Computational Complexity
According to the deﬁnition of the CPS model  at the k-th stage  one needs to compute (n − k)
coset-permutation distances. At ﬁrst glance  the complexity of computing each coset-permutation

4

(cid:0)1(i)) (cid:0) i)2 +

((cid:27)((cid:25)

(cid:0)1(i)) (cid:0) j)2;

((cid:27)((cid:25)

j(cid:27)((cid:25)

(cid:0)1(i)) (cid:0) ij +

j(cid:27)((cid:25)

(cid:0)1(i)) (cid:0) jj;

n∑
1
n (cid:0) k
n∑
1
n (cid:0) k
n∑
k∑

i=k+1

n∑
n∑

j=k+1

i=k+1

j=k+1

(9)

(10)

(11)

distance is about O((n − k)!)  since the coset contains this number of permutations. This is clearly
intractable. The good news is that the real complexity for computing the coset-permutation distance
induced by several popular permutation distances is much lower than O((n − k)!). Actually  they
can be as low as O(n2)  according to the following theorem.
Theorem 1. The coset-permutation distances induced from Spearman’s rank correlation dr  Spear-
man’s footrule df   and Kendall’s tau dt can all be computed with a complexity of O(n2). More
speciﬁcally  for k = 1; 2; : : : ; n − 2  we have2

k∑
k∑

i=1

i=1

^dr(Sn(cid:0)k(cid:25); (cid:27)) =

^df (Sn(cid:0)k(cid:25); (cid:27)) =

^dt(Sn(cid:0)k(cid:25); (cid:27)) =

1
4

(n (cid:0) k)(n (cid:0) k (cid:0) 1) +

1f(cid:27)((cid:25)(cid:0)1(i))>(cid:27)((cid:25)(cid:0)1(j))g:

i=1

j=i+1

According to the above theorem  each induced coset-permutation distance can be computed with a
time complexity of O(n2). If we compute the CPS model according to Eq. (7)  the time complexity
will then be O(n4). This is clearly much more efﬁcient than O((n − k)!). Moreover  with careful
implementations  the time complexity of O(n4) can be further reduced to O(n2)  as indicated by
the following theorem.
Theorem 2. For the coset distances induced from dr  df and dt  the CPS model in Eq. (7) can be
computed with a time complexity of O(n2).

3.3 Relationship with Previous Models

The CPS model as deﬁned above has strong connections with both the Luce model and the Mallows
model  as shown below.
The similarity between the CPS model and the Luce model is that they are both deﬁned in a stage-
wise manner. This stagewise deﬁnition enables efﬁcient inference for both models. The difference
between the CPS model and the Luce model lies in that the CPS model has a much richer expres-
siveness than the Luce model. This is mainly because the CPS model is a distance based model
while the Luce model is not. Our experiments in Section 5 show that different distances may be
appropriate for different applications and datasets  which means a model with rich expressiveness
has the potential to be applied for versatile applications.
The similarity between the CPS model and the Mallows model is that they are both based on dis-
tances. Actually when the coset-permutation distance in the CPS model is induced by the Kendall’s
tau dt  the CPS model is even mathematically equivalent to the Mallows model deﬁned with dt.
The major difference between the CPS model and the Mallows model lies in the computational
efﬁciency. The CPS model can be computed efﬁciently with a polynomial time complexity  as dis-
cussed in the previous sub section. However  for most permutation distances  the complexity of the
Mallows model is as huge as O(n!).3
According to the above discussions  we can see that the CPS model inherits the advantages of both
the Luce model and the Mallows model  and avoids their limitations.

4 Algorithms for Rank Aggregation

In this section  we show how to apply the extended CPS model to solve the problem of rank aggrega-
tion. Here we take meta search as an example  and consider the supervised case of rank aggregation.
That is  given a set of training queries  we need to learn the parameters (cid:18) in the CPS model and
apply the model with the learned parameters to aggregate rankings for new test queries.

2Note that ^d(Sn(cid:0)k(cid:25); (cid:27)) = d((cid:25); (cid:27)) for k = n (cid:0) 1; n.
3An exception is that for Kendall’s tau distance  the Mallows model can be as efﬁcient as the CPS model

because they are mathematically equivalent.

5

Algorithm 1 Sequential inference

Initialize the set of n objects: D = f1; 2; : : : ; ng.
(cid:0)1(1) = arg minj2D
(cid:25)

Input: parameters (cid:18)  input rankings (cid:27)
Inference:
1:
2:
3: Remove object (cid:25)
4:

(cid:0)1(1) from set D.

∑

m (cid:18)m ^d(Sn(cid:0)1(< j >); (cid:27)m).

∑

(

(cid:0)1(k) = arg minj2D

for k = 2 to n
(4.1): (cid:25)
(4.2): Remove object (cid:25)
end

5:
Output: the ﬁnal ranking (cid:25).

m (cid:18)m ^d
(cid:0)1(k) from set D.

Sn(cid:0)k(< (cid:25)

(cid:0)1(1); : : : ; (cid:25)

(cid:0)1(k (cid:0) 1); j >); (cid:27)m

)

;

4.1 Learning
Let D = {((cid:25)(l);(cid:27)(l))} be the set of training queries  in which (cid:25)(l) is the ground truth ranking for
query ql  and (cid:27)(l) is the set of M input rankings.
In order to learn the parameters (cid:18) in Eq. (8)  we employ maximum likelihood estimation. Speciﬁ-
cally  the log likelihood of the training data for the CPS model can be written as below 

∑

l

∏
P ((cid:25)(l)j(cid:18); (cid:27)(l)) =
n∑

(cid:0) M∑

l

m=1

L((cid:18)) = log

∑

=

l

k=1

log P ((cid:25)(l)j(cid:18); (cid:27)(l))
n∑

m ) (cid:0) log

(cid:0)∑

e

M

m=1 (cid:18)m ^d(Sn(cid:0)k((cid:25)(l);k;j);(cid:27)

(l)
m )

 (12)

j=k

(cid:18)m ^d(Sn(cid:0)k(cid:25)(l); (cid:27)(l)

It is not difﬁcult to prove that L((cid:18)) is concave with respect to (cid:18). Therefore  we can use simple
optimization techniques like gradient ascent to ﬁnd the globally optimal (cid:18).

4.2

Inference

In the test phase  given a new query and its associated M input rankings  we need to infer a ﬁnal
ranking with the learned parameters (cid:18).
A straightforward method is to ﬁnd the permutation with the largest probability conditioned on the
M input rankings  just as the widely-used inference algorithm for the Mallows model [12]. We call
the method global inference since it ﬁnds the globally most likely one from all possible permutations.
The problem with global inference lies in that its complexity is as high as O(n!). As a consequence 
it cannot handle applications with a large number of objects to rank. Considering the stagewise
deﬁnition of the CPS model  we propose a sequential inference algorithm. The algorithm decom-
poses the inference into n steps. At the k-th step  we select the object j that can minimize the
^d(Sn(cid:0)k(⟨(cid:25)
(cid:0)1(k− 1); j⟩; (cid:27)m)  and put it at the k-th
coset-permutation distance
position. The procedure is listed in Algorithm 1.
In fact  sequential inference is an approximation of global inference  with a much lower complexity.
Theorem 3 shows that the complexity of sequential inference is just O(M n2). Our experiments in
the next section indicate that such an approximation does not hurt the ranking accuracy by much 
while signiﬁcantly speeds up the inference process.
Theorem 3. For the coset distance induced from dr  df   and dt  the stagewise inference as shown
in Algorithm 1 can be conducted with a time complexity of O(M n2) .

(cid:0)1(1); : : : ; (cid:25)

∑

m (cid:18)m

5 Experimental Results

We have performed experiments to test the efﬁciency and effectiveness of the proposed CPS model.

6

5.1 Settings

We take meta search as the target application  and use the LETOR [13] benchmark datasets in the
experiments. LETOR is a public collection created for ranking research.4 There are two meta search
datasets in LETOR  MQ2007-agg and MQ2008-agg. In addition to using them  we also composed a
smaller dataset from MQ2008-agg  referred to as MQ2008-small  by selecting queries with no more
than 8 documents from the MQ2008-agg dataset. This small dataset is used to perform detailed
investigations on the CPS model and other baseline models.
There are three levels of relevance labels in all the datasets: highly relevant  relevant  and irrelevant.
We used NDCG [10] as the evaluation measure in our experiments. NDCG is a widely-used IR
measure for multi-level relevance judgments. The larger the NDCG value  the better the aggregation
accuracy.
The 5-fold cross validation strategy was adopted for all the datasets. All the results reported in this
section are the average results over the ﬁve folds.
For the CPS model  we tested two inference methods: global inference (denoted as CPS-G) and se-
quential inference (denoted as CPS-S). For comparison  we implemented the Mallows model. When
applied to supervised rank aggregation  the learning process of the Mallows model is also maximum
likelihood estimation. For inference  we chose the permutation with the maximal probability as the
ﬁnal aggregated ranking. The time complexity of both learning and inference of the Mallows model
with distance dr and df is O(n!). We also implemented an approximate algorithm as suggested
by [12] using MCMC sampling to speed up the learning process. We refer to this approximate al-
gorithm as MallApp. Note that the time complexity of the inference of MallApp is still O(n!) for
distance dr and df . Furthermore  as a reference  we also tested a traditional method  BordaCount
[1]  which is based on majority voting. We did not compare with the Luce model because it is not
straightforward to be applied to supervised rank aggregation  as far as we know.
Note that Mallows  MallApp and CPS-G cannot handle the large datasets MQ2007-agg and
MQ2008-agg  and were only tested on the small dataset MQ2008-small.

5.2 Results

First  we report the results of these algorithms on the MQ2008-small dataset.
The aggregation accuracies in terms of NDCG are listed in Table 1(a). Note that the accuracy
of Mallows(dt) is the same as that of CPS-G(dt) because of the mathematical equivalence of the
two models. Therefore  we omit Mallows(dt) in the table. We did not implement the sampling-
based learning algorithm for the Mallows model with distance dt  because in this case the learning
algorithm has already been efﬁcient enough.
From the table  we have the following observations.

• For the Mallows model  exact learning is a little better than the approximate learning  es-
pecially for distance df . This is in accordance with our intuition. Sampling can improve
the efﬁciency of the algorithm  but also miss some information contained in the original
permutation probability space.
• For the CPS model  the sequential inference does not lead to much accuracy drop as com-
pared to global inference. For distances df and dr  the CPS model outperforms the Mallows
model. For example  when df is used  the CPS model wins the Mallows model by about
0.04 in terms of NDCG@2  which corresponds to a relative improvement of 10%.
• For the same model  with different distance functions  the performances differ signiﬁcantly.
• All the probabilistic model based methods are better than BordaCount  the heuristic

This indicates that one should select the most suitable distance for a given application.

method.

In addition to the comparison of aggregation accuracy  we have also logged the running time of each
model. For example  on our test machine (with 2.13Ghz CPU and 4GB memory)  it took about 12

4The datasets can be downloaded from http://research.microsoft.com/˜letor.

7

(a) Results on MQ2008-small
@6
0.479
0.518
0.517
0.490
0.491
0.519
0.519
0.491
0.490
0.530
0.534

@2
0.335
0.392
0.389
0.350
0.343
0.387
0.388
0.333
0.343
0.414
0.419

@4
0.421
0.471
0.471
0.449
0.440
0.476
0.478
0.442
0.440
0.485
0.489

NDCG

BordaCount
CPS-G(df )
CPS-S(df )
Mallows(df )
MallApp(df )
CPS-G(dr)
CPS-S(dr)
Mallows(dr)
MallApp(dr)
CPS-G(dt)
CPS-S(dt)

Table 1: Results

(b) Results on MQ2008-agg and MQ2007-agg

@8
0.420
0.446
0.444
0.422
0.420
0.443
0.441
0.420
0.419
0.451
0.454

NDCG

BordaCount
CPS-S(dt)
CPS-S(dr)
CPS-S(df )

NDCG

BordaCount
CPS-S(dt)
CPS-S(dr)
CPS-S(df )

@4
0.343
0.379
0.376
0.352

on MQ2008-agg
@2
0.281
0.312
0.314
0.276
on MQ2007-agg
@2
0.201
0.298
0.332
0.298

@4
0.213
0.311
0.341
0.312

@6
0.389
0.420
0.419
0.399

@6
0.225
0.322
0.352
0.323

@8
0.372
0.403
0.398
0.383

@8
0.238
0.335
0.362
0.336

seconds for CPS-G(df ) 5 30 seconds for MallApp(df )  and 12 hours for Mallows(df ) to ﬁnish the
training process. The inference of the Mallows model based algorithms and the global inference
of the CPS model based algorithms took more time than sequential inference of the CPS model 
although the difference was not signiﬁcant (this is mainly because n ≤ 8 for MQ2008-small).
From these results  we can see that the proposed CPS model plus sequential inference is the most
efﬁcient one  and its accuracy is also very good as compared to other methods.

Second  we report the results on MQ2008-agg and MQ2007-agg in Table 1(b). Note that the results
of the Mallows model based algorithms and that of the CPS model with global inference are not
available because of the high computational complexity for their learning or inference. The results
show that the CPS model with sequential inference outperforms BordaCount  no matter which dis-
tance is used. Moreover  the CPS model with dt performs the best on MQ2008-agg  and the model
with dr performs the best on MQ2007-agg. This indicates that we can achieve good ranking per-
formance by choosing the most suitable distances for different datasets (and so applications). This
provides a side evidence that it is beneﬁcial for a probabilistic model on permutations to have rich
expressiveness.
To sum up  the experimental results indicate that the CPS model based learning and sequential
inference algorithms can achieve state-of-the-art ranking accuracy and are more efﬁcient than other
algorithms.

6 Conclusions and Future Work

In this paper  we have proposed a new probabilistic model  named the CPS model  on permutations
for rank aggregation. The model is based on coset-permutation distance and deﬁned in a stagewise
manner.
It inherits the advantages of the Luce model (high efﬁciency) and the Mallows model
(rich expressiveness)  and avoids their limitations. We have applied the model to supervised rank
aggregation and investigated how to perform learning and inference. Experiments on public datasets
demonstrate the effectiveness and efﬁciency of the CPS model.
As future work  we plan to investigate the following issues. (1) We have shown that three induced
coset-permutation distances can be computed efﬁciently. We will explore whether other distances
also have such properties. (2) We have applied the CPS model to the supervised case of rank aggre-
gation. We will study the unsupervised case. (3) We will investigate other applications of the model 
and discuss how to select the most suitable distance for a given application.

5The training process of CPS-G and CPS-S is exactly the same.

8

References
[1] J. Aslam and M. Montague. Models for metasearch. In Proceedings of the 24th SIGIR  pages

276–284  2001.

[2] J. A. Aslam and M. Montague. Models for metasearch. In SIGIR ’01: Proceedings of the 24th
annual international ACM SIGIR conference on Research and development in information
retrieval  pages 276–284  New York  NY  USA  2001. ACM.

[3] M. Beg. Parallel Rank Aggregation for the World Wide Web. World Wide Web. Kluwer Aca-

demic Publishers  6(1):5–22  2004.

[4] D. Critchlow. Metric methods for analyzing partially ranked data. 1980.
[5] H. Daniels. Rank correlation and population models. Journal of the Royal Statistical Society.

Series B (Methodological)  pages 171–191  1950.

[6] P. Diaconis. Group representations in probability and statistics.

Statistics Hayward  CA  1988.

Institute of Mathematical

[7] C. Dwork  R. Kumar  M. Naor  and D. Sivakumar. Rank aggregation methods for the web.
In WWW ’01: Proceedings of the 10th international conference on World Wide Web  pages
613–622  New York  NY  USA  2001. ACM.

[8] R. Fagin  R. Kumar  and D. Sivakumar. Efﬁcient similarity search and classiﬁcation via rank
aggregation. In SIGMOD ’03: Proceedings of the 2003 ACM SIGMOD international confer-
ence on Management of data  pages 301–312  New York  NY  USA  2003. ACM.

[9] M. Fligner and J. Verducci. Distance based ranking models. Journal of the Royal Statistical

Society. Series B (Methodological)  48(3):359–369  1986.

[10] J. Kalervo and K. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans.

Inf. Syst.  20(4):422–446  2002.

[11] A. Klementiev  D. Roth  and K. Small. Unsupervised rank aggregation with distance-based

models. In Proceedings of the 25th ICML  pages 472–479  2008.

[12] G. Lebanon and J. Lafferty. Cranking: Combining rankings using conditional probability

models on permutations. In ICML2002  pages 363–370  2002.

[13] T. Liu  J. Xu  T. Qin  W. Xiong  and H. Li. LETOR: Benchmark dataset for research on learning
to rank for information retrieval. In SIGIR2007 Workshop on Learning to Rank for Information
Retrieval  pages 3–10  2007.

[14] R. D. Luce. Individual Choice Behavior. Wiley  1959.
[15] C. L. Mallows. Non-null ranking models. Biometrika  44:114–130  1957.
[16] R. Manmatha  T. Rath  and F. Feng. Modeling score distributions for combining the outputs
of search engines. In SIGIR ’01: Proceedings of the 24th annual international ACM SIGIR
conference on Research and development in information retrieval  pages 267–275  New York 
NY  USA  2001. ACM.

[17] M. Montague and J. A. Aslam. Relevance score normalization for metasearch. In CIKM ’01:
Proceedings of the tenth international conference on Information and knowledge management 
pages 427–433  New York  NY  USA  2001. ACM.

[18] R. L. Plackett. The analysis of permutations. Applied Statistics  24(2):193–202  1975.
[19] L. Thurstone. A law of comparative judgment. Psychological review  34(4):273–286  1927.

9

,Vibhav Vineet
Carsten Rother
Philip Torr
Changyou Chen
Jun Zhu
Xinhua Zhang
Sida Wang
Arun Tejasvi Chaganty
Percy Liang
Gamaleldin Elsayed
Simon Kornblith
Quoc Le