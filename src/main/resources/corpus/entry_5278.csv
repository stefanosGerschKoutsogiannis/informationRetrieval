2015,Streaming  Distributed Variational Inference for Bayesian Nonparametrics,This paper presents a methodology for creating streaming  distributed inference algorithms for Bayesian nonparametric (BNP) models. In the proposed framework  processing nodes receive a sequence of data minibatches  compute a variational posterior for each  and make asynchronous streaming updates to a central model. In contrast to previous algorithms  the proposed framework is truly streaming  distributed  asynchronous  learning-rate-free  and truncation-free. The key challenge in developing the framework  arising from fact that BNP models do not impose an inherent ordering on their components  is finding the correspondence between minibatch and central BNP posterior components before performing each update. To address this  the paper develops a combinatorial optimization problem over component correspondences  and provides an efficient solution technique. The paper concludes with an application of the methodology to the DP mixture model  with experimental results demonstrating its practical scalability and performance.,Streaming  Distributed Variational Inference for

Bayesian Nonparametrics

Trevor Campbell1
Jonathan P. How1
{tdjc@   jstraub@csail.   fisher@csail.   jhow@}mit.edu

John W. Fisher III2

Julian Straub2

1LIDS  2CSAIL  MIT

Abstract

This paper presents a methodology for creating streaming  distributed inference al-
gorithms for Bayesian nonparametric (BNP) models. In the proposed framework 
processing nodes receive a sequence of data minibatches  compute a variational
posterior for each  and make asynchronous streaming updates to a central model.
In contrast to previous algorithms  the proposed framework is truly streaming  dis-
tributed  asynchronous  learning-rate-free  and truncation-free. The key challenge
in developing the framework  arising from the fact that BNP models do not impose
an inherent ordering on their components  is ﬁnding the correspondence between
minibatch and central BNP posterior components before performing each update.
To address this  the paper develops a combinatorial optimization problem over
component correspondences  and provides an efﬁcient solution technique. The
paper concludes with an application of the methodology to the DP mixture model 
with experimental results demonstrating its practical scalability and performance.

1

Introduction

Bayesian nonparametric (BNP) stochastic processes are streaming priors – their unique feature is
that they specify  in a probabilistic sense  that the complexity of a latent model should grow as the
amount of observed data increases. This property captures common sense in many data analysis
problems – for example  one would expect to encounter far more topics in a document corpus after
reading 106 documents than after reading 10 – and becomes crucial in settings with unbounded  per-
sistent streams of data. While their ﬁxed  parametric cousins can be used to infer model complexity
for datasets with known magnitude a priori [1  2]  such priors are silent with respect to notions of
model complexity growth in streaming data settings.
Bayesian nonparametrics are also naturally suited to parallelization of data processing  due to the
exchangeability  and thus conditional independence  they often exhibit via de Finetti’s theorem. For
example  labels from the Chinese Restaurant process [3] are rendered i.i.d. by conditioning on the
underlying Dirichlet process (DP) random measure  and feature assignments from the Indian Buffet
process [4] are rendered i.i.d. by conditioning on the underlying beta process (BP) random measure.
Given these properties  one might expect there to be a wealth of inference algorithms for BNPs that
address the challenges associated with parallelization and streaming. However  previous work has
only addressed these two settings in concert for parametric models [5  6]  and only recently has each
been addressed individually for BNPs. In the streaming setting  [7] and [8] developed streaming
inference for DP mixture models using sequential variational approximation. Stochastic variational
inference [9] and related methods [10–13] are often considered streaming algorithms  but their per-
formance depends on the choice of a learning rate and on the dataset having known  ﬁxed size a
priori [5]. Outside of variational approaches  which are the focus of the present paper  there exist
exact parallelized MCMC methods for BNPs [14  15]; the tradeoff in using such methods is that they
provide samples from the posterior rather than the distribution itself  and results regarding assessing

1

ηo1

Retrieve Original
Central Posterior

Central Node

ηo1

Node

Retrieve Data

. . .

yt+1

yt

. . .

Minibatch
Posterior

Intermediate
Posterior

ηi2

ηi1

ηm3

ηm2

ηm1

η3

η2

η1

σ

ηm3

ηm2

ηm1

ηi2

ηi1

η3

η2

η1

(= ηm2)

(= ηi2 + ηm3 − η0)

(= ηi1 + ηm1 − ηo1)

Central Node

Node

Central Node

Node

Central Node

Node

. . .

yt+1

yt

. . .

. . .

yt+1

yt

. . .

. . .

yt+2

yt+1

. . .

Data Stream

Data Stream

Data Stream

Data Stream

(a) Retrieve the data/prior
Figure 1: The four main steps of the algorithm that is run asynchronously on each processing node.

(c) Perform component ID

(b) Perform inference

(d) Update the model

K1

convergence remain limited. Sequential particle ﬁlters for inference have also been developed [16] 
but these suffer issues with particle degeneracy and exponential forgetting.
The main challenge posed by the streaming  distributed setting for BNPs is the combinatorial prob-
lem of component identiﬁcation. Most BNP models contain some notion of a countably inﬁnite set
of latent “components” (e.g. clusters in a DP mixture model)  and do not impose an inherent order-
ing on the components. Thus  in order to combine information about the components from multiple
processors  the correspondence between components must ﬁrst be found. Brute force search is in-

tractable even for moderately sized models – there are(cid:0)K1+K2

(cid:1) possible correspondences for two

sets of components of sizes K1 and K2. Furthermore  there does not yet exist a method to evaluate
the quality of a component correspondence for BNP models. This issue has been studied before in
the MCMC literature  where it is known as the “label switching problem”  but past solution tech-
niques are generally model-speciﬁc and restricted to use on very simple mixture models [17  18].
This paper presents a methodology for creating streaming  distributed inference algorithms for
Bayesian nonparametric models. In the proposed framework (shown for a single node A in Fig-
ure 1)  processing nodes receive a sequence of data minibatches  compute a variational posterior
for each  and make asynchronous streaming updates to a central model using a mapping obtained
from a component identiﬁcation optimization. The key contributions of this work are as follows.
First  we develop a minibatch posterior decomposition that motivates a learning-rate-free streaming 
distributed framework suitable for Bayesian nonparametrics. Then  we derive the component iden-
tiﬁcation optimization problem by maximizing the probability of a component matching. We show
that the BNP prior regularizes model complexity in the optimization; an interesting side effect of this
is that regardless of whether the minibatch variational inference scheme is truncated  the proposed
algorithm is truncation-free. Finally  we provide an efﬁciently computable regularization bound for
the Dirichlet process prior based on Jensen’s inequality1. The paper concludes with applications of
the methodology to the DP mixture model  with experimental results demonstrating the scalability
and performance of the method in practice.

2 Streaming  distributed Bayesian nonparametric inference

The proposed framework  motivated by a posterior decomposition that will be discussed in Section
2.1  involves a collection of processing nodes with asynchronous access to a central variational pos-
terior approximation (shown for a single node in Figure 1). Data is provided to each processing
node as a sequence of minibatches. When a processing node receives a minibatch of data  it obtains
the central posterior (Figure 1a)  and using it as a prior  computes a minibatch variational posterior
approximation (Figure 1b). When minibatch inference is complete  the node then performs compo-
nent identiﬁcation between the minibatch posterior and the current central posterior  accounting for
possible modiﬁcations made by other processing nodes (Figure 1c). Finally  it merges the minibatch
posterior into the central variational posterior (Figure 1d).
In the following sections  we use the DP mixture [3] as a guiding example for the technical de-
velopment of the inference framework. However  it is emphasized that the material in this paper
generalizes to many other BNP models  such as the hierarchical DP (HDP) topic model [19]  BP la-
tent feature model [20]  and Pitman-Yor (PY) mixture [21] (see the supplement for further details).

1Regularization bounds for other popular BNP priors may be found in the supplement.

2

2.1 Posterior decomposition

Consider a DP mixture model [3]  with cluster parameters θ  assignments z  and observed data y.
For each asynchronous update made by each processing node  the dataset is split into three subsets
y = yo ∪ yi ∪ ym for analysis. When the processing node receives a minibatch of data ym  it
queries the central processing node for the original posterior p(θ  zo|yo)  which will be used as the
prior for minibatch inference. Once inference is complete  it again queries the central processing
node for the intermediate posterior p(θ  zo  zi|yo  yi) which accounts for asynchronous updates from
other processing nodes since minibatch inference began. Each subset yr  r ∈ {o  i  m}  has Nr
j=1  and each variable zrj ∈ N assigns yrj to cluster parameter θzrj . Given the
observations {yrj}Nr
independence of θ and z in the prior  and the conditional independence of the data given the latent
parameters  Bayes’ rule yields the following decomposition of the posterior of θ and z given y 

Updated Central Posterior

(cid:122) (cid:125)(cid:124) (cid:123)
p(θ  z|y)∝ p(zi  zm|zo)
p(zi|zo)p(zm|zo)

(cid:122)

(cid:125)(cid:124)

(cid:123)

(cid:122)

(cid:125)(cid:124)

Original Posterior

Minibatch Posterior

Intermediate Posterior

·

p(θ  zo|yo)−1 ·

p(θ  zm  zo|ym  yo) ·

p(θ  zi  zo|yi  yo).

(1)

(cid:123)

(cid:122)

(cid:125)(cid:124)

(cid:123)

This decomposition suggests a simple streaming  distributed  asynchronous update rule for a pro-
cessing node: ﬁrst  obtain the current central posterior density p(θ  zo|yo)  and using it as a prior 
compute the minibatch posterior p(θ  zm  zo|yo  ym); and then update the central posterior density
by using (1) with the current central posterior density p(θ  zi  zo|yi  yo). However  there are two
issues preventing the direct application of the decomposition rule (1):
Unknown component correspondence: Since it is generally intractable to ﬁnd the minibatch pos-
teriors p(θ  zm  zo|yo  ym) exactly  approximate methods are required. Further  as (1) requires the
multiplication of densities  sampling-based methods are difﬁcult to use  suggesting a variational ap-
proach. Typical mean-ﬁeld variational techniques introduce an artiﬁcial ordering of the parameters
in the posterior  thereby breaking symmetry that is crucial to combining posteriors correctly using
density multiplication [6]. The use of (1) with mean-ﬁeld variational approximations thus requires
ﬁrst solving a component identiﬁcation problem.
Unknown model size: While previous posterior merging procedures required a 1-to-1 matching
between the components of the minibatch posterior and central posterior [5  6]  Bayesian nonpara-
metric posteriors break this assumption. Indeed  the datasets yo  yi  and ym from the same non-
parametric mixture model can be generated by the same  disjoint  or an overlapping set of cluster
parameters. In other words  the global number of unique posterior components cannot be determined
until the component identiﬁcation problem is solved and the minibatch posterior is merged.

2.2 Variational component identiﬁcation

Suppose we have the following mean-ﬁeld exponential family prior and approximate variational
posterior densities in the minibatch decomposition (1) 

p(θk) = h(θk)eηT

0 T (θk)−A(η0) ∀k ∈ N

p(θ  zo|yo) (cid:39) qo(θ  zo) = ζo(zo)

h(θk)eηT

okT (θk)−A(ηok)

Ko(cid:89)

k=1

Km(cid:89)
Ki(cid:89)

k=1

k=1

K(cid:89)

k=1

3

p(θ  zm  zo|ym  yo) (cid:39) qm(θ  zm  zo) = ζm(zm)ζo(zo)

h(θk)eηT

mkT (θk)−A(ηmk)

(2)

p(θ  zi  zo|yi  yo) (cid:39) qi(θ  zi  zo) = ζi(zi)ζo(zo)

h(θk)eηT

ikT (θk)−A(ηik) 

where ζr(·)  r ∈ {o  i  m} are products of categorical distributions for the cluster labels zr  and the
goal is to use the posterior decomposition (1) to ﬁnd the updated posterior approximation

p(θ  z|y) (cid:39) q(θ  z) = ζ(z)

h(θk)eηT

k T (θk)−A(ηk).

(3)

As mentioned in the previous section  the artiﬁcial ordering of components causes the na¨ıve appli-
cation of (1) with variational approximations to fail  as disparate components from the approximate
posteriors may be merged erroneously. This is demonstrated in Figure 3a  which shows results from
a synthetic experiment (described in Section 4) ignoring component identiﬁcation. As the number of
parallel threads increases  more matching mistakes are made  leading to decreasing model quality.
To address this  ﬁrst note that there is no issue with the ﬁrst Ko components of qm and qi; these can
be merged directly since they each correspond to the Ko components of qo. Thus  the component
m = Km − Ko
identiﬁcation problem reduces to ﬁnding the correspondence between the last K(cid:48)
i = Ki − Ko components of the intermediate
components of the minibatch posterior and the last K(cid:48)
posterior. For notational simplicity (and without loss of generality)  ﬁx the component ordering
of the intermediate posterior qi  and deﬁne σ : [Km] → [Ki + K(cid:48)
m] to be the 1-to-1 mapping
from minibatch posterior component k to updated central posterior component σ(k)  where [K] :=
{1  . . .   K}. The fact that the ﬁrst Ko components have no ordering ambiguity can be expressed as
σ(k) = k ∀k ∈ [Ko]. Note that the maximum number of components after merging is Ki + K(cid:48)
m 
since each of the last K(cid:48)
m components in the minibatch posterior may correspond to new components
in the intermediate posterior. After substituting the three variational approximations (2) into (1)  the
goal of the component identiﬁcation optimization is to ﬁnd the 1-to-1 mapping σ(cid:63) that yields the
largest updated posterior normalizing constant  i.e. matches components with similar densities 

σ(cid:63) ← argmax

σ

z

θ

p(zi  zm|zo)

p(zi|zo)p(zm|zo)

qo(θ  zo)−1qσ

m(θ  zm  zo)qi(θ  zi  zo)

(cid:90)

(cid:88)

h(θσ(k))eηT

mkT (θσ(k))−A(ηmk)

(4)

Km(cid:89)

s.t.

m(zm)

qσ
m(θ  zm) = ζ σ
σ(k) = k  ∀k ∈ [Ko]   σ 1-to-1

k=1

m(zm) is the distribution such that Pζσ

(zmj = σ(k)) = Pζm (zmj = k). Taking the
where ζ σ
logarithm of the objective and exploiting the mean-ﬁeld decoupling allows the separation of the
objective into a sum of two terms: one expressing the quality of the matching between components
(the integral over θ)  and one that regularizes the ﬁnal model size (the sum over z). While the ﬁrst
term is available in closed form  the second is in general not. Therefore  using the concavity of
the logarithm function  Jensen’s inequality yields a lower bound that can be used in place of the
intractable original objective  resulting in the ﬁnal component identiﬁcation optimization:

m

m(cid:88)

Ki+K(cid:48)

σ(cid:63) ← argmax

σ

s.t.

ζ [log p(zi  zm  zo)]

A (˜ησ

k ) + Eσ
mk − ˜ηok

k=1
˜ησ
k = ˜ηik + ˜ησ
σ(k) = k ∀k ∈ [Ko]   σ 1-to-1.

(5)

A more detailed derivation of the optimization may be found in the supplement. Eσ
tation under the distribution ζo(zo)ζi(zi)ζ σ

m(zm)  and

ζ denotes expec-

k ≤ Kr
k > Kr

∀r ∈ {o  i  m} 

˜ησ
mk =

k ∈ σ ([Km])
k /∈ σ ([Km])

 

(6)

(cid:26) ηrk

η0

˜ηrk =

(cid:26) ηmσ−1(k)

η0

where σ ([Km]) denotes the range of the mapping σ. The deﬁnitions in (6) ensure that the prior η0
is used whenever a posterior r ∈ {i  m  o} does not contain a particular component k. The intuition
for the optimization (5) is that it combines ﬁnding component correspondences with high similarity
(via the log-partition function) with a regularization term2 on the ﬁnal updated posterior model size.
Despite its motivation from the Dirichlet process mixture  the component identiﬁcation optimization
(5) is not speciﬁc to this model. Indeed  the derivation did not rely on any properties speciﬁc to the
Dirichlet process mixture; the optimization applies to any Bayesian nonparametric model with a set
of “components” θ  and a set of combinatorial “indicators” z. For example  the optimization applies
to the hierarchical Dirichlet process topic model [10] with topic word distributions θ and local-to-
global topic correspondences z  and to the beta process latent feature model [4] with features θ and

(cid:104)

ζo(zo)ζi(zi)ζ σ

m(zm)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) p(zi  zm  zo)

(cid:105)

.

2This is equivalent to the KL-divergence regularization −KL

4

binary assignment vectors z. The form of the objective in the component identiﬁcation optimization
(5) reﬂects this generality. In order to apply the proposed streaming  distributed method to a partic-
ular model  one simply needs a black-box variational inference algorithm that computes posteriors
of the form (2)  and a way to compute or bound the expectation in the objective of (5).

2.3 Updating the central posterior

(cid:26)

(cid:27)

To update the central posterior  the node ﬁrst locks it and solves for σ(cid:63) via (5). Locking prevents
other nodes from solving (5) or modifying the central posterior  but does not prevent other nodes
from reading the central posterior  obtaining minibatches  or performing inference; the synthetic
experiment in Section 4 shows that this does not incur a signiﬁcant time penalty in practice. Then
the processing node transmits σ(cid:63) and its minibatch variational posterior to the central processing
node where the product decomposition (1) is used to ﬁnd the updated central variational posterior q
in (3)  with parameters

K = max

Ki  max
k∈[Km]

σ(cid:63)(k)

 

ζ(z) = ζi(zi)ζo(zo)ζ σ(cid:63)

m (zm) 

ηk = ˜ηik + ˜ησ(cid:63)

mk − ˜ηok.

(7)

Finally  the node unlocks the central posterior  and the next processing node to receive a new mini-
batch will use the above K  ζ(z)  and ηk from the central node as their Ko  ζo(zo)  and ηok.

3 Application to the Dirichlet process mixture model

The expectation in the objective of (5) is typically intractable to compute in closed-form; therefore 
a suitable lower bound may be used in its place. This section presents such a bound for the Dirichlet
process  and discusses the application of the proposed inference framework to the Dirichlet process
mixture model using the developed bound. Crucially  the lower bound decomposes such that the op-
timization (5) becomes a maximum-weight bipartite matching problem. Such problems are solvable
in polynomial time [22] by the Hungarian algorithm  leading to a tractable component identiﬁcation
step in the proposed streaming  distributed framework.

3.1 Regularization lower bound

For the Dirichlet process with concentration parameter α > 0  p(zi  zm  zo) is the Exchangeable
Partition Probability Function (EPPF) [23]

(nk − 1)! 

(8)

where nk is the amount of data assigned to cluster k  and K is the set of labels of nonempty clusters.
Given that the variational distribution ζr(zr)  r ∈ {i  m  o} is a product of independent categor-
  Jensen’s inequality may be used to bound the

ical distributions ζr(zr) = (cid:81)Nr

regularization in (5) below (see the supplement for further details) by

ζ [log p(zi  zm  zo)] ≥
Eσ

log α + log Γ(cid:0)max(cid:8)2  ˜tσ

(cid:9)(cid:1) + C

k

k∈K

p(zi  zm  zo) ∝ α|K|−1 (cid:89)
(cid:81)Kr
m(cid:88)

1[zrj =k]
rjk

Ki+K(cid:48)

k=1 π

(cid:17)

(cid:16)

j=1

1 − e˜sσ

k

k=1

(9)

(10)

˜tσ
k = ˜tik + ˜tσ
where C is a constant with respect to the component mapping σ  and

˜sσ
k = ˜sik + ˜sσ

mk + ˜sok 

mk + ˜tok 

j=1 log(1−πrjk)

k≤Kr
k>Kr

j=1 log(1−πmjσ−1(k))

∀r∈{o i m}

˜trk =

k∈σ([Km])
k /∈σ([Km])

˜tσ
mk =

j=1 πrjk

k≤Kr
k>Kr

∀r∈{o i m}

j=1 πmjσ−1 (k)

k∈σ([Km])
k /∈σ([Km])

.

(cid:26) (cid:80)Nr
(cid:26) (cid:80)Nm

0

0

(cid:26) (cid:80)Nr
(cid:26) (cid:80)Nm

0

0

˜srk =

˜sσ
mk =

Note that the bound (9) allows incremental updates: after ﬁnding the optimal mapping σ(cid:63)  the central
update (7) can be augmented by updating the values of sk and tk on the central node to

sk ← ˜sik + ˜sσ(cid:63)

mk + ˜sok 

tk ← ˜tik + ˜tσ(cid:63)

mk + ˜tok.

(11)

5

Increasing α

Increasing α

Figure 2: The Dirichlet process regularization and lower bound  with (2a) fully uncertain labelling and varying
number of clusters  and (2b) the number of clusters ﬁxed with varying labelling uncertainty.

(a)

(b)

datapoints for various DP concentration parameter values α ∈(cid:2)10−3  103(cid:3). The true regularization

As with K  ηk  and ζ from (7)  after performing the regularization statistics update (11)  a processing
node that receives a new minibatch will use the above sk and tk as their sok and tok  respectively.
Figure 2 demonstrates the behavior of the lower bound in a synthetic experiment with N = 100
log Eζ [p(z)] was computed by sample approximation with 104 samples. In Figure 2a  the number of
K . This ﬁgure demonstrates
clusters K was varied  with symmetric categorical label weights set to 1
two important phenomena. First  the bound increases as K → 0; in other words  it gives preference
to fewer  larger clusters  which is the typical BNP “rich get richer” property. Second  the behavior of
the bound as K → N depends on the concentration parameter α – as α increases  more clusters are
preferred. In Figure 2b  the number of clusters K was ﬁxed to 10  and the categorical label weights

were sampled from a symmetric Dirichlet distribution with parameter γ ∈(cid:2)10−3  103(cid:3). This ﬁgure

demonstrates that the bound does not degrade signiﬁcantly with high labelling uncertainty  and is
nearly exact for low labelling uncertainty. Overall  Figure 2a demonstrates that the proposed lower
bound exhibits similar behaviors to the true regularization  supporting its use in the optimization (5).

3.2 Solving the component identiﬁcation optimization

Given that both the regularization (9) and component matching score in the objective (5) decompose
as a sum of terms for each k ∈ [Ki + K(cid:48)
m]  the objective can be rewritten using a matrix of matching
scores R ∈ R(Ki+K(cid:48)
m). Setting
Xkj = 1 indicates that component k in the minibatch posterior is matched to component j in the
intermediate posterior (i.e. σ(k) = j)  providing a score Rkj deﬁned using (6) and (10) as

Rkj = A (˜ηij + ˜ηmk − ˜ηoj)+(cid:0)1 − e˜sij +˜smk+˜soj(cid:1) log α+log Γ(cid:0)max(cid:8)2  ˜tij + ˜tmk + ˜toj

m) and selector variables X ∈ {0  1}(Ki+K(cid:48)

m)×(Ki+K(cid:48)

m)×(Ki+K(cid:48)

(cid:9)(cid:1) .

The optimization (5) can be rewritten in terms of X and R as

tr(cid:2)XT R(cid:3)

X(cid:63) ← argmax

X

s.t. X1 = 1  XT 1 = 1  Xkk = 1 ∀k ∈ [Ko]

X ∈ {0  1}(Ki+K(cid:48)

m)×(Ki+K(cid:48)

m) 

1 = [1  . . .   1]T .

(12)

(13)

(14)

The ﬁrst two constraints express the 1-to-1 property of σ(·). The constraint Xkk = 1∀k ∈ [Ko] ﬁxes
the upper Ko×Ko block of X to I (due to the fact that the ﬁrst Ko components are matched directly) 
and the off-diagonal blocks to 0. Denoting X(cid:48)  R(cid:48) to be the lower right (K(cid:48)
i + K(cid:48)
m)
blocks of X  R  the remaining optimization problem is a linear assignment problem on X(cid:48) with
cost matrix −R(cid:48)  which can be solved using the Hungarian algorithm3. Note that if Km = Ko
or Ki = Ko  this implies that no matching problem needs to be solved – the ﬁrst Ko components
of the minibatch posterior are matched directly  and the last K(cid:48)
m are set as new components. In
practical implementation of the framework  new clusters are typically discovered at a diminishing
rate as more data are observed  so the number of matching problems that are solved likewise tapers
off. The ﬁnal optimal component mapping σ(cid:63) is found by ﬁnding the nonzero elements of X(cid:63):

m) × (K(cid:48)

i + K(cid:48)

σ(cid:63)(k) ← argmax

kj ∀k ∈ [Km] .
X(cid:63)

j

3For the experiments in this work  we used the implementation at github.com/hrldcpr/hungarian.

6

020406080100NumberofClusters-600-400-2000200400600RegularizationTruthLowerBound0.001Certain0.010.11.01000UncertainClusteringUncertaintyγ406080100120140RegularizationTruthLowerBound(a) SDA-DP without component ID

(b) SDA-DP with component ID

(c) Test log-likelihood traces

(e) Cluster/component ID counts

(f) Final cluster/component ID counts

(d) CPU time for component ID

Figure 3: Synthetic results over 30 trials. (3a-3b) Computation time and test log likelihood for SDA-DP with
varying numbers of parallel threads  with component identiﬁcation disabled (3a) and enabled (3b). (3c) Test log
likelihood traces for SDA-DP (40 threads) and the comparison algorithms. (3d) Histogram of computation time
(in microseconds) to solve the component identiﬁcation optimization. (3e) Number of clusters and number of
component identiﬁcation problems solved as a function of the number of minibatch updates (40 threads). (3f)
Final number of clusters and matchings solved with varying numbers of parallel threads.
4 Experiments

In this section  the proposed inference framework is evaluated on the DP Gaussian mixture with a
normal-inverse-Wishart (NIW) prior. We compare the streaming  distributed procedure coupled with
standard variational inference [24] (SDA-DP) to ﬁve state-of-the-art inference algorithms: memo-
ized online variational inference (moVB) [13]  stochastic online variational inference (SVI) [9] with
learning rate (t+10)− 1
2   sequential variational approximation (SVA) [7] with cluster creation thresh-
old 10−1 and prune/merge threshold 10−3  subcluster splits MCMC (SC) [14]  and batch variational
inference (Batch) [24]. Priors were set by hand and all methods were initialized randomly. Meth-
ods that use multiple passes through the data (e.g. moVB  SVI) were allowed to do so. moVB was
allowed to make birth/death moves  while SVI/Batch had ﬁxed truncations. All experiments were
performed on a computer with 24 CPU cores and 12GiB of RAM.
Synthetic: This dataset consisted of 100 000 2-dimensional vectors generated from a Gaussian mix-
ture model with 100 clusters and a NIW(µ0  κ0  Ψ0  ν0) prior with µ0 = 0  κ0 = 10−3  Ψ0 = I 
and ν0 = 4. The algorithms were given the true NIW prior  DP concentration α = 5  and mini-
batches of size 50. SDA-DP minibatch inference was truncated to K = 50 components  and all other
algorithms were truncated to K = 200 components. Figure 3 shows the results from the experiment
over 30 trials  which illustrate a number of important properties of SDA-DP. First and foremost 
ignoring the component identiﬁcation problem leads to decreasing model quality with increasing
number of parallel threads  since more matching mistakes are made (Figure 3a). Second  if compo-
nent identiﬁcation is properly accounted for using the proposed optimization  increasing the number
of parallel threads reduces execution time  but does not affect the ﬁnal model quality (Figure 3b).
Third  SDA-DP (with 40 threads) converges to the same ﬁnal test log likelihood as the comparison
algorithms in signiﬁcantly reduced time (Figure 3c). Fourth  each component identiﬁcation opti-
mization typically takes ∼ 10−5 seconds  and thus matching accounts for less than a millisecond of
total computation and does not affect the overall computation time signiﬁcantly (Figure 3d). Fifth 
the majority of the component matching problems are solved within the ﬁrst 80 minibatch updates
(out of a total of 2 000) – afterwards  the true clusters have all been discovered and the processing
nodes contribute to those clusters rather than creating new ones  as per the discussion at the end of
Section 3.2 (Figure 3e). Finally  increased parallelization can be advantageous in discovering the
correct number of clusters; with only one thread  mistakes made early on are built upon and persist 
whereas with more threads there are more component identiﬁcation problems solved  and thus more
chances to discover the correct clusters (Figure 3f).

7

12481624324048#Threads0.02.04.06.08.010.012.0CPUTime(s)-11.0-10.0-9.0-8.0-7.0-6.0-5.0TestLogLikelihoodCPUTimeTestLogLikelihood12481624324048#Threads0.02.04.06.08.010.012.0CPUTime(s)-11.0-10.0-9.0-8.0-7.0-6.0-5.0TestLogLikelihoodCPUTimeTestLogLikelihood-5-4-3-2-101234Time(Log10s)-11-10-9-8-7-6TestLogLikelihoodSDA-DPBatchSVASVImoVBSC010203040506070MergeTime(microseconds)051015202530354045Count020406080100#MinibatchesMerged020406080100120140Count#Clusters#MatchingsTrue#Clusters12481624324048#Threads020406080100120140Count#Clusters#MatchingsTrue#Clusters(a) Airplane trajectory clusters

(b) Airplane cluster weights

(c) MNIST clusters

(d) Numerical results on Airplane  MNIST  and SUN

Airplane

MNIST

SUN

Time (s)
9.4
568.9
10.4
1258.1
1618.4
1881.5

TestLL
-150.3
-149.9
-152.8
-149.7
-150.6
-149.7

Algorithm Time (s)

SDA-DP
0.66
SVI
1.50
SVA 3.00
moVB 0.69
SC 393.6
1.07

Batch

TestLL
-0.55
-0.59
-4.71
-0.72
-1.06
-0.72

Time (s)
3.0
117.4
57.0
645.9
1639.1
829.6

TestLL
-145.3
-147.1
-145.0
-149.2
-146.8
-149.5

Figure 4: (4a-4b) Highest-probability instances and counts for 10 trajectory clusters generated by SDA-DP.
(4c) Highest-probability instances for 20 clusters discovered by SDA-DP on MNIST. (4d) Numerical results.
Airplane Trajectories: This dataset consisted of ∼3 000 000 automatic dependent surveillance
broadcast (ADS-B) messages collected from planes across the United States during the period 2013-
03-22 01:30:00UTC to 2013-03-28 12:00:00UTC. The messages were connected based on plane call
sign and time stamp  and erroneous trajectories were ﬁltered based on reasonable spatial/temporal
bounds  yielding 15 022 trajectories with 1 000 held out for testing. The latitude/longitude points
in each trajectory were ﬁt via linear regression  and the 3-dimensional parameter vectors were clus-
tered. Data was split into minibatches of size 100  and SDA-DP used 16 parallel threads.
MNIST Digits [25]: This dataset consisted of 70 000 28 × 28 images of hand-written digits  with
10 000 held out for testing. The images were reduced to 20 dimensions with PCA prior to clustering.
Data was split into minibatches of size 500  and SDA-DP used 48 parallel threads.
SUN Images [26]: This dataset consisted of 108 755 images from 397 scene categories  with 8 755
held out for testing. The images were reduced to 20 dimensions with PCA prior to clustering. Data
was split into minibatches of size 500  and SDA-DP used 48 parallel threads.
Figure 4 shows the results from the experiments on the three real datasets. From a qualitative stand-
point  SDA-DP discovers sensible clusters in the data  as demonstrated in Figures 4a–4c. However 
an important quantitative result is highlighted by Table 4d: the larger a dataset is  the more the
beneﬁts of parallelism provided by SDA-DP become apparent. SDA-DP consistently provides a
model quality that is competitive with the other algorithms  but requires orders of magnitude less
computation time  corroborating similar ﬁndings on the synthetic dataset.

5 Conclusions

This paper presented a streaming  distributed  asynchronous inference algorithm for Bayesian non-
parametric models  with a focus on the combinatorial problem of matching minibatch posterior com-
ponents to central posterior components during asynchronous updates. The main contributions are
a component identiﬁcation optimization based on a minibatch posterior decomposition  a tractable
bound on the objective for the Dirichlet process mixture  and experiments demonstrating the per-
formance of the methodology on large-scale datasets. While the present work focused on the DP
mixture as a guiding example  it is not limited to this model – exploring the application of the
proposed methodology to other BNP models is a potential area for future research.

Acknowledgments
This work was supported by the Ofﬁce of Naval Research under ONR MURI grant N000141110688.

8

0123456789Cluster0500100015002000250030003500CountReferences
[1] Agostino Nobile. Bayesian Analysis of Finite Mixture Distributions. PhD thesis  Carnegie Mellon Uni-

versity  1994.

[2] Jeffrey W. Miller and Matthew T. Harrison. A simple example of Dirichlet process mixture inconsistency

for the number of components. In Advances in Neural Information Processing Systems 26  2013.

[3] Yee Whye Teh. Dirichlet processes. In Encyclopedia of Machine Learning. Springer  New York  2010.
[4] Thomas L. Grifﬁths and Zoubin Ghahramani. Inﬁnite latent feature models and the Indian buffet process.

In Advances in Neural Information Processing Systems 22  2005.

[5] Tamara Broderick  Nicholas Boyd  Andre Wibisono  Ashia C. Wilson  and Michael I. Jordan. Streaming

variational Bayes. In Advances in Neural Information Procesing Systems 26  2013.

[6] Trevor Campbell and Jonathan P. How. Approximate decentralized Bayesian inference. In Proceedings

of the 30th Conference on Uncertainty in Artiﬁcial Intelligence  2014.

[7] Dahua Lin. Online learning of nonparametric mixture models via sequential variational approximation.

In Advances in Neural Information Processing Systems 26  2013.

[8] Xiaole Zhang  David J. Nott  Christopher Yau  and Ajay Jasra. A sequential algorithm for fast ﬁtting of
Dirichlet process mixture models. Journal of Computational and Graphical Statistics  23(4):1143–1162 
2014.

[9] Matt Hoffman  David Blei  Chong Wang  and John Paisley. Stochastic variational inference. Journal of

Machine Learning Research  14:1303–1347  2013.

[10] Chong Wang  John Paisley  and David M. Blei. Online variational inference for the hierarchical Dirichlet
In Proceedings of the 11th International Conference on Artiﬁcial Intelligence and Statistics 

process.
2011.

[11] Michael Bryant and Erik Sudderth. Truly nonparametric online variational inference for hierarchical

Dirichlet processes. In Advances in Neural Information Proecssing Systems 23  2009.

[12] Chong Wang and David Blei. Truncation-free stochastic variational inference for Bayesian nonparametric

models. In Advances in Neural Information Processing Systems 25  2012.

[13] Michael Hughes and Erik Sudderth. Memoized online variational inference for Dirichlet process mixture

models. In Advances in Neural Information Processing Systems 26  2013.

[14] Jason Chang and John Fisher III. Parallel sampling of DP mixture models using sub-clusters splits. In

Advances in Neural Information Procesing Systems 26  2013.

[15] Willie Neiswanger  Chong Wang  and Eric P. Xing. Asymptotically exact  embarassingly parallel MCMC.

In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence  2014.

[16] Carlos M. Carvalho  Hedibert F. Lopes  Nicholas G. Polson  and Matt A. Taddy. Particle learning for

general mixtures. Bayesian Analysis  5(4):709–740  2010.

[17] Matthew Stephens. Dealing with label switching in mixture models. Journal of the Royal Statistical

Society: Series B  62(4):795–809  2000.

[18] Ajay Jasra  Chris Holmes  and David Stephens. Markov chain Monte Carlo methods and the label switch-

ing problem in Bayesian mixture modeling. Statistical Science  20(1):50–67  2005.

[19] Yee Whye Teh  Michael I. Jordan  Matthew J. Beal  and David M. Blei. Hierarchical Dirichlet processes.

Journal of the American Statistical Association  101(476):1566–1581  2006.

[20] Finale Doshi-Velez and Zoubin Ghahramani. Accelerated sampling for the Indian buffet process.

Proceedings of the International Conference on Machine Learning  2009.

In

[21] Avinava Dubey  Sinead Williamson  and Eric Xing. Parallel Markov chain Monte Carlo for Pitman-Yor

mixture models. In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence  2014.

[22] Jack Edmonds and Richard Karp. Theoretical improvements in algorithmic efﬁciency for network ﬂow

problems. Journal of the Association for Computing Machinery  19:248–264  1972.

[23] Jim Pitman. Exchangeable and partially exchangeable random partitions. Probability Theory and Related

Fields  102(2):145–158  1995.

[24] David M. Blei and Michael I. Jordan. Variational inference for Dirichlet process mixtures. Bayesian

Analysis  1(1):121–144  2006.

[25] Yann LeCun  Corinna Cortes  and Christopher J.C. Burges. MNIST database of handwritten digits. On-

line: yann.lecun.com/exdb/mnist.

[26] Jianxiong Xiao  James Hays  Krista A. Ehinger  Aude Oliva  and Antonio Torralba. SUN 397 image

database. Online: vision.cs.princeton.edu/projects/2010/SUN.

9

,Sergey Levine
Vladlen Koltun
Trevor Campbell
Julian Straub
John Fisher III