2018,Robustness of conditional GANs to noisy labels,We study the problem of learning conditional generators from noisy labeled samples  where the labels are corrupted by random noise. A standard training of conditional GANs will not only produce samples with wrong labels  but also generate poor quality samples. We consider two scenarios  depending on whether the noise model is known or not. When the distribution of the noise is known  we introduce a novel architecture which we call Robust Conditional GAN (RCGAN). The main idea is to corrupt the label of the generated sample before feeding to the adversarial discriminator  forcing the generator to produce samples with clean labels. This approach of passing through a matching noisy channel is justified by accompanying multiplicative approximation bounds between the loss of the RCGAN and the distance between the clean real distribution and the generator distribution. This shows that the proposed approach is robust  when used with a carefully chosen discriminator architecture  known as projection discriminator. When the distribution of the noise is not known  we provide an extension of our architecture  which we call RCGAN-U  that learns the noise model simultaneously while training the generator. We show experimentally on MNIST and CIFAR-10 datasets that both the approaches consistently improve upon baseline approaches  and RCGAN-U closely matches the performance of RCGAN.,RobustnessofconditionalGANstonoisylabelsKiranKoshyThekumparampil† AshishKhetan† ZinanLin‡ SewoongOh††UniversityofIllinoisatUrbana-Champaign ‡CarnegieMellonUniversityAbstractWestudytheproblemoflearningconditionalgeneratorsfromnoisylabeledsam-ples wherethelabelsarecorruptedbyrandomnoise.AstandardtrainingofconditionalGANswillnotonlyproducesampleswithwronglabels butalsogener-atepoorqualitysamples.Weconsidertwoscenarios dependingonwhetherthenoisemodelisknownornot.Whenthedistributionofthenoiseisknown weintroduceanovelarchitecturewhichwecallRobustConditionalGAN(RCGAN).Themainideaistocorruptthelabelofthegeneratedsamplebeforefeedingtotheadversarialdiscriminator forcingthegeneratortoproducesampleswithcleanlabels.ThisapproachofpassingthroughamatchingnoisychannelisjustiﬁedbyaccompanyingmultiplicativeapproximationboundsbetweenthelossoftheRCGANandthedistancebetweenthecleanrealdistributionandthegeneratordistribution.Thisshowsthattheproposedapproachisrobust whenusedwithacarefullychosendiscriminatorarchitecture knownasprojectiondiscriminator.Whenthedistributionofthenoiseisnotknown weprovideanextensionofourarchitecture whichwecallRCGAN-U thatlearnsthenoisemodelsimultaneouslywhiletrainingthegenerator.WeshowexperimentallyonMNISTandCIFAR-10datasetsthatboththeapproachesconsistentlyimproveuponbaselineapproaches andRCGAN-UcloselymatchestheperformanceofRCGAN.1IntroductionConditionalgenerativeadversarialnetworks(GAN)havebeenwidelysuccessfulinseveralapplica-tionsincludingimprovingimagequality semi-supervisedlearning reinforcementlearning categorytransformation styletransfer imagede-noising compression in-painting andsuper-resolution[30 13 49 36 26 58].ThegoaloftrainingaconditionalGANistogeneratesamplesfromdistribu-tionssatisfyingcertainconditioningonsomecorrelatedfeatures.Concretely givensamplesfromjointdistributionofadatapointxandalabely wewanttolearntogeneratesamplesfromthetrueconditionaldistributionoftherealdataPX|Y.AcanonicalconditionalGANstudiedinliteratureisthecaseofdiscretelabely[30 36 35 32].Signiﬁcantprogresseshavebeenmadeinthissetting whicharetypicallyevaluatedonthequalityoftheconditionalsamples.TheseincludemeasuringinceptionscoresandintraFréchetinceptiondistances visualinspectionondownstreamtaskssuchascategorymorphingandsuperresolution[32] andfaithfulnessofthesamplesasmeasuredbyhowaccuratelywecaninfertheclassthatgeneratedthesample[36].WestudytheproblemoftrainingconditionalGANswithnoisydiscretelabels.Bynoisylabels werefertoasettingwherethelabelyforeachexampleinthetrainingsetisrandomlycorrupted.Suchnoisecanresultfromanadversarydeliberatelycorruptingthedata[7]orfromhumanerrorsincrowdsourcedlabelcollection[12 18].Thiscanbemodeledasarandomprocess whereacleandataAuthoremailsarethekump2@illinois.edu ashish.khetan09@gmail.com zinanl@andrew.cmu.edu andswoh@illinois.edu.ThisworkusedtheExtremeScienceandEngineeringDiscoveryEnvironment(XSEDE) whichissupportedbyNationalScienceFoundationgrantnumberOCI-1053575.Speciﬁcally itusedtheBridgessystem whichissupportedbyNSFawardnumberACI-1445606 atthePittsburghSupercomputingCenter(PSC).32ndConferenceonNeuralInformationProcessingSystems(NeurIPS2018) Montréal Canada.pointx∈Xanditslabely∈[m]aredrawnfromajointdistributionPX Ywithmclasses.Foreachdatapoint thelabeliscorruptedbypassingthroughanoisychannelrepresentedbyarow-stochasticconfusionmatrixC∈Rm×mdeﬁnedasCij P(eY=j|Y=i).Thisdeﬁnesajointdistributionforthedatapointxandanoisylabeley:ePX eY.IfwetrainastandardconditionalGANonnoisysamples thenitsolvesthefollowingoptimization:minG∈GmaxD∈FV(G D)=E(x ey)∼ePX eY[φ(D(x ey))]+Ez∼N y∼ePeY[φ(1−D(G(z;y) y))] (1)whereφisafunctionofchoice DandGarethediscriminatorandthegeneratorrespectivelyoptimizedoverfunctionclassesGandFofourchoice andNisthedistributionofthelatentrandomvector.Fortypicalchoicesofφ forexamplelog(·) andlargeenoughfunctionclassesGandF theoptimalconditionalgeneratorlearnstogeneratesamplesfromePX|eY thecorruptedconditionaldistribution.Inotherwords itgeneratessamplesXfromclassesotherthanwhatitisconditionedon.Asthelearneddistributionexhibitssuchabias wecallthisnaiveapproachtheBiasedGAN.Underthissetting thereisafundamentalquestionofinterest:canwedesignanovelconditionalGANthatcangeneratesamplesfromthetrueconditionaldistributionPX|Y evenwhentrainedonnoisysamples?Severalaspectsofthisproblemmakeitchallengingandinteresting.First theperformanceofsuchrobustGANshoulddependonhownoisythechannelCis.IfCisrank-deﬁcient forinstance thentherearemultipledistributionsthatresultinthesamedistributionafterthecorruption andhencenoreliablelearningofthetruedistributionispossible.Wewouldideallywantatheoreticalguaranteethatshowssuchtrade-offbetweenCandtherobustnessofGANs.Next whenthenoiseisfromerrorsincrowdsourcedlabels wemighthavesomeaccesstotheconfusionmatrixCfromhistoricaldata.Onothercasesofadversarialcorruption wemightnothaveanyinformationofC.Wewanttoproviderobustsolutionstoboth.Finally animportantpracticalchallengeinthissettingistocorrectthenoisylabelsinthetrainingdata.Weaddressallsuchvariationsinourapproachesandmakethefollowingcontributions.Ourcontributions.WeintroducetwoarchitecturestotrainconditionalGANswithnoisysamples.First whenwehavetheknowledgeoftheconfusionmatrixC weproposeRCGAN(RobustConditionalGAN)inSection2.WeﬁrstprovethatminimizingtheRCGANlossprovablyrecoversthecleandistributionPX|Y(Theorem2) undercertainconditionsontheclassFofdiscriminatorsweoptimizeover(Assumption1).WeshowthatsuchaconditiononFisalsonecessary aswithoutit thetraininglosscanbearbitrarilysmallwhilethegenerateddistributioncanbefarfromthereal(Theorem4).TheassumptionleadstoourparticularchoiceofthediscriminatorinRCGAN calledprojectiondiscriminator[32]thatsatisﬁesalltheconditions(Remark1).Finally weprovideaﬁnitesamplegeneralizationboundshowingthatthelossminimizedintrainingRCGANdoesgeneralize andresultsinthelearneddistributionbeingclosetothecleanconditionaldistributionPX|Y(Theorem3).ExperimentalresultsinbenchmarkdatasetsconﬁrmthatRCGANisrobustagainstnoisysamples andimprovessigniﬁcantlyoverthenaiveBiasedGAN.Secondly whenwedonothaveaccesstoC weproposeRCGAN-U(RCGANwithUnknownnoisedistribution)inSection4.WeprovideexperimentalresultsshowingthatperformancegainssimilartothatofRCGANcanbeachieved.Finally weshowcasethepracticaluseofthuslearnedconditionalGANs byusingittoﬁxthenoisylabelsinthetrainingdata.NumericalexperimentsconﬁrmthattheRCGANframeworkprovidesamorerobustapproachtocorrectingthenoisylabels comparedtothestate-of-the-artmethodsthatrelyonlyondiscriminators.Relatedwork.Twopopulartrainingmethodsforgenerativemodelsarevariationalauto-encoders[22]andadversarialtraining[14].Theadversarialtrainingapproachhasmadesigniﬁcantadvancesinseveralapplicationsofpracticalinterest.[37 2 5]proposenewarchitecturesthatsigniﬁcantlyimprovethetraininginpracticalimagedatasets.[58 16]proposenewarchitecturestotransferthestyleofoneimagetotheotherdomain.[26 43]showhowtoenhanceagivenimagewithlearnedgenerator byenhancingtheresolutionormakingitmorerealistic.[27 50]showhowtogeneratevideosand[51 1]demonstratethat3-dimensionalmodelscanbegeneratedfromadversarialtraining.[23]proposesanewarchitectureencodingcausalstructuresinconditionalGANs.[42]introducesthestate-of-the-artconditionalindependencetester.Onadifferentdirection severalrecentapproachesshowcasehowthemanifoldlearnedbytheadversarialtrainingcanbeusedtosolveinverseproblems[9 57 53].2ConditionalGANshavebeenproposedasasuccessfultoolforvariousapplications includingclassconditionalimagegeneration[36] imagetoimagetranslation[21] andimagegenerationfromtext[38 55].MostoftheconditionalGANsincorporatetheclassinformationbynaivelyconcatenatingittotheinputorfeaturevectoratsomemiddlelayer[30 13 38 55].AC-GANs[36]createsanauxiliaryclassiﬁertoincorporateclassinformation.ProjectiondiscriminatorGAN[32]takesaninnerproductbetweentheembeddedclassvectorandthefeaturevector.Arecentwork[31]whichproposesspectralnormalizationshowsthathighqualityimagegenerationon1000-classILSVRC2012dataset[39]canbeachievedusingprojectionconditionaldiscriminator.Robustnessof(unconditional)GANsagainstadversarialorrandomnoisehasrecentlybeenstudiedin[10 52].[52]studiesanadversarialattackthatperturbsthediscriminatoroutput.TheproposedarchitectureofRCGANisinspiredbyacloselyrelatedworkofAmbientGANin[10].AmbientGANisageneralframeworkaddressinganycorruptionontheimageitself(notnecessarilyjustthelabels).Givencorruptedsampleswithaknowncorruption AmbientGANappliesthatcorruptiontotheoutputofthegeneratorbeforefeedingittothediscriminator.MotivatedbythesuccessofAmbientGANinde-noising weproposeRCGAN.Animportantdistinctionisthatwemakespeciﬁcarchitecturalchoicesguidedbyourtheoreticalanalysisthatgivesasigniﬁcantgaininpractice(AppendixJ).Underthescenarioofinterestwithnoisylabels weprovidesharpanalysesforboththepopulationlossandtheﬁnitesampleloss.SuchsharpcharacterizationsdonotexistforthemoregeneralAmbientGANscenarios.Further ourRCGAN-Udoesnotrequiretheknowledgeoftheconfusionmatrix departingfromtheAmbientGANapproach.Learningclassiﬁersfromnoisylabelsisacloselyrelatedproblem.Recently[34 20]proposedatheoreticallymotivatedclassiﬁerwhichminimizesthemodiﬁedlossinpresenceofnoisylabelsandshowedimprovementovertherobustclassiﬁers[29 45 46].[47]proposedaddingnoisetotheclassiﬁeroutputtomatchthenoisedistribution.Notation.Foravector kxkp=(Pi|xi|p)1/pisthe‘p-norm.Foramatrix let|||A|||p=maxkxkp=1kAxkpdenotetheoperatornorm.Then|||A|||∞=maxiPj|Aij| |||A|||1=maxjPi|Aij|and|||A|||2=σmax(A) themaximumsingularvalue.1isallonesvectorandIisidentitymatrix.[n]={1 ... n}.Foravectorx∈Rn xi(i∈[n])isitsi-thcoordinate.2Ourﬁrstarchitecture:RCGANTrainingaconditionalGANwithnoisysamplesresultsinabiasedgenerator.WeproposeRobustConditionalGAN(RCGAN)architecturewhichhasthefollowingpre-processing discriminatorupdate andgeneratorupdatesteps.WeassumeinthissectionthattheconfusionsmatrixCisknown(andthemarginalPYcaneasilybeinferred) andaddressthecaseofunknownCinSection4.GDCzyx˜y˜yrealxrealpermutationregularizeradversariallosshFigure1:TheoutputxoftheconditionalgeneratorGispairedwithanoisylabeleycorruptedbythechannelC.ThediscriminatorDestimateswhetheragivenlabeledsampleiscomingfromtherealdata(xreal ˜yreal)orgenerateddata(x ˜y).Thepermutationregularizerhispre-trainedonrealdata.Pre-processing:Wetrainaclassiﬁerh∗topredictthenoisylabeleygivenxunderalossl trainedonh∗∈argminh∈HE(x ey)∼ePX eY[‘(h(x) ey)] whereHisaparametricfamilyofclassiﬁers(typicallyneuralnetworks)andePX eYisthejointdistributionofrealxandcorrespondingrealnoisyey.D-step:Wetrainonthefollowingadversarialloss.Inthesecondtermbelow yisgeneratedaccordingtoPYandcorrespondingnoisylabelsaregeneratedbycorruptingtheyaccordingtotheconditionaldistributionCywhichisthey-throwoftheconfusionmatrix(assumedtobeknown):maxD∈FE(x ey)∼ePX eY[φ(D(x ey))]+Ez∼N y∼PYey|y∼Cy[φ(1−D(G(z;y) ey))] 3wherePYisthetruemarginaldistributionofthelabels Nisthedistributionofthelatentrandomvector andFisafamilyofdiscriminators.G-step:Wetrainonthefollowinglosswithsomeλ>0:minG∈GEz∼N y∼PYey|y∼Cy(cid:2)φ(1−D(G(z;y) ey))+λ‘(h∗(G(z;y)) y)(cid:3) (2)whereGisafamilyofgenerators.Theideaofusingauxiliaryclassiﬁershavebeenusedtoimprovethequalityoftheimageandstabilityofthetraining forexampleinauxiliaryclassiﬁerGAN(AC-GAN)[36] andimprovethequalityofclusteringinthelatentspace[33].Weproposeanauxiliaryclassiﬁersh mitigatingapermutationerror whichweempiricallyidentiﬁedonnaiveimplementationofourideawithnoregularizers.Permutationregularizer(controlledbyλ).Permutationerroroccursif whenaskedtoproducesamplesfromatargetclass thetrainedgeneratorproducessamplesdominantlyfromasingleclassbutdifferentfromthetargetclass.Weproposearegularizerh∗ whichpredictsthenoisylabeley.Aslongastheconfusionmatrixisdiagonallydominant whichisanecessaryconditionforidentiﬁability thisregularizerencouragesthecorrectpermutationofthelabels.Moreregularizerscouldpotentiallyprovideadditonalrobustnessandwediscussonesuchregularizer(similartotheInfoGANloss[11])inAppendixK.TheoreticalmotivationforRCGAN.Whenλ=0 wegetthestandardconditionalGANupdatesteps albeitonewhichtriestominimizediscriminatorlossbetweenthenoisyrealdistributionePandthedistributioneQofthegeneratorwhenthelabelispassedthroughthesamenoisychannelparameterizedbyC.ThemainideaofRCGANistominimizeacertaindivergencebetweennoisyrealdataandnoisygenerateddata.Forexample thechoiceofboundedfunctionsF={D:X×[m]→[0 1]}andidentitymapφ(a)=aleadstoatotalvariationminimization;ThelossminimizedintheG-stepisthetotalvariationdTV(eP eQ) supS∈X×[m]{eP(S)−eQ(S)}betweenthetwodistributionswithcorruptedlabels uptosomescalingandsomeshift.IfwechooseF={D:X×[m]→[0 1]}andφ(a)=log(a) thenweareminimizingtheJensen-ShannondivergencedJS(eP eQ) (1/2)dKL(ePk(eP+eQ)/2)+(1/2)dKL(eQk(eP+eQ)/2) wheredKL(·k·)denotestheKullback-Leiblerdivergence.Thefollowingtheoremprovidesapproximationguaranteesforsomecommondivergencemeasuresovernoisychannel justifyingourproposedpracticalapproach.WerefertoAppendixBforaproof.Theorem1.LetPX YandQX YbetwodistributionsonX×[m].LetePX eY eQX eYbethecorre-spondingdistributionswhensamplesfromP QarepassedthroughthenoisychannelgivenbytheconfusionmatrixC∈Rm×m(asdeﬁnedinSection1).IfCisfull-rank weget dTV(cid:16)eP eQ(cid:17)≤dTV(P Q)≤|||C−1|||∞dTV(cid:16)eP eQ(cid:17) and(3)dJS(cid:16)eP(cid:13)(cid:13)(cid:13)eQ(cid:17)≤dJS(PkQ)≤|||C−1|||∞r8dJS(cid:16)eP(cid:13)(cid:13)(cid:13)eQ(cid:17).(4)Tointerpretthistheorem letQdenotethedistributionofthegenerator.ThetheoremimpliesthatwhenthenoisygeneratordistributioneQbecomesclosetothenoisyrealdistributionePintotalvariationorinJensen-Shannondivergence thenthegeneratordistributionQmustbeclosetothedistributionofrealdataPinthesamemetric.ThisjustiﬁestheuseoftheproposedarchitectureRCGAN.Inpractice weminimizethesampledivergenceofthetwodistributions insteadofthepopulationdivergenceasanalyzedintheabovetheorem.However thesestandarddivergencesareknowntonotgeneralizeintrainingGANs[3].Tothisend weprovideinSection3analysesonneuralnetworkdistances whichareknowntogeneralize andprovideﬁnitesamplebounds.3TheoreticalAnalysisofRCGANItwasshownin[3]thatstandardGANlossesofJensen-ShannondivergenceandWassersteindistancebothfailtogeneralizewithaﬁnitenumberofsamples.Ontheotherhand morerecentadvancesinanalyzingGANsin[56 6 4]showpromisinggeneralizationboundsbyeitherassumingLipschitzconditionsonthegeneratormodelorbyrestrictingtheanalysistocertainclassesofdistributions.Underthoseassumptions whereJSdivergencegeneralizes Theorem1justiﬁestheuseofthe4proposedRCGAN.However thoserequirethedistributiontobeGaussian mixtureofGaussians oroutputofaneuralnetworkgenerator forexamplein[4].Inthissection weprovideanalysesofRCGANonadistancethatgeneralizeswithoutanyassumptionsonthedistributionoftherealdataasprovenin[3]:neuralnetworkdistance.Formally consideraclassofreal-valuedfunctionsFandafunctionφ:[0 1]→Rwhichiseitherconvexorconcave.TheneuralnetworkdistanceisdeﬁnedasdF φ(P Q) supD∈FE(x y)∼P[φ(D(x y))]+E(x y)∼Q[φ(1−D(x y))]−µφ.(5)wherePisthedistributionoftherealdata Qisthatofthegenerateddata andµφistheconstantcorrectiontermtoensurethatdF φ(P P)=0.WefurtherassumethatFincludesthreeconstantfunctionsD(x y)=0 D(x y)=1/2 andD(x y)=1 inordertoensurethatdF φ(P Q)≥0anddF φ(P P)=0 asshowninLemma1intheAppendix.TheproposedRCGANwithλ=0approximatelyminimizestheneuralnetworkdistancedF φ(eP eQ)betweenthetwocorrupteddistributions.Inpractice Fisaparametricfamilyoffunctionsfromaspeciﬁcneuralnetworkarchitecturethatthedesignerhaschosen.Intheory weaimtoidentifyhowthechoiceofclassFprovidesthedesiredapproximationboundssimilartothoseinTheorem1 butforneuralnetworkdistances.Thisanalysisleadstothechoiceofprojectiondiscriminator[32]tobeusedinRCGAN(Remark1).Ontheotherhand weshowinTheorem4thataninappropriatechoiceofthediscriminatorarchitecturecancausenon-approximation.Further weprovidethesamplecomplexityoftheapproximationboundsinTheorem3.Werefertotheun-regularizedversionwithλ=0assimplyRCGAN.Inthissection wefocusonaclassoflossfunctionscalledIntegralProbabilityMetrics(IPM)whereφ(x)=x[44].ThisisapopularchoiceoflossinGANsinpractice[48 2 8]andinanalyses[4].WewritetheinducedneuralnetworkdistanceasdF(P Q) droppingtheφinthenotation.3.1ApproximationboundsforneuralnetworkdistancesWedeﬁneanoperation◦overamatrixT∈Rm×mandaclassFoffunctionsonX×[m]→RasT◦F ng(x y)=Xey∈[m]Tyeyf(x ey)|f∈Fo.(6)ThismakesitconvenienttorepresenttheneuralnetworkdistancecorruptedbynoisewithaconfusionmatrixC∈Rm×m whereCyeyistheprobabilityalabelyiscorruptedasey.Formally itfollowsfrom(5)and(6)thatdF(eP eQ)=dC◦F(P Q).WerefertoAppendixFforaproof.FordF(eP eQ)tobeagoodapproximationofdF(P Q) weshowthatthefollowingconditionissufﬁcient.Assumption1.WeassumethattheclassofdiscriminatorfunctionsFcanbedecomposedintothreepartsF={f1+f2+c|f1∈F1 f2∈F2}suchthatc∈Risanyconstantand•F1satisﬁestheinclusioncondition:T◦F1⊆F1 (7)forall|||T|||∞ maxiPj|Tij|=1;and•F2satisﬁesthelabelinvariancecondition:thereexistsaclassF02offunctionsoveronlyx suchthatF2=(cid:8)αg(x y)|g(x y)=f(x) foranyf(x)∈F02 andα∈[0 1](cid:9).(8)WediscussthenecessityandpracticalimplicationsofthisassumptioninSection3.2 andgiveexamplessatisfyingtheseassumptionsinRemark1andAppendixC.Noticethatatrivialclasswithasingleconstantzerofunctionsatisﬁesbothinclusionandlabelinvarianceconditions.Forexample wecanchoosec=0andalsochoosetoseteitherF1={f(x y)=0}orF2={f(x y)=0} inwhichcaseFonlyneedstosatisfyeitheroneoftheconditionsinAssumption1.TheﬂexibilitythatwegainbyallowingthesetadditionF1+F2iscriticalinapplyingtheseconditionstopracticaldiscriminators especiallyinprovingRemark1.NotethatintheinclusionconditioninEq.7 we5requiretheconditiontoholdforallmax-normboundedset:{T:maxiPj|Tij|=1}.Thereasonaweakerconditionofallrow-stochasticmatrices {T:PjTij=1} doesnotsufﬁceisthatinordertoprovetheupperboundinEq.9 weneedtoapplytheinvarianceconditionto|||C−1|||−1∞C−1◦F.Thismatrix|||C−1|||−1∞C−1isnotrow-stochastic butstillmax-normbounded.WeﬁrstshowthatAssumption1issufﬁcientforapproximabilityoftheneuralnetworkdistancefromcorruptedsamples.FortwodistributionsPX YandQX YonX×[m] letePX eYandeQX eYbethecorrespondingcorrupteddistributionsrespectively wherethelabelYispassedthroughthenoisychanneldeﬁnedbytheconfusionmatrixC∈Rm×m i.e.eP(x ey)=PyP(x y)Cy ey.Theorem2.IfaclassoffunctionsFsatisﬁesAssumption1 thendF(eP eQ)≤dF(P Q)≤|||C−1|||∞dF(eP eQ) (9)wherewefollowtheconventionthat|||C−1|||∞=∞ifCisnotfullrank.WerefertoAppendixFforaproof.Thisgivesasharpcharacterizationonhowtwodistancesarerelated:theonewecanminimizeintrainingRCGAN(i.e.dF(eP eQ))andthetruemeasureofcloseness(i.e.dF(P Q)).Althoughthelattercannotbedirectlyevaluatedorminimized RCGANisapproximatelyminimizingthetrueneuralnetworkdistancedF(P Q)asdesired.Thelowerboundprovesaspecialcaseofthedata-processinginequality.TworandomvariablesfromPandQgetcloserinneuralnetworkdistance whenpassedthroughastochastictransformation.TheupperboundputsalimitonhowmuchcloserePandeQcanget dependingonthenoiselevel.Thisfundamentaltrade-offiscapturedby|||C−1|||∞.UnderthenoiselesscasewhereCistheidentitymatrix wehave|||C−1|||∞=1andwerecoveratrivialfactthatthetwodistancesareequal.Ontheotherextreme ifCisrankdeﬁcient weusetheconventionthat|||C−1|||∞=∞andthetwodistancescanbearbitrarilydifferent.Theapproximationfactorof|||C−1|||∞captureshowmuchthespaceFcanshrinkbythenoiseC.ThiscoincideswithTheorem1 whereasimilartrade-offwasidentiﬁedfortheTVdistance.InRemark3inAppendixD weshowthattheseboundscannotbetightenedforgeneralP Q andF.Theorem2showsthat(i)RCGANcanlearnthetrueconditionaldistribution justifyingitsuse;and(ii)performanceofRCGANisdeterminedbyhownoisythesamplesarevia|||C−1|||∞.Therearestilltwolooseends.First doespracticalimplementationofRCGANarchitecturesatisfytheinclusionand/orlabelinvarianceassumptions?Secondly inpracticewecannotminimizedF(eP eQ)asweonlyhaveaﬁnitenumberofsamples.Howmuchdoweloseinthisﬁnitesampleregime?Wegivepreciseanswerstoeachquestioninthefollowingtwosections.3.2InclusionandlabelinvarianceassumptionsForRCGAN weproposeapopularstate-of-the-artdiscriminatorforconditionalGANsknownastheprojectiondiscriminator[32] parametrizedbyV∈Rm×dV v∈Rdv andθ∈Rdθ:DV v θ(x y)=vec(y)TVψ(x;θ)+vTψ0(x;θ) (10)whereψ(x;θ)∈RdVandψ0(x;θ)∈RdvarevectorvaluedparametricfunctionsforsomeintegersdV dv andvec(y)T=[Iy=1 ... Iy=m].Theﬁrsttermsatisﬁestheinclusioncondition asanyoperationwithTcanbeabsorbedintoV.Thesecondtermislabelinvariantasitdoesnotdependony.Thisismadepreciseinthefollowingremark whoseproofisprovidedinAppendixG.Togetherwiththisremark theapproximabilityresultinTheorem2justiﬁestheuseofprojectiondiscriminatorsinRCGAN whichweuseinallourexperiments.Remark1.Theclassofprojectiondiscriminators{DV v θ(x y)}V∈V1 v∈V2 θ∈ΘdeﬁnedinEq.10satisﬁesAssumption1foranyψ ψ0 andΘ ifV1=(cid:8)V∈Rm×dV(cid:12)(cid:12)maxi|Vij|≤1forallj∈[dV](cid:9) andV2=(cid:8)v∈Rdv(cid:12)(cid:12)kvk≤1(cid:9).OtherchoicesofV1andV2arealsopossible.Forexample V01={V∈Rm×dV|Pjmaxi|Vij|≤1}orV001={V∈Rm×dV||||V|||∞=maxiPj|Vij|≤1}arealsosufﬁcient.WeﬁndtheproposedchoiceofV1easytoimplement asacolumn-wiseL∞-normnormalizationviaprojectedgradientdescent.WedescribeimplementationdetailsinAppendixL.InAppendixE weshowthatAssumption1isalsonecessary.63.3FinitesampleanalysisInpractice wedonothaveaccesstotheprobabilitydistributionsePandeQ.Instead weobserveasetofsamplesofaﬁnitesizen fromeachofthem.IntrainingGAN weminimizetheempiricalneuralnetworkdistance dF(ePn eQn) whereePnandeQndenotetheempiricaldistributionofnsamples.Inspiredfromtherecentgeneralizationresultsin[3] weshowthatthisempiricaldistanceminimizationleadstosmalldF(P Q)uptoanadditiveerrorthatvanisheswithanincreasingsamplesize.Asshownin[3] LipschitzandboundedfunctionclassesarecriticalinachievingsampleefﬁciencyforGANs.Wefollowthesameapproachoverasimilarfunctionclass.LetFp L={Du(x y)∈[0 1]|Du(x y)isL-Lipschitzinuandu∈U⊆Rp} (11)beaclassofboundedfunctionswithparameteru∈Rp.WesaythatFisL-Lipschitzinuif|Du1(x y)−Du2(x y)|≤Lku1−u2k ∀u1 u2∈U x∈X y∈[m].(12)Theorem3.ForanyclassFp LofboundedLipschitzfunctionsDu(x y)satisfyingAssumption1 thereexistsauniversalconstantc>0suchthatdFp L(ePn eQn)−≤dFp L(P Q)≤|||C−1|||∞(cid:0)dFp L(ePn eQn)+(cid:1) (13)withprobabilityatleast1−e−pforanyε>0andnlargeenough n≥(cp/2)log(pL/).WerefertoAppendixIforaproof.ThisjustiﬁestheproposedRCGANwhichminimizesdF(ePn eQn) asitleadstothegeneratorQbeingclosetotherealdistributionPinneuralnetworkdistance dF(P Q).TheseboundsinherittheapproximabilityofthepopulationversionfromTheorem2.4Oursecondarchitecture:RCGAN-UInmanyrealworldscenariostheconfusionmatrixCisunknown.WeproposeRCGAN-Unknown(RCGAN-U)algorithmwhichjointlyestimatestherealdistributionPandthenoisemodelC.Thepre-processingandDstepsoftheRCGAN-UarethesameasthoseofRCGAN assumingthecurrentguessMoftheconfusionmatrix.AstheG-stepin(2)isnotdifferentiableinC weusethefollowingreparameterizedestimatoroftheloss motivatedbysimilartechniqueintrainingclassiﬁersfromnoisylabels:minG∈G M∈CEz∼Ny∼PY(cid:2)φM(G(z;y) y D)+λl(h∗(G(z;y)) y)(cid:3)whereCisthesetofalltransitionmatricesandφM(x y D)=Pey∈[m]Myeyφ(1−D(x ey)).5ExperimentsImplementationdetailsareexplainedinAppendixL.Weconsiderone-coinbasedmodels whichareparameterizedbytheirlabelaccuracyprobabilityα.Inthismodelasamplewithtruelabelyisﬂippeduniformlyatrandomtolabeleyin[m]\{y}withprobability1−α.TheentriesofitsconfusionmatrixC willthenbeCii=αandCi6=j=(1−α)/(m−1) wheremisthenumberofclasses.Wecallthismodeluniformﬂippingmodel.Codetoreproduceourexperimentsisavailableathttps://github.com/POLane16/Robust-Conditional-GAN.Baselines.FirstisthebiasedGAN whichisaconditionalGANapplieddirectlyonthenoisydata.Thelossishencebiased andthetrueconditionaldistributionisnottheoptimalsolutionofthisbiasedloss.Nextnaturalbaselineisusingde-biasedclassiﬁerasthediscriminator motivatedbytheapproachof[34]onlearningclassiﬁersfromnoisylabels.ThemaininsightistomodifythelossfunctionaccordingtoC suchthatinexpectationthelossmatchesthatofthecleandata.WerefertothisapproachasunbiasedGAN.Concretely whentrainingthediscriminator weproposethefollowing(modiﬁed)de-biasedloss:maxD∈FE(x ey)∼ePX eY(cid:2)Xy∈[m](C−1)eyyφ(D(x y))(cid:3)+Ez∼Ny∼PY(cid:2)φ(1−D(G(z;y) y))(cid:3).(14)Thisisunbiased astheﬁrsttermisequivalenttoE(x y)∼PX Y[φ(D(x y))] whichisthestandardGANlosswithcleansamples.However suchde-biasingissensitivetotheconditionnumberofC andcanbecomenumericallyunstablefornoisychannelsasC−1haslargeentries[20].Forboththedataset weuselinearclassiﬁersforpermutationregularizeroftheRCGAN-Uarchitecture.70.00.10.20.30.40.50.60.70.80.90.00.20.40.60.81.0RCGAN+yRCGANRCGAN-UUn-biasedBiasednoiseintherealdata(1−α)generatorlabelaccuracy0.00.10.20.30.40.50.60.70.80.90.00.20.40.60.81.0RCGAN+yRCGANRCGAN-UUn-biasedBiasedUn-biasedclassiﬁernoiseintherealdata(1−α)labelrecoveryaccuracyFigure2:NoisyMNISTdataset:OurRCGANmodelsconsistentlyimprovesuponallcompetingbaselineapproachesingeneratorlabelaccuracy(left).Thetrendcontinuesinlabelrecoveryaccuracy(right) whereourproposedRCGAN-classiﬁersimprovesuponunbiasedclassiﬁer[34] whichisoneofthestate-of-the-artapproachestailoredforlabelrecovery.5.1MNISTWetrainﬁvearchitecturesonMNISTdatasetcorruptedbytheuniformﬂippingnoise:RCGAN+y RCGAN RCGAN-U unbiasedGAN andbiasedGAN.RCGAN+yarchitecturehasthesamearchitectureasRCGANbuttheinputtotheﬁrstlayerofitsdiscriminatorisconcatenatedwithaone-hotrepresentationofthelabel.WediscussourtechniquestoovercomethechallengesinvolvedintrainingRCGAN+yinAppendixL.Conditionalgeneratorscanbeusedtogeneratesamplesxfromaparticularclassy intheclassesitlearned.Wethencanuseapre-trainedclassiﬁerftocompareytothetrueclassofthesample f(x)(asperceivedbytheclassiﬁerf).WecomparethegeneratorlabelaccuracydeﬁnedasEy∼PY Z∼N[I{y=f(G(z y))}] inFigure2 leftpanel.Wegenerated10klabelschosenuniformlyatrandomandcorrespondingconditionalsamplesfromthegenerators andcalculatedthegeneratorlabelaccuracyusingaCNNclassiﬁerpre-trainedonthecleanMNISTdatatoanaccuracyof99.2%.TheproposedRCGANsigniﬁcantlyimprovesuponthecompetingbaselines andachievesalmostperfectlabelaccuracyuntilahighnoiseofα=0.3.RCGAN+yfurtherimprovesuponRCGANandtogainveryhighaccuracyevenatα=0.125.ThehighaccuracyofRCGAN-UsuggeststhatrobusttrainingispossiblewithoutpriorknowledgeoftheconfusionmatrixC.Asexpected biasedGANhasanaccuracyofapproximately1−α.AnimmediateapplicationofrobustGANsisrecoveringthetruelabelsofthenoisytrainingdata whichisanimportantandchallengingproblemincrowdsourcing.Weproposeanewmeta-algorithm whichwecallcGAN-label-recovery whichuseanyconditionalgeneratorG(z y)trainedonthenoisysamples toestimatethetruelabel asˆy ofasamplexusingthefollowingoptimization.ˆy∈argminy∈[m](cid:8)minzy|||G(zy y)−x|||22(cid:9).(15)IntherightpanelofFigure2wecomparethelabelrecoveryaccuracyofthemeta-algorithmusingtheﬁveconditionalGANs on500randomlychosennoisytrainingsamples.Thisisalsocomparedtoastate-of-the-artmethod[34]forlabelrecovery whichproposedminimizingunbiasedlossfunctiongiventhenoisylabelsandtheconfusionmatrix.Thisunbiasedclassiﬁer wasshowntooutperformstherobustclassiﬁers[29 45 46]andcanbeusedtopredictthetruelabelofthetrainingexamples.InFigures5ofAppendixM weshowexampleimagesfromallthegenerators.5.2CIFAR-10InFigure3 weshowtheinceptionscore[40]andthelabelaccuracyoftheconditionalgeneratorforthefourapproaches:ourproposedRCGANandRCGAN-U againstthebaselinesUnbiased(Section5)andBiased(Section1)GANstrainedusingCIFAR-10images[24] whilevaryingthelabelaccuracyoftherealdataunderuniformﬂippingmodel.InRCGAN-U evenwiththeregularizer thelearnedconfusionmatrixwasapermutedversionofthetrueC possiblybecausealinearclassiﬁermightbetoosimpletoclassifyCIFARimages.Tocombatthis weinitializedtheconfusionmatrixMtobediagonallydominant(AppendixL).80.00.20.40.60.87.47.67.88.08.28.4RCGAN-URCGANUn-biasedBiasednoiseintherealdata(1−α)Inceptionscore0.00.20.40.60.80.10.20.30.40.50.60.70.8RCGAN-URCGANUn-biasedBiasednoiseintherealdata(1−α)GeneratorlabelaccuracyFigure3:NoisyCIFAR-10dataset:OurRCGAN(red)andRCGAN-U(blue)consistentlyimprovesuponUnbiased(magenta)andBiased(black)GANstrainedonnoisyCIFAR-10ininceptionscores(left)andingeneratorlabelaccuracy(right).IntheleftpanelofFigure3 ourRCGANandRCGAN-Uconsistentlyachievehigherinceptionscoresthantheothertwoapproaches.TheUnbiasedGANishighlyunstableandhenceproducesgarbageimagesforlargenoise(Fig.6) possiblyduetonumericalinstabilityof|||C−1|||∞ asnotedin[20].ThisconﬁrmsthatrobustGANsnotonlyproduceimagesfromthecorrectclass butalsoproducebetterqualityimages.IntherightpanelofFigure3 wereportthegeneratorlabelaccuracy(Section5.1)on1ksamplesgeneratedbyeachGAN.WeclassifythegeneratorimagesusingaResNet-110model1trainedtoanaccuracyof92.3%onthenoiselessCIFAR-10dataset.BiasedGANhassigniﬁcantlylowerlabelaccuracywhereastheUnbiasedGANhaslowinceptionscore.InFigure6inAppendixM weshowexampleimagesfromthethreegeneratorsforthedifferentﬂippingprobabilities.WebelievethatthegaininusingtheproposedrobustGANswillbelarger whenwetraintohigheraccuracywithlargernetworksandextensivehyperparametertuning withlatestinnovationsinGANarchitectures forexample[54 28 17 19 41].6ConclusionStandardconditionalGANscanbesensitivetonoiseinthelabelsofthetrainingdata.Weproposetwonewarchitecturestomakethemrobust onerequiringtheknowledgeofthedistributionofthenoiseandanotherwhichdoesnot anddemonstratetherobustnessonbenchmarkdatasetsofCIFAR-10andMNIST.Wefurthershowcasehowthelearnedgeneratorcanbeusedtorecoverthecorruptedlabelsinthetrainingdata whichcanpotentiallybeusedinpracticalapplications.TheproposedarchitecturecombinesthenoiseaddingideaofAmbientGAN[10] projectiondiscriminatorof[32] andregularizerssimilartothoseinInfoGAN[11].InspiredbyAmbientGAN[10] themainideaistopairthegeneratoroutputimagewithalabelthatispassedthroughanoisychannel beforefeedingtothediscriminator.Wejustifythisideaofnoiseaddingbyidentifyingacertainclassofdiscriminatorsthathavegoodgeneralizationproperties.Inparticular weprovethatprojectiondiscriminator introducedin[32] hasagoodgeneralizationproperty.Weshowcasethattheproposedarchitecture whentrainedwitharegularizer hassuperiorrobustnessonbenchmarkdatasets.AcknowledgementThisworkissupportedbyNSFawardsCNS-1527754 CCF-1553452 CCF-1705007 RI-1815535andGoogleFacultyResearchAward.ThisworkusedtheExtremeScienceandEngineeringDiscoveryEnvironment(XSEDE) whichissupportedbyNationalScienceFoundationgrantnumberOCI-1053575.Speciﬁcally itusedtheBridgessystem whichissupportedbyNSFawardnumberACI-1445606 atthePittsburghSupercomputingCenter(PSC).ThisworkispartiallysupportedbythegenerousresearchcreditsonAWScloudcomputingresourcesfromAmazon.1https://github.com/wenxinxu/resnet-in-tensorflow9References[1]PanosAchlioptas OlgaDiamanti IoannisMitliagkas andLeonidasGuibas.Representationlearningandadversarialgenerationof3Dpointclouds.arXivpreprintarXiv:1707.02392 2017.[2]MartinArjovsky SoumithChintala andLéonBottou.WassersteinGAN.arXivpreprintarXiv:1701.07875 2017.[3]SanjeevArora RongGe YingyuLiang TengyuMa andYiZhang.Generalizationandequilibriumingenerativeadversarialnets(GANs).arXivpreprintarXiv:1703.00573 2017.[4]YuBai TengyuMa andAndrejRisteski.ApproximabilityofdiscriminatorsimpliesdiversityinGANs.arXivpreprintarXiv:1806.10586 2018.[5]DavidBerthelot TomSchumm andLukeMetz.BEGAN:Boundaryequilibriumgenerativeadversarialnetworks.arXivpreprintarXiv:1703.10717 2017.[6]GBiau BCadre MSangnier andUTanielian.SometheoreticalpropertiesofGANs.arXivpreprintarXiv:1803.07819 2018.[7]BattistaBiggio BlaineNelson andPavelLaskov.Supportvectormachinesunderadversariallabelnoise.InAsianConferenceonMachineLearning pages97–112 2011.[8]MikołajBi´nkowski DougalJSutherland MichaelArbel andArthurGretton.DemystifyingMMDGANs.arXivpreprintarXiv:1801.01401 2018.[9]AshishBora AjilJalal EricPrice andAlexandrosGDimakis.Compressedsensingusinggenerativemodels.arXivpreprintarXiv:1703.03208 2017.[10]AshishBora EricPrice andAlexandrosGDimakis.AmbientGAN:Generativemodelsfromlossymeasurements.InInternationalConferenceonLearningRepresentations(ICLR) 2018.[11]XiChen YanDuan ReinHouthooft JohnSchulman IlyaSutskever andPieterAbbeel.Info-GAN:Interpretablerepresentationlearningbyinformationmaximizinggenerativeadversarialnets.InAdvancesinNeuralInformationProcessingSystems pages2172–2180 2016.[12]AlexanderPhilipDawidandAllanMSkene.Maximumlikelihoodestimationofobservererror-ratesusingtheemalgorithm.Appliedstatistics pages20–28 1979.[13]EmilyLDenton SoumithChintala RobFergus etal.DeepgenerativeimagemodelsusingaLaplacianpyramidofadversarialnetworks.InAdvancesinneuralinformationprocessingsystems pages1486–1494 2015.[14]IanGoodfellow JeanPouget-Abadie MehdiMirza BingXu DavidWarde-Farley SherjilOzair AaronCourville andYoshuaBengio.Generativeadversarialnets.InAdvancesinneuralinformationprocessingsystems pages2672–2680 2014.[15]IshaanGulrajani FarukAhmed MartinArjovsky VincentDumoulin andAaronCCourville.ImprovedtrainingofWassersteinGANs.InAdvancesinNeuralInformationProcessingSystems pages5769–5779 2017.[16]PhillipIsola Jun-YanZhu TinghuiZhou andAlexeiAEfros.Image-to-imagetranslationwithconditionaladversarialnetworks.arXivpreprint 2017.[17]AlexiaJolicoeur-Martineau.Therelativisticdiscriminator:akeyelementmissingfromstandardGAN.arXivpreprintarXiv:1807.00734 2018.[18]DavidRKarger SewoongOh andDevavratShah.Iterativelearningforreliablecrowdsourcingsystems.InAdvancesinneuralinformationprocessingsystems pages1953–1961 2011.[19]TeroKarras TimoAila SamuliLaine andJaakkoLehtinen.ProgressivegrowingofGANsforimprovedquality stability andvariation.arXivpreprintarXiv:1710.10196 2017.[20]AshishKhetan ZacharyCLipton andAnimaAnandkumar.Learningfromnoisysingly-labeleddata.arXivpreprintarXiv:1712.04577 2017.10[21]TaeksooKim MoonsuCha HyunsooKim JungkwonLee andJiwonKim.Learningtodiscovercross-domainrelationswithgenerativeadversarialnetworks.arXivpreprintarXiv:1703.05192 2017.[22]DiederikPKingmaandMaxWelling.Auto-encodingvariationalbayes.arXivpreprintarXiv:1312.6114 2013.[23]MuratKocaoglu ChristopherSnyder AlexandrosGDimakis andSriramVishwanath.Causal-GAN:Learningcausalimplicitgenerativemodelswithadversarialtraining.arXivpreprintarXiv:1709.02023 2017.[24]AlexKrizhevskyandGeoffreyHinton.Learningmultiplelayersoffeaturesfromtinyimages.Technicalreport Citeseer 2009.[25]YannLeCun.Themnistdatabaseofhandwrittendigits.http://yann.lecun.com/exdb/mnist/ 1998.[26]ChristianLedig LucasTheis FerencHuszár JoseCaballero AndrewCunningham AlejandroAcosta AndrewAitken AlykhanTejani JohannesTotz ZehanWang etal.Photo-realisticsingleimagesuper-resolutionusingagenerativeadversarialnetwork.arXivpreprint 2016.[27]XiaodanLiang LisaLee WeiDai andEricPXing.DualmotionGANforfuture-ﬂowembeddedvideoprediction.arXivpreprint 2017.[28]ZinanLin AshishKhetan GiuliaFanti andSewoongOh.PacGAN:Thepoweroftwosamplesingenerativeadversarialnetworks.arXivpreprintarXiv:1712.04086 2017.[29]BingLiu YangDai XiaoliLi WeeSunLee andPhilipSYu.Buildingtextclassiﬁersusingpositiveandunlabeledexamples.InDataMining 2003.ICDM2003.ThirdIEEEInternationalConferenceon pages179–186.IEEE 2003.[30]MehdiMirzaandSimonOsindero.Conditionalgenerativeadversarialnets.arXivpreprintarXiv:1411.1784 2014.[31]TakeruMiyato ToshikiKataoka MasanoriKoyama andYuichiYoshida.Spectralnormalizationforgenerativeadversarialnetworks.arXivpreprintarXiv:1802.05957 2018.[32]TakeruMiyatoandMasanoriKoyama.cGANswithprojectiondiscriminator.arXivpreprintarXiv:1802.05637 2018.[33]SudiptoMukherjee HimanshuAsnani EugeneLin andSreeramKannan.ClusterGAN:Latentspaceclusteringingenerativeadversarialnetworks.arXivpreprintarXiv:1809.03627 2018.[34]NagarajanNatarajan InderjitSDhillon PradeepKRavikumar andAmbujTewari.Learningwithnoisylabels.InAdvancesinneuralinformationprocessingsystems pages1196–1204 2013.[35]AnhNguyen JasonYosinski YoshuaBengio AlexeyDosovitskiy andJeffClune.Plug&playgenerativenetworks:Conditionaliterativegenerationofimagesinlatentspace.arXivpreprintarXiv:1612.00005 2016.[36]AugustusOdena ChristopherOlah andJonathonShlens.Conditionalimagesynthesiswithauxiliaryclassiﬁergans.arXivpreprintarXiv:1610.09585 2016.[37]AlecRadford LukeMetz andSoumithChintala.Unsupervisedrepresentationlearningwithdeepconvolutionalgenerativeadversarialnetworks.arXivpreprintarXiv:1511.06434 2015.[38]ScottReed ZeynepAkata XinchenYan LajanugenLogeswaran BerntSchiele andHonglakLee.Generativeadversarialtexttoimagesynthesis.arXivpreprintarXiv:1605.05396 2016.[39]OlgaRussakovsky JiaDeng HaoSu JonathanKrause SanjeevSatheesh SeanMa ZhihengHuang AndrejKarpathy AdityaKhosla MichaelBernstein etal.Imagenetlargescalevisualrecognitionchallenge.InternationalJournalofComputerVision 115(3):211–252 2015.11[40]TimSalimans IanGoodfellow WojciechZaremba VickiCheung AlecRadford andXiChen.ImprovedtechniquesfortrainingGANs.InAdvancesinNeuralInformationProcessingSystems pages2234–2242 2016.[41]MaziarSanjabi JimmyBa MeisamRazaviyayn andJasonDLee.SolvingapproximateWassersteinGANstostationarity.arXivpreprintarXiv:1802.08249 2018.[42]RajatSen KarthikeyanShanmugam HimanshuAsnani ArmanRahimzamani andSreeramKannan.Mimicandclassify:Ameta-algorithmforconditionalindependencetesting.arXivpreprintarXiv:1806.09708 2018.[43]AshishShrivastava TomasPﬁster OncelTuzel JoshSusskind WendaWang andRussWebb.Learningfromsimulatedandunsupervisedimagesthroughadversarialtraining.InTheIEEEConferenceonComputerVisionandPatternRecognition(CVPR) volume3 page6 2017.[44]BharathKSriperumbudur KenjiFukumizu ArthurGretton BernhardSchölkopf andGertRGLanckriet.Onintegralprobabilitymetrics φ-divergencesandbinaryclassiﬁcation.arXivpreprintarXiv:0901.2698 2009.[45]GuillaumeStempfelandLivaRalaivola.Learningkernelperceptronsonnoisydatausingrandomprojections.InInternationalConferenceonAlgorithmicLearningTheory pages328–342.Springer 2007.[46]GuillaumeStempfelandLivaRalaivola.LearningSVMsfromsloppilylabeleddata.InInternationalConferenceonArtiﬁcialNeuralNetworks pages884–893.Springer 2009.[47]SainbayarSukhbaatar JoanBruna ManoharPaluri LubomirBourdev andRobFergus.Trainingconvolutionalnetworkswithnoisylabels.arXivpreprintarXiv:1406.2080 2014.[48]DougalJSutherland Hsiao-YuTung HeikoStrathmann SoumyajitDe AadityaRamdas AlexSmola andArthurGretton.Generativemodelsandmodelcriticismviaoptimizedmaximummeandiscrepancy.arXivpreprintarXiv:1611.04488 2016.[49]AaronvandenOord NalKalchbrenner andKorayKavukcuoglu.Pixelrecurrentneuralnetworks.arXivpreprintarXiv:1601.06759 2016.[50]CarlVondrick HamedPirsiavash andAntonioTorralba.Generatingvideoswithscenedynamics.InAdvancesInNeuralInformationProcessingSystems pages613–621 2016.[51]JiajunWu ChengkaiZhang TianfanXue BillFreeman andJoshTenenbaum.Learningaprobabilisticlatentspaceofobjectshapesvia3Dgenerative-adversarialmodeling.InAdvancesinNeuralInformationProcessingSystems pages82–90 2016.[52]ZhiXu ChengtaoLi andStefanieJegelka.RobustGANsagainstdishonestadversaries.arXivpreprintarXiv:1802.09700 2018.[53]RaymondYeh ChenChen TeckYianLim MarkHasegawa-Johnson andMinhNDo.Semanticimageinpaintingwithperceptualandcontextuallosses.arXivpreprintarXiv:1607.07539 2016.[54]HanZhang IanGoodfellow DimitrisMetaxas andAugustusOdena.Self-attentiongenerativeadversarialnetworks.arXivpreprintarXiv:1805.08318 2018.[55]HanZhang TaoXu HongshengLi ShaotingZhang XiaoleiHuang XiaogangWang andDimitrisMetaxas.StackGAN:Texttophoto-realisticimagesynthesiswithstackedgenerativeadversarialnetworks.InIEEEInt.Conf.Comput.Vision(ICCV) pages5907–5915 2017.[56]PengchuanZhang QiangLiu DengyongZhou TaoXu andXiaodongHe.Onthediscrimination-generalizationtradeoffinGANs.arXivpreprintarXiv:1711.02771 2017.[57]Jun-YanZhu PhilippKrähenbühl EliShechtman andAlexeiAEfros.Generativevisualmanipulationonthenaturalimagemanifold.InEuropeanConferenceonComputerVision pages597–613.Springer 2016.[58]Jun-YanZhu TaesungPark PhillipIsola andAlexeiAEfros.Unpairedimage-to-imagetranslationusingcycle-consistentadversarialnetworks.arXivpreprintarXiv:1703.10593 2017.12,Kiran Thekumparampil
Ashish Khetan
Zinan Lin
Sewoong Oh