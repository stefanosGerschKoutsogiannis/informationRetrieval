2014,Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning,The combination of modern Reinforcement Learning and Deep Learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection. The Arcade Learning Environment (ALE) provides a set of Atari games that represent a useful benchmark set of such applications. A recent breakthrough in combining model-free reinforcement learning with deep learning  called DQN  achieves the best real-time agents thus far. Planning-based approaches achieve far higher scores than the best model-free approaches  but they exploit information that is not available to human players  and they are orders of magnitude slower than needed for real-time play. Our main goal in this work is to build a better real-time Atari game playing agent than DQN. The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play. We proposed new agents based on this idea and show that they outperform DQN.,Deep Learning for Real-Time Atari Game Play
Using Ofﬂine Monte-Carlo Tree Search Planning

Xiaoxiao Guo

Computer Science and Eng.

University of Michigan
guoxiao@umich.edu

Satinder Singh

Computer Science and Eng.

University of Michigan
baveja@umich.edu

Honglak Lee

Computer Science and Eng.

University of Michigan
honglak@umich.edu

Richard Lewis

Department of Psychology

University of Michigan
rickl@umich.edu

Xiaoshi Wang

Computer Science and Eng.

University of Michigan

xiaoshiw@umich.edu

Abstract

The combination of modern Reinforcement Learning and Deep Learning ap-
proaches holds the promise of making signiﬁcant progress on challenging appli-
cations requiring both rich perception and policy-selection. The Arcade Learning
Environment (ALE) provides a set of Atari games that represent a useful bench-
mark set of such applications. A recent breakthrough in combining model-free
reinforcement learning with deep learning  called DQN  achieves the best real-
time agents thus far. Planning-based approaches achieve far higher scores than the
best model-free approaches  but they exploit information that is not available to
human players  and they are orders of magnitude slower than needed for real-time
play. Our main goal in this work is to build a better real-time Atari game playing
agent than DQN. The central idea is to use the slow planning-based agents to pro-
vide training data for a deep-learning architecture capable of real-time play. We
proposed new agents based on this idea and show that they outperform DQN.

Introduction

1
Many real-world Reinforcement Learning (RL) problems combine the challenges of closed-loop
action (or policy) selection with the already signiﬁcant challenges of high-dimensional perception
(shared with many Supervised Learning problems). RL has made substantial progress on theory
and algorithms for policy selection (the distinguishing problem of RL)  but these contributions have
not directly addressed problems of perception. Deep learning (DL) approaches have made remark-
able progress on the perception problem (e.g.  [13  20]) but do not directly address policy selection.
RL and DL methods share the aim of generality  in that they both intend to minimize or eliminate
domain-speciﬁc engineering  while providing “off-the-shelf” performance that competes with or ex-
ceeds systems that exploit control heuristics and hand-coded features. Combining modern RL and
DL approaches therefore offers the potential for general methods that address challenging applica-
tions requiring both rich perception and policy-selection.
The Arcade Learning Environment (ALE) is a relatively new and widely accessible class of bench-
mark RL problems [1] that provide a particularly challenging combination of policy selection and
perception. ALE includes an emulator and a large number of Atari 2600 (a 1970s–80s home-video
console) games. The complexity and diversity of the games—both in terms of perceptual challenges
in mapping pixels to useful features for control and in terms of the control policies needed—make
ALE a useful set of benchmark RL problems  especially for evaluating general methods intended to
achieve success without hand-engineered features.

1

Since the introduction of ALE  there have been a number of attempts to build general-purpose Atari
game playing agents. The departure point for this paper is a recent and signiﬁcant breakthrough [19]
that combines RL and DL to build agents for multiple Atari games. It achieved the best machine-
agent real-time game play to date (in some games close to or better than human-level play)  does not
require feature engineering  and indeed reuses the same perception architecture and RL algorithm
across all the games. We believe that continued progress on the ALE environment that preserves
these advantages will extend to broad advances in other domains with signiﬁcant perception and
policy selection challenges. Thus  our immediate goal in the work reported here is to build even
better performing general-purpose Atari game playing agents. We achieve this by introducing new
methods for combining RL and DL that use slow  off-line Monte Carlo tree search planning methods
to generate training data for a deep-learned classiﬁer capable of state-of-the-art real-time play.
2 Brief Background on RL and DL and Challenges of Perception
RL and more broadly decision-theoretic planning have a suite of methods that address the chal-
lenge of selecting/learning good policies  including value function approximation  policy search  and
Monte-Carlo Tree Search [11  12] (MCTS). These methods have different strengths and weaknesses 
and there is increasing understanding of how to match them to different types of RL-environments.
Indeed  an accumulating number of applications attest to this success. But it is still not the case that
there are reasonably off-the-shelf approaches to solving complex RL problems of interest to Artiﬁ-
cial Intelligence (AI)  such as the games in ALE. One reason for this is that despite major advances
there hasn’t been an off-the-shelf approach to signiﬁcant perception problems. The perception prob-
lem itself has two components: 1) the sensors at any time step do not capture all the information
in the history of observations  leading to partial observability  and 2) the sensors provide very high-
dimensional observations that introduce computational and sample-complexity challenges for policy
selection.
One way to handle the perception challenges when a model of the RL environment is available is
to avoid the perception problem entirely by eschewing the building of an explicit policy and instead
using repeated incremental planning via MCTS methods such as UCT [12] (discussed below). Either
when a model is not available  or when an explicit representation of the policy is required  the usual
approach to applied RL success has been to use expert-developed task-speciﬁc features of a short
history of observations in combination with function approximation methods and some trial-and-
error on the part of the application developer (on small enough problems this can be augmented with
some automated feature selection methods). Eliminating the dependence of applied RL success on
engineered features motivates our interest in combining RL and DL (though see [23] for an example
of early work in this direction).
Over the past decades  deep learning (see [5  22] for a survey) has emerged as a powerful technique
for learning feature representations from data (again  this is in contrast to the usual approach of
hand-crafting of features by domain experts). For example  DL has achieved state-of-the-art results
in image classiﬁcation [13  6]  speech recognition [18  20  8]  and activity recognition [15  10]. In
DL  features are learned in a compositional hierarchy. Speciﬁcally  low-level features are learned to
encode low-level statistical dependencies (e.g.  “edges” in images)  and higher-level features encode
higher-order dependencies of the lower-level features (e.g.  “object parts”) [17]. In particular  for
data that has strong spatial or temporal dependencies  convolutional neural networks [16] have been
shown to learn invariant high-level features that are informative for supervised tasks. Such convolu-
tional neural networks were used in the recent successful combination of DL and RL for Atari Game
playing [19] that forms the departure point of our work. We describe this work in more detail below.
3 Existing Work on Atari Games and a Performance Gap
While the games in ALE are simpler than many modern games  they still pose signiﬁcant challenges
to human players. In RL terms  for a human player these games are Partially-Observable Markov
Decision Processes (POMDPs). The true state of each game at any given point is captured by the
contents of the limited random-access memory (RAM). A human player does not observe the state
and instead perceives the game screen (frame) which is a 2D array of 7-bit pixels  160 pixels wide
by 210 pixels high. The action space available to the player depends on the game but maximally
consists of the 18 discrete actions deﬁned by the joystick controller. The next state is a deterministic
function of the previous state and the player’s action choice. Any stochasticity in these games is
limited to the choice of the initial state of the game (which can include a random number seed

2

stored in RAM). So even though the state transitions are deterministic  the transitions from history
of observations and actions to next observation can be stochastic (because of the stochastic initial
hidden state). The immediate reward at any given step is deﬁned by the game and made available by
the ALE; it is usually a function of the current frame or the difference between current and previous
frames. When running in real-time  the simulator generates 60 frames per second. All the games we
consider terminate in a ﬁnite number of time-steps (and so are episodic). The goal in these games is
to select an optimal policy  i.e.  to select actions in such a way so as to maximize the expected value
of the cumulative sum of rewards until termination.
Model-Free RL Agents for Atari Games. Here we discuss work that does not access the state
in the games and thus solves the game as a POMDP. In principle  one could learn a state repre-
sentation and infer an associated MDP model using frame-observation and action trajectories  but
these games are so complex that this is rarely done (though see [4] for a recent attempt). Instead 
partial observability is dealt with by hand-engineering features of short histories of frames observed
so far and model-free RL methods are used to learn good policies as a function of those feature rep-
resentations. For example  the paper that introduced ALE [1]  used SARSA with several different
hand-engineered feature sets. The contingency awareness approach [2] improved performance of
the SARSA algorithm by augmenting the feature sets with a learned representation of the parts of
the screen that are under the agent’s control. The sketch-based approach [3] further improves per-
formance by using the tug-of-war sketch features. HyperNEAT-GGP [9] introduces an evolutionary
policy search based Atari game player. Most recently Deep Q-Network (hereafter DQN) [19] uses
a modiﬁed version of Q-Learning with a convolutional neural network (CNN) with three hidden
layers for function approximation. This last approach is the state of the art in this class of methods
for Atari games and is the basis for our work; we present the relevant details in Section 5. It does
not use hand-engineered features but instead provides the last four raw frames as input (four instead
of one to alleviate partial observability).
Planning Agents for Atari Games Based on UCT. These approaches access the state of the game
from the emulator and hence face a deterministic MDP (other than the random choice of initial state).
They incrementally plan the action to take in the current state using UCT  an algorithm widely used
for games. UCT has three parameters  the number of trajectories  the maximum-depth (uniform for
each trajectory)  and an exploration parameter (a scalar set to 1 in all our experiments). In general 
the larger the trajectory & depth parameters are  the slower UCT is but the better it is. UCT uses the
emulator as a model to simulate trajectories as follows. Suppose it is generating the kth trajectory
and the current node is at depth d and the current state is s. It computes a score for each possible
action a in state-depth pair (s  d) as the sum of two terms  an exploitation term that is the Monte-
Carlo average of the discounted sum of rewards obtained from experiences with state-depth pair

(s  d) in the previous k − 1 trajectories  and an exploration term that is(cid:112)log (n(s  d))/n(s  a  d)

where n(s  d) and n(s  a  d) are the number of simulated-experiences of state-depth pair (s  d)  and
of action a in state-depth pair (s  d) respectively in the previous k − 1 trajectories. UCT selects the
action to simulate in order to extend the trajectory greedily with respect to this summed score. Once
the input-parameter number of trajectories are generated each to maximum depth  UCT returns the
exploitation term for each action at the root node (which is the current state it is planning an action
for) as its estimate of the utility of taking that action in the current state of the game. UCT has the
nice theoretical property that the number of simulation steps (number of trajectories × maximum-
depth) needed to ensure any bound on the loss of following the UCT-based policy is independent of
the size of the state space. This result captures the fact that the use of UCT avoids the perception
problem  but at the cost of requiring substantial computation for every time step of action selection
because it never builds an explicit policy.
Performance Gap & Our Opportunity. The opportunity for this paper arises from the following
observations. The model-free RL agents for Atari games are fast (indeed faster than real-time  e.g. 
the CNN-based approach from our paper takes 10−4 seconds to select an action on our computer)
while the UCT-based planning agents are several orders of magnitude slower (much slower than
real-time  e.g.  they take seconds to select an action on the same computer). On the other hand 
the performance of UCT-based planning agents is much better than the performance of model-free
RL agents (this will be evident in our results below). Our goal is to develop methods that retain
the DL advantage of not needing hand-crafted features and the online real-time play ability of the
model-free RL agents by exploiting data generated by UCT-planning agents.

3

4 Methods for Combining UCT-based RL with DL
We ﬁrst describe the baseline UCT agent  and then three agents that instantiate different methods of
combining the UCT agent with DL. Recall that in keeping with the goal of building general-purpose
methods as in the DQN work we impose the constraint of reusing the same input representations 
the same function approximation architecture  and the same planning method for all the games.
4.1 Baseline UCT Agent that Provides Training Data
This agent requires no training. It does  however  require speciﬁcation of its two parameters  the
number of trajectories and the maximum-depth. Recall that our proposed new agents will all use
data from this UCT-agent to train a CNN-based policy and so it is reasonable that the resulting
performance of our proposed agents will be worse than that of the UCT-agent. Therefore  in our
experiments we set these two parameters large enough to ensure that they outscore the published
DQN scores  but not so large that they make our computational experiments unreasonably slow.
Speciﬁcally  we elected to use 300 as maximum-depth and 10000 as number of trajectories for all
games but two. Pong turns out to be a much simpler game and we could reduce the number of
trajectories to 500  and Enduro turned out to have more distal rewards than the other games and so
we used a maximum-depth of 400. As will be evident from the results in Section 5  this allowed the
UCT agent to signiﬁcantly outperform DQN in all games but Pong in which DQN already performs
perfectly. We emphasize that the UCT agent does not meet our goal of real-time play. For example 
to play a game just 800 times with the UCT agent (we do this to collect training data for our agent’s
below) takes a few days on a recent multicore computer for each game.
4.2 Our Three Methods and their Corresponding Agents
Method 1: UCTtoRegression (for UCT to CNN via Regression). The key idea is to use the action
values computed by the UCT-agent to train a regression-based CNN. The following is done for each
game. Collect 800 UCT-agent runs by playing the game 800 times from start to ﬁnish using the UCT
agent above. Build a dataset (table) from these runs as follows. Map the last four frames of each state
along each trajectory into the action-values of all the actions as computed by UCT. This training data
is used to train the CNN via regression (see below for CNN details). The UCTtoRegression-agent
uses the CNN learned by this training procedure to select actions during evaluation.
Method 2: UCTtoClassiﬁcation (for UCT to CNN via Classiﬁcation). The key idea is to use the
action choice computed by the UCT-agent (selected greedily from action-values) to train a classiﬁer-
based CNN. The following is done for each game. Collect 800 UCT-agent runs as above. These runs
yield a table in which the rows correspond to the last four frames at each state along each trajectory
and the single column is the choice of action that is best according to the UCT-agent at that state
of the trajectory. This training data is used to train the CNN via multiclass classiﬁcation (see below
for CNN details). The UCTtoClassiﬁcation-agent uses the CNN-classiﬁer learned by this training
procedure to select actions during evaluation.
One potential issue with the above two agents is that the training data’s input distribution is generated
by the UCT-agent while during testing the UCTtoRegression and UCTtoClassiﬁcation agents will
perform differently from the UCT-agent and thus could experience an input distribution quite differ-
ent from that of the UCT-agent’s. This could limit the testing performance of the UCTtoRegression
and UCTtoClassiﬁcation agents. Thus  it might be desirable to somehow bias the distribution over
inputs to those likely to be encountered by these agents; this observation motivates our next method.
Method 3: UCTtoClassiﬁcation-Interleaved (for UCT to CNN via Classiﬁcation-Interleaved).
The key idea is to focus UCT planning on that part of the state space experienced by the (partially
trained) CNN player. The method accomplishes this by interleaving training and data collection as
follows.1 Collect 200 UCT-agent runs as above; these will obviously have the same input distribution
concern raised above. The data from these runs is used to train the CNN via multinomial classiﬁca-
tion just as in the UCTtoClassiﬁcation-agent’s method (we do not do this for the UCTtoRegression-
agent because as we show below it performs worse than the UCTtoClassiﬁcation-agent). The trained
CNN is then used to decide action choices in collecting further 200 runs (though 5% of the time a

1Our UCTtoClassiﬁcation-Interleaved method is a special case of DAgger [21] (in the use of a CNN-
classiﬁer and in the use of speciﬁc choices of parameters β1 = 1  and for i > 1  βi = 0). As a small
point of difference  we note that our emphasis in this paper was in the use of CNNs to avoid the use of hand-
crafted domain speciﬁc features  while the empirical work for DAgger did not have the same emphasis and so
used hand-crafted features.

4

Figure 1: The CNN architecture from DQN [19] that we adopt in our agents. See text for details.

random action is chosen to ensure some exploration). At each state of the game along each tra-
jectory  UCT is asked to compute its choice of action and the original data set is augmented with
the last four frames for each state as the rows and the column as UCT’s action choice. This 400
trajectory dataset’s input distribution is now potentially different from that of the UCT-agent. This
dataset is used to train the CNN again via multinomial classiﬁcation. This interleaved procedure is
repeated until there are a total of 800 runs worth of data in the dataset for the ﬁnal round of training
of the CNN. The UCTtoClassiﬁcation-Interleaved agent uses the ﬁnal CNN-classiﬁer learned by this
training procedure to select actions during testing.
In order to focus our empirical evaluation on the contribution of the non-DL part of our three new
agents  we reused the same convolutional neural network architecture as used in the DQN work (we
describe this architecture in brief detail below). The DQN work modiﬁed the reward functions for
some of the games (by saturating them at +1 and −1) while we use unmodiﬁed reward functions
(these only play a role in the UCT-agent components of our methods and not in the CNN compo-
nent). We also follow DQN’s frame-skipping techniques: the agent sees and selects actions on every
kth frame instead of every frame (k = 3 for Space Invaders and k = 4 for all other games)  and the
latest chosen-action is repeated on subsequently-skipped frames.

4.3 Details of Data Preprocessing and CNN Architecture
Preprocessing (identical to DQN to the best of our understanding). Raw Atari game frames are
160 × 210 pixel images with a 128-color palette. We convert the RGB representation to gray-scale
and crop a 160× 160 region of the image that captures the playing area  and then the cropped image
is down-sampled to 84 × 84 in order to reuse DQN’s CNN architecture. This procedure is applied
to the last 4 frames associated with a state and stacked to produce a 84 × 84 × 4 preprocessed
input representation for each state. We subtracted the pixel-level means and scale the inputs to lie in
the range [-1  1]. We shufﬂe the training data to break the strong correlations between consecutive
samples  which therefore reduces the variance of the updates.
CNN Architecture. We use the same deep neural network architecture as DQN [19] for our agents 
except for the nonlinearity units of convolutional layers.2 As depicted in Figure 1  our network
consists of three hidden layers. The input to the neural network is an 84 × 84 × 4 image produced
by the preprocessing procedure above. The ﬁrst hidden layer convolves 16  8 × 8  ﬁlters with
stride 4 with the input image and applies a nonlinearity (tanh). The second hidden layer convolves
32  4 × 4  ﬁlters with stride 2 again followed by a nonlinearity (tanh). The ﬁnal hidden layer
is fully connected and consists of 256 rectiﬁed linear units (relu). In the multi-regression-based
agent (UCTtoRegression)  the output layer is a fully connected linear layer with a single output
for each valid action. In the classiﬁcation-based agents (UCTtoClassiﬁcation  UCTtoClassiﬁcation-
Interleaved)  a softmax (instead of linear) function is applied to the ﬁnal output layer. We refer the
reader to the DQN paper for further detail.

5 Experimental Results
First we present our main performance results and then present some visualizations to help under-
stand the performance of our agents.3 In Table 1 we compare and contrast the performance of the
four real-time game playing agents  three of which (UCTtoRegression  UCTtoClassiﬁcation  and
UCTtoClassiﬁcation-Interleaved) we implemented and evaluated; the performance of the DQN was
obtained from [19].

2We also tested with rectiﬁed linear nonlinearity for convolutional layers  but did not ﬁnd signiﬁcant differ-

ences in the game performance.

3Our code and game play videos will be available at: sites.google.com/site/nips2014atari/.

5

848442020169932256conv-layer (tanh)conv-layer (tanh)fully-connected-layer (max(0 x))fully-connected-layer (linear)Table 1: Performance (game scores) of the four real-time game playing agents  where UCR is short for UCT-
toRegression  UCC is short for UCTtoClassiﬁcation  and UCC-I is short for UCTtoClassiﬁcation-Interleaved.

Agent
DQN

-best

UCC

-best
-greedy

UCC-I
-best
-greedy

UCR

B.Rider
4092
5184
5342 (20)
10514
5676
5388 (4.6)
10732
5702
2405 (12)

Breakout
168
225
175 (5.63)
351
269
215 (6.69)
413
380
143 (6.7)

Enduro
470
661
558 (14)
942
692
601 (11)
1026
741
566 (10.2)

Pong
20
21
19 (0.3)
21
21
19 (0.14)
21
21
19 (0.3)

Q*bert
1952
4500
11574(44)
29725
19890
13189 (35.3)
29900
20025
12755 (40.7)

Seaquest
1705
1740
2273 (23)
5100
2760
2701 (6.09)
6100
2995
1024 (13.8)

S.Invaders
581
1075
672 (5.3)
1200
680
670 (4.24)
910
692
441 (8.1)

Table 2: Performance (game scores) of the off-line UCT game playing agent.

Agent
UCT

B.Rider
7233

Breakout
406

Enduro
788

Pong Q*bert
18850
21

Seaquest
3257

S.Invaders
2354

The columns correspond to the seven games named in the header  and the rows correspond to differ-
ent assessments of the four agents. Throughout the table  the numbers in parentheses are standard-
errors. The DQN row reports the average performance (game score) of the DQN agent (a random
action is chosen 5% of the time during testing). The DQN-best row reports the best performance
of the DQN agent over all the attempts at each game. Comparing the performance of the UCT-
toClassiﬁcation and UCTtoRegression agents (both use 5% exploration)  we see that the UCTto-
Classiﬁcation agent either competes well with or signiﬁcantly outperforms the UCTtoRegression
agent. More importantly the UCTtoClassiﬁcation agent outperforms the DQN agent in all games
but Pong (in which both agents do nearly perfectly because the maximum score in this game is 21).
The percentage-performance gain of UCTtoClassiﬁcation over DQN is quite large for most games.
Similar gains are obtained in the comparison of UCTtoClassiﬁcation-best to DQN-best.
We used 5% exploration in our agents to match what the DQN agent does  but it is not clear why
one should consider random action selection during testing.
In any case  the effect of this ran-
domness in action-selection will differ across games (based  e.g.  on whether a wrong action can
be terminal). Thus  we also present results for the UCTtoClassiﬁcation-greedy agent in which we
don’t do any exploration. As seen by comparing the rows corresponding to UCTtoClassiﬁcation
and UCTtoClassiﬁcation-greedy  the latter agent always outperforms the former and in four games
(Breakout  Enduro  Q*Bert  and Seaquest) achieves further large-percentage improvements.
Table 2 gives the performance of our non-realtime UCT agent (again  with 5% exploration). As
discussed above we selected UCT-agent’s parameters to ensure that this agent outperforms the DQN
agent allowing room for our agents to perform in the middle.
Finally  recall that the UCTtoClassiﬁcation-Interleaved agent was designed so that its input distribu-
tion during training is more likely to match its input distribution during evaluation and we hypothe-
sized that this would improve performance relative to UCTtoClassiﬁcation. Indeed  in all games but
B. Rider  Pong and S.Invaders in which the two agents perform similarly  UCTtoClassiﬁcation-
Interleaved signiﬁcantly outperforms UCTtoClassiﬁcation. The same holds when comparing
UCTtoClassiﬁcation-Interleaved-best and UCTtoClassiﬁcation-best as well as UCTtoClassiﬁcation-
Interleaved-greedy and UCTtoClassiﬁcation-greedy.
Overall 
the average game performance of our best performing agent (UCTtoClassiﬁcation-
Interleaved) is signiﬁcant higher than that of DQN for most games  such as B.Rider (32%)  Breakout
(28%)  Enduro (28%)  Q*Bert (580%)  Sequest (58%) and S.Invaders (15%).
In a further preliminary exploration of the effectiveness of the UCTtoClassiﬁcation-Interleaved
in exploiting additional computational resources for generating UCT runs  on the game Enduro
we compared UCTtoClassiﬁcation and UCTtoClassiﬁcation-Interleaved where we allowed each of
them twice the number of UCT runs used in producing the Table 1 above  i.e.  1600 runs while

6

Figure 2: Visualization of the ﬁrst-layer features learned from Seaquest. (Left) visualization of four ﬁrst-layer
ﬁlters; each ﬁlter covers four frames  showing the spatio-temporal template. (Middle) a captured screen. (Right)
gray-scale version of the input screen which is fed into the CNN. Four ﬁlters were color-coded and visualized
as dotted bounding boxes at the locations where they get activated. This ﬁgure is best viewed in color.

keeping a batch size of 200. The performance of UCTtoClassiﬁcation improves from 558 to 581
while the performance of UCTtoClassiﬁcation-Interleaved improves from 601 to 670  i.e.  the in-
terleaved method improved more in absolute and percentage terms as we increased the amount of
training data. This is encouraging and is further conﬁrmation of the hypothesis that motivated the
interleaved method  because the interleaved input distribution would be even more like that of the
ﬁnal agent with the larger data set.
Learned Features from Convolutional Layers. We
provide visualizations of the learned ﬁlters in order to
gain insight on what the CNN learns. Speciﬁcally  we
apply the “optimal stimuli” method [7  14] to visualize
the features CNN learned after training. The method
picks the input image patches that generate the greatest
responses after convolution with the trained ﬁlters. We
select 8 × 8 × 4 input patches to visualize the ﬁrst con-
volutional layer features and 20×20×4 to visualize the
second convolutional layer ﬁlters. Note that these patch
sizes correspond to receptive ﬁeld sizes of the learned
features in each layer.
In Figure 2  we show four ﬁrst-layer ﬁlters of the CNN
trained from Seaquest for the UCTtoClassiﬁcation-
agent. Speciﬁcally  each ﬁlter covers four frames of
8 × 8 pixels  which can be viewed as a spatio-temporal
template that captures speciﬁc patterns and their tempo-
ral changes. We also show an example screen capture
and visualize where the ﬁlters get activated in the gray-scale version of the image (which is the actual
input to the CNN model). The visualization suggests that the ﬁrst-layer ﬁlters capture “object-part”
patterns and their temporal movements.
Figure 3 visualizes the four second-layer features via the optimal stimulus method  where each row
corresponds to a ﬁlter. We can see that the second-layer features capture bigger spatial patterns
(often covering beyond the size of individual objects)  while encoding interactions between objects 
such as two enemies moving together  and a submarine moving along a direction. Overall  these
qualitative results suggest that the CNN learns relevant patterns useful for game playing.
Visualization of Learned Policy. Here we present visualizations of the policy learned by the UCT-
toClassiﬁcation agent with the aim of illustrating both what it does well and what it does not.
Figure 4 shows the policy learned by UCTtoClassiﬁcation to destroy nearby enemies. The CNN
changes the action from ”Fire” to ”Down+Fire” at time step 70 when the enemies ﬁrst show up at
the right columns of the screen  which will move the submarine to the same horizontal position of the
closest enemy. At time step 75  the submarine is at the horizontal position of the closest enemy and

Figure 3: Visualization of the second-layer
features learned from Seaquest.

7

“submarine” “enemy” “diver” “enemy+diver” frame: t-3 t-2 t-1 t Figure 4: A visualization of the UCTtoClassiﬁcation agent’s policy as it kills an enemy agent.

the action changes to “Right+Fire”. The “Right+Fire” action is repeated until the enemy is destroyed
at time step 79. At time step 79  the predicted action is changed to “Down+Fire” again to move the
submarine to the horizontal position of the next closest enemy. This shows the UCTtoClassiﬁcation
agent’s ability to deal with delayed reward as it learns to take a sequence of unrewarded actions
before it obtains any reward when it ﬁnally destroys an enemy.
Figure 4 also shows a shortcoming in the UCTtoClassiﬁcation agent’s policy  namely it does not
purposefully take actions to save a diver (although saving many divers can lead to a large reward).
For example  at time step 69  even though there are two divers below and to the right of the submarine
(our agent)  the learned policy does not move the submarine downward. This phenomenon was
observed frequently. The reason for this shortcoming is that it can take a large number of time steps
to capture 6 divers and bring them to surface (bringing fewer divers to the surface does not yield
a reward); this takes longer than the planning depth of UCT. Thus  even our UCT agent does not
purposefully save divers and thus the training data collected reﬂects that defect which is then also
present in the play of the UCTtoClassiﬁcation (and UCTtoClassiﬁcation-Interleaved) agent.

6 Conclusion
UCT-based planning agents are unrealistic for Atari game play in at least two ways. First  to play
the game they require access to the state of the game which is unavailable to human players  and
second they are orders of magnitude slower than realtime. On the other hand  by slowing the game
down enough to allow UCT to play leads to the highest scores on the games they have been tried on.
Indeed  by allowing UCT more and more time (and thus allowing for larger number of trajectories
and larger maximum-depth) between moves one can presumably raise the score higher and higher.
We identiﬁed a gap between the UCT-based planning agent’s performance and the best realtime
player DQN’s performance and developed new agents to partially ﬁll this gap. Our main applied
result is that at the time of the writing of this paper we have the best realtime Atari game playing
agents on the same 7 games that were used to evaluate DQN. Indeed  in most of the 7 games our best
agent beats DQN signiﬁcantly. Another result is that at least in our experiments training the CNN to
learn a classiﬁer that maps game observations to actions was better than training the CNN to learn
a regression function that maps game observations to action-values (we intend to do further work
to conﬁrm how general this result is on ALE). Finally  we hypothesized that the difference in input
distribution between the UCT agent that generates the training data and the input distribution expe-
rienced by our learned agents would diminish performance. The UCTtoClassiﬁcation-Interleaved
agent we developed to deal with this issue indeed performed better than the UCTtoClassiﬁcation
agent indirectly conﬁrming our hypothesis and solving the underlying issue.
Acknowledgments. This work was supported in part by NSF grant IIS-1148668 (to SS)  ONR grant
N00014-13-1-0762 (to HL)  and a Google Faculty Research Award (to HL). Any opinions  ﬁndings 
conclusions  or recommendations expressed here are those of the authors and do not necessarily
reﬂect the views of the sponsors.

References

[1] M. G. Bellemare  Y. Naddaf  J. Veness  and M. Bowling. The arcade learning environment: an
evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research  47(1):253–
279  2013.

[2] M. G. Bellemare  J. Veness  and M. Bowling. Investigating contingency awareness using Atari

2600 games. In the 26th AAAI Conference on Artiﬁcial Intelligence  2012.

8

Step 69: FIREStep 70: DOWN+FIREStep 74:DOWN+FIREStep 75:RIGHT+FIREStep 76:RIGHT+FIREStep 78: RIGHT+FIREStep 79:DOWN+FIRE[3] M. G. Bellemare  J. Veness  and M. Bowling. Sketch-based linear value function approxima-

tion. In Advances in Neural Information Processing Systems  pages 2222–2230  2012.

[4] M. G. Bellemare  J. Veness  and E. Talvitie. Skip context tree switching. In Proceedings of the

International Conference on Machine Learning  2014.

[5] Y. Bengio. Learning deep architectures for AI. Foundations and trends in Machine Learning 

2(1):1–127  2009.

[6] D. Ciresan  U. Meier  and J. Schmidhuber. Multi-column deep neural networks for image

classiﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition  2012.

[7] D. Erhan  Y. Bengio  A. Courville  and P. Vincent. Visualizing higher-layer features of a deep

network. Technical report  University of Montreal  2009.

[8] A. Graves  A. Mohamed  and G. Hinton. Speech recognition with deep recurrent neural net-
works. In IEEE International Conference on Acoustics  Speech and Signal Processing  pages
6645–6649  2013.

[9] M. Hausknecht  P. Khandelwal  R. Miikkulainen  and P. Stone. HyperNEAT-GGP: A
hyperNEAT-based Atari general game player. In Proceedings of the Fourteenth International
Conference on Genetic and Evolutionary Computation Conference  pages 217–224  2012.

[10] A. Karpathy  G. Toderici  S. Shetty  T. Leung  R. Sukthankar  and L. Fei-Fei. Large-scale
In IEEE Conference on Computer

video classiﬁcation with convolutional neural networks.
Vision and Pattern Recognition  2014.

[11] M. Kearns  Y. Mansour  and A. Y. Ng. A sparse sampling algorithm for near-optimal planning

in large Markov decision processes. Machine Learning  49(2-3):193–208  2002.

[12] L. Kocsis and C. Szepesv´ari. Bandit based Monte-Carlo planning. In European Conference on

Machine Learning  pages 282–293. 2006.

[13] A. Krizhevsky  I. Sutskever  and G. E. Hinton. ImageNet classiﬁcation with deep convolutional

neural networks. In Advances in Neural Information Processing Systems  2012.

[14] Q. V. Le  M. Ranzato  R. Monga  M. Devin  K. Chen  G. S. Corrado  J. Dean  and A. Y. Ng.
In Proceedings of the

Building high-level features using large scale unsupervised learning.
29th International Conference on Machine Learning  2012.

[15] Q. V. Le  W. Y. Zou  S. Y. Yeung  and A. Y. Ng. Learning hierarchical invariant spatio-temporal
features for action recognition with independent subspace analysis. In IEEE Conference on
Computer Vision and Pattern Recognition  pages 3361–3368  2011.

[16] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[17] H. Lee  R. Grosse  R. Ranganath  and A. Y. Ng. Convolutional deep belief networks for
In Proceedings of the 26th

scalable unsupervised learning of hierarchical representations.
International Conference on Machine Learning  pages 609–616  2009.

[18] H. Lee  P. Pham  Y. Largman  and A. Y. Ng. Unsupervised feature learning for audio classiﬁca-
tion using convolutional deep belief networks. In Advances in Neural Information Processing
Systems  pages 1096–1104  2009.

[19] V. Mnih  K. Kavukcuoglu  D. Silver  A. Graves  I. Antonoglou  D. Wierstra  and M. Ried-
miller. Playing Atari with deep reinforcement learning. In Deep Learning  Neural Information
Processing Systems Workshop  2013.

[20] A. Mohamed  G. E. Dahl  and G. Hinton. Acoustic modeling using deep belief networks. IEEE

Transactions on Audio  Speech  and Language Processing  20(1):14–22  2012.

[21] S. Ross  G. J. Gordon  and J. A. Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Proceedings of the 14th International Conference
on Artiﬁcial Intelligence and Statistics  2011.

[22] J. Schmidhuber.

Deep learning in neural networks: An overview.

arXiv:1404.7828  2014.

arXiv preprint

[23] G. Tesauro. Temporal difference learning and TD-gammon. Communications of the ACM 

38(3):58–68  1995.

9

,Xiaoxiao Guo
Satinder Singh
Honglak Lee
Richard Lewis
Xiaoshi Wang
Roel Dobbe
David Fridovich-Keil
Claire Tomlin