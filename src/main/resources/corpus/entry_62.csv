2010,A Dirty Model for Multi-task Learning,We consider the multiple linear regression problem  in a setting where some of the set of relevant features could be shared across the tasks. A lot of recent research has studied the use of $\ell_1/\ell_q$ norm block-regularizations with $q > 1$  for such (possibly) block-structured problems  establishing strong guarantees on recovery even under high-dimensional scaling where the number of features scale with the number of observations. However  these papers also caution that the performance of such block-regularized methods are very dependent on the {\em extent} to which the features are shared across tasks. Indeed they show~\citep{NWJoint} that if the extent of overlap is less than a threshold  or even if parameter {\em values} in the shared features are highly uneven  then block $\ell_1/\ell_q$ regularization could actually perform {\em worse} than simple separate elementwise $\ell_1$ regularization. We are far away from a realistic multi-task setting: not only do the set of relevant features have to be exactly the same across tasks  but their values have to as well.  Here  we ask the question: can we leverage support and parameter overlap when it exists  but not pay a penalty when it does not? Indeed  this falls under a more general question of whether we can model such \emph{dirty data} which may not fall into a single neat structural bracket (all block-sparse  or all low-rank and so on). Here  we take a first step  focusing on developing a dirty model for the multiple regression problem. Our method uses a very simple idea: we decompose  the parameters into two components and {\em regularize these differently.} We show both theoretically and empirically  our method strictly and noticeably outperforms both $\ell_1$ and $\ell_1/\ell_q$ methods  over the entire range of possible overlaps. We also provide theoretical guarantees that the method performs well under high-dimensional scaling.,A Dirty Model for Multi-task Learning

Ali Jalali

University of Texas at Austin
alij@mail.utexas.edu

Pradeep Ravikumar

University of Texas at Asutin

pradeepr@cs.utexas.edu

Sujay Sanghavi

University of Texas at Austin

sanghavi@mail.utexas.edu

Chao Ruan

University of Texas at Austin
ruan@cs.utexas.edu

Abstract

We consider multi-task learning in the setting of multiple linear regression  and
where some relevant features could be shared across the tasks. Recent research
has studied the use of ℓ1/ℓq norm block-regularizations with q > 1 for such block-
sparse structured problems  establishing strong guarantees on recovery even under
high-dimensional scaling where the number of features scale with the number of
observations. However  these papers also caution that the performance of such
block-regularized methods are very dependent on the extent to which the features
are shared across tasks. Indeed they show [8] that if the extent of overlap is less
than a threshold  or even if parameter values in the shared features are highly
uneven  then block ℓ1/ℓq regularization could actually perform worse than sim-
ple separate elementwise ℓ1 regularization. Since these caveats depend on the
unknown true parameters  we might not know when and which method to apply.
Even otherwise  we are far away from a realistic multi-task setting: not only do the
set of relevant features have to be exactly the same across tasks  but their values
have to as well.
Here  we ask the question: can we leverage parameter overlap when it exists 
but not pay a penalty when it does not ? Indeed  this falls under a more general
question of whether we can model such dirty data which may not fall into a single
neat structural bracket (all block-sparse  or all low-rank and so on). With the
explosion of such dirty high-dimensional data in modern settings  it is vital to
develop tools – dirty models – to perform biased statistical estimation tailored
to such data. Here  we take a ﬁrst step  focusing on developing a dirty model
for the multiple regression problem. Our method uses a very simple idea: we
estimate a superposition of two sets of parameters and regularize them differently.
We show both theoretically and empirically  our method strictly and noticeably
outperforms both ℓ1 or ℓ1/ℓq methods  under high-dimensional scaling and over
the entire range of possible overlaps (except at boundary cases  where we match
the best method).

1

Introduction: Motivation and Setup

High-dimensional scaling. In ﬁelds across science and engineering  we are increasingly faced with
problems where the number of variables or features p is larger than the number of observations n.
Under such high-dimensional scaling  for any hope of statistically consistent estimation  it becomes
vital to leverage any potential structure in the problem such as sparsity (e.g. in compressed sens-
ing [3] and LASSO [14])  low-rank structure [13  9]  or sparse graphical model structure [12]. It is in
such high-dimensional contexts in particular that multi-task learning [4] could be most useful. Here 

1

multiple tasks share some common structure such as sparsity  and estimating these tasks jointly by
leveraging this common structure could be more statistically efﬁcient.

Block-sparse Multiple Regression. A common multiple task learning setting  and which is the focus
of this paper  is that of multiple regression  where we have r > 1 response variables  and a common
set of p features or covariates. The r tasks could share certain aspects of their underlying distri-
butions  such as common variance  but the setting we focus on in this paper is where the response
variables have simultaneously sparse structure: the index set of relevant features for each task is
sparse; and there is a large overlap of these relevant features across the different regression prob-
lems. Such “simultaneous sparsity” arises in a variety of contexts [15]; indeed  most applications
of sparse signal recovery in contexts ranging from graphical model learning  kernel learning  and
function estimation have natural extensions to the simultaneous-sparse setting [12  2  11].

It is useful to represent the multiple regression parameters via a matrix  where each column corre-
sponds to a task  and each row to a feature. Having simultaneous sparse structure then corresponds
to the matrix being largely “block-sparse” – where each row is either all zero or mostly non-zero 
and the number of non-zero rows is small. A lot of recent research in this setting has focused on
ℓ1/ℓq norm regularizations  for q > 1  that encourage the parameter matrix to have such block-
sparse structure. Particular examples include results using the ℓ1/ℓ∞ norm [16  5  8]  and the ℓ1/ℓ2
norm [7  10].

Dirty Models. Block-regularization is “heavy-handed” in two ways. By strictly encouraging shared-
sparsity  it assumes that all relevant features are shared  and hence suffers under settings  arguably
more realistic  where each task depends on features speciﬁc to itself in addition to the ones that are
common. The second concern with such block-sparse regularizers is that the ℓ1/ℓq norms can be
shown to encourage the entries in the non-sparse rows taking nearly identical values. Thus we are
far away from the original goal of multitask learning: not only do the set of relevant features have
to be exactly the same  but their values have to as well. Indeed recent research into such regularized
methods [8  10] caution against the use of block-regularization in regimes where the supports and
values of the parameters for each task can vary widely. Since the true parameter values are unknown 
that would be a worrisome caveat.

We thus ask the question: can we learn multiple regression models by leveraging whatever overlap
of features there exist  and without requiring the parameter values to be near identical? Indeed this
is an instance of a more general question on whether we can estimate statistical models where the
data may not fall cleanly into any one structural bracket (sparse  block-sparse and so on). With
the explosion of dirty high-dimensional data in modern settings  it is vital to investigate estimation
of corresponding dirty models  which might require new approaches to biased high-dimensional
estimation. In this paper we take a ﬁrst step  focusing on such dirty models for a speciﬁc problem:
simultaneously sparse multiple regression.

Our approach uses a simple idea: while any one structure might not capture the data  a superposition
of structural classes might. Our method thus searches for a parameter matrix that can be decomposed
into a row-sparse matrix (corresponding to the overlapping or shared features) and an elementwise
sparse matrix (corresponding to the non-shared features). As we show both theoretically and em-
pirically  with this simple ﬁx we are able to leverage any extent of shared features  while allowing
disparities in support and values of the parameters  so that we are always better than both the Lasso
or block-sparse regularizers (at times remarkably so).

The rest of the paper is organized as follows: In Sec 2. basic deﬁnitions and setup of the problem
are presented. Main results of the paper is discussed in sec 3. Experimental results and simulations
are demonstrated in Sec 4.
Notation: For any matrix M  we denote its jth row as Mj  and its k-th column as M (k). The set
of all non-zero rows (i.e. all rows with at least one non-zero element) is denoted by RowSupp(M )
|  i.e. the sums of

and its support by Supp(M ). Also  for any matrix M  let kM k1 1 :=Pj k |M (k)
absolute values of the elements  and kM k1 ∞ :=Pj kMjk∞ where  kMjk∞ := maxk |M (k)

j

|.

j

2

2 Problem Set-up and Our Method

Multiple regression. We consider the following standard multiple linear regression model:

y(k) = X (k) ¯θ(k) + w(k) 

k = 1  . . .   r 

where y(k) ∈ Rn is the response for the k-th task  regressed on the design matrix X (k) ∈ Rn×p
(possibly different across tasks)  while w(k) ∈ Rn is the noise vector. We assume each w(k) is
drawn independently from N (0  σ2). The total number of tasks or target variables is r  the number
of features is p  while the number of samples we have for each task is n. For notational convenience 
we collate these quantities into matrices Y ∈ Rn×r for the responses  ¯Θ ∈ Rp×r for the regression
parameters and W ∈ Rn×r for the noise.
Dirty Model. In this paper we are interested in estimating the true parameter ¯Θ from data by lever-
aging any (unknown) extent of simultaneous-sparsity. In particular  certain rows of ¯Θ would have
many non-zero entries  corresponding to features shared by several tasks (“shared” rows)  while
certain rows would be elementwise sparse  corresponding to those features which are relevant for
some tasks but not all (“non-shared rows”)  while certain rows would have all zero entries  corre-

automatically adapt to different levels of sharedness  and yet enjoy the following guarantees:

the estimator succeeds. We note that this is stronger than merely recovering the row-support of ¯Θ 
which is union of its supports for the different tasks. In particular  denoting Uk for the support of the

sponding to those features that are not relevant to any task. We are interested in estimators bΘ that
Support recovery: We say an estimator bΘ successfully recovers the true signed support if
sign(Supp(bΘ)) = sign(Supp( ¯Θ)). We are interested in deriving sufﬁcient conditions under which
k-th column of ¯Θ  and U =Sk Uk.
estimator bΘ 

Error bounds: We are also interested in providing bounds on the elementwise ℓ∞ norm error of the

max

k=1 ... r(cid:12)(cid:12)(cid:12)bΘ(k)

j − ¯Θ(k)

j

kbΘ − ¯Θk∞ = max

j=1 ... p

(cid:12)(cid:12)(cid:12) .

2.1 Our Method

Our method explicitly models the dirty block-sparse structure. We estimate a sum of two parameter
matrices B and S with different regularizations for each: encouraging block-structured row-sparsity
in B and elementwise sparsity in S. The corresponding “clean” models would either just use block-
sparse regularizations [8  10] or just elementwise sparsity regularizations [14  18]  so that either
method would perform better in certain suited regimes. Interestingly  as we will see in the main
results  by explicitly allowing to have both block-sparse and elementwise sparse component  we are
able to outperform both classes of these “clean models”  for all regimes ¯Θ.

Algorithm 1 Dirty Block Sparse
Solve the following convex optimization problem:

1
2n

rXk=1(cid:13)(cid:13)(cid:13)y(k) − X (k)(cid:16)S(k) + B(k)(cid:17)(cid:13)(cid:13)(cid:13)

S B

(bS  bB) ∈ arg min
Then output bΘ = bB + bS.
3 Main Results and Their Consequences

2

2

+ λskSk1 1 + λbkBk1 ∞.

(1)

We now provide precise statements of our main results. A number of recent results have shown that
the Lasso [14  18] and ℓ1/ℓ∞ block-regularization [8] methods succeed in recovering signed sup-
ports with controlled error bounds under high-dimensional scaling regimes. Our ﬁrst two theorems
extend these results to our dirty model setting. In Theorem 1  we consider the case of deterministic
design matrices X (k)  and provide sufﬁcient conditions guaranteeing signed support recovery  and
elementwise ℓ∞ norm error bounds. In Theorem 2  we specialize this theorem to the case where the

3

rows of the design matrices are random from a general zero mean Gaussian distribution: this allows
us to provide scaling on the number of observations required in order to guarantee signed support
recovery and bounded elementwise ℓ∞ norm error.
Our third result is the most interesting in that it explicitly quantiﬁes the performance gains of our
method vis-a-vis Lasso and the ℓ1/ℓ∞ block-regularization method. Since this entailed ﬁnding the
precise constants underlying earlier theorems  and a correspondingly more delicate analysis  we
follow Negahban and Wainwright [8] and focus on the case where there are two-tasks (i.e. r = 2) 
and where we have standard Gaussian design matrices as in Theorem 2. Further  while each of two
tasks depends on s features  only a fraction α of these are common. It is then interesting to see how
the behaviors of the different regularization methods vary with the extent of overlap α.
Comparisons. Negahban and Wainwright [8] show that there is actually a “phase transition” in the
scaling of the probability of successful signed support-recovery with the number of observations.
Denote a particular rescaling of the sample-size θLasso(n  p  α) =
s log(p−s) . Then as Wainwright
[18] show  when the rescaled number of samples scales as θLasso > 2 + δ for any δ > 0  Lasso
succeeds in recovering the signed support of all columns with probability converging to one. But
when the sample size scales as θLasso < 2−δ for any δ > 0  Lasso fails with probability converging
to one. For the ℓ1/ℓ∞-reguralized multiple linear regression  deﬁne a similar rescaled sample size
s log(p−(2−α)s) . Then as Negahban and Wainwright [8] show there is again a
θ1 ∞(n  p  α) =
transition in probability of success from near zero to near one  at the rescaled sample size of θ1 ∞ =
(4 − 3α). Thus  for α < 2/3 (“less sharing”) Lasso would perform better since its transition is at
a smaller sample size  while for α > 2/3 (“more sharing”) the ℓ1/ℓ∞ regularized method would
perform better.

n

n

As we show in our third theorem  the phase transition for our method occurs at the rescaled sample
size of θ1 ∞ = (2 − α)  which is strictly before either the Lasso or the ℓ1/ℓ∞ regularized method
except for the boundary cases: α = 0  i.e. the case of no sharing  where we match Lasso  and for
α = 1  i.e. full sharing  where we match ℓ1/ℓ∞. Everywhere else  we strictly outperform both
methods. Figure 3 shows the empirical performance of each of the three methods; as can be seen 
they agree very well with the theoretical analysis. (Further details in the experiments Section 4).

3.1 Sufﬁcient Conditions for Deterministic Designs

We ﬁrst consider the case where the design matrices X (k) for k = 1  · · ·  r are deterministic 
and start by specifying the assumptions we impose on the model. We note that similar sufﬁcient
conditions for the deterministic X (k)’s case were imposed in papers analyzing Lasso [18] and
block-regularization methods [8  10].

j (cid:13)(cid:13)(cid:13)2 ≤ √2n for all j = 1  . . .   p  k = 1  . . .   r.

A0 Column Normalization(cid:13)(cid:13)(cid:13)X (k)
Let Uk denote the support of the k-th column of ¯Θ  and U = Sk Uk denote the union of
A1 Incoherence Condition γb := 1 − max

supports for each task. Then we require that

  X (k)

  X (k)

> 0.

j∈U c

Uk

j

We will also ﬁnd it useful to deﬁne γs := 1−max1≤k≤r maxj∈U c
Note that by the incoherence condition A1  we have γs > 0.
A2 Eigenvalue Condition Cmin := min
1≤k≤r

  X (k)

Uk

A3 Boundedness Condition Dmax := max

j

  X (k)

rXk=1

(cid:28)X (k)

(cid:13)(cid:13)(cid:13)(cid:13)
λmin(cid:18) 1
n DX (k)
1≤k≤r(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:18) 1
n DX (k)
2(2 − γs)σplog(pr)

UkE(cid:17)−1(cid:29)(cid:13)(cid:13)(cid:13)(cid:13)1
UkE(cid:16)DX (k)

Uk (cid:16)DX (k)
k(cid:13)(cid:13)(cid:13)(cid:13)DX (k)
UkE(cid:19) > 0.
UkE(cid:19)−1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∞ 1
2(2 − γb)σplog(pr)

< ∞.

  X (k)

λb >

and

Uk

Uk

γb√n

λs >

γs√n

  X (k)

UkE(cid:17)−1(cid:13)(cid:13)(cid:13)(cid:13)1

.

.

(2)

Further  we require the regularization penalties be set as

4

 

Dirty Model

L1/Linf Reguralizer

LASSO

p=128
p=256
p=512

2.5

3

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
s
e
c
c
u
S

 
f

o

 
y
t
i
l
i

b
a
b
o
r
P

Dirty Model

L1/Linf Reguralizer

LASSO

 

p=128
p=256
p=512

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
s
e
c
c
u
S

 
f

o

 
y
t
i
l
i

b
a
b
o
r
P

 
0
0.5

1

1.5

1.7

2

Control Parameter θ

2.5

3

3.1

3.5

4

 
0
0.5

1

1.333

1.5

Control Parameter θ

2

(a) α = 0.3

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

s
s
e
c
c
u
S

 
f

o

 
y
t
i
l
i

b
a
b
o
r
P

Dirty Model

 L1/Linf
Reguralizer

 
0
0.5

1

1.2

Control Parameter θ

1.5

1.6

(b) α = 2
3

 

LASSO

p=128
p=256
p=512

2

2.5

(c) α = 0.8

Figure 1: Probability of success in recovering the true signed support using dirty model  Lasso and ℓ1/ℓ∞
regularizer. For a 2-task problem  the probability of success for different values of feature-overlap fraction α
is plotted. As we can see in the regimes that Lasso is better than  as good as and worse than ℓ1/ℓ∞ regularizer
((a)  (b) and (c) respectively)  the dirty model outperforms both of the methods  i.e.  it requires less number of
observations for successful recovery of the true signed support compared to Lasso and ℓ1/ℓ∞ regularizer. Here
s = ⌊ p
Theorem 1. Suppose A0-A3 hold  and that we obtain estimate bΘ from our algorithm with regular-

ization parameters chosen according to (2). Then  with probability at least 1 − c1 exp(−c2n) → 1 
we are guaranteed that the convex program (1) has a unique optimum and

10⌋ always.

Supp(bΘ) ⊆ Supp( ¯Θ) 

(a) The estimate bΘ has no false inclusions  and has bounded ℓ∞ norm error so that
}

and kbΘ − ¯Θk∞ ∞ ≤r 4σ2 log (pr)
{z
(j k)∈Supp( ¯Θ)(cid:12)(cid:12)(cid:12)¯θ(k)
(cid:12)(cid:12)(cid:12) > bmin.

(b) sign(Supp(bΘ)) = sign(cid:0)Supp( ¯Θ)(cid:1) provided that

+ λsDmax

n Cmin

min

bmin

|

j

.

Here the positive constants c1  c2 depend only on γs  γb  λs  λb and σ  but are otherwise independent
of n  p  r  the problem dimensions of interest.

Remark: Condition (a) guarantees that the estimate will have no false inclusions; i.e. all included
features will be relevant. If in addition  we require that it have no false exclusions and that recover
the support exactly  we need to impose the assumption in (b) that the non-zero elements are large
enough to be detectable above the noise.

3.2 General Gaussian Designs

Often the design matrices consist of samples from a Gaussian ensemble. Suppose that for each task
k = 1  . . .   r the design matrix X (k) ∈ Rn×p is such that each row X (k)
∈ Rp is a zero-mean
Gaussian random vector with covariance matrix Σ(k) ∈ Rp×p  and is independent of every other
row. Let Σ(k)
V U ∈ R|V|×|U | be the submatrix of Σ(k) with rows corresponding to V and columns to
U. We require these covariance matrices to satisfy the following conditions:
C1 Incoherence Condition γb := 1 − max

> 0

j∈U c

j Uk

i

rXk=1

(cid:13)(cid:13)(cid:13)(cid:13)Σ(k)

Uk Uk(cid:17)−1(cid:13)(cid:13)(cid:13)(cid:13)1
 (cid:16)Σ(k)

5

C2 Eigenvalue Condition Cmin := min
1≤k≤r
is bounded away from zero.

C3 Boundedness Condition Dmax :=(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)Σ(k)

λmin(cid:16)Σ(k)
Uk Uk(cid:17)−1(cid:13)(cid:13)(cid:13)(cid:13)∞ 1

< ∞.

Uk Uk(cid:17) > 0 so that the minimum eigenvalue

These conditions are analogues of the conditions for deterministic designs; they are now imposed
on the covariance matrix of the (randomly generated) rows of the design matrix.
Further  deﬁning s := maxk |Uk|  we require the regularization penalties be set as

λs > (cid:0)4σ2Cmin log(pr)(cid:1)1/2
γs√nCmin −p2s log(pr)

and

λb > (cid:0)4σ2Cminr(r log(2) + log(p))(cid:1)1/2
γb√nCmin −p2sr(r log(2) + log(p))

.

(3)

 

Cminγ 2
s

2sr(cid:0)r log(2)+log(p)(cid:1)

Theorem 2. Suppose assumptions C1-C3 hold  and that the number of samples scale as n >

with probability at least 1 − c1 exp (−c2 (r log(2) + log(p))) − c3 exp(−c4 log(rs)) → 1 for some

max(cid:18) 2s log(pr)
(cid:19) . Suppose we obtain estimate bΘ from algorithm (3). Then 
positive numbers c1 − c4  we are guaranteed that the algorithm estimate bΘ is unique and satisﬁes

the following conditions:

Cminγ 2
b

Supp(bΘ) ⊆ Supp( ¯Θ) 

(a) the estimate bΘ has no false inclusions  and has bounded ℓ∞ norm error so that
Cmin√n

|
(b) sign(Supp(bΘ)) = sign(cid:0)Supp( ¯Θ)(cid:1) provided that

and kbΘ − ¯Θk∞ ∞ ≤r 50σ2 log(rs)
(j k)∈Supp( ¯Θ)(cid:12)(cid:12)(cid:12)¯θ(k)

+ λs(cid:18)
{z
(cid:12)(cid:12)(cid:12) > gmin.

nCmin

min

gmin

4s

j

.

+ Dmax(cid:19)
}

3.3 Sharp Transition for 2-Task Gaussian Designs

This is one of the most important results of this paper. Here  we perform a more delicate and
ﬁner analysis to establish precise quantitative gains of our method. We focus on the special case
where r = 2 and the design matrix has rows generated from the standard Gaussian distribution
N (0  In×n)  so that C1 − C3 hold  with Cmin = Dmax = 1. As we will see both analytically and
experimentally  our method strictly outperforms both Lasso and ℓ1/ℓ∞-block-regularization over
for all cases  except at the extreme endpoints of no support sharing (where it matches that of Lasso)
and full support sharing (where it matches that of ℓ1/ℓ∞). We now present our analytical results; the
empirical comparisons are presented next in Section 4. The results will be in terms of a particular
rescaling of the sample size n as

θ(n  p  s  α) :=

n

(2 − α)s log (p − (2 − α)s)

.

We will also require the assumptions that

F1 λs >

(cid:16)4σ2(1 − ps/n)(log(r) + log(p − (2 − α)s))(cid:17)1/2

(n)1/2 − (s)1/2 − ((2 − α) s (log(r) + log(p − (2 − α)s)))1/2  

F2 λb >

(cid:16)4σ2(1 − ps/n)r(r log(2) + log(p − (2 − α)s))(cid:17)1/2

(n)1/2 − (s)1/2 − ((1 − α/2) sr (r log(2) + log(p − (2 − α)s)))1/2 .

Theorem 3. Consider a 2-task regression problem (n  p  s  α)  where the design matrix has rows

generated from the standard Gaussian distribution N (0  In×n).

6

Suppose maxj∈B∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)Θ∗(1)

j

(cid:12)(cid:12)(cid:12) −

= o(λs)  where B∗ is the submatrix of Θ∗ with rows where both entries are non-zero.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

j

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)Θ∗(2)
Then the estimate bΘ of the problem (1) satisﬁes the following:

(Success) Suppose the regularization coefﬁcients satisfy F1 − F2. Further  assume that the number
of samples scales as θ(n  p  s  α) > 1. Then  with probability at least 1 − c1 exp(−c2n) for

(Failure) If θ(n  p  s  α) < 1 there is no solution ( ˆB  ˆS) for any choices of λs and λb such that

and ℓ∞ error bound conditions (a-b) in Theorem 2.

some positive numbers c1 and c2  we are guaranteed that bΘ satisﬁes the support-recovery
sign(cid:16)Supp(bΘ)(cid:17) = sign(cid:0)Supp( ¯Θ)(cid:1).
(cid:12)(cid:12)(cid:12) −(cid:12)(cid:12)(cid:12)Θ∗(2)

to be small only on rows where both entries are

non-zero. As we show in a more general theorem in the appendix  even in the case where the gap is
large  the dependence of the sample scaling on the gap is quite weak.

We note that we require the gap(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)Θ∗(1)

j

j

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

4 Empirical Results

In this section  we investigate the performance of our dirty block sparse estimator on synthetic and
real-world data. The synthetic experiments explore the accuracy of Theorem 3  and compare our
estimator with LASSO and the ℓ1/ℓ∞ regularizer. We see that Theorem 3 is very accurate indeed.
Next  we apply our method to a real world datasets containing hand-written digits for classiﬁcation.
Again we compare against LASSO and the ℓ1/ℓ∞.
(a multi-task regression dataset) with r = 2 tasks. In both of this real world dataset  we show that
dirty model outperforms both LASSO and ℓ1/ℓ∞ practically. For each method  the parameters are
chosen via cross-validation; see supplemental material for more details.

4.1 Synthetic Data Simulation

We consider a r = 2-task regression problem as discussed in Theorem 3  for a range of parameters
(n  p  s  α). The design matrices X have each entry being i.i.d. Gaussian with mean 0 and variance
1. For each ﬁxed set of (n  s  p  α)  we generate 100 instances of the problem. In each instance 
given p  s  α  the locations of the non-zero entries of the true ¯Θ are chosen at randomly; each non-
zero entry is then chosen to be i.i.d. Gaussian with mean 0 and variance 1. n samples are then
generated from this. We then attempt to estimate using three methods: our dirty model  ℓ1/ℓ∞
regularizer and LASSO. In each case  and for each instance  the penalty regularizer coefﬁcients are
found by cross validation. After solving the three problems  we compare the signed support of the
solution with the true signed support and decide whether or not the program was successful in signed
support recovery. We describe these process in more details in this section.
Performance Analysis: We ran the algorithm for ﬁve different values of the overlap ratio α ∈
{0.3  2
3   0.8} with three different number of features p ∈ {128  256  512}. For any instance of the
problem (n  p  s  α)  if the recovered matrix ˆΘ has the same sign support as the true ¯Θ  then we
count it as success  otherwise failure (even if one element has different sign  we count it as failure).

n

As Theorem 3 predicts and Fig 3 shows  the right scaling for the number of oservations is
s log(p−(2−α)s)   where all curves stack on the top of each other at 2 − α. Also  the number of obser-
vations required by dirty model for true signed support recovery is always less than both LASSO and
ℓ1/ℓ∞ regularizer. Fig 1(a) shows the probability of success for the case α = 0.3 (when LASSO
is better than ℓ1/ℓ∞ regularizer) and that dirty model outperforms both methods. When α = 2
3
(see Fig 1(b))  LASSO and ℓ1/ℓ∞ regularizer performs the same; but dirty model require almost
33% less observations for the same performance. As α grows toward 1  e.g. α = 0.8 as shown in
Fig 1(c)  ℓ1/ℓ∞ performs better than LASSO. Still  dirty model performs better than both methods
in this case as well.

7

L1/Linf Regularizer

4

3.5

3

2.5

2

1.5

Dirty Model

l

d
o
h
s
e
r
h
T
n
o

 

i
t
i
s
n
a
r
T
e
s
a
h
P

 

1

 
0

0.1

0.2

0.3

0.4

Shared Support Parameter α

0.5

0.6

 

p=128
p=256
p=512

LASSO

0.7

0.8

0.9

1

Figure 2: Veriﬁcation of the result of the Theorem 3 on the behavior of phase transition threshold by changing
the parameter α in a 2-task (n  p  s  α) problem for dirty model  LASSO and ℓ1/ℓ∞ regularizer. The y-axis
is
10⌋. Our
dirty model method shows a gain in sample complexity over the entire range of sharing α. The pre-constant in
Theorem 3 is also validated.

s log(p−(2−α)s)   where n is the number of samples at which threshold was observed. Here s = ⌊ p

n

n
10

Average Classiﬁcation Error

Variance of Error

Average Row Support Size

Average Support Size

20

Average Classiﬁcation Error

Variance of Error

Average Row Support Size

Average Support Size

40

Average Classiﬁcation Error

Variance of Error

Average Row Support Size

Average Support Size

Our Model

8.6%
0.53%

B:165
S:18

B:211
S:34

B:270
S:67

B + S:171
B + S:1651
3.0%
0.56%

B + S:226
B + S:2118
2.2%
0.57%

B + S:299
B + S:2761

ℓ1/ℓ∞ LASSO
10.8%
9.9%
0.51%
0.64%
170
123
539
1700
4.1%
3.5%
0.68%
0.62%
173
217
821
2165
3.2%
2.8%
0.85%
0.68%
354
368
3669
2053

Table 1: Handwriting Classiﬁcation Results for our model  ℓ1/ℓ∞ and LASSO

Scaling Veriﬁcation: To verify that the phase transition threshold changes linearly with α as pre-
dicted by Theorem 3  we plot the phase transition threshold versus α. For ﬁve different values of
α ∈ {0.05  0.3  2
3   0.8  0.95} and three different values of p ∈ {128  256  512}  we ﬁnd the phase
transition threshold for dirty model  LASSO and ℓ1/ℓ∞ regularizer. We consider the point where
the probability of success in recovery of signed support exceeds 50% as the phase transition thresh-
old. We ﬁnd this point by interpolation on the closest two points. Fig 2 shows that phase transition
threshold for dirty model is always lower than the phase transition for LASSO and ℓ1/ℓ∞ regular-
izer.
4.2 Handwritten Digits Dataset

We use the handwritten digit dataset [1]  containing features of handwritten numerals (0-9) extracted
from a collection of Dutch utility maps. This dataset has been used by a number of papers [17  6]
as a reliable dataset for handwritten recognition algorithms. There are thus r = 10 tasks  and each
handwritten sample consists of p = 649 features.
Table 1 shows the results of our analysis for different sizes n of the training set . We measure the
classiﬁcation error for each digit to get the 10-vector of errors. Then  we ﬁnd the average error and
the variance of the error vector to show how the error is distributed over all tasks. We compare our
method with ℓ1/ℓ∞ reguralizer method and LASSO. Again  in all methods  parameters are chosen
via cross-validation.

For our method we separate out the B and S matrices that our method ﬁnds  so as to illustrate how
many features it identiﬁes as “shared” and how many as “non-shared”. For the other methods we
just report the straight row and support numbers  since they do not make such a separation.

Acknowledgements

We acknowledge support from NSF grant IIS-101842  and NSF CAREER program  Grant 0954059.

8

and D.J. Newman.

UCI Machine

Learning Repository 
of

University

References
[1] A. Asuncion

http://www.ics.uci.edu/ mlearn/MLRepository.html.
California  School of Information and Computer Science  Irvine  CA  2007.

[2] F. Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine

Learning Research  9:1179–1225  2008.

[3] R. Baraniuk. Compressive sensing. IEEE Signal Processing Magazine  24(4):118–121  2007.
[4] R. Caruana. Multitask learning. Machine Learning  28:41–75  1997.
[5] C.Zhang and J.Huang. Model selection consistency of the lasso selection in high-dimensional

linear regression. Annals of Statistics  36:1567–1594  2008.

[6] X. He and P. Niyogi. Locality preserving projections. In NIPS  2003.
[7] K. Lounici  A. B. Tsybakov  M. Pontil  and S. A. van de Geer. Taking advantage of sparsity in

multi-task learning. In 22nd Conference On Learning Theory (COLT)  2009.

[8] S. Negahban and M. J. Wainwright. Joint support recovery under high-dimensional scaling:
In Advances in Neural Information Processing

Beneﬁts and perils of ℓ1 ∞-regularization.
Systems (NIPS)  2008.

[9] S. Negahban and M. J. Wainwright. Estimation of (near) low-rank matrices with noise and

high-dimensional scaling. In ICML  2010.

[10] G. Obozinski  M. J. Wainwright  and M. I. Jordan. Support union recovery in high-dimensional

multivariate regression. Annals of Statistics  2010.

[11] P. Ravikumar  H. Liu  J. Lafferty  and L. Wasserman. Sparse additive models. Journal of the

Royal Statistical Society  Series B.

[12] P. Ravikumar  M. J. Wainwright  and J. Lafferty. High-dimensional ising model selection using

ℓ1-regularized logistic regression. Annals of Statistics  2009.

[13] B. Recht  M. Fazel  and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. In Allerton Conference  Allerton House  Illinois 
2007.

[14] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society  Series B  58(1):267–288  1996.

[15] J. A. Tropp  A. C. Gilbert  and M. J. Strauss. Algorithms for simultaneous sparse approx-
imation. Signal Processing  Special issue on “Sparse approximations in signal and image
processing”  86:572–602  2006.

[16] B. Turlach  W.N. Venables  and S.J. Wright. Simultaneous variable selection. Techno- metrics 

27:349–363  2005.

[17] M. van Breukelen  R.P.W. Duin  D.M.J. Tax  and J.E. den Hartog. Handwritten digit recogni-

tion by combined classiﬁers. Kybernetika  34(4):381–386  1998.

[18] M. J. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of sparsity using
ℓ1-constrained quadratic programming (lasso). IEEE Transactions on Information Theory  55:
2183–2202  2009.

9

,Niv Nayman
Asaf Noy
Tal Ridnik
Itamar Friedman
Rong Jin
Lihi Zelnik