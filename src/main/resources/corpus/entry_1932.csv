2012,Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress,Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example  PAC-MDP approaches such as Rmax base their model certainty on the amount of collected data  while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis  discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions.,Exploration in Model-based Reinforcement Learning

by Empirically Estimating Learning Progress

Manuel Lopes

INRIA

Bordeaux  France

Tobias Lang
FU Berlin
Germany

Marc Toussaint

FU Berlin
Germany

Abstract

Pierre-Yves Oudeyer

INRIA

Bordeaux  France

Formal exploration approaches in model-based reinforcement learning estimate
the accuracy of the currently learned model without consideration of the empir-
ical prediction error. For example  PAC-MDP approaches such as R-MAX base
their model certainty on the amount of collected data  while Bayesian approaches
assume a prior over the transition dynamics. We propose extensions to such
approaches which drive exploration solely based on empirical estimates of the
learner’s accuracy and learning progress. We provide a “sanity check” theoreti-
cal analysis  discussing the behavior of our extensions in the standard stationary
ﬁnite state-action case. We then provide experimental studies demonstrating the
robustness of these exploration measures in cases of non-stationary environments
or where original approaches are misled by wrong domain assumptions.

1

Introduction

Reinforcement learning (RL) agents need to solve the exploitation-exploration tradeoff. They have
to exploit their current model of the environment. At the same time they need to explore the en-
vironment sufﬁciently to learn more about its reward-relevant structure. Established model-based
approaches like E3 [8] and R-MAX [4] take into account how often a state-action pair has been
visited. With an efﬁcient model learner  the estimated transition model can be guaranteed to be
approximately correct after a sufﬁcient  and efﬁcient  number of visitations in a stationary domain.
An alternative approach to exploration is Bayesian RL [11]. Bayesian RL exploits prior knowledge
about the transition dynamics to reason explicitly about the uncertainty of the estimated model. In-
terestingly  these existing approaches estimate the accuracy of the currently learned model based
only on visitation counts. They do not consider the actual empirical prediction performance or
learning rate of the learner w.r.t. the data seen so far.
What happens if the fundamental presumption of R-MAX and Bayesian RL fails that each single
seen data-point will increase the agent’s certainty about its model? This is the case if the transi-
tion dynamics change over time (so there is no correct stationary prior)  or if we want to be able
to ignore non-learnable  currently “too difﬁcult” parts of the state space  either to the level of noise
or limitations of a learning algorithm. For example  a household robot cannot learn how to repair
a cupboard until it has achieved a basic understanding for the handling of tools. Such scenarios
require the development of new methods and even new measures of success. Previous work into this
direction emphasizes the concept of intrinsic motivation [10  13  12] and has shown empirical suc-
cess in developmental robotics [1]. An interesting aspect about this work is its reliance on empirical
measures of learning progress to drive exploration in reinforcement learning [17  6]. However  to
our knowledge this has not been made rigorous or related to fundamental methods like R-MAX or
Bayesian RL.
In this paper  we aim to draw these relations and make the following contributions: (i) We propose
to drive exploration in model-based RL by the estimated progress in model learning. We estimate
this progress in terms of the loss over the training data used for model learning. (ii) We introduce

1

two algorithms based on modiﬁcations of R-MAX and the recent Bayesian exploration bonus (BEB)
approach [9]. In contrast to the existing approaches  our algorithms do not have to assume correct
prior knowledge or that the visitation counts translate directly to model certainty. Hence  they can
also cope with changing dynamics.
(iii) While our extensions are targeted at scenarios that go
beyond the standard domain of stationary unstructured  ﬁnite state and action spaces  we provide
a kind of theoretical sanity check of our extensions exactly under these standard assumptions: We
discuss exploration guarantees under standard assumptions analogous to those of R-MAX and BEB.
In the next section  we review background work on exploration in Markov decision processes. Then
we present our approaches for exploration based on an empirical estimate of the future model learn-
ing progress. Thereafter  we discuss guarantees on their exploration efﬁciency. Finally  we present
an empirical evaluation before we conclude.

2 Background on Exploration

for a ﬁxed horizon H  R = (cid:80)H

t=1 R(st)  or for an inﬁnite horizon  R = (cid:80)

We model the interaction of an agent with its environment as a Markov decision process (MDP).
An MDP is a discrete-time stochastic control process where at each time-step the process is in one
of a ﬁxed set S of states and the agent can choose an action from a set A. The transition model T
speciﬁes the conditional transition distribution T (s(cid:48) | a  s) over successor states s(cid:48) when executing
an action a in a given state s. In unstructured ﬁnite state and action spaces T can be deﬁned by
separate multinomial distributions T (s  a) over successor states for each state-action pair s  a. The
agent receives rewards in states according to a function R : S → R. A policy π : S → A
speciﬁes for each state the action to take. The goal of planning in an MDP is to ﬁnd the optimal
policy π∗ which maximizes the expected future rewards E[R]. The future rewards R can be deﬁned
t γtR(st)  using a
discount factor 0 < γ < 1. The value of state s when acting according to policy π is deﬁned as
the expected future rewards when starting from s  V π(s) = E[R | s1 = s  π]. The optimal policy
π∗ can be found by classical algorithms such as value iteration or policy iteration. In reinforcement
learning  the agent does not know the transition model T . In a model-based approach  the agent
estimates ˆT from its interaction trace with the environment ∆ = (cid:104)s1  r1  a1  . . .   sT   rT(cid:105). Based on
ˆT it computes (approximately) optimal policies. A simple approach to the exploitation-exploration
tradeoff is -greedy:
the agent performs a random action for exploration with probability  and
exploits otherwise by executing a greedy policy with respect to ˆT . If  decreases over time towards
0  -greedy exploration converges to π∗. However  this may take an inefﬁciently large number of
non-optimal actions which is exponential in |S| and |A|.
In the PAC-MDP (probably approximately correct) framework  the efﬁciency of an exploration
algorithm A is measured by its sample complexity [7]. This is the number of time-steps when fol-
(st) − .
lowing A where its policy πA
Given δ with 0 < δ < 1  A is said to be PAC-MDP efﬁcient if with probability 1 − δ its sample com-
plexity scales polynomially in quantities describing T as well as in δ and  (and γ). The model-based
RL algorithms E3 [8] and R-MAX [4] are PAC-MDP efﬁcient exploration methods for unstructured
ﬁnite state and action spaces: their sample complexity scales polynomially in |S| and |A|. E3
and R-MAX share the central concept of known states and actions which have been observed suf-
ﬁciently often. (Counts are also used in the theoretical analysis of alternative PAC-MDP efﬁcient
algorithms like MBIE-EB [15].) If the visitation count c(s  a) of a state-action pair s  a is larger
than some threshold m  the estimate ˆT (s  a) is guaranteed to be with high probability -close to
the true model. Following a policy in these known states achieves approximately the same rewards
in the learned model ˆT and the true model T . To drive exploration  R-MAX is “optimistic in the
face of uncertainty” and assumes maximum reward Rmax in unknown states. This gives the reward
function

t at time t is not -optimal  that is  where V πA

t (st) < V π∗

(cid:26) R(s  a) c(s  a)≥ m (s  a known)

Rmax

c(s  a) < m (s  a unknown) .

RR-MAX(s  a) =

Typically  the threshold m is very large as E3 and R-MAX need to account for all possible model
instantiations within the model class of T [14]. For instance  for conservative choices  = 0.1 
γ = 0.9  δ = 0.1  S = 10  A = 5  we get m > 106  which is unfeasible in practice. PAC-MDP
approaches like R-MAX ignore the current empirical progress in learning T : the threshold m is ﬁxed

2

a-priori and remains the same for all s  a independently of the agent’s experiences or its estimated
relevance of s  a for large rewards.
Bayesian reinforcement learning [11] is an alternative approach to exploration. Here  the agent
takes its uncertainty about the learned model ˆT explicitly into account. This allows to incorporate
prior knowledge. More formally  the agent maintains a posterior belief b over all possible transition
models T given its previous experiences ∆ and a prior. The value function for a deterministic policy
π(b  s) is deﬁned in terms of the state s and the belief state b and fulﬁlls

V π(b  s) = R(s  π(b  s)) +

T (b(cid:48)  s(cid:48) | b  s  π(b  s)) V π(b(cid:48)  s(cid:48)

) .

(cid:90)

b(cid:48) s(cid:48)

(1)
The optimal Bayesian policy π∗ = argmaxπ V π(b  s) solves the exploitation-exploration tradeoff
implicitly: π∗ considers how actions affect not only the state of the world  but also the agent’s inter-
nal belief about the world. In a Bayesian RL context for a ﬁnite horizon H  the sample complexity
of an algorithm A can be deﬁned as the number of time-steps when following A where its policy
t at time t is not -Bayesian-optimal  that is  where V πA
H (bt  st) − . Due to
πA
the complexity of the belief state  however  Bayesian RL is typically intractable in terms of both
planning and updating the belief after an action. A recent approximate solution to Bayesian RL
for unstructured ﬁnite representations is the Bayesian exploration bonus (BEB) [9] which resembles
closely MBIE-EB [15]. In this setting  the belief b over T can be expressed by means of a separate
(cid:80)
Dirichlet distribution α(s  a  s(cid:48)) for each s  a with a mean estimator ˆTb(s(cid:48) | s  a) = α(s a s(cid:48))
s(cid:48) α(s a s(cid:48)).
BEB avoids reasoning in the belief space: it solves an MDP built from the mean estimate ˆTb using
an additional exploration bonus β/(1 + c(s  a)) to reward state-action pairs inversely according to
their visitation counts c(s  a). In the undiscounted  ﬁxed horizon H formulation BEB has a Bayesian
sample complexity which with probability 1− δ is polynomial in |S| and |A| when setting β = 2H 2

and updating the belief for s  a is stopped once(cid:80)

s(cid:48) α(s  a  s(cid:48)) > 4H 3/ [9].

H (bt  st) < V π∗

t

Bayesian RL approaches are vulnerable to incorrect priors. Fard and Pineau [5] have combined
Bayesian and PAC approaches to derive bounds on the approximation error in the value function of
policies regardless of the correctness of the prior. However  their bounds do not apply to changing
transition dynamics and it remains unanswered how to incorporate them in efﬁcient exploration
algorithms.
In a wider context of RL and developmental robotics  many strategies for efﬁcient exploration have
been subsumed by the concept of intrinsic motivation [10  13] which is also termed fun or curiosity
[12]. Many of these approaches take empirical learning progress into account. This includes meth-
ods that estimate from the agent’s experience the amount of potential learning progress in different
regions of the state space. Thereby  exploration focuses on those areas where learning progress
can indeed be made: areas which are neither already well-understood nor currently too difﬁcult to
learn. The resulting algorithms enable an agent to develop progressively more complex skills. For
instance  this has been demonstrated for learning in robot control [1] and options learning in hierar-
chical reinforcement learning [16] and has also been considered lately in machine learning under the
name of curriculum learning [3]. Under the RL formalism only very recently have the concept of in-
trinsic motivation been mixed with standard exploration-exploitation strategies [6]. So far  however 
guarantees about the sample complexity of intrinsic motivation based exploration approaches have
been missing. In this paper  we take up the ideas of intrinsic motivation to extend the theoretically
founded exploration approaches described above.

3 Exploration by Empirically Estimated Model Accuracy and Learning

Progress

PAC-MDP approaches like R-MAX and Bayesian RL approaches like BEB have been developed
in the context of stationary ﬁnite state-action domains. In those problems  we know that after a
ﬁxed number of visits to a state its estimated transition model becomes approximately correct and
we can perform exact belief updates to guarantee Bayesian optimality. In the following  we present
extensions which rely instead on previous exploration and learning experience to estimate in which
parts of the state and action space further exploration is promising and where not. This is helpful
in situations where the basic assumptions of R-MAX and BEB about model improvement might be
violated: for example  when we have an incorrect prior (large misspeciﬁed priors may impair the

3

(cid:26) R(s  a)

Rmax

performance of BEB[9])  when we cannot come up with an a-priori threshold on the data number
for learning accurate models (e.g. in domains where a polynomial KWIK learner is not available)  or
when the transition dynamics change over time. We can also see our approach as a method to adjust
the standard R-MAX threshold online  allowing to adapt to different levels of noise on different parts
of the state space.

3.1 Exploration Driven by Learning Progress
Let ζ : S × A (cid:55)→ R denote a measure for the expected learning and exploration progress when
visiting a state-action pair s  a. We discuss concrete deﬁnitions of ζ later. Clearly  ζ is a non-
stationary function which changes with new experiences. Hence  an exploration strategy based on ζ
needs to re-estimate ζ with each new experience. We use ζ to deﬁne two exploration algorithms.
Our ﬁrst approach ζ-R-MAX is based on R-MAX [4]. ζ-R-MAX acts greedily with respect to the
optimal policy for the reward function

Rζ-R-MAX(s  a) =

ζ(s  a) < m
else

.

(2)

Instead of rewarding arbitrary states with low visitation counts (considered unknown) directly as
in R-MAX  ζ-R-MAX gets large reward for exploring such state-action pairs where the expected
learning progress is large.
Our second approach ζ-EB is based on Bayesian Exploration Bonus (BEB) [9]. ζ-EB acts greedily
with respect to the optimal policy for the reward function
Rζ-EB(s  a) = R(s  a) +

(3)

β
1√

ζ(s a)

1 +

for some constant β.
Instead of setting the exploration bonus directly proportional to visitation
counts as in BEB   ζ-EB gets a bonus for exploring state-actions pairs where the expected learning
progress is large. The idea of using expected learning progress to drive exploration is that we can
estimate ζ empirically from the interaction data ∆ = (cid:104)s1  a1  r1  s2  . . .   sT   rT(cid:105).

3.2 Empirically Estimated Model Accuracy and Learning Progress

We start by considering an empirical estimate of the current model accuracy. In a classical learning
context  model accuracy is ideally tested on a held-out test data set. However  to exploit the full
available data for model selection and algorithm comparison  cross validation methods have become
a standard method. The work [2] discusses the importance of estimating the variance of a cross-
validation estimator of the model accuracy  for instance to include this uncertainty of model accuracy
in a model selection criterion. We base our following treatment of ζ on this previous work.
Let ˆT denote the estimated transition model based on data ∆  which approximates the true model
T . In general  we assume that learning ˆT (s  a) implies minimizing the loss L( ˆT (s  a); Ds a) where
Ds a = {s(cid:48)
i) from s  a in ∆. ˆT may generalize
over states and actions  in which case the data for learning ˆT (s  a) and evaluating the loss may
include experience sets Ds(cid:48) a(cid:48) with s(cid:48) (cid:54)= s  a(cid:48) (cid:54)= a (this is for example important for relational or
continuous domains). A typical loss is the neg-log data-likelihood 

i}ns a
i=1 are the successor states in the transitions (s  a  s(cid:48)

L( ˆT ; Ds a) = − 1

|Ds a| log

Given such a loss  the predictive error is deﬁned as

(cid:89)

s(cid:48)∈Ds a

ˆT (s(cid:48) | s  a) .

(4)

(5)
An empirical estimator of the PE based on the available data Ds a is the leave-one-out cross-
validation estimator

P E(s  a) = Es(cid:48)∼T (s a) L( ˆT ;{s(cid:48)}) .

CV (Ds a  s  a) =

1

|Ds a|

(cid:96)loo
s(cid:48)

 

(cid:96)loo
s(cid:48)

:= L( ˆT −s(cid:48)

;{s(cid:48)})

(6)

(cid:88)

s(cid:48)∈D

4

is the model learned from data D−s(cid:48)

where ˆT −s(cid:48)
Putting an absolute threshold directly on the loss to decide whether a state is known or unknown

is hard. Note that the predictive error P E(s  a) = KL(cid:0)T (s  a)

(cid:12)(cid:12)(cid:12)(cid:12) ˆT (s  a)(cid:1) + H(cid:0)T (s  a)(cid:1) has the

s a = Ds a \ {s(cid:48)}.

entropy of the true distribution as a lower bound  which is unknown. Therefore  we propose to
drive exploration based on the learning progress instead of the current learner accuracy. Using the
change in loss we may gain robustness by becoming independent of the loss’ absolute value and can
potentially detect time-varying conditions.
i ∈
We deﬁne ζ in terms of the change in the (empirically estimated) loss as follows. Let D−k
Ds a | i < ns a − k} denote the experiences in Ds a except the last k. ˆT −k is the transition model
s a  in contrast to ˆT which is learned from all data Ds a. We
learned from the reduced data-set D−k
deﬁne

s a = {s(cid:48)

ˆζ(s  a) := CV (D−k

s a  s  a) − CV (Ds a  s  a) ≈ L( ˆT −k; Ds a) − L( ˆT ; Ds a) .

(7)
This estimates to which extent the last k experiences help to learn a better model as evaluated over
the complete data. Thus  if ˆζ(s  a) is small  then the last k visitations in the data-set Ds a did not
have a signiﬁcant effect on improving ˆT and in turn s  a does not require further exploration.
The estimator ˆζ(s  a) deﬁned above is only a mean estimator of the learning progress. The ζ(s  a) we
use in concrete exploration algorithms ζ-EB and ζ-R-MAX includes an additional variance margin 

ζ(s  a) := ˆζ(s  a) + α(cid:112)ν(s  a)  

where ν(s  a) is an estimate of the CV variance (discussed in more detail below) 

ν(s  a) =

1

|Ds a|

s(cid:48) − CV (Ds a  s  a)]2 .
[(cid:96)loo

(cid:88)

s(cid:48)∈Ds a

(8)

(9)

The variance margin increases robustness and is motivated by the following analysis.

3.3 Guarantees on the Exploration Efﬁciency

As discussed in the introduction  we propose our extensions of R-MAX and BEB to target scenarios
which go beyond the standard setting of stationary unstructured  ﬁnite state and action spaces. In
this subsection  however  we go back and consider the behavior of our extensions exactly under
these classical assumptions—this is meant as a sanity check to ensure that our extensions inherit
the standard exploration efﬁciency properties under standard assumptions. We will directly relate a
threshold on the empirical ζ(s  a) to a threshold on model accuracy.

We start by providing two properties of the mean

under random data. First we ﬁnd

that the expected ˆζ(s  a) converges with 1/n2:
Lemma 1. For a Dirichlet learner in a stationary environment  we have

(cid:69)
(cid:68)ˆζ(s  a)
(cid:18) 1

Ds a

(cid:19)

.

= O

Ds a

n2

s a

(cid:68)ˆζ(s  a)
(cid:69)

(cid:68)ˆζ(s  a)
(cid:69)

A proof is given in the supplementary material. Similarly  a threshold on the mean
implies a model accuracy threshold:
Lemma 2. Given an approximated model ˆT of a true model T   for any  there exists an (cid:48) such
that:

Ds a

| < (cid:48) ⇒ | ˆT (s  a) − T (s  a)| <  .

(10)

|(cid:68)ˆζ(s  a)

(cid:69)

Ds a

Sketch of proof. For the case of multinomial variables  we know that the maximum likelihood
estimator is consistent and unbiased and is equal to the normalized visit counts. In this situation we
n(ˆp − p) (cid:32) N (0  Σ). Our measure of progress ζ is the difference between
know that as n → ∞ 

√

5

(cid:68)ˆζ(s  a)
(cid:69)

two maximum likelihood estimators and so  as n approaches inﬁnity we have the same limiting result
on the model quality  with a higher variance due to the subtraction between two different random
variables and the correlation between them. (cid:3)
under random
With the previous results we know that if we had access to the mean
data we would be able to assess model error by looking at its value. Unfortunately  the agent only
has access to an empirically estimate. To ensure that we can deﬁne robust criteria for considering a
state as known  we have to consider the variability of the estimator ˆζ(s  a) under random data Ds a.
As discussed in [2]  the estimator is unbiased  that is (cid:104)CV (Ds a  s  a)(cid:105)D = P E(s  a) and  in the
limit |Ds a| → ∞  the ( ˆT −T ) becomes Gaussian. Its variance can be described by ﬁrst considering
the covariance matrix C of the vector ((cid:96)loo
s(cid:48) )s(cid:48)∈Ds a under random Ds a. The diagonal entries are the
variances VarD{(cid:96)loo
s(cid:48) } of each single (cid:96)loo
s(cid:48) under random data  which are independent of s(cid:48) (assuming
i.i.d. data) and therefore equally estimated as ν(s  a) given in Eq. (9). The off-diagonal entries of C
capture the correlations between different (cid:96)loo
s(cid:48)(cid:48) and are constant (due to i.i.d. data; see [2] for
details). By assuming these correlations to vanish we over-estimate the CV variance and therefore
have  from Eq. (6) 

s(cid:48) and (cid:96)loo

Ds a

VarDs a{CV (Ds a  s  a)} ≤ ν(s  a)

(11)
Having an overestimation of the variance of the loss we will consider what is the variance of the
estimation of ˆζ(s  a). Both terms L( ˆT ; Ds a) and L( ˆT −k; Ds a) are estimated using LOO-CV 
allowing us to bound the ˆζ’s variance under random data  from Eq. (11)  as:

VarDs a{ˆζ(s  a)} ≤ 2ν(Ds a  s  a) .

Now that we have a conﬁdence measure on the estimator we can show that a threshold on the
:

(cid:68)ˆζ(s  a)
empirical estimator ζ(s  a) = ˆζ(s  a) + α(cid:112)ν(s  a) implies a threshold on the mean
| < (cid:3) with probability 1 − δ .

(cid:2)|ˆζ(s  a)| + α(cid:112)ν(s  a) <  ⇒ |(cid:68)ˆζ(s  a)
(cid:69)

Lemma 3. For any given δ with 0 < δ < 1 and  > 0 there exists an α such that

(cid:69)

(13)

Ds a

(12)

√
Proof. For a Gaussian variable x with mean µ and variance ν we know that x < µ + α
probability given by the error function δ = 1/2 + erf(α/
the above.

ν with
2)/2. By inverting this we get α to fulﬁll

Ds a

√

Finally  we show that our exploration method using the empirical ζ(s  a) to drive exploration is
PAC-MDP efﬁcient.
Lemma 4. There is a threshold m such that ζ-R-MAX using a Dirichlet learner in the standard
setting of stationary unstructured  ﬁnite state and action spaces is PAC-MDP efﬁcient.

Proof. From Lemma 3 we know that a threshold on our empirical measure implies a threshold on
the mean measure. From Lemma 2 we know that a small
corresponds  with high
probability  to a low model error. Under these conditions a state is only marked as known if the
empirical measure is below a certain threshold; this ensures with high probability that the error in
the model for the state-action is low. From the standard conditions of R-MAX  ζ-R-MAX is PAC-
MDP efﬁcient.

Ds a

(cid:68)ˆζ(s  a)

(cid:69)

4 Evaluation

We compare the empirical performance of our exploration algorithms ζ-R-MAX and ζ-EB with
the performance of R-MAX  BEB and simple model-based -greedy exploration with optimistic
initialization in unstructured ﬁnite state and action spaces. We investigate different scenarios where
the assumptions of R-MAX and BEB are fulﬁlled or violated. We deﬁne these scenarios by varying
the level of stochasticity in state transitions. BEB assumes to have a correct a prior about this
stochasticity. R-MAX assumes to know correct thresholds m for the number of visits to states to

6

ensure accurate transition models. We simulate satisﬁed or violated assumptions of R-MAX by
setting individual thresholds for states: in general  states with higher noise require more samples to
achieve the same level of model accuracy as states with low noise. Setting individual thresholds is
equivalent to setting individual initials counts (instead of 0s) for states.
We investigate three questions: (a) How close do our algorithms ζ-R-MAX and ζ-EB get to the
performance of the original algorithms when the assumptions of the latter are correct? (b) How
much do ζ-R-MAX and ζ-EB gain if the assumptions of the original algorithms are violated? (c)
And are our approaches more robust to unexpected changes in the transition dynamics?
Our evaluation environment (shown in Fig. 1(a)) is a discrete MDP with 25 states and ﬁve actions:
up  down  left  right and stop. There is a single goal state with a reward of 1 (marked with “G”) and
several states with negative rewards −0.1 (darker states). The lighter states are noisy states: their
actions have less predictable effects. The transition probabilities of the noisy states are sampled
from a Dirichlet distribution with parameters α = 0.1  while the probabilities for all other states are
sampled from a Dirichlet with α = 1.0  in both cases eventually permuted to ensure that the highest
probability corresponds to the expected next state according to the name deﬁnition of the actions.
The shortest path from start to goal is not optimal due to the uncertainty in the transitions. Instead 
the optimal path avoids the lighter states and takes the route below. To ﬁnd this optimal path and
avoid local minima  an exploration algorithm needs to explore sufﬁciently and estimate the state
values accurately. We evaluate the performance of the algorithms in terms of the reward collected
in the true model T using the optimal policy π∗
ˆT for their learned model ˆT . In our results  we report
the policy value error deﬁned as VT (sI ; π∗
T ) in the value of the start state sI with
respect to the optimal policy π∗. In our experiments  the agent is reset to sI every 30 steps. All
ﬁgures presented in the following show the means and standard deviations over 20 runs. For the ζ
estimation we use k = 10.

ˆT ) − VT (sI ; π∗

Experiment 1: Correct Assumptions
In our ﬁrst experiment  the assumptions of BEB and R-
MAX are fulﬁlled: BEB is given the correct prior; R-MAX uses appropriate thresholds for states
(depending on the state stochasticity). ζ-R-MAX  ζ-EB and -greedy are not given any knowledge.
The results presented in Fig. 1(b) show that our exploration methods ζ-R-MAX and ζ-EB achieve
similar performance to the original methods  even without having a correct prior or state-dependent
thresholds. Both ζ-R-MAX and ζ-EB converge to the correct ﬁnal policy  requiring only moder-
ately more steps. In contrast  -greedy does not ﬁnd the optimal policy in reasonable time. Clearly 
the original algorithms could also be executed based on likewise correct  but less informative as-
sumptions: BEB with an uninformative prior  R-MAX with more conservative (larger) uniform
threeholds. Then  R-MAX would need longer learning time; BEB might not converge  see [9].

Experiment 2: Violated Assumptions Here  the assumptions of R-MAX and BEB are violated
(wrong thresholds/priors). This may well happen in practice where correct priors cannot always be
speciﬁed or the counts cannot be translated directly to model certainty. In each run  we initialize
R-MAX and BEB with a random uniform prior (in the interval of the minimum and maximum values
of the true prior  translated to counts for R-MAX). The results in Fig. 1(c) show that as expected
R-MAX and BEB do not converge any longer to the optimal policy: they explore states too long
whose dynamics are already well estimated  while neglecting states which require more samples for
an accurate model estimation. In contrast  ζ-R-MAX and ζ-EB do not rely on these assumptions and
again converge to the optimal policy.

Experiment 3: Change in Dynamics
In our ﬁnal experiment  the transition dynamics for a ran-
domly chosen state along the optimal path get permuted after 900 steps. As Fig. 1(d) shows  R-MAX
and BEB with correct assumptions for the original problem (before time-step 900) cannot compen-
sate for this as they base their estimate of the model certainty only on the visitation counts  but do
not look at the data itself. In contrast  ζ-R-MAX and ζ-EB detect such an unexpected event and can
refocus their exploration efforts.

5 Conclusions and Future Work

We have proposed to drive exploration in model-based reinforcement learning using the estimated
future progress in model learning. When estimating this learning progress empirically  exploration

7

(a) Evaluation Environment

(b) Experiment 1—Correct Assumptions

(c) Experiment 2—Violated Assumptions

(d) Experiment 3—Change in Dynamics

Figure 1: Experiments: (a) The agent starts at state “I” and needs to get to goal “G”. Grey (darker)
states incur a negative reward. Green (lighter) states have very noisy transition dynamics. (b) Like
R-MAX and BEB with correct assumptions  our algorithms ζ-R-MAX and ζ-EB based on an em-
pirical estimation of the learning progress converge to the optimal policy without relying on these
assumptions  but take a small extra amount of time. (c) When their assumptions are violated  R-
MAX and BEB fail to converge  while ζ-R-MAX and ζ-EB don’t rely on these assumptions and
again ﬁnd the optimal policy. (d) In contrast to existing methods  ζ-R-MAX and ζ-EB can cope with
the change in transition dynamics after 900 steps and refocus their exploration.

algorithms can be deﬁned which do not rely on correct prior knowledge and can cope with changing
transition dynamics. As a theoretical “sanity check” we have discussed efﬁciency guarantees of our
approaches similar to the ones of the established algorithms. Our novel problem settings provide
interesting opportunities for the development of RL algorithms and theoretical analyses for relevant
real-world scenarios  in particular for structured  continuous and non-stationary domains. It is also
worth to investigate in more depth the relation of our approach to the general concept of intrinsic
motivation as proposed in developmental robotics. In our view  a combination of methods which
trades off both strong prior assumptions together with empirical estimates of the learning progress
seems to be the most promising direction for future work on exploration in the real world.

Acknowledgments

Work supported by the Flowers Team (INRIA/ENSTA-Paristech)  Conseil R´egional d’Aquitaine
and the ERC grant EXPLORERS 24007. Tobias Lang and Marc Toussaint were supported by the
German Research Foundation under grants TO 409/1-3 and TO 409/7-1.

8

GIReferences
[1] A. Baranes and P.Y. Oudeyer. Active learning of inverse models with intrinsically motivated

goal exploration in robots. Robotics and Autonomous Systems  2012.

[2] Yoshua Bengio and Yves Grandvalet. No unbiased estimator of the variance of k-fold cross-

validation. Journal of Machine Learning Research (JMLR)  5:1089–1105  2004.

[3] Yoshua Bengio  J´erˆome Louradour  Ronan Collobert  and Jason Weston. Curriculum learning.

In Int. Conf. on Machine Learning (ICML)  pages 41–48  2009.

[4] Ronen I. Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for
near-optimal reinforcement learning. Journal of Machine Learning Research (JMLR)  3:213–
231  2002.

[5] Mahdi Milani Fard and Joelle Pineau. Pac-bayesian model selection for reinforcement learn-

ing. In Conf. on Neural Information Processing Systems (NIPS). 2010.

[6] Todd Hester and Peter Stone. Intrinsically motivated model learning for a developing curious

agent. In AAMAS Workshop on Adaptive Learning Agents (ALA)  2012.

[7] Sham Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis  Gatsby

Computational Neuroscience Unit  University College London  2003.

[8] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.

Machine Learning Journal  49(2-3):209–232  2002.

[9] J. Zico Kolter and Andrew Ng. Near-Bayesian exploration in polynomial time. In Int. Conf.

on Machine Learning (ICML)  pages 513–520  2009.

[10] P.Y. Oudeyer  F. Kaplan  and V.V. Hafner. Intrinsic motivation systems for autonomous mental

development. IEEE Transactions on Evolutionary Computation  11(2):265–286  2007.

[11] Pascal Poupart  Nikos Vlassis  Jesse Hoey  and Kevin Regan. An analytic solution to discrete

Bayesian reinforcement learning. In Int. Conf. on Machine Learning (ICML)  2006.

[12] J¨urgen Schmidhuber. Curious model-building control systems. In Proc. of Int. Joint Conf. on

Neural Networks  volume 2  pages 1458–1463  1991.

[13] Satinder Singh  Andrew G. Barto  and Nuttapong Chentanez. Intrinsically motivated reinforce-
ment learning. In Conf. on Neural Information Processing Systems (NIPS)  pages 1281–1288.
2005.

[14] Alexander L. Strehl  Lihong Li  and Michael Littman. Reinforcement learning in ﬁnite MDPs:

PAC analysis. Journal of Machine Learning Research (JMLR)  2009.

[15] Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation

for markov decision processes. J. Comput. Syst. Sci.  74(8):1309–1331  2008.

[16] Christopher M. Vigorito and Andrew G. Barto. Intrinsically motivated hierarchical skill learn-
IEEE Transactions on Autonomous Mental Development

ing in structured environments.
(TAMD)  2(2)  2010.

[17] Marco Wiering and J¨urgen Schmidhuber. Efﬁcient model-based exploration. In International

Conference on Simulation of Adaptive Behavior: From Animals to Animats 6  1998.

9

,Jason Lee
Jonathan Taylor