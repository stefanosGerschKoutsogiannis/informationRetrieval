2019,Sampling Sketches for Concave Sublinear Functions of Frequencies,We consider massive distributed datasets that consist of elements modeled as key-value pairs and the task of computing statistics or aggregates where the contribution of each key is weighted by a function of its frequency (sum of values of its elements). This fundamental problem has a wealth of applications in data analytics and machine learning  in particular  with concave sublinear functions of the frequencies that mitigate the disproportionate effect of keys with high frequency. The family of concave sublinear functions includes low frequency moments ($p \leq 1$)  capping  logarithms  and their compositions. A common approach is to sample keys  ideally  proportionally to their contributions and estimate statistics from the sample. A simple but costly way to do this is by aggregating the data to produce a table of keys and their frequencies  apply our function to the frequency values  and then apply a weighted sampling scheme. Our main contribution is the design of composable sampling sketches that can be tailored to any concave sublinear function of the frequencies. Our sketch structure size is very close to the desired sample size and our samples provide statistical guarantees on the estimation quality that are very close to that of an ideal sample of the same size computed over aggregated data. Finally  we demonstrate experimentally the simplicity and effectiveness of our methods.,Sampling Sketches for

Concave Sublinear Functions of Frequencies

Edith Cohen

Google Research  CA

Tel Aviv University  Israel
edith@cohenwang.com

Oï¬r Geri

Stanford University  CA

ofirgeri@cs.stanford.edu

Abstract

We consider massive distributed datasets that consist of elements modeled as key-
value pairs and the task of computing statistics or aggregates where the contribution
of each key is weighted by a function of its frequency (sum of values of its elements).
This fundamental problem has a wealth of applications in data analytics and
machine learning  in particular  with concave sublinear functions of the frequencies
that mitigate the disproportionate effect of keys with high frequency. The family
of concave sublinear functions includes low frequency moments (ğ‘ â‰¤ 1)  capping 
logarithms  and their compositions. A common approach is to sample keys  ideally 
proportionally to their contributions and estimate statistics from the sample. A
simple but costly way to do this is by aggregating the data to produce a table of keys
and their frequencies  apply our function to the frequency values  and then apply
a weighted sampling scheme. Our main contribution is the design of composable
sampling sketches that can be tailored to any concave sublinear function of the
frequencies. Our sketch structure size is very close to the desired sample size and
our samples provide statistical guarantees on the estimation quality that are very
close to that of an ideal sample of the same size computed over aggregated data.
Finally  we demonstrate experimentally the simplicity and effectiveness of our
methods.

1

Introduction

sum of values of the elements with that key  i.e.  ğœˆğ‘¥ :=âˆ‘ï¸€

We consider massive distributed datasets that consist of elements that are key-value pairs ğ‘’ =
(ğ‘’.key  ğ‘’.val ) with ğ‘’.val > 0. The elements are generated or stored on a large number of servers
or devices. A key ğ‘¥ may repeat in multiple elements  and we deï¬ne its frequency ğœˆğ‘¥ to be the
ğ‘’|ğ‘’.key=ğ‘¥ ğ‘’.val. For example  the keys
can be search queries  videos  terms  users  or tuples of entities (such as video co-watches or term
co-occurrences) and each data element can correspond to an occurrence or an interaction involving
this key: the search query was issued  the video was watched  or two terms co-occurred in a typed
sentence. An instructive common special case is when all elements have the same value 1 and the
frequency ğœˆğ‘¥ of each key ğ‘¥ in the dataset is simply the number of elements with key ğ‘¥.
A common task is to compute statistics or aggregates  which are sums over key contributions.
The contribution of each key ğ‘¥ is weighted by a function of its frequency ğœˆğ‘¥. One example of
ğ‘¥âˆˆğ» ğœˆğ‘¥ for some domain (subset of keys)
ğ». The domains of interest are often overlapping and speciï¬ed at query time. Sum aggregates
also arise as components of a larger pipeline  such as the training of a machine learning model
with parameters ğœƒ  labeled examples ğ‘¥ âˆˆ ğ’³ with frequencies ğœˆğ‘¥  and a loss objective of the form
ğ‘¥ ğ‘“ (ğœˆğ‘¥)ğ¿(ğ‘¥; ğœƒ). The function ğ‘“ that is applied to the frequencies can be any concave
sublinear function. Concave sublinear functions  which we discuss further below  are used in

such sum aggregates are queries of domain statisticsâˆ‘ï¸€
â„“(ğ’³ ; ğœƒ) =âˆ‘ï¸€

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

applications to mitigate the disproportionate effect of keys with very high frequencies. The training
of the model typically involves repeated evaluation of the loss function (or of its gradient that also
has a sum form) for different values of ğœƒ. We would like to compute these aggregates on demand 
without needing to go over the data many times.
When the number of keys is very large it is often helpful to compute a smaller random sample
ğ‘† âŠ† ğ’³ of the keys from which aggregates can be efï¬ciently estimated. In some applications 
obtaining a sample can be the end goal. For example  when the aggregate is a gradient  we can use
the sample itself as a stochastic gradient. To provide statistical guarantees on our estimate quality 
the sampling needs to be weighted (importance sampling)  with heavier keys sampled with higher
probability  ideally  proportional to their contribution (ğ‘“ (ğœˆğ‘¥)). When the weights of the keys are
known  there are classic sampling schemes that provide estimators with tight worst-case variance
bounds [28  13  6  10  32  33].
The datasets we consider here are presented in an unaggregated form: each key can appear multiple
times in different locations. The focus of this work is designing composable sketch structures
(formally deï¬ned below) that allow to compute a sample over unaggregated data with respect to
the weights ğ‘“ (ğœˆğ‘¥). One approach to compute a sample from unaggregated data is to ï¬rst aggregate
the data to produce a table of key-frequency pairs (ğ‘¥  ğœˆğ‘¥)  compute the weights ğ‘“ (ğœˆğ‘¥)  and apply a
weighted sampling scheme. This aggregation can be performed using composable structures that are
essentially a table with an entry for each distinct key that occurred in the data. The number of distinct
keys  however  and hence the size of that sketch  can be huge. For our sampling application  we
would hope to use sketches of size that is proportional to the desired sample size  which is generally
much smaller than the number of unique keys  and still provide statistical guarantees on the estimate
quality that are close to that of a weighted sample computed according to ğ‘“ (ğœˆğ‘¥).

Concave Sublinear Functions. Typical datasets have a skewed frequency distribution  where
a small fraction of the keys have very large frequencies and we can get better results or learn a
better model of the data by suppressing their effect. The practice is to apply a concave sublinear
function ğ‘“ to the frequency  so that the importance weight of the key is ğ‘“ (ğœˆğ‘¥) instead of simply its
ğ‘¥ for ğ‘ â‰¤ 1  ln(1 + ğœˆğ‘¥) 
frequency ğœˆğ‘¥. This family of functions includes the frequency moments ğœˆğ‘
capğ‘‡ (ğœˆğ‘¥) = min{ğ‘‡  ğœˆğ‘¥} for a ï¬xed ğ‘‡ â‰¥ 0  their compositions  and more. A formal deï¬nition
appears in Section 2.3.
Two hugely popular methods for producing word embeddings from word co-occurrences use this form
of mitigation: word2vec [25] uses ğ‘“ (ğœˆ) = ğœˆ0.5 and ğ‘“ (ğœˆ) = ğœˆ0.75 for positive and negative examples 
respectively  and GloVe [30] uses ğ‘“ (ğœˆ) = min{ğ‘‡  ğœˆ0.75} to mitigate co-occurrence frequencies.
When the data is highly distributed  for example  when it originates or resides at millions of mobile
devices (as in federated learning [24])  it is useful to estimate the loss or compute a stochastic gradient
update efï¬ciently via a weighted sample.
The suppression of higher frequencies may also directly arise in applications. One example is
campaign planning for online advertising  where the value of showing an ad to a user diminishes
with the number of views. Platforms allow an advertiser to specify a cap value ğ‘‡ on the number of
times the same ad can be presented to a user [19  29]. In this case  the number of opportunities to
display an ad to a user ğ‘¥ is a cap function of the frequency of the user ğ‘“ (ğœˆğ‘¥) = min{ğ‘‡  ğœˆğ‘¥}  and
ğ‘¥âˆˆğ» ğ‘“ (ğœˆğ‘¥). When planning a campaign  we
need to quickly estimate the statistics for different segments  and this can be done from a sample that
ideally is weighted by ğ‘“ (ğœˆğ‘¥).

the number for a segment of users ğ» is the statisticsâˆ‘ï¸€

Our Contribution.
In this work  we design composable sketches that can be tailored to any concave
sublinear function ğ‘“  and allow us to compute a weighted sample over unaggregated data with respect
to the weights ğ‘“ (ğœˆğ‘¥). Using the sample  we will be able to compute unbiased estimators for the
aggregates mentioned above. In order to compute the estimators  we need to make a second pass over
the data: In the ï¬rst pass  we compute the set of sampled keys  and in the second pass we compute
their frequencies. Both passes can be done in a distributed manner.
A sketch ğ‘†(ğ·) is a data structure that summarizes a set ğ· of data elements  so that the output of
interest for ğ· (in our case  a sample of keys) can be recovered from the sketch ğ‘†(ğ·). A sketch
structure is composable if we can obtain a sketch ğ‘†(ğ·1 âˆª ğ·2) of two sets of elements ğ·1 and ğ·2
from the sketches ğ‘†(ğ·1) and ğ‘†(ğ·2) of the sets. This property alone gives us full ï¬‚exibility to

2

parallelize or distribute the computation. The size of the sketch determines the communication and
storage needs of the computation.
We provide theoretical guarantees on the quality (variance) of the estimators. The baseline for our
analysis is the bounds on the variance that are guaranteed by PPSWOR on aggregated data. PPSWOR
[32  33] is a sampling scheme with tight worst-case variance bounds. The estimators provided by our
sketch have variance at most 4/((1 âˆ’ ğœ€)2) times the variance bound for PPSWOR. The parameter
ğœ€ â‰¤ 1/2 mostly affects the run time of processing a data element  which grows near-linearly in 1/ğœ€.
Thus  our sketch allows us to get approximately optimal guarantees on the variance while avoiding
the costly aggregation of the data. We remark that these guarantees are for soft concave sublinear
functions and the extension to any concave sublinear function incurs a factor of (1 + 1/(ğ‘’ âˆ’ 1))2 in
the variance. The space required by our sketch signiï¬cantly improves upon the previous methods
(which all require aggregating the data). In particular  if the desired sample size is ğ‘˜  we show that
the space required by the sketch at any given time is ğ‘‚(ğ‘˜) in expectation and is well concentrated.
We complement our work with a small-scale experimental study. We use a simple implementation of
our sampling sketch to study the actual performance in terms of estimate quality and sketch size. In
particular  we show that the estimate quality is even better than the (already adequate) guarantees
provided by our worst-case bounds. We additionally compare the estimate quality to that of two
popular sampling schemes for aggregated data  PPSWOR [32  33] and priority (sequential Poisson)
sampling [28  13]. In the experiments  we see that the estimate quality of our sketch is close to what
achieved by PPSWOR and priority sampling  while our sketch uses much less space by eliminating
the need for aggregation. This paper presents our sketch structures and states our results. The full
version (including proofs and additional details) can be found in the supplementary material.

Related Work. Composable weighted sampling schemes with tight worst-case variance for aggre-
gated datasets (where keys are unique to elements) include priority (sequential Poisson) sampling
[28  13]  VarOpt sampling [6  10]  and PPSWOR [32  33]. We use PPSWOR as our base scheme
because it extends to unaggregated datasets  where multiple elements can additively contribute to the
frequency/weight of each key. A proliï¬c line of research developed sketch structures for different
tasks over streamed or distributed unaggregated data [26  16  1]. Composable sampling sketches
for unaggregated datasets have the goal of meeting the quality of samples computed on aggregated
frequencies while using a sketch structure that can only hold a ï¬nal-sample-size number of distinct
keys. Prior work includes a folklore sketch for distinct sampling (ğ‘“ (ğœˆ) = 1 when ğœˆ > 0) [22  34] 
sum sampling (ğ‘“ (ğœˆ) = ğœˆ) [9  18  14  11] based on PPSWOR  cap functions (ğ‘“ (ğœˆ) = min{ğ‘‡  ğœˆ}) [7] 
and universal (multi-objective) samples with a logarithmic overhead that simultaneously support all
concave sublinear ğ‘“. In the current work we propose sampling sketches that can be tailored to any
concave sublinear function and only have a small constant overhead. An important line of work uses
random linear projections to estimate frequency statistics and to sample. In particular  â„“ğ‘ sampling
sketches [17  27  2  21  20] sample (roughly) according to ğ‘“ (ğœˆ) = ğœˆğ‘. These sketches have higher
overhead than sample-based sketches and are more limited in their application. Their advantage is that
they can be used with super-linear (e.g.  moments with ğ‘ âˆˆ (1  2]) functions of frequencies and can
also support signed element values (the turnstile model). For the more basic problem of sketches that
estimate frequency statistics over the full data  a characterization of sketchable frequency functions is
provided in [5  3]. Universal sketches for estimating â„“ğ‘ norms of subsets were recently considered in
[4]. A double logarithmic size sketch (extending [15] for distinct counting) that computes statistics
over the entire dataset for all soft concave sublinear functions is provided in [8]. Our design builds on
components of that sketch.

2 Preliminaries

Sumğ·(ğ‘§) := âˆ‘ï¸€
sum and the max-distinct statistics of ğ· are deï¬ned  respectively  as Sumğ· :=âˆ‘ï¸€

Consider a set ğ· of data elements of the form ğ‘’ = (ğ‘’.key  ğ‘’.val ) where ğ‘’.val > 0. We denote
the set of possible keys by ğ’³ . For a key ğ‘§ âˆˆ ğ’³   we let Maxğ·(ğ‘§) := maxğ‘’âˆˆğ·|ğ‘’.key=ğ‘§ ğ‘’.val and
ğ‘’âˆˆğ·|ğ‘’.key=ğ‘§ ğ‘’.val denote the maximum value of a data element in ğ· with key
ğ‘§ and the sum of values of data elements in ğ· with key ğ‘§  respectively. Each key ğ‘§ âˆˆ ğ’³ that
appears in ğ· is called active. If there is no element ğ‘’ âˆˆ ğ· with ğ‘’.key = ğ‘§  we say that ğ‘§ is
inactive and deï¬ne Maxğ·(ğ‘§) := 0 and Sumğ·(ğ‘§) := 0. When ğ· is clear from context  it is omitted.
For a key ğ‘§  we use the shorthand ğœˆğ‘§ := Sumğ·(ğ‘§) and refer to it as the frequency of ğ‘§. The
ğ‘’âˆˆğ· ğ‘’.val and

3

MxDistinctğ· :=âˆ‘ï¸€

ğ‘§âˆˆğ’³ Maxğ·(ğ‘§). For a function ğ‘“  ğ‘“ğ· :=âˆ‘ï¸€

ğ‘§âˆˆğ’³ ğ‘“ (Sumğ·(ğ‘§)) =âˆ‘ï¸€

ğ‘§âˆˆğ’³ ğ‘“ (ğœˆğ‘§) is

the ğ‘“-frequency statistics of ğ·.

2.1 The Composable Bottom-ğ‘˜ Structure

In this work  we will use composable sketch struc-
tures in order to efï¬ciently summarize streamed or dis-
tributed data elements. A composable sketch structure
is speciï¬ed by three operations: The initialization of an
empty sketch structure ğ‘   the processing of a data ele-
ment ğ‘’ into a structure ğ‘   and the merging of two sketch
structures ğ‘ 1 and ğ‘ 2. To sketch a stream of elements  we
start with an empty structure and sequentially process
data elements while storing only the sketch structure.
The merge operation is useful with distributed or par-
allel computation and allows us to compute the sketch
ğ‘– ğ·ğ‘– of data elements by merg-
ing the sketches of the parts ğ·ğ‘–. In particular  one of
the main building blocks that we use is the bottom-ğ‘˜
structure [12]  speciï¬ed in Algorithm 1. The structure
maintains ğ‘˜ data elements: For each key  consider only
the element with that key that has the minimum value.
Of these elements  the structure keeps the ğ‘˜ elements
that have the lowest values.

of a large set ğ· = â‹ƒï¸€

2.2 The PPSWOR Sampling Sketch

Algorithm 1: Bottom-ğ‘˜ Sketch Structure
// Initialize structure
Input: the structure size ğ‘˜
ğ‘ .ğ‘ ğ‘’ğ‘¡ â† âˆ… // Set of â‰¤ ğ‘˜ key-value
pairs
// Process element
Input: element ğ‘’ = (ğ‘’.key  ğ‘’.val )  a
if ğ‘’.key âˆˆ ğ‘ .ğ‘ ğ‘’ğ‘¡ then

bottom-ğ‘˜ structure ğ‘ 

replace the current value ğ‘£ of ğ‘’.key in
ğ‘ .ğ‘ ğ‘’ğ‘¡ with min{ğ‘£  ğ‘’.val}

else

insert (ğ‘’.key  ğ‘’.val ) to ğ‘ .ğ‘ ğ‘’ğ‘¡
if |ğ‘ .ğ‘ ğ‘’ğ‘¡| = ğ‘˜ + 1 then

Remove the element ğ‘’â€² with
maximum value from ğ‘ .ğ‘ ğ‘’ğ‘¡
// Merge two bottom-ğ‘˜ structures
Input: ğ‘ 1 ğ‘ 2 // Bottom-ğ‘˜ structures
Output: ğ‘  // Bottom-ğ‘˜ structure
ğ‘ƒ â† ğ‘ 1.ğ‘ ğ‘’ğ‘¡ âˆª ğ‘ 2.ğ‘ ğ‘’ğ‘¡
ğ‘ .ğ‘ ğ‘’ğ‘¡ â† the (at most) ğ‘˜ elements of ğ‘ƒ with
lowest values (at most one element per key)

bility ğ‘¤ğ‘¥/âˆ‘ï¸€

Algorithm 2: PPSWOR Sampling Sketch
// Initialize structure
Input: the sample size ğ‘˜
Initialize a bottom-ğ‘˜ structure ğ‘ .sample
// Algorithm 1
// Process element
Input: element ğ‘’ = (ğ‘’.key  ğ‘’.val )  PPSWOR
ğ‘£ âˆ¼ Exp[ğ‘’.val ]
Process the element (ğ‘’.key  ğ‘£) into the bottom-ğ‘˜
structure ğ‘ .sample
// Merge two structures ğ‘ 1  ğ‘ 2 to obtain ğ‘ 
ğ‘ .sample â† Merge the bottom-ğ‘˜ structures
ğ‘ 1.sample and ğ‘ 2.sample

In this subsection  we describe a scheme to pro-
duce a sample of ğ‘˜ keys  where at each step the
probability that a key is selected is proportional
to its weight. That is  the sample we produce
will be equivalent to performing the following
ğ‘˜ steps. At each step we select one key and
add it to the sample. At the ï¬rst step  each key
ğ‘¥ âˆˆ ğ’³ (with weight ğ‘¤ğ‘¥) is selected with proba-
ğ‘¦ ğ‘¤ğ‘¦. At each subsequent step  we
choose one of the remaining keys  again with
probability proportional to its weight. This pro-
cess is called probability proportional to size
and without replacement (PPSWOR) sampling.
A classic method for PPSWOR sampling is the
following scheme [32  33]. For each key ğ‘¥ with weight ğ‘¤ğ‘¥  we independently draw seed(ğ‘¥) âˆ¼
Exp(ğ‘¤ğ‘¥). The output sample will include the ğ‘˜ keys with smallest seed(ğ‘¥). This method together
with a bottom-ğ‘˜ structure can be used to implement PPSWOR sampling over a set of data elements
ğ· according to ğœˆğ‘¥ = Sumğ·(ğ‘¥). The sampling sketch is presented here as Algorithm 2. This sketch
is due to [9] (based on [18  14  11]).

sample structure ğ‘ 

2.3 Concave Sublinear Functions
A function ğ‘“ : [0 âˆ) â†’ [0 âˆ) is soft concave sublinear if for some ğ‘(ğ‘¡) â‰¥ 0 it can be expressed as

âˆ«ï¸ âˆ

ğ‘“ (ğœˆ) = â„’c[ğ‘](ğœˆ) :=

ğ‘(ğ‘¡)(1 âˆ’ ğ‘’âˆ’ğœˆğ‘¡)ğ‘‘ğ‘¡ .

(1)

â„’c[ğ‘](ğœˆ) is called the complement Laplace transform of ğ‘ at ğœˆ. The sampling schemes we present in
this work will be deï¬ned for soft concave sublinear functions of the frequencies. However  this will
allow us to estimate well any function that is within a small multiplicative constant of a soft concave
sublinear function. In particular  we can estimate concave sublinear functions. These functions can

0

4

Algorithm 3: Sampling Sketch Structure for ğ‘“
// Initialize empty structure ğ‘ 
Input: ğ‘˜: Sample size  ğœ€  ğ‘(ğ‘¡) â‰¥ 0
Initialize ğ‘ . SumMax // SumMax sketch of
size ğ‘˜ (Algorithm 5)
Initialize ğ‘ .ğ‘ğ‘ğ‘ ğ‘¤ğ‘œğ‘Ÿ // PPSWOR sketch of size
ğ‘˜ (Algorithm 2)
Initialize ğ‘ .ğ‘ ğ‘¢ğ‘š â† 0 // A sum of all the
elements seen so far
Initialize ğ‘ .ğ›¾ â† âˆ // Threshold
Initialize ğ‘ .Sideline // A composable
max-heap/priority queue
// Merge two structures ğ‘ 1 and ğ‘ 2 to ğ‘  (with

ğ‘ .ğ‘ ğ‘¢ğ‘š

same ğ‘˜  ğœ€  ğ‘ and same â„ in SumMax
sub-structures)

ğ‘ .ğ‘ ğ‘¢ğ‘š â† ğ‘ 1.ğ‘ ğ‘¢ğ‘š + ğ‘ 2.ğ‘ ğ‘¢ğ‘š
ğ‘ .ğ›¾ â† 2ğœ€
ğ‘ .Sideline â† merge ğ‘ 1.Sideline and ğ‘ 2.Sideline
// merge priority queues.
ğ‘ . SumMax â† merge ğ‘ 1. SumMax and
ğ‘ 2. SumMax // Merge SumMax structures
(Algorithm 5)
while ğ‘ .Sideline contains an element
ğ‘” = (ğ‘”.key  ğ‘”.val ) with ğ‘”.val â‰¥ ğ‘ .ğ›¾ do

Remove ğ‘” from ğ‘ .Sideline

ifâˆ«ï¸€ âˆ
Process element (ğ‘”.key âˆ«ï¸€ âˆ

ğ‘”.val ğ‘(ğ‘¡)ğ‘‘ğ‘¡ > 0 then

ğ‘”.val ğ‘(ğ‘¡)ğ‘‘ğ‘¡) by

ğ‘ . SumMax

be expressed as

âˆ«ï¸ âˆ

// Process element
Input: Element ğ‘’ = (ğ‘’.key  ğ‘’.val )  structure ğ‘ 
Process ğ‘’ by ğ‘ .ğ‘ğ‘ğ‘ ğ‘¤ğ‘œğ‘Ÿ
ğ‘ .ğ‘ ğ‘¢ğ‘š â† ğ‘ .ğ‘ ğ‘¢ğ‘š + ğ‘’.val
ğ‘ .ğ›¾ â† 2ğœ€
foreach ğ‘– âˆˆ [ğ‘Ÿ] do // ğ‘Ÿ = ğ‘˜/ğœ€

ğ‘ .ğ‘ ğ‘¢ğ‘š

ğ‘¦ âˆ¼ Exp[ğ‘’.val ] // exponentially
distributed with parameter ğ‘’.val
// Process in Sideline
if The key (ğ‘’.key  ğ‘–) appears in ğ‘ .Sideline then

Update the value of (ğ‘’.key  ğ‘–) to be the
minimum of ğ‘¦ and the current value

else

Add the element ((ğ‘’.key  ğ‘–)  ğ‘¦) to ğ‘ .Sideline

while ğ‘ .Sideline contains an element
ğ‘” = (ğ‘”.key  ğ‘”.val ) with ğ‘”.val â‰¥ ğ‘ .ğ›¾ do

Remove ğ‘” from ğ‘ .Sideline

ifâˆ«ï¸€ âˆ
Process element (ğ‘”.key âˆ«ï¸€ âˆ

ğ‘”.val ğ‘(ğ‘¡)ğ‘‘ğ‘¡ > 0 then

ğ‘ . SumMax

ğ‘”.val ğ‘(ğ‘¡)ğ‘‘ğ‘¡) by

ğ‘(ğ‘¡) min{1  ğœˆğ‘¡}ğ‘‘ğ‘¡

ğ‘“ (ğœˆ) =

(2)
for ğ‘(ğ‘¡) â‰¥ 0. The concave sublinear family includes all functions such that ğ‘“ (0) = 0  ğ‘“ is monotoni-
cally non-decreasing  ğœ•+ğ‘“ (0) < âˆ  and ğœ•2ğ‘“ â‰¤ 0.
Any concave sublinear function ğ‘“ can be approximated by a soft concave sublinear function as
follows. Consider the corresponding soft concave sublinear function Ëœğ‘“ using the same coefï¬cients
ğ‘(ğ‘¡). The function Ëœğ‘“ closely approximates ğ‘“ pointwise [8]:

0

(1 âˆ’ 1/ğ‘’)ğ‘“ (ğœˆ) â‰¤ Ëœğ‘“ (ğœˆ) â‰¤ ğ‘“ (ğœˆ) .

ğ·). The statistics ğ‘“ğ· =âˆ‘ï¸€

Our weighted sample for Ëœğ‘“ will respectively approximate a weighted sample for ğ‘“.
ğ‘¥ ğ‘“ (Sumğ·(ğ‘¥)) =âˆ‘ï¸€
Consider a soft concave sublinear ğ‘“ and a set of data elements ğ· with the respective frequency
function ğ‘Š : (0 âˆ) â†’ N âˆª {0} (for every ğœˆ > 0  ğ‘Š (ğœˆ) is the number of keys with frequency ğœˆ in
âˆ«ï¸ ğ‘
ğ‘¥ ğ‘“ (ğœˆğ‘¥) can then be expressed as ğ‘“ğ· = â„’c[ğ‘Š ][ğ‘]âˆ
ğ‘(ğ‘¡)â„’c[ğ‘Š ](ğ‘¡)ğ‘‘ğ‘¡ .

with the notation

â„’c[ğ‘Š ][ğ‘]ğ‘

(3)

0

ğ›¾ :=

ğ›¾

3 Sketch Overview

Given a set ğ· of elements ğ‘’ = (ğ‘’.key  ğ‘’.val )  we wish to maintain a sample of ğ‘˜ keys  that will be
close to PPSWOR according to a soft concave sublinear function of their frequencies ğ‘“ (ğœˆğ‘¥). At a high
âˆ«ï¸€ âˆ
level  our sampling sketch is guided by the sketch for estimating the statistics ğ‘“ğ· due to Cohen [8].
Recall that a soft concave sublinear function ğ‘“ can be represented as ğ‘“ (ğ‘¤) = â„’c[ğ‘](ğ‘¤)âˆ
0 =
0 ğ‘(ğ‘¡)(1 âˆ’ ğ‘’âˆ’ğ‘¤ğ‘¡)ğ‘‘ğ‘¡ for ğ‘(ğ‘¡) â‰¥ 0. Using this representation  we express ğ‘“ (ğœˆğ‘¥) as a sum of two
contributions for each key ğ‘¥:

ğ‘“ (ğœˆğ‘¥) = â„’c[ğ‘](ğœˆğ‘¥)ğ›¾

0 + â„’c[ğ‘](ğœˆğ‘¥)âˆ
ğ›¾  

5

seed pairs

ğ‘ . SumMax

ifâˆ«ï¸€ ğ›¾

ifâˆ«ï¸€ âˆ

ğ›¾ ğ‘(ğ‘¡)ğ‘‘ğ‘¡) by sketch

foreach ğ‘’ âˆˆ ğ‘ . SumMax .sample do

ğ‘’.val â† ğ‘Ÿ * ğ‘’.val // scale
sample by ğ‘Ÿ

ğ›¾ ğ‘(ğ‘¡)ğ‘‘ğ‘¡ > 0 then
foreach ğ‘’ âˆˆ ğ‘ .Sideline do

(ğ‘’.key âˆ«ï¸€ âˆ

Process element

Algorithm 4: Produce a Final Sam-
ple from a Sampling Sketch Structure
(Algorithm 3)
Input: sampling sketch structure ğ‘  for ğ‘“
Output: sample of size ğ‘˜ of key and

0 ğ‘¡ğ‘(ğ‘¡)ğ‘‘ğ‘¡ > 0 then
foreach ğ‘’ âˆˆ ğ‘ .ppswor.sample do
âˆ«ï¸€ ğ›¾
ğ‘’.val â† ğ‘’.val
sample â† merge
ğ‘ . SumMax .sample and
ğ‘ .ppswor.sample // bottom-ğ‘˜
merge (Algorithm 1)
sample â† ğ‘ . SumMax .sample

where ğ›¾ is a value we will set adaptively while processing the elements. Our sampling sketch is
described in Algorithm 3. It maintains a separate sampling sketch for each set of contributions. In
order to produce a sample from the sketch  these separate sketches need to be combined. Algorithm 4
describes how to produce a ï¬nal sample from the sketch.
Running Algorithm 3 and then Algorithm 4 requires one
pass over the data. In order to use the ï¬nal sample to esti-
mate statistics  we need to compute the Horvitz-Thompson
inverse-probability estimator (cid:92)ğ‘“ (ğœˆğ‘¥) for each of the sam-
pled keys. Informally  the estimator for key ğ‘¥ in the sam-
ple is ğ‘“ (ğœˆğ‘¥)/ Pr[ğ‘¥ in sample] (and 0 for keys not in the
sample). To compute the estimator  we need to know the
values ğ‘“ (ğœˆğ‘¥) for the keys in the sample  which we get
from a second pass over the data  and the conditional in-
clusion probabilities (the denominator)  that have a closed
form and can be computed. The parameter ğœ€ trades off the
running time of processing an element with the bound on
the variance of the inverse-probability estimator.
We continue with an overview of the different compo-
nents of the sketch. As mentioned above  we represent
ğ‘“ (ğœˆğ‘¥) = â„’c[ğ‘](ğœˆğ‘¥)ğ›¾
0 + â„’c[ğ‘](ğœˆğ‘¥)âˆ
ğ›¾   and for each sum-
mand we maintain a separate sample of size ğ‘˜ (which
will later be merged). For â„’c[ğ‘](ğœˆğ‘¥)ğ›¾
0  we maintain a stan-
dard PPSWOR sketch. For â„’c[ğ‘](ğœˆğ‘¥)âˆ
ğ›¾   we build on a
result from [8]  which shows a way to map each input
element into an temporary â€œoutputâ€ element with a ran-
dom value  such that if we look at all the output elements 
E[Max(ğ‘¥)] = â„’c[ğ‘](ğœˆğ‘¥)âˆ
ğ›¾ . These components were used
in [8] to estimate the ğ‘“-statistics of the data.
However  in this work we need to produce a sample according to â„’c[ğ‘](ğœˆğ‘¥)âˆ
ğ›¾ (as opposed to
estimating the sum of these quantities for all keys). In particular  when we look at the output elements 
we only see their random value  but we are interested in producing a weighted sample according to
their expected value. For that  we introduce the analysis of PPSWOR with stochastic inputs  which
appears in Section 4. In that analysis  we establish the conditions that are needed in order for the
sample according to the random values to be close to a sample according to the expected values.
The conditions in the analysis of stochastic PPSWOR require creating ğ‘˜/ğœ€ independent output
elements for each element we see  and subsequently  the sample we need for the range â„’c[ğ‘](ğœˆğ‘¥)âˆ
ğ›¾
is a PPSWOR sample of the output elements according to the weights SumMax(ğ‘¥) (deï¬ned in
Section 5). That is the purpose of the SumMax sketch structure  which is presented in Section 5.
Each of the two samples we maintain (the PPSWOR and SumMax samples) has a ï¬xed size and
stores at most ğ‘˜ keys at any time. The ğ›¾ threshold is chosen to guarantee that we get the desired
approximation ratio. The only structure that can use more space is the Sideline structure. As part
of the analysis  we bound the size of the Sideline and show that in expectation  it is ğ‘‚(ğ‘˜) and also
provide worst case bounds on its maximum size during the run of the algorithm. The output elements
that are processed by the SumMax sketch have a value that depends on ğ›¾ (which changes as we
process the data)  and the purpose of the Sideline structure is to store elements until ğ›¾ decreases
enough that their value is ï¬xed (and then they are removed from the Sideline and processed by the
SumMax sketch).
The analysis results in the following main theorem.
Theorem 3.1. Let ğ‘˜ â‰¥ 3 and 0 < ğœ€ â‰¤ 1
of size ğ‘˜ âˆ’ 1  where each key ğ‘¥ has weight ğ‘‰ğ‘¥ that satisï¬es ğ‘“ (ğœˆğ‘¥) â‰¤ E[ğ‘‰ğ‘¥] â‰¤ 1
per-key inverse-probability estimator of ğ‘“ (ğœˆğ‘¥) is unbiased and has variance

2 . Algorithms 3 and 4 produce a stochastic PPSWOR sample
(1âˆ’ğœ€) ğ‘“ (ğœˆğ‘¥). The

else

return sample

0 ğ‘¡ğ‘(ğ‘¡)ğ‘‘ğ‘¡

[ï¸(cid:92)ğ‘“ (ğœˆğ‘¥)

]ï¸ â‰¤ 4ğ‘“ (ğœˆğ‘¥)âˆ‘ï¸€

Var

ğ‘§âˆˆğ’³ ğ‘“ (ğœˆğ‘§)

(1 âˆ’ ğœ€)2(ğ‘˜ âˆ’ 2)

.

6

The space required by the sketch at any given time is ğ‘‚(ğ‘˜) in expectation. Additionally  with proba-
bility at least 1 âˆ’ ğ›¿  the space will not exceed ğ‘‚
at any time while processing ğ·  where ğ‘š is the number of elements in ğ·  Min(ğ·) is the minimum
value of an element in ğ·  and Sum(ğ‘Š ) is the sum of frequencies of all keys.

ğ‘˜ + min{log ğ‘š  log log

ğ›¿

(ï¸ Sum(ğ‘Š )

Min(ğ·)

)ï¸} + log(ï¸€ 1

)ï¸€)ï¸

(ï¸

4 Stochastic PPSWOR Sampling

In the PPSWOR sampling scheme described in Section 2.2  the weights ğ‘¤ğ‘¥ of the keys were part
of the deterministic input to the algorithm. In this section  we consider PPSWOR sampling when
the weights are random variables. We will show that under certain assumptions  PPSWOR sampling
according to randomized inputs is close to sampling according to the expected values of these random
inputs.
Formally  let ğ’³ be a set of keys. Each key ğ‘¥ âˆˆ ğ’³ is associated with ğ‘Ÿğ‘¥ â‰¥ 0 independent random
variables ğ‘†ğ‘¥ 1  . . .   ğ‘†ğ‘¥ ğ‘Ÿğ‘¥ in the range [0  ğ‘‡ ] (for some constant ğ‘‡ > 0). The weight of key ğ‘¥ is the
ğ‘–=1 ğ‘†ğ‘¥ ğ‘–. We additionally denote its expected weight by ğ‘£ğ‘¥ := E[ğ‘†ğ‘¥]  and

random variable ğ‘†ğ‘¥ :=âˆ‘ï¸€ğ‘Ÿğ‘¥
the expected sum statistics by ğ‘‰ :=âˆ‘ï¸€

ğ‘¥ ğ‘£ğ‘¥.

A stochastic PPSWOR sample is a PPSWOR sample computed for the key-value pairs (ğ‘¥  ğ‘†ğ‘¥). That
is  we draw the random variables ğ‘†ğ‘¥  then we draw for each ğ‘¥ a random variable seed(ğ‘¥) âˆ¼ Exp[ğ‘†ğ‘¥] 
and take the ğ‘˜ keys with lowest seed values.
The following result bounds the variance of estimating ğ‘£ğ‘¥ using a stochastic PPSWOR sample. We
consider the conditional inverse-probability estimator of ğ‘£ğ‘¥. Note that even though the PPSWOR

sample was computed using the random weight ğ‘†ğ‘¥  the estimator Ì‚ï¸€ğ‘£ğ‘¥ is computed using ğ‘£ğ‘¥ and will

be
Pr[seed(ğ‘¥)<ğœ ] for keys ğ‘¥ in the sample. It sufï¬ces to bound the per-key variance and relate it to
the per-key variance bound for a PPSWOR sample computed directly for ğ‘£ğ‘¥. We show that when
ğ‘‰ â‰¥ ğ‘‡ ğ‘˜  the overhead due to the stochastic sample is at most 4 (that is  the variance grows by a
multiplicative factor of 4). The proof details would also reveal that when ğ‘‰ â‰« ğ‘‡ ğ‘˜  the worst-case
bound on the overhead is actually closer to 2.
Theorem 4.1. Let ğ‘˜ â‰¥ 3. In a stochastic PPSWOR sample  if ğ‘‰ â‰¥ ğ‘‡ ğ‘˜  then for every key ğ‘¥ âˆˆ ğ’³  
the variance Var [Ë†ğ‘£ğ‘¥] of the bottom-ğ‘˜ inverse probability estimator of ğ‘£ğ‘¥ is bounded by

ğ‘£ğ‘¥

Var [Ë†ğ‘£ğ‘¥] â‰¤ 4ğ‘£ğ‘¥ğ‘‰
ğ‘˜ âˆ’ 2

.

5 SumMax Sampling Sketch

We present an auxiliary sketch that processes elements ğ‘’ = (ğ‘’.key  ğ‘’.val ) with keys ğ‘’.key =
(ğ‘’.key.ğ‘  ğ‘’.key.ğ‘ ) that are structured to have a primary key ğ‘’.key.ğ‘ and a secondary key ğ‘’.key.ğ‘ . For
each primary key ğ‘¥  we deï¬ne

SumMaxğ·(ğ‘¥) :=

Maxğ·(ğ‘§)

âˆ‘ï¸

ğ‘§|ğ‘§.ğ‘=ğ‘¥

where Max is as deï¬ned in Section 2. If there are no elements ğ‘’ âˆˆ ğ· such that ğ‘’.key.ğ‘ = ğ‘¥  then
by deï¬nition Maxğ·(ğ‘§) = 0 for all ğ‘§ with ğ‘§.ğ‘ = ğ‘¥ (as there are no elements in ğ· with key ğ‘§) and
therefore SumMaxğ·(ğ‘¥) = 0. The SumMax sampling sketch (Algorithm 5) produces a PPSWOR
sample of primary keys ğ‘¥ according to weights SumMaxğ·(ğ‘¥). Note that while the key space of the
input elements contains structured keys of the form ğ‘’.key = (ğ‘’.key.ğ‘  ğ‘’.key.ğ‘ )  the key space for the
output sample will be the space of primary keys only. The sketch structure consists of a bottom-ğ‘˜
structure and a hash function â„. We assume we have a perfectly random hash function â„ such that
for every key ğ‘§ = (ğ‘§.ğ‘  ğ‘§.ğ‘ )  â„(ğ‘§) âˆ¼ Exp[1] independently (in practice  we assume that the hash
function is provided by the platform on which we run). We process an input element ğ‘’ by generating
a new data element with key ğ‘’.key.ğ‘ (the primary key of the key of the input element) and value

ElementScore(ğ‘’) := â„(ğ‘’.key)/ğ‘’.val

and then processing that element by our bottom-ğ‘˜ structure. The bottom-ğ‘˜ structure holds our current
sample of primary keys. By deï¬nition  the bottom-ğ‘˜ structure retains the ğ‘˜ primary keys ğ‘¥ with

7

Algorithm 5: SumMax sampling sketch
// Initialize empty structure ğ‘ 
Input: Sample size ğ‘˜
ğ‘ .â„ â† independent random hash with range Exp[1]
Initialize ğ‘ .sample // A bottom-ğ‘˜ structure (Algorithm 1)
// Process element ğ‘’ = (ğ‘’.key  ğ‘’.val ) where ğ‘’.key = (ğ‘’.key.ğ‘  ğ‘’.key.ğ‘ )
Process element (ğ‘’.key.ğ‘  ğ‘ .â„(ğ‘’.key)/ğ‘’.val ) to structure ğ‘ .sample // bottom-ğ‘˜ process element
// Merge structures ğ‘ 1  ğ‘ 2 (with ğ‘ 1.â„ = ğ‘ 2.â„) to get ğ‘ 
ğ‘ .â„ â† ğ‘ 1.â„ // ğ‘ 1.â„ = ğ‘ 2.â„
ğ‘ .sample â† Merge ğ‘ 1.sample  ğ‘ 2.sample// bottom-ğ‘˜ merge (Algorithm 1)

minimum

seedğ·(ğ‘¥) :=

min

ğ‘’âˆˆğ·|ğ‘’.key.ğ‘=ğ‘¥

ElementScore(ğ‘’) .

To establish that this is a PPSWOR sample according to SumMaxğ·(ğ‘¥)  we study the distribution of
seedğ·(ğ‘¥).
Lemma 5.1. For all primary keys ğ‘¥ that appear in elements of ğ·  seedğ·(ğ‘¥) âˆ¼
Exp[SumMaxğ·(ğ‘¥))]. The random variables seedğ·(ğ‘¥) are independent.

Note that the distribution of seedğ·(ğ‘¥)  which is Exp[SumMaxğ·(ğ‘¥)]  does not depend on the
particular structure of ğ· or the order in which elements are processed  but only on the parameter
SumMaxğ·(ğ‘¥). The bottom-ğ‘˜ sketch structure maintains the ğ‘˜ primary keys with smallest seedğ·(ğ‘¥)
values. We therefore get the following corollary.
Corollary 5.2. Given a stream or distributed set of elements ğ·  the sampling sketch Algorithm 5
produces a PPSWOR sample according to the weights SumMaxğ·(ğ‘¥).

6 Experiments

We implemented our sampling sketch in Python and report here the results of experiments on real
and synthetic datasets. The implementation follows the pseudocode except that we incorporated
two practical optimizations: removing redundant keys from the PPSWOR subsketch and removing
redundant elements from Sideline. These optimizations do not affect the outcome of the computation
or the worst-case analysis  but reduce the sketch size in practice. We used the following datasets:

tag  and the value is the number of times it appeared in a certain folder.

âˆ™ abcnews [23]: News headlines. For each word  we created an element with value 1.
âˆ™ flicker [31]: Tags used by Flickr users to annotate images. The key of each element is a
âˆ™ Three synthetic generated datasets that contain 2 Ã— 106 data elements. Each element has
value 1  and the key was chosen according to the Zipf distribution (numpy.random.zipf) 
with Zipf parameter values ğ›¼ âˆˆ {1.1  1.2  1.5}. The Zipf family in this range is often a good
model to real-world frequency distributions.

repetitions where we used the ï¬nal sample to estimate the sumâˆ‘ï¸€

We applied our sampling sketch with sample size parameter values ğ‘˜ âˆˆ {25  50  75  100} and set the
parameter ğœ€ = 0.5 in all experiments. We sampled according to two concave sublinear functions: the
frequency moment ğ‘“ (ğœˆ) = ğœˆ0.5 and ğ‘“ (ğœˆ) = ln(1 + ğœˆ). Tables 1 reports aggregated results of 200
âˆš
ğ‘¥âˆˆğ’³ ğ‘“ (ğœˆğ‘¥). For error bounds  we
list the worst-case bound on the CV (which depends only on ğ‘˜ and ğœ€ and is âˆ 1/
ğ‘˜) and report the
actual normalized root of the average squared error (NRMSE). In addition  we report the NRMSE that
we got from 200 repetitions of estimating the same statistics using two common sampling schemes
for aggregated data  PPSWOR and priority sampling  which we use as benchmarks. We also consider
the size of the sketch after processing each element. Since the representation of each key can be
explicit and require a lot of space  we separately consider the number of distinct keys and the number
of elements stored in the sketch. We report the maximum number of distinct keys stored in the sketch
at any point (the average and the maximum over the 200 repetitions) and the respective maximum
number of elements stored in the sketch at any point during the computations (again  the average
and the maximum over the 200 repetitions). We can see that the actual error reported is signiï¬cantly

8

Table 1: Experimental Results: ğ‘“ (ğœˆ) = ğœˆ0.5   ln(1 + ğœˆ)
max #elem
ave

Benchmark
Pri.

max #keys
ave

NRMSE

bound

actual

ppswor

max

ğ‘˜

max

Dataset: abcnews (7.07 Ã— 106 elements  91.7 Ã— 103 keys)

ğ‘“ (ğœˆ) = ğœˆ0.5  200 reps

25
50
75
100

25
50
75
100

25
50
75
100

25
50
75
100

25
50
75
100

25
50
75
100

25
50
75
100

25
50
75
100

25
50
75
100

25
50
75
100

0.213
0.142
0.120
0.105

0.200
0.144
0.123
0.115

0.215
0.123
0.109
0.106

0.199
0.144
0.122
0.098

0.201
0.152
0.115
0.098

0.208
0.138
0.130
0.102

0.227
0.144
0.119
0.097

0.201
0.127
0.116
0.107

0.209
0.147
0.120
0.098

0.213
0.128
0.111
0.098

0.190
0.147
0.114
0.095

0.198
0.137
0.115
0.103

0.208
0.138
0.116
0.109

0.217
0.136
0.099
0.115

0.199
0.151
0.121
0.104

0.204
0.132
0.122
0.106

0.195
0.144
0.111
0.106

0.217
0.137
0.110
0.103

0.208
0.142
0.110
0.099

0.217
0.131
0.114
0.097

0.214
0.145
0.124
0.096

0.194
0.142
0.117
0.103

0.180
0.129
0.109
0.095

0.234
0.129
0.110
0.104

0.218
0.139
0.113
0.102

31.7
58.5
85.4
111.2

31.2
57.8
83.7
108.9

31.8
58.7
84.7
111.2

31.1
57.9
83.9
109.6

29.5
54.9
80.0
104.9

28.0
53.3
78.2
102.7

29.2
54.4
79.6
104.5

28.5
53.7
78.8
103.9

37
66
94
120

37
64
91
116

39
66
91
119

38
65
90
115

35
60
86
113

34
60
85
109

31
59
83
106

34
58
84
109

33
57
84
108

0.834
0.577
0.468
0.404

0.834
0.577
0.468
0.404

0.834
0.577
0.468
0.404

0.834
0.577
0.468
0.404

0.834
0.577
0.468
0.404

0.834
0.577
0.468
0.404

0.834
0.577
0.468
0.404

0.834
0.577
0.468
0.404

0.834
0.577
0.468
0.404

0.834
0.577
0.468
0.404

Dataset: flickr (7.64 Ã— 106 elements  572.4 Ã— 103 keys)

Dataset: zipf1.1 (2.00 Ã— 106 elements  652.2 Ã— 103 keys)

Dataset: zipf1.2 (2.00 Ã— 106 elements  237.3 Ã— 103 keys)

Dataset: zipf1.5 (2.00 Ã— 106 elements  22.3 Ã— 103 keys)

0.207
0.139
0.115
0.094
ğ‘“ (ğœˆ) = ln(1 + ğœˆ)  200 reps

0.194
0.142
0.112
0.086

30.1
56.1
81.6
107.1

Dataset: abcnews (7.07 Ã— 106 elements  91.7 Ã— 103 keys)

Dataset: flickr (7.64 Ã— 106 elements  572.4 Ã— 103 keys)

Dataset: zipf1.1 (2.00 Ã— 106 elements  652.2 Ã— 103 keys)

Dataset: zipf1.2 (2.00 Ã— 106 elements  237.3 Ã— 103 keys)

Dataset: zipf1.5 (2.00 Ã— 106 elements  22.3 Ã— 103 keys)

0.210
0.141
0.124
0.100

0.197
0.146
0.112
0.101

0.226
0.149
0.106
0.099

27.2
52.1
76.9
101.9

30
55
79
104

50.9
95.1
134.8
171.1

53.1
94.6
131.7
173.4

52.5
95.0
135.2
176.3

53.2
98.4
138.2
179.2

53.4
101.5
151.8
196.3

49.1
80.9
111.1
140.7

41.4
72.2
99.8
130.3

48.8
80.4
110.9
139.8

48.0
80.5
111.4
140.3

45.2
78.9
110.5
139.1

76
136
181
256

77
130
175
223

75
130
186
221

83
139
173
227

74
136
199
248

71
110
152
184

69
101
135
166

71
119
142
165

72
113
143
173

66
104
146
173

lower than the worst-case bound. Furthermore  the error that our sketch gets is close to the error
achieved by the two benchmark sampling schemes. We can also see that the maximum number of
distinct keys stored in the sketch at any time is relatively close to the speciï¬ed sample size of ğ‘˜ and
that the total sketch size in terms of elements rarely exceeded 3ğ‘˜  with the relative excess seeming to
decrease with ğ‘˜. In comparison  the benchmark schemes require space that is the number of distinct
keys (for the aggregation)  which is signiï¬cantly higher than the space required by our sketch.

7 Conclusion

We presented composable sampling sketches for weighted sampling of unaggregated data tailored
to a concave sublinear function of the frequencies of keys. We experimentally demonstrated the
simplicity and efï¬cacy of our design: Our sketch size is nearly optimal in that it is not much larger
than the ï¬nal sample size  and the estimate quality is close to that provided by a weighted sample
computed directly over the aggregated data.

9

Acknowledgments

Oï¬r Geri was supported by NSF grant CCF-1617577  a Simons Investigator Award for Moses
Charikar  and the Google Graduate Fellowship in Computer Science in the School of Engineering
at Stanford University. The computing for this project was performed on the Sherlock cluster. We
would like to thank Stanford University and the Stanford Research Computing Center for providing
computational resources and support that contributed to these research results.

References
[1] N. Alon  Y. Matias  and M. Szegedy. The space complexity of approximating the frequency

moments. J. Comput. System Sci.  58:137â€“147  1999.

[2] A. Andoni  R. Krauthgamer  and K. Onak. Streaming algorithms via precision sampling. In
2011 IEEE 52nd Annual Symposium on Foundations of Computer Science  pages 363â€“372  Oct
2011.

[3] V. Braverman  S. R. Chestnut  D. P. Woodruff  and L. F. Yang. Streaming space complexity of

nearly all functions of one variable on frequency vectors. In PODS. ACM  2016.

[4] V. Braverman  R. Krauthgamer  and L. F. Yang. Universal streaming of subset norms. CoRR 

abs/1812.00241  2018.

[5] V. Braverman and R. Ostrovsky. Zero-one frequency laws. In STOC. ACM  2010.

[6] M. T. Chao. A general purpose unequal probability sampling plan. Biometrika  69(3):653â€“656 

1982.

[7] E. Cohen. Stream sampling for frequency cap statistics. In KDD. ACM  2015. full version:

http://arxiv.org/abs/1502.05955.

[8] E. Cohen. Hyperloglog hyperextended: Sketches for concave sublinear frequency statistics. In

KDD. ACM  2017. full version: https://arxiv.org/abs/1607.06517.

[9] E. Cohen  G. Cormode  and N. Dufï¬eld. Donâ€™t let the negatives bring you down: Sampling

from streams of signed updates. In Proc. ACM SIGMETRICS/Performance  2012.

[10] E. Cohen  N. Dufï¬eld  H. Kaplan  C. Lund  and M. Thorup. Efï¬cient stream sampling for

variance-optimal estimation of subset sums. SIAM J. Comput.  40(5)  2011.

[11] E. Cohen  N. Dufï¬eld  H. Kaplan  C. Lund  and M. Thorup. Algorithms and estimators for

accurate summarization of unaggregated data streams. J. Comput. System Sci.  80  2014.

[12] E. Cohen and H. Kaplan. Summarizing data using bottom-k sketches. In ACM PODC  2007.

[13] N. Dufï¬eld  M. Thorup  and C. Lund. Priority sampling for estimating arbitrary subset sums. J.

Assoc. Comput. Mach.  54(6)  2007.

[14] C. Estan and G. Varghese. New directions in trafï¬c measurement and accounting. In SIGCOMM.

ACM  2002.

[15] P. Flajolet  E. Fusy  O. Gandouet  and F. Meunier. Hyperloglog: The analysis of a near-optimal

cardinality estimation algorithm. In Analysis of Algorithms (AofA). DMTCS  2007.

[16] P. Flajolet and G. N. Martin. Probabilistic counting algorithms for data base applications. J.

Comput. System Sci.  31:182â€“209  1985.

[17] G. Frahling  P. Indyk  and C. Sohler. Sampling in dynamic data streams and applications.

International Journal of Computational Geometry & Applications  18(01n02):3â€“28  2008.

[18] P. Gibbons and Y. Matias. New sampling-based summary statistics for improving approximate

query answers. In SIGMOD. ACM  1998.

[19] Google. Frequency capping: AdWords help  December 2014. https://support.google.

com/adwords/answer/117579.

10

[20] R. Jayaram and D. P. Woodruff. Perfect lp sampling in a data stream. In 2018 IEEE 59th Annual

Symposium on Foundations of Computer Science (FOCS)  pages 544â€“555  Oct 2018.

[21] H. Jowhari  M. SaË˜glam  and G. Tardos. Tight bounds for lp samplers  ï¬nding duplicates in
streams  and related problems. In Proceedings of the Thirtieth ACM SIGMOD-SIGACT-SIGART
Symposium on Principles of Database Systems  PODS â€™11  pages 49â€“58  2011.

[22] D. E. Knuth. The Art of Computer Programming  Vol 2  Seminumerical Algorithms. Addison-

Wesley  1st edition  1968.

[23] R. Kulkarni. A million news headlines [csv data ï¬le]. https://www.kaggle.com/therohk/

million-headlines/home  2017.

[24] B. McMahan  E. Moore  D. Ramage  S. Hampson  and B. Aguera y Arcas. Communication-
Efï¬cient Learning of Deep Networks from Decentralized Data. In Aarti Singh and Jerry Zhu 
editors  Proceedings of the 20th International Conference on Artiï¬cial Intelligence and Statistics 
volume 54 of Proceedings of Machine Learning Research  pages 1273â€“1282. PMLR  2017.

[25] T. Mikolov  I. Sutskever  K. Chen  G. S. Corrado  and J. Dean. Distributed representations
of words and phrases and their compositionality. In Proceedings of the 26th International
Conference on Neural Information Processing Systems - Volume 2  NIPSâ€™13  pages 3111â€“3119 
2013.

[26] J. Misra and D. Gries. Finding repeated elements. Technical report  Cornell University  1982.

[27] M. Monemizadeh and D. P. Woodruff. 1-pass relative-error lp-sampling with applications. In

Proc. 21st ACM-SIAM Symposium on Discrete Algorithms. ACM-SIAM  2010.

[28] E. Ohlsson. Sequential poisson sampling. J. Ofï¬cial Statistics  14(2):149â€“162  1998.

[29] M. Osborne. Facebook Reach and Frequency Buying  October 2014. http://citizennet.

com/blog/2014/10/01/facebook-reach-and-frequency-buying/.

[30] J. Pennington  R. Socher  and C. D. Manning. GloVe: Global vectors for word representation.

In EMNLP  2014.

[31] A. Plangprasopchok  K. Lerman  and L. Getoor. Growing a tree in the forest: Constructing
folksonomies by integrating structured metadata. In Proceedings of the 16th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining  KDD â€™10  pages 949â€“958 
2010.

[32] B. RosÃ©n. Asymptotic theory for successive sampling with varying probabilities without

replacement  I. The Annals of Mathematical Statistics  43(2):373â€“397  1972.

[33] B. RosÃ©n. Asymptotic theory for order sampling. J. Statistical Planning and Inference 

62(2):135â€“158  1997.

[34] J.S. Vitter. Random sampling with a reservoir. ACM Trans. Math. Softw.  11(1):37â€“57  1985.

11

,Arun Venkatraman
Nicholas Rhinehart
Wen Sun
Martial Hebert
Byron Boots
Kris Kitani
J. Bagnell
Edith Cohen
Ofir Geri