2015,Column Selection via Adaptive Sampling,Selecting a good column (or row) subset of massive data matrices has found many applications in data analysis and machine learning. We propose a new adaptive sampling algorithm that can be used to improve any relative-error column selection algorithm.  Our algorithm delivers a tighter theoretical bound on the approximation error which we also demonstrate empirically using two well known relative-error column subset selection algorithms. Our experimental results on synthetic and real-world data show that our algorithm outperforms non-adaptive sampling as well as prior adaptive sampling approaches.,Column Selection via Adaptive Sampling

Saurabh Paul

Global Risk Sciences  Paypal Inc.

saupaul@paypal.com

Malik Magdon-Ismail

CS Dept.  Rensselaer Polytechnic Institute

magdon@cs.rpi.edu

Petros Drineas

CS Dept.  Rensselaer Polytechnic Institute

drinep@cs.rpi.edu

Abstract

Selecting a good column (or row) subset of massive data matrices has found many
applications in data analysis and machine learning. We propose a new adap-
tive sampling algorithm that can be used to improve any relative-error column
selection algorithm. Our algorithm delivers a tighter theoretical bound on the ap-
proximation error which we also demonstrate empirically using two well known
relative-error column subset selection algorithms. Our experimental results on
synthetic and real-world data show that our algorithm outperforms non-adaptive
sampling as well as prior adaptive sampling approaches.

1

Introduction

In numerous machine learning and data analysis applications  the input data are modelled as a matrix
A ∈ Rm×n  where m is the number of objects (data points) and n is the number of features. Often 
it is desirable to represent your solution using a few features (to promote better generalization and
interpretability of the solutions)  or using a few data points (to identify important coresets of the
data)  for example PCA  sparse PCA  sparse regression  coreset based regression  etc. [1  2  3  4].
These problems can be reduced to identifying a good subset of the columns (or rows) in the data
matrix  the column subset selection problem (CSSP). For example  ﬁnding an optimal sparse linear
encoder for the data (dimension reduction) can be explicitly reduced to CSSP [5]. Motivated by the
fact that in many practical applications  the left and right singular vectors of a matrix A lacks any
physical interpretation  a long line of work [6  7  8  9  10  11  12  13  14  15]  focused on extracting
a subset of columns of the matrix A  which are approximately as good as Ak at reconstructing A.
To make our discussion more concrete  let us formally deﬁne CSSP.
Column Subset Selection Problem  CSSP: Find a matrix C ∈ Rm×c containing c columns of

A for which (cid:13)(cid:13)A − CC+A(cid:13)(cid:13)F is small.1 In the prior work  one measures the quality of a CSSP-
algorithms to ﬁnd C with c ≈ 2k/ columns  for which (cid:13)(cid:13)A − CC+A(cid:13)(cid:13)F ≤ (1 + )(cid:107)A − Ak(cid:107)F .

solution against Ak  the best rank-k approximation to A obtained via the singular value decompo-
sition (SVD)  where k is a user speciﬁed target rank parameter. For example  [15] gives efﬁcient

Our contribution is not to directly attack CSSP. We present a novel algorithm that can improve an
existing CSSP algorithm by adaptively invoking it  in a sense actively learning which columns to
sample next based on the columns you have already sampled. If you use the CSSP-algorithm from
[15] as a strawman benchmark  you can obtain c columns all at once and incur an error roughly
(1 + 2k/c)(cid:107)A − Ak(cid:107)F . Or  you can invoke the algorithm to obtain  for example  c/2 columns 
and then allow the algorithm to adapt to the columns already chosen (for example by modifying A)
before choosing the remaining c/2 columns. We refer to the former as continued sampling and to the

1CC+A is the best possible reconstruction of A by projection into the space spanned by the columns of C.

1

latter as adaptive sampling. We prove performance guarantees which show that adaptive sampling
improves upon continued sampling  and we present experiments on synthetic and real data that
demonstrate signiﬁcant empirical performance gains.

1.1 Notation
A  B  . . . denote matrices and a  b  . . . denote column vectors; In is the n × n identity matrix.
[A  B] and [A; B] denote matrix concatenation operations in a column-wise and row-wise manner 
respectively. Given a set S ⊆ {1  . . . n}  AS is the matrix that contains the columns of A ∈ Rm×n
indexed by S. Let rank(A) = ρ ≤ min{m  n}. The (economy) SVD of A is

(cid:18) Σk

A = (Uk Uρ−k)

0

0 Σρ−k

(cid:19)(cid:18) VT

k
VT
ρ−k

(cid:19)

ρ(cid:88)

i=1

=

σi(A)uivT
i

(cid:80)

where Uk ∈ Rm×k and Uρ−k ∈ Rm×(ρ−k) contain the left singular vectors ui  Vk ∈ Rn×k
and Vρ−k ∈ Rn×(ρ−k) contain the right singular vectors vi  and Σ ∈ Rρ×ρ is a diagonal matrix
containing the singular values σ1(A) ≥ . . . ≥ σρ(A) > 0. The Frobenius norm of A is (cid:107)A(cid:107)2
F =
ij; Tr(A) is the trace of A; the pseudoinverse of A is A+ = VΣ−1UT ; and  Ak  the best
i=1 σiuivT
i .

rank-k approximation to A under any unitarily invariant norm is Ak = UkΣkVT

k =(cid:80)k

i j A2

1.2 Our Contribution: Adaptive Sampling

We design a novel CSSP-algorithm that adaptively selects columns from the matrix A in rounds. In
each round we remove from A the information that has already been “captured” by the columns that
have been thus far selected. Algorithm 1 selects tc columns of A in t rounds  where in each round
c columns of A are selected using a relative-error CSSP-algorithm from prior work.

Input: A ∈ Rm×n; target rank k; # rounds t; columns per round c
Output: C ∈ Rm×tc  tc columns of A and S  the indices of those columns.
1: S = {}; E0 = A
2: for (cid:96) = 1 ···   t do
3:
4:
5:
6: return C  S

Sample indices S(cid:96) of c columns from E(cid:96)−1 using a CSSP-algorithm.
S ← S ∪ S(cid:96).
Set C = AS and E(cid:96) = A − (CC+A)(cid:96)k.

Algorithm 1: Adaptive Sampling

At round (cid:96) in Step 3  we compute column indices S (and C = AS) using a CSSP-algorithm on the
residual E(cid:96)−1 of the previous round. To compute this residual  remove from A the best rank-((cid:96)−1)k
approximation to A in the span of the columns selected from the ﬁrst (cid:96) − 1 rounds 

E(cid:96)−1 = A − (CC+A)((cid:96)−1)k.

A similar strategy was developed in [8] with sequential adaptive use of (additive error) CSSP-
algorithms. These (additive error) CSSP-algorithms select columns according to column norms [11].
In [8]  the residual in step 5 is deﬁned differently  as E(cid:96) = A − CC+A. To motivate our result  it
helps to take a closer look at the reconstruction error E = A − CC+A after t adaptive rounds of
the strategy in [8] with the CSSP-algorithm in [11].

# rounds Continued sampling: tc columns using

t = 2

t

CSSP-algorithm from [11]. ( = k/c)
(cid:107)A(cid:107)2
(cid:107)A(cid:107)2

F ≤ (cid:107)A − Ak(cid:107)2
F ≤ (cid:107)A − Ak(cid:107)2

(cid:107)E(cid:107)2
(cid:107)E(cid:107)2

F +

F +

F

F


2

t

Adaptive sampling: t rounds of the strategy
in [8] with the CSSP-algorithm from [11].
F + 2 (cid:107)A(cid:107)2
F +t (cid:107)A(cid:107)2

F ≤ (1 + )(cid:107)A − Ak(cid:107)2
F ≤ (1 + O()) (cid:107)A − Ak(cid:107)2

(cid:107)E(cid:107)2
(cid:107)E(cid:107)2

F

F

2

F (cid:29) (cid:107)A − Ak(cid:107)2

Typically (cid:107)A(cid:107)2
F and  is small (i.e.  c (cid:29) k)  so adaptive sampling `a la [8] wins
over continued sampling for additive error CSSP-algorithms. This is especially apparent after t
rounds  where continued sampling only attenuates the big term (cid:107)A(cid:107)2
F by /t  but adaptive sampling
exponentially attenuates this term by t.
Recently  powerful CSSP-algorithms have been developed which give relative-error guarantees [15].
We can use the adaptive strategy from [8] together with these newer relative error CSSP-algorithms.
If one carries out the analysis from [8] by replacing the additive error CSSP-algorithm from [11]
with the relative error CSSP-algorithm in [15]  the comparison of continued and adaptive sampling
using the strategy from [8] becomes (t = 2 rounds sufﬁces to see the problem):

# rounds Continued sampling: tc columns using
CSSP-algorithm from [15]. ( = 2k/c)

F ≤(cid:16)

(cid:107)E(cid:107)2

1 +


2

t = 2

(cid:17) (cid:107)A − Ak(cid:107)2

F

Adaptive sampling: t rounds of the strategy
in [8] with the CSSP-algorithm from [15].
(cid:107)A − Ak(cid:107)2

(cid:107)E(cid:107)2

1 +

+

(cid:18)

(cid:19)

F ≤

F


2

2
2

Adaptive sampling from [8] gives a worse theoretical guarantee than continued sampling for relative
error CSSP-algorithms. In a nutshell  no matter how many rounds of adaptive sampling you do 
the theoretical bound will not be better than (1 + k/c)(cid:107)A − Ak(cid:107)2
F if you are using a relative error
CSSP-algorithm. This raises an obvious question: is it possible to combine relative-error CSSP-
algorithms with adaptive sampling to get (provably and empirically) improved CSSP-algorithms?
The approach of [8] does not achieve this objective. We provide a positive answer to this question.
Our approach is a subtle modiﬁcation to the approach in [8]: in Step 5 of Algorithm 1. When we
compute the residual matrix in round (cid:96)  we subtract (CC+A)(cid:96)k from A  the best rank-(cid:96)k approxi-
mation to the projection of A onto the current columns selected  as opposed to subtracting the full
projection CC+A. This subtle change  is critical in our new analysis which gives a tighter bound
on the ﬁnal error  allowing us to boost relative-error CSSP-algorithms. For t = 2 rounds of adaptive
sampling  we get a reconstruction error of

(cid:107)E(cid:107)2

F ≤ (1 + )(cid:107)A − A2k(cid:107)2

F + (1 + )(cid:107)A − Ak(cid:107)2
F  

F   and the dependence on (cid:107)A − Ak(cid:107)2

where  = 2k/c. The critical improvement in the bound is that the dominant O(1)-term depends on
(cid:107)A − A2k(cid:107)2
F is now O(). To highlight this improved theoret-
ical bound in an extreme case  consider a matrix A that has rank exactly 2k  then (cid:107)A − A2k(cid:107)F = 0.
F   where as our adaptive sampling gives
Continued sampling gives an error-bound (1+ 
an error-bound ( + 2)(cid:107)A − Ak(cid:107)2
F   which is clearly better in this extreme case. In practice  data
matrices have rapidly decaying singular values  so this extreme case is not far from reality (See
Figure 1).

2 )(cid:107)A − Ak(cid:107)2

Figure 1: Figure showing the singular value decay for two real world datasets.

To state our main theoretical result  we need to more formally deﬁne a relative error CSSP-algorithm.
Deﬁnition 1 (Relative Error CSSP-algorithm A(X  k  c)). A relative error CSSP-algorithm A takes
as input a matrix X  a rank parameter k < rank(X) and a number of columns c  and outputs column
indices S with |S| = c  so that the columns C = XS satisfy:

(cid:104)(cid:107)X − (CC+X)k(cid:107)2

(cid:105) ≤ (1 + (c  k))(cid:107)X − Xk(cid:107)2

EC

F  

F

3

02040608010002004006008001000Singular Values of TechTC−300 avgd over 49 datasetsSingular Values0200400600800100012000200400600800Singular Values of HGDP avgd over 22 chromosomesSingular Valueswhere (c  k) depends on A and the expectation is over random choices made in the algorithm.2

Our main theorem bounds the reconstruction error when our adaptive sampling approach is used to
boost A. The boost in performance depends on the decay of the spectrum of A.
Theorem 1. Let A ∈ Rm×n be a matrix of rank ρ and let k < ρ be a target rank. If  in Step 3 of
Algorithm 1  we use the relative error CSSP-algorithm A with (c  k) =  < 1  then

(cid:104)(cid:107)A − (CC+A)tk(cid:107)2

(cid:105) ≤ (1 + )(cid:107)A − Atk(cid:107)2

F

EC

F + 

(1 + )t−i (cid:107)A − Aik(cid:107)2
F .

t−1(cid:88)

i=1

t−1(cid:88)

Comments.
1. The dominant O(1) term in our bound is (cid:107)A − Atk(cid:107)F   not (cid:107)A − Ak(cid:107)F . This is a major im-
provement since the former is typically much smaller than the latter in real data. Further  we
need a bound on the reconstruction error (cid:107)A − CC+A(cid:107)F . Our theorem give a stronger result
than needed because (cid:107)A − CC+A(cid:107)F ≤ (cid:107)A − (CC+A)tk(cid:107)F .
2. We presented our result for the case of a relative error CSSP-algorithm with a guarantee on the
expected reconstruction error. Clearly  if the CSSP-algorithm is deterministic  then Theorem 1
will also hold deterministically. The result in Theorem 1 can also be boosted to hold with high
probability  by repeating the process log 1
δ times and picking the columns which performed best.
Then  with probability at least 1 − δ 

(cid:107)A − (CC+A)tk(cid:107)2

F ≤ (1 + 2)(cid:107)A − Atk(cid:107)2

F + 2

(1 + )t−i (cid:107)A − Aik(cid:107)2
F .

i=1

If the CSSP-algorithm itself only gives a high-probability (at least 1−δ) guarantee  then the bound
in Theorem 1 also holds with high probability  at least 1 − tδ  which is obtained by applying a
union bound to the probability of failure in each round.
3. Our results hold for any relative error CSSP-algorithm combined with our adaptive sampling
strategy. The relative error CSSP-algorithm in [15] has (c  k) ≈ 2k/c. The relative error CSSP-
algorithm in [16] has (c  k) = O(k log k/c). Other algorithms can be found in [8  9  17].
We presented the simplest form of the result  which can be generalized to sample a different
number of columns in each round  or even use a different CSSP-algorithm in each round. We
have not optimized the sampling schedule  i.e. how many columns to sample in each round. At
the moment  this is largely dictated by the CSSP algorithm itself  which requires a minimum
number of samples in each round to give a theoretical guarantee. From the empirical perspective
(for example using leverage score sampling to select columns)  strongest performance may be
obtained by adapting after every column is selected.

4. In the context of the additive error CSSP-algorithm from [11]  our adaptive sampling strategy
gives a theoretical performance guarantee which is at least as good as the adaptive sampling
strategy from [8].

Lastly  we also provide the ﬁrst empirical evaluation of adaptive sampling algorithms. We imple-
mented our algorithm using two relative-error column selection algorithms (the near-optimal column
selection algorithm of [18  15] and the leverage-score sampling algorithm of [19]) and compared it
against the adaptive sampling algorithm of [8] on synthetic and real-world data. The experimental
results show that our algorithm outperforms prior approaches.

1.3 Related Work

Column selection algorithms have been extensively studied in prior literature. Such algorithms
include rank-revealing QR factorizations [6  20] for which only weak performance guarantees can
be derived. The QR approach was improved in [21] where the authors proposed a memory efﬁcient
implementation. The randomized additive error CSSP-algorithm [11] was a breakthrough  which led
to a series of improvements producing relative CSSP-algorithms using a variety of randomized and

(cid:104)(cid:107)X − (CC+X)k(cid:107)2

(cid:105) ≤ (cid:107)X − Xk(cid:107)2

F

F + (c  k)(cid:107)X(cid:107)2
F .

2For an additive-error CSSP algorithm  EC

4

deterministic techniques. These include leverage score sampling [19  16]  volume sampling [8  9 
17]  the two-stage hybrid sampling approach of [22]  the near-optimal column selection algorithms
of [18  15]  as well as deterministic variants presented in [23]. We refer the reader to Section 1.5
of [15] for a detailed overview of prior work. Our focus is not on CSSP-algorithms per se  but rather
on adaptively invoking existing CSSP-algorithms. The only prior adaptive sampling with a provable
guarantee was introduced in [8] and further analyzed in [24  9  25]; this strategy is speciﬁcally boosts
the additive error CSSP-algorithm  but does not work with relative error CSSP-algorithms which are
currently in use. Our modiﬁcation of the approach in [8] is delicate  but crucial to the new analysis
we perform in the context of relative error CSSP-algorithms.
Our work is motivated by relative error CSSP-algorithms satisfying deﬁnition 1. Such algorithms
exist which give expected guarantees [15] as well as high probability guarantees [19]. Speciﬁcally 
given X ∈ Rm×n and a target rank k  the leverage-score sampling approach of [19] selects c =
probability at least 1 − δ. Similarly  [18  15] proposed near-optimal relative error CSSP-algorithms
selecting c ≈ 2c/ columns and giving a (1 + )-relative error guarantee in expectation  which can
be boosted to a high probability guarantee via independent repetition.

O(cid:0)(cid:0)k/2(cid:1) log(cid:0)k/2(cid:1)(cid:1) columns of A to form a matrix C ∈ Rm×c to give a (1+)-relative error with

2 Proof of Theorem 1

We now prove the main result which analyzes the performance of our adaptive sampling in Algo-
rithm 1 for a relative error CSSP-algorithm. We will need the following linear algebraic Lemma.
Lemma 1. Let X  Y ∈ Rm×n and suppose that rank(Y) = r. Then 

σi(X − Y) ≥ σr+i(X).

Proof. Observe that σi(X− Y) = (cid:107)(X − Y) − (X − Y)i−1(cid:107)2. The claim is now immediate from
the Eckart-Young theorem because Y + (X − Y)i−1 has rank at most r + i − 1  therefore

σi(X − Y) = (cid:107)X − (Y + (X − Y)i−1)(cid:107)2 ≥ (cid:107)X − Xr+i−1(cid:107)2 = σr+i(X).

We are now ready to prove Theorem 1 by induction on t  the number of rounds of adaptive sampling.
When t = 1  the claim is that

E(cid:104)(cid:107)A − (CC+A)k(cid:107)2

(cid:105) ≤ (1 + )(cid:107)A − Ak(cid:107)2

F  

F

which is immediate from the deﬁnition of the relative error CSSP-algorithm. Now for the induction.
Suppose that after t rounds  columns Ct are selected  and we have the induction hypothesis that

(cid:104)(cid:107)A − (CtCt+A)tk(cid:107)2

(cid:105) ≤ (1 + )(cid:107)A − Atk(cid:107)2

F

ECt

F + 

(1 + )t−i (cid:107)A − Aik(cid:107)2
F .

(1)

t−1(cid:88)

i=1

In the (t + 1)th round  we use the residual Et = A − (CtCt+A)tk to select new columns C(cid:48). Our
relative error CSSP-algorithm A gives the following guarantee:

(cid:104)(cid:107)Et − (C(cid:48)C(cid:48)+Et)k(cid:107)2

F

EC(cid:48)

(cid:12)(cid:12)(cid:12) Et(cid:105) ≤ (1 + )(cid:13)(cid:13)Et − Et
(cid:32)(cid:13)(cid:13)Et(cid:13)(cid:13)2
(cid:32)(cid:13)(cid:13)Et(cid:13)(cid:13)2

≤ (1 + )

(cid:13)(cid:13)2
F − k(cid:88)
F − k(cid:88)

= (1 + )

k

F

i=1

(cid:33)

i (Et)
σ2

(cid:33)

σ2
tk+i(A)

.

(2)

(The last step follows because σ2
X = A  Y = (CtCt+A)tk and r = rank(Y) = tk  to obtain σ2

i (A − (CtCt+A)tk) and we can apply Lemma 1 with
tk+i(A).) We now take

i (Et) ≥ σ2

i (Et) = σ2

i=1

5

the expectation of both sides with respect to the columns Ct 

(cid:104)EC(cid:48)
(cid:32)

ECt

≤ (1 + )

(cid:12)(cid:12)(cid:12) Et(cid:105)(cid:105)
(cid:33)

σ2
tk+i(A)

.

F

ECt

(cid:104)(cid:107)Et − (C(cid:48)C(cid:48)+Et)k(cid:107)2
(cid:104)(cid:13)(cid:13)Et(cid:13)(cid:13)2
(cid:105) − k(cid:88)
t−1(cid:88)
F − k(cid:88)

(cid:107)A − Atk(cid:107)2

F + 

i=1

i=1

F

i=1

(cid:32)

(cid:33)

= (1 + )

t−1(cid:88)

i=1

+

(cid:104)EC(cid:48)

ECt

F

(1 + )t+1−i (cid:107)A − Aik(cid:107)2

F + 

t(cid:88)
(cid:12)(cid:12)(cid:12) Et(cid:105)(cid:105)
(cid:104)(cid:107)Et − (C(cid:48)C(cid:48)+Et)k(cid:107)2

i=1

F

k(cid:88)

i=1

σ2
tk+i(A)

(3)

(a)≤ (1 + )2(cid:107)A − Atk(cid:107)2

(1 + )t+1−i (cid:107)A − Aik(cid:107)2

F − (1 + )

σ2
tk+i(A)

+ (1 + )(cid:107)A − Atk(cid:107)2

F

= (1 + )(cid:107)A − A(t+1)k(cid:107)2

(1 + )t+1−i (cid:107)A − Aik(cid:107)2

F

(a) follows  because of the induction hypothesis (eqn. 1). The columns chosen after round t + 1 are
Ct+1 = [Ct  C(cid:48)]. By the law of iterated expectation 

(cid:104)(cid:107)Et − (C(cid:48)C(cid:48)+Et)k(cid:107)2

(cid:105)

.

F

= E

Ct+1

Observe that Et − (C(cid:48)C(cid:48)+Et)k = A − (CtCt+A)tk − (C(cid:48)C(cid:48)+Et)k = A − Y  where Y is in the
column space of Ct+1 = [Ct  C(cid:48)]; further  rank(Y) ≤ (t + 1)k. Since (Ct+1Ct+1+
A)(t+1)k is the
best rank-(t + 1)k approximation to A in the column space of Ct+1  for any realization of Ct+1 

Combining (4) with (3)  we have that

(cid:107)A − (Ct+1Ct+1+

A)(t+1)k(cid:107)2

(cid:21)

F ≤ (cid:107)Et − (C(cid:48)C(cid:48)+Et)k(cid:107)2
F .
t(cid:88)

E
Ct+1

(cid:107)A − (Ct+1Ct+1+

A)(t+1)k(cid:107)2

F

≤ (1+)(cid:107)A − A(t+1)k(cid:107)2

F +

(1+)t+1−i (cid:107)A − Aik(cid:107)2
F .

(cid:20)

(4)

This is the desired bound after t + 1 rounds  concluding the induction.
It is instructive to understand where our new adaptive sampling strategy is needed for the proof to
go through. The crucial step is (2) where we use Lemma 1 – it is essential that the residual was a
low-rank perturbation of A.

i=1

3 Experiments

We compared three adaptive column sampling methods  using two real and two synthetic data sets.3

Adaptive Sampling Methods

ADP-AE: the prior adaptive method which uses the additive error CSSP-algorithm [8].
ADP-LVG: our new adaptive method using the relative error CSSP-algorithm [19].
ADP-Nopt: our adaptive method using the near optimal relative error CSSP-algorithm [15].

Data Sets

HGDP 22 chromosomes: SNPs human chromosome data from the HGDP database [26]. We
use all 22 chromosome matrices (1043 rows; 7 334-37 493 columns) and report the average.
Each matrix contains +1  0 −1 entries  and we randomly ﬁlled in missing entries.
TechTC-300: 49 document-term matrices [27] (150-300 rows (documents); 10 000-40 000
columns (words)). We kept 5-letter or larger words and report averages over 49 data-sets.
Synthetic 1: Random 1000 × 10000 matrices with σi = i−0.3 (power law).
Synthetic 2: Random 1000 × 10000 matrices with σi = exp(1−i)/10 (exponential).

3ADP-Nopt: has two stages. The ﬁrst stage is a deterministic dual set spectral-Frobenius column selection
in which ties could occur. We break ties in favor of the column not already selected with the maximum norm.

6

For randomized algorithms  we repeat the experiments ﬁve times and take the average. We use the
synthetic data sets to provide a controlled environment in which we can see performance for different
types of singular value spectra on very large matrices. In prior work it is common to report on the
quality of the columns selected C by comparing the best rank-k approximation within the column-

span of C to Ak. Hence  we report the relative error (cid:13)(cid:13)A − (CC+A)k

(cid:13)(cid:13)F /(cid:107)A − Ak(cid:107)F when

comparing the algorithms. We set the target rank k = 5 and the number of columns in each round to
c = 2k. We have tried several choices for k and c and the results are qualitatively identical so we only
report on one choice. Our ﬁrst set of results in Figure 2 is to compare the prior adaptive algorithm
ADP-AE with the new adaptive ones ADP-LVG and ADP-Nopt which boose relative error CSSP-
algorithms. Our two new algorithms are both performing better the prior existing adaptive sampling
algorithm. Further  ADP-Nopt is performing better than ADP-LVG  and this is also not surprising 
because ADP-Nopt produces near-optimal columns – if you boost a better CSSP-algorithm  you get
better results. Further  by comparing the performance on Synthetic 1 with Synthetic 2  we see that
our algorithm (as well as prior algorithms) gain signiﬁcantly in performance for rapidly decaying
singular values; our new theoretical analysis reﬂects this behavior  whereas prior results do not.
The theory bound depends on  = c/k. The ﬁgure
to the right shows a result for k = 10; c = 2k
(k increases but  is constant). Comparing the ﬁg-
ure with the HGDP plot in Figure 2  we see that
the quantitative performance is approximately the
same  as the theory predicts (since c/k has not
changed). The percentage error stays the same
even though we are sampling more columns be-
cause the benchmark (cid:107)A − Ak(cid:107)F also get smaller
when k increases. Since ADP-Nopt is the supe-
rior algorithm  we continue with results only for
this algorithm.
Our next experiment is to test which adaptive strat-
egy works better in practice given the same ini-
tial selection of columns. That is  in Figure 2 
ADP-AE uses an adaptive sampling based on the
residual A − CC+A and then adaptively sam-
ples according to the adaptive strategy in [8]; the
initial columns are chosen with the additive error
algorithm. Our approach chooses initial columns
with the relative error CSSP-algorithm and then
continues to sample adaptively based on the rel-
ative error CSSP-algorithm and the residual A −
(CC+A)tk. We now give all the adaptive sam-
pling algorithms the beneﬁt of the near-optimal
initial columns chosen in the ﬁrst round by the al-
gorithm from [15]. The result shown to the right conﬁrms that ADP-Nopt is best even if all adaptive
strategies start from the same initial near-optimal columns.
Adaptive versus Continued Sequential Sam-
pling. Our last experiment is to demonstrate that
adaptive sampling works better than continued se-
quential sampling. We consider the relative error
CSSP-algorithm in [15] in two modes. The ﬁrst
is ADP-Nopt  which is our adaptive sampling al-
gorithms which selects tc columns in t rounds of
c columns each. The second is SEQ-Nopt  which
is just the relative error CSSP-algorithm in [15]
sampling tc columns  all in one go. The results
are shown on the right. The adaptive boosting of
the relative error CSSP-algorithm can gives up to
a 1% improvement in this data set.

7

1234511.021.041.06HGDP 22 chromosomes  k:10 c=2k# of rounds||A−(CC+A)k||F/||A−Ak||F ADP−AEADP−LVGADP−Nopt12345# of rounds11.051.11.151.2||A-(CC+A)k||F/||A-Ak||FTechTC-300 49 Datasets  k:5 c=2kADP-AEADP-LVGADP-Nopt12345# of rounds11.0051.011.0151.02||A-(CC+A)k||F/||A-Ak||F TechTC300 49 datasets  k:5 c:2kADP-NoptSEQ-NoptFigure 2: Plots of relative error ratio (cid:13)(cid:13)A − (CC+A)k

(cid:13)(cid:13)F

/(cid:107)A − Ak(cid:107)F for various adaptive sampling al-
gorithms for k = 5 and c = 2k. In all cases  performance improves with more rounds of sampling  and rapidly
converges to a relative reconstruction error of 1. This is most so in data matrices with singular values that decay
quickly (such as TechTC and Synthetic 2). The HGDP singular values decay slowly because missing entries
are selected randomly  and Synthetic 1 has slowly decaying power-law singular values by construction.

4 Conclusion
We present a new approach for adaptive sampling algorithms which can boost relative error CSSP-
algorithms  in particular the near optimal CSSP-algorithm in [15]. We showed theoretical and exper-
imental evidence that our new adaptively boosted CSSP-algorithm is better than the prior existing
adaptive sampling algorithm which is based on the additive error CSSP-algorithm in [11]. We also
showed evidence (theoretical and empirical) that our adaptive sampling algorithms are better than
sequentially sampling all the columns at once. In particular  our theoretical bounds give a result
which is tighter for matrices whose singular values decay rapidly.
Several interesting questions remain. We showed that the simplest adaptive sampling algorithm
which samples a constant number of columns in each round improves upon sequential sampling all
at once. What is the optimal sampling schedule  and does it depend on the singular value spectrum
of the data matric? In particular  can improved theoretical bounds or empirical performance be
obtained by carefully choosing how many columns to select in each round?
It would also be interesting to see the improved adaptive sampling boosting of CSSP-algorithms in
the actual applications which require column selection (such as sparse PCA or unsupervised feature
selection). How do the improved theoretical estimates we have derived carry over to these problems
(theoretically or empirically)? We leave these directions for future work.
Acknowledgements
Most of the work was done when SP was a graduate student at RPI. PD was supported by IIS-
1447283 and IIS-1319280. MMI was partially supported by the Army Research Laboratory under
Cooperative Agreement Number W911NF-09-2-0053 (the ARL Network Science CTA). The views
and conclusions contained in this document are those of the authors and should not be interpreted
as representing the ofﬁcial policies  either expressed or implied  of the Army Research Laboratory
or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copyright notation here on.

8

1234511.021.041.06HGDP 22 chromosomes  k:5 c=2k# of rounds||A−(CC+A)k||F/||A−Ak||F ADP−AEADP−LVGADP−Nopt1234511.051.11.15TechTC−300 49 Datasets  k:5 c=2k# of rounds||A−(CC+A)k||F/||A−Ak||F ADP−AEADP−LVGADP−Nopt1234511.011.021.031.04Synthetic Data 1  k:5 c=2k# of rounds||A−(CC+A)k||F/||A−Ak||F ADP−AEADP−LVGADP−Nopt1234511.051.1Synthetic Data 2  k:5 c=2k# of rounds||A−(CC+A)k||F/||A−Ak||F ADP−AEADP−LVGADP−NoptReferences
[1] Christos Boutsidis  Petros Drineas  and Malik Magdon-Ismail. Near optimal coresets for least-squares

regression. IEEE Transactions on Information Theory  59(10)  October 2013.

[2] C. Boutsidis and M. Magdon-Ismail. A note on sparse least-squares regression. Information Processing

Letters  115(5):273–276  2014.

[3] Christos Boutsidis and Malik Magdon-Ismail. Deterministic feature selection for k-means clustering.

IEEE Transactions on Information Theory  59(9)  September 2013.

[4] Christos Boutsidis  Petros Drineas  and Malik Magdon-Ismail. Sparse features for pca-like regression. In

Proc. 25th Annual Conference on Neural Information Processing Systems (NIPS)  2011. to appear.

[5] Malik Magdon-Ismail and Christos Boutsidis. Optimal sparse linear auto-encoders and sparse pca.

arXiv:1502.06626  2015.

[6] T. F. Chan and P. C. Hansen. Some applications of the rank revealing QR factorization. SIAM J. Sci. Stat.

Comput.  13(3):727–741  1992.

[7] A. Deshpande and L. Rademacher. Efﬁcient volume sampling for row/column subset selection. In Pro-

ceedings of the IEEE 51st FOCS  pages 329–338  2010.

[8] A. Deshpande and S. Vempala. Adaptive sampling and fast low-rank matrix approximation. In Approx-
imation  Randomization  and Combinatorial Optimization. Algorithms and Techniques  pages 292–303.
Springer  2006.

[9] A. Deshpande  L. Rademacher  S. Vempala  and G. Wang. Matrix approximation and projective clustering

via volume sampling. Theory of Computing  2(1):225–247  2006.

[10] P. Drineas  I. Kerenidis  and P. Raghavan. Competitive recommendation systems. In Proceedings of the

34th STOC  pages 82–90  2002.

[11] A. Frieze  R. Kannan  and S. Vempala. Fast monte-carlo algorithms for ﬁnding low-rank approximations.

Journal of the ACM (JACM)  51(6):1025–1041  2004.

[12] N. Halko  P. G. Martinsson  and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms

for constructing approximate matrix decompositions. SIAM Rev.  53(2):217–288  May 2011.

[13] E. Liberty  F. Woolfe  P.G. Martinsson  V. Rokhlin  and M. Tygert. Randomized algorithms for the low-

rank approximation of matrices. PNAS  104(51):20167–20172  2007.

[14] Michael W Mahoney and Petros Drineas. CUR matrix decompositions for improved data analysis. PNAS 

106(3):697–702  2009.

[15] C. Boutsidis  P. Drineas  and M. Magdon-Ismail. Near-optimal column-based matrix reconstruction.

SIAM Journal of Computing  43(2):687–717  2014.

[16] P. Drineas  M. W Mahoney  and S Muthukrishnan. Subspace sampling and relative-error matrix approx-
imation: Column-based methods. In Approximation  Randomization  and Combinatorial Optimization.
Algorithms and Techniques  pages 316–326. Springer  2006.

[17] Venkatesan Guruswami and Ali Kemal Sinop. Optimal column-based low-rank matrix reconstruction. In
Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms  pages 1207–1214  2012.
[18] C. Boutsidis  P. Drineas  and M. Magdon-Ismail. Near optimal column-based matrix reconstruction. In

IEEE 54th Annual Symposium on FOCS  pages 305–314  2011.

[19] Petros Drineas  Michael W Mahoney  and S Muthukrishnan. Relative-error cur matrix decompositions.

SIAM Journal on Matrix Analysis and Applications  30(2):844–881  2008.

[20] T.F. Chan. Rank revealing QR factorizations. Linear Algebra and its Applications  8889(0):67 – 82  1987.
[21] Crystal Maung and Haim Schweitzer. Pass-efﬁcient unsupervised feature selection. In Advances in Neural

Information Processing Systems  pages 1628–1636  2013.

[22] C. Boutsidis  M. W Mahoney  and P. Drineas. An improved approximation algorithm for the column

subset selection problem. In Proceedings of the 20th SODA  pages 968–977  2009.

[23] D. Papailiopoulos  A. Kyrillidis  and C. Boutsidis. Provable deterministic leverage score sampling. In

Proc. SIGKDD  pages 997–1006  2014.

[24] A. Deshpande  L. Rademacher  S. Vempala  and G. Wang. Matrix approximation and projective clustering

via volume sampling. In Proc. SODA  pages 1117–1126  2006.

[25] P. Drineas and M. W Mahoney. A randomized algorithm for a tensor-based generalization of the singular

value decomposition. Linear algebra and its applications  420(2):553–571  2007.

[26] P. Paschou  J. Lewis  A. Javed  and P. Drineas. Ancestry informative markers for ﬁne-scale individual

assignment to worldwide populations. Journal of Medical Genetics  47(12):835–47  2010.

[27] D. Davidov  E. Gabrilovich  and S. Markovitch. Parameterized generation of labeled datasets for text

categorization based on a hierarchical directory. In Proc. SIGIR  pages 250–257  2004.

9

,Saurabh Paul
Malik Magdon-Ismail
Petros Drineas