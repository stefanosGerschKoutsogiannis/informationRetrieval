2019,Twin Auxilary Classifiers GAN,Conditional generative models enjoy significant progress over the past few years. One of the popular conditional models is Auxiliary Classifier GAN (AC-GAN) that generates highly discriminative images by extending the loss function of GAN with an auxiliary classifier. However  the diversity of the generated samples by AC-GAN tends to decrease as the number of classes increases. In this paper  we identify the source of low diversity issue theoretically and propose a practical solution to the problem. We show that the auxiliary classifier in AC-GAN imposes perfect separability  which is disadvantageous when the supports of the class distributions have significant overlap. To address the issue  we propose Twin Auxiliary Classifiers Generative Adversarial Net (TAC-GAN) that adds a new player that interacts with other players (the generator and the discriminator) in GAN. Theoretically  we demonstrate that our TAC-GAN can effectively minimize the divergence between generated and real data distributions. Extensive experimental results show that our TAC-GAN can successfully replicate the true data distributions on simulated data  and significantly improves the diversity of class-conditional image generation on real datasets.,Twin Auxiliary Classiﬁers GAN

Mingming Gong *1 3  Yanwu Xu *1  Chunyuan Li2  Kun Zhang3  and Kayhan Batmanghelich1

1Department of Biomedical Informatics  University of Pittsburgh  {mig73 yanwuxu kayhan}@pitt.edu

2Microsoft Research  Redmond  cl319@duke.edu

3Department of Philosophy  Carnegie Mellon University  kunz1@cmu.edu

Abstract

Conditional generative models enjoy remarkable progress over the past few years.
One of the popular conditional models is Auxiliary Classiﬁer GAN (AC-GAN) 
which generates highly discriminative images by extending the loss function of
GAN with an auxiliary classiﬁer. However  the diversity of the generated samples
by AC-GAN tends to decrease as the number of classes increases  hence limiting its
power on large-scale data. In this paper  we identify the source of the low diversity
issue theoretically and propose a practical solution to solve the problem. We
show that the auxiliary classiﬁer in AC-GAN imposes perfect separability  which
is disadvantageous when the supports of the class distributions have signiﬁcant
overlap. To address the issue  we propose Twin Auxiliary Classiﬁers Generative
Adversarial Net (TAC-GAN) that further beneﬁts from a new player that interacts
with other players (the generator and the discriminator) in GAN. Theoretically  we
demonstrate that TAC-GAN can effectively minimize the divergence between the
generated and real-data distributions. Extensive experimental results show that our
TAC-GAN can successfully replicate the true data distributions on simulated data 
and signiﬁcantly improves the diversity of class-conditional image generation on
real datasets.

1

Introduction

Generative Adversarial Networks (GANs) [1] are a framework to learn the data generating distribution
implicitly. GANs mimic sampling from a target distribution by training a generator that maps samples
drawn from a canonical distribution to the data space. A distinctive feature of GANs is that the
discriminator that evaluates the separability of the real and generated data distributions [1–4]. If the
discriminator can hardly distinguish between real and generated data  the generator is likely to provide
a good approximation to the true data distribution. To generate high-ﬁdelity images  much recent
research has focused on designing more advanced network architectures [5  6]  developing more
stable objective functions [7–9  3]  enforcing appropriate constraints on the discriminator [10–12]  or
improving training techniques [7  13].
Conditional GANs (cGANs) [14] are a variant of GANs that take advantage of extra information
(condition) and have been widely used for generation of class-conditioned images [15–18] and text [19 
20]. A major difference between cGANs and GANs is that the cGANs feed the condition to both the
generator and the discriminator to lean the joint distributions of images and the condition random
variables. Most methods feed the conditional information by concatenating it (or its embedding)
with the input or the feature vector at speciﬁc layers [14  21  15  22  23  20]. Recently  Projection-
cGAN [24] improves the quality of the generated images using a speciﬁc discriminator that takes the
inner product between the embedding of the conditioning variable and the feature vector of the input

(cid:63)Equal Contribution
The code is available at https://github.com/batmanlab/twin_ac.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

image  obtaining state-of-the-art class-conditional image generation on large-scale datasets such as
ImageNet1000 [25].
Among cGANs  the Auxiliary Classiﬁer GAN (AC-GAN) has received much attention due to its
simplicity and extensibility to various applications [17]. AC-GAN incorporates the conditional
information (label) by training the GAN discriminator with an additional classiﬁcation loss. AC-GAN
is able to generate high-quality images and has been extended to various learning problems  such as
text-to-image generation [26]. However  it is reported in the literature [17  24] that as the number
of labels increases  AC-GAN tends to generate near-identical images for most classes. Miyato et
al. [24] observed this phenomenon on ImageNet1000 and conjectured that the auxiliary classiﬁer
might encourage the generator to produce less diverse images so that they can be easily discernable.
Despite these insightful ﬁndings  the exact source of the low-diversity problem is unclear  let alone
its remedy. In this paper  we aim to provide an understanding of this phenomenon and accordingly
develop a new method that is able to generate diverse and realistic images. First  we show that due to
a missing term in the objective of AC-GAN  it does not faithfully minimize the divergence between
real and generated conditional distribution. We show that missing that term can result in a degenerate
solution  which explains the lack of diversity in the generated data. Based on our understanding  we
introduce a new player in the min-max game of AC-GAN that enables us to estimate the missing
term in the objective. The resulting method properly estimates the divergence between real and
generated conditional distributions and signiﬁcantly increases sample diversity within each class.
We call our method Twin Auxiliary Classiﬁers GAN (TAC-GAN) since the new player is also a
classiﬁer. Compared to AC-GAN  our TAC-GAN successfully replicates the real data distributions
on simulated data and signiﬁcantly improves the quality and diversity of the class-conditional image
generation on CIFAR100 [27]  VGGFace2 [28]  and ImageNet1000 [25] datasets. In particular  to
our best knowledge  our TAC-GAN is the ﬁrst cGAN method that can generate good quality images
on the VGGFace dataset  demonstrating the advantage of TAC-GAN on ﬁne-grained datasets.

2 Method

In this section  we review the Generative Adversarial Network (GAN) [1] and its conditional variant
(cGAN) [14]. We review one of the most popular variants of cGAN  which is called Auxiliary
Classiﬁer GAN (AC-GAN) [17]. We ﬁrst provide an understanding of the observation of low-
diversity samples generated by AC-GAN from a distribution matching perspective. Second  based
on our new understanding of the problem  we propose a new method that enables learning of real
distributions and increasing sample diversity.

2.1 Background
Given a training set {xi}n
i=1 ⊆ X drawn from an unknown distribution PX  GAN estimates PX by
specifying a distribution QX implicitly. Instead of an explicit parametrization  it trains a generator
function G(Z) that maps samples from a canonical distribution  i.e.  Z ∼ PZ  to the training data.
The generator is obtained by ﬁnding an equilibrium of the following mini-max game that effectively
minimizes the Jensen-Shannon Divergence (JSD) between QX and PX:

min

G

max

D

E

X∼PX

[log D(X)] + E
Z∼PZ

[log(1 − D(G(Z)))] 

(1)

where D is a discriminator. Notice that the QX is not directly modeled.
i=1 ⊆ X × Y drawn from the joint
Given a pair of observation (x) and a condition (y)  {xi  y}n
distribution (xi  y) ∼ PXY   the goal of cGAN is to estimate a conditional distribution PX|Y . Let
QX|Y denote the conditional distribution speciﬁed by a generator G(Y  Z) and QXY := QX|Y PY .
A generic cGAN trains G to implicitly minimize the JSD divergence between the joint distributions
QXY and PXY :

min

G

max

D

E

(X Y )∼PXY

[log D(X  Y )] +

E

Z∼PZ  Y ∼PY

[log(1 − D(G(Z  Y )  Y ))].

(2)

In general  Y can be a continuous or discrete variable. In this paper  we focus on case that Y is the
(discrete) class label  i.e.  Y = {1  . . .   K}.

2

(cid:124)
(cid:124)

(cid:123)(cid:122)

E
a(cid:13)
−λc

(cid:125)

(cid:125)

2.2

Insight on Auxiliary Classiﬁer GAN (AC-GAN)

AC-GAN introduces a new player C which is a classiﬁer that interacts with the D and G players. We
use Qc
Y |X to denote the conditional distribution induced by C. The AC-GAN optimization combines
the original GAN loss with cross-entropy classiﬁcation loss:
min
G C

LAC(G  D C) = E
X∼PX

[log(1 − D(G(Z  Y )))]

Z∼PZ  Y ∼PY

[log D(X)] +

max

D

− λc

E

(X Y )∼PXY

(cid:123)(cid:122)

[log C(X  Y )]
b(cid:13)

(cid:125)

E

Z∼PZ  Y ∼PY

(cid:124)

(cid:123)(cid:122)

[log(C(G(Z  Y )  Y ))]

  (3)

(cid:21)

E

E

(cid:20)

log

=

=

E

E

E

(X Y )∼PXY

(X Y )∼PXY

(X Y )∼PXY

(X Y )∼PXY

−HP (Y |X) + b(cid:13) =

[log C(X  Y )]
[log Qc(Y |X)]

(X Y )∼PXY
= KL(PY |X||Qc

c(cid:13)
where λc is a hyperparameter balancing GAN and auxiliary classiﬁcation losses.
Here we decompose the objective of AC-GAN into three terms. Clearly  the ﬁrst term a(cid:13) cor-
responds to the Jensen-Shannon divergence (JSD) between QX and PX. The second term b(cid:13)
is the cross-entropy loss on real data.
It is straightforward to show that the second term min-
imizes Kullback-Leibler (KL) divergence between the real data distribution PY |X and the dis-
tribution Qc
Y |X speciﬁed by C. To show that  we can add the negative conditional entropy 
−HP (Y |X) = E(X Y )∼PXY [log P (Y |X)]  to b(cid:13)  we have
[log P (Y |X)] −
[log P (Y |X)] −
P (Y |X)
Qc(Y |X)

(4)
Since the negative conditional entropy −HP (Y |X) is a constant term  minimizing b(cid:13) w.r.t. the
network parameters in C effectively minimizes the KL divergence between PY |X and Qc
The third term c(cid:13) is the cross-entropy loss on the generated data. Similarly  if one adds the negative
entropy −HQ(Y |X) = E(X Y )∼QXY [log Q(Y |X)] to c(cid:13) and obtain the following result:
−HQ(Y |X) + c(cid:13) =
Y |X ).
When updating C  −HQ(Y |X) can be considered a constant term  thus minimizing c(cid:13) w.r.t. C
effectively minimizes the KL divergence between QY |X and Qc
Y |X. However  when updating G 
−HQ(Y |X) cannot be considered as a constant term  because QY |X is the conditional distribution
speciﬁed by the generator G. AC-GAN ignores −HQ(Y |X) and only minimizes c(cid:13) when updating
G in the optimization procedure  which fails to minimize the KL divergence between QY |X and
Y |X. We hypothesize that the likely reason behind low diversity samples generated by AC-GAN is
Qc
that it fails to account for the missing term while updating G. In fact  the following theorem shows
that AC-GAN can converge to a degenerate distribution:
Theorem 1. Suppose PX = QX. Given an auxiliary classiﬁer C which speciﬁes a conditional
Y |X  the optimal G∗ that minimizes c(cid:13) induces the following degenerate conditional
distribution Qc
distribution Q∗
Y |X 
∗
Q

if k=arg maxi Qc(Y = i|X = x) 

[log Qc(Y |X)] = KL(QY |X||Qc

(Y = k|X = x) =

[log Q(Y |X)] −

(cid:26) 1 

(X Y )∼QXY

(X Y )∼QXY

Y |X ).

Y |X.

(5)

0 

otherwise.

E

E

Proof is given in Section S1 of the Supplementary Material (SM). Theorem 1 shows that  even when
the marginal distributions are perfectly matched by GAN loss a(cid:13)  AC-GAN is not able to model the
probability when class distributions have support overlaps. It tends to generate data in which Y is
deterministically related to X. This means that the generated images for each class are conﬁned by
the regions induced by the decision boundaries of the auxiliary classiﬁer C  which fails to replicate
conditional distribution Qc
Y |X  implied by C  and reduces the distributional support of each class. The
theoretical result is consistent with the empirical results in [24] that AC-GAN generates discriminable
images with low intra-class diversity. It is thus essential to incorporate the missing term  −HQ(Y |X) 
in the objective to penalize this behavior and minimize the KL divergence between QY |X and Qc
Y |X.

3

Figure 1: Illustration of the proposed TAC-GAN. The generator G synthesizes fake samples X
conditioned input label Y . The discriminator D distinguishes between real/fake samples. The
auxiliary classiﬁer C is trained to classify labels on both real and fake pairs  while the proposed twin
auxiliary classiﬁer C mi is trained on fake pairs only.

2.3 Twin Auxiliary Classiﬁers GAN (TAC-GAN)
Our analysis in the previous section motivates adding the missing term  −HQ(Y |X)  back to the
objective function. While minimizing c(cid:13) forces G to concentrate the conditional density mass on
the training data  −HQ(Y |X) works in the opposite direction by increasing the entropy. However 
estimating −HQ(Y |X) is a challenging task since we do not have access to QY |X. Various methods
have been proposed to estimate the (conditional) entropy  such as [29–32]; however  these estimators
cannot be easily used as an objective function to learn G via backpropagation. Below we propose to
estimate the conditional entropy by adding a new player in the mini-max game.
The general idea is to introduce an additional auxiliary classiﬁer  C mi  that aims to identify the labels
of the samples drawn from QX|Y ; the low-diversity case makes this task easy for C mi. Similar to
GAN  the generator tries to compete with the C mi. The overall idea of TAC-GAN is illustrated in
Figure 1. In the following  we demonstrate its connection with minimizing −HQ(Y |X).
Proposition: Let us assume  without loss of generality  that all classes are equally likely 1 (i.e. 
K ). Minimizing −HQ(Y |X) is equivalent to minimizing (1) the mutual information
P (Y = k) = 1
between Y and X and (2) the JSD between the conditional distributions {QX|Y =1  . . .   QX|Y =K}.

Proof.

IQ(Y  X) = H(Y ) − HQ(Y |X) = HQ(X) − HQ(X|Y )

K(cid:88)
K(cid:88)

k=1

k=1

= − 1
K

=

1
K

K(cid:88)

k=1

E

X∼QX|Y =k

log Q(X) +

1
K

E

X∼QX|Y =k

log Q(X|Y = k)

KL(QX|Y =k||QX ) = JSD(QX|Y =1  . . .   QX|Y =K).

(6)

(1) follows from the fact that entropy of Y is constant with respect to Q  (2) is shown above.
Based on the connection between −HQ(Y |X) and JSD  we extend the two-player minimax approach
in GAN [1] to minimize the JSD between multiple distributions. More speciﬁcally  we use another
auxiliary classiﬁer C mi whose last layer is a softmax function that predicts the probability of X
belong to a class Y = k. We deﬁne the following minimax game:

min

G

max
Cmi

V (G  C mi) =

E

Z∼PZ  Y ∼PY

[log(C mi(G(Z  Y )  Y ))].

(7)

The following theorem shows that the minimax game can effectively minimize the JSD between
{QX|Y =1  . . .   QX|Y =K}.
Theorem 2. Let U (G) = max
V (G  C mi). The global mininum of the minimax game is achieved if
Cmi
and only if QX|Y =1 = QX|Y =2 = ··· = QX|Y =K. At the optimal point  U (G) achieves the value
−K log K.

1If the dataset is imbalanced  we can apply biased batch sampling to enforce this condition.

4

GeneratorDiscriminatorX<latexit sha1_base64="hf6hOeTjseL13iz+i/MO/ptaY5E=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2R2UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AtbmM3A==</latexit><latexit sha1_base64="hf6hOeTjseL13iz+i/MO/ptaY5E=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2R2UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AtbmM3A==</latexit><latexit sha1_base64="hf6hOeTjseL13iz+i/MO/ptaY5E=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2R2UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AtbmM3A==</latexit><latexit sha1_base64="hf6hOeTjseL13iz+i/MO/ptaY5E=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2R2UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AtbmM3A==</latexit>Y<latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit>G<latexit sha1_base64="tUjvE4wkWRlcVIxRgxZQP+zWaG8=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRgx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHm/WMyw==</latexit><latexit sha1_base64="tUjvE4wkWRlcVIxRgxZQP+zWaG8=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRgx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHm/WMyw==</latexit><latexit sha1_base64="tUjvE4wkWRlcVIxRgxZQP+zWaG8=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRgx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHm/WMyw==</latexit><latexit sha1_base64="tUjvE4wkWRlcVIxRgxZQP+zWaG8=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRgx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHm/WMyw==</latexit>Z<latexit sha1_base64="GVcMqvlue25tqa0uw4YcPnBW1rQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae2oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBuMGM3g==</latexit><latexit sha1_base64="GVcMqvlue25tqa0uw4YcPnBW1rQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae2oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBuMGM3g==</latexit><latexit sha1_base64="GVcMqvlue25tqa0uw4YcPnBW1rQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae2oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBuMGM3g==</latexit><latexit sha1_base64="GVcMqvlue25tqa0uw4YcPnBW1rQ=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae2oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBuMGM3g==</latexit>D<latexit sha1_base64="NsLos3laW3WlkybC0/eUHSYSJS0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNRDx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHl2mMyA==</latexit><latexit sha1_base64="NsLos3laW3WlkybC0/eUHSYSJS0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNRDx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHl2mMyA==</latexit><latexit sha1_base64="NsLos3laW3WlkybC0/eUHSYSJS0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNRDx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHl2mMyA==</latexit><latexit sha1_base64="NsLos3laW3WlkybC0/eUHSYSJS0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GNRDx5bsB/QhrLZTtq1m03Y3Qgl9Bd48aCIV3+SN/+N2zYHbX0w8Hhvhpl5QSK4Nq777RTW1jc2t4rbpZ3dvf2D8uFRS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfj25nffkKleSwfzCRBP6JDyUPOqLFS465frrhVdw6ySrycVCBHvV/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXUkkj1H42P3RKzqwyIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynZELzll1dJ66LquVWvcVmp3eRxFOEETuEcPLiCGtxDHZrAAOEZXuHNeXRenHfnY9FacPKZY/gD5/MHl2mMyA==</latexit>Cmi<latexit sha1_base64="XzNwmXZXhygeIDgcH/C/ttq5dow=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdgLh4jmAcka5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjSuz/z2E9WGKXlvJwkNBR5KFjOCrZNa9YdMsGm/XPGr/hxolQQ5qUCORr/81RsokgoqLeHYmG7gJzbMsLaMcDot9VJDE0zGeEi7jkosqAmz+bVTdOaUAYqVdiUtmqu/JzIsjJmIyHUKbEdm2ZuJ/3nd1MbXYcZkkloqyWJRnHJkFZq9jgZMU2L5xBFMNHO3IjLCGhPrAiq5EILll1dJ66Ia+NXg7rJSu8njKMIJnMI5BHAFNbiFBjSBwCM8wyu8ecp78d69j0VrwctnjuEPvM8fntSPJQ==</latexit><latexit sha1_base64="XzNwmXZXhygeIDgcH/C/ttq5dow=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdgLh4jmAcka5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjSuz/z2E9WGKXlvJwkNBR5KFjOCrZNa9YdMsGm/XPGr/hxolQQ5qUCORr/81RsokgoqLeHYmG7gJzbMsLaMcDot9VJDE0zGeEi7jkosqAmz+bVTdOaUAYqVdiUtmqu/JzIsjJmIyHUKbEdm2ZuJ/3nd1MbXYcZkkloqyWJRnHJkFZq9jgZMU2L5xBFMNHO3IjLCGhPrAiq5EILll1dJ66Ia+NXg7rJSu8njKMIJnMI5BHAFNbiFBjSBwCM8wyu8ecp78d69j0VrwctnjuEPvM8fntSPJQ==</latexit><latexit sha1_base64="XzNwmXZXhygeIDgcH/C/ttq5dow=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdgLh4jmAcka5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjSuz/z2E9WGKXlvJwkNBR5KFjOCrZNa9YdMsGm/XPGr/hxolQQ5qUCORr/81RsokgoqLeHYmG7gJzbMsLaMcDot9VJDE0zGeEi7jkosqAmz+bVTdOaUAYqVdiUtmqu/JzIsjJmIyHUKbEdm2ZuJ/3nd1MbXYcZkkloqyWJRnHJkFZq9jgZMU2L5xBFMNHO3IjLCGhPrAiq5EILll1dJ66Ia+NXg7rJSu8njKMIJnMI5BHAFNbiFBjSBwCM8wyu8ecp78d69j0VrwctnjuEPvM8fntSPJQ==</latexit><latexit sha1_base64="XzNwmXZXhygeIDgcH/C/ttq5dow=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMdgLh4jmAcka5idzCZj5rHMzAphyT948aCIV//Hm3/jJNmDJhY0FFXddHdFCWfG+v63V1hb39jcKm6Xdnb39g/Kh0cto1JNaJMornQnwoZyJmnTMstpJ9EUi4jTdjSuz/z2E9WGKXlvJwkNBR5KFjOCrZNa9YdMsGm/XPGr/hxolQQ5qUCORr/81RsokgoqLeHYmG7gJzbMsLaMcDot9VJDE0zGeEi7jkosqAmz+bVTdOaUAYqVdiUtmqu/JzIsjJmIyHUKbEdm2ZuJ/3nd1MbXYcZkkloqyWJRnHJkFZq9jgZMU2L5xBFMNHO3IjLCGhPrAiq5EILll1dJ66Ia+NXg7rJSu8njKMIJnMI5BHAFNbiFBjSBwCM8wyu8ecp78d69j0VrwctnjuEPvM8fntSPJQ==</latexit>C<latexit sha1_base64="/9k1Jv5/ArFaNoalXd7NEFzrfuc=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOxF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA/KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryu1uzyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AleWMxw==</latexit><latexit sha1_base64="/9k1Jv5/ArFaNoalXd7NEFzrfuc=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOxF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA/KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryu1uzyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AleWMxw==</latexit><latexit sha1_base64="/9k1Jv5/ArFaNoalXd7NEFzrfuc=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOxF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA/KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryu1uzyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AleWMxw==</latexit><latexit sha1_base64="/9k1Jv5/ArFaNoalXd7NEFzrfuc=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GOxF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoJJfe53nlBpHssHM03Qj+hI8pAzaqzUrA/KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNeGtn3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1U9t+o1ryu1uzyOIpzBOVyCBzdQg3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AleWMxw==</latexit>NoiseTwinAuxiliaryInput+LabelAuxiliaryReal/Fake+Real+Label+Y<latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit>Input+LabelX<latexit sha1_base64="hf6hOeTjseL13iz+i/MO/ptaY5E=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2R2UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AtbmM3A==</latexit><latexit sha1_base64="hf6hOeTjseL13iz+i/MO/ptaY5E=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2R2UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AtbmM3A==</latexit><latexit sha1_base64="hf6hOeTjseL13iz+i/MO/ptaY5E=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2R2UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AtbmM3A==</latexit><latexit sha1_base64="hf6hOeTjseL13iz+i/MO/ptaY5E=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2R2UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AtbmM3A==</latexit>Fake+RealY<latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit>Input+LabelY<latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit><latexit sha1_base64="T3f5yAtJWEWlpP50X1BS1VdMuC0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ie0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHst7M0nQj+hQ8pAzaqzUeOiXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHp0X5935WLQWnHzmGP7A+fwBtz2M3Q==</latexit>A complete proof of Theorem 2 is given in Section S2 of the SM. It is worth noting that the global
optimum of U (G) cannot be achieved in our model because of other terms in our TAC-GAN objective
function  which is obtained by combing (7) and the original AC-GAN objective (3):
LTAC(G  D  C  C mi) = LAC(G  D  C) + λcV (G  C mi).

(8)

min
G C

max
D Cmi

The following theorem provides approximation guarantees for the joint distribution PXY   justifying
the validity of our proposed approach.
Theorem 3. Let PY X and QY X denote the data distribution and the distribution speciﬁed by the
Y |X denote the conditional distribution of Y given X speciﬁed by
generator G  respectively. Let Qc
the auxiliary classiﬁer C. We have
JSD(PXY   QXY ) ≤ 2c1

2JSD(PX   QX ) + c2

2KL(QY |X||Qc

(cid:112)

(cid:113)

(cid:113)

where c1 and c2 are upper bounds of 1
2
measure)  respectively. A proof of Theorem 3 is provided in Section S3 of the SM.

2

2KL(PY |X||Qc

(cid:82) |PY |X (y|x)|µ(x  y) and 1

Y |X ) + c2

(cid:82) |QX (x)|µ(x) (µ is a σ-ﬁnite

Y |X ) 

3 Related Works

TAC-GAN learns an unbiased distribution. Shu et al. [33] ﬁrst show that AC-GAN tends to
down-sample the data points near the decision boundary  causing a biased estimation of the true
distribution. From a Lagrangian perspective  they consider AC-GAN as minimizing JSD(PX   QX )
with constraints enforced by classiﬁcation losses. If λc is very large such that JSD(PX   QX ) become
less effective  the generator will push the generated images away from the boundary. However  on real
datasets  we can also observe low diversity when λc is small  which cannot be explained by the analy-
sis in [33]. We take a different perspective by constraining JSD(PX   QX ) to be small and investigate
the properties of the conditional QY |X. Our analysis suggests that even when JSD(PX   QX ) = 0 
the AC-GAN cross-entropy loss can still result in biased estimate of QY |X  reducing the support of
each class in the generated distribution  compared to the true distribution. Furthermore  we propose a
solution that can remedy the low diversity problem based on our understandings.

Connecting TAC-GAN with Projection cGAN. AC-GAN was once the state-of-the-art method
before the advent of Projection cGAN [24]. Projection cGAN  AC-GAN  and our TAC-GAN share
the similar spirits in that image generation performance can be improved when the joint distribution
matching problem is decomposed into two easier sub-problems: marginal matching and conditional
matching [32]. Projection cGAN decomposes the density ratio  while AC-GAN and TAC-GAN
directly decompose the distribution. Both Projection cGAN and TAC-GAN are theoretically sound
when using the cross-entropy loss. However  in practice  hinge loss is often preferred for real data. In
this case  Projection cGAN loses the theoretical guarantee  while TAC-GAN is less affected  because
only the GAN loss is replaced by the hinge loss.

4 Experiments

We ﬁrst compare the distribution matching ability of AC-GAN  Projection cGAN  and our TAC-GAN
on Mixture of Gaussian (MoG) and MNIST [34] synthetic data. We evaluate the image generation
performance of TAC-GAN on three image datatest including CIFAR100 [27]  ImageNet1000 [25] and
VGGFace2 [28]. In our implementation  the twin auxiliary classiﬁers share the same convolutional
layers  which means TAC-GAN only adds a negligible computation cost to AC-GAN. The detailed
experiment setups are shown in the SM. We implemented TAC-GAN in Pytorch. To illustrate
the algorithm  we submit the implementation on the synthetic datasets in SM. The source code to
reproduce the full experimental results will be made public on GitHub.

4.1 MoG Synthetic Data

We start with a simulated dataset to verify that TAC-GAN can accurately match the target distribution.
We draw samples from a one-dimensional MoG distribution with three Gaussian components  labeled

5

Figure 2: Comparison of sample quality on a synthetic MoG dataset.

Figure 3: The MMD evaluation. The x-axis means the distance between the means of adjacent
Gaussian components (dm). Lower score is better.

as Class_0  Class_1  and Class_2  respectively. The standard deviations of the three components
are ﬁxed to σ0 = 1  σ1 = 2  and σ2 = 3. The differences between the means are set to µ1 − µ0 =
µ2 − µ1 = dm  in which dm ranges from 1 to 5. These values are chosen such that the supports of
the three distributions have different overlap sizes. We detail the experimental setup in Section S4 of
the SM.
Figure 2 shows the ground truth density functions when µ0 = 0  µ1 = 3  µ2 = 6 and the estimated
ones by AC-GAN  TAC-GAN  and Projection cGAN. The estimated density function is obtained
by applying kernel density estimation [35] on the generated data. When using cross-entropy loss 
AC-GAN learns a biased distribution where all the classes are perfectly separated by the classiﬁcation
decision function  verifying our Theorem 1. Both our TAC-GAN and Projection cGAN can accurately
learn the original distribution. Using Hinge loss  our model can still learn the distribution well  while
neither AC-GAN nor Projection cGAN can replicate the real distribution (see Supplementary S4
for more experiments). We also conduct simulation on a 2D dataset and the details are given in
Supplementary S5. The results show that our TAC-GAN is able to learn the true data distribution.
Figure 3 reports the Maximum Mean Discrepancy (MMD) [36] distance between the real data and
generated data for different dm values. Here all the GAN models are trained using cross-entropy
loss (log loss). The TAC-GAN produces near-zero MMD values for all dm’s  meaning that the data
generated by TAC-GAN is very close to the ground truth data. Projection cGAN performs slightly
worse than TAC-GAN and AC-GAN generates data that have a large MMD distance to the true data.
4.2 Overlapping MNIST
Following experiments in [33] to show that AC-GAN learns a biased distribution  we use the
overlapping MNIST dataset to demonstrate the robustness of our TAC-GAN. We randomly sample
from MNIST training set to construct two image groups: Group A contains 5 000 digit ‘1’ and
5 000 digit ‘0’  while Group B contains 5 000 digit ‘2’ and 5 000 digit ‘0’ to simulate overlapping
distributions  where digit ‘0’ appears in both groups. Note that the ground truth proportion of digit
‘0’  ‘1’ and ‘2’ in this dataset are 0.5  0.25 and 0.25  respectively.
Figure 4 (a) shows the generated images under different λc. It shows that AC-GAN tends to down
sample ‘0’ as λc increases  while TAC-GAN can always generate ‘0’s in both groups. To quantitatively
measure the distribution of generated images  we pre-train a “perfect” classiﬁer on a MNIST subset
only containing digit ‘0’  ‘1’  and ‘2’  and use the classiﬁer to predict the labels of the generated data.
Figure 4 (b) reports the label proportions for the generated images. It shows that the label proportion
produced by TAC-GAN is very close to the ground truth values regardless of λc  while AC-GAN
generates less ‘0’s as λc increases. More results and detail setting are shown in Section S6 of the SM.

6

0510150.00.10.20.30.40.5DensityTarget Distribution0510150.00.10.20.30.40.5DensityAC-GAN (Log Loss)0510150.00.10.20.30.40.5DensityTAC-GAN (Log Loss)0510150.00.10.20.30.40.5DensityProjection cGAN (Log Loss)0510150.00.10.20.30.40.5DensityAC-GAN (Hinge Loss)0510150.00.10.20.30.40.5DensityTAC-GAN (Hinge Loss)0510150.00.10.20.30.40.5DensityProjection cGAN (Hinge Loss)12345dm(Distance between means)010203040506070MMDMarginal12345dm (Distance between means)0.00.20.40.60.8MMDClass_0Projection cGANAC-GANTAC-GAN12345dm (Distance between means)0102030405060MMDClass_112345dm (Distance between means)0100200300400500600700MMDClass_2(a) Generated samples

(b) Quantitative result

Figure 4: (a) Visualization of the generated MNIST digits with various λc values. For each section 
the top row digits are sampled from group A and the bottom row digits are from group B. (b) The
label proportion for generated digits of two methods. The ground truth proportion for digit 0 1 2 is
[0.5  0.25  0.25]  visualized as dashed dark lines.

Figure 5: Generated images from ﬁve classes of CIFAR100.

Figure 6: Impact of λc on the image generation quality on CIFAR100.

4.3 CIFAR100

CIFAR100 [27] has 100 classes  each of which contains 500 training images and 100 testing images
at the resolution of 32 × 32. The current best deep classiﬁcation model achieves 91.3% accuracy on
this dataset [37]  which suggests that the class distributions may have certain support overlaps.
Figure 5 shows the generated images for ﬁve randomly selected classes. AC-GAN generates images
with low intra-class diversity. Both TAC-GAN and Projection cGAN generate visually appealing and
diverse images. We provide the generated images for all the classes in Section S6 of the SM.
To quantitatively compare the generated images  we consider the two popular evaluation criteria 
including Inception Score (IS) [38] and Fréchet Inception Distance (FID) [39]. We also use the
recently proposed Learned Perceptual Image Patch Similarity (LPIPS)  which measures the perceptual
diversity within each class [40]. The scores are reported in Table 1. TAC-GAN achieves lower FID
than Projection cGAN  and outperforms AC-GAN by a large margin  which demonstrates the efﬁcacy
of the twin auxiliary classiﬁers. We report the scores for all the classes in Section S7 of the SM. In
Section S7.2 of the SM  we explore the compatibility of our model with the techniques that increase
diversity of unsupervised GANs. Speciﬁcally  we combine pacGAN [41] with AC-GAN and our
TAC-GAN  and the results show that pacGAN can improve both AC-GAN and TAC-GAN  but it
cannot fully address the drawbacks of AC-GAN.
Effects of hyper-parameters λc. We study the impact of λc on AC-GAN and TAC-GAN  and
report results under different λc values in Figure 6. It shows that TAC-GAN is robust to λc  while
AC-GAN requires a very small λc to achieve good scores. Even so  AC-GAN generates images with
low intra-class diversity  as shown in Figure 5.

7

Real	DataAC-GAN 	!"=1TAC-GAN !"=1AC-GAN 	!"=2TAC-GAN 	!"=2(a)<latexit sha1_base64="UjrVwUxZgAHtwgV18OZcVqIbS4E=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSLUS0lU0GPRi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKQw6Lrfzsrq2vrGZmGruL2zu7dfOjhsmjjVjDdYLGPdDqjhUijeQIGStxPNaRRI3gpGt1O/9cS1EbF6xHHC/YgOlAgFo2ilhwo965XKbtWdgSwTLydlyFHvlb66/ZilEVfIJDWm47kJ+hnVKJjkk2I3NTyhbEQHvGOpohE3fjY7dUJOrdInYaxtKSQz9fdERiNjxlFgOyOKQ7PoTcX/vE6K4bWfCZWkyBWbLwpTSTAm079JX2jOUI4toUwLeythQ6opQ5tO0YbgLb68TJrnVe+i6t1flms3eRwFOIYTqIAHV1CDO6hDAxgM4Ble4c2Rzovz7nzMW1ecfOYI/sD5/AGJp41N</latexit>(b)<latexit sha1_base64="go5upAekznw8gMCNtJ1/tvcnpo8=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSLUS0lU0GPRi8eK9gPaUDbbSbt0swm7G6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSATXxnW/nZXVtfWNzcJWcXtnd2+/dHDY1HGqGDZYLGLVDqhGwSU2DDcC24lCGgUCW8Hoduq3nlBpHstHM07Qj+hA8pAzaqz0UAnOeqWyW3VnIMvEy0kZctR7pa9uP2ZphNIwQbXueG5i/Iwqw5nASbGbakwoG9EBdiyVNELtZ7NTJ+TUKn0SxsqWNGSm/p7IaKT1OApsZ0TNUC96U/E/r5Oa8NrPuExSg5LNF4WpICYm079JnytkRowtoUxxeythQ6ooMzadog3BW3x5mTTPq95F1bu/LNdu8jgKcAwnUAEPrqAGd1CHBjAYwDO8wpsjnBfn3fmYt644+cwR/IHz+QOLLI1O</latexit>0.51.01.52.02.53.0Weight of Classifier c0.10.20.30.40.5Relative FrequencyAC-GAN  0TAC-GAN  0AC-GAN  1TAC-GAN  1AC-GAN  2TAC-GAN  2AppleFishTreeMotorcycleMountainTAC-GAN  𝜆𝑐=1.0Projection cGANAC-GAN  𝜆𝑐=0.20.20.40.60.81.0Weight of Classifier6810Inception ScoreTAC-GANAC-GAN0.20.40.60.81.0Weight of Classifier20406080FID0.20.40.60.81.0Weight of Classifier0.00.10.20.3LPIPSFigure 7: Comparison of generated face samples from three identities in VGGFace2 dataset.

Table 1: The quantitative results of all models on three datasets.

TAC-GAN (Ours) (λc = 1)

Projection cGAN

IS ↑

AC-GAN (λc = 1)
FID ↓
5.37 ± 0.064
82.45
7.26 ± 0.113 184.41
27.81 ± 0.29
95.70
25.96 ± 0.32
31.90

Methods
Metrics

CIFAR100

ImageNet1000
VGGFace200
VGGFace500
VGGFace1000
VGGFace2000

4.4

ImageNet1000

IS ↑

9.34 ± 0.077
28.86 ± 0.298
48.94 ± 0.63
77.76 ± 1.61
108.89 ± 2.63
109.04 ± 2.44

FID ↓
7.22
23.75
29.12
12.42
13.60
13.79

IS ↑

9.56 ± 0.133
38.05 ± 0.790
32.50 ± 0.44
35.96 ± 0.62
71.15 ± 0.93
79.51 ± 1.03

FID ↓
8.92
22.77
66.23
43.10
24.07
22.42

We further apply TAC-GAN to the large-scale ImageNet dataset [25] containing 1000 classes  each of
which has around 1 300 images. We pre-process the data by center-cropping and resizing the images
to 128 × 128. We detail the experimental setup and attach generated images in Section S8 of the SM.
Table 1 reports the IS and FID metrics of all models. Our TAC-GAN again outperforms AC-GAN
by a large margin. In addition  TAC-GAN has lower IS than Projection cGAN. We hypothesize
that TAC-GAN has a chance to generate images that do not belong to the given class in the overlap
regions  because it aims to model the true conditional distribution.
4.5 VGGFace2

VGGFace2 [28] is a large-scale face recognition dataset  with around 362 images for each person.
Its main difference to CIFAR100 and ImageNet1000 is that this dataset is more ﬁne-grained with
smaller intra-class diversities  making the generative task more difﬁcult. We resize the center-cropped
images to 64 × 64. To compare different algorithms  we randomly choose 200  500  1000 and 2000
identities to construct the VGGFace200  VGGFace500 VGGFACE1000 and VGGFACE2000 datasets 
respectively.
Figure 7 shows the generated face images for ﬁve randomly selected identities from VGGFACE200.
AC-GAN collapses to the class center  generating very similar images for each class. Though
Projection cGAN generate diverse images  it has blurry effects. Our TAC-GAN generates diverse
and sharp images. To quantitatively compare the methods  we ﬁnetune a Inception Net [42] classiﬁer
on the face data and then use it to calculate IS and FID score. We report IS and FID scores for all
the methods in Table 1. It shows that TAC-GAN produces much better/higher IS better/lower FID
score than Projection cGAN  which is consistent with the qualitative observations. These results
suggest that TAC-GAN is a promising method for ﬁne-grained datasets. More generated identities
are attached in in section S9 of the SM.

5 Conclusion

In this paper  we have theoretically analyzed the low intra-class diversity problem of the widely
used AC-GAN method from a distribution matching perspective. We showed that the auxiliary
classiﬁer in AC-GAN imposes perfect separability  which is disadvantageous when the supports of
the class distributions have signiﬁcant overlaps. Based on the analysis  we further proposed the Twin
Auxiliary Classiﬁers GAN (TAC-GAN) method  which introduces an additional auxiliary classiﬁer
to adversarially play with the players in AC-GAN. We demonstrated the efﬁcacy of the proposed

8

ID1ID2ID3TAC-GANProjection cGANAC-GANmethod both theoretically and empirically. TAC-GAN can resolve the issue of AC-GAN to learn an
unbiased distribution  and generate high-quality samples on ﬁne-grained image datasets.

Acknowledgments

This work was partially supported by NIH Award Number 1R01HL141813-01  NSF 1839332
Tripod+X  and SAP SE. We gratefully acknowledge the support of NVIDIA Corporation with the
donation of the Titan X Pascal GPU used for this research. We were also grateful for the computational
resources provided by Pittsburgh SuperComputing grant number TG-ASC170024.

References
[1] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.

Generative adversarial nets. In NIPS  2014.

[2] Sebastian Nowozin  Botond Cseke  and Ryota Tomioka. f-GAN: Training generative neural samplers using

variational divergence minimization. In NIPS  pages 271–279  2016.

[3] Chun-Liang Li  Wei-Cheng Chang  Yu Cheng  Yiming Yang  and Barnabás Póczos. MMD GAN: Towards

deeper understanding of moment matching network. In NIPS  pages 2203–2213  2017.

[4] Mikolaj Binkowski  Dougal J. Sutherland  Michael Arbel  and Arthur Gretton. Demystifying MMD GANs.

CoRR  abs/1801.01401  2018.

[5] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with deep

convolutional generative adversarial networks. In ICLR  2016.

[6] Han Zhang  Ian J. Goodfellow  Dimitris N. Metaxas  and Augustus Odena. Self-attention generative

adversarial networks. arXiv:1805.08318  2018.

[7] Tim Salimans  Ian Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  and Xi Chen. Improved

techniques for training GAN. In NIPS  pages 2234–2242  2016.

[8] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein generative adversarial networks. In

ICML  pages 214–223  2017.

[9] Xudong Mao  Qing Li  Haoran Xie  Raymond YK Lau  Zhen Wang  and Stephen Paul Smolley. Least

squares generative adversarial networks. In ICCV  pages 2794–2802  2017.

[10] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville. Improved

training of wasserstein GAN. In NIPS  pages 5767–5777  2017.

[11] Lars Mescheder  Sebastian Nowozin  and Andreas Geiger. Which training methods for GANs do actually

converge? In ICML  2018.

[12] Takeru Miyato  Toshiki Kataoka  Masanori Koyama  and Yuichi Yoshida. Spectral normalization for

generative adversarial networks. In ICLR  2018.

[13] Andrew Brock  Jeff Donahue  and Karen Simonyan. Large scale GAN training for high ﬁdelity natural

image synthesis. In ICLR  2019.

[14] M. Mirza and S. Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784  2014.

[15] Phillip Isola  Jun-Yan Zhu  Tinghui Zhou  and Alexei A Efros. Image-to-image translation with conditional

adversarial networks. arxiv  2016.

[16] Anh Nguyen  Jeff Clune  Yoshua Bengio  Alexey Dosovitskiy  and Jason Yosinski. Plug & play generative
networks: Conditional iterative generation of images in latent space. In CVPR  pages 4467–4477  2017.

[17] Augustus Odena  Christopher Olah  and Jonathon Shlens. Conditional image synthesis with auxiliary

classiﬁer GANs. In ICML  pages 2642–2651. JMLR. org  2017.

[18] Dingdong Yang  Seunghoon Hong  Yunseok Jang  Tianchen Zhao  and Honglak Lee. Diversity-sensitive

conditional generative adversarial networks. In ICLR  2019.

[19] Scott Reed  Zeynep Akata  Xinchen Yan  Lajanugen Logeswaran  Bernt Schiele  and Honglak Lee.
Generative adversarial text to image synthesis. In Maria Florina Balcan and Kilian Q. Weinberger  editors 
ICML  pages 1060–1069  2016.

9

[20] Han Zhang  Tao Xu  Hongsheng Li  Shaoting Zhang  Xiaogang Wang  Xiaolei Huang  and Dimitris N
Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks.
In ICCV  pages 5907–5915  2017.

[21] Emily L Denton  Soumith Chintala  Rob Fergus  et al. Deep generative image models using a laplacian

pyramid of adversarial networks. In NIPS  pages 1486–1494  2015.

[22] Guim Perarnau  Joost Van De Weijer  Bogdan Raducanu  and Jose M Álvarez. Invertible conditional gans

for image editing. arXiv preprint arXiv:1611.06355  2016.

[23] Vincent Dumoulin  Ishmael Belghazi  Ben Poole  Olivier Mastropietro  Alex Lamb  Martin Arjovsky  and

Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704  2016.

[24] Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In ICLR  2018.

[25] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng Huang 
Andrej Karpathy  Aditya Khosla  Michael Bernstein  Alexander C. Berg  and Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge. IJCV  115(3):211–252  2015.

[26] Ayushman Dash  John Cristian Borges Gamboa  Sheraz Ahmed  Marcus Liwicki  and Muhammad Zeshan
Afzal. Tac-gan-text conditioned auxiliary classiﬁer generative adversarial network. arXiv preprint
arXiv:1703.06412  2017.

[27] Alex Krizhevsky  Vinod Nair  and Geoffrey Hinton. CIFAR-100 (canadian institute for advanced research).

[28] Qiong Cao  Li Shen  Weidi Xie  Omkar M. Parkhi  and Andrew Zisserman. Vggface2: A dataset for

recognising faces across pose and age. In FG. IEEE Computer Society  2018.

[29] Kumar Sricharan  Dennis Wei  and Alfred O Hero. Ensemble estimators for multivariate entropy estimation.

IEEE transactions on information theory  59(7):4374–4388  2013.

[30] Shashank Singh and Barnabás Póczos. Finite-sample analysis of ﬁxed-k nearest neighbor density functional

estimators. In NIPS  pages 1217–1225  2016.

[31] Weihao Gao  Sreeram Kannan  Sewoong Oh  and Pramod Viswanath. Estimating mutual information for

discrete-continuous mixtures. In NIPS  pages 5986–5997  2017.

[32] Chunyuan Li  Hao Liu  Changyou Chen  Yuchen Pu  Liqun Chen  Ricardo Henao  and Lawrence Carin.

ALICE: Towards understanding adversarial learning for joint distribution matching. In NIPS  2017.

[33] Rui Shu  Hung Bui  and Stefano Ermon. Ac-gan learns a biased distribution.

[34] Yann LeCun. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/.

[35] Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathematical

statistics  1962.

[36] Arthur Gretton  Karsten M. Borgwardt  Malte J. Rasch  Bernhard Schölkopf  and Alexander Smola. A

kernel two-sample test. J. Mach. Learn. Res.  2012.

[37] Yanping Huang  Yonglong Cheng  Dehao Chen  HyoukJoong Lee  Jiquan Ngiam  Quoc V Le  and Zhifeng
Chen. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism. arXiv preprint
arXiv:1811.06965  2018.

[38] Tim Salimans  Ian J. Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  and Xi Chen. Improved

techniques for training gans. CoRR  abs/1606.03498  2016.

[39] Martin Heusel  Hubert Ramsauer  Thomas Unterthiner  Bernhard Nessler  Günter Klambauer  and Sepp
Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. CoRR 
abs/1706.08500  2017.

[40] Richard Zhang  Phillip Isola  Alexei A Efros  Eli Shechtman  and Oliver Wang. The unreasonable

effectiveness of deep features as a perceptual metric. In CVPR  pages 586–595  2018.

[41] Zinan Lin  Ashish Khetan  Giulia Fanti  and Sewoong Oh. Pacgan: The power of two samples in generative

adversarial networks. In NeurIPS  pages 1498–1507  2018.

[42] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jonathon Shlens  and Zbigniew Wojna. Rethinking

the inception architecture for computer vision. CoRR  abs/1512.00567  2015.

10

,Mingming Gong
Yanwu Xu
Chunyuan Li
Kun Zhang
Kayhan Batmanghelich