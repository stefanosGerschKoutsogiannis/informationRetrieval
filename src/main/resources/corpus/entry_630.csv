2019,AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification,Extreme multi-label text classification (XMTC) is an important problem in the 
era of {\it big data}  for tagging a given text with the most relevant multiple 
labels from an extremely large-scale label set. XMTC can be found in many 
applications  such as item categorization  web page tagging  and news 
annotation.
Traditionally most methods used bag-of-words (BOW) as inputs  ignoring word 
context as well as deep semantic information. Recent attempts to overcome the 
problems of BOW by deep learning still suffer from 1) failing to capture the 
important subtext for each label and 2) lack of scalability against the huge 
number of labels.
We propose a new label tree-based deep learning model for XMTC  called 
AttentionXML  with two unique features: 1) a multi-label attention mechanism 
with raw text as input  which allows to capture the most relevant part of text 
to each label; and 2) a shallow and wide probabilistic label tree (PLT)  which 
allows to handle millions of labels  especially for "tail labels".
We empirically compared the performance of AttentionXML with those of eight 
state-of-the-art methods over six benchmark datasets  including Amazon-3M with 
around 3 million labels. AttentionXML outperformed all competing methods 
under all experimental settings.
Experimental results also show that AttentionXML achieved the best performance 
against tail labels among label tree-based methods. The code and datasets are 
available at \url{http://github.com/yourh/AttentionXML} .,AttentionXML: Label Tree-based Attention-Aware

Deep Model for High-Performance Extreme

Multi-Label Text Classiﬁcation

Ronghui You1  Zihan Zhang1  Ziye Wang2  Suyang Dai1 

Hiroshi Mamitsuka5 6  Shanfeng Zhu1 3 4 ∗

1 Shanghai Key Lab of Intelligent Information Processing  School of Computer Science 

2 Centre for Computational Systems Biology  School of Mathematical Sciences 

3 Shanghai Institute of Artiﬁcial Intelligence Algorithms and ISTBI 

4 Key Lab of Computational Neuroscience and Brain-Inspired Intelligence (MOE) 

Fudan University  Shanghai  China;

5 Bioinformatics Center  Institute for Chemical Research  Kyoto University  Japan;
6 Department of Computer Science  Aalto University  Espoo and Helsinki  Finland

{rhyou18 zhangzh17 zywang17 sydai16}@fudan.edu.cn

mami@kuicr.kyoto-u.ac.jp  zhusf@fudan.edu.cn

Abstract

Extreme multi-label text classiﬁcation (XMTC) is an important problem in the era
of big data  for tagging a given text with the most relevant multiple labels from an
extremely large-scale label set. XMTC can be found in many applications  such as
item categorization  web page tagging  and news annotation. Traditionally most
methods used bag-of-words (BOW) as inputs  ignoring word context as well as
deep semantic information. Recent attempts to overcome the problems of BOW
by deep learning still suffer from 1) failing to capture the important subtext for
each label and 2) lack of scalability against the huge number of labels. We propose
a new label tree-based deep learning model for XMTC  called AttentionXML 
with two unique features: 1) a multi-label attention mechanism with raw text as
input  which allows to capture the most relevant part of text to each label; and 2) a
shallow and wide probabilistic label tree (PLT)  which allows to handle millions
of labels  especially for "tail labels". We empirically compared the performance
of AttentionXML with those of eight state-of-the-art methods over six benchmark
datasets  including Amazon-3M with around 3 million labels. AttentionXML
outperformed all competing methods under all experimental settings. Experimental
results also show that AttentionXML achieved the best performance against tail
labels among label tree-based methods. The code and datasets are available at
http://github.com/yourh/AttentionXML .

1

Introduction

Extreme multi-label text classiﬁcation (XMTC) is a natural language processing (NLP) task for
tagging each given text with its most relevant multiple labels from an extremely large-scale label set.
XMTC predicts multiple labels for a text  which is different from multi-class classiﬁcation  where
each instance has only one associated label. Recently  XMTC has become increasingly important 
due to the fast growth of the data scale. In fact  over hundreds of thousands  even millions of labels
and samples can be found in various domains  such as item categorization in e-commerce  web page
tagging  news annotation  to name a few. XMTC poses great computational challenges for developing
effective and efﬁcient classiﬁers with limited computing resource  such as an extremely large number
of samples/labels and a large number of "tail labels" with very few positive samples.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Many methods have been proposed for addressing the challenges of XMTC. They can be categorized
into the following four types: 1) 1-vs-All [3 4 30 31]  2) Embedding-based [7 27]  3) Instance [10 25]
or label tree-based [11  13  24  28]) and 4) Deep learning-based methods [17] (see Appendix for
more descriptions on these methods). The most related methods to our work are deep learning-based
and label tree-based methods. A pioneering deep learning-based method is XML-CNN [17]  which
uses a convolutional neural network (CNN) and dynamic pooling to learn the text representation.
XML-CNN however cannot capture the most relevant parts of the input text to each label  because
the same text representation is given for all labels. Another type of deep learning-based methods is
sequence-to-sequence (Seq2Seq) learning-based methods  such as MLC2Seq [21]  SGM [29] and
SU4MLC [15]. These Seq2Seq learning-based methods use a recurrent neural network (RNN) to
encode a given raw text and an attentive RNN as a decoder to generate predicted labels sequentially.
However the underlying assumption of these models is not reasonable since in reality there are no
orders among labels in multi-label classiﬁcation. In addition  the requirement of extensive computing
in the existing deep learning-based methods makes it unbearable to deal with datasets with millions
of labels.
To handle such extreme-scale datasets  label tree-based methods use a probabilistic label tree (PLT)
[11] to partition labels  where each leaf in PLT corresponds to an original label and each internal
node corresponds to a pseudo-label (meta-label). Then by maximizing a lower bound approximation
of the log likelihood  each linear binary classiﬁer for a tree node can be trained independently with
only a small number of relevant samples [24]. Parabel [24] is a state-of-the-art label tree-based
method using bag-of-words (BOW) features. This method constructs a binary balanced label tree by
recursively partitioning nodes into two balanced clusters until the cluster size (the number of labels in
each cluster) is less than a given value (e.g. 100). This produces a "deep" tree (with a high tree depth)
for an extreme scale dataset  which deteriorates the performance due to an inaccurate approximation
of likelihood  and the accumulated and propagated errors along the tree. In addition  using balanced
clustering with a large cluster size  many tail labels are combined with other dissimilar labels and
grouped into one cluster. This reduces the classiﬁcation performance on tail labels. On the other hand 
another PLT-based method EXTREMETEXT [28]  which is based on FASTTEXT [12]  uses dense
features instead of BOW. Note that EXTREMETEXT ignores the order of words without considering
context information  which underperforms Parabel.
We propose a label tree-based deep learning model  AttentionXML  to address the current challenges
of XMTC. AttentionXML uses raw text as its features with richer semantic context information
than BOW features. AttentionXML is expected to achieve a high accuracy by using a BiLSTM
(bidirectional long short-term memory) to capture long-distance dependency among words and a
multi-label attention to capture the most relevant parts of texts to each label. Most state-of-the-art
methods  such as DiSMEC [3] and Parabel [24]  used only one representation for all labels including
many dissimilar (unrelated) tail labels. It is difﬁcult to satisfy so many dissimilar labels by the
same representation. With multi-label attention  AttentionXML represents a given text differently
for each label  which is especially helpful for many tail labels. In addition  by using a shallow
and wide PLT and a top-down level-wise model training  AttentionXML can handle extreme-scale
datasets. Most recently  Bonsai [13] also uses shallow and diverse PLTs by removing the balance
constraint in the tree construction  which improves the performance by Parabel. Bonsai  however 
needs high space complexity  such as a 1TB memory for extreme-scale datasets  because of using
linear classiﬁers. Note that we conceive our idea that is independent from Bonsai  and apply it in deep
learning based method using deep semantic features other than BOW features used in Bonsai. The
experimental results over six benchmarks datasets including Amazon-3M [19] with around 3 million
labels and 2 millions samples show that AttentionXML outperformed other state-of-the-art methods
with competitive costs on time and space. The experimental results also show that AttentionXML is
the best label tree-based method against tail labels.

2 AttentionXML

2.1 Overview

The main steps of AttentionXML are: (1) building a shallow and wide PLT (Figs. 1a and 1b);
and (2) for each level d (d > 0) of a given constructed PLT  training an attention-aware deep
model AttentionXMLd with a BiLSTM and a multi-label attention (Fig. 1c). The pseudocodes for
constructing PLT  training and prediction of AttentionXML are presented in Appendix.

2

(a)

(b)

(c)

Figure 1: Label tree-based deep model AttentionXML for XMTC. (a) An example of PLT used in AttentionXML.
(b) An example of a PLT building process with settings of K = M = 8 = 23 and H = 3 for L = 8000. The
numbers from left to right show those of nodes for each level from top to down. The numbers in red show
those of nodes in Th that are removed in order to obtain Th+1. (c) Overview of attention-aware deep model
in AttentionXML with text (length ˆT ) as its input and predicted scores ˆz as its output. The ˆxi ∈ R ˆD is the
embeddings of i-th wo rd(where ˆD is the dimension of embeddings)  α ∈ R ˆT×L are the attention coefﬁcients
and ˆW1 and ˆW2 are parameters of the fully connected layer and output layer.

2.2 Building Shallow and Wide PLT

PLT [10] is a tree with L leaves where each leaf corresponds to an original label. Given a sample
x  we assign a label zn ∈ {0  1} for each node n  which indicates whether the subtree rooted at
node n has a leaf (original label) relevant to this sample. PLT estimates the conditional probability
P (zn|zP a(n) = 1  x) to each node n. The marginal probability P (zn = 1|x) for each node n can be
easily derived as follows by the chain rule of probability:

P (zn = 1|x) =

P (zi = 1|zP a(i) = 1  x)

(1)

(cid:89)

i∈P ath(n)

where P a(n) is the parent of node n and P ath(n) is the set of nodes on the path from node n to the
root (excluding the root).
As mentioned in Introduction  large tree height H (excluding the root and leaves) and large cluster
size M will harm the performance. So in AttentionXML  we build a shallow (a small H) and wide (a
small M) PLT TH. First  we built an initial PLT  T0  by a top-down hierarchical clustering  which
was used in Parabel [24]  with a small cluster size M. In more detail  we represent each label by
normalizing the sum of BOW features of text annotated by this label. The labels are then recursively
partitioned into two smaller clusters  which correspond to internal tree nodes  by a balanced k-means
(k=2) until the number of labels smaller than M [24]. T0 is then compressed into a shallow and wide
PLT  i.e. TH  which is a K(= 2c) ways tree with the height of H. This compress operation is similar
to the pruning strategy in some hierarchical multi-class classiﬁcation methods [1  2]. We ﬁrst choose
all parents of leaves as S0 and then conduct compress operations H times  resulting in TH. The
compress operation has three steps: for example in the h-th compress operation over Th−1  we (1)
choose c-th ancestor nodes (h < H) or the root (h = H) as Sh  (2) remove nodes between Sh−1 and
Sh  and (3) then reset nodes in Sh as parents of corresponding nodes in Sh−1. This ﬁnally results in
a shallow and wide tree TH. Practically we use M = K so that each internal node except the root has
no more than K children. Fig 1b shows an example of building PLT. More examples can be found in
Appendix.

3

α22α11BiLSTMBiLSTMBiLSTMα21αT1α12α1Lα2LαTLαT2m1m2mLW1W1W1W2W2W2Word Representataion LayerBidirectional LSTM LayerFully Connected LayerOutput LayerAttention Layer··················x1ˆ x1ˆ ˆ xTˆ xTˆ z1ˆ z1ˆ z2ˆ z2ˆ zLˆ zLˆ ˆ x2x2ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ 2.3 Learning AttentionXML

Given a built PLT  training a deep model against nodes at a deeper level is more difﬁcult because
nodes at a deeper level have less positive examples. Training a deep model for all nodes of different
levels together is hard to optimize and harms the performance  which can only speed up marginally.
Thus we train AttentionXML in a level-wise manner as follows:

1. AttentionXML trains a single deep model for each level of a given PLT in a top-down
manner. Note that labeling each level of the PLT is still a multi-label classiﬁcation problem.
For the nodes of ﬁrst level (children of the root)  AttentionXML (named AttentionXML1 for
the ﬁrst level) can be trained for these nodes directly.
2. AttentionXMLd for the d-th level (d > 1) of the given PLT is only trained by candidates g(x)
for each sample x. Speciﬁcally  we sort nodes of the (d − 1)-th level by zn (from positives
to negatives) and then their scores predicted by AttentionXMLd−1 in the descending order.
We keep the top C nodes at the (d − 1)-th level and choose their children as g(x). It’s like a
kind of additional negative sampling and we can get a more precise approximation of log
likelihood than only using nodes with positive parents.

3. During prediction  for the i-th sample  the predicted score ˆyij for j-th label can be computed
easily based on the probability chain rule. For the prediction efﬁciency  we use beam
search [13  24]: for the d-th (d > 1) level we only predict scores of nodes that belong to
nodes of the (d − 1)-th level with top C predicted scores.

We can see that the deep model without using a PLT can be regarded as a special case of AttentionXML
with a PLT with only the root and L leaves.

2.4 Attention-Aware Deep Model

Attention-aware deep model in AttentionXML consists of ﬁve layers: 1) Word Representation Layer 
2) Bidirectional LSTM Layer  3) Multi-label Attention Layer  4) Fully Connected Layer and 5)
Output Layer. Fig. 1c shows a schematic picture of attention-aware deep model in AttentionXML.

2.4.1 Word Representation Layer

The input of AttentionXML is raw tokenized text with length ˆT . Each word is represented by a
deep semantic dense vector  called word embedding [22]. In our experiments  we use pre-trained
300-dimensional GloVe [22] word embedding as our initial word representation.

2.4.2 Bidirectional LSTM Layer

RNN is a type of neural network with a memory state to process sequence inputs. Traditional RNN
has a problem called gradient vanishing and exploding during training [6]. Long short-term memory
(LSTM) [8] is proposed for solving this problem. We use a Bidirectional LSTM (BiLSTM) to capture
both the left- and right-sides context (Fig. 1c)  where at each time step t the output ˆht is obtained by
concatenating the forward output

−→
h t and the backward output

←−
h t.

2.4.3 Multi-Label Attention

Recently  an attention mechanism in neural networks has been successfully used in many NLP tasks 
such as machine translation  machine comprehension  relation extraction  and speech recognition
[5  18]. The most relevant context to each label can be different in XMTC. AttentionXML computes
the (linear) combination of context vectors ˆhi for each label through a multi-label attention mechanism 
inspired by [16]  to capture various intensive parts of a text. That is  the output of multi-label attention
layer ˆmj ∈ R2 ˆN of the j-th label can be obtained as follows:

ˆmj =

αij ˆhi 

αij =

 

(2)

where αij is the normalized coefﬁcient of ˆhi and ˆwj ∈ R2 ˆN is the so-called attention parameters.
Note that ˆwj is different for each label.

ˆT(cid:88)

i=1

(cid:80) ˆT

eˆhi ˆwj
t=1 eˆht ˆwj

4

Table 1: Datasets we used in our experiments.

Ntest
3 865
6 616
306 782
153 025
769 421
742 507

D
186 104
101 938
203 882
135 909
2 381 304
337 067

L
5.30
18.64
5.04
5.45
4.75
36.04

Dataset
EUR-Lex
Wiki10-31K

Ntrain
15 449
14 146
AmazonCat-13K 1 186 239
Amazon-670K
490 449
1 779 881
1 717 899

W test
1230.40
2425.45
245.98
241.22
808.56
104.18
Ntrain: #training instances  Ntest: #test instances  D: #features  L: #labels  L: average #labels per instance  ˆL:

L
3 956
30 938
13 330
670 091
501 008
2 812 281

ˆL W train
1248.58
2484.30
246.61
247.33
808.66
104.08

20.79
8.52
448.57
3.99
16.86
22.02

Wiki-500K
Amazon-3M

the average #instances per label  W train: the average #words per training instance and W test: the average

#words per test instance. The partition of training and test is from the data source.

2.4.4 Fully Connected and Output Layer

AttentionXML has one (or two) fully connected layers and one output layer. The same parameter
values are used for all labels at the fully connected (and output) layers  to emphasize differences of
attention among all labels. Also sharing the parameter values of fully connected layers among all
labels can largely reduce the number of parameters to avoid overﬁtting and keep the model scale
small.

2.4.5 Loss Function

AttentionXML uses the binary cross-entropy loss function  which is used in XML-CNN [17] as the
loss function. Since the number of labels for each instance varies  we do not normalize the predicted
probability which is done in multi-class classiﬁcation.

2.5

Initialization on parameters of AttentionXML

We initialize the parameters of AttentionXMLd (d > 1) by using the parameters of trained
AttentionXMLd−1  except the attention layers. This initialization helps models of deeper levels
converge quickly  resulting in improvement of the ﬁnal accuracy.

2.6 Complexity Analysis

The deep model without a PLT is hard to deal with extreme-scale datasets  because of high time and
space complexities of the multi-label attention mechanism. Multi-label attention in the deep model
needs O(BL ˆN ˆT ) time and O(BL( ˆN + ˆT )) space for each batch iteration  where B is the batch size.
For large number of labels (L > 100k)  the time cost is huge. Also the whole model cannot be saved
in the limited memory space of GPUs. On the other hand  the time complexity of AttentionXML
with a PLT is much smaller than that without a PLT  although we need train H + 1 different deep
models. That is  the label size of AttentionXML1 is only L/K H  which is much smaller than L.
Also the number of candidate labels of AttentionXMLd(d > 1) is only C × K  which is again much
smaller than L. Thus our efﬁcient label tree-based AttentionXML can be run even with the limited
GPU memory.

3 Experimental Results

3.1 Dataset

We used six most common XMTC benchmark datasets (Table 1): three large-scale datasets (L
ranges from 4K to 30K) : EUR-Lex1 [20]  Wiki10-31K2 [32]  and AmazonCat-13K 2 [19]; and
three extreme-scale datasets (L ranges from 500K to 3M): Amazon-670K2 [19]  Wiki-500K2 and
Amazon-3M2 [19]. Note that both Wiki-500K and Amazon-3M have around two million samples for
training.

1http://www.ke.tu-darmstadt.de/resources/eurlex/eurlex.html
2http://manikvarma.org/downloads/XC/XMLRepository.html

5

Table 2: Hyperparameters we used in our experiments  practical computation time and model size.

Datasets

E

B

ˆN

ˆNf c

H M
= K

C

Train
(hours)

Model Size

(GB)

EUR-Lex
Wiki10-31K

30
30
AmazonCat-13K 10
10
Amazon-670K
5
5

Wiki-500K
Amazon-3M

40
40
200
200
200
200

256
256
512
512
512
512

256
256

512 256
512 256
512 256
512 256

-
-
-
3
1
3

-
-
-
8
64
8

-
-
-
160
15
160

0.51
1.27
13.11
13.90
19.55
31.67

E: The number of epoch; B: The batch size; N: The hidden unit size of LSTM; Nf c: The hidden unit size of
fully connected layers; H: The height of PLT (excluding the root and leaves); M: The maximum cluster size;

K: The parameters of the compress process  and here we set M = K = 2c; C: The number of parents of

candidate nodes.

sample)

Test
(ms/

2.07
4.53
1.63
5.27
2.46
5.92

0.20
0.62
0.63
5.52
3.11
16.14

k(cid:88)

l=1

P @k =

1
k

3.2 Evaluation Measures

We chose P@k (Precision at k) [10] as our evaluation metrics for performance comparison  since
P @k is widely used for evaluating the methods for XMTC.

yrank(l)

(3)

where y ∈ {0  1}L is the true binary vector  and rank(l) is the index of the l-th highest predicted
label. Another common evaluation metric is N @k (normalized Discounted Cumulative Gain at k).
Note that P @1 is equivalent to N @1. We evaluated performance by N @k  and conﬁrmed that the
performance of N @k kept the same trend as P @k. We thus omit the results of N @k in the main text
due to space limitation (see Appendix).

3.3 Competing Methods and Experimental Settings

We compared the state-of-the-art and most representative XMTC methods (implemented by the
original authors) with AttentionXML: AnnexML3 (embedding)  DiSMEC4 (1-vs-All)  MLC2Seq5
(deep learning)  XML-CNN2 (deep learning)  PfastreXML2 (instance tree)  Parabel2 (label tree) and
XT6 (ExtremeText) (label tree) and Bonsai7 (label tree).
For each dataset  we used the most frequent words in the training set as a limited-size vocabulary
(not over 500 000). Word embeddings were ﬁne-tuned during training except EUR-Lex and Wiki10-
31K. We truncated each text after 500 words for training and predicting efﬁciently. We used
dropout [26] to avoid overﬁtting after the embedding layer with the drop rate of 0.2 and after the
BiLSTM with the drop rate of 0.5. Our model was trained by Adam [14] with the learning rate of
1e-3. We also used SWA (stochastic weight averaging) [9] with a constant learning rate to enhance
the performance. We used a three PLTs ensemble in AttentionXML similar to Parabel [23] and
Bonsai [13]. We also examined performance of AttentionXML with only one PLT (without ensemble) 
called AttentionXML-1. On three large-scale datasets  we used AttentionXML with a PLT including
only a root and L leaves(which can also be considered as the deep model without PLTs). Other
hyperparameters in our experiments are shown in Tabel 2.

3.4 Performance comparison

Table 3 shows the performance results of AttentionXML and other competing methods by P @k over
all six benchmark datasets. Following the previous work on XMTC  we focus on top predictions by
varying k at 1  3 and 5 in P @k  resulting in 18 (= three k × six datasets) values of P @k for each
method.

3https://s.yimg.jp/dl/docs/research_lab/annexml-0.0.1.zip
4https://sites.google.com/site/rohitbabbar/dismec
5https://github.com/JinseokNam/mlc2seq.git
6https://github.com/mwydmuch/extremeText
7https://github.com/xmc-aalto/bonsai

6

Table 3: Performance comparisons of AttentionXML and other competing methods over six bench-
marks. The results with the stars are from Extreme Classiﬁcation Repository directly.

Methods

P@3

P@5

Methods

P@1=N@1
Amazon-670K

AnnexML
DiSMEC

PfastreXML

Parabel

XT

Bonsai

MLC2Seq
XML-CNN

AttentionXML-1
AttentionXML

AnnexML
DiSMEC

PfastreXML*

Parabel

XT

Bonsai

MLC2Seq
XML-CNN

AttentionXML-1
AttentionXML

AnnexML
DiSMEC

PfastreXML*

Parabel

XT

Bonsai

MLC2Seq
XML-CNN

AttentionXML-1
AttentionXML

P@1=N@1
EUR-Lex
79.66
83.21
73.14
82.12
79.17
82.30
62.77
75.32
85.49
87.12

Wiki10-31K

86.46
84.13
83.57
84.19
83.66
84.52
80.79
81.41
87.05
87.47

93.54
93.81
91.75
93.02
92.50
92.98
94.29
93.26
95.65
95.92

64.94
70.39
60.16
68.91
66.80
69.55
59.06
60.14
73.08
73.99

74.28
74.72
68.61
72.46
73.28
73.76
58.59
66.23
77.78
78.48

78.36
79.08
77.97
79.14
78.12
79.13
69.45
77.06
81.93
82.41

53.52
58.73
50.54
57.89
56.09
58.35
51.32
49.21
61.10
61.92

64.20
65.94
59.10
63.37
64.51
64.69
54.66
56.11
68.78
69.37

63.30
64.06
63.68
64.51
63.51
64.46
57.55
61.40
66.90
67.31

AnnexML
DiSMEC

PfastreXML*

Parabel

XT

Bonsai

MCL2Seq
XML-CNN

AttentionXML-1
AttentionXML

AnnexML
DiSMEC

PfastreXML

Parabel

XT

Bonsai

MCL2Seq
XML-CNN

AttentionXML-1
AttentionXML

AnnexML
DiSMEC*

PfastreXML*

Parabel

XT

Bonsai

MCL2Seq
XML-CNN

AttentionXML-1
AttentionXML

P@3

P@5

36.61
39.72
34.23
39.77
37.93
40.39

-

30.00
40.67
42.61

32.75
36.17
32.09
35.98
34.63
36.60

-

27.42
36.94
38.92

43.15
50.57
37.32
49.57
46.32
49.80

32.79
39.68
28.16
38.64
36.15
38.83

56.49
58.42

44.41
46.14

45.55
44.96
41.81
44.66
39.28
45.65

43.11
42.80
40.09
42.55
37.24
43.49

-
-

-
-

-
-

-
-

Wiki-500K

42.09
44.78
36.84
44.91
42.54
45.58

-

33.41
45.66
47.58

64.22
70.21
56.25
68.70
65.17
69.26

75.07
76.95

49.30
47.34
43.83
47.42
42.20
48.45

-
-

-
-

AmazonCat-13K

Amazon-3M

49.08
50.86

46.04
48.04

43.88
45.83

1) AttentionXML (with a three PLTs ensemble) outperformed all eight competing methods by P @k.
For example  for P @5  among all datasets  AttentionXML is at least 4% higher than the second best
method (Parabel on AmazonCat-13K). For Wiki-500K  AttentionXML is even more than 17% higher
than the second best method (DiSMEC). AttentionXML also outperformed AttentionXML-1 (without
ensemble)  especially on three extreme-scale datasets. That’s because on extreme-scale datasets 
the ensemble with different PLTs reduces more variance  while on large-scale datasets models the
ensemble is with the same PLTs (only including the root and leaves). Note that AttentionXML-1 is
much more efﬁcient than AttentionXML  because it only trains one model without ensemble.
2) AttentionXML-1 outperformed all eight competing methods by P @k  except only one case.
Performance improvement was especially notable for EUR-Lex  Wiki10-31K and Wiki-500K  with
longer texts than other datasets (see Table 1). For example  for P @5  AttentionXML-1 achieved
44.41  68.78 and 61.10 on Wiki-500K  Wiki10-31K and EUR-Lex  which were around 12%  4%
and 4% higher than the second best  DiSMEC with 39.68  65.94 and 58.73  respectively. This result
highlights that longer text has larger amount of context information  where multi-label attention can
focus more on the most relevant parts of text and extract the most important information on each
label.
3) Parabel  a method using PLTs  can be considered as taking the advantage of both tree-based
(PfastreXML) and 1-vs-All (DiSMEC) methods. It outperformed PfastreXML and achieved a similar
performance to DiSMEC (which is however much more inefﬁcient). ExtremeText (XT) is an online

7

Figure 2: P SP @k of label tree-based methods.

learning method with PLTs (similar to Parabel)  which used dense instead of sparse representations and
achieved slightly lower performance than parabel. Bonsai  another method using PLTs  outperformed
Parabel on all datasets except AmazonCat-13K. In addition  Bonsai achieved better performance than
DiSMEC on Amazon-670K and Amazon-3M. This result indicates that the shallow and diverse PLTs
in Bonsai improves its performance. However  Bonsai needs much more memory than Parabel  for
example  1TB memory for extreme-scale datasets. Note that AttentionXML-1 with only one shallow
and wide PLT  still signiﬁcantly outperformed both Parabel and Bonsai on all extreme-scale datasets 
especially Wiki-500K.
4) MLC2Seq  a deep learning-based method  obtained the worst performance on the three large-scale
datasets  probably because of its unreasonable assumption. XML-CNN  another deep learning-based
method with a simple dynamic pooling was much worse than the other competing methods  except
MLC2Seq. Note that both MLC2Seq and XML-CNN are unable to deal with datasets with millions
of labels.
5) AttentionXML was the best method among all the competing methods  on the three extreme-scale
datasets (Amazon-670K  Wiki-500K and Amazon-3M). Although the improvement by AttentionXML-
1 over the second and third best methods (Bonsai and DiSMEC) is rather slight  AttentionXML-1 is
much faster than DiSMEC and uses much less memory than Bonsai. In addition  the improvement by
AttentionXML with a three PLTs ensemble over Bonsai and DiSMEC is more signiﬁcant  which is
still faster than DiSMEC and uses much less memory than Bonsai.
6) AnnexML  the state-of-the-art embedding-based method  reached the second best P @1 on Amazon-
3M and Wiki10-31K  respectively. Note that the performance of AnnexML was not necessarily so on
the other datasets. The average number of labels per sample of Amazon-3M (36.04) and Wiki10-31K
(18.64) is several times larger than those of other datasets (only around 5). This suggests that each
sample in these datasets has been well annotated. Under this case  embedding-based methods may
acquire more complete information from the nearest samples by using KNN (k-nearset neighbors)
and might gain a relatively good performance on such datasets.

3.5 Performance on tail labels

We examined the performance on tail labels by P SP @k (propensity scored precision at k) [10]:

k(cid:88)

l=1

PSP@k =

1
k

yrank(l)
prank(l)

(4)

where prank(l) is the propensity score [10] of label rank(l). Fig 2 shows the results of three label
tree-based methods (Parabel  Bonsai and AttentionXML) on the three extreme-scale datasets. Due to
space limitation  we reported P SP @k results of AttentionXML and all compared methods including
ProXML [4] (a state-of-the-art method on P SP @k) on six benchmarks in Appendix.
AttentionXML outperformed both Parabel and Bonsai in P SP @k on all datasets. AttentionXML use
a shallow and wide PLT  which is different from Parabel. Thus this result indicates that this shallow
and wide PLT in AttentionXML is promising to improve the performance on tail labels. Additionally 
multi-label attention in AttentionXML would be also effective for tail labels  because of capturing

8

Table 4: P@5 of XML-CNN  BiLSTM and AttentionXML (all without ensemble)

Dataset

XML-CNN BiLSTM AttentionXML
(BiLSTM+Att)

AttentionXML

(BiLSTM+Att+SWA)

61.10
68.78
66.90

EUR-Lex
Wiki10-31K

AmazonCat-13K

49.21
56.21
61.40

53.12
59.55
63.57

59.61
66.51
66.29

Table 5: Performance of variant number of trees in AttentionXML.

Amazon-670K

Wiki-500K

Amazon-3M

Trees

1
2
3
4

P@1
45.66
46.86
47.58
48.03

P@3
40.67
41.95
42.61
43.05

P@5
36.94
38.27
38.92
39.32

P@1
75.07
76.44
76.95
77.21

P@3
56.49
57.92
58.42
58.72

P@5
44.41
45.68
46.14
46.40

P@1
49.08
50.34
50.86
51.66

P@3
46.04
47.45
48.04
48.39

P@5
43.88
45.26
45.83
46.23

the most important parts of text for each label  while Bonsai uses just the same BOW features for all
labels.

3.6 Ablation Analysis

For examining the impact of BiLSTM and multi-label attention  we also run a model which consists of
a BiLSTM  a max-pooling (instead of the attention layer of AttentionXML)  and the fully connected
layers (from XML-CNN). Tabel 4 shows the P @5 results on three large-scale datasets. BiLSTM
outperformed XML-CNN on all three datasets  probably because of capturing the long-distance
dependency among words. AttentionXML (BiLSTM+Attn) further outperformed XML-CNN and
BiLSTM  especially on EUR-Lex and Wiki10-31K  which have long texts. Comparing with a simple
dynamic pooling  obviously multi-label attention can extract the most important information to each
label from long texts more easily. In addition  Table 4 shows that SWA has a favorable effect on
improving prediction accuracy.

3.7

Impact of Number of PLTs in AttentionXML

We examined the performance of ensemble with different number of PLTs in AttentionXML. Table 5
shows the performance comparison of AttentionXML with different number of label trees. We can
see that more trees much improve the prediction accuracy. However  using more trees needs much
more time for both training and prediction. So its a trade-off between performance and time cost.

3.8 Computation Time and Model Size

AttentionXML runs on 8 Nvidia GTX 1080Ti GPUs. Table 2 shows the computation time for training
(hours) and testing (milliseconds/per sample)  as well as the model size (GB) of AttentionXML with
only one PLT for each dataset. For the ensemble of several trees  AttentionXML can be trained and
predicted on a single machine sequentially  or on a distributed settings simultaneously and efﬁciently
(without any network communication).

4 Conclusion

We have proposed a new label tree-based deep learning model  AttentionXML  for XMTC  with two
distinguished features: the multi-label attention mechanism  which allows to capture the important
parts of texts most relevant to each label  and a shallow and wide PLT  which allows to handle millions
of labels efﬁciently and effectively. We examined the predictive performance of AttentionXML 
comparing with eight state-of-the-art methods over six benchmark datasets including three extreme-
scale datasets. AttentionXML outperformed all other competing methods over all six datasets 
particularly datasets with long texts. Furthermore  AttentionXML revealed the performance advantage
in predicting long tailed labels.

9

Acknowledgments

S. Z. is supported by National Natural Science Foundation of China (No. 61572139 and No.
61872094) and Shanghai Municipal Science and Technology Major Project (No. 2017SHZDZX01).
R. Y.  Z. Z.  Z. W.  S. Y. are supported by the 111 Project (NO. B18015)  the key project of Shanghai
Science & Technology (No. 16JC1420402)  Shanghai Municipal Science and Technology Major
Project (No. 2018SHZDZX01) and ZJLab. H.M. has been supported in part by JST ACCEL [grant
number JPMJAC1503]  MEXT Kakenhi [grant numbers 16H02868 and 19H04169]  FiDiPro by
Tekes (currently Business Finland) and AIPSE by Academy of Finland.

References

[1] R. Babbar  I. Partalas  E. Gaussier  and M. R. Amini. On ﬂat versus hierarchical classiﬁcation
in large-scale taxonomies. In Advances in neural information processing systems  pages 1824–
1832  2013.

[2] R. Babbar  I. Partalas  E. Gaussier  M.-R. Amini  and C. Amblard. Learning taxonomy adaptation
in large-scale classiﬁcation. The Journal of Machine Learning Research  17(1):3350–3386 
2016.

[3] R. Babbar and B. Schölkopf. DiSMEC: distributed sparse machines for extreme multi-label
classiﬁcation. In Proceedings of the Tenth ACM International Conference on Web Search and
Data Mining  pages 721–729. ACM  2017.

[4] R. Babbar and B. Schölkopf. Data scarcity  robustness and extreme multi-label classiﬁcation.

Machine Learning  pages 1–23  2019.

[5] D. Bahdanau  K. Cho  and Y. Bengio. Neural machine translation by jointly learning to align

and translate. arXiv preprint arXiv:1409.0473  2014.

[6] Y. Bengio  P. Simard  and P. Frasconi. Learning long-term dependencies with gradient descent

is difﬁcult. IEEE transactions on neural networks  5(2):157–166  1994.

[7] K. Bhatia  H. Jain  P. Kar  M. Varma  and P. Jain. Sparse local embeddings for extreme multi-
label classiﬁcation. In Advances in Neural Information Processing Systems  pages 730–738 
2015.

[8] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation  9(8):1735–

1780  1997.

[9] P. Izmailov  D. Podoprikhin  T. Garipov  D. Vetrov  and A. G. Wilson. Averaging weights leads

to wider optima and better generalization. arXiv preprint arXiv:1803.05407  2018.

[10] H. Jain  Y. Prabhu  and M. Varma. Extreme multi-label loss functions for recommendation 
tagging  ranking & other missing label applications. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining  pages 935–944. ACM 
2016.

[11] K. Jasinska  K. Dembczynski  R. Busa-Fekete  K. Pfannschmidt  T. Klerx  and E. Huller-
meier. Extreme f-measure maximization using sparse probability estimates. In International
Conference on Machine Learning  pages 1435–1444  2016.

[12] A. Joulin  E. Grave  P. Bojanowski  and T. Mikolov. Bag of tricks for efﬁcient text classiﬁca-
tion. In Proceedings of the 15th Conference of the European Chapter of the Association for
Computational Linguistics: Volume 2  Short Papers  volume 2  pages 427–431  2017.

[13] S. Khandagale  H. Xiao  and R. Babbar. Bonsai-diverse and shallow trees for extreme multi-label

classiﬁcation. arXiv preprint arXiv:1904.08249  2019.

[14] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[15] J. Lin  Q. Su  P. Yang  S. Ma  and X. Sun. Semantic-unit-based dilated convolution for multi-
label text classiﬁcation. In Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing  pages 4554–4564  2018.

[16] Z. Lin  M. Feng  C. N. d. Santos  M. Yu  B. Xiang  B. Zhou  and Y. Bengio. A structured

self-attentive sentence embedding. arXiv preprint arXiv:1703.03130  2017.

10

[17] J. Liu  W.-C. Chang  Y. Wu  and Y. Yang. Deep learning for extreme multi-label text classi-
ﬁcation. In Proceedings of the 40th International ACM SIGIR Conference on Research and
Development in Information Retrieval  pages 115–124. ACM  2017.

[18] T. Luong  H. Pham  and C. D. Manning. Effective approaches to attention-based neural machine
translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing  pages 1412–1421  2015.

[19] J. McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating dimensions
with review text. In Proceedings of the 7th ACM conference on Recommender systems  pages
165–172. ACM  2013.

[20] E. L. Mencia and J. Fürnkranz. Efﬁcient pairwise multilabel classiﬁcation for large-scale
In Joint European Conference on Machine Learning and

problems in the legal domain.
Knowledge Discovery in Databases  pages 50–65. Springer  2008.

[21] J. Nam  E. L. Mencía  H. J. Kim  and J. Fürnkranz. Maximizing subset accuracy with recurrent
neural networks in multi-label classiﬁcation. In Advances in Neural Information Processing
Systems  pages 5413–5423  2017.

[22] J. Pennington  R. Socher  and C. Manning. Glove: Global vectors for word representation.
In Proceedings of the 2014 conference on empirical methods in natural language processing
(EMNLP)  pages 1532–1543  2014.

[23] Y. Prabhu  A. Kag  S. Gopinath  K. Dahiya  S. Harsola  R. Agrawal  and M. Varma. Extreme
multi-label learning with label features for warm-start tagging  ranking & recommendation. In
Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining 
pages 441–449. ACM  2018.

[24] Y. Prabhu  A. Kag  S. Harsola  R. Agrawal  and M. Varma. Parabel: Partitioned label trees for
extreme classiﬁcation with application to dynamic search advertising. In Proceedings of the
2018 World Wide Web Conference on World Wide Web  pages 993–1002. International World
Wide Web Conferences Steering Committee  2018.

[25] Y. Prabhu and M. Varma. FastXML: A fast  accurate and stable tree-classiﬁer for extreme
multi-label learning. In Proceedings of the 20th ACM SIGKDD international conference on
Knowledge discovery and data mining  pages 263–272. ACM  2014.

[26] N. Srivastava  G. Hinton  A. Krizhevsky  I. Sutskever  and R. Salakhutdinov. Dropout: a simple
way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research 
15(1):1929–1958  2014.

[27] Y. Tagami. AnnexML: approximate nearest neighbor search for extreme multi-label classiﬁ-
cation. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining  pages 455–464. ACM  2017.

[28] M. Wydmuch  K. Jasinska  M. Kuznetsov  R. Busa-Fekete  and K. Dembczynski. A no-regret
generalization of hierarchical softmax to extreme multi-label classiﬁcation. In Advances in
Neural Information Processing Systems  pages 6355–6366  2018.

[29] P. Yang  X. Sun  W. Li  S. Ma  W. Wu  and H. Wang. SGM: sequence generation model for multi-
label classiﬁcation. In Proceedings of the 27th International Conference on Computational
Linguistics  pages 3915–3926  2018.

[30] I. E. Yen  X. Huang  W. Dai  P. Ravikumar  I. Dhillon  and E. Xing. PPDsparse: a parallel
primal-dual sparse method for extreme classiﬁcation. In Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining  pages 545–553. ACM 
2017.

[31] I. E.-H. Yen  X. Huang  P. Ravikumar  K. Zhong  and I. Dhillon. PD-Sparse: a primal and dual
sparse approach to extreme multiclass and multilabel classiﬁcation. In International Conference
on Machine Learning  pages 3069–3077  2016.

[32] A. Zubiaga.

arXiv:1202.5469  2012.

Enhancing navigation on wikipedia with social

tags.

arXiv preprint

11

,Ronghui You
Zihan Zhang
Ziye Wang
Suyang Dai
Hiroshi Mamitsuka
Shanfeng Zhu