2009,Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation,We introduce the first  temporal-difference  learning algorithms that converge  with smooth value function approximators  such as neural networks. Conventional temporal-difference (TD) methods  such as TD($\lambda$)  Q-learning and Sarsa have been used successfully with function approximation in many applications.  However  it is well known that off-policy sampling  as well as nonlinear function approximation  can cause these algorithms to become unstable (i.e.  the parameters of the approximator may diverge). Sutton et al (2009a b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function  related to the Bellman-error  and algorithms that perform stochastic gradient-descent on this function. In this paper  we generalize their work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the  asymptotic almost-sure convergence  of both algorithms for any finite Markov decision process and any smooth value function approximator  under usual stochastic approximation conditions. The computational complexity per iteration scales linearly with the number of parameters of the approximator. The algorithms are incremental and are guaranteed to converge to locally optimal solutions.,Convergent Temporal-Difference Learning with

Arbitrary Smooth Function Approximation

Hamid R. Maei

University of Alberta
Edmonton  AB  Canada

Csaba Szepesv´ari∗
University of Alberta
Edmonton  AB  Canada

Shalabh Bhatnagar

Indian Institute of Science

Bangalore  India

Doina Precup

McGill University

Montreal  QC  Canada

David Silver

University of Alberta 
Edmonton  AB  Canada

Richard S. Sutton
University of Alberta 
Edmonton  AB  Canada

Abstract

We introduce the ﬁrst temporal-difference learning algorithms that converge with
smooth value function approximators  such as neural networks. Conventional
temporal-difference (TD) methods  such as TD(λ)  Q-learning and Sarsa have
been used successfully with function approximation in many applications. How-
ever  it is well known that off-policy sampling  as well as nonlinear function ap-
proximation  can cause these algorithms to become unstable (i.e.  the parameters
of the approximator may diverge). Sutton et al. (2009a  2009b) solved the prob-
lem of off-policy learning with linear TD algorithms by introducing a new objec-
tive function  related to the Bellman error  and algorithms that perform stochastic
gradient-descent on this function. These methods can be viewed as natural gener-
alizations to previous TD methods  as they converge to the same limit points when
used with linear function approximation methods. We generalize this work to non-
linear function approximation. We present a Bellman error objective function and
two gradient-descent TD algorithms that optimize it. We prove the asymptotic
almost-sure convergence of both algorithms  for any ﬁnite Markov decision pro-
cess and any smooth value function approximator  to a locally optimal solution.
The algorithms are incremental and the computational complexity per time step
scales linearly with the number of parameters of the approximator. Empirical re-
sults obtained in the game of Go demonstrate the algorithms’ effectiveness.

Introduction

1
We consider the problem of estimating the value function of a given stationary policy of a Markov
Decision Process (MDP). This problem arises as a subroutine of generalized policy iteration and
is generally thought to be an important step in developing algorithms that can learn good control
policies in reinforcement learning (e.g.  see Sutton & Barto  1998). One widely used technique
for value-function estimation is the TD(λ) algorithm (Sutton  1988). A key property of the TD(λ)
algorithm is that it can be combined with function approximators in order to generalize the observed
data to unseen states. This generalization ability is crucial when the state space of the MDP is large
or inﬁnite (e.g.  TD-Gammon  Tesauro  1995; elevator dispatching  Crites & Barto  1997; job-shop
scheduling  Zhang & Dietterich  1997). TD(λ) is known to converge when used with linear function
approximators  if states are sampled according to the policy being evaluated – a scenario called on-
policy learning (Tsitsiklis & Van Roy  1997). However  the absence of either of these requirements
can cause the parameters of the function approximator to diverge when trained with TD methods
(e.g.  Baird  1995; Tsitsiklis & Van Roy  1997; Boyan & Moore  1995). The question of whether it
is possible to create TD-style algorithms that are guaranteed to converge when used with nonlinear
function approximation has remained open until now. Residual gradient algorithms (Baird  1995)

∗On leave from MTA SZTAKI  Hungary.

1

attempt to solve this problem by performing gradient descent on the Bellman error. However  unlike
TD  these algorithms usually require two independent samples from each state. Moreover  even if
two samples are provided  the solution to which they converge may not be desirable (Sutton et al. 
2009b provides an example).
In this paper we deﬁne the ﬁrst TD algorithms that are stable when used with smooth nonlinear
function approximators (such as neural networks). Our starting point is the family of TD-style algo-
rithms introduced recently by Sutton et al. (2009a  2009b). Their goal was to address the instability
of TD learning with linear function approximation  when the policy whose value function is sought
differs from the policy used to generate the samples (a scenario called off-policy learning). These al-
gorithms were designed to approximately follow the gradient of an objective function whose unique
optimum is the ﬁxed point of the original TD(0) algorithm. Here  we extend the ideas underlying
this family of algorithms to design TD-like algorithms which converge  under mild assumptions 
almost surely  with smooth nonlinear approximators. Under some technical conditions  the limit
points of the new algorithms correspond to the limit points of the original (not necessarily conver-
gent) nonlinear TD algorithm. The algorithms are incremental  and the cost of each update is linear
in the number of parameters of the function approximator  as in the original TD algorithm.
Our development relies on three main ideas. First  we extend the objective function of Sutton et
al. (2009b)  in a natural way  to the nonlinear function approximation case. Second  we use the
weight-duplication trick of Sutton et al. (2009a) to derive a stochastic gradient algorithm. Third 
in order to implement the parameter update efﬁciently  we exploit a nice idea due to Pearlmutter
(1994)  allowing one to compute exactly the product of a vector and a Hessian matrix in linear
time. To overcome potential instability issues  we introduce a projection step in the weight update.
The almost sure convergence of the algorithm then follows from standard two-time-scale stochastic
approximation arguments.
In the rest of the paper  we ﬁrst introduce the setting and our notation (Section 2)  review previ-
ous relevant work (Section 3)  introduce the algorithms (Section 4)  analyze them (Section 5) and
illustrate the algorithms’ performance (Section 6).
2 Notation and Background
We consider policy evaluation in ﬁnite state and action Markov Decision Processes (MDPs).1
An MDP is described by a 5-tuple (S A  P  r γ )  where S is the ﬁnite state space  A is the ﬁ-
nite action space  P = (P (s￿|s  a))s s￿∈S a∈A are the transition probabilities (P (s￿|s  a) ≥ 0 
￿s￿∈S P (s￿|s  a) = 1  for all s ∈S   a ∈A )  r = (r(s  a  s￿))s s￿∈S a∈A are the real-valued
immediate rewards and γ ∈ (0  1) is the discount factor. The policy to be evaluated is a map-
ping π : S×A→ [0  1]. The value function of π  V π : S→ R  maps each state s to a
number representing the inﬁnite-horizon expected discounted return obtained if policy π is fol-
lowed from state s. Formally  let s0 = s and for t ≥ 0 let at ∼ π(st ·)  st+1 ∼ P (·|st  at)
and rt+1 = r(st  at  st+1). Then V π(s) = E[￿∞t=0 γtrt+1]. Let Rπ : S → R  with
Rπ(s) = ￿s￿∈S￿a∈A π(s  a)P (s￿|s  a)r(s  a  s￿)  and let P π : S×S→ [0  1] be deﬁned as
P π(s  s￿) = ￿a∈A π(s  a)P (s￿|s  a). Assuming a canonical ordering on the elements of S  we

can treat V π and Rπ as vectors in R|S|  and P π as a matrix in R|S|×|S|. It is well-known that V π
satisﬁes the so-called Bellman equation:

V π = Rπ + γP πV π.

Deﬁning the operator T π : R|S| → R|S| as T πV = Rπ+γP πV  the Bellman equation can be written
compactly as V π = T πV π. To simplify the notation  from now on we will drop the superscript π
everywhere  since the policy to be evaluated will be kept ﬁxed.
Assume that
the policy to be evaluated is followed and it gives rise to the trajectory
(s0  a0  r1  s1  a1  r2  s2  . . .). The problem is to estimate V   given a ﬁnite preﬁx of this trajec-
tory. More generally  we may assume that we are given an inﬁnite sequence of 3-tuples  (sk  rk  s￿k) 
that satisﬁes the following:
Assumption A1 (sk)k≥0 is an S-valued stationary Markov process  sk ∼ d(·)  rk = R(sk) and
s￿k ∼ P (sk ·).

1Under appropriate technical conditions  our results  can be generalized to MDPs with inﬁnite state spaces 

but we do not address this here.

2

We call (sk  rk  s￿k) the kth transition. Since we assume stationarity  we will sometimes drop the
index k and use (s  r  s￿) to denote a random transition. Here d(·) denotes the probability distribution
over initial states for a transition; let D ∈ R|S|×|S| be the corresponding diagonal matrix. The
problem is still to estimate V given a ﬁnite number of transitions.
When the state space is large (or inﬁnite) a function approximation method can be used to facilitate
the generalization of observed transitions to unvisited or rarely visited states. In this paper we focus
on methods that are smoothly parameterized with a ﬁnite-dimensional parameter vector θ ∈ Rn. We
denote by Vθ(s) the value of state s ∈S returned by the function approximator with parameters θ.
The goal of policy evaluation becomes to ﬁnd θ such that Vθ ≈ V .
3 TD Algorithms with function approximation
The classical TD(0) algorithm with function approximation (Sutton  1988; Sutton & Barto  1998)
starts with an arbitrary value of the parameters  θ0. Upon observing the kth transition  it computes
the scalar-valued temporal-difference error 

which is then used to update the parameter vector as follows:

δk = rk + γVθk(s￿k) − Vθk(sk) 
θk+1 ← θk + αk δk∇Vθk(sk).

(1)
Here αk is a deterministic positive step-size parameter  which is typically small  or (for the purpose

of convergence analysis) is assumed to satisfy the Robbins-Monro conditions: ￿∞k=0 αk = ∞ 
￿∞k=0 α2

When the TD algorithm converges  it must converge to a parameter value where  in expectation  the
parameters do not change:

k < ∞. We denote by ∇Vθ(s) ∈ Rn the gradient of V w.r.t. θ at s.

E[δ ∇Vθ(s)] = 0 

(2)
where s  δ are random and share the common distribution underlying (sk δ k); in particular  (s  r  s￿)
are drawn as in Assumption A1 and δ = r + γVθ(s￿) − Vθ(s).
However  it is well known that TD(0) may not converge; the stability of the algorithm is affected
both by the actual function approximator Vθ and by the way in which transitions are sampled. Sutton
et al (2009a  2009b) tackled this problem in the case of linear function approximation  in which
Vθ(s) = θ￿φ(s)  where φ : S→ Rn  but where transitions may be sampled in an off-policy
manner. From now on we use the shorthand notation φ = φ(s)  φ￿ = φ(s￿).
Sutton et al.
(2009b) rely on an error function  called mean-square projected Bellman error
(MSPBE)2  which has the same unique optimum as Equation (2). This function  which we de-
note J  projects the Bellman error measure  T Vθ − Vθ onto the linear space M = {Vθ | θ ∈ Rn}
with respect to the metric ￿·￿ D. Hence  ΠV = arg min V ￿∈M ￿V ￿ − V ￿2
(3)
D =￿s∈S d(s)V (s)2  and the scalar
where ￿V ￿D is the weighted quadratic norm deﬁned by ￿V ￿2
TD(0) error for a given transition (s  r  s￿) is δ = r + γθ￿φ￿ − θ￿φ.
The negative gradient of the MSPBE objective function is:

J(θ) =￿ Π(T Vθ − Vθ) ￿2

D= E[δφ]￿E[φφ￿]−1E[δφ] 

D=￿ Π T Vθ − Vθ ￿2

D. More precisely:

1

−

2∇J(θ) = E￿(φ − γφ￿)φ￿w￿ = E[δφ] − γE￿φ￿φ￿￿ w 

(4)
where w = E[φφ￿]−1E[δφ]. Note that δ depends on θ  hence w depends on θ. In order to develop
an efﬁcient (O(n)) stochastic gradient algorithm  Sutton et al. (2009a) use a weight-duplication
trick. They introduce a new set of weights  wk  whose purpose is to estimate w for a ﬁxed value of
the θ parameter. These weights are updated on a “fast” timescale  as follows:

(5)
The parameter vector θk is updated on a “slower” timescale. Two update rules can be obtained 
based on two slightly different calculations:

wk+1 = wk + βk(δk − φ￿k wk)φk.

θk+1 = θk + αk(φk − γφ￿k)(φ￿k wk) (an algorithm called GTD2)  or
θk+1 = θk + αkδkφk − αkγφ￿k(φ￿k wk) (an algorithm called TDC).

(6)
(7)

2This error function was also described in (Antos et al.  2008)  although the algorithmic issue of how to
minimize it is not pursued there. Algorithmic issues in a batch setting are considered by Farahmand et al.
(2009) who also study regularization.

3

4 Nonlinear Temporal Difference Learning
Our goal is to generalize this approach to the case in which Vθ is a smooth  nonlinear function
approximator. The ﬁrst step is to ﬁnd a good objective function on which to do gradient descent. In
the linear case  MSPBE was chosen as a projection of the Bellman error on a natural hyperplane–the
subspace to which Vθ is restricted. However  in the nonlinear case  the value function is no longer
restricted to a plane  but can move on a nonlinear surface. More precisely  assuming that Vθ is a
differentiable function of θ  M = {Vθ ∈ R|S| | θ ∈ Rn} becomes a differentiable submanifold
of R|S|. Projecting onto a nonlinear manifold is not computationally feasible; to get around this
problem  we will assume that the parameter vector θ changes very little in one step (given that
learning rates are usually small); in this case  the surface is locally close to linear  and we can
project onto the tangent plane at the given point. We now detail this approach and show that this is
indeed a good objective function.
The tangent plane PMθ of M at θ is the hyperplane of R|S| that (i) passes through Vθ and (ii)
is orthogonal to the normal of M at θ. The tangent space TMθ is the translation of PMθ to the
origin. Note that TMθ = {Φθa| a ∈ Rn}  where Φθ ∈ R|S|×n is deﬁned by (Φθ)s i = ∂
Vθ(s).
Let Πθ be the projection that projects vectors of (R|S| ￿·￿ D) to TMθ. If Φ￿θ DΦθ is non-singular
then Πθ can be written as:
(8)

Πθ =Φ θ(Φ￿θ DΦθ)−1Φ￿θ D.

∂θi

The objective function that we will optimize is:

J(θ) = ￿ Πθ(T Vθ − Vθ) ￿2
D .

(9)

This is a natural generalization of the objective function deﬁned by (3)  as the plane on which we
project is parallel to the tangent plane at θ. More precisely  let Υθ be the projection to PMθ and
let Πθ be the projection to TMθ. Because the two hyperplanes are parallel  for any V ∈ R|S| 
ΥθV − Vθ =Π θ(V − Vθ). In other words  projecting onto the tangent space gives exactly the same
distance as projecting onto the tangent plane  while being mathematically more convenient. Fig. 1
illustrates visually this objective function.

T Vθ

Υ
θ

T

Υ θT V θ

Vθ

(θ)
￿J

θ ∗

V

T

V θ∗

Υθ∗T Vθ∗ = Vθ∗

Tangent plane  

TD(0) solution 

Figure 1: The MSPBE objective for nonlin-
ear function approximation at two points in
the value function space. The ﬁgure shows a
point  Vθ  at which  J(θ)  is not 0 and a point 
Vθ∗  where J(θ∗) = 0  thus Υθ∗T Vθ∗ = Vθ∗ 
so this is a TD(0) solution.

We now show that J(θ) can be re-written in the same way as done in (Sutton et al.  2009b).
Lemma 1. Assume Vθ(s0) is continuously differentiable as a function of θ  for any s0 ∈S s.t.
d(s0) > 0. Let (s  δ) be jointly distributed random variables as in Section 3 and assume that
E[∇Vθ(s)∇Vθ(s)￿] is nonsingular. Then

J(θ) = E[ δ ∇Vθ(s) ]￿ E[∇Vθ(s)∇Vθ(s)￿ ]−1 E[ δ ∇Vθ(s) ].

(10)

Proof. The identity is obtained similarly to Sutton et. al (2009b)  except that here Πθ is expressed
by (8). Details are omitted for brevity.

Note that the assumption that E[∇Vθ(s)∇Vθ(s)￿ ]−1 is non-singular is akin to the assumption that
the feature vectors are independent in the linear function approximation case. We make this assump-
tion here for convenience; it can be lifted  but the proofs become more involved.
Corollary 1. Under the conditions of Lemma 1  J(θ) = 0  if and only if Vθ satisﬁes (2).

4

This is an important corollary  because it shows that the global optima of the proposed objective
function will not modify the set of solutions that the usual TD(0) algorithm would ﬁnd (if it would
indeed converge). We now proceed to compute the gradient of this objective.
Theorem 1. Assume that (i) Vθ(s0) is twice continuously differentiable in θ for any s0 ∈
S s.t. d(s0) > 0 and (ii) W (·) deﬁned by W (ˆθ) = E[∇Vˆθ ∇V ￿ˆθ
] is non-singular in a small neigh-
borhood of θ. Let (s  δ) be jointly distributed random variables as in Section 3. Let φ ≡ ∇Vθ(s) 
φ￿ ≡ ∇Vθ(s￿) and
(11)
where u ∈ Rn. Then

h(θ  u) = −E[ (δ − φ￿u)∇2Vθ(s)u ] 

1
2∇J(θ) = −E[(γφ￿ − φ)φ￿w] + h(θ  w) = −E[δφ] − γE[φ￿φ￿w] + h(θ  w) 

(12)

−

where w = E[φ φ￿]−1 E[δφ].
The main difference between Equation (12) and Equation (4)  which shows the gradient for the linear
case  is the appearance of the term h(θ  w)  which involves second-order derivatives of Vθ (which
are zero when Vθ is linear in θ).

Proof. The conditions of Lemma 1 are satisﬁed  so (10) holds.
deﬁnition and the assumptions  W (u) is a symmetric  positive deﬁnite matrix  so d
− W −1(θ) ( d
exists in a small neighborhood of θ. From this identity  we have:
1
2

du W|u=θ) W −1(θ)  where we use the assumption that d

1

−

Denote ∂i = ∂
∂θi

. From its
du(W −1)|u=θ =
du W exists at θ and W −1

[∇J(θ)]i = −(∂iE[δφ])￿E[φφ￿]−1E[δφ] −
= −(∂iE[δφ])￿E[φφ￿]−1E[δφ] +
= −E[∂i(δφ)]￿(E[φφ￿]−1E[δφ]) +

2 E[δφ]￿ ∂i￿E[φφ￿]−1￿ E[δφ]
1
2 E[δφ]￿ E[φφ￿]−1(∂iE[φφ￿]) E[φφ￿]−1 E[δφ]
1
2

(E[φφ￿]−1E[δφ])￿ E[∂i(φφ￿)] (E[φφ￿]−1E[δφ]).

The interchange between the gradient and expectation is possible here because of assumptions (i)
and (ii) and the fact that S is ﬁnite. Now consider the identity
1
2 x￿∂i(φφ￿)x = φ￿x (∂iφ￿)x 

which holds for any vector x ∈ Rn. Hence  using the deﬁnition of w 
1
2 w￿E[∂i(φφ￿)]w

[∇J(θ)]i = −E[∂i(δφ)]￿w +

−

1
2

= −E[(∂iδ)φ￿w] − E[δ(∂iφ￿)w] + E[φ￿w(∂iφ￿)w].

Using ∇δ = γφ￿ − φ and ∇φ￿ = ∇2Vθ(s)  we get

1
2∇J(θ) = −E[(γφ￿ − φ)φ￿w] − E[(δ − φ￿w)∇2V (s)w] 

−

Finally  observe that :

E[(γφ￿ − φ)φ￿w] = E[(φ − γφ￿)φ]￿ (E[φφ￿]−1E[δφ])

= E[δφ] − E[γφ￿φ￿](E[φφ￿]−1E[δφ]) = E[δφ] − E[γφ￿φ￿w].

which concludes the proof.

Theorem 1 suggests straightforward generalizations of GTD2 and TDC (cf. Equations (6) and (7))
to the nonlinear case. Weight wk is updated as before on a “faster” timescale:

The parameter vector θk is updated on a “slower” timescale  either according to

wk+1 = wk + βk(δk − φ￿k wk)φk.

(13)

θk+1 =Γ￿θk + αk￿(φk − γφ￿k)(φ￿k wk) − hk￿￿ 

5

(non-linear GTD2)

(14)

hk = (δk − φ￿k wk)∇2Vθk(sk)wk.

(16)
Besides hk  the only new ingredient compared to the linear case is Γ: Rn → Rn  a mapping
that projects its argument into an appropriately chosen compact set C with a smooth boundary.
The purpose of this projection is to prevent the parameters to diverge in the initial phase of the
algorithm  which could happen due to the presence of the nonlinearities in the algorithm. Projection
is a common technique for stabilizing the transient behavior of stochastic approximation algorithms
(see  e.g.  Kushner & Yin  2003). In practice  if one selects C large enough so that it contains the
set of possible solutions U = { θ | E[ δ ∇Vθ(s)] = 0} (by using known bounds on the size of the
rewards and on the derivative of the value function)  it is very likely that no projections will take
place at all during the execution of the algorithm. We expect this to happen frequently in practice:
the main reason for the projection is to facilitate convergence analysis.
Let us now analyze the computational complexity per update. Assume that Vθ(s) and its gradi-
ent can each be computed in O(n) time  the usual case for approximators of interest (e.g.  neu-
ral networks). Equation (16) also requires computing the product of the Hessian of Vθ(s) and
w. Pearlmutter (1994) showed that this can be computed exactly in O(n) time. The key is to
note that ∇2Vθk(sk)wk = ∇(∇Vθk(s)￿wk)  because wk does not depend on θk. The scalar term
∇Vθk(s)￿wk can be computed in O(n) and its gradient  which is a vector  can also be computed in
O(n). Hence  the computation time per update for the proposed algorithms is linear in the number
of parameters of the function approximator (just like in TD(0)).
5 Convergence Analysis
Given the compact set C ⊂ Rn  let C(C) be the space of C → Rn continuous functions. Given
projection Γ onto C  let operator ˆΓ: C(C) →C (Rn) be

ˆΓv (θ) = lim
0<ε→0

Γ￿θ + εv (θ)￿ − θ

ε

.

or  according to

θk+1 =Γ￿θk + αk￿δkφk − γφ￿k(φ￿k wk) − hk￿￿ 

where

(non-linear TDC)

(15)

(0) ∈ C.

˙θ = ˆΓ(− 1

2∇J)(θ) θ

By assumption  Γ(θ) = arg minθ￿∈C ￿θ￿−θ￿ and the boundary of C is smooth  so ˆΓ is well deﬁned.
In particular  ˆΓv (θ) = v(θ) when θ ∈ C◦  otherwise  if θ ∈ ∂C  ˆΓv (θ) is the projection of v(θ) to
the tangent space of ∂C at θ. Consider the following ODE:
(17)
Let K be the set of all asymptotically stable equilibria of (17). By the deﬁnitions  K ⊂ C. Further-
more  U ∩ C ⊂ K.
The next theorem shows that under some technical conditions  the iterates produced by nonlinear
GTD2 converge to K with probability one.
Theorem 2 (Convergence of nonlinear GTD2). Let (sk  rk  s￿k)k≥0 be a sequence of transitions that
satisﬁes A1. Consider the nonlinear GTD2 updates (13)  (14). with positive step-size sequences that
satisfy￿∞k=0 αk =￿∞k=0 βk = ∞ ￿∞k=0 α2
βk → 0  as k → ∞. Assume
that for any θ ∈ C and s0 ∈S s.t. d(s0) > 0  Vθ(s0) is three times continuously differentiable.
Further assume that for each θ ∈ C  E[φθφ￿θ ] is nonsingular. Then θk → K  with probability one 
as k → ∞.
Proof. Let (s  r  s￿) be a random transition. Let φθ = ∇Vθ(s)  φ￿θ = ∇Vθ(s￿)  φk = ∇Vθk(sk)  and
φ￿k = ∇Vθk(s￿k). We begin by rewriting the updates (13)-(14) as follows:
(18)
θk+1 =Γ￿θk + αk(g(θk  wk) + Nk+1)￿ 
(19)

where
f(θk  wk) = E[δkφk|θk] − E[φkφ￿k |θk]wk 
g(θk  wk) = E￿(φk − γφ￿k)φ￿k wk − hk|θk  wk￿   Nk+1 = ((φk − γφ￿k)φ￿k wk − hk) − g(θk  wk).

Mk+1 = (δk − φ￿k wk)φk − f(θk  wk) 

wk+1 = wk + βk(f(θk  wk) + Mk+1) 

k ￿∞k=0 β2

k < ∞ and αk

6

We need to verify that there exists a compact set B ⊂ R2n such that (a) the functions f(θ  w) 
g(θ  w) are Lipschitz continuous over B  (b) (Mk Gk)  (Nk Gk)  k ≥ 1 are martingale difference
sequences  where Gk = σ(ri θ i  wi  si  i ≤ k; s￿i  i < k)  k ≥ 1 are increasing sigma ﬁelds  (c)
{(wk(θ) θ )} with wk(θ) obtained as δk(θ) = rk + γVθ(s￿k) − Vθ(sk)  φk(θ) = ∇Vθ(sk) 

wk+1(θ) = wk(θ) + βk￿δk(θ) − φk(θ)￿wk(θ)￿φk(θ)

˙w = E[δθφθ] − E[φθφ￿θ ]w 

almost surely stays in B for any choice of (w0(θ) θ ) ∈ B  and (d) {(w  θk)} almost surely stays in
B for any choice of (w  θ0) ∈ B. From these and the conditions on the step-sizes  using standard
arguments (c.f. Theorem 2 of Sutton et al. (2009b))  it follows that θk converges almost surely to
the set of asymptotically stable equilibria of ˙θ = ˆΓF (θ)  (θ(0) ∈ C)  where F (θ) = g(θ  wθ). Here
for θ ∈ C ﬁxed  wθ is the (unique) equilibrium point of
(20)
where δθ = r + γVθ(s￿) − Vθ(s). Clearly  wθ = E￿φθφ￿θ￿−1 E[δθφθ]  which exists by assumption.
Then by Theorem 1 it follows that F (θ) = − 1
2 ∇J(θ). Hence  the statement will follow once (a)–(d)
are veriﬁed.
Note that (a) is satisﬁed because Vθ is three times continuously differentiable. For (b)  we need to
verify that for any k ≥ 0  E[Mk+1 | Gk] = 0 and E[Nk+1 | Gk] = 0  which in fact follow from
the deﬁnitions. Condition (c) follows since  by a standard argument (e.g.  Borkar & Meyn  2000) 
wk(θ) converges to wθ  which by assumption stays bounded if θ comes from a bounded set. For
condition (d)  note that {θk} is uniformly bounded since for any k ≥ 0  θk ∈ C  and by assumption
C is a compact set.
Theorem 3 (Convergence of nonlinear TDC). Under the same conditions as in Theorem 2  the
iterates computed via (13)  (15) satisfy θk → K  with probability one  as k → ∞.
The proof follows in a similar manner as that of Theorem 2 and is omitted for brevity.
6 Empirical results
To illustrate the convergence properties of the algorithms  we applied them to the “spiral” coun-
terexample of Tsitsikilis & Van Roy (1997)  originally used to show the divergence of TD(0) with
nonlinear function approximation. The Markov chain with 3 states is shown in the left panel of
Figure 2. The reward is always zero and the discount factor is γ = 0.9. The value function has a sin-

gle parameter  θ  and takes the nonlinear spiral form Vθ(s) =￿a(s) cos (ˆλθ) − b(s) sin (ˆλθ)￿ e￿θ.
The true value function is V = (0  0  0)￿ which is achieved as θ → −∞. Here we used
V0 = (100 −70 −30)￿  a = V0  b = (23.094 −98.15  75.056)￿  ˆλ = 0.866 and ￿ = 0.05.
Note that this is a degenerate example  in which our theorems do not apply  because the optimal
parameter values are inﬁnite. Hence  we run our algorithms without a projection step. We also use
constant learning rates  in order to facilitate gradient descent through an error surface which is es-
sentially ﬂat. For TDC we used α = 0.5  β = 0.05  and for GTD2  α = 0.8 and β = 0.1. For TD(0)
we used α = 2× 10−3 (as argued by Tsitsiklis & Van Roy (1997)  tuning the step-size does not help
dθ Vθ￿. The graph shows
with the divergence problem). All step sizes are then normalized by ￿V ￿θ D d
the performance measure  √J  as a function of the number of updates (we used expected updates
for all algorithms). GTD2 and TDC converge to the correct solution  while TD(0) diverges. We note
that convergence happens despite the fact that this example is outside the scope of the theory.
To assess the performance of the new algorithms on a large scale problem  we used them to learn
an evaluation function in 9x9 computer Go. We used a version of RLGO (Silver  2009)  in which a
logistic function is ﬁt to evaluate the probability of winning from a given position. Positions were
described using 969 894 binary features corresponding to all possible shapes in every 3x3  2x2  and
1x1 region of the board. Using weight sharing to take advantage of symmetries  the million features
were reduced to a parameter vector of n = 63  303 components. Experience was generated by self-
play  with actions chosen uniformly randomly among the legal moves. All rewards were zero  except
upon winning the game  when the reward was 1. We applied four algorithms to this problem: TD(0) 
the proposed algorithms (GTD2 and TDC) and residual gradient (RG). In the experiments  RG was

7

15

10

J
√

5

0
0

TDC

TD

0.60

0.55

RMSE

TDC

RG

TD

GTD2

TDC

TDC

RG

TD

.00001

.0001

.001

α

.01

.1

GTD2

1
2

1
2

2

1
2
1
2

1

1
2

3

1
2

1000

Time step

2000

3000

0.50

0.45

0.40

Figure 2: Empirical evaluation results. Left panel: example MDP from Tsitsiklis & Van Roy (1994).
Right panel: 9x9 Computer Go.
run with only one sample3. In each run  θ was initialized to random values uniformly distributed in
[−0.1  0.1]; for GTD2 and TDC  the second parameter vector  w  was initialized to 0. Training then
proceeded for 5000 complete games  after which θ was frozen. This problem is too large to compute
the objective function J. Instead  to assess the quality of the solutions obtained  we estimated the
average prediction error of each algorithm. More precisely  we generated 2500 test games; for
every state occurring in a game  we computed the squared error between its predicted value and the
actual return that was obtained in that game. We then computed the root of the mean-squared error 
averaged over all time steps. The right panel in Figure 2 plots this measure over a range of values of
the learning rate α. The results are averages over 50 independent runs. For TDC and GTD we used
several values of the β parameter  which generate the different curves. As was noted in previous
empirical work  TD provides slightly better estimates than the RG algorithm. TDC’s performance
is very similar to TD  for a wide range of parameter values. GTD2 is slightly worse. These results
are very similar in ﬂavor to those obtained in Sutton et al. (2009b) using the same domain  but with
linear function approximation.
7 Conclusions and future work
In this paper  we solved a long-standing open problem in reinforcement learning  by establishing a
family of temporal-difference learning algorithms that converge with arbitrary differentiable func-
tion approximators (including neural networks). The algorithms perform gradient descent on a nat-
ural objective function  the projected Bellman error. The local optima of this function coincide with
solutions that could be obtained by TD(0). Of course  TD(0) need not converge with non-linear
function approximation. Our algorithms are on-line  incremental and their computational cost per
update is linear in the number of parameters. Our theoretical results guarantee convergence to a
local optimum  under standard technical assumptions. Local optimality is the best one can hope
for  since nonlinear function approximation creates non-convex optimization problems. The early
empirical results obtained for computer Go are very promising. However  more practical experience
with these algorithms is needed. We are currently working on extensions of these algorithms using
eligibility traces  and on using them for solving control problems.
Acknowledgments
This research was supported in part by NSERC  iCore  AICML and AIF. We thank the three anonymous re-
viewers for their useful comments on previous drafts of this paper.

3Unlike TD  RG would require two independent transition samples from a given state. This requires knowl-
edge about the model of the environment which is not always available. In the experiments only one transition
sample was used following Baird’s original recommendation.

8

References
Antos  A.  Szepesv´ari  Cs. & Munos  R. (2008). Learning near-optimal policies with Bellman-
residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning 71:
89–129.
Baird  L. C. (1995). Residual algorithms: Reinforcement learning with function approximation.
In Proceedings of the Twelfth International Conference on Machine Learning  pp. 30–37. Morgan
Kaufmann.
Borkar  V. S. & Meyn  S. P. (2000). The ODE method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control And Optimization 38(2): 447–469.
Boyan  J. A. & Moore  A.W. (1995). Generalization in Reinforcement Learning: Safely Approxi-
mating the Value Function. In Advances in Neural Information Processing Systems 7  pp. 369–376 
MIT Press.
Crites  R. H. & Barto  A.G. (1995). Improving Elevator Performance Using Reinforcement Learning
In Advances in Neural Information Processing Systems 8  pp. 1017-1023. MIT Press.
Farahmand  A.m.  Ghavamzadeh  M.  Szepesvari  C. & Mannor  S. (2009). Regularized Policy
Iteration In Advances in Neural Information Processing Systems 21  pp. 441–448.
Kushner  H. J. & Yin  G. G. (2003). Stochastic Approximation Algorithms and Applications. Second
Edition  Springer-Verlag.
Pearlmutter  B. A (1994). Fast exact multiplication by the Hessian. Neural Computation 6(1) 
pp. 147–160.
Silver  D. (2009). Reinforcement Learning and Simulation-Based Search in Computer Go. Univer-
sity of Alberta Ph.D. thesis.
Sutton  R. S. (1988). Learning to predict by the method of temporal differences. Machine Learning
3:9–44.
Sutton  R. S. & Barto  A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
Sutton  R. S.  Szepesv´ari  Cs. & Maei  H. R. (2009a). A convergent O(n) algorithm for off-policy
temporal-difference learning with linear function approximation. In Advances in Neural Information
Processing Systems 21  pp. 1609–1616. MIT Press.
Sutton  R. S.  Maei  H. R  Precup  D.  Bhatnagar  S.  Silver  D.  Szepesv´ari  Cs. & Wiewiora 
E. (2009b). Fast gradient-descent methods for temporal-difference learning with linear function
approximation.
In Proceedings of the 26th International Conference on Machine Learning  pp.
993–1000. Omnipress.
Tesauro  G. (1992) Practical issues in temporal difference learning. Machine Learning 8: 257-277.
Tsitsiklis  J. N. & Van Roy  B. (1997). An analysis of temporal-difference learning with function
approximation. IEEE Transactions on Automatic Control 42:674–690.
Zhang  W. & Dietterich  T. G. (1995) A reinforcement learning approach to job-shop scheduling. In
Proceedings of the Fourteenth International Joint Conference on Artiﬁcial Intelligence  pp. 1114-
1120. AAAI Press.

9

,Martin Royer