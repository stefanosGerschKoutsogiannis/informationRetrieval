2019,Policy Evaluation with Latent Confounders via Optimal Balance,Evaluating novel contextual bandit policies using logged data is crucial in applications where exploration is costly  such as medicine. But it usually relies on the assumption of no unobserved confounders  which is bound to fail in practice. We study the question of policy evaluation when we instead have proxies for the latent confounders and develop an importance weighting method that avoids fitting a latent outcome regression model. Surprisingly  we show that there exist no single set of weights that give unbiased evaluation regardless of outcome model  unlike the case with no unobserved confounders where density ratios are sufficient. Instead  we propose an adversarial objective and weights that minimize it  ensuring sufficient balance in the latent confounders regardless of outcome model. We develop theory characterizing the consistency of our method and tractable algorithms for it. Empirical results validate the power of our method when confounders are latent.,Policy Evaluation with Latent Confounders via

Optimal Balance

Andrew Bennett˚
Cornell University

awb222@cornell.edu

Nathan Kallus˚
Cornell University

kallus@cornell.edu

Abstract

Evaluating novel contextual bandit policies using logged data is crucial in appli-
cations where exploration is costly  such as medicine. But it usually relies on the
assumption of no unobserved confounders  which is bound to fail in practice. We
study the question of policy evaluation when we instead have proxies for the latent
confounders and develop an importance weighting method that avoids ﬁtting a
latent outcome regression model. We show that unlike the unconfounded case
no single set of weights can give unbiased evaluation for all outcome models 
yet we propose a new algorithm that can still provably guarantee consistency by
instead minimizing an adversarial balance objective. We further develop tractable
algorithms for optimizing this objective and demonstrate empirically the power of
our method when confounders are latent.

1

Introduction

Personalized intervention policies are of increasing importance in education [32]  healthcare [3] 
and public policy [26]. In many of these domains exploration is costly or otherwise prohibitive 
and so it is crucial to evaluate new policies using existing observational data. Usually  this relies
on an assumption of no unobserved confounding (aka unconfoundedness or ignorability): that
conditioned on observables  interventions are independent of idiosyncrasies that affect outcomes 
so that counterfactuals can be reliably and correctly predicted. In particular  this enables the use
of inverse propensity score (IPS) estimators of policy value [4  21  29] that eschew the need to
actually ﬁt outcome prediction models and doubly robust estimators that work even if such models
are misspeciﬁed [8].
In practice  however  it may be unlikely that we observe confounders exactly. Nonetheless  if we
observe very many features they may serve as good proxies for the true confounders  which can enable
an alternative route to identiﬁcation [22  30]. In particular  noisy observations of true confounders can
serve as valid proxies. For example  if intelligence is latent but affects both selection and outcome  we
can instead use many noisy observations of intelligence such as school grades  IQ test  etc. Similarly 
many medical measurements taken together can serve as proxies for underlying healthfulness.
In this paper  we study the problem of policy evaluation from observational data where we observe
proxies instead of true confounders and we develop new weighting estimators based on optimizing
balance in the latent confounders. Unlike the unconfounded setting where IPS weights ensure balance
regardless of outcome model  we show that in this new setting there cannot exist any single of weights
that ensure such unbiasedness regardless of outcome model. Instead  we develop an adversarial
objective that bounds the conditional mean square error (CMSE) of any weighted estimator and 
by appealing to game theoretic and empirical process arguments  we show that this objective can
actually be driven to zero by a single set of weights. We therefore propose a novel policy evaluation

˚Alphabetical order.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

method that minimizes this objective  thus provably ensuring consistent estimation in the face of
latent confounders. We develop tractable algorithms for this optimization problem. Finally  we
provide empirical evidence demonstrating our method’s consistent evaluation compared to standard
evaluation methods and its improved performance compared to using ﬁtted latent outcome models.

2 Problem

2.1 Setting and Assumptions
We consider a contextual decision making setting with m possible treatments (aka actions or interven-
tions). Each unit is associated with a set of potential outcomes Y p1q  . . .   Y pmq P R corresponding
to the reward/loss for each treatment  an observed treatment T P t1  . . .   mu  an observed outcome
Y “ Y pTq  true but latent confounders Z P Z Ñ Rp  and observed covariates X P X Ñ Rq. Our
data consists of iid observations Xi  Ti  Yi of X  T  Y . Both the latent confounders and potential out-
comes of unassigned treatments are unobserved. Note that Yi “ YipTiq encapsulates the assumptions
of consistency between observed and potential outcomes and non-interference between units.
A policy is a rule for assigning the probability of each treatment option given the observed covariates
X. Given a policy ⇡  we use the notation ⇡tpxq to indicate the probability of assigning treatment t
when observed covariates are x. We deﬁne the value of a policy  ⌧ ⇡  as the expected outcome that
would be obtained from following the policy in the population. Formally:
Deﬁnition 1 (Policy Value). ⌧ ⇡ “ Er∞m

We encapsulate the assumption that Z are sufﬁcient for unconfoundedness
and that X is a proxy for Z in the following assumption. Figure 1 provides
a representation of this setting using a causal DAG [36]. Note importantly
that we do not assume ignorability given X.
Assumption 1 (Z are true confounders). For every t P t1  . . .   mu  Y ptq is
independant of pX  Tq  given Z.
We next deﬁne the average mean outcome given Z and its conditional expectations given observables:

Figure 1: DAG repre-
sentation of problem.

t“1 ⇡tpXqY ptqs.

T

X

Z

Y

µtpzq “ ErY ptq | Z “ zs 
⌫tpx  t1q “ ErµtpZq | X “ x  T “ t1s “ ErY ptq | X “ x  T “ t1s 

⇢tpxq “ ErµtpZq | X “ xs “ ErY ptq | X “ xs.

We further deﬁne the propensity function and its conditional expectation given observables:

etpzq “ PpT “ t | Z “ zq  
⌘tpxq “ PpT “ t | X “ xq “ EretpZq | X “ xs .

Finally  we denote by 'pz; x  tq the conditional density of Z given X “ x  T “ t. This density
represents the latent variable model underlying the observables. For example  this can be a Gaussian
mixture model  a PCA-type model as in [22]  or a deep variational autoencoder as in [30]. Because
we focus on how one might use such a latent model rather than the estimation of this model  we just

assume we have some approximate oracle ˆ' for calculating its values  such that ˆ' “ ' ` Opp1{?nq
in L1. (Note that for fair comparison  in experiments in Section 5  we similarly let the outcome
regression methods use this oracle.)
We further make the following regularity assumptions:
Assumption 2 (Weak Overlap). E“e´2
Remark 1. Given Assumption 2 it trivially follows that for every t P t1  . . .   mu  x P X   z P Z that
etpzq° 0 and ⌘tpxq° 0.
Assumption 3 (Bounded Variance). The conditional variance of our potential outcomes given X  T
is bounded: VrY ptq | X  Ts§ 2.
2.2 The Policy Evaluation Task
The problem we consider is to estimate the policy value ⌧ ⇡ given a policy ⇡ and data X1:n  T1:n  Y1:n.
One standard approach to this is the direct method [38]  which given an estimate ˆ⇢t of ⇢t predicts the

t pZq‰ †8

2

policy value as

ˆ⇢ “ 1
ˆ⌧ ⇡

n

nÿi“1

mÿt“1

⇡tpXiqˆ⇢tpXiq.

(1)

However this method is known to be biased and doesn’t generalize well [4]. Furthermore given
Assumption 1 ˆ⇢ is not straightforward to estimate  since the mean value of Y observed in our logged
data given X “ x and T “ t is ⌫tpx  tq not ⇢tpxq  so ﬁtting ˆ⇢ would require controlling for the
effects of the unobserved Z.
An alternative to this is to come up with weights W1:n “ fWpX1:n  T1:nq according to some function
fW of the observed covariates and treatments  in order to re-weight the outcomes to look more like
those that would be observed under ⇡. Using these weights we can deﬁne the weighted estimator

W “ 1
ˆ⌧ ⇡

n

nÿi“1

WiYi.

(2)

This weighted estimator has the advantage that it does not require modeling the outcome distributions.
Furthermore we could combine the weights W1:n with an outcome model ˆ⇢t to calculate the doubly
robust estimator [8]  which is deﬁned as

W ˆ⇢ “ 1
ˆ⌧ ⇡

n

nÿi“1

mÿt“1

⇡tpXiqˆ⇢tpXiq ` 1

n

nÿi“1

WipYi ´ ˆ⇢TipXiqq.

(3)

The doubly robust estimator is known to be consistent when either the weighted or direct estimator is
consistent and can attain local efﬁciency [19  39].
Various approaches exist for coming up with weights for either the weighted or doubly robust estima-
tors  which we discuss below. However none of these methods are applicable given Assumption 1 
and so we develop a theory for weighting using proxy variables in Section 3.

2.3 Related Work
One of the most standard approaches for policy evaluation is using the weighted or doubly robust
estimator deﬁned in Eqs. (2) and (3)  using inverse propensity score (IPS) weights. These are given
by Wi “ ⇡TipXiq{eTipZiq [5]  where et are known or estimated logging probabilities. Since these
weights can be extreme  both normalization [2  31  41] and clipping [10  12  40] are often employed.
In addition some other approaches include recursive partitioning [14]. None of these methods are
applicable to our setting however  since we do not know the true confounders Z1:n.
An alternative to approaches based on ﬁxed formulae for computing the importance weights is to
compute weights that optimize an imbalance objective function [1  15  17  18]. For policy evaluation 
Kallus [16] propose to choose weights that adversarially minimize the conditional mean squared
error of policy evaluation in the worst case of possible mean outcome functions in some reproducing
kernel Hilbert space (RKHS)  by solving a linearly constraint quadratic program (LCQP). Our work
follows a very similar style to this  however instead of using the true confounders we only assume
access to proxies  and we prove our theory for more general families of functions. This  we show 
provides a unique imperative – separate from stability and variance control – to obtain importance
weights via optimal balancing: while no single unbiased importance weights exist when we have
proxies instead of true confounders  optimal balancing obtains weights in a model-robust manner that
ensures consistency.
Finally there has been a long history of work in causal inference using proxies for true confounders
[11  42]. As in our problem setup  much of this work is based on the model of using an identiﬁed latent
variable model for the proxies [9  27  34  37  43]. Some recent work on this problem involves using
techniques such as matrix completion [22] or variational autoencoders [30] to infer confounders from
the proxies. Additionally  there is recent work on robustness to unidentiﬁability due to unobserved
confounding [20  23  24]; in contrast we focus on a setting where  while confounders are unobserved 
effects are identiﬁable via proxies. In particular  there is a variety of work that studies sufﬁcient
conditions for the identiﬁability of latent confounder models [6  34  37]. Our work is complementary
to this line of research in that we assume access to an accurate latent confounder model  but do not
study how to estimate such models. Furthermore our work is novel in combining proxy variable
models with optimal balancing and applying it to ﬁnding importance weights for policy evaluation.

3

3 Weight-Balancing Objectives

Infeasibility of IPS-Style Unbiased Weighting

3.1
If we had unconfoundedness given X (i.e.  Y ptq KK T | X)  the IPS weights ⇡TpXq{⌘TpXq are
immediately gotten as the solution to making every term in the weighted sum Eq. (2) unbiased:
(4)
Notably the IPS weights do not depend on the outcome function. However  without unconfoundedness
given X and given only Assumptions 1 to 3  this approach fails.
Theorem 1. If Wpx  tq satisﬁes Eq. (4) then for any t P t1  . . .   mu

ErWpX  TqTitY ptqs “ Er⇡tpXqY ptqs.

WpX  tq “ ⇡tpXq∞t1P⌧ ⌘t1pXq⌫tpX  t1q ` ⌦tpXq

⌘tpXq⌫tpX  tq

 

(5)

for some ⌦tpxq such that E⌦tpXq “ 0@t.
The proof of Theorem 1 is given in Appendix A.1.
Note that if we had unconfoundedness given X then ⌫tpx  t1q “ ⌫tpx  tq “ ⇢tpxq so that choosing
⌦tpXq “ 0 would recover the standard IPS weights. However  in our setting we generally have
⌫tpx  t1q ‰ ⌫tpx  tq  and so Theorem 1 tells us that we cannot do unbiased IPS-style weighted
evaluation without knowing the mean outcome functions ⌫tpx  t1q. In particular  there exists no single
weight function that is simultaneously unbiased for all outcome functions.
On the other hand  Theorem 1 tells us that there do exist some weights that give unbiased and
consistent policy evaluation via Eq. (2) or Eq. (3): we just may not be able to calculate them. The
existence of such weights motivates our subsequent approach  which seeks weights that mimic these
weights for a wide class of possible outcome functions.

3.2 Adversarial Error Objective

Erpˆ⌧ ⇡

Over all weights that are functions of X1:n  T1:n  the optimal choice of weights for estimating ⌧ ⇡ via
Eq. (2) would minimize the (unknown) conditional MSE (CMSE):
W ´ ⌧ ⇡q2 | X1:n  T1:ns.

(6)
In particular  the weights in Eq. (5) achieve Opp1{nq control on this CMSE for many outcome
functions  as long as the denominator is well behaved  which can be seen by applying concentration
inequalities to Eq. (6). However  as discussed above the outcome function is unknown and these
weights are therefore practically infeasible. Our aim is to ﬁnd weights with similar near-optimal
behavior but that do not depend on the particular unknown outcome function. To do this  we will ﬁnd
an upper bound for Eq. (6) that we can actually compute.
Let fit “ WiTit ´ ⇡tpXiq and

J˚pW  µq “˜ 1

n

nÿi“1

mÿt“1

fit⌫tpXi  Tiq¸2

` 22

n2 }W}2
2 

W ´ ⌧ ⇡q2 | X1:n  T1:ns§ 2J˚pW  µq ` Opp1{nq.

where we embedded the dependence on µ inside ⌫tpx  t1q “ ErµtpZq | X “ x  T “ t1s.
Theorem 2. Erpˆ⌧ ⇡
Note that J˚ above is deﬁned in terms of the true posterior '  which is infeasible to compute in
practice. Therefore we deﬁne J by replacing the conditional measure of Z given X and T used
to compute ⌫t in J˚ with the approximate measure given by ˆ'. Note that in the remainder of this
section and corresponding proofs we make slight abuse of notation by deﬁning J in terms of ⌫t; these
⌫t terms should be interpreted below as deﬁned in terms of the approximate conditional measure
given by ˆ'. Then applying the fact that ˆ' “ ' ` Opp1{?nq in L1  we obtain the following corollary
which trivially follows from Theorem 2 and Slutsky’s theorem:
Corallary 1. Erpˆ⌧ ⇡

W ´ ⌧ ⇡q2 | X1:n  T1:ns§ 2JpW  µq ` Opp1{?nqaJpW  µq ` Opp1{nq.

4

Therefore  if we ﬁnd weights that obtain Opp1{nq control on JpW  µq  we can ensure that we also
have Opp1{nq control on Erpˆ⌧ ⇡
W ´ ⌧ ⇡q2 | X1:n  T1:ns. Combined with the following result  which
follows from [13  Lemma 31]  this would give root-n consistent estimation.
Lemma 1. If Erpˆ⌧ ⇡
It remains to ﬁnd weights that control JpW  µq. The key obstacle for this is that µ is unknown. Instead 
we show how we can obtain weights that control JpW  µq over a whole class of given functions µ.
Suppose we are given a set F of functions mapping Z to Rm  where each µ P F corresponds to a
vector of mean outcome functions µ “ pµ1  . . .   µmq. Then  motivated by Theorem 2 and Lemma 1 
we deﬁne our adversarial optimization problem as

W ´ ⌧ ⇡q2 | X1:n  T1:ns “ Opp1{nq then ˆ⌧ ⇡

W “ ⌧ ⇡ ` Opp1{?nq.

W˚ “ arg min
WPW

sup
µPF

JpW  µq.

(7)

One question the reader might ask at this point is why not solve the above optimization problem
by ignoring the hidden confounders and directly balancing the conditional mean outcome functions
⌫tpx  t1q? The problem is that this would be impossible to do over any kind of generic ﬂexible
function space  since we have no data corresponding to terms in the form ⌫tpx  t1q when t ‰ t1  so
this is akin to an overlap problem. Conversely  if we were to ignore the conditioning on t and balance
against functions of the form ⌫tpxq “ ⌫tpx  tq this would be inadequate  as we couldn’t hope for such
a space to cover the true µ since we don’t assume ignorability given Z.
In light of these limitations  we can view what we are doing in optimizing Eq. (7)  using an identiﬁed
model ˆ'pz; x  tq  as implicitly balancing some controlled space of functions ⌫tpx  t1q that do not have
this overlap issue between different t values. The following lemma makes this explicit  as it implies
that that the terms ⌫tpx  t1q are all mutually bounded by each other for ﬁxed x and t:
Lemma 2. Assuming }µt}8 § b  under Assumption 2  for all x P X   and t  t1  t2 P t1  . . .   mu we
have
t pZq | X “ x  T “ t1‰|⌫tpx  t1q|.

⌘t2pxqb8bE“e´2

|⌫tpx  t2q| § ⌘t1pxq

3.3 Consistency of Adversarially Optimal Estimator
Now we analyze the consistency of our weighted estimator based on Eq. (7). Given Lemma 1  all we
need to justify to prove consistency is that µ P F and that inf W supµPF JpW  µq P Opp 1
nq. Deﬁne Ft
as the space of all functions for treatment level t allowed by F. That is Ft “ tµt : Dpµ11  . . .   µ1mq P
F with µ1t “ µtu. We will use the following assumptions about F to prove control of J:
Assumption 4 (Normed). For each t P t1  . . .   mu there exists a norm }¨}t on spanpFtq  and there ex-
ists a norm }¨} on spanpFq which is deﬁned given some Rm norm as }µ} “ }p}µ1}1  . . .  }µm}mq}.
Assumption 5 (Absolutely Star Shaped). For every µ P F and ||§ 1  we have µ P F.
Assumption 6 (Convex Compact). F is convex and compact
Assumption 7 (Square Integrable). For each t P t1  . . .   mu the space Ft is a subset of L2pZq  and
its norm dominates the L2 norm (i.e.  inf µtPFt }µt}{}µt}L2 ° 0).
Assumption 8 (Nondegeneracy). Deﬁne Bpq “ tµ P spanpFq : }µ}§ u. Then we have
BpqÑ F for some  ° 0.
Assumption 9 (Boundedness). supµPF }µ}8 †8 .
Deﬁnition 2 (Rademacher Complexity). RnpFq “ ErsupfPF
Rademacher random variables.
Assumption 10 (Complexity). For each t P t1 . . .   mu we have RnpFtq “ op1q.
These assumptions are satisﬁed for many commonly-used families of functions  such as RKHSs and
families of neural networks. We shall prove this claim for RKHSs in Section 4.
In order to justify that we can control J  ﬁrst we will show that these assumptions allow us to reverse
the order of minimization and maximization in our optimization problem. This means we can reduce
the problem to ﬁnding weights to control any particular µ rather than controlling all of F.

n∞n
i“1 ✏ifpZiqs  where ✏i are iid

1

5

n∞n
i“1∞m
Lemma 3. Let BpW  µq “ 1
M ° 0 we have the bound
JpW  µq§ sup
min
W
µPF

sup
µPF

t“1 fit⌫tpXi  Tiq. Then under Assumptions 5 to 7 for every

BpW  µq2 ` 2

n2 M 2.

min

}W}2§M

Next  we note that Lemma 3 means that we can choose weights given µ to set BpW  µq “ 0  and
therefore we have our desired control as long as we can justify that these weights have controlled
euclidean norm. Using this strategy and optimizing for the minimum norm weights of this kind  we
are able to prove the following:
Lemma 4. Under Assumptions 4 to 10 we have inf W supµPF JpW  µq “ Opp1{nq.
This is the key lemma in proving our main consistency theorem:
W ˚ “ ⌧ ⇡ ` Opp1{?nq.
Theorem 3. Under Assumptions 4 to 10 and assuming that µ P F we have ˆ⌧ ⇡
This theorem follows immediately from our previous results  since µ P F and Lemma 4 imply that
JpW ˚  µq “ Opp1{nq. This combined with Corollary 1 imply that Erpˆ⌧ ⇡
W ˚ ´ ⌧ ⇡q2 | X1:n  T1:ns “
Opp1{nq  which in turn combined with Lemma 1 gives us our result. Furthermore  given some
additional assumptions  we can take advantage of L8 universal approximation of µ to obtain the
following corollary which does not depend on the assumption that µ P F:
Assumption 11 (Continuous Mean Outcome). For each t  µt is a continuous function of Z.
Assumption 12 (Universal Approximation). Fm is a universally approximating sequence of function
classes. That is  for every vector of continuous functions µ and every m  there exists f P Fm such
that }ft ´ µt}8 § ✏m for each t  and ✏m Ñ 0.
Corallary 2. Under Assumptions 4 to 12  and using a universally approximating sequence of function
classes Fn to compute W ˚  we have ˆ⌧ ⇡
This corollary follows from observing that by the universally approximating property of Fn  we can
obtain an Opp1{?nq ` ✏ error bound for every ✏ ° 0 (where the Op term’s constants can depend
on ✏). Thus Opp1q error bound follows trivially. Note that the universal approximation property of
Assumption 12 is obtainable for many classes of functions such as Gaussian RKHSs [35].

W ˚ “ ⌧ ⇡ ` Opp1q.

4 Algorithms for Optimal Kernel Balancing

K.
t“1 ||µt||2

4.1 Kernel Function Class
We now provide an algorithm for optimal balancing when our function class consists of vectors of
RKHS functions. Formally  given a kernel K and corresponding RKHS norm } ¨ }K  we deﬁne the
space F K as follows:

Deﬁnition 3 (Kernel Class). F K “ tµ : ||µ|| § 1u  where ||pµ1  . . .   µmq|| “a∞m
Theorem 4. Assuming K is a Mercer kernel [44] and is bounded  F K satisﬁes Assumptions 4 to 10.
We can remark that the commonly used Gaussian kernel is both Mercer and bounded  so it satisﬁes the
conditions of Theorem 4. Given this  and assuming that F K covers the real mean outcome function
µ  we can apply Theorem 3 to see that solving Eq. (7) using F K gives consistent evaluation.
Note that the F K having maximum norm 1 is without loss of generality  because if we wanted the
maximum norm to instead be m we could replace the standard deviation  by {m in our objective
function  resulting in an equivalent re-scaled optimization problem. To make this explicit  we will
replace  in the objective with   where it is assumed that  is a freely chosen hyperparameter to
allow for varying regularization. For ease of notation below we deﬁne  to be the diagonal matrix
such that ii “  for every i.
4.2 Kernel Balancing Algorithm
In order to optimize Eq. (7) over a class of kernel functions as deﬁned by Deﬁnition 3  we can
observe that the deﬁnition of JpW  µq looks very similar to the adversarial objective of Kallus [16] 

6

except that we have ⌫tpXi  Tiq terms instead of µtpXiq terms. This motivates the idea that  given
our identiﬁed posterior model ˆ'pz; x  tq  we may be able to employ a similar quadratic programming
(QP)-based approach. The following theorem makes this explicit  by deﬁning a QP objective for W
that we can approximate by sampling from ˆ':
Theorem 5. Deﬁne Qij “ ErKpZi  Z1jqs  Gij “ 1
n2pQijTiTj ` ijq  and ai “
n2∞n
j“1 Qij⇡TjpXiq  where for each i Zi and Z1i are iid shadow variables  and the expectation is
2
deﬁned condional on the observed data using the approximate posterior ˆ'. Then for some c that is
constant in W we have the identity

sup
µPF K

JpW  µq “ W T GW ´ aT W ` c.

Given this our balancing algorithm is natural and straightforward  and is summarized by Algorithm 1.
Note that we provide an optional weight space constraint W in this algorithm  since standard weighted
estimator approaches for policy evaluation regularize by forcing constraints such as W P nn. Under
this kind of constraint our unconstrained QP becomes a LCQP. However our theory does not support
this constraint  and we ﬁnd that it hurts performance in practice  especially when  is large  so we do
not use this constraint in our main experiments.

  number samples B  optional weight space W (defaults to Rn if not provided)

Algorithm 1 Optimal Kernel Balancing
Input: Data pX1:n  T1:nq  policy ⇡  kernel function K  posterior density ˆ'  regularization matrix
Output: Optimal balancing weights W1:n
1: for i P t1  . . .   nu do
i from the posterior ˆ'p¨ ; Xi  Tiq
2:
3: end for
b“1∞B
4: Estimate Q. Calculate Qij “ 1
c“1 KpZb
5: Calculate QP Inputs. Calculate Gij “ QijTiTj ` ij  and ai “ 2∞n
6: Solve Quadratic Program. Calculate W “ arg minWPW W T GW ´ aT W

Sample Data. Draw B data points Zb

j“1 Qij⇡TjpXiq 

B2∞B

i q
i   Zc

5 Experiments

5.1 Experimental Setup
We now present a brief set of experiments to explore our methodology. The aim of these experiments
is to be a proof of concept of our theory. We seek to show that given an identiﬁed posterior model ˆ'
policy as discussed in Section 2.1  evaluation using the weights deﬁned by Eq. (7) can give unbiased
policy evaluation even in the face of sufﬁciently strong confounding where standard benchmark
approaches that rely on ignorability given X fail. We experiment with the following generalized
linear model-style scenario:

Z „ Np0  1q
T „ softmaxpPTq

X „ Np↵T Z ` ↵0  Xq
Wptq „ Np⇣ptqT Z ` ⇣0ptq  Y q

PT “ T Z ` 0
Y ptq “ gpWptqq

In our experiments Z is 1-dimensional  X is 10-dimensional  and we have two possible treatment
levels (m “ 2). We experiment with a parametric policy ⇡ and multiple link functions g as follows:

t Xq

expp T
1 Xq ` expp T

2 Xq

⇡tpXq “
exp: gpwq “ exppwq

expp T

step: gpwq “ 3 tw•0u´6
We experiment with the following methods in this evaluation:

cubic: gpwq “ w3

linear: gpwq “ w

1. OptZ Our method  using  “  Identitypnq for  P t0.001  0.2  1.0  5.0u.
2. IPS IPS weights based on X using estimated ˆ⌘t.
3. OptX The optimal weighting method of Kallus [16] with same values of  as our method.

7

4. DirX Direct method by ﬁtting ˆ⇢tpxq incorrectly assuming ignorability given X.
5. DirZ Direct method by ﬁrst ﬁtting ˆµt using posterior samples from ˆ'  then using the
estimate ˆ⇢tpxq “ p1{Dq∞D

6. D:W Doubly robust estimation using direct estimator D and weighted estimator W.

i“1 ˆµtpz1iq  where z1i are sampled from ˆ'p¨; x  tq.

Finally we detail all choices for scenario parameters in Appendix B.1  and provide implementation
details of our methods in Appendix B.2.2

5.2 Results

We display results for our experiments using the step link function in Tables 1 and 2. For each
of n P t200  500  1000  2000u we estimate the RMSE of policy evaluation using each method  as
well as doubly robust evaluation using our best performing weights  by averaging over 64 runs. In
addition  in Tables 3 and 4 we display the estimated bias from the evaluations. It is clear that the
naive methods that assume ignorability given X all hit a performance ceiling  where bias converges
to some non-zero value. In particular for IPS we separately ran it on up to one million data points
and found that the bias converged to 0.418 ˘ 0.001. On the other hand  for our method it appears
like we have consistency. This is particularly evident when we look at Table 3  as bias seems to be
approximately converging to zero with vanishing variance. We can also observe that doubly robust
estimation using either direct method does not appear to improve performance.
It is noteworthy that the DirZ benchmark method fails horribly  despite being a correctly speciﬁed
regression estimate. From our experience we observed that it is difﬁcult to train the µt functions
accurately if there is a high amount of overlap in the ˆ'p¨; x  tq posteriors for ﬁxed t. Therefore we
postulate that in highly confounded settings this benchmark is inherently difﬁcult to train using a
ﬁnite number of samples from ˆ'p¨; x  tq  and the result seems to collapse to degenerate solutions.
Next we note that we observed similar trends to this using our other link functions  and other doubly
robust estimators. We present more extensive tables of results in Appendix C.1. In addition we
present some results there on the negative impact on our method’s performance using the constraint
W P nn  as mentioned in Section 4.2.
Finally we provide some more detailed experiments investigating the impact of changing the dimen-
sionality of Z and the level of confounding by replacing Z with X in Appendix C.2 and Appendix C.3
respectively. In brief the results of these experiments are as expected: increasing the dimensionality
of Z gives the same overall pattern of results but with slower convergence  and increasing the level of
confounding strongly decreases the performance of benchmark methods  while our method appears
to maintain its unbiasedness.

6 Conclusion

We presented theory for how to do optimal balancing for policy evaluation when we only have
proxies for the true confounders  given an already identiﬁed model for the confounders  treatment 
and proxies  but not for the outcomes. We provided an adversarial objective for selecting optimal
weights given some class of mean outcome functions to be balanced  and proved that under mild
conditions these optimal weights result in consistent policy evaluation. In addition  we presented
a tractable algorithm for minimizing this objective when our function class is an RKHS  and we
conducted a series of experiments to demonstrate that our method can achieve consistent evaluation
even under sufﬁcient levels of confounding where standard approaches fail.
For future work we note that the adversarial objective and theory presented here is fairly general 
and could be used to develop new algorithms for balancing different function classes such as neural
networks. Indeed neural networks with weight decay easily satisfy the conditions of both Theorem 3
and Corollary 2  and thus it might be possible to learn balancing weights by optimizing a GAN-like
objective. An alternative direction would be to study how best to apply this methodology when an
identiﬁed model is not already given.

2Code available online at https://github.com/CausalML/LatentConfounderBalancing.

8

n
200
500
1000
2000

n
200
500
1000
2000

n
200
500
1000
2000

n
200
500
1000
2000

OptZ0.001
.39 ˘ .07
.19 ˘ .02
.11 ˘ .01
.08 ˘ .01
Table 1: Convergence of RMSE for for policy evaluation using our weights.

OptZ0.2
.24 ˘ .02
.18 ˘ .02
.11 ˘ .01
.08 ˘ .01

OptZ1.0
.36 ˘ .02
.23 ˘ .02
.13 ˘ .01
.09 ˘ .01

OptZ5.0
.81 ˘ .02
.49 ˘ .02
.27 ˘ .01
.17 ˘ .01

.57 ˘ .06
.55 ˘ .03
.49 ˘ .02
.48 ˘ .01

.41 ˘ .07
.20 ˘ .02
.11 ˘ .01
.08 ˘ .01

DirX:OptZ0.001 DirZ:OptZ0.001

IPS

.47 ˘ .03
.48 ˘ .03
.39 ˘ .02
.40 ˘ .01

OptX0.001
2.0 ˘ .03
2.0 ˘ .02
2.0 ˘ .01
2.0 ˘ .01

OptX0.2
2.1 ˘ .03
2.1 ˘ .02
2.1 ˘ .01
2.1 ˘ .01

OptX1.0
2.3 ˘ .02
2.3 ˘ .02
2.3 ˘ .01
2.3 ˘ .01

OptX5.0
2.5 ˘ .02
2.6 ˘ .02
2.5 ˘ .01
2.5 ˘ .01

DirX
.52 ˘ .02
.48 ˘ .02
.48 ˘ .02
.45 ˘ .02

DirZ
2.6 ˘ .02
2.6 ˘ .01
2.6 ˘ .01
2.6 ˘ .01

Table 2: Convergence of RMSE for benchmark methods.

OptZ0.001
.03 ˘ .39
.09 ˘ .17
.02 ˘ .11
.03 ˘ .07
Table 3: Convergence of bias for policy evaluation using our weights.

OptZ0.2
.11 ˘ .21
.10 ˘ .15
.05 ˘ .09
.05 ˘ .06

OptZ1.0
.29 ˘ .21
.17 ˘ .16
.08 ˘ .09
.07 ˘ .07

OptZ5.0
.78 ˘ .18
.47 ˘ .15
.25 ˘ .09
.16 ˘ .07

.43 ˘ .38
.51 ˘ .19
.47 ˘ .13
.47 ˘ .09

.05 ˘ .40
.10 ˘ .18
.04 ˘ .11
.03 ˘ .07

DirX:OptZ0.001 DirZ:OptZ0.001

IPS

.40 ˘ .25
.43 ˘ .21
.37 ˘ .12
.39 ˘ .10

OptX0.001
1.9 ˘ .21
2.0 ˘ .16
2.0 ˘ .10
2.0 ˘ .08

OptX0.2
2.1 ˘ .20
2.1 ˘ .15
2.1 ˘ .09
2.1 ˘ .07

OptX1.0
2.3 ˘ .19
2.3 ˘ .14
2.3 ˘ .09
2.3 ˘ .07

OptX5.0
2.5 ˘ .18
2.6 ˘ .13
2.5 ˘ .08
2.5 ˘ .07

DirX
.49 ˘ .18
.45 ˘ .16
.46 ˘ .15
.42 ˘ .17

DirZ
2.6 ˘ .14
2.6 ˘ .12
2.6 ˘ .11
2.6 ˘ .11

Table 4: Convergence of bias for benchmark methods.

Acknowledgements

This material is based upon work supported by the National Science Foundation under Grant No.
1846210. This research was funded in part by JPMorgan Chase & Co. Any views or opinions
expressed herein are solely those of the authors listed  and may differ from the views and opinions
expressed by JPMorgan Chase & Co. or its afﬁliates. This material is not a product of the Research
Department of J.P. Morgan Securities LLC. This material should not be construed as an individual
recommendation for any particular client and is not intended as a recommendation of particular
securities  ﬁnancial instruments or strategies for a particular client. This material does not constitute
a solicitation or offer in any jurisdiction.

References
[1] S. Athey  G. W. Imbens  and S. Wager. Approximate residual balancing: debiased inference of
average treatment effects in high dimensions. Journal of the Royal Statistical Society: Series B
(Statistical Methodology)  80(4):597–623  2018.

[2] P. C. Austin and E. A. Stuart. Moving towards best practice when using inverse probability of
treatment weighting (iptw) using the propensity score to estimate causal treatment effects in
observational studies. Statistics in medicine  34(28):3661–3679  2015.

[3] D. Bertsimas  N. Kallus  A. M. Weinstein  and Y. D. Zhuo. Personalized diabetes management

using electronic medical records. Diabetes care  40(2):210–217  2017.

[4] A. Beygelzimer and J. Langford. The offset tree for learning with partial labels. In Proceedings
of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining 
pages 129–138. ACM  2009.

9

[5] L. Bottou  J. Peters  J. Q. Candela  D. X. Charles  M. Chickering  E. Portugaly  D. Ray  P. Y.
Simard  and E. Snelson. Counterfactual reasoning and learning systems: the example of
computational advertising. Journal of Machine Learning Research  14(1):3207–3260  2013.

[6] Z. Cai and M. Kuroki. On identifying total effects in the presence of latent variables and
selection bias. In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artiﬁcial
Intelligence  pages 62–69. AUAI Press  2008.

[7] B. Carpenter  A. Gelman  M. D. Hoffman  D. Lee  B. Goodrich  M. Betancourt  M. Brubaker 
J. Guo  P. Li  and A. Riddell. Stan: A probabilistic programming language. Journal of statistical
software  76(1)  2017.

[8] M. Dudík  J. Langford  and L. Li. Doubly robust policy evaluation and learning. In Proceedings
of the 28th International Conference on International Conference on Machine Learning  pages
1097–1104. Omnipress  2011.

[9] J. K. Edwards  S. R. Cole  and D. Westreich. All your data are always missing: incorporating
bias due to measurement error into the potential outcomes framework. International journal of
epidemiology  44(4):1452–1459  2015.

[10] M. R. Elliott. Model averaging methods for weight trimming. Journal of ofﬁcial statistics  24

(4):517  2008.

[11] P. A. Frost. Proxy variables and speciﬁcation bias. The review of economics and Statistics 

pages 323–325  1979.

[12] E. L. Ionides. Truncated importance sampling. Journal of Computational and Graphical

Statistics  17(2):295–311  2008.

[13] N. Kallus. Generalized optimal matching methods for causal inference. arXiv preprint

arXiv:1612.08321  2016.

[14] N. Kallus. Recursive partitioning for personalization using observational data. In International

Conference on Machine Learning (ICML)  pages 1789–1798  2017.

[15] N. Kallus. A framework for optimal matching for causal inference. In Artiﬁcial Intelligence

and Statistics (AISTATS)  pages 372–381  2017.

[16] N. Kallus. Balanced policy evaluation and learning.

Processing Systems  pages 8895–8906  2018.

In Advances in Neural Information

[17] N. Kallus. Optimal a priori balance in the design of controlled experiments. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  80(1):85–112  2018.

[18] N. Kallus. Discussion: “entropy learning for dynamic treatment regimes”. Statistica Sinica  29

(4):1697–1705  2019.

[19] N. Kallus and M. Uehara. Intrinsically efﬁcient  stable  and bounded off-policy evaluation for

reinforcement learning. In Advances in Neural Information Processing Systems  2019.

[20] N. Kallus and A. Zhou. Confounding-robust policy improvement. In Advances in Neural

Information Processing Systems  pages 9269–9279  2018.

[21] N. Kallus and A. Zhou. Policy evaluation and optimization with continuous treatments. In

International Conference on Artiﬁcial Intelligence and Statistics  pages 1243–1251  2018.

[22] N. Kallus  X. Mao  and M. Udell. Causal inference with noisy and missing covariates via matrix
factorization. In Advances in Neural Information Processing Systems  pages 6921–6932  2018.
[23] N. Kallus  A. M. Puli  and U. Shalit. Removing hidden confounding by experimental grounding.

In Advances in Neural Information Processing Systems  pages 10888–10897  2018.

[24] N. Kallus  X. Mao  and A. Zhou. Interval estimation of individual-level causal effects under
unobserved confounding. In The 22nd International Conference on Artiﬁcial Intelligence and
Statistics  pages 2281–2290  2019.

10

[25] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[26] A. Kube  S. Das  and P. J. Fowler. Allocating interventions based on predicted outcomes: A
case study on homelessness services. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence  2019.

[27] M. Kuroki and J. Pearl. Measurement bias and effect restoration in causal inference. Biometrika 

101(2):423–437  2014.

[28] M. Ledoux and M. Talagrand. Probability in Banach Spaces: isoperimetry and processes.

Springer Science & Business Media  2013.

[29] L. Li  W. Chu  J. Langford  and X. Wang. Unbiased ofﬂine evaluation of contextual-bandit-
based news article recommendation algorithms. In Proceedings of the fourth ACM international
conference on Web search and data mining  pages 297–306. ACM  2011.

[30] C. Louizos  U. Shalit  J. M. Mooij  D. Sontag  R. Zemel  and M. Welling. Causal effect inference
with deep latent-variable models. In Advances in Neural Information Processing Systems  pages
6446–6456  2017.

[31] J. K. Lunceford and M. Davidian. Stratiﬁcation and weighting via the propensity score in
estimation of causal treatment effects: a comparative study. Statistics in medicine  23(19):
2937–2960  2004.

[32] T. Mandel  Y.-E. Liu  S. Levine  E. Brunskill  and Z. Popovic. Ofﬂine policy evaluation across
representations with applications to educational games. In Proceedings of the International
Conference on Autonomous Agents and Multi-agent Systems  pages 1077–1084. International
Foundation for Autonomous Agents and Multiagent Systems  2014.

[33] S. Mendelson. On the performance of kernel classes. Journal of Machine Learning Research  4

(Oct):759–771  2003.

[34] W. Miao  Z. Geng  and E. T. Tchetgen. Identifying causal effects with proxy variables of an

unmeasured confounder. arXiv preprint arXiv:1609.08816  2016.

[35] C. A. Micchelli  Y. Xu  and H. Zhang. Universal kernels. Journal of Machine Learning

Research  7(Dec):2651–2667  2006.

[36] J. Pearl. Causality: models  reasoning and inference. Cambridge University Press  2000.
[37] J. Pearl. On measurement bias in causal inference. arXiv preprint arXiv:1203.3504  2012.
[38] M. Qian and S. A. Murphy. Performance guarantees for individualized treatment rules. Annals

of statistics  39(2):1180  2011.

[39] J. M. Robins  A. Rotnitzky  and L. P. Zhao. Estimation of regression coefﬁcients when some
regressors are not always observed. Journal of the American statistical Association  89(427):
846–866  1994.

[40] A. Swaminathan and T. Joachims. Counterfactual risk minimization: Learning from logged

bandit feedback. In ICML  pages 814–823  2015.

[41] A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learning. In

Advances in Neural Information Processing Systems  pages 3231–3239  2015.

[42] M. R. Wickens. A note on the use of proxy variables. Econometrica: Journal of the Econometric

Society  pages 759–761  1972.

[43] J. M. Wooldridge. On estimating ﬁrm-level production functions using proxy variables to

control for unobservables. Economics Letters  104(3):112–114  2009.

[44] D.-X. Zhou. The covering number in learning theory. Journal of Complexity  18(3):739–767 

2002.

11

,Andrew Bennett
Nathan Kallus