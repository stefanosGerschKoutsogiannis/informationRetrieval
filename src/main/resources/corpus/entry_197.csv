2017,A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis,Existing strategies for finite-armed stochastic bandits mostly depend on a parameter of scale that must be known in advance. Sometimes this is in the form of a bound on the payoffs  or the knowledge of a variance or subgaussian parameter. The notable exceptions are the analysis of Gaussian bandits with unknown mean and variance by Cowan and Katehakis [2015a] and of uniform distributions with unknown support [Cowan and Katehakis  2015b]. The results derived in these specialised cases are generalised here to the non-parametric setup  where the learner knows only a bound on the kurtosis of the noise  which is a scale free measure of the extremity of outliers.,A Scale Free Algorithm for Stochastic Bandits with

Bounded Kurtosis

Tor Lattimore∗

tor.lattimore@gmail.com

Abstract

Existing strategies for ﬁnite-armed stochastic bandits mostly depend on a param-
eter of scale that must be known in advance. Sometimes this is in the form of a
bound on the payoffs  or the knowledge of a variance or subgaussian parameter.
The notable exceptions are the analysis of Gaussian bandits with unknown mean
and variance by Cowan et al. [2015] and of uniform distributions with unknown
support [Cowan and Katehakis  2015]. The results derived in these specialised
cases are generalised here to the non-parametric setup  where the learner knows
only a bound on the kurtosis of the noise  which is a scale free measure of the
extremity of outliers.

1

Introduction

SpaceBandits is a ﬁctional company that specialises in optimising the power output of satellite-
mounted solar panels. The data science team wants to use a bandit algorithm to adjust the knobs
on a legacy satellite  but they don’t remember the units of the sensors  and have limited knowledge
about the noise distribution of the panel output or sensors. The SpaceBandits data science team
searches the literature for an algorithm that does not depend on the scale or location of the means of
the arms  and ﬁnd this simple paper  in NIPS 2017.
It turns out that logarithmic regret is possible for ﬁnite-armed bandits with no assumptions on the
noise of the payoffs except for a known ﬁnite bound on the kurtosis  which corresponds to knowing
the likelihood/magnitude of outliers [DeCarlo  1997]. Importantly  the kurtosis is independent of
the location of the mean and scale of the central tendency (the variance). This generalises the ideas
of Cowan et al. [2015] beyond the Gaussian case with unknown mean and variance to the non-
parametric setting.
The setup is as follows. Let k ≥ 2 be the number of bandits (or arms). In each round 1 ≤ t ≤ n
the player should choose an action At ∈ {1  . . .   k} and subsequently receives a reward Xt ∼ νAt 
where ν1  . . .   νk are a set of distributions that are not known in advance. Let µi be the mean payoff
of the ith arm and µ∗ = maxi µi and ∆i = µ∗ − µi. The regret measures the expected deﬁcit of the
player relative to the optimal choice of distribution:

(cid:34) n(cid:88)

(cid:35)

Rn = E

∆At

.

(1)

t=1

The table below summarises many of the known results on the optimal achievable asymptotic regret
under different assumptions on (νi)i. A reference for each of the upper bounds is given in Table 1 
while the lower bounds are mostly due to Lai and Robbins [1985] and Burnetas and Katehakis
[1996]. An omission from the table is when the distributions are known to lie in a single-parameter
exponential family (which does not ﬁt well with the columns). Details are by Capp´e et al. [2013].

∗Now at DeepMind  London.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Assumption

1 Bernoulli

Lai and Robbins [1985]

2 Bounded

Honda and Takemura [2010]

3 Discrete

Burnetas and Katehakis [1996]

4

Semi-bounded
Honda and Takemura [2015]

5 Gaussian (known var.)
Katehakis and Robbins [1995]

6 Uniform

Cowan and Katehakis [2015]

7

Subgaussian
Bubeck and Cesa-Bianchi [2012]

8 Known variance
Bubeck et al. [2013]

Known
Supp(νi) ⊆ {0  1}

Supp(νi) ⊆ [0  1]

Supp(νi) ⊆ A
|A| < ∞
Supp(νi) ⊆ (−∞  1]

νi = N (µi  σ2
i )

νi = U(ai  bi)

distribution

µi ∈ R

ai  bi

log Mνi (λ) ≤ λ2σ2

i

2

∀λ

distribution

V[νi] ≤ σ2

i

distribution

Unknown
µi ∈ [0  1]

distribution

limn→∞ Rn/ log(n)

(cid:88)

1

d(µi  µ∗)

i:∆i>0
it’s complicated

distribution

it’s complicated

(cid:17)

it’s complicated

(cid:16)

i:∆i>0

i:∆i>0

2σ2
i
∆i

(cid:88)
(cid:88)
(cid:88)
 (cid:88)
(cid:88)

log
2σ2
i
∆i

i:∆i>0

i:∆i>0

O

∆i



2∆i

σ2
i
∆i

1 + 2∆i
bi−ai

9 Gaussian

νi = N (µi  σ2)

µi ∈ R  σ2

i > 0

Cowan et al. [2015]

i /σ2
i )
d(p  q) = p log(p/q) + (1 − p) log((1 − p)/(1 − q)) and Mν (λ) = EX∼ν exp((X − µ)λ) with µ the
mean of ν is the centered moment generating function. All asymptotic results are optimal except for the
grey cells.

log (1 + ∆2

i:∆i>0

Table 1: Typical distributional assumptions and asymptotic regret

With the exception of rows 6 and 9 in Table 1  all entries essentially depend on some kind of scale
parameter. Missing is an entry for a non-parametric assumption that is scale free. This paper ﬁlls
that gap with the following assumption and regret guarantee.
Assumption 1. There exists a known κ◦ ∈ R such that for all 1 ≤ i ≤ k  the kurtosis of X ∼ νi is
at most Kurt[X] = E[(X − E[X])4]/V[X]2 ≤ κ◦.
Theorem 2. If Assumption 1 holds  then the algorithm described in §2 satisﬁes

Rn
log(n)

≤ C

lim sup
n→∞

(cid:18)

(cid:88)

i:∆i>0

∆i

κ◦ − 1 +

σ2
i
∆2
i

(cid:19)

 

i is the variance of νi and C > 0 is a universal constant.

where σ2
What are the implications of this result? The ﬁrst point is that the algorithm in §2 is scale and
translation invariant in the sense that its behaviour does not change if the payoffs are multiplied by
a positive constant or shifted. The regret also depends appropriately on the scale so that multiplying
the rewards of all arms by a positive constant factor also multiplies the regret by this factor. As far
as I know  this is the ﬁrst scale free bandit algorithm for a non-parametric class. The assumption
on the boundedness of the kurtosis is much less restrictive than assuming an exact Gaussian model
(which has kurtosis 3) or uniform (kurtosis 9/5). See Table 2 for other examples.
As mentioned  the kurtosis is a measure of the likelihood/existence of outliers of a distribution 
and it makes intuitive sense that a bandit strategy might depend on some kind of assumption on
this quantity. How else to know whether or not to cease exploring an unpromising action? The
assumption can also be justiﬁed from a mathematical perspective. If the variance of an arm is not
assumed known  then calculating conﬁdence intervals requires an estimate of the variance from the
data. Let X  X1  X2  . . .   Xn be a sequence of i.i.d. centered random variables with variance σ2 and

2

kurtosis κ. A reasonable estimate of σ2 is

ˆσ2 =

n(cid:88)

t=1

1
n

X 2
t .

Clearly this estimator is unbiased and has variance

V[ˆσ2] =

E[X 4] − E[X 2]2

σ4 (κ − 1)

n

=

n

(2)

.

µ(1−µ)

Kurtosis
3
1−3µ(1−µ)

µ ∈ R  σ2 > 0
µ ∈ [0  1]
λ > 0
µ ∈ R  b > 0
a < b ∈ R

Distribution Parameters
Gaussian
Bernoulli
Exponential
Laplace
Uniform

Therefore  if we are to expect good estima-
tion of σ2  then the kurtosis should be ﬁ-
nite. Note that if σ2 is estimated by (2)  then
the central limit theorem combined with ﬁ-
nite kurtosis is enough for an estimation er-
ror of O(σ2((κ − 1)/n)1/2) asymptotically.
For bandits  however  ﬁnite-time bounds are
required  which are not available using (2)
without additional moment assumptions (for
example  on the moment generating func-
tion). An example demonstrating the neces-
sity of the limit in the standard central limit
theorem is as follows. Suppose that X1  . . .   Xn are Bernoulli with bias p = 1/n  then for large n
the distribution of the sum is closely approximated by a Poisson distribution with parameter 1  which
is very different to a Gaussian. Finite kurtosis alone is enough if the classical empirical estimator is
replaced by a robust estimator such as the median-of-means estimator [Alon et al.  1996] or Catoni’s
estimator [Catoni  2012]. Of course  if the kurtosis were not known  then you could try and estimate
it with assumptions on the eighth moment  and so on. Is there any justiﬁcation to stop here? The
main reason is that this seems like a useful place to stop. Large classes of distributions have known
bounds on their kurtosis (see table) and the independence of scale is a satisfying property.

Table 2: Kurtosis

9/5

9

9

Additional notation Let Ti(t) =(cid:80)t

Contributions The main contribution is the new assumption  algorithm  and the proof of Theorem
2 (see §2). The upper bound is also complemented by an asymptotic lower bound (§3) that applies
to all strategies with sub-polynomial regret and all bandit problems with bounded kurtosis.

s=1

1{As = i} be the number of times arm i has been played
after round t. For measures P  Q on the same probability space  KL(P  Q) is the relative entropy
between P and Q and χ2(P  Q) is the χ2 distance. The following lemma is well known.
Lemma 3. Let X1  X2 be independent random variables with Xi having variance σ2
κi < ∞ and skewness γi = E[(Xi − E[Xi])3/σ3
(b) γ1 ≤ √

(a) Kurt[X1 + X2] = 3 +

i and kurtosis

κ1 − 1 .

i ]  then:

1(κ1 − 3) + σ4
2(κ2 − 3)
σ4
2)2
1 + σ2

(σ2

2 Algorithm and upper bound

Like the robust upper conﬁdence bound algorithm by Bubeck et al. [2013]  the new algorithm makes
use of the robust median-of-means estimator.

Median-of-means estimator Let Y1  Y2  . . .   Yn be a sequence of independent and identically
distributed random variables. The median-of-means estimator ﬁrst partitions the data into m blocks
of equal size (up to rounding errors). The empirical mean of each block is then computed and the
estimate is the median of the means of each of the blocks. The number of blocks depends on the
desired conﬁdence level and should be O(log(1/δ)). The median-of-means estimator at conﬁdence
level δ ∈ (0  1) is denoted by (cid:91)MMδ((Yt)n
Lemma 4 (Bubeck et al. 2013). Let Y1  Y2  . . .   Yn be a sequence of independent and identically
distributed random variables with mean µ and variance σ2 < ∞.

t=1).

(cid:32)(cid:12)(cid:12)(cid:12)(cid:91)MMδ ((Yt)n

P

t=1) − µ

(cid:115)

(cid:12)(cid:12)(cid:12) ≥ C1

(cid:19)(cid:33)

(cid:18) C2

σ2
n

log

δ

≤ δ  

3

√

where C1 =

12 · 16 and C2 = exp(1/8) are universal constants.

Upper conﬁdence bounds The new algorithm is a generalisation of UCB  but with optimistic
estimates of the mean and variance using conﬁdence bounds about the median-of-means estimator.
Let δ ∈ (0  1) and Y1  Y2  . . .   Yt be a sequence of independent and identically distributed random
variables with mean µ  variance σ2 and kurtosis κ < κ◦. Furthermore  let

(cid:19)(cid:41)

(cid:18) C2

.

t ((Ys)t
˜σ2
s=1  θ  δ)
t

log

δ

˜µ((Ys)t

s=1  δ) = sup

θ ∈ R : θ ≤ (cid:91)MMδ

(cid:40)

(Ys)t

(cid:16)
(cid:16)(cid:0)(Ys − θ)2(cid:1)t
(cid:113) κ◦−1

(cid:17)
(cid:17)
log(cid:0) C2

s=1

s=1

δ

t

+ C1

(cid:115)
(cid:1)(cid:27) .

(cid:26)

(cid:91)MMδ
0  1 − C1

where ˜σ2((Ys)t

s=1  θ  δ) =

max

Note that ˜µ((Ys)t
s=1  δ) may be (positive) inﬁnity if t is insufﬁciently large. The computation of
˜µ(·) seems non-trivial and is discussed in the summary at the end of the paper where a roughly
equivalent and efﬁciently computable alternative is given. The following two lemmas show that ˜µ is
indeed optimistic with high probability  and also that it concentrates with reasonable speed around
the true mean.

Lemma 5. P(cid:0)˜µ((Ys)t

s=1  δ) ≤ µ(cid:1) ≤ 2δ .

Proof. By Lemma 4 and the fact that V[(Ys − µ)2] = σ4(κ − 1) ≤ σ4(κ◦ − 1) it holds with
probability at least 1 − δ that ˜σ2((Ys)t
s=1  µ  δ) ≥ σ2. Another application of Lemma 4 along with
a union bound ensures that with probability at least 1 − 2δ 

(cid:115)

(cid:115)

(cid:18) C2

(cid:19)

δ

(cid:91)MMδ((Ys)t

s=1) ≤ C1

σ2
t

log

≤ C1

t ((Ys)t
˜σ2
s=1  µ  δ)
t

log

(cid:18) C2

(cid:19)

.

δ

Therefore with probability at least 1 − 2δ the true mean µ is in the set of which ˜µ is the supremum
and in this case ˜µ((Ys)t
Lemma 6. Let δt be monotone decreasing and ˜µt = ˜µ((Ys)t
constant C3 > 0 such that for any ε > 0 

s=1  δt). Then there exists a universal

s=1  δ) ≥ µ as required.
(cid:26)

(cid:27)

P (˜µt ≥ µ + ε) ≤ C3 max

κ◦ − 1 

σ2
ε2

log

δt .

n(cid:88)

t=1

t=1

δn

+ 2

(cid:19)

n(cid:88)

(cid:18) C2
(cid:19)(cid:33)
(cid:18) C2
≤ n(cid:88)
(cid:19)(cid:33)
(cid:18) C2

t=1

δt

log

δ

log

σ2
t

(cid:115)

κ◦ − 1

t

δt .

(3)

≤ n(cid:88)

t=1

δt .

(4)

Similarly 

Proof. First  by Lemma 4

Suppose that t is a round where all of the following hold:

P

P

t=1

t=1

s=1

(Ys)t

(cid:115)
(cid:32)(cid:12)(cid:12)(cid:12)(cid:91)MMδt
(cid:12)(cid:12)(cid:12) ≥ C1
(cid:16)
(cid:17) − µ
n(cid:88)
(cid:32)(cid:12)(cid:12)(cid:12)(cid:91)MMδt
(cid:17) − σ2(cid:12)(cid:12)(cid:12) ≥ C1σ2
(cid:16)(cid:0)(Ys − µ)2(cid:1)t
n(cid:88)
(cid:115)
(cid:19)
(cid:18) C2
(cid:12)(cid:12)(cid:12) < C1
(cid:12)(cid:12)(cid:12)(cid:91)MMδt
(cid:17) − µ
(cid:16)
(cid:115)
(cid:17) − σ2(cid:12)(cid:12)(cid:12) < C1σ2
(cid:12)(cid:12)(cid:12)(cid:91)MMδt
(cid:16)(cid:0)(Ys − µ)2(cid:1)t
(cid:19)
(cid:18) C2

(Ys)t

σ2
t

log

s=1

s=1

s=1

δt

t

.

(c) t ≥ 16C 2

1 (κ◦ − 1) log

.

δt

(a)

(b)

4

κ◦ − 1

(cid:18) C2

(cid:19)

δt

.

log

(cid:16)
(cid:16)(cid:0)(Ys − ˜µt)2(cid:1)t

(Ys)t

s=1

s=1

(cid:17)
(cid:17)

 

s=1  ˜µt  δt) and ˆµt = (cid:91)MMδt

˜σ2
t =

Abbreviating ˜σ2
(cid:91)MMδt
1 − C1
≤ 4(cid:91)MMδt
≤ 4(cid:91)MMδt

s=1

κ◦−1

t = ˜σ2((Ys)t

(cid:16)(cid:0)(Ys − ˜µs)2(cid:1)t
(cid:114)
(cid:16) C2
(cid:16)(cid:0)(Ys − µ)2(cid:1)t
(cid:16)(cid:0)(Ys − µ)2(cid:1)t
(cid:115)

log

δt

t

κ◦ − 1

s=1

s=1

(cid:17)
(cid:17) ≤ 2(cid:91)MMδt
(cid:17)
(cid:17)

(cid:18) C2

(cid:19)

δt

log

t

< 4σ2 + 4C1σ2

+ 4(˜µt − µ)2
+ 8(˜µt − ˆµt)2 + 8(ˆµt − µ)2

t )(κ◦ − 1)

8C 2

1 (σ2 + ˜σ2
t

+

(cid:18) C2

(cid:19)

δt

log

≤ 11
2

σ2 +

˜σ2
t
2

 

where the ﬁrst inequality follows from (c)  the second since (x − y)2 ≤ 2x2 + 2y2 and the fact that

(cid:91)MMδ((aYs + b)t

s=1) = a(cid:91)MMδ((Ys)t

s=1) + b .

The third inequality again uses (x − y)2 ≤ 2x2 + 2y2  while the last uses the deﬁnition of ˜µt and
(a b). Therefore ˜σ2

1 σ2
(d) t ≥ 19C 2
ε2

.

log

(cid:19)

(cid:18) 1

t ≤ 11σ2  which means that if (a b c) and additionally
(cid:115)
(cid:19)

(cid:18) C2
(cid:115)

(cid:115)
(cid:19)

(cid:115)

(cid:19)

˜σ2
t
t

+ C1

σ2
t

log

δn

δn

(cid:18) C2

(cid:18) C2

11σ2

log

t

δn

+ C1

σ2
t

log

δn

≤ ε .

≤ C1

(cid:18) C2

(cid:19)

δn

log

Then |˜µt − µ| ≤ |˜µt − ˆµt| + |ˆµt − µ| < C1

Combining this with (3) and (4) and choosing C3 = 19C 2

1 completes the result.

Algorithm and Proof of Theorem 2 Let δt = 1/(t2 log(1+t)) and ˜µi(t) = ˜µ((Xs)s∈[t] As=i  δt).
In each round the algorithm chooses At = arg maxi∈[k] ˜µi(t − 1)  where ties are broken arbitrarily.
Proof of Theorem 2. Assume without loss of generality that µ1 = µ∗. Then suboptimal arm i is
only played in round t if either ˜µ1(t − 1) ≤ µ1 or ˜µi(t − 1) ≥ µ1. Therefore

P (˜µ1(t − 1) ≤ µ1) +

P (˜µi(t − 1) ≥ µ1 and At = i)

(5)

n(cid:88)

t=1

E[Ti(n)] ≤ n(cid:88)
n(cid:88)

t=1

P (˜µ1(t − 1) ≤ µ1 and T1(t − 1) = u)

δt = 2

tδt = o(log(n)) .

(By Lem. 5)

The two sums are bounded using Lemmas 5 and 6 respectively:

t=1

P (˜µ1(t − 1) ≤ µ1) ≤ n(cid:88)
t(cid:88)
t(cid:88)
n(cid:88)
P (˜µi(t − 1) ≥ µ1 and At = i) ≤ n(cid:88)
(cid:27)

(cid:26)

≤ 2

u=1

t=1

t=1

t=1

u=1

≤ C3 max

κ◦ − 1 

σ2
i
∆2
i

(cid:18) C2

(cid:19)

δn

n(cid:88)

t=1

n(cid:88)

t=1

n(cid:88)

t=1

position Rn =(cid:80)k

i=1 ∆iE[Ti(n)].

P (˜µi(t − 1) − µi ≥ ∆i)

5

And the result follows by substituting the above bounds into Eq. (5) and then into the regret decom-

log

+ 2

δt = o(log(n)) .

(By Lem. 6)

3 Lower bound
Let Hκ◦ = {ν : ν has kurtosis less than κ◦} be the class of all distributions with kurtosis bounded
by κ◦. Following the nomenclature of Lai and Robbins [1985]  a bandit strategy is called consistent
over H if Rn = o(np) for all p ∈ (0  1) and bandits (νi)i with νi ∈ Hκ◦ for all i. The next theorem
shows that the upper bound derived in the previous section is nearly tight up to constant factors. Let
H be a family of distributions and let (νi)i be a bandit with νi ∈ H for all i. Burnetas and Katehakis
[1996] showed that for any consistent strategy 
for all i ∈ [k] :

≥(cid:0)inf(cid:8)KL(νi  ν(cid:48)

[X] > µ∗(cid:9)(cid:1)−1

i) : ν(cid:48)

i ∈ H and EX∼ν(cid:48)

(6)

.

lim inf
n→∞

E[Ti(n)]
log(n)

i

In parameterised families of distributions  the optimisation problem can often be evaluated ana-
lytically (eg.  Bernoulli  Gaussian with known variance  Gaussian with unknown variance  Expo-
nential). For non-parametric families the calculation is much more challenging. The following
theorem takes the ﬁrst steps towards understanding this problem for the class of distributions Hκ◦
for κ◦ ≥ 7/2.
Theorem 7. Let κ◦ ≥ 7/2 and ∆ > 0 and ν ∈ Hκ◦ with mean µ  variance σ2 > 0 and kurtosis κ.
Then for appropriately chosen universal constant C  C(cid:48) > 0 

(cid:26) 1

(cid:27)

 

∆
σ

κ◦

.

inf {KL(ν  ν(cid:48)) : ν(cid:48) ∈ Hκ and EX∼ν(cid:48)[X] > µ + ∆} ≤ 7
5

min

If additionally it holds that κ + C(cid:48)∆κ1/2(κ + 1) ≤ κ◦  then

inf {KL(ν  ν(cid:48)) : ν(cid:48) ∈ Hκ and EX∼ν(cid:48)[X] > µ + ∆} ≤ C

∆2
σ2

Therefore provided that ν ∈ Hκ◦ is not too close to the boundary of Hκ◦ in the sense that its kurtosis
is not too close to κ◦  then the lower bound derived from Theorem 7 and Eq. (6) matches the upper
bound up to constant factors. This condition is probably necessary because distributions like the
Bernoulli with kurtosis close to κ◦ have barely any wiggle room to increase the mean without also
increasing the kurtosis.

Proof of Theorem 7. Let ∆ε = ∆ + ε for small ε > 0. Assume without loss of generality that ν is
centered and has variance σ2 = 1  which can always be achieved by shifting and scaling (neither
effects the kurtosis or the relative entropy). The ﬁrst part of the claim is established by considering
the perturbed distribution obtained by adding a Bernoulli ‘outlier’. Let X be a random variable
sampled from ν and B be a Bernoulli with parameter p = min{∆ε  1/κ◦}. Let Z = X + Y where
Y = ∆εB/p. Then E[Z] = ∆ε > ∆ and

Kurt[Z] = 3 +

κ − 3 + V[Y ]2(Kurt[Y ] − 3)

κ − 3 +

= 3 +

≤ 3 +

κ◦ − 3 +

(cid:16)

1−6p(1−p)

p(1−p)

(cid:17)2

≤ κ◦  

(1 + V[Y ])2

(cid:16) (1−p)∆2

(cid:17)2

ε

p

1 + (1−p)∆2

ε

p

(cid:17)2

(cid:16) (1−p)∆2
(cid:16)

p

ε

1 + (1−p)∆2

ε

p

1−6p(1−p)

p(1−p)

(cid:17)2

where the ﬁrst inequality used Lemma 3 and the ﬁnal inequality follows from simple case-based
analysis  calculus and the assumption that κ◦ ≥ 7/2 (see Lemma 9 in the appendix). Let ν(cid:48) = L(Y )
be the law of Y . Then

(cid:90)

(cid:90)

KL(ν  ν(cid:48)) =

log

dν

dν(cid:48) dν ≤

R

R

log

1
1 − p

dν = log

1
1 − p

≤ p
1 − p

≤ 7
5

min

∆ε 

.

Taking the limit as ε tends to 0 completes the proof of the ﬁrst part of the theorem. Moving onto the
second claim and using C for a universal positive constant that changes from equation to equation.
aκ} and ¯A = R − A. Deﬁne

Let a > 0 be a constant to be chosen later and A = {x : |x| ≤ √

6

(cid:26)

(cid:27)

1
κ◦

alternative measure ν(cid:48)(E) =(cid:82)
and β chosen so that (cid:90)
(cid:90)

R

Solving for α and β shows that

(cid:82)

∆ε

A x2dν(x) − ((cid:82)

β =

This implies that(cid:82)

E(1 + g)dν where g(x) = (α + βx)1{x ∈ A} for some constants α

(cid:90)
(cid:90)

A

(cid:90)

(cid:90)

A

g(x)dν(x) = α

dν(x) + β

xdν(x) = 0 .

g(x)xdν(x) = α

xdν(x) + β

R

A

A

x2dν(x) = ∆ε .

(cid:82)

ν(A)(cid:82)

A x2dν(x) −(cid:0)(cid:82)

A xdν(x)

A xdν(x)(cid:1)2 .

∆ε

It remains to show that ν(cid:48) is a
probability measure with kurtosis bounded by κ◦. That ν(cid:48) is a probability measure will follow from
the positivity of 1 − g(·). The ﬁrst step is to control each of the terms appearing in the deﬁnitions of
α and β. By Cauchy-Schwarz and Chebyshev’s inequalities  ν( ¯A) = ν(x2 ≥ aκ) ≤ 1/(κa2) and

Similarly  since ν is centered 

and α = −

ν(A)

A xdν(x))2

x2dν(x) = 1 −

R dν(cid:48)(x) = 1 and(cid:82)
R xdν(cid:48)(x) = ∆ε > ∆.
(cid:90)
(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
A xdν(x)(cid:1)2

x2dν(x) ≥ 1 −(cid:113)
(cid:12)(cid:12)(cid:12)(cid:12) ≤(cid:113)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤
a(cid:0)(cid:0)1 − 1

xdν(x)

¯A

¯A

A

A

xdν(x)

(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:82)
A x2dν(x) −(cid:0)(cid:82)
ν(A)(cid:82)
(cid:82)
A x2dν(x) − ((cid:82)
|g(x)| = max(cid:8)|g(

A xdν(x)

A xdν(x))2

√

∆ε

ν(A)

|α| = ∆ε

κν( ¯A) ≥ 1 − 1
a

.

√
σ2ν( ¯A) ≤ 1
a

.

κ

√
∆ε/

(cid:1)(cid:0)1 − 1

κ

(cid:1) − 1

a

a2κ

(cid:1) ≤ 4∆ε√

κ

κa2

Therefore by choosing a = 2 and using the fact that the kurtosis is always larger than 1 

1

≤

a2κ )

max
x∈R

a −

|β| =

≤ 6∆ε .

1 − 1

aκ)| |g(−√

∆ε
κa2(1− 1
aκ)|(cid:9) ≤ |α| +
Now g(x) is a linear function supported on compact set A  so
√
aκ|β| ≤ 4∆ε√
κ
√
where the last inequality follows by assuming that ∆ε ≤ √
2κ)) = O(κ−1/2)  which
κ/(4(2 + 3
is reasonable without loss of generality  since if ∆ε is larger than this quantity  then we would prefer
the bound that depends on κ◦ derived in the ﬁrst part of the proof. The relative entropy between ν
(cid:19)2
and ν(cid:48) is bounded by
(cid:90)

KL(ν  ν(cid:48)) ≤ χ2(ν  ν(cid:48)) =

(cid:18) dν(x)

2κ ≤ 1
2

dν(cid:48)(x) =

dν(cid:48)(x)

1 + g(x)

+ 6∆ε

− 1

g(x)2

dν(x)

(cid:90)

(cid:90)

(cid:90)

√

R

A

 

g(x)2dν(x) ≤ 4

α2dν(x) + 4

A

A

β2x2dν(x) ≤ 4α2 + 4β2

+ 4 · 36∆2

ε ≤ C∆2
ε .

In order to bound the kurtosis we need to evaluate the moments:

(cid:90)
(cid:90)

A

A

x3dν(x) ≤ 1 + C∆ε

√

κ .

x5dν(x) ≤ κ(cid:0)1 + C∆ε

κ(cid:1) .

√

(cid:90)
(cid:90)

g(x)x2dν = 1 + α

g(x)x2dν ≥ 1 − C∆ε

g(x)x4dν = κ + α

A

x2dν(x) + β
√

κ .

A

x4dν(x) + β
√

Cκ .

A

x2dν(cid:48)(x)

(cid:90)

R

x4dν(cid:48)(x) ≤

7

ε

A

κ

(cid:90)
≤ 2
≤ 4 · 16∆2
(cid:90)
(cid:90)
(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12) ≤

(cid:115)(cid:90)

R

R

R

R

x4dν +

x2dν +

x2dν +

(cid:90)
(cid:90)
(cid:90)

A

A

(cid:90)
(cid:90)
(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

R

R

R

R

x2dν(cid:48) =

x2dν(cid:48) =

x4dν(cid:48) =

x3dν(cid:48)(x)

Therefore if κ(cid:48) is the kurtosis of ν(cid:48)  then

(cid:82)
(cid:1)2 =
(cid:0)(cid:82)
R(x − ∆ε)4dν(cid:48)(x)
R x2dν(cid:48)(x) − ∆2

ε

(cid:82)

κ(cid:48) =

Therefore

κ(cid:48) =

(cid:82)
A x3dν(x)(cid:1)2
R x3dν(cid:48)(x)

(cid:82)
A x2dν(x) + β(cid:82)
R x2dν(cid:48)(x) − 4∆ε
(cid:82)
(cid:1)2
R x3dν(cid:48)(x)

ε + 6∆2
ε

(cid:0)1 − ∆2
ε + α(cid:82)
R x4dν(cid:48)(x) − 3∆4
(cid:82)
(cid:82)
(cid:0)(cid:82)
R x2dν(cid:48)(x) − 4∆ε
R x4dν(cid:48)(x) − 3∆4
≤ κ(cid:0)1 + C∆εκ1/2(cid:1) + 6∆2
R x2dν(cid:48)(x) − ∆2
(cid:1)2
(cid:0)1 − C∆εκ1/2 − ∆2

ε + 6∆2
ε

ε

ε

ε(1 + C∆εκ1/2) + C∆εκ1/2

≤ κ + C∆εκ1/2(κ + 1)

1 − C∆εκ1/2

≤ κ + C∆εκ1/2(κ + 1) .

Therefore κ(cid:48) ≤ κ◦ provided ∆ε is sufﬁciently small  which after taking the limit as ε → 0 completes
the proof.

4 Summary

The assumption of ﬁnite kurtosis generalises the parametric Gaussian assumption to a comparable
non-parametric setup with a similar basic structure. Of course there are several open questions.

Optimal constants The leading constants in the main results (Theorem 2 and Theorem 7) are
certainly quite loose. Deriving the optimal form of the regret is an interesting challenge  with both
lower and upper bounds appearing quite non-trivial. It may be necessary to resort to an implicit
analysis showing that (6) is (or is not) achievable when H is the class of distributions with kurtosis
bounded by some κ◦. Even then  constructing an efﬁcient algorithm would remain a challenge.
Certainly what has been presented here is quite far from optimal. At the very least the median-of-
means estimator needs to be replaced  or the analysis improved. An excellent candidate is Catoni’s
estimator [Catoni  2012]  which is slightly more complicated than the median-of-means  but also
comes with smaller constants and could be plugged into the algorithm with very little effort. An
alternative approach is to use the theory of self-normalised processes [Pe˜na et al.  2008]  but even this
seems to lead to suboptimal constants. For the lower bound  there appears to be almost no work on
the explicit form of the lower bounds presented by Burnetas and Katehakis [1996] in interesting non-
parametric classes beyond rewards with bounded or semi-bounded support [Honda and Takemura 
2010  2015].

Absorbing other improvements There has recently been a range of improvements to the conﬁ-
dence level for the classical upper conﬁdence bound algorithms that shave logarithmic terms from
the worst-case regret or improve the lower-order terms in the ﬁnite-time bounds [Audibert and
Bubeck  2009  Lattimore  2015]. Many of these enhancements can be incorporated into the al-
gorithm presented here  which may lead to practical and theoretical improvements.

Computation complexity The main challenge is the computation of the index  which as written
seems challenging. The easiest solution is to change the algorithm slightly by estimating

ˆµi(t) = (cid:91)MMδt((Xs)s∈[t] As=i)

i (t) = (cid:91)MMδt((X 2
ˆσ2

s )s∈[t] As=i) − ˆµi(t)2 .

Then an upper conﬁdence bound on ˆµi(t) is easily derived from Lemma 4 and the rest of the analysis
goes through in about the same way. Naively the computational complexity of the above is Ω(t) in
round t  which would lead to a running time over n rounds of Ω(n2). Provided the number of buckets
used between rounds t and t + 1 is the same  then the median-of-means estimator can be updated
incrementally in O(Bt) time  where Bt is the number of buckets. Now Bt = O(log(1/δt)) =
O(log(t)) so there are at most O(log(n)) changes over n rounds. Therefore the total computation is
O(nk + n log(n)).

8

Comparison to Bernoulli Table 2 shows that the kurtosis for a Bernoulli random variable
(cid:80)
with mean µ is κ = O(1/(µ(1 − µ)))  which is obviously not bounded as µ tends towards
the boundaries. The optimal asymptotic regret for the Bernoulli case is limn→∞ Rn/ log(n) =
i:∆i>0 ∆i/d(µi  µ∗). The interesting differences occur near the boundary of the parameter space.
Suppose that µi ≈ 0 for some arm i and µ∗ > 0 is close to zero. An easy calculation shows that
d(µi  µ∗) ≈ log(1/(1 − ∆i)) ≈ ∆i. Therefore
≈

1

.

lim inf
n→∞

E[Ti(n)]
log(n)

log(1/(1 − ∆i))

≈ 1
∆i

Here we see an algorithm is enjoying logarithmic regret on a class with inﬁnite kurtosis! But this
is a special case and is not possible in general. The reason is that the structure of the hypothesis
class allows strategies to (essentially) estimate the kurtosis with reasonable accuracy and anticipate
outliers more/less depending on the data observed so far. Another way of saying it is that when the
kurtosis is actually small  the algorithms can learn this fact by examining the empirical mean.

A Technical calculations

This section completes some of the calculations required in the proof of Theorem 7.
Lemma 8. Let κ◦ ≥ 7/2 and f (x) = 3 + (κ◦ − 3 + x)/(1 + x)2. Then f (x) ≤ κ◦ for all x ≥ 0.
Proof. Clearly f (0) = κ◦ and for κ◦ ≥ 7/2 and x ≥ 0 

(cid:18)
1 − 2(κ◦ − 3 + x)

(cid:19)

1 + x

≤ 0 .

Therefore f (x) = κ◦ +(cid:82) x

1

f(cid:48)(x) =
(1 + x)2
0 f(cid:48)(y)dy ≤ κ◦.

Lemma 9. If κ◦ ≥ 7/2 and p = min{∆  1/κ◦}  then

κ◦ − 3 +

(cid:16)

3 +

(cid:16) (1−p)∆2

(cid:17)2

p

1 + (1−p)∆2

p

1−6p(1−p)

p(1−p)

(cid:17)2

≤ κ◦ .

Proof. Suppose that p = ∆. Then since κ◦ ≥ 7/2 ≥ 1  p ≤ 1. Therefore

κ◦ − 3 +

(cid:16)

3 +

(cid:16) (1−p)∆2

(cid:17)2

p

1 + (1−p)∆2

p

1−6p(1−p)

p(1−p)

(cid:17)2

κ◦ − 3 + ∆(1 − ∆)(1 − 6∆(1 − ∆))

(1 + ∆(1 − ∆))2

κ◦ − 3 + ∆(1 − ∆)
(1 + ∆(1 − ∆))2

= 3 +

≤ 3 +
≤ κ◦  

where the last inequality follows from Lemma 8. Now suppose that p = 1/κ◦. Then

κ◦ − 3 +

(cid:16)

3 +

(cid:16) (1−p)∆2

(cid:17)2

p

1 + (1−p)∆2

p

1−6p(1−p)

p(1−p)

(cid:17)2

≤ 3 +

κ◦ − 3 +

(cid:40)

(cid:17)2
(cid:16) (1−p)∆2
(cid:16) (1−p)∆2
(cid:41)

p

p

1 +

1−6p(1−p)

p(1−p)

(cid:17)2

κ◦ 

κ◦
1 − 1

κ◦

− 3

≤ max
≤ κ◦  

where the ﬁrst inequality follows since (a+b)2 ≥ a2 +b2 for a  b ≥ 0. The second since the average
is less than the maximum. The third since κ◦ ≥ 7/2 > 4/3.

9

References
Noga Alon  Yossi Matias  and Mario Szegedy. The space complexity of approximating the frequency
moments. In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing 
pages 20–29. ACM  1996.

Jean-Yves Audibert and S´ebastien Bubeck. Minimax policies for adversarial and stochastic bandits.

In Proceedings of Conference on Learning Theory (COLT)  pages 217–226  2009.

Sebastian Bubeck  Nicolo Cesa-Bianchi  and G´abor Lugosi. Bandits with heavy tail. Information

Theory  IEEE Transactions on  59(11):7711–7717  2013.

S´ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-
armed Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorpo-
rated  2012. ISBN 9781601986269.

Apostolos N Burnetas and Michael N Katehakis. Optimal adaptive policies for sequential allocation

problems. Advances in Applied Mathematics  17(2):122–142  1996.

Olivier Capp´e  Aur´elien Garivier  Odalric-Ambrym Maillard  R´emi Munos  and Gilles Stoltz.
Kullback–Leibler upper conﬁdence bounds for optimal sequential allocation. The Annals of
Statistics  41(3):1516–1541  2013.

Olivier Catoni. Challenging the empirical mean and empirical variance: a deviation study.

In
Annales de l’Institut Henri Poincar´e  Probabilit´es et Statistiques  volume 48  pages 1148–1185.
Institut Henri Poincar´e  2012.

Wesley Cowan and Michael N Katehakis. An asymptotically optimal policy for uniform bandits of

unknown support. arXiv preprint arXiv:1505.01918  2015.

Wesley Cowan  Junya Honda  and Michael N Katehakis. Normal bandits of unknown means and
variances: Asymptotic optimality  ﬁnite horizon regret bounds  and a solution to an open problem.
arXiv preprint arXiv:1504.05823v2  2015.

Lawrence T DeCarlo. On the meaning and use of kurtosis. Psychological methods  2(3):292  1997.

Junya Honda and Akimichi Takemura. An asymptotically optimal bandit algorithm for bounded
support models. In Proceedings of Conference on Learning Theory (COLT)  pages 67–79  2010.

Junya Honda and Akimichi Takemura. Non-asymptotic analysis of a new bandit algorithm for semi-

bounded rewards. Journal of Machine Learning Research  16:3721–3756  2015.

Michael N Katehakis and Herbert Robbins. Sequential choice from several populations. Proceedings

of the National Academy of Sciences of the United States of America  92(19):8584  1995.

Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances

in applied mathematics  6(1):4–22  1985.

Tor Lattimore. Optimally conﬁdent UCB: Improved regret for ﬁnite-armed bandits. arXiv preprint

arXiv:1507.07880  2015.

Victor H Pe˜na  Tze Leung Lai  and Qi-Man Shao. Self-normalized processes: Limit theory and

Statistical Applications. Springer Science & Business Media  2008.

10

,Tor Lattimore