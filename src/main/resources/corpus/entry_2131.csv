2007,Fixing Max-Product: Convergent Message Passing Algorithms for MAP LP-Relaxations,We present a novel message passing algorithm for approximating the MAP problem in graphical models. The algorithm is similar in structure to max-product but unlike max-product it always converges  and can be proven to find the exact MAP solution in various settings. The algorithm is derived via block coordinate descent in a dual of the LP relaxation of MAP  but does not require any tunable parameters such as step size or tree weights. We also describe a generalization of the method to cluster based potentials. The new method is tested on synthetic and real-world problems  and compares favorably with previous approaches.,Fixing Max-Product: Convergent Message Passing

Algorithms for MAP LP-Relaxations

Amir Globerson Tommi Jaakkola

Computer Science and Artiﬁcial Intelligence Laboratory

Massachusetts Institute of Technology

Cambridge  MA 02139

gamir tommi@csail.mit.edu

Abstract

We present a novel message passing algorithm for approximating the MAP prob-
lem in graphical models. The algorithm is similar in structure to max-product but
unlike max-product it always converges  and can be proven to ﬁnd the exact MAP
solution in various settings. The algorithm is derived via block coordinate descent
in a dual of the LP relaxation of MAP  but does not require any tunable parameters
such as step size or tree weights. We also describe a generalization of the method
to cluster based potentials. The new method is tested on synthetic and real-world
problems  and compares favorably with previous approaches.

Graphical models are an effective approach for modeling complex objects via local interactions. In
such models  a distribution over a set of variables is assumed to factor according to cliques of a graph
with potentials assigned to each clique. Finding the assignment with highest probability in these
models is key to using them in practice  and is often referred to as the MAP (maximum aposteriori)
assignment problem. In the general case the problem is NP hard  with complexity exponential in the
tree-width of the underlying graph.

Linear programming (LP) relaxations have proven very useful in approximating the MAP problem 
and often yield satisfactory empirical results. These approaches relax the constraint that the solution
is integral  and generally yield non-integral solutions. However  when the LP solution is integral 
it is guaranteed to be the exact MAP. For some classes of problems the LP relaxation is provably
correct. These include the minimum cut problem and maximum weight matching in bi-partite graphs
[8]. Although LP relaxations can be solved using standard LP solvers  this may be computationally
intensive for large problems [13]. The key problem with generic LP solvers is that they do not use
the graph structure explicitly and thus may be sub-optimal in terms of computational efﬁciency.

The max-product method [7] is a message passing algorithm that is often used to approximate the
MAP problem.
In contrast to generic LP solvers  it makes direct use of the graph structure in
constructing and passing messages  and is also very simple to implement. The relation between
max-product and the LP relaxation has remained largely elusive  although there are some notable
exceptions: For tree-structured graphs  max-product and LP both yield the exact MAP. A recent
result [1] showed that for maximum weight matching on bi-partite graphs max-product and LP also
yield the exact MAP [1]. Finally  Tree-Reweighted max-product (TRMP) algorithms [5  10] were
shown to converge to the LP solution for binary xi variables  as shown in [6].
In this work  we propose the Max Product Linear Programming algorithm (MPLP) - a very simple
variation on max-product that is guaranteed to converge  and has several advantageous properties.
MPLP is derived from the dual of the LP relaxation  and is equivalent to block coordinate descent in
the dual. Although this results in monotone improvement of the dual objective  global convergence
is not always guaranteed since coordinate descent may get stuck in suboptimal points. This can
be remedied using various approaches  but in practice we have found MPLP to converge to the LP

1

solution in a majority of the cases we studied. To derive MPLP we use a special form of the dual
LP  which involves the introduction of redundant primal variables and constraints. We show how
the dual variables corresponding to these constraints turn out to be the messages in the algorithm.
We evaluate the method on Potts models and protein design problems  and show that it compares
favorably with max-product (which often does not converge for these problems) and TRMP.

1 The Max-Product and MPLP Algorithms

The max-product algorithm [7] is one of the most often used methods for solving MAP problems.
Although it is neither guaranteed to converge to the correct solution  or in fact converge at all  it
provides satisfactory results in some cases. Here we present two algorithms: EMPLP (edge based
MPLP) and NMPLP (node based MPLP)  which are structurally very similar to max-product  but
have several key advantages:

• After each iteration  the messages yield an upper bound on the MAP value  and the se-
quence of bounds is monotone decreasing and convergent. The messages also have a limit
point that is a ﬁxed point of the update rule.

• No additional parameters (e.g.  tree weights as in [6]) are required.
• If the ﬁxed point beliefs have a unique maximizer then they correspond to the exact MAP.
• For binary variables  MPLP can be used to obtain the solution to an LP relaxation of the
MAP problem. Thus  when this LP relaxation is exact and variables are binary  MPLP will
ﬁnd the MAP solution. Moreover  for any variable whose beliefs are not tied  the MAP
assignment can be found (i.e.  the solution is partially decodable).

Pseudo code for the algorithms (and for max-product) is given in Fig. 1. As we show in the next
sections  MPLP is essentially a block coordinate descent algorithm in the dual of a MAP LP re-
laxation. Every update of the MPLP messages corresponds to exact minimization of a set of dual
variables. For EMPLP minimization is over the set of variables corresponding to an edge  and for
NMPLP it is over the set of variables corresponding to all the edges a given node appears in (i.e.  a
star). The properties of MPLP result from its relation to the LP dual. In what follows we describe
the derivation of the MPLP algorithms and prove their properties.

2 The MAP Problem and its LP Relaxation

We consider functions over n variables x = {x1  . . .   xn} deﬁned as follows. Given a graph G =
(V  E) with n vertices  and potentials θij(xi  xj) for all edges ij ∈ E  deﬁne the function1

f (x; θ) = Xij∈E

θij (xi  xj) .

(1)

The MAP problem is deﬁned as ﬁnding an assignment xM that maximizes the function f (x; θ).
Below we describe the standard LP relaxation for this problem. Denote by {µij(xi  xj)}ij∈E distri-
butions over variables corresponding to edges ij ∈ E and {µi(xi)}i∈V distributions corresponding
to nodes i ∈ V . We will use µ to denote a given set of distributions over all edges and nodes. The
set ML(G) is deﬁned as the set of µ where pairwise and singleton distributions are consistent

µij(ˆxi  xj) = µj(xj )   Pˆxj

µi(xi) = 1

µij (xi  ˆxj) = µi(xi) ∀ij ∈ E  xi  xj

∀i ∈ V

(cid:27)

ML(G) = (cid:26)µ ≥ 0(cid:12)(cid:12)(cid:12)(cid:12)

Pˆxi
Pxi

Now consider the following linear program:

MAPLPR :

µL∗ = arg max

µ∈ML(G)

µ · θ .

(2)

where µ·θ is shorthand for µ·θ = Pij∈E Pxi xj

θij(xi  xj )µij(xi  xj). It is easy to show (see e.g. 
[10]) that the optimum of MAPLPR yields an upper bound on the MAP value  i.e. µL∗ ·θ ≥ f (xM ).
Furthermore  when the optimal µi(xi) have only integral values  the assignment that maximizes
µi(xi) yields the correct MAP assignment. In what follows we show how the MPLP algorithms can
be derived from the dual of MAPLPR.

1We note that some authors also add a term Pi∈V θi(xi) to f (x; θ). However  these terms can be included

in the pairwise functions θij (xi  xj)  so we ignore them for simplicity.

2

3 The LP Relaxation Dual

Since MAPLPR is an LP  it has an equivalent convex dual. In App. A we derive a special dual of
MAPLPR using a different representation of ML(G) with redundant variables. The advantage of
this dual is that it allows the derivation of simple message passing algorithms. The dual is described
in the following proposition.

Proposition 1 The following optimization problem is a convex dual of MAPLPR

DMAPLPR :
min

s.t.

Pi

max

xi Pk∈N (i)

max
xk

βki(xk  xi)

(3)

βji(xj   xi) + βij(xi  xj) = θij(xi  xj)  

where the dual variables are βij (xi  xj) for all ij  ji ∈ E and values of xi and xj.

xi Pk∈N (i)

max
xk

The dual has an intuitive interpretation in terms of re-parameterizations. Consider the star
shaped graph Gi consisting of node i and all its neighbors N (i). Assume the potential on
edge ki (for k ∈ N (i)) is βki(xk  xi). The value of the MAP assignment for this model is
βki(xk  xi). This is exactly the term in the objective of DMAPLPR. Thus the dual
max

corresponds to individually decoding star graphs around all nodes i ∈ V where the potentials on the
graph edges should sum to the original potential. It is easy to see that this will always result in an
upper bound on the MAP value. The somewhat surprising result of the duality is that there exists a
β assignment such that star decoding yields the optimal value of MAPLPR.

4 Block Coordinate Descent in the Dual

To obtain a convergent algorithm we use a simple block coordinate descent strategy. At every
iteration  ﬁx all variables except a subset  and optimize over this subset. It turns out that this can
be done in closed form for the cases we consider. We begin by deriving the EMPLP algorithm.
Consider ﬁxing all the β variables except those corresponding to some edge ij ∈ E (i.e.  βij and
βji)  and minimizing DMAPLPR over the non-ﬁxed variables. Only two terms in the DMAPLPR
objective depend on βij and βji. We can write those as

i (xi) + max

xj

f (βij  βji) = max

xi (cid:20)λ−i

βji(xj  xi)(cid:21) + max

xi (cid:20)λ−j
where we deﬁned λ−j
i (xi) = Pk∈N (i)\j λki(xi) and λki(xi) = maxxk βki(xk  xi) as in App. A.
Note that the function f (βij   βji) depends on the other β values only through λ−i
i (xi).
This implies that the optimization can be done solely in terms of λij (xj) and there is no need to
store the β values explicitly. The optimal βij   βji are obtained by minimizing f (βij  βji) subject
to the re-parameterization constraint βji(xj   xi) + βij(xi  xj) = θij (xi  xj ). The following propo-
sition characterizes the minimum of f (βij  βji). In fact  as mentioned above  we do not need to
characterize the optimal βij (xi  xj ) itself  but only the new λ values.

βij(xi  xj)(cid:21)

j (xj) and λ−j

j (xj ) + max

xi

(4)

Proposition 2 Maximizing the function f (βij  βji) yields the following λji(xi) (and the equivalent
expression for λij (xj))

λji(xi) = −

1
2

λ−j
i (xi) +

1
2

max

xj (cid:2)λ−i

j (xj ) + θij(xi  xj )(cid:3)

The proposition is proved in App. B. The λ updates above result in the EMPLP algorithm  described
in Fig. 1. Note that since the β optimization affects both λji(xi) and λij (xj )  both these messages
need to be updated simultaneously.

We proceed to derive the NMPLP algorithm. For a given node i ∈ V   we consider all its neighbors
j ∈ N (i)  and wish to optimize over the variables βji(xj  xi) for ji  ij ∈ E (i.e.  all the edges in a
star centered on i)  while the other variables are ﬁxed. One way of doing so is to use the EMPLP
algorithm for the edges in the star  and iterate it until convergence. We now show that the result of

3

Inputs: A graph G = (V  E)  potential functions θij(xi  xj) for each edge ij ∈ E.

Initialization: Initialize messages to any value.

Algorithm:

• Iterate until a stopping criterion is satisﬁed:

– Max-product: Iterate over messages and update (cji shifts the max to zero)

mji(xi)← max

xj hm−i

j (xj) + θij(xi  xj)i − cji

– EMPLP: For each ij ∈ E  update λji(xi) and λij(xj) simultaneously (the update

for λij(xj) is the same with i and j exchanged)

λji(xi)← −

1
2

λ−j

i (xi) +

1
2

max

xj hλ−i

j (xj) + θij (xi  xj)i

– NMPLP: Iterate over nodes i ∈ V and update all γij(xj) where j ∈ N (i)

γij(xj)← max

xi

2
4

θij(xi  xj) − γji(xi) +

2

|N (i)| + 1 X

k∈N(i)

γki(xi)

3
5

• Calculate node “beliefs”: Set bi(xi) to be the sum of incoming messages into node i ∈ V

(e.g.  for NMPLP set bi(xi) = Pk∈N(i) γki(xi)).

Output: Return assignment x deﬁned as xi = arg maxˆxi b(ˆxi).

Figure 1: The max-product  EMPLP and NMPLP algorithms. Max-product  EMPLP and NMPLP use mes-
sages mij  λij and γij respectively. We use the notation m−i

j (xj) = Pk∈N(j)\i mkj(xj).

this optimization can be found in closed form. The assumption about β being ﬁxed outside the star
implies that λ−i
yields the following relation between λ−j

j (xj ) is ﬁxed. Deﬁne: γji(xi) = maxxj (cid:2)θij (xi  xj) + λ−i

j (xj )(cid:3). Simple algebra

i (xi) and γki(xi) for k ∈ N (i)

λ−j
i (xi) = −γji(xi) +

γki(xi)

(5)

2

|N (i)| + 1 Xk∈N (i)

Plugging this into the deﬁnition of γji(xi) we obtain the NMPLP update in Fig. 1. The messages
for both algorithms can be initialized to any value since it can be shown that after one iteration they
will correspond to valid β values.

5 Convergence Properties

The MPLP algorithm decreases the dual objective (i.e.  an upper bound on the MAP value) at every
iteration  and thus its dual objective values form a convergent sequence. Using arguments similar to
[5] it can be shown that MPLP has a limit point that is a ﬁxed point of its updates. This in itself does
not guarantee convergence to the dual optimum since coordinate descent algorithms may get stuck
at a point that is not a global optimum. There are ways of overcoming this difﬁculty  for example by
smoothing the objective [4] or using techniques as in [2] (see p. 636). We leave such extensions for
further work. In this section we provide several results about the properties of the MPLP ﬁxed points
and their relation to the corresponding LP. First  we claim that if all beliefs have unique maxima then
the exact MAP assignment is obtained.

Proposition 3 If the ﬁxed point of MPLP has bi(xi) such that for all i the function bi(xi) has a
unique maximizer x∗

i   then x∗ is the solution to the MAP problem and the LP relaxation is exact.

Since the dual objective is always greater than or equal to the MAP value  it sufﬁces to show that
there exists a dual feasible point whose objective value is f (x∗). Denote by β∗  λ∗ the value of the
corresponding dual parameters at the ﬁxed point of MPLP. Then the dual objective satisﬁes

Xi

max

xi Xk∈N (i)

λ∗

ki(xi) = Xi Xk∈N (i)

max
xk

ki(xk  x∗
β∗

i ) = Xi Xk∈N (i)

4

ki(x∗
β∗

k  x∗

i ) = f (x∗)

i ) = maxxi xj λ−j

j ) = maxxi xj λ−i

i (xi) + βji(xj  xi) and
j (xj ) + βij(xi  xj). By the equalization property in Eq. 9 the arguments of
j are

To see why the second equality holds  note that bi(x∗
bj(x∗
the two max operations are equal. From the unique maximum assumption it follows that x∗
the unique maximizers of the above. It follows that βji  βij are also maximized by x∗
In the general case  the MPLP ﬁxed point may not correspond to a primal optimum because of the
local optima problem with coordinate descent. However  when the variables are binary  ﬁxed points
do correspond to primal solutions  as the following proposition states.

j .
i   x∗

i   x∗

Proposition 4 When xi are binary  the MPLP ﬁxed point can be used to obtain the primal optimum.

i (x∗

ij (x∗

i ) to 1. If bi  bj are not tied we set µ∗

i (xi) to 0.5
The claim can be shown by constructing a primal optimal solution µ∗. For tied bi  set µ∗
j ) = 1. If bi is not tied but bj
and for untied bi  set µ∗
i   xj) = 0.5. If bi  bj are tied then βji  βij can be shown to be maximized at either
is  we set µ∗
ij to be 0.5 at one of these assignment
x∗
i   x∗
pairs. The resulting µ∗ is clearly primal feasible. Setting δ∗
i we obtain that the dual variables
(δ∗  λ∗  β∗) and primal µ∗ satisfy complementary slackness for the LP in Eq. 7 and therefore µ∗ is
primal optimal. The binary optimality result implies partial decodability  since [6] shows that the
LP is partially decodable for binary variables.

j = (0  1)  (1  0). We then set µ∗

j = (0  0)  (1  1) or x∗

i = b∗

ij (x∗

i   x∗

i   x∗

6 Beyond pairwise potentials: Generalized MPLP

In the previous sections we considered maximizing functions which factor according to the edges of
the graph. A more general setting considers clusters c1  . . .   ck ⊂ {1  . . .   n} (the set of clusters is

denoted by C)  and a function f (x; θ) = Pc θc(xc) deﬁned via potentials over clusters θc(xc). The

MAP problem in this case also has an LP relaxation (see e.g. [11]). To deﬁne the LP we introduce
the following deﬁnitions: S = {c ∩ ˆc : c  ˆc ∈ C  c ∩ ˆc 6= ∅} is the set of intersection between clusters
and S(c) = {s ∈ S : s ⊆ c} is the set of overlap sets for cluster c.We now consider marginals over
the variables in c ∈ C and s ∈ S and require that cluster marginals agree on their overlap. Denote
this set by ML(C). The LP relaxation is then to maximize µ · θ subject to µ ∈ ML(C).
As in Sec. 4  we can derive message passing updates that result in monotone decrease of the dual
LP of the above relaxation. The derivation is similar and we omit the details. The key observation
is that one needs to introduce |S(c)| copies of each marginal µc(xc) (instead of the two copies
in the pairwise case). Next  as in the EMPLP derivation we assume all β are ﬁxed except those
corresponding to some cluster c. The resulting messages are λc→s(xs) from a cluster c to all of its
intersection sets s ∈ S(c). The update on these messages turns out to be:

1

|S(c)|(cid:19) λ−c

λc→s(xs) = −(cid:18)1 −

ˆs (xˆs) + θc(xc)

where for a given c ∈ C all λc→s should be updated simultaneously for s ∈ S(c)  and λ−c
s (xs) is
deﬁned as the sum of messages into s that are not from c. We refer to this algorithm as Generalized
EMPLP (GEMPLP). It is possible to derive an algorithm similar to NMPLP that updates several
clusters simultaneously  but its structure is more involved and we do not address it here.


 Xˆs∈S(c)\s

s (xs) +

max
xc\s

|S(c)|

λ−c

1

7 Related Work

Weiss et al. [11] recently studied the ﬁxed points of a class of max-product like algorithms. Their
analysis focused on properties of ﬁxed points rather than convergence guarantees. Speciﬁcally  they
showed that if the counting numbers used in a generalized max-product algorithm satisfy certain
properties  then its ﬁxed points will be the exact MAP if the beliefs have unique maxima  and for
binary variables the solution can be partially decodable. Both these properties are obtained for the
MPLP ﬁxed points  and in fact we can show that MPLP satisﬁes the conditions in [11]  so that
we obtain these properties as corollaries of [11]. We stress however  that [11] does not address
convergence of algorithms  but rather properties of their ﬁxed points  if they converge.

MPLP is similar in some aspects to Kolmogorov’s TRW-S algorithm [5]. TRW-S is also a monotone
coordinate descent method in a dual of the LP relaxation and its ﬁxed points also have similar

5

guarantees to those of MPLP [6]. Furthermore  convergence to a local optimum may occur  as it
does for MPLP. One advantage of MPLP lies in the simplicity of its updates and the fact that it is
parameter free. The other is its simple generalization to potentials over clusters of nodes (Sec. 6).
Recently  several new dual LP algorithms have been introduced  which are more closely related to
our formalism. Werner [12] presented a class of algorithms which also improve the dual LP at every
iteration. The simplest of those is the max-sum-diffusion algorithm  which is similar to our EMPLP
algorithm  although the updates are different from ours. Independently  Johnson et al. [4] presented
a class of algorithms that improve duals of the MAP-LP using coordinate descent. They decompose
the model into tractable parts by replicating variables and enforce replication constraints within the
Lagrangian dual. Our basic formulation in Eq. 3 could be derived from their perspective. However 
the updates in the algorithm and the analysis differ. Johnson et al. also presented a method for
overcoming the local optimum problem  by smoothing the objective so that it is strictly convex.
Such an approach could also be used within our algorithms. Vontobel and Koetter [9] recently
introduced a coordinate descent algorithm for decoding LDPC codes. Their method is speciﬁcally
tailored for this case  and uses updates that are similar to our edge based updates.

Finally  the concept of dual coordinate descent may be used in approximating marginals as well. In
[3] we use such an approach to optimize a variational bound on the partition function. The derivation
uses some of the ideas used in the MPLP dual  but importantly does not ﬁnd the minimum for each
coordinate. Instead  a gradient like step is taken at every iteration to decrease the dual objective.

8 Experiments

We compared NMPLP to three other message passing algorithms:2 Tree-Reweighted max-product
(TRMP) [10] 3 standard max-product (MP)  and GEMPLP. For MP and TRMP we used the standard
approach of damping messages using a factor of α = 0.5. We ran all algorithms for a maximum of
2000 iterations  and used the hit-time measure to compare their speed of convergence. This measure
is deﬁned as follows: At every iteration the beliefs can be used to obtain an assignment x with value
f (x). We deﬁne the hit-time as the ﬁrst iteration at which the maximum value of f (x) is achieved.4
We ﬁrst experimented with a 10 × 10 grid graph  with 5 values per state. The function f (x) was

a Potts model: f (x) = Pij∈E θijI(xi = xj) + Pi∈V θi(xi).5 The values for θij and θi(xi)

were randomly drawn from [−cI  cI ] and [−cF   cF ] respectively  and we used values of cI and
cF in the range range [0.1  2.35] (with intervals of 0.25)  resulting in 100 different models. The
clusters for GEMPLP were the faces of the graph [14]. To see if NMPLP converges to the LP
solution we also used an LP solver to solve the LP relaxation. We found that the the normalized
difference between NMPLP and LP objective was at most 10−3 (median 10−7)  suggesting that
NMPLP typically converged to the LP solution. Fig. 2 (top row) shows the results for the three
algorithms. It can be seen that while all non-cluster based algorithms obtain similar f (x) values 
NMPLP has better hit-time (in the median) than TRMP and MP  and MP does not converge in many
cases (see caption). GEMPLP converges more slowly than NMPLP  but obtains much better f (x)
values. In fact  in 99% of the cases the normalized difference between the GEMPLP objective and
the f (x) value was less than 10−5  suggesting that the exact MAP solution was found.
We next applied the algorithms to the real world problems of protein design.
In [13]  Yanover
et al. show how these problems can be formalized in terms of ﬁnding a MAP in an appropriately
constructed graphical model.6 We used all algorithms except GNMPLP (since there is no natural
choice for clusters in this case) to approximate the MAP solution on the 97 models used in [13].
In these models the number of states per variable is 2 − 158  and there are up to 180 variables per
model. Fig. 2 (bottom) shows results for all the design problems. In this case only 11% of the MP
runs converged  and NMPLP was better than TRMP in terms of hit-time and comparable in f (x)
value. The performance of MP was good on the runs where it converged.

2As expected  NMPLP was faster than EMPLP so only NMPLP results are given.
3The edge weights for TRMP corresponded to a uniform distribution over all spanning trees.
4This is clearly a post-hoc measure since it can only be obtained after the algorithm has exceeded its maxi-

mum number of iterations. However  it is a reasonable algorithm-independent measure of convergence.

5The potential θi(xi) may be folded into the pairwise potential to yield a model as in Eq. 1.
6Data available from http://jmlr.csail.mit.edu/papers/volume7/yanover06a/Rosetta Design Dataset.tgz

6

(a)

100

50

0

−50

−100

(b)

l

)
e
u
a
V
(
∆

0.04

0.02

0

−0.02

−0.04

−0.06

(c)

2000

1000

0

−1000

(d)

0.6

0.4

0.2

0

−0.2

−0.4

l

)
e
u
a
V
(
∆

)
e
m
T

i

 
t
i

H
(
∆

MP

TRMP

GMPLP

MP

TRMP

GMPLP

MP

TRMP

MP

TRMP

)
e
m
T

i

 
t
i

H
(
∆

Figure 2: Evaluation of message passing algorithms on Potts models and protein design problems.
(a c):
Convergence time results for the Potts models (a) and protein design problems (c). The box-plots (horiz. red
line indicates median) show the difference between the hit-time for the other algorithms and NMPLP. (b d):
Value of integer solutions for the Potts models (b) and protein design problems (d). The box-plots show the
normalized difference between the value of f (x) for NMPLP and the other algorithms. All ﬁgures are such
that better MPLP performance yields positive Y axis values. Max-product converged on 58% of the cases for
the Potts models  and on 11% of the protein problems. Only convergent max-product runs are shown.

9 Conclusion

We have presented a convergent algorithm for MAP approximation that is based on block coordi-
nate descent of the MAP-LP relaxation dual. The algorithm can also be extended to cluster based
functions  which result empirically in improved MAP estimates. This is in line with the observa-
tions in [14] that generalized belief propagation algorithms can result in signiﬁcant performance
improvements. However generalized max-product algorithms [14] are not guaranteed to converge
whereas GMPLP is. Furthermore  the GMPLP algorithm does not require a region graph and only
involves intersection between pairs of clusters. In conclusion  MPLP has the advantage of resolving
the convergence problems of max-product while retaining its simplicity  and offering the theoretical
guarantees of LP relaxations. We thus believe it should be useful in a wide array of applications.

A Derivation of the dual

Before deriving the dual  we ﬁrst express the constraint set ML(G) in a slightly different way. The
deﬁnition of ML(G) in Sec. 2 uses a single distribution µij(xi  xj) for every ij ∈ E. In what
follows  we use two copies of this pairwise distribution for every edge  which we denote ¯µij(xi  xj )
and ¯µji(xj   xi)  and we add the constraint that these two copies both equal the original µij(xi  xj).
For this extended set of pairwise marginals  we consider the following set of constraints which
is clearly equivalent to ML(G). On the rightmost column we give the dual variables that will
correspond to each constraint (we omit non-negativity constraints).

¯µij(xi  xj ) = µij(xi  xj)
¯µji(xj   xi) = µij(xi  xj)

∀ij ∈ E  xi  xj βij(xi  xj)
∀ij ∈ E  xi  xj βji(xj   xi)

¯µij (ˆxi  xj) = µj(xj ) ∀ij ∈ E  xj
∀ji ∈ E  xi
¯µji(ˆxj   xi) = µi(xi)
∀i ∈ V
µi(xi) = 1

λij (xj )
λji(xi)
δi

(6)

Pˆxi
Pˆxj
Pxi

We denote the set of (µ  ¯µ) satisfying these constraints by ¯ML(G). We can now state an LP that
is equivalent to MAPLPR  only with an extended set of variables and constraints. The equivalent
problem is to maximize µ · θ subject to (µ  ¯µ) ∈ ¯ML(G) (note that the objective uses the original
µ copy). LP duality transformation of the extended problem yields the following LP

min Pi δi

s.t.

λij (xj ) − βij(xi  xj ) ≥ 0
βij (xi  xj) + βji(xj  xi) = θij (xi  xj) ∀ij ∈ E  xi  xj

∀ij  ji ∈ E  xi  xj

(7)

−Pk∈N (i) λki(xi) + δi ≥ 0

∀i ∈ V  xi

We next simplify the above LP by eliminating some of its constraints and variables. Since each
variable δi appears in only one constraint  and the objective minimizes δi it follows that δi =

maxxi Pk∈N (i) λki(xi) and the constraints with δi can be discarded. Similarly  since λij (xj ) ap-

pears in a single constraint  we have that for all ij ∈ E  ji ∈ E  xi  xj λij(xj ) = maxxi βij(xi  xj )
and the constraints with λij (xj )  λji(xi) can also be discarded. Using the eliminated δi and λji(xi)

7

variables  we obtain that the LP in Eq. 7 is equivalent to that in Eq. 3. Note that the objective in
Eq. 3 is convex since it is a sum of point-wise maxima of convex functions.

B Proof of Proposition 2

We wish to minimize f in Eq. 4 subject to the constraint that βij + βji = θij. Rewrite f as

f (βij  βji) = max

xi xj hλ−j

i (xi) + βji(xj   xi)i + max
in the max is λ−j

xi xj (cid:2)λ−i

j (xj) + βij(xi  xj)(cid:3)

i (xi) + λ−i

Thus

the minimum must be greater

j (xj) + θij(xi  xj )
than

(8)

sum of

The
(because of
1

two arguments
constraints on β).

the
the
i (xi) + λ−i

2 maxxi xj hλ−j
mum is obtained by requiring an equalization condition:7

j (xj ) + θij(xi  xj)i. One assignment to β that achieves this mini-

j (xj ) + βij(xi  xj ) = λ−j
λ−i

i (xi) + βji(xj  xi) =

1

2 (cid:16)θij (xi  xj) + λ−j

i (xi) + λ−i

j (xj )(cid:17)

(9)

which implies βij(xi  xj) = 1
The resulting λij (xj ) = maxxi βij (xi  xj) are then the ones in Prop. 2.

2 (cid:16)θij (xi  xj ) + λ−j

i (xi) − λ−i

j (xj)(cid:17) and a similar expression for βji.

Acknowledgments
The authors acknowledge support from the Defense Advanced Research Projects Agency (Transfer
Learning program). Amir Globerson was also supported by the Rothschild Yad-Hanadiv fellowship.

References
[1] M. Bayati  D. Shah  and M. Sharma. Maximum weight matching via max-product belief propagation.

IEEE Trans. on Information Theory (to appear)  2007.

[2] D. P. Bertsekas  editor. Nonlinear Programming. Athena Scientiﬁc  Belmont  MA  1995.
[3] A. Globerson and T. Jaakkola. Convergent propagation algorithms via oriented trees. In UAI. 2007.
[4] J.K. Johnson  D.M. Malioutov  and A.S. Willsky. Lagrangian relaxation for map estimation in graphical

models. In Allerton Conf. Communication  Control and Computing  2007.

[5] V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. IEEE Transac-

tions on Pattern Analysis and Machine Intelligence  28(10):1568–1583  2006.

[6] V. Kolmogorov and M. Wainwright. On the optimality of tree-reweighted max-product message passing.

In 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI). 2005.

[7] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann  1988.
[8] B. Taskar  S. Lacoste-Julien  and M. Jordan. Structured prediction  dual extragradient and bregman pro-

jections. Journal of Machine Learning Research  pages 1627–1653  2006.

[9] P.O. Vontobel and R. Koetter. Towards low-complexity linear-programming decoding. In Proc. 4th Int.

Symposium on Turbo Codes and Related Topics  2006.

[10] M. J. Wainwright  T. Jaakkola  and A. S. Willsky. Map estimation via agreement on trees: message-

passing and linear programming. IEEE Trans. on Information Theory  51(11):1120–1146  2005.

[11] Y. Weiss  C. Yanover  and T. Meltzer. Map estimation  linear programming and belief propagation with

convex free energies. In UAI. 2007.

[12] T. Werner. A linear programming approach to max-sum  a review. IEEE Trans. on PAMI  2007.
[13] C. Yanover  T. Meltzer  and Y. Weiss. Linear programming relaxations and belief propagation – an

empirical study. Jourmal of Machine Learning Research  7:1887–1907  2006.

[14] J.S. Yedidia  W.T. W.T. Freeman  and Y. Weiss. Constructing free-energy approximations and generalized

belief propagation algorithms. IEEE Trans. on Information Theory  51(7):2282–2312  2005.

7Other solutions are possible but may not yield some of the properties of MPLP.

8

,Daniel Bartz
Klaus-Robert Müller
Yuanyuan Liu
Fanhua Shang
Wei Fan
James Cheng
Hong Cheng
Kai-Wei Chang
He He
Stephane Ross
Hal Daume III
John Langford