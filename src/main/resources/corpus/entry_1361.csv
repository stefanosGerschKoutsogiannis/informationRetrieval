2017,Deep Learning with Topological Signatures,Inferring topological and geometrical information from data can offer an alternative perspective in machine learning problems. Methods from topological data analysis  e.g.  persistent homology  enable us to obtain such information  typically in the form of summary representations of topological features. However  such topological signatures often  come with an unusual structure (e.g.  multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations  they suffer from being agnostic to the target learning task. In contrast  we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classification experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and  in case of the latter  we even outperform the state-of-the-art by a large margin.,Deep Learning with Topological Signatures

Christoph Hofer

Department of Computer Science
University of Salzburg  Austria

chofer@cosy.sbg.ac.at

Roland Kwitt

Department of Computer Science
University of Salzburg  Austria
Roland.Kwitt@sbg.ac.at

Marc Niethammer

UNC Chapel Hill  NC  USA

mn@cs.unc.edu

Andreas Uhl

Department of Computer Science
University of Salzburg  Austria

uhl@cosy.sbg.ac.at

Abstract

Inferring topological and geometrical information from data can offer an alternative
perspective on machine learning problems. Methods from topological data analysis 
e.g.  persistent homology  enable us to obtain such information  typically in the form
of summary representations of topological features. However  such topological
signatures often come with an unusual structure (e.g.  multisets of intervals) that is
highly impractical for most machine learning techniques. While many strategies
have been proposed to map these topological signatures into machine learning
compatible representations  they suffer from being agnostic to the target learning
task. In contrast  we propose a technique that enables us to input topological
signatures to deep neural networks and learn a task-optimal representation during
training. Our approach is realized as a novel input layer with favorable theoretical
properties. Classiﬁcation experiments on 2D object shapes and social network
graphs demonstrate the versatility of the approach and  in case of the latter  we
even outperform the state-of-the-art by a large margin.

1

Introduction

Methods from algebraic topology have only recently emerged in the machine learning community 
most prominently under the term topological data analysis (TDA) [7]. Since TDA enables us to
infer relevant topological and geometrical information from data  it can offer a novel and potentially
beneﬁcial perspective on various machine learning problems. Two compelling beneﬁts of TDA
are (1) its versatility  i.e.  we are not restricted to any particular kind of data (such as images 
sensor measurements  time-series  graphs  etc.) and (2) its robustness to noise. Several works have
demonstrated that TDA can be beneﬁcial in a diverse set of problems  such as studying the manifold
of natural image patches [8]  analyzing activity patterns of the visual cortex [28]  classiﬁcation of 3D
surface meshes [27  22]  clustering [11]  or recognition of 2D object shapes [29].
Currently  the most widely-used tool from TDA is persistent homology [15  14]. Essentially1 
persistent homology allows us to track topological changes as we analyze data at multiple “scales”.
As the scale changes  topological features (such as connected components  holes  etc.) appear and
disappear. Persistent homology associates a lifespan to these features in the form of a birth and
a death time. The collection of (birth  death) tuples forms a multiset that can be visualized as a
persistence diagram or a barcode  also referred to as a topological signature of the data. However 
leveraging these signatures for learning purposes poses considerable challenges  mostly due to their

1We will make these concepts more concrete in Sec. 2.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Illustration of the proposed network input layer for topological signatures. Each signature  in the
form of a persistence diagram D ∈ D (left)  is projected w.r.t. a collection of structure elements. The layer’s
learnable parameters θ are the locations µi and the scales σi of these elements; ν ∈ R+ is set a-priori and
meant to discount the impact of points with low persistence (and  in many cases  of low discriminative power).
The layer output y is a concatenation of the projections. In this illustration  N = 2 and hence y = (y1  y2)(cid:62).

unusual structure as a multiset. While there exist suitable metrics to compare signatures (e.g.  the
Wasserstein metric)  they are highly impractical for learning  as they require solving optimal matching
problems.
Related work. In order to deal with these issues  several strategies have been proposed. In [2] for
instance  Adcock et al. use invariant theory to “coordinatize” the space of barcodes. This allows to
map barcodes to vectors of ﬁxed size which can then be fed to standard machine learning techniques 
such as support vector machines (SVMs). Alternatively  Adams et al. [1] map barcodes to so-called
persistence images which  upon discretization  can also be interpreted as vectors and used with
standard learning techniques. Along another line of research  Bubenik [6] proposes a mapping
of barcodes into a Banach space. This has been shown to be particularly viable in a statistical
context (see  e.g.  [10]). The mapping outputs a representation referred to as a persistence landscape.
Interestingly  under a speciﬁc choice of parameters  barcodes are mapped into L2(R2) and the
inner-product in that space can be used to construct a valid kernel function. Similar  kernel-based
techniques  have also recently been studied by Reininghaus et al. [27]  Kwitt et al. [20] and Kusano
et al. [19].
While all previously mentioned approaches retain certain stability properties of the original repre-
sentation with respect to common metrics in TDA (such as the Wasserstein or Bottleneck distances) 
they also share one common drawback: the mapping of topological signatures to a representation that
is compatible with existing learning techniques is pre-deﬁned. Consequently  it is ﬁxed and therefore
agnostic to any speciﬁc learning task. This is clearly suboptimal  as the eminent success of deep
neural networks (e.g.  [18  17]) has shown that learning representations is a preferable approach.
Furthermore  techniques based on kernels [27  20  19] for instance  additionally suffer scalability
issues  as training typically scales poorly with the number of samples (e.g.  roughly cubic in case of
kernel-SVMs). In the spirit of end-to-end training  we therefore aim for an approach that allows to
learn a task-optimal representation of topological signatures. We additionally remark that  e.g.  Qi et
al. [25] or Ravanbakhsh et al. [26] have proposed architectures that can handle sets  but only with
ﬁxed size. In our context  this is impractical as the capability of handling sets with varying cardinality
is a requirement to handle persistent homology in a machine learning setting.

Contribution. To realize this idea  we advocate a novel input layer for deep neural networks that
takes a topological signature (in our case  a persistence diagram)  and computes a parametrized
projection that can be learned during network training. Speciﬁcally  this layer is designed such that
its output is stable with respect to the 1-Wasserstein distance (similar to [27] or [1]). To demonstrate
the versatility of this approach  we present experiments on 2D object shape classiﬁcation and the
classiﬁcation of social network graphs. On the latter  we improve the state-of-the-art by a large
margin  clearly demonstrating the power of combining TDA with deep learning in this context.
2 Background

For space reasons  we only provide a brief overview of the concepts that are relevant to this work and
refer the reader to [16] or [14] for further details.
Homology. The key concept of homology theory is to study the properties of some object X by
means of (commutative) algebra. In particular  we assign to X a sequence of modules C0  C1  . . .

2

ν∆DeathBirth(µ1 σ1)(x0 x1)sµ σ ν((x0 x1))x=ρ(p)p=(b d)InputLayerParam.:θ=(µi σi)N−1i=0(1)RotatepointsinDbyπ/4(2)Transform&Project(y1 y2)(cid:62)∈R0+×R0Output:Death-BirthBirth+Death(persistence)(µ2 σ2)νInput:D∈Dwhich are connected by homomorphisms ∂n : Cn → Cn−1 such that im ∂n+1 ⊆ ker ∂n. A structure
of this form is called a chain complex and by studying its homology groups Hn = ker ∂n/ im ∂n+1
we can derive properties of X.
A prominent example of a homology theory is simplicial homology. Throughout this work  it is
the used homology theory and hence we will now concretize the already presented ideas. Let K
be a simplicial complex and Kn its n-skeleton. Then we set Cn(K) as the vector space generated
(cid:80)n
i=0[x0  . . .   xi−1  xi+1  . . .   xn] and linearly extend this to Cn(K)  i.e.  ∂n((cid:80) σi) =(cid:80) ∂n(σi).
(freely) by Kn over Z/2Z2. The connecting homomorphisms ∂n : Cn(K) → Cn−1(K) are
called boundary operators. For a simplex σ = [x0  . . .   xn] ∈ Kn  we deﬁne them as ∂n(σ) =
Persistent homology. Let K be a simplicial complex and (K i)m
i=0 a sequence of simplicial com-
i=0 is called a ﬁltration of K. If we
plexes such that ∅ = K 0 ⊆ K 1 ⊆ ··· ⊆ K m = K. Then  (K i)m
use the extra information provided by the ﬁltration of K  we obtain the following sequence of chain
complexes (left) 

where C i
homology groups  deﬁned by

n = Cn(K i

n) and ι denotes the inclusion. This then leads to the concept of persistent

H i j

n = ker ∂i
n   of these homology groups (i.e.  the n-th persistent Betti numbers) 
The ranks  βi j
capture the number of homological features of dimensionality n (e.g.  connected components for
n = 0  holes for n = 1  etc.) that persist from i to (at least) j. In fact  according to [14  Fundamental
Lemma of Persistent Homology]  the quantities

n+1 ∩ ker ∂i
n)

n = rank H i j

i ≤ j .

n/(im ∂j

for

n = (βi j−1
µi j

n

n ) − (βi−1 j−1

− βi j

n

− βi−1 j

n

)

for

i < j

(1)

encode all the information about the persistent Betti numbers of dimension n.
Topological signatures. A typical way to obtain a ﬁltration of K is to consider sublevel sets of a
function f : C0(K) → R. This function can be easily lifted to higher-dimensional chain groups of
K by

f ([v0  . . .   vn]) = max{f ([vi]) : 0 ≤ i ≤ n} .

i=0 by setting K0 = ∅ and Ki = f−1((−∞  ai]) for
Given m = |f (C0(K))|  we obtain (Ki)m
1 ≤ i ≤ m  where a1 < ··· < am is the sorted sequence of values of f (C0(K)). If we construct
a multiset such that  for i < j  the point (ai  aj) is inserted with multiplicity µi j
n   we effectively
encode the persistent homology of dimension n w.r.t. the sublevel set ﬁltration induced by f. Upon
adding diagonal points with inﬁnite multiplicity  we obtain the following structure:
Deﬁnition 1 (Persistence diagram). Let ∆ = {x ∈ R2
diagonal R2
(cid:63) = {(x0  x1) ∈ R2 : x1 > x0}. A persistence diagram  D  is a multiset of the form
R2

∆ : mult(x) = ∞} be the multiset of the
∆ = {(x0  x1) ∈ R2 : x0 = x1}  where mult denotes the multiplicity function and let

D = {x : x ∈ R2

(cid:63)} ∪ ∆ .

We denote by D the set of all persistence diagrams of the form |D \ ∆| < ∞ .
For a given complex K of dimension nmax and a function f (of the discussed form)  we can interpret
persistent homology as a mapping (K  f ) (cid:55)→ (D0  . . .  Dnmax−1)  where Di is the diagram of
dimension i and nmax the dimension of K. We can additionally add a metric structure to the space of
persistence diagrams by introducing the notion of distances.

2Simplicial homology is not speciﬁc to Z/2Z  but it’s a typical choice  since it allows us to interpret n-chains

as sets of n-simplices.

3

···C12C11C100···C22C21C200···Cm2Cm1Cm00∂3ι∂2ι∂1ι∂0∂3ι∂2ι∂1ι∂0∂3∂2∂1∂0ExampleK1K2K3⊆⊆v2v4v3v1C20=[[v1] [v2] [v3]]Z2C21=[[v1 v3] [v2 v3]]Z2C22=0C10=[[v1] [v2]]Z2C11=0C12=0C20=[[v1] [v2] [v3] [v4]]Z2C21=[[v1 v3] [v2 v3] [v3 v4]]Z2C32=0Deﬁnition 2 (Bottleneck  Wasserstein distance). For two persistence diagrams D and E  we deﬁne
their Bottleneck (w∞) and Wasserstein (wq

p) distances by

(cid:32)(cid:88)
x∈D ||x − η(x)||p

q

(cid:33) 1

p

 

(2)

w∞(D E) = inf

η

x∈D ||x − η(x)||∞ and wq
sup

p(D E) = inf

η

where p  q ∈ N and the inﬁmum is taken over all bijections η : D → E.
Essentially  this facilitates studying stability/continuity properties of topological signatures w.r.t.
metrics in the ﬁltration or complex space; we refer the reader to [12] [13]  [9] for a selection of
important stability results.
Remark. By setting µi ∞
referred to as essential. This change can be lifted to D by setting R2
x1 > x0}. In Sec. 5  we will see that essential features can offer discriminative information.
3 A network layer for topological signatures

  we extend Eq. (1) to features which never disappear  also
(cid:63) = {(x0  x1) ∈ R × (R ∪ {∞}) :

n −βi−1 m

n = βi m

n

In this section  we introduce the proposed (parametrized) network layer for topological signatures
(in the form of persistence diagrams). The key idea is to take any D and deﬁne a projection w.r.t. a
collection (of ﬁxed size N) of structure elements.
In the following  we set R+ := {x ∈ R : x > 0} and R+
0 := {x ∈ R : x ≥ 0}  resp.  and start by
rotating points of D such that points on R2
∆ lie on the x-axis  see Fig. 1. The y-axis can then be
interpreted as the persistence of features. Formally  we let b0 and b1 be the unit vectors in directions
(1  1)(cid:62) and (−1  1)(cid:62) and deﬁne a mapping ρ : R2
0 such that x (cid:55)→ ((cid:104)x  b0(cid:105) (cid:104)x  b1(cid:105)).
∆ clock-wise by π/4. We will later see that this construction is beneﬁcial
This rotates points in R(cid:63) ∪R2
for a closer analysis of the layers’ properties.
Similar to [27  19]  we choose exponential functions as structure elements  but other choices are
possible (see Lemma 1). Differently to [27  19]  however  our structure elements are not at ﬁxed
locations (i.e.  one element per point in D)  but their locations and scales are learned during training.
Deﬁnition 3. Let µ = (µ0  µ1)(cid:62)

∈ R × R+  σ = (σ0  σ1) ∈ R+ × R+ and ν ∈ R+. We deﬁne

∆ → R×R+

(cid:63)∪R2

(3)

(4)

as follows:

(cid:0)(x0  x1)(cid:1) =

sµ σ ν

sµ σ ν : R × R+

0 → R

0 (x0−µ0)2−σ2

1 (x1−µ1)2

 

0 (x0−µ0)2−σ2

1 (ln(

x1

ν )+ν−µ1)2



e−σ2
e−σ2

0 

x1 ∈ [ν ∞)
  x1 ∈ (0  ν)

x1 = 0

Remark. Note that sµ σ ν is continuous in x1 as

(cid:88)
A persistence diagram D is then projected w.r.t. sµ σ ν via
(cid:0)(x0  x1)(cid:1) = 0 = sµ σ ν
(cid:1) + ν(cid:1)

and e(·) is continuous. Further  sµ σ ν is differentiable on R × R+  since

Sµ σ ν : D → R 
(cid:17)

∂(cid:0)ln(cid:0) x1

sµ σ ν(ρ(x)) .

x = lim
x→ν

(cid:16) x

D (cid:55)→

lim
x1→0

sµ σ ν

lim
x→ν

x∈D

and

+ ν

ln

ν

1 = lim
x→ν+

∂x1
∂x1

(x) and

lim
x→ν−

ν

∂x1

(x) = lim
x→ν−

= 1 .

ν
x

(cid:0)(x0  0)(cid:1)

Also note that we use the log-transform in Eq. (4) to guarantee that sµ σ ν satisﬁes the conditions of
Lemma 1; this is  however  only one possible choice.

4

Remark. The intuition behind ν is the following. It is the threshold at which the log-transform starts
to operate. The log-transform  on the other hand  stretches the space between the x-axis and the line
∆. This is necessary since
drawn at x + ν to inﬁnite length. As a consequence  sµ σ ν = 0 for x ∈ R2
otherwise Sµ σ ν(D) = ∞ for D ∈ D (as each persistence diagram contains points at the diagonal
with inﬁnite multiplicity).

Finally  given a collection of Sµi σi ν  we combine them to form the output of the network layer.
Deﬁnition 4. Let N ∈ N  θ = (µi  σi)N−1
i=0 ∈
Sθ ν : D → (R+
0 )N D (cid:55)→

(cid:0)(R × R+) × (R+ × R+)(cid:1)N and ν ∈ R+. We deﬁne

(cid:0)Sµi σi ν(D)(cid:1)N−1

i=0 .

as the concatenation of all N mappings deﬁned in Eq. (4).

Importantly  a network layer implementing Def. 4 is trainable via backpropagation  as (1) sµi σi ν is
differentiable in µi  σi  (2) Sµi σi ν(D) is a ﬁnite sum of sµi σi ν and (3) Sθ ν is just a concatenation.
4 Theoretical properties

In this section  we demonstrate that the proposed layer is stable w.r.t. the 1-Wasserstein distance wq
1 
see Eq. (2). In fact  this claim will follow from a more general result  stating sufﬁcient conditions on
functions s : R2
Lemma 1. Let

0 such that a construction in the form of Eq. (3) is stable w.r.t. wq
1.

∆ → R+

(cid:63) ∪ R2

s : R2

(cid:63) ∪ R2

∆ → R+

0

(i) s is Lipschitz continuous w.r.t. (cid:107) · (cid:107)q and constant Ks

have the following properties:

(ii) s(x(cid:1) = 0  for x ∈ R2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:88)

x∈D

∆

Then  for two persistence diagrams D E ∈ D  it holds that

(cid:88)

y∈E

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ Ks · wq

s(x) −

s(y)

1(D E) .

(5)

Proof. see Appendix B
Remark. At this point  we want to clarify that Lemma 1 is not speciﬁc to sµ σ ν (e.g.  as in Def. 3).
Rather  Lemma 1 yields sufﬁcient conditions to construct a w1-stable input layer. Our choice of
sµ σ ν is just a natural example that fulﬁls those requirements and  hence  Sθ ν is just one possible
representative of a whole family of input layers.
With the result of Lemma 1 in mind  we turn to the speciﬁc case of Sθ ν and analyze its stability
properties w.r.t. wq
Lemma 2. sµ σ ν has absolutely bounded ﬁrst-order partial derivatives w.r.t. x0 and x1 on R × R+.
Proof. see Appendix B
Theorem 1. Sθ ν is Lipschitz continuous with respect to wq
Proof. Lemma 2 immediately implies that sµ σ ν from Eq. (3) is Lipschitz continuous w.r.t || · ||q.
Consequently  s = sµ σ ν ◦ ρ satisﬁes property (i) from Lemma 1; property (ii) from Lemma 1 is
satisﬁed by construction. Hence  Sµ σ ν is Lipschitz continuous w.r.t. wq
1. Consequently  Sθ ν is
Lipschitz in each coordinate and therefore Liptschitz continuous.

1. The following lemma is important in this context.

1 on D.

Interestingly  the stability result of Theorem 1 is comparable to the stability results in [1] or [27]
(which are also w.r.t. wq
1 and in the setting of diagrams with ﬁnitely-many points). However  contrary
to previous works  if we would chop-off the input layer after network training  we would then have a
mapping Sθ ν of persistence diagrams that is speciﬁcally-tailored to the learning task on which the
network was trained.

5

Figure 2: Height function ﬁltration of a “clean” (left  green points) and a “noisy” (right  blue points) shape
along direction d = (0 −1)(cid:62). This example demonstrates the insensitivity of homology towards noise  as the
added noise only (1) slightly shifts the dominant points (upper left corner) and (2) produces additional points
close to the diagonal  which have little impact on the Wasserstein distance and the output of our layer.

5 Experiments

To demonstrate the versatility of the proposed approach  we present experiments with two totally
different types of data: (1) 2D shapes of objects  represented as binary images and (2) social network
graphs  given by their adjacency matrix. In both cases  the learning task is classiﬁcation. In each
experiment we ensured a balanced group size (per label) and used a 90/10 random training/test
split; all reported results are averaged over ﬁve runs with ﬁxed ν = 0.1. In practice  points in input
diagrams were thresholded at 0.01 for computational reasons. Additionally  we conducted a reference
experiment on all datasets using simple vectorization (see Sec. 5.3) of the persistence diagrams in
combination with a linear SVM.
Implementation. All experiments were implemented in PyTorch3  using DIPHA4 and Perseus [23].
Source code is publicly-available at https://github.com/c-hofer/nips2017.

5.1 Classiﬁcation of 2D object shapes

We apply persistent homology combined with our proposed input layer to two different datasets of
binary 2D object shapes: (1) the Animal dataset  introduced in [3] which consists of 20 different
animal classes  100 samples each; (2) the MPEG-7 dataset which consists of 70 classes of different
object/animal contours  20 samples each (see [21] for more details).
Filtration. The requirements to use persistent homology on 2D shapes are twofold: First  we need
to assign a simplicial complex to each shape; second  we need to appropriately ﬁltrate the complex.
While  in principle  we could analyze contour features  such as curvature  and choose a sublevel set
ﬁltration based on that  such a strategy requires substantial preprocessing of the discrete data (e.g. 
smoothing). Instead  we choose to work with the raw pixel data and leverage the persistent homology
transform  introduced by Turner et al. [29]. The ﬁltration in that case is based on sublevel sets of
the height function  computed from multiple directions (see Fig. 2). Practically  this means that we
directly construct a simplicial complex from the binary image. We set K0 as the set of all pixels
which are contained in the object. Then  a 1-simplex [p0  p1] is in the 1-skeleton K1 iff p0 and p1
are 4–neighbors on the pixel grid. To ﬁltrate the constructed complex  we denote by b the barycenter
of the object and with r the radius of its bounding circle around b. Finally  we deﬁne  for [p] ∈ K0
and d ∈ S1  the ﬁltration function by f ([p]) = 1/r · (cid:104)p − b  d(cid:105). Function values are lifted to K1 by
taking the maximum  cf. Sec. 2. Finally  let di be the 32 equidistantly distributed directions in S1 
starting from (1  0)(cid:62). For each shape  we get a vector of persistence diagrams (Di)32
i=1 where Di is
the 0-th diagram obtained by ﬁltration along di. As most objects do not differ in homology groups of
higher dimensions (> 0)  we did not use the corresponding persistence diagrams.
Network architecture. While the full network is listed in the supplementary material  the key
architectural choices are: 32 independent input branches  i.e.  one for each ﬁltration direction. Further 
the i-th branch gets  as input  the vector of persistence diagrams from directions di−1  di and di+1.
This is a straightforward approach to capture dependencies among the ﬁltration directions. We use
cross-entropy loss to train the network for 400 epochs  using stochastic gradient descent (SGD) with
mini-batches of size 128 and an initial learning rate of 0.1 (halved every 25-th epoch).

3https://github.com/pytorch/pytorch
4https://bitbucket.org/dipha/dipha

6

a1a2a3b1b9b2b3b4b5b8b7b6νPersistencediagram(0-dim.features)shiftduetonoiseArtiﬁciallyaddednoiseS1Filtrationdirectionsa1a2a3a1a2a3b1b2 3 4b5b6b7b8b9MPEG-7

Animal

‡Skeleton paths
‡Class segment sets
†ICS
†BCF
Ours

86.7
90.9
96.6
97.2

91.8

67.9
69.7
78.4
83.4

69.5

Figure 3: Left: some examples from the MPEG-7 (bottom) and Animal (top) datasets. Right: Classiﬁcation
results  compared to the two best (†) and two worst (‡) results reported in [30].

Results. Fig. 3 shows a selection of 2D object shapes from both datasets  together with the obtained
classiﬁcation results. We list the two best (†) and two worst (‡) results as reported in [30]. While 
on the one hand  using topological signatures is below the state-of-the-art  the proposed architecture
is still better than other approaches that are speciﬁcally tailored to the problem. Most notably  our
approach does not require any speciﬁc data preprocessing  whereas all other competitors listed in
Fig. 3 require  e.g.  some sort of contour extraction. Furthermore  the proposed architecture readily
generalizes to 3D with the only difference that in this case di ∈ S2. Fig. 4 (Right) shows an exemplary
visualization of the position of the learned structure elements for the Animal dataset.

5.2 Classiﬁcation of social network graphs

In this experiment  we consider the problem of graph classiﬁcation  where vertices are unlabeled
and edges are undirected. That is  a graph G is given by G = (V  E)  where V denotes the set of
vertices and E denotes the set of edges. We evaluate our approach on the challenging problem of
social network classiﬁcation  using the two largest benchmark datasets from [31]  i.e.  reddit-5k
(5 classes  5k graphs) and reddit-12k (11 classes  ≈12k graphs). Each sample in these datasets
represents a discussion graph and the classes indicate subreddits (e.g.  worldnews  video  etc.).
Filtration. The construction of a simplicial complex from G = (V  E) is straightforward: we set
K0 = {[v] ∈ V } and K1 = {[v0  v1] : {v0  v1} ∈ E}. We choose a very simple ﬁltration based on
the vertex degree  i.e.  the number of incident edges to a vertex v ∈ V . Hence  for [v0] ∈ K0 we get
f ([v0]) = deg(v0)/ maxv∈V deg(v) and again lift f to K1 by taking the maximum. Note that chain
groups are trivial for dimension > 1  hence  all features in dimension 1 are essential.
Network architecture. Our network has four input branches: two for each dimension (0 and 1) of
the homological features  split into essential and non-essential ones  see Sec. 2. We train the network
for 500 epochs using SGD and cross-entropy loss with an initial learning rate of 0.1 (reddit_5k)  or
0.4 (reddit_12k). The full network architecture is listed in the supplementary material.
Results. Fig. 5 (right) compares our proposed strategy to state-of-the-art approaches from the
literature. In particular  we compare against (1) the graphlet kernel (GK) and deep graphlet kernel
(DGK) results from [31]  (2) the Patchy-SAN (PSCN) results from [24] and (3) a recently reported
graph-feature + random forest approach (RF) from [4]. As we can see  using topological signatures
in our proposed setting considerably outperforms the current state-of-the-art on both datasets. This is
an interesting observation  as PSCN [24] for instance  also relies on node degrees and an extension of
the convolution operation to graphs. Further  the results reveal that including essential features is key
to these improvements.

5.3 Vectorization of persistence diagrams

Here  we brieﬂy present a reference experiment we conducted following Bendich et al. [5]. The idea
is to directly use the persistence diagrams as features via vectorization. For each point (b  d) in a
persistence diagram D we calculate its persistence  i.e.  d− b. We then sort the calculated persistences
by magnitude from high to low and take the ﬁrst N values. Hence  we get  for each persistence
diagram  a vector of dimension N (if |D \ ∆| < N  we pad with zero). We used this technique
on all four data sets. As can be seen from the results in Table 4 (averaged over 10 cross-validation
runs)  vectorization performs poorly on MPEG-7 and Animal but can lead to competitive rates on
reddit-5k and reddit-12k. Nevertheless  the obtained performance is considerably inferior to our
proposed approach.

7

5

81.8
48.8
37.1
24.2

10

82.3
50.0
38.2
24.6

N

20

79.7
46.2
39.7
27.9

40

74.5
42.4
42.1
29.8

80

68.2
39.3
43.8
31.5

160

64.4
36.0
45.2
31.6

Ours

91.8
69.5
54.5
44.5

MPEG-7
Animal
reddit-5k
reddit-12k

Figure 4: Left: Classiﬁcation accuracies for a linear SVM trained on vectorized (in RN ) persistence diagrams
(see Sec. 5.3). Right: Exemplary visualization of the learned structure elements (in 0-th dimension) for the
Animal dataset and ﬁltration direction d = (−1  0)(cid:62). Centers of the learned elements are marked in blue.

reddit-5k

reddit-12k

GK [31]
DGK [31]
PSCN [24]
RF [4]
Ours (w/o essential)
Ours (w/ essential)

41.0
41.3
49.1
50.9

49.1
54.5

31.8
32.2
41.3
42.7

38.5
44.5

Figure 5: Left: Illustration of graph ﬁltration by vertex degree  i.e.  f ≡ deg (for different choices of ai  see
Sec. 2). Right: Classiﬁcation results as reported in [31] for GK and DGK  Patchy-SAN (PSCN) as reported in
[24] and feature-based random-forest (RF) classiﬁcation from [4].

.

Finally  we remark that in both experiments  tests with the kernel of [27] turned out to be computa-
tionally impractical  (1) on shape data due to the need to evaluate the kernel for all ﬁltration directions
and (2) on graphs due the large number of samples and the number of points in each diagram.

6 Discussion

We have presented  to the best of our knowledge  the ﬁrst approach towards learning task-optimal
stable representations of topological signatures  in our case persistence diagrams. Our particular
realization of this idea  i.e.  as an input layer to deep neural networks  not only enables us to learn with
topological signatures  but also to use them as additional (and potentially complementary) inputs to
existing deep architectures. From a theoretical point of view  we remark that the presented structure
elements are not restricted to exponential functions  so long as the conditions of Lemma 1 are met.
One drawback of the proposed approach  however  is the artiﬁcial bending of the persistence axis (see
Fig. 1) by a logarithmic transformation; in fact  other strategies might be possible and better suited
in certain situations. A detailed investigation of this issue is left for future work. From a practical
perspective  it is also worth pointing out that  in principle  the proposed layer could be used to handle
any kind of input that comes in the form of multisets (of Rn)  whereas previous works only allow
to handle sets of ﬁxed size (see Sec. 1). In summary  we argue that our experiments show strong
evidence that topological features of data can be beneﬁcial in many learning tasks  not necessarily to
replace existing inputs  but rather as a complementary source of discriminative information.

Acknowledgements. This work was partially funded by the Austrian Science Fund FWF (KLI
project 00012) and the Spinal Cord Injury and Tissue Regeneration Center Salzburg (SCI-TReCS) 
Paracelsus Medical University  Salzburg.

8

0.00.20.40.60.81.00.00.20.40.60.81.0BirthDeathG=(V E)212311115f−1((−∞ 2])f−1((−∞ 5])f−1((−∞ 3])1A Technical results
Lemma 3. Let α ∈ R+ and β ∈ R. We have

i)

lim
x→0

ln(x)

x

· e−α(ln(x)+β)2 = 0

ii)

lim
x→0

1

x · e−α(ln(x)+β)2 = 0 .

Proof. We omit the proof for brevity (see supplementary material for details)  but remark that only i)
needs to be shown as ii) follows immediately.

B Proofs

(cid:124)

(cid:124)

D = ϕ
= (ϕ

−1(∆)
∪ (ϕ

(cid:123)(cid:122)
(cid:125)
−1(E0) ∩ ∆)

Proof of Lemma 1. Let ϕ be a bijection between D and E which realizes wq
1(D E) and let D0 =
D \ ∆  E0 = E \ ∆. To show the result of Eq. (5)  we consider the following decomposition:
(cid:123)(cid:122)
(cid:125)
−1(∆) ∩ ∆)
Except for the term D  all sets are ﬁnite. In fact  ϕ realizes the Wasserstein distance wq

ϕ(cid:12)(cid:12)D
1 which implies
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
= id. Therefore  s(x) = s(ϕ(x)) = 0 for x ∈ D since D ⊂ ∆. Consequently  we can ignore D
(cid:88)
(cid:88)
in the summation and it sufﬁces to consider E = A ∪ B ∪ C. It follows that

(cid:88)

(cid:88)

s(x) −

s(x) −

s(ϕ(x))

∪ (ϕ

x∈D

x∈D

x∈E

s(y)

y∈E

(6)

B

(cid:124)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:88)

C

D

(cid:124)

∪ (ϕ

(cid:123)(cid:122)
(cid:125)
−1(∆) \ ∆)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)
(cid:88)
|s(x) − s(ϕ(x))|
x∈D ||x − ϕ(x)||q = Ks · wq

s(x) −

s(ϕ(x))

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

x∈E

A

−1(E0) ∪ ϕ
(cid:125)
(cid:123)(cid:122)
−1(E0) \ ∆)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

x∈D

=

x∈E
≤ Ks ·

x∈E
||x − ϕ(x)||q = Ks ·

s(x) − s(ϕ(x))

(cid:88)

x∈E

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤

1(D E) .

Proof of Lemma 2. Since sµ σ ν is deﬁned differently for x1 ∈ [ν ∞) and x1 ∈ (0  ν)  we need to
distinguish these two cases. In the following x0 ∈ R.
(1) x1 ∈ [ν ∞): The partial derivative w.r.t. xi is given as
−σ2

i (xi−µi)2(cid:19)

(cid:18) ∂

(cid:18) ∂

(cid:19)

(x0  x1)

(7)

sµ σ ν

∂xi

(x0  x1) = C ·
= C · e

e

∂xi
−σ2

i (xi−µi)2

· (−2σ2

i )(xi − µi)  

where C is just the part of exp(·) which is not dependent on xi. For all cases  i.e.  x0 → ∞  x0 →
−∞ and x1 → ∞  it holds that Eq. (7) → 0.
(2) x1 ∈ (0  ν): The partial derivative w.r.t. x0 is similar to Eq. (7) with the same asymptotic
behaviour for x0 → ∞ and x0 → −∞. However  for the partial derivative w.r.t. x1 we get

(cid:19)

(cid:18) ∂

sµ σ ν

∂x1

(x0  x1) = C ·

e

x1

1 (ln(

∂x1

−σ2

(cid:18) ∂
(cid:16)

ν )+ν−µ1)2(cid:19)
(cid:16)
(cid:16) x1
(cid:17)
(cid:19)
(cid:18)
(cid:16) x1
(cid:17)
= C · e( ... ) · (−2σ2
1) ·
(cid:125)
(cid:123)(cid:122)
ln
= C

(cid:124)
e( ... ) ·

ν
1
x1

ln

ν

·

·

(cid:48)

(a)

(x0  x1)

(cid:17)

ν
x1

·

+ ν − µ1
(cid:123)(cid:122)
+(ν − µ1) · e( ... ) ·

(cid:124)

(8)

(cid:17)

.

1
x1

(cid:125)

(b)

As x1 → 0  we can invoke Lemma 3 i) to handle (a) and Lemma 3 ii) to handle (b); conclusively 
Eq. (8) → 0. As the partial derivatives w.r.t. xi are continuous and their limits are 0 on R  R+  resp. 
we conclude that they are absolutely bounded.

9

References
[1] H. Adams  T. Emerson  M. Kirby  R. Neville  C. Peterson  P. Shipman  S. Chepushtanova  E. Hanson 
F. Motta  and L. Ziegelmeier. Persistence images: A stable vector representation of persistent homology.
JMLR  18(8):1–35  2017. 2  5

[2] A. Adcock  E. Carlsson  and G. Carlsson. The ring of algebraic functions on persistence bar codes. CoRR 

2013. https://arxiv.org/abs/1304.0530. 2

[3] X. Bai  W. Liu  and Z. Tu. Integrating contour and skeleton for shape classiﬁcation. In ICCV Workshops 

2009. 6

[4] I. Barnett  N. Malik  M.L. Kuijjer  P.J. Mucha  and J.-P. Onnela. Feature-based classiﬁcation of networks.

CoRR  2016. https://arxiv.org/abs/1610.05868. 7  8

[5] P. Bendich  J.S. Marron  E. Miller  A. Pieloch  and S. Skwerer. Persistent homology analysis of brain artery

trees. Ann. Appl. Stat  10(2)  2016. 7

[6] P. Bubenik. Statistical topological data analysis using persistence landscapes. JMLR  16(1):77–102  2015.

2

[7] G. Carlsson. Topology and data. Bull. Amer. Math. Soc.  46:255–308  2009. 1
[8] G. Carlsson  T. Ishkhanov  V. de Silva  and A. Zomorodian. On the local behavior of spaces of natural

images. IJCV  76:1–12  2008. 1

[9] F. Chazal  D. Cohen-Steiner  L. J. Guibas  F. Mémoli  and S. Y. Oudot. Gromov-Hausdorff stable signatures

for shapes using persistence. Comput. Graph. Forum  28(5):1393–1403  2009. 4

[10] F. Chazal  B.T. Fasy  F. Lecci  A. Rinaldo  and L. Wassermann. Stochastic convergence of persistence

landscapes and silhouettes. JoCG  6(2):140–161  2014. 2

[11] F. Chazal  L.J. Guibas  S.Y. Oudot  and P. Skraba. Persistence-based clustering in Riemannian manifolds.

J. ACM  60(6):41–79  2013. 1

[12] D. Cohen-Steiner  H. Edelsbrunner  and J. Harer. Stability of persistence diagrams. Discrete Comput.

Geom.  37(1):103–120  2007. 4

[13] D. Cohen-Steiner  H. Edelsbrunner  J. Harer  and Y. Mileyko. Lipschitz functions have Lp-stable persistence.

Found. Comput. Math.  10(2):127–139  2010. 4

[14] H. Edelsbrunner and J. L. Harer. Computational Topology : An Introduction. American Mathematical

Society  2010. 1  2  3

[15] H. Edelsbrunner  D. Letcher  and A. Zomorodian. Topological persistence and simpliﬁcation. Discrete

Comput. Geom.  28(4):511–533  2002. 1

[16] A. Hatcher. Algebraic Topology. Cambridge University Press  Cambridge  2002. 2
[17] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR  2016. 2
[18] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In NIPS  2012. 2

[19] G. Kusano  K. Fukumizu  and Y. Hiraoka. Persistence weighted Gaussian kernel for topological data

analysis. In ICML  2016. 2  4

[20] R. Kwitt  S. Huber  M. Niethammer  W. Lin  and U. Bauer. Statistical topological data analysis - a kernel

perspective. In NIPS  2015. 2

[21] L. Latecki  R. Lakamper  and T. Eckhardt. Shape descriptors for non-rigid shapes with a single closed

contour. In CVPR  2000. 6

[22] C. Li  M. Ovsjanikov  and F. Chazal. Persistence-based structural recognition. In CVPR  2014. 1
[23] K. Mischaikow and V. Nanda. Morse theory for ﬁltrations and efﬁcient computation of persistent homology.

Discrete Comput. Geom.  50(2):330–353  2013. 6

[24] M. Niepert  M. Ahmed  and K. Kutzkov. Learning convolutional neural networks for graphs. In ICML 

2016. 7  8

[25] C.R. Qi  H. Su  K. Mo  and L.J. Guibas. PointNet: Deep learning on point sets for 3D classiﬁcation and

segmentation. In CVPR  2017. 2

[26] S. Ravanbakhsh  S. Schneider  and B. Póczos. Deep learning with sets and point clouds. In ICLR  2017. 2
[27] R. Reininghaus  U. Bauer  S. Huber  and R. Kwitt. A stable multi-scale kernel for topological machine

learning. In CVPR  2015. 1  2  4  5  8

[28] G. Singh  F. Memoli  T. Ishkhanov  G. Sapiro  G. Carlsson  and D.L. Ringach. Topological analysis of

population activity in visual cortex. J. Vis.  8(8)  2008. 1

10

[29] K. Turner  S. Mukherjee  and D. M. Boyer. Persistent homology transform for modeling shapes and

surfaces. Inf. Inference  3(4):310–344  2014. 1  6

[30] X. Wang  B. Feng  X. Bai  W. Liu  and L.J. Latecki. Bag of contour fragments for robust shape classiﬁcation.

Pattern Recognit.  47(6):2116–2125  2014. 7

[31] P. Yanardag and S.V.N. Vishwanathan. Deep graph kernels. In KDD  2015. 7  8

11

,Christoph Hofer
Roland Kwitt
Marc Niethammer
Andreas Uhl