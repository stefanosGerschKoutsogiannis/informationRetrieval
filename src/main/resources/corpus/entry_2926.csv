2018,Learning Hierarchical Semantic Image Manipulation through Structured Representations,Understanding  reasoning  and manipulating semantic concepts of images have been a fundamental research problem for decades. Previous work mainly focused on direct manipulation of natural image manifold through color strokes  key-points  textures  and holes-to-fill. In this work  we present a novel hierarchical framework for semantic image manipulation. Key to our hierarchical framework is that we employ structured semantic layout as our intermediate representations for manipulation. Initialized with coarse-level bounding boxes  our layout generator first creates pixel-wise semantic layout capturing the object shape  object-object interactions  and object-scene relations. Then our image generator fills in the pixel-level textures guided by the semantic layout. Such framework allows a user to manipulate images at object-level by adding  removing  and moving one bounding box at a time. Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models  both qualitatively and quantitatively. Benefits of the hierarchical framework are further demonstrated in applications such as semantic object manipulation  interactive image editing  and data-driven image manipulation.,Learning Hierarchical Semantic Image Manipulation

through Structured Representations

Xinchen Yan†

Thomas Huang†

Honglak Lee‡ †

Seunghoon Hong†

†University of Michigan
†{hongseu xcyan thomaseh honglak}@umich.edu

‡Google Brain

‡honglak@google.com

Abstract

Understanding  reasoning  and manipulating semantic concepts of images have
been a fundamental research problem for decades. Previous work mainly focused
on direct manipulation on natural image manifold through color strokes  key-
points  textures  and holes-to-ﬁll. In this work  we present a novel hierarchical
framework for semantic image manipulation. Key to our hierarchical framework is
that we employ structured semantic layout as our intermediate representation for
manipulation. Initialized with coarse-level bounding boxes  our structure generator
ﬁrst creates pixel-wise semantic layout capturing the object shape  object-object
interactions  and object-scene relations. Then our image generator ﬁlls in the
pixel-level textures guided by the semantic layout. Such framework allows a
user to manipulate images at object-level by adding  removing  and moving one
bounding box at a time. Experimental evaluations demonstrate the advantages
of the hierarchical manipulation framework over existing image generation and
context hole-ﬁling models  both qualitatively and quantitatively. Beneﬁts of the
hierarchical framework are further demonstrated in applications such as semantic
object manipulation  interactive image editing  and data-driven image manipulation.

Introduction

1
Learning to perceive  reason and manipulate images has been one of the core research problems in
computer vision  machine learning and graphics for decades [1  7  8  9]. Recently the problem has
been actively studied in interactive image editing using deep neural networks  where the goal is to
manipulate an image according to the various types of user-controls  such as color strokes [20  32] 
key-points [19  32]  textures [26]  and holes-to-ﬁll (in-painting) [17]. While these interactive image
editing approaches have made good advances in synthesizing high-quality manipulation results  they
are limited to direct manipulation on natural image manifold.
The main focus of this paper is to achieve semantic-level manipulation of images.
Instead of
manipulating images on natural image manifold  we consider semantic label map as an interface for
manipulation. By editing the label map  users are able to specify the desired images at semantic-level 
such as the location  object class  and object shape. Recently  approaches based on image-to-
image translation [3  11  24] have demonstrated promising results on semantic image manipulation.
However  the existing works mostly focused on learning a style transformation function from label
maps to pixels  while manipulation of structure of the labels remains fully responsible to users. The
requirement on direct control over pixel-wise labels makes the manipulation task still challenging
since it requires a precise and considerable amount of user inputs to specify the structure of the
objects and scene. Although the problem can be partly addressed by template-based manipulation
interface (e.g. adding the objects from the pre-deﬁned sets of template masks [24]  blind pasting
of the object mask is problematic since the structure of the object should be determined adaptively
depending on the surrounding context.
In this work  we tackle the task of semantic image manipulation as a hierarchical generative process.
We start our image manipulation task from a coarse-level abstraction of the scene: a set of semantic

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Overall pipeline of the proposed semantic manipulation framework.

bounding boxes which provide both semantic (what) and spatial (where) information of the scene in
an object-level. Such representation is natural and ﬂexible that enables users to manipulate the scene
layout by adding  removing  and moving each bounding box. To facilitate the image manipulation
from coarse-level semantic bounding boxes  we introduce a hierarchical generation model that
predicts the image in multiple abstraction levels. Our model consists of two parts: layout and image
generators. Speciﬁcally  our structure generator ﬁrst infers the ﬁne-grained semantic label maps
from the coarse object bounding boxes  which produces structure (shape) of the manipulated object
aligned with the context. Given the predicted layout  our image generator infers the style (color and
textures) of the object considering the perceptual consistency to the surroundings. This way  when
adding  removing  and moving semantic bounding boxes  our model can generate an appropriate
image seamlessly integrated into the surrounding image.
We present two applications of the proposed method on interactive and data-driven image editing.
In the experiment on interactive image editing  we show that our method allows users to easily
manipulate images using object bounding boxes  while the structure and style of the generated objects
are adaptively determined depending on the context and naturally blended into the scene. Also  we
show that our simple object-level manipulation interface can be naturally extended to data-driven
image manipulation  by automatically sampling the object boxes and generating the novel images.
The beneﬁts of the hierarchical image manipulation are three-fold. First  it supports richer manip-
ulation tasks such as adding  moving or removing objects through object-level control while the
ﬁne-grained object structures are inferred by the model. Second  when conditioned on coarse and
ﬁne-grained semantic representations  the proposed model produces better image manipulation results
compared to models learned without structural control. Finally  we demonstrate the effectiveness of
the proposed idea on interactive and automatic image manipulation.
2 Related Work
Deep visual manipulation. Visual manipulation is a task of synthesizing the new image by ma-
nipulating parts of a reference image. Thanks to the emergence of generative adversarial networks
(GANs) [6] and perceptual features discovered from deep convolutional neural networks [13  21  22] 
neural image manipulation [5  14  17  20  30  32] has gained increasing popularity in recent years.
Zhu et al. [32] presented an novel image editing framework with a direct constraint capturing real
image statistics using deep convolutional generative adversarial networks [18]. Li et al. [14] and Yeh
et al. [30] investigated semantic face image editing and completion using convolution encoder-decoder
architecture  jointly trained with a pixel-wise reconstruction constraint  a perceptual (adversarial)
constraint  and a semantic structure constraint. In addition  Pathak et al. [17] and Denton et al.
[5] studied context-conditioned image generation that performs pixel-level hole-ﬁlling given the
surrounding regions using deep neural networks  trained with adversarial and reconstruction loss.
Contrary to the previous works that manipulate images based on low-level visual controls such
as visual context [5  14  17  30] or color strokes [20  32]  our model allows semantic control over
manipulation process through labeled bounding box and the inferred semantic layout.
Structure-conditional image generation. Starting from the pixel-wise semantic structure  recent
breakthroughs approached the structure-conditional image generation through direct image-to-image
translation [3  11  16  33]. Isola et al. [11] employed a convolutional encoder-decoder network
with conditional adversarial objective to learn label-to-pixel mapping. Later approaches improved
the generation quality by incoorporating perceptual loss from a pre-trained classiﬁer [3] or feature-

2

matching loss from multi-scale discriminators [24]. In particular  Wang et al. [24] demonstrated a
high-resolution image synthesis results and its application to semantic manipulation by controlling the
pixel-wise labels. Contrary to the previous works focusing on learning a direct mapping from label
to image  our model learns hierarchical mapping from coarse bounding box to image by inferring
intermediate label maps. Our work is closely related to Hong et al. [10]  which generates an image
from a text description through multiple levels of abstraction consisting of bounding boxes  semantic
layouts  and ﬁnally pixels. Contrary to Hong et al. [10]  however  our work focuses on manipulation
of parts of an image  which requires incorporation of both semantic and visual context of surrounding
regions in the hierarchical generation process in such a way that structure and style of the generated
object are aligned with the other parts of an image.
3 Hierarchical Image Manipulation
Given an input image I in ∈ RH×W×3  our goal is to synthesize the new image I out by manipulating
its underlying semantic structure. Let M in ∈ RH×W×C denotes a semantic label map of the image
deﬁned over C categories  which is either given as ground-truth (in training time) or inferred by the
pre-trained visual recognition model (in testing time). Then our goal is to synthesize the new image
by manipulating M in  which allows the semantically-guided manipulation of an image.
The key idea of this paper is to introduce an object bounding box B as an abstracted interface to
manipulate the semantic label map. Speciﬁcally  we deﬁne a controllable bounding box B = {b  c}
as a combination of box corners b ∈ R4 and a class label c = {0  . . .   C}  which represents the
location  size and category of an object. By adding the new box or modifying the parameters of
existing boxes  a user can manipulate the image through adding  moving or deleting the objects1.
Given an object-level speciﬁcation by B  the image manipulation is then posed as a hierarchical
generation task from a coarse bounding box to pixel-level predictions of structure and style.
Figure 1 illustrates the overall pipeline of the proposed algorithm. When the new bounding box B
is given  our algorithm ﬁrst extracts the local observations of label map M ∈ RS×S×C and image
I ∈ RS×S×3 by cropping squared windows of size S × S centered around B. Then conditioned on
M  I and B  our model generates the manipulated image by the following procedures:

• Given a bounding box B and the semantic label map M  the structure generator predicts the

manipulated semantic label map by ˆM = GM (M  B) (Section 3.1)

• Given the manipulated label map ˆM and image I  the image generator predicts the manipu-

lated image ˆI by ˆI = GI ( ˆM   I) (Section 3.2)

After generating the manipulated image patch ˆI  we place it back to the original image to ﬁnish the
manipulation task. The manipulation of multiple objects is performed iteratively by applying the
above procedures for each box. In the following  we explain the manipulation pipeline on a single
box B.
3.1 Structure generator
The goal of the structure generator is to infer the latent structure of the region speciﬁed by B = {b  c}
in the form of pixel-wise class labels ˆM ∈ RS×S×C. The outputs of the structure generator should
reﬂect the class-speciﬁc structure of the object deﬁned by B as well as interactions of the generated
object with the surrounding context (e.g. a person riding a motorcycle). To consider both conditions
in the generation process  the structure generator incorporates the label map M and the bounding box
B as inputs and performs a conditional generation by ˆM = GM (M  B).
Figure 2 illustrates the overall architecture of the structure generator. The structure generator takes in
the masked layout ¯M ∈ RS×S×C and the binary mask ¯B ∈ RS×S×1  where ¯Mijc = 1 and ¯Bij = 1
for all pixels (i  j) inside the box for class c. Given these inputs  the model predicts the manipulated
outcome using two decoding pathways.
Our design principle is motivated by generative layered image modeling [19  23  27  28  29]  which
generates foreground (i.e. object) and background (i.e. context) using separate output streams. In our
model  the foreground output stream produces the predictions on binary object mask ˆMobj ∈ RS×S×1 
which deﬁnes the object shape tightly bounded by object box B. The background output stream
produces per-pixel label maps ˆMctx ∈ RS×S×C inside B.

1We used c = 0 to indicate a deletion operation  where the model ﬁlls the labels with surroundings.

3

Figure 2: Architecture of the structure generator. Given a masked layout ¯M and a binary mask
¯B encoding the class and location of the object  respectively  the model produces the manipulated
layout ˆM by the outputs from the two-stream decoder corresponding to the binary mask of object
and semantic label map of entire region inside the box.
The objective for our structure generator is then given by

Llayout = Ladv( ˆMobj  M∗

obj) + λobjLrecon( ˆMobj  M∗

obj) + λctxLrecon( ˆMctx  ¯M ) 

(1)
obj is the ground-truth binary object mask on B and Lrecon(· ·) is the reconstruction loss
obj) is the conditional adversarial loss deﬁned on object mask

where M∗
using cross-entropy. Ladv( ˆMobj  M∗
ensuring the perceptual quality of predicted object shape  which is given by

Ladv( ˆMobj  M∗

obj) = EM∗

obj

(cid:2)log(DM (M∗

obj  ¯M ))(cid:3) + E ˆMobj

(cid:104)

(cid:105)

1 − log(DM ( ˆMobj  ¯M ))

 

(2)

where DM (· ·) is a conditional discriminator.
During inference  we construct the manipulated layout ˆM using ˆMobj and ˆMctx. Contrary to the prior
works on layered image model  we selectively choose outputs from either foreground or background
streams depending on the manipulation operation deﬁned by B. The output ˆM is given by

ˆM =

if c = 0 (deletion)
( ˆMobj1c) + (1 − ( ˆMobj1c)) (cid:12) M otherwise (addition)

ˆMctx

 

(3)

(cid:26)

Image generator

where c is the class label of the bounding box B and 1c ∈ {0  1}1×C is the one-hot encoded vector
of the class c.
3.2
Given an image I and the manipulated layout ˆM obtained by the structure generator  the image
generator outputs a pixel-level prediction of the contents inside the regions deﬁned by B. To make
the prediction being semantically meaningful and perceptually plausible  the output from the image
generator should reﬂect the semantic structure deﬁned by the layout while being coherent in its style
(e.g. color and texture) with the surrounding image. We design the conditional image generator
GI such that ˆI = GI (I  ˆM )  where I  ˆI ∈ RS×S×3 represent the local image before and after
manipulation with respect to bounding box B.
Figure 3 illustrates the overall architecture of the image generator. The model takes the masked
image ¯I whose pixels inside the box are ﬁlled with 0 and the manipulated layout ˆM as inputs  and
produces the manipulated image ˆI as output. We design the image generator GI to have a two-stream
convolutional encoder and a single-stream convolutional decoder connected by an intermediate
feature map F . As we see in the ﬁgure  the convolutional encoder has two separate downsampling
streams  which we referred to as image encoder fimage( ¯I) and layout encoder flayout( ˆM )  respectively.
The intermediate feature F is obtained by an element-wise feature interaction layer gated by the
binary mask BF on object location.

F = flayout( ˆM ) (cid:12) BF + fimage( ¯I) (cid:12) (1 − BF ).

(4)
Finally  the convolutional decoder gimage(·) takes the fused feature F as input and produces the
manipulated image through ˆI = gimage(F ). Note that we use the ground-truth layout M during model
training but the predicted layout ˆM is used at the inference time during testing.

4

Masked LayoutForeground branchBackground branchFigure 3: Architecture of the image generator. Given a masked image ¯I and the semantic layout ˆM 
the model encodes visual style and semantic structure of the object using separate encoding pathways
and produces the manipulated image.
We deﬁne the following loss for the image generation task.

(5)

Limage = Ladv( ˆI  I) + λfeatureLfeature( ˆI  I).
The ﬁrst term Ladv( ˆI  I) is the conditional adversarial loss deﬁned by

(cid:104)

(cid:105)

(cid:104)

+ E ˆI

(cid:105)

(cid:88)

Ladv( ˆI  I) = EI

(6)
The second term Lfeature( ˆI  I) is the feature matching loss [24]. Speciﬁcally  we compute the distance
between the real and manipulated images using the intermediate features from the discriminator by

log(DI (I  ˆM ))

1 − log(DI ( ˆI  ˆM ))

.

Lfeature( ˆI  I) = E

I  ˆI

(cid:107)D(i)(I  ˆM ) − D(i)( ˆI  ˆM )(cid:107)2
F  

(7)

i=1

Implementation Details

where D(i) is the outputs of ith layer in discriminator  (cid:107) · (cid:107)F is the Frobenius norm.
Discussions. The proposed model encodes both image and layout using two-stream encoding
pathways. With only image encoder  it simply performs image in-painting [17]  which attempts to ﬁll
the hole with patterns coherent with the surrounding region. On the other hand  our model with only
layout encoder becomes similar to image-to-image translation models [3  11  24]  which translates
the pixel-wise semantic label maps to the RGB pixel values. Intuitively  by combining information
from both encoders  the model learns to manipulate images that reﬂect the underlying image structure
deﬁned by the label map with appearance patterns naturally blending into the surrounding context 
which is semantically meaningful and perceptually plausible.
4 Experiments
4.1
Datasets. We conduct both quantitative and qualitative evaluations on the Cityscape dataset [4]  a
semantic understanding benchmark of European urban street scenes containing 5 000 high-resolution
images with ﬁne-grained annotations including instance-wise map and semantic map from 30
categories. Among 30 semantic categories  we treat 10 of them including person  rider  car 
truck  bus  caravan  trailer  train  motorcycle  bicycle as our foreground object classes
while leaving the rest as background classes for image editing purpose. For evaluation  we measure
the generation performance on each of the foreground object bounding box in 500 validation images.
To further demonstrate the image manipulation results on more complex scene  we also conduct
qualitative experiments on bedroom images from ADE20K dataset [31]. Among 49 object categories
frequently appearing in a bedroom  we select 31 movable ones as foreground objects for manipulation.
Training. As one challenge  collecting ground-truth examples before and after image manipulation
is expensive and time-consuming. Instead  we simulate the addition (c ∈ {1  ...  C}) and deletion
(c = 0) operations by sampling boxes from the object and random image regions  respectively. For
training  we employ an Adam optimizer [12] with learning rate 0.0002  β1 = 0.5  β2 = 0.999 and
linearly decrease the learning rate after the ﬁrst 100-epochs for training. The hyper-parameters λobj 
λctx  λfeature are set to 10. Our PyTorch implementation will be open-sourced.

5

Image EncoderLayout EncoderPredicted ImageSingleStream-Image
SingleStream-Layout
SingleStream-Concat

TwoStream

TwoStream-Pred

Layout

-
GT
GT
GT

Predicted

SSIM
0.285
0.291
0.331
0.336
0.299

Segmentation (%) Human eval. (%)

59.6
71.5
76.7
78.5
77.9

12.3
9.2
23.2
33.6
20.7

Table 1: Comparisons between variants of the proposed method. The last two rows (TwoStream and
TwoStream-Pred) correspond to our full model using ground-truth and predicted layout  respectively.

Figure 4: Qualitative comparisons to the baselines in Table 1. The ﬁrst two columns show the
masked image and ground-truth layout used as input to the models (except TwoStream-Pred). The
manipulated objects are indicated by blue arrows. Best viewed in color.

Evaluation metrics. We employ three metrics to measure the perceptual and conditional generation
quality. We use Structural Similarity Index (SSIM) [25] to evaluate the similarity of the ground-
truth and predicted images based on low-level visual statistics. To measure the quality of layout-
conditional image generation  we apply a pre-trained semantic segmentation model DeepLab v3 [2]
to the generated images  and measure the consistency between the input layout and the predicted
segmentation labels in terms of pixel-wise accuracy (layout → image → layout). Finally  we conduct
user study using Mechanical Turk (AMT) to evaluate the perceptual generation quality. We present
the manipulation results of different methods together with the input image and the class label of the
bounding box and ask users to choose the best method based on how natural the manipulated images
are. We collect the results for 1 000 examples  each of which is evaluated by 5 different Turkers. We
report the performance of each method based on the ratio that method ranked as the best in AMT.
4.2 Quantitative Evaluation
Ablation study. To analyze the effectiveness of the proposed method  we conduct an ablation
study on several variants of our method. We ﬁrst consider three baselines of our image generator:
conditioned only on image context (SingleStream-Image)  conditioned only on semantic layout
(SingleStream-Layout)  or conditioned on both by concatenation but using a single encoding
pathway (SingleStream-Concat). We also compare our full model using a ground-truth and the
predicted layouts (TwoStream and TwoStream-Pred). Table 1 summarize the results.
As shown in Table 1  conditioning the generation with only image or layout leads to either
poor class-conditional generation (SingleStream-Image) or less perceptually plausible results
(SingleStream-Layout).
It is because the former misses the semantic layout encoding criti-
cal information on which object to generate  while the later misses color and textures of the im-
age that makes the generation results visually consistent with its surroundings. Combining both
(SingleStream-Concat)  the generation quality improves in all metrics  which shows comple-
mentary beneﬁts of both conditions in image manipulation. In addition  a comparison between
SingleStream-Concat and TwoStream shows that modeling the image and layout conditions
using separate encoding pathways further improves the generation quality. Finally  replacing the
layout condition from the ground-truth (TwoStream) to the predicted one (TwoStream-Pred) leads

6

Context LayoutSingleStreamImageSingleStreamLayoutSingleStreamConcatTwoStreamTwoStreamPredContext Encoder [17]
Context Encoder++

Pix2PixHD [24]

TwoStream

Layout

-
GT
GT
GT

SSIM
0.265
0.269
0.288
0.336

Table 2: Quantitative comparison to other methods.

Segmentation (%) Human eval. (%)

26.4
35.7
79.6
78.5

1.1
1.4
18.0
79.5

Figure 5: Generation results on various locations in an image.

Figure 6: Generation results in diverse contexts.

to a small degree of degradation in perceptual quality partly due to the prediction errors in layout
generation. However  clear improvement of TwoStream-Pred over SingleStream-Image shows
the effectiveness of layout prediction in image generation.
Figure 4 shows the qualitative comparisons of the baselines. Among all variants  our TwoStream
model tends to exhibit most recognizable and coherent appearance with the surrounding environment.
Interestingly  our model with predicted layout TwoStream-Pred generates objects different from
the ground-truth layout but still match the bounding box condition. such as a person walking in the
different direction (the ﬁrst row) and objects placed in different order (the second row).
Comparison to other methods. We also compare against a few existing work on context hole-
ﬁling and structure-conditioned image generation. First  we consider the recent work on high-
resolution pixel-to-pixel translation [24] (referred as Pix2PixHD in Table 2). Compared to
our SingleStream-Layout model  Pix2PixHD model generates an entire image from seman-
tic layout. Second  we consider the work for context-driven image in-painting [17] (referred as
ContextEncoder) as another baseline. Similar to our SingleStream-Image  ContextEncoder
does not have access to the semantic layout during training and testing. For fair comparisons  we
extended ContextEncoder so that it takes segmentation layout as additional input. We refer this
model as ContextEncoder++ in Table 2. As we see in the Table 2  our two-stream model still
achieves the best SSIM and competitive segmentation pixel-wise accuracy. The segmentation accu-
racy of Pix2PixHD is higher than ours since it generates higher resolution images and object textures
non-relevant to the input image  but our method still achieves perceptually plausible results consistent
with the surrounding image. Note that the motivations of Pix2PixHD and our model are different  as
Pix2PixHD performs image generation from scratch while our focus is local image manipulation.
4.3 Qualitative Analysis
Semantic object manipulation. To demonstrate how our hierarchical model manipulates structure
and style of an object depending on the context  we conduct qualitative analysis in various settings.
In Figure 5  we present the manipulation results by moving the same bounding box of a car to
different locations in an image. As we see in the ﬁgure  our model generates a car with diverse but

7

ABCDEABCDEocclusiongeometric contextreasonably-looking shape and appearance when we move its bounding box from one side of the road
to another. Interestingly  the shape  orientation  and appearance of the car also change according to
the scene layout and shadow in the surrounding regions. Figure 6 illustrates generation results in
more diverse contexts. It shows that our model generates appropriate structure and appearance of the
object considering the context (e.g. occlusion with other objects  interaction with the scene  etc). In
addition to generating object matching the surroundings  we can also easily extend our framework to
allow users to directly control object style  which we demonstrate below.
Extension to style manipulation Although we focused on addition and deletion of objects as ma-
nipulation tasks  we can easily extend our framework to add control over object styles by conditioning
the image generator with additional style vector s by GI ( ˆM   I  s). To demonstrate this capability  we
deﬁne the style vector as a mean color of the object  and synthesized objects while changing the input
color (Figure 7). The results show that our model successfully synthesizes various objects with the
speciﬁed color while retaining other parts of the images unchanged. Modeling more complicated
styles (e.g. texture) can be achieved by learning a style-encoder s = E(X) where X is an object
template that user can select  but we leave it as a future work.

Figure 7: Controlling object color with style vector (left-upper corners indicate conditions).

Interactive and data-driven image editing. As one of the key applications  we perform interactive
image manipulation by adding  removing  and moving object bounding boxes. Figure 8 illustrates the
results. It shows that our method generates reasonable semantic layouts and images that smoothly
augment content of the original image. In addition to interactive manipulation  we can also automate
the manipulation process by sampling bounding boxes in an image in a data-driven manner. To
demonstrate this idea  we present an application of data-driven image manipulation in Figure 9. In this
demo  we implement box sampling using a simple non-parametric approach; Given a query image  we
ﬁrst compute its nearest neighbor from the training set based on low-level similarity. Then we compute
the geometric transformation between a query and a training image using SIFT Flow [15]  and move
object bounding boxes from one scene to another based on the scene-level transformation. As shown
in the ﬁgure  the proposed methods reasonably sample boxes from appropriate locations. However 
directly placing the object (or its mask) may lead to unnatural manipulation due to mismatches
in scene conﬁguration (e.g. occlusion and orientation). Our hierarchical model generates objects
adaptive to the new context.

Figure 8: Examples of manipulation of multiple objects in images. The line style indicates
manipulation operation (solid: addition  doted: deletion) and the color indicates the object class.

8

OriginalManipulatedFigure 9: Example of data-driven image manipulation. We manipulate the target image by transferring
bounding boxes from source image (blue boxes).
Results on indoor scene dataset.
In addition to Cityscape dataset  we conduct qualitative experi-
ments on bedroom images using ADE20K datasets [31]. Figure 10 illustrates the interactive image
manipulation results. Since objects in the indoor images involve much more diverse categories and
appearances  generating appropriate object shapes and textures aligned with other components in a
scene is much more challenging than the street images. We observe that the generated objects by the
proposed method usually are looking consistent the surrounding context.

Figure 10: Examples of image manipulation results on indoor images.

5 Conclusions
In this paper  we presented a hierarchical framework for semantic image manipulation. We ﬁrst learn
to generate the pixel-wise semantic label maps given the initial object bounding boxes. Then we learn
to generate the manipulated image from the predicted label maps. Such framework allows the user to
manipulate images at object-level by adding  removing  and moving an object bounding box at a time.
Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework
over existing image generation and context hole-ﬁling models  both qualitatively and quantitatively.
We further demonstrate its practical beneﬁts in semantic object manipulation  interactive image
editing and data-driven image editing.

9

Source ImageTarget ImageManipulated ImageOriginalManipulatedOriginalManipulatedPicturePillowCurtainCushionChairCabinetDresserMirrorSconceTVBlindsBedTableWindowDoorCarpetVaseArmchairPlantFlowerBookShelfFlowerpotSofaPicturePillowCurtainCushionChairCabinetDresserMirrorSconceTVBlindsLampArmchairVaseFlowerAcknowledgement This work was supported in part by ONR N00014-13-1-0762  NSF CAREER
IIS-1453651  DARPA Explainable AI (XAI) program #313498  Sloan Research Fellowship  and
Adobe Research Fellowship and Google PhD Fellowship to X. Yan.

References
[1] C. Barnes  E. Shechtman  A. Finkelstein  and D. B. Goldman. Patchmatch: A randomized
correspondence algorithm for structural image editing. ACM Transactions on Graphics  28(3):
24  2009.

[2] L.-C. Chen  Y. Zhu  G. Papandreou  F. Schroff  and H. Adam. Encoder-decoder with atrous

separable convolution for semantic image segmentation. arXiv:1802.02611  2018.

[3] Q. Chen and V. Koltun. Photographic image synthesis with cascaded reﬁnement networks. In

ICCV  2017.

[4] M. Cordts  M. Omran  S. Ramos  T. Rehfeld  M. Enzweiler  R. Benenson  U. Franke  S. Roth 
and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR  2016.

[5] E. Denton  S. Gross  and R. Fergus. Semi-supervised learning with context-conditional genera-

tive adversarial networks. arXiv preprint arXiv:1611.06430  2016.

[6] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and

Y. Bengio. Generative adversarial nets. In NIPS  2014.

[7] A. Gupta  A. A. Efros  and M. Hebert. Blocks world revisited: Image understanding using

qualitative geometry and mechanics. In ECCV  2010.

[8] D. Hoiem  A. A. Efros  and M. Hebert. Geometric context from a single image. In ICCV  2005.

[9] D. Hoiem  A. A. Efros  and M. Hebert. Putting objects in perspective. International Journal of

Computer Vision  80(1):3–15  2008.

[10] S. Hong  D. Yang  J. Choi  and H. Lee. Inferring semantic layout for hierarchical text-to-image

synthesis. In CVPR  2018.

[11] P. Isola  J.-Y. Zhu  T. Zhou  and A. A. Efros. Image-to-image translation with conditional

adversarial networks. In CVPR  2017.

[12] D. Kingma and J. Ba. Adam: A method for stochastic optimization.

arXiv:1412.6980  2014.

arXiv preprint

[13] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In NIPS  2012.

[14] Y. Li  S. Liu  J. Yang  and M.-H. Yang. Generative face completion. In CVPR  2017.

[15] C. Liu  J. Yuen  and A. Torralba. Sift ﬂow: Dense correspondence across scenes and its

applications. In Dense Image Correspondences for Computer Vision  pages 15–49. 2016.

[16] M.-Y. Liu  T. Breuel  and J. Kautz. Unsupervised image-to-image translation networks. In

NIPS  2017.

[17] D. Pathak  P. Krahenbuhl  J. Donahue  T. Darrell  and A. A. Efros. Context encoders: Feature

learning by inpainting. In CVPR  2016.

[18] A. Radford  L. Metz  and S. Chintala. Unsupervised representation learning with deep convolu-

tional generative adversarial networks. In ICLR  2016.

[19] S. E. Reed  Z. Akata  S. Mohan  S. Tenka  B. Schiele  and H. Lee. Learning what and where to

draw. In NIPS  2016.

[20] P. Sangkloy  J. Lu  C. Fang  F. Yu  and J. Hays. Scribbler: Controlling deep image synthesis

with sketch and color. In CVPR  2017.

10

[21] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image

recognition. arXiv preprint arXiv:1409.1556  2014.

[22] C. Szegedy  W. Liu  Y. Jia  P. Sermanet  S. Reed  D. Anguelov  D. Erhan  V. Vanhoucke 

A. Rabinovich  et al. Going deeper with convolutions. CVPR  2015.

[23] C. Vondrick  H. Pirsiavash  and A. Torralba. Generating videos with scene dynamics. In NIPS 

2016.

[24] T.-C. Wang  M.-Y. Liu  J.-Y. Zhu  A. Tao  J. Kautz  and B. Catanzaro. High-resolution image

synthesis and semantic manipulation with conditional gans. In ICCV  2017.

[25] Z. Wang  A. C. Bovik  H. R. Sheikh  and E. P. Simoncelli. Image quality assessment: from error
visibility to structural similarity. IEEE transactions on Image Processing  13(4):600–612  2004.

[26] W. Xian  P. Sangkloy  J. Lu  C. Fang  F. Yu  and J. Hays. Texturegan: Controlling deep image

synthesis with texture patches. In CVPR  2018.

[27] X. Yan  J. Yang  K. Sohn  and H. Lee. Attribute2image: Conditional image generation from

visual attributes. In ECCV  2016.

[28] J. Yang  A. Kannan  D. Batra  and D. Parikh. Lr-gan: Layered recursive generative adversarial

networks for image generation. In ICLR  2017.

[29] Y. Yang  S. Hallman  D. Ramanan  and C. C. Fowlkes. Layered object models for image
segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence  34(9):1731–
1743  2012.

[30] R. A. Yeh  C. Chen  T. Y. Lim  A. G. Schwing  M. Hasegawa-Johnson  and M. N. Do. Semantic

image inpainting with deep generative models. In CVPR  pages 5485–5493  2017.

[31] B. Zhou  H. Zhao  X. Puig  S. Fidler  A. Barriuso  and A. Torralba. Scene parsing through

ade20k dataset. In CVPR  2017.

[32] J.-Y. Zhu  P. Krähenbühl  E. Shechtman  and A. A. Efros. Generative visual manipulation on

the natural image manifold. In ECCV  2016.

[33] J.-Y. Zhu  T. Park  P. Isola  and A. A. Efros. Unpaired image-to-image translation using

cycle-consistent adversarial networks. In ICCV  2017.

11

,Seunghoon Hong
Xinchen Yan
Thomas Huang
Honglak Lee