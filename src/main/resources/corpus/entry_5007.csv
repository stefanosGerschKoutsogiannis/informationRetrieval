2011,ShareBoost: Efficient multiclass learning with feature sharing,Multiclass prediction is the problem of classifying an object into a    relevant target class.  We consider the problem of learning a    multiclass predictor that uses only few features  and in particular     the number of used features should increase sub-linearly with the    number of possible classes. This implies that features should be    shared by several classes. We describe and analyze the ShareBoost    algorithm for learning a multiclass predictor that uses few shared    features. We prove that ShareBoost efficiently finds a predictor    that uses few shared features (if such a predictor exists) and that    it has a small generalization error. We also describe how to use    ShareBoost for learning a non-linear predictor that has a fast    evaluation time. In a series of experiments with natural data sets    we demonstrate the benefits of ShareBoost and evaluate its success    relatively to other state-of-the-art approaches.,ShareBoost: Efﬁcient Multiclass Learning with

Feature Sharing

Shai Shalev-Shwartz⇤

Yonatan Wexler†

Amnon Shashua‡

Abstract

Multiclass prediction is the problem of classifying an object into a relevant target
class. We consider the problem of learning a multiclass predictor that uses only
few features  and in particular  the number of used features should increase sub-
linearly with the number of possible classes. This implies that features should be
shared by several classes. We describe and analyze the ShareBoost algorithm for
learning a multiclass predictor that uses few shared features. We prove that Share-
Boost efﬁciently ﬁnds a predictor that uses few shared features (if such a predictor
exists) and that it has a small generalization error. We also describe how to use
ShareBoost for learning a non-linear predictor that has a fast evaluation time. In a
series of experiments with natural data sets we demonstrate the beneﬁts of Share-
Boost and evaluate its success relatively to other state-of-the-art approaches.

1

Introduction

Learning to classify an object into a relevant target class surfaces in many domains such as docu-
ment categorization  object recognition in computer vision  and web advertisement. In multiclass
learning problems we use training examples to learn a classiﬁer which will later be used for accu-
rately classifying new objects. Typically  the classiﬁer ﬁrst calculates several features from the input
object and then classiﬁes the object based on those features. In many cases  it is important that the
runtime of the learned classiﬁer will be small. In particular  this requires that the learned classiﬁer
will only rely on the value of few features.
We start with predictors that are based on linear combinations of features. Later  in Section 3  we
show how our framework enables learning highly non-linear predictors by embedding non-linearity
in the construction of the features. Requiring the classiﬁer to depend on few features is therefore
equivalent to sparseness of the linear weights of features. In recent years  the problem of learning
sparse vectors for linear classiﬁcation or regression has been given signiﬁcant attention. While  in
general  ﬁnding the most accurate sparse predictor is known to be NP hard  two main approaches
have been proposed for overcoming the hardness result. The ﬁrst approach uses `1 norm as a surro-
gate for sparsity (e.g. the Lasso algorithm [33] and the compressed sensing literature [5  11]). The
second approach relies on forward greedy selection of features (e.g. Boosting [15] in the machine
learning literature and orthogonal matching pursuit in the signal processing community [35]).
A popular model for multiclass predictors maintains a weight vector for each one of the classes. In
such case  even if the weight vector associated with each class is sparse  the overall number of used
features might grow with the number of classes. Since the number of classes can be rather large 
and our goal is to learn a model with an overall small number of features  we would like that the
weight vectors will share the features with non-zero weights as much as possible. Organizing the
weight vectors of all classes as rows of a single matrix  this is equivalent to requiring sparsity of the
columns of the matrix.

⇤School of Computer Science and Engineering  the Hebrew University of Jerusalem  Israel
†OrCam Ltd.  Jerusalem  Israel
‡OrCam Ltd.  Jerusalem  Israel

1

In this paper we describe and analyze an efﬁcient algorithm for learning a multiclass predictor whose
corresponding matrix of weights has a small number of non-zero columns. We formally prove that
if there exists an accurate matrix with a number of non-zero columns that grows sub-linearly with
the number of classes  then our algorithm will also learn such a matrix. We apply our algorithm
to natural multiclass learning problems and demonstrate its advantages over previously proposed
state-of-the-art methods.
Our algorithm is a generalization of the forward greedy selection approach to sparsity in columns.
An alternative approach  which has recently been studied in [26  12]  generalizes the `1 norm based
approach  and relies on mixed-norms. We discuss the advantages of the greedy approach over mixed-
norms in Section 1.2.

1.1 Formal problem statement
Let V be the set of objects we would like to classify. For example  V can be the set of gray scale
images of a certain size. For each object v 2V   we have a pool of predeﬁned d features  each of
which is a real number in [1  1]. That is  we can represent each v 2V as a vector of features
x 2 [1  1]d. We note that the mapping from v to x can be non-linear and that d can be very large.
For example  we can deﬁne x so that each element xi corresponds to some patch  p 2 {±1}q⇥q  and
a threshold ✓  where xi equals 1 if there is a patch of v whose inner product with p is higher than
✓. We discuss some generic methods for constructing features in Section 3. From this point onward
we assume that x is given.
The set of possible classes is denoted by Y = {1  . . .   k}. Our goal is to learn a multiclass pre-
dictor  which is a mapping from the features of an object into Y. We focus on the set of predictors
parametrized by matrices W 2 Rk d that takes the following form:

hW (x) = argmax

y2Y

(W x)y .

(1)

That is  the matrix W maps each d-dimensional feature vector into a k-dimensional score vector 
and the actual prediction is the index of the maximal element of the score vector. If the maximizer
is not unique  we break ties arbitrarily.
Recall that our goal is to ﬁnd a matrix W with few non-zero columns. We denote by W· i the i’th
column of W and use the notation kWk1 0 = |{i : kW· ik1 > 0}| to denote the number of
columns of W which are not identically the zero vector. More generally  given a matrix W and a
pair of norms k·k p k·k r we denote kWkp r = k(kW· 1kp  . . .  kW· dkp)kr  that is  we apply the
p-norm on the columns of W and the r-norm on the resulting d-dimensional vector.
The 01 loss of a multiclass predictor hW on an example (x  y) is deﬁned as 1[hW (x) 6= y]. That
is  the 01 loss equals 1 if hW (x) 6= y and 0 otherwise. Since this loss function is not convex
with respect to W   we use a surrogate convex loss function based on the following easy to verify
inequalities:

1[hW (x) 6= y]  1[hW (x) 6= y]  (W x)y + (W x)hW (x)

1[y0 6= y]  (W x)y + (W x)y0
e1[y06=y](W x)y+(W x)y0 .

 max
y02Y
 lnXy02Y

(2)

(3)

We use the notation `(W  (x  y)) to denote the right-hand side (eqn. (3)) of the above. The loss given
in eqn. (2) is the multi-class hinge loss [7] used in Support-Vector-Machines  whereas `(W  (x  y)) is

the result of performing a “soft-max” operation: maxx f(x)  (1/p) lnPx epf (x)  where equality
holds for p ! 1.
This logistic multiclass loss function `(W  (x  y)) has several nice properties — see for example
[39]. Besides being a convex upper-bound on the 01 loss  it is smooth. The reason we need the
loss function to be both convex and smooth is as follows. If a function is convex  then its ﬁrst order
approximation at any point gives us a lower bound on the function at any other point. When the
function is also smooth  the ﬁrst order approximation gives us both lower and upper bounds on the

2

value of the function at any other point1. ShareBoost uses the gradient of the loss function at the
current solution (i.e. the ﬁrst order approximation of the loss) to make a greedy choice of which
column to update. To ensure that this greedy choice indeed yields a signiﬁcant improvement we
must know that the ﬁrst order approximation is indeed close to the actual loss function  and for that
we need both lower and upper bounds on the quality of the ﬁrst order approximation.
Given a training set S = (x1  y1)  . . .   (xm  ym)  the average training loss of a matrix W is:
L(W ) = 1

mP(x y)2S `(W  (x  y)). We aim at approximately solving the problem

(4)

min
W2Rk d

L(W ) s.t. kWk1 0  s .

That is  ﬁnd the matrix W with minimal training loss among all matrices with column sparsity of at
most s  where s is a user-deﬁned parameter. Since `(W  (x  y)) is an upper bound on 1[hW (x) 6= y] 
by minimizing L(W ) we also decrease the average 01 error of W over the training set. In Section 4
we show that for sparse models  a small training error is likely to yield a small error on unseen
examples as well.
Regrettably  the constraint kWk1 0  s in eqn. (4) is non-convex  and solving the optimization
problem in eqn. (4) is NP-hard [24  9]. To overcome the hardness result  the ShareBoost algorithm
will follow the forward greedy selection approach. The algorithm comes with formal generalization
and sparsity guarantees (described in Section 4) that makes ShareBoost an attractive multiclass
learning engine due to efﬁciency (both during training and at test time) and accuracy.

1.2 Related Work

The centrality of the multiclass learning problem has spurred the development of various approaches
for tackling the task. Perhaps the most straightforward approach is a reduction from multiclass to
binary  e.g.
the one-vs-rest or all pairs constructions. The more direct approach we choose  in
particular  the multiclass predictors of the form given in eqn. (1)  has been extensively studied and
showed a great success in practice — see for example [13  37  7].
An alternative construction  abbreviated as the single-vector model  shares a single weight vector 
for all the classes  paired with class-speciﬁc feature mappings. This construction is common in
generalized additive models [17]  multiclass versions of boosting [16  28]  and has been popularized
lately due to its role in prediction with structured output where the number of classes is exponentially
large (see e.g. [31]). While this approach can yield predictors with a rather mild dependency of the
required features on k (see for example the analysis in [39  31  14])  it relies on a-priori assumptions
on the structure of X and Y.
In contrast  in this paper we tackle general multiclass prediction
problems  like object recognition or document classiﬁcation  where it is not straightforward or even
plausible how one would go about to construct a-priori good class speciﬁc feature mappings  and
therefore the single-vector model is not adequate.
The class of predictors of the form given in eqn. (1) can be trained using Frobenius norm regular-
ization (as done by multiclass SVM – see e.g. [7]) or using `1 regularization over all the entries of
W . However  as pointed out in [26]  these regularizers might yield a matrix with many non-zeros
columns  and hence  will lead to a predictor that uses many features.
The alternative approach  and the most relevant to our work  is the use of mix-norm regularizations
like kWk1 1 or kWk2 1 [21  36  2  3  26  12  19]. For example  [12] solves the following problem:
(5)

min
W2Rk d

L(W ) + kWk1 1 .

which can be viewed as a convex approximation of our objective (eqn. (4)). This is advantageous
from an optimization point of view  as one can ﬁnd the global optimum of a convex problem  but
it remains unclear how well the convex program approximates the original goal. For example 
in Section C we show cases where mix-norm regularization does not yield sparse solutions while
ShareBoost does yield a sparse solution. Despite the fact that ShareBoost tackles a non-convex
program  and thus limited to local optimum solutions  we prove in Theorem 2 that under mild
1Smoothness guarantees that |f (x)  f (x0)  rf (x0)(x  x0)| kx  x0k2 for some  and all x  x0.
Therefore one can approximate f (x) by f (x0) +rf (x0)(x x0) and the approximation error is upper bounded
by the difference between x  x0.

3

conditions ShareBoost is guaranteed to ﬁnd an accurate sparse solution whenever such a solution
exists and that the generalization error is bounded as shown in Theorem 1.
We note that several recent papers (e.g. [19]) established exact recovery guarantees for mixed norms 
which may seem to be stronger than our guarantee given in Theorem 2. However  the assumptions
in [19] are much stronger than the assumptions of Theorem 2. In particular  they have strong noise
assumptions and a group RIP like assumption (Assumption 4.1-4.3 in their paper).
In contrast 
we impose no such restrictions. We would like to stress that in many generic practical cases  the
assumptions of [19] will not hold. For example  when using decision stumps  features will be highly
correlated which will violate Assumption 4.3 of [19].
Another advantage of ShareBoost is that its only parameter is the desired number of non-zero
columns of W . Furthermore  obtaining the whole-regularization-path of ShareBoost  that is  the
curve of accuracy as a function of sparsity  can be performed by a single run of ShareBoost  which
is much easier than obtaining the whole regularization path of the convex relaxation in eqn. (5).
Last but not least  ShareBoost can work even when the initial number of features  d  is very large 
as long as there is an efﬁcient way to choose the next feature. For example  when the features are
constructed using decision stumps  d will be extremely large  but ShareBoost can still be imple-
mented efﬁciently. In contrast  when d is extremely large mix-norm regularization techniques yield
challenging optimization problems.
As mentioned before  ShareBoost follows the forward greedy selection approach for tackling the
hardness of solving eqn. (4). The greedy approach has been widely studied in the context of learning
sparse predictors for linear regression. However  in multiclass problems  one needs sparsity of
groups of variables (columns of W ). ShareBoost generalizes the fully corrective greedy selection
procedure given in [29] to the case of selection of groups of variables  and our analysis follows
similar techniques.
Obtaining group sparsity by greedy methods has been also recently studied in [20  23]  and indeed 
ShareBoost shares similarities with these works. We differ from [20] in that our analysis does not
impose strong assumptions (e.g. group-RIP) and so ShareBoost applies to a much wider array of
applications. In addition  the speciﬁc criterion for choosing the next feature is different. In [20]  a
ratio between difference in objective and different in costs is used. In ShareBoost  the L1 norm of
the gradient matrix is used. For the multiclass problem with log loss  the criterion of ShareBoost
is much easier to compute  especially in large scale problems. [23] suggested many other selection
rules that are geared toward the squared loss  which is far from being an optimal loss function for
multiclass problems.
Another related method is the JointBoost algorithm [34]. While the original presentation in
[34] seems rather different than the type of predictors we describe in eqn. (1)  it is possible
to show that JointBoost in fact learns a matrix W with additional constraints.
In particular 
the features x are assumed to be decision stumps and each column W· i is constrained to be
↵i(1[1 2 Ci]   . . .   1[k 2 Ci])  where ↵i 2 R and Ci ⇢Y . That is  the stump is shared by all
classes in the subset Ci. JointBoost chooses such shared decision stumps in a greedy manner by
applying the GentleBoost algorithm on top of this presentation. A major disadvantage of JointBoost
is that in its pure form  it should exhaustively search C among all 2k possible subsets of Y. In prac-
tice  [34] relies on heuristics for ﬁnding C on each boosting step. In contrast  ShareBoost allows
the columns of W to be any real numbers  thus allowing ”soft” sharing between classes. Therefore 
ShareBoost has the same (or even richer) expressive power comparing to JointBoost. Moreover 
ShareBoost automatically identiﬁes the relatedness between classes (corresponding to choosing the
set C) without having to rely on exhaustive search. ShareBoost is also fully corrective  in the sense
that it extracts all the information from the selected features before adding new ones. This leads to
higher accuracy while using less features as was shown in our experiments on image classiﬁcation.
Lastly  ShareBoost comes with theoretical guarantees.
Finally  we mention that feature sharing is merely one way for transferring information across classes
[32] and several alternative ways have been proposed in the literature such as target embedding
[18  4]  shared hidden structure [22  1]  shared prototypes [27]  or sharing underlying metric [38].

4

2 The ShareBoost Algorithm

ShareBoost is a forward greedy selection approach for solving eqn. (4). Usually  in a greedy ap-
proach  we update the weight of one feature at a time. Now  we will update one column of W at a
time (since the desired sparsity is over columns). We will choose the column that maximizes the `1
norm of the corresponding column of the gradient of the loss at W . Since W is a matrix we have that
rL(W ) is a matrix of the partial derivatives of L. Denote by rrL(W ) the r’th column of rL(W ) 
that is  the vector⇣ @L(W )

@Wk r⌘. A standard calculation shows that
m X(x y)2SXc2Y
⇢c(x  y) xr(1[q = c]  1[q = y])

@W1 r
@L(W )
@Wq r

  . . .   @L(W )

=

1

where

@L(W )
@Wq r

(6)

=

(7)

⇢c(x  y) =

e1[c6=y](W x)y+(W x)c

.

1

Py02Y e1[y06=y](W x)y+(W x)y0
Note that Pc ⇢c(x  y) = 1 for all (x  y).
mP(x y) xr(⇢q(x  y)  1[q = y]) . Based on the above we have
xr(⇢q(x  y)  1[q = y])

X(x y)

krrL(W )k1 =

mXq2Y

1

.

Therefore  we can rewrite 

Finally  after choosing the column for which krrL(W )k1 is maximized  we re-optimize all the
columns of W which were selected so far. The resulting algorithm is given in Algorithm 1.

Algorithm 1 ShareBoost
1: Initialize: W = 0 ; I = ;
2: for t=1 2 . . .  T do
3:
4:
5:
6:
7: end for

For each class c and example (x  y) deﬁne ⇢c(x  y) as in eqn. (6)
Choose feature r that maximizes the right-hand side of eqn. (7)
I I [{ r}
Set W argminW L(W ) s.t. W· i = 0 for all i /2 I

The runtime of ShareBoost is as follows. Steps 3-5 requires O(mdk). Step 6 is a convex optimiza-
tion problem in tk variables and can be performed using various methods. In our experiments  we
used Nesterov’s accelerated gradient method [25] whose runtime is O(mtk/p✏) for a smooth objec-
tive  where ✏ is the desired accuracy. Therefore  the overall runtime is O(T mdk + T 2mk/p✏). It is
interesting to compare this runtime to the complexity of minimizing the mixed-norm regularization
objective given in eqn. (5). Since the objective is no longer smooth  the runtime of using Nesterov’s
accelerated method would be O(mdk/✏) which can be much larger than the runtime of ShareBoost
when d  T .
2.1 Variants of ShareBoost

We now describe several variants of ShareBoost. The analysis we present in Section 4 can be easily
adapted for these variants as well.

Modifying the Greedy Choice Rule ShareBoost chooses the feature r which maximizes the `1
norm of the r-th column of the gradient matrix. Our analysis shows that this choice leads to a sufﬁ-
cient decrease of the objective function. However  one can easily develop other ways for choosing
a feature which may potentially lead to an even larger decrease of the objective. For example  we
can choose a feature r that minimizes L(W ) over matrices W with support of I [{ r}. This will
lead to the maximal possible decrease of the objective function at the current iteration. Of course 
the runtime of choosing r will now be much larger. Some intermediate options are to choose r that
minimizes min↵2R W + ↵rrL(W ) or to choose r that minimizes minw2Rk W + we†r  where e†r is
the all-zero row vector except 1 in the r’th position.

5

Selecting a Group of Features at a Time
In some situations  features can be divided into groups
where the runtime of calculating a single feature in each group is almost the same as the runtime of
calculating all features in the group. In such cases  it makes sense to choose groups of features at
each iteration of ShareBoost. This can be easily done by simply choosing the group of features J

that maximizesPj2J krjL(W )k1.
Adding Regularization Our analysis implies that when |S| is signiﬁcantly larger than ˜O(T k)
then ShareBoost will not overﬁt. When this is not the case  we can incorporate regularization in the
objective of ShareBoost in order to prevent overﬁtting. One simple way is to add to the objective
function L(W ) a Frobenius norm regularization term of the form Pi j W 2
i j  where  is a reg-
ularization parameter. It is easy to verify that this is a smooth and convex function and therefore
we can easily adapt ShareBoost to deal with this regularized objective. It is also possible to rely
on other norms such as the `1 norm or the `1/`1 mixed-norm. However  there is one technical-
ity due to the fact that these norms are not smooth. We can overcome this problem by deﬁning
smooth approximations to these norms. The main idea is to ﬁrst note that for a scalar a we have
|a| = max{a a} and therefore we can rewrite the aforementioned norms using max and sum
operations. Then  we can replace each max expression with its soft-max counterpart and obtain a
smooth version of the overall norm function. For example  a smooth version of the `1/`1 norm
i=1(eWi j + eWi j )⌘   where   1 controls the tradeoff
j=1 log⇣Pk
Pd
will be kWk1 1 ⇡ 1
between quality of approximation and smoothness.

3 Non-Linear Prediction Rules

We now demonstrate how ShareBoost can be used for learning non-linear predictors. The main idea
is similar to the approach taken by Boosting and SVM. That is  we construct a non-linear predictor
by ﬁrst mapping the original features into a higher dimensional space and then learning a linear
predictor in that space  which corresponds to a non-linear predictor over the original feature space.
To illustrate this idea we present two concrete mappings. The ﬁrst is the decision stumps method
which is widely used by Boosting algorithms. The second approach shows how to use ShareBoost
for learning piece-wise linear predictors and is inspired by the super-vectors construction recently
described in [40].

3.1 ShareBoost with Decision Stumps
Let v 2 Rp be the original feature vector representing an object. A decision stump is a binary
feature of the form 1[vi  ✓]  for some feature i 2{ 1  . . .   p} and threshold ✓ 2 R. To construct
a non-linear predictor we can map each object v into a feature-vector x that contains all possible
decision stumps. Naturally  the dimensionality of x is very large (in fact  can even be inﬁnite) 
and calculating Step 4 of ShareBoost may take forever. Luckily  a simple trick yields an efﬁcient
solution. First note that for each i  all stump features corresponding to i can get at most m + 1
values on a training set of size m. Therefore  if we sort the values of vi over the m examples in the
training set  we can calculate the value of the right-hand side of eqn. (7) for all possible values of
✓ in total time of O(m). Thus  ShareBoost can be implemented efﬁciently with decision stumps.

3.2 Learning Piece-wise Linear Predictors with ShareBoost

To motivate our next construction let us consider ﬁrst a sim-
ple one dimensional function estimation problem. Given sample
(x1  yi)  . . .   (xm  ym) we would like to ﬁnd a function f : R !
R such that f(xi) ⇡ yi for all i. The class of piece-wise linear
functions can be a good candidate for the approximation function
f. See for example an illustration in Fig. 1. In fact  it is easy to
verify that all smooth functions can be approximated by piece-
wise linear functions (see for example the discussion in [40]). In
general  we can express piece-wise linear vector-valued functions
j=1 1[kv  vjk < rj] (huj  vi + bj)   where q is

as f(v) = Pq

6

2

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

Figure 1: Motivating super vec-
tors.

the number of pieces  (uj  bj) represents the linear function corresponding to piece j  and (vj  rj)
represents the center and radius of piece j. This expression can be also written as a linear function
over a different domain  f(v) = hw  (v)i where

 (v) = [ 1[kv  v1k < r1] [v   1]   . . .   1[kv  vqk < rq] [v   1] ] .

In the case of learning a multiclass predictor  we shall learn a predictor v 7! W (v)  where W will
be a k by dim( (v)) matrix. ShareBoost can be used for learning W . Furthermore  we can apply
the variant of ShareBoost described in Section 2.1 to learn a piece-wise linear model with few pieces
(that is  each group of features will correspond to one piece of the model). In practice  we ﬁrst deﬁne
a large set of candidate centers by applying some clustering method to the training examples  and
second we deﬁne a set of possible radiuses by taking values of quantiles from the training examples.
Then  we train ShareBoost so as to choose a multiclass predictor that only use few pairs (vj  rj).
The advantage of using ShareBoost here is that while it learns a non-linear model it will try to ﬁnd
a model with few linear “pieces”  which is advantageous both in terms of test runtime as well as in
terms of generalization performance.

4 Analysis

In this section we provide formal guarantees for the ShareBoost algorithm. The proofs are deferred
to the appendix. We ﬁrst show that if the algorithm has managed to ﬁnd a matrix W with a small
number of non-zero columns and a small training error  then the generalization error of W is also
small. The bound below is in terms of the 01 loss. A related bound  which is given in terms of the
convex loss function  is described in [39].

Theorem 1 Suppose that the ShareBoost algorithm runs for T iterations and let W be its output
matrix. Then  with probability of at least 1   over the choice of the training set S we have that
[hW (x) 6= y]+O s T k log(T k) log(k) + T log(d) + log(1/)

[hW (x) 6= y]  P

!

P

(x y)⇠D

(x y)⇠S

|S|

Next  we analyze the sparsity guarantees of ShareBoost. As mentioned previously  exactly solving
eqn. (4) is known to be NP hard. The following main theorem gives an interesting approximation
guarantee. It tells us that if there exists an accurate solution with small `1 1 norm  then the Share-
Boost algorithm will ﬁnd a good sparse solution.

Theorem 2 Let ✏> 0 and let W ? be an arbitrary matrix. Assume that we run the ShareBoost
algorithm for T =⌃4 1
and L(W )  L(W ?) + ✏.
5 Experiments

1 1⌥ iterations and let W be the output matrix. Then  kWk1 0  T

✏ kW ?k2

In this section we demonstrate the merits (and pitfalls) of ShareBoost by comparing it to alternative
algorithms in different scenarios. The ﬁrst experiment exempliﬁes the feature sharing property of
ShareBoost. We perform experiments with an OCR data set and demonstrate a mild growth of the
number of features as the number of classes grows from 2 to 36. The second experiment shows
that ShareBoost can construct predictors with state-of-the-art accuracy while only requiring few
features  which amounts to fast prediction runtime. The third experiment  which due to lack of
space is deferred to Appendix A.3  compares ShareBoost to mixed-norm regularization and to the
JointBoost algorithm of [34]. We follow the same experimental setup as in [12]. The main ﬁnding
is that ShareBoost outperforms the mixed-norm regularization method when the output predictor
needs to be very sparse  while mixed-norm regularization can be better in the regime of rather dense
predictors. We also show that ShareBoost is both faster and more accurate than JointBoost.

Feature Sharing The main motivation for deriving the ShareBoost algorithm is the need for
a multiclass predictor that uses only few features  and in particular 
the number of features
should increase slowly with the number of classes. To demonstrate this property of Share-
Boost we experimented with the Char74k data set which consists of images of digits and

7

letters. We trained ShareBoost with the number of classes varying from 2 classes to the
36 classes corresponding to the 10 digits and 26 capital letters. We calculated how many
features were required to achieve a certain ﬁxed accuracy as a function of the number of
classes. Due to lack of space  the description of the feature space is deferred to the appendix.

We compared ShareBoost to the 1-vs-rest approach  where in the
latter  we trained each binary classiﬁer using the same mechanism
as used by ShareBoost. Namely  we minimize the binary logistic
loss using a greedy algorithm. Both methods aim at construct-
ing sparse predictors using the same greedy approach. The dif-
ference between the methods is that ShareBoost selects features
in a shared manner while the 1-vs-rest approach selects features
for each binary problem separately. In Fig. 2 we plot the overall
number of features required by both methods to achieve a ﬁxed
accuracy on the test set as a function of the number of classes. As
can be easily seen  the increase in the number of required features
is mild for ShareBoost but signiﬁcant for the 1-vs-rest approach.

s
e
r
u

t

a
e

f
 

#

350

300

250

200

150

100

50

0

 
0

 

5

10

15

20

25

30

35

40

# classes

Figure 2: The number of features
required to achieve a ﬁxed accu-
racy as a function of the number
of classes for ShareBoost (dashed)
and the 1-vs-rest
(solid-circles).
The blue lines are for a target error
of 20% and the green lines are for
8%.

Constructing fast and accurate predictors The goal of our
this experiment is to show that ShareBoost achieves state-of-
the-art performance while constructing very fast predictors. We
experimented with the MNIST digit dataset  which consists of
a training set of 60  000 digits represented by centered size-
normalized 28 ⇥ 28 images  and a test set of 10  000 digits. The MNIST dataset has been ex-
tensively studied and is considered a standard test for multiclass classiﬁcation of handwritten dig-
its. The SVM algorithm with Gaussian kernel achieves an error rate of 1.4% on the test set.
The error rate achieved by the most advanced algorithms are below 1% of the test set. See
http://yann.lecun.com/exdb/mnist/. In particular  the top MNIST performer [6] uses
a feed-forward Neural-Net with 7.6 million connections which roughly translates to 7.6 million
multiply-accumulate (MAC) operations at run-time as well. During training  geometrically dis-
torted versions of the original examples were generated in order to expand the training set following
[30] who introduced a warping scheme for that purpose. The top performance error rate stands at
0.35% at a run-time cost of 7.6 million MAC per test example
The error-rate of ShareBoost with T = 266 rounds stands on
0.71% using the original training set and 0.47% with the ex-
panded training set of 360  000 examples generated by adding
ﬁve deformed instances per original example and with T = 305
rounds. Fig. 3 displays the convergence curve of error-rate as a
function of the number of rounds. Note that the training error
is higher than the test error. This follows from the fact that the
training set was expanded with 5 fairly strong deformed versions
of each input  using the method in [30]. As can be seen  less than
75 features sufﬁces to obtain an error rate of < 1%.
In terms of run-time on a test image  the system requires 305 con-
volutions of 7⇥7 templates and 540 dot-product operations which
totals to roughly 3.3·106 MAC operations — compared to around
7.5· 106 MAC operations of the top MNIST performer. The error
rate of 0.47% is better than that reported by [10] who used a 1-vs-all SVM with a 9-degree polyno-
mial kernel and with an expanded training set of 780  000 examples. The number of support vectors
(accumulated over the ten separate binary classiﬁers) was 163  410 giving rise to a run-time of 21-
fold compared to ShareBoost. Moreover  due to the fast convergence of ShareBoost  75 rounds are
enough for achieving less than 1% error.

Figure 3: The test error rate of
ShareBoost on the MNIST dataset
as a function of the number of
rounds using patch based features.

Train
Test

Rounds

250

300

450

500

150

200

550

600

350

400

 

0
50

100

 

1.5

1

0.5
0.47

Acknowledgements: We would like to thank Itay Erlich and Zohar Bar-Yehuda for their contri-
bution to the implementation of ShareBoost and to Ronen Katz for helpful comments.

8

References
[1] Y. Amit  M. Fink  N. Srebro  and S. Ullman. Uncovering shared structures in multiclass classiﬁcation. In International Conference on

Machine Learning  2007.

[2] A. Argyriou  T. Evgeniou  and M. Pontil. Multi-task feature learning. In NIPS  pages 41–48  2006.
[3] F.R. Bach. Consistency of the group lasso and multiple kernel learning. J. of Machine Learning Research  9:1179–1225  2008.
[4] S. Bengio  J. Weston  and D. Grangier. Label embedding trees for large multi-class tasks. In NIPS  2011.
[5] E.J. Candes and T. Tao. Decoding by linear programming. IEEE Trans. on Information Theory  51:4203–4215  2005.
[6] D. C. Ciresan  U. Meier  L. Maria G.  and J. Schmidhuber. Deep big simple neural nets excel on handwritten digit recognition. CoRR 

2010.

[7] K. Crammer and Y. Singer. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research 

3:951–991  2003.

[8] A. Daniely  S. Sabato  S. Ben-David  and S. Shalev-Shwartz. Multiclass learnability and the erm principle. In COLT  2011.
[9] G. Davis  S. Mallat  and M. Avellaneda. Greedy adaptive approximation. Journal of Constructive Approximation  13:57–98  1997.
[10] D. Decoste and S. Bernhard. Training invariant support vector machines. Mach. Learn.  46:161–190  2002.
[11] D.L. Donoho. Compressed sensing. In Technical Report  Stanford University  2006.
[12] J. Duchi and Y. Singer. Boosting with structural sparsity. In Proc. ICML  pages 297–304  2009.
[13] R. O. Duda and P. E. Hart. Pattern Classiﬁcation and Scene Analysis. Wiley  1973.
[14] M. Fink  S. Shalev-Shwartz  Y. Singer  and S. Ullman. Online multiclass learning by interclass hypothesis sharing. In International

Conference on Machine Learning  2006.

[15] Y. Freund and R. E. Schapire. A short introduction to boosting. J. of Japanese Society for AI  pages 771–780  1999.
[16] Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer

and System Sciences  pages 119–139  1997.

[17] T.J. Hastie and R.J. Tibshirani. Generalized additive models. Chapman & Hall  1995.
[18] D. Hsu  S.M. Kakade  J. Langford  and T. Zhang. Multi-label prediction via compressed sensing. In NIPS  2010.
[19] J. Huang and T. Zhang. The beneﬁt of group sparsity. Annals of Statistics  38(4)  2010.
[20] J. Huang  T. Zhang  and D.N. Metaxas. Learning with structured sparsity. In ICML  2009.
[21] G.R.G. Lanckriet  N. Cristianini  P.L. Bartlett  L. El Ghaoui  and M.I. Jordan. Learning the kernel matrix with semideﬁnite programming.

J. of Machine Learning Research  pages 27–72  2004.

[22] Y. L. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of IEEE 

pages 2278–2324  1998.

[23] A. Majumdar and R.K. Ward. Fast group sparse classiﬁcation. Electrical and Computer Engineering  Canadian Journal of  34(4):136–

144  2009.

[24] B. Natarajan. Sparse approximate solutions to linear systems. SIAM J. Computing  pages 227–234  1995.
[25] Y. Nesterov and I.U.E. Nesterov. Introductory lectures on convex optimization: A basic course  volume 87. Springer Netherlands  2004.
[26] A. Quattoni  X. Carreras  M. Collins  and T. Darrell. An efﬁcient projection for l 1 inf inity regularization. In ICML  page 108  2009.
[27] A. Quattoni  M. Collins  and T. Darrell. Transfer learning for image classiﬁcation with sparse prototype representations. In CVPR  2008.
[28] R. E. Schapire and Y. Singer. Improved boosting algorithms using conﬁdence-rated predictions. Machine Learning  37(3):1–40  1999.
[29] S. Shalev-Shwartz  T. Zhang  and N. Srebro. Trading accuracy for sparsity in optimization problems with sparsity constraints. Siam

Journal on Optimization  20:2807–2832  2010.

[30] P. Y. Simard  Dave S.  and John C. Platt. Best practices for convolutional neural networks applied to visual document analysis. Document

Analysis and Recognition  2003.

[31] B. Taskar  C. Guestrin  and D. Koller. Max-margin markov networks. In NIPS  2003.
[32] S. Thrun. Learning to learn: Introduction. Kluwer Academic Publishers  1996.
[33] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B.  58(1):267–288  1996.
[34] A. Torralba  K. P. Murphy  and W. T. Freeman. Sharing visual features for multiclass and multiview object detection. IEEE Transactions

on Pattern Analysis and Machine Intelligence (PAMI)  pages 854–869  2007.

[35] J.A. Tropp and A.C. Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. Information Theory  IEEE

Transactions on  53(12):4655–4666  2007.

[36] B. A Turlach  W. N V.  and Stephen J Wright. Simultaneous variable selection. Technometrics  47  2000.
[37] V. N. Vapnik. Statistical Learning Theory. Wiley  1998.
[38] E. Xing  A.Y. Ng  M. Jordan  and S. Russell. Distance metric learning  with application to clustering with side-information. In NIPS 

2003.

[39] T. Zhang. Class-size independent generalization analysis of some discriminative multi-category classiﬁcation. In NIPS  2004.
[40] X. Zhou  K. Yu  T. Zhang  and T. Huang. Image classiﬁcation using super-vector coding of local image descriptors. Computer Vision–

ECCV 2010  pages 141–154  2010.

9

,Maximilian Nickel
Xueyan Jiang
Volker Tresp
Chen Huang
Chen Change Loy
Xiaoou Tang
Songbai Yan
Chicheng Zhang