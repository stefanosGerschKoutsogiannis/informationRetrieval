2012,Graphical Models via Generalized Linear Models,Undirected graphical models  or Markov networks  such as Gaussian graphical models and Ising models enjoy popularity in a variety of applications.  In many settings  however  data may not follow a Gaussian or binomial distribution assumed by these models. We introduce a new class of graphical models based on generalized linear models (GLM) by assuming that node-wise conditional distributions arise from exponential families.  Our models allow one to estimate networks for a wide class of exponential distributions  such as the Poisson  negative binomial  and exponential  by fitting penalized GLMs to select the neighborhood for each node. A major contribution of this paper is the rigorous statistical analysis showing that with high probability  the neighborhood of our graphical models can be recovered exactly. We provide examples of high-throughput genomic networks learned via our GLM graphical models for multinomial and Poisson distributed data.,Graphical Models via Generalized Linear Models

Eunho Yang

Department of Computer Science

University of Texas at Austin
eunho@cs.utexas.edu

Pradeep Ravikumar

Department of Computer Science

University of Texas at Austin

pradeepr@cs.utexas.edu

Genevera I. Allen

Department of Statistics

Rice University

gallen@rice.edu

Zhandong Liu

Department of Pediatrics-Neurology

Baylor College of Medicine

zhandonl@bcm.edu

Abstract

Undirected graphical models  also known as Markov networks  enjoy popularity
in a variety of applications. The popular instances of these models such as Gaus-
sian Markov Random Fields (GMRFs)  Ising models  and multinomial discrete
models  however do not capture the characteristics of data in many settings. We
introduce a new class of graphical models based on generalized linear models
(GLMs) by assuming that node-wise conditional distributions arise from expo-
nential families. Our models allow one to estimate multivariate Markov networks
given any univariate exponential distribution  such as Poisson  negative binomial 
and exponential  by ﬁtting penalized GLMs to select the neighborhood for each
node. A major contribution of this paper is the rigorous statistical analysis show-
ing that with high probability  the neighborhood of our graphical models can be
recovered exactly. We also provide examples of non-Gaussian high-throughput
genomic networks learned via our GLM graphical models.

1

Introduction

Undirected graphical models  also known as Markov random ﬁelds  are an important class of sta-
tistical models that have been extensively used in a wide variety of domains  including statistical
physics  natural language processing  image analysis  and medicine. The key idea in this class of
models is to represent the joint distribution as a product of clique-wise compatibility functions; given
an underlying graph  each of these compatibility functions depends only on a subset of variables
within any clique of the underlying graph. Such a factored graphical model distribution can also be
related to an exponential family distribution [1]  where the unnormalized probability is expressed
as the exponential of a weighted linear combination of clique-wise sufﬁcient statistics. Learning a
graphical model distribution from data within this exponential family framework can be reduced to
learning weights on these sufﬁcient statistics. An important modeling question is then  how do we
choose suitable sufﬁcient statistics? In the case of discrete random variables  sufﬁcient statistics can
be taken as indicator functions as in the Ising or Potts model. These  however  are not suited to all
kinds of discrete variables such as that of non-negative integer counts. Similarly  in the case of con-
tinuous variables  Gaussian Markov Random Fields (GMRFs) are popular. The multivariate normal
distribution imposed by the GMRF  however  is a stringent assumption; the marginal distribution of
any variable must also be Gaussian.
In this paper  we propose a general class of graphical models beyond the Ising model and the GMRF
to encompass variables arising from all exponential family distributions. Our approach is motivated
by recent state of the art methods for learning the standard Ising and Gaussian MRFs [2  3  4].

1

The key idea in these recent methods is to learn the MRF graph structure by estimating node-
neighborhoods  which are estimated by maximizing the likelihood of each node conditioned on
the rest of the nodes. These node-wise ﬁtting methods have been shown to be both computationally
and statistically attractive. Here  we study the general class of models obtained by the following
construction: suppose the node-conditional distributions of each node conditioned on the rest of the
nodes are Generalized Linear Models (GLMs) [5]. By the Hammersley-Clifford Theorem [6] and
some algebra as derived in [7]  these node-conditional distributions entail a global distribution that
factors according to cliques deﬁned by the graph obtained from the node-neighborhoods. Moreover 
these have a particular set of potential functions speciﬁed by the GLM. The resulting class of MRFs
broadens the class of models available off-the-shelf  from the standard Ising  indicator-discrete  and
Gaussian MRFs.
Beyond our initial motivation of ﬁnding more general graphical model sufﬁcient statistics  a broader
class of parametric graphical models are important for a number of reasons. First  our models pro-
vide a principled approach to model multivariate distributions and network structures among a large
number of variables. For many non-Gaussian exponential families  multivariate distributions typi-
cally do not exist in an analytical or computationally tractable form. Graphical model GLMs provide
a way to “extend” univariate exponential families of distributions to the multivariate case and model
and study relationships between variables for these families of distributions. Second  while some
have proposed to extend the GMRF to a non-parametric class of graphical models by ﬁrst Gaussian-
izing the data and then ﬁtting a GMRF over the transformed variables [8]  the sample complexity of
such non-parametric methods is often inferior to parametric methods. Thus for modeling data that
closely follows a non-Gaussian distribution  statistical power for network recovery can be gained
by directly ﬁtting parametric GLM graphical models. Third  and speciﬁcally for multivariate count
data  others have suggested combinatorial approaches to ﬁtting graphical models  mostly in the con-
text of contingency tables [6  9  1  10]. These approaches  however  are computationally intractable
for even moderate numbers of variables.
Finally  potential applications for our GLM graphical models abound. Networks of call-times  time
spent on websites  diffusion processes  and life-cycles can be modeled with exponential graphical
models; other skewed multivariate data can be modeled with gamma or chi-squared graphical mod-
els. Perhaps the most interesting motivating applications are for multivariate count data such as from
website visits  user-ratings  crime and disease incident reports  bibliometrics  and next-generation
genomic sequencing technologies. The latter is a relatively new high-throughput technology to mea-
sure gene expression that is rapidly replacing the microarray [11]. As Gaussian graphical models are
widely used to infer genomic regulatory networks from microarray data  Poisson and negative bino-
mial graphical models may be important for inferring genomic networks from the multivariate count
data arising from this emerging technology. Beyond next generation sequencing  there has been a
recent proliferation of new high-throughput genomic technologies that produce non-Gaussian data.
Thus  our more general class of GLM graphical models can be used for inferring genomic networks
from these new high-throughput technologies.
The construction of our GLM graphical models also suggests a natural method for learning such
models: node-wise neighborhood estimation by ﬁtting sparsity constrained GLMs. A main contri-
bution of this paper is to provide a sparsistency analysis for the recovery of the underlying graph
structure of this new class of MRFs. The presence of non-linearities arising from the GLM poses
subtle technical issues not present in the linear case [2]. Indeed  for the speciﬁc cases of logistic  and
multinomial respectively  [3  4] derive such a sparsistency analysis via fairly extensive arguments
which were tuned to those speciﬁc cases. Here  we generalize their analysis to general GLMs  which
requires a slightly modiﬁed M-estimator and a more subtle theoretical analysis. We note that this
analysis might be of independent interest even outside the context of modeling and recovering graph-
ical models. In recent years  there has been a trend towards uniﬁed statistical analyses that provide
statistical guarantees for broad classes of models via general theorems [12]. Our result is in this vein
and provides structure recovery for the class of sparsity constrained generalized linear models. We
hope that the techniques we introduce might be of use to address the outstanding question of sparsity
constrained M-estimation in its full generality.

2

2 A New Class of Graphical Models

Problem Setup and Background. Suppose X = (X1  . . .   Xp) is a random vector  with each
variable Xi taking values in a set X . Suppose G = (V  E) is an undirected graph over p nodes
corresponding to the p variables; the corresponding graphical model is a set of distributions that
satisfy Markov independence assumptions with respect to the graph. By the Hammersley-Clifford
theorem  any such distribution also factors according to the graph in the following way. Let C be
a set of cliques (fully-connected subgraphs) of the graph G  and let {φc(Xc) c ∈ C} be a set of
clique-wise sufﬁcient statistics. With this notation  any distribution of X within the graphical model
family represented by the graph G takes the form:
P (X) ∝ exp

(1)
where {θc} are weights over the sufﬁcient statistics. With a pairwise graphical model distribution 
the set of cliques consists of the set of nodes V and the set of edges E  so that

(cid:26)(cid:88)

θcφc(Xc)

(cid:27)

c∈C

 

P (X) ∝ exp

θsφs(Xs) +

θstφst(Xs  Xt)

.

(2)

(cid:27)

(cid:26)(cid:88)

s∈V

(cid:88)

(s t)∈E

As previously discussed  an important question is how to select the class of sufﬁcient statistics  φ  in
particular to obtain as a multivariate extension of speciﬁed univariate parametric distributions? We
next outline a subclass of graphical models where the node-conditional distributions are exponential
family distributions  with an important special case where these node-conditional distributions are
generalized linear models (GLMs). Then  in Section 3  we will study how to learn the underlying
graph structure  or infer the edge set E  providing an M-estimator and sufﬁcient conditions under
which the estimator recovers the graph structure with high probability.
Graphical Models via GLMs. In this section  we investigate the class of models that arise from
specifying the node-conditional distributions as exponential families. Speciﬁcally  suppose we are
given a univariate exponential family distribution 

P (Z) = exp(θ B(Z) + C(Z) − D(θ)) 

with sufﬁcient statistics B(Z)  base measure C(Z)  and D(θ) as the log-normalization constant.
Let X = (X1  X2  . . .   Xp) be a p-dimensional random vector; and let G = (V  E) be an undi-
rected graph over p nodes corresponding to the p variables. Now suppose the distribution of Xs
given the rest of nodes XV \s is given by the above exponential family  but with the canonical expo-
nential family parameter set to a linear combination of k-th order products of univariate functions
{B(Xt)}t∈N (s). This gives the following conditional distribution:

P (Xs|XV \s) = exp

B(Xs)

θs +

θst B(Xt) +

θs t2t3 B(Xt2)B(Xt3)

(cid:88)

t2 t3∈N (s)

(cid:16)

(cid:110)
(cid:88)

+

t2 ... tk∈N (s)

(cid:88)
k(cid:89)

j=2

t∈N (s)

(cid:17)

θs t2...tk

B(Xtj )

+ C(Xs) − ¯D(XV \s)

(3)

where C(Xs) is speciﬁed by the exponential family  and ¯D(XV \s) is the log-normalization constant.
By the Hammersley-Clifford theorem  and some elementary calculation  this conditional distribution
can be shown to specify the following unique joint distribution P (X1  . . .   Xp):
Proposition 1. Suppose X = (X1  X2  . . .   Xp) is a p-dimensional random vector  and its node-
conditional distributions are speciﬁed by (3). Then its joint distribution P (X1  . . .   Xp) is given by:

P (X) = exp

θsB(Xs) +

θst B(Xs)B(Xt)

(cid:111)

 

(cid:41)

(cid:88)

+

(cid:88)

s∈V

t2 ... tk∈N (s)

s

θs...tk B(Xs)

k(cid:89)

where A(θ) is the log-normalization constant.

(cid:40)(cid:88)

(cid:88)

(cid:88)
(cid:88)

s∈V

t∈N (s)

B(Xtj ) +

j=2

s

3

C(Xs) − A(θ)

 

(4)

An important question is whether the conditional and joint distributions speciﬁed above have the
most general form  under just the assumption of exponential family node-conditional distributions?
In particular  note that the canonical parameter in the previous proposition is a tensor factorization
of the univariate sufﬁcient statistic  with pair-wise and higher-order interactions  which seems a bit
stringent. Interestingly  by extending the argument from [7] and the Hammersley-Clifford Theorem 
we can show that indeed (3) and (4) have the most general form.
Proposition 2. Suppose X = (X1  X2  . . .   Xp) is a p-dimensional random vector  and its node-
conditional distributions are speciﬁed by an exponential family 

P (Xs|XV \s) = exp{E(XV \s) B(Xs) + C(Xs) − ¯D(XV \s)} 

(5)
where the function E(XV \s) (and hence the log-normalization constant ¯D(XV \s)) only depends on
variables Xt in N (s). Further  suppose the corresponding joint distribution factors according to the
graph G = (V  E)  with the factors over cliques of size at most k. Then  the conditional distribution
in (5) has the tensor-factorized form in (3)  and the corresponding joint distribution has the form in
(4).

The proposition thus tells us that under the general assumptions that (a) the joint distribution is a
graphical model that factors according to a graph G  and has clique-factors of size at most k  and
(c) its node-conditional distribution follows an exponential family  it necessarily follows that the
conditional and joint distributions are given by (3) and (4) respectively.
An important special case is when the joint distribution has factors of size at most two. The condi-
tional distribution then is given by:

P (Xs|XV \s) = exp

θst B(Xs)B(Xt) + C(Xs) − ¯D(XV \s)

while the joint distribution is given as

P (X) = exp

θst B(Xs)B(Xt) +

(cid:88)

s

C(Xs) − A(θ)

Note that when the univariate sufﬁcient statistic function B(·) is a linear function B(Xs) = Xs 
then the conditional distribution in (6) is precisely a generalized linear model [5] in canonical form 

(cid:88)

t∈N (s)

θs B(Xs) +
(cid:88)

θsB(Xs) +

s

(s t)∈E

(cid:88)
θs Xs +
(cid:88)

θsXs +

s

(cid:88)

(cid:88)

t∈N (s)

(s t)∈E

  
 .
  
 .

(6)

(7)

(8)

(9)

P (Xs|XV \s) = exp

θst Xs Xt + C(Xs) − ¯D(XV \s; θ)

while the joint distribution has the form 

P (X) = exp

(cid:88)

s

C(Xs) − A(θ)

θst Xs Xt +

In the subsequent sections  we will refer to the entire class of models in (7) as GLM graphical
models  but focus on the case (9) with linear functions B(Xs) = Xs.
Examples. The GLM graphical models provide multivariate or Markov network extensions of uni-
variate exponential family distributions. The popular Gaussian graphical model and Ising model can
thus also be represented by (7). Consider the latter  for example  where for the Bernoulli distribution 
we have that B(X) = X  C(X) = 0  and A(θ) is the log-partition function; plugging these into (9) 
we have the form of the Ising model studied in [3]. The form of the multinomial graphical model 
an extension of the Ising model  can also be represented by (7) and has been previously studied in
[4] and others.
It is instructive to consider the domain of the set of all possible valid parameters in the GLM graph-
ical model (9); namely those that ensure that the density is normalizable  or equivalently  so that the
log-partition function satisﬁes A(θ) < +∞. The Ising model imposes no constraint on its param-
eters  {θst}  for normalizability  since there are ﬁnitely many conﬁgurations of the binary random

4

vector X. For other exponential families  with countable discrete or continuous valued variables  the
GLM graphical model does impose additional constraints on valid parameters. Consider the example
of the Poisson and exponential distributions. The Poisson family has sufﬁcient statistic B(X) = X
and base measure C(X) = −log(X!). With some algebra  we can show that A(θ) < +∞ implies
θst ≤ 0 ∀ s  t. Thus  the Poisson graphical model can only capture negative conditional relationships
between variables. Consider the exponential distribution with sufﬁcient statistic B(X) = −X  base
measure C(X) = 0. To ensure that the density is ﬁnitely integrable  so that A(θ) < +∞  we then
require that θst ≥ 0 ∀ s  t. Similar constraints on the parameter space are necessary to ensure proper
density functions for several other exponential family graphical models as well.

3 Statistical Guarantees

In this section  we study the problem of learning the graph structure of an underlying GLM graphical
model given iid samples. Speciﬁcally  we assume that we are given n samples X n
i=1 
from a GLM graphical model:

1 = {X (i)}n

 (cid:88)

(s t)∈E∗

(cid:88)

s

 .

P (X; θ∗) = exp

θ∗
st Xs Xt +

C(Xs) − A(θ)

(10)

We have removed node-wise terms for simplicity  noting that our analysis extends to the general
case. The goal in graphical model structure recovery is to recover the edges E∗ of the underlying
graph G = (V  E∗). Following [3  4]  we will approach this problem via neighborhood estimation 
where we estimate the neighborhood of each node individually  and then stitch these together to
N ∗(s)  then we can estimate the overall graph structure as:

form the global graph estimate. Speciﬁcally  if we have an estimate (cid:98)N (s) for the true neighborhood

(cid:16) (cid:88)

(cid:98)E = ∪s∈V ∪t∈(cid:98)N (s) {(s  t)}.
Xs
P(cid:0)X (i)

1 = {X (i)}n

(cid:1) =

s |X (i)\s   θ\s

n(cid:88)

θ∗
stXt

t∈N (s)

(cid:17)

−X (i)

1
n

i=1

n(cid:89)

i=1

(11)

(12)

θ∗
stXt

(cid:16) (cid:88)

(cid:17) .
s (cid:104)θ\s  X (i)\s (cid:105) + D(cid:0)(cid:104)θ\s  X (i)\s (cid:105)(cid:1).

t∈N (s)
st for t ∈ N (s) and θ∗

\s = {θ∗

Let θ∗
for t (cid:54)∈ N (s). Given n samples X n
distribution (12) as:

st}t∈V \s ∈ Rp−1 be a zero-padded vector  with entries θ∗

st = 0 
i=1  we can write the conditional log-likelihood of the

In order to estimate the neighborhood of any node  we consider the sparsity constrained conditional
MLE. Given the joint distribution in (10)  the conditional distribution of Xs given the rest of the
nodes is given by:

P (Xs|XV \s) = exp

+ C(Xs) − D

(cid:96)(θ\s; X n

1 ) := − 1
n

log

We can then solve the (cid:96)1 regularized conditional log-likelihood loss for each node Xs:

(13)

min

(cid:96)(θ\s; X n

1 ) + λn(cid:107)θ\s(cid:107)1.

θ\s∈Rp−1

Given the solution(cid:98)θ\s of the M-estimation problem above  we then estimate the node-neighborhood
of s as (cid:98)N (s) = {t ∈ V \s : (cid:98)θst (cid:54)= 0}. In the following when we focus on a ﬁxed node s ∈ V  

we will overload notation  and use θ ∈ Rp−1 as the parameters of the conditional distribution 
suppressing the dependence on s.
In the rest of the section  we ﬁrst discuss the assumptions we impose on the GLM graphical model
parameters. The ﬁrst set of assumptions are standard irrepresentable-type conditions imposed for
structure recovery in high-dimensional statistical estimators  and in particular  our assumptions mir-
ror those in [3]. The second set of assumptions are key to our generalized analysis of the class of
GLM graphical models as a whole. We then follow with our main theorem  that guarantees structure
recovery under these assumptions  with high probability even in high-dimensional regimes.

5

Our ﬁrst set of assumptions use the Fisher Information matrix  Q∗
1 )  which is the
Hessian of the node-conditional log-likelihood. In the following  we will simply use Q∗ instead of
s where the reference node s should be understood implicitly. We also use S = {(s  t) : t ∈ N (s)}
Q∗
to denote the true neighborhood of node s  and Sc to denote its complement. We use Q∗
SS to denote
the d × d sub-matrix indexed by S. Our ﬁrst two assumptions   and are as follows:
λmin. Moreover  there exists a constant λmax < ∞ such that λmax((cid:98)E[X\sX T\s]) ≤ λmax.
SS) ≥
Assumption 1 (Dependency condition). There exists a constant λmin > 0 such that λmin(Q∗

s = ∇2(cid:96)(θ∗

s ; X n

tS(Q∗

SS)−1(cid:107)1 ≤ 1 − α.

Assumption 2 (Incoherence condition). We also need an incoherence or irrepresentable condition
on the ﬁsher information matrix as in [3]. Speciﬁcally  there exists a constant α > 0  such that
maxt∈Sc (cid:107)Q∗
A key technical facet of the linear  logistic  and multinomial models in [2  3  4] and used heavily in
their proofs  is that the random variables {Xs} there were bounded with high probability. Unfortu-
nately  in the general GLM distribution in (12)  we cannot assume this explicitly. Nonetheless  we
show that we can analyze the corresponding regularized M-estimation problems  provided the ﬁrst
and second moments are bounded.
Assumption 3. The ﬁrst and second moments of the distribution in (10) are bounded as follows. The
t ] ≤ κv.
ﬁrst moment µ∗ := E[X]   satisﬁes (cid:107)µ∗(cid:107)2 ≤ κm; the second moment satisﬁes maxt∈V E[X 2
We also need smoothness assumptions on the log-normalization constants :
Assumption 4. The log-normalization constant A(·) of the joint distribution (10) satisﬁes:
maxu:(cid:107)u(cid:107)≤1 λmax(∇2A(θ∗ + u)) ≤ κh.
Assumption 5. The log-partition function D(·) of
satisﬁes: There exist constants κ1 and κ2 (that depend on the exponential
max{|D(cid:48)(cid:48)(κ1 log η)| |D(cid:48)(cid:48)(cid:48)(κ1 log η)|} ≤ nκ2 where η = max{n  p}  κ1 ≥ 9
[0  1/4].

the node-conditional distribution (12)
family) s.t.
2(cid:107)θ∗(cid:107)2 and κ2 ∈

Assumptions 3 and 4 are the key technical conditions under which we can generalize the analyses
in [2  3  4] to the general GLM case. In particular  we can show that the statements of the following
propositions hold  which show that the random vectors X following the GLM graphical model in
(10) are suitably well-behaved:
Proposition 3. Suppose X is a random vector with the distribution speciﬁed in (10). Then  for any
vector u ∈ Rp such that (cid:107)u(cid:107)2 ≤ c(cid:48)  any positive constant δ  and some constants c > 0 

Proposition 4. Suppose X is a random vector with the distribution speciﬁed in (10). Then  for
δ ≤ min{2κv/3  κh + κv}  and some constant c > 0 

P(cid:0)|(cid:104)u  X(cid:105)| ≥ δ log η(cid:1) ≤ cη−δ/c(cid:48)
n(cid:88)
(cid:0)X (i)

(cid:1)2 ≥ δ

≤ 2 exp(cid:0)−c n δ2(cid:1) .

(cid:33)

.

s

(cid:32)

P

1
n

i=1

√

10
λmin

Putting these key technical results and assumptions together  we arrive at our main result:
Theorem 1. Consider a GLM graphical model distribution as speciﬁed in (10)  with true parameter
st| ≥
θ∗ and associated edge set E∗ that satisﬁes Assumptions 1-5. Suppose that min(s t)∈E∗ |θ∗
dλn where d is the maximum neighborhood size. Suppose also that the regularization pa-
n1−κ2 for some constant M > 0. Then  there exist
1−3κ2   then with

positive constants L  K1 and K2 such that if n ≥ L(cid:8)d2 log p(max{log n  log p})2(cid:9) 1

rameter is chosen such that λn ≥ M (2−α)

probability at least 1 − exp(−K1λ2
(a) (Unique Solution) For each node s ∈ V   the solution of the M-estimation problem in (13) is

nn) − K2 max{n  p}−5/4  the following statements hold:

(cid:113) log p

α

(b) (Correct Neighborhood Recovery) The M-estimate also recovers the true neighborhood exactly 

unique  and

so that (cid:98)N (s) = N (s).

6

Figure 1: Probabilities of successful support recovery for a Poisson grid structure (ω = −0.1). The
probability of successful edge recovery vs. n (Left)  and the probability of successful edge recovery
vs. control parameter β = n/(c log p) (Right).

Note that if the neighborhood of each node is recovered with high probability  then by a simple

union bound  the estimate in (11)  (cid:98)E = ∪s∈V ∪t∈(cid:98)N (s) {(s  t)} is equal to the true edge set E∗ with

high-probability.
Also note that κ2 in the statement is a constant from Assumption 5. The Poisson family has one
of the steepest log-partition function: D(η) = exp(η). Hence  in order to satisfy Assumption 5 
we need (cid:107)θ∗(cid:107)2 ≤ 1
log p with κ2 = 1/4. On the other hand  for the binomial  multinomial or
Gaussian cases studied in [2  3  4]  we can recover their results with κ2 = 0 since the log-partition
function D(·) of these families are upper bounded by some constant for any input. Nevertheless  we
need to restrict θ∗ to satisfy Assumption 4 so that the variables are bounded with high probability in
Proposition 3 and 4 for any GLM case.

log n

18

4 Experiments

(cid:113) log p

Experiments on Simulated Networks. We provide a small simulation study that demonstrates the
consequences of Theorem 1 when the conditional distribution in (12) has the form of Poisson distri-
bution. We performed experiments on lattice (4 nearest neighbor) graphs with identical edge weight
ω for all edges. Simulating data via Gibbs sampling  we solved the sparsity-constrained optimization
problem with a constant factor of
n for λn. The left panel of Figure 1 shows the probability of
successful edge recovery for different numbers of nodes  p = {64  100  169  225}. In the right panel
of Figure 1  we re-scale the sample size n using the “control parameter” β = n/(c log p) for some
constant c. Each point in the plot indicates the probability that all edges are successfully recovered
out of 50 trials. We can see that the curves for different problem sizes are well aligned with the
results of Theorem 1.
Learning Genomic Networks. Gaussian graphical models learned from microarray data have often
been used to study high-throughput genomic regulatory networks. Our GLM graphical models will
be important for understanding genomic networks learned from other high-throughput technologies
that do not produce approximately Gaussian data. Here  we demonstrate the versatility of our model
by learning two cancer genomic networks  a genomic copy number aberration network (from aCGH
data) for Glioblastoma learned by multinomial graphical models and a meta-miRNA inhibitory net-
work (from next generation sequencing data) for breast cancer learned by Poisson graphical models.
Level III data  breast cancer miRNA expression (next generation sequencing) [13] and copy number
variation (aCGH) Glioblastoma data [14]  was obtained from the the Cancer Genome Atlas (TCGA)
data portal (http://tcga-data.nci.nih.gov/tcga/)  and processed according to standard techniques. Data
descriptions and processing details are given in the supplemental materials.
A Poisson graphical model and a multinomial graphical model were ﬁt to the processed miRNA
data and aberration data respectively by performing neighborhood selection with the sparsity of the
graph determined by stability selection [15]. Our GLM graphical models  Figure 2  reveal results
consistent with the cancer genomics literature. The meta-miRNA inhibitory network has three major
hubs  two of which  mir-519 and mir-520  are known to be breast cancer tumor suppressors [16  17].
Interestingly  let-7  a well-known miRNA involved in tumor metastasis [18]  plays a central role

7

4006008001000120000.20.40.60.81nSuccess probability  p = 64p = 100p = 169p = 2251.522.533.5400.20.40.60.81βSuccess probability  p = 64p = 100p = 169p = 225Figure 2: Genomic copy number aberration network for Glioblastoma learned via multinomial
graphical models (left) and meta-miRNA inhibitory network for breast cancer learned via Poisson
graphical models (right).

in our network  sharing edges with the ﬁve largest hubs; this suggests that our model has learned
relevant negative associations between tumor suppressors and enhancers. The Glioblastoma copy
number aberration network reveals ﬁve major modules  color coded on the left panel in Figure 2 
and three of these modules have been previously implicated in Glioblastoma: EGFR in the yellow
module  PTEN in the purple module  and CDK2A in the blue module [19].

5 Discussion

We have introduced a new class of graphical models that arise when we assume that node-wise
conditional distributions follow an exponential family distribution. We have also provided simple
M-estimators for learning the network by ﬁtting node-wise penalized GLMs that enjoy strong sta-
tistical recovery properties. Our work has broadened the class of off-the-shelf graphical models to
encompass a wide range of parametric distributions. These classes of graphical models may be of
further interest to the statistical community as they provide closed form multivariate densities for
several exponential family distributions (e.g. Poisson  exponential  negative binomial) where few
currently exist. Furthermore  the statistical analysis of our M-estimator required subtle techniques
that may be of general interest in the analysis of sparse M-estimation.
Our work outlines the general class of graphical models for exponential family distributions  but
there are many avenues for future work in studying this model for speciﬁc distributional families.
In particular  our model sometimes places restrictions on the parameter space. A question remains 
can these restrictions be relaxed for speciﬁc exponential family distributions? Additionally  we have
focused on families with linear sufﬁcient statistics (e.g. Gaussian  Bernoulli  Poisson  exponential 
negative binomial); our models can be studied with non-linear sufﬁcient statistics or multi-parameter
distributions as well. Overall  our work has opened the door for learning Markov Networks from
a broad class of distributions  the properties and applications of which leave much room for future
research.

Acknowledgments

E.Y. and P.R. acknowledge support from NSF IIS-1149803. G.A. and Z.L. acknowledge sup-
port from the Collaborative Advances in Biomedical Computing seed funding program at the Ken
Kennedy Institute for Information Technology at Rice University supported by the John and Ann
Doerr Fund for Computational Biomedicine and by the Center for Computational and Integrative
Biomedical Research seed funding program at Baylor College of Medicine. G.A. also acknowl-
edges support from NSF DMS-1209017.

8

9949539135748593514754164918955601951769074731009229708711379321282002348440183338144264658631015823822532524471780502349711261303142132629794616834356869366754724589276665567996887857886775298mir-1431517428mir-1051816342312191025113531338229262714293136243262mir-519-amir-315673020mir-518c mir-520a mir-449mir-150let-7References
[1] M.J. Wainwright and M.I. Jordan. Graphical models  exponential families  and variational inference.

Foundations and Trends R(cid:13) in Machine Learning  1(1-2):1–305  2008.

[2] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with the Lasso. Annals

of Statistics  34:1436–1462  2006.

[3] P. Ravikumar  M. J. Wainwright  and J. Lafferty. High-dimensional ising model selection using (cid:96)1-

regularized logistic regression. Annals of Statistics  38(3):1287–1319  2010.

[4] A. Jalali  P. Ravikumar  V. Vasuki  and S. Sanghavi. On learning discrete graphical models using group-

sparse regularization. In Inter. Conf. on AI and Statistics (AISTATS)  14  2011.

[5] P. McCullagh and J.A. Nelder. Generalized linear models. Monographs on statistics and applied proba-

bility 37. Chapman and Hall/CRC  New York  1989.

[6] S.L. Lauritzen. Graphical models  volume 17. Oxford University Press  USA  1996.
[7] J. Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical

Society. Series B (Methodological)  36(2):192–236  1974.

[8] H. Liu  J. Lafferty  and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimen-

sional undirected graphs. The Journal of Machine Learning Research  10:2295–2328  2009.

[9] Y.M.M. Bishop  S.E. Fienberg  and P.W. Holland. Discrete multivariate analysis. Springer Verlag  2007.
[10] Trevor. Hastie  Robert. Tibshirani  and JH (Jerome H.) Friedman. The elements of statistical learning.

Springer  2 edition  2009.

[11] J.C. Marioni  C.E. Mason  S.M. Mane  M. Stephens  and Y. Gilad. Rna-seq: an assessment of technical
reproducibility and comparison with gene expression arrays. Genome research  18(9):1509–1517  2008.
[12] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers  2010.

[13] Cancer Genome Atlas Research Network. Comprehensive molecular portraits of human breast tumours.

Nature  490(7418):61–70  2012.

[14] Cancer Genome Atlas Research Network. Comprehensive genomic characterization deﬁnes human

glioblastoma genes and core pathways. Nature  455(7216):1061–1068  October 2008.

[15] H. Liu  K. Roeder  and L. Wasserman. Stability approach to regularization selection (stars) for high

dimensional graphical models. Arxiv preprint arXiv:1006.3316  2010.

[16] K. Abdelmohsen  M.M. Kim  S. Srikantan  E.M. Mercken  S.E. Brennan  G.M. Wilson  R. de Cabo  and
M. Gorospe. mir-519 suppresses tumor growth by reducing hur levels. Cell cycle (Georgetown  Tex.) 
9(7):1354  2010.

[17] I. Keklikoglou  C. Koerner  C. Schmidt  JD Zhang  D. Heckmann  A. Shavinskaya  H. Allgayer 
B. G¨uckel  T. Fehm  A. Schneeweiss  et al. Microrna-520/373 family functions as a tumor suppressor
in estrogen receptor negative breast cancer by targeting nf-κb and tgf-β signaling pathways. Oncogene 
2011.

[18] F. Yu  H. Yao  P. Zhu  X. Zhang  Q. Pan  C. Gong  Y. Huang  X. Hu  F. Su  J. Lieberman  et al.

regulates self renewal and tumorigenicity of breast cancer cells. Cell  131(6):1109–1123  2007.

let-7

[19] R. McLendon  A. Friedman  D. Bigner  E.G. Van Meir  D.J. Brat  G.M. Mastrogianakis  J.J. Olson 
T. Mikkelsen  N. Lehman  K. Aldape  et al. Comprehensive genomic characterization deﬁnes human
glioblastoma genes and core pathways. Nature  455(7216):1061–1068  2008.

[20] Jianhua Zhang. Convert segment data into a region by sample matrix to allow for other high level com-

putational analyses  version 1.2.0 edition. Bioconductor package.

[21] Gerald B W Wertheim  Thomas W Yang  Tien-chi Pan  Anna Ramne  Zhandong Liu  Heather P Gardner 
Katherine D Dugan  Petra Kristel  Bas Kreike  Marc J van de Vijver  Robert D Cardiff  Carol Reynolds 
and Lewis A Chodosh. The Snf1-related kinase  Hunk  is essential for mammary tumor metastasis.
Proceedings of the National Academy of Sciences of the United States of America  106(37):15855–15860 
September 2009.

[22] J.T. Leek  R.B. Scharpf  H.C. Bravo  D. Simcha  B. Langmead  W.E. Johnson  D. Geman  K. Baggerly 
and R.A. Irizarry. Tackling the widespread and critical impact of batch effects in high-throughput data.
Nature Reviews Genetics  11(10):733–739  2010.

[23] J. Li  D.M. Witten  I.M. Johnstone  and R. Tibshirani. Normalization  testing  and false discovery rate

estimation for rna-sequencing data. Biostatistics  2011.

[24] G. I. Allen and Z. Liu. A Log-Linear Graphical Model for Inferring Genetic Networks from High-
Throughput Sequencing Data. IEEE International Conference on Bioinformatics and Biomedicine  2012.
[25] J. Bullard  E. Purdom  K. Hansen  and S. Dudoit. Evaluation of statistical methods for normalization and

differential expression in mrna-seq experiments. BMC bioinformatics  11(1):94  2010.

9

,Karthika Mohan
Judea Pearl
Jin Tian
Francesco Orabona