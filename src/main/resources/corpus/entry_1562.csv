2018,Distributed $k$-Clustering for Data with Heavy Noise,In this paper  we consider the $k$-center/median/means clustering with outliers problems (or the $(k  z)$-center/median/means problems) in the distributed setting.  Most previous distributed algorithms have their communication costs linearly depending on $z$  the number of outliers.  Recently Guha et al.[10] overcame this dependence issue by considering bi-criteria approximation algorithms that output solutions with $2z$ outliers.  For the case where $z$ is large  the extra $z$ outliers discarded by the algorithms might be too large  considering that the data gathering process might be costly. In this paper  we improve the number of outliers to the best possible $(1+\epsilon)z$  while maintaining the $O(1)$-approximation ratio and independence of communication cost on $z$.  The problems we consider include the $(k  z)$-center problem  and $(k  z)$-median/means problems in Euclidean metrics. Implementation of the our algorithm for $(k  z)$-center shows that it outperforms many previous algorithms  both in terms of the communication cost and quality of the output solution.,Distributed k-Clustering for Data with Heavy Noise

Xiangyu Guo

University at Buffalo
Buffalo  NY 14260

xiangyug@buffalo.edu

Shi Li

University at Buffalo
Buffalo  NY 14260
shil@buffalo.edu

Abstract

In this paper  we consider the k-center/median/means clustering with outliers
problems (or the (k  z)-center/median/means problems) in the distributed setting.
Most previous distributed algorithms have their communication costs linearly
depending on z  the number of outliers. Recently Guha et al. [10] overcame this
dependence issue by considering bi-criteria approximation algorithms that output
solutions with 2z outliers. For the case where z is large  the extra z outliers
discarded by the algorithms might be too large  considering that the data gathering
process might be costly. In this paper  we improve the number of outliers to
the best possible (1 + )z  while maintaining the O(1)-approximation ratio and
independence of communication cost on z. The problems we consider include the
(k  z)-center problem  and (k  z)-median/means problems in Euclidean metrics.
Implementation of the our algorithm for (k  z)-center shows that it outperforms
many previous algorithms  both in terms of the communication cost and quality of
the output solution.

1

Introduction

Clustering is a fundamental problem in unsupervised learning and data analytics. In many real-life
datasets  noises and errors unavoidably exist. It is known that even a few noisy data points can
signiﬁcantly inﬂuence the quality of the clustering results. To address this issue  previous work has
considered the clustering with outliers problem  where we are given a number z on the number of
outliers  and need to ﬁnd the optimum clustering where we are allowed to discard z points  under
some popular clustering objective such as k-center  k-median and k-means.
Due to the increase in volumes of real-life datasets  and the emergence of modern parallel computation
frameworks such as MapReduce and Hadoop  computing a clustering (with or without outliers) in the
distributed setting has attracted a lot of attention in recent years. The set of points are partitioned into
m parts that are stored on m different machines  who collectively need to compute a good clustering
by sending messages to each other. Often  the time to compute a good solution is dominated by the
communications among machines. Many recent papers on distributed clustering have focused on
designing O(1)-approximation algorithms with small communication cost [2  13  10].
Most previous algorithms for clustering with outliers have the communication costs linearly depending
on z  the number of outliers. Such an algorithm performs poorly when data is very noisy. Consider
the scenario where distributed sensory data are collected by a crowd of people equipped with portable
sensory devices. Due to different skill levels of individuals and the quality of devices  it is reasonable
to assume that a small constant fraction of the data points are unreliable.
Recently  Guha et al. [10] overcame the linear dependence issue  by giving distributed O(1)-
approximation algorithms for k-center/median/means with outliers problems with communication
cost independent of z. However  the solutions produced by their algorithms have 2z outliers. Such
a solution discards z more points compared to the (unknown) optimum one  which may greatly

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

decrease the efﬁciency of data usage. Consider an example where a research needs to be conducted
using the inliers of a dataset containing 10% noisy points; a ﬁltering process is needed to remove
the outliers. A solution with 2z outliers will only preserve 80% of data points  as opposed to the
promised 90%. As a result  the quality of the research result may be reduced.
Unfortunately  a simple example (described in the supplementary material) shows that if we need to
produce any multiplicatively approximate solution with only z outliers  then the linear dependence
on z can not be avoided. We show that  even deciding whether the optimum clustering with z outliers
has cost 0 or not  for a dataset distributed on 2 machines  requires a communication cost of Ω(z) bits.
Given such a negative result and the positive results of Guha et al. [10]  the following question is
interesting from both the practical and theoretical points of view:
Can we obtain distributed O(1)-approximation algorithms for k-center/median/means with outliers
that have communication cost independent of z and output solutions with (1 + )z outliers  for any
 > 0?
On the practical side  an algorithm discarding z additional outliers is acceptable  as the number
can be made arbitrarily small  compared to both the promised number z of outliers and the number
n − z of inliers. On the theoretical side  the (1 + )-factor for the number of outliers is the best we
can hope for if we are aiming at an O(1)-approximation algorithm with communication complexity
independent of z; thus answering the question in the afﬁrmative can give the tight tradeoff between
the number of outliers and the communication cost in terms of z.
In this paper  we make progress in answering the above question for many cases. For the k-center
objective  we solve the problem completely by giving a (24(1 + )  1 + )-bicriteria approximation
algorithm with communication cost O
  where ∆ is the aspect ratio of the metric.
(24(1 + ) is the approximation ratio  1 +  is the multiplicative factor for the number of outliers our
algorithm produces; the formal deﬁnition appears later.) For k-median/means objective  we give a
distributed (1 +   1 + )-bicrteria approximation algorithm for the case of Euclidean metrics. The

(cid:17)
   k  D  m  log ∆(cid:1)  where D is the dimension
communication complexity of the algorithm is poly(cid:0) 1

of the underlying Euclidean metric. (The exact communication complexity is given in Theorem 1.2.)
Using dimension reduction techniques  we can assume D = O( log n
2 )  by incurring a (1+)-distortion
in pairwise distances. So  the setting indeed covers a broad range of applications  given that the term
“k-means clustering” is deﬁned and studied exclusively in the context of Euclidean metrics. The
(1 +   1 + )-bicriteria approximation ratio comes with a caveat: our algorithm has running time
exponential in many parameters such as 1
   k  D and m (though it has no exponential dependence on
n or z).

· log ∆



(cid:16) km



1.1 Formulation of Problems

p∈P (cid:48) d(p  C) and(cid:80)

centers and a set P (cid:48) ⊆ P of n − z points so as to minimize maxp∈P (cid:48) d(p  C) (resp.(cid:80)
and(cid:80)
maxp∈P (cid:48) d(p  C) (cid:80)

We call the k-center (resp. k-median and k-means) problem with z outliers as the (k  z)-center (resp.
(k  z)-median and (k  z)-means) problem. Formally  we are given a set P of n points that reside in a
metric space d  two integers k ≥ 1 and z ∈ [0  n]. The goal of the problem is to ﬁnd a set C of k
p∈P (cid:48) d(p  C)
p∈P (cid:48) d2(p  C))  where d(p  C) = minc∈C d(p  c) is the minimum distance from p to a center
in C. For all the 3 objectives  given a set C ⊆ P of k centers  the best set P (cid:48) can be derived from P
by removing the z points p ∈ P with the largest d(p  C). Thus  we shall only use a set C of k centers
to denote a solution to a (k  z)-center/median/means instance. The cost of a solution C is deﬁned as
p∈P (cid:48) d2(p  C) respectively for a (k  z)-center  median and
means instance  where P (cid:48) is obtained by applying the optimum strategy. The n − z points in P (cid:48) and
the z points in P \ P (cid:48) are called inliers and outliers respectively in the solution.
As is typical in the machine learning literature  we consider general metrics for (k  z)-center  and
Euclidean metrics for (k  z)-median/means. In the (k  z)-center problem  we assume that each point
p in the metric space d can be described using O(1) words  and given the descriptions of two points p
and q  one can compute d(p  q) in O(1) time. In this case  the set C of centers must be from P since
these are all the points we have. For (k  z)-median/means problem  points in P and centers C are
from Euclidean space RD  and it is not required that C ⊆ P . One should treat D as a small number 
since dimension reduction techniques can be applied to project points to a lower-dimension space.

2

Bi-Criteria Approximation We say an algorithm for the (k  z)-center/median/means problem
achieves a bi-criteria approximation ratio (or simply approximation ratio) of (α  β)  for some α  β ≥ 1 
if it outputs a solution with at most βz outliers  whose cost is at most α times the cost of the optimum
solution with z outliers.
Distributed Clustering
In the distributed setting  the dataset P is split among m machines  where
Pi is the set of data points stored on machine i. We use ni to denote |Pi|. Following the communica-
tion model of [8] and [10]  we assume there is a central coordinator  and communications can only
happen between the coordinator and the m machines. The communication cost is measured in the
total number of words sent. Communications happen in rounds  where in each round  messages are
sent between the coordinator and the m machines. A message sent by a party (either the coordinator
or some machine) in a round can only depends on the input data given to the party  and the messages
received by the party in previous rounds. As is common in most of the previous results  we require
the number of rounds used to be small  preferably a small constant.
Our distributed algorithm needs to output a set C of k centers  as well as an upper bound L on the
maximum radius of the generated clusters. For simplicity  only the coordinator needs to know C and
L. We do not require the coordinator to output the set of outliers since otherwise the communication
cost is forced to be at least z. In a typical clustering task  each machine i can ﬁgure out the set of
outliers in its own dataset Pi based on C and L (1 extra round may be needed for the coordinator to
send C and L to all machines).

1.2 Prior Work

In the centralized setting  we know the best possible approximation ratios of 2 and 3 [4] for the k-
center and (k  z)-center problems respectively  and thus our understanding in this setting is complete.
There has been a long stream of research on approximation algorithms k-median and k-means 
leading to the current best approximation ratio of 2.675 [3] for k-median  9 [1] for k-means  and
6.357 for Euclidean k-means [1]. The ﬁrst O(1)-approximation algorithm for (k  z)-median is
given by Chen  [7]. Recently  Krishnaswamy et al. [12] developed a general framework that gives
O(1)-approximations for both (k  z)-median and (k  z)-means.
Much of
the recent work has focused on solving k-center/median/means and (k  z)-
center/median/means problems in the distributed setting [9  2  11  13  11  13  8  6  10  5]. Many
distributed O(1) approximation algorithms with small communication complexity are known for
these problems. However  for (k  z)-center/median/means problems  most known algorithms have
communication complexity linearly depending on z  the number of outliers. Guha et al. [10] over-
came the dependence issue  by giving (O(1)  2 + )-bicriteria approximation algorithms for all the
three objectives. The communication costs of their algorithms are ˜O(m/ + mk)  where ˜O hides a
logarithmic factor.

1.3 Our Contributions

Our main contributions are in designing (O(1)  1 + )-bicriteria approximation algorithms for the
(k  z)-center/median/means problems. The algorithm for (k  z)-center works for general metrics:
Theorem 1.1. There is a 4-round  distributed algorithm for the (k  z)-center problem  that achieves
a (24(1 + )  1 + )-bicriteria approximation and O
communication cost  where ∆ is
the aspect ratio of the metric.

(cid:16) km

(cid:17)

· log ∆





We give a high-level picture of the algorithm. By guessing  we assume that we know the optimum
cost L∗ (since we do not know  we need to lose the log ∆
-factor in the communication complexity).
In the ﬁrst round of the algorithm  each machine i will call a procedure called aggregating  on its
set Pi. This procedure performs two operations. First  it discards some points from Pi; second  it
moves each of the survived points by a distance of at most O(1)L∗. After the two operations  the
points will be aggregated at a few locations. Thus  machine i can send a compact representation of
these points to the coordinator: a list of (p  w(cid:48)
p is the number of
points aggregated at p. The coordinator will collect all the data points from all the machines  and run
the algorithm of [4] for (k  z(cid:48))-center instance on the collected points  for some suitable z(cid:48).

p) pairs  where p is a location and w(cid:48)



3

To analyze the algorithm  we show that the set P (cid:48) of points collected by the coordinator well-
approximates the original set P . The main lemma is that the total number of non-outliers removed
by the aggregation procedure on all machines is at most z. This incurs the additive factor of z in
the number of outliers. We prove this by showing that inside any ball of radius L∗  and for every
machine i ∈ [m]  we removed at most z
km points in Pi. Since the non-outliers are contained in the
union of k balls of radius L∗  and there are m machines  the total number of removed non-outliers is
at most z. For each remaining point  we shift it by a distance of O(1)L∗  leading to an O(1)-loss in
the approximation ratio of our algorithm.
We perform experiments comparing our main algorithm stated in Theorem 1.1 with many previous
ones on real-world datasets. The results show that it matches the state-of-art method in both solution
quality (objective value) and communication cost. We remark that the qualities of solutions are
measured w.r.t removing only z outliers. Theoretically  we need (1 + )z outliers in order to achieve
an O(1)-approximation ratio and our constant 24 is big. In spite of this  empirical evaluations suggest
that the algorithm on real-word datasets performs much better than what can be proved theoretically
in the worst case.
For (k  z)-median/means problems  our algorithm works for the Euclidean metric case and has
communication cost depending on the dimension D of the Euclidean space. One can w.l.o.g. assume
D = O(log n/2) by using the dimension reduction technique. Our algorithm is given in the
following theorem:

Theorem 1.2. There is a 2-round  distributed algorithm for the (k  z)-median/means problems
in D-dimensional Euclidean space  that achieves a (1 +   1 + )-bicriteria approximation ratio
with probability 1 − δ. The algorithm has communication cost O
  where ∆

ΦD · log(n∆/)

is the aspect ratio of the input points  Φ = O(cid:0) 1
Φ = O(cid:0) 1
(cid:1) for (k  z)-means.

δ ) + mk log mk

4 (kD + log 1

2 (kD + log 1

δ

(cid:16)

(cid:17)



δ ) + mk(cid:1) for (k  z)-median  and
(cid:16)(cid:80)

(cid:17)
p∈P dL(p  C) − zL



(cid:17)

(cid:16) log(∆n/)

We now give an overview of our algorithm for (k  z)-median/means. First  it is not hard to reformulate
the objective of the (k  z)-median problem as minimizing supL≥0
  where
dL is obtained from d by truncating all distances at L. By discretization  we can construct a set
L of O
interesting values that the L under the superior operator can take. Thus  our
goal becomes to ﬁnd a set C  that is simultaneously good for every k-median instance deﬁned
by dL  L ∈ L. Since now we are handling k-median instances (without outliers)  we can use the
communication-efﬁcient algorithm of [2] to construct an -coreset QL with weights wL for every
L ∈ L. Roughly speaking  the coreset QL is similar to the set P for the task of solving the k-median
problem under metric dL. The size of each -coreset QL is at most Φ  implying the communication
cost stated in the theorem. After collecting all the coresets  the coordinator can approximately solve
the optimization problem on them. This will lead to an (1 + O()  1 + O())-bicriteria approximate
solution. The running time of the algorithm  however  is exponential in the total size of the coresets.
The argument can be easily adapted to the (k  z)-means setting.
Organization In Section 2  we prove Theorem 1.1  by giving the (24(1 + )  1 + )-approximation
algorithm. The empirical evaluations of our algorithm for (k  z)-center and the proof of Theorem 1.2
are provided in the supplementary material.

set Q of points  and a set S ⊆ Q  we use w(S) =(cid:80)

Notations Throughout the paper  point sets are multi-sets  where each element has its own identity.
By a copy of some point p  we mean a point with the same description as p but a different identity.
For a set Q of points  a point p  and a radius r ≥ 0  we deﬁne ballQ(p  r) = {q ∈ Q : d(p  q) ≤ r}
to be the set of points in Q that have distances at most r to p. For a weight vector w ∈ ZQ≥0 on some
p∈S wp to denote the total weight of points in S.
Throughout
the paper  P is always the set of input points. We shall use dmin =
minp q∈P :d(p q)>0 d(p  q) and dmax = maxp q∈P d(p  q) to denote the minimum and maximum
non-zero pairwise distance between points in P . Let ∆ = dmax
denote the aspect ratio of the metric.
dmin

4

· log ∆







(cid:16) km



(cid:16) log ∆

(cid:17)

2 Distributed (k  z)-Center Algorithm with (1 + )z Outliers

between 0 and w(Q) =(cid:80)

(cid:1)  that either returns a (k  (1+)z)-center

(cid:17)
to design a main algorithm with communication cost O(cid:0) km

In this section  we prove Theorem 1.1  by giving the (24(1 + )  1 + )-approximation algorithm for
. Let L∗ be the cost of the optimum (k  z)-
(k  z)-center  with communication cost O
center solution (which is not given to us). We assume we are given a parameter L ≥ 0 and our goal is
solution of cost at most 24L  or certiﬁes that L∗ > L. Notice that L∗ ∈ {0}∪ [dmin/2  dmax]. We can
obtain our (24(1 + )  1 + )-approximation by running the main algorithm for O
different
values of L in parallel  and among all generated solutions  returning the one correspondent to the
smallest L. A naive implementation requires all the parties to know dmin and dmax in advance; we
show in the supplementary material that the requirement can be removed.
In intermediate steps  we may deal with (k  z)-center instances where points have integer weights. In
this case  the instance is deﬁned as (Q  w)  where Q is a set of points  w ∈ ZQ
>0  and z is an integer
q∈Q wq. The instance is equivalent to the instance ˆQ  the multi-set where
we have wq copies of each q ∈ Q.
[4] gave a 3-approximation algorithm for the (k  z)-center problem. However  our setting is slightly
more general so we can not apply the result directly. We are given a weighted set Q of points that
deﬁnes the (k  z)-center instance. The optimum set C∗ of centers  however  can be from the superset
P ⊇ Q which is hidden to us. Thus  our algorithm needs output a set C of k centers from Q and
compare it against the optimum set C∗ of centers from P . Notice that by losing a factor of 2  we can
assume centers are in Q; this will lead to a 6-approximation. Indeed  by applying the framework of
[4] more carefully  we can obtain a 4-approximation for this general setting. We state the result in the
following theorem:
Theorem 2.1 ([4]). Let d be a metric over the set P of points  Q ⊆ P and w ∈ ZQ
>0. There is an
algorithm kzc (Algorithm 1) that takes inputs k  z(cid:48) ≥ 1  (Q  w(cid:48)) with |Q| = n(cid:48)  the metric d restricted
to Q  and a real number L(cid:48) ≥ 0. In time O(n(cid:48)2)  the algorithm either outputs a (k  z(cid:48))-center solution
C(cid:48) ⊆ Q to the instance (Q  w(cid:48)) of cost at most 4L(cid:48)  or certiﬁes that there is no (k  z(cid:48))-center solution
C∗ ⊆ P of cost at most L(cid:48) and outputs “No”.
The main algorithm is dist-kzc (Algorithm 3)  which calls an important procedure called aggregating
(Algorithm 2). We describe aggregating and dist-kzc in Section 2.1 and 2.2 respectively.

2.1 Aggregating Points
The procedure aggregating  as described in Algorithm 2  takes as input the set Q ⊆ P of points to be
aggregated (which will be some Pi when we actually call the procedure)  the guessed optimum cost
L  and y ≥ 0  which controls how many points can be removed from Q. It returns a set Q(cid:48) of points
obtained from aggregation  along with their weights w(cid:48).
Algorithm 1 kzc(k  z(cid:48)  (Q  w(cid:48))  L(cid:48))
1: U ← Q  C(cid:48) ← ∅;
2: for i ← 1 to k do
pi ← p ∈ Q with largest w(cid:48)(ballU (p  2L(cid:48)))
3:
C(cid:48) ← C(cid:48) ∪ {pi}
4:
U ← U \ ballU (pi  4L(cid:48))
5:
6: if w(cid:48)(U ) > z(cid:48) then return “No” else return C(cid:48)
In aggregating  we start from U = Q and Q(cid:48) = ∅ and keep removing points from U. In each iteration 
we check if there is a p ∈ Q with |ballU (p  2L)| ≥ y. If yes  we add p to Q(cid:48)  remove ballU (p  4L)
from U and let wp be the number of points removed. We repeat thie procedure until such a p can not
be found. We remark that the procedure is very similar to the algorithm kzc (Algorithm 1) in [4].
We start from some simple observations about the algorithm.

Algorithm 2 aggregating(Q  L  y)
1: U ← Q  Q(cid:48) ← ∅;
2: while ∃p ∈ Q with |ballU (p  2L)| > y do
p ← |ballU (p  4L)|
3:
4:
5: return (Q(cid:48)  w(cid:48))

Q(cid:48) ← Q(cid:48) ∪ {p}  w(cid:48)
U ← U \ ballU (p  4L)

Claim 2.2. We deﬁne V =(cid:83)

p∈Q(cid:48) ballQ(p  4L) to be the set of points in Q with distance at most 4L
to some point in Q(cid:48) at the end of Algorithm 2. Then  the following statements hold at the end of the
algorithm:

5

1:
2.3.

Two

cases
In Figure

Figure
in
of
Lemma
(a) 
balls
{ballU (c  L) : c ∈ C∗  d(p  c) ≤ 3L}
cir-
cles) are all empty. So  ballU (p  2L) ⊆ O. In Figure
(b) 
there is a non-empty ballU (c  L) for some
c ∈ C∗ with d(p  c) ≤ 3L (the red circle). The ball
is contained in ballU (p  4L).

proof
the
(red

Figure 2: Illustration for proof of Lemma 2.7.
fi : Vi → P (cid:48)
i is indicated by the dashed lines 
each of whom is of length at most 4L. The
km.
number of crosses in a circle is at most z

1. U = Q \ V .

2. (cid:12)(cid:12)ballU (p  2L)(cid:12)(cid:12) ≤ y for every p ∈ Q.

Q(cid:48).

3. There is a function f : V → Q(cid:48) such that d(p  f (p)) ≤ 4L  ∀p ∈ V   and w(cid:48)(q) = |f−1(q)| ∀q ∈

Proof. U is exactly the set of points in Q with distance more than 4L to any point in Q(cid:48) and thus
U = Q \ V . Property 2 follows from the termination condition of the algorithm. Property 3 holds
by the way we add points to Q(cid:48) and remove points from U. If in some iteration we added q to Q(cid:48) 
we can deﬁne f (p) = q for every point p ∈ ballU (p  4L)  i.e  every point removed from U in the
iteration.

We think of U as the set of points we discard from Q and V as the set of survived points. We then
move each p ∈ V to f (p) ∈ Q(cid:48) and thus V will be aggregated at the set Q(cid:48) of locations. The
following crucial lemma upper bounds |Q(cid:48)|:
Lemma 2.3. Let ˆz ≥ 0 and assume there is a (k  ˆz)-center solution C∗ ⊆ P to the instance Q with
cost at most L. Then  at the end of Algorithm 2 we have |Q(cid:48)| ≤ k + ˆz
y .

Proof. Let O = Q\(cid:83)

c∈C∗ ballQ(c  L) be the set of outliers according to solution C∗. Thus |O| ≤ ˆz.
Focus on the moment before we run Step 3 in some iteration of aggregating. See Figure 1 for the
two cases we are going to consider. In case (a)  every center c ∈ ballC∗ (p  3L) has ballU (c  L) = ∅.
In this case  every point q ∈ ballU (p  2L) has d(q  C∗) > L: if d(p  c) > 3L for some c ∈ C∗  then
d(q  c) ≥ d(p  c)−d(p  q) > 3L−2L = L by triangle inequality; for some c ∈ C∗ with d(p  c) ≤ 3L 
we have ballU (c  L) = ∅  implying that d(q  c) > L as q ∈ U. Thus  ballU (p  2L) ⊆ O. So  Step 3
in this iteration will decrease |O ∩ U| by at least |ballU (p  4L)| ≥ |ballU (p  2L)| > y.
Consider the case (b) where some c ∈ ballC∗ (p  3L) has ballU (c  L) (cid:54)= ∅. Then ballU (p  4L) ⊇
ballU (c  L) will be removed from U by Step 3 in this iteration. Thus 
1. if case (a) happens  then |U ∩ O| is decreased by more than y in this iteration;
2. otherwise case (b) happens; then for some c ∈ C∗  ballU (c  L) changes from non-empty to ∅.
The ﬁrst event can happen for at most |O|/y ≤ ˆz/y iterations and the second event can happen for at
most |C∗| ≤ k iterations. So  |Q(cid:48)| ≤ k + ˆz/y.

6

p2L3LpointsinC∗pointsinULp(a)(b)4L≤zkmballsforC∗pointsinVipointsinUipointsinP(cid:48)i2.2 The Main Algorithm

We are now ready to describe the main algorithm for the (k  z)-center problem  given in Algorithm 3.
In the ﬁrst round  each machine will call aggregating(Pi  L  z
i). All the machines
will ﬁrst send their corresponding |P (cid:48)
i| to the coordinator. In Round 2 the algorithm will check if
i| is small or not. If yes  send a “Yes” message to all machines; otherwise return “No” and
terminate the algorithm. In Round 3  if a machine i received a “Yes” message from the coordinator 
then it sends the dataset P (cid:48)
i to the coordinator. Finally in Round 4  the
i∈[m] P (cid:48)

(cid:80)
i∈[m] |P (cid:48)
coordinator collects all the weighted points P (cid:48) =(cid:83)

i with the weight vector w(cid:48)

i and run kzc on these points.

km ) to obtain (P (cid:48)

i   w(cid:48)

Algorithm 3 dist-kzc
input on all parties: n  k  z  m  L  
input on machine i: dataset Pi with |Pi| = ni
output: a set C(cid:48) ⊆ P or “No” (which certiﬁes L∗ > L)
Round 1 on machine i ∈ [m]
i) ← aggregating(Pi  L  z
i   w(cid:48)
1: (P (cid:48)
km )
2: send |P (cid:48)
i| to the coordinator
Round 2 on the coordinator

1: if(cid:80)

i∈[m] |P (cid:48)

i| > km(1 + 1/) then return “No” else send “Yes” to each machine i ∈ [m]

1: let P (cid:48) ←(cid:83)m

Round 3 on machine i ∈ [m]
1: Upon receiving of a “Yes” message from the coordinator  respond by sending (P (cid:48)
Round 4 on the coordinator
2: let w(cid:48) be the function from P (cid:48) to Z>0 obtained by merging w(cid:48)
3: let z(cid:48) ← (1 + )z + w(cid:48)(P (cid:48)) − n
4: if z(cid:48) < 0 then return “No” else return kzc(k  z(cid:48)  (P (cid:48)  w(cid:48))  L(cid:48) = 5L)

2 ···   w(cid:48)

i=1 P (cid:48)

1  w(cid:48)

m

i

i   w(cid:48)
i)

(cid:80)m
i=1 |P (cid:48)

An immediate observation about the algorithm is that its communication cost is small:
Claim 2.4. The communication cost of dist-kzc is O( km
Proof. The total communication cost of Round 1 and Round 2 is O(m). We run Round 3 only
when the coordinator sent the “Yes” message  in which case the communication cost is at most

 ).

i| ≤ km(1 + 1/) = O( km
 ).

i

p∈P (cid:48)

i∈[m] P (cid:48)

i be the P (cid:48)

Let V = (cid:83)

i constructed in Round 1 on machine i. Let Vi =(cid:83)
i∈[m] Vi  P (cid:48) = (cid:83)

It is convenient to deﬁne some notations before we make further analysis. For every machine i ∈ [m] 
let P (cid:48)
ballPi(p  4L) be the set of
points in Pi that are within distance at most 4L to some point in P (cid:48)
i . Notice that this is the deﬁnition
of V in Claim 2.2 for the execution of aggregating on machine i. Let Ui = Pi \ Vi; this is the set U
at the end of this execution. Let fi be the mapping from Vi to P (cid:48)
i satisfying Property 3 of Claim 2.2.
i and f be the function from V to P (cid:48)  obtained by merging
f1  f2 ···   fm. Thus (p  f (p)) ≤ 4L ∀p ∈ V and w(cid:48)(q) = |f−1(q)| ∀q ∈ P (cid:48).
Claim 2.5. If dist-kzc returns a set C(cid:48)  then C(cid:48) is a (k  (1 + )z)-center solution to the instance P
with cost at most 24L.
Proof. C(cid:48) must be returned in Step 4 in Round 4. By Theorem 2.1 for kzc  C(cid:48) is a (k  z(cid:48))-center

c∈C(cid:48) ballP (cid:48)(c  20L)(cid:1) ≤ z(cid:48).
c∈C(cid:48) ballP (cid:48)(c  20L)(cid:1) ≥ w(cid:48)(P (cid:48)) − z(cid:48) = n − (1 + )z. Notice that for each q ∈ P (cid:48) 
c∈C(cid:48) ballP (c  24L)(cid:12)(cid:12) ≤ (1 + )z.

solution to (P (cid:48)  w(cid:48)) of cost at most 4 · 5L = 20L. That is  w(cid:48)(cid:0)P (cid:48) \(cid:83)
This implies w(cid:48)(cid:0)(cid:83)
c∈C(cid:48) ballP (c  24L)(cid:12)(cid:12) ≥ n− (1 + )z  which is exactly(cid:12)(cid:12)P \(cid:83)
(cid:12)(cid:12)(cid:83)
return a set C(cid:48). We deﬁne C∗ ⊆ P to be a set of size k such that |P \(cid:83)
I =(cid:83)
Lemma 2.6. After Round 1  we have(cid:80)

We can now assume L ≥ L∗ and we need to prove that we must reach Step 4 in Round 4 and
c∈C∗ ball(c  L)| ≤ z. Let
c∈C∗ ballP (c  L) be the set of “inliers” according to C∗ and O = P \ I be the set of outliers.

the set f−1(q) ⊆ V ⊆ P of points are within distance 4L from q and w(cid:48)(q) = |f−1(q)|. So 

Thus  |I| ≥ n − z and |O| ≤ z.

i| ≤ km(1 + 1/).

i∈[m] |P (cid:48)
7

(cid:12)(cid:12)(cid:12)Pi \(cid:83)

Proof. Let zi = |Pi ∩ O| =
a (k  zi)-center solution to the instance Pi with cost at most L. By Lemma 2.3  we have that
|P (cid:48)
i| ≤ k + zi

c∈C∗ ballPi(c  L)

(cid:12)(cid:12)(cid:12) be the set of outliers in Pi. Then  C∗ is
i∈[m] zi ≤ km(cid:0)1 + 1

(cid:1) .



(cid:80)

(cid:80)
z/(km). So  we have
i∈[m] |P (cid:48)

i| ≤ km + km

z

Therefore  the coordinator will not return “No” in Round 2. It remains to prove the following Lemma.
Lemma 2.7. Algorithm 3 will reach Step 4 in Round 4 and return a set C(cid:48).
Proof. See Figure 2 for the illustration of the proof. By Property 2 of Claim 2.2  we have
|ballUi (p  2L)| ≤ z
km for every p ∈ Ui since Ui ⊆ Pi. This implies that for every c ∈ C∗ 
we have |ballUi(c  L)| ≤ z
km. (Otherwise  taking an arbitrary p in the ball leads to a contradiction.)
(cid:88)
(cid:88)

|ballUi(c  L)| ≤ (cid:88)

≤ z
m
= |I| − z ≥ n − (1 + )z.

(cid:0)|I ∩ Pi| − |I ∩ Ui|(cid:1) ≥ (cid:88)

(cid:16)|I ∩ Pi| − z

(cid:12)(cid:12)(cid:12) ≤ (cid:88)

c∈C∗
|I ∩ Vi| =

(cid:12)(cid:12)(cid:12) (cid:91)

|Ui ∩ I| =

∀i ∈ [m].

ballUi(c  L)

z
km

c∈C∗

c∈C∗

(cid:17)

 

i∈[m]

i∈[m]

i∈[m]

m

So  w(cid:48)(P (cid:48) \(cid:83)

For every p ∈ V ∩ I  f (p) will have distance at most L + 4L = 5L to some center in C∗. Also 
notice that w(cid:48)(q) = |f−1(q)| for every q ∈ P (cid:48)  we have that

w(cid:48)(cid:0)(cid:83)

c∈C∗ ballP (cid:48)(c  5L)(cid:1) ≥ |V ∩ I| ≥ n − (1 + )z.

c∈C∗ ballP (cid:48)(c  5L)) ≤ w(P (cid:48)) − n + (1 + )z = z(cid:48). This implies that z(cid:48) ≥ 0  and there
is a (k  z(cid:48))-center solution C∗ ⊆ P to the instance (P (cid:48)  w(cid:48)) of cost at most 5L. Thus dist-kzc will
reach Step 4 in Round 4 and returns a set C(cid:48). This ﬁnishes the proof of the Lemma.
We now brieﬂy analyze the running times of algorithms on all parties. The running time of computing
P (cid:48)
i ) and this is the bottleneck for machine i. Considering
i on each machine i in round 1 is O(n2
all possible values of L  the running time on machine i is O
. The running time of the

(cid:16)

. We sort all the interesting
round-4 algorithm of the central coordinator for one L will be O
L values in increasing order. The central coordinator can use binary search to ﬁnd some L(cid:48) such that
the main algorithm outputs a set C(cid:48) for L = L(cid:48) but outputs “No” for L being the value before L(cid:48) in
the ordering. So  the running time of the central coordinator can be made O

(cid:1)2 · log log ∆

(cid:16)(cid:0) km

(cid:17)

.







The quadratic dependence of running time of machine i on ni might be an issue when ni is big; we
discuss how to alleviate the issue in the supplementary material.

(cid:17)
(cid:1)2(cid:17)

i · log ∆
n2

(cid:16)(cid:0) km



3 Conclusion

(cid:17)



(cid:16) km
(cid:16)



· log ∆

In this paper  we give a distributed (24(1 + )  1 + )-bicriteria approximation for the (k  z)-center
problem  with communication cost O
. The running times of the algorithms for all
parties are polynomial. We evaluate the algorithm on realworld data sets and it outperforms most
previous algorithms  matching the performance of the state-of-art method[10].
For the (k  z)-median/means problem  we give a distributed (1 +   1 + )-bicriteria approximation
algorithm with communication cost O
  where Φ is the upper bound on the size of the
coreset constructed using the algorithm of [2]. The central coordinator needs to solve the optimiza-
tion problem of ﬁnding a solution that is simultaneously good for O
k-median/means
instances. Since the approximation ratio for this problem will go to both factors in the bicriteria
ratio  we really need a (1 + )-approximation for the optimization problem. Unfortunately  solving
k-median/means alone is already APX-hard  and we don’t know a heuristic algorithm that works
well in practice (e.g  a counterpart to Lloyd’s algorithm for k-means). It is interesting to study if
a different approach can lead to a polynomial time distributed algorithm with O(1)-approximation
guarantee.

(cid:16) log(∆n/)

ΦD · log ∆

(cid:17)

(cid:17)





8

Acknowledgments

This research was supported by NSF grants CCF-1566356 and CCF-1717134.

References
[1] Sara Ahmadian  Ashkan Norouzi-Fard  Ola Svensson  and Justin Ward. Better guarantees for
k-means and euclidean k-median by primal-dual algorithms. In 58th IEEE Annual Symposium
on Foundations of Computer Science  FOCS 2017  Berkeley  CA  USA  October 15-17  2017 
pages 61–72  2017.

[2] Maria-Florina Balcan  Steven Ehrlich  and Yingyu Liang. Distributed k-means and k-median
clustering on general communication topologies. In Advances in Neural Information Processing
Systems 26  NIPS 2013  December 5-8  2013  Lake Tahoe  Nevada  United States.  pages
1995–2003  2013.

[3] Jaroslaw Byrka  Thomas Pensyl  Bartosz Rybicki  Aravind Srinivasan  and Khoa Trinh. An
improved approximation for k-median and positive correlation in budgeted optimization. ACM
Trans. Algorithms  13(2):23:1–23:31  2017.

[4] Moses Charikar  Samir Khuller  David M. Mount  and Giri Narasimhan. Algorithms for facility
location problems with outliers. In Proceedings of the 12th Annual Symposium on Discrete
Algorithms  January 7-9  2001  Washington  DC  USA.  pages 642–651  2001.

[5] Jiecao Chen  Erfan Sadeqi Azer  and Qin Zhang. A practical algorithm for distributed clustering

and outlier detection. CoRR  abs/1805.09495  2018.

[6] Jiecao Chen  He Sun  David P. Woodruff  and Qin Zhang. Communication-optimal distributed
clustering. In Advances in Neural Information Processing Systems 29: Annual Conference on
Neural Information Processing Systems 2016  December 5-10  2016  Barcelona  Spain  pages
3720–3728  2016.

[7] Ke Chen. A constant factor approximation algorithm for k-median clustering with outliers. In
Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete Algorithms  SODA 2008 
San Francisco  California  USA  January 20-22  2008  pages 826–835  2008.

[8] Hu Ding  Yu Liu  Lingxiao Huang  and Jian Li. k-means clustering with distributed dimensions.
In Proceedings of the 33rd International Conference on Machine Learning  ICML 2016  New
York City  NY  USA  June 19-24  2016  pages 1339–1348  2016.

[9] Alina Ene  Sungjin Im  and Benjamin Moseley. Fast clustering using mapreduce. In Proceedings
of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 
San Diego  CA  USA  August 21-24  2011  pages 681–689  2011.

[10] Sudipto Guha  Yi Li  and Qin Zhang. Distributed partial clustering. In Proceedings of the 29th
ACM Symposium on Parallelism in Algorithms and Architectures  SPAA 2017  Washington DC 
USA  July 24-26  2017  pages 143–152  2017.

[11] Sungjin Im and Benjamin Moseley. Brief announcement: Fast and better distributed mapreduce
algorithms for k-center clustering. In Proceedings of the 27th ACM on Symposium on Parallelism
in Algorithms and Architectures  SPAA 2015  Portland  OR  USA  June 13-15  2015  pages
65–67  2015.

[12] Ravishankar Krishnaswamy  Shi Li  and Sai Sandeep. Constant approximation for k-median and
k-means with outliers via iterative rounding. In Proceedings of the 50th Annual ACM SIGACT
Symposium on Theory of Computing  STOC 2018  Los Angeles  CA  USA  June 25-29  2018 
pages 646–659  2018.

[13] Gustavo Malkomes  Matt J. Kusner  Wenlin Chen  Kilian Q. Weinberger  and Benjamin Moseley.
In Advances in Neural
Fast distributed k-center clustering with outliers on massive data.
Information Processing Systems 28  NIPS 2015  December 7-12  2015  Montreal  Quebec 
Canada  pages 1063–1071  2015.

9

,Shi Li
Xiangyu Guo