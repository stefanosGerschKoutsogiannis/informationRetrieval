2017,Efficient Optimization for Linear Dynamical Systems with Applications to Clustering and Sparse Coding,Linear Dynamical Systems (LDSs) are fundamental tools for modeling spatio-temporal data in various disciplines. Though rich in modeling  analyzing LDSs is not free of difficulty  mainly because LDSs do not comply with Euclidean geometry and hence conventional learning techniques can not be applied directly. In this paper  we propose an efficient projected gradient descent method to minimize a general form of a loss function and demonstrate how clustering and sparse coding with LDSs can be solved by the proposed method efficiently. To this end  we first derive a novel canonical form for representing the parameters of an LDS  and then show how gradient-descent updates through the projection on the space of LDSs can be achieved dexterously. In contrast to previous studies  our solution avoids any approximation in LDS modeling or during the optimization process. Extensive experiments reveal the superior performance of the proposed method in terms of the convergence and classification accuracy over state-of-the-art techniques.,Efﬁcient Optimization for Linear Dynamical Systems
with Applications to Clustering and Sparse Coding

Wenbing Huang1 3  Mehrtash Harandi2  Tong Zhang2

Lijie Fan3  Fuchun Sun3  Junzhou Huang1

1 Tencent AI Lab. ;

2 Data61  CSIRO and Australian National University  Australia;

3 Department of Computer Science and Technology  Tsinghua University 
Tsinghua National Lab. for Information Science and Technology (TNList);

1{helendhuang  joehhuang}@tencent.com

2{mehrtash.harandi@data61.csiro.au  tong.zhang@anu.edu.cn}

3{flj14@mails  fcsun@mail}.tsinghua.edu.cn

Abstract

Linear Dynamical Systems (LDSs) are fundamental tools for modeling spatio-
temporal data in various disciplines. Though rich in modeling  analyzing LDSs is
not free of difﬁculty  mainly because LDSs do not comply with Euclidean geometry
and hence conventional learning techniques can not be applied directly. In this
paper  we propose an efﬁcient projected gradient descent method to minimize a
general form of a loss function and demonstrate how clustering and sparse coding
with LDSs can be solved by the proposed method efﬁciently. To this end  we ﬁrst
derive a novel canonical form for representing the parameters of an LDS  and then
show how gradient-descent updates through the projection on the space of LDSs
can be achieved dexterously. In contrast to previous studies  our solution avoids
any approximation in LDS modeling or during the optimization process. Extensive
experiments reveal the superior performance of the proposed method in terms of
the convergence and classiﬁcation accuracy over state-of-the-art techniques.

1

Introduction

Learning from spatio-temporal data is an active research area in computer vision  signal processing
and robotics. Examples include dynamic texture classiﬁcation [1]  video action recognition [2  3  4]
and robotic tactile sensing [5]. One kind of the popular models for analyzing spatio-temporal data
is Linear Dynamical Systems (LDSs) [1]. Speciﬁcally  LDSs apply parametric equations to model
the spatio-temporal data. The optimal system parameters learned from the input are employed as
the descriptor of each spatio-temporal sequence. The beneﬁts of applying LDSs are two-fold: 1.
LDSs are generative models and their parameters are learned in an unsupervised manner. This makes
LDSs suitable choices for not only classiﬁcation but also interpolation/extrapolation/generation of
spatio-temporal sequences [1  6  7]; 2. Unlike vectorial ARMA models [8]  LDSs are less prone to
the curse of dimensionality as a result of their lower-dimensional state space [9].
Clustering [10] and coding [5] LDSs are two fundamental problems that motivate this work. The
clustering task is to group LDS models based on some given similarity metrics. The problem of
coding  especially sparse coding  is to identify a dictionary of LDSs along their associated sparse
codes to best reconstruct a collection of LDSs. Given a set of LDSs  the key problems of clustering
and sparse coding are computing the mean and ﬁnding the LDS atoms  respectively  both of which
are not easy tasks by any measure. Due to an inﬁnite number of equivalent transformations for

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

the system parameters [1]  the space of LDSs is non-Euclidean. This in turn makes the direct
use of traditional techniques (e.g.  conventional sparse solvers) inapplicable. To get around the
difﬁculties induced by the non-Euclidean geometry  previous studies (e.g.  [11  12  13  5]) resort to
various approximations  either in modeling or during optimization. For instance  the authors in [11]
approximated the clustering mean by ﬁnding the closest sample under a certain embedding. As we
will see in our experiments  involving approximations into the solutions exhibits inevitable limitations
to the algorithmic performance.
This paper develops a gradient-based method to solve the clustering and sparse coding tasks efﬁciently
without any approximation involved. To this end  we reformulate the optimization problems for
these two different tasks and then unify them into one common problem by making use of the kernel
trick. However  there exist several challenges to address this common problem efﬁciently. The ﬁrst
challenge comes from the aforementioned invariance property on the LDS parameters. To attack
this challenge  we introduce a novel canonical form of the system parameters that is insensitive to
the equivalent changes. The second challenge comes from the fact that the optimization problem of
interest requires solving Discrete Lyapunov Equations (DLEs). At ﬁrst glance  such a dependency
makes backpropagating the gradients through DLEs more complicated. Interestingly  we prove
that the gradients can be exactly derived by solving another DLE in the end  which makes our
optimization much simpler and more efﬁcient. Finally  as suggested by [14]  the LDS parameters  i.e. 
the transition and measurement matrices require to be stable and orthogonal  respectively. Under our
canonical representation  the stability constraint is reduced to the bound constraint. We then make use
of the Cayley-transformation [15] to maintain orthogonality and perform the bound-normalization
to accomplish stability. Clustering and sparse coding can be combined with high-level pooling
frameworks (e.g.  bag-of-systems [11] and spatial-temporal-pyramid-matching [16]) for classifying
dynamic textures. Our experiments on such kind of data demonstrate that the proposed methods
outperform state-of-the-art techniques in terms of the convergence and classiﬁcation accuracy.

2 Related Work

LDS modeling. In the literature  various non-Euclidean metrics have been proposed to measure
the distances between LDSs  such as Kullback-Leibler divergence [17]  Chernoff distance [18] 
Binet-Cauchy kernel [19] and group distance [14]. This paper follows the works in [20  21  11  12]
to represent an LDS by making use of the extended observability subspace; comparing LDSs is then
achieved by measuring the subspace angles [22].
Clustering LDSs. In its simplest form  clustering LDSs can be achieved by alternating between two
sub-processes: 1) assigning LDSs to the closest clusters using a similarity measure; 2) computing
the mean of the LDSs within the same cluster. However  as the space of LDSs is non-Euclidean 
computing means on this space is not straightforward. In [12]  the authors embedded LDSs into a
ﬁnite Grassmann manifold by representing each LDS with its ﬁnite observability subspace and then
cluster LDSs on that manifold. In contrast  our method applies the extended observability subspace
to represent LDSs. In this way  not only the fully temporal evolution of the input sequence is taken
into account  but also and as will be shown shortly  the computational cost is reduced. The solution
proposed by [11] also represent LDSs with extended observability subspaces; but it approximates the
mean by ﬁnding a sample that is closest to the mean using the concept of Multidimensional Scaling
(MDS). Instead  our method ﬁnds the system tuple of the exact mean for the given group of LDSs
without relying on any approximation. Afsari et al. [14] cluster LDSs by ﬁrst aligning the parameters
of LDSs in their equivalence space. However  the method of Afsari et al. is agnostic to the joint
behavior of transition and measurement matrices and treat them independently. Other related studies
include probabilistic framework for clustering LDSs [23  24].
Sparse Coding with LDSs. Combining sparse coding with LDS modeling could further promote
the classiﬁcation performance [13]. However  similar to the clustering task  the non-Euclidean
structure makes it hard to formulate the reconstruction objective and update the dictionary atoms
on the space of LDSs. To address this issue  [13] embedded LDSs into the space of symmetric
matrices by representing each LDS with its ﬁnite observability subspace. With this embedding 
dictionary learning can be performed in the Euclidean space. In [5]  the authors employ the extended
observability subspaces as the LDS descriptors; however  to update the dictionary  the authors enforce
symmetric constraints on the the transition matrices. Different from previous studies  our model

2

works on the the original LDS model and does not enforce any additional constraint to the transition
matrices.
To sum up  in contrast to previous studies [12  11  14  13  5]  this paper solves the clustering and
sparse coding problems in a novel way regarding the following aspects. First  we unify the optimizing
objective functions for both clustering and sparse coding; Second  we avoid any additional constraints
(e.g. symmetric transition in [5] and ﬁnite observability in [12  13]) for the solution; Finally  we
propose a canonical formulation of the LDS tuple to facilitate the optimization.

3 LDS Modeling

LDSs describe time series through the following model [1]:

= y + Cx(t) + w(t)

(cid:26) y(t)

x(t + 1) = Ax(t) + Bv(t) 

(1)
with Rm×τ (cid:51) Y = [y(1) ···   y(τ )] and Rn×τ (cid:51) X = [x(1) ···   x(τ )] representing the observed
variables and the hidden states of the system  respectively. Furthermore  y ∈ Rm is the mean of Y ;
A ∈ Rn×n is the transition matrix of the model; B ∈ Rn×nv (nv ≤ n) is the noise transformation
matrix; C ∈ Rm×n is the measurement matrix; v(t) ∼ N (0  Inv ) and w(t) ∼ N (0  Ω) denoting
the process and measurement noise components  respectively. We also assume that n (cid:28) m and
C has full rank. Overall  generating the observed variables is governed by the parameters Θ =
{x(1)  y  A  B  C  Ω}.
System Identiﬁcation. The system parameters A and C of Eq. (1) describe the dynamics and spatial
patterns of the input sequence  respectively [11]. Therefore  the tuple (A  C) is a desired descriptor
for spatio-temporal data. Finding the optimal tuple (A  C) is known as system identiﬁcation. A
popular and efﬁcient method for system identiﬁcation is proposed in [1]. This method requires the
columns of C to be orthogonal  i.e.  C is a point on the Stiefel manifold deﬁned as ST(m  n) = {C ∈
Rm×n|CTC = In}. The transition matrix A obtained by the method of [1] is not naturally stable.
An LDS is stable if its spectral radius  i.e. the maximum eigenvalue of its transition matrix denoted by
ρ(A) is less than one. To obtain a stable transition matrix  [5] propose a soft-normalization technique
which is our choice in this paper. Therefore  we are interested in the LDS tuple with the constraints 
(2)

C = {CTC = In  ρ(A) < 1}.

(A  C) ∼ (P T AP   CP )

Equivalent Representation. Studying Eq. (1) shows that the output of the system remains unchanged
under linear transformations of the state basis [1]. More speciﬁcally  an LDS has an equivalent class
of representations  i.e. 
for any P ∈ O(n)1. For simplicity  the equivalence in Eq.(3) is called as P-equivalence.
Obviously comparing LDSs through Euclidean distance between the associated tuples is inaccurate
as a result of P-equivalence. To circumvent this difﬁculty  a family of approaches apply the extended
observability subspace to represent an LDS [20  21  11  5]. Below  we brieﬂy review this topic.
Extended Observability Subspace. The expected output sequence of Eq. (1) [12] is calculated as
(4)
where O∞(A  C) ∈ R∞×n is called as the extended observability matrix of the LDS associated
to (A  C). Let S(A  C) denote the extended observability subspace spanned by the columns of
O∞(A  C). Obviously  the extended observability subspace is invariant to P-equivalence  i.e. 
S(A  C) = S(P TAP   CP ).
In addition  the extended observability subspace is capable of
containing the fully temporal evolution of the input sequence as observed from Eq. (4).

[E[y(1)]; E[y(2)]; E[y(3)];··· ] = [C; CA; CA2;··· ]x(1) = O∞(A  C)x(1) 

(3)

4 Our Approach

In this section  we ﬁrst unify the optimizations for clustering and sparse coding with LDSs by making
use of the kernel functions. Next  we present our method to address this optimization problem.

1In general  (A  C) ∼ (P −1AP   CP ) for P ∈ GL(n) with GL(n) denoting non-singular n× n matrices.
Since we are interested in orthogonal measurement matrices (i.e.  C ∈ ST(m  n))  the equivalent class takes
the form described in Eq. (3).

3

4.1 Problem Formulation

We recall that each LDS is represented by its extended observability subspace. Clustering or sparse
coding in the space of extended observability subspaces is not straightforward because the underlying
geometry is non-Euclidean. Our idea here is to implicitly map the subspaces to a Reproducing Kernel
Hilbert Space (RKHS). For better readability  we simplify the subspace induced by S(Ai  Ci) as Si
in the rest of this section if no ambiguity is caused. We denote the implicit mapping deﬁned by a
positive deﬁnite kernel k(S1  S2) = φ(S1)Tφ(S2) as φ : S (cid:55)→ H. Various kernels [25  19  5] based
on extended observability subspaces have been proposed to measure the similarity between LDSs.
Though the proposed method is general in nature  in the rest of the paper we employ the projection
kernel [5] due to its simplicity. The projection kernel is deﬁned as
22 G21) 

(cid:80)∞
(5)
where Tr(·) computes the trace and the product matrices Gij = OT∞(Ai  Ci)O∞(Aj  Cj) =

j  for i  j ∈ {1  2} are obtained by solving the following DLE

kp(S1  S2) = Tr(G−1

11 G12G−1

i CjAt

i )tCT

t=0(AT

i GijAj − Gij = −CT
AT

(6)
The solution of DLE exists and is unique when both Ai and Aj are stable [22]. DLE can be solved
by a numerical algorithm with the computational complexity of O(n3) [26]  where n is the hidden
dimension and is usually very small (see Eq. (1)).
Clustering. As discussed before  the key of clustering is to compute the mean for the given set of
LDSs. While several works [12  11  14] have been developed for computing the mean  none of their
solutions are derived in the kernel form. The mean deﬁned by the implicit mapping is

i Cj.

(cid:107)φ(Sm) − φ(Si)(cid:107)2

s.t. (Am  Cm) ∈ C 

(7)

where Sm is the mean subspace and Si are data subspaces. Removing the terms that are independent
from Sm (e.g.  φ(Sm)Tφ(Sm) = 1) leads to

k(Sm  Si)

s.t. (Am  Cm) ∈ C.

(8)

N(cid:88)

i

min

Am Cm

1
N

min

N(cid:88)
(cid:107)φ(Si) − J(cid:88)

− 2
N

Am Cm

i

N(cid:88)

i

j=1

j=1

j}J

min
j  C(cid:48)

s.t. (A(cid:48)

j)(cid:107)2 + λ(cid:107)zi(cid:107)1 
j}J

Sparse Coding. The problem of sparse coding in the RKHS is written as [13]
j  C(cid:48)

zi jφ(S(cid:48)
1
N
i=1 are the data subspaces; {S(cid:48)

{A(cid:48)
where {Si}N
j=1 are the dictionary subspaces; zi j is the sparse code
j; RJ ∈ zi = [zi 1;··· ; zi J ] and λ is the sparsity factor. Eq. (9) shares
of data Si over atom S(cid:48)
the same form as those in [13  5]; however  here we apply the extended observability subspaces and
perform no additional constraint on the transition matrices.
To perform sparse coding  we alternative between the two phases: 1) computing the sparse codes
given LDS dictionary  which is similar to the conventional sparse coding task [13]; 2) optimizing
each dictionary atom with the codes ﬁxed. Speciﬁcally  updating the r-th atom with other atoms ﬁxed
gives the kernel formulation of the objective as

j) ∈ C  j = 1 ···   J;

(9)

−zi rk(S(cid:48)

r  Si) +

zi rzi jk(S(cid:48)

r  S(cid:48)

j).

J(cid:88)

j=1 j(cid:54)=r

N(cid:88)
N(cid:88)

i

i=1

Γr =

1
N

min
A C

1
N

(10)

(11)

Common Problem. Clearly  Eq. (8) and (10) have the common form as

βik(S(A  C)  S(Ai  Ci))

s.t. (A  C) ∈ C.
i=1 are given LDSs; {βi}N

Here  (A  C) is the LDS tuple to be identiﬁed; {(Ai  Ci)}N
task-dependent coefﬁcients (are speciﬁed in Eq. (8) and Eq. (10)).
To minimize (11)  we resort to the Projected Gradient Descent (PGD) method. Note that the solution
space in (11) is redundant due to the invariance induced by P-equivalence (Eq. (3)). We thus devise
a canonical representation of the system tuple (see Theorem 1). The canonical form not only
conﬁnes the search space but also simpliﬁes the stability constraint to a bound constraint. We then
compute the gradients with respect to the system tuple by backpropagating the gradients through
DLEs (see Theorem 4). Finally  we project the gradients to feasible regions of the system tuples via
Caylay-transformation (Eq. (16-17) and bound-normalization (Eq. (18)). We now present the details.

i=1 are the

4

4.2 Canonical Representation
Theorem 1. For any given LDS  the system tuple (A  C) ∈ Rn×n × Rm×n and all its equivalent
representations have the canonical form (ΛV   U )  where U ∈ ST(m  n)  V ∈ O(n) and Λ ∈
Rn×n is diagonal with the diagonal elements arranged in a descend order  i.e. λ1 ≥ λ2 ≥ ··· ≥ λn
2.
Remark 2. The proof of Theorem 1 (presented in the supplementary material) requires the SVD
decomposition that is not necessarily unique [27]  thus the canonical form of a system tuple is not
unique. Even so  the free dimensionality of the canonical space (i.e.  mn) is less than that of the
original tuples (i.e.  mn + n(n−1)
) within the feasible region of C. This is due to the invariance
induced by P-equivalence (Eq. (3)) if one optimizes (11) in the original form of the system tuple.
Remark 3. It is easy to see that the stability (i.e.  ρ(A) < 1) translates into the constraint |λi| < 1
in the canonical representation with λi being the i-th diagonal element of Λ. As such  problem (11)
can be cast as

2

βik(S(ΛV   U )  S(Ai  Ci)) 

min
Λ V  U
s.t. V TV = In; U TU = In; |λi| < 1  i = 1 ···   n.

i=1

(12)

N(cid:88)

1
N

A feasible solution of (11) can be obtained by minimizing (12) and the stability constraint in (11) is
reduced to a bound constraint in (12).

The canonical form derived from Theorem 1 is central to our methods. It is because with the canonical
form  we can simplify the stability constraint to a bound one  thus making the solution simpler and
more efﬁcient. We note that even with conditions on one single LDS  optimizing the original form of
A with the stability constraint is tedious (e.g.  [7] and we note that the tasks addressed in our paper
are more complicated where far more than one LDS are required to optimize). Furthermore  the
canonical form enables us to reduce the redundancy of the LDS tuple (see Remark 3). To be speciﬁc 
with canonical form  one needs to update only n singular values rather than the entire A matrix. Also
optimization with the canonical representations avoids numerical instabilities related to equivalent
classes  thus facilitating the optimization.

4.3 Passing Gradients Through DLEs

(cid:80)∞

According to the deﬁnition of the projection kernel  to obtain k(S(A  C)  S(Ai  Ci)) for (11)
(note that in the canonical form A = ΛV and C = U)  computing the product-matrices Gi =
i are required. To compute the gradients of the objective in (11) shown by Γ

t=0(AT)tCTCiAt

w.r.t. the tuple Θ = (A  C)  we make use of the chain rule in the vectorized form as

∂Γ
∂Θ :

=

∂Γ

∂Gi :

∂Gi :
∂Θ :

.

(13)

(cid:88)

i

∂Gi: is straightforward  deriving ∂Gi:

While computing ∂Γ
∂Θ: is non-trivial as the values of the product-
matrices Gi are obtained by an inﬁnite summation. The following theorem proves that the gradients
are derived by solving an induced DLE.
Theorem 4. Let the extended observability matrices of two LDSs (A1  C1) and (A2  C2) be O1 and
O2  respectively. Furthermore  let G12 = OT
2 be the product-matrix
between O1 and O2. Given the gradient of the objective function with respect to the product-matrix
∂Γ
∂G12

.
= H  the gradients with respect to the system parameters are

1 O2 =(cid:80)∞

t=0(AT

1 C2At

1 )tCT

∂Γ
∂A1

= G12A2RT
12 

∂Γ
∂A2
where R12 is obtained by solving the following DLE

= C2RT
12 

∂Γ
∂C1

= GT

12A1R12 

∂Γ
∂C2

= C1R12 

(14)

A1R12AT

2 − R12 + H = 0.

(15)

2All the proofs of the theorems in this paper are provided in the supplementary material.

5

4.4 Constraint-Aware Updates

We cannot preserve the orthogonality of V   U and the stability of Λ if we use conventional gradient-
descent methods to update the parameters Λ  V   U of (12). Optimization on the space of orthogonal
matrices is a well-studied problem [15]. Here  we employ the Cayley transformation [15] to maintain
orthogonality for V and U. In particular  we update V by

V = V − τ LV (I2n +

V LV )−1RT
RT

V V  

(16)
where LV = [∇V   V ] and RV = [V  −∇V ]  ∇V is the gradient of the objective w.r.t. V   and τ
is the learning rate. Similarly  to update U  we use
τ
2

(17)
where LU = [∇U   U ] and RU = [U  −∇U ]. As shown in [15]  the Cayley transform follows the
descent curve  thus updating V by Eq. (16) and U by Eq. (17) decreases the objective for sufﬁciently
small τ.
To accomplish stability  we apply the following bound normalization on Λ  i.e. 

U = U − τ LU (I2n +

U LU )−1RT
RT

U U  

τ
2

ε

(λk − τ∇λk) 

λk =

max(ε |λk − τ∇λk|)

(18)
where λk is the k-th diagonal element of Λ; ∇λk denotes the gradient w.r.t. λk; and ε < 1 is a
threshold (we set ε = 0.99 in all of our experiments in this paper). From the above  we immediately
have the following result 
Theorem 5. The update direction in Eq. (18) is a descent direction.
The authors in [5] constrain the eigenvalues of the transition matrix to be in (−1  1) using a Sigmoid
function. However  the Sigmoid function is easier to saturate and its gradient will vanish when λk is
close to the bound. In contrast  Eq. (18) does not suffer from this issue.
For reader’s convenience  all the aforementioned details for optimizing (11) are summarized in
Algorithm 1. The full details about how to use Algorithm 1 to solve clustering and sparse coding are
provided in the supplementary material.

Algorithm 1 The PGD method to optimize problem (11)

Input: The given tuples {(Ai  Cj)}; the initialization of (A  C); and the learning rate τ;
According to Theorem 1  compute the canonical formulations of {(Ai  Ci)}N
{(Λi  V i  U i)}N
for t = 1 to maxIter do

i=1 and (Λ  V   U )  respectively;

i=1 and (A  C) as

Compute the gradients according to Theorem 4: ∇Λ ∇V  ∇U;
Update V : V = V − τ LV (I2n + τ
Update U: U = U − τ LU (I2n + τ
Update Λ: λk =

max(ε |λk−τ∇λk|) (λk − τ∇λk);

V LV )−1RT
U LU )−1RT

2 RT
2 RT

ε

V V with LV and RV deﬁned in Eq. (16);
U U with LU and RU deﬁned in Eq. (17);

end for
Output: the system tuple (Λ  V   U ).

4.5 Extensions for Other Kernels

The proposed solution is general in nature and can be used with other kernel functions such as the
Martin kernel [25] and Binet-Cauchy kernel [19]. The Martin kernel is deﬁned as

km

(cid:16)
(cid:0)(A1  C1)  (A2  C2)(cid:1) = det
11 G12G−1
G−1
(cid:17)
(cid:16)
(cid:0)(A1  C1)  (A2  C2)(cid:1) = det

C1M C T
2

kb

 

(cid:17)

22 G21

 

(19)

(20)

with Gij as in Eq.(5). The determinant version of the Binet-Cauchy kernel is deﬁned as

where M satisﬁes e−λb A1M AT
2 (1)  λb is the exponential discounting rate 
and x1(1)  x2(1) are the initial hidden states of the two compared LDSs. Both the Martin kernel
and Binet-Cauchy kernel are computed by DLEs. Thus  Theorem 4 can be employed to compute the
gradients w.r.t. the system tuple for them.

2 − M = −x1(1)xT

6

5 Experiments

In this section  we ﬁrst compare the performance of our proposed method (see Algorithm 1)  called
as PGD  with previous state-of-the-art methods for the task of clustering and sparse coding using
the DynTex++ [28] dataset. We then evaluate the classiﬁcation accuracies of various state-of-the-art
methods with PGD on two video datasets  namely the YUPENN [29] and the DynTex [30] datasets.
The above datasets have been widely used in evaluating LDS-based algorithms in the literature  and
their details are presented in the supplementary material. In all experiments  the hidden order of LDS
(n in Eq. (1)) is ﬁxed to 10. To learn an LDS dictionary  we use the sparsity factor of 0.1 (λ in Eq.(9)).
The LDS tuples for all input sequences are learned by the method in [1] and the transition matrices
are stabilized by the soft-normalization technique in [5].

5.1 Models Comparison

This experiment uses the DynTex++ datasets. We extract the histogram of LBP from Three Orthogonal
Planes (LBP-TOP) [31] by splitting each video into sub-videos of length 8  with a 6-frame overlap.
The LBP-TOP features are fed to LDSs to identify the system parameters. For clustering  we compare
our PGD with the MDS method with the Martin Kernel [11] and the Align algorithm [14]. For sparse
coding  two related methods are compared: Grass [13] and LDSST [5]. We follow [13] and use 3-step
observability matrices for the Grass method (hence Grass-3 below). In LDSST  the transition matrices
are enforced to be symmetric. All algorithms are randomly initialized and the average results over 10
times are reported.

(cid:80)

Figure 1: The clustering performance of the MDS  Align and PGD algorithms with varying number
of clusters on DynTex++.
5.1.1 Clustering

To evaluate the clustering performance  we apply the purity metric [32]  which is given by p =
k maxi ci k  where ci k counts the number of samples from i-th class in k-th cluster; N is the
1
N
number of the data. A higher purity means a better performance.
For the Align algorithm  we varied the learning rate when optimizing the aligning matrices and chose
the value that delivered the best performance. For our PGD algorithm  we selected the learning
rate as 0.1 for Λ and V and 1 for U. Fig. 1 reports the clustering performance of the compared
methods. Our method consistently outperforms both MDS and Align methods over various number of
clusters. We also report the running time for one epoch of each algorithm in Fig. 1. Here  one epoch
means one update of the clustering centers through all data samples. Fig. 1 shows that PGD performs
faster that both the MDS and Align algorithms  probably because the MDS method recomputes
the kernel-matrix for the embedding at each epoch and the Align algorithm calculates the aligning
distance in an iterative way.

5.1.2 Sparse Coding

In this experiment  we used half of samples from DynTex++ for training the dictionary and the other
half for testing. As the objective of (11) is in a sum-minimize form  we can employ the stochastic
version of Algorithm 1 to optimize (11) for large-scale dataset. This can be achieved by sampling
a mini-batch to update the system tuple at each iteration. Therefore  in addition to the full batch
version  we also carried out the stochastic PGD with the mini-bach of size 128  which is denoted as
PGD-128. The learning rates of both full PGD and PGD-128 were selected as 0.1 for Λ and V and 1
for U  and their values were decreased by half every 10 epoch. Different from PGD  the Grass and
LDSST methods require the whole dataset in hand for learning the dictionary at each epoch  and thus
they can not support the update via mini-batches.

7

4 8 16 32 64 128NumberOfClusters00.20.40.60.8PurityPGDAlignMDS4 8 16 32 64 128NumberOfClusters0200400600Time Per EpochPGDAlignMDS(a) J = 4

Figure 2: Testing reconstruction errors of Grass-3  LDSST  PGD-full and PGD-128 with different
dictionary sizes on DynTex++. The PGD-128 method converges much faster than other counterparts.
Although Grass-3 converges to a bit smaller error than PGD-128 when J = 4 (see (a))  it performs
worse than PGD-128 when the value of J is increasing (see (b) and (c)).

(b) J = 8

(c) J = 16

Rinit

It is unfair to directly compare the reconstruction errors (Eq. (9)) of different methods  since their
values are calculated by different metrics. Therefore  we make use of the normalized reconstruction
error deﬁned as N R = Rt−Rinit
  where Rinit and Rt are corresponded to the reconstruction errors
at the initial step and the t-th epoch  respectively. Fig. 2 shows the normalized reconstruction errors
on testing set of PGDs  Grass-3 and the LDSST method during the learning process for various
dictionary sizes. PGD-128 converges to lower errors than PGD-full on all experiments  indicating
that the stochastic sampling strategy is helpful to escaping from the poor local minima. PGD-128
consistently outperforms both Grass-3 and LDSST in terms of the learning speed and the ﬁnal error.
The computational complexities of updating one dictionary atom for the Grass and the LDSST method
are O((J + N )L2n2m2)) and O((J + N )n2m2))  respectively. Here  J is the dictionary size  N is
the number of data  and n and m are LDS parameters deﬁned in Eq. (1). In contrast  PGD requires to
calculate the projected gradients of the canonical tuples which scales to only O((J + N )n2m). As
shown in Fig. 2  PGD is more than 50 times faster than the Grass-3 and LDSST methods per epoch.

5.2 Video Classiﬁcation

Classifying YUPENN or DynTex videos is challenging as the videos are recoded under various
viewpoints and scales. To deliver robust features  we implement two kinds of high-level pooling
frameworks: Bag-of-Systems (BoS) [11] and Spatial-Temporal-Pyramid-Matching (STPM) [16]3.
In particular  1) BoS is performed with the clustering methods  i.e.  MDS  Align and PGD. The
BoS framework models the local spatio-temporal blocks with LDSs and then clusters the LDS
descriptors to obtain the codewords; 2)The STPM framework works in conjunction with the sparse
coding approaches (i.e.  Grass-3  LDSST and the PGD methods). Unlike BoS that represents a
video by unordered local descriptors  STPM partitions a video into segments under different scales
(2-level scales are considered here) and concatenates all local descriptors for each segment to form a
vectorized representation. The codewords are provided by learning a dictionary. For the BoS methods 
we apply the nonlinear SVM as the classiﬁer where the radial basis kernel with χ2 distance [33] is
employed; while for the STPM methods  we utilize linear SVM for classiﬁcation.

Table 1: Mean classiﬁcation accuracies (percentage) on the YUPENN and DynTex datasets.

Datasets References
YUPENN 85 [10]
DynTex

-

+BoS

+STPM

MDS Align PGD Grass-3 LDSST PGD
90.7 93.6
83.3 82.1 84.1
59.5 62.7 65.4
75.1 76.5

91.6
75.1

YUPENN. The non-overlapping spatio-temporal blocks of size 8 × 8 × 25 were sampled from the
videos. The number of the codewords for all BoS and STPM methods was set to 128. We sampled 50
blocks from each video to learn the codewords for the MDS  Align  Grass-3 and LDSST methods. For
PGD  we updated the codewords by mini-batches. To maintain the diversity within each mini-batch  a

3 In the experiments  we consider the projection kernel as deﬁned in Eq. (5). We have also conducted
additional experiments by considering a new kernel  namely the Martin kernel (Eq. (19)). The results are
provided in the supplementary material.

8

10 100 1000 10000Time (s)-1.5-1-0.50Testing NRPGD-fullPGD-128LDSSTGrass-310 100 1000 1000050000Time (s)-1-0.8-0.6-0.4-0.20Testing NRPGD-fullPGD-128LDSSTGrass-310 100 1000 1000050000Time (s)-0.6-0.5-0.4-0.3-0.2-0.10Testing NRPGD-fullPGD-128LDSSTGrass-3hierarchical approach was used. In particular  at each iteration  we ﬁrst randomly sampled 20 videos
from the dataset and then sampled 4 blocks from each of the videos  leading to a mini-batch of size
N(cid:48) = 80. The learning rates were set as 0.5 for Λ and V and 5 for U  and their values were decreased
by half every 10 epochs. The test protocol is the leave-one-video-out as suggested in [29]  leading
to a total of 420 trials. Table 1 shows that the STPM methods achieve better accuracies than the
BoS approaches; within the same pooling framework  our PGD always outperforms other compared
models. For the probabilistic clustering method [10]  the result on YUPENN is 85% reported in
Table 1. Note that in [10]  a richer number of dictionary has been applied.
DynTex. For the Dyntex dataset  the spatio-temporal blocks of size 16 × 16 × 50 were sampled in a
non-overlapping way. The number of the codewords for all methods was chosen as 64. We applied
the same sampling strategy as that on YUPENN to learn the codewords for all compared methods. As
shown in Table 1  the proposed method is superior compared to the studied models with both BoS
and STPM coding strategies.

6 Conclusion

We propose an efﬁcient Projected-Gradient-Decent (PGD) method to optimize problem (11). Our
algorithm can be used to perform clustering and sparse coding with LDSs. In contrast to previous
studies  our solution avoids any approximation in LDS modeling or during the optimization process.
Extensive experiments on clustering and sparse coding verify the effectiveness of the proposed method
in terms of the convergence performance and learning speed. We also explore the combination of
PGD with two high-level pooling frameworks  namely Bag-of-Systems (BoS) and Spatial-Temporal-
Pyramid-Matching for video classiﬁcation. The experimental results demonstrate that our PGD
method outperforms state-of-the-art methods consistently.

Acknowledgments

This research was supported in part by the National Science Foundation of China (NSFC) (Grant No:
91420302  91520201 61210013 and 61327809)  the NSFC and the German Research of Foundation
(DFG) in project Crossmodal Learning (Grant No: NSFC 61621136008/ DFG TRR-169)  and the
National High-Tech Research and Development Plan under Grant 2015AA042306. Besides  Tong
Zhang was supported by Australian Research Council’s Discovery Projects funding scheme (project
DP150104645).

References
[1] Gianfranco Doretto  Alessandro Chiuso  Ying Nian Wu  and Stefano Soatto. Dynamic textures. Interna-

tional Journal of Computer Vision (IJCV)  51(2):91–109  2003.

[2] Tae-Kyun Kim and Roberto Cipolla. Canonical correlation analysis of video volume tensors for action
categorization and detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 
31(8):1415–1428  2009.

[3] Chuang Gan  Naiyan Wang  Yi Yang  Dit-Yan Yeung  and Alex G Hauptmann. Devnet: A deep event

network for multimedia event detection and evidence recounting. In CVPR  pages 2568–2577.

[4] Chuang Gan  Ting Yao  Kuiyuan Yang  Yi Yang  and Tao Mei. You lead  we exceed: Labor-free video

concept learning by jointly exploiting web videos and images. In CVPR  pages 923–932  2016.

[5] Wenbing Huang  Fuchun Sun  Lele Cao  Deli Zhao  Huaping Liu  and Mehrtash Harandi. Sparse coding
and dictionary learning with linear dynamical systems. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). IEEE  2016.

[6] Sajid M Siddiqi  Byron Boots  and Geoffrey J Gordon. A constraint generation approach to learning stable

linear dynamical systems. In Advances in Neural Information Processing Systems (NIPS)  2007.

[7] Wenbing Huang  Lele Cao  Fuchun Sun  Deli Zhao  Huaping Liu  and Shanshan Yu. Learning stable
linear dynamical systems with the weighted least square method. In Proceedings of the International Joint
Conference on Artiﬁcial Intelligence (IJCAI)  2016.

[8] Søren Johansen. Likelihood-based inference in cointegrated vector autoregressive models. Oxford

University Press on Demand  1995.

9

[9] Bijan Afsari and René Vidal. Distances on spaces of high-dimensional linear stochastic processes: A

survey. In Geometric Theory of Information  pages 219–242. Springer  2014.

[10] Adeel Mumtaz  Emanuele Coviello  Gert RG Lanckriet  and Antoni B Chan. A scalable and accurate
descriptor for dynamic textures using bag of system trees. IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI)  37(4):697–712  2015.

[11] Avinash Ravichandran  Rizwan Chaudhry  and Rene Vidal. Categorizing dynamic textures using a
bag of dynamical systems. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 
35(2):342–353  2013.

[12] Pavan Turaga  Ashok Veeraraghavan  Anuj Srivastava  and Rama Chellappa. Statistical computations on
Grassmann and Stiefel manifolds for image and video-based recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence (TPAMI)  33(11):2273–2286  2011.

[13] Mehrtash Harandi  Richard Hartley  Chunhua Shen  Brian Lovell  and Conrad Sanderson. Extrinsic
methods for coding and dictionary learning on Grassmann manifolds. International Journal of Computer
Vision (IJCV)  114(2):113–136  2015.

[14] Bijan Afsari  Rizwan Chaudhry  Avinash Ravichandran  and René Vidal. Group action induced distances
for averaging and clustering linear dynamical systems with applications to the analysis of dynamic scenes.
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 2208–2215. IEEE  2012.

[15] Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints. Mathe-

matical Programming  142(1-2):397–434  2013.

[16] Jianchao Yang  Kai Yu  Yihong Gong  and Thomas Huang. Linear spatial pyramid matching using sparse
coding for image classiﬁcation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
pages 1794–1801. IEEE  2009.

[17] Antoni B Chan and Nuno Vasconcelos. Probabilistic kernels for the classiﬁcation of auto-regressive visual
processes. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) 
volume 1  pages 846–851. IEEE  2005.

[18] Franco Woolfe and Andrew Fitzgibbon. Shift-invariant dynamic texture recognition.

Conference on Computer Vision (ECCV)  pages 549–562. Springer  2006.

In European

[19] SVN Vishwanathan  Alexander J Smola  and René Vidal. Binet-Cauchy kernels on dynamical systems
and its application to the analysis of dynamic scenes. International Journal of Computer Vision (IJCV) 
73(1):95–119  2007.

[20] Payam Saisan  Gianfranco Doretto  Ying Nian Wu  and Stefano Soatto. Dynamic texture recognition. In
IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)  volume 2  pages
II–58. IEEE  2001.

[21] Antoni B Chan and Nuno Vasconcelos. Classifying video with kernel dynamic textures. In IEEE Conference

on Computer Vision and Pattern Recognition (CVPR)  pages 1–6. IEEE  2007.

[22] Katrien De Cock and Bart De Moor. Subspace angles between ARMA models. Systems & Control Letters 

46(4):265–270  2002.

[23] Antoni B. Chan  Emanuele Coviello  and Gert RG Lanckriet. Clustering dynamic textures with the
hierarchical EM algorithm. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
pages 2022–2029. IEEE  2010.

[24] Antoni B. Chan  Emanuele Coviello  and Gert RG Lanckriet. Clustering dynamic textures with the

hierarchical EM algorithm for modeling video. 35(7):1606–1621  2013.

[25] Richard J Martin. A metric for ARMA processes. IEEE Transactions on Signal Processing  48(4):1164–

1170  2000.

[26] A Barraud. A numerical algorithm to solve aˆ{T} xa-x= q. IEEE Transactions on Automatic Control 

22(5):883–885  1977.

[27] Dan Kalman. A singularly valuable decomposition: the svd of a matrix. The college mathematics journal 

27(1):2–23  1996.

[28] Bernard Ghanem and Narendra Ahuja. Maximum margin distance learning for dynamic texture recognition.

In European Conference on Computer Vision (ECCV)  pages 223–236. Springer  2010.

10

[29] Konstantinos G Derpanis  Matthieu Lecce  Kostas Daniilidis  and Richard P Wildes. Dynamic scene
understanding: The role of orientation features in space and time in scene classiﬁcation. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR)  pages 1306–1313. IEEE  2012.

[30] Renaud Péteri  Sándor Fazekas  and Mark J. Huiskes. DynTex : a Comprehensive Database of Dynamic Tex-
tures. Pattern Recognition Letters  doi: 10.1016/j.patrec.2010.05.009  2010. http://projects.cwi.nl/dyntex/.

[31] Guoying Zhao and Matti Pietikainen. Dynamic texture recognition using local binary patterns with
an application to facial expressions. IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI)  29(6):915–928  2007.

[32] Anna Huang. Similarity measures for text document clustering. In Proceedings of the sixth new zealand
computer science research student conference (NZCSRSC2008)  Christchurch  New Zealand  pages 49–56 
2008.

[33] Richard O Duda  Peter E Hart  and David G Stork. Pattern classiﬁcation. John Wiley & Sons  2012.

11

,Wenbing Huang
Mehrtash Harandi
Tong Zhang
Lijie Fan
Fuchun Sun
Junzhou Huang