2018,Smoothed analysis of the low-rank approach for smooth semidefinite programs,We consider semidefinite programs (SDPs) of size $n$ with equality constraints. In order to overcome scalability issues  Burer and Monteiro proposed a factorized approach based on optimizing over a matrix $Y$ of size $n\times k$ such that $X=YY^*$ is the SDP variable. The advantages of such formulation are twofold: the dimension of the optimization variable is reduced  and positive semidefiniteness is naturally enforced. However  optimization in $Y$ is non-convex. In prior work  it has been shown that  when the constraints on the factorized variable regularly define a smooth manifold  provided $k$ is large enough  for almost all cost matrices  all second-order stationary points (SOSPs) are optimal. Importantly  in practice  one can only compute points which approximately satisfy necessary optimality conditions  leading to the question: are such points also approximately optimal? To this end  and under similar assumptions  we use smoothed analysis to show that approximate SOSPs for a randomly perturbed objective function are approximate global optima  with $k$ scaling like the square root of the number of constraints (up to log factors). We particularize our results to an SDP relaxation of phase retrieval.,Smoothed analysis of the low-rank approach

for smooth semideﬁnite programs

Thomas Pumir∗
ORFE Department
Princeton University

Samy Jelassi∗

ORFE Department
Princeton University

tpumir@princeton.edu

sjelassi@princeton.edu

Nicolas Boumal

Department of Mathematics

Princeton University

nboumal@math.princeton.edu

Abstract

We consider semideﬁnite programs (SDPs) of size n with equality constraints. In
order to overcome scalability issues  Burer and Monteiro proposed a factorized
approach based on optimizing over a matrix Y of size n× k such that X = Y Y ∗ is
the SDP variable. The advantages of such formulation are twofold: the dimension
of the optimization variable is reduced  and positive semideﬁniteness is naturally
enforced. However  optimization in Y is non-convex. In prior work  it has been
shown that  when the constraints on the factorized variable regularly deﬁne a
smooth manifold  provided k is large enough  for almost all cost matrices  all
second-order stationary points (SOSPs) are optimal.
Importantly  in practice 
one can only compute points which approximately satisfy necessary optimality
conditions  leading to the question: are such points also approximately optimal?
To answer it  under similar assumptions  we use smoothed analysis to show that
approximate SOSPs for a randomly perturbed objective function are approximate
global optima  with k scaling like the square root of the number of constraints (up
to log factors). Moreover  we bound the optimality gap at the approximate solution
of the perturbed problem with respect to the original problem. We particularize our
results to an SDP relaxation of phase retrieval.

(cid:104)C  X(cid:105)

min
X∈Sn×n
subject to A(X) = b 

X (cid:23) 0 

(SDP)

Introduction

1
We consider semideﬁnite programs (SDP) over K = R or C of the form:

with (cid:104)A  B(cid:105) = Re[Tr(A∗B)] the Frobenius inner product (A∗ is the conjugate-transpose of A)  Sn×n
the set of self-adjoint matrices of size n (real symmetric for R  or Hermitian for C)  C ∈ Sn×n
the cost matrix  and A : Sn×n → Rm a linear operator capturing m equality constraints with right
hand side b ∈ Rm: for each i  A(X)i = (cid:104)Ai  X(cid:105) = bi for given matrices A1  . . .   Am ∈ Sn×n. The
optimization variable X is positive semideﬁnite. We let C be the feasible set of (SDP):

C =(cid:8)X ∈ Sn×n : A(X) = b and X (cid:23) 0(cid:9) .

(1)

∗Equal contribution

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Large-scale SDPs have been proposed for machine learning applications including matrix comple-
tion [Candès and Recht  2009]  community detection [Abbé  2018] and kernel learning [Lanckriet
et al.  2004] for K = R  and in angular synchronization [Singer  2011] and phase retrieval [Wald-
spurger et al.  2015] for K = C. Unfortunately  traditional methods to solve (SDP) do not scale (due
to memory and computational requirements)  hence the need for alternatives.
In order to address such scalability issues  Burer and Monteiro [2003  2005] restrict the search to
the set of matrices of rank at most k by factorizing X as X = Y Y ∗  with Y ∈ Kn×k. It has been
shown that if the search space C (1) is compact  then (SDP) admits a global optimum of rank at most
r  where dim Sr×r ≤ m [Barvinok  1995  Pataki  1998]  with dim Sr×r = r(r+1)
for K = R and
dim Sr×r = r2 for K = C. In other words  restricting C to the space of matrices with rank at most k
with dim Sk×k > m does not change the optimal value. This factorization leads to a quadratically
constrained quadratic program:

2

(cid:104)C  Y Y ∗(cid:105)

min
Y ∈Kn×k
subject to A(Y Y ∗) = b.
Although (P) is in general non-convex because its feasible set

M = Mk =(cid:8)Y ∈ Kn×k : A(Y Y ∗) = b(cid:9)

(P)

(2)

is non-convex  considering (P) instead of the original SDP presents signiﬁcant advantages: the
number of variables is reduced from O(n2) to O(nk)  and the positive semideﬁniteness of the matrix
is naturally enforced. Solving (P) using local optimization methods is known as the Burer–Monteiro
method and yields good results in practice: Kulis et al. [2007] underlined the practical success of
such low-rank approaches in particular for maximum variance unfolding and for k-means clustering
(see also [Carson et al.  2017]). Their approach is signiﬁcantly faster and more scalable. However 
the non-convexity of (P) means further analysis is needed to determine whether it can be solved to
global optimality reliably.
For K = R  in the case where M is a compact  smooth manifold (see Assumption 1 below for a
precise condition)  it has been shown recently that  up to a zero-measure set of cost matrices  second-
order stationary points (SOSPs) of (P) are globally optimal provided dim Sk×k > m [Boumal et al. 
2016  2018b]. Algorithms such as the Riemannian trust-regions method (RTR) converge globally to
SOSPs  but unfortunately they can only guarantee approximate satisfaction of second-order optimality
conditions in a ﬁnite number of iterations [Boumal et al.  2018a].
The aforementioned papers close with a question  crucial in practice: when is it the case that
approximate SOSPs  which we now call ASOSPs  are approximately optimal? Building on recent
proof techniques by Bhojanapalli et al. [2018]  we provide some answers here.

Contributions

This paper formulates approximate global optimality conditions holding for (P) and  consequently 
for (SDP). Our results rely on the following core assumption as set in [Boumal et al.  2016].
Assumption 1 (Smooth manifold). For all values of k up to n such that Mk is non-empty  the
constraints on (P) deﬁned by A1  . . .   Am ∈ Sn×n and b ∈ Rm satisfy at least one of the following:

1. {A1Y  . . .   AmY } are linearly independent in Kn×k for all Y ∈ Mk; or
2. {A1Y  . . .   AmY } span a subspace of constant dimension in Kn×k for all Y in an open

neighborhood of Mk in Kn×k.

In [Boumal et al.  2018b]  it is shown that (a) if the assumption above is veriﬁed for k = n  then it
automatically holds for all values of k ≤ n such that Mk is non-empty; and (b) for those values of k 
the dimension of the subspace spanned by {A1Y  . . .   AmY } is independent of k: we call it m(cid:48).
When Assumption 1 holds  we refer to problems of the form (SDP) as smooth SDPs because M is
then a smooth manifold. Examples of smooth SDPs for K = R are given in [Boumal et al.  2018b].
For K = C  we detail an example in Section 4. Our main theorem is a smooth analysis result (cf.
Theorem 3.1 for a more formal statement). An ASOSP is an approximate SOSP (a precise deﬁnition
follows.)

2

Theorem 1.1 (Informal). Let Assumption 1 hold and assume C is compact. Randomly perturb the cost
m)  any ASOSP Y ∈ Kn×k for (P) is an approximate
matrix C. With high probability  if k = ˜Ω(
global optimum  and X = Y Y ∗ is an approximate global optimum for (SDP) (with the perturbed C.)

√

The high probability proviso is with respect to the perturbation only: if the perturbation is “good” 
then all ASOSPs are as described in the statement. If C is compact  then so is M and known
algorithms for optimization on manifolds produce an ASOSP in ﬁnite time (with explicit bounds).
Theorem 1.1 ensures that  for k large enough and for any cost matrix C  with high probability upon a
random perturbation of C  such algorithms produce an approximate global optimum of (P).
Theorem 1.1 is a corollary of two intermediate arguments  developed in Lemmas 3.1 and 3.2:

1. Probabilistic argument (Lemma 3.1): By perturbing the cost matrix in the objective function
of (P) with a Gaussian Wigner matrix  with high probability  any approximate ﬁrst-order
stationary point Y of the perturbed problem (P) is almost column-rank deﬁcient.

2. Deterministic argument (Lemma 3.2): If an approximate second-order stationary point Y
for (P) is also almost column-rank deﬁcient  then it is an approximate global optimum and
X = Y Y ∗ is an approximate global optimum for (SDP).

The ﬁrst argument is motivated by smoothed analysis [Spielman and Teng  2004] and draws heavily
on a recent paper by Bhojanapalli et al. [2018]. The latter work introduces smoothed analysis to
analyze the performance of the Burer–Monteiro factorization  but it analyzes a quadratically penalized
version of the SDP: its solutions do not satisfy constraints exactly. This affords more generality 
but  for the special class of smooth SDPs  the present work has the advantage of analyzing an exact
formulation. The second argument is a smoothed extension of well-known on-off results [Burer and
Monteiro  2003  2005  Journee et al.  2010]. Implications of this theorem for a particular SDP are
derived in Section 4  with applications to phase retrieval and angular synchronization.
Thus  for smooth SDPs  our results improve upon [Bhojanapalli et al.  2018] in that we address
exact-feasibility formulations of the SDP. Our results also improve upon [Boumal et al.  2016] by
providing approximate optimality results for approximate second-order points with relaxation rank
k scaling only as ˜Ω(
m)  whereas the latter reference establishes such results only for k = n + 1.
Finally  we aim for more generality by covering both real and complex SDPs  and we illustrate the
relevance of complex SDPs in Section 4.

√

Related work

A number of recent works focus on large-scale SDP solvers. Among the direct approaches (which
proceed in the convex domain directly)  Hazan [2008] introduced a Frank–Wolfe type method for a
restricted class of SDPs. Here  the key is that each iteration increases the rank of the solution only
by one  so that if only a few iterations are required to reach satisfactory accuracy  then only low
dimensional objects need to be manipulated. This line of work was later improved by Laue [2012] 
Garber [2016] and Garber and Hazan [2016] through hybrid methods. Still  if high accuracy solutions
are desired  a large number of iterations will be required  eventually leading to large-rank iterates.
In order to overcome such issue  Yurtsever et al. [2017] recently proposed to combine conditional
gradient and sketching techniques in order to maintain a low rank representation of the iterates.
Among the low-rank approaches  our work is closest to (and indeed largely builds upon) recent results
of Bhojanapalli et al. [2018]. For the real case  they consider a penalized version of problem (SDP)
(which we here refer to as (P-SDP)) and its related penalized Burer–Monteiro formulation  here called
(P-P). With high probability upon random perturbation of the cost matrix  they show approximate
global optimality of ASOSPs for (P-P)  assuming k grows with
m and either the SDP is compact
or its cost matrix is positive deﬁnite. Given that there is a zero-measure set of SDPs where SOSPs
may be suboptimal  there can be a small-measure set of SDPs where ASOSPs are not approximately
optimal [Bhojanapalli et al.  2018]. In this context  the authors resort to smoothed analysis  in the
same way that we do here. One drawback of that work is that the ﬁnal result does not hold for the
original SDP  but for a non-equivalent penalized version of it. This is one of the points we improve
here  by focusing on smooth SDPs as deﬁned in [Boumal et al.  2016].

√

3

The associated Frobenius norm is deﬁned as (cid:107)A(cid:107) =(cid:112)(cid:104)A  A(cid:105). For a linear map f between matrix

Notation
We use K to refer to R or C when results hold for both ﬁelds. For matrices A  B of same size  we use
the inner product (cid:104)A  B(cid:105) = Re[Tr(A∗B)]  which reduces to (cid:104)A  B(cid:105) = Tr(AT B) in the real case.
(cid:107)f (A)(cid:107)
spaces  this yields a subordinate operator norm as (cid:107)f(cid:107)op = supA(cid:54)=0
(cid:107)A(cid:107) . The set of self-adjoint
matrices of size n over K  Sn×n  is the set of symmetric matrices for K = R or the set of Hermitian
matrices for K = C. We also write Hn×n to denote Sn×n for K = C. A self-adjoint matrix X is
positive semideﬁnite (X (cid:23) 0) if and only if u∗Xu ≥ 0 for all u ∈ Kn. Furthermore  I is the identity
operator and In is the identity matrix of size n. The integer m(cid:48) is deﬁned after Assumption 1.

2 Geometric framework and near-optimality conditions

In this section  we present properties of the smooth geometry of (P) and approximate global optimality
conditions for this problem. In covering these preliminaries  we largely parallel developments
in [Boumal et al.  2016]. As argued in that reference  Assumption 1 implies that the search space
M of (P) is a submanifold in Kn×k of codimension m(cid:48). We can associate tangent spaces to a
submanifold. Intuitively  the tangent space TY M to the submanifold M at a point Y ∈ M is a
subspace that best approximates M around Y   when the subspace origin is translated to Y . It is
obtained by linearizing the equality constraints.
Lemma 2.1 (Boumal et al. [2018b  Lemma 2.1]). Under Assumption 1  the tangent space at Y to
M (2)  denoted by TY M  is:
TY M =

(cid:111)
(cid:110) ˙Y ∈ Kn×k : A( ˙Y Y ∗ + Y ˙Y ∗) = 0
(cid:110) ˙Y ∈ Kn×k : (cid:104)AiY  ˙Y (cid:105) = 0 for i = 1  . . .   m

(3)
By equipping each tangent space with a restriction of the inner product (cid:104)· ·(cid:105)  we turn M into a
Riemannian submanifold of Kn×k. We also introduce the orthogonal projector ProjY : Kn×k →
TY M which  given a matrix Z ∈ Kn×k  projects it to the tangent space TY M:

=

(cid:111)

.

ProjY Z := argmin
˙Y ∈TY M

(cid:107) ˙Y − Z(cid:107).

(4)

This projector will be useful to phrase optimality conditions. It is characterized as follows.
Lemma 2.2 (Boumal et al. [2018b  Lemma 2.2]). Under Assumption 1  the orthogonal projector
admits the closed form

ProjY Z = Z − A∗(cid:0)G†A(ZY ∗)(cid:1) Y 

where A∗ : Rm → Sn×n is the adjoint of A  G is a Gram matrix deﬁned by Gij = (cid:104)AiY  AjY (cid:105) (it is
a function of Y )  and G† denotes the Moore–Penrose pseudo-inverse of G (differentiable in Y ).
(See a proof in Appendix A.) To properly state the approximate ﬁrst- and second-order necessary
optimality conditions for (P)  we further need the notions of Riemannian gradient and Riemannian
Hessian on the manifold M. We recall that (P) minimizes the function g  deﬁned by

g(Y ) = (cid:104)CY  Y (cid:105)  

(5)
on the manifold M. The Riemannian gradient of g at Y   grad g(Y )  is the unique tangent vector at
Y such that  for all tangent ˙Y   (cid:104)grad g(Y )  ˙Y (cid:105) = (cid:104)∇g(Y )  ˙Y (cid:105)  with ∇g(Y ) = 2CY the Euclidean
(classical) gradient of g evaluated at Y . Intuitively  grad g(Y ) is the tangent vector at Y that points
in the steepest ascent direction for g as seen from the manifold’s perspective. A classical result states
that  for Riemannian submanifolds  the Riemannian gradient is given by the projection of the classical
gradient to the tangent space [Absil et al.  2008  eq. (3.37)]:

grad g(Y ) = ProjY (∇g(Y )) = 2(cid:0)C − A∗(cid:0)G†A(CY Y ∗)(cid:1)(cid:1) Y.

(6)
This leads us to deﬁne the matrix S ∈ Sn×n which plays a key role to guarantee approximate global
optimality for problem (P)  as discussed in Section 3:

µiAi 

(7)

S = S(Y ) = C − A∗(µ) = C − m(cid:88)

i=1

4

where µ = µ(Y ) = G†A(CY Y ∗). We can write the Riemannian gradient of g evaluated at Y as

grad g(Y ) = 2SY.

(8)

The Riemannian gradient enables us to deﬁne an approximate ﬁrst-order necessary optimality
condition below. To deﬁne the approximate second-order necessary optimality condition  we need
to introduce the notion of Riemannian Hessian. The Riemannian Hessian of g at Y is a self-adjoint
operator on the tangent space at Y obtained as the projection of the derivative of the Riemannian
gradient vector ﬁeld [Absil et al.  2008  eq. (5.15)]. Boumal et al. [2018b] give a closed form
expression for the Riemannian Hessian of g at Y :

Hess g(Y )[ ˙Y ] = 2 · ProjY (S ˙Y ).

(9)

We can now formally deﬁne the approximate necessary optimality conditions for problem (P).
Deﬁnition 2.1 (εg-FOSP). Y ∈ M is an εg–ﬁrst-order stationary point for (P) if the norm of the
Riemannian gradient of g at Y almost vanishes  speciﬁcally 

(cid:107)grad g(Y )(cid:107) = (cid:107)2SY (cid:107) ≤ εg 

where S is deﬁned as in equation (7).
Deﬁnition 2.2 ((εg  εH )-SOSP). Y ∈ M is an (εg  εH)–second-order stationary point for (P) if
it is an εg–ﬁrst-order stationary point and the Riemannian Hessian of g at Y is almost positive
semideﬁnite  speciﬁcally 

(cid:68) ˙Y   Hess g(Y )[ ˙Y ]

(cid:69)

∀ ˙Y ∈ TY M 

1
2

= (cid:104) ˙Y   S ˙Y (cid:105) ≥ −εH(cid:107) ˙Y (cid:107)2.

From these deﬁnitions  it is clear that S encapsulates the approximate optimality conditions of
problem (P).

3 Approximate second-order points and smoothed analysis

We state our main results formally in this section. As announced  following [Bhojanapalli et al.  2018] 
we resort to smoothed analysis [Spielman and Teng  2004]. To this end  we consider perturbations
of the cost matrix C of (SDP) by a Gaussian Wigner matrix. Intuitively  smoothed analysis tells us
how large the variance of the perturbation should be in order to obtain a new SDP which  with high
probability  is sufﬁciently distant from any pathological case. We start by formally deﬁning the notion
of Gaussian Wigner matrix  following [Ben Arous and Guionnet  2010].
Deﬁnition 3.1 (Gaussian Wigner matrix). The random matrix W = W ∗ in Sn×n is a Gaussian
Wigner matrix with variance σ2
W if its entries on and above the diagonal are independent  zero-mean
Gaussian variables (real or complex depending on context) with variance σ2
Besides Assumption 1  another important assumption for our results is that the search space C (1)
of (SDP) is compact. In that scenario  there exists a ﬁnite constant R such that

W .

(10)
Thus  for all Y ∈ M  (cid:107)Y (cid:107)2 = Tr(Y Y ∗) ≤ R. Another consequence of compactness of C is that the
operator A∗ ◦ G† ◦ A is uniformly bounded  that is  there exists a ﬁnite constant K such that

∀X ∈ C  Tr(X) ≤ R.

∀Y ∈ M 

(cid:107)A∗ ◦ G† ◦ A(cid:107)op ≤ K 

(11)
where G† is a continuous function of Y as in Lemma 2.2. We give explicit expressions for the
constants R and K for the case of phase retrieval in Section 4.
We now state the main theorem  whose proof is in Appendix E.
Theorem 3.1. Let Assumption 1 hold for (SDP) with cost matrix C ∈ Sn×n and m constraints.
Assume C (1) is compact  and let R and K be as in (10) and (11). Let W be a Gaussian Wigner
matrix with variance σ2

W and let δ ∈ (0  1) be any tolerance. Deﬁne κ as:
√

κ = κ(R  K  C  n  σW ) = RK(cid:0)(cid:107)C(cid:107)op + 3σW

n(cid:1) .

(12)

5

There exists a universal constant c0 such that  if the rank k for the low-rank problem (P) satisﬁes

(cid:34)

log(n) +(cid:112)log(1/δ) +

(cid:115)

k ≥ 3

(cid:18)

(cid:19)(cid:35)

√
6κ

c0n

σW

m · log

1 +

 

(13)

then  with probability at least 1 − δ − e− n
of (P) with perturbed cost matrix C + W has bounded optimality gap:

2 on the random matrix W   any (εg  εH )-SOSP Y ∈ Kn×k

0 ≤ g(Y ) − f (cid:63) ≤ (εH + ε2

gη)R +

εg
2

√

R 

(14)

(15)

with g the cost function of (P)  f (cid:63) the optimal value of (SDP) (both perturbed)  and

η = η(R  K  C  n  m  σW ) =

c0nK(2 + KR)2 ((cid:107)C(cid:107)op + 3σW

9mσ2

W log

√

(cid:17)

n))

.

(cid:16)

√

σW

c0n

1 + 6κ
√

√

k scales with(cid:112)log(1/σW )  so that a smaller σW may require k to be a larger multiple of

This result indicates that  as long as the rank k is on the order of
m (up to logarithmic factors)  the
optimality gap in the perturbed problem is small if a sufﬁciently good approximate second-order
point is computed. Since (SDP) may admit a unique solution of rank as large as Θ(
m) (see for
example [Laurent and Poljak  1996  Thm. 3.1(ii)] for the Max-Cut SDP)  we conclude that the scaling
of k with respect to m in Theorem 3.1 is essentially optimal.
There is an incentive to pick σW small  since the optimality gap is phrased in terms of the perturbed
problem. As expected though  taking σW small comes at a price. Speciﬁcally  the required rank
m.
Furthermore  the optimality gap is bounded in terms of η with a dependence in ε2
W ; this may force
us to compute more accurate approximate second-order points (smaller εg) for a similar guarantee
when σW is smaller: see also Corollary 3.1 below.
As announced  the theorem rests on two arguments which we now present—a probabilistic one  and a
deterministic one:

g/σ2

√

1. Probabilistic argument: In the smoothed analysis framework  we show  for k large enough 
that εg-FOSPs of (P) have their smallest singular value near zero  with high probability upon
perturbation of C. This implies that such points are almost column-rank deﬁcient.

2. Deterministic argument: If Y is an (εg  εH )-SOSP of (P) and it is almost column-rank
deﬁcient  then the matrix S(Y ) deﬁned in equation (7) is almost positive semideﬁnite. From
there  we can derive a bound on the optimality gap.

Formal statements for both follow  building on the notation in Theorem 3.1. Proofs are in Appen-
dices C and D  with supporting lemmas in Appendix B.
Lemma 3.1. Let Assumption 1 hold for (SDP). Assume C (1) is compact. Let W be a Gaussian
W and let δ ∈ (0  1) be any tolerance. There exists a universal
Wigner matrix with variance σ2
constant c0 such that  if the rank k for the low-rank problem (P) is lower bounded as in (13)  then 
with probability at least 1 − δ − e− n

2 on the random matrix W   we have

(cid:107)W(cid:107)op ≤ 3σW

√

n 

and furthermore: any εg-FOSP Y ∈ Kn×k of (P) with perturbed cost matrix C + W satisﬁes

√

σk(Y ) ≤ εg
σW

c0n
k

 

where σk(Y ) is the kth singular value of the matrix Y .
Lemma 3.2. Let Assumption 1 hold for (SDP) with cost matrix C. Assume C is compact. Let
Y ∈ Kn×k be an (εg  εH )-SOSP of (P) (for any k). Then  the smallest eigenvalue of S = S(Y ) (7)
is bounded below as

λmin(S) ≥ −εH − ζ(cid:107)C(cid:107)op · σ2

k(Y ) 

where ζ = K(2 + KR)2 with R  K as in (10) and (11)  and σk(Y ) is the kth singular value of Y (it
is zero if k > n). This holds deterministically for any cost matrix C.

6

Combining the two above lemmas  the key step in the proof of Theorem 3.1 is to deduce a bound on
the optimality gap from a bound on the smallest eigenvalue of S: see Appendix E.
We have shown in Theorem 3.1 that a perturbed version of (P) can be approximately solved to global
optimality  with high probability on the perturbation. In the corollary below  we further bound the
optimality gap at the approximate solution of the perturbed problem with respect to the original 
unperturbed problem. The proof is in Appendix F.
Corollary 3.1. Assume C is compact and let R be as deﬁned in (10). Let X ∈ C be an approximate
solution for (SDP) with perturbed cost matrix C + W   so that the optimality gap in the perturbed
problem is bounded by εf . Let f (cid:63) denote the optimal value of the unperturbed problem (SDP)  with
cost matrix C. Then  the optimality gap for X with respect to the unperturbed problem is bounded
as:

0 ≤ (cid:104)C  X(cid:105) − f (cid:63) ≤ εf + 2(cid:107)W(cid:107)opR.

Under the conditions of Theorem 3.1  with the prescribed probability  εf and (cid:107)W(cid:107)op can be bounded
so that for an (εg  εH )-SOSP Y of the perturbed problem (P) we have:

0 ≤ (cid:104)CY  Y (cid:105) − f (cid:63) ≤ (εH + ε2

gη)R +

εg
2

R + 6σW

nR 

√

√

where η is as deﬁned in (15) and σ2

W is the variance of the Wigner perturbation W .

4 Applications

The approximate global optimality results established in the previous section can be applied to deduce
guarantees on the quality of ASOSPs of the low-rank factorization for a number of SDPs that appear
in machine learning problems. Of particular interest  we focus on the phase retrieval problem. This
problem consists in retrieving a signal z ∈ Cd from n amplitude measurements b = |Az| ∈ Rn
+ (the
absolute value of vector Az is taken entry-wise). If we can recover the complex phases of Az  then z
can be estimated through linear least-squares. Following this approach  Waldspurger et al. [2015]
argue that this task can be modeled as the following non-convex problem:

u∗Cu

min
u∈Cn
subject to |ui| = 1  for i = 1  . . .   n 

(PR)

where C = diag(b)(I − AA†)diag(b) and diag : Rn → Hn×n maps a vector to the corresponding
diagonal matrix. The classical relaxation is to rewrite the above in terms of X = uu∗ (lifting) without
enforcing rank(X) = 1  leading to a complex SDP which Waldspurger et al. [2015] call PhaseCut:

min

X∈Hn×n
subject to diag(X) = 1 

(cid:104)C  X(cid:105)

X (cid:23) 0.

(PhaseCut)

The same SDP relaxation also applies to a problem called angular synchronization [Singer  2011].
The Burer–Monteiro factorization of (PhaseCut) is an optimization problem over a matrix Y ∈ Cn×k
as follows:

(cid:104)CY  Y (cid:105)

min
Y ∈Cn×k
subject to diag(Y Y ∗) = 1.

(PhaseCut-BM)

For a feasible Y   each row has unit norm: the search space is a Cartesian product of spheres in Ck 
which is a smooth manifold. We can check that Assumption 1 holds for all k ≥ 1. Furthermore  the
feasible space of the SDP is compact. Therefore  Theorem 3.1 applies.
In this setting  Tr(X) = n for all feasible X  and (cid:107)A∗ ◦ G† ◦ A(cid:107)op = 1 for all feasible Y (because
G = G(Y ) = Im for all feasible Y —see Lemma 2.2—and A∗ ◦ A is an orthogonal projector from
Hermitian matrices to diagonal matrices). For this reason  the constants deﬁned in (10) and (11) can
be set to R = n and K = 1.
As a comparison  Mei et al. [2017] also provide an optimality gap for ASOSPs of (PhaseCut) without
perturbation. Their result is more general in the sense that it holds for all possible values of k.

7

Figure 1: Computation time of the dedicated interior-point method (IPM) and of the Burer–Monteiro
approach (BM) to solve (PhaseCut). For increasing values of n (horizontal axis)  we display the
computation time averaged over four independent realizations of the problem (vertical axis). The
smallest and largest observed computation times are represented with dashed lines. At n = 3000 
BM is about 40 times faster than IPM. For the largest value of n  IPM runs out of memory.

However  when k is large  it does not accurately capture the fact that SOSPs are optimal  thus
incurring a larger bound on the optimality gap of ASOSPs. In contrast  our bounds do show that for k
large enough  as εg  εH go to zero  the optimality gap goes to zero  with the trade-off that they do so
for a perturbed problem (though see Corollary 3.1)  with high probability.

Numerical Experiments

We present the empirical performance of the low-rank approach in the case of (PhaseCut). We
compare it with a dedicated interior-point method (IPM) implemented by Helmberg et al. [1996]
for real SDPs and adapted to phase retrieval as done by Waldspurger et al. [2015]. This adaptation
involves splitting the real and the imaginary parts of the variables in (PhaseCut) and forming an
equivalent real SDP with double the dimension. The Burer–Monteiro approach (BM) is implemented
in complex form directly using Manopt  a toolbox for optimization on manifolds [Boumal et al. 
2014]. In particular  a Riemannian Trust-Region method (RTR) is used [Absil et al.  2007]. Theory
supports that these methods can return an ASOSP in a ﬁnite number of iterations [Boumal et al. 
2018a]. We stress that the SDP is not perturbed in these experiments: the role of the perturbation
in the analysis is to understand why the low-rank approach is so successful in practice despite the
existence of pathological cases. In practice  we do not expect to encounter pathological cases.
Our numerical experiment setup is as follows. We seek to recover a signal of dimension d  z ∈ Cd 
from n measurements encoded in the vector b ∈ Rn
+ such that b = |Az| +   where A ∈ Cn×d is
the sensing matrix and  ∼ N (0  Id) is standard Gaussian noise. For the numerical experiments 
we generate the vectors z as complex random vectors with i.i.d. standard Gaussian entries  and we
randomly generate the complex sensing matrices A also with i.i.d. standard Gaussian entries. We do
so for values of d ranging from 10 to 1000  and always for n = 10d (that is  there are 10 magnitude
measurements per unknown complex coefﬁcient  which is an oversampling factor of 5.) Lastly  we
generate the measurement vectors b as described above and we cap its values from below at 0.01 in
order to avoid small (or even negative) magnitude measurements.
For n up to 3000  both methods solve the same problem  and indeed produce the same answer up to
small discrepancies. The BM approach is more accurate  at least in satisfying the constraints  and  for
n = 3000  it is also about 40 times faster than IPM. BM is run with k =
n (rounded up)  which
is expected to be generically sufﬁcient to include the global optimum of the SDP (as conﬁrmed in
practice). For larger values of n  the IPM ran into memory issues and we had to abort the process.

√

8

5 Conclusion

We considered the low-rank (or Burer–Monteiro) approach to solve equality-constrained SDPs. Our
key assumptions are that (a) the search space of the SDP is compact  and (b) the search space of its
low-rank version is smooth (the actual condition is slightly stronger). Under these assumptions  we
proved using smoothed analysis that  provided k = ˜Ω(
m) where m is the number of constraints  if
the cost matrix is perturbed randomly  with high probability  approximate second-order stationary
points of the perturbed low-rank problem map to approximately optimal solutions of the perturbed
SDP. We also related optimality gaps in the perturbed SDP to optimality gaps in the original SDP.
Finally  we applied this result to an SDP relaxation of phase retrieval (also applicable to angular
synchronization).

√

Acknowledgments

NB is partially supported by NSF award DMS-1719558.

References
E. Abbé. Community Detection and Stochastic Block Models  volume 14. Now Publishers inc.  2018.

doi:10.1561/0100000067.

P.-A. Absil  C. Baker  and K. Gallivan. Trust-region methods on Riemannian manifolds. Foundations

of Computational Mathematics  7(3):303–330  2007. doi:10.1007/s10208-005-0179-9.

P.-A. Absil  R. Mahony  and R. Sepulchre. Optimization algorithms on matrix manifolds. Princeton

University Press  2008.

A. Bandeira  N. Boumal  and A. Singer. Tightness of the maximum likelihood semideﬁnite
relaxation for angular synchronization. Mathematical Programming  163(1):145–167  2017.
doi:10.1007/s10107-016-1059-6.

A. Barvinok. Problems of distance geometry and convex properties of quadratic maps. Discrete &

Computational Geometry  13(1):189–202  1995. doi:10.1007/BF02574037.

G. Ben Arous and A. Guionnet. Wigner matrices. In Handbook on Random Matrices  pages 433–451.

Oxford University Press  2010.

R. Bhatia. Positive deﬁnite matrices. Princeton University Press  2007.

S. Bhojanapalli  N. Boumal  P. Jain  and P. Netrapalli. Smoothed analysis for low-rank solutions to
semideﬁnite programs in quadratic penalty form. In S. Bubeck  V. Perchet  and P. Rigollet  editors 
Proceedings of the 31st Conference On Learning Theory  volume 75 of Proceedings of Machine
Learning Research  pages 3243–3270. PMLR  06–09 Jul 2018. URL http://proceedings.mlr.
press/v75/bhojanapalli18a.html.

N. Boumal  B. Mishra  P.-A. Absil  and R. Sepulchre. Manopt  a Matlab toolbox for optimization
on manifolds. Journal of Machine Learning Research  15:1455–1459  2014. URL https:
//www.manopt.org.

N. Boumal  V. Voroninski  and A. Bandeira. The non-convex Burer-Monteiro approach works on
smooth semideﬁnite programs. In Advances in Neural Information Processing Systems  pages
2757–2765  2016.

N. Boumal  P.-A. Absil  and C. Cartis. Global rates of convergence for nonconvex optimization on

manifolds. IMA Journal of Numerical Analysis  2018a. doi:10.1093/imanum/drx080.

N. Boumal  V. Voroninski  and A. Bandeira. Deterministic guarantees for Burer–Monteiro factor-
izations of smooth semideﬁnite programs. To appear in Communications on Pure and Applied
Mathematics  2018b.

S. Burer and R. D. Monteiro. A nonlinear programming algorithm for solving semideﬁnite programs

via low-rank factorization. Mathematical Programming  95(2):329–357  2003.

9

S. Burer and R. D. Monteiro. Local minima and convergence in low-rank semideﬁnite programming.

Mathematical Programming  103(3):427–444  2005.

E. J. Candès and B. Recht. Exact matrix completion via convex optimization. Foundations of

Computational mathematics  9(6):717  2009.

T. Carson  D. G. Mixon  and S. Villar. Manifold optimization for k-means clustering. In 2017
International Conference on Sampling Theory and Applications (SampTA)  pages 73–77  July
2017. doi:10.1109/SAMPTA.2017.8024388.

D. Garber. Faster projection-free convex optimization over the spectrahedron. Advances in Neural

Information Processing Systems  pages 874–882  2016.

D. Garber and E. Hazan. Sublinear time algorithms for approximate semideﬁnite programming.

Mathematical Programming: Series A and B archive  158:329–361  2016.

G. H. Golub and V. Pereyra. The differentiation of pseudo-inverses and nonlinear least squares
problems whose variables separate. SIAM Journal on Numerical Analysis  10(2):413–432  1973.
doi:10.1137/0710036.

E. Hazan. Sparse approximate solutions to semideﬁnite programs. LATIN 2008: Theoretical

Informatics  pages 306–316  2008.

C. Helmberg  F. Rendl  R. J. Vanderbei  and H. Wolkowicz. An interior-point method for semideﬁnite

programming. SIAM Journal on Optimization  6(2):342–361  1996.

M. Journee  F. Bach  P.-A. Absil  and R. Sepulchre. Low-rank optimization on the cone of positive

semideﬁnite matrices. SIAM Journal on Optimization  20(5):2327–2351  2010.

B. Kulis  A. C. Surendran  and J. C. Platt. Fast low-rank semideﬁnite programming for embedding

and clustering. In Artiﬁcial Intelligence and Statistics  pages 235–242  2007.

G. R. Lanckriet  N. Cristianini  P. Bartlett  L. E. Ghaoui  and M. I. Jordan. Learning the kernel matrix

with semideﬁnite programming. Journal of Machine learning research  5(Jan):27–72  2004.

S. Laue. A hybrid algorithm for convex semideﬁnite optimization. ICML  pages 177–184  2012.
M. Laurent and S. Poljak. On the facial structure of the set of correlation matrices. SIAM Journal on

Matrix Analysis and Applications  17(3):530–547  1996. doi:10.1137/0617031.

S. Mei  T. Misiakiewicz  A. Montanari  and R. I. Oliveira. Solving sdps for synchronization and
maxcut problems via the grothendieck inequality. In Proceedings of the 30th Conference on
Learning Theory  COLT 2017  Amsterdam  The Netherlands  7-10 July 2017  pages 1476–1515 
2017. URL http://proceedings.mlr.press/v65/mei17a.html.

H. H. Nguyen. Random matrices: Overcrowding estimates for the spectrum. Journal of Functional

Analysis  275(8):2197–2224  2018. doi:10.1016/j.jfa.2018.06.010.

G. Pataki. On the rank of extreme matrices in semideﬁnite programs and the multiplic-
ity of optimal eigenvalues. Mathematics of operations research  23(2):339–358  1998.
doi:10.1287/moor.23.2.339.

P. Rigollet and J.-C. Hütter. High dimensional statistics  2017. URL http://www-math.mit.edu/

~rigollet/PDFs/RigNotes17.pdf.

A. Singer. Angular synchronization by eigenvectors and semideﬁnite programming. Applied and

computational harmonic analysis  30(1):20–36  2011.

D. Spielman and S.-H. Teng.

rithm usually takes polynomial time.
doi:10.1145/990308.990310.

Smoothed analysis of algorithms: Why the simplex algo-
Journal of the ACM  51(3):385–463  May 2004.

I. Waldspurger  A. d’Aspremont  and S. Mallat. Phase recovery  maxcut and complex semideﬁnite

programming. Mathematical Programming  149(1-2):47–81  2015.

A. Yurtsever  M. Udell  J. A. Tropp  and V. Cevher. Sketchy decisions: Convex low-rank matrix
optimization with optimal storage. Proceedings of the 20th International Conference on Artiﬁcial
Intelligence and Statistics  54:1188–1196  2017.

10

,Rishabh Iyer
Jeff Bilmes
Mohammad Saberian
Nuno Vasconcelos
Thomas Pumir
Samy Jelassi
Nicolas Boumal