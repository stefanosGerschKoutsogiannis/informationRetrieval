2019,A Family of Robust Stochastic Operators for Reinforcement Learning,We consider a new family of stochastic operators for reinforcement learning with the goal of alleviating negative effects and becoming more robust to approximation or estimation errors. Various theoretical results are established  which include showing that our family of operators preserve optimality and increase the action gap in a stochastic sense. Our empirical results illustrate the strong benefits of our robust stochastic operators  significantly outperforming the classical Bellman operator and recently proposed operators.,A Family of Robust Stochastic Operators for

Reinforcement Learning

Yingdong Lu  Mark S. Squillante  Chai Wah Wu

Yorktown Heights  NY 10598  USA

{yingdong  mss  cwwu}@us.ibm.com

Mathematical Sciences

IBM Research

Abstract

We consider a new family of stochastic operators for reinforcement learning that
seeks to alleviate negative effects and become more robust to approximation or
estimation errors. Theoretical results are established  showing that our family of
operators preserve optimality and increase the action gap in a stochastic sense.
Empirical results illustrate the strong beneﬁts of our robust stochastic operators 
signiﬁcantly outperforming the classical Bellman and recently proposed operators.

1

Introduction

Reinforcement learning has a rich history within the machine learning community to solve a wide
variety of decision making problems in environments with unknown and possibly unstructured
dynamics. Through iterative application of a convergent operator  value-based reinforcement learning
(RL) generates successive reﬁnements of an initial value function [14  22  21]. Q-learning [24] is
a particular RL technique in which the computations of value iteration consist of evaluating the
corresponding Bellman equation without a model of the environment.
While Q-learning continues to be broadly and successfully used in RL to determine the optimal actions
of an agent  the development of new Q-learning approaches that improve convergence speed  accuracy
and robustness remains of great interest. One area of particular interest concerns environments in
which there exist approximation or estimation errors. Of course  when no approximation/estimation
errors are present  then the corresponding Markov decision process (MDP) can be solved exactly
with the Bellman operator. However  in the presence of nonstationary errors – a typically encountered
example being when a discrete-state  discrete-time MDP is used to approximate a continuous-state 
continuous-time system – then the optimal state-action value function obtained through the Bellman
operator does not always describe the value of stationary policies. Hence  when the optimal state-
action value function and the suboptimal state-action value functions are reasonably close to each
other  approximation/estimation errors can cause suboptimal actions to be chosen instead of an
optimal action and thus in turn potentially causing errors in identifying truly optimal actions.
To help explain and formalize this phenomena  Farahmand [13] introduced the notion of action-gap
regularity and showed that a larger action-gap regularity implies a smaller loss in performance.
Building on action-gap regularity and its beneﬁts with respect to (w.r.t.) performance loss  Bellemare
et al. [6] considered a particular approach to having the value iteration converge to an alternative
action-value function Q associated with the same optimal action policy – i.e.  maintain properties of
optimality-preserving – while at the same time achieving a larger separation between the Q-values of
optimal actions and those of suboptimal actions – i.e.  maintain properties of action-gap increasing.
The former properties ensure optimality whereas the latter properties may assist the value-iteration
algorithm to determine the optimal actions of an agent faster  more easily  and with less errors of
mislabeling suboptimal actions. Therefore  by exploiting weaker optimality conditions than the

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Bellman equation and due to the known beneﬁts of larger action-gap regularity  this approach can
potentially lead to alternatives to the classical Bellman operator that improve the convergence speed 
accuracy and robustness of RL in environments with approximation/estimation errors.
Following this approach  Bellemare et al. [6] propose purely deterministic operator alternatives
to the classical Bellman operator and show that the proposed operators satisfy the properties of
optimality-preserving and gap-increasing. Then  after empirically demonstrating the beneﬁts of
their proposed deterministic operator alternatives  the authors raise a number of open fundamental
questions w.r.t. the possibility of weaker conditions for optimality  the statistical efﬁciency of their
proposed operators  and the possibility of ﬁnding a maximally efﬁcient operator.
At the heart of the problem is a fundamental tradeoff between the degree to which the preservation of
optimality is violated and the degree to which the action gap is increased. Although the beneﬁts of
increasing action-gap regularity are known [13]  increasing the action gap beyond a certain region
in a deterministic sense can lead to violations of optimality preservation (due to deviating too far
from Bellman optimality)  thus rendering value iterations that may not ensure convergence to optimal
solutions. Hence  any purely deterministic operator alternative is unfortunately limited in the degree
to which it can be both gap-increasing and optimality-preserving  and thus in turn limited in the
degree to which it can address the above problems of approximation/estimation errors in RL.
We therefore consider an approach based on a novel stochastic framework that can increase the action
gap well beyond such a deterministic region for individual value iterations – via a random variable
(r.v.) – while controlling in a probabilistic manner the overall value iterations – via a sequence of
r.v.s – to ensure optimality preservation in a stochastic sense. Our general approach is applicable to
arbitrary Q-value approximation schemes in which the sequence of r.v.s provides support to devalue
suboptimal actions while preserving the set of optimal policies almost surely (a.s.)  thus making it
possible to increase the action gap between the Q-values of optimal and suboptimal actions beyond
the deterministic region; this can be important in practice because of the potential advantages of
increasing the action gap when there are approximation/estimation errors. In devising a family of
operators within our framework endowed with these properties  we provide a general stochastic
approach that can address the inherent deﬁciencies of purely deterministic operator alternatives to
the classical Bellman operator and that can potentially yield greater robustness w.r.t. mislabeling
suboptimal actions in the presence of approximation/estimation errors. To the best of our knowledge 
this paper presents the ﬁrst proposal and theoretical analysis of such types of robust stochastic
operators (RSOs)  which is an approach not often seen in the RL literature and should be exploited to
a much greater extent.
The research literature contains a wide variety of studies of operator alternatives to the Bellman oper-
ator  including the -greedy method [24]  speedy Q-learning [3]  policy iteration-like Q-learning [8] 
and the Boltzmann softmax operator and its variants [2]. Each of these operator alternatives seeks to
address certain issues in RL. In this paper we complement these previous studies of operator alterna-
tives and focus on operators that seek to achieve greater robustness w.r.t. approximation/estimation
errors; in fact  our empirical studies are based on Q-learning with the -greedy method.
Our theoretical results include proving that our stochastic operators are optimality-preserving and
gap-increasing in a stochastic sense. Since the value-iteration sequence generated under our stochastic
operators is based on realizations of independent nonnegative r.v.s  our family of RSOs subsumes the
family of purely deterministic operators in [6] as a strict subset (because the realizations of r.v.s can be
ﬁxed to match that of any deterministic operators as a special case). We further prove that stochastic
and variability orderings among the sequence of random operators lead to corresponding orderings
among the action gaps. Our stochastic framework and theoretical results shed new light on the
open fundamental questions raised in [6]  which includes our family of RSOs rendering signiﬁcantly
weaker conditions for optimality and signiﬁcantly stronger statistical efﬁciency. Another important
implication of our results is that the search space for the maximally efﬁcient operator should be
an inﬁnite dimensional space of sequences of r.v.s  instead of the ﬁnite dimensional space alluded
to in [6]. Yet another important implication is that the order relationships among the sequences
of random operators w.r.t. action gaps  corresponding to our stochastic and variability ordering
results  may potentially lead to determining the best sequence of r.v.s and possibly even lead to
maximally efﬁcient operators. These theoretical results further extend our understanding of the
relationship between action-gap regularity and the effectiveness of Q-learning in environments with
approximation/estimation errors beyond the initial studies in [13  6].

2

We subsequently apply our RSOs to obtain empirical results for various problems in the OpenAI Gym
framework [10]. Using these existing codes with minor modiﬁcations  we compare the empirical
results under our family of stochastic operators against those under both the classical Bellman
operator and the consistent Bellman operator [6]. These experiments consistently show that our
RSOs outperform both of these deterministic operators. Appendix C of the supplement provides the
corresponding python code modiﬁcations used in our experiments.

2 Preliminaries

We consider a standard RL framework (see  e.g.  [7]) in which a learning agent interacts with a
stochastic environment. This interaction is modeled as a discrete-space  discrete-time discounted
MDP denoted by (X  A  P  R  γ)  where X represents the set of states  A the set of actions  P the
transition probability kernel  R the reward function mapping state-action pairs into a bounded subset
of R  and γ ∈ [0  1) the discount factor. Let Q denote the set of bounded real-valued functions over
X × A. For Q ∈ Q  deﬁne V (x) := maxa Q(x  a) and use the same deﬁnition for variants such as
ˆQ ∈ Q and ˆV (x). Let x(cid:48) always denote the next state r.v. For the current state x in which action a
is taken  i.e.  (x  a) ∈ X × A  denote by P(·|x  a) the conditional transition probability for the next
state x(cid:48) and deﬁne EP := Ex(cid:48)∼P(·|x a) to be the expectation w.r.t. P(·|x  a).
A stationary policy π(·|x) : X → A deﬁnes the distribution of control actions given the current state
x  which reduces to a deterministic policy when the conditional distribution renders a constant action
for each state x; with slight abuse of notation  we always write the policy π(x). The stationary policy
π induces a value function V π : X → R and an action-value function Qπ : X × A → R where
V π(x) := Qπ(x  π(x)) deﬁnes the expected discounted cumulative reward under policy π starting in
state x and where Qπ satisﬁes the Bellman equation

Qπ(x  a) = R(x  a) + γEPQπ(x(cid:48)  π(x(cid:48))).

(1)

TBQ(x  a) := R(x  a) + γEP max

Our goal is to determine a policy π∗ that achieves the optimal value function V ∗(x)
:=
supπ V π(x) ∀x ∈ X  which also produces the optimal action-value function Q∗(x  a) :=
supπ Qπ(x  a) ∀(x  a) ∈ X × A. The Bellman operator TB : Q → Q is deﬁned pointwise as
(2)
or equivalently TBQ(x  a) = R(x  a) + γEPV (x(cid:48)). The Bellman operator TB is known (see  e.g. 
[7]) to be a contraction mapping in supremum norm  and its unique ﬁxed point coincides with the
optimal action-value function  namely Q∗(x  a) = R(x  a) + γEP maxb∈A Q∗(x(cid:48)  b)  or equivalently
Q∗(x  a) = R(x  a) + γEPV ∗(x(cid:48)). This in turn indicates that the optimal policy π∗ can be obtained
by π∗(x) = arg maxa∈A Q∗(x  a)  ∀x ∈ X.
While the Bellman operator can exactly solve the MDP when there are no approximation/estimation
errors  the previously noted differences between optimal and suboptimal state-action value functions
in the presence of such errors can result in incorrectly identifying the optimal actions. To address these
and related nonstationary effects of approximation/estimation errors arising in practice  Bellemare et
al. [6] propose the so-called consistent Bellman operator deﬁned as

b∈A Q(x(cid:48)  b) 

b∈A Q(x(cid:48)  b) + 1{x=x(cid:48)}Q(x  a)] 

TCQ(x  a) := R(x  a) + γEP[1{x(cid:54)=x(cid:48)} max

(3)
where 1{·} denotes the indicator function. The consistent Bellman operator TC preserves a local
form of stationarity by redeﬁning the action-value function Q such that  if an action a ∈ A is taken
from the state x ∈ X and the next state x(cid:48) = x  then action a is taken again. Bellemare et al. [6]
proceed to show that the consistent Bellman operator yields the optimal policy π∗  and in particular
that TC is both optimality-preserving and gap-increasing  according to (deterministic) deﬁnitions that
they provide which are compatible with those from Farahmand [13].
The proofs of our theoretical results involve mathematical arguments and technical details that are
unique to stochastic operators and stochastic orderings  and distinct from any previous deterministic
operators. In particular  a r.v. X is stochastically greater than or equal to (≥st) a r.v. Y if P[X >
z] ≥ P[Y > z] ∀z  and a r.v. X is greater than or equal to (≥cx) a r.v. Y under a convex ordering
if and only if E[f (X)] ≥ E[f (Y )]  ∀ convex functions f. Additional technical details on these and
other probabilistic terms and results underlying our theoretical results can be found in [9  11  18].

3

3 Robust Stochastic Operators

In this section we present our stochastic framework which includes proposing a general family of
RSOs  providing precise deﬁnitions of the concepts of optimality-preserving and gap-increasing in a
stochastic sense for a sequence of random operators  and establishing that any sequence of this general
family of operators is optimality-preserving and gap-increasing. Our introduction of a new family
of operators and our shifting the focus from one deterministic operator to a sequence of stochastic
operators has signiﬁcant implications w.r.t. the open questions raised in [6]. Speciﬁcally  our results
show that the conditions for optimality are much weaker and the statistical efﬁciency of our operators
can be made much stronger  both allowing signiﬁcant degrees of freedom in ﬁnding alternatives to the
Bellman operator for different purposes and applications. Meanwhile  these important improvements
completely alter and clarify the question of ﬁnding the maximally efﬁcient operators from a ﬁnite
dimensional parameter optimization problem suggested in [6] to an optimization problem in an
inﬁnite dimensional space (of the inﬁnite sequences of r.v.s)  for which we establish that stochastic
and variability orderings among the sequence of random operators lead to corresponding orderings
among the action gaps. It is important to note that our approach can be extended to variants of the
Bellman operator such as SARSA [17]  policy evaluation [19] and ﬁtted Q-iteration [12].
For all Q0 ∈ Q  x ∈ X  a ∈ A and sequences {βk : k ∈ Z+} of independent nonnegative r.v.s with
expectation βk := Eβ[βk] between 0 and 1 inclusively for each k ∈ Z+  we deﬁne

Tβk Qk(x  a) := R(x  a) + γEP max

b∈A Qk(x(cid:48)  b) − βk(Vk(x) − Qk(x  a)) 

(4)
or equivalently Tβk Qk(x  a) := R(x  a) + γEPV (x(cid:48)) − βk(Vk(x) − Qk(x  a)). (Note that the
operator in (4) is equivalent to the Bellman operator whenever the action a is optimal or βk = 0  thus
making the difference term zero in these cases.) Then members of the general family of RSOs include
the sequence {Tβk} deﬁned over all probability distributions for each r.v. in the sequence {βk} with
βk ∈ [0  1]. (Note  in particular  that the r.v.s βk can follow a different probability distribution for
each k.) We further deﬁne T F
β to be the general family of RSOs comprising all sequences of operators
{T }  each as deﬁned in (4)  such that there exists a sequence of {βk} and  for all x ∈ X and a ∈ A 
the following inequalities hold

TBQ(x  a) − βk(Vk(x) − Qk(x  a)) ≤ T Q(x  a) ≤ TBQ(x  a).

Observe that  for any (x  a) in (4) where a is not the optimal action  we have Vk(x) > Qk(x  a)
occurring very often (i.e.  for many k)  causing Q(x  a) to (eventually) deviate more from V (x);
otherwise  for a such that Q(x  a) = V (x)  then Vk(x) > Qk(x  a) will only happen relatively rarely 
thus not affecting the end value of V (x). Since the value function V (x) does not change but the action-
value function Q(x  a) does indeed change  this can lead to a larger action gap and can potentially
render more efﬁcient ways of ultimately ﬁnding V (x) through the iterative updating of Q(x  a)  as
indicated in [13  6]. Moreover  we observe that the multiplier βk in front of Vk(x) − Qk(x  a) is
desired to be relatively large individually  but its overall efforts should not be so large as to affect
the end value of V (x). We therefore introduce a family of RSOs where βk is allowed to take on any
value as long as its average βk remains less than or equal to 1. Obviously  these conditions are strictly
weaker than those identiﬁed in [6] – theirs being purely deterministic and constrained to [0  1)  and
ours based on r.v.s βk that can take on values well outside of [0  1). Since the r.v.s βk need not be
identically distributed (with the sole requirement that βk is between 0 and 1 inclusively) and since
realizations of βk can take on values far beyond or equal to 1  the family of operators T F
β clearly
subsumes the family of previously identiﬁed deterministic operators as a special case.
For the analysis of our family of stochastic operators  we consider the following key deﬁnitions.
Deﬁnition 3.1. A sequence of random operators {Tk} for M = (X  A  P  R  γ) is optimality-
preserving in a stochastic sense if for any Q0 ∈ Q and x ∈ X  and for the sequence of r.v.s
Qk+1 := TkQk  the following properties hold: Vk(x) := maxa∈A Qk(x  a) converges a.s. to a
constant ˆV (x) as k → ∞  and for all a ∈ A  we have a.s.

(5)
Deﬁnition 3.2. A sequence of random operators {Tk} for M = (X  A  P  R  γ) is gap-increasing in
a stochastic sense if for all Q0 ∈ Q  x ∈ X and a ∈ A  the following inequality holds a.s.:

Q∗(x  a) < V ∗(x) ⇒ lim sup
k→∞

Qk(x  a) < ˆV (x).

A(x  a) := lim inf

k→∞ [Vk(x) − Qk(x  a)] ≥ V ∗(x) − Q∗(x  a).

(6)

4

β is still optimality-preserving. Moreover  the deﬁnition of T F

The property of the optimality-preserving deﬁnition essentially ensures a.s. that at least one optimal
action remains optimal and all suboptimal actions remain suboptimal  while the property of the
gap-increasing deﬁnition implies robustness when the inequality (6) is strict a.s. for at least one
(x  a) ∈ X × A. In particular  as the action gap of an operator increases while remaining optimality-
preserving  the end result can be greater robustness to approximation/estimation errors [13].
We next present one of our main theoretical results establishing that our general family of RSOs is
both optimality-preserving and gap-increasing in a stochastic sense.
Theorem 3.1. Let TB be the Bellman operator deﬁned in (2) and {Tβk} a sequence of RSOs as
deﬁned in (4). Considering the sequence of r.v.s Qk+1 := Tβk Qk on a sample path basis with
Q0 ∈ Q  the sequence of operators {Tβk} is both optimality-preserving and gap-increasing in a
stochastic sense  a.s. Furthermore  all operators in the family T F
β are optimality-preserving and
gap-increasing in a stochastic sense  a.s.
Even though the stochastic operators in T F
β are not contraction mappings and therefore do not have a
ﬁxed point (as is also true for TC [6])  Theorem 3.1 establishes that each of these stochastic operators
in T F
β and Theorem 3.1 signiﬁcantly
enlarge the set of optimality-preserving and gap-increasing operators beyond the purely deterministic
operators identiﬁed in [6]. In particular  our new sufﬁcient conditions for optimality-preserving
operators in a stochastic sense implies that signiﬁcant deviation from the Bellman operator is possible
without loss of optimality; in comparison  the deterministic operator in [6] never allows a value of βk
equal to or greater than 1. More importantly  the deﬁnition of T F
β and Theorem 3.1 imply that the
search space for maximally efﬁcient operators is an inﬁnite dimensional space of sequences of r.v.s 
instead of the ﬁnite dimensional space for maximally efﬁcient operators alluded to in [6]. To this
end and due to our stochastic framework  we now establish results on stochastic ordering properties
among the sequences of r.v.s {βk} that lead to corresponding ordering properties among the action
gaps of the random operators. These results offer key relational insights into important orderings of
different operators in T F
β   which further demonstrates the beneﬁt of our RSOs and can potentially be
exploited in searching for and attempting to ﬁnd maximally efﬁcient operators in practice.
Theorem 3.2. For all ˆQ0 = ˜Q0 = Q0 ∈ Q and for each integer k ≥ 0  suppose ˆQk+1 and ˜Qk+1
are respectively updated with two different RSOs T ˆβk
that are distinguished by ˆβk and ˜βk
ˆQk and ˜Qk+1 = T ˜βk
satisfying the stochastic ordering ˆβk ≥st
˜Qk. Then we
have that the action gaps of the two systems are stochastically ordered in the same direction  namely
ˆA(x  a) ≥st ˜A(x  a).
Theorem 3.3. For all ˆQ0 = ˜Q0 = Q0 ∈ Q and for each integer k ≥ 0  suppose ˆQk+1 and ˜Qk+1
are respectively updated with two different RSOs T ˆβk
that are distinguished by ˆβk and ˜βk
satisfying the convex ordering ˆβk ≥cx
˜Qk. Then
we have that the action gaps of the two systems are convex ordered in the same direction  namely
ˆA(x  a) ≥cx ˜A(x  a).
Theorem 3.4. For all ˆQ0 = ˜Q0 = Q0 ∈ Q and for each integer k ≥ 0  suppose ˆQk+1 and ˜Qk+1
are respectively updated with two different RSOs T ˆβk
that are distinguished by ˆβk and ˜βk
satisfying E[ ˆβk] = E[ ˜βk] and Var[ ˆβk] ≤ Var[ ˜βk]; namely ˆQk+1 = T ˆβk
˜Qk.
Then we have Var[ ˆQk+1] ≤ Var[ ˜Qk+1]. Furthermore  the action gaps of the two systems have the
following properties: E[ ˆA(x  a)] = E[ ˜A(x  a)] and Var[ ˆA(x  a)] ≤ Var[ ˜A(x  a)].

˜βk; namely ˆQk+1 = T ˆβk

˜βk; namely ˆQk+1 = T ˆβk

ˆQk and ˜Qk+1 = T ˜βk

ˆQk and ˜Qk+1 = T ˜βk

and T ˜βk

and T ˜βk

and T ˜βk

The ﬁrst two theorems conclude that  among the sequences of βk that preserve optimality  those
stochastically larger and more variable sequences can produce larger action gaps w.r.t. two standard
and important stochastic orderings. Theorem 3.4 points out that a larger variance for βk  with the
same ﬁxed mean value  leads to a larger variance for Qk(x  a) while rendering the same expectation
for the action gap and a larger variance in the action gap. We know that  in the limit  the optimal
action will maintain its state-action value function. Then  when k is sufﬁciently large  we can expect
that the state-value function Qk(x  b∗) for the optimal action b∗ in state x will be very close to the
optimal value Q∗(x  b∗). A larger variance therefore suggests the potential for a greater separation
between Qk(x  b∗) and the state-value function Qk(x  a) for sub-optimal actions a  and thus the

5

latter can be understood to have a larger action gap in the limit. Hence  sequences of βk with large
variances can be seen as a very simple instance of the stochastic ordering results.

4 Experimental Results

Within the general RL framework of interest  we consider a standard  yet generic  form for Q-learning
so as to cover the various problems empirically examined in this section. Speciﬁcally  for all Q0 ∈ Q 
x ∈ X  a ∈ A and an operator of interest T   we consider the sequence of action-value Q-functions
based on the following generic update rule:

Qk+1(x  a) = (1 − αk)Qk(x  a) + αkT Qk(x  a) 

(7)
where αk is the learning rate for iteration k. Our theoretical results study the behavior of Q(x  a)
under a general class of different operators  establishing the beneﬁts of our RSOs over previously
proposed operators. We now turn to our empirical comparisons that consist of the Bellman operator
TB  the consistent Bellman operator TC  and instances of our family of RSOs Tβk under different
distributions for the sequence of βk.
We conduct various experiments across several well-known problems using the OpenAI Gym frame-
work [10]  namely Acrobot  Mountain Car  Cart Pole and Lunar Lander. This collection of problems
spans a variety of RL examples with different characteristics  dimensions  parameters  and so on. In
each case  the state space is continuous and discretized to a ﬁnite set of states; i.e.  each dimension is
discretized into equally spaced bins where the number of bins depends on the problem to be solved
and the reference codebase used. For every problem  the speciﬁc Q-learning algorithms considered
are deﬁned as in (7) where the appropriate operator of interest TB  TC or Tβk is substituted for T ;
at each timestep  (7) is iteratively applied to the Q-function at the current state and action. The
experiments for each problem from the OpenAI Gym were run using the existing code found at [23  1]
exactly as is with the default parameter settings and the sole change consisting of the replacement
of the Bellman operator in the code with corresponding implementations of either the consistent
Bellman operator or RSO; see Appendix C of the supplement for the corresponding python code. It
is apparent from these codes that RSO can be directly and easily implemented as a replacement for
the classical Bellman operator.
We note that each of the algorithms from the OpenAI Gym implements a form of the -greedy method
(e.g.  occasionally picking a random action or using a randomly perturbed Q-function for determining
the action) to enable some form of exploration in addition to the exploitation-based search of the
optimal policy using the Q-function. Our experiments were therefore repeated over a wide range of
values for   where we found that the relative performance trends of the various operators did not
depend signiﬁcantly on the amount of exploration under the -greedy algorithm. In particular  the
same performance trends were observed over a wide range of  values and hence we present results
based on the default value of  used in the reference codebase.
Multiple experimental trials are run for each problem  where we ensured the setting of the random
starting state to be the same in each experimental trial for all of the operators considered by
initializing them with the same random seed. We observe in general across all experimental results
that for different problems and different variants of the Q-learning algorithm  simply replacing the
Bellman operator or the consistent Bellman operator with an RSO results in signiﬁcant performance
improvements. The RSOs considered in every set of experimental trials for each problem consist
of different distributions for the sequence of βk. Speciﬁcally  we empirically study the following
instances of our family of RSOs:
2 and Var[βk] = 1
1. βk sampled from a uniform distribution over [0  1)  thus E[βk] = 1
12;
2. βk sampled from a uniform distribution over [0  2)  thus E[βk] = 1 and Var[βk] = 1
3;
3. βk sampled from a uniform distribution over [0.5  1.5)  thus E[βk] = 1 and Var[βk] = 1
12;
5 plus a r.v. sampled from a Beta(2  3) distribution  thus E[βk] = 1 and Var[βk] = 1
4. βk set to 3
25;
9 plus a r.v. sampled from a Beta(2  7) distribution  thus E[βk] = 1 and Var[βk] = 7
5. βk set to 7
405;
6. βk set to a r.v. sampled from a Pareto(1  2) distribution minus 1  thus E[βk] = 1  Var[βk] = ∞;
2  thus E[βk] = 1  Var[βk] = 3
4;
7. βk set to a r.v. sampled from a Pareto(1  3) distribution minus 1
8. βk set to 0.5 and 1.5 in an alternating manner  thus having E[βk] = 1 and Var[βk] = 1
12;
9. βk set to 1  thus having E[βk] = 1 and Var[βk] = 0.

6

Observe that the ﬁrst and second RSO instances include values of βk that are equal or relatively close
to 0; since xm = 1 in the sixth instance together with the subtraction of 1  this also includes values of
βk that are equal or relatively close to 0; all other RSO instances exclude values of βk that are equal
or relatively close to 0. We note that the last RSO instance is consistent with the advantage learning
operator in [4  6]  though it is important to note that β = 1 was disallowed in [6]  unnecessarily so as
our results have established. To gain insight on the different RSO instances  the results presented in
this section focus on the simple case of operators Tβk associated with sequences of r.v.s {βk} drawn
from speciﬁc probability distributions in an independent and identically distributed manner. We
note  however  that various experiments were performed with very simple combinations of different
distributions for βk over the iterations k ∈ Z+. As a speciﬁc example  we considered βk ∼ U [0  1)
for β0  . . .   βk(cid:48) and then βk ∼ U [0  2) for βk(cid:48)+1  . . .  but these results were not considerably better 
and often worse  than those presented below for βk ∼ U [0  2).

(a) Acrobot problem (training).

(b) Mountain Car problem (training).

Figure 1: Average number of steps needed to solve minimization problems during training phase.

4.1 Acrobot

This problem is ﬁrst discussed in [20]. The state vector is 6-dimensional with three actions possible
in each state  and the score represents the number of timesteps needed to solve the problem. The
position and velocity are discretized into 8 bins whereas the other state components are discretized
into 10 bins. We ran 50 experimental trials over many episodes  with a goal of minimizing the score.
Figure 1a plots the score  averaged over moving windows of 1000 episodes across the 50 trials  as a
function of the number of episodes for a subset of operators during the training phase; the full set of
results are provided in Figure 3. We observe that the average score under the RSOs generally exhibit
much better performance than under the Bellman operator or the consistent Bellman operator  with
the βk sequences of all ones and from Beta(2  7) rendering the best performance. Table 1 presents
the average score over the last 1000 episodes across the 50 trials together with the corresponding
95% conﬁdence intervals. We observe that the conﬁdence intervals for all operators are quite small
and that the best average scores are consistent with those plotted in Figure 3.
Figure 2b presents the average score over 1000 episodes across the 50 trials for all operators during
the testing phase  together with the corresponding 95% conﬁdence intervals. We again observe that
the best average scores are obtained under many of the RSOs and that the conﬁdence intervals for all
operators are quite small. We further observe the differences in the performance orderings among the
operators in comparison with the results in Table 1  where the βk sequences from Pareto(1  2) and
alternating 0.5 and 1.5 render the best performance followed by βk sequences from U [0  1).

4.2 Mountain Car

This problem is ﬁrst discussed in [16]. The state vector is 2-dimensional with a total of three possible
actions  and the score represents the number of timesteps needed to solve the problem. The state
space is discretized into a 40× 40 grid. We ran 50 experimental trials over many episodes for training 
each of which consists of up to 200 steps and with a goal of minimizing the score.
Figure 1b plots the score  averaged over moving windows of 1000 episodes across the 50 trials  as a
function of the number of episodes for a subset of operators during the training phase; the full set

7

020000400006000080000100000episode140150160170180190200average scoreAcrobotBellmanconsistent BellmanRSO: k U[0 2)RSO: k=1.0RSO: k {0.5 1.5}RSO: k Beta(2 7)0200040006000800010000episode185.0187.5190.0192.5195.0197.5200.0mean of scoreMountain CarBellmanconsistent BellmanRSO: k U[0 2)RSO: k=1.0RSO: k {0.5  1.5}RSO: k Pareto(2)of results are provided in Figure 4. We observe that the average score under the RSOs generally
exhibit considerably better performance than under the Bellman operator or the consistent Bellman
operator  with the βk sequences from Pareto(1  2) and alternating 0.5 and 1.5 rendering the best
performance followed by βk sequences from U [0  2). Table 1 presents the average score over the
last 1000 episodes across the 50 trials together with the corresponding 95% conﬁdence intervals. We
observe that the conﬁdence intervals for all operators are quite small and that the best average scores
are consistent with those plotted in Figure 4.
Figure 2b presents the average score over 1000 episodes across the 50 trials for all operators during
the testing phase  together with the corresponding 95% conﬁdence intervals. We again observe that
the best average scores are generally obtained under the RSOs and that the conﬁdence intervals for
all operators are quite small. We further observe the differences in the average score performance
orderings among the operators in comparison with the results in Table 1  where the βk sequences
from Pareto(1  3) and U [0  2) render the best average score performance.

4.3 Cart Pole

This problem is ﬁrst discussed in [5]. The state vector is 4-dimensional with two actions possible in
each state  and the score represents the number of steps where the cart pole stays upright before either
falling over or going out of bounds. The position and velocity are discretized into 8 bins whereas
the angle and angular velocity are discretized into 10 bins. We ran 50 experimental trials over many
episodes  each of which consists of up to 200 steps with a goal of maximizing the score. The problem
is considered solved when the score exceeds 195.
Table 1 presents the average score over the last 1000 episodes across the 50 trials for all operators
during the training phase  together with the corresponding 95% conﬁdence intervals. We observe
that the best average scores are obtained under many of the RSOs  with the βk sequences of all ones
and from Beta(2  7) rendering the best performance followed by βk sequences from U [0.5  1.5). We
further observe that the conﬁdence intervals for all operators are quite small.
Table 2 presents the average score over 1000 episodes across the 50 trials for all operators during
the testing phase  together with the corresponding 95% conﬁdence intervals. We again observe that
the best average scores are obtained under many of the RSOs and that the conﬁdence intervals for
all operators are quite small. We further observe the differences in the average score performance
orderings among the operators in comparison with the results in Table 1  where the βk sequences
from U [0.5  1.5) and U [0  2) render the best average score performance.

(a) Average Lunar Lander score (training).

(b) Table of average scores (testing).

Figure 2: Average number of steps needed to solve Lunar Lander maximization problem during
training phase; Average scores for all RSO instances and three problems during testing phase.

4.4 Lunar Lander

This problem is discussed in [10]. The state vector is 8-dimensional with a total of four possible
actions  and the physics of the problem is known to be notoriously more difﬁcult than the foregoing
problems. The 6 continuous state variables are each discretized into 4 bins. The score represents
the cumulative reward comprising positive points for successful degrees of landing and negative
points for fuel usage and crashing. We ran 50 experimental trials over many episodes  each of which
consists of up to 200 steps with a goal of maximizing the score.

8

020004000600080001000012000episode220200180160140120mean of scoreLunar LanderBellmanconsistent BellmanRSO: k U[0 2)RSO: k=1.0RSO: k {0.5  1.5}RSO: k Beta(2 3)TestingScoreAcrobotMountainCarLunarLanderBellman189.1±0.17%131.2±0.23%−231.0±0.92%ConsistentBellman185.3±0.20%127.2±0.22%−185.1±0.98%βk∼U[0 2)189.5±0.16%121.2±0.21%−164.4±1.05%βk∼U[0 1)184.9±0.18%126.9±0.23%−207.0±0.94%βk=1.0189.2±0.18%121.9±0.21%−157.8±1.10%βk∈{0.5 1.5}181.3±0.23%122.3±0.20%−174.0±1.01%βk∼U[0.5 1.5)192.4±0.13%122.8±0.21%−168.1±1.08%βk∼Beta(2 3)185.0±0.20%122.6±0.21%-163.5±1.13%βk∼Beta(2 7)186.2±0.19%122.3±0.21%−164.8±1.06%βk∼Pareto(2)180.7±0.37%125.0±0.20%−216.9±0.94%βk∼Pareto(3)186.6±0.21%121.1±0.21%−166.2±1.04%1Figure 2a plots the score  averaged over moving windows of 1000 episodes across the 50 trials  as a
function of the number of episodes for a subset of operators during the training phase; the full set of
results are provided in Figure 5. We observe that the average score under the RSOs generally exhibit
better performance than under the Bellman operator or the consistent Bellman operator  with the
βk sequences from Beta(2  3) and of all ones rendering the best performance. Table 1 presents the
average score over the last 1000 episodes across the 50 trials together with the corresponding 95%
conﬁdence intervals. We observe that the conﬁdence intervals for all operators are quite small and
that the best average scores are consistent with those plotted in Figure 5.
Figure 2b presents the average score over 1000 episodes across the 50 trials for all operators during
the testing phase  together with the corresponding 95% conﬁdence intervals. We again observe that
the best average scores are generally obtained under the RSOs and that the conﬁdence intervals for
all operators are quite small. We further observe some consistencies in the performance orderings
among the operators in comparison with the results in Table 1  where the βk sequences of all ones
and from Beta(2  3) render the best performance followed by βk sequences from U [0  2).

5 Conclusions and Discussion

Building on the work of Farahmand [13] and Bellemare et al. [6]  who argue that increasing the
action gap while preserving optimality can improve the performance of value-iteration algorithms
in environments with approximation or estimation errors  we propose and analyze a new general
family of RSOs for RL that subsumes as a strict subset the classical Bellman operator and other
purely deterministic operators proposed in the literature. Our theoretical results include proving that
our stochastic operators are optimality-preserving and gap-increasing in a stochastic sense and that
stochastic and variability orderings among the sequence of random operators lead to corresponding
orderings among the action gaps. In addition  our stochastic framework and theoretical results shed
new light on and help to resolve the open fundamental questions raised in [6] related to the possibility
of weaker optimality conditions  the statistical efﬁciency of proposed deterministic operators  and the
possibility of ﬁnding maximally efﬁcient operators. Speciﬁcally  our theoretical results show that the
conditions for optimality are much weaker and the statistical efﬁciency of our stochastic operators
can be made much stronger  both allowing signiﬁcant degrees of freedom in ﬁnding alternatives to the
Bellman operator for different purposes and applications. Meanwhile  these important improvements
completely alter and clarify the question of ﬁnding the maximally efﬁcient operators from a ﬁnite
dimensional parameter optimization problem suggested in [6] to an optimization problem in an
inﬁnite dimensional space (of the inﬁnite sequences of r.v.s)  for which our established stochastic and
variability orderings among sequences of random operators can potentially assist in searching for
maximally efﬁcient operators in practice. Our family of RSOs represents a stochastic approach not
often seen in the RL literature that should be exploited to a much greater extent.
A collection of empirical results – based on well-known problems within the OpenAI Gym frame-
work spanning various RL examples with diverse characteristics – support our theoretical results 
consistently demonstrating and quantifying the signiﬁcant performance improvements obtained with
our RSOs over existing operators. We note that  while the focus of our empirical results has been on
Q-learning  our family of RSOs are applicable to other RL approaches such as DQN [15].
It is important to highlight a few fundamental tradeoffs in identifying maximally efﬁcient operators for
different RL problems  based on our theoretical and empirical results. On the one hand  when sampled
values of βk are relatively small  then it is possible for the small offset by βk(Vk(x) − Qk(x  a))
on truly suboptimal actions a to have limited or no effect on the separation between optimal and
suboptimal actions. On the other hand  when sampled values of βk are relatively large  then it is
possible for the large offset of βk(Vk(x) − Qk(x  a)) to be applied against the truly optimal action
a∗ due to approximation or estimation errors. In addition  the level of impact of these and related
factors associated with the sequence of r.v.s {βk} can vary over the value iterations moving from
k = 0 to the limit as k → ∞. We view the problem of ﬁnding maximally efﬁcient operators for RL
problems as identifying sequences of random operators that address these fundamental tradeoffs in
order to maximize action-gap regularity for the suboptimal actions of each state. Our theoretical
and empirical results further raise a related fundamental issue that concerns whether maximizing the
action gap is sufﬁcient to improve the performance of value-iteration algorithms in environments
with approximation or estimation errors.

9

References
[1] M. Alzantot.

https://gist.github.com/malzantot /9d1d3fa4fdc4a101bc48a135d8f9a289  2017.

Solution of mountaincar OpenAI Gym problem using Q-learning.

[2] K. Asadi and M. L. Littman. An alternative softmax operator for reinforcement learning. In

Proc. 34th International Conference on Machine Learning  2017.

[3] M. Azar  R. Munos  M. Gavamzadeh  and H. Kappen. Speedy Q-learning. Advances in Neural

Information Processing Systems  24  2011.

[4] L. Baird. Reinforcement Learning through Gradient Descent. PhD thesis  Carnegie Mellon

University  Pittsburgh  PA  U.S.A.  1999.

[5] A. G. Barto  R. S. Sutton  and C. W. Anderson. Neuronlike adaptive elements that can solve
difﬁcult learning control problems. IEEE Transactions on Systems  Man  and Cybernetics 
SMC-13(5):834–846  Sept. 1983.

[6] M. G. Bellemare  G. Ostrovski  A. Guez  P. S. Thomas  and R. Munos. Increasing the action
gap: New operators for reinforcement learning. In Proc. Thirtieth AAAI Conference on Artiﬁcial
Intelligence  AAAI’16  pages 1476–1483. AAAI Press  2016.

[7] D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc  1996.

[8] D. Bertsekas and H. Yu. Q-learning and enhanced policy iteration in discounted dynamic

programming. Mathematics of Operations Research  37  2012.

[9] P. Billingsley. Convergence of Probability Measures. Wiley  New York  Second edition  1999.

[10] G. Brockman  V. Cheung  L. Pettersson  J. Schneider  J. Schulman  J. Tang  and W. Zaremba.

OpenAI Gym. CoRR  abs/1606.01540  2016.

[11] Y. S. Chow and H. Teicher. Probability Theory: Independence  Interchangeability  Martingales.

Springer  3rd edition  2003.

[12] D. Ernst  P. Geurts  and L. Wehenkel. Tree-based batch mode reinforcement learning. Journal

of Machine Learning Research  6:503–556  2005.

[13] A. Farahmand. Action-gap phenomenon in reinforcement learning. Advances in Neural

Information Processing Systems  24  2011.

[14] L. Kaelbling  M. Littman  and A. Moore. Reinforcement learning: A survey. Journal of

Artiﬁcial Intelligence Research  4:237–285  1996.

[15] V. Mnih  K. Kavukcuoglu  D. Silver  A. Graves  I. Antonoglou  D. Wierstra  and M. Riedmiller.
Playing Atari with deep reinforcement learning. In Proc. NIPS Deep Learning Workshop  2013.

[16] A. Moore. Efﬁcient Memory-Based Learning for Robot Control. PhD thesis  University of

Cambridge  Cambridge  U.K.  1990.

[17] G. Rummery and M. Niranjan. On-line Q-learning using connectionist systems. Technical

report  Cambridge University  1994.

[18] M. Shaked and J. Shanthikumar. Stochastic Orders. Springer Series in Statistics. Springer New

York  2007.

[19] R. Sutton. Learning to predict by the methods of temporal differences. Machine Learning 

3(1):9–44  1988.

[20] R. S. Sutton. Generalization in Reinforcement Learning: Successful Examples Using Sparse

Coarse Coding. Advances in Neural Information Processing Systems  8:1038–1044  1996.

[21] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press  2011.

[22] C. Szepesvari. Algorithms for reinforcement learning. In Synthesis Lectures on Artiﬁcial

Intelligence and Machine Learning  volume 4.1  pages 1–103. Morgan & Claypool  2010.

10

[23] V. M. Vilches.

Basic reinforcement

Gym.
/README.md  May 2016.

https://github.com/vmayoral

learning tutorial 4: Q-learning in OpenAI
/basic_reinforcement_learning/blob/master/tutorial4

[24] C. Watkins. Learning from Delayed Rewards. PhD thesis  University of Cambridge  Cambridge 

U.K.  1989.

11

,Yingdong Lu
Mark Squillante
Chai Wah Wu