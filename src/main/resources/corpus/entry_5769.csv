2009,Unsupervised Feature Selection for the $k$-means Clustering Problem,We present a novel feature selection algorithm for the $k$-means clustering problem. Our algorithm is randomized and  assuming an accuracy parameter $\epsilon \in (0 1)$  selects and appropriately rescales in an unsupervised manner $\Theta(k \log(k / \epsilon) / \epsilon^2)$ features from a dataset of arbitrary dimensions. We prove that  if we run any $\gamma$-approximate $k$-means algorithm ($\gamma \geq 1$) on the features selected using our method  we can find a $(1+(1+\epsilon)\gamma)$-approximate partition with high probability.,Unsupervised Feature Selection for the

k-means Clustering Problem

Christos Boutsidis

Department of Computer Science
Rensselaer Polytechnic Institute

Troy  NY 12180

Michael W. Mahoney

Department of Mathematics

Stanford University
Stanford  CA 94305

boutsc@cs.rpi.edu

mmahoney@cs.stanford.edu

Petros Drineas

Department of Computer Science
Rensselaer Polytechnic Institute

Troy  NY 12180

drinep@cs.rpi.edu

Abstract

We present a novel feature selection algorithm for the k-means clustering problem.
Our algorithm is randomized and  assuming an accuracy parameter ϵ ∈ (0  1) 
selects and appropriately rescales in an unsupervised manner Θ(k log(k/ϵ)/ϵ2)
features from a dataset of arbitrary dimensions. We prove that  if we run any
γ-approximate k-means algorithm (γ ≥ 1) on the features selected using our
method  we can ﬁnd a (1 + (1 + ϵ)γ)-approximate partition with high probability.

1 Introduction

Clustering is ubiquitous in science and engineering  with numerous and diverse application domains 
ranging from bioinformatics and medicine to the social sciences and the web [15]. Perhaps the most
well-known clustering algorithm is the so-called “k-means” algorithm or Lloyd’s method [22]  an
iterative expectation-maximization type approach  which attempts to address the following objec-
tive: given a set of points in a Euclidean space and a positive integer k (the number of clusters)  split
the points into k clusters so that the total sum of the (squared Euclidean) distances of each point to
its nearest cluster center is minimized. This optimization objective is often called the k-means clus-
tering objective. (See Deﬁnition 1 for a formal discussion of the k-means objective.) The simplicity
of the objective  as well as the good behavior of the associated algorithm (Lloyd’s method [22  28]) 
have made k-means enormously popular in applications [32].
In recent years  the high dimensionality of the modern massive datasets has provided a considerable
challenge to k-means clustering approaches. First  the curse of dimensionality can make algorithms
for k-means clustering very slow  and  second  the existence of many irrelevant features may not
allow the identiﬁcation of the relevant underlying structure in the data [14]. Practitioners addressed
such obstacles by introducing feature selection and feature extraction techniques. It is worth not-
ing that feature selection selects a small subset of actual features from the data and then runs the
clustering algorithm only on the selected features  whereas feature extraction constructs a small set
of artiﬁcial features and then runs the clustering algorithm on the constructed features. Despite the
signiﬁcance of the problem  as well as the wealth of heuristic methods addressing it (see Section
3)  there exist no provably accurate feature selection methods and extremely few provably accurate
feature extraction methods for the k-means clustering objective (see Section 3.1 for the later case).

1

Our work here addresses this shortcoming by presenting the ﬁrst provably accurate feature selection
algorithm for k-means clustering. Our algorithm constructs a probability distribution for the feature
space  and then selects a small number of features (roughly k log(k)  where k is the number of
clusters) with respect to the computed probabilities. (See Section 2 for a detailed description of
our algorithm.) Then  we argue that running k-means clustering algorithms on the selected features
returns a constant-factor approximate partition to the optimal. (See Theorem 1 in Section 2.)
We now formally deﬁne the k-means clustering problem using the so-called cluster indicator matrix.
Also  recall that the Frobenius norm of a matrix (denoted by ∥·∥
F ) is equal to the square root of the
sum of the squares of its elements. (See also Section 4.1 for useful notation.)

Deﬁnition 1 [THE K-MEANS CLUSTERING PROBLEM]
Given a matrix A ∈ Rn×d (representing n points – rows – described with respect to d features –
columns) and a positive integer k denoting the number of clusters  ﬁnd the n × k indicator matrix
Xopt such that

(cid:13)(cid:13)A − XX T A
(cid:13)(cid:13)2
(cid:13)(cid:13)A − XoptX T
(cid:13)(cid:13)2

F .

F =

optA

(1)

(2)

.

(cid:13)(cid:13)2

F

The optimal value of the k-means clustering objective is

Xopt = arg min
X∈X

(cid:13)(cid:13)A − XX T A

Fopt = min
X∈X

In the above X denotes the set of all n × k indicator matrices X.
We brieﬂy expand on the notion of an n × k indicator matrix X. Such matrices have exactly one
non-zero element per row  which denotes cluster membership. Equivalently  for all i = 1  . . .   n and
√
j = 1  . . .   k  the i-th row (point) of A belongs to the j-th cluster if and only if Xij is non-zero;
sj  where sj is the number of points in the corresponding cluster (i.e. the
in particular Xij = 1/
number of non-zero elements in the j-th column of X). Note that the columns of X are normalized
and pairwise orthogonal so that their Euclidean norm is equal to one  and X T X = Ik  where Ik
is the k × k identity matrix. An example of such an indicator matrix X representing three points
(rows in X) belonging to two different clusters (columns in X) is given below; note that the points
corresponding to the ﬁrst two rows of X belong to the ﬁrst cluster (s1 = 2) and the other point to
the second cluster (s2 = 1):

 1/

√
√
1/
0

2
2

 .
(cid:13)(cid:13)A − XX T A
(cid:13)(cid:13)2

0
√
0
1/

1

X =

∑

The above deﬁnition of the k-means objective is exactly equivalent with the standard deﬁnition of
||A(i) − X(i)X T A||2
2 
k-means clustering [28]. To see this notice that
while for i = 1  ...  n  X(i)X T A denotes the centroid of the cluster the i-th point belongs to. In the
above  A(i) and X(i) denote the i-th rows of A and X  respectively.

F =

n
i=1

2 The feature selection algorithm and the quality-of-clustering results
Algorithm 1 takes as inputs the matrix A ∈ Rn×d  the number of clusters k  and an accuracy
parameter ϵ ∈ (0  1). It ﬁrst computes the top-k right singular vectors of A (columns of Vk ∈ Rd×k).
Using these vectors  it computes the so-called (normalized) leverage scores [4  24]; for i = 1  ...  d
the i-th leverage score equals the square of the Euclidian norm of the i-th row of Vk (denoted by
(Vk)(i)). The i-th leverage score characterizes the importance of the i-th feature with respect to the
′
k-means objective. Notice that these scores (see the deﬁnition of p
is in step 2 of Algorithm 1) form
a probability distribution over the columns of A since
i=1 pi = 1. Then  the algorithm chooses
a sampling parameter r that is equal to the number of (rescaled) features that we want to select.
In order to prove our theoretical bounds  r should be ﬁxed to r = Θ(k log(k/ϵ)/ϵ2) at this step
(see section 4.4). In practice though  a small value of r  for example r = 10k  seems suﬁcient (see
section 5). Having r ﬁxed  Algorithm 1 performs r i.i.d random trials where in each trial one column
of A is selected by the following random process: we throw a biased die with d faces with each face
corresponding to a column of A  where for i = 1  ...  d the i-th face occurs with probability pi. We
select the column of A that corresponds to the face we threw in the current trial. Finally  note that the
running time of Algorithm 1 is dominated by the time required to compute the top-k right singular
vectors of the matrix A  which is at most O

(
min{nd2  n2d})

∑

n

.

2

Input: n × d matrix A (n points  d features)  number of clusters k  parameter ϵ ∈ (0  1).

1. Compute the top-k right singular vectors of A  denoted by Vk ∈ Rd×k.
2. Compute the (normalized) leverage scores pi  for i = 1  . . .   d 

(cid:13)(cid:13)(cid:13)(Vk)(i)

(cid:13)(cid:13)(cid:13)2

2

pi =

/k.

3. Fix a sampling parameter r = Θ(k log(k/ϵ)/ϵ2).
4. For t = 1  . . .   r i.i.d random trials:

• keep the i-th feature with probability pi and multiply it by the factor (rpi)

5. Return the n × r matrix ˜A containing the selected (rescaled) features.

−1/2.

Output: n × r matrix ˜A  with r = Θ(k log(k/ϵ)/ϵ2).

Algorithm 1: A randomized feature selection algorithm for the k-means clustering problem.

In order to theoretically evaluate the accuracy of our feature selection algorithm  and provide some
a priori guarantees regarding the quality of the clustering after feature selection is performed  we
chose to report results on the optimal value of the k-means clustering objective (the Fopt of Deﬁ-
nition 1). This metric of accuracy has been extensively used in the Theoretical Computer Science
community in order to analyze approximation algorithms for the k-means clustering problem. In
particular  existing constant factor or relative error approximation algorithms for k-means (see  for
example  [21  1] and references therein) invariably approximate Fopt.
Obviously  Algorithm 1 does not return a partition of the rows of A. In a practical setting  it would
be employed as a preprocessing step. Then  an approximation algorithm for the k-means clustering
problem would be applied on ˜A in order to determine the partition of the rows of A. In order to
formalize our discussion  we borrow a deﬁnition from the approximation algorithms literature.

Deﬁnition 2 [K-MEANS APPROXIMATION ALGORITHM]
An algorithm is a “γ-approximation” for the k-means clustering problem (γ ≥ 1) if it takes inputs
A and k  and returns an indicator matrix Xγ that satisﬁes with probability at least 1 − δγ 

(cid:13)(cid:13)A − XγX T

γ A

(cid:13)(cid:13)2

F

≤ γ min
X∈X

(cid:13)(cid:13)A − XX T A
(cid:13)(cid:13)2

F .

(3)

In the above δγ ∈ [0  1) is the failure probability of the algorithm.
Clearly  when γ = 1  then Xγ is the optimal partition  which is a well-known NP-hard objective. If
we allow γ > 1  then many approximation algorithms exist in the literature. For example  the work
of [21]  achieves γ = 1 + ϵ  for some ϵ ∈ (0  1] in time linear on the size of the input. Similarly 
the k-means++ method of [1] achieves γ = O(log(k)) using the popular Lloyd’s algorithm and a
sophisticated randomized seeding. Theorem 1 (see Section 4 for its proof) is our main quality-of-
approximation result for our feature selection algorithm.
Theorem 1 Let the n×d matrix A and the positive integer k be the inputs of the k-means clustering
problem. Let ϵ ∈ (0  1)  and run Algorithm 1 with inputs A  k  and ϵ in order to construct the n × r
matrix ˜A containing the selected features  where r = Θ(k log(k/ϵ)/ϵ2).
If we run any γ-approximation algorithm (γ ≥ 1) for the k-means clustering problem  whose fail-
ure probability is δγ  on inputs ˜A and k  the resulting cluster indicator matrix X~γ satisﬁes with
probability at least 0.5 − δγ 

(4)

(cid:13)(cid:13)A − X~γX T

~γ A

(cid:13)(cid:13)2

F

≤ (1 + (1 + ϵ)γ) min
X∈X

(cid:13)(cid:13)A − XX T A

(cid:13)(cid:13)2

F .

The failure probability of the above theorem can be easily reduced using standard boosting methods.

3

3 Related work

Feature selection has received considerable attention in the machine learning and data mining com-
munities. A large number of different techniques appeared in prior work  addressing the feature
selection within the context of both clustering and classiﬁcation. Surveys include [13]  as well
as [14]  which reports the results of the NIPS 2003 challenge in feature selection. Popular feature
selection techniques include the Laplacian scores [16]  the Fisher scores [9]  or the constraint scores
[33]. In this section  we opt to discuss only a family of feature selection methods that are closely
related to the leverage scores of our algorithm. To the best of our knowledge  all previous feature
selection methods come with no theoretical guarantees of the form that we describe here.
Given as input an n× d object-feature matrix A and a positive integer k  feature selection for Princi-
pal Components Analysis (PCA) corresponds to the task of identifying a subset of k columns from
A that capture essentially the same information as do the top k principal components of A. Jol-
liffe [18] surveys various methods for the above task. Four of them (called B1  B2  B3  and B4
in [18]) employ the Singular Value Decomposition of A in order to identify columns that are some-
how correlated with its top k left singular vectors. In particular  B3 employs exactly the leverage
scores in order to greedily select the k columns corresponding to the highest scores; no theoretical
results are reported. An experimental evaluation of the methods of [18] on real datasets appeared
in [19]. Another approach employing the matrix of the top k right singular vectors of A and a
Procrustes-type criterion appeared in [20]. From an applications perspective 
[30] employed the
methods of [18] and [20] for gene selection in microarray data analysis. From a complementary
viewpoint  feature selection for clustering seeks to identify those features that have the most dis-
criminative power among the set of all features. Continuing the aforementioned line of research 
many recent papers present methods that somehow employ the SVD of the input matrix in order
to select discriminative features; see  for example  [23  5  25  26]. Finally  note that employing
the leverage scores in a randomized manner similar to Algorithm 1 has already been proven to be
accurate for least-squares regression [8] and PCA [7  2].

3.1 Connections with the SVD

≤ (cid:13)(cid:13)A − XoptX T

(cid:13)(cid:13)2

A well-known property connects the SVD of a matrix and k-means clustering. Recall Deﬁnition
optA is a matrix of rank at most k. From the SVD optimality [11]  we
1  and notice that XoptX T
immediately get that (see section 4.1 for useful notation)

∥Aρ−k∥2

F = ∥A − Ak∥2

F

F

optA

= Fopt.

(5)
A more interesting connection between the SVD and k-means appeared in [6]. If the n × d matrix
A is projected on the subspace spanned by its top k left singular vectors  then the resulting n × k
matrix ˆA = UkΣk corresponds to a mapping of the original d-dimensional space to the optimal
k-dimensional space. This process is equivalent to feature extraction: the top k left singular vectors
(the columns of Uk) correspond to the constructed features (Σk is a simple rescaling operator).
Prior to the work of [6]  it was empirically known that running k-means clustering algorithms on
the low-dimensional matrix ˆA was a viable alternative to clustering the high-dimensional matrix A.
The work of [6] formally argued that if we let the cluster indicator matrix ˆXopt denote the optimal
k-means partition on ˆA  i.e. 

then using this partition on the rows of the original matrix A is a 2-approximation to the optimal
partition  a.k.a. 

The above result is the starting point of our work here. Indeed  we seek to replace the k artiﬁcial
features that are extracted via the SVD with a small number (albeit slightly larger than k) of actual
features. On the positive side  an obvious advantage of feature selection vs. feature extraction is
the immediate interpretability of the former. On the negative side  our approximation accuracy is
slightly worse (2 + ϵ  see Theorem 1 with γ = 1) and we need slightly more than k features.

4

(cid:13)(cid:13)(cid:13) ˆA − XX T ˆA
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)A − XX T A
(cid:13)(cid:13)2

F

 

≤ 2 min
X∈X

F .

ˆXopt = arg min
X∈X

(cid:13)(cid:13)(cid:13)A − ˆXopt ˆX T

optA

(cid:13)(cid:13)(cid:13)2

F

(6)

(7)

4 The proof of Theorem 1

This section gives the proof of Theorem 1. We start by introducing useful notation; then  we present
a preliminary lemma and the proof itself.

F and ∥A∥

4.1 Notation
Given an n × d matrix A  let Uk ∈ Rn×k (resp. Vk ∈ Rd×k) be the matrix of the top k left (resp.
right) singular vectors of A  and let Σk ∈ Rk×k be a diagonal matrix containing the top k singular
values of A. If we let ρ be the rank of A  then Aρ−k is equal to A − Ak  with Ak = UkΣkV T
k .
∥A∥
2 denote the Frobenius and the spectral norm of a matrix A  respectively. A+
denotes the pseudo-inverse of A and ||A+||2 = σmax(A+) = 1/σmin(A)  where σmax(X) and
σmin(X) denote the largest and the smallest non-zero singular values of a matrix X  respectively.
A useful property of matrix norms is that for any two matrices X and Y   ∥XY ∥
∥Y ∥
and ∥XY ∥
2
F ; this is a stronger version of the standard submultiplicavity property
for matrix norms. We call P a projector matrix if it is square and P 2 = P . We use E[y] to take the
expectation of a random variable y and Pr[e] to take the probability of a random event e. Finally  we
abbreviate “independent identically distributed” to “i.i.d” and “with probability” to “w.p”.

≤ ∥X∥

∥Y ∥

F

2

≤ ∥X∥

F

F

4.2 Sampling and rescaling matrices

We introduce a simple matrix formalism in order to conveniently represent the sampling and rescal-
ing processes of Algorithm 1. Let S be a d × r sampling matrix that is constructed as follows: S
is initially empty. For all t = 1  . . .   r  in turn  if the i-th feature of A is selected by the random
sampling process described in Algorithm 1  then ei (a column vector of all-zeros  except for its i-th
entry which is set to one) is appended to S. Also  let D be a r × r diagonal rescaling matrix con-
√
structed as follows: D is initially an all-zeros matrix. For all t = 1  . . .   r  in turn  if the i-th feature
rpi. Thus  by using the notation of
of A is selected  then the next diagonal entry of D is set to 1/
this paragraph  Algorithm 1 outputs the matrix ˜A = ASD ∈ Rn×r.

4.3 A preliminary lemma and sufﬁcient conditions

k SD||2  ||(V T

k SD  respectively. This also implies that V T

Lemma 1 presented below gives upper and lower bounds for the largest and the smallest singular
values of the matrix V T
k SD has full rank. Finally  it
argues that the matrix ASD can be used to provide a very accurate approximation to the matrix Ak.
Lemma 1 provides four sufﬁcient conditions for designing provably accurate feature selection al-
gorithms for k-means clustering. To see this notice that  in the proof of eqn. (4) given below  the
results of Lemma 1 are sufﬁcient to prove our main theorem; the rest of the arguments apply to all
sampling and rescaling matrices S and D. Any feature selection algorithm  i.e. any sampling matrix
S and rescaling matrix D  that satisfy bounds similar to those of Lemma 1  can be employed to
design a provably accurate feature selection algorithm for k-means clustering. The quality of such
an approximation will be proportional to the tightness of the bounds of the three terms of Lemma
1 (||V T
k SD)+||2  and ||E||F ). Where no rescaling is allowed in the selected features 
the bottleneck in the approximation accuracy of a feature selection algorithm would be to ﬁnd a
sampling matrix S such that only ||(V T
k S)+||2 is bounded from above. To see this notice that  in
k S||2 ≤ 1  and (after applying the submultiplicavity property of Section
Lemma 1  for any S  ||V T
4.1 in eqn. 13) ||E||F ≤ ||(V T
It is worth emphasizing that the same factor
||(V T
k S)+||2 appeared to be the bottleneck in the design of provably accurate column-based low-
rank approximations (see  for example  Theorem 1.5 in [17] and eqn. (3.19) in [12]). It is evident
from the above observations that other column sampling methods (see  for example  [17  3  2] and
references therein)  satisfying similar bounds to those of Lemma 1  immediately suggest themselves
for the design of provably accurate feature selection algorithms for k-means clustering. Finally 
equations (101) and (102) of Lemma 4.4 in [31] suggest that a sub-sampled randomized Fourier
transform can be used for the design of a provably accurate feature extraction algorithm for k-means
clustering  since they provide bounds similar to those of Lemma 1 by replacing the matrices S and
D of our algorithm with a sub-sampled randomized Fourier transform matrix (see the matrix R of
eqn. (6) in [31]).

k S)+||2||A − Ak||.

5

Lemma 1 Assume that the sampling matrix S and the rescaling matrix D are constructed using
Algorithm 1 (see also Section 4.2) with inputs A  k  and ϵ ∈ (0  1). Let co and c1 be absolute
constants that will be speciﬁed later. If the sampling parameter r of Algorithm 1 satisﬁes

then all four statements below hold together with probability at least 0.5:

ok log(c1c2

ok/ϵ2)/ϵ2 

(cid:13)(cid:13)

(cid:13)(cid:13)

(cid:13)(cid:13)V T
(cid:13)(cid:13)(V T

1.

2.

3. V T

k SD

2 = σmax(V T

r ≥ 2c1c2
k SD) ≤ √
k SD) ≤√

1 + λ.

F

ok/ϵ2)) +

√

k log(r)/r.

∑
∑

2

k log(r)/r 

√

pi)(V T

(cid:13)(cid:13)V T

2 =

V T
k SD

i

(V T

k )(i) 1√

pi

(V T

4. Ak = (ASD)(V T

To simplify notation  we set λ = ϵ

k SDDST Vk − Ik

k SDDST Vk − Ik

≤ µ∥A − Ak∥
F .
o log(c1c2
6/(2c1c2

k SD)+

2 = 1/σmin(V T

1/(1 − λ.
k SD is a full rank matrix  i.e. rank(V T
k SD) = k.
√
√
k + E  with ∥E∥
36/c1 and µ = ϵ

k SD)+V T

for a sufﬁciently large (unspeciﬁed in [29]) constant co. Standard matrix perturbation theory re-
sults [11] imply that for i = 1  ...  k

(cid:13)(cid:13)V T
(cid:13)(cid:13)Ak − ASD(V T

√
6λ2/(1 − λ).
Proof: First  we will apply Theorem 3.1 of [29] for an appropriate random vector y. Toward that end 
for i = 1  ...  d  the i-th column of the matrix V T
k )(i). We deﬁne the random vector
k is denoted by (V T
y ∈ Rk as follows: for i = 1  ...  d Pr[y = yi] = pi  where yi = (1/
k )(i) is a realization of
y. This deﬁnition of y and the deﬁnition of the sampling and rescaling matrices S and D imply that
k. Note
k SDDST Vk = 1
V T
k Vk = Ik. Obviously  ||E[yyT ]||2 =
r
also that E[yyT ] =
1. Our choice of r allows us to apply Theorem 3.1 of [29]  which  combined with the Markov’s
2 implies that w.p at least 1 − 1/6 
inequality on the random variable z =

k )(i)||2/k implies that ||y||2 ≤ √
i . Our choice of pi = ||(V T
d
i=1 yiyT
(cid:13)(cid:13)
(cid:13)(cid:13)V T
1√
d
k )(i))T = V T
i=1 pi
√
(cid:13)(cid:13)
pi
k SDDST Vk − Ik
≤ 6c0
(cid:12)(cid:12) ≤ 6co
(cid:12)(cid:12)σ2
(cid:13)(cid:13)
) − 1
(
Our choice of r and simple algebra sufﬁces to show that log(r)/r ≤ ϵ2/(c1c2
ok)  which implies that
the ﬁrst two statements of the Lemma hold w.p at least 1 − 5/6. To prove the third statement  we
(cid:13)(cid:13)Ak − AkSD(V T
k SD is positive. Our choice of ϵ ∈ (0  1) and
only need to show that the k-th singular value of V T
≤ (cid:13)(cid:13)Ak − AkSD(V T
k SD) > 0. To prove the fourth statement:
the second condition of the Lemma imply that σk(V T
{z
|
k SD)+V T
k
(cid:13)(cid:13)Ak − UkΣkV T
(cid:13)(cid:13)
(cid:13)(cid:13)Ak − UkΣkIkV T
(cid:13)(cid:13)Uρ−kΣρ−kV T
≤ (cid:13)(cid:13)Σρ−kV T
√
(cid:13)(cid:13)
≤ (ϵ

(10)
(11)
(11) we set
k SD is a rank-k matrix w.p 1 − 5/6. The second term of

(12)
ρ−kSD(V T
(13)
k SD)+
In the above  in eqn. (12) we replaced Aρ−k by Uρ−kΣρ−kV T
k can
be dropped without increasing a unitarily invariant norm such as the Frobenius matrix norm. If the
ﬁrst three statements of the lemma hold w.p at least 1 − 5/6  then w.p at least 1 − 1/3 

ρ−k  and in eqn. (13) Uρ−k and V T

In the above  in eqn. (8) we replaced A by Ak +Aρ−k  and in eqn. (9) we used the triangle inequality.
The ﬁrst term of eqn. (9) is bounded by

(cid:13)(cid:13)Aρ−kSD(V T
{z
|

− Aρ−kSD(V T
}
(cid:13)(cid:13)

√
6λ2/(1 − λ))∥A − Ak∥

In the above 
(V T
eqn. (9) is bounded by

k SD)(V T

F .
(The proof of this last argument is omitted from this extended abstract.) Finally  notice that the ﬁrst
three statements have the same failure probability 1/6 and the fourth statement fails w.p 1/3; the
union bound implies that all four statements hold together with probability at least 0.5.
⋄

ok/ϵ2)) +

o log(c1c2

6/(2c1c2

(10) we replaced Ak by UkΣkV T

(cid:13)(cid:13)Σρ−kV T

k SD)+ = Ik  since V T

k SD)+V T
k
k SD)+V T
k

k SD(V T

k SD)+V T
k

F
k SD)+V T
k

k   and in eqn.

k SD)+V T
k

F =

ρ−kSD(V T

k SD)+

k SD)+V T
k

F

(cid:13)(cid:13)

.

F

(cid:13)(cid:13)

+

F

θ1 =

=

ρ−kSD(V T

F

(cid:13)(cid:13)

θ1

θ2

k

F = 0.

θ2 =

F

in eqn.

(cid:13)(cid:13)

(cid:13)(cid:13)

(cid:13)(cid:13)

(8)
.(9)
F

}

6

4.4 The proof of eqn. (4) of Theorem 1

We assume that Algorithm 1 ﬁxes r to the value speciﬁed in Lemma 1; note that this does not violate
the asymptotic notation used in Algorithm 1. We start by manipulating the term
F
in eqn. (4). Replacing A by Ak + Aρ−k  and using the Pythagorean theorem (the subspaces spanned
by the components Ak − X~γX T

(cid:13)(cid:13)A − X~γX T

(cid:13)(cid:13)(I − X~γX T
{z
|
We ﬁrst bound the second term of eqn. (14). Since I−X~γX T
without increasing a unitarily invariant norm. Now eqn. (5) implies that

(cid:13)(cid:13)(I − X~γX T
(cid:13)(cid:13)2
~γ Ak and Aρ−k − X~γX T
|
{z
}

~γ Aρ−k are perpendicular) we get
+

~γ is a projector matrix  it can be dropped

~γ )Aρ−k

(cid:13)(cid:13)2

~γ )Ak

(14)

~γ A

~γ A

=

θ2
4

θ2
3

F

F

(cid:13)(cid:13)2

(cid:13)(cid:13)A − X~γX T
(cid:13)(cid:13)2
}

F

.

We now bound the ﬁrst term of eqn. (14):

θ2
4

(cid:13)(cid:13)

≤ Fopt.

θ3 ≤ (cid:13)(cid:13)(I − X~γX T
≤ (cid:13)(cid:13)(I − X~γX T
(cid:13)(cid:13)(I − XoptX T
(cid:13)(cid:13)(I − XoptX T
(cid:13)(cid:13)(I − XoptX T
|

≤ √
≤ √
√

(cid:13)(cid:13)
(cid:13)(cid:13)(VkSD)+
(cid:13)(cid:13)
+ ∥E∥
(cid:13)(cid:13)(VkSD)+
(cid:13)(cid:13)
(cid:13)(cid:13)
2 + ∥E∥
(cid:13)(cid:13)
2 + ∥E∥
(cid:13)(cid:13)
∥(VkSD)∥
{z
}

opt)ASD
opt)ASD(VkSD)+
F
opt)ASD(VkSD)+V T
k

~γ )ASD(VkSD)+V T
k
~γ )ASD

=

γ

γ

γ

F

F

F

F

F

F

2

∥(VkSD)∥

F

(cid:13)(cid:13)(VkSD)+
(cid:13)(cid:13)
(cid:13)(cid:13)(VkSD)+

2

(cid:13)(cid:13)
2 + ∥E∥

F

2 + ∥E∥

(15)

(16)
(17)
(18)
(19)
F (20)

θ5

(cid:13)(cid:13)(

)

(cid:13)(cid:13)2

)

F

ASD

≤ γ min
X∈X

(cid:13)(cid:13)2
θ5 ≤ (cid:13)(cid:13)(I − XoptX T
≤ (cid:13)(cid:13)(I − XoptX T
√

In eqn. (16) we used Lemma 1  the triangle inequality  and the fact that I − ˜Xγ ˜X T
γ is a projector
matrix and can be dropped without increasing a unitarily invariant norm.
In eqn. (17) we used
submultiplicativity (see Section 4.1) and the fact that V T
k can be dropped without changing the
spectral norm. In eqn. (18) we replaced X~γ by Xopt and the factor
γ appeared in the ﬁrst term. To
better understand this step  notice that X~γ gives a γ-approximation to the optimal k-means clustering
of the matrix ASD  and any other n × k indicator matrix (for example  the matrix Xopt) satisﬁes

√

~γ

I − X~γX T

I − XoptX T
introduced the k × k identity matrix Ik = (V T

opt
F
(19) we ﬁrst
k SD)+(V T
k SD)
k SD) = k) and then we used submultiplicativity (see Section 4.1). In eqn. (20) we intro-
k without changing the Frobenius norm. We further manipulate the term θ5 of eqn. (20):

ASD

.

In eqn.
(rank(V T
duced V T

(cid:13)(cid:13)(I − XX T )ASD

F

≤ γ

(cid:13)(cid:13)(
(cid:13)(cid:13)2
(cid:13)(cid:13)(I − XoptX T

F

F

opt)E

+ ||E||F

opt)Ak
+
opt)AVkV T
k

(21)
(22)
(23)
In eqn. (21) we used Lemma 1 and the triangle inequality. In eqn. (22) we replaced Ak by AVkV T
and dropped I − XoptX T
k
opt is a projector matrix and does not
increase the Frobenius norm). In eqn. (23) we dropped the projector matrix VkV T
k and used eqn. (5)
and Deﬁnition 1. Combining equations (20)  (23)  (5)  Lemma 1  and the fact that γ ≥ 1  we get

≤ (1 + µ)
opt from the second term (I − XoptX T
√
}

√
|

θ3 ≤ √

1 + λ
1 − λ

(1 + µ) + µ)

{z

Fopt.

Fopt

γ (

F

(cid:13)(cid:13)

(cid:13)(cid:13)

(cid:13)(cid:13)

Simple algebra sufﬁces to show that for any ϵ ∈ (0  1)  for any positive integer k ≥ 1  and for some
sufﬁciently large constant c1  it is

thus

(24)
Combining eqn. (24) with eqns. (14) and (15) concludes the proof of eqn. (4). Using asymptotic
notation our choice of r satisﬁes r = Ω(k log(k/ϵ)/ϵ2). Note that Theorem 1 fails only if Lemma 1
or the γ-approximation k-means clustering algorithm fail  which happens w.p at most 0.5 + δγ.

θ2
3

θ6 ≤ √
≤ γ(1 + ϵ)Fopt.

1 + ϵ 

θ6

7

NIPS (k = 3)
Bio (k = 3)

r = 5k
F
P
.758
.847
.742
.764

r = 10k
F
P
.751
.847
.935
0.726

r = 20k
F
P
.749
.859
.709

1

All

P
.881

1

F
.747
.709

Table 1: Numerics from our experiments (Leverage scores).

Figure 1: Leverage scores for the NIPS dataset.

5 Empirical study

We present an empirical evaluation of Algorithm 1 on two real datasets. We show that it selects the
most relevant features (Figure 1) and that the clustering obtained after feature selection is performed
is very accurate (Table 1). It is important to note that the choice of r in the description of Algorithm
1 is a sufﬁcient - not necessary - condition to prove our theoretical bounds. Indeed  a much smaller
choice of r  for example r = 10k  is often sufﬁcient for good empirical results.
We ﬁrst experimented with a NIPS documents dataset (see http://robotics.stanford.
edu/˜gal/ and [10]). The data consist of a 184 × 6314 document-term matrix A  with Aij de-
noting the number of occurrences of the j-th term in the i-th document. Each document is a paper
that appeared in the proceedings of NIPS 2001  2002  or 2003  and belongs to one of the following
three topic categories: (i) Neuroscience  (ii) Learning Theory  and (iii) Control and Reinforcement
Learning. Each term appeared at least once in one of the 184 documents. We evaluated the accuracy
of Algorithm 1 by running the Lloyd’s heuristic1 on the rescaled features returned by our method.
In order to drive down the failure probability of Algorithm 1  we repeated it 30 times (followed by
the Lloyd’ heuristic each time) and kept the partition that minimized the objective value. We report
the percentage of correctly classiﬁed objects (denoted by P   0 ≤ P ≤ 1)  as well as the value of
the k-means objective (i.e.  the value F = ||A − X~γX T
F of Theorem 1; the division by
the ||A||2
F is for normalization). Results are depicted in Table 1. Notice that only a small subset of
features sufﬁces to approximately reproduce the partition obtained when all features were kept. In
Figure 1 we plotted the distribution of the leverage scores for the 6314 terms (columns) of A; we
also highlighted the features returned by Algorithm 1 when the sampling parameter r is set to 10k.
We observed that terms corresponding to the largest leverage scores had signiﬁcant discriminative
power. In particular  ruyter appeared almost exclusively in documents of the ﬁrst and third cate-
gories  hand appeared in documents of the third category  information appeared in documents
of the ﬁrst category  and code appeared in documents of the second and third categories only. We
also experimented with microarray data showing the expression levels of 5520 genes (features) for
31 patients (objects) having three different cancer types [27]: 10 patients with gastrointestinal stro-
mal tumor  12 with leiomyosarcoma  and 9 with synovial sarcoma. Table 1 depicts the results from
our experiments by choosing k = 3. Note that the Lloyd’s heuristic worked almost perfectly when r
was set to 10k and perfectly when r was set to 20k. Experimental parameters set to the same values
as in the ﬁrst experiment.

F /||A||2

~γ A||2

1We ran 30 iterations of the E-M step with 30 different random initializations and returned the partition that
minimized the k-means objective function  i.e. we ran kmeans(A  k  ’Replicates’  30  ’Maxiter’  30) in MatLab.

8

0100020003000400050006000700000.010.020.030.040.050.060.070.08featuresLeverage ScoresNIPSall leverage scoresbest set ( r = 30 )ruyterhandinformationcodeuniversalitysourcestishbynaftalihebrewneuralcenterReferences
[1] D. Arthur and S. Vassilvitskii. k-means++: the advantages of careful seeding. In Proceedings of the 18th Annual ACM-SIAM Symposium

on Discrete algorithms (SODA)  pages 1027–1035  2007.

[2] C. Boutsidis  M. W. Mahoney  and P. Drineas. Unsupervised feature selection for Principal Components Analysis. In Proceedings of the

14th Annual ACM SIGKDD Conference (KDD)  pages 61–69  2008.

[3] S. Chandrasekaran and I. Ipsen. On rank-revealing factorizations. SIAM Journal on Matrix Analysis and Applications  15:592–622 

1994.

[4] S. Chatterjee and A. S. Hadi. Inﬂuential observations  high leverage points  and outliers in linear regression. Statistical Science  1:379–

393  1986.

[5] Y. Cui and J. G. Dy. Orthogonal principal feature selection. manuscript.
[6] P. Drineas  A. Frieze  R. Kannan  S. Vempala  and V. Vinay. Clustering in large graphs and matrices. In Proceedings of the 10th Annual

ACM-SIAM Symposium on Discrete Algorithms (SODA)  pages 291–299  1999.

[7] P. Drineas  M. Mahoney  and S. Muthukrishnan. Relative-Error CUR Matrix Decompositions. SIAM Journal on Matrix Analysis and

Applications  30:844–881  2008.

[8] P. Drineas  M. Mahoney  and S. Muthukrishnan. Sampling algorithms for ℓ2 regression and applications. In Proceedings of the 17th

Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)  pages 1127–1136  2006.

[9] D. Foley and J. Sammon  J.W. An optimal set of discriminant vectors. IEEE Transactions on Computers  C-24(3):281–289  March 1975.
[10] A. Globerson  G. Chechik  F. Pereira  and N. Tishby. Euclidean Embedding of Co-occurrence Data. The Journal of Machine Learning

Research  8:2265–2295  2007.

[11] G. Golub and C. V. Loan. Matrix Computations. Johns Hopkins University Press  Baltimore  1989.
[12] S. A. Goreinov  E. E. Tyrtyshnikov  and N. L. Zamarashkin A theory of pseudoskeleton approximations. Linear Algebra and Its

[13]
[14]

Applications  261:1-21  1997.
I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning Research  3:1157–1182  2003.
I. Guyon  S. Gunn  A. Ben-Hur  and G. Dror. Result analysis of the NIPS 2003 feature selection challenge. In Advances in Neural
Information Processing Systems (NIPS) 17  pages 545–552  2005.

[15] J.A. Hartigan. Clustering algorithms. John Wiley & Sons  Inc. New York  NY  USA  1975.
[16] X. He  D. Cai  and P. Niyogi. Laplacian score for feature selection. In Advances in Neural Information Processing Systems (NIPS) 18 

pages 507–514. 2006.

[17] Y.P. Hong and C.T. Pan. Rank-revealing QR factorizations and the singular value decomposition. Mathematics of Computation 

58:213232  1992.
I. Jolliffe. Discarding variables in a principal component analysis. I: Artiﬁcial data. Applied Statistics  21(2):160–173  1972.
I. Jolliffe. Discarding variables in a principal component analysis. II: Real data. Applied Statistics  22(1):21–31  1973.

[18]
[19]
[20] W. Krzanowski. Selection of variables to preserve multivariate data structure  using principal components. Applied Statistics  36(1):22–

33  1987.

[21] A. Kumar  Y. Sabharwal  and S. Sen. A simple linear time (1 + ϵ)-approximation algorithm for k-means clustering in any dimensions.

In Proceedings of the 45th Annual IEEE Symposium on Foundations of Computer Science (FOCS)  pages 454–462  2004.

[22] S.P. Lloyd. Least squares quantization in PCM. Unpublished Bell Lab. Tech. Note  portions presented at the Institute of Mathematical
Statistics Meeting Atlantic City  NJ  September 1957. Also  IEEE Trans Inform Theory (Special Issue on Quantization)  vol IT-28  pages
129-137  March 1982.

[23] Y. Lu  I. Cohen  X. S. Zhou  and Q. Tian. Feature selection using principal feature analysis. In Proceedings of the 15th international

conference on Multimedia  pages 301–304  2007.

[24] M. W. Mahoney and P. Drineas. CUR Matrix Decompositions for Improved Data Analysis. In Proceedings of the National Academy of

Sciences  USA (PNAS)  106  pages 697-702  2009.

[25] A. Malhi and R. Gao. PCA-based feature selection scheme for machine defect classiﬁcation. IEEE Transactions on Instrumentation and

Measurement  53(6):1517–1525  Dec. 2004.

[26] K. Mao. Identifying critical variables of principal components for unsupervised feature selection. IEEE Transactions on Systems  Man 

and Cybernetics  35(2):339–344  April 2005.

[27] T. Nielsen et al. Molecular characterisation of soft tissue tumors: A gene expression study. Lancet  359:1301–1307  2002.
[28] R. Ostrovsky  Y. Rabani  L. J. Schulman  and C. Swamy. The effectiveness of Lloyd-type methods for the k-means problem.

Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)  pages 165–176  2006.

In

[29] M. Rudelson  and R. Vershynin  Sampling from large matrices: An approach through geometric functional analysis. Journal of the ACM

(JACM)  54(4)  July 2007.

[30] A. Wang and E. A. Gehan. Gene selection for microarray data analysis using principal component analysis. Stat Med  24(13):2069–2087 

July 2005.

[31] F. Woolfe  E. Liberty  V. Rokhlin  and M. Tygert. A fast randomized algorithm for the approximation of matrices. Applied and Compu-

tational Harmonic Analysis  25 (3): 335-366  2008.

[32] X. Wu et al. Top 10 algorithms in data mining analysis. Knowl. Inf. Syst.  14(1):1–37  2007.
[33] D. Zhang  S. Chen  and Z.-H. Zhou. Constraint score: A new ﬁlter method for feature selection with pairwise constraints. Pattern

Recognition  41(5):1440–1451  2008.

9

,Saurabh Singh
Derek Hoiem
David Forsyth