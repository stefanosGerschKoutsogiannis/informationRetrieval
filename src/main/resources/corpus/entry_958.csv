2014,Clustered factor analysis of multineuronal spike data,High-dimensional  simultaneous recordings of neural spiking activity are often explored  analyzed and visualized with the help of latent variable or factor models. Such models are however ill-equipped to extract structure beyond shared  distributed aspects of firing activity across multiple cells. Here  we extend unstructured factor models by proposing a model that discovers subpopulations or groups of cells from the pool of recorded neurons. The model combines aspects of mixture of factor analyzer models for capturing clustering structure  and aspects of latent dynamical system models for capturing temporal dependencies. In the resulting model  we infer the subpopulations and the latent factors from data using variational inference and model parameters are estimated by Expectation Maximization (EM). We also address the crucial problem of initializing parameters for EM by extending a sparse subspace clustering algorithm to integer-valued spike count observations. We illustrate the merits of the proposed model by applying it to calcium-imaging data from spinal cord neurons  and we show that it uncovers meaningful clustering structure in the data.,Clustered factor analysis of multineuronal spike data

Lars Buesing1  Timothy A. Machado1 2  John P. Cunningham1 and Liam Paninski1

1 Department of Statistics  Center for Theoretical Neuroscience

& Grossman Center for the Statistics of Mind

2 Howard Hughes Medical Institute & Department of Neuroscience

Columbia University  New York  NY

{lars cunningham liam}@stat.columbia.edu

Abstract

High-dimensional  simultaneous recordings of neural spiking activity are often
explored  analyzed and visualized with the help of latent variable or factor mod-
els. Such models are however ill-equipped to extract structure beyond shared 
distributed aspects of ﬁring activity across multiple cells. Here  we extend un-
structured factor models by proposing a model that discovers subpopulations or
groups of cells from the pool of recorded neurons. The model combines aspects
of mixture of factor analyzer models for capturing clustering structure  and as-
pects of latent dynamical system models for capturing temporal dependencies. In
the resulting model  we infer the subpopulations and the latent factors from data
using variational inference and model parameters are estimated by Expectation
Maximization (EM). We also address the crucial problem of initializing parame-
ters for EM by extending a sparse subspace clustering algorithm to integer-valued
spike count observations. We illustrate the merits of the proposed model by ap-
plying it to calcium-imaging data from spinal cord neurons  and we show that it
uncovers meaningful clustering structure in the data.

1

Introduction

Recent progress in large-scale techniques for recording neural activity has made it possible to study
the joint ﬁring statistics of 102 up to 105 cells at single-neuron resolution. Such data sets grant
unprecedented insight into the temporal and spatial structure of neural activity and will hopefully
lead to an improved understanding of neural coding and computation.
These recording techniques have spurred the development of statistical analysis tools which help to
make accessible the information contained in simultaneously recorded activity time-series. Amongst
these tools  latent variable models prove to be particularly useful for analyzing such data sets [1 
2  3  4]. They aim to capture shared structure in activity across different neurons and therefore
provide valuable summary statistics of high-dimensional data that can be used for exploratory data
analysis as well as for visualization purposes. The majority of latent variable models  however 
being relatively general purpose tools  are not designed to extract additional structure from the data.
This leads to latent variables that can be hard to interpret biologically. Furthermore  additional
information from other sources  such as spatial structure or genetic cell type information  cannot be
readily integrated into these models.
An approach to leveraging simultaneous activity recordings that is complementary to applying un-
structured factor models  is to infer detailed circuit properties from the data. By modelling the
detailed interactions between neurons in a local micro-circuit  multiple tools aim at inferring the
existence  type  and strength of synaptic connections between neurons [5  6]. In spite of algorithmic
progress [7]  the feasibility of this approach has only been demonstrated in circuits of up to three

1

neurons [8]  as large scale data with ground truth connectivity is currently only rarely available.
This lack of validation data sets also makes it difﬁcult to asses the impact of model mismatch and
unobserved  highly-correlated noise sources (“common input”).
Here  we propose a statistical tool for analyzing multi-cell recordings that offers a middle ground
between unstructured latent variable models and models for inferring detailed network connectivity.
The basic goal of the model is to cluster neurons into groups based on their joint activity statistics.
Clustering is a ubiquitous and valuable tool in statistics and machine learning as it often yields
interpretable structure (a partition of the data)  and is of particular relevance in neuroscience because
neurons often can be categorized into distinct groups based on their morphology  physiology  genetic
identity or stimulus-response properties. In many experimental setups  side-information allowing for
a reliable supervised partitioning of the recorded neurons is not available. Hence  the main goal of
the paper is to develop a method for clustering neurons based on their activity recordings.
We model the ﬁring time-series of a cluster of neurons using latent factors  assuming that different
clusters are described by disjoint sets of factors. The resulting model is similar to a mixture of factor
analyzers [9  10] with Poisson observations  where each mixture component describes a subpopu-
lation of neurons. In contrast to a mixture of factor analyzers model which assumes independent
factors  we put a Markovian prior over the factors  capturing temporal dependencies of neural ac-
tivity as well as interactions between different clusters over time. The resulting model  which we
call mixture of Poisson linear dynamical systems (mixPLDS) model  is able to capture more struc-
ture using the cluster assignments compared to latent variable models previously applied to neural
recordings  while at the same time still providing low-dimensional latent trajectories for each cluster
for exploratory data analysis and visualization. In contrast to the lack of connectivity ground truth
for neurons from large-scale recordings  there are indeed large-scale activity recordings available
that exhibit rich and biologically interpretable clustering structure  allowing for a validation of the
mixPLDS model in practice.

2 Mixture of Poisson linear dynamical systems for modelling neural

subpopulations

2.1 Model deﬁnition

Let ykt denote the observed spike count of neuron k = 1  . . .   K in time-bin t = 1  . . .   T . For
the mixture of Poisson linear dynamical systems (mixPLDS) model  we assume that each neuron k
belongs to exactly one of M groups (subpopulations  clusters)  indicated by the discrete (categorical)
variable sk ∈ {1  . . .   M}. The sk are modelled as i.i.d.:

K(cid:89)

K(cid:89)

p(s) =

p(sk) =

k=1

k=1

Disc(sk|φ0) 

(1)

0  . . .   φM

0 ) are the natural parameters of the categorical distribution. In the remain-
where φ0 := (φ1
der of the paper we use the convention that the group-index m = 1  . . .   M is written as superscript.
t ∈ Rdm. We
The activity of each subpopulation m at time t is modeled by a latent variable xm
assume that these latent variables (we will also call them factors) are jointly normal and we model
interactions between different groups by a linear dynamical system (LDS) prior:

 x1

...
xM
t

t

 = Axt−1 + ηt =

 A11

...
AM 1



x1

t−1
...
xM
t−1

 + ηt 

··· A1M
...
··· AMM

xt =

(2)

where the block-matrices Aml ∈ Rdm×dl capture the interactions between groups m and l. The
innovations ηt are i.i.d. from N (0  Q) and the starting distribution is given by x1 ∼ N (µ1  Q1). If
neuron k belongs to group m  i.e. sk = m  we model its activity ykt at time t as Poisson distributed
spike count with a log-rate given by an afﬁne combination of the factors of group m:

zkt | sk = m = C m
ykt | zkt  sk ∼ Poisson(exp(zkt + bk)) 

k: xm
t

(3)
(4)

2

C := (C 1 ··· CM ) ∈ RK×d  where d := (cid:80)M

where b ∈ RK captures the baseline of the ﬁring rates. We denote with C m ∈ RK×dm the
group loading matrix with rows C m
k: for neurons k in group m and ﬁll in the remaining rows
with 0s for all neurons not in group m. We concatenate these into the total loading matrix
m=1 dm is the total latent dimension. If the neu-
rons are sorted with respect to their group membership  then the total loading C has block-diagonal
structure. Further  we denote with yk: := (yk 1 ··· yk T ) the activity time series of neuron k and
n T ) ∈ R1×T for n = 1  . . .   dm. The model
use an analogous notation for xm
parameters are θ := (A  Q  Q1  µ1  C  b); we consider the hyper-parameters φ0 to be given and
ﬁxed.
For known clusters s  the mixPLDS model can be regarded as a special case of the Poisson linear
dynamical system (PLDS) model [3]  where the loading C is block-diagonal. For unknown group
memberships s  the mixPLDS model deﬁned above is similar to a mixture of factor analyzers (e.g.
see [9  10]) with Poisson observations over neurons k = 1  . . .   K. In the mixPLDS model however 
we do not restrict the factors of the mixture components to be independent but allow for interactions
over time which are modeled by a LDS.

n 1 ··· xm

n := (xm

2.2 Variational inference and parameter estimation for the mixPLDS model

When applying the mixPLDS model to data y  we are interested in inferring the group memberships
s and the latent trajectories x as well as estimating the parameters θ. For known parameters θ 
the posterior p(x  s|y  θ) (even in the special case of a single mixture component M = 1) is not
available in closed form and needs approximating. Here we propose to approximate the posterior
using variational inference with the following factorization assumption:

p(x  s|y  θ) ≈ q(x)q(s).

(5)
We further restrict q(x) to be a normal distribution q(x) = N (x|m  V ) with mean m and covariance
k q(sk) where q(sk) is a
categorical distribution with natural parameters φk = (φ1
k ). The variational parameters
m  V and φ = (φ1  . . .   φK) are obtained by maximizing the variational lower bound of the log
marginal likelihood log p(y|θ):
L(m  V  φ  θ) =

V . Under the assumption (5)  q(s) further factorizes into the product(cid:81)
(cid:0)log |V | − tr[Σ−1V ] − (m − µ)(cid:62)Σ−1(m − µ)(cid:1) +
K(cid:88)
M(cid:88)

DKL[q(sk)(cid:107)p(sk)]

k  . . .   φM

T(cid:88)

K(cid:88)

k=1

πm
k (ykthm

kt − exp(hm

kt + ρm

kt/2)) + const

1
2

+

(6)

m=1

k=1
:= C mmt + b 

t=1

hm
t

:= diag(C mVtC m(cid:62)

) 

ρm
t

k ∝ exp(φm
πm
k ) 

where Vt = Covq(x)[xt] and µ ∈ RdT   Σ ∈ RdT×dT are the mean and covariance of the LDS
prior over x. The ﬁrst two terms in (6) are the Kullback-Leibler divergence between the prior
p(x  s) = p(x)p(s) and its approximation q(x)q(s)  penalizing a variational posterior that is far
away from the prior. The third term in (6) is given by the expected log-likelihood of the data 
promoting a posterior approximation that explains the observed data well. We optimize L in a
coordinate ascent manner  i.e. we hold φ ﬁxed and optimize jointly over m  V and vice versa. A
naive implementation of the optimization of L over {m  V } is prohibitively costly for data sets with
large T   as the posterior covariance V has O((dT )2) elements and has to be optimized over the set
of semi-deﬁnite matrices. Instead of solving this large program  we apply a method proposed in
[11]  where the authors show that Gaussian variational inference for latent Gaussian models with
Poisson observations can be solved more efﬁciently using the dual problem. We generalize their
approach to the mixture of Poisson observation model (3) considered here  and we also leverage the
Markovian structure of the LDS prior to speed up computations (see below). In the supplementary
material  we derive this approach to inference in the mixPLDS model in detail. The optimization
over φ is available in closed form and is also given in the supplementary material. We iterate updates
over m  V and φ. In practice  this method converges very quickly  often requiring only two or three
iterations to reach a reasonable convergence criterion.
The most computationally intensive part of the proposed variational inference method is the update
of m  V . Using properties of the LDS prior (i.e. the prior precision Σ−1 is block-tri-diagonal) 

3

we can show that evaluation of L  its dual and the gradient of the latter all cost O(KT d + T d3) 
which is the same complexity as Kalman smoothing in a LDS with Gaussian observations or a
single iteration of Laplace inference over x. While having the same cost as Laplace approximation 
variational inference has the advantage of a non-deceasing variational lower bound L  which can be
used for monitoring convergence as well as for model comparison.
We can also get estimates for the model parameters by maximizing the lower bound L over θ. To
this end  we interleave updates of φ and m  V with maximizations over θ. The latter corresponds to
standard parameter updates in a LDS model with Poisson observations and are discussed e.g. in [3].
This procedure implements variational Expectation Maximization (VEM) in the mixPLDS model.

2.3

Initialization by Poisson subspace clustering

In principle  for a given number of groups M with given dimensions d1  . . .   dM one can estimate
the parameters of the mixPLDS using VEM as described above. In practice we ﬁnd however that
this yields poor results without having reasonable initial membership assignments s  i.e. reasonable
initial values for the variational parameters φ. Furthermore  VEM requires the a priori speciﬁcation
of the latent dimensions d1  . . .   dM . Here we show that a simple extension to an existing subspace
clustering algorithm provides  given the number of groups M  a sufﬁciently accurate initializer for
φ and allows for an informed choice for the dimensions d1  . . .   dM .
We ﬁrst illustrate the connection of the mixPLDS model to the subspace clustering problem (for
a review of the latter see e.g. [12]). Assume that we observe the log-rates zkt deﬁned in equation
(3) directly; we denote the corresponding data matrix as Z ∈ RK×T . For unknown loading C  the
row Zk: lies on a dm-dimensional subspace spanned by the “basis-trajectories” xm
dm :  if
neuron k is in group m. If s and x are unobserved  we only know that the rows of Z lie on a union
of M subspaces of dimensions d1  . . .   dm in an ambient space of dimension T . Reconstructing the
subspaces and the subspace assignments is known as a subspace clustering problem and connections
to mixtures of factor analyzers have been pointed out in [13]. The authors of [13] propose to solve
the subspace clustering problem by the means of the following sparse regression problem:

1 :  . . .   xm

min

W∈RK×K
s.t.

(cid:107)Z − W Z(cid:107)2

1
2
diag(W ) = 0.

F + λ(cid:107)W(cid:107)1

(7)

This optimization can be interpreted as trying to reconstruct each row Zk: by the remaining rows
Z\k: using sparse reconstruction weights W . Intuitively  a point on a subspace can be reconstructed
using the fewest reconstruction weights by points on the same subspace  i.e. Wkl = 0 if k and l lie
on different subspaces. The symmetrized  sign-less weights |W| + |W|(cid:62) are then interpreted as the
adjacency matrix of a graph and spectral clustering  with a user deﬁned number of clusters M  is
applied to obtain a subspace clustering solution. In the noise-free case (and taking λ → 0 in eqn 7) 
under linear independence assumptions on the subspaces  [13] shows that this procedure recovers
the correct subspace assignments.
If the matrix Z is not observed directly but only through the observation model (3)  the subspace
clustering approach does not directly apply. The observed data Y generated from the model (3)
is corrupted by Poisson noise and furthermore the non-linear link function transforms the union of
subspaces into a union of manifolds. We can circumvent these problems using the simple observa-
tion that not only Z but also the rows Ck: of the loading matrix C lie on a union of subspaces of
dimensions d1  . . .   dm (where the ambient space has dimension d). This can be easily seen from the
block-diagonal structure of C (if the neurons are sorted by their true cluster assignments) mentioned
in section 2.1. Hence we can use an estimate ˜C of the loading C as input to the subspace clustering
optimization (7). In order to get an initial estimate ˜C we can use a variety of dimensionality re-
duction methods with exp-Poisson observations  e.g. exponential family PCA [14]  a nuclear norm
based method [15]  subspace identiﬁcation methods [16] and EM-based PLDS learning [16]; here
we use the nuclear norm based method [15] for reasons that will become obvious below. Because of
the non-identiﬁability of latent factor models  these methods only yield an estimate of C · D with an
unknown  invertible transformation D ∈ Rd×d. Nevertheless  the rows of C·D still lie on a union of
subspaces (which are however not axis-aligned anymore as is the case for C)  and therefore the clus-
ter assignments can still be recovered. Given these cluster assignments  we can get initial estimates
of the non-zero rows of C m by applying nuclear norm minimization to the individual clusters. This

4

method also returns a singular value spectrum associated with each subspace  which can be used to
determine the dimension dm. One can specify e.g. a threshold σmin  and determine the dimension
dm as the number of singular values > σmin.

2.4 The full parameter estimation algorithm

We brieﬂy summarize the proposed parameter estimation algorithm for the mixPLDS model. The
procedure requires the user to deﬁne the number of groups M. This choice can either be informed
by biological prior knowledge or one can use standard model selection methods  such as cross-
validation on the variational approximation of the marginal likelihood. We ﬁrst get an initial es-
timate ˜C of the total loading matrix by nuclear-norm-penalized Poisson dimensionality reduction.
Then  subspace clustering on ˜C yields initial group assignments. Based on these assignments  for
each cluster we estimate the group dimension dm and the group loading ˜C m. Keeping the cluster
assignments ﬁxed  we do a few VEM steps in the mixPLDS model with an initial estimation for the
loading matrix given by ( ˜C 1  . . .   ˜CM ). This last step provides reasonable initial parameters for the
parameters A  Q  Q1  µ1 of the dynamical system prior. Finally  we do full VEM iterations in the
mixPLDS model to reﬁne the initial parameters. We monitor the increase of the variational lower
bound L and use its increments in a termination criterion for the VEM iterations.

2.5 Non-negativity constraints on the loading C

Each component m of the mixPLDS model  representing a subpopulation of neurons  can be a
very ﬂexible model by itself (depending on the latent dimension dm). This ﬂexibility can in some
situations lead to counter-intuitive clustering results. Consider the following example. Let half of
the recorded neurons oscillate in phase and the remaining neurons oscillate with a phase shift of
π relative to the ﬁrst half. Depending on the context  we might be interested in clustering the ﬁrst
and second half of the neurons into separate groups reﬂecting oscillation phase. The mixPLDS
model could however end up putting all neurons into a single cluster  by modelling them with one
oscillating latent factor that has positive loadings on the ﬁrst half of neurons and negative on the
second half (or vice versa). We can prevent this behavior  by imposing element-wise non-negativity
constraints on the loading matrix C  denoted as C ≥ 0 (and by simultaneously constraining the
latent dimensions of each group). The constraints guarantee that the inﬂuence of each factor on its
group has the same sign across all neurons. The suitability of these constraints strongly depends on
the biological context. In the application of the mixPLDS model in section 3.2  we found them to
be essential for obtaining meaningful results.
We modify the subspace clustering initialization to respect the constraints C ≥ 0 in the follow-
ing way. Instead of solving the unconstrained reconstruction problem (7) with respect to W   we
add non-negativity constraints W ≥ 0. These sign constraints restrict the points that can be re-
constructed from a given set of points to the convex cone of these points (instead of the subspace
containing these points). Hence  under these assumptions  all data points in a cluster can be ap-
proximately reconstructed by a (non-negative) convex combination of some “time-series basis”. We
empirically observed that this yields initial loading matrix estimates with only very few negative
elements (after possible row-wise sign inversions). For the full mixPLDS model we enforce C ≥ 0
by the reparametrization C = exp(χ) and doing VEM updates on χ.

3 Experiments

3.1 Artiﬁcial data

Here we validate the parameter estimation procedure for the mixPLDS model on artiﬁcial data. We
generate 35 random ground truth mixPLDS models with M = 3  d1 = d2 = d3 = 2 and 20 observed
neurons per cluster. We sampled from each ground truth model a data set consisting of 4 i.i.d. trials
with T = 250 times steps each. Ground truth parameters were generated such that the resulting data
was sparse (12% of the bins non-empty). We compared the ability of different clustering methods
to recover the 3 clusters from each data set. We report the results in ﬁg. 1A in terms of the fraction
of misclassiﬁed neurons (class labels were determined by majority vote in each cluster). We applied
K-Means with careful initialization of the cluster centers [17] to the data. For K-Means  we pre-

5

A

B

Figure 1: Finding clusters of neurons in artiﬁcial data. A: Performance of different clustering
algorithms  reported in terms of frequency of misclassiﬁed neurons  on artiﬁcial data sampled from
ground truth mixPLDs models. Red bars indicate medians and blue boxes the 25% and 75% per-
centiles. Standard clustering methods (data plotted in black) such as K-Means  spectral clustering
(“specCl”)  and subspace clustering (“subCl”) are substantially outperformed by the two methods
proposed here (data plotted in red). Poisson subspace clustering (“PsubCl”) yielded accurate initial
cluster estimates that were signiﬁcantly improved by application of the full mixPLDs model. B:
Misclassiﬁcation rate as a function of the cluster assignment uncertainty for the mixPLDS model.
This shows that the posterior over cluster assignments returned by the mixPLDS model is well cali-
brated  as neurons with low assignment uncertainty as rarely misclassiﬁed.

processed the data in a standard way by smoothing (Gaussian kernel  standard deviation 10 time-
steps)  mean-centering and scaling (such that each dimension k = 1  . . .   K has variance 1). We
found K-Means yielded reasonable clusters when all populations are one-dimensional (i.e. ∀m dm =
1  data not shown) but it fails when clustering multi-dimensional groups of neurons. An alternative
approach is to cluster the cross-correlation matrix of neurons (computed from pre-processed data as
above) with standard spectral clustering [18]. We found that this approach works well when all the
factors have small variances  as in this case the link function of the observation model is only mildly
non-linear. However  with growing variances of the factors (larger dynamic ranges of neurons)
spectral clustering performance quickly degrades. Standard sparse subspace clustering [13] on the
spike trains (pre-processed as above) yielded very similar results to spectral clustering. We found
our novel Poisson subspace clustering algorithm proposed in section 2.3 to robustly outperform the
other approaches  as long as reasonable amounts of data were available (roughly T > 100 for the
above system). The mixPLDS model initialized with the Poisson subspace clustering consistently
yielded the best results  as it is able to integrate information over time and denoise the observations.
One advantage of the mixPLDS model is that it not only returns cluster assignments for neurons
but also provides a measure of uncertainty over these assignments. However  variational inference
tends to return over-conﬁdent posteriors in general and the factorization approximation (5) might
yield posterior uncertainty that is uninformative. To show that the variational posterior uncertainty
is well-calibrated we computed the entropy of the posterior cluster assignment q(sk) for all neurons
as a measure for assignment uncertainty. We binned the neurons according to their assignment
uncertainty and report the misclassiﬁcation rate for each bin in ﬁg. 1B. 89% of the neurons have low
posterior uncertainty and reside in the ﬁrst bin having a low misclassiﬁcation rate of ≈ 0.1  whereas
few neurons (5%) have an assignment uncertainty larger than 0.3 nats and they are misclassiﬁed
with a rate of ≈ 0.4.

3.2 Calcium imaging of spinal cord neurons

We tested the mixPLDS model on calcium imaging data obtained from an in vitro  neonatal mouse
spinal cord that expressed the calcium indicator GCaMP3 in all motor neurons. When an isolated
spinal cord is tonically excited by a cocktail of rhythmogenic drugs (5 µM NMDA  10 µM 5-HT 
50 µM DA)  motor neurons begin to ﬁre rhythmically. In this network state  spatially clustered en-
sembles of motor neurons ﬁre in phase with each other [19]. Since multiple ensembles that have
distinct phase tunings can be visualized in a single imaging ﬁeld  this data represents a convenient

6

KmeansspecClsubClPsubClmixPLDS00.5freq. of misclassification00.10.20.30.400.5assignment uncertaintyfreq. of misclassificationA

B

C

Figure 2: Application of the mixPLDS model to recordings from spinal cord neurons. A  top
panel: 500 frames of input data to the mixPLDS model. Middle panel: Same data as in upper panel 
but rows are sorted by mixPLDS clusters and factor loading. Inferred latent factors (red: cluster 1 
blue: cluster 2  solid: factor 1  dashed: factor 2) are also shown. Bottom panel: Inferred (smoothed)
ﬁring rates. B: Loading matrix C of the mixPLDS model showing how factors 1 2 of cluster 1 and
factors 3 4 of cluster 2 inﬂuence the neurons. C: Preferred phases shown as a function of (sorted)
neuron index and colored by posterior probability of belonging to cluster 1. Clearly visible are two
clusters as well as an (approximately) increasing ordering within a cluster.

setting for testing our algorithm. The data (90 second long movies) were acquired at 15 Hz from
a custom two-photon microscope equipped with a resonant scanner (downsampled from 60 Hz to
boost SNR). The frequency of the rhythmic activity was typically 0.2 Hz. In addition  aggregate mo-
tor neuron activity was simultaneously acquired with each movie using a suction electrode attached
to a ventral root. This electrophysiology recording (referred to here as ephys-trace) was used as an
external phase reference point to compute phase tuning curves for imaged neurons  which we used
to validate our mixPLDS results.
A deconvolution algorithm [20] was applied to the recorded calcium time-series to estimate the
spiking activity of 70 motor neurons. The output of the deconvolution  a 70 × 1140 (neurons ×
frames) matrix of posterior expected number of spikes  was used as input to the mixPLDS model.
The non-empty bins of the the ﬁrst 500 out of the 1140 frames of input data (thresholded at 0.1)
are shown in ﬁg. 2A (upper panel). We used a mixPLDS model with M = 2 groups with two
latent dimensions each  i.e. d1 = d2 = 2. We imposed the non-negativity constraints C ≥ 0 on the
loading matrix; these were found to be crucial for ﬁnding a meaningful clustering of the neurons 
as discussed above. The mixPLDS clustering reveals two groups with strongly periodic but phase-
shifted population activities  as can be seen from the inferred latent factors shown in ﬁg. 2A (middle
panel  factors of cluster 1 shown in red  factors of cluster 2 in blue). For each cluster  the model
learned a stronger (higher variance) latent factor (solid line) and a weaker one (dashed line); we
interpret the former as capturing the main activity structure in a cluster and the latter as describing
deviations. Based on the estimated mixPLDS model  we sorted the neurons for visualization into
two clusters according to their most likely cluster assignment argmaxsk=1 2 q(sk). Within each
cluster  we sorted the neurons according to the ratio of the loading coefﬁcient onto the stronger
factor over the loading onto the weaker factor. Re-plotting the spike-raster with this sorting in ﬁg.
2A (middle panel) reveals interesting structure. First  it shows that the initial choice of two clusters
was well justiﬁed for this data set. Second  the sorting reveals that the majority of neurons tend to

7

701170frames1500701sortedneuron #sortedneuron #unsortedneuron #factorscluster 1factorscluster 2latent dimsorted neuron #14170ﬁre at a preferred phase relative to the oscillation cycle  and the mixPLDS-based sorting corresponds
to an increasing ordering of preferred phases. Fig. 2B shows the loading matrix C of the mixPLDS 
which is found to be approximately block-diagonal.
On this data set we also have the opportunity to validate the unsupervised clustering by taking into
account the simultaneously recorded ephys-trace. We computed for each neuron a phase tuning
curve based on the ephys-trace history of the last 80 times steps (estimated via L2 regularized gen-
eralized linear model estimation  with an exp-Poisson observation model). For each neuron  we
extracted the peak location of this phase tuning curve  which we call the preferred phase. Fig. 2C
shows these preferred phases as a function of (sorted) neuron index  revealing that the two clusters
found by the mixPLDS model coincide well with the two modes of the bi-model distribution of pre-
ferred phases. Furthermore  within each cluster  the preferred phases are (approximately) increasing 
showing that the mixPLDS-sorting of neurons reﬂects the phase-relation of the neurons to the global 
oscillatory ephys-trace. We emphasize that the latter was not used for ﬁtting the mixPLDS; i.e.  this
constitutes an independent validation of our results.
We conclude that the mixPLDS model successfully uncovered clustering structure from the record-
ings that can be validated using the side information from electrophysiological tuning  and further-
more allowed for a meaningful sorting within each cluster capturing neural response properties. In
addition  the mixPLDS model leverages the temporal structure in recordings  automatically optimiz-
ing for the temporal smoothness level and revealing the main time-constants in the data (in the above
data set 1.8 and 6.5 sec) as well as main oscillation frequencies (0.2 and 0.45Hz). Furthermore  ei-
ther the latent trajectories or the inferred ﬁring rates shown in ﬁg. 2A can be used as smoothed
proxies for their corresponding population activities for subsequent analyses.

4 Discussion

One can generalize the mixPLDS model in several ways. Here we assumed that  given the latent fac-
tors  all neurons ﬁre independently. This is presumably a good assumption if the recorded neurons
are spatially distant  but it might break down if neurons are densely sampled from a local population
and have strong  monosynaptic connections. This more general case can be accounted for by incor-
porating direct interaction terms between neurons into the observation model in the spirit of coupled
GLMs (see [21]); inference and parameter learning are still tractable in this model using VEM. Fur-
thermore  in addition to the activity recordings  one might have access to other covariates that are
informative about the clustering structure of the population  such as cell location  genetic markers 
or cell morphology. We can add such data as additional observations into the mixPLDS model to
facilitate clustering of the cells. An especially relevant example are stimulus-response properties of
cells. We can add a mixture model over receptive-ﬁeld parameters using the cluster assignments s.
This extension would provide a clustering of neurons based on their joint activity statistics (such as
shared trial-to-trial variability) as well as on their receptive ﬁeld properties.
We presented three technical contributions  that we expect to be useful outside the context of the
mixPLDS model. First  we proposed a simple extension of the sparse subspace clustering algorithm
to Poisson observations. We showed that if the dimension of the union of subspaces is much smaller
than the ambient dimension  our method substantially outperforms other approaches. Second  we
introduced a version of subspace clustering with non-negativity constraints on the reconstruction
weights  which therefore clusters points into convex cones. We expect this variant to be particularly
useful when clustering activity traces of cells  allowing for separating anti-phasic oscillations. Third 
we applied the dual variational inference approach of [11] to a model with a Markovian prior and
with mixtures of Poisson observations. The resulting inference method proved itself numerically
robust  and we expect it to be a valuable tool for analyzing time-series of sparse count variables.

Acknowledgements This work was supported by Simons Foundation (SCGB#325171 and
SCGB#325233)  Grossman Center at Columbia University  and Gatsby Charitable Trust as well
as grants MURI W911NF-12-1-0594 from the ARO  vN00014-14-1-0243 from the ONR  W91NF-
14-1-0269 from DARPA and an NSF CAREER award (L.P.).

8

References
[1] Anne C Smith and Emery N Brown. Estimating a state-space model from point process observations.

Neural Computation  15(5):965–991  2003.

[2] Lauren M Jones  Alfredo Fontanini  Brian F Sadacca  Paul Miller  and Donald B Katz. Natural stimuli
evoke dynamic sequences of states in sensory cortical ensembles. Proceedings of the National Academy
of Sciences  104(47):18772–18777  2007.

[3] Jakob H Macke  Lars Buesing  John P Cunningham  M Yu Byron  Krishna V Shenoy  and Maneesh

Sahani. Empirical models of spiking in neural populations. In NIPS  pages 1350–1358  2011.

[4] Byron M Yu  John P Cunningham  Gopal Santhanam  Stephen I Ryu  Krishna V Shenoy  and Maneesh
Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population
activity. In NIPS  pages 1881–1888  2008.

[5] Murat Okatan  Matthew A Wilson  and Emery N Brown. Analyzing functional connectivity using a
network likelihood model of ensemble neural spiking activity. Neural Computation  17(9):1927–1961 
2005.

[6] Yuriy Mishchenko  Joshua T Vogelstein  Liam Paninski  et al. A Bayesian approach for inferring neuronal
connectivity from calcium ﬂuorescent imaging data. The Annals of Applied Statistics  5(2B):1229–1261 
2011.

[7] Suraj Keshri  Eftychios Pnevmatikakis  Ari Pakman  Ben Shababo  and Liam Paninski. A shotgun
arXiv preprint

sampling solution for the common input problem in neural connectivity inference.
arXiv:1309.3724  2013.

[8] Felipe Gerhard  Tilman Kispersky  Gabrielle J Gutierrez  Eve Marder  Mark Kramer  and Uri Eden. Suc-
cessful reconstruction of a physiological circuit with known connectivity from spiking activity alone.
PLoS computational biology  9(7):e1003138  2013.

[9] Michael E Tipping and Christopher M Bishop. Mixtures of probabilistic principal component analyzers.

Neural Computation  11(2):443–482  1999.

[10] Zoubin Ghahramani  Geoffrey E Hinton  et al. The EM algorithm for mixtures of factor analyzers. Tech-

nical report  Technical Report CRG-TR-96-1  University of Toronto  1996.

[11] Mohammad Emtiyaz Khan  Aleksandr Aravkin  Michael Friedlander  and Matthias Seeger. Fast dual
variational inference for non-conjugate latent gaussian models. In Proceedings of The 30th International
Conference on Machine Learning  pages 951–959  2013.

[12] Ren´e Vidal. A tutorial on subspace clustering. IEEE Signal Processing Magazine  28(2):52–68  2010.
[13] Ehsan Elhamifar and Ren´e Vidal. Sparse subspace clustering: Algorithm  theory  and applications. Pattern

Analysis and Machine Intelligence  IEEE Transactions on  35(11):2765–2781  Nov 2013.

[14] Michael Collins  Sanjoy Dasgupta  and Robert E Schapire. A generalization of principal component

analysis to the exponential family. In NIPS  volume 13  page 23  2001.

[15] David Pfau  Eftychios A Pnevmatikakis  and Liam Paninski. Robust learning of low-dimensional dynam-

ics from large neural ensembles. In NIPS  pages 2391–2399  2013.

[16] Lars Buesing  Jakob H Macke  and Maneesh Sahani.

Spectral learning of linear dynamics from
generalised-linear observations with application to neural population data. In NIPS  pages 1691–1699 
2012.

[17] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings
of the eighteenth annual ACM-SIAM symposium on Discrete algorithms  pages 1027–1035. Society for
Industrial and Applied Mathematics  2007.

[18] Andrew Y Ng  Michael I Jordan  and Yair Weiss. On spectral clustering1 analysis and an algorithm. Pro-
ceedings of Advances in Neural Information Processing Systems. Cambridge  MA: MIT Press  14:849–
856  2001.

[19] Timothy A. Machado  Eftychios Pnevmatikakis  Liam Paninski  Thomas M. Jessell  and Andrew Miri.
In 79th Cold Spring

Functional organization of spinal motor neurons revealed by ensemble imaging.
Harbor Symposium on Quantitative Biology Cognition  2014.

[20] E. A. Pnevmatikakis  Y. Gao  D. Soudry  D. Pfau  C. Laceﬁeld  K. Poskanzer  R. Bruno  R. Yuste  and
L. Paninski. A structured matrix factorization framework for large scale calcium imaging data analysis.
ArXiv e-prints  September 2014.

[21] Jonathan W Pillow  Jonathon Shlens  Liam Paninski  Alexander Sher  Alan M Litke  EJ Chichilnisky  and
Eero P Simoncelli. Spatio-temporal correlations and visual signalling in a complete neuronal population.
Nature  454(7207):995–999  2008.

9

,Lars Buesing
Timothy Machado
John Cunningham
Liam Paninski