2019,Online Optimal Control with Linear Dynamics and Predictions: Algorithms and Regret Analysis,This paper studies the online optimal control problem with time-varying convex stage costs for a time-invariant linear dynamical system  where a finite lookahead window of accurate predictions of the stage costs are available at each time. We design online algorithms  Receding Horizon Gradient-based Control (RHGC)  that utilize the predictions through finite steps of gradient computations. We study the algorithm performance measured by dynamic regret: the online performance minus the optimal performance in hindsight. It is shown that the dynamic regret of RHGC decays exponentially with the size of the lookahead window. In addition  we provide a fundamental limit of the dynamic regret for any online algorithms by considering linear quadratic tracking problems. The regret upper bound of one RHGC method almost reaches the fundamental limit  demonstrating the effectiveness of the algorithm. Finally  we numerically test our algorithms for both linear and nonlinear systems to show the effectiveness and generality of our RHGC.,Online Optimal Control with Linear Dynamics and

Predictions: Algorithms and Regret Analysis

Yingying Li

SEAS

Xin Chen

SEAS

Harvard University

Cambridge  MA  02138

Harvard University

Cambridge  MA  02138

yingyingli@g.harvard.edu

chen_xin@g.harvard.edu

Na Li
SEAS

Harvard University

Cambridge  MA  02138

nali@seas.harvard.edu

Abstract

This paper studies the online optimal control problem with time-varying convex
stage costs for a time-invariant linear dynamical system  where a ﬁnite lookahead
window of accurate predictions of the stage costs are available at each time. We
design online algorithms  Receding Horizon Gradient-based Control (RHGC)  that
utilize the predictions through ﬁnite steps of gradient computations. We study
the algorithm performance measured by dynamic regret: the online performance
minus the optimal performance in hindsight. It is shown that the dynamic regret of
RHGC decays exponentially with the size of the lookahead window. In addition 
we provide a fundamental limit of the dynamic regret for any online algorithms
by considering linear quadratic tracking problems. The regret upper bound of one
RHGC method almost reaches the fundamental limit  demonstrating the effective-
ness of the algorithm. Finally  we numerically test our algorithms for both linear
and nonlinear systems to show the effectiveness and generality of our RHGC.

1

Introduction

In this paper  we consider a N-horizon discrete-time sequential decision-making problem. At each
time t = 0  . . .   N − 1  the decision maker observes a state xt of a dynamical system  receives
a W -step lookahead window of future cost functions of states and control actions  i.e. ft(x) +
gt(u)  . . .   ft+W−1(x) + gt+W−1(u)  then decides the control input ut which drives the system to a
new state xt+1 following some known dynamics. For simplicity  we consider a linear time-invariant
(LTI) system xt+1 = Axt + But with (A  B) known in advance. The goal is to minimize the overall
cost over the N time steps. This problem enjoys many applications in  e.g. data center management
[1  2]  robotics [3]  autonomous driving [4  5]  energy systems [6]  manufacturing [7  8]. Hence  there
has been a growing interest on the problem  from both control and online optimization communities.
In the control community  studies on the above problem focus on economic model predictive control
(EMPC)  which is a variant of model predictive control (MPC) with a primary goal on optimizing
economic costs [9  10  11  12  13  14  15  16]. Recent years have seen a lot of attention on the
optimality performance analysis of EMPC  under both time-invariant costs [17  18  19] and time-
varying costs [20  12  14  21  22]. However  most studies focus on asymptotic performance and there
is still limited understanding on the non-asymptotic performance  especially under time-varying
costs. Moreover  for computationally efﬁcient algorithms  e.g. suboptimal MPC and inexact MPC
[23  24  25  26]  there is limited work on the optimality performance guarantee.
In online optimization  on the contrary  there are many papers on the non-asymptotic performance
analysis  where the performance is usually measured by regret  e.g.  static regrets[27  28]  dynamic
regrets[29]  etc.  but most work does not consider predictions and/or dynamical systems. Further 
motivated by the applications with predictions  e.g. predictions of electricity prices in data center

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

management problems [30  31]  there is a growing interest on the effect of predictions on the online
problems [32  33  30  34  31  35  36]. However  though some papers consider switching costs which
can be viewed as a simple and special dynamical model [37  36]  there is a lack of study on the
general dynamical systems and on how predictions affect the online problem with dynamical systems.
In this paper  we propose novel gradient-based online control algorithms  receding horizon gradient-
based control (RHGC)  and provide nonasymptotic optimality guarantees by dynamic regrets. RHGC
can be based on many gradient methods  e.g. vanilla gradient descent  Nesterov’s accelerated gradient 
triple momentum  etc.  [38  39]. Due to the space limit  this paper only presents receding horizon
gradient descent (RHGD) and receding horizon triple momentum (RHTM). For the theoretical
analysis  we assume strongly convex and smooth cost functions  whereas applying RHGC does not
require these conditions. Speciﬁcally  we show that the regret bounds of RHGD and RHTM decay
exponentially with the prediction window’s size W   demonstrating that our algorithms efﬁciently
utilize the prediction. Besides  our regret bounds decrease when the system is more “agile” in the
sense of a controllability index [40]. Further  we provide a fundamental limit for any online control
algorithms and show that the fundamental lower bound almost matches the regret upper bound of
RHTM. This indicates that RHTM achieves near-optimal performance at least in the worst case. We
also provide some discussion on the classic linear quadratic tracking problems  a widely studied
control problem in literature  to provide more insightful interpretations of our results. Finally  we
numerically test our algorithms. In addition to linear systems  we also apply RHGC to a nonlinear
dynamical system: path tracking by a two-wheeled robot. Results show that RHGC works effectively
for nonlinear systems though RHGC is only presented and theoretical analyzed on LTI systems.
Results in this paper are built on a paper on online optimization with switching costs [36]. Compared
with [36]  this paper studies online optimal control with general linear dynamics  which includes
[36] as a special case; and studies how the system controllability index affects the regrets.
There has been some recent work on online optimal control problems with time-varying costs
[41  42  37  43] and/or time-varying disturbances [43]  but most papers focus on the no-prediction
cases. As we show later in this paper  these algorithms can be used in our RHGC methods as
initialization oracles. Moreover  our regret analysis shows that RHGC can reduce the regret of these
no-prediction online algorithms by a factor exponentially decaying with the prediction window’s size.
Finally  we would like to mention another related line of work: learning-based control [44  45  46 
47  48]. In some sense  the results in this paper are orthogonal to that of the learning-based control 
because the learning-based control usually considers a time-invariant environment but unknown
dynamics  and aims to learn system dynamics or optimal controllers by data; while this paper
considers a time-varying scenario with known dynamics but changing objectives and studies decision
making with limited predictions. It is an interesting future direction to combine the two lines of work
for designing more applicable algorithms.
Notations. Consider matrices A and B  A ≥ B means A − B is positive semideﬁnite and [A  B]
denotes a block matrix. The norm (cid:107) · (cid:107) refers to the L2 norm for both vectors and matrices. Let xi
denote the ith entry of the vector. Consider a set I = {k1  . . .   km}  then xI = (xk1  . . .   xkm )(cid:62) 
and A(I  :) denotes the I rows of matrix A stacked together. Let Im be an identity matrix in Rm×m.

2 Problem formulation and preliminaries

Consider a ﬁnite-horizon discrete-time optimal control problem with time-varying cost functions
ft(xt) + gt(ut) and a linear time-invariant (LTI) dynamical system:

N−1(cid:88)

t=0

min
x u

J(x  u) =

[ft(xt) + gt(ut)] + fN (xN )

(1)

s.t. xt+1 = Axt + But 

t ≥ 0
N )(cid:62)  u = (u(cid:62)
0   . . .   u(cid:62)

where xt ∈ Rn  ut ∈ Rm  x = (x(cid:62)
N−1)(cid:62)  x0 is given  fN (xN ) is the
terminal cost.1 To solve the optimal control problem (1)  all cost functions from t = 0 to t = N
are needed. However  at each time t  usually only a ﬁnite lookahead window of cost functions are
available and the decision maker needs to make an online decision ut using the available information.

1   . . .   x(cid:62)

1The results in this paper can be extended to cost ct(xt  ut) with proper assumptions.

2

In particular  we consider a simpliﬁed prediction model: at each time t  the decision maker obtains
accurate predictions for the next W time steps  ft  gt  . . .   ft+W−1  gt+W−1  but no further prediction
beyond these W steps  meaning that ft+W   gt+W   . . . can even be adversarially generated. Though
this prediction model may be too optimistic in the short term and over pessimistic in the long term 
this model i) captures a commonly observed phenomenon in predictions that short-term predictions
are usually much more accurate than the long-term predictions; ii) allows researchers to derive
insights for the role of predictions and possibly to extend to more complicated cases [31  30  49  50].
The online optimal control problem is described as follows: at each time step t = 0  1  . . .  
• the agent observes state xt and receives prediction ft  gt  . . .   ft+W−1  gt+W−1;
• the agent decides and implements a control ut and suffers the cost ft(xt) + gt(ut);
• the system evolves to the next state xt+1 = Axt + But.2

) 

s=0

(2)

An online control algorithm  denoted as A  can be deﬁned as a mapping from the prediction informa-
tion and the history information to the control action  denoted by ut(A):

ut(A) = A(xt(A)  . . .   x0(A) {fs  gs}t+W−1

t ≥ 0 
where xt(A) is the state generated by implementing A and x0(A) = x0 is given.
This paper evaluates the performance of online control algorithms by comparing against the optimal
control cost J∗ in hindsight  that is  J∗ := min{J(x  u) | xt+1 = Axt + But  ∀ t ≥ 0}.
In this paper  the performance of an online algorithm A is measured by 3

Regret(A) := J(A) − J∗ = J(x(A)  u(A)) − J∗ 

(3)
which is sometimes called as dynamic regret [29  51] or competitive difference [52]. Another popular
regret notion is the static regret  which compares the online performance with the optimal static
controller/policy [42  41]. The benchmark in static regret is weaker than that in dynamic regret
because the optimal controller may be far from being static  and it has been shown in literature that
o(N ) static regret can be achieved even without predictions (i.e.  W = 0). Thus  we will focus on the
dynamic regret analysis and study how predictions can improve the dynamic regret.
Example 1 (Linear quadratic (LQ) tracking). Consider a discrete-time tracking problem for a system
xt+1 = Axt + But. The goal is to minimize the quadratic tracking loss of a trajectory {θt}N

t=0

J(x  u) =

1
2

(xt − θt)

(cid:62)

Qt(xt − θt) + u

(cid:62)
t Rtut

(xN − θN )

(cid:62)

QN (xN − θN ).

+

1
2

t=0 a priori  what are revealed

In practice  it is usually difﬁcult to know the complete trajectory {θt}N
are usually the next few steps  making it an online control problem with predictions.
Assumptions and useful concepts. Firstly  we assume controllability  which is standard in control
theory and roughly means that the system can be steered to any state by proper control inputs [53].
Assumption 1. The LTI system xt+1 = Axt + But is controllable.
It is well-known that any controllable LTI system can be linearly transformed to a canonical form
[40] and the linear transformation can be computed efﬁciently a priori using A and B  which can
further be used to reformulate the cost functions ft  gt. Thus  without loss of generality  this paper
only considers LTI systems in the canonical form  deﬁned as follows.
Deﬁnition 1 (Canonical form). A system xt+1 = Axt + But is said to be in the canonical form if

N−1(cid:88)

(cid:104)

t=0

(cid:105)





0

...

0 ...

0
1
0

...
...
0 ···
0
...
···
1 ···
···
···

...
0
···
0 ···
...
...
0 ···1

0



 



0

1

0

0 1

...

...

...
∗ ∗ ··· ∗ ∗ ∗ ... ∗
0
...

...

0 1

...

···

∗

0 1

···

∗ ∗ ··· ∗ ∗ ∗ ··· ∗ ··· ∗ ···
···

∗
0 1 ··· 0
...
...
...
0 1
∗ ∗ ··· ∗ ∗ ∗ ··· ∗ ··· ∗ ∗ ··· ∗

A =

  B =

2We assume known A  B  no process noises  state feedback  and leave relaxing assumptions as future work.
3The optimality gap depends on initial state x0 and {ft  gt}N
t=0  but we omit them for simplicity of notation.

3

where each * represents a (possibly) nonzero entry  and the rows of B with 1 are the same rows of A
with * and the indices of these rows are denoted as {k1  . . .   km} =: I. Moreover  let pi = ki − ki−1
for 1 ≤ i ≤ m  where k0 = 0. The controllability index of a canonical-form (A  B) is deﬁned as

p = max{p1  . . .   pm}.

Next  we introduce assumptions on the cost functions and their optimal solutions.
Assumption 2. Assume ft is µf strongly convex and lf Lipschitz smooth for 0 ≤ t ≤ N  and gt is
convex and lg Lipschitz smooth for 0 ≤ t ≤ N − 1 for some µf   lf   lg > 0.
Assumption 3. Assume the minimizers to ft  gt  denoted as θt = arg minx ft(x)  ξt =
arg minu gt(u)  are uniformly bounded  i.e. there exist ¯θ  ¯ξ such that (cid:107)θt(cid:107) ≤ ¯θ  (cid:107)ξt(cid:107) ≤ ¯ξ  ∀ t.
These assumptions are commonly adopted in convex analysis. The uniform bounds rule out extreme
cases. Notice that the LQ tracking problem in Example 1 satisﬁes Assumption 2 and 3 if Qt  Rt are
positive deﬁnite with uniform bounds on the eigenvalues and if θt are uniformly bounded for all t.

3 Online control algorithms: receding horizon gradient-based control

This section introduces our online control algorithms  receding horizon gradient-based control
(RHGC). The design is by ﬁrst converting the online control problem to an equivalent online
optimization problem with ﬁnite temporal-coupling costs  then designing gradient-based online
optimization algorithms by utilizing this ﬁnite temporal-coupling property.

3.1 Problem transformation

Firstly  we notice that the ofﬂine optimal control problem (1) can be viewed as an optimization with
equality constraints over x and u. The individual stage cost ft(xt) + gt(ut) only depends on the
current xt and ut but the equality constraints couple xt  ut with xt+1 for each t. In the following  we
will rewrite (1) in an equivalent form of an unconstrained optimization problem on some entries of
xt for all t  but the new stage cost at each time t will depend on these new entries across a few nearby
time steps. We will harness this structure to design our online algorithm.
In particular  the entries of xt adopted in the reformulation are: xk1
{k1  . . .   km} is deﬁned in Deﬁnition 1. For ease of notation  we deﬁne

  where I =

t   . . .   xkm

t

(cid:124)

(cid:123)(cid:122)

p1

(cid:125)

(cid:124)

(cid:123)(cid:122)

p2

(cid:125)

)(cid:62) 
t where j = 1  . . .   m. Let z := (z(cid:62)

t   . . .   xkm

zt := (xk1

t

t = xkj

and write zj
equality constraint xt = Axt−1 + But−1  we have xi
zt−p+1  . . .   zt in the following way:
  z2

t−p1+1  . . .   z1

t−p2+1  . . .   z2

xt = (z1

t

t

  . . .   zm

t = xi+1

t ≥ 0

(4)
N )(cid:62). By the canonical-form
1   . . .   . . .   z(cid:62)
t−1 for i (cid:54)∈ I  so xt can be represented by
(cid:124)

t−pm+1  . . .   zm

t ≥ 0 

(cid:123)(cid:122)

)(cid:62) 

(cid:125)

(5)

t

pm

where zt for t ≤ 0 is determined by x0 in a way to let (5) hold for t = 0. For ease of exposition and
without loss of generality  we consider x0 = 0 in this paper; then we have zt = 0 for t ≤ 0. Similarly 
ut can be determined by zt−p+1  . . .   zt  zt+1 by
ut = zt+1 − A(I  :)xt = zt+1 − A(I  :)(z1
t ≥ 0 (6)
where A(I  :) consists of k1  . . .   km rows of A.
It is straightforward to verify that equations (4  5  6) describe a bijective transformation between
{(x  u) | xt+1 = Axt +But} and z ∈ RmN   since the LTI constraint xt+1 = Axt +But is naturally
embedded in the relation (5  6). Therefore  based on the transformation  an optimization problem with
respect to z ∈ RmN can be designed to be equivalent with (1). Notice that the resulting optimization
problem has no constraint on z. Moreover  the cost functions on z can be obtained by substituting (5 
6) into ft(xt) and gt(ut)  i.e. ˜ft(zt−p+1  . . .   zt) := ft(xt) and ˜gt(zt−p+1  . . .   zt  zt+1) := gt(ut).
Correspondingly  the objective function of the equivalent optimization with respect to z is

t−pm+1  . . .   zm

t−p1+1  . . .   z1

t   . . .   zm

t )(cid:62) 

N(cid:88)

N−1(cid:88)

C(z) :=

˜ft(zt−p+1  . . .   zt) +

˜gt(zt−p+1  . . .   zt+1)

(7)

C(z) has many nice properties  some of which are formally stated below.

t=0

t=0

4

Lemma 1. The function C(z) has the following properties:

i) C(z) is µc = µf strongly convex and lc smooth for lc = plf + (p + 1)lg(cid:107)[Im −A(I  :)](cid:107)2.
ii) For any (x  u) s.t. xt+1 = Axt+But  C(z) = J(x  u) where z is deﬁned in (4). Conversely 

∀ z  the (x  u) determined by (5 6) satisﬁes xt+1 = Axt + But and J(x  u) = C(z);

iii) Each stage cost ˜ft + ˜gt in (7) only depends on zt−p+1  . . .   zt+1.

Property ii) implies that any online algorithm for deciding z can be translated to an online algorithm
for x and u by (5  6) with the same costs. Property iii) highlights one nice property  ﬁnite temporal-
coupling  of C(z)  which serves as a foundation for our online algorithm design.
Example 2. For illustration  consider the following dynamical system with n = 2  m = 1:

(cid:20) x1

(cid:21)

(cid:20) 0

+

=

t+1

t+1

ut

a1

x2

t = x2

(8)
t−1 and xt = (zt−1  zt)(cid:62).
t+1 − A(I  :)xt = zt+1 − A(I  :)(zt−1  zt)(cid:62). Hence  ˜ft(zt−1  zt) = ft(xt) =

t
x2
t
Here  k1 = 2  I = {2}  A(I  :) = (a1  a2)  and zt = x2
t . By (8)  x1
Similarly  ut = x2
ft((zt−1  zt)(cid:62))  ˜gt(zt−1  zt  zt+1) = gt(ut) = gt(zt+1 − A(I  :)(zt−1  zt)(cid:62)).
Remark 1. This paper considers a reparameterization method with respect to states x via the canonical
form  and it might be interesting to compare it with the more direct reparameterization with respect
to control inputs u. The control-based reparameterization has been discussed in literature [54]. It
has been observed in [54] that when A is not stable  the condition number of the cost function
derived from the control-based reparameterization goes to inﬁnity as W → +∞  which may result in
computation issues when W is large. However  the state-based reparameterization considered in this
paper can guarantee bounded condition number for all W even for unstable A  as shown in Lemma 1.
This is one major advantage of the state-based reparameterization method considered in this paper.

(cid:21)(cid:20) x1

(cid:21)

1
a2

(cid:21)

(cid:20) 0

1

3.2 Online algorithm design: RHGC

This section introduces our RHGC based on the reformulation (7) and inspired by [36]. As mentioned
earlier  any online algorithm for zt can be translated to an online algorithm for xt  ut. Hence  we
will focus on designing an online algorithm for zt in the following. By the ﬁnite temporal-coupling
property of C(z)  the partial gradient of the total cost C(z) only depends on the ﬁnite neighboring
stage costs { ˜fτ   ˜gτ}t+p−1

and ﬁnite neighboring stage variables (zt−p  . . .   zt+p) =: zt−p:t+p.

τ =t

t+p−1(cid:88)

τ =t

t+p−1(cid:88)

τ =t−1

∂C
∂zt

(z) =

∂ ˜fτ
∂zt

(zτ−p+1  . . .   zτ ) +

∂˜gτ
∂zt

(zτ−p+1  . . .   zτ +1)

Without causing any confusion  we use ∂C
(z) for highlighting the local
∂zt
dependence. Thanks to the local dependence  despite the fact that not all the future costs are available 
it is still possible to compute the partial gradient of the total cost by using only a ﬁnite lookahead
window of the cost functions. This observation motivates the design of our receding horizon gradient-
based control (RHGC) methods  which are the online implementation of gradient methods  such as
vanilla gradient descent  Nesterov’s accelerated gradient  triple momentum  etc.  [38  39].

(zt−p:t+p) to denote ∂C
∂zt

p (cid:99)  stepsize γg  initialization oracle ϕ.

Algorithm 1: Receding Horizon Gradient Descent (RHGD)
1: inputs: Canonical form (A  B)  W ≥ 1  K = (cid:98) W−1
2: for t = 1 − W : N − 1 do
3:
4:
5:

Step 1: initialize zt+W (0) by oracle ϕ.
for j = 1  . . .   K do

Step 2: update zt+W−jp(j) by gradient descent
zt+W−jp(j) = zt+W−jp(j − 1) − γg

(zt+W−(j+1)p:t+W−(j−1)p(j − 1)).
end for
Step 3: compute ut by zt+1(K) and the observed state xt: ut = zt+1(K) − A(I  :)xt

∂zt+W −jp

∂C

6:
7:
8: end for

Firstly  we illustrate the main idea of RHGC by receding horizon gradient descent (RHGD) based
on vanilla gradient descent. In RHGD (Algorithm 1)  index j refers to the iteration number of the

5

s=0

corresponding gradient update of C(z). There are two major steps to decide zt. Step 1 is initializing
the decision variables z(0). Here  we do not restrict the initialization algorithm ϕ and allow any
oracle/online algorithm without using lookahead information  i.e. zt+W (0) is selected based only on
the information up to t + W − 1: zt+W (0) = ϕ({ ˜fs  ˜gs}t+W−1
). One example of ϕ will be provided
in Section 4. Step 2 is using the W -lookahead costs to conduct gradient updates. Notice that the
gradient update from zτ (j−1) to zτ (j) is implemented in a backward order of τ  i.e. from τ = t+W
requires the local decision variables zt−p:t+p  given
to τ = t. Moreover  since the partial gradient ∂C
∂zt
W -lookahead information  RHGD can only conduct K = (cid:98) W−1
p (cid:99) iterations of gradient descent for
the total cost C(z). For more discussion  we refer the reader to [36] for the p = 1 case.
In addition to RHGD  RHGC can also incorporate accelerated gradient methods in the same way  such
as Nesterov’s accelerated gradient and triple momentum. For the space limit  we only formally present
receding horizon triple momentum (RHTM) in Algorithm 2 based on triple momentum [39]. RHTM
also consists of two major steps when determining zt: initialization and gradient updates based
on the lookahead window. The two major differences from RHGD are that the decision variables
in RHTM include not only z(j) but also auxiliary variables ω(j) and y(j)  which are adopted in
triple momentum to accelerate the convergence  and that the gradient update is by triple momentum
p (cid:99) iterations of triple
instead of gradient descent. Nevertheless  RHTM can also conduct K = (cid:98) W−1
momentum for C(z) since the triple momentum update requires the same neighboring cost functions.
Though it appears that RHTM does not fully exploit the lookahead information since only a few
gradient updates are used  in Section 5  we show that RHTM achieves near-optimal performance with
respect to W   which means that RHTM successfully extracts and utilizes the prediction information.
Finally  we brieﬂy introduce MPC[55] and suboptimal MPC[23]  and compare them with our
algorithms. MPC tries to solve a W -stage optimization at each t and implements the ﬁrst control input.
Suboptimal MPC  as a variant of MPC aiming at reducing computation  conducts an optimization
method only for a few iterations without solving the optimization completely. Our algorithm’s
computation time is similar to that of suboptimal MPC with a few gradient iterations. However 
the major difference between our algorithm and suboptimal MPC is that suboptimal MPC conducts
gradient updates for a truncated W -stage optimal control problem based on W -lookahead information 
while our algorithm is able to conduct gradient updates for the complete N-stage optimal control
problem based on the same W -lookahead information by utilizing the reformulation (4  5  6  7).

4 Regret upper bounds

Because our RHTM (RHGD) is designed to exactly implement the triple momentum (gradient
descent) of C(z) for K iterations  it is straightforward to have the following regret guarantees that
connect the regrets of RHTM and RHGD with the regret of the initialization oracle ϕ 

Algorithm 2: Receding Horizon Triple Momentum (RHTM)

inputs: Canonical form (A  B)  W ≥ 1  K = (cid:98) W−1
for t = 1 − W : N − 1 do

p (cid:99)  stepsizes γc  γz  γω  γy > 0  oracle ϕ.
Step 1: initialize zt+W (0) by oracle ϕ  then let ωt+W (−1)  ωt+W (0)  yt+W (0) be zt+W (0)
for j = 1  . . .   K do

Step 2: update ωt+W−jp(j)  yt+W−jp(j)  zt+W−jp(j) by triple momentum.
ωt+W−jp(j) = (1 + γω)ωt+W−jp(j − 1) − γωωt+W−jp(j − 2)

− γc

(yt+W−(j+1)p:t+W−(j−1)p(j − 1))

∂C

∂yt+W−jp

yt+W−jp(j) = (1 + γy)ωt+W−jp(j) − γyωt+W−jp(j − 1)
zt+W−jp(j) = (1 + γz)ωt+W−jp(j) − γzωt+W−jp(j − 1)

end for
Step 3: compute ut by zt+1(K) and the observed state xt: ut = zt+1(K) − A(I  :)xt

end for

6

(cid:18)√

ζ − 1√

(cid:19)2K

Theorem 1. Consider W ≥ 1 and stepsizes γg = 1
γz = φ2

(1+φ)(2−φ)  
ζ  and let ζ = lc/µc denote C(z)’s condition number. For any oracle ϕ 

  γc = 1+φ
lc

2−φ   γy =

  γω = φ2

√

φ2

lc

(cid:18) ζ − 1

(cid:19)K

1−φ2   φ = 1 − 1/
Regret(RHGD) ≤ ζ
where K = (cid:98) W−1

ζ

ζ

Regret(ϕ)  Regret(RHT M ) ≤ ζ 2

ζ )K and ζ 2(

p (cid:99) gradient updates. Moreover  the factors decay exponentially with K = (cid:98) W−1

Regret(ϕ)
p (cid:99)  Regret(ϕ) is the regret of the initial controller: ut(0) = zt+1(0)− A(I  :)xt(0).
Theorem 1 suggests that for any online algorithm ϕ without predictions  RHGD and RHTM can use
√
ζ−1√
predictions to lower the regret by a factor of ζ( ζ−1
ζ )2K respectively via additional
K = (cid:98) W−1
p (cid:99)  and K
almost linearly increases with W . This indicates that RHGD and RHTM improve the performance
exponentially fast with an increase in the prediction window W for any initialization method. In
addition  K = (cid:98) W−1
p (cid:99) decreases with p  implying that the regrets increase with the controllability
index p (Deﬁnition 1). This is intuitive because p roughly indicates how fast the controller can
inﬂuence the system state effectively: the larger the p is  the longer it takes. To see this  consider
Example 2. Since ut−1 does not directly affect x1
t to a
desirable value. Finally  RHTM’s regret decays faster than RHGD’s  which is intuitive because triple
momentum converges faster than gradient descent. Thus  we will focus on RHTM in the following.
An initialization method: follow the optimal steady state (FOSS). To complete the regret analysis
for RHTM  we provide a simple initialization method  FOSS  and its dynamic regret bound. As
mentioned before  any online control algorithm without predictions  e.g. [42  41]  can be applied as
an initialization oracle ϕ. However  most literature study static regrets rather than dynamic regrets.
Deﬁnition 2 (Follow the optimal steady state (FOSS)). The optimal steady state for stage cost
f (x) + g(u) refers to (xe  ue) := arg minx=Ax+Bu(f (x) + g(u)).
Follow the optimal steady state algorithm (FOSS) ﬁrst solves the optimal steady state (xe
cost ft(x) + gt(u)  then determines zt+1 by xe

t   it takes at least p = 2 steps to change x1

t   ue
)(cid:62) at each t + 1.

t   i.e. zt+1 = (xe k1

  . . .   xe km

t ) for

t

t

(cid:80)N−1

FOSS is motivated by the fact that the optimal steady state cost is the optimal inﬁnite-horizon
average cost for LTI systems with time-invariant cost functions [56]  so FOSS should yield acceptable
performance at least for slowly changing cost functions. Nevertheless  we admit that FOSS is
proposed mainly for analytical purposes and other online algorithms may outperform FOSS in various
perspectives. The following is a regret bound for FOSS  relying on the solution to Bellman equations.
Deﬁnition 3 (Solution to the Bellman equations [57]). Consider optimal control problem:
t=0 (f (xt) + g(ut)) where xt+1 = Axt + But. Let λe be the optimal steady
min limN→+∞ 1
N
state cost f (xe) + g(ue)  which is also the optimal inﬁnite-horizon average cost [56]. The Bellman
equations for the problem is he(x) + λe = minu(f (x) + g(u) + he(Ax + Bu)). The solution to
the Bellman equations  denoted by he(x)  is sometimes called as a bias function [57]. To ensure the
uniqueness of the solution  some extra conditions  e.g. he(0) = 0  are usually imposed.
Theorem 2 (Regret bound of FOSS). Let (xe
the bias function with respect to cost ft(x) + gt(u) respectively for 0 ≤ t ≤ N − 1. Suppose he
exists for 0 ≤ t ≤ N − 1 4 then the regret of FOSS can be bounded by

t (x) denote the optimal steady state and
t (x)

t ) and he

t   ue

((cid:107)xe

t (x∗
t ))

t−1(x∗

t(cid:107) + he

t−1 − xe

Regret(FOSS) = O

N (x) = fN (x)  xe

t ) − he
t}N
t=0 denotes the optimal state trajectory for (1)  xe−1 = x∗

where {x∗
0  he
ization FOSS is Regret(RHTM) = O

0 = x0 = 0  he−1(x) =
N = θN . Consequently  by Theorem 1  the regret bound of RHTM with initial-
.

t(cid:107) + he
Theorem 2 bounds the regret by the variation of the optimal steady states xe
t and the bias functions
0(cid:107) +
t . If ft and gt do not change  xe
he
t exists  the existence is guaranteed
0(x0))  matching our intuition. Though Theorem 2 requires he
he
for many control problems  e.g. LQ tracking and control problems with turnpike properties [58  22].

t do not change  yielding a small O(1) regret  i.e. O((cid:107)xe

ζ )2K(cid:80)N

t−1 − xe

t=0((cid:107)xe

t ) − he

t−1(x∗

t (x∗
t ))

t and he

√
ζ−1√

(cid:17)

ζ 2(

 

(cid:32) N(cid:88)
(cid:16)

t=0

(cid:33)

4he

t may not be unique  so extra conditions can be imposed on he

t for more interesting regret bounds.

7

5 Linear quadratic tracking: regret upper bounds and a fundamental limit

To provide more intuitive meaning for our regret analysis in Theorem 1 and Theorem 2  we apply
RHTM to the LQ tracking problem in Example 1. Results for the time varying Qt  Rt  θt are provided
in Appendix E; whereas here we focus on a special case which gives clean expressions for regret
bounds: both an upper bound for RHTM with initialization FOSS and a lower bound for any online
algorithm. Further  we show that the lower bound and the upper bound almost match each other 
implying that our online algorithm RHTM uses the predictions in a nearly optimal way even though
it only conducts a few gradient updates at each time step .
The special case of LQ tracking problems is in the following form 

N−1(cid:88)

t=0

1
2

(cid:2)(xt − θt)(cid:62)Q(xt − θt) + u(cid:62)

t Rut

(cid:3) +

x(cid:62)
N P exN  

1
2

(9)

where Q > 0  R > 0  and P e is the solution to the algebraic Riccati equation with respect to Q  R
[59]. Basically  in this special case  Qt = Q  Rt = R for 0 ≤ t ≤ N − 1  QN = P e  θN = 0  and
only θt changes for t = 0  1  . . .   N − 1. The LQ tracking problem (9) aims to follow a time-varying
trajectory {θt} with constant weights on the tracking cost and the control cost.
Regret upper bound. Firstly  based on Theorem 1 and Theorem 2  we have the following bound.
Corollary 1. Under the stepsizes in Theorem 1  RHTM with FOSS as the initialization rule satisﬁes

Regret(RHT M ) = O

ζ 2(

(cid:32)

√

ζ − 1√

ζ

)2K

N(cid:88)

t=0

(cid:33)

(cid:107)θt − θt−1(cid:107)

where K = (cid:98)(W − 1)/p(cid:99)  ζ is the condition number of the corresponding C(z)  θ−1 = 0.
This corollary shows that the regret can be bounded by the total variation of θt for constant Q  R.
Fundamental limit. For any online algorithm  we have the following lower bound.
Theorem 3 (Lower Bound). Consider 1 ≤ W ≤ N/3  any condition number ζ > 1  any variation
budget 4¯θ ≤ LN ≤ (2N + 1)¯θ  and any controllability index p ≥ 1. For any online algorithm
A  there exists an LQ tracking problem in form (9) where i) the canonical-form system (A  B) has
t=0 (cid:107)θt − θt−1(cid:107) ≤ LN  
and iii) the corresponding C(z) has condition number ζ  such that the following lower bound holds

controllability index p  ii) the sequence {θt} satisﬁes the variation budget(cid:80)N
N(cid:88)

(cid:32)

(cid:33)

(cid:18)

(cid:19)

)2KLN

ζ − 1
ζ + 1

(cid:107)θt − θt−1(cid:107)

Regret(A) = Ω

ζ − 1
ζ + 1
where K = (cid:98)(W − 1)/p(cid:99) and θ−1 = 0.
Firstly  the lower bound in Theorem 3 almost matches the upper bound in Corollary 1  especially
when ζ is large  demonstrating that RHTM utilizes the predictions in a near-optimal way. The major
conditions in Theorem 3 require that the prediction window is short compared with the horizon:
W ≤ N/3  and the variation of the cost functions should not be too small: LN ≥ 4¯θ  otherwise the
online control problem is too easy and the regret can be very small. Moreover  the small gap between
the regret bounds is conjectured to be nontrivial  because this gap coincides with the long lasting gap
in the convergence rate of the ﬁrst-order algorithms for strongly convex and smooth optimization. In
particular  the lower bound in Theorem 3 matches the fundamental convergence limit in [38]  and the
upper bound is by triple momentum’s convergence rate  which is the best one to our knowledge.

(10)

= Ω

(

√
√

(

√
√

)2K

t=0

6 Numerical experiments

LQ tracking problem in Example 1. The system considered here has n = 2  m = 1  and p = 2.
More details of the experiment settings are provided in Appendix H. We compare RHGC with a
suboptimal MPC algorithm  fast gradient MPC (subMPC) [23]. Roughly speaking  subMPC solves
the W -stage truncated optimal control from t to t + W − 1 by Nesterov’s accelerated gradient
[38]  and one iteration of Nesterov’s accelerated gradient requires 2W gradient evaluations of stage

8

Figure 1: Regret for LQ tracking.

Figure 2: Two-wheel robot tracking with nonlinear dynamics.

cost function since W stages are considered and each stage has two costs ft and gt. This implies
that  in terms of the number of gradient evaluations  subMPC with one iteration corresponds to our
RHTM because RHTM also requires roughly 2W gradient evaluations per stage. Therefore  Figure 1
compares our RHGC algorithms with subMPC with one iteration. Figure 1 also plots subMPC with 3
and 5 iterations for more insights. Besides  Figure 1 plots not only RHGD and RHTM  but also RHAG 
which is based on Nesterov’s accelerated gradient. Figure 1 shows that all our algorithms achieve
exponential decaying regrets with respect to W   and the regrets are piecewise constant  matching
Theorem 1. Further  it is observed that RHTM and RHAG perform better than RHGD  which is
intuitive because triple momentum and Nesterov’s accelerated gradient are accelerated versions of
gradient descent. In addition  our algorithms are much better than subMPC with 1 iteration  implying
that our algorithms utilize the lookahead information more efﬁciently given similar computational
time. Finally  subMPC achieves better performance by increasing the iteration number but the
improvement saturates as W increases  in contrast to the steady improvement of RHGC.
Path tracking for a two-wheel mobile robot. Though we presented our online algorithms on an
LTI system  our RHGC methods are applicable to some nonlinear systems as well. Here we consider
a two-wheel mobile robot with nonlinear kinematic dynamics ˙x = v cos δ  ˙y = v sin δ  ˙δ = w
where (x  y) is the robot location  v and w are the tangential and angular velocities respec-
tively  δ denotes the tangent angle between v and the x axis [60]. The control is directly on
the v and w  e.g.  via the pulse-width modulation (PWM) of the motor [61]. Given a refer-
t )  the objective is to balance the tracking performance and the control cost  i.e. 
ence path (xr

t · (wt)2(cid:3). We discretize the dynamics

t )2(cid:1) + cv

(cid:2)ct ·(cid:0)(xt − xr

t   yr

min (cid:80)N

t=0

t )2 + (yt − yr

t · (vt)2 + cw

with time interval ∆t = 0.025s; then follow similar ideas in this paper to reformulate the optimal
path tracking problem to an unconstrained optimization with respect to (xt  yt) and apply RHGC.
See Appendix H for details. Figure 2 plots the tracking results with window W = 40 and W = 80
corresponding to lookahead time 1s and 2s. A video showing the dynamic processes with different W
is provided at https://youtu.be/fal56LTBD1s. It is observed that the robot follows the reference
trajectory well especially when the path is smooth but deviates a little more when the path has sharp
turns  and a longer lookahead window leads to better tracking performance. These results conﬁrm
that our RHGC works effectively on nonlinear systems.

7 Conclusion

This paper studies the role of predictions on dynamic regrets of online control problems with linear
dynamics. We design RHGC algorithms and provide regret upper bounds of two speciﬁc algorithms:
RHGD and RHTM. We also provide a fundamental limit and show the fundamental limit almost
matches RHTM’s upper bound. This paper leads to many interesting future directions  some of which
are brieﬂy discussed below. The ﬁrst direction is to study more realistic prediction models which
considers random prediction noises  e.g. [33  35  62]. The second direction is to consider unknown
systems with process noises  possibly by applying learning-based control tools [44  46  48]. Further 
more studies could be conducted on general control problems including nonlinear control and control
with input and state constraints. Besides  it is interesting to consider other performance metrics 
such as competitive ratio  since the dynamic regret is non-vanishing. Finally  other future directions
include closing the gap of the regret bounds and more discussion on the effect of the canonical-form
transformation on the condition number.

9

2468101214Prediction W-10-5059log(regret)RHGDRHAGRHTMsubMPC Iter = 1subMPC Iter = 3subMPC Iter = 5-20-1001020X-20-1001020YW = 40referencerobot path-20-1001020X-20-1001020YW = 80referencerobot pathAcknowledgement

This work was supported by NSF Career 1553407  ARPA-E NODES  AFOSR YIP and ONR YIP
programs.

References

[1] Nevena Lazic  Craig Boutilier  Tyler Lu  Eehern Wong  Binz Roy  MK Ryu  and Greg Imwalle.
Data center cooling using model-predictive control. In Advances in Neural Information Pro-
cessing Systems  pages 3814–3823  2018.

[2] Wei Xu  Xiaoyun Zhu  Sharad Singhal  and Zhikui Wang. Predictive control for dynamic
resource allocation in enterprise data centers. In 2006 IEEE/IFIP Network Operations and
Management Symposium NOMS 2006  pages 115–126. IEEE  2006.

[3] Tomas Baca  Daniel Hert  Giuseppe Loianno  Martin Saska  and Vijay Kumar. Model predictive
trajectory tracking and collision avoidance for reliable outdoor deployment of unmanned aerial
vehicles. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 
pages 6753–6760. IEEE  2018.

[4] Jackeline Rios-Torres and Andreas A Malikopoulos. A survey on the coordination of connected
and automated vehicles at intersections and merging at highway on-ramps. IEEE Transactions
on Intelligent Transportation Systems  18(5):1066–1077  2016.

[5] Kyoung-Dae Kim and Panganamala Ramana Kumar. An mpc-based approach to provable
system-wide safety and liveness of autonomous ground trafﬁc. IEEE Transactions on Automatic
Control  59(12):3341–3356  2014.

[6] Samir Kouro  Patricio Cortés  René Vargas  Ulrich Ammann  and José Rodríguez. Model pre-
dictive control—a simple and powerful method to control power converters. IEEE Transactions
on industrial electronics  56(6):1826–1838  2008.

[7] Edgar Perea-Lopez  B Erik Ydstie  and Ignacio E Grossmann. A model predictive control
strategy for supply chain optimization. Computers & Chemical Engineering  27(8-9):1201–
1218  2003.

[8] Wenlin Wang  Daniel E Rivera  and Karl G Kempf. Model predictive control strategies for
supply chain management in semiconductor manufacturing. International Journal of Production
Economics  107(1):56–77  2007.

[9] Moritz Diehl  Rishi Amrit  and James B Rawlings. A lyapunov function for economic optimizing

model predictive control. IEEE Transactions on Automatic Control  56(3):703–707  2010.

[10] Matthias A Müller and Frank Allgöwer. Economic and distributed model predictive control:
Recent developments in optimization-based control. SICE Journal of Control  Measurement 
and System Integration  10(2):39–52  2017.

[11] Matthew Ellis  Helen Durand  and Panagiotis D Christoﬁdes. A tutorial review of economic

model predictive control methods. Journal of Process Control  24(8):1156–1178  2014.

[12] Antonio Ferramosca  James B Rawlings  Daniel Limón  and Eduardo F Camacho. Economic
mpc for a changing economic criterion. In 49th IEEE Conference on Decision and Control
(CDC)  pages 6131–6136. IEEE  2010.

[13] Matthew Ellis and Panagiotis D Christoﬁdes. Economic model predictive control with time-
varying objective function for nonlinear process systems. AIChE Journal  60(2):507–519 
2014.

[14] David Angeli  Alessandro Casavola  and Francesco Tedesco. Theoretical advances on economic
model predictive control with time-varying costs. Annual Reviews in Control  41:218–224 
2016.

[15] Rishi Amrit  James B Rawlings  and David Angeli. Economic optimization using model

predictive control with a terminal cost. Annual Reviews in Control  35(2):178–186  2011.

[16] Lars Grüne. Economic receding horizon control without terminal constraints. Automatica 

49(3):725–734  2013.

10

[17] David Angeli  Rishi Amrit  and James B Rawlings. On average performance and stability of
economic model predictive control. IEEE transactions on automatic control  57(7):1615–1626 
2012.

[18] Lars Grüne and Marleen Stieler. Asymptotic stability and transient optimality of economic mpc

without terminal conditions. Journal of Process Control  24(8):1187–1196  2014.

[19] Lars Grüne and Anastasia Panin. On non-averaged performance of economic mpc with terminal
conditions. In 2015 54th IEEE Conference on Decision and Control (CDC)  pages 4332–4337.
IEEE  2015.

[20] Antonio Ferramosca  Daniel Limon  and Eduardo F Camacho. Economic mpc for a changing
economic criterion for linear systems. IEEE Transactions on Automatic Control  59(10):2657–
2667  2014.

[21] Lars Grüne and Simon Pirkelmann. Closed-loop performance analysis for economic model
predictive control of time-varying systems. In 2017 IEEE 56th Annual Conference on Decision
and Control (CDC)  pages 5563–5569. IEEE  2017.

[22] Lars Grüne and Simon Pirkelmann. Economic model predictive control for time-varying system:

Performance and stability results. Optimal Control Applications and Methods  2018.

[23] Melanie Nicole Zeilinger  Colin Neil Jones  and Manfred Morari. Real-time suboptimal
model predictive control using a combination of explicit mpc and online optimization. IEEE
Transactions on Automatic Control  56(7):1524–1534  2011.

[24] Yang Wang and Stephen Boyd. Fast model predictive control using online optimization. IEEE

Transactions on Control Systems Technology  18(2):267–278  2010.

[25] Knut Graichen and Andreas Kugi. Stability and incremental improvement of suboptimal mpc
without terminal constraints. IEEE Transactions on Automatic Control  55(11):2576–2580 
2010.

[26] Douglas A Allan  Cuyler N Bates  Michael J Risbeck  and James B Rawlings. On the inherent
robustness of optimal and suboptimal nonlinear mpc. Systems & Control Letters  106:68–78 
2017.

[27] E. Hazan. Introduction to Online Convex Optimization. Foundations and Trends(r) in Optimiza-

tion Series. Now Publishers  2016.

[28] S. Shalev-Shwartz. Online Learning and Online Convex Optimization. Foundations and

Trends(r) in Machine Learning. Now Publishers  2012.

[29] Ali Jadbabaie  Alexander Rakhlin  Shahin Shahrampour  and Karthik Sridharan. Online
optimization: Competing with dynamic comparators. In Artiﬁcial Intelligence and Statistics 
pages 398–406  2015.

[30] Minghong Lin  Adam Wierman  Lachlan LH Andrew  and Eno Thereska. Dynamic right-
sizing for power-proportional data centers. IEEE/ACM Transactions on Networking (TON) 
21(5):1378–1391  2013.

[31] Minghong Lin  Zhenhua Liu  Adam Wierman  and Lachlan LH Andrew. Online algorithms
for geographical load balancing. In Green Computing Conference (IGCC)  2012 International 
pages 1–10. IEEE  2012.

[32] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In

Conference on Learning Theory  pages 993–1019  2013.

[33] Niangjun Chen  Anish Agarwal  Adam Wierman  Siddharth Barman  and Lachlan LH Andrew.
Online convex optimization using predictions. In ACM SIGMETRICS Performance Evaluation
Review  volume 43  pages 191–204. ACM  2015.

[34] Masoud Badiei  Na Li  and Adam Wierman. Online convex optimization with ramp constraints.
In Decision and Control (CDC)  2015 IEEE 54th Annual Conference on  pages 6730–6736.
IEEE  2015.

[35] Niangjun Chen  Joshua Comden  Zhenhua Liu  Anshul Gandhi  and Adam Wierman. Using
predictions in online optimization: Looking forward with an eye on the past. In Proceedings
of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of
Computer Science  pages 193–206. ACM  2016.

11

[36] Yingying Li  Guannan Qu  and Na Li. Online optimization with predictions and switching costs:

Fast algorithms and the fundamental limit. arXiv preprint arXiv:1801.07780  2018.

[37] Gautam Goel and Adam Wierman. An online algorithm for smoothed regression and lqr control.
In The 22nd International Conference on Artiﬁcial Intelligence and Statistics  pages 2504–2513 
2019.

[38] Yurii Nesterov. Introductory lectures on convex optimization: A basic course  volume 87.

Springer Science & Business Media  2013.

[39] Bryan Van Scoy  Randy A Freeman  and Kevin M Lynch. The fastest known globally convergent
ﬁrst-order method for minimizing strongly convex functions. IEEE Control Systems Letters 
2(1):49–54  2017.

[40] David Luenberger. Canonical forms for linear multivariable systems. IEEE Transactions on

Automatic Control  12(3):290–293  1967.

[41] Yasin Abbasi-Yadkori  Peter Bartlett  and Varun Kanade. Tracking adversarial targets. In

International Conference on Machine Learning  pages 369–377  2014.

[42] Alon Cohen  Avinatan Hasidim  Tomer Koren  Nevena Lazic  Yishay Mansour  and Kunal
Talwar. Online linear quadratic control. In International Conference on Machine Learning 
pages 1028–1037  2018.

[43] Naman Agarwal  Brian Bullins  Elad Hazan  Sham Kakade  and Karan Singh. Online control
with adversarial disturbances. In International Conference on Machine Learning  pages 111–
119  2019.

[44] Sarah Dean  Horia Mania  Nikolai Matni  Benjamin Recht  and Stephen Tu. On the sample

complexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688  2017.

[45] Sarah Dean  Horia Mania  Nikolai Matni  Benjamin Recht  and Stephen Tu. Regret bounds for
robust adaptive control of the linear quadratic regulator. In Advances in Neural Information
Processing Systems  pages 4188–4197  2018.

[46] Stephen Tu and Benjamin Recht. Least-squares temporal difference learning for the linear

quadratic regulator. arXiv preprint arXiv:1712.08642  2017.

[47] Kyriakos G Vamvoudakis and Frank L Lewis. Online actor–critic algorithm to solve the
continuous-time inﬁnite horizon optimal control problem. Automatica  46(5):878–888  2010.
[48] Yi Ouyang  Mukul Gagrani  and Rahul Jain. Learning-based control of unknown linear systems

with thompson sampling. arXiv preprint arXiv:1709.04047  2017.

[49] Lian Lu  Jinlong Tu  Chi-Kin Chau  Minghua Chen  and Xiaojun Lin. Online energy generation
scheduling for microgrids with intermittent energy sources and co-generation  volume 41. ACM 
2013.

[50] Allan Borodin  Nathan Linial  and Michael E Saks. An optimal on-line algorithm for metrical

task system. Journal of the ACM (JACM)  39(4):745–763  1992.

[51] Aryan Mokhtari  Shahin Shahrampour  Ali Jadbabaie  and Alejandro Ribeiro. Online optimiza-
tion in dynamic environments: Improved regret rates for strongly convex problems. In 2016
IEEE 55th Conference on Decision and Control (CDC)  pages 7195–7201. IEEE  2016.

[52] Lachlan Andrew  Siddharth Barman  Katrina Ligett  Minghong Lin  Adam Meyerson  Alan
Roytman  and Adam Wierman. A tale of two metrics: Simultaneous bounds on competitiveness
and regret. In Conference on Learning Theory  pages 741–763  2013.

[53] Joao P Hespanha. Linear systems theory. Princeton university press  2018.
[54] Stefan Richter  Colin Neil Jones  and Manfred Morari. Computational complexity certiﬁcation
for real-time mpc with input constraints based on the fast gradient method. IEEE Transactions
on Automatic Control  57(6):1391–1403  2011.

[55] JB Rawlings and DQ Mayne. Postface to model predictive control: Theory and design. Nob

Hill Pub  pages 155–158  2012.

[56] David Angeli  Rishi Amrit  and James B Rawlings. Receding horizon cost optimization for
overly constrained nonlinear plants. In Proceedings of the 48h IEEE Conference on Decision
and Control (CDC) held jointly with 2009 28th Chinese Control Conference  pages 7972–7977.
IEEE  2009.

12

[57] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.

John Wiley & Sons  2014.

[58] Tobias Damm  Lars Grüne  Marleen Stieler  and Karl Worthmann. An exponential turnpike
theorem for dissipative discrete time optimal control problems. SIAM Journal on Control and
Optimization  52(3):1935–1957  2014.

[59] Dimitri P Bertsekas. Dynamic programming and optimal control  volume 1. 2011.
[60] Gregor Klancar  Drago Matko  and Saso Blazic. Mobile robot control on a reference path. In
Proceedings of the 2005 IEEE International Symposium on  Mediterrean Conference on Control
and Automation Intelligent Control  2005.  pages 1343–1348. IEEE  2005.

[61] Pololu Corporation. Pololu m3pi User’s Guide. Available at https://www.pololu.com/

docs/pdf/0J48/m3pi.pdf.

[62] Heejung Bang and James M Robins. Doubly robust estimation in missing data and causal

inference models. Biometrics  61(4):962–973  2005.

13

,Yingying Li
Xin Chen
Na Li