2013,Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture,This paper presents a novel algorithm  based upon the dependent Dirichlet process mixture model (DDPMM)  for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a low-variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM  and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms  while providing higher accuracy on the examined datasets.,Dynamic Clustering via Asymptotics of the

Dependent Dirichlet Process Mixture

Trevor Campbell

MIT

Cambridge  MA 02139

Miao Liu

Duke University

Durham  NC 27708

tdjc@mit.edu

miao.liu@duke.edu

Brian Kulis

Ohio State University
Columbus  OH 43210

Jonathan P. How

MIT

Cambridge  MA 02139

kulis@cse.ohio-state.edu

jhow@mit.edu

Lawrence Carin
Duke University

Durham  NC 27708
lcarin@duke.edu

Abstract

This paper presents a novel algorithm  based upon the dependent Dirichlet pro-
cess mixture model (DDPMM)  for clustering batch-sequential data containing
an unknown number of evolving clusters. The algorithm is derived via a low-
variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM 
and provides a hard clustering with convergence guarantees similar to those of the
k-means algorithm. Empirical results from a synthetic test with moving Gaussian
clusters and a test with real ADS-B aircraft trajectory data demonstrate that the al-
gorithm requires orders of magnitude less computational time than contemporary
probabilistic and hard clustering algorithms  while providing higher accuracy on
the examined datasets.

1

Introduction

The Dirichlet process mixture model (DPMM) is a powerful tool for clustering data that enables
the inference of an unbounded number of mixture components  and has been widely studied in the
machine learning and statistics communities [1–4]. Despite its ﬂexibility  it assumes the observa-
tions are exchangeable  and therefore that the data points have no inherent ordering that inﬂuences
their labeling. This assumption is invalid for modeling temporally/spatially evolving phenomena  in
which the order of the data points plays a principal role in creating meaningful clusters. The depen-
dent Dirichlet process (DDP)  originally formulated by MacEachern [5]  provides a prior over such
evolving mixture models  and is a promising tool for incrementally monitoring the dynamic evolu-
tion of the cluster structure within a dataset. More recently  a construction of the DDP built upon
completely random measures [6] led to the development of the dependent Dirichlet process Mixture
model (DDPMM) and a corresponding approximate posterior inference Gibbs sampling algorithm.
This model generalizes the DPMM by including birth  death and transition processes for the clusters
in the model.
The DDPMM is a Bayesian nonparametric (BNP) model  part of an ever-growing class of prob-
abilistic models for which inference captures uncertainty in both the number of parameters and
their values. While these models are powerful in their capability to capture complex structures in
data without requiring explicit model selection  they suffer some practical shortcomings. Inference
techniques for BNPs typically fall into two classes: sampling methods (e.g.  Gibbs sampling [2]

1

or particle learning [4]) and optimization methods (e.g.  variational inference [3] or stochastic vari-
ational inference [7]). Current methods based on sampling do not scale well with the size of the
dataset [8]. Most optimization methods require analytic derivatives and the selection of an upper
bound on the number of clusters a priori  where the computational complexity increases with that
upper bound [3  7]. State-of-the-art techniques in both classes are not ideal for use in contexts where
performing inference quickly and reliably on large volumes of streaming data is crucial for timely
decision-making  such as autonomous robotic systems [9–11]. On the other hand  many classical
clustering methods [12–14] scale well with the size of the dataset and are easy to implement  and
advances have recently been made to capture the ﬂexibility of Bayesian nonparametrics in such
approaches [15]. However  as of yet there is no classical algorithm that captures dynamic cluster
structure with the same representational power as the DDP mixture model.
This paper discusses the Dynamic Means algorithm  a novel hard clustering algorithm for spatio-
temporal data derived from the low-variance asymptotic limit of the Gibbs sampling algorithm for
the dependent Dirichlet process Gaussian mixture model. This algorithm captures the scalability
and ease of implementation of classical clustering methods  along with the representational power
of the DDP prior  and is guaranteed to converge to a local minimum of a k-means-like cost function.
The algorithm is signiﬁcantly more computationally tractable than Gibbs sampling  particle learning 
and variational inference for the DDP mixture model in practice  while providing equivalent or better
clustering accuracy on the examples presented. The performance and characteristics of the algorithm
are demonstrated in a test on synthetic data  with a comparison to those of Gibbs sampling  particle
learning and variational inference. Finally  the applicability of the algorithm to real data is presented
through an example of clustering a spatio-temporal dataset of aircraft trajectories recorded across
the United States.

2 Background

R+ (cid:82)
The Dirichlet process (DP) is a prior over mixture models  where the number of mixture components
is not known a priori[16]. In general  we denote D ∼ DP(µ)  where αµ ∈ R+ and µ : Ω →
Ω dµ = αµ are the concentration parameter and base measure of the DP  respectively. If
∞
k=0 ⊂ Ω × R+  where θk ∈ Ω and πk ∈ R+[17]. The reader is
D ∼ DP  then D = {(θk  πk)}
directed to [1] for a more thorough coverage of Dirichlet processes.
The dependent Dirichlet process (DDP)[5]  an extension to the DP  is a prior over evolving mixture
models. Given a Poisson process construction[6]  the DDP essentially forms a Markov chain of DPs
(D1  D2  . . . )  where the transitions are governed by a set of three stochastic operations: Points θk
may be added  removed  and may move during each step of the Markov chain. Thus  they become
parameterized by time  denoted by θkt. In slightly more detail  if Dt is the DP at time step t  then
the following procedure deﬁnes the generative model of Dt conditioned on Dt−1 ∼ DP(µt−1):

A

|θ)µ(dθ).

θ(cid:48)

t to be the collection of points (θ(cid:48)  π). Then D(cid:48)(cid:48)
Ω T (θ(cid:48)

1. Subsampling: Deﬁne a function q : Ω → [0  1]. Then for each point (θ  π) ∈ Dt−1 
(cid:82)
sample a Bernoulli distribution bθ ∼ Be(q(θ)). Set D(cid:48)
t to be the collection of points (θ  π)
such that bθ = 1  and renormalize the weights. Then D(cid:48)
t ∼ DP(qµt−1)  where (qµ)(A) =
A q(θ)µ(dθ).
where (T µ)(A) =(cid:82)
(cid:82)
2. Transition: Deﬁne a distribution T : Ω × Ω → R+. For each point (θ  π) ∈ D(cid:48)
t  sample
|θ)  and set D(cid:48)(cid:48)
∼ T (θ(cid:48)
t ∼ DP(T qµt−1) 
3. Superposition: Sample F ∼ DP(ν)  and sample (cD  cF ) ∼ Dir(T qµt−1(Ω)  ν(Ω)).
Then set Dt to be the union of (θ  cDπ) for all (θ  π) ∈ D(cid:48)(cid:48)
t and (θ  cF π) for all (θ  π) ∈ F .
Thus  Dt is a random convex combination of D(cid:48)(cid:48)
t and F   where Dt ∼ DP(T qµt−1 + ν).
If the DDP is used as a prior over a mixture model  these three operations allow new mixture com-
ponents to arise over time  and old mixture components to exhibit dynamics and perhaps disappear
over time. As this is covered thoroughly in [6]  the mathematics of the underlying Poisson point
process construction are not discussed in more depth in this work. However  an important result of
using such a construction is the development of an explicit posterior for Dt given observations of the
points θkt at timestep t. For each point k that was observed in Dτ for some τ : 1 ≤ τ ≤ t  deﬁne:
nkt ∈ N as the number of observations of point k in timestep t; ckt ∈ N as the number of past

2

(cid:32)

(cid:88)

observations of point k prior to timestep t  i.e. ckt = (cid:80)t−1

τ =1 nkτ ; qkt ∈ (0  1) as the subsampling
weight on point k at timestep t; and ∆tk as the number of time steps that have elapsed since point k
was last observed. Further  let νt be the measure for unobserved points at time step t. Then 

(cid:88)

(cid:33)

Dt|Dt−1 ∼ DP

νt +

qktcktT (·|θk(t−∆tk)) +

k:nkt=0

k:nkt>0

(ckt + nkt)δθkt

(1)

where ckt = 0 for any point k that was ﬁrst observed during timestep t. This posterior leads directly
to the development of a Gibbs sampling algorithm for the DDP  whose low-variance asymptotics are
discussed further below.

3 Asymptotic Analysis of the DDP Mixture

The dependent Dirichlet process Gaussian mixture model (DDP-GMM) serves as the foundation
upon which the present work is built. The generative model of a DDP-GMM at time step t is

∞
k=1 ∼ DP(µt)
{θkt  πkt}
{zit}Nt
i=1 ∼ Categorical({πkt}
{yit}Nt
i=1 ∼ N (θzitt  σI)

∞
k=1)

(2)

where θkt is the mean of cluster k  πkt is the categorical weight for class k  yit is a d-dimensional
observation vector  zit is a cluster label for observation i  and µt is the base measure from equation
(1). Throughout the rest of this paper  the subscript kt refers to quantities related to cluster k at time
step t  and subscript it refers to quantities related to observation i at time step t.
The Gibbs sampling algorithm for the DDP-GMM iterates between sampling labels zit for dat-
apoints yit given the set of parameters {θkt}  and sampling parameters θkt given each group of
data {yit : zit = k}. Assuming the transition model T is Gaussian  and the subsampling func-
tion q is constant  the functions and distributions used in the Gibbs sampling algorithm are: the
prior over cluster parameters  θ ∼ N (φ  ρI); the likelihood of an observation given its cluster pa-
rameter  yit ∼ N (θkt  σI); the distribution over the transitioned cluster parameter given its last
known location after ∆tk time steps  θkt ∼ N (θk(t−∆tk)  ξ∆tkI); and the subsampling function
q(θ) = q ∈ (0  1). Given these functions and distributions  the low-variance asymptotic limits
(i.e. σ → 0) of these two steps are discussed in the following sections.
3.1 Setting Labels Given Parameters

In the label sampling step  a datapoint yit can either create a new cluster  join a current cluster  or
revive an old  transitioned cluster. Using the distributions deﬁned previously  the label assignment
probabilities are

(cid:17)

(cid:17)

||yit−φ||2
−
2(σ+ρ)
||yit−θkt||2
−
||yit−θk(t−∆tk )||2
−

2(σ+ξ∆tk)

2σ

(cid:17)

k = K + 1

nkt > 0

nkt = 0

(3)

(cid:16)
(cid:16)
(cid:16)

αt(2π(σ + ρ))−d/2 exp
(ckt + nkt)(2πσ)−d/2 exp
qktckt(2π(σ + ξ∆tk))−d/2 exp


αν = (1 + ρ/σ)d/2 exp(cid:0)
 ||yit − θkt||2

{Jk}   Jk =

Q∆tk +
λ

− λ

2σ

||yit−θk(t−∆tk )||2

τ ∆tk+1

1−qt
where qkt = q∆tk due to the fact that q(θ) is constant over Ω  and αt = αν
1−q where αν is the
concentration parameter for the innovation process  Ft. The low-variance asymptotic limit of this
label assignment step yields meaningful assignments as long as αν  ξ  and q vary appropriately with
σ; thus  setting αν  ξ  and q as follows (where λ  τ and Q are positive constants):
− Q

(cid:1)  

q = exp

ξ = τ σ 

(cid:17)

(cid:16)

(4)

2σ

yields the following assignments in the limit as σ → 0:

if θk instantiated
if θk old  uninstantiated
if θk new

.

(5)

p(zit = k| . . . ) ∝

zit = arg min

k

In this assignment step  Q∆tk acts as a cost penalty for reviving old clusters that increases with the
time since the cluster was last seen  τ ∆tk acts as a cost reduction to account for the possible motion
of clusters since they were last instantiated  and λ acts as a cost penalty for introducing a new cluster.

3

θpost = σpost

Then letting σ → 0 

θkt|{yit : zit = k} ∼ N (θpost  σpost)
nkt
σ

i=1 yit
σ

  σpost =

+

+

ρ

(cid:18) 1

ρ

(cid:19)−1

(cid:19)

(cid:80)nkt
((cid:80)nkt

θkt =

i=1 yit)
nkt

def= mkt

(7)

(8)

(cid:18) φ

(cid:90)

3.2 Setting Parameters given Labels

In the parameter sampling step  the parameters are sampled using the distribution

p(θkt|{yit : zit = k}) ∝ p({yit : zit = k}|θkt)p(θkt)

(6)
There are two cases to consider when setting a parameter θkt. Either ∆tk = 0 and the cluster is new
in the current time step  or ∆tk > 0 and the cluster was previously created  disappeared for some
amount of time  and then was revived in the current time step.

New Cluster Suppose cluster k is being newly created. In this case  θkt ∼ N (φ  ρ). Using the
fact that a normal prior is conjugate a normal likelihood  the closed-form posterior for θkt is

where mkt is the mean of the observations in the current timestep.

Revived Cluster Suppose there are ∆tk time steps where cluster k was not observed  but there
are now nkt data points with mean mkt assigned to it in this time step. In this case 

Again using conjugacy of normal likelihoods and priors 

p(θkt) =

θ

T (θkt|θ)p(θ) dθ  θ ∼ N (θ(cid:48)  σ(cid:48)).
(cid:80)nkt

θkt|{yit : zit = k} ∼ N (θpost  σpost)
1

(cid:18)

(cid:19)

θ(cid:48)

(cid:18)

θpost = σpost

ξ∆tk + σ(cid:48) +

  σpost =

ξ∆tk + σ(cid:48) +

i=1 yit
σ

(9)

(10)

(cid:19)−1

nkt
σ

Similarly to the label assignment step  let ξ = τ σ. Then as long as σ(cid:48) = σ/w  w > 0 (which holds
if equation (10) is used to recursively keep track of the parameter posterior)  taking the asymptotic
limit of this as σ → 0 yields:

θkt =

θ(cid:48)(w−1 + ∆tkτ )−1 + nktmkt

(w−1 + ∆tkτ )−1 + nkt

(11)

that is to say  the revived θkt is a weighted average of estimates using current timestep data and
previous timestep data. τ controls how much the current data is favored - as τ increases  the weight
on current data increases  which is explained by the fact that our uncertainty in where the old θ(cid:48)
transitioned to increases with τ. It is also noted that if τ = 0  this reduces to a simple weighted
average using the amount of data collected as weights.

Combined Update Combining the updates for new cluster parameters and old transitioned cluster
parameters yields a recursive update scheme:

γkt =(cid:0)(wk(t−∆tk))−1 + ∆tkτ(cid:1)−1

θk0 = mk0
wk0 = nk0

and

θkt =

θk(t−∆tk)γkt + nktmkt

γkt + nkt

wkt = γkt + nkt

(12)

where time step 0 here corresponds to when the cluster is ﬁrst created. An interesting interpre-
tation of this update is that it behaves like a standard Kalman ﬁlter  in which w−1
kt serves as the
current estimate variance  τ serves as the process noise variance  and nkt serves as the inverse of the
measurement variance.

4

Algorithm 1 Dynamic Means
Input: {Yt}tf

t=1  Q  λ  τ

C1 ← ∅
for t = 1 → tf do

(Kt  Zt  Lt) ←CLUSTER(Yt  Ct  Q  λ  τ)
Ct+1 ←UPDATEC(Zt  Kt  Ct)

end for
return {Kt  Zt  Lt}tf

t=1

4 The Dynamic Means Algorithm

Algorithm 2 CLUSTER
Input: Yt  Ct  Q  λ  τ
Kt ← ∅  Zt ← ∅  L0 ← ∞
for n = 1 → ∞ do

(Zt  Kt) ←ASSIGNLABELS(Yt  Zt  Kt  Ct)
(Kt  Ln) ←ASSIGNPARAMS(Yt  Zt  Ct)
if Ln = Ln−1 then
return Kt  Zt  Ln

end if

end for

In this section  some further notation is required for brevity:
i=1  Zt = {zit}Nt

Yt = {yit}Nt
Kt = {(θkt  wkt) : nkt > 0} 

i=1

Ct = {(∆tk  θk(t−∆tk)  wk(t−∆tk))}

where Yt and Zt are the sets of observations and labels at time step t  Kt is the set of currently active
clusters (some are new with ∆tk = 0  and some are revived with ∆tk > 0)  and Ct is the set of old
cluster information.

(13)

4.1 Algorithm Description

As shown in the previous section  the low-variance asymptotic limit of the DDP Gibbs sampling
algorithm is a deterministic observation label update (5) followed by a deterministic  weighted least-
squares parameter update (12). Inspired by the original K-Means algorithm  applying these two
updates iteratively yields an algorithm which clusters a set of observations at a single time step
given cluster means and weights from past time steps (Algorithm 2). Applying Algorithm 2 to a
sequence of batches of data yields a clustering procedure that is able to track a set of dynamically
evolving clusters (Algorithm 1)  and allows new clusters to emerge and old clusters to be forgotten.
While this is the primary application of Algorithm 1  the sequence of batches need not be a temporal
sequence. For example  Algorithm 1 may be used as an any-time clustering algorithm for large
datasets  where the sequence of batches is generated by selecting random subsets of the full dataset.
The ASSIGNPARAMS function is exactly the update from equation (12) applied to each k ∈ Kt.
Similarly  the ASSIGNLABELS function applies the update from equation (5) to each observation;
however  in the case that a new cluster is created or an old one is revived by an observation  AS-
SIGNLABELS also creates a parameter for that new cluster based on the parameter update equation
(12) with that single observation. Note that the performance of the algorithm depends on the order
in which ASSIGNLABELS assigns labels. Multiple random restarts of the algorithm with different
assignment orders may be used to mitigate this dependence. The UPDATEC function is run after
clustering observations from each time step  and constructs Ct+1 by setting ∆tk = 1 for any new or
revived cluster  and by incrementing ∆tk for any old cluster that was not revived:
(14)

Ct+1 = {(∆tk + 1  θk(t−∆tk)  wk(t−∆tk)) : k ∈ Ct  k /∈ Kt} ∪ {(1  θkt  wkt) : k ∈ Kt}

An important question is whether this algorithm is guaranteed to converge while clustering data in
each time step. Indeed  it is; Theorem 1 shows that a particular cost function Lt monotonically
decreases under the label and parameter updates (5) and (12) at each time step. Since Lt ≥ 0  and it
is monotonically decreased by Algorithm 2  the algorithm converges. Note that the Dynamic Means
is only guaranteed to converge to a local optimum  similarly to the k-means[12] and DP-Means[15]
algorithms.
Theorem 1. Each iteration in Algorithm 2 monotonically decreases the cost function Lt  where

(cid:125)(cid:124)
(cid:122)
γkt||θkt − θk(t−∆tk)||2

2 +

Weighted-Prior Sum-Squares Cost

(cid:88)

yit∈Yt
zit=k

 (15)
(cid:123)
||yit − θkt||2

2

 New Cost
(cid:122)
(cid:125)(cid:124)

(cid:88)

k∈Kt

(cid:123)

(cid:122)(cid:125)(cid:124)(cid:123)

Revival Cost

Lt =

λ [∆tk = 0] +

Q∆tk +

The cost function is comprised of a number of components for each currently active cluster k ∈ Kt:
A penalty for new clusters based on λ  a penalty for old clusters based on Q and ∆tk  and ﬁnally

5

a prior-weighted sum of squared distance cost for all the observations in cluster k. It is noted that
for new clusters  θkt = θk(t−∆tk) since ∆tk = 0  so the least squares cost is unweighted. The
ASSIGNPARAMS function calculates this cost function in each iteration of Algorithm 2  and the
algorithm terminates once the cost function does not decrease during an iteration.

4.2 Reparameterizing the Algorithm

In order to use the Dynamic Means algorithm  there are three free parameters to select: λ  Q  and τ.
While λ represents how far an observation can be from a cluster before it is placed in a new cluster 
and thus can be tuned intuitively  Q and τ are not so straightforward. The parameter Q represents
a conceptual added distance from any data point to a cluster for every time step that the cluster is
not observed. The parameter τ represents a conceptual reduction of distance from any data point
to a cluster for every time step that the cluster is not observed. How these two quantities affect the
algorithm  and how they interact with the setting of λ  is hard to judge.
Instead of picking Q and τ directly  the algorithm may be reparameterized by picking NQ  kτ ∈ R+ 
NQ > 1  kτ ≥ 1  and given a choice of λ  setting
Q =λ/NQ τ =

NQ(kτ − 1) + 1

(16)

.

NQ − 1

If Q and τ are set in this manner  NQ represents the number (possibly fractional) of time steps a
cluster can be unobserved before the label update (5) will never revive that cluster  and kτ λ repre-
sents the maximum squared distance away from a cluster center such that after a single time step  the
label update (5) will revive that cluster. As NQ and kτ are speciﬁed in terms of concrete algorithmic
behavior  they are intuitively easier to set than Q and τ.

5 Related Work

Prior k-means clustering algorithms that determine the number of clusters present in the data have
primarily involved a method for iteratively modifying k using various statistical criteria [13  14  18].
In contrast  this work derives this capability from a Bayesian nonparametric model  similarly to
the DP-Means algorithm [15]. In this sense  the relationship between the Dynamic Means algo-
rithm and the dependent Dirichlet process [6] is exactly that between the DP-Means algorithm and
Dirichlet process [16]  where the Dynamic Means algorithm may be seen as an extension to the
DP-Means that handles sequential data with time-varying cluster parameters. MONIC [19] and
MC3 [20] have the capability to monitor time-varying clusters; however  these methods require dat-
apoints to be identiﬁable across timesteps  and determine cluster similarity across timesteps via the
commonalities between label assignments. The Dynamic Means algorithm does not require such
information  and tracks clusters essentially based on similarity of the parameters across timesteps.
Evolutionary clustering [21  22]  similar to Dynamic Means  minimizes an objective consisting of
a cost for clustering the present data set and a cost related to the comparison between the current
clustering and past clusterings. The present work can be seen as a theoretically-founded extension
of this class of algorithm that provides methods for automatic and adaptive prior weight selection 
forming correspondences between old and current clusters  and for deciding when to introduce new
clusters. Finally  some sequential Monte-Carlo methods (e.g. particle learning [23] or multi-target
tracking [24  25]) can be adapted for use in the present context  but suffer the drawbacks typical of
particle ﬁltering methods.

6 Applications

6.1 Synthetic Gaussian Motion Data
In this experiment  moving Gaussian clusters on [0  1] × [0  1] were generated synthetically over a
period of 100 time steps. In each step  there was some number of clusters  each having 15 data points.
The data points were sampled from a symmetric Gaussian distribution with a standard deviation of
0.05. Between time steps  the cluster centers moved randomly  with displacements sampled from
the same distribution. At each time step  each cluster had a 0.05 probability of being destroyed.

6

(a)

(b)

(c)

(d)

(e)

(f)

Figure 1: (1a - 1c): Accuracy contours and CPU time histogram for the Dynamic Means algorithm.
(1d - 1e): Comparison with Gibbs
sampling  variational inference  and particle learning. Shaded region indicates 1σ interval; in (1e)  only upper half is shown. (1f): Comparison
of accuracy when enforcing (Gibbs  DynMeans) and not enforcing (Gibbs NC  DynMeans NC) correct cluster tracking.

This data was clustered with Dynamic Means (with 3 random assignment ordering restarts)  DDP-
GMM Gibbs sampling [6]  variational inference [3]  and particle learning [4] on a computer with
an Intel i7 processor and 16GB of memory. First  the number of clusters was ﬁxed to 5  and the
parameter space of each algorithm was searched for the best possible cluster label accuracy (taking
into account correct cluster tracking across time steps). The results of this parameter sweep for
the Dynamic Means algorithm with 50 trials at each parameter setting are shown in Figures 1a–1c.
Figures 1a and 1b show how the average clustering accuracy varies with the parameters after ﬁxing
either kτ or TQ to their values at the maximum accuracy parameter setting over the full space. The
Dynamic Means algorithm had a similar robustness with respect to variations in its parameters as
the comparison algorithms. The histogram in Figure 1c demonstrates that the clustering speed is
robust to the setting of parameters. The speed of Dynamic Means  coupled with the smoothness of
its performance with respect to its parameters  makes it well suited for automatic tuning [26].
Using the best parameter setting for each algorithm  the data as described above were clustered in
50 trials with a varying number of clusters present in the data. For the Dynamic Means algorithm 
parameter values λ = 0.04  TQ = 6.8  and kτ = 1.01 were used  and the algorithm was again given
3 attempts with random labeling assignment orders  where the lowest cost solution of the 3 was
picked to proceed to the next time step. For the other algorithms  the parameter values α = 1 and
q = 0.05 were used  with a Gaussian transition distribution variance of 0.05. The number of samples
for the Gibbs sampling algorithm was 5000 with one recorded for every 5 samples  the number of
particles for the particle learning algorithm was 100  and the variational inference algorithm was run
to a tolerance of 10−20 with the maximum number of iterations set to 5000.
In Figures 1d and 1e  the labeling accuracy and clustering time (respectively) for the algorithms is
shown. The sampling algorithms were handicapped to generate Figure 1d; the best posterior sample
in terms of labeling accuracy was selected at each time step  which required knowledge of the true
labeling. Further  the accuracy computation included enforcing consistency across timesteps  to
allow tracking individual cluster trajectories. If this is not enforced (i.e. accuracy considers each
time step independently)  the other algorithms provide accuracies more comparable to those of the
Dynamic Means algorithm. This effect is demonstrated in Figure 1f  which shows the time/accuracy
tradeoff for Gibbs sampling (varying the number of samples) and Dynamic Means (varying the
number of restarts). These examples illustrate that Dynamic Means outperforms standard inference
algorithms in both label accuracy and computation time for cluster tracking problems.

7

0.020.040.060.080.100.120.140.16λ2345678910TQ0.2400.3200.3200.4000.4000.4800.4800.5600.5600.020.040.060.080.100.120.140.16λ123456kτ0.2400.3200.3200.4000.4800.560−4.0−3.8−3.6−3.4−3.2−3.0−2.8−2.6−2.4−2.2CPUTime(log10s)perStep05010015020025005101520#Clusters020406080100%LabelAccuracyGibbsVBPLDynMeans05101520#Clusters10−510−410−310−210−1100101102CPUTime(s)perStepGibbsVBPLDynMeans10−510−410−310−210−1100101102CPUTime(s)perStep20406080100%AccuracyGibbsGibbsNCDynMeansDynMeansNCFigure 2: Results of the GP aircraft trajectory clustering. Left: A map (labeled with major US city airports) showing the overall aircraft ﬂows
for 12 trajectories  with colors and 1σ conﬁdence ellipses corresponding to takeoff region (multiple clusters per takeoff region)  colored dots
indicating mean takeoff position for each cluster  and lines indicating the mean trajectory for each cluster. Right: A track of plane counts for
the 12 clusters during the week  with color intensity proportional to the number of takeoffs at each time.

6.2 Aircraft Trajectory Clustering

In this experiment  the Dynamic Means algorithm was used to ﬁnd the typical spatial and tem-
poral patterns in the motions of commercial aircraft. Automatic dependent surveillance-broadcast
(ADS-B) data  including plane identiﬁcation  timestamp  latitude  longitude  heading and speed 
was collected from all transmitting planes across the United States during the week from 2013-3-22
1:30:0 to 2013-3-28 12:0:0 UTC. Then  individual ADS-B messages were connected together based
on their plane identiﬁcation and timestamp to form trajectories  and erroneous trajectories were ﬁl-
tered based on reasonable spatial/temporal bounds  yielding 17 895 unique trajectories. Then  for
each trajectory  a Gaussian process was trained using the latitude and longitude of each ADS-B
point along the trajectory as the inputs and the North and East components of plane velocity at those
points as the outputs. Next  the mean latitudinal and longitudinal velocities from the Gaussian pro-
cess were queried for each point on a regular lattice across the USA (10 latitudes and 20 longitudes) 
and used to create a 400-dimensional feature vector for each trajectory. Of the resulting 17 895
feature vectors  600 were hand-labeled (each label including a conﬁdence weight in [0  1]). The
feature vectors were clustered using the DP-Means algorithm on the entire dataset in a single batch 
and using Dynamic Means / DDPGMM Gibbs sampling (with 50 samples) with half-hour takeoff
window batches.
The results of this exercise are provided in Figure 2 and Table 1.
Figure 2 shows the spatial and temporal properties of the 12 most
popular clusters discovered by Dynamic Means  demonstrating
that the algorithm successfully identiﬁed major ﬂows of commer-
cial aircraft across the US. Table 1 corroborates these qualitative
results with a quantitative comparison of the computation time
and accuracy for the three algorithms tested over 20 trials. The
conﬁdence-weighted accuracy was computed by taking the ratio between the sum of the weights
for correctly labeled points and the sum of all weights. The DDPGMM Gibbs sampling algorithm
was handicapped as described in the synthetic experiment section. Of the three algorithms  Dynamic
Means provided the highest labeling accuracy  while requiring orders of magnitude less computation
time than both DP-Means and DDPGMM Gibbs sampling.

% Acc. Time (s)
2.7 × 102
55.9
3.1 × 103
55.6
1.4 × 104
36.9

Table 1: Mean computational time & accuracy
on hand-labeled aircraft trajectory data

Alg.
DynM
DPM
Gibbs

7 Conclusion

This work developed a clustering algorithm for batch-sequential data containing temporally evolving
clusters  derived from a low-variance asymptotic analysis of the Gibbs sampling algorithm for the
dependent Dirichlet process mixture model. Synthetic and real data experiments demonstrated that
the algorithm requires orders of magnitude less computational time than contemporary probabilistic
and hard clustering algorithms  while providing higher accuracy on the examined datasets. The
speed of inference coupled with the convergence guarantees provided yield an algorithm which
is suitable for use in time-critical applications  such as online model-based autonomous planning
systems.

Acknowledgments

This work was supported by NSF award IIS-1217433 and ONR MURI grant N000141110688.

8

−0.2−0.10.00.10.2−0.5−0.4−0.3−0.2−0.10.0−0.2−0.10.00.10.2JFKMIAHOULAXSEAORDMSPFriSatSunMonTueWedThuFriUTCDate0123456789101112Cluster#References
[1] Yee Whye Teh. Dirichlet processes. In Encyclopedia of Machine Learning. Springer  New York  2010.
[2] Radford M. Neal. Markov chain sampling methods for dirichlet process mixture models. Journal of

Computational and Graphical Statistics  9(2):249–265  2000.

[3] David M. Blei and Michael I. Jordan. Variational inference for dirichlet process mixtures. Bayesian

Analysis  1(1):121–144  2006.

[4] Carlos M. Carvalho  Hedibert F. Lopes  Nicholas G. Polson  and Matt A. Taddy. Particle learning for

general mixtures. Bayesian Analysis  5(4):709–740  2010.

[5] Steven N. MacEachern. Dependent nonparametric processes. In Proceedings of the Bayesian Statistical

Science Section. American Statistical Association  1999.

[6] Dahua Lin  Eric Grimson  and John Fisher. Construction of dependent dirichlet processes based on

poisson processes. In Neural Information Processing Systems  2010.

[7] Matt Hoffman  David Blei  Chong Wang  and John Paisley. Stochastic variational inference. arXiv ePrint

1206.7051  2012.

[8] Finale Doshi-Velez and Zoubin Ghahramani. Accelerated sampling for the indian buffet process.

Proceedings of the International Conference on Machine Learning  2009.

In

[9] Felix Endres  Christian Plagemann  Cyrill Stachniss  and Wolfram Burgard. Unsupervised discovery of

object classes from range data using latent dirichlet allocation. In Robotics Science and Systems  2005.

[10] Matthias Luber  Kai Arras  Christian Plagemann  and Wolfram Burgard. Classifying dynamic objects:

An unsupervised learning approach. In Robotics Science and Systems  2004.

[11] Zhikun Wang  Marc Deisenroth  Heni Ben Amor  David Vogt  Bernard Sch¨olkopf  and Jan Peters. Prob-
abilistic modeling of human movements for intention inference. In Robotics Science and Systems  2008.
[12] Stuart P. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory  28(2):129–

137  1982.

[13] Dan Pelleg and Andrew Moore. X-means: Extending k-means with efﬁcient estimation of the number of

clusters. In Proceedings of the 17th International Conference on Machine Learning  2000.

[14] Robert Tibshirani  Guenther Walther  and Trevor Hastie. Estimating the number of clusters in a data set

via the gap statistic. Journal of the Royal Statistical Society B  63(2):411–423  2001.

[15] Brian Kulis and Michael I. Jordan. Revisiting k-means: New algorithms via bayesian nonparametrics.
In Proceedings of the 29th International Conference on Machine Learning (ICML)  Edinburgh  Scotland 
2012.

[16] Thomas S. Ferguson. A bayesian analysis of some nonparametric problems. The Annals of Statistics 

1(2):209–230  1973.

[17] Jayaram Sethuraman. A constructive deﬁnition of dirichlet priors. Statistica Sinica  4:639–650  1994.
[18] Tsunenori Ishioka. Extended k-means with an efﬁcient estimation of the number of clusters. In Proceed-
ings of the 2nd International Conference on Intelligent Data Engineering and Automated Learning  pages
17–22  2000.

[19] Myra Spiliopoulou  Irene Ntoutsi  Yannis Theodoridis  and Rene Schult. Monic - modeling and monitor-
ing cluster transitions. In Proceedings of the 12th International Conference on Knowledge Discovering
and Data Mining  pages 706–711  2006.

[20] Panos Kalnis  Nikos Mamoulis  and Spiridon Bakiras. On discovering moving clusters in spatio-temporal
In Proceedings of the 9th International Symposium on Spatial and Temporal Databases  pages

data.
364–381. Springer  2005.

[21] Deepayan Chakraborti  Ravi Kumar  and Andrew Tomkins. Evolutionary clustering. In Proceedings of

the SIGKDD International Conference on Knowledge Discovery and Data Mining  2006.

[22] Kevin Xu  Mark Kliger  and Alfred Hero III. Adaptive evolutionary clustering. Data Mining and Knowl-

edge Discovery  pages 1–33  2012.

[23] Carlos M. Carvalho  Michael S. Johannes  Hedibert F. Lopes  and Nicholas G. Polson. Particle learning

and smoothing. Statistical Science  25(1):88–106  2010.

[24] Carine Hue  Jean-Pierre Le Cadre  and Patrick P´erez. Tracking multiple objects with particle ﬁltering.

IEEE Transactions on Aerospace and Electronic Systems  38(3):791–812  2002.

[25] Jaco Vermaak  Arnaud Doucet  and Partick P´erez. Maintaining multi-modality through mixture tracking.

In Proceedings of the 9th IEEE International Conference on Computer Vision  2003.

[26] Jasper Snoek  Hugo Larochelle  and Ryan Adams. Practical bayesian optimization of machine learning

algorithms. In Neural Information Processing Systems  2012.

9

,Trevor Campbell
Miao Liu
Brian Kulis
Jonathan How
Lawrence Carin