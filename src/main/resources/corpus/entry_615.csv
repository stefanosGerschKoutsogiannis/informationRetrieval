2018,Asymptotic optimality of adaptive importance sampling,\textit{Adaptive importance sampling} (AIS) uses past samples to update the \textit{sampling policy} $q_t$ at each stage $t$. Each stage $t$ is formed with two steps : (i) to explore the space with $n_t$ points according to $q_t$ and (ii) to exploit the current amount of information to update the sampling policy. The very fundamental question raised in this paper concerns the behavior of empirical sums based on AIS. Without making any assumption on the \textit{allocation policy} $n_t$  the theory developed involves no restriction on the split of computational resources between the explore (i) and the exploit (ii) step. It is shown that AIS is asymptotically optimal : the asymptotic behavior of AIS is the same as some ``oracle'' strategy that knows the targeted sampling policy from the beginning. From a practical perspective  weighted AIS is introduced  a new method that allows to forget poor samples from early stages.,Asymptotic optimality of adaptive importance

sampling

Bernard Delyon

IRMAR

University of Rennes 1

bernard.delyon@univ-rennes1.fr

François Portier
Télécom ParisTech

University of Paris-Saclay

francois.portier@gmail.com

Abstract

Adaptive importance sampling (AIS) uses past samples to update the sampling
policy qt. Each stage t is formed with two steps : (i) to explore the space with nt
points according to qt and (ii) to exploit the current amount of information to update
the sampling policy. The very fundamental question raised in this paper concerns
the behavior of empirical sums based on AIS. Without making any assumption
on the allocation policy nt  the theory developed involves no restriction on the
split of computational resources between the explore (i) and the exploit (ii) step. It
is shown that AIS is asymptotically optimal : the asymptotic behavior of AIS is
the same as some “oracle” strategy that knows the targeted sampling policy from
the beginning. From a practical perspective  weighted AIS is introduced  a new
method that allows to forget poor samples from early stages.

1

Introduction

The adaptive choice of a sampling policy lies at the heart of many ﬁelds of Machine Learning where
former Monte Carlo experiments guide the forthcoming ones. This includes for instance reinforcment
learning [19  27  30] where the optimal policy maximizes the reward; inference in Bayesian [6] or
graphical models [21]; optimization based on stochastic gradient descent [34] or without using the
gradient [18]; rejection sampling [12]. Adaptive importance sampling (AIS) [25  2]  which extends
the basic Monte Carlo integration approach  offers a natural probabilistic framework to describe the
evolution of sampling policies. The present paper establishes  under fairly reasonable conditions  that
AIS is asymptotically optimal  i.e.  learning the sampling policy has no cost asymptotically.

Suppose we are interested in computing some integral value(cid:82) ϕ  where ϕ : Rd → R is called the
integrand. The importance sampling estimate of(cid:82) ϕ based on the sampling policy q  is given by

n(cid:88)

i=1

n−1

ϕ(xi)
q(xi)

 

(1)

Estimators usually employed are

i.i.d.∼ q. The previous estimate is unbiased. It is well known  e.g.  [16  13]  that
where (x1  . . . xn)
the optimal sampling policy  regarding the variance  is when q is proportional to |ϕ|. A slightly
different context where importance sampling still applies is Bayesian estimation. Here the targeted

quantity is(cid:82) ϕπ and we only have access to an unnormalized version πu of the density π = πu/(cid:82) πu.
In this case  the optimal sampling policy q is proportional to |ϕ −(cid:82) ϕπ|π (see [9] or section B.3 in

(cid:44) n(cid:88)

πu(xi)
q(xi)

.

ϕ(xi)πu(xi)

n(cid:88)

i=1

q(xi)

(2)

i=1

the supplementary material).

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Because appropriate policies naturally depend on ϕ or π  we generally cannot simulate from them.
They are then approximated adaptively  by densities from which we can simulate  using the infor-
mation gathered from the past stages. This is the very spirit of AIS. At each stage t  the value It 
standing for the current estimate  is updated using i.i.d. new samples xt 1  . . . xt nt from qt  where qt
is a probability density function that might depend on the past stages 1  . . . t − 1. The distribution qt 
called the sampling policy  targets some optimal  at least suitable  sampling policy. The sequence
(nt) ⊂ N∗  called the allocation policy  contains the number of particles generated at each stage.
The following algorithm describes the AIS schemes for the classical integration problem. For the
Bayesian problem  it sufﬁces to change the estimate according to (2). This is a generic representation
of AIS as no explicit update rule is speciﬁed (this will be discussed just below).

Algorithm 1 (AIS).
Inputs: The number of stages T ∈ N∗  the allocation policy (nt)t=1 ...T ⊂ N∗  the sampler
update procedure  the initial density q0.

Set S0 = 0  N0 = 0. For t in 1  . . . T :

(i) (Explore) Generate (xt 1  . . . xt nt) from qt−1
(ii) (Exploit)

(a) Update the estimate:

St = St−1 +

nt(cid:88)

(b) Update the sampler qt

i=1
Nt = Nt−1 + nt
It = N−1

t St

ϕ(xt i)
qt−1(xt i)

Pioneer works on adaptive schemes include [20] where  within a two-stages procedure  the sampling
policy is chosen out of a parametric family; this is further formalized in [14]; [25] introduces the
idea of a multi-stages approach where all the previous stages are used to update the sampling policy
(see also [29] regarding the choice of the loss function); [26] investigates the use of control variates
coupled with importance sampling; the population Monte Carlo approach [3  2] offers a general
framework for AIS and has been further studied using parametric mixtures [8  9]; see also [5  32] for a
variant called multiple adaptive importance sampling; see [11] for a recent review. In [33  23]  using
kernel smoothing  nonparametric importance sampling is introduced. The approach of choosing
qt out of a parametric family should also be contrasted with the non parametric approach based
on particles often refereed to as sequential Monte Carlo [6  4  10] whose context is different as
traditionally the targeted distribution changes with t. The distribution qt−1 is then a weighted sum of

i wt−1 iδxt−1 i  and updating qt follows from adjustment of the weights.

Dirac masses(cid:80)

The theoretical properties of adaptive schemes are difﬁcult to derive due to the recycling of the past
samples at each stage and hence to the lack of independence between samples. Among the update
based on a parametric family  the convergence properties of the Kullback-Leibler divergence between
the estimated and the targeted distribution are studied in [8]. Properties related to the asymptotic
variance are given in [9]. Among nonparametric update  [33] establishes fast convergence rates in a
two-stages strategy where the number of samples used in each stage goes to inﬁnity. For sequential
Monte Carlo  limit theorems are given for instance in [6  4  10]. All these results are obtained when
T is ﬁxed and nT → ∞ and therefore misses the true nature of the adaptive schemes for which the
asymptotic should be made with respect to T .
Recently  a more realistic asymptotic regime was considered in [22] in which the allocation policy
(nt) is a ﬁxed growing sequence of integers. The authors establish the consistency of the estimate
when the update is conducted with respect to a parametric family but depends only on the last stage.
They focus on multiple adaptive importance sampling [5  32] which is different than AIS (see Remark
2 below for more details).
In this paper  folllowing the same spirit as [8  9  2]  we study parametric AIS as presented in the AIS
algorithm when the policy is chosen out of a parametric family of probability density functions. Our
analysis focuses on the following 3 key points which are new to the best of our knowledge.

2

• A central limit theorem is established for the AIS estimate It.

It involves high-level
conditions on the sampling policy estimate qt (which will be easily satisﬁed for parametric
updates). Based on the martingale property associated to some sequences of interest  the
asymptotic is not with T ﬁxed and nT → ∞  but with the number of samples n1 + ··· +
nT → ∞. In particular  the allocation policy (nt) is not required to grow to inﬁnity. This is
presented in section 2.
• The high-level conditions are veriﬁed in the case of parametric sampling policies with
updates taking place in a general framework inspired by the paradigm of empirical risk
minimization (several concrete examples are provided). This establishes the asymptotic
optimality of AIS in the sense that the rate and the asymptotic variance coincide with some
“oracle” procedure where the targeted policy is known from the beginning. The details are
given in section 3.
• A new method  called weighted AIS (wAIS) is designed in section 4 to eventually forget
bad samples drawn during the early stages of AIS. Our numerical experiments shows that
(i) wAIS accelerates signiﬁcantly the convergence of AIS and (ii) small allocation policies
(nt) (implying more frequent updates) give better results than large (nt) (at equal number
of requests to ϕ). This last point supports empirically the theoretical framework adopted in
the paper.

All the proofs are given in the supplementary material.

2 Central limit theorems for AIS

consider the multivariate case where ϕ = (ϕ1  . . . ϕp) : Rd → Rp. In the whole paper (cid:82) ϕ is with

The aim of the section is to provide conditions on the sampling policy (qt) under which a central
limit theorem holds for AIS and normalized AIS.
For the sake of generality and because it will be useful in the treatment of normalized estimators  we
respect to the Lebesgue measure  (cid:107) · (cid:107) is the Euclidean norm  Ip is the identity matrix of size (p  p).
To study the AIS algorithm  it is appropriate to work at the sample time scale as described below
rather than at the sampling policy scale as described in the introduction. The sample xt i (resp. the
policy qt) of the previous section (t is the block index and i the sample index within the block) is now
simply denoted xj (resp. qj)  where j = n1 + . . . nt + i is the sample index in the whole sequence
1  . . . n  with n = NT . The following algorithm is the same as Algorithm 1 (no explicit update rule is
provided) but is expressed at the sample scale.
Algorithm 2 (AIS at sample scale).
Inputs: The number of stages T ∈ N∗  the allocation policy (nt)t=1 ...T ⊂ N∗  the sampler
update procedure  the initial density q0.

Set S0 = 0. For j in 1  . . . n :

(i) (Explore) Generate xj from qj−1
(ii) (Exploit)

(a) Update the estimate:

(b) Update the sampler qj whenever j ∈ {Nt =(cid:80)t

Sj = Sj−1 +
Ij = j−1Sj

s=1 ns : t (cid:62) 1}

ϕ(xj)

qj−1(xj)

2.1 The martingale property

Deﬁne ∆j as the j-th centered contribution to the sum Sj: ∆j = ϕ(xj)/qj−1(xj) −(cid:82) ϕ. Deﬁne  for

all n (cid:62) 1 

Mn =

∆j.

n(cid:88)

j=1

3

j=1

j | Fj−1

E(cid:2)∆j∆T

The ﬁltration we consider is given by Fn = σ(x1  . . . xn). The quadratic variation of M is given by

(cid:3). Set
(cid:90) (cid:0)ϕ(x) − q(x)(cid:82) ϕ(cid:1)(cid:0)ϕ(x) − q(x)(cid:82) ϕ(cid:1)T

(cid:104)M(cid:105)n =(cid:80)n
sequence (Mn  Fn) is a martingale. In particular  In is an unbiased estimate of(cid:82) ϕ. In addition 
the quadratic variation of M satisﬁes (cid:104)M(cid:105)n =(cid:80)n

(3)
Lemma 1. Assume that for all 1 (cid:54) j (cid:54) n  the support of qj contains the support of ϕ  then the

V (q  ϕ) =

j=1 V (qj−1  ϕ).

q(x)

dx.

2.2 A central limit theorem for AIS

The following theorem describes the asymptotic behavior of AIS. The conditions will be veriﬁed for
parametric updates in section 3 (see Theorem 3) in which case the asymptotic variance V∗ will be
explicitly given.
Theorem 1 (central limit theorem for AIS). Assume that the sequence qn satisﬁes

V (qn  ϕ) → V∗ 
for some V∗ (cid:62) 0 and that there exists η > 0 such that

a.s.

(4)

(5)

Then we have

sup
j∈N

√

n

(cid:90) (cid:107)ϕ(cid:107)2+η
(cid:90)
(cid:16)

q1+η
j

In −

ϕ

< ∞ 

a.s.

(cid:17) d→ N (0  V∗).

√

n(In−(cid:82) ϕ) = op(1) 

Remark 1 (zero-variance estimate). Suppose that p = 1 (recalling that ϕ : Rd → Rp). Theorem 1
includes the degenerate case V∗ = 0. This happens when the integrand has constant sign and the

sampling policy is well chosen  i.e. qn → |ϕ|/(cid:82) |ϕ|. In this case  we have that

√
meaning that the standard Monte Carlo convergence rate (1/
n) has been improved. This is inline
with the results presented in [33] where fast rates of convergence (compared to standard Monte Carlo)
are obtained under restrictive conditions on the allocation policy (nt). Note that other techniques
such as control variates  kernel smoothing or Gaussian quadrature can achieve fast convergence
rates [24  28  7  1].
Remark 2 (adaptive multiple importance sampling). Another way to compute the importance weights 
called multiple adaptive importance sampling  has been introduced in [32] and has been successfully
i=1 qi−1/j  xj
still being drawn under qj−1. The intuition is that this averaging will reduce the effect of exceptional
points xj for which |ϕ(xj)| (cid:29) qj−1(xj) (but |ϕ(xj)| (cid:54)(cid:29) ¯qj−1(xj)). Our approach is not able to
study this variant  simply because the martingale property described previously is not anymore
satisﬁed.

used in [26  5]. This consists in replacing qj−1 in the computation of Sj by ¯qj−1 =(cid:80)j

2.3 Normalized AIS

The normalization technique described in (2) is designed to compute(cid:82) ϕπ  where π is a density. It

is useful in the Bayesian context where π is only known up to a constant. As this technique seems
to provide substantial improvements compared to unnormalized estimates (i.e.  (1) with ϕ replaced
by ϕπ)  we recommend to use it even when the normalized constant of π is known. Normalized
estimators are given by

I (norm)
n

=

In(ϕπ)
In(π)

 

with In(ψ) = n−1

ψ(xj)/qj−1(xj).

function a (cid:55)→(cid:80)n

j=1(π(xj)/qj−1(xj))(ϕ(xj) − a)2. In contrast with In  I (norm)

Interestingly  normalized estimators are weighted least-squares estimates as they minimize the
has the following
shift-invariance property : whenever ϕ is shifted by µ  I (norm)
+ µ. Because
In(ϕπ) and In(π) are of the same kind as In deﬁned in the second AIS algorithm  a straightforward
application of Theorem 1 (with (ϕT π  π)T in place of ϕ).

simply becomes I (norm)

n

n

n

n(cid:88)

j=1

4

Corollary 1 (central limit theorem for normalized AIS). Suppose that (4) and (5) hold with
(ϕT π  π)T (in place of ϕ). Then we have

(cid:16)

√

n

(cid:90)

(cid:17) d→ N (0  U V∗U T ) 

I (norm)
n

−

ϕπ

with U = (Ip −(cid:82) ϕπ).

3 Parametric sampling policy

From this point forward  the sampling policies qt  t = 1  . . . T (we are back again to the sampling
policy scale as in Algorithm 1)  are chosen out of a parametric family of probability density functions
{qθ : θ ∈ Θ}. All our examples ﬁt the general framework of empirical risk minimization over the
parameter space Θ ⊂ Rq  where θt is given by

(6)

θt ∈ argminθ∈Θ Rt(θ) 

t(cid:88)

ns(cid:88)

Rt(θ) =

mθ(xs i)
qs−1(xs i)

 

section for examples). Note that Rt/Nt is an unbiased estimate of the risk r(θ) =(cid:82) mθ.

where qs is a shortcut for qθs  mθ : Rd → R might be understood as a loss function (see the next

s=1

i=1

3.1 Examples of sampling policy

We start by introducing a particular case  which is one of the simplest way to implement AIS. Then
we will provide more general approaches. In what follows  the targeted policy  denoted by f  is
chosen by the user and represents the distribution from which we wish to sample. It often reﬂects
some prior knowledge on the problem of interest. If ϕ : Rd → Rp  with p = 1  then (as discussed in

the introduction) f ∝ |ϕ| is optimal for (1) and f ∝ |ϕ −(cid:82) ϕπ|π is optimal for (2). In the Bayesian
context where many integrals(cid:82) (ϕ1  . . . ϕp)dπ need to be computed  a usual choice is f = π. All the

following methods only require calls to an unnormalized version of f.

t(cid:88)
ns(cid:88)
(cid:18) ν − 2

s=1

i=1

ν

µt =

Σt =

Method of moments with Student distributions.
In this case (qθ)θ∈Θ is just the family of mul-
tivariate Student distributions with ν > 2 degrees of freedom (ﬁxed parameter). The parameter θ
contains a location and a scale parameter µ and Σ. This family has two advantages: the parameter ν
allows tuning for heavy tails  and estimation is easy because moments of qθ are explicitly related to θ.
i=1 xs if (xs i)/qs−1(xs i)  but  as mentioned

A simple unbiased estimate for µ is (1/Nt)(cid:80)t
(cid:44) t(cid:88)
ns(cid:88)

in section 2.3  we prefer to use the normalized estimate (using the shortcut qs for qθs):

(cid:80)ns

f (xs i)

s=1

qs−1(xs i)

xs i

(cid:19) t(cid:88)

ns(cid:88)

f (xs i)

qs−1(xs i)

 

s=1

i=1

(cid:44) t(cid:88)

ns(cid:88)

f (xs i)

(7)

(xs i − µt)(xs i − µt)T f (xs i)
qs−1(xs i)

qs−1(xs i)

.

(8)

i=1

s=1

policy is chosen according to a moment matching condition  i.e. (cid:82) gqθ =(cid:82) gf for some function
Student case). Following [17]  choosing θ such that the empirical moments of g coincide with(cid:82) gqθ

Generalized method of moments (GMM). This approach includes the previous example. The
g : Rd → RD. For instance  g might be given by x (cid:55)→ x or x (cid:55)→ xxT (both are considered in the

s=1

i=1

might be impossible. We rather compute θt as the minimum of

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)Eθ(g) −

(cid:32) t(cid:88)

ns(cid:88)

s=1

i=1

(cid:44) t(cid:88)

ns(cid:88)

s=1

i=1

(cid:33)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

.

g(xs i)

f (xs i)

qs−1(xs i)

f (xs i)

qs−1(xs i)

Equivalently 

(cid:107)Eθ(g) − g(xs i)(cid:107)2
which embraces the form given by (6)  with mθ = (cid:107)Eθ(g) − g(cid:107)2f.

θt ∈ argminθ∈Θ

s=1

i=1

f (xs i)

qs−1(xs i)

 

t(cid:88)

ns(cid:88)

5

r(θ) = −(cid:82) log(qθ)f. Update of θt is done by minimizing the current estimator of Ntr(θ) given by

Kullback-Leibler approach. Following [31  section 5.5]  deﬁne the Kullback-Leibler risk as

Rt(θ) = Rt−1(θ) − nt(cid:88)

the variance over the class of sampling policies. In this case  deﬁne r(θ) =(cid:82) ϕ2/qθ  and follow a

Variance approach. Another approach  when ϕ : Rd → Rp with p = 1  consists in minimizing

i=1

similar approach as before by minimizing at each stage 

log(qθ(xt i))f (xt i)

qt−1(xt i)

.

(9)

Rt(θ) = Rt−1(θ) +

.

(10)

nt(cid:88)

ϕ(xt i)2

qθ(xt i)qt−1(xt i)

i=1

This case represents a different situation than the Kullback-Leibler approach and the GMM. Here 
the sampling policy is selected optimally with respect to a particular function ϕ whereas for KL and
GMM the sampling policy is driven by a targeted distribution f.
Remark 3 (computation cost). The update rule (6) might be computationally costly but alternatives
exist. For instance  when qθ is a family of Gaussian distributions  closed formulas are available
for (10). In fact we are in the case of weighted maximum likelihood estimation for which we ﬁnd
exactly (7) and (8)  with ν = ∞. This is computed online at no cost. Another strategy to reduce the
computation time is to use online stochastic gradient descent in (6).
Remark 4 (block estimator). In [22]  the authors suggest to update θ based only on the parti-
cles from the last stage. For the Kullback-Leibler update  (9) would be replaced by Rt(θ) =
i=1 log(qθ(xt i))f (xt i)/qt−1(xt i). While this update makes easier the theoretical analysis
(assuming that nt → ∞)  its main drawback is that most of the computing effort is forgotten at each
stage as the previous computations are not used.

−(cid:80)nt

3.2 Consistency of the sampling policy and asymptotic optimality of AIS

The updates described before using GMM  the Kullback-Leibler divergence or the variance  all ﬁt
within the framework of empirical risk minimization  given by (6)  which rewritten at the sample
scale gives

mθ(xj)
Rj(θ) = Rj−1(θ) +
qj−1(xj)
− if j ∈ {Nt : t (cid:62) 1} then :

− else :

θj ∈ argminθ∈Θ Rj(θ)
qj = qθj
qj = qj−1.

The proof follows from a standard approach from M-estimation theory [31  Theorem 5.7] but a
particular attention shall be payed to the uniform law of large numbers because of the missing i.i.d.
property of the sequences of interest.
Theorem 2 (concistency of the sampling policy). Set M (x) = supθ∈Θ mθ(x). Assume that Θ ⊂ Rq
(cid:90)
is a compact set and that

(cid:90) M (x)2

(cid:90)

(cid:90)

dx < ∞ 

and

∀θ (cid:54)= θ∗  r(θ) =

M (x)dx < ∞ 

mθ >

mθ∗ .

(11)

sup
θ∈Θ

qθ(x)

If moreover  for any x ∈ Rd  the function θ (cid:55)→ mθ(x) is continuous on Rq  then

θn → θ∗ 

a.s.

The conclusion given in Theorem 2 permits to check the conditions of Theorem 1. This leads to the
following result.
Theorem 3 (asymptotic optimality of AIS). Under the assumptions of Theorem 2  if there exists
η > 0 such that supθ∈Θ

(cid:90)
θ < ∞  we have
In −
where V (· ·) is deﬁned in Equation (3).

(cid:17) d→ N(cid:0)0  V (qθ∗   ϕ)(cid:1) 

(cid:82) (cid:107)ϕ(cid:107)2+η/q1+η
(cid:16)

√

ϕ

n

6

Remark 5 (the oracle property). From (11)  we deduce that qθ∗ is the unique minimizer of the risk
function r. The risk function based on GMM or the Kullback-Leibler approach (described in section
3.1) is derived from a certain targeted density f in such a way that if qθ = f  then r(θ) is a minimum.
Hence under the identiﬁability conditions of Theorem 2  if in addition f ∈ {qθ : θ ∈ Θ}  we
have that qθ∗ = f. This means that asymptotically  AIS achives the same variance as the “oracle”
importance sampling method based on the (ﬁxed) sampler f.
Corollary 2 (asymptotic optimality for normalized AIS). Under the assumptions of Theorem 2  if
there exists η > 0 such that supθ∈Θ
−

(cid:82) (cid:107)(ϕT π  π)(cid:107)2+η/q1+η
(cid:90)
(cid:17) d→ N(cid:16)

0  U V (qθ∗   (ϕT π  π)T )U T(cid:17)

θ < ∞  we have

I (norm)
n

(cid:16)

√

ϕπ

n

 

with U deﬁned in Corollary 1 and V (· ·) deﬁned in Equation (3).

4 Weighted AIS

We follow ideas from [9  section 4] to develop a novel method to estimate(cid:82) ϕπ. The method is called

weighted adaptive importance sampling (wAIS)  and will automatically re-weights each sample
depending on its accuracy. It allows in practice to forget poor samples generated during the early
stages. For clarity  suppose that ϕ : Rd → Rp with p = 1. Deﬁne the weighted estimate  for any
function ψ 

nt(cid:88)
Note that for any sequence (αT 1  . . . αT T ) such that(cid:80)T
estimate of(cid:82) ψ. Let σ2
(cid:80)T

T (ψ) = N−1
I (α)

T(cid:88)

αT t

t=1

i=1

T

T (ψ) is an unbiased
t = E[V (qt−1  ϕ)] where V (· ·) is deﬁned in Equation (3). The variance of
T (ϕ) is N−2
I (α)
  for each t = 1  . . . T .
T tntσ2
In [9]  a re-weighting is proposed using estimates of σt (based on sample of the t-th stage). We
propose the following weights

t which minimized w.r.t. (α) gives αT t ∝ σ−2

t=1 ntαT t = NT   I (α)

t=1 α2

T

t

ψ(xt i)
qt−1(xt i)

.

T t ∝ nt(cid:88)

α−1

i=1

(cid:18) π(xt i)

qt−1(xt i)

(cid:19)2

− 1

 

(12)

t=1 ntαT t = NT . The wAIS estimate is the (weighted and normalized)

satisfying the constraints(cid:80)T

AIS estimate given by

T (π).

I (α)
T (ϕπ)/I (α)

(13)
In contrast with the approach in [9]  because our weights are based on the estimated variance of
π/qt−1  our proposal is free from the integrand ϕ and thus reﬂects the overall quality of the t-th sample.
This makes sense whenever many functions need to be integrated making inappropriate a re-weighting
depending on a speciﬁc function. Another difference with [9] is that we use the true expectation  1  in
i=1 π(xt i)/qt−1(xt i). This permits
to avoid the situation (common in high dimensional settings) where a poor sampler qt−1 is such that
π(xt i)/qt−1(xt i) (cid:39) 0  for all i = 1  . . . nt  implying that the classical estimate of the variance is
near 0  leading (unfortunately) to a large weight.

the estimate of the variance  rather than the estimate (1/nt)(cid:80)nt

5 Numerical experiments

In this section  we study a toy Gaussian example to illustrate the practical behavior of AIS. Special
interest is dedicated to the effect of the dimension d  the practical choice of (nt) and the gain given
by wAIS introduced in the previous section. We set NT = 1e5 and we consider d = 2  4  8  16. The
code is made available at https://github.com/portierf/AIS.

The aim is to compute µ∗ = (cid:82) xφµ∗ σ∗ (x)dx where φµ σ : Rd → R is the probability density

of N (µ  σ2Id)  µ∗ = (5  . . . 5)T ∈ Rd  σ∗ = 1. The sampling policy is taken in the collection
of multivariate Student distributions of degree ν = 3 denoted by {qµ Σ0 : µ ∈ Rd} with Σ0 =

7

Figure 1: From left-to-right and top-to-bottom d = 2  4  8  16. AIS and wAIS are computed with
T = 50 with a constant allocation policy nt = 2e3. Plotted is the logarithm of the MSE (computed
for each method over 100 replicates) with respect to the number of requests to the integrand.

σ0Id(ν − 2)/ν and σ0 = 5. The initial sampling policy is set as µ0 = (0  . . . 0) ∈ Rd. The mean µt
is updated at each stage t = 1  . . . T following the GMM approach as described in section 3  leading
to the simple update formula

t(cid:88)

ns(cid:88)

s=1

i=1

µt =

(cid:44) t(cid:88)

ns(cid:88)

s=1

i=1

xs i

f (xs i)

qs−1(xs i)

f (xs i)

qs−1(xs i)

 

with f = φµ∗ σ∗. In section C of the supplementary ﬁle  other results considering the update of the
variance within the student family are provided.
As the results for the unnormalized approaches were far from being competitive with the normalized
ones  we consider only normalized estimators. We also tried the weights proposed in [9] but the
results were not competitive. The (normalized) AIS estimate of µ∗ is simply given by µt as displayed
above. The wAIS estimate of µ∗ is computed using (13) with weights (12).
We also include the adaptive MH proposed in [15]  where the proposal  assuming that Xi−1 = x 

is given by N(cid:0)x  (2.4)2(Ci + Id)/d(cid:1)  if i > i0  and N (x Id)  if i (cid:54) i0  with Ci the empirical

covariance matrix of (X0  X1  . . . Xi−1)  i0 = 1000 and  = 0.05 (other conﬁgurations as for
instance using only half of the chain have been tested without improving the results). Finally we

8

0e+002e+044e+046e+048e+041e+05−10−8−6−4−2sample sizelog of MSEAISwAISAMHoracle 0e+002e+044e+046e+048e+041e+05−10−8−6−4−202sample sizelog of MSEAISwAISAMHoracle 0e+002e+044e+046e+048e+041e+05−505sample sizelog of MSEAISwAISAMHoracle 0e+002e+044e+046e+048e+041e+05−505sample sizelog of MSEAISwAISAMHoracle Figure 2: From left-to-right and top-to-bottom d = 2  4  8  16. AIS and wAIS are computed with
T = 5  20  50  each with a constant allocation policy  resp. nt = 2e4  5e3  2e3. Plotted is the
logarithm of the MSE (computed for each method over 100 replicates) with respect to the number of
requests to the integrand.

consider a so called “oracle” method : importance sampling with ﬁx policy qµ∗ Σ∗  with Σ∗ =
σ∗Id(ν − 2)/ν.
For each method that returns µ  the mean squared error (MSE) is computed as the average of
(cid:107)µ − µ∗(cid:107)2 computed over 100 replicates of µ.
In Figure 1  we compare the evolution of all the mentioned algorithms with respect to stages
t = 1  . . . T = 50 with constant allocation policy nt = 2e3 (for AIS and wAIS). The clear winner
is wAIS. Note that the oracle policy qµ∗ Σ∗  which is not the optimal one (see section B.3 in the
supplementary material)  seems to give worse results than the the policy qµ∗ Σ0  as wAIS with sig_0
performs better than the “oracle” after some time.
In Figure 2  we examine 3 constant allocation policies given by T = 50 and nt = 2e3; T = 20 and
nt = 5e3; T = 5 and nt = 2e4. We clearly notice that the rate of convergence is inﬂuenced by the
number of update steps (at least at the beginning). The results call for updating as soon as possible
the sampling policy. This empirical evidence supports the theoretical framework studied in the paper
which imposes no condition on the growth of (nt).

9

0e+002e+044e+046e+048e+041e+05−10−8−6−4−2sample sizelog of MSEwAISAMHoracle T=5T=20T=500e+002e+044e+046e+048e+041e+05−10−8−6−4−202sample sizelog of MSEwAISAMHoracle T=5T=20T=500e+002e+044e+046e+048e+041e+05−505sample sizelog of MSEwAISAMHoracle T=5T=20T=500e+002e+044e+046e+048e+041e+05−505sample sizelog of MSEwAISAMHoracle T=5T=20T=50Acknowledgments

The authors are grateful to Rémi Bardenet for useful comments and additional references.

References
[1] Rémi Bardenet and Adrien Hardy. Monte carlo with determinantal point processes. arXiv

preprint arXiv:1605.00361  2016.

[2] Olivier Cappé  Randal Douc  Arnaud Guillin  Jean-Michel Marin  and Christian P Robert.
Adaptive importance sampling in general mixture classes. Statistics and Computing  18(4):447–
459  2008.

[3] Olivier Cappé  Arnaud Guillin  Jean-Michel Marin  and Christian P Robert. Population monte

carlo. Journal of Computational and Graphical Statistics  13(4):907–929  2004.

[4] Nicolas Chopin. Central limit theorem for sequential monte carlo methods and its application

to bayesian inference. The Annals of Statistics  32(6):2385–2411  2004.

[5] Jean Cornuet  Jean-Michel Marin  Antonietta Mira  and Christian P Robert. Adaptive multiple

importance sampling. Scandinavian Journal of Statistics  39(4):798–812  2012.

[6] Pierre Del Moral  Arnaud Doucet  and Ajay Jasra. Sequential monte carlo samplers. Journal of

the Royal Statistical Society: Series B (Statistical Methodology)  68(3):411–436  2006.

[7] Bernard Delyon  François Portier  et al. Integral approximation by kernel smoothing. Bernoulli 

22(4):2177–2208  2016.

[8] Randal Douc  Arnaud Guillin  J-M Marin  and Christian P Robert. Convergence of adaptive

mixtures of importance sampling schemes. The Annals of Statistics  pages 420–448  2007.

[9] Randal Douc  Arnaud Guillin  J-M Marin  and Christian P Robert. Minimum variance impor-
tance sampling via population monte carlo. ESAIM: Probability and Statistics  11:427–447 
2007.

[10] Randal Douc and Eric Moulines. Limit theorems for weighted samples with applications to

sequential monte carlo methods. The Annals of Statistics  pages 2344–2376  2008.

[11] Víctor Elvira  Luca Martino  David Luengo  and Mónica F Bugallo. Generalized multiple

importance sampling. arXiv preprint arXiv:1511.03095  2015.

[12] Akram Erraqabi  Michal Valko  Alexandra Carpentier  and Odalric Maillard. Pliable rejection

sampling. In International Conference on Machine Learning  pages 2121–2129  2016.

[13] Michael Evans and Tim Swartz. Approximating integrals via Monte Carlo and deterministic

methods. Oxford Statistical Science Series. Oxford University Press  Oxford  2000.

[14] John Geweke. Bayesian inference in econometric models using monte carlo integration. Econo-

metrica: Journal of the Econometric Society  pages 1317–1339  1989.

[15] Heikki Haario  Eero Saksman  and Johanna Tamminen. An adaptive metropolis algorithm.

Bernoulli  7(2):223–242  2001.

[16] John Michael Hammersley and David Christopher Handscomb. General principles of the monte

carlo method. In Monte Carlo Methods  pages 50–75. Springer  1964.

[17] Lars Peter Hansen. Large sample properties of generalized method of moments estimators.

Econometrica: Journal of the Econometric Society  pages 1029–1054  1982.

[18] Tatsunori B Hashimoto  Steve Yadlowsky  and John C Duchi. Derivative free optimization via

repeated classiﬁcation. arXiv preprint arXiv:1804.03761  2018.

[19] Tang Jie and Pieter Abbeel. On a connection between importance sampling and the likelihood
ratio policy gradient. In Advances in Neural Information Processing Systems  pages 1000–1008 
2010.

10

[20] Tuen Kloek and Herman K Van Dijk. Bayesian estimates of equation system parameters: an
application of integration by monte carlo. Econometrica: Journal of the Econometric Society 
pages 1–19  1978.

[21] Qi Lou  Rina Dechter  and Alexander T Ihler. Dynamic importance sampling for anytime
bounds of the partition function. In Advances in Neural Information Processing Systems  pages
3199–3207  2017.

[22] Jean-Michel Marin  Pierre Pudlo  and Mohammed Sedki. Consistency of the adaptive multiple

importance sampling. arXiv preprint arXiv:1211.2548  2012.

[23] Jan C Neddermeyer. Computationally efﬁcient nonparametric importance sampling. Journal of

the American Statistical Association  104(486):788–802  2009.

[24] Chris J. Oates  Mark Girolami  and Nicolas Chopin. Control functionals for Monte Carlo

integration. J. R. Statist. Soc. B  79(3):695–718  2017.

[25] Man-Suk Oh and James O. Berger. Adaptive importance sampling in Monte Carlo integration.

J. Statist. Comput. Simulation  41(3-4):143–168  1992.

[26] Art Owen and Yi Zhou. Safe and effective importance sampling. J. Amer. Statist. Assoc. 

95(449):135–143  2000.

[27] Jan Peters  Katharina Mülling  and Yasemin Altun. Relative entropy policy search. In AAAI 

pages 1607–1612. Atlanta  2010.

[28] François Portier and Johan Segers. Monte carlo integration with a growing number of control

variates. arXiv preprint arXiv:1801.01797  2018.

[29] Jean-Francois Richard and Wei Zhang. Efﬁcient high-dimensional importance sampling. Journal

of Econometrics  141(2):1385–1411  2007.

[30] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning  pages 1889–1897 
2015.

[31] A. W. van der Vaart. Asymptotic statistics  volume 3 of Cambridge Series in Statistical and

Probabilistic Mathematics. Cambridge University Press  Cambridge  1998.

[32] Eric Veach and Leonidas J Guibas. Optimally combining sampling techniques for monte carlo
rendering. In Proceedings of the 22nd annual conference on Computer graphics and interactive
techniques  pages 419–428. ACM  1995.

[33] Ping Zhang. Nonparametric importance sampling. J. Amer. Statist. Assoc.  91(435):1245–1253 

1996.

[34] Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized

loss minimization. In international conference on machine learning  pages 1–9  2015.

11

,Patrick Putzky
Giacomo Bassetto
Jakob Macke
François Portier
Bernard Delyon