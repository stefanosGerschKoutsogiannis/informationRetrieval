2019,Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints,Inverse reinforcement learning (IRL) enables an agent to learn complex behavior by observing demonstrations from a (near-)optimal policy. The typical assumption is that the learner's goal is to match the teacher‚Äôs demonstrated behavior. In this paper  we consider the setting where the learner has its own preferences that it additionally takes into consideration. These preferences can for example capture behavioral biases  mismatched worldviews  or physical constraints. We study two teaching approaches: learner-agnostic teaching  where the teacher provides demonstrations from an optimal policy ignoring the learner's preferences  and learner-aware teaching  where the teacher accounts for the learner‚Äôs preferences. We design learner-aware teaching algorithms and show that significant performance improvements can be achieved over learner-agnostic teaching.,Learner-aware Teaching: Inverse Reinforcement

Learning with Preferences and Constraints

Sebastian Tschiatschek‚àó

Microsoft Research

setschia@microsoft.com

gahana@mpi-sws.org

Ahana Ghosh‚àó

MPI-SWS

Luis Haug‚àó
ETH Zurich

lhaug@inf.ethz.ch

Rati Devidze
MPI-SWS

rdevidze@mpi-sws.org

Adish Singla
MPI-SWS

adishs@mpi-sws.org

Abstract

Inverse reinforcement learning (IRL) enables an agent to learn complex behavior
by observing demonstrations from a (near-)optimal policy. The typical assumption
is that the learner‚Äôs goal is to match the teacher‚Äôs demonstrated behavior. In this
paper  we consider the setting where the learner has its own preferences that it
additionally takes into consideration. These preferences can for example capture
behavioral biases  mismatched worldviews  or physical constraints. We study
two teaching approaches: learner-agnostic teaching  where the teacher provides
demonstrations from an optimal policy ignoring the learner‚Äôs preferences  and
learner-aware teaching  where the teacher accounts for the learner‚Äôs preferences.
We design learner-aware teaching algorithms and show that signiÔ¨Åcant performance
improvements can be achieved over learner-agnostic teaching.

1

Introduction

Inverse reinforcement learning (IRL) enables a learning agent (learner) to acquire skills from
observations of a teacher‚Äôs demonstrations. The learner infers a reward function explain-
ing the demonstrated behavior and optimizes its own behavior accordingly.
IRL has been
studied extensively [Abbeel and Ng  2004  Ratliff et al.  2006  Ziebart  2010  Boularias et al.  2011 
Osa et al.  2018] under the premise that the learner can and is willing to imitate the teacher‚Äôs behavior.
In real-world settings  however  a learner typically does not blindly follow the teacher‚Äôs demonstra-
tions  but also has its own preferences and constraints. For instance  consider demonstrating to an
auto-pilot of a self-driving car how to navigate from A to B by taking the most fuel-efÔ¨Åcient route.
These demonstrations might conÔ¨Çict with the preference of the auto-pilot to drive on highways in
order to ensure maximum safety. Similarly  in robot-human interaction with the goal of teaching
people how to cook  a teaching robot might demonstrate to a human user how to cook ‚Äúroast chicken‚Äù 
which could conÔ¨Çict with the preferences of the learner who is ‚Äúvegetarian‚Äù. To give yet another
example  consider a surgical training simulator which provides virtual demonstrations of expert
behavior; a novice learner might not be conÔ¨Ådent enough to imitate a difÔ¨Åcult procedure because of
safety concerns. In all these examples  the learner might not be able to acquire useful skills from the
teacher‚Äôs demonstrations.
In this paper  we formalize the problem of teaching a learner with preferences and constraints. First 
we are interested in understanding the suboptimality of learner-agnostic teaching  i.e.  ignoring the
learner‚Äôs preferences. Second  we are interested in designing learner-aware teachers who account

‚àóAuthors contributed equally to this work.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

for the learner‚Äôs preferences and thus enable more efÔ¨Åcient learning. To this end  we study a learner
model with preferences and constraints in the context of the Maximum Causal Entropy (MCE) IRL
framework [Ziebart  2010  Ziebart et al.  2013  Zhou et al.  2018]. This enables us to formulate the
teaching problem as an optimization problem  and to derive and analyze algorithms for learner-aware
teaching. Our main contributions are:

I We formalize the problem of IRL under preference constraints (Section 2 and Section 3).
II We analyze the problem of optimizing demonstrations for the learner when preferences are
known to the teacher  and we propose a bilevel optimization approach to the problem (Section 4).
III We propose strategies for adaptively teaching a learner with preferences unknown to the teacher 

and we provide theoretical guarantees under natural assumptions (Section 5).

IV We empirically show that signiÔ¨Åcant performance improvements can be achieved by learner-

aware teachers as compared to learner-agnostic teachers (Section 6).

r(cid:107)1 ‚â§ 1 ensures that |R(s)| ‚â§ 1 for all s.

for policies we are interested in is the expected discounted reward R(œÄ) := E ((cid:80)‚àû

r   ¬µr(œÄ)(cid:105)  where ¬µr : Œ† ‚Üí Rdr  œÄ (cid:55)‚Üí E ((cid:80)‚àû
(cid:80)

2 Problem Setting
Environment. Our environment is described by a Markov decision process (MDP) M :=
(S A  T  Œ≥  P0  R). Here S and A denote Ô¨Ånite sets of states and actions. T : S √ó S √ó A ‚Üí [0  1]
describes the state transition dynamics  i.e.  T (s(cid:48)|s  a) is the probability of landing in state s(cid:48) by
taking action a from state s. Œ≥ ‚àà (0  1) is the discounting factor. P0 : S ‚Üí [0  1] is an initial
distribution over states. R : S ‚Üí R is the reward function. We assume that there exists a feature
map œÜr : S ‚Üí [0  1]dr such that the reward function is linear  i.e.  R(s) = (cid:104)w‚àó
r   œÜr(s)(cid:105) for some
r ‚àà Rdr. Note that a bound of (cid:107)w‚àó
w‚àó
Basic deÔ¨Ånitions. A policy is a map œÄ : S√óA ‚Üí [0  1] such that œÄ( ¬∑ | s) is a probability distribution
over actions for every state s. We denote by Œ† the set of all such policies. The performance measure
t=0 Œ≥tR(st))  where
the expectation is taken with respect to the distribution over trajectories Œæ = (s0  s1  s2  . . .) induced
by œÄ together with the transition probabilities T and the initial state distribution P0. A policy œÄ is
optimal for the reward function R if œÄ ‚àà arg maxœÄ(cid:48)‚ààŒ† R(œÄ(cid:48))  and we denote an optimal policy by œÄ‚àó.
Note that R(œÄ) = (cid:104)w‚àó
t=0 Œ≥tœÜr(st))  is the map taking a
policy to its vector of (discounted) feature expectations. We denote by ‚Ñ¶r = {¬µr(œÄ) : œÄ ‚àà Œ†} the
image ¬µr(Œ†) of this map. Note that the set ‚Ñ¶r ‚àà Rdr is convex (see [Ziebart  2010  Theorem 2.8]
and [Abbeel and Ng  2004])  and also bounded due to the discounting factor Œ≥ ‚àà (0  1). For a Ô¨Ånite
collection of trajectories Œû = {si
2  . . .}i=1 2 ... obtained by executing a policy œÄ in the MDP
M  we denote the empirical counterpart of ¬µr(œÄ) by ÀÜ¬µr(Œû) := 1|Œû|
An IRL learner and a teacher. We consider a learner L implementing an inverse reinforcement
learning (IRL) algorithm and a teacher T. The teacher has access to the full MDP M; the learner
knows the MDP and the parametric form of reward function R(s) = (cid:104)wr  œÜr(s)(cid:105) but does not know
the true reward parameter w‚àó
r. The learner  upon receiving demonstrations from the teacher  outputs
a policy œÄL using its algorithm. The teacher‚Äôs objective is to provide a set of demonstrations ŒûT to
the learner that ensures that the learner‚Äôs output policy œÄL achieves high reward R(œÄL).
The standard IRL algorithms are based on the idea of feature matching [Abbeel and Ng  2004 
Ziebart  2010  Osa et al.  2018]: The learner‚Äôs algorithm Ô¨Ånds a policy œÄL that matches the feature
expectations of the received demonstrations  ensuring that (cid:107)¬µr(œÄL) ‚àí ÀÜ¬µr(ŒûT)(cid:107)2 ‚â§  where 
speciÔ¨Åes a desired level of accuracy. In this standard setting  the learner‚Äôs primary goal is to imitate
the teacher (via feature matching) and this makes the teaching process easy. In fact  the teacher just
needs to provide a sufÔ¨Åciently rich pool of demonstrations ŒûT obtained by executing œÄ‚àó  ensuring
(cid:107)ÀÜ¬µr(ŒûT)‚àí ¬µr(œÄ‚àó)(cid:107)2 ‚â§ . This guarantees that (cid:107)¬µr(œÄL)‚àí ¬µr(œÄ‚àó)(cid:107)2 ‚â§ 2. Furthermore  the linearity
of rewards and (cid:107)w‚àó
r(cid:107)1 ‚â§ 1 ensures that the learner‚Äôs output policy œÄL satisÔ¨Åes R(œÄL) ‚â• R(œÄ‚àó) ‚àí 2.
Key challenges in teaching a learner with preference constraints. In this paper  we study a novel
setting where the learner has its own preferences which it additionally takes into consideration when
learning a policy œÄL using teacher‚Äôs demonstrations. We formally specify our learner model in the
next section; here we highlight the key challenges that arise in teaching such a learner. Given that
the learner‚Äôs primary goal is no longer just imitating the teacher via feature matching  the learner‚Äôs
output policy can be suboptimal with respect to the true reward even if it had access to ¬µr(œÄ‚àó)  i.e. 

0  si

1  si

(cid:80)

t Œ≥tœÜr(si

t).

i

2

(a) Environment

(b) Set of ¬µr(œÄ) vectors

Figure 1: An illustrative example to showcase the suboptimality of teaching when the learner
has preferences and constraints. Environment: Figure 1a shows a grid-world environment in-
spired by the object-world and gathering game environments [Levine et al.  2010  Leibo et al.  2017 
Mendez et al.  2018]. Each cell represents a state  there are Ô¨Åve actions given by ‚Äúleft"  ‚Äúup"  ‚Äúright" 
"down"  ‚Äústay"  the transitions are deterministic  and the starting state is the top-left cell. The agent‚Äôs
goal is to collect objects in the environment: Collecting a ‚Äústar" provides a reward of 1.0 and a ‚Äúplus"
a reward of 0.9; objects immediately appear again upon collection  and the rewards are discounted
with Œ≥ close to 1. The optimal policy œÄ‚àó is to go to the nearest ‚Äústar" and then ‚Äústay" there. Pref-
erences: A small number of states in the environment are distractors  depicted by colored cells in
Figure 1a. We consider a learner who prefers to avoid ‚Äúgreen" distractors: it has a hard constraint that
the probability of having a ‚Äúgreen" distractor within a 3x3 neighborhood  i.e.  1-cell distance  is at
most  = 0.1. Feature expectation vectors: Figure 1b shows the set of feature expectation vectors
{¬µr(œÄ) : œÄ ‚àà Œ†}. The x-axis and the y-axis represent the discounted feature count for collecting
‚Äústar" and ‚Äúplus" objects  respectively. The striped region represents policies that are feasible w.r.t. the
learner‚Äôs constraint. Suboptimality of teaching: Upon receiving demonstrations from an optimal
policy œÄ‚àó with feature vector ¬µr(œÄ‚àó)  the learner under its preference constraint can best match the
teacher‚Äôs demonstrations (in a sense of minimizing (cid:107)¬µr(œÄL) ‚àí ¬µr(œÄ‚àó)(cid:107)2) by outputting a policy with
¬µr(œÄ2)  which is clearly suboptimal w.r.t. the true rewards. Policy œÄ3 with feature vector ¬µr(œÄ3)
represents an alternate teaching policy which would have led to higher reward for the learner.
the feature expectation vector of an optimal policy œÄ‚àó. Figure 1 provides an illustrative example
to showcase the suboptimality of teaching when the learner has preferences and constraints. The
key challenge that we address in this paper is that of designing a teaching algorithm that selects
demonstrations while accounting for the learner‚Äôs preferences.

3 Learner Model
In this section we describe the learner models we consider  including different ways of deÔ¨Åning
preferences and constraints. First  we introduce some notation and deÔ¨Ånitions that will be helpful.
We capture learner‚Äôs preferences via a feature map œÜc : S ‚Üí [0  1]dc. We deÔ¨Åne œÜ(s) as a con-
catenation of the two feature maps œÜr(s) and œÜc(s) given by [œÜr(s)‚Ä†  œÜc(s)‚Ä†]‚Ä† and let d = dr + dc.
t=0 Œ≥tœÜc(st)) and ¬µ : Œ† ‚Üí Rd 
t=0 Œ≥tœÜ(st)). Similar to ‚Ñ¶r  we deÔ¨Åne ‚Ñ¶c ‚äÜ Rdc and ‚Ñ¶ ‚äÜ Rd as the images of the maps

Similar to the map ¬µr  we deÔ¨Åne ¬µc : Œ† ‚Üí Rdc  œÄ (cid:55)‚Üí E ((cid:80)‚àû
œÄ (cid:55)‚Üí E ((cid:80)‚àû

t=0 Œ≥tE(cid:104) ‚àí log œÄ(at | st)
(cid:105)
(cid:80)‚àû

¬µc(Œ†) and ¬µ(Œ†). Note that for any policy œÄ ‚àà Œ†  we have ¬µ(œÄ) = [¬µr(œÄ)‚Ä†  ¬µc(œÄ)‚Ä†]‚Ä†.
Standard (discounted) MCE-IRL. Our learner models build on the (discounted) Maximum
Causal Entropy (MCE) IRL framework [Ziebart et al.  2008  Ziebart  2010  Ziebart et al.  2013 
Zhou et al.  2018]. In the standard (discounted) MCE-IRL framework  a learning agent aims to iden-
tify a policy that matches the feature expectations of the teacher‚Äôs demonstrations while simultaneously
maximizing the (discounted) causal entropy given by H(œÄ) := H({at}t=0 1 ...(cid:107){st}t=0 1 ...) :=
. More background is provided in Appendix D of the supplementary.
Including preference constraints. The standard framework can be readily extended to include
learner‚Äôs preferences in the form of constraints on the preference features œÜc. Clearly  the learner‚Äôs
preferences can render exact matching of the teacher‚Äôs demonstrations infeasible and hence we relax
this condition. To this end  we consider the following generic learner model:

max
r ‚â•0  Œ¥soft

œÄ  Œ¥soft

c ‚â•0
s.t.

r (cid:107)p ‚àí Cc ¬∑ (cid:107)Œ¥soft

H(œÄ) ‚àí Cr ¬∑ (cid:107)Œ¥soft
|¬µr(œÄ)[i] ‚àí ÀÜ¬µr(ŒûT)[i]| ‚â§ Œ¥hard
gj(¬µc(œÄ)) ‚â§ Œ¥hard

c (cid:107)p
[i] + Œ¥soft
[j] + Œ¥soft

r

r

c

(1)

[i] ‚àÄi ‚àà {1  2  . . .   dr}
[j] ‚àÄj ‚àà {1  2  . . .   m} 

c

3

ùúáùëü(ùúã‚àó)ùúáùëü(ùúã2)ùúáùëü(ùúã3)ùúñmin

œÄ
s.t.

(cid:107)¬µr(œÄ) ‚àí ÀÜ¬µr(ŒûT)(cid:107)p
gj(¬µc(œÄ)) ‚â§ 0 ‚àÄj ‚àà {1  2  . . .   m}.

(2)

Here  g : Rdc (cid:55)‚Üí R are m convex functions representing preference constraints. The coefÔ¨Åcients Cr
and Cc are the learner‚Äôs parameters which quantify the relative importance of matching the teacher‚Äôs
demonstrations and satisfying the learner‚Äôs preferences. The learner model is further characterized
c ‚àà Rm‚â•0).
by parameters Œ¥hard
The optimization variables for the learner are given by œÄ  Œ¥soft
[j] (we will use the vector
c
notation as Œ¥soft
  Œ¥hard
) and optimization variables
(Œ¥soft

[j] (we will use the vector notation as Œ¥hard
[i]  and Œ¥soft
c ‚àà Rm‚â•0). These parameters (Œ¥hard

[i] and Œ¥hard
r ‚àà Rdr‚â•0 and Œ¥soft

) characterize the following behavior:

r ‚àà Rdr‚â•0 and Œ¥hard

  Œ¥soft

r

r

r

c

c

r

c
‚Ä¢ While a mismatch of up to Œ¥hard
tions incurs no cost regarding the optimization objective  a mismatch larger than Œ¥hard
a cost of Cr ¬∑ (cid:107)Œ¥soft
‚Ä¢ Similarly  while a violation of up to Œ¥hard
no cost regarding the optimization objective  a violation larger than Œ¥hard
Cc ¬∑ (cid:107)Œ¥soft

between the learner‚Äôs and teacher‚Äôs reward feature expecta-
incurs

of the learner‚Äôs preference constraints incurs
incurs a cost of

r (cid:107)p.

c (cid:107)p.

r

c

c

r

Next  we discuss two special instances of this generic learner model.

3.1 Learner Model with Hard Preference Constraints

It is instructive to study a special case of the above-mentioned generic learner model. Let us consider
c = 0  and a limiting case with Cr  Cc (cid:29) 0 such that the term
the model in Eq. 1 with Œ¥hard
H(œÄ) can be neglected. Now  if we additionally assume that Cc (cid:29) Cr  the learner‚Äôs objective can be
thought of as Ô¨Ånding a policy œÄ that minimizes the Lp norm distance to the reward feature expectations
of the teacher‚Äôs demonstration while satisfying the constraints gj(¬µc(œÄ)) ‚â§ 0 ‚àÄj ‚àà {1  2  . . .   m}.
More formally  we study the following learner model given in Eq. 2 below:

r = 0  Œ¥hard

To get a better understanding of the model  we can deÔ¨Åne the learner‚Äôs constraint set as ‚Ñ¶L := {¬µ :
¬µ ‚àà ‚Ñ¶ s.t. gj(¬µc) ‚â§ 0 ‚àÄj ‚àà {1  2  . . .   m}}. Similar to ‚Ñ¶L  we deÔ¨Åne ‚Ñ¶L
r is the
projection of the set ‚Ñ¶L to the subspaces Rdr. We can now rewrite the above optimization problem as
minœÄ : ¬µr(œÄ)‚àà‚Ñ¶L
(i) Learner can match: When ÀÜ¬µr(ŒûT) ‚àà ‚Ñ¶L
r  the learner outputs a policy œÄL s.t. ¬µr(œÄL) = ÀÜ¬µr(ŒûT).
(ii) Learner cannot match: Otherwise  the learner outputs a policy œÄL such that ¬µr(œÄL) is given by

(cid:107)¬µr(œÄ) ‚àí ÀÜ¬µr(ŒûT)(cid:107)p. Hence  the learner‚Äôs behavior is given by:

r ‚äÜ ‚Ñ¶r where ‚Ñ¶L

r

the Lp norm projection of the vector ÀÜ¬µr(ŒûT) onto the set ‚Ñ¶L
r.

Figure 1 provides an illustration of the behavior of this learner model. We will design learner-aware
teaching algorithms for this learner model in Section 4.1 and Section 5.

3.2 Learner Model with Soft Preference Constraints

Another interesting learner model that we study in this paper arises from the generic learner when
we consider m = dc number of box-type linear constraints with gj(¬µc(œÄ)) = ¬µc(œÄ)[j] ‚àÄj ‚àà
{1  2  . . .   dc}. We consider an L1 norm penalty on violation  and for simplicity we consider
Œ¥hard
r

[i] = 0 ‚àÄi ‚àà {1  2  . . .   dr}. In this case  the learner‚Äôs model is given by

max
r ‚â•0  Œ¥soft

œÄ  Œ¥soft

c ‚â•0
s.t.

H(œÄ) ‚àí Cr ¬∑ (cid:107)Œ¥soft
|¬µr(œÄ)[i] ‚àí ÀÜ¬µr(ŒûT)[i]| ‚â§ Œ¥soft
¬µc(œÄ)[j] ‚â§ Œ¥hard

c (cid:107)1
r (cid:107)1 ‚àí Cc ¬∑ (cid:107)Œ¥soft
[i] ‚àÄi ‚àà {1  2  . . .   dr}
[j] + Œ¥soft

r

c

[j] ‚àÄj ‚àà {1  2  . . .   dc} 

c

(3)

The solution to the above problem corresponds to a softmax policy with a reward function RŒª(s) =
(cid:104)wŒª  œÜ(s)(cid:105) where wŒª ‚àà Rd is parametrized by Œª. The optimal parameters Œª can be computed
efÔ¨Åciently and the corresponding softmax policy is then obtained by Soft-Value-Iteration procedure
(see [Ziebart  2010  Algorithm. 9.1]  [Zhou et al.  2018]). Details are provided in Appendix E of
the supplementary. We will design learner-aware teaching algorithms for this learner model in
Section 4.2.

4

4 Learner-aware Teaching under Known Constraints

In this section  we analyze the setting when the teacher has full knowledge of the learner‚Äôs constraints.

4.1 A Learner-aware Teacher for Hard Preferences: AWARE-CMDP

Here  we design a learner-aware teaching algorithm when considering the learner from Section 3.1.
Given that the teacher has full knowledge of the learner‚Äôs preferences  it can compute an optimal
teaching policy by maximizing the reward over policies that satisfy the learner‚Äôs preference constraints 
i.e.  the teacher solves a constrained-MDP problem (see [De  1960  Altman  1999]) given by

(cid:104)w‚àó

r   ¬µr(œÄ)(cid:105)

s.t. ¬µr(œÄ) ‚àà ‚Ñ¶L
r.

max

œÄ

We refer to an optimal solution of this problem as œÄaware and the corresponding teacher as AWARE-
CMDP. We can make the following observation formalizing the value of learner-aware teaching:
Theorem 1. For simplicity  assume that the teacher can provide an exact feature expectation ¬µ(œÄ)
of a policy instead of providing demonstrations to the learner. Then  the value of learner-aware
teaching is

(cid:68)

(cid:69) ‚àí(cid:68)

max

œÄ s.t. ¬µr(œÄ)‚àà‚Ñ¶L

r

w‚àó
r   ¬µr(œÄ)

w‚àó
r   Proj‚Ñ¶L

r

(cid:0)¬µr(œÄ‚àó)(cid:1)(cid:69) ‚â• 0.

When the set ‚Ñ¶L is deÔ¨Åned via a set of linear constraints  the above problem can be formulated as a
linear program and solved exactly. Details are provided in Appendix F the supplementary material.

4.2 A Learner-aware Teacher for Soft Preferences: AWARE-BIL

For the learner models in Section 3  the optimal learner-aware teaching problem can be naturally
formalized as the following bi-level optimization problem:
s.t. œÄL ‚àà arg max

IRL(œÄ  ¬µ(œÄT)) 

R(œÄL)

(4)

œÄ

max
œÄT

where IRL(œÄ  ¬µ(œÄT)) stands for the IRL problem solved by the learner given demonstrations from
œÄT and can include preferences of the learner (see Eq. 1 in Section 3).
There are many possibilities for solving this bi-level optimization problem‚Äîsee for exam-
ple [Sinha et al.  2018] for an overview. In this paper we adopted a single-level reduction approach
to simplify the above bi-level optimization problem as this results in particularly intuitive optimizia-
tion problems for the teacher. The basic idea of single-level reduction is to replace the lower-level
problem  i.e.  arg maxœÄ IRL(œÄ  ¬µ(œÄT))  by the optimality conditions for that problem given by the
Karush-Kuhn-Tucker conditions [Boyd and Vandenberghe  2004  Sinha et al.  2018]. For the learner
model outlined in Section 3.2  these reductions take the following form (see Appendix G in the
supplementary material for details):

{0 ‚â§ Œ≤ ‚â§ Cc AND ¬µc(œÄŒª) ‚â§ Œ¥hard

c } OR {Œ≤ = Cc AND ¬µc(œÄŒª) ‚â• Œ¥hard
c }

where œÄŒª corresponds to a softmax policy with a reward function RŒª(s) = (cid:104)wŒª  œÜ(s)(cid:105) for
wŒª = [(Œ±low ‚àí Œ±up)‚Ä† ‚àíŒ≤‚Ä†]‚Ä†. Thus  Ô¨Ånding optimal demonstrations means optimization over
softmax teaching policies while respecting the learner‚Äôs preferences. To actually solve the above opti-
mization problem and Ô¨Ånd good teaching policies  we use an approach inspired by the Frank-Wolfe
algorithm [Jaggi  2013] detailed in Appendix G of the supplementary material. We refer to a teacher
implementing this approach as AWARE-BIL.

5 Learner-Aware Teaching Under Unknown Constraints

In this section  we consider the more realistic and challenging setting in which the teacher T does not
know the learner L‚Äôs constraint set ‚Ñ¶L
r. Without feedback from L  T can generally not do better than

5

max

Œª:={Œ±low‚ààRdr   Œ±up‚ààRdr   Œ≤‚ààRdc}
s.t.

(cid:104)w‚àó
r   ¬µr(œÄŒª)(cid:105)
0 ‚â§ Œ±low ‚â§ Cr
0 ‚â§ Œ±up ‚â§ Cr

(5)

the agnostic teacher who simply ignores any constraints. We therefore assume that T and L interact
in rounds as described by Algorithm 1. The two versions of the algorithm we describe in Sections 5.1
and 5.2 are obtained by specifying how T adapts the teaching policy in each round.

Algorithm 1 Teacher-learner interaction in the adaptive teaching setting
1: Initial teaching policy œÄT 0 (e.g.  optimal policy ignoring any constraints)
2: for round i = 0  1  2  . . . do
3:
4:
5:

Teacher provides demonstrations with feature vector ¬µT i
Learner upon receiving ¬µT i
Teacher observes learner‚Äôs feature vector ¬µL i

r using policy œÄT i
computes a policy œÄL i with feature vector ¬µL i
r
r and adapts the teaching policy

r

In this section  we assume that L is as described in Section 3.1: Given demonstrations ŒûT  L
Ô¨Ånds a policy œÄL such that ¬µr(œÄL) matches the L2-projection of ÀÜ¬µr(ŒûT) onto ‚Ñ¶L
r. For the sake of
simplifying the presentation and the analysis  we also assume that L and T can observe the exact
feature expectations of their respective policies  e.g.  ÀÜ¬µr(ŒûT) = ¬µr(œÄT) if ŒûT is sampled from œÄT.

5.1 An Adaptive Learner-aware Teacher Using Volume Search: ADAWARE-VOL

In our Ô¨Årst adaptive teaching algorithm ADAWARE-VOL  T maintains an estimate ÀÜ‚Ñ¶L
r of the
learner‚Äôs constraint set  which in each round gets updated by intersecting the current version with
a certain afÔ¨Åne halfspace  thus reducing the volume of ÀÜ‚Ñ¶L
r. The new teaching policy is then any
policy œÄT i+1 which is optimal under the constraint that ¬µT i+1 ‚àà ÀÜ‚Ñ¶L
r. The interaction ends as soon
r (cid:107)2 ‚â§  for a threshold . Details are provided in Appendix C.1 of the supplementary.
as (cid:107)¬µL i
Theorem 2. Upon termination of ADAWARE-VOL  L‚Äôs output policy œÄL satisÔ¨Åes R(œÄL) ‚â•
R(œÄaware) ‚àí  for any policy œÄaware which is optimal under L‚Äôs constraints. For the special case that
r is a polytope deÔ¨Åned by m linear inequalities  the algorithm terminates in O(mdr ) iterations.
‚Ñ¶L

r ‚àí ¬µT i

r ‚äÉ ‚Ñ¶L

5.2 An Adaptive Learner-aware Teacher Using Line Search: ADAWARE-LIN

r + Œ±w‚àó

= ¬µL i

r + Œ±iw‚àó

r ‚àí Œ±minw‚àó

‚àà arg min¬µr‚àà‚Ñ¶r (cid:107)¬µr ‚àí ¬µL i

In our second adaptive teaching algorithm  ADAWARE-LIN  T adapts the teaching policy by per-
forming a binary search on a line segment of the form {¬µL i
r | Œ± ‚àà [Œ±min  Œ±max]} ‚äÇ Rdr
to Ô¨Ånd a vector ¬µT i+1
r that is the vector of feature expectations of a policy; here
Œ±max > Œ±min > 0 are Ô¨Åxed constants. If that is not successful  the teacher Ô¨Ånds a teaching policy with
r(cid:107)2. The following theorem analyzes the convergence
¬µT i+1
r
of L‚Äôs performance to RL := max¬µr‚àà‚Ñ¶r R(¬µr) under the assumption that T‚Äôs search succeeds in
every round. The proof and further details are provided in Appendix C.2 of the supplementary.
Theorem 3. Fix some Œµ > 0 and assume that there exists a constant Œ±min > 0 such that  as long as
RL ‚àí R(¬µL i
r + Œ±iw‚àó
for some Œ±i ‚â• Œ±min. Then the learner‚Äôs performance increases monotonically in each round of
r
ADAWARE-LIN  i.e.  R(¬µL i+1
Œµ ) teaching steps 
) > R(¬µL i
the learner‚Äôs performance satisÔ¨Åes R(¬µL i

r ). Moreover  after at most O( D2
ŒµŒ±min
r ) > RL ‚àí 2Œµ. Here we abbreviate D := diam ‚Ñ¶r.

r ) > Œµ  the teacher can Ô¨Ånd a teaching policy œÄT i+1 satisfying ¬µT i+1

= ¬µL i

r

log D

r

r

6 Experimental Evaluation

In this section we evaluate our teaching algorithms for different types of learners on the environment
introduced in Figure 1. The environment we consider here has three types of reward objects  i.e.  a
‚Äústar" object with reward of 1.0  a ‚Äúplus" object with reward of 0.9  and a ‚Äúdot" object with reward of
0.2. Two objects of each type are placed randomly on the grid such that there is always only a single
object in each grid cell. The presence of an object of type ‚Äústar‚Äù  ‚Äúplus‚Äù  or ‚Äúdot‚Äù in some state s is
encoded in the reward features œÜr(s) by a binary-indicator for each type such that dr = 3. We use a
discount factor of Œ≥ = 0.99. Upon collecting an object  there is a 0.1 probability of transiting to a
terminal state.
Learner models. We consider a total of 5 different learners whose preferences can be described by
distractors in the environment. Each learner prefers to avoid a certain subset of these distractors.

6

There is a total of 4 of distractors: (i) two ‚Äúgreen" distractors are randomly placed at a distance of
0-cell and 1-cell to the ‚Äústar" objects  respectively; (ii) two ‚Äúyellow" distractors are randomly placed
at a distance of 1-cell and 2-cells to the ‚Äúplus" objects  respectively  see Figure 2a.
Through these distractors we deÔ¨Åne learners L1-L5 as follows: (L1) no preference features (dc = 0);
(L2) two preference features (dc = 2) such that œÜc(s)[1] and œÜc(s)[2] are binary indicators of whether
there is a ‚Äúgreen" distractor at a distance of 0-cells or 1-cell  respectively; (L3) four preference features
(dc = 4) such that œÜc(s)[1]  œÜc(s)[2] are as for L2  and œÜc(s)[3] and œÜc(s)[4] are binary indicators of
whether there is a ‚Äúgreen" distractor at a distance of 2-cells or a ‚Äúyellow‚Äù distractor at a distance of
0-cells  respectively; (L4) Ô¨Åve preference features (dc = 5) such that œÜc(s)[1]  . . .   œÜc(s)[4] are as
for L3  and œÜc(s)[5] is a binary indicator whether there is a ‚Äúyellow‚Äù distractor at a distance of 1-cell;
and (L5) six preference features (dc = 6) such that œÜc(s)[1]  . . .   œÜc(s)[5] are as for L4  and œÜc(s)[6]
is a binary indicator whether there is a ‚Äúyellow‚Äù distractor at a distance of 2-cells.
The Ô¨Årst row in Figure 2 shows an instance of the considered object-worlds and indicates the
preference of the learners to avoid certain regions by the gray area.

(a) Environments and learners‚Äô preferences for 5 different learners L1  . . .  L5

(b) Learners‚Äô rewards inferred from learner-agnostic teacher‚Äôs (AGNOSTIC) demonstrations

(c) Learners‚Äô rewards inferred from learner-aware teacher‚Äôs (AWARE-BIL) demonstrations

Figure 2: Teaching in object-world environments under full knowledge of the learner‚Äôs preferences.
Green and yellow cells indicate distractors associated with either ‚Äústar" or ‚Äúplus" objects  respectively.
Learner‚Äôs preferences to avoid cells are indicated in gray. Learner model from Section 3.2 with
Cr = 5  Cc = 10  and Œ¥hard
c = 0 is considered for these experiments. The learner-aware teacher
enable the learner to infer reward functions that are compatible with the learner‚Äôs preferences and
achieve higher average rewards. In Figure 2b and Figure 2c  blue color represents positive reward 
red color represents negative reward  and the magnitude of the reward is indicated by color intensity.

6.1 Teaching under known constraints

In this section we consider learners with soft constraints from Section 3.2  with preference features as
described above  and parameters Cr = 5  Cc = 10  and Œ¥hard
c = 0 (more experimental results for dif-
ferent values of Cr and Cc are provided in Appendix B.1 of the supplementary). Our Ô¨Årst results are
presented in Figure 2. The second and third rows show the rewards inferred by the learners for demon-
strations provided by a learner-agnostic teacher who ignores any constraints (AGNOSTIC) and the bi-
level learner-aware teacher (AWARE-BIL)  respectively. We observe that AGNOSTIC fails to teach the
learner about objects‚Äô positive rewards in cases where the learners‚Äô preferences conÔ¨Çict with the posi-
tion of the most rewarding objects (second row). In contrast  AWARE-BIL always successfully teaches
the learners about rewarding objects that are compatible with the learners‚Äô preferences (third row).
We also compare AGNOSTIC and AWARE-BIL in terms of reward achieved by the learner after
teaching for object worlds of size 10 √ó 10 in Table 1. The numbers show the average reward over 10
randomly generated object-worlds. Note that AWARE-BIL has to solve a non-convex optimization
problem to Ô¨Ånd the optimal teaching policy  cf. Eq. 5. Because we use a gradient-based optimization

7

approach  the teaching policies found can depend on the initial point for optimization. Hence  we
always consider the following two initial points for optimization and select the teaching policy which
results in a higher objective value: (i) all optimization variables in Eq. 5 are set to zero  and (ii) the
optimization variables are initialized as Œ±low[i] = max{wŒª[i]  0}  Œ±up[i] = max{‚àíwŒª[i]  0}  and
Œ≤ = 0  where wŒª is as inferred by the learner when taught by AGNOSTIC and i ‚àà {1  . . .   dr}  cf.
Section 3.2. From Table 1 we observe that a learner can learn better policies from a teacher that
accounts for the learner‚Äôs preferences.

Table 1: Learners‚Äô average rewards after teaching. L1  . . .  L5 correspond to learners with preferences
as shown in Figure 2. Results are averaged over 10 random object-worlds  ¬± standard error

L1

AGNOSTIC 7.99 ¬± 0.02
AWARE-BIL 8.00 ¬± 0.02

Teacher

L2

0.01 ¬± 0.00
7.20 ¬± 0.01

Learner (Cr = 5  Cc = 10)
L4

L3

0.01 ¬± 0.00
4.86 ¬± 0.30

0.01 ¬± 0.00
3.15 ¬± 0.27

L5

0.00 ¬± 0.00
1.30 ¬± 0.07

6.2 Teaching under unknown constraints

c

In this section we evaluate the teaching algorithms from Section 5. We consider the learner model
from Section 3.1 that uses L2-projection to match reward feature expectations as studied in Section 5 
cf. Eq. 2.2 For modeling the hard constraints  we consider box-type linear constraints with Œ¥hard
[j] =
2.5 ‚àÄj ‚àà {1  2  . . .   dc} for the preference features  cf. Eq. 3.
We study the learners L1  L2  and L3 with preferences corresponding to the Ô¨Årst three object-worlds
shown in Figure 2a. We report the results for learner L2 below; results for learners L1 and L3 are
deferred to the Appendix B.2 of the supplementary material.
In this context it is instructive to investigate how quickly these adaptive teaching strategies converge
to the performance of a teacher who has full knowledge about the learner. Results comparing the
adaptive teaching strategies (ADAWARE-VOL and ADAWARE-LIN) are shown in Figure 3a. We can
observe that both teaching strategies get close to the best possible performance under full knowledge
about the learner (AWARE-CMDP). We also provide results showing the performance achieved by
the adaptive teaching strategies on object-worlds of varying sizes  see Figure 3b.
Note that the performance of ADAWARE-VOL decreases slightly when teaching for more rounds 
i.e.  comparing the results after 3 teaching rounds and at the end of the teaching process. This is
because of approximations when learner is computing the policy via projection  which in turn leads
to errors on the teacher side when approximating ÀÜ‚Ñ¶L
r (refer to discussion in Footnote 2). In contrast 
ADAWARE-LIN performance always increases when teaching for more rounds.

7 Related Work

Our work is closely related to algorithmic machine teaching [Goldman and Kearns  1995  Zhu  2015 
Zhu et al.  2018]  whose general goal is to design teaching algorithms that optimize the data that is
provided to a learning algorithm. Most works in machine teaching so far focus on supervised learning
tasks and assume that the learning algorithm is fully known to the teacher  see e.g.  [Zhu  2013 
Singla et al.  2014  Liu and Zhu  2016  Mac Aodha et al.  2018].
In the IRL setting  few works study how to provide maximally informative demonstrations
to the learner  e.g.  [Cakmak and Lopes  2012  Brown and Niekum  2019].
In contrast to our
work  their teacher fully knows the learner model and provides the demonstrations without any
adaptation to the learner. The question of how a teacher should adaptively react to a learner
has been addressed by [Singla et al.  2013  Liu et al.  2018  Chen et al.  2018  Melo et al.  2018 
Yeo et al.  2019  Hunziker et al.  2019]  but only in the supervised setting.
In a recent work 
[Kamalaruban et al.  2019] have studied the problem of adaptively teaching an IRL agent by pro-

2To implement the learner in Eq. 2  we approximated the learner‚Äôs projection onto the set ‚Ñ¶L

r as follows: We
implemented the learner based on the optimization problem given in Eq. 3 with a hard constraint on preferences
and L2 norm penalty on reward mismatch scaled with a large value of Cr = 20.

8

XXXXXXXXX

Teacher
AWARE-CMDP

Env

AGNOSTIC
CONSERV

ADAWARE-VOL (3rd)
ADAWARE-VOL (end)
ADAWARE-LIN (3rd)
ADAWARE-LIN (end)

10 √ó 10
7.62 ¬± 0.02
3.94 ¬± 0.09
1.68 ¬± 0.01
7.50 ¬± 0.14
6.85 ¬± 0.33
6.14 ¬± 0.08
7.64 ¬± 0.02

15 √ó 15
7.44 ¬± 0.04
3.84 ¬± 0.06
1.67 ¬± 0.012
7.50 ¬± 0.04
7.06 ¬± 0.06
6.28 ¬± 0.10
7.53 ¬± 0.03

20 √ó 20
7.19 ¬± 0.04
3.95 ¬± 0.06
1.62 ¬± 0.02
7.29 ¬± 0.05
6.77 ¬± 0.08
6.37 ¬± 0.08
7.29 ¬± 0.06

(b) Varying grid-size

(a) Reward over teaching rounds
Figure 3: Performance of adaptive teaching strategies ADAWARE-VOL and ADAWARE-LIN. (left)
Figure 3a shows the reward for learner‚Äôs policy over number of teaching interactions. The horizontal
lines indicate the performance of learner‚Äôs policy for the learner-aware teacher with full knowledge
of the learner‚Äôs constraints AWARE-CMDP  the learner-agnostic teacher AGNOSTIC who ignores
any constraints  and a conservative teacher CONSERV who considers all 6 constraints (assuming the
learner model L5 in Figure 2). Our adaptive teaching strategies ADAWARE-VOL and ADAWARE-LIN
signiÔ¨Åcantly outperform baselines (AGNOSTIC and CONSERV) and quickly converge towards the
optimal performance of AWARE-CMDP. The dotted lines ADAWARE-VOL:T and ADAWARE-LIN:T
show the rewards corresponding to teacher‚Äôs policy at a round and are shown to highlight the very
different behavior of two adaptive teaching strategies. (right) Table 3b shows results for varying
grid-size of the environment. Results are reported at i = 3rd round and at the ‚Äúend" round when
algorithm reaches it‚Äôs stopping criterion. Results are reported as average over 10 runs ¬± standard
error  where each run corresponds to a random environment.

viding an informative sequence of demonstrations. However  they assume that the teacher has full
knowlege of the learner‚Äôs dynamics.
Within the area of IRL  there is a line of work on active learning approaches [Cohn et al.  2011 
Brown et al.  2018  Brown and Niekum  2018  Amin et al.  2017  Cui and Niekum  2018]  which is
related to our work. In contrast to us  they take the perspective of the learner who actively inÔ¨Çuences
the demonstrations it receives. A few papers have addressed the problem that arises when the learner
does not have full access to the reward features  e.g.  [Levine et al.  2010] and [Haug et al.  2018].
Our work is also loosely related to multi-agent reinforcement learning. [Dimitrakakis et al.  2017]
studied the interaction between agents with misaligned models with a focus on the question of how to
jointly optimize a policy. [Ghosh et al.  2019] studied the problem of designing robust AI agent that
can interact with another agent of unknown type. However  these works do not tackle the problem of
teaching an agent by demonstrations. Another related work is [HadÔ¨Åeld-Menell et al.  2016] which
studied the cooperation of agents who do not perfectly understand each other.
8 Conclusions and Outlook
In this paper we considered inverse reinforcement learning in the context of learners with preferences
and constraints. In this setting  the learner does not only focus on matching the teacher‚Äôs demonstrated
behavior but also takes its own preferences  e.g.  behavioral biases or physical constraints  into
account. We developed a theoretical framework for this setting  and proposed and studied algorithms
for learner-aware teaching in which the teacher accounts for the learner‚Äôs preferences for the cases of
known and unknown preference constraints. We demonstrated signiÔ¨Åcant performance improvements
of our learner-aware teaching strategies as compared to learner-agnostic teaching both theoretically
and empirically. Our theoretical framework and our proposed algorithms foster the application of
IRL in real-world settings in which the learner does not blindly follow a teacher‚Äôs demonstrations.
There are several promising directions for future work  including but not limited to: The evaluation
of our approach in machine-human and human-machine tasks; extensions of our approach to other
learner models; approaches for learning efÔ¨Åciently from a learner‚Äôs point of view from a Ô¨Åxed set of
(potentially suboptimal) demonstrations in the case of preference constraints.

9

012345678910Adaptiveteaching:Roundi13579Learner‚ÄôsrewardAwAgCoAdA-VolAdA-LinAdA-Vol:TAdA-Lin:TAcknowledgements

This work was supported by Microsoft Research through its PhD Scholarship Programme.

References

[Abbeel and Ng  2004] Abbeel  P. and Ng  A. Y. (2004). Apprenticeship learning via inverse rein-

forcement learning. In ICML.

[Altman  1999] Altman  E. (1999). Constrained Markov decision processes  volume 7. CRC Press.
[Amin et al.  2017] Amin  K.  Jiang  N.  and Singh  S. P. (2017). Repeated inverse reinforcement

learning. In NIPS  pages 1813‚Äì1822.

[Boularias et al.  2011] Boularias  A.  Kober  J.  and Peters  J. (2011). Relative entropy inverse

reinforcement learning. In AISTATS  pages 182‚Äì189.

[Boyd and Vandenberghe  2004] Boyd  S. and Vandenberghe  L. (2004). Convex optimization. Cam-

bridge university press.

[Brown et al.  2018] Brown  D. S.  Cui  Y.  and Niekum  S. (2018). Risk-Aware Active Inverse

Reinforcement Learning. In Conference on Robot Learning  pages 362‚Äì372.

[Brown and Niekum  2018] Brown  D. S. and Niekum  S. (2018). EfÔ¨Åcient probabilistic performance
In Thirty-Second AAAI Conference on ArtiÔ¨Åcial

bounds for inverse reinforcement learning.
Intelligence.

[Brown and Niekum  2019] Brown  D. S. and Niekum  S. (2019). Machine teaching for inverse

reinforcement learning: Algorithms and applications. In AAAI.

[Cakmak and Lopes  2012] Cakmak  M. and Lopes  M. (2012). Algorithmic and human teaching of

sequential decision tasks. In AAAI.

[Chen et al.  2018] Chen  Y.  Singla  A.  Mac Aodha  O.  Perona  P.  and Yue  Y. (2018). Understand-
ing the role of adaptivity in machine teaching: The case of version space learners. In Advances in
Neural Information Processing Systems  pages 1476‚Äì1486.

[Cohn et al.  2011] Cohn  R.  Durfee  E.  and Singh  S. (2011). Comparing Action-query Strategies

in Semi-autonomous Agents. In AAMAS  pages 1287‚Äì1288  Richland  SC.

[Cui and Niekum  2018] Cui  Y. and Niekum  S. (2018). Active reward learning from critiques. In
2018 IEEE International Conference on Robotics and Automation (ICRA)  pages 6907‚Äì6914.
IEEE.

[De  1960] De  G. G. (1960). Les problemes de decisions sequentielles. cahiers du centre d‚Äôetudes

de recherche operationnelle vol. 2  pp. 161-179.

[Dimitrakakis et al.  2017] Dimitrakakis  C.  Parkes  D. C.  Radanovic  G.  and Tylkin  P. (2017).
Multi-view decision processes: the helper-ai problem. In Advances in Neural Information Pro-
cessing Systems  pages 5443‚Äì5452.

[Ghosh et al.  2019] Ghosh  A.  Tschiatschek  S.  Mahdavi  H.  and Singla  A. (2019). Towards
deployment of robust AI agents for human-machine partnerships. In Workshop on Safety and
Robustness in Decision Making (SRDM) at NeurIPS‚Äô19.

[Goldman and Kearns  1995] Goldman  S. A. and Kearns  M. J. (1995). On the complexity of

teaching. Journal of Computer and System Sciences  50(1):20‚Äì31.

[HadÔ¨Åeld-Menell et al.  2016] HadÔ¨Åeld-Menell  D.  Russell  S. J.  Abbeel  P.  and Dragan  A. (2016).

Cooperative inverse reinforcement learning. In NIPS.

[Haug et al.  2018] Haug  L.  Tschiatschek  S.  and Singla  A. (2018). Teaching Inverse Reinforce-
ment Learners via Features and Demonstrations. In Advances in Neural Information Processing
Systems  pages 8473‚Äì8482.

[Hunziker et al.  2019] Hunziker  A.  Chen  Y.  Mac Aodha  O.  Rodriguez  M. G.  Krause  A. 
Perona  P.  Yue  Y.  and Singla  A. (2019). Teaching multiple concepts to a forgetful learner. In
Advances in Neural Information Processing Systems.

[Jaggi  2013] Jaggi  M. (2013). Revisiting Frank-Wolfe: Projection-free sparse convex optimization.

In Proceedings of the 30th International Conference on Machine Learning  pages 427‚Äì435.

10

[Kamalaruban et al.  2019] Kamalaruban  P.  Devidze  R.  Cevher  V.  and Singla  A. (2019). Inter-

active teaching algorithms for inverse reinforcement learning. In IJCAI  pages 2692‚Äì2700.

[Leibo et al.  2017] Leibo  J. Z.  Zambaldi  V.  Lanctot  M.  Marecki  J.  and Graepel  T. (2017).
Multi-agent reinforcement learning in sequential social dilemmas. In Proceedings of the 16th
Conference on Autonomous Agents and MultiAgent Systems  pages 464‚Äì473.

[Levine et al.  2010] Levine  S.  Popovic  Z.  and Koltun  V. (2010). Feature construction for inverse

reinforcement learning. In NIPS  pages 1342‚Äì1350.

[Liu and Zhu  2016] Liu  J. and Zhu  X. (2016). The teaching dimension of linear learners. Journal

of Machine Learning Research  17(162):1‚Äì25.

[Liu et al.  2018] Liu  W.  Dai  B.  li  X.  Rehg  J. M.  and Song  L. (2018). Towards black-box

iterative machine teaching. In ICML.

[Mac Aodha et al.  2018] Mac Aodha  O.  Su  S.  Chen  Y.  Perona  P.  and Yue  Y. (2018). Teaching
categories to human learners with visual explanations. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 3820‚Äì3828.

[Melo et al.  2018] Melo  F. S.  Guerra  C.  and Lopes  M. (2018). Interactive optimal teaching with

unknown learners. In IJCAI  pages 2567‚Äì2573.

[Mendez et al.  2018] Mendez  J. A. M.  Shivkumar  S.  and Eaton  E. (2018). Lifelong inverse
reinforcement learning. In Advances in Neural Information Processing Systems  pages 4507‚Äì4518.
[Osa et al.  2018] Osa  T.  Pajarinen  J.  Neumann  G.  Bagnell  J. A.  Abbeel  P.  Peters  J.  et al.
(2018). An algorithmic perspective on imitation learning. Foundations and Trends¬Æ in Robotics 
7(1-2):1‚Äì179.

[Ratliff et al.  2006] Ratliff  N. D.  Bagnell  J. A.  and Zinkevich  M. A. (2006). Maximum margin

planning. In ICML  pages 729‚Äì736.

[Singla et al.  2013] Singla  A.  Bogunovic  I.  Bart√≥k  G.  Karbasi  A.  and Krause  A. (2013). On

actively teaching the crowd to classify. In NIPS Workshop on Data Driven Education.

[Singla et al.  2014] Singla  A.  Bogunovic  I.  Bart√≥k  G.  Karbasi  A.  and Krause  A. (2014).

Near-optimally teaching the crowd to classify. In ICML.

[Sinha et al.  2018] Sinha  A.  Malo  P.  and Deb  K. (2018). A review on bilevel optimization:
from classical to evolutionary approaches and applications. IEEE Transactions on Evolutionary
Computation  22(2):276‚Äì295.

[Yeo et al.  2019] Yeo  T.  Kamalaruban  P.  Singla  A.  Merchant  A.  Asselborn  T.  Faucon  L. 
Dillenbourg  P.  and Cevher  V. (2019). Iterative classroom teaching. In AAAI  pages 5684‚Äì5692.
[Zhou et al.  2018] Zhou  Z.  Bloem  M.  and Bambos  N. (2018). InÔ¨Ånite time horizon maximum
causal entropy inverse reinforcement learning. IEEE Trans. Automat. Contr.  63(9):2787‚Äì2802.
[Zhu  2013] Zhu  X. (2013). Machine teaching for bayesian learners in the exponential family. In

NIPS  pages 1905‚Äì1913.

[Zhu  2015] Zhu  X. (2015). Machine teaching: An inverse problem to machine learning and an

approach toward optimal education. In AAAI  pages 4083‚Äì4087.

[Zhu et al.  2018] Zhu  X.  Singla  A.  Zilles  S.  and Rafferty  A. N. (2018). An Overview of

Machine Teaching. arXiv:1801.05927.

[Ziebart  2010] Ziebart  B. D. (2010). Modeling purposeful adaptive behavior with the principle of

maximum causal entropy. Carnegie Mellon University.

[Ziebart et al.  2013] Ziebart  B. D.  Bagnell  J. A.  and Dey  A. K. (2013). The principle of maximum
causal entropy for estimating interacting processes. IEEE Transactions on Information Theory 
59(4):1966‚Äì1980.

[Ziebart et al.  2008] Ziebart  B. D.  Maas  A. L.  Bagnell  J. A.  and Dey  A. K. (2008). Maximum

entropy inverse reinforcement learning. In AAAI.

11

,Kurtland Chua
Roberto Calandra
Rowan McAllister
Sergey Levine
Sebastian Tschiatschek
Ahana Ghosh
Luis Haug
Rati Devidze
Adish Singla