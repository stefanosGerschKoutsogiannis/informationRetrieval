2018,Learning long-range spatial dependencies with horizontal gated recurrent units,Progress in deep learning has spawned great successes in many engineering applications. As a prime example  convolutional neural networks  a type of feedforward neural networks  are now approaching -- and sometimes even surpassing -- human accuracy on a variety of visual recognition tasks. Here  however  we show that these neural networks and their recent extensions struggle in recognition tasks where co-dependent visual features must be detected over long spatial ranges. We introduce a visual challenge  Pathfinder  and describe a novel recurrent neural network architecture called the horizontal gated recurrent unit (hGRU) to learn intrinsic horizontal connections -- both within and across feature columns. We demonstrate that a single hGRU layer matches or outperforms all tested feedforward hierarchical baselines including state-of-the-art architectures with orders of magnitude more parameters.,Learning long-range spatial dependencies

with horizontal gated recurrent units

Drew Linsley  Junkyung Kim  Vijay Veerabadran  Charlie Windolf  Thomas Serre

Carney Institute for Brain Science

Department of Cognitive Linguistic & Psychological Sciences

{drew_linsley junkyung_kim vijay_veerabadran thomas_serre}@brown.edu

Brown University

Providence  RI 02912

Abstract

Progress in deep learning has spawned great successes in many engineering appli-
cations. As a prime example  convolutional neural networks  a type of feedforward
neural networks  are now approaching – and sometimes even surpassing – human
accuracy on a variety of visual recognition tasks. Here  however  we show that
these neural networks and their recent extensions struggle in recognition tasks
where co-dependent visual features must be detected over long spatial ranges. We
introduce a visual challenge  Pathﬁnder  and describe a novel recurrent neural
network architecture called the horizontal gated recurrent unit (hGRU) to learn
intrinsic horizontal connections – both within and across feature columns. We
demonstrate that a single hGRU layer matches or outperforms all tested feedfor-
ward hierarchical baselines including state-of-the-art architectures with orders of
magnitude more parameters.

1

Introduction

Consider Fig. 1a which shows a sample image from a representative segmentation dataset [1]
(left) and the corresponding contour map produced by a state-of-the-art deep convolutional neural
network (CNN) [2] (right). Although this task has long been considered challenging because of the
need to integrate global contextual information with inherently ambiguous local edge information 
modern CNNs are capable to detect contours in natural scenes at a level that rivals that of human
observers [2–6]. Now  consider Fig. 1b which depicts a variant of a visual psychology task referred
to as “Pathﬁnder” [7]. Reminiscent of the everyday task of reading a subway map to plan a commute
(Fig. 1c)  the goal in Pathﬁnder is to determine if two white circles in an image are connected by a
path. These images are visually simple compared to natural images like the one shown in Fig. 1a 
and the task is indeed easy for human observers [7]. Nonetheless  we will demonstrate that modern
CNNs struggle to solve this task.
Why is it that a CNN can accurately detect contours in a natural scene like Fig. 1a but also struggle
to integrate paths in the stimuli shown in Fig. 1b? In principle  the ability of CNNs to learn such
long-range spatial dependencies is limited by their localized receptive ﬁelds (RFs) – hence the need
to consider deeper networks because they allow the buildup of larger and more complex RFs. Here 
we use a large-scale analysis of CNN performance on the Pathﬁnder challenge to demonstrate that
simply increasing depth in feedforward networks constitutes an inefﬁcient solution to learning the
long-range spatial dependencies needed to solve the Pathﬁnder challenge.
An alternative solution to problems that stress long-range spatial dependencies is provided by biology.
The visual cortex contains abundant horizontal connections which mediate non-linear interactions
between neurons across distal regions of the visual ﬁeld [8  9]. These intrinsic connections  popularly

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: State-of-the-art CNNs excel at detecting contours in natural scenes  but they are strained
by a task that requires the detection of long-range spatial dependencies. (a) Representative contour
detection performance of a leading neural network architecture [23].
(b) Exemplars from the
Pathﬁnder challenge: a task consisting of synthetic images which are parametrically controlled for
long-range dependencies. (c) Long-range dependencies similar to those in the Pathﬁnder challenge
are critical for everyday behaviors  such as reading a subway map to navigate a city.

called “association ﬁelds”  are thought to form the main substrate for mechanisms of contour grouping
according to Gestalt principles  by mutually exciting colinear elements while also suppressing
clutter elements that do not form extended contours [10–15]. Such “extra-classical receptive ﬁeld”
mechanisms  mediated by horizontal connections  allow receptive ﬁelds to adaptively “grow” without
additional processing depth. Building on previous computational neuroscience work [e.g.  10  16–19] 
our group has recently developed a recurrent network model of classical and extra-classical receptive
ﬁelds that is constrained by the anatomy and physiology of the visual cortex [20]. The model was
shown to account for diverse visual illusions providing computational evidence for a novel canonical
circuit that is shared across visual modalities.
Here  we show how this computational neuroscience model can be turned into a modern end-to-
end trainable neural network module. We describe an extension of the popular gated recurrent unit
(GRU) [21]  which we call the horizontal GRU (hGRU). Unlike CNNs  which exhibit a sharp decrease
in accuracy for increasingly long paths  we show that the hGRU is highly effective at solving the
Pathﬁnder challenge with just one layer and a fraction of the number of parameters and training
samples needed by CNNs. We further ﬁnd that  when trained on natural scenes  the hGRU learns
connection patterns that coarsely resemble anatomical patterns of horizontal connectivity found in
the visual cortex  and exhibits a detection proﬁle that strongly correlates with human behavior on a
classic contour detection task [22].

Related work Much previous work on recurrent neural networks (RNNs) has focused on modeling
sequences with learnable gates in the form of long-short term memory (LSTM) units [24] or gated
recurrent units (GRUs) [21]. RNNs have also been extended to learning spatial dependencies
in static images with broad applications [25–29]. In this approach  images are transformed into
one-dimensional sequences that are used to train an RNN. In recent years  several approaches
have introduced convolutions into RNNs  using the recursive application of convolutional ﬁlters
as a method for increasing the depth of processing through time on tasks like object recognition
and super-resolution without additional parameters [30–32]. Other groups have constrained these
convolutional-RNNs with insights from neuroscience and cognitive science  engineering speciﬁc
patterns of connectivity between processing layers [33–36]. The proposed hGRU builds on this line of
biologically-inspired implementations of RNNs  adding connectivity patterns and circuit mechanisms
that are typically found in computational neuroscience models of neural circuits [e.g.  10  16–20].
Another class of models related to our proposed approach is Conditional Random Fields (CRFs) 
probabilistic models aimed at explicitly capturing associations between nearby features. The con-
nectivity implemented in CRFs is similar to the horizontal connections used in the hGRU  and has
been successfully applied as a post-processing stage in visual tasks such as segmentation [37  38]
to smooth out and increase the spatial resolution of prediction maps. Recently  such probabilistic
methods have been successfully incorporated in a generative vision model shown to break text-based
CAPTCHAs [39]. Originally formulated as probabilistic models  CRFs can also be cast as RNNs [40].

2

(a)(b)STARTGOAL(c)2 Horizontal gated recurrent units (hGRUs)

Original contextual neural circuit model We begin by referencing the recurrent neural model of
contextual interactions developed by Mély et al. [20]. Below we adapted the model notations to a
computer vision audience. Model units are indexed by their 2D positions (x  y) and feature channel
k. Neural activity is governed by the following differential equations (see Supp. Material for the full
treatment):

η ˙H (1)

xyk + 2H (1)

xyk =

τ ˙H (2)

xyk + σ2H (2)

xyk =

(cid:104)
(cid:104)

(cid:105)

γC (2)
xyk

.

+

(cid:105)

ξXxyk − (αH (1)

xyk + µ) C (1)

xyk

+

(1)

where

xyk = (WI ∗ H(2))xyk
C (1)
xyk = (WE ∗ H(1))xyk 
C (2)

Here  X ∈ RW×H×K is the feedforward drive (i.e.  neural responses to a stimulus)  H(1) ∈ RW×H×K
is the recurrent circuit input  and H(2) ∈ RW×H×K the recurrent circuit output. Modeling input and
output states separately allows for the implementation of a particular form of inhibition known as
“shunting” (or divisive) inhibition. Unlike the excitation in the model which acts linearly on a unit’s
input  inhibition acts on a unit’s output and hence  regulates the unit response non-linearly (i.e.  given
a ﬁxed amount of inhibition and excitation  inhibition will increase with the unit’s activity unlike
excitation which will remained constant).
The convolutional kernels WI   WE ∈ RS×S×K×K describe inhibitory vs. excitatory hypercolumn
connectivity (constrained by anatomical data1). The scalar parameters µ and α control linear and
quadratic (i.e.  shunting) inhibition by C(1) ∈ RW×H×K  γ scales excitation by C(2) ∈ RW×H×K  and
ξ scales the feedforward drive. Activity at each stage is linearly rectiﬁed (ReLU) [·]+ = max(·  0).
Finally  η    τ and σ are time constants. To make this model amenable to modern computer vision
applications  we set out to develop a version where all parameters could be trained from data. If we
let η = τ and σ =  for symmetry and apply Euler’s method to Eq. 1 with a time step of ∆t = η/2 
then we obtain the discrete-time equations:

xyk[t] = −2(cid:104)
xyk[t] = −2(cid:104)

H (1)

H (2)

ξXxyk − (αH (1)

xyk[t − 1] + µ)C (1)

xyk[t]

(cid:105)

(cid:105)

+

γC (2)

.

+

xyk[t]

(2)
Here  ·[t] denotes the approximation at the t-th discrete timestep. This results in a trainable convo-
lutional recurrent neural network (RNN) which performs Euler integration of a dynamical system
similar to the neural model of [20].
hGRU formulation We build on Eq. 2 to introduce the hGRU – a model with the ability to learn
complex interactions between units via horizontal connections within a single processing layer
(Fig. 2). The hGRU extends the derivation from Eq. 2 with three modiﬁcations that improve the
training of the model with gradient descent and its expressiveness2. (i) We introduce learnable gates 
borrowed from the gated recurrent unit (GRU) framework (see Supp. Material for the full derivation
from Eq. 2). (ii) The hGRU makes the operations for computing H(2) (excitation) symmetric with
those of H(1) (inhibition)  providing the circuit the ability to learn how to implement linear and
quadratic interactions at each of these processing stages. (iii) To control unstable gradients  the hGRU
uses a squashing pointwise non-linearity and a learned parameter to globally scale activity at every
processing timestep (akin to a constrained version of the recurrent batchnorm [41]).

1There are four separate connectivity patterns in [20] to describe inhibition vs. excitation and near vs. far
interactions between units. We combine these into a separate inhibitory vs. excitatory kernels to simplify
notation.

2These modiﬁcations involved relaxing several constraints from the original neuroscience model that are less
useful for solving the tasks investigated here (see Supp. Material for performance of an hGRU with constrained
inhibition and excitation.)

3

Figure 2: The hGRU circuit. The hGRU can learn highly non-linear interactions between spatially
neighboring units in the feedforward drive X  which are encoded in its hidden state H(2). This
computation involves two stages  which are inspired by a recurrent neural circuit of horizontal
connections [20]. First  the horizontal inhibition (blue) is calculated by applying a gain to H(2)[t− 1] 
and convolving the resulting activity with the kernel W which characterizes these interactions. Linear
(+ symbol) and quadratic (× symbol) operations control the convergence of this inhibition onto
X. Second  the horizontal excitation (red) is computed by convolving H(1)[t] with W . Another
set of linear and quadratic operations modulate this activity  before it is mixed with the persistent
hidden state H(2)[t − 1]. Note that the excitation computation involves an additional “peephole”
connection  not depicted here. Small solid-line squares within the hypothetical activities that the
circuit operates on denote the unit indexed by 2D position (x  y) and feature channel k  whereas
dotted-line squares depict the unit’s receptive ﬁeld (a union of both classical and extra-classical
deﬁnitions) in the previous activity.

In our hGRU implementation  the feedforward drive X corresponds to activity from a preceding
convolutional layer. The hGRU encodes spatial dependencies between feedforward units via its
(time-varying) hidden states H(1) and H(2). Updates to the hidden states are managed using two
activities  referred to as the reset and update “gates”: G(1) and G(2). These activities are derived from
convolutions  denoted by ∗  between the kernels U(1)  U(2) ∈ R1×1×K×K and hidden states H(1) and
H(2)  shifted by biases b(1)  b(2) ∈ R1×1×K  respectively. The pointwise non-linearity σ is applied
to each activity  normalizing them in the range [0  1]. Because these activities are real-valued  we
hereafter refer to the reset gate as the “gain”  and the update gate as the “mix”.
Horizontal interactions between units are calculated by the kernel W ∈ RS×S×K×K  where S describes
the spatial extent of these connections in a single timestep (Fig. 2; but see Supp. Material for a version
with separate kernels for excitation vs. inhibition  as in Eq. 2). Consistent with computational models
of neural circuits (e.g.  [10  16–20])  W is constrained to have symmetric weights between channels 
such that the weight Wx0+∆x y0+∆y k1 k2 is equal to the weight Wx0+∆x y0+∆y k2 k1 where x0 and
y0 denote the center of the kernel. This constraint reduces the number of learnable parameters by
nearly half vs. a normal convolutional kernel. Hidden states H(1) and H(2) are recomputed via
horizontal interactions at every timestep t ∈ [0  T ]. We begin by describing computation of H(1)[t]:
(3)
(4)

G(1)[t] = σ(U(1) ∗ H(2)[t − 1] + b(1))
xyk[t] = (W ∗ (G(1)[t] (cid:12) H(2)[t − 1]))xyk
C (1)
xyk[t] = ζ(Xxyk − C (1)
H (1)

(5)

xyk[t](αkH (2)

xyk[t − 1] + µk))

4

Wσ++InhibitionExcitation++σMixGainζζU(1)U(2)W(x  y  k)(x  y  k)H(2)[t-1]H(2)[t]H(1)[t]XChannels in H(2)[t − 1] are ﬁrst modulated by the gain3 G(1)[t]. The resulting activity is convolved
with W to compute C(1)[t]  which is the horizontal inhibition of the hGRU at this timestep. This inhi-
bition is applied to X via the parameters µ and α  which are k-dimensional vectors that respectively
scale linear and quadratic (akin to shunting inhibition described in Eq. 1) terms of the horizontal
interaction with X. The pointwise ζ is a hyperbolic tangent that squashes activity into the range
[−1  1] (but see Supp. Material for a hGRU with a rectiﬁed linearity). Importantly  in contrast to the
original circuit  in this formulation the update to H(1)[t] (Eq. 5) is calculated by combining horizontal
connection contributions of C(1)[t] with H(2)[t− 1] rather than H(1)[t− 1]  which we found improved
learning on the visual tasks explored here.
The updated H(1)[t] is next used to calculate H(2)[t].

xyk[t] = σ((U(2) ∗ H(1)[t])xyk + b(2)
G(2)
k )
xyk[t] = (W ∗ H(1)[t])xyk
C (2)
xyk[t] = ζ(κkH (1)
˜H (2)
H (2)
xyk[t] = ηt(H (2)

xyk[t] + βkC (2)
xyk[t − 1](1 − G(2)

xyk[t] + ωkH (1)
xyk[t]) + ˜H (2)

xyk[t]C (2)
xyk[t]G(2)

xyk[t])

xyk[t])

(6)

(7)

(8)

(9)

The mix G(2)[t] is calculated by convolving U(2)[t] with H(1)[t]  followed by the addition of b(2).
The activity C(2)[t] represents the excitation of horizontal connections onto the newly-computed
H(1)[t]. Linear and quadratic contributions of horizontal interactions at this stage are controlled by
the k-dimensional parameters κ  ω  and β. The parameters κ and ω control the linear and quadratic
contributions of horizontal connections to ˜H(2)
[t]. The parameter β is a gain applied to C(2)[t]  giving
W an additional degree of freedom in expressing this excitation. With this full suite of interactions 
the hGRU can in principle implement both a linear and a quadratic form of excitation (i.e.  to assess
self-similarity)  each of which play speciﬁc computational roles in perception [42]. Note that the
inclusion of H(1)[t] in Eq. 8 functions as a “peephole” connection between it and ˜H(2)
[t]. Finally  the
mix G(2) integrates the candidate ˜H(2)
. The learnable T -dimensional parameter η  which
we refer to as a time-gain  helps control unstable gradients during training. This time-gain modulates
H(2)
t with the scalar  ηt  which as we show in our experiments below improves model performance.

t with H(2)

t

3 The Pathﬁnder challenge

We evaluated the limits of feedforward and recurrent architectures on the “Pathﬁnder challenge”  a
synthetic visual task inspired by cognitive psychology [7]. The task  depicted in Fig. 1b  involves
detecting whether two circles are connected by a path. This is made more difﬁcult by allowing
target paths to curve and introducing multiple shorter unconnected “distractor” paths. The Pathﬁnder
challenge involves three separate datasets  for which the length of paths and distractors are parametri-
cally increased. This challenge therefore screens models for their effectiveness in detecting complex
long-range spatial relationships in cluttered scenes.

Stimulus design Pathﬁnder images were generated by placing oriented “paddles” on a canvas to
form dashed paths. Each image contained two paths made of a ﬁxed number of paddles and multiple
distractors made of one third as many paddles. Positive examples were generated by placing two
circles at the ends of a single path (Fig. 1b  left) and negative examples by placing one circle at
the end of each of the paths (Fig. 1b  right). The paths were curved and variably shaped  with the
possible number of shapes exponential to the path length. The Pathﬁnder challenge consisted of three
datasets  in which path and distractor length was successively increased  and with them  the overall
task difﬁculty. These datasets had path lengths of 6  9 and 14 paddles  and each contained 1 000 000
unique images of 150×150 pixels. See Supp. Material for a detailed description of the stimulus
generation procedure.

3GRU gate activities are often a function of a hidden state and X[t]. Because the feedforward drive here is

constant w.r.t. time  we omit it from these calculations. In practice  its inclusion did not affect performance.

5

Model implementation We performed a large-scale analysis of the effectiveness of feedforward
and recurrent computations on the Pathﬁnder challenge. We controlled for the effects of idiosyncratic
model speciﬁcations by using a standard architecture  consisting of “input”  “feature extraction” 
and “readout” processing stages. Swapping different feedforward or recurrent layers into the feature
extraction stage let us measure the relative effectiveness of each on the challenge. All models except
for state-of-the-art “residual networks” (ResNets) [43] and per-pixel prediction architectures were
embedded in this architecture  and these exceptions are detailed below. See Supp. Material for a
detailed description of the input and readout stage. Models were trained on each Pathﬁnder challenge
dataset (Fig. 3d)  with 90% of the images used for training (900 000) and the remainder for testing
(100 000). We measured model performance in two ways. First  as the accuracy on test images.
Second  as the “area under the learning curve” (ALC)  or mean accuracy on the test set evaluated
after every 1000 batches of training  which summarized the rate at which a model learned the task.
Accuracy and ALC were taken from the model that achieved the highest accuracy across 5 separate
runs of model training. All models were trained for two epochs except for the ResNets  which were
trained for four. Model training procedures are detailed in Supp. Material.

Recurrent models We tested 6 different recurrent layers in the feature extraction stage of the
standard architecture: hGRUs with 8  6  and 4-timesteps of processing; a GRU; and hGRUs with
lesions applied to parameters controlling linear or quadratic horizontal interactions. Both the GRU
and lesioned versions of the hGRU ran for 8 timesteps. These layers had 15×15 horizontal connection
kernels (W ) with an equal number of channels as their input layer (25 channels).
We observed 3 overarching trends: First  each model’s performance monotonically decreased  or
“strained”  as path length increased. Increasing path length reduced model accuracy (Fig. 3a)  and
increased the number of batches it took to learn a task (Fig. 3b). Second  the 8-timestep hGRU was
more effective than any other recurrent model  and it outperformed each of its lesioned variants
as well as a standard GRU. Notably  this hGRU was strained the least by the Pathﬁnder challenge
out of all tested models  with a negligible drop in accuracy as path length increased. This ﬁnding
highlights the effectiveness of the hGRU for processing long-range spatial dependencies  and how the
dynamics implemented by its linear and quadratic horizontal interactions are important. Third  hGRU
performance monotonically decreased with processing time. This revealed a minimum number of
timesteps that the hGRU needed to solve each Pathﬁnder dataset: 4 for the length-6 condition  6 for
the length-9 condition  and 8 for the length-14 condition (ﬁrst vs. second columns in Fig. 3a). Such
time-dependency in the Pathﬁnder task is consistent with the accuracy-reaction-time tradeoff found
in humans as the distance between endpoints of a curve increases [7].

Feedforward models We screened an array of feedforward models on the Pathﬁnder challenge.
Model performance revealed the importance of kernel size vs. kernel width  model depth  and
feedforward operations for incorporating additional scene context for solving Pathﬁnder. Model
construction began by embedding the feature extraction stage of the standard model with kernels
of one of three different sizes: 10×10  15×15  or 20×20. These are referred to as small  medium 
and large kernel models (Fig. 3). To control for the effect of network capacity on performance  the
number of kernels given to each model was varied so that the number of parameters in each model
conﬁguration was equal to each other and the hGRU (36  16  and 9 kernels). We also tested two
other feedforward models that featured candidate operations for incorporating contextual information
into local convolutional activities. One version used (2-pixel) dilated convolutions  which involves
applying a stride to the kernel before convolving the input [44  45]  and has been found useful for
many computer vision problems [38  46  47]. The other version applied a non-local operation to
convolutional activities [48]  which can introduce (non-recurrent) interactions between units in a
layer. These operations were incorporated into the ﬁrst feature extraction layer of the medium kernel
(15×15 ﬁlter) model described above. We also considered deeper versions of each of the above
“1-layer” models (referring to the depth of the feature extraction stage)  stacking them to build 3- and
5-layer versions. This yielded a total of 15 different feedforward models.
Without exception  the performance of each feedforward model was signiﬁcantly strained by the
Pathﬁnder challenge. The magnitude of this straining was well predicted by model depth and size  and
operations for incorporating additional contextual information made no discernible difference to the
overall pattern of results. The 1-layer models were most effective on the 6-length Pathﬁnder dataset 
but were unable to do better than chance on the remaining conditions. Increasing model capacity to 3
layers rescued the performance of all but the small kernel model on the 9-length Pathﬁnder dataset 

6

Figure 3: The hGRU efﬁciently learns long-range spatial dependencies that otherwise strain feedfor-
ward architectures. (a) Model accuracy is plotted for the three Pathﬁnder challenge datasets  which
featured paths of 6- 9- and 14-paddles. Each panel depicts the accuracy of a different model class
after training on for each pathﬁnder dataset (see Supp. Material for additional models). Only the
hGRU and state-of-the-art models for classiﬁcation (the two right-most panels) approached perfect
accuracy on each dataset. (b) Measuring the area under the learning curve (ALC) of each model
(mean accuracy) demonstrates that the rate of learning achieved by the hGRU across the Pathﬁnder
challenge is only rivaled by the U-Net architecture (far right). (c) The hGRU is signiﬁcantly more
parameter-efﬁcient than feedforward models at the Pathﬁnder challenge  with its closest competitors
needing at least 200× the number of parameters to match its performance. The x-axis shows the
number of parameters in each model versus the hGRU  as a multiple of the latter. The y-axis depicts
model accuracy on the 14-length Pathﬁnder dataset. (d) Pathﬁnder challenge exemplars of different
path lengths (all are positive examples).

but even then did little to improve performance on the 14-length dataset. Of the 5-layer models  only
the large kernel conﬁguration came close to solving the 14-length dataset. The ALC of this model 
however  demonstrates that its rate of learning was slow  especially compared to the hGRU (Fig. 3b).
The failures of these feedforward models is all the more striking when considering that each had
between 1× and 10× the number of parameters as the hGRU (Fig. 3c  compare the red and green
markers).

Residual networks We reasoned that if the performance of feedforward models on the Pathﬁnder
challenge is a function of model depth  then state-of-the-art networks for object recognition with
many times the number of layers should easily solve the challenge. We tested this possibility by

7

Path Length ResNet-50ResNet-152ResNet-18hGRUFF  large kernelsFF  dilated kernelsFF  non-local kernelsFF  medium kernelsFF  small kernelshGRU (additive lesion)hGRU (No time-gain)GRUhGRU (multiplicative lesion)AccuracyhGRU (4 timesteps)hGRU (6 timesteps)(1-layer)(3-layer)(5-layer)FCNU-NetSegNet(b)Accuracy (Curve length = 14)Free parameter multiplierFF (5-layer)FCNResNet-152ResNet-50ResNet-18FF (3-layer)FF (1-layer)hGRUhGRU (multiplicative lesion)SegNet(c)U-Net6 9 14 Area under the Validation Curve(a)Path Length 6 9 14 (d)training ResNets with 18  50  and 152 layers on the Pathﬁnder challenge. Each model was trained
“from scratch” with standard weight initialization [49]  and given additional epochs of training (4)
to learn the task because of their large number of parameters. However  even with this additional
training  only the deepest 152-layer ResNet was able to solve the challenge (Fig. 3a). Even so  the
152-layer ResNet was less efﬁcient at learning the 14-length dataset than the hGRU (Fig. 3b)  and
achieved its performance with nearly 1000× as many parameters (Fig. 3c; see Supp. Material for
additional ResNet experiments).

Per-pixel prediction models We considered the possibility that CNN architectures for per-pixel
prediction tasks  such as contour detection and segmentation  might be better suited to the Pathﬁnder
challenge than those designed for classiﬁcation. We therefore tested three representative per-pixel
prediction models: the fully-convolutional network (FCN)  the skip-connection U-Net  and the
unpooling SegNet. These models used an encoder/decoder style architecture  which was followed by
the readout processing stage of the standard architecture described above to make them suitable for
Pathﬁnder classiﬁcation. Encoders were the VGG16 [50]  and each model was trained from scratch
with Xavier initialized weights. Like the ResNets  these models were given 4 epochs of training to
accommodate their large number of parameters.
The fully-convolutional network (FCN) architecture is one of the ﬁrst successful uses of CNNs for
per-pixel prediction [3  44  51  52]. Decoders in these models use “1×1” convolutions to combine
upsampled activity maps from several layers of the encoder. We created an FCN model which applied
this procedure to the last layer of each of the 5 VGG16-convolution blocks. These activity maps were
upsampled by learnable kernels  which were initialized with weights for bilinear interpolation. In
contrast to the feedforward models discussed above  the FCN successfully learned all conditions in
the Pathﬁnder challenge (Fig. 3a  purple circle). It did so less efﬁciently than the hGRU  however 
with a lower ALC score on the 14-length dataset and 200× as many free parameters (Fig. 3b).
Another approach to per-pixel prediction uses “skip connections” to connect speciﬁc layers of a
model’s encoder to its decoder. This approach was ﬁrst described in [44] as a method for more
effectively merging coarse-layer information into a model’s decoder  and later extended to the U-
Net [53]. We implemented a version of the U-Net architecture that had a VGG16 encoder and a
decoder. The decoder consisted of 5 randomly initialized and learned upsampling layers  which had
additive connections to the ﬁnal convolutional layer in each of the encoder’s VGG16 blocks. Using
standard VGG16 nomenclature to deﬁne one of these connections  this meant that “conv 4_3” activity
from the encoder was added to the second upsampled activity map in the decoder. The U-Net was on
par with the hGRU and the FCN at solving the Pathﬁnder challenge. It was also nearly as efﬁcient as
the hGRU in doing so (Fig. 3b)  but used over 350× as many parameters as the hGRU (Fig. 3c; see
Supp. Materials for additional U-Net experiments).
Unpooling models eliminate the need for feature map upsampling by routing decoded activities
to the locations of the winning max-pooling units derived from the encoder. Unpooling is also a
leading approach for a variety of dense per-pixel prediction tasks  including segmentation  which
is exempliﬁed by SegNet [54]. We tested a SegNet on the Pathﬁnder challenge. This model has
a decoder that mirrors its encoder  with unpooling operations replacing its pooling. The SegNet
achieved high accuracy on each of the Pathﬁnder datasets  but was less efﬁcient at learning them
than the hGRU  with worse ALC scores across the challenge (Fig. 3b). The SegNet also featured the
second-most parameters of any model tested  which was 400× more than the hGRU.

4 Explaining biological horizontal connections with the hGRU

Statistical image analysis studies have suggested that cortical patterns of horizontal connections 
commonly referred to as “association ﬁelds”  may reﬂect the geometric regularities of oriented
elements present in natural scenes [55]. Because the hGRU is designed to capture such spatial
regularities  we investigated whether it learned patterns of horizontal connections that resemble these
association ﬁelds. We visualized the horizontal kernels learned by the hGRU to solve tasks (Fig. S5).
When trained on the the Pathﬁnder challenge  hGRU kernels resembled the dominant patterns of
horizontal connectivity in visual cortex. Prominent among these patterns are (1) the antagonistic
near-excitatory vs. far-inhibitory surround organization also found in the visual cortex [56]; (2) the
association ﬁeld  with collinear excitation and orthogonal inhibition [8  9]; and (3) other higher-order
surround computations [57]. We also visualized these patterns after training the hGRU to detect

8

contours in the naturalistic BSDS500 image dataset [1]. These horizontal kernels took on similar
patterns of connectivity  but with far more deﬁnition and regularity  suggesting that the hGRU learns
best from natural scene statistics.
How well does the hGRU explain human psychophysics data? We tested this by recreating the
synthetic contour detection dataset used in [22]. This task had human participants detect a contour
formed by co-linearly aligned paddles in an array of randomly oriented distractors. Multiple versions
of the task were created by varying the distance between paddles in the contour (5 conditions).
Contour detection accuracy of the hGRU was recorded on each of dataset for comparison with
participants in [22]  whose responses were digitally extracted with WebPlotDigitizer from [22]
and averaged (N=2). Plotting hGRU accuracy against the reported “detection score” revealed that
increasing inter-paddle distance caused similar performance straining for both (Fig. S6).

5 Discussion
The present study demonstrates that long-range spatial dependencies generally strain CNNs  with
only very deep and state-of-the-art networks overcoming the visual variability introduced by long
paths in the Pathﬁnder challenge. Although feedforward networks are generally effective at learning
and detecting relatively rigid objects shown in well-deﬁned poses  these models tend towards a
brute-force solution when tasked with the recognition of less constrained structures  such as a path
connecting two distant locations. This study adds to a body of work highlighting examples of routine
visual tasks where CNNs fall short of human performance [58–62].
We demonstrate a solution to the Pathﬁnder challenge inspired by neuroscience. The hGRU leverages
computational principles of visual cortical circuits to learn complex spatial interactions between
units. For the Pathﬁnder challenge  this translates into an ability to represent the elements forming an
extended path while ignoring surrounding clutter. We ﬁnd that the hGRU can reliably detect paths of
any tested length or form using just a single layer. This contrasts sharply with the successful state-of-
the-art feedforward alternatives  which used much deeper architectures and orders of magnitude more
parameters to achieve similar success. The key mechanisms underlying the hGRU’s performance
are well known in computational neuroscience [10  16–20]. However  these mechanisms have been
typically overlooked in computer vision (but see [39] for a successful vision model using horizontal
connections and shown to break text-based CAPTCHAs).
We also found that hGRU performance on the Pathﬁnder challenge is a function of the amount of
time it was given for processing. This ﬁnding suggests that it concurrently expands the facilitative
inﬂuence of one end of a target curve to the other while suppressing the inﬂuence of distractors. The
performance of the hGRU on the Pathﬁnder challenge captures the iterative nature of computations
used by our visual system during similar tasks [63] – exhibiting a comparable tradeoff between
performance and processing-time [7]. Visual cortex is replete with association ﬁelds that are thought
to underlie perceptual grouping [11  64]. Theoretical models suggest that patterns of horizontal
connections reﬂect the statistics of natural scenes  and here too we ﬁnd that horizontal kernels in the
hGRU learned from natural scenes resemble cortical patterns of horizontal connectivity  including
association ﬁelds and the paired near-excitatory / far-inhibitory surrounds that may be responsible
for many contextual illusions [20  56]. The horizontal connections learned by the hGRU reproduce
another aspect of human behavior  in which the saliency of a straight contour decreases as the
distance between its paddles increases. This sheds light on a possible relationship between horizontal
connections and saliency computation.
In summary  this work diagnoses a computational deﬁciency of feedforward networks  and intro-
duces a biologically-inspired solution that can be easily incorporated into existing deep learning
architectures. The weights and patterns of behavior learned by the hGRU appear consistent with
those associated with the visual cortex  demonstrating its potential for establishing novel connections
between machine learning  cognitive science  and neuroscience.

Acknowledgments

This research was supported by NSF early career award [grant number IIS-1252951] and DARPA
young faculty award [grant number YFA N66001-14-1-4037]. Additional support was provided by
the Carney Institute for Brain Science and the Center for Computation and Visualization (CCV) at
Brown University.

9

References
[1] P. Arbelaez  M. Maire  C. Fowlkes  and J. Malik. Contour detection and hierarchical image
segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence  33(5):898–916 
May 2011.

[2] C.-Y. Lee  S. Xie  P. Gallagher  Z. Zhang  and Z. Tu. Deeply-Supervised nets. In Proceedings of
the International Conference on Artiﬁcial Intelligence and Statistics  pages 562–570  February
2015.

[3] S. Xie and Z. Tu. Holistically-Nested edge detection. International Journal of Computer Vision 

125(1):3–18  December 2017.

[4] Y. Liu  M. M. Cheng  X. Hu  K. Wang  and X. Bai. Richer convolutional features for edge
detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 5872–5881  July 2017.

[5] K.-K. Maninis  J. Pont-Tuset  P. Arbelaez  and L. Van Gool. Convolutional oriented boundaries:
From image segmentation to High-Level tasks. IEEE Transactions on Pattern Analysis and
Machine Intelligence  40(4):819–833  April 2018.

[6] Y. Wang  X. Zhao  and K. Huang. Deep crisp boundaries. In Proceedings of the IEEE Conference

on Computer Vision and Pattern Recognition  pages 3892–3900  2017.

[7] R. Houtkamp and P. R. Roelfsema. Parallel and serial grouping of image elements in visual
perception. Journal of Experimental Psychology: Human Perception and Performance  36(6):
1443–1459  December 2010.

[8] D. D. Stettler  A. Das  J. Bennett  and C. D. Gilbert. Lateral connectivity and contextual

interactions in macaque primary visual cortex. Neuron  36(4):739–750  November 2002.

[9] K. S. Rockland and J. S. Lund. Intrinsic laminar lattice connections in primate visual cortex.

Journal of Comparative Neurology  216(3):303–318  May 1983.

[10] S. Grossberg and E. Mingolla. Neural dynamics of perceptual grouping: textures  boundaries 

and emergent segmentations. Perception & Psychophysics  38(2):141–171  August 1985.

[11] D. J. Field  A. Hayes  and R. F. Hess. Contour integration by the human visual system: Evidence

for a local “association ﬁeld”. Vision Research  33(2):173–193  1993.

[12] G. W. Lesher and E. Mingolla. The role of edges and line-ends in illusory contour formation.

Vision Research  33(16):2253–2270  November 1993.

[13] W. Li  V. Piëch  and C. D. Gilbert. Contour saliency in primary visual cortex. Neuron  50(6):

951–962  June 2006.

[14] W. Li  V. Piëch  and C. D. Gilbert. Learning to link visual contours. Neuron  57(3):442–451 

February 2008.

[15] S. W. Zucker. Stereo  shading  and surfaces: Curvature constraints couple neural computations.

Proceedings of the IEEE  102(5):812–829  May 2014.

[16] P. Series  J. Lorenceau  and Y. Frégnac. The “silent” surround of V1 receptive ﬁelds: theory

and experiments. Journal of Physiology-Paris  97:453–474  2003.

[17] L. Zhaoping. Neural circuit models for computations in early visual cortex. Current Opinion in

Neurobiology  21(5):808–815  October 2011.

[18] S. Shushruth  P. Mangapathy  J. M. Ichida  P. C. Bressloff  L. Schwabe  and A. Angelucci.
Strong recurrent networks compute the orientation tuning of surround modulation in the primate
primary visual cortex. Journal of Neuroscience  32(1):308–321  2012.

[19] D. B. Rubin  S. D. Van Hooser  and K. D. Miller. The stabilized supralinear network: a unifying
circuit motif underlying multi-input integration in sensory cortex. Neuron  85(2):402–417 
January 2015.

10

[20] D. A. Mély  D. Linsley  and T. Serre. Complementary surrounds explain diverse contextual

phenomena across visual modalities. Psychological Review  125(5):769–784  October 2018.

[21] K. Cho  B. van Merrienboer  C. Gulcehre  D. Bahdanau  F. Bougares  H. Schwenk  and
Y. Bengio. Learning phrase representations using RNN Encoder–Decoder for statistical machine
translation. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing  pages 1724–1734  Stroudsburg  PA  USA  2014.

[22] W. Li and C. D. Gilbert. Global contour saliency and local colinear interactions. Journal of

Neurophysiology  88(5):2846–2856  November 2002.

[23] S. Xie and Z. Tu. Holistically-Nested edge detection. International Journal of Computer Vision 

125(1-3):3–18  December 2017.

[24] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation  9(8):

1735–1780  November 1997.

[25] A. Graves  S. Fernández  and J. Schmidhuber. Multi-dimensional recurrent neural networks.
In Proceedings of the International Conference on Artiﬁcial Neural Networks  pages 549–558 
Berlin  Heidelberg  2007.

[26] A. Graves and J. Schmidhuber. Ofﬂine handwriting recognition with multidimensional recurrent
neural networks. In Advances in Neural Information Processing Systems  pages 545–552  2009.

[27] S. Bell  C. Lawrence Zitnick  K. Bala  and R. Girshick. Inside-outside net: Detecting objects in
context with skip pooling and recurrent neural networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition  pages 2874–2883  2016.

[28] L. Theis and M. Bethge. Generative image modeling using spatial LSTMs. In Advances in

Neural Information Processing Systems  pages 1927–1935  2015.

[29] A. Van Den Oord  N. Kalchbrenner  and K. Kavukcuoglu. Pixel recurrent neural networks. In
Proceedings of the International Conference on Machine Learning  vol. 48  pages 1747–1756 
New York  NY  USA  2016.

[30] M. Liang and X. Hu. Recurrent convolutional neural network for object recognition.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
3367–3375  2015.

[31] Q. Liao and T. Poggio. Bridging the gaps between residual learning  recurrent neural networks

and visual cortex. arXiv preprint arXiv:1604.03640  April 2016.

[32] J. Kim  J. K. Lee  and K. M. Lee. Deeply-Recursive convolutional network for image Super-
Resolution. In Proceeedings of the IEEE Conference on Computer Vision and Pattern Recogni-
tion  pages 1637–1645  2016.

[33] C. J. Spoerer  P. McClure  and N. Kriegeskorte. Recurrent convolutional neural networks: A
better model of biological object recognition. Frontiers in Psychology  8:1551  September 2017.

[34] W. Lotter  G. Kreiman  and D. Cox. Deep predictive coding networks for video prediction
In Proceedings of the International Conference on Learning

and unsupervised learning.
Representations  2017.

[35] A. R. Zamir  T.-L. Wu  L. Sun  W. Shen  B. E. Shi  J. Malik  and S. Savarese. Feedback

Networks. arXiv preprint arxiv: 1612.09508  December 2016.

[36] A. Nayebi  D. Bear  J. Kubilius  K. Kar  S. Ganguli  D. Sussillo  J. J. DiCarlo  and D. L. K.
Yamins. Task-Driven convolutional recurrent models of the visual system. arXiv preprint
arXiv:1807.00053  June 2018.

[37] I. Kokkinos. Pushing the boundaries of boundary detection using deep learning. In Proceedings

of the International Conference on Learning Representations  2016.

11

[38] L.-C. Chen  G. Papandreou  I. Kokkinos  K. Murphy  and A. L. Yuille. DeepLab: Semantic
image segmentation with deep convolutional nets  atrous convolution  and fully connected CRFs.
IEEE Transactions on Pattern Analysis and Machine Intelligence  40(4):834–848  April 2018.
[39] D. George  W. Lehrach  K. Kansky  M. Lázaro-Gredilla  C. Laan  B. Marthi  X. Lou  Z. Meng 
Y. Liu  H. Wang  A. Lavin  and D. S. Phoenix. A generative vision model that trains with high
data efﬁciency and breaks text-based CAPTCHAs. Science  358(6368)  December 2017.

[40] S. Zheng  S. Jayasumana  B. Romera-Paredes  V. Vineet  Z. Su  D. Du  C. Huang  and P. H. S.
Torr. Conditional random ﬁelds as recurrent neural networks. In Proceedings of the IEEE
International Conference on Computer Vision  pages 1529–1537  2015.

[41] T. Cooijmans  N. Ballas  C. Laurent  Ç. Gülçehre  and A. Courville. Recurrent batch nor-
In Proceedings of the International Conference on Learning Representations 

malization.
2017.

[42] D. R. Martin  C. C. Fowlkes  and J. Malik. Learning to detect natural image boundaries using
local brightness  color  and texture cues. IEEE Transactions on Pattern Analysis and Machine
Intelligence  26(5):530–549  May 2004.
[43] K. He  X. Zhang  S. Ren  and J. Sun.

In
Proceedings of the European Conference on Computer Vision  Amsterdam  The Netherlands 
pages 630–645  2016.

Identity mappings in deep residual networks.

[44] J. Long  E. Shelhamer  and T. Darrell. Fully convolutional networks for semantic segmentation.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
3431–3440  2015.

[45] F. Yu and V. Koltun. Multi-Scale context aggregation by dilated convolutions. In Proceedings

of the International Conference on Learning Representations  2016.

[46] T. Wang  M. Sun  and K. Hu. Dilated deep residual network for image denoising. In 29th
IEEE International Conference on Tools with Artiﬁcial Intelligence  Boston  MA  USA  pages
1272–1279  2017.

[47] R. Hamaguchi  A. Fujita  K. Nemoto  T. Imaizumi  and S. Hikosaka. Effective use of di-
lated convolutions for segmenting small object instances in remote sensing imagery. CoRR 
abs/1709.00179  2017.

[48] X. Wang  R. Girshick  A. Gupta  and K. He. Non-Local neural networks. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition  2018.

[49] K. He  X. Zhang  S. Ren  and J. Sun. Delving deep into rectiﬁers: Surpassing human-level
performance on imagenet classiﬁcation. In Proceedings of the IEEE International Conference
on Computer Vision  pages 1026–1034  Washington  DC  USA  2015.

[50] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image

recognition. CoRR  abs/1409.1556  2014.

[51] G. Papandreou  I. Kokkinos  and P.-A. Savalle. Modeling local and global deformations in deep
learning: Epitomic convolution  multiple instance learning  and sliding window detection. In
Proceedings of the Conference on Computer Vision and Pattern Recognition  pages 390–399 
June 2015.

[52] K.-K. Maninis  J. Pont-Tuset  P. Arbelaez  and L. Van Gool. Convolutional oriented boundaries:
From image segmentation to High-Level tasks. IEEE Transactions on Pattern Analysis and
Machine Intelligence  May 2017.

[53] O. Ronneberger  P. Fischer  and T. Brox. U-Net: Convolutional networks for biomedical
image segmentation. In Medical Image Computing and Computer-Assisted Intervention  pages
234–241. Springer International Publishing  2015.

[54] V. Badrinarayanan  A. Kendall  and R. Cipolla. SegNet: A deep convolutional Encoder-Decoder
architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine
Intelligence  39(12):2481–2495  December 2017.

12

[55] O. Ben-Shahar and S. Zucker. Geometrical computations explain projection patterns of long-
range horizontal connections in visual cortex. Neural Computation  16(3):445–476  March
2004.

[56] S. Shushruth  L. Nurminen  M. Bijanzadeh  J. M. Ichida  S. Vanni  and A. Angelucci. Different
orientation tuning of near- and far-surround suppression in macaque primary visual cortex
mirrors their tuning in human perception. Journal of Neuroscience  33(1):106–119  January
2013.

[57] H. Tanaka and I. Ohzawa. Surround suppression of V1 neurons mediates orientation-based
representation of high-order visual features. Journal of Neurophysiology  101(3):1444–1462 
March 2009.

[58] J. Kim  M. Ricci  and T. Serre. Not-So-CLEVR: learning same-different relations strains

feedforward neural networks. Interface Focus  8(4):20180011  August 2018.

[59] C. Szegedy et al. Intriguing properties of neural networks. In Proceedings of the International

Conference on Learning Representations  2014.

[60] A. Nguyen  J. Yosinski  and J. Clune. Deep neural networks are easily fooled: High conﬁdence
predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 427–436  2015.

[61] A. Volokitin  G. Roig  and T. A. Poggio. Do deep neural networks suffer from crowding? In

Advances in Neural Information Processing Systems 30  pages 5628–5638  2017.

[62] K. Ellis  A. Solar-Lezama  and J. Tenenbaum. Unsupervised learning by program synthesis. In

Advances in Neural Information Processing Systems 28  pages 973–981  2015.

[63] P. R. Roelfsema and R. Houtkamp. Incremental grouping of image elements in vision. Attention 

Perception & Psychophysics  73(8):2542–2572  November 2011.

[64] C. D. Gilbert and W. Li. Top-down inﬂuences on visual processing. Nature Reviews Neuro-

science  14(5):350–363  May 2013.

[65] C. Tallec and Y. Ollivier. Can recurrent neural networks warp time? arXiv preprint arxiv:

1804.11188  March 2018.

[66] J. W. Peirce. PsychoPy—Psychophysics software in python. Journal of Neuroscience Methods 

162(1):8–13  May 2007.

[67] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics 
pages 249–256  Sardinia  Italy  2010.

[68] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arxiv:

1412.6980  December 2014.

[69] M. Abadi et al. TensorFlow: A system for large-scale machine learning. In Proceedings of
the USENIX Conference on Operating Systems Design and Implementation  pages 265–283 
Berkeley  CA  USA  2016.

[70] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. arXiv preprint arxiv: 1502.03167  February 2015.

[71] R. K. Srivastava  K. Greff  and J. Schmidhuber. Highway networks. arXiv preprint arxiv:

1505.00387  May 2015.

13

,Adria Recasens
Aditya Khosla
Carl Vondrick
Antonio Torralba
Drew Linsley
Junkyung Kim
Vijay Veerabadran
Charles Windolf
Thomas Serre