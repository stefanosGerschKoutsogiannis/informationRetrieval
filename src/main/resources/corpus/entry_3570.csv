2014,Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Matrix Decomposition,In this paper  we consider a multi-step version of the stochastic ADMM method with efficient guarantees for high-dimensional problems. We first analyze the simple setting  where the optimization problem consists of a loss function and a single regularizer (e.g. sparse optimization)  and then extend to the multi-block setting with multiple regularizers and multiple variables (e.g. matrix decomposition into sparse and low rank components). For the sparse optimization problem  our method achieves the minimax rate of $O(s\log d/T)$ for $s$-sparse problems in $d$ dimensions in $T$ steps  and is thus  unimprovable by any method up to constant factors. For the matrix decomposition problem with a general loss function  we analyze the multi-step ADMM with multiple blocks. We establish $O(1/T)$ rate and efficient scaling as the size of matrix grows. For natural noise models (e.g. independent noise)  our convergence rate is minimax-optimal. Thus  we establish tight convergence guarantees for multi-block ADMM in high dimensions. Experiments show that for both sparse optimization and matrix decomposition problems  our algorithm outperforms the state-of-the-art methods.,Multi-Step Stochastic ADMM in High Dimensions:

Applications to Sparse Optimization

and Matrix Decomposition

Hanie Sedghi

Univ. of Southern California

Los Angeles  CA 90089
hsedghi@usc.edu

Anima Anandkumar
University of California

Irvine  CA 92697

a.anandkumar@uci.edu

Edmond Jonckheere

Univ. of Southern California

Los Angeles  CA 90089
jonckhee@usc.edu

Abstract

In this paper  we consider a multi-step version of the stochastic ADMM method
with efﬁcient guarantees for high-dimensional problems. We ﬁrst analyze the
simple setting  where the optimization problem consists of a loss function and
a single regularizer (e.g. sparse optimization)  and then extend to the multi-block
setting with multiple regularizers and multiple variables (e.g. matrix decomposi-
tion into sparse and low rank components). For the sparse optimization problem 
our method achieves the minimax rate of O(s log d/T ) for s-sparse problems in
d dimensions in T steps  and is thus  unimprovable by any method up to constant
factors. For the matrix decomposition problem with a general loss function  we
analyze the multi-step ADMM with multiple blocks. We establish O(1/T ) rate
and efﬁcient scaling as the size of matrix grows. For natural noise models (e.g.
independent noise)  our convergence rate is minimax-optimal. Thus  we establish
tight convergence guarantees for multi-block ADMM in high dimensions. Experi-
ments show that for both sparse optimization and matrix decomposition problems 
our algorithm outperforms the state-of-the-art methods.

1

Introduction

Stochastic optimization techniques have been extensively employed for online machine learning
on data which is uncertain  noisy or missing. Typically it involves performing a large number of
inexpensive iterative updates  making it scalable for large-scale learning. In contrast  traditional
batch-based techniques involve far more expensive operations for each update step. Stochastic opti-
mization has been analyzed in a number of recent works.
The alternating direction method of multipliers (ADMM) is a popular method for online and dis-
tributed optimization on a large scale [1]  and is employed in many applications. It can be viewed as
a decomposition procedure where solutions to sub-problems are found locally  and coordinated via
constraints to ﬁnd the global solution. Speciﬁcally  it is a form of augmented Lagrangian method
which applies partial updates to the dual variables. ADMM is often applied to solve regularized
problems  where the function optimization and regularization can be carried out locally  and then
coordinated globally via constraints. Regularized optimization problems are especially relevant in
the high dimensional regime since regularization is a natural mechanism to overcome ill-posedness
and to encourage parsimony in the optimal solution  e.g.  sparsity and low rank. Due to the efﬁciency
of ADMM in solving regularized problems  we employ it in this paper.
We consider a simple modiﬁcation to the (inexact) stochastic ADMM method [2] by incorporating
multiple steps or epochs  which can be viewed as a form of annealing. We establish that this simple
modiﬁcation has huge implications in achieving tight bounds on convergence rate as the dimensions

1

T

of the problem instances scale. In each iteration  we employ projections on to certain norm balls
of appropriate radii  and we decrease the radii in epochs over time. For instance  for the sparse
optimization problem  we constrain the optimal solution at each step to be within an (cid:96)1-norm ball of
the initial estimate  obtained at the beginning of each epoch. At the end of the epoch  an average is
computed and passed on to the next epoch as its initial estimate. Note that the (cid:96)1 projection can be
solved efﬁciently in linear time  and can also be parallelized easily [3]. For matrix decomposition
with a general loss function  the ADMM method requires multiple blocks for updating the low rank
and sparse components. We apply the same principle and project the sparse and low rank estimates
on to (cid:96)1 and nuclear norm balls  and these projections can be computed efﬁciently.
Theoretical implications: The above simple modiﬁcations to ADMM have huge implications
for high-dimensional problems. For sparse optimization  our convergence rate is O( s log d
)  for
s-sparse problems in d dimensions in T steps. Our bound has the best of both worlds: efﬁcient
high-dimensional scaling (as log d) and efﬁcient convergence rate (as 1
T ). This also matches the
minimax rate for the linear model and square loss function [4]  which implies that our guarantee is
unimprovable by any (batch or online) algorithm (up to constant factors). For matrix decomposition 
our convergence rate is O((s + r)β2(p) log p/T )) + O(max{s + r  p}/p2) for a p × p input matrix
in T steps  where the sparse part has s non-zero entries and low rank part has rank r. For many nat-
ural noise models (e.g. independent noise  linear Bayesian networks)  β2(p) = p  and the resulting
convergence rate is minimax-optimal. Note that our bound is not only on the reconstruction error 
but also on the error in recovering the sparse and low rank components. These are the ﬁrst conver-
gence guarantees for online matrix decomposition in high dimensions. Moreover  our convergence
rate holds with high probability when noisy samples are input  in contrast to expected convergence
rate  typically analyzed in the literature. See Table 1  2 for comparison of this work with related
frameworks. Proof of all results and implementation details can be found in the longer version [5].
Practical implications: The proposed algorithms provide signiﬁcantly faster convergence in high
dimension and better robustness to noise. For sparse optimization  our method has signiﬁcantly
better accuracy compared to the stochastic ADMM method and better performance than RADAR 
based on multi-step dual averaging [6]. For matrix decomposition  we compare our method with the
state-of-art inexact ALM [7] method. While both methods have similar reconstruction performance 
our method has signiﬁcantly better accuracy in recovering the sparse and low rank components.
Related Work: ADMM: Existing online ADMM-based methods lack high-dimensional guaran-
tees. They scale poorly with the data dimension (as O(d2))  and also have slow convergence for
general problems (as O( 1√
)). Under strong convexity  the convergence rate can be improved to
O( 1
T ) but only in expectation: such analyses ignore the per sample error and consider only the
expected convergence rate(see Table 1). In contrast  our bounds hold with high probability. Some
stochastic ADMM methods  Goldstein et al. [8]  Deng [9] and Luo [10] provide faster rates for
stochastic ADMM  than the rate noted in Table 1. However  they require strong conditions which
are not satisﬁed for the optimization problems considered here  e.g.  Goldstein et al. [8] require both
the loss function and the regularizer to be strongly convex.
Related Work: Sparse Optimization: For the sparse optimization problem  (cid:96)1 regularization is
employed and the underlying true parameter is assumed to be sparse. This is a well-studied problem
in a number of works (for details  refer to [6]). Agarwal et al. [6] propose an efﬁcient online method
based on dual averaging  which achieves the same optimal rates as the ones derived in this paper. The
main difference is that our ADMM method is capable of solving the problem for multiple random
variables and multiple conditions while their method cannot incorporate these extensions.
Related Work: Matrix Decomposition: To the best of our knowledge  online guarantees for high-
dimensional matrix decomposition have not been provided before. Wang et al. [12] propose a multi-
block ADMM method for the matrix decomposition problem but only provide convergence rate
analysis in expectation and it has poor high dimensional scaling (as O(p4) for a p × p matrix)
without further modiﬁcations. Note that they only provide convergence rate on difference between
loss function and optimal loss  whereas we provide the convergence rate on individual errors of the
sparse and low rank components (cid:107) ¯S(T ) − S∗(cid:107)2F (cid:107) ¯L(T ) − L∗(cid:107)2F. See Table 2 for comparison of
guarantees for matrix decomposition problem.
Notation In the sequel  we use lower case letter for vectors and upper case letter for matrices.
Moreover  X ∈ Rp×p. (cid:107)x(cid:107)1  (cid:107)x(cid:107)2 refer to (cid:96)1  (cid:96)2 vector norms respectively. The term (cid:107)X(cid:107)∗ stands

T

2

Method

ST-ADMM [2]
ST-ADMM [2]
BADMM [11]
RADAR [6]

REASON 1 (this paper)

Minimax bound [4]

Assumptions
L  convexity

SC  E

convexity  E

LSC  LL
LSC  LL

Eigenvalue conditions

√
√

Convergence rate
O(d2/
T )
O(d2 log T /T )
O(d2/
T )
O(s log d/T )
O(s log d/T )
O(s log d/T )

Table 1: Comparison of online sparse optimization methods under s sparsity level for the optimal
paramter  d dimensional space  and T number of iterations. SC = Strong Convexity  LSC = Local
Strong Convexity  LL = Local Lipschitz  L = Lipschitz property  E=in Expectation. The last row
provides the minimax-optimal rate for any method. The results hold with high probability.

Method

Multi-block-ADMM[12]

Batch method[13]

REASON 2 (this paper)

Minimax bound[13]

Assumptions

L  SC  E

Convergence rate

O(p4/T )

LL  LSC  DF
LSC  LL  DF O((s + r)β2(p) log p/T ))+O(max{s + r  p}/p2)
(cid:96)2  IN  DF

O((s log p + rp)/T )+O(s/p2)
O((s log p + rp)/T )+O(s/p2)

√

Table 2: Comparison of optimization methods for sparse+low rank matrix decomposition for a p× p
matrix under s sparsity level and r rank matrices and T is the number of samples. Abbreviations
are as in Table 1  IN = Independent noise model  DF = diffuse low rank matrix under the optimal
p) O(p) and its value depends the model. The last row provides the
parameter. β(p) = Ω(
minimax-optimal rate for any method under the independent noise model. The results hold with high
probability unless otherwise mentioned. For Multi-block-ADMM [12] the convergence rate is on the
difference of loss function from optimal loss  for the rest of works in the table  the convergence rate is
on the individual estimates of the sparse and low rank components: (cid:107) ¯S(T )− S∗(cid:107)2F +(cid:107) ¯L(T )− L∗(cid:107)2F.
for nuclear norm of X. In addition  (cid:107)X(cid:107)2  (cid:107)X(cid:107)F denote spectral and Frobenius norms respectively.

We use vectorized (cid:96)1  (cid:96)∞ norm for matrices  i.e.  (cid:107)X(cid:107)1 =(cid:80)

|Xij|  (cid:107)X(cid:107)∞ = max

|Xij|.

i j

i j

(cid:96)1 Regularized Stochastic Optimization

2
We consider the optimization problem θ∗ ∈ arg min E[f (θ  x)]  θ ∈ Ω where θ∗ is a sparse vector.
The loss function f (θ  xk) is a function of a parameter θ ∈ Rd and samples xi. In stochastic setting 
we do not have access to E[f (θ  x)] nor to its subgradients. In each iteration we have access to one
noisy sample. In order to impose sparsity we use regularization. Thus  we solve a sequence

θk ∈ arg min
θ∈Ω(cid:48)

f (θ  xk) + λ(cid:107)θ(cid:107)1  Ω(cid:48) ⊂ Ω 

(1)

where the regularization parameter λ > 0 and the constraint sets Ω(cid:48) change from epoch to epoch.

2.1 Epoch-based Stochastic ADMM Algorithm

We now describe the modiﬁed inexact ADMM algorithm for the sparse optimization problem in (1) 
and refer to it as REASON 1  see Algorithm 1. We consider an epoch length T0  and in each epoch
i  we project the optimal solution on to an (cid:96)1 ball with radius Ri centered around ˜θi  which is the
initial estimate of θ∗ at the start of the epoch. The θ-update is given by

θk+1 = arg min
1≤R2

(cid:107)θ−˜θi(cid:107)2

i

{(cid:104)∇f (θk)  θ − θk(cid:105) − (cid:104)zk  θ − yk(cid:105) +

(cid:107)θ − yk(cid:107)2

2 +

ρ
2

ρx
2

(cid:107)θ − θk(cid:107)2
2}.

(2)

Note that this is an inexact update since we employ the gradient ∇f (·) rather than optimize directly
on the loss function f (·) which is expensive. The above program can be solved efﬁciently since
it is a projection on to the (cid:96)1 ball  whose complexity is linear in the sparsity level of the gradient 
when performed serially  and O(log d) when performed in parallel using d processors [3]. For the
regularizer  we introduce the variable y  and the y-update is yk+1 = arg min{λi(cid:107)yk(cid:107)1−(cid:104)zk  θk+1−

3

Algorithm 1: Regularized Epoch-based Admm for Stochastic Opt. in high-dimensioN 1 (REASON 1)

Input ρ  ρx  epoch length T0   initial prox center ˜θ1  initial radius R1  regularization parameter
{λi}kT
i=1.
Deﬁne Shrinkκ(a) = (a − κ)+ − (−a − κ)+.
for Each epoch i = 1  2  ...  kT do
Initialize θ0 = y0 = ˜θi
for Each iteration k = 0  1  ...  T0 − 1 do

{(cid:104)∇f (θk)  θ − θk(cid:105) − (cid:104)zk  θ − yk(cid:105) +

ρ
2

(cid:107)θ − yk(cid:107)2

ρx
2
zk+1 = zk − τ (θk+1 − yk+1)

2 +

(cid:107)θ − θk(cid:107)2
2}

θk+1 = arg min
(cid:107)θ−˜θi(cid:107)1≤Ri

yk+1 = Shrinkλi/ρ(θk+1 − zk
ρ

) 

(cid:80)T0−1

k=0 θk for epoch i and ˜θi+1 = θ(Ti).

Return : θ(Ti) := 1
T
Update : R2
i /2.
i+1 = R2
y(cid:105)+ ρ
2}. This update can be simpliﬁed to the form given in REASON 1  where Shrinkκ(·)
2(cid:107)θk+1−y(cid:107)2
is the soft-thresholding or shrinkage function [1]. Thus  each step in the update is extremely simple
to implement. When an epoch is complete  we carry over the average θ(Ti) as the next epoch center
and reset the other variables.

2.2 High-dimensional Guarantees

2(cid:107)θ2 − θ1(cid:107)2
2.

We now provide convergence guarantees for the proposed method under the following assumptions.
Assumption A1: Local strong convexity (LSC): The function f : S → R satisﬁes an R-local form
of strong convexity (LSC) if there is a non-negative constant γ = γ(R) such that for any θ1  θ2 ∈ S
with (cid:107)θ1(cid:107)1 ≤ R and (cid:107)θ2(cid:107)1 ≤ R  f (θ1) ≥ f (θ2) + (cid:104)∇f (θ2)  θ1 − θ2(cid:105) + γ
Note that the notion of strong convexity leads to faster convergence rates in general. Intuitively 
strong convexity is a measure of curvature of the loss function  which relates the reduction in the
loss function to closeness in the variable domain. Assuming that the function f is twice continuously
differentiable  it is strongly convex  if and only if its Hessian is positive semi-deﬁnite  for all feasible
θ. However  in the high-dimensional regime  where there are fewer samples than data dimension  the
Hessian matrix is often singular and we do not have global strong convexity. A solution is to impose
local strong convexity which allows us to provide guarantees for high dimensional problems. This
notion has been exploited before in a number of works on high dimensional analysis  e.g.  [14  13  6].
It holds for various loss functions such as square loss.
Assumption A2: Sub-Gaussian stochastic gradients: Let ek(θ) := ∇f (θ  xk) − E[∇f (θ  xk)].
There is a constant σ = σ(R) such that for all k > 0  E[exp((cid:107)ek(θ)(cid:107)2∞)/σ2] ≤ exp(1)  for all θ
such that (cid:107)θ − θ∗(cid:107)1 ≤ R.
Remark: The bound holds with σ = O(
sub-Gaussian tails [6].
Assumption A3: Local Lipschitz condition: For each R > 0  there is a constant G = G(R) such
that  |f (θ1)−f (θ2)| ≤ G(cid:107)θ1−θ2(cid:107)1  for all θ1  θ2 ∈ S such that (cid:107)θ−θ∗(cid:107)1 ≤ R and (cid:107)θ1−θ∗(cid:107)1 ≤ R.
The design parameters are as below where λi is the regularization for (cid:96)1 term in epoch i  ρ and ρx
are penalties in θ-update as in (2) and τ is the step size for the dual update.

log d) whenever each component of the error vector has

√

√

√

λ2
i =

√
γRi
s
T0

G2(ρ + ρx)2

log d +

T 2
0

+ σ2

i log(

3
δi

) 

ρ ∝

T0 log d

Ri

 

ρx > 0 

τ ∝

T0
Ri

.

(3)
Theorem 1. Under Assumptions A1 − A3  λi as in (3)   with ﬁxed epoch lengths T0 = T log d/kT  
where T is the total number of iterations and

(cid:115)

kT = log2

s2(log d + γ

1T

γ2R2
s G + 12σ2 log( 6

δ ))

 

4

and T0 satisﬁes T0 = O(log d)  for any θ∗ with sparsity s  with probability at least 1 − δ we have

(cid:107)¯θT − θ∗(cid:107)2

2 = O

s

log d + γ

s G + (log(1/δ) + log(kT /log d))σ2

T

log d
kT

(cid:18)

(cid:19)

 

where ¯θT is the average for the last epoch for a total of T iterations.

Improvement of log d factor : The above theorem covers the practical case where the epoch length
T0 is ﬁxed. We can improve the above results using varying epoch length (which depend on the
problem parameters) such that (cid:107)¯θT − θ∗(cid:107)2
2 = O(s log d/T ). The details can be found in the longer
version [5].This convergence rate of O(s log d/T ) matches the minimax lower bounds for sparse
estimation [4]. This implies that our guarantees are unimprovable up to constant factors.

3 Extension to Doubly Regularized Stochastic Optimization
We consider the optimization problem M∗ ∈ arg min E[f (M  X)]  where we want to decompose
M into a sparse matrix S ∈ Rp×p and a low rank matrix L ∈ Rp×p. f (M  Xk) is a function of a
parameter M and samples Xk. Xk can be a matrix (e.g. independent noise model) or a vector (e.g.
Gaussian graphical model). In stochastic setting  we do not have access to E[f (M  X)] nor to its
subgradients. In each iteration  we have access to one noisy sample and update our estimate based
on that. We impose the desired properties with regularization. Thus  we solve a sequence

(cid:99)Mk := arg min{(cid:98)f (M  Xk) + λn(cid:107)S(cid:107)1 + µn(cid:107)L(cid:107)∗}

s.t. M = S + L 

(cid:107)L(cid:107)∞ ≤ α
p

.

(4)

We propose an online program based on multi-block ADMM algorithm. In addition to tailoring
projection ideas employed for sparse case  we impose an (cid:96)∞ constraint of α/p on each entry of L.
This constraint is also imposed for the batch version of the problem (4) in [13]  and we assume that
the true matrix L∗ satisﬁes this constraint. Intuitively  the (cid:96)∞ constraint controls the “spikiness”
of L∗. If α ≈ 1  then the entries of L are O(1/p)  i.e. they are “diffuse” or “non-spiky”  and no
entry is too large. When the low rank matrix L∗ has diffuse entries  it cannot be a sparse matrix 
and thus  can be separated from the sparse S∗ efﬁciently. In fact  the (cid:96)∞ constraint is a weaker form
of the incoherence-type assumptions needed to guarantee identiﬁability [15] for sparse+low rank
decomposition. For more discussions  see Section 3.2.

3.1 Epoch-based Multi-Block ADMM Algorithm

We now extend the ADMM method proposed in REASON 1 to multi-block ADMM. The details
are in Algorithm 2  and we refer to it as REASON 2. Recall that the matrix decomposition setting
assumes that the true matrix M∗ = S∗ + L∗ is a combination of a sparse matrix S∗ and a low rank
matrix L∗. In REASON 2  the updates for matrices M  S  L are done independently at each step. The
updates follow deﬁnition of ADMM and ideas presented in Section 2. We consider epochs of lengths
T0. We do not need to project the update of matrix M. The update rules for S  L are result of doing
an inexact proximal update by considering them as a single block  which can then be decoupled.
We impose an (cid:96)1-norm projection for the sparse estimate S around the epoch initialization ˜Si. For
the low rank estimate L  we impose a nuclear norm projection around the epoch initialization ˜Li.
Intuitively  the nuclear norm projection  which is an (cid:96)1 projection on the singular values  encourages
sparsity in the spectral domain leading to low rank estimates. We also require an (cid:96)∞ constraint on
L. Thus  the update rule for L has two projections  i.e. inﬁnity and nuclear norm projections. We
decouple it into ADMM updates L  Y with dual variable U corresponding to this decomposition.

3.2 High-dimensional Guarantees

We now prove that REASON 2 recovers both the sparse and low rank estimates in high dimensions
efﬁciently. We need the following assumptions  in addition to Assumptions A2  A3.
Assumption A4: Spectral Bound on the Gradient Error Let Ek(M  Xk) := ∇f (M  Xk) −
E[∇f (M  Xk)]  (cid:107)Ek(cid:107)2 ≤ β(p)σ  where σ := (cid:107)Ek(cid:107)∞.

5

Recall from Assumption A2 that σ = O(log p)  under sub-Gaussianity. Here  we require spectral
bounds in addition to (cid:107) · (cid:107)∞ bound in A2.
Assumption A5: Bound on spikiness of low-rank matrix (cid:107)L∗(cid:107)∞ ≤ α
Assumption A6: Local strong convexity (LSC) The function f : Rd1×d2 → Rn1×n2 satisﬁes
an R-local form of strong convexity (LSC) if there is a non-negative constant γ = γ(R) such that
f (B1) ≥ f (B2) + Tr(∇f (B2)(B1 − B2)) + γ
2(cid:107)B2 − B1(cid:107)F  for any (cid:107)B1(cid:107) ≤ R and (cid:107)B2(cid:107) ≤ R 
which is essentially the matrix version of Assumption A1.
We choose algorithm parameters as below where λi  µi are the regularization for (cid:96)1 and nuclear
norm respectively  ρ  ρx correspond to penalty terms in M-update and τ is dual update step size.

p   as discussed before.

γ

λ2
i =

(R2

i + ˜R2
i )
√
T0
(s + r)

log p+

G2(ρ + ρx)2

T 2
0

+β2(p)σ2

i log(

3
δi

)+

α2
p2 +

β2(p)σ2

T0

log p+log

(cid:18)

(cid:19)

1
δ

(cid:115)

(5)

T0
i + ˜R2

i

R2

(cid:113)

(cid:115)
(cid:115)

i = cµλ2
µ2
i  

ρ ∝

T0 log p
i + ˜R2
R2

i

 

ρx > 0 

τ ∝

(cid:18) (s + r)2

(cid:20)

γ2R2

1T

kT (cid:39) − log

Theorem 2. Under Assumptions A2 − A6  parameter settings (5)  let T denote total number of
iterations and T0 = T log p/kT   where

log p +

+ β2(p)σ2 [(1 + G)(log(6/δ) + log kT ) + log p]

G

s + r

and T0 satisﬁes T0 = O(log p)  with probability at least 1 − δ we have
(cid:107) ¯S(T ) − S∗(cid:107)2F + (cid:107) ¯L(T ) − L∗(cid:107)2F =

log p + G + β2(p)σ2(cid:104)

(s + r)

O

(1 + G)(log 6

δ + log kT

log p ) + log p

T

(cid:105)

(cid:18)

+

log p
kT

1 +

s + r
γ2p

(cid:21)(cid:19)

 

(cid:19) α2

p

.

Improvement of log p factor : The above result can be improved by a log p factor by considering
varying epoch lengths (which depend on problem parameters). The resulting convergence rate is
O((s + r)p log p/T + α2/p). The details can be found in the longer version [5].
√
p) ≤ β(p)Θ(p). This implies that the conver-
Scaling of β(p): We have the following bounds Θ(
√
gence rate (with varying epoch lengths) is O((s + r)p log p/T + α2/p)  when β(p) = Θ(
p) and
when β(p) = Θ(p)  it is O((s + r)p2 log p/T + α2/p). The upper bound on β(p) arises trivially by
converting the max-norm (cid:107)Ek(cid:107)∞ ≤ σ to the bound on the spectral norm (cid:107)Ek(cid:107)2. In many interesting
scenarios  the lower bound on β(p) is achieved  as outlined below in Section 3.2.1.
Comparison with the batch result: Agarwal et al. [13] consider the batch version of the same
problem (4)  and provide a convergence rate of O((s log p + rp)/T + sα2/p2). This is also the
minimax lower bound under the independent noise model. With respect to the convergence rate  we
√
match their results with respect to the scaling of s and r  and also obtain a 1/T rate. We match
p) attains the lower bound 
the scaling with respect to p (up to a log factor)  when β(p) = Θ(
and we discuss a few such instances below. Otherwise  we are worse by a factor of p compared
to the batch version. Intuitively  this is because we require different bounds on error terms Ek in
the online and the batch settings. The batch setting considers an empirical estimate  hence operates
on the averaged error. Whereas in the online setting we suffer from the per sample error. Efﬁcient
concentration bounds exist for the batch case [16]  while for the online case  no such bounds exist in
general. Hence  we conjecture that our bounds in Theorem 2 are unimprovable in the online setting.
Approximation Error: Note that the optimal decomposition M∗ = S∗ + L∗ is not identiﬁable
in general without the incoherence-style conditions [15  17].
In this paper  we provide efﬁcient
guarantees without assuming such strong incoherence constraints. This implies that there is an
approximation error which is incurred even in the noiseless setting due to model non-identiﬁability.

6

Algorithm 2: Regularized Epoch-based Admm for Stochastic Opt. in high-dimensioN 2 (REASON 2)

Input ρ  ρx  epoch length T0   regularization parameters {λi  µi}kT
initial radii R1  ˜R1.
Deﬁne Shrinkκ(a) shrinkage operator as in REASON 1  GMk = Mk+1 − Sk − Lk − 1
for each epoch i = 1  2  ...  kT do
Initialize S0 = ˜Si  L0 = ˜Li  M0 = S0 + L0.
for each iteration k = 0  1  ...  T0 − 1 do

i=1  initial prox centers ˜S1  ˜L1 

ρ Zk.

Mk+1 =

Sk+1 =

Lk+1 =

−∇f (Mk) + Zk + ρ(Sk + Lk) + ρxMk

min

(cid:107)S− ˜Si(cid:107)1≤Ri

λi(cid:107)S(cid:107)1 +
µi(cid:107)L(cid:107)∗ +
(cid:107)L− ˜Li(cid:107)∗≤ ˜Ri
(cid:107)Y − (Lk + τkGMk )(cid:107)2F +
ρ
2τk

ρ + ρx
(cid:107)S − (Sk + τkGMk )(cid:107)2F
ρ
2τk
(cid:107)L − Yk − Uk/ρ(cid:107)2F
ρ
2
ρ
2

min

(cid:107)Lk+1 − Y − Uk/ρ(cid:107)2F

(cid:107)Y (cid:107)∞≤α/p

Yk+1 = min
Zk+1 = Zk − τ (Mk+1 − (Sk+1 + Lk+1))
Uk+1 = Uk − τ (Lk+1 − Yk+1).

(cid:80)T0−1

k=0 Sk and ˜Li+1 := 1
T0

(cid:80)T0−1

k=0 Lk
i+1 = R2

Set: ˜Si+1 = 1
T0
if R2
else STOP;

i > 2(s + r + (s+r)2

pγ2 ) α2

p then Update R2

i /2  ˜R2

i+1 = ˜Ri

2

/2;

Dimension Run Time (s)

Method

error at 0.02T error at 0.2T error at T

d=20000

T=50

d=2000

T=5

d=20

T=0.2

ST-ADMM
RADAR
REASON
ST-ADMM
RADAR
REASON
ST-ADMM
RADAR
REASON

1.022
0.116
1.5e-03
0.794
0.103
0.001
0.212
0.531
0.100

1.002

2.10e-03
2.20e-04

0.380

4.80e-03
2.26e-04

0.092

4.70e-03
2.02e-04

0.996

6.26e-05
1.07e-08

0.348

1.53e-04
1.58e-08

0.033

4.91e-04
1.09e-08

Table 3: Least square regression problem  epoch size Ti = 2000  Error=

(cid:107)θ−θ∗(cid:107)2
(cid:107)θ∗(cid:107)2

.

Agarwal et al. [13] achieve an approximation error of sα2/p2 for their batch algorithm. Our online
algorithm has an approximation error of max{s + r  p}α2/p2  which is decaying with p. It is not
clear if this bound can be improved by any other online algorithm.

3.2.1 Optimal Guarantees for Various Statistical Models

We now list some statistical models under which we achieve the batch-optimal rate for sparse+low
rank decomposition.
1) Independent Noise Model: Assume we sample i.i.d. matrices Xk = S∗ + L∗ + Nk  where
the noise Nk has independent bounded sub-Gaussian entries with maxi j Var(Nk(i  j)) = σ2. We
consider the square loss function  (cid:107)Xk − S − L(cid:107)2F. Hence Ek = Xk − S∗ − L∗ = Nk. From [Thm.
1.1][18]  we have w.h.p. (cid:107)Nk(cid:107) = O(σ
p). We match the batch bound in [13] in this setting.
Moreover  Agarwal et al. [13] provide a minimax lower bound for this model  and we match it as
well. Thus  we achieve the optimal convergence rate for online matrix decomposition for this model.
2) Linear Bayesian Network: Consider a p-dimensional vector y = Ah + n  where h ∈ Rr with
r ≤ p  and n ∈ Rp. The variable h is hidden  and y is the observed variable. We assume that the
vectors h and n are each zero-mean sub-Gaussian vectors with i.i.d entries  and are independent of

√

7

Run Time

Error

REASON 2

IALM

(cid:107)M∗−S−L(cid:107)F

(cid:107)M∗(cid:107)F
2.20e-03
5.11e-05

T = 50 sec
(cid:107)S−S∗(cid:107)F
(cid:107)S∗(cid:107)F
0.004
0.12

(cid:107)L∗−L(cid:107)F
(cid:107)L∗(cid:107)F
0.01
0.27

(cid:107)M∗−S−L(cid:107)F

(cid:107)M∗(cid:107)F
5.55e-05
8.76e-09

T = 150 sec
(cid:107)S−S∗(cid:107)F
(cid:107)S∗(cid:107)F

1.50e-04

0.12

(cid:107)L∗−L(cid:107)F
(cid:107)L∗(cid:107)F

3.25e-04

0.27

Table 4: REASON 2 and inexact ALM  matrix decomposition problem. p = 2000  η2 = 0.01

h and σ2

hAA(cid:62) has rank at most r.

y y = S∗ + L∗  where S∗ = σ2

one another. Let σ2
n be the variances for the entries of h and n respectively. Without loss
of generality  we assume that the columns of A are normalized  as we can always rescale A and
σh appropriately to obtain the same model. Let Σ∗
y y be the true covariance matrix of y. From the
independence assumptions  we have Σ∗
nI is a diagonal matrix and
L∗ = σ2
In each step k  we obtain a sample yk from the Bayesian network. For the square loss function f 
k −
we have the error Ek = yky(cid:62)
√
nI(cid:107)2 = O(
h). We thus have with probability 1 − T e−cp 
(cid:107)hkh(cid:62)
σ2
∀ k ≤ T. When (cid:107)A(cid:107)2 is bounded  we obtain the optimal
h + σ2
bound in Theorem 2  which matches the batch bound. If the entries of A are generically drawn (e.g. 
are also “diffuse”  and thus  the low rank matrix L∗ satisﬁes Assumption A5  with α ∼ polylog(p).
Intuitively  when A is generically drawn  there are diffuse connections from hidden to observed
variables  and we have efﬁcient guarantees under this setting.

(cid:107)Ek(cid:107)2 ≤ O(cid:0)√
from a Gaussian distribution)  we have (cid:107)A(cid:107)2 = O(1 +(cid:112)r/p). Moreover  such generic matrices A

y y. Applying [Cor. 5.50][19]  we have  with w.h.p. (cid:107)nkn(cid:62)
hI(cid:107)2 = O(

k − Σ∗
k − σ2

n)(cid:1)  

pσ2
n) 
p((cid:107)A(cid:107)2σ2

√

pσ2

4 Experiments

REASON 1:
For sparse optimization problem  we compare REASON 1 with RADAR and
ST-ADMM under the least-squares regression setting. Samples (xt  yt) are generated such that
xt ∈ Unif[−B  B] and yt = (cid:104)θ∗  x(cid:105) + nt. θ∗ is s-sparse with s = (cid:100)log d(cid:101). nt ∼ N (0  η2).
With η2 = 0.5 in all cases. We consider d = 20  2000  20000 and s = 1  3  5 respectively.
The experiments are performed on a 2.5 GHz In-
tel Core i5 laptop with 8 GB RAM. See Table 3
for experiment results. It should be noted that
RADAR is provided with information of θ∗ for
epoch design and recentering. In addition  both
RADAR and REASON 1 have the same initial
radius. Nevertheless  REASON 1 reaches bet-
ter accuracy within the same run time even for
small time frames. In addition  we compare rel-
ative error (cid:107)θ − θ∗(cid:107)2/(cid:107)θ∗(cid:107)2 in REASON 1 and
ST-ADMM in the ﬁrst epoch. We observe that in
higher dimension error ﬂuctuations for ADMM
increases noticeably (see Figure 1). Therefore 
projections of REASON 1 play an important role
in denoising and obtaining good accuracy.
REASON 2: We compare REASON 2 with state-of-the-art inexact ALM method for matrix de-
composition problem (ALM codes are downloaded from [20]). Table 4 shows that with equal time 
inexact ALM reaches smaller (cid:107)M∗−S−L(cid:107)F
error while in fact this does not provide a good decompo-
sition. Further  REASON 2 reaches useful individual errors. Experiments with η2 ∈ [0.01  1] show
similar results. Similar experiments on exact ALM shows worse performance than inexact ALM.

Figure 1: Least square regression  Error=
vs. iteration number  d1 = 20 and d2 = 20000.

(cid:107)θ−θ∗(cid:107)2
(cid:107)θ∗(cid:107)2

(cid:107)M∗(cid:107)F

Acknowledgment

We acknowledge detailed discussions with Majid Janzamin and thank him for valuable comments on
sparse and low rank recovery. The authors thank Alekh Agarwal for detailed discussions of his work
and the minimax bounds. A. Anandkumar is supported in part by Microsoft Faculty Fellowship  NSF
Career award CCF-1254106  NSF Award CCF-1219234  and ARO YIP Award W911NF-13-1-0084.

8

50010001500200000.20.40.60.81x 10−4ttttt2500100015002000051015202530rrttttt350010001500200001234rrttttt45001000150020000246810ttttt1References
[1] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in
Machine Learning  3(1):1–122  2011.

[2] H. Ouyang  N. He  L. Tran  and A. G Gray. Stochastic alternating direction method of multi-
pliers. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) 
pages 80–88  2013.

[3] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the (cid:96)1-
ball for learning in high dimensions. In Proceedings of the 25th international conference on
Machine learning  pages 272–279. ACM  2008.

[4] G. Raskutti  M. J. Wainwright  and B. Yu. Minimax rates of estimation for high-dimensional
linear regression over (cid:96)q-balls. IEEE Trans. Information Theory  57(10):6976—6994  October
2011.

[5] Hanie Sedghi  Anima Anandkumar  and Edmond Jonckheere. Guarantees for multi-step

stochastic ADMM in high dimensions. arXiv preprint arXiv:1402.5131  2014.

[6] A. Agarwal  S. Negahban  and M. J. Wainwright. Stochastic optimization and sparse statistical

recovery: Optimal algorithms for high dimensions. In NIPS  pages 1547–1555  2012.

[7] Z. Lin  M. Chen  and Y. Ma. The augmented lagrange multiplier method for exact recovery of

corrupted low-rank matrices. arXiv preprint arXiv:1009.5055  2010.

[8] T Goldstein  B. ODonoghue  and S. Setzer. Fast alternating direction optimization methods.

CAM report  pages 12–35  2012.

[9] W. Deng  W.and Yin. On the global and linear convergence of the generalized alternating

direction method of multipliers. Technical report  DTIC Document  2012.

[10] Zhi-Quan Luo. On the linear convergence of the alternating direction method of multipliers.

arXiv preprint arXiv:1208.3922  2012.

[11] H. Wang and A. Banerjee. Bregman alternating direction method of multipliers. arXiv preprint

arXiv:1306.3203  2013.

[12] X. Wang  M. Hong  S. Ma  and Z. Luo. Solving multiple-block separable convex minimiza-
tion problems using two-block alternating direction method of multipliers. arXiv preprint
arXiv:1308.5294  2013.

[13] A. Agarwal  S. Negahban  and M. Wainwright. Noisy matrix decomposition via convex relax-

ation: Optimal rates in high dimensions. The Annals of Statistics  40(2):1171–1197  2012.

[14] S. Negahban  P. Ravikumar  M. Wainwright  and B. Yu. A uniﬁed framework for high-
dimensional analysis of M-estimators with decomposable regularizers. Statistical Science 
27(4):538–557  2012.

[15] V. Chandrasekaran  S. Sanghavi  Pablo A Parrilo  and A. S Willsky. Rank-sparsity incoherence

for matrix decomposition. SIAM Journal on Optimization  21(2):572–596  2011.

[16] J. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-

tional Mathematics  12(4):389–434  2012.

[17] Daniel Hsu  Sham M Kakade  and Tong Zhang. Robust matrix decomposition with sparse

corruptions. Information Theory  IEEE Transactions on  57(11):7221–7234  2011.

[18] Van H Vu. Spectral norm of random matrices. In Proceedings of the thirty-seventh annual

ACM symposium on Theory of computing  pages 423–430. ACM  2005.

[19] Roman Vershynin.

Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027  2010.

[20] Low-rank matrix recovery and completion via convex optimization.

http://
perception.csl.illinois.edu/matrix-rank/home.html. Accessed: 2014-
05-02.

9

,Hanie Sedghi
Anima Anandkumar
Linus Hamilton
Frederic Koehler
Claudia Shi
David Blei
Victor Veitch