2019,Stochastic Variance Reduced Primal Dual Algorithms for Empirical Composition Optimization,We consider a generic empirical composition optimization problem  where there are empirical averages present both outside and inside nonlinear loss functions. Such a problem is of interest in various machine learning applications  and cannot be directly solved by standard methods such as stochastic gradient descent (SGD). We take a novel approach to solving this problem by reformulating the original minimization objective into an equivalent min-max objective  which brings out all the empirical averages that are originally inside the nonlinear loss functions. We exploit the rich structures of the reformulated problem and develop a stochastic primal-dual algorithms  SVRPDA-I  to solve the problem efficiently. We carry out extensive theoretical analysis of the proposed algorithm  obtaining the convergence rate  the total computation complexity and the storage complexity. In particular  the algorithm is shown to converge at a linear rate when the problem is strongly convex. Moreover  we also develop an approximate version of the algorithm  named SVRPDA-II  which further reduces the memory requirement. Finally  we evaluate the performance of our algorithms on several real-world benchmarks and experimental results show that they significantly outperform existing techniques.,Stochastic Variance Reduced Primal Dual Algorithms

for Empirical Composition Optimization

Adithya M. Devraj∗ and

Jianshu Chen†

Abstract

We consider a generic empirical composition optimization problem  where there are
empirical averages present both outside and inside nonlinear loss functions. Such
a problem is of interest in various machine learning applications  and cannot be
directly solved by standard methods such as stochastic gradient descent. We take a
novel approach to solving this problem by reformulating the original minimization
objective into an equivalent min-max objective  which brings out all the empirical
averages that are originally inside the nonlinear loss functions. We exploit the
rich structures of the reformulated problem and develop a stochastic primal-dual
algorithm  SVRPDA-I  to solve the problem efﬁciently. We carry out extensive
theoretical analysis of the proposed algorithm  obtaining the convergence rate  the
computation complexity and the storage complexity. In particular  the algorithm is
shown to converge at a linear rate when the problem is strongly convex. Moreover 
we also develop an approximate version of the algorithm  named SVRPDA-II 
which further reduces the memory requirement. Finally  we evaluate our proposed
algorithms on several real-world benchmarks  and experimental results show that
the proposed algorithms signiﬁcantly outperform existing techniques.

1

Introduction

In this paper  we consider the following regularized empirical composition optimization problem:

nX−1(cid:88)

i=0

φi

min

θ

1
nX

(cid:18) 1

nYi−1(cid:88)

nYi

j=0

(cid:19)

fθ(xi  yij)

+ g(θ) 

(1)

where (xi  yij) ∈ Rmx × Rmy is the (i  j)-th data sample  fθ : Rmx × Rmy → R(cid:96) is a function
parameterized by θ ∈ Rd  φi : R(cid:96) → R+ is a convex merit function  which measures a certain loss of
the parametric function fθ  and g(θ) is a µ-strongly convex regularization term.
Problems of the form (1) widely appear in many machine learning applications such as reinforcement
learning [5  3  2  13]  unsupervised sequence classiﬁcation [12  21] and risk-averse learning [15  18 
9  10  19] — see our detailed discussion in Section 2. Note that the cost function (1) has an empirical
average (over xi) outside the (nonlinear) merit function φi(·) and an empirical average (over yij)
inside the merit function  which makes it different from the empirical risk minimization problems
that are common in machine learning [17]. Problem (1) can be understood as a generalized version
of the one considered in [9  10].3 In these prior works  yij and nYi are assumed to be independent of
∗Department of Electrical and Computer Engineering  University of Florida  Gainesville  USA. Email:
†Tencent AI Lab  Bellevue  WA  USA. Email: jianshuchen@tencent.com.
3In addition to the term in (2)  the cost function in [10] also has another convex regularization term.

adithyamdevraj@ufl.edu. The work was done during an internship at Tencent AI Lab  Bellevue  WA.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

i and fθ is only a function of yj so that problem (1) can be reduced to the following special case:

nX−1(cid:88)

i=0

φi

(cid:18) 1

nY −1(cid:88)

nY

j=0

(cid:19)

fθ(yj)

.

min

θ

1
nX

(2)

Our more general problem formulation (1) encompasses wider applications (see Section 2). Fur-
thermore  different from [2  19  18]  we focus on the ﬁnite sample setting  where we have empirical
averages (instead of expectations) in (1). As we shall see below  the ﬁnite-sum structures allows us to
develop efﬁcient stochastic gradient methods that converges at linear rate.
While problem (1) is important in many machine learning applications  there are several key chal-
lenges in solving it efﬁciently. First  the number of samples (i.e.  nX and nYi) could be extremely
large: they could be larger than one million or even one billion. Therefore  it is unrealistic to use
batch gradient descent algorithm to solve the problem  which requires going over all the data samples
at each gradient update step. Moreover  since there is an empirical average inside the nonlinear merit
function φi(·)  it is not possible to directly apply the classical stochastic gradient descent (SGD) algo-
rithm. This is because sampling from both empirical averages outside and inside φi(·) simultaneously
would make the stochastic gradients intrinsically biased (see Appendix A for a discussion).
To address these challenges  in this paper  we ﬁrst reformulate the original problem (1) into an
equivalent saddle point problem (i.e.  min-max problem)  which brings out all the empirical averages
inside φi(·) and exhibits useful dual decomposition and ﬁnite-sum structures (Section 3.1). To fully
exploit these properties  we develop a stochastic primal-dual algorithm that alternates between a dual
step of stochastic variance reduced coordinate ascent and a primal step of stochastic variance reduced
gradient descent (Section 3.2). In particular  we develop a novel variance reduced stochastic gradient
estimator for the primal step  which achieves better variance reduction with low complexity (Section
3.3). We derive the convergence rate  the ﬁnite-time complexity bound  and the storage complexity of
our proposed algorithm (Section 4). In particular  it is shown that the proposed algorithms converge at
a linear rate when the problem is strongly convex. Moreover  we also develop an approximate version
of the algorithm that further reduces the storage complexity without much performance degradation
in experiments. We evaluate the performance of our algorithms on several real-world benchmarks 
where the experimental results show that they signiﬁcantly outperform existing methods (Section 5).
Finally  we discuss related works in Section 6 and conclude our paper in Section 7.

2 Motivation and Applications

To motivate our composition optimization problem (1)  we discuss several important machine learning
applications where cost functions of the form (1) arise naturally.

Unsupervised sequence classiﬁcation: Developing algorithms that can learn classiﬁers from unla-
beled data could beneﬁt many machine learning systems  which could save a huge amount of human
labeling costs. In [12  21]  the authors proposed such unsupervised learning algorithms by exploiting
the sequential output structures. The developed algorithms are applied to optical character recognition
(OCR) problems and automatic speech recognition (ASR) problems. In these works  the learning
algorithms seek to learn a sequence classiﬁer by optimizing the empirical output distribution match
(Empirical-ODM) cost  which is in the following form (written in our notation):

(cid:26)

−nX−1(cid:88)

i=0

min

θ

(cid:18) 1

nY −1(cid:88)

nY

j=0

(cid:19)(cid:27)

pLM(xi) log

fθ(xi  yj)

 

(3)

where pLM is a known language model (LM) that describes the distribution of output sequence (e.g. 
xi represents different n-grams)  and fθ is a functional of the sequence classiﬁer to be learned  with
θ being its model parameter vector. The key idea is to learn the classiﬁer so that its predicted output
n-gram distribution is close to the prior n-gram distribution pLM (see [12  21] for more details).
The cost function (3) can be viewed as a special case of (1) by setting nYi = nY   yij = yj and
φi(u) = −pLM (xi) log(u). Note that the formulation (2) cannot be directly used here  because of
the dependency of the function fθ on both xi and yj.

Risk-averse learning: Another application where (1) arises naturally is the risk-averse learning
problem  which is common in ﬁnance [15  18  9  10  19  20]. Let xi ∈ Rd be a vector consisting of

2

the rewards from d assets at the i-th instance  where 0 ≤ i ≤ n − 1. The objective in risk-averse
learning is to ﬁnd the optimal weights of the d assets so that the average returns are maximized while
the risk is minimized. It could be formulated as the following optimization problem:

n−1(cid:88)

i=0

(cid:18)

n−1(cid:88)

i=0

(cid:19)2

n−1(cid:88)

j=0

− 1
n

min

θ

(cid:104)xi  θ(cid:105)+

1
n

(cid:104)xi  θ(cid:105)− 1
n

(cid:104)xj  θ(cid:105)

 

(4)

where θ ∈ Rd denotes the weight vector. The objective function in (4) seeks a tradeoff between the
mean (the ﬁrst term) and the variance (the second term). It can be understood as a special case of (2)
(which is a further special case of (1)) by making the following identiﬁcations:
nX = nY = n  yi≡ xi  fθ(yj) = [θT  −(cid:104)yj  θ(cid:105)]T  φi(u) = ((cid:104)xi  u0:d−1(cid:105)+ud)2−(cid:104)xi  u0:d−1(cid:105) 
(5)
where u0:d−1 denotes the subvector constructed from the ﬁrst d elements of u  and ud denotes the
d-th element. An alternative yet simpler way of dealing with (4) is to treat the second term in (4) as a
special case of (1) by setting

nX = nYi = n  yij ≡ xj  fθ(xi  yij) = (cid:104)xi − yij  θ(cid:105)  φi(u) = u2  u ∈ R.

(6)
In addition  we observe that the ﬁrst term in (4) is in standard empirical risk minimization form 
which can be dealt with in a straightforward manner. This second formulation leads to algorithms
with lower complexity due to the lower dimension of the functions: (cid:96) = 1 instead of (cid:96) = d + 1 in the
ﬁrst formulation. Therefore  we will adopt this formulation in our experiment section (Section 5).

Other applications: Cost functions of the form (1) also appear in reinforcement learning [5  2  3]
and other applications [18]. In Appendix D  we demonstrate its applications in policy evaluation.

3 Algorithms

3.1 Saddle point formulation
Recall from (1) that there is an empirical average inside each (nonlinear) merit function φi(·)  which
prevents the direct application of stochastic gradient descent to (1) due to the inherent bias (see
Appendix A for more discussions). Nevertheless  we will show that minimizing the original cost
function (1) can be transformed into an equivalent saddle point problem  which brings out all the
empirical averages inside φi(·). In what follows  we will use the machinery of convex conjugate
functions [14]. For a function ψ : R(cid:96) → R  its convex conjugate function ψ∗ : R(cid:96) → R is deﬁned as
ψ∗(y) = supx ∈ R(cid:96)((cid:104)x  y(cid:105)− ψ(x)). Under certain mild conditions on ψ(x) [14]  one can also express
ψ(x) as a functional of its conjugate function: ψ(x) = supy ∈ R(cid:96) ((cid:104)x  y(cid:105)− ψ∗(y)). Let φ∗
i (wi) denote
the conjugate function of φi(u). Then  we can express φi(u) as

φi(u) = sup
wi∈R(cid:96)

((cid:104)u  wi(cid:105) − φ∗

i (wi)) 

(7)

where wi is the corresponding dual variable. Substituting (7) into the original minimization problem
(1)  we obtain its equivalent min-max problem as:

nX−1(cid:88)

(cid:104)(cid:68) 1

nYi−1(cid:88)

i=0

nYi

j=0

(cid:69) − φ∗

(cid:105)

(cid:27)

fθ(xi  yij)  wi

i (wi)

+ g(θ)

 

(8)

min

θ

max

w

L(θ  w) + g(θ) (cid:44) 1
nX

(cid:26)

where w(cid:44){w0  . . .   wnX−1}  is a collection of all dual variables. We note that the transformation of
the original problem (1) into (8) brings out all the empirical averages that are present inside φi(·).
This new formulation allows us to develop stochastic variance reduced algorithms below.

3.2 Stochastic variance reduced primal-dual algorithm

One common solution for the min-max problem (8) is to alternate between the step of minimization
(with respect to the primal variable θ) and the step of maximization (with respect to the dual variable
w). However  such an approach generally suffers from high computation complexity because each
minimization/maximization step requires a summation over many components and requires a full

3

pass over all the data samples. The complexity of such a batch algorithm would be prohibitively
high when the number of data samples (i.e.  nX and nYi) is large (e.g.  they could be larger than one
million or even one billion in applications like unsupervised speech recognition [21]). On the other
hand  problem (8) indeed has rich structures that we can exploit to develop more efﬁcient solutions.
To this end  we make the following observations. First  expression (8) implies that when θ is ﬁxed  the
maximization over the dual variable w can be decoupled into a total of nX individual maximizations
over different wi’s. Second  the objective function in each individual maximization (with respect to
wi) contains a ﬁnite-sum structure over j. Third  by (8)  for a ﬁxed w  the minimization with respect
to the primal variable θ is also performed over an objective function with a ﬁnite-sum structure. Based
on these observations  we will develop an efﬁcient stochastic variance reduced primal-dual algorithm
(named SVRPDA-I). It alternates between (i) a dual step of stochastic variance reduced coordinate
ascent and (ii) a primal step of stochastic variance reduced gradient descent. The full algorithm is
summarized in Algorithm 1  with its key ideas explained below.

Dual step: stochastic variance reduced coordinate ascent. To exploit the decoupled dual maxi-
mization over w in (8)  we can randomly sample an index i  and update wi according to:

w(k)

i = arg min
wi

fθ(k−1)(xi  yij)  wi

+ φ∗

i (wi) +

1

2αw

(cid:107)wi − w(k−1)

i

while keeping all other wj’s (j (cid:54)= i) unchanged  where αw denotes a step-size. Note that each step
of recursion (9) still requires a summation over nYi components. To further reduce the complexity 
we approximate the sum over j by a variance reduced stochastic estimator deﬁned in (12) (to be
discussed in Section 3.3). The dual step in our algorithm is summarized in (13)  where we assume
that the function φ∗
i (wi) is in a simple form so that the argmin could be solved in closed-form. Note
that we ﬂip the sign of the objective function to change maximization to minimization and apply
coordinate descent. We will still refer to the dual step as “coordinate ascent” (instead of descent).

Primal step: stochastic variance reduced gradient descent We now consider the minimization
in (8) with respect to θ when w is ﬁxed. The gradient descent step for minimizing L(θ  w) is given by

(cid:69)

(cid:27)

(cid:110) −(cid:68) 1

nYi−1(cid:88)

nYi

j=0

(cid:69)

(cid:107)2(cid:111)

 

(9)

(cid:26)(cid:68) nX−1(cid:88)

nYi−1(cid:88)

i=0

j=0

θ(k) = arg min

θ

1

nX nYi

f(cid:48)
θ(k−1)(xi  yij)w(k)

i

  θ

+

1
2αθ

(cid:107)θ − θ(k−1)(cid:107)2

 

(10)

where αθ denotes a step-size. It is easy to see that the update equation (10) has high complexity  it
θ(· ·) at every data sample. To reduce the complexity 
requires evaluating and averaging the gradient f(cid:48)
we use a variance reduced gradient estimator  deﬁned in (15)  to approximate the sums in (10) (to be
discussed in Section 3.3). The primal step in our algorithm is summarized in (16) in Algorithm 1.

3.3 Low-complexity stochastic variance reduced estimators

We now proceed to explain the design of the variance reduced gradient estimators in both the dual
and the primal updates. The main idea is inspired by the stochastic variance reduced gradient (SVRG)
algorithm [7]. Speciﬁcally  for a vector-valued function h(θ) = 1
i=0 hi(θ)  we can construct its
n
SVRG estimator δk at each iteration step k by using the following expression:

(cid:80)n−1

δk = hik (θ) − hik (˜θ) + h(˜θ) 

(17)
where ik is a randomly sampled index from {0  . . .   n − 1}  and ˜θ is a reference variable that is
updated periodically (to be explained below). The ﬁrst term hi(θ) in (17) is an unbiased estimator
of h(θ) and is generally known as the stochastic gradient when h(θ) is the gradient of a certain
cost function. The last two terms in (17) construct a control variate that has zero mean and is
negatively correlated with hi(θ)  which keeps δk unbiased while signiﬁcantly reducing its variance.
The reference variable ˜θ is usually set to be a delayed version of θ: for example  after every M
updates of θ  it can be reset to the most recent iterate of θ. Note that there is a trade-off in the choice
of M: a smaller M further reduces the variance of δk since ˜θ will be closer to θ and the ﬁrst two
terms in (17) cancel more with each other; on the other hand  it will also require more frequent
evaluations of the costly batch term h(˜θ)  which has a complexity of O(n).

4

Algorithm 1 SVRPDA-I
1: Inputs: data {(xi  yij) : 0≤ i < nX   0≤ j < nYi}; step-sizes αθ and αw; # inner iterations M.
2: Initialization: ˜θ0 ∈ Rd and ˜w0 ∈ R(cid:96)nX .
3: for s = 1  2  . . . do
4:

Set ˜θ = ˜θs−1  θ(0) = ˜θ  ˜w = ˜ws−1  w(0) = ˜ws−1  and compute the batch quantities (for each 0≤ i < nX):

−1(cid:88)

nYi

nX−1(cid:88)

−1(cid:88)

nYi

U0 =

f(cid:48)
˜θ(xi  yij)w(0)

i

nX nYi

  f i(˜θ) (cid:44)

f ˜θ(xi  yij)

(cid:48)
i(˜θ) =

  f

nYi

i=0

j=0
for k = 1 to M do
Randomly sample ik ∈ {0  . . .   nX−1} and then jk ∈ {0  . . .   nYik
Compute the stochastic variance reduced gradient for dual update:

j=0

k = fθ(k−1) (xik   yikjk ) − f ˜θ(xik   yikjk ) + f ik
δw
Update the dual variables:

k   wi(cid:105) + φ

∗
i (wi) +

1

2αw

(cid:107)wi − w(k−1)

i

(cid:104) − (cid:104)δw

arg min

w(k−1)

wi

i

w(k)

i =

−1(cid:88)

nYi

f(cid:48)
˜θ(xi  yij)

j=0

nYi

.

(11)

−1} at uniform.

(˜θ).

(cid:107)2(cid:105)

(12)

(13)

if i = ik
if i (cid:54)= ik

.

5:
6:
7:

8:

9:

10:

Update Uk (primal batch gradient at ˜θ and w(k)) according to the following recursion:

Uk = Uk−1 +

(cid:48)
ik

f

1
nX

(cid:1).

(˜θ)(cid:0)w(k)

ik

− w(k−1)
k ∈ {0  . . .   nYi(cid:48)

ik

(14)
− 1}  independent of ik and jk 

Randomly sample i(cid:48)
and compute the stochastic variance reduced gradient for primal update:

k ∈ {0  . . .   nX − 1} and then j(cid:48)

k

δθ
k = f

11:

Update the primal variable:

(cid:48)
θ(k−1) (xi(cid:48)

k

)w(k)
i(cid:48)

k

− f

(cid:48)
˜θ(xi(cid:48)

k

kj(cid:48)

k

  yi(cid:48)

(cid:104)(cid:104)δθ

  yi(cid:48)

kj(cid:48)

k

)w(k)
i(cid:48)

+ Uk.

k

(cid:107)θ − θ(k−1)(cid:107)2(cid:105)

.

(15)

(16)

θ(k) = arg min

θ

k  θ(cid:105) + g(θ) +

1
2αθ

end for

12:
13: Option I: Set ˜ws = w(M ) and ˜θs = θ(M ).
14: Option II: Set ˜ws = w(M ) and ˜θs = θ(t) for randomly sampled t ∈ {0  . . .   M−1}.
15: end for
16: Output: ˜θs at the last outer-loop iteration.

Based on (17)  we develop two stochastic variance reduced estimators  (12) and (15)  to approximate
the ﬁnite-sums in (9) and (10)  respectively. The dual gradient estimator δw
k in (12) is constructed in a
standard manner using (17)  where the reference variable ˜θ is a delayed version of θ(k)4. On the other
k in (15) is constructed by using reference variables (˜θ  w(k));
hand  the primal gradient estimator δθ
that is  we uses the most recent w(k) as the dual reference variable  without any delay. As discussed
earlier  such a choice leads to a smaller variance in the stochastic estimator δk
θ at a potentially higher
computation cost (from more frequent evaluation of the batch term). Nevertheless  we are able to
show that  with the dual coordinate ascent structure in our algorithm  the batch term Uk in (15)  which
is the summation in (10) evaluated at (˜θ  w(k))  can be computed efﬁciently. To see this  note that 
after each dual update step in (13)  only one term inside this summation in (10)  has been changed 
i.e.  the one associated with i = ik. Therefore  we can correct Uk for this term by using recursion
(14)  which only requires an extra O(d(cid:96))-complexity per step (same complexity as (15)).

(cid:48)
i(˜θ) in (11)  which is
Note that SVRPDA-I (Algorithm 1) requires to compute and store all the f
O(nX d(cid:96))-complexity in storage and could be expensive in some applications. To avoid the cost 
we develop a variant of Algorithm 1  named as SVRPDA-II (see Algorithm 1 in the supplementary
material)  by approximating f ik
k is another randomly sampled
index from {0  . . .   nYi − 1}  independent of all other indexes. By doing this  we can signiﬁcantly

(˜θ) in (14) with f(cid:48)
(xik   yikj(cid:48)(cid:48)
˜θ

)  where j(cid:48)(cid:48)

k

4As in [7]  we also consider Option II wherein ˜θ is randomly chosen from the previous M θ(k)’s.

5

Table 1: The total complexities of different stochastic composition optimization algorithms. For C-
SAGA  α = 2/3 in the minibatch setting and α = 1 when batch-size=1. In the bound for ASCVRG 
the dependency on κ has been dropped since it was not reported in [10].

Methods

SVRPDA-I (Ours)

Comp-SVRG [9]

MSPBE-SVRG/SAGA [5]

ASCVRG [10]

General: problem (1)
Special: problem (2)
Special: (2) & nX = 1

(nX nY +nX κ)ln 1

(nX+nY +nX κ)ln 1

(nY +κ) ln 1


 (nX+nY +κ3)ln 1
(nY +κ3) ln 1




 (nX+nY +(nX+nY )ακ)ln 1



(nY +nα

Y κ) ln 1


(nY +κ2) ln 1






(nX+nY )ln 1
 + 1
3

nY ln 1

+ 1
3

C-SAGA [22]



reduce the memory requirement from O(nX d(cid:96)) in SVRPDA-I to O(d + nX (cid:96)) in SVRPDA-II (see
Section 4.2). In addition  experimental results in Section 5 will show that such an approximation only
cause slight performance loss compared to that of SVRPDA-I algorithm.

4 Theoretical Analysis

4.1 Computation complexity

We now perform convergence analysis for the SVRPDA-I algorithm and also derive their complexities
in computation and storage. To begin with  we ﬁrst introduce the following assumptions.
Assumption 4.1. The function g(θ) is µ-strongly convex in θ  and each φi is 1/γ-smooth.
Assumption 4.2. The merit functions φi(u) are Lipschitz with a uniform constant Bw:

|φi(u) − φi(u(cid:48))| ≤ Bw(cid:107)u − u(cid:48)(cid:107) 

∀u  u(cid:48); ∀i = 0  . . .   nX − 1.

Assumption 4.3. fθ(xi  yij) is Bθ-smooth in θ  and has bounded gradients with constant Bf :
∀θ  θ1  θ2  ∀i  j.

(xi  yij)(cid:107) ≤ Bθ(cid:107)θ1 − θ2(cid:107) 

θ(xi  yij)(cid:107) ≤ Bf  

(xi  yij) − f(cid:48)

(cid:107)f(cid:48)

(cid:107)f(cid:48)

θ1

θ2

Assumption 4.4. For each given w in its domain  the function L(θ  w) deﬁned in (8) is convex in θ:

L(θ1  w) − L(θ2  w) ≥ (cid:104)L(cid:48)

θ(θ2  w)  θ1 − θ2(cid:105) 

∀θ1  θ2.

The above assumptions are commonly used in existing compositional optimization works [9  10  18 
19  22]. Based on these assumptions  we establish the non-asymptotic error bounds for SVRPDA-
I (using either Option I or Option II in Algorithm 1). The main results are summarized in the
following theorems  and their proofs can be found in Appendix E.
Theorem 4.5. Suppose Assumptions 4.1–4.4 hold. If in Algorithm 1 (with Option I) we choose

αθ  M =(cid:6)78.8nX κ+1.3nX +1.3(cid:7)

1

αθ =

nX µ(64κ + 1)

  αw =

nX µ

γ

64κ+3

where (cid:100)x(cid:101) denotes the roundup operation and κ = B2
Ps := E(cid:107)˜θs − θ∗(cid:107)2 + γ
µ ·
overall computational cost (in number of oracle calls5) for reaching Ps ≤  is upper bounded by

θ /µ2  then the Lyapunov function
64nX κ+nX +1 E(cid:107) ˜ws − w∗(cid:107)2 satisﬁes Ps ≤ (3/4)sP0. Furthermore  the

f /γµ + B2

O(cid:0)(nX nY + nX κ + nX ) ln(1/)(cid:1).

where  with a slight abuse of notation  nY is deﬁned as nY = (nY0 + ··· + nYnX −1 )/nX.
Theorem 4.6. Suppose Assumptions 4.1–4.4 hold. If in Algorithm 1 (with Option II) we choose

wB2

(18)

(cid:17)−1

(cid:18) 10
αθµ
nX µ E(cid:107) ˜ws−w∗(cid:107)2 ≤ (5/8)sP0. Furthermore  let κ =

80B2
wB2
θ
µ

  M = max

  αw =

40B2
f

µ

 

2nX
αwγ
γµ + B2

(cid:19)

  4nX

 

then Ps := E(cid:107)˜θs−θ∗(cid:107)2 + γ
. Then 
the overall computational cost (in number of oracle calls) for reaching Ps ≤  is upper bounded by
(19)

O(cid:0)(nX nY + nX κ + nX ) ln(1/)(cid:1).

wB2
µ2

B2
f

θ

(cid:16) 25B2

f

γ

αθ =

+10BθBw +

The above theorems show that the Lyapunov function Ps for SVRPDA-I converges to zero at a linear
rate when either Option I or II is used. Since E(cid:107)˜θs − θ∗(cid:107)2 ≤ Ps  they imply that the computational
cost (in number of oracle calls) for reaching E(cid:107)˜θs − θ∗(cid:107)2 ≤  is also upper bounded by (18) and (19).

5One oracle call is deﬁned as querying fθ  f(cid:48)

θ  or φi(u) for any 0 ≤ i < n and u ∈ R(cid:96).

6

Table 2: The storage complexity of SVRPDA-I and SVRPDA-II.

U0

˜θ

(cid:48)
i}
Methods
δw
k
SVRPDA-I O(d) O(nX (cid:96)) O(nX d(cid:96)) O(d) O(d) O(nX (cid:96)) O(d) O((cid:96))
SVRPDA-II O(d) O(nX (cid:96)) 

O(nX d(cid:96))
O(d) O(d) O(nX (cid:96)) O(d) O((cid:96)) O(d+nX (cid:96))

{w(k)
i }

{f i}

Total

θ(k)

{f

δθ
k

Comparison with existing composition optimization algorithms Table 1 summarizes the com-
plexity bounds for our SVRPDA-I algorithm and compares them with existing stochastic composition
optimization algorithms. First  to our best knowledge  none of the existing methods consider the
general objective function (1) as we did. Instead  they consider its special case (2)  and even in this
special case  our algorithm still has better (or comparable) complexity bound than other methods. For
example  our bound is better than that of [9] since κ2 > nX generally holds  and it is better than that
of ASCVRG  which does not achieve linear convergence rate (as no strong convexity is assumed).
In addition  our method has better complexity than C-SAGA algorithm when nX = 1 (regardless
of mini-batch size in C-SAGA)  and it is better than C-SAGA for (2) when the mini-batch size is
1.6 However  since we have not derived our bound for mini-batch setting  it is unclear which one
is better in this case  and is an interesting topic for future work. One notable fact from Table 1 is
that in this special case (2)  the complexity of SVRPDA-I is reduced from O((nX nY +nX κ) ln 1
 ) to
 ). This is because the complexity for evaluating the batch quantities in (11)
O((nX +nY +nX κ) ln 1
(Algorithm 1) can be reduced from O(nX nY ) in the general case (1) to O(nX + nY ) in the special
case (2). To see this  note that fθ and nYi = nY become independent of i in (2) and (11)  meaning
that we can factor U0 in (11) as U0 = 1
  where the two sums can be
evaluated independently with complexity O(nY ) and O(nX )  respectively. The other two quantities
in (11) need only O(nY ) due to their independence of i. Second  we consider the further special case
of (2) with nX = 1  which simpliﬁes the objective function (1) so that there is no empirical average
outside φi(·). This takes the form of the unsupervised learning objective function that appears in [12].
Note that our results O((nY +κ) log 1
 ) enjoys a linear convergence rate (i.e.  log-dependency on )
due to the variance reduction technique. In contrast  stochastic primal-dual gradient (SPDG) method
in [12]  which does not use variance reduction  can only have sublinear convergence rate (i.e.  O( 1
 )).
Relation to SPDC [23] Lastly  we consider the case where nYi = 1 for all 1 ≤ i ≤ nX and fθ is a
linear function in θ. This simpliﬁes (1) to the problem considered in [23]  known as the regularized
empirical risk minimization of linear predictors. It has applications in support vector machines 
regularized logistic regression  and more  depending on how the merit function φi is deﬁned. In this
special case  the overall complexity for SVRPDA-I becomes (see Appendix F):

(cid:80)nY −1
j=0 f(cid:48)

(yj)(cid:80)nX

i=0 w(0)

nX nY

˜θ

i

√

(20)
f /µγ. In comparison  the authors in [23] propose a stochastic
where the condition number κ = B2
primal dual coordinate (SPDC) algorithm for this special case and prove an overall complexity of

(cid:1)(cid:1) to achieve an -error solution. It is interesting to note that the complexity

nX κ(cid:1) ln(cid:0) 1

O(cid:0)(cid:0)nX +

result in (20) and the complexity result in [23] only differ in their dependency on κ. This difference
is most likely due to the acceleration technique that is employed in the primal update of the SPDC
algorithm. We conjecture that the dependency on the condition number of SVRPDA-I can be further
improved using a similar acceleration technique.



O(cid:0)(nX + κ) ln(1/)(cid:1)  

4.2 Storage complexity

We now brieﬂy discuss and compare the storage complexities of both SVRPDA-I and SVRPDA-II. In
Table 2  we report the itemized and total storage complexities for both algorithms  which shows that
SVRPDA-II signiﬁcantly reduces the memory footprint. We also observe that the batch quantities
(cid:48)
i(˜θ)  dominates the storage complexity in SVRPDA-I. On the other hand  the
in (11)  especially f
memory usage in SVRPDA-II is more uniformly distributed over different quantities. Furthermore 
although the total complexity of SVRPDA-II  O(d + nX (cid:96))  grows with the number of samples nX 
the nX (cid:96) term is relatively small because the dimension (cid:96) is small in many practical problems (e.g. 
(cid:96) = 1 in (3) and (4)). This is similar to the storage requirement in SPDC [23] and SAGA [4].

6In Appendix D  we also show that our algorithms outperform C-SAGA in experiments.

7

Figure 1: Performance of different algorithms on the risk-averse learning for portfolio management
optimization problem. The performance is measured in terms of the number of oracle calls required
to achieve a certain objective gap.

5 Experiments

In this section we consider the problem of risk-averse learning for portfolio management optimization
[9  10]  introduced in Section 2.7 Speciﬁcally  we want to solve the optimization problem (4) for a
given set of reward vectors {xi ∈ Rd : 0 ≤ i ≤ n − 1}. As we discussed in Section 2  we adopt
the alternative formulation (6) for the second term so that it becomes a special case of our general
problem (1). Then  we rewrite the cost function into a min-max problem by following the argument in
Section 3.1 and apply our SVRPDA-I and SVRPDA-II algorithms (see Appendix C.1 for the details).
We evaluate our algorithms on 18 real-world US Research Returns datasets obtained from the
Center for Research in Security Prices (CRSP) website8  with the same setup as in [10]. In each
of these datasets  we have d = 25 and n = 7240. We compare the performance of our proposed
SVRPDA-I and SVRPDA-II algorithms9 with the following state-of-the art algorithms designed
to solve composition optimization problems: (i) Compositional-SVRG-1 (Algorithm 2 of [9])  (ii)
Compositional-SVRG-2 (Algorithm 3 of [9])  (iii) Full batch gradient descent  and (iv) ASCVRG
algorithm [10]. For the compositional-SVRG algorithms  we follow [9] to formulate it as a special
case of the form (2) by using the identiﬁcation (5). Note that we cannot use the identiﬁcation (6) for
the compositional SVRG algorithms because it will lead to the more general formulation (1) with fθ
depending on both xi and yij ≡ xj. For further details  the reader is referred to [9].
As in previous works  we compare different algorithms based on the number of oracle calls required
to achieve a certain objective gap (the difference between the objective function evaluated at the
current iterate and at the optimal parameters). One oracle call is deﬁned as accessing the function fθ 
θ  or φi(u) for any 0 ≤ i < n and u ∈ R(cid:96). The results are shown in Figure 1  which
its derivative f(cid:48)
shows that our proposed algorithms signiﬁcantly outperform the baseline methods on all datasets. In
addition  we also observe that SVRPDA-II also converges at a linear rate  and the performance loss
caused by the approximation is relatively small compared to SVRPDA-I.

7Additional experiments on the application to policy evaluation in MDPs can be found in Appendix D.
8The processed data in the form of .mat ﬁle was obtained from https://github.com/tyDLin/SCVRG
9The choice of the hyper-parameters can be found in Appendix C.2  and the code will be released publicly.

8

OP datasetsME datasetsINV datasets6 Related Works

Composition optimization have attracted signiﬁcant attention in optimization literature. The stochastic
version of the problem (2)  where the empirical averages are replaced by expectations  is studied
in [18]. The authors propose a two-timescale stochastic approximation algorithm known as SCGD 
and establish sublinear convergence rates. In [19]  the authors propose the ASC-PG algorithm by
using a proximal gradient method to deal with nonsmooth regularizations. The works that are more
closely related to our setting are [9] and [10]  which consider a ﬁnite-sum minimization problem (2)
(a special case of our general formulation (1)). In [9]  the authors propose the compositional-SVRG
methods  which combine SCGD with the SVRG technique from [7] and obtain linear convergence
rates. In [10]  the authors propose the ASCVRG algorithms that extends to convex but non-smooth
objectives. Recently  the authors in [22] propose a C-SAGA algorithm to solve the special case of (2)
with nX = 1  and extend to general nX. Different from these works  we take an efﬁcient primal-dual
approach that fully exploits the dual decomposition and the ﬁnite-sum structures.
On the other hand  problems similar to (1) (and its stochastic versions) are also examined in different
speciﬁc machine learning problems. [16] considers the minimization of the mean square projected
Bellman error (MSPBE) for policy evaluation  which has an expectation inside a quadratic loss.
The authors propose a two-timescale stochastic approximation algorithm  GTD2  and establish its
asymptotic convergence. [11] and [13] independently showed that the GTD2 is a stochastic gradient
method for solving an equivalent saddle-point problem. In [2] and [3]  the authors derived saddle-
point formulations for two other variants of costs (MSBE and MSCBE) in the policy evaluation and
the control settings  and develop their stochastic primal-dual algorithms. All these works consider
the stochastic version of the composition optimization and the proposed algorithms have sublinear
convergence rates. In [5]  different variance reduction methods are developed to solve the ﬁnite-sum
version of MSPBE and achieve linear rate even without strongly convex regularization. Then the
authors in [6] extends this linear convergence results to the general convex-concave problem with
linear coupling and without strong convexity. Besides  problem of the form (1) was also studied in
the context of unsupervised learning [12  21] in the stochastic setting (with expectations in (1)).
Finally  our work is inspired by the stochastic variance reduction techniques in optimization [8  7  4 
1  23]  which considers the minimization of a cost that is a ﬁnite-sum of many component functions.
Different versions of variance reduced stochastic gradients are constructed in these works to achieve
linear convergence rate. In particular  our variance reduced stochastic estimators are constructed
based on the idea of SVRG [7] with a novel design of the control variates. Our work is also related to
the SPDC algorithm [23]  which also integrates dual coordinate ascent with variance reduced primal
gradient. However  our work is different from SPDC in the following aspects. First  we consider a
more general composition optimization problem (1) while SPDC focuses on regularized empirical
risk minimization with linear predictors  i.e.  nYi ≡ 1 and fθ is linear in θ. Second  because of the
composition structures in the problem  our algorithms also needs SVRG in the dual coordinate ascent
update  while SPDC does not. Third  the primal update in SPDC is speciﬁcally designed for linear
predictors. In contrast  our work is not restricted to that by using a novel variance reduced gradient.

7 Conclusions and Future Work

We developed a stochastic primal-dual algorithms  SVRPDA-I to efﬁciently solve the empirical
composition optimization problem. This is achieved by fully exploiting the rich structures inherent in
the reformulated min-max problem  including the dual decomposition and the ﬁnite-sum structures.
It alternates between (i) a dual step of stochastic variance reduced coordinate ascent and (ii) a primal
step of stochastic variance reduced gradient descent. In particular  we proposed a novel variance
reduced gradient for the primal update  which achieves better variance reduction with low complexity.
We derive a non-asymptotic bound for the error sequence and show that it converges at a linear
rate when the problem is strongly convex. Moreover  we also developed an approximate version
of the algorithm named SVRPDA-II  which further reduces the storage complexity. Experimental
results on several real-world benchmarks showed that both SVRPDA-I and SVRPDA-II signiﬁcantly
outperform existing techniques on all these tasks  and the approximation in SVRPDA-II only caused a
slight performance loss. Future extensions of our work include the theoretical analysis of SVRPDA-II 
the generalization of our algorithms to Bregman divergences  and applying it to large-scale machine
learning problems with non-convex cost functions (e.g.  unsupervised sequence classiﬁcations).

9

References
[1] P. Balamurugan and F. Bach. Stochastic variance reduction methods for saddle-point problems.

In Advances in Neural Information Processing Systems  pages 1416–1424  2016.

[2] B. Dai  N. He  Y. Pan  B. Boots  and L. Song. Learning from conditional distributions via dual

embeddings. In Artiﬁcial Intelligence and Statistics  pages 1458–1467  2017.

[3] B. Dai  A. Shaw  L. Li  L. Xiao  N. He  Z. Liu  J. Chen  and L. Song. SBEED: Convergent rein-
forcement learning with nonlinear function approximation. In Proc. International Conference
on Machine Learning  pages 1133–1142  2018.

[4] A. Defazio  F. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Advances in neural information
processing systems  pages 1646–1654  2014.

[5] S. S. Du  J. Chen  L. Li  L. Xiao  and D. Zhou. Stochastic variance reduction methods for policy
evaluation. In Proc. International Conference on Machine Learning  pages 1049–1058  2017.

[6] S. S. Du and W. Hu. Linear convergence of the primal-dual gradient method for convex-concave
saddle point problems without strong convexity. In Proc. International Conference on Artiﬁcial
Intelligence and Statistics  pages 196–205  2019.

[7] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[8] N. Le Roux  M. W. Schmidt  F. R. Bach  et al. A stochastic gradient method with an exponential
convergence rate for ﬁnite training sets. In Advances in Neural Information Processing Systems 
pages 2672–2680  2012.

[9] X. Lian  M. Wang  and J. Liu. Finite-sum composition optimization via variance reduced
gradient descent. In Proc. International Conference on Artiﬁcial Intelligence and Statistics 
pages 1159–1167  2017.

[10] T. Lin  C. Fan  M. Wang  and M. I. Jordan. Improved oracle complexity for stochastic composi-

tional variance reduced gradient. arXiv preprint arXiv:1806.00458  2018.

[11] B. Liu  J. Liu  M. Ghavamzadeh  S. Mahadevan  and M. Petrik. Finite-sample analysis of
proximal gradient td algorithms. In Proc. Conference on Uncertainty in Artiﬁcial Intelligence 
pages 504–513  2015.

[12] Y. Liu  J. Chen  and L. Deng. Unsupervised sequence classiﬁcation using sequential output

statistics. In Advances in Neural Information Processing Systems  pages 3550–3559  2017.

[13] S. V. Macua  J. Chen  S. Zazo  and A. H. Sayed. Distributed policy evaluation under multiple

behavior strategies. IEEE Transactions on Automatic Control  60(5):1260–1274  2015.

[14] R. T. Rockafellar. Convex analysis. Princeton university press  2015.

[15] A. Ruszczy´nski and A. Shapiro. Optimization of risk measures. In Probabilistic and randomized

methods for design under uncertainty  pages 119–157. Springer  2006.

[16] R. S. Sutton  H. R. Maei  D. Precup  S. Bhatnagar  D. Silver  C. Szepesv´ari  and E. Wiewiora.
Fast gradient-descent methods for temporal-difference learning with linear function approxima-
tion. In Proc. International Conference on Machine Learning  pages 993–1000  2009.

[17] V. Vapnik. Statistical learning theory. 1998  volume 3. Wiley  New York  1998.

[18] M. Wang  E. X. Fang  and H. Liu. Stochastic compositional gradient descent: algorithms for
minimizing compositions of expected-value functions. Mathematical Programming  161(1-
2):419–449  2017.

[19] M. Wang  J. Liu  and E. Fang. Accelerating stochastic composition optimization. In Advances

in Neural Information Processing Systems  pages 1714–1722  2016.

10

[20] T. Xie  B. Liu  Y. Xu  M. Ghavamzadeh  Y. Chow  D. Lyu  and D. Yoon. A block coordinate
ascent algorithm for mean-variance optimization. In Advances in Neural Information Processing
Systems  pages 1065–1075  2018.

[21] C.-K. Yeh  J. Chen  C. Yu  and D. Yu. Unsupervised speech recognition via segmental empirical
output distribution matching. In Proc. International Conference on Learning Representations 
2019.

[22] J. Zhang and L. Xiao. A composite randomized incremental gradient method. In International

Conference on Machine Learning  pages 7454–7462  2019.

[23] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk

minimization. Journal of Machine Learning Research  18(1):2939–2980  2017.

11

,Adithya M Devraj
Jianshu Chen