2019,Towards Hardware-Aware Tractable Learning of Probabilistic Models,Smart portable applications increasingly rely on edge computing due to privacy and latency concerns. But guaranteeing always-on functionality comes with two major challenges: heavily resource-constrained hardware; and dynamic application conditions. Probabilistic models present an ideal solution to these challenges: they are robust to missing data  allow for joint predictions and have small data needs. In addition  ongoing efforts in field of tractable learning have resulted in probabilistic models with strict inference efficiency guarantees. However  the current notions of tractability are often limited to model complexity  disregarding the hardware's specifications and constraints.  We propose a novel resource-aware cost metric that takes into consideration the hardware's properties in determining whether the inference task can be efficiently deployed. We use this metric to evaluate the performance versus resource trade-off relevant to the application of interest  and we propose a strategy that selects the device-settings that can optimally meet users' requirements. We showcase our framework on a mobile activity recognition scenario  and on a variety of benchmark datasets representative of the field of tractable learning and of the applications of interest.,Towards Hardware-Aware Tractable

Learning of Probabilistic Models

Laura I. Galindez Olascoaga†  Wannes Meert‡   Nimish Shah†

Marian Verhelst† and Guy Van den Broeck††
† Electrical Engineering Department  KU Leuven
‡ Computer Science Department  KU Leuven

†† Computer Science Department  University of California  Los Angeles

{laura.galindez nimish.shah marian.verhelst}@esat.kuleuven.be

wannes.meert@cs.kuleuven.be  guyvdb@cs.ucla.edu

Abstract

Smart portable applications increasingly rely on edge computing due to privacy
and latency concerns. But guaranteeing always-on functionality comes with two
major challenges: heavily resource-constrained hardware; and dynamic application
conditions. Probabilistic models present an ideal solution to these challenges:
they are robust to missing data  allow for joint predictions and have small data
needs. In addition  ongoing efforts in the ﬁeld of tractable learning have resulted
in probabilistic models with strict inference efﬁciency guarantees. However  the
current notions of tractability are often limited to model complexity  disregarding
the hardware’s speciﬁcations and constraints. We propose a novel resource-aware
cost metric that takes into consideration the hardware’s properties in determining
whether the inference task can be efﬁciently deployed. We use this metric to
evaluate the performance versus resource trade-off relevant to the application
of interest  and we propose a strategy that selects the device settings that can
optimally meet users’ requirements. We showcase our framework on a mobile
activity recognition scenario  and on a variety of benchmark datasets representative
of the ﬁeld of tractable learning and of the applications of interest.

1

Introduction

Tractable learning aims to balance the trade-off between how well the resulting models ﬁt the
available data and how efﬁciently queries are answered. Most implementations focus on maximizing
model performance and only factor in query efﬁciency by subjecting the learning stage to a ﬁxed
tractability constraint (e.g. max treewidth [2]). While recent notions of tractability consider the cost
of probabilistic inference as the number of arithmetic operations involved in a query [26  27]  they
still disregard hardware implementation nuances of the target application. This is of special concern
for edge computing on embedded applications  where the target algorithm must run in resource
constrained hardware  such as a small ARM or RISC-V embedded processor  or a microcontroller.
For such architectures running a lightweight operating system  the overall compute cost is mostly
determined by the cost of fundamental arithmetic operations  the interaction with sensor interfaces
and the device’s memory transactions [17  12].
In addition  efforts towards hardware-efﬁcient realizations of probabilistic inference are currently
scarce [36  21  34]. This is in stark contrast with the tremendous progress achieved by embedded
neural network implementations [37  18  29] .
We address these limitations of the ﬁeld of tractable learning by proposing a novel resource-aware cost
metric that takes into consideration the target embedded device’s properties (e.g. energy consumption);

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Arithmetic Circuit from a compiled noisy-OR and its mapping to hardware.

and system-level conﬁguration (e.g. sensors used). We map these hardware characteristics to the cost
vs. performance trade-off space  and propose a set of techniques to ﬁnd the optimal system-level
conﬁguration. Speciﬁcally  we address the following points: (a) Section 3 discusses the relevant
hardware-aware tractability metrics  and Section 4 deﬁnes the problem statement; (b) Section 5
discusses how to exploit the model’s properties to exchange task-performance for hardware efﬁciency 
and introduces techniques to ﬁnd the optimal set of system conﬁgurations in the cost vs. performance
trade-off space; and (c) Section 6 shows practical examples of these optimal solutions. This work
constitutes one of the ﬁrst efforts to introduce the ﬁeld of tractable probabilistic reasoning to the
emerging domain of edge computing. This is motivated by probabilistic models’ traits  several of
which are ideal for portable applications that require reasoning on the edge: robustness to missing
information  small data needs  joint predictions  and expert knowledge integration. Moreover  unlike
ﬁxed neural architecture training  tractable learning allows to explicitly vary the level of complexity
of the inference task  which allows us to model resource tunability.

2 Background and motivation

We use standard notation: random variables are denoted by upper case letters X and their instantiations
by lower case letters x. Sets of variables are denoted in bold upper case X and their joint instantiations
in bold lower case x. Sets of variable sets are denoted with X .
The model representation of choice in this paper is the Arithmetic Circuit (AC)  a state-of-the-art 
compact representation for a variety of machine learning models such as probabilistic graphical
models (PGMs) [6] and probabilistic programs [10]. Recent developments show how the structure
of ACs can also be learned from data [24  23]. Furthermore  ACs can be complemented with deep
learning architectures [41  28] to achieve the best of both worlds. An alternative representation
of ACs are Sum-Product Networks (SPNs)  which can also encode probability distributions as a
computational graph [32  13]. SPNs can be efﬁciently converted to ACs and vice versa [33].

2.1 Probabilistic inference with Arithmetic Circuits

An AC is a directed acyclic graph where inner nodes represent addition or multiplication and leaf
nodes are real-valued variables. ACs constitute a standard representation for computing polynomials 
but they have proven to be efﬁcient for reasoning over knowledge bases and probabilistic models
when a number of additional properties are enforced on them [6]. Once the circuit is known  the
complexity of executing the encoded formula is also known  since marginalization and partition
function operations are polynomial in the size of the circuit [4]  thus making them a well-suited
representation for tractable learning. ACs represent a joint probability distribution over a set of
random variables X: the leaf nodes are either binary indicator variables λX=x  where X ∈ X  or
parameters θ. Figure 1 shows an example of an AC that encodes the joint probability distribution of a
noisy-OR model [15].
This representation allows to perform inference to answer a number of probabilistic queries. For
example  given an instantiation f of F ⊆ X  the marginal probability Pr(f ) can be computed by
setting the indicator variables to 1 if they correspond to instantiations consistent with the observed
values  λX=x ← 1x∼f   and subsequently performing an upward pass on the AC [4]. In a binary
classiﬁcation task  one can deﬁne a class variable C  a feature set F and a classiﬁcation threshold T  
assumed to be 0.5 in this work. For a given instance f  the task consists of selecting the class CT

2

+××P(B)+P(¬B)×P(A|B ¬C)P(¬A|B ¬C)×P(C)P(A|¬B C)localsave×localfetchP(A|¬B C)localsavememoryfetch×λCP(C)memoryfetchPr(A)=XB CPr(A|B C)|{z}Noisy-ORPr(B)Pr(C)=Figure 2: Sensory embedded classiﬁcation example.

for which the condition P r(C|f ) ≥ 0.5 is met. The conditional probability can be calculated by
performing two upward passes on the AC1 that encodes Pr(C  F)  after setting the indicator variables
λ in accordance to instance f. ACs’ straightforward mapping to embedded hardware and the fact that
they readily encode the algorithm necessary for inference  motivates our choice for this probabilistic
model representation. Moreover  the process of learning them already entails a trade-off between
their predictive performance and their computational efﬁciency. The following section motivates our
proposed hardware-aware tractability metric.

2.2 Motivating example

the mobile

classiﬁcation scenario in Figure 2  where

Consider
set
F={FA1 FA2 FB1 FB2 FD1 FD2} is extracted from sensors A  B and D  and where the
AC is assumed to be the most compact model that maximizes classiﬁcation accuracy. Suppose there
are two feature subsets available  F1={FA1  FB2  FD1} and F2={FB1  FB2  FD1}; and that they
attain the same accuracy. Hence  the goal is to ﬁnd the least expensive subset. The solution to this
problem would clearly be F1 when considering only feature cost  a common approach to address the
problem of feature subset selection. But when considering also the costs of the sensors  F2 turns out
to be a better choice  as sensor A is unused and can be turned off. This example shows that trade-off
opportunities can be missed when failing to describe realistic hardware-aware system-level costs.

the

feature

3 Hardware-aware cost

In this section we formalize the notion of hardware-aware cost  the basis of our optimization frame-
work. Let α = (cid:104)+ ×  θ(cid:105) be an AC that encodes a joint probability distribution over variables F 
extracted from the set of sensor interfaces S. The hardware-aware cost (CHA) is deﬁned as:

(cid:88)

S∈S

CHA(α  S  F) = CAC(α) +

CSI(S  FS) 

(1)

where CAC are the computation costs  pertaining to inference on arithmetic circuit α  CSI are the
sensor interfacing and feature extraction costs  and FS is the feature subset extracted from sensor S.

Computation costs. At a high level  a typical embedded hardware architecture entails two com-
ponents: an on-chip main memory (typically SRAM)  which commonly houses the algorithm’s
parameter set; and a processing unit  where operations are performed and intermediate values are
cached in a local memory. Performing an upward pass on an AC involves the following actions
(see Figure 1): 1) fetching parameters from the main memory  2) performing arithmetic operations 
consisting of additions and multiplications  3) caching intermediate values in a local memory (e.g.
register ﬁle or low level cache) and 4) fetching intermediate values from local memory  as needed.2
Each action has a signiﬁcantly different hardware resource cost. For example  post synthesis energy
models of a simple embedded CPU show that multiplications can require 4 times as much energy
as additions  and memory transactions 5 times as much energy as multiplications [17]. When it

1Conditional probability can also be performed by an upward and a downward pass [6].
2In this work we assume that the local cache size is sufﬁciently large to store intermediate values  but not
large enough to store parameters. However  for some learned circuits  there are about as many parameters as
edges  so depending on the local memory size  one might need to store intermediate values also in main memory.

3

comes to the design of embedded hardware  energy efﬁciency is indeed one of the main challenges to
address. Hence  we continue to focus on this resource as a proof of concept without loss of generality;
examples of other relevant hardware resource metrics are throughput and latency. It is evident that the
total hardware cost of performing a pass on an AC must factor in all the aforementioned transactions.
Let nb be the number of bits used to represents parameters θ and perform arithmetic operations +
and ×. The computation cost (CAC) of AC α is deﬁned as:

CAC(α  nb) = C+(nb) + C×(nb) + Cmem(nb) + Ccache(nb) 

(2)
where the terms in CAC deﬁne the cost incurred by each type of operation. Here  C+ and C× are the
costs of addition and multiplication; Cmem is the cost from fetching parameter leaf nodes from main
memory and Ccache is the cost from storing and fetching from local cache (as in Figure 1):

a[a (cid:54)=t + and a (cid:54)=t ×] · φmem(nb) 
a[a =t + or a =t ×] · φcache(nb) 

C+(nb) =(cid:80)
C×(nb) =(cid:80)

a[a =t +] · φ+(nb) 
a[a =t ×] · φ×(nb) 

Cmem(nb) =(cid:80)
Ccache(nb) =(cid:80)

where a denotes a node in α  the equality =t holds when node a matches the operation type on
the right side and [β] is equal to 1 when β is true. The function φ(.) describes the effective cost of
the particular operation and can be derived from empirical benchmarks  customized to the target
hardware [17  35]. When expressing cost in terms of energy consumption  computation costs scale
with the precision in number of bits used to represent parameters and perform arithmetic operations
(nb)  which is typically the same for all nodes in the AC. To conclude  the cost incurred by each node
in an AC is determined by its type (whether addition  multiplication  local parameter fetch  or remote
memory access) and the resolution of the operation or parameter (in nb).

Sensor interfacing costs The computational block described above is often part of a larger system 
which repeatedly performs a task based on external inputs or observations  such as classiﬁcation. In
this scenario  one must factor in the costs incurred by interfacing with the environment or the user.
A sensory interface consists of a set of sensors S  which gather  process and digitize environmental
information (typically in the analog domain)  and a (typically digital) feature extraction unit  which
generates the feature set F to be used by the machine learning algorithm. Let S be the set of available
sensors and F the feature set extracted from them. The sensor interfacing cost (CSI) is:

CSI(S  FS) = CS(S) +

(3)
where CS describes the cost incurred by sensor S and CF the cost of extracting feature set FS ⊆ F.
The sensing cost function CS can be customized to the target platform and applications through
measurements or data sheets. Note that  if no features from a given sensor are used  it can be shut
down  and its cost dropped (see Figure 1). In most systems  CF can be deﬁned from the type and
number of arithmetic and memory operations involved  in a similar fashion to the computation cost
estimation CAC  as will be illustrated in the experiments (Section 6.1).

CF(FS) 

FS∈FS

(cid:88)

4 Problem statement

We have seen so far that CHA depends on four system properties:1) the complexity of model α 
determined by the number and type of its operations; 2) the size and type of the feature set F; 3)
the size and type of the available sensor set S; and 4) the number of bits nb used within α. We
refer to an instantiation of these four properties σ ={α  F  S  nb} as a system conﬁguration. Clearly 
the system conﬁguration also determines the algorithm’s performance  deﬁned according to the
application of interest. The methods proposed in this work can accommodate any performance metric
or miss-classiﬁcation cost  but we will only consider accuracy  due to its generality. Speciﬁcally 
we set the classiﬁcation threshold to T = 0.5  and we consider the accuracy of the Bayes-optimal
predictions (Acc) over a set of feature instantiations {f1  ...  fl}.
Section 2.2 asks to identify the system conﬁguration that incurs the lowest cost for a desired accuracy.
Similarly  we might be interested in the conﬁguration that achieves the highest accuracy for a given
cost constraint. Thus  the problem we aim to address is how to select the system conﬁgurations
that map to the Pareto-frontier on the hardware-cost vs. accuracy trade-off space. The inputs to
our problem are the class variable C  the available features F and sensors S sets  and the set
of available precisions nb. The output is the set of Pareto-optimal system conﬁgurations σ∗ =
{{α∗

i  nb∗

i }i=1:p}.

i   F∗

i  S∗

4

5 Trade-off space search

We propose to search the cost vs. accuracy trade-off space by scaling four properties (see Section 4):
Model complexity scaling. We learn a set of ACs α of increasing complexity. Each maps to a
speciﬁc classiﬁcation accuracy and computation cost CAC (see Eq. 2). Although discriminative AC
learners have shown state-of-the-art classiﬁcation accuracy [24]  we have opted for a generative
learning strategy: the LearnPSDD algorithm [23]. The motivation for this choice is twofold: this
algorithm improves the model incrementally  but each iteration already leads to a valid AC  that can
be used to populate the set α. Moreover  the learned ACs encode a joint probability distribution 
which ensures they are robust to missing data  as demanded by the application range of interest:
embedded reasoning tasks must often deal with missing values  either due to malfunction (e.g.  a
sensor is blocked in an autonomous driving system)  or to enforce hardware-cost efﬁciency (e.g. 
when energy consumption is excessive  the driving system has the choice to turn off an expensive
sensor and the features extracted from it).
Feature and sensor set scaling. We scale the feature set F by sequentially pruning individual
features (see Section 5.1). The effect of feature pruning on classiﬁcation accuracy has been discussed
in numerous works [11  5  22] and the impact on the hardware-aware cost is clear from Eq. 3. Pruning
features can also have an impact on the computation costs CAC: if a variable is always unobserved  its
indicator variables are ﬁxed (see Section 2.1)  which we exploit to simplify the circuit (see Algorithm
2). In addition  sensor S ∈ S can be pruned when none of the features it originates is used anymore;
a strategy that has not been explored by the state of the art  but that is straightforward with our
approach  since it considers the full system.
Precision scaling. We consider four different standard IEEE 754 ﬂoating point representations  as
they can be implemented in almost any embedded hardware platform. Reducing the precision of arith-
metic operations and numerical representations entails information loss and results in performance
degradation [35]. The effect on computation costs CAC is clear from Eq. 2.

5.1 Search strategy

Finding the smallest possible AC that computes a given function is Σp
2-hard [3]  thus computationally
harder than NP. No single optimal solution is known for this problem; it is a central question in the
ﬁeld of knowledge compilation [7]. Optimizing for the lowest-cost/highest-accuracy AC  further
increases complexity. We therefore opt for a greedy optimization algorithm. Speciﬁcally  we rely
on a series of heuristics to search the trade-off space. In each step  we independently scale one of
the available conﬁguration properties (cid:104)α  F  S  nb(cid:105)  as described in the previous section  and aim to
ﬁnd its locally optimal setting. The search begins by learning the model set α={αk}k=1:n. Then 
as shown by Algorithm 1  starting from each model αk  we perform a greedy neighborhood search
that aims to maximize cost savings and minimize accuracy losses by sequentially pruning the sets F
and S  and simplifying αk accordingly (Algorithm 2). At each iteration  we evaluate the accuracy
and cost of m feature subset candidates  where each considers the impact of removing a feature from
the user-deﬁned prunable set Fprun ⊆ F. We then select the feature and sensor subsets Fsel ⊆ F 
Ssel ⊆ S and the simpliﬁed model αsel that maximizes the objective function OF = acc/costnorm 
where costnorm is the evaluated hardware-aware cost CHA  normalized according to the maximum
achievable cost (from the most complex model available αn). Note that feature subset selection drives
sensor subset selection Ssel  as described before  and deﬁned in lines 12 and 19 of Algorithm 1.
The output of Algorithm 1  (cid:104)F (k) S (k) M(k)(cid:105)  is a set of system conﬁgurations of the form
{{Fsel 1   Ssel 1   αsel 1}  . . .  {Fsel q   Ssel q   αsel q}}  where q=|Fprunable|  and the superscript (k)
denotes the number of the input model αk  taken from α. For each conﬁguration resulting from
Algorithm 1  we can sweep the available precision conﬁgurations nb  for a ﬁnal space described
by σ=(cid:104)F S M N(cid:105) of size |α| · |Fprunable| · |N|  where N contains the selected precision. In the
experimental section we show a work-around to reduce search space size and the number of steps
required by the Pareto-optimal search. Regarding complexity  the feature selection in Algorithm 1 is
a greedy search  thus its complexity is linear in the number of features times the number of iterations
needed for convergence. 3 The AC pruning routine consists of an upward pass on the AC and its
complexity is therefore linear in the size of the AC.

3In Alg. 1 the user can provide the desired accuracy or cost as the while-loop break criterion.

5

Algorithm 1: ScaleSI(αk Fprun Sprun)
Input: αk: the kth model in α   Fprun Sprun: set of
Output: (cid:104)F k S k Mk(cid:105)  acc  cost: kth collection of
pruned features  sensors and model sets  their
accuracy and cost.

prunable features and sensors.

1 Fsel ← Fprun  Ssel ← Sprun  αsel ← αk
2 (cid:104)F k S k Mk(cid:105) ← (cid:104)Fsel   Ssel   αsel(cid:105)
3 accsel=Acc(αsel   Fsel )  costsel=CHA(αsel   Fsel   Ssel )
4 (cid:104)acc  cost(cid:105) ← (cid:104)accsel   costsel(cid:105)
5 while |Fsel| > 1 do

// Initialize objective value

Algorithm 2: PruneAC(α F)
Input: α: the input AC  F: the observed feature set

used to guide the pruning of α.

Output: αpr: the pruned AC.
1 αpr ← copy(α)
/* Loop through AC  children before parents
2 foreach a in αpr do
3
4
5
6

if a is an indicator variable λF =f and F /∈ F then
else if a is + or × with constant children then
replace a in αpr by an equivalent constant

replace a in αpr by a constant 1

*/

7 return αpr

6
7
8
9
10
11
12

obmax ← 0
foreach F ∈ Fprun do
Fca ← Fsel \ F
Sca ← Ssel
foreach S ∈ Sprun do
Sca ← Sca \ S

if Fca ∩ FS = ∅ then

13
14
15
16
17
18
19
20

21
22

αca ←PruneAC(αsel Fca)
accca ← Acc(αca Fca)
costca ← CHA(αca Fca Sca)
obca ← OF(accca   costca )
if obca > obmax then

obmax ← obca
Fsel ← Fca  Ssel ← Sca  αsel ← αca
accsel ← accca costsel ← costca

F k.insert(Fsel)  S k.insert(Ssel)  Mk.insert(αsel)
acc.insert(accsel)  cost.insert(costsel)

23 return (cid:104)F k S k Mk(cid:105)  acc  cost

5.2 Pareto-optimal conﬁguration selection

Algorithm 3: GetPareto(σ acc cost)
Input: σ  acc  cost: Conﬁguration set  their accuracy

// Prune sensor

and cost.

Output: σ∗  acc∗  cost∗: Pareto optimal

conﬁgurations  their accuracy and cost.

1 (cid:104)cost∗  σ∗  acc∗(cid:105) ← (cid:104){} {} {}(cid:105) ;
/* Sort according to ascending cost
2 (cid:104)cost  σ  acc(cid:105) ← sorted((cid:104)cost  σ  acc(cid:105));
3 i ← |σ| + 1;
4 while i > 0 do
5
6
7
8
9
10 return σ∗  acc∗  cost∗

i ← arg max acc0:i
σ∗.insert(σi)
acc∗.insert(acci)
cost∗.insert(costi)
i ← i − 1

*/

i  S∗

i   F∗

i  nb∗

Algorithm 3 describes how we extract the Pareto-optimal conﬁguration subset  but any convex hull
algorithm can be used. The input is the conﬁguration set σ=(cid:104)F S M N(cid:105) and their corresponding
accuracy (acc) and cost (cost). The output of this algorithm is the set of Pareto-optimal system
conﬁgurations σ∗={{α∗
i }i=1:p}  each guaranteed to achieve the largest reachable
accuracy for any given cost; or the lowest reachable cost for any given accuracy (acc∗  cost∗).
Note that the framework introduced thus far can balance the trade-off between hardware-aware cost
and any other application-speciﬁc performance metric  by simply replacing the accuracy terms in
Algorithms 1  2 and 3 with such a metric. For instance  medical applications often aim to balance
precision and recall  and may favor the latter at night under scarce medical supervision. Furthermore 
our framework can be used density estimation tasks by deploying the model complexity scaling
followed by precision scaling stages and forgoing the pruning stages of Algorithms 1 and 2  in order
to keep the full joint distribution. The next section illustrates how our methodology can reap the
beneﬁts of scalable embedded hardware.

6 Experimental evaluation

We empirically evaluate the proposed techniques on a relevant embedded sensing use case: the Human
Activity Recognition (HAR) benchmark [1]. Additionally  we show our method’s general applicability
on a number of other publicly available datasets [8  14  20  25  30]  two of them commonly used for
density estimation benchmarks and the rest commonly used for classiﬁcation (see Table 1).4

Computational costs. The computation costs CAC are based on the energy benchmarks discussed
in [17] and [35]. Table 2 shows the relative costs of each term in CAC and how they scale with

4Code available at https://github.com/laurago894/HwAwareProb.

6

precision nb. The baseline is 64 ﬂoating point bits because it is the standard IEEE representation
in software environments. For the rest of the experiments  we consider three other standard low
precision representations: 32 bits (8 exponent and 24 mantissa)  16 bits (5 exponent and 11 mantissa)
and 8 bits (4 exponent and 4 mantissa) [19].

Dataset pre-processing. For the classiﬁcation bench-
marks  we discretized numerical features using the method
in [9]. We then binarized them using a one-hot encoding
and subjected them to a 75%-train  10%-validation and
15%-test random split. For the HAR benchmark  we pre-
served the timeseries information by using the ﬁrst 85%
samples for training and validation and the last for test-
ing. For the density estimation datasets  we used the splits
provided in [25] and we assumed the last feature in the
set to be the class variable. On all datasets  we performed
wrapper feature selection (evaluating the features’ value
on a Tree Augmented Naive Bayes classiﬁer) before going
through the hardware-aware optimization process to avoid
over-ﬁtting on the baseline model and ensure it is a fair
reference point. The number of effectively used features |F| is shown in Table 1. In addition  we
consider all the features to be in the prunable set Fprun for datasets with less than 30 features. For
the rest  we consider the 20 with the highest correlation to the class variable. Within the context of an
application  the prunable set can be user-deﬁned. For instance  in a multi-sensor seizure detection
application  medical experts might advise against pruning features extracted from an EEG monitor.

Table 1: Experimental datasets
†: Classiﬁcation   (cid:63): Density est.
Dataset
|α|
Banknote†
11
HAR †
11
Houses †
11
Jester (cid:63)
11
Madelone †
11
Nltcs (cid:63)
11
Six-HAR †
11
Wilt †
11

|F|
15
28
36
99
20
15
54
11

15
28
20
20
20
15
20
11

|Fprun|

Table 2: Experiment computational costs.

Model learning We learned the models on the
train and validation sets with the LearnPSDD
algorithm [23]  using the same settings reported
therein  and following the bottom-up vtree in-
duction strategy. To populate the model set α 
we retained a model after every N/10 iterations 
where N is the number of iterations needed for
convergence (in the algorithm this is until the
log-likelihood on validation data stagnates). Table 1 shows |α| for each dataset. Furthermore  as a
baseline  we trained a Tree Augmented Naive Bayes (TAN) classiﬁer and compiled it to an AC.5

Operation At 64 bits
Cmem
Ccache
C×
C+

φmem = γmem · nb
φcache = γcache · nb
φ× = γ2
× · nb2 · log(nb)

1
0.2
0.6
0.1

Operation cost

φ+ = γ+ · nb

6.1 Embedded Human Activity Recognition

The HAR dataset aims to recognize the activity a person is performing based on statistical and
energy features extracted from smartphone accelerometer and gyroscope data. We perform binary
classiﬁcation by discerning “walking downstairs” from the other activities. For the experiments 
we use a total of 28 binary features  8 of which are extracted from the gyroscope’s signal and the
rest from the accelerometer  as detailed in Appendix B.4. All computation costs for this dataset are
normalized according to the energy consumption trends of an embedded ARM M9 CPU  assuming
0.1nJ per operation [38]. Sensors are estimated to consume 2mWatt  while the costs of all features is
deﬁned as 30 MAC operations (see Appendix B.1 for more details).
Pareto optimal conﬁguration. This experiment consisted of three stages  performed on the training
set (Figure 3(a)): 1) We ﬁrst mapped each model in α to the trade-off space  as shown in black. 2)
Starting from each model  we scaled the feature and the sensor sets F  S  as shown in blue. 3) We
then scaled the precision nb of each of these pruned conﬁgurations (shown by the grey curves) and
we ﬁnally extracted the Pareto front shown in red. As shown by the Pareto conﬁgurations highlighted
in green  our method preserves the highest baseline train-set accuracy by pruning 11 of the available
28 features  which results in CHA savings of 53%. When willing to tolerate further accuracy losses
of 0.4%  our method outputs a conﬁguration that consumes only 13% of the original cost by using
a smaller model (α3)  pruning 18 features  turning one sensor off and using a 32 bit representation.
Figures 3(c d) break down the computational cost CAC and the sensor costs CSI. When considering
only the costs of the AC evaluation (graph (c))  our method results in savings of almost two orders of

5Using the ACE compiler available at http://reasoning.cs.ucla.edu/ace/.

7

Figure 3: Experiments on the Human Activity Recognition benchmark.

magnitude with accuracy losses lower than 1%  and up to 3 orders of magnitude when tolerating more
losses. Indeed  these computational cost savings result from the joint effects of precision reduction
and of simplifying the AC with Algorithm 2  with savings of about 2 to 10 % per feature pruned (see
Appendix B.2). Sensor and feature costs  as shown in graph (d) only scale up to 50%  since at least
one of the sensors must always be operating. This demonstrates the importance of taking these costs
into account: even though computation costs savings are impressive  the system is still limited by the
sensing costs.
Robustness and online deployment. The red curve in Figure 3(b) shows that the evaluation of the
selected Pareto conﬁgurations against testing data stays within a range of ±1% with respect to Figure
3(a). Comparing our method to the TAN classiﬁer denoted with the cyan marker  we can see that it
provides further cost saving opportunities  while achieving competitive accuracy. We also assessed
the robustness of our method by simulating  per conﬁguration  ten iterations of random failure of
varying sizes of feature sets (|F|/10 |F|/5 |F|/2). The green and magenta dotted curves show the
median of these experiments for the Pareto conﬁgurations and for the original model set. These trials
stay within a range of −2% compared to the fully functional results in red and black  which validates
our choice of a generative learner that can naturally cope with missing features at prediction time.
In embedded sensing scenarios  environmental circumstances  power consumption requirements
and accuracy constraints commonly vary over time. This calls for dynamic operating settings 
where the system can switch between different accuracy-cost operating points at run-time. Figure
3(e) shows such a scenario for nine operating points selected off-line (as highlighted with circular
markers in the background graph)  which comply with hypothetical user needs of accuracy>95%
and normalized cost<20%. The implemented policy assumes an energy efﬁcient mode when the
classiﬁer has recently identiﬁed that there is no ongoing activity  and a high reliability – and costlier –
mode when it has identiﬁed that there is ongoing activity (see Appendix B.3 for more details). The
foreground of Figure 3(e) contrasts the test-set cost-accuracy performance attained when always
using the same model (in red)  with the cost-accuracy performance resulting from the implementation
of our model-switching policy (purple cross). Even with its simplicity  the proposed policy attains
accuracy vs. cost improvements that go beyond the static Pareto front. Figure 3(f) shows that this

8

P1P2P3P4Table 3: Results for benchmarking datasets [CAC Acctr% Accte%].

Dataset
Banknote
Houses
Jester

Madelone

Nltcs

Six-HAR

Wilt

Operating pt. 1
[1 94.5 95.6]
[1 97.6 97.4]
[1 75.6 76.4]
[1 68.1 68.4]
[1 93.5 93.9]
[1 91.5 89.8]
[1 97.1 97.5]

Operating pt.2
[0.09 94.5 95.6]
[0.12 97.6 97.3]
[0.35 75.6 76.4]
[0.05 68.6 69.1]
[0.19 93.6 93.8]
[0.38 91.6 89.9]
[0.07 97.1 97.5]

Operating pt. 3
[0.04 89.9 93.1]
[0.04 97.1 96.6]
[0.12 74.7 75.7]
[0.02 66.9 68.8]
[0.03 93.4 94.2]
[0.15 89.3 89.3]
[0.03 97.1 97.5]

Operating pt. 4
[0.01 84.5 86.8]
[0.01 94.3 94.0]
[0.02 72.6 73.1]
[0.01 62.6 62.9]
[0.01 91.7 92.0]
[0.04 89.8 89.8]
[0.01 96.9 97.5]

TAN

[0.24 93.8 92.2]
[0.05 97.2 97.1]
[0.12 73.1 72.3]
[0.13 66.0 65.7]
[0.11 91.4 91.9]
[0.36 91.7 90.3]
[0.25 97.1 97.5]

is achieved by making a balanced use of the nine available conﬁgurations. Note that this switching
action incurs overhead only in terms of memory since the set of Pareto switching conﬁgurations is
always determined off-line  and will be only fetched when needed. In most portable applications 
predictions must be made at a much higher frequency than conﬁguration changes are necessary [12].
The incurred memory overhead in our experiments is less than 3% of the total cost  since model
switching is only necessary on 120 out of the 1470 predictions.

6.2 Generality of the method: evaluation on benchmark datasets

We now apply our optimization sequence to the datasets in Table 1. For lack of information on the
hardware that originated these datasets  we only consider the computation cost CAC  again evaluated
on the cost model of the ARM M9 CPU. Table 3 shows this cost along with the training and testing
accuracy (Acctr Accte) at four operating points for every dataset. Note that we have also included the
six-class HAR benchmark  to demonstrate the applicability of our method beyond binary classiﬁcation.
We can see that all the benchmarks strongly beneﬁt from our proposed methodology  that they are all
robust when contrasted against the test dataset  and that they are competitive when compared to a
TAN classiﬁer. Appendix A shows a ﬁgure with the Pareto fronts for all the experiments herewith.

7 Related work

The problem of hardware-efﬁcient probabilistic inference has been addressed by the probabilistic-
models and the embedded-hardware communities from several perspectives. The works by Tschi-
atschek and Pernkopf [39] and Piatkowski et al. [31] propose reduced precision and integer represen-
tation schemes for PGMs as a strategy to address the constraints of embedded devices. In [35]  Shah
et al. propose a framework that automatically chooses an appropriate energy-efﬁcient low precision
representation and generates custom hardware. Other efﬁcient hardware implementation efforts have
been made by Zermani et al. [42]  Schuman et al. [34]  and Sommer et al. [36]  who have proposed
to accelerate inference on SPNs  capitalizing on their tractable properties.
Our work constitutes an effort to integrate the expertise from both communities under a uniﬁed
framework  which considers the impact of all scalable aspects of the model to optimize it in a
hardware-aware fashion. To that end  it leverages the properties of the selected AC representation.
Such representation enables the use of our framework with any probabilistic model that is compute-
efﬁcient at prediction time: see [16] by Holtzen et al. and [40] by Vlasselaer et al. for examples
of probabilistic program compilation to ACs; and [27] by Lowd and Rooshenas on how to perform
efﬁcient inference with Markov networks represented as ACs.

8 Conclusions

We proposed a novel hardware-aware cost metric to deal with the limitations of the efﬁciency vs.
performance trade-off considered by the ﬁeld of tractable learning. Our method obtains the Pareto-
optimal system-conﬁguration set in the hardware-aware cost vs. accuracy space. The proposed
solution consists of a sequential hardware-aware search and a Pareto-optimal conﬁguration selection
stage. Experiments on a variety of benchmarks demonstrated the effectiveness of the approach and
sacriﬁce little to no accuracy for signiﬁcant cost savings. This opens up opportunities for the efﬁcient
implementation of probabilistic models in resource-constrained edge devices  operating in dynamic
environments.

9

Acknowledgements

We thank Yitao Liang for the LearnPSDD algorithm and for helpful feedback. This work is partially
supported by the EU ERC Project Re-SENSE under Grant ERC-2016-STG-71503; NSF grants
#IIS-1633857  #CCF-1837129  DARPA XAI grant #N66001-17-2-4032  NEC Research  and gifts
from Intel and Facebook Research.

References
[1] D. Anguita  A. Ghio  L. Oneto  X. Parra  and J. L. Reyes-Ortiz. A public domain dataset for human
activity recognition using smartphones. In 21th European Symposium on Artiﬁcial Neural Networks 
Computational Intelligence and Machine Learning (ESANN)  2013.

[2] F. R. Bach and M. I. Jordan. Thin junction trees. In Advances in Neural Information Processing Systems 

pages 569–576  2002.

[3] D. Buchfuhrer and C. Umans. The complexity of boolean formula minimization.
Colloquium on Automata  Languages  and Programming  pages 24–35. Springer  2008.

In International

[4] M. Chavira and A. Darwiche. On probabilistic inference by weighted model counting. Artiﬁcial Intelligence 

172(6-7):772–799  2008.

[5] Y. Choi and G. Van den Broeck. On robust trimming of bayesian network classiﬁers. In Proceedings of the

27th International Joint Conference on Artiﬁcial Intelligence (IJCAI)  July 2018.

[6] A. Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge University Press  2009.

[7] A. Darwiche and P. Marquis. A knowledge compilation map. Journal of Artiﬁcial Intelligence Research 

17:229–264  2002.

[8] D. Dua and C. Graff. UCI machine learning repository  2017.

[9] U. Fayyad and K. Irani. Multi-interval discretization of continuous-valued attributes for classiﬁcation
learning. Proceedings of the 13th International Joint Conference on Artiﬁcial Intelligence (IJCAI)  1993.

[10] D. Fierens  G. Van den Broeck  I. Thon  B. Gutmann  and L. De Raedt.

Inference and learning in
probabilistic logic programs using weighted CNFs. Theory and Practice of Logic Programming  15  2015.

[11] N. Friedman  D. Geiger  and M. Goldszmidt. Bayesian network classiﬁers. Machine learning  29(2-3):131–

163  1997.

[12] L. Galindez Olascoaga  K. Badami  J. Vlasselaer  W. Meert  and M. Verhelst. Dynamic sensor-frontend
tuning for resource efﬁcient embedded classiﬁcation. IEEE Journal on Emerging and Selected Topics in
Circuits and Systems  8(4):858–872  Dec 2018.

[13] R. Gens and P. Domingos. Learning the structure of sum-product networks. In International conference on

machine learning  pages 873–880  2013.

[14] I. Guyon and S. Gunn. Nips feature selection challenge  2003.

[15] D. Heckerman. Causal independence for knowledge acquisition and inference. In Proceedings of the

Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  pages 122–127. Elsevier  1993.

[16] S. Holtzen  T. Millstein  and G. Van den Broeck. Symbolic exact inference for discrete probabilistic
programs. In Proceedings of the ICML Workshop on Tractable Probabilistic Modeling (TPM)  June 2019.

[17] M. Horowitz. 1.1 computing’s energy problem (and what we can do about it). In 2014 IEEE International

Solid-State Circuits Conference Digest of Technical Papers (ISSCC)  pages 10–14  Feb 2014.

[18] A. G. Howard  M. Zhu  B. Chen  D. Kalenichenko  W. Wang  T. Weyand  M. Andreetto  and H. Adam.
Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint
arXiv:1704.04861  2017.

[19] IEEE. Ieee standard for ﬂoating-point arithmetic. IEEE Std 754-2008  pages 1–70  Aug 2008.

[20] B. A. Johnson  R. Tateishi  and N. T. Hoan. A hybrid pansharpening approach and multiscale object-
based image analysis for mapping diseased pine and oak trees. International journal of remote sensing 
34(20):6969–6982  2013.

10

[21] O. U. Khan and D. D. Wentzloff. Hardware accelerator for probabilistic inference in 65-nm cmos. IEEE

Transactions on Very Large Scale Integration (VLSI) Systems  24(3):837–845  2016.

[22] P. Khosravi  Y. Liang  Y. Choi  and G. Van den Broeck. What to expect of classiﬁers? reasoning about
logistic regression with missing features. In Proceedings of the 28th International Joint Conference on
Artiﬁcial Intelligence (IJCAI)  aug 2019.

[23] Y. Liang  J. Bekker  and G. Van den Broeck. Learning the structure of probabilistic sentential decision

diagrams. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2017.

[24] Y. Liang and G. Van den Broeck. Learning logistic circuits. Proceedings of the 33rd Conference on

Artiﬁcial Intelligence (AAAI)  2019.

[25] D. Lowd and J. Davis. Learning markov network structure with decision trees. In 2010 IEEE International

Conference on Data Mining  pages 334–343. IEEE  2010.

[26] D. Lowd and P. Domingos. Learning arithmetic circuits. arXiv preprint arXiv:1206.3271  2012.
[27] D. Lowd and A. Rooshenas. Learning markov networks with arithmetic circuits. In Artiﬁcial Intelligence

and Statistics  pages 406–414  2013.

[28] R. Manhaeve  S. Dumanˇci´c  A. Kimmig  T. Demeester  and L. De Raedt. Deepproblog: Neural probabilistic

logic programming. Advances in Neural Information Processing Systems 31 (NeurIPS)  2018.

[29] B. Moons  D. Bankman  L. Yang  B. Murmann  and M. Verhelst. Binareye: An always-on energy-accuracy-
scalable binary cnn processor with all memory on chip in 28nm cmos. In 2018 IEEE Custom Integrated
Circuits Conference (CICC)  pages 1–4. IEEE  2018.

[30] R. K. Pace and R. Barry. Sparse spatial autoregressions. Statistics & Probability Letters  33(3):291–297 

1997.

[31] N. Piatkowski  S. Lee  and K. Morik. Integer undirected graphical models for resource-constrained systems.

Neurocomputing  173:9–23  2016.

[32] H. Poon and P. Domingos. Sum-product networks: A new deep architecture. In 2011 IEEE International

Conference on Computer Vision Workshops (ICCV Workshops)  pages 689–690. IEEE  2011.

[33] A. Rooshenas and D. Lowd. Learning sum-product networks with direct and indirect variable interactions.

In International Conference on Machine Learning  pages 710–718  2014.

[34] J. Schumann  K. Y. Rozier  T. Reinbacher  O. J. Mengshoel  T. Mbaya  and C. Ippolito. Towards real-time 
on-board  hardware-supported sensor and software health management for unmanned aerial systems.
International Journal of Prognostics and Health Management  2015.

[35] N. Shah  L. I. Galindez Olascoaga  W. Meert  and M. Verhelst. Problp: A framework for low-precision
probabilistic inference. In Proceedings of the 56th Annual Design Automation Conference 2019  page 190.
ACM  2019.

[36] L. Sommer  J. Oppermann  A. Molina  C. Binnig  K. Kersting  and A. Koch. Automatic mapping of the
sum-product network inference problem to fpga-based accelerators. In 2018 IEEE 36th International
Conference on Computer Design (ICCD)  pages 350–357. IEEE  2018.

[37] M. Tan  B. Chen  R. Pang  V. Vasudevan  and Q. V. Le. Mnasnet: Platform-aware neural architecture

search for mobile. arXiv preprint arXiv:1807.11626  2018.

[38] S. Tarkoma  M. Siekkinen  E. Lagerspetz  and Y. Xiao. Smartphone energy consumption: modeling and

optimization. Cambridge University Press  2014.

[39] S. Tschiatschek and F. Pernkopf. On bayesian network classiﬁers with reduced precision parameters.

Pattern Analysis and Machine Intelligence  IEEE Transactions on  37(4):774–785  2015.

[40] J. Vlasselaer  G. Van den Broeck  A. Kimmig  W. Meert  and L. De Raedt. Tp-compilation for inference in

probabilistic logic programs. International Journal of Approximate Reasoning  June 2016.

[41] J. Xu  Z. Zhang  T. Friedman  Y. Liang  and G. Van den Broeck. A semantic loss function for deep learning
with symbolic knowledge. In Proceedings of the 35th International Conference on Machine Learning
(ICML)  July 2018.

[42] S. Zermani  C. Dezan  R. Euler  and J.-P. Diguet. Bayesian network-based framework for the design of
reconﬁgurable health management monitors. In 2015 NASA/ESA Conference on Adaptive Hardware and
Systems (AHS)  pages 1–8. IEEE  2015.

11

,Laura Galindez Olascoaga
Wannes Meert
Nimish Shah
Marian Verhelst
Guy Van den Broeck