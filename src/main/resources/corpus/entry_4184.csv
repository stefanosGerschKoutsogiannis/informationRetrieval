2008,Overlaying classifiers: a practical approach for optimal ranking,ROC curves are one of the most widely used displays to evaluate performance of scoring functions. In the paper  we propose a statistical method for directly optimizing the ROC curve. The target is known to be the regression function up to an increasing transformation and this boils down to recovering the level sets of the latter. We propose to use classifiers obtained by empirical risk minimization of a weighted classification error and then to construct a scoring rule by overlaying these classifiers. We show the consistency and rate of convergence to the optimal ROC curve of this procedure in terms of supremum norm and also  as a byproduct of the analysis  we derive an empirical estimate of the optimal ROC curve.,Overlaying classiﬁers:

a practical approach for optimal ranking

St´ephan Cl´emenc¸on

Telecom Paristech (TSI) - LTCI UMR Institut Telecom/CNRS 5141

stephan.clemencon@telecom-paristech.fr

Nicolas Vayatis

ENS Cachan & UniverSud - CMLA UMR CNRS 8536

vayatis@cmla.ens-cachan.fr

Abstract

ROC curves are one of the most widely used displays to evaluate performance
of scoring functions. In the paper  we propose a statistical method for directly
optimizing the ROC curve. The target is known to be the regression function up
to an increasing transformation and this boils down to recovering the level sets of
the latter. We propose to use classiﬁers obtained by empirical risk minimization of
a weighted classiﬁcation error and then to construct a scoring rule by overlaying
these classiﬁers. We show the consistency and rate of convergence to the optimal
ROC curve of this procedure in terms of supremum norm and also  as a byproduct
of the analysis  we derive an empirical estimate of the optimal ROC curve.

1 Introduction

In applications such as medical diagnosis  credit risk screening or information retrieval  one aims at
ordering instances under binary label information. The problem of ranking binary classiﬁcation data
is known in the machine learning literature as the bipartite ranking problem ([FISS03]  [AGH+05] 
[CLV08]). A natural approach is to ﬁnd a real-valued scoring function which mimics the order
induced by the regression function. A classical performance measure for scoring functions is the
Receiver Operating Characteristic (ROC) curve which plots the rate of true positive against false
positive ([vT68]  [Ega75]). The ROC curve offers a graphical display which permits to judge rapidly
how a scoring rule discriminates the two populations (positive against negative). A scoring rule
whose ROC curve is close to the diagonal line does not discriminate at all  while the one lying above
all others is the best possible choice. From a statistical learning perspective  risk minimization (or
performance maximization) strategies for bipartite ranking have been based mostly on a popular
summary of the ROC curve known as the Area Under a ROC Curve (AUC - see [CLV08]  [FISS03] 
[AGH+05]) which corresponds to the L1-metric on the space of ROC curves. In the present paper 
we propose a statistical methodology to estimate the optimal ROC curve in a stronger sense than
the AUC sense  namely in the sense of the supremum norm. In the same time  we will explain how
to build a nearly optimal scoring function. Our approach is based on a simple observation: optimal
scoring functions can be represented from the collection of level sets of the regression function.
Hence  the bipartite ranking problem may be viewed as a ’continuum’ of classiﬁcation problems
with asymmetric costs where the targets are the level sets. In a nonparametric setup  regression
or density level sets can be estimated with plug-in methods ([Cav97]  [RV06]  [AA07]  [WN07] 
...). Here  we take a different approach based on a weighted empirical risk minimization principle.
We provide rates of convergence with which an optimal point of the ROC curve can be recovered
according to this principle. We also develop a practical ranking method based on a discretization of
the original problem. From the resulting classiﬁers and their related empirical errors  we show how

1

to build a linear-by-part estimate of the optimal ROC curve and a quasi-optimal piecewise constant
scoring function. Rate bounds in terms of the supremum norm on ROC curves for these procedures
are also established.
The rest of the paper is organized as follows: in Section 2  we present the problem and give some
properties of ROC curves  in Section 3  we provide a statistical result for the weighted empirical risk
minimization  and in Section 4  we develop the main results of the paper which describe the statisti-
cal performance of a scoring rule based on overlaying classiﬁers as well as the rate of convergence
of the empirical estimate of the optimal ROC curve.

2 Bipartite ranking  scoring rules and ROC curves

Setup. We study the ranking problem for classiﬁcation data with binary labels. The data are assumed
to be generated as i.i.d. copies of a random pair (X  Y ) ∈ X × {−1  +1} where X is a random
descriptor living in the measurable space X and Y represents its binary label (relevant vs. irrelevant 
healthy vs. sick  ...). We denote by P = (µ  η) the distribution of (X  Y )  where µ is the marginal
distribution of X and η is the regression function (up to an afﬁne transformation): η(x) = P{Y =
1 | X = x}  x ∈ X . We will also denote by p = P{Y = 1} the proportion of positive labels.
In the sequel  we assume that the distribution µ is absolutely continuous with respect to Lebesgue
measure.

Optimal scoring rules. We consider the approach where the ordering can be derived by the means
of a scoring function s : X → R: one expects that the higher the value s(X) is  the more likely the
event ”Y = +1” should be observed. The following deﬁnition sets the goal of learning methods in
the setup of bipartite ranking.

Deﬁnition 1 (Optimal scoring functions) The class of optimal scoring functions is given by the set

S∗ = { s∗ = T ◦ η | T : [0  1] → R strictly increasing }.

Interestingly  it is possible to make the connection between an arbitrary (bounded) optimal scoring
function s∗ ∈ S∗ and the distribution P (through the regression function η) completely explicit.
Proposition 1 (Optimal scoring functions representation  [CV08]) A bounded scoring function
s∗ is optimal if and only if there exist a nonnegative integrable function w and a continuous random
variable V in (0  1) such that:
∀x ∈ X  

s∗(x) = infX s∗ + E (w(V ) · I{η(x) > V }) .

A crucial consequence of the last proposition is that solving the bipartite ranking problem amounts
to recovering the collection {x ∈ X | η(x) > u}u∈(0 1) of level sets of the regression function η.
Hence  the bipartite ranking problem can be seen as a collection of overlaid classiﬁcation problems.
This view was ﬁrst introduced in [CV07] and the present paper is devoted to the description of a
statistical method implementing this idea.

ROC curves. We now recall the concept of ROC curve and explain why it is a natural choice of
performance measure for the ranking problem with classiﬁcation data. We consider here only true
ROC curves which correspond to the situation where the underlying distribution is known. First 
we need to introduce some notations. For a given scoring rule s  the conditional cdfs of the random
variable s(X) are denoted by Gs and Hs. We also set  for all z ∈ R:

¯Gs(z) = 1 − Gs(z) = P{s(X) > z | Y = +1}  
¯Hs(z) = 1 − Hs(z) = P{s(X) > z | Y = −1} .

to be the residual conditional cdfs of the random variable s(X). When s = η  we shall denote the
previous functions by G∗  H∗  ¯G∗  ¯H∗ respectively.
We introduce the notation Q(Z  α) to denote the quantile of order 1 − α for the distribution of a
random variable Z conditioned on the event Y = −1. In particular  the following quantile will be
of interest:

Q∗(α) = Q(η(X)  α) = ¯H∗−1(α)  

2

where we have used here the notion of generalized inverse F −1 of a c`adl`ag function F : F −1(z) =
inf{t ∈ R | F (t) ≥ z}. We now turn to the deﬁnition of the ROC curve.
Deﬁnition 2 (True ROC curve) The ROC curve of a scoring function s is the parametric curve:

z (cid:55)→(cid:0) ¯Hs(z)  ¯Gs(z)(cid:1)

for thresholds z ∈ R. It can also be deﬁned as the plot of the function:

ROC(s 

· ) : α ∈ [0  1] (cid:55)→ ¯Gs ◦ ¯H−1

s (α) = ¯Gs (Q(s(X)  α)) .

By convention  points of the curve corresponding to possible jumps (due to possible degenerate
points of Hs or Gs) are connected by line segments  so that the ROC curve is always continuous.
For s = η  we take the notation ROC∗(α) = ROC(η  α).
The residual cdf ¯Gs is also called the true positive rate while ¯Hs is the false positive rate  so that
the ROC curve is the plot of the true positive rate against the false positive rate.
Note that  as a functional criterion  the ROC curve induces a partial order over the space of all scor-
ing functions. Some scoring function might provide a better ranking on some part of the observation
space and a worst one on some other. A natural step to take is to consider local properties of the
ROC curve in order to focus on best instances but this is not straightforward as explained in [CV07].
We expect optimal scoring functions to be those for which the ROC curve dominates all the others
for all α ∈ (0  1). The next proposition highlights the fact that the ROC curve is relevant when
evaluating performance in the bipartite ranking problem.
Proposition 2 The class S∗ of optimal scoring functions provides the best possible ranking with
respect to the ROC curve. Indeed  for any scoring function s  we have:
∀α ∈ (0  1)   ROC∗(α) ≥ ROC(s  α)  

and ∀s∗ ∈ S∗  ∀α ∈ (0  1)   ROC(s∗  α) = ROC∗(α).
The following result will be needed later.

Proposition 3 We assume that the optimal ROC curve is differentiable. Then  we have  for any α
such that Q∗(α) < 1:

ROC∗(α) =

d
dα

1 − p
p

· Q∗(α)
1 − Q∗(α) .

For proofs of the previous propositions and more details on true ROC curves  we refer to [CV08].

3 Recovering a point on the optimal ROC curve

We consider here the problem of recovering a single point of the optimal ROC curve from a sample
of i.i.d. copies {(Xi  Yi)}i=1 ... n of (X  Y ). This amounts to recovering a single level set of the
regression function η but we aim at controlling the error in terms of rates of false positive and true
positive. For any measurable set C ⊂ X   we set the following notations:

α(C) = P(X ∈ C | Y = −1) and β(C) = P(X ∈ C | Y = +1) .

We also deﬁne the weighted classiﬁcation error:

Lω(C) = 2p(1 − ω) (1 − β(C)) + 2(1 − p)ω α(C)  

with ω ∈ (0  1) being the asymmetry factor.
Proposition 4 The optimal set for this error measure is C∗
for all C ⊂ X :
ω) ≤ Lω(C) .

Lω(C∗

Also the optimal error is given by:

ω = {x : η(x) > ω}. We have indeed 

The excess risk for an arbitrary set C can be written:

Lω(C∗

ω) = 2E min{ω(1 − η(X))  (1 − ω)η(X)} .
ω) = 2E (| η(X) − ω | I{X ∈ C∆C∗

ω})  

Lω(C) − Lω(C∗

where ∆ stands for the symmetric difference between sets.

3

The empirical counterpart of the weighted classiﬁcation error can be deﬁned as:

ˆLω(C) =

2ω
n

I{Yi = −1  Xi ∈ C} +

2(1 − ω)

n

I{Yi = +1  Xi /∈ C} .

This leads to consider the weighted empirical risk minimizer over a class C of candidate sets:

n(cid:88)

i=1

n(cid:88)

i=1

ˆCω = arg min

C∈C

ˆLω(C).

The next result provides rates of of convergence of the weighted empirical risk minimizer ˆCω to the
best set in the class in terms of the two types of error α and β.
Theorem 1 Let ω ∈ (0  1). Assume that C is of ﬁnite VC dimension V and contains C∗
ω. Suppose
also that both G∗ and H∗ are twice continuously differentiable with strictly positive ﬁrst derivatives
and that ROC∗ has a bounded second derivative. Then  for all δ > 0  there exist constants c(V )
independent of ω such that  with probability at least 1 − δ:

c(V )(cid:112)p(1 − ω)
factor term of(cid:112)(1 − p)ω in the denominator instead .

|α( ˆCω) − α(C∗

ω)| ≤

·

(cid:18)log(1/δ)

(cid:19) 1

3

.

n

The same result also holds for the excess risk of ˆCω in terms of the rate β of true positive with a

It is noteworthy that  while convergence in terms of classiﬁcation error is expected to be of the order
of n−1/2  its two components corresponding to the rate of false positive and true positive present
slower rates.

4 Nearly optimal scoring rule based on overlaying classiﬁers

Main result. We now propose to collect the classiﬁers studied in the previous section in order to
build a scoring function for the bipartite ranking problem. From Proposition 1  we can focus on
optimal scoring rules of the form:

s∗(x) =

I{x ∈ C∗

ω} ν(dω) 

(1)

(cid:90)

where the integral is taken w.r.t. any positive measure ν with same support as the distribution of
η(X).
Consider a ﬁxed partition ω0 = 0 < ω1 ≤ . . . ≤ ωK ≤ 1 = ωK+1 of the interval (0  1). We can
then construct an estimator of s∗ by overlaying a ﬁnite collection of (estimated) density level sets
ˆCω1  . . .   ˆCωK :

K(cid:88)

ˆs(x) =

I{x ∈ ˆCωi} 

i=1

which may be seen as an empirical version of a discrete version of the target s∗.
In order to consider the performance of such an estimator  we need to compare the ROC curve of ˆs to
the optimal ROC curve. However  if the sequence { ˆCωi}i=1 ... K is not decreasing  the computation
of the ROC curve as a function of the errors of the overlaying classiﬁers becomes complicated.
The main result of the paper is the next theorem which is proved for a modiﬁed sequence which
yields to a different estimator. We introduce: { ˜Cωi}1≤i≤K deﬁned by:

˜Cω1 = ˆCω1 and ˜Cωi+1 = ˜Cωi ∪ ˆCωi+1 for all i ∈ {1  . . .   K − 1} .

The corresponding scoring function is then given by:

˜sK(x) =

I{x ∈ ˜Cωi} .

(2)

K(cid:88)

i=1

4

Hence  the ROC curve of ˜sK is simply the broken line that connects the knots (α( ˜Cωi)  β( ˜Cωi)) 
0 ≤ i ≤ K + 1.
The next result offers a rate bound in the ROC space  equipped with a sup-norm. Up to our knowl-
edge  this is the ﬁrst result on the generalization ability of decision rules in such a functional space.

Theorem 2 Under the same assumptions as in Theorem 1 and with the previous notations  we set
K = Kn ∼ n1/8. Fix  > 0. Then  there exists a constant c such that  with probability at least
1 − δ  we have:

sup

α∈[ 1−]

|ROC∗(α) − ROC(˜sK  α)| ≤ c log(1/δ)

.

n1/4

Remark 1 (PERFORMANCE OF CLASSIFIERS AND ROC CURVES.) In the present paper  we have
adopted a scoring approach to ROC analysis which is somehow related to the evaluation of the
performance of classiﬁers in ROC space. Using combinations of such classiﬁers to improve perfor-
mance in terms of ROC curves has also been pointed out in [BDH06] and [BCT07].

Remark 2 (PLUG-IN ESTIMATOR OF THE REGRESSION FUNCTION.) Note that taking ν = λ
the Lebesgue measure over [0  1] in the expression of s∗ leads to the regression function η(x) =
ω} dω. An estimator for the regression function could be the following: ˆηK(x) =

(cid:82) I{x ∈ C∗
(cid:80)K+1
i=1 (ωi − ωi−1)I{x ∈ ˜Cωi}.

Remark 3 (ADAPTIVITY OF THE PARTITION.) A natural extension of the approach would be to
consider a ﬂexible partition (ωi)i which could possibly be adaptively chosen depending on the local
regularity of the ROC curve. For now  it is not clear how to extend the method of the paper to
take into account adaptive partitions  however we have investigated such partitions corresponding
to different approximation schemes of the optimal ROC curve elsewhere ([CV08])  but the rates of
convergence obtained in the present paper are faster.

Optimal ROC curve approximation and estimation. We now provide some insights on the pre-
vious result. The key for the proof of Theorem 2 is the idea of a piecewise linear approximation of
the optimal ROC curve.
We introduce some notations. Let ω0 = 0 < ω1 < . . . < ωK < ωK+1 = 1 be a given partition
of [0  1] such that maxi∈{0 ... K}{ωi+1 − ωi} ≤ δ. Set: ∀i ∈ {0  . . .   K + 1}  α∗
) and
β∗
i = β(C∗
The broken line that connects the knots {(α∗
i ); 0 ≤ i ≤ K + 1} provides a piecewise linear
(concave) approximation/interpolation of the optimal ROC curve ROC∗. In the spirit of the ﬁnite
element method (FEM  see [dB01] for instance)  we introduce the ”hat functions” deﬁned by:

i = α(C∗

i   β∗

).

ωi

ωi

∀i ∈ {1  . . .   K − 1}  φ∗

i−1  α∗

i )) − φ( · ; (α∗

i   α∗

i+1)) 

with the notation φ(α  (α1  α2)) = (α − α1)/(α2 − α1) · I{α ∈ [α1  α2]} for all α1 < α2. We
also set φ∗
K  1)) for notational convenience. The piecewise linear approximation
of ROC∗ may then be written as:

K( · ) = φ( · ; (α∗

i ( · ) = φ( · ; (α∗
K(cid:88)

∗

(α) =

(cid:93)ROC

β∗
i φ∗

i (α) .

i=1
∗

In order to obtain an empirical estimator of (cid:93)ROC
true level set C∗
corresponding errors ˆαi and ˆβi using a test sample {(X(cid:48)

(α)  we propose: i) to ﬁnd an estimate ˆCωi of the
ωi based on the training sample {(Xi  Yi)}i=1 ... n as in Section 3  ii) to compute the

i  Y (cid:48)
i = −1} and ˆβi(C) =

n(cid:88)
i = +1} = n − n−. We set ˆαi = ˆαi( ˆCωi) and ˆβi = ˆβi( ˆCωi). We propose

i )}i=1 ... n. Hence we deﬁne:

with n+ =(cid:80)n

1
n−
I{Y (cid:48)

i ∈ C  Y (cid:48)

i ∈ C  Y (cid:48)

i = +1} 

n(cid:88)

ˆαi(C) =

I{X(cid:48)

I{X(cid:48)

1
n+

i=1

i=1

i=1

the following estimator of (cid:93)ROC

∗

(α):

(cid:92)ROC∗(α) =

ˆβi

ˆφi(α) 

K(cid:88)

i=1

5

where ˆφK(α) = φ(.; (ˆαK  1)) and ˆφi(α) = φ(.; (ˆαi−1  ˆαi)) − φ(.; (ˆαi  ˆαi+1)) for 1 ≤ i < K.
Hence  (cid:91)ROC is the broken line connecting the empirical knots {(ˆαi  ˆβi); 0 ≤ i ≤ K + 1}.
The next result takes the form of a deviation bound for the estimation of the optimal ROC curve.
It quantiﬁes the order of magnitude of a conﬁdence band in supremum norm around an empirical
estimate based on the previous approximation scheme with empirical counterparts.

Theorem 3 Under the same assumptions as in Theorem 1 and with the previous notations  set K =
Kn ∼ n1/6. Fix  > 0. Then  there exists a constant c such that  with probability at least 1 − δ 

(cid:18)log(n/δ)

(cid:19)1/3

.

n

|(cid:92)ROC∗(α) − ROC∗(α)| ≤ c−1

sup

α∈[ 1−]

5 Conclusion

We have provided a strategy based on overlaid classiﬁers to build a nearly-optimal scoring function.
Statistical guarantees are provided in terms of rates of convergence for a functional criterion which
is the ROC space equipped with a supremum norm. This is the ﬁrst theoretical result of this nature.
To conclude  we point out that ROC analysis raises important and novel issues for statistical learning
and we hope that the present contribution gives a ﬂavor of possible research directions.

Appendix - Proof section

Proof of Theorem 1. The idea of the proof is to relate the excess risk in terms of α-error to the excess
risk in terms of weighted classiﬁcation error. First we re-parameterize the weighted classiﬁcation
error. Set C(α) = {x ∈ X | η(x) > Q∗(α)} and:

(cid:96)ω(α) = Lω(C(α)) = 2(1 − p)ω α + 2p(1 − ω)(1 − ROC∗(α))

ω) minimizes (cid:96)ω(α). Denote by (cid:96)∗

Since ROC∗ is assumed to be differentiable and using Proposition 3  it is easy to check that the
ω = (cid:96)ω(α∗). It follows from a Taylor expansion
value α∗ = α(C∗
of (cid:96)ω(α) around α∗ at the second order that there exists α0 ∈ [0  1] such that:
dα2 ROC∗(α0) (α − α∗)2

ω − p(1 − ω) d2

Using also the fact that ROC∗ dominates any other curve of the ROC space  we have: ∀C ⊂ X
measurable  β(C) ≤ ROC∗(α(C)). Also  by assumption  there exists m such that: ∀α ∈ [0  1] 
dα2 ROC∗(α) ≥ −m. Hence  since (cid:96)ω(α( ˆCω)) = Lω( ˆCω)  we have:
d2

(cid:96)ω(α) = (cid:96)∗

(cid:16)

(cid:17)2 ≤

α( ˆCω) − α(C∗
ω)

(cid:16)

1

mp(1 − ω)

Lω( ˆCω) − Lω(C∗
ω)

.

(cid:17)

We have obtained the desired inequality. It remains to get the rate of convergence for the weighted
empirical risk.
Now set: F ∗ = pG∗ + (1 − p)H∗. We observe that: ∀t > 0  P(|η(X) − ω| ≤ t) = F ∗(ω +
t) − F ∗(ω − t) ≤ 2t supu(F ∗)(cid:48)(u). We have thus shown that the distribution satisﬁes a modiﬁed
Tsybakov’s margin condition [Tsy04]  for all ω ∈ [0  1]  of the form:

P(|η(X) − ω| ≤ t) ≤ D t

γ

1−γ .

with γ = 1/2 and D = 2 supu(F ∗)(cid:48)(u). Adapting slightly the argument used in [Tsy04]  [BBL05] 
we have that  under the modiﬁed margin condition  there exists a constant c such that  with proba-
bility 1 − δ:

Lω( ˆCω) − L∗

ω(C∗

ω) ≤ c

(cid:18)log(1/δ)

(cid:19) 1

2−γ

n

.

φ( · ; (˜αi  ˜αi+1)). We then have ROC(˜sK  α) = (cid:80)K

Proof of Theorem 2. We note ˜αi = α( ˜Cωi)  ˜βi = β( ˜Cωi) and also ˜φi( · ) = φ( · ; (˜αi−1  ˜αi)) −
˜φi(α) and we can use the following

˜βi

i=1

6

decomposition  for any α ∈ [0  1]:

ROC∗(α) − ROC(˜sK  α) =

(cid:32)
ROC∗(α) − K(cid:88)

(cid:33)

ROC∗(˜αi) ˜φi(α)

+

K(cid:88)

(ROC∗(˜αi) − ˜βi) ˜φi(α) .

It is well-known folklore in linear approximation theory ([dB01]) that if ˜sK is a piecewise constant
scoring function whose ROC curve interpolates the points {(˜αi  ROC∗(˜αi))}i=0 ... K of the optimal
ROC curve  then we can bound the ﬁrst term (which is positive)  ∀α ∈ [0  1]  by:

i=1

i=1

−1
8

inf

α∈[0 1]

d2

dα2 ROC∗(α) · max
0≤i≤K

(˜αi+1 − ˜αi)2 .

Now  to control the second term  we upper bound the following quantity:

|ROC∗(˜αi) − ˜βi| ≤ sup
α∈[0 1]

d
dα

ROC∗(α) · |˜αi − α∗

i | + |β∗

i − ˜βi|

We further bound: |˜αi − α∗
ﬁrst term  the next lemma will be needed:
Lemma 1 We have  for all k ∈ {1  . . .   K}:

i | ≤ |˜αi − αi| + |αi − α∗

i | where αi = α( ˆCi). In order to deal with the

α( ˜Ck) = α( ˆCk) + (k − 1)OP(n−1/4) .

where the notation OP(1) is used for a r.v. which is bounded in probability.
From the lemma  it follows that: max1≤i≤K |˜αi − αi| = OP(Kn−1/4). We can then use Theorem
1 with δ replaced by δ/K to get that max1≤i≤K |αi − α∗
i | = OP((n−1 log K)1/3). The same
inequalities hold with the β’s. It remains to control the quantity ˜αi+1 − ˜αi. We have:

| ˜αi+1 − ˜αi |≤ max
1≤k≤K

| α( ˆCk) − α( ˆCk−1) | +K OP(n−1/4) .

We have that:

max
1≤k≤K

| α( ˆCk) − α( ˆCk−1) |≤ 2 max
1≤k≤K

| α( ˆCk) − α(C∗

k) | + max
1≤k≤K

| α(C∗

k) − α(C∗

k−1) |

As before  we have that the ﬁrst term is of the order (log K/n)1/3 and since the second derivative
of the optimal ROC curve is bounded  the second term is of the order K−1. Eventually  we choose
K in order to optimize the quantity: K−2 + (log K/n)2/3 + K 2n−1/2 + Kn−1/4 + (log K/n)1/3.
As only the ﬁrst and the third term matter  this leads to the choice of K = Kn ∼ n1/8.
Proof of Lemma 1.
We have that α( ˜C2) = α( ˆC2) + α( ˆC1 \ ˆC2). Therefore  since C∗

2 and observing that

1 ⊂ C∗

α( ˆC1 \ ˆC2) = α((( ˆC1 \ C∗

1 ) ∪ ( ˆC1 ∩ C∗

1 )) \ (( ˆC2 \ C∗

2 ) ∪ ( ˆC2 ∩ C∗

2 ))  

it sufﬁces to use the additivity of the probability measure α(.) to get: α( ˜C2) = α( ˆC2) + OP(n−1/4).
Eventually  errors are stacked and we obtain the result.

Proof of Theorem 3.
We use the following decomposition  for any ﬁxed α ∈ (0  1):

(cid:92)ROC∗(α)−ROC∗(α) =

ROC∗(ˆαi) ˆφi(α)

Therefore  we have by a triangular inequality: ∀α ∈ [0  1] 

(cid:32)
(cid:92)ROC∗(α) − K(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ max

i=1

1≤i≤K

ROC∗(ˆαi) ˆφi(α)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:92)ROC∗(α) − K(cid:88)

i=1

(cid:33)

(cid:32) K(cid:88)

+

i=1

(cid:33)

ROC∗(ˆαi) ˆφi(α) − ROC∗(α)

.

| ˆβi − βi| + |βi − β∗

i | + |ROC∗(α∗

i ) − ROC∗(ˆαi)| .

7

And  by the ﬁnite increments theorem  we have:

|ROC∗(α∗

i ) − ROC∗(ˆαi)| ≤

(cid:33)

ROC∗(α)

(|α∗

i − αi| + |αi − ˆαi|) .

(cid:32)

d
dα

sup
α∈[0 1]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ −1

8

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) K(cid:88)

For the other term  we use the same result on approximation as in the proof of Theorem 2:

d2

inf

α∈[0 1]

i=1
max
0≤i≤K

(ˆαi+1 − ˆαi) ≤ max
0≤i≤K

ROC∗(ˆαi) ˆφi(α) − ROC∗(α)

dα2 ROC∗(α) · max
0≤i≤K
|α∗
i − αi| + 2 max
1≤i≤K
i } is of the
We recall that: max1≤i≤K |ˆαi − αi|. = OP(Kn−1/2). Moreover  max0≤i≤K{α∗
i − αi| is bounded as
order of K−1. And with probability at least 1 − δ  we have that max1≤i≤K |α∗
in Theorem 1  except that δ is replaced by δ/K in the bound. Eventually  we get the generalization
bound: K−2 + (log K/n)1/3  which is optimal for a number of knots: K ∼ n1/6.

|ˆαi − αi| .
i+1 − α∗

i ) + 2 max
1≤i≤K

(ˆαi+1 − ˆαi)2

(α∗

i+1 − α∗

References
[AA07]

J.-Y. Audibert and A.Tsybakov. Fast learning rates for plug-in classiﬁers. Annals of
statistics  35(2):608–633  2007.

[AGH+05] S. Agarwal  T. Graepel  R. Herbrich  S. Har-Peled  and D. Roth. Generalization bounds

[BBL05]

for the area under the ROC curve. J. Mach. Learn. Res.  6:393–425  2005.
S. Boucheron  O. Bousquet  and G. Lugosi. Theory of Classiﬁcation: A Survey of Some
Recent Advances. ESAIM: Probability and Statistics  9:323–375  2005.

[BCT07] M. Barreno  A.A. Cardenas  and J.D. Tygar. Optimal ROC curve for a combination of

[BDH06]

[Cav97]

[CLV08]

[CV07]

[CV08]

classiﬁers. In NIPS’07  2007.
F.R. Bach  D.Heckerman  and Eric Horvitz. Considering cost asymmetry in learning
classiﬁers. Journal of Machine Learning Research  7:1713–1741  2006.
L. Cavalier. Nonparametric estimation of regression level sets. Statistics  29:131–160 
1997.
S. Cl´emenc¸on  G. Lugosi  and N. Vayatis. Ranking and empirical risk minimization of
U-statistics. The Annals of Statistics  36(2):844–874  2008.
S. Cl´emenc¸on and N. Vayatis. Ranking the best instances. Journal of Machine Learning
Research  8:2671–2699  2007.
S. Cl´emenc¸on and N. Vayatis. Tree-structured ranking rules and approximation of the
optimal ROC curve. Technical Report hal-00268068  HAL  2008.
C. de Boor. A practical guide to splines. Springer  2001.
J.P. Egan. Signal Detection Theory and ROC Analysis. Academic Press  1975.

[dB01]
[Ega75]
[FISS03] Y. Freund  R. D. Iyer  R. E. Schapire  and Y. Singer. An efﬁcient boosting algorithm for

[RV06]

[Tsy04]

[vT68]
[WN07]

combining preferences. Journal of Machine Learning Research  4:933–969  2003.
P. Rigollet and R. Vert. Fast rates for plug-in estimators of density level sets. Technical
Report arXiv:math/0611473v2  arXiv:math/0611473v2  2006.
A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statis-
tics  32(1):135–166  2004.
H.L. van Trees. Detection  Estimation  and Modulation Theory  Part I. Wiley  1968.
R. Willett and R. Nowak. Minimax optimal level set estimation. IEEE Transactions on
Image Processing  16(12):2965–2979  2007.

8

,Zeyuan Allen-Zhu
Yuanzhi Li