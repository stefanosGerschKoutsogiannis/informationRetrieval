2008,Kernelized Sorting,Object matching is a fundamental operation in data analysis. It typically requires the definition of a similarity measure between the classes of objects to be matched. Instead  we develop an approach which is able to perform matching by requiring a similarity measure only within each of the classes. This is achieved by maximizing the dependency between matched pairs of observations by means of the Hilbert Schmidt Independence Criterion. This problem can be cast as one of maximizing a quadratic assignment problem with special structure and we present a simple algorithm for finding a locally optimal solution.,Kernelized Sorting

Novi Quadrianto

RSISE  ANU & SML  NICTA

Canberra  ACT  Australia
novi.quad@gmail.com

Alex J. Smola
Yahoo! Research

Santa Clara  CA  USA

alex@smola.org

Le Song
SCS  CMU

Pittsburgh  PA  USA
lesong@cs.cmu.edu

Abstract

Object matching is a fundamental operation in data analysis. It typically requires
the deﬁnition of a similarity measure between the classes of objects to be matched.
Instead  we develop an approach which is able to perform matching by requiring a
similarity measure only within each of the classes. This is achieved by maximizing
the dependency between matched pairs of observations by means of the Hilbert
Schmidt Independence Criterion. This problem can be cast as one of maximizing
a quadratic assignment problem with special structure and we present a simple
algorithm for ﬁnding a locally optimal solution.

1 Introduction

Matching pairs of objects is a fundamental operation of unsupervised learning. For instance  we
might want to match a photo with a textual description of a person  a map with a satellite image 
or a music score with a music performance. In those cases it is desirable to have a compatibility
function which determines how one set may be translated into the other. For many such instances
we may be able to design a compatibility score based on prior knowledge or to observe one based
on the co-occurrence of such objects.
In some cases  however  such a match may not exist or it may not be given to us beforehand. That
is  while we may have a good understanding of two sources of observations  say X and Y  we may
not understand the mapping between the two spaces. For instance  we might have two collections of
documents purportedly covering the same content  written in two different languages. Here it should
be our goal to determine the correspondence between both sets and to identify a mapping between
the two domains. In the following we present a method which is able to perform such matching
without the need of a cross-domain similarity measure.
Our method relies on the fact that one may estimate the dependence between sets of random variables
even without knowing the cross-domain mapping. Various criteria are available. We choose the
Hilbert Schmidt Independence Criterion between two sets and we maximize over the permutation
group to ﬁnd a good match. As a side-effect we obtain an explicit representation of the covariance.
We show that our method generalizes sorting. When using a different measure of dependence 
namely an approximation of the mutual information  our method is related to an algorithm of [1].
Finally  we give a simple approximation algorithm for kernelized sorting.

1.1 Sorting and Matching
The basic idea underlying our algorithm is simple. Denote by X = {x1  . . .   xm} ⊆ X and Y =
{y1  . . .   ym} ⊆ Y two sets of observations between which we would like to ﬁnd a correspondence.
That is  we would like to ﬁnd some permutation π ∈ Πm on m terms  that is

Πm :=

π|π ∈ {0  1}m×m and π1m = 1m and π(cid:62)1m = 1m

(cid:110)

(cid:111)

 

(1)

such that the pairs Z(π) :=(cid:8)(xi  yπ(i)) for 1 ≤ i ≤ m(cid:9) correspond to dependent random variables.

Here 1m ∈ Rm is the vector of all ones. We seek a permutation π such that the mapping xi → yπ(i)
and its converse mapping from y to x are simple. Denote by D(Z(π)) a measure of the dependence
between x and y. Then we deﬁne nonparametric sorting of X and Y as follows

(2)
This paper is concerned with measures of D and approximate algorithms for (2). In particular we
will investigate the Hilbert Schmidt Independence Criterion and the Mutual Information.

π∗ := argmaxπ∈Πm D(Z(π)).

2 Hilbert Schmidt Independence Criterion

Let sets of observations X and Y be drawn jointly from some probability distribution Prxy. The
Hilbert Schmidt Independence Criterion (HSIC) [2] measures the dependence between x and y by
computing the norm of the cross-covariance operator over the domain X× Y in Hilbert Space. It can
be shown  provided the Hilbert Space is universal  that this norm vanishes if and only if x and y are
independent. A large value suggests strong dependence with respect to the choice of kernels.
Formally  let F be the Reproducing Kernel Hilbert Space (RKHS) on X with associated kernel
k : X× X → R and feature map φ : X → F. Let G be the RKHS on Y with kernel l and feature map
ψ. The cross-covariance operator Cxy : G (cid:55)→ F is deﬁned by [3] as

Cxy = Exy[(φ(x) − µx) ⊗ (ψ(y) − µy)] 

(3)
where µx = E[φ(x)]  µy = E[ψ(y)]  and ⊗ is the tensor product. HSIC  denoted as D  is then
deﬁned as the square of the Hilbert-Schmidt norm of Cxy [2] via D(F  G  Prxy) := (cid:107)Cxy(cid:107)2
HS. In
term of kernels HSIC can be expressed as
Exx(cid:48)yy(cid:48)[k(x  x(cid:48))l(y  y(cid:48))] + Exx(cid:48)[k(x  x(cid:48))]Eyy(cid:48)[l(y  y(cid:48))] − 2Exy[Ex(cid:48)[k(x  x(cid:48))]Ey(cid:48)[l(y  y(cid:48))]] 

(4)
is the expectation over both (x  y) ∼ Prxy and an additional pair of vari-
where Exx(cid:48)yy(cid:48)
ables (x(cid:48)  y(cid:48)) ∼ Prxy drawn independently according to the same law. Given a sample Z =
{(x1  y1)  . . .   (xm  ym)} of size m drawn from Prxy an empirical estimate of HSIC is

D(F  G  Z) = (m − 1)−2 tr HKHL = (m − 1)−2 tr ¯K ¯L.

(5)
where K  L ∈ Rm×m are the kernel matrices for the data and the labels respectively  i.e. Kij =
k(xi  xj) and Lij = l(yi  yj). Moreover  Hij = δij − m−1 centers the data and the labels in feature
space. Finally  ¯K := HKH and ¯L := HLH denote the centered versions of K and L respectively.
Note that (5) is a biased estimate where the expectations with respect to x  x(cid:48)  y  y(cid:48) have all been
replaced by empirical averages over the set of observations.

2.1 Kernelized Sorting

Previous work used HSIC to measure independence between given random variables [2]. Here we
use it to construct a mapping between X and Y by permuting Y to maximize dependence. There
are several advantages in using HSIC as a dependence criterion. First  HSIC satisﬁes concentration
of measure conditions [2]. That is  for random draws of observation from Prxy  HSIC provides
values which are very similar. This is desirable  as we want our mapping to be robust to small
changes. Second  HSIC is easy to compute  since only the kernel matrices are required and no
density estimation is needed. The freedom of choosing a kernel allows us to incorporate prior
knowledge into the dependence estimation process. The consequence is that we are able to generate
a family of methods by simply choosing appropriate kernels for X and Y .
Lemma 1 The nonparametric sorting problem is given by π∗ = argmaxπ∈Πm tr ¯Kπ(cid:62) ¯Lπ.
Proof We only need to establish that Hπ = πH since the rest follows from the deﬁnition of (5).
Note that since H is a centering matrix  it has the eigenvalue 0 for the vector of all ones and the
eigenvalue 1 for all vectors orthogonal to that. Next note that the vector of all ones is also an eigen-
vector of any permutation matrix π with π1 = 1. Hence H and π matrices commute.
Next we show that the objective function is indeed reasonable: for this we need the following in-
equality due to Polya  Littlewood and Hardy:

Lemma 2 Let a  b ∈ Rm where a is sorted ascendingly. Then a(cid:62)πb is maximized for π = argsort b.
Lemma 3 Let X = Y = R and let k(x  x(cid:48)) = xx(cid:48) and l(y  y(cid:48)) = yy(cid:48). Moreover  assume that x is
sorted ascendingly. In this case (5) is maximized by either π = argsort y or by π = argsort −y.

Proof Under the assumptions we have that ¯K = Hxx(cid:62)H and ¯L = Hyy(cid:62)H. Hence we may

rewrite the objective as(cid:2)(Hx)(cid:62)π(Hy)(cid:3)2. This is maximized by sorting Hy ascendingly. Since the

centering matrix H only changes the offset but not the order this is equivalent to sorting y. We have
two alternatives  since the objective function is insensitive to sign reversal of y.
This means that sorting is a special case of kernelized sorting  hence the name. In fact  when solving
the general problem  it turns out that a projection onto the principal eigenvectors of ¯K and ¯L is a
good initialization of an optimization procedure.

2.2 Diagonal Dominance

(cid:88)

(cid:88)

KijLuv −

2

m(m−1)2

i(cid:54)=j u(cid:54)=v

(cid:88)

In some cases the biased estimate of HSIC as given in (5) leads to very undesirable results  in
particular in the case of document analysis. This is the case since kernel matrices on texts tend to be
diagonally dominant: a document tends to be much more similar to itself than to others. In this case
the O(1/m) bias of (5) is signiﬁcant. Unfortunately  the minimum variance unbiased estimator [2]
does not have a computationally appealing form. This can be addressed as follows at the expense of
a slightly less efﬁcient estimator with a considerably reduced bias: we replace the expectations (4)
by sums where no pairwise summation indices are identical. This leads to the objective function

1

m(m−1)

KijLij +

1

m2(m−1)2

i(cid:54)=j

i j(cid:54)=i v(cid:54)=i

KijLiv.

(6)

This estimator still has a small degree of bias  albeit signiﬁcantly reduced since it only arises from
the product of expectations over (potentially) independent random variables. Using the shorthand
˜Kij = Kij(1− δij) and ˜Lij = Lij(1− δij) for kernel matrices where the main diagonal terms have
been removed we arrive at the expression (m − 1)−2 tr H ˜LH ˜K. The advantage of this term is that
it can be used as a drop-in replacement in Lemma 1.

2.3 Mutual Information

An alternative  natural means of studying the dependence between random variables is to compute
the mutual information between the random variables xi and yπ(i).
In general  this is difﬁcult 
since it requires density estimation. However  if we assume that x and y are jointly normal in the
Reproducing Kernel Hilbert Spaces spanned by the kernels k  l and k · l we can devise an effec-
tive approximation of the mutual information. Our reasoning relies on the fact that the differential
entropy of a normal distribution with covariance Σ is given by
2 log |Σ| + constant.

(7)
Since the mutual information between random variables X and Y is I(X  Y ) = h(X) + h(Y ) −
h(X  Y ) we will obtain maximum mutual information by minimizing the joint entropy h(X  Y ).
Using the Gaussian upper bound on the joint entropy we can maximize a lower bound on the mutual
information by minimizing the joint entropy of J(π) := h(X  Y ). By deﬁning a joint kernel on
X × Y via k((x  y)  (x(cid:48)  y(cid:48))) = k(x  x(cid:48))l(y  y(cid:48)) we arrive at the optimization problem

h(p) = 1

argminπ∈Πm log |HJ(π)H| where Jij = KijLπ(i) π(j).

(8)

Note that this is related to the optimization criterion proposed by Jebara [1] in the context of sorting
via minimum volume PCA. What we have obtained here is an alternative derivation of Jebara’s
criterion based on information theoretic considerations. The main difference is that [1] uses the
setting to align bags of observations by optimizing log |HJ(π)H| with respect to re-ordering within
each of the bags. We will discuss multi-variable alignment at a later stage.
In terms of computation (8) is considerably more expensive to optimize. As we shall see  for the
optimization in Lemma 1 a simple iteration over linear assignment problems will lead to desirable
solutions  whereas in (8) even computing derivatives is a computational challenge.

3 Optimization

DC Programming To ﬁnd a local maximum of the matching problem we may take recourse to a
well-known algorithm  namely DC Programming [4] which in machine learning is also known as
the Concave Convex Procedure [5]. It works as follows: for a given function f(x) = g(x) − h(x) 
where g is convex and h is concave  a lower bound can be found by

f(x) ≥ g(x0) + (cid:104)x − x0  ∂xg(x0)(cid:105) − h(x).

(9)
This lower bound is convex and it can be maximized effectively over a convex domain. Subsequently
one ﬁnds a new location x0 and the entire procedure is repeated.
Lemma 4 The function tr ¯Kπ(cid:62) ¯Lπ is convex in π.
Since ¯K  ¯L (cid:23) 0 we may factorize them as ¯K = U(cid:62)U and ¯L = V (cid:62)V we may rewrite the objective

(cid:110)
M ∈ Rm×m where Mij ≥ 0 and (cid:88)

function as(cid:13)(cid:13)V πU(cid:62)(cid:13)(cid:13)2 which is clearly a convex quadratic function in π.
Mij = 1 and (cid:88)
(cid:2)tr ¯Kπ(cid:62) ¯Lπi
(cid:3)

πi+1 = (1 − λ)πi + λ argmaxπ∈Pm

has only integral vertices  namely admissible permutation matrices. This means that the following
procedure will generate a succession of permutation matrices which will yield a local maximum for
the assignment problem:

(11)
Here we may choose λ = 1 in the last step to ensure integrality. This optimization problem is well
known as a Linear Assignment Problem and effective solvers exist for it [6].

Note that the set of feasible permutations π is constrained in a unimodular fashion  that is  the set

Pm :=

(cid:111)

i

Mij = 1

j

(10)

Lemma 5 The algorithm described in (11) for λ = 1 terminates in a ﬁnite number of steps.

We know that the objective function may only increase for each step of (11). Moreover  the solution
set of the linear assignment problem is ﬁnite. Hence the algorithm does not cycle.

Nonconvex Maximization When using the bias corrected version of the objective function the
problem is no longer guaranteed to be convex. In this case we need to add a line-search procedure
along λ which maximizes tr H ˜KH[(1 − λ)πi + λˆπi](cid:62)H ˜LH[(1 − λ)πi + λˆπi]. Since the function
is quadratic in λ we only need to check whether the search direction remains convex in λ; otherwise
we may maximize the term by solving a simple linear equation.

Initialization Since quadratic assignment problems are in general NP hard we may obviously not
hope to achieve an optimal solution. That said  a good initialization is critical for good estimation
performance. This can be achieved by using Lemma 3. That is  if ¯K and ¯L only had rank-1  the
problem could be solved by sorting X and Y in matching fashion. Instead  we use the projections
onto the ﬁrst principal vectors as initialization in our experiments.

Relaxation to a constrained eigenvalue problem Yet another alternative is to ﬁnd an approximate
solution of the problem in Lemma 1 by solving

maximizeη η(cid:62)M η subject to Aη = b

(12)
Here the matrix M = ¯K ⊗ ¯L ∈ Rm2×m2 is given by the outer product of the constituting kernel
matrices  η ∈ Rm2 is a vectorized version of the permutation matrix π  and the constraints imposed
by A and b amount to the polytope constraints imposed by Πm. This is essentially the approach
proposed by [7] in the context of balanced graph matching  albeit with a suboptimal optimization
procedure. Instead  one may use the exact algorithm proposed by [8].
The problem with the relaxation (12) is that it does not scale well to large estimation problems (the
size of the optimization problem scales O(m4)) and that the relaxation does not guarantee a feasible
solution which means that subsequent projection heuristics need to be found. Hence we did not
pursue this approach in our experiments.

4 Multivariate Extensions

(cid:35)

T(cid:89)

(cid:34) T(cid:89)

(cid:35)

T(cid:89)

(cid:35)

(cid:33)

A natural extension is to align several sets of observations. For this purpose we need to introduce a
multivariate version of the Hilbert Schmidt Independence Criterion. One way of achieving this goal
is to compute the Hilbert Space norm of the difference between the expectation operator for the joint
distribution and the expectation operator for the product of the marginal distributions.
Formally  let there be T random variables xi ∈ Xi which are jointly drawn from some distribution
p(x1  . . .   xm). Moreover  denote by ki : Xi × Xi → R the corresponding kernels. In this case we
can deﬁne a kernel on X1 ⊗ . . . ⊗ XT by k1 · . . . kT . The expectation operator with respect to the
joint distribution and with respect to the product of the marginals is given by [2]

Ex1 ... xT

ki(xi ·)

and

Exi [ki(xi ·)]

(13)

i=1

i=1

(cid:34) T(cid:89)

respectively. Both terms are equal if and only if all random variables are independent. The squared
difference between both is given by

ExT

i=1 x(cid:48) T

i=1

ki(xi  x(cid:48)
i)

+

Exi x(cid:48)

i

[ki(xi  x(cid:48)

i)] − 2ExT

i=1

[k(xi  x(cid:48)
i)]

Ex(cid:48)

i

.

(14)

i=1

i=1

i=1

which we refer to as multiway HSIC. A biased empirical estimate of the above is obtained by re-
placing sums by empirical averages. Denote by Ki the kernel matrix obtained from the kernel ki on
the set of observations Xi := {xi1  . . .   xim}. In this case the empirical estimate of (14) is given by

(cid:34) T(cid:89)

(cid:33)

(cid:32) T(cid:75)

i=1

T(cid:89)

i=1

(cid:32) T(cid:75)

i=1

HSIC[X1  . . .   XT ] := 1(cid:62)

m

Ki

1m +

mKi1m − 2 · 1(cid:62)
1(cid:62)

m

Ki1m

(15)

i Kiπi.

where (cid:12)T
t=1∗ denotes elementwise product of its arguments (the ’.*’ notation of Matlab). To apply
this to sorting we only need to deﬁne T permutation matrices πi ∈ Πm and replace the kernel
matrices Ki by π(cid:62)
Without loss of generality we may set π1 = 1  since we always have the freedom to ﬁx the order of
one of the T sets with respect to which the other sets are to be ordered. In terms of optimization the
same considerations as presented in Section 3 apply. That is  the objective function is convex in the
permutation matrices πi and we may apply DC programming to ﬁnd a locally optimal solution. The
experimental results for multiway HSIC can be found in the appendix.

5 Applications

To investigate the performance of our algorithm (it is a fairly nonstandard unsupervised method) we
applied it to a variety of different problems ranging from visualization to matching and estimation.
In all our experiments  the maximum number of iterations used in the updates of π is 100 and we
terminate early if progress is less than 0.001% of the objective function.

5.1 Data Visualization

In many cases we may want to visualize data according to the metric structure inherent in it. In
particular  we want to align it according to a given template  such as a grid  a torus  or any other
ﬁxed structure. Such problems occur when presenting images or documents to a user. While there
is a large number of algorithms for low dimensional object layout (self organizing maps  maximum
variance unfolding  local-linear embedding  generative topographic map  . . . )  most of them suffer
from the problem that the low dimensional presentation is nonuniform. This has the advantage of
revealing cluster structure but given limited screen size the presentation is undesirable.
Instead  we may use kernelized sorting to align objects. Here the kernel matrix L is given by the
similarity measure between the objects xi that are to be aligned. The kernel K  on the other hand 
denotes the similarity between the locations where objects are to be aligned to. For the sake of
simplicity we used a Gaussian RBF kernel between the objects to laid out and also between the

Figure 1: Layout of 284 images into a ‘NIPS 2008’ letter grid using kernelized sorting.

positions of the grid  i.e. k(x  x(cid:48)) = exp(−γ (cid:107)x − x(cid:48)(cid:107)2). The kernel width γ was adjusted to the
inverse median of (cid:107)x − x(cid:48)(cid:107)2 such that the argument of the exponential is O(1). Our choice of the
Gaussian RBF kernel is likely not optimal for the speciﬁc set of observations (e.g. SIFT feature
extraction followed by a set kernel would be much more appropriate for images). That said we want
to emphasize that the gains arise from the algorithm rather than a speciﬁc choice of a function class.
We obtained 284 images from http://www.flickr.com which were resized and downsampled
to 40 × 40 pixels. We converted the images from RGB into Lab color space  yielding 40 × 40 × 3
dimensional objects. The grid  corresponding to X is a ‘NIPS 2008’ letters on which the images
are to be laid out. After sorting we display the images according to their matching coordinates
(Figure 1). We can see images with similar color composition are found at proximal locations.
We also lay out the images (we add 36 images to make the number 320) into a 2D grid of 16 ×
20 mesh using kernelized sorting. For comparison we use a Self-Organizing Map (SOM) and a
Generative Topographic Mapping (GTM) and the results are shown in the appendix. Although the
images are also arranged according to the color grading  the drawback of SOM (and GTM) is that it
creates blank spaces in the layout. This is because SOM maps several images into the same neuron.
Hence some neurons may not have data associated with them. While SOM is excellent in grouping
similar images together  it falls short in exactly arranging the images into 2D grid.

5.2 Matching

To obtain more quantiﬁable results rather than just generally aesthetically pleasing pictures we apply
our algorithm to matching problems where the correct match is known.
Image matching: Our ﬁrst test was to match image halves. For this purpose we used the data
from the layout experiment and we cut the images into two 20 × 40 pixel patches. The aim was to
ﬁnd an alignment between both halves such that the dependence between them is maximized. In
other words  given xi being the left half of the image and yi being the right half  we want to ﬁnd a
permutation π which lines up xi and yi.
This would be a trivial undertaking when being able to compare the two image halves xi and yi.
While such comparison is clearly feasible for images where we know the compatibility function  it
may not be possible for generic objects. The ﬁgure is presented in the appendix. For a total of 320
images we recovered 140 pairs. This is quite respectable given that chance level would be 1 correct
pair (a random permutation matrix has on expectation one nonzero diagonal entry).
Estimation In a next experiment we aim to determine how well the overall quality of the matches is.
That is  whether the objects matched share similar properties. For this purpose we used binary  multi-

Type
Binary

Multiclass

Regression

Table 1: Error rate for matching problems

Data set
australian
breastcancer
derm
optdigits
wdbc
satimage
segment
vehicle
abalone
bodyfat

m
690
683
358
765
569
620
693
423
417
252

Kernelized Sorting Baseline
0.49
0.46
0.43
0.49
0.47
0.80
0.86
0.75
18.7
7.20

0.29±0.02
0.06±0.01
0.08±0.01
0.01±0.00
0.11±0.04
0.20±0.01
0.58±0.02
0.58±0.08
13.9±1.70
4.5±0.37

Reference
0.21±0.04
0.06±0.03
0.00±0.00
0.01±0.00
0.05±0.02
0.13±0.04
0.05±0.02
0.24±0.07
6.44±3.14
3.80±0.76

Table 2: Number of correct matches (out of 300) for English aligned documents.
De
Source language
95
Kernelized Sorting
Baseline (length match)
4
284
Reference (dictionary)

Es
218
12
298

Pt
252
9
298

Sv
150
6
296

Fr
246
8
298

Da
230
6
297

It
237
11
300

Nl
223
7
298

class  and regression datasets from the UCI repository http://archive.ics.uci.edu/ml
and the LibSVM site http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools.
In our setup we split the dimensions of the data into two sets and permute the data in the second
set. The so-generated two datasets are then matched and we use the estimation error to quantify the
quality of the match. That is  assume that yi is associated with the observation xi. In this case we
compare yi and yπ(i) using binary classiﬁcation  multiclass  or regression loss accordingly.
To ensure good dependence between the subsets of variables we choose a split which ensures corre-
lation. This is achieved as follows: we pick the dimension with the largest correlation coefﬁcient as
a reference. We then choose the coordinates that have at least 0.5 correlation with the reference and
split those equally into two sets  set A and set B. We also split the remainder coordinates equally
into the two existing sets and ﬁnally put the reference coordinate into set A. This ensures that the set
B of dimensions will have strong correlation with at least one dimension in the set A. The listing of
the set members for different datasets can be found in the appendix.
The results are summarized in Table 1. As before  we use a Gaussian RBF kernel with median
adjustment of the kernel width. To obtain statistically meaningful results we subsample 80% of the
data 10 times and compute the error of the match on the subset (this is done in lieu of cross-validation
since the latter is meaningless for matching). As baseline we compute the expected performance of
random permutations which can be done exactly. Finally  as reference we use SVM classiﬁcation /
regression with results obtained by 10-fold cross-validation. Matching is able to retrieve signiﬁcant
information about the labels of the corresponding classes  in some cases performing as well as a full
classiﬁcation approach.
Multilingual Document Matching To illustrate that kernelized sorting is able to recover nontrivial
similarity relations we applied our algorithm to the matching of multilingual documents. For this
purpose we used the Europarl Parallel Corpus. It is a collection of the proceedings of the Euro-
pean Parliament  dating back to 1996 [9]. We select the 300 longest documents of Danish (Da) 
Dutch (Nl)  English (En)  French (Fr)  German (De)  Italian (It)  Portuguese (Pt)  Spanish (Es)  and
Swedish (Sv). The purpose is to match the non-English documents (source languages) to its English
translations (target language). Note that our algorithm does not require a cross-language dictionary.
In fact  one could use kernelized sorting to generate a dictionary after initial matching has occurred.
In keeping with the choice of a simple kernel we used standard TF-IDF (term frequency - inverse
document frequency) features of a bag of words kernel. As preprocessing we remove stopwords (via
NLTK) and perform stemming using http://snowball.tartarus.org. Finally  the feature
vectors are normalized to unit length in term of (cid:96)2 norm. Since kernel matrices on documents are
notoriously diagonally dominant we use the bias-corrected version of our optimization problem.

As baseline we used a fairly straightforward means of document matching via its length. That is 
longer documents in one language will be most probably translated into longer documents in the
other language. This observation has also been used in the widely adopted sentence alignment
method [10]. As a dictionary-based alternative we translate the documents using Google’s trans-
lation engine http://translate.google.com to ﬁnd counterparts in the source language.
Smallest distance matches in combination with a linear assignment solver are used for the matching.
The experimental results are summarized in Table 2. We describe a line search procedure in Section
3. In practice we ﬁnd that ﬁxing λ at a given step size and choosing the best solution in terms of
the objective function for λ ∈ {0.1  0.2  . . .   1.0} works better. Further details can be found in the
appendix. Low matching performance for the document length-based method might be due to small
variance in the document length after we choose the 300 longest documents. The dictionary-based
method gives near-to-perfect matching performance. Further in forming the dictionary  we do not
perform stemming on English words and thus the dictionary is highly customized to the problem
at hand. Our method produces results consistent to the dictionary-based method with notably low
performance for matching German documents to its English translations. We conclude that the
difﬁculty of German-English document matching is inherent to this dataset [9]. Arguably the results
are quite encouraging as our method uses only a within class similarity measure while still matches
more than 2/3 of what is possible by a dictionary-based method.

6 Summary and Discussion

In this paper  we generalized sorting by maximizing the dependency between matched pairs or
observations by means of the Hilbert Schmidt Independence Criterion. This way we are able to
perform matching without the need of a cross-domain similarity measure. The proposed sorting
algorithm is efﬁcient and it can be applied to a variety of different problems ranging from data
visualization to image and multilingual document matching and estimation. Further examples of
kernelized sorting and of reference algorithms are given in the appendix.

Acknowledgments NICTA is funded through the Australian Government’s Backing Australia’s
Ability initiative  in part through the ARC.This research was supported by the Pascal Network. Parts
of this work were done while LS and AJS were working at NICTA.

References
[1] T. Jebara. Kernelizing sorting  permutation  and alignment for minimum volume PCA.

In
Conference on Computational Learning Theory (COLT)  volume 3120 of LNAI  pages 609–
623. Springer  2004.

[2] A.J. Smola  A. Gretton  L. Song  and B. Sch¨olkopf. A hilbert space embedding for distri-
In E. Takimoto  editor  Algorithmic Learning Theory  Lecture Notes on Computer

butions.
Science. Springer  2007.

[3] K. Fukumizu  F. R. Bach  and M. I. Jordan. Dimensionality reduction for supervised learning

with reproducing kernel Hilbert spaces. J. Mach. Learn. Res.  5:73–99  2004.

[4] T. Pham Dinh and L. Hoai An. A D.C. optimization algorithm for solving the trust-region

subproblem. SIAM Journal on Optimization  8(2):476–505  1988.

[5] A.L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation  15:915–

936  2003.

[6] R. Jonker and A. Volgenant. A shortest augmenting path algorithm for dense and sparse linear

assignment problems. Computing  38:325–340  1987.

[7] T. Cour  P. Srinivasan  and J. Shi. Balanced graph matching. In B. Sch¨olkopf  J. Platt  and
T. Hofmann  editors  Advances in Neural Information Processing Systems 19  pages 313–320.
MIT Press  December 2006.

[8] W. Gander  G.H. Golub  and U. von Matt. A constrained eigenvalue problem.

In Linear

Algebra Appl. 114-115  pages 815–839  1989.

[9] P. Koehn. Europarl: A parallel corpus for statistical machine translation. In Machine Transla-

tion Summit X  pages 79–86  2005.

[10] W. A. Gale and K. W. Church. A program for aligning sentences in bilingual corpora.

In

Meeting of the Association for Computational Linguistics  pages 177–184  1991.

,Tim Roughgarden
Michael Kearns
Daniele Calandriello
Alessandro Lazaric
Marcello Restelli