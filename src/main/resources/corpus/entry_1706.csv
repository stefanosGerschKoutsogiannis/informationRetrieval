2016,Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis,We study the stochastic optimization of canonical correlation analysis (CCA)  whose objective is nonconvex and does not decouple over training samples. Although several stochastic gradient based optimization algorithms have been recently proposed to solve this problem  no global convergence guarantee was provided by any of them. Inspired by the alternating least squares/power iterations formulation of CCA  and the shift-and-invert preconditioning method for PCA  we propose two globally convergent meta-algorithms for CCA  both of which transform the original problem into sequences of least squares problems that need only be solved approximately. We instantiate the meta-algorithms with state-of-the-art SGD methods and obtain time complexities that significantly improve upon that of previous work. Experimental results demonstrate their superior performance.,Efﬁcient Globally Convergent Stochastic

Optimization for Canonical Correlation Analysis

Weiran Wang1∗
1Toyota Technological Institute at Chicago
{weiranwang dgarber nati}@ttic.edu

Jialei Wang2∗

Dan Garber1

Nathan Srebro1

2University of Chicago
jialei@uchicago.edu

Abstract

Σ− 1

2

Σ− 1

2

2

We study the stochastic optimization of canonical correlation analysis (CCA) 
whose objective is nonconvex and does not decouple over training samples. Al-
though several stochastic gradient based optimization algorithms have been re-
cently proposed to solve this problem  no global convergence guarantee was pro-
vided by any of them. Inspired by the alternating least squares/power iterations
formulation of CCA  and the shift-and-invert preconditioning method for PCA  we
propose two globally convergent meta-algorithms for CCA  both of which trans-
form the original problem into sequences of least squares problems that need only
be solved approximately. We instantiate the meta-algorithms with state-of-the-art
SGD methods and obtain time complexities that signiﬁcantly improve upon that
of previous work. Experimental results demonstrate their superior performance.

Introduction

1
Canonical correlation analysis (CCA  [1]) and its extensions are ubiquitous techniques in sci-
entiﬁc research areas for revealing the common sources of variability in multiple views of the
same phenomenon. In CCA  the training set consists of paired observations from two views  de-
noted (x1  y1)  . . .   (xN   yN )  where N is the training set size  xi ∈ Rdx and yi ∈ Rdy for
i = 1  . . .   N. We also denote the data matrices for each view2 by X = [x1  . . .   xN ] ∈ Rdx×N and
Y = [y1  . . .   yN ] ∈ Rdy×N   and d := dx + dy. The objective of CCA is to ﬁnd linear projections
of each view such that the correlation between the projections is maximized:

max
u v

u⊤Σxyv

s.t. u⊤Σxxu = v⊤Σyyv = 1

N XY⊤ is the cross-covariance matrix  Σxx = 1

N XX⊤ + γxI and Σyy = 1
where Σxy = 1
γyI are the auto-covariance matrices  and (γx  γy) ≥ 0 are regularization parameters [2].
We denote by (u∗  v∗) the global optimum of (1)  which can be computed in closed-form. Deﬁne

N YY⊤ +

T := Σ− 1

xx ΣxyΣ− 1

2

yy ∈ Rdx×dy  

and let (φ  ψ) be the (unit-length) left and right singular vector pair associated with T’s largest
singular value ρ1. Then the optimal objective value  i.e.  the canonical correlation between the
views  is ρ1  achieved by (u∗  v∗) = (Σ− 1

yy ψ). Note that

xx φ  Σ− 1

2

2

(1)

(2)

ρ1 = kTk ≤

xx X

yy Y

≤ 1.

Furthermore  we are guaranteed to have ρ1 < 1 if (γx  γy) > 0.

∗The ﬁrst two authors contributed equally.
2We assume that X and Y are centered at the origin for notational simplicity; if they are not  we can center

them as a pre-processing operation.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.
















2



















· log2

· log2

(local)

· log2

1
η

1
η

1
η

1
η

1
η

1
η

2

1
η

1
η




q
q

1

1





�



Table 1: Time complexities of different algorithms for achieving η-suboptimal solution (u  v) to
≥ 1 − η. GD=gradient descent  AGD=accelerated
CCA  i.e.  min
GD  SVRG=stochastic variance reduced gradient  ASVRG=accelerated SVRG. Note ASVRG pro-
vides speedup over SVRG only when ˜κ > N  and we show the dominant term in its complexity.

(u⊤Σxxu∗)2  (v⊤Σyyv∗)2

Algorithm

Least squares solver

Time complexity

AppGrad [3]

CCALin [6]

This work:

Alternating least
squares (ALS)

This work:

Shift-and-invert

preconditioning (SI)

GD

AGD

AGD

SVRG

ASVRG

AGD

SVRG

ASVRG

˜O
˜O
˜O
˜O
˜O
˜O
˜O
˜O

dN ˜κ ρ2
2 · log
ρ2
1−ρ2
dN√˜κ ρ2
2 · log
1−ρ2
ρ2
dN√˜κ
ρ2
1
1−ρ2
ρ2
ρ2
1
1−ρ2
ρ2
2
2

2

d(N + ˜κ)
d√N ˜κ
dN√˜κ
d

N + (˜κ 1
4√˜κ

3

dN

2

ρ2
· log2
1
1−ρ2
ρ2
ρ1−ρ2 · log2
1
ρ1−ρ2
ρ1−ρ2 · log2

)2

1

For large and high dimensional datasets  it is time and memory consuming to ﬁrst explicitly form
the matrix T (which requires eigen-decomposition of the covariance matrices) and then compute its
singular value decomposition (SVD). For such datasets  it is desirable to develop stochastic algo-
rithms that have efﬁcient updates  converges fast  and takes advantage of the input sparsity. There
have been recent attempts to solve (1) based on stochastic gradient descent (SGD) methods [3  4  5] 
but none of these work provides rigorous convergence analysis for their stochastic CCA algorithms.
The main contribution of this paper is the proposal of two globally convergent meta-algorithms for
solving (1)  namely  alternating least squares (ALS  Algorithm 2) and shift-and-invert precondition-
ing (SI  Algorithm 3)  both of which transform the original problem (1) into sequences of least
squares problems that need only be solved approximately. We instantiate the meta algorithms with
state-of-the-art SGD methods and obtain efﬁcient stochastic optimization algorithms for CCA.
In order to measure the alignments between an approximate solution (u  v) and the optimum
(u∗  v∗)  we assume that T has a positive singular value gap Δ := ρ1 − ρ2 ∈ (0  1] so its top
left and right singular vector pair is unique (up to a change of sign).
Table 1 summarizes the time complexities of several algorithms for achieving η-suboptimal align-
ments  where ˜κ =
min(σmin(Σxx)  σmin(Σyy)) is the upper bound of condition numbers of least squares
problems solved in all cases.3 We use the notation ˜O(·) to hide poly-logarithmic dependencies (see
Sec. 3.1.1 and Sec. 3.2.3 for the hidden factors). Each time complexity may be preferrable in certain
regime depending on the parameters of the problem.
Notations We use σi(A) to denote the i-th largest singular value of a matrix A  and use σmax(A)
and σmin(A) to denote the largest and smallest singular values of A respectively.

max(kxik2  kyik2)

max

i

2 Motivation: Alternating least squares
Our solution to (1) is inspired by the alternating least squares (ALS) formulation of CCA [7  Al-
gorithm 5.2]  as shown in Algorithm 1. Let the nonzero singular values of T be 1 ≥ ρ1 ≥ ρ2 ≥
··· ≥ ρr > 0  where r = rank(T) ≤ min(dx  dy)  and the corresponding (unit-length) left and right
singular vector pairs be (a1  b1)  . . .   (ar  br)  with a1=φ and b1 = ψ. Deﬁne

C =

0 T
T⊤ 0

∈ Rd×d.

(3)

3For the ALS meta-algorithm  its enough to consider a per-view conditioning. And when using AGD as the
least squares solver  the time complexities dependends on σmax(Σxx) instead  which is less than maxi �xi�2.

2

q
q



a1
b1

1
2





p
p










2

n


NX

ar
br

n

�


1
2



�

n
n



o
n

 ˜φ0

   ψ0 ← ˜ψ0/

 ˜ψ0

o
o
o

   ψt ← ˜ψt/

 ˜φt

o

 ˜ψt






2

xx ΣxyΣ− 1
yy Σ⊤xyΣ− 1

{(φT   ψT ) → (φ  ψ)}

yy ψt−1
xx φt−1

˜φt/|| ˜φt||
˜ψt/|| ˜ψt||

> 0.4
≥

a1
−b1

˜φ0  ˜ψ0

yy ˜vt.

1√2

(5)

1
2

.

2

2

2

+

γx
2 kuk2 .

(6)

˜φt ← Σ− 1
˜ψt ← Σ− 1

2

Algorithm 1 Alternating least squares for CCA.
Input: Data matrices X ∈ Rdx×N   Y ∈ Rdy×N   regularization parameters (γx  γy).

˜v0 ∈ Rdy .

˜u⊤0 Σxx ˜u0  v0 ← ˜v0/

˜v⊤0 Σyy ˜v0

φ0 ← ˜φ0/

Initialize ˜u0 ∈ Rdx  
u0 ← ˜u0/
for t = 1  2  . . .   T do
xx Σxyvt−1
yy Σ⊤xyut−1

˜ut ← Σ−1
˜vt ← Σ−1
ut ← ˜ut/

end for

˜u⊤t Σxx ˜ut  vt ← ˜vt/

˜v⊤t Σyy ˜vt

φt ← ˜φt/

Output: (uT   vT ) → (u∗  v∗) as T → ∞.

It is straightforward to check that the nonzero eigenvalues of C are:
ρ1 ≥ ··· ≥ ρr ≥ −ρr ≥ ··· ≥ −ρ1 
1√2

  . . .  

1√2

with corresponding eigenvectors 1√2
The key observation is that Algorithm 1 effectively runs a variant of power iterations on C to extract
its top eigenvector. To see this  make the following change of variables

  . . .  

 

.

ar
−br

1
2

xxut 

φt = Σ

(4)
Then we can equivalently rewrite the steps of Algorithm 1 in the new variables as in {} of each line.
Observe that the iterates are updated as follows from step t − 1 to step t:

ψt = Σ

xx ˜ut 

yyvt 

˜ψt = Σ

˜φt = Σ

˜φt
˜ψt

←

0 T
T⊤ 0

φt−1
ψt−1

 

φt
ψt

←

Except for the special normalization steps which rescale the two sets of variables separately  Algo-
rithm 1 is very similar to the power iterations [8].
We show the convergence rate of ALS below (see its proof in Appendix A). The ﬁrst measure of
progress is the alignment of φt to φ and the alignment of ψt to ψ  i.e.  (φ⊤t φ)2 = (u⊤t Σxxu∗)2
and (ψ⊤t ψ)2 = (v⊤t Σyyv∗)2. The maximum value for such alignments is 1  achieved when the
iterates completely align with the optimal solution. The second natural measure of progress is the
objective of (1)  i.e.  u⊤t Σxyvt  with the maximum value being ρ1.
Theorem 1 (Convergence of Algorithm 1). Let µ := min
Then for t ≥ ⌈ ρ2
1−ρ2
ρ2
1 − η  and u⊤t Σxyvt ≥ ρ1(1 − 2η).
Remarks We have assumed a nonzero singular value gap in Theorem 1 to obtain linear conver-
gence in both the alignments and the objective. When there exists no singular value gap  the top
singular vector pair is not unique and it is no longer meaningful to measure the alignments. Nonethe-
less  it is possible to extend our proof to obtain sublinear convergence for the objective in this case.
Observe that  besides the steps of normalization to unit length  the basic operation in each iteration
of Algorithm 1 is of the form ˜ut ← Σ−1
N XY⊤vt−1  which is
equivalent to solving the following regularized least squares (ridge regression) problem

⌉  we have in Algorithm 1 that min

(u⊤0 Σxxu∗)2  (v⊤0 Σyyv∗)2

(u⊤t Σxxu∗)2  (v⊤t Σyyv∗)2

xx Σxyvt−1 = ( 1

N XX⊤ + γxI)−1 1

1
µη

log

2

1

min

u

1
2N

u⊤X − v⊤t−1Y

+

γx
2 kuk2 ≡ min

u

1
N

1
2

i=1

u⊤xi − v⊤t−1yi

In the next section  we show that  to maintain the convergence of ALS  it is unnecessary to solve the
least squares problems exactly. This enables us to use state-of-the-art SGD methods for solving (6)
to sufﬁcient accuracy  and to obtain a globally convergent stochastic algorithm for CCA.

4One can show that µ is bounded away from 0 with high probability using random initialization (u0  v0).

3

p
p
p

q
q

2

2

4

q
q

Algorithm 2 The alternating least squares (ALS) meta-algorithm for CCA.
Input: Data matrices X ∈ Rdx×N   Y ∈ Rdy×N   regularization parameters (γx  γy).

Initialize ˜u0 ∈ Rdx   ˜v0 ∈ Rdy .
˜u0 ← ˜u0/
for t = 1  2  . . .   T do

˜u⊤0 Σxx ˜u0 

˜v0 ← ˜v0/

˜v⊤0 Σyy ˜v0 

u0 ← ˜u0 

v0 ← ˜v0

1
2N

1
2N

u

v

ft(u) :=

u⊤X − v⊤t−1Y

Solve min
approximate solution ˜ut satisfying ft(˜ut) ≤ minu ft(u) + ǫ.
Solve min
approximate solution ˜vt satisfying gt(˜vt) ≤ minv gt(v) + ǫ.
ut ← ˜ut/

v⊤Y − u⊤t−1X

vt ← ˜vt/

˜u⊤t Σxx ˜ut 

˜v⊤t Σyy ˜vt

gt(v) :=

+

+

γx
2 kuk2 with initialization ˜ut−1  and output
γy
2 kvk2 with initialization ˜vt−1  and output

end for

Output: (uT   vT ) is the approximate solution to CCA.

3 Our algorithms

3.1 Algorithm I: Alternating least squares (ALS) with variance reduction
Our ﬁrst algorithm consists of two nested loops. The outer loop runs inexact power iterations while
the inner loop uses advanced stochastic optimization methods  e.g.  stochastic variance reduced
gradient (SVRG  [9]) to obtain approximate matrix-vector multiplications. A sketch of our algorithm
is provided in Algorithm 2. We make the following observations from this algorithm.
Connection to previous work At step t  if we optimize ft(u) and gt(v) crudely by a single batch
gradient descent step from the initialization (˜ut−1  ˜vt−1)  we obtain the following update rule:

˜ut ← ˜ut−1 − 2ξ X(X⊤ ˜ut−1 − Y⊤vt−1)/N 
˜vt ← ˜vt−1 − 2ξ Y(Y⊤ ˜vt−1 − X⊤ut−1)/N 

ut ← ˜ut/
vt ← ˜vt/

˜u⊤t Σxx ˜ut

˜v⊤t Σyy ˜vt

˜u⊤t Σxx ˜ut to ensure the constraints  where ˜u⊤t Σxx ˜ut = 1

where ξ > 0 is the stepsize (assuming γx = γy = 0). This coincides with the AppGrad algorithm
of [3  Algorithm 3]  for which only local convergence is shown. Since the objectives ft(u) and gt(v)
decouple over training samples  it is convenient to apply SGD methods to them. This observation
motivated the stochastic CCA algorithms of [3  4]. We note however  no global convergence guar-
antee was shown for these stochastic CCA algorithms  and the key to our convergent algorithm is to
solve the least squares problems to sufﬁcient accuracy.
Warm-start Observe that for different t  the least squares problems ft(u) only differ in their targets
as vt changes over time. Since vt−1 is close to vt (especially when near convergence)  we may use
˜ut as initialization for minimizing ft+1(u) with an iterative algorithm.
Normalization At the end of each outer loop  Algorithm 2 implements exact normalization of the
form ut ← ˜ut/
N (˜u⊤t X)(˜u⊤t X)⊤ +
γx k˜utk2 requires computing the projection of the training set ˜u⊤t X. However  this does not in-
troduce extra computation because we also compute this projection for the batch gradient used by
SVRG (at the beginning of time step t + 1). In contrast  the stochastic algorithms of [3  4] (possibly
adaptively) estimate the covariance matrix from a minibatch of training samples and use the esti-
mated covariance for normalization. This is because their algorithms perform normalizations after
each update and thus need to avoid computing the projection of the entire training set frequently.
But as a result  their inexact normalization steps introduce noise to the algorithms.
Input sparsity For high dimensional sparse data (such as those used in natural language process-
ing [10])  an advantage of gradient based methods over the closed-form solution is that the former
takes into account the input sparsity. For sparse inputs  the time complexity of our algorithm depends
on nnz(X  Y)  i.e.  the total number of nonzeros in the inputs instead of dN.
Canonical ridge When (γx  γy) > 0  ft(u) and gt(v) are guaranteed to be strongly convex due
to the ℓ2 regularizations  in which case SVRG converges linearly. It is therefore beneﬁcial to use








2
µη

2



.




1
η






















�

�

�









1
η


P
�

�



1
ǫ

2

2



small nonzero regularization for improved computational efﬁciency  especially for high dimensional
datasets where inputs X and Y are approximately low-rank.
Convergence By the analysis of inexact power iterations where the least squares problems are
solved (or the matrix-vector multiplications are computed) only up to necessary accuracy  we pro-
vide the following theorem for the convergence of Algorithm 2 (see its proof in Appendix B). The
key to our analysis is to bound the distances between the iterates of Algorithm 2 and that of Algo-
rithm 1 at all time steps  and when the errors of the least squares problems are sufﬁciently small (at
the level of η2)  the iterates of the two algorithms have the same quality.
Theorem 2 (Convergence of Algorithm 2). Fix T ≥ ⌈ ρ2
1−ρ2
ρ2

⌉  and set ǫ(T ) ≤
Then we have u⊤T ΣxxuT = v⊤T ΣyyvT = 1 

in Algorithm 2.

log

2

2

1

η2ρ2
r
128
min

(2ρ1/ρr)−1
(2ρ1/ρr)T −1

·
(u⊤T Σxxu∗)2  (v⊤T Σyyv∗)2

≥ 1 − η  and u⊤T ΣxyvT ≥ ρ1(1 − 2η).

3.1.1 Stochastic optimization of regularized least squares
We now discuss the inner loop of Algorithm 2  which approximately solves problems of the form (6).
Owing to the ﬁnite-sum structure of (6)  several stochastic optimization methods such as SAG [11] 
SDCA [12] and SVRG [9]  provide linear convergence rates. All these algorithms can be readily ap-
plied to (6); we choose SVRG since it is memory efﬁcient and easy to implement. We also apply the
recently developed accelerations techniques for ﬁrst order optimization methods [13  14] to obtain
an accelerated SVRG (ASVRG) algorithm. We give the sketch of SVRG for (6) in Appendix C.

+ γx

1
ǫ

.

dx (N + κx) log

dx√N κx log

u⊤xi − v⊤yi

where κx = maxikxik2

N
i=1 f i(u) where each component f i(u) = 1
2

2 kuk2
Note that f (u) = 1
N
is kxik2-smooth  and f (u) is σmin(Σxx)-strongly convex5 with σmin(Σxx) ≥ γx. We show in
Appendix D that the initial suboptimality for minimizing ft(u) is upper-bounded by constant when
using the warm-starts. We quote the convergence rates of SVRG [9] and ASVRG [14] below.
Lemma 3. The SVRG algorithm [9] ﬁnds a vector ˜u satisfying6 E[f (˜u)] − minu f (u) ≤ ǫ in time
σmin(Σxx) . The ASVRG algorithm [14] ﬁnds a such solution
O
in time O
Remarks As mentioned in [14]  the acceleration version provides speedup over normal SVRG
only when κx > N and we only show the dominant term in the above complexity.
By combining the iteration complexity of
complexity of
˜O
· log2
for
and ˜O(·) hides poly-logarithmic depen-
ALS+ASVRG  where κ := max
dences on 1
. Our algorithm does not require the initialization to be close to the optimum
and converges globally. For comparison  the locally convergent AppGrad has a time complexity
[3  Theorem 2.1] of ˜O
. Note 
in this complexity  the dataset size N and the least squares condition number κ′ are multiplied to-
gether because AppGrad essentially uses batch gradient descent as the least squares solver. Within
our framework  we can use accelerated gradient descent (AGD  [15]) instead and obtain a globally
convergent algorithm with a total time complexity of ˜O

loop (Theorem 2) and the time
time complexity of

loop (Lemma 3)  we obtain the total

for ALS+SVRG and ˜O

the inner
ρ2
1
1−ρ2
ρ2

maxikxik2
σmin(Σxx)   maxikyik2

σmax(Σxx)

σmin(Σxx)   σmax(Σyy)

σmin(Σyy)

  where κ′ := max

dN√κ′

d√N κ

µ and 1
ρr

ρ2
1
1−ρ2
ρ2

ρ2
1
1−ρ2
ρ2

2

the outer

2 · log

· log2

· log2

1
η

d (N + κ)

2

ρ2
1
1−ρ2
ρ2

2

σmin(Σyy)

dN κ′

1
η

2

3.2 Algorithm II: Shift-and-invert preconditioning (SI) with variance reduction
The second algorithm is inspired by the shift-and-invert preconditioning method for PCA [16  17].
Instead of running power iterations on C as deﬁned in (3)  we will be running power iterations on

Mλ = (λI − C)−1 =

λI
−T⊤

−T
λI

−1

∈ Rd×d 

(7)

5We omit the regularization in these constants  which are typically very small  to have concise expressions.
6The expectation is taken over random sampling of component functions. High probability error bounds

can be obtained using the Markov’s inequality.

5



NX
�
q

1
N

−1

i=1




1
2







1
2

1


�

















1
2

#






"



�






1
2









η2
64
log

˜Δ
18

u
v

 



�

"






1
2

#



(11)

(8)

(9)



where λ > ρ1. It is straightforward to check that Mλ is positive deﬁnite and its eigenvalues are:

λ − ρ1 ≥ ··· ≥

1

λ − ρr ≥ ··· ≥
1√2

ar
br

1

λ + ρr ≥ ··· ≥
1√2

ar
−br

 

λ + ρ1
1√2

a1
b1

  . . .  

with eigenvectors 1√2
The main idea behind shift-and-invert power iterations is that when λ − ρ1 = c(ρ1 − ρ2) with c ∼
O(1)  the relative eigenvalue gap of Mλ is large and so power iterations on Mλ converges quickly.
Our shift-and-invert preconditioning (SI) meta-algorithm for CCA is sketched in Algorithm 3 (in
Appendix E due to space limit) and it proceeds in two phases.

  . . .  

  . . .  

.

a1
−b1

3.2.1 Phase I: shift-and-invert preconditioning for eigenvectors of Mλ
Using an estimate of the singular value gap ˜Δ and starting from an over-estimate of ρ1 (1 + ˜Δ
sufﬁces)  the algorithm gradually shrinks λ(s) towards ρ1 by crudely estimating the leading eigen-
vector/eigenvalues of each Mλ(s) along the way and shrinking the gap λ(s) − ρ1  until we reach
a λ(f ) ∈ (ρ1  ρ1 + c(ρ1 − ρ2)) where c ∼ O(1). Afterwards  the algorithm ﬁxes λ(f ) and runs
inexact power iterations on Mλ(f ) to obtain an accurate estimate of its leading eigenvector. Note

in this phase  power iterations implicitly operate on the concatenated variables 1√2

Σ

Σ

xx ˜ut
yy ˜vt

1
2

and

1√2

1
2

Σ

Σ

xxut
yyvt

1
2

in Rd (but without ever computing Σ

xx and Σ

yy).

Matrix-vector multiplication

ut−1
vt−1
where λ varies over time in order to locate λ(f ). This is equivalent to solving

The matrix-vector multiplications in Phase I have the form
λΣxx −Σxy
−Σ⊤xy
λΣyy

˜ut
˜vt

Σxx

←

Σyy

 

˜ut
˜vt

← min

u v

u⊤v⊤

λΣxx −Σxy
−Σ⊤xy
λΣyy

u
v

− u⊤Σxxut−1 − v⊤Σyyvt−1.

And as in ALS  this least squares problem can be further written as ﬁnite-sum:

min
u v

ht(u  v) =

hi
t(u  v)

where

hi
t(u  v) =

u⊤v⊤

λ

xix⊤i + γxI
−yix⊤i

−xiy⊤i
yiy⊤i + γyI

λ

We could directly apply SGD methods to this problem as before.
Normalization The normalization steps in Phase I have the form

ut
vt

√2

←

˜ut
˜vt

˜u⊤t Σxx ˜ut + ˜v⊤t Σyy ˜vt 

− u⊤Σxxut−1 − v⊤Σyyvt−1.

and so the following remains true for the normalized iterates in Phase I:

for

t = 1  . . .   T.

u⊤t Σxxut + v⊤t Σyyvt = 2 

(10)
Unlike the normalizations in ALS  the iterates ut and vt in Phase I do not satisfy the original CCA
constraints  and this is taken care of in Phase II.
We have the following convergence guarantee for Phase I (see its proof in Appendix F).
Theorem 4 (Convergence of Algorithm 3  Phase I). Let Δ = ρ1 − ρ2 ∈ (0  1]  and ˜µ :=
> 0  and ˜Δ ∈ [c1Δ  c2Δ] where 0 < c1 ≤ c2 ≤ 1. Set
in
4 log

  η4
m1 = ⌈8 log
410
Algorithm 3. Then the (uT   vT ) output by Phase I of Algorithm 3 satisﬁes (10) and

u⊤0 Σxxu∗ + v⊤0 Σyyv∗

⌉  and ˜ǫ ≤ min

⌉  m2 = ⌈ 5

m2−1

m1−1

128
˜µη2

˜Δ
18

16
˜µ

3084

1
4

1

2

(u⊤T Σxxu∗ + v⊤T Σyyv∗)2 ≥ 1 −
and the number of calls to the least squares solver of ht(u  v) is O

1
4

6

1
˜µ

log

1
Δ

+ log

1

˜µη2

.

1



"

q
�






q

Σ




1
2
xx

q

P


"





#







3.2.2 Phase II: ﬁnal normalization
In order to satisfy the CCA constraints  we perform a last normalization

ˆu ← uT /

u⊤T ΣxxuT  

ˆv ← vT /

v⊤T ΣyyvT .

(12)

And we output (ˆu  ˆv) as our ﬁnal approximate solution to (1). We show that this step does not cause
much loss in the alignments  as stated below (see it proof in Appendix G).
Theorem 5 (Convergence of Algorithm 3  Phase II). Let Phase I of Algorithm 3 outputs (uT   vT )
that satisfy (11). Then after (12)  we obtain an approximate solution (ˆu  ˆv) to (1) such that
≥ 1−η  and ˆu⊤Σxy ˆv ≥ ρ1(1−2η).
ˆu⊤Σxx ˆu = ˆv⊤Σyy ˆv = 1  min

(ˆu⊤Σxxu∗)2  (ˆv⊤Σyyv∗)2

3.2.3 Time complexity
We have shown in Theorem 4 that Phase I only approximately solves a small number of instances
of (9). The normalization steps (10) require computing the projections of the traning set which are
reused for computing batch gradients of (9). The ﬁnal normalization (12) is done only once and
costs O(dN ). Therefore  the time complexity of our algorithm mainly comes from solving the least
squares problems (9) using SGD methods in a blackbox fashion. And the time complexity for SGD
methods depends on the condition number of (9). Denote

Qλ =

λΣxx −Σxy
−Σ⊤xy
λΣyy

=

1
2
yy

Σ

λI
−T⊤

−T
λI

1
2
xx

Σ

.

1
2
yy

Σ

(13)

It is clear that

σmax(Qλ) ≤ (λ + ρ1) · max (σmax(Σxx)  σmax(Σyy))  
σmin(Qλ) ≥ (λ − ρ1) · min (σmin(Σxx)  σmin(Σyy)) .

N
i=1 hi

1

ρ1−ρ2

˜κ  where ˜κ :=

λ−ρ1 ≤ 9

˜Δ ≤ 9

σmin(Qλ) ≤ 9/c1
ρ1−ρ2

We have shown in the proof of Theorem 4 that λ+ρ1
c1Δ throughout Algorithm 3 (cf.
Lemma 10  Appendix F.2)  and thus the condtion number for AGD is σmax(Qλ)
˜κ′  where
˜κ′ := max(σmax(Σxx)  σmax(Σyy))
min(σmin(Σxx)  σmin(Σyy)) . For SVRG/ASVRG  the relevant condition number depends on the
gradient Lipschitz constant of individual components. We show in Appendix H (Lemma 12) that the
maxi max(kxik2  kyik2)
relevant condition number is at most 9/c1
min(σmin(Σxx)  σmin(Σyy)) . An interesting
ρ1−ρ2
issue for SVRG/ASVRG is that  depending on the value of λ  the independent components hi
t(u  v)
may be nonconvex. If λ ≥ 1  each component is still guaranteed to by convex; otherwise  some
components might be non-convex  with the overall average 1
t being convex. In the later
N
case  we use the modiﬁed analysis of SVRG [16  Appendix B] for its time complexity. We use warm-
start in SI as in ALS  and the initial suboptimality for each subproblem can be bounded similarly.
The total time complexities of our SI meta-algorithm are given in Table 1. Note that ˜κ (or ˜κ′)
are multiplied together  giving the effective condition number. When using SVRG as
and
the least squares solver  we obtain the total time complexity of ˜O
if all components are convex  and ˜O
)2) · log2
d√N√˜κ
ing ASVRG  we have ˜O
˜O
ρ1−ρ2 · log2
and 1
N from other parameters in the time complexities.
Parallel work In a parallel work [6]  the authors independently proposed a similar ALS algorithm7 
and they solve the least squares problems using AGD. The time complexity of their algorithm for ex-
tracting the ﬁrst canonical correlation is ˜O
  which has linear dependence
(so their algorithm is linearly convergent  but our complexity for ALS+AGD has
on
quadratic dependence on this factor)  but typically worse dependence on N and κ′ (see remarks in
Section 3.1.1). Moreover  our SI algorithm tends to signiﬁcantly outperform ALS theoretically and
empirically. It is future work to remove extra log

if all components are convex  and
otherwise. Here ˜O(·) hides poly-logarithmic dependences on 1
dN
Δ . It is remarkable that the SI meta-algorithm is able to separate the dependence of dataset size

d(N + (˜κ 1
ρ1−ρ2 · log2

d(N + ˜κ 1
1
η

dependence in our analysis.

otherwise. When us-

dN√κ′

ρ2
1
ρ2
1−ρ2

2 · log

) · log2

ρ2
1
ρ2
1−ρ2

2

ρ1−ρ2

1
η

log

1
η

3

4√˜κ

1

ρ1−ρ2

1
η

1
η

1
η

1

1
η

7Our arxiv preprint for the ALS meta-algorithm was posted before their paper got accepted by ICML 2016.

7


q






#





˜µ



0

0

0



0

0

0

γx = γy = 10−3

γx = γy = 10−2

κ′ = 534.4  δ = 4.256

κ′ = 54.34  δ = 2.548

0

10
S-AppGrad

0

10
S-AppGrad

CCALin

AppGrad

SI-AVR

ALS-AVR

ALS-VR

SI-VR

100

200

300

400

500

600

-5

10

-10

10

-15

10

SI-AVR

AppGrad

ALS-VR

SI-VR

CCALin

ALS-AVR

100

200

300

400

500

600

κ′ = 34070  δ = 10.58

κ′ = 3416  δ = 9.082

10

0
CCALin

0

S-AppGrad
10

AppGrad

SI-AVR

ALS-VR

S-AppGrad

ALS-AVR

100

200

300

400

500

600

SI-VR

-5

10

-10

10

-15

10

ALS-VR

AppGrad

CCALin

ALS-AVR

SI-AVR

SI-VR

100

200

300

400

500

600

κ′ = 22350  δ = 12.30

κ′ = 2236  δ = 9.874

0

10

ALS-VR

0

10
S-AppGrad

AppGrad

CCALin

S-AppGrad

ALS-AVR

SI-VR

SI-AVR

100

200

300

400

500

600

# Passes

-5

10

-10

10

-15

10

AppGrad

CCALin

ALS-VR

ALS-AVR

SI-VR

SI-AVR

100

200

300

400

500

600

# Passes

γx = γy = 10−5

κ′ = 53340  δ = 5.345

γx = γy = 10−4

κ′ = 5335  δ = 4.924

10

0
CCALin

-2

10

SI-AVR

-4

10

-6

10

AppGrad

CCALin

S-AppGrad

0

10

AppGrad

SI-VR

ALS-VR

S-AppGrad

-5

10

SI-AVR

ALS-AVR

-10

10

-15

10

ALS-AVR

SI-VR

ALS-VR

0

100

200

300

400

500

600

0

100

200

300

400

500

600

κ′ = 2699000  δ = 11.22

κ′ = 332800  δ = 11.10

0

10

CCALin

-1

10

SI-AVR

-2

10

AppGrad

0

10

-2

10

AppGrad

S-AppGrad

SI-VR

-3

10

SI-VR

ALS-VR

ALS-AVR

-4

10

-4

10

-6

10

CCALin

S-AppGrad

ALS-VR

ALS-AVR

SI-AVR

0

100

200

300

400

500

600

0

100

200

300

400

500

600

κ′ = 2235000  δ = 12.82

κ′ = 223500  δ = 12.75

0

10

-2

10

-4

10

-6

10

AppGrad

0

10

CCALin

AppGrad

ALS-AVR

S-AppGrad

CCALin

ALS-AVR

S-AppGrad

-5

10

ALS-VR

ALS-VR

-10

10

SI-AVR

SI-VR

SI-VR

SI-AVR

-15

10

0

100

200

300

400

500

600

0

100

200

300

400

500

600

# Passes

# Passes

l
l
i

m
a
i
d
e
M

y
t
i
l
a
m

i
t
p
o
b
u
S

1
1
W

J

y
t
i
l
a
m

i
t
p
o
b
u
S

y
t
i
l
a
m

i
t
p
o
b
u
S

T
S
I
N
M

-5

10

-10

10

-15

10

-5

10

-10

10

-5

10

-10

10

-15

10

Figure 1: Comparison of suboptimality vs. # passes for different algorithms. For each dataset and
regularization parameters (γx  γy)  we give κ′ = max

σmin(Σxx)   σmax(Σyy)

σmax(Σxx)

σmin(Σyy)

.

and δ = ρ2
ρ2
1−ρ2

2

1

Extension to multi-dimensional projections To extend our algorithms to L-dimensional projec-
tions  we can extract the dimensions sequentially and remove the explained correlation from Σxy
each time we extract a new dimension [18]. For the ALS meta-algorithm  a cleaner approach is
to extract the L dimensions simultaneously using (inexact) orthogonal iterations [8]  in which case
the subproblems become multi-dimensional regressions and our normalization steps are of the form
Ut ← ˜Ut( ˜U⊤t Σxx ˜Ut)− 1
2 (the same normalization is used by [3  4]). Such normalization involves
the eigenvalue decomposition of a L × L matrix and can be solved exactly as we typically look
for low dimensional projections. Our analysis for L = 1 can be extended to this scenario and the
convergence rate of ALS will depend on the gap between ρL and ρL+1.

4 Experiments
We demonstrate the proposed algorithms  namely ALS-VR  ALS-AVR  SI-VR  and SI-AVR  abbre-
viated as “meta-algorithm – least squares solver” (VR for SVRG  and AVR for ASVRG) on three
real-world datasets: Mediamill [19] (N = 3 × 104)  JW11 [20] (N = 3 × 104)  and MNIST [21]
(N = 6 × 104). We compare our algorithms with batch AppGrad and its stochastic version
s-AppGrad [3]  as well as the CCALin algorithm in parallel work [6]. For each algorithm  we
compare the canonical correlation estimated by the iterates at different number of passes over the
data with that of the exact solution by SVD. For each dataset  we vary the regularization parameters
γx = γy over {10−5  10−4  10−3  10−2} to vary the least squares condition numbers  and larger
regularization leads to better conditioning. We plot the suboptimality in objective vs. # passes for
each algorithm in Figure 1. Experimental details (e.g. SVRG parameters) are given in Appendix I.
We make the following observations from the results. First  the proposed stochastic algorithms sig-
niﬁcantly outperform batch gradient based methods AppGrad/CCALin. This is because the least
squares condition numbers for these datasets are large  and SVRG enable us to decouple depen-
dences on the dataset size N and the condition number κ in the time complexity. Second  SI-VR
converges faster than ALS-VR as it further decouples the dependence on N and the singular value gap
of T. Third  inexact normalizations keep the s-AppGrad algorithm from converging to an accurate
solution. Finally  ASVRG improves over SVRG when the the condition number is large.

Acknowledgments
Research partially supported by NSF BIGDATA grant 1546500.

8

References

[1] H. Hotelling. Relations between two sets of variates. Biometrika  28(3/4):321–377  1936.
[2] H. D. Vinod. Canonical ridge and econometrics of joint production. J. Econometrics  1976.
[3] Z. Ma  Y. Lu  and D. Foster. Finding linear structure in large datasets with scalable canonical

correlation analysis. In ICML  2015.

[4] W. Wang  R. Arora  N. Srebro  and K. Livescu. Stochastic optimization for deep CCA via

nonlinear orthogonal iterations. In ALLERTON  2015.

[5] B. Xie  Y. Liang  and L. Song. Scale up nonlinear component analysis with doubly stochastic

gradients. In NIPS  2015.

[6] R. Ge  C. Jin  S. Kakade  P. Netrapalli  and A. Sidford. Efﬁcient algorithms for large-scale
generalized eigenvector computation and canonical correlation analysis. arXiv  April 13 2016.
[7] G. Golub and H. Zha. Linear Algebra for Signal Processing  chapter The Canonical Correla-

tions of Matrix Pairs and their Numerical Computation  pages 27–49. 1995.

[8] G. Golub and C. van Loan. Matrix Computations. third edition  1996.
[9] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In NIPS  2013.

[10] Y. Lu and D. Foster. Large scale canonical correlation analysis with iterative least squares. In

NIPS  2014.

[11] M. Schmidt  N. Le Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. Technical Report HAL 00860051  École Normale Supérieure  2013.

[12] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research  2013.

[13] R. Frostig  R. Ge  S. Kakade  and A. Sidford. Un-regularizing: Approximate proximal point

and faster stochastic algorithms for empirical risk minimization. In ICML  2015.

[14] H. Lin  J. Mairal  and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In NIPS 

2015.

[15] Y. Nesterov. Introductory Lectures on Convex Optimization. A Basic Course. Springer  2004.
[16] D. Garber and E. Hazan. Fast and simple PCA via convex optimization. arXiv  2015.
[17] C. Jin  S. Kakade  C. Musco  P. Netrapalli  and A. Sidford. Robust shift-and-invert precondi-

tioning: Faster and more sample efﬁcient algorithms for eigenvector computation. 2015.

[18] D. Witten  R. Tibshirani  and T. Hastie. A penalized matrix decomposition  with applications

to sparse principal components and canonical correlation analysis. Biostatistics  2009.

[19] C. Snoek  M. Worring  J. van Gemert  J. Geusebroek  and A. Smeulders. The challenge prob-
lem for automated detection of 101 semantic concepts in multimedia. In MULTIMEDIA  2006.

[20] J. Westbury. X-Ray Microbeam Speech Production Database User’s Handbook  1994.
[21] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proc. IEEE  86(11):2278–2324  1998.

[22] M. Warmuth and D. Kuzmin. Randomized online PCA algorithms with regret bounds that are

logarithmic in the dimension. Journal of Machine Learning Research  2008.

[23] R. Arora  A. Cotter  K. Livescu  and N. Srebro. Stochastic optimization for PCA and PLS. In

ALLERTON  2012.

[24] A. Balsubramani  S. Dasgupta  and Y. Freund. The fast convergence of incremental PCA. In

NIPS  2013.

[25] O. Shamir. A stochastic PCA and SVD algorithm with an exponential convergence rate. In

ICML  2015.

[26] F. Yger  M. Berar  G. Gasso  and A. Rakotomamonjy. Adaptive canonical correlation analysis

based on matrix manifolds. In ICML  2012.

9

,Weiran Wang
Jialei Wang
Dan Garber
Dan Garber
Nati Srebro
Gamaleldin Elsayed
Dilip Krishnan
Hossein Mobahi
Kevin Regan
Samy Bengio