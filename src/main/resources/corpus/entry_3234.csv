2009,Linear-time Algorithms for Pairwise Statistical Problems,Several key computational bottlenecks in machine learning involve pairwise distance computations  including all-nearest-neighbors (finding the nearest neighbor(s) for each point  e.g.  in manifold learning) and kernel summations (e.g. in kernel density estimation or kernel machines).  We consider the general  bichromatic case for these problems  in addition to the scientific problem of N-body potential calculation.  In this paper we show for the first time O(N) worst case runtimes for practical algorithms for these problems based on the cover tree data structure (Beygelzimer  Kakade  Langford  2006).,Linear-time Algorithms for Pairwise Statistical

Problems

Parikshit Ram  Dongryeol Lee  William B. March and Alexander G. Gray

Computational Science and Engineering  Georgia Institute of Technology

Atlanta  GA 30332

{p.ram@ dongryel@cc. march@ agray@cc.}gatech.edu

Abstract

Several key computational bottlenecks in machine learning involve pairwise dis-
tance computations  including all-nearest-neighbors (ﬁnding the nearest neigh-
bor(s) for each point  e.g. in manifold learning) and kernel summations (e.g. in
kernel density estimation or kernel machines). We consider the general  bichro-
matic case for these problems  in addition to the scientiﬁc problem of N-body
simulation. In this paper we show for the ﬁrst time O( ) worst case runtimes for
practical algorithms for these problems based on the cover tree data structure [1].

1 Introduction

Pairwise distance computations are fundamental to many important computations in machine learn-
ing and are some of the most expensive for large datasets. In particular  we consider the class of
all-query problems  in which the combined interactions of a reference set ℛ of  points in ℝ is
computed for each point  in a query set  of size O( ). This class of problems includes the pair-
wise kernel summation used in kernel density estimation and kernel machines and the all-nearest
neighbors computation for classiﬁcation and manifold learning. All-query problems can be solved
directly by scanning over the  reference points for each of the O( ) queries  for a total running
time of O( 2). Since quadratic running times are too slow for even modestly-sized problems 
previous work has sought to reduce the number of distance computations needed.

We consider algorithms that employ space-partitioning trees to improve the running time. In all the
problems considered here  the magnitude of the effect of any reference  on a query  is inversely
proportional to the distance metric (  ). Therefore  the net effect on the query is dominated
by references that are “close by”. A space-partitioning tree divides the space containing the point
set in a hierarchical fashion  allowing for variable resolution to identify major contributing points
efﬁciently.
Single-Tree Algorithms. One approach for employing space-partitioning trees is to consider each
query point separately – i.e. to consider the all-query problem as many single-query problems. This
approach lends itself to single-tree algorithms  in which the references are stored in a tree  and the
tree is traversed once for each query. By considering the distance between the query and a collection
of references stored in a tree node  the effect of the references can be approximated or ignored if the
distances involved are large enough  with appropriate accuracy guarantees for some methods.

The -tree structure [2] was developed to obtain the nearest-neighbors of a given query in expected
logarithmic time and has also been used for efﬁcient kernel summations [3  4]. However  these
methods lack any guarantees on worst-case running time. A hierarchical data structure was also
developed for efﬁcient combined potential calculation in computational physics in Barnes & Hut 
1986 [5]. This data structure provides an O(log  ) bound on the potential computation for a single
query  but has no error guarantees. Under their deﬁnition of intrinsic dimension  Karger & Ruhl [6]
describe a randomized algorithm with O(log  ) time per query for nearest neighbor search for low-
intrinsic-dimensional data. Krauthgamer & Lee proved their navigating nets algorithm can compute

1

a single-query nearest-neighbor in O(log  ) time under a more robust notion of low intrinsic di-
mensionality. The cover tree data structure [1] improves over these two results by both guaranteeing
a worst-case runtime for nearest-neighbor and providing efﬁcient computation in practice relative to
-trees. All of these data structures rely on the triangle inequality of the metric space containing
ℛ in order to prune references that have little effect on the query.
Dual-Tree Algorithms. The approach described above can be applied to every single query to im-
prove the O( 2) running time of all-query problems to O( log  ). A faster approach to all-query
problems uses an algorithmic framework inspired by efﬁcient particle simulation [7] and general-
ized to statistical machine learning [8] which takes advantage of spatial proximity in both  and
ℛ by constructing a space-partitioning tree on each set. Both trees are descended  allowing the
contribution from a distant reference node to be pruned for an entire node of query points. These
dual-tree algorithms have been shown to be signiﬁcantly more efﬁcient in practice than the cor-
responding single-tree algorithms for nearest neighbor search and kernel summations [9  10  11].
Though conjectured to have O( ) growth  they lack rigorous  general runtime bounds.
All-query problems fall into two categories: monochromatic  where  = ℛ and bichromatic  where
 is distinct from ℛ. Most of the existing work has only addressed the monochromatic case. The fast
multipole method (FMM)[7] for particle simulations  considered one of the breakthrough algorithms
of the 20th century  has a non-rigorous runtime analysis based on the uniform distribution. An
improvement to the FMM for the -body problem was suggested by Aluru et.al. [12]  but was
regarding the construction time of the tree and not the querying time. Methods based on the well-
separated pair decomposition (WSPD) [13] have been proposed for the all nearest neighbors problem
and particle simulations [14]  but are inefﬁcient in practice. These methods have O( ) runtime
bounds for the monochromatic case  but it is not clear how to extend the analysis to a bichromatic
problem. In addition to this difﬁculty  the WSPD-based particle simulation method is restricted to
the (1/)-kernel. In Beygelzimer et.al.  2006 [1]  the authors conjecture  but do not prove  that the
cover tree data structure using a dual-tree algorithm can compute the monochromatic all-nearest-
neighbors problem in O( ).
Our Contribution. In this paper  we prove O( ) runtime bounds for several important instances
of the dual-tree algorithms for the ﬁrst time using the cover tree data structure [1]. We prove the ﬁrst
worst-case bounds for any practical kernel summation algorithms. We also provide the ﬁrst general
runtime proofs for dual-tree algorithms on bichromatic problems. In particular  we give the ﬁrst
proofs of worst-case O( ) runtimes for the following all-query problems:

∙ All Nearest-neighbors: For all queries  ∈   ﬁnd ∗() ∈ ℛ such that ∗() =

∙ Kernel summations: For a given kernel function (⋅)  compute the kernel summation

∙ N-body potential calculation: Compute the net electrostatic or gravitational potential

arg min∈ℛ (  ).
 () =ࢣ∈ℛ
 () =ࢣ∈ℛ ∕= (  )−1 at each  ∈ .

((  )) for all  ∈ .

Outline. In the remainder of this paper  we give our linear running time proofs for dual-tree al-
gorithms. In Section 2  we review the cover tree data structure and state the lemmas necessary for
the remainder of the paper. In Section 3  we state the dual-tree all-nearest-neighbors algorithm and
prove that it requires O( ) time. In Section 4  we state the absolute and relative error guaran-
tees for kernel summations and again prove the linear running time of the proposed algorithms. In
the same section  we apply the kernel summation result to the -body simulation problem from
computational physics  and we draw some conclusions in Section 5.

2 Cover Trees

A cover tree [1]  stores a data set ℛ of size  in the form of a levelled tree. The structure has an
O( ) space requirement and O( log  ) construction time. Each level is a “cover” for the level
beneath it and is indexed by an integer scale  which decreases as the tree is descended. Let 
denote the set of nodes at scale . For all scales   the following invariants hold:

∙ (nesting invariant)  ⊂ −1
∙ (covering tree invariant) For every  ∈ −1  there exists a  ∈  satisfying (  ) ≤ 2 
∙ (separation invariant) For all    ∈   (  ) > 2.

and exactly one such  is a parent of .

2

The intrinsic dimensionality measure considered here is the expansion

Representations. The cover tree has two different representations: The implicit representation
consists of inﬁnitely many levels  with the level ∞ containing a single node which is the root
and the level −∞ containing every point in the dataset as a node. The explicit representation is
required to store the tree in O( ) space. It coalesces all nodes in the tree for which the only child
is the self-child. This implies that every explicit node either has a parent other than the self-parent
or has a child other than a self-child.
Structural properties.
dimension from Karger & Ruhl  2002 [6] deﬁned as follows:
Deﬁnition 2.1. Let ℛ(  ) = { ∈ ℛ ⊂  : (  ) ≤ } denote a closed ball of radius
 around a  ∈ ℛ. Then  the expansion constant of ℛ is deﬁned as the smallest  ≥ 2 such
∣ℛ(  2)∣ ≤  ∣ℛ(  )∣ ∀ ∈ ℛ and ∀ > 0. The intrinsic dimensionality (or expansion
dimension) of ℛ is given by (ℛ) = log .
We make use of the following lemmas from Beygelzimer et.al.  2006 [1] in our runtime proofs.
Lemma 2.1. (Width bound) The number of children of any node  is bounded by 4.
Lemma 2.2. (Growth bound) For all  ∈ ℛ and  > 0  if there exists a point  ∈ ℛ such that

2 < (  ) ≤ 3  then ∣(  4)∣ ≥1 + 1

2 ∣(  )∣ .

Lemma 2.3. (Depth bound) The maximum depth of any point  in the explicit representation is
O(2 log  ).
Single point search: Single tree nearest neighbor. Given a cover tree  built on a set ℛ  the
nearest neighbor of a query  can be found with the FindNN subroutine in Algorithm 1. The
algorithm uses the triangular inequality to prune away portions of the tree that contain points distant
from . The following theorem provides a runtime bound for the single point search.
Theorem 2.1. (Query time) If the dataset ℛ ∪ {} has expansion constant   the nearest neighbor
of  can be found in time O(12 log  ).
Batch Query: The dual tree algorithm for all-nearest-neighbor (FindAllNN subroutine in Algo-
rithm 1) using cover trees is provided in Beygelzimer et.al.  2006 [15] as batch-nearest-neighbor.
3 Runtime Analysis of All-Nearest-Neighbors
In the bichromatic case  the performance of the FindAllNN algorithm (or any dual-tree algorithm)
will depend on the degree of difference between the query and reference sets. If the sets are nearly
identical  then the runtime will be close to the monochromatic case. If the inter-point distances in the
query set are very large relative to those between references  then the algorithm may have to descend
to the leaves of the query tree before making any descends in the reference tree. This case offers no
improvement over the performance of the single-tree algorithm applied to each query. In order to
quantify this difference in scale for our runtime analysis  we introduce the degree of bichromaticity:
Deﬁnition 3.1. Let  and  be cover trees built on query set  and reference set ℛ respectively.
Consider a dual-tree algorithm with the property that the scales of  and  are kept as close as
possible – i.e. the tree with the larger scale is always descended. Then  the degree of bichromaticity
 of the query-reference pair (  ℛ) is the maximum number of descends in  between any two
descends in  .

In the monochromatic case  the trees are identical and the traversal alternates between them. Thus 
the degree of bichromaticity is  = 1. As the difference in scales of the two data sets increases 
more descends in the query tree become necessary  giving a higher degree of bichromaticity. Using
this deﬁnition  we can prove the main result of this section.
Theorem 3.1. Given a reference set ℛ of size  and expansion constant ℛ  a query set  of size
O( ) and expansion constant   and bounded degree of bichromaticity  of the (  ℛ) pair  the
FindAllNN subroutine of Algorithm 1 computes the nearest neighbor in ℛ of each point in  in
O(12
ℛ
Proof. The computation at Line 3 is done for each of the query nodes at most once  hence takes
O(max ∣∣ ∗  ) computations.
The traversal of a reference node is duplicated over the set of queries only if the query tree is
descended just before the reference tree descend. For every query descend  there would be at most
) duplications (width bound) for every reference node traversal. Since the number of query
O(4


 ) time.

4


3

Algorithm 1 Single tree and batch query algorithm for Nearest Neighbor search and Approximate
Kernel summation
FindNN(ℛ-Tree    query )

Initialize ∞ = ∞.
for  = ∞ to −∞ do

3:  = {ℎ() :  ∈ }

−1 = { ∈  : (  ) ≤ (  ) + 2}

Initialize Δ () ← 0∀ ∈ ∞
AllKernelSum(-subtree  

ℛ-cover set )

if  = −∞ then

for ∀ ∈ () do

(  )

3:

ˆ () = ˆ ()

end for

6: return arg min
∈−∞

if  = −∞ then

FindAllNN(-subtree   ℛ-cover set )

∀ ∈ () return arg min
∈−∞
// () is the set of all the leaves of the subtree .

(  ).

3: else if  <  then

(  ) ≤ (  ) + 2 + 2+2}

 = {ℎ() :  ∈ }
−1 = { ∈  :
FindAllNN (  −1)
∀−1 ∈ ℎ() FindAllNN(−1  )

6:

else

9: end if
KernelSum(ℛ-tree    query )

Initialize ∞ = ∞  ˆ () = 0
for  = ∞ to −∞ do
3:  = {ℎ() :  ∈ }

−1 = { ∈  : ℎ((  ) − 2)
ˆ () = ˆ () + ࢣ∈{−−1}
return ˆ () = ˆ () + ࢣ∈−∞

6: end for

ℎ((  ))

−ℎ((  ) + 2) > }

ℎ((  )) ⋅ ∣()∣

ℎ((  ))

+ ࢣ∈−∞

+Δ ()

end for
Δ () = 0

if  <  then

6: else

9:

12:

15:

18:

 = {ℎ() :  ∈ }
−1 = { ∈  :

ℎ((  ) − 2 − 2+1)
−ℎ((  ) + 2 + 2+1)
> }

Δ () = Δ ()+

ℎ((  )) ⋅ ∣()∣

ࢣ∈∖−1

else

AllKernelSum(  −1)
for ∀−1 ∈ ℎ() do

Δ (−1) = Δ (−1)+Δ ()
AllKernelSum(−1  )

end for
Δ () = 0

end if

end if

descends between any two reference descends is upper bounded by  and the number of explicit
reference nodes is O( )  the total number of reference node considered in Line 5 in the whole
algorithm is at most O(4


 ).

ℛ

6
ℛ

max ∣∣ log  ) in the whole algorithm.

max ∣∣ (width bound)  and the
log  ) (depth bound)  the number of nodes
max ∣∣ log  ). Since the traversal down the query tree causes
  Line 6 takes at most

Since at any level of recursion  the size of  is bounded by 4
ℛ
maximum depth of any point in the explicit tree is O(2
ℛ
encountered in Line 6 is O(4+2
duplication  and the duplication of any reference node is upper bounded by 4

O(4

Line 9 is executed just once for each of the explicit nodes of the query tree and hence takes at most
O( ) time.
Consider any −1 = { ∈  : (  ) ≤ +2+2+2} where  = (  ). Given that −1 is the
(−1)ℎ level of the reference tree −1 = (  +2+2+2)∩ ⊆ (  +2+2+2)∩−1 ⊆
(   + 2 + 2+1) ∩ −1 since  ⊆ −1 and  <  in this part of the recursion. If  > 2+2 
. Now  ≤ (  ℛ) + 2 since  ⊆ −1
∣(   + 2 + 2+1)∣ ≤ ∣(  2)∣ ≤ 2

ℛ(cid:12)(cid:12)(  
2 )(cid:12)(cid:12)
2 )(cid:12)(cid:12) = ∣{}∣ = 1. Hence ∣−1∣ ≤ 2
and  > 2+2  (  ℛ) > 2+1  making(cid:12)(cid:12)(  

If  ≤ 2+2  as in Beygelzimer et.al. [1] the number of disjoint balls of radius 2−2 that can be packed
in (  +2+2+1) is bounded as ∣(  +2+2+1+2−2)∣ ≤ ∣(  2(+2+2+1)+2−2)∣ ≤
(  2−2)∣ for some  ∈ −1. Any such
∣(  2+3 + 2+1 + 2+2 + 2−2)∣ ≤ ∣(  2+4)∣ ≤ ∣6
ℛ
ball (  2−2) can contain at most one point in −1  making ∣−1∣ ≤ 6
ℛ

.

ℛ

.

4

 + 12
ℛ

 + 4


Thus  the algorithm takes O(6
ℛ
Corollary 3.1. In the monochromatic case with a dataset ℛ of size  having an expansion constant
  the FindAllNN subroutine of Algorithm 1 has a runtime bound of O(16 ).
Proof. In the monochromatic case  ∣∣ = ∣ℛ∣ =    = ℛ =  and the degree of bichromaticity
 = 1 since the query and the reference tree are the same. Therefore  by Theorem 3.1  the result
follows.

log  +  ) which is O(12
ℛ

4


4


 ).

4 Runtime Analysis of Approximate Kernel Summations

For inﬁnite tailed kernels (⋅)  the exact computation of kernel summations is infeasible without

Deﬁnition 4.2. An algorithm guarantees  relative error bound  if for each exact value  () for

(⋅) is a monotonically decreasing non-negative kernel function. We employ the two widely used
approximating schemes listed below:
Deﬁnition 4.1. An algorithm guarantees  absolute error bound  if for each exact value  () for

O( 2) operations. Hence the goal is to efﬁciently approximate  () = ࢣ ((  )) where
 ∈   it computes ˆ () such that(cid:12)(cid:12)(cid:12)
 ∈   it computes ˆ () ∈ ℝ such that(cid:12)(cid:12)(cid:12)

Approximate kernel summation is more computationally intensive than nearest neighbors because
pruning is not based on the distances alone but also on the analytical properties of the kernel
(i.e. smoothness and extent). Therefore  we require a more extensive runtime analysis  especially for
kernels with an inﬁnite extent  such as the Gaussian kernel. We ﬁrst prove logarithmic running time
for the single-query kernel sum problem under an absolute error bound and then show linear running
time for the dual-tree algorithm. We then extend this analysis to include relative error bounds.

ˆ () −  ()(cid:12)(cid:12)(cid:12)
ˆ () −  ()(cid:12)(cid:12)(cid:12)

≤  ∣ ()∣.

≤  .

4.1 Single Tree Approximate Kernel Summations Under Absolute Error

The algorithm for computing the approximate kernel summation under absolute error is shown in the
KernelSum subroutine of Algorithm 1. The following theorem proves that KernelSum produces
an approximation satisfying the  absolute error.
Theorem 4.1. The KernelSum subroutine of Algorithm 1 outputs ˆ () such that ∣ ˆ ()− ()∣ ≤  .
Proof. A subtree rooted at  ∈ −1 is pruned as per Line 5 of KernelSum since for ∀′ ∈ () 
((  ) + 2) ≤ ((  ′)) ≤ ((  ) − 2) and ∣((  )) − ((  ′))∣ ≤ . This
amounts to limiting the error per each kernel evaluation to be less than  (which also holds true
for each contribution computed exactly for  ∈ −∞  and by the triangle inequality the kernel
approximate sum ˆ () will be within   of the true kernel sum  ().
The following theorem proves the runtime of the single-query kernel summation with smooth and
monotonically decreasing kernels using a cover tree.
Theorem 4.2. Given a reference set ℛ of size  and expansion constant   an error value   and a
monotonically decreasing smooth non-negative kernel function (⋅) concave for  ∈ [0  ℎ] and
convex for  ∈ (ℎ  ∞) for some ℎ > 0  the KernelSum subroutine of Algorithm 1 computes
the kernel summation at a query  approximately up to  absolute error with a runtime bound of
O(2(1+max{−1+3 −1+4 4}) log  ) time where

 =ऄlog2  (−1) ()अ   = ⌈log2 ℎ⌉  1 =ælog2 −

 ′(ℎ)æ  and ′(⋅) is the derivative of (⋅).

Proof. We assume that any argument of (⋅) is lower bounded at 0. Now deﬁne the following sets:


−1 = { ∈ −1 : (  ) ≤ ℎ − 2}

−1 = { ∈ −1 : ℎ − 2 < (  ) ≤ ℎ + 2}

−1 = { ∈ −1 : (  ) > ℎ + 2}
−1 ∪ 

−1  and are pairwise disjoint. For  ∈ 

−1:

such that −1 = 

−1 ∪ 

 <(max(0  ((  ) − 2))) − ((  ) + 2)

≤(((  ) + 2) − 2+1′((  ) + 2)) − ((  ) + 2) = −2+1′((  ) + 2)

5

because of the concavity of the kernel function (⋅). Now 

′(−1)

[0 ℎ−2] −

2+1 − 2 < (  ) ≤ ℎ − 2

[ ] () is 1) the inverse function of the ′(); 2) the output value is restricted to be in the

where ′(−1)
interval [  ] for the given argument . For  ∈ 
−1 

 < (max(0  ((  ) − 2))) − ((  ) + 2) ≤ −2+1′(ℎ)

which implies that

Similarly  for  ∈ 

 ≥ log2 −

′(ℎ) − 1
−1   < −2+1′((  ) − 2) implying
(ℎ+2 ∞) −

ℎ + 2 < (  ) < ′(−1)

2+1 + 2.
2+1 ≥ ′(ℎ) and
−1 = ∅. In addition  below the level

−1 = 

Note that 0 ≥ ′((  )) ≥ ′(ℎ) for (  ) > ℎ + 2  which implies that −

thus  ≥ ælog2 −
−1 = ∅.

 ′(ℎ)æ = 1. Below the level 1  

1 − 1  
Case 1:  > 1
Trivially  for  ∈ −1  (−2) >  where  = max∈−1 (  ). We can invert the ker-
nel function to obtain:  <  (−1)
(ℎ+2 ∞) ()+2. This implies that (  ) ≤  <  (−1) ()+
2 We can count up the number of balls of radius 2−2 inside    (−1) () + 2 + 2−2. Let
 =ऄlog2  (−1) ()अ. Then 
max ∣−1∣ ≤ ∣(  2+2+2−2)∩−1∣ ≤⎧⎨
⎩

∣(  2+1) ∩ −1∣ ≤ 3   < 
∣(  2+2) ∩ −1∣ ≤ 4   = 
∣(  2+1) ∩ −1∣ ≤ −+3 = −1+3   > 

Case 2:  = 1 − 1
Let  = ⌈log2 ℎ⌉. Similar to the case above  we count the number of balls of radius 2−2 inside

  2 + 2 + 2−2.
max ∣−1∣ ≤ ∣(  2+2+2−2)∩−1∣ ≤⎧⎨
⎩

∣(  2+1) ∩ −1∣ ≤ 3   < 
∣(  2+2) ∩ −1∣ ≤ 4   = 
∣(  2+1) ∩ −1∣ ≤ −+3 = −1+4   > 
From the runtime proof of the single-tree nearest neighbor algorithm using cover tree in Beygelzimer
et.al.  2006  the running time is bounded by:

O( max ∣−1∣2 +  max ∣−1∣4) ≤ O(2(1+max{−1+3 −1+4 4}) log  )

4.2 Dual Tree Approximate Kernel Summations Under Absolute Error
An algorithm for the computation of kernel sums for multiple queries is shown in the AllKernelSum
subroutine of Algorithm 1  analogous to FindAllNN for batch nearest-neighbor query. The dual-tree
version of the algorithm requires a stricter pruning rule to ensure correctness for all the queries in a
query subtree. Additionally  every query node  has an associated O(1) storage Δ () that accu-
mulates the postponed kernel contribution for all query points under the subtree . The following
theorem proves the correctness of the AllKernelSum subroutine of Algorithm 1.
Theorem 4.3. For all  in the in the query set   the AllKernelSum subroutine of Algorithm 1
computes approximations ˆ () such that ∣ ˆ () −  ()∣ ≤  .
Proof. Line 9 of the algorithm guarantees that ∀ ∈ ∖−1 at a given level  

∣((  )) − ((  ))∣ ≤ ∣((  ) − 2 − 2+1) − ((  ) + 2 + 2+1)∣ ≤ 

for all  ∈ (). Basically  the minimum distance is decreased and the maximum distance is
increased by 2+1  which denotes the maximum possible distance from  to any of its descendants.
Trivially  contributions added in Line 3 (the base case) satisfy the  absolute error for each kernel
value and the result follows by the triangle inequality.

6

Based on the runtime analysis of the batch nearest neighbor  the runtime bound of AllKernelSum is
given by the following theorem:
Theorem 4.4. Let ℛ be a reference set of size  and expansion constant ℛ  and let  be a
query set of size O( ) and expansion constant . Let the (  ℛ) pair have a bounded degree of
bichromaticity. Let (⋅) be a monotonically-decreasing smooth non-negative kernel function that is
concave for  ∈ [0  ℎ] and convex for  ∈ (ℎ  ∞) for some ℎ > 0. Then  given an error tolerance  
the AllKernelSum subroutine of Algorithm 1 computes an approximation ˆ () ∀ ∈  that satisﬁes
the  absolute error bound in time O( ).
Proof. We ﬁrst bound max ∣−1∣. Note that in Line 9 to Line 13 of the AllKernelSum   ≤  + 1 
and thus 2 + 2+1 ≤ 2 + 2 = 2+1. Similar to the proof for the single-tree case  we deﬁne:

−1 = { ∈ −1 : (  ) ≤ ℎ − 2+1}

−1 = { ∈ −1 : ℎ − 2+1 < (  ) ≤ ℎ + 2+1}

−1 = { ∈ −1 : (  ) > ℎ + 2+1}


−1  and pairwise disjoint. From here  we can follow the tech-
such that −1 = 
niques shown for the single-tree case to show that max ∣−1∣ is constant dependent on . Therefore 
the methodology of the runtime analysis of batch nearest neighbor gives the O( ) runtime for batch
approximate kernel summation.

−1 ∪ 

−1 ∪ 

4.3 Approximations Under Relative Error

We now extend the analysis for absolute error bounds to cover approximations under the relative
error criterion given in Deﬁnition 4.2.
Single-tree case. For a query point   the goal is compute ˆ () satisfying Deﬁnition 4.2. An approx-
imation algorithm for a relative error bound is similar to the KernelSum subroutine of Algorithm 1
except that the deﬁnition of −1 (i.e. the set of reference points that are not pruned at the given
level ) needs to be changed to satisfy the relative error constraint as follows:

−1 = { ∈  : ((  ) − 2) − ((  ) + 2) >

 ()



}

where  () is the unknown query sum. Hence  let  = max
∈ℛ

(  )  and expand the set −1 to:
(1)

−1 ⊆ { ∈  : ((  ) − 2) − ((  ) + 2) > ()}

Note that  can be trivially upper bounded by:  ≤ (  ) + 2+1 =   where  is
the scale of the root of the reference cover tree in the explicit representation.
Theorem 4.5. Let the conditions of Thm. 4.2 hold. Then  the KernelSum subroutine of Algorithm 1
with Line 5 redeﬁned as Eqn. 1 computes the kernel summation ˆ () at a query  with  relative
error in O(log  ) time.
Proof. A node  ∈ −1 can be pruned by the above pruning rule since for ′ ∈ ()  ((  ) +
2) ≤ ((  ′)) ≤ ((  ) − 2) and ∣((  )) − ((  ′))∣ ≤ ( ). This amounts
to limiting the error per each kernel evaluation to be less than ( ) (which also holds true
for each contribution computed exactly for  ∈ −∞  and by the triangle inequality the kernel
approximate sum ˆ () will be within  ( ) ≤  () of the true kernel sum  (). Since the
relative error is an instance of the absolute error  the algorithm also runs in O(log  ).
Dual-tree case. In this case  for each query point  ∈   an approximation ˆ () is to be computed
as per Deﬁnition 4.2. As in the absolute error case  we must satisfy a more difﬁcult condition.
Therefore    is larger  taking into account both the maximum possible distance from the root
of the query tree to its descendants and the maximum possible distance from the root of the reference
tree to its descendants. Hence −1 is deﬁned as follows:

−1 = { ∈  : ((  ) − 2 − 2+1) − ((  ) + 2 + 2+1) > ( )}

(2)
where (  ) + 2+1 + 2ℛ+1 =   and   ℛ are the scales of the roots of the
query and reference cover trees respectively in the explicit representations. The correctness of the
algorithm follows naturally from Theorems 4.4 and 4.5.

7

 () =ࢣ

1

( ) is considered ﬁrst under the assumption that min∈ℛ ∕= (  ) > 0.

Corollary 4.2. Given a reference set ℛ of size  and expansion constant   an error value  and
the kernel () = 1/(  )  the KernelSum subroutine of Algorithm 1 computes the potential
summation at a query  with  error in O(log  ) time.
Proof. Let  = min

(  ). Let  () be the  2 continuous construction [16] such that:

Corollary 4.1. Let the conditions of Thm. 4.4 hold. Then  given an error value   the AllKernel-
Sum subroutine of Algorithm 1 with Line 11 redeﬁned as Eq. 2 computes an approximate kernel
summation ˆ () ∀ ∈  that satisﬁes an  relative error bound with a runtime bound of O( ).
Note that for the single-tree and dual-tree algorithms under the relative error criterion  the pruning
rules that generate −1 shown above are sub-optimal in practice  because they require every pair-
wise kernel value that is pruned to be within  relative error. There is a more sophisticated way of
accelerating this using an alternative method [9  10  11] that is preferable in practice.

4.4 -body Simulation
-body potential summation is an instance of the kernel summation problem that arises in com-
putational physics and chemistry. These computations use the Coulombic kernel () = 1/ 
which describes gravitational and electrostatic interactions. This kernel is inﬁnite at zero distance
and has no inﬂection point (i.e. it is convex for  ∈ (0  ∞)). Nevertheless  it is possible to obtain
the runtime behavior using the results shown in the previous sections. The single query problem

∈ℛ ∕=

() = 1

  15

8 − 5
1
    ≥ 

2
4 

+ 3

4    < 
8 

The effective kernel () can be constructed in O(log  ) time using the single-tree algorithm for
nearest neighbor described in Beygelzimer et.al.  2006 [1]. Note that the second derivative of the
√5
effective kernel is ′′ () = −5
3 
and convex otherwise  so the second derivative agrees at  = . Note that () agrees with
() for  ≥ . Hence  by considering  equivalent to the bandwidth ℎ in Theorem 4.2 and
applying the same theorem on the KernelSum subroutine of Algorithm 1 with the aforementioned
kernel  we prove the O(log  ) runtime bound.

2()5 for  < . Thus it is concave for  <

2()3 + 92

The runtime analysis for the batch case of the algorithm follows naturally.
Corollary 4.3. Given a reference set ℛ of size  and expansion constant ℛ and a query set  of
size O( ) and expansion constant  with a bounded degree of bichromaticity for the (  ℛ) pair 
an error value  and the kernel () = 1/(  )  the AllKernelSum subroutine of Algorithm 1
approximates the potential summation ∀ ∈  up to  error with a runtime bound of O( ).
(  ).
Proof. The same effective kernel as Corollary 4.2 is used  except that  = min
∈
The result follows from applying Theorem 4.4  and noting that running the dual-tree computation
with ((  )) = 1/(  ) is equivalent to running the algorithm with ((  )).

∈ℛ ∕=

min

5 Conclusions

Extensive work has attempted to reduce the quadratic scaling of the all-query problems in statistical
machine learning. So far  the improvements in runtimes have only been empirical with no rigorous
runtime bounds [2  8  9  17  18]. Previous work has provided algorithms with rough linear runtime
arguments for certain instances of these problems [14  5  13]  but these results only apply to the
monochromatic case. In this paper  we extend the existing work [6  1  19  20] to provide algorithms
for two important instances of the all-query problem (namely all-nearest-neighbor and all-kernel-
summation) and obtain for the ﬁrst time a linear runtime bound for dual-tree algorithms for the more
general bichromatic case of the all-query problems.

These results provide an answer to the long-standing question of the level of improvement possible
over the quadratic scaling of the all-query problems. The techniques used here ﬁnally point the way
to analyzing a host of other tree-based algorithms used in machine learning  including those that
involve -tuples  such as the -point correlation (which na¨ıvely require O( ) computations).

8

References

[1] A. Beygelzimer  S. Kakade  and J.C. Langford. Cover Trees for Nearest Neighbor. Proceedings

of the 23rd International Conference on Machine learning  pages 97–104  2006.

[2] J. H. Freidman  J. L. Bentley  and R. A. Finkel. An Algorithm for Finding Best Matches in

Logarithmic Expected Time. ACM Trans. Math. Softw.  3(3):209–226  September 1977.
[3] K. Deng and A. W. Moore. Multiresolution Instance-Based Learning. pages 1233–1242.
[4] D. Lee and A. G. Gray. Faster Gaussian Summation: Theory and Experiment. In Proceedings

of the Twenty-second Conference on Uncertainty in Artiﬁcial Intelligence. 2006.

[5] J. Barnes and P. Hut. A Hierarchical ( log  ) Force-Calculation Algorithm. Nature  324 

1986.

[6] D. R. Karger and M. Ruhl. Finding Nearest Neighbors in Growth-Restricted Metrics. Proceed-
ings of the Thiry-Fourth Annual ACM Symposium on Theory of Computing  pages 741–750 
2002.

[7] L. Greengard and V. Rokhlin. A Fast Algorithm for Particle Simulations. Journal of Compu-

tational Physics  73:325–248  1987.

[8] A. G. Gray and A. W. Moore. ‘-Body’ Problems in Statistical Learning. In NIPS  volume 4 

pages 521–527  2000.

[9] A. G. Gray and A. W. Moore. Nonparametric Density Estimation: Toward Computational

Tractability. In SIAM International Conference on Data Mining  2003.

[10] D. Lee  A. G. Gray  and A. W. Moore. Dual-Tree Fast Gauss Transforms.

In Y. Weiss 
B. Sch¨olkopf  and J. Platt  editors  Advances in Neural Information Processing Systems 18 
pages 747–754. MIT Press  Cambridge  MA  2006.

[11] D. Lee and A. G. Gray. Fast High-dimensional Kernel Summations Using the Monte Carlo
Multipole Method. In To appear in Advances in Neural Information Processing Systems 21.
2009.

[12] S. Aluru  G. M. Prabhu  and J. Gustafson. Truly distribution-independent algorithms for the
N-body problem. In Proceedings of the 1994 conference on Supercomputing  pages 420–428.
IEEE Computer Society Press Los Alamitos  CA  USA  1994.

[13] P. B. Callahan. Dealing with Higher Dimensions: the Well-Separated Pair Decomposition and

its applications. PhD thesis  Johns Hopkins University  Baltimore  Maryland  1995.

[14] P. B. Callahan and S. R. Kosaraju. A Decomposition of Multidimensional Point Sets with Ap-
plications to k-Nearest-Neighbors and n-body Potential Fields. Journal of the ACM  62(1):67–
90  January 1995.

[15] A. Beygelzimer  S. Kakade  and J.C. Langford. Cover trees for Nearest Neighbor. 2006.

http://hunch.net/˜jl/projects/cover tree/paper/paper.ps.

[16] R. D. Skeel  I. Tezcan  and D. J. Hardy. Multiple Grid Methods for Classical Molecular Dy-

namics. Journal of Computational Chemistry  23(6):673–684  2002.

[17] A. G. Gray and A. W. Moore. Rapid Evaluation of Multiple Density Models.

Intelligence and Statistics 2003  2003.

In Artiﬁcial

[18] A. G. Gray and A. W. Moore. Very Fast Multivariate Kernel Density Estimation via Computa-

tional Geometry. In Joint Statistical Meeting 2003  2003. to be submitted to JASA.

[19] R. Krauthgamer and J. R. Lee. Navigating Nets: Simple Algorithms for Proximity Search.

15th Annual ACM-SIAM Symposium on Discrete Algorithms  pages 791–801  2004.

[20] K. Clarkson. Fast Algorithms for the All Nearest Neighbors Problem.

In Proceedings of
the Twenty-fourth Annual IEEE Symposium on the Foundations of Computer Science  pages
226–232  1983.

9

,Ashish Khetan
Sewoong Oh