2018,Mean Field for the Stochastic Blockmodel: Optimization Landscape and Convergence Issues,Variational approximation has been widely used in large-scale Bayesian inference recently  the simplest kind of which involves imposing a mean field assumption to approximate complicated latent structures. Despite the computational scalability of mean field  theoretical studies of its loss function surface and the convergence behavior of iterative updates for optimizing the loss are far from complete. In this paper  we focus on the problem of community detection for a simple two-class Stochastic Blockmodel (SBM). Using batch co-ordinate ascent (BCAVI) for updates  we give a complete characterization of all the critical points and show different convergence behaviors with respect to initializations. When the parameters are known  we show a significant proportion of random initializations will converge to ground truth. On the other hand  when the parameters themselves need to be estimated  a random initialization will converge to an uninformative local optimum.,Mean Field for the Stochastic Blockmodel:

Optimization Landscape and Convergence Issues

Soumendu Sunder Mukherjee∗

Purnamrita Sarkar∗

Interdisciplinary Statistical Research Unit (ISRU)

Department of Statistics and Data Science

Indian Statistical Institute  Kolkata

Kolkata 700108  India

soumendu041@gmail.com

University of Texas  Austin

Austin  TX 78712

purna.sarkar@austin.utexas.edu

Y. X. Rachel Wang∗

University of Sydney
NSW 2006  Australia

rachel.wang@sydney.edu.au

School of Mathematics and Statistics

Department of Statistics and Data Science

Bowei Yan

University of Texas  Austin

Austin  TX 78712

boweiy@utexas.edu

Abstract

Variational approximation has been widely used in large-scale Bayesian inference
recently  the simplest kind of which involves imposing a mean ﬁeld assumption to
approximate complicated latent structures. Despite the computational scalability
of mean ﬁeld  theoretical studies of its loss function surface and the convergence
behavior of iterative updates for optimizing the loss are far from complete. In
this paper  we focus on the problem of community detection for a simple two-
class Stochastic Blockmodel (SBM). Using batch co-ordinate ascent (BCAVI) for
updates  we show different convergence behavior with respect to different initial-
izations. When the parameters are known  we show that a random initialization can
converge to the ground truth  whereas in the case when the parameters themselves
need to be estimated  a random initialization will converge to an uninformative
local optimum.

1

Introduction

Variational approximation has recently gained a huge momentum in contemporary Bayesian statis-
tics [13  5  11]. Mean ﬁeld is the simplest type of variational approximation  and is a popular tool in
large scale Bayesian inference. It is particularly useful for problems which involve complicated latent
structure  so that direct computation with the likelihood is not feasible. The main idea of variational
approximation is to obtain a tractable lower bound on the complete log-likelihood of any model. This
is  in fact  akin to the Expectation Maximization algorithm [6]  where one obtains a lower bound on
the marginal log-likelihood function via the expectation with respect to the conditional distribution of
the latent variables under the current estimates of the underlying parameters. In contrast  for mean
ﬁeld variational approximation  the lower bound or ELBO is computed using the expectation with
respect to a product distribution over the latent variables.
While there are many advances in developing new mean ﬁeld type approximation methods for
Bayesian models  the theoretical behavior of these algorithms is not well understood. There is one
line of work that studies the asymptotic consistency of variational inference. Most of the existing
theoretical work focuses on the global optimizer of variational methods. For example  for Latent

∗Equal contribution.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Dirichlet Allocation (LDA) [5] and Gaussian mixture models  it is shown in [16] that the global
optimizer is statistically consistent. [23] connects variational estimators to proﬁle M-estimation 
and shows consistency and asymptotic normality of those estimators. For Stochastic Blockmodels
(SBM) [10  9]  [3] shows that the global optimizer of the variational log-likelihood is consistent
and asymptotically normal. For more general cases  [22] proves a variational Bernstein-von Mises
theorem  which states that the variational posterior converges to “the Kullback-Leibler minimizer of
a normal distribution  centered at the truth”.
Recently  a lot more effort is being directed towards understanding the statistical convergence behavior
of non-convex algorithms in general. For Gaussian mixture models and exponential families with
missing data  [19  21] prove local convergence to the true parameters. The same authors also show
that the covariance matrix from variational Bayesian approximation for the Gaussian mixture model is
“too small” compared with that obtained for the maximum likelihood estimator [20]. The robustness
of variational Bayes estimators is further discussed in [8]. For LDA  [2] shows that  with proper
initialization  variational inference algorithms converge to the global optimum.
To be concrete  let us take the community detection problem in networks. Here the latent structure
involves unknown community memberships. Optimization of the likelihood involves a combinatorial
search  and thus is infeasible for large-scale graphs. The mean ﬁeld approximation has been used
popularly for this task [4  26]. In [3]  it was proved that the global optimum of the mean ﬁeld
approximation to the likelihood behaves optimally in the dense degree regime  where the average
expected degree of the network grows faster than the logarithm of the number of vertices.
In [26]  it is shown that if the initialization of mean ﬁeld is close enough to the truth then one gets
convergence to the truth at the minimax rate. However  in practice  it is usually not possible to
initialize like that unless one uses a pilot algorithm. Most initialization techniques like Spectral
Clustering [17  15] will return correct clustering in the dense degree regime  thus rendering the need
for mean ﬁeld updates redundant.
Indeed  in most practical scenarios  one simply uses multiple random initializations  which usually
fails miserably. However  to understand the behavior of random initializations  one needs to better
understand the landscape of the mean ﬁeld loss. There are few such works for non-convex optimization
in the literature; notable examples include [14  7  12  24]. In [24]  the authors fully characterize the
landscape of the likelihood of the equal proportion Gaussian Mixture Model with two components 
where the main message is that most random initializations should indeed converge to the ground truth.
In contrast  for topic models  it has been established that  for some parameter regimes  variational
inference exhibits instability and returns a posterior mean that is uncorrelated with the truth [7]. In
this respect  for network models  there has not been much work characterizing the behavior of the
variational loss surface.
In this article  in the context of a stochastic blockmodel  we give a complete characterization of all
the critical points and establish the behavior of random initializations for batch co-ordinate ascent
(BCAVI) updates for mean ﬁeld likelihood (with known and unknown model parameters). Our results
thus complement the results of [25].
For simplicity  we work with equal-sized two class stochastic blockmodels. We show that  when
the model parameters are known  random initializations can converge to the ground truth. We also
analyze the setting with unknown model parameters  where they are estimated jointly with the cluster
memberships. In this case  we see that indeed  with high probability  a random initialization never
converges to the ground truth  thus showing the critical importance of a good initialization for network
models.

2 Setup and preliminaries

The stochastic blockmodel SBM(B  Z  π) is a generative model of networks with community structure
on n nodes. Its dynamics is as follows: there are K communities {1  . . .   K} and each node belongs
to a single community  where this membership is captured by the rows of the n × K matrix Z 
where the ith row of Z  i.e. Zi(cid:63)  is the community membership vector of the ith node and has a
Multinomial(1; π) distribution  independently of the other rows. Given the community structure 
links between pairs of nodes are determined solely by the block memberships of the nodes in an
independent manner. That is  if A denotes the adjacency matrix of the network  then given Z  Aij

2

and Akl are independent for (i  j) (cid:54)= (k  l)  i < j  k < l  and

P(Aij = 1 | Z) = P(Aij = 1 | Zia = 1  Zjb = 1) = Bab.

B = ((Bab)) is called the block (or community) probability matrix. We have the natural restriction
that B is symmetric for undirected networks.
The block memberships are hidden variables and one only observes the network in practice. The
goal often is to ﬁt an appropriate SBM to learn the community structure  if any  and also estimate the
parameters B and π.
The complete likelihood for the SBM is given by

(cid:89)

(cid:89)

P(A  Z; B  π) =

(BAij

ab (1 − Bab)1−Aij )ZiaZjb(cid:89)
(cid:88)

P(A  Z; B  π) 

(cid:89)

a

πZia
a

.

(1)

(2)

i
As Z is not observable  if we integrate out Z  we get the data likelihood

i<j

a b

P(A; B  π) =

Z∈Z

where Z is the space of all n × K matrices with exactly one 1 in each row.
In principle we can optimize the data likelihood to estimate B and π. However  P(A; B  π) involves
a sum over a complicated large ﬁnite set (the cardinality of this set is K n)  and hence is not easy to
deal with. A well-known alternative approach is to optimize the variational log-likelihood [3]  which
has a less complicated dependency structure  the simplest of which is mean ﬁeld log-likelihood (see 
e.g.  [18]). We defer a detailed discussion of the mean ﬁeld principle in the supplementary material.
For the SBM  the variational log-likelihood with respect to a distribution ψ is given by

ψ(Z) = Eψ

ZiaZjb(θabAij − f (θab))

− KL(ψ||π⊗n) 

(cid:88)

Z

log

(cid:19)

(cid:18)P(A  Z; B  π)
(cid:16) Bab
(cid:17)

ψ(Z)

(cid:18) (cid:88)

i<j a b

1−Bab

  f (θ) = log(1 + eθ) and π⊗n denotes the product measure on Z with
where θab = log
the rows of Z being i.i.d. Multinomial(1; π). A special case of the variational log-likelihood is the
mean ﬁeld log-likelihood (see  e.g.  [18])  where one approximates Ψ by
ψj(zj)}.
(3)
i KL(ψi||π). For SBM the mean ﬁeld

Deﬁne (cid:96)M F (ψ  θ  π) =(cid:80)

ΨM F ≡ {ψ : ψ(z1  . . .   zn) =

j=1

n(cid:89)
i<j a b ψiaψjb(θabAij − f (θab))−(cid:80)
subject to(cid:88)

(cid:96)M F (ψ  θ  π)
ψia = 1  for all 1 ≤ i ≤ n

max

approximation is equivalent to optimizing (cid:96)M F (ψ  θ  π) as follows:

ψ

(cid:19)

where each ψi is a discrete probability distribution over {1  . . .   K}.

a

ψia ≥ 0  for all 1 ≤ i ≤ n  1 ≤ a ≤ K 

2.1 Mean ﬁeld updates for a two-parameter two-block SBM
Consider the stochastic blockmodel with two blocks with prior block probability π  1− π respectively
and block probability matrix B = (p− q)I + qJ  where p > q  I is the identity matrix  and J = 11(cid:62)
is the matrix of all 1’s. For simplicity  we will denote ψi1 as ψi. Then the mean ﬁeld log-likelihood is

(cid:96)(ψ  p  q  π) =

1
2

[ψi(1 − ψj) + ψj(1 − ψi)][Aij log

+ log(1 − q)]

(cid:18) q
(cid:19)
(cid:18) p

1 − q

(cid:19)

[ψiψj + (1 − ψi)(1 − ψj)][Aij log

+ log(1 − p)]

(cid:18) ψi

(cid:19)

π

[log

(cid:19)

(cid:18) 1 − ψi

1 − π

1 − p
(1 − ψi)].

(cid:88)

i j:i(cid:54)=j

+

1
2

(cid:88)
−(cid:88)

i j:i(cid:54)=j

i

ψi + log

3

∂(cid:96)
∂ψi

=

1
2

2[1 − 2ψj][Aij log

(cid:88)

j:j(cid:54)=i
1
2

(cid:88)

+

(cid:88)
2 log(cid:0) p(1−q)

q(1−p)

= 4t

j:j(cid:54)=i

2[2ψj − 1][Aij log

j:j(cid:54)=i
(ψj − 1
2

)(Aij − λ) − log

(cid:1) and λ = 1

2t log(cid:0) 1−q

(cid:19)

(cid:19)

1 − q

1 − p

+ log(1 − q)]

(cid:18) ψi

+ log(1 − p)] − log

(cid:18) q
(cid:19)
(cid:19)
(cid:18) p
(cid:18) ψi
(cid:1). Detailed calculations of other ﬁrst and second order
(cid:88)

1 − ψi

1 − ψi

 

For simplicity of exposition  we will assume that π (which is essentially a prior on the block
|C1|
memberships) is known and equals 1/2. Let Ci  i = 1  2 be the two communities. Let ˜π =
n . It is
clear that ˜π = 1
2 from the start will not change our conclusions but
make the algebra a lot nicer  which we do henceforth. Now

n ). Assuming ˜π = 1

2 + OP ( 1√

where t = 1
partial derivatives are given in Section 2 of the supplementary article [1]. The co-ordinate ascent
(CAVI) updates for ψ are

1−p

log

i

ψ(new)
1 − ψ(new)

i

= 4t

j(cid:54)=i

(ψj − 1
2

)(Aij − λ).

Introducing an intermediate variable ξ for the updates  let f (x) = log( x
iteration s  the batch version (BCAVI) of this is

1−x ) and ξi = f (ψi). Then at

ξ(s) = 4t(A − λ(J − I))(ψ(s−1) − 1
2

1) 

and ψ(s) = g(ξ(s)) with g(x) = 1/(1 + e−x). The population version (replacing A by E(A | Z) =
ZBZ(cid:62) − pI =: P − pI) of BCAVI is

ξ(s) = 4t(P − pI − λ(J − I))(ψ(s−1) − 1
2

1).

The matrix M := P − pI − λ(J − I) will appear many times later. There are updates for p  q as
well  which can be expressed compactly in terms of ψ. We describe these in detail in (8).

3 Main results

In this section  we state and discuss our main results. All the proofs appear in the supplementary
article [1].
Note: In the following  we will see the following vectors repeatedly: ψ = 1
2 1  1  0  1C1   1C2. Among
these  1 corresponds to the case where every node is assigned by ψ to C1  and  similarly  for 0  to
C2. On the other hand  1Ci are the indicators of the clusters Ci and hence correspond to the ground
truth community assignment. Finally  1
2 1 corresponds to the solution where a node belong to each
community with equal probability.
Proposition 3.1. Suppose 1 > p > q > 0. Then

1. (p−q)(1+p−q)

2(1−q)p < t < (p−q)(1−p+q)

2(1−p)q

  and

2. q < λ < p.

The eigendecomposition of P − λJ will play a crucial role in our analysis. Note that it has rank
two and two eigenvalues e± = nα±  where α+ = p+q
2   with eigenvectors 1 and
1C1 − 1C2 respectively.
Now  the eigenvalues of M are ν1 = e+ − (p − λ)  ν2 = e− − (p − λ) and νj = −(p − λ) 
j = 3  . . .   n. The eigenvector of M corresponding to ν1 is u1 = 1  and the one corresponding to ν2
is u2 = 1C1 − 1C2 .

2 − λ  α− = p−q

4

3.1 Known p  q:

In this case  we need only consider the updates for ψ. The population BCAVI updates are

ξ(s+1) = 4tM (ψ(s) − 1
2

(4)
We consider the case where the true p  q are of the same order  that is  p (cid:16) q (cid:16) ρn with ρn possibly
2 1 is a saddle point of the population mean ﬁeld log-likelihood.
going to 0. In the known p  q case 1
Proposition 3.2. ψ = 1
2 1 is a saddle point of the population mean ﬁeld log-likelihood when p and q
are known  for all n large enough.

1).

Now we will write the BCAVI updates in the eigenvector coordinates of M. To this end  deﬁne
i = (cid:104)ψ(s)  ui(cid:105)/(cid:107)ui(cid:107)2 = (cid:104)ψ(s)  ui(cid:105)/n  for i = 1  2. We can then write
ζ (s)
ψ(s) = (cid:104)ψ(s)  u1/(cid:107)u1(cid:107)(cid:105)u1/(cid:107)u1(cid:107) + (cid:104)ψ(s)  u2/(cid:107)u2(cid:107)(cid:105)u2/(cid:107)u2(cid:107) + v(s) = ζ (s)
1 u1 + ζ (s)
So  using (4) in conjunction with the above decomposition  coordinate-wise we have:

2 u2 + v(s).

(cid:19)

(cid:18)

ξ(s+1)
i

= 4tn

)α+ + σiζ (s)

2 α−

+ 4tν3

1 − 1
(ζ (s)
2

) + σiζ (s)

2 + v(s)

i

(cid:18)

1 − 1
(ζ (s)
2
+ b(s)

 

i

=: na(s)
σi

(cid:19)

(5)

(6)

where σi = 1  if i is in C1  and −1 otherwise.
Theorem 3.3 (Population behavior). The limit behavior of the population BCAVI updates is charac-
terized by the signs of α+ and a(0)±1  where α+ = (p + q)/2 − λ and a(s)±1 for iteration s is deﬁned
in (5). Assume that |na(0)±1| → ∞. Deﬁne (cid:96)(ψ(0)) = 1(a(0)
+1 > 0)1C1 + 1(a(0)−1 > 0)1C2. Then  we
have

= O(exp(−Θ(n min{|a(0)

+1| |a(0)−1|}))) = o(1).

(cid:107)ψ(1) − (cid:96)(ψ(0))(cid:107)2

n
We also have for any s ≥ 2
(cid:107)ψ(s) − (cid:96)(ψ(0))(cid:107)2

n

=

(cid:40)

O(exp(−Θ(ntα−))) 
O(exp(−Θ(nt|α+|)) 

if a(0)
if a(0)

+1a(0)−1 < 0 
+1a(0)−1 > 0  and α+ > 0.

Finally  if a(0)

+1a(0)−1 > 0 and α+ < 0  then  for any s ≥ 2  we have

(cid:26)(cid:107)ψ(s) − 1(cid:107)2

min

n

(cid:107)ψ(s) − 0(cid:107)2

n

 

(cid:27)

= O(exp(−Θ(nt|α+|)).

In fact  in this case  ψ(s) cycles between 1 and 0  in the sense that it is close to 1 is one iteration  and
to 0 in the next and so on.
Remark 3.1. We see from Theorem 3.3 that  essentially  we have exponential convergence within
two iterations.

Now we turn to the sample behavior. To distinguish from the population case  we denote the sample
BCAVI updates as

ˆξ(s+1) = 4t ˆM ( ˆψ(s) − 1
2

1) 

(7)

where ˆM = A − λ(J − I) and ˆψ(s) depends on A for s ≥ 1. Note that ˆψ(0) = ψ(0).
Theorem 3.4 (Sample behavior). For all s ≥ 1  the same conclusion as Theorem 3.3 holds for the
sample BCAVI updates with high probability as long as n|a(0)±1| (cid:29) max{√
2(cid:107)∞  1} 
√
nρn = Ω(log n) and ψ(0) is independent of A.

nρn(cid:107)ψ(0) − 1

From Theorem 3.3  we can calculate lower bounds to the volumes of the basins of attractions of the
limit points of the population BCAVI updates. We have the following corollary.

5

Corollary 3.5. Deﬁne the set of initialization points converging to a stationary point c as

Sc := {v | lim sup
s→∞

n−1(cid:107)ψ(s) − c(cid:107)2 = O(exp(−Θ(nt min{|α+|  α−})))  when ψ(0) = v}.

Let M be some measure on [0  1]n  absolutely continuous with respect to the Lebesgue measure.
Consider the stationary point 1  then

M(S1) ≥ lim
γ↑1

M(H γ

+ ∩ H γ− ∩ [0  1]n) 

where the half-spaces H γ± are given as

H γ± =(cid:8)x | (cid:104)x  α+u1 ± α−u2(cid:105) >

(cid:9).

nα+

2

+

n1−γ
4t

Similar formulas can be obtained for the other stationary points.

For speciﬁc measures M  one can obtain explicit formulas for these volumes. In practice  these are
quite easy to calculate by Monte Carlo simulations.
In fact  using arguments that goes into the proof of Theorem 3.3  we can show that in the large n limit 
there are only ﬁve stationary points of the mean ﬁeld log-likelihood  namely 1
2 1  1  0  1C1  and 1C2.

3.2 Unknown p  q:

In this case  the BCAVI updates are

p(s) =

q(s) =

t(s) =

(ψ(s−1))(cid:62)Aψ(s−1) + (1 − ψ(s−1))(cid:62)A(1 − ψ(s−1))

(ψ(s−1))(cid:62)(J − I)ψ(s−1) + (1 − ψ(s−1))(cid:62)(J − I)(1 − ψ(s−1))

(ψ(s−1))(cid:62)A(1 − ψ(s−1))

(cid:18) p(s)(1 − q(s))

(ψ(s−1))(cid:62)(J − I)(1 − ψ(s−1))
1
2

q(s)(1 − p(s))

(cid:19)

  λ(s) =

log

 

(cid:18) 1 − q(s)

1 − p(s)

(cid:19)

 

1

2t(s)

log

 

(8)

(9)

ξ(s) = 4t(s)(A − λ(s)(J − I))(ψ(s−1) − 1
2

1).

Similar to before  p (cid:16) q (cid:16) ρn with ρn possibly going to 0. In the population version  we would
replace A with E(A | Z) = P − pI.
In this case with unknown p  q  our next result shows that 1
tion 3.2) to a local maximum.
Proposition 3.6. Let n ≥ 2. Then (ψ  p  q) = ( 1
mean ﬁeld log-likelihood.

n(n−1) ) is a strict local maximum of the

2 1 changes from a saddle point (Proposi-

n(n−1)   1(cid:62)A1

2 1  1(cid:62)A1

Since p  q and ψ are unknown and need to be estimated iteratively  we have the following updates for
p(1) and q(1) given the initialization ψ(0) and show that they can be written in terms of the projection
of the initialization in the principal eigenspace of P .
Lemma 3.1. Let x = ψT ψ +(1−ψ)T (1−ψ) and y = 2ψT (1−ψ) = n−x. If ψ = ζ1u1 +ζ2u2 +w 
where w ∈ span{u1  u2}⊥  then
p + q

Since ψT (1 − ψ) > 0  we have ζ1(1 − ζ1) ≥ ζ 2

(cid:18) p + q

2

p(1) ∈

√
+ OP (

6

p(1) =

q(1) =

2

p + q

2

+

2 + y/2n2)

2 − x/2n2)

(p − q)(ζ 2
√
1 + (1 − ζ1)2 − x/n2 + OP (
ζ 2
− (p − q)(ζ 2
√
2ζ1(1 − ζ1) − y/n2 + OP (
(cid:21)

2 . This gives:
q(1) ∈

ρn/n)  p

 

p + q

(cid:20)

q 

ρn/n) 

ρn/n).

√
+ OP (

2

(cid:19)

ρn/n)

.

(10)

(11)

√

n) as n → ∞.

ρn/n) close to
It is interesting to note that p(1) is always smaller than q(1) except when it is O(
(p + q)/2. In that regime  one needs to worry about the sign of t and λ. In all other regimes  t  λ are
positive.
Using the update forms in Lemma 3.1  the following result shows that the stationary points of the
population mean ﬁeld log-likelihood lie in the principle eigenspace span{u1  u2} of P in a limiting
sense.
Proposition 3.7. Consider the case with unknown p  q and ρn → 0  nρn → ∞. Let (ψ  ˜p  ˜q) be
a stationary point of the population mean ﬁeld log-likelihood. If ψ = ψu + ψu⊥  where ψu ∈
√
span{u1  u2} and ψu⊥ ⊥ span{u1  u2}  then (cid:107)ψu⊥(cid:107) = o(
Lemma 3.1 basically shows that if ζ2 is vanishing  then p(1) and q(1) concentrates around the average
of the conditional expectation matrix  i.e. (p+q)/2. The next result shows that if one uses independent
and identically distributed initialization  then ζ2 is indeed vanishing. This is not surprising  since ζ2
measures correlation with the second eigenvector of P u2 which is basically the 1C1 − 1C2 vector.
Consider a simple random initialization  where the entries of ψ(0) are i.i.d with mean µ and show
2 with small deviations within one update. This shows the futility of random
that it converges to 1
initialization.
iid∼ fµ where f is a distribution supported
Lemma 3.2. Consider the initial distribution ψ(0)
on (0  1) with mean µ. If µ is bounded away from 0 and 1 and nρn = Ω(log2 n)  then ψ(1)
i =
1
2 + OP (log n/
Perhaps  it is also instructive to analyze the case where the initialization is in fact correlated with the
truth  i.e. E[ψ(0)
Lemma 3.3. Consider an initial ψ(0) such that

] = µσi. To this end  we will consider the following initialization scheme.

n) uniformly for all i  where ψ(1) is computed using the sample update.

√

i

i

ζ1 =

ζ2 =

(ψ(0))T 1

n

(ψ(0))T u2

=

n

µ1 + µ2
µ1 − µ2

2

=

√
+ OP (1/

n) 
√
+ OP (1/

n).

2|µ1 + µ2 − 1| + OP



2

(cid:113)

  

(cid:18) ρn log n

n(p − q)2

(cid:19)1/3  

ρn log2 n/n
p − q

(12)

(13)

If µ1  µ2 are bounded away from 0 and 1 and satisfy

|µ1 − µ2| > max

and nρn = Ω(log2 n)  then ψ(1) = 1C1 + OP (exp(−Ω(log n))) or 1C2 + OP (exp(−Ω(log n))) 
where the error term is uniform for all the coordinates.
Remark 3.2. The lemma states that provided the separation between p and q does not vanish too
fast  if the initial ψ(0) is centered around two slightly different means  e.g.  µ1 = 1/2 + cn and
µ2 = 1/2 − cn for some constant cn → 0  then we converge to the truth within one iteration.

4 Numerical results

In Figure 1-(a)  we have generated a network from an SBM with parameters p = 0.4  q = 0.025  and
two equal sized blocks of 100 nodes each. We generate 5000 initializations ψ(0) from Beta(α  β)⊗n
(for four sets of α and β) and map them to a(0)±1. We perform sample BCAVI updates on ψ(0) with
known p  q and color the points in the a(0)±1 co-ordinates according the limit points they have converged
+1a(0)−1 < 0
to. In this case  α+ > 0  hence based on Theorems 3.3 and 3.4  we expect points with a(0)
to converge to the ground truth (colored green or magenta) and those with a(0)
+1a(0)−1 > 0 to converge
to 0 or 1. As expected  points falling in the center of the ﬁrst and third quadrants have converged to 0
or 1. The points converging to the ground truth lie more toward the boundaries but mostly remain
in the same quadrants  suggesting possible perturbations arising from the sample noise and small
network size. We see that this issue is alleviated when we increase n.

7

(a)

(c)

(b)

(d)

Figure 1: n = 200 and 5000  ψ(0) ∼ Beta(α  β)⊗n for various values of α and β. These ψ(0) are
mapped to (a(0)
+1  a(0)−1) (see (5)) and plotted. C1 (magenta) and C2 (green) correspond to the limit
points 1C1 and 1C2. Other limit points are ‘Ones’  i.e. 1 (blue) and ‘Zeros’  i.e. 0 (red).

The notable thing is  in Figure 1-(a) and (d)  the Beta distribution has mean 0.16 and 0.71 respectively.
So the initialization is more skewed towards values that are closer to zero or closer to one. In these
cases most of the random runs converge to the all zeros or all ones  with very few converging to the
ground truth. However  for Figure 1-(b) and (d)  the mean of the Beta is 0.3 and 0.7  and we see
considerably more convergences to the ground truth. Also  (b) and (d) are  in some sense  mirror
images of each other  i.e. in one  the majority converges to 0; whereas in the other  the majority
converges to 1.
In Figure 2-(a)  we examine initializations of the type described in Lemma 3.3 and the resulting
estimation error. For each c0  we initialize ψ(0) such that E(ψ(0)) = (1/2 + c0)1C1 + (1/2 − c0)1C2
with iid noise. The y-axis shows the average distance between ψ(20) and the true Z from 500 such
initializations  as measured by (cid:107)ψ(20) − Z(cid:107)1/n. For every choice of p  q  a network of size 400 with
two equal sized blocks was generated. In all cases  sufﬁciently large c0 guarantees convergence to the
truth. We also observe that the performance deteriorates when p − q becomes small  either when p
decreases or when the network becomes sparser.

5 Discussion

In this paper  we work with the BCAVI mean ﬁeld variational algorithm for a simple two class
stochastic blockmodel with equal sized classes. Mean ﬁeld methods are used widely for their
scalability. However  existing theoretical works typically analyze the behavior of the global optima 
or the local convergence behavior when initialized near the ground truth. In the simple setting
considered  we show two interesting results. First  we show that  when the model parameters are

8

known  random initializations may lead to convergence to the ground truth. In contrast  when the
parameters are not known  but estimated  we show that a random initialization converges  with
high probability  to a meaningless local optimum. This shows the futility of using multiple random
initializations  which is typically done in practice when no prior knowledge is available.
In view of recent works on the optimization landscape for Gaussian mixtures [12  24]  we would like
to comment that  despite falling into the category of latent variable models  the SBM has fundamental
differences from Gaussian mixtures which require different analysis techniques. The posterior
probabilities of the latent labels in the latter model can be easily estimated when the parameters are
known  whereas this is not the case for SBM since the posterior probability P(Zi|A) depends on
the entire network. The signiﬁcance of Theorem 3.3 lies in characterizing the convergence of label
estimates given the correct parameters for general initializations  which is different from the type of
parameter convergence shown in [12  24]. Furthermore  as most of the existing literature for the SBM
focuses on estimating the labels ﬁrst  our results provide an important complementary direction by
suggesting that one could start with parameter estimation instead. A natural direction is to investigate
how robust the results on the known p  q setting are when we can estimate p and q within some small
error.
While we only show results for two classes  we expect that our main theoretical results generalize
well to K > 2 and will leave the analysis for future work. As an illustration  consider a setting
similar to that of Figure 1-(a) but for n = 450 with K = 3 equal sized classes. p = 0.5  q = 0.01
are known and ψ0 is initialized with a Dirichlet(0.1  0.1  0.1) distribution. Each row of the matrix in
Figure 2-(b) represents a stationary cluster membership vector from a random initialization.
In Figure 2  all 1000 random initializations converge to stationary points ψ lying in the span of
{1C1  1C2  1C3}  which are the membership vectors for each class. We represent the node member-

(cid:1) = 4 different types of stationary points  not counting

label permutations. Another stationary point (the all ones vector that puts everyone in the same class)
can be obtained with other initialization schemes  e.g.  when the rows of ψ(0) are identical. For a
general K- blockmodel  we conjecture that the number of stationary points grows exponentially
with K. Similar to Figure 1-(a)  a signiﬁcant fraction of the random initializations converge to the
ground truth. On the other hand  when p  q are unknown  random initializations always converge to
the uninformative stationary point (1/3  1/3  1/3)  analogous to Lemma 3.2.

ships with different colors  and there are 1 +(cid:0)3

2

(a)

(b)

Figure 2: (a) Average distance between the estimated ψ and the true Z with respect to c0  where
E(ψ(0)) = (1/2 + c0)1C1 + (1/2 − c0)1C2. (b) Convergence to stationary points for known p  q 
K = 3. Rows permuted for clarity.

Acknowledgements

SSM thanks Professor Peter J. Bickel for helpful discussions. PS is partially funded by NSF grant
DMS1713082. YXRW is supported by the ARC DECRA Fellowship.

9

References
[1] Appendix for “Mean Field for the Stochastic Blockmodel: Optimization Landscape and Conver-

gence Issues”. 2018.

[2] Pranjal Awasthi and Andrej Risteski. On some provably correct cases of variational inference
for topic models. In Advances in Neural Information Processing Systems  pages 2098–2106 
2015.

[3] Peter Bickel  David Choi  Xiangyu Chang  and Hai Zhang. Asymptotic normality of maximum
likelihood and its variational approximation for stochastic blockmodels. The Annals of Statistics 
pages 1922–1943  2013.

[4] David M. Blei  Alp Kucukelbir  and Jon D. McAuliffe. Variational inference: A review for

statisticians. Journal of the American Statistical Association  112(518):859–877  2017.

[5] David M. Blei  Andrew Y. Ng  and Michael I. Jordan. Latent Dirichlet allocation. J. Mach.

Learn. Res.  3:993–1022  March 2003.

[6] Arthur P. Dempster  Nan M. Laird  and Donald B. Rubin. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the royal statistical society. Series B (methodological) 
pages 1–38  1977.

[7] Behrooz Ghorbani  Hamid Javadi  and Andrea Montanari. An instability in variational inference

for topic models. arXiv preprint arXiv:1802.00568  2018.

[8] Ryan Giordano  Tamara Broderick  and Michael I. Jordan. Covariances  robustness  and

variational Bayes. arXiv preprint arXiv:1709.02536  2017.

[9] Jake M. Hofman and Chris H. Wiggins. Bayesian approach to network modularity. Physical

review letters  100(25):258701  2008.

[10] Paul W. Holland  Kathryn Blackmond Laskey  and Samuel Leinhardt. Stochastic blockmodels:

First steps. Social networks  5(2):109–137  1983.

[11] Tommi S. Jaakkola and Michael I. Jordon. Improving the mean ﬁeld approximation via the
use of mixture distributions. In Learning in Graphical Models  pages 163–173. MIT Press 
Cambridge  MA  USA  1999.

[12] Chi Jin  Yuchen Zhang  Sivaraman Balakrishnan  Martin J. Wainwright  and Michael I. Jordan.
Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic
conse quences. In Advances in Neural Information Processing Systems  pages 4116–4124 
2016.

[13] Michael I. Jordan  Zoubin Ghahramani  Tommi S. Jaakkola  and Lawrence K. Saul. An intro-
duction to variational methods for graphical models. Mach. Learn.  37(2):183–233  November
1999.

[14] Song Mei  Yu Bai  and Andrea Montanari. The landscape of empirical risk for non-convex

losses. 07 2016.

[15] Andrew Y. Ng  Michael I. Jordan  and Yair Weiss. On spectral clustering: Analysis and an

algorithm. In Advances in neural information processing systems  pages 849–856  2002.

[16] Debdeep Pati  Anirban Bhattacharya  and Yun Yang. On statistical optimality of variational

Bayes. arXiv preprint arXiv:1712.08983  2017.

[17] Karl Rohe  Sourav Chatterjee  and Bin Yu. Spectral clustering and the high-dimensional

stochastic blockmodel. The Annals of Statistics  pages 1878–1915  2011.

[18] Martin J. Wainwright and Michael I. Jordan. Graphical models  exponential families  and

variational inference. Found. Trends Mach. Learn.  1(1-2):1–305  January 2008.

[19] Bo Wang and D. M. Titterington. Convergence and asymptotic normality of variational Bayesian
approximations for exponential family models with missing values. In Proceedings of the 20th
conference on Uncertainty in artiﬁcial intelligence  pages 577–584. AUAI Press  2004.

10

[20] Bo Wang and D. M. Titterington. Inadequacy of interval estimates corresponding to variational

Bayesian approximations. In AISTATS  2005.

[21] Bo Wang and D. M. Titterington. Convergence properties of a general algorithm for calculating
variational Bayesian estimates for a normal mixture model. Bayesian Analysis  1(3):625–650 
2006.

[22] Yixin Wang and David M. Blei. Frequentist consistency of variational Bayes. arXiv preprint

arXiv:1705.03439  2017.

[23] Ted Westling and Tyler H. McCormick. Beyond prediction: A framework for inference with

variational approximations in mixture models. arXiv preprint arXiv:1510.08151  2015.

[24] Ji Xu  Daniel J. Hsu  and Arian Maleki. Global analysis of expectation maximization for
mixtures of two gaussians. In D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett 
editors  Advances in Neural Information Processing Systems 29  pages 2676–2684. Curran
Associates  Inc.  2016.

[25] Anderson Y. Zhang and Harrison H. Zhou. Theoretical and computational guarantees of mean
ﬁeld variational inference for community detection. arXiv preprint arXiv:1710.11268  2017.

[26] Fengshuo Zhang and Chao Gao. Convergence rates of variational posterior distributions. arXiv

preprint arXiv:1712.02519  2017.

11

,Soumendu Sundar Mukherjee
Purnamrita Sarkar
Y. X. Rachel Wang
Bowei Yan