2014,Stochastic Gradient Descent  Weighted Sampling  and the Randomized Kaczmarz algorithm,We improve a recent gurantee of Bach and Moulines on the linear convergence of SGD for smooth and strongly convex objectives  reducing a quadratic dependence on the strong convexity to a linear dependence. Furthermore  we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence  and obtain a linear dependence on average smoothness  dominating previous results  and more broadly discus how importance sampling for SGD can improve convergence also in other scenarios. Our results are based on a connection we make between SGD and the randomized Kaczmarz algorithm  which allows us to transfer ideas between the separate bodies of literature studying each of the two methods.,Stochastic Gradient Descent  Weighted Sampling  and

the Randomized Kaczmarz algorithm

Department of Mathematical Sciences

Claremont McKenna College

Deanna Needell

Claremont CA 91711
dneedell@cmc.edu

Nathan Srebro

Toyota Technological Institute at Chicago
and Dept. of Computer Science  Technion

nati@ttic.edu

Rachel Ward

Department of Mathematics

Univ. of Texas  Austin

rward@math.utexas.edu

Abstract

We improve a recent guarantee of Bach and Moulines on the linear convergence
of SGD for smooth and strongly convex objectives  reducing a quadratic depen-
dence on the strong convexity to a linear dependence. Furthermore  we show how
reweighting the sampling distribution (i.e. importance sampling) is necessary in
order to further improve convergence  and obtain a linear dependence on average
smoothness  dominating previous results  and more broadly discus how impor-
tance sampling for SGD can improve convergence also in other scenarios. Our
results are based on a connection between SGD and the randomized Kaczmarz al-
gorithm  which allows us to transfer ideas between the separate bodies of literature
studying each of the two methods.

1

Introduction

This paper concerns two algorithms which until now have remained somewhat disjoint in the liter-
ature: the randomized Kaczmarz algorithm for solving linear systems and the stochastic gradient
descent (SGD) method for optimizing a convex objective using unbiased gradient estimates. The
connection enables us to make contributions by borrowing from each body of literature to the other.
In particular  it helps us highlight the role of weighted sampling for SGD and obtain a tighter guar-
antee on the linear convergence regime of SGD.
Our starting point is a recent analysis on convergence of the SGD iterates. Considering a stochastic
objective F (x) = Ei[fi(x)]  classical analyses of SGD show a polynomial rate on the suboptimality
of the objective value F (xk) − F (x(cid:63)). Bach and Moulines [1] showed that if F (x) is µ-strongly
convex  fi(x) are Li-smooth (i.e. their gradients are Li-Lipschitz)  and x(cid:63) is a minimizer of (almost)
all fi(x) (i.e. Pi(∇fi(x(cid:63)) = 0) = 1)  then E(cid:107)xk − x(cid:63)(cid:107) goes to zero exponentially  rather then
polynomially  in k. That is  reaching a desired accuracy of E(cid:107)xk − x(cid:63)(cid:107)2 ≤ ε requires a number of
steps that scales only logarithmically in 1/ε. Bach and Moulines’s bound on the required number of
iterations further depends on the average squared conditioning number E[(Li/µ)2].
In a seemingly independent line of research  the Kaczmarz method was proposed as an iterative
method for solving overdetermined systems of linear equations [7]. The simplicity of the method
makes it popular in applications ranging from computer tomography to digital signal processing [5 

1

9  6]. Recently  Strohmer and Vershynin [19] proposed a variant of the Kaczmarz method which
selects rows with probability proportional to their squared norm  and showed that using this selection
strategy  a desired accuracy of ε can be reached in the noiseless setting in a number of steps that
scales with log(1/ε) and only linearly in the condition number. As we discuss in Section 5  the
randomized Kaczmarz algorithm is in fact a special case of stochastic gradient descent.
Inspired by the above analysis  we prove improved convergence results for generic SGD  as well as
for SGD with gradient estimates chosen based on a weighted sampling distribution  highlighting the
role of importance sampling in SGD:
We ﬁrst show that without perturbing the sampling distribution  we can obtain a linear dependence
on the uniform conditioning (sup Li/µ)  but it is not possible to obtain a linear dependence on
the average conditioning E[Li]/µ. This is a quadratic improvement over [1] in regimes where the
components have similar Lipschitz constants (Theorem 2.1 in Section 2).
We then show that with weighted sampling we can obtain a linear dependence on the average con-
ditioning E[Li]/µ  dominating the quadratic dependence of [1] (Corollary 3.1 in Section 3).
In Section 4  we show how also for smooth but not-strongly-convex objectives  importance sampling
can improve a dependence on a uniform bound over smoothness  (sup Li)  to a dependence on the
average smoothness E[Li]—such an improvement is not possible without importance sampling.
For non-smooth objectives  we show that importance sampling can eliminate a dependence on the
variance in the Lipschitz constants of the components.
Finally  in Section 5  we turn to the Kaczmarz algorithm  and show we can improve known guaran-
tees in this context as well.

2 SGD for Strongly Convex Smooth Optimization
We consider the problem of minimizing a strongly convex function of the form F (x) = Ei∼Dfi(x)
where fi : H → R are smooth functionals over H = Rd endowed with the standard Euclidean
norm (cid:107)·(cid:107)2  or over a Hilbert space H with the norm (cid:107)·(cid:107)2. Here i is drawn from some source
distribution D over an arbitrary probability space. Throughout this manuscript  unless explicitly
speciﬁed otherwise  expectations will be with respect to indices drawn from the source distribution
D. We denote the unique minimum x(cid:63) = arg min F (x) and denote by σ2 the “residual” quantity at
the minimum  σ2 = E(cid:107)∇fi(x(cid:63))(cid:107)2
2.
Assumptions Our bounds will be based on the following assumptions and quantities: First  F has
strong convexity parameter µ; that is  (cid:104)x − y ∇F (x) − ∇F (y)(cid:105) ≥ µ(cid:107)x − y(cid:107)2
2 for all vectors x
and y. Second  each fi is continuously differentiable and the gradient function ∇fi has Lipschitz
constant Li; that is  (cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ Li(cid:107)x − y(cid:107)2 for all vectors x and y. We denote sup L
the supremum of the support of Li  i.e. the smallest L such that Li ≤ L a.s.  and similarly denote
inf L the inﬁmum. We denote the average Lipschitz constant as L = ELi.
An unbiased gradient estimate for F (x) can be obtained by drawing i ∼ D and using ∇fi(x) as the
estimate. The SGD updates with (ﬁxed) step size γ based on these gradient estimates are given by:
(2.1)
2 of the iterates

where {ik} are drawn i.i.d. from D. We are interested in the distance (cid:107)xk − x(cid:63)(cid:107)2
from the unique minimum  and denote the initial distance by ε0 = (cid:107)x0 − x(cid:63)(cid:107)2
2.
Bach and Moulines [1  Theorem 1] considered this setting1 and established that

xk+1 ← xk − γ∇fik (xk)

(cid:16)EL2

(cid:17)

k = 2 log(ε0/ε)

i

µ2 +

σ2
µ2ε

(2.2)

SGD iterations of the form (2.1)  with an appropriate step-size  are sufﬁcient
E(cid:107)xk − x(cid:63)(cid:107)2

to ensure
2 ≤ ε  where the expectation is over the random sampling. As long as σ2 = 0  i.e. the
1Bach and Moulines’s results are somewhat more general. Their Lipschitz requirement is a bit weaker and
more complicated  but in terms of Li yields (2.2). They also study the use of polynomial decaying step-sizes 
but these do not lead to improved runtime if the target accuracy is known ahead of time.

2

E(cid:107)xk − x(cid:63)(cid:107)2

1 − 2γµ(1 − γ sup L)

That is  for any desired ε  using a step-size of

2 ≤(cid:104)

µε

(cid:17)(cid:105)k (cid:107)x0 − x(cid:63)(cid:107)2

2 +

γσ2

µ(cid:0)1 − γ sup L(cid:1) .
(cid:17)
(cid:16) sup L

+

σ2
µ2ε

µ

(2.3)

(2.4)

same minimizer x(cid:63) minimizes all components fi(x) (though of course it need not be a unique min-
imizer of any of them); this yields linear convergence to x(cid:63)  with a graceful degradation as σ2 > 0.
However  in the linear convergence regime  the number of required iterations scales with the ex-
pected squared conditioning EL2
i /µ2. In this paper  we reduce this quadratic dependence to a linear
dependence. We begin with a guarantee ensuring linear dependence on sup L/µ:
Theorem 2.1 Let each fi be convex where ∇fi has Lipschitz constant Li  with Li ≤ sup L a.s. 
and let F (x) = Efi(x) be µ-strongly convex. Set σ2 = E(cid:107)∇fi(x(cid:63))(cid:107)2
2  where x(cid:63) = argminx F (x).
Suppose that γ ≤ 1/µ. Then the SGD iterates given by (2.1) satisfy:

γ =

2εµ sup L + 2σ2

ensures that after k = 2 log(ε0/ε)

SGD iterations  E(cid:107)xk − x(cid:63)(cid:107)2
respect to the sampling of {ik}.
Proof sketch: The crux of the improvement over [1] is a tighter recursive equation. Instead of:

2 ≤ ε  where ε0 = (cid:107)x0 − x(cid:63)(cid:107)2

2 and where both expectations are with

2 ≤(cid:0)1 − 2γµ + 2γ2L2
2 ≤(cid:0)1 − 2γµ + 2γ2µLik

(cid:1)(cid:107)xk − x(cid:63)(cid:107)2
(cid:1)(cid:107)xk − x(cid:63)(cid:107)2

ik

2 + 2γ2σ2 

2 + 2γ2σ2.

(cid:107)xk+1 − x(cid:63)(cid:107)2

(cid:107)xk+1 − x(cid:63)(cid:107)2

we use the co-coercivity Lemma (Lemma A.1 in the supplemental material) to obtain:

The signiﬁcant difference is that one of the factors of Lik  an upper bound on the second derivative
(where ik is the random index selected in the kth iteration) in the third term inside the parenthesis 
is replaced by µ  a lower bound on the second derivative of F . A complete proof can be found in the
supplemental material.

Comparison to [1] Our bound (2.4) improves a quadratic dependence on µ2 to a linear depen-
dence and replaces the dependence on the average squared smoothness EL2
i with a linear dependence
on the smoothness bound sup L. When all Lipschitz constants Li are of similar magnitude  this is a
quadratic improvement in the number of required iterations. However  when different components
fi have widely different scaling  i.e. Li are highly variable  the supremum might be signiﬁcantly
larger then the average square conditioning.

Tightness Considering the above  one might hope to obtain a linear dependence on the average
smoothness L. However  as the following example shows  this is not possible. Consider a uniform
source distribution over N + 1 quadratics  with the ﬁrst quadratic f1 being N (x[1] − b)2 and all
others being x[2]2  and b = ±1. Any method must examine f1 in order to recover x to within
error less then one  but by uniformly sampling indices i  this takes N iterations in expectation.
We can calculate sup L = L1 = 2N  L = 2(2N−1)
  and µ = 1. Both
sup L/µ = EL2
i /µ2 = O(N ) scale correctly with the expected number of iterations  while error
reduction in O(L/µ) = O(1) iterations is not possible for this example.
We therefore see that the choice between EL2
i and sup L is unavoidable. In the next Section  we
will show how we can obtain a linear dependence on the average smoothness L  using importance
sampling  i.e. by sampling from a modiﬁed distribution.

i = 4(N 2+N−1)

  EL2

N

N

Importance Sampling

3
For a weight function w(i) which assigns a non-negative weight w(i) ≥ 0 to each index i  the
weighted distribution D(w) is deﬁned as the distribution such that
PD(w) (I) ∝ Ei ∼D [1I (i)w(i)]  

3

where I is an event (subset of indices) and 1I (·) its indicator function. For a discrete distribution
D with probability mass function p(i) this corresponds to weighting the probabilities to obtain a
new probability mass function  which we write as p(w)(i) ∝ w(i)p(i). Similarly  for a continuous
distribution  this corresponds to multiplying the density by w(i) and renormalizing. Importance
sampling has appeared in both the Kaczmarz method [19] and in coordinate-descent methods [14 
15]  where the weights are proportional to some power of the Lipschitz constants (of the gradient
coordinates). Here we analyze this type of sampling in the context of SGD.
One way to construct D(w) is through rejection sampling: sample i ∼ D  and accept with probability
w(i)/W   for some W ≥ supi w(i). Otherwise  reject and continue to re-sample until a suggestion
i is accepted. The accepted samples are then distributed according to D(w).
We use E(w)[·] = Ei∼D(w) [·] to denote expectation where indices are sampled from the weighted
distribution D(w). An important property of such an expectation is that for any quantity X(i):

w(i) X(i)

= E [w(i)] · E [X(i)]  

(3.1)

In particular  when
In fact  we will consider only weights

E(w)(cid:104) 1
E[w(i)] = 1  we have that E(w)(cid:104) 1

(cid:105)
(cid:105)

where recall that the expectations on the r.h.s. are with respect to i ∼ D.

= EX(i).
s.t. E[w(i)] = 1  and refer to such weights as normalized.

w(i) X(i)

Reweighted SGD For any normalized weight function w(i)  we can write:

f (w)
i

(x) =

1

w(i)

fi(x)

and F (x) = E(w)[f (w)

i

(x)].

(3.2)

This is an equivalent  and equally valid  stochastic representation of the objective F (x)  and we can
just as well base SGD on this representation. In this case  at each iteration we sample i ∼ D(w)
and then use ∇f (w)
w(i)∇fi(x) as an unbiased gradient estimate. SGD iterates based on the
representation (3.2)  which we will refer to as w-weighted SGD  are then given by

(x) = 1

i

xk+1 ← xk − γ

w(ik)

∇fik (xk)

(3.3)

where {ik} are drawn i.i.d. from D(w).
The important observation here is that all SGD guarantees are equally valid for the w-weighted
updates (3.3)–the objective is the same objective F (x)  the sub-optimality is the same  and the
minimizer x(cid:63) is the same. We do need  however  to calculate the relevant quantities controlling SGD
convergence with respect to the modiﬁed components f (w)

and the weighted distribution D(w).

i

Strongly Convex Smooth Optimization using Weighted SGD We now return to the analysis of
strongly convex smooth optimization and investigate how re-weighting can yield a better guarantee.
The Lipschitz constant L(w)
w(i) Li.
The supremum is then given by:

is now scaled  and we have L(w)

of each component f (w)

i = 1

i

i

sup L(w) = sup

i

L(w)

i = sup

i

Li
w(i)

.

It is easy to verify that (3.4) is minimized by the weights

w(i) =

Li
L

 

so that

sup L(w) = sup

i

Li

Li/L

= L.

Before applying Theorem 2.1  we must also calculate:

(3.4)

(3.5)

(w) = E(w)[(cid:107)∇f (w)
σ2

i

(x(cid:63))(cid:107)2

2] = E[

1

w(i)

(cid:107)∇fi(x(cid:63))(cid:107)2

2] = E[

L
Li

(cid:107)∇fi(x(cid:63))(cid:107)2

2] ≤ L
inf L

σ2.

(3.6)

4

(cid:16) L

(cid:16) sup L(w)

(cid:17)

Now  applying Theorem 2.1 to the w-weighted SGD iterates (3.3) with weights (3.5)  we have that 
with an appropriate stepsize 

µ

L

+

= 2 log(ε0/ε)

k = 2 log(ε0/ε)

σ2
(w)
(3.7)
+
µ2ε
iterations are sufﬁcient for E(w)(cid:107)xk − x(cid:63)(cid:107)2
2 ≤ ε  where x(cid:63)  µ and ε0 are exactly as in Theorem 2.1.
If σ2 = 0  i.e. we are in the “realizable” situation  with true linear convergence  then we also have
(w) = 0. In this case  we already obtain the desired guarantee: linear convergence with a linear
σ2
dependence on the average conditioning L/µ  strictly improving over the best known results [1].
However  when σ2 > 0 we get a dissatisfying scaling of the second term  by a factor of L/inf L.
Fortunately  we can easily overcome this factor. To do so  consider sampling from a distribution
which is a mixture of the original source distribution and its re-weighting:

· σ2
µ2ε

inf L

µ

(cid:17)

w(i) =

1
2

+

1
2

· Li
L

.

(3.8)

We refer to this as partially biased sampling. Instead of an even mixture as in (3.9)  we could also
use a mixture with any other constant proportion  i.e. w(i) = λ + (1 − λ)Li/L for 0 < λ < 1.
Using these weights  we have
2 · Li

Li ≤ 2L and σ2

(cid:107)∇fi(x(cid:63))(cid:107)2

2] ≤ 2σ2.

sup L(w) = sup

1
2 + 1

1
2 + 1

(w) = E[

2 · Li

(3.9)

1

1

i

L

L

Corollary 3.1 Let each fi be convex where ∇fi has Lipschitz constant Li and let F (x) =
Ei∼D[fi(x)]  where F (x) is µ-strongly convex.
2  where x(cid:63) =
argminx F (x). For any desired ε  using a stepsize of

Set σ2 = E(cid:107)∇fi(x(cid:63))(cid:107)2

(cid:16) L

µ

(cid:17)

+

σ2
µ2ε

(3.10)
2 ≤ ε  where

µε

γ =

4(εµL + σ2)

ensures that after k = 4 log(ε0/ε)

2 and L = ELi.

iterations of w-weighted SGD (3.3) with weights speciﬁed by (3.8)  E(w)(cid:107)xk − x(cid:63)(cid:107)2
ε0 = (cid:107)x0 − x(cid:63)(cid:107)2
This result follows by substituting (3.9) into Theorem 2.1. We now obtain the desired linear scaling
on L/µ  without introducing any additional factor to the residual term  except for a constant factor.
We thus obtain a result which dominates Bach and Moulines (up to a factor of 2) and substantially
improves upon it (with a linear rather than quadratic dependence on the conditioning). Such “par-
tially biased weights” are not only an analysis trick  but might indeed improve actual performance
over either no weighting or the “fully biased” weights (3.5)  as demonstrated in Figure 1.

Implementing Importance Sampling In settings where linear systems need to be solved repeat-
edly  or when the Lipschitz constants are easily computed from the data  it is straightforward to
sample by the weighted distribution. However  when we only have sampling access to the source
distribution D (or the implied distribution over gradient estimates)  importance sampling might be
difﬁcult. In light of the above results  one could use rejection sampling to simulate sampling from
D(w). For the weights (3.5)  this can be done by accepting samples with probability proportional
to Li/ sup L. The overall probability of accepting a sample is then L/ sup L  introducing an addi-
tional factor of sup L/L. This yields a sample complexity with a linear dependence on sup L  as
in Theorem 2.1  but a reduction in the number of actual gradient calculations and updates. In even
less favorable situations  if Lipschitz constants cannot be bounded for individual components  even
importance sampling might not be possible.

4

Importance Sampling for SGD in Other Scenarios

In the previous Section  we considered SGD for smooth and strongly convex objectives  and were
particularly interested in the regime where the residual σ2 is low  and the linear convergence term is
dominant. Weighted SGD is useful also in other scenarios  and we now brieﬂy survey them  as well
as relate them to our main scenario of interest.

5

Figure 1: Performance of SGD with weights w(i) = λ + (1 − λ) Li
on synthetic overdetermined least
squares problems of the form (5.1) (λ = 1 is unweighted  λ = 0 is fully weighted). Left: ai are standard
spherical Gaussian  bi = (cid:104)ai  x0(cid:105) + N (0  0.12). Center: ai is spherical Gaussian with variance i  bi =
(cid:104)ai  x0(cid:105) + N (0  202). Right: ai is spherical Gaussian with variance i  bi = (cid:104)ai  x0(cid:105) + N (0  0.12). In all
cases  matrix A with rows ai is 1000 × 100 and the corresponding least squares problem is strongly convex;
the stepsize was chosen as in (3.10).

L

Figure 2: Performance of SGD with weights w(i) = λ + (1 − λ) Li
on synthetic underdetermined least
squares problems of the form (5.1) (λ = 1 is unweighted  λ = 0 is fully weighted). We consider 3 cases. Left:
ai are standard spherical Gaussian  bi = (cid:104)ai  x0(cid:105)+N (0  0.12). Center: ai is spherical Gaussian with variance
i  bi = (cid:104)ai  x0(cid:105) + N (0  202). Right: ai is spherical Gaussian with variance i  bi = (cid:104)ai  x0(cid:105) + N (0  0.12).
In all cases  matrix A with rows ai is 50 × 100 and so the corresponding least squares problem is not strongly
convex; the step-size was chosen as in (3.10).

L

Smooth  Not Strongly Convex When each component fi is convex  non-negative  and has an
Li-Lipschitz gradient  but the objective F (x) is not necessarily strongly convex  then after

(cid:18) (sup L)(cid:107)x(cid:63)(cid:107)2

2

(cid:19)

· F (x(cid:63)) + ε

ε

ε

k = O

(4.1)
iterations of SGD with an appropriately chosen step-size we will have F (xk) ≤ F (x(cid:63)) + ε  where
xk is an appropriate averaging of the k iterates [18]. The relevant quantity here determining the iter-
ation complexity is again sup L. Furthermore  the dependence on the supremum is unavoidable and
cannot be replaced with the average Lipschitz constant L [3  18]: if we sample gradients according
to the source distribution D  we must have a linear dependence on sup L.
The only quantity in the bound (4.1) that changes with a re-weighting is sup L—all other quantities
((cid:107)x(cid:63)(cid:107)2
2  F (x(cid:63))  and the sub-optimality ε) are invariant to re-weightings. We can therefore replace
the dependence on sup L with a dependence on sup L(w) by using a weighted SGD as in (3.3). As we
already calculated  the optimal weights are given by (3.5)  and using them we have sup L(w) = L.
In this case  there is no need for partially biased sampling  and we obtain that

(cid:18) L(cid:107)x(cid:63)(cid:107)2

2

ε

k = O

(cid:19)

· F (x(cid:63)) + ε

ε

(4.2)

iterations of weighed SGD updates (3.3) using the weights (3.5) sufﬁce. Empirical evidence suggests
that this is not a theoretical artifact; full weighted sampling indeed exhibits better convergence rates
compared to partially biased sampling in the non-strongly convex setting (see Figure 2)  in contrast

6

010002000300010−1100101Iteration kError || xk − x* ||2 λ = 0λ = 0.2λ = 101000200030004000500010−1100101Iteration kError || xk − x* ||2 λ = 0λ = 0.2λ = 101000200030004000500010−1100101Iteration kError || xk − x* ||2 λ = 0λ = 0.2λ = 10200040006000800010−2100102Iteration kError F(xk) − F(x*) λ=0λ=.4λ=1050001000015000100102Iteration kError F(xk) − F(x*) λ=0λ=.4λ=1050001000015000100101Iteration kError F(xk) − F(x*) λ=0λ = .4λ = 1to the strongly convex regime (see Figure 1). We again see that using importance sampling allows us
to reduce the dependence on sup L  which is unavoidable without biased sampling  to a dependence
on L. An interesting question for further consideration is to what extent importance sampling can
also help with stochastic optimization procedures such as SAG [8] and SDCA [17] which achieve
faster convergence on ﬁnite data sets. Indeed  weighted sampling was shown empirically to achieve
faster convergence rates for SAG [16]  but theoretical guarantees remain open.

Non-Smooth Objectives We now turn to non-smooth objectives  where the components fi might
not be smooth  but each component is Gi-Lipschitz. Roughly speaking  Gi is a bound on the ﬁrst
derivative (the subgradients) of fi  while Li is a bound on the second derivatives of fi. Here 
the performance of SGD (actually stochastic subgradient decent) depends on the second moment
G2 = E[G2
i ] [12]. The precise iteration complexity depends on whether the objective is strongly
convex or whether x(cid:63) is bounded  but in either case depends linearly on G2.
Using weighted SGD  we get linear dependence on

(w) = E(w)(cid:104)

G2

)2(cid:105)

(cid:20) G2

i

(cid:21)

w(i)2

(cid:21)

(cid:20) G2

i
w(i)

(G(w)

i

= E(w)

= E

(4.3)

i = Gi/w(i) is the Lipschitz constant of the scaled f (w)

where G(w)
weights w(i) = Gi/G  where G = EGi  yielding G2
(w) = G
therefore reduce the dependence on G2 to a dependence on G
2
G
Parallel work we recently became aware of [22] shows a similar improvement for a non-smooth
composite objective. Rather than relying on a specialized analysis as in [22]  here we show this
follows from SGD analysis applied to different gradient estimates.

. This is minimized by the
2. Using importance sampling  we
2. Its helpful to recall that G2 =

+ Var[Gi]. What we save is thus exactly the variance of the Lipschitz constants Gi.

i

Non-Realizable Regime Returning to the smooth and strongly convex setting of Sections 2 and 3 
let us consider more carefully the residual term σ2 = E(cid:107)∇fi(x(cid:63))(cid:107)2
2. This quantity depends on the
weighting  and in Section 3  we avoided increasing it  introducing partial biasing for this purpose.
However  if this is the dominant term  we might want to choose weights to minimize this term. The
optimal weights here would be proportional to (cid:107)∇fi(x(cid:63))(cid:107)2  which is not known in general.
An alternative approach is to bound (cid:107)∇fi(x(cid:63))(cid:107)2 ≤ Gi and so σ2 ≤ G2. Taking this bound  we are
back to the same quantity as in the non-smooth case  and the optimal weights are proportional to Gi.
Note that this differs from using weights proportional to Li  which optimize the linear-convergence
term as studied in Section 3.
To understand how weighting according to Gi and Li are different  consider a generalized linear
objective fi(x) = φi((cid:104)zi  x(cid:105))  where φi is a scalar function with bounded |φ(cid:48)
i |. We have
that Gi ∝ (cid:107)zi(cid:107)2 while Li ∝ (cid:107)zi(cid:107)2
2. Weighting according to (3.5)  versus weighting with w(i) =
2 versus (cid:107)zi(cid:107)2  and are rather different. E.g. 
Gi/G  thus corresponds to weighting according to (cid:107)zi(cid:107)2
weighting by Li ∝ (cid:107)zi(cid:107)2
(w) = G2: the same sub-optimal dependence as if no weighting
at all were used. A good solution could be to weight by a mixture of Gi and Li  as in the partial
weighting scheme of Section 3.

2 yields G2

i|  |φ(cid:48)(cid:48)

5 The least squares case and the Randomized Kaczmarz Method

A special case of interest is the least squares problem  where
((cid:104)ai  x(cid:105) − bi)2 =

F (x) =

n(cid:88)

i=1

1
2

(cid:107)Ax − b(cid:107)2

2

(5.1)

1
2

with b ∈ Cn  A an n × d matrix with rows ai  and x(cid:63) = argminx
2 is the least-squares
solution. We can also write (5.1) as a stochastic objective  where the source distribution D is uniform
over {1  2  . . .   n} and fi = n
2 is the residual error

2(cid:107)Ax − b(cid:107)2
2 ((cid:104)ai  x(cid:105)− bi)2. In this setting  σ2 = (cid:107)Ax(cid:63) − b(cid:107)2

1

7

at the least squares solution x(cid:63)  which can also be interpreted as noise variance in a linear regression
model.
The randomized Kaczmarz method introduced for solving the least squares problem (5.1) in the
case where A is an overdetermined full-rank matrix  begins with an arbitrary estimate x0  and in the
kth iteration selects a row i at random from the matrix A and iterates by:

xk+1 = xk + c · bi − (cid:104)ai  xk(cid:105)

(cid:107)ai(cid:107)2

2

ai 

(5.2)

where c = 1 in the standard method. This is almost an SGD update with step-size γ = c/n  except
for the scaling by (cid:107)ai(cid:107)2
2.
Strohmer and Vershynin [19] provided the ﬁrst non-asymptotic convergence rates  showing that
drawing rows proportionally to (cid:107)ai(cid:107)2
2 leads to provable exponential convergence in expectation [19].
With such a weighting  (5.2) is exactly weighted SGD  as in (3.3)  with the fully biased weights
(3.5).
The reduction of the quadratic dependence on the conditioning to a linear dependence in Theorem
2.1  and the use of biased sampling  was inspired by the analysis of [19]. Indeed  applying Theorem
2.1 to the weighted SGD iterates with weights as in (3.5) and a stepsize of γ = 1 yields precisely the
guarantee of [19]. Furthermore  understanding the randomized Kaczmarz method as SGD  allows
us to obtain the following improvements:
Partially Biased Sampling. Using partially biased sampling weights (3.8) yields a better depen-
dence on the residual over the fully biased sampling weights (3.5) considered by [19].
Using Step-sizes. The randomized Kaczmarz method with weighted sampling exhibits exponential
convergence  but only to within a radius  or convergence horizon  of the least-squares solution [19 
10]. This is because a step-size of γ = 1 is used  and so the second term in (2.3) does not vanish.
It has been shown [21  2  20  4  11] that changing the step size can allow for convergence inside of
this convergence horizon  but only asymptotically. Our results allow for ﬁnite-iteration guarantees
with arbitrary step-sizes and can be immediately applied to this setting.
Uniform Row Selection. Strohmer and Vershynin’s variant of the randomized Kaczmarz method
calls for weighted row sampling  and thus requires pre-computing all the row norms. Although
certainly possible in some applications  in other cases this might be better avoided. Understanding
the randomized Kaczmarz as SGD allows us to apply Theorem 2.1 also with uniform weights (i.e. to
the unbiased SGD)  and obtain a randomized Kaczmarz using uniform sampling  which converges
to the least-squares solution and enjoys ﬁnite-iteration guarantees.

6 Conclusion

We consider this paper as making three main contributions. First  we improve the dependence on
the conditioning for smooth and strongly convex SGD from quadratic to linear. Second  we investi-
gate SGD and importance sampling and show how it can yield improvements not possible without
reweighting. Lastly  we make connections between SGD and the randomized Kaczmarz method.
This connection along with our new results show that the choice in step-size of the Kaczmarz method
offers a tradeoff between convergence rate and horizon and also allows for a convergence bound
when the rows are sampled uniformly.
For simplicity  we only considered SGD with ﬁxed step-size γ  which is appropriate when the target
accuracy in known in advance. Our analysis can be adapted also to decaying step-sizes.
Our discussion of importance sampling is limited to a static reweighting of the sampling distribution.
A more sophisticated approach would be to update the sampling distribution dynamically as the
method progresses  and as we gain more information about the relative importance of components
(e.g. about (cid:107)∇fi(x(cid:63))(cid:107)). Such dynamic sampling is sometimes attempted heuristically  and obtaining
a rigorous framework for this would be desirable.

8

References
[1] F. Bach and E. Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine

learning. Advances in Neural Information Processing Systems (NIPS)  2011.

[2] Y. Censor  P. P. B. Eggermont  and D. Gordon. Strong underrelaxation in Kaczmarz’s method for incon-

sistent systems. Numerische Mathematik  41(1):83–92  1983.

[3] R. Foygel and N. Srebro. Concentration-based guarantees for low-rank matrix reconstruction. 24th Ann.

Conf. Learning Theory (COLT)  2011.

[4] M. Hanke and W. Niethammer. On the acceleration of Kaczmarz’s method for inconsistent linear systems.

Linear Algebra and its Applications  130:83–98  1990.

[5] G. T. Herman. Fundamentals of computerized tomography: image reconstruction from projections.

Springer  2009.

[6] G. N Hounsﬁeld. Computerized transverse axial scanning (tomography): Part 1. description of system.

British Journal of Radiology  46(552):1016–1022  1973.

[7] S. Kaczmarz. Angen¨aherte auﬂ¨osung von systemen linearer gleichungen. Bull. Int. Acad. Polon. Sci. Lett.

Ser. A  pages 335–357  1937.

[8] N. Le Roux  M. W. Schmidt  and F. Bach. A stochastic gradient method with an exponential convergence
rate for ﬁnite training sets. Advances in Neural Information Processing Systems (NIPS)  pages 2672–2680 
2012.

[9] F. Natterer. The mathematics of computerized tomography  volume 32 of Classics in Applied Mathemat-
ics. Society for Industrial and Applied Mathematics (SIAM)  Philadelphia  PA  2001. ISBN 0-89871-493-
1. doi: 10.1137/1.9780898719284. URL http://dx.doi.org/10.1137/1.9780898719284.
Reprint of the 1986 original.

[10] D. Needell.
(2):395–403 
http://dx.doi.org/10.1007/s10543-010-0265-5.

Randomized Kaczmarz

ISSN 0006-3835.

for
doi:

solver

2010.

linear

noisy
10.1007/s10543-010-0265-5.

systems.

BIT 

50
URL

[11] D. Needell and R. Ward. Two-subspace projection method for coherent overdetermined linear systems.

Journal of Fourier Analysis and Applications  19(2):256–269  2013.

[12] Arkadi Nemirovski. Efﬁcient methods in convex programming. 2005.
[13] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer  2004.
[14] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM J.

Optimiz.  22(2):341–362  2012.

[15] P. Richt´arik and M. Tak´aˇc.

Iteration complexity of randomized block-coordinate descent methods for

minimizing a composite function. Math. Program.  pages 1–38  2012.

[16] M. Schmidt  N. Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient. arXiv

preprint arXiv:1309.2388  2013.

[17] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss. J. Mach.

Learn. Res.  14(1):567–599  2013.

[18] N. Srebro  K. Sridharan  and A. Tewari. Smoothness  low noise and fast rates. In Advances in Neural

Information Processing Systems  2010.

[19] T. Strohmer and R. Vershynin. A randomized Kaczmarz algorithm with exponential convergence. J.
Fourier Anal. Appl.  15(2):262–278  2009. ISSN 1069-5869. doi: 10.1007/s00041-008-9030-4. URL
http://dx.doi.org/10.1007/s00041-008-9030-4.

[20] K. Tanabe. Projection method for solving a singular system of linear equations and its applications.

Numerische Mathematik  17(3):203–214  1971.

[21] T. M. Whitney and R. K. Meany. Two algorithms related to the method of steepest descent. SIAM Journal

on Numerical Analysis  4(1):109–118  1967.

[22] P. Zhao and T. Zhang. Stochastic optimization with importance sampling. Submitted  2014.

9

,Deanna Needell
Rachel Ward
Nati Srebro