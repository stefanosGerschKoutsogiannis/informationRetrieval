2013,Message Passing Inference with Chemical Reaction Networks,Recent work on molecular programming has explored new possibilities for computational abstractions with biomolecules  including logic gates  neural networks  and linear systems.  In the future such abstractions might enable nanoscale devices that can sense and control the world at a molecular scale.  Just as in macroscale robotics  it is critical that such devices can learn about their environment and reason under uncertainty. At this small scale  systems are typically modeled as chemical reaction networks. In this work  we develop a procedure that can take arbitrary probabilistic graphical models  represented as factor graphs over discrete random variables  and compile them into chemical reaction networks that implement inference.  In particular  we show that marginalization based on sum-product message passing can be implemented in terms of reactions between chemical species whose concentrations represent probabilities.  We show algebraically that the steady state concentration of these species correspond to the marginal distributions of the random variables in the graph and validate the results in simulations.  As with standard sum-product inference  this procedure yields exact results for tree-structured graphs  and approximate solutions for loopy graphs.,Message Passing Inference with Chemical Reaction

Networks

Wyss Institute for Biologically Inspired Engineering

School of Engineering and Applied Sciences

Nils Napp

Ryan Prescott Adams

Harvard University

Cambridge  MA 02138

Harvard University

Cambridge  MA 02138

nnapp@wyss.harvard.edu

rpa@seas.harvard.edu

Abstract

Recent work on molecular programming has explored new possibilities for com-
putational abstractions with biomolecules  including logic gates  neural networks 
and linear systems. In the future such abstractions might enable nanoscale devices
that can sense and control the world at a molecular scale. Just as in macroscale
robotics  it is critical that such devices can learn about their environment and rea-
son under uncertainty. At this small scale  systems are typically modeled as chem-
ical reaction networks. In this work  we develop a procedure that can take arbitrary
probabilistic graphical models  represented as factor graphs over discrete random
variables  and compile them into chemical reaction networks that implement infer-
ence. In particular  we show that marginalization based on sum-product message
passing can be implemented in terms of reactions between chemical species whose
concentrations represent probabilities. We show algebraically that the steady state
concentration of these species correspond to the marginal distributions of the ran-
dom variables in the graph and validate the results in simulations. As with stan-
dard sum-product inference  this procedure yields exact results for tree-structured
graphs  and approximate solutions for loopy graphs.

1 Introduction

Recent advances in nanoscale devices and biomolecular synthesis have opened up new and exciting
possibilities for constructing microscopic systems that can sense and autonomously manipulate the
world. Necessary to such advances is the development of computational mechanisms and associated
abstractions for algorithmic control of these nanorobots. Work on molecular programming has ex-
plored the power of chemical computation [3  6  11] and resulted in in vitro biomolecular implemen-
tations of various such abstractions  including logic gates [16]  artiﬁcial neural networks [9  10  14] 
tiled self-assembly models [12  15]  and linear functions and systems [4  13  20]. Similarly  in
vivo gene regulatory networks can be designed that when transformed into cells implement devices
such as oscillators [8]  intracellularly coupled oscillators [7]  or distributed algorithms like pattern
formation [1]. Many critical information processing tasks can be framed in terms of probabilistic
inference  in which noisy or incomplete information is accumulated to produce statistical estimates
of hidden structure. In fact  we believe that this particular computational abstraction is ideally suited
to the noisy and often poorly characterized microscopic world. In this work  we develop a chemical
reaction network for performing inference in probabilistic graphical models. We show that message
passing schemes  such as belief propagation  map relatively straightforwardly onto sets of chemical
reactions  which can be thought of as the “assembly language” of both in vitro and in vivo compu-
tation at the molecular scale. The long-term possibilities of such technology are myriad: adaptive
tissue-sensitive drug delivery  in situ chemical sensing  and identiﬁcation of disease states.

1

Abstract Problem & Algorithm

Low-Level ”Assembly” Language

Physical

Implementation

ψ1

ψ2

P(1→1)

S(1→1)

P(2→2)

S(2→2)

P(1→3)

P(2→3)

x1

ψ3

x2

S(3→1)

S(3→2)

(a)

S(3→2)

0

S(3→2)

0

1

S(1→1)
S(1→1)

2

κr
GGGGB S(1→1)
κr
GGGGB S(1→1)

0

0

.
.
.

1

+ P(1→3)
+ P(1→3)

2

ψ3(1 1)
GGGGGGGB S(3→2)
ψ3(2 1)
GGGGGGGB S(3→2)

1

1

.
.
.
(b)

1

+ P(1→3)
+ P(1→3)

2

(c)

Figure 1:
Inference at different levels of abstraction. (a) Factor graph over two random variables.
Inference can be performed efﬁciently by passing messages (shown as gray arrows) between ver-
tices  see Section 2. (b) Message passing implemented at a lower level of abstraction. Chemical
species represent the different components of message vectors. The chemical reaction networks
constructed in Section 3 perform the same computation as the sum-product message passing algo-
rithm. (c) Schematic representation of DNA strand displacement. A given reaction network can be
implemented in different physical systems  e.g. DNA strand displacement cascades [5  17].

At the small scales of interest systems are typically modeled as deterministic chemical reaction net-
works or their stochastic counterparts that explicitly model ﬂuctuations and noise. However  chem-
ical reactions are not only models  but can be thought of as speciﬁcations or abstract computational
frameworks themselves. For example  arbitrary reaction networks can be simulated by DNA strand
displacement systems [5  17]  where some strands correspond to the chemical species in the specify-
ing reaction network. Reactions rates in these systems can be tuned over many orders of magnitude
by adjusting the toehold length of displacement steps  and high order reactions can be approximated
by introducing auxiliary species. We take advantage of this abstraction by ”compiling” the sum-
product algorithm for discrete variable factor graphs into a chemical reaction network  where the
concentrations of some species represent conditional and marginal distributions of variables in the
graph. In some ways  this representation is very natural: while normalization is a constant concern
in digital systems  our chemical design conserves species within some subsets and thus implicitly
and continuously normalizes its estimates. The computation is complete when the reaction network
reaches equilibrium. Variables in the graph can be conditioned upon by adjusting the reaction rates
corresponding to unary potentials in the graph.

Section 2 provides a brief review of factor graphs and the sum-product algorithm. Section 3 in-
troduces notation and concepts for chemical reaction networks. Section 4 shows how inference on
factor graphs can be compiled into reaction networks  and in Section 5  we show several example
networks and compare the results of molecular simulations to standard digital inference procedures.

To aid parsing the potentially tangled notation resulting from mixing probabilistic inference tools
with chemical reaction models  this paper follows these general notational guidelines: capital letters
denote constants  such as set sizes  and other quantities  such as tuples and message types; lower
case letters denote parameters  such as reaction rates and indices; bold face letters denote vectors
and subscripts elements of that vector; scripted upper letters indicate sets; random variables are
always denoted by x or their vector version; and species names have a sans-serif font.

2 Graphical Models and Probabilistic Inference

Graphical models are popular tools for reasoning about complicated probability distributions. In
most types of graphical models  vertices represent random variables and edges reﬂect dependence
structure. Here  we focus on the factor graph formalism  in which there are two types of vertices
that have a bipartite structure: variable nodes (typically drawn as circles)  which represent random
variables  and factor nodes (typically drawn as squares)  which represent potentials (also called
compatibility functions) coupling the random variables. Factor graphs  encode the factorization of a
probability distribution and therefore its conditional independence structure. Other graphical mod-
els  such as bayesian networks  can be converted to factor graphs  and thus factor graph algorithms
are directly applicable to other types of graphical models  see [2  Ch. 8].

2

Let G be a factor graph over N random variables {xn}N
n=1 where xn takes one of Kn discrete values.
The global N -dimensional random variable x takes on values in the (potentially huge) product space
K = QN
n=1{1  ...  Kn}. The other nodes of G are called factors and every edge in G connects
exactly one factor node and one variable node. In general  G can have J factors {ψj(xj)}J
j=1 where
we use xj to indicate the subset of random variables that are neighbors of factor j  i.e. {xn | n ∈
ne(j)}. Each xj takes on values in the (potentially much smaller) space Kj = Qn∈ne(j){1  ...  Kn} 
and each ψj is a non-negative scalar function on Kj. Together the structure of G and the particular
factors ψj deﬁne a joint distribution on x

Pr(x) = Pr(x1  x2  · · ·   xN ) =

1
Z

J

Y
j=1

ψj(xj) 

(1)

where Z is the appropriate normalizing constant. Figure 1a shows a simple factor graph with two
variable nodes and three factors. It implies that the the joint distribution x1 and x2 has the form
Pr(x1  x2) = 1
The sum-product algorithm (belief propagation) is an dynamic programming technique for perform-
ing marginalization in a factor graph. That is  it computes sums of the form

Z ψ1(x1)ψ2(x2)ψ3(x1  x2).

Pr(xn) =

1
Z X

x\xn

J

Y
j=1

ψj(xj).

(2)

For tree-structured factor graphs  the sum-product algorithm efﬁciently recovers the exact marginals.
For more general graphs the sum-product algorithm often converges to useful approximations  in
which case it is called loopy belief propagation.

The sum-product algorithm proceeds by passing “messages” along the graph edges. There are two
kinds of messages messages from a factor node to a variable node and messages from a variable
node to a factor node. In order to make clear what quantities are represented by chemical species
concentrations in Section 4  we use somewhat unconventional notation. The kth entry of the sum
message from factor node j to variable node n is denoted S(j→n)
and the entire Kn-dimensional
vector is denoted by S(j→n). The kth entry of the product message from variable n to factor node j
is denoted by P(n→j)
and the entire Kj-dimensional vector is denoted P(n→j). Figure 1a shows a
simple factor graph with message names and their directions shown as gray arrows. Sum messages
from j are computed as the weighted sum of product messages over the domain Kj of ψj

k

k

S(j→n)

k

= X
n=k

kj

n′∈ne(j)\n

ψj(xj = kj) Y

P(n′→j)

kj

n′

 

(3)

n = k to the set of all
where ne(j)\n refers to the variable node neighbors of j except n and kj
kj ∈ Kj where the entry in the dimension of n is ﬁxed to k. Product messages are computed by
taking the component-wise product of incoming sum messages
S(j ′→n)

P(n→j)

(4)

.

k

= Y
j ′∈ne(n)\j

k

Up to normalization  the marginals can be computed from the product of incoming sum messages

Pr(xn = k) = Y

S(j→n)

k

.

j∈ne(n)

(5)

The sum-product algorithm corresponds to ﬁxed-point iterations that are minimizing the Bethe free
energy. This observation leads to both partial-update or damped variants of sum-product  as well
as asynchronous versions [18  Ch.6][19]. The validity of damped asynchronous sum-product is
what enables us to frame the computation as a chemical reaction network. The continuous ODE
description of species concentrations that represent messages can be thought of as an inﬁnitesimally
small version of damped asynchronous update rules.

3 Chemical Reaction Networks

The following model describes how a set of M chemical species Z = {Z1  Z2  · · ·   ZM } interact
and their concentrations evolve over time. Each reaction has the general form

3

r1Z1 + r2Z2 + · · · + rM ZM

κ

GGGGGGGB p1Z1 + p2Z2 + · · · + pM ZM .

(6)

In this generic representation most of the coefﬁcients rm ∈ N and pm ∈ N are typically zero (where
N indicates non-negative integers). The species on the left with non-zero coefﬁcients are called
reactants and are consumed during the reaction. The species on the right with non-zero entries
are called products and are produced during the reaction. Species that participate in a reaction 
i.e.  rm > 0  but where no net consumption or production occurs  i.e. rm = pm  are called catalysts.
They change the dynamics of a particular reaction without being changed themselves.

A reaction network over a given set of
R = {R1  R2  · · ·   RQ}  where each reaction is a triple of reaction parameters (6) 

species consists of a set of Q reactions

(7)
For example  in a reaction Rq ∈ R where species Z1 and Z3 form a new chemical species Z2 at a
rate of κq  the reactant vector rq is zero everywhere except at rq
3 = 1. The associated product
vector pq is zero everywhere except at pq
2 = 1. In the reaction notation where non-participating
species are dropped reaction Rq is can be compactly written as

Rq = (rq   κq  pq).

1 = rq

Z1 + Z3

κq

GGGGGGGB Z2.

(8)

3.1 Mass Action Kinetics

The concentration of each chemical species Zm is denoted by [Zm]. A reaction network describes
the evolution of species concentrations as a set of coupled non-linear differential equations. For each
species concentration [Zm] the rate of change is given by mass action kinetics 

d[Zm]

dt

=

Q

X
q=1

κq

M

Y
m′=1

[Zm′ ]rq

m′ (pq

m − rq

m).

(9)

Based on the fact that reactant coefﬁcients appear as powers  the sum PM
m=1 rm is called the order
of a reaction. For example  if the only reaction in a network were the second order reaction (8)  the
concentration dynamics of [Z1] would be

d[Z1]
dt

= −κq[Z1][Z3].

(10)

Similar to [4] we design reaction networks where the equilibrium concentration of some species
corresponds to the results we are interested in computing. The reaction networks in the following
section conserve mass and do not require ﬂux in or out of the system  and therefore the solutions are
guaranteed to be bounded. While we cannot rule out oscillations in general  the message passing
methods these reactions are simulating correspond to an energy minimization problem. As such  we
suspect that the particular reaction networks presented here always converge to their equilibrium.

4 Representing Graphical Models with Reaction Networks

In the following compilation procedure  each message and marginal probability is represented by
a set of distinct chemical species. We design networks that cause them to interact in such a way
that  at steady state  the concentration of some species represent the marginal distributions of the
variable nodes in a factor graph. When information arrives the network equilibrates to the new 
correct  value. Since messages in the sum-product inference algorithm are computed from other
messages  the reaction networks that implement sending messages describe how species from one
message catalyze the species of another message.

  has an associated chemical species S(j→n)

Beliefs and messages are represented as concentrations of chemical species: each component of a
sum message  S(j→n)
; each component of a product
k
k
message  P(n→j)
  has an associated chemical species P(n→j)
; and each component of a marginal
probability distribution  Pr(xn = k)  has an associated chemical species Pn
k . In addition  each
message and marginal probability distribution has a chemical species with a zero subscript that rep-
resents unassigned probability mass. Together  the set of species associated with a messages or

k

k

4

marginal probability are called a belief species  and the reaction networks presented in the subse-
quent sections are designed to conserve species – and by extension their concentrations – with each
such set. For example  the concentration of belief species P n = {Pn
k=0 of Pr(xn) have a constant
sum  PKn
k ]   determined by the initial concentrations. These sets belief species are a chemi-
cal representation of the left hand sides of Equations 3–5. The next few sections present reaction
networks whose dynamics implement their right hand sides.

k=0[Pn

k }Kn

4.1 Belief Recycling Reactions

Each set of belief species has an associated set of reactions that recycle assigned probabilities to the
unassigned species. By continuously and dynamically reallocating probability mass  the resulting
reaction network can adapt to changing potential functions ψj  i.e. new information.
For example  the factor graph shown in Figure 1a has 8 distinct sets of belief species – 2 representing
marginal probabilities of x1 and x2  and 6 (ignoring messages to leaf factor nodes) representing
messages. The associate recycling reactions are

P1
k
P2
k

κr

GGGGB P1
0
κr
GGGGB P2
0

k

S(1→1)
S(2→2)

k

κr
GGGGB S(1→1)
κr
GGGGB S(2→2)

0

0

k

S(3→1)
S(3→2)

k

κr
GGGGB S(3→1)
κr
GGGGB S(3→2)

0

0

k

P(1→3)
P(2→3)

k

κr
GGGGB P(1→3)
κr
GGGGB P(2→3)

0

0

.
(11)

By choosing a smaller rate κr less of the probability mass will be unassigned at steady state  i.e.
quantities will be closer to normalized  however the speed at which the reaction network reaches
steady state decreases  see Section 5.

4.2 Sum Messages

In the reactions that implement messages from factor to variable nodes  message species of incoming
messages catalyze the assignment of message species belonging to outgoing messages. The entries
in factor tables determine the associated rate constants. The kth message component from a factor
node ψj to the variable node xn is implemented by a reactions of the form

S(j→n)

0

+ X

P(n′→j)

kj

n′

n′∈ne(j)\n

ψj (xj =kj )
GGGGGGGB S(j→n)

k

+ X

P(n′→j)

kj

n′

n′∈ne(j)\n

 

(12)

where the nth component of kj is clamped to k  kj
for each sum message species are given by

n = k. Using the law of mass action  the kinetics

d[S(j→n)

k
dt

]

ψj(xj = kj)[S(j→n)

0

= X
n=k

kj

[P(n′→j)

kj

] Y
n′∈ne(j)\n

n′

] − κr[S(j→n)

k

].

(13)

At steady state the concentration of S(j→n)

k

is given by

κr

[S(j→n)

0

]

[S(j→n)

k

] = X
n=k

kj

ψj(xj = kj ) Y

[P(n′→j)

kj

n′

n′∈ne(j)\n

] 

(14)

k

] species concentrations have the same factor

where all [S(j→n)
. Their relative concentra-
tions are exactly the message according to the to Equation (3). As κr decreases  the concentration of
unassigned probability mass decreases and the concentration normalized by the constant sum of all
the related belief species can be interpreted as a probability. For example  the four factor-to-variable
messages in Figure 1(a) can be implemented with the following reactions

[S(j→n)

κr

0

]

S(1→1)

0

S(2→2)

0

ψ1(k)

GGGGGGGB S(1→1)

ψ2(k)

GGGGGGGB S(2→2)

k

k

ψ3(k k′)
GGGGGGGB S(3→1)
ψ3(k′ k)
GGGGGGGB S(3→2)

k

k

S(3→1)

0

S(3→2)

0

k′

+ P(2→3)
+ P(1→3)

k′

5

k′

+ P(2→3)
+ P(1→3)

k′

.

(15)

ψ1

x1

ψ4

ψ3

x3

ψ2

x2

ψ6

x4

(a)

ψ5

ψ7

ψ1

x1

ψ2

ψ3

x2

ψ4

x3

ψ6

ψ5

(b)

ψ3(1)

2

ψ3(2)

1

ψ3(3)

1

ψ7(1)

1

ψ7(2)

1

ψ1(1)

1

ψ1(2)

0.1

ψ′

1(1)
0.1

ψ′

1(2)
1

ψ4(1  ·)
ψ4(2  ·)

ψ4(·  1)

1
0.1

ψ4(·  2)

0.1
3

ψ5(1  ·)
ψ5(2  ·)

ψ2(1)

1
ψ5(·  1)

ψ2(2)

0.1

0.1
3

ψ5(·  2)

2
0.1

ψ5(·  3)

0.1
1

ψ6(1  ·)
ψ6(2  ·)

ψ6(·  1)

0.1
1

ψ6(·  2)

0.1
0.1

(c)

Figure 2: Examples of non-trivial factor graphs. (a) Four variable factor graph with binary factors.
The factor leafs can be used to specify information about a particular variable. (b) Example of a
small three variable cyclic graph. (c) Factors for (a) used in simulation experiments in Section. 5.1.

4.3 Product Messages

Reaction networks that implement variable to factor node messages have a similar  but slightly
simpler  structure. Again  each components species of the message is catalyzed by all incoming
messages species but only of the same component species. The rate constant for all product message
reactions is the same κprod resulting in reactions of the following form

P(n→j)

0

S(j ′→n)

+ X

k
j ′∈ne(n)\j

κprod

GGGGGGGB P(n→j)

k

S(j ′→n)

+ X

k
j ′∈ne(n)\j

.

(16)

The dynamics of the message component species is given by

d[P(n→j)

k
dt

]

= κprod[P(n→j)

0

] Y
j ′∈ne(n)\j

[S(j ′→n)

k

] − κr[P(n→j)

k

].

At steady state the concentration of P(n→j)

is given by

k

κr

κprod[P(n→j)

0

]

[P(n→j)

k

] = Y

[S(j ′→n)

k

].

j ′∈ne(n)\j

(17)

(18)

[P(n→j)
Since all component species of product messages have the same multiplier
] 
the steady state species concentrations compute the correct message according to Equation 4. For
example  the two different sets of variable to factor messages in Figure 1a are

κprod[P(n→j)

κr

k

0

]

P(1→3)

+ S(2→2)
Similarly  the reactions to compute the marginal probabilities of x1 and x2 in Figure 1a are

+ S(1→1)

+ S(1→1)

+ S(2→2)

P(2→3)

0

0

k

k

k

k

k

k

κprod
GGGGB P(1→3)

κprod
GGGGB P(2→3)

0 + S(3→1)
P1
0 + S(3→2)
P2

k

k

k

+ S(1→1)
+ S(2→2)

k

κprod
GGGGB P1
κprod
GGGGB P2

k + S(3→1)
k + S(3→2)

k

k

k

+ S(1→1)
+ S(2→2)

.

k

(19)

.

(20)

The two rate constants κprod and κr can be adjusted to tradeoff speed vs. accuracy  see Section 5.
Together  reactions for recycling probability mass  implementing sum-product messages  and imple-
menting product messages deﬁne a reaction network whose equilibrium computes the messages and
marginal probabilities via the sum-product algorithm. As probability mass is continuously recycled 
messages computed on partial information will readjust and settle to the correct value. There is a
clear dependence of messages. Sum messages from leaf nodes do not depend on any other mes-
sages. Once they are computed  i.e. the reactions have equilibrated  the message species continue to
catalyze the next set of messages until they have reached the the correct value  etc.

6

Pr(x1)

Pr(x2)

Pr(x3)

1
0
.
0
=

r
κ

1
.
0
=

r
κ

exact
slow
fast

Pr(x1)

0.692
0.690
0.661

0.308
0.306
0.294

Pr(x2)

0.598
0.583
0.449

0.402
0.393
0.302

0.392
0.394
0.379

Pr(x3)
0.526
0.520
0.508

0.083
0.083
0.080

Pr(x4)

0.664
0.665
0.646

0.336
0.333
0.326

Figure 3: Inference results for factor graph in Figure 2(a). Colored boxes show the trajectories of
a belief species set in a simulated reaction network. The simulation time (3000 sec) is along the
x–dimension. Half way though the simulation the factor attached to x1 changes from ψ1 to ψ′
1  and
the exact marginal distribution for each period is shown as a back-white dashed line. The white area
at the top indicates unassigned probability mass. These plots show the clear tradeoff between speed
(higher value of κr) and accuracy (less unassigned probability mass). The exact numerical answers
at 3000 sec are given in the table.

5 Simulation Experiments

This section presents simulation results of factor graphs that have been compiled into reaction net-
works via the procedure in Section 4. All simulations were performed using the SimBiology Toolbox
in Matlab with the “sundials” solver. The conserved concentrations for all sets of belief species were
set to 1  so plots of concentrations can be directly interpreted as probabilities. Figure 2 shows two
graphical models for which we present detailed simulation results in the next two sections.

5.1 Tree-Structured Factor Graphs

To demonstrate the functionality and features of the compilation procedure described in Section 4 
we compiled the 4 variable factor graph shown in Figure 2a into a reaction network. When x1  x2  x3
and x4 have discrete states K1 = K2 = K4 = 2 and K3 = 3  the resulting network has 64 chemical
species and 105 reactions. The largest reaction is of 5th order to compute the marginal distribution
of x2. We instantiated the factors as shown in Figure 2c and initialized all message and marginal
species to be uniform. To show that the network continuously performs inference and can adapt
1 half way through the simulation. In terms of
to new information  we changed the factor ψ1 to ψ′
information  the new factor implies that Pr(x1 = 2) is suddenly more likely. In terms of reactions
the change means that S(1→1)
In a biological reaction
network  such a change could be induced by up-regulating  or activating a catalyst due to a new
chemical signal. This new information changes the probability distribution of all variables in the
graph and the network equilibrates to these new values  see Figure 3.
The only two free parameters are κprod and κr. Since only κr has an direct effect on all sets of belief
species  we ﬁxed κprod = 50 and varied κr. Small values of κr results in better approximation as less
of the probability mass in each belief species set is in an unassigned state. However  small values of
κr slow the dynamics of the network. Larger values of κr result in faster dynamics  but more of the
probability mass remains unassigned  top white area in Figure 3. We should note  that at equilibrium 
the relative assignments of probabilities are still correct  see Equation 14 and Equation 18.

is now more likely to turn into S(1→1)

0

.

2

The compilation procedure also works for factor graphs with larger factors. When replacing the two
of the binary factors ψ5 and ψ6 in Figure 2a with a new tertiary factor ψ′
5 that is connected to x2 x3 
and x4 the compiled reaction network has 58 species and 115 reactions. The largest reaction is of
order 4. Larger factors can reduce the number of species since there are fewer edges and associated
messages to represent  however  the domain sizes Kj of the individual factors grows exponentially
and in the number of neighbors and thus require more reactions to implement.

7

(a)

(b)

Figure 4:
(a) The belief of Pr(x1 = 1) as function of iteration in loopy belief propagation. All
messages are updated simultaneously at every time step. After 100 iterations the oscillations abate
and the belief converges to the correct estimate indicated by the dashed line. (b) Trajectory of PAi
species concentrations. The simulation time is 3000 sec and the different colors indicate the belief
of about either of the two states. The dotted line indicates the exact marginal distribution of x1.

5.2 Loopy Belief Propagation
These networks can also be used on factor graphs that are not trees. Figure 2b shows a cyclic graph
which we compiled to reactions and simulated. When Kn = 2 for all variables the resulting reaction
network has 54 species and 84 reactions. We chose factor tables that anti-correlate neighbors and
leaf factors that prefer the same state.

Figure 4 shows the results of performing both loopy belief propagation and simulation results for
the compiled reaction network. Both exhibit decaying oscillations  but settle to the correct marginal
distribution. Since the reaction network is essentially performing damped loopy belief propagation
with an inﬁnitesimal time step  the reaction network implementation should always converge.

6 Conclusion

We present a compilation procedure for taking arbitrary factor graphs over discrete random vari-
ables and construct a reaction network that performs the sum-product message passing algorithm
for computing marginal distributions.

These reaction networks exploit the fact that the message structure of the sum-product algorithm
maps neatly onto the model of mass action kinetics. By construction  conserved sets of belief species
in the network perform implicit and continuous normalization of all messages and marginal distri-
butions. The correct behavior of the network implementation relies on higher order reactions to
implement multiplicative operations. However  physically high order reaction are exceedingly un-
likely to proceed in a single step. While we can simulate and validate our implementation with
respect to the mass action model  a physical implementation will require an additional translation
step  e.g. along the lines of [17] with intermediate species of binary reactions.

One aspect that this paper did not address  but we believe is important  is how parameter uncer-
tainty and noise affect the reaction network implementations of inference algorithms. Ideally  they
would be robust to parameter uncertainty and random ﬂuctuations. To address the former one could
directly compute the parameter sensitivity in this deterministic model. To address the latter  we
plan to look at other semantic interpretations of chemical reaction networks  such as the linear noise
approximation or the stochastic chemical kinetics model.

In addition to further analyzing this particular algorithm we would like to implement others  e.g.
max-product  parameter learning  and dynamic state estimation  as reaction networks. We believe
that statistical inference provides the right tools for tackling noise and uncertainty at a microscopic
level  and that reaction networks are the right language for specifying systems at that scale.

Acknowledgements

We are grateful to Wyss Institute for Biologically Inspired Engineering at Harvard  especially Prof.
Radhika Nagpal  for supporting this research. We would also like to thank our colleagues and
reviewers for their helpful feedback.

8

References

[1] Subhayu Basu  Yoram Gerchman  Cynthia H. Collins  Frances H. Arnold  and Ron Weiss. A synthetic

multicellular system for programmed pattern formation. Nature  434:1130–1134  2005.

[2] Christopher M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics.

Springer  2006.

[3] Luca Cardelli and Gianluigi Zavattaro. On the computational power of biochemistry. In Katsuhisa Hori-
moto  Georg Regensburger  Markus Rosenkranz  and Hiroshi Yoshida  editors  Algebraic Biology  volume
5147 of Lecture Notes in Computer Science  pages 65–80. Springer Berlin Heidelberg  2008.

[4] Ho-Lin Chen  David Doty  and David Soloveichik. Deterministic function computation with chemical
reaction networks. In Darko Stefanovic and Andrew Turberﬁeld  editors  DNA Computing and Molec-
ular Programming  volume 7433 of Lecture Notes in Computer Science  pages 25–42. Springer Berlin
Heidelberg  2012.

[5] Yuan-Jyue Chen  Neil Dalchau  Niranjan Srinivas  Andrew Phillips  Luca Cardelli  David Soloveichik 
and Georg Seelig. Programmable chemical controllers made from DNA. Nature Nanotechnology 
8(10):755–762  October 2013.

[6] Matthew Cook  David Soloveichik  Erik Winfree  and Jehoshua Bruck. Programmability of chemical
In Algorithmic Bioprocesses  Natural Computing Series  pages 543–584. Springer

reaction networks.
Berlin Heidelberg  2009.

[7] Tal Danino  Octavio Mondragon-Palomino  Lev Tsimring  and Jeff Hasty. A synchronized quorum of

genetic clocks. Nature  463:326–330  2010.

[8] Michael B. Elowitz and Stanislas Leibler. A synthetic oscillatory network of transcriptional regulators.

Nature  403:335–338  2000.

[9] A Hjelmfelt  E D Weinberger  and J Ross. Chemical implementation of neural networks and Turing

machines. Proceedings of the National Academy of Sciences  88(24):10983–10987  1991.

[10] Erik Winfree Jongmin Kim  John J. Hopﬁeld. Neural network computation by in vitro transcriptional

circuits. In Advances in Neural Information Processing Systems 17 (NIPS 2004). MIT Press  2004.

[11] Marcelo O. Magnasco. Chemical kinetics is Turing universal. Phys. Rev. Lett.  78:1190–1193  Feb 1997.
[12] Chengde Mao  Thomas H. LaBean  John H. Reif  and Nadrian C. Seeman. Logical computation using

algorithmic self-assembly of DNA triple-crossover molecules. Nature  407:493–496  2000.

[13] K. Oishi and E. Klavins. Biomolecular implementation of linear I/O systems. Systems Biology  IET 

5(4):252–260  2011.

[14] Lulu Qian  Erik Winfree  and Jehoshua Bruck. Neural network computation with DNA strand displace-

ment cascades. Nature  475:368–372  2011.

[15] Paul W. K Rothemund  Nick Papadakis  and Erik Winfree. Algorithmic self-assembly of DNA Sierpinski

triangles. PLoS Biol  2(12):e424  12 2004.

[16] Georg Seelig  David Soloveichik  David Yu Zhang  and Erik Winfree. Enzyme-free nucleic acid logic

circuits. Science  314(5805):1585–1588  2006.

[17] David Soloveichik  Georg Seelig  and Erik Winfree. DNA as a universal substrate for chemical kinetics.

Proceedings of the National Academy of Sciences  107(12):5393–5398  2010.

[18] Benjamin Vigoda. Analog Logic: Continuous-Time Analog Circuits for Statistical Signal Processing.

PhD thesis  Massachusetts Institute of Technology  2003.

[19] Jonathan S. Yedidia  W.T. Freeman  and Y. Weiss. Constructing free-energy approximations and gen-
Information Theory  IEEE Transactions on  51(7):2282–2312 

eralized belief propagation algorithms.
2005.

[20] David Yu Zhang and Georg Seelig. DNA-based ﬁxed gain ampliﬁers and linear classiﬁer circuits. In
Yasubumi Sakakibara and Yongli Mi  editors  DNA Computing and Molecular Programming  volume
6518 of Lecture Notes in Computer Science  pages 176–186. Springer Berlin Heidelberg  2011.

9

,Nils Napp
Ryan Adams
Yi Sun
Yuheng Chen
Xiaogang Wang
Xiaoou Tang