2019,Multivariate Triangular Quantile Maps for Novelty Detection,Novelty detection  a fundamental task in machine learning  has  drawn a lot of recent attention due to its wide-ranging applications and the rise of neural approaches. In this work  we present a general framework for neural novelty detection that centers around a multivariate extension of the univariate quantile function. Our framework unifies and extends many classical and recent novelty detection algorithms  and opens the way to exploit recent advances in flow-based neural density estimation. We adapt the multiple gradient descent algorithm to obtain the first efficient end-to-end implementation of our framework that is free of tuning hyperparameters. Extensive experiments over a number of real datasets confirm the efficacy of our proposed method against state-of-the-art alternatives.,Multivariate Triangular Quantile Maps

for Novelty Detection

Jingjing Wang1  Sun Sun2  Yaoliang Yu1

University of Waterloo1  National Research Council Canada2

{jingjing.wang  sun.sun  yaoliang.yu}@uwaterloo.ca

Abstract

Novelty detection  a fundamental task in machine learning  has drawn a lot of recent
attention due to its wide-ranging applications and the rise of neural approaches. In
this work  we present a general framework for neural novelty detection that centers
around a multivariate extension of the univariate quantile function. Our framework
uniﬁes and extends many classical and recent novelty detection algorithms  and
opens the way to exploit recent advances in ﬂow-based neural density estimation.
We adapt the multiple gradient descent algorithm to obtain the ﬁrst efﬁcient end-
to-end implementation of our framework that is free of tuning hyperparameters.
Extensive experiments over a number of real datasets conﬁrm the efﬁcacy of our
proposed method against state-of-the-art alternatives.

1

Introduction

Novelty detection refers to the fundamental task in machine learning that detects “novel” or “unusual”
samples in a data stream. It has wide-ranging applications such as network intrusion detection [14] 
medical signal processing [17]  jet design [19]  video surveillance [42  43]  image scene analysis
[25  47]  document classiﬁcation [29  30]  reinforcement learning [39]  etc.; see the review articles
[7  31  32  41] for more insightful applications. Over the last two decades or so  many novelty
detection algorithms have been proposed and studied in the machine learning ﬁeld  of which the
statistical approach that aims to identify low-density regions of the underlying data distribution has
been most popular [e.g. 4  49  51  53]. More recently  new novelty detection algorithms based on
deep neural networks [e.g. 1  9  11  18  26  40  44  46  48  56  58  59] have drawn a lot of attention as
they signiﬁcantly improve their non-neural counterparts  especially in domains (such as image and
video) where complex high-dimensional structures abound.
This work offers a closer look of these recent neural novelty detection algorithms  by making a connec-
tion to recent ﬂow-based generative modelling techniques [22]. In §2 we show that the triangular map
studied in [22] for neural density estimation serves as a natural extension of the classical univariate
quantile function to the multivariate setting. Since density estimation is extremely challenging in
high dimensions  recent neural novelty detection algorithms all extract a lower dimensional latent
representation  whose probabilistic properties can then by captured by our multivariate triangular
quantile map. Based on this observation we propose a general framework for neural novelty detection
that includes as special cases many classical approaches such as one-class SVM [49] and support
vector data description [53]  as well as many recent neural approaches [e.g. 1  40  46  58  59]. This
uniﬁed view of neural novelty detection enables us to better understand the similarities and subtle dif-
ferences of the many existing approaches  and provides some guidance on designing next-generation
novelty detection algorithms.
More importantly  our general framework makes it possible to effortlessly plug-in recent ﬂow-based
neural density estimators  which have been shown to be surprisingly effective even in moderately high
dimensions. Furthermore  centering our framework around the (multivariate) triangular quantile map
(TQM) also enables us to unify the two scoring strategies in the literature [34]: we can either threshold
33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the density function [4  51] or the (univariate) quantile function [49  53]. Using the multivariate
triangular quantile map  for the ﬁrst time we can simultaneously perform both  without incurring any
additional cost. In §3  motivated by the sub-optimality of pre-training we cast our novelty detection
framework as multi-objective optimization [35] and apply the multiple gradient descent algorithm
[12  15  36] for the ﬁrst time. We present an efﬁcient implementation that learns the TQM consistently 
end-to-end and free of tuning hyperparameters. In §4 we perform extensive experiments on a variety
of datasets and verify the effectiveness of our framework against state-of-the-art alternatives.
We summarize our main contributions as follows:
• We extend the univariate quantile function to the multivariate setting through increasing triangular
maps. This multivariate triangular quantile map may be of independent interest for many other
problems involving multivariate probabilistic modelling.
• We present a new framework for neural novelty detection  which uniﬁes and extends many existing
• For the ﬁrst time we apply the multiple gradient descent algorithm to novelty detection and obtain
an efﬁcient end-to-end implementation of our framework that is free of any tuning hyperparameters.
• We perform extensive experiments to compare to existing novelty detection baselines and to

approaches including the celebrated one-class SVM and many recent neural ones.

conﬁrm the efﬁcacy of our proposed framework.

Our code is available at https://github.com/GinGinWang/MTQ.

2 A General Framework for Novelty Detection

In this section we present a general framework for novelty detection. Our framework builds on recent
progresses in generative modelling and uniﬁes and extends many existing works.

We follow the standard setup for novelty detection [e.g. 7]: Given n i.i.d. samples(cid:42)X1  . . .   Xn(cid:43)

from an unknown distribution P over Rd  we want to decide if a new (unseen) sample ˜X is “novel ”
i.e. if it is unlikely to come from the same distribution P . Due to lack of supervision  the notion of
“novelty” is not well-deﬁned. Practically  a popular surrogate is to identify the low-density regions
of the distribution P [4  49  51]  as samples from these areas are probabilistically unlikely. For
simplicity we assume the underlying distribution P has a density p w.r.t. the Lebesgue measure.
We exploit the following multivariate generalization of the quantile function. Recall that the cumula-
tive distribution function (CDF) F and the quantile function Q of a univariate random variable X is
deﬁned as:

F (x) = Pr(X ≤ x) 

Q(u) = F −1(u) := inf{x : F (x) ≥ u}.

While the CDF can be easily generalized to the multivariate setting  it is not so obvious for the
quantile function  as its deﬁnition intrinsically relies on the total ordering on the real line. However 
following [e.g. 13  16] we observe that if U follows the uniform distribution over the interval [0  1] 
then Q(U ) follows the distribution F . In other words  the quantile function can be deﬁned as a
mapping that pushes the uniform distribution over [0  1] into the distribution F of interest. This
alternative interpretation allows us to extend the quantile function to the multivariate setting. We
recall that a mapping T = (T1  . . .   Td) : Rd → Rd is called triangular if for all j = 1  . . .   d  the
j-th component Tj depends only on the ﬁrst j coordinates of the input  and it is called increasing if
for all j  Tj is increasing w.r.t. the j-th coordinate when all other coordinates are ﬁxed. We call T
triangular since its derivative is always a triangular matrix (and vice versa).
Deﬁnition 1 (Triangular Quantile Map (TQM)) Let X be a random vector in Rd  and let U be
uniform over the unit hypercube [0  1]d. We call an increasing triangular map Q = QX : [0  1]d →
Rd the triangular quantile map of X if Q(U) ∼ X  where ∼ means equality in distribution.
Note that the TQM Q is vector-valued  unlike the CDF which is always real-valued. The existence
and uniqueness of Q follows from results in [5]. Our deﬁnition immediately leads to the following
quantile change-of-variable formula (cf. the usual change-of-variable formula for densities):
Proposition 1 Let T : Rd → Rd be an increasing triangular map. If Y = T(X)  then

QY = T ◦ QX.

2

(1)

Practically  eq. (1) allows us to easily stack elementary parameterizations of increasing triangular
maps together and still obtain a valid TQM.
To our best knowledge  a similar deﬁnition  through conditional univariate quantiles  appeared in
a number of works [2  10  37  45]  albeit mostly as a theoretical tool. Our deﬁnition makes the
important triangular structure explicit and amenable to parameterization through deep networks.
Needless to say  when d = 1  the triangular property is vacuous and our deﬁnition reduces to the
classical quantile function. For a more comprehensive introduction to triangular maps and its recent
rise in machine learning  see [22  33  50].

Remark 1 A different deﬁnition of the multivariate quantile map  based on the theory of optimal
transport [54]  is discussed in a number of recent works [e.g. 8  13  16]: Q is instead constrained
to be maximally cyclically monotone  i.e. it is the subdifferential of some convex function. On one
hand  this deﬁnition is invariant to permutations of the input coordinates while ours is not. On the
other hand  our deﬁnition is composition friendly (see Proposition 1) hence can easily exploit recent
progresses in deep generative models  as we will see shortly. The two deﬁnitions coincide with each
other only when reduced to the univariate case.
We note that the recent work of Inouye and Ravikumar [21] proposed yet another similar deﬁnition
where Q (termed density destructor there) is only required to be invertible. However  this deﬁnition
does not lead to a unique quantile map and it is less computationally convenient.
We are now ready to present our general framework for novelty detection. Let f : Rd → Rm be a
feature map and X a random sample from the unknown density p. We propose to learn the density1
f#p of the latent random vector Z = f (X) using the approach illustrated in [22]. In details  we learn
the feature map f and the TQM Q simultaneously by minimizing the following objective:

γKL(f#p(cid:107)Q#q) + λ(cid:96)(f ) + ζg(Q) 

min
f  Q

(2)

where g embodies some potential constraints on the increasing triangular map Q  (cid:96) is some loss asso-
ciated with learning the feature map f  q is a ﬁxed reference density (in our case the uniform density
over the hypercube [0  1]m)  ζ  λ  γ ≥ 0 are regularization constants  and we use the KL-divergence to
measure the discrepancy between two densities. Exploiting Proposition 1 we parameterize the TQM
as the composition Q = T ◦ Φ−1  where Φ = (Φ  . . .   Φ) with Φ the CDF of standard univaraite
Gaussian and T : Rd → Rd an increasing triangular map. Note that unlike Q whose support is
constrained to the unit hypercube  there is no constraint on the support of T  hence it is easier to
handle the latter computationally.
Once the feature map f and TQM Q are estimated (see next section)  we can detect novel test samples
by either thresholding the density function of the latent variable Z or thresholding its TQM. In details 
the density of Z = f (X) = Q(U) = T(Φ−1(U))  using the change-of-variable formula  is
ϕ([T−1(z)]j)  where ϕ = Φ(cid:48).

|T(cid:48)(T−1(z))| · m(cid:89)

pZ(z) = 1/|Q(cid:48)(Q−1(z))| =

1

j=1

Thus  we declare a test sample ˜X to be “novel” if

log |T(cid:48)(T−1(f ( ˜X)))| + 1

2(cid:107)T−1(f ( ˜X))(cid:107)2

2 ≥ τ 

(3)
where τ is some chosen threshold. Crucially  since T is increasing triangular  T−1 and the triangular
determinant |T(cid:48)| can both be computed very efﬁciently [22]. The (slight) downside of this density
approach is that the scale of an appropriate threshold τ is usually difﬁcult to guess.
Alternatively  we can declare a test sample ˜X to be “novel” by directly thresholding the TQM Q.
Indeed  let N ⊆ [0  1]m be a subset whose (uniform) measure is 1 − α for some α ∈ (0  1)  then we
say ˜X is “novel” iff

(4)
For instance  we can choose N to be the cube centered at (1/2  . . .   1/2) and with side length
(1 − α)1/m  in which case

Q−1(f ( ˜X)) (cid:54)∈ N.

Q−1(f ( ˜X)) (cid:54)∈ N ⇐⇒ (cid:107)Q−1(f ( ˜X)) − 1

2(cid:107)∞ ≥ (1 − α)1/m/2.

1The notation T#p stands for the push-forward density  i.e.  the density of T(X) when X ∼ p.

3

The upside of this quantile approach is that we can control Type-I error (i.e. false positive) precisely 
i.e. if ˜X is indeed sampled from p  then we will declare it to be novel with probability at most α.
Before proceeding to the implementation details of (2)  let us mention the advantages of our general
framework (2) for novelty detection: (a) It allows us to perform feature extraction on the original
sample X in an end-to-end fashion. As is well-known  density estimation hence also novelty detection
becomes extremely challenging when the dimension d is high. Our framework alleviates this curse-
of-dimensionality by setting m (cid:28) d and employing f to perform dimensionality reduction. (b) Our
end-to-end framework enables us to adopt the recent ﬂow-based density estimation algorithms  which
have been shown to be universally consistent [20  22] and extremely effective in practice. (c) By
estimating the TQM Q once  we can employ the two scoring rules  i.e. the density scoring rule (3)
and the quantile scoring rule (4)  simultaneously  without incurring any extra overhead. This allows us
to perform a fair and comprehensive experimental comparison of the two complementary approaches.
(d) Last but not least  our framework recovers  uniﬁes  and extends many existing approaches in the
literature. Let us conclude this section with some examples.

Example 1 (One-class SVM [49]) As shown in [52]  the one-class SVM minimizes precisely the
conditional value-at-risk  which is the average of the tail of a distribution:

CVaRα(f (X)) + λ(cid:107)f(cid:107)2Hκ

  where CVaRα(Z) := E(Z|Z ≥ QZ(α)) 

min

f

QZ(α) is the α-th quantile of the real random variable Z  and Hκ is the reproducing kernel Hilbert
space (RKHS) induced by some kernel κ. This approach employs the quantile scoring rule (4).
To cast one-class SVM into our framework (2)  let us set m = 1 hence the TQM reduces to the
and g(Q) = CVaRα(Q#q). Now with ζ = 1 and γ = ∞ in (2) we
classical one. Let (cid:96)(f ) = (cid:107)f(cid:107)2Hκ
recover the celebrated one-class SVM.
If instead of choosing f from an RKHS  we represent f using a deep network  then we recover the
recent approach in [6].

Example 2 (Support Vector Data Description (SVDD) [53]) Similar to one-class SVM  it is easy
to show that SVDD also minimizes the conditional value-at-risk:
CVaRα((cid:107)ϕ(X) − c(cid:107)2Hκ

) 

min
c∈Hκ

where ϕ : Rd → Hκ is the canonical feature map of the RKHS. This approach also employs the
quantile scoring rule (4). It is well-known known that SVDD and one-class SVM are equivalent for
radial kernels [e.g. 49].
Again in this case m = 1. Let f (X) = (cid:107)ϕ(X) − c(cid:107)2Hκ
approaches ∞ in (2)  we recover the SVDD formulation.
If instead of choosing ϕ as the canonical feature map of an RKHS  we represent ϕ using a deep
network  then we recover the recent approach in [44].

  (cid:96) ≡ 0 and g(Q) = CVaRα(Q#q). As γ

Example 3 (Latent Space Autoregression (LSA) [1]) The recent work [1]  following a sequence
of previous attempts [40  46  58  59]  proposed to learn the feature map f using an auto-encoder
structure  and to learn the density of the latent variable Z = f (X) using an autoregressive model 
which  as argued in [22]  exactly corresponds to a triangular map. In other words  if we set f as the
parameters of an auto-encoder  (cid:96) to be its reconstruction loss  and g ≡ 0  then our framework (2)
reduces to LSA. However  our general framework opens the way to exploit more advanced ﬂow-based
density estimation algorithms  as well as the quantile scoring rule (4).

3 Estimating TQM Using Deep Networks

In this section we show how to estimate the TQM Q in (2) based on samples(cid:42)X1  . . .   Xn(cid:43) i.i.d.∼ p.

In particular  any ﬂow-based neural density estimator can be plugged into our framework.
Our framework (2) has three components which we implement as follows:
• A feature extractor f for performing dimensionality reduction. Following previous works [1  40 
46  58  59] we implement f through a deep autoencoder that consists of one encoder Z = E(X; θE)

4

and one decoder ˆX = D(Z; θD) . We use the Euclidean reconstruction loss:

(cid:96)(f ) = (cid:96)(θE  θD) =(cid:80)n

i=1 (cid:107)Xi − ˆXi(cid:107)2.

As argued in [3]  the reconstruction error  aside from low likelihood  is an important indicator for
“novelty.” Indeed  since the autoencoder is trained on nominal data  a test sample will incur a large
reconstruction error only when it is novel  as such samples have never been encountered before.
• A ﬂow-based neural density estimator for Q. Here we adopt the sum-of-squares (SOS) ﬂow
proposed in [22]  although other neural density estimators would apply equally well. The SOS ﬂow
consists of two parts: an increasing (univariate) polynomial P2r+1(u; a) with degree 2r + 1 for
modelling conditional densities and a conditioner network Cj(u1  . . .   uj−1; θQ) for generating
the coefﬁcients a of the polynomial:

P2r+1(u; a) = c +(cid:82) u

(cid:80)k

0

s=1

l=0 al stl(cid:1)2
(cid:0)(cid:80)r
(cid:0)uj; Cj(u1  . . .   uj−1; θQ)(cid:1).

dt 

(1 − λ)∇h(Xi; θt) + λ∇(cid:96)(Xi; θt)

5

where c ∈ R is an arbitrary constant  r ∈ N is the degree of polynomial  and k can be chosen as
small as 2. In other words  the TQM Q learned using SOS ﬂow has the following form:

Q = T ◦ Φ−1  where ∀j  Tj(u1  . . .   uj) = P2r+1

(5)
Any regularization term on the conditioner network weights θQ can be put into the function g(Q)
in our framework (2).
• Lastly  the KL-divergence term in (2) can be approximated empirically using the given sample

(cid:42)X1  . . .   Xn(cid:43). Upon dropping irrelevant constants we reduce the KL term in (2) to:

log |Q(cid:48)(Q−1(f (Xi)))| − log q(Q−1(f (Xi)))

 

(cid:105)

n(cid:88)

(cid:104)

i=1

min
θQ

where each component of Q is given in (5). Crucially  since Q is increasing triangular  evaluating
the inverse Q−1 and the Jacobian |Q(cid:48)| can both be done in linear time [22].

Since q is the uniform density over the hypercube  upon simpliﬁcation the ﬁnal training objective we
use in our experiments is as follows. Let Zi = E(Xi; θE)  we aim to solve:

(1 − λ)

log |T(cid:48)(T−1(Zi))| + (cid:107)T−1(Zi)(cid:107)2

+ λ(cid:107)Xi − D(Zi; θD)(cid:107)2

 

(6)

(cid:123)(cid:122)

(cid:105)

(cid:125)

2/2

(cid:124)

(cid:123)(cid:122)

(cid:125)

n(cid:88)

i=1

min

θ

(cid:104)

(cid:124)

negative log-likelihood h(Xi;θ)

reconstruction loss (cid:96)(Xi;θ)

and recall that Q = T ◦ Φ−1 is parameterized through the conditioner network weights θQ in (5).
We did not ﬁnd it necessary to further regularize Q hence set g ≡ 0 in (2) and w.l.o.g. γ = 1 − λ.
The ﬁrst KL term in (2)  as is well-known  reduces to the negative log-likelihood of the latent
random vectors Zi in (6)  and the second term is the standard reconstruction loss. The two terms
share the encoder weights θE and the trade-off is balanced through the hyperparameter λ. This
design choice conforms to the psychology ﬁndings in [3]. In practice  we found that the variance
of the log-likelihood is much larger than that of the reconstruction loss  and as a consequence we
observed substantial difﬁculty in directly minimizing the weighted objective in (6). A popular pre-
training heuristic is to train the whole model in two stages: we ﬁrst minimize the reconstruction
loss (cid:96)(θE  θD) and then  with the learned hidden vector Z  we estimate the TQM Q by maximum
likelihood. However  as shown in [59]  the latent representation learned in the ﬁrst stage does not
necessarily help the task in the second stage.
Instead  we cast the two competing objectives in (6) as multi-objective optimization  which we solve
using the multiple gradient descent algorithm (MGDA) [12  15  36]. Our motivation comes from the
following observation: the two-stage procedure amounts to ﬁrst setting λ = 1 and running gradient
descent (GD) for a number of iterations  then switching to λ = 0 (or λ = 0.5 say) and running GD
for the remaining iterations. Naturally  instead of any pre-determined schedule for the hyperparameter
λ (such as switching from 1 to 0 or 0.5)  why not let GD decide what λ to use in each iteration? This
is precisely the main idea behind MGDA  where at iteration t we solve

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:110)

(cid:110)

= min

1  max

0 

(cid:104)∇hI−∇(cid:96)I  ∇hI(cid:105)
(cid:107)∇hI−∇(cid:96)I(cid:107)2

(cid:111)(cid:111)

 

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)

i∈I

λt = argmin
0≤λ≤1

where I ⊆ {1  . . .   n} is a minibatch of samples  and obviously ∇hI = (cid:80)

similarly for ∇(cid:96)I. With λt calculated we can continue the gradient update:

i∈I ∇h(Xi; θt) and

θt+1 = θt − η[(1 − λt)∇hI + λt∇(cid:96)I ] 

where η ≥ 0 is the step size. As shown in [12]  this algorithm converges to a Pareto-optimal solution
under fairly general conditions. Pleasantly  MGDA eliminates the need of tuning the hyperparameter
λ as it is determined automatically on the ﬂy. To our best knowledge  our work is the ﬁrst to
demonstrate the effectiveness of MGDA on novelty detection tasks.
We end our discussion by pointing out that the algorithm we develop here can easily be adapted to
other design choices that ﬁt into our general framework (2). For instance  if we use a variational
autoencoder [23] or a denoising autoencoder [55]  then we need only replace the square reconstruction
loss in (6) accordingly.

4 Empirical Results

In this section  we evaluate the performance of our proposed method for novelty detection and
compare it with the traditional and state-of-the-art alternatives. For evaluation  we use precision 
recall  F1 score  and the Area Under Receiver Operating Characteristic (AUROC) curve as our
performance metrics  which are commonly used in previous works.

4.1 Datasets

In our experiments  we use two public image datasets: MNIST and Fashion-MNIST  as well as two
non-image datasets: KDDCUP and Thyroid. A detailed description of these datasets  the applied
network architectures  and the training hyperparameters can be found in Appendix A. For MNIST
and Fashion-MNIST  each of the ten classes is deemed as the nominal class while the rest of the nine
classes are deemed as the novel class. We use the standard training and test splits. For every class 
we hold out 10% of the training set as the validation set  which is used to tune hyperparameters and
to monitor the training process.

4.2 Competitor Algorithms

We compare our method with the following alternative algorithms:
• OC-SVM [49]. OC-SVM is a traditional kernel-based quantile approach which has been widely
used in practice for novelty detection. We use the RBF kernel in our experiments. We consider two
OC-SVM-based methods for comparison. 1) RAW-OC-SVM: the input is directly fed to OC-SVM;
2) CAE-OC-SVM: a convolutional autoencoder is ﬁrst applied to the input data for dimensionality
reduction  and then the low-dimensional latent representation is fed to OC-SVM.
• Geometric transformation (GT) [18]. A self-labeled multi-class dataset is ﬁrst created by ap-
plying a set of geometric transformations to the original nominal examples. Then  a multi-class
classiﬁer is trained to discriminate the geometric transformations of each nominal example. The
scoring function in GT is the conditional probability of the softmax responses of the classiﬁer given
the geometric transformations.
• Variational autoencoder (VAE) [23]. The evidence lower bound is used as the scoring function.
• Denoising autoencoder (DAE) [55]. The reconstruction error is used as the scoring function.
• Deep structured energy-based models (DSEBM) [58]. DSEBM employs a deterministic deep
neural network to output the energy function (i.e.  negative log-likelihood)  which is used to form
the density of nominal data. The network is trained by score matching in a way similar to training
DAE. Two scoring functions based on reconstruction error and energy score are considered.
• Deep autoencoding Gaussian mixture model (DAGMM) [59]. DAGMM consists of a compression
network implemented using a deep autoencoder and a Gaussian mixture estimation network that
outputs the joint density of the latent representations and some reconstruction features from the
autoencoder. The energy function is used as the scoring function.
• Generative probabilistic novelty detection (GPND) [40]. GPND  based on adversarial autoen-
coders  employs an extra adversarial loss to impose priors on the output distribution. The density is

6

Table 1: AUROC of Variants of Our Method on MNIST

Scoring function
NLL
TQM1
TQM2
TQM∞

λ = 0.99
0.9729
0.9622
0.9666
0.9499

0.9

0.9692
0.9616
0.9645
0.9527

0.5

0.9537
0.9430
0.9465
0.9371

0.1

Optimized

0.9389
0.9319
0.9347
0.9128

0.9728
0.9666
0.9699
0.9531

Table 2: Average Precision  Recall  and F1 Score on Non-image Datasets

Method
RAW-OC-SVM *
DSEBM *
DAGMM *
Ours-REC
Ours-NLL
Ours-TQM1
Ours-TQM2
Ours-TQM∞

Thyroid
Precision Recall
0.4239
0.3639
0.0404
0.0403
0.4834
0.4766

–

0.7312
0.5269
0.5806
0.7527

–

0.7312
0.5269
0.5806
0.7527

KDDCUP
Precision Recall
0.8523
0.7457
0.7369
0.7477
0.9442
0.9297
0.6287
0.6305
0.9622
0.9622
0.9621
0.9621
0.9622
0.9622
0.9622
0.9622

F1

0.7954
0.7423
0.9369
0.6296
0.9622
0.9621
0.9622
0.9622

F1

0.3887
0.0403
0.4782

–

0.7312
0.5269
0.5806
0.7527

used as the scoring function. By linearizing the manifold that nominal data resides on  its density
is factorized into two product terms  which are then approximately computed using nominal data.
• Latent space autoregression (LSA) [1]. A parametric autoregressive model is used to estimate
the density of the latent representation generated by a deep autoencoder  where the conditional
probability densities are modeled as multinomials over quantized latent representations. The sum
of the normalized reconstruction error and log-likelihood is used as the scoring function.

4.3 Variants of Our Method

2/2;

In this subsection  we ﬁrst compare some variants of our proposed method. With regard to the network
conﬁguration  except on Thyroid whose dimension is too small to require any form of dimentionality
reduction  all other experiments contain both an autoencoder and an estimation network.
We consider the following ﬁve scoring functions that we threshold at some level τ. In particular 
given a test example ˜X  we denote its reconstruction by ˆX and its latent representation by ˜Z = f ( ˜X).
(cid:107) ˜X − ˆX(cid:107)2;
• Reconstruction error (REC):
log |T(cid:48)(T−1( ˜Z))| + (cid:107)T−1( ˜Z)(cid:107)2
• Negative log-likelihood (NLL):
(cid:107)Φ(T−1( ˜Z)) − 1
• 1-norm of quantile (TQM1):
• 2-norm of quantile (TQM2):
(cid:107)Φ(T−1( ˜Z)) − 1
• Inﬁnity norm of quantile (TQM∞): (cid:107)Φ(T−1( ˜Z)) − 1
In Table 1  we compare two approaches on MNIST for selecting the hyperparameter λ in the training
phase: 1) chosen from a pre-set family using the validation set; and 2) automatically optimized using
MGDA [12  15  36]. We report the average AUROC over 10 classes. It is clear that for all scoring
functions  the optimized λ generally leads to the highest AUROC. This is also observed on other
datasets such as Fashion-MNIST. Within the proposed variants  NLL results in the highest AUROC
among all scoring functions  followed by TQM2. In Table 2  on the two non-image datasets we
evaluate the average precision  recall  and F1 score. The superscript ∗ on the baselines indicates that
the results are directly quoted from the respective references. The threshold is chosen by assuming
the prior knowledge of the ratio between the novel and nominal examples in the test set. Under this
assumption  the number of false positives is equal to that of false negatives  thus the value of the
three metrics coincides. On Thyroid  TQM∞ is slightly better than the density-based method. On
KDDCUP  the density and quantile-based approaches have the same performance  while REC results
in the worst performance. On both datasets  our proposed methods are superior to the benchmarks.

2(cid:107)1 
2(cid:107)2;
2(cid:107)∞.

7

Table 3: AUROC on MNIST and Fashion-MNIST

MNIST

Class

OC-SVM

RAW

0.995
0.999
0.926
0.936
0.967
0.955
0.987
0.966
0.903
0.962
0.960

CAE

0.990
0.999
0.919
0.939
0.946
0.936
0.979
0.951
0.896
0.960
0.952

0
1
2
3
4
5
6
7
8
9
avg

Class

OC-SVM

RAW

0.919
0.990
0.894
0.942
0.907
0.918
0.834
0.988
0.903
0.982
0.928

CAE

0.908
0.987
0.884
0.911
0.913
0.865
0.820
0.984
0.877
0.955
0.910

0
1
2
3
4
5
6
7
8
9
avg

VAE

DAE

LSA

GT

DAGMM

GPND

DSEBM

Ours-NLL

Ours-TQM2

0.985
0.997
0.943
0.916
0.945
0.929
0.977
0.975
0.864
0.967
0.950

0.982
0.998
0.936
0.929
0.940
0.928
0.982
0.971
0.857
0.974
0.950

0.998
0.999
0.923
0.974
0.955
0.966
0.992
0.969
0.935
0.969
0.968

0.982
0.893
0.993
0.987
0.993
0.994
0.999
0.966
0.974
0.993
0.977

0.500
0.766
0.326
0.319
0.368
0.490
0.515
0.500
0.467
0.813
0.508

Fashion-MNIST

0.999
0.999
0.980
0.968
0.980
0.987
0.998
0.988
0.929
0.993
0.982

0.320
0.987
0.482
0.753
0.696
0.727
0.954
0.911
0.536
0.905
0.727

0.995
0.998
0.953
0.963
0.966
0.962
0.992
0.969
0.955
0.977
0.973

0.993
0.997
0.948
0.957
0.963
0.960
0.990
0.966
0.951
0.976
0.970

VAE

DAE

LSA

GT

DAGMM

GPND

DSEBM

Ours-NLL

Ours-TQM2

0.874
0.977
0.816
0.912
0.872
0.916
0.738
0.976
0.795
0.965
0.884

0.867
0.978
0.808
0.914
0.865
0.921
0.738
0.977
0.782
0.963
0.881

0.916
0.983
0.878
0.923
0.897
0.907
0.841
0.977
0.910
0.984
0.922

0.903
0.993
0.927
0.906
0.907
0.954
0.832
0.981
0.976
0.994
0.937

0.303
0.311
0.475
0.481
0.499
0.413
0.420
0.374
0.518
0.378
0.472

0.917
0.983
0.878
0.945
0.906
0.924
0.785
0.984
0.916
0.876
0.911

0.891
0.560
0.861
0.903
0.884
0.859
0.782
0.981
0.865
0.967
0.855

0.922
0.958
0.899
0.930
0.922
0.894
0.844
0.980
0.945
0.983
0.928

0.917
0.950
0.899
0.925
0.921
0.884
0.838
0.972
0.943
0.983
0.923

4.4 Comparison with Baseline Methods

In this section  we compare our method with the baseline approaches. Note that except RAW-OC-SVM
and GT  all other methods  including our own  are based on autoencoders.
In Table 3  we show the comparison of AUROC on the image datasets. Among the proposed quantile
scoring functions we only list TQM2  which outputs the highest value of AUROC. We observe that
on both datasets our proposed methods are superior to most of the benchmarks  with the density
scoring function being slightly better than the quantile one. On MNIST  GPND and GT have better
performance; and on Fashion-MNIST  GT outputs the highest value of AUROC followed by Ours-
NLL and RAW-OC-SVM. However  since GT explicitly extracts features by using a set of geometric
transformations  it inevitably suffers a high computational and space complexity. In Appendix B  we
further compare and discuss the proposed density and quantile-based approaches in detail.

4.5 Comparison with Two-Stage Training

In our proposed algorithm the autoencoder and the estimation network are trained jointly by employing
MGDA. For comparison  we also consider the following two-stage training strategies:
• We ﬁrst train the autoencoder  then ﬁx the autoencoder and train the estimation network alone
• we ﬁrst pretrain the autoencoder  then jointly train the autoencoder and the estimation network

(denoted as Fix-).

with the weight λ ﬁxed to 0.5 (denoted as Pretrain-).

The comparison regarding AUROC on MNIST is shown in Table 4. We found that the proposed
joint training method leads to the best performance for both the density-based and the quantile-based
scoring functions. This is consistent with the ﬁndings in many existing works [e.g. 1  6  44  58]. For
the ﬁxed two-stage method  our understanding is that the latent representation learned in the ﬁrst
stage may not be the most beneﬁcial for the training of the estimation network in the second stage 
which in turn degrades the overall performance. For the pretrained two-stage method  although in
the second stage the two parts are trained jointly the autoencoder is initialized with the parameters

8

Table 4: Comparison between joint and two-stage training: AUROC on MNIST

Class

0
1
2
3
4
5
6
7
8
9
avg

Fix-NLL
0.9939
0.9971
0.9403
0.9568
0.9703
0.9612
0.9878
0.9629
0.9549
0.9736
0.9699

Pretrain-NLL Ours-NLL

0.9954
0.9988
0.9677
0.9496
0.9445
0.9564
0.9907
0.9676
0.9587
0.9733
0.9703

0.9951
0.9977
0.9526
0.9627
0.9657
0.9618
0.9915
0.9686
0.9551
0.9768
0.9728

Fix-TQM2
0.9904
0.9972
0.9188
0.9481
0.9700
0.9525
0.9841
0.9587
0.9397
0.9742
0.9634

Pretrain-TQM2 Ours-TQM2

0.9939
0.9985
0.9568
0.9414
0.9388
0.9486
0.9881
0.9656
0.9527
0.9641
0.9649

0.9925
0.9969
0.9479
0.9567
0.9625
0.9601
0.9895
0.9660
0.9512
0.9756
0.9699

Figure 1: Distributional comparison on training and test scoring statistics on MNIST (nominal: digit
1). From left to right: 1) NLL; 2) TQM1; 3) TQM2; and 4) TQM∞.

learned in the ﬁrst stage  which might prevent it from being updated to a more suitable local optimum.
The comparison on Fashion-MNIST dataset is similar and is shown in Appendix C.

4.6 Visualization

In Figure 1  we show the violin plots of the scoring statistics NLL  TQM1  TQM2  and TQM∞ on
MNIST test set (with digit 1 serving the nominal class). We use the network parameters produced at
every 20 epochs in training to generate each curve. We can see that  in the beginning the nominal and
novel data have a large region of overlap and after more training epochs they are gradually separated.
After about 20 epochs of training they can be clearly distinguished under NLL  TQM1  and TQM2 
which indicates the effectiveness of these scoring functions. For TQM∞  the distribution of novel data
is concentrated within a narrow region  which is near the boundary of that of nominal data. More
results on visualization can be found in Appendix D.

5 Conclusion

The univariate quantile function was extended to the multivariate setting through increasing triangular
maps  which in turn motivates us to develop a general framework for neural novelty detection. Our
framework uniﬁes and extends many existing algorithms in novelty detection. We adapted the
multiple gradient algorithm to obtain an efﬁcient  end-to-end implementation of our framework that
is free of any tuning hyperparameters. We performed extensive experiments on a number of datasets
to conﬁrm the competitiveness of our method against state-of-the-art alternatives. In the future we
will study the consistency of our estimation algorithm for the multivariate triangular quantile map
and we plan to apply it to other multivariate probabilistic modelling tasks.

Acknowledgement

We thank the reviewers for their constructive comments. We thank Priyank Jaini for bringing
Decurninge’s work to our attention. This work is supported by NSERC.

9

References
[1] Davide Abati  Angelo Porrello  Simone Calderara  and Rita Cucchiara. Latent Space Autore-
gression for Novelty Detection. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition  2019.

[2] Elja Arjas and Tapani Lehtonen. Approximating Many Server Queues by Means of Single

Server Queues. Mathematics of Operations Research  3(3):205–223  1978.

[3] Andrew Barto  Marco Mirolli  and Gianluca Baldassarre. Novelty or Surprise? Frontiers in

Psychology  4:907  2013.

[4] Shai Ben-David and Michael Lindenbaum. Learning Distributions by Their Density Levels:
A Paradigm for Learning without a Teacher. Journal of Computer and System Sciences 
55(1):171–182  1997.

[5] Vladimir Igorevich Bogachev  Aleksandr Viktorovich Kolesnikov  and Kirill Vladimirovich
Medvedev. Triangular transformations of measures. Sbornik: Mathematics  196(3):309–335 
2005.

[6] Raghavendra Chalapathy  Aditya Krishna Menon  and Sanjay Chawla. Anomaly Detection

using One-Class Neural Networks  2018. arXiv:1802.06360.

[7] Varun Chandola  Arindam Banerjee  and Vipin Kumar. Anomaly Detection: A Survey. ACM

Computing Surveys  41(3):15:1–15:58  2009.

[8] Victor Chernozhukov  Alfred Galichon  Marc Hallin  and Marc Henry. Monge–Kantorovich

depth  quantiles  ranks and signs. The Annals of Statistics  45(1):223–256  2017.

[9] Sanjoy Dasgupta  Timothy C. Sheehan  Charles F. Stevens  and Saket Navlakha. A neural data

structure for novelty detection. Proceedings of the National Academy of Sciences  2018.

[10] Alexis Decurninge. Univariate and multivariate quantiles  probabilistic and statistical ap-

proaches; radar applications. PhD thesis  2015.

[11] Lucas Deecke  Robert Vandermeulen  Lukas Ruff  Stephan Mandt  and Marius Kloft. Im-
age Anomaly Detection with Generative Adversarial Networks. In Machine Learning and
Knowledge Discovery in Databases  pages 3–17  2019.

[12] Jean-Antoine Désidéri. Multiple-gradient descent algorithm (MGDA) for multiobjective opti-

mization. Comptes Rendus Mathematique  350(5):313–318  2012.

[13] Ivar Ekeland  Alfred Galichon  and Marc Henry. Comonotonic Measures of Multivariate Risks.

Mathematical Finance  22(1):109–132  2012.

[14] W. Fan  M. Miller  S. Stolfo  W. Lee  and P. Chan. Using artiﬁcial anomalies to detect unknown

and known network intrusions. Knowledge and Information Systems  6(5):507–527  2004.

[15] Jörg Fliege and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization.

Mathematical Methods of Operations Research  51(3):479–494  2000.

[16] Alfred Galichon and Marc Henry. Dual theory of choice with multivariate risks. Journal of

Economic Theory  147(4):1501–1516  2012.

[17] Andrew B. Gardner  Abba M. Krieger  George Vachtsevanos  and Brian Litt. One-Class Novelty
Detection for Seizure Analysis from Intracranial EEG. Journal of Machine Learning Research 
7:1025–1044  2006.

[18] Izhak Golan and Ran El-Yaniv. Deep Anomaly Detection Using Geometric Transformations. In

Advances in Neural Information Processing Systems 31  pages 9758–9769. 2018.

[19] Paul M. Hayton  Bernhard Schölkopf  Lionel Tarassenko  and Paul Anuzis. Support Vector
Novelty Detection Applied to Jet Engine Vibration Spectra. In Advances in Neural Information
Processing Systems 13  pages 946–952  2001.

10

[20] Chin-Wei Huang  David Krueger  Alexandre Lacoste  and Aaron Courville. Neural Autoregres-

sive Flows. In ICML  2018.

[21] David Inouye and Pradeep Ravikumar. Deep Density Destructors. In Proceedings of the 35th

International Conference on Machine Learning  pages 2167–2175  2018.

[22] Priyank Jaini  Kira A. Selby  and Yaoliang Yu. Sum-of-Squares Polynomial Flow. In Proceed-

ings of The 36th International Conference on Machine Learning  2019.

[23] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In International

Conference on Learning Representations  2014.

[24] Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/

exdb/mnist/.

[25] W. Li  V. Mahadevan  and N. Vasconcelos. Anomaly Detection and Localization in Crowded
Scenes. IEEE Transactions on Pattern Analysis and Machine Intelligence  36(1):18–32  2014.

[26] Shiyu Liang  Yixuan Li  and R. Srikant. Enhancing The Reliability of Out-of-distribution Image
Detection in Neural Networks. In International Conference on Learning Representations  2018.

[27] Moshe Lichman. UCI machine learning repository. http://kdd.ics.uci.edu/

databases/kddcup99.

[28] Moshe Lichman. UCI machine learning repository. http://archive.ics.uci.edu/

ml.

[29] Larry Manevitz and Malik Yousef. One-class document classiﬁcation via Neural Networks.

Neurocomputing  70(7):1466–1481  2007.

[30] Larry M. Manevitz and Malik Yousef. One-Class SVMs for Document Classiﬁcation. Journal

of Machine Learning Research  2:139–154  2001.

[31] Markos Markou and Sameer Singh. Novelty detection: a review—part 1: statistical approaches.

Signal Processing  83(12):2481–2497  2003.

[32] Markos Markou and Sameer Singh. Novelty detection: a review—part 2: neural network based

approaches. Signal Processing  83(12):2499–2521  2003.

[33] Youssef Marzouk  Tarek Moselhy  Matthew Parno  and Alessio Spantini. Sampling via Measure

Transport: An Introduction  pages 1–41. Springer  2016.

[34] Aditya Krishna Menon and Robert C. Williamson. A loss framework for calibrated anomaly
detection. In Advances in Neural Information Processing Systems 31  pages 1494–1504  2018.

[35] Mary M. Moya and Don R. Hush. Network constraints and multi-objective optimization for

one-class classiﬁcation. Neural Networks  9(3):463–474  1996.

[36] H. Mukai. Algorithms for multicriterion optimization. IEEE Transactions on Automatic Control 

25(2):177–186  1980.

[37] G. L. O’Brien. The Comparison Method for Stochastic Processes. The Annals of Probability 

3(1):80–88  1975.

[38] George Papamakarios  Theo Pavlakou  and Iain Murray. Masked autoregressive ﬂow for density
estimation. In Advances in Neural Information Processing Systems  pages 2338–2347  2017.

[39] Deepak Pathak  Pulkit Agrawal  Alexei A. Efros  and Trevor Darrell. Curiosity-driven Explo-
ration by Self-supervised Prediction. In Proceedings of the 34th International Conference on
Machine Learning  pages 2778–2787  2017.

[40] Stanislav Pidhorskyi  Ranya Almohsen  and Gianfranco Doretto. Generative Probabilistic Nov-
elty Detection with Adversarial Autoencoders. In Advances in Neural Information Processing
Systems 31  pages 6823–6834  2018.

11

[41] Marco A.F. Pimentel  David A. Clifton  Lei Clifton  and Lionel Tarassenko. A review of novelty

detection. Signal Processing  99:215–249  2014.

[42] M. Ravanbakhsh  M. Nabi  E. Sangineto  L. Marcenaro  C. Regazzoni  and N. Sebe. Abnormal
event detection in videos using generative adversarial nets. In IEEE International Conference
on Image Processing (ICIP)  pages 1577–1581  2017.

[43] M. Ravanbakhsh  E. Sangineto  M. Nabi  and N. Sebe. Training Adversarial Discriminators
In IEEE Winter Conference on

for Cross-Channel Abnormal Event Detection in Crowds.
Applications of Computer Vision (WACV)  pages 1896–1904  2019.

[44] Lukas Ruff  Robert Vandermeulen  Nico Goernitz  Lucas Deecke  Shoaib Ahmed Siddiqui 
Alexander Binder  Emmanuel Müller  and Marius Kloft. Deep One-Class Classiﬁcation. In
Proceedings of the 35th International Conference on Machine Learning  volume 80  pages
4393–4402  2018.

[45] Ludger Rüschendorf. Stochastically ordered distributions and monotonicity of the oc-function

of sequential probability ratio tests. Series Statistics  12(3):327–338  1981.

[46] M. Sabokrou  M. Khalooei  M. Fathy  and E. Adeli. Adversarially Learned One-Class Classiﬁer
for Novelty Detection. In IEEE/CVF Conference on Computer Vision and Pattern Recognition 
pages 3379–3388  2018.

[47] Mohammad Sabokrou  Mohsen Fayyaz  Mahmood Fathy  Zahra. Moayed  and Reinhard Klette.
Deep-anomaly: Fully convolutional neural network for fast anomaly detection in crowded
scenes. Computer Vision and Image Understanding  172:88–97  2018.

[48] Thomas Schlegl  Philipp Seeböck  Sebastian M. Waldstein  Georg Langs  and Ursula Schmidt-
Erfurth. f-AnoGAN: Fast unsupervised anomaly detection with generative adversarial networks.
Medical Image Analysis  54:30–44  2019.

[49] Bernhard Schölkopf  John C. Platt  John Shawe-Taylor  Alex J. Smola  and Robert C.
Williamson. Estimating the Support of a High-Dimensional Distribution. Neural Compu-
tation  13(7):1443–1471  2001.

[50] Alessio Spantini  Daniele Bigoni  and Youssef Marzouk.

Inference via low-dimensional

couplings. Journal of Machine Learning Research  19:1–71  2018.

[51] Ingo Steinwart  Don Hush  and Clint Scovel. A classiﬁcation framework for anomaly detection.

Journal of Machine Learning Research  6:211–232  2005.

[52] Akiko Takeda and Masashi Sugiyama. ν-Support Vector Machine as Conditional Value-at-Risk
Minimization. In 25th International Conference on Machine Learning  pages 1056–1063  2008.
[53] David M. J. Tax and Robert P. W. Duin. Support vector data description. Machine learning 

54(1):45–66  2004.

[54] Cédric Villani. Optimal Transport: Old and New  volume 338. Springer  2008.
[55] Pascal Vincent  Hugo Larochelle  Yoshua Bengio  and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th international
conference on Machine learning  pages 1096–1103  2008.

[56] Y. Xia  X. Cao  F. Wen  G. Hua  and J. Sun. Learning Discriminative Reconstructions for
Unsupervised Outlier Removal. In IEEE International Conference on Computer Vision (ICCV) 
pages 1511–1519  2015.

[57] Han Xiao  Kashif Rasul  and Roland Vollgraf. Fashion-MNIST: a novel image dataset for

benchmarking machine learning algorithms. arXiv:1708.07747  2017.

[58] Shuangfei Zhai  Yu Cheng  Weining Lu  and Zhongfei Zhang. Deep Structured Energy Based
Models for Anomaly Detection. In Proceedings of The 33rd International Conference on
Machine Learning  volume 48  pages 1100–1109  2016.

[59] Bo Zong  Qi Song  Martin Renqiang Min  Wei Cheng  Cristian Lumezanu  Daeki Cho  and
Haifeng Chen. Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly
Detection. In International Conference on Learning Representations  2018.

12

,Jingjing Wang
Sun Sun
Yaoliang Yu