2019,Submodular Function Minimization with Noisy Evaluation Oracle,This paper considers submodular function minimization with \textit{noisy evaluation oracles} that return the function value of a submodular objective with zero-mean additive noise. For this problem  we provide an algorithm that returns an $O(n^{3/2}/\sqrt{T})$-additive approximate solution in expectation  where $n$ and $T$ stand for the size of the problem and the number of oracle calls  respectively. There is no room for reducing this error bound by a factor smaller than $O(1/\sqrt{n})$. Indeed  we show that any algorithm will suffer additive errors of $\Omega(n/\sqrt{T})$ in the worst case. Further  we consider an extended problem setting with \textit{multiple-point feedback} in which we can get the feedback of $k$ function values with each oracle call. Under the additional assumption that each noisy oracle is submodular and that $2 \leq k = O(1)$  we provide an algorithm with an $O(n/\sqrt{T})$-additive error bound as well as a worst-case analysis including a lower bound of $\Omega(n/\sqrt{T})$  which together imply that the algorithm achieves an optimal error bound up to a constant.,Submodular Function Minimization

with Noisy Evaluation Oracle

Shinji Ito∗

NEC Corporation  The University of Tokyo

i-shinji@nec.com

Abstract

This paper considers submodular function minimization with noisy evaluation ora-
√
cles that return the function value of a submodular objective with zero-mean addi-
tive noise. For this problem  we provide an algorithm that returns an O(n3/2/
T )-
additive approximate solution in expectation  where n and T stand for the size of
√
the problem and the number of oracle calls  respectively. There is no room for
reducing this error bound by a factor smaller than O(1/
n). Indeed  we show that
any algorithm will suffer additive errors of Ω(n/
T ) in the worst case. Further 
we consider an extended problem setting with multiple-point feedback in which
we can get the feedback of k function values with each oracle call. Under the
additional assumption that each noisy oracle is submodular and that 2 ≤ k = O(1) 
T )-additive error bound as well as a
we provide an algorithm with an O(n/
worst-case analysis including a lower bound of Ω(n/
T )  which together imply
that the algorithm achieves an optimal error bound up to a constant.

√

√

√

1

Introduction

Submodular function minimization (SFM) is an important problem that appears in a wide range of
research areas  including image segmentation [31; 33]  learning with structured regularization [6] 
and pricing optimization [26]. The goal in this problem is to ﬁnd a minimizer of a submodular
function  a function f : 2[n] → R deﬁned on the subsets of a given ﬁnite set [n] := {1  2  . . .   n} and
satisfying the following inequality:

f (X) + f (Y ) ≥ f (X ∩ Y ) + f (X ∪ Y ).

(1)

This condition is equivalent to the diminishing marginal returns property (see  e.g.  [17]): for every
X ⊆ Y ⊆ [n] and i ∈ [n] \ Y   f (X ∪ {i}) − f (X) ≥ f (Y ∪ {i}) − f (Y ).
Existing studies on SFM assume access to an evaluation oracle for f that returns the value f (X)
for any X in the feasible region. Under this assumption  a number of efﬁcient algorithms have been
discovered  in which the number of oracle calls as well as other computational time is bounded by a
polynomial in n. The ﬁrst polynomial-time algorithm was given by Grötschel  Lovász  and Schri-
jver [19] and was based on the ellipsoid method. Combinatorial strongly polynomial-time algorithms
have been independently proposed by Iwata  Fleischer  and Fujishige [28] and by Schrijver [38]. The
current best computational time is of O(n3 log2 n · EO + n4 logO(1) n) by Lee et al. [34]  where
EO denotes the time taken by the evaluation oracle to answer a single query. For approximate
optimization  Chakrabarty et al. [11] have proposed an algorithm that ﬁnds an ε-additive approximate
solution in ˜O(n5/3 · EO/ε2) time. The time complexity has been improved to ˜O(n · EO/ε2) by
Axelrod et al. [4].

∗This work was supported by JST  ACT-I  Grant Number JPMJPR18U5  Japan.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Additive error bounds for submodular minimization with noisy evaluation oracle.

Do not assume ˆft: submodular

(Ω(cid:48)(·) := Ω(min{1 ·})

single-point feedback

k-point feedback
(2 ≤ k ≤ n)
(n + 1)-point feedback

T 1/3

(cid:17)

Assume ˆft: submodular

[23]1: O(cid:0) n
(cid:1) (if T = Ω(n3))
(cid:16) n3/2√
and Ω(cid:48)(cid:16) n√
and Ω(cid:48)(cid:16) n√
(cid:17)
(cid:16) n√
(cid:17)
(cid:16) √
(cid:17)
and Ω(cid:48)(cid:16) √
(cid:17)

[This paper]: O
[This paper]:
O
[23]:
O

√
n√
T

2kT

+

kT

T

n√
T

n√
T

(cid:17)
and Ω(cid:48)(cid:16) n√
(cid:17)
(cid:16) n3/2√
and Ω(cid:48)(cid:16) √
(cid:17)
(cid:16) n√
(cid:17)

T
[This paper]:
O
[This paper]:
O

kT

kT

n√
T

T

(cid:17)

In some applications  however  evaluation oracles are not always available  and only noisy function
values are observable. For example  in the pricing optimization problem  let us consider selling n
types of products  where the value of the objective function f (X) corresponds to the expected gross
proﬁt  and the variable X ⊆ [n] corresponds to the set of discounted products. In this scenario 
Ito and Fujimaki [26] have shown that −f (X) is a submodular function under certain assumptions 
which means that the problem of maximizing the gross proﬁt f (X) is an example of SFM. In a
realistic situation  however  we are not given an explicit form of f  and the only thing we can do is to
observe the sales of products while changing prices. The observed gross proﬁt does not coincide with
its expectation f (X)  but changes randomly due to the inherent randomness of purchasing behavior
or some temporary events. This means that exact values of f (X) are unavailable  and  consequently 
existing works do not directly apply to this situation.
To deal with such problems  we introduce SFM with noisy evaluation oracles that return a random
value with expectation f (X). In other words  the noisy evaluation oracle ˆf returns ˆf (X) = f (X)+ξ 
where ξ is a zero-mean noise that may or may not depend on X. We assume access to T independent
noisy evaluation oracles ˆf1  ˆf2  . . .   ˆfT with bounded ranges. We start with the single-point feedback
setting and then study the more general multiple-point feedback (or k-point feedback) setting: In the
former setting  for each t ∈ [T ]  we choose one query Xt to feed ˆft  and get feedback of ˆft(Xt). In
the latter setting  we are given a positive integer k  and for each t  choose k queries to feed ˆft and
observe k real values of feedback. Such a situation with multiple-point feedback can be assumed
in some applications. For example  in the case of pricing optimization for E-commerce  we can get
multiple-point feedback by employing the A/B-testing framework  i.e.  by showing different prices to
randomly divided groups of customers. Note that each ˆft is not necessarily submodular even if its
expectation is submodular.
√
Our contribution is two-fold  positive results (algorithms  Theorem 1) and negative results (worst-case
√
analyses  Theorem 2): We propose algorithms that return O(1/
T )-additive approximate solutions 
and we show that arbitrary algorithms suffer additive errors of Ω(1/
T ) in the worst case. The
results are summarized in Table 1 with positive results in O(·) notation and negative ones in Ω(cid:48)(·)
notation.
√
As shown in Table 1  for the single-point feedback setting  we propose an algorithm that ﬁnds an
T )-additive approximate solution. Moreover  there is no room for reducing this additive
O(n3/2/
error bound by a smaller factor than O(1/
Indeed  our Theorem 2 implies that arbitrary
algorithms  including those requiring exponential time and space  suffer at least Ω(n/
T ) additive
errors. For the k-point feedback setting  both the lower and the upper bounds are decreased by
k factors  without additional assumptions. Under the assumption that each ˆft is submodular
1/
√
(Assumption 1)  however  the situation changes: Our proposed algorithm achieves O(n/
kT )-
√
n)-times smaller than without Assumption 1. We also show the lower
additive error  which is O(1/
bound of Ω(n/
kT )  which implies that  if k = O(1) or k = Ω(n)  then our algorithm
n/
is optimal up to constant factors  i.e.  no algorithms achieve additive errors of a smaller order.

2kT +

n).

√

√

√

√

√

√

1 This work applies to more general problem settings than ours  bandit submodular minimization and online

submodular minimization. See Section 2 for details.

2

To construct the algorithms  we combine a convex relaxation technique based on the Lovász extension
and stochastic gradient descent (SGD) method. The Lovász extension for a submodular function is a
convex function of which minimizers lead to solutions for SFM. Thanks to this  we can reduce SFM to
a convex optimization problem. In this study  we seek a minimizer of the Lovàsz extension by means
of SGD  in which we need to construct unbiased estimators of subgradients. The performance of the
SGD depends strongly on the variance of subgradient estimators. We present ways for constructing
subgradient estimators  and it turns out that Assumption 1 enables us to obtain estimators with smaller
variances. The combination of Lovász extension and SGD has been already introduced in the work
on bandit submodular minimization by Hazan and Kale [23]. Our work  however  considers different
problem settings  including multiple-point feedback  and presents tighter and more detailed analyses.
Details in the difference are given in Section 2.
A key technique for our lower bounds comes from the proof of regret lower bounds for bandit
problems by Auer et al. [3]. Their proof consists of two steps: they ﬁrst construct a probabilistic
distribution of inputs for which it is hard to detect a good arm offering a large reward  and then
show that any algorithm actually chooses the good arm only infrequently. We follow a line similar
to these two steps to prove Theorem 2  in which a number of technical issues arise. In the case of
multiple-point feedback  in particular  we need to assess the KL divergence carefully for the observed
signals from evaluation oracles.

2 Related Work

(cid:80)T

√

T ) regret bounds.

t=1 ft(Xt) − minX⊆[n]

by the regret deﬁned as RegretT :=(cid:80)T

Bandit submodular minimization (BSM) by Hazan and Kale [23] is strongly related to our model.
BSM is described as follows: in each iteration t ∈ [T ]  a decision maker chooses Xt ⊆ [n] and
observe ft(Xt)  where each ft : 2[n] → [−1  1] is a submodular function. In contrast to our model 
no stochastic models for ft are assumed  and the performance of the decision maker is measured
t=1 ft(X). This BSM problem
can be regarded as a generalization of our problem with single-point feedback under Assumption 1.
Indeed  given a BSM algorithm achieving RegretT ≤ b(n  T ) for some function b  one can construct
an SFM algorithm that returns b(n  T )/T -additive approximate solutions (see  e.g.  [25]). Since a
BSM algorithm with an O(nT 2/3) regret bound has been proposed in [23]  an O(n/T 1/3)-additive
approximate algorithm immediately follows  as in Table 1. In BSM  however  it has been left as an
open problem whether or not one can achieve O(nO(1)
With respect to SFM with an exact evaluation oracle  there is a large body of literature [6; 10; 27; 37;
42; 13]  in addition to the works mentioned in Section 1. The Fujishige-Wolfe algorithm [17]  based
on Wolfe’s minimum norm point algorithm [42] and the connection between minimum norm points
and the SFM shown in [16]  is known to have the best empirical performance in many cases [5; 18].
Chakrabarty et al. [10] have shown that the Fujishige-Wolfe algorithm ﬁnds an ε-additive approximate
solution with a running time of O(n2(EO + n)/ε2). The same runtime bound can be achieved by a
gradient descent approach presented by Bach [6].
For submodular function maximization with noisy evaluation oracles  there have been many studies.
Hassani et al. [21] provided a nearly 1/2-approximate algorithm for monotone submodular maximiza-
tion. Singla et al. [41] considered a similar problem with applications to crowdsourcing. Karimi et al.
[32] considered maximizing weighted coverage functions  a special case of submodular functions 
under matroid constraints  and presented an efﬁcient nearly (1 − 1/e)-approximate algorithm. Has-
sidim and Singer [22] provided a nearly (1 − 1/e)-approximate algorithm for monotone submodular
maximization with cardinality constraints. Mokhtari et al. [36] showed that a stochastic continuous
greedy method works well for monotone submodular function maximization subject to a convex
body constraint. For minimization problems with similar assumptions  in contrast to maximization
problems  only a little literature can be found. Blais et al. [8] considered approximate submodular
minimization with an approximate oracle model  and presented a polynomial-time algorithm with a
high-probability error bound. While their model is more general than ours  their algorithm requires
more the computational cost and oracle calls than ours  to achieve a similar error bound. Halabi
and Jegelka [20] dealt with minimization of weakly DR-submodular functions  which is a class of
approximately submodular functions  and provided algorithms with reasonable approximation ratios.
Zero-order or derivative-free convex optimization [2; 29; 39]  optimization problems with evaluation
oracle for convex objectives without access to gradients  is also related to our model because Lovász

3

√

extensions are convex. For general convex objectives  Agarwal et al. [2]  Belloni et al. [7] and Bubeck
et al. [9] have proposed algorithms that return ˜O(1/
T )-additive approximate solutions  ignoring
factors of polynomials in log T and nO(1)  where n stands for the dimension of the feasible region.
Though the error bounds in these results include factors larger than O(n3)  it has been reported [5; 24]
that dependence w.r.t. n can be improved under such additional assumptions as the smoothness and
the strong convexity of the objectives. These improved results  however  do not apply to our problems
because Lovász extensions are neither smooth nor strongly convex. Multiple-point feedback has been
considered in zero-order convex optimization  and some algorithms have been reported to achieve
optimal performance in such problem settings [1; 15; 40]. In terms of the lower bound on the additive
error  Jamieson et al. [29] and Shamir [39] have shown lower bounds of Ω(1/
T ) or Ω(1/T ) for
various classes of convex objectives  which  however  do not directly apply to our model.

√

3 Problem Setting
Let n be a positive integer  and let [n] = {1  2  . . .   n} stand for the ﬁnite set consisting of positive
integers at most n. Let L ⊆ 2[n] be a distributive lattice  i.e.  we assume that X  Y ∈ L implies
X ∩ Y  X ∪ Y ∈ L. Let f : L → [−1  1] be a submodular function that we aim to minimize. In
our problem setting  we are not given access to exact values of f  but given noisy evaluation oracles
{ ˆft}T
t=1 of f  where ˆft are random functions from L to [−1  1] that satisfy E[ ˆft(X)] = f (X) for all
t = 1  2  . . .   T and X ∈ L. We also assume that ˆf1  ˆf2  . . .   ˆfT are independent.
Our goal is to construct algorithms for solving the following problem: First  the algorithm is given
the decision set L and the number T of available oracle calls. For t = 1  2  . . .   T   the algorithm
chooses Xt ∈ L and observes ˆft(Xt). The chosen query Xt can depend on previous observations
{ ˆfj(Xj)}t−1
j=1. After T rounds of observation  the algorithm outputs ˆX ∈ L. We call this problem a
single-point feedback setting. In an alternative problem setting  a multi-point or k-point feedback
setting  we are given a parameter k ≥ 2 in addition to T and L. In the k-point feedback setting 
∈ L  and  after that  it observes the
the algorithm can choose k queries X (1)
) from the evaluation oracle in each round t ∈ T . In both
values ˆft(X (1)
settings  the performance of the algorithm is evaluated in terms of the additive error ET deﬁned as
ET = f ( ˆX) − minX∈L f (X).
A part of our results relies on the following assumption. Note that the following is assumed only
when it is explicitly mentioned.
Assumption 1. Assume that each ˆft : L → [−1  1] is submodular and that k ≥ 2.

)  . . .   ˆft(X (k)

)  ˆft(X (2)

  . . .   X (k)

t

t

t

  X (2)

t

t

t

4 Our Contribution

Our contribution is two-fold: positive results (Theorem 1) and negative results (Theorem 2).
Theorem 1. Suppose 1 ≤ k ≤ n + 1. For the problem with k-point feedback  there is an algorithm
that returns ˆX such that

(2)

(3)

√
E[ET ] = E[f ( ˆX)] − minX∈L f (X) = O(n3/2/
If Assumption 1 holds  there is an algorithm that returns ˆX such that
E[ET ] = E[f ( ˆX)] − minX∈L f (X) = O(n/

√

kT ).

kT ).

The expectation is taken w.r.t. the randomness of oracles ˆft and the algorithm’s internal randomness.
In both algorithms  the running time is bounded by O((kEO + n log n)T ) if L = 2[n]  where EO
stands for the time taken by an evaluation oracle to answer a single query.

If we can choose the number T of oracle calls arbitrarily  we are then able to compute ε-additive
approximate solution (in expectation) for arbitrary ε > 0  by means of the algorithm with the error
bound (2). The computational time for it is of O( n3
k log n)). Indeed  to ﬁnd an ε-additive
ε2 (EO + n
approximate solution  it sufﬁces to set T so that ε = Θ( n3/2√
)  which is equivalent to T = Θ( n3
kε2 ).

kT

4

ε2 (EO+ n

ε2 (EO + n

k log n)) time.

The computational time is then bounded as O((kEO+n log n)T ) = O( n3
k log n)). Similarly 
if Assumption 1 holds and the algorithm achieving (3) is used  an ε-additive approximate solution
can be found in O( n2
The following theorem gives an insight regarding how tight the above error bounds in Theorem 1 are.
Theorem 2. There is a probability distribution of instances for which any algorithm suffers errors of
(4)
where we denote Ω(cid:48)(·) := Ω(min{1 ·}). In addition  there is a probability distribution of instances
satisfying Assumption 1 for which any algorithm suffers errors of
√
E[ET ] = E[f ( ˆX) − minX∈L f (X)] = Ω(cid:48)(n/

(5)
The expectation is taken w.r.t. the randomness of the instance f and oracles ˆft  and the algorithm’s
internal randomness.

E[ET ] = E[f ( ˆX) − minX∈L f (X)] = Ω(cid:48)(n/

2kT +(cid:112)n/T ).

√

kT ) 

From (4) in this theorem  we can see that at least Ω( n2
ε2 EO) computational time is required to ﬁnd an
ε-additive approximate solution. This can be shown by an argument similar to that after Theorem 1.
For the problem with exact evaluation oracles  on the other hand  Chakrabarty et al. [11] have
proposed an algorithm running in ˜O( n5/3
ε2 EO)-time. By comparing these two results  we can see that
SFM with noisy oracle is essentially harder than SFM with exact oracle.

5 Algorithm

˜f (x) =(cid:82) 1

5.1 Preliminary
Lovász extension of submodular function For a [0  1]-valued vector x = (x1  . . .   xn)(cid:62) ∈ [0  1]d
and a real value u ∈ [0  1]  deﬁne Hx(u) ⊆ [n] to be the set of indices i for which xi ≥ u  i.e. 
Hx(u) = {i ∈ [n] | xi ≥ u}. For a distributive lattice L  deﬁne a convex hull ˜L ⊆ [0  1]n of L as
follows: ˜L = {x ⊆ [0  1]n | Hx(u) ∈ L for all u ∈ [0  1]}. Given a function f : L → R  we deﬁne
the Lovász extension ˜f : ˜L → R of f as

0 f (Hx(u))du.

(6)
From the deﬁnition  we have ˜f (χX ) = f (X) for all X ∈ L  i.e.  ˜f is an extension of f.2 The
following theorem provides a connection between submodular functions and convex functions:
Theorem 3 ([35]). A function f : L → R is submodular if and only if ˜f is convex. For a submodular
function f : L → R  we have minX∈L f (X) = minx∈ ˜L
For a proof of this theorem  see  e.g.  [17; 35].
For x ∈ [0  1]n  let σ : [n] → [n] be a permutation over [n] such that xσ(1) ≥ xσ(2) ≥ ··· ≥ xσ(n).
For any permutation σ over [n]  deﬁne Sσ(i) = {σ(j) | j ≤ i}. The Lovász extension deﬁned by (6)
can then be rewritten as

˜f (x)

n(cid:88)

˜f (x) = f ([0]) +

(f (Sσ(i)) − f (Sσ(i − 1)))xσ(i)

i=1

= f ([0])(1 − xσ(1)) +

n−1(cid:88)

f (Sσ(i))(xσ(i) − xσ(i+1)) + f ([n])xσ(n).

(7)

(8)

Similar expression can be found  e.g.  Lemma 6.19 in the book [17].

i=1

Subgradient of Lovász extension From the above two expressions (7) and (8) of the Lovász
extension  we obtain two alternative ways to express its subgradient. For a permutation σ over [n]
and i ∈ {0  1  . . .   n}  deﬁne ψσ(i) ∈ {−1  0  1}n as

ψσ(0) = −χσ(1)  ψσ(n) = χσ(n)  ψσ(i) = χσ(i) − χσ(i+1)
(9)
2 χX ∈ {0  1}n denotes the indicator vector of X  i.e.  (χX )i = 1 for i ∈ X and (χX )i = 0 for i ∈ [n]\X.

(i = 1  2  . . .   n − 1).

5

n(cid:88)

A subgradient of ˜f at x can then be expressed by g(σx) deﬁned as

g(σ) :=

(f (Sσ(i)) − f (Sσ(i − 1)))χσ(i)

i=1

= −f ([0])χσ(1) +

n−1(cid:88)

f (Sσ(i))(χσ(i) − χσ(i+1)) + f ([n])χσ(n) =

where (10) and (11) come from (7) and (8)  respectively.

i=1

(10)

f (Sσ(i))ψσ(i) 

(11)

n(cid:88)

i=0

xt+1 = P ˜L(xt − ηˆgt) 

5.2 Stochastic Gradient Descent Method
Our algorithm is based on the stochastic gradient descent method for ˜f : ˜L → [0  1]. To start with 
2 · 1 ∈ ˜L. For t = 1  2  . . .   T   we update xt by iteratively calling the oracle ˆft to
we initialize x1 = 1
obtain xt+1. In each update  we construct an unbiased estimator ˆgt of a subgradient of ˜f at xt (a
more concrete construction will be given later)  and xt+1 is given by

(12)
(cid:107)y − x(cid:107)2 

where P ˜L : Rn → ˜L stands for a Euclidean projection to ˜L  i.e.  P ˜L(x) ∈ arg min
y∈ ˜L
and η > 0 is a parameter that we can change arbitrarily. We then compute ¯x = 1
t=1 xt
T
and draw u from a uniform distribution over [0  1]  and output ˆX = H¯x(u). From (6)  we have
E[f ( ˆX)] = E[ ˜f (¯x)]. To analyze the performance of our algorithm  we use the following theorem:
Theorem 4. Let D ∈ Rn be a compact convex set containing 0. For a convex function ˜f : D → R 
let x1  . . .   xT be deﬁned by x1 = 0 and xt+1 = PD(xt − ηˆgt)  where E[ˆgt|xt] is a subgradient of ˜f
at xt for each t. Then  ¯x := 1
T
E[ ˜f (¯x)] − min
x∗∈D

t=1 xt satisﬁes
˜f (x∗) ≤ 1
T

maxx∈D (cid:107)x(cid:107)2

E[(cid:107)ˆgt(cid:107)2
2]

(cid:80)T

T(cid:88)

(cid:80)T

(cid:32)

(cid:33)

(13)

η
2

2η

+

2

.

t=1

n

For completeness  we give a proof of this theorem in Appendix A. A similar analysis can be
found in  e.g.  Lemma 11 of [23]. When setting D = ˜L − 1
2 ≤
2 · 1  we have maxx∈D (cid:107)x(cid:107)2
4 . From this  Theorems 3 and 4  if ˆgt is bounded as E[(cid:107)ˆgt(cid:107)2
2] ≤ G2 for all t  we then have
E[f ( ˆX)] − minX∗∈L f (X∗) ≤ 1
. The performance of the algorithm here depends
on G  an upper bound on the expected norm of unbiased estimator ˆgt. We evaluate the magnitude of
G for speciﬁc examples of ˆgt  in the following subsection.

(cid:16) n

8η + η

2 G2T

(cid:17)

T

5.3 Unbiased Estimators of Subgradients

In this subsection  we present two different ways to construct unbiased estimators for a subgradient
of ˜f that are based on (11) and (10)  respectively. The latter is available for the case of multiple-point
feedback  i.e.  k ≥ 2  and produces a smaller error bound under Assumption 1. Without such an
assumption  the former gives a better error bound. Given xt = (xt1  xt2  . . .   xtn)(cid:62) ∈ [0  1]n  let
σ : [n] → [n] be a permutation over [n] for which xtσ(1) ≥ xtσ(2) ≥ ··· ≥ xtσ(n).
An estimator based on the expression (11) Suppose k ∈ [n + 1]. Consider choosing queries
t }k
j=1 ⊆ {0  1  . . .   n} of size k  uniformly
{X (j)
at random  i.e.  It follows a uniform distribution over the subset family {I ⊆ {0  1  . . .   n} | |I| = k}.
(cid:80)
Then let X (j)

j=1 randomly as follows: Choose a subset It = {i(j)

(14)
where ψσ(i) is deﬁned in (9). Note that ˆgt relies on xt since σ depends on xt. Then  ˆgt is an unbiased
estimator of a subgradient and satisﬁes E[(cid:107)ˆgt(cid:107)2
Lemma 1. Suppose that ˆgt is given by (14). We then have

t } and observe ˆft(X (j)
t
)ψσ(i(j)
i∈It

) for j ∈ [k]. Deﬁne ˆgt as
ˆft(Sσ(i))ψσ(i) 

t ) = {σ(j) | j ≤ i(j)

2] = O(n2/k):

t = Sσ(i(j)

t ) = n+1
k

(cid:80)k

ˆgt = n+1
k

ˆft(X (j)

t }k

j=1

t

E[ˆgt|xt] ∈ ∂ ˜f (xt)  E[(cid:107)ˆgt(cid:107)2

2] ≤ 2(n + 1)(n + k)/k.

(15)

Proofs of all lemmas in this paper are given in Appendix B.

6

Algorithm 1 An algorithm for submoudular function minimization with noisy evaluation oracle
Require: The size n ≥ 1 of the problem  the number T ≥ 1 of oracle calls  the number k ∈ [n + 1]

2 · 1.

of feedback values per oracle call  and the learning rate η > 0.

else

Let σ : [n] → [n] be a permutation corresponding to xt  i.e.  xtσ(1) ≥ ··· ≥ xtσ(n).
if Assumption 1 holds then

Choose a subset Jt ⊆ [n] of size l = (cid:98)k/2(cid:99)  uniformly at random.
Call the evaluation oracle ˆft to observe ˆft(Sσ(i)) and ˆft(Sσ(i − 1)) for i ∈ Jt.
Construct an unbiased estimator ˆgt of a subgradient of ˜f at xt  as (16).
Choose a subset It ⊆ {0  1  . . .   n} of size k  uniformly at random.
Call the evaluation oracle ˆft to observe ˆft(Sσ(i)) for i ∈ It.
Construct an unbiased estimator ˆgt of a subgradient of ˜f at xt  as (14).

1: Set x1 = 1
2: for t = 1  2  . . .   T do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end for
15: Set ¯x = 1
T
16: Draw u from a uniform distribution over [0  1]  and output ˆX = H¯x(u) = {i ∈ [n] | ¯xi ≥ u}.

end if
Compute xt+1 from xt and ˆgt on the basis of (12).

(cid:80)T

t=1 xt.

An estimator based on the expression (10) Suppose 2 ≤ k ≤ n + 1 holds  and let l denote
j=1 randomly as follows: Choose a subset Jt ⊆
l = (cid:98)k/2(cid:99) ≥ 1. Consider choosing queries {X (j)
{1  . . .   n} of size l  uniformly at random. Then  set queries {X (j)
{Sσ(i)  Sσ(i −
1)} ⊆ {X (j)

i∈Jt
j=1  and observe ˆft(Sσ(i)) and ˆft(Sσ(i − 1)) for i ∈ Jt. Deﬁne ˆgt as

j=1 so that(cid:83)

t }k

t }k

t }k

(cid:80)

ˆgt = n
l

( ˆft(Sσ(i)) − ˆft(Sσ(i − 1))χσ(i).
Then  ˆgt is an unbiased estimator of a subgradient and satisﬁes E[(cid:107)ˆgt(cid:107)2
submodular function  then E[(cid:107)ˆgt(cid:107)2
Lemma 2. Suppose that ˆgt is given by (16). We then have

2] = O(n/k) holds.

i∈Jt

E[ˆgt|xt] ∈ ∂ ˜f (xt)  E[(cid:107)ˆgt(cid:107)2
In addition  if ˆft is a submodular function  we then have

2] ≤ 4n2/l ≤ 12n2/k.

2] = O(n2/k)  and if ˆft is a

(16)

(17)

E[(cid:107)ˆgt(cid:107)2

2] ≤ 16n/l ≤ 48n/k.

(18)
A key factor in the advantage of the estimator deﬁned by (16) is that the vector (ft(Sσ(i))−ft(Sσ(i−
i=0 ∈ Rn+1  which is implied by Lemma 8 in
1)))n
[23] or Lemma 1 in [30].

i=1 ∈ Rn has a smaller norm than (ft(Sσ(i)))n

5.4 Proof of Theorem 1

˜f (x∗) = E[f ( ˆX)] − minX∗∈L f (X∗) from (6) and Theorem 3.

By combining SGD described in Section 5.2 and unbiased estimators deﬁned by (14) or (16)  we
obtain Algorithm 1. Let us evaluate the additive errors for this algorithm. Note that we have
E[ ˜f (¯x)] − minx∗∈ ˜L
Suppose ˆX is produced by Algorithm 1 in which Steps 9–11 are chosen. From Theorem 4 and
Lemma 1  we have E[f ( ˆX)] − minX∗∈L f (X∗) ≤ 1
. The right-hand side is
8T (n+1)(n+k). We then have E[f ( ˆX)] − minX∗∈L f (X∗) ≤
minimized when η is chosen as η =
(cid:17)

Suppose that Assumption 1 holds and that ˆX is produced by Algorithm 1  where Steps 5–7 are
chosen. From Theorem 4 and Lemma 1  we have E[f ( ˆX)] − minX∗∈L f (X∗) ≤ 1
.

(cid:113) n(n+1)(n+k)

kT )  which proves (2).

8η + ηT (n+1)(n+k)

(cid:16) n

(cid:16) n

= O( n3/2

(cid:113)

(cid:17)

2kT

kn

T

k

8η + 24ηT n

k

T

7

(cid:113) k

12n√
kT

192T . We then have E[f ( ˆX)] −

= O( n√
kT

)  which proves (3).

The right-hand side is minimized when η is chosen as η =
minX∗∈L f (X∗) ≤ √
The computational time of Algorithm 1 can be evaluated as follows: Step 3 can be conducted
by a sorting algorithm  which takes O(n log n) time. Step 5 can be done by generating uniform
random numbers over [m] for m = n  n − 1  . . .   n − k + 1  which takes O(k log n) times. Step
6 requires O(kEO) time computation. Step 7 can be computed with O(n) arithmetic operations.
Steps 9–11 are similar to Steps 5–7. Step 13 takes O(n) time since xt − ηˆgt can be computed
with O(n) arithmetic operations and since P ˜L(x) has an explicit form. Hence  Steps 2–14 require
O((n log n + k log n + kEO + n + n) · T ) = O((kEO + n log n)T ) time. The other steps do not
require time greater than this. Therefore  the overall time complexity is of O((kEO + n log n)T ).

6 Lower Bound

(cid:26) −1

1

(i ∈ X)
(i /∈ X)

6.1 Construction of Hard Instance
. for i ∈ [n]. Fix a subset S∗ ⊆ [n] and
Deﬁne hi : 2[n] → {−1  1} as hi(X) :=
a positive real value ε ∈ [0  1]. Consider the following procedure that produces a function ˆf : 2[n] →
{−1  1}: (1) Choose i ∈ [n] uniformly at random  and set s = 1 with probability 1−ε
2   s = −1 with
2 . (2) Deﬁne ˆf : 2[n] → {−1  1} by ˆf (X) = s· hi(S∗(cid:52)X) = s· hi(S∗)hi(X)  where
probability 1+ε
S∗(cid:52)X stands for the symmetric difference between S∗ and X  i.e.  S∗(cid:52)X = (S∗ \ X) ∪ (X \ S∗).
Let F (S∗  ε) denote the distribution of functions generated by the above procedure. A similar
construction can be found in [14]  which is for a lower bound of bandit linear optimization.
In addition  deﬁne F (cid:48)(S∗  ε) similarly  so that all function values of f ∼ F (cid:48)(S∗  ε) are stochastically
independent: Choose iX ∈ [n] and sX with the probability deﬁned as the above  independently for all
X ⊆ [n]  and deﬁne ˆf (X) = sX · hiX (S∗)hi(X). Let F (cid:48)(S∗  ε) denote the distribution of functions
generated by this procedure. Note that each ˆf generated from F (S∗  ε) is a modular function and that
this does not always hold for F (cid:48)(S∗  ε). If DS∗ = F (S∗  ε) or if DS∗ = F (cid:48)(S∗  ε)  the expectation
of ˆf ∼ DS∗ is a submodular function expressed as

(cid:80)n
i=1 hi(S∗)hi(X) = ε

n (2|S∗(cid:52)X| − n) 

(19)

fS∗ ε(X) := E

ˆf∼DS∗

[ ˆf (X)] = − ε

n

where the second equality comes from E[s] = E[sX ] = 1−ε

2 − 1+ε

2 = −ε.

6.2 Proof of Theorem 2

To prove Theorem 2  we start with bounding the additive error from below by means of KL
divergences. Fix X (1)  X (2)  . . .   X (k) ⊆ [n] arbitrarily. For a class {DS∗
| S∗ ⊆ [n]}
of distributions over { ˆf : 2[n] → {−1  1}} 
let PS∗ denote the distribution of y( ˆf ) =
( ˆf (X (1))  ˆf (X (2)) . . .   ˆf (X (k)))(cid:62) ∈ Rk for ˆf ∼ DS∗. We then have the following:
Lemma 3. Suppose that a class of distributions {DS∗ | S∗ ⊆ [d]} satisﬁes (19) for all S∗ ⊆ [d]. In
addition  suppose that the following holds for arbitrary S∗  X (1)  X (2)  . . .   X (k) ⊆ [n]:

(cid:80)n
i=1 DKL(PS∗||PS∗(cid:52){i}) ≤ n
2T .

(cid:104)

(20)
If S∗ is chosen uniformly at random from 2[n]  and ˆft follows DS∗ i.i.d. for t = 1  2  . . .   T   then any
algorithm suffers an additive error of E[ET ] = E
2   where the
expectation is taken w.r.t. S∗  ˆft  and the internal randomness of algorithms.
Intuitively  the condition (20) means that the distribution of the observed values y does not change
much even if the optimal solution S∗ is perturbed. Consequently  under the condition (20)  it is hard
for any algorithm to detect S∗. Sufﬁcient conditions for (20) are given in the following two lemmas:
Lemma 4. Suppose that {PS∗} is deﬁned by DS∗ = F (cid:48)(S∗  ε) for 0 ≤ ε ≤ min{ 1
}. Then
6  
(20) holds for arbitrary S∗  X (1)  . . .   X (k) ⊆ [n].

fS∗ ε( ˆX) − minS∈2[n] fS∗ ε(S)

(cid:105) ≥ ε

n√

8kT

8

(cid:113)

6   n

5

24T min{2k 2n}}. Then (20) holds for arbitrary S∗  X (1)  . . .   X (k) ⊆ [n].

Lemma 5. Suppose that {PS∗} is deﬁned by DS∗ = F (S∗  ε) for 0 ≤ ε ≤
min{ 1
Theorem 2 can be proven by combining Lemmas 3  4  and 5. From Lemmas 3 and 4  if S∗ is chosen
uniformly at random from 2[n] and if ˆft follows F (cid:48)(S∗  ε) with ε = min{ 1
}  i.i.d. for t ∈ [T ] 
6  
)  which proves (4). If k ≥ 2 and if S∗ is
we then have E[ET ] ≥ ε
chosen uniformly at random from 2[n]  and ˆft follows F (S∗  ε) with ε = min{ 1
24T min{2k 2n}} 
i.i.d. for t ∈ [T ]  then Assumption 1 is satisﬁed since ˆft ∼ F (S∗  ε) is a submodular. Further  from
T }) =
Lemmas 3 and 5  we have E[ET ] ≥ ε
Ω(cid:48)( n√
T )  which proves (5).

T 2k  (cid:112) n

96T min{2k 2n}} = Ω(cid:48)(max{ n√

5

T 2k +(cid:112) n

2 = min{ 1

12   n

} = Ω(cid:48)( n√

2 = min{ 1
12  

n√

32kT

(cid:113)

n√

8kT

6   n

(cid:113)

kT

5

7 Conclusion and Open Questions

We have introduced submodular function minimization with noisy evaluation oracle  and have
provided algorithms and lower bounds  which together implies that the proposed algorithms achieve
nearly optimal additive errors  modulo O(
n) factors. For the special cases of k-point feedback
settings  in which 2 ≤ k = O(1) and each noisy evaluation oracle itself is a submodular function  we
have provided a tight error bound. For the other cases  we leave it as an open question to ﬁnd tight
bounds.

√

Acknowledgment

The author thanks Satoru Iwata for valuable discussions and for pointing out important literature. The
author also thanks reviewers for many helpful comments and suggestions.

References
[1] A. Agarwal  O. Dekel  and L. Xiao. Optimal algorithms for online convex optimization with

multi-point bandit feedback. In Conference on Learning Theory  pages 28–40  2010.

[2] A. Agarwal  D. P. Foster  D. J. Hsu  S. M. Kakade  and A. Rakhlin. Stochastic convex
optimization with bandit feedback. In Advances in Neural Information Processing Systems 
pages 1035–1043  2011.

[3] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. The nonstochastic multiarmed bandit

problem. SIAM Journal on Computing  32(1):48–77  2002.

[4] B. Axelrod  Y. P. Liu  and A. Sidford. Near-optimal approximate discrete and continuous
In Proceedings of the Fourteenth Annual ACM-SIAM

submodular function minimization.
Symposium on Discrete Algorithms  pages 837–853. SIAM  2020.

[5] F. Bach. Convex analysis and optimization with submodular functions: a tutorial. arXiv preprint

arXiv:1010.4207  2010.

[6] F. Bach. Learning with submodular functions: A convex optimization perspective. Foundations

and Trends R(cid:13) in Machine Learning  6(2-3):145–373  2013.

[7] A. Belloni  T. Liang  H. Narayanan  and A. Rakhlin. Escaping the local minima via simulated
annealing: Optimization of approximately convex functions. In Conference on Learning Theory 
pages 240–265  2015.

[8] E. Blais  C. L. Canonne  T. Eden  A. Levi  and D. Ron. Tolerant junta testing and the connection
to submodular optimization and function isomorphism. In Proceedings of the Twenty-Ninth
Annual ACM-SIAM Symposium on Discrete Algorithms  pages 2113–2132. SIAM  2018.

[9] S. Bubeck  Y. T. Lee  and R. Eldan. Kernel-based methods for bandit convex optimization.
In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing  pages
72–85. ACM  2017.

9

[10] D. Chakrabarty  P. Jain  and P. Kothari. Provable submodular minimization using wolfe’s

algorithm. In Advances in Neural Information Processing Systems  pages 802–809  2014.

[11] D. Chakrabarty  Y. T. Lee  A. Sidford  and S. C.-w. Wong. Subquadratic submodular function
minimization. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
Computing  pages 1220–1231. ACM  2017.

[12] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons  2012.

[13] D. Dadush  L. A. Végh  and G. Zambelli. Geometric rescaling algorithms for submodular
function minimization. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
Discrete Algorithms  pages 832–848. SIAM  2018.

[14] V. Dani  S. M. Kakade  and T. P. Hayes. The price of bandit information for online optimization.

In Advances in Neural Information Processing Systems  pages 345–352  2008.

[15] J. C. Duchi  M. I. Jordan  M. J. Wainwright  and A. Wibisono. Optimal rates for zero-order
convex optimization: The power of two function evaluations. IEEE Transactions on Information
Theory  61(5):2788–2806  2015.

[16] S. Fujishige. Lexicographically optimal base of a polymatroid with respect to a weight vector.

Mathematics of Operations Research  5(2):186–196  1980.

[17] S. Fujishige. Submodular Functions and Optimization  volume 58. Elsevier  2005.

[18] S. Fujishige and S. Isotani. A submodular function minimization algorithm based on the

minimum-norm base. Paciﬁc Journal of Optimization  7(1):3–17  2011.

[19] M. Grötschel  L. Lovász  and A. Schrijver. The ellipsoid method and its consequences in

combinatorial optimization. Combinatorica  1(2):169–197  1981.

[20] M. E. Halabi and S. Jegelka. Minimizing approximately submodular functions. arXiv preprint

arXiv:1905.12145  2019.

[21] H. Hassani  M. Soltanolkotabi  and A. Karbasi. Gradient methods for submodular maximization.

In Advances in Neural Information Processing Systems  pages 5841–5851  2017.

[22] A. Hassidim and Y. Singer. Submodular optimization under noise. In Conference on Learning

Theory  pages 1069–1122  2017.

[23] E. Hazan and S. Kale. Online submodular minimization. Journal of Machine Learning Research 

13(Oct):2903–2922  2012.

[24] E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Advances in

Neural Information Processing Systems  pages 784–792  2014.

[25] E. Hazan et al. Introduction to online convex optimization. Foundations and Trends R(cid:13) in

Optimization  2(3-4):157–325  2016.

[26] S. Ito and R. Fujimaki. Large-scale price optimization via network ﬂow. In Advances in Neural

Information Processing Systems  pages 3855–3863  2016.

[27] S. Iwata. A faster scaling algorithm for minimizing submodular functions. SIAM Journal on

Computing  32(4):833–840  2003.

[28] S. Iwata  L. Fleischer  and S. Fujishige. A combinatorial strongly polynomial algorithm for

minimizing submodular functions. Journal of the ACM (JACM)  48(4):761–777  2001.

[29] K. G. Jamieson  R. Nowak  and B. Recht. Query complexity of derivative-free optimization. In

Advances in Neural Information Processing Systems  pages 2672–2680  2012.

[30] S. Jegelka and J. Bilmes. Online submodular minimization for combinatorial structures. In

International Conference on Machine Learning  pages 345–352  2011.

10

[31] S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: Coupling edges in graph
cuts. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 1897–1904  2011.

[32] M. Karimi  M. Lucic  H. Hassani  and A. Krause. Stochastic submodular maximization: The
case of coverage functions. In Advances in Neural Information Processing Systems  pages
6853–6863  2017.

[33] P. Kohli and P. H. Torr. Dynamic graph cuts and their applications in computer vision. In

Computer Vision  pages 51–108. Springer  2010.

[34] Y. T. Lee  A. Sidford  and S. C.-w. Wong. A faster cutting plane method and its implications for
combinatorial and convex optimization. In 2015 IEEE 56th Annual Symposium on Foundations
of Computer Science  pages 1049–1065. IEEE  2015.

[35] L. Lovász. Submodular functions and convexity. In Mathematical Programming The State of

the Art  pages 235–257. Springer  1983.

[36] A. Mokhtari  H. Hassani  and A. Karbasi. Conditional gradient method for stochastic submodular
maximization: Closing the gap. In International Conference on Artiﬁcial Intelligence and
Statistics  pages 1886–1895  2018.

[37] J. B. Orlin. A faster strongly polynomial time algorithm for submodular function minimization.

Mathematical Programming  118(2):237–251  2009.

[38] A. Schrijver. A combinatorial algorithm minimizing submodular functions in strongly polyno-

mial time. Journal of Combinatorial Theory  Series B  80(2):346–355  2000.

[39] O. Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In

Conference on Learning Theory  pages 3–24  2013.

[40] O. Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point

feedback. Journal of Machine Learning Research  18(52):1–11  2017.

[41] A. Singla  S. Tschiatschek  and A. Krause. Noisy submodular maximization via adaptive
sampling with applications to crowdsourced image collection summarization. In Thirtieth AAAI
Conference on Artiﬁcial Intelligence  2016.

[42] P. Wolfe. Finding the nearest point in a polytope. Mathematical Programming  11(1):128–149 

1976.

11

,Shinji Ito