2013,Restricting exchangeable nonparametric distributions,Distributions over exchangeable matrices with infinitely many columns are useful in constructing nonparametric latent variable models. However  the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper  we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point  and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution.,Restricting exchangeable nonparametric distributions

Sinead A. Williamson

University of Texas at Austin

Steven N. MacEachern
The Ohio State University

Eric P. Xing

Carnegie Mellon University

Abstract

Distributions over matrices with exchangeable rows and inﬁnitely many columns
are useful in constructing nonparametric latent variable models. However  the dis-
tribution implied by such models over the number of features exhibited by each
data point may be poorly-suited for many modeling tasks. In this paper  we pro-
pose a class of exchangeable nonparametric priors obtained by restricting the do-
main of existing models. Such models allow us to specify the distribution over the
number of features per data point  and can achieve better performance on data sets
where the number of features is not well-modeled by the original distribution.

1

Introduction

The Indian buffet process [IBP  11] is one of several distributions over matrices with exchangeable
rows and inﬁnitely many columns  only a ﬁnite (but random) number of which contain any non-zero
entries. Such distributions have proved useful for constructing ﬂexible latent feature models that do
not require us to specify the number of latent features a priori. In such models  each column of the
random matrix corresponds to a latent feature  and each row to a data point. The non-zero elements
of a row select the subset of features that contribute to the corresponding data point.
However  distributions such as the IBP make certain assumptions about the structure of the data that
may be inappropriate. Speciﬁcally  such priors impose distributions on the number of data points that
exhibit a given feature  and on the number of features exhibited by a given data point. For example 
in the IBP  the number of features exhibited by a data point is marginally Poisson-distributed  and
the probability of a data point exhibiting a previously-observed feature is proportional to the number
of times that feature has been seen so far.
These distributional assumptions may not be appropriate for many modeling tasks. For example 
the IBP has been used to model both text [17] and network [13] data. It is well known that word
frequencies in text corpora and degree distributions of networks often exhibit power-law behavior;
it seems reasonable to suppose that this behavior would be better captured by models that assume
a heavy-tailed distribution over the number of latent features  rather than the Poisson distribution
assumed by the IBP and related random matrices.
In certain cases we may instead wish to add constraints on the number of latent features exhibited
per data point  particularly in cases where we expect  or desire  the latent features to correspond
to interpretable features  or causes  of the data [20]. For example  we might believe that each data
point exhibits exactly S features – corresponding perhaps to speakers in a dialog  members of a
team  or alleles in a genotype – but be agnostic about the total number of features in our data set. A
model that explicitly encodes this prior expectation about the number of features per data point will
tend to lead to more interpretable and parsimonious results. Alternatively  we may wish to specify
a minimum number of latent features. For example  the IBP has been used to select possible next
states in a hidden Markov model [10]. In such a model  we do not expect to see a state that allows
no transitions (including self-transitions). Nonetheless  because a data point in the IBP can have
zero features with non-zero probability  this situation can occur  resulting in an invalid transition
distribution.

1

In this paper  we propose a method for modifying the distribution over the number of non-zero ele-
ments per row in arbitrary exchangeable matrices  allowing us to control the number of features per
data point in a corresponding latent feature model. We show that our construction yields exchange-
able distributions  and present Monte Carlo methods for posterior inference. Our experimental eval-
uation shows that this approach allows us to incorporate prior beliefs about the number of features
per data point into our model  yielding superior modeling performance.

2 Exchangeability

We say a ﬁnite sequence (X1  . . .   XN ) is exchangeable [see  for example  1] if its distribution
is unchanged under any permutation σ of {1  . . .   N}. Further  we say that an inﬁnite sequence
X1  X2  . . . is inﬁnitely exchangeable if all of its ﬁnite subsequences are exchangeable. Such distri-
butions are appropriate when we do not believe the order in which we see our data is important. In
such cases  a model whose posterior distribution depends on the order in which we see our data is
not justiﬁed. In addition  exchangeable models often yield efﬁcient Gibbs samplers.
De Finetti’s law tells us that a sequence is exchangeable iff the observations are i.i.d. given some
latent distribution. This means that we can write the probability of any exchangeable sequence as

(cid:90)

(cid:89)

P (X1 = x1  X2 = x2  . . . ) =

µθ(Xi = xi)ν(dθ)

(1)

Θ

i

for some probability distribution ν over parameter space Θ  and some parametrized family {µθ}θ∈Θ
of conditional probability distributions.
Throughout this paper  we will use the notation p(x1  x2  . . . ) = P (X1 = x1  X2 = x2  . . . ) to
represent the joint distribution over an exchangeable sequence x1  x2  . . . ; p(xn+1|x1  . . .   xn) to
i=1 µθ(Xi = xi)ν(θ) to
represent the joint distribution over the observations and the parameter θ.

represent the associated predictive distribution; and p(x1  . . .   xn  θ) :=(cid:81)n

2.1 Distributions over exchangeable matrices

assign independent masses to disjoint subsets of Ω  that can be written in the form Γ =(cid:80)∞

The Indian buffet process [IBP  11] is a distribution over binary matrices with exchangeable rows and
inﬁnitely many columns. In the de Finetti representation  the mixing distribution ν is a beta process 
the parameter θ is a countably inﬁnite measure with atom sizes πk ∈ (0  1]  and the conditional
distribution µθ is a Bernoulli process [17]. The beta process and the Bernoulli process are both
completely random measures [CRM  12] – distributions over random measures on some space Ω that
k=1 πkδφk.
We can think of each atom of θ as determining the latent probability for a column of a matrix with
inﬁnitely many columns  and the Bernoulli process as sampling binary values for the entries of that
column of the matrix. The resulting matrix has a ﬁnite number of non-zero entries  with the number
of non-zero entries in each row distributed as Poisson(α) and the total number of non-zero columns
in N rows distributed as Poisson(αHN )  where HN is the Nth harmonic number. The number of
rows with a non-zero entry for a given column exhibits a “rich gets richer” property – a new row has
a one in a given column with probability proportional to the number of times a one has appeared in
that column in the preceding rows.
Different patterns of behavior can be obtained with different choices of CRM. A three-parameter
extension to the IBP [15] replaces the beta process with a completely random measure called the
stable-beta process  which includes the beta process as a special case. The resulting random matrix
exhibits power law behavior: the total number of features exhibited in a data set of size N grows
as O(N s) for some s > 0  and the number of data points exhibiting each feature also follows
a power law. The number of features per data point  however  remains Poisson-distributed. The
inﬁnite gamma-Poisson process [iGaP  18] replaces the beta process with a gamma process  and
the Bernoulli process with a Poisson process  to give a distribution over non-negative integer-valued
matrices with inﬁnitely many columns and exchangeable rows. In this model  the sum of each row is
distributed according to a negative binomial distribution  and the number of non-zero entries in each
row is Poisson-distributed. The beta-negative binomial process [21] replaces the Bernoulli process
with a negative binomial process to get an alternative distribution over non-negative integer-valued
matrices.

2

3 Removing the Poisson assumption

While different choices of CRMs in the de Finetti construction can alter the distribution over the
number of data points that exhibit a feature and (in the case of non-binary matrices) the row sums 
they retain a marginally Poisson distribution over the number of distinct features exhibited by a
given data point. The construction of Caron [4] extends the IBP to allow the number of features in
each row to follow a mixture of Poissons  by assigning data point-speciﬁc parameters that have an
effect equivalent to a monotonic transformation on the atom sizes in the underlying beta process;
however conditioned on these parameters  the sum of each row is still Poisson-distributed.
This repeatedly occurring Poisson distribution is a direct result of the construction of a binary matrix
from a combination of CRMs. To elaborate on this  note that  marginally  the distribution over the
value of each element zik of a row zi of the IBP (or a three-parameter IBP) is given by a Bernoulli
k zik is distributed according to a

distribution. Therefore  by the law of rare events  the sum (cid:80)
distribution  hence the number of non-zero elements (cid:80)
bution over the row sum (cid:80)

Poisson distribution.
A similar argument applies to integer-valued matrices such as the inﬁnite gamma-Poisson process.
Marginally  the distribution over whether an element zik is greater than zero is given by a Bernoulli
k zik ∧ 1  is Poisson-distributed. The distri-

k zik  will depend on the choice of CRMs.

It follows that  if we wish to circumvent the requirement of a Poisson (or mixture of Poisson) number
of features per data point in an IBP-like model  we must remove the completely random assumption
on either the de Finetti mixing distribution or the family of conditional distributions. The remainder
of this section discusses how we can obtain arbitrary marginal distributions over the number of
features per row by using conditional distributions that are not completely random.

3.1 Restricting the family of conditional distributions in the de Finetti representation

Recall from Section 2 that any exchangeable sequence can be represented as a mixture over some
family of conditional distributions. The support of this family determines the support of the ex-
changeable sequence. For example  in the IBP the family of conditional distributions is the Bernoulli
process  which has support in {0  1}∞. A sample from the IBP therefore has support in {{0  1}∞}N .
We are familiar with the idea of restricting the support of a distribution to a measurable subset. For
example  a truncated Gaussian is a Gaussian distribution restricted to a contiguous section of the real
line. In general  we can restrict an arbitrary probability distribution µ with support Ω to a measurable
subset A ⊂ Ω by deﬁning µ|A(·) := µ(·)I(· ∈ A)/µ(A).
Theorem 1 (Restricted exchangeable distributions). We can restrict the support of an exchangeable
distribution by restricting the family of conditional distributions {µθ}θ∈Θ introduced in Equation 1 
to obtain an exchangeable distribution on the restricted space.

p(x1  . . .   xN   θ) = (cid:81)N

an

Proof. Consider
representation
Let p|A be the restriction of p such that
Xi ∈ A  i = 1  2  . . .   obtained by restricting the family of conditional distributions {µθ} to
|A
{µ
θ } as described above. Then

unrestricted
i=1 µθ(Xi = xi)ν(θ).

exchangeable model with

de Finetti

p|A(x1  . . .   xN   θ) =(cid:81)N

θ (Xi = xi)ν(θ) =(cid:81)N
(cid:81)N +1
(cid:81)N +1
i=1 µθ(Xi=xi)
i=1 µθ(Xi∈A)
is an exchangeable sequence by construction  according to de Finetti’s law.

p|A(xN +1|x1  . . .   xN ) ∝ I(xN +1 ∈ A)

i=1 µ

(cid:90)

and

|A

i=1

Θ

µθ(Xi=xi)I(xi∈A)

µθ(Xi∈A)

ν(θ)  

ν(dθ)

(2)

We give three examples of exchangeable matrices where the number of non-zero entries per row is
restricted to follow a given distribution. While our focus is on exchangeability of the rows  we note
that the following distributions (like their unrestricted counterparts) are invariant under reordering
of the columns  and that the resulting matrices are separately exchangeable [2].
Example 1 (Restriction of the IBP to a ﬁxed number of non-zero entries per row). The family of
conditional distributions in the IBP is given by the Bernoulli process. We can restrict the support

3

of the Bernoulli process to an arbitrary measurable subset A ⊂ {0  1}∞ – for example  the set of
k zk = S for some integer S. The conditional distribution of a
matrix Z = {z1  . . .   zN} under such a distribution is given by:

all vectors z ∈ {0  1}∞ such that(cid:80)
i=1 µB(Zi = zi)I((cid:80)
(cid:81)N
(µB((cid:80)
(cid:81)∞

|S
B (Z = Z) =
µ

k Zik = S))N
k (1 − πk)N−mk
k=1)N

k=1 πmk
PoiBin(S|{πk}∞

=

where mk =(cid:80)

k zik = S)

N(cid:89)

(cid:18) ∞(cid:88)

I

(cid:19)

zik = S

 

(3)

i=1

k=1

i zik and PoiBin(·|{πk}∞

k=1) is the inﬁnite limit of the Poisson-binomial distribution
[6]  which describes the distribution over the number of successes in a sequence of independent
but non-identical Bernoulli trials. The probability of Z given in Equation 3 is the inﬁnite limit of
the conditional Bernoulli distribution [6]  which describes the distribution of the locations of the
successes in such a trial  conditioned on their sum.
Example 2 (Restriction of the iGaP to a ﬁxed number of non-zero entries per row). The fam-
(cid:80)
ily of conditional distributions in the iGaP is given by the Poisson process  which has support in
N∞. Following Example 1  we can restrict this support to the set of all vectors z ∈ N∞ such that
k zk ∧ 1 = S for some integer S – i.e. the set of all non-negative integer-valued inﬁnite vectors
with S non-zero entries. The conditional distribution of a matrix Z = {z1  . . .   zN} under such a
distribution is given by:

|S
µ
G (Z = Z) =

=

(cid:81)N
i=1 µG(Zi = zi)I((cid:80)∞
(µG((cid:80)∞
k=1 zik ∧ 1 = S)
(cid:81)∞
k=1 Zik ∧ 1 = S))N
N(cid:89)
(cid:81)N
k e−λk
i=1 zik!
PoiBin(S|{e−λk}∞

(cid:18) ∞(cid:88)

k=1

mk

I

λ

k=1)N

i=1

k=1

(cid:19)

.

(4)

zik ∧ 1 = S

Example 3 (Restriction of the IBP to a random number of non-zero entries per row). Rather than
specify the number of non-zero entries in each row a priori  we can allow it to be random  with
some arbitrary distribution f (·) over the non-negative integers. A Bernoulli process restricted to
have f-marginals can be described as

N(cid:89)
where Sn = (cid:80)∞

|f
µ
B (Z) =

i=1

|Si
µ
B (Zi = zi)f (Si) =

k=1 znk. If we marginalize over B = (cid:80)∞

PoiBin(Si|{πk}∞

i=1

k=1 zik = Si)

k=1)

exchangeable  because mixtures of i.i.d. distributions are i.i.d.

f (Si)I((cid:80)∞

N(cid:89)

∞(cid:89)

k=1

k (1 − πk)N−mk  
πmk

(5)

k=1 πkδφk  the resulting distribution is

We note that  even if we choose f to be Poisson(α)  we will not recover the IBP. The IBP has
Poisson(α) marginals over the number non-zero elements per row  but the conditional distribution
is described by a Poisson-binomial distribution. The Poisson-restricted IBP  however  will have
Poisson marginal and conditional distributions.
Figure 1 shows some examples of samples from the single-parameter IBP  with parameter α = 5 
with various restrictions applied.

Figure 1: Samples from restricted IBPs.

3.2 Direct restriction of the predictive distributions

The construction in Section 3.1 is explicitly conditioned on a draw B from the de Finetti mix-
ing distribution ν. Since it might be cumbersome to explicitly represent the inﬁnite dimensional

4

IBP1 per row5 per row10 per rowUniform{1 ... 20}Power−law (s=2)object B  it is tempting to consider constructions that directly restrict the predictive distribution
p(XN +1|X1  . . .   XN )  where B has been marginalized out.
Unfortunately  the distribution over matrices obtained by this approach does not (in general – see the
appendix for a counter-example) correspond to the distribution over matrices obtained by restricting
the family of conditional distributions. Moreover  the resulting distribution will not in general be
exchangeable. This means it is not appropriate for data sets where we have no explicit ordering of
the data  and also means we cannot directly use the predictive distribution to deﬁne a Gibbs sampler
(as is possible in exchangeable models).
Theorem 2 (Sequences obtained by directly restricting the predictive distribution of an exchangeable
sequence are not  in general  exchangeable). Let p be the distribution of the unrestricted exchange-
able model introduced in the proof of Theorem 1. Let p∗|A be the distribution obtained by directly
restricting this unrestricted exchangeable model such that Xi ∈ A  i.e.

p∗|A(xN +1|x1  . . .   xN ) ∝ I(xN +1 ∈ A)

.

(6)

(cid:82)
(cid:82)

Θ

Θ

(cid:81)N +1
(cid:81)N +1
i=1 µθ(X = xi)ν(dθ)
i=1 µθ(X ∈ A)ν(dθ)

In general  this will not be equal to Equation 2  and cannot be expressed as a mixture of i.i.d.
distributions.

Proof. To demonstrate that this is true  consider the counterexample given in Example 4.
Example 4 (A three-urn buffet). Consider a simple form of the Indian buffet process  with a base
measure consisting of three unit-mass atoms. We can represent the predictive distribution of such
a model using three indexed urns  each containing one red ball (representing a one in the resulting
matrix) and one blue ball (representing a zero in the resulting matrix). We generate a sequence of
ball sequences by repeatedly picking a ball from each urn  noting the ordered sequence of colors 
and returning the balls to their urns  plus one ball of each sampled color.
Proposition 1. The three-urn buffet is exchangeable.

Proof. By using the fact that a sequence is exchangeable iff the predictive distribution given the ﬁrst
N elements of the sequence of the N + 1st and N + 2nd entries is exchangeable [9]  it is trivial to
show that this model is exchangeable and that  for example 

p(XN +1 = (r  b  r)  XN +2 = (r  r  b)|X1:N )
m1m2(N + 1 − m3)

· (m + 1 + 1)(N + 1 − m2)m3

=
(N + 2)3
=p(XN +1 = (r  r  b)  XN +2 = (r  b  r)|X1:N )  

(N + 1)3

where mi is the number of times in the ﬁrst N samples that the ith ball in a sample has been red.
Proposition 2. The directly restricted three-urn scheme (and  by extension  the directly restricted
IBP) is not exchangeable.

Proof. Consider the same scheme  but where the outcome is restricted such that there is one  and
only one  red ball per sample. The probability of a sequence in this restricted model is given by

(7)

(8)

and  for example 

(cid:80)3

(cid:80)3

mk

N +1−mk

k=1

I(xi = r)

p∗(XN +1 = x|X1:N ) =

N +1−mk
p∗(XN +1 = (r  b  b)  XN +2 = (b  r  b)|X1:N )

k=1

mk

(cid:80)

m1

N +1−m1
mk

·

m2

N +2−m3

+(cid:80)

=
(cid:54)=p∗(XN +1 = (b  r  b)  XN +2 = (r  b  b)|X1:N )  

− m2

N +1−mk

N +1−m2

N +2−m2

m2

k

k

mk

N +1−mk

therefore the restricted model is not exchangeable. By introducing a normalizing constant – cor-
responding to restricting over a subset of {0  1}3 – that depends on the previous samples  we have
broken the exchangeability of the sequence.
By extension  a model obtained by directly restricting the predictive distribution of the IBP is not
exchangeable.

5

We note that there may well be situations where a non-exchangeable model  such as that described
in Proposition 2  is appropriate for our data – for example where there is an explicit ordering on the
data. It is not  however  an appropriate model if we believe our data to be exchangeable  or if we
are interested in ﬁnding a single  stationary latent distribution describing our data. This exchange-
able setting is the focus of this paper  so we defer exploration of distribution of non-exchangeable
matrices obtained by restriction of the predictive distribution to future work.

4

Inference

We focus on models obtained by restricting the IBP to have f-marginals over the number of non-
zero elements per row  as described in Example 3. Note that when f = δS  this yields the setting
described in Example 1. Extension to other cases  such as the restricted iGaP model of Example 2 
are straightforward. We work with a truncated model  where we approximate the countably inﬁnite
sequence {πk}∞
k=1 with a large  but ﬁnite  vector π := (π1  . . .   πK)  where each atom πk is dis-
tributed according to Beta(α/K  1). An alternative approach would be to develop a slice sampler
that uses a random truncation  avoiding the error introduced by the ﬁxed truncation [14  16]. We

assume a likelihood function g(X|Z) =(cid:81)

i g(xi|zi).

4.1 Sampling the binary matrix Z
For marginal functions f that assign probability mass to a contiguous  non-singleton subset of N 
we can Gibbs sample each entry of Z according to
p((cid:80)
j(cid:54)=k zij = a) ∝ πk
j(cid:54)=k zij = a) ∝ (1 − πk)

k zk=a+1|π) g(xi|zik = 1  Z¬ik)
p((cid:80)
Where f = δS  this approach will fail  since any move that changes zik must change(cid:80)
k zk=a|π) g(xi|zik = 0  Z¬ik).

p(zik = 1|xi  π  Z¬ik (cid:80)
p(zik = 0|xi  π  Z¬ik (cid:80)

f (a+1)

(9)

k zik. In this

setting  instead  we sample the locations of the non-zero entries z(j)
) ∝ πk(1 − πk)−1g(xi|z(j)

i = k|xi  π  z(¬j)

(10)
To improve mixing  we also include Metropolis-Hastings moves that propose an entire row of Z.
We include details in the supplementary material.

  j = 1  . . .   S of zi:
i = k  z(¬j)

p(z(j)

f (a)

) .

i

i

i

4.2 Sampling the beta process atoms π

(cid:81)K

ν({πk}∞

k=1|Z) ∝ µ

Conditioned on Z  the the distribution of π is
|f{πk}(Z = Z)ν({πk}∞

(cid:81)N
The Poisson-binomial term can be calculated exactly in O(K(cid:80)
i=1 PoiBin(Si|π)
k zik) using either a recursive algo-
rithm [3  5] or an algorithm based on the characteristic function that uses the Discrete Fourier Trans-
form [8]. It can also be approximated using a skewed-normal approximation to the Poisson-binomial
distribution [19]. We can therefore sample from the posterior of π using Metropolis Hastings steps.
Since we believe our posterior will be close to the posterior for the unrestricted model  we use the
proposal distribution q(πk|Z) = Beta(α/K + mk  N + 1 − mk) to propose new values of πk.

(1 − πk)N−mk

k=1 π(mk+ α

k=1) ∝

K −1)

(11)

k

.

4.3 Evaluating the predictive distribution
In certain cases  we may wish to directly evaluate the predictive distribution p|f (zN +1|z1  . . .   zN ).
Unfortunately  in the case of the IBP  we are unable to perform the integral in Equation 2 analyti-
cally. We can  however  estimate the predictive distribution using importance sampling. We sample
T measures π(t) ∼ ν(π|Z)  where ν(π|Z) is the posterior distribution over π in the ﬁnite approxi-
mation to the IBP  and then weight them to obtain the restricted predictive distribution

(cid:80)T

(cid:80)

t=1 wtµ

|f
π(t) (zN +1)
t wt

 

(12)

p|f (zN +1|z1  . . .   zN ) ≈ 1
T

6

Figure 2: Top row: True features. Bottom row: Sample data points for S = 2.

IBP
rIBP

S = 2

7297.4 ± 2822.8

57.2 ± 66.4

S = 5

8982.2 ± 1981.7
3469.7 ± 133.7

S = 8

7442.8 ± 3602.0
5963.8 ± 871.4

S = 11

8862.1 ± 3920.2
11413 ± 1992.9

S = 14

20244 ± 6809.7
12199 ± 2593.8

Table 1: Structure error on synthetic data with 100 data points and S features per data point.

|f
where wt = µ
π(t) (z1  . . .   zN )/µπ(t)(z1  . . .   zN )  and

π (Z) ∝ N(cid:89)

µ|f

f (Si)I((cid:80)K

k=1 zik = Si)

PoiBin(Si|π)

K(cid:89)

k=1

k (1 − πk)N−mk .
πmk

i=1

5 Experimental evaluation

In this paper  we have described how distributions over exchangeable matrices  such as the IBP 
can be modiﬁed to allow more ﬂexible control over the distributions over the number of latent
features. In this section  we perform experiments on both real and synthetic data. The synthetic data
experiments are designed to show that appropriate restriction can yield more interpretable features.
The experiments on real data are designed to show that careful choice of the distribution over the
number of latent features in our models can lead to improved predictive performance.

5.1 Synthetic data

The IBP has been used to discover latent features that correspond to interpretable phenomena  such
as latent causes behind patient symptoms [20]. If we have prior knowledge about the number of la-
tent features per data point – for example  the number of players in a team  or the number of speakers
in a conversation – we may expect both better predictive performance  and more interpretable latent
features. In this experiment  we evaluate this hypothesis on synthetic data  where the true latent
features are known. We generated images by randomly selecting S of 16 binary features  shown in
Figure 2  superimposing them  and adding isotropic Gaussian noise (σ2 = 0.25). We modeled the
resulting data using an uncollapsed linear Gaussian model  as described in [7]  using both the IBP 
and the IBP restricted to have S features per row. To compare the generating matrix Z0 and our pos-
terior estimate Z  we looked at the structure error [20]. This is the sum absolute difference between
0 and E[ZZT ]  and is a general measure of graph dissimilarity.
the upper triangular portions of Z0ZT
Table 1 shows the structure error obtained using both a standard IBP model (IBP) and an IBP re-
stricted to have the correct number of latent features (rIBP)  for varying numbers of features S. In
each case  the number of data points is 100  the IBP parameter α is ﬁxed to S  and the model is
truncated to 50 features. Each experiment was repeated 10 times on independently generated data
sets; we present the mean and standard deviation. All samplers were run for 5000 samples; the ﬁrst
2500 were discarded as burn-in.
Where the number of features per data point is small relative to the total number of features  the
restricted model does a much better job at recovering the “correct” latent structure. While the IBP
may be able to explain the training data set as well as the restricted model  it will not in general
recover the desired latent structure – which is important if we wish to interpret the latent structure.
Once the number of features per data point increases beyond half the total number of features  the
model is ill-speciﬁed – it is more parsimonious to represent features via the absence of a bar. As
a result  both models perform poorly at recovering the generating structure. The restricted model
– and indeed the IBP – should only be expected to recover easily interpretable features where the
number of such features per data point is small relative to the total number of features.

7

1

0.591
0.622

11

0.961
0.971

2

0.726
0.749

12

0.969
0.978

3

0.796
0.819

13

0.974
0.981

4

0.848
0.864

14

0.978
0.983

5

0.878
0.899

15

0.982
0.988

6

0.905
0.918

16

0.989
0.992

7

0.923
0.935

17

0.991
0.998

8

0.936
0.948

18

0.996
1.000

9

0.952
0.959

19

0.997
1.000

10

0.958
0.966

20

1.000
1.000

IBP
rIBP

IBP
rIBP

Table 2: Proportion correct at n on classifying documents from the 20newsgroup data set.

5.2 Classiﬁcation of text data

The IBP and its extensions have been used to directly model text data [17  15]. In such settings 
the IBP is used to directly model the presence or absence of words  and so the matrix Z is observed
rather than latent  and the total number of features is given by the vocabulary size. We hypothesize
that the Poisson assumption made by the IBP is not appropriate for text data  as the statistics of word
use in natural language tends to follow a heavier tailed distribution [22]. To test this hypothesis  we
modeled a collection of corpora using both an IBP  and an IBP restricted to have a negative Binomial
distribution over the number of words. Our corpora were 20 collections of newsgroup postings on
various topics (for example  comp.graphics  rec.autos  rec.sport.hockey)1. No pre-processing of the
documents was performed. Since the vocabulary (and hence the feature space) is ﬁnite  we truncated
both models to the vocabulary size. Due to the very large state space  we restricted our samples such
that  in a single sample  atoms with the same posterior distribution were assigned the same value.
For each model  α was set to the mean number of words per document in the corresponding group 
and the maximum likelihood parameters were used for the negative Binomial distribution.
To evaluate the quality of the models  we classiﬁed held out documents based on their likelihood
under each of the 20 newsgroups. This experiment is designed to replicate an experiment performed
by [15] to compare the original and three-parameter IBP models. For both models  we estimated the
predictive distribution by generating 1000 samples from the posterior of the beta process in the IBP
model. For the IBP  we used these samples directly to estimate the predictive distribution; for the
restricted model  we used the importance-weighted samples obtained using Equation 12. For each
model  we trained on 1000 randomly selected documents  and tested on a further 1000 documents.
Table 2 shows the fraction of documents correctly classiﬁed in the ﬁrst n labels – i.e. the fraction
of documents for which the correct labels is one of the n most likely. The restricted IBP (rIBP)
performs uniformly better than the unrestricted model.

6 Discussion and future work

The framework explored in this paper allows us to relax the distributional assumptions made by
existing exchangeable nonparametric processes. As future work  we intend to explore which appli-
cations and models can most beneﬁt from this greater ﬂexibility.

We note that the model  as posed  suffers from an identiﬁability issue. Let ˜B =(cid:80)∞
measure obtained by transforming B =(cid:80)∞

k=1 ˜πkδφk be the
k=1 πkδφk such that ˜πk = πk/(1 − πk). Then  scaling
˜B by a positive scalar does not affect the likelihood of a given matrix Z. We intend to explore the
consequences of this in future work.

Acknowledgments

We would like to thank Zoubin Ghahramani for valuable suggestions and discussions throughout
this project. We would also like to thank Finale Doshi-Velez and Ryan Adams for pointing out
the non-identiﬁability mentioned in Section 6. This research was supported in part by NSF grants
DMS-1209194 and IIS-1111142  AFOSR grant FA95501010247  and NIH grant R01GM093156.

1http://people.csail.mit.edu/jrennie/20Newsgroups/

8

References
[1] D. Aldous. Exchangeability and related topics.

XIII  pages 1–198  1985.

´Ecole d’ ´Et´e de Probabilit´es de Saint-Flour

[2] D. J. Aldous. Representations for partially exchangeable arrays of random variables. Journal

of Multivariate Analysis  11(4):581–598  1981.

[3] R. E. Barlow and K. D. Heidtmann. Computing k-out-of-n system reliability. IEEE Transac-

tions on Reliability  33:322–323  1984.

[4] F. Caron. Bayesian nonparametric models for bipartite graphs. In Neural Information Process-

ing Systems  2012.

[5] S. X Chen  A. P. Dempster  and J. S. Liu. Weighted ﬁnite population sampling to maximize

entropy. Biometrika  81:457–469  1994.

[6] S. X. Chen and J. S. Liu. Statistical applications of the Poisson-binomial and conditional

Bernoulli distributions. Statistica Sinica  7:875–892  1997.

[7] F. Doshi-Velez and Z. Ghahramani. Accelerated Gibbs sampling for the Indian buffet process.

In International Conference on Machine Learning  2009.

[8] M. Fern´andez and S. Williams. Closed-form expression for the Poisson-binomial probability

density function. IEEE Transactions on Aerospace Electronic Systems  46:803–817  2010.

[9] S. Fortini  L. Ladelli  and E. Regazzini. Exchangeability  predictive distributions and paramet-

ric models. Sankhy¯a: The Indian Journal of Statistics  Series A  pages 86–109  2000.

[10] E. B. Fox  E. B. Sudderth  M. I. Jordan  and A. S. Willsky. Sharing features among dynamical

systems with beta processes. In Neural Information Processing Systems  2010.

[11] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process.

In Neural Information Processing Systems  2005.

[12] J. F. C. Kingman. Completely random measures. Paciﬁc Journal of Mathematics  21(1):59–78 

1967.

[13] K. T. Miller  T. L. Grifﬁths  and M. I. Jordan. Nonparametric latent feature models for link

prediction. In Neural Information Processing Systems  2009.

[14] R. M. Neal. Slice sampling. Annals of Statistics  31(3):705–767  2003.
[15] Y. W. Teh and D. G¨or¨ur. Indian buffet processes with power law behaviour. In Neural Infor-

mation Processing Systems  2009.

[16] Y. W. Teh  D. G¨or¨ur  and Z. Ghahramani. Stick-breaking construction for the Indian buffet

process. In Artiﬁcial Intelligence and Statistics  2007.

[17] R. Thibaux and M.I. Jordan. Hierarchical beta processes and the Indian buffet process.

Artiﬁcial Intelligence and Statistics  2007.

In

[18] M. Titsias. The inﬁnite gamma-Poisson feature model.

Systems  2007.

In Neural Information Processing

[19] A. Y. Volkova. A reﬁnement of the central limit theorem for sums of independent random

indicators. Theory of Probability and its Applications  40:791–794  1996.

[20] F. Wood  T. L. Grifﬁths  and Z. Ghahramani. A non-parametric Bayesian method for inferring

hidden causes. In Uncertainty in Artiﬁcial Intelligence  2006.

[21] M. Zhou  L. A. Hannah  D. B. Dunson  and L. Carin. Beta-negative binomial process and

Poisson factor analysis. In Artiﬁcial Intelligence and Statistics  2012.

[22] G. K. Zipf. Selective Studies and the Principle of Relative Frequency in Language. Harvard

University Press  1932.

9

,Sinead Williamson
Steve MacEachern
Eric Xing
Scott Linderman
Christopher Stock
Ryan Adams