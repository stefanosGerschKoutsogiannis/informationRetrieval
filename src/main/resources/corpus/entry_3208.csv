2018,Lifted Weighted Mini-Bucket,Many graphical models  such as Markov Logic Networks (MLNs) with evidence  possess highly symmetric substructures but no exact symmetries.  Unfortunately  there are few principled methods that exploit these symmetric substructures to perform efficient approximate inference.  In this paper  we present a lifted variant of the Weighted Mini-Bucket elimination algorithm which provides a principled way to (i) exploit the highly symmetric substructure of MLN models  and (ii) incorporate high-order inference terms which are necessary for high quality approximate inference.  Our method has significant control over the accuracy-time trade-off of the approximation  allowing us to generate any-time approximations.  Experimental results demonstrate the utility of this class of approximations  especially in models with strong repulsive potentials.,Lifted Weighted Mini-Bucket

Nicholas Gallo

University of California Irvine

Irvine  CA 92637-3435

ngallo1@uci.edu

Alexander Ihler

University of California Irvine

Irvine  CA 92637-3435
ihler@ics.uci.edu

Abstract

Many graphical models  such as Markov Logic Networks (MLNs) with evidence 
possess highly symmetric substructures but no exact symmetries. Unfortunately 
there are few principled methods that exploit these symmetric substructures to
perform efﬁcient approximate inference. In this paper  we present a lifted variant of
the Weighted Mini-Bucket elimination algorithm which provides a principled way
to (i) exploit the highly symmetric substructure of MLN models  and (ii) incorporate
high-order inference terms which are necessary for high quality approximate
inference. Our method has signiﬁcant control over the accuracy-time trade-off of
the approximation  allowing us to generate any-time approximations. Experimental
results demonstrate the utility of this class of approximations  especially in models
with strong repulsive potentials.

1

Introduction

Many applications require computing likelihoods and marginal probabilities over a distribution
deﬁned by a graphical model  tasks which are intractable in general [24]. This has motivated the
development of approximate inference techniques with controlled computational cost. Inference in
these settings often involves reasoning over a set of regions (subsets of variables)  with larger regions
providing higher accuracy at a higher cost. This paper utilizes the Weighted Mini-Bucket (WMB)
[10] algorithm which employs a simple heuristic method of region selection that mimics a variable
elimination procedure.
Recently  there has been interest in modeling large problems with repeated potentials and structure 
often described with a Markov Logic Network (MLN) language [16]. Such models arise in many
settings such as social network analysis (e.g. estimating voting habits)  collective classiﬁcation (e.g.
classifying text in connected web-pages)  and many others [16]. In these settings  lifted inference
refers to a broad class of techniques  both exact [3  15  21] and approximate [4  11  13  5  12  9] 
that exploit model symmetries. Most of these methods work well when the model possesses well-
deﬁned symmetries [4  11  13  5  12  14]  but break down in unpredictable ways in the presence of
unstructured model perturbations present in most practical settings. The problem of asymmetries in
the approximate inference structure is compounded when higher order inference terms (for which
lifted inference requires higher order model symmetry) are incorporated [14  20  5].
Methods to control computational cost in the presence of asymmetries are largely heuristic  such
as [19] which presents a belief propagation procedure that approximates messages in a symmetric
form. Other works create an over-symmetric approximate model [23  22] on which inference is
run  but provide no guarantees on its relation to the original problem. Similar to our work  [17]
employ (non-weighted) mini-bucket inference; however  they too rely on over-symmetric model
approximation heuristics to control computational cost.
This paper addresses the shortcomings of the methods described above with a lifted variant of
Weighted mini-bucket (LWMB) that is able to (i) trade-off inference cost with accuracy in a controlled

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

manner in the presence of asymmetries  and (ii) incorporate higher-order approximate inference
terms  which are crucial for high quality inference. This work can be seen as a high-order (property
(ii)) extension of [9] (which is qualitatively identical in property (i)). Additionally  this work employs
efﬁcient region selection and representation for MLN models  and hence never grounds the graph as
many others are required to do for symmetry detection (e.g. [9  20  14]).

2 Background

A Markov random ﬁeld (MRF) over n discrete random variables (RVs) X = [X1 . . . Xn] taking
values x = [x1 . . . xn] ∈ (X 1 × . . . × X n) has probability density function

p(X = x) =

1
Z

(cid:89)

α∈I

fα(xα);

Z =

fα(xα)

(cid:88)

···(cid:88)

xn

x1

(cid:89)

α∈I

where I indexes subsets of variables and each α ∈ I is associated with potential table fα. The
partition function Z normalizes the distribution. Calculating Z is a central problem in many learning
and inference tasks  but exact evaluation of the summation is exponential in n  and hence intractable.

2.1 Bucket and Mini-Bucket Elimination

Bucket Elimination (BE) [6] is an exact inference algorithm that directly eliminates RVs along a
sequence o called the elimination order. Without loss of generality  we assume that each factor
index α ∈ I is ordered according to o. BE operates by performing the summation (2) along each
RV in sequence. The computation is organized with a set of buckets B1 . . . Bn where initially each
Bv = {fα | α1 = v} is the set of model factors whose earliest eliminated RV index is v. Proceeding
sequentially along o  we multiply the factors in Bv  then sum over xv producing a message

mv→w(xpa(v)) =

f(cid:48)
α(xα)

(1)

(cid:88)

(cid:89)

xv

α∈Bv

where pa(v) is the arguments of factors in Bv not including v. The message is then placed in bucket
Bw. If w = ∅ the message is a scalar. All such messages are multiplied to form Z. The computational
cost is exponential in the scope of the largest message  which is prohibitive in most applications.

Mini-Bucket Elimination (MBE) Mini-Bucket Elimination [7] avoids the complexity of BE
by upper (or lower) bounding the message (1) as the product of terms each over a controlled
number iBound of RVs. During elimination  factors in bucket Bv are grouped into partitions
v ∈ Qv is called a mini-bucket and is associated with factors that
Qv = {q1
(collectively) use at most iBound + 1 RVs. The true message is bounded using the inequality

v . . . qk

v}  where each qj
(cid:88)

(cid:89)

xv

α∈Bv

α(xα) ≤ (cid:88)

f(cid:48)

(cid:89)

xv

α∈q1

v

α(xα) ·
f(cid:48)

f(cid:48)
α(xα)

(2)

|Qv|(cid:89)

j=2

(cid:89)

α∈qj

v

max
xv

Each message is an upper bound on the exact message  hence the full procedure yields an upper
bound on Z.

2.2 Weighted mini-bucket elimination (WMB)

WMB [10] generalizes MBE by using a tighter bound based on Holder’s inequality

w(cid:88)

f (x) =(cid:2)(cid:88)

f (x)1/w(cid:3)w

(3)

h(x)  where

x

x

x

g(x) · 1−w(cid:88)

(cid:88)
inequality(cid:80)

x

g(x) · h(x) ≤ w(cid:88)
x g(x) · h(x) ≤(cid:80)

x

is the power-sum operator and w ≥ 0  h(x) ≥ 0  g(x) ≥ 0. The power-sum reduces to standard
sum when w = 1 and approaches maxx f (x) as w → 0+. Thus  Holder’s inequality generalizes the

x g(x) · maxx h(x) used by mini-bucket elimination (MBE).

2

WMB associates a weight wq ≥ 0 with each mini-bucket q ∈ Qv where(cid:80)

then forms the bound

(cid:88)

(cid:89)

xv

α∈Bv

α(xα) ≤ (cid:89)

f(cid:48)

q∈Qv

wq(cid:88)

xv

(cid:89)

α∈q

f(cid:48)
α(xα).

q∈Qv

wr = 1 for all v 

(4)

Variational Optimization. The weights can be optimized to provide tighter bounds. Additionally 
applying WMB to any parameterization of the distribution yields a bound on Z  thus it makes sense
to optimize over all valid parameterizations as well. Each parameterization is obtained by shifting
factor potentials between mini-buckets. That is  for each v  associated with each mini-bucket q ∈ Qv
is the cost-shifting parameter φq(xv) such that

mq→q(cid:48)(xq(cid:48)) =

f φ
q (xq)

where

f(cid:48)
α(xα)

(∀q ∈ Qv)

(5)

wq(cid:88)

xv

q (xq) = φq(xv)−1(cid:89)

f φ

α∈q

is the reparameterized potential of bucket q. The cost-shifting terms that were divided out of each
q ∈ Qv are multiplied into an aggregated cost-shifting term

(cid:89)

q∈Qv

φ0

v(xv) =

φq(xv)

(6)

q (xq) · φ0
f φ

v(xv). We then have the following bound (rather than

q∈Qv

(cid:20) wv(cid:88)

(cid:89)

xv

q∈Qv

(cid:21) (cid:89)

q∈Qv

φq(xv)

mq→q(cid:48)(xq(cid:48))

(7)

α(xα) ≤
f(cid:48)

q∈Qv

(4)) on the exact BE message

such that(cid:81)

fq(xq) =(cid:81)
(cid:89)
(cid:88)
inequality we require wv +(cid:80)

α∈Bv

xv

Augmented with wv ≥ 0  this is simply another term in the product that was bounded with Holder’s
wq = 1. We search for the tightest bound by performing convex
optimization over (δ w) where log(φq) = δq for all q. Gradients can be computed and a black-box
solver can be used  or a ﬁxed point iteration [10] can be used.

q∈Bv

2.3 Symmetric models and lifted inference

Many models of interest  such as MLNs  are deﬁned by repeated potentials organized in a symmetric
structure. Lifted inference refers to a broad class of techniques that exploit this structure for exact or
approximate inference. The basic idea is to represent identical terms in the model and identical terms
generated during ground inference implicitly with a single template.
The simplest form of symmetry used for lifted inference is based on the stable vertex coloring of a
graph [1] in which two vertices of the same color have identically colored neighborhoods. In the
context of lifted inference  we require a stable coloring of the ground factor graph where factor nodes
of the same color are required to have the same ordered node neighborhood and factor potential table.
Nodes of the same color behave identically during approximate inference (e.g. [18  12]).
RV nodes of the same color are grouped together to form the index Vi ⊂ {1 . . . n} and denote
¯V = {V1 . . . VN}. Factor nodes of the same color are grouped together to form Aj ⊂ I and denote
¯I = {A1 . . . AM}. Thus  given a stable partition (coloring) we can deﬁne
Deﬁnition 2.1. The lifted scope of A ∈ ¯I (relative to ¯V) is σA = [Vb1 . . . Vbk ] where each α ∈ A
has |α| = k and αi ∈ σA
This simple symmetry will help us organize higher order symmetries throughout the LWMB algorithm.
Note  that the lifted scope (unlike the scope of a ground factor) may have repeated elements. This
occurs  for example in a complete symmetric graph  where N = 1 and σA = [V1  V1].

i for i = 1 . . . k.

3

2.3.1 Markov Logic Networks

A Markov Logic Network (MLN) [16] deﬁnes a large symmetric model implicitly via a compact ﬁrst
order logic (FOL) language. The MLN predicates deﬁnes the set of RVs  and each MLN formula
deﬁnes a set of factors with identical potential. Both are parameterized compactly by a set of logical
variables (LVs) taking values in a ﬁnite domain (for example the set of all people ∆P ).
The predicates of an MLN represent an attribute associated with domain elements or a relationship
among domain elements. The instantiation of a predicate with a domain element is the index of a
model RV. For example  the attribute predicate “Sm” (for smokes) over the domain of all people (∆P )
corresponds to the set of ground RV indices {Sm(y) | y ∈ ∆P} (meaning that Sm(Ana) indexes
xSm(Ana)). An example relationship predicate “Fr” (for friends) among all pairs of people is the set
of indices {F r(x  y) | x (cid:54)= y ∈ ∆P}.1
A formula speciﬁes a soft-logic rule applied identically to all people (or groups of people). An example
relating smoking habits between friends is “(∀ y (cid:54)= z ∈ ∆P ) F r(y  z) ∧ (Sm(y) ⇔ Sm(z))  γ”.
This corresponds to the set of R = { [F r(y  z)  Sm(y)  Sm(z)] | y (cid:54)= z ∈ ∆P} and where
each α ∈ R  fα(xα) = fR(¯xR) where fR(¯xR) is a template with log potential taking value γ if
F r(y  z) ∧ (Sm(y) ⇔ Sm(z)) is true and 0 otherwise (“Fr(y z)” corresponds to ¯x1 in the template).
The FOL expressions deﬁning formulas can be arbitrary  but they often have the form of simple
domain constraints on LVs  with all-diff constraints on LVs ranging over identical domain (note
all-diff(y z) is equivalent to y (cid:54)= z used in Fr-Smoker formula above). The stable coloring of the factor
graph groups predicate RVs together and factors associated with each formula together. Formulas of
this form also possess many higher order symmetries which we exploit later.

3 Lifted Weighted Mini-Bucket (LWMB)

This section presents a variant of WMB that operates on lifted factors  each of which is a group of
identical ground factors  and eliminates blocks of random variables simultaneously. A key difﬁculty is
choosing an approximating structure that guarantees symmetric messages (which can be represented
as a lifted factor) are produced and  furthermore  that allows forming high order (high iBound)
symmetric inference terms. We ﬁrst discuss these operations in models that possess the necessary
symmetric structure  then discuss modiﬁcations that allow us to control the size of the LWMB
graph in the presence of model asymmetries. Algorithm 1 summarizes the LWMB tree construction
algorithm (similar to ground mini-bucket construction [7]) developped in this section.

Deﬁnition 3.1. A lifted factor FG(xG) =(cid:81)

3.1 LWMB in Symmetric Models
First order symmetries speciﬁed by a stable partition of variables ¯V = {V1 . . . VN} and model factors
¯I = {A1 . . . AM} are necessary to provide a LWMB bound. We further require that the lifted scope
σA not have repeated elements (we relax both of these restrictions later). The computations for
LWMB will be described by their equivalence to a set of ground operations. To this end  we deﬁne
α∈G fG(xα) is the product of the template potential
fG(¯x) applied to all sets of ground RVs indexed by elements of G  which range over the same domain
as ¯x used to deﬁne the template.2
Blocks of ground RVs indexed by V ∈ ¯V are eliminated simultaneously  along a lifted elimination
order O. We assume the lifted scope σA of all lifted factors in the input and generated during
inference are ordered by O. The computation is organized with a set of buckets {BV1 . . . BVN}
1 = V } is the set of lifted model factors whose earliest eliminated
where initially each BV = {FA | σA
lifted RV block is V .

Lifted multiplication Having collected lifted factors in lifted buckets  an RV partition V will be
processed by ﬁrst forming lifted mini-buckets ¯QV = {q1
V } each of which groups together
and multiplies lifted factors. The lifted product corresponds to a product of ground terms and may 
in general  not have a lifted factor representation. This situation can cause symmetries to break

V . . . qk

1In general  can have > 2 LVs and > 1 domain type
2xG abuses notation  refering to xG(cid:48) where G(cid:48) = ∪α∈G α is the set of all RVs used by elements of G

4

repeat

A∈q∪q(cid:48) FA(xA)

1 = V  A ∈ ¯I(cid:48) }

lifted elimination order O(cid:48)  iBound

is valid and has template size ≤ iBound + 1

Select q  q(cid:48) ∈ QV s.t.(cid:81)

Bv ← Bv ∪ (q ∪ q(cid:48)) \ {q  q(cid:48)}
For all b  replace Mb→q or Mb→q(cid:48) with Mb→(q∪q(cid:48)).

Algorithm 1 LWMB Tree Build
1: Input: Lifted model factors ¯I(cid:48)  RV partition ¯V(cid:48) 
2: BV = { FA | σA
3: Initialize empty set of messages and empty QV ∀V ∈ V(cid:48)
4: for (V = First(O(cid:48)); ¯v (cid:54)= ∅; ¯v ← Next(V  O(cid:48))) do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end for
18: Output Mini-Buckets {QV | V ∈ ¯V(cid:48)}  and messages structure {mq→q(cid:48) | ∀q ∈ ∪V ∈ ¯V (cid:48)QV }

Set p to indices where σC (cid:54)= σC
D = {cp | c ∈ C} 
1
σD = σC
p
Add {D} to BσD

and add message pointer mq→{D}.

q = q(cid:48) = ∅

until
for q ∈ Bv do

FC =(cid:81)

(cid:46) Simulate mini-bucket message pass
(cid:46) Get C: ground regions associated with product

(cid:46) Merge MBs  delete old
(cid:46) Re-route incoming messages

(cid:46) D : scope indices of ground messages

(cid:46) Lifted multiply

end for

1

A∈q FA(xA)

(cid:89)

(cid:89)

in arbitrarily complex ways (e.g.  during lifted variable elimination; see [15] [21]). We need to
understand lifted multiplication to design LWMB bounds that avoid this situation.
To compute the lifted product FT (xT ) = FR(xR)FS(xS)  we require a symmetric join  for given
index vectors p and q  to exist. This means there exists a T where for each t ∈ T   tp ∈ R and
tq ∈ S  and furthermore that for each r ∈ R  |{t | tp = r  t ∈ T}| = |T|/|R|  meaning that each r
participates in the same number of elements of T in position p (and similarly for S). We then have

fR(xtp )|R|/|T|fS(xtq )|S|/|T|

t∈T

t∈T

fT (xt) =

FT (xT ) =

(8)
where fT (¯x) = fR(¯xp)fS(¯xq). In the simplest case  |R| = |S| = |T| and there is a one-to-one
mapping  corresponding to a series of standard ground multiplications. Otherwise  it corresponds to
“spreading” a ground factor across many identical (up to renaming of RVs) ground multiplications. If
the symmetric join does not exist  we say the lifted multiplication is invalid.
Only one set of p and q can be valid when the lifted scopes have unique elements. The lifted scope of
the multiplication (if valid) will be σT = [σR  σS]. Hence we set p and q such that σT
p = σR and
q = σS. This matches the lifted factors on ﬁrst order symmetries  which is a necessary but not
σT
sufﬁcient condition for higher order symmetries.

Symmetric join with FOL formulas
If R and S are represented with FOL formulas and each
contains only domain constraints  the symmetric join can be performed quickly (or determine none
exists). If the domain constraints between R and S are either disjoint or identical  we simply match
the two on their LVs with identical domain.
For example  multiplying factors deﬁned by {[A(x)  B(x)] | x ∈ ∆1} and {[A(x)  C(x)] | x ∈ ∆1}
produces {[A(x)  B(x)  C(x)] | x ∈ ∆1}. As another example  {[A(x)  B(x)] | x ∈ ∆1} and
{[A(x)  C(z)] | x ∈ ∆1  z ∈ ∆3} will produce {[A(x)  B(x)  C(z)] | x ∈ ∆1  z ∈ ∆3} if
∆1 ∩ ∆3 = ∅. The main algorithm in section 4 uses many symmetric lifted factors of this form.
Lifted cost-shifting For each V   each lifted mini-bucket q ∈ BV is associated with a weight Wq
and cost-shifting lifted factor Φq

V (xV ) with template φq(¯x). We form (analogous to 5)
Q (xQ) = Φq
F Φ

F (cid:48)
A(xA)

V (xV )−1(cid:89)

(9)

A∈q

where F (cid:48)
set of ground factors associated with the product of all of mini-bucket q’s lifted factors.

A represents lifted factors arising from model or a message in mini-bucket  Q represents the

5

(a) Ground graph

(b) Initial LWMB graph

(c) LWMB graph after split

Figure 1: (a) Symmetric graph with potential ¯f on each edge and a distinct unary potential at each
node  (b) LWMB graph with partition A = {a(1)  a(2)}  B = {b(1)  b(2)  b(3)} and lifted elimination
order (A  B)  (c) LWMB graph with B partitioned into B(1) = {b(1)} and B(2) = {b(2)  b(3)}. Each
RV partition is connected to its associated ground RVs with a horizontal edge through a solid square
node. Each other horizontally oriented edge (e.g. between nodes (A) and (A  B) in panel (b)) is
associated with a cost-shifting term.

Lifted message passing.
The message Q sends should be a lifted factor equal to the product of
identical messages sent from each α ∈ Q. The result will be a lifted factor over D = {α\α1 | α ∈ Q}.
Since each v ∈ V appears |Q|/|V | times in Q (due to symmetry)  each ground factor should be
eliminated with weight wq = Wq/(|V |/|Q|) yielding the template

(cid:18) wq(cid:88)

(cid:19)|Q|/|D|

q→q(cid:48)(¯x \ ¯x1) =
m(cid:48)

f Φ
Q(¯x)

(10)

The power |Q|/|D| arises since each d ∈ D receives (by symmetry) |Q|/|D| copies of the message.
The lifted factor message  denoted Mq→q(cid:48)(xD) has template m(cid:48)

q→q(cid:48) applied at all indices in D.

¯x1

3.2 Handling asymmetries

Suppose for a lifted factor FG there are K = |σG = σG

K RV’s simultaneously(cid:80)wq

The exact symmetries necessary to perform lifted inference rarely exist in practice. In the extreme
case  model asymmetries cause lifted algorithms to ground the model. Here  we extend LWMB
to handle asymmetries (i) induced by the elimination order (to prevent  for example  grounding a
complete symmetric graph)  and (ii) induced by unstructured unary evidence.
1 | > 1 copies
Sequential Asymmetries.
of the earliest variable partition in the lifted scope σG. In this case  any ground elimination order will
treat RVs in the partition differently (hence requires grounding). The way around this is to eliminate
fG(¯x)  where wq = Wq/(|V |/|Q| · K). This can be justiﬁed
by applying Holder’s inequality with any elimination order and appropriately tied weights  noting
that the power-sum with tied weights commute (details omitted for space).
Distinct Unary Evidence.
The LWMB bound can be modiﬁed to incorporate distinct single-RV
potentials. The trick (similar to [9]) is to aggregate the lifted cost-shifting terms and multiply the
result with the ground unary terms. That is  deﬁne

···(cid:80)wq

x1

xK

(cid:89)

WV(cid:88)

v∈V

xv

where φV (¯x) =(cid:81)

evidence terms.

q∈QV

mV →∅ =

fv(xv) · φV (xv)

(11)

V (¯x).3 Figure 1b illustrates a LWMB graph with aggregated approximate
φq

4 Coarse to ﬁne LWMB for MLN models

In this section we build a sequence of LWMB approximations of gradually increasing accuracy and
computational cost. Starting with a LWMB tree using the coarsest possible partition  we iteratively

3Identical potentials  such hard evidence  can be grouped into a single computation (omitted for space).

6

Algorithm 2 Coarse To Fine LWMB for MLNs
1: Initialize Choose elimination order O on MLN predicates
2: Initialize Build LWMB tree with MLN predicates and formulas
3: repeat
4:
5:
6:
7:
8:
9:
10:
11: until Exact answer computed

Associate unary evidence with lifted RVs
Optimize bound over (¯δ  W )
FQ ← F Φ
Set domain partition ∆d → {∆1
Split (¯δ  W )  ¯V  and lifted factors that use domain ∆d
Update lifted elimination order O
Build LWMB tree with new lifted MB regions and RVs as input

Q /(Mq→q(cid:48)) ∀q

d}
d  ∆2

(cid:46) To compute (11) during inference

(cid:46) See “Maintaining Monotonicity”
(cid:46) via gradient cluster or another method
(cid:46) Section 4.1

improve the approximation with Splitting and Joining operations. Splitting partitions the cost-shifting
parameters (eq. (11)) into ﬁner groupings  allowing a more ﬂexible interaction with evidence. Joining
incorporates high-order inference terms by performing LWMB Tree Build with high iBound. This
procedure is summarized in Algorithm 2 and described in this section. An example of the effect of
splitting on the LWMB graph in Figure 1b is show in Figure 1c.

4.1 Splitting

1   . . .   V (cid:48)

LWMB splitting is similar to the splitting operation presented in [9] for factor-graph models  but op-
erates by partitioning a group of MLN domain elements rather than a group of variational parameters
(as in [9]). That is  we partition a single domain ∆ ∈ ¯∆ into two disjoint domains (∆(1) ∪ ∆(2) = ∆
and ∆(1) ∩ ∆(2) = ∅). Then  we split all lifted factors and RV partitions that use M ≥ 1 LVs with
domain ∆ into 2M ﬁner lifted factors. An important property of this splitting scheme is that lifted
factors with FOL form are split into lifted factors with FOL form.
Example 1. A variable partition V = {Sm(y) | y ∈ ∆)} splits into {{Sm(y) | y ∈ ∆(i)} | i ∈
{1  2}}. A lifted factor FR with R = {[F r(y  z)  Sm(y)  Sm(z)] | ∀y (cid:54)= z ∈ ∆} splits into 4 lifted
factors FR(cid:48) where fR(cid:48) = fR  wR(cid:48) = wR and where R(cid:48) ∈ {{[F r(x  y)  Sm(x)  Sm(y)] | ∀x (cid:54)=
y  x ∈ ∆(i)  y ∈ ∆(j)} | (i  j) ∈ {1  2}2}.
Another important property is that lifted factors with M > 1 LVs with the same domain split into
lifted factors with LVs of all distinct domains. Lifted factors of this form can participate in lifted
multiplications resulting in higher order joins (section 3). For example  in Example 1 when i (cid:54)= j  the
x (cid:54)= y constraint is superﬂuous and can be dropped.
Updating the lifted elimination order.
into V (cid:48)
eliminate RVs sequentially V (cid:48)
1   . . .   V (cid:48)
example  in Figure 1c (A  B(1)  B(2)) is a valid elimination order while (B(2)  A  B(1)) is not.
Maintaining Monotonicity. Modiﬁcations to the LWMB tree (either splitting or joining) can cause
unpredictable changes in the message structure. Qualitatively  we must ensure that each new region
can send a forward message  creating new regions as necessary. Such structural modiﬁcations to
the ﬂow of messages can cause an increase in the bound. To guarantee a monotonically improving
bound  we replace each lifted factor with the cost-shifted mini-bucket functions divided by their
forward message FQ ← F Φ
Q /(Mq→q(cid:48)) (a similar idea was used in the ground case in [8]). This is
simply a reparameterization of the model  but ensures that each node in the LWMB tree sends a
uniform message of all 1’s. Hence  after split or join we can simply call LWMB Tree Build with the
reparameterized terms and guarantee a monotonic bound improvement. In practice  we apply this
technique only to nodes in the tree affected by a split or join operation  leaving the message structure
of other nodes unchanged.
Choosing the domain partition.
The goal is to cheaply ﬁnd a reasonably good split grouping. A
similar problem was considered in [9]. They compute a 2-way clustering of the gradient of their infer-
ence objective with respect to variational parameters. The main idea is if parameters are constrained
to be identical (for computational improvement with lifting) but their greedy unconstrained next step

The domain split causes an RV partition V ∈ ¯V to split
2M . Since the previous LWMB bound eliminated RVs xV simultaneously  we must
2M in the same position in O relative to other RV partitions. For

7

(a) γ = −0.5

(b) γ = −0.05

(c) γ = +0.05

Figure 2: Repulsive (γ < 0) vs. attractive (γ > 0) collective classiﬁcation example. (a)-(b)High
iBound is extremely important in the presence of strong repulsive potentials  (c) but performs slightly
worse than the baseline (“No join”) in the attractive case. Dashed black lines indicate when a batch
of splitting occurs for the blue curve (split transitions for other curves occur at similar locations).

would have been similar  then the lifting restriction incurs little error. Here  we perform a similar
operation  but a 2-way split of a domain partition ∆d induces a split of many parameters  associated
with lifted RVs that use domain ∆d  into 2 groups. Our clustering objective is a sum over squared
error of all these terms (details omitted for space).

5 Experiments

This section provides an empirical illustration of our LWMB algorithm. We demonstrate the superior-
ity of utilizing high order LWMB approximations for models with repulsive potentials. In models
with strictly attractive potentials  low order approximations work slightly better  likely due to their
ability to split more  obtaining better approximations of the evidence.
Setup. We consider a standard collective classiﬁcation MLN with formula (∀x (cid:54)= y ∈ ∆)L(x  y)∧
(C(x) ⇔ C(y))  γ. If γ < 0  a hard true observation on link L(x  y) induces a repulsive potential
between C(x) and C(y). γ > 0 induces attractive potentials. We run experiments with N =
|∆| = 512  with clustered evidence. We randomly assign elements of ∆ to one of K = 16 clusters.
Evidence on C predicate has potentials [0; a]. Each cluster generates a (scalar) center on N (0  2)
each member of the cluster is then perturbed from its center by N (0  0.4) noise. Relational evidence
on L is generated as follows: each of the K blocks has all true evidence with each other block with
probability 0.25. We then ﬂip 25% of the evidence uniformly at random.
Optimization. We call a black-box convex optimization (using non-linear conjugate gradients)
allowing a maximum of 1000 function evaluations. The gradient of the LWMB objective (derivation
omitted for space) is computable in time roughly equal to the cost of evaluating the objective.
Timing. We report only time spent doing inference (optimization)  and allow each method 250
seconds total. Inference is the algorithmic bottleneck  and code has been written in C++. The rest
of Algorithm 2 simply updates the LWMB structure (performing less work than a single inference
iteration) but is coded in MATLAB and thus yields unreliable timing. We note that other works on
lifted inference report only inference time [12  2  9]  yet incur signiﬁcant overhead of symmetry
detection (that could require touching the ground model) which we never do.
Results.
Figure 2 shows results for (a) strongly repulsive case  (b) weakly repulsive case  (c)
weakly attractive case. Strongly attractive (γ = 0.5) was qualitatively similar to (c) and omitted
for space.4 We see in (a) that lifted inference with higher order terms signiﬁcantly outperforms the
fully relaxed (“No Join”) method. In the attractive case (c) higher order performs slightly worse. We
believe this is because the cheaper inference method builds approximations of ﬁner resolution. For
panel (c)  by the end  Blue performed 31 splits  Red 60 splits  and Green 73 splits.

4Bumps in the curves are due to optimization initialization. Numerical issues arise when power-sum weights

are small. Hence at the beginning of each optimization we ﬂoor them at 10−4.

8

100101102Inference time (seconds)-1200-1000-800-600-400-2000200logZ Upper BoundiBound=5iBound=2No Join100101102Inference time (seconds)-40-20020406080100120140160logZ Upper Bound10-1100101102Inference time (seconds)361036203630364036503660367036803690logZ Upper BoundAcknowledgements

This work is sponsored in part by NSF grants IIS-1526842  IIS-1254071  by the United States Air
Force under Contract No. FA9453-16-C-0508  and DARPA Contract No. W911NF-18-C-0015.

References
[1] C. Berkholz  P. Bonsma  and M. Grohe. Tight lower and upper bounds for the complexity of canonical

colour reﬁnement. In European Symposium on Algorithms  pages 145–156. Springer  2013.

[2] G. V. d. Broeck and M. Niepert. Lifted probabilistic inference for asymmetric graphical models. arXiv

preprint arXiv:1412.0315  2014.

[3] H. B. Bui  T. N. Huynh  and R. de Salvo Braz. Exact lifted inference with distinct soft evidence on every

object. In AAAI  2012.

[4] H. H. Bui  T. N. Huynh  and S. Riedel. Automorphism groups of graphical models and lifted variational

inference. arXiv preprint arXiv:1207.4814  2012.

[5] H. H. Bui  T. N. Huynh  and D. Sontag. Lifted tree-reweighted variational inference. arXiv preprint

arXiv:1406.4200  2014.

[6] R. Dechter. Bucket elimination: A unifying framework for reasoning. Artiﬁcial Intelligence  113(1-2):41–

85  1999.

[7] R. Dechter and I. Rish. A scheme for approximating probabilistic inference. In Proc. Uncertainty in

Artiﬁcial Intelligence  pages 132–141. Morgan Kaufmann Publishers Inc.  1997.

[8] S. Forouzan and A. T. Ihler. Incremental region selection for mini-bucket elimination bounds. In UAI 

pages 268–277  2015.

[9] N. Gallo and A. Ihler. Lifted generalized dual decomposition. In AAAI. AAAI Press  2018.
[10] Q. Liu and A. T. Ihler. Bounding the partition function using holder’s inequality. In ICML-11  pages

849–856  2011.

[11] M. Mladenov  B. Ahmadi  and K. Kersting. Lifted linear programming. In AISTATS  pages 788–797  2012.
[12] M. Mladenov  A. Globerson  and K. Kersting. Lifted message passing as reparametrization of graphical

models. In UAI  pages 603–612  2014.

[13] M. Mladenov and K. Kersting. Equitable partitions of concave free energies. In UAI  pages 602–611  2015.
[14] M. Mladenov  K. Kersting  and A. Globerson. Efﬁcient lifting of MAP LP relaxations using k-locality. In

AISTATS  pages 623–632  2014.

[15] D. Poole. First-order probabilistic inference. In IJCAI  volume 3  pages 985–991  2003.
[16] M. Richardson and P. Domingos. Markov logic networks. Machine learning  62(1):107–136  2006.
[17] P. Sen  A. Deshpande  and L. Getoor. Bisimulation-based approximate lifted inference. In Proceedings of

the twenty-ﬁfth conference on uncertainty in artiﬁcial intelligence  pages 496–505. AUAI Press  2009.

[18] P. Singla and P. M. Domingos. Lifted ﬁrst-order belief propagation. In AAAI  volume 8  pages 1094–1099 

2008.

[19] P. Singla  A. Nath  and P. M. Domingos. Approximate lifting techniques for belief propagation. In AAAI 

pages 2497–2504  2014.

[20] D. Smith  P. Singla  and V. Gogate.

arXiv:1606.09637  2016.

Lifted region-based belief propagation.

arXiv preprint

[21] N. Taghipour  D. Fierens  J. Davis  and H. Blockeel. Lifted variable elimination with arbitrary constraints.

In International Conference on Artiﬁcial Intelligence and Statistics  pages 1194–1202  2012.

[22] G. Van den Broeck and A. Darwiche. On the complexity and approximation of binary evidence in lifted

inference. In Advances in Neural Information Processing Systems  pages 2868–2876  2013.

[23] D. Venugopal and V. Gogate. Evidence-based clustering for scalable inference in Markov logic. In Joint
European Conference on Machine Learning and Knowledge Discovery in Databases  pages 258–273.
Springer  2014.

[24] M. J. Wainwright  M. I. Jordan  et al. Graphical models  exponential families  and variational inference.

Foundations and Trends R(cid:13) in Machine Learning  1(1–2):1–305  2008.

9

,Nicholas Gallo
Alexander Ihler
Jean-Baptiste Alayrac
Jonathan Uesato
Po-Sen Huang
Alhussein Fawzi
Robert Stanforth
Pushmeet Kohli