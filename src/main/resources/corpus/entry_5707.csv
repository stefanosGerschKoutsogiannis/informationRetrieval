2019,Unconstrained Monotonic Neural Networks,Monotonic neural networks have recently been proposed as a way to define invertible transformations. These transformations can be combined into powerful autoregressive flows that have been shown to be universal approximators of continuous probability distributions. Architectures that ensure monotonicity typically enforce constraints on weights and activation functions  which enables invertibility but leads to a cap on the expressiveness of the resulting transformations.  In this work  we propose the Unconstrained Monotonic Neural Network (UMNN) architecture based on the insight that a function is monotonic as long as its derivative is strictly positive. In particular  this latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output.  We evaluate our new invertible building block within a new autoregressive flow (UMNN-MAF) and demonstrate its effectiveness on density estimation experiments.  We also illustrate the ability of UMNNs to improve variational inference.,Unconstrained Monotonic Neural Networks

Antoine Wehenkel
University of Liège

Gilles Louppe

University of Liège

Abstract

Monotonic neural networks have recently been proposed as a way to deﬁne in-
vertible transformations. These transformations can be combined into powerful
autoregressive ﬂows that have been shown to be universal approximators of con-
tinuous probability distributions. Architectures that ensure monotonicity typically
enforce constraints on weights and activation functions  which enables invertibil-
ity but leads to a cap on the expressiveness of the resulting transformations. In
this work  we propose the Unconstrained Monotonic Neural Network (UMNN)
architecture based on the insight that a function is monotonic as long as its deriva-
tive is strictly positive. In particular  this latter condition can be enforced with a
free-form neural network whose only constraint is the positiveness of its output.
We evaluate our new invertible building block within a new autoregressive ﬂow
(UMNN-MAF) and demonstrate its effectiveness on density estimation experi-
ments. We also illustrate the ability of UMNNs to improve variational inference.

1

Introduction

Monotonic neural networks have been known as powerful tools to build monotone models of a
response variable with respect to individual explanatory variables [Archer and Wang  1993  Sill 
1998  Daniels and Velikova  2010  Gupta et al.  2016  You et al.  2017]. Recently  strictly mono-
tonic neural networks have also been proposed as a way to deﬁne invertible transformations. These
transformations can be combined into effective autoregressive ﬂows that can be shown to be univer-
sal approximators of continuous probability distributions. Examples include Neural Autoregressive
Flows [NAF  Huang et al.  2018] and Block Neural Autoregressive Flows [B-NAF  De Cao et al. 
2019]. Architectures that ensure monotonicity typically enforce constraints on weight and activa-
tion functions  which enables invertibility but leads to a cap on the expressiveness of the resulting
transformations. For neural autoregressive ﬂows  this does not impede universal approximation but
typically requires either complex conditioners or a composition of multiple ﬂows.

Nevertheless  autoregressive ﬂows deﬁned as stacks of reversible transformations have proven
to be quite efﬁcient for density estimation of empirical distributions [Papamakarios et al.  2019 
2017  Huang et al.  2018]  as well as to improve posterior modeling in Variational Auto-Encoders
(VAE) [Germain et al.  2015  Kingma et al.  2016  Huang et al.  2018]. Practical successes of these
models include speech synthesis [van den Oord et al.  2016  Oord et al.  2018]  likelihood-free infer-
ence [Papamakarios et al.  2019]  probabilistic programming [Tran et al.  2017] and image genera-
tion [Kingma and Dhariwal  2018]. While stacking multiple reversible transformations improves the
capacity of the full transformation to represent complex probability distributions  it remains unclear
which class of reversible transformations should be used.

In this work  we propose a class of reversible transformations based on a new Unconstrained Mono-
tonic Neural Network (UMNN) architecture. We base our contribution on the insight that a function
is monotonic as long as its derivative is strictly positive. This latter condition can be enforced with
a free-form neural network whose only constraint is for its output to remain strictly positive.

We summarize our contributions as follows:

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

• We introduce the Unconstrained Monotonic Neural Network (UMNN) architecture  a new

reversible scalar transformation deﬁned via a free-form neural network.

• We combine UMNN transformations into an autoregressive ﬂow (UMNN-MAF) and we

demonstrate competitive or state-of-the-art results on benchmarks for normalizing ﬂows.

• We empirically illustrate the scalability of our approach by applying UMNN on high di-

mensional density estimation problems.

2 Unconstrained monotonic neural networks

Our primary contribution consists in a neural network architecture that enables learning arbitrary
monotonic functions. More speciﬁcally  we want to learn a strictly monotonic scalar function
F (x; ψ) : R → R without imposing strong constraints on the expressiveness of the hypothesis
class. In UMNNs  we achieve this by only imposing the derivative f (x; ψ) = ∂F (x;ψ)
to remain of
constant sign or  without loss of generality  to be strictly positive. As a result  we can parameterize
the bijective mapping F (x; ψ) via its strictly positive derivative f (x; ψ) as

∂x

f (t; ψ) dt + F (0; ψ)

 

(1)

F (x; ψ) =Z x

0

β

| {z }

where f (t; ψ) : R → R+ is a strictly positive parametric function and β ∈ R is a scalar. We make
f arbitrarily complex using an unconstrained neural network whose output is forced to be strictly
positive through an ELU activation unit increased by 1. ψ denotes the parameters of this neural
network.

Forward integration The forward evaluation of F (x; ψ) requires solving the integral in Equa-
tion (1). While this might appear daunting  such integrals can often be efﬁciently approximated nu-
merically using Clenshaw-Curtis quadrature. The better known trapezoidal rule  which corresponds
to the two-point Newton-Cotes quadrature rule  has an exponential convergence when the integrand
is periodic and the range of integration corresponds to its period. Clenshaw-Curtis quadrature takes
advantage of this property by using a change of variables followed by a cosine transform. This
extends the exponential convergence of the trapezoidal rule for periodic functions to any Lipschitz
continuous function. As a result  the number of evaluation points required to reach convergence
grows with the Lipschitz constant of the function.

Backward integration Training the integrand neural network f requires evaluating the gradient of
F with respect to its parameters. While this gradient could be obtained by backpropagating directly
through the integral solver  this would also result in a memory footprint that grows linearly with the
number of integration steps. Instead  the derivative of an integral with respect to a parameter ω can
be expressed with the Leibniz integral rule:

d

dω Z b(ω)

a(ω)

f (t; ω) dt! = f (b(ω); ω)

d
dω

b(ω) − f (a(ω); ω)

d
dω

a(ω) +Z b(ω)

a(ω)

∂
∂ω

f (t; ω) dt .

(2)

Applying Equation (2) to evaluate the derivative of Equation (1) with respect to the parameters ψ 
we ﬁnd

∇ψF (x; ψ) = f (x; ψ)∇ψ (x) − f (0; ψ)∇ψ (0) +Z x

0

∇ψf (t; ψ) dt +∇ψβ

∇ψf (t; ψ) dt +∇ψβ.

(3)

=Z x

0

When using a UMNN block in a neural architecture  it is also important to be able to compute its
derivative with respect to its input x. In this case  applying Equation (2) leads to

d
dx

F (x; ψ) = f (x; ψ).

2

(4)

Equations (3) and (4) make the memory footprint for the backward pass independent from the num-
ber of integration steps  and therefore also from the desired accuracy. Indeed  instead of computing
the gradient of the integral (which requires keeping track of all the integration steps)  we integrate the
gradient (which is memory efﬁcient  as this corresponds to summing gradients at different evaluation
points). We provide the pseudo-code of the forward and backward passes using Clenshaw-Curtis
quadrature in Appendix B.

Numerical inversion In UMMNs  the modeled monotonic function F is arbitrary. As a result 
computing its inverse cannot be done analytically. However  since F is strictly monotonic  it admits
a unique inverse x for any point y = F (x; ψ) in its image  therefore inversion can be computed
efﬁciently with common root-ﬁnding algorithms. In our experiments  search algorithms such as the
bisection method proved to be fast enough.

3 UMNN autoregressive models

3.1 Normalizing ﬂows

A Normalizing Flow [NF  Rezende and Mohamed  2015] is deﬁned as a sequence of invertible
transformations ui : Rd → Rd (i = 1  ...  k) composed together to create an expressive invertible
mapping u = u1 ◦ · · · ◦ uk : Rd → Rd. It is common for normalizing ﬂows to stack the same
parametric function ui (with different parameters values) and to reverse variables ordering after each
transformation. For this reason  we will focus on how to build one of these repeated transformations 
which we further refer to as g : Rd → Rd.

Density estimation NFs are most commonly used for density estimation  that map empirical sam-
ples to unstructured noise. Using normalizing ﬂows  we deﬁne a bijective mapping u(·; θ) : Rd →
Rd from a sample x ∈ Rd to a latent vector z ∈ Rd equipped with a density pZ(z). The transfor-
mation u implicitly deﬁnes a density p(x; θ) as given by the change of variables formula 

where Ju(x;θ) is the Jacobian of u(x; θ) with respect to x. The resulting model is trained by
maximizing the likelihood of the data {x1  ...  xN }.

p(x; θ) = pZ(u(x; θ))(cid:12)(cid:12)det Ju(x;θ)(cid:12)(cid:12)  

(5)

Variational auto-encoders NFs are also used in VAE to improve posterior modeling. In this case 
a normalizing ﬂow transforms a distribution pZ into a complex distribution q which can better model
the variational posterior. The change of variables formula yields

3.2 Autoregressive transformations

q(u(z; θ)) = pZ(z)(cid:12)(cid:12)det Ju(z;θ)(cid:12)(cid:12)

−1

.

(6)

To be of practical use  NFs must be composed of transformations for which the determinant of
the Jacobian can be computed efﬁciently  otherwise its evaluation would be running in O(d3). A
common solution consists in making the transformation g autoregressive  i.e.  such that g(x; θ) can
be rewritten as a vector of d scalar functions 

g(x; θ) =(cid:2)g1(x1; θ)

. . . xi]T

. . .

gi(x1:i; θ)

. . .

gd(x1:d; θ)(cid:3)  

where x1:i = [x1
is the vector including the i ﬁrst elements of the full vector x. The
Jacobian of this function is lower triangular  which makes the computation of its determinant O(d).
Enforcing the bijectivity of each component gi is then sufﬁcient to make g bijective as well.

For the multivariate density p(x; θ) induced by g(x; θ) and pZ(z)  we can use the chain rule to
express the joint probability of x as a product of d univariate conditional densities 

p(xi+1|x1:i; θ).

(7)

p(x; θ) = p(x1; θ)

d−1

Yi=1

3

(a) Normalizing ﬂow

x

g

g

...

g

z

(b) UMNN-MAF

transformation

(c) UMNN

x3

x

h1

h2

h3

1

g

2

g

3

g

t

h3

Z

z3

dt

Figure 1: (a) A normalizing ﬂow made of repeated UMNN-MAF transformations g with identical
architectures. (b) A UMNN-MAF which transforms a vector x ∈ R3. (c) The UMNN network used
to map x3 to z3 conditioned on the embedding h3(x1:2).

When pZ(z) is a factored distribution pZ(z) = Qd

i=1 p(zi)  we identify that each component zi
coupled with the corresponding function gi encodes for the conditional p(xi|x1:i−1; θ). Autore-
gressive transformations strongly rely on the expressiveness of the scalar functions gi. In this work 
we propose to use UMNNs to create powerful bijective scalar transformations.

3.3 UMNN autoregressive transformations (UMNN-MAF)

We now combine UMNNs with an embedding of the conditioning variables to build invertible au-
toregressive functions gi. Speciﬁcally  we deﬁne

gi(x1:i; θ) = F i(xi  hi(x1:i−1; φi); ψi)

=Z xi

0

f i(t  hi(x1:i−1; φi); ψi) dt +βi(hi(x1:i−1; φi)) 

(8)

where hi(·; φi) : Ri−1 → Rq is a q-dimensional neural embedding of the conditioning variables
x1:i−1 and β(·)i : Ri−1 → R. Both degenerate into constants for g1(x1). The parameters θ of the
whole transformation g(·; θ) is the union of all parameters φi and ψi. For simplicity we remove the
parameters of the networks by rewriting f i(·; ψi) as f i(·) and hi(·; φi) as hi(·).

In our implementation  we use a Masked Autoregressive Network [Germain et al.  2015  Kingma
et al.  2016  Papamakarios et al.  2017] to simultaneously parameterize the d embeddings. In what
follows we refer to the resulting UMNN autoregressive transformation as UMNN-MAF. Figure 1
summarizes the complete architecture.

Log-density The change of variables formula applied to the UMMN autoregressive transformation
results in the log-density

log p(x; θ) = log pZ(g(x; θ))(cid:12)(cid:12)det Jg(x;θ)(cid:12)(cid:12)
= log pZ(g(x; θ)) + log(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Yi=1
Xi=1

= log pZ(g(x; θ)) +

d

d

∂F i(xi  hi(x1:i−1))

∂xi

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(9)

log f i(xi  hi(x1:i−1)).

Therefore  the transformation leads to a simple expression of (the determinant of) its Jacobian  which
can be computed efﬁciently with a single forward pass. This is different from FFJORD [Grathwohl
et al.  2018] which relies on numerical methods to compute both the Jacobian and the transformation
between the data and the latent space. Therefore our proposed method makes the computation of
the Jacobian exact and efﬁcient at the same time.

Sampling Generating samples require evaluating the inverse transformation g−1(z; θ). The com-
ponents of the inverse vector xinv = g−1(z; θ) can be computed recursively by inverting each

4

component of g(x; θ):

xinv

xinv

1 =(cid:0)g1(cid:1)−1(cid:0)z1; h1(cid:1)
i =(cid:0)gi(cid:1)−1(cid:0)zi; hi(cid:0)xinv

1:i−1(cid:1)(cid:1)

if

if

i = 1

i > 1

(10)

(11)

where (gi)−1 is the inverse of gi. Another approach to invert an autoregressive model would be to
approximate its inverse with another autoregressive network [Oord et al.  2018]. In this case  the
evaluation of the approximated inverse model is as fast as the forward model.

Universality Since the proof is straightforward  we only sketch that UMNN-MAF is a univer-
sal density approximator of continuous random variables. We rely on the inverse sampling the-
orem to prove that UMNNs are universal approximators of continuously derivable (C1) mono-
Indeed  if UMNNs can represent any C1 monotonic function  then they can
tonic functions.
also represent the (inverse) cumulative distribution function of any continuous random variable.
Any continuously derivable function f : D → I can be expressed as the following integral:
dx is a continuous positive function and
the universal approximation theorem of NNs ensures it can be successfully approximated with a NN
of sufﬁcient capacity (such as those used in UMNNs).

f (x) = R x

∀x  a ∈ D. The derivative df

df
dx dx + f (a) 

a

4 Related work

The most similar work to UMNN-MAF are certainly Neural Autoregressive Flow [NAF  Huang
et al.  2018] and Block Neural Autoregressive Flow [B-NAF  De Cao et al.  2019]  which both rely
on strictly monotonic transformations for building bijective mappings. In NAF  transformations are
deﬁned as neural networks which activation functions are all constrained to be strictly monotonic
and which weights are the output of a strictly positive and autoregressive HyperNetwork [Ha et al. 
2017]. Huang et al. [2018] shows that NAFs are universal density approximators. In B-NAF  the
authors improve on the scalability of the NAF architecture by making use of masking operations
instead of HyperNetworks. They also present a proof of the universality of B-NAF  which extends to
UMNN-MAF. Our work differs from both NAF and B-NAF in the sense that the UMNN monotonic
transformation is based on free-form neural networks for which no constraint  beyond positiveness
of the output  is enforced on the hypothesis class. This leads to multiple advantages: it enables the
use of any state-of-the-art neural architecture  simpliﬁes weight initialization  and leads to a more
lightweight evaluation of the Jacobian.

More generally  UMNN-MAF relates to works on normalizing ﬂows built upon autoregressive net-
works and afﬁne transformations. Germain et al. [2015] ﬁrst introduced masking as an efﬁcient
way to build autoregressive networks  and proposed autoregressive networks for density estimation
of high dimensional binary data. Masked Autoregressive Flows [Papamakarios et al.  2017] and
Inverse Autoregressive Flows [Kingma et al.  2016] have generalized this approach to real data  re-
spectively for density estimation and for latent posterior representation in variational auto-encoders.
More recently  Oliva et al. [2018] proposed to stack various autoregressive architectures to create
powerful reversible transformations. Meanwhile  Jaini et al. [2019] proposed a new Sum-of-Squares
ﬂow that is deﬁned as the integral of a second order polynomial parametrized by an autoregressive
NN.

With NICE  Dinh et al. [2015] introduced coupling layers  which correspond to bijective transfor-
mations splitting the input vector into two parts. They are deﬁned as

z1:k = x1:k

and zk+1:d = eσ(x1:k) ⊙ xk+1:d + µ(x1:k) 

(12)

where σ and µ are two unconstrained functions Rd−k → Rd−k. The same authors introduced
RealNVP [Dinh et al.  2017]  which combines coupling layers with normalizing ﬂows and multi-
scale architectures for image generation. Glow [Kingma and Dhariwal  2018] extends RealNVP by
introducing invertible 1x1 convolutions between each step of the ﬂow. In this work we have used
UMNNs in the context of autoregressive architectures  however UMNNs could also be applied to
replace the linear transformation in coupling layers.

Finally  our architecture also shares a connection with Neural Ordinary Differential Equa-
tions [NODE  Chen et al.  2018]. The core idea of this architecture is to learn an ordinary dif-
ferential equation which dynamic is parameterized by a neural network. Training can be carried

5

Figure 2: Density estimation and sampling with a UMNN-MAF network on 2D toy problems. Top:
Samples from the empirical distribution p(x). Middle: Learned density p(x; θ). Bottom: Sam-
ples drawn by numerical inversion. UMNN-MAF manages to precisely capture multi-modal and/or
discontinuous distributions. Sampling is possible even if the model is not invertible analytically.

out by backpropagating efﬁciently through the ODE solver  with constant memory requirements.
Among other applications  NODE can be used to model a continuous normalizing ﬂow with a free-
form Jacobian as in FFJORD [Grathwohl et al.  2018]. Similarly  a UMNN transformation can be
seen as a structured neural ordinary differential equation in which the dynamic of the vector ﬁeld is
separable and can be solved efﬁciently by direct integration.

5 Experiments

In this section  we evaluate the expressiveness of UMNN-MAF on a variety of density estimation
benchmarks  as well as for approximate inference in variational auto-encoders. The source code to
reproduce our experiments will be made available on Github at the end of the reviewing process.

Experiments were carried out using the same integrand neural network in the UMNN component
– i.e.  in Equation 8  f i = f with shared weights ψi = ψ for i ∈ {1  . . .   d}. The functions
βi are taken to be equal to one of the outputs of the embedding network. We observed in our
experiments that sharing the same integrand function does not impact performance. Therefore  the
neural embedding function hi must produce a ﬁxed size output for i ∈ {1  . . .   d}.

5.1

2D toy problems

We ﬁrst train a UMNN-MAF on 2-dimensional toy distributions  as deﬁned by Grathwohl et al.
[2018]. To train the model  we minimize the negative log-likelihood of observed data

L(θ) = −

N

Xn=1"log pZ(g(xn; θ)) +

d

Xi=1

log f (xn

i   hi(xn

1:i−1))# .

(13)

The ﬂow used to solve these tasks is the same for all distributions and is composed of a single
transformation. More details can be found in Appendix A.1.

Figure 2 demonstrates that our model is able to learn a change of variables that warps a simple
isotropic Gaussian into multimodal and/or discontinuous distributions. We observe from the ﬁgure
that our model precisely captures the density of the data. We also observe that numerical inversion
for generating samples yields good results.

5.2 Density estimation

We further validate UMNN-MAF by comparing it to state-of-the-art normalizing ﬂows. We carry
out experiments on tabular datasets (POWER  GAS  HEPMASS  MINIBOONE  BSDS300) as well
as on MNIST. We follow the experimental protocol of Papamakarios et al. [2017]. All training
hyper-parameters and architectural details are given in Appendix A.1. For each dataset  we report

6

Table 1: Average negative log-likelihood on test data over 3 runs  error bars are equal to the standard
deviation. Results are reported in nats for tabular data and bits/dim for MNIST; lower is better. The
best performing architecture for each dataset is written in bold and the best performing architecture
per category is underlined. (a) Non-autoregressive models  (b) Autoregressive models  (c) Mono-
tonic and autoregressive models. UMNN outperforms other monotonic transformations on 4 tasks
over 6 and is the overall best performing model on 2 tasks over 6.

Dataset

POWER

GAS

HEPMASS MINIBOONE

BSDS300

MNIST

RealNVP - Dinh et al. [2017]

−0.17±.01 −8.33±.14 18.71±.02

13.55±.49 −153.28±1.78

(a)

Glow - Kingma and Dhariwal [2018] −0.17±.01 −8.15±.40 19.92±.08

11.35±.07 −155.07±.03

-

-

FFJORD - Grathwohl et al. [2018] −0.46±.01

−8.59±.12 14.92±.08
3.08±.03 −3.56±.04 20.98±.02

10.43±.04 −157.40±.19
15.59±.50 −148.85±.28 2.04±.01

-

MADE - Germain et al. [2015]

(b)

MAF - Papamakarios et al. [2017] −0.24±.01 −10.08±.02 17.70±.02

11.75±.44 −155.69±.28 1.89±.01

TAN - Oliva et al. [2018]

−0.60±.01

−12.06

±.02

13.78

±.02 11.01±.48 −159.80

±.07

1.19

NAF - Huang et al. [2018]

−0.62±.01 −11.96±.33 15.09±.40

8.86

±.15 −157.73±.30

-

-

(c)

B-NAF - De Cao et al. [2019]

SOS - Jaini et al. [2019]

UMNN-MAF (ours)

−0.61±.01 −12.06±.09 14.71±.38
−0.60±.01 −11.99±.41 15.15±.1

8.95±.07 −157.36±.03

8.90±.11 −157.48±.41

1.81

−0.63

±.01

−10.89±.7 13.99±.21

9.67±.13 −157.98±.01

1.13

±.02

results on test data for our best performing model (selected on the validation data). At testing time
we use a large number of integration steps (100) to compute the integral  this ensures its correctness
and avoids misestimating the performance of UMNN-MAF.

Table 1 summarizes our results  where we can see that on tabular datasets  our method is competitive
with other normalizing ﬂows. For POWER  our architecture slightly outperforms all others. It is
also better than other monotonic networks (category (c)) on 3 tabular datasets over 5. From these
results  we could conclude that Transformation Autoregressive Networks [TAN  Oliva et al.  2018]
is overall the best method for density estimation. It is however important to note that TAN is a ﬂow
composed of many heterogeneous transformations (both autoregressive and non-autoregressive).
For this reason  it should not be directly compared to the other models which respective results
are speciﬁc to a single architecture. However  TAN provides the interesting insight that combining
heterogeneous components into a ﬂow leads to better results than an homogeneous ﬂow.

Notably  we do not make use of a multi-scale architecture to train our model on MNIST. On this task 
UMNN-MAF slightly outperforms all other models by a reasonable margin. Samples generated
by a conditional model are shown on Figure 3  for which it is worth noting that UMNN-MAF is
the ﬁrst monotonic architecture that has been inverted to generate samples. Indeed  MNIST can be
considered as a high dimensional dataset (d = 784) for standard feed forward neural networks which
autoregressive networks are part of. NAF and B-NAF do not report any result for this benchmark 
presumably because of memory explosion.
In comparison  BSDS300  which data dimension is
one order of magnitude smaller than MNIST (63 ≪ 784)  are the largest data they have tested
on. Table 2 shows the number of parameters used by UMNN-MAF in comparison to B-NAF and
NAF. For bigger datasets  UMNN-MAF requires less parameters than NAF to reach similar or better
performance. This could explain why NAF has never been used for density estimation on MNIST.

Table 2: Comparison of the number of param-
eters between NAF  B-NAF and UMNN-MAF.
In high dimensional datasets  UMNN-MAF re-
quires fewer parameters than NAF and a similar
number to B-NAF.

Dataset

NAF B-NAF UMNN-MAF

POWER (d = 6)

4.14e5 3.07e5

5.09e5

GAS (d = 8)

4.02e5 5.44e5

8.15e5

HEPMASS (d = 21)

9.27e6 3.72e6

3.62e6

MINIBOONE (d = 43) 7.49e6 4.09e6

3.46e6

BSDS300 (d = 63)

3.68e7 8.76e6

1.56e7

Figure 3: Samples generated by numerical in-
version of a conditional UMNN-MAF trained
on MNIST. Samples z are drawn from an
isotropic Gaussian with σ = .75. See Appendix
C for more details.

7

Table 3: Average negative evidence lower bound of VAEs over 3 runs  error bars are equal to the
standard deviation. Results are reported in bits per dim for Freyfaces and in nats for the other
datasets; lower is better. UMNN-NAF is performing slightly better than IAF but is outperformed by
B-NAF. We believe that the gap in performance between B-NAF and UMNN is due to the way the
NF is conditioned by the encoder’s output.

Dataset

MNIST Freyfaces Omniglot Caltech 101

VAE - Kingma and Welling [2013]

86.65±.06 4.53±.02 104.28±.39 110.80±.46

Planar - Rezende and Mohamed [2015] 86.06±.32 4.40±.06 102.65±.42 109.66±.42

(a)

IAF - Kingma et al. [2016]

84.20±.17 4.47±.05 102.41±.04 111.58±.38

Sylvester - Berg et al. [2018]

83.32±.06 4.45±.04 99.00±.04 104.62±.29

FFJORD - Grathwohl et al. [2018]

82.82±.01 4.39±.01 98.33±.09 104.03±.43

(b)

B-NAF - De Cao et al. [2019]

83.59±.15 4.42±.05 100.08±.07 105.42±.49

UMNN-MAF (ours)

84.11±.05 4.51±.01 100.98±.13 110.45±.69

5.3 Variational auto-encoders

To assess the performance of our model  we follow the experimental setting of Berg et al. [2018] for
VAE. The encoder and the decoder architectures can be found in the appendix of their paper. In VAE
it is usual to let the encoder output the parameters of the ﬂow. For UMNN-MAF  this would cause
the encoder output’s dimension to be too large. Instead  the encoder output is passed as additional
entries of the UMNN-MAF. Like other architectures  the UMNN-MAF also takes as input a vector
of noise drawn from an isotropic Gaussian of dimension 64.

Table 3 presents our results. It shows that on MNIST and Omniglot  UMNN-MAF slightly outper-
forms the classical VAE as well as planar ﬂows. Moreover  on these datasets and Freyfaces  IAF 
B-NAF and UMNN-MAF achieve similar results. FFJORD is the best among all  however it is
worth noting that the roles of encoder outputs in FFJORD  B-NAF  IAF and Sylvester are all differ-
ent. We believe that the heterogeneity of the results could be  at least in part  due to the different
amortizations.

6 Discussion and summary

Static integral quadrature can be inaccurate. Computing the integral with static Clenshaw-
Curtis quadrature only requires the evaluation of the integrand at predeﬁned points. As such  batches
of points can be processed all at once  which makes static Clenshaw-Curtis quadrature well suited
for neural networks. However  static quadratures do not account for the error made during the
integration. As a consequence  the quadrature is inaccurate when the integrand is not smooth enough
and the number of integration steps is too small. In this work  we have reduced the integration error
by applying the normalization described by Gouk et al. [2018] in order to control the Lipschitz
constant of the integrand and appropriately set the number of integration steps. We observed that as
long as the Lipschitz constant of the network does not increase dramatically (< 1000)  a reasonable
number of integration steps (< 100) is sufﬁcient to ensure the convergence of the quadrature. An
alternative solution would be to use dynamic quadrature such as dynamic Clenshaw-Curtis.

Efﬁciency of numerical inversion. Architectures relying on linear transformations [Papamakar-
ios et al.  2017  Kingma et al.  2016  Dinh et al.  2017  Kingma and Dhariwal  2018] are trivially
exactly and efﬁciently invertible. In contrast  the UMNN transformation has no analytic inverse.
Nevertheless  it can be inverted numerically using root-ﬁnding algorithms. Since most such algo-
rithms rely on multiple nested evaluations of the function to be inverted  applying them naively to
a numerical integral would quickly become very inefﬁcient. However  the Clenshaw-Curtis quadra-
ture is part of the nested quadrature family  meaning that the evaluation of the integral at multiple
nested points can take advantage of previous evaluations and thus be implemented efﬁciently. As an
alternative  Oord et al. [2018] have shown that an invertible model can always be distilled to learn its
inverse  and thus make the inversion efﬁcient whatever the cost of inversion of the original model.

Scalability and complexity analysis. UMNN-MAF is particularly well suited for density estima-
tion because the computation of the Jacobian only requires a single forward evaluation of a NN.

8

Together with the Leibniz integral rule  they make the evaluation of the log-likelihood derivative
as memory efﬁcient as usual supervised learning  which is equivalent to a single backward pass on
the computation graph. By contrast  density estimation with previous monotonic transformations
typically requires a backward evaluation of the computation graph of the transformer NN to obtain
the Jacobian. Then  this pass must be evaluated backward again in order to obtain the log-likelihood
derivative. Both NAF and B-NAF provide a method to make this computation numerically stable 
however both fail at not increasing the size of the computation graph of the log-likelihood derivative 
hence leading to a memory overhead. The memory saved by the Leibniz rule may serve to speed
up the quadrature computation. In the case of static Clenshaw-Curtis  the function values at each
evaluation point can be computed in parallel using batch of points. In consequence  when the GPU
memory is large enough to store "meta-batches" of size d × N × B (with d the dimension of the
data  N the number of integration steps and B the batch size) the computation is approximately as
fast as a forward evaluation of the integrand network.

Summary We have introduced Unconstrained Monotonic Neural Networks  a new invertible
transformation built upon free-form neural networks allowing the use of any state-of-the-art ar-
chitecture. Monotonicity is guaranteed without imposing constraints on the expressiveness of the
hypothesis class  contrary to classical approaches. We have shown that the resulting integrated
neural network can be evaluated efﬁciently using standard quadrature rule while its inverse can be
computed using numerical algorithms. We have shown that our transformation can be composed
into an autoregressive ﬂow  with competitive or state-of-the-art results on density estimation and
variational inference benchmarks. Moreover  UMNN is the ﬁrst monotonic transformation that has
been successfully applied for density estimation on high dimensional data distributions (MNIST) 
showing better results than the classical approaches.

We identify several avenues for improvement and further research. First  we believe that numerical
integration could be fasten up during training  by leveraging the fact that controlled numerical errors
can actually help generalization. Moreover  the UMNN transformation would certainly proﬁt from
using a dynamic integration scheme  both in terms of accuracy and efﬁciency. Second  it would
be worth comparing the newly introduced monotonic transformation with common approaches for
modelling monotonic functions in machine learning. On a similar track  these common approaches
could be combined into an autoregressive ﬂow as shown in Section 3.3. Finally  our monotonic
transformation could be used within other neural architectures than generative autoregressive net-
works  such as multi-scale architectures [Dinh et al.  2017] and learnable 1D convolutions [Kingma
and Dhariwal  2018].

Acknowledgments

The authors would like to acknowledge Matthia Sabatelli  Nicolas Vecoven  Antonio Sutera and
Louis Wehenkel for useful feedback on the manuscript. They would also like to thank the anony-
mous reviewers for many relevant remarks. Antoine Wehenkel is a research fellow of the F.R.S.-
FNRS (Belgium) and acknowledges its ﬁnancial support.

9

References

N. P. Archer and S. Wang. Application of the back propagation neural network algorithm with
monotonicity constraints for two-group classiﬁcation problems. Decision Sciences  24(1):60–75 
1993.

R. v. d. Berg  L. Hasenclever  J. M. Tomczak  and M. Welling. Sylvester normalizing ﬂows for

variational inference. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2018.

T. Q. Chen  Y. Rubanova  J. Bettencourt  and D. K. Duvenaud. Neural ordinary differential equa-

tions. In Advances in Neural Information Processing Systems  pages 6571–6583  2018.

H. Daniels and M. Velikova. Monotone and partially monotone neural networks. IEEE Transactions

on Neural Networks  21(6):906–917  2010.

N. De Cao 

I. Titov  and W. Aziz.

Block neural autoregressive ﬂow.

arXiv preprint

arXiv:1904.04676  2019.

L. Dinh  D. Krueger  and Y. Bengio. Nice: Non-linear independent components estimation.

In

International Conference in Learning Representations workshop track  2015.

L. Dinh  J. Sohl-Dickstein  and S. Bengio. Density estimation using real nvp.

In International

Conference in Learning Representations  2017.

M. Germain  K. Gregor  I. Murray  and H. Larochelle. Made: Masked autoencoder for distribution

estimation. In International Conference on Machine Learning  pages 881–889  2015.

H. Gouk  E. Frank  B. Pfahringer  and M. Cree. Regularisation of neural networks by enforcing

lipschitz continuity. arXiv preprint arXiv:1804.04368  2018.

W. Grathwohl  R. T. Chen  J. Bettencourt  I. Sutskever  and D. Duvenaud. Ffjord: Free-form contin-
uous dynamics for scalable reversible generative models. In International Conference on Machine
Learning  2018.

M. Gupta  A. Cotter  J. Pfeifer  K. Voevodski  K. Canini  A. Mangylov  W. Moczydlowski  and
A. Van Esbroeck. Monotonic calibrated interpolated look-up tables. The Journal of Machine
Learning Research  17(1):3790–3836  2016.

D. Ha  A. M. Dai  and Q. V. Le. Hypernetworks.

In 5th International Conference on Learning
Representations  ICLR 2017  Toulon  France  April 24-26  2017  Conference Track Proceedings 
2017.

C.-W. Huang  D. Krueger  A. Lacoste  and A. Courville. Neural autoregressive ﬂows. In Interna-

tional Conference on Machine Learning  pages 2083–2092  2018.

P. Jaini  K. A. Selby  and Y. Yu. Sum-of-squares polynomial ﬂow. arXiv preprint arXiv:1905.02325 

2019.

D. P. Kingma and P. Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. In Advances

in Neural Information Processing Systems  pages 10236–10245  2018.

D. P. Kingma and M. Welling. Auto-encoding variational bayes. In 2nd International Conference

on Learning Representations (ICLR)  2013.

D. P. Kingma  T. Salimans  R. Jozefowicz  X. Chen  I. Sutskever  and M. Welling. Improved varia-
tional inference with inverse autoregressive ﬂow. In Advances in neural information processing
systems  pages 4743–4751  2016.

J. Oliva  A. Dubey  M. Zaheer  B. Poczos  R. Salakhutdinov  E. Xing  and J. Schneider. Trans-
In International Conference on Machine Learning  pages

formation autoregressive networks.
3895–3904  2018.

A. Oord  Y. Li  I. Babuschkin  K. Simonyan  O. Vinyals  K. Kavukcuoglu  G. Driessche  E. Lockhart 
L. Cobo  F. Stimberg  et al. Parallel wavenet: Fast high-ﬁdelity speech synthesis. In International
Conference on Machine Learning  pages 3915–3923  2018.

10

G. Papamakarios  T. Pavlakou  and I. Murray. Masked autoregressive ﬂow for density estimation.

In Advances in Neural Information Processing Systems  pages 2338–2347  2017.

G. Papamakarios  D. C. Sterratt  and I. Murray. Sequential neural likelihood: Fast likelihood-free
inference with autoregressive ﬂows. In 22nd International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS)  2019.

D. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. In International Con-

ference on Machine Learning  pages 1530–1538  2015.

J. Sill. Monotonic networks. In Advances in neural information processing systems  pages 661–667 

1998.

D. Tran  M. D. Hoffman  R. A. Saurous  E. Brevdo  K. Murphy  and D. M. Blei. Deep probabilistic

programming. In 5th International Conference on Learning Representations (ICLR)  2017.

A. van den Oord  S. Dieleman  H. Zen  K. Simonyan  O. Vinyals  A. Graves  N. Kalchbrenner 
A. Senior  and K. Kavukcuoglu. Wavenet: A generative model for raw audio. In 9th ISCA Speech
Synthesis Workshop  pages 125–125  2016.

S. You  D. Ding  K. Canini  J. Pfeifer  and M. Gupta. Deep lattice networks and partial monotonic

functions. In Advances in Neural Information Processing Systems  pages 2981–2989  2017.

11

,Antoine Wehenkel
Gilles Louppe