2016,Differential Privacy without Sensitivity,The exponential mechanism is a general method to construct a randomized estimator that satisfies $(\varepsilon  0)$-differential privacy. Recently  Wang et al. showed that the Gibbs posterior  which is a data-dependent probability distribution that contains the Bayesian posterior  is essentially equivalent to the exponential mechanism under certain boundedness conditions on the loss function. While the exponential mechanism provides a way to build an $(\varepsilon  0)$-differential private algorithm  it requires boundedness of the loss function  which is quite stringent for some learning problems. In this paper  we focus on $(\varepsilon  \delta)$-differential privacy of Gibbs posteriors with convex and Lipschitz loss functions. Our result extends the classical exponential mechanism  allowing the loss functions to have an unbounded sensitivity.,Differential Privacy without Sensitivity

Kentaro Minami

The University of Tokyo

Hiromi Arai

The University of Tokyo

kentaro minami@mist.i.u-tokyo.ac.jp

arai@dl.itc.u-tokyo.ac.jp

Issei Sato

The University of Tokyo
sato@k.u-tokyo.ac.jp

Hiroshi Nakagawa

The University of Tokyo

nakagawa@dl.itc.u-tokyo.ac.jp

Abstract

The exponential mechanism is a general method to construct a randomized estima-
tor that satisﬁes (ε  0)-differential privacy. Recently  Wang et al. showed that the
Gibbs posterior  which is a data-dependent probability distribution that contains
the Bayesian posterior  is essentially equivalent to the exponential mechanism un-
der certain boundedness conditions on the loss function. While the exponential
mechanism provides a way to build an (ε  0)-differential private algorithm  it re-
quires boundedness of the loss function  which is quite stringent for some learning
problems. In this paper  we focus on (ε  δ)-differential privacy of Gibbs posteriors
with convex and Lipschitz loss functions. Our result extends the classical expo-
nential mechanism  allowing the loss functions to have an unbounded sensitivity.

1

Introduction

Differential privacy is a notion of privacy that provides a statistical measure of privacy protection
for randomized statistics. In the ﬁeld of privacy-preserving learning  constructing estimators that
satisfy (ε  δ)-differential privacy is a fundamental problem. In recent years  differentially private
algorithms for various statistical learning problems have been developed [8  14  3].
Usually  the estimator construction procedure in statistical learning contains the following mini-
mization problem of a data-dependent function. Given a dataset Dn = {x1  . . .   xn}  a statistician
chooses a parameter θ that minimizes a cost function L(θ  Dn). A typical example of cost function
is the empirical risk function  that is  a sum of loss function (cid:96)(θ  xi) evaluated at each sample point
xi ∈ Dn. For example  the maximum likelihood estimator (MLE) is given by the minimizer of
empirical risk with loss function (cid:96)(θ  x) = − log p(x | θ).
To achieve a differentially private estimator  one natural idea is to construct an algorithm based on a
posterior sampling  namely drawing a sample from a certain data-dependent probability distribution.
The exponential mechanism [16]  which can be regarded as a posterior sampling  provides a general
method to construct a randomized estimator that satisﬁes (ε  0)-differential privacy. The probabil-
ity density of the output of the exponential mechanism is proportional to exp(−βL(θ  Dn))π(θ) 
where π(θ) is an arbitrary prior density function  and β > 0 is a parameter that controls the
degree of concentration. The resulting distribution is highly concentrated around the minimizer
θ∗
∈ argminθ L(θ  Dn). Note that most differential private algorithms involve a procedure to add
some noise (e.g. the Laplace mechanism [12]  objective perturbation [8  14]  and gradient perturba-
tion [3])  while the posterior sampling explicitly designs the density of the output distribution.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Figure 1: An example of a logistic loss function (cid:96)(θ  x) := log(1 + exp(−yθ(cid:62)z)). Considering two
points x± = (z ±1)  the difference of the loss |(cid:96)(θ  x+) − (cid:96)(θ  x−)| increases proportionally to the
size of the parameter space (solid lines). In this case  the value of the β in the exponential mecha-
nism  which is inversely proportional to the maximum difference of the loss function  becomes very
small. On the other hand  the difference of the gradient |∇(cid:96)(θ  x+) − ∇(cid:96)(θ  x−)| does not exceed
twice of the Lipschitz constant (dashed lines). Hence  our analysis based on Lipschitz property does
not be inﬂuenced by the size of the parameter space.

Table 1: Regularity conditions for (ε  δ)-differential privacy of the Gibbs posterior. Instead of the
boundedness of the loss function  our analysis in Theorem 7 requires its Lipschitz property and
convexity. Unlike the classical exponential mechanism  our result explains “shrinkage effect” or
“contraction effect”  namely  the upper bound for β depends on the concavity of the prior π and the
size of the dataset n.

Exponential
mechanism [16]
Theorem 7
Theorem 10

Loss function (cid:96)

(ε  δ)
δ = 0 Bounded sensitivity

Prior π
Arbitrary

δ > 0 Lipschitz and convex
Lipschitz
δ > 0 Bounded 

and strongly convex

Log-concave
Log-concave

Shrinkage

No

Yes
Yes

We deﬁne the density of the Gibbs posterior distribution as

exp(−β(cid:80)n
(cid:82) exp(−β(cid:80)n

Gβ(θ | Dn) :=

i=1 (cid:96)(θ  xi))π(θ)
i=1 (cid:96)(θ  xi))π(θ)dθ

.

(1)

The Gibbs posterior plays important roles in several learning problems  especially in PAC-Bayesian
learning theory [6  21]. In the context of differential privacy  Wang et al. [20] recently pointed out
that the Bayesian posterior  which is a special version of (1) with β = 1 and a speciﬁc loss function 
satisﬁes (ε  0)-differential privacy because it is equivalent to the exponential mechanism under a
certain regularity condition. Bassily  et al. [3] studied an application of the exponential mechanism
to private convex optimization.
In this paper  we study the (ε  δ)-differential privacy of the posterior sampling with δ > 0.
particular  we consider the following statement.
Claim 1. Under a suitable condition on loss function (cid:96) and prior π  there exists an upper bound
B(ε  δ) > 0  and the Gibbs posterior Gβ(θ | Dn) with β ≤ B(ε  δ) satisﬁes (ε  δ)-differential
privacy. The value of B(ε  δ) does not depend on the boundedness of the loss function.

In

2

θLoss‘(θ x−)‘(θ x+)θDifferenceθLossgradient|∇‘(θ x−)||∇‘(θ x+)|We point out here the analyses of (ε  0)-differential privacy and (ε  δ)-differential privacy with δ > 0
are conceptually different in the regularity conditions they require. On one hand  the exponential
mechanism essentially requires the boundedness of the loss function to satisfy (ε  0)-differential
privacy. On the other hand  the boundedness is not a necessary condition in (ε  δ)-differential pri-
vacy. In this paper  we give a new sufﬁcient condition for (ε  δ)-differential privacy based on the
convexity and the Lipschitz property. Our analysis widens the application ranges of the exponential
mechanism in the following aspects (See also Table 1).

• (Removal of boundedness assumption) If the loss function is unbounded  which is usually
the case when the parameter space is unbounded  the Gibbs posterior does not satisfy (ε  0)-
differential privacy in general. Still  in some cases  we can build an (ε  δ)-differential
private estimator.

• (Tighter evaluation of β) Even when the difference of the loss function is bounded  our
analysis can yield a better scheme in determining the appropriate value of β for a given
privacy level. Figure 1 shows an example of logistic loss.

• (Shrinkage and contraction effect) Intuitively speaking  the Gibbs posterior becomes robust
against a small change of the dataset  if the prior π has a strong shrinkage effect (e.g. a
Gaussian prior with a small variance)  or if the size of the dataset n tends to inﬁnity. In
our analysis  the upper bound of β depends on π and n  which explains such shrinkage and
contraction effects.

1.1 Related work

(ε  δ)-differential privacy of Gibbs posteriors has been studied by several authors. Mir ([18]  Chapter
5) proved that a Gaussian posterior in a speciﬁc problem satisﬁes (ε  δ)-differential privacy. Dim-
itrakakis et al. [10] considered Lipschitz-type sufﬁcient conditions  yet their result requires some
modiﬁcation of the deﬁnition of the neighborhood on the database.
In general  the utility of sensitivity-based methods suffers from the size of the parameter space
Θ. Thus  getting around the dependency on the size of Θ is a fundamental problem in the study
of differential privacy. For discrete parameter spaces  a general range-independent algorithm for
(ε  δ)-differential private maximization was developed in [7].

1.2 Notations
+(Θ). A map
The set of all probability measures on a measurable space (Θ T ) is denoted by M1
between two metric spaces f : (X  dX ) → (Y  dY ) is said to be L-Lipschitz  if dY (f (x1)  f (x2)) ≤
LdX (x1  x2) holds for all x1  x2 ∈ X. Let f be a twice continuously differentiable function f
deﬁned on a subset of Rd. f is said to be m(> 0)-strongly convex  if the eigenvalues of its Hessian
∇2f are bounded by m from below. f is said to be M-smooth 
2 Differential privacy with sensitivity

In this section  we review the deﬁnition of (ε  δ)-differential privacy and the exponential mechanism.

2.1 Differential privacy

Differential privacy is a notion of privacy that provides a degree of privacy protection in a statistical
sense. More precisely  differential privacy deﬁnes a closeness between any two output distributions
that correspond to adjacent datasets.
In this paper  we assume that a dataset D = Dn = (x1  . . .   xn) is a vector that consists of n points
in abstract attribute space X   where each entry xi ∈ X represents information contributed by one
individual. Two datasets D  D(cid:48) are said to be adjacent if dH (D  D(cid:48)) = 1  where dH is the Hamming
distance deﬁned on the space of all possible datasets X d.
We describe the deﬁnition of differential privacy in terms of randomized estimators. A randomized
estimator is a map ρ : X n → M1
+(Θ) from the space of datasets to the space of probability
measures.

3

Deﬁnition 2 (Differential privacy). Let ε > 0 and δ ≥ 0 be given privacy parameters. We say that
a randomized estimator ρ : X n → M1
+(Θ) satisﬁes (ε  δ)-differential privacy  if for any adjacent
datasets D  D(cid:48)

∈ X n  an inequality

ρD(A) ≤ eερD(cid:48)(A) + δ

holds for every measurable set A ⊂ Θ.
2.2 The exponential mechanism

(2)

(3)

The exponential mechanism [16] is a general construction of (ε  0)-differentially private distribu-
tions. For an arbitrary function L : Θ × X n → R  we deﬁne the sensitivity by

∆L :=

sup

D D(cid:48)∈X n:
dH (D D(cid:48))=1

sup

θ∈Θ|L(θ  D) − L(θ  D(cid:48))| 

We consider the particular case that

the cost function is given as sum form L(θ  Dn) =
i=1 (cid:96)(θ  xi). Recently  Wang et al. [20] examined two typical cases in which ∆L is ﬁnite. The

which is the largest possible difference of two adjacent functions f (·  D) and f (·  D(cid:48)) with respect
to supremum norm.
Theorem 3 (McSherry and Talwar). Suppose that the sensitivity of the function L(θ  Dn) is ﬁnite.
Let π be an arbitrary base measure on Θ. Take a positive number β so that β ≤ ε/2∆L. Then a
probability distribution whose density with respect to π is proportional to exp(−βL(θ  Dn)) satisﬁes
(ε  0)-differential privacy.
(cid:80)n
following statement slightly generalizes their result.
Theorem 4 (Wang  et al.). (a) Suppose that the loss function (cid:96) is bounded by A  namely |(cid:96)(θ  x)| ≤
A holds for all x ∈ X and θ ∈ Θ. Then ∆L ≤ 2A  and the Gibbs posterior (1) satisﬁes (4βA  0)-
differential privacy.
(b) Suppose that for any ﬁxed θ ∈ Θ  the difference |(cid:96)(θ  x1) − (cid:96)(θ  x2)| is bounded by L for all
x1  x2 ∈ X . Then ∆L ≤ L  and the Gibbs posterior (1) satisﬁes (2βL  0)-differential privacy.
The condition ∆L < ∞ is crucial for Theorem 3 and cannot be removed. However  in practice 
statistical models of interest do not necessarily satisfy such boundedness conditions. Here we have
two simple examples of Gaussian and Bernoulli mean estimation problems  in which the sensitivities
are unbounded.

• (Bernoulli mean) Let (cid:96)(p  x) = −x log p−(1−x) log(1−p) (p ∈ (0  1)  x ∈ {0  1}) be the
negative log-likelihood of the Bernoulli distribution. Then |(cid:96)(p  0)− (cid:96)(p  1)| is unbounded.
2 (θ − x)2 (θ ∈ R  x ∈ R) be the negative log-likelihood
• (Gaussian mean) Let (cid:96)(θ  x) = 1
of the Gaussian distribution with a unit variance. Then |(cid:96)(θ  x) − (cid:96)(θ  x(cid:48))| is unbounded if
x (cid:54)= x(cid:48).

Thus  in the next section  we will consider an alternative proof technique for (ε  δ)-differential pri-
vacy so that it does not require such boundedness conditions.

3 Differential privacy without sensitivity

In this section  we state our main results for (ε  δ)-differential privacy in the form of Claim 1.
There is a well-known sufﬁcient condition for the (ε  δ)-differential privacy:
Theorem 5 (See for example Lemma 2 of [13]). Let ε > 0 and δ > 0 be privacy parameters.
Suppose that a randomized estimator ρ : X n → M1
+(Θ) satisﬁes a tail-bound inequality of log-
density ratio

(cid:26)

(cid:27)

for every adjacent pair of datasets D  D(cid:48). Then ρ satisﬁes (ε  δ)-differential privacy.

ρD

log

dρD
dρD(cid:48) ≥ ε

≤ δ

(4)

4

(cid:27)

(cid:26)

To control the tail behavior (4) of the log-density ratio function log dρD
dρD(cid:48)   we consider the concen-
tration around its expectation. Roughly speaking  inequality (4) holds if there exists an increasing
function α(t) that satisﬁes an inequality

ρD

log

dρD
dρD(cid:48) ≥ DKL(ρD  ρD(cid:48)) + t

≤ exp(−α(t)) 

∀t > 0 
is the log-density ratio function  and DKL(ρD  ρD(cid:48)) := EρD log dρD
dρD(cid:48)

where log dGβ D
is the
dGβ D(cid:48)
Kullback-Leibler (KL) divergence. Suppose that the Gibbs posterior Gβ D  whose density G(θ | D)
is deﬁned by (1)  satisﬁes an inequality (5) for a certain α(t) = α(t  β). Then Gβ D satisﬁes (4) if
there exist β  t > 0 that satisfy the following two conditions.
1. KL-divergence bound: DKL(Gβ D  Gβ D(cid:48)) + t ≤ ε
2. Tail-probability bound: exp(−α(t  β)) ≤ δ

(5)

3.1 Convex and Lipschitz loss

Here  we examine the case in which the loss function (cid:96) is Lipschitz and convex  and the parameter
space Θ is the entire Euclidean space Rd. Due to the unboundedness of the domain  the sensitivity
∆L can be inﬁnite  in which case the exponential mechanism cannot be applied.
Assumption 6. (i) Θ = Rd.
(ii) For any x ∈ X   (cid:96)(·  x) is non-negative  L-Lipschitz and convex.
(iii) − log π(·) is twice differentiable and mπ-strongly convex.
In Assumption 6  the loss function (cid:96)(·  x) and the difference |(cid:96)(·  x1) − (cid:96)(·  x2)| can be unbounded.
Thus  the classical argument of the exponential mechanism in Section 2.2 cannot be applied. Nev-
ertheless  our analysis shows that the Gibbs posterior satisﬁes (ε  δ)-differential privacy.
Theorem 7. Let β ∈ (0  1] be a ﬁxed parameter  and D  D(cid:48)
Under Assumption 6  inequality

∈ X n be an adjacent pair of datasets.
(cid:18)

(cid:19)2(cid:33)

(cid:32)

(cid:27)

(cid:26)

dGβ D
dGβ D(cid:48) ≥ ε

≤ exp

−

mπ

8L2β2

2L2β2

mπ

ε −

(6)

Gβ D

log

holds for any ε > 2L2β2
mπ

.

Gibbs posterior Gβ D satisﬁes (ε  δ)-differential privacy if β > 0 is taken so that the right-hand side
of (6) is bounded by δ. It is elementary to check the following statement:
Corollary 8. Let ε > 0 and 0 < δ < 1 be privacy parameters. Taking β so that it satisﬁes

(cid:114)

ε
2L

β ≤

mπ

1 + 2 log(1/δ)

 

(7)

Gibbs posterior Gβ D satisﬁes (ε  δ)-differential privacy.

Note that the right-hand side of (6) depends on the strong concavity mπ. The strong concavity
parameter corresponds to the precision (i.e.
inverse variance) of the Gaussian  and a distribution
with large mπ becomes spiky. Intuitively  if we use a prior that has a strong shrinkage effect  then
the posterior becomes robust against a small change of the dataset  and consequently the differential
privacy can be satisﬁed with a little effort. This observation is justiﬁed in the following sense: the
upper bound of β grows proportionally to √mπ. In contrast  the classical exponential mechanism
does not have that kind of prior-dependency.

3.2 Strongly convex loss
Let ˜(cid:96) be a strongly convex function deﬁned on the entire Euclidean space Rd. If (cid:96) is a restriction
of ˜(cid:96) to a compact L2-ball  the Gibbs posterior can satisfy (ε  0)-differential privacy with a certain
privacy level ε > 0 because of the boundedness of (cid:96). However  using the boundedness of ∇(cid:96) rather
than that of (cid:96) itself  we can give another guarantee for (ε  δ)-differential privacy.

5

Assumption 9. Suppose that a function ˜(cid:96) : Rd × X → R is a twice differentiable and m(cid:96)-strongly
convex with respect to its ﬁrst argument. Let ˜π be a ﬁnite measure over Rd that − log ˜π(·) is twice
differentiable and mπ-strongly convex. Let ˜Gβ D is a Gibbs posterior on Rd whose density with
˜(cid:96)(θ  xi))˜π(θ). Assume that the mean

respect to the Lebesgue measure is proportional to exp(−β(cid:80)
(cid:13)(cid:13)(cid:13)2 ≤ κ.
(i) Θ is a compact L2-ball centered at the origin  and its radius RΘ satisﬁes RΘ ≤ κ + α(cid:112)d/mπ.

of ˜Gβ D is contained in a L2-ball of radius κ:
∀D ∈ X n 

Deﬁne a positive number α > 1. Assume that (Θ  (cid:96)  π) satisﬁes the following conditions.

(cid:13)(cid:13)(cid:13)E ˜Gβ D

(8)

[θ]

i

In other words  L :=

(ii) For any x ∈ X  
supx∈X supθ∈Θ (cid:107)∇θ(cid:96)(θ  x)(cid:107)2 is bounded.
(iii) π is given by a restriction of ˜π to Θ.

(cid:96)(·  x) is L-Lipschitz  and convex.

The following statements are the counterparts of Theorem 7 and its corollary.
Theorem 10. Let β ∈ (0  1] be a ﬁxed parameter  and D  D(cid:48)
Under Assumption 9  inequality

(cid:26)

(cid:27)

(cid:32)

Gβ D

log

dGβ D
dGβ D(cid:48) ≥ ε

≤ exp

−

nm(cid:96)β + mπ

4C(cid:48)β2

ε −

nm(cid:96)β + mπ

∈ X n be an adjacent pair of datasets.
(cid:18)

(cid:19)2(cid:33)

C(cid:48)β2

(9)

nm(cid:96)β+mπ

holds for any ε > C(cid:48)β2
. Here  we deﬁned C(cid:48) := 2CL2(1 + log(α2/(α2 − 1)))  where C > 0
is a universal constant that does not depend on any other quantities.
Corollary 11. Under Assumption 9  there exists an upper bound B(ε  δ) = B(ε  δ  n  m(cid:96)  mπ  α) >
0  and Gβ(θ | Dn) with β ≤ B(ε  δ) satisﬁes (ε  δ)-differential privacy.
Similar to Corollary 8  the upper bound on β depends on the prior. Moreover  the right-hand side of
(9) decreases to 0 as the size of dataset n increases  which implies that (ε  δ)-differential privacy is
satisﬁed almost for free if the size of the dataset is large.

3.3 Example: Logistic regression

In this section  we provide an application of Theorem 7 to the problem of linear binary classiﬁcation.
Let Z := {z ∈ Rd (cid:107)z(cid:107)2 ≤ r} be a space of the input variables. The space of the observation is the
set of input variables equipped with binary label X := {x = (z  y) ∈ Z ×{−1  +1}}. The problem
is to determine a parameter θ = (a  b) of linear classiﬁer fθ(z) = sgn(a(cid:62)z + b).
Deﬁne a loss function (cid:96)LR by

The (cid:96)2-regularized logistic regression estimator is given by

(cid:96)LR(θ  x) := log(1 + exp(−y(a(cid:62)z + b))).
(cid:41)

(cid:40)

ˆθLR = argmin
θ∈Rd+1

(cid:96)LR(θ  xi) +

λ
2(cid:107)θ(cid:107)2

2

n(cid:88)

i=1

1
n

(10)

(11)

 

where λ > 0 is a regularization parameter. Corresponding Gibbs posterior has a density

Gβ(θ | D) ∝

σ(yi(a(cid:62)zi + b))βφd+1(θ | 0  (nλ)−1I) 

(12)
where σ(u) = (1 + exp(−u))−1 is a sigmoid function  and φd+1(θ | µ  Σ) is a density of (d + 1)-
dimensional Gaussian distribution. It is easy to check that (cid:96)LR(· x) is r-Lipschitz and convex  and
− log φd+1(· | 0  (nλ−1)I) is (nλ)-strongly convex. Hence  by Corollary 8  the Gibbs posterior
satisﬁes (ε  δ)-differential privacy if

i=1

n(cid:89)

(cid:115)

ε
2r

β ≤

nλ

1 + 2 log(1/δ)

.

6

(13)

4 Approximation Arguments

In practice  exact samplers of Gibbs posteriors (1) are rarely available. Actual implementations
involve some approximation processes. Markov Chain Monte Carlo (MCMC) methods and Varia-
tional Bayes (VB) [1] are commonly used to obtain approximate samplers of Gibbs posteriors. The
next proposition  which is easily obtained as a variant of Proposition 3 of [20]  gives a differential
privacy guarantee under approximation.
Proposition 12. Let ρ : X n → M1
privacy. If for all D  there exist approximate sampling procedure ρ(cid:48)
then the randomized mechanism D (cid:55)→ ρ(cid:48)
dTV(µ  ν) = supA∈T |µ(A) − ν(A)| is the total variation distance.
We now describe a concrete example of MCMC  the Langevin Monte Carlo (LMC). Let θ(0) ∈ Rd
be an initial point of the Markov chain. The LMC algorithm for Gibbs posterior Gβ D contains the
following iterations:

+(Θ) be a randomized estimator that satisﬁes (ε  δ)-differential
D) ≤ γ 
D satisﬁes (εδ + (1 + eε)γ)-differential privacy. Here 

D such that dTV(ρD  ρ(cid:48)

θ(t+1) = θ(t) − h∇U (θ(t)) + √2hη(t+1)

U (θ) = β

(cid:96)(θ  xi) − log π(θ).

n(cid:88)

i=1

(14)

(15)

Here η(1)  η(2)  . . . ∈ Rd are noise vectors independently drawn from a centered Gaussian N (0  I).
This algorithm can be regarded as a discretization of a stochastic differential equation that has a
stationary distribution Gβ D  and its convergence property has been studied in ﬁnite-time sense
[9  5  11]. Let us denote by ρ(t) the law of θ(t). If dTV(ρ(t)  Gβ D) ≤ γ holds for all t ≥ T   then
the privacy of the LMC sampler is obtained from Proposition 12. In fact  we can prove by Corollary
1 of [9] the following proposition.
Proposition 13. Assume that Assumption 6 holds. Let (cid:96)(θ  x) be M(cid:96)-smooth for all x ∈ X   and
− log π(θ) be Mπ-smooth. Let d ≥ 2 and γ ∈ (0  1/2). We can choose β > 0  by Corollary 8  so
that Gβ D satisﬁes (ε  δ)-differential privacy. Let us set step size h of the LMC algorithm (14) as

h =

and set T as

d(nβM(cid:96) + Mπ)2

4 log

+ d log

(cid:104)

(cid:20)

2mπγ2

(cid:17)

(cid:16) 1
(cid:19)
(cid:18) 1

γ

γ

(cid:17)(cid:105)  
(cid:16) nβM(cid:96)+Mπ
(cid:19)(cid:21)2
(cid:18) nβM(cid:96) + Mπ

mπ

mπ

(16)

.

(17)

T =

d(nβM(cid:96) + Mπ)2

4mπγ2

4 log

+ d log

Then  after T iterations of (14)  θ(T ) satisﬁes (ε  δ + (1 + eε)γ)-differential privacy.

The algorithm suggested in Proposition 13 is closely related to the differentially private stochastic
gradient Langevin dynamics (DP-SGLD) proposed by Wang  et al. [20]. Ignoring the computational
cost  we can take the approximation error level γ > 0 arbitrarily small  while the convergence
property to the target posterior distribution is not necessarily ensured about DP-SGLD.

5 Proofs

In this section  we give a formal proof of Theorem 7 and a proof sketch of 10.
There is a vast literature on techniques to obtain a concentration inequality in (5) (see  for example 
[4]). Logarithmic Sobolev inequality (LSI) is a useful tool for this purpose. We say that a probability
measure µ over Θ ⊂ Rd satisﬁes LSI with constant DLS if inequality
(cid:19)

(18)
holds for any integrable function f  provided the expectations in the expression are deﬁned. It is
known that [15  4]  if µ satisﬁes LSI  then every real-valued L-Lipschitz function F behaves in a
sub-Gaussian manner:

Eµ[f 2 log f 2] − Eµ[f 2] log Eµ[f 2] ≤ 2DLSEµ (cid:107)∇f(cid:107)2

(cid:18)

2

µ{F ≥ Eµ[F ] + t} ≤ exp

t2

−

2L2DLS

.

(19)

7

In our analysis  we utilize the LSI technique for the following two reasons: (a) a sub-Gaussian tail
bound of the log-density ratio is obtained from (19)  and (b) an upper bound on the KL-divergence
is directly obtained from LSI  which appears to be difﬁcult to prove by any other argument.
Roughly speaking  LSI holds if the logarithm of the density is strongly concave. In particular  for a
Gibbs measure on Rd  the following fact is known.
Lemma 14 ([15]). Let U : Rd → R be a twice differential  m-strongly convex and integrable
function. Let µ be a probability measure on Rd whose density is proportional to exp(−U ). Then µ
satisﬁes LSI (18) with constant DLS = m−1.
In this context  the strong convexity of U is related to the curvature-dimension condition CD(m ∞) 
which can be used to prove LSI on general Riemannian manifolds [19  2].

Proof of Theorem 7. For simplicity  we assume that (cid:96)(·  x) (∀x ∈ X ) is twice differentiable. For
general Lipschitz and convex loss functions (e.g. hinge loss)  the theorem can be proved using a
i (cid:96)(·  xi) − log π(·) is mπ-strongly convex  Gibbs posterior
Gβ D satisﬁes LSI with constant m−1
π .
Let D  D(cid:48)
∈ X n be a pair of adjacent datasets. Considering appropriate permutation of the elements 
we can assume that D = (x1  . . .   xn) and D(cid:48) = (x(cid:48)
n) differ in the ﬁrst element  namely 
x1 (cid:54)= x(cid:48)

molliﬁer argument. Since U (·) = β(cid:80)
(cid:13)(cid:13)(cid:13)(cid:13)2

i (i = 2  . . .   n). By the assumption that (cid:96)(·  x) is L-Lipschitz  we have

= β(cid:107)∇((cid:96)(θ  x1) − (cid:96)(θ  x(cid:48)

1 and xi = x(cid:48)

1))(cid:107)2 ≤ 2βL 

dGβ D
dGβ D(cid:48)

1  . . .   x(cid:48)

(20)

dGβ D(cid:48) is 2βL-Lipschitz. Then  by concentration inequality for Lipschitz

and log-density ratio log dGβ D
function (19)  we have

(cid:27)

(cid:18)

(cid:19)

(cid:13)(cid:13)(cid:13)(cid:13)∇ log
(cid:26)

∀t > 0  Gβ D

log

dGβ D
dGβ D(cid:48) ≥ DKL(Gβ D  Gβ D(cid:48)) + t

≤ exp

−

mπt2
8L2β2

(21)

We will show an upper bound of the KL-divergence. To simplify the notation  we will write F :=
dGβ D
dGβ D(cid:48) . Noting that

and that

√F(cid:107)2
2 = (cid:107)∇ exp(2−1 log F )(cid:107)2
2 = (cid:107)
(cid:107)∇
DKL(Gβ D  Gβ D(cid:48)) = EGβ D [log F ]

√F
2 ∇ log F(cid:107)2

2 ≤

F
4 · (2βL)2

(22)

(23)

(24)

(25)

(cid:27)

we have  from LSI (18) with f = √F  
2
mπ

DKL(Gβ D  Gβ D(cid:48)) ≤
Combining (21) and (24)  we have

(cid:27)

(cid:26)

Gβ D

log

dGβ D
dGβ D(cid:48) ≥ ε

≤ Gβ D

EGβ D(cid:48)(cid:107)∇
(cid:26)
(cid:32)

log

≤ exp

−

for any ε > 2L2β2
mπ

.

= EGβ D(cid:48) [F log F ] − EGβ D(cid:48) [F ]EGβ D(cid:48) [log F ] 

√F(cid:107)2
2 ≤

2L2β2

mπ

EGβ D(cid:48) [F ] =

2L2β2

mπ

.

dGβ D
dGβ D(cid:48) ≥ ε + DKL(Gβ D  Gβ D(cid:48)) −
mπ

(cid:19)2(cid:33)

2L2β2

(cid:18)

2L2β2

mπ

8L2β2

ε −

mπ

Proof sketch for Theorem 10. The proof is almost the same as that of Theorem 7. It is sufﬁcient to
show that the set of Gibbs posteriors {Gβ D  D ∈ X n} simultaneously satisﬁes LSI with the same
constant. Since the logarithm of the density is m := (nm(cid:96)β + mπ)-strongly convex  a probability
measure ˜Gβ D satisﬁes LSI with constant m−1. By the Poincar´e inequality for ˜Gβ D  the variance
of (cid:107)θ(cid:107)2 is bounded by d/m ≤ d/mπ. By the Chebyshev inequality  we can check that the mass
of parameter space is lower-bounded as ˜Gβ D(Θ) ≥ p := 1 − α−2. Then  by Corollary 3.9 of
[17]  Gβ D := ˜Gβ D|Θ satisﬁes LSI with constant C(1 + log p−1)m−1  where C > 0 is a universal
numeric constant.

8

Acknowledgments

This work was supported by JSPS KAKENHI Grant Number JP15H02700.

References
[1] P. Alquier  J. Ridgway  and N. Chopin. On the properties of variational approximations of

Gibbs posteriors  2015. Available at http://arxiv.org/abs/1506.04091.

[2] D. Bakry  I. Gentil  and M. Ledoux. Analysis and Geometry of Markov Diffusion Operators.

Springer  2014.

[3] R. Bassily  A. Smith  and A. Thakurta. Differentially private empirical risk minimization:

Efﬁcient algorithms and tight error bounds. In FOCS  2014.

[4] S. Boucheron  G. Lugosi  and P. Massart. Concentration Inequalities: A Nonasymptotic Theory

of Independence. Oxford University Press  2013.

[5] S. Bubeck  R. Eldan  and J. Lehec. Finite-time analysis of projected Langevin Monte Carlo.

In NIPS  2015.

[6] O. Catoni. Pac-Bayesian Supervised Classiﬁcation: The Thermodynamics of Statistical Learn-

ing. IMS  2007.

[7] K. Chaudhuri  D. Hsu  and S. Song. The large margin mechanism for differentially private

maximization. In NIPS  2014.

[8] K. Chaudhuri  C. Monteleoni  and A.D. Sarwate. Differentially private empirical risk mini-

mization. Journal of Machine Learning Research  12:1069–1109  2011.

[9] A. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave

densities  2014. Available at http://arxiv.org/abs/1412.7392.

[10] C. Dimitrakakis  B. Nelson  and B. Rubinstein. Robust and private Bayesian inference. In

Algorithmic Learning Theory  2014.

[11] A. Durmus and E. Moulines. Non-asymptotic convergence analysis for the unadjusted langevin

algorithm  2015. Available at http://arxiv.org/abs/1507.05021.

[12] C. Dwork. Differential privacy. In ICALP  pages 1–12  2006.

[13] R. Hall  A. Rinaldo  and L. Wasserman. Differential privacy for functions and functional data.

Journal of Machine Learning Research  14:703–727  2013.

[14] D. Kifer  A. Smith  and A. Thakurta. Private convex empirical risk minimization and high-

dimensional regression. In COLT  2012.

[15] M. Ledoux. Concentration of Measure and Logarithmic Sobolev Inequalities  volume 1709 of

S´eminaire de Probabilit´es XXXIII Lecture Notes in Mathematics. Springer  1999.

[16] F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS  2007.

[17] E. Milman. Properties of isoperimetric  functional and Transport-Entropy inequalities via con-

centration. Probability Theory and Related Fields  152:475–507  2012.

[18] D. Mir. Differential privacy: an exploration of the privacy-utility landscape. PhD thesis 

Rutgers University  2013.

[19] C. Villani. Optimal Transport: Old and New. Springer  2009.

[20] Y. Wang  S. Fienberg  and A. Smola. Privacy for free: Posterior sampling and stochastic

gradient monte carlo. In ICML  2015.

[21] T. Zhang. From ε-entropy to KL-entropy: Analysis of minimum information complexity den-

sity estimation. The Annals of Statistics  34(5):2180–2210  2006.

9

,Kentaro Minami
HItomi Arai
Issei Sato
Hiroshi Nakagawa
Shali Jiang
Gustavo Malkomes
Matthew Abbott
Benjamin Moseley
Roman Garnett