2012,Weighted Likelihood Policy Search with Model Selection,Reinforcement learning (RL) methods based on direct policy search (DPS) have been actively discussed to achieve an efficient approach to complicated Markov decision processes (MDPs). Although they have brought much progress in practical applications of RL  there still remains an unsolved problem in DPS related to model  selection for the policy. In this paper  we propose a novel DPS method  {\it  weighted likelihood policy search (WLPS)}  where a policy is efficiently learned through the weighted likelihood estimation. WLPS naturally connects DPS to the statistical inference problem and thus various sophisticated techniques in statistics can be applied to DPS problems directly. Hence  by following the idea of the {\it information criterion}  we develop a new measurement for model comparison in DPS based on the weighted log-likelihood.,Weighted Likelihood Policy Search

with Model Selection

Tsuyoshi Ueno (cid:3)

Japan Science and Technology Agency
ueno@ar.sanken.osaka-u.ac.jp

Takashi Washio
Osaka University

Kohei Hayashi

University of Tokyo

hayashi.kohei@gmail.com

Yoshinobu Kawahara

Osaka University

washio@ar.sanken.osaka-u.ac.jp

kawahara@ar.sanken.osaka-u.ac.jp

Abstract

Reinforcement learning (RL) methods based on direct policy search (DPS) have
been actively discussed to achieve an efﬁcient approach to complicated Markov
decision processes (MDPs). Although they have brought much progress in prac-
tical applications of RL  there still remains an unsolved problem in DPS related
to model selection for the policy. In this paper  we propose a novel DPS method 
weighted likelihood policy search (WLPS)  where a policy is efﬁciently learned
through the weighted likelihood estimation. WLPS naturally connects DPS to the
statistical inference problem and thus various sophisticated techniques in statis-
tics can be applied to DPS problems directly. Hence  by following the idea of the
information criterion  we develop a new measurement for model comparison in
DPS based on the weighted log-likelihood.

1 Introduction
In the last decade  several direct policy search (DPS) methods have been developed in the ﬁeld of
reinforcement learning (RL) [1  2  3  4  5  6  7  8  9] and have been successfully applied to practical
decision making applications [5  7  9]. Unlike classical approaches [10]  DPS characterizes a policy
as a parametric model and explores parameters such that the expected reward is maximized in a
given model space. Hence  if one employs a model with a reasonable number of DoF (degrees of
freedom)  DPS could ﬁnd a reasonable policy efﬁciently even when the target decision making task
has a huge number of DoF. Therefore  the development of an efﬁcient model selection methodology
for the policy is crucial in RL research.
In this paper  we propose weighted likelihood policy search (WLPS): an efﬁcient iterative policy
search algorithm that allows us to select an appropriate model automatically from a set of candidate
models. To this end  we ﬁrst introduce a log-likelihood function weighted by the discounted sum of
future rewards as the cost function for DPS. In WLPS  the policy parameters are updated by itera-
tively maximizing the weighted log-likelihood for the obtained sample sequence. A key property of
WLPS is that the maximization of weighted log-likelihood corresponds to that of the lower bound of
the expected reward and thus  WLPS is guaranteed to increase the expected reward monotonically at
each iteration. This can be shown to converge to the same solution as the expectation-maximization
policy search (EMPS) [1  4  9]. In this way  our framework gives a natural connection between
DPS and the statistical inference problem for maximum likelihood estimation. One beneﬁt of this
approach is that we can directly apply the information criterion scheme [11  12]  which is a familiar
theory in statistics  to the weighted log-likelihood. This enables us to construct a model selection
strategy for the policy by comparing the information criterion based on the weighted log-likelihood.
The contribution of this paper is summarized as follows:

(cid:3)

https://sites.google.com/site/tsuyoshiueno/

1

1. We prove that each update to the policy parameters resulting from the maximization of the
weighted log-likelihood has consistency and asymptotic normality  which have been not
elucidated yet in DPS  and converges to the same solution as EMPS.

2. We introduce prior distribution on the policy parameter  and analyze the asymptotic behav-
ior of the marginal weighted likelihood given by marginalizing out the policy parameter.
We then propose a measure of the goodness of the policy model based on the posterior
probability of the model in a similar way as Bayesian information criterion [12].

The rest of the paper is organized as follows. We ﬁrst give a formulation of the DPS problem in RL 
and a short overview of EMPS (Section 2). Next  we present our new DPS framework  WLPS  and
investigate the theoretical aspects thereof (Section 3). In addition  we construct the model selection
strategy for the policy (Section 4). Finally  we present a statistical interpretation of WLPS and
discuss future directions of study in this regard (Section 5).
Related Works Several approaches have been proposed for the model selection problem in the
estimation of a state-action value function [13  14].
[14] derived the PAC-Bayesian bounds for
estimating state-action value functions. [13] developed a complexity regularization based model
selection algorithm from the viewpoint of the minimization of the Bellman error  and investigated its
theoretical aspects. Although these studies allow us to select a good model for a state-value function
with theoretical supports  they cannot be applied to model selection for DPS directly. [15] developed
a model selection strategy for DPS by reusing the past observed sample sequences through the
importance weighted cross-validation (IWCV). However  IWCV requires heavy computational costs
and includes computational instability when estimating the importance sampling weights.
Recently  there are a number of studies that reformulate stochastic optimal control and RL as a
minimization problem of Kullback-Leiblar (KL) divergence [16  17  18]. Our approach is closely
related to these works; in fact  WLPS can also be interpreted as the minimization problem of the
reverse form of KL divergence compared with that used in [16  17  18].
2 Preliminary: EMPS
We consider discrete-time inﬁnite horizon Markov Decision Processes (MDPs)  deﬁned as the
quadruple (X ;U; p; r): X (cid:18) Rdx is a state space; U (cid:18) Rdu is an action space; p(x
′jx; u) is a station-
′ when taking action u at state x; and r : X (cid:2) U 7! R+
ary transition distribution to the next state x
is a positive reward received with the state transition. Let (cid:25)(cid:18)(ujx) := p(ujx; (cid:18)) be the stochastic
parametrized policy followed by the agent  where an m-dimensional vector (cid:18) 2 (cid:2); (cid:2) (cid:18) Rm means
an adjustable parameter. Given initial state x1 and parameter vector (cid:18)  the joint distribution of the
sample sequence  fx2:n; u1:ng  of the MDP is described as

p(cid:18)(x2:n; u1:njx1) = (cid:25)(cid:18)(u1jx1)

p(xijxi(cid:0)1; ui(cid:0)1)(cid:25)(cid:18)(uijxi):

(1)

n∏

i=2

We further impose the following assumptions on MDPs.
Assumption 1. For any (cid:18) 2 (cid:2)  the MDP given by Eq. (1)  is aperiodic and Harris recurrent. Hence 
MDP (1) is ergodic and has a unique invariant stationary distribution (cid:22)(cid:18)(x)  for any (cid:18) 2 (cid:2) [19].
Assumption 2. For any x 2 X and u 2 U  reward r(x; u) is uniformly bounded.
Assumption 3. Policy (cid:25)(cid:18)(ujx) is thrice continuously differentiable with respect to parameter (cid:18).
The general goal of DPS is to ﬁnd an optimal policy parameter (cid:18)
reward deﬁned by

(cid:3) 2 (cid:2) that maximizes the expected

∑
p(cid:18)(x2:n; u1:njx1)fRng dx2:ndu1:n;
where Rn := Rn(x1:n; u1:n) = (1=n)
i=1 r(xi; ui). In general  the direct maximization of ob-
jective function (2) forces us to solve a non-convex optimization problem with a high non-linearity.
Thus  instead of maximizing Eq. (2)  many of the DPS methods maximize the lower bound on
Eq. (2)  which may be much more tractable than the original objective function.
Lemma 1 shows that there is a tight lower bound on objective function (2).
Lemma 1. [1  4  9] The following inequality holds for any distribution q(x2:n; u1:njx1):
p(cid:18)(x2:n; u1:njx1)Rn
q(x2:n; u1:njx1)

ln (cid:17)n((cid:18)) (cid:21) Fn(q; (cid:18)) :=

dx2:ndu1:n; 8n

q(x2:n; u1:njx1)

ln

(cid:17)((cid:18)) := lim
n!1

∫∫

(2)

(3)

∫∫

n

}

{

2

∫∫

where (cid:17)n((cid:18)) =
maximizer of Fn(q; (cid:18)) for some (cid:18)  i.e.  q
when q

(x2:n; u1:njx1) / p(cid:18)(x2:n; u1:njx1)fRng.

p(x2:n; u1:njx1)fRng dx2:ndu1:n. The equality holds if q(x2:n; u1:njx1) is a
Fn(q; (cid:18))  which is satisﬁed

(x2:n; u1:njx1) = argmaxq

(cid:3)

(cid:3)

∫∫

(

n∑

)

The proof is given in Section 1 in the supporting material. Lemma 1 leads to an effective iterative
algorithm  the so-called EMPS  which breaks down the potentially difﬁcult maximization problem
(cid:18)′(x2:n; u1:njx1) /
(cid:3)
for the expected reward into two stages: 1) evaluation of the path distribution q
p(cid:18)′(x2:n; u1:njx1)fRng at the current policy parameter (cid:18)
(cid:3)
(cid:18)′; (cid:18)) with
respect to parameter (cid:18). This EMPS cycle is guaranteed to increase the value of the expected reward
unless the parameters already correspond to a local maximum [1  4  9].
Taking the partial derivative of the policy with respect to parameter (cid:18)  a new parameter vector ~(cid:18) that
maximizes Fn(q

(cid:3)
(cid:18)′ ; (cid:18)) is found by solving the following equation:

′  and 2) maximization of Fn(q

p(cid:18)′(x2:n; u1:njx1)

i=1

 ~(cid:18)(xi; ui)

Rndx2:ndu1:n = 0;

(4)
where   : X (cid:2) U (cid:2) (cid:2) denotes a partial derivative of the logarithm of the policy with respect to
parameter (cid:18)  i.e.   (cid:18)(x; u) := (@)=(@(cid:18)) ln (cid:25)(cid:18)(ujx).
′jx; u) is known  we can easily derive parameter
Note that if the state transition distribution p(x
′jx; u) is generally unknown  and it is a non-trivial
(cid:22)(cid:18) analytically or numerically. However  p(x
problem to identify this distribution in large-scale applications. Thus  it is desirable to ﬁnd parameter
(cid:22)(cid:18) in model-free ways  i.e.  parameter is estimated from the sample sequences alone  instead of using
′jx; u). Although many variants of model-free EMPS algorithms [4  6  9  15] have hitherto been
p(x
developed  their fundamental theoretical properties such as consistency and asymptotic normality at
each iteration have not yet been elucidated. Moreover  it is not even obvious whether they have such
desirable statistical properties.

3 Proposed framework: WLPS
In this section  we newly introduce a weighted likelihood as the objective function for DPS (Def-
inition 1)  and derive the WLPS algorithm  executed by iterating two steps: evaluation and maxi-
mization of the weighted log-likelihood function (Algorithm 1). Then  in Section 3.2  we show that
WLPS is guaranteed to increase the expected reward at each iteration  and to converge to the same
solution as EMPS (Theorem 1).
3.1 Overview of WLPS
In this study  we consider the following weighted likelihood function.
Deﬁnition 1. Suppose that given initial state x1  a random sequence fx2:n; u1:ng is gener-
ated from model p(cid:18)′ (x2:n; u1:njx1) of the MDP. Then  we deﬁne a weighted likelihood function
^p(cid:18)′;(cid:18)(x2:n; u1:njx1) and a weighted log-likelihood function L(cid:18)
(cid:25)(cid:18)(uijxi)Q(cid:12)

^p(cid:18)′;(cid:18)(x2:n; u1:njx1) := (cid:25)(cid:18)(u1jx1)Q(cid:12)

(5)

1

′

n ((cid:18)) := ln ^p(cid:18)′;(cid:18)(x2:n; u1:njx1) :=
L(cid:18)

i ln (cid:25)(cid:18)(uijxi) +
Q(cid:12)

(6)

′
n ((cid:18))  respectively  as
i p(xijxi(cid:0)1; ui(cid:0)1)
ln p(xijxi(cid:0)1; ui(cid:0)1);
∑

n∑

i=2

n∏
n∑

i=2

i=1

i is the discounted sum of the future rewards given by Q(cid:12)

where Q(cid:12)
is a discounted factor such that (cid:12) 2 [0; 1).
Now  let us consider the maximization of weighted log-likelihood function (6). Taking the partial
derivative of weighted log-likelihood (6) with respect to parameter (cid:18)  we can obtain the maximum
weighted log-likelihood estimator ^(cid:18)n := ^(cid:18)(x1:n; u1:n) as a solution of the following estimation
equation:

:=

i

n

j=i (cid:12)j(cid:0)ir(xj; uj)  and (cid:12)

n∑

n∑

n∑

′
n (^(cid:18)n) :=

G(cid:18)

 ^(cid:18)n

(xi; ui)Q(cid:12)

i =

(cid:12)j(cid:0)i ^(cid:18)n

(xi; ui)r(xj; uj) = 0:

(7)

i=1

i=1

j=i

3

Note that when policy (cid:25)(cid:18) is modeled by an exponential family  estimating equation (7) can easily
be solved analytically or numerically using convex optimization techniques. In WLPS  the update
of the policy parameter is performed by evaluating estimating equation (7) and ﬁnding estimator ^(cid:18)n
iteratively from this equation. Algorithm 1 gives an outline of the WLPS procedure.
Algorithm 1 (WLPS).

1. Generate a sample sequence fx1:n; u1:ng by employing the current policy parameter (cid:18) 

and evaluate estimating equation (7).

2. Find a new estimator by solving estimating equation (7) and check for convergence. If

convergence is not satisﬁed  return to step 1.

It should be noted that WLPS guarantees to monotonically increase the expected reward (cid:17)((cid:18))  and to
converge asymptotically under certain conditions to the same solution as EMPS  given by Eq. (4). In
the next subsection  we discuss the reason why WLPS satisﬁes such desirable statistical properties.
3.2 Convergence of WLPS
To begin with  we show consistency and asymptotic normality of estimator ^(cid:18)n given by Eq. (7)
when (cid:12) is any constant between 0 and 1. To this end  we ﬁrst introduce the notion of uniform
mixing  which plays an important role when discussing statistical properties in stochastic processes
[19]. The deﬁnition of uniform mixing is given below.
Deﬁnition 2. Let fYi : i = f(cid:1)(cid:1)(cid:1) ;(cid:0)1; 0; 1;(cid:1)(cid:1)(cid:1)gg be a strictly stationary process on a probabilistic
k be the (cid:27)-algebra generated by fYk;(cid:1)(cid:1)(cid:1) ; Ymg. Then  process fYig is said
space (Ω; F; P )  and F m
to be uniform mixing (φ-mixing) if φ(s) ! 0 as s ! 1  where

φ(s) :=

sup

A2F k(cid:0)1;B2F1

k+s

jP (BjA) (cid:0) P (B)j = 0; P (A) ̸= 0:

Function φ(s) is called the mixing coefﬁcient  and if the mixing coefﬁcient converges to zero ex-
ponentially fast  i.e.  there exist constants D > 0 and (cid:26) 2 [0; 1) such that φ(s) < D(cid:26)s  then the
stochastic process is called geometrically uniform mixing. Note that if a stochastic process is a
strictly stationary ﬁnite-state Markov process and ergodic  the process satisﬁes the geometrically
uniform mixing conditions [19].
Now  we impose certain conditions for proving the consistency and asymptotic normality of estima-
tor ^(cid:18)n  summarized as follows.
Assumption 4. For any (cid:18) 2 (cid:2)  MDP p(cid:18)(x2:n; u1:njx1) is geometrically uniform mixing.
Assumption 5. For any x 2 X   u 2 U  and (cid:18) 2 (cid:2)  function  (cid:18)(x; u) is uniformly bounded.
Assumption 6. For any (cid:18) 2 (cid:2)  there exists a parameter value (cid:22)(cid:18) 2 (cid:2) such that

35 = 0;

(cid:12)j(cid:0)1r(xj; uj)

[(cid:1)] denotes the expectation over fx2:1; u1:1g with respect

x1(cid:24)(cid:22)(cid:18)

where E(cid:25)(cid:18)
n!1 (cid:22)(cid:18)(x1)(cid:25)(cid:18)(u1jx1)
lim
[
Assumption 7. For any (cid:18) 2 (cid:2) and ϵ > 0 

i=2 p(xijxi; ui)(cid:25)(cid:18)(uijxi).

n

sup

(cid:18)′:j(cid:18)′(cid:0) (cid:22)(cid:18)j>ϵ

 (cid:18)′ (x1; u1)

Assumption 8. For any (cid:18) 2 (cid:2)  matrix A := A((cid:22)(cid:18)) = E(cid:25)(cid:18)
invertible  where K(cid:18)(x; u) := @(cid:18) (cid:18)(x; u) = @2=(@(cid:18)@(cid:18)

⊤

E(cid:25)(cid:18)
x1(cid:24)(cid:22)(cid:18)

∏

1∑

j=1

24 (cid:22)(cid:18)(x1; u1)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E(cid:25)(cid:18)

x1(cid:24)(cid:22)(cid:18)

(8)

to distribution

1∑

j=1

](cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > 0:
]
∑1
j=1 (cid:12)j(cid:0)1r(xj; uj)

is

(cid:12)j(cid:0)1r(xj; uj)

[
x1(cid:24)(cid:22)1
) ln (cid:25)(cid:18)(ujx).

K (cid:22)(cid:18)(x1; u1)

Under the conditions given in Assumptions 1-7  estimator ^(cid:18)n converges to (cid:22)(cid:18) in probability  as shown
in the following lemma.
Lemma 2. Suppose that given initial state x1  a random sequence fx2:n; u1:ng is generated from
model fp(cid:18)(x2:n; u1:njx1)j(cid:18)g of the MDP. If Assumptions 1-7 are satisﬁed  then estimator ^(cid:18)n given by
estimating equation (7) shows consistency  i.e.  estimator ^(cid:18)n converges to parameter (cid:22)(cid:18) in probability.

4

The proof is given in Section 2 in the supporting material. Note that if the policy is characterized as
an exponential family  we can replace Assumption 7 with Assumption 8 to prove the result in Lemma
3. Next  we show the asymptotic convergence rate of the estimator given a consistent estimator.
Lemma 3 shows that the estimator converges at the rate Op(n
Lemma 3. Suppose that given initial state x1  a random sequence fx2:n; u1:ng is generated from
model p(cid:18)′(x2:n; u1:njx1)  and Assumptions 1-6 and 8 are satisﬁed. If estimator ^(cid:18)n  given by esti-
mating equation (7) converges to (cid:22)(cid:18) in probability  then we have

(cid:0)1=2).

p

n

n(^(cid:18)n (cid:0) (cid:22)(cid:18)) = (cid:0) 1p
)(∑1

and
=

hand

right

the

Furthermore 
bution whose mean
where (cid:6)
(cid:6)((cid:22)(cid:18))
E(cid:25)(cid:18)′
x1(cid:24)(cid:22)(cid:18)′

[(∑1

:=

j=1 (cid:12)j(cid:0)1r(xj; uj)

(cid:0)1

A

(cid:12)j(cid:0)i (cid:22)(cid:18)(xi; ui)r(xj; uj) + op(1):

(9)

side

covariance
(cid:0)((cid:22)(cid:18)) +

j′=1+i (cid:12)j

∑1

of Eq.
)
are 
i=2 (cid:0)i((cid:22)(cid:18)) +
′(cid:0)1r(xj′ ; uj′ )

converges

(9)
respectively 

∑1

j=2 (cid:0)j((cid:22)(cid:18))

to
zero
⊤.

  (cid:22)(cid:18)(x1; u1)  (cid:22)(cid:18)(xi; ui)

.

a Gaussian
and A

(cid:0)1(cid:6)(A
Here  (cid:0)i((cid:22)(cid:18))
⊤

]

distri-
(cid:0)1)
⊤ 
:=

n∑

n∑

i=1

j=i

The proof is given in Section 3 in the supporting material.
Now we consider the relation between WLPS and EMPS. The following theorem shows that the
estimator ^(cid:18)n given by Eq. (7) converges to the same solution as that of EMPS asymptotically  when
taking the limit of (cid:12) to 1.
Theorem 1. Suppose that Assumptions 1-7 are satisﬁed. If (cid:12) approaches to 1 from below  WLPS
leads to the same solution with EMPS given by Eq. (4) as n ! 11.

Proof. We introduce the following support lemma.
Lemma 4. Suppose that Assumptions 1-6 are satisﬁed. Then  the partial derivative of the lower
bound with q

[

(cid:3)
(cid:18)′ satisﬁes
@
@(cid:18)

lim
n!1

Fn(q

(cid:3)
(cid:18)′ ; (cid:18)) = lim
(cid:12)!1(cid:0)

E(cid:25)(cid:18)′
x1(cid:24)(cid:22)(cid:18)′

 (cid:18)(x1; u1)

where (cid:12) ! 1

(cid:0) denotes that (cid:12) converges to 1 from below.

]
(cid:12)j(cid:0)1r(xj; uj)

;

1∑

j=1

The proof is given in Section 4 in the supporting material. From the results in Lemmas 2 and 4  it
is obvious that the estimator ^(cid:18)n given by Eq. (7) converges to the same solution as that of EMPS as
(cid:12) ! 1 from bellow.

Theorem 1 implies that WLPS monotonically increases the expected reward. It should be empha-
sized that WLPS provides us with an important insight into DPS  i.e.  the parameter update of EMPS
can be interpreted as a well-studied maximum (weighted) likelihood estimation problem. This al-
lows us to naturally apply various sophisticated techniques for model selection  which are well
established in statistics  to DPS. In the next section  we discuss model selection for policy (cid:25)(cid:18)(ujx).
4 Model selection with WLPS
Common model selection strategies are carried out by comparing candidate models  which are spec-
iﬁed in advance  based on a criterion that evaluates the goodness of ﬁt of the model estimated from
the obtained samples. Since the motivation for RL is to maximize the expected reward given in (2) 
it would be natural to seek an appropriate model for the policy through the computation of some
reasonable measure to evaluate the expected reward from the sample sequences. However  since dif-
ferent policy models give different generative models for sample sequences  we need to obtain new
sample sequences to evaluate the measure each time the model is changed. Therefore  employing a
strategy of model selection based directly on the expected reward would be hopelessly inefﬁcient.

1In practice  the constant (cid:12) is set to an arbitrary value close to one. If we can analyze the ﬁnite sample
behavior of the expected reward with the WLPS estimator  we may obtain a better estimator by ﬁnding an
optimal (cid:12) in the sense of the maximization of the expected reward. Some researches have recently tackled to
establish the ﬁnite sample analysis for RL based on statistical learning theory [20  21]. These works might
provide us with some insights into the ﬁnite sample analysis of WLPS.

5

Instead  to develop a tractable model selection  we focus on the weighted likelihood given by Eq. (5).
As mentioned before  the policy with the maximum weighted log-likelihood must satisfy the max-
imum of the lower bound of the expected reward asymptotically. Moreover  since the weighted
likelihood is deﬁned under a certain ﬁxed generative process for the sample sequences  unlike the
expected reward case  the weighted likelihood can be evaluated using unique sample sequences
even when the model has been changed. These observations lead to the fact that if it were possible
to choose a good model from the candidate models in the sense of the weighted likelihood at each
iteration in WLPS  we could realize an efﬁcient DPS algorithm with model selection that achieves a
monotonic increase in the expected reward.
In this study  we develop a criterion for choosing a suitable model by following the analogy of the
Bayesian information criterion (BIC) [12]  designed through asymptotic analysis of the posterior
probability of the models given the data. Let M1; M2;(cid:1)(cid:1)(cid:1) ; Mk be k candidate policy models  and
assume that each model Mj is characterized by a parametric policy (cid:25)(cid:18)j (ujx) and the prior distri-
bution p((cid:18)jjMj) of the policy parameter. Also  deﬁne the marginal weighted likelihood of the j-th
candidate model ^p(cid:18)′;j(x2:n; u1:njx1) as

^p(cid:18)′;j(x2:n; u1:njx1) :=

(cid:25)(cid:18)j (u1jx1)Q(cid:12)

1

(cid:25)(cid:18)j (uijxi)Q(cid:12)

i p(xijxi(cid:0)1; ui(cid:0)1)p((cid:18)jjMj)d(cid:18)j:

(10)

∫

n∏

i=1

In a similar manner to the BIC  we now consider the posterior probability of the j-th model given the
sample sequence by introducing the prior probability of the j-th model p(Mj). From the generalized
Bayes’ rule  the posterior distribution of the j-th model is given by

p(Mjjx1:n; u1:n) :=

∑

^p(cid:18)′;j(x2:n; u1:njx1)p(Mj)
j′=1 ^p(cid:18)′;j′(x2:n; u1:njx1)p(Mj′)

k

:

(11)

and in our model selection strategy  we adopt the model with the largest posterior probability.
For notational simplicity  in the following discussion we omit the subscript that represents the index
indicating the number of models. Assuming that the prior probability is uniform in all models 
the model with the maximum posterior probability corresponds to that of the marginal weighted
likelihood. The behavior of the marginal weighted likelihood can be evaluated when the integrand of
marginal weighted likelihood (10) is concentrated in a neighborhood of the weighted log-likelihood
estimator given by estimating equation (7)  as described in the following theorem.
Theorem 2. Suppose that  given an initial state x1  a random sequence fx2:n; u1:ng is generated
from the model p(cid:18)′(x2:n; u1:njx1) of the MDP. Suppose that Assumptions 1-3 and 5 are satisﬁed. If
the following conditions

(a) The estimator ^(cid:18)n given by Eq. (7) converges to (cid:18) at the rate of Op(n
(b) The prior distribution p((cid:18)jM ) satisﬁes p(^(cid:18)njM ) = Op(1).
(c) The matrix A((cid:18)) := E(cid:25)(cid:18)′
(d) For any x 2 X   u 2 U and (cid:18) 2 (cid:2)  K(cid:18)(x; u) is uniformly bounded.

∑1
j=1 (cid:12)j(cid:0)ir(xj; uj)] is invertible.

x1(cid:24)(cid:22)(cid:18)′ [K(cid:18)(x1; u1)

(cid:0)1=2).

are satisﬁed  the log marginal weighted likelihood can be calculated as

ln ^p(cid:18)′(x2:n; u1:njx1) = L(cid:18)

′

n (^(cid:18)n) (cid:0) 1

m ln n + Op(1);

2

where recall that m denotes the number of dimensional of the model (policy parameter).

∑

n

is given in Section 5 in the supporting material.

the term 
The proof
i=2 ln p(xijxi(cid:0)1; ui(cid:0)1) in L(cid:18)
′
n (^(cid:18)n)  does not depend on the model. Therefore  when evaluat-
ing the posterior probability of the model  it is sufﬁcient to compute the following model selection
criterion:

Note that

n∑

IC =

(uijxi)Q(cid:12)

i (cid:0) 1
2

ln (cid:25)^(cid:18)n

i=1

m ln n:

(12)

As can be seen  this model selection criterion consists of two terms  where the ﬁrst term is the
weighted log-likelihood of the policy and the second is a penalty term that penalizes highly complex
models. Also  since the ﬁrst term is larger than the second term  this criterion gives the model with
the maximum weighted log-likelihood asymptotically. Algorithm 2 describes the algorithm ﬂow of
WLPS including the model selection strategy.

6

Algorithm 2 (WLPS with model selection).

1. Generate a sample sequence fx1:n; u1:ng by employing the current policy parameter (cid:18).
2. For all models  ﬁnd estimator ^(cid:18)n by solving estimating equation (7) and evaluate model

selection criterion (12).

3. Choose the best model based on model selection criterion (12) and check for convergence.

If convergence is not satisﬁed  return to 1.

Empirical Example We evaluated the performance of the proposed model-selection method us-
ing a simple one-dimensional linear quadratic Gaussian (LQG) problem. This problem is known
to be sufﬁciently difﬁcult as an empirical evaluation  while it is analytically solvable.
In this
problem  we characterized the state transition distribution p(xijxi(cid:0)1; ui(cid:0)1) as a Gaussian distri-
bution N (xij(cid:22)xi; (cid:27)) with mean (cid:22)xi = xi(cid:0)1 + ui(cid:0)1 and variance (cid:27) = 0:52. The reward function
was set to a quadratic function r(xi; ui) = (cid:0)x2
i + c  where c is a positive scalar value for
preventing the reward r(x; u) being negative. The control signal ui was generated from a Gaus-
sian distribution N (uij(cid:22)ui; (cid:27)
= 0:5. We used a linear model
with polynomial basis functions deﬁned as (cid:22)ui =
j + (cid:18)0  where k is the order of the
polynomial. Note that  in this LQG setting  the optimal controller can be represented as a linear
model  i.e.  the optimal policy can be obtained when the order of polynomial is selected as k = 1.

′
) with mean (cid:22)ui and variance (cid:27)
j=1 (cid:18)jxj

′

i

(cid:0) u2
∑

k

Figure 1: Distribution of order k se-
lected by our model selection criterion
(left bar) and the weighted likelihood
(right bar).

In this experiment  we validated whether the proposed model
selection method can detect the true order of the polyno-
mial. To illustrate how our proposed model selection crite-
rion works  we compared the performance of the proposed
model selection method with a na¨ıve method based on the
weighted log-likelihood (6). The weighted-log-likelihood-
based selection  similarly to the proposed method  was per-
formed by computing the weighted log-likelihood scores (6)
over all candidate models and selecting the model with the
maximum score among the candidates.
Figure 1 shows the distribution on the scores of the selected
polynomial orders k in the learned policies from ﬁrst to ﬁfth
order by using the weighted log-likelihood and our model se-
lection criterion. The distributions of the scores were obtained by repeating random 1000 trials. A
learning process was performed by 200 iterations of WLPS  each of which contained 200 samples
generated by the current policy. The discounted factor (cid:12) was set to 0:99. As shown in Figure 1  in
the proposed method  the peak of the selected order was located at the true order k = 1. On the
other hand  in the weighted log-likelihood method  the distribution of the orders converged to a one
with two peaks at k = 1 and k = 4. This result seems to show that the penalized term in our model
selection criterion worked well.
5 Discussion
In this study  we have discussed a DPS problem in the framework of weighted likelihood estimation.
We introduced a weighted likelihood function as the objective function of DPS  and proposed an
incremental algorithm  WLPS  based on the iteration of maximum weighted log-likelihood estima-
tion. WLPS shows desirable theoretical properties  namely  consistency  asymptotic normality  and
a monotonic increase in the expected reward at each iteration. Furthermore  we have constructed a
model selection strategy based on the posterior probability of the model given a sample sequence
through asymptotic analysis of the marginal weighted likelihood.
WLPS framework has a potential to bring a new theoretical insight to DPS  and derive more efﬁcient
algorithms based on the theoretical considerations. In the rest of this paper  we summarize some key
issues that need to be addressed in future research.
5.1 Statistical interpretation of model-free and model-based WLPS
One of the important open issues in RL is how to combine model-free and model-based approaches
with theoretical support. To this end  it is necessary to clarify the difference between model-based
and model-free approaches in the theoretical sense. WLPS provides us with an interesting insight
into the relation between model-free and model-based DPS from the viewpoint of statistics.

7

  700600500400300200100012345proposed model selectionweighted log-likelihoodThe oder of basis functionsThe order of basis functions	′

i=2

′jx; u) as a parametric model p(cid:20)(x

We begin by introducing the model-based WLPS method. Let us specify the state transition distri-
′-dimensional
bution p(x
′jx; u) with respect to parameter (cid:20) and taking the partial derivative
n∑
parameter vector. Assuming p(cid:20)(x
of the log weighted likelihood (6)  we obtain the estimating equation for parameter (cid:20):

′jx; u; (cid:20))  where (cid:20) is an m

′jx; u) := p(x

(cid:24)^(cid:20)n(xi(cid:0)1; ui(cid:0)1; xi) = 0;

) is the partial derivative of the state transition distribution p(cid:20)(x

(13)
′jx; u) with respect
where (cid:24)(cid:20)(x; u; x
to (cid:20). As can be seen  estimating equation (13) corresponds to the likelihood equation  i.e.  the esti-
mator  ^(cid:20)n = ^(cid:20)n(x1:n; u1:n(cid:0)1)  given by (13) is the maximum likelihood estimator. This observation
indicates that the weighted likelihood integrates two different objective functions: one for learning
policy (cid:25)(cid:18)(ujx)  and the other for the state predictor  p(cid:20)(x
′jx; u). Having obtained estimator ^(cid:20)n
from estimating equation (13)  the model-based WLPS estimates the policy parameter by ﬁnding
the solution  (cid:20)(cid:18)n := (cid:20)(cid:18)(x1:n; u1:n)  of the following estimating equation:
(cid:12)j(cid:0)i  (cid:20)(cid:18)n (xi; ui)r(xj; uj)

p(cid:18)′;^(cid:20)n (x2:n; u1:njx1)

{
n∑

dx2:ndu1:n = 0:

(14)

∫∫

n∑

}

i=1

j=i

Note that estimating equation (14) is derived by taking the integral of Eq. (7) over the sample se-
quence fx2:n; u1:ng based on the current estimated model p(cid:18)′;^(cid:20)n (x2:n; u1:njx1). Thus  the model-
′jx; u) is well
based WLPS converges to the same parameter as the model-free WLPS  if model p(cid:20)(x
speciﬁed2.
We now consider the general treatment for model-free and model-based WLPS from a statistical
viewpoint. Model-based WLPS fully speciﬁes the weighted likelihood by using the parametric
policy and parametric state transition models  and estimates all the parameters that appear in the
parametric weighted likelihood. Hence  model-based WLPS can be framed as a parametric statis-
tical inference problem. Meanwhile  model-free WLPS partially speciﬁes the weighted likelihood
by only using the parametric policy model. This can be seen as a semiparametric statistical model
[22  23]  which includes not only parameters of interest  but also additional nuisance parameters
with possibly inﬁnite DoF  where only the policy is modeled parametrically and the other unspec-
iﬁed part corresponds to the nuisance parameters. Therefore  model-free WLPS can be framed as
a semiparametric statistical inference problem. Hence  the difference between model-based and
model-free WLPS methods can be interpreted as the difference between parametric and semipara-
metric statistical inference. The theoretical aspects of both parametric and semiparametric inference
have been actively investigated and several approaches for combining their estimators have been
proposed [23  24  25]. Therefore  by following these works  we have developed a novel hybrid DPS
algorithm that combines model-free and model-based WLPS with desirable statistical properties.
5.2 Variance reduction technique for WLPS
In order to perform fast learning of the policy  it is necessary to ﬁnd estimators that can reduce the
estimation variance of the policy parameters in DPS. Although variance reduction techniques have
been proposed in DPS [26  27  28]  these employ indirect approaches  i.e.  instead of considering
the estimation variance of the policy parameters  they reduce the estimation variance of the mo-
ments necessary to learn the policy parameter. Unfortunately  these variance reduction techniques
do not guarantee decreasing the estimation variance of the policy parameters  and thus it is desir-
able to develop a direct approach that can evaluate and reduce the estimation variance of the policy
parameters.
As stated above  we can interpret model-free WLPS as a semiparametric statistical inference prob-
lem. This interpretation allows us to apply the estimating function method [22  23]  which has been
well established in semiparametric statistics  directly to WLPS. The estimating function method is
a powerful tool for the design of consistent estimators and the evaluation of the estimation variance
of parameters in a semiparametric inference problem. The advantage of considering the estimating
function is the ability 1) to characterize an entire set of consistent estimators  and 2) to ﬁnd the opti-
mal estimator with the minimum parameter estimation variance from the set of estimators [23  29].
Therefore  by applying this to WLPS  we can characterize an entire set of estimators  which maxi-
mizes the expected reward without identifying the state transition distribution  and ﬁnd the optimal
estimator with the minimum estimation variance.

2In the following discussion  in order to clarify the difference between the model-free and the model-based

manners  we write original WLPS as model-free WLPS.

8

References
[1] P. Dayan and G. Hinton  “Using expectation-maximization for reinforcement learning ” Neural Compu-

tation  vol. 9  no. 2  pp. 271–278  1997.

[2] J. Baxter and P. L. Bartlett  “Inﬁnite-horizon policy-gradient estimation ” Journal of Artiﬁcial Intelligence

Research  vol. 15  no. 4  pp. 319–350  2001.

[3] V. R. Konda and J. N. Tsitsiklis  “On actor-critic algorithms ” SIAM Journal on Control and Optimization 

vol. 42  no. 4  pp. 1143–1166  2003.

[4] J. Peters and S. Schaal  “Reinforcement learning by reward-weighted regression for operational space

control ” in Proceedings of the 24th International Conference on Machine Learning  2007.

[5] ——  “Natural actor-critic ” Neurocomputing  vol. 71  no. 7-9  pp. 1180–1190  2008.
[6] N. Vlassis  M. Toussaint  G. Kontes  and S. Piperidis  “Learning model-free robot control by a monte

carlo em algorithm ” Autonomous Robots  vol. 27  no. 2  pp. 123–130  2009.

[7] E. Theodorou  J. Buchli  and S. Schaal  “A generalized path integral control approach to reinforcement

learning ” Journal of Machine Learning Research  vol. 11  pp. 3137–3181  2010.

[8] J. Peters  K. M¨ulling  and Y. Alt¨un  “Relative entropy policy search ” in Proceedings of the 24-th National

Conference on Artiﬁcial Intelligence  2010.

[9] J. Kober and J. Peters  “Policy search for motor primitives in robotics ” Machine Learning  vol. 84  no.

1-2  pp. 171–203  2011.

[10] R. S. Sutton and A. G. Barto  Reinforcement Learning: An Introduction. MIT Press  1998.
[11] H. Akaike  “A new look at the statistical model identiﬁcation ” IEEE Transactions on Automatic Control 

vol. 19  no. 6  pp. 716–723  1974.

[12] G. Schwarz  “Estimating the dimension of a model ” The Annals of Statistics  vol. 6  no. 2  pp. 461–464 

1978.

[13] A. Farahmand and C. Szepesv´ari  “Model selection in reinforcement learning ” Machine Learning  pp.

1–34  2011.

[14] M. M. Fard and J. Pineau  “PAC-Bayesian model selection for reinforcement learning ” in Advances in

Neural Information Processing Systems 22  2010.

[15] H. Hachiya  J. Peters  and M. Sugiyama  “Reward-weighted regression with sample reuse for direct policy

search in reinforcement learning ” Neural Computation  vol. 23  no. 11  pp. 2798–2832  2011.

[16] M. G. Azar and H. J. Kappen  “Dynamic policy programming ” Tech. Rep. arXiv:1004.202  2010.
[17] H. Kappen  V. G´omez  and M. Opper  “Optimal control as a graphical model inference problem ” Machine

learning  pp. 1–24  2012.

[18] K. Rawlik  M. Toussaint  and S. Vijayakumar  “On stochastic optimal control and reinforcement learning

by approximate inference ” in International Conference on Robotics Science and Systems  2012.

[19] R. C. Bradley  “Basic properties of strong mixing conditions. A survey and some open questions ” Prob-

ability Surveys  vol. 2  pp. 107–144  2005.

[20] R. Munos and C. Szepesv´ari  “Finite-time bounds for ﬁtted value iteration ” Journal of Machine Learning

Research  vol. 9  pp. 815–857  2008.

[21] A. Lazaric  M. Ghavamzadeh  and R. Munos  “Finite-sample analysis of least-squares policy iteration ”

Journal of Machine Learning Research  vol. 13  p. 30413074  2012.

[22] V. P. Godambe  Ed.  Estimating Functions. Oxford University Press  1991.
[23] S. Amari and M. Kawanabe  “Information geometry of estimating functions in semi-parametric statistical

models ” Bernoulli  vol. 3  no. 1  pp. 29–54  1997.

[24] P. J. Bickel  C. A. Klaassen  Y. Ritov  and J. A. Wellner  Efﬁcient and Adaptive Estimation for Semipara-

metric Models. Springer  1998.

[25] G. Bouchard and B. Triggs  “The tradeoff between generative and discriminative classiﬁers ” in Proceed-

ings 1998 16th IASC International Symposium on Computational Statistics  2004  pp. 721–728.

[26] E. Greensmith  P. L. Bartlett  and J. Baxter  “Variance reduction techniques for gradient estimates in

reinforcement learning ” Journal of Machine Learning Research  vol. 5  pp. 1471–1530  2004.

[27] R. Munos  “Geometric variance reduction in markov chains: application to value function and gradient

estimation ” Journal of Machine Learning Research  vol. 7  pp. 413–427  2006.

[28] T. Zhao  H. Hachiya  G. Niu  and M. Sugiyama  “Analysis and improvement of policy gradient estima-

tion ” Neural Networks  2011.

[29] T. Ueno  S. Maeda  M. Kawanabe  and S. Ishii  “Generalized TD learning ” Journal of Machine Learning

Research  vol. 12  pp. 1977–2020  2011.

9

,Omer Levy
Yoav Goldberg
Kohei Hayashi
Yuichi Yoshida
Giacomo De Palma
Bobak Kiani
Seth Lloyd