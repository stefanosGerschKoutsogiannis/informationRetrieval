2019,Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness,Machine Learning (ML) models trained on data from multiple demographic groups can inherit representation disparity (Hashimoto et al.  2018) that may exist in the data: the model may be less favorable to groups contributing less to the training process; this in turn can degrade population retention in these groups over time  and exacerbate representation disparity in the long run. In this study  we seek to understand the interplay between ML decisions and the underlying group representation  how they evolve in a sequential framework  and how the use of fairness criteria plays a role in this process. We show that the representation disparity can easily worsen over time under a natural user dynamics (arrival and departure) model when decisions are made based on a commonly used objective and fairness criteria  resulting in some groups diminishing entirely from the sample pool in the long run. It highlights the fact that fairness criteria have to be defined while taking into consideration the impact of decisions on user dynamics. Toward this end  we explain how a proper fairness criterion can be selected based on a general user dynamics model.,Group Retention when Using Machine Learning in
Sequential Decision Making: the Interplay between

User Dynamics and Fairness

University of Michigan  AnnArbor  USA

University of Michigan  AnnArbor  USA

Xueru Zhang∗

xueru@umich.edu

Mohammad Mahdi Khalili∗

khalili@umich.edu

Cem Tekin

Bilkent University  Ankara  Turkey
cemtekin@ee.bilkent.edu.tr

Mingyan Liu

University of Michigan  AnnArbor  USA

mingyan@umich.edu

Abstract

Machine Learning (ML) models trained on data from multiple demographic groups
can inherit representation disparity [7] that may exist in the data:
the model
may be less favorable to groups contributing less to the training process; this in
turn can degrade population retention in these groups over time  and exacerbate
representation disparity in the long run. In this study  we seek to understand the
interplay between ML decisions and the underlying group representation  how they
evolve in a sequential framework  and how the use of fairness criteria plays a role in
this process. We show that the representation disparity can easily worsen over time
under a natural user dynamics (arrival and departure) model when decisions are
made based on a commonly used objective and fairness criteria  resulting in some
groups diminishing entirely from the sample pool in the long run. It highlights
the fact that fairness criteria have to be deﬁned while taking into consideration the
impact of decisions on user dynamics. Toward this end  we explain how a proper
fairness criterion can be selected based on a general user dynamics model.

1

Introduction

Machine learning models developed from real-world data can inherit pre-existing bias in the dataset.
When these models are used to inform decisions involving humans  it may exhibit similar discrimi-
nation against sensitive attributes (e.g.  gender and race) [6  14  15]. Moreover  these decisions can
inﬂuence human actions  such that bias in the decision is then captured in the dataset used to train
future models. This closed feedback loop becomes self-reinforcing and can lead to highly undesirable
outcomes over time by allowing biases to perpetuate. For example  speech recognition products such
as Amazon’s Alexa and Google Home are shown to have accent bias against non-native speakers [6] 
with native speakers experience much higher quality than non-native speakers. If this difference leads
to more native speakers using such products while driving away non-native speakers  then over time
the data used to train the model may become even more skewed toward native speakers  with fewer
and fewer non-native samples. Without intervention  the resulting model becomes even more accurate
for the former and less for the latter  which then reinforces their respective user experience [7].
To address the fairness issues  one commonly used approach is to impose fairness criteria such that
certain statistical measures (e.g.  positive classiﬁcation rate  false positive rate  etc.) across different

∗Equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

demographic groups are (approximately) equalized [1]. However  their effectiveness is studied mostly
in a static framework  where only the immediate impact of the learning algorithm is assessed but
not their long-term consequences. Consider an example where a lender decides whether or not to
approve a loan application based on the applicant’s credit score. Decisions satisfying an identical
true positive rate (equal opportunity) across different racial groups can make the outcome seem
fairer [5]. However  this can potentially result in more loans issued to less qualiﬁed applicants in
the group whose score distribution skews toward higher default risk. The lower repayment among
these individuals causes their future credit scores to drop  which moves the score distribution of that
group further toward high default risk [13]. This shows that intervention by imposing seemingly fair
decisions in the short term can lead to undesirable results in the long run.
In this paper we are particularly interested in understanding what happens to group representation
over time when models with fairness guarantee are used  and how it is affected when the underlying
feature distributions are also affected/reshaped by decisions. Toward this end  we introduce a user
retention model to capture users’ reaction (stay or leave) to the decision. We show that under relatively
mild and benign conditions  group representation disparity exacerbates over time and eventually
the disadvantaged groups may diminish entirely from the system. This condition unfortunately can
be easily satisﬁed when decisions are made based on a typical algorithm (e.g.  taking objective as
minimizing the total loss) under some commonly used fairness criteria (e.g.  statistical parity  equal
of opportunity  etc.). Moreover  this exacerbation continues to hold and can accelerate when feature
distributions are affected and change over time. A key observation is that if the factors equalized
by the fairness criterion do not match what drives user retention  then the difference in (perceived)
treatment will exacerbate representation disparity over time. Therefore  fairness has to be deﬁned
with a good understanding of how users are affected by the decisions  which can be challenging in
practice as we typically have only incomplete/imperfect information. However  we show that if a
model for the user dynamics is available  then it is possible to ﬁnd the proper fairness criterion that
mitigates representation disparity.
The impact of fairness intervention on both individuals and society has been studied in [7  9  10  12 
13] and [7  9  13] are the most relevant to the present study. Speciﬁcally  [9  13] focus on the impact
on reshaping features over two time steps  while we study the impact on group representation over
an inﬁnite horizon. [7] studies group representation disparity in a sequential framework but without
inspecting the impact of fairness criteria or considering feature distributions reshaped by decision.
More on related work can be found in Appendix B.
The remainder of this paper is organized as follows. Section 2 formulates the problem. The impact of
various fairness criteria on group representation disparity is analyzed and presented in Section 3  as
well as potential mitigation. Experiments are presented in Section 4. Section 5 concludes the paper.
All proofs and a table of notations can be found in the appendices.

k(t) + α1

k(t)

k(t) the size of Gj

k ⊂ Gk the subgroup with label j  j ∈ {0  1}  k ∈ {a  b}  f j

2 Problem Formulation
Consider two demographic groups Ga  Gb distinguished based on some sensitive attribute K ∈ {a  b}
(e.g.  gender  race). An individual from either group has feature X ∈ Rd and label Y ∈ {0  1}  both
can be time varying. Denote by Gj
k t(x)
its feature distribution and αj
k as a fraction of the entire population at time t. Then
k(t) is the size of Gk as a fraction of the population and the difference between
αk(t) := α0
αa(t) and αb(t) measures the representation disparity between two groups at time step t. Denote by
k t = αj
gj
αk(t) the fraction of label j ∈ {0  1} in group k at time t  then the distribution of X over Gk
is given by fk t(x) = g1
Consider a sequential setting where the decision maker at each time makes a decision on each
individual based on feature x. Let hθ(x) be the decision rule parameterized by θ ∈ Rd and θk(t) be
the decision parameter for Gk at time t  k ∈ {a  b}. The goal of the decision maker at time t is to ﬁnd
the best parameters θa(t)  θb(t) such that the corresponding decisions about individuals from Ga  Gb
maximize its utility (or minimize its loss) in the current time. Within this context  the commonly
studied fair machine learning problem is the one-shot problem stated as follows  at time step t:

k t(x) and fa t (cid:54)= fb t.

k tf 1

k t(x) + g0

k tf 0

min
θa θb

OOOt(θa  θb; αa(t)  αb(t)) = αa(t)Oa t(θa) + αb(t)Ob t(θb) s.t. ΓC t(θa  θb) = 0  

(1)

2

where OOOt(θa  θb; αa(t)  αb(t)) is the overall objective of the decision maker at time t  which consists
of sub-objectives from two groups weighted by their group proportions.2 ΓC t(θa  θb) = 0 charac-
terizes fairness constraint C  which requires the parity of certain statistical measure (e.g.  positive
classiﬁcation rate  false positive rate  etc.) across different demographic groups. Some commonly
used criteria will be elaborated in Section 3.1. Both Ok t(θk) and ΓC t(θa  θb) = 0 depend on fk t(x).
The resulting solution (θa(t)  θb(t)) will be referred to as the one-shot fair decision under fairness C 
where the optimality only holds for a single time step t.
In this study  we seek to understand how the group representation evolves in a sequential setting over
the long run when different fairness criteria are imposed. To do so  the impact of the current decision
on the size of the underlying population is modeled by the following discrete-time retention/attrition
dynamics. Denote by Nk(t) ∈ R+ the expected number of users in group k at time t:

Nk(t + 1) = Nk(t) · πk t(θk(t)) + βk  ∀k ∈ {a  b} 

(2)
where πk t(θk(t)) is the retention rate  i.e.  the probability of a user from Gk who was in the system at
time t remaining in the system at time t + 1. This is assumed to be a function of the user experience 
which could be the actual accuracy of the algorithm or their perceived (mis)treatment. This experience
is determined by the application and is different under different contexts. For instance  in domains of
speaker veriﬁcation and medical diagnosis  it can be considered as the average loss  i.e.  a user stays
if he/she can be classiﬁed correctly; in loan/job application scenarios  it can be the rejection rates 
i.e.  user stays if he/she gets approval. βk is the expected number of exogenous arrivals to Gk and
is treated as a constant in our analysis  though our main conclusion holds when this is modeled as
a random variable. Accordingly  the relative group representation for time step t + 1 is updated as
αk(t + 1) =

Nk(t+1)

Na(t+1)+Nb(t+1)  ∀k ∈ {a  b}.

For the remainder of this paper  αa(t)
αb(t) is used to measure the group representation disparity at time t.
As αk(t) and fk t(x) change over time  the one-shot problem (1) is also time varying. In the next
section  we examine what happens to αa(t)

αb(t) when one-shot fair decisions are applied in each step.

3 Analysis of Group Representation Disparity in the Sequential Setting

Below we present results on the monotonic change of αa(t)
αb(t) when applying one-shot fair decisions in
each step. It shows that the group representation disparity can worsen over time and may lead to the
extinction of one group under a monotonicity condition stated as follows.
Monotonicity Condition. Consider two one-shot problems deﬁned in (1) with objectives

 

(cid:98)OOO(θa  θb;(cid:98)αa (cid:98)αb) and (cid:101)OOO(θa  θb;(cid:101)αa (cid:101)αb) over distributions (cid:98)fk(x)  (cid:101)fk(x) respectively. Let ((cid:98)θa (cid:98)θb) 
ity condition given a dynamic model if for any(cid:98)αa +(cid:98)αb = 1 and(cid:101)αa +(cid:101)αb = 1 such that (cid:98)αa(cid:98)αb
< (cid:101)αa(cid:101)αb
((cid:101)θa (cid:101)θb) be the corresponding fair decisions. We say that two problems(cid:98)OOO and(cid:101)OOO satisfy the monotonic-
the resulting retention rates satisfy(cid:98)πa((cid:98)θa) <(cid:101)πa((cid:101)θa) and(cid:98)πb((cid:98)θb) >(cid:101)πb((cid:101)θb).

Note that this condition is deﬁned over two one-shot problems and a given dynamic model. It is not
limited to speciﬁc families of objective or constraint functions; nor is it limited to one-dimensional
features. The only thing that matters is the group proportions within the system and the retention
rates determined by the decisions and the dynamics. It characterizes a situation where when one
group’s representation increases  the decision becomes more in favor of this group and less favorable
to the other  so that the retention rate is higher for the favored group and lower for the other.
Theorem 1. [Exacerbation of representation disparity] Consider a sequence of one-shot problems (1)
with objective OOOt(θa  θb; αa(t)  αb(t)) at each time t. Let (θa(t)  θb(t)) be the corresponding solution
and πk t(θk(t)) be the resulting retention rate of Gk  k ∈ {a  b} under a dynamic model (2). If the
  Nk(2) > Nk(1) 3 and one-shot problems in any two consecutive
initial states satisfy Na(1)
time steps  i.e.  OOOt  OOOt+1  satisfy the monotonicity condition under the given dynamic model  then
2This is a typical formulation if the objective OOOt measures the average performance of decisions over
|Ga|+|Gb| (|Ga|Oa t + |Gb|Ob t)  where Oi
t
i∈Gk

all samples  i.e.  OOOt =
Oi
measures the performance of each sample i and Ok t = 1|Gk|

|Ga|+|Gb| ((cid:80)

t is the average performance of Gk.

t +(cid:80)

Nb(1) = βa

βb

(cid:80)

t) =

i∈Ga

Oi

1

Oi

3This condition will always be satisﬁed when the system starts from a near empty state.

1

i∈Gb

3

αa(t+1)

αb(t) and πa t+1(θa(t + 1)) (cid:5) πa t(θa(t)) (cid:5) πb t(θb(t)) (cid:5) πb t+1(θb(t + 1))  ∀t.

the following holds. Let (cid:5) denote either “ < ” or “ = ” or “ > ”  if πa 1(θa(1)) (cid:5) πb 1(θb(1))  then
αb(t+1) (cid:5) αa(t)
Theorem 1 says that once a group’s proportion starts to change (increase or decrease)  it will continue
to change in the same direction. This is because under the monotonicity condition  there is a feedback
loop between representation disparity and the one-shot decisions: the former drives the latter which
results in different user retention rates in the two groups  which then drives future representation.
The monotonicity condition can be satisﬁed under some commonly used objectives  dynamics and
fairness criteria. This is characterized in the following theorem.
Theorem 2. [A case satisfying monotonicity condition] Consider two one-shot problems deﬁned in

(1) with objectives (cid:101)O(θa  θb;(cid:98)αa (cid:98)αb) =(cid:98)αaOa(θa) +(cid:98)αbOb(θb) and (cid:98)O(θa  θb;(cid:101)αa (cid:101)αb) =(cid:101)αaOa(θa) +
(cid:101)αbOb(θb) over the same distribution fk(x) with(cid:98)αa +(cid:98)αb = 1 and(cid:101)αa +(cid:101)αb = 1. Let ((cid:98)θa (cid:98)θb)  ((cid:101)θa (cid:101)θb)
be the corresponding solutions. Under the condition that Ok((cid:98)θk) (cid:54)= Ok((cid:101)θk) for all possible(cid:98)αk (cid:54)=(cid:101)αk 
if the dynamics satisfy πk(θk) = hk(Ok(θk)) for some decreasing function hk(·)  then (cid:101)O and (cid:98)O

satisfy the monotonicity condition.

The above theorem identiﬁes a class of cases satisfying the monotonicity condition; these are cases
where whenever the group proportion changes  the decision will cause the sub-objective function
value to change as well  and the sub-objective function value drives user departure.
For the rest of the paper we will focus on the one-dimensional setting. Some of the cases we consider
are special cases of Theorem 2 (Sec. 3.2). Others such as the time-varying feature distribution fk t(x)
considered in Sec. 3.3 also satisfy the monotonicity condition but are not captured by Theorem 2.

3.1 The one-shot problem
Consider a binary classiﬁcation problem based on feature X ∈ R. Let decision rule hθ(x) = 1(x ≥ θ)
be a threshold policy parameterized by θ ∈ R and L(y  hθ(x)) = 1(y (cid:54)= hθ(x)) the 0-1 loss incurred
by applying decision θ on individuals with data (x  y).
The goal of the decision maker at each time is to ﬁnd a pair (θa(t)  θb(t)) subject to criterion C
such that the total expected loss is minimized  i.e.  OOOt(θa  θb; αa(t)  αb(t)) = αa(t)La t(θa) +
αb(t)Lb t(θb)  where Lk t(θk) = g1
k t(x)dx is the expected loss Gk
f 0
experiences at time t. Some examples of ΓC t(θa  θb) are as follows and illustrated in Fig. 1.

k t(x)dx + g0

k t(cid:82) θk−∞ f 1

θk

k t(cid:82) ∞
a t(x)dx −(cid:82) ∞
fa t(x)dx −(cid:82) ∞

θb

θb

θa

f 0

θa

the same decision parameter is used for both groups.

1. Simple fair (Simple): ΓSimple t = θa − θb. Imposing this criterion simply means we ensure
2. Equal opportunity (EqOpt): ΓEqOpt t =(cid:82) ∞
b t(x)dx. This requires the
f 0
false positive rate (FPR) be the same for different groups (Fig. 1(c)) 4 i.e.  Pr(hθa (X) =
1|Y = 0  K = a) = Pr(hθb (X) = 1|Y = 0  K = b).
3. Statistical parity (StatPar): ΓStatPar t = (cid:82) ∞
fb t(x)dx. This requires
different groups be given equal probability of being labelled 1 (Fig. 1(b))  i.e.  Pr(hθa (X) =
1|K = a) = Pr(hθb (X) = 1|K = b).
4. Equalized loss (EqLos): ΓEqLos t = La t(θa) − Lb t(θb). This requires that the expected
loss across different groups be equal (Fig. 1(d)).
a  θ(cid:48)
b)

Notice that for Simple  EqOpt and StatPar criteria  the following holds: ∀t  (θa  θb)  and (θ(cid:48)
that satisfy ΓC t(θa  θb) = ΓC t(θ(cid:48)
b) = 0  we have θa ≥ θ(cid:48)
Some technical assumptions on the feature distributions are in order.
b t(x) have bounded support on
We assume f 0
a t(x)  f 0
a t(x)  f 1
0
t ] and [b1
t ]  [b0
k t(x) and
t   a1
t   a0
[a0
t   b
1
0
k t(x) overlap  i.e.  a0
t . The
f 0
t < a1
t < b
t < b
main technical assumption is stated as follows.

b t(x)  f 1
1
t ] respectively  and that f 1
t   b
t < a0
t < a1

a if and only if θb ≥ θ(cid:48)
b.

k t(x)  k ∈ {a  b}

t and b0

Fig. 2: f j

t < b1

t ]  [a1

a  θ(cid:48)

4Depending on the context  this criterion can also refer to equal false negative rate (FNR)  true positive rate

(TPR)  or true negative rate (TNR)  but the analysis is essentially the same.

4

k0tk1tk0tk1t0.000.020.04probabilitydensityf0k t(x)f1k t(x)(a) each f j

k (x) for Gj

k

(b) Statistical parity

(c) Equal opportunity

(d) Equalized Loss

Fig. 1: For Ga  Gb with group proportions α1
fair under each criterion stated in Fig. 1(b)-1(d) requires the corresponding colored areas be equal.

a = 0.55  α0

a = 0.15  α1

b = 0.1  α0

b = 0.2  a pair of (θa  θb) is

a t(x) (resp. f 0

t   a0
b t(x) and f 1

k t(x) is strictly increasing and f 0

0
t ] (resp. Tb t = [b1
t ]) be the overlapping interval between f 0
t   b
b t(x)). Distribution f 1

Assumption 1. Let Ta t = [a1
and f 1
strictly decreasing over Tk t  ∀k ∈ {a  b}.
For bell-shaped feature distributions (e.g.  Normal  Cauchy  etc.)  Assumption 1 implies that f 1
k t(x)
and f 0
k t(x) are sufﬁciently separated. An example is shown in Fig. 2. As we show later  this
assumption helps us establish the monotonic convergence of decisions (θa(t)  θb(t)) but is not
necessary for the convergence of group representation. We next ﬁnd the one-shot decision to this
problem under Simple  EqOpt  and StatPar fairness criteria.
Lemma 1. Under Assumption 1  ∀k ∈ {a  b}  the optimal decision at time t for Gk without
considering fairness is

a t(x)
k t(x) is

k tf 1

k(t)  k

θk

0

0
t ) > g0

k tf 0

k t(k

0
t )

t   θ∗

k tf 1

k t(k

k tf 0
k tf 0
k tf 0

k tf 1
k tf 1
k tf 1

k t(k1
k t(k1
k t(k

θ∗
k(t) = arg min

a(t) = δa t and θ∗

Lk t(θk) =

k(t)] and increasing over [θ∗

k t(δk t). Moreover  Lk t(θk) is decreas-

k t(k1
t )
k t(k1
t ) & g1
0
k t(k
t )

t ) ≥ g0
t ) < g0
t ) ≤ g0
k t(δk t) = g0
k tf 0
1
t ].

k1
t   if g1
δk t  if g1
0
t   if g1
k
where δk t ∈ Tk t is deﬁned such that g1
ing in θk over [k0
Below we will focus on the case when θ∗
b (t) = δb t  while analysis for the other
cases are essentially the same. For Simple  StatPar and EqOpt fairness  ∃ a strictly increasing
function φC t  such that ΓC t(φC t(θb)  θb) = 0. Denote by φ−1C t the inverse of φC t. Without loss of
generality  we will assign group labels a and b such that φC t(δb t) < δa t and φ−1C t(δa t) > δb t  ∀t. 5
Lemma 2. Under Simple  EqOpt  StatPar fairness criteria  one-shot fair decision at time t satisﬁes
(θ∗
a(t)  θ∗
αa(t)La t(θa)+αb(t)Lb t(θb) ∈ {(θa  θb)|θa ∈ [φC t(δb t)  δa t]  θb ∈
[δb t  φ−1C t(δa t)]  ΓC t(θa  θb) = 0} (cid:54)= ∅ regardless of group proportions αa(t)  αb(t).
Lemma 2 shows that given feature distributions fa t(x)  fb t(x)  although one-shot fair decisions can
be different under different group proportions αa(t)  αb(t)  these solutions are all bounded by the
same compact intervals (Fig. 3). Theorem 3 below describes the more speciﬁc relationship between
group representation αa(t)
Theorem 3. [Impact of group representation disparity on the one-shot decision] Consider the
one-shot problem with group proportions αa(t)  αb(t) at time step t  let (θa(t)  θb(t)) be the corre-
sponding one-shot decision under either Simple  EqOpt or StatPar criterion. Under Assumption 1 
(θa(t)  θb(t)) is unique and satisﬁes the following:

αb(t) and the corresponding one-shot decision (θa(t)  θb(t)).

b (t)) = arg minθa θb

ΨC t(θa(t)  θb(t)) =

αa(t)
αb(t)

 

(3)

where ΨC t is some function increasing in θa(t) and θb(t)  with details illustrated in Table 1.

5If the change of fa t(x) and fb t(x) w.r.t. the decisions follows the same rule (e.g.  examples given in

Section 3.3)  then this relationship holds ∀t.

5

probability densityθa ∈ [a0

t   a1

(cid:16) g1

b t

g0

b t

f 1
b t(θb)
f 0
b t(θb)

t ]  θb ∈ Tb t
(cid:17) g0

− 1

b t

g0

a t

EqOpt

StatPar

1 −

Simple

2

b t

f 1
f 0

b t

(θb )
(θb )

+1

b t

g1
g0

b t

(cid:16)

1 −

b t

g1
g0

b t

θa ∈ Ta t  θb ∈ Tb t

θa ∈ Ta t  θb ∈ [b

0
t   b

1
t ]

b t

g1
b t
g0
1− g1
g0

b t

−1

f 1
b t (θb )
f 0
(θb )
f 1
a t(θa )
f 0
a t(θa )

(cid:17)(cid:16)

a t

a t

2

g0
b t
g0

a t

f 1
f 0

b t

+1

(θb )
(θb )
b t
b t(θb)−g0
b tf 1
g1
a t(θa)−g1
g0
a tf 0

2
a tf 1
1− g1
a t(θa)
g0
a tf 0
a t(θa)

b tf 0
a tf 1

b t(θb)
a t(θa)

(cid:17)

− 1

− 1

2
f 1
a t(θa )
f 0
a t(θa )

1− g1
g0

a t

a t

Table 1: The form of ΨC t(θa  θb) for C = EqOpt  StatPar  Simple.6

k t(θk)
k t(θk) and g1

k tf 1
Note that under Assumption 1  both g1
k t(θk) are strictly increasing
k tf 0
g0
in θk ∈ Tk t  k ∈ {a  b}  and θa(t) = φC t(θb(t)) for some strictly increasing function. According
to ΨC t(θa  θb) given in Table 1  the larger αa(t)
k t(θk) −
k t(θk)  thus the larger θa(t) and θb(t). The above theorem characterizes the impact of the
g0
k tf 0
underlying population on the one-shot decisions. Next we investigate how the one-shot decision
impacts the underlying population.

αb(t) results in the larger g1

k t(θk)
k t(θk) and g1

k t(θk)− g0

k tf 1
k tf 0
g0

k tf 0

k tf 1

k tf 1

3.2 Participation dynamics

How a user reacts to the decision is captured by the retention dynamics (2) which is fully characterized
by the retention rate. Below we introduce two types of (perceived) mistreatment as examples when
the monotonicity condition is satisﬁed.
(1) User departure driven by model accuracy: Examples include discontinuing the use of products
viewed as error-prone  e.g.  speech recognition software  or medical diagnostic tools. In these
cases  the determining factor is the classiﬁcation error  i.e.  users who experience low accuracy
have a higher probability of leaving the system. The retention rate at time t can be modeled as
πk t(θk) = ν(Lk t(θk)) for some strictly decreasing function ν(·) : [0  1] → [0  1].
(2) User departure driven by intra-group disparity: Participation can also be affected by intra-
group disparity  that between users from the same demographic group but with different labels  i.e. 
Gj
k for j ∈ {0  1}. An example is in making ﬁnancial assistance decisions where one expects to
see more awards given to those qualiﬁed than to those unqualiﬁed. Denote by Dk t(θk) = Pr(Y =
1  hθk (X) = 1|K = k)−Pr(Y = 0  hθk (X) = 1|K = k) =(cid:82) ∞
k t(x)(cid:1)dx as intra-
θk (cid:0)g1
group disparity of Gk at time t  then the retention rate can be modeled as πk t(θk) = w(Dk t(θk))
for some strictly increasing function w(·) mapping to [0  1].
Theorem 4. Consider the one-shot problem (1) deﬁned in Sec. 3.1 under either Simple  EqOpt or
StatPar criterion  and assume distributions fk t(x) = fk(x) are ﬁxed over time. Then the one-
shot problems in any two consecutive time steps  i.e.  OOOt  OOOt+1  satisfy the monotonicity condition
under dynamics (2) with πk(·) being either ν(Lk(·)) or w(Dk(·)).7 This implies that Theorem 1
holds and (θa(t)  θb(t)) converges monotonically to a constant decision (θ∞
b ). Furthermore 
lim
t→∞

1−πb(θ∞
b )
a ) .
1−πa(θ∞

k t(x)−g0

αb(t) = βa

a   θ∞

kf 0

kf 1

αa(t)

βb

When distributions are ﬁxed  the discrepancy between πa(θa(t)) and πb(θb(t)) increases over time
as (θa(t)  θb(t)) changes. The process is illustrated in Fig. 3  where θa(t) ∈ [φC(δb)  δa]  θb(t) ∈
[δb  φ−1C (δa)] are constrained by the same interval ∀t. Left and right plots illustrate cases when
πk(θk) = ν(Lk(θk)) and πk(θk) = w(Dk(θk)) respectively.
Note that the case considered in Theorem 4 is a special case of Theorem 2  with distributions fk t(x) =
fk(x) ﬁxed  Ok(θk) = Lk(θk) and both dynamics πk(·) = ν(Lk(·)) and πk(·) = w(Dk(·)) some
6The cases represented by blank cells cannot happen. When C = Simple  the table only illustrates the result
when δa t  δb t ∈ Ta t ∩ Tb t (cid:54)= ∅.
7When fk t(x) = fk(x)  ∀t  subscript t is omitted in some notations (φC t  δk t  πk t  etc.) for simplicity.

6

decreasing functions of Lk(·).8 In this special case we obtain the additional result of monotonic
convergence of decisions  which holds due to Assumption 1.
Once αa(t)
αb(t) starts to increase  the corre-
sponding one-shot solution (θa(t)  θb(t))
also increases (Theorem 3)  meaning that
θa(t) moves closer to θ∗
a = δa and θb(t)
moves further away from θ∗
b = δb (solid ar-
rows in Fig. 3). Consequently  La(θa(t))
and Db(θb(t)) decrease while Lb(θb(t))
and Da(θa(t)) increase. Under both dy-
namics  πa(θa(t)) increases and πb(θb(t))
decreases  resulting in the increase of
αa(t+1)
αb(t+1) ; the feedback loop becomes self-
reinforcing and representation disparity
worsens.

Fig. 3: Illustration of Lk(θk) and Dk(θk) w.r.t. θk: Each black
triangle represents the one-shot decision θk; size of the colored
area represents the value of Lk(θk) (left) or Dk(θk) (right). Note
that for the right plot  there are two gray regions and the darker
one is for compensating the lighter one thus they are of the same
size; the smaller gray regions result in the larger Da(θa).

3.3

Impact of decisions on reshaping feature distributions

Fig. 4: Visualization of decisions
shaping feature distributions.

k  G1

k tf 1

k t(x) + g0

k tf 0

k t(x) = f j

k (x) remain ﬁxed but gj

k t changes over time given Gj

k t+1 > gi
k but for subgroup Gi

k t 9 In other words  for i ∈ {0  1} and t ≥ 2 such that Li
k t  where −i := {0  1} \ {i}.

Our results so far show the potential adverse impact on group rep-
resentation when imposing certain fairness criterion  while their
underlying feature distributions are assumed ﬁxed. Below we
examine what happens when decisions also affect feature distri-
butions over time  i.e.  fk t(x) = g1
k t(x)  which
is not captured by Theorem 2. We will focus on the dynamics
πk t(θk) = ν(Lk t(θk)). Since G0
k may react differently to the
same θk  we consider two scenarios as illustrated in Fig. 4  which
shows the change in distribution from t to t + 1 when G1
k (resp.
k) experiences the higher (resp. lower) loss at t than t − 1 (see
G0
Appendix I for more detail): ∀j ∈ {0  1} 
Case (i): f j
by its perceived loss Lj
k t−1(θk(t − 1))  we have gi
Li
Case (ii): gj
make extra effort such that f i
words  for i ∈ {0  1} and t ≥ 2 such that Li
k t(x)  ∀x ∈ Tk  while f−i
f i
In both cases  under the condition that fk t(x) is relatively insensitive to the change in one-shot
decisions  representation disparity can worsen and deterioration accelerates. The precise conditions
are formally given in Conditions 1 and 2 in Appendix I  which describes the case where the change
from fk t(x) to fk t+1(x) is sufﬁciently small while the change from αa(t)
αb(t+1) and the
resulting decisions from θk(t) to θk(t + 1) are sufﬁciently large. These conditions hold in scenarios
when the change in feature distributions induced by the one-shot decisions is a slow process.
Theorem 5. [Exacerbation in representation disparity can accelerate] Consider the one-shot problem
deﬁned in (1) under either Simple  EqOpt or StatPar fairness criterion. Let the one-shot decision 
representation disparity and retention rate at time t be given by θf
k (t))
when distribution fk(x) is ﬁxed ∀t. Let the same be denoted by θr
k(t))
when fk t(x) changes according to either case (i) or (ii) deﬁned above. Assume we start from the

k t and g−i
k that is less favored by the decision over time  its members
k t(x) skews toward the direction of lowering their losses.10 In other
k t+1(x) <

k (t)  αf
a(t)
  and πf
αf
b (t)
k(t)  αr
a(t)
b (t)   and πr

k t(θf
k t(θr

k t(x)  ∀x  where −i := {0  1} \ {i}.

k’s retention determined
k t(θk(t)) <

k t−1(θk(t − 1))  we have f i

k t+1(x) = f−i

αb(t) to αa(t+1)

k t+1 < g−i

k t(θk(t)) > Li

k t = gj

αr

k t(θk) =(cid:82) θk−∞ f 1

k − Lk(θ).
8By Fig. 3  we have Dk(θ) = g1
9Here L1
k t(x)dx and L0
10Suppose Assumption 1 holds for all f j

k t(θk) =(cid:82) ∞

θk

k t(x)dx.
f 0

overlap over Tk = [k1  k

0

]  ∀t.

k t(x) and their support does not change  then f 1

k t(x) and f 0

k t(x)

7

C(b)a0.000.020.04densityag0af0a(x)g1af1a(x)b1C(a)0.000.020.04densitybg0bf0b(x)g1bf1b(x)C(b)a0.000.020.04densityag0af0a(x)g1af1a(x)b1C(a)0.000.020.04densitybg0bf0b(x)g1bf1b(x)k0tk1tk0tk1t0.000.030.05densityg0k tf0k(x)↑g1k tf1k(x)↓g0k t+1f0k(x)g1k t+1f1k(x)Case(i)fjk(x)gjk tfjk t(x)gjk t+1fjk t+1(x)k0tk1tk0tk1t0.000.030.05densityg0kf0k t(x)=g0kf1k t(x)→g0kf0k t+1(x)g1kf1k t+1(x)Case(ii)same distribution fk 1(x) = fk(x). Under Conditions 1 and 2 in Appendix I  if πf
πr
a 1(θr
αf
a(t+1)
αf
b (t+1)

a(1)) (cid:5) πf
(accelerates)  ∀t  where (cid:5) represents either “ < ”or “ > ”.

a 1(θf
b (t) (disparity worsens) and αr

b (t+1) (cid:5) αr

b (1))  then αr

b (1)) = πr

b 1(θf

b 1(θr

a(t+1)

a(t)

αr

αr

αr

a (1)) =
a(t+1)
b (t+1) (cid:5)

3.4 Potential mitigation & ﬁnding the proper fairness criterion from participation dynamics

βb

αa(t)

αb(t) = βa

The above results show that when the objective is to minimize the average loss over the entire
population  applying commonly used and seemingly fair decisions at each time can exacerbate
representation disparity over time under reasonable participation dynamics. It highlights the fact
that fairness has to be deﬁned with a good understanding of how users are affected by the algorithm 
and how they may react to it. For instance  consider the dynamics with πk t(θk) = ν(Lk t(θk)) 
then imposing EqLos fairness (Fig. 1(d)) at each time step would sustain group representations  i.e. 
  as we are essentially equalizing departure when equalizing loss. In contrast  under
lim
t→∞
other fairness criteria the factors that are equalized do not match what drives departure  and different
losses incurred to different groups cause signiﬁcant change in group representation over time.
In reality the true dynamics is likely a function of a mixture of factors given the application context 
and a proper fairness constraint C should be adopted accordingly. Below we illustrate a method for
ﬁnding the proper criterion from a general dynamics model deﬁned below when fk t(x) = fk(x) ∀t:
(4)
m=1 (e.g. accuracy  true
where user retention in Gk is driven by M different factors {πm
positives  etc.) and each of them depends on decision θk(t). Constant βk is the intrinsic growth
k (θk(t)). The expected number of users at time
rate while the actual arrivals may depend on πm
t + 1 depends on users at t and new users; both may be effected by πm
k (θk(t)). This relationship is
characterized by a general function Λ. Let Θ be the set of all possible decisions.
Assumption 2. ∃(θa  θb) ∈ Θ × Θ such that ∀k ∈ {a  b}  ˆNk = Λ( ˆNk {πm
k (θk)}M
|Λ(cid:48)( ˆNk {πm
(θa  θb) have stable ﬁxed points  where Λ(cid:48) denotes the derivative of Λ with respect to Nk.
To ﬁnd the proper fairness constraint  let C be the set of decisions (θa  θb) that can sustain group
representation. It can be found via the following optimization problem; the set of feasible solutions is
guaranteed to be non-empty under Assumption 2.

m=1  βk) and
m=1  βk)| < 1 hold for some ˆNk  i.e.  dynamics (4) under some decision pairs

m=1  βk)  ∀k ∈ {a  b} 
k (θk(t))}M

Nk(t + 1) = Λ(Nk(t) {πm

k (θk(t))}M

k (θk)}M

k (θk)}M

m=1  βk) ∈ R+  θk ∈ Θ ∀k ∈ {a  b}.

βa

˜Na
˜Nb −

(θa θb) (cid:12)(cid:12)(cid:12)

βb(cid:12)(cid:12)(cid:12) s.t. ˜Nk = Λ( ˜Nk {πm

C = arg min
The idea is to ﬁrst select decision pairs whose
corresponding dynamics can lead to stable ﬁxed
points ( ˜Na  ˜Nb); then among them select those
that are best in sustaining group representation 
which may or may not be unique. Sometimes
guaranteeing the perfect fairness can be unre-
alistic and a relaxed version is preferred  in
˜Na
which case all pairs (θa  θb) satisfying |
˜Nb −
βb |}+∆ constitute the ∆-fair
βb | ≤ min{|
set. An example under dynamics Nk(t + 1) =
k(θk(t)) is illustrated in
Nk(t)π2
Fig. 5  where all curves with  ≤ ∆ βb
consti-
tute ∆-fair set (perfect fairness set is given by
the deepest red curve with  = 0). See Appendix
K for more details.

k(θk(t)) + βkπ1

˜Nb − βa

˜Na

βa

βa

k(θk) = ν((cid:82) ∞

θk

fk(x)dx)  π1
k(θk) = ν(Lk(θk))  π1

Fig. 5: Left plot: π2
k(θk) =
ν(Lk(θk)); right plot: π2
k(θk) =
1  and ν(x) = 1 − x. Value of each pair (θa  θb) corre-
| measuring how well it can sustain
sponds to | ˜Na
˜Nb
the group representation. All points (θa  θb) with the
same value of | ˜Na
 form a curve of the
˜Nb
same color with  ∈ [0  1] shown in the color bar.

| = βa

− βa

− βa

βb

βb

βb

4 Experiments

We ﬁrst performed a set of experiments on synthetic data where every Gj
k  k ∈ {a  b}  j ∈ {0  1}
follows the truncated normal (Fig. 2) distributions. A sequence of one-shot fair decisions are used

8

and group representation changes over time according to dynamics (2) with πk(θk) = ν(Lk(θk)).
Parameter settings and more experimental results (e.g.  sample paths  results under other dynamics
and when feature distributions are learned from data) are presented in Appendix L.

(a) Simple fair

(b) StatPar fair

(c) EqOpt fair

(d) EqLos fair

Fig. 6: Each dot in Fig. 6(a)-6(d) represents the ﬁnal group proportion limt→∞ αa(t) of one sample path under
a pair of arriving rates (βa  βb). If the group representation is sustained  then limt→∞ αa(t) =
for
each pair of (βa  βb)  as shown in Fig. 6(d) under EqLos fairness. However  under Simple  StatPar and
EqOpt fairness  limt→∞ αa(t) = 1/(1 + βb(1−ν(La(θ∞
βa(1−ν(Lb(θ∞

1+βb/βa

1

a )))
b ))) ).

Fig. 6 illustrates the ﬁnal group proportion (the converged state) limt→∞ αa(t) as a function of the
exogenous arrival sizes βa and βb under different fairness criteria. With the exception of EqLos
fairness  group representation is severely skewed in the long run 
with the system consisting mostly of Gb  even for scenarios when
Ga has larger arrival  i.e.  βa > βb. Moreover  decisions under an
inappropriate fairness criterion (Simple  EqOpt or StatPar) can
result in poor robustness  where a minor change in βa and βb can
result in very different representation in the long run (Fig. 6(b)).
We also consider the dynamics presented in Fig. 5 and show the
effect of ∆ =  βa
-fair decision found with method in Sec. 3.4
βb
on αa(t). Each curve in Fig. 7 represents a sample path under
different  where (θa(t)  θb(t)) is from a small randomly selected
subset of ∆-fair set  ∀t (to model the situation where perfect
fairness is not feasible) and βa = βb. We observe that fairness
is always violated at the beginning in lower plot even with small
. This is because the fairness set is found based on stable ﬁxed
points  which only concerns fairness in the long run.
We also trained binary classiﬁers over Adult dataset [4] by min-
imizing empirical loss where features are individual data points
such as sex  race  and nationality  and labels are their annual
income (≥ 50k or < 50k). Since the dataset does not reﬂect
dynamics  we employ (2) with πk(θk) = ν(Lk(θk)) and βa = βb.
We examine the monotonic convergence of representation dis-
parity under Simple  EqOpt (equalized false positive/negative
cost(FPC/FNC)) and EqLos  and consider cases where Ga  Gb
are distinguished by the three features mentioned above. These
results are shown in Fig. 8.

Fig. 7: Effect of ∆-fair decisions
found with proposed method.

Fig. 8: Illustration of group represen-
tation disparity using Adult dataset.

5 Conclusion

This paper characterizes the impact of fairness intervention on group representation in a sequential
setting. We show that the representation disparity can easily get exacerbated over time under relatively
mild conditions. Our results suggest that fairness has to be deﬁned with a good understanding of
participation dynamics. Toward this end  we develop a method of selecting a proper fairness criterion
based on prior knowledge of participation dynamics. Note that we do not always have full knowledge
of participation dynamics; modeling dynamics from real-world measurements and ﬁnding a proper
fairness criterion based on the obtained model is a potential direction for future work.

9

200040006000800010000b200040006000800010000a0.20.40.60.8200040006000800010000b200040006000800010000a0.20.40.60.8200040006000800010000b200040006000800010000a0.20.40.60.8200040006000800010000b200040006000800010000a0.20.40.60.80102030400.300.400.50αa(t)π2k(θk)=ν(Lk(θk)) π1k(θk)=1=0.02=0.1=0.3=0.7=1.0010203040t0.430.450.480.50αa(t)π2k(θk)=ν(R∞θkfk(x)dx) π1k(θk)=ν(Lk(θk))=0.001=0.1=0.3=0.5=0.8010203040t0.430.450.480.500.530.550.580.600.62a(t)SimpleEqual FPCEqual FNCEqual Lossrace: White (Ga) vs. Non-white (Gb)sex: Female (Ga) vs. Male (Gb)nationality: US (Ga) vs. Non-US (Gb)Acknowledgments

This work is supported by the NSF under grants CNS-1616575  CNS-1646019  CNS-1739517. The
work of Cem Tekin was supported by BAGEP 2019 Award of the Science Academy.

References
[1] Solon Barocas  Moritz Hardt  and Arvind Narayanan. Fairness and Machine Learning. fairml-

book.org  2019. http://www.fairmlbook.org.

[2] Avrim Blum  Suriya Gunasekar  Thodoris Lykouris  and Nati Srebro. On preserving non-
discrimination when combining expert advice. In Advances in Neural Information Processing
Systems  pages 8386–8397  2018.

[3] Christos Dimitrakakis  Yang Liu  David C Parkes  and Goran Radanovic. Bayesian fairness. In

Proceedings of the AAAI Conference on Artiﬁcial Intelligence  pages 509–516  2019.

[4] Dheeru Dua and Casey Graff. UCI machine learning repository  2017. http://archive.ics.

uci.edu/ml.

[5] Moritz Hardt  Eric Price  Nati Srebro  et al. Equality of opportunity in supervised learning. In

Advances in neural information processing systems  pages 3315–3323  2016.

[6] Drew Harwell. Amazon’s alexa and google home show accent bias  with chinese and spanish

hardest to understand. 2018. http://bit.ly/2QFA1MR.

[7] Tatsunori Hashimoto  Megha Srivastava  Hongseok Namkoong  and Percy Liang. Fairness
without demographics in repeated loss minimization. In Proceedings of the 35th International
Conference on Machine Learning  pages 1929–1938  2018.

[8] Hoda Heidari and Andreas Krause. Preventing disparate treatment in sequential decision
making. In Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence 
pages 2248–2254  2018.

[9] Hoda Heidari  Vedant Nanda  and Krishna P. Gummadi. On the long-term impact of algorithmic
decision policies: Effort unfairness and feature segregation through social learning. Proceedings
of the 39th International Conference on Machine Learning  pages 2692–2701  2019.

[10] Lily Hu and Yiling Chen. A short-term intervention for long-term fairness in the labor market.
In Proceedings of the 2018 World Wide Web Conference on World Wide Web  pages 1389–1398 
2018.

[11] Shahin Jabbari  Matthew Joseph  Michael Kearns  Jamie Morgenstern  and Aaron Roth. Fairness
in reinforcement learning. In Proceedings of the 34th International Conference on Machine
Learning  pages 1617–1626  2017.

[12] Sampath Kannan  Aaron Roth  and Juba Ziani. Downstream effects of afﬁrmative action. In
Proceedings of the Conference on Fairness  Accountability  and Transparency  pages 240–248 
2019.

[13] Lydia T. Liu  Sarah Dean  Esther Rolf  Max Simchowitz  and Moritz Hardt. Delayed impact
of fair machine learning. In Proceedings of the 35th International Conference on Machine
Learning  pages 3150–3158  2018.

[14] Lauren Rhue. Emotion-reading tech fails the racial bias test. 2019. http://bit.ly/2Ty3aLG.

[15] Abhishek Tiwari. Bias and fairness in machine learning. 2017. http://bit.ly/2RIr89A.

[16] Isabel Valera  Adish Singla  and Manuel Gomez Rodriguez. Enhancing the accuracy and
fairness of human decision making. In Advances in Neural Information Processing Systems 
pages 1769–1778  2018.

[17] Chongjie Zhang and Julie A Shah. Fairness in multi-agent sequential decision-making. In

Advances in Neural Information Processing Systems  pages 2636–2644  2014.

10

,Xueru Zhang
Mohammadmahdi Khaliligarekani
Cem Tekin
mingyan liu