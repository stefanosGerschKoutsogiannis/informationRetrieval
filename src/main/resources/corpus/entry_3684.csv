2018,Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes,While designing the state space of an MDP  it is common to include states that are transient or not reachable by any policy (e.g.  in mountain car  the product space of speed and position contains configurations that are not physically reachable). This results in weakly-communicating or multi-chain MDPs. In this paper  we introduce TUCRL  the first algorithm able to perform efficient exploration-exploitation in any finite Markov Decision Process (MDP) without requiring any form of prior knowledge. In particular  for any MDP with $S^c$ communicating states  $A$ actions and $\Gamma^c \leq S^c$ possible communicating next states  we derive a $O(D^c \sqrt{\Gamma^c S^c A T}) regret bound  where $D^c$ is the diameter (i.e.  the length of the longest shortest path between any two states) of the communicating part of the MDP. This is in contrast with optimistic algorithms (e.g.  UCRL  Optimistic PSRL) that suffer linear regret in weakly-communicating MDPs  as well as posterior sampling or regularised algorithms (e.g.  REGAL)  which require prior knowledge on the bias span of the optimal policy to bias the exploration to achieve sub-linear regret. We also prove that in weakly-communicating MDPs  no algorithm can ever achieve a logarithmic growth of the regret without first suffering a linear regret for a number of steps that is exponential in the parameters of the MDP. Finally  we report numerical simulations supporting our theoretical findings and showing how TUCRL overcomes the limitations of the state-of-the-art.,Near Optimal Exploration-Exploitation in

Non-Communicating Markov Decision Processes

Ronan Fruit

Sequel Team - Inria Lille
ronan.fruit@inria.fr

Matteo Pirotta

Sequel Team - Inria Lille

matteo.pirotta@inria.fr

Alessandro Lazaric
Facebook AI Research

lazaric@fb.com

Abstract

While designing the state space of an MDP  it is common to include states that are
transient or not reachable by any policy (e.g.  in mountain car  the product space of
speed and position contains conﬁgurations that are not physically reachable). This
results in weakly-communicating or multi-chain MDPs. In this paper  we introduce
TUCRL  the ﬁrst algorithm able to perform efﬁcient exploration-exploitation in
any ﬁnite Markov Decision Process (MDP) without requiring any form of prior
knowledge. In particular  for any MDP with SC communicating states  A actions

and ΓC ≤ SC possible communicating next states  we derive a (cid:101)O(DC√ΓCSCAT )

regret bound  where DC is the diameter (i.e.  the length of the longest shortest
path between any two states) of the communicating part of the MDP. This is in
contrast with existing optimistic algorithms (e.g.  UCRL  Optimistic PSRL) that
suffer linear regret in weakly-communicating MDPs  as well as posterior sampling
or regularised algorithms (e.g.  REGAL)  which require prior knowledge on the bias
span of the optimal policy to achieve sub-linear regret. We also prove that in weakly-
communicating MDPs  no algorithm can ever achieve a logarithmic growth of the
regret without ﬁrst suffering a linear regret for a number of steps that is exponential
in the parameters of the MDP. Finally  we report numerical simulations supporting
our theoretical ﬁndings and showing how TUCRL overcomes the limitations of the
state-of-the-art.

Introduction

1
Reinforcement learning (RL) [1] studies the problem of learning in sequential decision-making
problems where the dynamics of the environment is unknown  but can be learnt by performing
actions and observing their outcome in an online fashion. A sample-efﬁcient RL agent must trade
off the exploration needed to collect information about the environment  and the exploitation of
the experience gathered so far to gain as much reward as possible. In this paper  we focus on the
regret framework in inﬁnite-horizon average-reward problems [2]  where the exploration-exploitation
performance is evaluated by comparing the rewards accumulated by the learning agent and an optimal
policy. Jaksch et al. [2] showed that it is possible to efﬁciently solve the exploration-exploitation
dilemma using the optimism in face of uncertainty (OFU) principle. OFU methods build conﬁdence
intervals on the dynamics and reward (i.e.  construct a set of plausible MDPs)  and execute the optimal
policy of the “best” MDP in the conﬁdence region [e.g.  2  3  4  5  6]. An alternative approach is
posterior sampling (PS) [7]  which maintains a posterior distribution over MDPs and  at each step 
samples an MDP and executes the corresponding optimal policy [e.g.  8  9  10  11  12].
Weakly-communicating MDPs and misspeciﬁed states. One of the main limitations of UCRL [2]
and optimistic PSRL [12] is that they require the MDP to be communicating so that its diameter
D (i.e.  the length of the longest path among all shortest paths between any pair of states) is ﬁnite.
While assuming that all states are reachable may seem a reasonable assumption  it is rarely veriﬁed in
practice. In fact  it requires a designer to carefully deﬁne a state space S that contains all reachable

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(a) Breakout

(b) Mountain Car

Figure 1: Examples of non-communicating domains. Fig. b represents a phase plane plot of the
Mountain car domain (x  ˙x) ∈ [−1.2  0.6] × [−0.07  0.07]. The initial state is (−0.5  0) and the red
area corresponds to non-reachable states from the initial state. Other non-reachable states may exist.
Fig. a shows the initial state  one reachable state (middle) and an unreachable one (right).

states (otherwise it may not be possible to learn the optimal policy)  but it excludes unreachable
states (otherwise the resulting MDP would be non-communicating). This requires a considerable
amount of prior knowledge about the environment. Consider a problem where we learn from images
e.g.  the Atari Breakout game [13]. The state space is the set of “plausible” conﬁgurations of the
brick wall  ball and paddle positions. The situation in which the wall has an hole in the middle is a
valid state (e.g.  as an initial state) but it cannot be observed/reached starting from a dense wall (see
Fig. 1a). As such  it should be removed to obtain a “well-designed” state space. While it may be
possible to design a suitable set of “reachable” states that deﬁne a communicating MDP  this is often
a difﬁcult and tedious task  sometimes even impossible. Now consider a continuous domain e.g.  the
Mountain Car problem [14]. The state is decribed by the position x and velocity ˙x along the x-axis.
The state space of this domain is usually deﬁned as the cartesian product [−1.2  0.6] × [−0.07  0.07].
Unfortunately  this set contains conﬁgurations that are not physically reachable as shown on Fig. 1b.
The dynamics of the system is constrained by the evolution equations. Therefore  the car can not go
arbitrarily fast. On the leftmost position (x = −1.2) the speed ˙x cannot exceed 0 due to the fact that
such position can be reached only with velocity ˙x ≤ 0. To have a higher velocity  the car would need
to acquire momentum from further left (i.e.  x < −1.2) which is impossible by design (−1.2 is the
left-boundary of the position domain). The maximal speed reachable for x > −1.2 can be attained by
applying the maximum acceleration at any time step starting from the state (x  ˙x) = (−1.2  0). This
identiﬁes the curve reported in the Fig. 1b which denotes the boundary of the unreachable region.
Note that other states may not be reachable. Whenever the state space is misspeciﬁed or the MDP is
weakly communicating (i.e.  D = +∞)  OFU-based algorithms (e.g. UCRL) optimistically attribute
large reward and non-zero probability to reach states that have never been observed  and thus they
tend to repeatedly attempt to explore unreachable states. This results in poor performance and linear
regret. A ﬁrst attempt to overcome this major limitation is REGAL.C [3] (Fruit et al. [6] recently
proposed SCAL  an implementable efﬁcient version of REGAL.C)  which requires prior knowledge of
an upper-bound H to the span (i.e.  range) of the optimal bias function h∗. The optimism of UCRL
is then “constrained” to policies whose bias has span smaller than H. This implicitly “removes”
non-reachable states  whose large optimistic reward would cause the span to become too large.
Unfortunately  an accurate knowledge of the bias span may not be easier to obtain than designing
a well-speciﬁed state space. Bartlett and Tewari [3] proposed an alternative algorithm – REGAL.D–
that leverages on the doubling trick [15] to avoid any prior knowledge on the span. Nonetheless 
we recently noticed a major ﬂaw in the proof of [3  Theorem 3] that questions the validity of the
algorithm (see App. A for further details). PS-based algorithms also suffer from similar issues.1 To
the best of our knowledge  the only regret guarantees available in the literature for this setting are
[17  18  19]. However  the counter-example of Osband and Roy [20] seems to invalidate the result of
Abbasi-Yadkori and Szepesvári [17]. On the other hand  Ouyang et al. [18] and Theocharous et al.
[19] present PS algorithms with expected Bayesian regret scaling linearly with H  where H is an
upper-bound on the optimal bias spans of all the MDPs that can be drawn from the prior distribution
([18  Asm. 1] and [19  Sec. 5]). In [18  Remark 1]  the authors claim that their algorithm does not
require the knowledge of H to derive the regret bound. However  in App. B we show on a very simple
example that for most continuous prior distributions (e.g.  uninformative priors like Dirichlet)  it is
very likely that H = +∞ implying that the regret bound may not hold (similarly for [19]). As a
1We notice that the problem of weakly-communicating MDPs and misspeciﬁed states does not hold in the
more restrictive setting of ﬁnite horizon [e.g.  8] since exploration is directly tailored to the states that are
reachable within the known horizon  or under the assumption of the existence of a recurrent state [e.g.  16].

2

Initialstates1Reachablefroms1NOTreachablefroms1−1.2−1−0.8−0.6−0.4−0.200.20.4−0.0500.05Unreachablestatess1PositionxVelocity˙xresult  similarly to REGAL.C  the prior distribution should contain prior knowledge on the bias span to
avoid poor performance.
In this paper  we present TUCRL  an algorithm designed to trade-off exploration and exploitation in
weakly-communicating and multi-chain MDPs (e.g.  MDPs with misspeciﬁed states) without any
prior knowledge and under the only assumption that the agent starts from a state in a communicating
subset of the MDP (Sec. 3). In communicating MDPs  TUCRL eventually (after a ﬁnite number
of steps) performs as UCRL  thus achieving problem-dependent logarithmic regret. When the

true MDP is weakly-communicating  we prove that TUCRL achieves a (cid:101)O(√T ) regret that with

polynomial dependency on the MDP parameters. We also show that it is not possible to design
an algorithm achieving logarithmic regret in weakly-communicating MDPs without having an
exponential dependence on the MDP parameters (see Sec. 5). TUCRL is the ﬁrst computationally
tractable algorithm in the OFU literature that is able to adapt to the MDP nature without any prior
knowledge. The theoretical ﬁndings are supported by experiments on several domains (see Sec. 4).
2 Preliminaries
We consider a ﬁnite weakly-communicating Markov decision process [21  Sec. 8.3] M = (cid:104)S A  r  p(cid:105)
with a set of states S and a set of actions A =(cid:83)s∈S As. Each state-action pair (s  a) ∈ S × As
is characterized by a reward distribution with mean r(s  a) and support in [0  rmax] as well as a
transition probability distribution p(·|s  a) over next states. In a weakly-communicating MDP  the
state-space S can be partioned into two subspaces [21  Section 8.3.1]: a communicating set of states
(denoted SC in the rest of the paper) with each state in SC accessible –with non-zero probability–
from any other state in SC under some stationary deterministic policy  and a –possibly empty– set
of states that are transient under all policies (denoted ST). We also denote by S = |S|  SC = |SC|
and A = maxs∈S |As| the number of states and actions  and by ΓC = maxs∈SC a∈A (cid:107)p(·|s  a)(cid:107)0 the
maximum support of all transition probabilities p(·|s  a) with s ∈ SC. The sets SC and ST form a
partition of S i.e.  SC ∩ ST = ∅ and SC ∪ ST = S. A deterministic policy π : S → A maps states to
actions and it has an associated long-term average reward (or gain) and a bias function deﬁned as

E(cid:20) 1

T

T(cid:88)t=1

r(cid:0)st  π(st)(cid:1)(cid:21);

E(cid:20) T(cid:88)t=1(cid:0)r(st  π(st)) − gπ

M (st)(cid:1)(cid:21) 

M (s) := C- lim
hπ
T→∞

gπ
M (s) := lim
T→∞
where the bias hπ
M (s) measures the expected total difference between the rewards accumulated by
π starting from s and the stationary reward in Cesaro-limit2 (denoted C- lim). Accordingly  the
difference of bias values hπ
M (s(cid:48)) quantiﬁes the (dis-)advantage of starting in state s rather
than s(cid:48). In the following  we drop the dependency on M whenever clear from the context and
denote by spS {hπ} := maxs∈S hπ(s) − mins∈S hπ(s) the span of the bias function. In weakly
communicating MDPs  any optimal policy π∗ ∈ arg maxπ gπ(s) has constant gain  i.e.  gπ
(s) = g∗
for all s ∈ S. Finally  we denote by D  resp. DC  the diameter of M  resp. the diameter of the
communicating part of M (i.e.  restricted to the set SC):
DC :=
(1)

M (s) − hπ

D :=

∗

max

(s s(cid:48))∈S×S s(cid:54)=s(cid:48){τM (s → s(cid:48))} 

max

(s s(cid:48))∈SC×SC s(cid:54)=s(cid:48){τM (s → s(cid:48))} 

where τM (s → s(cid:48)) is the expected time of the shortest path from s to s(cid:48) in M.
Learning problem. Let M∗ be the true (unknown) weakly-communicating MDP. We consider the
learning problem where S  A and rmax are known  while sets SC and ST  rewards r and transition
probabilities p are unknown and need to be estimated on-line. We evaluate the performance of a
learning algorithm A after T time steps by its cumulative regret ∆(A  T ) = T g∗ −(cid:80)T
t=1 rt(st  at).
Furthermore  we state the following assumption.
Assumption 1. The initial state s1 belongs to the communicating set of states SC.
While this assumption somehow restricts the scenario we consider  it is fairly common in practice.
For example  all the domains that are characterized by the presence of a resetting distribution (e.g. 
episodic problems) satisfy this assumption (e.g.  mountain car  cart pole  Atari games  taxi  etc.).
Multi-chain MDPs. While we consider weakly-communicating MDPs for ease of notation  all our
results extend to the more general case of multi-chain MDPs.3 In this case  there may be multiple

2For policies whose associated Markov chain is aperiodic  the standard limit exists.
3In the case of misspeciﬁed states  we implicitly deﬁne a multi-chain MDP  where each non-reachable state

has a self-loop dynamics and it deﬁnes a “singleton” communicating subset.

3

βsa

r k(s  a)bk δ
N +
k (s  a)

+

49
3 rmaxbk δ
N±k (s  a)

 

communicating and transient sets of states and the optimal gain g∗ is different in each communicating
subset. In this case we deﬁne SC as the set of states that are accessible –with non-zero probability–
from s1 (s1 included) under some stationary deterministic policy. ST is deﬁned as the complement of
SC in S i.e.  ST := S \ SC. With these new deﬁnitions of SC and ST  Asm. 1 needs to be reformulated
as follows:
Assumption 1 for Multi-chain MDPs. The initial state s1 is accessible –with non-zero probability–
from any other state in SC under some stationary deterministic policy. Equivalently  SC is a commu-
nicating set of states.
Note that the states belonging to ST can either be transient or belong to other communicating subsets
of the MDP disjoint from SC. It does not really matter because the states in ST will never be visited
by deﬁnition. As a result  the regret is still deﬁned as before  where the learning performance is
compared to the optimal gain g∗(s1) related to the communicating set of states SC (cid:51) s1.
3 Truncated Upper-Conﬁdence for Reinforcement Learning (TUCRL)
In this section we introduce Truncated Upper-Conﬁdence for Reinforcement Learning (TUCRL) 
an optimistic online RL algorithm that efﬁciently balances exploration and exploitation to learn in
non-communicating MDPs without prior knowledge (Fig. 2).
Similar to UCRL  at the beginning of each episode k  TUCRL constructs conﬁdence intervals for the
reward and the dynamics of the MDP. Formally  for any (s  a) ∈ S × A we deﬁne

Bp k(s  a) =(cid:110)(cid:101)p(·|s  a) ∈ C : ∀s(cid:48) ∈ S |(cid:101)p(s(cid:48)|s  a) −(cid:98)p(s(cid:48)|s  a)| ≤ βsas
p k (cid:111)  
Br k(s  a) := [(cid:98)rk(s  a) − βsa

r k (cid:98)rk(s  a) + βsa

r k] ∩ [0  rmax] 

(cid:48)

(cid:48)
βsas

p k :=(cid:115) 14(cid:98)σ2

where Nk(s  a) is the number of visits in (s  a) before episode k  N +

size of the conﬁdence intervals is constructed using the empirical Bernstein’s inequality [22  23] as

where C = {p ∈ RS|∀s(cid:48)  p(s(cid:48)) ≥ 0 ∧(cid:80)s(cid:48) p(s(cid:48)) = 1} is the (S − 1)-probability simplex  while the
r k :=(cid:115) 14(cid:98)σ2
p k(s(cid:48)|s  a)bk δ
N +
k (s  a)
k (s  a) := max{1  Nk(s  a)} 
N±k (s  a) := max{1  Nk(s  a)−1} (cid:98)σ2
p k(s(cid:48)|s  a) are the empirical variances of r(s  a)
and p(s(cid:48)|s  a) and bk δ = ln(2SAtk/δ). The set of plausible MDPs associated with the conﬁdence
intervals is then Mk =(cid:8)M = (S A (cid:101)r (cid:101)p) : (cid:101)r(s  a) ∈ Br k(s  a)  (cid:101)p(·|s  a) ∈ Bp k(s  a)(cid:9). UCRL
is optimistic w.r.t. the conﬁdence intervals so that for all states s that have never been visited the
optimistic reward(cid:101)r(s  a) is set to rmax  while all transitions to s (i.e. (cid:101)p(s|· ·)) are set to the largest
value compatible with Bp k(· ·). Unfortunately  some of the states with Nk(s  a) = 0 may be actually
unreachable (i.e.  s ∈ ST) and UCRL would uniformly explore the policy space with the hope that at
least one policy reaches those (optimistically desirable) states. TUCRL addresses this issue by ﬁrst
constructing empirical estimates of SC and ST (i.e.  the set of communicating and transient states in
Nk(s  a) > 0(cid:9) ∪
M∗) using the states that have been visited so far  that is SC
k := S \ SC
{stk} and ST
In order to avoid optimistic exploration attempts to unreachable states  we could simply execute
UCRL on SC
k  which is guaranteed to contain only states in the communicating set (since s1 ∈ SC by
k ⊆ SC). Nonetheless  this algorithm could under-explore state-action pairs
Asm. 1  we have that SC
that would allow discovering other states in SC  thus getting stuck in a subset of the communicating
states of the MDP and suffering linear regret. While the states in SC
k are guaranteed to be in the
communicating subset  it is not possible to know whether states in ST
k are actually reachable from
SC
k or not. Then TUCRL ﬁrst “guesses” a lower bound on the probability of transition from states
s ∈ SC
k and whenever the maximum transition probability from s to s(cid:48) compatible with the
conﬁdence intervals (i.e. (cid:98)pk(s(cid:48)|s  a)+βsas
p k ) is below the lower bound  it assumes that such transition
is not possible. This strategy is based on the intuition that a transition either does not exist or it should
have a sufﬁciently “big” mass. However  these transitions should be periodically reconsidered in
order to avoid under-exploration issues. More formally  let (ρt)t∈N be a non-increasing sequence
k  s ∈ SC
to be deﬁned later  for all s(cid:48) ∈ ST
p k(s(cid:48)|s  a) are zero (i.e.  this transition has never been observed so far)  so the largest probability
(cid:98)σ2

k and a ∈ As  the empirical mean(cid:98)pk(s(cid:48)|s  a) and variance

k :=(cid:8)s ∈ S(cid:12)(cid:12) (cid:80)a∈As

k  where tk is the starting time of episode k.

r k(s  a) and(cid:98)σ2

+

49
3 bk δ
N±k (s  a)

 

k to s(cid:48) ∈ ST

(2)

(3)

(cid:48)

4

Input: Conﬁdence δ ∈]0  1[  rmax  S  A
Initialization: Set N0(s  a) := 0 for any (s  a) ∈ S × A  t := 1 and observe s1.
For episodes k = 1  2  ... do
1. Set tk = t and episode counters νk(s  a) = 0

(cid:48)|s  a) (cid:98)rk(s  a) and a set Mk
2. Compute estimates(cid:98)pk(s
tk-approximation(cid:101)πk of Eq. 5
(cid:16)(cid:80)
(a) Execute at =(cid:101)πk(st)  obtain reward rt  and observe st+1

3. Compute an rmax/
4. While tk == t or

a∈Ast

√

Nk(st  a) > 0 and νk(st (cid:101)πk(st)) ≤ max{1  Nk (st (cid:101)πk(st))}(cid:17)

(b) Set νk(st  at) += 1 and set t += 1
5. Set Nk+1(s  a) = Nk(s  a) + νk(s  a)

do

Figure 2: TUCRL algorithm.

3

N

bk δ
±
k (s a)

k (s(cid:48)|s  a) = 49

. TUCRL
k (s(cid:48)|s  a) to ρtk and forces all transition probabilities below the threshold to zero  while
k) are

(most optimistic) of transition from s to s(cid:48) through any action a is(cid:101)p+
compares(cid:101)p+
the conﬁdence intervals of transitions to states that have already been explored (i.e.  in SC
preserved unchanged. This corresponds to constructing the alternative conﬁdence interval
Bp k(s  a) = Bp k(s  a) ∩(cid:8)(cid:101)p(·|s  a) ∈ C : ∀s(cid:48) ∈ ST
k and(cid:101)p+
((cid:102)Mk (cid:101)πk) = arg max
M∈Mk π{gπ
M}.

(4)
Given Bp k  TUCRL (implicitly) constructs the corresponding set of plausible MDPs Mk and then
solves the optimistic optimization problem

k (s(cid:48)|s  a) < ρtk  (cid:101)p(s(cid:48)|s  a) = 0(cid:9) .

The resulting algorithm follows the same structure as UCRL and it is shown in Fig. 2. The episode
stopping condition at line 4 is slightly modiﬁed w.r.t. UCRL. In fact  it guarantees that one action is
always executed and it forces an episode to terminate as soon as a state previously in ST
k is visited
(i.e.  Nk(st  a) = 0). This minor change guarantees that Nk+1(s  a) = 0 for all the states s ∈ ST
k that
were not reachable at the beginning of the episode. The algorithm also needs minor modiﬁcations
to the extended value iteration (EVI) algorithm used to solve (5) to guarantee both efﬁciency and
convergence. All technical details are reported in App. C.

(5)

3 (cid:113) SA

In practice  we set ρt = 49bt δ

t   so that the condition to remove transition reduces to N±k (s  a) >

(cid:112)tk/SA. This shows that only transitions from state-action pairs that have been poorly visited so
far are enabled  while if the state-action pair has already been tried often and yet no transition to
s(cid:48) ∈ ST
k is observed  then it is assumed that s(cid:48) is not reachable from s  a. When the number of visits
in (s  a) is big  the transitions to “unvisited” states should be discarded because if the transition
actually exists  it is most likely extremely small and so it is worth exploring other parts of the MDP
ﬁrst. Symmetrically  when the number of visits in (s  a) is small  the transitions to “unvisited” states
should be enabled because the transitions are quite plausible and the algorithm should try to explore
the outcome of taking action a in s and possibly reach states in ST
k. We denote the set of state-action
pairs that are not sufﬁciently explored by Kk =(cid:8)(s  a) ∈ SC

k × A : N±k (s  a) ≤(cid:112)tk/SA(cid:9).

3.1 Analysis of TUCRL
We prove that the regret of TUCRL is bounded as follows.
Theorem 1. For any weakly communicating MDP M  with probability at least 1 − δ it holds that for
any T > 1  the regret of TUCRL is bounded as
δ (cid:19)(cid:33) .

∆(TUCRL  T ) = O(cid:32)rmaxDC(cid:115)ΓCSCAT ln(cid:18) SAT

S3A ln2(cid:18) SAT

δ (cid:19) + rmax(cid:16)DC(cid:17)2

The ﬁrst term in the regret shows the ability of TUCRL to adapt to the communicating part of the
true MDP M∗ by scaling with the communicating diameter DC and MDP parameters SC and ΓC. The
second term corresponds to the regret incurred in the early stage where the regret grows linearly.

5

When M∗ is communicating  we match the square-root term of UCRL (ﬁrst term)  while the second
term is bigger than the one appearing in UCRL by a multiplicative factor DCS (ignoring logarithmic
terms  see Sec. 5).
We now provide a sketch of the proof of Thm. 1 (the full proof is reported in App. D). In order to
preserve readability  all following inequalities should be interpreted up to minor approximations and
in high probability.

∆k · 1{tk ≥ C(k)}

∆k · 1{tk < C(k)} +

k × A for which transitions to ST

number of visits to s  a in episode k. We decompose the regret as

∆k · 1{M∗ ∈ Mk} (cid:46) m(cid:88)k=1

Let ∆k := (cid:80)s a νk(s  a)(g∗ − r(s  a)) be the regret incurred in episode k  where νk(s  a) is the
∆(TUCRL  T ) (cid:46) m(cid:88)k=1
where C(k) = O(cid:0)(DC)2S3A ln2(2SAtk/δ)(cid:1) deﬁnes the length of a full exploratory phase  where the
agent may suffer linear regret.
Optimism. The ﬁrst technical difﬁculty is that whenever some transitions are disabled  the plausible
set of MDPs Mk may actually be biased and not contain the true MDP M∗. This requires to prove
that TUCRL (i.e.  the gain of the solution returned by EVI) is always optimistic despite “wrong”
conﬁdence intervals. The following lemma helps to identify the possible scenarios that TUCRL can
produce (see App. D.2).4
Lemma 1. Let episode k be such that M∗ ∈ Mk  ST
(case I) or Kk (cid:54)= ∅  i.e.  ∃(s  a) ∈ SC
k ⊃ ST (i.e.  some states have not been reached) and
This result basically excludes the case where ST
yet no transition from SC
k = ∅  the true MDP
k = ST then
M∗ ∈ Mk = Mk w.h.p. by construction of the conﬁdence intervals. Similarly  if ST
M∗ ∈ Mk w.h.p.  since TUCRL only truncates transitions that are indeed forbidden in M∗ itself. In
both cases  we can use the same arguments in [2] to prove optimism. In case II the gain of any state
s(cid:48) ∈ ST
k  the gain of the solution returned
the precision of EVI).

k to them is enabled. We start noticing that when ST

k is set to rmax and  since there exists a path from SC

k (cid:54)= ∅ and tk ≥ C(k). Then  either ST
k are allowed (case II).

(cid:101)∆k =(cid:88)s∈S

inequalities. Nonetheless  we would be left with the problem of bounding the (cid:96)∞ norm of wk

νk(s (cid:101)πk(s))(cid:18)(cid:88)s(cid:48)∈S(cid:101)pk(s(cid:48)|s (cid:101)πk(s))(cid:101)hk(s(cid:48)) −(cid:101)hk(s)(cid:19) = ν(cid:48)k(cid:16)(cid:101)Pk − I(cid:17) wk

by EVI is rmax  which makes it trivially optimistic. As a result we can conclude that(cid:101)gk (cid:38) g∗ (up to
Per-episode regret. After bounding the optimistic reward(cid:101)rk(s  a) w.r.t. r(s  a)  the only part left to
bound the per-episode regret ∆k is the term (cid:101)∆k =(cid:80)s a νk(s  a)((cid:101)gk −(cid:101)rk(s  a)). Similar to UCRL 
we could use the (optimistic) optimality equation and rewrite (cid:101)∆k as
where wk :=(cid:101)hk − mins∈S{(cid:101)hk}e is a shifted version of the vector(cid:101)hk returned by EVI at episode
k  and then proceed by bounding the difference between (cid:101)Pk and Pk using standard concentration
(i.e.  the range of the optimistic vector(cid:101)hk) over the whole state space  i.e.  (cid:107)wk(cid:107)∞ = spS{(cid:101)hk} =
maxs∈S(cid:101)hk(s)− mins∈S(cid:101)hk(s). While in communicating MDPs  it is possible to bound this quantity
by the diameter of the MDP as spS {hk} ≤ D [2  Sec. 4.3]  in weakly-communicating MDPs
D = +∞  thus making this result uninformative. As a result  we need to restrict our attention to the
subset of communicating states SC  where the diameter is ﬁnite. We then split the per-step regret
over states depending on whether they are explored enough or not as ∆k (cid:46) (cid:80)s a νk(s  a)((cid:101)gk −
(cid:101)rk(s  a))1{(s  a) /∈ Kk} + rmax(cid:80)s a νk(s  a)1{(s  a) ∈ Kk}. We start focusing on the poorly
visited state-action pairs  i.e.  (s  a) ∈ Kk. In this case TUCRL may suffer the maximum per-step
regret rmax but the number of times this event happen is cumulatively “small” (App. D.4.1):
Lemma 2. For any T ≥ 1 and any sequence of states and actions {s1  a1  . . . . . . sT   aT} we have:
(st  at) ≤(cid:112)t/SA(cid:111) ≤ 2(cid:16)√SCAT + SCA(cid:17)
m(cid:88)k=1(cid:88)s a

1(cid:110)N±kt

T(cid:88)t=1

4Notice that M

∗ ∈ Mk is true w.h.p. since Mk is obtained using non-truncated conﬁdence intervals.

νk(s  a)1{N±k (s  a) ≤(cid:112)tk/SA
(cid:125)

(s a)∈Kk

(cid:123)(cid:122)

(cid:124)

} ≤

k = ST

(6)

m(cid:88)k=1

k to ST

6

Figure 3: Cumulative regret in the taxi with misspeciﬁed states (left-top) and in the communicating
taxi (left-bottom)  and in the weakly communicating three-states domain with D = +∞ (right).
Conﬁdence intervals βr k and βp k are shrunk by a factor 0.05 and 0.01 for the three-states domain
and taxi  respectively. Results are averaged over 20 runs and 95% conﬁdence intervals are reported.

When (s  a) /∈ Kk (i.e.  N±k (s  a) >(cid:112)tk/SA holds) (cid:80)s a νk(s  a)((cid:101)gk −(cid:101)rk(s  a)) · 1{(s  a) /∈ Kk}

can be bounded as in Eq. 6 but now restricted on SC

k  so that 

νk((cid:101)Pk − I)(cid:101)hk = (cid:88)s∈SC

k

νk(s (cid:101)πk(s))(cid:18) (cid:88)s(cid:48)∈SC

k(cid:101)pk(s(cid:48)|s (cid:101)πk(s))wk(s(cid:48)) − wk(s)(cid:19).

Mk

Mk

k  i.e.  spSC

k{wk} = maxs∈SC

Since the stopping condition guarantees that νk(s (cid:101)πk(s)) = 0 for all s ∈ ST
k  we can ﬁrst restrict
the outer summation to states in SC. Furthermore  all state-action pairs (s  a) /∈ Kk are such that
the optimistic transition probability(cid:101)pk(s(cid:48)|s  a) is forced to zero for all s(cid:48) ∈ ST
k  thus reducing the
inner summation. We are then left with providing a bound for the range of wk restricted to the
states in SC
k{wk}. We recall that EVI run on a set of plausible MDPs
Mk returns a function(cid:101)hk such that(cid:101)hk(s(cid:48)) −(cid:101)hk(s) ≤ rmax · τ
(s → s(cid:48))  for any pair s  s(cid:48) ∈ S 
(s → s(cid:48)) is the expected shortest path in the extended MDP Mk. Furthermore  since
where τ
M∗ ∈ Mk  for all s  s(cid:48) ∈ SC
k  τMk (s → s(cid:48)) ≤ DC. Unfortunately  since M∗ may not belong to Mk 
the bound on the shortest path in Mk (i.e.  τMk (s → s(cid:48))) may not directly translate into a bound
for the shortest path in Mk  thus preventing from bounding the range of(cid:101)hk even on the subset of
states in SC
k. Nonetheless  in App. E we show that a minor modiﬁcation to the conﬁdence intervals of
Mk makes the shortest paths between any two states s  s(cid:48) ∈ SC
k equivalent in both sets of plausible
k{wk} ≤ DC. 5 The ﬁnal regret in Thm. 1 is then obtained by
MDPs  thus providing the bound spSC
combining all different terms.
4 Experiments
In this section  we present experiments to validate the theoretical ﬁndings of Sec. 3. We compare
TUCRL against UCRL and SCAL.6 We ﬁrst consider the taxi problem [24] implemented in OpenAI
Gym [25].7 Even such a simple domain contains misspeciﬁed states  since the state space is con-
structed as the outer product of the taxi position  the passenger position and the destination. This
leads to states that cannot be reached from any possible starting conﬁguration (all the starting states
belong to SC). More precisely  out of 500 states in S  100 are non-reachable. On Fig. 3(left) we
compare the regret of UCRL  SCAL and TUCRL when the misspeciﬁed states are present (top)
{wk} under
control. In App. F we present an alternative modiﬁcations for which the shortest paths between any two states
s  s

5Note that there is not a single way to modify the conﬁdence intervals of Mk to keep spSC
(cid:48) ∈ SC
6To the best of out knowledge  there exists no implementable algorithm to solve the optimization step of

k is not equal but smaller than in Mk thus ensuring that spSC

{wk} ≤ DC.

k

REGAL and REGAL.D.

7The code is available on GitHub.

k

7

0123·108024·107DurationTRegret∆(T)SCALc=200TUCRLUCRL02468·10611.522.5·104Regret∆(T)SCALc=2TUCRLUCRL01 0002 0003 0004 0005 0006 000PTt=11{N±k(s a)≤qtkSA}00.20.40.60.811.21.4·10702004006008001 0001 2001 400DurationTCumulativeRegret∆(T)SCALc=10SCALc=5TUCRL∝√T1 − ε

ε
r = 0
b

(b)

r = 1

b

d

y

r = 0

(a)

E[∆(UCRL  T  M )]

(cid:16) D2S2A

γ

O

ln(T )

(cid:17)

(cid:112)

AT ln(T ))

O(T ) O(DS

r = 1/2

Regret upper-bound

r = 0

0

†
M

T

∗
M

T

T

r = 1/2

x

x

d

b

d

r = 0
d
r = 1
b

y

(c)

Figure 4: 4a Expected regret of UCRL (with known horizon T given as input) as a function of T .
4b 4c Toy example illustrating the difﬁculty of learning non-communicating MDPs. We represent a
family of possible MDPs M = (Mε)ε∈[0 1] where the probability ε to go from x to y lies in [0  1].

+

and when they are removed (bottom). In the presence of misspeciﬁed states (top)  the regret of
UCRL clearly grows linearly with T while TUCRL is able to learn as expected. On the other hand 
when the MDP is communicating (bottom) TUCRL performs similarly to UCRL. The small loss in
performance is most likely due to the initial exploration phase during which the conﬁdence intervals
on the transition probabilities used by UCRL (see deﬁnition of Mk) are tighter than those used by
k ). TUCRL uses a “loose” bound on the (cid:96)1-norm while UCRL uses S
TUCRL (see deﬁnition of M
different bounds  one for every possible next state. Finally  SCAL outperforms TUCRL by exploiting
prior knowledge on the bias span.
We further study TUCRL regret in the simple three-state domain introduced in [6] (see App. H
for details) with different reward distributions (uniform instead of Bernouilli). The environment is
composed of only three states (s0  s1 and s2) and one action per state  except in s2 where two actions
are available. As a result  the agent only has the choice between two possible policies. Fig. 3(left)
shows the cumulative regret achieved by TUCRL and SCAL (with different upper-bounds on the
bias span) when the diameter is inﬁnite i.e.  SC = {s0  s2} and ST = {s1} (we omit UCRL  since
it suffers linear regret). Both SCAL and TUCRL quickly achieve sub-linear regret as predicted by
theory. However  SCAL and TUCRL seem to achieve different growth rates in regret: while SCAL
appears to reach a logarithmic growth  the regret of TUCRL seems to grow as √T with periodic
“jumps” that are increasingly distant (in time) from each other. This can be explained by the way the
algorithm works: while most of the time TUCRL is optimistic on the restricted state space SC (i.e. 
SC
k = SC)  it periodically allows transitions to the set ST (i.e.  SC
k = S)  which is indeed not reachable.
Enabling these transitions triggers aggressive exploration during an entire episode. The policy played
is then sub-optimal creating a “jump” in the regret. At the end of this exploratory episode  SC
k will be
set again to SC and the regret will stop increasing until the condition N±k ≤(cid:112)tk/SA occurs again
(the time between two consecutive exploratory episodes grows quadratically). The cumulative regret
incurred during exploratory episodes can be bounded by the term plotted in green on Fig. 3(left). In
Lem. 2 we proved that this term is always bounded by O(√SCAT ). Therefore  it is not surprising
to observe a √T increase of both the green and red curves. Unfortunately  the growth rate of the
regret will keep increasing as √T and will never become logarithmic unlike SCAL (or UCRL when
the MDP is communicating). This is because the condition N±k ≤(cid:112)tk/SA will always be triggered
Θ(√T ) times for any T . In Sec. 5 we show that this is not just a drawback speciﬁc to TUCRL  but it
is rather an intrinsic limitation of learning in weakly-communicating MDPs.

5 Exploration-exploitation dilemma with inﬁnite diameter

In this section we further investigate the empirical difference between SCAL and TUCRL and prove
an impossibility result characterising the exploration-exploitation dilemma when the diameter is
allowed to be inﬁnite and no prior knowledge on the optimal bias span is available.

8

rmaxT (by deﬁnition)

D2ΓSA

γ

M (s) : gπ

(7)

We ﬁrst recall that the expected regret E[∆(UCRL  M  T )] of UCRL (with input parameter δ = 1/3T )
after T ≥ 1 time steps and for any ﬁnite MDP M can be bounded in several ways:

E[∆(UCRL  M  T )] ≤

3 [2  Theorem 2]
ln(T ) + C3(M ) [2  Theorem 4]

C1 · rmaxD(cid:112)ΓSAT ln(3T 2) + 1
C2 · rmax
where γ = g∗M − maxs π{gπ
M (s) < g∗M} is the gap in gain  C1 := 34 and C2 := 342 are
numerical constants independent of M  and C3(M ) := O(maxπ:π(s)=a Tπ) with Tπ a measure of
the “mixing time” of policy π. The three different bounds lead to three different growth rates for the
function T (cid:55)−→ E[∆(UCRL  M  T )] (see Fig. 4a): 1) for T †M ≥ T ≥ 0  the expected regret is linear
in T   2) for T ∗M ≥ T ≥ T †M the expected regret grows as √T   3) ﬁnally for T ≥ T ∗M   the increase in
regret is only logarithmic in T . These different “regimes” can be observed empirically (see [6  Fig.
5  12]). Using (7)  it is easy to show that the time it takes for UCRL to achieve sub-linear regret is
at most T †M = (cid:101)O(D2ΓSA). We say that an algorithm is efﬁcient when it achieves sublinear regret
after a number of steps that is polynomial in the parameters of the MDP (i.e.  UCRL is then efﬁcient).
We now show with an example that without prior knowledge  any efﬁcient learning algorithm must
satisfy T ∗M = +∞ when M has inﬁnite diameter (i.e.  it cannot achieve logarithmic regret).
Example 1. We consider a family of weakly-communicating MDPs M = (Mε)ε∈[0 1] represented
on Fig. 4(right). Every MDP instance in M is characterised by a speciﬁc value of ε ∈ [0  1] which
corresponds to the probability to go from x to y. For ε > 0 (Fig. 4b)  the optimal policy of Mε is
such that π∗(x) = b and the optimal gain is g∗ε = 1 while for ε = 0 (Fig. 4c) the optimal policy is
such that π∗(x) = d and the optimal gain is g∗0 = 1/2. We assume that the learning agent knows
that the true MDP M∗ belongs to M but does not know the value ε∗ associated to M∗ = Mε∗. We
assume that all rewards are deterministic and that the agent starts in state x (coloured in grey).
Lemma 3. Let C1  C2  α  β > 0 be positive real numbers and f a function deﬁned for all ε ∈]0  1]
by f (ε) = C1(1/ε)α. There exists no learning algorithm AT (with known horizon T ) satisfying both
1. for all ε ∈]0  1]  there exists T †ε ≤ f (ε) such that E[∆(AT   Mε  x  T )] < 1/6 · T for all T ≥ T †ε  
2. and there exists T ∗0 < +∞ such that E[∆(AT   M0  x  T )] ≤ C2(ln(T ))β for all T ≥ T ∗0 .
Note that point 1 in Lem. 3 formalizes the concept of “efﬁcient learnability” introduced by Sutton
and Barto [26  Section 11.6] i.e.  “learnable within a polynomial rather than exponential number of
time steps”. All the MDPs in M share the same number of states S = 2 ≥ Γ  number of actions
A = 2  and gap in average reward γ = 1/2. As a result  any function of S  Γ  A and γ will be
considered as constant. For ε > 0  the diameter coincides with the optimal bias span of the MDP and
D = spS {h∗} = 1/ε < +∞  while for ε = 0  D = +∞ but spS {h∗} = 1/2. As shown in Eq. 7
and Thm. 1  UCRL and TUCRL satisfy property 1. of Lem. 3 with α = 2 and C1 = O(S2A) but do
not satisfy 2. On the other hand  SCAL satisﬁes 2. with β = 1 and C2 = O(H 2SA/γ) (although this
result is not available in the literature  it is straightforward to adapt the proof of UCRL [2  Theorem
4] to SCAL) but since [6  Theorem 12] holds only when H ≥ spS {h∗}  SCAL only satisﬁes 1. for
ε ≥ 1/H and ε = 0 (not for ε ∈]0  1/H[). Lem. 3 proves that no algorithm can actually achieve both
1. and 2. As a result  since TUCRL satisﬁes 1.  it cannot satisfy 2. This matches the empirical results
presented in Sec. 4 where we observed that when the diameter is inﬁnite  the growth rates of the regret
of SCAL and TUCRL were respectively logarithmic and of order Θ(√T ). An algorithm that does not
satisfy 1. could potentially satisfy 2. but  by deﬁnition of 1.  it would suffer linear regret for a number
of steps that is more than polynomial in the parameters of the MDP (more precisely  eD1/β ). This is
not a very desirable property and we claim that an efﬁcient learning algorithm should always prefer
ﬁnite time guarantees (1.) over asymptotic guarantees (2.) when they cannot be accommodated.
6 Conclusion
We introduced TUCRL  an algorithm that efﬁciently balances exploration and exploitation in weakly-
communicating and multi-chain MDPs  when the starting state s1 belongs to a communicating set
(Asm. 1). We showed that TUCRL achieves a square-root regret bound and that  in the general case 
it is not possible to design algorithm with logarithmic regret and polynomial dependence on the MDP
parameters. Several questions remain open: 1) relaxing Asm. 1 by considering a transient initial state
(i.e.  s1 ∈ ST)  2) reﬁning the lower bound of Jaksch et al. [2] to ﬁnally understand whether it is
possible to scale with spS {h∗} (at least in communicating MDPs) instead of D without any prior
knowledge (the ﬂaw in REGAL.D may suggest it is indeed impossible).

9

Acknowledgments

This research was supported in part by French Ministry of Higher Education and Research  Nord-Pas-
de-Calais Regional Council and French National Research Agency (ANR) under project ExTra-Learn
(n.ANR-14-CE24-0010-01).

References
[1] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press

Cambridge  1998.

[2] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11:1563–1600  2010.

[3] Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement

learning in weakly communicating MDPs. In UAI  pages 35–42. AUAI Press  2009.

[4] Ronan Fruit  Matteo Pirotta  Alessandro Lazaric  and Emma Brunskill. Regret minimization in

mdps with options without prior knowledge. In NIPS  pages 3169–3179  2017.

[5] Mohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-aware regret bounds for
undiscounted reinforcement learning in mdps. In ALT  volume 83 of Proceedings of Machine
Learning Research  pages 770–805. PMLR  2018.

[6] Ronan Fruit  Matteo Pirotta  Alessandro Lazaric  and Ronald Ortner. Efﬁcient bias-span-
constrained exploration-exploitation in reinforcement learning. CoRR  abs/1802.04020  2018.

[7] William R. Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika  25(3-4):285–294  1933.

[8] Ian Osband  Daniel Russo  and Benjamin Van Roy. (more) efﬁcient reinforcement learning via

posterior sampling. In NIPS  pages 3003–3011  2013.

[9] Yasin Abbasi-Yadkori and Csaba Szepesvári. Bayesian optimal control of smoothly parameter-

ized systems. In UAI  pages 1–11. AUAI Press  2015.

[10] Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for
reinforcement learning? In ICML  volume 70 of Proceedings of Machine Learning Research 
pages 2701–2710. PMLR  2017.

[11] Yi Ouyang  Mukul Gagrani  Ashutosh Nayyar  and Rahul Jain. Learning unknown markov

decision processes: A thompson sampling approach. In NIPS  pages 1333–1342  2017.

[12] Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning:

worst-case regret bounds. In NIPS  pages 1184–1194  2017.

[13] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A. Rusu  Joel Veness  Marc G.
Bellemare  Alex Graves  Martin A. Riedmiller  Andreas Fidjeland  Georg Ostrovski  Stig Pe-
tersen  Charles Beattie  Amir Sadik  Ioannis Antonoglou  Helen King  Dharshan Kumaran  Daan
Wierstra  Shane Legg  and Demis Hassabis. Human-level control through deep reinforcement
learning. Nature  518(7540):529–533  2015.

[14] Andrew William Moore. Efﬁcient memory-based learning for robot control. Technical report 

University of Cambridge  1990.

[15] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. Gambling in a rigged casino: The
adversarial multi-armed bandit problem. In Proceedings of IEEE 36th Annual Foundations of
Computer Science  pages 322–331  Oct 1995. doi: 10.1109/SFCS.1995.492488.

[16] Aditya Gopalan and Shie Mannor. Thompson sampling for learning parameterized markov
decision processes. In COLT  volume 40 of JMLR Workshop and Conference Proceedings 
pages 861–898. JMLR.org  2015.

10

[17] Yasin Abbasi-Yadkori and Csaba Szepesvári. Bayesian optimal control of smoothly param-
eterized systems. In Proceedings of the Thirty-First Conference on Uncertainty in Artiﬁcial
Intelligence  UAI’15  pages 2–11  Arlington  Virginia  United States  2015. AUAI Press. ISBN
978-0-9966431-0-8.

[18] Yi Ouyang  Mukul Gagrani  Ashutosh Nayyar  and Rahul Jain. Learning unknown markov
In Advances in Neural Information

decision processes: A thompson sampling approach.
Processing Systems 30  pages 1333–1342. Curran Associates  Inc.  2017.

[19] Georgios Theocharous  Zheng Wen  Yasin Abbasi-Yadkori  and Nikos Vlassis. Posterior

sampling for large scale reinforcement learning. CoRR  abs/1711.07979  2017.

[20] Ian Osband and Benjamin Van Roy. Posterior sampling for reinforcement learning without

episodes. CoRR  abs/1608.02731  2016.

[21] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

John Wiley & Sons  Inc.  New York  NY  USA  1994. ISBN 0471619779.

[22] Jean-Yves Audibert  Rémi Munos  and Csaba Szepesvári. Tuning bandit algorithms in stochastic
environments. In Algorithmic Learning Theory  pages 150–165  Berlin  Heidelberg  2007.
Springer Berlin Heidelberg.

[23] Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample-variance

penalization. In COLT  2009.

[24] Thomas G. Dietterich. Hierarchical reinforcement learning with the MAXQ value function

decomposition. J. Artif. Intell. Res.  13:227–303  2000.

[25] Greg Brockman  Vicki Cheung  Ludwig Pettersson  Jonas Schneider  John Schulman  Jie Tang 

and Wojciech Zaremba. Openai gym. CoRR  abs/1606.01540  2016.

[26] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. Adaptive
computation and machine learning. MIT Press  second edition  2018. ISBN 9780262039246.

[27] Odalric-Ambrym Maillard  Phuong Nguyen  Ronald Ortner  and Daniil Ryabko. Optimal regret
bounds for selecting the state representation in reinforcement learning. In Proceedings of the
30th International Conference on Machine Learning  volume 28 of Proceedings of Machine
Learning Research  pages 543–551  Atlanta  Georgia  USA  17–19 Jun 2013. PMLR.

11

,Deepak Venugopal
Vibhav Gogate
Haoran Tang
Rein Houthooft
Davis Foote
Adam Stooke
OpenAI Xi Chen
John Schulman
Pieter Abbeel
Ronan Fruit
Matteo Pirotta
Alessandro Lazaric
Matthew Holland