2018,Regret Bounds for Online Portfolio Selection with a Cardinality Constraint,Online portfolio selection is a sequential decision-making problem in which a learner repetitively selects a portfolio over a set of assets  aiming to maximize long-term return. In this paper  we study the problem with the cardinality constraint that the number of assets in a portfolio is restricted to be at most k  and consider two scenarios: (i) in the full-feedback setting  the learner can observe price relatives (rates of return to cost) for all assets  and (ii) in the bandit-feedback setting  the learner can observe price relatives only for invested assets. We propose efficient algorithms for these scenarios that achieve sublinear regrets. We also provide regret (statistical) lower bounds for both scenarios which nearly match the upper bounds when k is a constant. In addition  we give a computational lower bound which implies that no algorithm maintains both computational efficiency  as well as a small regret upper bound.,Regret Bounds for Online Portfolio Selection

with a Cardinality Constraint

Shinji Ito

NEC Corporation

Daisuke Hatano

RIKEN AIP

Hanna Sumita

Tokyo Metropolitan University

Akihiro Yabe

NEC Corporation

Takuro Fukunaga

RIKEN AIP  JST PRESTO

Naonori Kakimura

Keio University

Ken-ichi Kawarabayashi

National Institute of Informatics

Abstract

Online portfolio selection is a sequential decision-making problem in which a
learner repetitively selects a portfolio over a set of assets  aiming to maximize
long-term return. In this paper  we study the problem with the cardinality constraint
that the number of assets in a portfolio is restricted to be at most k  and consider
two scenarios: (i) in the full-feedback setting  the learner can observe price relatives
(rates of return to cost) for all assets  and (ii) in the bandit-feedback setting  the
learner can observe price relatives only for invested assets. We propose efﬁcient
algorithms for these scenarios  which achieve sublinear regrets. We also provide
regret (statistical) lower bounds for both scenarios which nearly match the upper
bounds when k is a constant. In addition  we give a computational lower bound 
which implies that no algorithm maintains both computational efﬁciency  as well
as a small regret upper bound.

1

Introduction

Online portfolio selection [10  22] is a fundamental problem in ﬁnancial engineering  in which a
learner sequentially selects a portfolio over a set of assets  aiming to maximize cumulative wealth. For
this problem  principled algorithms (e.g.  the universal portfolio algorithm [10]) have been proposed 
which behave as if one knew the empirical distribution of future market performance. On the other
hand  these algorithms work only under the strong assumption that we can hold portfolios of arbitrary
combinations of assets  and that we can observe price relatives  the multiplicative factors by which
prices change  for all assets. Due to these limitations  this framework does not directly apply to such
real-world applications as investment in advertising or R&D  where the available combination of
assets is restricted and/or price relatives (return on investment) are revealed only for assets that have
been invested in.
In order to overcome such issues  we consider the following problem setting: Suppose that there are
T rounds and a market has d assets  represented by [d] := {1  . . .   d}. In each round t  we design a
(cid:80)d
portfolio  that represents the proportion of the current wealth invested in each of the d assets. That is 
a portfolio can be expressed as a vector xt = [xt1  . . .   xtd](cid:62) such that xti ≥ 0 for all i ∈ [d] and
i=1 xti ≤ 1. The combination of assets is restricted with a set of available combinations S ⊆ 2[d] 
that is  a portfolio xt must satisfy supp(xt) = {i ∈ [d] | xti (cid:54)= 0} ∈ S. Thus  in each period t 
we choose St from S and determine a portfolio xt only from assets in St. A typical example of
S can be given by cardinality constraints  i.e.  Sk := {S ⊆ [d] | |S| = k} for some k ≤ d. We
denote by rt = [rt1  . . .   rtd](cid:62) a price relative vector  where 1 + rti is the price relative for the i-th
asset in the t-th period. Then the wealth AT resulting from the sequentially rebalanced portfolios
t xt). The best constant portfolio strategy earns the wealth

x1  . . .   xt is given by AT =(cid:81)T

t=1(1 + r(cid:62)

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Table 1: Regret bounds for the full-feedback setting.

Constraints
Single asset (S = S1)
Combination (S = Sk) RT = O

Upper bound by Algorithm 1 Lower bound
RT = O(√T log d)
T k log d
k

(cid:19)
(cid:18)(cid:113)
(cid:1)poly(k)-time )
( run in T(cid:0)d

(cid:18)(cid:113)

RT = Ω(√T log d)
for d ≥ 17k
T log d
RT = Ω
k
and no poly(d  k  T )-time algorithm
achieves RT ≤ T 1−δpoly(d  k)

(cid:19)

k

Table 2: Regret bounds for the bandit-feedback setting.

Constraint
Single asset (S = S1)
Combination (S = Sk) RT = O

Upper bound by Algorithm 2 Lower bound
RT = O(√dT log T )

(cid:1)k(cid:19)

(cid:18)(cid:113)
T(cid:0) d

RT = Ω(√dT )
RT = Ω
and no poly(d  k  T )-time algorithm
achieves RT ≤ T 1−δpoly(d  k)

for d > k

Ck3

(cid:18)(cid:113)
T k(cid:0)d

k

(cid:19)

(cid:1) log T

( run in T poly(d  k)-time )

(cid:81)T
t=1(1 + r(cid:62)

A∗
t x) subject to the constraint that x is a portfolio satisfying supp(x) ∈ S.
T := maxx
The performance of our portfolio selection is measured by RT = log A∗
T − log AT   which we call
regret. The reason that we use log AT rather than AT comes from capital growth theory [16  21].1
In terms of the observable information  we consider two different settings: (i) in the full-feedback
setting  we can observe all the price relatives rti for i = 1  . . .   d  and (ii) in the bandit-feedback
setting  we can observe the price relatives rti only for i ∈ St. Note that in each round t a portfolio xt
has to be determined before knowing rti in either of the settings. Note also that we do not make any
statistical assumption about the behavior of rti  but we assume that rti is bounded in a closed interval
[C1  C2]  where C1 and C2 are constants satisfying −1 < C1 ≤ C2.
Our problem is a generalization of the standard online portfolio selection problem. In fact  if portfolios
combining all assets are available  i.e.  if S = 2[d]  then our problem coincides with the standard
online portfolio selection problem. For this special case  it has been shown that some online convex
optimization (OCO) methods [18  17  27] (e.g.  the online Newton step method) achieve regret of
O(d log T )  and that any algorithm will suffer from regret of Ω(d log T ) in the worst case [26].
Our contribution is twofold; algorithms with sublinear regret upper bounds  and analyses proving
regret lower bounds. First  we propose the following two algorithms:

• Algorithm 1 for the full-feedback setting  achieving regret of O((cid:112)T log |S|).
• Algorithm 2 for the bandit-feedback setting  achieving regret of O((cid:112)T k|S| log T )  where

k denotes the largest cardinality among elements in S  i.e.  k = maxS∈S |S|.

Tables 1 and 2 summarize the regret bounds for the special case in which the cardinality of assets
is restricted to be at most 1 or at most k. As shown in Table 1  Algorithm 1 can achieve regret of

O(√T poly(d)) even if k = Ω(d) when S has an exponentially large size with respect to d. In such a

case  however  Algorithm 1 requires exponentially large computational time. For the bandit-feedback
setting  the regret upper bound can be exponential w.r.t. d if k = Ω(d)  but it is still sublinear in
T . One main idea behind our algorithms is to combine the multiplicative weight update method
(MWU) [3  14] (in the full-feedback setting) / multi-armed bandit algorithms (MAB) [5  6] (in the
bandit-feedback setting) with OCO. Speciﬁcally  for choosing the combination St of assets  we
employ MWU/MAB  which are online decision making methods over a ﬁnite set of actions. For
maintaining the proportion xt of portfolios  we use OCO  that is  online decision making methods for
convex objectives over a convex set of actions.
Second  we show regret lower bounds for both the full-feedback setting and the bandit-feedback
setting where S = Sk  which give insight into the tightness of regret upper bounds achieved with our
algorithms. As shown in Table 1  the proven lower bounds for the full-feedback setting are tight up to
the O(√k) term. For the bandit-feedback setting  the lower bounds are also tight up to the O(√log T )
term  if k = O(1). Note that  if k = d then the problem coincides with the standard online portfolio

1 For more details  see Appendix A in the supplementary material.

2

selection problem  and hence  there exist algorithms achieving RT = O(√T log d). This implies
that the assumption of d = Ω(k) is essential for proving the lower bounds of Ω(√T ). We also
note that these statistical lower bounds are valid for arbitrary learners  including exponential-time
algorithms. Besides statistical ones  we also show computational lower bounds suggesting that there
is no polynomial-time algorithm achieving a regret bound with a sublinear term in T and a polynomial
term w.r.t. d and k  unless NP ⊆ BPP. This means that we cannot improve the computational
efﬁciency of Algorithm 1 to O(poly(d  k  T ))-time while preserving its regret upper bound.
To prove the regret lower bounds  we use three different techniques: for the statistical lower bound
for the full-feedback setting  we consider a completely random market and evaluate how well the
“best” strategy worked after observing the market behavior  in a similar way to that for the lower
bound for MWU [3]; for the bandit-feedback setting  we construct a “good” combination S∗
∈ S
of assets so that it is hard to distinguish it from the others  and bound the number of choosing this
“good” combination via a technique similar to that used in the proof of the regret lower bound for
MAB [5]; to prove the computational lower bound  we reduce the 3-dimensional matching problem
(3DM)  one of Karp’s 21 NP-complete problems [20]  to our problem.

2 Related work

Online portfolio selection has been studied in many research areas  including ﬁnance  statistics 
machine learning  and optimization [1  10  19  22  23] since Cover [10] formulated the problem
setting and proposed a universal portfolio algorithm that achieves regret of O(d log T ) with ex-
ponential computation cost. This regret upper bound was shown to be optimal by Ordentlich and
Cover [26]. The computation cost was reduced by the celebrated work on the online gradient method
of Zinkervich [29] for solving online convex optimization (OCO) [17  27]  a general framework
including online portfolio selection  but the regret bound is O(d√T ) and suboptimal for online
portfolio selection. A breakthrough w.r.t. this suboptimality came with the online Newton step and the
follow-the-approximation-leader method of Hazan et al. [18]  which are computationally efﬁcient and
achieve regret of O(d log T ) for a special case of OCO  including online portfolio selection. Among
studies on online portfolio selection  the work by Das et al. [12] has a motivation similar to ours: the
aim of selecting portfolios with a group-sparse structure. However  their problem setting differs from
ours in that they did not put constraints about sparsity but  rather  deﬁned regret containing regularizer
inducing group sparsity  and that they supposed that a learner can observe price relatives for all
assets after determining portfolios. In contrast to this  our work deals with the sparsity constraint
on portfolios  and our methods work even for the bandit-feedback setting  in which feedbacks are
observed only on assets that have been invested in.
Another closely related topic is the multi-armed bandit problem (MAB) [4  5  6]. For nonstochastic
MAB problems  a nearly optimal regret bound is achieved by the Exp3 algorithm [5]  which our algo-
rithm strongly relies on. For combinatorial bandit problems [7  8  9] in which each arm corresponds
to a subset  the work by Chen et al. [8] gives solutions to a wide range of problems. However  this
work does not directly apply to our setting  because we need to maintain not only subsets St but also
continuous variables xt  and both of them affect regret.

3 Upper bounds

3.1 Notation and preliminary consideration

(cid:110)
x | xi ≥ 0 (i ∈ [d]) (cid:80)d

Let us introduce some notations. For S ⊆ [d]  denote by ∆S the set of portfolios whose supports are
. Let (S∗  x∗) denote
included in S  i.e.  ∆S =
the optimal ﬁxed strategy for T rounds  i.e.  (S∗  x∗) ∈ arg max
t x). Let xt denote
S∈S x∈∆S
the output of an algorithm for the t-th round. Then the regret RT of the algorithm can be expressed as
T(cid:88)

i=1 xi ≤ 1  supp(x) ⊆ S

T(cid:88)

T(cid:88)

(cid:111)
(cid:80)T
t=1 log(1+r(cid:62)

RT = max

S∈S x∈∆S

t=1

log(1 + r(cid:62)

t x) −

log(1 + r(cid:62)

t x∗) −

log(1 + r(cid:62)

t xt).

log(1 + r(cid:62)

t xt) =

t=1

t=1

3

T(cid:88)

t=1

Algorithm 1 An algorithm for the full-feedback setting.
Input: The number T of rounds. The number d of assets. The set of available subsets S ⊆ 2[d].
1: Set w1 = (wS
2: for t = 1  . . .   T do
3:

1 = 0  respectively  for S ∈ S.

1 )S∈S ∈ RS and (xS

Parameters η > 0 and β > 0.

1 = 1 and xS

1 )S∈S by wS

t   i.e.  choose S with

Set St by randomly choosing S ∈ S with a probability proportional to wS
probability wS
t /(cid:107)wt(cid:107)1.
Output St and xt = xSt
Update wt; set wt+1 by wS
Update xS

t and observe rti for all i ∈ [d].

t xS
t+1 by equation (3) for S ∈ S.

t )η for S ∈ S.

t (1 + r(cid:62)

t+1 = wS

t ; set xS

4:
5:
6:
7: end for

The algorithms presented in this section maintain vectors xS
of the t-th round. They then choose St from S  and output (St  xSt
(S (cid:54)= St) do not appear in the output  they are used to compute outputs in subsequent rounds.
In the computation of xS

t ∈ ∆S for all S ∈ S at the beginning
t ). Although other vectors xS
t

t+1  we refer to the following vectors gt and matrices H S
t :

t have the following property which plays an important role in our analysis:

gS
t =
t1  . . .   r(cid:48)

rt|S
  H S
t =
1 + r(cid:62)
td](cid:62) is deﬁned by r(cid:48)

t xS
t

(1 + C1)2
(1 + C2)2 gS

t gS(cid:62)
ti = rti for i ∈ S and r(cid:48)

where rt|S = [r(cid:48)
and H S
Lemma 1. For any x ∈ ∆S  it holds that
t x) − log(1 + r(cid:62)
t xS

log(1 + r(cid:62)

t ) −
For the proof  see Appendix B in the supplementary material.

(x − xS

t ) ≤ gS(cid:62)

t

t = C3gS

t gS(cid:62)

t

 

(1)

ti = 0 for i ∈ [d] \ S. These gS

t

1
2

(x − xS

t )(cid:62)H S

t (x − xS
t ).

(2)

3.2 Algorithm for the full-feedback setting

We propose an algorithm for the full-feedback setting  created by combining the multiplicative
weight update method (MWU) [3] and the follow-the-approximate-leader method (FTAL) [18]. More
speciﬁcally  our proposed algorithm updates the probability of choosing a subset S ∈ S by MWU
t by FTAL. The entire algorithm is summarized in Algorithm 1.
and updates the portfolio vector xS
Our algorithm maintains weight wS
t for each subset S ∈ S at the
t ≥ 0 and a portfolio vector xS
begining of the t-th round  where wS
1 = 0 for all S ∈ S.
1 = 1 and xS
1 are initialized by wS
1 and xS
In each round t  a subset St is chosen with a probability proportional to wS
t . Given the feedback
t by multiplying
rt  the algorithm computes wS
(1 + r(cid:62)
t+1 is computed
by FTAL as follows:

t+1 is obtained from wS
t )η  where η > 0 is a parameter we optimize later. The portfolio vector xS

t+1. The weight wS

t+1 and xS

t xS

(cid:19)

  

(cid:18)

 t(cid:88)
2 = (cid:80)d

j=1

xS
t+1 ∈ arg max
x∈∆S

gS(cid:62)

j

(x − xS

j ) −

1
2

(x − xS

j )(cid:62)H S

j (x − xS
j )

β
2 (cid:107)x(cid:107)2

2

−

(3)

i=1 x2

where β is a regularization parameter optimized later  and (cid:107) · (cid:107) stands for the (cid:96)2 norm:
(cid:107)[x1  . . .   xd](cid:62)
i . Since (3) is a convex quadratic programming problem with lin-
(cid:107)2
ear constraints  xS
t+1 can be computed efﬁciently by  e.g.  interior point methods [24]. Recently  Ye
(cid:17)
et al. [28] have proposed a more efﬁcient algorithm for solving (3). For the special case of the single
{i}
asset selection setting  i.e.  if S = S1 = {{i} | i ∈ [d]}  then x
t+1 = (0  . . .   0  xt+1 i  0  . . .   0)
has a closed-form expression: xt+1 i = π[0 1]
and π[0 1](·)
stands for a projection onto [0  1] deﬁned by π[0 1](y) = 0 for y < 0  π[0 1](y) = y for 0 ≤ y ≤ 1 
and π[0 1](y) = 1 for y > 1.
Our algorithm achieves the regret described below for arbitrary inputs  where constants C3  C4  C5
are given by C3 = (1+C1)2

(cid:16) (cid:80)t
(cid:80)t

  and C5 = max{C2

  where gji := rji

(1+C2)2   C4 = log 1+C2

2}
1  C2
(1+C1)2

1+rjixji

j=1 g2
ji

j=1 gji

β+C3

1+C1

.

4

1 )S∈S ∈ RS and (xS

Parameters η > 0  γ ∈ (0  1) and β > 0.

Algorithm 2 An algorithm for the bandit-feedback setting.
Input: The number T of rounds. The number d of assets. The set of available subsets S ⊆ 2[d].
1: Set w1 = (wS
2: for t = 1  . . .   T do
3:
4:
5:

1 = 0  respectively  for S ∈ S.
t = γ|S| + (1 − γ) wS
t(cid:107)wt(cid:107)1

Set the probability vector pt = (pS
Randomly choose St ∈ S on the basis of the probability vector pt.
Output St and xt = xSt
Update wt; set wS
t ; set xS
Update xS

1 = 1 and xS
t )S∈S ∈ [0  1]S by pS
(cid:16) 1+r(cid:62)

(cid:17)η/ptit and wS

t+1 by wSt
t+1 = wtit
t+1 by equation (7).

t   and observe rti for i ∈ St.

t for S ∈ S \ {St}.

1 )S∈S by wS

t+1 = wS

t xt
1+C1

.

6:
7:
8: end for

(cid:19)
Theorem 2. Algorithm 1 achieves the following regret upper bound if η ≤ 1/C4:

+ C 2

1
2

β +

(cid:27)

4 ηT +

(cid:18)
(cid:113) log |S|
(cid:16)(cid:112)T log |S| + k log T + log |S|
(cid:17)

and β = 1  we obtain

k
C3

1 +

log

T

β

.

C3C5T

.

E[RT ] ≤
In particular  setting η = 1
C4

log |S|

η

(cid:26)

min

1 

E[RT ] = O

(4)

(5)

Running time
If (3) can be computed in p(k)-time  Algorithm 1 runs in O(|S|p(k))-time per round.
If S is an exponentially large set  e.g.  if S = {S ⊆ [d] | |S| = k} and k = Θ(d)  the computational
time for O(|S|p(k)) will be exponentially large w.r.t. d. This computational complexity is shown
to be inevitable in Section 4.1. For the special case of the single asset selection setting  i.e.  if
S = S1 = {{i} | i ∈ [d]}  Algorithm 1 runs in O(d)-time per round since each x
can be updated
in constant time.

{i}
t

3.3 Algorithm for the bandit-feedback setting

We construct an algorithm for the bandit-feedback setting by combining the Exp3 algorithm [5]
for the multi-armed bandit problem and FTAL. Similarly to the process used in Algorithm 1  the
algorithm updates the probability of choosing St ∈ S by the Exp3 algorithm (in place of MWU)
and updates portfolios xS
t by FTAL. The main difﬁculty comes from the fact that the learner cannot
t for all S ∈ S.
observe all the entries of (rti)d
In order to deal with this problem  we construct unbiased estimators of gS
t for each S ∈ S
by
(6)

i=1. Due to this limitation  we cannot always update xS

t and H S

ˆH St

 

 

ˆgS
t = 0 

ˆgSt
t =

t =

ˆH S
t = O (S ∈ S \ {St}) 

H St
t
pSt
t

gSt
t
pSt
t

j=1

1
2

(cid:19)

(cid:18)

ˆgS(cid:62)

j

j ) −

t+1 = xS

t and ˆH S

j )(cid:62) ˆH S

(x − xS

 .

 t(cid:88)

xS
t+1 ∈ arg max
x∈∆S

(x − xS
t = 0 and ˆH S

t for each S ∈ S \{St} since ˆgS

where pS
t is the probability of choosing S in round t  which is computed by a procedure similar to that
used in the Exp3 algorithm. Note that ˆgS
t can be calculated from the observed information
t+1 by FTAL as follows:
alone. Using these unbiased estimators  we compute the portfolio vectors xS
(7)
j (x − xS
j )
Note that xS
t = O. Hence the convex quadratic
programming problem (7) is solved only once in each round. The entire algorithm is summarized in
Algorithm 2.
Theorem 3. Algorithm 2 achieves the following regret upper bound if η ≤ γ
(cid:17)

(cid:18)
(cid:113) log |S|
(cid:110)
(cid:111)
(cid:112)k log |S| log T + |S|k

(cid:27)
(cid:16)(cid:112)T|S|k log T + |S|

(cid:113) k|S| log(1+T )

and β = C3C5  we obtain

4 η|S| + C4γ)T +

T
E[RT ] = O

C4|S| :
C3C5T

Setting γ = min

E[RT ] ≤

C4|S| min

β(cid:107)x(cid:107)2

log |S|

  η = γ

k|S|
C3γ

+ (C 2

(cid:26)

log

1 +

(cid:19)

1
2

−

β

.

k log(1+T )

β +

(8)

1
2

1 

1 

η

2

.

5

for some S ∈ S and computing the preﬁx sum(cid:80)i

Running time Algorithm 2 runs in O(p(k) + log2(|S|))-time per round  assuming that (7) can
be computed in p(k)-time. In fact  from the deﬁnition (6) of ˆgS
t given
by (7) is needed only for S = St. Furthermore  for S = {S1  S2  . . .   S|S|}  both updating wS
t
for some i ∈ [|S|] can be performed in
t = γ|S| + wS
t(cid:107)wt(cid:107)S

O(log |S|)-time by using a Fenwick tree [13]. This implies that sampling St w.r.t. pS
can be performed in O(log2 |S|)-time.
4 Lower bounds

t   the update of xS

t and ˆH S

j=1 wSj

t

In this section  we present lower bounds on regrets achievable by algorithms for the online portfolio
selection problem. We focus on the case of S = Sk = {S ⊆ [d] | |S| = k} throughout this section.
4.1 Computational complexity

We show that  unless the complexity class BPP includes NP  there exists no algorithm for the
online problem with a cardinality constraint such that its running time is polynomial both on d and T
and its regret is bounded by a polynomial in d and sublinear in T . This fact is shown by presenting a
reduction from the 3-dimensional matching problem (3DM). An instance U of 3DM consists of
3-tuples (x1  y1  z1)  . . .   (xd  yd  zd) ∈ [k] × [k] × [k]. Two tuples  (xi  yi  zi) and (xj  yj  zj)  are
called disjoint if xi (cid:54)= xj  yi (cid:54)= yj  and zi (cid:54)= zj. The task of 3DM is to determine whether or not
there exist k pairwise-disjoint tuples; if they do exist  we write U ∈ 3DM.
From a 3DM instance U = {(xj  yj  zj)}d
j=1  we construct an input sequence (rt)t=1 ... T of
the online portfolio selection problem as follows. Let A = (aij) ∈ {0  1}3k×d be a matrix such
that aij = 1 if i = xj or i = k + yj or i = 2k + zj  and aij = 0 otherwise. From A  we
construct B ∈ R3k×(d+1) by B = 1
3k [A −13k]  where 13k is the all-one vector of dimension 3k.
Let T ≥ max{(4 · 5184k4)2  (5184k4 · p2(d)) 1
δ } for an arbitrary polynomial p2 and an arbitrary
positive parameter δ. For each t ∈ [T ]  take zt from the uniform random distribution on {−1  1}3k 
independently. Then  rt can be deﬁned by rt = 1d+1 + B(cid:62)zt for each t ∈ [T ]. Note that
rt ∈ [0  2](d+1) holds for each t ∈ [T ].
We give the sequence (rt)t=1 ... T to an algorithm A. Let (xt)t=1 ... T denote the sequence output
5184k4 ) holds  while
otherwise we determine U /∈ 3DM to hold. We can prove that this determination is correct with a
probability of at least 2/3. For the proof  see Appendix E in the supplementary material.
Theorem 4. Let δ be an arbitrary positive number  and p1 and p2 be arbitrary polynomials. Assume
that there exists a p1(d  T )-time algorithm A for the full-feedback online portfolio selection problem
with S = Sk+1 that achieves regret RT ≤ p2(d)T 1−δ with a probability of at least 2/3. Then  given
a 3DM instance U ⊆ [k] × [k] × [k]  one can decide if U ∈ 3DM with a probability of at least 2/3
in p1(|U|  max{k8  (k4p2(|U|)) 1
Corollary 5. Under the assumption of NP (cid:54)⊆ BPP  if an algorithm achieves O(p(d  k)T 1−δ)
regret for arbitrary d and arbitrary k  the algorithm will not run in polynomial time  i.e.  the running
time will be larger than any polynomial for some d and some k.

by A. We determine that U ∈ 3DM if(cid:80)T

t xt) ≥ T (log 2 − 1

t=1 log(1 + r(cid:62)

δ })-time.

Note that the computational lower bounds described in Theorem 4 and Corollary 5 are also valid for
the bandit-feedback setting  since algorithms for the bandit-feedback settings can be used for the
full-feedback setting.

4.2 Regret lower bound for the full-feedback setting
We show here that  for the full-feedback setting of the online portfolio selection problem with S = Sk 
in the
every algorithm (including exponential-time algorithms) suffers from regret of Ω
worst case. We can show this by analyzing the behavior of an algorithm for a certain random input.
In the analysis  we use the fact that the following two inequalities hold when rt follows the discrete
uniform distribution on {0  1}d independently:

(cid:18)(cid:113)

T log d
k

(cid:19)

6

(cid:34) T(cid:88)
T(cid:88)

t=1

t=1

E
rt xt

log(1 + r(cid:62)

t xt)

max

S∈Sk x∈∆S

log(1 + r(cid:62)

t x)

(cid:35)
(cid:35)

≤ T E

X

(cid:20)

log

(cid:20)

(cid:18)
(cid:18)

1 +

≥ T · E

X

log

1 +

1
k

X

1
k

(cid:19)(cid:21)
(cid:19)(cid:21)

 

(cid:32)(cid:114)

X

+ Ω

T log

(cid:33)

 

d
k

(cid:34)

E
rt xt

where X is a binomial random variable following B(k  1/2). See Appendix F for details regarding
the proof.
(cid:19)
Theorem 6. Let d ≥ 17k  and consider the online portfolio selection problem with d assets and
t=1 such
available combinations S = Sk. There is a probability distribution of input sequences {rt}T
that the regret of any algorithm for the full-feedback setting is bounded as E[RT ] = Ω
 
T log d
k
where the expectation is with respect to the randomness of both r and the algorithm.

(cid:18)(cid:113)

4.3 Regret lower bound for the bandit-feedback setting

(cid:18)(cid:113)

(cid:19)

i∈S

zi =

 

zi =

i∈S∗

(cid:89)

T ( d

Ck3 )k

−1 w.p. 1/2 + 

(S ∈ 2[d] \ {∅  S∗

when the input sequence is deﬁned as follows. Let S∗

(cid:26) 1 w.p. 1/2
(cid:89)
(cid:26) 1 w.p. 1/2 − 

−1 w.p. 1/2

In this subsection  we consider the bandit-feedback setting of the online portfolio selection problem
with S = Sk. We show that every algorithm (including exponential-time algorithms) for this setting
suffers from regret of Ω
∈ Sk.
We deﬁne a random distribution DS∗ on {−1  1}d so that a random vector z = [z1  . . .   zd](cid:62)
following this distribution satisﬁes
(cid:26) 1 w.p. 1/2

(cid:26) 1 w.p. 1/2 − 

∈ S∗  let zi =
Such a distribution can be constructed as follows: ﬁx an index i∗
−1 w.p. 1/2
(cid:81)
for each i ∈ [d] \ {i∗
independently. Deﬁne zi∗ =
}  and let z0 =
−1 w.p. 1/2 + 
i∈S∗\{i∗} zi. Then z = [z1  . . .   zd](cid:62)
∼ DS∗. The price relative vector rt in the t-th round can
be deﬁned by rt = 1d − zt  where zt ∼ D∗
S independently for t ∈ [T ]. We can show that rt|S
follows a uniform distribution for any S ∈ Sk \ {S∗
} and only rt|S∗ follows a slightly different
distribution. Because of this  it is difﬁcult for algorithms to distinguish S∗ from others  which makes
their regrets large. For more details  see Appendix G.
Theorem 7. Let d ≥ k − 1  and consider the online portfolio selection problem with d assets and
(cid:18)
available combinations S = Sk. There is a probability distribution of input sequences {rt}T
t=1
such that the regret of any algorithm for the bandit-feedback setting is bounded as E[RT ] =
  where the expectation is with respect to the randomness of both r

(cid:27)(cid:19)

(cid:113)

T

min

k(Ck)k  

Ω
and the algorithm  and C is a constant depending on C1 and C2.

Ck3 )k

T ( d

(cid:26)

}).

z0

5 Experimental evaluation

We show the empirical performance of our algorithms through experiments over synthetic and real-
world data. In this section  we consider the online portfolio selection problem with S = S1. A
problem instance is parameterized by a tuple (d  T {rt}T
t=1). A synthetic instance is generated as
follows: given parameters d  T   C1  and C2  we randomly choose an asset i∗ from [d]  and generate
rti∗ ∼ U ((C2 + C1)/2  C2) and rti ∼ U (C1  C2) for i ∈ [d] \ {i∗
We also conduct our experiments for two real-world instances. The ﬁrst is based on crypto coin
historical data2  including dates and price data for 19 crypto coins. From this data  we select 7 crypto
coins  each having 929 prices  and obtain price relatives rti of coin i at time t by (pti/pt−1 i) − 1 
where pti indicates the price of coin i at time t. Thus  d = 7 and T = 928 in this instance. The other

}.

2https://www.kaggle.com/sudalairajkumar/cryptocurrencypricehistory

7

instance is based on S&P 500 stock data3  including dates and price data for 505 companies. From
this data  we choose d = 470 companies  each having 1259 stock prices  and compute T = 1258
price relatives for each company in the same way.
For purposes of comparison  we prepare three baseline algorithms: Exp3_cont  Exp3_disc  and
MWU_disc. MWU_disc (based on MWU [3]) works in the full-feedback setting and is compared
with Algorithm 1. Exp3_cont and Exp3_disc (based on Exp3 [5]) work in the bandit-feedback setting
and are compared with Algorithm 2. These baseline algorithms have different ways of updating xS
t
t can be expressed as
from those of Algorithms 1 and 2. Note that since S = S1 = {{i} | i ∈ [d]}  xS
xS
t = x

{i}
t = [0  . . .   0  xti  0  . . .   0](cid:62). Below  we offer a brief explanation of the comparisons.

j=1 rji ≥ 0 and xti = 0 otherwise. For each t ∈ [T ]  select it
i=1  and output

by MWU  where rewards in the t-th round are given by [log(1 + rtixti)]d
it  x

MWU_disc Set xti = 1 if(cid:80)t−1
Exp3_disc Set xti = 1 if(cid:80)
j∈[t−1]:ij =i rji ≥ 0 and xti = 0 otherwise. For each t ∈ [T ]  select it
{it}
.
by Exp3  where reward in the t-th round is given by log(1 + rtitxtit)  and output it  x
t
Exp3_cont Set a parameter B ∈ N  and consider an MAB problem instance with d(B + 1)
arms in which the rewards for the d(B + 1) arms in the t-th round are given by
(log(1 + rtib/B))1≤i≤d 0≤b≤B. Apply Exp3 to this MAB problem instance.

{it}
t

.

We assess the performance of the algorithms on the basis of regrets for synthetic instances and of
cumulative price relatives for real-world instances  where regrets and cumulative price relatives are
averaged over 10 executions. We set parameters η according to Theorem 2 for Algorithm 1 and
MWU_disc  and η and γ according to Theorem 3 for Algorithm 2  Exp3_disc  and Exp3_cont.
Figure 1 shows average regrets for a synthetic instance with (d  T  C1  C2) = (20  10000 −0.5  0.5).
We observe that both Algorithms 1 and 2 converge faster than MWU_disc  Exp3_cont  and Exp3_disc.
In addition  the results empirically show that our theoretical bounds are correct.
Figures 2 and 3 show average cumulative price relatives for a real-world instance of S&P 500 stock
data with (d  T  C1  C2) = (470  1258 −0.34  1.04) and for a real-world instance of crypto coin data
with (d  T  C1  C2) = (7  928 −0.7  3.76)  respectively. From these ﬁgures  we observe that the
cumulative price relatives of our algorithms are higher than those of baseline algorithms.

Figure 2: The average cumu-
lative price relatives over S&P
500 stock dataset

Figure 3: The average cumu-
lative price relatives over the
cryptocoin historical dataset

The
average
Figure 1:
regrets over
the synthetic
dataset with (d  T  C1  C2) =
(20  10000 −0.5  0.5)

Acknowledgement

This work was supported by JST ERATO Grant Number JPMJER1201  Japan  and JSPS KAKENHI
Grant Number JP18H05291.

3https://www.kaggle.com/camnugent/sandp500

8

00.20.40.60.81·10401 0002 000TRTAlgorithm1MWU_discAlgorithm2Exp3_contExp3_disc02004006008001 0001 200−0.4−0.20TCumulativepricerelativeAlgorithm1MWU_discAlgorithm2Exp3_contExp3_disc02004006008001 0000246TCumulativepricerelativeAlgorithm1MWU_discAlgorithm2Exp3_contExp3_discReferences
[1] A. Agarwal  E. Hazan  S. Kale  and R. E. Schapire. Algorithms for portfolio management based
on the Newton method. Proceedings of the 23rd International Conference on Machine Learning
- ICML ’06  pages 9–16  2006.

[2] N. Alon and J. H. Spencer. The Probabilistic Method. John Wiley & Sons  2004.

[3] S. Arora  E. Hazan  and S. Kale. The multiplicative weights update method: a meta-algorithm

and applications. Theory of Computing  8(1):121–164  2012.

[4] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning  47(2-3):235–256  2002.

[5] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. The nonstochastic multiarmed bandit

problem. SIAM Journal on Computing  32(1):48–77  2002.

[6] S. Bubeck  N. Cesa-Bianchi  et al. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends R(cid:13) in Machine Learning  5(1):1–122  2012.
Sciences  78(5):1404–1422  2012.

[7] N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System

[8] W. Chen  Y. Wang  and Y. Yuan. Combinatorial multi-armed bandit: General framework and

applications. In International Conference on Machine Learning  pages 151–159  2013.

[9] R. Combes  M. S. T. M. Shahi  A. Proutiere  et al. Combinatorial bandits revisited. In Advances

in Neural Information Processing Systems  pages 2116–2124  2015.

[10] T. M. Cover. Universal portfolios. Mathematical Finance  1(1):1–29  1991.

[11] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons  2012.

[12] P. Das  N. Johnson  and A. Banerjee. Online portfolio selection with group sparsity.

Twenty-Eighth AAAI Conference on Artiﬁcial Intelligence  2014.

In

[13] P. M. Fenwick. A new data structure for cumulative frequency tables. Software: Practice and

Experience  24(3):327–336  1994.

[14] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of Computer and System Sciences  55(1):119–139  1997.

[15] J. E. Gentle. Computational Statistics. Springer Science & Business Media  2009.

[16] N. H. Hakansson and W. T. Ziemba. Capital growth theory. Handbooks in Operations Research

and Management Science  9:65–86  1995.

[17] E. Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimiza-

tion  2(3-4):157–325  2016.

[18] E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimiza-

tion. Machine Learning  69(2-3):169–192  2007.

[19] A. Kalai and S. Vempala. Efﬁcient algorithms for universal portfolios. Journal of Machine

Learning Research  3(Nov):423–440  2002.

[20] R. M. Karp. Reducibility among combinatorial problems. In Complexity of Computer Compu-

tations  pages 85–103. Springer  1972.

[21] J. Kelly. A new interpretation of information rate. Bell Sys. Tech. Journal  35:917–926  1956.

[22] B. Li and S. C. Hoi. Online portfolio selection: A survey. ACM Computing Surveys (CSUR) 

46(3):35  2014.

[23] B. Li and S. C. H. Hoi. On-Line Portfolio Selection with Moving Average Reversion. Proceed-
ings of the 29th International Conference on Machine Learning (ICML-12)  pages 273–280 
2012.

9

[24] M. S. Lobo  L. Vandenberghe  S. Boyd  and H. Lebret. Applications of second-order cone

programming. Linear Algebra and Its Applications  284(1-3):193–228  1998.

[25] J. Matoušek and J. Vondrák. The probabilistic method. Lecture Notes  Department of Applied

Mathematics  Charles University  Prague  2001.

[26] E. Ordentlich and T. M. Cover. The cost of achieving the best portfolio in hindsight. Mathematics

of Operations Research  23(4):960–982  1998.

[27] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends R(cid:13)

in Machine Learning  4(2):107–194  2012.

[28] Y. Ye  L. Lei  and C. Ju. Hones: A fast and tuning-free homotopy method for online newton
step. In Proceedings of the Twenty-First International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS-18)  pages 2008–2017  2018.

[29] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03)  pages
928–936  2003.

10

,Shinji Ito
Daisuke Hatano
Hanna Sumita
Takuro Fukunaga
Naonori Kakimura
Ken-Ichi Kawarabayashi