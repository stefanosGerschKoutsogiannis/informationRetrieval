2008,Biasing Approximate Dynamic Programming with a Lower Discount Factor,Most algorithms for solving Markov decision processes rely on a discount factor  which ensures their convergence. In fact  it is often used in problems with is no intrinsic motivation. In this paper  we show that when used in approximate dynamic programming  an artificially low discount factor may significantly improve the performance on some problems  such as Tetris. We propose two explanations for this phenomenon. Our first justification follows directly from the standard approximation error bounds: using a lower discount factor may decrease the approximation error bounds. However  we also show that these bounds are loose  a thus their decrease does not entirely justify a better practical performance. We thus propose another justification: when the rewards are received only sporadically (as it is the case in Tetris)  we can derive tighter bounds  which support a significant performance increase with a decrease in the discount factor.,Biasing Approximate Dynamic Programming with a

Lower Discount Factor

Marek Petrik

Department of Computer Science

University of Massachusetts Amherst

Amherst  MA 01003

petrik@cs.umass.edu

Bruno Scherrer

LORIA Campus Scientiﬁque B.P. 239
54506 Vandoeuvre-les-Nancy  France

bruno.scherrer@loria.fr

Abstract

Most algorithms for solving Markov decision processes rely on a discount factor 
which ensures their convergence. It is generally assumed that using an artiﬁcially
low discount factor will improve the convergence rate  while sacriﬁcing the solu-
tion quality. We however demonstrate that using an artiﬁcially low discount factor
may signiﬁcantly improve the solution quality  when used in approximate dynamic
programming. We propose two explanations of this phenomenon. The ﬁrst jus-
tiﬁcation follows directly from the standard approximation error bounds: using
a lower discount factor may decrease the approximation error bounds. However 
we also show that these bounds are loose  thus their decrease does not entirely
justify the improved solution quality. We thus propose another justiﬁcation: when
the rewards are received only sporadically (as in the case of Tetris)  we can derive
tighter bounds  which support a signiﬁcant improvement in the solution quality
with a decreased discount factor.

1 Introduction

Approximate dynamic programming methods often offer surprisingly good performance in practical
problems modeled as Markov Decision Processes (MDP) [6  2]. To achieve this performance  the
parameters of the solution algorithms typically need to be carefully tuned. One such important pa-
rameter of MDPs is the discount factor γ. Discount factors are important in inﬁnite-horizon MDPs 
in which they determine how the reward is counted. The motivation for the discount factor originally
comes from economic models  but has often no meaning in reinforcement learning problems. Nev-
ertheless  it is commonly used to ensure that the rewards are bounded and that the Bellman operator
is a contraction [8]. In this paper  we focus on the quality of the solutions obtained by approximate
dynamic programming algorithms. For simplicity  we disregard the computational time  and use
performance to refer to the quality of the solutions that are eventually obtained.
In addition to regularizing the rewards  using an artiﬁcially low discount factor sometimes has a
signiﬁcant effect on the performance of the approximate algorithms. Speciﬁcally  we have observed
a signiﬁcant improvement of approximate value iteration when applied to Tetris  a common rein-
forcement learning benchmark problem. The natural discount factor in Tetris is 1  since the received
rewards have the same importance  independently of when received. Currently  the best results
achieved with approximate dynamic programming algorithms are on average about 6000 lines re-
moved in a single game [4  3]. Our results  depicted in Figure 1  with approximate value iteration
and standard features [1] show that setting the discount factor to γ ∈ (0.84  0.88) gives the best
expected total number of removed lines  a bit more than 20000. That is ﬁve times the performance
with discount factor of γ = 1 (about 4000). The improved performance for γ ∈ (0.84  0.88) is sur-
prising  since computing a policy for this discount factor dramatically improves the return calculated
with γ = 1.

Figure 1: Performance of approximate value iteration on Tetris with different discount factors. For
each value of γ  we ran the experiments 10 times and recorded the evolution of the score (the
evaluation of the policy with γ = 1) on the 100 games  and averaged over 10 learning runs.

In this paper  we study why using a lower discount factor improves the quality of the solution with
regard to a higher discount factor. First  in Section 2  we deﬁne the framework for our analysis.
In Section 3 we analyze the inﬂuence of the discount factor on the standard approximation error
bounds [2]. Then in Section 4 we argue that  in the context of this paper  the existing approximation
error bounds are loose. Though these bounds may be tightened by a lower discount factor  they are
not sufﬁcient to explain the improved performance. Finally  to explain the improved performance 
we identify a speciﬁc property of Tetris in Section 5 that enables the improvement. In particular 
the rewards in Tetris are received sparsely  unlike the approximation error  which makes the value
function less sensitive to the discount factor than the approximation error.

2 Framework and Notations

In this section we formalize the problem of adjusting the discount factor in approximate dynamic
programming. We assume γ-discounted inﬁnite horizon problems  with γ < 1. Tetris does not
directly ﬁt in this class  since its natural discount factor is 1.
It has been shown  however  that
undiscounted inﬁnite horizon problems with a ﬁnite total reward can be treated as discounted prob-
lems [7]. Blackwell optimality implies that there exists γ∗ < 1 such that for all γ > γ∗ the
γ-discounted problem and the undiscounted problem have the same optimal policy. We therefore
treat Tetris as a discounted problem with a discount factor γ∗ < 1 near one. The analysis is based
on Markov decision processes  deﬁned as follows.
Deﬁnition 1. A Markov Decision Process is a tuple (S  A  P  r). S is the set of states  A is the set of
actions  P : S × S × A (cid:55)→ [0  1] is the transition function (P (s(cid:48)  s  a) is the probability of transiting
to state s(cid:48) from state s given action a)  and r : S × A (cid:55)→ R+ is a (non-negative) reward function.
We assume that the number of states and actions is ﬁnite  but possibly very large. For sake of sim-
plicity  we also assume that the rewards are non-negative; our analysis can be extended to arbitrary
rewards in a straight-forward way. We write (cid:107)r(cid:107)∞ to denote the maximal reward for any action and
state.
Given a Markov decision process (S  A  P  r) and some discount factor γ  the objective is to ﬁnd a
policy  i.e. a mapping π : S (cid:55)→ A  with the maximal value from any initial states s. The value vπ(s)
of π from state s is deﬁned as the γ-discounted inﬁnite horizon return:

(cid:35)

(cid:34) ∞(cid:88)

vπ(s) := E

γtr(st  at) s0 = s  a0 = π(s0)  . . .   at = π(st)

.

t=0

It is well known [7  2] that this problem can be solved by computing the optimal value function v∗ 
which is the ﬁxed point of the Bellman operator Lv = maxπ rπ + γPπv. Here rπ is the vector on S
with components r(s  π(s)) and P π is the stochastic matrix associated with a policy π.

 0 5000 10000 15000 20000 25000 0 10 20 30 40 50 60 70 80 90 100Average of 10 runs of average scores on 100 gamesIterations0.80.840.880.920.961.0We consider in this paper that the MDP is solved with 1) an approximate dynamic programming
algorithm and 2) a different discount factor β < γ. In particular  our analysis applies to approximate
value and policy iteration with existing error bounds. These methods invariably generate a sequence
of approximate value functions  which we denote as ˜vβ. Then  πβ is a policy greedy with regard to
the approximate value function ˜vβ.
As we have two different discount factors  we use a subscript to denote the discount factor used in
calculating the value. Let δ be a discount factor and π any policy. We use vπ
δ to represent the value
of policy π calculated with the discount factor δ; when π is the optimal policy corresponding to the
discount δ  we will simply denote its value vδ. As mentioned above  our objective is to compare 
for the discount factor γ  the value vγ of the optimal policy and the value vπβ
γ . Here  πβ is the
policy derived from the approximate β-discount value. The following shows how this error may be
decomposed in order to simplify the analysis. Most of our analysis is in terms of L∞ mainly because
this is the most common measure used in the existing error bounds. Moreover  the results could be
extended to L2 norm in a rather straight-forward way without a qualitative difference in the results.
From the optimality of vγ  vγ ≥ vπβ
γ and from the non-negativity of the rewards  it is easy to show
γ ≥ vπβ
that the value function is monotonous with respect to the discount factor  and therefore: vπβ
β .
Thus 0 ≤ vγ − vπβ

γ ≤ vγ − vπβ

β and consequently:

e(β) := (cid:107)vγ − vπβ

γ (cid:107)∞ ≤ (cid:107)vγ − vπβ

β (cid:107)∞ ≤ (cid:107)vγ − vβ(cid:107)∞ + (cid:107)vβ − vπβ

β (cid:107)∞ = ed(β) + ea(β).

β (cid:107)∞ the approxi-
where ed(β) := (cid:107)vγ − vβ(cid:107)∞ denotes the discount error  and ea(β) := (cid:107)vβ − vπβ
mation error. In other words  a bound of the loss due to using πβ instead of the optimal policy for
discount factor γ is the sum of the error on the optimal value function due to the change of discount
and the error due to the approximation for discount β. In the remainder of the paper  we analyze
each of these error terms.

3 Error Bounds

In this section  we develop a discount error bound and overview the existing approximation error
bounds. We also show how these bounds motivate decreasing the discount factor in the majority of
MDPs. First  we bound the discount error as follows.
Theorem 2. The discount error due to using a discount factor β instead of γ is:

ed(β) = (cid:107)vγ − vβ(cid:107)∞ ≤

γ − β

(1 − β)(1 − γ)

(cid:107)r(cid:107)∞.

Proof. Let Lγ and Lβ be the Bellman operators for the corresponding discount factors. We have
now:
(cid:107)vγ − vβ(cid:107)∞ = (cid:107)Lγvγ − Lβvβ(cid:107)∞ = (cid:107)Lγvγ − Lβvγ + Lβvγ − Lβvβ(cid:107)∞

≤ (cid:107)Lγvγ − Lβvγ(cid:107)∞ + (cid:107)Lβvγ − Lβvβ(cid:107)∞ ≤ (cid:107)Lγvγ − Lβvγ(cid:107)∞ + β(cid:107)vγ − vβ(cid:107)∞
Let Pγ  rγ and Pβ  rβ be the transition matrices and rewards of policies greedy with regard to vγ for
γ and β respectively. Then we have:

Lγvγ − Lβvγ = (γPγvγ + rγ) − (βPβvγ + rβ) ≤ (γ − β)Pγvγ
Lγvγ − Lβvγ = (γPγvγ + rγ) − (βPβvγ + rβ) ≥ (γ − β)Pβvγ.

Finally  the bound follows from above as:

(cid:107)vγ − vβ(cid:107)∞ ≤ 1
1 − β

max{(cid:107)(γ − β)Pγvγ(cid:107)∞ (cid:107)(γ − β)Pβvγ(cid:107)∞} ≤

γ − β

(1 − γ)(1 − β)

(cid:107)r(cid:107)∞.

Remark 3. This bound is trivially tight  that is there exists a problem for which the bound reduces to
equality. It is however also straightforward to construct a problem in which the bound is not tight.

Figure 2: Example e(β) function in a
problem with γ = 0.9 and  = 0.01
and (cid:107)r(cid:107)∞ = 10.

Figure 3: The dependence of  on γ
needed for the improvement in Propo-
sition 6.

3.1 Approximation Error Bound

β)k≥0 with πk

We now discuss the dependence of the approximation error ea(β) on the discount factor β. Approxi-
mate dynamic programming algorithms like approximate value and policy iteration build a sequence
of value functions (˜vk
β. These algorithms
are approximate because at each iteration the value ˜vk
β is an approximation of some target value
β  which is hard to compute. The analysis of [2] (see Section 6.5.3 and Proposition 6.1 for value
vk
iteration  and Proposition 6.2 for policy iteration) bounds the loss of using the policies πk
β instead of
the optimal policy:

β being the policy greedy with respect to ˜vk

lim sup
k→∞

(cid:107)vβ − v

πk
β

β (cid:107)∞ ≤

2β

(1 − β)2 sup

k

(cid:107)˜vk

β − vk

β(cid:107)∞.

(1)

β − vk

To completely describe how Eq. (1) depends on the discount factor  we need to bound the one-step
approximation error (cid:107)˜vk
β(cid:107) in terms of β. Though this speciﬁc error depends on the particu-
lar approximation framework used and is in general difﬁcult to estimate  we propose to make the
following assumption.
Assumption 4. There exists  ∈ (0  1/2)  such that for all k  the single-step approximation error is
bounded by:

(cid:107)˜vk

β − vk

β(cid:107)∞ ≤ 
1 − β

(cid:107)r(cid:107)∞.

We consider only  ≤ 1/2 because the above assumption holds with  = 1/2 and the trivial constant
β = (cid:107)r(cid:107)∞/2.
approximation ˜vk
Remark 5. Alternatively to Assumption 4  we could assume that the approximation error is constant
in the discount factor β  i.e. (cid:107)˜vk
β(cid:107)∞ ≤  = O(1) for some  for all β. We believe that such a
bound is unlikely in practice. To show that  consider an MDP with two states s0 and s1  and a single
√
action. The transitions loop from each state to itself  and the rewards are r(s0) = 0 and r(s1) = 1.
2]. The approximation
Assume a linear least-squares approximation with basis M = [1/
error in terms of β is: 1/2(1 − β) = O(1/(1 − β)).
If Assumption 4 holds  we see from Eq. (1) that the approximation error ea is bounded as:

√
2; 1/

β − vk

ea(β) ≤

2β

(1 − β)3 (cid:107)r(cid:107)∞.

3.2 Global Error Bound

Using the results above  and considering that Assumption 4 holds  the cumulative error bound when
using approximate dynamic programming with a discount factor β < γ is:

e(β) = ea(β) + ed(β) ≤

γ − β

(1 − β)(1 − γ)

(cid:107)r(cid:107)∞ +

2β

(1 − β)3 (cid:107)r(cid:107)∞.

An example of this error bound is shown in Figure 2: the bound is minimized for β (cid:39) 0.8 < γ. This
is because the approximation error decreases rapidly in comparison with the increasing discount
error. More generally  the following proposition suggests how we should choose β.

00.20.40.60.8160708090100110b00.20.40.60.8100.10.20.30.40.5geProposition 6. If the approximation factor  introduced in Assumption 4 is sufﬁciently large  pre-
cisely if  > (1 − γ)2/2(1 + 2γ)  then the best error bound e(β) will be achieved for the discount

factor β = (2 + 1) −(cid:112)(2 + 1)2 + (2 − 1) < γ.

Figure 3 shows the approximation error fraction necessary to improve the performance. Notice that
the fraction decreases rapidly when γ → 1.
Proof. The minimum of β (cid:55)→ e(β) can be derived analytically by taking its derivative:

e(cid:48)(β) = −(1 − β)−2(cid:107)r(cid:107)∞ + (1 − β)−32(cid:107)r(cid:107)∞ + (−3)2β(−1)(1 − β)−4(cid:107)r(cid:107)∞
(cid:107)r(cid:107)∞.

(1 − β)2 + 2(1 − β) + 6β

−β2 + 2(2 + 1)β + 2 − 1

(cid:107)r(cid:107)∞ =

=

(1 − β)4

(1 − β)4

So we want to know when β (cid:55)→ −1/2β2 + (2 + 1)β + 1/2(2 − 1) equals 0. The discriminant
∆ = (2 + 1)2 + (2 − 1) = 2(2 + 3) is always positive. Therefore e(cid:48)(β) equals 0 for the points
β− = (2 + 1) − √
√
∆ and is positive in between and negative outside.
This means that β− is a local minimum of e and β+ a local maximum.
It is clear that β+ > 1 > γ. From the deﬁnition of ∆ and the fact (cf Assumption 4) that  ≤ 1/2 
we see that β− ≥ 0. Then  the condition β− < γ is satisﬁed if and only if:

∆ and β+ = (2 + 1) +

β− < γ ⇔ (2 + 1) −(cid:112)(2 + 1)2 + (2 − 1) < γ ⇔ 1 −

(cid:115)

2 − 1
(2 + 1)2 <

γ

2 + 1

1 +

(cid:115)

2 − 1
⇔ 1 − γ
(2 + 1)2 ⇔ 1 − 2
⇔ −2γ(2 + 1) + γ2 < 2 − 1 ⇔ (1 − γ)2

2 + 1

1 +

<

γ

2 + 1

+

γ2

(2 + 1)2 < 1 +

2 − 1
(2 + 1)2

< 2

1 + 2γ

where the inequality holds after squaring  since both sides are positive.

4 Bound Tightness
We show in this section that the bounds on the approximation error ea(β) are very loose for β → 1
and thus the analysis above does not fully explain the improved performance. In particular  there
exists a naive bound on the approximation error that is dramatically tighter than the standard bounds
when β is close to 1.
Lemma 7. There exists a constant c ∈ R+ such that for all β we have (cid:107)vβ − ˜vβ(cid:107)∞ ≤ c/(1 − β).
Proof. Let P ∗  r∗ and ˆP   ˆr be the transition reward functions of the optimal approximate policies
respectively. The functions may depend on the discount factor  but we omit that to simplify the
notation. Then the approximation error is:
−1r

∗(cid:107)∞ + (cid:107)ˆr(cid:107)∞) .

−1 ˆr(cid:107)∞ ≤ 1

((cid:107)r

∗

(cid:107)vβ − ˆvβ(cid:107)∞ = (cid:107)(I − βP

∗ − (I − β ˆP )
Thus setting c = 2 maxπ (cid:107)rπ(cid:107)∞ proves the lemma.

)

1 − β

2β

β − vk

(1−β)2  > c

Lemma 7 implies that for every MDP  there exists a discount factor β  such that Eq. (1) is not
tight. Consider even that the single-step approximation error is bounded by a constant  such that
lim supk→∞ (cid:107)˜vk
β(cid:107)∞ ≤ . This is impractical  as discussed in Remark 5  but it tightens the
bound. Such a bound implies that: ea(β) ≤ 2β/(1 − β)2. From Lemma 7  this bound is loose
1−β . Thus we have that there exists β < 1 for which the standard approximation
when
error bounds are loose  whenever  > 0. The looseness of the bound will be more apparent in
problems with high discount factors. For example in the MDP formulation of Blackjack [5] the
discount factor γ = 0.999  in which case the error bound may overestimate the true error by a factor
up to 1/(1 − γ) = 1000.
The looseness of the approximation error bounds may seem to contradict Example 6.4 in [2]  which
shows that Eq. (1) is tight. The discrepancy is because in our analysis we assume that the MDP has

Figure 4: Looseness of the
Bellman error bound.

Figure 5: Bellman error
bound as a function of β for
a problem with γ = 0.9.

Figure 6: The approximation
error with a = ˜vβ and b =
vγ.

ﬁxed rewards and number of states  while the example in [2] assumes that the reward depends on
the discount factor and the number of states is potentially inﬁnite. Another way to put it is to say
that Example 6.4 shows that for any discount factor β there exists an MDP (which depends on β)
for which the bound Eq. (1) is tight. We  on the other hand  show that there does not exist a ﬁxed
MDP such that for all discount factor β the bound Eq. (1) is tight.
Proposition 6 justiﬁes the improved performance with a lower discount factor by a more rapid de-
crease in ea with β than the increase in ed. The naive bound from Lemma 7 however shows that ea
may scale with 1/(1 − β)  the same as ed. As a result  while the approximation error will decrease 
it may not be sufﬁcient to offset the increase in the discount error.
Some of the standard approximation error bound may be tightened by using a lower discount factor.
For example consider the standard a-posteriori approximation error bound for the value function
˜vβ [7] :

(cid:107)Lβ ˜vβ − ˜vβ(cid:107)∞ 

(cid:107)vβ − v ˜π
(cid:19)

β(cid:107)∞ ≤ 1
1 − β
(cid:19)
(cid:18)0

P2 =

1
1

0

0
1

(cid:18)1

0

P1 =

where ˜πβ is greedy with respect to ˜vβ. This bound is widely used and known as Bellman error
bound. The following example demonstrates that the Bellman error bound may also be loose for β
close to 1:

r1 =(cid:0)1

2(cid:1)

r2 =(cid:0)2

2(cid:1)

Assume that the current value function is the value of a policy with the transition matrix and reward
P1  r1  while the optimal policy has the transition matrix and reward P2  r2. The looseness of the
bound is depicted in Figure 4. The approximation error bound scales with
(1−γ)2   while the true
1−γ . As a result  for γ = 0.999  the bound is 1000 times the true error value in
error scales with 1
this example. The intuitive reason for the looseness of the bound is that the bound treats each state
as recurrent  even when is it transient.
The global error bound may be also tightened by using a lower discount factor β as follows:

1

(cid:107)vγ − v ˜πβ

γ (cid:107)∞ ≤ 1
1 − β

(cid:107)Lβ ˜vβ − ˜vβ(cid:107)∞ +

γ − β

(1 − β)(1 − γ)

(cid:107)r(cid:107)∞.

Finding the discount factor β that minimizes this error is difﬁcult  because the function may not
be convex or differentiable. Thus the most practical method is a sub-gradient optimization method.
The global error bound the MDP example above is depicted in Figure 5.

5 Sparse Rewards

In this section  we propose an alternative explanation for the performance improvement in Tetris
that does not rely on the loose approximation error bounds. A speciﬁc property of Tetris is that the
rewards are not received in every step  i.e. they are sparse. The value function  on the other hand 
is approximated in every step. As a result  the return should be less sensitive to the discount factor
than the approximation error. Decreasing the discount factor will thus reduce the approximation
error more signiﬁcantly than it increases the discount error. The following assumption formalizes
this intuition.
Assumption 8 (Sparse rewards). There exists an integer q such that for all m ≥ 0 and all instantia-

tions ri with non-zero probability:(cid:80)m

i=0 ri ≤ (cid:98)m/q(cid:99) and ri ∈ {0  1}.

0.80.850.90.951050100150200gBellman error / true error00.20.40.60.8150100150200250bBellman error00.20.40.60.822.22.42.62.83β|| a − b ||∞Now deﬁne uβ =(cid:80)∞

i=0 βiti  where ti = 1 when i ≡ 0 mod q. Then let Im = {i ri = 1  i ≤ m}
and Jm = {j tj = 1  j ≤ m} and let I = I∞ and J = J∞. From the deﬁnition  these two sets
satisfy that |Im| ≤ |Jm|. First we show the following lemma.
Lemma 9. Given sets Im and Jm  there exists an injective function f : I → J  such that f(i) ≤ i.

Proof. By induction on m. The base case m = 0 is trivial. For the inductive case  consider the
following two cases: 1) rm+1 = 0. From the inductive assumption  there exists a function that maps
Im to Jm. Now  this is also an injective function that maps Im+1 = Im to Jm+1. 2) rm+1 = 1. Let
j∗ = max Jm+1. Then if j∗ = m + 1 then the function f : Im → Jm can be extended by setting
f(m + 1) = j∗. If j∗ ≤ m then since |Jm+1| − 1 = |Jj∗−1| ≥ |Im|  such an injective function
exists from the inductive assumption.

In the following  let Ri be the random variable representing the reward received in step i. It is
possible to prove that the discount error scales with a coefﬁcient that is lower than in Theorem 2:
Theorem 10. Let β ≤ γ − φ  let k = − log(1 − γ)/(log(γ) − log(γ − φ))  and let ρ =
E

. Then assuming the reward structure as deﬁned in Assumption 8 we have that:

(cid:104)(cid:80)k

(cid:105)

i=0 γiRi

(cid:107)vγ − vβ(cid:107)∞ ≤ γk(cid:107)uγ − uβ(cid:107)∞ + ρ ≤ γk(γq − βq)
(1 − γq)(1 − βq)

+ ρ.

Proof. Consider π be the optimal policy for the discount factor γ. Then we have: 0 ≤ vγ − vβ ≤
γ − vπ
β . In the remainder of the proof  we drop the superscript π for simplicity  that is vβ = vπ
β  
vπ
not the optimal value function. Intuitively  the proof is based on “moving” the rewards to earlier
steps to obtain a regular rewards structure. A small technical problem with this approach is that
moving the rewards that are close to the initial time step decreases the bound. Therefore  we treat
these rewards separately within the constant ρ. First  we show that for f(i) ≥ k  we have that
γi − βi ≤ γf (i) − βf (i). Let j = f(i) = i − k  for some k ≥ 0. Then:

γj − βj ≥ γj+k − βj+k

j ≥

max

β∈[0 γ−φ]

log(1 − βk) − log(1 − γk)

log(γ) − log(β)

≥

− log(1 − γk)

log(γ) − log(γ − φ)

 

with the maximization used to get a sufﬁcient condition independent of β. Since the function f
maps only at most (cid:98)k/q(cid:99) values of Im to j < k  there is such |Iz| = k  that ∀x ∈ Im \ Iz f(x) ≥ k
Then we have for j > k:



(γf (i) − βf (i))tf (i)

 (cid:88)

 ≤ ρ + lim

m→∞ E

 (cid:88)

i=1...m∧f (i)≥k

0 ≤ vγ − vβ = lim

(γi − βi)

m→∞ E

∞(cid:88)

≤ ρ +

i∈Im\Iz
(γj − βj)tj = ρ + γk(uγ − uβ).

j=k

Because the playing board in Tetris is 10 squares wide  and each piece has 4 squares  it takes on
average 2.5 moves to remove a line. Since Theorem 10 applies only to integer values of q  we use a
Tetris formulation in which dropping each piece requires two steps. A proper Tetris action is taken
in the ﬁrst step  and there is no action in the second one. To make this model identical to the original
1
2 . Then the upper bound from Theorem 10 on the
formulation  we change the discount factor to γ
discount error is: (cid:107)vγ − vβ(cid:107)∞ ≤ γk(γ2.5 − β2.5)/(1 − γ2.5)(1 − β2.5) + ρ  Notice that ρ is a
constant; it is independent of the new discount factor β.
The sparse rewards property can now be used to motivate the performance increase  even if the
approximation error is bounded by /(1 − β) instead of by /(1 − β)3 (as Lemma 7 suggests).
The approximation error bound will not  in most cases  satisfy the sparsity assumption  as the errors
are typically distributed almost uniformly over the state space and is received in every step as a
result. Therefore  for sparse rewards  the discount error increase will typically be offset by the larger
decrease in the approximation error.

The cumulative error bounds derived above predict it is beneﬁcial to reduce the discount factor to β
when:

(γ2.5 − β2.5)

(1 − γ2.5)(1 − β2.5)

+ ρ + 

1 − β

(cid:107)vγ − vβ(cid:107)∞ ≤ γk



<

1 − γ

.

The effective discount factor γ∗ in Tetris is not known  but consider for example that it is γ∗ =
0.99. Assuming φ = 0.1 we have that k = 48  which means that the ﬁrst (cid:98)48/2.5(cid:99) rewards must
be excluded  and included in ρ. The bounds then predict that for  ≥ 0.4 the performance of
approximate value iteration may be expected to improve using β ≤ γ − φ.
We end by empirically illustrating the inﬂuence of reward sparsity in a general context. Consider
a simple 1-policy  7-state chain problem. Consider two reward instances  one with a single reward
of 1  and the other with randomly generated rewards. We show the comparison of the effects of a
lower discount factor of these two examples in Figure 6. The dotted line represents the global error
with sparse rewards  and the solid line represents the cumulative error with dense rewards. Sparsity
of rewards makes a decrease of the discount factor more interesting.

6 Conclusion and Future Work

We show in this paper that some common approximation error bounds may be tightened with a lower
discount factor. We also identiﬁed a class of problems in which a lower discount factor is likely to
increase the performance of approximate dynamic programming algorithms. In particular  these are
problems in which the rewards are received relatively sparsely. We concentrated on a theoretical
analysis of the inﬂuence of the discount factor  not on the speciﬁc methods which could be used to
determine a discount factor. The actual dependence of the performance on the discount factor may
be non-trivial  and therefore hard to predict based on simple bounds. Therefore  the most practical
approach is to ﬁrst predict an improving discount factor based on the theoretical predictions  and
then use line search to ﬁnd a discount factor that ensures good performance. This is possible since
the discount factor is a single-dimensional variable with a limited range.
The central point of our analysis is based on bounds that are in general quite loose. An important
future direction is to analyze the approximation error more carefully. We shall do experiments
in order to see if we can have some insight on the form (i.e.
the distribution) of the error for
several settings (problems  approximation architecture). If such errors follow some law  it might be
interesting to see whether it helps to tighten the bounds.

Acknowledgements This work was supported in part by the Air Force Ofﬁce of Scientiﬁc Research Grant
No. FA9550-08-1-0171 and by the National Science Foundation Grant No. 0535061. The ﬁrst author was also
supported by a University of Massachusetts Graduate Fellowship.

References
[1] Dimitri P. Bertsekas and Sergey Ioffe. Temporal differences-based policy iteration and applications in

neuro-dynamic programming. Technical Report LIDS-P-2349  LIDS  1997.

[2] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-dynamic programming. Athena Scientiﬁc  1996.
[3] V.F. Farias and B. Van Roy. Probabilistic and Randomized Methods for Design Under Uncertainty  chapter

6: Tetris: A Study of Randomized Constraint Sampling. Springer-Verlag  2006.

[4] Sham Machandranath Kakade. A Natural Policy Gradient. In Advances in neural information processing

systems  pages 1531–1538. MIT Press  2001.

[5] Ronald Parr  Lihong Li  Gavin Taylor  Christopher Painter-Wakeﬁeld  and Michael L. Littman. An analysis
of linear models  linear value function approximation  and feature selection for reinforcement learning. In
International Conference on Machine Learning  2008.

[6] Warren B. Powell. Approximate Dynamic Programming. Wiley-Interscience  2007.
[7] Martin L. Puterman. Markov decision processes: Discrete stochastic dynamic programming. John Wiley

& Sons  Inc.  2005.

[8] Richard S. Sutton and Andrew Barto. Reinforcement learning. MIT Press  1998.

,Christopher Tosh
Sanjoy Dasgupta