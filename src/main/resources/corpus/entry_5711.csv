2018,Understanding Batch Normalization,Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet  despite its enormous success  there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN  following an empirical approach. We conduct several experiments  and show that BN primarily enables training with larger learning rates  which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth  which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation  which enables larger gradient steps  yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory  shedding new light on classical initialization schemes and their consequences.,Understanding Batch Normalization

Johan Bjorck  Carla Gomes  Bart Selman  Kilian Q. Weinberger

Cornell University

{njb225 gomes selman kqw4} @cornell.edu

Abstract

Batch normalization (BN) is a technique to normalize activations in intermediate
layers of deep neural networks.
Its tendency to improve accuracy and speed
up training have established BN as a favorite technique in deep learning. Yet 
despite its enormous success  there remains little consensus on the exact reason
and mechanism behind these improvements. In this paper we take a step towards a
better understanding of BN  following an empirical approach. We conduct several
experiments  and show that BN primarily enables training with larger learning rates 
which is the cause for faster convergence and better generalization. For networks
without BN we demonstrate how large gradient updates can result in diverging loss
and activations growing uncontrollably with network depth  which limits possible
learning rates. BN avoids this problem by constantly correcting activations to
be zero-mean and of unit standard deviation  which enables larger gradient steps 
yields faster convergence and may help bypass sharp local minima. We further show
various ways in which gradients and activations of deep unnormalized networks are
ill-behaved. We contrast our results against recent ﬁndings in random matrix theory 
shedding new light on classical initialization schemes and their consequences.

1

Introduction

Normalizing the input data of neural networks to zero-mean and constant standard deviation has been
known for decades [29] to be beneﬁcial to neural network training. With the rise of deep networks 
Batch Normalization (BN) naturally extends this idea across the intermediate layers within a deep
network [23]  although for speed reasons the normalization is performed across mini-batches and not
the entire training set. Nowadays  there is little disagreement in the machine learning community that
BN accelerates training  enables higher learning rates  and improves generalization accuracy [23]
and BN has successfully proliferated throughout all areas of deep learning [2  17  21  46]. However 
despite its undeniable success  there is still little consensus on why the beneﬁts of BN are so
pronounced. In their original publication [23] Ioffe and Szegedy hypothesize that BN may alleviate
“internal covariate shift” – the tendency of the distribution of activations to drift during training  thus
affecting the inputs to subsequent layers. However  other explanations such as improved stability of
concurrent updates [13] or conditioning [42] have also been proposed.
Inspired by recent empirical insights into deep learning [25  36  57]  in this paper we aim to clarify
these vague intuitions by placing them on solid experimental footing. We show that the activations
and gradients in deep neural networks without BN tend to be heavy-tailed. In particular  during an
early on-set of divergence  a small subset of activations (typically in deep layer) “explode”. The
typical practice to avoid such divergence is to set the learning rate to be sufﬁciently small such that
no steep gradient direction can lead to divergence. However  small learning rates yield little progress
along ﬂat directions of the optimization landscape and may be more prone to convergence to sharp
local minima with possibly worse generalization performance [25].
BN avoids activation explosion by repeatedly correcting all activations to be zero-mean and of unit
standard deviation. With this “safety precaution”  it is possible to train networks with large learning

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

rates  as activations cannot grow incrontrollably since their means and variances are normalized.
SGD with large learning rates yields faster convergence along the ﬂat directions of the optimization
landscape and is less likely to get stuck in sharp minima.
We investigate the interval of viable learning rates for networks with and without BN and conclude
that BN is much more forgiving to very large learning rates. Experimentally  we demonstrate that the
activations in deep networks without BN grow dramatically with depth if the learning rate is too large.
Finally  we investigate the impact of random weight initialization on the gradients in the network
and make connections with recent results from random matrix theory that suggest that traditional
initialization schemes may not be well suited for networks with many layers — unless BN is used to
increase the network’s robustness against ill-conditioned weights.

1.1 The Batch Normalization Algorithm

As in [23]  we primarily consider BN for convolutional neural networks. Both the input and output of
a BN layer are four dimensional tensors  which we refer to as Ib c x y and Ob c x y  respectively. The
dimensions corresponding to examples within a batch b  channel c  and two spatial dimensions x  y
respectively. For input images the channels correspond to the RGB channels. BN applies the same
normalization for all activations in a given channel 
(cid:112)
Ib c x y − µc
(cid:80)
σ2
c + 

Here  BN subtracts the mean activation µc = 1|B|
b x y Ib c x y from all input activations in channel
c  where B contains all activations in channel c across all features b in the entire mini-batch and all
spatial x  y locations. Subsequently  BN divides the centered activation by the standard deviation σc
(plus  for numerical stability) which is calculated analogously. During testing  running averages of
the mean and variances are used. Normalization is followed by a channel-wise afﬁne transformation
parametrized through γc  βc  which are learned during training.

Ob c x y ← γc

+ βc

∀ b  c  x  y.

(1)

1.2 Experimental Setup

To investigate batch normalization we will use an experimental setup similar to the original Resnet
paper [17]: image classiﬁcation on CIFAR10 [27] with a 110 layer Resnet. We use SGD with
momentum and weight decay  employ standard data augmentation and image preprocessing tech-
niques and decrease learning rate when learning plateaus  all as in [17] and with the same parameter
values. The original network can be trained with initial learning rate 0.1 over 165 epochs  how-
ever which fails without BN. We always report the best results among initial learning rates from
{0.1  0.003  0.001  0.0003  0.0001  0.00003} and use enough epochs such that learning plateaus. For
further details  we refer to Appendix B in the online version [4].

2 Disentangling the beneﬁts of BN

Without batch normalization  we have found that the initial learning rate of the Resnet model needs
to be decreased to α = 0.0001 for convergence and training takes roughly 2400 epochs. We refer to
this architecture as an unnormalized network. As illustrated in Figure 1 this conﬁguration does not
attain the accuracy of its normalized counterpart. Thus  seemingly  batch normalization yields faster
training  higher accuracy and enable higher learning rates. To disentangle how these beneﬁts are
related  we train a batch normalized network using the learning rate and the number of epochs of an
unnormalized network  as well as an initial learning rate of α = 0.003 which requires 1320 epochs for
training. These results are also illustrated in Figure 1  where we see that a batch normalized networks
with such a low learning schedule performs no better than an unnormalized network. Additionally 
the train-test gap is much larger than for normalized networks using lr α = 0.1  indicating more
overﬁtting. A learning rate of α = 0.003 gives results in between these extremes. This suggests that
it is the higher learning rate that BN enables  which mediates the majority of its beneﬁts; it improves
regularization  accuracy and gives faster convergence. Similar results can be shown for variants of
BN  see Table 4 in Appendix K of the online version [4].

2

Figure 1: The training (left) and testing (right) accuracies as a function of progress through the training cycle.
We used a 110-layer Resnet with three distinct learning rates 0.0001  0.003  0.1. The smallest  0.0001 was
picked such that the network without BN converges. The ﬁgure shows that with matching learning rates  both
networks  with BN and without  result in comparable testing accuracies (red and green lines in right plot). In
contrast  larger learning rates yield higher test accuracy for BN networks  and diverge for unnormalized networks
(not shown). All results are averaged over ﬁve runs with std shown as shaded region around mean.

2.1 Learning rate and generalization

To explain these observations we consider a simple model of SGD; the loss function (cid:96)(x) is a sum
(cid:80)
over the losses of individual examples in our dataset (cid:96)(x) = 1
i=1 (cid:96)i(x). We model SGD as
N
sampling a set B of examples from the dataset with replacements  and then with learning rate α
i∈B ∇(cid:96)i(x). If we subtract and add α∇(cid:96)(x) from
estimate the gradient step as α∇SGD(x) = α|B|
(cid:88)
this expression we can restate the estimated gradient ∇SGD(x) as the true gradient  and a noise term
(cid:124) (cid:123)(cid:122) (cid:125)
We note that since we sample uniformly we have E(cid:2) α|B|
(cid:0)∇(cid:96)i(x) − ∇(cid:96)(x)(cid:1)(cid:3) = 0. Thus the
noise quantity C of a single gradient estimate such that C = E(cid:2)(cid:107)∇(cid:96)i(x) − ∇(cid:96)(x)||2(cid:3). Using basic

gradient estimate is unbiased  but will typically be noisy. Let us deﬁne an architecture dependent

α∇SGD(x) = α∇(cid:96)(x)

(cid:80)N
(cid:0)∇(cid:96)i(x) − ∇(cid:96)(x)(cid:1)
(cid:125)
(cid:123)(cid:122)
(cid:80)

error term

linear algebra and probability theory  see Apppendix D  we can upper-bound the noise of the gradient
step estimate given by SGD as

gradient

+

α
|B|

(cid:124)

i∈B

.

i∈B

E(cid:2)(cid:107)α∇(cid:96)(x) − α∇SGD(x)(cid:107)2(cid:3) ≤ α2

|B| C.

(2)

Depending on the tightness of this bound  it suggests that the noise in an SGD step is affected similarly
by the learning rate as by the inverse mini-batch size 1|B|. This has indeed been observed in practice
in the context of parallelizing neural networks [14  49] and derived in other theoretical models [24]. It
is widely believed that the noise in SGD has an important role in regularizing neural networks [6  57].
Most pertinent to us is the work of Keskar et al. [25]  where it is empirically demonstrated that large
mini-batches lead to convergence in sharp minima  which often generalize poorly. The intuition is
that larger SGD noise from smaller mini-batches prevents the network from getting “trapped” in
sharp minima and therefore bias it towards wider minima with better generalization. Our observation
from (2) implies that SGD noise is similarly affected by the learning rate as by the inverse mini-bath
size  suggesting that a higher learning rate would similarly bias the network towards wider minima.
We thus argue that the better generalization accuracy of networks with BN  as shown in Figure 1  can
be explained by the higher learning rates that BN enables.

3 Batch Normalization and Divergence

So far we have provided empirical evidence that the beneﬁts of batch normalization are primarily
caused by higher learning rates. We now investigate why BN facilitates training with higher learning
rates in the ﬁrst place. In our experiments  the maximum learning rates for unnormalized networks

3

have been limited by the tendency of neural networks to diverge for large rates  which typically
happens in the ﬁrst few mini-batches. We therefore focus on the gradients at initialization. When
comparing the gradients between batch normalized and unnormalized networks one consistently
ﬁnds that the gradients of comparable parameters are larger and distributed with heavier tails in
unnormalized networks. Representative distributions for gradients within a convolutional kernel are
illustrated in Figure 2.

Figure 2: Histograms over the gradients at initialization for (midpoint) layer 55 of a network with BN (left) and
without (right). For the unnormalized network  the gradients are distributed with heavy tails  whereas for the
normalized networks the gradients are concentrated around the mean. (Note that we have to use different scales
for the two plots because the gradients for the unnormalized network are almost two orders of magnitude larger
than for the normalized on.)

A natural way of investigating divergence is to look at the loss landscape along the gradient direction
during the ﬁrst few mini-batches that occur with the normal learning rate (0.1 with BN  0.0001
without). In Figure 3 we compare networks with and without BN in this regard. For each network
we compute the gradient on individual batches and plot the relative change in loss as a function of
the step-size (i.e. new_loss/old_loss). (Please note the different scales along the vertical axes.) For
unnormalized networks only small gradient steps lead to reductions in loss  whereas networks with
BN can use a far broader range of learning rates.
Let us deﬁne network divergence as the point when the loss of a mini-batch increases beyond
103 (a point from which networks have never managed to recover to acceptable accuracies in

Figure 3: Illustrations of the relative loss over a mini-batch as a function of the step-size (normalized by the loss
before the gradient step). Several representative batches and networks are shown  each one picked at the start of
the standard training procedure. Throughout all cases the network with BN (bottom row) is far more forgiving
and the loss decreases over larger ranges of α. Networks without BN show divergence for larger step sizes.

4

102<latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit>102<latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit><latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit><latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit><latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit>104<latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit><latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit><latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit><latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit>106<latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit><latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit><latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit><latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit>102<latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit>102<latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit><latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit><latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit><latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit>104<latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit><latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit><latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit><latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit>106<latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit><latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit><latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit><latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit>102<latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit>102<latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit><latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit><latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit><latexit sha1_base64="A/It/qc33VO3maO2UZA8awI9Mgo=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJWZIuiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVatarv+O6iUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANIrjp8=</latexit>104<latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit><latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit><latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit><latexit sha1_base64="hVNG2l+8/ng3/UUqTnRsnHgMv1c=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZKeiy6MZlBXuBdiyZNNPGZpIhyQhl6Du4caGIW9/HnW9j2s5CW38IfPznHHLOHyaCG+t536iwtr6xuVXcLu3s7u0flA+PWkalmrImVULpTkgME1yypuVWsE6iGYlDwdrh+GZWbz8xbbiS93aSsCAmQ8kjTol1Vsv3HrLatF+ueFVvLrwKfg4VyNXol796A0XTmElLBTGm63uJDTKiLaeCTUu91LCE0DEZsq5DSWJmgmy+7RSfOWeAI6XdkxbP3d8TGYmNmcSh64yJHZnl2sz8r9ZNbXQVZFwmqWWSLj6KUoGtwrPT8YBrRq2YOCBUc7crpiOiCbUuoJILwV8+eRVaF1Xf8V2tUr/O4yjCCZzCOfhwCXW4hQY0gcIjPMMrvCGFXtA7+li0FlA+cwx/hD5/ANU1jqE=</latexit>106<latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit><latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit><latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit><latexit sha1_base64="acCUN7yPLGqtrlfKJPQ2GUV8HMs=">AAAB7XicbZDLSgMxFIZP6q3WW9Wlm2ARXJUZEXVZdOOygr1AO5ZMmmljM8mQZIQy9B3cuFDEre/jzrcxbWehrT8EPv5zDjnnDxPBjfW8b1RYWV1b3yhulra2d3b3yvsHTaNSTVmDKqF0OySGCS5Zw3IrWDvRjMShYK1wdDOtt56YNlzJeztOWBCTgeQRp8Q6q+l7D9nFpFeueFVvJrwMfg4VyFXvlb+6fUXTmElLBTGm43uJDTKiLaeCTUrd1LCE0BEZsI5DSWJmgmy27QSfOKePI6XdkxbP3N8TGYmNGceh64yJHZrF2tT8r9ZJbXQVZFwmqWWSzj+KUoGtwtPTcZ9rRq0YOyBUc7crpkOiCbUuoJILwV88eRmaZ1Xf8d15pXadx1GEIziGU/DhEmpwC3VoAIVHeIZXeEMKvaB39DFvLaB85hD+CH3+ANg/jqM=</latexit>104<latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit>101<latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit>102<latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit>103<latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit>104<latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit>101<latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit>102<latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit>103<latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit>104<latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit>101<latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit>102<latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit>103<latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit>104<latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit>101<latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit>102<latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit>103<latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit>104<latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit>101<latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit>102<latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit>103<latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit>104<latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit><latexit sha1_base64="gDxbt4I0hhQ6Gj4YuyWodNSdV/Q=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuatN+ueJW3bnIKng5VCBXo1/+6g1ilkYoDRNU667nJsbPqDKcCZyWeqnGhLIxHWLXoqQRaj+brzslZ9YZkDBW9klD5u7viYxGWk+iwHZG1Iz0cm1m/lfrpia89jMuk9SgZIuPwlQQE5PZ7WTAFTIjJhYoU9zuStiIKsqMTahkQ/CWT16F1mXVs3xfq9Rv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc/+o7Y</latexit>101<latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit><latexit sha1_base64="UKbwS1Ie99APchL93wQrL+utz3o=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJpKDHohePFewHtLFstpN26WYTdjdCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNEcG1c99sprK1vbG4Vt0s7u3v7B+XDo5aOU8WwyWIRq05ANQousWm4EdhJFNIoENgOxrezevsJleaxfDCTBP2IDiUPOaPGWm3PfcwuvGm/XHGr7lxkFbwcKpCr0S9/9QYxSyOUhgmqdddzE+NnVBnOBE5LvVRjQtmYDrFrUdIItZ/N152SM+sMSBgr+6Qhc/f3REYjrSdRYDsjakZ6uTYz/6t1UxNe+xmXSWpQssVHYSqIicnsdjLgCpkREwuUKW53JWxEFWXGJlSyIXjLJ69C67LqWb6vVeo3eRxFOIFTOAcPrqAOd9CAJjAYwzO8wpuTOC/Ou/OxaC04+cwx/JHz+QM7a47V</latexit>102<latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit><latexit sha1_base64="Th8nDr96+mObTECMD5ILVXJA+hY=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJiqDHohePFewHtLFstpt26WYTdidCCf0RXjwo4tXf481/47bNQVtfWHh4Z4adeYNECoOu++0U1tY3NreK26Wd3b39g/LhUcvEqWa8yWIZ605ADZdC8SYKlLyTaE6jQPJ2ML6d1dtPXBsRqwecJNyP6FCJUDCK1mp77mN2UZv2yxW36s5FVsHLoQK5Gv3yV28QszTiCpmkxnQ9N0E/oxoFk3xa6qWGJ5SN6ZB3LSoaceNn83Wn5Mw6AxLG2j6FZO7+nshoZMwkCmxnRHFklmsz879aN8Xw2s+ESlLkii0+ClNJMCaz28lAaM5QTixQpoXdlbAR1ZShTahkQ/CWT16FVq3qWb6/rNRv8jiKcAKncA4eXEEd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+CPn8wc88I7W</latexit>103<latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit><latexit sha1_base64="lmEPsdYEEewUZBa9xCdouYdvCG4=">AAAB7nicbZBNS8NAEIYn9avWr6pHL4tF8GJJVNBj0YvHCvYD2lg22027dLMJuxOhhP4ILx4U8erv8ea/cdvmoK0vLDy8M8POvEEihUHX/XYKK6tr6xvFzdLW9s7uXnn/oGniVDPeYLGMdTughkuheAMFSt5ONKdRIHkrGN1O660nro2I1QOOE+5HdKBEKBhFa7U89zE7u5j0yhW36s5ElsHLoQK56r3yV7cfszTiCpmkxnQ8N0E/oxoFk3xS6qaGJ5SN6IB3LCoaceNns3Un5MQ6fRLG2j6FZOb+nshoZMw4CmxnRHFoFmtT879aJ8Xw2s+ESlLkis0/ClNJMCbT20lfaM5Qji1QpoXdlbAh1ZShTahkQ/AWT16G5nnVs3x/Wand5HEU4QiO4RQ8uIIa3EEdGsBgBM/wCm9O4rw4787HvLXg5DOH8EfO5w8+dY7X</latexit>step sizestep sizestep sizerelative lossrelative lossmini-batch 1mini-batch 4mini-batch 7with BNw/o BNFigure 4: Heatmap of channel means and variances during a diverging gradient update (without BN). The
vertical axis denote what percentage of the gradient update has been applied  100% corresponds to the endpoint
of the update. The moments explode in the higher layer (note the scale of the color bars).

our experiments). With this deﬁnition  we can precisely ﬁnd the gradient update responsible for
divergence. It is interesting to see what happens with the means and variances of the network
activations along a ’diverging update’. Figure 4 shows the means and variances of channels in
three layers (8 44 80) during such an update (without BN). The color bar reveals that the scale
of the later layer’s activations and variances is orders of magnitudes higher than the earlier layer.
This seems to suggest that the divergence is caused by activations growing progressively larger
with network depth  with the network output “exploding” which results in a diverging loss. BN
successfully mitigates this phenomenon by correcting the activations of each channel and each layer
to zero-mean and unit standard deviation  which ensures that large activations in lower levels cannot
propagate uncontrollably upwards. We argue that this is the primary mechanism by which batch
normalization enables higher learning rates. This explanation is also consistent with the general
folklore observations that shallower networks allow for larger learning rates  which we verify in
Appendix H. In shallower networks there aren’t as many layers in which the activation explosion can
propagate.

4 Batch Normalization and Gradients

Figure 4 shows that the moments of unnormalized networks explode during network divergence and
Figure 5 depicts the moments as a function of the layer depth after initialization (without BN) in
log-scale. The means and variances of channels in the network tend to increase with the depth of
the network even at initialization time — suggesting that a substantial part of this growth is data
independent. In Figure 5 we also note that the network transforms normalized inputs into an output
that reaches scales of up to 102 for the largest output channels. It is natural to suspect that such a
dramatic relationship between output and input are responsible for the large gradients seen in Figure
2. To test this intuition  we train a Resnet that uses one batch normalization layer only at the very last
layer of the network  normalizing the output of the last residual block but no intermediate activation.
Such an architecture allows for learning rates up to 0.03 and yields a ﬁnal test accuracy of 90.1%  see
Appendix E — capturing two-thirds of the overall BN improvement (see Figure 1). This suggests that
normalizing the ﬁnal layer of a deep network may be one of the most important contributions of BN.
For the ﬁnal output layer corresponding to the classiﬁcation  a large channel mean implies that the
network is biased towards the corresponding class. In Figure 6 we created a heatmap of ∂Lb
after
∂Ob j

5

Figure 5: Average channel means and variances as a function of network depth at initialization (error bars show
standard deviations) on log-scale for networks with and without BN. The batch normalized network the mean
and variances stays relatively constant throughout the network. For an unnormalized network  they seem to grow
almost exponentially with depth.

Figure 6: A heat map of the output gradients in the ﬁnal classiﬁcation layer after initialization. The columns
correspond to a classes and the rows to images in the mini-batch. For an unnormalized network (left)  it is
evident that the network consistently predicts one speciﬁc class (very right column)  irrespective of the input. As
a result  the gradients are highly correlated. For a batch normalized network  the dependence upon the input is
much larger.

initialization  where Lb is the loss for image b in our mini-batch  and activations j corresponds to
class j at the ﬁnal layer. A yellow entry indicates that the gradient is positive  and the step along the
negative gradient would decrease the prediction strength of this class for this particular image. A
dark blue entry indicates a negative gradient  indicating that this particular class prediction should
be strengthened. Each row contains one dark blue entry  which corresponds to the true class of this
particular image (as initially all predictions are arbitrary). A striking observation is the distinctly
yellow column in the left heatmap (network without BN). This indicates that after initialization the
network tends to almost always predict the same (typically wrong) class  which is then corrected
with a strong gradient update. In contrast  the network with BN does not exhibit the same behavior 
instead positive gradients are distributed throughout all classes. Figure 6 also sheds light onto why
the gradients of networks without BN tend to be so large in the ﬁnal layers: the rows of the heatmap
(corresponding to different images in the mini-batch) are highly correlated. Especially the gradients
in the last column are positive for almost all images (the only exceptions being those image that
truly belong to this particular class label). The gradients  summed across all images in the minibatch 
therefore consist of a sum of terms with matching signs and yield large absolute values. Further 
these gradients differ little across inputs  suggesting that most of the optimization work is done to
rectify a bad initial state rather than learning from the data.

4.1 Gradients of convolutional parameters

We observe that the gradients in the last layer can be dominated by some arbitrary bias towards a
particular class. Can a similar reason explain why the gradients for convolutional weights are larger

6

a =(cid:80)

cociij|

bxy |dbxy
7.5e-05
1.9e-05
6.6e-06
6.3e-05
2.2e-04
2.6e-04

Layer 18  with BN
Layer 54  with BN
Layer 90  with BN
Layer 18  w/o BN
Layer 54  w/o BN
Layer 90  w/o BN

b = |(cid:80)

bij dbxy

cociij|

3.0e-07
1.7e-07
1.6e-07
3.6e-05
8.4e-05
1.2e-04

a/b
251.8
112.8
40.7
1.7
2.6
2.1

Table 1: Gradients of a convolutional kernel as described in (4) at initialization. The table compares the absolute
value of the sum of gradients  and the sum of absolute values. Without BN these two terms are similar in
magnitude  suggesting that the summands have matching signs throughout and are largely data independent. For
a batch normalized network  those two differ by about two orders of magnitude.

for unnormalized networks. Let us consider a convolutional weight Ko i x y  where the ﬁrst two
dimensions correspond to the outgoing/ingoing channels and the two latter to the spatial dimensions.
For notational clarity we consider 3-by-3 convolutions and deﬁne S = {−1  0  1}×{−1  0  1} which
indexes into K along spatial dimensions. Using deﬁnitions from section 1.1 we have

Ob c x y =

Ib c(cid:48) x+x(cid:48) y+y(cid:48)Kc c(cid:48) x(cid:48) y(cid:48)

(3)

(cid:88)

(cid:88)

c(cid:48)

x(cid:48) y(cid:48)∈S

Now for some parameter Ko i x y inside the convolutional weight K  its derivate with respect to the
loss is given by the backprop equation [40] and (3) as

∂L

∂Ko i x(cid:48) y(cid:48)

=

dbxy
o i x(cid:48) y(cid:48) 

where

dbxy
o i x(cid:48) y(cid:48) =

∂L

∂ Ob o x y

Ib i x+x(cid:48) y+y(cid:48).

(4)

(cid:88)

b x y

The gradient for Ko i x y is the sum over the gradients of examples within the mini-batch  and over the
convoluted spatial dimensions. We investigate the signs of the summands in (4) across both network
types and probe the sums at initialization in Table 1. For an unnormalized networks the absolute value
of (4) and the sum of the absolute values of the summands generally agree to within a factor 2 or less.
For a batch normalized network  these expressions differ by a factor of 102  which explains the stark
difference in gradient magnitude between normalized and unnormalized networks observed in Figure
2. These results suggest that for an unnormalized network  the summands in (4) are similar across
both spatial dimensions and examples within a batch. They thus encode information that is neither
input-dependent or dependent upon spatial dimensions  and we argue that the learning rate would
be limited by the large input-independent gradient component and that it might be too small for the
input-dependent component. We probe these questions further in Appendix J  where we investigate
individual parameters instead of averages.
Table 1 suggests that for an unnormalized network the gradients are similar across spatial dimensions
and images within a batch. It’s unclear however how they vary across the input/output channels i  o.
oixy| at initialization  which intuitively
measures the average gradient magnitude of kernel parameters between input channel i and output
channel o. Representative results are illustrated in Figure 7. The heatmap shows a clear trend that
some channels constantly are associated with larger gradients while others have extremely small
gradients by comparison. Since some channels have large means  we expect in light of (4) that
weights outgoing from such channels would have large gradients which would explain the structure
in Figure 7. This is indeed the case  see Appendix G in the online version [4].

To study this we consider the matrix Mi o =(cid:80)

xy |(cid:80)

bxy dbxy

5 Random initialization

In this section argue that the gradient explosion in networks without BN is a natural consequence of
random initialization. This idea seems to be at odds with the trusted Xavier initialization scheme [12]
which we use. Doesn’t such initialization guarantee a network where information ﬂows smoothly
between layers? These initialization schemes are generally derived from the desiderata that the
variance of channels should be constant when randomization is taken over random weights. We argue
that this condition is too weak. For example  a pathological initialization that sets weights to 0 or

7

Figure 7: Average absolute gradients for parameters between in and out channels for layer 45 at initialization.
For an unnormalized network  we observe a dominant low-rank structure. Some in/out-channels have consistently
large gradients while others have consistently small gradients. This structure is less pronounced with batch
normalization (right).

100 with some probability could fulﬁll it. In [12] the authors make simplifying assumptions that
essentially result in a linear neural network. We consider a similar scenario and connect them with
recent results in random matrix theory to gain further insights into network generalization. Let us
consider a simple toy model: a linear feed-forward neural network where At ... A2A1x = y  for
weight matrices A1  A2...An. While such a model clearly abstracts away many important points they
have proven to be valuable models for theoretical studies [12  15  32  56]. CNNs can  of course 
be ﬂattened into fully-connected layers with shared weights. Now  if the matrices are initialized
randomly  the network can simply be described by a product of random matrices. Such products have
recently garnered attention in the ﬁeld of random matrix theory  from which we have the following
recent result due to [30].

Theorem 1 Singular value distribution of products of independent Gaussian matrices [30]. As-
sume that X = X1X2...XM   where Xi are independent N × N Gaussian matrices s.t. E[Xi  jk] = 0
i /N for all matrices i and indices j  k. In the limit N → ∞  the expected singular
and E[X 2

value density ρM (x) of X for x ∈(cid:0)0  (M + 1)M +1/M M(cid:1) is given by

i  jk] = σ2

ρM (x) =

1
πx

sin((M + 1)ϕ)

sin(M ϕ)

sin ϕ 

where

x =

(cid:0) sin((M + 1)ϕ)(cid:1)M +1

sin ϕ(sin(M ϕ))M

(5)

Figure 8 illustrates some density plots for var-
ious values of M and θ. A closer look at
(5) reveals that the distribution blows up as
x−M/(M +1) nears the origin  and that the largest
singular value scales as O(M ) for large matri-
ces. In Figure 9 we investigate the singular value
distribution for practically sized matrices. By
multiplying more matrices  which represents a
deeper linear network  the singular values distri-
bution becomes signiﬁcantly more heavy-tailed.
Intuitively this means that the ratio between the
largest and smallest singular value (the condi-
tion number) will increase with depth  which we
verify in Figure 20 in Appendix K.
Consider minAi  i=1 2 ... t (cid:107)At ... A2A1x−y(cid:107)2 
this problem is similar to solving a linear system
minx (cid:107)Ax − y(cid:107)2 if one only optimizes over a
single weight matrix Ai. It is well known that
the complexity of solving minx (cid:107)Ax − y(cid:107) via
gradient descent can be characterized by the
condition number κ of A  the ratio between largest σmax and smallest singular value σmin. Increasing

Figure 8: Distribution of singular values according to
theorem 1 for some M. The theoretical distribution
becomes increasingly heavy-tailed for more matrices  as
does the empirical distributions of Figure 9

8

⇢M(x)<latexit sha1_base64="p3TdDI+2yscOZS7kr71RCZ92dFQ=">AAAB8HicbZDLSgMxFIbP1Futt6pLN8Ei1E2ZEUGXRTduhAr2Iu1QMmnahuYyJBmxDH0KNy4UcevjuPNtTNtZaOsPgY//nEPO+aOYM2N9/9vLrayurW/kNwtb2zu7e8X9g4ZRiSa0ThRXuhVhQzmTtG6Z5bQVa4pFxGkzGl1P681Hqg1T8t6OYxoKPJCszwi2znro6KHq3pafTrvFkl/xZ0LLEGRQgky1bvGr01MkEVRawrEx7cCPbZhibRnhdFLoJIbGmIzwgLYdSiyoCdPZwhN04pwe6ivtnrRo5v6eSLEwZiwi1ymwHZrF2tT8r9ZObP8yTJmME0slmX/UTziyCk2vRz2mKbF87AATzdyuiAyxxsS6jAouhGDx5GVonFUCx3fnpepVFkcejuAYyhDABVThBmpQBwICnuEV3jztvXjv3se8NedlM4fwR97nDxk5j+4=</latexit><latexit sha1_base64="p3TdDI+2yscOZS7kr71RCZ92dFQ=">AAAB8HicbZDLSgMxFIbP1Futt6pLN8Ei1E2ZEUGXRTduhAr2Iu1QMmnahuYyJBmxDH0KNy4UcevjuPNtTNtZaOsPgY//nEPO+aOYM2N9/9vLrayurW/kNwtb2zu7e8X9g4ZRiSa0ThRXuhVhQzmTtG6Z5bQVa4pFxGkzGl1P681Hqg1T8t6OYxoKPJCszwi2znro6KHq3pafTrvFkl/xZ0LLEGRQgky1bvGr01MkEVRawrEx7cCPbZhibRnhdFLoJIbGmIzwgLYdSiyoCdPZwhN04pwe6ivtnrRo5v6eSLEwZiwi1ymwHZrF2tT8r9ZObP8yTJmME0slmX/UTziyCk2vRz2mKbF87AATzdyuiAyxxsS6jAouhGDx5GVonFUCx3fnpepVFkcejuAYyhDABVThBmpQBwICnuEV3jztvXjv3se8NedlM4fwR97nDxk5j+4=</latexit><latexit sha1_base64="p3TdDI+2yscOZS7kr71RCZ92dFQ=">AAAB8HicbZDLSgMxFIbP1Futt6pLN8Ei1E2ZEUGXRTduhAr2Iu1QMmnahuYyJBmxDH0KNy4UcevjuPNtTNtZaOsPgY//nEPO+aOYM2N9/9vLrayurW/kNwtb2zu7e8X9g4ZRiSa0ThRXuhVhQzmTtG6Z5bQVa4pFxGkzGl1P681Hqg1T8t6OYxoKPJCszwi2znro6KHq3pafTrvFkl/xZ0LLEGRQgky1bvGr01MkEVRawrEx7cCPbZhibRnhdFLoJIbGmIzwgLYdSiyoCdPZwhN04pwe6ivtnrRo5v6eSLEwZiwi1ymwHZrF2tT8r9ZObP8yTJmME0slmX/UTziyCk2vRz2mKbF87AATzdyuiAyxxsS6jAouhGDx5GVonFUCx3fnpepVFkcejuAYyhDABVThBmpQBwICnuEV3jztvXjv3se8NedlM4fwR97nDxk5j+4=</latexit><latexit sha1_base64="p3TdDI+2yscOZS7kr71RCZ92dFQ=">AAAB8HicbZDLSgMxFIbP1Futt6pLN8Ei1E2ZEUGXRTduhAr2Iu1QMmnahuYyJBmxDH0KNy4UcevjuPNtTNtZaOsPgY//nEPO+aOYM2N9/9vLrayurW/kNwtb2zu7e8X9g4ZRiSa0ThRXuhVhQzmTtG6Z5bQVa4pFxGkzGl1P681Hqg1T8t6OYxoKPJCszwi2znro6KHq3pafTrvFkl/xZ0LLEGRQgky1bvGr01MkEVRawrEx7cCPbZhibRnhdFLoJIbGmIzwgLYdSiyoCdPZwhN04pwe6ivtnrRo5v6eSLEwZiwi1ymwHZrF2tT8r9ZObP8yTJmME0slmX/UTziyCk2vRz2mKbF87AATzdyuiAyxxsS6jAouhGDx5GVonFUCx3fnpepVFkcejuAYyhDABVThBmpQBwICnuEV3jztvXjv3se8NedlM4fwR97nDxk5j+4=</latexit>x<latexit sha1_base64="IArQpDG4Gw7Ax+5Wri9CZWKD4Bo=">AAAB6HicbZBNS8NAEIYn9avWr6pHL4tF8FQSEfRY9OKxBfsBbSib7aRdu9mE3Y1YQn+BFw+KePUnefPfuG1z0NYXFh7emWFn3iARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e2s3n5EpXks780kQT+iQ8lDzqixVuOpX664VXcusgpeDhXIVe+Xv3qDmKURSsME1brruYnxM6oMZwKnpV6qMaFsTIfYtShphNrP5otOyZl1BiSMlX3SkLn7eyKjkdaTKLCdETUjvVybmf/VuqkJr/2MyyQ1KNniozAVxMRkdjUZcIXMiIkFyhS3uxI2oooyY7Mp2RC85ZNXoXVR9Sw3Liu1mzyOIpzAKZyDB1dQgzuoQxMYIDzDK7w5D86L8+58LFoLTj5zDH/kfP4A5juM/A==</latexit><latexit sha1_base64="IArQpDG4Gw7Ax+5Wri9CZWKD4Bo=">AAAB6HicbZBNS8NAEIYn9avWr6pHL4tF8FQSEfRY9OKxBfsBbSib7aRdu9mE3Y1YQn+BFw+KePUnefPfuG1z0NYXFh7emWFn3iARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e2s3n5EpXks780kQT+iQ8lDzqixVuOpX664VXcusgpeDhXIVe+Xv3qDmKURSsME1brruYnxM6oMZwKnpV6qMaFsTIfYtShphNrP5otOyZl1BiSMlX3SkLn7eyKjkdaTKLCdETUjvVybmf/VuqkJr/2MyyQ1KNniozAVxMRkdjUZcIXMiIkFyhS3uxI2oooyY7Mp2RC85ZNXoXVR9Sw3Liu1mzyOIpzAKZyDB1dQgzuoQxMYIDzDK7w5D86L8+58LFoLTj5zDH/kfP4A5juM/A==</latexit><latexit sha1_base64="IArQpDG4Gw7Ax+5Wri9CZWKD4Bo=">AAAB6HicbZBNS8NAEIYn9avWr6pHL4tF8FQSEfRY9OKxBfsBbSib7aRdu9mE3Y1YQn+BFw+KePUnefPfuG1z0NYXFh7emWFn3iARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e2s3n5EpXks780kQT+iQ8lDzqixVuOpX664VXcusgpeDhXIVe+Xv3qDmKURSsME1brruYnxM6oMZwKnpV6qMaFsTIfYtShphNrP5otOyZl1BiSMlX3SkLn7eyKjkdaTKLCdETUjvVybmf/VuqkJr/2MyyQ1KNniozAVxMRkdjUZcIXMiIkFyhS3uxI2oooyY7Mp2RC85ZNXoXVR9Sw3Liu1mzyOIpzAKZyDB1dQgzuoQxMYIDzDK7w5D86L8+58LFoLTj5zDH/kfP4A5juM/A==</latexit><latexit sha1_base64="IArQpDG4Gw7Ax+5Wri9CZWKD4Bo=">AAAB6HicbZBNS8NAEIYn9avWr6pHL4tF8FQSEfRY9OKxBfsBbSib7aRdu9mE3Y1YQn+BFw+KePUnefPfuG1z0NYXFh7emWFn3iARXBvX/XYKa+sbm1vF7dLO7t7+QfnwqKXjVDFssljEqhNQjYJLbBpuBHYShTQKBLaD8e2s3n5EpXks780kQT+iQ8lDzqixVuOpX664VXcusgpeDhXIVe+Xv3qDmKURSsME1brruYnxM6oMZwKnpV6qMaFsTIfYtShphNrP5otOyZl1BiSMlX3SkLn7eyKjkdaTKLCdETUjvVybmf/VuqkJr/2MyyQ1KNniozAVxMRkdjUZcIXMiIkFyhS3uxI2oooyY7Mp2RC85ZNXoXVR9Sw3Liu1mzyOIpzAKZyDB1dQgzuoQxMYIDzDK7w5D86L8+58LFoLTj5zDH/kfP4A5juM/A==</latexit>Figure 9: An illustration of the distributions of singular values of random square matrices and product of
independent matrices. The matrices have dimension N=1000 and all entries independently drawn from a standard
Gaussian distribution. Experiments are repeated ten times and we show the total number of singular values
among all runs in every bin  distributions for individual experiments look similar. The left plot shows all
three settings. We see that the distribution of singular values becomes more heavy-tailed as more matrices are
multiplied together.

κ has the following effects on solving a linear system with gradient descent: 1) convergence becomes
slower  2) a smaller learning rate is needed  3) the ratio between gradients in different subspaces
increases [3]. There are many parallels between these results from numerical optimization  and
what is observed in practice in deep learning. Based upon Theorem 1  we expect the conditioning
of a linear neural network at initialization for more shallow networks to be better which would
allow a higher learning rate. And indeed  for an unnormalized Resnet one can use a much larger
learning if it has only few layers  see Appendix H. An increased condition number also results in
different subspaces of the linear regression problem being scaled differently  although the notion of
subspaces are lacking in ANNs  Figure 5 and 7 show that the scale of channels differ dramatically
in unnormalized networks. The Xavier [12] and Kaming initialization schemes [16] amounts to a
random matrix with iid entries that are all scaled proportionally to n−1/2  the same exponent as
in Theorem 1  with different constant factors. Theorem 1 suggests that such an initialization will
yield ill-conditioned matrices  independent of these scale factors. If we accept these shortcomings of
Xavier-initialization  the importance of making networks robust to initialization schemes becomes
more natural.

6 Related Work

The original batch normalization paper posits that internal covariate explains the beneﬁts of BN
[23]. We do not claim that internal covariate shift does not exist  but we believe that the success of
BN can be explained without it. We argue that a good reason to doubt that the primary beneﬁt of
BN is eliminating internal covariate shift comes from results in [34]  where an initialization scheme
that ensures that all layers are normalized is proposed. In this setting  internal covariate shift would
not disappear. However  the authors show that such initialization can be used instead of BN with
a relatively small performance loss. Another line of work of relevance is [48] and [47]  where the
relationship between various network parameters  accuracy and convergence speed is investigated 
the former article argues for the importance of batch normalization to facilitate a phenomenon dubbed
’super convergence’. Due to space limitations  we defer discussion regarding variants of batch
normalization  random matrix theory  generalization as well as further related work to Appendix A in
the online version [4].

7 Conclusions

We have investigated batch normalization and its beneﬁts  showing how the latter are mainly mediated
by larger learning rates. We argue that the larger learning rate increases the implicit regularization
of SGD  which improves generalization. Our experiments show that large parameter updates to
unnormalized networks can result in activations whose magnitudes grow dramatically with depth 
which limits large learning rates. Additionally  we have demonstrated that unnormalized networks
have large and ill-behaved outputs  and that this results in gradients that are input independent. Via
recent results in random matrix theory  we have argued that the ill-conditioned activations are natural
consequences of the random initialization.

9

Acknowledgements

We would like to thank Yexiang Xue  Guillaume Perez  Rich Bernstein  Zdzislaw Burda  Liam
McAllister  Yang Yuan  Vilja Järvi  Marlene Berke and Damek Davis for help and inspiration. This
research is supported by NSF Expedition CCF-1522054 and Awards FA9550-18-1-0136 and FA9550-
17-1-0292 from AFOSR. KQW was supported in part by the III-1618134  III-1526012  IIS-1149882 
IIS-1724282  and TRIPODS- 1740822 grants from the National Science Foundation  and generous
support from the Bill and Melinda Gates Foundation  the Ofﬁce of Naval Research  and SAP America
Inc.

References
[1] Martin Arjovsky  Amar Shah  and Yoshua Bengio. Unitary evolution recurrent neural networks.

In International Conference on Machine Learning  pages 1120–1128  2016.

[2] Jimmy Lei Ba  Jamie Ryan Kiros  and Geoffrey E Hinton. Layer normalization. arXiv preprint

arXiv:1607.06450  2016.

[3] Dimitri P Bertsekas and Athena Scientiﬁc. Convex optimization algorithms. Athena Scientiﬁc

Belmont  2015.

[4] Johan Bjorck  Carla Gomes  and Bart Selman. Understanding batch normalization. arXiv

preprint arXiv:1806.02375  2018.

[5] Pratik Chaudhari  Anna Choromanska  Stefano Soatto  and Yann LeCun. Entropy-sgd: Biasing

gradient descent into wide valleys. arXiv preprint arXiv:1611.01838  2016.

[6] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference 

converges to limit cycles for deep networks. arXiv preprint arXiv:1710.11029  2017.

[7] Yunpeng Chen  Jianan Li  Huaxin Xiao  Xiaojie Jin  Shuicheng Yan  and Jiashi Feng. Dual path

networks. In Advances in Neural Information Processing Systems  pages 4470–4478  2017.

[8] Anna Choromanska  Mikael Henaff  Michael Mathieu  Gérard Ben Arous  and Yann LeCun.
The loss surfaces of multilayer networks. In Artiﬁcial Intelligence and Statistics  pages 192–204 
2015.

[9] Tim Cooijmans  Nicolas Ballas  César Laurent  Ça˘glar Gülçehre  and Aaron Courville. Recur-

rent batch normalization. arXiv preprint arXiv:1603.09025  2016.

[10] Chris de Sa. Advanced machine learning systems : lecture notes  2017.
[11] Alan Edelman. Eigenvalues and condition numbers of random matrices. SIAM Journal on

Matrix Analysis and Applications  9(4):543–560  1988.

[12] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward
In Proceedings of the Thirteenth International Conference on Artiﬁcial

neural networks.
Intelligence and Statistics  pages 249–256  2010.

[13] Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep Learning. MIT Press  2016.

http://www.deeplearningbook.org.

[14] Priya Goyal  Piotr Dollár  Ross Girshick  Pieter Noordhuis  Lukasz Wesolowski  Aapo Kyrola 
Andrew Tulloch  Yangqing Jia  and Kaiming He. Accurate  large minibatch sgd: training
imagenet in 1 hour. arXiv preprint arXiv:1706.02677  2017.

[15] Moritz Hardt and Tengyu Ma.

arXiv:1611.04231  2016.

Identity matters in deep learning.

arXiv preprint

[16] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
international conference on computer vision  pages 1026–1034  2015.

[17] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[18] Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma  Technische

Universität München  91(1)  1991.

[19] Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation  9(1):1–42  1997.

10

[20] Elad Hoffer  Itay Hubara  and Daniel Soudry. Train longer  generalize better: closing the
generalization gap in large batch training of neural networks. In Advances in Neural Information
Processing Systems  pages 1729–1739  2017.

[21] Gao Huang  Zhuang Liu  Kilian Q Weinberger  and Laurens van der Maaten. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition  volume 1  page 3  2017.

[22] Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-
normalized models. In Advances in Neural Information Processing Systems  pages 1942–1950 
2017.

[23] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning  pages
448–456  2015.

[24] Stanislaw Jastrzkebski  Zachary Kenton  Devansh Arpit  Nicolas Ballas  Asja Fischer  Yoshua
arXiv preprint

Bengio  and Amos Storkey. Three factors inﬂuencing minima in sgd.
arXiv:1711.04623  2017.

[25] Nitish Shirish Keskar  Dheevatsa Mudigere  Jorge Nocedal  Mikhail Smelyanskiy  and Ping
Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.
arXiv preprint arXiv:1609.04836  2016.

[26] Günter Klambauer  Thomas Unterthiner  Andreas Mayr  and Sepp Hochreiter. Self-normalizing
neural networks. In Advances in Neural Information Processing Systems  pages 972–981  2017.
[27] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

2009.

[28] César Laurent  Gabriel Pereyra  Philémon Brakel  Ying Zhang  and Yoshua Bengio. Batch
normalized recurrent neural networks. In Acoustics  Speech and Signal Processing (ICASSP) 
2016 IEEE International Conference on  pages 2657–2661. IEEE  2016.

[29] Yann LeCun  Léon Bottou  Genevieve B Orr  and Klaus-Robert Müller. Efﬁcient backprop. In

Neural networks: Tricks of the trade  pages 9–50. Springer  1998.

[30] Dang-Zheng Liu  Dong Wang  Lun Zhang  et al. Bulk and soft-edge universality for singu-
lar values of products of ginibre random matrices. In Annales de l’Institut Henri Poincaré 
Probabilités et Statistiques  volume 52  pages 1734–1762. Institut Henri Poincaré  2016.

[31] Cosme Louart  Zhenyu Liao  and Romain Couillet. A random matrix approach to neural

networks. arXiv preprint arXiv:1702.05419  2017.

[32] Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint

arXiv:1702.08580  2017.

[33] Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks.

arXiv preprint arXiv:1804.07612  2018.

[34] Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422 

2015.

[35] Carles Roger Riera Molina and Oriol Pujol Vila. Solving internal covariate shift in deep learning

with linked neurons. arXiv preprint arXiv:1712.02609  2017.

[36] Behnam Neyshabur  Srinadh Bhojanapalli  David McAllester  and Nati Srebro. Exploring
generalization in deep learning. In Advances in Neural Information Processing Systems  pages
5949–5958  2017.

[37] Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random

matrix theory. In International Conference on Machine Learning  pages 2798–2806  2017.

[38] Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning.
[39] James Renegar. Incorporating condition measures into the complexity theory of linear program-

ming. SIAM Journal on Optimization  5(3):506–524  1995.

[40] David E Rumelhart  Geoffrey E Hinton  and Ronald J Williams. Learning representations by

back-propagating errors. nature  323(6088):533  1986.

[41] Sirpa Saarinen  Randall Bramley  and George Cybenko. Ill-conditioning in neural network

training problems. SIAM Journal on Scientiﬁc Computing  14(3):693–714  1993.

11

[42] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. In Advances in Neural Information Processing
Systems  pages 901–909  2016.

[43] Fabian Schilling. The effect of batch normalization on deep convolutional neural networks 

2016.

[44] Samuel S Schoenholz  Justin Gilmer  Surya Ganguli  and Jascha Sohl-Dickstein. Deep informa-

tion propagation. arXiv preprint arXiv:1611.01232  2016.

[45] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the

log-likelihood function. Journal of statistical planning and inference  90(2):227–244  2000.

[46] David Silver  Julian Schrittwieser  Karen Simonyan  Ioannis Antonoglou  Aja Huang  Arthur
Guez  Thomas Hubert  Lucas Baker  Matthew Lai  Adrian Bolton  et al. Mastering the game of
go without human knowledge. Nature  550(7676):354  2017.

[47] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning

rate  batch size  momentum  and weight decay. arXiv preprint arXiv:1803.09820  2018.

[48] Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of residual networks

using large learning rates. arXiv preprint arXiv:1708.07120  2017.

[49] Samuel L Smith  Pieter-Jan Kindermans  and Quoc V Le. Don’t decay the learning rate  increase

the batch size. arXiv preprint arXiv:1711.00489  2017.

[50] Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic

gradient descent. 2018.

[51] D Ulyanov  A Vedaldi  and VS Lempitsky. Instance normalization: The missing ingredient for

fast stylization. corr (2016). arXiv preprint arXiv:1607.08022.

[52] Patrick Van Der Smagt and Gerd Hirzinger. Solving the ill-conditioning in neural network

learning. In Neural networks: tricks of the trade  pages 193–206. Springer  1998.

[53] Shuang Wu  Guoqi Li  Lei Deng  Liu Liu  Yuan Xie  and Luping Shi. L1-norm batch nor-
malization for efﬁcient training of deep neural networks. arXiv preprint arXiv:1802.09769 
2018.

[54] Yuxin Wu and Kaiming He. Group normalization. arXiv preprint arXiv:1803.08494  2018.
[55] Lechao Xiao  Yasaman Bahri  Jascha Sohl-Dickstein  Samuel S Schoenholz  and Jeffrey Pen-
nington. Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10 000-layer vanilla
convolutional neural networks. arXiv preprint arXiv:1806.05393  2018.

[56] Chulhee Yun  Suvrit Sra  and Ali Jadbabaie. Global optimality conditions for deep neural

networks. arXiv preprint arXiv:1707.02444  2017.

[57] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530  2016.

[58] Tullio Zolezzi. Condition number theorems in optimization. SIAM Journal on Optimization 

14(2):507–516  2003.

12

,Nils Bjorck
Carla Gomes
Bart Selman
Kilian Weinberger