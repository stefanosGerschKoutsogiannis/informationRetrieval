2019,Preference-Based Batch and Sequential Teaching: Towards a Unified View of Models,Algorithmic machine teaching studies the interaction between a teacher and a learner where the teacher selects labeled examples aiming at teaching a target hypothesis. In a quest to lower teaching complexity and to achieve more natural teacher-learner interactions  several teaching models and complexity measures have been proposed for both the batch settings (e.g.  worst-case  recursive  preference-based  and non-clashing models) as well as the sequential settings (e.g.  local preference-based model). To better understand the connections between these different batch and sequential models  we develop a novel framework which captures the teaching process via preference functions $\Sigma$. In our framework  each function $\sigma \in \Sigma$ induces a teacher-learner pair with teaching complexity as $\TD(\sigma)$. We show that the above-mentioned teaching models are equivalent to specific types/families of preference functions in our framework. This equivalence  in turn  allows us to study the differences between two important teaching models  namely $\sigma$ functions inducing the strongest batch (i.e.  non-clashing) model and $\sigma$ functions inducing a weak sequential (i.e.  local preference-based) model.  Finally  we identify preference functions inducing a novel family of sequential models with teaching complexity linear in the VC dimension of the hypothesis class: this is in contrast to the best known complexity result for the batch models which is quadratic in the VC dimension.,Preference-Based Batch and Sequential Teaching:

Towards a Uniﬁed View of Models

Farnam Mansouri† Yuxin Chen‡ Ara Vartanian‹ Xiaojin Zhu‹ Adish Singla†
†Max Planck Institute for Software Systems (MPI-SWS)  {mfarnam  adishs}@mpi-sws.org 

‡University of Chicago  chenyuxin@uchicago.edu 

‹University of Wisconsin-Madison  {aravart  jerryzhu}@cs.wisc.edu

Abstract

Algorithmic machine teaching studies the interaction between a teacher and a
learner where the teacher selects labeled examples aiming at teaching a target
hypothesis. In a quest to lower teaching complexity and to achieve more natural
teacher-learner interactions  several teaching models and complexity measures have
been proposed for both the batch settings (e.g.  worst-case  recursive  preference-
based  and non-clashing models) as well as the sequential settings (e.g.  local
preference-based model). To better understand the connections between these dif-
ferent batch and sequential models  we develop a novel framework which captures
the teaching process via preference functions Σ. In our framework  each function
σ P Σ induces a teacher-learner pair with teaching complexity as TDpσq. We show
that the above-mentioned teaching models are equivalent to speciﬁc types/families
of preference functions in our framework. This equivalence  in turn  allows us to
study the differences between two important teaching models  namely σ functions
inducing the strongest batch (i.e.  non-clashing) model and σ functions induc-
ing a weak sequential (i.e.  local preference-based) model. Finally  we identify
preference functions inducing a novel family of sequential models with teaching
complexity linear in the VC dimension of the hypothesis class: this is in contrast to
the best known complexity result for the batch models which is quadratic in the
VC dimension.

Introduction

1
Algorithmic machine teaching studies the interaction between a teacher and a learner where the
teacher’s goal is to ﬁnd an optimal training sequence to steer the learner towards a target hypothesis
[GK95  ZLHZ11  Zhu13  SBB`14  Zhu15  ZSZR18]. An important quantity of interest is the
teaching dimension (TD) of the hypothesis class  representing the worst-case number of examples
needed to teach any hypothesis in a given class. Given that the teaching complexity depends on
what assumptions are made about teacher-learner interactions  different teaching models lead to
different notions of teaching dimension. In the past two decades  several such teaching models have
been proposed  primarily driven by the motivation to lower teaching complexity and to ﬁnd models
for which the teaching complexity has better connections with learning complexity measured by
Vapnik–Chervonenkis dimension (VCD) [VC71] of the class.
Most of the well-studied teaching models are for the batch setting (e.g.  worst-case [GK95  Kuh99] 
recursive [ZLHZ08  ZLHZ11  DFSZ14]  preference-based [GRSZ17]  and non-clashing [KSZ19]
models). In these batch models  the teacher ﬁrst provides a set of examples to the learner and then
the learner outputs a hypothesis. In a quest to achieve more natural teacher-learner interactions and
enable richer applications  various different models have been proposed for the sequential setting
(e.g.  local preference-based model for version space learners [CSMA`18]  models for gradient
learners [LDH`17  LDL`18  KDCS19]  models inspired by control theory [Zhu18  LZZ19]  models

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

for sequential tasks [CL12  HTS18  TGH`19]  and models for human-centered applications that
require adaptivity [SBB`13  HCMA`19]).
In this paper  we seek to gain a deeper understanding of how different teaching models relate to each
other. To this end  we develop a novel teaching framework which captures the teaching process via
preference functions Σ. Here  a preference function σ P Σ models how a learner navigates in the
version space as it receives teaching examples (see §2 for formal deﬁnition); in turn  each function σ
induces a teacher-learner pair with teaching dimension TDpσq (see §3). We highlight some of the key
results below:

• We show that the well-studied teaching models in batch setting corresponds to speciﬁc
families of σ functions in our framework (see §4 and Table 1).
• We study the differences in the family of σ functions inducing the strongest batch
model [KSZ19] and functions inducing a weak sequential model [CSMA`18] (§5.2) (also 
see the relationship between Σgvs and Σlocal in Figure 1).
• We identify preference functions inducing a novel family of sequential models with teaching
complexity linear in the VCD of the hypothesis class. We provide a constructive procedure
to ﬁnd such σ functions with low teaching complexity (§5.3).

Our key ﬁndings are highlighted in Figure 1 and Ta-
ble 1. Here  Figure 1 illustrates the relationship between
different families of preference functions that we in-
troduce  and Table 1 summarizes the key complexity
results we obtain for different families. Our uniﬁed
view of the existing teaching models in turn opens up
several intriguing new directions such as (i) using our
constructive procedures to design preference functions
for addressing open questions of whether RTD/ NCTD
is linear in VCD  and (ii) understanding the notion of
collusion-free teaching in sequential models. We discuss
these directions further in §6.

Families

Reduction

Complexity Results

Σconst
TD
–

Σglobal

RTD / PBTD
OpVCD2q

Σlocal

Σlvs

Σglobal

Σconst

Σgvs

Figure 1: Venn diagram for different fami-
lies of preference functions.

Σgvs
NCTD
OpVCD2q
[KSZ19]

Σlocal

Local-PBTD
OpVCD2q
[CSMA`18]

Σlvs
–

OpVCDq

Table 1: Overview of our main results – reduction to existing models and teaching complexity.

[GK95]

[ZLHZ11  GRSZ17  HWLW17]

2 The Teaching Model
The teaching domain. Let X   Y be a ground set of unlabeled instances and the set of labels. Let H
be a ﬁnite class of hypotheses; each element h P H is a function h : X Ñ Y. Here  we only consider
boolean functions and hence Y “ t0  1u. In our model  X   H  and Y are known to both the teacher
and the learner. There is a target hypothesis h‹ P H that is known to the teacher  but not the learner.
Let Z Ď X ˆ Y be the ground set of labeled examples. Each element z “ pxz  yzq P Z represents
a labeled example where the label is given by the target hypothesis h‹  i.e.  yz “ h‹pxzq. For any
Z Ď Z  the version space induced by Z is the subset of hypotheses HpZq Ď H that are consistent
with the labels of all the examples  i.e.  HpZq :“ th P H | @z “ pxz  yzq P Z  hpxzq “ yzu.
Learner’s preference function. We consider a generic model of the learner that captures our
assumptions about how the learner adapts her hypothesis based on the labeled examples received from
the teacher. A key ingredient of this model is the learner’s preference function over the hypotheses.
The learner  based on the information encoded in the inputs of preference function—which include the
current hypothesis and the current version space—will choose one hypothesis in H. Our model of the
learner strictly generalizes the local preference-based model considered in [CSMA`18]  where the
learner’s preference was only encoded by her current hypothesis. Formally  we consider preference
functions of the form σ : H ˆ 2H ˆ H Ñ R. For any two hypotheses h1  h2  we say that the learner
prefers h1 to h2 based on the current hypothesis h and version space H Ď H  iff σph1; H  hq ă
σph2; H  hq. If σph1; H  hq “ σph2; H  hq  then the learner could pick either one of these two.

2

Interaction protocol and teaching objective. The teacher’s goal is to steer the learner towards
the target hypothesis h‹ by providing a sequence of labeled examples. The learner starts with an
initial hypothesis h0 P H before receiving any labeled examples from the teacher. At time step t 
the teacher selects a labeled example zt P Z  and the learner makes a transition from the current
hypothesis to the next hypothesis. Let us denote the labeled examples received by the learner up to
(and including) time step t via Zt. Further  we denote the learner’s version space at time step t as
Ht “ HpZtq  and the learner’s hypothesis before receiving zt as ht´1. The learner picks the next
hypothesis based on the current hypothesis ht´1  version space Ht  and preference function σ:

ht P arg min
h1PHt

σph1; Ht  ht´1q.

(2.1)

Upon updating the hypothesis ht  the learner sends ht as feedback to the teacher. Teaching ﬁnishes
here if the learner’s updated hypothesis ht equals h‹. We summarize the interaction in Protocol 1.1

Protocol 1 Interaction protocol between the teacher and the learner
1: learner’s initial version space is H0 “ H and learner starts from an initial hypothesis h0 P H
2: for t “ 1  2  3  . . . do
3:
4:
5:

learner receives zt “ pxt  ytq; updates Ht “ Ht´1 X Hptztuq; picks ht per Eq. (2.1);
teacher receives ht as feedback from the learner;
if ht “ h‹ then teaching process terminates

3 The Complexity of Teaching

3.1 Teaching Dimension for a Fixed Preference Function

1 

Our objective is to design teaching algorithms that can steer the learner towards the target hypothesis
in a minimal number of time steps. We study the worst-case number of steps needed  as is common
when measuring information complexity of teaching [GK95  ZLHZ11  GRSZ17  Zhu18]. Fix the
ground set of instances X and the learner’s preference σ. For any version space H Ď H  the
worst-case optimal cost for steering the learner from h to h‹ is characterized by

"
1 ` minz maxh2PCσpH h zq DσpH X Hptzuq  h2  h‹q  otherwise

DσpH  h  h‹q “
where CσpH  h  zq “ arg minh1PHXHptzuq σph1; H X Hptzuq  hq denotes the set of candidate hy-
potheses most preferred by the learner. Note that our deﬁnition of teaching dimension is similar in
spirit to the local preference-based teaching complexity deﬁned by [CSMA`18]. We shall see in the
next section  this complexity measure in fact reduces to existing notions of teaching complexity for
speciﬁc families of preference functions.
Given a preference function σ and the learner’s initial hypothesis h0  the teaching dimension w.r.t. σ
is deﬁned as the worst-case optimal cost for teaching any target h‹:
h‹ DσpH  h0  h‹q.

Dz  s.t. CσpH  h  zq “ th˚u

TDX  H h0pσq “ max

(3.1)

3.2 Teaching Dimension for a Family of Preference Functions

In this paper  we will investigate several families of preference functions (as illustrated in Figure 1).
For a family of preference functions Σ  we deﬁne the teaching dimension w.r.t the family Σ as the
teaching dimension w.r.t. the best σ in that family:
Σ-TDX  H h0 “ min
σPΣ

TDX  H h0pσq.

(3.2)

1It is important to note that in our teaching model  the teacher and the learner use the same preference
function. This assumption of shared knowledge of the preference function is also considered in existing teaching
models for both the batch settings (e.g.  as in [ZLHZ11  GRSZ17]) and the sequential settings [CSMA`18]).

3

3.3 Collusion-free Preference Functions

An important consideration when designing teaching models is to ensure that the teacher and the
learner are “collusion-free”  i.e.  they are not allowed to collude or use some “coding-trick” to
achieve arbitrarily low teaching complexity. A well-accepted notion of collusion-freeness in the
batch setting is one proposed by [GM96] (also see [AK97  OS99  KSZ19]). Intuitively  it captures
the idea that a learner conjecturing hypothesis h will not change its mind when given additional
information consistent with h. In comparison to batch models  the notion of collusion-free teaching
in the sequential models is not well understood. We introduce a novel notion of collusion-freeness
for the sequential setting  which captures the following idea: if h is the only hypothesis in the most
preferred set deﬁned by σ  then the learner will always stay at h as long as additional information
received by the learner is consistent with h. We formalize this notion in the deﬁnition below. Note
that for σ functions corresponding to batch models (see §4)  Deﬁnition 1 reduces to the collusion-free
deﬁnition of [GM96].

Deﬁnition 1 (Collusion-free preference) Consider a time t where the learner’s current hypothesis
is ht´1 and version space is Ht (see Protocol 1). Further assume that the learner’s preferred
hypothesis for time t is uniquely given by arg minh1PHt σph1; Ht  ht´1q “ tˆhu. Let S be additional
examples provided by an adversary from time t onwards. We call a preference function collusion-free 
if for any S consistent with ˆh  it holds that arg minh1PHtXHpSq σph1; Ht X HpSq  ˆhq “ tˆhu.
In this paper  we study preference functions that are collusion-free. In particular  we use ΣCF to
denote the set of preference functions that induce collusion-free teaching:

ΣCF “ tσ | σ is collusion-freeu.

4 Preference-based Batch Models
4.1 Families of Preference Functions

We consider three families of preference functions which do not depend
on the learner’s current hypothesis. The ﬁrst one is the family of uniform
preference functions  denoted by Σconst  which corresponds to constant
preference functions:

Σconst “ tσ P ΣCF | Dc P R  s.t. @h1  H  h  σph1; H  hq “ cu

The second family  denoted by Σglobal  corresponds to the preference
functions that do not depend on the learner’s current hypothesis and
version space. In other words  the preference functions capture some
global preference ordering of the hypotheses:

Σglobal

Σconst

Σgvs

Figure 2: Batch models.

Σglobal “ tσ P ΣCF | D g : H Ñ R  s.t. @h1  H  h  σph1; H  hq “ gph1qu

The third family  denoted by Σgvs  corresponds to the preference functions that depend on the learner’s
version space  but do not depend on the learner’s current hypothesis:

Σgvs “ tσ P ΣCF | D g : H ˆ 2H Ñ R  s.t. @h1  H  h  σph1; H  hq “ gph1  Hqu

Figure 2 illustrates the relationship between these preference families.

4.2 Complexity Results

We ﬁrst provide several deﬁnitions  including the formal deﬁnition of VC dimension as well as several
existing notions of teaching dimension.
Deﬁnition 2 (Vapnik–Chervonenkis dimension [VC71]) The VC dimension for H Ď H w.r.t. a
ﬁxed set of unlabeled instances X Ď X   denoted by VCDpH  Xq  is the cardinality of the largest set
of points X1 Ď X that are “shattered”.2 Formally  let H|X “ tphpx1q  ...  hpxnqq | @h P Hu denote
all possible patterns of H on X. Then VCDpH  Xq “ max|X1|  s.t. X1 Ď X and |H|X1| “ 2|X1|.
2In the classical deﬁnition of VCD  only the ﬁrst argument H is present; the second argument X is omitted
and is by default the ground set of unlabeled instances X .

4

Deﬁnition 3 (Teaching dimension [GK95]) For any hypothesis h P H  we call a set of instances
Tphq Ď X a teaching set for h  if it can uniquely identify h P H. The teaching dimension for H 
denoted by TDpHq  is the maximum size of the minimum teaching set for any h P H: TDpHq “
maxhPH min|Tphq|.
As noted by [ZLHZ08]  the teaching dimension of [GK95] does not always capture the intuitive idea
of cooperation between teacher and learner. The authors then introduced a model of cooperative
teaching that resulted in the complexity notion of recursive teaching dimension  as deﬁned below.
Deﬁnition 4 (Recursive teaching dimension [ZLHZ08  ZLHZ11]) The recursive teaching dimen-
sion (RTD) of H  denoted by RTDpHq  is the smallest number k  such that one can ﬁnd an ordered
sequence of hypotheses in H  denoted by ph1  . . .   hi  . . .   h|H|q  where every hypothesis hi has a
teaching set of size no more than k to be distinguished from the hypotheses in the remaining sequence.

In this paper we consider ﬁnite hypothesis classes. Under this setting  RTD is equivalent to preference-
based teaching dimension (PBTD) [GRSZ17].
In a recent work of [KSZ19]  a new notion of teaching complexity  called non-clashing teaching
dimension or NCTD  was introduced (see deﬁnition below). Importantly  NCTD is the optimal
teaching complexity among teaching models in the batch setting that satisfy the collusion-free
property of [GM96].
Deﬁnition 5 (Non-clashing teaching dimension [KSZ19]) Let H be a hypothesis class and T :
H Ñ 2X be a “teacher mapping” on H  i.e.  mapping a given hypothesis to a teaching set.3 We say
that T is non-clashing on H iff there are no two distinct h  h1 P H such that Tphq is consistent with h1
and Tph1q is consistent with h. The non-clashing Teaching Dimension of H  denoted by NCTDpHq 
is deﬁned as NCTDpHq “ minT is non-clashingtmaxhPH |Tphq|u.
We show in the following  that the teaching dimension Σ-TD in Eq. (3.2) uniﬁes the above deﬁnitions
of TD’s for batch models.
Theorem 1 (Reduction to existing notions of TD’s) Fix X  H  h0. The teaching complexity for the
three families reduces to the existing notions of teaching dimensions:

1. Σconst-TDX  H h0 “ TDpHq
2. Σglobal-TDX  H h0 “ RTDpHq “ OpVCDpH Xq2q
3. Σgvs-TDX  H h0 “ NCTDpHq “ OpVCDpH Xq2q

Our teaching model strictly generalizes the local-preference based model of [CSMA`18]  which
reduces to the “worst-case” model when σ P Σconst (corresponding to TD) [GK95] and the global
“preference-based” model when σ P Σglobal. Hence we get Σconst-TDX  H h0 “ TDpHq and
Σglobal-TDX  H h0 “ RTDpHq. To establish the equivalence between Σgvs-TDX  H h0 and NCTDpHq 
it sufﬁces to show that for any X  H  h0  the following holds: (i) Σgvs-TDX  H h0 ě NCTDpHq  and
(ii) Σgvs-TDX  H h0 ď NCTDpHq. The full proof is provided in Appendix A.2 of the supplementary.
In Table 2  we consider the well known Warmuth hypothesis class [DFSZ14] where Σconst-TD “ 3 
Σglobal-TD “ 3  and Σgvs-TD “ 2. Table 2b and Table 2d show preference functions σ P Σconst 
σ P Σglobal  and σ P Σgvs that achieve the minima in Eq. (3.2). Table 2a shows the teaching sequences
achieving these teaching dimensions for these preference functions. In Appendix A.1  we provide
another hypothesis class where Σconst-TD “ 3  Σglobal-TD “ 2  and Σgvs-TD “ 1.
5 Preference-based Sequential Models
5.1 Families of Preference Functions

In this section  we investigate two families of preference functions that depend on the learner’s
current hypothesis ht´1. The ﬁrst one is the family of local preference-based functions [CSMA`18] 
denoted by Σlocal  which corresponds to preference functions that depend on the learner’s current
(local) hypothesis  but do not depend on the learner’s version space:

Σlocal “ tσ P ΣCF | D g : H ˆ H Ñ R  s.t. @h1  H  h  σph1; H  hq “ gph1  hqu

3We refer the reader to the original paper [KSZ19] for a more formal description of “teacher mapping".

5

HHHHH

x

h
h1
h2
h3
h4
h5
h6
h7
h8
h9
h10

x1
1
0
0
0
1
1
0
1
0
1

x2
1
1
0
0
0
1
1
0
1
0

x3
0
1
1
0
0
0
1
1
0
1

x4
0
0
1
1
0
1
0
1
1
0

x5
0
0
0
1
1
0
1
0
1
1

Sconst “ Sglobal
px1  x2  x4q
px2  x3  x5q
px1  x3  x4q
px2  x4  x5q
px1  x3  x5q
px1  x2  x4q
px2  x3  x5q
px1  x3  x4q
px2  x4  x5q
px1  x3  x5q

Sgvs
px1  x2q
px2  x3q
px3  x4q
px4  x5q
px1  x5q
px2  x4q
px3  x5q
px1  x4q
px2  x5q
px1  x3q

Slocal
px1q
px3q
px3  x4q
px5  x4q
px5q
px4q
px3  x5q
px4  x3q
px4  x5q
px5  x3q

Slvs
px1q
px2q
px3q
px4q
px5q
px3q
px4q
px5q
px1q
px2q

(a) The Warmuth hypothesis class and the corresponding teaching sequences (denoted by S).

h1

σconstph1;¨ ¨q
σglobalph1;¨ ¨q

@h1 P H

0

(b) σconst and σglobal
h1
H

h2
h1
th2  h7u
th1  h6u
th2u
th1u
0
0
(d) σgvsph1; H ¨q

σgvs

. . .
. . .
. . .
. . .

hzh1

σlocalph1;¨  h “ h1q

. . .

h1 h2 h3 h4 h5 h6 h7 h8 h9 h10
0

4

4

2

3

3

1

3

3

2

(c) σlocal representing the Hamming distance between h1 and h.

h1
H

h1
th1uY

th5  h6  h8  h10u˚

h
σlvs

h1
0

h2
th2uY

th1  h7  h6  h9u˚
h1
0

h2
0

. . .
. . .
. . .
. . .
. . .

(e) σlvsph1; H  hq. Here  t¨u˚ denotes all subsets.

Table 2: Teaching sequences with different preference functions for the Warmuth hypothesis class
[DFSZ14].4 Full preference functions are given in Appendix B of the supplementary.

The second family  denoted by Σlvs  corresponds to the preference functions that depend on all three
arguments of σph1; H  hq. The dependence of σ on the learner’s current (local) hypothesis and the
version space renders a powerful family of preference functions:

Σlvs “ tσ P ΣCF | D g : H ˆ 2H ˆ H Ñ R  s.t. @h1  H  h  σph1; H  hq “ gph1  H  hqu

Figure 1 illustrates the relationship between these preference families. As an example  in Table 2c
and Table 2e  we provide the preference functions σlocal and σlvs for the Warmuth hypothesis class
that achieve the minima in Eq. (3.2).

5.2 Comparing Σgvs-TD and Σlocal-TD

In the following  we show that substantial differences arise as we transition from σ functions
inducing the strongest batch (i.e.  non-clashing) model to σ functions inducing a weak sequential
(i.e.  local preference-based) model. We provide the full proof of Theorem 2 in Appendix C of the
supplementary.

Theorem 2 Neither of the families Σgvs and Σlocal dominates the other. Speciﬁcally 

1. Σgvs X Σlocal “ Σglobal
2. There exist H  X   where @h0 P H  Σlocal-TDX  H h0 ą Σgvs-TDX  H h0
3. There exist H  X   where @h0 P H  Σlocal-TDX  H h0 ă Σgvs-TDX  H h0

5.3 Complexity Results

We now connect the teaching complexity of the sequential models with the VC dimension.
Theorem 3 Σlocal-TDX  H h0 “ OpVCDpH Xq2q  and Σlvs-TDX  H h0 “ OpVCDpH Xqq.
To establish the proof  we ﬁrst introduce an important deﬁnition (Deﬁnition 6) and a key lemma
(Lemma 4).

4The Warmuth hypothesis class is the smallest concept class for which RTD exceeds VCD.

6

Deﬁnition 6 (Compact-Distinguishable Set) Fix H Ď H and X Ď X   where X “ tx1  ...  xnu.
Let H|X “ tphpx1q  ...  hpxnqq | @h P Hu denote all possible patterns of H on X. Then  we say that
X is compact-distinguishable on H  if |H|X| “ |H| and @X1 Ă X  |H|X1| ă |H|. We will use ΨH
to denote a compact-distinguishable set on H.

In words  one can uniquely identify any hypothesis in H with a (sub)set of examples from ΨH (also
see the deﬁnition of distinguishing sets in [DFSZ14]). Our deﬁnition of compact-distinguishable
set further implies that there are no “redundant” examples in ΨH. It can be shown that a compact-
distinguishable set satisﬁes the following two properties: (i) it does not contain any pair of distinct
instances x  x1 such that p@h P H : hpxq “ hpx1qq or p@h P H : hpxq ‰ hpx1qq; and (ii) it does not
contain any instance x such that p@h P H : hpxq “ 1q or p@h P H : hpxq “ 0q.
Lemma 4 Consider a subset H Ď H and any compact-distinguishable set ΨH “ tx1  ...  x|ΨH|u.
Fix any hypothesis hH P H. Let d “ VCDpH  ΨHq denote the VC dimension of H on ΨH. If d ě 1 
we can divide H into m “ |ΨH| ` 1 separate hypothesis classes tH 1  ...  H mu  such that
(i) @j P rms  there exists a compact-distinguishable set ΨH j s.t. VCDpH j  ΨH jq ď d ´ 1.
(ii) @j P rm ´ 1s  H j is not empty and H j|txju “ tp1 ´ hHpxjqqu.
(iii) H m “ thHu.
Lemma 4 suggests that for any H X   one can partition the hypothesis class H into m ď |X| ` 1
subsets with lower VC dimension with respect to some compact-distinguishable set.5 The main idea
of the lemma is similar to the reduction of a concept class w.r.t. some instance x to lower VCD as done
in Theorem 9 of [FW95]. The key distinction of Lemma 4 is that we consider compact-distinguishable
sets for this partitioning  which in turn ensures the uniqueness of the version spaces associated with
these partitions (see proof of Theorem 3). Another key novelty in our proof of Theorem 3 is to
recursively apply the reduction step from the lemma.
To prove the lemma  we provide a constructive procedure to partition the hypothesis class  and show
that the resulting partitions have reduced VC dimensions on some compact-distinguishable set. We
highlight the procedure for constructing the partitions in Algorithm 2 (Line 7– Line 10). In Figure 3 
we provide an illustrative example for creating such partitions for the Warmuth hypothesis class from
Table 2a. We sketch the proof of Lemma 4 below  and defer the detailed proof to Appendix D.1.
Proof [Proof Sketch of Lemma 4] Let us deﬁne Hx “ th P H : h(cid:52)x|ΨH P H|ΨHu. Here  h(cid:52)x
denotes the hypothesis that only differs with h on the label of x  and h|ΨH denotes the patterns of
h on ΨH. Fix a reference hypothesis hH. For all j P rm ´ 1s  let yj “ 1 ´ hHpxjq be the opposite
label of xj P ΨH as provided by hH. As shown in Line 9 of Algorithm 2  we consider the set
x1 “ th P Hx1 : hpx1q “ y1u as the ﬁrst partition. In the appendix  we show that |H 1| ą 0.
H 1 :“ H y1
Next  we show that VCDpH 1  ΨHztx1uq ď d ´ 1. When d ą 1  we prove the statement as follows:
VCDpH 1  ΨHztx1uq ď VCDpH y1
  ΨHq “ VCDpHx1   ΨHq ´ 1 ď VCDpH  ΨHq ´ 1 ď d ´ 1
In the appendix  we prove the statement for d “ 1  and further show that there exists a compact-
distinguishable set ΨH 1 Ď ΨHztx1u for the ﬁrst partition H 1. Then  we conclude that the ﬁrst
partition H 1 has VCDpH 1  ΨH 1q ď d ´ 1.
Next  we remove the ﬁrst partition H 1 from H  and continue to create the above mentioned partitions
on Hrest “ HzH 1 and Xrest “ ΨHztx1u. As discussed in the appendix  we show that Xrest is a
compact-distinguishable set on Hrest. Therefore  we can repeat the above procedure (Line 7– Line 10 
Algorithm 2) to create the subsequent partitions. This process continues until the size of Xrest reduces
to 1  i.e. Xrest “ txm´1u. Until then  we obtain partitions tH 1  ...  H m´2u. By construction  H j
satisfy properties (i) and (ii) for all j P rm ´ 2s.
It remains to show that H m´1 and H m also satisfy the properties in Lemma 4. Since Xrest “
txm´1u before we start iteration m ´ 1  and Xrest is a compact-distinguishable set for Hrest  there
must exist exactly two hypotheses in Hrest  and therefore |H m´1| |H m| “ 1. This implies that
VCDpH m´1  ΨHm´1q “ VCDpH m  ΨHmq “ 0. Furthermore  @j P rm ´ 1s and h P H j  we have
hHpxjq ‰ hpxjq. This indicates hH P Hm  and hence Hm “ thHu which completes the proof.

x1

5When VCDpH  ΨHq “ 0  this implies |H| “ 1.

7

h1
px1  0q

H 0
x1

h3

0 0 1 1 0

px2  0q

H 6

1 1 0 0 0

px5  1q

px4  1q

H 1
x5

h5

1 0 0 0 1

if h1 “ h
o.w.

σlvsph1; H  hq Ð

Algorithm 2 Recursive procedure for constructing σlvs
achieving TDX  H h0pσlvsq ď VCDpH Xq
1: Let I : H Ñ t1  . . .  |H|u be any bijective mapping
2: For all h1 P H  H Ď H  h P H  initialize

Input: X   H  h0

"
0
|H| ` 1
3: SETPREFERENCEpH H X   h0q
4: function SETPREFERENCE(V  H  X  h)
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

Create compact-distinguishable set ΨH Ď X
Hrest :“ H  Xrest :“ ΨH
for x P ΨH do
y “ 1 ´ hpxq
x Ð th1 P Hrest : h1(cid:52)x|Xrest P Hrest|Xrest   h1pxq “ yu
H y
Hrest Ð HrestzH y
Vnext Ð V X Hptpx  yquq
for h1 P H y
hnext Ð arg minh1PH y
Iph1q
SETPREFERENCEpVnext  H y

x do σlvsph1; Vnext  hq Ð Iph1q
x   ΨHztxu  hnextq

x  Xrest Ð Xrestztxu

x

H 0
x2
0 0 0 1 1
1 0 1 0 1

h4
h10

px3  1q

H 1
x4

h6
h9

1 1 0 1 0
0 1 0 1 1

H 1
x3

h2
h8
h7

0 1 1 0 0
1 0 1 1 0
0 1 1 0 1

Figure 3: Illustration of Lemma 4 on the
Warmuth class. The grouped hypotheses
in the leaf clusters correspond to the sets
H y

x created in Line 9 of Algorithm 2.

px1  0q

h1

px2  0q

1 1 0 0 0

px3  1q

px4  1q

px5  1q

h3

0 0 1 1 0

0 0 0 1 1

h4
px3  1q

h2
px4  1q

0 1 1 0 0

h6

px5  1q

1 1 0 1 0

px5  1q

h5

1 0 0 0 1

h10

1 0 1 0 1

h8

1 0 1 1 0

h7

0 1 1 0 1

h9

0 1 0 1 1

Figure 4: Illustration of Theorem 3 proof – constructing a σlvs P Σlvs for the Warmuth class.

Recursive construction of σlvs. As a part of the Theorem 3 proof  we provide a recursive procedure
for constructing a σlvs P Σlvs achieving TDX  H h0pσlvsq “ O pVCDpH Xqq.
Proof [Proof of Theorem 3] In a nutshell  the proof consists of three steps: (i) initialization of σlvs 
(ii) setting the preferences by recursively invoking the constructive procedure for Lemma 4  and (iii)
showing that there exists a teaching sequence of length up to d for any target hypothesis h‹. We
summarize the recursive procedure in Algorithm 2.
Step (i). To begin with  we initialize σlvs with default values which induce high σ values (i.e. 
low preference)  except for σph1; H  hq “ 0 where h1 “ h (c.f. Line 2 of Algorithm 2). The
self-preference guarantees that σlvs is collusion-free as per Deﬁnition 1.
Step (ii). The recursion begins at the top level with H “ H  current version space V “ H  and
initial hypothesis h “ h0. Lemma 4 suggests that we can partition H into m “ |ΨH| ` 1 groups
tH 1  ...  H mu  where for all j P rms  there exists a compact-distinguishable set ΨH j that satisﬁes
the properties in Lemma 4.
Now consider the hypothesis h :“ h0. We show that for j P rm ´ 1s  every pxj  yjq  where xj P ΨH
and yj “ 1 ´ hpxjq  corresponds to a unique version space V j :“ th P V : hpxjq “ yju. To
prove this statement  we consider Rj :“ V j X H “ th P H : hpxjq “ yju. As is discussed in
Appendix D.2 of the supplementary  we know that none of Rj for j P rm ´ 1s are equal. This
indicates that none of V j for j P rm ´ 1s are equal.
We then set the values of the preference function σlvsp¨; V j  hq for all j P rm´ 1s and yj “ 1´ hpxjq
(Line 12). Upon receiving pxj  yjq  the learner will be steered to the next “search space” H j  with
version space V j. By Lemma 4 we have VCDpH j  ΨH jq ď VCDpH  ΨHq ´ 1.
We will build the preference function σlvs recursively m ´ 1 times for each pV j  H j  ΨH j   hnextq 
where hnext corresponds to the unique hypothesis identiﬁed by function I (Line 13–Line 14). At

8

each level of recursion  VCD reduces by 1. We stop the recursion when VCDpH j; ΨH jq “ 0  which
corresponds to the scenario |H j| “ 1.
Step (iii). Given the preference function constructed in Algorithm 2  we can build up the set of
(labeled) teaching examples recursively. Consider the beginning of the teaching process  where the
learner’s current hypothesis is h0 and version space is H  and the goal of the teacher is to teach h‹.
Consider the ﬁrst level of the recursion in Algorithm 2  where we divide H into m “ |ΨH|` 1 groups
tH 1  ...  H mu. Let us consider the case where h‹ P H j‹
with j‹ P rm ´ 1s. The teacher provides
an example given by px “ xj‹   y “ h‹pxj‹qq. After receiving the teaching example  the resulting
partition H j‹
will stay in the version space; meanwhile  h0 will be removed from the version space.
The new version space will be V j‹
. The learner’s new hypothesis induced by the preference function
is given by hnext P H j‹
. By repeating this teaching process for a maximum of d steps  the learner
reaches a partition of size 1 (see Step (ii) for details). At this step h‹ must be the only hypothesis left
in the search space. Therefore  hnext “ h‹  and the learner has reached h‹.
Figure 4 illustrates the recursive construction of a σlvs P Σlvs for the Warmuth class  with
TDX  H h0pσlvsq “ 2.
6 Discussion and Conclusion

We now discuss a few thoughts related to different families of preference functions. First of all  the
size of the families grows exponentially as we change our model from Σconst  Σglobal to Σgvs/Σlocal
and ﬁnally to Σlvs  thus resulting in more powerful models with lower teaching complexity. While
run time has not been the focus of this paper  it would be interesting to characterize the presumably
increased run time complexity of sequential learners and teachers with complex preference functions.
Furthermore  as the size of the families grow  the problem of ﬁnding the best preference function σ in
a given family Σ that achieve the minima in Eq. (3.2) becomes more computationally challenging.
The recursive procedure in Algorithm 2 creates a preference function σlvs P Σlvs that has teaching
complexity at most VCD. It is interesting to note that the resulting preference function σlvs has the
characteristic of “win-stay  loose shift" [BDGG14  CSMA`18]: Given that for any hypothesis we
have σph;¨  hq “ 0  the learner prefers her current hypothesis as long as it remains consistent. Prefer-
ence functions with this characteristic naturally exhibit the collusion-free property in Deﬁnition 1.
For some problems  one can achieve lower teaching complexity for a σ P Σlvs. In fact  the preference
function σlvs we provided for the Warmuth class in Table 2e has teaching complexity 1  while the
preference function constructed in Figure 4 has teaching complexity 2.
One fundamental aspect of modeling teacher-learner interactions is the notion of collusion-free
teaching. Collusion-freeness for the batched setting is well established in the research community
and NCTD characterizes the complexity of the strongest collusion-free batch model. In this paper 
we are introducing a new notion of collusion-freeness for the sequential setting (Deﬁnition 1). As
discussed above  a stricter condition is the “win-stay lose-shift” model  which is easier to validate
without running the teaching algorithm. In contrast  the condition of Deﬁnition 1 is more involved
in terms of validation and is a joint property of the teacher-learner pair. One intriguing question for
future work is deﬁning notions of collusion-free teaching in sequential models and understanding
their implications on teaching complexity.
Another interesting direction of future work is to better understand the properties of the teaching
parameter Σ-TD. One question of particular interest is showing that the teaching parameter is not
upper bounded by any constant independent of the hypothesis class  which would suggest a strong
collusion in our model. We can show that for certain hypothesis classes  Σ-TD is lower bounded by a
function of VCD. In particular  for the power set class of size d (which has VCD “ d)  Σ-TD is lower
bounded by Ω
. Another direction of future work is to understand whether this parameter is
additive or subadditive over disjoint domains. Also  we consider a generalization of our results to the
inﬁnite VC classes as a very interesting direction for future work.
Our framework provides novel tools for reasoning about teaching complexity by constructing prefer-
ence functions. This opens up an interesting direction of research to tackle important open problems 
such as proving whether NCTD or RTD is linear in VCD [SZ15  CCT16  HWLW17  KSZ19]. In this
paper  we showed that neither of the families Σgvs and Σlocal dominates the other (Theorem 2). As a
direction for future work  it would be important to further quantify the complexity of Σlocal family.

´

¯

d

log d

9

Acknowledgements

This work was done in part when Yuxin Chen was at Caltech. Xiaojin Zhu is supported by NSF
1545481  1561512  1623605  1704117  1836978 and the MADLab AF CoE FA9550-18-1-0166.

References

[AK97] Dana Angluin and M¯artin  š Krik  is. Teachers  learners and black boxes. In Proceedings
of the tenth annual conference on Computational learning theory  pages 285–297.
ACM  1997.

[BDGG14] Elizabeth Bonawitz  Stephanie Denison  Alison Gopnik  and Thomas L Grifﬁths.
Win-stay  lose-sample: A simple sequential algorithm for approximating bayesian
inference. Cognitive psychology  74:35–65  2014.

[CCT16] Xi Chen  Yu Cheng  and Bo Tang. On the recursive teaching dimension of vc classes.

In Advances in Neural Information Processing Systems  pages 2164–2171  2016.

[CL12] Maya Cakmak and Manuel Lopes. Algorithmic and human teaching of sequential

decision tasks. In AAAI  2012.

[CSMA`18] Yuxin Chen  Adish Singla  Oisin Mac Aodha  Pietro Perona  and Yisong Yue. Under-
standing the role of adaptivity in machine teaching: The case of version space learners.
In Advances in Neural Information Processing Systems  pages 1476–1486  2018.

[DFSZ14] Thorsten Doliwa  Gaojian Fan  Hans Ulrich Simon  and Sandra Zilles. Recursive
teaching dimension  vc-dimension and sample compression. JMLR  15(1):3107–3131 
2014.

[FW95] Sally Floyd and Manfred Warmuth. Sample compression  learnability  and the vapnik-

chervonenkis dimension. Machine learning  21(3):269–304  1995.

[GK95] Sally A Goldman and Michael J Kearns. On the complexity of teaching. Journal of

Computer and System Sciences  50(1):20–31  1995.

[GM96] Sally A Goldman and H David Mathias. Teaching a smarter learner. Journal of

Computer and System Sciences  52(2):255–267  1996.

[GRSZ17] Ziyuan Gao  Christoph Ries  Hans U Simon  and Sandra Zilles. Preference-based

teaching. JMLR  18(31):1–32  2017.

[HCMA`19] Anette Hunziker  Yuxin Chen  Oisin Mac Aodha  Manuel Gomez Rodriguez  Andreas
Krause  Pietro Perona  Yisong Yue  and Adish Singla. Teaching multiple concepts to a
forgetful learner. In Advances in Neural Information Processing Systems  2019.

[HTS18] Luis Haug  Sebastian Tschiatschek  and Adish Singla. Teaching inverse reinforce-
ment learners via features and demonstrations. In Advances in Neural Information
Processing Systems  pages 8464–8473  2018.

[HWLW17] Lunjia Hu  Ruihan Wu  Tianhong Li  and Liwei Wang. Quadratic upper bound
for recursive teaching dimension of ﬁnite VC classes. In Proceedings of the 30th
Conference on Learning Theory  COLT  pages 1147–1156  2017.

[KDCS19] Parameswaran Kamalaruban  Rati Devidze  Volkan Cevher  and Adish Singla. In-
teractive teaching algorithms for inverse reinforcement learning. In IJCAI  pages
2692–2700  2019.

[KSZ19] David Kirkpatrick  Hans U. Simon  and Sandra Zilles. Optimal collusion-free teaching.
In Proceedings of the 30th International Conference on Algorithmic Learning Theory 
volume 98  pages 506–528  2019.

[Kuh99] Christian Kuhlmann. On teaching and learning intersection-closed concept classes. In
European Conference on Computational Learning Theory  pages 168–182. Springer 
1999.

10

[LDH`17] Weiyang Liu  Bo Dai  Ahmad Humayun  Charlene Tay  Chen Yu  Linda B. Smith 
James M. Rehg  and Le Song. Iterative machine teaching. In ICML  pages 2149–2158 
2017.

[LDL`18] Weiyang Liu  Bo Dai  Xingguo Li  Zhen Liu  James M. Rehg  and Le Song. Towards

black-box iterative machine teaching. In ICML  pages 3147–3155  2018.

[LZZ19] Laurent Lessard  Xuezhou Zhang  and Xiaojin Zhu. An optimal control approach to

sequential machine teaching. In AISTATS  pages 2495–2503  2019.

[OS99] Matthias Ott and Frank Stephan. Avoiding coding tricks by hyperrobust learning. In
European Conference on Computational Learning Theory  pages 183–197. Springer 
1999.

[SBB`13] Adish Singla  Ilija Bogunovic  G Bartók  A Karbasi  and A Krause. On actively
teaching the crowd to classify. In NIPS Workshop on Data Driven Education  2013.
[SBB`14] Adish Singla  Ilija Bogunovic  Gábor Bartók  Amin Karbasi  and Andreas Krause.

Near-optimally teaching the crowd to classify. In ICML  pages 154–162  2014.

[SZ15] Hans U Simon and Sandra Zilles. Open problem: Recursive teaching dimension versus

vc dimension. In Conference on Learning Theory  pages 1770–1772  2015.

[TGH`19] Sebastian Tschiatschek  Ahana Ghosh  Luis Haug  Rati Devidze  and Adish Singla.
Learner-aware teaching: Inverse reinforcement learning with preferences and con-
straints. In Advances in Neural Information Processing Systems  2019.

[VC71] VN Vapnik and A Ya Chervonenkis. On the uniform convergence of relative fre-
quencies of events to their probabilities. Theory of Probability and its Applications 
16(2):264  1971.

[Zhu13] Xiaojin Zhu. Machine teaching for bayesian learners in the exponential family. In

Advances in Neural Information Processing Systems  pages 1905–1913  2013.

[Zhu15] Xiaojin Zhu. Machine teaching: An inverse problem to machine learning and an

approach toward optimal education. In AAAI  pages 4083–4087  2015.

[Zhu18] Xiaojin Zhu. An optimal control view of adversarial machine learning. arXiv preprint

arXiv:1811.04422  2018.

[ZLHZ08] Sandra Zilles  Steffen Lange  Robert Holte  and Martin Zinkevich. Teaching dimen-

sions based on cooperative learning. In COLT  pages 135–146  2008.

[ZLHZ11] Sandra Zilles  Steffen Lange  Robert Holte  and Martin Zinkevich. Models of coopera-

tive teaching and learning. JMLR  12(Feb):349–384  2011.

[ZSZR18] Xiaojin Zhu  Adish Singla  Sandra Zilles  and Anna N. Rafferty. An overview of

machine teaching. CoRR  abs/1801.05927  2018.

11

,Farnam Mansouri
Yuxin Chen
Ara Vartanian
Jerry Zhu
Adish Singla