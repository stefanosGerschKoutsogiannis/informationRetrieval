2018,SLANG: Fast Structured Covariance Approximations for Bayesian Deep Learning with Natural Gradient,Uncertainty estimation in large deep-learning models is a computationally challenging
task  where it is difficult to form even a Gaussian approximation to the
posterior distribution. In such situations  existing methods usually resort to a diagonal
approximation of the covariance matrix despite the fact that these matrices
are known to give poor uncertainty estimates. To address this issue  we propose
a new stochastic  low-rank  approximate natural-gradient (SLANG) method for
variational inference in large deep models. Our method estimates a “diagonal
plus low-rank” structure based solely on back-propagated gradients of the network
log-likelihood. This requires strictly less gradient computations than methods that
compute the gradient of the whole variational objective. Empirical evaluations
on standard benchmarks confirm that SLANG enables faster and more accurate
estimation of uncertainty than mean-field methods  and performs comparably to
state-of-the-art methods.,SLANG: Fast Structured Covariance Approximations
for Bayesian Deep Learning with Natural Gradient

University of British Columbia

Ecole Polytechnique Fédérale de Lausanne

Aaron Mishkin∗

Vancouver  Canada

amishkin@cs.ubc.ca

Frederik Kunstner∗

Lausanne  Switzerland

frederik.kunstner@epfl.ch

Didrik Nielsen

RIKEN Center for AI Project

Tokyo  Japan

didrik.nielsen@riken.jp

Mark Schmidt

University of British Columbia

Vancouver  Canada

schmidtm@cs.ubc.ca

Mohammad Emtiyaz Khan
RIKEN Center for AI Project

Tokyo  Japan

emtiyaz.khan@riken.jp

Abstract

Uncertainty estimation in large deep-learning models is a computationally chal-
lenging task  where it is difﬁcult to form even a Gaussian approximation to the
posterior distribution. In such situations  existing methods usually resort to a diag-
onal approximation of the covariance matrix despite the fact that these matrices are
known to result in poor uncertainty estimates. To address this issue  we propose
a new stochastic  low-rank  approximate natural-gradient (SLANG) method for
variational inference in large  deep models. Our method estimates a “diagonal
plus low-rank” structure based solely on back-propagated gradients of the network
log-likelihood. This requires strictly less gradient computations than methods that
compute the gradient of the whole variational objective. Empirical evaluations
on standard benchmarks conﬁrm that SLANG enables faster and more accurate
estimation of uncertainty than mean-ﬁeld methods  and performs comparably to
state-of-the-art methods.

1

Introduction

Deep learning has had enormous recent success in ﬁelds such as speech recognition and computer
vision. In these problems  our goal is to predict well and we are typically less interested in the
uncertainty behind the predictions. However  deep learning is now becoming increasingly popular
in applications such as robotics and medical diagnostics  where accurate measures of uncertainty
are crucial for reliable decisions. For example  uncertainty estimates are important for physicians
who use automated diagnosis systems to choose effective and safe treatment options. Lack of such
estimates may lead to decisions that have disastrous consequences.
The goal of Bayesian deep learning is to provide uncertainty estimates by integrating over the
posterior distribution of the parameters. Unfortunately  the complexity of deep learning models makes
∗Equal contributions. This work was conducted during an internship at the RIKEN Center for AI project.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: This ﬁgure illustrates the advantages of SLANG method over mean-ﬁeld approaches on
the USPS dataset (see Section 4.1 for experimental details). The ﬁgure on the left compares our
structured covariance approximation with the one obtained by a full Gaussian approximation. For
clarity  only off-diagonal entries are shown. We clearly see that our approximation becomes more
accurate as the rank is increased. The ﬁgures on the right compare the means and variances (the
diagonal of the covariance). The means match closely for all methods  but the variance is heavily
underestimated by the mean-ﬁeld method. SLANG’s covariance approximations do not suffer form
this problem  which is likely due to the off-diagonal structure it learns.

it infeasible to perform the integration exactly. Sampling methods such as stochastic-gradient Markov
chain Monte Carlo [9] have been applied to deep models  but they usually converge slowly. They
also require a large memory to store the samples and often need large preconditioners to mix well
[2  5  32]. In contrast  variational inference (VI) methods require much less memory and can scale
to large problems by exploiting stochastic gradient methods [7  12  28]. However  they often make
crude simpliﬁcations  like the mean-ﬁeld approximation  to reduce the memory and computation cost.
This can result in poor uncertainty estimates [36]. Fast and accurate estimation of uncertainty for
large models remains a challenging problem in Bayesian deep learning.
In this paper  we propose a new variational inference method to estimate Gaussian approximations
with a diagonal plus low-rank covariance structure. This gives more accurate and ﬂexible approxima-
tions than the mean-ﬁeld approach. Our method also enables fast estimation by using an approximate
natural-gradient algorithm that builds the covariance estimate solely based on the back-propagated
gradients of the network log-likelihood. We call our method stochastic low-rank approximate natural-
gradient (SLANG). SLANG requires strictly less gradient computations than methods that require
gradients of the variational objective obtained using the reparameterization trick [24  26  35]. Our
empirical comparisons demonstrate the improvements obtained over mean-ﬁeld methods (see Figure
1 for an example) and show that SLANG gives comparable results to the state-of-the-art on standard
benchmarks.
The code to reproduce the experimental results in this paper is available at https://github.com/
aaronpmishkin/SLANG.

1.1 Related Work

Gaussian variational distributions with full covariance matrices have been used extensively for
shallow models [6  8  16  22  24  30  34  35]. Several efﬁcient ways of computing the full covariance
matrix are discussed by Seeger [31]. Other works have considered various structured covariance
approximations  based on the Cholesky decomposition [8  35]  sparse covariance matrices [34] and
low-rank plus diagonal structure [6  26  30]. Recently  several works [24  26] have applied stochastic
gradient descent on the variational objective to estimate such a structure. These methods often employ
an adaptive learning rate method  such as Adam or RMSprop  which increases the memory cost. All
of these methods have only been applied to shallow models  and it remains unclear how they will
perform (and whether they can be adapted) for deep models. Moreover  a natural-gradient method is
preferable to gradient-based methods when optimizing the parameters of a distribution [3  15  18].

2

Full GaussianSLANG (Rank 1)SLANG (Rank 5)SLANG (Rank 10)-0.4-0.3-0.2-0.10.00.1×1020.60.40.20.00.20.4MeanFull GaussianMFSLANG-1SLANG-5SLANG-10Image pixel (by row)0.020.030.04VarianceOur work shows that a natural-gradient method not only has better convergence properties  but also
has lower computation and memory cost than gradient-based methods.
For deep models  a variety of methods have been proposed based on mean-ﬁeld approximations.
These methods optimize the variational objective using stochastic-gradient methods and differ from
each other in the way they compute those gradients [7  12  14  19  28  38]. They all give poor
uncertainty estimates in the presence of strong posterior correlations and also shrink variances [36].
SLANG is designed to add extra covariance structure and ensure better performance than mean-ﬁeld
approaches.
A few recent works have explored structured covariance approximations for deep models. In [38] 
the Kronecker-factored approximate curvature (K-FAC) method is applied to perform approximate
natural-gradient VI. Another recent work has applied K-FAC to ﬁnd a Laplace approximation [29].
However  the Laplace approximation can perform worse than variational inference in many scenarios 
e.g.  when the posterior distribution is not symmetric [25]. Other types of approximation methods
include Bayesian dropout [10] and methods that use matrix-variate Gaussians [21  33]. All of these
approaches make structural assumptions that are different from our low-rank plus diagonal structure.
However  similarly to our work  they provide new ways to improve the speed and accuracy of
uncertainty estimation in deep learning.

2 Gaussian Approximation with Natural-Gradient Variational Inference

Our goal is to estimate the uncertainty in deep models using Bayesian inference. Given N data
examples D = {Di}N
i=1  a Bayesian version of a deep model can be speciﬁed by using a likelihood
p(Di|θ) parametrized by a deep network with parameters θ ∈ RD and a prior distribution p(θ).
For simplicity  we assume that the prior is a Gaussian distribution  such as an isotropic Gaussian
p(θ) ∼ N (0  (1/λ)I) with the scalar precision parameter λ > 0. However  the methods presented
in this paper can easily be modiﬁed to handle many other types of prior distributions. Given
such a model  Bayesian approaches compute an estimate of uncertainty by using the posterior
distribution: p(θ|D) = p(D|θ)p(θ)/p(D). This requires computation of the marginal likelihood

p(D) =(cid:82) p(D|θ)p(θ)dθ  which is a high-dimensional integral and difﬁcult to compute.

Variational inference (VI) simpliﬁes the problem by approximating p(θ|D) with a distribution
q(θ).
In this paper  our focus is on obtaining approximations that have a Gaussian form  i.e. 
q(θ) = N (θ|µ  Σ) with mean µ and covariance Σ. The parameters µ and Σ are referred to as
the variational parameters and can be obtained by maximizing a lower bound on p(D) called the
evidence lower bound (ELBO) 

ELBO: L(µ  Σ) := Eq [log p(D|θ)] − D

KL[q(θ)(cid:107) p(θ)].

(1)

where DKL[·] denotes the Kullback-Leibler divergence.
A straightforward and popular approach to optimize L is to use stochastic gradient methods [24  26 
28  35]. However  natural-gradients are preferable when optimizing the parameters of a distribution
[3  15  18]. This is because natural-gradient methods perform optimization on the Riemannian
manifold of the parameters  which can lead to a faster convergence when compared to gradient-based
methods. Typically  natural-gradient methods are difﬁcult to implement  but many easy-to-implement
updates have been derived in recent works [15  17  19  38]. We build upon the approximate natural-
gradient method proposed in [19] and modify it to estimate structured covariance-approximations.
Speciﬁcally  we extend the Variational Online Gauss-Newton (VOGN) method [19]. This method
uses the following update for µ and Σ (a derivation is in Appendix A) 
t+1 = (1 − βt)Σ−1
gi(θt)  and ˆG(θt) := − N
M

(cid:105)
 
gi(θt)gi(θt)(cid:62) 

(cid:104) ˆG(θt) + λI
(cid:88)

µt+1 = µt − αtΣt+1 [ˆg(θt) + λµt]   Σ−1

with ˆg(θt) := − N
M

(2)

t + βt

i∈M

(cid:88)

i∈M

where t is the iteration number  αt  βt > 0 are learning rates  θt ∼ N (θ|µt  Σt)  gi(θt) :=
∇θ log p(Di|θt) is the back-propagated gradient obtained on the i’th data example  ˆG(θt) is an
Empirical Fisher (EF) matrix  and M is a minibatch of M data examples. This update is an
approximate natural-gradient update obtained by using the EF matrix as an approximation of the

3

Hessian [23] in a method called Variational Online Newton (VON) [19]. This is explained in more
detail in Appendix A. As discussed in [19]  the VOGN method is an approximate Natural-gradient
update which may not have the same properties as the exact natural-gradient update. However  an
advantage of the update (2) is that it only requires back-propagated gradients  which is a desirable
feature when working with deep models.
The update (2) is computationally infeasible for large deep models because it requires the storage
and inversion of the D × D covariance matrix. Storage takes O(D2) memory space and inversion
requires O(D3) computations  which makes the update very costly to perform for large models. We
cannot form Σ or invert it when D is in millions. Mean-ﬁeld approximations avoid this issue by
restricting Σ to be a diagonal matrix  but they often give poor Gaussian approximations. Our idea is
to estimate a low-rank plus diagonal approximation of Σ that reduces the computational cost while
preserving some off-diagonal covariance structure. In the next section  we propose modiﬁcations to
the update (2) to obtain a method whose time and space complexity are both linear in D.

3 Stochastic  Low-rank  Approximate Natural-Gradient (SLANG) Method

Our goal is to modify the update (2) to obtain a method whose time and space complexity is linear in
D. We propose to approximate the inverse of Σt by a “low-rank plus diagonal” matrix:

t ≈ ˆΣ−1
Σ−1

t

:= UtU(cid:62)

(3)
where Ut is a D × L matrix with L (cid:28) D and Dt is a D × D diagonal matrix. The cost of storing
and inverting this matrix is linear in D and reasonable when L is small. We now derive an update
for Ut and Dt such that the resulting ˆΣ−1
t+1 closely approximates the update shown in (2). We start
by writing an approximation to the update of Σ−1
t+1 where we replace covariance matrices by their
structured approximations:

t + Dt 

t+1 := Ut+1U(cid:62)
ˆΣ−1

t+1 + Dt+1 ≈ (1 − βt) ˆΣ−1

t + βt

(4)

(cid:104) ˆG(θt) + λI

(cid:105)

(cid:105)

t + βt

(1 − βt) ˆΣ−1

(cid:104) ˆG(θt) + λI

This update cannot be performed exactly without potentially increasing the rank of the low-rank
component Ut+1  since the structured components on the right hand side are of rank at most L + M 
where M is the size of the minibatch. This is shown in (5) below where we have rearranged the left
hand side of (4) as the sum of a structured component and a diagonal component. To obtain a rank L
approximation to the left hand side of (5)  we propose to approximate the structured component by
an eigenvalue decomposition as shown in (6) below 

(cid:123)(cid:122)
(cid:123)(cid:122)
where Q1:L is a D×L matrix containing the ﬁrst L leading eigenvectors of (1−βt)UtU(cid:62)
t +βt ˆG(θt)
and Λ1:L is an L × L diagonal matrix containing the corresponding eigenvalues. Figure 2 visualizes
the update from (5) to (6).
The low-rank component Ut+1 can now be updated to mirror the low-rank component of (6) 

+ (1 − βt)Dt + βtλI

+ (1 − βt)Dt + βtλI

= (1 − βt)UtU(cid:62)

Rank at most L + M

(cid:124)
Q1:LΛ1:LQ(cid:62)

Rank L approximation

(cid:125)

t + βt ˆG(θt)

(cid:123)(cid:122)
(cid:123)(cid:122)

Diagonal component

Diagonal component

(cid:124)
(cid:124)

(cid:125)
(cid:125)

 

 

(cid:124)

(cid:125)

(5)

(6)

≈

1:L

(cid:104)

and the diagonal Dt+1 can be updated to match the diagonal of the left and right sides of (4)  i.e. 

diag

Ut+1U(cid:62)

t+1 + Dt+1

= diag

(1 − βt)UtU(cid:62)

t + βt ˆG(θt) + (1 − βt)Dt + βtλI

(cid:105)

(7)

 

(8)

(9)

(10)

Ut+1 = Q1:LΛ1/2

1:L   

(cid:105)

(cid:104)

(cid:104)

This gives us the following update for Dt+1 using a diagonal correction ∆t 

Dt+1 = (1 − β)Dt + βtλI + ∆t 

∆t = diag

(1 − β)UtU(cid:62)

t + βt ˆG(θt) − Ut+1U(cid:62)

t+1

.

(cid:105)

This step is cheap since computing the diagonal of the EF matrix is linear in D.

4

D × L

L × D

D × M

M × D

D × L

L × D

+

=

fast_eig

≈

(1 − β)UtU(cid:62)

t

βG(θt)

Ut+1U(cid:62)

t+1

Figure 2: This ﬁgure illustrates Equations (6) and (7) which are used to derive SLANG.

The new covariance approximation can now be used to update µt+1 according to (2) as shown below:

SLANG: µt+1 = µt − αt

Ut+1U(cid:62)

t+1 + Dt+1

[ˆg(θt) + λµt]  

(11)

(cid:104)

(cid:105)−1

The above update uses a stochastic  low-rank covariance estimate to approximate natural-gradient
updates  which is why we use the name SLANG.
When L = D  Ut+1U(cid:62)
t+1 is full rank and SLANG is identical to the approximate natural-gradient
update (2). When L < D  SLANG produces matrices ˆΣ−1
t with diagonals matching (2) at every
iteration. The diagonal correction ensures that no diagonal information is lost during the low-rank
approximation of the covariance. A formal statement and proof is given in Appendix D.
We also tried an alternative method where Ut+1 is learned using an exponential moving-average of
the eigendecompositions of ˆG(θ). This previous iteration of SLANG is discussed in Appendix B 
where we show that it gives worse results than the SLANG update.
Next  we give implementation details of SLANG.

3.1 Details of the SLANG Implementation

t + Dt

The pseudo-code for SLANG is shown in Algorithm 1 in Figure 3.
At every iteration  we generate a sample θt ∼ N (θ|µt  UtU(cid:62)
t + Dt). This is implemented in
line 4 of Algorithm 1 using the subroutine fast_sample. Pseudocode for this subroutine is given
in Algorithm 3. This function uses the Woodbury identity and to compute the square-root matrix

(cid:1)−1/2 [4]. The sample is then computed as θt = µt + At  where  ∼ N (0  I).

At =(cid:0)UtU(cid:62)

The function fast_sample requires computations in O(DL2 + DLS) to generate S samples  which
is linear in D. More details are given in Appendix C.4.
Given a sample  we need to compute and store all the individual stochastic gradients gi(θt) for all
examples i in a minibatch M. The standard back-propagation implementation does not allow this.
We instead use a version of the backpropagation algorithm outlined in a note by Goodfellow [11] 
which enables efﬁcient computation of the gradients ˆgi(θt). This is shown in line 6 of Algorithm 1 
where a subroutine backprop_goodfellow is used (see details of this subroutine in Appendix C.1).
In line 7  we compute the eigenvalue decomposition of (1 − βt)UtUt + βt ˆG(θt) by using the
fast_eig subroutine. The subroutine fast_eig implements a randomized eigenvalue decomposi-
tion method discussed in [13]. It computes the top-L eigendecomposition of a low-rank matrix in
O(DLM S + DL2). More details on the subroutine are given in Appendix C.2. The matrix Ut+1
and Dt+1 are updated using the eigenvalue decomposition in lines 8  9 and 10.
In lines 11 and 12  we compute the update vector [Ut+1U(cid:62)
t+1 + Dt+1]−1 [ˆg(θt) + λµt]  which
requires solving a linear system. We use the subroutine fast_inverse shown in Algorithm 2. This
subroutine uses the Woodbury identity to efﬁciently compute the inverse with a cost O(DL2). More
details are given in Appendix C.3. Finally  in line 13  we update µt+1.
The overall computational complexity of SLANG is O(DL2 + DLM S) and its memory cost is
O(DL + DM S). Both are linear in D and M. The cost is quadratic in L  but since L (cid:28) D (e.g.  5

5

Algorithm 1 SLANG
Require: Data D  hyperparameters M  L  λ  α  β
1: Initialize µ  U  d
2: δ ← (1 − β)
3: while not converged do
4:
5: M ← sample a minibatch
6:
7:
8:
9:
10:
11:
12:
13:
14: end while
15: return µ  U  d

θ ← fast_sample(µ  U  d)
[g1  ..  gM ] ← backprop_goodfellow(DM  θ)
V ← fast_eig(δu1  ..  δuL  βg1  ..  βgM   L)
U ← V
d ← δd + ∆d + λ1
∆µ ← fast_inverse(ˆg  U  d)
µ ← µ − α∆µ

∆d ←(cid:80)L
ˆg ←(cid:80)

i +(cid:80)M

i −(cid:80)L

i gi + λµ

i=1 δu2

i=1 βg2

i=1 v2
i

Algorithm 2 fast_inverse(g  U  d)
1: A ← (IL + U(cid:62)d
2: y ← d
3: return y

−1U)−1
−1UAU(cid:62)d

−1g − d

−1g

−1/2 (cid:12) U

Algorithm 3 fast_sample(µ  U  d)
1:  ∼ N (0  ID)
2: V ← d
3: A ← Cholesky(V(cid:62)V)
4: B ← Cholesky(IL + V(cid:62)V)
5: C ← A−(cid:62)(B − IL)A−1
6: K ← (C + V(cid:62)V)−1
7: y ← d
−1/2 − VKV(cid:62)
8: return µ + y

Figure 3: Algorithm 1 gives the pseudo-code for SLANG. Here  M is the minibatch size  L is
the number of low-rank factors  λ is the prior precision parameter  and α  β are learning rates. The
diagonal component is denoted with a vector d and the columns of the matrix U and V are denoted
by uj and vj respectively. The algorithm depends on multiple subroutines  described in more details
in Section 3.1. The overall complexity of the algorithm is O(DL2 + DLM ).

or 10)  this only adds a small multiplicative constant in the runtime. SLANG reduces the cost of the
update (2) signiﬁcantly while preserving some posterior correlations.

4 Experiments

In this section  our goal is to show experimental results in support of the following claims: (1)
SLANG gives reasonable posterior approximations  and (2) SLANG performs well on standard
benchmarks for Bayesian neural networks. We present evaluations on several LIBSVM datasets  the
UCI regression benchmark  and MNIST classiﬁcation. SLANG beats mean-ﬁeld methods on almost
all tasks considered and performs comparably to state-of-the-art methods. SLANG also converges
faster than mean-ﬁeld methods.

4.1 Bayesian Logistic Regression

We considered four benchmark datasets for our comparison: USPS 3vs5  Australian  Breast-Cancer 
and a1a. Details of the datasets are in Table 8 in Appendix E.2 along with the implementation details
of the methods we compare to. We use L-BFGS [37] to compute the optimal full-Gaussian variational
approximation that minimizes the ELBO using the method described in Marlin et al. [22]. We refer
to the optimal full-Gaussian variational approximation as the “Full-Gaussian Exact” method. We also
compute the optimal mean-ﬁeld Gaussian approximation and refer to it as “MF Exact”.
Figure 1 shows a qualitative comparison of the estimated posterior means  variances  and covariances
for the USPS-3vs5 dataset (N = 770  D = 256). The ﬁgure on the left compares covariance
approximations obtained with SLANG to the Full-Gaussian Exact method. Only off-diagonal entries
are shown. We see that the approximation becomes more and more accurate as the rank is increased.
The ﬁgures on the right compare the means and variances. The means match closely for all methods 
but the variance is heavily underestimated by the MF Exact method; we see that the variances obtained
under the mean-ﬁeld approximation estimate a high variance where Full-Gaussian Exact has a low
variance and vice-versa. This “trend-reversal” is due to the typical shrinking behavior of mean-ﬁeld
methods [36]. In contrast  SLANG corrects the trend reversal problem even when L = 1. Similar
results for other datasets are shown in Figure 7 in Appendix E.1.

6

Table 1: Results on Bayesian logistic regression where we compare SLANG to three full-Gaussian
methods and three mean-ﬁeld methods. We measure negative ELBO  test log-loss  and symmetric
KL-divergence between each approximation and the Full-Gaussian Exact method (last column).
Lower values are better. SLANG nearly always gives better results than the mean-ﬁeld methods  and
with L = 10 is comparable to Full-Gaussian methods. This shows that our structured covariance
approximation is reasonably accurate for Bayesian logistic regression.

Dataset Metrics

Australian

Breast
Cancer

a1a

USPS
3vs5

Mean-Field Methods
EF Hess. Exact
0.614 0.613 0.593
ELBO
0.348 0.347 0.341
NLL
KL ( ×104) 2.240 2.030 0.195
ELBO
0.122 0.121 0.121
NLL
0.095 0.094 0.094
KL ( ×100) 8.019 9.071 7.771
ELBO
0.384 0.383 0.383
NLL
0.339 0.339 0.339
KL (×102) 2.590 2.208 1.295
ELBO
0.268 0.268 0.267
NLL
0.139 0.139 0.138
KL (×101) 7.684 7.188 7.083

SLANG

L = 1 L = 5 L = 10
0.574 0.569 0.566
0.342 0.339 0.338
0.033 0.008 0.002
0.112 0.111 0.111
0.092 0.092 0.092
0.911 0.842 0.638
0.377 0.374 0.373
0.339 0.339 0.339
0.305 0.173 0.118
0.210 0.198 0.193
0.132 0.132 0.131
1.492 0.755 0.448

Full Gaussian

EF Hess. Exact
0.560 0.558 0.559
0.340 0.339 0.338
0.000 0.000 0.000
0.111 0.109 0.109
0.092 0.091 0.091
0.637 0.002 0.000
0.369 0.368 0.368
0.339 0.339 0.339
0.014 0.000 0.000
0.189 0.186 0.186
0.131 0.130 0.130
0.180 0.001 0.000

The complete results for Bayesian logistic regression are summarized in Table 1  where we also
compare to four additional methods called “Full-Gaussian EF”  “Full-Gaussian Hessian”  “Mean-
Field EF”  and “Mean-Field Hessian”. The Full-Gaussian EF method is the natural-gradient update
(2) which uses the EF matrix ˆG(θ)  while the Full-Gaussian Hessian method uses the Hessian instead
of the EF matrix (the updates are given in (12) and (13) in Appendix A). The last two methods are the
mean-ﬁeld versions of the Full-Gaussian EF and Full-Gaussian Hessian methods  respectively. We
compare negative ELBO  test log-loss using cross-entropy  and symmetric KL-divergence between the
approximations and the Full-Gaussian Exact method. We report averages over 20 random 50%-50%
training-test splits of the dataset. Variances and results for SLANG with L = 2 are omitted here due
to space constraints  but are reported in Table 6 in Appendix E.1.
We ﬁnd that SLANG with L = 1 nearly always produces better approximations than the mean-ﬁeld
methods. As expected  increasing L improves the quality of the variational distribution found by
SLANG according to all three metrics. We also note that Full-Gaussian EF method has similar
performance to the Full-Gaussian Hessian method  which indicates that the EF approximation may
be acceptable for Bayesian logistic regression.
The left side in Figure 4 shows convergence results on the USPS 3vs5 and Breast Cancer datasets.
The three methods SLANG(1  2  3) refer to SLANG with L = 1  5  10. We compare these three
SLANG methods to Mean-Field Hessian and Full-Gaussian Hessian. SLANG converges faster
than the mean-ﬁeld method  and matches the convergence of the full-Gaussian method when L is
increased.

4.2 Bayesian Neural Networks (BNNs)

An example for Bayesian Neural Networks on a synthetic regression dataset is given in Appendix F.1 
where we illustrate the quality of SLANG’s posterior covariance.
The right side in Figure 4 shows convergence results for the USPS 3vs5 and Breast Cancer datasets.
Here  the three methods SLANG(1  2  3) refer to SLANG with L = 8  16  32. We compare SLANG
to a mean-ﬁeld method called Bayes by Backprop [7]. Similar to the Bayesian logistic regression
experiment  SLANG converges much faster than the mean-ﬁeld method. However  the ELBO
convergence for SLANG shows that the optimization procedure does not necessarily converge to a
local minimum. This issue does not appear to affect the test log-likelihood. While it might only be
due to stochasticity  it is possible that the problem is exacerbated by the replacement of the Hessian
with the EF matrix. We have not determined the speciﬁc cause and it warrants further investigation in
future work.

7

Figure 4: This ﬁgure compares the convergence behavior on two datasets: USPS 3vs5 (top) and
Breast Cancer (bottom); and two models: Bayesian logistic regression (left) and Bayesian neural
networks (BNN) (right). The three methods SLANG(1  2  3) refer to SLANG with L = 1  5  10 for
logistic regression. For BNN  they refer to SLANG with L = 8  16  32. The mean-ﬁeld method
is a natural-gradient mean-ﬁeld method for logistic regression (see text) and BBB [7] for BNN.
This comparison clearly shows that SLANG converges faster than the mean-ﬁeld method  and 
for Bayesian logistic regression  matches the convergence of the full-Gaussian method when L is
increased.

Table 2: Comparison on UCI datasets using Bayesian neural networks. We repeat the setup used
in Gal and Ghahramani [10]. SLANG uses L = 1  and outperforms BBB but gives comparable
performance to Dropout.

Test RMSE
Dropout

BBB

Dataset
SLANG
3.43 ± 0.20 2.97 ± 0.19 3.21 ± 0.19
Boston
Concrete 6.16 ± 0.13 5.23 ± 0.12 5.58 ± 0.19
0.97 ± 0.09 1.66 ± 0.04 0.64 ± 0.03
Energy
Kin8nm 0.08 ± 0.00 0.10 ± 0.00 0.08 ± 0.00
0.00 ± 0.00 0.01 ± 0.00 0.00 ± 0.00
Naval
4.21 ± 0.03 4.02 ± 0.04 4.16 ± 0.04
Power
0.64 ± 0.01 0.62 ± 0.01 0.65 ± 0.01
Wine
1.13 ± 0.06 1.11 ± 0.09 1.08 ± 0.06
Yacht

Test log-likelihood

BBB

Dropout

SLANG
-2.66 ± 0.06 -2.46 ± 0.06 -2.58 ± 0.05
-3.25 ± 0.02 -3.04 ± 0.02 -3.13 ± 0.03
-1.45 ± 0.10 -1.99 ± 0.02 -1.12 ± 0.01
1.07 ± 0.00
1.06 ± 0.00
4.61 ± 0.01
4.76 ± 0.00
-2.86 ± 0.01 -2.80 ± 0.01 -2.84 ± 0.01
-0.97 ± 0.01 -0.93 ± 0.01 -0.97 ± 0.01
-1.56 ± 0.02 -1.55 ± 0.03 -1.88 ± 0.01

0.95 ± 0.01
3.80 ± 0.01

Next  we present results on the UCI regression datasets which are common benchmarks for Bayesian
neural networks [14]. We repeat the setup2 used in Gal and Ghahramani [10]. Following their
work  we use neural networks with one hidden layer with 50 hidden units and ReLU activation
functions. We compare SLANG with L = 1 to the Bayes By Backprop (BBB) method [7] and the
Bayesian Dropout method of [10]. For the 5 smallest datasets  we used a mini-batch size of 10 and 4
Monte-Carlo samples during training. For the 3 larger datasets  we used a mini-batch size of 100
and 2 Monte-Carlo samples during training. More details are given in Appendix F.3. We report test
RMSE and test log-likelihood in Table 2. SLANG with just one rank outperforms BBB on 7 out
of 8 datasets for RMSE and on 5 out of 8 datasets for log-likelihood. Moreover  SLANG shows
comparable performance to Dropout.

2We use the data splits available at https://github.com/yaringal/DropoutUncertaintyExps

8

Table 3: Comparison of SLANG on the MNIST dataset. We use a two layer neural network with 400
units each. SLANG obtains good performances for moderate values of L.

SLANG

Test Error

BBB
1.82%

L = 1
L = 16 L = 32
2.00% 1.95% 1.81% 1.92% 1.77% 1.73%

L = 2

L = 4

L = 8

Finally  we report results for classiﬁcation on MNIST. We train a BNN with two hidden layers of
400 hidden units each. The training set consists of 50 000 examples and the remaining 10 000 are
used as a validation set. The test set is a separate set which consists of 10 000 examples. We use
SLANG with L = 1  2  4  8  16  32. For each value of L  we choose the prior precision and learning
rate based on performance on the validation set. Further details can be found in Appendix F.4. The
test accuracies are reported in Table 3 and compared to the results obtained in [7] by using BBB.
For SLANG  a good performance can be obtained for a moderate L. Note that there might be small
differences between our experimental setup and the one used in [7] since BBB implementation is not
publicly available. Therefore  the results might not be directly comparable. Nevertheless  SLANG
appears to perform well compared to BBB.

5 Conclusion

We consider the challenging problem of uncertainty estimation in large deep models. For such
problems  it is infeasible to form a Gaussian approximation to the posterior distribution. We address
this issue by estimating a Gaussian approximation that uses a covariance with low-rank plus diagonal
structure. We proposed an approximate natural-gradient algorithm to estimate the structured covari-
ance matrix. Our method  called SLANG  relies only on the back-propagated gradients to estimate the
covariance structure  which is a desirable feature when working with deep models. Empirical results
strongly suggest that the accuracy of our method is better than those obtained by using mean-ﬁeld
methods. Moreover  we observe that  unlike mean-ﬁeld methods  our method does not drastically
shrink the marginal variances. Experiments also show that SLANG is highly ﬂexible and that its
accuracy can be improved by increasing the rank of the covariance’s low-rank component. Finally 
our method converges faster than the mean-ﬁeld methods and can sometimes converge as fast as VI
methods that use a full-Gaussian approximation.
The experiments presented in this paper are restricted to feed-forward neural networks. This is partly
because existing deep-learning software packages do not support individual gradient computations.
Individual gradients  which are required in line 6 of Algorithm 1  must be manually implemented for
other types of architectures. Further work is therefore necessary to establish the usefulness of our
method on other types of network architectures.
SLANG is based on a natural-gradient method that employs the empirical Fisher approximation [19].
Our empirical results suggest that this approximation is reasonably accurate. However  it is not clear
if this is always the case. It is important to investigate this issue to gain better understanding of the
effect of the approximation  both theoretically and empirically.
During this work  we also found that comparing the quality of covariance approximations is a
nontrivial task for deep neural networks. We believe that existing benchmarks are not sufﬁcient to
establish the quality of an approximate Bayesian inference method for deep models. An interesting
and useful area of further research is the development of good benchmarks that better reﬂect the
quality of posterior approximations. This will facilitate the design of better inference algorithms.

Acknowledgements

We thank the anonymous reviewers for their helpful feedback. We greatly appreciate useful discus-
sions with Shun-ichi Amari (RIKEN)  Rio Yokota (Tokyo Institute of Technology)  Kazuki Oosawa
(Tokyo Institute of Technology)  Wu Lin (University of British Columbia)  and Voot Tangkaratt
(RIKEN). We are also thankful for the RAIDEN computing system and its support team at the RIKEN
Center for Advanced Intelligence Project  which we used extensively for our experiments.

9

References
[1] Martín Abadi  Paul Barham  Jianmin Chen  Zhifeng Chen  Andy Davis  Jeffrey Dean  Matthieu Devin 
Sanjay Ghemawat  Geoffrey Irving  Michael Isard  Manjunath Kudlur  Josh Levenberg  Rajat Monga 
Sherry Moore  Derek Gordon Murray  Benoit Steiner  Paul A. Tucker  Vijay Vasudevan  Pete Warden 
Martin Wicke  Yuan Yu  and Xiaoqiang Zheng. Tensorﬂow: A system for large-scale machine learning. In
12th USENIX Symposium on Operating Systems Design and Implementation  OSDI  pages 265–283  2016.

[2] Sungjin Ahn  Anoop Korattikara Balan  and Max Welling. Bayesian posterior sampling via stochastic
gradient ﬁsher scoring. In Proceedings of the 29th International Conference on Machine Learning  ICML 
2012.

[3] Shun-ichi Amari. Natural gradient works efﬁciently in learning. Neural Computation  10(2):251–276 

1998.

[4] Sivaram Ambikasaran and Michael O’Neil. Fast symmetric factorization of hierarchical matrices with

applications. CoRR  abs/1405.0223  2014.

[5] Anoop Korattikara Balan  Vivek Rathod  Kevin P. Murphy  and Max Welling. Bayesian dark knowledge.
In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information
Processing Systems  pages 3438–3446  2015.

[6] David Barber and Christopher M. Bishop. Ensemble learning for multi-layer networks. In Advances in

Neural Information Processing Systems 10  pages 395–401  1997.

[7] Charles Blundell  Julien Cornebise  Koray Kavukcuoglu  and Daan Wierstra. Weight uncertainty in neural

networks. CoRR  abs/1505.05424  2015.

[8] Edward Challis and David Barber. Concave gaussian variational approximations for inference in large-scale
bayesian linear models. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence
and Statistics  AISTATS  pages 199–207  2011.

[9] Tianqi Chen  Emily B. Fox  and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo.

In
Proceedings of the 31th International Conference on Machine Learning  ICML  pages 1683–1691  2014.

[10] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty
in deep learning. In Proceedings of the 33nd International Conference on Machine Learning  ICML  pages
1050–1059  2016.

[11] Ian J. Goodfellow. Efﬁcient per-example gradient computations. CoRR  abs/1510.01799  2015.

[12] Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information
Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems.  pages
2348–2356  2011.

[13] Nathan Halko  Per-Gunnar Martinsson  and Joel A. Tropp. Finding structure with randomness: Probabilistic

algorithms for constructing approximate matrix decompositions. SIAM Review  53(2):217–288  2011.

[14] José Miguel Hernández-Lobato and Ryan P. Adams. Probabilistic backpropagation for scalable learning of
bayesian neural networks. In Proceedings of the 32nd International Conference on Machine Learning 
ICML  pages 1861–1869  2015.

[15] Matthew D. Hoffman  David M. Blei  Chong Wang  and John William Paisley. Stochastic variational

inference. Journal of Machine Learning Research  14(1):1303–1347  2013.

[16] Tommi Jaakkola and Michael Jordan. A variational approach to Bayesian logistic regression problems
and their extensions. In Proceedings of the Sixth International Workshop on Artiﬁcial Intelligence and
Statistics  AISTATS  1997.

[17] Mohammad Emtiyaz Khan and Wu Lin. Conjugate-computation variational inference: Converting varia-
tional inference in non-conjugate models to inferences in conjugate models. In Proceedings of the 20th
International Conference on Artiﬁcial Intelligence and Statistics  AISTATS  pages 878–887  2017.

[18] Mohammad Emtiyaz Khan and Didrik Nielsen. Fast yet simple natural-gradient descent for variational

inference in complex models. CoRR  abs/1807.04489  2018.

[19] Mohammad Emtiyaz Khan  Didrik Nielsen  Voot Tangkaratt  Wu Lin  Yarin Gal  and Akash Srivastava.
Fast and scalable Bayesian deep learning by weight-perturbation in Adam. In Proceedings of the 35th
International Conference on Machine Learning  pages 2611–2620  2018.

10

[20] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR  abs/1412.6980 

2014.

[21] Christos Louizos and Max Welling. Structured and efﬁcient variational deep learning with matrix gaussian
posteriors. In Proceedings of the 33nd International Conference on Machine Learning  ICML  pages
1708–1716  2016.

[22] Benjamin M. Marlin  Mohammad Emtiyaz Khan  and Kevin P. Murphy. Piecewise bounds for estimating
bernoulli-logistic latent gaussian models. In Proceedings of the 28th International Conference on Machine
Learning  ICML  pages 633–640  2011.

[23] James Martens. New perspectives on the natural gradient method. CoRR  abs/1412.1193  2014.

[24] Andrew C. Miller  Nicholas J. Foti  and Ryan P. Adams. Variational boosting: Iteratively reﬁning posterior
approximations. In Proceedings of the 34th International Conference on Machine Learning  ICML  pages
2420–2429  2017.

[25] Hannes Nickisch and Carl Edward Rasmussen. Approximations for binary gaussian process classiﬁcation.

Journal of Machine Learning Research  9(Oct):2035–2078  2008.

[26] Victor M.-H. Ong  David J. Nott  and Michael S. Smith. Gaussian variational approximation with a factor

covariance structure. Journal of Computational and Graphical Statistics  27(3):465–478  2018.

[27] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito  Zeming
Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in pytorch. In Autodiff
Workshop  during the Annual Conference on Neural Information Processing Systems  2017.

[28] Rajesh Ranganath  Sean Gerrish  and David M. Blei. Black box variational inference. In Proceedings of
the Seventeenth International Conference on Artiﬁcial Intelligence and Statistics  AISTATS  pages 814–822 
2014.

[29] Hippolyt Ritter  Aleksandar Botev  and David Barber. A scalable laplace approximation for neural networks.

In International Conference on Learning Representations  2018.

[30] Matthias W. Seeger. Bayesian model selection for support vector machines  gaussian processes and other

kernel classiﬁers. In Advances in Neural Information Processing Systems 12  pages 603–609  1999.

[31] Matthias W. Seeger. Gaussian covariance and scalable variational inference. In Proceedings of the 27th

International Conference on Machine Learning  ICML  pages 967–974  2010.

[32] Umut Simsekli  Roland Badeau  A. Taylan Cemgil  and Gaël Richard. Stochastic quasi-newton langevin
monte carlo. In Proceedings of the 33nd International Conference on Machine Learning  ICML  pages
642–651  2016.

[33] Shengyang Sun  Changyou Chen  and Lawrence Carin. Learning structured weight uncertainty in bayesian
In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and

neural networks.
Statistics  AISTATS  pages 1283–1292  2017.

[34] Linda S. L. Tan and David J. Nott. Gaussian variational approximation with sparse precision matrices.

Statistics and Computing  28(2):259–275  2018.

[35] Michalis K. Titsias and Miguel Lázaro-Gredilla. Doubly stochastic variational bayes for non-conjugate
inference. In Proceedings of the 31th International Conference on Machine Learning  ICML  pages
1971–1979  2014.

[36] Richard E. Turner and Maneesh Sahani. Two problems with variational expectation maximisation for
time-series models. In D. Barber  T. Cemgil  and S. Chiappa  editors  Bayesian Time series models 
chapter 5  pages 109–130. Cambridge University Press  2011.

[37] Stephen J. Wright and J Nocedal. Numerical optimization. Springer New York  1999.

[38] Guodong Zhang  Shengyang Sun  David K. Duvenaud  and Roger B. Grosse. Noisy natural gradient as
variational inference. In Proceedings of the 35th International Conference on Machine Learning  ICML 
pages 5847–5856  2018.

11

,Aaron Mishkin
Frederik Kunstner
Didrik Nielsen
Mark Schmidt
Mohammad Emtiyaz Khan