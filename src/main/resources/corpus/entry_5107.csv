2011,Sparse Features for PCA-Like Linear Regression,Principal Components Analysis~(PCA) is often used as a feature extraction procedure. Given a matrix $X \in \mathbb{R}^{n \times d}$  whose rows represent $n$ data points with respect to $d$ features  the top $k$ right singular vectors of $X$ (the so-called \textit{eigenfeatures})  are arbitrary linear combinations of all available features. The eigenfeatures are very useful in data analysis  including the regularization of linear regression. Enforcing sparsity on the eigenfeatures  i.e.  forcing them to be linear combinations of only a \textit{small} number of actual features (as opposed to all available features)  can promote better generalization error and improve the interpretability of the eigenfeatures. We present deterministic and randomized algorithms that construct such sparse eigenfeatures while \emph{provably} achieving in-sample performance comparable to regularized linear regression. Our algorithms are relatively simple and practically efficient  and we demonstrate their performance on several data sets.,Sparse Features for PCA-Like Linear Regression

Christos Boutsidis

Mathematical Sciences Department
IBM T. J. Watson Research Center

Yorktown Heights  New York
cboutsi@us.ibm.com

Petros Drineas

Computer Science Department
Rensselaer Polytechnic Institute

Troy  NY 12180

drinep@cs.rpi.edu

Malik Magdon-Ismail

Computer Science Department
Rensselaer Polytechnic Institute

Troy  NY 12180

magdon@cs.rpi.edu

Abstract

Principal Components Analysis (PCA) is often used as a feature extraction proce-
dure. Given a matrix X ∈ Rn×d  whose rows represent n data points with respect
to d features  the top k right singular vectors of X (the so-called eigenfeatures) 
are arbitrary linear combinations of all available features. The eigenfeatures are
very useful in data analysis  including the regularization of linear regression. En-
forcing sparsity on the eigenfeatures  i.e.  forcing them to be linear combinations
of only a small number of actual features (as opposed to all available features)  can
promote better generalization error and improve the interpretability of the eigen-
features. We present deterministic and randomized algorithms that construct such
sparse eigenfeatures while provably achieving in-sample performance comparable
to regularized linear regression. Our algorithms are relatively simple and practi-
cally efﬁcient  and we demonstrate their performance on several data sets.

1 Introduction

Least-squares analysis was introduced by Gauss in 1795 and has since has bloomed into a staple of
the data analyst. Assume the usual setting with n tuples (x1  y1)  . . .   (xn  yn) in Rd  where xi are
points and yi are targets. The vector of regression weights w∗ ∈ Rd minimizes (over all w ∈ Rd)
the RMS in-sample error

E(w) =vuut

n

Xi=1
(xi · w − yi)2 = kXw − yk2.

In the above  X ∈ Rn×d is the data matrix whose rows are the vectors xi (i.e.  Xij = xi[j]); and 
y ∈ Rn is the target vector (i.e.  y[i] = yi). We will use the more convenient matrix formulation1 
namely given X and y  we seek a vector w∗ that minimizes kXw − yk2. The minimal-norm vector
w∗ can be computed via the Moore-Penrose pseudo-inverse of X: w∗ = X+y. Then  the optimal
in-sample error is equal to:

1For the sake of simplicity  we assume d ≤ n and rank (X) = d in our exposition; neither assumption is

necessary.

E(w∗) = ky − XX+yk2.

1

When the data is noisy and X is ill-conditioned  X+ becomes unstable to small perturbations and
overﬁtting can become a serious problem. Practitioners deal with such situations by regularizing
the regression. Popular regularization methods include  for example  the Lasso [28]  Tikhonov
regularization [17]  and top-k PCA regression or truncated SVD regularization [21]. In general 
such methods are encouraging some form of parsimony  thereby reducing the number of effective
degrees of freedom available to ﬁt the data. Our focus is on top-k PCA regression which can be
viewed as regression onto the top-k principal components  or  equivalently  the top-k eigenfeatures.
The eigenfeatures are the top-k right singular vectors of X and are arbitrary linear combinations
of all available input features. The question we tackle is whether one can efﬁciently extract sparse
eigenfeatures (i.e.  eigenfeatures that are linear combinations of only a small number of the available
features) that have nearly the same performance as the top-k eigenfeatures.
Basic notation. A  B  . . . are matrices; a  b  . . . are vectors; i  j  . . . are integers; In is the n × n
identity matrix; 0m×n is the m × n matrix of zeros; ei is the standard basis (whose dimensionality
will be clear from the context). For vectors  we use the Euclidean norm k · k2; for matrices  the
Frobenius and the spectral norms: kXk2
ij and kXk2 = σ1 (X)  i.e.  the largest singular
value of X.
Top-k PCA Regression. Let X = UΣVT be the singular value decomposition of X  where U
(resp. V) is the matrix of left (resp. right) singular vectors of X with singular values in the diagonal
matrix Σ. For k ≤ d  let Uk  Σk  and Vk contain only the top-k singular vectors and associated
singular values. The best rank-k reconstruction of X in the Frobenius norm can be obtained from
this truncated singular value decomposition as Xk = UkΣkVT
k. The k right singular vectors in Vk
are called the top-k eigenfeatures. The projections of the data points onto the top k eigenfeatures are
obtained by projecting the xi’s onto the columns of Vk to obtain Fk = XVk = UΣVTVk = UkΣk.
Now  each data point (row) in Fk only has k dimensions. Each column of Fk contains a particular
eigenfeature’s value for every data point and is a linear combination of the columns of X.

F =Pi j X2

The top-k PCA regression uses Fk as the data matrix and y as the target vector to produce regression
weights w∗

k = F+
ky − Fkw∗

k y. The in-sample error of this k-dimensional regression is equal to
kyk2 = ky − UkUT

k yk2 = ky − UkΣkΣ−1

kk2 = ky − FkF+

kyk2.

k UT

k

kyk2.

E(Vkw∗

k) = ky − XVkw∗
k and Vkw∗

k are k-dimensional and cannot be applied to X  but the equivalent weights Vkw∗

The weights w∗
can be applied to X and they have the same in-sample error with respect to X:
kk2 = ky − UkUT

kk2 = ky − Fkw∗
k as the top-k PCA regression weights (the dimension will
Hence  we will refer to both w∗
k to refer to both
make it clear which one we are talking about) and  for simplicity  we will overload w∗
these weight vectors (the dimension will make it clear which). In practice  k is chosen to measure
the “effective dimension” of the data  and  typically  k ≪ rank(X) = d. One way to choose k is so
that kX − XkkF ≪ σk(X) (the “energy” in the k-th principal component is large compared to the
energy in all smaller principal components). We do not argue the merits of top-k PCA regression;
we just note that top-k PCA regression is a common tool for regularizing regression.
Problem Formulation. Given X ∈ Rn×d  k (the number of target eigenfeatures for top-k PCA
regression)  and r > k (the sparsity parameter)  we seek to extract a set of at most k sparse eigenfea-
tures ˆVk which use at most r of the actual dimensions. Let ˆFk = X ˆVk ∈ Rn×k denote the matrix
whose columns are the k extracted sparse eigenfeatures  which are a linear combination of a set of at
most r actual features. Our goal is to obtain sparse features for which the vector of sparse regression
weights ˆwk = ˆF+
k yk2 that is close to the top-k PCA
regression error ky − FkF+
k yk2. Just as with top-k PCA regression  we can deﬁne the equivalent
d-dimensional weights ˆVk ˆwk; we will overload ˆwk to refer to these weights as well.
Finally  we conclude by noting that while our discussion above has focused on simple linear regres-
sion  the problem can also be deﬁned for multiple regression  where the vector y is replaced by a
matrix Y ∈ Rn×ω  with ω ≥ 1. The weight vector w becomes a weight matrix  W  where each
column of W contains the weights from the regression of the corresponding column of Y onto the
features. All our results hold in this general setting as well  and we will actually present our main
contributions in the context of multiple regression.

k y results in an in-sample error ky − ˆFk ˆF+

2

2 Our contributions

Recall from our discussion at the end of the introduction that we will present all our results in the
general setting  where the target vector y is replaced by a matrix Y ∈ Rn×ω. Our ﬁrst theorem
argues that there exists a polynomial-time deterministic algorithm that constructs a feature matrix
ˆFk ∈ Rn×k  such that each feature (column of ˆFk) is a linear combination of at most r actual
features (columns) from X and results in small in-sample error . Again  this should be contrasted
with top-k PCA regression  which constructs a feature matrix Fk  such that each feature (column of
Fk) is a linear combination of all features (columns) in X. Our theorems argue that the in-sample
error of our features is almost as good as the in-sample error of top-k PCA regression  which uses
dense features.
Theorem 1 (Deterministic Feature Extraction). Let X ∈ Rn×d and Y ∈ Rn×ω be the input matrices
in a multiple regression problem. Let k > 0 be a target rank for top-k PCA regression on X and Y.
For any r > k  there exists an algorithm that constructs a feature matrix ˆFk = X ˆVk ∈ Rn×k  such
that every column of ˆFk is a linear combination of (the same) at most r columns of X  and

= kY − ˆFk ˆF+

k YkF ≤ kY − XW∗

Y − X ˆWk(cid:13)(cid:13)(cid:13)F

(σk(X) is the k-th singular value of X.) The running time of the proposed algorithm is T (Vk) +

(cid:13)(cid:13)(cid:13)
O(cid:0)ndk + nrk2(cid:1)  where T (Vk) is the time required to compute the matrix Vk  the top-k right sin-

gular vectors of X.

kkF + 1 +r 9k

r ! kX − XkkF

σk(X)

kYk2.

Theorem 1 says that one can construct k features with sparsity O(k) and obtain a comparble regres-
sion error to that attained by the dense top-k PCA features  up to additive term that is proportional
to ∆k = kX − XkkF /σk(X).
To construct the features satisfying the guarantees of the above theorem  we ﬁrst employ the Al-
gorithm DSF-Select (see Table 1 and Section 4.3) to select r columns of X and form the matrix
C ∈ Rn×r. Now  let ΠC k (Y) denote the best rank-k approximation (with respect to the Frobenius
norm) to Y in the column-span of C. In other words  ΠC k (Y) is a rank-k matrix that minimizes
kY − ΠC k (Y)kF over all rank-k matrices in the column-span of C. Efﬁcient algorithms are known
for computing ΠC k(X) and have been described in [2]. Given ΠC k(Y)  the sparse eigenfeatures
can be computed efﬁciently as follows: ﬁrst  set Ψ = C+ΠC k(Y). Observe that

CΨ = CC+ΠC k(Y) = ΠC k(Y).

The last equality follows because CC+ projects onto the column span of C and ΠC k(Y) is already
in the column span of C. Ψ has rank at most k because ΠC k(Y) has rank at most k. Let the
ψ and set ˆFk = CUψΣψ ∈ Rn×k. Clearly  each column of ˆFk is a
SVD of Ψ be Ψ = UψΣψVT
linear combination of (the same) at most r columns of X (the columns in C). The sparse features
themselves can also be obtained because ˆFk = X ˆVk  so ˆVk = X+ˆFk.
To prove that ˆFk are a good set of sparse features  we ﬁrst relate the regression error from using ˆFk
to how well ΠC k(Y) approximates Y.
kY − ΠC k(Y)kF = kY − CΨkF = kY − CUψΣψVT
k YkF .
The last inequality follows because ˆF+
k Y are the optimal regression weights for the features ˆFk. The
reverse inequality also holds because ΠC k(Y) is the best rank-k approximation to Y in the column
span of C. Thus 

ψkF ≥ kY − ˆFk ˆF+

= kY − ˆFkVT

ψkF

kY − ˆFk ˆF+

k YkF = kY − ΠC k(Y)kF .

The upshot of the above discussion is that if we can ﬁnd a matrix C consisting of columns of X for
which kY − ΠC k(Y)kF is small  then we immediately have good sparse eigenfeatures. Indeed  all
that remains to complete the proof of Theorem 1 is to bound kY − ΠC k(Y)kF for the columns C
returned by the Algorithm DSF-Select.
Our second result employs the Algorithm RSF-Select (see Table 2 and Section 4.4) to select r
columns of X and again form the matrix C ∈ Rn×r. One then proceeds to construct ΠC k(Y) and
ˆFk as described above. The advantage of this approach is simplicity  better efﬁciency and a slightly
better error bound  at the expense of logarithmically worse sparsity.

3

Theorem 2 (Randomized Feature Extraction). Let X ∈ Rn×d and Y ∈ Rn×ω be the input matrices
in a multiple regression problem. Let k > 0 be a target rank for top-k PCA regression on X and
Y. For any r > 144k ln(20k)  there exists a randomized algorithm that constructs a feature matrix
ˆFk = X ˆVk ∈ Rn×k  such that every column of ˆFk is a linear combination of at most r columns
of X  and  with probability at least .7 (over random choices made in the algorithm) 

(cid:13)(cid:13)(cid:13)

Y − X ˆWk(cid:13)(cid:13)(cid:13)F

The running time of the proposed algorithm is T (Vk) + O(dk + r log r).

= kY − ˆFk ˆF+

k YkF ≤ kY − XW∗

kkF +r 36k ln(20k)

r

kX − XkkF

σk(X)

kYk2.

3 Connections with prior work

A variant of our problem is the identiﬁcation of a matrix C consisting of a small number (say r)
columns of X such that the regression of Y onto C (as opposed to k features from C) gives small in-
sample error. This is the sparse approximation problem  where the number of non-zero weights in the
regression vector is restricted to r. This problem is known to be NP-hard [25]. Sparse approximation
has important applications and many approximation algorithms have been presented [29  9  30];
proposed algorithms are typically either greedy or are based on convex optimization relaxations of
the objective. An important difference between sparse approximation and sparse PCA regression is
that our goal is not to minimize the error under a sparsity constraint  but to match the top-k PCA
regularized regression under a sparsity constraint. We argue that it is possible to achieve a provably
accurate sparse PCA-regression  i.e.  use sparse features instead of dense ones.

If X = Y (approximating X using the columns of X)  then this is the column-based matrix recon-
struction problem  which has received much attention in existing literature [16  18  14  26  5  12  20].
In this paper  we study the more general problem where X 6= Y  which turns out to be considerably
more difﬁcult.

Input sparseness is closely related to feature selection and automatic relevance determination. Re-
search in this area is vast  and we refer the reader to [19] for a high-level view of the ﬁeld. Again 
the goal in this area is different than ours  namely they seek to reduce dimensionality and improve
out-of-sample error. Our goal is to provide sparse PCA features that are almost as good as the ex-
act principal components. While it is deﬁnitely the case that many methods outperform top-k PCA
regression  especially for d ≫ n  this discussion is orthogonal to our work.
The closest result to ours in prior literature is the so-called rank-revealing QR (RRQR) factoriza-
tion [8]. The authors use a QR-like decomposition to select exactly k columns of X and compare
their sparse solution vector ˆwk with the top-k PCA regularized solution w∗

k. They show that

kw∗

k − ˆwkk2 ≤pk(n − k) + 1 kX − Xkk2

σk(X)

∆ 

where ∆ = 2 k ˆwkk2 + ky − Xw∗
kk2 /σk(X). This bound is similar to our bound in Theorem 1 
but only applies to r = k and is considerably weaker. For example pk(n − k) + 1kX − Xkk2 ≥
√k kX − XkkF ; note also that the dependence of the above bound on 1/σk(X) is generally worse

than ours.

The importance of the right singular vectors in matrix reconstruction problems (including PCA)
has been heavily studied in prior literature  going back to work by Jolliffe in 1972 [22]. The idea of
sampling columns from a matrix X with probabilities that are derived from VT
k (as we do in Theorem
2) was introduced in [15] in order to construct coresets for regression problems by sampling data
points (rows of the matrix X) as opposed to features (columns of the matrix X). Other prior work
including [15  13  27  6  4] has employed variants of this sampling scheme; indeed  we borrow
proof techniques from the above papers in our work. Finally  we note that our deterministic feature
selection algorithm (Theorem 1) uses a sparsiﬁcation tool developed in [2] for column based matrix
reconstruction. This tool is a generalization of algorithms originally introduced in [1].

4

4 Our algorithms

Our algorithms emerge from the constructive proofs of Theorems 1 and 2. Both algorithms necessi-
tate access to the right singular vectors of X  namely the matrix Vk ∈ Rd×k. In our experiments  we
used PROPACK [23] in order to compute Vk iteratively; PROPACK is a fast alternative to the exact
SVD. Our ﬁrst algorithm (DSF-Select) is deterministic  while the second algorithm (RSF-Select)
is randomized  requiring logarithmically more columns to guarantee the theoretical bounds. Prior
to describing our algorithms in detail  we will introduce useful notation on sampling and rescaling
matrices as well as a matrix factorization lemma (Lemma 3) that will be critical in our proofs.

4.1 Sampling and rescaling matrices

Let C ∈ Rn×r contain r columns of X ∈ Rn×d. We can express the matrix C as C = XΩ  where
the sampling matrix Ω ∈ Rd×r is equal to [ei1   . . .   eir ] and ei are standard basis vectors in Rd. In
our proofs  we will make use of S ∈ Rr×r  a diagonal rescaling matrix with positive entries on the
diagonal. Our column selection algorithms return a sampling and a rescaling matrix  so that XΩS
contains a subset of rescaled columns from X. The rescaling is benign since it does not affect the
span of the columns of C = XΩ and thus the quantity of interest  namely ΠC k(Y).

4.2 A structural result using matrix factorizations

We now present a matrix reconstruction lemma that will be the starting point for our algorithms.
Let Y ∈ Rn×ω be a target matrix and let X ∈ Rn×d be the basis matrix that we will use in order
to reconstruct Y. More speciﬁcally  we seek a sparse reconstruction of Y from X  or  in other
words  we would like to choose r ≪ d columns from X and form a matrix C ∈ Rn×r such that
kY − ΠC k(Y)kF is small. Let Z ∈ Rd×k be an orthogonal matrix (i.e.  ZTZ = Ik)  and express the
matrix X as follows:

X = HZT + E 

where H is some matrix in Rn×k and E ∈ Rn×d is the residual error of the factorization. It is easy
to prove that the Frobenius or spectral norm of E is minimized when H = XZ. Let Ω ∈ Rd×r and
S ∈ Rr×r be a sampling and a rescaling matrix respectively as deﬁned in the previous section  and
let C = XΩ ∈ Rn×r. Then  the following lemma holds (see [3] for a detailed proof).
Lemma 3 (Generalized Column Reconstruction). Using the above notation  if the rank of the matrix
ZTΩS is equal to k  then

kY − ΠC k(Y)kF ≤ kY − HH+YkF + kEΩS(ZTΩS)+H+YkF .

(1)

We now parse the above lemma carefully in order to understand its implications in our setting. For
our goals  the matrix C essentially contains a subset of r features from the data matrix X. Recall that
ΠC k(Y) is the best rank-k approximation to Y within the column space of C; and  the difference
Y − ΠC k(Y) measures the error from performing regression using sparse eigenfeatures that are
constructed as linear combinations of the columns of C. Moving to the right-hand side of eqn. (1) 
the two terms reﬂect a tradeoff between the accuracy of the reconstruction of Y using H and the
error E in approximating X by the product HZT. Ideally  we would like to choose H so that Y can
be accurately approximated and  at the same time  the matrix X is approximated by the product HZT
with small residual error E. In general  these two goals might be competing and a balance must be
struck. Here  we focus on one extreme of this trade off  namely choosing Z so that the (Frobenius)
norm of the matrix E is minimized. More speciﬁcally  since Z has rank k  the best choice for HZT in
order to minimize kEkF is Xk; then  E = X − Xk. Using the SVD of Xk  namely Xk = UkΣkVT
k 
we apply Lemma 3 setting H = UkΣk and Z = Vk. The following corollary is immediate.
Lemma 4 (Generalization of Lemma 7 in [2]). Using the above notation  if the rank of the matrix
VT

kΩS is equal to k  then

kY − ΠC k(Y)kF ≤ kY − UkUT

kYkF + k(X − Xk)ΩS(VT

kΩS)+Σ−1

k UT

kYkF .

Our main results will follow by carefully choosing Ω and S in order to control the right-hand side of
the above inequality.

5

Algorithm: DSF-Select

1: Input: X  k  r.
2: Output: r columns of X in C.
3: Compute Vk and

E = X − Xk = X − XVkVT
k.

4: Run DetSampling to construct sam-

pling and rescaling matrices Ω and S:

Algorithm: DetSampling
1: Input: VT = [v1  . . .   vd]  A = [a1  . . .   ad]  r.
2: Output: Sampling and rescaling matrices [Ω  S].
3: Initialize B0 = 0k×k  Ω = 0d×r  and S = 0r×r.
4: for τ = 1 to r − 1 do
√rk.
5:
6:

Set Lτ = τ −
Pick index i ∈ {1  2  ...  n} and t such that
1
t ≤ L(vi  Bτ −1  Lτ ).

U (ai) ≤

[Ω  S] = DetSampling(VT

k  E  r).

5: Return C = XΩ.

Update Bτ = Bτ −1 + tvivT
i .
Set Ωiτ = 1 and Sτ τ = 1/√t.

7:
8:
9: end for
10: Return Ω and S.

Table 1: DSF-Select: Deterministic Sparse Feature Selection

4.3 DSF-Select: Deterministic Sparse Feature Selection

DSF-Select deterministically selects r columns of the matrix X to form the matrix C (see Table 1
and note that the matrix C = XΩ might contain duplicate columns which can be removed without
any loss in accuracy). The heart of DSF-Select is the subroutine DetSampling  a near-greedy
algorithm which selects columns of VT
k iteratively to satisfy two criteria: the selected columns should
form an approximately orthogonal basis for the columns of VT
kΩS)+ is well-behaved;
and EΩS should also be well-behaved. These two properties will allow us to prove our results via
Lemma 4. The implementation of the proposed algorithm is quite simple since it relies only on
standard linear algebraic operations.
DetSampling takes as input two matrices: VT ∈ Rk×d (satisfying VTV = Ik) and A ∈ Rn×d. In
order to describe the algorithm  it is convenient to view these two matrices as two sets of column
vectors  VT = [v1  . . .   vd] (satisfying Pd
i = Ik) and A = [a1  . . .   ad]. In DSF-Select
we set VT = VT
k and A = E = X − Xk. Given k and r  the algorithm iterates from τ = 0 up to
τ = r − 1 and its main operation is to compute the functions φ(L  B) and L(v  B  L) that are deﬁned
as follows:

k so that (VT

i=1 vivT

φ (L  B) =

k

Xi=1

1

λi − L

 

L (v  B  L) =

vT (B − (L + 1) Ik)−2
φ (L + 1  B) − φ (L  B) − vT (B − (L + 1) Ik)−1

v

v.

In the above  B ∈ Rk×k is a symmetric matrix with eigenvalues λ1  . . .   λk and L ∈ R is a parameter.
We also deﬁne the function U (a) for a vector a ∈ Rn as follows:
r! aTa
kAk2

U (a) = 1 −r k

F

.

At every step τ  the algorithm selects a column ai such that U (ai) ≤ L(vi  Bτ −1  Lτ ); note that
Bτ −1 is a k × k matrix which is also updated at every step of the algorithm (see Table 1). The
existence of such a column is guaranteed by results in [1  2].

It is worth noting that in practical implementations of the proposed algorithm  there might exist
multiple columns which satisfy the above requirement. In our implementation we chose to break
such ties arbitrarily. However  more careful and informed choices  such as breaking the ties in a way
that makes maximum progress towards our objective  might result in considerable savings. This is
indeed an interesting open problem.

The running time of our algorithm is dominated by the search for a column which satisﬁes
U (ai) ≤ L(vi  Bτ −1  Lτ ). To compute the function L  we ﬁrst need to compute φ(Lτ   Bτ −1) (which
necessitates the eigenvalues of Bτ −1) and then we need to compute the inverse of Bτ −1−(L + 1) Ik.
These computations need O(k3) time per iteration  for a total of O(rk3) time over all r iterations.
Now  in order to compute the function L for each vector vi for all i = 1  . . .   d  we need an additional

6

Algorithm: RSF-Select

1: Input: X  k  r.
2: Output: r columns of X in C.
3: Compute Vk.
4: Run RandSampling to construct sam-

pling and rescaling matrices Ω and S:

[Ω  S] = RandSampling(VT

k  r).

5: Return C = XΩ.

Algorithm: RandSampling
1: Input: VT = [v1  . . .   vd] and r.
2: Output: Sampling and rescaling matrices [Ω  S].
3: For i = 1  ...  d compute probabilities

pi =

1
kkvik2
2.

4: Initialize Ω = 0d×r and S = 0r×r.
5: for τ = 1 to r do
6:

Select an index iτ ∈ {1  2  ...  d} where the
probability of selecting index i is equal to pi.
Set Ωiτ τ = 1 and Sτ τ = 1/√rpiτ .

7:
8: end for
9: Return Ω and S.

Table 2: RSF-Select: Randomized Sparse Feature Selection

O(dk2) time per iteration; the total time for all r iterations is O(drk2). Next  in order to compute
the function U   we need to compute aT
i ai (for all i = 1  . . .   d) which necessitates O(nnz(A)) time 
where nnz(A) is the number of non-zero elements of A. In our setting  A = E ∈ Rn×d  so the
overall running time is O(drk2 + nd). In order to get the ﬁnal running time we also need to account
for the computation of Vk and E.
The theoretical properties of DetSampling were analyzed in detail in [2]  building on the original
analysis of [1]. The following lemma from [2] summarizes important properties of Ω.
Lemma 5 ([2]). DetSampling with inputs VT and A returns a sampling matrix Ω ∈ Rd×r and a
rescaling matrix S ∈ Rr×r satisfying

k(VTΩS)+k2 ≤ 1 −r k

r

;

kAΩSkF ≤ kAkF .

We apply Lemma 5 with V = VT
proof of Theorem 1; see [3] for details.

k and A = E and we combine it with Lemma 4 to conclude the

4.4 RSF-Select: Randomized Sparse Feature Selection

RSF-Select is a randomized algorithm that selects r columns of the matrix X in order to form the
matrix C (see Table 2). The main differences between RSF-Select and DSF-Select are two: ﬁrst 
RSF-Select only needs access to VT
k and  second  RSF-Select uses a simple sampling procedure in
order to select the columns of X to include in C. This sampling procedure is described in algorithm
RandSampling and essentially selects columns of X with probabilities that depend on the norms of
k. Thus  RandSampling ﬁrst computes a set of probabilities that are proportional
the columns of VT
to the norms of the columns of VT
k and then samples r columns of X in r independent identical trials
with replacement  where in each trial a column is sampled according to the computed probabilities.
Note that a column could be selected multiple times. In terms of running time  and assuming that
the matrix Vk that contains the top k right singular vectors of X has already been computed  the
proposed algorithm needs O(dk) time to compute the sampling probabilities and an additional O(d+
r log r) time to sample r columns from X. Similar to Lemma 5  we can prove analogous properties
for the matrices Ω and S that are returned by algorithm RandSampling. Again  combining with
Lemma 4 we can prove Theorem 2; see [3] for details.

5 Experiments

The goal of our experiments is to illustrate that our algorithms produce sparse features which per-
form as well in-sample as the top-k PCA regression. It turns out that the out-of-sample performance
is comparable (if not better in many cases  perhaps due to the sparsity) to top-k PCA-regression.

7

Data

(n; d)

k = 5  r = k + 1

Arcene

I-sphere

LibrasMov

Madelon

HillVal

Spambase

(100;10 000)

(351;34)

(45;90)

(2 000;500)

(606;100)

(4601;57)

w∗
k

0.93
0.99
0.57
0.58
2.9
3.3
0.98
0.98
0.68
0.68
0.30
0.30

k

ˆwDSF
0.88
0.94
0.52
0.53
2.9
3.6
0.98
0.98
0.66
0.67
0.30
0.30

k

ˆwRSF
0.91
0.98
0.55
0.57
3.1
3.7
0.98
0.98
0.67
0.68
0.31
0.30

k

ˆwrnd
1.0
1.0
0.57
0.57
3.7
3.7
1.0
1.0
0.68
0.68
0.28
0.38

w∗
k

0.93
1.0
0.57
0.58
2.9
3.3
0.98
0.98
0.68
0.68
0.3
0.3

k

k

k = 5  r = 2k
ˆwDSF
ˆwRSF
0.86
0.89
0.98
0.97
0.52
0.51
0.54
0.55
2.6
2.4
3.6
3.3
0.97
0.97
0.98
0.98
0.65
0.67
0.69
0.67
0.3
0.3
0.3
0.3

k

ˆwrnd
1.0
1.0
0.56
0.56
3.6
3.6
1.0
1.0
0.69
0.69
0.25
0.35

Table 3: Comparison of DSF-select and RSF-select with top-k PCA. The top entry in each cell
is the in-sample error  and the bottom entry is the out-sample error. In bold is the method achieving
the best out-sample error.

Compared to top-k PCA  our algorithms are efﬁcient and work well in practice  even better than the
theoretical bounds suggest.

We present our ﬁndings in Table 3 using data sets from the UCI machine learning repository. We
used a ﬁve-fold cross validation design with 1 000 random splits: we computed regression weights
using 80% of the data and estimated out-sample error in the remaining 20% of the data. We set k = 5
in the experiments (no attempt was made to optimize k). Table 3 shows the in- and out-sample error
k; r-sparse features regression using DSF-select  ˆwDSF
for four methods: top-k PCA regression  w∗
;
r-sparse features regression using RSF-select  ˆwRSF
; r-sparse features regression using r random
columns  ˆwrnd
k .

k

k

6 Discussion

The top-k PCA regression constructs “features” without looking at the targets – it is target-agnostic.
So are all the algorithms we discussed here  as our goal was to compare with top-k PCA. However 
there is unexplored potential in Lemma 3. We only explored one extreme choice for the factorization 
namely the minimization of some norm of the matrix E. Other choices  in particular non-target-
agnostic choices  could prove considerably better. Such investigations are left for future work.

As mentioned when we discussed our deterministic algorithm  it will often be the case that in some
steps of the greedy selection process  multiple columns could satisfy the criterion for selection. In
such a situation  we are free to choose any one; we broke ties arbitrarily in our implementation 
and even as is  the algorithm performed as well or better than top-k PCA. However  we expect that
breaking the ties so as to optimize the ultimate objective would yield considerable additional beneﬁt;
this would also be non-target-agnostic.

Acknowledgments

This work has been supported by two NSF CCF and DMS grants to Petros Drineas and Malik
Magdon-Ismail.

References
[1] J. Batson  D. Spielman  and N. Srivastava. Twice-ramanujan sparsiﬁers. In Proceedings of ACM STOC 

pages 255–262  2009.

[2] C. Boutsidis  P. Drineas  and M. Magdon-Ismail. Near-optimal column based matrix reconstruction. In

Proceedings of IEEE FOCS  2011.

[3] C. Boutsidis  P. Drineas  and M. Magdon-Ismail.

manuscript  2011.

Sparse features for PCA-like linear regression.

[4] C. Boutsidis and M. Magdon-Ismail.

arXiv:1109.5664v1  2011.

Deterministic feature selection for k-means clustering.

8

[5] C. Boutsidis  M. W. Mahoney  and P. Drineas. An improved approximation algorithm for the column

subset selection problem. In Proceedings of ACM -SIAM SODA  pages 968–977  2009.

[6] C. Boutsidis  M. W. Mahoney  and P. Drineas. Unsupervised feature selection for the k-means clustering

problem. In Proceedings of NIPS  2009.

[7] J. Cadima and I. Jolliffe. Loadings and correlations in the interpretation of principal components. Applied

Statistics  22:203–214  1995.

[8] T. Chan and P. Hansen. Some applications of the rank revealing QR factorization. SIAM Journal on

Scientiﬁc and Statistical Computing  13:727–741  1992.

[9] A. Das and D. Kempe. Algorithms for subset selection in linear regression.

STOC  2008.

In Proceedings of ACM

[10] A. Dasgupta  P. Drineas  B. Harb  R. Kumar  and M. W. Mahoney. Sampling algorithms and coresets for

Lp regression. In Proceedings of ACM-SIAM SODA  2008.

[11] A. d’Aspremont  L. El Ghaoui  M. I. Jordan  and G. R. G. Lanckriet. A direct formulation for sparse PCA

using semideﬁnite programming. In Proceedings of NIPS  2004.

[12] A. Deshpande and L. Rademacher. Efﬁcient volume sampling for row/column subset selection. In Pro-

ceedings of ACM STOC  2010.

[13] P. Drineas  R. Kannan  and M. Mahoney. Fast Monte Carlo algorithms for matrices I: Approximating

matrix multiplication. SIAM Journal of Computing  36(1):132–157  2006.

[14] P. Drineas  M. Mahoney  and S. Muthukrishnan. Polynomial time algorithm for column-row based

relative-error low-rank matrix approximation. Technical Report 2006-04  DIMACS  March 2006.

[15] P. Drineas  M. Mahoney  and S. Muthukrishnan. Sampling algorithms for ℓ2 regression and applications.

In Proceedings of ACM-SIAM SODA  pages 1127–1136  2006.

[16] G. Golub. Numerical methods for solving linear least squares problems. Numerische Mathematik  7:206–

216  1965.

[17] G. Golub  P. Hansen  and D. O’Leary. Tikhonov regularization and total least squares. SIAM Journal on

Matrix Analysis and Applications  21(1):185–194  2000.

[18] M. Gu and S. Eisenstat. Efﬁcient algorithms for computing a strong rank-revealing QR factorization.

SIAM Journal on Scientiﬁc Computing  17:848–869  1996.

[19] I. Guyon and A. Elisseeff. Special issue on variable and feature selection. Journal of Machine Learning

Research  3  2003.

[20] N. Halko  P. Martinsson  and J. Tropp. Finding structure with randomness: probabilistic algorithms for

constructing approximate matrix decompositions. SIAM Review  2011.

[21] P. Hansen. The truncated SVD as a method for regularization. BIT Numerical Mathematics  27(4):534–

553  1987.

[22] I. Jolliffe. Discarding variables in Principal Component Analysis: asrtiﬁcial data. Applied Statistics 

21(2):160–173  1972.

[23] R. Larsen.

PROPACK: A software package for the symmetric eigenvalue problem and sin-
gular value problems on Lanczos and Lanczos bidiagonalization with partial reorthogonalization.
http://soi.stanford.edu/∼rmunk/∼PROPACK/.
In Proceedings of NIPS  2005.

[24] B. Moghaddam  Y. Weiss  and S. Avidan. Spectral bounds for sparse PCA: exact and greedy algorithms.

[25] B. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing  24(2):227–

234  1995.

[26] M. Rudelson and R. Vershynin. Sampling from large matrices: An approach through geometric functional

analysis. Journal of the ACM  54  2007.

[27] N. Srivastava and D. Spielman. Graph sparsiﬁcations by effective resistances. In Proceedings of ACM

STOC  pages 563–568  2008.

[28] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

pages 267–288  1996.

[29] J. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information

Theory  50(10):2231–2242  2004.

[30] T. Zhang. Generating a d-dimensional linear subspace efﬁciently. In Adaptive forward-backward greedy

algorithm for sparse learning with linear models  2008.

9

,Aaron van den Oord
Sander Dieleman
Benjamin Schrauwen
Qiang Liu
Dilin Wang
Cedric Josz
Yi Ouyang
Richard Zhang
Javad Lavaei
Somayeh Sojoudi