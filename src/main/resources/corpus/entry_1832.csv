2018,Entropy Rate Estimation for Markov Chains with Large State Space,Entropy estimation is one of the prototypical problems in distribution property testing. To consistently estimate the Shannon entropy of a distribution on $S$ elements with independent samples  the optimal sample complexity scales sublinearly with $S$ as $\Theta(\frac{S}{\log S})$ as shown by Valiant and Valiant \cite{Valiant--Valiant2011}. Extending the theory and algorithms for entropy estimation to dependent data  this paper considers the problem of estimating the entropy rate of a stationary reversible Markov chain with $S$ states from a sample path of $n$ observations. We show that
\begin{itemize}
	\item Provided the Markov chain mixes not too slowly  \textit{i.e.}  the relaxation time is at most $O(\frac{S}{\ln^3 S})$  consistent estimation is achievable when $n \gg \frac{S^2}{\log S}$.
	\item Provided the Markov chain has some slight dependency  \textit{i.e.}  the relaxation time is at least $1+\Omega(\frac{\ln^2 S}{\sqrt{S}})$  consistent estimation is impossible when $n \lesssim \frac{S^2}{\log S}$.
\end{itemize}
Under both assumptions  the optimal estimation accuracy is shown to be $\Theta(\frac{S^2}{n \log S})$. In comparison  the empirical entropy rate requires at least $\Omega(S^2)$ samples to be consistent  even when the Markov chain is memoryless. In addition to synthetic experiments  we also apply the estimators that achieve the optimal sample complexity to estimate the entropy rate of the English language in the Penn Treebank and the Google One Billion Words corpora  which provides a natural benchmark for language modeling and relates it directly to the widely used perplexity measure.,Entropy Rate Estimation for Markov Chains with

Large State Space

Yanjun Han

Department of Electrical Engineering

Stanford University
Stanford  CA 94305

yjhan@stanford.edu

Department of Electrical Engineering and Computer Sciences

Jiantao Jiao

University of California  Berkeley

Berkeley  CA 94720

jiantao@berkeley.edu

Chuan-Zheng Lee  Tsachy Weissman
Department of Electrical Engineering

Stanford University
Stanford  CA 94305

{czlee  tsachy}@stanford.edu

Yihong Wu

Department of Statistics and Data Science

Yale University

New Haven  CT 06511
yihong.wu@yale.edu

Tiancheng Yu

Department of Electronic Engineering

Tsinghua University

Haidian  Beijing 100084

thueeyutc14@foxmail.com

Abstract

Entropy estimation is one of the prototypical problems in distribution property
testing. To consistently estimate the Shannon entropy of a distribution on S ele-
ments with independent samples  the optimal sample complexity scales sublinearly
with S as Θ( S
log S ) as shown by Valiant and Valiant [4]. Extending the theory
and algorithms for entropy estimation to dependent data  this paper considers the
problem of estimating the entropy rate of a stationary reversible Markov chain with
S states from a sample path of n observations. We show that

• Provided the Markov chain mixes not too slowly  i.e.  the relaxation time is at

ln3 S )  consistent estimation is achievable when n (cid:29) S2
log S .

• Provided the Markov chain has some slight dependency  i.e.  the relaxation
)  consistent estimation is impossible when n (cid:46)

most O( S

time is at least 1 + Ω( ln2 S√
S2
log S .

S

Under both assumptions  the optimal estimation accuracy is shown to be Θ( S2
n log S ).
In comparison  the empirical entropy rate requires at least Ω(S2) samples to be
consistent  even when the Markov chain is memoryless. In addition to synthetic
experiments  we also apply the estimators that achieve the optimal sample complex-

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

ity to estimate the entropy rate of the English language in the Penn Treebank and
the Google One Billion Words corpora  which provides a natural benchmark for
language modeling and relates it directly to the widely used perplexity measure.

Introduction

1
Consider a stationary stochastic process {Xt}∞
of size S. The Shannon entropy rate (or simply entropy rate) of this process is deﬁned as [1]

t=1  where each Xt takes values in a ﬁnite alphabet X

¯H = lim
n→∞

1
n

H(X n) 

(1)

where

H(X n) =

PX n (xn) ln

1

PX n (xn)

(cid:88)

xn∈X n

is the Shannon entropy (or entropy) of the random vector X n = (X1  X2  . . .   Xn) and PX n (xn) =
P (X1 = x1  . . .   Xn = xn) is the joint probability mass function. Since the entropy of a random
variable depends only on its distribution  we also refer to the entropy H(P ) of a discrete distribution

P = (p1  p2  . . .   pS)  deﬁned as H(P ) =(cid:80)S

.

i=1 pi ln 1
pi

The Shannon entropy rate is the fundamental limit of the expected logarithmic loss when predicting
the next symbol  given the all past symbols. It is also the fundamental limit of data compressing
for stationary stochastic processes in terms of the average number of bits required to represent each
symbol [1  7]. Estimating the entropy rate of a stochastic process is a fundamental problem in
information theory  statistics  and machine learning; and it has diverse applications—see  for example 
[3  2  3  3  4  2].
There exists extensive literature on entropy rate estimation. It is known from data compression theory
that the normalized codelength of any universal code is a consistent estimator for the entropy rate as
the number of samples approaches inﬁnity. This observation has inspired a large variety of entropy
rate estimators; see e.g. [4  2  1  6  1]. However  most of this work has been in the asymptotic regime
[3  8]. Attention to non-asymptotic analysis has only been more recent  and to date  almost only
for i.i.d. data. There has been little work on the non-asymptotic performance of an entropy rate
estimator for dependent data—that is  where the alphabet size is large (making asymptotically large
datasets infeasible) and the stochastic process has memory. An understanding of this large-alphabet
regime is increasingly important in modern machine learning applications  in particular  language
modeling. There have been substantial recent advances in probabilistic language models  which have
been widely used in applications such as machine translation and search query completion. The
entropy rate of (say) the English language represents a fundamental limit on the efﬁcacy of a language
model (measured by its perplexity)  so it is of great interest to language model researchers to obtain
an accurate estimate of the entropy rate as a benchmark. However  since the alphabet size here is
exceedingly large  and Google’s One Billion Words corpus includes about two million unique words 1
it is unrealistic to assume the large-sample asymptotics especially when dealing with combinations of
words (bigrams  trigrams  etc). It is therefore of signiﬁcant practical importance to investigate the
optimal entropy rate estimator with limited sample size.
In the context of non-asymptotic analysis for i.i.d. samples  Paninski [3] ﬁrst showed that the Shannon
entropy can be consistently estimated with o(S) samples when the alphabet size S approaches inﬁnity.
The seminal work of [4] showed that when estimating the entropy rate of an i.i.d. source  n (cid:29) S
log S
samples are necessary and sufﬁcient for consistency. The entropy estimators proposed in [4] and
reﬁned in [4]  based on linear programming  have not been shown to achieve the minimax estimation
rate. Another estimator proposed by the same authors [4] has been shown to achieve the minimax
rate in the restrictive regime of S
ln S . Using the idea of best polynomial approximation 
ln S
the independent work of [4] and [1] obtained estimators that achieve the minimax mean-square error
n log S )2 + log2 S
log S ) sample complexity in the
Θ((
1This exceeds the estimated vocabulary of the English language partly because different forms of a word
count as different words in language models  and partly because of edge cases in tokenization  the automatic
splitting of text into “words”.

n ) for entropy estimation. The intuition for the Θ( S

(cid:46) n (cid:46) S1.03

S

2

independent case can be interpreted as follows: as opposed to estimating the entire distribution which
has S − 1 parameters and requires Θ(S) samples  estimating the scalar functional (entropy) can be
done with a logarithmic factor reduction of samples. For Markov chains which are characterized by
the transition matrix consisting of S(S − 1) free parameters  it is reasonable to expect an Θ( S2
log S )
sample complexity. Indeed  we will show that this is correct provided the mixing is not too slow.
Estimating the entropy rate of a Markov chain falls in the general area of property testing and
estimation with dependent data. The prior work [2] provided a non-asymptotic analysis of maximum-
likelihood estimation of entropy rate in Markov chains and showed that it is necessary to assume
certain assumptions on the mixing time for otherwise the entropy rate is impossible to estimate. There
has been some progress in related questions of estimating the mixing time from sample path [1  2] 
estimating the transition matrix [1]  and testing symmetric Markov chains [1]. The current paper
makes contribution to this growing ﬁeld. In particular  the main results of this paper are highlighted
as follows:

• We provide a tight analysis of the sample complexity of the empirical entropy rate for
Markov chains when the mixing time is not too large. This reﬁnes results in [2] and shows
that when mixing is not too slow  the sample complexity of the empirical entropy does
not depend on the mixing time. Precisely  the bias of the empirical entropy rate vanishes
uniformly over all Markov chains regardless of mixing time and reversibility as long as the
number of samples grows faster than the number of parameters. It is its variance that may
explode when the mixing time becomes gigantic.

• We obtain a characterization of the optimal sample complexity for estimating the entropy
rate of a stationary reversible Markov chain in terms of the sample size  state space size 
and mixing time  and partially resolve one of the open questions raised in [2]. In particular 
we show that when the mixing is neither too fast nor too slow  the sample complexity
(up to a constant) does not depend on mixing time. In this regime  the performance of the
optimal estimator with n samples is essentially that of the empirical entropy rate with n log n
samples. As opposed to the lower bound for estimating the mixing time in [1] obtained by
applying Le Cam’s method to two Markov chains which are statistically indistinguishable 
the minimax lower bound in the current paper is much more involved  which  in addition to a
series of reductions by means of simulation  relies on constructing two stationary reversible
Markov chains with random transition matrices [4]  so that the marginal distributions of the
sample paths are statistically indistinguishable.

• We construct estimators that are efﬁciently computable and achieve the minimax sample
complexity. The key step is to connect the entropy rate estimation problem to Shannon
entropy estimation on large alphabets with i.i.d. samples. The analysis uses the idea of
simulating Markov chains from independent samples by Billingsley [3] and concentration
inequalities for Markov chains.

• We compare the empirical performance of various estimators for entropy rate on a vari-
ety of synthetic data sets  and demonstrate the superior performances of the information-
theoretically optimal estimators compared to the empirical entropy rate.

• We apply the information-theoretically optimal estimators to estimate the entropy rate of the
Penn Treebank (PTB) and the Google One Billion Words (1BW) datasets. We show that
even only with estimates using up to 4-grams  there may exist language models that achieve
better perplexity than the current state-of-the-art.

The rest of the paper is organized as follows. After setting up preliminary deﬁnitions in Section 2 
we summarize our main ﬁndings in Section 3  with proofs sketched in Section 4. Section 5 provides
empirical results on estimating the entropy rate of the Penn Treebank (PTB) and the Google One
Billion Words (1BW) datasets. Detailed proofs and more experiments are deferred to the appendices.

2 Preliminaries
Consider a ﬁrst-order Markov chain X0  X1  X2  . . . on a ﬁnite state space X = [S] with transition
kernel T . We denote the entries of T as Tij  that is  Tij = PX2|X1(j|i) for i  j ∈ X . Let Ti denote
the ith row of T   which is the conditional law of X2 given X1 = i. Throughout the paper  we focus

3

on ﬁrst-order Markov chains  since any ﬁnite-order Markov chain can be converted to a ﬁrst-order
one by extending the state space [3].
We say that a Markov chain is stationary if the distribution of X1  denoted by π (cid:44) PX1  satisﬁes
πT = π. We say that a Markov chain is reversible if it satisﬁes the detailed balance equations 
πiTij = πjTji for all i  j ∈ X . If a Markov chain is reversible  the (left) spectrum of its transition
matrix T contains S real eigenvalues  which we denote as 1 = λ1 ≥ λ2 ≥ ··· ≥ λS ≥ −1. We deﬁne
the spectral gap and the absolute spectral gap of T as γ(T ) = 1 − λ2 and γ∗(T ) = 1 − maxi≥2 |λi| 
respectively  and the relaxation time of a reversible Markov chain as

τrel(T ) =

1

γ∗(T )

.

(2)

The relaxation time of a reversible Markov chain (approximately) captures its mixing time  which
roughly speaking is the smallest n for which the marginal distribution of Xn is close to the Markov
chain’s stationary distribution. We refer to [3] for a survey.
We consider the following observation model. We observe a sample path of a stationary ﬁnite-state
Markov chain X0  X1  . . .   Xn  whose Shannon entropy rate ¯H in (1) reduces to

Tij ln

1
Tij

= H(X1  X2) − H(X1)

(3)

where π is the stationary distribution of this Markov chain. Let M2(S) be the set of transition
matrices of all stationary Markov chains on a state space of size S. Let M2 rev(S) be the set of
transition matrices of all stationary reversible Markov chains on a state space of size S. We deﬁne
the following class of stationary Markov reversible chains whose relaxation time is at most 1
γ∗ :

M2 rev(S  γ∗) = {T ∈ M2 rev(S)  γ∗(T ) ≥ γ∗}.

(4)
The goal is to characterize the sample complexity of entropy rate estimation as a function of S  γ∗ 
and the estimation accuracy.
Note that the entropy rate of a ﬁrst-order Markov chain can be written as

S(cid:88)

S(cid:88)

¯H =

πi

i=1

j=1

S(cid:88)

i=1

¯H =

πiH(X2|X1 = i).

(5)

Given a sample path X = (X0  X1  . . .   Xn)  let ˆπ denote the empirical distribution of states  and
the subsequence of X containing elements following any occurrence of the state i as X(i) = {Xj :
Xj ∈ X  Xj−1 = i  j ∈ [n]}. A natural idea to estimate the entropy rate ¯H is to use ˆπi to estimate πi
and an appropriate Shannon entropy estimator to estimate H(X2|X1 = i). We deﬁne two estimators:

1. The empirical entropy rate: ¯Hemp =(cid:80)S
2. Our entropy rate estimator: ¯Hopt = (cid:80)S

(cid:0)X(i)(cid:1). Note that ˆHemp(Y) computes
(cid:0)X(i)(cid:1)  where ˆHopt is any minimax

the Shannon entropy of the empirical distribution of its argument Y = (Y1  Y2  . . .   Ym).

i=1 ˆπi ˆHemp

i=1 ˆπi ˆHopt

rate-optimal Shannon entropy estimator designed for i.i.d. data  such as those in [4  4  1].

3 Main results

Our ﬁrst result provides performance guarantees for the empirical entropy rate ¯Hemp and our entropy
rate estimator ¯Hopt:
Theorem 1. Suppose (X0  X1  . . .   Xn) is a sample path from a stationary reversible Markov chain
with spectral gap γ. If S0.01 (cid:46) n (cid:46) S2.99 and 1
n ln n ln3 S   there exists some constant
C > 0 independent of n  S  γ such that the entropy rate estimator ¯Hopt satisﬁes:2 as S → ∞ 

(cid:46)

S3

γ

| ¯Hopt − ¯H| ≤ C

P

→ 1

(6)

(cid:18)

S

ln n ln2 S ∧
(cid:19)

S2

n ln S

2The asymptotic results in this section are interpreted by parameterizing n = nS and γ = γS and S → ∞

subject to the conditions of each theorem.

4

Under the same conditions  there exists some constant C(cid:48) > 0 independent of n  S  γ such that the
empirical entropy rate ¯Hemp satisﬁes: as S → ∞ 

(cid:18)

(cid:19)

P

| ¯Hemp − ¯H| ≤ C(cid:48) S2
n

→ 1.

(7)

nγ )  our dominating term O( S2

Theorem 1 shows that when the sample size is not too large  and the mixing is not too slow  it sufﬁces
to take n (cid:29) S2
ln S for the estimator ¯Hopt to achieve a vanishing error  and n (cid:29) S2 for the empirical
entropy rate. Theorem 1 improves over [2] in the analysis of the empirical entropy rate in the sense
that unlike the error term O( S2
n ) does not depend on the mixing time.
Note that we have made mixing time assumptions in the upper bound analysis of the empirical
entropy rate in Theorem 1  which is natural since [2] showed that it is necessary to impose mixing
time assumptions to provide meaningful statistical guarantees for entropy rate estimation in Markov
chains. The following result shows that mixing assumptions are only needed to control the variance of
the empirical entropy rate: the bias of the empirical entropy rate vanishes uniformly over all Markov
chains regardless of reversibility and mixing time assumptions as long as n (cid:29) S2.
Theorem 2. Let n  S ≥ 1. Then 

(cid:16) n

(cid:17)

sup

T∈M2(S)

| ¯H − E[ ¯Hemp]| ≤ 2S2
n

ln

S2 + 1

+

(S2 + 2) ln 2

n

.

(8)

Theorem 2 implies that if n (cid:29) S2  the bias of the empirical entropy rate estimator universally
vanishes for any stationary Markov chains.
Now we turn to the lower bounds  which show that the scalings in Theorem 1 are in fact tight. The
next result shows that the bias of the empirical entropy rate ¯Hemp is non-vanishing unless n (cid:29) S2 
even when the data are independent.
Theorem 3. If {X0  X1  . . .   Xn} are mutually independent and uniformly distributed  then

(cid:18)

(cid:19)

| ¯H − E[ ¯Hemp]| ≥ ln

S2

n + S − 1

.

(9)

The following corollary is immediate.
Corollary 1. There exists a universal constant c > 0 such that when n ≤ cS2  the absolute value of
the bias of ¯Hemp is bounded away from zero even if the Markov chain is memoryless.

The next theorem presents a minimax lower bound for entropy rate estimation which applies to any
estimation scheme regardless of its computational cost. In particular  it shows that ¯Hopt is minimax
rate-optimal under mild assumptions on the mixing time.
(ln S)2   γ∗ ≤ 1 − C2
Theorem 4. For n ≥ S2

ln S   ln n (cid:28) S

(cid:113)

S ln3 S

  we have

n

(cid:18)

(cid:19)

lim inf
S→∞ inf
ˆH

sup

T∈M2 rev(S γ∗)

P

| ˆH − ¯H| ≥ C1

S2

n ln S

≥ 1
2

.

(10)

Here C1  C2 are universal constants from Theorem 6.

The following corollary  which follows from Theorem 1 and 4  presents the critical scaling that
determines whether consistent estimation of the entropy rate is possible.
Corollary 2. If ln3 S
rate with a uniformly vanishing error over Markov chains M2 rev(S  γ∗) if and only if n (cid:29) S2
ln S .
To conclude this section we summarize our result in terms of the sample complexity for estimating
the entropy rate within a few bits ( = Θ(1))  classiﬁed according to the relaxation time:

  there exists an estimator ˆH which estimates the entropy

S (cid:28) γ∗ ≤ 1 − C2

ln2 S√
S

• τrel = 1: this is the i.i.d. case and the sample complexity is Θ( S

ln S );

5

• 1 < τrel (cid:28) 1 + Ω( ln2 S√

S

and no matching lower bound is known;

): in this narrow regime the sample complexity is at most O( S2

ln S )

) ≤ τrel (cid:28) S

ln3 S : the sample complexity is Θ( S2

ln S );

ln3 S : the sample complexity is Ω( S2

ln S ) and no matching upper bound is known. In

this case the chain mixes very slowly and it is likely that the variance will dominate.

• 1 + Ω( ln2 S√
• τrel (cid:38) S

S

4 Sketch of the proof

In this section we sketch the proof of Theorems 1  2 and 4  and defer the details to the appendix.

4.1 Proof of Theorem 1

A key step in the analysis of ¯Hemp and ¯Hopt is the idea of simulating a ﬁnite-state Markov chain from
independent samples [3  p. 19]: consider an independent collection of random variables X0 and Win
(i = 1  2  . . .   S; n = 1  2  . . .) such that PX0(i) = πi  PWin (j) = Tij. Imagine the variables Win
set out in the following array:

W11 W12
W21 W22
...
...
WS1 WS2

. . . W1n
. . . W2n
...
...
. . . WSn

. . .
. . .

...

. . .

First  X0 is sampled. If X0 = i  then the ﬁrst variable in the ith row of the array is sampled 
and the result is assigned by deﬁnition to X1. If X1 = j  then the ﬁrst variable in the jth row
is sampled  unless j = i  in which case the second variable is sampled. In any case  the result
of the sampling is by deﬁnition X2. The next variable sampled is the ﬁrst one in row X2 which
has not yet been sampled. This process thus continues. After collecting {X0  X1  . . .   Xn} from
the model  we assume that the last variable sampled from row i is Wini. It can be shown that
observing a Markov chain {X0  X1  . . .   Xn} is equivalent to observing {X0 {Wij}i∈[S] j∈[ni]} 
and consequently ˆπi = ni/n  X(i) = (Wi1  . . .   Wini).
The main reason to introduce the above framework is to analyze ˆHemp(X(i)) and ˆHopt(X(i)) as if the
argument X(i) is an i.i.d. vector. Speciﬁcally  although Wi1 ···   Wim conditioned on ni = m are
not i.i.d.  they are i.i.d. as Ti for any ﬁxed m. Hence  using the fact that each ni concentrates around
nπi (cf. Deﬁnition 2 and Lemma 4 for details)  we may use the concentration properties of ˆHemp and
ˆHopt (cf. Lemma 3) on i.i.d. data for each ﬁxed m ≈ nπi and apply the union bound in the end.
Based on this alternative view  we have the following theorem  which implies Theorem 1.
Theorem 5. Suppose (X0  X1  . . .   Xn) comes from a stationary reversible Markov chain with
spectral gap γ. Then  with probability tending to one  the entropy rate estimators satisfy

(cid:18) S
(cid:19)0.495
(cid:19)0.495
(cid:18) S

+

n

+

n

+

S ln S
n0.999 +

S ln S ln n

nγ

S ln S
n0.999 +

S ln S ln n

nγ

+

+

(cid:115)

(cid:115)

S ln n ln2 S

nγ

 

(11)

S ln n ln2 S

nγ

.

(12)

| ¯Hopt − ¯H| (cid:46) S2
n ln S

| ¯Hemp − ¯H| (cid:46) S2
n

+

4.2 Proof of Theorem 2

By the concavity of entropy  the empirical entropy rate ¯Hemp underestimates the truth ¯H in expectation.
On the other hand  the average codelength ¯L of any lossless source code is at least ¯H by Shannon’s
source coding theorem. As a result  ¯H − E[ ¯Hemp] ≤ ¯L − E[ ¯Hemp]  and we may ﬁnd a good lossless
code to make the RHS small.

6

Since the conditional empirical distributions maximizes the likelihood for Markov chains (Lemma 13) 
we have

(cid:34)

(cid:35)

(cid:34)
(cid:34)

≥ EP

≥ EP

(cid:35)

1

= ¯H

(cid:35)

EP

1
n

ln

1

QX n

1 |X0(X n

1 |X0)

1
n

ln

1

PX n

1 |X0 (X n

1 |X0)

min

P∈M2(S)

1
n

ln

PX n

1 |X0 (X n

1 |X0)

= E[ ¯Hemp]

where M2(S) denotes the space of all ﬁrst-order Markov chains with state [S]. Hence 

| ¯H − E[ ¯Hemp]| ≤ inf

Q

sup

P∈M2(S) xn

0

1
n

ln

P (xn
Q(xn

1|x0)
1|x0)

.

(13)

(14)

(15)

The following lemma provides a non-asymptotic upper bound on the RHS of (15) and completes the
proof of Theorem 2.
Lemma 1. [3] Let M2(S) denote the space of Markov chains with alphabet size S for each symbol.
Then  the worst case minimax redundancy is bounded as

(cid:16) n

(cid:17)

P (xn
Q(xn

1|x0)
1|x0)

≤ 2S2
n

ln

S2 + 1

+

(S2 + 2) ln 2

n

.

(16)

inf
Q

sup

P∈M2(S) xn

0

1
n

ln

4.3 Proof of Theorem 4

To prove the lower bound for Markov chains  we ﬁrst introduce an auxiliary model  namely  the
independent Poisson model and show that the sample complexity of the Markov chain model is lower
bounded by that of the independent Poisson model. Then we apply the so-called method of fuzzy
hypotheses [4  Theorem 2.15] (see also [1  Lemma 11]) to prove a lower bound for the independent
Poisson model. We introduce the independent Poisson model below  which is parametrized by an
S × S symmetric matrix R  an integer n and a parameter λ > 0.
Deﬁnition 1 (Independent Poisson model). Given an S × S symmetric matrix R = (Rij) with
Rij ≥ 0 and a parameter λ > 0  under the independent Poisson model  we observe X0 ∼ π = π(R) 
and an S × S matrix C = (Cij) with independent entries distributed as Cij ∼ Poi (λRij)  where

S(cid:88)

S(cid:88)

πi = πi(R) =

ri
r

 

ri =

Rij 

r =

ri.

(17)

j=1

i=1

For each symmetric matrix R  by normalizing the rows we can deﬁne a transition matrix T = T (R)
of a reversible Markov chain with stationary distribution π = π(R). Upon observing the Poisson
matrix C  the functional to be estimated is the entropy rate ¯H of T (R). Given τ > 0 and γ  q ∈ (0  1) 
deﬁne the following collection of symmetric matrices:

R(S  γ  τ  q) =

R ∈ RS×S

+

: R = R(cid:62)  γ∗(T ) ≥ γ 

Rij ≥ τ  πmin ≥ q

 

(18)

(cid:40)

(cid:41)

(cid:88)

i j

where πmin = mini πi. The reduction to independent Poisson model is summarized below:
Lemma 2. If there exists an estimator ˆH1 for the Markov chain model with parameter n such that
P(| ˆH1 − ¯H| ≥ ) ≤ δ under any T ∈ M2 rev(S  γ)  then there exists another estimator ˆH2 for the
independent Poisson model with parameter λ = 4n

P(cid:16)| ˆH2 − ¯H(T (R))| ≥ 

τ such that

(cid:17) ≤ δ + 2Sn

sup

provided q ≥ c3 ln n

R∈R(S γ τ q)
nγ   where c3 ≥ 20 is a universal constant.

− c2
4+10c3 + Sn−c3/2 

3

(19)

To prove the lower bound for the independent Poisson model  the goal is to construct two sym-
metric random matrices (whose distributions serve as the priors)  such that (a) they are sufﬁ-
ciently concentrated near the desired parameter space R(S  γ  τ  q) for properly chosen parameters

7

γ  τ  q; (b) their entropy rates are separated; (c) the induced marginal laws of the sufﬁcient statistic
C = X0∪{Cij +Cji : i (cid:54)= j  1 ≤ i ≤ j ≤ S}∪{Cii : 1 ≤ i ≤ S} are statistically indistinguishable.
The prior construction in Deﬁnition 4 satisﬁes all these three properties (cf. Lemmas 10  11  12)  and
thereby lead to the following theorem:
Theorem 6. If n ≥ S2
ln S   ln n (cid:28) S

  we have

(cid:113)

S ln3 S

n

(cid:18)
(ln S)2   γ∗ ≤ 1 − C2

(cid:19)

lim inf
S→∞ inf
ˆH

sup

R∈R(S γ∗ τ q)

P

| ˆH − ¯H| ≥ C1

S2

n ln S

≥ 1
2

(20)

where τ = S  q =

√
5

1
n ln S

  and C1  C2 > 0 are two universal constants.

5 Application: Fundamental limits of language modeling

In this section  we apply entropy rate estimators to estimate the fundamental limits of language
modeling. A language model speciﬁes the joint probability distribution of a sequence of words 
QX n(xn). It is common to use a (k − 1)th-order Markov assumption to train these models  using
sequences of k words (also known as k-grams 3 sometimes with Latin preﬁxes unigrams  bigrams 
etc.)  with values of k of up to 5 [2]. A commonly used metric to measure the efﬁcacy of a model
QX n is the perplexity (whose logarithm is called the cross-entropy rate):

(cid:115)

perplexityQ (X n) = n

1

QX n (X n)

.

If a language is modeled as a stationary and ergodic stochastic process with entropy rate ¯H  and X n
is drawn from the language with true distribution PX n  then [2]

¯H ≤ lim inf
n→∞

1
n

log

1

QX n (X n)

n→∞ log(cid:2)perplexityQ (X n)(cid:3)  

= lim inf

with equality when Q = P . In this section  all logarithms are with respect to base 2 and all entropy
are measured in bits.
The entropy rate of the English language is of signiﬁcant interest to language model researchers:
since 2 ¯H is a tight lower bound on perplexity  this quantity indicates how close a given language
model is to the optimum. Several researchers have presented estimates in bits per character [3  9  5];
because language models are trained on words  these estimates are not directly relevant to the present
task. In one of the earliest papers on this topic  Claude Shannon [3] gave an estimate of 11.82 bits
per word. This latter ﬁgure has been comprehensively beaten by recent models; for example  [2]
achieved a perplexity corresponding to a cross-entropy rate of 4.55 bits per word.
To produce an estimate of the entropy rate of English  we used two well-known linguistic corpora:
the Penn Treebank (PTB) and Google’s One Billion Words (1BW) benchmark. Results based on
these corpora are particularly relevant because of their widespread use in training models. We used
the conditional approach proposed in this paper with the JVHW estimator describe in Section D. The
PTB corpus contains about n ≈ 1.2 million words  of which S ≈ 47  000 are unique. The 1BW
corpus contains about n ≈ 740 million words  of which S ≈ 2.4 million are unique.
We estimate the conditional entropy H(Xk|X k−1) for k = 1  2  3  4  and our results are shown in
Figure 1. The estimated conditional entropy ˆH(Xk|X k−1) provides us with a reﬁned analysis of the
intrinsic uncertainty in language prediction with context length of only k − 1. For 4-grams  using the
JVHW estimator on the 1BW corpus  our estimate is 3.46 bits per word. With current state-of-the-art
models trained on the 1BW corpus having an cross-entropy rate of about 4.55 bits per word [2] 
this indicates that language models are still at least 0.89 bits per word away from the fundamental
limit. (Note that since H(Xk|X k−1) is decreasing in k  H(X4|X 3) > ¯H.) Similarly  for the much
smaller PTB corpus  we estimate an entropy rate of 1.50 bits per word  compared to state-of-the-art
models that achieve a cross-entropy rate of about 5.96 bits per word [4]  at least 4.4 bits away from
the fundamental limit.
More detailed analysis  e.g.  the accuracy of the JVHW estimates  is shown in the Appendix E.

3In the language modeling literature these are typically known as n-grams  but we use k to avoid conﬂict

with the sample size.

8

1

)
1
−
k
X
|
k
X

(

ˆH
y
p
o
r
t
n
e

.

d
n
o
c

d
e
t
a
m

i
t
s
e

PTB
1BW

best known model for PTB
[4]

best known model for 1BW
[2]

10

8

6

4

2

0

1

2

3

4

memory length k

Figure 1: Estimates of conditional entropy based on linguistic corpora

9

References
[1] Jayadev Acharya  Hirakendu Das  Alon Orlitsky  and Ananda Theertha Suresh. A uniﬁed
maximum likelihood approach for estimating symmetric properties of discrete distributions. In
International Conference on Machine Learning  pages 11–21  2017.

[2] András Antos and Ioannis Kontoyiannis. Convergence properties of functional estimates for

discrete distributions. Random Structures & Algorithms  19(3-4):163–193  2001.

[3] Patrick Billingsley. Statistical methods in Markov chains. The Annals of Mathematical Statistics 

pages 12–40  1961.

[4] Charles Bordenave  Pietro Caputo  and Djalil Chafai. Spectrum of large random reversible
Markov chains: two examples. ALEA: Latin American Journal of Probability and Mathematical
Statistics  7:41–64  2010.

[5] Peter F. Brown  Vincent J. Della Pietra  Robert L. Mercer  Stephen A. Della Pietra  and
Jennifer C. Lai. An estimate of an upper bound for the entropy of english. Comput. Linguist. 
18(1):31–40  March 1992.

[6] Haixiao Cai  Sanjeev R. Kulkarni  and Sergio Verdú. Universal entropy estimation via block

sorting. IEEE Trans. Inf. Theory  50(7):1551–1561  2004.

[7] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction  learning  and games. Cambridge University

Press  2006.

[8] Gabriela Ciuperca and Valerie Girardin. On the estimation of the entropy rate of ﬁnite Markov
chains. In Proceedings of the International Symposium on Applied Stochastic Models and Data
Analysis  2005.

[9] Thomas Cover and Roger King. A convergent gambling estimate of the entropy of English.

IEEE Transactions on Information Theory  24(4):413–421  1978.

[10] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley  New York 

second edition  2006.

[11] Constantinos Daskalakis  Nishanth Dikkala  and Nick Gravin. Testing symmetric markov chains

from a single trajectory. arXiv preprint arXiv:1704.06850  2017.

[12] Michelle Effros  Karthik Visweswariah  Sanjeev R Kulkarni  and Sergio Verdú. Universal
lossless source coding with the Burrows Wheeler transform. IEEE Transactions on Information
Theory  48(5):1061–1081  2002.

[13] Moein Falahatgar  Alon Orlitsky  Venkatadheeraj Pichapati  and Ananda Theertha Suresh.
Learning markov distributions: Does estimation trump compression? In Information Theory
(ISIT)  2016 IEEE International Symposium on  pages 2689–2693. IEEE  2016.

[14] Yanjun Han  Jiantao Jiao  Tsachy Weissman  and Yihong Wu. Optimal rates of entropy

estimation over lipschitz balls. arxiv preprint arxiv:1711.02141  Nov 2017.

[15] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of

the American statistical association  58(301):13–30  1963.

[16] Daniel J Hsu  Aryeh Kontorovich  and Csaba Szepesvári. Mixing time estimation in reversible
Markov chains from a single sample path. In Advances in neural information processing systems 
pages 1459–1467  2015.

[17] Qian Jiang. Construction of transition matrices of reversible Markov chains. M. Sc. Major

Paper. Department of Mathematics and Statistics. University of Windsor  2009.

[18] Jiantao Jiao  H.H. Permuter  Lei Zhao  Young-Han Kim  and T. Weissman. Universal estimation
of directed information. Information Theory  IEEE Transactions on  59(10):6220–6242  2013.

[19] Jiantao Jiao  Kartik Venkat  Yanjun Han  and Tsachy Weissman. Minimax estimation of
functionals of discrete distributions. Information Theory  IEEE Transactions on  61(5):2835–
2885  2015.

10

[20] Jiantao Jiao  Kartik Venkat  Yanjun Han  and Tsachy Weissman. Maximum likelihood estimation
of functionals of discrete distributions. IEEE Transactions on Information Theory  63(10):6774–
6798  Oct 2017.

[21] Daniel Jurafsky and James H. Martin. Speech and Language Processing (2Nd Edition). Prentice-

Hall  Inc.  Upper Saddle River  NJ  USA  2009.

[22] Sudeep Kamath and Sergio Verdú. Estimation of entropy rate and Rényi entropy rate for Markov
chains. In Information Theory (ISIT)  2016 IEEE International Symposium on  pages 685–689.
IEEE  2016.

[23] John C Kieffer. Sample converses in source coding theory. IEEE Transactions on Information

Theory  37(2):263–268  1991.

[24] Ioannis Kontoyiannis  Paul H Algoet  Yu M Suhov  and AJ Wyner. Nonparametric entropy esti-
mation for stationary processes and random ﬁelds  with applications to English text. Information
Theory  IEEE Transactions on  44(3):1319–1327  1998.

[25] Coco Krumme  Alejandro Llorente  Alex Manuel Cebrian  and Esteban Moro Pentland. The

predictability of consumer visitation patterns. Scientiﬁc reports  3  2013.

[26] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. CoRR 

abs/1703.10722  2017.

[27] J Kevin Lanctot  Ming Li  and En-hui Yang. Estimating DNA sequence entropy. In Symposium
on discrete algorithms: proceedings of the eleventh annual ACM-SIAM symposium on discrete
algorithms  volume 9  pages 409–418  2000.

[28] David A Levin and Yuval Peres. Estimating the spectral gap of a reversible markov chain from

a short trajectory. arXiv preprint arXiv:1612.05330  2016.

[29] Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomized algorithms and

probabilistic analysis. Cambridge University Press  2005.

[30] Ravi Montenegro and Prasad Tetali. Mathematical aspects of mixing times in Markov chains.

Foundations and Trends R(cid:13) in Theoretical Computer Science  1(3):237–354  2006.

[31] Liam Paninski. Estimation of entropy and mutual information. Neural Computation  15(6):1191–

1253  2003.

[32] Liam Paninski. Estimating entropy on m bins given fewer than m samples. Information Theory 

IEEE Transactions on  50(9):2200–2203  2004.

[33] Daniel Paulin. Concentration inequalities for markov chains by marton couplings and spectral

methods. Electronic Journal of Probability  20  2015.

[34] Claude E. Shannon. Prediction and entropy of printed English. The Bell System Technical

Journal  30(1):50–64  Jan 1951.

[35] Paul C Shields. The ergodic theory of discrete sample paths. Graduate Studies in Mathematics 

American Mathematics Society  1996.

[36] Chaoming Song  Zehui Qu  Nicholas Blumm  and Albert-László Barabási. Limits of predictabil-

ity in human mobility. Science  327(5968):1018–1021  2010.

[37] Taro Takaguchi  Mitsuhiro Nakamura  Nobuo Sato  Kazuo Yano  and Naoki Masuda. Pre-

dictability of conversation partners. Physical Review X  1(1):011008  2011.

[38] Terence Tao. Topics in random matrix theory  volume 132. American Mathematical Society

Providence  RI  2012.

[39] Kedar Tatwawadi  Jiantao Jiao  and Tsachy Weissman. Minimax redundancy for markov chains

with large state space. arXiv preprint arXiv:1805.01355  2018.

[40] A. Tsybakov. Introduction to Nonparametric Estimation. Springer-Verlag  2008.

11

[41] Gregory Valiant and Paul Valiant. Estimating the unseen: an n/ log n-sample estimator for
entropy and support size  shown optimal via new CLTs. In Proceedings of the 43rd annual
ACM symposium on Theory of computing  pages 685–694. ACM  2011.

[42] Gregory Valiant and Paul Valiant. The power of linear estimators. In Foundations of Computer

Science (FOCS)  2011 IEEE 52nd Annual Symposium on  pages 403–412. IEEE  2011.

[43] Paul Valiant and Gregory Valiant. Estimating the unseen: improved estimators for entropy and
other properties. In Advances in Neural Information Processing Systems  pages 2157–2165 
2013.

[44] Chunyan Wang and Bernardo A Huberman. How random are online social interactions?

Scientiﬁc reports  2  2012.

[45] Yihong Wu and Pengkun Yang. Minimax rates of entropy estimation on large alphabets via best
polynomial approximation. IEEE Transactions on Information Theory  62(6):3702–3720  2016.

[46] Aaron D. Wyner and Jacob Ziv. Some asymptotic properties of the entropy of a stationary
ergodic data source with applications to data compression. IEEE Trans. Inf. Theory  35(6):1250–
1258  1989.

[47] Jacob Ziv and Abraham Lempel. Compression of individual sequences via variable-rate coding.

Information Theory  IEEE Transactions on  24(5):530–536  1978.

[48] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. CoRR 

abs/1611.01578  2016.

12

,Yanjun Han
Jiantao Jiao
Chuan-Zheng Lee
Tsachy Weissman
Yihong Wu
Tiancheng Yu