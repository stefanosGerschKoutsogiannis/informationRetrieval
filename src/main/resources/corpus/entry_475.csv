2018,Algorithms and Theory for Multiple-Source Adaptation,We present a number of novel contributions to the multiple-source adaptation problem. We derive new normalized solutions with strong theoretical guarantees for the cross-entropy loss and other similar losses. We also provide new guarantees that hold in the case where the conditional probabilities for the source domains are distinct. Moreover  we give new algorithms for determining the distribution-weighted combination solution for the cross-entropy loss and other losses. We report the results of a series of experiments with real-world datasets. We find that our algorithm outperforms competing approaches by producing a single robust model that performs well on any target mixture distribution. Altogether  our theory  algorithms  and empirical results provide a full solution for the multiple-source adaptation problem with very practical benefits.,Algorithms and Theory for
Multiple-Source Adaptation

Judy Hoffman

CS Department UC Berkeley

Berkeley  CA 94720

jhoffman@eecs.berkeley.edu

Mehryar Mohri

Courant Institute and Google

New York  NY 10012
mohri@cims.nyu.edu

Ningshan Zhang

New York University
New York  NY 10012

nzhang@stern.nyu.edu

Abstract

We present a number of novel contributions to the multiple-source adaptation
problem. We derive new normalized solutions with strong theoretical guarantees
for the cross-entropy loss and other similar losses. We also provide new guarantees
that hold in the case where the conditional probabilities for the source domains
are distinct. Moreover  we give new algorithms for determining the distribution-
weighted combination solution for the cross-entropy loss and other losses. We
report the results of a series of experiments with real-world datasets. We ﬁnd that
our algorithm outperforms competing approaches by producing a single robust
model that performs well on any target mixture distribution. Altogether  our theory 
algorithms  and empirical results provide a full solution for the multiple-source
adaptation problem with very practical beneﬁts.

1

Introduction

In many modern applications  often the learner has access to information about several source
domains  including accurate predictors possibly trained and made available by others  but no direct
information about a target domain for which one wishes to achieve a good performance. The target
domain can typically be viewed as a combination of the source domains  that is a mixture of their
joint distributions  or it may be close to such mixtures. In addition  often the learner does not have
access to all source data simultaneously  for legitimate reasons such as privacy or storage limitation.
Thus  the learner cannot simply pool all source data together to learn a predictor.
Such problems arise commonly in speech recognition where different groups of speakers (domains)
yield different acoustic models and the problem is to derive an accurate acoustic model for a broader
population that may be viewed as a mixture of the source groups (Liao  2013). In object recognition 
multiple image databases exist  each with its own bias and labeled categories (Torralba and Efros 
2011)  but the target application may contain images which most closely resemble only a subset of
the available training data. Finally  in sentiment analysis  accurate predictors may be available for
sub-domains such as TVs  laptops and CD players  each previously trained on labeled data  but no
labeled data or predictor may be at the learner’s disposal for the more general category of electronics 
which can be modeled as a mixture of the sub-domains (Blitzer et al.  2007; Dredze et al.  2008).
The problem of transfer from a single source to a known target domain (Ben-David  Blitzer  Crammer 
and Pereira  2006; Mansour  Mohri  and Rostamizadeh  2009b; Cortes and Mohri  2014; Cortes 
Mohri  and Muñoz Medina  2015)  either through unsupervised adaptation techniques (Gong et al. 
2012; Long et al.  2015; Ganin and Lempitsky  2015; Tzeng et al.  2015)  or via lightly supervised
ones (some amount of labeled data from the target domain) (Saenko et al.  2010; Yang et al.  2007;
Hoffman et al.  2013; Girshick et al.  2014)  has been extensively investigated in the past. Here  we
focus on the problem of multiple-source domain adaptation and ask how the learner can combine
relatively accurate predictors available for each source domain to derive an accurate predictor for

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

any new mixture target domain? This is known as the multiple-source adaptation (MSA) problem
ﬁrst formalized and analyzed theoretically by Mansour  Mohri  and Rostamizadeh (2008  2009a)
and later studied for various applications such as object recognition (Hoffman et al.  2012; Gong
et al.  2013a b). Recently  Zhang et al. (2015) studied a causal formulation of this problem for a
classiﬁcation scenario  using the same combination rules as Mansour et al. (2008  2009a). A closely
related problem to the MSA problem is that of domain generalization (Pan and Yang  2010; Muandet
et al.  2013; Xu et al.  2014)  where knowledge from an arbitrary number of related domains is
combined to perform well on a previously unseen domain. Appendix G includes a more detailed
discussion of previous work related to the MSA problem.
Mansour  Mohri  and Rostamizadeh (2008  2009a) gave strong theoretical guarantees for a distribution-
weighted combination to address the MSA problem  but they did not provide an algorithmic solution
to determine that combination. Furthermore  the solution they proposed could not be used for loss
functions such as cross-entropy  which require a normalized predictor. Their work also assumed a
deterministic scenario (non-stochastic) with the same labeling function for all source domains.
This work makes a number of novel contributions to the MSA problem. We give new normalized
solutions with strong theoretical guarantees for the cross-entropy loss and other similar losses. Our
guarantees hold even when the conditional probabilities for the source domains are distinct. A
by-product of our analysis is the extension of the theoretical results of Mansour et al. (2008  2009a)
to the stochastic scenario  where there is a joint distribution over the input and output space.
Moreover  we give new algorithms for determining the distribution-weighted combination solution
for the cross-entropy loss and other losses. We prove that the problem of determining that solution
can be cast as a DC-programming (difference of convex) and prove explicit DC-decompositions for
the cross-entropy loss and other losses. We also give a series of experimental results with several
datasets demonstrating that our distribution-weighted combination solution is remarkably robust. Our
algorithm outperforms competing approaches and performs well on any target mixture distribution.
Altogether  our theory  algorithms  and empirical results provide a full solution for the MSA problem
with very practical beneﬁts.

2 Problem setup

extension is needed for the analysis of the most common and realistic learning setups in practice. We

adaptation (MSA) problem in the general stochastic scenario where there is a distribution over the

LetX denote the input space andY the output space. We consider a multiple-source domain
joint input-output spaceX×Y. This is a more general setup than the deterministic scenario in
(Mansour et al.  2008  2009a)  where a target function mapping fromX toY is assumed. This
will assume thatX andY are discrete  but the predictors we consider can take real values. Our theory
the proofs. We will identify a domain with a distribution overX×Y and consider the scenario where
the learner has access to a predictor hk  for each domain Dk  k∈[p]={1  . . .   p}.

We consider two types of predictor functions hk  and their associated loss functions L under the
regression model (R) and the probability model (P) respectively 

can be straightforwardly extended to the continuous case with summations replaced by integrals in

2

(R)
(P)

hk∶X→ R
L∶ R×Y→ R+
hk∶X×Y→[0  1] L∶[0  1]→ R+
E(x y)∼D￿L(h  x  y)￿= ￿(x y)∈X×Y

Much of our theory only assumes that L is convex and continuous. But  we will be particularly

We abuse the notation and write L(h  x  y) to denote the loss of a predictor h at point(x  y)  that
is L(h(x)  y) in the regression model  and L(h(x  y)) in the probability model. We will denote by
L(D  h) the expected loss of a predictor h with respect to the distribution D:
D(x  y)L(h  x  y).
interested in the case where  in the regression model  L(h(x)  y)=(h(x)− y)2 is the squared loss 
and where  in the probability model  L(h(x  y))=− log h(x  y) is the cross-entropy loss (log-loss).
✏> 0 such thatL(Dk  hk)≤ ✏ for all k ∈[p]. We will also assume that the loss of the source
hypotheses hk is bounded  that is L(hk  x  y)≤ M for all(x  y)∈X×Y and all k∈[p].

We will assume that each hk is a relatively accurate predictor for the distribution Dk: there exists

L(D  h)=

In the MSA problem  the learner’s objective is to combine these predictors to design a predictor with
small expected loss on a target domain that could be an arbitrary and unknown mixture of the source
domains  the case we are particularly interested in  or even some other arbitrary distribution. It is
worth emphasizing that the learner has no knowledge of the target domain.

In Appendix A (Lemmas 9 and 10) we show that no convex combination rule will perform well even
in very simple MSA problems. These results generalize a previous lower bound of Mansour et al.
(2008). Next  we show that the distribution-weighted combination rule is a suitable solution.
Extending the deﬁnition given by Mansour et al. (2008)  we deﬁne the distribution-weighted combi-

How do we combine the hks? Can we use a convex combination rule ∑p
nation of the functions hk  k∈[p] as follows. For any ⌘> 0  z∈   and(x  y)∈X×Y 

k=1 khk  for some ∈ ?

(R)

(P)

(1)

(2)

h⌘

z(x)= p￿k=1
z(x  y)= p￿k=1

h⌘

zkD1

k(x)+ ⌘ U1(x)p
k(x)+ ⌘U1(x) hk(x) 
∑p
k=1 zkD1
zkDk(x  y)+ ⌘ U(x y)p
j=1 zjDj(x  y)+ ⌘ U(x  y) hk(x  y) 
∑p

where we denote by D1 the marginal distribution overX   for all x∈X   D1(x)=∑y∈Y D(x  y) 
and by U1 the uniform distribution overX . This extension may seem technically straightforward in

hindsight  but the form of the predictor was not immediately clear in the stochastic case.

3 Theoretical guarantees

In this section  we present a series of theoretical guarantees for distribution-weighted combinations
with a suitable choice of the parameters z and ⌘  both for the regression model and for the probability
model. We ﬁrst give our main result for the general stochastic scenario. Next  for the probability
model with cross-entropy loss  we introduce a normalized distribution weighted combination and
prove that it beneﬁts from strong theoretical guarantees.
Our theoretical results rely on a measure of divergence between two distributions. The one that
naturally comes up in our analysis is the Rényi Divergence (Rényi  1961). We will denote by

See Appendix F for more details about the notion of Rényi Divergence.

3.1 General guarantees for regression and probability models

probability distribution on the target and the source domain k respectively. We do not assume that the

This is a signiﬁcant extension of the MSA scenario with respect to the one considered by Mansour
et al. (2009a)  which assumed exactly the same labeling function f for all source domains  in the
deterministic scenario.
Let DT be a mixture of source distributions  such that D1

d↵(D∥ D′)= eD↵(D∥D′) the exponential of the ↵-Rényi Divergence of two distributions D and D′.
Let DT be an unknown target distribution. We will denote by DT(⋅￿x) and Dk(⋅￿x) the conditional
target and source conditional probabilities DT(⋅￿x) and Dk(⋅￿x) coincide for all k∈[p] and x∈X .
k∶ ∈ } in the
k=1 kDk∶ ∈ } in the probability model. We also assume
regression model  or DT ∈D={∑p
Fix ↵> 1 and deﬁne ✏T by
k￿d↵(DT(⋅￿x)∥ Dk(⋅￿x))↵−1￿￿
k∈[p]￿ E
✏T= max
x∼D1
distribution DT(⋅￿x) and the source ones Dk(⋅￿x) ∀k∈[p]  with the expectation taken over the
k  and the maximum taken over k∈[p]. When the target conditional is
conditional probabilities coincide  for ↵=+∞  we have ✏T= ✏.

source marginal distribution D1
close to all source ones  ↵ can be chosen to be very large and ✏T is close to ✏. In particular  when the

that under the regression model  all possible target distributions DT admit the same (unknown)
conditional probability distribution.

✏T depends on the maximal expected Rényi divergence between the target conditional probability

T ∈D1 ={∑p

k=1 kD1

↵−1
↵ M

✏

1

↵ .

1
↵

3

Theorem 1. For any > 0  there exist ⌘> 0 and z∈  such that the following inequalities hold for
any ↵> 1 and any target distribution DT that is a mixture of source distributions:

L(DT   h⌘
L(DT   h⌘

z)≤ ✏T+  
z)≤ ✏+ .

(R)
(P)

1
↵

✏

1

↵ .

↵−1
↵ M

✏↵(Q)= max

As discussed later  the proof of more general results (Theorem 2 and Theorem 14) is given in
Appendix B. The learning guarantees for the regression and the probability model are slightly
different  since the deﬁnitions of the distribution-weighted combinations are different for the two

small. It is even more remarkable that  in the probability model (P)  the loss of h⌘

z on DT is
z is at most ✏ on
z is a robust hypothesis with favorable property for any such

models. Theorem 1 shows the existence of ⌘> 0 and a mixture weight z∈  with a remarkable
property: in the regression model (R)  for any target distribution DT whose conditional DT(⋅￿x) is
on average not too far away from Dk(⋅￿x) for any k∈[p]  and D1
any target distribution DT∈D. Thus  h⌘

target distribution DT .
We now present a more general result  Theorem 2  that relaxes the assumptions under the regression
model that all possible target distributions DT admit the same conditional probability distribution 
and that the target’s marginal distribution is a mixture of source ones. In Appendix B  we show that
Theorem 2 coincides with Theorem 1 under those assumptions. In Appendix B  we further give a
more general result than Theorem 1 under the probability model (Theorem 14).
To present this more general result  we ﬁrst introduce some additional notation. Given a conditional

T ∈D1  the loss of h⌘

probability distribution Q(⋅￿x) deﬁned for all x∈X   deﬁne ✏↵(Q) as follows:
k￿d↵(Q(⋅￿x)∥ Dk(⋅￿x))↵−1￿￿
k∈[p]￿ E
x∼D1
Thus  ✏↵(Q) depends on the maximal expected ↵-Rényi divergence between Q(⋅￿x) and Dk(⋅￿x) 
and ✏↵(Q)= ✏T when Q(⋅￿x)= DT(⋅￿x). When there exists Q(⋅￿x) such that the expected ↵-Rényi
divergence is small for all k∈[p]  then ✏↵(Q) is close to ✏ for ↵=+∞. In addition  we will use the
k(x)Q(y￿x) andDP Q=￿∑p
following deﬁnitions: Dk Q(x  y)= D1
Theorem 2 (Regression model). Fix a conditional probability distribution Q(⋅￿x) deﬁned for all
x∈X . Then  for any > 0  there exist ⌘> 0 and z∈  such that the following inequality holds for
any ↵  > 1 and any target distribution DT :
z)≤￿￿✏↵(Q)+ ￿ d(DT∥DP Q)￿ −1
probabilities of the source and target domains and a ﬁxed pivot Q(⋅￿x). In particular  when there exists
a pivot Q(⋅￿x) that is close to DT(⋅￿x) and Dk(⋅￿x)  for all k∈[p]  then the guarantee is signiﬁcant.
One candidate for such a pivot is a conditional probability distribution Q(⋅￿x) minimizing ✏↵(Q).

In many learning tasks  it is reasonable to assume that the conditional probability of the output
labels is the same across source domains. For example  a dog picture represents a dog regardless
of whether the picture belongs to an individual’s personal collection or to a broader database of
pictures from multiple individuals. This is a straightforward extension of the assumption adopted by
Mansour et al. (2008) in the deterministic scenario  where exactly the same labeling function f is

The learning guarantee of Theorem 2 depends on the Rényi divergence between the conditional

k=1 kDk Q∶ ∈ ￿.

assumed for all source domains. In that case  we have DT(⋅￿x)= Dk(⋅￿x) ∀k∈[p] and therefore
d↵(DT(⋅￿x)∥ Dk(⋅￿x))= 1. Setting ↵=+∞  we recover the main result of Mansour et al. (2008).
Corollary 3. Assume that the conditional probability distributions Dk(⋅￿x) do not depend on k. Then 
for any > 0  there exist ⌘> 0 and z∈  such thatL(D  h⌘
z)≤ ✏+  for any mixture parameter
∈ .
Corollary 3 shows the existence of a parameter ⌘> 0 and a mixture weight z∈  with a remarkable
property: for any > 0  regardless of which mixture weight ∈  deﬁnes the target distribution  the
z is at most ✏+   that is arbitrarily close to ✏. h⌘
distributions Dk are not directly available to the learner  and instead estimateŝDk have been derived

loss of h⌘
favorable property for any mixture target distribution.
To cover the realistic cases in applications  we further extend this result to the case where the

z is therefore a robust hypothesis with a

L(DT   h⌘



M

1

 .

4

↵

1

z(x  y).
z(x  y) deﬁned by (2):

↵  

M

Rényi divergence of DT and the family of mixtures of source distributions.

from data  and further to the case where the target distribution DT is not a mixture of source
z the distribution-weighted combination rule based on the estimates

distributions. We will denote bŷh⌘
̂Dk. Our learning guarantee for̂h⌘
z depends on the Rényi divergence of̂Dk and Dk  as well as the
Corollary 4. For any > 0  there exist ⌘> 0 and z∈   such that the following inequality holds for
any ↵> 1 and arbitrary target distribution DT :
z)≤￿(̂✏+ ) d↵(DT∥ ̂D)￿ ↵−1
L(DT  ̂h⌘
wherê✏= maxk∈[p]￿✏ d↵(̂Dk∥ Dk)￿ ↵−1
z based on the estimate distributions ̂Dk that is
Corollary 4 shows that there exists a predictor̂h⌘
̂✏-accurate with respect to any target distribution DT whose Rényi divergence with respect to the
family ̂D is not too large (d↵(DT∥ ̂D) close to 1). Furthermore ̂✏ is close to ✏  provided that̂Dks
are good estimates of Dks (that is d↵(̂Dk∥ Dk) close to 1). The proof is given in Appendix B.

3.2 Guarantees for the probability model with the cross-entropy loss
Here  we discuss the important special case where L coincides with the cross-entropy loss in the
probability model  and present a guarantee for a normalized distribution-weighted combination
solution. This analysis is a complement to Theorem 1  which only holds for the unnormalized
hypothesis h⌘
The cross-entropy loss assumes normalized hypotheses. Thus  here  we assume that the source

k=1 k̂Dk∶ ∈ ￿.

↵   and ̂D=￿∑p

M 1

↵

⌘

⌘

h

h⌘

combination h⌘

z(x  y)=

functions are normalized for every x: ∑y∈Y hk(x  y)= 1  ∀x∈X  ∀k∈[p]. For any ⌘> 0 and
z(x  y) that is based on distribution-weighted
z∈   we deﬁne a normalized weighted combination h
z(x  y)
z(x  y) .
∑y∈Y h⌘
We will ﬁrst assume the conditional probability distributions Dk(⋅￿x) do not depend on k.
Theorem 5. Assume that there exists µ> 0 such that Dk(x  y)≥ µU(x  y) for all k ∈[p] and
(x  y)∈X×Y. Then  for any > 0  there exist ⌘> 0 and z∈  such thatL(D  h
z)≤ ✏+  for any
mixture parameter ∈ .

Theorem 5 provides a strong guarantee that is the analogue of Corollary 3 for normalized distribution-
weighted combinations. The theorem can also be extended to the case of arbitrary target distributions
and estimated densities. When the conditional probabilities are distinct across the source domains 
we propose a marginal distribution-weighted combination rule  which is already normalized. We
can directly apply Theorem 1 to that solution and achieve favorable guarantees. More details are
presented in Appendix C.
These results are non-trivial and important  as they provide a guarantee for an accurate and robust
predictor for a commonly used loss function  the cross-entropy loss.

⌘

4 Algorithms

We have shown that  for both the regression and the probability model  there exists a vector z deﬁning
z that admits very favorable guarantees. But how
a distribution-weighted combination hypothesis h⌘
can we ﬁnd a such z? This is a key question in the MSA problem which was not addressed by Mansour
et al. (2008  2009a): no algorithm was previously reported to determine the mixture parameter z 
even for the deterministic scenario. Here  we give an algorithm for determining that vector z.
In this section  we give practical and efﬁcient algorithms for ﬁnding the vector z in the important
cases of the squared loss in the regression model  or the cross-entropy loss in the probability model  by
leveraging the differentiability of the loss functions. We ﬁrst show that z is the solution of a general
optimization problem. Next  we give a DC-decomposition (difference of convex decomposition)

5

of the objective for both models  thereby proving an explicit DC-programming formulation of the
problem. This leads to an efﬁcient DC algorithm that is guaranteed to converge to a stationary point.
Additionally  we show that it is straightforward to test if the solution obtained is the global optimum.
While we are not proving that the local stationary point found by our algorithm is the global optimum 
empirically  we observe that that is indeed the case.

4.1 Optimization problem
Theorem 1 shows that the hypothesis h⌘
generalization guarantee. A key step in proving Theorem 1 is to show the following lemma.

z based on the mixture parameter z beneﬁts from a strong

Lemma 6. For any ⌘  ⌘′> 0  there exists z∈   with zk≠ 0 for all k∈[p]  such that the following

holds for the distribution-weighted combining rule h⌘
z:

∀k∈[p]  L(Dk  h⌘

z)+ ⌘′.
zjL(Dj  h⌘
z)−L(Dz  h⌘
be formulated as a min-max problem: minz∈ maxk∈[p]L(Dk  h⌘

equivalently formulated as the following optimization problem:

Lemma 6 indicates that for the solution z  h⌘
z has essentially the same loss on all source domains.
Thus  our problem consists of ﬁnding a parameter z verifying this property. This  in turn  can

z)  which can be

z)≤ p￿j=1

(3)



min

z∈ ∈R

s.t.L(Dk  h⌘

z)−L(Dz  h⌘

z)≤  ∀k∈[p].

4.2 DC-decomposition
We provide explicit DC decompositions of the objective of Problem (4) for the regression model with
the squared loss and for the probability model with the cross-entropy loss. The derivations are given
in Appendix D. We ﬁrst rewrite h⌘
z as the division of two afﬁne functions for both the regression (R)

and the probability (P) model  hz= Jz￿Kz  where we adopt the following deﬁnitions and notation:
Jz(x)= p￿k=1
Jz(x  y)= p￿k=1
Proposition 7 (Regression model  squared loss). Let L be the squared loss. Then  for any k∈[p] 
L(Dk  h⌘
z)= uk(z)− vk(z)  where uk and vk are convex functions deﬁned for all z by

U(x  y)hk(x  y)  Kz(x  y)= Dz(x  y)+ ⌘ U(x  y).

z(x)+ ⌘ U1(x) 

Kz(x)= D1

(R)

(P)

p

(4)

p

zkD1

U1(x)hk(x) 

k(x)hk(x)+ ⌘
zkDk(x  y)hk(x  y)+ ⌘
z)−L(Dz  h⌘
uk(z)=L￿Dk+ ⌘U1Dk(⋅￿x)  h⌘
vk(z)=L￿Dz+ ⌘U1Dk(⋅￿x)  h⌘
z)−L(Dz  h⌘

z￿− 2M￿x(D1
z￿− 2M￿x(D1

k+ ⌘U1)(x) log Kz(x) 
k+ ⌘U1)(x) log Kz(x).

Proposition 8 (Probability model  cross-entropy loss). Let L be the cross-entropy loss. Then  for

z)= uk(z)− vk(z)  where uk and vk are convex functions deﬁned for

all z by

k∈[p] L(Dk  h⌘
uk(z)=−￿x y￿Dk(x  y)+ ⌘ U(x  y)￿ log Jz(x  y) 
vk(z)=￿x y

Kz(x  y) log￿ Kz(x  y)

Jz(x  y)￿−[Dk(x  y)+ ⌘ U(x  y)] log Kz(x  y).

4.3 DC algorithm
Our DC decompositions prove that the optimization problem (4) can be cast as the following
variational form of a DC-programming problem (Tao and An  1997  1998; Sriperumbudur and
Lanckriet  2012):


min

s.t.￿uk(z)− vk(z)≤ ￿∧￿− zk≤ 0￿∧￿∑p

k=1 zk− 1= 0￿  ∀k∈[p].

z∈ ∈R

(5)

6

Figure 1: MSE sentiment analysis under mixture of two domains:
electronics; (b) (right ﬁgure) kitchen and books.

(a) (left ﬁgure) dvd and

solving the following convex optimization problem:



(6)

of Problem (4) (Yuille and Rangarajan  2003; Sriperumbudur and Lanckriet  2012). Note that

The DC-programming algorithm works as follows. Let(zt)t be the sequence deﬁned by repeatedly
zt+1∈ argmin
z ∈R
s.t.￿uk(z)− vk(zt)−(z− zt)∇vk(zt)≤ ￿∧￿− zk≤ 0￿∧￿∑p
k=1 zk− 1= 0￿  ∀k∈[p] 
where z0∈  is an arbitrary starting value. Then (zt)t is guaranteed to converge to a local minimum
Problem (6) is a relatively simple optimization problem: uk(z) is a weighted sum of the negative
Problem (4) seeks a parameter z verifyingL(Dk  h⌘
z) ≤   for all k ∈ [p] for an
arbitrarily small value of . SinceL(Dz  h⌘
z) is a weighted average of the
z)=∑p
expected lossesL(Dk  h⌘
z)  k∈[p]  the solution  cannot be negative. Furthermore  by Lemma 6  a
parameter z verifying that inequality exists for any > 0. Thus  the global solution  of Problem (4)

must be close to zero. This provides us with a simple criterion for testing the global optimality of the
solution z we obtain using a DC-programming algorithm with a starting parameter z0.

logarithm of an afﬁne function of z  plus a weighted sum of rational functions of z (squared loss) 
and all other terms appearing in the constraints are afﬁne functions of z.

z)−L(Dz  h⌘
k=1 zkL(Dk  h⌘

5 Experiments

This section reports the results of our experiments with our DC-programming algorithm for ﬁnding
a robust domain generalization solution when using squared loss and cross-entropy loss. We ﬁrst
evaluated our algorithm using an artiﬁcial dataset assuming known densities where we could compare
our result to the global solution and found that indeed our global objective approached the known
optimum of zero (see Appendix E for more details). Next  we evaluated our DC-programming
solution applied to real-world datasets: a sentiment analysis dataset (Blitzer et al.  2007) with the
squared loss  a visual domain adaptation benchmark dataset Ofﬁce (Saenko et al.  2010)  as well as a
generalization of digit recognition task  with the cross-entropy loss.
For all real-world datasets  the probability distributions Dk are not readily available to the learner.
However  Corollary 4 extends the learning guarantees of our solution to the case where an estimate

̂Dk is used in lieu of the ideal distribution Dk. Thus  we used standard density estimation methods to
derive an estimatêDk for each k∈[p]. While density estimation can be a difﬁcult task in general 
for our purpose  straightforward techniques were sufﬁcient for our predictor̂h⌘

z to achieve a high
performance  since the approximate densities only serve to indicate the relative importance of each
source domain. We give full details about our density estimation procedure in Appendix E.

5.1 Sentiment analysis task with the squared loss
We used the sentiment analysis dataset proposed by Blitzer et al. (2007) and used for multiple-source
adaptation by Mansour et al. (2008  2009a). This dataset consists of product review text and rating
labels taken from four domains: books (B)  dvd (D)  electronics (E)  and kitchen (K)  with
2 000 samples for each domain. We deﬁned a vocabulary of 2 500 words that occur at least twice in
the intersection of the four domains. These words were used to deﬁne feature vectors  where every
sample was encoded by the number of occurrences of each word. We trained our base hypotheses
using support vector regression with the same hyper-parameters as in (Mansour et al.  2008  2009a).

7

Table 1: MSE on the sentiment analysis dataset of source-only baselines for each domain  K D  B E 
the uniform weighted predictor unif  KMM  and the distribution-weighted method DW based on the
learned z. DW outperforms all competing baselines.

K
D
B
E
unif
KMM
DW(ours)

K

1.46±0.08
2.12±0.08
2.18±0.11
1.69±0.09
1.62±0.05
1.63±0.15
1.45±0.08

D

2.20±0.14
1.78±0.08
2.01±0.09
2.31±0.12
1.84±0.09
2.07±0.12
1.78±0.08

B

2.29±0.13
2.12±0.08
1.73±0.12
2.40±0.11
1.86±0.09
1.93±0.17
1.72±0.12

E

1.69±0.12
2.10±0.07
2.24±0.07
1.50±0.06
1.62±0.07
1.69±0.12
1.49±0.06

Table 2: Digit dataset statistics.

SVHN MNIST USPS

# train images
# test images
image size
color

73 257
26 032
32x32
rgb

60 000
10 000
28x28
gray

7 291
2 007
16x16
gray

Test Data

KD

1.83±0.08
1.95±0.07
2.10±0.09
2.00±0.09
1.73±0.06
1.83±0.07
1.62±0.07

DBE

KDB

BE

1.99±0.10
2.11±0.07
1.99±0.08
1.95±0.07
1.74±0.07
1.82±0.07
1.61±0.08

KDB

KBE

1.81±0.07
2.11±0.06
2.05±0.06
1.86±0.04
1.70±0.05
1.75±0.07
1.56±0.04

2.06±0.07
1.98±0.06
2.00±0.06
2.01±0.06
1.99±0.05
1.98±0.06
2.07±0.06
2.14±0.06
1.77±0.05
1.77±0.04
1.89±0.07
1.86±0.09
1.66±0.05
1.65±0.04
Table 3: Digit dataset accuracy.

1.78±0.07
2.00±0.06
2.14±0.06
1.84±0.06
1.69±0.04
1.78±0.06
1.58±0.05

KDBE

1.91±0.06
2.03±0.06
2.04±0.05
1.98±0.05
1.74±0.04
1.82±0.06
1.61±0.04

CNN-s
CNN-m
CNN-u
CNN-unif
DW (ours)
CNN-joint

svhn
92.3
15.7
16.7
75.7
91.4
90.9

mnist
66.9
99.2
62.3
91.3
98.8
99.1

usps
65.6
79.7
96.6
92.2
95.6
96.0

Test Data
mu
66.7
96.0
68.1
91.4
98.3
98.6

su
90.4
20.3
22.5
76.9
91.7
91.3

sm
85.2
38.9
29.4
80.0
93.5
93.2

smu mean
78.8
84.2
41.0
55.8
46.9
32.9
84.0
80.7
94.7
93.6
93.3
94.6

We compared our method (DW) against each source hypothesis  hk. We also computed a privileged

k=1 khk. -comb is of course not accessible

baseline using the oracle  mixing parameter  -comb:∑p

in practice since the target mixture  is not known to the user. We also compared against a previously
proposed domain adaptation algorithm (Huang et al.  2006) known as KMM. It is important to note
that the KMM model requires access to the unlabeled target data during adaptation and learns a new
predictor for every target domain  while DW does not use any target data. Thus KMM operates in a
favorable learning setting when compared to our solution.
We ﬁrst considered the same test scenario as in (Mansour et al.  2008)  where the target is a mixture of
two source domains. The plots of Figures 1a and 1b report the results of our experiments. They show
that our distribution-weighted predictor DW outperforms all baseline predictors despite the privileged
learning scenarios of -comb and KMM. We also compared our results with the weighted predictor
used in the empirical studies by Mansour et al. (2008)  which is not a realistic solution since it is using
the unknown target mixture  as z to compute hz. Nevertheless  we observed that the performance
of this ”cheating” solution almost always coincides with that of our DW algorithm and thus did not
include it in our plots and tables to avoid confusion.
Next  we compared the performance of DW with accessible baseline predictors on various target
mixtures. Since  is not known in practice  we replaced -comb with the uniform combination of all

hypotheses (unif) ∑p

k=1 hk￿p. Table 1 reports the mean and standard deviations of MSE over 10

repetitions. Each column corresponds to a different target test data source. Our distribution-weighted
method DW outperforms all baseline predictors across all test domains. Observe that  even when the
target is a single source domain  our method successfully outperforms the predictor which is trained
and tested on the same domain. Results on more target mixtures are available in Appendix E.

5.2 Recognition tasks with the cross-entropy loss

We considered two real-world domain adaptation tasks: a generalization of a digit recognition task
and a standard visual adaptation Ofﬁce dataset.
For each individual domain  we trained a convolutional neural network (CNN) and used the output
from the softmax score layer as our base predictors hk. We computed the uniformly weighted

combination of source predictors  hunif=∑p

k=1 hk￿p. As a privileged baseline  we also trained a

model on all source data combined  hjoint. Note  this approach is often not feasible if independent
entities contribute classiﬁers and densities  but not full training datasets. Thus this approach is not
consistent with our scenario  and it operates in a much more favorable learning setting than our
solution. Finally  our distribution weighted predictor DW was computed with hks  density estimates 
and our learned weighting  z. Our baselines then consists of the classiﬁers from hk  hunif  hjoint 
and DW.

8

Table 4: Ofﬁce dataset accuracy: We report accuracy across six possible test domains. We show
performance all baselines: CNN-a w d  CNN-unif  DW based on the learned z  and the jointly
trained model CNN-joint. DW outperforms all competing models.

CNN-a
CNN-w
CNN-d
CNN-unif
DW (ours)
CNN-joint

amazon

75.7± 0.3
45.3± 0.5
50.4± 0.4
69.7± 0.3
75.2± 0.4
72.1± 0.3

webcam

53.8± 0.7
91.1± 0.8
89.6± 0.9
93.1± 0.6
93.7± 0.6
93.7± 0.5

dslr

53.4± 1.3
91.7± 1.2
90.9± 0.8
93.2± 0.9
94.0± 1.0
93.7± 0.5

aw

71.4± 0.3
54.4± 0.5
58.3± 0.4
74.4± 0.4
78.9± 0.4
76.4± 0.4

Test Data

ad

73.5± 0.2
50.0± 0.5
54.6± 0.4
72.1± 0.3
77.2± 0.4
76.4± 0.4

wd

53.6± 0.8
91.3± 0.8
90.0± 0.7
93.1± 0.5
93.8± 0.6
93.7± 0.5

awd

69.9± 0.3
57.5± 0.4
61.0± 0.4
75.9± 0.3
80.2± 0.3
79.3± 0.4

mean

64.5± 0.6
68.8± 0.7
70.7± 0.6
81.6± 0.5
84.7± 0.5
83.6± 0.4

We began our study with a generalization of digit recognition task  which consists of three digit
recognition datasets: Google Street View House Numbers (SVHN)  MNIST  and USPS. Dataset
statistics as well as example images can be found in Table 2. We trained the ConvNet (or CNN)
architecture following Taigman et al. (2017) as our source models and joint model. We used the
second fully-connected layer’s output as our features for density estimation  and the output from
the softmax score layer as our predictors. We used the full training sets per domain to learn the
source model and densities. Note  these steps are completely isolated from one another and may be
performed by unique entities and in parallel. Finally  for our DC-programming algorithm we used a
small subset of 200 real image-label pairs from each domain to learn the parameter z.
Our next experiment used the standard visual adaptation Ofﬁce dataset  which has 3 domains: amazon 
webcam  and dslr. The dataset contains 31 recognition categories of objects commonly found in an
ofﬁce environment. There are 4 110 images total with 2 817 from amazon  795 from webcam  and
498 from dslr.
We followed the standard protocol from Saenko et al. (2010)  whereby 20 labeled examples are
available for training from the amazon domain and 8 labeled examples are available from both the
webcam and dslr domains. The remaining examples from each domain are used for testing. We
used the AlexNet Krizhevsky et al. (2012) ConvNet (CNN) architecture  and used the output from
the softmax score layer as our base predictors  pre-trained on ImageNet and used fc7 activations as
our features for density estimation Donahue et al. (2014).
We report the performance of our algorithm and that of baselines on the digit recognition dataset in
Table 3  and report the performance on the Ofﬁce dataset in Table 4. On both datasets  we evaluated
on various test distributions: each individual domain  the combination of each two domains and the
fully combined set. When the test distribution equals one of the source distributions  our distribution-
weighted classiﬁer successfully outperforms (webcam dslr) or maintains the performance of the
classiﬁer which is trained and tested on the same domain. For the more realistic scenario where the
target domain is a mixture of any two or all three source domains  the performance of our method
is comparable or marginally superior to that of the jointly trained network  despite the fact that
we do not retrain any network parameters in our method and that we only use a small number of
per-domain examples to learn the distribution weights – an optimization which may be solved on a
single CPU in a matter of seconds for this problem. This again demonstrates the robustness of our
distribution-weighted combined classiﬁer to a varying target domain.

6 Conclusion

We presented practically applicable multiple-source domain adaptation algorithms for the squared
loss and the cross-entropy loss. Our algorithms beneﬁt from a series of very favorable theoretical
guarantees. Our results further demonstrate empirically their effectiveness and their importance in
adaptation problems in practice.

Acknowledgments
We thank Cyril Allauzen for comments on a previous draft of this paper. This work was partly funded
by NSF CCF-1535987 and NSF IIS-1618662.

9

References
C. Arndt. Information Measures: Information and its Description in Science and Engineering.

Signals and Communication Technology. Springer Verlag  2004.

S. Ben-David  J. Blitzer  K. Crammer  and F. Pereira. Analysis of representations for domain

adaptation. In NIPS  pages 137–144  2006.

G. Blanchard  G. Lee  and C. Scott. Generalizing from several related classiﬁcation tasks to a new

unlabeled sample. In NIPS  pages 2178–2186  2011.

J. Blitzer  M. Dredze  and F. Pereira. Biographies  bollywood  boom-boxes and blenders: Domain

adaptation for sentiment classiﬁcation. In ACL  pages 440–447  2007.

C. Cortes and M. Mohri. Domain adaptation and sample bias correction theory and algorithm for

regression. Theor. Comput. Sci.  519:103–126  2014.

C. Cortes  M. Mohri  and A. Muñoz Medina. Adaptation algorithm and theory based on generalized

discrepancy. In KDD  pages 169–178  2015.

T. M. Cover and J. M. Thomas. Elements of Information Theory. Wiley-Interscience  2006.
K. Crammer  M. J. Kearns  and J. Wortman. Learning from multiple sources. Journal of Machine

Learning Research  9(Aug):1757–1774  2008.

J. Donahue  Y. Jia  O. Vinyals  J. Hoffman  N. Zhang  E. Tzeng  and T. Darrell. Decaf: A deep
convolutional activation feature for generic visual recognition. In ICML  volume 32  pages 647–655 
2014.

M. Dredze  K. Crammer  and F. Pereira. Conﬁdence-weighted linear classiﬁcation. In ICML  volume

307  pages 264–271  2008.

L. Duan  I. W. Tsang  D. Xu  and T. Chua. Domain adaptation from multiple sources via auxiliary

classiﬁers. In ICML  volume 382  pages 289–296  2009.

L. Duan  D. Xu  and I. W. Tsang. Domain adaptation from multiple sources: A domain-dependent
regularization approach. IEEE Transactions on Neural Networks and Learning Systems  23(3):
504–518  2012.

Y. Ganin and V. S. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML 

volume 37  pages 1180–1189  2015.

R. B. Girshick  J. Donahue  T. Darrell  and J. Malik. Rich feature hierarchies for accurate object

detection and semantic segmentation. In CVPR  pages 580–587  2014.

B. Gong  Y. Shi  F. Sha  and K. Grauman. Geodesic ﬂow kernel for unsupervised domain adaptation.

In CVPR  pages 2066–2073  2012.

B. Gong  K. Grauman  and F. Sha. Connecting the dots with landmarks: Discriminatively learning
In ICML  volume 28  pages

domain-invariant features for unsupervised domain adaptation.
222–230  2013a.

B. Gong  K. Grauman  and F. Sha. Reshaping visual datasets for domain adaptation. In NIPS  pages

1286–1294  2013b.

J. Hoffman  B. Kulis  T. Darrell  and K. Saenko. Discovering latent domains for multisource domain

adaptation. In ECCV  volume 7573  pages 702–715  2012.

J. Hoffman  E. Rodner  J. Donahue  K. Saenko  and T. Darrell. Efﬁcient learning of domain-invariant

image representations. In ICLR  2013.

J. Huang  A. J. Smola  A. Gretton  K. M. Borgwardt  and B. Schölkopf. Correcting sample selection

bias by unlabeled data. In NIPS  pages 601–608  2006.

A. Khosla  T. Zhou  T. Malisiewicz  A. A. Efros  and A. Torralba. Undoing the damage of dataset

bias. In ECCV  volume 7572  pages 158–171  2012.

10

A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In NIPS  pages 1106–1114  2012.

H. Liao. Speaker adaptation of context dependent deep neural networks. In ICASSP  pages 7947–7951 

2013.

M. Long  Y. Cao  J. Wang  and M. I. Jordan. Learning transferable features with deep adaptation

networks. In ICML  volume 37  pages 97–105  2015.

Y. Mansour  M. Mohri  and A. Rostamizadeh. Domain adaptation with multiple sources. In NIPS 

pages 1041–1048  2008.

Y. Mansour  M. Mohri  and A. Rostamizadeh. Multiple source adaptation and the Rényi divergence.

In UAI  pages 367–374  2009a.

Y. Mansour  M. Mohri  and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms.

In COLT  2009b.

K. Muandet  D. Balduzzi  and B. Schölkopf. Domain generalization via invariant feature representa-

tion. In ICML  volume 28  pages 10–18  2013.

S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng.  22(10):

1345–1359  2010.

Z. Pei  Z. Cao  M. Long  and J. Wang. Multi-adversarial domain adaptation.

3934–3941  2018.

In AAAI  pages

A. Rényi. On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium

on Mathematical Statistics and Probability  pages 547–561  1961.

B. Roark  R. Sproat  C. Allauzen  M. Riley  J. Sorensen  and T. Tai. The opengrm open-source

ﬁnite-state grammar software libraries. In ACL (System Demonstrations)  pages 61–66  2012.

K. Saenko  B. Kulis  M. Fritz  and T. Darrell. Adapting visual category models to new domains. In

ECCV  volume 6314  pages 213–226  2010.

B. K. Sriperumbudur and G. R. G. Lanckriet. A proof of convergence of the concave-convex

procedure using Zangwill’s theory. Neural Computation  24(6):1391–1407  2012.

Y. Taigman  A. Polyak  and L. Wolf. Unsupervised cross-domain image generation. In ICLR  2017.
P. D. Tao and L. T. H. An. Convex analysis approach to DC programming: theory  algorithms and

applications. Acta Mathematica Vietnamica  22(1):289–355  1997.

P. D. Tao and L. T. H. An. A DC optimization algorithm for solving the trust-region subproblem.

SIAM Journal on Optimization  8(2):476–505  1998.

A. Torralba and A. A. Efros. Unbiased look at dataset bias. In CVPR  pages 1521–1528  2011.
E. Tzeng  J. Hoffman  T. Darrell  and K. Saenko. Simultaneous deep transfer across domains and

tasks. In ICCV  pages 4068–4076  2015.

Z. Xu  W. Li  L. Niu  and D. Xu. Exploiting low-rank structure from latent domains for domain

generalization. In ECCV  volume 8691  pages 628–643  2014.

J. Yang  R. Yan  and A. G. Hauptmann. Cross-domain video concept detection using adaptive svms.

In ACM Multimedia  pages 188–197  2007.

A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation  15(4):915–936 

2003.

K. Zhang  M. Gong  and B. Schölkopf. Multi-source domain adaptation: A causal view. In AAAI 

pages 3150–3157  2015.

11

,Judy Hoffman
Mehryar Mohri
Ningshan Zhang