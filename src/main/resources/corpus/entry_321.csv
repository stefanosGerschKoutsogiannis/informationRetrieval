2017,Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds,In this paper  we propose an accelerated first-order method for geodesically convex optimization  which is the generalization of the standard Nesterov's accelerated method from Euclidean space to nonlinear Riemannian space. We first derive two equations and obtain two nonlinear operators for geodesically convex optimization instead of the linear extrapolation step in Euclidean space. In particular  we analyze the global convergence properties of our accelerated method for geodesically strongly-convex problems  which show that our method improves the convergence rate from O((1-\mu/L)^{k}) to O((1-\sqrt{\mu/L})^{k}). Moreover  our method also improves the global convergence rate on geodesically general convex problems from O(1/k) to O(1/k^{2}). Finally  we give a specific iterative scheme for matrix Karcher mean problems  and validate our theoretical results with experiments.,Accelerated First-order Methods for Geodesically
Convex Optimization on Riemannian Manifolds

Yuanyuan Liu1  Fanhua Shang1∗  James Cheng1  Hong Cheng2  Licheng Jiao3
1Dept. of Computer Science and Engineering  The Chinese University of Hong Kong

2Dept. of Systems Engineering and Engineering Management 

The Chinese University of Hong Kong  Hong Kong

3Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education 

School of Artiﬁcial Intelligence  Xidian University  China

{yyliu  fhshang  jcheng}@cse.cuhk.edu.hk; hcheng@se.cuhk.edu.hk;

lchjiao@mail.xidian.edu.cn

Abstract

In this paper  we propose an accelerated ﬁrst-order method for geodesically convex
optimization  which is the generalization of the standard Nesterov’s accelerated
method from Euclidean space to nonlinear Riemannian space. We ﬁrst derive two
equations and obtain two nonlinear operators for geodesically convex optimization
instead of the linear extrapolation step in Euclidean space. In particular  we analyze
the global convergence properties of our accelerated method for geodesically
strongly-convex problems  which show that our method improves the convergence

rate from O((1−µ/L)k) to O((1−(cid:112)µ/L)k). Moreover  our method also improves

the global convergence rate on geodesically general convex problems from O(1/k)
to O(1/k2). Finally  we give a speciﬁc iterative scheme for matrix Karcher mean
problems  and validate our theoretical results with experiments.

1

Introduction

In this paper  we study the following Riemannian optimization problem:

such that x ∈ X ⊂ M 

min f (x)

(1)
where (M  ) denotes a Riemannian manifold with the Riemannian metric   X ⊂M is a nonempty 
compact  geodesically convex set  and f :X → R is geodesically convex (G-convex) and geodesically
L-smooth (G-L-smooth). Here  G-convex functions may be non-convex in the usual Euclidean space
but convex along the manifold  and thus can be solved by a global optimization solver. [5] presented
G-convexity and G-convex optimization on geodesic metric spaces  though without any attention
to global complexity analysis. As discussed in [11]  the topic of "geometric programming" may be
viewed as a special case of G-convex optimization. [25] developed theoretical tools to recognize
and generate G-convex functions as well as cone theoretic ﬁxed point optimization algorithms.
However  none of these three works provided a global convergence rate analysis for their algorithms.
Very recently  [31] provided the global complexity analysis of ﬁrst-order algorithms for G-convex
optimization  and designed the following Riemannian gradient descent rule:

xk+1 = Expxk

(−η gradf (xk)) 

where k is the iterate index  Expxk
step-size or learning rate  and gradf (xk) is the Riemannian gradient of f at xk ∈X .

is an exponential map at xk (see Section 2 for details)  η is a

∗Corresponding author.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

In this paper  we extend the Nesterov’s accelerated gradient descent method [19] from Euclidean
space to nonlinear Riemannian space. Below  we ﬁrst introduce the Nesterov’s method and its variants
for convex optimization on Euclidean space  which can be viewed as a special case of our method 
when M =Rd  and  is the Euclidean inner product. Nowadays many real-world applications involve
large data sets. As data sets and problems are getting larger in size  accelerating ﬁrst-order methods
is of both practical and theoretical interests. The earliest ﬁrst-order method for minimizing a convex
function f is perhaps the gradient method. Thirty years ago  Nesterov [19] proposed an accelerated
gradient method  which takes the following form: starting with x0 and y0 = x0  and for any k≥ 1 

xk = yk−1 − η∇f (yk−1) 
yk = xk + τk(xk − xk−1) 

k2

(2)
where 0 ≤ τk ≤ 1 is the momentum parameter. For a ﬁxed step-size η = 1/L  where L is the
Lipschitz constant of ∇f  this scheme with τk = (k−1)/(k+2) exhibits the optimal convergence
rate  f (xk)−f (x(cid:63)) ≤ O( L(cid:107)x(cid:63)−x0(cid:107)2
)  for general convex (or non-strongly convex) problems [20] 
where x(cid:63) is any minimizer of f. In contrast  standard gradient descent methods can only achieve
a convergence rate of O(1/k). We can see that this improvement relies on the introduction of the
momentum term τk(xk − xk−1) as well as the particularly tuned coefﬁcient (k−1)/(k+2)≈ 1−3/k.
Inspired by the success of the Nesterov’s momentum  there has been much work on the development
of ﬁrst-order accelerated methods  see [2  8  21  26  27] for example. In addition  for strongly convex

problems and setting τk ≡ (1−(cid:112)µ/L)/(1+(cid:112)µ/L)  Nesterov’s accelerated gradient method attains
a convergence rate of O((1−(cid:112)µ/L)k)  while standard gradient descent methods achieve a linear

convergence rate of O((1− µ/L)k). It is then natural to ask whether our accelerated method in
nonlinear Riemannian space has the same convergence rates as its Euclidean space counterparts (e.g. 
Nesterov’s accelerated method [20])?

1.1 Motivation and Challenges

Zhang and Sra [31] proposed an efﬁcient Riemannian gradient descent (RGD) method  which
attains the convergence rates of O((1 − µ/L)k) and O(1/k) for geodesically strongly-convex and
geodesically convex problems  respectively. Hence  there still remain gaps in convergence rates
between RGD and the Nesterov’s accelerated method.
As discussed in [31]  a long-time question is whether the famous Nesterov’s accelerated gradient
descent algorithm has a counterpart in nonlinear Riemannian spaces. Compared with standard
gradient descent methods in Euclidean space  Nesterov’s accelerated gradient method involves a
linear extrapolation step: yk = xk + τk(xk − xk−1)  which can improve its convergence rates for both
strongly convex and non-strongly convex problems. It is clear that ϕk(x) := f (yk)+(cid:104)∇f (yk)  x−yk(cid:105)
is a linear function in Euclidean space  while its counterpart in nonlinear space  e.g.  ϕk(x) :=
f (yk) + (cid:104)gradf (yk)  Exp−1
is the inverse of the
  and (cid:104)· ·(cid:105)y is the inner product (see Section 2 for details). Therefore  in
exponential map Expyk
nonlinear Riemannian spaces  there is no trivial analogy of such a linear extrapolation step. In
other words  although Riemannian geometry provides tools that enable generalization of Euclidean
algorithms mentioned above [1]  we must overcome some fundamental geometric hurdles to analyze
the global convergence properties of our accelerated method as in [31].

(x)(cid:105)yk  is a nonlinear function  where Exp−1

yk

yk

1.2 Contributions

To answer the above-mentioned open problem in [31]  in this paper we propose a general accelerated
ﬁrst-order method for nonlinear Riemannian spaces  which is in essence the generalization of the
standard Nesterov’s accelerated method. We summarize the key contributions of this paper as follows.
• We ﬁrst present a general Nesterov’s accelerated iterative scheme in nonlinear Riemannian
spaces  where the linear extrapolation step in (2) is replaced by a nonlinear operator. Fur-
thermore  we derive two equations and obtain two corresponding nonlinear operators for
both geodesically strongly-convex and geodesically convex cases  respectively.
• We provide the global convergence analysis of our accelerated algorithms  which shows

that our algorithms attain the convergence rates of O((1−(cid:112)µ/L)k) and O(1/k2) for

geodesically strongly-convex and geodesically convex objectives  respectively.

2

• Finally  we present a speciﬁc iterative scheme for matrix Karcher mean problems. Our

experimental results verify the effectiveness and efﬁciency of our accelerated method.

2 Notation and Preliminaries

w∈ TxM is deﬁned as (cid:107)w(cid:107)x =(cid:112)x(w  w)  where the metric  induces an inner product structure

We ﬁrst introduce some key notations and deﬁnitions about Riemannian geometry (see [23  30] for
details). A Riemannian manifold (M  ) is a real smooth manifold M equipped with a Riemannian
metric . Let (cid:104)w1  w2(cid:105)x = x(w1  w2) denote the inner product of w1  w2 ∈ TxM; and the norm of
in each tangent space TxM associated with every x ∈ M. A geodesic is a constant speed curve
γ : [0  1]→M that is locally distance minimizing. Let y ∈M and w ∈ TxM  then an exponential
map y = Expx(w) : TxM→M maps w to y on M  such that there is a geodesic γ with γ(0) = x 
γ(1) = y and ˙γ(0) = w. If there is a unique geodesic between any two points in X ⊂ M  the
exponential map has inverse Exp−1
x (y)  and the geodesic is the unique
y (x)(cid:107)y = d(x  y)  where d(x  y) is the geodesic distance
shortest path with (cid:107)Exp−1
between x  y ∈X . Parallel transport Γy
xw ∈ TyM 
x : TxM→ TyM maps a vector w ∈ TxM to Γy
and preserves inner products and norm  that is  (cid:104)w1  w2(cid:105)x =(cid:104)Γy
xw1(cid:107)y 
where w1  w2∈ TxM.
For any x  y ∈ X and any geodesic γ with γ(0) = x  γ(1) = y and γ(t) ∈ X for t ∈ [0  1] such
that f (γ(t)) ≤ (1 − t)f (x) + tf (y)  then f is geodesically convex (G-convex)  and an equivalent
deﬁnition is formulated as follows:

x :X → TxM  i.e.  w = Exp−1

x (y)(cid:107)x =(cid:107)Exp−1

xw1  Γy

xw2(cid:105)y and (cid:107)w1(cid:107)x =(cid:107)Γy

f (y) ≥ f (x) + (cid:104)gradf (x)  Exp−1

x (y)(cid:105)x 

where gradf (x) is the Riemannian gradient of f at x. A function f : X →R is called geodesically
µ-strongly convex (µ-strongly G-convex) if for any x  y∈X   the following inequality holds

f (y) ≥ f (x) + (cid:104)gradf (x)  Exp−1

x (y)(cid:105)x +

(cid:107)Exp−1

x (y)(cid:107)2
x.

µ
2

A differential function f is geodesically L-smooth (G-L-smooth) if its gradient is L-Lipschitz  i.e. 

f (y) ≤ f (x) + (cid:104)gradf (x)  Exp−1

x (y)(cid:105)x +

(cid:107)Exp−1

x (y)(cid:107)2
x.

L
2

3 An Accelerated Method for Geodesically Convex Optimization

In this section  we propose a general acceleration method for geodesically convex optimization 
which can be viewed as a generalization of the famous Nesterov’s accelerated method from Euclidean
space to Riemannian space. Nesterov’s accelerated method involves a linear extrapolation step as
in (2)  while in nonlinear Riemannian spaces  we do not have a simple way to ﬁnd an analogy to
such a linear extrapolation. Therefore  some standard analysis techniques do not work in nonlinear
space. Motivated by this  we derive two equations to bridge the gap for both geodesically strongly-
convex and geodesically convex cases  and then generalized Nesterov’s algorithms are proposed for
geodesically convex optimization by solving these two equations.
We ﬁrst propose to replace the classical Nesterov’s scheme in (2) with the following update rules for
geodesically convex optimization in Riemannian space:

(−η gradf (yk−1)) 

xk = Expyk−1
yk = S(yk−1  xk  xk−1) 

(3)

where yk  xk ∈X   S denotes a nonlinear operator  and yk = S(yk−1  xk  xk−1) can be obtained by
solving the two proposed equations (see (4) and (5) below  which can be used to deduce the key
analysis tools for our convergence analysis) for strongly G-convex and general G-convex cases  re-
(−ηgradf (xk))) 
spectively. Different from the Riemannian gradient descent rule (e.g.  xk+1 =Expxk
the Nesterov’s accelerated technique is introduced into our update rule of yk. Compared with the
Nesterov’s scheme in (2)  the main difference is the update rule of yk. That is  our update rule for yk
is an implicit iteration process as shown below  while that of (2) is an explicit iteration one.

3

Figure 1: Illustration of geometric interpretation for Equations (4) and (5).

Algorithm 1 Accelerated method for strongly G-convex optimization
Input: µ  L
Initialize: x0  y0  η.
1: for k = 1  2  . . .   K do
2:
3:
4:
5: end for
Output: xK

Computing the gradient at yk−1: gk−1 = gradf (yk−1);
xk = Expyk−1
yk = S(yk−1  xk  xk−1) by solving (4).

(−ηgk−1);

yk

√

(cid:16)

where β = 4/

yk gradf (yk) =

(xk) − βΓyk−1

3.1 Geodesically Strongly Convex Cases
We ﬁrst design the following equation with respect to yk ∈X for the µ-strongly G-convex case:

(cid:17)
(cid:17)3/2
(cid:16)
1 −(cid:112)µ/L
1 −(cid:112)µ/L
equation (4) for the strongly G-convex case  where uk = (1−(cid:112)µ/L)Exp−1
and wk−1 = (1−(cid:112)µ/L)3/2Exp−1

yk Exp−1
Γyk−1
(4)
µL−1/L > 0. Figure 1(a) illustrates the geometric interpretation of the proposed
(xk)  vk =−βgradf (yk) 
(xk−1). The vectors uk  vk ∈ TykM are parallel transported
to Tyk−1M  and the sum of their parallel translations is equal to wk−1 ∈ Tyk−1M  which means
that the equation (4) holds. We design an accelerated ﬁrst-order algorithm for solving geodesically
strongly-convex problems  as shown in Algorithm 1. In real applications  the proposed equation
(4) can be manipulated into simpler forms. For example  we will give a speciﬁc equation for the
averaging real symmetric positive deﬁnite matrices problem below.

Exp−1
yk−1

(xk−1) 

yk−1

yk

(cid:18) k

(cid:19)
(xk)−D(cid:98)gk

3.2 Geodesically Convex Cases
Let f be G-convex and G-L-smooth  the diameter of X be bounded by D (i.e.  maxx y∈X d(x  y) ≤
D)  the variable yk ∈ X can be obtained by solving the following equation:

k−1
α−1

Exp−1
yk−1

(xk−1)−D(cid:98)gk−1 +

(k+α−2)η

Γyk−1
yk

Exp−1

yk

=

α−1

where gk−1 = gradf (yk−1)  and (cid:98)gk = gk/(cid:107)gk(cid:107)yk  and α ≥ 3 is a given constant. Figure 1(b)

illustrates the geometric interpretation of the proposed equation (5) for the G-convex case  where
gk−1. We also present an accelerated ﬁrst-order
uk = k
algorithm for solving geodesically convex problems  as shown in Algorithm 2.

(xk)−D(cid:98)gk  and vk−1 = (k+α−2)η

α−1Exp−1

α − 1

α−1

yk

gk−1 

(5)

3.3 Key Lemmas
For the Nesterov’s accelerated scheme in (2) with τk = k−1
k+2 (for example  the general convex case)
in Euclidean space  the following result in [3  20] plays a key role in the convergence analysis of
Nesterov’s accelerated algorithm.

(cid:2)(cid:107)zk − x(cid:63)(cid:107)2 − (cid:107)zk+1− x(cid:63)(cid:107)2(cid:3) 

(6)

2

k+2

(cid:104)∇f (yk)  zk−x(cid:63)(cid:105) − η
2

(cid:107)∇f (yk)(cid:107)2 =

2

η(k+2)2

4

Algorithm 2 Accelerated method for general G-convex optimization
Input: L  D  α
Initialize: x0  y0  η.
1: for k = 1  2  . . .   K do
2:
3:
4:
5: end for
Output: xK

Computing the gradient at yk−1: gk−1 = gradf (yk−1) and ˆgk−1 = gk−1/(cid:107)gk−1(cid:107)yk−1;
xk = Expyk−1
yk = S(yk−1  xk  xk−1) by solving (5).

(−ηgk−1);

where zk = (k+2)yk/2 − (k/2)xk. Correspondingly  we can also obtain the following analysis tools
for our convergence analysis using the proposed equations (4) and (5). In other words  the following
equations (7) and (8) can be viewed as the Riemannian space counterparts of (6).
Lemma 1 (Strongly G-convex). If f : X → R is geodesically µ-strongly convex and G-L-smooth 
and {yk} satisﬁes the equation (4)  and zk is deﬁned as follows:

(cid:16)

(cid:17)
1 −(cid:112)µ/L

Then the following results hold:

zk =

Γyk−1
yk

−(cid:104)gradf (yk)  zk(cid:105)yk +

β
2

(zk − βgradf (yk)) =
(cid:107)gradf (yk)(cid:107)2
1
2β

=

yk

Exp−1

yk

(xk) ∈ TykM.
(cid:17)1/2
(cid:16)
1 −(cid:112)µ/L
(cid:16)
(cid:17)(cid:107)zk−1(cid:107)2
1 −(cid:112)µ/L

zk−1 

− 1
2β

(cid:107)zk(cid:107)2

yk

.

(7)

yk−1

For general G-convex objectives  we have the following result.
Lemma 2 (General G-convex). If f : X → R is G-convex and G-L-smooth  the diameter of X is
bounded by D  and {yk} satisﬁes the equation (5)  and zk is deﬁned as

zk =

k

α − 1

Exp−1

yk

Then the following results hold:

Γyk

yk+1

zk+1 = zk +

α−1
k+α−1

(cid:104)gradf (yk) −zk(cid:105)yk − η
2

(cid:107)gradf (yk)(cid:107)2

yk

(xk) − D(cid:98)gk ∈ TykM.
(cid:16)(cid:107)zk(cid:107)2

2(α−1)2
η(k+α−1)2

(k + α − 1)η

gradf (yk) 

α − 1

=

yk

(cid:17)

.

(8)

− (cid:107)zk+1(cid:107)2

yk+1

The proofs of Lemmas 1 and 2 are provided in the Supplementary Materials.

4 Convergence Analysis

In this section  we analyze the global convergence properties of the proposed algorithms (i.e. 
Algorithms 1 and 2) for both geodesically strongly convex and general convex problems.
Lemma 3. If f : X → R is G-convex and G-L-smooth for any x ∈ X   and {xk} is the sequence
produced by Algorithms 1 and 2 with η ≤ 1/L  then the following result holds:

f (xk+1) ≤ f (x) + (cid:104)gradf (yk)  −Exp−1

yk

(x)(cid:105)yk − η
2

(cid:107)gradf (yk)(cid:107)2

yk

.

The proof of this lemma can be found in the Supplementary Materials. For the geodesically strongly
convex case  we have the following result.
Theorem 1 (Strongly G-convex). Let x(cid:63) be the optimal solution of Problem (1)  and {xk} be the
sequence produced by Algorithm 1. If f : X → R is geodesically µ-strongly convex and G-L-smooth 
then the following result holds

f (xk+1) − f (x(cid:63)) ≤(cid:16)

(cid:17)k(cid:20)
1 −(cid:112)µ/L

(cid:16)

1 −(cid:112)µ/L

(cid:17)(cid:107)z0(cid:107)2

(cid:21)

 

y0

f (x0) − f (x(cid:63)) +

1
2β

where z0 is deﬁned in Lemma 1.

5

Table 1: Comparison of convergence rates for geodesically convex optimization algorithms.

Algorithms
Strongly G-convex and smooth
General G-convex and smooth

O(cid:0)(1 − min{ 1
(cid:17)
(cid:16) c

RGD [31]
c   µ

L})k(cid:1)

O

c + k

(cid:16)

(cid:17)

RSGD [31]

O (1/k)
√
k

1/

O

Ours

O(cid:0)(1 −(cid:112) µ
L )k(cid:1)
O(cid:0)1/k2(cid:1)

that the proposed algorithm attains a linear convergence rate of O((1−(cid:112)µ/L)k) for geodesically

The proof of Theorem 1 can be found in the Supplementary Materials. From this theorem  we can see

strongly convex problems  which is the same as that of its Euclidean space counterparts and signiﬁ-
cantly faster than that of non-accelerated algorithms such as [31] (i.e.  O((1−µ/L)k))  as shown in
Table 1. For the geodesically non-strongly convex case  we have the following result.
Theorem 2 (General G-convex). Let {xk} be the sequence produced by Algorithm 2. If f :X → R
is G-convex and G-L-smooth  and the diameter of X is bounded by D  then

where z0 = −D(cid:98)g0  as deﬁned in Lemma 2.

f (xk+1) − f (x(cid:63)) ≤

(α − 1)2

2η(k + α − 2)2(cid:107)z0(cid:107)2

y0

 

The proof of Theorem 2 can be found in the Supplementary Materials. Theorem 2 shows that for
general G-convex objectives  our acceleration method improves the theoretical convergence rate from
O(1/k) (e.g.  RGD [31]) to O(1/k2)  which matches the optimal rate for general convex settings in
Euclidean space. Please see the detail in Table 1  where the parameter c is deﬁned in [31].

5 Application for Matrix Karcher Mean Problems

In this section  we give a speciﬁc accelerated scheme for a type of conic geometric optimization
problems [25]  e.g.  the matrix Karcher mean problem. Speciﬁcally  the loss function of the Karcher
mean problem for a set of N symmetric positive deﬁnite (SPD) matrices {Wi}N

i=1 is deﬁned as

N(cid:88)

i=1

f (X) :=

1
2N

(cid:107)log(X−1/2WiX−1/2)(cid:107)2
F  

(9)

where X ∈ P := {Z ∈ Rd×d  s.t.  Z = Z T (cid:31) 0}. The loss function f is known to be non-convex
in Euclidean space but geodesically 2N-strongly convex. The inner product of two tangent vectors at
point X on the manifold is given by

(10)
where tr(·) is the trace of a real square matrix. For any matrices X  Y ∈ P  the Riemannian distance
is deﬁned as follows:

(cid:104)ζ  ξ(cid:105)X = tr(ζX−1ξX−1)  ζ  ξ ∈ TXP 

d(X  Y ) = (cid:107)log(X− 1

2 Y X− 1

2 )(cid:107)F .

5.1 Computation of Yk

For the accelerated update rules in (3) for Algorithm 1  we need to compute Yk via solving the
equation (4). However  for the speciﬁc problem in (9) with the inner product in (10)  we can derive a
simpler form to solve Yk below. We ﬁrst give the following properties:
Property 1. For the loss function f in (9) with the inner product in (10)  we have

−1/2
log(Y
k

k

Yk

1. Exp−1

(Xk) = Y 1/2

(cid:80)N
(Xk)(cid:11)
3. (cid:10)gradf (Yk)  Exp−1

2. gradf (Yk) = 1
N

i=1 Y 1/2

k

Yk

Yk

log(Y 1/2
= (cid:104)U  V (cid:105);

−1/2
XkY
k

)Y 1/2

k

;

k W −1

i Y 1/2

k

)Y 1/2

k

;

4. (cid:107)gradf (Yk)(cid:107)2

Yk

= (cid:107)U(cid:107)2
F  

6

(cid:80)N

i=1log(Y 1/2

k W −1

i Y 1/2

k

where U = 1
N

) ∈ Rd×d  and V = log(Y

−1/2
k

−1/2
XkY
k

) ∈ Rd×d.

Proof. In this part  we only provide the proof of Result 1 in Property 1  and the proofs of the other
results are provided in the Supplementary Materials. The inner product in (10) on the Riemannian
manifold leads to the following exponential map:

(11)
where ξX ∈ TXP denotes the tangent vector with the geometry  and tangent vectors ξX are expressed
as follows (see [17] for details):

ExpX (ξX ) = X

2 )X

2  

1

2 exp(X− 1

2 ξX X− 1

1

ξX = X

1

2 sym(∆)X

1

2   ∆ ∈ Rd×d 

where sym(·) extracts the symmetric part of its argument  that is  sym(A) = (AT +A)/2. Then we
can set Exp−1
(Xk)  we have
(Exp−1
ExpYk

∈ TYkP. By the deﬁnition of Exp−1

(Xk) = Y 1/2
(Xk)) = Xk  that is 

sym(∆Xk )Y 1/2

Yk

Yk

k

k

Yk

ExpYk

(Y 1/2

k

sym(∆Xk )Y 1/2

k

) = Xk.

(12)

Using (11) and (12)  we have

−1/2
sym(∆Xk ) = log(Y
k

−1/2
XkY
k

) ∈ Rd×d.

Therefore  we have

Exp−1

Yk

(Xk) = Y 1/2

k

sym(∆Xk )Y 1/2

k = Y 1/2

k

−1/2
log(Y
k

−1/2
XkY
k

k = −Yk log(X−1
)Y 1/2

k Yk) 

where the last equality holds due to the fact that log(X−1Y X) = X−1 log(Y )X.

Result 3 in Property 1 shows that the inner product of two tangent vectors at Yk is equal to the
Euclidean inner-product of two vectors U  V ∈ Rd×d. Thus  we can reformulate (4) as follows:

N(cid:88)

i=1

(cid:18)

(cid:114) µ

(cid:19) 3

2

L

(cid:18)

(cid:114) µ

(cid:19)

1−

L

log(Y
√
where β = 4/
k W −1
log(Y
i Y

1
2

− 1
− 1
k XkY
k

2

2

) − β
N

log(Y

1
2

k W −1
i Y

1
2

k ) =

1−

log(Y

− 1
k−1X−1

k−1Y

2

− 1
k−1) 

2

(13)
µL−1/L. Then Yk can be obtained by solving (13). From a numerical perspective 
k ) can be approximated by log(Y

k−1)  and then Yk is given by

1
2

1
2

(cid:34)(cid:18)

(cid:114) µ

(cid:19) 1
where δ = 1/(1−(cid:112)µ/L)  and Yk ∈ P.

k exp−1

Yk = X

1 −

L

1
2

2

1
2

k−1W −1
i Y
− 1
k−1) +

2

N(cid:88)

δβ
N

log(Y

− 1
k−1Xk−1Y

2

(cid:35)

log(Y

1
2

k−1W −1
i Y

1
2

k−1)

X

1
2

k  

i=1

(14)

6 Experiments

In this section  we validate the performance of our accelerated method for averaging SPD matrices
under the Riemannian metric  e.g.  the matrix Karcher mean problem (9)  and also compare our
method against the state-of-the-art methods: Riemannian gradient descent (RGD) [31] and limited-
memory Riemannian BFGS (LRBFGS) [29]. The matrix Karcher mean problem has been widely
applied to many real-world applications such as elasticity [18]  radar signal and image processing [6 
15  22]  and medical imaging [9  7  13]. In fact  this problem is geodesically strongly convex  but
non-convex in Euclidean space.
Other methods for solving this problem include the relaxed Richardson iteration algorithm [10] 
the approximated joint diagonalization algorithm [12]  and Riemannian stochastic gradient descent
(RSGD) [31]. Since all the three methods achieve similar performance to RGD  especially in data
science applications where N is large and relatively small optimization error is not required [31]  we
only report the experimental results of RGD. The step-size η of both RGD and LRBFGS is selected
with a line search method as in [29] (see [29] for details)  while η of our accelerated method is set to
1/L. For the algorithms  we initialize X using the arithmetic mean of the data set as in [29].

7

Figure 2: Comparison of RGD  LRBFGS and our accelerated method for solving geodesically
strongly convex Karcher mean problems on data sets with d = 100 (the ﬁrst row) and d = 200 (the
second row). The vertical axis represents the distance in log scale  and the horizontal axis denotes the
number of iterations (left) or running time (right).

The input synthetic data are random SPD matrices of size 100×100 or 200×200 generated by using
the technique in [29] or the matrix mean toolbox [10]  and all matrices are explicitly normalized so
that their norms are all equal to 1. We report the experimental results of RGD  LRBFGS and our
accelerated method on the two data sets in Figure 2  where N is set to 100  and the condition number
C of each matrix {Wi}N
i=1 is set to 102. Figure 2 shows the evolution of the distance between the
exact Karcher mean and current iterate (i.e.  dist(X∗  Xk)) of the methods with respect to number
of iterations and running time (seconds)  where X∗ is the exact Karcher mean. We can observe that
our method consistently converges much faster than RGD  which empirically veriﬁes our theoretical
result in Theorem 1 that our accelerated method has a much faster convergence rate than RGD.
Although LRBFGS outperforms our method in terms of number of iterations  our accelerated method
converges much faster than LRBFGS in terms of running time.

7 Conclusions

In this paper  we proposed a general Nesterov’s accelerated gradient method for nonlinear Riemannian
space  which is a generalization of the famous Nesterov’s accelerated method for Euclidean space.
We derived two equations and presented two accelerated algorithms for geodesically strongly-convex
and general convex optimization problems  respectively. In particular  our theoretical results show
that our accelerated method attains the same convergence rates as the standard Nesterov’s accelerated
method in Euclidean space for both strongly G-convex and G-convex cases. Finally  we presented a
special iteration scheme for solving matrix Karcher mean problems  which in essence is non-convex
in Euclidean space  and the numerical results verify the efﬁciency of our accelerated method.
We can extend our accelerated method to the stochastic setting using variance reduction techniques [14 
16  24  28]  and apply our method to solve more geodesically convex problems in the future  e.g. 
the general G-convex problem with a non-smooth regularization term as in [4]. In addition  we
can replace exponential mapping by computationally cheap retractions together with corresponding
theoretical guarantees [31]. An interesting direction of future work is to design accelerated schemes
for non-convex optimization in Riemannian space.

8

020406010−1010−5100Numberofiterationsdist(X∗ Xk) RGDLRBFGSOurs0510152010−1010−5100Runningtime(s)dist(X∗ Xk) RGDLRBFGSOurs020406010−1010−5100Numberofiterationsdist(X∗ Xk) RGDLRBFGSOurs02040608010010−1010−5100Runningtime(s)dist(X∗ Xk) RGDLRBFGSOursAcknowledgments

This research is supported in part by Grants (CUHK 14206715 & 14222816) from the Hong Kong
RGC  the Major Research Plan of the National Natural Science Foundation of China (Nos. 91438201
and 91438103)  and the National Natural Science Foundation of China (No. 61573267).

References
[1] P.-A. Absil  R. Mahony  and R. Sepulchre. Optimization algorithms on matrix manifolds.

Princeton University Press  Princeton  N.J.  2009.

[2] Z. Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In STOC 

pages 1200–1205  2017.

[3] H. Attouch and J. Peypouquet. The rate of convergence of Nesterov’s accelerated forward-

backward method is actually faster than 1/k2. SIAM J. Optim.  26:1824–1834  2015.

[4] D. Azagra and J. Ferrera. Inf-convolution and regularization of convex functions on Riemannian

manifolds of nonpositive curvature. Rev. Mat. Complut.  2006.

[5] M. Bacak. Convex analysis and optimization in Hadamard spaces. Walter de Gruyter GmbH &

Co KG  2014.

[6] F. Barbaresco. New foundation of radar Doppler signal processing based on advanced differential

geometry of symmetric spaces: Doppler matrix CFAR radar application. In RADAR  2009.

[7] P. G. Batchelor  M. Moakher  D. Atkinson  F. Calamante  and A. Connelly. A rigorous framework

for diffusion tensor calculus. Magn. Reson. Med.  53:221–225  2005.

[8] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM J. Imaging Sci.  2(1):183–202  2009.

[9] R. Bhatia. Positive deﬁnite matrices  Princeton Series in Applied Mathematics. Princeton

University Press  Princeton  N.J.  2007.

[10] D. A. Bini and B. Iannazzo. Computing the Karcher mean of symmetric positive deﬁnite

matrices. Linear Algebra Appl.  438:1700–1710  2013.

[11] S. Boyd  S.-J. Kim  L. Vandenberghe  and A. Hassibi. A tutorial on geometric programming.

Optim. Eng.  8:67–127  2007.

[12] M. Congedo  B. Afsari  A. Barachant  and M. Moakher. Approximate joint diagonalization and

geometric mean of symmetric positive deﬁnite matrices. PloS one  10:e0121423  2015.

[13] P. T. Fletcher and S. Joshi. Riemannian geometry for the statistical analysis of diffusion tensor

data. Signal Process.  87:250–262  2007.

[14] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In NIPS  pages 315–323  2013.

[15] J. Lapuyade-Lahorgue and F. Barbaresco. Radar detection using Siegel distance between

autoregressive processes  application to HF and X-band radar. In RADAR  2008.

[16] Y. Liu  F. Shang  and J. Cheng. Accelerated variance reduced stochastic ADMM. In AAAI 

pages 2287–2293  2017.

[17] G. Meyer  S. Bonnabel  and R. Sepulchre. Regression on ﬁxed-rank positive semideﬁnite

matrices: A Riemannian approach. J. Mach. Learn. Res.  12:593–625  2011.

[18] M. Moakher. On the averaging of symmetric positive-deﬁnite tensors. J. Elasticity  82:273–296 

2006.

[19] Y. Nesterov. A method of solving a convex programming problem with convergence rate

O(1/k2). Soviet Mathematics Doklady  27:372–376  1983.

9

[20] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic

Publ.  Boston  2004.

[21] Y. Nesterov. Gradient methods for minimizing composite functions. Math. Program.  140:125–

161  2013.

[22] X. Pennec  P. Fillard  and N. Ayache. A Riemannian framework for tensor computing. Interna-

tional Journal of Computer Vision  66:41–66  2006.

[23] P. Petersen. Riemannian Geometry. Springer-Verlag  New York  2016.

[24] F. Shang. Larger is better: The effect of learning rates enjoyed by stochastic optimization with

progressive variance reduction. arXiv:1704.04966  2017.

[25] S. Sra and R. Hosseini. Conic geometric optimization on the manifold of positive deﬁnite

matrices. SIAM J. Optim.  25(1):713–739  2015.

[26] W. Su  S. Boyd  and E. J. Candes. A differential equation for modeling Nesterov’s accelerated

gradient method: Theory and insights. J. Mach. Learn. Res.  17:1–43  2016.

[27] P. Tseng. On aacelerated proximal gradient methods for convex-concave optimization. 2008.

[28] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM J. Optim.  24(4):2057–2075  2014.

[29] X. Yuan  W. Huang  P.-A. Absil  and K. Gallivan. A Riemannian limited-memory BFGS
algorithm for computing the matrix geometric mean. Procedia Computer Science  80:2147–
2157  2016.

[30] H. Zhang  S. Reddi  and S. Sra. Riemannian SVRG: Fast stochastic optimization on Riemannian

manifolds. In NIPS  pages 4592–4600  2016.

[31] H. Zhang and S. Sra. First-order methods for geodesically convex optimization. In COLT  pages

1617–1638  2016.

10

,Chris Junchi Li
Zhaoran Wang
Han Liu
Yuanyuan Liu
Fanhua Shang
James Cheng
Hong Cheng
Licheng Jiao