2011,Iterative Learning for Reliable Crowdsourcing Systems,Crowdsourcing systems  in which tasks are electronically distributed to numerous ``information piece-workers''  have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification  data entry  optical character recognition  recommendation  and proofreading. Because these low-paid workers can be unreliable  nearly all crowdsourcers must devise schemes to increase confidence in their answers  typically by assigning each task multiple times and combining the answers in some way such as majority voting. In this paper  we consider a general model of such  rowdsourcing tasks  and pose the problem of minimizing the total price (i.e.  number of task assignments) that must be paid to achieve a target overall reliability. We give new algorithms for deciding which tasks to assign to which workers and for inferring correct answers from the workers’ answers. We show that our algorithm significantly outperforms majority voting and  in fact  are asymptotically optimal through comparison to an oracle that knows the reliability of every worker.,Iterative Learning for Reliable Crowdsourcing

Systems

David R. Karger
Department of Electrical Engineering and Computer Science

Sewoong Oh

Devavrat Shah

Massachusetts Institute of Technology

Abstract

Crowdsourcing systems  in which tasks are electronically distributed to numerous
“information piece-workers”  have emerged as an effective paradigm for human-
powered solving of large scale problems in domains such as image classiﬁcation 
data entry  optical character recognition  recommendation  and proofreading. Be-
cause these low-paid workers can be unreliable  nearly all crowdsourcers must
devise schemes to increase conﬁdence in their answers  typically by assigning
each task multiple times and combining the answers in some way such as ma-
jority voting. In this paper  we consider a general model of such crowdsourcing
tasks  and pose the problem of minimizing the total price (i.e.  number of task as-
signments) that must be paid to achieve a target overall reliability. We give a new
algorithm for deciding which tasks to assign to which workers and for inferring
correct answers from the workers’ answers. We show that our algorithm signiﬁ-
cantly outperforms majority voting and  in fact  is asymptotically optimal through
comparison to an oracle that knows the reliability of every worker.

1

Introduction

Background. Crowdsourcing systems have emerged as an effective paradigm for human-powered
problem solving and are now in widespread use for large-scale data-processing tasks such as image
classiﬁcation  video annotation  form data entry  optical character recognition  translation  recom-
mendation  and proofreading. Crowdsourcing systems such as Amazon Mechanical Turk provide a
market where a “taskmaster” can submit batches of small tasks to be completed for a small fee by
any worker choosing to pick them up. For example  a worker may be able to earn a few cents by indi-
cating which images from a set of 30 are suitable for children (one of the beneﬁts of crowdsourcing
is its applicability to such highly subjective questions).
Since typical crowdsourced tasks are tedious and the reward is small  errors are common even among
workers who make an effort. At the extreme  some workers are “spammers”  submitting arbitrary
answers independent of the question in order to collect their fee. Thus  all crowdsourcers need
strategies to ensure the reliability of answers. Because the worker crowd is large  anonymous  and
transient  it is generally difﬁcult to build up a trust relationship with particular workers.1 It is also
difﬁcult to condition payment on correct answers  as the correct answer may never truly be known
and delaying payment can annoy workers and make it harder to recruit them for your future tasks.
Instead  most crowdsourcers resort to redundancy  giving each task to multiple workers  paying
them all irrespective of their answers  and aggregating the results by some method such as majority
voting. For such systems there is a natural core optimization problem to be solved. Assuming the
e-mail: {karger swoh devavrat}@mit.edu. This work was supported in parts by AFOSR com-

plex networks project  MURI on network tomography  and National Science Foundation.

1For certain high-value tasks  crowdsourcers can use entrance exams to “prequalify” workers and block
spammers  but this increases the cost and still provides no guarantee that the prequaliﬁed workers will try hard.

1

taskmaster wishes to achieve a certain reliability in their answers  how can they do so at minimum
cost (which is equivalent to asking how they can do so while asking the fewest possible questions)?
Several characteristics of crowdsourcing systems make this problem interesting. Workers are neither
persistent nor identiﬁable; each batch of tasks will be solved by a worker who may be completely
new and who you may never see again. Thus one cannot identify and reuse particularly reliable
workers. Nonetheless  by comparing one worker’s answer to others’ on the same question  it is
possible to draw conclusions about a worker’s reliability  which can be used to weight their answers
to other questions in their batch. However  batches must be of manageable size  obeying limits on
the number of tasks that can be given to a single worker. Another interesting aspect of this problem
is the choice of task assignments. Unlike many inference problems which makes inferences based
on a ﬁxed set of signals  our algorithm can choose which signals to measure by deciding which
questions to ask which workers.
In the following  we ﬁrst deﬁne a formal model that captures these aspects of the problem. We will
then describe a scheme for deciding which tasks to assign to which workers and introduce a novel
iterative algorithm to infer the correct answers from the workers’ responses.
Setup. We model a set of m tasks {ti}i∈[m] as each being associated with an unobserved ‘correct’
answer si ∈ {±1}. Here and after  we use [N ] to denote the set of ﬁrst N integers. In the earlier im-
age categorization example  each task corresponds to labeling an image as suitable for children (+1)
or not (−1). We assign these tasks to n workers from the crowd  which we denote by {wj}j∈[n].
When a task is assigned to a worker  we get a possibly inaccurate answer from the worker. We
use Aij ∈ {±1} to denote the answer if task ti is assigned to worker wj. Some workers are more
diligent or have more expertise than others  while some other workers might be spammers. We
choose a simple model to capture this diversity in workers’ reliability: we assume that each worker
wj is characterized by a reliability pj ∈ [0  1]  and that they make errors randomly on each question
they answer. Precisely  if task ti is assigned to worker wj then

(cid:26) si with probability pj  

−si with probability 1 − pj  

Aij =

and Aij = 0 if ti is not assigned to wj. The random variable Aij is independent of any other
event given pj. (Throughout this paper  we use boldface characters to denote random variables and
random matrices unless it is clear from the context.) The underlying assumption here is that the
error probability of a worker does not depend on the particular task and all the tasks share an equal
level of difﬁculty. Hence  each worker’s performance is consistent across different tasks.
We further assume that the reliability of workers {pj}j∈[n] are independent and identically dis-
tributed random variables with a given distribution on [0  1]. One example is spammer-hammer
model where  each worker is either a ‘hammer’ with probability q or is a ‘spammer’ with probability
1 − q. A hammer answers all questions correctly  in which case pj = 1  and a spammer gives
random answers  in which case pj = 1/2. Given this random variable pj  we deﬁne an important
parameter q ∈ [0  1]  which captures the ‘average quality’ of the crowd: q ≡ E[(2pj − 1)2]. A
value of q close to one indicates that a large proportion of the workers are diligent  whereas q close
to zero indicates that there are many spammers in the crowd. The deﬁnition of q is consistent with
use of q in the spammer-hammer model. We will see later that our bound on the error rate of our
inference algorithm holds for any distribution of pj but depends on the distribution only through
this parameter q. It is quite realistic to assume the existence of a prior distribution for pj. The model
is therefore quite general: in particular  it is met if we simply randomize the order in which we up-
load our task batches  since this will have the effect of randomizing which workers perform which
batches  yielding a distribution that meets our requirements. On the other hand  it is not realistic
to assume that we know what the prior is. To execute our inference algorithm for a given number
of iterations  we do not require any knowledge of the distribution of the reliability. However  q is
necessary in order to determine how many times a task should be replicated and how many iterations
we need to run to achieve certain reliability.
Under this crowdsourcing model  a taskmaster ﬁrst decides which tasks should be assigned to which
workers  and then estimates the correct solutions {si}i∈[m] once all the answers {Aij} are submit-
ted. We assume a one-shot scenario in which all questions are asked simultaneously and then an
estimation is performed after all the answers are obtained. In particular  we do not allow allocating

2

tasks adaptively based on the answers received thus far. Then  assigning tasks to nodes amounts to
designing a bipartite graph G({ti}i∈[m] ∪ {wj}j∈[n]  E) with m task and n worker nodes. Each
edge (i  j) ∈ E indicates that task ti was assigned to worker wj.
Prior Work. A naive approach to identify the correct answer from multiple workers’ responses is to
use majority voting. Majority voting simply chooses what the majority of workers agree on. When
there are many spammers  majority voting is error-prone since it weights all the workers equally.
We will show that majority voting is provably sub-optimal and can be signiﬁcantly improved upon.
To infer the answers of the tasks and also the reliability of workers  Dawid and Skene [1  2] proposed
an algorithm based on expectation maximization (EM) [3]. This approach has also been applied in
classiﬁcation problems where the training data is annotated by low-cost noisy ‘labelers’ [4  5]. In
[6] and [7]  this EM approach has been applied to more complicated probabilistic models for image
labeling tasks. However  the performance of these approaches are only empirically evaluated  and
there is no analysis that proves performance guarantees. In particular  EM algorithms require an
initial starting point which is typically randomly guessed. The algorithm is highly sensitive to this
initialization  making it difﬁcult to predict the quality of the resulting estimate. The advantage of
using low-cost noisy ‘labelers’ has been studied in the context of supervised learning  where a set
of labels on a training set is used to ﬁnd a good classiﬁer. Given a ﬁxed budget  there is a trade-
off between acquiring a larger training dataset or acquiring a smaller dataset but with more labels
per data point. Through extensive experiments  Sheng  Provost and Ipeirotis [8] show that getting
repeated labeling can give considerable advantage.

Contributions. In this work  we provide a rigorous treatment of designing a crowdsourcing sys-
tem with the aim of minimizing the budget to achieve completion of a set of tasks with a certain
reliability. We provide both an asymptotically optimal graph construction (random regular bipartite
graph) and an asymptotically optimal algorithm for inference (iterative algorithm) on that graph.
As the main result  we show that our algorithm performs as good as the best possible algorithm.
The surprise lies in the fact that the optimality of our algorithm is established by comparing it with
the best algorithm  one that is free to choose any graph  regular or irregular  and performs optimal
estimation based on the information provided by an oracle about reliability of the workers. Previous
approaches focus on developing inference algorithms assuming that a graph is already given. None
of the prior work on crowdsourcing provides any systematic treatment of the graph construction.
To the best of our knowledge  we are the ﬁrst to study both aspects of crowdsourcing together and 
more importantly  establish optimality.
Another novel contribution of our work is the analysis technique. The iterative algorithm we in-
troduce operates on real-valued messages whose distribution is a priori difﬁcult to analyze. To
overcome this challenge  we develop a novel technique of establishing that these messages are sub-
Gaussian (see Section 3 for a deﬁnition) using recursion  and compute the parameters in a closed
form. This allows us to prove the sharp result on the error rate  and this technique could be of
independent interest for analyzing a more general class of algorithms.

2 Main result

Under the crowdsourcing model introduced  we want to design algorithms to assign tasks and es-
timate the answers. In what follows  we explain how to assign tasks using a random regular graph
and introduce a novel iterative algorithm to infer the correct answers. We state the performance
guarantees for our algorithm and provide comparisons to majority voting and an oracle estimator.

Task allocation. Assigning tasks amounts to designing a bipartite graph G(cid:0){ti}i∈[m] ∪
{wj}j∈[n]  E(cid:1)  where each edge corresponds to a task-worker assignment. The taskmaster makes a

choice of how many workers to assign to each task (the left degree l) and how many tasks to assign
to each worker (the right degree r). Since the total number of edges has to be consistent  the number
of workers n directly follows from ml = nr. To generate an (l  r)-regular bipartite graph we use
a random graph generation scheme known as the conﬁguration model in random graph literature
[9  10]. In principle  one could use arbitrary bipartite graph G for task allocation. However  as we
show later in this paper  random regular graphs are sufﬁcient to achieve order-optimal performance.

3

Inference algorithm. We introduce a novel iterative algorithm which operates on real-valued task
messages {xi→j}(i j)∈E and worker messages {yj→i}(i j)∈E. The worker messages are initialized
as independent Gaussian random variables. At each iteration  the messages are updated according
to the described update rule  where ∂i is the neighborhood of ti. Intuitively  a worker message yj→i
represents our belief on how ‘reliable’ the worker j is  such that our ﬁnal estimate is a weighted sum

of the answers weighted by each worker’s reliability: ˆsi = sign((cid:80)

j∈∂i Aijyj→i).

Output: Estimation ˆs(cid:0){Aij}(cid:1)

Iterative Algorithm
Input: E  {Aij}(i j)∈E  kmax
1: For all (i  j) ∈ E do

2: For k = 1  . . .   kmax do

j→i with random Zij ∼ N (1  1) ;

Initialize y(0)
For all (i  j) ∈ E do x(k)
For all (i  j) ∈ E do
y(k)
j∈∂i Aijy(kmax−1)

3: For all i ∈ [m] do xi ←(cid:80)
4: Output estimate vector ˆs(cid:0){Aij}(cid:1) = [sign(xi)] .

i→j ←(cid:80)
j→i ←(cid:80)

j(cid:48)∈∂i\j Aij(cid:48)y(k−1)
j(cid:48)→i
i(cid:48)∈∂j\i Ai(cid:48)jx(k)
i(cid:48)→j ;
j→i

;

;

While our algorithm is inspired by the standard Belief Propagation (BP) algorithm for approximat-
ing max-marginals [11  12]  our algorithm is original and overcomes a few critical limitations of the
standard BP. First  the iterative algorithm does not require any knowledge of the prior distribution of
pj  whereas the standard BP requires the knowledge of the distribution. Second  there is no efﬁcient
way to implement standard BP  since we need to pass sufﬁceint statistics (or messages) which under
our general model are distributions over the reals. On the otherhand  the iterative algorithm only
passes messages that are real numbers regardless of the prior distribution of pj  which is easy to im-
plement. Third  the iterative algorithm is provably asymptotically order-optimal. Density evolution 
explained in detail in Section 3  is a standard technique to analyze the performance of BP. Although
we can write down the density evolution for the standard BP  we cannot analyze the densities  ana-
lytically or numerically. It is also very simple to write down the density evolution equations for the
iterative algorithm  but it is not trivial to analyze the densities in this case either. We develop a novel
technique to analyze the densities and prove optimality of our algorithm.

2.1 Performance guarantee

We state the main analytical result of this paper: for random (l  r)-regular bipartite graph based task
assignments with our iterative inference algorithm  the probability of error decays exponentially in
lq  up to a universal constant and for a broad range of the parameters l  r and q. With a reasonable
choice of l = r and both scaling like (1/q) log(1/)  the proposed algorithm is guarantted to achieve
error less than  for any  ∈ (0  1/2). Further  an algorithm independent lower bound that we
establish suggests that such an error dependence on lq is unavoidable. Hence  in terms of the task
allocation budget  our algorithm is order-optimal. The precise statements follow next. Let µ =
E[2pj − 1] and recall q = E[(2pj − 1)2]. To lighten the notation  let ˆl ≡ l− 1 and ˆr ≡ r− 1. Deﬁne

(cid:16)

(cid:17) 1 − (1/q2ˆlˆr)k−1

1 − (1/q2ˆlˆr)

1
qˆr

.

k ≡
ρ2

2q

+

3 +

µ2(q2ˆlˆr)k−1
k such that

ρ2∞ = (cid:0)3 + (1/qˆr)(cid:1)q2ˆlˆr/(q2ˆlˆr − 1) .

For q2ˆlˆr > 1  let ρ2∞ ≡ limk→∞ ρ2

Then we can show the following bound on the probability of making an error.
Theorem 2.1. For ﬁxed l > 1 and r > 1  assume that m tasks are assigned to n = ml/r workers
according to a random (l  r)-regular graph drawn from the conﬁguration model. If the distribution
of the worker reliaiblity satisfy µ ≡ E[2pj − 1] > 0 and q2 > 1/(ˆlˆr)  then for any s ∈ {±1}m  the
estimates from k iterations of the iterative algorithm achieve

m(cid:88)

P(cid:16)

(cid:0){Aij}(i j)∈E

(cid:1)(cid:17) ≤ e−lq/(2ρ2

k) .

si (cid:54)= ˆsi

(1)

lim sup
m→∞

1
m

i=1

4

As we increase k  the above bound converges to a non-trivial limit.
Corollary 2.2. Under the hypotheses of Theorem 2.1 

lim sup
k→∞

lim sup
m→∞

1
m

(cid:0){Aij}(i j)∈E

(cid:1)(cid:17) ≤ e−lq/(2ρ2∞) .

si (cid:54)= ˆsi

m(cid:88)

P(cid:16)

i=1

One implication of this corollary is that  under a mild assumption that r ≥ l  the probabiilty of
error is upper bounded by e−(1/8)(lq−1). Even if we ﬁx the value of q = E[(2pj − 1)2]  different
q]. Surprisingly  the asymptotic
distributions of pj can have different values of µ in the range of [q 
bound on the error rate does not depend on µ. Instead  as long as q is ﬁxed  µ only affects how fast
the algorithm converges (cf. Lemma 2.3).
Notice that the bound in (2) is only meaningful when it is less than a half  whence ˆlˆrq2 > 1 and
lq > 6 log(2) > 4. While as a task master the case of ˆlˆrq2 < 1 may not be of interest  for the purpose
of completeness we comment on the performance of our algorithm in this regime. Speciﬁcally  we
empirically observe that the error rate increases as the number of iterations k increases. Therefore 
it makes sense to use k = 1. In which case  the algorithm essentially boils down to the majority
rule. We can prove the following error bound. The proof is omitted due to a space constraint.

√

(cid:0){Aij}(i j)∈E

(cid:1)(cid:17) ≤ e−lµ2/4 .

si (cid:54)= ˆsi

m(cid:88)

P(cid:16)

i=1

lim sup
m→∞

1
m

2.2 Discussion

(2)

(3)

√

Here we make a few comments relating to the execution of the algorithm and the interpretation of
the main results. First  the iterative algorithm is efﬁcient with runtime comparable to the simple
majority voting which requires O(ml) operations.
Lemma 2.3. Under the hypotheses of Theorem 2.1  the total computational cost sufﬁcient to achieve
the bound in Corollary 2.2 up to any constant factor in the exponent is O(ml log(q/µ2)/ log(q2ˆlˆr)).
By deﬁnition  we have q ≤ µ ≤ √
q. The runtime is the worst when µ = q  which happens under
the spammer-hammer model  and it is the best when µ =
q)/2
deterministically. There exists a (non-iterative) polynomial time algorithm with runtime independent
of q for computing the estimate which achieves (2)  but in practice we expect that the number
of iterations needed is small enough that the iterative algorithm will outperform this non-iterative
algorithm. Detailed proof of Lemma 2.3 will be skipped here due to a space constraint.
Second  the assumption that µ > 0 is necessary. If there is no assumption on µ  then we cannot
distinguish if the responses came from tasks with {si}i∈[m] and workers with {pj}j∈[n] or tasks
with {−si}i∈[m] and workers with {1 − pj}j∈[n]. Statistically  both of them give the same output.
In the case when we know that µ < 0  we can use the same algorithm changing the sign of the ﬁnal
output and get the same performance guarantee.
Third  our algorithm does not require any information on the distribution of pj. Further  unlike
other EM based algorithms  the iterative algorithm is not sensitive to initialization and with random
initialization converges to a unique estimate with high probability. This follows from the fact that
the algorithm is essentially computing a leading eigenvector of a particular linear operator.

q which happens if pj = (1 +

√

2.3 Relation to singular value decomposition

The leading singular vectors are often used to capture the important aspects of datasets in matrix
form. In our case  the leading left singular vector of A can be used to estimate the correct answers 
where A ∈ {0 ±1}m×n is the m × n adjacency matrix of the graph G weighted by the submitted
answers. We can compute it using power iteration: for u ∈ Rm and v ∈ Rn  starting with a
randomly initialized v  power iteration iteratively updates u and v according to

for all i  ui =

(cid:88)

j∈∂i

(cid:88)

i∈∂j

Aijui

.

Aijvj  and for all j  vj =

5

It is known that normalized u converges exponentially to the leading left singular vector. This update
rule is very similar to that of our iterative algorithm. But there is one difference that is crucial in the
analysis: in our algorithm we follow the framework of the celebrated belief propagation algorithm
[11  12] and exclude the incoming message from node j when computing an outgoing message
to j. This extrinsic nature of our algorithm and the locally tree-like structure of sparse random
graphs [9  13] allow us to perform asymptotic analysis on the average error rate. In particular  if we
use the leading singular vector of A to estimate s  such that si = sign(ui)  then existing analysis
techniques from random matrix theory does not give the strong performance guarantee we have.
These techniques typically focus on understanding how the subspace spanned by the top singular
vector behaves. To get a sharp bound  we need to analyze how each entry of the leading singular
vector is distributed. We introduce the iterative algorithm in order to precisely characterize how
each of the decision variable xi is distributed. Since the iterative algorithm introduced in this paper
is quite similar to power iteration used to compute the leading singular vectors  this suggests that
our analysis may shed light on how to analyze the top singular vectors of a sparse random matrix.

2.4 Optimality of our algorithm

As a taskmaster  the natural core optimization problem of our concern is how to achieve a certain
reliability in our answers with minimum cost. Since we pay equal amount for all the task assign-
ments  the cost is proportional to the total number of edges of the graph G. Here we compute the
total budget sufﬁcient to achieve a target error rate using our algorithm and show that this is within a
constant factor from the necessary budget to achieve the given target error rate using any graph and
the best possible inference algorithm. The order-optimality is established with respect to all algo-
rithms that operate in one-shot  i.e. all task assignments are done simultaneously  then an estimation
is performed after all the answers are obtained. The proofs of the claims in this section are skipped
here due to space limitations.
Formally  consider a scenario where there are m tasks to complete and a target accuracy  ∈ (0  1/2).
To measure accuracy  we use the average probability of error per task denoted by dm(s  ˆs) ≡
sary and sufﬁcient to achieve the target error rate: dm(s  ˆs) ≤ . To establish this fundamental limit 
we use the following minimax bound on error rate. Consider the case where nature chooses a set of
correct answers s ∈ {±1}m and a distribution of the worker reliability pj ∼ f. The distribution f
is chosen from a set of all distributions on [0  1] which satisfy Ef [(2pj − 1)2] = q. We use F(q) to
denote this set of distributions. Let G(m  l) denote the set of all bipartite graphs  including irregular
graphs  that have m task nodes and ml total number of edges. Then the minimax error rate achieved
by the best possible graph G ∈ G(m  l) using the best possible inference algorithm is at least

P(si (cid:54)= ˆsi). We will show that Ω(cid:0)(1/q) log(1/)(cid:1) assignments per task is neces-

(1/m)(cid:80)

i∈[m]

inf

ALGO G∈G(m l)

sup

s f∈F (q)

dm(s  ˆsG ALGO) ≥ (1/2)e−(lq+O(lq2))  

(4)

where ˆsG ALGO denotes the estimate we get using graph G for task allocation and algorithm ALGO
for inference. This minimax bound is established by computing the error rate of an oracle esitimator 
which makes an optimal decision based on the information provided by an oracle who knows how
reliable each worker is. Next  we show that the error rate of majority voting decays signiﬁcantly
slower: the leading term in the error exponent scales like −lq2. Let ˆsMV be the estimate produced
by majority voting. Then  for q ∈ (0  1)  there exists a numerical constant C1 such that

inf

G∈G(m l)

sup

s f∈F (q)

dm(s  ˆsMV) = e−(C1lq2+O(lq4+1)) .

(5)

The lower bound in (4) does not depend on how many tasks are assigned to each worker. However 
our main result depends on the value of r. We show that for a broad range of parameters l  r  and q
our algorithm achieves optimality. Let ˆsIter be the estimate given by random regular graphs and the
iterative algorithm. For ˆlq ≥ C2  ˆrq ≥ C3 and C2C3 > 1  Corollary 2.2 gives

lim
m→∞ sup

s f∈F (q)

dm(s  ˆsIter) ≤ e−C4lq .

(6)

This is also illustrated in Figure 1. We ran numerical experiments with 1000 tasks and 1000 work-
ers from the spammer-hammer model assigned according to random graphs with l = r from the
conﬁguration model. For the left ﬁgure  we ﬁxed q = 0.3 and for the right ﬁgure we ﬁxed l = 25.

6

PError

PError

l

q

Figure 1: The iterative algorithm improves over majority voting and EM algorithm [8].

Now  let ∆LB be the minimum cost per task necessary to achieve a target accuracy  ∈ (0  1/2)
using any graph and any possible algorithm. Then (4) implies ∆LB ∼ (1/q) log(1/)  where x ∼ y
indicates that x scales as y. Let ∆Iter be the minimum cost per task sufﬁcient to achieve a target
accuracy  using our proposed algorithm. Then from (6) we get ∆Iter ∼ (1/q) log(1/). This
establishes the order-optimality of our algorithm.
It is indeed surprising that regular graphs are
sufﬁcient to achieve this optimality. Further  let ∆Majority be the minimum cost per task necessary
to achieve a target accuracy  using the Majority voting. Then ∆Majority ∼ (1/q2) log(1/)  which
signiﬁcantly more costly than the optimal scaling of (1/q) log(1/) of our algorithm.

3 Proof of Theorem 2.1

(1/m)(cid:80)

i∈[m]

P(si (cid:54)= ˆsi) ≤ P(x(k)

By symmetry  we can assume all si’s are +1. If I is a random integer drawn uniformly in [m]  then
denotes the decision variable for task i after
k iterations of the iterative algorithm. Asymptotically  for a ﬁxed k  l and r  the local neighborhood
I ≤ 0)  we use a standard proba-
of xI converges to a regular tree. To analyze limm→∞ P(x(k)
bilistic analysis technique known as ‘density evolution’ in coding theory or ‘recursive distributional
equations’ in probabilistic combinatorics [9  13]. Precisely  we use the following equality.

I ≤ 0)  where x(k)

i

P(x(k)

I ≤ 0) = P(ˆx(k) ≤ 0)  

lim
m→∞

(7)

where ˆx(k) is deﬁned through density evolution equations (8) and (9) in the following.
Density evolution. In the large system limit as m → ∞  the (l  r)-regular random graph locally
converges in distribution to a (l  r)-regular tree. Therefore  for a randomly chosen edge (i  j)  the
messages xi→j and yj→i converge in distribution to x and ypj deﬁned in the following density
evolution equations (8). Here and after  we drop the superscript k denoting the iteration number
whenever it is clear from the context. We initialize yp with a Gaussian distribution independent of
p: y(0)

p ∼ N (1  1). Let d= denote equality in distribution. Then  for k ∈ {1  2  . . .} 

x(k) d=

zpi iy(k−1)

pi i

 

y(k)

p

d=

zp jx(k)

j

 

(8)

(cid:88)

j∈[r−1]

(cid:88)

i∈[l−1]

where xj’s  pi’s  and yp i’s are independent copies of x  p  and yp  respectively. Also  zp i’s
and zp j’s are independent copies of zp. p ∈ [0  1] is a random variable distributed according to
the distribution of the worker’s quality. zp j’s and xj’s are independent. zpi i’s and ypi i’s are
conditionally independent conditioned on pi. Finally  zp is a random variable which is +1 with
probability p and −1 with probability 1 − p. Then  for a randomly chosen I  the decision variable
x(k)
I

converges in distribution to

ˆx(k) d=

zpi iy(k−1)

pi i

.

(9)

(cid:88)

i∈[l]

Analyzing the density. Our strategy to provide an upper bound on P(ˆx(k) ≤ 0) is to show
that ˆx(k) is sub-Gaussian with appropriate parameters and use the Chernoff bound. A random

7

 1e-05 0.0001 0.001 0.01 0.1 1 0 5 10 15 20 25 30Majority VotingEM AlgorithmIterative AlgorithmLower Bound 1e-06 1e-05 0.0001 0.001 0.01 0.1 1 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4Majority VotingEM AlgorithmIterative AlgorithmLower Boundvariable z with mean m is said to be sub-Gaussian with parameter σ if for all λ ∈ R the fol-
lowing inequality holds: E[eλz] ≤ emλ+(1/2)σ2λ2. Deﬁne σ2
k ≡ 2ˆl(ˆlˆr)k−1 + µ2ˆl3ˆr(3qˆr +
1)(qˆlˆr)2k−4(1 − (1/q2ˆlˆr)k−1)/(1 − (1/q2ˆlˆr)) and mk ≡ µˆl(qˆlˆr)k−1 for k ∈ Z. We will ﬁrst
show that  x(k) is sub-Gaussian with mean mk and parameter σ2
k for a regime of λ we are interested
in. Precisely  we will show that for |λ| ≤ 1/(2mk−1ˆr) 

E[eλx(k)

] ≤ emkλ+(1/2)σ2

kλ2

.

(10)

] = E[eλx(k)

](l/ˆl). Therefore  it
kλ2. Applying the Chernoff bound

By deﬁnition  due to distributional independence  we have E[eλˆx(k)
] ≤ e(l/ˆl)mkλ+(l/2ˆl)σ2
follows from (10) that ˆx(k) satisﬁes E[eλˆx(k)
with λ = −mk/(σ2

P(cid:0)ˆx(k) ≤ 0(cid:1) ≤ E(cid:2)eλˆx(k)(cid:3) ≤ e−l m2

k)  we get

k/(2 ˆl σ2

(11)
k) ≤ µ2ˆl2(qˆlˆr)2k−3/(3µ2qˆl3ˆr2(qˆlˆr)2k−4) = 1/(3ˆr)  it is easy to check that

Since mkmk−1/(σ2
|λ| ≤ 1/(2mk−1ˆr). Substituting (11) in (7)  this ﬁnishes the proof of Theorem 2.1.
Now we are left to prove that x(k) is sub-Gaussian with appropriate parameters. We can write down
a recursive formula for the evolution of the moment generating functions of x and yp as

k)  

E(cid:2)eλx(k)(cid:3) =
E(cid:2)eλy(k)
p (cid:3) =

(cid:16)Ep
(cid:104)
(cid:16)
pE(cid:2)eλx(k)(cid:3) + ¯pE(cid:2)e−λx(k)(cid:3)(cid:17)ˆr

pE[eλy(k−1)

p

 

|p] + ¯pE[e−λy(k−1)

p

|p]

(cid:105)(cid:17)ˆl

 

(12)

(13)

p ] = eλ+(1/2)λ2 regardless of p. Substituting this into (12)  we get for any λ  E[eλx(1)

where ¯p = 1 − p and ¯p = 1 − p. We can prove that these are sub-Gaussian using induction.
1 = 2ˆl 
First  for k = 1  we show that x(1) is sub-Gaussian with mean m1 = µˆl and parameter σ2
where µ ≡ E[2p − 1]. Since yp is initialized as Gaussian with unit mean and variance  we have
E[eλy(0)
] =
(E[p]eλ + (1 − E[p])e−λ)ˆle(1/2)ˆlλ2 ≤ eˆlµλ+ˆlλ2  where the inequality follows from the fact that
aez + (1 − a)e−z ≤ e(2a−1)z+(1/2)z2 for any z ∈ R and a ∈ [0  1] (cf. [14  Lemma A.1.5]).
Next  assuming E[eλx(k)
emk+1λ+(1/2)σ2
the bound E[eλx(k)
Further applying this bound in (12)  we get

k+1λ2 for |λ| ≤ 1/(2mk ˆr)  and compute appropriate mk+1 and σ2

kλ2 for |λ| ≤ 1/(2mk−1ˆr)  we show that E[eλx(k+1)

] ≤
k+1. Substituting
kλ2.

p ] ≤ (pemkλ + ¯pe−mkλ)ˆre(1/2)ˆrσ2

kλ2 in (13)  we get E[eλy(k)

] ≤ emkλ+(1/2)σ2

] ≤ emkλ+(1/2)σ2

E(cid:104)
eλx(k+1)(cid:105) ≤ (cid:16)Ep

(cid:104)
p(pemkλ + ¯pe−mkλ)ˆr + ¯p(pe−mkλ + ¯pemkλ)ˆr(cid:105)(cid:17)ˆl
p(pez + ¯pe−z)ˆr + ¯p(¯pez + pe−z)ˆr(cid:105) ≤ eqˆrz+(1/2)(3qˆr2+ˆr)z2

To bound the ﬁrst term in the right-hand side  we use the next key lemma.
Lemma 3.1. For any |z| ≤ 1/(2ˆr) and p ∈ [0  1] such that q = E[(2p − 1)2]  we have

e(1/2)ˆlˆrσ2

] ≤ eqˆlˆrmkλ+(1/2)(cid:0)(3qˆlˆr2+ˆlˆr)m2

For the proof  we refer to the journal version of this paper. Applying this inequality to (14) gives
E[eλx(k+1)
1 as per our assumption  mk is non-decreasing in k. At iteration k  the above recursion holds for
|λ| ≤ min{1/(2m1ˆr)  . . .   1/(2mk−1ˆr)} = 1/(2mk−1ˆr). Hence  we get a recursion for mk and
σk such that (10) holds for |λ| ≤ 1/(2mk−1ˆr):

(cid:1)λ2  for |λ| ≤ 1/(2mk ˆr). In the regime where qˆlˆr ≥

k+ˆlˆrσ2

.(14)

Ep

(cid:104)

kλ2

.

k

mk+1 = qˆlˆrmk  

k+1 = (3qˆlˆr2 + ˆlˆr)m2
σ2

k + ˆlˆrσ2
k .

1 = 2ˆl  we have mk = µˆl(qˆlˆr)k−1 for k ∈ {1  2  . . .} and
k−1 + bck−2 for k ∈ {2  3  . . .}  with a = ˆlˆr  b = µ2ˆl2(3qˆlˆr2 + ˆlˆr)  and c = (qˆlˆr)2. After
(cid:96)=0 (a/c)(cid:96). For ˆlˆrq2 (cid:54)= 1  we have a/c (cid:54)= 1 

1ak−1 + bck−2(cid:80)k−2

With the initialization m1 = µˆl and σ2
σ2
k = aσ2
some algebra  it follows that σ2
whence σ2

1ak−1 + bck−2(1 − (a/c)k−1)/(1 − a/c). This ﬁnishes the proof of (10).

k = σ2

k = σ2

8

References
[1] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the
em algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics)  28(1):20–
28  1979.

[2] P. Smyth  U. Fayyad  M. Burl  P. Perona  and P. Baldi. Inferring ground truth from subjective
labelling of venus images. In Advances in neural information processing systems  pages 1085–
1092  1995.

[3] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via
the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological)  39(1):pp.
1–38  1977.

[4] R. Jin and Z. Ghahramani. Learning with multiple labels. In Advances in neural information

processing systems  pages 921–928  2003.

[5] V. C. Raykar  S. Yu  L. H. Zhao  G. H. Valadez  C. Florin  L. Bogoni  and L. Moy. Learning

from crowds. J. Mach. Learn. Res.  99:1297–1322  August 2010.

[6] J. Whitehill  P. Ruvolo  T. Wu  J. Bergsma  and J. Movellan. Whose vote should count more:
In Advances in Neural

Optimal integration of labels from labelers of unknown expertise.
Information Processing Systems  volume 22  pages 2035–2043  2009.

[7] P. Welinder  S. Branson  S. Belongie  and P. Perona. The multidimensional wisdom of crowds.

In Advances in Neural Information Processing Systems  pages 2424–2432  2010.

[8] V. S. Sheng  F. Provost  and P. G. Ipeirotis. Get another label? improving data quality and data
mining using multiple  noisy labelers. In Proceeding of the 14th ACM SIGKDD international
conference on Knowledge discovery and data mining  KDD ’08  pages 614–622. ACM  2008.
[9] T. Richardson and R. Urbanke. Modern Coding Theory. Cambridge University Press  march

2008.

[10] B. Bollob´as. Random Graphs. Cambridge University Press  January 2001.
[11] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann Publ.  San Mateo 

Califonia  1988.

[12] J. S. Yedidia  W. T. Freeman  and Y. Weiss. Understanding belief propagation and its gen-
eralizations  pages 239–269. Morgan Kaufmann Publishers Inc.  San Francisco  CA  USA 
2003.

[13] M. Mezard and A. Montanari.

Information  Physics  and Computation. Oxford University

Press  Inc.  New York  NY  USA  2009.

[14] N. Alon and J. H. Spencer. The Probabilistic Method. John Wiley  2008.

9

,Maria-Florina Balcan
Tuomas Sandholm
Ellen Vitercik