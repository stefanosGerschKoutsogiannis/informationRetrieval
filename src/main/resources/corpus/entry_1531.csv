2014,Discovering Structure in High-Dimensional Data Through Correlation Explanation,We introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective. Intuitively  the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information. The method is unsupervised  requires no model assumptions  and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems. We demonstrate that Correlation Explanation (CorEx) automatically discovers meaningful structure for data from diverse sources including personality tests  DNA  and human language.,Discovering Structure in High-Dimensional Data

Through Correlation Explanation

Greg Ver Steeg

Information Sciences Institute

University of Southern California

Marina del Rey  CA 90292

gregv@isi.edu

Aram Galstyan

Information Sciences Institute

University of Southern California

Marina del Rey  CA 90292
galstyan@isi.edu

Abstract

We introduce a method to learn a hierarchy of successively more abstract repre-
sentations of complex data based on optimizing an information-theoretic objec-
tive. Intuitively  the optimization searches for a set of latent factors that best ex-
plain the correlations in the data as measured by multivariate mutual information.
The method is unsupervised  requires no model assumptions  and scales linearly
with the number of variables which makes it an attractive approach for very high
dimensional systems. We demonstrate that Correlation Explanation (CorEx) auto-
matically discovers meaningful structure for data from diverse sources including
personality tests  DNA  and human language.

Introduction

1
Without any prior knowledge  what can be automatically learned from high-dimensional data? If
the variables are uncorrelated then the system is not really high-dimensional but should be viewed
as a collection of unrelated univariate systems. If correlations exist  however  then some common
cause or causes must be responsible for generating them. Without assuming any particular model for
these hidden common causes  is it still possible to reconstruct them? We propose an information-
theoretic principle  which we refer to as “correlation explanation”  that codiﬁes this problem in a
model-free  mathematically principled way. Essentially  we are searching for latent factors so that 
conditioned on these factors  the correlations in the data are minimized (as measured by multivariate
mutual information). In other words  we look for the simplest explanation that accounts for the most
correlations in the data. As a bonus  building on this information-based foundation leads naturally to
an innovative paradigm for learning hierarchical representations that is more tractable than Bayesian
structure learning and provides richer insights than neural network inspired approaches [1].
After introducing the principle of “Correlation Explanation” (CorEx) in Sec. 2  we show that it can
be efﬁciently implemented in Sec. 3. To demonstrate the power of this approach  we begin Sec. 4
with a simple synthetic example and show that standard learning techniques all fail to detect high-
dimensional structure while CorEx succeeds. In Sec. 4.2.1  we show that CorEx perfectly reverse
engineers the “big ﬁve” personality types from survey data while other approaches fail to do so. In
Sec. 4.2.2  CorEx automatically discovers in DNA nearly perfect predictors of independent signals
relating to gender  geography  and ethnicity.
In Sec. 4.2.3  we apply CorEx to text and recover
both stylistic features and hierarchical topic representations. After brieﬂy considering intriguing
theoretical connections in Sec. 5  we conclude with future directions in Sec. 6.

2 Correlation Explanation
Using standard notation [2]  capital X denotes a discrete random variable whose instances are writ-
ten in lowercase. A probability distribution over a random variable X  pX(X = x)  is shortened

1

to p(x) unless ambiguity arises. The cardinality of the set of values that a random variable can
take will always be ﬁnite and denoted by |X|. If we have n random variables  then G is a subset
of indices G ✓ Nn = {1  . . .   n} and XG is the corresponding subset of the random variables
(XNn is shortened to X). Entropy is deﬁned in the usual way as H(X) ⌘ EX[ log p(x)]. Higher-
order entropies can be constructed in various ways from this standard deﬁnition. For instance  the
mutual information between two random variables  X1 and X2 can be written I(X1 : X2) =
H(X1) + H(X2)  H(X1  X2).
The following measure of mutual information among many variables was ﬁrst introduced as “total
correlation” [3] and is also called multi-information [4] or multivariate mutual information [5].

T C(XG) =Xi2G

H(Xi)  H(XG)

(1)

For G = {i1  i2}  this corresponds to the mutual information  I(Xi1 : Xi2). T C(XG) is non-
negative and zero if and only if the probability distribution factorizes. In fact  total correlation can
also be written as a KL divergence  T C(XG) = DKL(p(xG)||Qi2G p(xi)).
is simply T C(X|Y ) =Pi H(Xi|Y )  H(X|Y ). We can measure the extent to which Y explains

The total correlation among a group of variables  X  after conditioning on some other variable  Y  

the correlations in X by looking at how much the total correlation is reduced.

T C(X; Y ) ⌘ T C(X)  T C(X|Y ) = Xi2Nn

I(Xi : Y )  I(X : Y )

(2)

We use semicolons as a reminder that T C(X; Y ) is not symmetric in the arguments  unlike mutual
information. T C(X|Y ) is zero (and T C(X; Y ) maximized) if and only if the distribution of X’s
conditioned on Y factorizes. This would be the case if Y were the common cause of all the Xi’s
in which case Y explains all the correlation in X. T C(XG|Y ) = 0 can also be seen as encoding
local Markov properties among a group of variables and  therefore  specifying a DAG [6]. This
quantity has appeared as a measure of the redundant information that the Xi’s carry about Y [7].
More connections are discussed in Sec. 5.
Optimizing over Eq. 2 can now be seen as a search for a latent factor  Y   that explains the correlations
in X. We can make this concrete by letting Y be a discrete random variable that can take one of k
possible values and searching over all probabilistic functions of X  p(y|x).

T C(X; Y )

max
p(y|x)

s.t.

|Y | = k 

(3)

The solution to this optimization is given as a special case in Sec. A. Total correlation is a functional
over the joint distribution  p(x  y) = p(y|x)p(x)  so the optimization implicitly depends on the data
through p(x). Typically  we have only a small number of samples drawn from p(x) (compared to
the size of the state space). To make matters worse  if x 2{ 0  1}n then optimizing over all p(y|x)
involves at least 2n variables. Surprisingly  despite these difﬁculties we show in the next section
that this optimization can be carried out efﬁciently. The maximum achievable value of this objective
occurs for some ﬁnite k when T C(X|Y ) = 0. This implies that the data are perfectly described by
a naive Bayes model with Y as the parent and Xi as the children.
Generally  we expect that correlations in data may result from several different factors. Therefore 
we extend the optimization above to include m different factors  Y1  . . .   Ym.1

max

Gj  p(yj|xGj )

mXj=1

T C(XGj ; Yj)

s.t.

|Yj| = k  Gj \ Gj06=j = ;

(4)

Here we simultaneously search subsets of variables Gj and over variables Yj that explain the cor-
relations in each group. While it is not necessary to make the optimization tractable  we impose
an additional condition on Gj so that each variable Xi is in a single group  Gj  associated with a
single “parent”  Yj. The reason for this restriction is that it has been shown that the value of the
objective can then be interpreted as a lower bound on T C(X) [8]. Note that this objective is valid

1Note that in principle we could have just replaced Y in Eq. 3 with (Y1  . . .   Ym)  but the state space would

have been exponential in m  leading to an intractable optimization.

2

and meaningful regardless of details about the data-generating process. We only assume that we are
given p(x) or iid samples from it.
The output of this procedure gives us Yj’s  which are probabilistic functions of X. If we itera-
tively apply this optimization to the resulting probability distribution over Y by searching for some
Z1  . . .   Z ˜m that explain the correlations in the Y ’s  we will end up with a hierarchy of variables
that forms a tree. We now show that the optimization in Eq. 4 can be carried out efﬁciently even for
high-dimensional spaces and small numbers of samples.

3 CorEx: Efﬁcient Implementation of Correlation Explanation
We begin by re-writing the optimization in Eq. 4 in terms of mutual informations using Eq. 2.

max

G p(yj|x)

I(Yj : Xi) 

I(Yj : XGj )

Next  we replace G with a set indicator variable  ↵i j = I[Xi 2 Gj] 2{ 0  1}.

mXj=1 Xi2Gj
nXi=1
mXj=1

mXj=1
mXj=1

(5)

(6)

max

↵ p(yj|x)

↵i jI(Yj : Xi) 

I(Yj : X)

The non-overlapping group constraint is enforced by demanding thatP¯j ↵i ¯j = 1. Note also that

we dropped the subscript Gj in the second term of Eq. 6 but this has no effect because solutions
must satisfy I(Yj : X) = I(Yj : XGj )  as we now show.
For ﬁxed ↵  it is straightforward to ﬁnd the solution of the Lagrangian optimization problem as the
solution to a set of self-consistent equations. Details of the derivation can be found in Sec. A.

p(yj|x) =

Zj(x)

1

p(yj)

nYi=1✓ p(yj|xi)

p(yj) ◆↵i j

p(yj|xi) =X¯x

p(yj|¯x)p(¯x)¯xi xi/p(xi) and p(yj) =X¯x

(7)

(8)

p(yj|¯x)p(¯x)

Note that  is the Kronecker delta and that Yj depends only on the Xi for which ↵i j is non-zero.
Remarkably  Yj’s dependence on X can be written in terms of a linear (in n  the number of variables)
number of parameters which are just the marginals  p(yj)  p(yj|xi). We approximate p(x) with the
empirical distribution  ˆp(¯x) =PN
l=1 ¯x x(l)/N. This approximation allows us to estimate marginals
with ﬁxed accuracy using only a constant number of iid samples from the true distribution. In Sec. A
we show that Eq. 7  which deﬁnes the soft labeling of any x  can be seen as a linear function followed
by a non-linear threshold  reminiscent of neural networks. Also note that the normalization constant
for any x  Zj(x)  can be calculated easily by summing over just |Yj| = k values.
For ﬁxed values of the parameters p(yj|xi)  we have an integer linear program for ↵ made easy by
the constraintP¯j ↵i ¯j = 1. The solution is ↵⇤i j = I[j = arg max¯j I(Xi : Y¯j)]. However  this leads
to a rough optimization space. The solution in Eq. 7 is valid (and meaningful  see Sec. 5 and [8]) for
arbitrary values of ↵ so we relax our optimization accordingly. At step t = 0 in the optimization 
i j ⇠U (1/2  1) uniformly at random (violating the constraints). At step t + 1  we make
we pick ↵t=0
a small update on ↵ in the direction of the solution.
↵t+1
i j = (1  )↵t
(9)

The second term  ↵⇤⇤i j = exp(I(Xi : Yj)  max¯j I(Xi : Y¯j))  implements a soft-max which
converges to the true solution for ↵⇤ in the limit  ! 1. This leads to a smooth optimization and
good choices for    can be set through intuitive arguments described in Sec. B.
Now that we have rules to update both ↵ and p(yj|xi) to increase the value of the objective  we
simply iterate between them until we achieve convergence. While there is no guarantee to ﬁnd the
global optimum  the objective is upper bounded by T C(X) (or equivalently  T C(X|Y ) is lower
bounded by 0). Pseudo-code for this approach is described in Algorithm 1 with additional details
provided in Sec. B and source code available online2. The overall complexity is linear in the number

i j + ↵⇤⇤i j

2Open source code is available at http://github.com/gregversteeg/CorEx.

3

: Set m  the number of latent variables  Yj  and k  so that |Yj| = k
for i 2 Nn  j 2 Nm  l 2 Nns  y 2 Nk  xi 2X i

input : A matrix of size ns ⇥ n representing ns samples of n discrete random variables
set
output: Parameters ↵i j  p(yj|xi)  p(yj)  p(y|x(l))
Randomly initialize ↵i j  p(y|x(l));
repeat

Estimate marginals  p(yj)  p(yj|xi) using Eq. 8;
Calculate I(Xi : Yj) from marginals;
Update ↵ using Eq. 9;
Calculate p(y|x(l))  l = 1  . . .   ns using Eq. 7;

until convergence;

Algorithm 1: Pseudo-code implementing Correlation Explanation (CorEx)

of variables. To bound the complexity in terms of the number of samples  we can always use mini-
batches of ﬁxed size to estimate the marginals in Eq. 8.
A common problem in representation learning is how to pick m  the number of latent variables
to describe the data. Consider the limit in which we set m = n. To use all Y1  . . .   Ym in our
representation  we would need exactly one variable  Xi  in each group  Gj. Then 8j  T C(XGj ) = 0
and  therefore  the whole objective will be 0. This suggests that the maximum value of the objective
must be achieved for some value of m < n. In practice  this means that if we set m too high 
only some subset of latent variables will be used in the solution  as we will demonstrate in Fig. 2.
In other words  if m is set high enough  the optimization will result in some number of clusters
m0 < m that is optimal with respect to the objective. Representations with different numbers of
layers  different m  and different k can be compared according to how tight of a lower bound they
provide on T C(X) [8].

4 Experiments

4.1 Synthetic data

Layer
2

Z

Y1

Y...

Yb

X1

X...

Xc

X...

Xn

Synthetic model

1

0

Figure 1: (Left) We compare methods to recover the clusters of variables generated according to the
model. (Right) Synthetic data is generated according to a tree of latent variables.

To test CorEx’s ability to recover latent structure from data we begin by generating synthetic data
according to the latent tree model depicted in Fig. 1 in which all the variables are hidden except
for the leaf nodes. The most difﬁcult part of reconstructing this tree is clustering of the leaf nodes.
If a clustering method can do that then the latent variables can be reconstructed for each cluster
easily using EM. We consider many different clustering methods  typically with several variations

4

(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)2425262728292102110.00.20.40.60.81.0#ObservedVariables nAccuracy(ARI)CorEx(cid:1)CorEx(cid:2)Spectral*(cid:3)K-means(cid:4)ICA(cid:5)NMF*(cid:6)N.Net:RBM*(cid:7)PCA(cid:8)SpectralBi*(cid:9)Isomap*(cid:10)LLE*(cid:1)Hierarch.*(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:3)(cid:2)(cid:7)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:10)(cid:2)(cid:12)(cid:2)(cid:14)(cid:4)(cid:9)(cid:4)(cid:11)(cid:5)(cid:14)(cid:3)of each technique  details of which are described in Sec. C. We use the adjusted Rand index (ARI)
to measure the accuracy with which inferred clusters recover the ground truth. 3
We generated samples from the model in Fig. 1 with b = 8 and varied c  the number of leaves per
branch. The Xi’s depend on Yj’s through a binary erasure channel (BEC) with erasure probability
. The capacity of the BEC is 1  so we let  = 1 2/c to reﬂect the intuition that the signal from
each parent node is weakly distributed across all its children (but cannot be inferred from a single
child). We generated max(200  2n) samples. In this example  all the Yj’s are weakly correlated
with the root node  Z  through a binary symmetric channel with ﬂip probability of 1/3.
Fig. 1 shows that for a small to medium number of variables  all the techniques recover the structure
fairly well  but as the dimensionality increases only CorEx continues to do so. ICA and hierarchical
clustering compete for second place. CorEx also perfectly recovers the values of the latent factors
in this example. For latent tree models  recovery of the latent factors gives a global optimum of the
objective in Eq. 4. Even though CorEx is only guaranteed to ﬁnd local optima  in this example it
correctly converges to the global optimum over a range of problem sizes.
Note that a growing literature on latent tree learning attempts to reconstruct latent trees with the-
oretical guarantees [9  1]. In principle  we should compare to these techniques  but they scale as
O(n2)  O(n5) (see [3]  Table 1) while our method is O(n). In a recent survey on latent tree learn-
ing methods  only one out of 15 techniques was able to run on the largest dataset considered (see
[3]  Table 3)  while most of the datasets in this paper are orders of magnitude larger than that one.

t = 0

i = 1  . . .   nv

t = 10

j
=
1
...
m

↵i j

I(Yj : Xi)

t = 50

0 

1 

Uncorrelated
variables

Figure 2: (Color online) A visualization of structure learning in CorEx  see text for details.

Fig. 2 visualizes the structure learning process.4 This example is similar to that above but includes
some uncorrelated random variables to show how they are treated by CorEx. We set b = 5 clusters
of variables but we used m = 10 hidden variables. At each iteration  t  we show which hidden vari-
ables  Yj  are connected to input variables  Xi  through the connectivity matrix  ↵ (shown on top).
The mutual information is shown on the bottom. At the beginning  we started with full connectivity 
but with nothing learned we have I(Yj : Xi) = 0. Over time  the hidden units “compete” to ﬁnd
a group of Xi’s for which they can explain all the correlations. After only ten iterations the overall
structure appears and by 50 iterations it is exactly described. At the end  the uncorrelated random
variables (Xi’s) and the hidden variables (Yj’s) which have not explained any correlations can be
easily distinguished and discarded (visually and mathematically  see Sec. B).

4.2 Discovering Structure in Diverse Real-World Datasets
4.2.1 Personality Surveys and the “Big Five” Personality Traits
One psychological theory suggests that there are ﬁve traits that largely reﬂect the differences in
personality types [1]: extraversion  neuroticism  agreeableness  conscientiousness and openness to
experience. Psychologists have designed various instruments intended to measure whether individ-
uals exhibit these traits. We consider a survey in which subjects rate ﬁfty statements  such as  “I
am the life of the party”  on a ﬁve point scale: (1) disagree  (2) slightly disagree  (3) neutral  (4)
slightly agree  and (5) agree.5 The data consist of answers to these questions from about ten thou-
sand test-takers. The test was designed with the intention that each question should belong to a

3Rand index counts the percentage of pairs whose relative classiﬁcation matches in both clusterings. ARI
adds a correction so that a random clustering will give a score of zero  while an ARI of 1 corresponds to a
perfect match.

4A video is available online at http://isi.edu/˜gregv/corex_structure.mpg.
5Data and full

list of questions are available at http://personality-testing.info/

_rawdata/.

5

cluster according to which personality trait the question gauges. Is it true that there are ﬁve factors
that strongly predict the answers to these questions?
CorEx learned a two-level hierarchical representation when applied to this data (full model shown
in Fig. C.2). On the ﬁrst level  CorEx automatically determined that the questions should cluster
into ﬁve groups. Surprisingly  the ﬁve clusters exactly correspond to the big ﬁve personality traits
as labeled by the test designers. It is unusual to recover the ground truth with perfect accuracy on
an unsupervised learning problem so we tried a number of other standard clustering methods to see
if they could reproduce this result. We display the results using confusion matrices in Fig. 3. The
details of the techniques used are described in Sec. C but all of them had an advantage over CorEx
since they required that we specify the correct number of clusters. None of the other techniques are
able to recover the ﬁve personality types exactly.
Interestingly  Independent Component Analysis (ICA) [1] is the only other method that comes close.
The intuition behind ICA is that it ﬁnd a linear transformation on the input that minimizes the multi-
information among the outputs (Yj). In contrast  CorEx searches for Yj’s so that multi-information
among the Xi’s is minimized after conditioning on Y . ICA assumes that the signals that give rise
to the data are independent while CorEx does not. In this case  personality traits like “extraversion”
and “agreeableness” are correlated  violating the independence assumption.
RI:0.5
a(A
h. Afric

*

3)

R

A

E

A

a

u

s

i

(

r

I

a
s
b
u
S

Subsah. Africa(ARI:0.55)
Subsah. Africa(ARI:0.99)
Subsah. Africa(A RI:0.98)
Subsah. Africa(ARI:0.98)
S/Eu/Ea(ARI:0.86)

h . A fri c

a ( A

S

u

a

b

s

5 )

R I: 0 . 9

Subsah. Africa(ARI:0.92)
Subsah. Africa(ARI:0.52)
America(ARI:0.99)
*

:

0

.

8

7

)

EurAsia(ARI:0.86)

e
u
r
T

Predicted

g e n d e r ( A R I : 0 . 9 5 )

*
East(A RI:0.51)
America(ARI:0.55)

7)

RI:0.8
st(A
a
E

)
7
8
.
0
:
I
R
A
(
t
s
a
E

)
0
0
.
1
:
I

*

i

R
A
(
a
n
a
e
c
O

E

a

s

t
(

A

R

I
:

0

.

7

4

)

Figure 3: (Left) Confusion matrix comparing predicted clusters to true clusters for the questions on
the Big-5 personality test. (Right) Hierarchical model constructed from samples of DNA by CorEx.

4.2.2 DNA from the Human Genome Diversity Project
Next  we consider DNA data taken from 952 individuals of diverse geographic and ethnic back-
grounds [1]. The data consist of 4170 variables describing different SNPs (single nucleotide poly-
morphisms).6 We use CorEx to learn a hierarchical representation which is depicted in Fig. 3. To
evaluate the quality of the representation  we use the adjusted Rand index (ARI) to compare clusters
induced by each latent variable in the hierarchical representation to different demographic variables
in the data. Latent variables which substantially match demographic variables are labeled in Fig. 3.
The representation learned (unsupervised) on the ﬁrst layer contains a perfect match for Oceania (the
Paciﬁc Islands) and nearly perfect matches for America (Native Americans)  Subsaharan Africa  and
gender. The second layer has three variables which correspond very closely to broad geographic
regions: Subsaharan Africa  the “East” (including China  Japan  Oceania  America)  and EurAsia.

4.2.3 Text from the Twenty Newsgroups Dataset
The twenty newsgroups dataset consists of documents taken from twenty different topical message
boards with about a thousand posts each [1]. For analyzing unstructured text  typical feature en-
gineering approaches heuristically separate signals like style  sentiment  or topics. In principle  all
6Data  descriptions of SNPs  and detailed demographics of subjects is available at ftp://ftp.cephb.

fr/hgdp_v3/.

6

threeofthesesignalsmanifestthemselvesintermsofsubtlecorrelationsinwordusage.Recentattemptsatlearninglarge-scaleunsupervisedhierarchicalrepresentationsoftexthaveproducedin-terestingresults[1] thoughvalidationisdifﬁcultbecausequantitativemeasuresofrepresentationqualityoftendonotcorrelatewellwithhumanjudgment[1].Tofocusonlinguisticsignals weremovedmeta-datalikeheaders footers andreplieseventhoughthesegivestrongsignalsforsupervisednewsgroupclassiﬁcation.Weconsideredthetoptenthou-sandmostfrequenttokensandconstructedabagofwordsrepresentation.ThenweusedCorExtolearnaﬁvelevelrepresentationofthedatawith326latentvariablesintheﬁrstlayer.DetailsaredescribedinSec.C.1.PortionsoftheﬁrstthreelevelsofthetreekeepingonlynodeswiththehighestnormalizedmutualinformationwiththeirparentsareshowninFig.4andinFig.C.1.7(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:1)(cid:10)(cid:11)(cid:12)(cid:5)(cid:6)(cid:7)(cid:9)(cid:8)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:13)(cid:14)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:9)(cid:14)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:9)(cid:15)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:14)(cid:16)(cid:3)(cid:17)(cid:4)(cid:18)(cid:19)(cid:18)(cid:17)(cid:4)(cid:18)(cid:17)(cid:1)(cid:20)(cid:10)(cid:21)(cid:22)(cid:3)(cid:17)(cid:23)(cid:1)(cid:24)(cid:17)(cid:4)(cid:18)(cid:21)(cid:19)(cid:3)(cid:1)(cid:18)(cid:10)(cid:2)(cid:18)(cid:25)(cid:3)(cid:26)(cid:10)(cid:17)(cid:21)(cid:10)(cid:25)(cid:2)(cid:18)(cid:17)(cid:3)(cid:10)(cid:17)(cid:10)(cid:25)(cid:2)(cid:18)(cid:17)(cid:3)(cid:10)(cid:17)(cid:21)(cid:10)(cid:25)(cid:27)(cid:28)(cid:29)(cid:25)(cid:17)(cid:18)(cid:4)(cid:3)(cid:21)(cid:25)(cid:10)(cid:18)(cid:11)(cid:30)(cid:18)(cid:20)(cid:21)(cid:25)(cid:29)(cid:21)(cid:21)(cid:3)(cid:10)(cid:17)(cid:2)(cid:3)(cid:11)(cid:11)(cid:3)(cid:24)(cid:17)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:3)(cid:12)(cid:2)(cid:13)(cid:1)(cid:14)(cid:13)(cid:15)(cid:16)(cid:2)(cid:17)(cid:18)(cid:4)(cid:5)(cid:6)(cid:7)(cid:19)(cid:12)(cid:17)(cid:15)(cid:2)(cid:14)(cid:15)(cid:20)(cid:10)(cid:16)(cid:9)(cid:2)(cid:17)(cid:18)(cid:4)(cid:5)(cid:6)(cid:21)(cid:8)(cid:16)(cid:10)(cid:2)(cid:15)(cid:3)(cid:3)(cid:17)(cid:22)(cid:14)(cid:17)(cid:11)(cid:2)(cid:17)(cid:18)(cid:4)(cid:5)(cid:6)(cid:23)(cid:24)(cid:25)(cid:22)(cid:10)(cid:16)(cid:26)(cid:17)(cid:3)(cid:10)(cid:20)(cid:17)(cid:2)(cid:17)(cid:18)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:3)(cid:12)(cid:13)(cid:18)(cid:11)(cid:2)(cid:17)(cid:16)(cid:12)(cid:25)(cid:20)(cid:15)(cid:16)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:27)(cid:5)(cid:3)(cid:12)(cid:25)(cid:2)(cid:3)(cid:12)(cid:28)(cid:17)(cid:1)(cid:25)(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:4)(cid:14)(cid:3)(cid:3)(cid:1)(cid:15)(cid:13)(cid:5)(cid:6)(cid:7)(cid:16)(cid:16)(cid:17)(cid:2)(cid:18)(cid:10)(cid:13)(cid:12)(cid:18)(cid:10)(cid:19)(cid:20)(cid:19)(cid:18)(cid:21)(cid:19)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:22)(cid:23)(cid:13)(cid:24)(cid:25)(cid:19)(cid:11)(cid:18)(cid:26)(cid:19)(cid:18)(cid:1)(cid:3)(cid:2)(cid:27)(cid:21)(cid:28)(cid:5)(cid:6)(cid:7)(cid:29)(cid:6)(cid:4)(cid:19)(cid:30)(cid:19)(cid:18)(cid:28)(cid:11)(cid:30)(cid:15)(cid:1)(cid:14)(cid:24)(cid:18)(cid:1)(cid:15)(cid:13)(cid:5)(cid:6)(cid:7)(cid:8)(cid:6)(cid:31)(cid:11)(cid:19)(cid:13)(cid:2)(cid:21)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:11)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:19)(cid:18)(cid:20)(cid:21)(cid:11)(cid:18)(cid:12)(cid:13)(cid:2)(cid:15)(cid:11)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:22)(cid:23)(cid:11)(cid:12)(cid:24)(cid:15)(cid:16)(cid:20)(cid:23)(cid:13)(cid:18)(cid:1)(cid:25)(cid:26)(cid:5)(cid:6)(cid:7)(cid:27)(cid:28)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:19)(cid:9)(cid:1)(cid:25)(cid:26)(cid:5)(cid:6)(cid:7)(cid:27)(cid:29)(cid:11)(cid:26)(cid:11)(cid:1)(cid:30)(cid:12)(cid:31)(cid:18)(cid:4)(cid:15)(cid:3)(cid:4)(cid:23)(cid:16)(cid:18)(cid:16)(cid:32)(cid:20)(cid:23)(cid:20)(cid:32)(cid:20)(cid:11)(cid:18)(cid:15)(cid:16)(cid:30)(cid:30)(cid:11)(cid:14)(cid:33)(cid:1)(cid:18)(cid:12)(cid:1)(cid:2)(cid:3)(cid:1)(cid:4)(cid:5)(cid:4)(cid:2)(cid:6)(cid:7)(cid:8)(cid:1)(cid:4)(cid:6)(cid:5)(cid:4)(cid:9)(cid:10)(cid:11)(cid:3)(cid:4)(cid:12)(cid:13)(cid:2)(cid:6)(cid:8)(cid:8)(cid:1)(cid:4)(cid:14)(cid:10)(cid:15)(cid:16)(cid:4)(cid:10)(cid:17)(cid:14)(cid:18)(cid:13)(cid:3)(cid:8)(cid:5)(cid:4)(cid:19)(cid:13)(cid:16)(cid:4)(cid:19)(cid:10)(cid:9)(cid:8)(cid:1)(cid:13)(cid:20)(cid:4)(cid:21)(cid:4)(cid:16)(cid:1)(cid:2)(cid:3)(cid:1)(cid:3)(cid:4)(cid:3)(cid:5)(cid:6)(cid:3)(cid:7)(cid:8)(cid:9)(cid:8)(cid:10)(cid:3)(cid:4)(cid:11)(cid:12)(cid:13)(cid:6)(cid:8)(cid:4)(cid:14)(cid:15)(cid:5)(cid:2)(cid:3)(cid:4)(cid:16)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:2)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:12)(cid:15)(cid:10)(cid:8)(cid:16)(cid:15)(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:11)(cid:12)(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:9)(cid:10)(cid:5)(cid:11)(cid:12)(cid:2)(cid:10)(cid:13)(cid:14)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:3)(cid:7)(cid:8)(cid:2)(cid:9)(cid:10)(cid:11)(cid:12)(cid:12)(cid:13)(cid:14)(cid:13)(cid:13)(cid:2)(cid:13)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:3)(cid:7)(cid:8)(cid:2)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:9)(cid:4)(cid:5)(cid:10)(cid:11)(cid:12)(cid:2)(cid:3)(cid:4)(cid:5)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:6)(cid:7)(cid:8)(cid:1)(cid:9)(cid:7)(cid:10)(cid:1)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:9)(cid:2)(cid:10)(cid:5)(cid:1)alt.atheismaarelcomp.graphicscgcompcomp.os.ms-windows.misccmscompcomp.sys.ibm.pc.hardwarecpccompcomp.sys.mac.hardwarecmaccompcomp.windows.xcwxcompmisc.forsalemfmiscrec.autosravehicrec.motorcyclesrmvehicrec.sport.baseballrsbsportrec.sport.hockeyrshsportsci.cryptscscisci.electronicssescisci.medsmscisci.spacessscisoc.religion.christiansrcreltalk.politics.gunstpgtalktalk.politics.mideasttmidtalktalk.politics.misctmisctalktalk.religion.misctrmrelFigure4:Portionsofthehierarchicalrepresentationlearnedforthetwentynewsgroupsdataset.Welabellatentvariablesthatoverlapsigniﬁcantlywithknownstructure.Newsgroupnames abbrevia-tions andbroadgroupingsareshownontheright.Toprovideamorequantitativebenchmarkoftheresults weagaintesttowhatextentlearnedrep-resentationsarerelatedtoknownstructureinthedata.Eachpostcanbelabeledbythenewsgroupitbelongsto accordingtobroadcategories(e.g.groupsthatinclude“comp”) orbyauthor.Mostlearnedbinaryvariableswereactiveinaround1%oftheposts sowereportthefractionofac-tivationsthatcoincidewithaknownlabel(precision)inFig.4.Mostvariablesclearlyrepresentsub-topicsofthenewsgrouptopics sowedonotexpecthighrecall.ThesmallportionofthetreeshowninFig.4reﬂectsintuitiverelationshipsthatcontainhierarchiesofrelatedsub-topicsaswellasclustersoffunctionwords(e.g.pronounslike“he/his/him”ortensewith“have/be”).Onceagain severallearnedvariablesperfectlycapturedknownstructureinthedata.Someuserssentimagesintextusinganencodedformat.Onefeaturematchedalltheimageposts(withper-fectprecisionandrecall)duetothecorrelatedpresenceofunusualshorttokens.Therewerealsoperfectmatchesforthreefrequentauthors:G.Banks D.Medin andB.Beauchaine.Notethatthelearnedvariablesdidnottriggerifjusttheirnamesappearedinthetext butonlyforpoststheyauthored.Theseauthorshadelaboratesignatureswithlong identiﬁablequotesthatevadedpre-processingbutcreatedastronglycorrelatedsignal.Anothervariablewithperfectprecisionforthe“forsale”newsgrouplabeledcomicbooksales(butdidnotactivatefordiscussionofcomicsinothernewsgroups).OthernearlyperfectpredictorsdescribedextensivediscussionsofArmenia/Turkeyintalk.politics.mideast(aﬁfthofalldiscussioninthatgroup) specializedunixjargon andamatchforsci.cryptwhichhad90%precisionand55%recall.WhenwerankedallthelatentfactorsaccordingtoanormalizedversionofEq.2 theseexamplesallshowedupinthetop20.5ConnectionsandRelatedWorkWhilethebasicmeasuresusedinEq.1andEq.2haveappearedinseveralcontexts[7 1 4 3 1] theinterpretationofthesequantitiesisanactiveareaofresearch[1 2].Theoptimizationswedeﬁnehavesomeinterestingbutlessobviousconnections.Forinstance theoptimizationinEq.3issimilar7Aninteractivetoolforexploringthefullhierarchyisavailableathttp://bit.ly/corexvis.7to one recently introduced as a measure of “common information” [2]. The objective in Eq. 6 (for
a single Yj) appears exactly as a bound on “ancestral” information [2]. For instance  if all the
↵i = 1/ then Steudel and Ay [2] show that the objective is positive only if at least 1 +  variables
share a common ancestor in any DAG describing them. This provides extra rationale for relaxing
our original optimization to include non-binary values of ↵i j.
The most similar learning approach to the one presented here is the information bottleneck [2] and
its extension the multivariate information bottleneck [2  2]. The motivation behind information
bottleneck is to compress the data (X) into a smaller representation (Y ) so that information about
some relevance term (typically labels in a supervised learning setting) is maintained. The second
term in Eq. 6 is analogous to the compression term. Instead of maximizing a relevance term  we
are maximizing information about all the individual sub-systems of X  the Xi. The most redundant
information in the data is preferentially stored while uncorrelated random variables are completely
ignored.
The broad problem of transforming complex data into simpler  more meaningful forms goes under
the rubric of representation learning [2] which shares many goals with dimensionality reduction and
subspace clustering. Insofar as our approach learns a hierarchy of representations it superﬁcially
resembles “deep” approaches like neural nets and autoencoders [2  2  2  3]. While those approaches
are scalable  a common critique is that they involve many heuristics discovered through trial-and-
error that are difﬁcult to justify. On the other hand  a rich literature on learning latent tree models [3 
3  9  1] have excellent theoretical properties but do not scale well. By basing our method on an
information-theoretic optimization that can nevertheless be performed quite efﬁciently  we hope to
preserve the best of both worlds.

6 Conclusion
The most challenging open problems today involve high-dimensional data from diverse sources
including human behavior  language  and biology.8 The complexity of the underlying systems makes
modeling difﬁcult. We have demonstrated a model-free approach to learn successfully more coarse-
grained representations of complex data by efﬁciently optimizing an information-theoretic objective.
The principle of explaining as much correlation in the data as possible provides an intuitive and fully
data-driven way to discover previously inaccessible structure in high-dimensional systems.
It may seem surprising that CorEx should perfectly recover structure in diverse domains without
using labeled data or prior knowledge. On the other hand  the patterns discovered are “low-hanging
fruit” from the right point of view. Intelligent systems should be able to learn robust and general pat-
terns in the face of rich inputs even in the absence of labels to deﬁne what is important. Information
that is very redundant in high-dimensional data provides a good starting point.
Several fruitful directions stand out. First  the promising preliminary results invite in-depth investi-
gations on these and related problems. From a computational point of view  the main work of the
algorithm involves a matrix multiplication followed by an element-wise non-linear transform. The
same is true for neural networks and they have been scaled to very large data using  e.g.  GPUs.
On the theoretical side  generalizing this approach to allow non-tree representations appears both
feasible and desirable [8].

Acknowledgments
We thank Virgil Grifﬁth  Shuyang Gao  Hsuan-Yi Chu  Shirley Pepke  Bilal Shaw  Jose-Luis Ambite 
and Nathan Hodas for helpful conversations. This research was supported in part by AFOSR grant
FA9550-12-1-0417 and DARPA grant W911NF-12-1-0034.

References
[1] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus.

properties of neural networks. In ICLR  2014.

Intriguing

8In principle  computer vision should be added to this list. However  the success of unsupervised feature
learning with neural nets for vision appears to rely on encoding generic priors about vision through heuristics
like convolutional coding and max pooling [3]. Since CorEx is a knowledge-free method it will perform
relatively poorly unless we ﬁnd a way to also encode these assumptions.

8

[2] Thomas M Cover and Joy A Thomas. Elements of information theory. Wiley-Interscience  2006.
[3] Satosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research

and development  4(1):66–82  1960.

[4] M Studen`y and J Vejnarova. The multiinformation function as a tool for measuring stochastic dependence.

In Learning in graphical models  pages 261–297. Springer  1998.

[5] Alexander Kraskov  Harald St¨ogbauer  Ralph G Andrzejak  and Peter Grassberger. Hierarchical clustering

using mutual information. EPL (Europhysics Letters)  70(2):278  2005.

[6] J. Pearl. Causality: Models  Reasoning and Inference. Cambridge University Press  NY  NY  USA  2009.
[7] Elad Schneidman  William Bialek  and Michael J Berry. Synergy  redundancy  and independence in

population codes. the Journal of Neuroscience  23(37):11539–11553  2003.

[8] Greg Ver Steeg and Aram Galstyan. Maximally informative hierarchical representations of high-

dimensional data. arXiv:1410.7404  2014.

[9] Animashree Anandkumar  Kamalika Chaudhuri  Daniel Hsu  Sham M Kakade  Le Song  and Tong Zhang.

Spectral methods for learning multivariate latent tree structure. In NIPS  pages 2025–2033  2011.

[10] Myung Jin Choi  Vincent YF Tan  Animashree Anandkumar  and Alan S Willsky. Learning latent tree

graphical models. The Journal of Machine Learning Research  12:1771–1812  2011.

[11] Lewis R Goldberg. The development of markers for the big-ﬁve factor structure. Psychological assess-

[12] Aapo Hyv¨arinen and Erkki Oja. Independent component analysis: algorithms and applications. Neural

ment  4(1):26  1992.

networks  13(4):411–430  2000.

[13] N.A. Rosenberg  J.k. Pritchard  J.L. Weber  H.M. Cann  K.K. Kidd  L.A. Zhivotovsky  and M.W. Feldman.

Genetic structure of human populations. Science  298(5602):2381–2385  2002.

[14] K. Bache and M. Lichman. UCI machine learning repository  2013.
[15] Tomas Mikolov  Kai Chen  Greg Corrado  and Jeffrey Dean. Efﬁcient estimation of word representations

in vector space. arXiv:1301.3781  2013.

[16] Jonathan Chang  Jordan L Boyd-Graber  Sean Gerrish  Chong Wang  and David M Blei. Reading tea

leaves: How humans interpret topic models. In NIPS  volume 22  pages 288–296  2009.

[17] Elad Schneidman  Susanne Still  Michael J Berry  William Bialek  et al. Network information and con-

nected correlations. Physical Review Letters  91(23):238701  2003.

[18] Nihat Ay  Eckehard Olbrich  Nils Bertschinger  and J¨urgen Jost. A unifying framework for complexity

measures of ﬁnite systems. Proceedings of European Complex Systems Society  2006.

[19] P.L. Williams and R.D. Beer. Nonnegative decomposition of multivariate information. arXiv:1004.2515 

[20] Virgil Grifﬁth and Christof Koch. Quantifying synergistic mutual information. arXiv:1205.4265  2012.
Exact common information.
[21] Gowtham Ramani Kumar  Cheuk Ting Li  and Abbas El Gamal.

[22] B. Steudel and N. Ay. Information-theoretic inference of common ancestors. arXiv:1010.5720  2010.
[23] Naftali Tishby  Fernando C Pereira  and William Bialek.

The information bottleneck method.

2010.

arXiv:1402.0062  2014.

arXiv:physics/0004057  2000.

tion  18(8):1739–1789  2006.

[24] Noam Slonim  Nir Friedman  and Naftali Tishby. Multivariate information bottleneck. Neural Computa-

[25] Noam Slonim. The information bottleneck: Theory and applications. PhD thesis  Citeseer  2002.
[26] Yoshua Bengio  Aaron Courville  and Pascal Vincent. Representation learning: A review and new per-

spectives. Pattern Analysis and Machine Intelligence  IEEE Transactions on  35(8):1798–1828  2013.

[27] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural net-

works. Science  313(5786):504–507  2006.

[28] Yann LeCun  L´eon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[29] Yann LeCun and Yoshua Bengio. Convolutional networks for images  speech  and time series. The

handbook of brain theory and neural networks  3361  1995.

[30] Yoshua Bengio  Pascal Lamblin  Dan Popovici  and Hugo Larochelle. Greedy layer-wise training of deep

networks. Advances in neural information processing systems  19:153  2007.

[31] Rapha¨el Mourad  Christine Sinoquet  Nevin L Zhang  Tengfei Liu  Philippe Leray  et al. A survey on

latent tree models and applications. J. Artif. Intell. Res.(JAIR)  47:157–203  2013.

[32] Ryan Prescott Adams  Hanna M Wallach  and Zoubin Ghahramani. Learning the structure of deep sparse

graphical models. arXiv:1001.0160  2009.

[33] H. Lee  R. Grosse  R. Ranganath  and A. Ng. Convolutional deep belief networks for scalable unsuper-

vised learning of hierarchical representations. In ICML  2009.

[34] F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel  P. Prettenhofer 
R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher  M. Perrot  and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research  12:2825–2830  2011.

9

,Greg Ver Steeg
Aram Galstyan
Dinesh Ramasamy
Upamanyu Madhow
Aaron Defazio