2013,Optimistic Concurrency Control for Distributed Unsupervised Learning,Research on distributed machine learning algorithms has focused primarily on one of two extremes---algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints.  We consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked. We view this optimistic concurrency control'' paradigm as particularly appropriate for large-scale machine learning algorithms  particularly in the unsupervised setting.  We demonstrate our approach in three problem areas: clustering  feature learning and online facility location.  We evaluate  our methods via large-scale experiments in a cluster computing environment.  ",OptimisticConcurrencyControlforDistributedUnsupervisedLearningXinghaoPan1JosephGonzalez1StefanieJegelka1TamaraBroderick1 2MichaelI.Jordan1 21DepartmentofElectricalEngineeringandComputerScience and2DepartmentofStatisticsUniversityofCalifornia BerkeleyBerkeley CAUSA94720{xinghao jegonzal stefje tab jordan}@eecs.berkeley.eduAbstractResearchondistributedmachinelearningalgorithmshasfocusedprimarilyononeoftwoextremes—algorithmsthatobeystrictconcurrencyconstraintsoralgorithmsthatobeyfewornosuchconstraints.Weconsideranintermediatealternativeinwhichalgorithmsoptimisticallyassumethatconﬂictsareunlikelyandifconﬂictsdoariseaconﬂict-resolutionprotocolisinvoked.Weviewthis“optimisticcon-currencycontrol”paradigmasparticularlyappropriateforlarge-scalemachinelearningalgorithms particularlyintheunsupervisedsetting.Wedemonstrateourapproachinthreeproblemareas:clustering featurelearningandonlinefacilitylo-cation.Weevaluateourmethodsvialarge-scaleexperimentsinaclustercomputingenvironment.1IntroductionThedesiretoapplymachinelearningtoincreasinglylargerdatasetshaspushedthemachinelearningcommunitytoaddressthechallengesofdistributedalgorithmdesign:partitioningandcoordinatingcomputationacrosstheprocessingresources.Inmanycases whencomputingstatisticsofiiddataortransformingfeatures thecomputationfactorsaccordingtothedataandcoordinationisonlyrequiredduringaggregation.Fortheseembarrassinglyparalleltasks themachinelearningcommunityhasembracedthemap-reduceparadigm whichprovidesatemplateforconstructingdistributedalgorithmsthatarefaulttolerant scalable andeasytostudy.However inpursuitofrichermodels weoftenintroducestatisticaldependenciesthatrequiremoresophisticatedalgorithms(e.g. collapsedGibbssamplingorcoordinateascent)whichweredevelopedandstudiedintheserialsetting.Becausethesealgorithmsiterativelytransformaglobalstate parallelizationcanbechallengingandoftenrequiresfrequentandcomplexcoordination.Recenteffortstodistributethesealgorithmscanbedividedintotwoprimaryapproaches.Themutualexclusionapproach adoptedby[1]and[2] guaranteesaserializableexecutionpreservingthetheo-reticalpropertiesoftheserialalgorithmbutattheexpenseofparallelismandcostlylockingoverhead.Alternatively inthecoordination-freeapproach proposedby[3]and[4] processorscommuni-catefrequentlywithoutcoordinationminimizingthecostofcontentionbutleadingtostochasticity data-corruption andrequiringpotentiallycomplexanalysistoprovealgorithmcorrectness.Inthispaperweexploreathirdapproach optimisticconcurrencycontrol(OCC)[5]whichofferstheperformancegainsofthecoordination-freeapproachwhileatthesametimeensuringaserializableexecutionandpreservingthetheoreticalpropertiesoftheserialalgorithm.Likethecoordination-freeapproach OCCexploitstheinfrequencyofdata-corruptingoperations.However insteadofallowingoccasionaldata-corruption OCCdetectsdata-corruptingoperationsandappliescorrectingcomputation.Asaconsequence OCCautomaticallyensurescorrectness andtheanalysisisonlynecessarytoguaranteeoptimalscalingperformance.1WeapplyOCCtodistributednonparametricunsupervisedlearning—includingbutnotlimitedtoclustering—andimplementdistributedversionsoftheDP-Means[6] BP-Means[7] andonlinefacilitylocation(OFL)algorithms.WedemonstratehowtoanalyzeOCCinthecontextoftheDP-MeansalgorithmandevaluatetheempiricalscalabilityoftheOCCapproachonallthreeoftheproposedalgorithms.Theprimarycontributionsofthispaperare:1.Concurrencycontrolapproachtodistributingunsupervisedlearningalgorithms.2.Reinterpretationofonlinenonparametricclusteringintheformoffacilitylocationwithapproximationguarantees.3.Analysisofoptimisticconcurrencycontrolforunsupervisedlearning.4.Applicationtofeaturemodelingandclustering.2OptimisticConcurrencyControlManymachinelearningalgorithmsiterativelytransformsomeglobalstate(e.g. modelparametersorvariableassignment)givingtheillusionofserialdependenciesbetweeneachoperation.However duetosparsity exchangeability andothersymmetries itisoftenthecasethatmany butnotall ofthestate-transformingoperationscanbecomputedconcurrentlywhilestillpreservingserializability:theequivalencetosomeserialexecutionwhereindividualoperationshavebeenreordered.Thisopportunityforserializableconcurrencyformsthefoundationofdistributeddatabasesystems.Forexample twocustomersmayconcurrentlymakepurchasesexhaustingtheinventoryofunrelatedproducts butiftheytrytopurchasethesameproductthenwemayneedtoserializetheirpurchasestoensuresufﬁcientinventory.Onesolution(mutualexclusion)associateslockswitheachproducttypeandforceseachpurchaseofthesameproducttobeprocessedserially.Thismightworkforanunpopular rareproductbutifweareinterestedinsellingapopularproductforwhichwehavealargeinventorytheserializationoverheadcouldleadtounnecessarilyslowresponsetimes.Toaddressthisproblem thedatabasecommunityhasadoptedoptimisticconcurrencycontrol(OCC)[5]inwhichthesystemtriestosatisfythecustomersrequestswithoutlockingandcorrectstransactionsthatcouldleadtonegativeinventory(e.g. byforcingthecustomertocheckoutagain).Optimisticconcurrencycontrolexploitssituationswheremostoperationscanexecuteconcurrentlywithoutconﬂictingorviolatingserializationinvariants.Forexample givensufﬁcientinventorytheorderinwhichcustomersaresatisﬁedisimmaterialandconcurrentoperationscanbeexecutedseriallytoyieldthesameﬁnalresult.However intherareeventthatinventoryisnearlydepletedtwoconcurrentpurchasesmaynotbeserializablesincetheinventorycanneverbenegative.Byshiftingthecostofconcurrencycontroltorareeventswecanadmitmorecostlyconcurrencycontrolmechanisms(e.g. re-computation)inexchangeforanefﬁcient simple coordination-freeexecutionforthemajorityoftheevents.Formally toapplyOCCwemustdeﬁneasetoftransactions(i.e. operationsorcollectionsofoperations) amechanismtodetectwhenatransactionviolatesserializationinvariants(i.e. cannotbeexecutedconcurrently) andamethodtocorrect(e.g. rollback)transactionsthatviolatetheserializationinvariants.Optimisticconcurrencycontrolismosteffectivewhenthecostofvalidatingconcurrenttransactionsissmallandconﬂictsoccurinfrequently.Machinelearningalgorithmsareidealforoptimisticconcurrencycontrol.Theconditionalinde-pendencestructureandsparsityinourmodelsanddataoftenleadstosparseparameterupdatessubstantiallyreducingthechanceofconﬂicts.Similarly symmetryinourmodelsoftenprovidestheﬂexibilitytoreorderserialoperationswhilepreservingalgorithminvariants.Becausethemodelsencodethedependencystructure wecaneasilydetectwhenanoperationviolatesserialinvariantsandcorrectbyrejectingthechangeandrerunningthecomputation.Alternatively wecanexploitthesemanticsoftheoperationstoresolvetheconﬂictbyacceptingamodiﬁedupdate.AsaconsequenceOCCallowsustoeasilyconstructprovablycorrectandefﬁcientdistributedalgorithmswithouttheneedtodevelopnewtheoreticaltoolstoanalyzecomplexnon-deterministicdistributedbehavior.22.1TheOCCPatternforMachineLearningOptimisticconcurrencycontrolcanbedistilledtoasimplepattern(meta-algorithm)forthedesignandimplementationofdistributedmachinelearningsystems.WebeginbyevenlypartitioningNdatapoints(andthecorrespondingcomputation)acrossthePavailableprocessors.Eachprocessormaintainsareplicatedviewoftheglobalstateandseriallyappliesthelearningalgorithmasasequenceofoperationsonitsassigneddataandtheglobalstate.Ifanoperationmutatestheglobalstateinawaythatpreservestheserializationinvariantsthentheoperationisacceptedlocallyanditseffectontheglobalstate ifany iseventuallyreplicatedtootherprocessors.However ifanoperationcouldpotentiallyconﬂictwithoperationsonotherprocessorsthenitissenttoauniqueserializingprocessorwhereitisrejectedorcorrectedandtheresultingglobalstatechangeiseventuallyreplicatedtotherestoftheprocessors.Meanwhiletheoriginatingprocessoreithertentativelyacceptsthestatechange(ifarollbackoperatorisdeﬁned)orproceedsasthoughtheoperationhasbeendeferredtosomepointinthefuture.Whileitispossibletoexecutethispatternasynchronouslywithminimalcoordination forsimplicityweadoptthebulk-synchronousmodelof[8]anddividethecomputationintoepochs.Withinanepocht bdatapointsB(p t)areevenlyassignedtoeachofthePprocessors.Anystatechangesorserializationoperationsaretransmittedattheendoftheepochandprocessedbeforethenextepoch.Whilepotentiallyslowerthananasynchronousexecution thebulk-synchronousexecutionisdeterministicandcanbeeasilyexpressedusingexistingsystemslikeHadooporSpark[9].3OCCforUnsupervisedLearningMuchoftheexistingliteratureondistributedmachinelearningalgorithmshasfocusedonclassiﬁcationandregressionproblems wheretheunderlyingmodeliscontinuous.InthispaperweapplytheOCCpatterntomachinelearningproblemsthathaveamorediscrete combinatorialﬂavor—inparticularunsupervisedclusteringandlatentfeaturelearningproblems.Theseproblemsexhibitsymmetryviatheirinvariancetobothdatapermutationandclusterorfeaturepermutation.Togetherwiththesparsityofinteractingoperationsintheirexistingserialalgorithms theseproblemsofferauniqueopportunitytodevelopOCCalgorithms.TheK-meansalgorithmprovidesaparadigmexample;heretheinferentialgoalistopartitionthedata.RatherthanfocusingsolelyonK-means however wehavebeeninspiredbyrecentworkinwhichageneralfamilyofK-means-likealgorithmshavebeenobtainedbytakingBayesiannonparametric(BNP)modelsbasedoncombinatorialstochasticprocessessuchastheDirichletprocess thebetaprocess andhierarchicalversionsoftheseprocesses andsubjectingthemtosmall-varianceasymptoticswheretheposteriorprobabilityundertheBNPmodelistransformedintoacostfunctionthatcanbeoptimized[7].Thealgorithmsconsideredtodateinthisliteraturehavebeendevelopedandanalyzedintheserialsetting;ourgoalistoexploredistributedalgorithmsforoptimizingthesecostfunctionsthatpreservethestructureandanalysisoftheirserialcounterparts.3.1OCCDP-MeansWeﬁrstconsidertheDP-meansalgorithm(Alg.1)introducedby[6].LiketheK-meansalgorithm DP-MeansalternatesbetweenupdatingtheclusterassignmentziforeachpointxiandrecomputingthecentroidsC={µk}Kk=1associatedwitheachclusters.However DP-Meansdiffersinthatthenumberofclustersisnotﬁxedapriori.Instead ifthedistancefromagivendatapointtoallexistingclustercentroidsisgreaterthanaparameterλ thenanewclusteriscreated.Whilethesecondphaseistriviallyparallel theprocessofintroducingclustersintheﬁrstphaseisinherentlyserial.However clusterstendtobeintroducedinfrequently andthusDP-MeansprovidesanopportunityforOCC.InAlg.3wepresentanOCCparallelizationoftheDP-MeansalgorithminwhicheachiterationoftheserialDP-MeansalgorithmisdividedintoN/(Pb)bulk-synchronousepochs.Thedataisevenlypartitioned{xi}i∈B(p t)acrossprocessor-epochsintoblocksofsizeb=|B(p t)|.Duringeachepocht eachprocessorpevaluatestheclustermembershipofitsassigneddata{xi}i∈B(p t)usingtheclustercentersCfromthepreviousepochandoptimisticallyproposesanewsetofclustercentersˆC.Attheendofeachepochtheproposedclustercenters ˆC areseriallyvalidatedusingAlg.2.3Algorithm1:SerialDP-meansInput:data{xi}Ni=1 thresholdλC←∅whilenotconvergeddofori=1toNdoµ∗←argminµ∈Ckxi−µkifkxi−µ∗k>λthenzi←xiC←C∪xi//Newclusterelsezi←µ∗//Usenearestforµ∈Cdo//RecomputeCentersµ←Mean({xi|zi=µ})Output:AcceptedclustercentersCAlgorithm2:DPValidateInput:SetofproposedclustercentersˆCC←∅forx∈ˆCdoµ∗←argminµ∈Ckx−µkifkxi−µ∗k<λthen//RejectRef(x)←µ∗//RollbackAssgselseC←C∪x//AcceptOutput:AcceptedclustercentersCAlgorithm3:ParallelDP-meansInput:data{xi}Ni=1 thresholdλInput:EpochsizebandPprocessorsInput:PartitioningB(p t)ofdata{xi}i∈B(p t)toprocessor-epochswhereb=|B(p t)|C←∅whilenotconvergeddoforepocht=1toN/(Pb)doˆC←∅//Newcandidatecentersforp∈{1 ... P}doinparallel//Processlocaldatafori∈B(p t)doµ∗←argminµ∈Ckxi−µk//OptimisticTransactionifkxi−µ∗k>λthenzi←Ref(xi)ˆC←ˆC∪xielsezi←µ∗//AlwaysSafe//SeriallyvalidateclustersC←C∪DPValidate(ˆC)forµ∈Cdo//RecomputeCentersµ←Mean({xi|zi=µ})Output:AcceptedclustercentersCFigure1:TheSerialDP-MeansalgorithmanddistributedimplementationusingtheOCCpattern.Thevalidationprocessacceptsclustercentersthatarenotcoveredby(i.e. notwithinλof)alreadyacceptedclustercenters.Whenaclustercenterisrejectedweupdateitsreferencetopointtothealreadyacceptedcenter therebycorrectingtheoriginalpointassignment.3.2OCCFacilityLocationTheDP-MeansobjectiveturnsouttobeequivalenttotheclassicFacilityLocation(FL)objective:J(C)=Px∈Xminµ∈Ckx−µk2+λ2|C| whichselectsthesetofclustercenters(facilities)µ∈Cthatminimizestheshortestdistancekx−µk2toeachpoint(customer)xaswellasthepenalizedcostoftheclustersλ2|C|.However whileDP-Meansallowstheclusterstobearbitrarypoints(e.g. C∈RD) FLconstrainstheclusterstobepointsC⊆FinasetofcandidatelocationsF.Hence weobtainalinkbetweencombinatorialBayesianmodelsandFLallowingustoapplyalgorithmswithknownapproximationboundstoBayesianinspirednonparametricmodels.AswewillseeinSection4 ourOCCalgorithmprovidesconstant-factorapproximationsforbothFLandDP-means.Facilitylocationhasbeenstudiedintensely.Webuildontheonlinefacilitylocation(OFL)algorithmdescribedbyMeyerson[10].TheOFLalgorithmprocesseseachdatapointxseriallyinasinglepassbyeitheraddingxtothesetofclusterswithprobabilitymin(1 minµ∈Ckx−µk2/λ2)orassigningxtothenearestexistingcluster.UsingOCCweareabletoconstructadistributedOFLalgorithm(Alg.4)whichisnearlyidenticaltotheOCCDP-Meansalgorithm(Alg.3)butwhichprovidesstrongapproximationbounds.TheOCCOFLalgorithmdiffersonlyinthatclustersareintroducedandvalidatedstochastically—thevalidationprocessensuresthatthenewclustersareacceptedwithprobabilityequaltotheserialalgorithm.3.3OCCBP-MeansBP-meansisanalgorithmforlearningcollectionsoflatentbinaryfeatures providingawaytodeﬁnegroupingsofdatapointsthatneednotbemutuallyexclusiveorexhaustivelikeclusters.4Algorithm4:ParallelOFLInput:SameasDP-Meansforepocht=1toN/(Pb)doˆC←∅forp∈{1 ... P}doinparallelfori∈B(p t)dod←minµ∈Ckxi−µkwithprobabilitymin(cid:8)d2 λ2(cid:9)/λ2ˆC←ˆC∪(xi d)C←C∪OFLValidate(ˆC)Output:AcceptedclustercentersCAlgorithm5:OFLValidateInput:SetofproposedclustercentersˆCC←∅for(x d)∈ˆCdod∗←minµ∈Ckx−µkwithprobabilitymin(cid:8)d∗2 d2(cid:9)/d2C←C∪x//AcceptOutput:AcceptedclustercentersCFigure2:TheOCCalgorithmforOnlineFacilityLocation(OFL).AswithserialDP-means therearetwophasesinserialBP-means(Alg.6).Intheﬁrstphase eachdatapointxiislabeledwithbinaryassignmentsfromacollectionoffeatures(zik=0ifxidoesn’tbelongtofeaturek;otherwisezik=1)toconstructarepresentationxi≈Pkzikfk.Inthesecondphase parametervalues(thefeaturemeansfk∈ˆC)areupdatedbasedontheassignments.Theﬁrststepalsoincludesthepossibilityofintroducinganadditionalfeature.Whilethesecondphaseistriviallyparallel theinherentlyserialnatureoftheﬁrstphasecombinedwiththeinfrequentintroductionofnewfeaturespointstotheusefulnessofOCCinthisdomain.TheOCCparallelizationforBP-meansfollowsthesamebasicstructureasOCCDP-means.Eachtransactionoperatesonadatapointxiintwophases.Intheﬁrst analysisphase theoptimalrepresentationPkzikfkisfound.Ifxiisnotwellrepresented(i.e. kxi−Pkzikfkk>λ) thedifferenceisproposedasanewfeatureinthesecondvalidationphase.Attheendofepocht theproposedfeatures{fnewi}areseriallyvalidatedtoobtainasetofacceptedfeatures˜C.Foreachproposedfeaturefnewi thevalidationprocessﬁrstﬁndstheoptimalrepresentationfnewi≈Pfk∈˜Czikfkusingnewlyacceptedfeatures.Iffnewiisnotwellrepresented thedifferencefnewi−Pfk∈˜Czikfkisaddedto˜Candacceptedasanewfeature.Finally toupdatethefeaturemeans letFbetheK-rowmatrixoffeaturemeans.ThefeaturemeansupdateF←(ZTZ)−1ZTXcanbeevaluatedasasingletransactionbycomputingthesumsZTZ=PizizTi(whereziisaK×1columnvectorsozizTiisaK×Kmatrix)andZTX=PizixTiinparallel.WepresentthepseudocodefortheOCCparallelizationofBP-meansinAppendixA.4AnalysisofCorrectnessandScalabilityIncontrasttothecoordination-freepatterninwhichscalabilityistrivialandcorrectnessoftenrequiresstrongassumptionsorholdsonlyinexpectation theOCCpatternleadstosimpleproofsofcorrectnessandchallengingscalabilityanalysis.However inmanycasesitispreferabletohavealgorithmsthatarecorrectandprobablyfastratherthanfastandpossiblycorrect.Weﬁrstestablishserializability:Theorem4.1(Serializability).ThedistributedDP-means OFL andBP-meansalgorithmsareseriallyequivalenttoDP-means OFLandBP-means respectively.Theproof(AppendixB)ofTheorem4.1isrelativelystraightforwardandisobtainedbyconstructingapermutationfunctionthatdescribesanequivalentserialexecutionforeachdistributedexecution.Theproofcaneasilybeextendedtomanyothermachinelearningalgorithms.Serializabilityallowsustoeasilyextendimportanttheoreticalpropertiesoftheserialalgorithmtothedistributedsetting.Forexample byinvokingserializability wecanestablishthefollowingresultfortheOCCversionoftheonlinefacilitylocation(OFL)algorithm:5Theorem4.2.Ifthedataisrandomlyordered thentheOCCOFLalgorithmprovidesaconstant-factorapproximationfortheDP-meansobjective.Ifthedataisadversariallyordered thenOCCOFLprovidesalog-factorapproximationtotheDP-meansobjective.Theproof(AppendixB)ofTheorem4.2isﬁrstderivedintheserialsettingthenextendedtothedistributedsettingthroughserializability.Incontrasttodivide-and-conquerschemes whoseapproximationboundscommonlydependmultiplicativelyonthenumberoflevels[11] Theorem4.2isunaffectedbydistributedprocessingandhasnocommunicationorcoarseningtradeoffs.Furthermore toretainthesamefactorsasabatchalgorithmonthefulldata divide-and-conquerschemesneedalargenumberofpreliminarycentersatlowerlevels[11 12].Inthatcase thecommunicationcostcanbehigh sinceallproposedclustersaresentatthesametime asopposedtotheOCCapproach.Weaddressthecommunicationoverhead(thenumberofrejections)forourschemenext.ScalabilityThescalabilityoftheOCCalgorithmsdependsonthenumberoftransactionsthatarerejectedduringvalidation(i.e. therejectionrate).Whileageneralscalabilityanalysiscanbechallenging itisoftenpossibletogainsomeinsightintotheasymptoticdependenciesbymakingsimplifyingassumptions.Incontrasttothecoordination-freeapproach wecanstillsafelyapplyOCCalgorithmsintheabsenceofascalabilityanalysisorwhensimplifyingassumptionsdonothold.ToillustratethetechniquesemployedinOCCscalabilityanalysiswestudytheDP-Meansalgorithm whosescalabilitylimitingfactorisdeterminedbythenumberofpointsthatmustbeseriallyvalidated.Weshowthatthecommunicationcostonlydependsonthenumberofclustersandprocessingresourcesanddoesnotdirectlydependonthenumberofdatapoints.TheproofisinAppendixC.Theorem4.3(DP-MeansScalability).AssumeNdatapointsaregeneratediidtoformarandomnumber(KN)ofwell-spacedclustersofdiameterλ:λisanupperboundonthedistanceswithinclustersandalowerboundonthedistancebetweenclusters.ThentheexpectednumberofseriallyvalidatedpointsisboundedabovebyPb+E[KN]forPprocessorsandbpointsperepoch.Undertheseparationassumptionsofthetheorem thenumberofclusterspresentinNdatapoints KN isexactlyequaltothenumberofclustersfoundbyDP-MeansinNdatapoints;callthislatterquantitykN.TheexperimentalresultsinFigure3suggestthattheboundofPb+kNmayholdmoregenerallybeyondtheassumptionsabove.SincethemastermustprocessatleastkNpoints theoverheadcausedbyrejectionsisPbandindependentofN.5EvaluationForourexperiments wegeneratedsyntheticdataforclustering(DP-meansandOFL)andfeaturemodeling(BP-means).Theclusterandfeatureproportionsweregeneratednonparametricallyasdescribedbelow.AlldatapointsweregeneratedinR16space.Weﬁxedthresholdparameterλ=1.Clustering:Theclusterproportionsandindicatorsweregeneratedsimultaneouslyusingthestick-breakingprocedureforDirichletprocesses—‘sticks’are‘broken’on-the-ﬂytogeneratenewclustersasnecessary.Forourexperiments weusedaﬁxedconcentrationparameterθ=1.Clustermeansweresampledµk∼N(0 I16) anddatapointsweregeneratedatxi∼N(µzi 14I16).Featuremodeling:Weusethestick-breakingprocedureof[13]togeneratefeatureweights.Un-likewithDirichletprocesses weareunabletoperformstick-breakingon-the-ﬂywithBetapro-cesses.Instead wegenerateenoughfeaturessothatwithhighprobability(>0.9999)there-mainingnon-generatedfeatureswillhavenegligibleweights(<0.0001).Theconcentrationpa-rameterwasalsoﬁxedatθ=1.Wegeneratedfeaturemeansfk∼N(0 I16)anddatapointsxi∼N(Pkzikfk 14I16).5.1SimulatedexperimentsTotesttheefﬁciencyofouralgorithms wesimulatedtheﬁrstiteration(onecompletepassoverallthedata wheremostclusters/featuresarecreatedandthusgreatestcoordinationisneeded)ofeachalgorithminMATLAB.Thenumberofdatapoints N wasvariedfrom256to2560inintervalsof256.WealsovariedPb thenumberofdatapointsprocessedinoneepoch from16to256inpowersof2.ForeachvalueofNandPb weempiricallymeasuredkN thenumberofacceptedclusters/6(a)OCCDP-means(b)OCCOFL(c)OCCBP-meansFigure3:SimulateddistributedDP-means OFLandBP-means:expectednumberofdatapointsproposedbutnotacceptedasnewclusters/featuresisindependentofsizeofdataset.features andMN thenumberofproposedclusters/features.Thiswasrepeated400timestoobtaintheempiricalaverageˆE[MN−kN]ofthenumberofrejections.ForOCCDP-means weobserveˆE[MN−kN]isboundedabovebyPb(Fig.3a) andthatthisboundisindependentofthedatasetsize evenwhentheassumptionsofThm4.3areviolated.(Wealsoveriﬁedthatsimilarempiricalresultsareobtainedwhentheassumptionsarenotviolated;seeAppendixC.)ThesamebehaviorisobservedfortheothertwoOCCalgorithms(Fig.3bandFig.3c).5.2DistributedimplementationandexperimentsWealsoimplemented1thedistributedalgorithmsinSpark[9] anopen-sourceclustercomputingsystem.TheDP-meansandBP-meansalgorithmswereinitializedbypre-processingasmallnumberofdatapoints(1/16oftheﬁrstPbpoints)—thisreducesthenumberofdatapointssenttothemasterontheﬁrstepoch whilestillpreservingserializabilityofthealgorithms.OurSparkimplementationsweretestedonAmazonEC2byprocessingaﬁxeddataseton1 2 4 8m2.4xlargeinstances.Ideally toprocessthesameamountofdata analgorithmandimplementationwithperfectscalingwouldtakehalftheruntimeon8machinesasitwouldon4 andsoon.TheplotsinFigure4showsthiscomparisonbydividingallruntimesbytheruntimeononemachine.DP-means:WeranthedistributedDP-meansalgorithmon227≈134Mdatapoints usingλ=2.TheblocksizebwaschosentokeepPb=223≈8Mconstant.Thealgorithmwasrunfor5iterations(completepassoveralldatain16epochs).Wewereabletogetperfectscaling(Figure4a)inallbuttheﬁrstiteration whenthemasterhastoperformthemostsynchronizationofproposedcenters.OFL:ThedistributedOFLalgorithmwasrunon220≈1Mdatapoints usingλ=2.UnlikeDP-meansandBP-means OFLisasingle-passalgorithmandwedidnotperformanyinitializationclustering.TheblocksizebwaschosensuchthatPb=216≈66Kdatapointsareprocessedeachepoch whichgivesus16epochs.Figure4bshowsthatwegetnoscalingintheﬁrstepoch whereallPbdatapointsaresenttothemaster.Scalingimprovesinthelaterepochs asthemaster’sworkloaddecreaseswithfewerproposalsbuttheworkers’workloadincreaseswithmorecenters.BP-means:DistributedBP-meanswasrunon223≈8Mdatapoints withλ=1;blocksizewaschosensuchthatPb=219≈0.5Misconstant.Fiveiterationswererun with16epochsperiteration.AswithDP-means wewereabletoachievenearlyperfectscaling;seeFigure4c.6RelatedworkOthershaveproposedalternativestomutualexclusionandcoordination-freeparallelismformachinelearningalgorithmdesign.[14]proposedtransformingtheunderlyingmodeltoexposeadditionalparallelismwhilepreservingthemarginalposterior.However suchconstructionscanbechallengingorinfeasibleandmanyhindermixingorconvergence.Likewise [15]proposedareparameterizationoftheunderlyingmodeltoexposeadditionalparallelismthroughconditionalindependence.Additional1Codewillbemadeavailableatourprojectpagehttps://amplab.cs.berkeley.edu/projects/ccml/.7(a)OCCDP-means(b)OCCOFL(c)OCCBP-meansFigure4:Normalizedruntimefordistributedalgorithms.Runtimeofeachiteration/epochisdividedbythatusing1machine(P=8).Ideally theruntimewith2 4 8machines(P=16 32 64)shouldberespectively1/2 1/4 1/8oftheruntimeusing1machine.OCCDP-meansandBP-meansobtainnearlyperfectscalingforalliterations.OCCOFLrejectsalotinitially butquicklygetsbetterinlaterepochs.worksimilarinspirittooursusingOCC-liketechniquesincludes[16]whoproposedanapproximateparallelsamplingalgorithmfortheIBPwhichismadeexactbyintroducinganadditionalMetropolis-Hastingsstep and[17]whoproposedalook-aheadstrategyinwhichfuturesamplesarecomputedoptimisticallybasedonthelikelyoutcomesofcurrentsamples.Therehasbeensubstantialworkonscalableclusteringalgorithms[18 19 20].Severalauthors[11 21 22 12]haveproposedstreamingapproximationalgorithmsthatrelyonhierarchicaldivide-and-conquerschemes.TheapproximationfactorsinthesealgorithmsaremultiplicativeinthehierarchyanddemandacarefultradeoffbetweencommunicationandapproximationqualitywhichisobviatedintheOCCframework.Severalmethods[12 25 21]ﬁrstcollectandthenre-clusterasetofcenters andthereforeneedtocommunicateallintermediatecenters.Ourapproachavoidsthesestages sinceacentercausesnorejectionsintheepochsafteritisestablished:therejectionratedoesnotgrowwithK.Finally theOCCframeworkcaneasilyintegrateandexploitmanyoftheideasinthecitedworks.7DiscussionInthispaperwehaveshownhowoptimisticconcurrencycontrolcanbeusefullyemployedinthedesignofdistributedmachinelearningalgorithms.Asopposedtopreviousapproaches thispreservescorrectness inmostcasesatasmallcost.WeestablishedtheequivalenceofourdistributedOCCDP-means OFLandBP-meansalgorithmstotheirserialcounterparts thuspreservingtheirtheoreticalproperties.Inparticular thestrongapproximationguaranteesofserialOFLtranslateimmediatelytothedistributedalgorithm.OurtheoreticalanalysisensuresOCCDP-meansachieveshighparallelismwithoutsacriﬁcingcorrectness.WeimplementedandevaluatedallthreeOCCalgorithmsonadistributedcomputingplatformanddemonstratestrongscalabilityinpractice.Webelievethatthereismuchmoretodointhisvein.Indeed machinelearningalgorithmshavemanypropertiesthatdistinguishthemfromclassicaldatabaseoperationsandmayallowgoingbeyondtheclassicformulationofOCC.Inparticularwemaybeabletopartiallyorprobabilisticallyacceptnon-serializableoperationsinawaythatpreservesunderlyingalgorithminvariants.Lawsoflargenumbersandconcentrationtheoremsmayprovidetoolsfordesigningsuchoperations.Moreover theconﬂictdetectionmechanismcanbetreatedasacontrolknob allowingustosoftlyswitchbetweenstable theoreticallysoundalgorithmsandpotentiallyfastercoordination-freealgorithms.AcknowledgmentsThisresearchissupportedinpartbyNSFCISEExpeditionsawardCCF-1139158andDARPAXDataAwardFA8750-12-2-0331 andgiftsfromAmazonWebServices Google SAP BlueGoji Cisco ClearstoryData Cloudera Ericsson Facebook GeneralElectric Hortonworks Intel Microsoft NetApp Oracle Samsung Splunk VMwareandYahoo!.ThismaterialisalsobaseduponworksupportedinpartbytheOfﬁceofNavalResearchundercontract/grantnumberN00014-11-1-0688.X.Pan’sworkisalsosupportedinpartbyaDSONationalLaboratoriesPostgraduateScholarship.T.Broderick’sworkissupportedbyaBerkeleyFellowship.8References[1]J.Gonzalez Y.Low A.Gretton andC.Guestrin.ParallelGibbssampling:Fromcoloredﬁeldstothinjunctiontrees.InProceedingsofthe14thInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS) pages324–332 2011.[2]YuchengLow JosephGonzalez AapoKyrola DannyBickson CarlosGuestrin andJ.M.Hellerstein.DistributedGraphLab:Aframeworkformachinelearninganddatamininginthecloud.InProceedingsofthe38thInternationalConferenceonVeryLargeDataBases(VLDB Istanbul 2012.[3]BenjaminRecht ChristopherRe StephenJ.Wright andFengNiu.Hogwild:Alock-freeapproachtoparallelizingstochasticgradientdescent.InAdvancesinNeuralInformationProcessingSystems(NIPS)24 pages693–701 Granada 2011.[4]AmrAhmed MohamedAly JosephGonzalez ShravanNarayanamurthy andAlexanderJ.Smola.Scalableinferenceinlatentvariablemodels.InProceedingsofthe5thACMInternationalConferenceonWebSearchandDataMining(WSDM) 2012.[5]Hsiang-TsungKungandJohnTRobinson.Onoptimisticmethodsforconcurrencycontrol.ACMTransactionsonDatabaseSystems(TODS) 6(2):213–226 1981.[6]BrianKulisandMichaelI.Jordan.Revisitingk-means:NewalgorithmsviaBayesiannonparametrics.InProceedingsof29thInternationalConferenceonMachineLearning(ICML) Edinburgh 2012.[7]TamaraBroderick BrianKulis andMichaelI.Jordan.MAD-bayes:MAP-basedasymptoticderivationsfromBayes.InProceedingsofthe30thInternationalConferenceonMachineLearning(ICML) 2013.[8]LeslieG.Valiant.Abridgingmodelforparallelcomputation.CommunicationsoftheACM 33(8):103–111 1990.[9]MateiZaharia MosharafChowdhury MichaelJFranklin ScottShenker andIonStoica.Spark:Clustercomputingwithworkingsets.InProceedingsofthe2ndUSENIXConferenceonHotTopicsinCloudComputing 2010.[10]A.Meyerson.Onlinefacilitylocation.InProceedingsofthe42ndAnnualSymposiumonFoundationsofComputerScience(FOCS) LasVegas 2001.[11]A.Meyerson N.Mishra R.Motwani andL.O’Callaghan.Clusteringdatastreams:Theoryandpractice.IEEETransactionsonKnowledgeandDataEngineering 15(3):515–528 2003.[12]N.Ailon R.Jaiswal andC.Monteleoni.Streamingk-meansapproximation.InAdvancesinNeuralInformationProcessingSystems(NIPS)22 Vancouver 2009.[13]JohnPaisley DavidBlei andMichaelIJordan.Stick-breakingBetaprocessesandthePoissonprocess.InProceedingsofthe15thInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS) 2012.[14]D.Newman A.Asuncion P.Smyth andM.Welling.DistributedinferenceforLatentDirichletAllocation.InAdvancesinNeuralInformationProcessingSystems(NIPS)20 Vancouver 2007.[15]D.Lovell J.Malmaud R.P.Adams andV.K.Mansinghka.ClusterCluster:ParallelMarkovchainMonteCarloforDirichletprocessmixtures.ArXive-prints April2013.[16]F.Doshi-Velez D.Knowles S.Mohamed andZ.Ghahramani.LargescalenonparametricBayesianinference:DataparallelisationintheIndianBuffetprocess.InAdvancesinNeuralInformationProcessingSystems(NIPS)22 Vancouver 2009.[17]TianbingXuandAlexanderIhler.MulticoreGibbssamplingindense unstructuredgraphs.InProceedingsofthe14thInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS).2011.[18]I.DhillonandD.S.Modha.Adata-clusteringalgorithmondistributedmemorymultiprocessors.InWorkshoponLarge-ScaleParallelKDDSystems 2000.[19]A.Das M.Datar A.Garg andS.Ragarajam.Googlenewspersonalization:Scalableonlinecollaborativeﬁltering.InProceedingsofthe16thWorldWideWebConference Banff 2007.[20]A.Ene S.Im andB.Moseley.FastclusteringusingMapReduce.InProceedingsofthe17thACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining SanDiego 2011.[21]M.Shindler A.Wong andA.Meyerson.Fastandaccuratek-meansforlargedatasets.InAdvancesinNeuralInformationProcessingSystems(NIPS)24 Granada 2011.[22]MosesCharikar LiadanO’Callaghan andRinaPanigrahy.Betterstreamingalgorithmsforclusteringproblems.InProceedingsofthe35thAnnualACMSymposiumonTheoryofComputing(STOC) 2003.[23]MihaiBˇadoiu SarielHar-Peled andPiotrIndyk.Approximateclusteringviacore-sets.InProceedingsofthe34thAnnualACMSymposiumonTheoryofComputing(STOC) 2002.[24]D.Feldman A.Krause andM.Faulkner.Scalabletrainingofmixturemodelsviacoresets.InAdvancesinNeuralInformationProcessingSystems(NIPS)24 Granada 2011.[25]B.Bahmani B.Moseley A.Vattani R.Kumar andS.Vassilvitskii.Scalablekmeans++.InProceedingsofthe38thInternationalConferenceonVeryLargeDataBases(VLDB) Istanbul 2012.9,Xinghao Pan
Joseph Gonzalez
Stefanie Jegelka
Tamara Broderick
Michael Jordan
Song Han
Jeff Pool
John Tran
William Dally