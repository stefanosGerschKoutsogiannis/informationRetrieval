2019,A unified variance-reduced accelerated gradient method for convex optimization,We propose a novel randomized incremental gradient algorithm  namely  VAriance-Reduced Accelerated Gradient (Varag)  for finite-sum optimization. Equipped with a unified step-size policy that adjusts itself to the value of the conditional number  Varag exhibits the unified optimal rates of convergence for solving smooth convex finite-sum problems directly regardless of their strong convexity. Moreover  Varag is the first accelerated randomized incremental gradient method that benefits from the strong convexity of the data-fidelity term to achieve the optimal linear convergence. It also establishes an optimal linear rate of convergence for solving a wide class of problems only satisfying a certain error bound condition rather than strong convexity. Varag can also be extended to solve stochastic finite-sum problems.,A uniﬁed variance-reduced accelerated gradient

method for convex optimization

H. Milton Stewart School of Industrial & Systems Engineering

Guanghui Lan

Georgia Institute of Technology

Atlanta  GA 30332

george.lan@isye.gatech.edu

Institute for Interdisciplinary Information Sciences

IBM Almaden Research Center

Zhize Li

Tsinghua University
Beijing 100084  China

zz-li14@mails.tsinghua.edu.cn

Yi Zhou

San Jose  CA 95120
yi.zhou@ibm.com

Abstract

We propose a novel randomized incremental gradient algorithm  namely  VAriance-
Reduced Accelerated Gradient (Varag )  for ﬁnite-sum optimization. Equipped
with a uniﬁed step-size policy that adjusts itself to the value of the condition
number  Varag exhibits the uniﬁed optimal rates of convergence for solving smooth
convex ﬁnite-sum problems directly regardless of their strong convexity. Moreover 
Varag is the ﬁrst accelerated randomized incremental gradient method that beneﬁts
from the strong convexity of the data-ﬁdelity term to achieve the optimal linear
convergence. It also establishes an optimal linear rate of convergence for solving
a wide class of problems only satisfying a certain error bound condition rather
than strong convexity. Varag can also be extended to solve stochastic ﬁnite-sum
problems.

1

Introduction

The problem of interest in this paper is the convex programming (CP) problem given in the form of

(cid:8)ψ(x) := 1

m

i=1fi(x) + h(x)(cid:9) .
(cid:80)m

ψ∗ := min
x∈X

(1.1)

(cid:107)∇fi(x1) − ∇fi(x2)(cid:107)∗ ≤ Li(cid:107)x1 − x2(cid:107)  ∀x1  x2 ∈ X 

Here  X ⊆ Rn is a closed convex set  the component function fi : X → R 
i = 1  . . .   m  are
smooth and convex function with Li-Lipschitz continuous gradients over X  i.e.  ∃Li ≥ 0 such that
(1.2)
and h : X → R is a relatively simple but possibly nonsmooth convex function. For notational
convenience  we denote f (x) := 1
i=1Li. It is easy to see that f has
L-Lipschitz continuous gradients  i.e.  for some Lf ≥ 0  (cid:107)∇f (x1) − ∇f (x2)(cid:107)∗ ≤ Lf(cid:107)x1 − x2(cid:107) ≤
m
L(cid:107)x1 − x2(cid:107)  ∀x1  x2 ∈ X. It should be pointed out that it is not necessary to assume h being
strongly convex. Instead  we assume that f is possibly strongly convex with modulus µ ≥ 0.
We also consider a class of stochastic ﬁnite-sum optimization problems given by

(cid:80)m

i=1fi(x) and L := 1

m

(cid:80)m

(cid:8)ψ(x) := 1

(cid:80)m

m

i=1

Eξi[Fi(x  ξi)] + h(x)(cid:9)  

(1.3)

ψ∗ := min
x∈X

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

where ξi’s are random variables with support Ξi ⊆ Rd. It can be easily seen that (1.3) is a special
case of (1.1) with fi = Eξi[Fi(x  ξi)]  i = 1  . . .   m. However  different from deterministic ﬁnite-
sum optimization problems  only noisy gradient information of each component function fi can be
accessed for the stochastic ﬁnite-sum optimization problem in (1.3). Particularly  (1.3) models the
generalization risk minimization in distributed machine learning problems.
Finite-sum optimization given in the form of (1.1) or (1.3) has recently found a wide range of
applications in machine learning (ML)  statistical inference  and image processing  and hence
becomes the subject of intensive studies during the past few years. In centralized ML  fi usually
denotes the loss generated by a single data point  while in distributed ML  it may correspond to the
loss function for an agent i   which is connected to other agents in a distributed network.
Recently  randomized incremental gradient (RIG) methods have emerged as an important class of
ﬁrst-order methods for ﬁnite-sum optimization (e.g. [5  14  27  9  24  18  1  2  13  20  19]). In an
important work  [24] (see [5] for a precursor) showed that by incorporating new gradient estimators
into stochastic gradient descent (SGD) one can possibly achieve a linear rate of convergence for
smooth and strongly convex ﬁnite-sum optimization. Inspired by this work  [14] proposed a stochastic
variance reduced gradient (SVRG) which incorporates a novel stochastic estimator of ∇f (xt−1).
More speciﬁcally  each epoch of SVRG starts with the computation of the exact gradient ˜g = ∇f (˜x)
for a given ˜x ∈ Rn and then runs SGD for a ﬁxed number of steps using the gradient estimator

Gt = (∇fit(xt−1) − ∇fit(˜x)) + ˜g 

where it is a random variable with support on {1  . . .   m}. They show that the variance of Gt vanishes
as the algorithm proceeds  and hence SVRG exhibits an improved linear rate of convergence  i.e. 
O{(m + L/µ) log(1/)}  for smooth and strongly convex ﬁnite-sum problems. See [27  9] for the
same complexity result. Moreover  [2] show that by doubling the epoch length SVRG obtains an
O{m log(1/) + L/} complexity bound for smooth convex ﬁnite-sum optimization.
Observe that the aforementioned variance reduction methods are not accelerated and hence they are
not optimal even when the number of components m = 1. Therefore  much recent research effort
has been devoted to the design of optimal RIG methods. In fact  [18] established a lower complexity
bound for RIG methods by showing that whenever the dimension is large enough  the number of
gradient evaluations required by any RIG methods to ﬁnd an -solution of a smooth and strongly
convex ﬁnite-sum problem i.e.  a point ¯x ∈ X s.t. E[(cid:107)¯x − x∗(cid:107)2

2] ≤   cannot be smaller than

(cid:16)(cid:16)

Ω

m +

(cid:113) mL

(cid:17)

µ

(cid:17)

log 1


.

(1.4)

As can be seen from Table 1  existing accelerated RIG methods are optimal for solving smooth and
strongly convex ﬁnite-sum problems  since their complexity matches the lower bound in (1.4).
Notwithstanding these recent progresses  there still remain a few signiﬁcant issues on the development
of accelerated RIG methods. Firstly  as pointed out by [25]  existing RIG methods can only establish
accelerated linear convergence based on the assumption that the regularizer h is strongly convex  and
fails to beneﬁt from the strong convexity from the data-ﬁdelity term [26]. This restrictive assumption
does not apply to many important applications (e.g.  Lasso models) where the loss function  rather
than the regularization term  may be strongly convex. Speciﬁcally  when dealing with the case that
only f is strongly convex but not h  one may not be able to shift the strong convexity of f  by
subtracting and adding a strongly convex term  to construct a simple strongly convex term h in the
objective function. In fact  even if f is strongly convex  some of the component functions fi may
only be convex  and hence these fis may become nonconvex after subtracting a strongly convex
term. Secondly  if the strongly convex modulus µ becomes very small  the complexity bounds of all
existing RIG methods will go to +∞ (see column 2 of Table 1)  indicating that they are not robust
against problem ill-conditioning. Thirdly  for solving smooth problems without strong convexity 
one has to add a strongly convex perturbation into the objective function in order to gain up to a
factor of
m over Nesterov’s accelerated gradient method for gradient computation (see column 3 of
Table 1). One signiﬁcant difﬁculty for this indirect approach is that we do not know how to choose
the perturbation parameter properly  especially for problems with unbounded feasible region (see
[2] for a discussion about a similar issue related to SVRG applied to non-strongly convex problems).
However  if one chose not to add the strongly convex perturbation term  the best-known complexity
would be given by Katyushans[1]  which are not more advantageous over Nesterov’s orginal method.
In other words  it does not gain much from randomization in terms of computational complexity.

√

2

Finally  it should be pointed out that only a few existing RIG methods  e.g.  RGEM[19] and [16] 
can be applied to solve stochastic ﬁnite-sum optimization problems  where one can only access the
stochastic gradient of fi via a stochastic ﬁrst-order oracle (SFO).

Table 1: Summary of the recent results on accelerated RIG methods

Deterministic smooth strongly convex Deterministic smooth convex
1

(cid:26)
(cid:26)
(cid:26)

O
O
O

(m +

(cid:27)
(cid:27)
(cid:27)

1

1



 ) log 1

(cid:113) mL
(cid:113) mL
(cid:113) mL
(cid:27)
(cid:113) mL

 +

 ) log2 1

 )





(m +

(cid:26)

(m log 1
O

m√

 +

NA

Algorithms

RPDG[18]

Catalyst[20]

Katyusha[1]

Katyushans[1]

RGEM[19]

(m +

µ ) log 1



(m +

µ ) log 1



O(cid:110)
O(cid:110)
O(cid:110)
O(cid:110)

(m +

(m +

(cid:113) mL
(cid:113) mL
(cid:113) mL
(cid:113) mL

NA

µ ) log 1



µ ) log 1



(cid:111)
(cid:111)1
(cid:111)
(cid:111)

Our contributions. In this paper  we propose a novel accelerated variance reduction type method 
namely the variance-reduced accelerated gradient (Varag ) method  to solve smooth ﬁnite-sum
optimization problems given in the form of (1.1). Table 2 summarizes the main convergence results
achieved by our Varag algorithm.

Table 2: Summary of the main convergence results for Varag

Problem

smooth optimization problems (1.1)

with or without strong convexity

Relations of m  1/ and L/µ

m ≥ D0



4µ

2 or m ≥ 3L
 ≤ 3L
4µ ≤ D0

4µ



m < D0

m < 3L

O(cid:110)

(cid:26)

Uniﬁed results

O(cid:8)m log 1
(cid:9)
(cid:113) mL
(cid:113) mL

m log m +





(cid:27)

O

m log m +

µ log D0/

3L/4µ

(cid:111) 3

Firstly  for smooth convex ﬁnite-sum optimization  our proposed method exploits a direct acceleration
scheme instead of employing any perturbation or restarting techniques to obtain desired optimal
convergence results. As shown in the ﬁrst two rows of Table 2  Varag achieves the optimal rate of
convergence if the number of component functions m is relatively small and/or the required accuracy
is high  while it exhibits a fast linear rate of convergence when the number of component functions
m is relatively large and/or the required accuracy is low  without requiring any strong convexity
assumptions. To the best of our knowledge  this is the ﬁrst time that these complexity bounds have
been obtained through a direct acceleration scheme for smooth convex ﬁnite-sum optimization in the
literature. In comparison with existing methods using perturbation techniques  Varag does not need
to know the target accuracy or the diameter of the feasible region a priori  and thus can be used to
solve a much wider class of smooth convex problems  e.g.  those with unbounded feasible sets.
Secondly  we equip Varag with a uniﬁed step-size policy for smooth convex optimization no matter
(1.1) is strongly convex or not  i.e.  the strongly convex modulus µ ≥ 0. With this step-size policy 
Varag can adjust to different classes of problems to achieve the best convergence results  without
knowing the target accuracy and/or ﬁxing the number of epochs. In particular  as shown in the last
column of Table 2  when µ is relatively large  Varag achieves the well-known optimal linear rate
of convergence. If µ is relatively small  e.g.  µ <   it obtains the accelerated convergence rate that
is independent of the condition number L/µ. Therefore  Varag is robust against ill-conditioning
of problem (1.1). Moreover  our assumptions on the objective function is more general comparing
to those used by other RIG methods  such as RPDG and Katyusha. Speciﬁcally  Varag does not
require to keep a strongly convex regularization term in the projection  and so we can assume that the
strong convexity is associated with the smooth function f instead of the simple proximal function
h(·). Some other advantages of Varag over existing accelerated SVRG methods  e.g.  Katyusha 
1These complexity bounds are obtained via indirect approaches  i.e.  by adding strongly convex perturbation.
2D0 = 2[ψ(x0) − ψ(x∗)] + 3LV (x0  x∗) where x0 is the initial point  x∗ is the optimal solution of (1.1)

and V is deﬁned in (1.5).

3Note that this term is less than O{(cid:113) mL

}.
µ log 1

3

include that it only requires the solution of one  rather than two  subproblems  and that it can allow
the application of non-Euclidean Bregman distance for solving all different classes of problems.
Finally  we extend Varag to solve two more general classes of ﬁnite-sum optimization problems.
We demonstrate that Varag is the ﬁrst randomized method that achieves the accelerated linear rate
of convergence when solving the class of problems that satisﬁes a certain error-bound condition
rather than strong convexity. We then show that Varag can also be applied to solve stochastic smooth
ﬁnite-sum optimization problems resulting in a sublinear rate of convergence.
This paper is organized as follows. In Section 2  we present our proposed algorithm Varag and
its convergence results for solving (1.1) under different problem settings. In Section 3 we provide
extensive experimental results to demonstrate the advantages of Varag over several state-of-the-art
methods for solving some well-known ML models  e.g.  logistic regression  Lasso  etc. We defer the
proofs of the main results in Appendix A.
Notation and terminology. We use (cid:107) · (cid:107) to denote a general norm in Rn without speciﬁc mention 
and (cid:107) · (cid:107)∗ to denote the conjugate norm of (cid:107) · (cid:107). For any p ≥ 1  (cid:107) · (cid:107)p denotes the standard p-norm
in Rn  i.e.  (cid:107)x(cid:107)p
i=1|xi|p  for any x ∈ Rn. For a given strongly convex function w : X → R
with modulus 1 w.r.t. an arbitrary norm (cid:107) · (cid:107)  we deﬁne a prox-function associated with w as

p =(cid:80)n
V (x0  x) ≡ Vw(x0  x) := w(x) −(cid:2)w(x0) + (cid:104)w(cid:48)(x0)  x − x0(cid:105)(cid:3)  

(1.5)

where w(cid:48)(x0) ∈ ∂w(x0) is any subgradient of w at x0. By the strong convexity of w  we have

V (x0  x) ≥ 1

2(cid:107)x − x0(cid:107)2  ∀x  x0 ∈ X.

(1.6)
Notice that V (· ·) described above is different from the standard deﬁnition for Bregman distance [6 
3  4  15  7] in the sense that w is not necessarily differentiable. Throughout this paper  we assume
that the prox-mapping associated with X and h  given by

argminx∈X {γ[(cid:104)g  x(cid:105) + h(x) + µV (x0  x)] + V (x0  x)}  

(1.7)
can be easily computed for any x0  x0 ∈ X  g ∈ Rn  µ ≥ 0  γ > 0. We denote logarithm with base 2
as log. For any real number r  (cid:100)r(cid:101) and (cid:98)r(cid:99) denote the ceiling and ﬂoor of r.

2 Algorithms and main results

This section contains two subsections. We ﬁrst present in Subsection 2.1 a uniﬁed optimal Varag for
solving the ﬁnite-sum problem given in (1.1) as well as its optimal convergence results. Subsection 2.2
is devoted to the discussion of several extensions of Varag . Throughout this section  we assume
that each component function fi is smooth with Li-Lipschitz continuous gradients over X  i.e.  (1.2)
holds for all component functions. Moreover  we assume that the objective function ψ(x) is possibly
strongly convex  in particular  for f (x) = 1
m

(cid:80)m
i=1fi(x)  ∃µ ≥ 0 s.t.

f (y) ≥ f (x) + (cid:104)∇f (x)  y − x(cid:105) + µV (x  y) ∀x  y ∈ X.

(2.1)
Note that we assume the strong convexity of ψ comes from f  and the simple function h is not
necessarily strongly convex. Clearly the strong convexity of h  if any  can be shifted to f since h is
assumed to be simple and its structural information is transparent to us. Also observe that (2.1) is
deﬁned based on a generalized Bregman distance  and together with (1.6) they imply the standard
deﬁnition of strong convexity w.r.t. Euclidean norm.

2.1 Varag for convex ﬁnite-sum optimization

The basic scheme of Varag is formally described in Algorithm 1. In each epoch (or outer loop) 
it ﬁrst computes the full gradient ∇f (˜x) at the point ˜x (cf. Line 3)  which will then be repeatedly
used to deﬁne a gradient estimator Gt at each iteration of the inner loop (cf. Line 8). This is the
well-known variance reduction technique employed by many algorithms (e.g.  [14  27  1  13]). The
inner loop has a similar algorithmic scheme to the accelerated stochastic approximation algorithm
[17  11  12] with a constant step-size policy. Indeed  the parameters used in the inner loop  i.e. 
{γs} {αs}  and {ps}  only depend on the index of epoch s. Each iteration of the inner loop requires
the gradient information of only one randomly selected component function fit  and maintains three
primal sequences  {xt} {xt} and {¯xt}  which play important role in the acceleration scheme.

4

Algorithm 1 The variance-reduced accelerated gradient (Varag ) method
Input: x0 ∈ X {Ts} {γs} {αs} {ps} {θt}  and a probability distribution Q = {q1  . . .   qm} on

{1  . . .   m}.
1: Set ˜x0 = x0.
2: for s = 1  2  . . . do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end for

end for

Set ˜x = ˜xs−1 and ˜g = ∇f (˜x).
Set x0 = xs−1  ¯x0 = ˜x and T = Ts.
for t = 1  2  . . .   T do

Pick it ∈ {1  . . .   m} randomly according to Q.
xt = [(1 + µγs)(1 − αs − ps)¯xt−1 + αsxt−1 + (1 + µγs)ps ˜x] /[1 + µγs(1 − αs)].
Gt = (∇fit(xt) − ∇fit(˜x))/(qitm) + ˜g.
xt = arg minx∈X {γs [(cid:104)Gt  x(cid:105) + h(x) + µV (xt  x)] + V (xt−1  x)}.
¯xt = (1 − αs − ps)¯xt−1 + αsxt + ps ˜x.

Set xs = xT and ˜xs =(cid:80)T

t=1(θt ¯xt)/(cid:80)T

t=1θt.

Note that Varag is closely related to stochastic mirror descent method [22  23] and SVRG[14  27].
By setting αs = 1 and ps = 0  Algorithm 1 simply combines the variance reduction technique
with stochastic mirror descent. In this case  the algorithm only maintains one primal sequence {xt}
and possesses the non-accelerated rate of convergence O{(m + L/µ) log(1/)} for solving (1.1).
Interestingly  if we use Euclidean distance instead of prox-function V (· ·) to update xt and set
X = Rn  Algorithm 1 will further reduce to prox-SVRG proposed in [27].
It is also interesting to observe the difference between Varag and Katyusha [1] because both are
accelerated variance reduction methods. Firstly  while Katyusha needs to assume that the strongly
convex term is speciﬁed as in the form of a simple proximal function  e.g.  (cid:96)1/(cid:96)2-regularizer  Varag
assumes that f is possibly strongly convex  which solves an open issue of the existing accelerated RIG
methods pointed out by [25]. Therefore  the momentum steps in Lines 7 and 10 are different from
Katyusha. Secondly  Varag has a less computationally expensive algorithmic scheme. Particularly 
Varag only needs to solve one proximal mapping (cf. Line 9) per iteration even if f is strongly
convex  while Katyusha requires to solve two proximal mappings per iteration. Thirdly  Varag
incorporates a prox-function V deﬁned in (1.5) rather than the Euclidean distance in the proximal
mapping to update xt. This allows the algorithm to take advantage of the geometry of the constraint
set X when performing projections. However  Katyusha cannot be fully adapted to the non-Euclidean
setting because its second proximal mapping must be deﬁned using the Euclidean distance regardless
the strong convexity of ψ. Finally  we will show in this section that Varag can achieve a much
better rate of convergence than Katyusha for smooth convex ﬁnite-sum optimization by using a novel
approach to specify step-size and to schedule epoch length.
We ﬁrst discuss the case when f is not necessarily strongly convex  i.e.  µ = 0 in (2.1). In Theorem 1 
we suggest one way to specify the algorithmic parameters  including {qi}  {θt}  {αs}  {γs}  {ps}
and {Ts}  for Varag to solve smooth convex problems given in the form of (1.1)  and discuss its
convergence properties of the resulting algorithm. We defer the proof of this result in Appendix A.1.

Theorem 1 (Smooth ﬁnite-sum optimization) Suppose that

i=1Li for i = 1  . . .   m  and weights {θt} are set as

the probabilities qi’s are set

to

Li/(cid:80)m

(αs + ps)

1 ≤ t ≤ Ts − 1
t = Ts.

Moreover  let us denote s0 := (cid:98)log m(cid:99) + 1 and set parameters {Ts}  {γs} and {ps} as

Ts =

  γs = 1

3Lαs

  and ps = 1

2   with

θt =

(cid:26)2s−1 

Ts0 

(cid:40) γs

αs
γs
αs

s ≤ s0
s > s0

(cid:40) 1

αs =

2  
s−s0+4  

2

s ≤ s0
s > s0

.

5

(2.2)

(2.3)

(2.4)

Then the total number of gradient evaluations of fi performed by Algorithm 1 to ﬁnd a stochastic
-solution of (1.1)  i.e.  a point ¯x ∈ X s.t. E[ψ(¯x) − ψ∗] ≤   can be bounded by

O(cid:8)m log D0
(cid:26)

O



m log m +

(cid:9)  
(cid:113) mD0



(cid:27)

¯N :=

m ≥ D0/ 
  m < D0/ 

where D0 is deﬁned as

D0 := 2[ψ(x0) − ψ(x∗)] + 3LV (x0  x∗).

(2.5)

(2.6)

complexity bounded by O{(cid:112)mD0/ + m log m}. Secondly  whenever(cid:112)mD0/ is dominating in

We now make a few observations regarding the results obtained in Theorem 1. Firstly  as mentioned
earlier  whenever the required accuracy  is low and/or the number of components m is large  Varag
can achieve a fast linear rate of convergence even under the assumption that the objective function
is not strongly convex. Otherwise  Varag achieves an optimal sublinear rate of convergence with
√
the second case of (2.5)  Varag can save up to O(
m) gradient evaluations of fi than the optimal
deterministic ﬁrst-order methods for solving (1.1). To the best our knowledge  Varag is the ﬁrst
accelerated RIG in the literature to obtain such convergence results by directly solving (1.1). Other
existing accelerated RIG methods  such as RPDG[18] and Katyusha[1]  require the application
of perturbation and restarting techniques to obtain such convergence results. Thirdly  Varag also
supports mini-batch approach where the component function fi is associated with a mini-batch of
data samples instead of a single data sample. In a more general case  for a given mini-batch size
b  we assume that the component functions can be split into subsets where each subset contains
exactly b number of component functions. Therefore  one can replace Line 8 in Algorithm 1 by
(∇fit(xt) − ∇fit(˜x))/(qitm) + ˜g with Sb being the selected subset and |Sb| = b
Gt = 1
b
Varag can obtain parallel linear speedup of factor b whenever the mini-batch size b ≤ √
and adjust the appropriate parameters to obtain the mini-batch version of Varag . The mini-batch
problem is almost not strongly convex  i.e.  µ ≈ 0. In the latter case  the term(cid:112)mL/µ log(1/)

Next we consider the case when f is possibly strongly convex  including the situation when the

will be dominating in the complexity of existing accelerated RIG methods (e.g.  [18  19  1  20]) and
will tend to ∞ as µ decreases. Therefore  these complexity bounds are signiﬁcantly worse than (2.5)
obtained by simply treating (1.1) as smooth convex problems. Moreover  µ ≈ 0 is very common in
ML applications. In Theorem 2  we provide a uniﬁed step-size policy which allows Varag to achieve
optimal rate of convergence for ﬁnite-sum optimization in (1.1) regardless of its strong convexity 
and hence it can achieve stronger rate of convergence than existing accelerated RIG methods if the
condition number L/µ is very large. The proof of this result can be found in Appendix A.2.

(cid:80)

it∈Sb

m.

qi’s are set to Li/(cid:80)m

Theorem 2 (A uniﬁed result for convex ﬁnite-sum optimization) Suppose that the probabilities
i=1Li for i = 1  . . .   m. Moreover  let us denote s0 := (cid:98)log m(cid:99) + 1 and assume
4µ . Otherwise 

that the weights {θt} are set to (2.2) if 1 ≤ s ≤ s0 or s0 < s ≤ s0 +
they are set to

(cid:113) 12L
mµ − 4  m < 3L
(cid:26)Γt−1 − (1 − αs − ps)Γt  1 ≤ t ≤ Ts − 1 
(cid:40) 1

where Γt = (1 + µγs)t. If the parameters {Ts}  {γs} and {ps} set to (2.3) with

t = Ts 

Γt−1 

θt =

(2.7)

(2.8)

2}(cid:111)

 

3L   1

s ≤ s0 
s > s0 

then the total number of gradient evaluations of fi performed by Algorithm 1 to ﬁnd a stochastic
-solution of (1.1) can be bounded by

αs =

2  
max

O(cid:8)m log D0

(cid:26)
O(cid:110)

O



m log m +

m log m +

(cid:110) 2
s−s0+4   min{(cid:112) mµ
(cid:9)  
(cid:113) mD0
(cid:113) mL

(cid:27)

(cid:111)

 

µ log D0/

3L/4µ



¯N :=

where D0 is deﬁned as in (2.6).

6

m ≥ D0
m < D0

 or m ≥ 3L
4µ  
 ≤ 3L
4µ  
4µ ≤ D0
 .

  m < 3L

(2.9)

Observe that the complexity bound (2.9) is a uniﬁed convergence result for Varag to solve deter-
ministic smooth convex ﬁnite-sum optimization problems (1.1). When the strong convex modulus µ
of the objective function is large enough  i.e.  3L/µ < D0/  Varag exhibits an optimal linear rate
of convergence since the third case of (2.9) matches the lower bound (1.4) for RIG methods. If µ
is relatively small  Varag treats the ﬁnite-sum problem (1.1) as a smooth problem without strong
convexity  which leads to the same complexity bounds as in Theorem 1. It should be pointed out that
the parameter setting proposed in Theorem 2 does not require the values of  and D0 given a priori.

2.2 Generalization of Varag

In this subsection  we extend Varag to solve two general classes of ﬁnite-sum optimization problems
as well as establishing its convergence properties for these problems.
Finite-sum problems under error bound condition. We investigate a class of weakly strongly
convex problems  i.e.  ψ(x) is smooth convex and satisﬁes the error bound condition given by

V (x  X∗) ≤ 1

¯µ (ψ(x) − ψ∗)  ∀x ∈ X 

(2.10)
where X∗ denotes the set of optimal solutions of (1.1). Many optimization problems satisfy (2.10) 
for instance  linear systems  quadratic programs  linear matrix inequalities and composite problems
(outer: strongly convex  inner: polyhedron functions)  see [8] and Section 6 of [21] for more examples.
Although these problems are not strongly convex  by properly restarting Varag we can solve them
with an accelerated linear rate of convergence  the best-known complexity result to solve this class of
problems so far. We formally present the result in Theorem 3  whose proof is given in Appendix A.3.

qi’s are set to Li/(cid:80)m

Theorem 3 (Convex ﬁnite-sum optimization under error bound) Assume that the probabilities
i=1Li for i = 1  . . .   m  and θt are deﬁned as (2.2). Moreover  let us set
parameters {γs}  {ps} and {αs} as in (2.3) and (2.4) with {Ts} being set as

(cid:26)T12s−1 

8T1 

Ts =

s ≤ 4
s > 4

 

(cid:113) L

¯µm  

(2.11)

(2.12)

where T1 = min{m  L

¯µ}. Then under condition (2.10)  for any x∗ ∈ X∗  s = 4 + 4

Moreover  if we restart Varag every time it runs s iterations for k = log ψ(x0)−ψ(x∗)
number of gradient evaluations of fi to ﬁnd a stochastic -solution of (1.1) can be bounded by

times  the total

E[ψ(˜xs) − ψ(x∗)] ≤ 5

¯N := k((cid:80)

s(m + Ts)) = O(cid:110)(cid:0)m +

16 [ψ(x0) − ψ(x∗)].
(cid:113) mL

(cid:1) log ψ(x0)−ψ(x∗)



¯µ



(cid:111)

.

(2.13)

Remark 1 Note that Varag can also be extended to obtain an uniﬁed result as shown in Theorem 2
for solving ﬁnite-sum problems under error bound condition. In particular  if the condition number is
very large  i.e.  s = O{L/(¯µm)} ≈ ∞  Varag will never be restarted  and the resulting complexity
bounds will reduce to the case for solving smooth convex problems provided in Theorem 1.

Stochastic ﬁnite-sum optimization. We now consider stochastic smooth convex ﬁnite-sum opti-
mization and online learning problems deﬁned as in (1.3)  where only noisy gradient information of
fi can be accessed via a SFO oracle. In particular  for any x ∈ X  the SFO oracle outputs a vector
Gi(x  ξj) such that

Eξj [Gi(x  ξj)] = ∇fi(x)  i = 1  . . .   m 
Eξj [(cid:107)Gi(x  ξj) − ∇fi(x)(cid:107)2∗] ≤ σ2  i = 1  . . .   m.

(2.14)
(2.15)

We present the variant of Varag for stochastic ﬁnite-sum optimization in Algorithm 2 as well as its
convergence results in Theorem 4  whose proof can be found in Appendix B.

Theorem 4 (Stochastic smooth ﬁnite-sum optimization) Assume that θt are deﬁned as in (2.2) 
i=1Li for i = 1  . . .   m. Moreover  let us

qim2 and the probabilities qi’s are set to Li/(cid:80)m

C :=(cid:80)m

i=1

1

7

Algorithm 2 Stochastic accelerated variance-reduced stochastic gradient descent (Stochastic Varag )
This algorithm is the same as Algorithm 1 except that for given batch-size parameters Bs and bs 
Line 3 is replaced by ˜x = ˜xs−1 and

˜g = 1
m

i=1

Gi(˜x) := 1
Bs

(cid:80)m

(cid:110)
(cid:80)bs

k=1

(cid:111)
(cid:80)Bs
k) − Git(˜x)(cid:1) + ˜g.
(cid:0)Git(xt  ξs

j=1Gi(˜x  ξs
j )

 

and Line 8 is replaced by

Gt = 1

qit mbs

(2.16)

(2.17)

(2.18)

denote s0 := (cid:98)log m(cid:99) + 1 and set Ts  αs  γs and ps as in (2.3) and (2.4). Then the number of calls
to the SFO oracle required by Algorithm 2 to ﬁnd a stochastic -solution of (1.1) can be bounded by

NSFO =(cid:80)

s(mBs + Tsbs) =

where D0 is given in (2.6).

O(cid:110) mCσ2
(cid:111)
(cid:111)
O(cid:110) Cσ2D0

L

L2

  m ≥ D0/ 
  m < D0/ 

Remark 2 Note that the constant C in (2.18) can be easily upper bounded by
min{Li}   and C = 1 if
Li = L ∀i. To the best of our knowledge  among a few existing RIG methods that can be applied to
solve the class of stochastic ﬁnite-sum problems  Varag is the ﬁrst to achieve such complexity results
as in (2.18) for smooth convex problems. RGEM[19] obtains nearly-optimal rate of convergence
for strongly convex case  but cannot solve stochastic smooth problems directly  and [16] required a
speciﬁc initial point  i.e.  an exact solution to a proximal mapping depending on the variance σ2  to

achieve O(cid:8)m log m + σ2/2(cid:9) rate of convergence for smooth convex problems.

L

3 Numerical experiments

suggesting sampling distribution should be non-uniform  i.e.  qi = Li/(cid:80)m

In this section  we demonstrate the advantages of our proposed algorithm  Varag over several state-
of-the-art algorithms  e.g.  SVRG++ [2] and Katyusha [1]  etc.  via solving several well-known
machine learning models. For all experiments  we use public real datasets downloaded from UCI
Machine Learning Repository [10] and uniform sampling strategy to select fi. Indeed  the theoretical
i=1Li  which results in the
optimal constant L appearing in the convergence results. However  a uniform sampling strategy will
only lead to a constant factor slightly larger than L = 1
i=1Li. Moreover  it is computationally
m
efﬁcient to estimate Li by performing maximum singular value decomposition of the Hessian since
only a rough estimation sufﬁces.
Unconstrained smooth convex problems. We ﬁrst investigate unconstrained logistic models which
cannot be solved via the perturbation approach due to the unboundedness of the feasible set. More
speciﬁcally  we applied Varag   SVRG++ and Katyushans to solve a logistic regression problem 

(cid:80)m

(cid:80)m
i=1fi(x)} where fi(x) := log(1 + exp(−biaT

i x))}.

(3.1)

{ψ(x) := 1

m

min
x∈Rn

Here (ai  bi) ∈ Rn × {−1  1} is a training data point and m is the sample size  and hence fi now
corresponds to the loss generated by a single training data. As we can see from Figure 1  Varag
converges much faster than SVRG++ and Katyusha in terms of training loss.

8

Diabetes (m = 1151) 
unconstrained logistic

Breast Cancer Wisconsin (m = 683) 

unconstrained logistic

Figure 1: The algorithmic parameters for SVRG++ and Katyushans are set according to [2] and [1]  respectively 
and those for Varag are set as in Theorem 1.
Strongly convex loss with simple convex regularizer. We now study the class of Lasso regression
problems with λ as the regularizer coefﬁcient  given in the following form

(cid:80)m
i=1fi(x) + h(x)} where fi(x) := 1

i x − bi)2  h(x) := λ(cid:107)x(cid:107)1.

{ψ(x) := 1

(3.2)

2 (aT

min
x∈Rn

m

Due to the assumption SVRG++ and Katyusha enforced on the objective function that the strong
convexity can only be associated with the regularizer  these methods always view Lasso as smooth
problems [25]  while Varag can treat Lasso as strongly convex problems. As can be seen from
Figure 2  Varag outperforms SVRG++ and Katyushans in terms of training loss.

Diabetes (m = 1151) 

Lasso λ = 0.001

Breast Cancer Wisconsin (m = 683) 

Lasso λ = 0.001

(cid:80)m
i=1fi(x)} where fi(x) := 1

{ψ(x) := 1

Figure 2: The algorithmic parameters for SVRG++ and Katyushans are set according to [2] and [1]  respectively 
and those for Varag are set as in Theorem 2.
Weakly strongly convex problems satisfying error bound condition. Let us consider a special
class of ﬁnite-sum convex quadratic problems given in the following form

m

min
x∈Rn

2 xT Qix + qT

(3.3)
Here qi = −Qixs and xs is a solution to the symmetric linear system Qix + qi = 0 with Qi (cid:23) 0.
[8][Section 6] and [21][Section 6.1] proved that (3.3) belongs to the class of weakly strongly convex
problems satisfying error bound condition (2.10). For a given solution xs  we use the following real
datasets to generate Qi and qi. We then compare the performance of Varag with fast gradient method
(FGM) proposed in [21]. As shown in Figure 3  Varag outperforms FGM for all cases. And as the
number of component functions m increases  Varag demonstrates more advantages over FGM. These
can save up to O{√
numerical results are consistent with the theoretical complexity bound (2.13) suggesting that Varag
m} number of gradient computations than deterministic algorithms  e.g.  FGM.

i x.

Diabetes (m = 1151)

Parkinsons Telemonitoring (m = 5875)

Figure 3: The algorithmic parameters for FGM and Varag are set according to [21] and Theorem 3  respectively.
More numerical experiment results on another problem case  strongly convex problems with small
strongly convex modulus  can be found in Appendix C.

9

References
[1] Zeyuan Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods.

ArXiv e-prints  abs/1603.05953  2016.

[2] Zeyuan Allen-Zhu and Yang Yuan. Improved svrg for non-strongly-convex or sum-of-non-
convex objectives. In International conference on machine learning  pages 1080–1089  2016.

[3] A. Auslender and M. Teboulle. Interior gradient and proximal methods for convex and conic

optimization. SIAM Journal on Optimization  16:697–725  2006.

[4] H.H. Bauschke  J.M. Borwein  and P.L. Combettes. Bregman monotone optimization algorithms.

SIAM Journal on Controal and Optimization  42:596–636  2003.

[5] D. Blatt  A. Hero  and H. Gauchman. A convergent incremental gradient method with a constant

step size. SIAM Journal on Optimization  18(1):29–51  2007.

[6] L.M. Bregman. The relaxation method of ﬁnding the common point convex sets and its
application to the solution of problems in convex programming. USSR Comput. Math. Phys. 
7:200–217  1967.

[7] Yair Censor and Arnold Lent. An iterative row-action method for interval convex programming.

Journal of Optimization theory and Applications  34(3):321–353  1981.

[8] Cong D Dang  Guanghui Lan  and Zaiwen Wen. Linearly convergent ﬁrst-order algorithms for

semideﬁnite programming. Journal of Computational Mathematics  35(4):452–468  2017.

[9] A. Defazio  F. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method
with support for non-strongly convex composite objectives. Advances of Neural Information
Processing Systems (NIPS)  27  2014.

[10] Dheeru Dua and Casey Graff. UCI machine learning repository  2017.

[11] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization  I: a generic algorithmic framework. SIAM Journal on
Optimization  22:1469–1492  2012.

[12] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization  II: shrinking procedures and optimal algorithms. SIAM
Journal on Optimization  23:2061–2089  2013.

[13] Elad Hazan and Haipeng Luo. Variance-reduced and projection-free stochastic optimization.

CoRR  abs/1602.02101  2  2016.

[14] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. Advances of Neural Information Processing Systems (NIPS)  26:315–323  2013.

[15] K.C. Kiwiel. Proximal minimization methods with generalized bregman functions. SIAM

Journal on Controal and Optimization  35:1142–1168  1997.

[16] Andrei Kulunchakov and Julien Mairal. Estimate sequences for stochastic composite optimiza-
tion: Variance reduction  acceleration  and robustness to noise. arXiv preprint arXiv:1901.08788 
2019.

[17] Guanghui Lan. An optimal method for stochastic composite optimization. Mathematical

Programming  133(1-2):365–397  2012.

[18] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. Mathematical

programming  pages 1–49  2017.

[19] Guanghui Lan and Yi Zhou. Random gradient extrapolation for distributed and stochastic

optimization. SIAM Journal on Optimization  28(4):2753–2782  2018.

[20] Hongzhou Lin  Julien Mairal  and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimiza-

tion. In Advances in Neural Information Processing Systems  pages 3384–3392  2015.

10

[21] Ion Necoara  Yu Nesterov  and Francois Glineur. Linear convergence of ﬁrst order methods for

non-strongly convex optimization. Mathematical Programming  pages 1–39  2018.

[22] A. S. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation

approach to stochastic programming. SIAM Journal on Optimization  19:1574–1609  2009.

[23] A. S. Nemirovski and D. Yudin. Problem complexity and method efﬁciency in optimization.

Wiley-Interscience Series in Discrete Mathematics. John Wiley  XV  1983.

[24] Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ﬁnite sums with the stochastic

average gradient. Mathematical Programming  162(1-2):83–112  2017.

[25] Junqi Tang  Mohammad Golbabaee  Francis Bach  et al. Rest-katyusha: Exploiting the solution’s
structure via scheduled restart schemes. In Advances in Neural Information Processing Systems 
pages 429–440  2018.

[26] Jialei Wang and Lin Xiao. Exploiting strong convexity from data with primal-dual ﬁrst-order
algorithms. In Proceedings of the 34th International Conference on Machine Learning-Volume
70  pages 3694–3702. JMLR. org  2017.

[27] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

11

,Guanghui Lan
Zhize Li
Yi Zhou