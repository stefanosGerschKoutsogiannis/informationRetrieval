2019,Differentially Private Bagging: Improved utility and cheaper privacy than subsample-and-aggregate,Differential Privacy is a popular and well-studied notion of privacy. In the era ofbig data that we are in  privacy concerns are becoming ever more prevalent and thusdifferential privacy is being turned to as one such solution. A popular method forensuring differential privacy of a classifier is known as subsample-and-aggregate in which the dataset is divided into distinct chunks and a model is learned on eachchunk  after which it is aggregated. This approach allows for easy analysis of themodel on the data and thus differential privacy can be easily applied. In this paper we extend this approach by dividing the data several times (rather than just once)and learning models on each chunk within each division. The first benefit of thisapproach is the natural improvement of utility by aggregating models trained ona more diverse range of subsets of the data (as demonstrated by the well-knownbagging technique). The second benefit is that  through analysis that we provide inthe paper  we can derive tighter differential privacy guarantees when several queriesare made to this mechanism.  In order to derive these guarantees  we introducethe upwards and downwards moments accountants and derive bounds for thesemoments accountants in a data-driven fashion. We demonstrate the improvementsour model makes over standard subsample-and-aggregate in two datasets (HeartFailure (private) and UCI Adult (public)).,Differentially Private Bagging: Improved utility and

cheaper privacy than subsample-and-aggregate

James Jordon

University of Oxford

james.jordon@wolfson.ox.ac.uk

Jinsung Yoon

University of California  Los Angeles

jsyoon0823@g.ucla.edu

Mihaela van der Schaar
University of Cambridge

mv472@cam.ac.uk  mihaela@ee.ucla.edu

University of California  Los Angeles

Alan Turing Institute

Abstract

Differential Privacy is a popular and well-studied notion of privacy. In the era of
big data that we are in  privacy concerns are becoming ever more prevalent and thus
differential privacy is being turned to as one such solution. A popular method for
ensuring differential privacy of a classiﬁer is known as subsample-and-aggregate 
in which the dataset is divided into distinct chunks and a model is learned on each
chunk  after which it is aggregated. This approach allows for easy analysis of the
model on the data and thus differential privacy can be easily applied. In this paper 
we extend this approach by dividing the data several times (rather than just once)
and learning models on each chunk within each division. The ﬁrst beneﬁt of this
approach is the natural improvement of utility by aggregating models trained on
a more diverse range of subsets of the data (as demonstrated by the well-known
bagging technique). The second beneﬁt is that  through analysis that we provide in
the paper  we can derive tighter differential privacy guarantees when several queries
are made to this mechanism. In order to derive these guarantees  we introduce
the upwards and downwards moments accountants and derive bounds for these
moments accountants in a data-driven fashion. We demonstrate the improvements
our model makes over standard subsample-and-aggregate in two datasets (Heart
Failure (private) and UCI Adult (public)).

1

Introduction

In the era of big data that we live in today  privacy concerns are becoming ever more prevalent. It
falls to the researchers using the data to ensure that adequate measures are taken to ensure any results
that are put into the public domain (such as the parameters of a model learned on the data) do not
disclose sensitive attributes of the real data. For example  it is well known that the high capacity
of deep neural networks can cause the networks to "memorize" training data; if such a network’s
parameters were made public  it may be possible to deduce some of the training data that was used to
train the model  thus resulting in real data being leaked to the public.
Several attempts have been made at rigorously deﬁning what it means for an algorithm  or an
algorithm’s output  to be "private". One particularly attractive and well-researched notion is that of
differential privacy [1]. Differential privacy is a formal deﬁnition that requires that the distribution of
the output of a (necessarily probabilistic) algorithm not be too different when a single data point is
included in the dataset or not. Typical methods for enforcing differential privacy involve bounding

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) Subsample-and-aggregate

(b) Differentially Private Bagging

Figure 1: A comparison of how the dataset is used in (a) subsample-and-aggregate and (b) our
differentially private bagging procedure. By partitioning the dataset multiple times we are able to
perform a tighter privacy analysis using our personalised moments accountant in addition to learning
a better performing underlying classiﬁer.

the effect that inclusion of a single sample can have on the output and then adding noise (typically
Laplacian or Gaussian) proportional to this effect. The most difﬁcult step in this process is in attaining
a good bound on the effect of inclusion.
One method for bypassing this difﬁculty  is to build a classiﬁer by dividing up the dataset into distinct
subsets  training a separate classiﬁer on each chunk  and then aggregating these classiﬁers. The effect
of a single sample is then bounded by the fact that it was used only to train exactly one of these
models and thus its inclusion or exclusion will affect only that model’s output. By dividing the data
into smaller chunks  we learn more models and thus the one model that a sample can effect becomes
a smaller "fraction" of the overall model  thus resulting in a smaller effect that any one sample has on
the model as a whole. This method is commonly referred to as subsample-and-aggregate [2  3  4].
In this work  we propose an extension to the subsample-and-aggregate methodology that has sim-
ilarities with bagging [5]. Fig. 1 depicts the key methodological difference between standard
subsample-and-aggregate and our proposed framework  Differentially Private Bagging (DPBag) 
namely that we partition the dataset many times. This multiple-partitioning not only improves utility
by building a better predictor  but also enjoys stronger privacy guarantees due to the fact that the
effect of adding or removing a single sample can be more tightly bounded within our framework.
In order to prove these guarantees  we introduce the personalised moments accountants  which are
data-driven variants of the moments accountant [6]  that allow us to track the privacy loss with respect
to each sample in the dataset and then deduce the ﬁnal privacy loss by taking the maximum loss
over all samples. The personalised moments accountant also lends itself to allowing for personalised
differential privacy [7] in which we may wish to allow each individual to specify their own privacy
parameters.
We demonstrate the efﬁcacy of our model on two classiﬁcation tasks  demonstrating that our model is
an improvement over the standard subsample-and-aggregate algorithm.

2 Related Works

Several works have proposed methods for differentially private classiﬁcation. Of particular interest
is the method of [6]  in which they propose a method for differentially private training of deep
neural networks. In particular  they introduce a new piece of mathematical machinery  the moments
accountant. The moments accountant allows for more efﬁcient composition of differentially private
mechanisms than either simple or advanced composition [1]. Fortunately  the moments accountant
is not exclusive to deep networks and has proven to be useful in other works. In this paper  we use
two variants of the moments accountant  which we refer to collectively as the personalised moments
accountants. Our algorithm lends itself naturally to being able to derive tighter bounds on these
personalised moments accountants than would be possible on the "global" moments accountant.
Most other methods use the subsample-and-aggregate framework (ﬁrst discussed in [2]) to guarantee
differential privacy. A popular  recent subsample-and-aggregate method is Private Aggregation of
Teacher Ensembles (PATE)  proposed in [8]. Their main contribution is to provide a data-driven
bound on the moments accountant for a given query to the subsample-and-aggregate mechanism that

2

they claim signiﬁcantly reduces the privacy cost over the standard data-independent bound. This is
further built on in [9] by adding a mechanism that ﬁrst determines whether or not a query will be too
expensive to answer or not  only answering those that are sufﬁciently cheap. Both works use standard
subsample-and-aggregate in which the data is partitioned only once. Our method is more fundamental
than PATE  in the sense that the techniques used by PATE to improve on subsample-and-aggregate
would also be applicable to our differentially private bagging algorithm. The bound they derive in [8]
on the moments accountant should translate to our personalised moments accountants in the same
way the data-independent bound does (i.e. by multiplying the dependence on the inverse noise scale
by a data-driven value) and as such our method would provide privacy improvements over PATE
similar to the improvements it provides over standard subsample-and-aggregate. We give an example
of our conjectured result for PATE in the Supplementary Materials for clarity.
Another method that utilises subsample-and-aggregate is [10]  in which they use the distance to
instability framework [4] combined with subsample-and-aggregate to privately determine whether a
query can be answered without adding any noise to it. In cases where the query can be answered 
no privacy cost is incurred. Whenever the query cannot be answered  no answer is given but a
privacy cost is incurred. Unfortunately  the gains to be had by applying our method over basic
subsample-and-aggregate to their work are not clear  but we believe that at the very least  the utility
of the answer provided may be improved on due to the ensemble having a higher utility in our case
(and the same privacy guarantees will hold that they prove).
In [11]  they build a method for learning a differentially private decision tree. Although they apply
bagging to their framework  they do not do so to create privacy  but only to improve the utility of
their learned classiﬁer. The privacy analysis they provide is performed only on each individual tree
and not on the ensemble as a whole.

3 Differential Privacy
Let us denote the feature space by X   the set of possible class labels by C and write U = X × C. Let
us denote by D the collection of all possible datasets consisting of points in U. We will write D to
denote a dataset in D  so that D = {ui}N
We ﬁrst provide some preliminaries on differential privacy [1] before describing our method; we refer
interested readers to [1] for a thorough exposition of differential privacy. We will denote an algorithm
by M  which takes as input a dataset D and outputs a value from some output space  R.
Deﬁnition 1 (Neighboring Datasets [1]). Two datasets D D(cid:48) are said to be neighboring if

i=1 = {(xi  yi)}N

i=1 for some N.

∃u ∈ U s.t. D \ {u} = D(cid:48) or D(cid:48) \ {u} = D.

Deﬁnition 2 (Differential Privacy [1]). A randomized algorithm  M  is (  δ)-differentially private if
for all S ⊂ R and for all neighboring datasets D D(cid:48):

P(M(D) ∈ S) ≤ eP(M(D(cid:48)) ∈ S) + δ

where P is taken with respect to the randomness of M.
Differential privacy provides an intuitively understandable notion of privacy - a particular sample’s
inclusion or exclusion in the dataset does not change the probability of a particular outcome very
much: it does so by a multiplicative factor of e and an additive amount  δ.

4 Differentially Private Bagging

In order to enforce differential privacy  we must bound the effect of a sample’s inclusion or exclusion
on the output of the model. In order to do this  we propose a model for which the maximal effect can
be easily deduced and moreover  for which we can actually show a lesser maximal effect by analysing
the training procedure and deriving data-driven privacy guarantees.
We begin by considering k (random) partitions of the dataset  D1  ... Dk with Di = {Di
n}
1  ...  Di
n (cid:99) or (cid:100)|D|
j is a set of size (cid:98)|D|
n (cid:101). We then train a "teacher" model  Tij on each
for each i  where Di
j). We note that each sample u ∈ D is in precisely one set
of these sets (i.e. Tij is trained on Di
from each partition and thus in precisely k sets overall; it is therefore used to train k teachers. We

3

collect the indices of the corresponding teachers in the set I(u) = {(i  j) : u ∈ Di
j} and denote by
T (u) = {Tij : (i  j) ∈ I(u)} the set of teachers trained using the sample u.
Given a new sample to classify x ∈ X   we ﬁrst compute for each class the number of teachers that
output that class  nc(x) = |{(i  j) : Tij(x) = c}|. The model then classiﬁes the sample as

ˆc(x) = arg max{nc(x) : c ∈ C}

i.e. by classifying it as the class with the most votes. To make the output differentially private  we
can add independent Laplacian noise to each of the resulting counts before taking arg max. So that
the classiﬁcation becomes

˜cλ(x) = arg max{nc(x) + Yc : c ∈ C}

where Yc  c ∈ C are independent Lap( k
λ ) random variables and where λ is a hyper-parameter of
our model. We scale the noise to the number of partitions because the number of partitions is
precisely the total number of teachers that any individual sample can effect. Thus the (naive) bound
on the (cid:96)1-sensitivity of this algorithm is k  giving us the following theorem  which tells us that our
differentially private bagging algorithm is at least as private as the standard subsample-and-aggregate
mechanism  independent of the number of partitions used.
Theorem 1. With k partitions and n teachers per partition  ˜cλ is 2λ-differentially private with respect
to the data D.

Proof. This follows immediately from noting that the (cid:96)1-sensitivity of nc(x) is k. See [1].

We note that the standard subsample-and-aggregate algorithm can be recovered from ours by setting
k = 1. In the next section  we will derive tighter bounds on the differential privacy of our bagging
algorithm when several queries are made to the classiﬁer.

4.1 Personalised Moments Accountants

In order to provide tighter differential privacy guarantees for our method  we now introduce the
personalised moments accountants. Like the original moments accountant from [6]  these will allow
us to compose a sequence of differentially private mechanisms more efﬁciently than using standard
or advanced composition [1]. We begin with a preliminary deﬁnition (found in [6]).
Deﬁnition 3 (Privacy Loss and Privacy Loss Random Variable [6]). Let M : D → R be a random-
ized algorithm  with D and D(cid:48) a pair of neighbouring datasets. Let aux be any auxiliary input. For
any outcome o ∈ R  we deﬁne the privacy loss at o to be:

c(o;M  aux D D(cid:48)) = log

P(M(D  aux) = o)
P(M(D(cid:48)  aux) = o)
C(M  aux D D(cid:48)) = c(M(D  aux)  aux D D(cid:48))

with the privacy loss random variable  C  being deﬁned by
i.e. the random variable deﬁned by evaluating the privacy loss at a sample from M(D  aux).
In deﬁning the moments accountant  an intermediate quantity  referred to by [6] as the "l-th moment"
is introduced. We divide the deﬁnition of this l-th moment into a downwards and an upwards version
(corresponding to whether D(cid:48) is obtained by either removing or adding an element to D  respectively).
We do this because the upwards moments accountant must be bounded among all possible points
u ∈ U that could be added  whereas the downwards moments accountants need only consider the
points that are already in D.
Deﬁnition 4. Let D be some dataset and let u ∈ D. Let aux be any auxiliary input. Then the
downwards moments accountant is given by
Deﬁnition 5. Let D be some dataset. Then the upwards moments accountant is deﬁned as

ˇαM(l; aux D  u) = log E(exp(lC(M  aux D D \ {u}))).
u∈U log E(exp(lC(M  aux D D ∪ {u}))).
ˆαM(l; aux D) = max

We can recover the original moments accountant from [6]  αM(l)  as

αM(l) = max

aux D{ˆαM(l; aux D)  max

u

ˇαM(l; aux D  u)}.

(1)

We will use this fact  together with the two theorems in the following subsection  to calculate the
ﬁnal global privacy loss of our mechanism.

4

4.2 Results inherited from the Moments Accountant

The following two theorems state two properties that our personalised moments accountants share
with the original moments accountant. Note that the composability in Theorem 2 is being applied to
each personalised moments accountant individually.
Theorem 2 (Composability). Suppose that an algorithm M consists of a sequence of adaptive
algorithms (i.e. algorithms that take as auxiliary input the outputs of the previous algorithms)

M1  ... Mm where Mi :(cid:81)i−1

j=1 Rj × D → Ri. Then  for any l
ˇαMi(l;D  u)

ˇαM(l;D  u) ≤ m(cid:88)
ˆαM(l;D) ≤ m(cid:88)

ˆαMi(l;D).

i=1

and

i=1

Proof. The statement of this theorem is a variation on Theorem 2 from [6]  applied to the personalised
moments accountants. Their proof involves proving this stronger result. See [6]  Theorem 2 proof.
Theorem 3 ((  δ) from α(l) [6]). Let δ > 0. Any mechanism M is (  δ)-differentially private for

 = min

l

αM(l) + log( 1
δ )

l

(2)

Proof. See [6]  Theorem 2.

Theorem 2 means that bounding each personalised moments accountant individually could provide a
signiﬁcant improvement on the overall bound for the moments accountant. Combined with Eq. 1  we
can ﬁrst sum over successive steps of the algorithm and then take the maximum. In contrast  original
approaches that bound only the overall moments accountant at each step essentially compute

m(cid:88)
aux D{ m(cid:88)

i=1

i=1

max

aux D{ˆαMi(l; aux D)  max
ˇαMi(l; aux D  u)}.
m(cid:88)

ˆαMi(l; aux D)  max

ˇαMi(l; aux D  u)}

u

u

i=1

αM(l) =

αM(l) = max

(3)

(4)

Our approach of bounding the personalised moments accountant allows us to compute the bound as

which is strictly smaller whenever there is not some personalised moments accountant that is always
larger than all other personalised moments accountants. The bounds we derive in the following
subsection and the subsequent remarks will make clear why this is an unlikely scenario.

4.3 Bounding the Personalised Moments Accountants

Having deﬁned the personalised moments accountants  we can now state our main theorems  which
provide a data-dependent bound on the personalised moments accountant for a single query to ˜cλ.
Theorem 4 (Downwards bound). Let xnew ∈ X be a new point to classify. For each c ∈ C and each
u ∈ D  deﬁne the quantities

|{(i  j) ∈ I(u) : Tij(xnew) = c}|

nc(xnew; u) =

k

i.e. nc(xnew; u) is the fraction of teachers that were trained on a dataset containing u that output
class c when classifying xnew. Let

m(xnew; u) = max

c

{1 − nc(xnew; u)}.

Then

ˇα˜cλ(xnew)(l;D  u) ≤ 2λ2m(xnew; u)2l(l + 1).

(5)

5

Proof. (Sketch.) The theorem follows from the fact that m(xnew; u) is the maximum change that can
occur in the vote fractions  nc  c ∈ C when the sample u is removed from the training of each model
in T (u)  corresponding to all teachers that were not already voting for the minority class switching
their vote to the minority class. m can thus be thought of as the personalised (cid:96)1-sensitivity of a
speciﬁc query to our algorithm  and so the standard sensitivity based argument gives us that ˜cλ(xnew)
is 2λm(xnew; u)-differentially private with respect to removing u. The bound on the (downwards)
moments accountant then follows using a similar argument to the proof of Prop. 3.3 in [12].

To prove the upwards bound  we must understand what happens when we add a point to our training
data - which is that it will be added to a training set for precisely 1 teacher in each of the k partitions.
Each dataset in a partition will either be of size (cid:100)|D|
n (cid:99). We assume (without loss of generality)
that a new point is added to the ﬁrst dataset in each partition that contains (cid:98)|D|
n (cid:99) samples. We collect
the indices of these datasets in I(∗) and denote the set of teachers trained on these subsets by T (∗).
Theorem 5 (Upwards bound). Let xnew ∈ X be a new point to classify. For each c ∈ C  deﬁne the
quantity

n (cid:101) or (cid:98)|D|

nc(xnew;∗) =

|{(i  j) ∈ I(∗) : Tij(xnew) = c}|

k

i.e. nc(xnew;∗) is the fraction of teachers whose training set would receive the new point that output
class c when classifying xnew. Let

m(xnew;∗) = max

{1 − nc(xnew;∗)}.

c

Then

ˆα˜cλ(xnew)(l;D) ≤ 2λ2m(xnew;∗)2l(l + 1).

(6)

Proof. The proof is exactly as for Theorem 4  replacing I(u) and T (u) with I(∗) and T (∗).

The standard bound on the moments accountant of a 2λ differentially private algorithm is 2λ2l(l + 1)
(see [12]). Thus  our theorems introduce a factor of m(xnew; u)2. Note that by deﬁnition m ≤ 1 and
thus our bound is in general tighter  but always at least as tight. It should be noted  however  that
for a single query  this bound may not improve on the naive 2λ2l(l + 1) bound  since in that case
equations 3 and 4 are equal. If there is any training sample u ∈ D ∪ {∗} and any class c ∈ C for
which all teachers in T (u) classify xnew as some class other than c then m(xnew; u) = 1. However 
over the course of several queries  it is unlikely that each set of teachers T (u) always exclude some
class  and as such the total bound according to Theorems 2  4 and 5 is lower than if we just used the
naive bound. In the case of binary classiﬁcation  for example  the bounds are only the same if there is
some set of teachers that are always unanimous when classifying new samples.
Remarks. (i) m(xnew; u) is smallest when the teachers in T (u) are divided evenly among the
classes when classifying xnew  this is intuitive because in such a situation  u is providing very little
information about how to classify u and thus little is being leaked about u when we classify xnew.
(ii) m(u) is bounded below by 1 − 1|C| and so our method will provide the biggest improvements for
binary classiﬁcation and the improvements will decay as the number of classes increases.
(iii) When k = 1  m(u) is always 1 because nc is 1 for some c ∈ C and then 0 for all remaining
classes and from this we recover the standard bound of 2λl(l + 1) used for subsample-and-aggregate.
(iv) For Eq. 3 and 4 to be equal  there must exist some u∗ for which m(xnew; u∗) > m(xnew; u) for
all u and xnew. This amounts to there being some set of teachers (corresponding to u∗) that are in
more agreement than every other set of teachers for every new point they are asked to classify. Other
than in this unlikely scenario  Eq. 4 will be strictly smaller than Eq. 3.

4.4 Semi-supervised knowledge transfer

We now discuss how best to leverage the fact that the best gains from our approach come from
answering several queries (as implied by equations 3 and 4). We ﬁrst note that the vanilla subsample-
and-aggregate method does not derive data-dependent privacy guarantees for an individual query  and
thus  for a ﬁxed  and δ  the number of queries that can be answered by the mechanism is known
in advance. In contrast  because our data-driven bounds on the personalised moments accountants

6

depend on the queries themselves  the cost of any given query is not known in advance and as such
the number of queries we can answer before using up our privacy allowance () is unknown.
Unfortunately  we cannot simply answer queries until the allowance is used up  because the number
of queries that we answer is a function of the data itself and thus we would need to introduce a
differentially private mechanism for determining when to stop (such as calculating  and δ after
each query using smooth-sensitivity as proposed in [8]). Instead  we follow [8] and leverage the fact
that we can answer more queries than standard subsample-and-aggregate to train a student model
using unlabelled public data. The ﬁnal output of our algorithm will then be a trained classiﬁer that
can be queried indeﬁnitely. To train this model  we take unlabelled public data P = {˜x1  ˜x2  ...}
and label it using ˜cλ until the privacy allowance has been used up. This will result in a (privately)
labelled dataset ˜P = {(˜x1  y1)  ...  (˜xp  yp)} where p is the number of queries answered. We train a
student model  S  on this dataset and the resultant classiﬁer can now be used to answer any future
queries. Because of our data-driven bound on the personalised moments accountant  we will typically
have that p > q where q is the number of queries that can be answered by a standard subsample-
and-aggregate procedure. The pseudo-code for learning a differentially private student classiﬁer
using our differentially private bagging model is given in Algorithm 1 (pseudo-code for training a
student model using standard subsample-and-aggregate is given in the Supplementary Materials for
comparison). Note that the majority of for loops (in particular the one on line 18) can be parallelized.

s=1

c∈C ys c log(T c

T

for i = 1  ...  n do

for j = 1  ...  k do

i Di j = D and Di1 j ∩ Di2 j = ∅ for all i1 (cid:54)= i2  j

i=1 j=1  θS  ˆ = 0  α(l; x) = 0 for l = 1  ...  L  x ∈ D ∪ {∗}

T }k n

i = 1  ...  n  j = 1  ...  k such that(cid:83)

i.i.d.∼ Di j

i j(xs))(cid:3) (multi-task cross-entropy loss)

size λ  maximum order of moments to be explored  L  unlabelled public data Dpub

−(cid:2)(cid:80)nmb

(cid:80)
Sample x1  ...  xnmb ∼ Dpub
for s = 1  ...  nmb do

Algorithm 1 Semi-supervised differentially private knowledge transfer using multiple partitions
1: Input:   δ  D  batch size nmb  number of partitions k  number of teachers per partition n  noise
2: Initialize: {θi j
3: Create n partitions of the dataset which are each made up of n disjoint subsets of the data Di j 
4: Set I(∗) = {(n  1)  ...  (n  k)}
5: while Teachers have not converged do
6:
7:
Sample (x1  y1)  ...  (xnmb   ynmb )
8:
Update teacher  Ti j  using SGD
9:
∇θi j
10:
11: while ˆ <  do
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
ˆ ← min
24:
l
25: Output: S

rs ← ˜cλ(xs)
Update the element-wise moments accountants
nc ← |{(i j):Ti j (xs)=c}|
for x ∈ D ∪ {∗} do

for c ∈ C
nc(x) ← |{(i j)∈I(x):Ti j (xs)=c}|
m(x) ← maxc{1 − nc(x)}
for l = 1  ...  L do

c∈C rs c log Sc(xs) (multi-task cross-entropy loss)
α(l;x)+log( 1
δ )

α(l; x) ← α(l; x) + 2λ2m(x)2l(l + 1)

∇θS −(cid:80)nmb
(cid:104)

(cid:80)

s=1
max

x

for c ∈ C  x ∈ D

k

l

k

(cid:105)

Update the student  S  using SGD

Theorem 6. The output of Algorithm 1 is (  δ)-differentially private with respect to D.

Proof. This follows from Theorems 2  3  4 and 5.

7

5 Experiments

In this section we compare our method (DPBag) against the standard subsample-and-aggregate
framework (SAA) to illustrate the improvements that can be achieved at a fundamental level by
using our model. Additionally  we compare against our method without the improved privacy bound
(DPBAG-) to quantify the improvements that are due to the bagging procedure and those that are due
to our improved privacy bound. We perform the experiments on two real-world datasets: Heart Failure
and UCI Adult (dataset description and results for UCI Adult can be found in the Supplementary
Materials). Implementation of DPBag can be found at https://bitbucket.org/mvdschaar/
mlforhealthlabpub/src/master/alg/dpbag/.
Heart Failure dataset: The Heart Failure dataset is a private dataset consisting of 24175 patients
who have suffered heart failure. We set the label of each patient as 3-year all-cause mortality 
excluding all patients who are censored before 3 years. The total number of features is 29 and the
number of patients is 24175. Among 24175 patients  10387 patients (43.0%) die within 3 years.
We randomly divide the data into 3 disjoint subsets: (1) a training set (33%)  (2) public data (33%) 
(3) a testing set (33%). In the main paper  we use logistic regression for the teacher and student
models in both algorithms; additional results for Gradient Boosting Method (GBM) can be found
in the Supplementary Materials. We set δ = 10−5. We vary  ∈ {1  3  5}  n ∈ {50  100  250}
and k ∈ {10  50  100}. In all cases we set λ = 2
n. To save space  we report DPBag results for
n ∈ {100  250}  k ∈ {50  100} and SAA results for n = 250 (the best performing) in the main
manuscript  with full tables reported in the Supplementary Materials. Results reported are the mean
of 10 runs of each experiment.

5.1 Results

In Table 1 we report the accuracy  AUROC and AUPRC of the 3 methods and we also report these for
a non-privately trained baseline model (NPB)  allowing us to quantify how much has been "lost due
to privacy". In Table 2  we report the total number of queries that could be made to each differentially
private classiﬁer before the privacy budget was used up.
In Table 1 we see that DPBag outperforms standard SAA for all values of  with Table 2 showing
that our method allows for a signiﬁcant increase in the number of public samples that can be labelled
(almost 100% more for  = 3).
The optimal number of teachers  n  varies with   for both DPBag and SAA. We see that for  = 1 
n = 250 performs best  but as we increase  the optimal number of teachers decreases. For small 
and small n  very few public samples can be labelled and so the student does not have enough data to
learn from. On the other hand  for large  and large n  the number of answered queries is much larger 
to the point where now the limiting factor is not the number of labels but is instead the quality of
the labels. Since we scale the noise to the number of teachers  the label quality improves with fewer
teachers because each teacher is trained on a larger portion of the training data. This is reﬂected by
both DPBag and SAA. In the SAA results  the performance does not saturate as quickly with respect
to  because the number of queries that  corresponds to for SAA is smaller than for DPBag.
As expected  we see that DPBAG- sits between SAA and DPBAG  enjoying performance gains due
to a stronger underlying model  and thus more accurately labelled training samples for the student 
but the improved privacy bound that DPBAG allows more samples to be labelled and thus further
gains are still made.
Table 2 also sheds light on the behavior of DPBag with repsect to k. We see in Table 1 that both
k = 50 and k = 100 can provide the best performance (depending on n and ). In Table 2  the
number of queries that can be answered increases with k. This implies that (as expected)  as we
increase k  the quantity m(u) gets closer to 0.5  and so each query costs less. However  when m(u)
is close to 0.5 for all samples  u  in the dataset then neither class will have a clear majority and thus
the labels are more susceptible to ﬂipping due to the noise added. k = 50 appears to balance this
trade-off when  is larger (and so we can already answer more queries) and when  is smaller we see
that answering more queries is more important than answering them well  so k = 100 is preferred.

8

Table 1: Prediction performance (Accuracy  AUROC  AUPRC) of DPBag and SAA with δ = 10−5
on the Heart Failure dataset using Logistic Regression. Bold indicates the best performance achieved
for the given metric and ﬁxed . DPBAG- is our method without the improved privacy analysis. NPB
is a non-private baseline model  included to indicate an upper bound on our performance.
AUPRC

Accuracy

AUROC

Model

n

k

100

250

100

250

250

1

50
100
50
100

50
100
50
100

-

DPBag

DPBag-

SAA

NPB

 = 1

 = 3

 = 5

 = 1

 = 3

 = 5

 = 1

 = 3

 = 5

.5639
.5667
.5888
.5986

.5614
.5596
.5855
.5875

.6085
.6050
.6061
.6077

.6019
.6007
.6051
.6061

.6154
.6142
.6099
.6091

.6128
.6108
.6086
.6110

.5547
.5626
.5954
.6096

.5544
.5609
.5896
.5884

.6326
.6295
.6320
.6373

.6288
.6174
.6295
.6321

.6453
.6448
.6391
.6398

.6429
.6354
.6366
.6407

.4793
.4895
.5161
.5289

.4792
.4767
.5093
.5103

.5530
.5496
.5526
.5542

.5411
.5338
.5498
.5518

.5656
.5652
.5607
.5644

.5612
.5525
.5565
.5615

.5798

.6019

.6024

.5778

.6284

.6356

.5023

.5496

.5559

.6527

.6992

.6281

Table 2: Number of labels provided by each method before the privacy budget    is used up on
the Heart Failure dataset. Note that DPBAG- and SAA have the same  non-data dependent privacy
analysis and so provide the same number of labels as each other.
 = 3
593
609
3785
4044
2108

 = 5
1487
1538
6380
6805
5269

k
50
100
50
100
-

74
76
468
507
264

Models

DPBag

SAA

n

100

250

250

 = 1

6 Discussion

In this work  we introduced a new methodology for developing a differentially private classiﬁer.
Building on the ideas of subsample-and-aggregate  we divide the dataset several times  allowing us to
derive (tighter) data-dependent bounds on the privacy cost of a query to our mechanism. To do so 
we deﬁned the personalised moments accountants  which we use to accumulate the privacy loss of a
query with respect to each sample in the dataset (and any potentially added sample) individually.
A key advantage of our model  like subsample-and-aggregate is that it is model agnostic  and can
be applied using any base learner  with the differential privacy guarantees holding regardless of the
learner used.
We believe this work opens up several interesting avenues for future research: (i) the privacy
guarantees could potentially be improved on by making assumptions about the base learners used  (ii)
the personalised moments accountants naturally allow for the development of an algorithm that affords
each sample a different level of differential privacy  i.e. personalised differential privacy [7]  (iii) we
believe bounds such as those derived in [8] and [9] that rely on the subsample-and-aggregate method
will have natural analogs with respect to our bagging procedure corresponding to tighter bounds on
the personalised moments accountants than can be shown for the global moments accountant using
simple subsample-and-aggregate (see the discussion in the Supplementary Materials).

Acknowledgments

This work was supported by the National Science Foundation (NSF grants 1462245 and 1533983) 
and the US Ofﬁce of Naval Research (ONR).

9

References
[1] Cynthia Dwork  Aaron Roth  et al. The algorithmic foundations of differential privacy. Foundations and

Trends R(cid:13) in Theoretical Computer Science  9(3–4):211–407  2014.

[2] Kobbi Nissim  Sofya Raskhodnikova  and Adam Smith. Smooth sensitivity and sampling in private data
analysis. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing  pages 75–84.
ACM  2007.

[3] Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In STOC  volume 9  pages 371–380 

2009.

[4] Abhradeep Guha Thakurta and Adam Smith. Differentially private feature selection via stability arguments 

and the robustness of the lasso. In Conference on Learning Theory  pages 819–850  2013.

[5] Leo Breiman. Bagging predictors. Machine learning  24(2):123–140  1996.

[6] Martin Abadi  Andy Chu  Ian Goodfellow  H Brendan McMahan  Ilya Mironov  Kunal Talwar  and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference
on Computer and Communications Security  pages 308–318. ACM  2016.

[7] Zach Jorgensen  Ting Yu  and Graham Cormode. Conservative or liberal? personalized differential privacy.

In 2015 IEEE 31St international conference on data engineering  pages 1023–1034. IEEE  2015.

[8] Nicolas Papernot  Martín Abadi  Ulfar Erlingsson  Ian Goodfellow  and Kunal Talwar. Semi-supervised
knowledge transfer for deep learning from private training data. arXiv preprint arXiv:1610.05755  2016.

[9] Nicolas Papernot  Shuang Song  Ilya Mironov  Ananth Raghunathan  Kunal Talwar  and Úlfar Erlingsson.

Scalable private learning with pate. arXiv preprint arXiv:1802.08908  2018.

[10] Raef Bassily  Om Thakkar  and Abhradeep Thakurta. Model-agnostic private learning via stability. arXiv

preprint arXiv:1803.05101  2018.

[11] Xiaoqian Liu  Qianmu Li  Tao Li  and Dong Chen. Differentially private classiﬁcation with decision tree

ensemble. Applied Soft Computing  62:807–816  2018.

[12] Mark Bun and Thomas Steinke. Concentrated differential privacy: Simpliﬁcations  extensions  and lower

bounds. In Theory of Cryptography Conference  pages 635–658. Springer  2016.

10

,James Jordon
Jinsung Yoon
Mihaela van der Schaar