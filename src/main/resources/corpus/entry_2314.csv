2019,Global Convergence of Least Squares EM for Demixing Two Log-Concave Densities,This work studies the location estimation problem for a mixture of two rotation invariant log-concave densities. We demonstrate that Least Squares EM  a variant of the EM algorithm  converges to the true location parameter from a randomly initialized point. Moreover  we establish the explicit convergence rates and sample complexity bounds  revealing their dependence on the signal-to-noise ratio and the tail property of the log-concave distributions. Our analysis generalizes previous techniques for proving the convergence results of Gaussian mixtures  and highlights that an angle-decreasing property is sufficient for establishing global convergence for Least Squares EM.,Global Convergence of Least Squares EM
for Demixing Two Log-Concave Densities

Wei Qian  Yuqian Zhang  Yudong Chen

School of Operations Research and Information Engineering

Cornell University

{wq34 yz2557 yudong.chen}@cornell.edu

Abstract

This work studies the location estimation problem for a mixture of two rotation
invariant log-concave densities. We demonstrate that Least Squares EM  a variant
of the EM algorithm  converges to the true location parameter from a randomly
initialized point. Moreover  we establish the explicit convergence rates and sample
complexity bounds  revealing their dependence on the signal-to-noise ratio and the
tail property of the log-concave distributions. Our analysis generalizes previous
techniques for proving the convergence results of Gaussian mixtures  and highlights
that an angle-decreasing property is sufﬁcient for establishing global convergence
for Least Squares EM.

1

Introduction

One important problem in statistics and machine learning [18  24] is learning a ﬁnite mixture of
distributions. In the parametric setting in which the functional form of the density is known  this
problem is to estimate parameters (e.g.  mean and covariance) that specify the distribution of each
mixture component. The parameter estimation problem for mixture models is inherently non-convex 
posing challenges for both computation and analysis. While many algorithms have been proposed 
rigorous performance guarantees are often elusive. One exception is the Gaussian Mixture Model
(GMM) for which much theoretical progress has been made in recent years. The goal of this paper is
to study algorithmic guarantees for a much broader class of mixture models  namely the mixture of
log-concave distributions. This class includes may common distributions1 and is interesting from
both modelling and theoretical perspectives [2  3  6  12  26  23].
We focus on the Expectation Maximization (EM) algorithm [11]  which is one of the most popular
methods for estimating mixture models. Understanding the convergence property of EM is highly non-
trivial due to the non-convexity of the negative log-likelihood function. The work in [4] developed
a general framework for establishing local convergence to the true parameter. Proving global
convergence of EM is more challenging  even in the simplest setting with a mixture of two Gaussians
(2GMM). The recent work in [10  28] considered balanced 2GMM with known covariance matrix and
showed for the ﬁrst time that EM converges to the true location parameter using random initialization.
Subsequent work established global convergence results for a mixture of two truncated Gaussians [19] 
two linear regressions (2MLR) [17  16]  and two one-dimensional Laplace distributions [5].
All the above results (with the exception of [5]) rely on the explicit density form and the speciﬁc
properties of the Gaussian distribution.
In particular  under the the Gaussian assumption  the
M-step in the EM algorithm has a closed-form expression  which allows a direct analysis of the
convergence behavior of the algorithm. However  for a general log-concave distribution the M-step
no longer admits a closed-form solution  and this poses signiﬁcant challenges for analysis. To address

1Familiar examples of log-concave distributions include Gaussian  Laplace  Gamma  and Logistics [3].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

this difﬁculty  we consider a modiﬁcation of the standard EM algorithm  called Least Squares EM
(LS-EM)  to learn the location parameter of a mixture of two log-concave distributions. This algorithm
admits an explicit update  which is computationally simple.
As the main result of this paper  we show that for a mixture of rotation invariant log-concave
distribution  LS-EM converges to the true location parameter with a random initialization. Moreover 
we provide explicit convergence rates and sample complexity bounds  which depend on the signal-to-
noise ratio as well as the tail property of the distribution. As the functional form of the true density
may be unknown  we further establish a robustness property of LS-EM when using a mis-speciﬁed
density. As a special case  we show that using a Gaussian density  LS-EM globally converges to a
solution close to the true parameter whenever the variance of the true log-concave density is moderate.

Technical Contributions We generalize the sensitivity analysis in [10] to a broad class of log-
concave distributions. In the process  we demonstrate that log-concavity and rotation invariance of the
distribution are the only properties required to guarantee the global convergence of LS-EM. Moreover 
our analysis highlights the fundamental role of an angle-decreasing property in establishing the
convergence of LS-EM to the true location parameter in the high dimension settings. Note the `2
distance contraction  upon which the previous convergence results were built  no longer holds for
general log-concave distributions.

Organization In Section 2  we describe the parameter estimation problem for a mixture of log-
concave distributions and review related work. In Section 3  we derive the LS-EM algorithm and
elucidate its connection with classical EM. Analysis of the global convergence of LS-EM is provided
in Section 4 for the population setting  in Section 5 for the ﬁnite-sample setting  and in Section 6 for
the model mis-speciﬁcation setting. The paper concludes with a discussion of future directions in
Section 7. Some details of the proofs and numerical experiments are deferred to the Appendix.
Notations We use x 2 R and x 2 Rd to denote scalars and vectors  respectively; X 2 R and
X 2 Rd to denote scalar and vector random variables  respectively. The i-th coordinate of x (or X)
is xi (or Xi)  and the j-th data point is denoted by xj or X j. The Euclidean norm in Rd is k·k 2.
For two vectors ↵   2 Rd  we use \(↵  ) 2 (0 ⇡ ) to denote the angle between them and h↵  i
to denote their inner product. Finally  I d is the d-by-d identity matrix.

2 Problem Setup

In this section  we set up the model for a mixture of log-concave distributions and discuss the
corresponding location estimation problem.

2.1 Data Generating Model
Let F be a class of rotation invariant log-concave densities in Rd deﬁned as follows:

F =⇢f :f (x) =

Z f (x) dx = 1 Z x2

1
Cg

exp  g(kxk2)  g is convex and strictly increasing on [0 1) 

(1)

i f (x) dx = 1 8i 2 [d] 
d f x

where we may assume g(0) = 0 without loss of generality.2 It can be veriﬁed that each f 2F
has mean 0 and covariance matrix I d. For each f 2F   we may generate a location-scale family
   which has mean  and covariance matrix 2I d.
consisting of the densities f (x) := 1
We assume that each data point is generated from D(⇤  )  a balanced mixture of two symmetric
members of the above log-concave location-scale family:

D(⇤  ) :=

1
2

f⇤  +

1
2

f⇤ .

(2)

2Note that x 7! g(kxk2) is a convex function  as it is the composition of a convex function and a convex
increasing function. The normalization constant Cg can be computed explicitly by Cg = Chdvd with Ch =
R 1
td1 exp(g(t)) dt  where vd := ⇡d/2

(d/2+1) is the volume of a unit ball in Rd.

0

2

Throughout this paper  we denote the signal-to-noise ratio (SNR) by

⌘ := k⇤k2/.

It is sometimes useful to view the above model as an equivalent latent variable model: for each i 2 [n]
an unobserved label Zi 2{ 1  2} is ﬁrst generated according to P(Z = 1) = P(Z = 2) = 1/2  and
then the data point X i is sampled from the corresponding mixture component  i.e.  from f⇤  if
Zi = 1 and from f⇤  otherwise.
Examples: Below are several familiar examples of one-dimensional log-concave distributions
f / exp(g) from F:

Gaussian distribution. When r = 1  it corresponds to the Laplace distribution.

1. Polynomial distributions: g(x) /| x|r with r  1. When r = 2  it corresponds to the
2. Logistic distribution: g(x) / log(e|x|/2 + e|x|/2).

These distributions can be generalized to higher dimension by replacing |x| with kxk2. In Appendix B 
we provide a review of some elementary properties of log-concave distributions.

2.2 Location Estimation and the EM Algorithm
We assume that  is known  and our goal is to estimate the location parameter ⇤ given data
X 1  X 2  . . .   X n 2 Rd sampled i.i.d. from the mixture distribution D(⇤  ) in (2). We ﬁrst
consider this problem for a given log-concave family for which the base density f (equivalently  g) is
known. The case with an unknown f is discussed in Section 6.
Since the negative log-likelihood function of the mixture (2) is non-convex  computing the standard
MLE for ⇤ involves a non-convex optimization problem. EM is a popular iterative method for
computing the MLE  consisting of an expectation (E) step and a maximization (M) step. In a standard
implementation of EM  the E-step computes the conditional distribution of the labels Zi under
the current estimate of ⇤  and the M-step computes a new estimate by maximizing the expected
log-likelihood based on the E-step. The LS-EM algorithm we consider  described in Section 3 to
follow  is a variant of the standard EM algorithm with a modiﬁed M-step.

2.3 Convergence of EM and Related Work
Despite the popularity and empirical success of the EM algorithm  our understanding of its theoretical
property is far from complete. Due to the non-convexity of negative log-likelihood functions  EM is
only guaranteed to converge to a stationary point [27]. Quantitative convergence results only began
to emerge in recent years. The work [4] proposed a general framework for establishing the local
convergence of EM when initialized near the true parameter  with applications to 2GMM  2MLR 
and regression with missing coefﬁcients. Extensions to multiple components are considered in [29].
Beyond local convergence  it is known that the likelihood function of GMM may have bad local
optima when there are more than two components  and EM fails to ﬁnd the true parameter without a
careful initialization [14]. Analysis of the global convergence of EM has hence been focused on the
two component setting  as is done in this paper. The work in [10] showed that EM converges from a
random initialization for 2GMM. Subsequent work in [17  16  28  19] established similar results in
other settings  most of which involve Gaussian models. An exception is [5]  which proved the global
convergence of EM for a mixture of 2 Laplace distributions and derived an explicit convergence rate 
but only in the one-dimensional population (inﬁnite sample) setting. In general  properties of EM for
mixtures of other distributions are much less understood  which is the problem we target at in this
paper.
The log-concave family we consider is a natural and ﬂexible generalization of Gaussian. This family
includes many common distributions  and has broad applications in economics [2  3]  reliability
theory [6] and sampling analysis [12]; see [26  23] for a further review. Existing work on estimating
log-concave distributions and mixtures has mostly considered the non-parametric setting [26  9 
15  21  9]; these methods are ﬂexible but typically more computational and data intensive than
the parameteric approach we consider. Other approaches of learning general mixtures include
spectral [1  22] and tensor methods [13  8]; EM is often applied to the output of these methods.

3

3 The Least Squares EM Algorithm (LS-EM)

The M-step in the standard EM involves maximizing the conditional log-likelihood. For GMM  the
M-step is equivalent to solving a least-squares problem. For a mixture of log-concave distributions 
the M-step is to solve a convex optimization problem  and this optimization problem does not admit a
closed form solution in general. This introduces complexity in both computation and analysis.
We instead consider Least Squares EM  a variant of EM that solves a least-squares problem in the
M-step even for non-Gaussian mixtures. To elucidate the algorithmic property  we consider LS-EM
in the population setting  where we have access to an inﬁnite number of data points from the mixture
distribution D(⇤  ). The ﬁnite sample version is discussed in Section 5.
Each iteration of the population LS-EM algorithm consists of the following two steps:
• E-step: Compute the conditional probabilities of the label Z 2{ 1  2} given the current location

estimate :

p1
 (X) :=

f (X)

f (X) + f (X)

 

p2
 (X) :=

f (X)

f (X) + f (X)

.

(3)

• Least-squares M-step: Update the location estimate  via weighted least squares:

+ = argmin

b

EX⇠D(⇤ )⇥p1
=EX⇠D(⇤ )X tanh 1

2

2 + p2

 (X)kX  bk2
g✓ 1
kX + k2◆ 

2⇤
 (X)kX + bk2
kX  k2◆! := M (⇤  ).
g✓ 1
1
2

(4)

In (4)  we minimize the sum of squared distances of X to each component’s location  weighted
by the conditional probability of X belonging to that component. One may interpret LS-EM as a
soft version of the K-means algorithm: instead of associating each X exclusively with one of the
components  we assign a probability  which is computed using the log-concave density.

3.1 Connection to Standard EM
In contrast to LS-EM  the M-step in the standard EM algorithm involves maximizing the weighted
log-likelihood function (or minimizing the weighted negative log-likelihood function):

Standard M-step:

argmax

b

Q(b | ) := EX⇠D(⇤ )⇥p1

 (X) log fb (X) + p2

 (X) log fb (X)⇤ .

(5)

The standard EM iteration  consisting of (3) and (5)  corresponds to a minorization-maximization
procedure for ﬁnding the MLE under the statistical setting (2). In particular  the function Q(· |
) above is a lower bound of the (marginal) log-likelihood function of (2)  and the standard M-
step (5) ﬁnds the maximizer of this lower bound. In general  this maximization can only be solved
approximately. For example  the “gradient EM” algorithm considered in [4] performs one gradient
ascent step on the Q(· | ) function.
The least-squares M-step (4) may also be viewed as an approximation to the standard M-step (5)  as
we observe numerically (see Appendix H.3) that the LS-EM update + satisﬁes

Q(+ | ) > Q( | )

if  6= ⇤.

(6)

This implies that the least-squares M-step ﬁnds an improved solution + (compared to the previous
iterate ) of the function Q(· | ).
4 Analysis of Least Squares EM

In this section  we analyze the convergence behavior of the LS-EM update (4) in the population
setting. We ﬁrst consider the one dimensional case (d = 1) in Section 4.1 and establish the global
convergence of LS-EM  extending the techniques in [10] for 2GMM to log-concave mixtures. In

4

Section 4.2  we prove global convergence in the multi-dimensional case (d > 1). In this setting 
the LS-EM update is not contractive in `2  so the analysis requires the new ingredient of an angle
decreasing property.

kX  k2;
For convenience  we introduce the shorthand F (X) := g 1
when  = 1  we simply write F ⌘ F 1. Since the integrand in (4) is an even function of X  the
update (4) can be simpliﬁed to an equivalent form by integrating over one component of the mixture:
(7)

kX + k2  g 1

+ = M (⇤  ) = EX⇠f⇤  X tanh0.5F (X).

Throughout the section  we refer to the technical conditions permitting the interchange of differentia-
tion and integration as the regularity condition. This condition is usually satisﬁed by log-concave
distributions — a detailed discussion is provided in Appendix E.

where the contraction factor

4.1 One Dimensional Case (d = 1)
For one dimensional log-concave mixtures  the behavior of LS-EM is similar to that of EM algorithm
for 2GMM: there exist only 3 ﬁxed points  0  ⇤ and ⇤  among which 0 is non-attractive. Conse-
quently  LS-EM converges to the true parameter (⇤ or ⇤) from any non-zero initial solution 0.
This is established in the following theorem.
Theorem 4.1 (Global Convergence  1D). Suppose that f 2F satisﬁes the regularity condition. The
LS-EM update (4)   7! M (⇤  )  has exactly three ﬁxed points: 0  ⇤ and ⇤. Moreover  the
following one-step bound holds:

|M (⇤  )  sign(⇤)⇤| (⇤   ) ·  sign(⇤)⇤ 
(⇤   ) := EX⇠fmin(|| |⇤|) ⇥1  tanh0.5Fmin(|| |⇤|) (X)⇤

satisﬁes 0 < (⇤   ) < 1 when  62 {0  ⇤ ⇤}.
We prove this theorem in Appendix C.1. The crucial property used in the proof is the self-consistency
of the LS-EM update (4)  namely M (   ) =  for all . This property allows us to extend the
sensitivity analysis technique for 2GMM to general log-concave distributions.
It can be further shown that the contraction factor (⇤   ) becomes smaller as the iterate moves
closer to the true ⇤ (see Lemma C.2). We thus obtain the following corollary on global convergence
at a geometric rate. Without loss of generality  assume ⇤ > 0.
Corollary 4.2 (t-step Convergence Rate  1D). Suppose that f 2F satisﬁes the regularity condition.
Let t denote the output of LS-EM after t iterations  starting from 0 6= 0. The following holds:
/log (⇤  0  )⌘ iterations
If 0 is in (0  0.5⇤) or (1.5⇤ 1)  running LS-EM for O⇣log 0.5⇤

|t  sign(0⇤)⇤| (⇤  0  )t ·0  sign(0⇤)⇤.

outputs a solution in (0.5⇤  1.5⇤). In addition  if 0 is in (0.5⇤  1.5⇤)  running LS-EM for
O (Cf (⌘) log(1/✏)) iterations outputs an ✏-close estimate of ⇤  where Cf (⌘) > 0 is a constant
depending only on f and the SNR ⌘ := ⇤/.

|0⇤|

Special cases We provide explicit convergence rates for mixtures of some common log-concave
distributions. In the following  we assume ⇤ > 0 and   0 without loss of generality  and set
z := min(   ⇤).
• Gaussian: (⇤   )  exp  z2/22 and Cf (⌘) = max1  1
⌘2.
and Cf (⌘) = max1  1
⌘.
• Laplace: (⇤   )  2 exp(
1+exp(2
• Logistic: (⇤   ) 

4 exp( ⇡z
p3
1+exp( 2⇡z
p3

)+2 exp( ⇡z
p3

p2
 z)
p2
 z)

)

) and Cf (⌘) = max1  1
⌘.

See Appendix C.2 for the proofs of the above results. Note that the convergence rate depends on
the signal-to-noise ratio ⌘ as well as the asymptotic growth rate  ⌘ f of the log-density function
g =  log f. In the above examples  (⇤   ) ⇡ exp (c(min(⇤  )/))  where  = 1 for
Laplace and Logistic  and  = 2 for Gaussian.

5

4.2 High Dimensional Case (d > 1)

Extension to higher dimensions is more challenging for log-concave mixtures than for Gaussian
mixtures. Unlike Gaussian  a log-concave distribution with diagonal covariance need not have
independent coordinates. A more severe challenge arises due to the fact that LS-EM is not contractive
in `2 distance for general log-concave mixtures. This phenomenon  proved in the lemma below 
stands in sharp contrast to the Gaussian mixture setting.
Lemma 4.3 (Non-contraction in `2). Consider a log-concave density of the form g(x) / kxkr
2 with
r  1. When r 2 [1  2]  0 is the only ﬁxed point of LS-EM in the direction ortoghonal to ⇤. When
r 2 (2 1)  there exists a ﬁxed point other than 0 in the orthogonal direction. Consequently  when
r > 2  there exists  such that kM (⇤  )  ⇤k2 > k  ⇤k2.
We prove Lemma 4.3 in Appendix D.3. The lemma shows that it is fundamentally impossible to
prove global convergence of LS-EM solely based on `2 distance  which was the approach taken
in [10] for Gaussian mixtures.
Despite the above challenges  we show afﬁrmatively that LS-EM converges globally to ±⇤ for
mixtures of rotation-invariant log-concave distributions  as long as the initial iterate is not orthogonal
to ⇤ (a measure zero set).
As the ﬁrst step  we use rotation invariance to show that the LS-EM iterates stay in a two-dimensional
space. The is done in the following lemma  which is proved in Appendix D.1.
Lemma 4.4 (LS-EM is 2-Dimensional). The LS-EM update satisﬁes: M (⇤  ) 2 span(  ⇤).
Moreover  if \(  ⇤) = 0 or \(  ⇤) = ⇡/2  then M (⇤  ) 2 span().
We now establish the asymptotic global convergence property of LS-EM.
Theorem 4.5 (Global Convergence  d-Dimensional). Suppose that f 2F satisﬁes the regularity
condition. The LS-EM algorithm converges to sign(h0  ⇤i)⇤ from any randomly initialized point
0 that is not orthogonal to ⇤.

The theorem is proved using a novel sensitivity analysis that shows decrease in angle rather than in `2
distance. The proof does not depend on the explicit form of the density  but only log-concavity and
rotation invariance. We sketch the main ideas of proof below  deferring the details to Appendix D.2.

Proof Sketch. Let 0 be the initial point that is not orthogonal to ⇤. Without loss of generality  we
assume h0  ⇤i > 0. Consequently  all the future iterates satisfy ht  ⇤i > 0 (see Lemma D.4).
If 0 is in the span of ⇤ (i.e.  0 parallels ⇤)  Lemma D.3 ensures that the iterates remain in the
direction of ⇤ and converge to ⇤. On the other hand  if 0 is not in the span of ⇤  we make use of
the following two key properties of the LS-EM update + = M (⇤  ):
1. Angle Decreasing Property (Lemma D.2): Whenever \  ⇤ 2 (0  ⇡
decreases the iterate’s angle toward ⇤  i.e.  \(+  ⇤) < \(  ⇤) ;
2. Local Contraction Region (Corollary D.7): there is a local region around ⇤ such that if any
iterate falls in that region  all the future iterates remain in that region.

2 )  the LS-EM update strictly

Since the sequence of LS-EM iterates is bounded  it must have accumulation points. Using the angle
decreasing property and the continuity of M (⇤  ) in the second variable   we show that all the
accumulation points must be in the direction of ⇤. In view of the dynamics of the 1-dimensional
case (Theorem 4.1)  we can further show that the set of accumulation points must fall into one of
the following three possibilities: {0}  {⇤}  or {0  ⇤}. Below we argue that {0} and {0  ⇤} are
impossible by contradiction.
• If {0} is the set of accumulation points  the sequence of non-zero iteratest would converge
to 0 and stay in a neighborhood of 0 after some time T ; in this case  Lemma D.8 states that the
norm of the iterates is bounded away from zero in the limit and hence they cannot converge to 0.
• If {0  ⇤} is the set of accumulation points  then there is at least one iterate in the local region
of ⇤; by the local contraction region property above  all the future iterates remain close to ⇤.
Therefore  0 cannot be another accumulation point.

We conclude that ⇤ is the only accumlation point  which LS-EM must converge to.

6

5 Finite Sample Analysis

In this section  we consider the ﬁnite sample setting in which we are given n data points X i sampled
i.i.d. from D(⇤  ). Using the equivalent expression (7) for the population LS-EM update  and
replacing the expectation with the sample average  we obtain the ﬁnite-sample LS-EM update:3

+

=

1
n

nXi=1

X i tanh(0.5F (X i)) 

where X i i.i.d.⇠ f⇤ .

(8)

e

One approach to extend the population results (in Section 4) to this case is by coupling the pop-
+. To this end  we make use of the fact that
log-concave distributions are automatically sub-exponential (see Lemma F.2)  so the random vari-
i=1 are i.i.d. sub-exponential for each coordinate j. Therefore  the

j tanh(0.5F (X i)) n

ulation update + with the ﬁnite-sample update e
ablesX i
 +k2 = eOp(k⇤k2
concentration bound ke
error of eOpd/n.

+

gence properties of the population LS-EM carry over to the ﬁnite-sample case  modulo a statistical

2 + 2)d/n holds  and we expect that the conver-

(9)

The above argument is made precise in following proposition for the one-dimensional case  which is
proved in Appendix F.1.
Proposition 5.1 (1-d Finite Sample). Suppose the density function f 2F satisﬁes the regularity
condition. With  2 R being the current estimate  the ﬁnite-sample LS-EM update (8) satisﬁes the
following bound with probability at least 1  :

|e+  ⇤| (⇤   ) · |  ⇤| + (⇤ + Cf ) · O r 1

n

1

!  

log

(1+Cf /⌘)2

where (⇤   ) is contraction factor deﬁned in Theorem 4.1 and Cf is the Orlicz 1 norm (i.e. 
the sub-exponential parameter) of a random variable with density f 2F .
Using Proposition 5.1  we further deduce the global convergence of LS-EM in the ﬁnite sample
case  which parallels the population result in Corollary 4.2. We present this result assuming sample
splitting  i.e.  each iteration uses a fresh  independent set of samples. This assumption is standard
in ﬁnite-sample analysis of EM [4  29  10  28  17  16]. In this setting  we establish the following
quantitative convergence guarantee for LS-EM initialized at any non-zero 0. Without loss of
generality  0  ⇤ > 0.
The convergence has two stages. In the ﬁrst stage  the LS-EM iterates enter a local neighborhood
around ⇤  regardless of whether 0 is close to or far from 0. This is the content of the result below.
Proposition 5.2 (First Stage: Escape from 0 and 1). Suppose the initial point 0 is in (0  0.5⇤) [
(1.5⇤ 1). After T = O⇣log 0.25⇤
/log (⇤  min(0  0.5⇤)  )⌘ iterations  with N/T =
⌘ fresh samples per iteration  LS-EM outputs a solution eT 2
⌦⇣
(1(⇤ min(0 0.5⇤) ))2 log 1
/log (⇤  min(0  0.5⇤)  )⌘.
(0.5⇤  1.5⇤) with probability at least 1   · O⇣log 0.25⇤

Within this local neighborhood  the LS-EM iterates converge to ⇤ geometrically  up to a statistical
error determined by the sample size. This second stage convergence result is given below.
Proposition 5.3 (Second Stage: Local Convergence). The following holds for any ✏> 0. Sup-
pose 0 2 (0.5⇤  1.5⇤). After T = O (log ✏/log (⇤  0.5⇤  )) iterations  with N/T =
 ) fresh samples per iteration  LS-EM outputs a solution eT satisfying
✏2(1(⇤ 0.5⇤ ))2 log 1
⌦(
|eT  ⇤| ✏⇤ with probability at least 1   · O (log ✏/log (⇤  0.5⇤  )).
We prove Propositions 5.2 and 5.3 in Appendix F.2.
Let us parse the above results in the special cases of Gaussian  Laplace and Logistic  assuming
that  = 1 for simplicity. In Section 4.1 we showed that (⇤   ) = exp ( min(   ⇤)) 
3This expression is for analytic purpose only. To actually implement LS-EM  we use samples X i from the

(⇤+Cf /⌘)2

|0⇤|

|0⇤|

mixture distribution D(⇤  )  which is equivalent to (8).

7

where  ⌘ f is the growth rate of the log density  log f. Consequently  the ﬁrst stage requires

O1/(min(0  ⇤)) iterations withe⌦1/(min(0  ⇤))2 samples per iteration  and the second
stage requires O (log(1/✏)/⌘) iterations withe⌦1/✏2⌘2 samples per iteration  where ⌘ := ⇤/

is the SNR. As can be seen  we have better iteration and sample complexities with a larger ⌘  1
(larger separation between the components) and a larger  (lighter tail of the components).
In contrast  in the low SNR regime with ⌘< 1  the sample complexity actually becomes worse for a
larger  (lighter tails). Indeed  low SNR means that two components are close in location when  = 1.
If their tails are lighter  then it becomes more likely that the mixture density (f⇤  + f⇤ )/2 has
a unique mode at 0 instead of two modes at ±⇤. In this case  the mixture problem becomes harder
as it is more difﬁcult to distinguish between the two components.
In the higher dimensional setting  we can similarly show coupling in `2 (i.e.  bounding ke

 +k2)
via sub-exponential concentration. However  extending the convergence results above to d > 1 is
more subtle  due to the issue of `2 non-contraction (see Lemma 4.3). Addressing this issue would
require coupling in a different metric (e.g.  in angle—see [17  28]); we leave this to future work.

+

6 Robustness Under Model Mis-speciﬁcation

+

(10)

In practice  it is sometimes difﬁcult to know a priori the exact parametric form of a log-concave
distribution that generates the data. This motivates us to consider the following scenario: the data
is from the mixture D(⇤  ) in (2) with a true log-concave distribution f 2F and unknown
location parameter ⇤  but we run LS-EM assuming some other log-concave distribution bf (·) =
C1
exp(bg(k·k 2)) 2F . Using the same symmetry argument as in deriving (7)  we obtain the
bg
following expression for the mis-speciﬁed LS-EM update in the population case:
= cM (⇤  ) := EX⇠f⇤  X tanh0.5bF (X) 
b
where bF (X) :=bg 1
kX + k2 bg 1
\(b
We provide such a result focusing on the setting in which bf is Gaussian  that is  we ﬁt a Gaussian
mixture to a true mixture of log concave distributions. In this setting  we can show that mis-speciﬁed
LS-EM has only 3 ﬁxed points {±  0} (Lemma G.1). Moreover  we can bound the distance between
 and the true ⇤  thereby establishing the following convergence result:
Proposition 6.1 (Fit with 2GMM). Under the above one dimensional setting with Gaussian bf  the
following holds for some absolute constant C0 > 0: If ⌘  C0  then the LS-EM algorithm with a
non-zero initialization point 0 converges to a solution  satisfying sign() = sign(0) and

Many properties of the LS-EM update are preserved in this mis-speciﬁcation setting. In particular 
using the same approach as in Lemma4.4 and Lemma D.2  we can show that the mis-speciﬁed
LS-EM update is also a two dimensional object and satisﬁes the same strict angle decreasing property
  ⇤) < \(  ⇤). Therefore  to study the convergence behavior of mis-speciﬁed LS-EM  it

sufﬁces to understand the one-dimensional case (i.e.  along the ⇤ direction).

kX  k2.

+

  sign(0⇤)⇤  10.

We prove this proposition in Appendix G.1. The proposition establishes a robust property of LS-EM:
even in the mis-speciﬁed setting  LS-EM still converges globally. Moreover  when the SNR ⌘ is high
(i.e.  small noise level )  the ﬁnal estimation error is small and scales linearly with .

7 Discussion

In this paper  we have established the global convergence of the Least Squares EM algorithm for a
mixture of two log-concave densities. Our theoretical results are proved under the following three
assumptions on the densities: (i) rotation invariance  (ii) log-concavity  and (iii) monotone increasing
of the log density g with respect to the norm; cf. Equation (1). As we discuss in greater details in
Appendix H  all these assumptions appear to be essential under our current framework of analysis.

8

Moreover  empirical results suggest that the log-convexity assumption cannot be relaxed completely:
Figure 1 provides an example where the LS-EM algorithm may converge to 0 (an undesired solution)
with constant probability when the log-concavity property is violated. See Appendix H for additional
numerical results.

Figure 1: The base distribution class f is proportional to exp(|x|0.25). This is log-convex instead
of log-concave. The ground truth location parameter is set to be 1. We initialize the LS-EM iterates
at  = 0.1  0.2  0.3  0.4  0.5  and it is seen that all converge to 0 after some iterations.

For future work  an immediate direction is to establish quantitative global convergence guarantees in
high dimensions for both the population and ﬁnite sample cases; doing so would require generalizing
the angle convergence property in [17] to log-concave distributions. In light of the discussion above 
it is also of interest to investigate to what extent the rotation invariance assumption can be relaxed  as
many interesting log-concave distributions are skewed. It is also interesting to consider mixtures with
multiple components.

Acknowledgement
W. Qian and Y. Chen are partially supported by NSF grants CCF-1657420 and CCF-1704828.

9

,James Newling
François Fleuret
Wei Qian
Yuqian Zhang
Yudong Chen