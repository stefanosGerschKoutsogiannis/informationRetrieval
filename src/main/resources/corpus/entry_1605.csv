2014,Do Convnets Learn Correspondence?,Convolutional neural nets (convnets) trained from massive labeled datasets have substantially improved the state-of-the-art in image classification and object detection. However  visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels  it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper  we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes  that they can be used to perform intraclass aligment as well as conventional hand-engineered features  and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011.,Do Convnets Learn Correspondence?

Trevor Darrell
Jonathan Long
{jonlong  nzhang  trevor}@cs.berkeley.edu

University of California – Berkeley

Ning Zhang

Abstract

Convolutional neural nets (convnets) trained from massive labeled datasets [1]
have substantially improved the state-of-the-art in image classiﬁcation [2] and ob-
ject detection [3]. However  visual understanding requires establishing correspon-
dence on a ﬁner level than object category. Given their large pooling regions and
training from whole-image labels  it is not clear that convnets derive their success
from an accurate correspondence model which could be used for precise localiza-
tion. In this paper  we study the effectiveness of convnet activation features for
tasks requiring correspondence. We present evidence that convnet features local-
ize at a much ﬁner scale than their receptive ﬁeld sizes  that they can be used to
perform intraclass aligment as well as conventional hand-engineered features  and
that they outperform conventional features in keypoint prediction on objects from
PASCAL VOC 2011 [4].

1

Introduction

Recent advances in convolutional neural nets [2] dramatically improved the state-of-the-art in image
classiﬁcation. Despite the magnitude of these results  many doubted [5] that the resulting features
had the spatial speciﬁcity necessary for localization; after all  whole image classiﬁcation can rely
on context cues and overly large pooling regions to get the job done. For coarse localization  such
doubts were alleviated by record breaking results extending the same features to detection on PAS-
CAL [3].
Now  the same questions loom on a ﬁner scale. Are the modern convnets that excel at classiﬁcation
and detection also able to ﬁnd precise correspondences between object parts? Or do large receptive
ﬁelds mean that correspondence is effectively pooled away  making this a task better suited for
hand-engineered features?
In this paper  we provide evidence that convnet features perform at least as well as conventional
ones  even in the regime of point-to-point correspondence  and we show considerable performance
improvement in certain settings  including category-level keypoint prediction.

1.1 Related work

Image alignment
Image alignment is a key step in many computer vision tasks  including face
veriﬁcation  motion analysis  stereo matching  and object recognition. Alignment results in cor-
respondence across different images by removing intraclass variability and canonicalizing pose.
Alignment methods exist on a supervision spectrum from requiring manually labeled ﬁducial points
or landmarks  to requiring class labels  to fully unsupervised joint alignment and clustering models.
Congealing [6] is an unsupervised joint alignment method based on an entropy objective. Deep
congealing [7] builds on this idea by replacing hand-engineered features with unsupervised feature
learning from multiple resolutions. Inspired by optical ﬂow  SIFT ﬂow [8] matches densely sampled
SIFT features for correspondence and has been applied to motion prediction and motion transfer. In
Section 3  we apply SIFT ﬂow using deep features for aligning different instances of the same class.

1

Keypoint localization Semantic parts carry important information for object recognition  object
detection  and pose estimation. In particular  ﬁne-grained categorization  the subject of many recent
works  depends strongly on part localization [9  10]. Large pose and appearance variation across
examples make part localization for generic object categories a challenging task.
Most of the existing works on part localization or keypoint prediction focus on either facial landmark
localization [11] or human pose estimation. Human pose estimation has been approached using tree
structured methods to model the spatial relationships between parts [12  13  14]  and also using
poselets [15] as an intermediate step to localize human keypoints [16  17]. Tree structured models
and poselets may struggle when applied to generic objects with large articulated deformations and
wide shape variance.

Deep learning Convolutional neural networks have gained much recent attention due to their suc-
cess in image classiﬁcation [2]. Convnets trained with backpropagation were initially succesful in
digit recognition [18] and OCR [19]. The feature representations learned from large data sets have
been found to generalize well to other image classiﬁcation tasks [20] and even to object detection
[3  21]. Recently  Toshev et al. [22] trained a cascade of regression-based convnets for human pose
estimation and Jain et al. [23] combine a weak spatial model with deep learning methods.
The latter work trains multiple small  independent convnets on 64 × 64 patches for binary body-
part detection. In contrast  we employ a powerful pretained ImageNet model that shares mid-elvel
feature representations among all parts in Section 5.
Several recent works have attempted to analyze and explain this overwhelming success. Zeiler and
Fergus [24] provide several heuristic visualizations suggesting coarse localization ability. Szegedy
et al. [25] show counterintuitive properties of the convnet representation  and suggest that individual
feature channels may not be more semantically meaningful than other bases in feature space. A
concurrent work [26] compares convnet features with SIFT in a standard descriptor matching task.
This work illuminates and extends that comparison by providing visual analysis and by moving
beyond single instance matching to intraclass correspondence and keypoint prediction.

1.2 Preliminaries

We perform experiments using a network architecture almost identical1 to that popularized by
Krizhevsky et al. [2] and trained for classiﬁcation using the 1.2 million images of the ILSVRC
2012 challenge dataset [1]. All experiments are implemented using caffe [27]  and our network
is the publicly available caffe reference model. We use the activations of each layer as features 
referred to as convn  pooln  or fcn for the nth convolutional  pooling  or fully connected layer 
respectively. We will use the term receptive ﬁeld  abbreviated rf  to refer to the set of input pixels
that are path-connected to a particular unit in the convnet.

2 Feature visualization

Table 1: Convnet receptive ﬁeld sizes and strides 
for an input of size 227 × 227.

In this section and Figures 1 and 2  we provide a
novel visual investigation of the effective pool-
ing regions of convnet features.
In Figure 1  we perform a nonparametric recon-
struction of images from features in the spirit
of HOGgles [28]. Rather than paired dictionary
learning  however  we simply replace patches
with averages of their top-k nearest neighbors
in a convnet feature space. To do so  we ﬁrst
compute all features at a particular layer  re-
sulting in an 2d grid of feature vectors. We as-
sociate each feature vector with a patch in the
original image at the center of the corresponding receptive ﬁeld and with size equal to the receptive
ﬁeld stride. (Note that the strides of the receptive ﬁelds are much smaller than the receptive ﬁelds

layer
conv1
conv2
conv3
conv4
conv5
pool5

rf size
11 × 11
51 × 51
99 × 99
131 × 131
163 × 163
195 × 195

stride
4 × 4
8 × 8
16 × 16
16 × 16
16 × 16
32 × 32

1Ours reverses the order of the response normalization and pooling layers.

2

conv3

conv4

conv5

uniform rf

r
o
b
h
g
i
e
n

1

s
r
o
b
h
g
i
e
n

5

r
o
b
h
g
i
e
n

1

s
r
o
b
h
g
i
e
n

5

Figure 1: Even though they have large receptive ﬁelds  convnet features carry local information at
a ﬁner scale. Upper left: given an input image  we replaced 16 × 16 patches with averages over
1 or 5 nearest neighbor patches  computed using convnet features centered at those patches. The
yellow square illustrates one input patch  and the black squares show the corresponding rfs for the
three layers shown. Right: Notice that the features retrieve reasonable matches for the centers of
their receptive ﬁelds  even though those rfs extend over large regions of the source image. In the
“uniform rf” column  we show the best that could be expected if convnet features discarded all
spatial information within their rfs  by choosing input patches uniformly at random from conv3-
sized neighborhoods. (Best viewed electronically.)

themselves  which overlap. Refer to Table 1 above for speciﬁc numbers.) We replace each such
patch with an average over k nearest neighbor patches using a database of features densely com-
puted on the images of PASCAL VOC 2011. Our database contains at least one million patches for
every layer. Features are matched by cosine similarity.
Even though the feature rfs cover large regions of the source images  the speciﬁc resemblance of
the resulting images shows that information is not spread uniformly throughout those regions. No-
table features (e.g.  the tires of the bicycle and the facial features of the cat) are replaced in their
corresponding locations. Also note that replacement appears to become more semantic and less
visually speciﬁc as the layer deepens: the eyes and nose of the cat get replaced with differently col-
ored or shaped eyes and noses  and the fur gets replaced with various animal furs  with the diversity
increasing with layer number.
Figure 2 gives a feature-centric rather than image-centric view of feature locality. For each column 
we ﬁrst pick a random seed feature vector (computed from a PASCAL image)  and ﬁnd k nearest
neighbor features  again by cosine similarity. Instead of averaging only the centers  we average
the entire receptive ﬁelds of the neighbors. The resulting images show that similar features tend to
respond to similar colors speciﬁcally in the centers of their receptive ﬁelds.

3

conv3

conv4

conv5

s
r
b
n
5

s
r
b
n
0
5

s
r
b
n
0
0
5

Figure 2: Similar convnet features tend to have similar receptive ﬁeld centers. Starting from a
randomly selected seed patch occupying one rf in conv3  4  or 5  we ﬁnd the nearest k neighbor
features computed on a database of natural images  and average together the corresponding receptive
ﬁelds. The contrast of each image has been expanded after averaging. (Note that since each layer
is computed with a stride of 16  there is an upper bound on the quality of alignment that can be
witnessed here.)
3

Intraclass alignment

(cid:88)

p

(cid:88)

(p q)∈E

(cid:107)w(p) − w(q)(cid:107)2
2 

We conjecture that category learning implicitly aligns instances by pooling over a discriminative
mid-level representation. If this is true  then such features should be useful for post-hoc alignment
in a similar fashion to conventional features. To test this  we use convnet features for the task of
aligning different instances of the same class. We approach this difﬁcult task in the style of SIFT
ﬂow [8]: we retrieve near neighbors using a coarse similarity measure  and then compute dense
correspondences on which we impose an MRF smoothness prior which ﬁnally allows all images to
be warped into alignment.
Nearest neighbors are computed using fc7 features. Since we are speciﬁcally testing the quality of
alignment  we use the same nearest neighbors for convnet or conventional features  and we compute
both types of features at the same locations  the grid of convnet rf centers in the response to a single
image.
Alignment is determined by solving an MRF formulated on this grid of feature locations. Let p be a
point on this grid  let fs(p) be the feature vector of the source image at that point  and let ft(p) be the
feature vector of the target image at that point. For each feature grid location p of the source image 
there is a vector w(p) giving the displacement of the corresponding feature in the target image. We
use the energy function

E(w) =

(cid:107)fs(p) − ft(p + w(p))(cid:107)2 + β

where E are the edges of a 4-neighborhood graph and β is the regularization parameter. Optimiza-
tion is performed using belief propagation  with the techniques suggested in [29]. Message passing
is performed efﬁciently using the squared Euclidean distance transform [30]. (Unlike the L1 regu-
larization originally used by SIFT ﬂow [8]  this formulation maintains rotational invariance of w.)
Based on its performance in the next section  we use conv4 as our convnet feature  and SIFT with
descriptor radius 20 as our conventional feature. From validation experiments  we set β = 3 · 10−3
for both conv4 and SIFT features (which have a similar scale).
Given the alignment ﬁeld w  we warp target to source using bivariate spline interpolation (imple-
mented in SciPy [31]). Figure 3 gives examples of alignment quality for a few different seed images 
using both SIFT and convnet features. We show ﬁve warped nearest neighbors as well as keypoints
transferred from those neighbors.
We quantitatively assess the alignment by measuring the accuracy of predicted keypoints. To obtain
good predictions  we warp 25 nearest neighbors for each target image  and order them from smallest
to greatest deformation energy (we found this method to outperform ordering using the data term).
We take the predicted keypoints to be the median points (coordinate-wise) of the top ﬁve aligned
keypoints according to this ordering.
We assess correctness using mean PCK [32]. We consider a ground truth keypoint to be correctly
predicted if the prediction lies within a Euclidean distance of α times the maximum of the bounding

4

target image

ﬁve nearest neighbors

w
o
ﬂ
4
v
n
o
c

w
o
ﬂ
T
F
I
S

w
o
ﬂ
4
v
n
o
c

w
o
ﬂ
T
F
I
S

Figure 3: Convnet features can bring different instances of the same class into good alignment at
least as well (on average) as traditional features. For each target image (left column)  we show
warped versions of ﬁve nearest neighbor images aligned with conv4 ﬂow (ﬁrst row)  and warped
versions aligned with SIFT ﬂow [8] (second row). Keypoints from the warped images are shown
copied to the target image. The cat shows a case where convnet features perform better  while the
bicycle shows a case where SIFT features perform better. (Note that each instance is warped to a
square bounding box before alignment. Best viewed in color.)

Table 2: Keypoint transfer accuracy using convnet ﬂow  SIFT ﬂow  and simple copying from nearest
neighbors. Accuracy (PCK) is shown per category using α = 0.1 (see text) and means are also
shown for the stricter values α = 0.05 and 0.025. On average  convnet ﬂow performs as well as
SIFT ﬂow  and performs a bit better for stricter tolerances.

aero bike bird boat bttl bus car

cat chair cow table dog horse mbike prsn plant sheep sofa train tv mean
conv4 ﬂow 28.2 34.1 20.4 17.1 50.6 36.7 20.9 19.6 15.7 25.4 12.7 18.7 25.9 23.1 21.4 40.2 21.1 14.5 18.3 33.3 24.9
SIFT ﬂow 27.6 30.8 19.9 17.5 49.4 36.4 20.7 16.0 16.1 25.0 16.1 16.3 27.7 28.3 20.2 36.4 20.5 17.2 19.9 32.9 24.7
NN transfer 18.3 24.8 14.5 15.4 48.1 27.6 16.0 11.1 12.0 16.8 15.7 12.7 20.2 18.5 18.7 33.4 14.0 15.5 14.6 30.0 19.9

mean α = 0.1 α = 0.05 α = 0.025

conv4 ﬂow
SIFT ﬂow
NN transfer

24.9
24.7
19.9

11.8
10.9
7.8

4.08
3.55
2.35

box width and height  picking some α ∈ [0  1]. We compute the overall accuracy for each type of
keypoint  and report the average over keypoint types. We do not penalize predicted keypoints that
are not visible in the target image.
Results are given in Table 2. We show per category results using α = 0.1  and mean results for
α = 0.1  0.05  and 0.025.
Indeed  convnet learned features are at least as capable as SIFT at
alignment  and better than might have been expected given the size of their receptive ﬁelds.

4 Keypoint classiﬁcation

In this section  we speciﬁcally address the ability of convnet features to understand semantic in-
formation at the scale of parts. As an initial test  we consider the task of keypoint classiﬁcation:
given an image and the coordinates of a keypoint on that image  can we train a classiﬁer to label the
keypoint?

5

Table 3: Keypoint classiﬁcation accuracies  in percent  on the twenty categories of PASCAL 2011
val  trained with SIFT or convnet features. The best SIFT and convnet scores are bolded in each
category.

SIFT 10 36
(radius) 20 37
40 35
80 33
160 27
1 16
2 37
3 42
4 44
5 44

conv
(layer)

42
50
54
43
36
14
43
50
53
51

36
39
37
37
34
15
40
46
49
49

aero bike bird boat bttl bus car cat chair cow table dog horse mbike prsn plant sheep sofa train tv mean
64 75 45
68 77 50
74 78 51
69 77 48
66 76 44
27 29 20
63 72 47
71 77 54
73 76 56
71 75 54

32 67 64 40 37 33
35 74 67 47 40 36
41 76 68 47 37 39
42 75 66 42 30 43
38 72 59 35 25 39
19 20 29 15 22 16
35 69 63 38 44 35
41 76 69 46 52 39
42 78 70 45 55 41
41 77 68 44 53 39

37
43
40
36
30
17
40
45
48
45

42
52
52
49
48
12
41
50
52
47

37
44
39
35
29
18
39
46
49
47

63
70
69
70
70
33
65
74
76
73

60
68
69
70
67
29
61
64
68
63

34
38
36
31
27
17
38
47
51
50

39
42
42
36
32
14
40
48
51
49

38
48
49
51
46
16
44
52
53
52

29
33
32
27
25
15
34
40
41
39

(a)

(b)

Figure 5: Cross validation scores for cat
keypoint classiﬁcation as a function of
the SVM parameter C. In (a)  we plot
mean accuracy against C for ﬁve dif-
ferent convnet features; in (b) we plot
the same for SIFT features of different
sizes. We use C = 10−6 for all experi-
ments in Table 3.

(a) cat left eye

(b) cat nose

Figure 4: Convnet features show ﬁne
localization ability  even beyond their
stride and in cases where SIFT features
do not perform as well. Each plot is
a 2D histogram of the locations of the
maximum responses of a classifer in a
21 by 21 pixel rectangle taken around a
ground truth keypoint.

For this task we use keypoint data [15] on the twenty classes of PASCAL VOC 2011 [4]. We extract
features at each keypoint using SIFT [33] and using the column of each convnet layer whose rf
center lies closest to the keypoint. (Note that the SIFT features will be more precisely placed as a
result of this approximation.) We trained one-vs-all linear SVMs on the train set using SIFT at ﬁve
different radii and each of the ﬁve convolutional layer activations as features (in general  we found
pooling and normalization layers to have lower performance). We set the SVM parameter C = 10−6
for all experiments based on ﬁve-fold cross validation on the training set (see Figure 5).
Table 3 gives the resulting accuracies on the val set. We ﬁnd features from convnet layers consis-
tently perform at least as well as and often better than SIFT at this task  with the highest performance
coming from layers conv4 and conv5. Note that we are speciﬁcally testing convnet features trained
only for classiﬁcation; the same net could be expected to achieve even higher performance if trained
for this task.
Finally  we study the precise location understanding of our classiﬁers by computing their responses
with a single-pixel stride around ground truth keypoint locations. For two example keypoints (cat
left eye and nose)  we histogram the locations of the maximum responses within a 21 pixel by 21
pixel rectangle around the keypoint  shown in Figure 4. We do not include maximum responses
that lie on the boundary of this rectangle. While the SIFT classiﬁers do not seem to be sensitive
to the precise locations of the keypoints  in many cases the convnet ones seem to be capable of
localization ﬁner than their strides  not just their receptive ﬁeld sizes. This observation motivates
our ﬁnal experiments to consider detection-based localization performance.

6

5 Keypoint prediction

We have seen that despite their large receptive ﬁeld sizes  convnets work as well as the hand-
engineered feature SIFT for alignment and slightly better than SIFT for keypoint classiﬁcation.
Keypoint prediction provides a natural follow-up test. As in Section 3  we use keypoint annotations
from PASCAL VOC 2011  and we assume a ground truth bounding box.
Inspired in part by [3  34  23]  we train sliding window part detectors to predict keypoint locations
independently. R-CNN [3] and OverFeat [34] have both demonstrated the effectiveness of deep con-
volutional networks on the generic object detection task. However  neither of them have investigated
the application of CNNs for keypoint prediction.2 R-CNN starts from bottom-up region proposal
[35]  which tends to overlook the signal from small parts. OverFeat  on the other hand  combines
convnets trained for classiﬁcation and for regression and runs in multi-scale sliding window fashion.
We rescale each bounding box to 500 × 500 and compute conv5 (with a stride of 16 pixels). Each
cell of conv5 contains one 256-dimensional descriptor. We concatenate conv5 descriptors from a
local region of 3 × 3 cells  giving an overall receptive ﬁeld size of 195 × 195 and feature dimension
of 2304. For each keypoint  we train a linear SVM with hard negative mining. We consider the ten
closest features to each ground truth keypoint as positive examples  and all the features whose rfs
do not contain the keypoint as negative examples. We also train using dense SIFT descriptors for
comparison. We compute SIFT on a grid of stride eight and bin size of eight using VLFeat [36]. For
SIFT  we consider features within twice the bin size from the ground truth keypoint to be positives 
while samples that are at least four times the bin size away are negatives.
We augment our SVM detectors with a spherical Gaussian prior over candidate locations constructed
by nearest neighbor matching. The mean of each Gaussian is taken to be the location of the keypoint
in the nearest neighbor in the training set found using cosine similarity on pool5 features  and we
use a ﬁxed standard deviation of 22 pixels. Let s(Xi) be the output score of our local detector for
keypoint Xi  and let p(Xi) be the prior score. We combine these to yield a ﬁnal score f (Xi) =
s(Xi)1−ηp(Xi)η  where η ∈ [0  1] is a tradeoff parameter. In our experiments  we set η = 0.1 by
cross validation. At test time  we predict the keypoint location as the highest scoring candidate over
all feature locations.
We evaluate the predicted keypoints using the measure PCK introduced in Section 3  taking α = 0.1.
A predicted keypoint is deﬁned as correct if the distance between it and the ground truth keypoint is
less than α · max(h  w) where h and w are the height and width of the bounding box. The results
using conv5 and SIFT with and without the prior are shown in Table 4. From the table  we can see
that local part detectors trained on the conv5 feature outperform SIFT by a large margin and that the
prior information is helpful in both cases. To our knowledge  these are the ﬁrst keypoint prediction
results reported on this dataset. We show example results from ﬁve different categories in Figure
6. Each set consists of rescaled bounding box images with ground truth keypoint annotations and
predicted keypoints using SIFT and conv5 features  where each color corresponds to one keypoint.
As the ﬁgure shows  conv5 outperforms SIFT  often managing satisfactory outputs despite the
challenge of this task. A small offset can be noticed for some keypoints like eyes and noses  likely
due to the limited stride of our scanning windows. A ﬁnal regression or ﬁner stride could mitigate
this issue.

6 Conclusion

Through visualization  alignment  and keypoint prediction  we have studied the ability of the in-
termediate features implicitly learned in a state-of-the-art convnet classiﬁer to understand speciﬁc 
local correspondence. Despite their large receptive ﬁelds and weak label training  we have found in
all cases that convnet features are at least as useful (and sometimes considerably more useful) than
conventional ones for extracting local visual information.
Acknowledgements This work was supported in part by DARPA’s MSEE and SMISC programs  by NSF
awards IIS-1427425  IIS-1212798  and IIS-1116411  and by support from Toyota.

2But see works cited in Section 1.1 regarding keypoint localization.

7

Table 4: Keypoint prediction results on PASCAL VOC 2011. The numbers give average accuracy
of keypoint prediction using the criterion described in Section 3  PCK with α = 0.1.

SIFT

aero bike bird boat bttl bus car
cat chair cow table dog horse mbike prsn plant sheep sofa train tv mean
17.9 16.5 15.3 15.6 25.7 21.7 22.0 12.6 11.3 7.6 6.5 12.5 18.3 15.1 15.9 21.3 14.7 15.1 9.2 19.9 15.7
SIFT+prior 33.5 36.9 22.7 23.1 44.0 42.6 39.3 22.1 18.5 23.5 11.2 20.6 32.2 33.9 26.7 30.6 25.7 26.5 21.9 32.4 28.4
38.5 37.6 29.6 25.3 54.5 52.1 28.6 31.5 8.9 30.5 24.1 23.7 35.8 29.9 39.3 38.2 30.5 24.5 41.5 42.0 33.3
conv5+prior 50.9 48.8 35.1 32.5 66.1 62.0 45.7 34.2 21.4 41.1 27.2 29.3 46.8 45.6 47.1 42.5 38.8 37.6 50.7 45.6 42.5

conv5

Groundtruth

SIFT+prior

conv5+prior

Groundtruth

SIFT+prior

conv5+prior

Figure 6: Examples of keypoint prediction on ﬁve classes of the PASCAL dataset: aeroplane  cat 
cow  potted plant  and horse. Each keypoint is associated with one color. The ﬁrst column is the
ground truth annotation  the second column is the prediction result of SIFT+prior and the third
column is conv5+prior. (Best viewed in color).

References
[1] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei.

Image Database. In CVPR  2009.

ImageNet: A Large-Scale Hierarchical

[2] A. Krizhevsky  I. Sutskever  and G. Hinton. Imagenet classiﬁcation with deep convolutional neural net-

works. In NIPS  2012.

[3] R. Girshick  J. Donahue  T. Darrell  and J. Malik. Rich feature hierarchies for accurate object detection

and semantic segmentation. In CVPR  2014.
[4] M. Everingham  L. Van Gool  C. K.

PASCAL Visual Object Classes Challenge 2011 (VOC2011) Results.
network.org/challenges/VOC/voc2011/workshop/index.html.

I. Williams 

J. Winn 

and A. Zisserman.

The
http://www.pascal-

[5] Debate on Yann LeCun’s Google+ page. https://plus.google.com/+YannLeCunPhD/posts/JBBFfv2XgWM.

Accessed: 2014-5-31.

[6] G. B. Huang  V. Jain  and E. Learned-Miller. Unsupervised joint alignment of complex images. In ICCV 

2007.

8

[7] G. B. Huang  M. A. Mattar  H. Lee  and E. Learned-Miller. Learning to align from scratch. In NIPS 

2012.

[8] C. Liu  J. Yuen  and A. Torralba. Sift ﬂow: Dense correspondence across scenes and its applications.

PAMI  33(5):978–994  2011.

[9] J. Liu and P. N. Belhumeur. Bird part localization using exemplar-based models with enforced pose and

subcategory consistenty. In ICCV  2013.

[10] T. Berg and P. N. Belhumeur. POOF: Part-based one-vs.-one features for ﬁne-grained categorization  face

veriﬁcation  and attribute estimation. In CVPR  2013.

[11] P. N. Belhumeur  D. W. Jacobs  D. J. Kriegman  and N. Kumar. Localizing parts of faces using a consensus

of exemplars. In CVPR  2011.

[12] Y. Yang and D. Ramanan. Articulated pose estimation using ﬂexible mixtures of parts. In CVPR  2011.
[13] M. Sun and S. Savarese. Articulated part-based model for joint object detection and pose estimation. In

ICCV  2011.

[14] X. Zhu and D. Ramanan. Face detection  pose estimation  and landmark localization in the wild.

CVPR  2012.

In

[15] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations. In

ICCV  2009.

[16] G. Gkioxari  B. Hariharan  R. Girshick  and J. Malik. Using k-poselets for detecting people and localizing

their keypoints. In CVPR  2014.

[17] G. Gkioxari  P. Arbelaez  L. Bourdev  and J. Malik. Articulated pose estimation using discriminative

armlet classiﬁers. In CVPR  2013.

[18] Y. LeCun  B. Boser  J.S. Denker  D. Henderson  R. E. Howard  W. Hubbard  and L. D. Jackel. Backprop-

agation applied to hand-written zip code recognition. In Neural Computation  1989.

[19] Y. Lecun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

In Proceedings of the IEEE  pages 2278–2324  1998.

[20] J. Donahue  Y. Jia  O. Vinyals  J. Hoffman  N. Zhang  E. Tzeng  and T. Darrell. DeCAF: A deep convo-

lutional activation feature for generic visual recognition. In ICML  2014.

[21] P. Sermanet  K. Kavukcuoglu  S. Chintala  and Y. LeCun. Pedestrian detection with unsupervised multi-

stage feature learning. In CVPR  2013.

[22] A. Toshev and C. Szegedy. DeepPose: Human pose estimation via deep neural networks. In CVPR  2014.
[23] A. Jain  J. Tompson  M. Andriluka  G. W. Taylor  and C. Bregler. Learning human pose estimation

features with convolutional networks. In ICLR  2014.

[24] M. D Zeiler and R. Fergus. Visualizing and understanding convolutional neural networks.

2014.

[25] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus.

properties of neural networks. In ICLR  2014.

In ECCV 

Intriguing

[26] P. Fischer  A. Dosovitskiy  and T. Brox. Descriptor Matching with Convolutional Neural Networks: a

Comparison to SIFT. ArXiv e-prints  May 2014.

[27] Y. Jia  E. Shelhamer  J. Donahue  S. Karayev  J. Long  R. Girshick  S. Guadarrama  and T. Darrell. Caffe:

Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093  2014.

[28] C. Vondrick  A. Khosla  T. Malisiewicz  and A. Torralba. HOGgles: Visualizing Object Detection Fea-

tures. In ICCV  2013.

[29] P. Felzenszwalb and D. P. Huttenlocher. Efﬁcient belief propagation for early vision. International journal

of computer vision  70(1):41–54  2006.

[30] P. Felzenszwalb and D. Huttenlocher. Distance transforms of sampled functions. Technical report  Cornell

University  2004.

[31] E. Jones  T. Oliphant  P. Peterson  et al. SciPy: Open source scientiﬁc tools for Python  2001.
[32] Y. Yang and D. Ramanan. Articulated human detection with ﬂexible mixtures of parts. In PAMI  2013.
[33] D.G. Lowe. Object recognition from local scale-invariant features. In ICCV  1999.
[34] P. Sermanet  D. Eigen  X. Zhang  M. Mathieu  R. Fergus  and Y. LeCun. Overfeat: Integrated recognition 

localization and detection using convolutional networks. In ICLR  2014.

[35] J. Uijlings  K. van de Sande  T. Gevers  and A. Smeulders. Selective search for object recognition. IJCV 

2013.

[36] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms.

http://www.vlfeat.org/  2008.

9

,Jonathan Long
Ning Zhang
Trevor Darrell
Ivan Glasser
Ryan Sweke
Nicola Pancotti
Jens Eisert
Ignacio Cirac