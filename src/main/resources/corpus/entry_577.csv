2012,Approximating Concavely Parameterized Optimization Problems,We consider an abstract class of optimization problems that are parameterized concavely in a single parameter  and show that the solution path along the parameter can always be approximated with accuracy $\varepsilon >0$ by a set of size $O(1/\sqrt{\varepsilon})$. A lower bound of size $\Omega (1/\sqrt{\varepsilon})$ shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an approximate path of size $O(1/\sqrt{\varepsilon})$. Finally  we provide an implementation of the oracle for soft-margin support vector machines  and a parameterized semi-definite program for matrix completion.,Approximating Concavely Parameterized

Optimization Problems

Joachim Giesen

Germany

Jens K. Mueller

Germany

S¨oren Laue

Germany

Sascha Swiercy

Germany

Friedrich-Schiller-Universit¨at Jena

Friedrich-Schiller-Universit¨at Jena

joachim.giesen@uni-jena.de

soeren.laue@uni-jena.de

Friedrich-Schiller-Universit¨at Jena

Friedrich-Schiller-Universit¨at Jena

jkm@informatik.uni-jena.de

sascha.swiercy@googlemail.com

Abstract

We consider an abstract class of optimization problems that are parameterized
√
concavely in a single parameter  and show that the solution path along the param-
√
eter can always be approximated with accuracy ε > 0 by a set of size O(1/
ε). A
lower bound of size Ω(1/
ε) shows that the upper bound is tight up to a constant
√
factor. We also devise an algorithm that calls a step-size oracle and computes an
approximate path of size O(1/
ε). Finally  we provide an implementation of the
oracle for soft-margin support vector machines  and a parameterized semi-deﬁnite
program for matrix completion.

Introduction

1
Problem description. Let D be a set  I ⊆ R an interval  and f : I × D → R such that

(1) f(t ·) is bounded from below for every t ∈ I  and
(2) f(·  x) is concave for every x ∈ D.

t ∈ D is called optimal at parameter value t if f(t  x∗

We study the parameterized optimization problem h(t) = minx∈D f(t  x).
t ) = h(t)  and x ∈ D is called
A solution x∗
an ε-approximation at t if ε(t  x) := f(t  x) − h(t) ≤ ε. Of course it holds ε(t  x∗
t ) = 0. A subset
P ⊆ D is called an ε-path if P contains an ε-approximation for every t ∈ I. The size of a smallest
ε-approximation path is called the ε-path complexity of the parameterized optimization problem.
The aim of this paper is to derive upper and lower bounds on the path complexity  and to provide
efﬁcient algorithms to compute ε-paths.

Motivation. The rather abstract problem from above is motivated by regularized optimization
problems that are abundant in machine learning  i.e.  by problems of the form

f(t  x) := r(x) + t · l(x) 

min
x∈D

where r(x) is a regularization- and l(x) a loss term. The parameter t controls the trade-off between
regularization and loss. Note that here f(·  x) is always linear and hence concave in the parameter t.

1

Previous work. Due to the widespread use of regularized optimization methods in machine learn-
ing regularization path following algorithms have become an active area of research. Initially  exact
path tracking methods have been developed for many machine learning problems [16  18  3  9]
starting with the algorithm for SVMs by Hastie et al. [10]. Exact tracking algorithms tend to be
slow and numerically unstable as they need to invert large matrices. Also  the exact regularization
path can be exponentially large in the input size [5  14]. Approximation algorithms can overcome
these problems [4]. Approximation path algorithms with approximation guarantees have been de-
veloped for SVMs with square loss [6]  the LASSO [14]  and matrix completion and factorization
problems [8  7].

√
Contributions. We provide a structural upper bound in O(1/
ε) for the ε-path complexity for
the abstract problem class described above. We show that this bound is tight up to a multiplicative
constant by constructing a lower bound in Ω(1/
ε). Finally  we devise a generic algorithm to com-
pute ε-paths that calls a problem speciﬁc oracle providing a step-size certiﬁcate. If such a certiﬁcate
exists  then the algorithm computes a path of complexity in O(1/
ε). Finally  we demonstrate the
implementation of the oracle for standard SVMs and a matrix completion problem. Resulting in the

ﬁrst algorithms for both problems that compute ε-paths of complexity in O(cid:0)1/
only known approximation path algorithm with complexity in O(cid:0)1/

approximation path algorithms have been known for standard SVMs but only a heuristic [12] and an
approximation algorithm for square loss SVMs [6] with complexity in O(1/ε). The best approxi-
mation path algorithm for matrix completion also has complexity in O(1/ε). To our knowledge  the

ε(cid:1). Previously  no
ε(cid:1) is [14] for the LASSO.

√

√

√

√

2 Upper Bound

ε).

−(t) and g(cid:48)

√
Here we show that any problem that ﬁts the problem deﬁnition from the introduction for a compact
interval I = [a  b] has an ε-path with complexity in O(1/
Let (a  b) be the interior of [a  b] and let g : (a  b) → R be concave  then g is continuous and has a
+(t)  respectively  at every point t ∈ I (see for example [15]).
left- and right derivative g(cid:48)
Note that f(·  x) is concave by assumption and h is concave as the minimum over a family of
concave functions.
Lemma 1. For all t ∈ (a  b)  h(cid:48)
Proof. For all t(cid:48) < t it holds h(t(cid:48)) ≤ f(t(cid:48)  x∗
implies

t ) and hence h(t) − h(t(cid:48)) ≥ f(t  x∗

t ) − f(t(cid:48)  x∗

−(t) ≥ f(cid:48)

t ) ≥ f(cid:48)

t ) ≥ h(cid:48)

−(t  x∗

+(t  x∗

t ) which

+(t).

f(t  x∗

h(t) − h(t(cid:48))

t ) − f(t(cid:48)  x∗
t )
t − t(cid:48)

≥ lim
t(cid:48)↑t

t ) ≥ h(cid:48)

h(cid:48)
−(t) := lim
t(cid:48)↑t
The inequality f(cid:48)
+(t  x∗
some algebra from the concavity of f(·  x∗
Deﬁnition 2. Let I = [a  b] be a compact interval  ε > 0  and t0 = a. Let

t − t(cid:48)
+(t) follows analogously  and f(cid:48)

Tk =(cid:8)t| t ∈ (tk−1  b] such that ε(t  x∗

tk−1) := f(t  x∗
and tk = min Tk for all integral k > 0 such that Tk (cid:54)= ∅. Finally  let
| k ∈ N such that Tk (cid:54)= ∅}.

P ∗ = {x∗

−(t  x∗

=: f(cid:48)
t ) ≥ f(cid:48)

−(t  x∗
t ).
+(t  x∗

tk−1) − h(t) = ε(cid:9) 

t ) and the deﬁnition of the derivatives (see [15]).

t ) follows after

tk

1 + . . . + s−1

Lemma 3. Let s1  . . .   sn ∈ R>0  then (s1 + . . . + sn)(s−1
Proof. The claim holds for n = 1 as s1s−1
let a = s1 + . . . + sn−1 and b = s−1
bsn has circumference 2(as−1
circumference for a given area we have 2(as−1
(a + sn)(b + s−1

1 = 1 = 12. Assume the claim holds for n − 1 and
1 + . . . + s−1
n−1. The rectangle with side lengths as−1
n and
n + bsn) and area as−1
n bsn = ab. Since the square minimizes the
ab. The claim for n now follows from
√
ab + 1)2 ≥ ((n− 1) + 1)2 = n2.

n + bsn + 1 ≥ ab + 2

√
ab + 1 = (

n ) = ab + as−1

n + bsn) ≥ 4

n ) ≥ n2.

√

2

Lemma 4. The size of P ∗ is at most

(cid:113)(cid:0)(b − a)(h(cid:48)−(a) − h(cid:48)−(b))(cid:1)/ε ∈ O(cid:0)1/

√

ε(cid:1).

Proof. Let a = t0 ≤ t1 ≤ . . . be the sequence from Deﬁnition 2. Deﬁne δk = tk+1 − tk and
∆k = h(cid:48)

−(tk+1). We have

−(tk) − h(cid:48)

∆k δk ≥ (f(cid:48)

−(tk  x∗

(cid:18) f(tk+1  x∗

tk

≥
= f(tk+1  x∗

tk

−(tk+1))(tk+1 − tk)

) − h(cid:48)
) − f(tk  x∗

tk+1 − tk
) − h(tk+1) = ε(tk+1  x∗

tk

tk

)

− h(tk+1) − h(tk)

tk+1 − tk

) 

tk

(cid:19)

(tk+1 − tk)

where the ﬁrst inequality follows from Lemma 1 and the second inequality follows from concavity
and the deﬁnition of derivatives (see [15]).
Thus  there exists sk > 0 such that δk ≥ εsk and ∆k ≥ s−1

εn2 ≤ ε(s1 + . . . + sn)(s−1

1 + . . . + s−1

≤ (b − a)(∆1 + . . . + ∆n) ≤ (b − a)(h(cid:48)

−(a) − h(cid:48)

−(b)) 

k . It follows from Lemma 3 that
n ) ≤ (δ1 + . . . + δn)(∆1 + . . . + ∆n)

−(b) ≤ h(cid:48)

(cid:113)(cid:0)(b − a)(h(cid:48)−(a) − h(cid:48)−(b))(cid:1)/ε.

−(t) for t ≤ b (which can be proved from con-
where the last inequality follows from h(cid:48)
cavity  see again [15]). Hence  the sequence (tk) and thus the size of P ∗ must be ﬁnite  or more
speciﬁcally n is bounded by
Theorem 5. P ∗ is an ε-path for I = [a  b].
Proof. For any x ∈ D  ε(·  x) is a continuous function. Hence  x∗
tk is an ε-approximation for all
t ∈ [tk  tk+1]  because if there would be t ∈ (tk  tk+1] with ε(t  x∗
) > ε  then by continuity  there
would be also t(cid:48) ∈ (tk  tk+1) with ε(t  x∗
tk
) = ε which contradicts the minimality of tk+1. The
claim of the theorem follows since the proof of Lemma 4 shows that the sequence (tk) is ﬁnite and
hence the intervals [tk  tk+1] cover the whole [a  b].

tk

3 Lower Bound

=

h(t) = min
x∈R

(cid:1)2 − tx∗

(cid:19)
(cid:18)1
2 x2 − tx and thus
2 x2 − tx

√
Here we show that there exists a problem that ﬁts the problem description from the introduction
whose ε-path complexity is in Ω(1/
ε). This shows that the upper bound from the previous section
is tight up to a constant.
Let I = [a  b]  D = R  f(t  x) = 1

where the last equality follows from the convexity and differentiability of f(t  x) in x which together
imply ∂f

(cid:0)x∗
For ε > 0 and x ∈ R let Ix = (cid:8)t ∈ [a  b](cid:12)(cid:12) ε(t  x) := 1
(cid:113)(cid:0)(b − a)(h(cid:48)−(a) − h(cid:48)−(b))(cid:1)/ε =

2 t2 ≤ ε(cid:9)   which is an interval
2 x2 − tx + 1
√
since 1
of x. Hence  the ε-path complexity for the problem is at least (b − a)/2
(cid:113) (b−a)2
Let us compare this lower bound with the upper from the previous section which gives for the
speciﬁc problem at hand 
ε . Hence the upper
√
bound is tight up to constant of at most 2

2 t2 is a quadratic function in t. The length of this interval is 2

2 x2 − tx + 1

t − t = 0.

2ε independent

∂x (t  x∗

t ) = x∗

t = −1

= b−a√

2 t2 

2ε.

√

1
2

2.

ε

t

4 Generic Algorithm

So far we have only discussed structural complexity bounds for ε-paths. Now we give a generic
ε). When applying the generic algorithm to
algorithm to compute an ε-path of complexity in O(1/

√

3

a speciﬁc problem a plugin-subroutine PATHPOLYNOMIAL needs to be implemented for the speciﬁc
problem. The generic algorithm builds on the simple idea that has been introduced in [6] to compute
an (ε/γ)-approximation (for γ > 1) and only update this approximation along the parameter inter-
val I = [a  b] when it fails to be an ε-approximation. The plugin-subroutine PATHPOLYNOMIAL
provides a bound on the step-size for the algorithm  i.e.  a certiﬁcate for how long the approximation
is valid along the interval I. Hence we describe the idea behind the construction of this certiﬁcate
ﬁrst.

4.1 Step-size certiﬁcate and algorithm

We always consider a problem that ﬁts the problem description from the introduction.
Deﬁnition 6. Let P be the set of all concave polynomials p : I → R of degree at most 2. For
t ∈ I  x ∈ D and ε > 0 let

Pt(x  ε) := {p ∈ P | p ≤ h  f(t  x) − p(t) ≤ ε} 

where p ≤ h means p(t(cid:48)) ≤ h(t(cid:48)) for all t(cid:48) ∈ I.
Note that P contains constant and linear polynomials with second derivative p(cid:48)(cid:48) = 0 and quadratic
polynomials with constant second derivative p(cid:48)(cid:48) < 0. If Pt(x  ε) (cid:54)= ∅  then x is an ε-approximation
at parameter value t  because there exists p ∈ P such that ε(t  x) ≤ f(t  x) − p(t) ≤ ε.
Deﬁnition 7. [Step-size] For t ∈ I = [a  b]  p ∈ P  ε > 0  and γ > 1  let δt := t − a and

ρt(p  ε) =

ε
γ δ2

t |p(cid:48)(cid:48)|   if p(cid:48)(cid:48) < 0 and δt > 0.

The step-size is given as

∆t(p  ε) =

where

∆(1)

t (p)

= δt(γ − 1)

∆(2)
∆(3)

 ∆(1)
(cid:115)
(cid:115)

∆(2)

t (p  ε)

∆(3)

t (p  ε)

=

=

t (p)
t (p  ε)
t (p  ε)

: p(cid:48)(cid:48) = 0
: p(cid:48)(cid:48) < 0  ρt(p  ε) ≥ 1
: p(cid:48)(cid:48) < 0  ρt(p  ε) ≤ 1

2

2

(cid:19)2 − δt

(cid:18)

ρt(p  ε) +

(cid:19)

1
2

(cid:18)
ρt(p  ε) − 1
(cid:19)
2

t

2ε
(cid:18)
|p(cid:48)(cid:48)| + δ2
2ε
|p(cid:48)(cid:48)|

1 − 1√
γ

To simplify the notation we will skip the argument ε of the step-size ∆t whenever the value of ε is
obvious from the context.
Observation 8. If ρt(p  ε) = 1/2  then ∆(2)

t (p)  because ρt(p  ε) = 1/2 implies δt =

t (p) = ∆(3)

(cid:113) 2ε

γ |p(cid:48)(cid:48)| .

Lemma 9. For t ∈ (a  b)  x ∈ D  ε > 0 and γ > 1. If there exists p ∈ Pt(x  ε/γ)  then x is an
ε-approximation for all t(cid:48) ∈ [t  b] with t(cid:48) ≤ t + ∆t(p).
Proof. Let g : [a  b] → R be the following linear function 
g(t(cid:48)) = (t(cid:48) − t) p(t) + ε/γ − p(a)

.

t − a

+ p(t) + ε
γ

Then  for all t(cid:48) ∈ [t  b] 
f(t(cid:48)  x) ≤ (t(cid:48) − t) f(t  x) − f(a  x)

t − a

+ f(t  x) ≤ (t(cid:48) − t) p(t) + ε/γ − p(a)

t − a

+ p(t) + ε
γ

= g(t(cid:48))

4

where the ﬁrst inequality follows from the concavity of f(·  x)  and the second inequality follows
from f(t  x) − p(t) ≤ ε/γ and from p(a) ≤ h(a) ≤ f(a  x). Thus  x is an ε-approximation for all
t(cid:48) ∈ [t  b] that satisfy g(t(cid:48)) − p(t(cid:48)) ≤ ε because

ε(t(cid:48)  x) = f(t(cid:48)  x) − h(t(cid:48)) ≤ f(t(cid:48)  x) − p(t(cid:48)) ≤ g(t(cid:48)) − p(t(cid:48)) ≤ ε.

t (p) = ∆t(p).

If p(cid:48)(cid:48) = 0  then g(t(cid:48)) − p(t(cid:48)) is a linear function in t(cid:48)  and g(t(cid:48)) − p(t(cid:48)) ≤ ε solves to t(cid:48) − t ≤

We ﬁnish the proof by considering three cases.
(i)
δt(γ − 1) = ∆(1)
If p(cid:48)(cid:48) < 0  then g(t(cid:48)) − p(t(cid:48)) is a quadratic polynomial in t(cid:48) with second derivative −p(cid:48)(cid:48) > 0 
(ii)
and the equation g(t(cid:48))−p(t(cid:48)) ≤ ε solves to t(cid:48)−t ≤ ∆(2)
t (p). Note that we do not need the condition
ρt(p) ≥ 1/2 here.
(iii) The case p(cid:48)(cid:48) < 0 and ρt(p) ≤ 1/2 can be reduced to Case (ii). From ρt(p) ≤ 1/2 we obtain

t − a = δt ≥(cid:113) 2ε|p(cid:48)(cid:48)|γ and thus a ≤ t −(cid:113) 2ε|p(cid:48)(cid:48)|γ =: ˆa. Let ˆp the restriction of p onto the interval
[ˆa  b] and ˆδt = t − ˆa  then ˆp(cid:48)(cid:48) = p(cid:48)(cid:48)  and thus ρt(ˆp) = ε/(cid:0)γ ˆδ2

t |ˆp(cid:48)(cid:48)|(cid:1) = 1

2. Hence by Observation 8 

t (ˆp) = ∆(2)

t (ˆp). The claim follows from Case (ii).

∆(3)
t (p) = ∆(3)
Assume now that we have an oracle PATHPOLYNOMIAL available that on input t ∈ (a  b) and
ε/γ > 0 returns x ∈ D and p ∈ Pt(x  ε/γ)  then the following algorithm GENERICPATH returns an
ε-path if it terminates.

Algorithm 1 GENERICPATH

Input: f : [a  b] × D → R that ﬁts the problem description  and ε > 0
Output: ε-path for the interval [a  b]
choose ˆt ∈ (a  b)
P := COMPUTEPATH (f  ˆt  ε)
deﬁne ˆf : [a  b] × D → R  (t  x) (cid:55)→ f(a + b − t  x) [then ˆf also ﬁts the problem description]
P := P ∪ COMPUTEPATH ( ˆf   a + b − ˆt  ε)
return P

Algorithm 2 COMPUTEPATH

Input: f : [a  b] × D → R that ﬁts the problem description  ˆt ∈ (a  b) and ε > 0
Output: ε-path for the interval [ˆt  b]
t := ˆt and P := ∅
while t ≤ b do

(x  p) := PATHPOLYNOMIAL(cid:0)t  ε/γ(cid:1)
t := min(cid:8)b  t + ∆t(p)(cid:9)

P := P ∪ {x}

end while
return P

4.2 Analysis of the generic algorithm

The running time of the algorithm GENERICPATH is essentially determined by the complexity of
the computed path times the cost of the oracle PATHPOLYNOMIAL. In the following we show that
the complexity of the computed path is at most O(1/

√

ε).

Observation 10. For c ∈ R let φc : R(cid:2)>

√

|c|(cid:3) → R  x (cid:55)→ √

x2 + c − x. Then we have

1. limx→∞ φc(x) = 0
2. φ(cid:48)

c(x) =
monotonously increasing.

x√

x2+c

− 1 for the derivative of φc. Thus  φ(cid:48)

c(x) > 0 for c < 0 and φc is

5

Furthermore  ∆(2)

t (p) =

=

=

(cid:1)2 − δt

(cid:113) 2ε|p(cid:48)(cid:48)| + δ2
(cid:114)
(cid:16)
(cid:114)
(cid:16)

δ2
t

t

δ2
t

(cid:0)ρt(p) − 1
(cid:17)2
(cid:17)2

2

ρt(p) + γ − 1
2
ρt(p) + γ − 1
2

(cid:16)

(cid:16)

(cid:17)(cid:17)

+ δ2
ρt(p) + γ − 1
2

(cid:0)ρt(p) + 1
(cid:1)
(cid:16)
(cid:16)

2

+ δt(γ − 1).

= φδ2

t γ(1−γ)

δt

+ δ2

t γ(1 − γ) − δt

ρt(p) +

t γ(1 − γ) − δt

ρt(p) + γ − 1
2

(cid:17)

+ δt(γ − 1)

(cid:17)

1
2

Lemma 11. Given t ∈ I and p ∈ P  then ∆t(p) is continuous in |p(cid:48)(cid:48)|.
Proof. The continuity for |p(cid:48)(cid:48)| > 0 follows from the deﬁnitions of ∆(2)
t (p)  and from
Observation 8. Since ρt(p) > 1/2 for small |p(cid:48)(cid:48)| the continuity at |p(cid:48)(cid:48)| = 0 follows from Observa-
tion 10  because

t (p) and ∆(3)

∆(2)

t (p) = lim
|p(cid:48)(cid:48)|→0

t γ(1−γ) (δt · (ρt(p) + γ − 1/2)) + δt(γ − 1) = δt(γ − 1) = ∆(1)
φδ2

lim
|p(cid:48)(cid:48)|→0
where we have used ρt(p) → ∞ as |p(cid:48)(cid:48)| → 0.
Lemma 12. Given t ∈ I and p1  p2 ∈ P  then ∆t(p1) ≥ ∆t(p2) if |p(cid:48)(cid:48)
Proof. The claim is that ∆t(p) is monotonously decreasing in |p(cid:48)(cid:48)|. Since ∆t is continuous in |p(cid:48)(cid:48)|
by Lemma 11 it is enough to check the monotonicity of ∆(1)
t (p). The mono-
tonicity of ∆(1)
t (p) follows directly from the deﬁnitions of the latter. The monotonicity
of ∆(2)

t (p) follows from Observation 10 since we have

t (p) and ∆(3)

t (p) and ∆(3)

1| ≤ |p(cid:48)(cid:48)
2|.

t (p)  ∆(2)

t (p) 

(cid:18)

(cid:18)

(cid:19)(cid:19)

∆(2)

t (p) = φδ2

t γ(1−γ)

δt

ρt(p) + γ − 1
2

t (p) is monotonously decreasing in |p(cid:48)(cid:48)| because δ2

and thus ∆(2)
monotonously decreasing in |p(cid:48)(cid:48)|.
Lemma 13. Given t ∈ I and p ∈ P  then ∆t(p) is monotonously increasing in δt and hence in t.

+ δt(γ − 1) 
t γ(1 − γ) < 0 and ρt(p) is

t (p)  ∆(2)

t (p) and ∆(3)

Proof. Since ∆t(p) is continuous in δt by Observation 8 it is enough to check the monotonicity of
∆(1)
t (p) and ∆(3)
t (p) follows directly from the
t (p) for ρt(p) ≥ 1
2. For c ≥ 0
deﬁnitions of the latter. It remains to show the monotonicity of ∆(2)
let φ−1 : R>0 → R  y (cid:55)→ 1
c (y) > 0 we have
φc(φ−1

. The notation is justiﬁed because for φ−1

t (p). The monotonicity of ∆(1)

c (y)) = y. Apparently  φ−1

(cid:16) c
y − y

(cid:17)

2

c

t (p) = φc1(φ−1
∆(2)

is monotonously decreasing  and we have
c2 (δt)) − δt = φc1(φ−1
c2 (δt)) − φc2(φ−1

c2 (δt)) 

c1 − φ(cid:48)

γ . Note that φ−1
c2 < 0 for c1 > c2  both φ−1

with c1 = 2ε|p(cid:48)(cid:48)| and c2 = c1
Because φ(cid:48)
respective arguments. Hence  ∆(2)
Theorem 14. If there exists p ∈ P and ˆε > 0 such that |q(cid:48)(cid:48)| ≤ |p(cid:48)(cid:48)| for all q that are returned by the
oracle PATHPOLYNOMIAL on input t ∈ [a  b] and ε ≤ ˆε. Then Algorithm 1 terminates after at most

c2 (δt) > 0 since ρt(p) ≥ 1
2  and c2 < c1 since γ > 1.
c2 and φc1 − φc2 are monotonously decreasing in their

ε(cid:1) steps  and thus returns an ε-path of complexity in O(1/

t (p) is monotonously increasing in δt.

O(cid:0)1/

√

√

ε).

Proof. For all t ∈ [ˆt  b]  where ˆt ∈ (a  b) is chosen in algorithm GENERICPATH  we have ∆t(q) ≥
∆t(p) ≥ ∆ˆt(p). Here the ﬁrst inequality is due to Lemma 12 and the second inequality is due
to Lemma 13. Hence  the number of steps in the ﬁrst call of COMPUTEPATH is upper bounded by
(b−ˆt)/(min{∆ˆt(p)  b−ˆt})+1. Similarly  the number of steps in the second call of COMPUTEPATH
is upper bounded by (ˆt − a)/(min{∆a+b−ˆt(p)  ˆt − a}) + 1.

6

(p) does not depend on ε for p(cid:48)(cid:48) = 0. For
For the asymptotic behavior  observe that ∆ˆt(p) = ∆(1)
ˆt
|p(cid:48)(cid:48)| > 0 observe that limε→0 ρˆt(p  ε) = 0. Hence  there exists ˆε > 0 such that ρˆt(p  ε) < 1/2 and
∆(3)
ˆt

(p  ε) ≤ b − ˆt for all ε < ˆε  and thus
min{∆ˆt(p)  b − ˆt} + 1 = b − ˆt

(cid:114)|p(cid:48)(cid:48)|
Analogously  (ˆt − a)/(min{∆a+b−ˆt(p)  ˆt − a}) + 1 ∈ O(cid:0)1/

b − ˆt

+ 1 =

∆(3)
ˆt

(p)

(cid:18) 1√
(cid:19)
ε(cid:1)  which completes the proof.

(b − ˆt) + 1 ∈ O

√
γ√
γ − 1
√

ε

2ε

.

5 Applications

Here we demonstrate on two examples that Lagrange duality can be a tool for implementing the ora-
cle PATHPOLYNOMIAL in the generic path algorithm. This approach obtains the step-size certiﬁcate
from an approximate solution that has to be computed anyway.

5.1 Support vector machines
Given data points xi ∈ Rd together with labels yi ∈ {±1} for i = 1  . . .   n. A support vector
machine (SVM) is the following parameterized optimization problem

(cid:33)

max{0  1 − yi(wT xi + b)} =: f(t  w)

(cid:32)

n(cid:88)

i=1

min

w∈Rd b∈R

(cid:107)w(cid:107)2 + t

1
2

(cid:18)

(cid:19)
−1
2 αT Kα + 1T α =: d(α)

max
α∈Rn

parameterized in the regularization parameter t ∈ [0 ∞). The Lagrangian dual of the SVM is given
as

s.t.

0 ≤ αi ≤ t  yT α = 0 

where K = AT A  A = (y1x1  . . .   ynxn) ∈ Rd×n and y = (y1  . . .   yn) ∈ Rn.

Algorithm 3 PATHPOLYNOMIALSVM
Input: t ∈ (0 ∞) and ε > 0
Output: w ∈ Rd and p ∈ Pt(w  ε)
compute a primal solution w ∈ Rd and a dual solution α ∈ Rn such that f(t  w) − d(α) < ε

deﬁne p : I → R  t(cid:48) (cid:55)→ d(cid:0)αt(cid:48)/t(cid:1)

return (w  p)

Lemma 15. Let (w  p) be the output of PATHPOLYNOMIALSVM on input t > 0 and ε > 0  then
p ∈ Pt(w  ε) and |p(cid:48)(cid:48)| ≤ max0≤ ˆα≤1 ˆαT K ˆα. [Hence  Theorem 14 applies here.]

Proof. Let α be the dual solution computed by PATHPOLYNOMIALSVM and p be the polynomial
deﬁned in PATHPOLYNOMIALSVM. Then 

p(t(cid:48)) = − t(cid:48)2

t2

1

2 αT Kα + t(cid:48)

t

1T α and thus

p(cid:48)(cid:48)(t(cid:48)) = − 1

t2 αT Kα ≤ 0

since K is positive semideﬁnite. Hence  p ∈ P. For p ∈ Pt(w  ε)  it remains to show that p ≤ h =
minw∈Rd f(·  w) and f(t  w) − p(t) ≤ ε. The latter follows immediately from p(t) = d(α). For
t(cid:48) > 0 let α(cid:48) = αt(cid:48)/t  then α(cid:48) is feasible for the dual SVM at parameter value t(cid:48) since α is feasible
for the dual SVM at t. It follows  p(t(cid:48)) = d(α(cid:48)) ≤ h(t(cid:48)) = minw∈Rd f(·  w). Finally  observe that
αi ≤ t implies |p(cid:48)(cid:48)| = 1

t2 αT Kα ≤ max0≤ ˆα≤1 ˆαT K ˆα.

The same results hold when using any positive kernel K. In the kernel case one has the following
primal SVM (see [2]) 

(cid:33)(cid:41)

(cid:33)

(cid:32)

min

β∈Rm b

βT Kβ + t ·

1
2

max

0  1 − yi

βjyjKij + b

=: f (t  β)

.

(cid:40)

n(cid:88)

(cid:32) n(cid:88)

i=1

j=1

7

We have implemented the algorithm GENERICPATH for SVMs in Matlab using LIBSVM [1] as the
SVM solver. To assess the practicability of the proposed algorithm we ran it on several datasets
taken from the LIBSVM website. For each dataset we have measured the size of the computed
ε-path (number of nodes) for t ∈ [0.1  10] and ε ∈ {2−i | i = 2  . . .   10}. Figure 5.1 shows the
size of paths as a function of ε using double logarithmic plots. A straight line plot with slope − 1
√
corresponds to an empirical path complexity that follows the function 1/

ε.

2

(a) Path complexity for a linear SVM

(b) Path complexity for a SVM with Gaussian
kernel exp(−γ(cid:107)u − v(cid:107)2

2) for γ = 0.5

5.2 Matrix completion
Matrix completion asks for a completion X of an (n × m)-matrix Y that has been observed only at
the indices in Ω ⊂ {1  . . .   m} × {1  . . .   n}. The problem can be solved by the following convex
semideﬁnite optimization approach  see [17  11  13] 

(cid:18) A X

X T B

(cid:19)

(cid:23) 0.

s.t.

(cid:88)

(cid:0)Xij − Yij

(i j)∈Ω

1
2

Λ2
ij + ΛijYij

s.t.

(cid:1)2 + t · 1
(cid:18) tI

2

ΛT

(cid:0)tr(A) + tr(B)(cid:1)
(cid:19)

Λ
tI

X∈Rn×m  A∈Rn×n  B∈Rm×m

min

− (cid:88)

(i j)∈Ω

max

Λ∈Rn×m

The Lagrangian dual of this convex semideﬁnite program is given as

(cid:23) 0  and Λij = 0 if (i  j) /∈ Ω.

Let f(t  ˆX) for ˆX = (X  A  B) be the primal objective function at parameter value t  and d(Λ) be
the dual objective function. Analogously to the SVM case we have the following:

Algorithm 4 PATHPOLYNOMIALMATRIXCOMPLETION

Input: t ∈ (0 ∞) and ε > 0
Output: ˆX and p ∈ Pt( ˆX  ε)
compute a primal solution ˆX and a dual solution Λ ∈ Rn×m such that f(t  ˆX) − d(Λ) < ε

return ( ˆX  p)

deﬁne p : I → R  t(cid:48) (cid:55)→ d(cid:0)t(cid:48)/t Λ(cid:1)
Λ ∈ Rn×m(cid:12)(cid:12)(cid:12) (cid:18) tI

(cid:26)

ΛT

Lemma 16. Let ( ˆX  p) be the output of PATHPOLYNOMIALMATRIXCOMPLETION on in-
put t > 0 and ε > 0 
F   where
Ft =
(cid:3)
Λ
tI

(cid:19)
then p ∈ Pt( ˆX  ε) and |p(cid:48)(cid:48)| ≤ max ˆΛ∈F1

(cid:23) 0  Λij = 0  ∀(i  j) /∈ Ω

(cid:107)ˆΛ(cid:107)2

(cid:27)

.

The proof for Lemma 16 is similar to the proof of Lemma 15  and Lemma 16 shows that Theorem 14
can be applied here.

Acknowledgments
schaft (GI-711/3-2).

This work has been supported by a grant of the Deutsche Forschungsgemein-

8

10−310−210−1101epsilon#nodes1/sqrt(epsilon)a1adukefourclassscalemushroomsw1a10−310−210−1101epsilon#nodes1/sqrt(epsilon)a1adukefourclassscalemushroomsw1aReferences
[1] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A library for support vector machines. ACM

Trans. Intell. Syst. Technol.  2(3):27:1–27:27  2011.

[2] Olivier Chapelle. Training a Support Vector Machine in the Primal. Neural Computation 

19(5):1155–1178  2007.

[3] Alexandre d’Aspremont  Francis R. Bach  and Laurent El Ghaoui. Full Regularization Path
for Sparse Principal Component Analysis. In Proceedings of the International Conference on
Machine Learning (ICML)  pages 177–184  2007.

[4] Jerome Friedman  Trevor Hastie  Holger H¨oﬂing  and Robert Tibshirani. Pathwise Coordinate

Optimization. The Annals of Applied Statistics  1(2):302–332  2007.

[5] Bernd G¨artner  Martin Jaggi  and Clement Maria. An Exponential Lower Bound on the Com-

plexity of Regularization Paths. arXiv.org  arXiv:0903.4817v  2010.

[6] Joachim Giesen  Martin Jaggi  and S¨oren Laue. Approximating Parameterized Convex Op-
timization Problems. In Proceedings of Annual European Symposium on Algorithms (ESA) 
pages 524–535  2010.

[7] Joachim Giesen  Martin Jaggi  and S¨oren Laue. Optimizing over the Growing Spectrahedron.
In Proceedings of Annual European Symposium on Algorithms (ESA)  pages 503–514  2012.
[8] Joachim Giesen  Martin Jaggi  and S¨oren Laue. Regularization Paths with Guarantees for
Convex Semideﬁnite Optimization. In Proceedings International Conference on Artiﬁcial In-
telligence and Statistics (AISTATS)  pages 432–439  2012.

[9] Bin Gu  Jian-Dong Wang  Guan-Sheng Zheng  and Yue cheng Yu. Regularization Path for ν-
Support Vector Classiﬁcation. IEEE Transactions on Neural Networks and Learning Systems 
23(5):800–811  2012.

[10] Trevor Hastie  Saharon Rosset  Robert Tibshirani  and Ji Zhu. The entire regularization path
for the support vector machine. The Journal of Machine Learning Research  5:1391–1415 
2004.

[11] Martin Jaggi and Marek Sulovsk´y. A Simple Algorithm for Nuclear Norm Regularized Prob-
lems. In Proceedings of the International Conference on Machine Learning (ICML)  pages
471–478  2010.

[12] Masayuki Karasuyama and Ichiro Takeuchi. Suboptimal Solution Path Algorithm for Sup-
port Vector Machine. In Proceedings of the International Conference on Machine Learning
(ICML)  pages 473–480  2011.

[13] S¨oren Laue. A hybrid algorithm for convex semideﬁnite optimization. In Proceedings of the

International Conference on Machine Learning (ICML)  2012.

[14] Julien Mairal and Bin Yu. Complexity Analysis of the Lasso Regularization Path. In Proceed-

ings of the International Conference on Machine Learning (ICML)  2012.

[15] A. Wayne Roberts and Dale Varberg. Convex functions. Academic Press  New York  1973.
[16] Saharon Rosset and Ji Zhu. Piecewise linear regularized solution paths. Annals of Statistics 

35(3):1012–1030  2007.

[17] Nathan Srebro  Jason D. M. Rennie  and Tommi Jaakkola. Maximum-Margin Matrix Factor-
In Proceedings of Advances in Neural Information Processing Systems 17 (NIPS) 

ization.
2004.

[18] Zhi-li Wu  Aijun Zhang  Chun-hung Li  and Agus Sudjianto. Trace Solution Paths for SVMs
via Parametric Quadratic Programming. In KDD Worskshop: Data Mining Using Matrices
and Tensors  2008.

9

,Alessandro Rudi
Guillermo Canas
Lorenzo Rosasco
Jiezhang Cao
Langyuan Mo
Yifan Zhang
Kui Jia
Chunhua Shen
Mingkui Tan