2014,Learning on graphs using Orthonormal Representation is Statistically Consistent,Existing research \cite{reg} suggests that embedding graphs on a unit sphere can be beneficial in learning labels on the vertices of a graph. However the choice of optimal embedding remains an open issue. \emph{Orthonormal representation} of graphs  a class of embeddings over the unit sphere  was introduced by Lov\'asz \cite{lovasz_shannon}. In this paper  we show that there exists orthonormal representations which are statistically consistent over a large class of graphs  including power law and random graphs. This result is achieved by extending the notion of consistency designed in the inductive setting to graph transduction. As part of the analysis  we explicitly derive relationships between the Rademacher complexity measure and structural properties of graphs  such as the chromatic number. We further show the fraction of vertices of a graph $G$  on $n$ nodes  that need to be labelled for the learning algorithm to be consistent  also known as labelled sample complexity  is $ \Omega\left(\frac{\vartheta(G)}{n}\right)^{\frac{1}{4}}$ where $\vartheta(G)$ is the famous Lov\'asz~$\vartheta$ function of the graph. This  for the first time  relates labelled sample complexity to graph connectivity properties  such as the density of graphs. In the multiview setting  whenever individual views are expressed by a graph  it is a well known heuristic that a convex combination of Laplacians \cite{lap_mv1} tend to improve accuracy. The analysis presented here easily extends to Multiple graph transduction  and helps develop a sound statistical understanding of the heuristic  previously unavailable.,Learning on graphs using Orthonormal
Representation is Statistically Consistent

Rakesh S

Department of Electrical Engineering

Indian Institute of Science
Bangalore  560012  INDIA

rakeshsmysore@gmail.com

Chiranjib Bhattacharyya

Department of CSA

Indian Institute of Science
Bangalore  560012  INDIA

chiru@csa.iisc.ernet.in

Abstract

Existing research [4] suggests that embedding graphs on a unit sphere can be ben-
eﬁcial in learning labels on the vertices of a graph. However the choice of optimal
embedding remains an open issue. Orthonormal representation of graphs  a class
of embeddings over the unit sphere  was introduced by Lov´asz [2]. In this paper 
we show that there exists orthonormal representations which are statistically con-
sistent over a large class of graphs  including power law and random graphs. This
result is achieved by extending the notion of consistency designed in the inductive
setting to graph transduction. As part of the analysis  we explicitly derive rela-
tionships between the Rademacher complexity measure and structural properties
of graphs  such as the chromatic number. We further show the fraction of vertices
of a graph G  on n nodes  that need to be labelled for the learning algorithm to be
4 where ϑ(G)
consistent  also known as labelled sample complexity  is Ω
is the famous Lov´asz ϑ function of the graph. This  for the ﬁrst time  relates la-
belled sample complexity to graph connectivity properties  such as the density of
graphs. In the multiview setting  whenever individual views are expressed by a
graph  it is a well known heuristic that a convex combination of Laplacians [7]
tend to improve accuracy. The analysis presented here easily extends to Multi-
ple graph transduction  and helps develop a sound statistical understanding of the
heuristic  previously unavailable.

(cid:16) ϑ(G)

(cid:17) 1

n

1

Introduction

In this paper we study the problem of graph transduction on a simple  undirected graph G = (V  E) 
with vertex set V = [n] and edge set E ⊆ V ×V . We consider individual vertices to be labelled with
binary values  ±1. Without loss of generality we assume that the ﬁrst f n vertices are labelled  i.e. 
the set of labelled vertices is given by S = [f n]  where f ∈ (0  1). Let ¯S = V \S be the unlabelled
vertex set  and let yS and y ¯S be the labels corresponding to subgraphs S and ¯S respectively.
Given G and yS  the goal of graph transduction is to learn predictions ˆy ∈ Rn  such that er0-1

(cid:3)  ¯y = sgn(ˆy) is small. To aid further discussion we introduce some notations.

1(cid:2)yj (cid:54)= ¯yj

¯S [ˆy] =

(cid:80)

j∈ ¯S

Notation Let S n−1 = {u ∈ Rn|(cid:107)u(cid:107)2 = 1} denote a (n − 1) dimensional sphere. Let Dn  Sn
n denote a set of n × n diagonal  square symmetric and square symmetric positive semi-
and S+
+ be a non-negative orthant. Let 1n ∈ Rn denote a vector
deﬁnite matrices respectively. Let Rn
of all 1’s. Let [n] := {1  . . .   n}. For any M ∈ Sn  let λ1(M) ≥ . . . ≥ λn(M) denote the
eigenvalues and Mi denote the ith row of M  ∀i ∈ [n]. We denote the adjacency matrix of a
graph G by A. Let di denote the degree of vertex i ∈ [n]  di := A(cid:62)
i 1n. Let D ∈ Dn  where

1

2 AD− 1

Dii = di ∀i ∈ [n]. We refer I − D− 1
Let ¯G denote the complement graph of G  with the adjacency matrix ¯A = 1n1n
K ∈ S+
ω(K  y) = maxα∈Rn
be the label  prediction and soft-prediction spaces over V . Given a graph G and labels y ∈ Y n

2 as the Laplacian  where I denotes the identity matrix.
(cid:62) − I − A. For
(cid:16)
n(cid:80)
. Let Y = ¯Y = {±1} (cid:98)Y ⊆ R
n and y ∈ {±1}n  the dual formulation of Support vector machine (SVM) is given by
Aij. We use (cid:96) : Y × (cid:98)Y → R+ to denote any loss function.

on V   let cut(A  y) := (cid:80)
In particular  for a ∈ Y  b ∈ (cid:98)Y  let (cid:96)0-1(a  b) = 1[ab < 0]  (cid:96)hinge(a  b) = (1 − ab)+

1 and
(cid:96)ramp(a  b) = min(1  (1 − ab)+) denote the 0-1  hinge and ramp loss respectively. The notations
O  o  Ω  Θ will denote standard measures deﬁned in asymptotic analysis [14].

αiαjyiyjKij

αi − 1

g(α  K  y)

n(cid:80)

yi(cid:54)=yj

(cid:17)

i j=1

i=1

=

2

+

Motivation Regularization framework is a widely used tool for learning labels on the vertices of
a graph [23  4]

(cid:96)(yi  ˆyi) + λˆy(cid:62)K−1 ˆy

(1)

(cid:88)

i∈S

min
ˆy∈Y n

1
|S|

ES

ˆy∈Y n

λ|S|

where K is a kernel matrix and λ > 0 is an appropriately chosen regularization parameter. It was
shown in [4] that the optimal ˆy∗ satisﬁes the following generalization bound

(cid:16) trp(K)
(cid:8)erV [ˆy] + λˆy(cid:62)K−1 ˆy(cid:9) + c2
(cid:2)er0-1
¯S [ˆy∗](cid:3) ≤ c1 inf
i∈H (cid:96)(·)(yi  ˆyi)  H ⊆ V 2; trp(K) = (cid:0) 1
(cid:80)n
(cid:80)

(cid:17)p
(cid:1)1/p  p > 0 and
optimal kernel from such a set. The important problem of consistency(cid:0)er ¯S → 0 as n → ∞  to be
formally deﬁned in Section 3(cid:1) of graph transduction algorithms was introduced in [5]. [5] showed

where er(·)
c1  c2 are dependent on (cid:96). [4] argued that for good generalization  trp(K) should be a constant 
which motivated them to normalize the diagonal entries of K. It is important to note that the set of
normalized kernels is quite big and the above presented analysis gives little insight in choosing the

H [ˆy] := 1|H|

i=1 Kp

that the formulation (1)  when used with a laplacian dependent kernel  achieves a generalization
error of ES [er ¯S [ˆy∗]] = O
  where q is the number of pure components3. Though [5]’s
algorithm is consistent for a small number of pure components  they achieve the above convergence
rate by choosing λ dependent on true labels of the unlabeled nodes  which is not practical [6].
In this paper  we formalize the notion of consistency of graph transduction algorithms and derive
novel graph-dependent statistical estimates for the following formulation.

(cid:16)(cid:113) q

(cid:17)

nf

ii

n

α(cid:62)Kα + C

1
2

i∈S

¯yj∈ ¯Y j∈ ¯S

ΛC(K  yS) = min

where ˆyk = (cid:80)

i∈S Kikyiαi +(cid:80)

min
α∈Rn
+
j∈ ¯S Kjk ¯yjαj  ∀k ∈ V . If all the labels are observed then [22]
showed that the above formulation is equivalent to (1). We note that the normalization step con-
sidered by [4] is equivalent to ﬁnding an embedding of a graph on a sphere. Thus  we study or-
thonormal representations of graphs [2]  which deﬁne a rich class of graph embeddings on an unit
sphere. We show that the formulation (2) working with orthonormal representations of graphs is
statistically consistent over a large class of graphs  including random and power law graphs. In the
sequel  we apply Rademacher complexity to orthonormal representations of graphs and derive novel
graph-dependent transductive error bound. We also extend our analysis to study multiple graph
transduction. More speciﬁcally  we make the following contributions.

j∈ ¯S

(2)

(cid:88)

(cid:96)(cid:0)ˆyi  yi

(cid:1) + C

(cid:88)

(cid:96)(cid:0)ˆyj  ¯yj

(cid:1)

Contributions The main contribution of this paper is that we show there exists orthonormal rep-
resentations of graphs that are statistically consistent on a large class of graph families Gc. For a
special orthonormal representation—LS labelling  we show consistency on Erd¨os R´enyi random
graphs. Given a graph G ∈ Gc  with a constant fraction of nodes labelled f = O(1)  we derive

1(a)+ = max(a  0).
2We drop the argument ˆy  when implicit from the context.
3Pure component is a connected subgraph  where all the nodes in the subgraph have the same label.

2

(cid:16) ϑ(G)

(cid:17) 1

n

n

an error convergence rate of er0-1

ϑ function of the graph G. Existing work [5] showed an expected convergence rate of O(cid:0)(cid:112) q

4   with high probability; where ϑ(G) is the Lov´asz

¯S = O

(cid:1) 

however q is dependent on the true labels of the unlabelled nodes. Hence their bound cannot be
computed explicitly [6]. We also apply Rademacher complexity measure to the function class as-
sociated with orthonormal representations and derive a tight bound relating to χ(G)  the chromatic
number of the graph G. We show that the Laplacian inverse [4] has O(1) complexity on graphs with
high connectivity  whereas LS labelling exhibits a complexity of Θ(n 1
4 ). Experiments demonstrate
superior performance of LS labelling on several real world datasets. We derive novel transductive
error bound  relating to graph structural measures. Using our analysis  we show that observing labels
4 fraction of the nodes is sufﬁcient to achieve consistency. We also propose an efﬁ-
of Ω
cient Multiple Kernel Learning (MKL) based algorithm  with generalization guarantees for multiple
graph transduction. Experiments demonstrate improved performance in combining multiple graphs.

(cid:16) ϑ(G)

(cid:17) 1

n

2 Preliminaries

Orthonormal Representation: [2] introduced the idea of orthonormal representations for the prob-
lem of embedding a graph on a unit sphere. More formally  an orthonormal representation of a
simple  undirected graph G = (V  E) with V = [n]  is a matrix U = [u1  . . .   un] ∈ Rd×n such
that uT
Let Lab(G) denote the set of all possible orthonormal representations of the graph G given by

Lab(G) :=(cid:8)U|U is an Orthonormal Representation(cid:9). [1] recently introduced the notion of graph

i uj = 0 whenever (i  j) /∈ E and ui ∈ S d−1 ∀i ∈ [n].

embedding to Machine Learning community and showed connections to graph kernel matrices. Con-
n|Kii = 1 ∀i ∈ [n]; Kij = 0 ∀(i  j) /∈ E}.
sider the set of graph kernels K(G) := {K ∈ S+
[1] showed that for every valid kernel K ∈ K(G)  there exists an orthonormal representation
U ∈ Lab(G); and it is easy to see the other way  K = U(cid:62)U ∈ K(G). Thus  the two sets 
Lab(G) and K(G) are equivalent. Orthonormal representation is also associated with an interesting

quantity  the Lov´asz number [2]  deﬁned as: ϑ(G) = 2(cid:0) minK∈K(G) ω(K  1n)(cid:1) [1]. ϑ function is a
Lov´asz Sandwich Theorem: [2] Given an undirected graph G = (V  E)  I(cid:0) ¯G(cid:1) ≤ ϑ(cid:0) ¯G(cid:1) ≤ χ(G);
where I(cid:0) ¯G(cid:1) is the independent number of the complement graph ¯G.

fundamental tool for combinatorial optimization and approximation algorithms for graphs.

3 Statistical Consistency of Graph Transduction Algorithms

¯Sn

¯Sn

In this section  we formalize the notion of consistency of graph transduction algorithms. Given
a graph Gn = (Vn  En) of n nodes  with labels of subgraph Sn ⊆ Vn observable  let er∗
:=
¯Sn
inf ˜y∈ ¯Y n er ¯Sn [˜y] denote the minimal unlabelled node set error. Consistency is a measure of the
quality of the learning algorithm A  comparing er ¯Sn [ˆy] to er∗
  where ˆy are the predictions made
by A. A related notion of loss consistency has been extensively studied in literature [3  12]  which
only show that the difference er ¯Sn [ˆy] − erSn [ˆy] → 0 as n → ∞ [6]. This does not conﬁrm the
optimality of A  that is er ¯Sn[ˆy] → er∗
. Hence  a notion stronger than loss consistency is needed.
Let Gn belong to a graph family G  ∀n. Let Πf be the uniform distribution over the random draw of
the labelled subgraph Sn ⊆ Vn  such that |Sn| = f n  f ∈ (0  1). As discussed earlier  we want the
(cid:96)-regret  RSn [A] = er ¯Sn [ˆy]− er∗
to be small. Since  the labelled nodes are drawn randomly  there
is a small probability that one gets an unrepresentative subgraph Sn. However  for large n  we want
(cid:96)-regret to be close to zero with high probability4. In other words  for every ﬁnite and ﬁxed n  we
want to have an estimate on the (cid:96)-regret  which decreases as n increases. We deﬁne the following
notion of consistency of graph transduction algorithms to capture this requirement
Deﬁnition 1. Let G be a graph family and f ∈ (0  1) be ﬁxed. Let V = {(vi  yi  Ei)}∞
i=1 be an
inﬁnite sequence of labelled node set  where yi ∈ Y and Ei is the edge information of node vi
with the previously observed nodes v1  . . .   vi−1  ∀i ≥ 2. Let Vn be the ﬁrst n nodes in V  and let
4If G is not deterministic (e.g.  Erd¨os R´eyni)  then there is small probability that one gets an unrepresentative

graph  in which case we want the (cid:96)-regret to be close to zero with high probability over Gn ∼ G.

¯Sn

3

Gn ∈ G be the graph deﬁned by (Vn  E1  . . .   En). Let Sn ⊆ Vn  and let yn  ySn be the labels of
Vn  Sn respectively. A learning algorithm A when given Gn and ySn returns soft-predictions ˆy is
said to be (cid:96)-consistent w.r.t G if  when the labelled subgraph Sn are random drawn from Πf   the
(cid:96)-regret converges in probability to zero  i.e.  ∀ > 0

PrSn∼Πf [RSn[A] ≥ ] → 0

as n → ∞

In Section 6 we show that the kernel learning style algorithm (2) working with orthonormal rep-
resentations is consistent on a large class of graph families. To the best of our knowledge  we are
not aware of any literature which provide an explicit empirical error convergence rate and prove
consistency of the graph transduction algorithm considered. Before we prove our main result  we
gather useful tools—a) complexity measure  which reacts to the structural properties of the graph
b) generalization analysis to bound er ¯S (Section 5). In the interest of space  we defer
(Section 4);
most of the proofs to the supplementary material5.

4 Graph Complexity Measures

In this section we apply Rademacher complexity to orthonormal representations of graphs  and relate
to the chromatic number. In particular  we study LS labelling  whose class complexity can be shown
to be greater than that of the Laplacian inverse on a large class of graphs.
(2) be solved for K ∈ K(G)  and let U ∈ Lab(G) be the orthonormal representation cor-
Let
responding to K (Section 2). Then by Representer’s theorem  the classiﬁer learnt by (2) is of the
form h = Uβ  β ∈ Rn. We deﬁne Rademacher complexity of the function class associated with
orthonormal representations
Deﬁnition 2 (Rademacher Complexity). Given a graph G = (V  E)  with V = [n]; let U ∈ Lab(G)
σ = (σ1  . . .   σn) be a vector of i.i.d. random variables such that σi ∼ {+1 −1  0} w.p. p  p and
1 − 2p respectively. The Rademacher complexity of the graph G deﬁned by U  ¯HU is given by
R( ¯HU  p) = 1

and ¯HU = (cid:8)h|h = Uβ  β ∈ Rn(cid:9) be the function class associated with U. For p ∈ (0  1/2]  let

σi (cid:104)h  ui(cid:105)(cid:105)

n(cid:80)

(cid:104)

Eσ

n

suph∈ ¯HU

i=1

The above deﬁnition is motivated from [9  3]. This is an empirical complexity measure  suited for
the transductive settings. We derive the following novel tight Rademacher bound
Theorem 4.1. Let G = (V  E) be a simple  undirected graph with V = [n]  U ∈ Lab(G) and
K = U(cid:62)U ∈ K(G) be the graph-kernel corresponding to U. The Rademacher complexity of graph
2 is a constant.

p ∈ (cid:2)1/n  1/2(cid:3). Let HU = (cid:8)h(cid:12)(cid:12) h = Uβ  β ∈ Rn (cid:107)β(cid:107)2 ≤ tC
G deﬁned by U is given by R(HU  p) = c0tC(cid:112)pλ1(K)  where 1/2

n(cid:9)  C > 0  t ∈ [0  1] and let
2 ≤ c0 ≤ √

√

√

The above result provides a lower bound for the Rademacher complexity for any unit sphere graph
embedding. While upper-bounds maybe available [9  3] but  to the best of our knowledge  this is the
ﬁrst attempt at establishing lower bounds. The use of orthonormal representations allow us to relate
class complexity measure to graph-structural properties.

Corollary 4.2. For C  t  p = O(1)  R(HU  p) = O((cid:112)χ(G)). (Suppl.)
as large as O((cid:112)χ(G))  which motivate us to ﬁnd substantially better regularizers. In particular  we

Such connections between learning theory complexity measures and graph properties was previously
unavailable [9  3]. Corollary 4.2 suggests that there exists graph regularizers with class complexity
investigate LS labelling [16]; given a graph G  LS labelling KLS ∈ K(G) is deﬁned as

KLS =

A
ρ

+ I  ρ ≥ |λn(A)|

(3)

LS labellinghas high Rademacher complexity on a large class of graphs  in particular
Corollary 4.3. For a random graph G(n  q)  q ∈ [0  1)  where each edge is present independently
w.p. q; for C  t  q = O(1) the Rademacher complexity of the function class associated with LS
labelling (3) is Θ(n 1

4 )  with high probability. (Suppl.)

5mllab.csa.iisc.ernet.in/rakeshs/nips14/suppl.pdf

4

For the limiting case of complete graphs  we can show that Laplacian inverse [4]  the most widely
used graph regularizer has O(1) complexity (Claim 2  Suppl.)  thus indicating that it may be subop-
timal for graphs with high connectivity. Experimental results illustrate our observation.
We derive a class complexity measure for unit sphere graph embeddings  which indicates the rich-
ness of the function class  and helps the learning algorithm to choose an effective embedding.

5 Generalization Error Bound

In the previous section  we applied Rademacher complexity to orthonormal representations.
In
this section we derive novel graph-dependent generalization error bounds  which will be used in
Section 6. Following a similar proof technique as in [3]  we propose the following error bound—
Theorem 5.1. Given a graph G = (V  E)  V = [n] with y ∈ Y n being the unknown binary labels
over V ; let U ∈ Lab(G)  and K ∈ K(G) be the corresponding kernel. Let ˜HU = {h|h = Uβ  β ∈
Rn  (cid:107)β(cid:107)∞ ≤ C}  C > 0. Let (cid:96) be any loss function  bounded in [0  B] and L-Lipschitz in its
second argument. For f ∈ (0  1/2]6  let labels of subgraph S ⊆ V be observable  |S| = nf. Let
¯S = V \S. For any δ > 0 and h ∈ ˜HU  with probability ≥ 1 − δ over S ∼ Πf

(cid:115)

(cid:114) 1

er ¯S[ˆy] ≤ erS[ˆy] + LC

2λ1(K)
f (1 − f )

+

c1B
1 − f

log

1
δ

(4)

nf

where ˆy = U(cid:62)h and c1 > 0 is a constant. (Suppl.)
Discussion Note that from [2]  λ1(K) ≤ χ(G) and χ(G) is in-turn bounded by the maximum
degree of the graph [21]. Thus  if L  B  f = O(1)  then for sparse  degree bounded graphs; for
the choice of parameter C = Θ(1/
n)  the slack term and the complexity term goes to zero as n
increases. Thus  making the bound useful. Examples include tree  cycle  path  star and d-regular
(with d = O(1)). Such connections relating generalization error to graph properties was not avail-
able before. We exploit this novel connection to analyze graph transduction algorithms in Section 6.
Also  in Section 7  we extend the above result to the problem of multiple graph transduction.

√

5.1 Max-margin Orthonormal Representation

labels on V   let ¯H = (cid:83)

To analyze er0-1
S relating to graph structural measure  the ϑ function  we study the maximum margin
induced by any orthonormal representation  in an oracle setting.
We study a fully ‘labelled graph’ G = (V  E  y)  where y ∈ Y n are the binary labels on the vertices
V . Given any U ∈ Lab(G)  the maximum margin classiﬁer is computed by solving ω(K  y) =
g(α∗  K  y) where K = U(cid:62)U ∈ K(G). It is interesting to note that knowing all the labels  the
max-margin orthonormal representation can be computed by solving an SDP. More formally
Deﬁnition 3. Given a labelled graph G = (V  E  y)  where V = [n] and y ∈ Y n are the binary
¯HU  where ¯HU = {h|h = Uβ  β ∈ Rn}. Let K ∈ K(G) be
the kernel corresponding to U ∈ Lab(G). The max-margin orthonormal representation associated
with the kernel function is given by Kmm = argminK∈K(G) ω(K  y).
By deﬁnition  Kmm induces the largest margin amongst other orthonormal representations  and
hence is optimal. The optimal margin has interesting connections to the Lov´asz ϑ function —
Theorem 5.2. Given a labelled graph G = (V  E  y)  with V = [n] and y ∈ Y n being the binary
labels on vertices. Let Kmm be as in Deﬁnition 3  then ω(Kmm  y) = ϑ(G)/2. (Suppl.)

U∈Lab(G)

Thus  knowing all the labels  computing Kmm is equivalent to solving the ϑ function. However 
in the transductive setting  Kmm cannot be computed. Alternatively  we explore LS labelling (3) 
which gives a constant factor approximation to the optimal margin on a large class of graphs.
Deﬁnition 4. A class of labelled graphs G = {G = (V  E  y)} is said to be a Labelled SVM-ϑ graph
family  if there exist a constant γ > 1 such that ∀G ∈ G  ω(KLS  y) ≤ γω(Kmm  y).

6We can generalize our result for f ∈ (0  1)  but for the simplicity of the proof we assume f ∈ (0  1/2].

This is also true in practice  where the number of labelled examples is usually very small.

5

Algorithm 1

Input: U  yS and C > 0.
Get α∗  ¯y∗
Return: ˆy = U(cid:62)hS  where hS = UYα∗; Y ∈ Dn   Y = yi  if i ∈ S  otherwise ¯y∗
i .

¯S by solving ΛC(K  yS) (2) for (cid:96)hinge and K = U(cid:62)U.

Such class of graphs are interesting  because one can get a constant factor approximation to the
optimal margin  without the knowledge of the true labels e.g.  Mixture of random graphs: G =
(V  E  y)  with y(cid:62)1n = 0  cut(A  y) ≤ c
n  for c > 1 being a constant and the subgraphs
corresponding to the two classes form G(n/2  1/2) random graphs (Claim 3  Suppl.).
We relate the maximum geometric margin induced by orthonormal representations to the ϑ function
of the graph. This allows us to derive novel graph dependent learning theory estimates.

√

6 Consistency of Orthonormal Representation of Graphs

.

Aggregating results from Section 4 and 5  we show that Algorithm 1 working with orthonormal
representations of graphs is consistent on a large class of graph families. For every ﬁnite and ﬁxed
n  we derive an estimate on er0-1
(cid:18)
¯Sn
Theorem 6.1. For the setting as in Deﬁnition 1  let f ∈ (0  1/2] be ﬁxed. Let ˆy be the predictions
learnt by Algorithm 1 with inputs Un ∈ Lab(Gn)  ySn and C∗ =
. Then ∃Un ∈
(cid:115)
(cid:19) 1
n over Sn ∼ Πf
Lab(Gn)  ∀Gn such that with probability atleast 1 − 1

ϑ2(Gn)(1−f )
23n2f ϑ( ¯Gn)

(cid:32)(cid:18) ϑ(Gn)

(cid:19) 1

(cid:33)

4

4

er0-1
¯Sn

[ˆy] = O

f 3(1 − f )n

+

1
1 − f

log n
nf

Proof. Let Kn ∈ K(Gn) be the max-margin kernel associated with Gn (Deﬁnition 3)  and let
Un ∈ Lab(G) be the corresponding orthonormal representation. Since (cid:96)ramp is an upper bound on
(cid:96)0-1  we concentrate on bounding erramp

[ˆy]. Note that for any C > 0

C|Sn| · erramp

Sn

¯Sn
[ˆy] ≤ C|Sn| · erhinge

Sn

[ˆy] ≤ ΛC(Kn  ySn )
ϑ(Gn)

≤ ΛC(Kn  yn) ≤ ω(Kn  yn) =

2

(cid:115)

The last inequality follows from Theorem 5.2. Note that for ramp loss L = B = 1; using Theo-
n over the random draw of Sn ∼ Πf  
rem 5.1 for δ = 1

n  it follows that with probability atleast 1 − 1

+ C

erramp

¯Sn

(cid:16) ϑ2(Gn)(1−f )

23n2f ϑ( ¯Gn)

[ˆy] ≤ ϑ(Gn)
2Cnf

where c1 = O(1). Using λ1(Kn) ≤ ϑ( ¯Gn)

+

log n
nf

c1
1 − f

2λ1(Kn)
f (1 − f )
[2] and optimizing RHS for C  we get C∗ =

(cid:1) = n [2] proves the claim.

(5)

(cid:17) 1
4 . Plugging back C∗ and using ϑ(Gn)ϑ(cid:0) ¯Gn
(cid:2)er ¯Sn

(cid:3) = O(cid:0)(cid:112) q

(cid:1). However  as noted in Section 1  the quantity q is dependent

n

[5] showed that ES
on y ¯Sn  and hence their bounds cannot be computed explicitly [6].
We assume that the graph does not contain duplicate nodes with opposite labels  er∗
= 0. Thus 
¯Sn
consistency follows from the fact that ϑ(G) ≤ n and for large families of graphs it is O(nc) where
0 ≤ c < 1. This theorem points to the fact that if f = O(1)  then by Deﬁnition 1  Algorithm 1 is
(cid:96)0-1- consistency over such class of graph families. Examples include
Power-law graphs: Graphs where the degree sequence follows a power law distribution. We show
that ϑ( ¯G) = O(
n) for naturally occurring power law graphs (Claim 4  Suppl.). Thus  working

with the complement graph(cid:0) ¯G(cid:1)  makes Algorithm 1 consistent.

√

(cid:115)

6

√

Random graphs: For G(n  q) graphs  q = O(1); with high probability ϑ(G(n  q)) = Θ(
n) [13].
Note that choosing Kn for various graph families is difﬁcult. Alternatively  for Labelled SVM-ϑ
graph family (Deﬁnition 4)  if Lov´asz ϑ function is sub-linear  then for the choice of LS labelling 
Algorithm 1 is (cid:96)0-1consistent. Examples include Mixture of random graphs (Section 5.1). Further-
more  we analyze the fraction of labelled nodes to be observed  such that Algorithm 1 is consistent.
Corollary 6.2 (Labelled Sample Complexity). Given a graph family Gc  such that ϑ(Gn) =
O(nc)  ∀Gn ∈ Gc where 0 ≤ c < 1. For C = C∗ as in Theorem 6.1; 1
  ε > 0
fraction of labelled nodes is sufﬁcient for Algorithm 1 to be (cid:96)0-1-consistent w.r.t. Gc.
The proof directly follows from Theorem 6.1. As a consequence of the above result  we can ar-
gue that for sparse graphs (ϑ(G) is large) one would need a larger fraction of nodes labelled  but
for denser graphs (ϑ(G) is small) even a smaller fraction of nodes being labelled sufﬁces. Such
connections relating sample complexity and graph properties was not available before.
To end this section  we discuss on the possible extensions to the inductive setting (Claim 5  Suppl.)—
we can show that that the uniform convergence of er ¯S to erS in the transductive setting (for f = 1/2)
is a necessary and sufﬁcient condition for the uniform convergence of erS to the generalization error.
Thus  the results presented here can be extended to the supervised setting. Furthermore  combining
Theorem 5.1 with the results of [9]  we can also extend our results to the semi-supervised setting.

(cid:16) ϑ(Gn)

(cid:17)1/3−ε

n

2

7 Multiple Graph Transduction

Many real world problems can be posed as learning on multiple graphs [19  ?]. Existing algorithms
for single graph transduction [10  15] cannot be trivially extended to the new setting. It is a well
known heuristic that taking a convex combination of Laplacian improves classiﬁcation performance
[7]  however the underlying principle is not well understood. We propose an efﬁcient MKL style
algorithm with generalization guarantees. Formally  the problem of multiple graph transduction is—

Problem 1. Given G = {G(1)  . . .   G(m)} a set of simple  undirected graphs G(k) = (cid:0)V  E(k)(cid:1) 

deﬁned on a common vertex set V = [n]. Without loss of generality we assume that the ﬁrst f n
vertices are labelled  i.e.  the set of labelled vertices is given by S = [f n]  where f ∈ (0  1). Let
¯S = V \S be the unlabelled node set. Let yS  y ¯S be the labels of S  ¯S respectively. Given G and
labels yS  the goal is to accurately predict the labels of y ¯S.
Let K = {K(1)  . . .   K(m)} be the set of kernels corresponding to graphs G; K(k) ∈ K(G(k)) ∀k ∈
[m]. We propose the following MKL style formulation for multiple graph transduction

(cid:19)

ΨC(K  yS) =

η∈Rm

min
+  (cid:107)η(cid:107)1=1

min

¯yj∈ ¯Y ∀j∈ ¯S

α∈Rn

max
+ (cid:107)α(cid:107)∞≤C

g

α 

ηkK(k)  [yS  ¯y ¯S]

(6)

the setting as

Extending our analysis from Section 5  we propose the following error bound
Theorem 7.1. For
let f
{K(1)  . . .   K(m)}  K(k) ∈ K(G(k))  ∀k ∈ [m]. Let α∗  η∗  ¯y∗
(6). Let ˆy =
δ > 0  with probability ≥ 1 − δ over the choice of S ⊆ V such that |S| = nf

kK(k) ¯Yα∗  where ¯Y ∈ Dn  ¯Yii = yi if i ∈ S  otherwise ¯y∗
η∗

and K =
¯S be the solution to ΨC(K  yS)
i . Then  for any

in Problem 1 

(0  1/2]7

m(cid:80)

∈

i=1

(cid:115)

(cid:18)

m(cid:88)

k=1

(cid:114) 1

¯S [ˆy] ≤ ¯Ψ(K  y)
er0-1

Cnf

+ C

2ϑ( ¯G∪)
f (1 − f )

+

c1
1 − f

log

1
δ

nf

where c1 = O(1)  ¯Ψ(K  y) = mink∈[m] ω(K(k)  y) and G∪ is the union of graphs G8. (Suppl.)
The above result gives us the ability for the ﬁrst time to analyze generalization performance of multi-
ple graph transduction algorithms. The expression ¯Ψ(K  y) suggests that combining multiple graphs
should improve performance over considering individual graphs separately. Similar to Section 6 

7As in Theorem 5.1  we can generalize our results for f ∈ (0  1).
8G∪ = (V  E∪)  where (i  j) ∈ E∪ if edge (i  j) is present in atleast one of the graphs G(k) ∈ G  k ∈ [m].

7

we can show that if one of the graph families G(l)  l ∈ [m] of G obey ϑ(G(l)
n ) = O(nc)  0 ≤ c < 1;
n ∈ G(l)  then there exists orthonormal representations K  such that the MKL style algorithm
G(l)
optimizing for (6) is (cid:96)0-1-consistent over G (Claim 6  Suppl.). We can also show that combining
graphs improves labelled sample complexity (Claim 7  Suppl.). This is a ﬁrst attempt in developing
a statistical understanding for the problem of multiple graph transduction.

8 Experimental results

We conduct two sets of experiments9.

Table 1: Superior performance of LS labelling.

LS-lab Un-Lap N-Lap KS-Lap
Dataset
AuralSonar∗
76.5
Yeast-SW-5-7∗
60.4
Yeast-SW-5-12∗ 78.6
Yeast-SW-7-12∗ 76.5
Diabetes†
73.1
Fourclass†
73.3

Superior performance of LS labelling: We
use two datasets—similarity matrices∗ from
69.2
53.3
[11] and RBF kernel10 as similarity matrices for
the UCI datasets†[8]. We built an unweighted
64.3
63.1
graph by thresholding the similarity matrices
about the mean. Let L = D − A. For the reg-
68.5
71.8
ularized formulation (1)  with 10% of labelled
nodes observable  we test four types of kernel
matrices—LS labelling(LS-lab)  (λ1I + L)−1 (Un-Lap)  (λ2I + D−1/2LD−1/2)−1 (N-Lap) and
K-Scaling (KS-Lap) [4]. We choose the parameters λ  λ1 and λ2 by cross validation. Table 1
summarizes the results. Each entry is accuracy in % w.r.t. 0-1 loss  and the results were averaged
over 100 iterations. Since we are thresholding by mean  the graphs have high connectivity. Thus 
from Corollary 4.3  the function class associated with LS labellingis rich and expressive  and hence
it outperforms previously proposed regularizers.

66.7
52.9
60.5
59.5
68.6
71.2

68.1
54.1
61.2
64.0
68.3
69.3

transduction

Table 2: Multiple Graphs Transduction.
Each entry is accuracy in %.

across Multiple-views:
Graph
Learning on mutli-view data has been of recent
interest [18]. Following a similar line of attack  we
pose the problem of classiﬁcation on multi-view
data as multiple graph transduction. We investigate
the recently launched Google dataset [17]  which
contains multiple views of video game YouTube
videos  consisting of 13 feature types of auditory
(Aud)  visual (Vis) and textual (Txt) description.
Each video is labelled one of 30 classes. For each
of the views we construct similarity matrices using
cosine distance and threshold about the mean to obtain
unweighted graphs. We considered 20% of the data to be labelled. We show results on pair-wise
classiﬁcation for the ﬁrst four classes. As a natural way of combining graphs  we compared our
algorithm (6) (MV) with union (Unn)  intersection (Int) and majority (Maj)11 of graphs. We used
LS labelling as the graph-kernel and (2) was used to solve single graph transduction. Table 2
summarizes the results  averaged over 20 iterations. We also state top accuracy in each of the views
for comparison. As expected from our analysis in Theorem 7.1  we observe that combining multiple
graphs signiﬁcantly improves classiﬁcation accuracy.

Graph 1vs2 1vs3 1vs4 2vs3 2vs4 3vs4
62.8 64.8 68.3 59.3 50.8 61.5
Aud
Vis
68.9 65.6 68.9 69.1 70.3 75.1
68.7 59.2 64.8 64.6 60.9 65.4
Txt
69.7 60.3 52.7 62.7 67.4 62.5
Unn
72.7 75.2 80.5 65.4 62.6 77.4
Maj
Int
80.6 83.6 86.0 90.9 75.3 91.8
MV 98.9 93.4 95.6 97.7 87.7 98.8

9 Conclusion

For the problem of graph transduction  we show that there exists orthonormal representations that
are consistent over a large class of graphs. We also note that the Laplacian inverse regularizer
is suboptimal on graphs with high connectivity  and alternatively show that LS labellingis not only
consistent  but also exhibits high Rademacher complexity on a large class of graphs. Using our anal-
ysis  we also develop a sound statistical understanding of the improved classiﬁcation performance
in combining multiple graphs.

9Relevant resources at: mllab.csa.iisc.ernet.in/rakeshs/nips14
10The (i  j)th entry of an RBF kernel is given by exp
11Majority graph is a graph where an edge (i  j) is present  if a majority of the graphs have the edge (i  j).

. We set σ to the mean distance.

2σ2

(cid:16) −(cid:107)xi−xj(cid:107)2

(cid:17)

8

References
[1] V. Jethava  A. Martinsson  C. Bhattacharyya  and D. P. Dubhashi The Lov´asz ϑ function  SVMs and

ﬁnding large dense subgraphs. Neural Information Processing Systems   pages 1169–1177  2012.

[2] L. Lov´asz. On the shannon capacity of a graph. IEEE Transactions on Information Theory  25(1):1–7 

1979.

[3] R. El-Yaniv and D. Pechyony. Transductive Rademacher complexity and its applications. In Learning

Theory  pages 151–171. Springer  2007.

[4] R. Ando  and T. Zhang. Learning on graph with Laplacian regularization. Neural Information Processing

Systems   2007.

[5] R. Johnson and T. Zhang. On the effectiveness of Laplacian normalization for graph semi-supervised

learning. Journal of Machine Learning Research  8(4)  2007.

[6] R. El-Yaniv and D. Pechyony. Transductive Rademacher complexity and its applications. Journal of

Machine Learning Research  35(1):193  2009

[7] A. Argyriou  M. Herbster  and M. Pontil. Combining graph Laplacians for semi-supervised learning.

Neural Information Processing Systems   2005.

[8] A. Asuncion  and D. Newman. UCI machine learning repository. 2000.
[9] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research  3:463–482  2003.

[10] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. International

Conference on Machine Learning  pages 19–26. Morgan Kaufmann Publishers Inc.  2001.

[11] Y. Chen  M. Gupta  and B. Recht. Learning kernels from indeﬁnite similarities. International Conference

on Machine Learning  pages 145–152. ACM  2009.

[12] C. Cortes  M. Mohri  D. Pechyony and A. Rastogi. Stability Analysis and Learning Bounds for Trans-

ductive Regression Algorithms. arXiv preprint arXiv:0904.0814  2009.

[13] A. Coja-Oghlan. The Lov´asz number of random graphs. Combinatorics  Probability and Computing  14

(04):439–465  2005.

[14] T. H. Cormen  C. E. Leiserson  R. L. Rivest  and C. Stein. Introduction to Algorithms. MIT Press  3rd

edition  2009.

[15] M. Szummer and T. Jaakkola Partially labeled classiﬁcation with Markov random walks. Neural Infor-

mation Processing Systems   14:945–952  2002.

[16] C. J. Luz and A. Schrijver. A convex quadratic characterization of the Lov´asz theta number. SIAM Journal

on Discrete Mathematics  19(2):382–387  2005.

[17] O. Madani  M. Georg  and D. A. Ross. On using nearly-independent feature families for high precision

and conﬁdence. Machine Learning  92:457–477  2013.

[18] W. Tang  Z. Lu  and I. S. Dhillon. Clustering with multiple graphs. International Conference on Data

Mining  pages 1016–1021. IEEE  2009.

[19] K. Tsuda  H. Shin  and B. Sch¨olkopf. Fast protein classiﬁcation with multiple networks. Bioinformatics 

21(suppl 2):ii59–ii65  2005.

[20] V. N. Vapnik and A. J. Chervonenkis. On the uniform convergence of relative frequencies of events to

their probabilities Theory of Probability & Its Applications  16(2):264–280. SIAM  1971.

[21] D. J. A. Welsh and M. B. Powell. An upper bound for the chromatic number of a graph and its application

to timetabling problems. The Computer Journal  10(1):85–86  1967.

[22] T. Zhang and R. Ando. Analysis of spectral kernel design based semi-supervised learning. Neural Infor-

mation Processing Systems   18:1601  2006.

[23] D. Zhou  O. Bousquet  T. N. Lal  J. Weston  and B. Sch¨olkopf. Learning with local and global consistency.

Neural Information Processing Systems   16(16):321–328  2008.

9

,Rakesh Shivanna
Chiranjib Bhattacharyya
Guy Uziel
Ran El-Yaniv
Dimitris Kalimeris
Gal Kaplun
Preetum Nakkiran
Benjamin Edelman
Tristan Yang
Boaz Barak
Haofeng Zhang