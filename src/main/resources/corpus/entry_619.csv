2017,Asynchronous Coordinate Descent under More Realistic Assumptions,Asynchronous-parallel algorithms have the potential to vastly speed up algorithms by eliminating costly synchronization. However  our understanding of these algorithms is limited because the current convergence theory of asynchronous block coordinate descent algorithms is based on somewhat unrealistic assumptions. In particular  the age of the shared optimization variables being used to update blocks is assumed to be independent of the block being updated. Additionally  it is assumed that the updates are applied to randomly chosen blocks.  In this paper  we argue that these assumptions either fail to hold or will imply less efficient implementations. We then prove the convergence of asynchronous-parallel block coordinate descent under more realistic assumptions  in particular  always without the independence assumption. The analysis permits both the deterministic (essentially) cyclic and random rules for block choices. Because a bound on the asynchronous delays may or may not be available  we establish convergence for both bounded delays and unbounded delays. The analysis also covers nonconvex  weakly convex  and strongly convex functions. The convergence theory involves a Lyapunov function that directly incorporates both objective progress and delays. A continuous-time ODE is provided to motivate the construction at a high level.,Asynchronous Coordinate Descent under More

Realistic Assumption∗

Tao Sun

National University of Defense Technology

Changsha  Hunan 410073  China

nudtsuntao@163.com

Robert Hannah

University of California  Los Angeles

Los Angeles  CA 90095  USA

RobertHannah89@math.ucla.edu

Wotao Yin

University of California  Los Angeles

Los Angeles  CA 90095  USA
wotaoyin@math.ucla.edu

Abstract

Asynchronous-parallel algorithms have the potential to vastly speed up algorithms
by eliminating costly synchronization. However  our understanding of these algo-
rithms is limited because the current convergence theory of asynchronous block
coordinate descent algorithms is based on somewhat unrealistic assumptions. In
particular  the age of the shared optimization variables being used to update blocks
is assumed to be independent of the block being updated. Additionally  it is
assumed that the updates are applied to randomly chosen blocks.
In this paper  we argue that these assumptions either fail to hold or will imply less
efﬁcient implementations. We then prove the convergence of asynchronous-parallel
block coordinate descent under more realistic assumptions  in particular  always
without the independence assumption. The analysis permits both the deterministic
(essentially) cyclic and random rules for block choices. Because a bound on the
asynchronous delays may or may not be available  we establish convergence for
both bounded delays and unbounded delays. The analysis also covers nonconvex 
weakly convex  and strongly convex functions. The convergence theory involves a
Lyapunov function that directly incorporates both objective progress and delays. A
continuous-time ODE is provided to motivate the construction at a high level.

1

Introduction

In this paper  we consider the asynchronous-parallel block coordinate descent (async-BCD) algorithm
for solving the unconstrained minimization problem

min
x∈RN

f (x) = f (x1  . . .   xN ) 

(1)
where f is a differentiable function and ∇f is L-Lipschitz continuous. Async-BCD [14  13  16]
has virtually the same implementation as regular BCD. The difference is that the threads doing the
parallel computation do not wait for all others to ﬁnish and share their updates before starting the next
iteration  but merely continue to update with the most recent solution-vector information available2.
∗The work is supported in part by the National Key R&D Program of China 2017YFB0202902  China

Scholarship Council  NSF DMS-1720237  and ONR N000141712162

2Additionally  the step size needs to be modiﬁed to ensure convergence results hold. However in practice

traditional step sizes appear to still allow convergence  barring extreme circumstances.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

In traditional algorithms  latency  bandwidth limits  and unexpected drains on resources  that delay
the update of even a single thread will cause the entire system to wait. By eliminating this costly idle
time  asynchronous algorithms can be much faster than traditional ones.
In async-BCD  each agent continually updates the solution vector  one block at a time  leaving
all other blocks unchanged. Each block update is a read-compute-update cycle. It begins with an
agent reading x from shared memory or a parameter server  and saving it in a local cache as ˆx. The
agent then computes − 1
L∇if (ˆx)  a block partial gradient3. The ﬁnal step of the cycle depends on
the parallel system setup. In a shared memory setup  the agent reads block xi again and writes
xi − γk
L ∇if (ˆx) to xi (where γk is the step size). In parameter-server setup  the agent can send
L∇if (ˆx) and let the server update xi. Other setups are possible  too. The iteration counter k
− 1
increments upon the completion of any block update  and the updating block is denoted as ik.
Many iterations may occur between the time a computing node reads the solution vector ˆx into
memory  and the time that the node’s corresponding update is applied to the shared solution vector.
Because of this  the iteration of asyn-BCD is  therefore  modeled [14] as

= xk

xk+1
ik

(2)
j for all non-updating blocks j (cid:54)= ik.
where ˆxk is a potentially outdated version of xk  and xk+1
The convergence behavior of this algorithm depends on the sequence of updated blocks ik  the step
size sequence γk  as well as the ages of the blocks of ˆxk relative to xk. We deﬁne the delay vector
(cid:126)j(k) = (j(k  1)  j(k  2)  . . .   j(k  N )) ∈ ZN   which represents the how outdated each of the blocks
are. Speciﬁcally  we have deﬁne:

∇ik f (ˆxk) 
j = xk

ik − γk
L

ˆxk = (xk−j(k 1)

  xk−j(k 2)

  . . .   xk−j(k N )

).

(3)

The k’th delay (or current delay) is j(k) = max1≤i≤N{j(k  i)}.

N

1

2

1.1 Dependence between delays and blocks

In previous analyses [13  14  16  9]  it is assumed that the block index ik and the delay (cid:126)j(k) were
N ∇f (ˆxk)
independent sequences. This simpliﬁes proofs  for example  giving Eik (Pik∇f (ˆxk)) = 1
when ik is chosen at random  where Pi denotes the projection to the ith block. Without independence 
(cid:126)j(k) will depend on ik  causing the distribution of ˆxk to be different for each possible ik  thus
breaking the previous equality. However  the independence assumption is unrealistic in practice.
Consider a problem where some blocks are more expensive to update than others4. Blocks that take
longer to update should have greater delays when they are updated because more other updates will
have occurred between the time that ˆx is read and when the update is applied. For the same reason 
updates on blocks assigned to slower or busier agents will generally have greater delays. Indeed
this turns out to be the case in practice. Experiments were performed on a cluster with 2 nodes 
each with 16 threads running on an Intel Xeon CPU E5-2690 v2. The algorithm was applied to the
logistic regression problem on the “news20” data set from LIBSVM  with 64 contiguous coordinate
blocks of equal size. Over 2000 epochs  blocks 0  1  and 15 had average delays of 351  115  and
28  respectively. ASync-BCD completed this over 7x faster than the corresponding synchronous
algorithm using the same computing resources  with a nearly equal decrease in objective function.
Even when blocks have balanced difﬁculty  and the computing nodes have equal computing power 
this dependence persists. We assigned 20 threads to each core  with each thread assigned to a block
of 40 coordinates with an equal numbers of nonzeros. The mean delay varied from 29 to 50 over
the threads. This may be due to the cluster scheduler or issues of data locality  which were hard to
examine. Clearly  there is strong dependence of the delays (cid:126)j(k) on the updated block ik.

1.2 Stochastic and deterministic block rules

This paper considers two different block rules: deterministic and stochastic. For the stochastic block
rule  at each update  a block is chosen from {1  2  . . .   N} uniformly at random5  for instance in
3The computing can start before the reading is completed. If ∇if (ˆx) does not require all components of ˆx 

only the required ones are read.

4say  because they are larger  bear more nonzero entries in the training set  or suffer poorer data locality.
5The distribution doesn’t have to be uniform. We need only assume that every block has a nonzero probability

of being updated. It is easy to adjust our analysis to this case.

2

[14  13  16]. For the deterministic rule  ik is an arbitrary sequence that is assumed to be essentially
cyclic. That is  there is an N(cid:48) ∈ N  N(cid:48) ≥ N  such that each block i ∈ {1  2  . . .   N} is updated at
least once in a window of N(cid:48)  that is 

For each t ∈ Z+  ∃ integer K(i  t) ∈ {tN(cid:48)  tN(cid:48) + 1  . . .   (1 + t)N(cid:48) − 1} such that iK(i t) = i.
This encompasses different kinds of cyclic rules such as ﬁxed ordering  random permutation  and
greedy selection. The stochastic block rule is easier to analyze because taking expectation will yield
a good approximation to the full gradient. It ensures the every block is updated at the speciﬁed
frequency. However  it can be expensive or even infeasible to implement for the following reasons.
In the shared memory setup  stochastic block rules require random data access  which is not only
signiﬁcantly slower than sequential data access but also cause frequent cache misses (waiting for data
being fetched from slower cache or the main memory). The cyclic rules clearly avoid these issues
since data requirements are predictable. In the parameter-server setup where workers update randomly
assigned blocks at each step  each worker must either store all the problem data necessary to update
any block (which may mean massive storage requirements) or read the required data from the server
at every step (which may mean massive bandwidth requirements). Clearly this permanently assigning
blocks to agents avoids these issues. On the other hand  the analysis of cyclic rules generally has to
consider the worst-case ordering and necessarily gives worse performance in the worst case[19]. In
practice  worst-case behavior is rare  and cyclic rules often lead to good performance [7  8  3].

1.3 Bounded and unbounded delays
We consider different delay assumptions as well. Bounded delay is when j(k) ≤ τ for some ﬁxed
τ ∈ Z+ and all iterations k; while the unbounded delay allows supk{j(k)} = +∞. Bounded and
unbounded delays can be further divided into deterministic and stochastic. Deterministic delays refer
to a sequence of delay vectors (cid:126)j(0) (cid:126)j(1) (cid:126)j(2)  . . . that is arbitrary or follows an unknown distribution
so is treated as arbitrary. Our stochastic delay results apply to distributions that decay faster than
O(k−3). Deterministic unbounded delays apply to the case when async-BCD runs on unfamiliar
hardware platforms. For convergence  we require a ﬁnite lim inf k{j(k)} and the current step size
ηk to be adaptively chosen to the current delay j(k)  which must be measured or overestimated.
Bounded delays and stochastic unbounded delays apply when the user can provide a bound or delay
distribution  respectively. The user can obtain these from previous experience or by running a pilot
test. In return  a ﬁxed step size allows convergence  and measuring the current delay is not needed.

1.4 Contributions

Our contributions are mainly convergence results for three kinds of delays: bounded  stochastic
unbounded  deterministic unbounded  that are obtained without the artiﬁcial independence between
the block index and the delay. The results are provided for nonconvex  convex  and strongly convex
functions with Lipschitz gradients. Sublinear rates and linear rates are provided  which match
the rates for the corresponding synchronous algorithms in terms of order of magnitude. Due to
space limitation  we restrict ourselves to Lipschitz differentiable functions and leave out nonsmooth
proximable functions. Like many analyses of asynchronous algorithms  our proofs are built on
the construction of Lyapunov functions. We provide a simple ODE-based (i.e.  continuous time)
construction for bounded delays to motivate the construction of the Lyapunov function in the standard
discrete setting. Our analysis brings great news to the practitioner. Roughly speaking  in a variety of
setting  even when there is no load balancing (thus the delays may depend on the block index) or
bound on the delays  convergence of async-BCD can be assured by using our provided step sizes.
Our proofs do not treat asynchronicity as noise  as many papers do6  because modelling delays in this
way appears to destroy valuable information  and leads to inequalities that are too blunt to obtain
stronger results. This is why sublinear and linear rates can be established for weak and strong convex
problems respectively  even when delays depend on the blocks and are potentially unbounded. Our
main focus was to prove new convergence results in a new setting  not to obtain the best possible
rates. Space limitations make this difﬁcult  and we leave it for future work. The main message is that
even without the independence assumption  convergence of the same order as for the corresponding
synchronous algorithm occurs. The step sizes and rates obtained may be overly pessimistic for the

6See  for example  (5.1) and (A.10) in [18]  and (14) and Lemma 4 in [6].

3

practitioner to use. In practice  we ﬁnd that using the standard synchronous step size results in
convergence  and the observed rate of convergence is extremely close to that of the synchronous
counterpart. With the independence assumption  convergence rates for asynchronous algorithms have
recently been proven to be asymptotically the same as their synchronous counterparts[10].

1.5 Related work

Our work extends the theory on asynchronous BCD algorithms such as [18  14  13]. However  their
analyses rely on the independence assumption and assume bounded delays. The bounded delay
assumption was weakened by recent papers [9  17]  but independence and random blocks were still
needed. Recently [12] proposes (in the SGD setting) an innovative “read after” sequence relabeling
technique to create the independence. However  enforcing independence in this way creates other
artiﬁcial implementation requirements that may be problematic: For instances  agents must read “all
shared data parameters and historical gradients before starting iterations”  even if not all of this is
required to compute updates. Our analysis does not require these kinds of implementation ﬁxes. Also 
our analysis also works for unbounded delays and deterministic block choices.
Related recent works also include [1  2]  which solve our problem with additional convex block-
separable terms in the objective. In the ﬁrst paper [1]  independence between blocks and delays is
avoided. However  they require a step size that diminishes at 1/k and that the sequence of iterate is
bounded (which in general may not be true). The second paper [2] relaxes independence by using
a different set of assumptions. In particular  their assumption D3 assumes that  regardless of the
previous updates  there is a universally positive chance for every block to be updated in the next step.
This Markov-type assumption relaxes the independence assumption but does not avoid it. Paper [15]
addressed this issue by decoupling the parameters read by each core from the virtual parameters on
which progress is actually deﬁned. Based on the idea of [16]  [12] addressed the dependence problem
in related work. In the convex case with a bounded delay τ  the step size in paper [14] is O(
τ 2/N ). In
their proofs  the Lyapunov function is based on (cid:107)xk − x∗(cid:107)2
2. Our analysis uses a Lyapunov function
consisting of both the function value and the sequence history  where the latter vanishes when delays
τ ) is better even
vanish. If the τ is much larger than the blocks of the problem  our step size O( 1
under our much weaker conditions. The step size bound in [16  9  4] is O(
)  which is better
than ours  but they need the independence assumption and the stochastic block rule. Recently  [20]
introduces an asynchronous primal-dual method for a problem similar to ours but having additional
afﬁne linear constraints. The analysis assumes bounded delays  random blocks  and independence.

1
1+2τ /

1

√

N

1.6 Notation
We let x∗ denote any minimizer of f. For the update in (2)  we use the following notation:

∆k := xk+1 − xk (2)

= − γk
L

∇ik f (ˆxk) 

dk := xk − ˆxk.

(4)

We also use the convention ∆k := 0 if k < 0. Let χk be the sigma algebra generated by
{x0  x1  . . .   xk}. Let E(cid:126)j(k) denote the expectation over the value of (cid:126)j(k) (when it is a random
variable). E denotes the expectation over all random variables.

2 Bounded delays

In this part  we present convergence results for the bounded delays. If the gradient of the function is
L-Lipschitz (even if the function is nonconvex)  we present the convergence for both the deterministic
and stochastic block rule. If the function is convex  we can obtain a sublinear convergence rate.
Further  if the function is restricted strongly convex  a linear convergence rate is obtained.

2.1 Continuous-time analysis

Let t be time in this subsection. Consider the ODE

(5)
If we set ˆx(t) ≡ x(t)  this system describes a gradient ﬂow  which monotoni-
where η > 0.
cally decreases f (x(t))  and its discretization is the gradient descent iteration. Indeed  we have

˙x(t) = −η∇f (ˆx(t)) 

4

dt f (x(t)) = (cid:104)∇f (x(t))  ˙x(t)(cid:105) (5)
= − 1
d
impose the bound c > 0 on the delays:

η(cid:107) ˙x(t)(cid:107)2

2. Instead  we allow delays (i.e.  ˆx(t) (cid:54)= x(t)) and

(cid:107)ˆx(t) − x(t)(cid:107)2 ≤

(cid:107) ˙x(s)(cid:107)2ds.

(cid:90) t

t−c

The delays introduce inexactness to the gradient ﬂow f (x(t)). We lose monotonicity. Indeed 

f (x(t)) = (cid:104)∇f (x(t))  ˙x(t)(cid:105) = (cid:104)∇f (ˆx(t))  ˙x(t)(cid:105) + (cid:104)∇f (x(t)) − ∇f (ˆx(t))  ˙x(t)(cid:105)

d
dt

a)≤ − 1
η

(cid:107) ˙x(t)(cid:107)2

2 + L(cid:107)x(t) − ˆx(t)(cid:107)2 · (cid:107) ˙x(t)(cid:107)2

b)≤ − 1
2η

(cid:107) ˙x(t)(cid:107)2

2 +

ηcL2

2

(cid:107) ˙x(s)(cid:107)2

2ds 

Here a) is from (5) and Lipschitzness of ∇f and b) is from the Cauchy-Schwarz inequality L(cid:107)x(t) −
ˆx(t)(cid:107)2·(cid:107) ˙x(t)(cid:107)2 ≤ (cid:107) ˙x(t)(cid:107)2
2ds. The inequalities
are generally unavoidable. Therefore  we design an energy function with both f and a weighted total
kinetic term  where γ > 0 will be decided below:

2η + ηL2(cid:107)x(t)−ˆx(t)(cid:107)2

t−c (cid:107) ˙x(s)(cid:107)2

2

2

2

(6)≤ c(cid:82) t
(cid:0)s − (t − c)(cid:1)(cid:107) ˙x(s)(cid:107)2

and (cid:107)x(t)−ˆx(t)(cid:107)2
(cid:90) t

2

t−c

2ds.

(cid:90) t

t−c

By substituting the bound on d

ξ(t) = f (x(t)) + γ

˙ξ(t) =

(cid:90) t

dt f (x(t)) in (7)  we get the time derivative:
f (x(t)) + γc(cid:107) ˙x(t)(cid:107)2
d
dt
≤ −(

t−c
2 − (γ − ηcL2
2

− γc)(cid:107) ˙x(t)(cid:107)2

(cid:107) ˙x(s)(cid:107)2

(cid:90) t

2 − γ

1
2η

2ds

)

t−c

(cid:107) ˙x(s)(cid:107)2

2ds

(6)

(7)

(8)

(9)

k−1(cid:88)

i=k−τ

Lc  there exists γ > 0 such that ( 1

2 ) > 0  so ξ(t) is
As long as η < 1
monotonically nonincreasing. Assume min f is ﬁnite. Since ξ(t) is lower bounded by min f  ξ(t)
must converge  hence ˙ξ → 0  ˙x(t) → 0 by (8). ∇f (ˆx(t)) → 0 by (5)  and ˆx(t) − x(t) → 0 by (6).
The last two results further yield ∇f (x(t)) → 0.

2η − γc) > 0 and (γ − ηcL2

2.2 Discrete analysis

The analysis for our discrete iteration (2) is based on the following Lyapunov function:

ξk := f (xk) +

L
2ε

(i − (k − τ ) + 1)(cid:107)∆i(cid:107)2
2.

(10)

for some ε > 0 determined later based on the step size and τ  the bound on the delays. The constant
ε is not an algorithm parameter. In the lemma below  we present a fundamental inequality  which
states  regardless of which block ik is updated and which ˆxk is used to compute the update in (2) 
there is a sufﬁcient descent in our Lyapunov function.

Lemma 1 (sufﬁcient descent for bounded delays) Conditions: Let f be a function (possibly non-
convex) with L-Lipschitz gradient and ﬁnite min f. Let (xk)k≥0 be generated by the async-BCD
algorithm (2)  and the delays be bounded by τ. Choose the step size γk ≡ γ = 2c
2τ +1 for arbitrary
ﬁxed 0 < c < 1. Result: we can choose ε > 0 to obtain

ξk − ξk+1 ≥ 1
2

(

1
γ

− 1
2

− τ )L · (cid:107)∆k(cid:107)2
2 

Consequently 

(cid:107)∆k(cid:107)2 = 0

lim
k

(12)

and

√
(cid:107)∆i(cid:107)2 = o(1/

k).

min
1≤i≤k

(11)

(13)

√
So we have that the smallest gradient obtained by step k decays faster than O(1/
lemma  we obtain a very general result for nonconvex problems.

k). Based on the

5

Theorem 1 Assume the conditions of Lemma 1  for f that may be nonconvex. Under the deterministic
block rule  we have

(cid:107)∇f (xk)(cid:107)2 = 0  min
1≤i≤k

lim
k

(cid:107)∇f (xk)(cid:107)2 = o(1/

√
k).

(14)

This rate has the same order of magnitude as standard gradient descent for a nonconvex function.

2.3 Stochastic block rule
Under the stochastic block rule  an agent picks a block from {1  2  ...  N} uniformly randomly at the
beginning of each update. For the kth completed update  the index of the chosen block is ik. Our
result in this subsection relies on the following assumption on the random variable ik:

Eik ((cid:107)∇ik f (xk−τ )(cid:107)2 | χk−τ ) =

1
N

(cid:107)∇if (xk−τ )(cid:107)2 

(15)

where χk = σ(x0  x1  . . .   xk (cid:126)j(0) (cid:126)j(1)  . . .  (cid:126)j(k))  k = 0  1  . . .  is the ﬁltration that represents
the information that is accumulated as our algorithm runs. It is important to note that (15) uses
xk−τ instead of ˆxk because ˆxk may depend on ik. This condition essentially states that  given the
information at iteration k − τ and earlier  ik is uniform at step k. We can relax (15) to nearly-uniform
distributions. Indeed  Theorem 2 below only needs that every block has a nonzero probability of
being updated given χk−τ   that is 

E((cid:107)∇ik f (xk−τ )(cid:107)2 | χk−τ ) ≥ ¯ε
N

(16)
for some universal ¯ε > 0. The interpretation is that though ik and ∇f (xk−τ ) are dependent  since τ
iterations have passed  ∇f (xk−τ ) has a limited inﬂuence on the distribution ik: There is a minimum
probability that each index is chosen given sufﬁcient time. For convenience and simplicity  we assume
(15) instead of (16) .
Next  we present a general result for a possibly nonconvex objective f.

(cid:107)∇if (xk−τ )(cid:107)2 

i=1

Theorem 2 Assume the conditions of Lemma 1.Under the stochastic block rule and assumption (15) 
we have:

E(cid:107)∇f (xk)(cid:107)2 = 0  min
1≤i≤k

lim
k

E(cid:107)∇f (xk)(cid:107)2

2 = o(1/k).

(17)

Fk := f (xk) + δ · k−1(cid:88)
2 − τ )] L

2.3.1 Sublinear rate under convexity
When the function f is convex  we can obtain convergence rates  for which we need a slightly
modiﬁed Lyapunov function

delays  the delays can be 0. We also deﬁne πk := E(Fk − min f )  S(k  τ ) :=(cid:80)k−1

(18)
2ε. Here  we assume τ ≥ 1. Since τ is just an upper bound of the
i=k−τ (cid:107)∆i(cid:107)2
2.

where δ := [1 + ε

γ − 1

2τ ( 1

i=k−τ

(i − (k − τ ) + 1)(cid:107)∆i(cid:107)2
2 

Lemma 2 Assume the conditions of Lemma 1. Furthermore  let f be convex and use the stochas-
tic block rule. Let xk denote the projection of xk to argmin f  assumed to exist  and let
β := max{ 8N L2

  (12N + 2)L2τ + δτ}  α := β/[ L

2 − τ )]. Then we have:

γ − 1

4τ ( 1

γ2

(πk)2 ≤ α(πk − πk+1) · (δτES(k  τ ) + E(cid:107)xk − xk(cid:107)2
2).

When τ = 1 (nearly no delay)  we can obtain β = O(N L2/γ2) and α = O(βγ/L) = O(N L/γ) 
which matches the result of standard BCD. This is used to prove sublinear convergence.

Theorem 3 Assume the conditions of Lemma 1. Furthermore  let f be convex and coercive7  and
use the stochastic block rule. Then we have:

7A function f is coercive if (cid:107)x(cid:107) → ∞ means f (x) → ∞.

E(f (xk) − min f ) = O(1/k).

(19)

(20)

N(cid:88)

i=1

N(cid:88)

6

2.3.2 Linear rate under convexity
We next consider when f is ν-restricted strongly convex8 in addition to having L-Lipschitz gradient.
That is  for x ∈ dom(f )  (cid:104)∇f (x)  x − Projargmin f (x)(cid:105) ≥ ν · dist2(x  argmin f ).
Theorem 4 Assume the conditions of Lemma 1. Furthermore  let f be ν-strongly convex  and use
the stochastic block rule. Then we have:

(cid:14)(1 +

E(f (xk) − min f ) = O(ck) 

α

min{ν 1} ) < 1 for α given in Lemma 2.

(21)

where c :=

α

min{ν 1}

3 Stochastic unbounded delay

In this part  the delay vector (cid:126)j(k) is allowed to be an unbounded random variable. Under some
mild restrictions on the distribution of (cid:126)j(k)  we can still establish convergence. In light of our
continuous-time analysis  we must develop a new bound for the last inner product in (7)  which
requires the tail distribution of j(k) to decay sufﬁciently fast. Speciﬁcally  we deﬁne a sequence of
l=i sl. Clearly 
c0 is larger than c1  c2  . . .  and we need c0 to be ﬁnite. Distributions with pj = O(j−t)  for t > 3 
and exponential-decay distributions satisfy this requirement. Deﬁne the Lyapunov function Gk
. To simplify the

ﬁxed parameters pj such that pj ≥ P(j(k) = j) ∀k  sl =(cid:80)+∞
as Gk := f (xk) + ¯δ ·(cid:80)k−1
presentation  we deﬁne R(k) :=(cid:80)k

j=l jpj  and ci :=(cid:80)+∞

2  where ¯δ := L

2ε + ( 1

i=0 ck−1−i(cid:107)∆i(cid:107)2

γ − 1

2 ) L
c0

− L√

c0

i=0 ck−iE(cid:107)∆i(cid:107)2
2.

Lemma 3 (Sufﬁcient descent for stochastic unbounded delays) Conditions: Let f be a function
(which may be nonconvex) with L-Lipschitz gradient and ﬁnite min f. Let delays be stochastic
unbounded. Use step size γk ≡ γ = 2c
√
c0+1 for arbitrary ﬁxed 0 < c < 1. Results: we can set
2
ε > 0 to ensures sufﬁcient descent:

And we have

E[Gk − Gk+1] ≥ L

c0

γ − 1
( 1

2 − √

c0)R(k).

E(cid:107)∆k(cid:107)2 = 0 and lim

k

lim
k

E(cid:107)dk(cid:107)2 = 0.

(22)

(23)

3.1 Deterministic block rule
Theorem 5 Let the conditions of Lemma 3 hold for f. Under the deterministic block rule (§1.2)  we
have:

E(cid:107)∇f (xk)(cid:107)2 = 0.

lim
k

(24)

3.2 Stochastic block rule

Recall that under the stochastic block rule  the block to update is selected uniformly at random from
{1  2  . . .   N}. The previous assumption (15)  which is made for bounded delays  need to be updated
into the following assumption for unbounded delays:

Eik ((cid:107)∇ik f (xk−j(k))(cid:107)2

2) =

1
N

(cid:107)∇if (xk−j(k))(cid:107)2
2 

(25)

where j(k) is still a variable on both sides. As argued below (15)  the uniform distribution can easily
be relaxed to a nearly-uniform distribution  but we use the former for simplicity.

Theorem 6 Let the conditions of Lemma 3 hold. Under the stochastic block rule and assumption
(25)  we have

E(cid:107)∇f (xk)(cid:107)2 = 0.

lim
k

(26)

8A condition weaker than ν-strong convexity and useful for problems involving an underdetermined linear

mapping Ax; see [11  13].

7

N(cid:88)

i=1

3.2.1 Convergence rate
When f is convex  we can derive convergence rates for φk := E(Gk − min f ).

γ2c0

Lemma 4 Let the conditions of Lemma 3 hold  and let f be convex. Let xk denote the projection of
xk to argmin f. Let β = max{ 8N L2
c0)]. Then we
have

  (12N + 2)L2 + ¯δ} and α = β/[ L

2 − √

(φk)2 ≤ ¯α(φk − φk+1) · (¯δR(k) + E(cid:107)xk − xk(cid:107)2
2) 

(27)
2} < +∞  which can be ensured
A sublinear convergence rate can be obtained if supk{E(cid:107)xk − xk(cid:107)2
by adding a projection to a large artiﬁcial box set that surely contains the solution. Here we only
present a linear convergence result.

γ − 1

2 ( 1

Theorem 7 Let the conditions of Lemma 3 hold. In addition  let f be ν-restricted strongly convex
and set step size γk ≡ γ <

c0+1   with c = ¯α max{1  1
ν }
ν } < 1. Then 
1+ ¯α max{1  1

√

2

2

E(f (xk) − min f ) = O(ck).

(28)

4 Deterministic unbounded delays

(cid:80)+∞
i=1 κi(cid:107)∆k−i(cid:107)2

(cid:80)+∞
j=i j obeys κ1 < +∞. Set Dj := 1

In this part  we consider deterministic unbounded delays  which require delay-adaptive step sizes.
Set positive sequence (i)i≥0 (which can be optimized later given actual delays) such that κi :=
. We use a new Lyapunov function
2. Let T ≥ lim inf j(k)  and let QT be the subsequence of
Hk := f (xk) + L
N where the current delay is less than T . We prove convergence on the family of subsequences
2
xk  k ∈ QT . The algorithm is independent of the choice of T . The algorithm is run as before  and
after completion  an arbitrarily large T ≥ lim inf j(k) can be chosen. Extending the result to standard
sequence convergence has proven intractable.

2 +(cid:80)j

2 + κ1

1
2i

i=1

Lemma 5 (sufﬁcient descent for unbounded deterministic delays) Conditions: Let f be a func-
tion (which may be nonconvex) with L-Lipschitz gradient and ﬁnite min f. The delays j(k) are
deterministic and obey lim inf j(k) < ∞. Use step size γk = c/Dj(k) for arbitrary ﬁxed 0 < c < 1.
Results: We have

Hk − Hk+1 ≥ L( 1

− Dj(k))(cid:107)∆k(cid:107)2
2 

γk

(cid:107)∆k(cid:107)2 = 0.

lim
k

(29)

On any subsequence QT (for arbitrarily large T ≥ lim inf j(k))  we have:
(cid:107)∇ik f (ˆxk)(cid:107)2 = 0 

(cid:107)dk(cid:107)2 = 0 

lim

lim

(k∈QT )→∞

(k∈QT )→∞

To prove our next result  we need a new assumption: essentially cyclically semi-unbounded delay
(ECSD)  which is slightly stronger than the essentially cyclic assumption. In every window of N(cid:48)
steps  every index i is updated at least once with a delay less than B (at iteration K(i  t)). The number
B just needs to exist and can be arbitrarily large. It does not affect the step size.

Theorem 8 Let the conditions of Lemma 5 hold. For the deterministic index rule under the ECSD
assumption  for T ≥ B  we have:

lim

(k∈QT )→∞

5 Conclusion

(cid:107)∇f (xk)(cid:107)2 = 0.

(30)

In summary  we have proven a selection of convergence results for async-BCD under bounded and
unbounded delays  and stochastic and deterministic block choices. These results do not require
the independence assumption that occurs in the vast majority of other work so far. Therefore they
better model the behavior of real asynchronous solvers. These results were obtained with the use
of Lyapunov function techniques  and treating delays directly  rather than modelling them as noise.
Future work may involve obtaining a more exhaustive list of convergence results  sharper convergence
rates  and an extension to asynchronous stochastic gradient descent-like algorithms  such as SDCA.

8

References
[1] Loris Cannelli  Francisco Facchinei  Vyacheslav Kungurtsev  and Gesualdo Scutari. Asynchronous parallel
algorithms for nonconvex big-data optimization: Model and convergence. arXiv preprint arXiv:1607.04818 
2016.

[2] Loris Cannelli  Francisco Facchinei  Vyacheslav Kungurtsev  and Gesualdo Scutari. Asynchronous parallel
algorithms for nonconvex big-data optimization. Part II: Complexity and numerical results. arXiv preprint
arXiv:1701.04900  2017.

[3] Yat Tin Chow  Tianyu Wu  and Wotao Yin. Cyclic coordinate update algorithms for ﬁxed-point problems:

Analysis and applications. SIAM Journal on Scientiﬁc Computing  accepted  2017.

[4] Damek Davis. The asynchronous palm algorithm for nonsmooth nonconvex problems. arXiv preprint

arXiv:1604.00526  2016.

[5] Damek Davis and Wotao Yin. Convergence rate analysis of several splitting schemes. In Splitting Methods

in Communication  Imaging  Science  and Engineering  pages 115–163. Springer  2016.

[6] Christopher M De Sa  Ce Zhang  Kunle Olukotun  and Christopher Ré. Taming the wild: A uniﬁed analysis
of hogwild-style algorithms. In Advances in neural information processing systems  pages 2674–2682 
2015.

[7] Jerome Friedman  Trevor Hastie  Holger Höﬂing  Robert Tibshirani  et al. Pathwise coordinate optimization.

The Annals of Applied Statistics  1(2):302–332  2007.

[8] Jerome Friedman  Trevor Hastie  and Rob Tibshirani. Regularization paths for generalized linear models

via coordinate descent. Journal of statistical software  33(1):1  2010.

[9] Robert Hannah and Wotao Yin. On unbounded delays in asynchronous parallel ﬁxed-point algorithms.

arXiv preprint arXiv:1609.04746  2016.

[10] Robert Hannah and Wotao Yin. More Iterations per Second  Same Quality – Why Asynchronous Algorithms

may Drastically Outperform Traditional Ones. arXiv preprint arXiv:1708.05136  2017.

[11] Ming-Jun Lai and Wotao Yin. Augmented (cid:96)1 and nuclear-norm models with a globally linearly convergent

algorithm. SIAM Journal on Imaging Sciences  6(2):1059–1091  2013.

[12] Rémi Leblond  Fabian Pedregosa  and Simon Lacoste-Julien. ASAGA: Asynchronous Parallel SAGA. In
Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics  pages 46–54 
2017.

[13] J. Liu and S. Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence properties.

SIAM Journal on Optimization  25(1):351–376  2015.

[14] Ji Liu  Stephen J. Wright  Christopher Ré  Victor Bittorf  and Srikrishna Sridhar. An asynchronous parallel

stochastic coordinate descent algorithm. J. Mach. Learn. Res.  16(1):285–322  2015.

[15] Horia Mania  Xinghao Pan  Dimitris Papailiopoulos  Benjamin Recht  Kannan Ramchandran  and
Michael I Jordan. Perturbed iterate analysis for asynchronous stochastic optimization. arXiv preprint
arXiv:1507.06970  2015.

[16] Zhimin Peng  Yangyang Xu  Ming Yan  and Wotao Yin. Arock: an algorithmic framework for asynchronous

parallel coordinate updates. SIAM Journal on Scientiﬁc Computing  38(5):A2851–A2879  2016.

[17] Zhimin Peng  Yangyang Xu  Ming Yan  and Wotao Yin. On the convergence of asynchronous parallel

iteration with arbitrary delays. arXiv preprint arXiv:1612:04425  2016.

[18] Benjamin Recht  Christopher Re  Stephen Wright  and Feng Niu. Hogwild!: A lock-free approach to
parallelizing stochastic gradient descent. In J. Shawe-Taylor  R. S. Zemel  P. L. Bartlett  F. Pereira  and
K. Q. Weinberger  editors  Advances in Neural Information Processing Systems 24  pages 693–701. Curran
Associates  Inc.  2011.

[19] Ruoyu Sun and Yinyu Ye. Worst-case Complexity of Cyclic Coordinate Descent: O(n2) Gap with

Randomized Version. arXiv preprint arXiv:1604.07130  2017.

[20] Yangyang Xu. Asynchronous parallel primal-dual block update methods. arXiv preprint arXiv:1705.06391 

2017.

9

,Tao Sun
Robert Hannah
Wotao Yin