2013,Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization,Majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function. Because of its simplicity and its wide applicability  this principle has been very popular in statistics and in signal processing. In this paper  we intend to make this principle scalable. We introduce a stochastic majorization-minimization scheme which is able to deal with large-scale or possibly infinite data sets. When applied to convex optimization problems under suitable assumptions  we show that it achieves an expected convergence rate of $O(1/\sqrt{n})$ after~$n$ iterations  and of $O(1/n)$ for strongly convex functions. Equally important  our scheme almost surely converges to stationary points for a large class of non-convex problems. We develop several efficient algorithms based on our framework. First  we propose a new stochastic proximal gradient method  which experimentally matches state-of-the-art solvers for large-scale $\ell_1$-logistic regression. Second  we develop an online DC programming algorithm for non-convex sparse estimation. Finally  we demonstrate the effectiveness of our technique for solving large-scale structured matrix factorization problems.,Stochastic Majorization-Minimization Algorithms

for Large-Scale Optimization

Julien Mairal

LEAR Project-Team - INRIA Grenoble

julien.mairal@inria.fr

Abstract

Majorization-minimization algorithms consist of iteratively minimizing a majoriz-
ing surrogate of an objective function. Because of its simplicity and its wide
applicability  this principle has been very popular in statistics and in signal pro-
cessing. In this paper  we intend to make this principle scalable. We introduce
a stochastic majorization-minimization scheme which is able to deal with large-
scale or possibly inﬁnite data sets. When applied to convex optimization problems
under suitable assumptions  we show that it achieves an expected convergence
rate of O(1/√n) after n iterations  and of O(1/n) for strongly convex functions.
Equally important  our scheme almost surely converges to stationary points for
a large class of non-convex problems. We develop several efﬁcient algorithms
based on our framework. First  we propose a new stochastic proximal gradient
method  which experimentally matches state-of-the-art solvers for large-scale ℓ1-
logistic regression. Second  we develop an online DC programming algorithm for
non-convex sparse estimation. Finally  we demonstrate the effectiveness of our
approach for solving large-scale structured matrix factorization problems.

1

Introduction

Majorization-minimization [15] is a simple optimization principle for minimizing an objective func-
tion. It consists of iteratively minimizing a surrogate that upper-bounds the objective  thus monoton-
ically driving the objective function value downhill. This idea is used in many existing procedures.
For instance  the expectation-maximization (EM) algorithm (see [5  21]) builds a surrogate for a
likelihood model by using Jensen’s inequality. Other approaches can also be interpreted under the
majorization-minimization point of view  such as DC programming [8]  where “DC” stands for dif-
ference of convex functions  variational Bayes techniques [28]  or proximal algorithms [1  23  29].

In this paper  we propose a stochastic majorization-minimization algorithm  which is is suitable for
solving large-scale problems arising in machine learning and signal processing. More precisely  we
address the minimization of an expected cost—that is  an objective function that can be represented
by an expectation over a data distribution. For such objectives  online techniques based on stochastic
approximations have proven to be particularly efﬁcient  and have drawn a lot of attraction in machine
learning  statistics  and optimization [3–6  9–12  14  16  17  19  22  24–26  30].

Our scheme follows this line of research. It consists of iteratively building a surrogate of the expected
cost when only a single data point is observed at each iteration; this data point is used to update the
surrogate  which in turn is minimized to obtain a new estimate. Some previous works are closely
related to this scheme: the online EM algorithm for latent data models [5  21] and the online matrix
factorization technique of [19] involve for instance surrogate functions updated in a similar fashion.
Compared to these two approaches  our method is targeted to more general optimization problems.

Another related work is the incremental majorization-minimization algorithm of [18] for ﬁnite train-
ing sets; it was indeed shown to be efﬁcient for solving machine learning problems where storing

1

dense information about the past iterates can be afforded. Concretely  this incremental scheme re-
quires to store O(pn) values  where p is the variable size  and n is the size of the training set.1
This issue was the main motivation for us for proposing a stochastic scheme with a memory load
independent of n  thus allowing us to possibly deal with inﬁnite data sets  or a huge variable size p.

We study the convergence properties of our algorithm when the surrogates are strongly convex and
chosen among the class of ﬁrst-order surrogate functions introduced in [18]  which consist of ap-
proximating the possibly non-smooth objective up to a smooth error. When the objective is convex 
we obtain expected convergence rates that are asymptotically optimal  or close to optimal [14  22].
More precisely  the convergence rate is of order O(1/√n) in a ﬁnite horizon setting  and O(1/n) for
a strongly convex objective in an inﬁnite horizon setting. Our second analysis shows that for non-
convex problems  our method almost surely converges to a set of stationary points under suitable
assumptions. We believe that this result is equally valuable as convergence rates for convex opti-
mization. To the best of our knowledge  the literature on stochastic non-convex optimization is rather
scarce  and we are only aware of convergence results in more restricted settings than ours—see for
instance [3] for the stochastic gradient descent algorithm  [5] for online EM  [19] for online matrix
factorization  or [9]  which provides stronger guarantees  but for unconstrained smooth problems.

We develop several efﬁcient algorithms based on our framework. The ﬁrst one is a new stochastic
proximal gradient method for composite or constrained optimization. This algorithm is related to a
long series of work in the convex optimization literature [6  10  12  14  16  22  25  30]  and we demon-
strate that it performs as well as state-of-the-art solvers for large-scale ℓ1-logistic regression [7]. The
second one is an online DC programming technique  which we demonstrate to be better than batch
alternatives for large-scale non-convex sparse estimation [8]. Finally  we show that our scheme can
address efﬁciently structured sparse matrix factorization problems in an online fashion  and offers
new possibilities to [13  19] such as the use of various loss or regularization functions.

This paper is organized as follows: Section 2 introduces ﬁrst-order surrogate functions for batch
optimization; Section 3 is devoted to our stochastic approach and its convergence analysis; Section 4
presents several applications and numerical experiments  and Section 5 concludes the paper.

2 Optimization with First-Order Surrogate Functions

Throughout the paper  we are interested in the minimization of a continuous function f : Rp → R:

min
θ∈Θ

f (θ) 

(1)

where Θ ⊆ Rp is a convex set. The majorization-minimization principle consists of computing a ma-
jorizing surrogate gn of f at iteration n and updating the current estimate by θn ∈ arg minθ∈Θ gn(θ).
The success of such a scheme depends on how well the surrogates approximate f . In this paper  we
consider a particular class of surrogate functions introduced in [18] and deﬁned as follows:

Deﬁnition 2.1 (Strongly Convex First-Order Surrogate Functions).
Let κ be in Θ. We denote by SL ρ(f  κ) the set of ρ-strongly convex functions g such that g ≥ f  
g(κ) = f (κ)  the approximation error g − f is differentiable  and the gradient ∇(g − f ) is L-
Lipschitz continuous. We call the functions g in SL ρ(f  κ) “ﬁrst-order surrogate functions”.
Among the ﬁrst-order surrogate functions presented in [18]  we should mention the following ones:
• Lipschitz Gradient Surrogates.
When f is differentiable and ∇f is L-Lipschitz  f admits the following surrogate g in S2L L(f  κ):

g : θ 7→ f (κ) + ∇f (κ)⊤(θ − κ) +

L
2 kθ − κk2
2.

When f is convex  g is in SL L(f  κ)  and when f is µ-strongly convex  g is in SL−µ L(f  κ).
Minimizing g amounts to performing a classical classical gradient descent step θ ← κ − 1
L∇f (κ).
• Proximal Gradient Surrogates.
Assume that f splits into f = f1 + f2  where f1 is differentiable  ∇f1 is L-Lipschitz  and f2 is
1To alleviate this issue  it is possible to cut the dataset into η mini-batches  reducing the memory load to

O(pη)  which remains cumbersome when p is very large.

2

convex. Then  the function g below is in S2L L(f  κ):

g : θ 7→ f1(κ) + ∇f1(κ)⊤(θ − κ) +

L
2 kθ − κk2

2 + f2(θ).

When f1 is convex  g is in SL L(f  κ). If f1 is µ-strongly convex  g is in SL−µ L(f  κ). Minimizing g
amounts to a proximal gradient step [1  23  29]: θ ← arg minθ
L f2(θ).
• DC Programming Surrogates.
Assume that f = f1 + f2  where f2 is concave and differentiable  ∇f2 is L2-Lipschitz  and g1 is in
SL1 ρ1(f1  κ)  Then  the following function g is a surrogate in SL1+L2 ρ1 (f  κ):

L∇f1(κ) − θk2

2kκ − 1

2 + 1

1

When f1 is convex  f1 + f2 is a difference of convex functions  leading to a DC program [8].

g : θ 7→ f1(θ) + f2(κ) + ∇f2(κ)⊤(θ − κ).

With the deﬁnition of ﬁrst-order surrogates and a basic “batch” algorithm in hand  we now introduce
our main contribution: a stochastic scheme for solving large-scale problems.

3 Stochastic Optimization

As pointed out in [4]  one is usually not interested in the minimization of an empirical cost on a
ﬁnite training set  but instead in minimizing an expected cost. Thus  we assume from now on that f
has the form of an expectation:

(2)
where x from some set X represents a data point  which is drawn according to some unknown
distribution  and ℓ is a continuous loss function. As often done in the literature [22]  we assume that
the expectations are well deﬁned and ﬁnite valued; we also assume that f is bounded below.

θ∈Θhf (θ)   Ex[ℓ(x  θ)]i  

min

We present our approach for tackling (2) in Algorithm 1. At each iteration  we draw a training
point xn  assuming that these points are i.i.d. samples from the data distribution. Note that in
practice  since it is often difﬁcult to obtain true i.i.d. samples  the points xn are computed by
cycling on a randomly permuted training set [4]. Then  we choose a surrogate gn for the function
θ 7→ ℓ(xn  θ)  and we use it to update a function ¯gn that behaves as an approximate surrogate for the
expected cost f . The function ¯gn is in fact a weighted average of previously computed surrogates 
and involves a sequence of weights (wn)n≥1 that will be discussed later. Then  we minimize ¯gn  and
obtain a new estimate θn. For convex problems  we also propose to use averaging schemes  denoted
by “option 2” and “option 3” in Alg. 1. Averaging is a classical technique for improving convergence
rates in convex optimization [10  22] for reasons that are clear in the convergence proofs.

Algorithm 1 Stochastic Majorization-Minimization Scheme
input θ0 ∈ Θ (initial estimate); N (number of iterations); (wn)n≥1  weights in (0  1];
1: initialize the approximate surrogate: ¯g0 : θ 7→ ρ
2: for n = 1  . . .   N do
3:
4:
5:
6:

draw a training point xn; deﬁne fn : θ 7→ ℓ(xn  θ);
choose a surrogate function gn in SL ρ(fn  θn−1);
update the approximate surrogate: ¯gn = (1 − wn)¯gn−1 + wngn;
update the current estimate:

2; ¯θ0 = θ0; ˆθ0 = θ0;

2kθ − θ0k2

θn ∈ arg min

θ∈Θ

¯gn(θ);

7:

8:

for option 2  update the averaged iterate: ˆθn   (1 − wn+1)ˆθn−1 + wn+1θn;
for option 3  update the averaged iterate: ¯θn   (1−wn+1)¯θn−1+wn+1θn

;

Pn+1

k=1 wk

9: end for
output (option 1): θN (current estimate  no averaging);
output (option 2): ¯θN (ﬁrst averaging scheme);
output (option 3): ˆθN (second averaging scheme).

We remark that Algorithm 1 is only practical when the functions ¯gn can be parameterized with a
small number of variables  and when they can be easily minimized over Θ. Concrete examples are
discussed in Section 4. Before that  we proceed with the convergence analysis.

3

3.1 Convergence Analysis - Convex Case

First  We study the case of convex functions fn : θ 7→ ℓ(θ  xn)  and make the following assumption:
(A) for all θ in Θ  the functions fn are R-Lipschitz continuous. Note that for convex functions 

this is equivalent to saying that subgradients of fn are uniformly bounded by R.

Assumption (A) is classical in the stochastic optimization literature [22]. Our ﬁrst result shows that
with the averaging scheme corresponding to “option 2” in Alg. 1  we obtain an expected convergence
rate that makes explicit the role of the weight sequence (wn)n≥1.
Proposition 3.1 (Convergence Rate).
When the functions fn are convex  under assumption (A)  and when ρ = L  we have

for all n ≥ 1 
where ¯θn−1 is deﬁned in Algorithm 1  θ⋆ is a minimizer of f on Θ  and f ⋆   f (θ⋆).

E[f (¯θn−1) − f ⋆] ≤

2 + R2
Lkθ⋆ − θ0k2
2Pn
k=1 wk

L Pn

k=1 w2

k

(3)

Such a rate is similar to the one of stochastic gradient descent with averaging  see [22] for example.
Note that the constraint ρ = L here is compatible with the proximal gradient surrogate.
From Proposition 3.1  it is easy to obtain a O(1/√n) bound for a ﬁnite horizon—that is  when the
total number of iterations n is known in advance. When n is ﬁxed  such a bound can indeed be
obtained by plugging constant weights wk = γ/√n for all k ≤ n in Eq. (3). Note that the upper-
bound O(1/√n) cannot be improved in general without making further assumptions on the objective
function [22]. The next corollary shows that in an inﬁnite horizon setting and with decreasing
weights  we lose a logarithmic factor compared to an optimal convergence rate [14 22] of O(1/√n).
Corollary 3.1 (Convergence Rate - Inﬁnite Horizon - Decreasing Weights).
Let us make the same assumptions as in Proposition 3.1 and choose the weights wn = γ/√n. Then 

E[f (¯θn−1) − f ⋆] ≤

Lkθ⋆ − θ0k2

2γ√n

2

+

R2γ(1 + log(n))

2L√n

  ∀n ≥ 2.

Our analysis suggests to use weights of the form O(1/√n). In practice  we have found that choosing
wn = √n0 + 1/√n0 + n performs well  where n0 is tuned on a subsample of the training set.

3.2 Convergence Analysis - Strongly Convex Case

In this section  we introduce an additional assumption:

(B) the functions fn are µ-strongly convex.

We show that our method achieves a rate O(1/n)  which is optimal up to a multiplicative constant
for strongly convex functions (see [14  22]).
Proposition 3.2 (Convergence Rate).
Under assumptions (A) and (B)  with ρ = L + µ. Deﬁne β   µ

ρ and wn   1+β

E[f (ˆθn−1) − f ⋆] +

ρ
2

E[kθ⋆ − θnk2

2] ≤ max(cid:18) 2R2

µ

2(cid:19)
  ρkθ⋆ − θ0k2

1+βn . Then 
1

βn + 1

for all n ≥ 1 

where ˆθn is deﬁned in Algorithm 1  when choosing the averaging scheme called “option 3”.

The averaging scheme is slightly different than in the previous section and the weights decrease
at a different speed. Again  this rate applies to the proximal gradient surrogates  which satisfy the
constraint ρ = L + µ. In the next section  we analyze our scheme in a non-convex setting.

3.3 Convergence Analysis - Non-Convex Case

Convergence results for non-convex problems are by nature weak  and difﬁcult to obtain for stochas-
tic optimization [4  9]. In such a context  proving convergence to a global (or local) minimum is out
of reach  and classical analyses study instead asymptotic stationary point conditions  which involve
directional derivatives (see [2  18]). Concretely  we introduce the following assumptions:

4

(C) Θ and the support X of the data are compact;
(D) The functions fn are uniformly bounded by some constant M ;
n√n < +∞;
(E) The weights wn are non-increasing  w1 = 1  Pn≥1 wn = +∞  and Pn≥1 w2
(F) The directional derivatives ∇fn(θ  θ′ − θ)  and ∇f (θ  θ′ − θ) exist for all θ and θ′ in Θ.

Assumptions (C) and (D) combined with (A) are useful because they allow us to use some uniform
convergence results from the theory of empirical processes [27]. In a nutshell  these assumptions
ensure that the function class {x 7→ ℓ(x  θ) : θ ∈ Θ} is “simple enough”  such that a uniform law
of large numbers applies. The assumption (E) is more technical: it resembles classical conditions
used for proving the convergence of stochastic gradient descent algorithms  usually stating that the
weights wn should be the summand of a diverging sum while the sum of w2
n should be ﬁnite; the

useful to characterize the stationary points of the problem. A classical necessary ﬁrst-order condi-

n√n < +∞ is slightly stronger. Finally  (F) is a mild assumption  which is
constraint Pn≥1 w2
tion [2] for θ to be a local minimum of f is indeed to have ∇f (θ  θ′− θ) non-negative for all θ′ in Θ.
We call such points θ the stationary points of the function f . The next proposition is a generalization
of a convergence result obtained in [19] in the context of sparse matrix factorization.

Proposition 3.3 (Non-Convex Analysis - Almost Sure Convergence).
Under assumptions (A)  (C)  (D)  (E)  (f (θn))n≥0 converges with probability one. Under assump-
tion (F)  we also have that

lim inf
n→+∞

inf
θ∈Θ

≥ 0 

∇ ¯fn(θn  θ − θn)

kθ − θnk2

where the function ¯fn is a weighted empirical risk recursively deﬁned as ¯fn = (1−wn) ¯fn−1+wnfn.
It can be shown that ¯fn uniformly converges to f .

Even though ¯fn converges uniformly to the expected cost f   Proposition 3.3 does not imply that the
limit points of (θn)n≥1 are stationary points of f . We obtain such a guarantee when the surrogates
that are parameterized  an assumption always satisﬁed when Algorithm 1 is used in practice.

Proposition 3.4 (Non-Convex Analysis - Parameterized Surrogates).
Let us make the same assumptions as in Proposition 3.3  and let us assume that the functions ¯gn are
parameterized by some variables κn living in a compact set K of Rd. In other words  ¯gn can be
written as gκn   with κn in K. Suppose there exists a constant K > 0 such that |gκ(θ) − gκ′ (θ)| ≤
Kkκ − κ′k2 for all θ in Θ and κ  κ′ in K. Then  every limit point θ∞ of the sequence (θn)n≥1 is a
stationary point of f —that is  for all θ in Θ 

∇f (θ∞  θ − θ∞) ≥ 0.

Finally  we show that our non-convex convergence analysis can be extended beyond ﬁrst-order sur-
rogate functions—that is  when gn does not satisfy exactly Deﬁnition 2.1. This is possible when
the objective has a particular partially separable structure  as shown in the next proposition. This
extension was motivated by the non-convex sparse estimation formulation of Section 4  where such
a structure appears.

Proposition 3.5 (Non-Convex Analysis - Partially Separable Extension).

Assume that the functions fn split into fn(θ) = f0 n(θ) +PK
k=1 fk n(γk(θ))  where the functions
γk : Rp → R are convex and R-Lipschitz  and the fk n are non-decreasing for k ≥ 1. Consider gn 0
in SL0 ρ1 (f0 n  θn−1)  and some non-decreasing functions gk n in SLk 0(fk n  γk(θn−1)). Instead
of choosing gn in SL ρ(fn  θn−1) in Alg 1  replace it by gn   θ7→ g0 n(θ)+gk n(γk(θ)).

Then  Propositions 3.3 and 3.4 still hold.

4 Applications and Experimental Validation

In this section  we introduce different applications  and provide numerical experiments. A
C++/Matlab implementation is available in the software package SPAMS [19].2 All experiments
were performed on a single core of a 2GHz Intel CPU with 64GB of RAM.

2http://spams-devel.gforge.inria.fr/.

5

4.1 Stochastic Proximal Gradient Descent Algorithm

Our ﬁrst application is a stochastic proximal gradient descent method  which we call SMM (Stochas-
tic Majorization-Minimization)  for solving problems of the form:

min
θ∈Θ

Ex[ℓ(x  θ)] + ψ(θ) 

(4)

where ψ is a convex deterministic regularization function  and the functions θ 7→ ℓ(x  θ) are dif-
ferentiable and their gradients are L-Lipschitz continuous. We can thus use the proximal gradient
surrogate presented in Section 2. Assume that a weight sequence (wn)n≥1 is chosen such that
w1 = 1. By deﬁning some other weights wi
for i < n and
wn
n

  wn  our scheme yields the update rule:

  (1− wn)wi−1

n recursively as wi
n

n

θn ← arg min

θ∈Θ

n

Xi=1

wi

n(cid:2)∇fi(θi−1)⊤θ + L

2 kθ − θi−1k2

2 + ψ(θ)(cid:3) .

(SMM)

Our algorithm is related to FOBOS [6]  to SMIDAS [25] or the truncated gradient method [16]
(when ψ is the ℓ1-norm). These three algorithms use indeed the following update rule:

θn ← arg min

θ∈Θ ∇fn(θn−1)⊤θ + 1

2ηnkθ − θn−1k2

2 + ψ(θ) 

(FOBOS)

Another related scheme is the regularized dual averaging (RDA) of [30]  which can be written as

θn ← arg min

θ∈Θ

1
n

n

Xi=1

∇fi(θi−1)⊤θ + 1

2ηnkθk2

2 + ψ(θ).

(RDA)

Compared to these approaches  our scheme includes a weighted average of previously seen gradi-
ents  and a weighted average of the past iterates. Some links can also be drawn with approaches
such as the “approximate follow the leader” algorithm of [10] and other works [12  14].

We now evaluate the performance of our method for ℓ1-logistic regression. In summary  the datasets
i=1  where the yi’s are in {−1  +1}  and the xi’s are in Rp with unit ℓ2-
consist of pairs (yi  xi)N
norm. The function ψ in (4) is the ℓ1-norm: ψ(θ)   λkθk1  and λ is a regularization parameter;
the functions fi are logistic losses: fi(θ)   log(1 + e−yi x⊤
i θ). One part of each dataset is devoted
to training  and another part to testing. We used weights of the form wn  p(n0 + 1)/(n + n0) 

where n0 is automatically adjusted at the beginning of each experiment by performing one pass on
5% of the training data. We implemented SMM in C++ and exploited the sparseness of the datasets 
such that each update has a computational complexity of the order O(s)  where s is the number of
non zeros in ∇fn(θn−1); such an implementation is non trivial but proved to be very efﬁcient.
We consider three datasets described in the table below. rcv1 and webspam are obtained from the
2008 Pascal large-scale learning challenge.3 kdd2010 is available from the LIBSVM website.4

name
rcv1
webspam
kdd2010

Ntr (train) Nte (test)
23 149
781 265
250 000
100 000
9 264 097

10 000 000

p

density (%)

size (GB)

47 152

16 091 143
28 875 157

0.161
0.023
10−4

0.95
14.95
4.8

We compare our implementation with state-of-the-art publicly available solvers: the batch algorithm
FISTA of [1] implemented in the C++ SPAMS toolbox and LIBLINEAR v1.93 [7]. LIBLINEAR
is based on a working-set algorithm  and  to the best of our knowledge  is one of the most efﬁcient
available solver for ℓ1-logistic regression with sparse datasets. Because p is large  the incremental
majorization-minimization method of [18] could not run for memory reasons. We run every method
on 1  2  3  4  5  10 and 25 epochs (passes over the training set)  for three regularization regimes 
respectively yielding a solution with approximately 100  1 000 and 10 000 non-zero coefﬁcients.
We report results for the medium regularization in Figure 1 and provide the rest as supplemental
material. FISTA is not represented in this ﬁgure since it required more than 25 epochs to achieve
reasonable values. Our conclusion is that SMM often provides a reasonable solution after one epoch 
and outperforms LIBLINEAR in the low-precision regime. For high-precision regimes  LIBLINEAR
should be preferred. Such a conclusion is often obtained when comparing batch and stochastic
algorithms [4]  but matching the performance of LIBLINEAR is very challenging.

3http://largescale.ml.tu-berlin.de.
4http://www.csie.ntu.edu.tw/˜cjlin/libsvm/.

6

t

i

i

 

 

e
S
g
n
n
a
r
T
n
o
e
v
i
t
c
e
b
O

 

j

t

i

i

 

 

e
S
g
n
n
a
r
T
n
o
e
v
i
t
c
e
b
O

 

j

t

i

 

e
S
g
n
n
a
r
T
n
o

 

i

 
e
v
i
t
c
e
b
O

j

0.35

0.3

0.25

 
0

0.2

0.15

0.1

0.05

 
0

0.2

0.15

0.1

0.05

0
 
0

 

LIBLINEAR
SMM

10

5
20
Epochs / Dataset rcv1

15

LIBLINEAR
SMM

5

10

15

20

Epochs / Dataset webspam

25

 

25

 

LIBLINEAR
SMM

10

5
20
Epochs / Dataset kddb

15

25

 

LIBLINEAR
SMM

0.35

0.3

0.25

 
100
102
Computation Time (sec) / Dataset rcv1

101

 

LIBLINEAR
SMM

0.2

0.15

0.1

0.05

i

i

t
e
S
 
g
n
n
a
r
T
 
n
o
 
e
v
i
t
c
e
b
O

j

t

i

i

 

 

e
S
g
n
n
a
r
T
n
o
e
v
i
t
c
e
b
O

 

j

 
101

102

103

Computation Time (sec) / Dataset webspam

i

i

t
e
S
 
g
n
n
a
r
T
 
n
o
 
e
v
i
t
c
e
b
O

j

 

LIBLINEAR
SMM

0.2

0.15

0.1

0.05

0
 
101
103
Computation Time (sec) / Dataset kddb

102

t

e
S
g
n

 

i
t
s
e
T
n
o

 

 
e
v
i
t
c
e
b
O

j

t

e
S
g
n

 

 

 

i
t
s
e
T
n
o
e
v
i
t
c
e
b
O

j

t

e
S
g
n

 

 

 

i
t
s
e
T
n
o
e
v
i
t
c
e
b
O

j

0.35

0.3

0.25

 
0

0.2

0.15

0.1

0.05

 
0

0.2

0.15

0.1

0.05

0
 
0

 

LIBLINEAR
SMM

10

5
20
Epochs / Dataset rcv1

15

LIBLINEAR
SMM

5

10

15

20

Epochs / Dataset webspam

25

 

25

 

LIBLINEAR
SMM

10

5
20
Epochs / Dataset kddb

15

25

Figure 1: Comparison between LIBLINEAR and SMM for the medium regularization regime.

4.2 Online DC Programming for Non-Convex Sparse Estimation

regularizer ψ : θ 7→ λPp

We now consider the same experimental setting as in the previous section  but with a non-convex
j=1 log(|θ[j]| + ε)  where θ[j] is the j-th entry in θ. A classical way for
minimizing the regularized empirical cost 1
i=1 fi(θ) + ψ(θ) is to resort to DC programming. It
consists of solving a sequence of reweighted-ℓ1 problems [8]. A current estimate θn−1 is updated
as a solution of minθ∈Θ

j=1 ηj|θ[j]|  where ηj   1/(|θn−1[j]| + ε).

N PN
i=1 fi(θ) + λPp

In contrast to this “batch” methodology  we can use our framework to address the problem online.
At iteration n of Algorithm 1  we deﬁne the function gn according to Proposition 3.5:

1

N PN

gn : θ 7→ fn(θn−1) + ∇fn(θn−1)⊤(θ − θn−1) + L

2 kθ − θn−1k2

2 + λPp

j=1

|θ[j]|

|θn−1[j]|+ε  

We compare our online DC programming algorithm against the batch one  and report the results in
Figure 2  with ε set to 0.01. We conclude that the batch reweighted-ℓ1 algorithm always converges
after 2 or 3 weight updates  but suffers from local minima issues. The stochastic algorithm exhibits
a slower convergence  but provides signiﬁcantly better solutions. Whether or not there are good
theoretical reasons for this fact remains to be investigated. Note that it would have been more
rigorous to choose a bounded set Θ  which is required by Proposition 3.5. In practice  it turns not to
be necessary for our method to work well; the iterates θn have indeed remained in a bounded set.

0

Online DC
Batch DC

t

i

 

e
S
n
a
r
T
n
o

 

 

e
v
i
t
c
e
b
O

j

-0.02

-0.04

-0.06

0.02

0.01

0

Online DC
Batch DC

 

t

e
S

 
t
s
e
T
n
o

 

 

e
v
i
t
c
e
b
O

j

-0.01

-0.02

-0.03

 

t

-4.52

-4.525

-4.53

-4.535

i

 

e
S
n
a
r
T
n
o

 

 

e
v
i
t
c
e
b
O

j

-4.54

 
0

Online DC
Batch DC

 

t

e
S

-4.37

-4.375

 
t
s
e
T
n
o

 

 

Online DC
Batch DC

 

e
v
i
t
c
e
b
O

j

-4.38

-4.385

 
0
Iterations - Epochs / Dataset rcv1

20

10

15

5

-0.04

25

 
0
Iterations - Epochs / Dataset rcv1

20

15

10

5

25

5

10

15

20

25

5

10

15

20

25

Iterations - Epochs / Dataset webspam

Iterations - Epochs / Dataset webspam

 
0

Figure 2: Comparison between batch and online DC programming  with medium regularization for
the datasets rcv1 and webspam. Additional plots are provided in the supplemental material. Note
that each iteration in the batch setting can perform several epochs (passes over training data).

4.3 Online Structured Sparse Coding

In this section  we show that we can bring new functionalities to existing matrix factorization tech-
niques [13  19]. We are given a large collection of signals (xi)N
i=1 in Rm  and we want to ﬁnd a

7

dictionary D in Rm×K that can represent these signals in a sparse way. The quality of D is mea-
sured through the loss ℓ(x  D)   minα∈RK
2  where the ℓ1-norm
can be replaced by any convex regularizer  and the squared loss by any convex smooth loss.

2 + λ1kαk1 + λ2

2 kαk2

1

2kx− Dαk2

Then  we are interested in minimizing the following expected cost:

min

D∈Rm×K

Ex [ℓ(x  D)] + ϕ(D) 

where ϕ is a regularizer for D. In the online learning approach of [19]  the only way to regularize D
is to use a constraint set  on which we need to be able to project efﬁciently; this is unfortunately not
always possible. In the matrix factorization framework of [13]  it is argued that some applications
can beneﬁt from a structured penalty ϕ  but the approach of [13] is not easily amenable to stochastic
optimization. Our approach makes it possible by using the proximal gradient surrogate

gn : D 7→ ℓ(xn  Dn−1) + Tr(cid:0)∇Dℓ(xn  Dn−1)⊤(D − Dn−1)(cid:1) + L
F + ϕ(D). (5)
It is indeed possible to show that D 7→ ℓ(xn  D) is differentiable  and its gradient is Lipschitz
continuous with a constant L that can be explicitly computed [18  19].

2 kD − Dn−1k2

We now design a proof-of-concept experiment. We consider a set of N = 400 000 whitened natural
image patches xn of size m = 20 × 20 pixels. We visualize some elements from a dictionary D
trained by SPAMS [19] on the left of Figure 3; the dictionary elements are almost sparse  but have
some residual noise among the small coefﬁcients. Following [13]  we propose to use a regularization
function ϕ encouraging neighbor pixels to be set to zero together  thus leading to a sparse structured
dictionary. We consider the collection G of all groups of variables corresponding to squares of 4
neighbor pixels in {1  . . .   m}. Then  we deﬁne ϕ(D)   γ1PK
j=1Pg∈G maxk∈g |dj[k]|+γ2kDk2
F 
where dj is the j-th column of D. The penalty ϕ is a structured sparsity-inducing penalty that en-
courages groups of variables g to be set to zero together [13]. Its proximal operator can be computed
efﬁciently [20]  and it is thus easy to use the surrogates (5). We set λ1 = 0.15 and λ2 = 0.01; after
trying a few values for γ1 and γ2 at a reasonable computational cost  we obtain dictionaries with the
desired regularization effect  as shown in Figure 3. Learning one dictionary of size K = 256 took
a few minutes when performing one pass on the training data with mini-batches of size 100. This
experiment demonstrates that our approach is more ﬂexible and general than [13] and [19]. Note that
it is possible to show that when γ2 is large enough  the iterates Dn necessarily remain in a bounded
set  and thus our convergence analysis presented in Section 3.3 applies to this experiment.

Figure 3: Left: Two visualizations of 25 elements from a larger dictionary obtained by the toolbox
SPAMS [19]; the second view ampliﬁes the small coefﬁcients. Right: the corresponding views of
the dictionary elements obtained by our approach after initialization with the dictionary on the left.

5 Conclusion

In this paper  we have introduced a stochastic majorization-minimization algorithm that gracefully
scales to millions of training samples. We have shown that it has strong theoretical properties and
some practical value in the context of machine learning. We have derived from our framework
several new algorithms  which have shown to match or outperform the state of the art for solving
large-scale convex problems  and to open up new possibilities for non-convex ones. In the future 
we would like to study surrogate functions that can exploit the curvature of the objective function 
which we believe is a crucial issue to deal with badly conditioned datasets.

Acknowledgments

This work was supported by the Gargantua project (program Mastodons - CNRS).

8

References

[1] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM J. Imaging Sci.  2(1):183–202  2009.

[2] J.M. Borwein and A.S. Lewis. Convex analysis and nonlinear optimization. Springer  2006.

[3] L. Bottou. Online algorithms and stochastic approximations. In David Saad  editor  Online Learning and

Neural Networks. 1998.

[4] L. Bottou and O. Bousquet. The trade-offs of large scale learning. In Adv. NIPS  2008.

[5] O. Capp´e and E. Moulines. On-line expectation–maximization algorithm for latent data models. J. Roy.

Stat. Soc. B  71(3):593–613  2009.

[6] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting. J. Mach.

Learn. Res.  10:2899–2934  2009.

[7] R.-E. Fan  K.-W. Chang  C.-J. Hsieh  X.-R. Wang  and C.-J. Lin. LIBLINEAR: A library for large linear

classiﬁcation. J. Mach. Learn. Res.  9:1871–1874  2008.

[8] G. Gasso  A. Rakotomamonjy  and S. Canu. Recovering sparse signals with non-convex penalties and DC

programming. IEEE T. Signal Process.  57(12):4686–4698  2009.

[9] S. Ghadimi and G. Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic programming.

Technical report  2013.

[10] E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimization. Mach.

Learn.  69(2-3):169–192  2007.

[11] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic

strongly-convex optimization. In Proc. COLT  2011.

[12] C. Hu  J. Kwok  and W. Pan. Accelerated gradient methods for stochastic optimization and online learn-

ing. In Adv. NIPS  2009.

[13] R. Jenatton  G. Obozinski  and F. Bach. Structured sparse principal component analysis. In Proc. AIS-

TATS  2010.

[14] G. Lan. An optimal method for stochastic composite optimization. Math. Program.  133:365–397  2012.

[15] K. Lange  D.R. Hunter  and I. Yang. Optimization transfer using surrogate objective functions. J. Comput.

Graph. Stat.  9(1):1–20  2000.

[16] J. Langford  L. Li  and T. Zhang. Sparse online learning via truncated gradient. J. Mach. Learn. Res. 

10:777–801  2009.

[17] N. Le Roux  M. Schmidt  and F. Bach. A stochastic gradient method with an exponential convergence

rate for ﬁnite training sets. In Adv. NIPS  2012.

[18] J. Mairal. Optimization with ﬁrst-order surrogate functions. In Proc. ICML  2013. arXiv:1305.3120.

[19] J. Mairal  F. Bach  J. Ponce  and G. Sapiro. Online learning for matrix factorization and sparse coding. J.

Mach. Learn. Res.  11:19–60  2010.

[20] J. Mairal  R. Jenatton  G. Obozinski  and F. Bach. Network ﬂow algorithms for structured sparsity. In

Adv. NIPS  2010.

[21] R.M. Neal and G.E. Hinton. A view of the EM algorithm that justiﬁes incremental  sparse  and other

variants. Learning in graphical models  89  1998.

[22] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM J. Optimiz.  19(4):1574–1609  2009.

[23] Y. Nesterov. Gradient methods for minimizing composite objective functions. Technical report  CORE

Discussion Paper  2007.

[24] S. Shalev-Schwartz and T. Zhang. Proximal stochastic dual coordinate ascent. arXiv 1211.2717v1  2012.

[25] S. Shalev-Shwartz  O. Shamir  N. Srebro  and K. Sridharan. Stochastic convex optimization. In Proc.

COLT  2009.

[26] S. Shalev-Shwartz and A. Tewari. Stochastic methods for ℓ1 regularized loss minimization.

In Proc.

ICML  2009.

[27] A. W. Van der Vaart. Asymptotic Statistics. Cambridge University Press  1998.

[28] M.J. Wainwright and M.I. Jordan. Graphical models  exponential families  and variational inference.

Found. Trends Mach. Learn.  1(1-2):1–305  2008.

[29] S. Wright  R. Nowak  and M. Figueiredo. Sparse reconstruction by separable approximation. IEEE T.

Signal Process.  57(7):2479–2493  2009.

[30] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. J. Mach.

Learn. Res.  11:2543–2596  2010.

9

,Julien Mairal
Michael Habeck