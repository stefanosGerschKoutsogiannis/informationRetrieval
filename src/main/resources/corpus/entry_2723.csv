2017,Is Input Sparsity Time Possible for Kernel Low-Rank Approximation?,Low-rank approximation is a common tool used to accelerate kernel methods: the $n \times n$ kernel matrix $K$ is approximated via a rank-$k$ matrix $\tilde K$ which can be stored in much less space and processed more quickly. In this work we study the limits of computationally efficient low-rank kernel approximation. We show that for a broad class of kernels  including the popular Gaussian and polynomial kernels  computing a relative error $k$-rank approximation to $K$ is at least as difficult as multiplying the input data matrix $A \in R^{n \times d}$ by an arbitrary matrix $C \in R^{d \times k}$. Barring a breakthrough in fast matrix multiplication  when $k$ is not too large  this requires $\Omega(nnz(A)k)$ time where $nnz(A)$ is the number of non-zeros in $A$. This lower bound matches  in many parameter regimes  recent work on subquadratic time algorithms for low-rank approximation of general kernels [MM16 MW17]  demonstrating that these algorithms are unlikely to be significantly improved  in particular to $O(nnz(A))$ input sparsity runtimes. At the same time there is hope: we show for the first time that $O(nnz(A))$ time approximation is possible for general radial basis function kernels (e.g.  the Gaussian kernel) for the closely related problem of low-rank approximation of the kernelized dataset.,Is Input Sparsity Time Possible for
Kernel Low-Rank Approximation?

Cameron Musco

MIT

cnmusco@mit.edu

David P. Woodruff

Carnegie Mellon University
dwoodruf@cs.cmu.edu

Abstract

Low-rank approximation is a common tool used to accelerate kernel methods: the
n⇥ n kernel matrix K is approximated via a rank-k matrix ˜K which can be stored
in much less space and processed more quickly. In this work we study the limits
of computationally efﬁcient low-rank kernel approximation. We show that for a
broad class of kernels  including the popular Gaussian and polynomial kernels 
computing a relative error k-rank approximation to K is at least as difﬁcult as
multiplying the input data matrix A 2 Rn⇥d by an arbitrary matrix C 2 Rd⇥k.
Barring a breakthrough in fast matrix multiplication  when k is not too large  this
requires ⌦(nnz(A)k) time where nnz(A) is the number of non-zeros in A. This
lower bound matches  in many parameter regimes  recent work on subquadratic
time algorithms for low-rank approximation of general kernels [MM16  MW17] 
demonstrating that these algorithms are unlikely to be signiﬁcantly improved  in
particular to O(nnz(A)) input sparsity runtimes. At the same time there is hope:
we show for the ﬁrst time that O(nnz(A)) time approximation is possible for
general radial basis function kernels (e.g.  the Gaussian kernel) for the closely
related problem of low-rank approximation of the kernelized dataset.

1

Introduction

i aj)q for some parameter c.

j aj = kajk2  and aT

The kernel method is a popular technique used to apply linear learning and classiﬁcation algorithms
to datasets with nonlinear structure. Given training input points a1  ...  an 2 Rd  the idea is to
replace the standard Euclidean dot product hai  aji = aT
i aj with the kernel dot product (ai  aj) 
where : Rd ⇥ Rd ! R+ is some positive semideﬁnite function. Popular kernel functions include
e.g.  the Gaussian kernel with (ai  aj) = ekaiajk2/ for some bandwidth parameter  and the
polynomial kernel of degree q with (ai  aj) = (c + aT
Throughout this work  we focus on kernels where (ai  aj) is a function of the dot products
i ai = kaik2  aT
i aj. Such functions encompass many kernels used in practice 
aT
including the Gaussian kernel  the Laplace kernel  the polynomial kernel  and the Matern kernels.
Letting F be the reproducing kernel Hilbert space associated with (· ·)  we can write (ai  aj) =
h(ai)  (aj)i where  : Rd !F is a typically non-linear feature map. We let =
[(a1)  ...  (an)]T denote the kernelized dataset  whose ith row is the kernelized datapoint (ai).
There is no requirement that  can be efﬁciently computed or stored – for example  in the case of the
Gaussian kernel  F is an inﬁnite dimensional space. Thus  kernel methods typically work with the
kernel matrix K 2 Rn⇥n with Ki j = (ai  aj). We will also sometimes denote K = { (ai  aj)}
to make it clear which kernel function it is generated by. We can equivalently write K = T . As
long as all operations of an algorithm only access  via the dot products between its rows  they can
thus be implemented using just K without explicitly computing the feature map.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Unfortunately computing K is expensive  and a bottleneck for scaling kernel methods to large
datasets. For the kernels we consider  where depends on dot products between the input points  we
must at least compute the Gram matrix AAT   requiring ⇥(n2d) time in general. Even if A is sparse 
this takes ⇥(nnz(A)n) time. Storing K then takes ⇥(n2) space  and processing it for downstream
applications like kernel ridge regression and kernel SVM can be even more expensive.

1.1 Low-rank kernel approximation
For this reason  a vast body of work studies how to efﬁciently approximate K via a low-rank sur-
rogate ˜K [SS00  AMS01  WS01  FS02  RR07  ANW14  LSS13  BJ02  DM05  ZTK08  BW09 
If ˜K is rank-k  it can be stored in factored form in O(nk) space and
CKS11  WZ13  GM13].
operated on quickly – e.g.  it can be inverted in just O(nk2) time to solve kernel ridge regression.
One possibility is to set ˜K = Kk where Kk is K’s best k-rank approximation – the projection onto
its top k eigenvectors. Kk minimizes  over all rank-k ˜K  the error kK  ˜KkF   where kMkF is
the Frobenius norm: (Pi j M 2
i j)1/2. It in fact minimizes error under any unitarily invariant norm 
e.g.  the popular spectral norm. Unfortunately  Kk is prohibitively expensive to compute  requiring
⇥(n3) time in practice  or n! in theory using fast matrix multiplication  where ! ⇡ 2.373 [LG14].
The idea of much prior work on low-rank kernel approximation is to ﬁnd ˜K which is nearly as good
as Kk  but can be computed much more quickly. Speciﬁcally  it is natural to ask for ˜K fulﬁlling the
following relative error guarantee for some parameter ✏> 0:

(1)
Other goals  such as nearly matching the spectral norm error kK  Kkk or approximating K entry-
wise have also been considered [RR07  GM13]. Of particular interest to our results is the closely
related goal of outputting an orthonormal basis Z 2 Rn⇥k satisfying for any  with T = K:

kK  ˜KkF  (1 + ✏)kK  KkkF .

k  ZZT kF  (1 + ✏)k  kkF .

(2)
(2) can be viewed as a Kernel PCA guarantee – its asks us to ﬁnd a low-rank subspace Z such that
the projection of our kernelized dataset  onto Z nearly optimally approximates this dataset. Given
Z  we can approximate K using ˜K = ZZT T ZZT = ZZT KZZT . Alternatively  letting P
be the projection onto the row span of ZZT   we can write ˜K = P T   which can be computed
efﬁciently  for example  when P is a projection onto a subset of the kernelized datapoints [MM16].

1.2 Fast algorithms for relative-error kernel approximation
Until recently  all algorithms achieving the guarantees of (1) and (2) were at least as expensive as
computing the full matrix K  which was needed to compute the low-rank approximation [GM13].
However  recent work has shown that this is not required. Avron  Nguyen  and Woodruff [ANW14]
demonstrate that for the polynomial kernel  Z satisfying (2) can be computed in O(nnz(A)q) +
n poly(3qk/✏) time for a polynomial kernel with degree q.
Musco and Musco [MM16] give a fast algorithm for any kernel  using recursive Nyström sampling 
which computes ˜K (in factored form) satisfying kK  ˜Kk    for input parameter . With
the proper setting of   it can output Z satisfying (2) (see Section C.3 of [MM16]). Computing
Z requires evaluating ˜O(k/✏) columns of the kernel matrix along with ˜O(n(k/✏)!1) additional
time for other computations. Assuming the kernel is a function of the dot products between the
input points  the kernel evaluations require ˜O(nnz(A)k/✏) time. The results of [MM16] can also be
used to compute ˜K satisfying (1) with ✏ = pn in ˜O(nnz(A)k + nk!1) time (see Appendix A of
[MW17]).
Woodruff and Musco [MW17] show that for any kernel  and for any ✏> 0  it is possible to

achieve (1) in ˜O(nnz(A)k/✏)+n poly(k/✏) time plus the time needed to compute an ˜O(pnk/✏2)⇥
˜O(pnk/✏) submatrix of K. If A has uniform row sparsity – i.e.  nnz(ai)  c nnz(A)/n for some
constant c and all i  this step can be done in ˜O(nnz(A)k/✏2.5) time. Alternatively  if d  (pnk/✏2)↵
for ↵< . 314 this can be done in ˜O(nk/✏4) = ˜O(nnz(A)k/✏4) time using fast rectangular matrix
multiplication [LG12  GU17] (assuming that there are no all zero data points so n  nnz(A).)

2

1.3 Our results

The algorithms of [MM16  MW17] make signiﬁcant progress in efﬁciently solving (1) and (2) for
general kernel matrices. They demonstrate that  surprisingly  a relative-error low-rank approxima-
tion can be computed signiﬁcantly faster than the time required to write down all of K.
A natural question is if these results can be improved. Even ignoring ✏ dependencies and typically
lower order terms  both algorithms use ⌦(nnz(A)k) time. One might hope to improve this to input
sparsity  or near input sparsity time  ˜O(nnz(A))  which is known for computing a low-rank approx-
imation of A itself [CW13]. The work of Avron et al. afﬁrms that this is possible for the kernel PCA
guarantee of (2) for degree-q polynomial kernels  for constant q. Can this result be extended to other
popular kernels  or even more general classes?

1.3.1 Lower bounds

We show that achieving the guarantee of (1) signiﬁcantly more efﬁciently than the work of [MM16 
MW17] is likely very difﬁcult. Speciﬁcally  we prove that for a wide class of kernels  the kernel
low-rank approximation problem is as hard as multiplying the input A 2 Rn⇥d by an arbitrary
C 2 Rd⇥k. We have the following result for some common kernels to which our techniques apply:
Theorem 1 (Hardness for low-rank kernel approximation). Consider any polynomial kernel
i mj)q  Gaussian kernel (mi  mj) = ekmimjk2/  or the linear ker-
 (mi  mj) = (c + mT
i mj. Assume there is an algorithm which given M 2 Rn⇥d with associated
nel (mi  mj) = mT
kernel matrix K = { (mi  mj)}  returns N 2 Rn⇥k in o(nnz(M )k) time satisfying:

kK  N N Tk2

F  kK  Kkk2

F

for some approximation factor . Then there is an o(nnz(A)k) + O(nk2) time algorithm for mul-
tiplying arbitrary integer matrices A 2 Rn⇥d  C 2 Rd⇥k.
The above applies for any approximation factor . While we work in the real RAM model  ignoring
bit complexity  as long as  = poly(n) and A  C have polynomially bounded entries  our reduction
from multiplication to low-rank approximation is achieved using matrices that can be represented
with just O(log(n + d)) bits per entry.
Theorem 1 shows that the runtime of ˜O(nnz(A)k + nk!1) for = pn achieved by [MM16]
for general kernels cannot be signiﬁcantly improved without advancing the state-of-the-art in matrix
multiplication. Currently no general algorithm is known for multiplying integer A 2 Rn⇥d  C 2
Rd⇥k in o(nnz(A)k) time  except when k  n↵ for ↵< . 314 and A is dense. In this case  AC can
be computed in O(nd) time using fast rectangular matrix multiplication [LG12  GU17].
As discussed  when A has uniform row sparsity or when d  (pnk/✏2)↵  the runtime of [MW17]
for = (1 + ✏)  ignoring ✏ dependencies and typically lower order terms  is ˜O(nnz(A)k)  which is
also nearly tight.
In recent work  Backurs et al. [BIS17] give lower bounds for a number of kernel learning problems 
including kernel PCA for the Gaussian kernel. However  their strong bound  of ⌦(n2) time  requires
very small error = exp( !(log2 n)  whereas ours applies for any relative error .
1.3.2 Improved algorithm for radial basis function kernels

In contrast to the above negative result  we demonstrate that achieving the alternative Kernel PCA
guarantee of (2) is possible in input sparsity time for any shift and rotationally invariant kernel – e.g. 
any radial basis function kernel where (xi  xj) = f (kxi  xjk). This result signiﬁcantly extends
the progress of Avron et al. [ANW14] on the polynomial kernel.
Our algorithm is based off a fast implementation of the random Fourier features method [RR07] 
which uses the fact that that the Fourier transform of any shift invariant kernel is a probability
distribution after appropriate scaling (a consequence of Bochner’s theorem). Sampling frequencies
from this distribution gives an approximation to (· ·) and consequentially the matrix K.

3

F also satisﬁes k  ZZT k2

(1  ✏)( ˜K + I)  K + I  (1 + ✏)( ˜K + I).

Fourier features sufﬁces to give ˜K = ˜ ˜T satisfying the spectral approximation guarantee:

✏2 random
We employ a new analysis of this method [AKM+17]  which shows that sampling ˜O n
If we set   k+1(K)/k  we can show that ˜ also gives a projection-cost preserving sketch
[CEM+15] for the kernelized dataset . This ensures that any Z satisfying k ˜  ZZT ˜k2
F 
(1 + ✏)k ˜  ˜kk2
F and thus achieves (2).
✏2k+1(K)⌘ random Fourier features  which naively re-
Our algorithm samples s = ˜O n

quires O(nnz(A)s) time. We show that this can be accelerated to O(nnz(A)) + poly(n  s) time  us-
ing a recent result of Kapralov et al. on fast multiplication by random Gaussian matrices [KPW16].
Our technique is analogous to the ‘Fastfood’ approach to accelerating random Fourier features using
fast Hadamard transforms [LSS13]. However  our runtime scales with nnz(A)  which can be signif-
icantly smaller than the ˜O(nd) runtime given by Fastfood when A is sparse. Our main algorithmic
result is:
Theorem 2 (Input sparsity time kernel PCA). There is an algorithm that given A 2 Rn⇥d along
with shift and rotation-invariant kernel function : Rd⇥Rd ! R+ with (x  x) = 1  outputs  with
probability 99/100  Z 2 Rn⇥k satisfying:

F  (1 + O(✏))k  kk2

✏2 = ˜O⇣

nk

for any  with T = K = { (ai  aj)} and any ✏> 0. Letting k+1 denote the (k + 1)th largest
eigenvalue of K and !< 2.373 be the exponent of fast matrix multiplication  the algorithm runs in

k  ZZT k2

F  (1 + ✏)k  kk2

F

O(nnz(A)) + ˜O✓n!+1.5 ·⇣ k

k+1✏2⌘!1.5◆ time.

We note that the runtime of our algorithm is O(nnz(A)) whenever n  k  1/k+1  and 1/✏ are not
too large. Due to the relatively poor dependence on n  the algorithm is relevant for very high
dimensional datasets with d  n. Such datasets are found often  e.g.  in genetics applications
[HDC+01  JDMP11]. While we have dependence on 1/k+1  in the natural setting  we only com-
pute a low-rank approximation up to an error threshold  ignoring very small eigenvalues of K  and
so k+1 will not be too small. We do note that if we apply Theorem 2 to the low-rank approximation
instances given by our lower bound construction  k+1 can be very small   1/ poly(n  d) for ma-
trices with poly(n) bounded entries. Thus  removing this dependence is an important open question
in understanding the complexity of low-rank kernel approximation.
We leave open the possibility of improving our algorithm  achieving O(nnz(A)) + n · poly(k  ✏)
runtime  which would match the state-of-the-art for low-rank approximation of non-kernelized ma-
trices [CW13]. Alternatively  it is possible that a lower bound can be shown  proving the that high n
dependence  or the 1/k+1 term are required even for the Kernel PCA guarantee of (2).

2 Lower bounds

Our lower bound proof argues that for a broad class of kernels  given input M  a low-rank approxi-
mation of the associated kernel matrix K achieving (1) can be used to obtain a close approximation
to the Gram matrix M M T . We write (mi  mj) as a function of mT
i mj (or kmi  mjk2 for dis-
tance kernels) and expand this function as a power series. We show that the if input points are
appropriately rescaled  the contribution of degree-1 term mT
i mj dominates  and hence our kernel
matrix approximates M M T   up to some easy to compute low-rank components.
We then show that such an approximation can be used to give a fast algorithm for multiplying any
two integer matrices A 2 Rn⇥d and C 2 Rd⇥k. The key idea is to set M = [A  wC] where w is a
large weight. We then have:

M M T = AAT

wCT AT w2CT C .

wAC

Since w is very large  the AAT block is relatively very small  and so M M T is nearly rank-2k –
it has a ‘heavy’ strip of elements in its last k rows and columns. Thus  computing a relative-error
rank-2k approximation to M M T recovers all entries except those in the AAT block very accurately 
and importantly  recovers the wAC block and so the product AC.

4

2.1 Lower bound for low-rank approximation of M M T .
We ﬁrst illustrate our lower bound technique by showing hardness of direct approximation of M M T .
Theorem 3 (Hardness of low-rank approximation for M M T ). Assume there is an algorithm A
which given any M 2 Rn⇥d returns N 2 Rn⇥k such that kM M T  N N Tk2
F  1kM M T 
(M M T )kk2
For any A 2 Rn⇥d and C 2 Rd⇥k each with integer entries in [2  2]  let B = [AT   wC]T
where w = 3p12
2nd. It is possible to compute the product AC in time T (B  2k) + O(nk!1).

F in T (M  k) time for some approximation factor 1.

wCT AT w2CT C .
Let Q 2 Rn⇥2k be an orthogonal span for the columns of the n ⇥ 2k matrix:

Proof. We can write the (n + k) ⇥ (n + k) matrix BBT as:
BBT = [AT   wC]T [A  wC] = AAT
 0
V w2CT C

wAC

wAC

where V 2 Rk⇥k spans the columns of wCT AT 2 Rk⇥n. The projection QQT BBT gives the best
Frobenius norm approximation to BBT in the span of Q. We can see that:

F AAT

0

0

0

2

F  4

2n2d2

(3)

kBBT  (BBT )2kk2

F  kBBT  QQT BBTk2

(BBT  N N T )2

F  1kBBT  (BBT )2kk2

since each entry of A is bounded in magnitude by 2 and so each entry of AAT is bounded by d2
2.
Let N be the matrix returned by running A on B with rank 2k. In order to achieve the approximation
bound of kBBT  N N Tk2
F we must have  for all i  j:
where the last inequality is from (3). This gives |BBT  N N T|i j  p12
2nd. Since A and
C have integer entries  each entry in the submatrix wAC of BBT is an integer multiple of w =
3p12
2nd  by simply rounding
(N N T )i j to the nearest multiple of w  we obtain the entry exactly. Thus  given N  we can exactly
recover AC in O(nk!1) time by computing the n⇥k submatrix corresponding to AC in BBT .
Theorem 3 gives our main bound Theorem 1 for the case of the linear kernel (mi  mj) = mT

2nd. Since (N N T )i j approximates this entry to error p12

i j  kBBT  N N Tk2

F  14

2n2d2

i mj.

Proof of Theorem 1 – Linear Kernel. We apply Theorem 3 after noting that for B = [AT   wC]T  
nnz(B)  nnz(A) + nk and so T (B  2k) = o(nnz(A)k) + O(nk2).
We show in Appendix A that there is an algorithm which nearly matches the lower bound of Theorem
1 for any = (1 + ✏) for any ✏> 0. Further  in Appendix B we show that even just outputting an
orthogonal matrix Z 2 Rn⇥k such that ˜K = ZZT M M T is a relative-error low-rank approximation
of M M T   but not computing a factorization of ˜K itself  is enough to give fast multiplication of
integer matrices A and C.

2.2 Lower bound for dot product kernels
We now extend Theorem 3 to general dot product kernels – where (ai  aj) = f (aT
function f. This includes  for example  the polynomial kernel.
Theorem 4 (Hardness of low-rank approximation for dot product kernels). Consider any kernel
 : Rd ⇥ Rd ! R+ with (ai  aj) = f (aT
i aj) for some function f which can be expanded as
f (x) =P1q=0 cqxq with c1 6= 0 and |cq/c1| Gq1 and for all q  2 and some G  1.
Assume there is an algorithm A which given M 2 Rn⇥d with kernel matrix K = { (mi  mj)} 
returns N 2 Rn⇥k satisfying kK  N N Tk2
For any A 2 Rn⇥d  C 2 Rd⇥k with integer entries in [2  2]  let B = [w1AT   w2C]T with w1 =
. Then it is possible to compute AC in time T (B  2k + 1) + O(nk!1).
12p12

F  1kK  Kkk in T (M  k) time.

i aj) for some

2nd  w2 =

4pGd2

w2

1

5

Proof. Using our decomposition of (· ·)  we can write the kernel matrix for B and as:
2CT C + c2K(2) + c3K(3) + ...

1 1 + c1 w2

K = c01 1

w1w2CT AT w2

w1w2AC

1AAT

(4)

1

2 < 1

i j = (bT

4pGd2
i bj| w2

  the fact that w1 < w2  and our bound on the entries of A and C 
2d2

where K(q)
i bj)q and 1 denotes the all ones matrix of appropriate size. The key idea is to show
that the contribution of the K(q) terms is small  and so any relative-error rank-(2k+1) approximation
to K must recover an approximation to BBT   and thus the product AC as in Theorem 3.
By our setting of w2 =
we have for all i  j  |bT
i j  c1|bT
i bj| ·

i bj|q1  c1|bT
1Xq=2
1Xq=2
Gq1|bT
Let ¯K be the matrix✓K  c01
1◆  with its top right n ⇥ n block set to 0. ¯K just has its last
k columns and rows non-zero  so has rank  2k. Let Q 2 Rn⇥2k+1 be an orthogonal span for the
columns ¯K along with the all ones vector of length n. Let N be the result of running A on B with
rank 2k + 1. Then we have:

16G. Thus  for any i  j  using that |cq/c1| Gq1:

Gq1
(16G)q1 

1Xq=2

c1|bT

cqK(q)

i bj|.

i bj|

1
12

(5)

1

1

where ˆK(q) denotes the top left n ⇥ n submatrix of K(q). By our bound on the entries of A and (5):

F

0

kK  N N Tk2

F  1kK  K2k+1k2

F  1kK  QQT Kk2

 1(c1w2
⇣c1w2
1AAT + c2 ˆK(2) + c3 ˆK(3) + ...⌘i j 
12c1w2
(K  N N T )i j  kK  N N TkF p1n2 · 2c1w2
p1n · 2c1d2
12p12
w1w2c1

2nd  this gives for any i  j:

12p12

13

w2

2

1d2
2

2nd · w1w2




.

6

Plugging back into (6) and using w1 =

1AAT + c2 ˆK(2) + ...)

2

F

(6)

0

0

1AATi j  2c1w2

1d2
2.

(7)

Since A and C have integer entries  each entry of c1w1w2AC is an integer multiple of c1w1w2. By
the decomposition of (4) and the bound of (5)  if we subtract c0 from the corresponding entry of
K and round it to the nearest multiple of c1w1w2  we will recover the entry of AC. By the bound
of (7)  we can likewise round the corresponding entry of N N T . Computing all nk of these entries
given N takes time O(nk!1)  giving the theorem.

Theorem 4 lets us lower bound the time to compute a low-rank kernel approximation for any kernel
function expressible as a reasonable power expansion of aT
i aj. As a straightforward example  it
gives the lower bound for the polynomial kernel of any degree stated in Theorem 1.

Proof of Theorem 1 – Polynomial Kernel. We apply Theorem 4  noting that (mi  mj) = (c +
j. Thus c1 6= 0
i mj)q can be written as f (mT
mT
and |cj/c1| Gj1 for G = (q/c). Finally note that nnz(B)  nnz(A) + nk giving the result.
2.3 Lower bound for distance kernels

j=0 cjxj with cj = cqjq

i mj) where f (x) =Pq

We ﬁnally extend Theorem 4 to handle kernels like the Gaussian kernel whose value depends on the
squared distance kai  ajk2 rather than just the dot product aT

i aj. We prove:

6

Theorem 5 (Hardness of low-rank approximation for distance kernels). Consider any kernel func-
tion : Rd⇥Rd ! R+ with (ai  aj) = f (kaiajk2) for some function f which can be expanded
as f (x) =P1q=0 cqxq with c1 6= 0 and |cq/c1| Gq1 and for all q  2 and some G  1.
Assume there is an algorithm A which given input M 2 Rn⇥d with kernel matrix K =
{ (mi  mj)}  returns N 2 Rn⇥k satisfying kK  N N Tk2
F  1kK  Kkk in T (M  k) time.
For any A 2 Rn⇥d  C 2 Rd⇥k with integer entries in [2  2]  let B = [w1AT   w2C]T with
2nd). It is possible to compute AC in T (B  2k + 3) +
w1 =
36p12
O(nk!1) time.

2nd  w2 =

2)(36p12

(16Gd24

w2

1

The proof of Theorem 5 is similar to that of Theorem 4  and relegated to Appendix C. The key
idea is to write K as a polynomial in the distance matrix D with Di j = kbi  bjk2
2. Since kbi 
i bj  D can be written as 2BBT plus a rank-2 component. By setting
bjk2
w1  w2 sufﬁciently small  as in the proof of Theorem 4  we ensure that the higher powers of D are
negligible  and thus that our low-rank approximation must accurately recover the submatrix of BBT
corresponding to AC. Theorem 5 gives Theorem 1 for the popular Gaussian kernel:

2 = kbik2

2 + kbjk2

2  2bT

(1/)q

Proof of Theorem 1 – Gaussian Kernel. (mi  mj) can be written as f (kmimjk2) where f (x) =
ex/ =P1q=0
xq. Thus c1 6= 0 and |cq/c1| Gq1 for G = 1/. Applying Theorem 5
and bounding nnz(B)  nnz(A) + nk  gives the result.
3

Input sparsity time kernel PCA for radial basis kernels

q!

Theorem 1 gives little hope for achieving o(nnz(A)k) time for low-rank kernel approximation.
However  the guarantee of (1) is not the only way of measuring the quality of ˜K. Here we show that
for shift/rotationally invariant kernels  including e.g.  radial basis kernels  input sparsity time can be
achieved for the kernel PCA goal of (2).

3.1 Basic algorithm
Our technique is based on the random Fourier features technique [RR07]. Given any shift-invariant
kernel  (x  y) = (x  y) with (0) = 1 (we will assume this w.l.o.g. as the function can always
be scaled)  there is a probability density function p(⌘) over vectors in Rd such that:

p(⌘) is just the (inverse) Fourier transform of (·)  and is a density function by Bochner’s theorem.
Informally  given A 2 Rn⇥d if we let Z denote the matrix with columns z(⌘) indexed by ⌘ 2 Rd.
z(⌘)j = e2⇡i⌘ T aj . Then (8) gives ZP Z⇤ = K where P is diagonal with P⌘ ⌘ = p(⌘)  and Z⇤
denotes the Hermitian transpose.
The idea of random Fourier features is to select s frequencies ⌘1  ... ⌘ s according to the density p(⌘)
and set ˜Z = 1ps [z(⌘1)  ...z(⌘s)]. ˜K = ˜Z ˜ZT is then used to approximate K.
In recent work  Avron et al. [AKM+17] give a new analysis of random Fourier features. Extending
prior work on ridge leverage scores in the discrete setting [AM15  CMM17]  they deﬁne the ridge
leverage function for parameter > 0:

⌧(⌘) = p(⌘)z(⌘)⇤(K + I)1z(⌘)

(9)

As part of their results  which seek ˜K that spectrally approximates K  they prove the following:
Lemma 6. For all ⌘  ⌧(⌘)  n/.
While simple  this bound is key to our algorithm. It was shown in [CMM17] that if the columns of
a matrix are sampled by over-approximations to their ridge leverage scores (with appropriately set
)  the sample is a projection-cost preserving sketch for the original matrix. That is  it can be used
as a surrogate in computing a low-rank approximation. The results of [CMM17] carry over to the
continuous setting giving  in conjunction with Lemma 6:

7

 (x  y) =ZRd

e2⇡i⌘ T (xy)p(⌘)d⌘.

(8)

Lemma 7 (Projection-cost preserving sketch via random Fourier features). Consider any A 2 Rn⇥d
and shift-invariant kernel (·) with (0) = 1  with associated kernel matrix K = { (ai  aj)}
kPn
for
and kernel Fourier transform p(⌘). For any 0 <  1
sufﬁciently large c and let ˜Z = 1ps [z(⌘1)  ...  z(⌘s)] where ⌘1  ... ⌘ s are sampled independently
according to p(⌘). Then with probability  1    for any orthonormal Q 2 Rn⇥k and any  with
T = K:
(10)

i=k+1 i(K)  let s = cn log(n/)

✏2

F  kQQT   k2

F  (1 + ✏)kQQT ˜Z  ˜Zk2
F .

(1  ✏)kQQT ˜Z  ˜Zk2

By (10) if we compute Q satisfying kQQT ˜Z  ˜Zk2
F  (1 + ✏)2k ˜Z  ˜Zkk2

kQQT   k2

F then we have:

(1 + ✏)2

F  (1 + ✏)k ˜Z  ˜Zkk2
1  ✏ kUkU T
F 
= (1 + O(✏))k  kk2

k   k2

F

F

where Uk 2 Rn⇥k contains the top k column singular vectors of . By adjusting constants on ✏ by
making c large enough  we thus have the relative error low-rank approximation guarantee of (2). It
remains to show that this approach can be implemented efﬁciently.

3.2

Input sparsity time implementation

Given ˜Z sampled as in Lemma 7  we can ﬁnd a near optimal subspace Q using any input sparsity
time low-rank approximation algorithm (e.g.  [CW13  NN13]). We have the following Corollary:
Corollary 8. Given ˜Z sampled as in Lemma 7 with s = ˜⇥(
in time ˜O(

✏2k+1(K) )  there is an algorithm running
✏2k+1(K) ) that computes Q satisfying with high probability  for any  with T = K:

n2k

nk

kQQT   k2

F  (1 + ✏)k  kk2
F .

With Corollary 8 in place the main bottleneck to our approach becomes computing ˜Z.

3.2.1 Sampling Frequencies
To compute ˜Z  we ﬁrst sample ⌘1  ... ⌘ s according to p(⌘). Here we use the rotational invariance of
 (·). In this case  p(⌘) is also rotationally invariant [LSS13] and so  letting ˆp(·) be the distribution
over norms of vectors sampled from p(⌘) we can sample ⌘1  ... ⌘ n by ﬁrst selecting s random
Gaussian vectors and then rescaling them to have norms distributed according to ˆp(·). That is  we
can write [⌘1  ... ⌘ n] = GD where G 2 Rd⇥s is a random Gaussian matrix and D is a diagonal
rescaling matrix with Dii = m
with m ⇠ ˆp. We will assume that ˆp can be sampled from in
kGik
O(1) time. This is true for many natural kernels – e.g.  for the Gaussian kernel  ˆp is just a Gaussian
density.

3.2.2 Computing ˜Z
Due to our large sample size  s > n  even writing down G above requires ⌦(nd) time. However 
to form ˜Z we do not need G itself:
it sufﬁces to compute for m = 1  ...  s the column z(⌘m)
with z(⌘m)j = e2⇡i⌘ T
maj . This requires computing AGD  which contains the appropriate dot
products aT
j ⌘m for all m  j. We use a recent result [KPW16] which shows that this can be performed
approximately in input sparsity time:
Lemma 9 (From Theorem 1 of [KPW16]). There is an algorithm running in O(nnz(A) +
log4 dn3s!1.5
) time which outputs random B whose distribution has total variation distance at most
 from the distribution of AG where G 2 Rd⇥s is a random Gaussian matrix. Here  !< 2.373 is
the exponent of fast matrix multiplication.



Proof. Theorem 1 of [KPW16] shows that for B to have total variation distance  from the distribu-
tion of AG it sufﬁces to set B = ACG0 where C is a d ⇥ O(log4 dn2s1/2/) CountSketch matrix

8

and G0 is an O(log4 dn2s1/2/) ⇥ s random Gaussian matrix. Computing AC requires O(nnz(A))
time. Multiplying the result by G0 then requires O( log4 dn3s1.5
) time if fast matrix multiplication is
not employed. Using fast matrix multiplication  this can be improved to O( log4 dn3s!1.5

).





Applying Lemma 9 with  = 1/200 lets us compute random BD with total variation distance 1/200
from AGD. Thus  the distribution of ˜Z generated from this matrix has total variation distance
 1/200 from the ˜Z generated from the true random Fourier features distribution. So  by Corollary
8  we can use ˜Z to compute Q satisfying kQQT   k2
F with probability
1/100 accounting for the the total variation difference and the failure probability of Corollary 8.
This yields our main algorithmic result  Theorem 2.

F  (1 + ✏)k  kk2

3.3 An alternative approach

We conclude by noting that near input sparsity time Kernel PCA can also be achieved for a broad
class of kernels using a very different approach. We can approximate (· ·) via an expansion into
polynomial kernel matrices as is done in [CKS11] and then apply the sketching algorithms for the
polynomial kernel developed in [ANW14]. As long as the expansion achieves high accuracy with
low degree  and as long as 1/k+1 is not too small – since this will control the necessary approxima-
tion factor  this technique can yield runtimes of the form ˜O(nnz(A))+poly(n  k  1/k+1  1/✏)  giv-
ing improved dependence on n for some kernels over our random Fourier features method. Improv-
ing the poly(n  k  1/k+1  1/✏) term in both these methods  and especially removing the 1/k+1
dependence and achieving linear dependence on n is an interesting open question for future work.

4 Conclusion

In this work we have shown that for a broad class of kernels  including the Gaussian  polynomial  and
linear kernels  given data matrix A  computing a relative error low-rank approximation to A’s kernel
matrix K (i.e.  satisfying (1)) requires at least ⌦(nnz(A)k) time  barring a major breakthrough in
the runtime of matrix multiplication.
In the constant error regime  this lower bound essentially
matches the runtimes given by recent work on subquadratic time kernel and PSD matrix low-rank
approximation [MM16  MW17].
We show that for the alternative kernel PCA guarantee of (2)  a potentially faster runtime of
O(nnz(A)) + poly(n  k  1/k+1  1/✏) can be achieved for general shift and rotation-invariant ker-
nels. Practically  improving the second term in our runtime  especially the poor dependence on
n  is an important open question. Generally  computing the kernel matrix K explicitly requires
O(n2d) time  and so our algorithm only gives runtime gains when d is large compared to n – at least
⌦(n!.5)  even ignoring k  k+1  and ✏ dependencies. Theoretically  removing the dependence on
k+1 would be of interest  as it would give input sparsity runtime without any assumptions on the
matrix A (i.e.  that k+1 is not too small). Resolving this question has strong connections to ﬁnding
efﬁcient kernel subspace embeddings  which approximate the full spectrum of K.

References
[AKM+17] Haim Avron  Michael Kapralov  Cameron Musco  Christopher Musco  Ameya Vel-
ingker  and Amir Zandieh. Random Fourier features for kernel ridge regression: Ap-
proximation bounds and statistical guarantees. In Proceedings of the 34th International
Conference on Machine Learning (ICML)  2017.

[AM15] Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression
with statistical guarantees. In Advances in Neural Information Processing Systems 28
(NIPS)  pages 775–783  2015.

[AMS01] Dimitris Achlioptas  Frank Mcsherry  and Bernhard Schölkopf. Sampling techniques
for kernel methods. In Advances in Neural Information Processing Systems 14 (NIPS) 
2001.

9

[ANW14] Haim Avron  Huy Nguyen  and David Woodruff. Subspace embeddings for the polyno-
mial kernel. In Advances in Neural Information Processing Systems 27 (NIPS)  pages
2258–2266  2014.

[BIS17] Arturs Backurs  Piotr Indyk  and Ludwig Schmidt. On the ﬁne-grained complexity
of empirical risk minimization: Kernel methods and neural networks. In Advances in
Neural Information Processing Systems 30 (NIPS)  2017.

[BJ02] Francis Bach and Michael I. Jordan. Kernel independent component analysis. Journal

of Machine Learning Research  3(Jul):1–48  2002.

[BW09] Mohamed-Ali Belabbas and Patrick J. Wolfe. Spectral methods in machine learning:
New strategies for very large datasets. Proceedings of the National Academy of Sci-
ences of the USA  106:369–374  2009.

[CEM+15] Michael B. Cohen  Sam Elder  Cameron Musco  Christopher Musco  and Madalina
Persu. Dimensionality reduction for k-means clustering and low rank approximation.
In Proceedings of the 47th Annual ACM Symposium on Theory of Computing (STOC) 
pages 163–172  2015.

[CKS11] Andrew Cotter  Joseph Keshet  and Nathan Srebro. Explicit approximations of the

Gaussian kernel. arXiv:1109.4603  2011.

[CMM17] Michael B. Cohen  Cameron Musco  and Christopher Musco. Input sparsity time low-
rank approximation via ridge leverage score sampling.
In Proceedings of the 28th
Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)  pages 1758–1777 
2017.

[CW13] Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression
in input sparsity time. In Proceedings of the 45th Annual ACM Symposium on Theory
of Computing (STOC)  pages 81–90  2013.

[CW17] Kenneth L. Clarkson and David P. Woodruff. Low-rank PSD approximation in input-
sparsity time. In Proceedings of the 28th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA)  pages 2061–2072  2017.

[DM05] Petros Drineas and Michael W Mahoney. On the Nyström method for approximat-
ing a Gram matrix for improved kernel-based learning. Journal of Machine Learning
Research  6:2153–2175  2005.

[FS02] Shai Fine and Katya Scheinberg. Efﬁcient SVM training using low-rank kernel repre-

sentations. Journal of Machine Learning Research  2:243–264  2002.

[FT07] Shmuel Friedland and Anatoli Torokhti. Generalized rank-constrained matrix approxi-

mations. SIAM Journal on Matrix Analysis and Applications  29(2):656–659  2007.

[GM13] Alex Gittens and Michael Mahoney. Revisiting the Nyström method for improved
large-scale machine learning. In Proceedings of the 30th International Conference on
Machine Learning (ICML)  pages 567–575  2013. Full version at arXiv:1303.1849.

[GU17] François Le Gall and Florent Urrutia. Improved rectangular matrix multiplication using

powers of the Coppersmith-Winograd tensor. arXiv:1708.05622  2017.

[HDC+01] Ingrid Hedenfalk  David Duggan  Yidong Chen  Michael Radmacher  Michael Bit-
tner  Richard Simon  Paul Meltzer  Barry Gusterson  Manel Esteller  Mark Raffeld 
et al. Gene-expression proﬁles in hereditary breast cancer. New England Journal of
Medicine  344(8):539–548  2001.

[JDMP11] Asif Javed  Petros Drineas  Michael W Mahoney  and Peristera Paschou. Efﬁcient
genomewide selection of PCA-correlated tSNPs for genotype imputation. Annals of
Human Genetics  75(6):707–722  2011.

[KPW16] Michael Kapralov  Vamsi Potluru  and David Woodruff. How to fake multiply by a
In Proceedings of the 33rd International Conference on Machine

Gaussian matrix.
Learning (ICML)  pages 2101–2110  2016.

10

[LG12] François Le Gall. Faster algorithms for rectangular matrix multiplication.

In Pro-
ceedings of the 53rd Annual IEEE Symposium on Foundations of Computer Science
(FOCS)  pages 514–523  2012.

[LG14] François Le Gall. Powers of tensors and fast matrix multiplication. In Proceedings
of the 39th international symposium on symbolic and algebraic computation  pages
296–303. ACM  2014.

[LSS13] Quoc Le  Tamás Sarlós  and Alexander Smola. Fastfood - Computing Hilbert space
expansions in loglinear time. In Proceedings of the 30th International Conference on
Machine Learning (ICML)  pages 244–252  2013.

[MM16] Cameron Musco and Christopher Musco. Recursive sampling for the Nyström method.

In Advances in Neural Information Processing Systems 30 (NIPS)  2016.

[MW17] Cameron Musco and David P Woodruff. Sublinear time low-rank approximation of
positive semideﬁnite matrices. In Proceedings of the 58th Annual IEEE Symposium on
Foundations of Computer Science (FOCS)  2017.

[NN13] Jelani Nelson and Huy L Nguyên. OSNAP: Faster numerical linear algebra algorithms
via sparser subspace embeddings. In Proceedings of the 54th Annual IEEE Symposium
on Foundations of Computer Science (FOCS)  pages 117–126  2013.

[RR07] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines.
In Advances in Neural Information Processing Systems 20 (NIPS)  pages 1177–1184 
2007.

[SS00] Alex J Smola and Bernhard Schökopf. Sparse greedy matrix approximation for ma-
chine learning. In Proceedings of the 17th International Conference on Machine Learn-
ing (ICML)  pages 911–918  2000.

[WS01] Christopher Williams and Matthias Seeger. Using the Nyström method to speed up
kernel machines. In Advances in Neural Information Processing Systems 14 (NIPS) 
pages 682–688  2001.

[WZ13] Shusen Wang and Zhihua Zhang. Improving CUR matrix decomposition and the Nys-
tröm approximation via adaptive sampling. Journal of Machine Learning Research 
14:2729–2769  2013.

[ZTK08] Kai Zhang  Ivor W. Tsang  and James T. Kwok. Improved Nyström low-rank approx-
In Proceedings of the 25th International Conference on

imation and error analysis.
Machine Learning (ICML)  pages 1232–1239  2008.

11

,Cameron Musco
David Woodruff