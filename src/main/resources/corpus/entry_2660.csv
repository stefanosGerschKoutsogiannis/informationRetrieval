2017,Information-theoretic analysis of generalization capability of learning algorithms,We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The bounds provide an information-theoretic understanding of generalization in learning problems  and give theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information.  We propose a number of methods for this purpose  among which are algorithms that regularize the ERM algorithm with relative entropy or with random noise. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.,Information-theoretic analysis of generalization

capability of learning algorithms

Aolin Xu

Maxim Raginsky

{aolinxu2 maxim}@illinois.edu ⇤

Abstract

We derive upper bounds on the generalization error of a learning algorithm in
terms of the mutual information between its input and output. The bounds provide
an information-theoretic understanding of generalization in learning problems 
and give theoretical guidelines for striking the right balance between data ﬁt and
generalization by controlling the input-output mutual information. We propose a
number of methods for this purpose  among which are algorithms that regularize
the ERM algorithm with relative entropy or with random noise. Our work extends
and leads to nontrivial improvements on the recent results of Russo and Zou.

1

Introduction

A learning algorithm can be viewed as a randomized mapping  or a channel in the information-
theoretic language  which takes a training dataset as input and generates a hypothesis as output.
The generalization error is the difference between the population risk of the output hypothesis and
its empirical risk on the training data. It measures how much the learned hypothesis suffers from
overﬁtting. The traditional way of analyzing the generalization error relies either on certain complexity
measures of the hypothesis space  e.g. the VC dimension and the Rademacher complexity [1]  or
on certain properties of the learning algorithm  e.g.  uniform stability [2]. Recently  motivated
by improving the accuracy of adaptive data analysis  Russo and Zou [3] showed that the mutual
information between the collection of empirical risks of the available hypotheses and the ﬁnal output
of the algorithm can be used effectively to analyze and control the bias in data analysis  which is
equivalent to the generalization error in learning problems. Compared to the methods of analysis
based on differential privacy  e.g.  by Dwork et al. [4  5] and Bassily et al. [6]  the method proposed
in [3] is simpler and can handle unbounded loss functions; moreover  it provides elegant information-
theoretic insights into improving the generalization capability of learning algorithms. In a similar
information-theoretic spirit  Alabdulmohsin [7  8] proposed to bound the generalization error in
learning problems using the total-variation information between a random instance in the dataset and
the output hypothesis  but the analysis apply only to bounded loss functions.
In this paper  we follow the information-theoretic framework proposed by Russo and Zou [3] to
derive upper bounds on the generalization error of learning algorithms. We extend the results in [3]
to the situation where the hypothesis space is uncountably inﬁnite  and provide improved upper
bounds on the expected absolute generalization error. We also obtain concentration inequalities for
the generalization error  which were not given in [3]. While the main quantity examined in [3] is the
mutual information between the collection of empirical risks of the hypotheses and the output of the
algorithm  we mainly focus on relating the generalization error to the mutual information between the
input dataset and the output of the algorithm  which formalizes the intuition that the less information
⇤Department of Electrical and Computer Engineering and Coordinated Science Laboratory  University of
Illinois  Urbana  IL 61801  USA. This work was supported in part by the NSF CAREER award CCF-1254041
and in part by the Center for Science of Information (CSoI)  an NSF Science and Technology Center  under
grant agreement CCF-0939370.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

a learning algorithm can extract from the input dataset  the less it will overﬁt. This viewpoint
provides theoretical guidelines for striking the right balance between data ﬁt and generalization by
controlling the algorithm’s input-output mutual information. For example  we show that regularizing
the empirical risk minimization (ERM) algorithm with the input-output mutual information leads to
the well-known Gibbs algorithm. As another example  regularizing the ERM algorithm with random
noise can also control the input-output mutual information. For both the Gibbs algorithm and the
noisy ERM algorithm  we also discuss how to calibrate the regularization in order to incorporate
any prior knowledge of the population risks of the hypotheses into algorithm design. Additionally 
we discuss adaptive composition of learning algorithms  and show that the generalization capability
of the overall algorithm can be analyzed by examining the input-output mutual information of the
constituent algorithms.
Another advantage of relating the generalization error to the input-output mutual information is that
the latter quantity depends on all ingredients of the learning problem  including the distribution of
the dataset  the hypothesis space  the learning algorithm itself  and potentially the loss function  in
contrast to the VC dimension or the uniform stability  which only depend on the hypothesis space or
on the learning algorithm. As the generalization error can strongly depend on the input dataset [9] 
the input-output mutual information can be more tightly coupled to the generalization error than the
traditional generalization-guaranteeing quantities of interest. We hope that our work can provide
some information-theoretic understanding of generalization in modern learning problems  which may
not be sufﬁciently addressed by the traditional analysis tools [9].
For the rest of this section  we deﬁne the quantities that will be used in the paper. In the standard
framework of statistical learning theory [10]  there is an instance space Z  a hypothesis space W 
and a nonnegative loss function ` : W ⇥ Z ! R+. A learning algorithm characterized by a Markov
kernel PW|S takes as input a dataset of size n  i.e.  an n-tuple

S = (Z1  . . .   Zn)

(1)

of i.i.d. random elements of Z with some unknown distribution µ  and picks a random element W of
W as the output hypothesis according to PW|S. The population risk of a hypothesis w 2 W on µ is
(2)

`(w  z)µ(dz).

Lµ(w)   E[`(w  Z)] =ZZ

The goal of learning is to ensure that the population risk of the output hypothesis W is small  either in
expectation or with high probability  under any data generating distribution µ. The excess risk of W
is the difference Lµ(W )  inf w2W Lµ(w)  and its expected value is denoted as Rexcess(µ  PW|S).
Since µ is unknown  the learning algorithm cannot directly compute Lµ(w) for any w 2 W  but can
instead compute the empirical risk of w on the dataset S as a proxy  deﬁned as

nXi=1
For a learning algorithm characterized by PW|S  the generalization error on µ is the difference
Lµ(W )  LS(W )  and its expected value is denoted as

LS(w)   1
n

`(w  Zi).

(3)

(4)
where the expectation is taken with respect to the joint distribution PS W = µ⌦n ⌦ PW|S. The
expected population risk can then be decomposed as

gen(µ  PW|S)   E[Lµ(W )  LS(W )] 

E[Lµ(W )] = E[LS(W )] + gen(µ  PW|S) 

(5)

where the ﬁrst term reﬂects how well the output hypothesis ﬁts the dataset  while the second term
reﬂects how well the output hypothesis generalizes. To minimize E[Lµ(W )] we need both terms in
(5) to be small. However  it is generally impossible to minimize the two terms simultaneously  and
any learning algorithm faces a trade-off between the empirical risk and the generalization error. In
what follows  we will show how the generalization error can be related to the mutual information
between the input and output of the learning algorithm  and how we can use these relationships to
guide the algorithm design to reduce the population risk by balancing ﬁtting and generalization.

2

2 Algorithmic stability in input-output mutual information

As discussed above  having a small generalization error is crucial for a learning algorithm to produce
an output hypothesis with a small population risk. It turns out that the generalization error of a learning
algorithm can be determined by its stability properties. Traditionally  a learning algorithm is said to be
stable if a small change of the input to the algorithm does not change the output of the algorithm much.
Examples include uniform stability deﬁned by Bousquet and Elisseeff [2] and on-average stability
deﬁned by Shalev-Shwartz et al. [11]. In recent years  information-theoretic stability notions  such as
those measured by differential privacy [5]  KL divergence [6  12]  total-variation information [7]  and
erasure mutual information [13]  have been proposed. All existing notions of stability show that the
generalization capability of a learning algorithm hinges on how sensitive the output of the algorithm is
to local modiﬁcations of the input dataset. It implies that the less dependent the output hypothesis W
is on the input dataset S  the better the learning algorithm generalizes. From an information-theoretic
point of view  the dependence between S and W can be naturally measured by the mutual information
between them  which prompts the following information-theoretic deﬁnition of stability. We say that
a learning algorithm is ("  µ)-stable in input-output mutual information if  under the data-generating
distribution µ 

Further  we say that a learning algorithm is "-stable in input-output mutual information if

I(S; W )  ".

sup
µ

I(S; W )  ".

(6)

(7)

According to the deﬁnitions in (6) and (7)  the less information the output of a learning algorithm can
provide about its input dataset  the more stable it is. Interestingly  if we view the learning algorithm
PW|S as a channel from Zn to W  the quantity supµ I(S; W ) can be viewed as the information
capacity of the channel  under the constraint that the input distribution is of a product form. The
deﬁnition in (7) means that a learning algorithm is more stable if its information capacity is smaller.
The advantage of the weaker deﬁnition in (6) is that I(S; W ) depends on both the algorithm and the
distribution of the dataset. Therefore  it can be more tightly coupled with the generalization error 
which itself depends on the dataset. We mainly focus on studying the consequence of this notion of
("  µ)-stability in input-output mutual information for the rest of this paper.

3 Upper-bounding generalization error via I(S; W )

In this section  we derive various generalization guarantees for learning algorithms that are stable in
input-output mutual information.

3.1 A decoupling estimate
We start with a digression from the statistical learning problem to a more general problem  which may
be of independent interest. Consider a pair of random variables X and Y with joint distribution PX Y .
Let ¯X be an independent copy of X  and ¯Y an independent copy of Y   such that P ¯X  ¯Y = PX ⌦ PY .
For an arbitrary real-valued function f : X ⇥ Y ! R  we have the following upper bound on the
absolute difference between E[f (X  Y )] and E[f ( ¯X  ¯Y )].
Lemma 1 (proved in Appendix A). If f ( ¯X  ¯Y ) is -subgaussian under P ¯X  ¯Y = PX ⌦ PY
2   then

3.2 Upper bound on expected generalization error
Upper-bounding the generalization error of a learning algorithm PW|S can be cast as a special case of
the preceding problem  by setting X = S  Y = W   and f (s  w) = 1
i=1 `(w  zi). For an arbitrary
w 2 W  the empirical risk can be expressed as LS(w) = f (S  w) and the population risk can be
expressed as Lµ(w) = E[f (S  w)]. Moreover  the expected generalization error can be written as
(9)

E[f (X  Y )]  E[f ( ¯X  ¯Y )] p22I(X; Y ).
nPn
gen(µ  PW|S) = E[f ( ¯S  ¯W )]  E[f (S  W )] 

(8)

2Recall that a random variable U is -subgaussian if log E[e(UEU )]  22/2 for all  2 R.

3

where the joint distribution of S and W is PS W = µ⌦n ⌦ PW|S. If `(w  Z) is -subgaussian for all
w 2 W  then f (S  w) is /pn-subgaussian due to the i.i.d. assumption on Zi’s  hence f ( ¯S  ¯W ) is
/pn-subgaussian. This  together with Lemma 1  leads to the following theorem.
Theorem 1. Suppose `(w  Z) is -subgaussian under µ for all w 2 W  then

I(S; W ).

(10)

gen(µ  PW|S) r 22

n

Theorem 1 suggests that  by controlling the mutual information between the input and the output
of a learning algorithm  we can control its generalization error. The theorem allows us to consider
unbounded loss functions as long as the subgaussian condition is satisﬁed. For a bounded loss
function `(· ·) 2 [a  b]  `(w  Z) is guaranteed to be (b  a)/2-subgaussian for all µ and all w 2 W.
Russo and Zou [3] considered the same problem setup with the restriction that the hypothesis space
W is ﬁnite  and showed that |gen(µ  PW|S)| can be upper-bounded in terms of I(⇤W(S); W )  where
(11)
is the collection of empirical risks of the hypotheses in W. Using Lemma 1 by setting X =⇤ W(S) 
Y = W   and f (⇤W(s)  w) = Ls(w)  we immediately recover the result by Russo and Zou even
when W is uncountably inﬁnite:
Theorem 2 (Russo and Zou [3]). Suppose `(w  Z) is -subgaussian under µ for all w 2 W  then
(12)

⇤W(S)  LS(w)w2W

I(⇤W(S); W ).

gen(µ  PW|S) r 22

n

It should be noted that Theorem 1 can be obtained as a consequence of Theorem 2 because

I(⇤W(S); W )  I(S; W ) 

(13)
which is due to the Markov chain ⇤W(S)  S  W   as for each w 2 W  LS(w) is a function of S.
However  if the output W depends on S only through the empirical risks ⇤W(S)  in other words 
when the Markov chain S  ⇤W(S)  W holds  then Theorem 1 and Theorem 2 are equivalent. The
advantage of Theorem 1 is that I(S; W ) can be much easier to evaluate than I(⇤W(S); W )  and can
provide better insights to guide the algorithm design. We will elaborate on this when we discuss the
Gibbs algorithm and the adaptive composition of learning algorithms.
Theorem 1 and Theorem 2 only provide upper bounds on the expected generalization error. We are
often interested in analyzing the absolute generalization error |Lµ(W )  LS(W )|  e.g.  its expected
value or the probability for it to be small. We need to develop stronger tools to tackle these problems 
which is the subject of the next two subsections.

3.3 A concentration inequality for |Lµ(W )  LS(W )|
For any ﬁxed w 2 W  if `(w  Z) is -subgaussian  the Chernoff-Hoeffding bound gives P[|Lµ(w) 
LS(w)| >↵ ]  2e↵2n/22. It implies that  if S and W are independent  then a sample size of

sufﬁces to guarantee

n =

22
↵2 log

2


P[|Lµ(W )  LS(W )| >↵ ]  .

(15)
The following results show that  when W is dependent on S  as long as I(S; W ) is sufﬁciently small 
a sample complexity polynomial in 1/↵ and logarithmic in 1/ still sufﬁces to guarantee (15)  where
the probability now is taken with respect to the joint distribution PS W = µ⌦n ⌦ PW|S.
Theorem 3 (proved in Appendix B). Suppose `(w  Z) is -subgaussian under µ for all w 2 W. If
a learning algorithm satisﬁes I(⇤W(S); W )  "  then for any ↵> 0 and 0 <  1  (15) can be
guaranteed by a sample complexity of

n =

+ log

82

↵2 ✓ "



4

2

◆ .

(14)

(16)

In view of (13)  any learning algorithm that is ("  µ)-stable in input-output mutual information
satisﬁes the condition I(⇤W(S); W )  ". The proof of Theorem 3 is based on Lemma 1 and an
adaptation of the “monitor technique” proposed by Bassily et al. [6]. While the high-probability
bounds of [4–6] based on differential privacy are for bounded loss functions and for functions with
bounded differences  the result in Theorem 3 only requires `(w  Z) to be subgaussian. We have the
following corollary of Theorem 3.
Corollary 1. Under the conditions in Theorem 3  if for some function g(n)  1  "  (g(n) 
1) log 2

   then a sample complexity that satisﬁes n/g(n)  82

 guarantees (15).

↵2 log 2

For example  taking g(n) = 2  Corollary 1 implies that if "   log(2/)  then (15) can be
guaranteed by a sample complexity of n = (162/↵2) log(2/)  which is on the same order of
the sample complexity when S and W are independent as in (14). As another example  taking
g(n) = pn  Corollary 1 implies that if "  (pn  1) log(2/)  then a sample complexity of
n = (644/↵4) (log(2/))2 guarantees (15).

3.4 Upper bound on E|Lµ(W )  LS(W )|
A byproduct of the proof of Theorem 3 (setting m = 1 in the proof) is an upper bound on the expected
absolute generalization error.
Theorem 4. Suppose `(w  Z) is -subgaussian under µ for all w 2 W. If a learning algorithm
satisﬁes that I(⇤W(S); W )  "  then

gen(µ  PW|S) r 22H(W )

n

.

(18)

For the ERM algorithm  the upper bounds for the expected generalization error also hold for the
expected excess risk  since the empirical risk of the ERM algorithm satisﬁes

E[LS(WERM)] = Eh inf

w2W

LS(w)i  inf

w2W

E[LS(w)] = inf
w2W

Lµ(w).

(19)

For an uncountable hypothesis space  we can always convert it to a ﬁnite one by quantizing the output
hypothesis. For example  if W ⇢ Rm  we can deﬁne the covering number N (r  W) as the cardinality
of the smallest set W0 ⇢ Rm such that for all w 2 W there is w0 2 W0 with kw  w0k  r  and we
can use W0 as the codebook for quantization. The ﬁnal output hypothesis W 0 will be an element of

5

ELµ(W )  LS(W ) r 22

n

(" + log 2).

(17)

Theorem 4 together with Markov’s inequality implies that (15) can be guaranteed by n = 22

This result improves [3  Prop. 3.2]  which states that ELS(W )  Lµ(W )  /pn + 36p22"/n.
↵22" +
log 2  but it has a worse dependence on  as compared to the sample complexity given by Theorem 3.

4 Learning algorithms with input-output mutual information stability

In this section  we discuss several learning problems and algorithms from the viewpoint of input-
output mutual information stability. We ﬁrst consider two cases where the input-output mutual
information can be upper-bounded via the properties of the hypothesis space. Then we propose
two learning algorithms with controlled input-output mutual information by regularizing the ERM
algorithm. We also discuss other methods to induce input-output mutual information stability  and
the stability of learning algorithms obtained from adaptive composition of constituent algorithms.

4.1 Countable hypothesis space

When the hypothesis space is countable  the input-output mutual information can be directly upper-
bounded by H(W )  the entropy of W . If |W| = k  we have H(W )  log k. From Theorem 1  if
`(w  Z) is -subgaussian for all w 2 W  then for any learning algorithm PW|S with countable W 

W0. If W lies in a d-dimensional subspace of Rm and maxw2W kwk = B  then setting r = 1/pn 
we have N (r  W)  (2Bpdn)d  and under the subgaussian condition of ` 
log2Bpdn.

gen(µ  PW 0|S) r 22d

4.2 Binary Classiﬁcation
For the problem of binary classiﬁcation  Z = X ⇥ Y  Y = {0  1}  W is a collection of classiﬁers
w : X ! Y  which could be uncountably inﬁnite  and `(w  z) = 1{w(x) 6= y}. Using Theorem 1 
we can perform a simple analysis of the following two-stage algorithm [14  15] that can achieve the
same performance as ERM. Given the dataset S  split it into S1 and S2 with lengths n1 and n2. First 
pick a subset of hypotheses W1 ⇢ W based on S1 such that (w(X1)  . . .   w(Xn1)) for w 2 W1 are
all distinct and {(w(X1)  . . .   w(Xn1))  w 2 W1} = {(w(X1)  . . .   w(Xn1))  w 2 W}. In other
words  W1 forms an empirical cover of W with respect to S1. Then pick a hypothesis from W1 with
the minimal empirical risk on S2  i.e. 

(20)

n

W = arg min
w2W1

LS2(w).

(21)

Denoting the nth shatter coefﬁcient and the VC dimension of W by Sn and V   we can upper-bound
the expected generalization error of W with respect to S2 as

E[Lµ(W )]  E[LS2(W )] = E⇥E[Lµ(W )  LS2(W )|S1]⇤ s V log(n1 + 1)

2n2

where we have used the fact that I(S2; W|S1 = s1)  H(W|S1 = s1)  log Sn1  V log(n1 + 1) 
by Sauer’s Lemma  and Theorem 1. It can also be shown that [14  15]

 

(22)

where the second expectation is taken with respect to W1 which depends on S1  and c is a constant.
Combining (22) and (23) and setting n1 = n2 = n/2  we have for some constant c 

E[LS2(W )]  Eh inf

w2W1

Lµ(w) + cr V

n1

w2W

Lµ(w)i  inf
Lµ(w) + cr V log n

n

 

(23)

P ⇤W|S=s(dw) =

eLs(w)Q(dw)
EQ[eLs(W )]

for each s 2 Zn.

6

E[Lµ(W )]  inf
w2W

.

(24)

From an information-theoretic point of view  the above two-stage algorithm effectively controls the
conditional mutual information I(S2; W|S1) by extracting an empirical cover of W using S1  while
maintaining a small empirical risk using S2.

4.3 Gibbs algorithm
As Theorem 1 shows that the generalization error can be upper-bounded in terms of I(S; W )  it is
natural to consider an algorithm that minimizes the empirical risk regularized by I(S; W ):

P ?
W|S = arg inf

PW|S ✓E[LS(W )] +

1


I(S; W )◆  

where > 0 is a parameter that balances ﬁtting and generalization. To deal with the issue that µ
is unknown to the learning algorithm  we can relax the above optimization problem by replacing
I(S; W ) with an upper bound D(PW|SkQ|PS) = I(S; W ) + D(PWkQ)  where Q is an arbitrary
distribution on W and D(PW|SkQ|PS) =RZn D(PW|S=skQ)µ⌦n(ds)  so that the solution of the
relaxed optimization problem does not depend on µ. It turns out that the well-known Gibbs algorithm
solves the relaxed optimization problem.
Theorem 5 (proved in Appendix C). The solution to the optimization problem

P ⇤W|S = arg inf
is the Gibbs algorithm  which satisﬁes

PW|S ✓E[LS(W )] +

1


D(PW|SkQ|PS)◆

(25)

(26)

(27)

We would not have been able to arrive at the Gibbs algorithm had we used I(⇤W(S); W )
as the regularization term instead of I(S; W ) in (25)  even if we upper-bound I(⇤W(S)) by
D(PW|⇤W(S)kQ|P⇤W(S)). Using the fact that the Gibbs algorithm is (2/n 0)-differentially pri-
vate when ` 2 [0  1] [16] and the group property of differential privacy [17]  we can upper-bound the
input-output mutual information of the Gibbs algorithm as I(S; W )  2. Then from Theorem 1 
bound on the expected generalization error for the Gibbs algorithm is obtained in [13]  which states
that if ` 2 [0  1] 

we know that for ` 2 [0  1] gen(µ  P ⇤W|S) p/n. Using Hoeffding’s lemma  a tighter upper

.

(28)

gen(µ  P ⇤W|S) 


2n

With the guarantee on the generalization error  we can analyze the population risk of the Gibbs
algorithm. We ﬁrst present a result for countable hypothesis spaces.
Corollary 2 (proved in Appendix D). Suppose W is countable. Let W denote the output of the
Gibbs algorithm applied on dataset S  and let wo denote the hypothesis that achieves the minimum
population risk among W. For ` 2 [0  1]  the population risk of W satisﬁes

.
2n

Lµ(w) +

(29)

E[Lµ(W )]  inf
w2W

Q(wo)

1


log

1

+

The distribution Q in the Gibbs algorithm can be used to express our preference  or our prior
knowledge of the population risks  of the hypotheses in W  in a way that a higher probability under
Q is assigned to a hypothesis that we prefer. For example  we can order the hypotheses according to
our prior knowledge of their population risks  and set Q(wi) = 6/⇡2i2 for the ith hypothesis in the
order  then  setting  = pn  (29) becomes

E[Lµ(W )]  inf
w2W

Lµ(w) +

2 log io + 1

pn

 

(30)

where io is the index of wo. It means that a better prior knowledge on the population risks leads to a
smaller sample complexity to achieve a certain expected excess risk. As another example  if |W| = k
and we have no preference on any hypothesis  then taking Q as the uniform distribution on W and
setting  = 2pn log k  (29) becomes E[Lµ(W )]  inf w2W Lµ(w) +p(1/n)log k.

For uncountable hypothesis spaces  we can do a similar analysis for the population risk under a
Lipschitz assumption on the loss function.
Corollary 3 (proved in Appendix E). Suppose W = Rd. Let wo be the hypothesis that achieves the
minimum population risk among W. Suppose ` 2 [0  1] and `(·  z) is ⇢-Lipschitz for all z 2 Z. Let
W denote the output of the Gibbs algorithm applied on dataset S. The population risk of W satisﬁes

E[Lµ(W )]  inf
w2W

Lµ(w) +


2n

+ inf

a>0✓a⇢pd +

1


DN (wo  a2Id)kQ◆ .

Again  we can use the distribution Q to express our preference of the hypotheses in W. For example 
we can choose Q = N (wQ  b2Id) with b = n1/4d1/4⇢1/2 and choose  = n3/4d1/4⇢1/2. Then 
setting a = b in (31)  we have

E[Lµ(W )]  inf
w2W

Lµ(w) +

d1/4⇢1/2

2n1/4

kwQ  wok2 + 3 .

This result essentially has no restriction on W  which could be unbounded  and only requires the
Lipschitz condition on `(·  z)  which could be non-convex. The sample complexity decreases with a
better prior knowledge of the optimal hypothesis.

(31)

(32)

4.4 Noisy empirical risk minimization
Another algorithm with controlled input-output mutual information is the noisy empirical risk
minimization algorithm  where independent noise Nw  w 2 W  is added to the empirical risk of each
hypothesis  and the algorithm outputs a hypothesis that minimizes the noisy empirical risks:
(33)

W = arg min

w2W LS(w) + Nw.

7

Similar to the Gibbs algorithm  we can express our preference of the hypotheses by controlling the
amount of noise added to each hypothesis  such that our preferred hypotheses will be more likely
to be selected when they have similar empirical risks as other hypotheses. The following result
formalizes this idea.
Corollary 4 (proved in Appendix F). Suppose W is countable and is indexed such that a hypothesis
with a lower index is preferred over one with a higher index. Also suppose ` 2 [0  1]. For the noisy
ERM algorithm in (33)  choosing Ni to be an exponential random variable with mean bi  we have

E[Lµ(W )]  min

i

Lµ(wi) + bio +vuut 1

2n

Lµ(wi)

bi  1Xi=1

1Xi=1

1

bi!1

 

(34)

(35)

where io = arg mini Lµ(wi). In particular  choosing bi = i1.1/n1/3  we have

E[Lµ(W )]  min

i

Lµ(wi) +

i1.1
o + 3
n1/3

.

Without adding noise  the ERM algorithm applied to the above case when |W| = k can achieve
E[Lµ(WERM)]  mini2[k] Lµ(wi) +p(1/2n)log k. Compared with (35)  we see that performing
noisy ERM may be beneﬁcial when we have high-quality prior knowledge of wo and when k is large.

4.5 Other methods to induce input-output mutual information stability

In addition to the Gibbs algorithm and the noisy ERM algorithm  many other methods may be
used to control the input-output mutual information of the learning algorithm. One method is to
preprocess the dataset S to obtain ˜S  and then run a learning algorithm on ˜S. The preprocessing
can be adding noise to the data or erasing some of the instances in the dataset  etc. In any case  we

method is the postprocessing of the output of a learning algorithm. For example  the weights ˜W
generated by a neural network training algorithm can be quantized or perturbed by noise. This

have the Markov chain S  ˜S  W  which implies I(S; W )  minI(S; ˜S)  I( ˜S; W ) . Another
gives rise to the Markov chain S  ˜W  W  which implies I(S; W )  minI( ˜W ; W )  I(S; ˜W ) .

Moreover  strong data processing inequalities [18] may be used to sharpen these upper bounds
on I(S; W ). Preprocessing of the dataset and postprocessing of the output hypothesis are among
numerous regularization methods used in the ﬁeld of deep learning [19  Ch. 7.5]. Other regularization
methods may also be interpreted as ways to induce the input-output mutual information stability of a
learning algorithm  and this would be an interesting direction of future research.

4.6 Adaptive composition of learning algorithms

Beyond analyzing the generalization error of individual learning algorithms  examining the input-
output mutual information is also useful for analyzing the generalization capability of complex
learning algorithms obtained by adaptively composing simple constituent algorithms. Under a k-fold
adaptive composition  the dataset S is shared by k learning algorithms that are sequentially executed.
For j = 1  . . .   k  the output Wj of the jth algorithm may be drawn from a different hypothesis
space Wj based on S and the outputs W j1 of the previously executed algorithms  according to
PWj|S W j1. An example with k = 2 is model selection followed by a learning algorithm using the
same dataset. Various boosting techniques in machine learning can also be viewed as instances of
adaptive composition. From the data processing inequality and the chain rule of mutual information 

I(S; Wk)  I(S; W k) =

kXj=1

I(S; Wj|W j1).

(36)

If the Markov chain S  ⇤Wj (S)  Wj holds conditional on W j1 for j = 1  . . .   k  then the
upper bound in (36) can be sharpened toPk
j=1 I(⇤Wj (S); Wj|W j1). We can thus control the
generalization error of the ﬁnal output by controlling the conditional mutual information at each step of
the composition. This also gives us a way to analyze the generalization error of the composed learning
algorithm using the knowledge of local generalization guarantees of the constituent algorithms.

8

Acknowledgement

We would like to thank Vitaly Feldman and Vivek Bagaria for pointing out errors in the earlier version
of this paper. We also would like to thank Peng Guan for helpful discussions.

References
[1] S. Boucheron  O. Bousquet  and G. Lugosi  “Theory of classiﬁcation: a survey of some recent

advances ” ESAIM: Probability and Statistics  vol. 9  pp. 323–375  2005.

[2] O. Bousquet and A. Elisseeff  “Stability and generalization ” J. Machine Learning Res.  vol. 2 

pp. 499–526  2002.

[3] D. Russo and J. Zou  “How much does your data exploration overﬁt? Controlling bias via
information usage ” arXiv preprint  2016. [Online]. Available: https://arxiv.org/abs/1511.05219
[4] C. Dwork  V. Feldman  M. Hardt  T. Pitassi  O. Reingold  and A. Roth  “Preserving statistical
validity in adaptive data analysis ” in Proc. of 47th ACM Symposium on Theory of Computing
(STOC)  2015.

[5] ——  “Generalization in adaptive data analysis and holdout reuse ” in 28th Annual Conference

on Neural Information Processing Systems (NIPS)  2015.

[6] R. Bassily  K. Nissim  A. Smith  T. Steinke  U. Stemmer  and J. Ullman  “Algorithmic stability
for adaptive data analysis ” in Proceedings of The 48th Annual ACM Symposium on Theory of
Computing (STOC)  2016.

[7] I. Alabdulmohsin  “Algorithmic stability and uniform generalization ” in 28th Annual Confer-

ence on Neural Information Processing Systems (NIPS)  2015.

[8] ——  “An information-theoretic route from generalization in expectation to generalization in
probability ” in 20th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 
2017.

[9] C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals  “Understanding deep learning requires
rethinking generalization ” in International Conference on Learning Representations (ICLR) 
2017.

[10] S. Shalev-Shwartz and S. Ben-David  Understanding Machine Learning: From Theory to

Algorithms. Cambridge University Press  2014.

[11] S. Shalev-Shwartz  O. Shamir  N. Srebro  and K. Sridharan  “Learnability  stability and uniform

convergence ” J. Mach. Learn. Res.  vol. 11  pp. 2635–2670  2010.

[12] Y.-X. Wang  J. Lei  and S. E. Fienberg  “On-average kl-privacy and its equivalence to gener-
alization for max-entropy mechanisms ” in Proceedings of the International Conference on
Privacy in Statistical Databases  2016.

[13] M. Raginsky  A. Rakhlin  M. Tsao  Y. Wu  and A. Xu  “Information-theoretic analysis of stability
and bias of learning algorithms ” in Proceedings of IEEE Information Theory Workshop  2016.
[14] K. L. Buescher and P. R. Kumar  “Learning by canonical smooth estimation. I. Simultaneous
estimation ” IEEE Transactions on Automatic Control  vol. 41  no. 4  pp. 545–556  Apr 1996.
[15] L. Devroye  L. Györﬁ  and G. Lugosi  A Probabilistic Theory of Pattern Recognition. Springer 

1996.

[16] F. McSherry and K. Talwar  “Mechanism design via differential privacy ” in Proceedings of 48th

Annual IEEE Symposium on Foundations of Computer Science (FOCS)  2007.

[17] C. Dwork and A. Roth  “The algorithmic foundations of differential privacy ” Foundations and

Trends in Theoretical Computer Science  vol. 9  no. 3-4  2014.

[18] M. Raginsky  “Strong data processing inequalities and -Sobolev inequalities for discrete

channels ” IEEE Trans. Inform. Theory  vol. 62  no. 6  pp. 3355–3389  2016.

[19] I. Goodfellow  Y. Bengio  and A. Courville  Deep Learning. MIT Press  2016.
[20] S. Boucheron  G. Lugosi  and P. Massart  Concentration Inequalities: A Nonasymptotic Theory

of Independence. Oxford Univ. Press  2013.

9

[21] T. Zhang  “Information-theoretic upper and lower bounds for statistical estimation ” IEEE Trans.

Inform. Theory  vol. 52  no. 4  pp. 1307 – 1321  2006.

[22] Y. Polyanskiy and Y. Wu  “Lecture Notes on Information Theory ” Lecture Notes for ECE563
(UIUC) and 6.441 (MIT)  2012-2016. [Online]. Available: http://people.lids.mit.edu/yp/
homepage/data/itlectures_v4.pdf

[23] S. Verdú  “The exponential distribution in information theory ” Problems of Information Trans-

mission  vol. 32  no. 1  pp. 86–95  1996.

10

,Huahua Wang
Arindam Banerjee
Aolin Xu
Maxim Raginsky