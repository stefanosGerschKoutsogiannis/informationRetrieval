2019,Computing Full Conformal Prediction Set with Approximate Homotopy,If you are predicting the label $y$ of a new object with $\hat y$  how confident are you that $y = \hat y$? Conformal prediction methods provide an elegant framework for answering such question by building a $100 (1 - \alpha)\%$ confidence region without assumptions on the distribution of the data. It is based on a refitting procedure that parses all the possibilities for $y$ to select the most likely ones. Although providing strong coverage guarantees  conformal set is impractical to compute exactly for many regression problems. We propose efficient algorithms to compute conformal prediction set using approximated solution of (convex) regularized empirical risk minimization. Our approaches rely on a new homotopy continuation technique for tracking the solution path with respect to sequential changes of the observations. We also provide a detailed analysis quantifying its complexity.,Computing Full Conformal Prediction Set with

Approximate Homotopy

Eugene Ndiaye

RIKEN Center for Advanced Intelligence Project

eugene.ndiaye@riken.jp

Ichiro Takeuchi

Nagoya Institute of Technology

takeuchi.ichiro@nitech.ac.jp

Abstract

If you are predicting the label y of a new object with ˆy  how conﬁdent are you
that y = ˆy? Conformal prediction methods provide an elegant framework for
answering such question by building a 100(1 − α)% conﬁdence region without
assumptions on the distribution of the data. It is based on a reﬁtting procedure that
parses all the possibilities for y to select the most likely ones. Although providing
strong coverage guarantees  conformal set is impractical to compute exactly for
many regression problems. We propose efﬁcient algorithms to compute conformal
prediction set using approximated solution of (convex) regularized empirical risk
minimization. Our approaches rely on a new homotopy continuation technique for
tracking the solution path with respect to sequential changes of the observations.
We also provide a detailed analysis quantifying its complexity.

1

Introduction

In many practical applications of regression models it is beneﬁcial to provide  not only a point-
prediction  but also a prediction set that has some desired coverage property. This is especially
true when a critical decision is being made based on the prediction  e.g.  in medical diagnosis or
experimental design. Conformal prediction is a general framework for constructing non-asymptotic
and distribution-free prediction sets. Since the seminal work of [23  21]  the statistical properties and
computational algorithms for conformal prediction have been developed for a variety of machine
learning problems such as density estimation  clustering  and regression - see the review of [2].
Let Dn = {(x1  y1) ···   (xn  yn)} be a sequence of features and labels of random variables in
Rp × R from a distribution P. Based on observed data Dn and a new test instance xn+1 in Rp  the
goal of conformal prediction is to build a 100(1 − α)% conﬁdence set that contains the unobserved
variable yn+1 for α in (0  1)  without any speciﬁc assumptions on the distribution P.
The conformal prediction set for yn+1 is deﬁned as the set of z ∈ R whose typicalness is sufﬁciently
large. The typicalness of each z is deﬁned based on the residuals of the regression model  trained
with an augmented training set Dn+1(z) = Dn ∪ (xn+1  z). On average  prediction sets constructed
within a conformal prediction framework are shown to have a desirable coverage property  as long
as the training instances {(xi  yi)}n+1
i=1 are exchangeable  and the regression estimator is symmetric
with respect to the training instances (even when the model is not correctly speciﬁed).
Despite these attractive properties  the computation of conformal prediction sets has been intractable
since one needs to ﬁt inﬁnitely many regression models with an augmented training set Dn+1(z)  for
all possible z ∈ R. Except for simple regression estimators with quadratic loss (such as least-square
regression  ridge regression or lasso estimators) where an explicit and exact solution of the model
parameter can be written as a piece of a linear function in the observation vectors  the computation of
the full and exact conformal set for the general regression problem is challenging and still open.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Contributions. We propose a general method to compute the full conformal prediction set for a
wider class of regression estimators. The main novelties are summarized in the following points:

• We introduce a new homotopy continuation technique  inspired by [8  16]  which can
efﬁciently update an approximate solution with tolerance  > 0  when the data are streamed
sequentially. For this  we show that the variation of the optimization error only depends on
the loss on the new input data. Thus  exploiting the regularity of the loss  we can provide
a range of observations for which an approximate solution is still valid. This allows us
to approximately ﬁt inﬁnitely many regression models for all possible z in a pre-selected
range [ymin  ymax]  using only a ﬁnite number of candidate z. For example  when the loss
function is smooth  the number of model ﬁttings required for constructing the prediction set
is O(1/√).

• Exploiting the approximation error bounds of the proposed homotopy continuation method 
we can construct the prediction set based on the -solution  which satisﬁes the same valid
coverage properties under the same mild assumptions as the conformal prediction framework.
When the approximation tolerance  decreases to 0  the prediction set converges to the exact
conformal prediction set which would be obtained by ﬁtting an inﬁnitely large number of
regression models. Furthermore  if the loss function of the regression estimator is smooth
and some other regularity conditions are satisﬁed  the prediction set constructed by the
proposed method is shown to contain the exact conformal prediction set.

For reproducibility  our implementation is available in

https://github.com/EugeneNdiaye/homotopy_conformal_prediction

Notation. For a non zero integer n  we denote [n] to be the set {1 ···   n}. The dataset of size n
is denoted Dn = (xi  yi)i∈[n]  the row-wise feature matrix X = [x1 ···   xn+1](cid:62)   and X[n] is its
restriction to the n ﬁrst rows. Given a proper  closed and convex function f : Rn → R ∪ {+∞}  we
denote domf = {x ∈ Rn : f (x) < +∞}. Its Fenchel-Legendre transform is f∗ : Rn → R ∪ {+∞}
deﬁned by f∗(x∗) = supx∈domf(cid:104)x∗  x(cid:105) − f (x). The smallest integer larger than a real value r
is denoted (cid:100)r(cid:101). We denote by Q1−α  the (1 − α)-quantile of a real valued sequence (Ui)i∈[n+1] 
deﬁned as the variable Q1−α = U((cid:100)(n+1)(1−α)(cid:101))  where U(i) are the i-th order statistics. For j in
i=1 1Ui≤Uj . The interval

[n + 1]  the rank of Uj among U1 ···   Un+1 is deﬁned as Rank(Uj) =(cid:80)n+1

[a − τ  a + τ ] will be denoted [a ± τ ].
2 Background and Problem Setup

We consider the framework of regularized empirical risk minimization (see for instance [22]) with a
convex loss function (cid:96) : R × R (cid:55)→ R  a convex regularizer Ω : R (cid:55)→ R and a positive scalar λ:

n(cid:88)

i=1

ˆβ ∈ arg min
β∈Rp

P (β) :=

(cid:96)(yi  x

(cid:62)
i β) + λΩ(β) .

(1)

For simplicity  we will assume that for any real values z and z0  we have (cid:96)(z0  z) and (cid:96)(z  z0) are
non negative  (cid:96)(z0  z0) and (cid:96)∗(z0  0) are equal to zero. These assumptions are easy to satisfy and we
refer the reader to the appendix for more details.

Examples. A popular example of a loss function found in the literature is power norm
regression  where (cid:96)(a  b) = |a − b|q. When q = 2  this corresponds to classical linear re-
gression. Cases where q ∈ [1  2) are common in robust statistics. In particular  q = 1 is known
as least absolute deviation. The logcosh loss (cid:96)(a  b) = γ log(cosh(a − b)/γ) is a differentiable
alternative to the (cid:96)∞-norm (Chebychev approximation). One can also have the Linex loss function
[9  4] which provides an asymmetric loss (cid:96)(a  b) = exp(γ(a − b)) − γ(a − b) − 1  for γ (cid:54)= 0. Any
convex regularization functions Ω e.g. Ridge [10] or sparsity inducing norms in [1] can be considered.
For a new test instance xn+1  the goal is to construct a prediction set ˆΓ(α)(xn+1) for yn+1 such that

Pn+1(yn+1 ∈ ˆΓ(α)(xn+1)) ≥ 1 − α for α ∈ (0  1) .

(2)

2

2.1 Conformal Prediction

Conformal prediction [23] is a general framework for constructing conﬁdence sets  with the remark-
able properties of being distribution free  having a ﬁnite sample coverage guarantee  and being able
to be adapted to any estimator under mild assumptions. We recall the arguments in [21  14].
Let us introduce the extension of the optimization problem (1) with augmented training data
n(cid:88)
Dn+1(z) := Dn ∪ {(xn+1  z)} for z ∈ R:
Pz(β) :=

(cid:62)
n+1β) + λΩ(β) .

(cid:62)
i β) + (cid:96)(z  x

(cid:96)(yi  x

(3)

ˆβ(z) ∈ arg min
β∈Rp

i=1

(cid:62)
∀i ∈ [n]  ˆRi(z) = ψ(yi  x
i

Then  for any z in R  we deﬁne the conformity measure for Dn+1(z) as
(cid:62)
ˆβ(z)) and ˆRn+1(z) = ψ(z  x
(4)
n+1
where ψ is a real-valued function that is invariant with respect to any permutation of the input data.
For example  in a linear regression problem  one can take the absolute value of the residual to be a
conformity measure function i.e. ˆRi(z) = |yi − x(cid:62)
The main idea for constructing a conformal conﬁdence set is to consider the typicalness of a candidate
point z measured as

ˆβ(z)|.

ˆβ(z))  

i

ˆπ(z) = ˆπ(Dn+1(z)) := 1 −

1

n + 1

Rank( ˆRn+1(z)) .

(5)

i

ˆβ(z)|.

ˆΓ(α)(xn+1) := {z ∈ R : ˆπ(z) > α} .

If the sequence (xi  yi)i∈[n+1] is exchangeable and identically distributed  then ( ˆRi(yn+1))i∈[n+1] is
also   by the invariance of ˆR w.r.t. permutations of the data. Since the rank of one variable among
an exchangeable and identically distributed sequence is (sub)-uniformly distributed (see [3]) in
{1 ···   n + 1}  we have Pn+1(ˆπ(yn+1) ≤ α) ≤ α for any α in (0  1). This implies that the function
ˆπ takes a small value on atypical data. Classical statistics for hypothesis testing  such as a p-value
function  satisfy such a condition under the null hypothesis (see [12  Lemma 3.3.1]). In particular  this
implies that the desired coverage guarantee in Equation (2) is veriﬁed by the conformal set deﬁned as
(6)
The conformal set gathers the real value z such that ˆπ(z) > α  if and only if ˆRn+1(z) is ranked no
higher than (cid:100)(n + 1)(1 − α)(cid:101)  among ˆRi(z) for all i in [n]. For regression problems where yn+1
lies in a subset of R  obtaining the conformal set ˆΓ(α)(xn+1) in Equation (6) is computationally
challenging. It requires re-ﬁtting the prediction model ˆβ(z) for inﬁnitely many candidates z in R in
order to compute a conformity measure such as ˆRi(z) = |yi − x(cid:62)
Existing Approaches for Computing a Conformal Prediction Set.
In Ridge regression  for any
x in Rp  z (cid:55)→ x(cid:62) ˆβ(z) is a linear function of z  implying that ˆRi(z) is piecewise linear. Exploiting
this fact  an exact conformal set ˆΓ(α)(xn+1) for Ridge regression was efﬁciently constructed in
[18]. Similarly  using the piecewise linearity in z of the Lasso solution  [13] proposed a piecewise
linear homotopy under mild assumptions  when a single input sample point is perturbed. Apart from
these cases of quadratic loss with Ridge and Lasso regularization  where an explicit formula of the
estimator is available  computing such a set is often infeasible. Also  a known drawback of exact path
computation is its exponential complexity in the worst case [7]  and numerical instabilities due to
multiple inversions of potentially ill-conditioned matrices.
Another approach is to split the dataset into a training set - in which the regression model is ﬁtted 
and a calibration set - in which the conformity scores and their ranks are computed. Although this
approach avoids the computational bottleneck of the full conformal prediction framework  statistical
efﬁciencies are lost both in the model ﬁtting stage and in the conformity score rank computation
stage  due to the effect of a reduced sample size. It also adds another layer of randomness  which
may be undesirable for the construction of prediction intervals [13].
A common heuristic approach in the literature is to evaluate the typicalness ˆπ(z) only for an arbitrary
ﬁnite number of grid points. Although the prediction set constructed by those ﬁnite number of
ˆπ(z) might roughly mimic the conformal prediction set  the desirable coverage properties are no
longer maintained. To overcome this issue  [5] proposed a discretization strategy with a more careful
procedure to round the observation vectors  but failed to exactly preserve the 1−α coverage guarantee.
In the appendix  we discuss in detail critical limitations of such an approach.

3

Algorithm 1 -online_homotopy

n+1β where β is an 0-solution for the problem (1) using only Dn

Input: Dn = {(x1  y1) ···   (xn  yn)}  xn+1  [ymin  ymax]  0 < 
Initialization: zt0 = x(cid:62)
repeat

(cid:113) 2
ν ( − 0) if the loss is ν-smooth

ztk+1 = ztk ± s where s =
Get β(ztk+1 ) by minimizing Pztk+1

up to accuracy 0 {warm started with β(ztk )}

until [ymin  ymax] is covered
Return: {ztk   β(ztk )}k∈[T]

3 Homotopy Algorithm

In constructing an exact conformal set  we need to be able to compute the entire path of the model
parameters ˆβ(z); which is obtained after solving the augmented optimization problem in Equation (3) 
for any z in R. In fact  two problems arise. First  even for a single z  ˆβ(z) may not be available
because  in general  the optimization problem cannot be solved exactly [17  Chapter 1]. Second 
except for simple regression problems such as Ridge or Lasso  the entire exact path of ˆβ(z) cannot
be computed inﬁnitely many times.
Our basic idea to circumvent this difﬁculty is to rely on approximate solutions at a given precision
 > 0. Here  we call an -solution any vector β such that its objective value satisﬁes

Pz(β) − Pz( ˆβ(z)) ≤  .

(7)
An -solution can be found efﬁciently  under mild assumptions on the regularity of the function being
optimized. In this section  we show that ﬁnite paths of -solutions can be computed for a wider class
of regression problems. Indeed  it is not necessary to re-calculate a new solution for neighboring
observations - i.e. β(z) and β(z0) have the same performance when z is close to z0. We develop a
precise analysis of this idea. Then  we show how this can be used to effectively approximate the
conformal prediction set in Equation (6) based on exact solution  while preserving the coverage
guarantee.
We recall the dual formulation [20  Chapter 31] of Equation (3):

ˆθ(z) ∈ arg max
θ∈Rn+1

Dz(θ) := −

∗
(cid:96)

∗
(yi −λθi) − (cid:96)

(z −λθn+1) − λΩ

∗

(X

(cid:62)

θ) .

(8)

For a primal/dual pair of vectors (β(z)  θ(z)) in domPz × domDz  the duality gap is deﬁned as

Gapz(β(z)  θ(z)) := Pz(β(z)) − Dz(θ(z)) .

Weak duality ensures that Pz(β(z)) ≥ Dz(θ(z))  which yields an upper bound for the approximation
error of β(z) in Equation (7) i.e.

Pz(β(z)) − Pz( ˆβ(z)) ≤ Gapz(β(z)  θ(z)) .

This will allow us to keep track of the approximation error when the parameters of the objective
function change. Given any β such that Gap(β  θ) ≤  i.e. an -solution for problem (1)  we explore
the candidates for yn+1 with the parameterization of the real line zt deﬁned as

(cid:62)
n+1β .

zt := z0 + t  for t ∈ R and z0 = x

(9)
This additive parameterization was used in [13] for the case of the Lasso. It provides the nice
property that adding (xn+1  z0) as the (n + 1)-th observation does not change the objective value of β
i.e. P (β) = Pz0 (β). Thus  if a vector β is an -solution for P   it will remain so for Pz0. Interestingly 
such a choice is still valid for a sufﬁciently small t. We show that  depending on the regularity of the
loss function  we can precisely derive a range of the parameter t so that β remains a valid -solution
for Pzt when the dataset Dn is augmented with {(xn+1  zt)}.
We deﬁne the variation of the duality gap between real values z and z0 to be
∆G(xn+1  z  z0) := Gapz(β  θ) − Gapz0 (β  θ) .

4

n(cid:88)

i=1

Lemma 1. For any (β  θ) ∈ domPw × domDw for w ∈ {z0  z}  we have

∆G(xn+1  z  z0) = [(cid:96)(z  x

(cid:62)
(cid:62)
n+1β) − (cid:96)(z0  x
n+1β)] + [(cid:96)

∗

∗
(z −λθn+1) − (cid:96)

(z0 −λθn+1)] .

Lemma 1 showed that the variation of the duality gap between z and z0 depends only on the variation
of the loss function (cid:96)  and its conjugate (cid:96)∗. Thus  it is enough to exploit the regularity (e.g. smoothness)
of the loss function in order to obtain an upper bound for the variation of the duality gap (and therefore
the optimization error).

Construction of Dual Feasible Vector. A generic method for producing a dual-feasible vector is
to re-scale the output of the gradient mapping. For a real value z  let β(z) be any primal vector and
let us denote Yz = (y1 ···   yn  z).
Optimality conditions for (3) and (8) implies ˆθ(z) = −∇(cid:96)(Yz  X ˆβ(z))/λ  which suggests we can
make use of [16]

θ(z) :=

max{λt  σ◦

−∇(cid:96)(Yz  Xβ(z))
domΩ∗ (X(cid:62)∇(cid:96)(Yz  Xβ(z)))} ∈ domDz  

(10)

where σ is the support function and σ◦ its polar function. When the regularization is a norm
Ω(·) = (cid:107)·(cid:107)  then σ◦
domΩ∗ is the associated dual norm (cid:107)·(cid:107)∗. When Ω is strongly convex  then the dual
vector in Equation (10) simpliﬁes to θ(z) = −∇(cid:96)(Yz  Xβ(z))/λ.
Using θ(z0) in Equation (10) with z0 = x(cid:62)
n+1β greatly simpliﬁes the expression for the variation of
the duality gap between zt and z0 in Lemma 1 to

∆G(xn+1  zt  z0) = (cid:96)(zt  x

(cid:62)
n+1β) .

This directly follows from the assumptions (cid:96)(z0  z0) = (cid:96)∗(z0  0) = 0 and by construction of the
dual vector θn+1 ∝ ∂2(cid:96)(z0  x(cid:62)
n+1β) = ∂2(cid:96)(z0  z0) = 0. Whence  assuming that the loss function
is ν-smooth (see the appendix for more details and extensions to other regularity assumptions) and
using the parameterization in Equation (9)  we obtain

∆G(xn+1  zt  z0) ≤

ν
2

t2 .

ν
2

(zt − z0)2 =
(cid:112)
(cid:112)

Proposition 1. Assuming that
the gap
∆G(xn+1  zt  z0) are smaller than  for all t in [−
2/ν]. Moreover  assuming that
Gapz0 (β(z0)  θ(z0)) ≤ 0 <   we have (β(z0)  θ(z0)) being a primal/dual -solution for the
optimization problem (3) with augmented data Dn ∪ {(xn+1  zt)} as long as

the loss function (cid:96) is ν-smooth 

the variations of

2/ν 

|zt − z0| ≤

( − 0) =: s .

(cid:114)
(cid:24) ymax − ymin

2
ν

s

(cid:25)

T ≤

Complexity. A given interval [ymin  ymax] can be covered by Algorithm 1 with T steps where

(cid:18) 1

(cid:19)

∈ O

√

.

We can notice that the step sizes s (smooth case) for computing the whole path are independent of
the data and the intermediate solutions. Thus  for computational efﬁciency  the latter can be computed
in parallel or by sequentially warm-starting the initialization. Also  since the grid can be constructed
by decreasing or increasing the value of zt  one can observe that the number of solutions calculated
along the path can be halved by using only β(zt) as an -solution on the whole interval [zt ± s].
Lower Bound. Using the same reasoning when the loss is µ-strongly convex  we have

µ
∆G(xn+1  zt  z0) ≥
2
Hence ∆G(xn+1  zt  z0) >  as soon as |zt − z0| >
approximation errors at any candidate zt  all the step sizes are necessarily of order √.

(zt − z0)2 .

µ ( − 0). Thus  in order to guarantee 

(cid:113) 2

5

(a) Exact conformal prediction set for ridge regression
with one hundred regularization parameters ranging
from λmax = log(p) to λmin = λmax/104  spaced
evenly on a log scale.

(b) Evolution of the conformal set of the proposed
homotopy method with different optimization errors 
spaced evenly on a geometric scale ranging from
max = (cid:107)(y1 ···   yn)(cid:107)2 to min = max/1010.

Figure 1: Illustration of conformal prediction sets at level α = 0.1 with exact solutions and ap-
proximate solutions for ridge regression. We use a synthetic data set generated using sklearn
with X  y = make_regression(n = 100  p = 50). We have chosen the hyperparameter with the
smallest conﬁdence set in Figure (a) to generate Figure (b).

Choice of [ymin  ymax]. We follow the actual practice in the literature [13  Remark 5] and set
ymin = y(1) and ymax = y(n). In that case  we have P(yn+1 ∈ [ymin  ymax]) ≥ 1 − 2/(n + 1). This
implies a loss in the coverage guarantee of 2/(n + 1)  which is negligible when n is sufﬁciently large.

Related Works on Approximate Homotopy. Recent papers [8  16] have developed approximation
path methods when a function is concavely parameterized. Such techniques cannot be used here
since  for any β ∈ Rp  the function z (cid:55)→ Pz(β) is not concave. Thus  it does not ﬁt within their
problem description.
Using homotopy continuation to update an exact Lasso solution in the online setting was performed
by [6  13]. Allowing an approximate solution allows us to extensively generalize those approaches to
a broader class of machine learning tasks  with a variety of regularity assumptions.

4 Practical Computation of a Conformal Prediction Set

We present how to compute a conformal prediction set  based on the approximate homotopy algorithm
in Section 3. We show that the set obtained preserves the coverage guarantee  and tends to the exact
set when the optimization error  decreases to zero. In the case of a smooth loss function  we present
a variant of conformal sets with an approximate solution  which contains the exact conformal set.

4.1 Conformal Sets Directly Based on Approximate Solution

For a real value z  we cannot evaluate ˆπ(z) in Equation (5) in many cases because it depends on the
exact solution ˆβ(z)  which is unknown. Instead  we only have access to a given -solution β(z) and
the corresponding (approximate) conformity measure given as:

(cid:62)
(cid:62)
i β(z)) and Rn+1(z) = ψ(z  x
∀i ∈ [n]  Ri(z) = ψ(yi  x
n+1β(z)) .

(11)
However  for establishing a coverage guarantee  one can note that any estimator that preserves
exchangeability can be used. Whence  we deﬁne

π(z  ) := 1 −

n + 1

1

Rank(Rn+1(z)) 

(12)
Proposition 2. Given a signiﬁcance level α ∈ (0  1) and an optimization tolerance  > 0  if the
observations (xi  yi)i∈[n+1] are exchangeable and identically distributed under probability P  then the
conformal set Γ(α )(xn+1) satisﬁes the coverage guarantee Pn+1(yn+1 ∈ Γ(α )(xn+1)) ≥ 1 − α.

Γ(α )(xn+1) := {z ∈ R : π(z  ) > α} .

6

01234log10(λmax/λ)−1.0−0.50.00.51.01.5RidgeConformalSetsTargetyn+10246810log10(max/)−1.0−0.50.00.51.01.52.0RidgeConformalSetsTargetyn+1Γ(α )ˆΓ(α)Oracle
Split
1e-2
1e-4
1e-6
1e-8

Coverage Length Time (s)
0.9
0.9
0.9
0.9
0.9
0.9

0.59
0.26
2.17
8.02
45.94
312.56

1.685
3.111
1.767
1.727
1.724
1.722

Table 1: Computing a conformal set for a Lasso regression problem on a climate data set NCEP/NCAR
Reanalysis [11] with n = 814 observations and p = 73570 features. On the left  we compare the
time needed to compute the full approximation path with our homotopy strategy  single coordinate
descent (CD) on the full data Dn+1(yn+1)  and an update of the solution after initialization with an
approximate solution using Dn. On the right  we display the coverage  length and time of different
methods averaged over 100 randomly held-out validation data sets.

The conformal prediction set Γ(α )(xn+1) (with an approximate solution) preserves the 1 − α
coverage guarantee and converges to Γ(α 0)(xn+1) = ˆΓ(α)(xn+1) (with an exact solution) when the
optimization error decreases to zero. It is also easier to compute in the sense that only a ﬁnite number
of candidates z need to be evaluated. Indeed  as soon as an approximate solution β(z) is allowed 
we have shown in Section 3 that a solution update is not necessary for neighboring observation
candidates.
We consider the parameterization in Equation (9). It holds that

Γ(α ) = {z ∈ R : π(z  ) > α} = {zt : t ∈ R  π(zt  ) > α} .

∀z ∈ [ymin  ymax] ∃k ∈ [T] such that Gapz(β(ztk )  θ(ztk )) ≤  .

Using Algorithm 1  we can build a set {zt1 ···   ztT} that covers [ymin  ymax] with -solutions i.e. :
Using the classical conformity measure ˆRi(z) = |yi − x(cid:62)
ˆβ(z)| and computing a piecewise constant
approximation of the solution path t (cid:55)→ ˆβ(zt) with the set {β(ztk ) : k ∈ [T]}  we have
(cid:62)
n+1β(ztk ) ± Q1−α(ztk )] .

Γ(α ) ∩ [ymin  ymax] =

(cid:91)

i

[ztk   ztk+1] ∩ [x

k∈[T]

where Q1−α(z) is the (1 − α)-quantile of the sequence of approximate residuals (Ri(z))i∈[n+1].
Details and extensions to the more general cases of conformity measures are discussed in the appendix.

4.2 Wrapping the Exact Conformal Set

Previously  we showed that a full conformal set can be efﬁciently computed with an approximate
solution  and it converges to the conformal set with an exact solution when the optimization error
decreases to zero. When the loss function is smooth and  under a gradient-based conformity measure
(introduced below)  we provide a stronger guarantee that the exact conformal set can be included in a
conformal set  using only approximate solutions. For this  we show how the conformity measure can
be bounded w.r.t. to the optimization error  when the input observation z changes.

Gradient based Conformity Measures. The separability of the loss function implies that the
coordinate-wise absolute value of the gradient of the loss function preserves the excheangeability of
the data  and then the coverage guarantee. Whence it can be safely used as a conformity measure i.e.
(13)
Using Equation (13)  we show how the function ˆπ can be approximated from above and below  thanks
to a ﬁne bound on the dual optimal solution [15]  which is related to the gradient of the loss function.

ˆR:(z) = |∇(cid:96)(Yz  X ˆβ(z))| 

R:(z) = |∇(cid:96)(Yz  Xβ(z))| .

7

10−210−410−610−8Dualitygap05101520253035Time(s)HomotopyCDonDn+1(yn+1)CDinitializedwithβ(Dn)(a) Linear regression with (cid:96)1 regularization on Diabetes
dataset (n = 442  p = 10).

(b) Logcosh regression with (cid:96)2
Boston dataset (n = 506  p = 13).

2 regularization on

Figure 2: Length of the conformal prediction sets at different coverage level α ∈ {0.1  0.2 ···   0.9}.
For all α  we display the average over 100 repetitions of randomly held-out validation data sets.

Lemma 2. If the loss function (cid:96)(z ·) is ν-smooth  for any real value z  we have

(cid:107)θ(z) − ˆθ(z)(cid:107)2 ≤

2ν
λ2 Gapz(β(z)  θ(z)) 

∀(β(z)  θ(z)) ∈ domPz × domDz .

Using Equation (13) and further assuming that the dual vector θ(z) constructed in Equation (10)
coincides 1 with −∇(cid:96)(Yz  Xβ(z))/λ in domDz  we have ˆR:(z) = (cid:107)λˆθ(z)(cid:107) and R:(z) = (cid:107)λθ(z)(cid:107).
Thus  combining the triangle inequality and Lemma 2 we have

∀i ∈ [n + 1]  (Ri(z) − ˆRi(z))2 ≤ (cid:107)R:(z) − ˆR:(z)(cid:107)2 = λ2(cid:107)θ(z) − ˆθ(z)(cid:107)2 ≤ 2ν  

where the last inequality holds as soon as we can maintain Gapz(β(z)  θ(z)) to be smaller than   for
any z in R. Whence  ˆRi(z) belongs to [Ri(z) ± √2ν] for any i in [n + 1]. Noting that

ˆπ(z) = 1 −

1

n + 1

Rank( ˆRn+1(z)) =

1

n + 1

i=1

1 ˆRi(z)≥ ˆRn+1(z)  

n+1(cid:88)

n+1(cid:88)

i=1

n+1(cid:88)

i=1

the function ˆπ can be easily approximated from above and below by the functions π(z  ) and π(z  ) 
which do not depend on the exact solution and are deﬁned as:

π(z  ) =

1

n + 1

1Ri(z)≥Rn+1(z)+2

√

2ν 

π(z  ) =

1

n + 1

1Ri(z)≥Rn+1(z)−2

√

2ν .

(α )

Γ

(α ) where
= {z ∈ R : π(z  ) > α} .

Proposition 3. We assume that the loss function is ν-smooth and that we use a gradient based
conformity measure (13). Then  we have π(z  ) ≤ ˆπ(z) ≤ π(z  ) and the approximated lower and
upper bounds of the exact conformal set are Γ(α ) ⊂ ˆΓ(α) ⊂ Γ

Γ(α ) = {z ∈ R : π(z  ) > α} 
(cid:91)
(cid:91)

∩ [ymin  ymax] =

In the baseline case of quadratic loss  such sets can be easily computed as
−
(cid:62)
1−α(tk)]  
n+1β(ztk ) ± Q
(cid:62)
n+1β(ztk ) ± Q+

[ztk   ztk+1] ∩ [x
1−α(tk)) as the (1− α)-quantile of the sequence of shifted
approximate residuals (Ri(ztk ) − 2√2ν)i∈[n+1] (resp. (Ri(ztk ) + 2√2ν)i∈[n+1]) corresponding
to the approximate solution β(ztk ) for k in [T].

k∈[T]
−
1−α(tk) (resp. Q+

Γ(α ) ∩ [ymin  ymax] =

where we have denoted Q

[ztk   ztk+1] ∩ [x

1−α(tk)]  

(α )

Γ

k∈[T]

1This holds whenever Ω is strongly convex or its domain is bounded. Also  one can guarantee this condition
when β(z) is build using any converging iterative algorithm  with sufﬁcient iterations  for solving Equation (3).

8

0.20.40.60.8Coveragelevelα0.51.01.52.0LengthofΓ(xn+1)OracleSplit1e-21e-41e-61e-80.20.40.60.8Coveragelevelα0.00.51.01.52.0LengthofΓ(xn+1)OracleSplit1e-21e-41e-61e-8Smooth Chebychev Approx.
Coverage
Length
Time (s)

Linex regression
Coverage
Length
Time (s)

Oracle

Split

1e-2

1e-4

1e-6

1e-8

0.92
1.940
0.019

0.91
2.189
0.013

0.95
2.271
0.016

0.92
1.998
0.073

0.92
1.990
0.409

0.92
1.987
3.742

0.92
1.981
36.977

0.93
2.447
0.012

0.91
2.231
0.050

0.91
2.209
0.234

0.91
2.205
2.054

0.91
2.199
20.712

Table 2: Computing a conformal set for a logcosh (resp. linex) regression problem regularized
with a Ridge penalty on the Boston (resp. Diabetes) dataset with n = 506 observations and p = 13
features (resp. n = 442 and p = 10). We display the coverage  length and time of the different
methods  averaged over 100 randomly held-out validation data sets.

5 Numerical Experiments

We illustrate the approximation of a full conformal prediction set for both linear and non-linear
regression problems  using synthetic and real datasets that are publicly available in sklearn. All
experiments were conducted with a coverage level of 0.9 (α = 0.1) and a regularization parameter
selected by cross-validation on a randomly separated training set (for real data  we used 33% of the
data).
In the case of Ridge regression  exact and full conformal prediction sets can be computed without any
assumptions [18]. We show in Figure 1  the conformal sets w.r.t. different regularization parameters
λ  and our proposed method based on an approximated solution for different optimization errors. The
results indicate that high precision is not necessary to obtain a conformal set close to the exact one.
For other problem formulations  we deﬁne an Oracle as the set [x(cid:62)
ˆβ(yn+1) ± ˆQ1−α(yn+1)]
obtained from the estimator trained with machine precision on the oracle data Dn+1(yn+1) (the
target variable yn+1 is not available in practice). For comparison  we display the average over 100
repetitions of randomly held-out validation data sets  the empirical coverage guarantee  the length 
and time needed to compute the conformal set with splitting and with our approach.
We illustrated in Table 1 the computational cost of our proposed homotopy for Lasso regression 
using vanilla coordinate descent (CD) optimization solvers in sklearn [19]. For a large range of
duality gap accuracies   the computational time of our method is roughly the same as a single run of
CD on the full data set. However  when  becomes very small (≈ 10−8)  we lose computational time
efﬁciency due to large complexity T. This is visible in regression problems with non-quadratic loss
functions Table 2.
The computational times depend only on the data ﬁtting part and the computation of the conformity
score functions. Thus  the computational efﬁciency is independent of the coverage level α. We show
in Figure 2  the variations of the length of the conformal prediction set for different coverage level.
Overall  the results indicate that the homotopy method provides valid and near-perfect coverage 
regardless of the optimization error . The lengths of the conﬁdence sets generated by homotopy
methods gradually increase as  increases  but all of the sets are consistently smaller than those of
splitting approaches. Our experiments showed that high accuracy has only limited beneﬁts.

n+1

Acknowledgments

We would like to thank the reviewers for their valuable feedbacks and detailed comments which
contributed to improve the quality of this paper. This work was partially supported by MEXT
KAKENHI (17H00758  16H06538)  JST CREST (JPMJCR1502)  RIKEN Center for Advanced
Intelligence Project  and JST support program for starting up innovation-hub on materials research
by information integration initiative.

9

References
[1] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Optimization with sparsity-inducing penalties.

Foundations and Trends in Machine Learning  2012.

[2] V. Balasubramanian  S-S. Ho  and V. Vovk. Conformal prediction for reliable machine learning:

theory  adaptations and applications. Elsevier  2014.

[3] J. Bröcker and H. Kantz. The concept of exchangeability in ensemble forecasting. Nonlinear

Processes in Geophysics  2011.

[4] Y-C. Chang and W-L. Hung. Linex loss functions with applications to determining the optimum

process parameters. Quality & Quantity  2007.

[5] W. Chen  K-J. Chun  and R. F. Barber. Discretized conformal prediction for efﬁcient distribution-

free inference. Stat  2018.

[6] P. Garrigues and L. E. Ghaoui. An homotopy algorithm for the lasso with online observations.

In Advances in neural information processing systems  pages 489–496  2009.

[7] B. Gärtner  M. Jaggi  and C. Maria. An exponential lower bound on the complexity of

regularization paths. Journal of Computational Geometry  2012.

[8] J. Giesen  J. K. Müller  S. Laue  and S. Swiercy. Approximating concavely parameterized

optimization problems. Advances in neural information processing systems  2012.

[9] M. Gruber. Regression estimators: A comparative study. JHU Press  2010.
[10] A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal

problems. Technometrics  1970.

[11] E. Kalnay  M. Kanamitsu  R. Kistler  W. Collins  D. Deaven  L. Gandin  M. Iredell  S. Saha 
G. White  J. Woollen  et al. The NCEP/NCAR 40-year reanalysis project. Bulletin of the
American meteorological Society  1996.

[12] E. L. Lehmann and J. P. Romano. Testing statistical hypotheses. Springer Science & Business

Media  2006.

[13] J. Lei. Fast exact conformalization of lasso using piecewise linear homotopy. Biometrika  2019.
[14] J. Lei  M. G’Sell  A. Rinaldo  R. J. Tibshirani  and L. Wasserman. Distribution-free predictive

inference for regression. Journal of the American Statistical Association  2018.

[15] E. Ndiaye  O. Fercoq  A. Gramfort  and J. Salmon. Gap safe screening rules for sparsity

enforcing penalties. J. Mach. Learn. Res  2017.

[16] E. Ndiaye  T. Le  O. Fercoq  J. Salmon  and I. Takeuchi. Safe grid search with optimal

complexity. ICML  2019.

[17] Y. Nesterov. Introductory lectures on convex optimization. Kluwer Academic Publishers  2004.
[18] I. Nouretdinov  T. Melluish  and V. Vovk. Ridge regression conﬁdence machine. ICML  2001.
[19] F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel 
P. Prettenhofer  R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher 
M. Perrot  and E. Duchesnay. Scikit-learn: Machine learning in Python. J. Mach. Learn. Res 
2011.

[20] R. T. Rockafellar. Convex analysis. Princeton University Press  1997.
[21] G. Shafer and V. Vovk. A tutorial on conformal prediction. Journal of Machine Learning

Research  2008.

[22] S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge university press  2014.

[23] V. Vovk  A. Gammerman  and G. Shafer. Algorithmic learning in a random world. Springer 

2005.

10

,Noga Alon
Nicolò Cesa-Bianchi
Claudio Gentile
Yishay Mansour
Xing Yan
Weizhong Zhang
Lin Ma
Wei Liu
Qi Wu
Eugene Ndiaye
Ichiro Takeuchi