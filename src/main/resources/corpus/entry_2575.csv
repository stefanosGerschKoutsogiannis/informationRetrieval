2016,Improved Dropout for Shallow and Deep Learning,Dropout has been witnessed with great success in training deep neural networks by independently   zeroing  out the outputs of neurons at random. It has also received a surge of interest for shallow learning  e.g.  logistic regression.  However  the independent  sampling for dropout could be suboptimal for the sake of convergence. In this paper  we propose to use multinomial  sampling for dropout  i.e.  sampling features or neurons according to  a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities  we analyze the shallow learning with multinomial  dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound  we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving  distribution of neurons in deep learning  we propose an efficient adaptive  dropout (named \textbf{evolutional dropout}) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve  not only much faster convergence and  but also a smaller testing error than the standard dropout.  For example  on the CIFAR-100 data  the evolutional  dropout achieves relative improvements  over 10\% on the prediction performance and over 50\% on the convergence speed compared to the standard dropout.,Improved Dropout for Shallow and Deep Learning

Zhe Li1  Boqing Gong2  Tianbao Yang1
1The University of Iowa  Iowa city  IA 52245

bgong@crcv.ucf.edu

2University of Central Florida  Orlando  FL 32816

{zhe-li-1 tianbao-yang}@uiowa.edu

Abstract

Dropout has been witnessed with great success in training deep neural networks by
independently zeroing out the outputs of neurons at random. It has also received
a surge of interest for shallow learning  e.g.  logistic regression. However  the
independent sampling for dropout could be suboptimal for the sake of conver-
gence. In this paper  we propose to use multinomial sampling for dropout  i.e. 
sampling features or neurons according to a multinomial distribution with different
probabilities for different features/neurons. To exhibit the optimal dropout proba-
bilities  we analyze the shallow learning with multinomial dropout and establish
the risk bound for stochastic optimization. By minimizing a sampling dependent
factor in the risk bound  we obtain a distribution-dependent dropout with sampling
probabilities dependent on the second order statistics of the data distribution. To
tackle the issue of evolving distribution of neurons in deep learning  we propose
an efﬁcient adaptive dropout (named evolutional dropout) that computes the sam-
pling probabilities on-the-ﬂy from a mini-batch of examples. Empirical studies on
several benchmark datasets demonstrate that the proposed dropouts achieve not
only much faster convergence and but also a smaller testing error than the standard
dropout. For example  on the CIFAR-100 data  the evolutional dropout achieves
relative improvements over 10% on the prediction performance and over 50% on
the convergence speed compared to the standard dropout.

1

Introduction

Dropout has been widely used to avoid overﬁtting of deep neural networks with a large number of
parameters [9  16]  which usually identically and independently at random samples neurons and sets
their outputs to be zeros. Extensive experiments [4] have shown that dropout can help obtain the
state-of-the-art performance on a range of benchmark data sets. Recently  dropout has also been
found to improve the performance of logistic regression and other single-layer models for natural
language tasks such as document classiﬁcation and named entity recognition [21].
In this paper  instead of identically and independently at random zeroing out features or neurons  we
propose to use multinomial sampling for dropout  i.e.  sampling features or neurons according to
a multinomial distribution with different probabilities for different features/neurons. Intuitively  it
makes more sense to use non-uniform multinomial sampling than identical and independent sampling
for different features/neurons. For example  in shallow learning if input features are centered  we
can drop out features with small variance more frequently or completely allowing the training to
focus on more important features and consequentially enabling faster convergence. To justify the
multinomial sampling for dropout and reveal the optimal sampling probabilities  we conduct a
rigorous analysis on the risk bound of shallow learning by stochastic optimization with multinomial
dropout  and demonstrate that a distribution-dependent dropout leads to a smaller expected risk (i.e. 
faster convergence and smaller generalization error).

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Inspired by the distribution-dependent dropout  we propose a data-dependent dropout for shallow
learning  and an evolutional dropout for deep learning. For shallow learning  the sampling probabili-
ties are computed from the second order statistics of features of the training data. For deep learning 
the sampling probabilities of dropout for a layer are computed on-the-ﬂy from the second-order
statistics of the layer’s outputs based on a mini-batch of examples. This is particularly suited for deep
learning because (i) the distribution of each layer’s outputs is evolving over time  which is known
as internal covariate shift [5]; (ii) passing through all the training data in deep neural networks (in
particular deep convolutional neural networks) is much more expensive than through a mini-batch
of examples. For a mini-batch of examples  we can leverage parallel computing architectures to
accelerate the computation of sampling probabilities.
We note that the proposed evolutional dropout achieves similar effect to the batch normalization
technique (Z-normalization based on a mini-batch of examples) [5] but with different ﬂavors. Both
approaches can be considered to tackle the issue of internal covariate shift for accelerating the
convergence. Batch normalization tackles the issue by normalizing the output of neurons to zero
mean and unit variance and then performing dropout independently 1. In contrast  our proposed
evolutional dropout tackles this issue from another perspective by exploiting a distribution-dependent
dropout  which adapts the sampling probabilities to the evolving distribution of a layer’s outputs. In
other words  it uses normalized sampling probabilities based on the second order statistics of internal
distributions. Indeed  we notice that for shallow learning with Z-normalization (normalizing each
feature to zero mean and unit variance) the proposed data-dependent dropout reduces to uniform
dropout that acts similarly to the standard dropout. Because of this connection  the presented
theoretical analysis also sheds some lights on the power of batch normalization from the angle
of theory. Compared to batch normalization  the proposed distribution-dependent dropout is still
attractive because (i) it is rooted in theoretical analysis of the risk bound; (ii) it introduces no
additional parameters and layers without complicating the back-propagation and the inference; (iii) it
facilitates further research because its shares the same mathematical foundation as standard dropout
(e.g.  equivalent to a form of data-dependent regularizer) [18].
We summarize the main contributions of the paper below.

• We propose a multinomial dropout and demonstrate that a distribution-dependent dropout
leads to a faster convergence and a smaller generalization error through the risk bound
analysis for shallow learning.
• We propose an efﬁcient evolutional dropout for deep learning based on the distribution-
• We justify the proposed dropouts for both shallow learning and deep learning by experimen-

dependent dropout.

tal results on several benchmark datasets.

In the remainder  we ﬁrst review some related work and preliminaries. We present the main results in
Section 4 and experimental results in Section 5.

2 Related Work

In this section  we review some related work on dropout and optimization algorithms for deep
learning.
Dropout is a simple yet effective technique to prevent overﬁtting in training deep neural networks [16].
It has received much attention recently from researchers to study its practical and theoretical properties.
Notably  Wager et al. [18]  Baldi and Sadowski [2] have analyzed the dropout from a theoretical
viewpoint and found that dropout is equivalent to a data-dependent regularizer. The most simple
form of dropout is to multiply hidden units by i.i.d Bernoulli noise. Several recent works also found
that using other types of noise works as well as Bernoulli noise (e.g.  Gaussian noise)  which could
lead to a better approximation of the marginalized loss [20  7]. Some works tried to optimize the
hyper-parameters that deﬁne the noise level in a Bayesian framework [23  7]. Graham et al. [3] used
the same noise across a batch of examples in order to speed up the computation. The adaptive dropout
proposed in[1] overlays a binary belief network over a neural netowrk  incurring more computational
overhead to dropout because one has to train the additional binary belief network. In constrast 

1The author also reported that in some cases dropout is even not necessary

2

the present work proposes a new dropout with noise sampled according to distribution-dependent
sampling probabilities. To the best of our knowledge  this is the ﬁrst work that rigorously studies this
type of dropout with theoretical analysis of the risk bound. It is demonstrated that the new dropout
can improve the speed of convergence.
Stochastic gradient descent with back-propagation has been used a lot in optimizing deep neural
networks. However  it is notorious for its slow convergence especially for deep learning. Recently 
there emerge a battery of studies trying to accelearte the optimization of deep learning [17  12  22  5  6] 
which tackle the problem from different perspectives. Among them  we notice that the developed
evolutional dropout for deep learning achieves similar effect as batch normalization [5] addressing
the internal covariate shift issue (i.e.  evolving distributions of internal hidden units).

3 Preliminaries

In this section  we present some preliminaries  including the framework of risk minimization in
machine learning and learning with dropout noise. We also introduce the multinomial dropout  which
allows us to construct a distribution-dependent dropout as revealed in the next section.
Let (x  y) denote a feature vector and a label  where x ∈ Rd and y ∈ Y. Denote by P the joint
distribution of (x  y) and denote by D the marginal distribution of x. The goal of risk minimization
is to learn a prediction function f (x) that minimizes the expected loss  i.e.  minf∈H EP [(cid:96)(f (x)  y)] 
where (cid:96)(z  y) is a loss function (e.g.  the logistic loss) that measures the inconsistency between z
and y and H is a class of prediction functions. In deep learning  the prediction function f (x) is
determined by a deep neural network. In shallow learning  one might be interested in learning a linear
model f (x) = w(cid:62)x. In the following presentation  the analysis will focus on the risk minimization
of a linear model  i.e. 

L(w) (cid:44) EP [(cid:96)(w(cid:62)x  y)]

min
w∈Rd

(1)

(2)

1

In this paper  we are interested in learning with dropout  i.e.  the feature vector x is corrupted by
a dropout noise. In particular  let  ∼ M denote a dropout noise vector of dimension d  and the

corrupted feature vector is given by(cid:98)x = x ◦   where the operator ◦ represents the element-wise
multiplication. Let (cid:98)P denote the joint distribution of the new data ((cid:98)x  y) and (cid:98)D denote the marginal
distribution of(cid:98)x. With the corrupted data  the risk minimization becomes

(cid:98)L(w) (cid:44) E(cid:98)P [(cid:96)(w(cid:62)(x ◦ )  y)]
In standard dropout [18  4]  the entries of the noise vector  are sampled independently according
1−δ ) = 1 − δ  i.e.  features are dropped with a probability δ and
to Pr(j = 0) = δ and Pr(j = 1
1−δ   where bj ∈ {0  1}  j ∈ [d]
scaled by 1
ensure that E[(cid:98)x] = x. It is obvious that using the standard dropout different features will have equal
are i.i.d Bernoulli random variables with Pr(bj = 1) = 1 − δ. The scaling factor
1−δ is added to

1−δ with a probability 1 − δ. We can also write j = bj

min
w∈Rd

i = mi
kpi

  i ∈ [d] and {m1  . . .   md} follow a multinomial distribution M ult(p1  . . .   pd; k) with

probabilities to be dropped out or to be selected independently. However  in practice some features
could be more informative than the others for learning purpose. Therefore  it makes more sense to
assign different sampling probabilities for different features and make the features compete with each
other.
To this end  we introduce the following multinomial dropout.

Deﬁnition 1. (Multinomial Dropout) A multinomial dropout is deﬁned as (cid:98)x = x ◦   where
(cid:80)d
i=1 pi = 1 and pi ≥ 0.
pi. As in the standard dropout  the normalization by kpi is to ensure that E[(cid:98)x] = x. The parameter k

Remark: The multinomial dropout allows us to use non-uniform sampling probabilities p1  . . .   pd
for different features. The value of mi is the number of times that the i-th feature is selected in k
independent trials of selection. In each trial  the probability that the i-th feature is selected is given by
plays the same role as the parameter 1 − δ in standard dropout  which controls the number of features
to be dropped. In particular  the expected total number of the kept features using multinomial dropout
is k and that using standard dropout is d(1 − δ). In the sequel  to make fair comparison between

3

the two dropouts  we let k = d(1 − δ). In this case  when a uniform distribution pi = 1/d is used
in multinomial dropout to which we refer as uniform dropout  then i = mi
1−δ   which acts similarly
to the standard dropout using i.i.d Bernoulli random variables. Note that another choice to make
the sampling probabilities different is still using i.i.d Bernoulli random variables but with different
probabilities for different features. However  multinomial dropout is more suitable because (i) it is
easy to control the level of dropout by varying the value of k; (ii) it gives rise to natural competition
i pi = 1; (iii) it allows us to minimize the sampling

among features because of the constraint(cid:80)

dependent risk bound for obtaining a better distribution than uniform sampling.

Dropout is a data-dependent regularizer Dropout as a regularizer has been studied in [18  2] for
logistic regression  which is stated in the following proposition for ease of discussion later.
Proposition 1. If (cid:96)(z  y) = log(1 + exp(−yz))  then

E(cid:98)P [(cid:96)(w(cid:62)(cid:98)x  y)] = EP [(cid:96)(w(cid:62)x  y)] + RD M(w)

where M denotes the distribution of  and RD M(w) = ED M
Remark: It is notable that RD M ≥ 0 due to the Jensen inequality. Using the second order Taylor
expansion  [18] showed that the following approximation of RD M(w) is easy to manipulate and
understand:

2 )+exp(−w(cid:62) x◦
2 )
exp(w(cid:62)x/2)+exp(−w(cid:62)x/2)

log exp(w(cid:62) x◦

.

(cid:104)

(3)

(cid:105)

(4)

(cid:98)RD M(w) =

ED[q(w(cid:62)x)(1 − q(w(cid:62)x))w(cid:62)CM(x ◦ )w]

2

1

where q(w(cid:62)x) =
1+exp(−w(cid:62)x/2)  and CM denotes the covariance matrix in terms of . In particular 
if  is the standard dropout noise  then CM[x ◦ ] = diag(x2
dδ/(1 − δ))  where
diag(s1  . . .   sn) denotes a d×d diagonal matrix with the i-th entry equal to si. If  is the multinomial
dropout noise in Deﬁnition 1  we have

1δ/(1 − δ)  . . .   x2

CM[x ◦ ] =

1
k

diag(x2

i /pi) − 1
k

xx(cid:62)

(5)

4 Learning with Multinomial Dropout

In this section  we analyze a stochastic optimization approach for minimizing the dropout loss
in (2). Assume the sampling probabilities are known. We ﬁrst obtain a risk bound of learning with
multinomial dropout for stochastic optimization. Then we try to minimize the factors in the risk
bound that depend on the sampling probabilities. We would like to emphasize that our goal here is
not to show that using dropout would render a smaller risk than without using dropout  but rather
focus on the impact of different sampling probabilities on the risk. Let the initial solution be w1. At
the iteration t  we sample (xt  yt) ∼ P and t ∼ M as in Deﬁnition 1 and then update the model by
(6)
where ∇(cid:96) denotes the (sub)gradient in terms of wt and ηt is a step size. Suppose we run the stochastic
t=1 wt.
We note that another approach of learning with dropout is to minimize the empirical risk by marginal-
izing out the dropout noise  i.e.  replacing the true expectations EP and ED in (3) with empirical
expectations over a set of samples (x1  y1)  . . .   (xn  yn) denoted by EPn and EDn. Since the
data dependent regularizer RDn M(w) is difﬁcult to compute  one usually uses an approximation

optimization by n steps (i.e.  using n examples) and compute the ﬁnal solution as (cid:98)wn = 1

(cid:98)RDn M(w) (e.g.  as in (4)) in place of RDn M(w). However  the resulting problem is a non-convex

wt+1 = wt − ηt∇(cid:96)(w(cid:62)

t (xt ◦ t)  yt)

optimization  which together with the approximation error would make the risk analysis much more
involved. In contrast  the update in (6) can be considered as a stochastic gradient descent update
for solving the convex optimization problem in (2)  allowing us to establish the risk bound based
on previous results of stochastic gradient descent for risk minimization [14  15]. Nonetheless  this
restriction does not lose the generality. Indeed  stochastic optimization is usually employed for
solving empirical loss minimization in big data and deep learning.

The following theorem establishes a risk bound of (cid:98)wn in expectation.

(cid:80)n

n

4

Theorem 1. Let L(w) be the expected risk of w deﬁned in (1). Assume E(cid:98)D[(cid:107)x ◦ (cid:107)2
2] ≤ B2 and
(cid:96)(z  y) is G-Lipschitz continuous. For any (cid:107)w∗(cid:107)2 ≤ r  by appropriately choosing η  we can have

E[L((cid:98)wn) + RD M((cid:98)wn)] ≤ L(w∗) + RD M(w∗) +

GBr√
n

where E[·] is taking expectation over the randomness in (xt  yt  t)  t = 1  . . .   n.
Remark: In the above theorem  we can choose w∗ to be the best model that minimizes the expected
risk in (1). Since RD M (w) ≥ 0  the upper bound in the theorem above is also the upper bound of

the risk of (cid:98)wn  i.e.  L((cid:98)wn)  in expectation. The proof of the above theorem follows the standard

analysis of stochastic gradient descent. The detailed proof of theorem is included in the appendix.

4.1 Distribution Dependent Dropout

Next  we consider the sampling dependent factors in the risk bounds. From Theorem 1  we can
see that there are two terms that depend on the sampling probabilities  i.e.  B2 - the upper bound

2]  and RD M(w∗) − RD M((cid:98)wn) ≤ RD M(w∗). We note that the second term also

2] and
present the discussion on minimizing RD M(w∗) later. From Theorem 1  we can see that minimizing
2] would lead to not only a smaller risk (given the same number of total examples  smaller
2] gives a smaller risk bound) but also a faster convergence (with the same number of

of E(cid:98)D[(cid:107)x ◦ (cid:107)2
depends on w∗ and(cid:98)wn  which is more difﬁcult to optimize. We ﬁrst try to minimize E(cid:98)D[(cid:107)x◦(cid:107)2
E(cid:98)D[(cid:107)x◦ (cid:107)2
E(cid:98)D[(cid:107)x ◦ (cid:107)2
iterations  smaller E(cid:98)D[(cid:107)x ◦ (cid:107)2
Due to the limited space  the proofs of Proposition 2  3  4 are included in supplement. The following
proposition simpliﬁes the expectation E(cid:98)D[(cid:107)x ◦ (cid:107)2
2].
Proposition 2. Let  follow the distribution M deﬁned in Deﬁnition 1. Then

2] gives a smaller optimization error).

d(cid:88)

i=1

1
k

k − 1
k

1
pi

i ] +

2] =

ED[x2

E(cid:98)D[(cid:107)x ◦ (cid:107)2
Given the expression of E(cid:98)D[(cid:107)x ◦ (cid:107)2
(cid:112)ED[x2
Proposition 3. The solution to p∗ = arg minp≥0 p(cid:62)1=1 E(cid:98)D[(cid:107)x ◦ (cid:107)2
(cid:113)
(cid:80)d

following result.

  i = 1  . . .   d

p∗
i =

i ]

j=1

ED[x2
j ]

d(cid:88)

i=1

ED[x2
i ]

2] is given by

(7)

(8)

(9)

2] in Proposition 2  we can minimize it over p  leading to the

1

(cid:16)(cid:80)d

Next  we examine RD M(w∗). Since direct manipulation on RD M(w∗) is difﬁcult  we try to

minimize the second order Taylor expansion (cid:98)RD M(w∗) for logistic loss. The following theorem
establishes an upper bound of (cid:98)RD M(w∗).
Proposition 4. Let  follow the distribution M deﬁned in Deﬁnition 1. We have (cid:98)RD M(w∗) ≤
(cid:17)
8k(cid:107)w∗(cid:107)2
Remark: By minimizing the relaxed upper bound in Proposition 4  we obtain the same sampling
probabilities as in (8). We note that a tighter upper bound can be established  however  which will
yield sampling probabilities dependent on the unknown w∗.

In summary  using the probabilities in (8)  we can reduce both E(cid:98)D[(cid:107)x ◦ (cid:107)2

2] and RD M(w∗) in the
risk bound  leading to a faster convergence and a smaller generalization error. In practice  we can use
empirical second-order statistics to compute the probabilities  i.e. 

− ED[(cid:107)x(cid:107)2
2]

ED[x2
i ]

i=1

pi

2

(cid:113) 1
(cid:80)n
(cid:113) 1
(cid:80)n

n

i(cid:48)=1

n

j=1[[xj]2
i ]

j=1[[xj]2
i(cid:48)]

(cid:80)d

pi =

where [xj]i denotes the i-th feature of the j-th example  which gives us a data-dependent dropout.
We state it formally in the following deﬁnition.

5

Evolutional Dropout for Deep Learning
1  . . .   xl

Input: a batch of outputs of a layer: X l = (xl
and dropout level parameter k ∈ [0  d]

m)

Output: (cid:98)X l = X l ◦ Σl

Compute sampling probabilities by (10)
For j = 1  . . .   m

1  . . .   pl

d; k)

Sample ml

Construct l

Let Σl = (l

j ∼ M ult(pl
j =
1  . . .   l

ml
j

kpl ∈ Rd  where pl = (pl
m) and compute (cid:98)X l = X l ◦ Σl

1  . . .   pl

d)(cid:62)

Figure 1: Evolutional Dropout applied to a layer over a mini-batch

data-dependent dropout is deﬁned as(cid:98)x = x ◦   where i = mi

Deﬁnition 2. (Data-dependent Dropout) Given a set of training examples (x1  y1)  . . .   (xn  yn). A
  i ∈ [d] and {m1  . . .   md} follow a

multinomial distribution M ult(p1  . . .   pd; k) with pi given by (9).
Remark: Note that if the data is normalized such that each feature has zero mean and unit variance
(i.e.  according to Z-normliazation)  the data-dependent dropout reduces to uniform dropout. It
implies that the data-dependent dropout achieves similar effect as Z-normalization plus uniform
dropout. In this sense  our theoretical analysis also explains why Z-normalization usually speeds up
the training [13].

kpi

4.2 Evolutional Dropout for Deep Learning

1  . . .   xl

Next  we discuss how to implement the distribution-dependent dropout for deep learning. In training
deep neural networks  the dropout is usually added to the intermediate layers (e.g.  fully connected
layers and convolutional layers). Let xl = (xl
d) denote the outputs of the l-th layer (with the
index of data omitted). Adding dropout to this layer is equivalent to multiplying xl by a dropout

noise vector l  i.e.  feeding (cid:98)xl = xl ◦ l as the input to the next layer. Inspired by the data-
dependent dropout  we can generate l according to a distribution given in Deﬁnition 1 with sampling
n} similar to that (9). However  deep learning is usually
probabilities pl
trained with big data and a deep neural network is optimized by mini-batch stochastic gradient
descent. Therefore  at each iteration it would be too expensive to afford the computation to pass
through all examples. To address this issue  we propose to use a mini-batch of examples to calculate
the second-order statistics similar to what was done in batch normalization. Let X l = (xl
m)
denote the outputs of the l-th layer for a mini-batch of m examples. Then we can calculate the
probabilities for dropout by

i computed from {xl

1  . . .   xl

1  . . .   xl

(cid:113) 1
(cid:80)m
(cid:113) 1
(cid:80)m

m

i(cid:48)=1

m

(cid:80)d

j=1[[xl

j]2
i ]

j=1[[xl

j]2
i(cid:48)]

pl
i =

  i = 1  . . .   d

(10)

which deﬁne the evolutional dropout named as such because the probabilities pl
i will also evolve as
the the distribution of the layer’s outputs evolve. We describe the evolutional dropout as applied to a
layer of a deep neural network in Figure 1.
Finally  we would like to compare the evolutional dropout with batch normalization. Similar to batch
normalization  evolutional dropout can also address the internal covariate shift issue by adapting
the sampling probabilities to the evolving distribution of layers’ outputs. However  different from
batch normalization  evolutional dropout is a randomized technique  which enjoys many beneﬁts
as standard dropout including (i) the back-propagation is simple to implement (just multiplying the

gradient of (cid:98)X l by the dropout mask to get the gradient of X l); (ii) the inference (i.e.  testing) remains

the same 2; (iii) it is equivalent to a data-dependent regularizer with a clear mathematical explanation;
2Different from some implementations for standard dropout which doest no scale by 1/(1 − δ) in training

but scale by 1 − δ in testing  here we do scale in training and thus do not need any scaling in testing.

6

(iv) it prevents units from co-adapting of neurons  which facilitate generalization. Moreover  the
evolutional dropout has its root in distribution-dependent dropout  which has theoretical guarantee to
accelerate the convergence and improve the generalization for shallow learning.

5 Experimental Results

In the section  we present some experimental results to justify the proposed dropouts. In all ex-
periments  we set δ = 0.5 in the standard dropout and k = 0.5d in the proposed dropouts for fair
comparison  where d represents the number of features or neurons of the layer that dropout is applied
to. For the sake of clarity  we divided the experiments into three parts. In the ﬁrst part  we compare
the performance of the data-dependent dropout (d-dropout) to the standard dropout (s-dropout)
for logistic regression. In the second part  we compare the performance of evolutional dropout
(e-dropout) to the standard dropout for training deep convolutional neural networks. Finally  we
compare e-dropout with batch normalization.

Figure 2: Left three: data-dependent dropout vs. standard dropout on three data sets (real-sim 
news20  RCV1) for logistic regression; Right: Evolutional dropout vs BN on CIFAR-10. (best seen
in color).
5.1 Shallow Learning

We implement the presented stochastic optimization algorithm. To evaluate the performance
of data-dependent dropout for shallow learning  we use the three data sets: real-sim  news20
and RCV13.
In this experiment  we use a ﬁxed step size and tune the step size in
[0.1  0.05  0.01  0.005  0.001  0.0005  0.0001] and report the best results in terms of convergence
speed on the training data for both standard dropout and data-dependent dropout. The left three
panels in Figure 2 show the obtained results on these three data sets. In each ﬁgure  we plot both
the training error and the testing error. We can see that both the training and testing errors using the
proposed data-dependent dropout decrease much faster than using the standard dropout and also a
smaller testing error is achieved by using the data-dependent dropout.

5.2 Evolutional Dropout for Deep Learning

We would like to emphasize that we are not aiming to obtain better prediction performance by trying
different network structures and different engineering tricks such as data augmentation  whitening 
etc.  but rather focus on the comparison of the proposed dropout to the standard dropout using
Bernoulli noise on the same network structure. In our experiments  we use the default splitting of
training and testing data in all data sets. We directly optimize the neural networks using all training
images without further splitting it into a validation data to be added into the training in later stages 
which explains some marginal gaps from the literature results that we observed (e.g.  on CIFAR-10
compared with [19]).
We conduct experiments on four benchmark data sets for comparing e-dropout and s-dropout: MNIST
[10]  SVHN [11]  CIFAR-10 and CIFAR-100 [8]. We use the same or similar network structure as in
the literatures for the four data sets. In general  the networks consist of convolution layers  pooling
layers  locally connected layers  fully connected layers  softmax layers and a cost layer. For the
detailed neural network structures and their parameters  please refer to the supplementary materials.
The dropout is added to some fully connected layers or locally connected layers. The rectiﬁed linear
activation function is used for all neurons. All the experiments are conducted using the cuda-convnet
library 4. The training procedure is similar to [9] using mini-batch SGD with momentum (0.9). The

3https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
4https://code.google.com/archive/p/cuda-convnet/

7

# of iters×1040123456error00.050.10.150.20.250.3s-dropout(tr)s-dropout(te)d-dropout(tr)d-dropout(te)# of iters×10401234error00.050.10.150.20.250.30.350.40.450.5s-dropout(tr)s-dropout(te)d-dropout(tr)d-dropout(te)# of iters×10502468error0.040.060.080.10.120.140.160.18s-dropout(tr)s-dropout(te)d-dropout(tr)d-dropout(te)# of iters×10401234567test accuracy00.10.20.30.40.50.60.70.80.9no BN and no DropoutBNBN+DropoutEvolutional Dropout(a) MNIST

(b) SVHN

(c) CIFAR-10

(d) CIFAR-100

Figure 3: Evolutional dropout vs. standard dropout on four benchmark datasets for deep learning
(best seen in color).

size of mini-batch is ﬁxed to 128. The weights are initialized based on the Gaussian distribution
with mean zero and standard deviation 0.01. The learning rate (i.e.  step size) is decreased after a
number of epochs similar to what was done in previous works [9]. We tune the initial learning rates
for s-dropout and e-dropout separately from 0.001  0.005  0.01  0.1 and report the best result on each
data set that yields the fastest convergence.
Figure 3 shows the training and testing error curves in the optimization process on the four data sets
using the standard dropout and the evolutional dropout. For SVHN data  we only report the ﬁrst
12000 iterations  after which the error curves of the two methods almost overlap. We can see that
using the evolutional dropout generally converges faster than using the standard dropout. On CIFAR-
100 data  we have observed signiﬁcant speed-up. In particular  the evolutional dropout achieves
relative improvements over 10% on the testing performance and over 50% on the convergence speed
compared to the standard dropout.

5.3 Comparison with the Batch Normalization (BN)

Finally  we make a comparison between the evolutional dropout and the batch normalization. For
batch normalization  we use the implementation in Caffe 5. We compare the evolutional dropout with
the batch normalization on CIFAR-10 data set. The network structure is from the Caffe package and
can be found in the supplement  which is different from the one used in the previous experiment.
It contains three convolutional layers and one fully connected layer. Each convolutional layer is
followed by a pooling layer. We compare four methods: (1) No BN and No dropout - without using
batch normalization and dropout; (2) BN; (3) BN with standard dropout; (4) Evolutional Dropout.
The rectiﬁed linear activation is used in all methods. We also tried BN with the sigmoid activation
function  which gives worse results. For the methods with BN  three batch normalization layers are
inserted before or after each pooling layer following the architecture given in Caffe package (see
supplement). For the evolutional dropout training  only one layer of dropout is added to the the last
convolutional layer. The mini-batch size is set to 100  the default value in Caffe. The initial learning
rates for the four methods are set to the same value (0.001)  and they are decreased once by ten times.
The testing accuracy versus the number of iterations is plotted in the right panel of Figure 2  from
which we can see that the evolutional dropout training achieves comparable performance with BN
+ standard dropout  which justiﬁes our claim that evolutional dropout also addresses the internal
covariate shift issue.
6 Conclusion
In this paper  we have proposed a distribution-dependent dropout for both shallow learning and
deep learning. Theoretically  we proved that the new dropout achieves a smaller risk and faster
convergence. Based on the distribution-dependent dropout  we developed an efﬁcient evolutional
dropout for training deep neural networks that adapts the sampling probabilities to the evolving
distributions of layers’ outputs. Experimental results on various data sets veriﬁed that the proposed
dropouts can dramatically improve the convergence and also reduce the testing error.

Acknowledgments

We thank anonymous reviewers for their comments. Z. Li and T. Yang are partially supported by
National Science Foundation (IIS-1463988  IIS-1545995). B. Gong is supported in part by NSF
(IIS-1566511) and a gift from Adobe.

5https://github.com/BVLC/caffe/

8

# of iters02000400060008000error00.10.20.30.40.50.60.70.80.9s-dropout(tr)s-dropout(te)e-dropout(tr)e-dropout(te)# of iters020004000600080001000012000error0.010.020.030.040.050.060.070.080.09s-dropout(tr)s-dropout(te)e-dropout(tr)e-dropout(te)# of Iters×1050123456error00.10.20.30.40.50.6s-dropout(tr)s-dropout(te)e-dropout(tr)e-dropout(te)# of iters×104024681012error0.10.20.30.40.50.60.70.80.91s-dropout(tr)s-dropout(te)e-dropout(tr)e-dropout(te)References
[1] Jimmy Ba and Brendan Frey. Adaptive dropout for training deep neural networks. In Advances in Neural

Information Processing Systems  pages 3084–3092  2013.

[2] Pierre Baldi and Peter J Sadowski. Understanding dropout. In Advances in Neural Information Processing

Systems  pages 2814–2822  2013.

[3] Benjamin Graham  Jeremy Reizenstein  and Leigh Robinson. Efﬁcient batchwise dropout training using

submatrices. CoRR  abs/1502.02478  2015.

[4] Geoffrey E Hinton  Nitish Srivastava  Alex Krizhevsky  Ilya Sutskever  and Ruslan R Salakhutdinov. Im-
proving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 
2012.

[5] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. arXiv preprint arXiv:1502.03167  2015.

[6] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR  abs/1412.6980 

2014.

[7] Diederik P. Kingma  Tim Salimans  and Max Welling. Variational dropout and the local reparameterization

trick. CoRR  abs/1506.02557  2015.

[8] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images  2009.
[9] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In Advances in neural information processing systems  pages 1097–1105  2012.

[10] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[11] Yuval Netzer  Tao Wang  Adam Coates  Alessandro Bissacco  Bo Wu  and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised
feature learning  volume 2011  page 4. Granada  Spain  2011.

[12] Behnam Neyshabur  Ruslan R Salakhutdinov  and Nati Srebro. Path-sgd: Path-normalized optimization in
deep neural networks. In Advances in Neural Information Processing Systems  pages 2413–2421  2015.
[13] Marc’Aurelio Ranzato  Alex Krizhevsky  and Geoffrey E. Hinton. Factored 3-way restricted boltzmann

machines for modeling natural images. In AISTATS  pages 621–628  2010.

[14] Shai Shalev-Shwartz  Ohad Shamir  Nathan Srebro  and Karthik Sridharan. Stochastic convex optimization.

In The 22nd Conference on Learning Theory (COLT)  2009.

[15] Nathan Srebro  Karthik Sridharan  and Ambuj Tewari. Smoothness  low noise and fast rates. In Advances

in Neural Information Processing Systems 23 (NIPS)  pages 2199–2207  2010.

[16] Nitish Srivastava  Geoffrey Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov. Dropout:
A simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research  15
(1):1929–1958  2014.

[17] Ilya Sutskever  James Martens  George Dahl  and Geoffrey Hinton. On the importance of initialization and
momentum in deep learning. In Proceedings of the 30th international conference on machine learning
(ICML-13)  pages 1139–1147  2013.

[18] Stefan Wager  Sida Wang  and Percy S Liang. Dropout training as adaptive regularization. In Advances in

Neural Information Processing Systems  pages 351–359  2013.

[19] Li Wan  Matthew Zeiler  Sixin Zhang  Yann L Cun  and Rob Fergus. Regularization of neural networks
using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) 
pages 1058–1066  2013.

[20] Sida Wang and Christopher Manning. Fast dropout training. In Proceedings of the 30th International

Conference on Machine Learning (ICML-13)  pages 118–126  2013.

[21] Sida I Wang  Mengqiu Wang  Stefan Wager  Percy Liang  and Christopher D Manning. Feature noising for

log-linear structured prediction. In EMNLP  pages 1170–1179  2013.

[22] Sixin Zhang  Anna Choromanska  and Yann LeCun. Deep learning with elastic averaging sgd. arXiv

preprint arXiv:1412.6651  2014.

[23] Jingwei Zhuo  Jun Zhu  and Bo Zhang. Adaptive dropout rates for learning with corrupted features. In

IJCAI  pages 4126–4133  2015.

9

,James Martens
Arkadev Chattopadhya
Toni Pitassi
Richard Zemel
Vitaly Kuznetsov
Mehryar Mohri
Umar Syed
Zhe Li
Boqing Gong
Tianbao Yang
Xiangyu Zheng
Song Xi Chen