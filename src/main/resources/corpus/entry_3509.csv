2013,Modeling Overlapping Communities with Node Popularities,We develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic blockmodel (MMSB). Our model integrates some of the basic properties of nodes in social networks: homophily and preferential connection to popular nodes. We develop a scalable algorithm for posterior inference  based on a novel nonconjugate variant of stochastic variational inference. We evaluate the link prediction accuracy of our algorithm on eight real-world networks with up to 60 000 nodes  and 24 benchmark networks. We demonstrate that our algorithm predicts better than the MMSB. Further  using benchmark networks we show that node popularities are essential to achieving high accuracy in the presence of skewed degree distribution and noisy links---both characteristics of real networks.,Modeling Overlapping Communities with

Node Popularities

Prem Gopalan1  Chong Wang2  and David M. Blei1

1Department of Computer Science  Princeton University  {pgopalan blei}@cs.princeton.edu

2Machine Learning Department  Carnegie Mellon University  {chongw}@cs.cmu.edu

Abstract

We develop a probabilistic approach for accurate network modeling using node
popularities within the framework of the mixed-membership stochastic block-
model (MMSB). Our model integrates two basic properties of nodes in social
networks: homophily and preferential connection to popular nodes. We develop a
scalable algorithm for posterior inference  based on a novel nonconjugate variant
of stochastic variational inference. We evaluate the link prediction accuracy of our
algorithm on nine real-world networks with up to 60 000 nodes  and on simulated
networks with degree distributions that follow a power law. We demonstrate that
the AMP predicts signiﬁcantly better than the MMSB.

1

Introduction

Social network analysis is vital to understanding and predicting interactions between network enti-
ties [6  19  21]. Examples of such networks include online social networks  collaboration networks
and hyperlinked blogs. A central problem in social network analysis is to identify hidden community
structures and node properties that can best explain the network data and predict connections [19].
Two node properties underlie the most successful models that explain how network connections
are generated. The ﬁrst property is popularity. This is the basis for preferential attachment [12] 
according to which nodes preferentially connect to popular nodes. The resulting degree distributions
from this process are known to satisfy empirically observed properties such as power laws [24]. The
second property that underlies many network models is homophily or similarity  according to which
nodes with similar observed or unobserved attributes are more likely to connect to each other. To best
explain social network data  a probabilistic model must capture these competing node properties.
Recent theoretical work [24] has argued that optimizing the trade-offs between popularity and simi-
larity best explains the evolution of many real networks. It is intuitive that combining both notions
of attractiveness  i.e.  popularity and similarity  is essential to explain how networks are generated.
For example  on the Internet a user’s web page may link to another user due to a common interest in
skydiving. The same user’s page may also link to popular web pages such as Google.com.
In this paper  we develop a probabilistic model of networks that captures both popularity and ho-
mophily. To capture homophily  our model is built on the mixed-membership stochastic blockmodel
(MMSB) [2]  a community detection model that allows nodes to belong to multiple communities.
(For example  a member of a large social network might belong to overlapping communities of
neighbors  co-workers  and school friends.) The MMSB provides better ﬁts to real network data
than single community models [23  27]  but cannot account for node popularities.
Speciﬁcally  we extend the assortative MMSB [9] to incorporate per-community node popularity.
We develop a scalable algorithm for posterior inference  based on a novel nonconjugate variant of
stochastic variational inference [11]. We demonstrate that our model predicts signiﬁcantly better

1

Figure 1: We visualize the discovered community structure and node popularities in a giant component of the
netscience collaboration network [22] (Left). Each link denotes a collaboration between two authors  colored
by the posterior estimate of its community assignment. Each author node is sized by its estimated posterior
popularity and colored by its dominant research community. The network is visualized using the Fructerman-
Reingold algorithm [7]. Following [14]  we show an example where incorporating node popularities helps in
accurately identifying communities (Right). The division of the political blog network [1] discovered by the
AMP corresponds closely to the liberal and conservative blogs identiﬁed in [1]; the MMSB has difﬁculty in
delineating these groups.

than the stochastic variational inference algorithm for the MMSB [9] on nine large real-world net-
works. Further  using simulated networks  we show that node popularities are essential for predictive
accuracy in the presence of power-law distributed node degrees.
Related work. There have been several research efforts to incorporate popularity into network mod-
els. Karrer et al. [14] proposed the degree-corrected blockmodel that extends the classic stochastic
blockmodels [23] to incorporate node popularities. Krivitsky et al. [16] proposed the latent cluster
random effects model that extends the latent space model [10] to include node popularities. Both
models capture node similarity and popularity  but assume that unobserved similarity arises from
each node participating in a single community. Finally  the Poisson community model [4] is a
probabilistic model of overlapping communities that implicitly captures degree-corrected mixed-
memberships. However  the standard EM inference under this model drives many of the per-node
community parameters to zero  which makes it ineffective for prediction or model metrics based on
prediction (e.g.  to select the number of communities).

2 Modeling node popularity and similarity

The assortative mixed-membership stochastic blockmodel (MMSB) [9] treats the links or non-links
yab of a network as arising from interactions between nodes a and b. Each node a is associated
with community memberships πa  a distribution over communities. The probability that two nodes
are linked is governed by the similarity of their community memberships and the strength of their
shared communities.
Given the communities of a pair of nodes  the link indicators yab are independent. We draw yab re-
peatedly by choosing a community assignment (za→b  za←b) for a pair of nodes (a  b)  and drawing
a binary value from a community distribution. Speciﬁcally  the conditional probability of a link in
MMSB is

p(yab = 1|za→b i  za←b j  β) =(cid:80)K

(cid:80)K

i=1

j=1 za→b iza←b jβij 

where β is the blockmodel matrix of community strength parameters to be estimated. In the as-
sortative MMSB [9]  the non-diagonal entries of the blockmodel matrix are set close to 0. This
captures node similarity in community memberships—if two nodes are linked  it is likely that the
latent community indicators were the same.

2

BARABASI  AJEONG  HNEWMAN  MSOLE  RPASTORSATORRAS  RHOLME  PNETSCIENCE COLLABORATION NETWORKPOLITICAL BLOG NETWORKAMPMMSBAMPIn the proposed model  assortative MMSB with node popularities  or AMP  we introduce latent
variables θa to capture the popularity of each node a  i.e.  its propensity to attract links independent
of its community memberships. We capture the effect of node popularity and community similarity
on link generation using a logit model

logit (p(yab = 1|za→b  za←b  θ  β)) ≡ θa + θb +(cid:80)K

abβk 

k=1 δk

(1)
ab is one if both nodes assume the

ab = za→b kza←b k. The indicator δk

effect θa captures the popularity of individual nodes while the(cid:80)K

where we deﬁne indicators δk
same community k.
Eq. 1 is a log-linear model [20]. In log-linear models  the random component  i.e.  the expected
probability of a link  has a multiplicative dependency on the systematic components  i.e.  the covari-
ates. This model is also similar in the spirit of the random effects model [10]—the node-speciﬁc
abβk term captures the in-
teractions through latent communities. Notice that we can easily extend the predictor in Eq. 1 to
include observed node covariates  if any.
We now deﬁne a hierarchical generative process for the observed link or non-link under the AMP:

k=1 δk

1. Draw K community strengths βk ∼ N (µ0  σ2
0).
2. For each node a 

(a) Draw community memberships πa ∼ Dirichlet(α).
(b) Draw popularity θa ∼ N (0  σ2
1).

3. For each pair of nodes a and b 

(a) Draw interaction indicator za→b ∼ πa.
(b) Draw interaction indicator za←b ∼ πb.
(c) Draw the probability of a link yab|za→b  za←b  θ  β ∼ logit−1(za→b  za←b  θ  β).

Under the AMP  the similarities between the nodes’ community memberships and their respective
popularities compete to explain the observations.
We can make AMP simpler by replacing the vector of K latent community strengths β with a
single community strength β. In §4  we demonstrate that this simpler model gives good predictive
performance on small networks.
the latent variables
We analyze data with the AMP via the posterior distribution over
p(π1:N   θ1:N   z  β1:K|y  α  µ0  σ2
1)  where θ1:N represents the node popularities  and the pos-
terior over π1:N represents the community memberships of the nodes. With an estimate of this
latent structure  we can characterize the network in many useful ways. Figure 1 gives an example.
This is a subgraph of the netscience collaboration network [22] with N = 1460 nodes. We analyzed
this network with K = 100 communities  using the algorithm from §3. This results in posterior
estimates of the community memberships and popularities for each node and posterior estimates
of the community assignments for each link. With these estimates  we visualized the discovered
community structure and the popular authors.
In general  with an estimate of this latent structure  we can study individual links  characterizing
the extent to which they occur due to similarity between nodes and the extent to which they are an
artifact of the popularity of the nodes.

0  σ2

3 A stochastic gradient algorithm for nonconjugate variational inference
Our goal is to compute the posterior distribution p(π1:N   θ1:N   z  β1:K|y  α  µ0σ2
ence is intractable; we use variational inference [13].
Traditionally  variational inference is a coordinate ascent algorithm. However  the AMP presents
two challenges. First  in variational inference the coordinate updates are available in closed form
only when all the nodes in the graphical model satisfy conditional conjugacy. The AMP is not condi-
tionally conjugate. To see this  note that the Gaussian priors on the popularity θ and the community
strengths β are not conjugate to the conditional likelihood of the data. Second  coordinate ascent
algorithms iterate over all the O(N 2) node pairs making inference intractable for large networks.

1). Exact infer-

0  σ2

3

We address these challenges by deriving a stochastic gradient algorithm that optimizes a tractable
lower bound of the variational objective [11]. Our algorithm avoids the O(N 2) computational cost
per iteration by subsampling a “mini-batch” of random nodes and a subset of their interactions in
each iteration [9].

3.1 The variational objective

In variational inference  we deﬁne a family of distributions over the hidden variables q(β  θ  π  z)
and ﬁnd the member of that family that is closest to the true posterior. We use the mean-ﬁeld family 
with the following variational distributions:

q(za→b = i  za←b = j) = φij
ab;
q(βk) = N (βk; µk  σ2
β);

q(πn) = Dirichlet(πn; γn);
q(θn) = N (θn; λn  σ2
θ ).

(2)

The posterior over the joint distribution of link community assignments per node pair (a  b) is pa-
1  the community memberships by γ  the com-
rameterized by the per-interaction memberships φab
munity strength distributions by µ and the popularity distributions by λ.
Minimizing the KL divergence between q and the true posterior is equivalent to optimizing an ev-
idence lower bound (ELBO) L  a bound on the log likelihood of the observations. We obtain this
bound by applying Jensen’s inequality [13] to the data likelihood. The ELBO is

L =(cid:80)
+(cid:80)
+(cid:80)
+(cid:80)
+(cid:80)

n

n

k

Eq[log p(πn|α)] −(cid:80)
1)] −(cid:80)
0)] −(cid:80)

Eq[log q(πn|γn)]
Eq[log q(θn|λn  σ2
θ )]
Eq[log q(βk|µk  σ2
β)]

Eq[log p(θn|σ2
Eq[log p(βk|µ0  σ2
Eq[log p(za→b|πa)] + Eq[log p(za←b|πb)] − Eq[log q(za→b  za←b|φab)]
Eq[log p(yab|za→b  za←b  θ  β)].

n

n

k

(3)

a b

a b

Notice that the ﬁrst three lines in Eq. 3 contains summations over communities and nodes; we call
these global terms. They relate to the global parameters which are (γ  λ  µ). The remaining lines
contain summations over all node pairs; we call these local terms. They relate to the local parame-
ters which are the φab. The distinction between the global and local parameters is important—the
updates to global parameters depends on all (or many) local parameters  while the updates to lo-
cal parameters for a pair of nodes only depends on the relevant global and local parameters in that
context.
Estimating the global variational parameters is a challenging computational problem. Coordinate
ascent inference must consider each pair of nodes at each iteration  but even a single pass through
the O(N 2) node pairs can be prohibitive. Previous work [9] has taken advantage of conditional con-
jugacy of the MMSB to develop fast stochastic variational inference algorithms. Unlike the MMSB 
the AMP is not conditionally conjugate. Nevertheless  by carefully manipulating the variational
objective  we can develop a scalable stochastic variational inference algorithm for the AMP.

3.2 Lower bounding the variational objective

To optimize the ELBO with respect to the local and global parameters we need its derivatives. The
data likelihood terms in the ELBO can be written as

Eq[log p(yab|za→b  za←b  θ  β)] = yabEq[xab] − Eq[log(1 + exp(xab))] 

(4)

where we deﬁne xab ≡ θa + θb +(cid:80)K

ab. The terms in Eq. 4 cannot be expanded analytically.
To address this issue  we further lower bound −Eq[log(1+exp(xab))] using Jensen’s inequality [13] 

k=1 βkδk

−Eq[log(1 + exp(xab))] ≥ − log[Eq(1 + exp(xab))]

= − log[1 + Eq[exp(θa + θb +(cid:80)K

= − log[1 + exp(λa + σ2

k=1 βkδk
θ /2) exp(λb + σ2

ab)]]
θ /2)sab] 

(5)

1Following [15]  we use a structured mean-ﬁeld assumption.

4

Algorithm 1 The stochastic AMP algorithm
1: Initialize variational parameters. See §3.5.
2: while convergence criteria is not met do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12: end while

Sample a mini-batch S of nodes. Let P be the set of node pairs in S.
local step
Optimize φab ∀(a  b) ∈ P using Eq. 11 and Eq. 12.
global step
Update memberships γa  for each node a ∈ S  using stochastic natural gradients in Eq. 6.
Update popularities λa  for each node a ∈ S using stochastic gradients in Eq. 7.
Update community strengths µ using stochastic gradients in Eq. 9.
Set ρa(t) = (τ0 + ta)−κ; ta ← ta + 1  for each node a ∈ S.
Set ρ(cid:48)(t) = (τ0 + t)−κ; t ← t + 1.

where we deﬁne sab ≡ (cid:80)K

β/2} + (1 −(cid:80)K

ab exp{µk + σ2

k=1 φkk

In simplifying Eq. 5 
we have used that q(θn) is a Gaussian. Using the mean of a log-normal distribution  we have
Eq[exp(θn)] = exp(λn + σ2
θ /2). A similar substitution applies for the terms involving βk in Eq. 5.
We substitute Eq. 5 in Eq. 3 to obtain a tractable lower bound L(cid:48) of the ELBO L in Eq. 3. This allows
us to develop a coordinate ascent algorithm that iteratively updates the local and global parameters
to optimize this lower bound on the ELBO.

k=1 φkk

ab ).

3.3 The global step

We optimize the ELBO with respect to the global variational parameters using stochastic gradient
ascent. Stochastic gradient algorithms follow noisy estimates of the gradient with a decreasing step-
size. If the expectation of the noisy gradient equals to the gradient and if the step-size decreases
according to a certain schedule  then the algorithm converges to a local optimum [26]. Subsampling
the data to form noisy gradients scales inference as we avoid the expensive all-pairs sums in Eq. 3.
The global step updates the global community memberships γ  the global popularity parameters λ
and the global community strength parameters µ with a stochastic gradient of the lower bound on
the ELBO L(cid:48). In [9]  the authors update community memberships of all nodes after each iteration
by obtaining the natural gradients of the ELBO 2 with respect to the vector γ of dimension N × K.
We use natural gradients for the memberships too  but use distinct stochastic optimizations for the
memberships and popularity parameters of each node and maintain a separate learning rate for each
node. This restricts the per-iteration updates to nodes in the current mini-batch.
Since the variational objective is a sum of terms  we can cheaply compute a stochastic gradient by
ﬁrst subsampling a subset of terms and then forming an appropriately scaled gradient. We use a
variant of the random node sampling method proposed in [9]. At each iteration we sample a node
uniformly at random from the N nodes in the network. (In practice we sample a “mini-batch” S of
nodes per update to reduce noise [11  9].) While a naive method will include all interactions of a
sampled node as the observed pairs  we can leverage network sparsity for efﬁciency; in many real
networks  only a small fraction of the node pairs are linked. Therefore  for each sampled node  we
include as observations all of its links and a small uniform sample of m0 non-links.
a be the natural gradient of L(cid:48) with respect to γa  and ∂λt
Let ∂γt
with respect to λa and µk  respectively. Following [2  9]  we have

k be the gradients of L(cid:48)

a and ∂µt

∂γt

(a b)∈links(a) φkk

(6)
where links(a) and nonlinks(a) correspond to the set of links and non-links of a in the training set.
Notice that an unbiased estimate of the summation term over non-links in Eq. 6 can be obtained from
a subsample of the node’s non-links. Therefore  the gradient of L(cid:48) with respect to the membership
parameter γa  computed using all of the nodes’ links and a subsample of its non-links  is a noisy but
unbiased estimate of the natural gradient in Eq. 6.

(a b)∈nonlinks(a) φkk

ab (t) 

a k + αk +(cid:80)

a k = −γt−1

ab (t) +(cid:80)

2The natural gradient [3] points in the direction of steepest ascent in the Riemannian space. The local
distance in the Riemannian space is deﬁned by KL divergence  a better measure of dissimilarity between prob-
ability distributions than Euclidean distance [11].

5

The gradient of the approximate ELBO with respect to the popularity parameter λa is

(a b)∈links(a) ∪ nonlinks(a)(yab − rabsab) 

(7)

∂λt

a = − λt−1

a
σ2
1

where we deﬁne rab as

+(cid:80)

(8)
Finally  the stochastic gradient of L(cid:48) with respect to the global community strength parameter µk is

.

θ /2} exp{λb+σ2
θ /2} exp{λb+σ2

θ /2}
θ /2}sab

∂µt

k = µ0−µt−1

k
σ2
0

+ N
2|S|

(a b)∈links(S) ∪ nonlinks(S) φkk

ab (yab − rab exp{µk + σ2

β/2}).

(9)

rab ≡ exp{λa+σ2
1+exp{λa+σ2
(cid:80)

As with the community membership gradients  notice that an unbiased estimate of the summation
term over non-links in Eq. 7 and Eq. 9 can be obtained from a subsample of the node’s non-links. To
obtain an unbiased estimate of the true gradient with respect to µk  the summation over a node’s links
and non-links must be scaled by the inverse probability of subsampling that node in Eq. 9. Since
each pair is shared between two nodes  and we use a mini-batch with S nodes  the summations over
the node pairs are scaled by N

2|S| in Eq. 9.

We can interpret the gradients in Eq. 7 and Eq. 9 by studying the terms involving rab in Eq. 7 and
Eq. 9. In Eq. 7  (yab − rabsab) is the residual for the pair (a  b)  while in Eq. 9  (yab − rab exp{µk +
β/2}) is the residual for the pair (a  b) conditional on the latent community assignment for both
σ2
nodes a and b being set to k. Further  notice that the updates for the global parameters of node a and
b  and the updates for µ depend only on the diagonal entries of the indicator variational matrix φab.
We can similarly obtain stochastic gradients for the variational variances σβ and σθ; however  in our
experiments we found that ﬁxing them already gives good results. (See §4.)
The global step for the global parameters follows the noisy gradient with an appropriate step-size:

γa ← γa + ρa(t)∂γt
a;

λa ← λa + ρa(t)∂λt

a; µ ← µ + ρ(cid:48)(t)∂µt.

(10)

that(cid:80)

We maintain separate learning rates ρa for each node a  and only update the γ and λ for the nodes
in the mini-batch in each iteration. There is a global learning rate ρ(cid:48) for the community strength
parameters µ  which are updated in every iteration. For each of these learning rates ρ  we require
t ρ(t) = ∞ for convergence to a local optimum [26]. We set ρ(t) (cid:44)
(τ0 + t)−κ  where κ ∈ (0.5  1] is the learning rate and τ0 ≥ 0 downweights early iterations.

t ρ(t)2 < ∞ and(cid:80)

3.4 The local step

We now derive the updates for the local parameters. The local step optimizes the per-interaction
memberships φ with respect to a subsample of the network. There is a per-interaction variational
parameter of dimension K × K for each node pair—φab—representing the posterior approximation
of which pair of communities are active in determining the link or non-link. The coordinate ascent
update for φab is

(cid:110)Eq[log πa k] + Eq[log πb k] + yabµk − rab(exp{µk + σ2
(cid:110)Eq[log πa i] + Eq[log πb j]

  i (cid:54)= j 

(cid:111)

(cid:111)

β/2} − 1)

ab ∝ exp
φkk
ab ∝ exp
φij

(11)

(12)

where rab is deﬁned in Eq. 8. We present the full stochastic variational inference in Algorithm 1.

3.5 Initialization and convergence

We initialize the community memberships γ using approximate posterior memberships from the
variational inference algorithm for the MMSB [9]. We initialized popularities λ to the logarithm of
the normalized node degrees added to a small random offset  and initialized the strengths µ to zero.
We measure convergence by computing the link prediction accuracy on a validation set with 1% of
the networks links  and an equal number of non-links. The algorithm stops either when the change
in log-likelihood on this validation set is less than 0.0001%  or if the log-likelihood decreases for
consecutive iterations.

6

Figure 2: Network data sets. N is the number of nodes  d is the percent of node pairs that are links and P is
the mean perplexity over the links and nonlinks in the held-out test set.

DATA SET
US AIR
POLITICAL BLOGS
NETSCIENCE
RELATIVITY
HEP-TH
HEP-PH
ASTRO-PH
COND-MAT
BRIGHTKITE

N
712
1224
1450
4158
8638
11204
17903
36458
56739

PAMP
d(%)
2.75 ± 0.04
1.7%
2.97 ± 0.03
1.9%
2.73 ± 0.11
0.2%
3.69 ± 0.18
0.1%
0.05% 12.35 ± 0.17
2.75 ± 0.06
0.16%
5.04 ± 0.02
0.11%
0.02% 10.82 ± 0.09
0.01% 10.98 ± 0.39

PMMSB
3.41 ± 0.15
3.12 ± 0.01
3.02 ± 0.19
6.53 ± 0.37
23.06 ± 0.87
3.310 ± 0.15
5.28 ± 0.07
13.52 ± 0.21
41.11 ± 0.89

TYPE

TRANSPORT
HYPERLINK
COLLAB.
COLLAB.
COLLAB.
COLLAB.
COLLAB.
COLLAB.
SOCIAL

SOURCE
[25]
[1]
[22]
[18]
[18]
[18]
[18]
[22]
[18]

Figure 3: The AMP model outperforms the MMSB model of [9] in predictive accuracy on real networks. Both
models were ﬁt using stochastic variational inference [11]. For the data sets shown  the number of communities
K was set to 100 and hyperparameters were set to the same values across data sets. The perplexity results are
based on ﬁve replications. A single replication is shown for the mean precision and mean recall.

4 Empirical study

We use the predictive approach to evaluating model ﬁtness [8]  comparing the predictive accuracy
of AMP (Algorithm 1) to the stochastic variational inference algorithm for the MMSB with link
sampling [9]. In all data sets  we found that AMP gave better ﬁts to real-world networks. Our
networks range in size from 712 nodes to 56 739 nodes. Some networks are sparse  having as
little as 0.01% of all pairs as links  while others have up to 2% of all pairs as links. Our data sets
contain four types of networks: hyperlink  transportation  collaboration and social networks. We
implemented Algorithm 1 in 4 800 lines of C++ code. 3

Metrics. We used perplexity  mean precision and mean recall in our experiments to evaluate the
predictive accuracy of the algorithms. We computed the link prediction accuracy using a test set of
node pairs that are not observed during training. The test set consists of 10% of randomly selected
links and non-links from each data set. During training  these test set observations are treated as
zeros. We approximate the predictive distribution of a held-out node pair yab under the AMP using
posterior estimates ˆθ  ˆβ and ˆπ as

p(yab|y) ≈(cid:80)

(cid:80)

za→b

za←b

p(yab|za→b  za←b  ˆθ  ˆβ)p(za→b|ˆπa)p(za←b|ˆπb).

(13)

3Our software is available at https://github.com/premgopalan/sviamp.

7

Number of recommendationsMean precision2%3%4%5%6%7%8%relativity10501000%1%2%3%4%5%6%astro10501004%6%8%10%12%hepph10501000%0.5%1%1.5%2%hepth10501000%0.2%0.4%0.6%0.8%cond−mat10501000%0.5%1%1.5%2%brightkite1050100ampmmsbNumber of recommendationsMean recall10%15%20%25%30%relativity10501000%5%10%15%astro10501005%10%15%20%hepph10501000%5%10%15%20%hepth10501000%2%4%6%8%10%12%cond−mat10501000%2%4%6%8%10%12%14%brightkite1050100ampmmsbFigure 4: The AMP predicts signiﬁcantly better than the MMSB [9] on 12 LFR benchmark networks [17].
Each plot shows 4 networks with increasing right-skewness in degree distribution. µ is the fraction of noisy
links between dissimilar nodes—nodes that share no communities. The precision is computed at 50 recommen-
dations for each node  and is averaged over all nodes in the network.

Perplexity is the exponential of the average predictive log likelihood of the held-out node pairs. For
mean precision and recall  we generate the top n pairs for each node ranked by the probability of
a link between them. The ranked list of pairs for each node includes nodes in the test set  as well
as nodes in the training set that were non-links. We compute precision-at-m  which measures the
fraction of the top m recommendations present in the test set; and we compute recall-at-m  which
captures the fraction of nodes in the test set present in the top m recommendations. We vary m from
10 to 100. We then obtain the mean precision and recall across all nodes. 4

Hyperparameters and constants. For the stochastic AMP algorithm  we set the “mini-batch” size
S = N/100  where N is the number of nodes in the network and we set the non-link sample size
m0 = 100. We set the number of communities K = 2 for the political blog network and K = 20
0 = 1.0 
for the US air; for all other networks  K was set to 100. We set the hyperparameters σ2
1 = 10.0 and µ0 = 0  ﬁxed the variational variances at σθ = 0.1 and σβ = 0.5 and set the learning
σ2
parameters τ0 = 65536 and κ = 0.5. We set the Dirichlet hyperparameter α = 1
K for the AMP and
the MMSB.

Results on real networks. Figure 2 compares the AMP and the MMSB stochastic algorithms on a
number of real data sets. The AMP deﬁnitively outperforms the MMSB in predictive performance.
All hyperparameter settings were held ﬁxed across data sets. The ﬁrst four networks are small in
size  and were ﬁt using the AMP model with a single community strength parameter. All other
networks were ﬁt with the AMP model with K community strength parameters. As N increases 
the gap between the mean precision and mean recall performance of these algorithms appears to
increase. Without node popularities  MMSB is dependent entirely on node memberships and com-
munity strengths to predict links. Since K is held ﬁxed  communities are likely to have more nodes
as N increases  making it increasingly difﬁcult for the MMSB to predict links. For the small US air 
political blogs and netscience data sets  we obtained similar performance for the replication shown
in Figure 2. For the AMP the mean precision at 10 for US Air  political blogs and netscience were
0.087  0.07  0.092  respectively; for the MMSB the corresponding values were 0.007  0.0  0.063 
respectively.

Results on synthetic networks. We generated 12 LFR benchmark networks [17]  each with 1000
nodes. Roughly 50% of the nodes were assigned to 4 overlapping communities  and the other 50%
were assigned to single communities. We set a community size range of [200  500] and a mean node
degree of 10 with power-law exponent set to 2.0. Figure 4 shows that the MMSB performs poorly as
the skewness is increased  while the AMP performs signiﬁcantly better in the presence of both noisy
links and right-skewness  both characteristics of real networks. The skewness in degree distributions
causes the community strength parameters of MMSB to overestimate or underestimate the linking
patterns within communities. The per-node popularities in the AMP can capture the heterogeneity
in node degrees  while learning the corrected community strengths.

Acknowledgments

David M. Blei
is supported by ONR N00014-11-1-0651  NSF CAREER IIS-0745520  and
the Alfred P. Sloan foundation. Chong Wang is supported by NSF DBI-0546594 and NIH
1R01GM093156.

4Precision and recall are better metrics than ROC AUC on highly skewed data sets [5].

8

Ratio of max. degree to avg. degreeMean precision0.0200.0250.0300.035mu 024680.0180.0200.0220.0240.0260.028mu 0.224680.0160.0180.0200.0220.0240.0260.028mu 0.42468ampmmsbReferences
[1] L. A. Adamic and N. Glance. The political blogosphere and the 2004 U.S. election: divided they blog. In

LinkKDD  LinkKDD ’05  page 3643  New York  NY  USA  2005. ACM.

[2] E. M. Airoldi  D. M. Blei  S. E. Fienberg  and E. P. Xing. Mixed membership stochastic blockmodels. J.

Mach. Learn. Res.  9:1981–2014  June 2008.

[3] S. Amari. Differential geometry of curved exponential Families-Curvatures and information loss. The

Annals of Statistics  10(2):357–385  June 1982.

[4] B. Ball  B. Karrer  and M. E. J. Newman. Efﬁcient and principled method for detecting communities in

networks. Physical Review E  84(3):036103  Sept. 2011.

[5] J. Davis and M. Goadrich. The relationship between precision-recall and ROC curves. In Proceedings
of the 23rd international conference on Machine learning  ICML ’06  pages 233–240  New York  NY 
USA  2006. ACM.

[6] S. Fortunato. Community detection in graphs. Physics Reports  486(35):75–174  Feb. 2010.
[7] T. M. J. Fruchterman and E. M. Reingold. Graph drawing by force-directed placement. Softw. Pract.

Exper.  21(11):1129–1164  Nov. 1991.

[8] S. Geisser and W. Eddy. A predictive approach to model selection. Journal of the American Statistical

Association  74:153–160  1979.

[9] P. K. Gopalan and D. M. Blei. Efﬁcient discovery of overlapping communities in massive networks.

Proceedings of the National Academy of Sciences  110(36):14534–14539  2013.

[10] P. Hoff  A. Raftery  and M. Handcock. Latent space approaches to social network analysis. Journal of the

American Statistical Association  97(460):1090–1098  2002.

[11] M. Hoffman  D. M. Blei  C. Wang  and J. Paisley. Stochastic variational inference. Journal of Machine

Learning Research  2013.

[12] H. Jeong  Z. Nda  and A. L. Barabsi. Measuring preferential attachment in evolving networks. EPL

(Europhysics Letters)  61(4):567  2003.

[13] M. I. Jordan  Z. Ghahramani  T. S. Jaakkola  and L. K. Saul. An introduction to variational methods for

graphical models. Mach. Learn.  37(2):183–233  Nov. 1999.

[14] B. Karrer and M. E. J. Newman. Stochastic blockmodels and community structure in networks. Phys.

Rev. E  83:016107  Jan 2011.

[15] D. I. Kim  P. Gopalan  D. M. Blei  and E. B. Sudderth. Efﬁcient online inference for bayesian nonpara-

metric relational models. In Neural Information Processing Systems  2013.

[16] P. N. Krivitsky  M. S. Handcock  A. E. Raftery  and P. D. Hoff. Representing degree distributions  clus-
tering  and homophily in social networks with latent cluster random effects models. Social Networks 
31(3):204–213  July 2009.

[17] A. Lancichinetti and S. Fortunato. Benchmarks for testing community detection algorithms on directed

and weighted graphs with overlapping communities. Physical Review E  80(1):016118  July 2009.

[18] J. Leskovec  K. J. Lang  A. Dasgupta  and M. W. Mahone. Community structure in large networks:

Natural cluster sizes and the absence of large well-deﬁned cluster. In Internet Mathematics  2008.

[19] D. Liben-Nowell and J. Kleinberg. The link prediction problem for social networks. In Proceedings of the
twelfth international conference on Information and knowledge management  CIKM ’03  pages 556–559 
New York  NY  USA  2003. ACM.

[20] P. McCullagh and J. A. Nelder. Generalized Linear Models  Second Edition. Chapman and Hall/CRC  2

edition  Aug. 1989.

[21] M. E. J. Newman. Assortative mixing in networks. Physical Review Letters  89(20):208701  Oct. 2002.
[22] M. E. J. Newman. Finding community structure in networks using the eigenvectors of matrices. Physical

Review E  74(3):036104  2006.

[23] K. Nowicki and T. A. B. Snijders. Estimation and prediction for stochastic blockstructures. Journal of

the American Statistical Association  96(455):1077–1087  Sept. 2001.

[24] F. Papadopoulos  M. Kitsak  M. . Serrano  M. Bogu  and D. Krioukov. Popularity versus similarity in

growing networks. Nature  489(7417):537–540  Sept. 2012.

[25] RITA. U.S. Air Carrier Trafﬁc Statistics  Bur. Trans. Stats  2010.
[26] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics 

22(3):400–407  Sept. 1951.

[27] Y. J. Wang and G. Y. Wong. Stochastic blockmodels for directed graphs. Journal of the American

Statistical Association  82(397):8–19  1987.

9

,Prem Gopalan
Chong Wang
David Blei
Jacob Abernethy
Chansoo Lee
Ambuj Tewari