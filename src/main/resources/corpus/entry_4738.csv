2017,Riemannian approach to batch normalization,Batch normalization (BN) has proven to be an effective algorithm for deep neural network training by normalizing the input to each neuron and reducing the internal covariate shift. The space of weight vectors in the BN layer can be naturally interpreted as a Riemannian manifold  which is invariant to linear scaling of weights. Following the intrinsic geometry of this manifold provides a new learning rule that is more efficient and easier to analyze. We also propose intuitive and effective gradient clipping and regularization methods for the proposed algorithm by utilizing the geometry of the manifold. The resulting algorithm consistently outperforms the original BN on various types of network architectures and datasets.,Riemannian approach to batch normalization

Minhyung Cho

Jaehyung Lee

Applied Research Korea  Gracenote Inc.

mhyung.cho@gmail.com

jaehyung.lee@kaist.ac.kr

Abstract

Batch Normalization (BN) has proven to be an effective algorithm for deep neural
network training by normalizing the input to each neuron and reducing the internal
covariate shift. The space of weight vectors in the BN layer can be naturally
interpreted as a Riemannian manifold  which is invariant to linear scaling of
weights. Following the intrinsic geometry of this manifold provides a new learning
rule that is more efﬁcient and easier to analyze. We also propose intuitive and
effective gradient clipping and regularization methods for the proposed algorithm
by utilizing the geometry of the manifold. The resulting algorithm consistently
outperforms the original BN on various types of network architectures and datasets.

1

Introduction

Batch Normalization (BN) [1] has become an essential component for breaking performance records
in image recognition tasks [2  3]. It speeds up training deep neural networks by normalizing the
distribution of the input to each neuron in the network by the mean and standard deviation of the
input computed over a mini-batch of training data  potentially reducing internal covariate shift [1] 
the change in the distributions of internal nodes of a deep network during the training.
The authors of BN demonstrated that applying BN to a layer makes its forward pass invariant to
linear scaling of its weight parameters [1]. They argued that this property prevents model explosion
with higher learning rates by making the gradient propagation invariant to linear scaling. Moreover 
the gradient becomes inversely proportional to the scale factor of each weight parameter. While this
property could stabilize the parameter growth by reducing the gradients for larger weights  it could
also have an adverse effect in terms of optimization since there can be an inﬁnite number of networks 
with the same forward pass but different scaling  which may converge to different local optima owing
to different gradients. In practice  networks may become sensitive to the parameters of regularization
methods such as weight decay.
This ambiguity in the optimization process can be removed by interpreting the space of weight
vectors as a Riemannian manifold on which all the scaled versions of a weight vector correspond
to a single point on the manifold. A properly selected metric tensor makes it possible to perform
a gradient descent on this manifold [4  5]  following the gradient direction while staying on the
manifold. This approach fundamentally removes the aforementioned ambiguity while keeping the
invariance property intact  thus ensuring stable weight updates.
In this paper  we ﬁrst focus on selecting a proper manifold along with the corresponding Riemannian
metric for the scale invariant weight vectors used in BN (and potentially in other normalization
techniques [6  7  8]). Mapping scale invariant weight vectors to two well-known matrix manifolds
yields the same metric tensor  leading to a natural choice of the manifold and metric. Then  we derive
the necessary operators to perform a gradient descent on this manifold  which can be understood
as a constrained optimization on the unit sphere. Next  we present two optimization algorithms -
corresponding to the Stochastic Gradient Descent (SGD) with momentum and Adam [9] algorithms.
An intuitive gradient clipping method is also proposed utilizing the geometry of this space. Finally 

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

BN(z) =

(1)

z − E[z]

(cid:112)Var[z]

=

(cid:112)

w(cid:62)(x − E[x])
w(cid:62)Rxxw

(cid:112)

u(cid:62)(x − E[x])
u(cid:62)Rxxu

=

we illustrate the application of these algorithms to networks with BN layers  together with an effective
regularization method based on variational inference on the manifold. Experiments show that the
resulting algorithm consistently outperforms the original BN algorithm on various types of network
architectures and datasets.

2 Background

2.1 Batch normalization

We brieﬂy revisit the BN transform and its properties. While it can be applied to any single activation
in the network  in practice it is usually inserted right before the nonlinearity  taking the pre-activation
z = w(cid:62)x as its input. In this case  the BN transform is written as

where w is a weight vector  x is a vector of activations in the previous layer  u = w/|w|  and Rxx
is the covariance matrix of x. Note that BN(w(cid:62)x) = BN(u(cid:62)x). It was shown in [1] that

∂BN(w(cid:62)x)

∂x

∂BN(u(cid:62)x)

∂x

=

and

∂BN(z)

∂w

=

1
|w|

∂BN(z)

∂u

(2)

illustrating the properties discussed in Sec. 1.

2.2 Optimization on Riemannian manifold

Recent studies have shown that various constrained optimization problems in Euclidian space can be
expressed as unconstrained optimization problems on submanifolds embedded in Euclidian space [5].
For applications to neural networks  we are interested in Stiefel and Grassmann manifolds [4  10].
We brieﬂy review them here. The Stiefel manifold V(p  n) is the set of p ordered orthonormal vectors
in Rn(p ≤ n). A point on the manifold is represented by an n-by-p orthonormal matrix Y   where
Y (cid:62)Y = Ip. The Grassmann manifold G(p  n) is the set of p-dimensional subspaces of Rn(p ≤ n).
It follows that span(A)  where A ∈ Rn×p  is understood to be a point on the Grassmann manifold
G(p  n) (note that two matrices A and B are equivalent if and only if span(A) = span(B)). A point
on this manifold can be speciﬁed by an arbitrary n-by-p matrix  but for computational efﬁciency 
an orthonormal matrix is commonly chosen to represent a point. Note that the representation is not
unique [5].
To perform gradient descent on those manifolds  it is essential to equip them with a Riemannian metric
tensor and derive geometric concepts such as geodesics  exponential map  and parallel translation.
Given a tangent vector v ∈ TxM on a Riemannian manifold M with its tangent space TxM at a
point x  let us denote γv(t) as a unique geodesic on M  with initial velocity v. The exponential map
is deﬁned as expx(v) = γv(1)  which maps v to the point that is reached in a unit time along the
geodesic starting at x. The parallel translation of a tangent vector on a Riemannian manifold can
be obtained by transporting the vector along the geodesic by an inﬁnitesimally small amount  and
removing the vertical component of the tangent space [11]. In this way  the transported vector stays
in the tangent space of the manifold at a new point.
Using the concepts above  a gradient descent algorithm for an abstract Riemannian manifold is given
in Algorithm 1 for reference. This reduces to the familiar gradient descent algorithm when M = Rn 
since expyt−1(−η · h) is given as yt−1 − η · ∇f (yt−1) in Rn.
Algorithm 1 Gradient descent of a function f on an abstract Riemannian manifold M
Require: Stepsize η
Initialize y0 ∈ M
for t = 1 ···   T

h ← gradf (yt−1) ∈ Tyt−1M where gradf (y) is the gradient of f at y ∈ M
yt ← expyt−1 (−η · h)

2

3 Geometry of scale invariant vectors

As discussed in Sec. 2.1  inserting the BN transform makes the weight vectors w  used to calculate
the pre-activation w(cid:62)x  invariant to linear scaling. Assuming that there are no additional constraints
on the weight vectors  we can focus on the manifolds on which the scaled versions of a vector collapse
to a point. A natural choice for this would be the Grassmann manifold since the space of the scaled
versions of a vector is essentially a one-dimensional subspace of Rn. On the other hand  the Stiefel
manifold can also represent the same space if we set p = 1  in which case V(1  n) reduces to the unit
sphere. We can map each of the weight vectors w to its normalized version  i.e.  w/|w|  on V(1  n).
We show that popular choices of metrics on those manifolds lead to the same geometry.
Tangent vectors to the Stiefel manifold V(p  n) at Z are all the n-by-p matrices ∆ such that Z(cid:62)∆ +
∆(cid:62)Z = 0 [4]. The canonical metric on the Stiefel manifold is derived based on the geometry of
quotient spaces of the orthogonal group [4] and is given by

gs(∆1  ∆2) = tr(∆(cid:62)

1 (I − ZZ(cid:62)/2)∆2)

(3)
where ∆1  ∆2 are tangent vectors to V(p  n) at Z. If p = 1  the condition Z(cid:62)∆ + ∆(cid:62)Z = 0 is
reduced to Z(cid:62)∆ = 0  leading to gs(∆1  ∆2) = tr(∆(cid:62)
Now  let an n-by-p matrix Y be a representation of a point on the Grassmann manifold G(p  n).
Tangent vectors to the manifold at span(Y ) with the representation Y are all the n-by-p matrices
∆ such that Y (cid:62)∆ = 0. Since Y is not a unique representation  the tangent vector ∆ changes with
the choice of Y . For example  given a representation Y1 and its tangent vector ∆1  if a different
representation is selected by performing right multiplication  i.e.  Y2 = Y1R  then the tangent vector
must be moved in the same way  that is ∆2 = ∆1R. The canonical metric  which is invariant under
the action of the orthogonal group and scaling [10]  is given by

1 ∆2).

gg(∆1  ∆2) = tr(cid:0)(Y (cid:62)Y )−1∆(cid:62)

(cid:1)

1 ∆2

(4)
where Y (cid:62)∆1 = 0 and Y (cid:62)∆2 = 0. For G(1  n) with a representation y  the metric is given by
gg(∆1  ∆2) = ∆(cid:62)

1 ∆2/y(cid:62)y. The metric is invariant to the scaling of y as shown below

∆(cid:62)
1 ∆2/y(cid:62)y = (k∆1)(cid:62)(k∆2)/(ky)(cid:62)(ky).

(5)
Without loss of generality  we can choose a representation with y(cid:62)y = 1 to obtain gg(∆1  ∆2) =
1 ∆2)  which coincides with the canonical metric for V(1  n). Hereafter  we will focus on the
tr(∆(cid:62)
geometry of G(1  n) with the metric and representation chosen above  derived from the general
formula in [4  10].
Gradient of a function

The gradient of a function f (y) deﬁned on G(1  n) is given by

gradf = g − (yT g)y

where gi = ∂f /∂yi.
Exponential map
emanating from y with initial velocity h is given by

Let h be a tangent vector to G(1  n) at y. The exponential map on G(1  n)

(6)

(7)

(8)

expy(h) = y cos|h| +

h

|h| sin|h|.

It can be easily shown that expy(h) = expy((1 + 2π/|h|)h).
Parallel translation
∆ along the geodesic with the initial velocity h in a unit time is given by

pty(∆; h) = ∆ −(cid:0)u(1 − cos|h|) + y sin|h|(cid:1)u(cid:62)∆ 

Let ∆ and h be tangent vectors to G(1  n) at y. The parallel translation of

where u = h/|h|. Note that |∆| = |pty(∆; h)|. If ∆ = h  it can be further simpliﬁed as

pty(h) = h cos|h| − y|h| sin|h|.

(9)
Note that BN(z) is not invariant to scaling with negative numbers. That is  BN(−z) = −BN(z). To
be precise  there is an one-to-one mapping between the set of weights on which BN(z) is invariant
and a point on V(1  n)  but not on G(1  n). However  the proposed method interprets each weight
vector as a point on the manifold only when the weight update is performed. As long as the weight
vector stays in the domain where V(1  n) and G(1  n) have the same invariance property  the weight
update remains equivalent. We prefer G(1  n) since the operators can easily be extended to G(p  n) 
opening up further applications.

3

(a) Gradient

(b) Exponential map

(c) Parallel translation

Figure 1: An illustration of the operators on the Grassmann manifold G(1  2). A 2-by-1 matrix y
is an orthonormal representation on G(1  2). (a) A gradient calculated in Euclidean coordinate is
projected onto the tangent space TyG(1  2). (b) y1 = expy(h). (c) h1 = pty(h)  |(cid:126)h| = |(cid:126)h1|.

4 Optimization algorithms on G(1  n)
In this section  we derive optimization algorithms on the Grassmann manifold G(1  n). The algorithms
given below are iterative algorithms to solve the following unconstrained optimization:

min

y∈G(1 n)

f (y).

(10)

4.1 Stochastic gradient descent with momentum
The application of Algorithm 1 to the Grassmann manifold G(1  n) is straightforward. We extend
this algorithm to the one with momentum to speed up the training [12]. Algorithm 2 presents the
pseudo-code of the SGD with momentum on G(1  n). This algorithm differs from conventional
SGD in three ways. First  it projects the gradient onto the tangent space at the point y  as shown in
Fig. 1 (a). Second  it moves the position by the exponential map in Fig. 1 (b). Third  it moves the
momentum by the parallel translation of the Grassmann manifold in Fig. 1 (c). Note that if the weight
is initialized with a unit vector  it remains a unit vector after the update.
Algorithm 2 has an advantage over conventional SGD in that the amount of movement is intuitive 
i.e.  it can be measured by the angle between the original point and the new point. As it returns
to the original point after moving by 2π (radian)  it is natural to restrict the maximum movement
induced by a gradient to 2π. For ﬁrst order methods like gradient descent  it would be beneﬁcial to
restrict the maximum movement even more so that it stays in the range where linear approximation
The overall contribution of h is(cid:80)∞
is valid. Let h be the gradient calculated at t = 0. The amount of the ﬁrst step by the gradient
of h is δ0 = η · |h| and the contributions to later steps are recursively calculated by δt = γ · δt−1.
t=0 δt = η · |h|/(1 − γ). In practice  we found it beneﬁcial
to restrict this amount to less than 0.2 (rad) ∼= 11.46◦ by clipping the norm of h at ν. For ex-
ample  with initial learning rate η = 0.2  setting γ = 0.9 and ν = 0.1 guarantees this condition.
Algorithm 2 Stochastic gradient descent with momentum on G(1  n)
Require: learning rate η  momentum coefﬁcient γ  norm_threshold ν
Initialize y0 ∈ Rn×1 with a random unit vector
Initialize τ0 ∈ Rn×1 with a zero vector
for t = 1 ···   T
g ← ∂f (yt−1)/∂y
Run a backward pass to obtain g
h ← g − (y(cid:62)
Project g onto the tangent space at yt−1
ˆh ← norm_clip(h  ν)† Clip the norm of the gradient at ν
d ← γτt−1 − ηˆh
yt ← expyt−1
(d)
τt ← ptyt−1
Note that h  ˆh  d ⊥ yt−1 and τt ⊥ yt where h  ˆh  d  yt−1  yt ∈ Rn×1

Update delta with momentum
Move to the new position by the exponential map in Eq. (7)
Move the momentum by the parallel translation in Eq. (9)

†norm_clip(h  ν) = ν · h/|h| if |h| > ν  else h

t−1g)yt−1

(d)

4

4.2 Adam

Adam [9] is a recently developed ﬁrst-order optimization algorithm based on adaptive estimates
of lower-order moments that has been successfully applied to training deep neural networks. In
this section  we derive Adam on the Grassmann manifold G(1  n). Adam computes the individual
adaptive learning rate for each parameter. In contrast  we assign one adaptive learning rate to each
weight vector that corresponds to a point on the manifold. In this way  the direction of the gradient is
not corrupted  and the size of the step is adaptively controlled. The pseudo-code of Adam on G(1  n)
is presented in Algorithm 3.
It was shown in [9] that the effective step size of Adam (|d| in Algorithm 3) has two upper bounds.
The ﬁrst occurs in the most severe case of sparsity  and the upper bound is given as η 1−β1√
since the
1−β2
previous momentum terms are negligible. The second case occurs if the gradient remains stationary
across time steps  and the upper bound is given as η. For the common selection of hyperparameters
β1 = 0.9  β2 = 0.99  two upper bounds coincide. In our experiments  η was chosen to be 0.05 and
the upper bound was |d| ≤ 0.05 (rad).
Algorithm 3 Adam on G(1  n)
Require: learning rate η  momentum coefﬁcients β1  β2  norm_threshold ν  scalar  = 10−8
Initialize y0 ∈ Rn×1 with a random unit vector
Initialize τ0 ∈ Rn×1 with a zero vector
Initialize a scalar v0 = 0
for t = 1 ···   T

ηt ← η(cid:112)1 − βt

2/(1 − βt
1)

Calculate the bias correction factor
Run a backward pass to obtain g
Project g onto the tangent space at yt−1
Clip the norm of the gradient at ν

g ← ∂f (yt−1)/∂y
h ← g − (y(cid:62)
t−1g)yt−1
ˆh ← norm_clip(h  ν)
mt ← β1 · τt−1 + (1 − β1) · ˆh
vt ← β2 · vt−1 + (1 − β2) · ˆh(cid:62)ˆh
√
d ← −ηt · mt/
yt ← expyt−1 (d)
τt ← ptyt−1
(mt; d)
Note that h  ˆh  mt  d ⊥ yt−1 and τt ⊥ yt where h  ˆh  mt  d  τt  yt−1  yt ∈ Rn×1

(vt is a scalar)
Calculate delta
Move to the new point by exponential map in Eq. (7)
Move the momentum by parallel translation in Eq. (8)

vt + 

5 Batch normalization on the product manifold of G(1 ·)
In Sec. 3  we have shown that a weight vector used to compute the pre-activation that serves as an
input to the BN transform can be naturally interpreted as a point on G(1  n). For deep networks
with multiple layers and multiple units per layer  there can be multiple weight vectors that the BN
transform is applied to. In this case  the training of neural networks is converted into an optimization
problem with respect to a set of points on Grassmann manifolds and the remaining set of parameters.
It is formalized as

minX∈ML(X ) where M = G(1  n1) × ··· × G(1  nm) × Rl

(11)

where n1 . . . nm are the dimensions of weight vectors  m is the number of the weight vectors on
G(1 ·) which will be optimized using Algorithm 2 or 3  and l is the number of remaining parameters
which include biases  learnable scaling and offset parameters in BN layers  and other weight matrices.
Algorithm 4 presents the whole process of training deep neural networks. The forward pass and
backward pass remain unchanged. The only change made is updating the weights by Algorithm 2
or Algorithm 3. Note that we apply the proposed algorithm only when the input layer to BN is
under-complete  that is  the number of output units is smaller than the number of input units  because
the regularization algorithm we will derive in Sec. 5.1 is only valid in this case. There should be ways
to expand the regularization to over-complete layers. However  we do not elaborate on this topic
since 1) the ratio of over-complete layers is very low (under 0.07% for wide resnets and under 5.5%
for VGG networks) and 2) we believe that over-complete layers are suboptimal in neural networks 
which should be avoided by proper selection of network architectures.

5

Algorithm 4 Batch normalization on product manifolds of G(1 ·)
Deﬁne the neural network model with BN layers
m ← 0
for W = {weight matrices in the network such that W (cid:62)x is an input to a BN layer}

Let W be an n × p matrix
if n > p

for i = 1 ···   p
m ← m + 1
Assign a column vector wi in W to ym ∈ G(1  n)

Assign remaining parameters to v ∈ Rl
minL(y1 ···   ym  v)† w.r.t yi ∈ G(1  ni) for i = 1 ···   m and v ∈ Rl
for t = 1 ···   T

Run a forward pass to calculate L
Run a backward pass to obtain ∂L
for i = 1 ···   m

∂yi

for i = 1 ···   m and ∂L

∂v

Update the point yi by Algorithm 2 or Algorithm 3

† For orthogonality regularization as in Sec. 5.1  L is replaced with L +(cid:80)

Update v by conventional optimization algorithms (such as SGD)

W LO(α  W )

5.1 Regularization using variational inference

In conventional neural networks  L2 regularization is normally adopted to regularize the networks.
However  it does not work on Grassmann manifolds because the gradient vector of the L2 regulariza-
tion is perpendicular to the tangent space of the Grassmann manifold. In [13]  the L2 regularization
was derived based on the Gaussian prior and delta posterior in the framework of variational inference.
We extend this theory to Grassmann manifolds in order to derive a proper regularization method in
this space.
Consider the complexity loss  which accounts for the cost of describing the network weights. It is
given by the Kullback-Leibler divergence between the posterior distribution Q(w|β) and the prior
distribution P (w|α) [13]:

LC(α  β) = DKL(Q(w|β) (cid:107) P (w|α)).

(12)

Factor analysis (FA) [14] establishes the link between the Grassmann manifold and the space of
probabilistic distributions [15]. The factor analyzer is given by

p(x) = N (u  C) 

C = ZZ(cid:62) + σ2I

(13)
where Z is a full-rank n-by-p matrix (n > p) and N denotes a normal distribution. Under the
condition that u = 0 and σ → 0  the samples from the analyzer lie in the linear subspace span(Z). In
this way  a linear subspace can be considered as an FA distribution.
Suppose that n-dimensional p weight vectors y1 ···   yp for n > p are in the same layer  which are
assumed as p points on G(1  n). Let yi be a representation of a point such that y(cid:62)
i yi = 1. With the
choice of delta posterior and β = [y1 ···   yp]  the corresponding FA distribution can be given by
q(x|Y ) = N (0  Y Y (cid:62) + σ2I)  where Y = [y1 ···   yp] with the subspace condition σ → 0. The
FA distribution for the prior is set to p(x|α) = N (0  αI) that depends on the hyperparameter α.
Substituting the FA distribution of the prior and posterior into Eq. (12) gives the complexity loss

(cid:0)q(x|Y ) (cid:107) p(x|α)(cid:1).

LC(α  Y ) = DKL

(14)

Eq. (14) is minimized when the column vectors of Y are orthogonal to each other (refer to Appendix A
for details). That is  minimizing LC(α  Y ) will maximally scatter the points away from each other
on G(1  n). However  it is difﬁcult to estimate its gradient. Alternatively  we minimize

(15)
where (cid:107) · (cid:107)F is the Frobenius norm. It has the same minimum as the original complexity loss and the
negative of its gradient is a descent direction of the original loss (refer to Appendix B).

LO(α  Y ) =

F

(cid:107) Y (cid:62)Y − I (cid:107)2

α
2

6

6 Experiments

We evaluated the proposed learning algorithm for image classiﬁcation tasks using three benchmark
datasets: CIFAR-10 [16]  CIFAR-100 [16]  and SVHN (Street View House Number) [17]. We used
the VGG network [18] and wide residual network [2  19  20] for experiments. The VGG network
is a widely used baseline for image classiﬁcation tasks  while the wide residual network [2] has
shown state-of-the-art performance on the benchmark datasets. We followed the experimental setups
described in [2] so that the performance of algorithms can be directly compared. Source code is
publicly available at https://github.com/MinhyungCho/riemannian-batch-normalization.
CIFAR-10 is a database of 60 000 color images in 10 classes  which consists of 50 000 training
images and 10 000 test images. CIFAR-100 is similar to CIFAR-10  except that it has 100 classes
and contains fewer images per class. For preprocessing  we normalized the data using the mean
and variance calculated from the training set. During training  the images were randomly ﬂipped
horizontally  padded by four pixels on each side with the reﬂection  and a 32×32 crop was randomly
sampled. SVHN [17] is a digit classiﬁcation benchmark dataset that contains 73 257 images in the
training set  26 032 images in the test set  and 531 131 images in the extra set. We merged the extra
set and the training set in our experiment  following the step in [2]. The only preprocessing done was
to divide the intensity by 255.
Detailed architectures for various VGG networks are described in [18]. We used 512 neurons in
fully connected layers rather than 4096 neurons  and the BN layer was placed before every ReLU
activation layer. The learnable scaling parameter in the BN layer was set to one because it does not
reduce the expressive power of the ReLU layer [21]. For SVHN experiments using VGG networks 
the dropout was applied after the pooling layer with dropout rate 0.4. For wide residual networks 
we adopted exactly the same model architectures in [2]  including the BN and dropout layers. In all
cases  the biases were removed except the ﬁnal layer.
For the baseline  the networks were trained by SGD with Nesterov momentum [22]. The weight
decay was set to 0.0005  momentum to 0.9  and minibatch size to 128. For CIFAR experiments  the
initial learning rate was set to 0.1 and multiplied by 0.2 at 60  120  and 160 epochs. It was trained for
a total of 200 epochs. For SVHN  the initial learning rate was set to 0.01 and multiplied by 0.1 at 60
and 120 epochs. It was trained for a total of 160 epochs.
For the proposed method  we used different learning rates for the weights in Euclidean space and
on Grassmann manifolds. Let us denote the learning rates for Euclidean space and Grassmann
manifolds as ηe and ηg  respectively. The selected initial learning rates were ηe = 0.01  ηg = 0.2 for
Algorithm 2 and ηe = 0.01  ηg = 0.05 for Algorithm 3. The same initial learning rates were used
for all CIFAR experiments. For SVHN  they were scaled by 1/10  following the same ratio as the
baseline [2]. The training algorithm for Euclidean parameters was identical to the one used in the
baseline with one exception. We did not apply weight decay to scaling and offset parameters of BN 
whereas the baseline did as in [2]. To clarify  applying weight decay to mean and variance parameters
of BN was essential for reproducing the performance of baseline. The learning rate schedule was
also identical to the baseline  both for ηe and ηg. The threshold for clipping the gradient ν was set to
0.1. The regularization strength α in Eq. (15) was set to 0.1  which gradually achieved near zero LO
during the course of the training  as shown in Fig. 2.

Figure 2: Changes in LO in
Eq. (15) during training for vari-
ous α values (y-axis on the left).
The red dotted line denotes the
learning rate (ηg  y-axis on the
right). VGG-11 was trained by
SGD-G on CIFAR-10.

6.1 Results

Tables 1 and 2 compare the performance of the baseline SGD and two proposed algorithms described
in Sec. 4 and 5  on CIFAR-10  CIFAR-100  and SVHN datasets. All the numbers reported are the
median of ﬁve independent runs. In most cases  the networks trained using the proposed algorithms

7

(a) CIFAR-10

(b) CIFAR-100

(c) SVHN

Figure 3: Training curves of the baseline and proposed optimization methods. (a) WRN-28-10 on
CIFAR-10. (b) WRN-28-10 on CIFAR-100. (c) WRN-22-8 on SVHN.

outperformed the baseline across various datasets and network conﬁgurations  especially for the ones
with more parameters. The best performance was 3.72% (SGD and SGD-G) and 17.85% (ADAM-G)
on CIFAR-10 and CIFAR-100  respectively  with WRN-40-10; and 1.55% (ADAM-G) on SVHN
with WRN-22-8.
Training curves of the baseline and proposed methods are presented in Figure 3. The training curves
for SGD suffer from instability or experience a plateau after each learning rate drop  compared to
the proposed methods. We believe that this comes from the inverse proportionality of the gradient to
the norm of BN weight parameters (as in Eq. (2)). During the training process  this norm is affected
by weight decay  hence the magnitude of the gradient. It is effectively equivalent to disturbing the
learning rate by weight decay. The authors of wide resnet also observed that applying weight decay
caused this phenomena  but weight decay was indispensable for achieving the reported performance
[2]. Proposed methods resolve this issue in a principled way.
Table 3 summarizes the performance of recently published algorithms on the same datasets. We
present the best performance of ﬁve independent runs in this table.

Table 1: Classiﬁcation error rate of various networks on CIFAR-10 and CIFAR-100 (median of ﬁve
runs). VGG-l denotes a VGG network with l layers. WRN-d-k denotes a wide residual network that
has d convolutional layers and a widening factor k. SGD-G and Adam-G denote Algorithm 2 and
Algorithm 3  respectively. The results in parenthesis show those reported in [2].
CIFAR-100

CIFAR-10

Dataset
Model
VGG-11
VGG-13
VGG-16
VGG-19
WRN-52-1
WRN-16-4
WRN-28-10
WRN-40-10†
†This model was trained on two GPUs. The gradients were summed from two minibatches of size 64  and
BN statistics were calculated from each minibatch.

SGD-G
28.02
25.29
25.64
25.79
28.13
24.51
18.19
18.04

27.44 (29.78)
23.41 (23.91)
18.66 (18.85)
18.39 (18.3)

Adam-G

28.05
24.89
25.29
25.59
28.16
24.24
18.30
17.85

7.59
6.05
5.98
6.02
6.58
5.28
3.78
3.80

SGD
29.25
26.17
26.84
27.62

SGD-G Adam-G

SGD
7.43
5.88
6.32
6.49

6.23 (6.28)
4.96 (5.24)
3.89 (3.89)
3.72 (3.8)

7.14
5.87
5.88
5.92
6.56
5.35
3.85
3.72

7 Conclusion and discussion
We presented new optimization algorithms for scale-invariant vectors by representing them on G(1  n)
and following the intrinsic geometry. Speciﬁcally  we derived SGD with momentum and Adam
algorithms on G(1  n). An efﬁcient regularization algorithm in this space has also been proposed.
Applying them in the context of BN showed consistent performance improvements over the baseline
BN algorithm with SGD on CIFAR-10  CIFAR-100  and SVHN datasets.

8

Table 2: Classiﬁcation error rate of various networks on SVHN (median of ﬁve runs).

SGD-G Adam-G

Model
VGG-11
VGG-13
VGG-16
VGG-19
WRN-52-1
WRN-16-4
WRN-16-8
WRN-22-8

SGD
2.11
1.78
1.85
1.94

1.68 (1.70)
1.64 (1.64)
1.60 (1.54)

1.64

2.14
1.72
1.76
1.77
1.67
1.61
1.68
1.55

2.10
1.74
1.76
1.81
1.72
1.67
1.69
1.63

7.47
6.55
6.37
6.05
4.91
4.62
3.8
3.491

Table 3: Performance comparison with previously published results.
SVHN
1.88

CIFAR-10 CIFAR-100

NormProp [7]

Method

ELU [23]

Scalable Bayesian optimization [24]

Generalizing pooling [25]

Stochastic depth [26]

ResNet-1001 [20]

Wide residual network [2]
Proposed (best of ﬁve runs)

29.24
24.28
27.4

-

24.98
22.71
18.3
17.592

-
-

-

1.69
1.75

1.54
1.493

1WRN-40-10+SGD-G 2WRN-40-10+Adam-G 3WRN-22-8+Adam-G

Our work interprets each scale invariant piece of the weight matrix as a separate manifold  whereas
natural gradient based algorithms [27  28  29] interpret the whole parameter space as a manifold and
constrain the shape of the cost function (i.e. to the KL divergence) to obtain a cost efﬁcient metric.
There are similar approaches to ours such as Path-SGD [30] and the one based on symmetry-invariant
updates [31]  but the comparison remains to be done.
Proposed algorithms are computationally as efﬁcient as their non-manifold versions since they do not
affect the forward and backward propagation steps  where majority of the computation takes place.
The weight update step is 2.5-3.5 times more expensive  but still O(n).
We did not explore the full range of possibilities offered by the proposed algorithm. For example 
techniques similar to BN  such as weight normalization [6] and normalization propagation [7]  have
scale invariant weight vectors and can beneﬁt from the proposed algorithm in the same way. Layer
normalization [8] is invariant to weight matrix rescaling  and simple vectorization of the weight
matrix enables the application of the proposed algorithm.

9

References

[1] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning  pages
448–456  2015.

[2] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC  2016.
[3] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pages 2818–2826  2016.

[4] Alan Edelman  Tomás A Arias  and Steven T Smith. The geometry of algorithms with orthogonality

constraints. SIAM journal on Matrix Analysis and Applications  20(2):303–353  1998.

[5] P-A Absil  Robert Mahony  and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.

[6] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate
In Advances in Neural Information Processing Systems 29  pages

Princeton University Press  2009.

training of deep neural networks.
901–909  2016.

[7] Devansh Arpit  Yingbo Zhou  Bhargava Kota  and Venu Govindaraju. Normalization propagation: A
parametric technique for removing internal covariate shift in deep networks. In Proceedings of The 33rd
International Conference on Machine Learning  pages 1168–1176  2016.

[8] Jimmy Lei Ba  Jamie Ryan Kiros  and Geoffrey E Hinton. Layer normalization. arXiv preprint

[9] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1607.06450  2016.

arXiv:1412.6980  2014.

[10] P-A Absil  Robert Mahony  and Rodolphe Sepulchre. Riemannian geometry of grassmann manifolds with

a view on algorithmic computation. Acta Applicandae Mathematicae  80(2):199–220  2004.

[11] M.P. do Carmo. Differential Geometry of Curves and Surfaces. Prentice-Hall  1976.
[12] David E. Rumelhart  Geoffrey E. Hinton  and Ronald J. Williams. Learning representations by back-

propagating errors. Nature  323(6088):533–536  10 1986.

[13] Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information

Processing Systems 24  pages 2348–2356  2011.

[14] Zoubin Ghahramani  Geoffrey E Hinton  et al. The EM algorithm for mixtures of factor analyzers.

Technical report  Technical Report CRG-TR-96-1  University of Toronto  1996.

[15] Jihun Hamm and Daniel D Lee. Extended grassmann kernels for subspace-based learning. In Advances in

Neural Information Processing Systems 21  pages 601–608  2009.

[16] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Master’s

thesis  Department of Computer Science  University of Toronto  2009.

[17] Yuval Netzer  Tao Wang  Adam Coates  Alessandro Bissacco  Bo Wu  and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised
feature learning  2011.

[18] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. arXiv preprint arXiv:1409.1556  2014.

[19] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 770–778 
2016.

[20] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Identity mappings in deep residual networks.

In European Conference on Computer Vision  pages 630–645. Springer  2016.

[21] Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models.

arXiv preprint arXiv:1702.03275  2017.

[22] Ilya Sutskever  James Martens  George Dahl  and Geoffrey Hinton. On the importance of initialization and
momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning 
pages 1139–1147  2013.

[23] Djork-Arné Clevert  Thomas Unterthiner  and Sepp Hochreiter. Fast and accurate deep network learning

by exponential linear units (ELUs). arXiv preprint arXiv:1511.07289  2015.

[24] Jasper Snoek  Oren Rippel  Kevin Swersky  Ryan Kiros  Nadathur Satish  Narayanan Sundaram  Mostofa
Patwary  Mr Prabhat  and Ryan Adams. Scalable bayesian optimization using deep neural networks. In
International Conference on Machine Learning  pages 2171–2180  2015.

[25] Chen-Yu Lee  Patrick W Gallagher  and Zhuowen Tu. Generalizing pooling functions in convolutional
neural networks: Mixed  gated  and tree. In International conference on artiﬁcial intelligence and statistics 
2016.

[26] Gao Huang  Yu Sun  Zhuang Liu  Daniel Sedra  and Kilian Q Weinberger. Deep networks with stochastic

depth. In European Conference on Computer Vision  pages 646–661. Springer  2016.

[27] Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural computation  10(2):251–276 

1998.

10

[28] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In International

Conference on Learning Representations  2014.

[29] James Martens and Roger B Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In Proceedings of The 32nd International Conference on Machine Learning  pages 2408–2417 
2015.

[30] Behnam Neyshabur  Ruslan R Salakhutdinov  and Nati Srebro. Path-SGD: Path-normalized optimization
in deep neural networks. In Advances in Neural Information Processing Systems  pages 2422–2430  2015.
[31] Vijay Badrinarayanan  Bamdev Mishra  and Roberto Cipolla. Understanding symmetries in deep networks.

arXiv preprint arXiv:1511.01029  2015.

11

,Minhyung Cho
Jaehyung Lee
Rajat Sen
Hsiang-Fu Yu
Inderjit Dhillon