2014,A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank Designs,We study the residual bootstrap (RB) method in the context of high-dimensional linear regression. Specifically  we analyze the distributional approximation of linear contrasts $c^{\top}(\hat{\beta}_{\rho}-\beta)$  where $\hat{\beta}_{\rho}$ is a ridge-regression estimator. When regression coefficients are estimated via least squares  classical results show that RB consistently approximates the laws of contrasts  provided that $p\ll n$  where the design matrix is of size $n\times p$. Up to now  relatively little work has considered how additional structure in the linear model may extend the validity of RB to the setting where $p/n\asymp 1$. In this setting  we propose a version of RB that resamples residuals obtained from ridge regression. Our main structural assumption on the design matrix is that it is nearly low rank --- in the sense that its singular values decay according to a power-law profile. Under a few extra technical assumptions  we derive a simple criterion for ensuring that RB consistently approximates the law of a given contrast. We then specialize this result to study confidence intervals for mean response values $X_i^{\top} \beta$  where $X_i^{\top}$ is the $i$th row of the design. More precisely  we show that conditionally on a Gaussian design with near low-rank structure  RB \emph{simultaneously} approximates all of the laws $X_i^{\top}(\hat{\beta}_{\rho}-\beta)$  $i=1 \dots n$. This result is also notable as it imposes no sparsity assumptions on $\beta$. Furthermore  since our consistency results are formulated in terms of the Mallows (Kantorovich) metric  the existence of a limiting distribution is not required.,A Residual Bootstrap for High-Dimensional
Regression with Near Low-Rank Designs

Miles E. Lopes

Department of Statistics

University of California  Berkeley

Berkeley  CA 94720

mlopes@stat.berkeley.edu

Abstract

We study the residual bootstrap (RB) method in the context of high-dimensional
linear regression. Speciﬁcally  we analyze the distributional approximation of lin-

ear contrasts c(cid:62)((cid:98)βρ − β)  where(cid:98)βρ is a ridge-regression estimator. When regres-

sion coefﬁcients are estimated via least squares  classical results show that RB
consistently approximates the laws of contrasts  provided that p (cid:28) n  where the
design matrix is of size n × p. Up to now  relatively little work has considered
how additional structure in the linear model may extend the validity of RB to the
setting where p/n (cid:16) 1. In this setting  we propose a version of RB that resamples
residuals obtained from ridge regression. Our main structural assumption on the
design matrix is that it is nearly low rank — in the sense that its singular values
decay according to a power-law proﬁle. Under a few extra technical assump-
tions  we derive a simple criterion for ensuring that RB consistently approximates
the law of a given contrast. We then specialize this result to study conﬁdence
intervals for mean response values X(cid:62)
is the ith row of the de-
sign. More precisely  we show that conditionally on a Gaussian design with near
low-rank structure  RB simultaneously approximates all of the laws X(cid:62)
i = 1  . . .   n. This result is also notable as it imposes no sparsity assumptions on
β. Furthermore  since our consistency results are formulated in terms of the Mal-
lows (Kantorovich) metric  the existence of a limiting distribution is not required.

i ((cid:98)βρ − β) 

i β  where X(cid:62)

i

1

Introduction

Until recently  much of the emphasis in the theory of high-dimensional statistics has been on “ﬁrst
order” problems  such as estimation and prediction. As the understanding of these problems has
become more complete  attention has begun to shift increasingly towards “second order” problems 
dealing with hypothesis tests  conﬁdence intervals  and uncertainty quantiﬁcation [1–6].
In this
direction  much less is understood about the effects of structure  regularization  and dimensionality
— leaving many questions open. One collection of such questions that has attracted growing interest
deals with the operating characteristics of the bootstrap in high dimensions [7–9] . Due to the fact
that bootstrap is among the most widely used tools for approximating the sampling distributions of
test statistics and estimators  there is much practical value in understanding what factors allow for
the bootstrap to succeed in the high-dimensional regime.

The regression model and linear contrasts.
In this paper  we focus our attention on high-
dimensional linear regression  and our aim is to know when the residual bootstrap (RB) method
consistently approximates the laws of linear contrasts. (A review of RB is given in Section 2.)

1

Y = Xβ + ε 

To specify the model  suppose that we observe a response vector Y ∈ Rn  generated according to
(1)
where X ∈ Rn×p is the observed design matrix  β ∈ Rp is an unknown vector of coefﬁcients  and
the error variables ε = (ε1  . . .   εn) are drawn i.i.d. from an unknown distribution F0  with mean
0 and unknown variance σ2 < ∞. As is conventional in high-dimensional statistics  we assume
the model (1) is embedded in a sequence of models indexed by n. Hence  we allow X  β  and p to
vary implicitly with n. We will leave p/n unconstrained until Section 3.3  where we will assume
p/n (cid:16) 1 in Theorem 3  and then in Section 3.4  we will assume further that p/n is bounded strictly
between 0 and 1. The distribution F0 is ﬁxed with respect to n  and none of our results require F0
to have more than four moments.
Although we are primarily interested in cases where the design matrix X is deterministic  we will
also study the performance of the bootstrap conditionally on a Gaussian design. For this reason 
we will use the symbol E[. . .|X] even when the design is non-random so that confusion does not
arise in relating different sections of the paper. Likewise  the symbol E[. . . ] refers to unconditional
expectation over all sources of randomness. Whenever the design is random  we will assume X ⊥⊥ ε 
denoting the distribution of X by PX  and the distribution of ε by Pε.

Within the context of the regression  we will be focused on linear contrasts c(cid:62)((cid:98)β−β)  where c ∈ Rp
is a ﬁxed vector and (cid:98)β ∈ Rp is an estimate of β. The importance of contrasts arises from the fact

that they unify many questions about a linear model. For instance  testing the signiﬁcance of the ith
coefﬁcient βi may be addressed by choosing c to be the standard basis vector c(cid:62) = e(cid:62)
i . Another
important problem is quantifying the uncertainty of point predictions  which may be addressed by
choosing c(cid:62) = X(cid:62)
i   i.e. the ith row of the design matrix. In this case  an approximation to the law
of the contrast leads to a conﬁdence interval for the mean response value E[Yi] = X(cid:62)
i β. Further
applications of contrasts occur in the broad topic of ANOVA [10].

ordinary least squares estimator  then it is a simple but important fact that contrasts can be written

Intuition for structure and regularization in RB. The following two paragraphs explain the core
conceptual aspects of the paper. To understand the role of regularization in applying RB to high-

dimensional regression  it is helpful to think of RB in terms of two ideas. First  if (cid:98)βLS denotes the
as c(cid:62)((cid:98)βLS − β) = a(cid:62)ε where a(cid:62):= c(cid:62)(X(cid:62)X)−1X(cid:62). Hence  if it were possible to sample directly
second key idea is to use the residuals of some estimator (cid:98)β as a proxy for samples from F0. When
squares tends to overﬁt when p/n (cid:16) 1. When (cid:98)βLS ﬁts “too well”  this means that its residuals are
“too small”  and hence they give a poor proxy for F0. Therefore  by using a regularized estimator(cid:98)β 
overﬁtting can be avoided  and the residuals of (cid:98)β may offer a better way of obtaining “approximate

from F0  then the law of any such contrast could be easily determined. Since F0 is unknown  the
p (cid:28) n  the least-squares residuals are a good proxy [11  12]. However  it is well-known that least-

samples” from F0.
The form of regularized regression we will focus on is ridge regression:

(cid:98)βρ := (X(cid:62)X + ρIp×p)−1X(cid:62)Y 

(2)
where ρ > 0 is a user-speciﬁced regularization parameter. As will be seen in Sections 3.2 and 3.3 
the residuals obtained from ridge regression lead to a particularly good approximation of F0 when
the design matrix X is nearly low-rank  in the sense that most of its singular values are close to
0. In essence  this condition is a form of sparsity  since it implies that the rows of X nearly lie
in a low-dimensional subspace of Rp. However  this type of structural condition has a signiﬁcant
advantage over the the more well-studied assumption that β is sparse. Namely  the assumption that
X is nearly low-rank can be inspected directly in practice — whereas sparsity in β is typically
In fact  our results will impose no conditions on β  other than that (cid:107)β(cid:107)2 remains
unveriﬁable.
bounded as (n  p) → ∞. Finally  it is worth noting that the occurrence of near low-rank design
matrices is actually very common in applications  and is often referred to as collinearity [13  ch.
17].

Contributions and outline. The primary contribution of this paper is a complement to the work of
Bickel and Freedman [12] (hereafter B&F 1983) — who showed that in general  the RB method fails

2

an alternative set of results  proving that even when p/n (cid:16) 1  RB can successfully approximate the

to approximate the laws of least-squares contrasts c(cid:62)((cid:98)βLS − β) when p/n (cid:16) 1. Instead  we develop
laws of “ridged contrasts” c(cid:62)((cid:98)βρ − β) for many choices of c ∈ Rp  provided that the design matrix
approximates the law c(cid:62)((cid:98)βρ − β) for a certain choice of c that was shown in B&F 1983 to “break”

X is nearly low rank. A particularly interesting consequence of our work is that RB successfully

RB when applied to least-squares. Speciﬁcally  such a c can be chosen as one of the rows of X with
a high leverage score (see Section 4). This example corresponds to the practical problem of setting
conﬁdence intervals for mean response values E[Yi] = X(cid:62)
i β. (See [12  p. 41]  as well as Lemma 2
and Theorem 4 in Section 3.4). Lastly  from a technical point of view  a third notable aspect of our
results is that they are formulated in terms of the Mallows-(cid:96)2 metric  which frees us from having to
impose conditions that force a limiting distribution to exist.
Apart from B&F 1983  the most closely related works we are aware of are the recent papers [7]
and [8]  which also consider RB in the high-dimensional setting. However  these works focus on
role of sparsity in β and do not make use of low-rank structure in the design  whereas our work deals
only with structure in the design and imposes no sparsity assumptions on β.
The remainder of the paper is organized as follows. In Section 2  we formulate the problem of
approximating the laws of contrasts  and describe our proposed methodology for RB based on ridge
regression. Then  in Section 3 we state several results that lay the groundwork for Theorem 4  which
shows that that RB can successfully approximate all of the laws L(X(cid:62)
conditionally on a Gaussian design. Due to space constraints  all proofs are deferred to material that
will appear in a separate work.

i ((cid:98)βρ − β)|X)  i = 1  . . .   n 

If U and V are random variables  then L(U|V ) denotes the law of U 
Notation and conventions.
conditionally on V . If an and bn are two sequences of real numbers  then the notation an (cid:46) bn
means that there is an absolute constant κ0 > 0 and an integer n0 ≥ 1 such that an ≤ κ0bn for all
n ≥ n0. The notation an (cid:16) bn means that an (cid:46) bn and bn (cid:46) an. For a square matrix A ∈ Rk×k
whose eigenvalues are real  we denote them by λmin(A) = λk(A) ≤ ··· ≤ λ1(A) = λmax(A).

2 Problem setup and methodology

Problem setup. For any c ∈ Rp  it is clear that conditionally on X  the law of c(cid:62)((cid:98)βρ − β) is

completely determined by F0  and hence it makes sense to use the notation

(3)
The problem we aim to solve is to approximate the distribution Ψρ(F0; c) for suitable choices of c.

Review of the residual bootstrap (RB) procedure. We brieﬂy explain the steps involved in the

residual bootstrap procedure  applied to the ridge estimator(cid:98)βρ of β. To proceed somewhat indirectly 

consider the following “bias-variance” decomposition of Ψρ(F0; c)  conditionally on X 

Ψρ(F0; c) := L(cid:0)c(cid:62)((cid:98)βρ − β)X(cid:1).

Ψρ(F0; c) = L(cid:0)c(cid:62)(cid:0)(cid:98)βρ − E[(cid:98)βρ|X](cid:1)X(cid:1)
(cid:125)

(cid:123)(cid:122)

(cid:124)

=: Φρ(F0;c)

+ c(cid:62)(cid:0)E[(cid:98)βρ|X] − β(cid:1)
(cid:124)
(cid:125)

(cid:123)(cid:122)

=: bias(Φρ(F0;c))

.

(4)

Note that the distribution Φ(F0; c) has mean zero  and so that the second term on the right side is
the bias of Φρ(F0; c) as an estimator of Ψρ(F0; c). Furthermore  the distribution Φρ(F0; c) may be
viewed as the “variance component” of Ψρ(F0; c). We will be interested in situations where the
regularization parameter ρ may be chosen small enough so that the bias component is small. In this
case  one has Ψρ(F0; c) ≈ Φρ(F0; c)  and then it is enough to ﬁnd an approximation to the law

Φρ(F0; c)  which is unknown. To this end  a simple manipulation of c(cid:62)((cid:98)βρ − E[(cid:98)βρ]) leads to
Now  to approximate Φρ(F0; c)  let (cid:98)F be any centered estimate of F0. (Typically  (cid:98)F is obtained by
n) ∈ Rn be an i.i.d. sample from (cid:98)F . Then  replacing ε with ε∗ in line (5) yields

Φρ(F0; c) = L(c(cid:62)(X(cid:62)X + ρIp×p)−1X(cid:62)εX).
Φρ((cid:98)F ; c) = L(c(cid:62)(X(cid:62)X + ρIp×p)−1X(cid:62)ε∗X).

using the centered residuals of some estimator of β  but this is not necessary in general.) Also  let
ε∗ = (ε∗

1  . . .   ε∗

(6)

(5)

3

Hence  it is clear that the RB approximation is simply a “plug-in rule”.

Ψρ(F0; c). One way of exploiting this ﬂexibility is to consider a two-stage approach  where a “pilot”

A two-stage approach. An important feature of the procedure just described is that we are free

At this point  we deﬁne the (random) measure Φρ((cid:98)F ; c) to be the RB approximation to Φρ(F0; c).
to use any centered estimator (cid:98)F of F0. This fact offers substantial ﬂexibility in approximating
ridge estimator (cid:98)β is used to ﬁrst compute residuals whose centered empirical distribution function
is (cid:98)F  say. Then  in the second stage  the distribution (cid:98)F is used to approximate Φρ(F0; c) via the
relation (6). To be more detailed  if ((cid:98)e1()  . . .  (cid:98)en()) =(cid:98)e() := Y − X(cid:98)β are the residuals of(cid:98)β 
then we deﬁne (cid:98)F to be the distribution that places mass 1/n at each of the values(cid:98)ei() − ¯e() with
(cid:80)n
i=1(cid:98)ei(). Here  it is important to note that the value  is chosen to optimize (cid:98)F as an
coverage probability for conﬁdence intervals based on Φρ((cid:98)F; c). Theorems 1  3  and 4 will offer

¯e() := 1
n
approximation to F0. By contrast  the choice of ρ depends on the relative importance of width and

some guidance in selecting  and ρ.

Resampling algorithm. To summarize the discussion above  if B is user-speciﬁed number of
bootstrap replicates  our proposed method for approximating Ψρ(F0; c) is given below.

1. Select ρ and   and compute the residuals(cid:98)e() = Y − X(cid:98)β.
2. Compute the centered distribution function (cid:98)F  putting mass 1/n at each(cid:98)ei() − ¯e().
• Draw a vector ε∗ ∈ Rn of n i.i.d. samples from (cid:98)F.

3. For j = 1  . . .   B:

• Compute zj := c(cid:62)(X(cid:62)X + ρIp×p)−1X(cid:62)ε∗.

4. Return the empirical distribution of z1  . . .   zB.

Clearly  as B → ∞  the empirical distribution of z1  . . .   zB converges weakly to Φρ((cid:98)F; c)  with
issues  and address only the performance of Φρ((cid:98)F; c) as an approximation to Ψρ(F0; c).

probability 1. As is conventional  our theoretical analysis in the next section will ignore Monte Carlo

3 Main results

The following metric will be central to our theoretical results  and has been a standard tool in the
analysis of the bootstrap  beginning with the work of Bickel and Freedman [14].

The Mallows (Kantorovich) metric. For two random vectors U and V in a Euclidean space  the
Mallows-(cid:96)2 metric is deﬁned by

2(L(U ) L(V )) := inf
d2
π∈Π

(7)
where the inﬁmum is over the class Π of joint distributions π whose marginals are L(U ) and L(V ).
It is worth noting that convergence in d2 is strictly stronger than weak convergence  since it also
requires convergence of second moments. Additional details may be found in the paper [14].

: (U  V ) ∼ π

2

(cid:110)E(cid:104)(cid:107)U − V (cid:107)2

(cid:105)

(cid:111)

3.1 A bias-variance decomposition for bootstrap approximation

To give some notation for analyzing the bias-variance decomposition of Ψρ(F0; c) in line (4)  we

deﬁne the following quantities based upon the ridge estimator(cid:98)βρ. Namely  the variance is

vρ = vρ(X; c) := var(Ψρ(F0; c)|X) = σ2(cid:107)c(cid:62)(X(cid:62)X + ρIp×p)−1X(cid:62)(cid:107)2
2.

To express the bias of Φρ(F0; c)  we deﬁne the vector δ(X) ∈ Rp according to

δ(X) := β − E[(cid:98)βρ] =(cid:2)Ip×p − (X(cid:62)X + ρIp×p)−1X(cid:62)X(cid:3)β 

(8)

4

and then put

b2
ρ = b2

ρ(X; c) := bias2(Φρ(F0; c)) = (c(cid:62)δ(X))2.

(9)

ρ(X; c) only depends on β through δ(X).

ρ to lighten notation. Note that vρ(X; c) does not

We will sometimes omit the arguments of vρ and b2
depend on β  and b2
The following result gives a regularized and high-dimensional extension of some lemmas in Freed-
man’s early work [11] on RB for least squares. The result does not require any structural conditions
on the design matrix  or on the true parameter β.

Theorem 1 (consistency criterion). Suppose X ∈ Rn×p is ﬁxed. Let (cid:98)F be any estimator of F0  and

let c ∈ Rp be any vector such that vρ = vρ(X; c) (cid:54)= 0. Then with Pε-probability 1  the following
inequality holds for every n ≥ 1  and every ρ > 0 

(cid:16) 1√

d2
2

Ψρ(F0; c) 

1√
vρ

vρ

Φρ((cid:98)F ; c)

(cid:17) ≤ 1

2(F0 (cid:98)F ) +

σ2 d2

b2
ρ
vρ

.

(10)

√

Remarks. Observe that the normalization 1/
vρ ensures that the bound is non-trivial  since the
√
vρ has variance equal to 1 for all n (and hence does not become degenerate
distribution Ψρ(F0; c)/
ρ/vρ decreases mono-
for large n). To consider the choice of ρ  it is simple to verify that the ratio b2
tonically as ρ decreases. Note also that as ρ becomes small  the variance vρ becomes large  and

likewise  conﬁdence intervals based on Φρ((cid:98)F ; c) become wider. In other words  there is a trade-off

between the width of the conﬁdence interval and the size of the bound (10).

Sufﬁcient conditions for consistency of RB. An important practical aspect of Theorem 1 is that
for any given contrast c  the variance vρ(X; c) can be easily estimated  since it only requires an

estimate of σ2  which can be obtained from (cid:98)F . Consequently  whenever theoretical bounds on
2(F0 (cid:98)F ) and b2
2(F0 (cid:98)F )|X] in the case where (cid:98)F is chosen to be (cid:98)F. Later on in

In this way 
d2
Theorem 1 offers a simple route for guaranteeing that RB is consistent. In Sections 3.2 and 3.3 to
follow  we derive a bound on E[d2
Section 3.4  we study RB consistency in the context of prediction with a Gaussian design  and there
ρ(X; c) where c is a particular row of X.
we derive high probability bounds on both vρ(X; c) and b2

ρ(X; c) are available  the right side of line (10) can be controlled.

as

3.2 A link between bootstrap consistency and MSPE

If (cid:98)β is an estimator of β  its mean-squared prediction error (MSPE)  conditionally on X  is deﬁned

mspe((cid:98)β |X) := 1

E(cid:2)(cid:107)X((cid:98)β − β)(cid:107)2

X(cid:3).

n

(11)
The previous subsection showed that in-law approximation of contrasts is closely tied to the approx-
imation of F0. We now take a second step of showing that if the centered residuals of an estimator

(cid:98)β are used to approximate F0  then the quality of this approximation can be bounded naturally in
terms of mspe((cid:98)β |X). This result applies to any estimator(cid:98)β computed from the observations (1).
Theorem 2. Suppose X ∈ Rn×p is ﬁxed. Let (cid:98)β be any estimator of β  and let (cid:98)F be the empirical
distribution of the centered residuals of (cid:98)β. Also  let Fn denote the empirical distribution of n i.i.d.

2

samples from F0. Then for every n ≥ 1 

2((cid:98)F   F0)X(cid:3) ≤ 2 mspe((cid:98)β |X) + 2 E[d2
E(cid:2)d2

2(Fn  F0)] + 2σ2
n .

(12)

Remarks. As we will see in the next section  the MSPE of ridge regression can be bounded in a

sharp way when the design matrix is approximately low rank  and there we will analyze mspe((cid:98)β|X)
for the pilot estimator. Consequently  when near low-rank structure is available  the only remaining
2(Fn  F0)|X]. The very
issue in controlling the right side of line (12) is to bound the quantity E[d2
recent work of Bobkov and Ledoux [15] provides an in-depth study of this question  and they derive
a variety bounds under different tail conditions on F0. We summarize one of their results below.
Lemma 1 (Bobkov and Ledoux  2014). If F0 has a ﬁnite fourth moment  then

E[d2

2(Fn  F0)] (cid:46) log(n)n−1/2.

(13)

5

Remarks. The fact that the squared distance is bounded at the rate of log(n)n−1/2 is an indica-
tion that d2 is a rather strong metric on distributions. For a detailed discussion of this result  see
Corollaries 7.17 and 7.18 in the paper [15]. Although it is possible to obtain faster rates when more

stringent tail conditions are placed on F0  we will only need a fourth moment  since the mspe((cid:98)β|X)

term in Theorem 2 will often have a slower rate than log(n)n−1/2  as discussed in the next section.

3.3 Consistency of ridge regression in MSPE for near low rank designs

In this subsection  we show that when the tuning parameter  is set at a suitable rate  the pilot ridge

estimator (cid:98)β is consistent in MSPE when the design matrix is near low-rank — even when p/n is

large  and without any sparsity constraints on β. We now state some assumptions.
A1. There is a number ν > 0  and absolute constants κ1  κ2 > 0  such that
i = 1  . . .   n ∧ p.

κ1i−ν ≤ λi((cid:98)Σ) ≤ κ2i−ν

for all

A2. There are absolute constants θ  γ > 0  such that for every n ≥ 1  
A3. The vector β ∈ Rp satisﬁes (cid:107)β(cid:107)2 (cid:46) 1.

n = n−γ.
Due to Theorem 2  the following bound shows that the residuals of (cid:98)β may be used to extract a

n = n−θ and ρ

consistent approximation to F0. Two other notable features of the bound are that it is non-asymptotic
and dimension-free.
Theorem 3. Suppose that X ∈ Rn×p is ﬁxed and that Assumptions 1–3 hold  with p/n (cid:16) 1. Assume
further that θ is chosen as θ = 2ν

ν+1 when ν > 1

2 . Then 

(cid:40)
3 when ν ∈ (0  1

mspe((cid:98)β|X) (cid:46)

2 )  and θ = ν
n− 2ν
n− ν

ν+1

if ν ∈ (0  1
2 ) 
if ν > 1
2 .

3

Also  both bounds in (14) are tight in the sense that β can be chosen so that(cid:98)β attains either rate.
Remarks. Since the eigenvalues λi((cid:98)Σ) are observable  they may be used to estimate ν and guide

the selection of /n = n−θ. However  from a practical point of view  we found it easier to select 
via cross-validation in numerical experiments  rather than via an estimate of ν.

(14)

A link with Pinsker’s Theorem.
In the particular case when F0 is a centered Gaussian distribu-
tion  the “prediction problem” of estimating Xβ is very similar to estimating the mean parameters of
a Gaussian sequence model  with error measured in the (cid:96)2 norm. In the alternative sequence-model
n X(cid:62)X translates into an ellipsoid constraint on
format  the decay condition on the eigenvalues of 1
the mean parameter sequence [16  17]. For this reason  Theorem 3 may be viewed as “regression
version” of (cid:96)2 error bounds for the sequence model under an ellipsoid constraint (cf. Pinsker’s The-
orem  [16  17]). Due to the fact that the latter problem has a very well developed literature  there
may be various “neighboring results” elsewhere. Nevertheless  we could not ﬁnd a direct reference
for our stated MSPE bound in the current setup. For the purposes of our work in this paper  the more
important point to take away from Theorem 3 is that it can be coupled with Theorem 2 for proving
consistency of RB.

3.4 Conﬁdence intervals for mean responses  conditionally on a Gaussian design
i ∈ Rp drawn
In this section  we consider the situation where the design matrix X has rows X(cid:62)
i.i.d. from a multivariate normal distribution N (0  Σ)  with X ⊥⊥ ε. (The covariance matrix Σ may
i ((cid:98)βρ − β)|X). As discussed in Section 1  this corresponds to the problem of
vary with n.) Conditionally on a realization of X  we analyze the RB approximation of the laws
Ψρ(F0; Xi) = L(X(cid:62)
setting conﬁdence intervals for the mean responses E[Yi] = X(cid:62)
i β. Assuming that the population
eigenvalues λi(Σ) obey a decay condition  we show below in Theorem 4 that RB succeeds with high
PX-probability. Moreover  this consistency statement holds for all of the laws Ψρ(F0; Xi) simul-
taneously. That is  among the n distinct laws Ψρ(F0; Xi)  i = 1  . . .   n  even the worst bootstrap
approximation is still consistent. We now state some population-level assumptions.

6

A4. The operator norm of Σ ∈ Rp×p satisﬁes (cid:107)Σ(cid:107)op (cid:46) 1.
Next  we impose a decay condition on the eigenvalues of Σ. This condition also ensures that Σ is
invertible for each ﬁxed p — even though the bottom eigenvalue may become arbitrarily small as p
becomes large. It is important to notice that we now use η for the decay exponent of the population
eigenvalues  whereas we used ν when describing the sample eigenvalues in the previous section.
A5. There is a number η > 0  and absolute constants k1  k2 > 0  such that for all i = 1  . . .   p 

k1i−η ≤ λi(Σ) ≤ k2i−η.

A6. There are absolute constants k3  k4 ∈ (0  1) such that for all n ≥ 3  we have the bounds

k3 ≤ p

n ≤ k4 and p ≤ n − 2.

The following lemma collects most of the effort needed in proving our ﬁnal result in Theorem 4.
Here it is also helpful to recall the notation ρ/n = n−γ and /n = n−θ from Assumption 2.
Lemma 2. Suppose that the matrix X ∈ Rn×p has rows X(cid:62)
i drawn i.i.d. from N (0  Σ)  and that
Assumptions 2–6 hold. Furthermore  assume that γ chosen so that 0 < γ < min{η  1}. Then  the
statements below are true.
(i) (bias inequality)
Fix any τ > 0. Then  there is an absolute constant κ0 > 0  such that for all large n  the following
event holds with PX-probability at least 1 − n−τ − ne−n/16 

ρ(X; Xi) ≤ κ0 · n−γ · (τ + 1) log(n + 2).
b2

max
1≤i≤n

(15)

(ii) (variance inequality)
There are absolute constants κ1  κ2 > 0 such that for all large n  the following event holds with
PX-probability at least 1 − 4n exp(−κ1n

γ

η ) 
vρ(X;Xi) ≤ κ2n1− γ

1

η .

max
1≤i≤n

(16)

(17)

(cid:40)

2 )  and that θ is chosen as θ = η

1+η when

(iii) (mspe inequalities)
Suppose that θ is chosen as θ = 2η/3 when η ∈ (0  1
η > 1

2 . Then  there are absolute constants κ3  κ4  κ5  κ6 > 0 such that for all large n 
with PX-probability at least 1 − exp(−κ3n2−4η/3) 

mspe((cid:98)β|X) ≤
the entire lemma may be explained in relatively simple terms. Viewing the quantities mspe((cid:98)β|X) 

κ4n− 2η
η+1 with PX-probability at least 1 − exp(−κ5n
− η
κ6n

Remarks. Note that the two rates in part (iii) coincide as η approaches 1/2. At a conceptual level 

if η ∈ (0  1
2 )
if η > 1
2 .

2
1+η ) 

3

ρ(X; Xi) and vρ(X; Xi) as functionals of a Gaussian matrix  the proof involves deriving concen-
b2
tration bounds for each of them. Indeed  this is plausible given that these quantities are smooth
functionals of X. However  the difﬁculty of the proof arises from the fact that they are also highly
non-linear functionals of X. We now combine Lemmas 1 and 2 with Theorems 1 and 2 to show that
all of the laws Ψρ(F0; Xi) can be simultaneously approximated via our two-stage RB method.
Theorem 4. Suppose that F0 has a ﬁnite fourth moment  Assumptions 2–6 hold  and γ is chosen
1+η < γ < min{η  1}. Also suppose that θ is chosen as θ = 2η/3 when η ∈ (0  1
2 )  and
so that
θ = η
2 . Then  there is a sequence of positive numbers δn with limn→∞ δn = 0  such
that the event

η+1 when η > 1

η

E(cid:104)

(cid:16) 1√

max
1≤i≤n

d2
2

Ψρ(F0; Xi) 

1√
vρ

vρ

Φρ((cid:98)F; Xi)

(cid:17)X

(cid:105) ≤ δn

has PX-probability tending to 1 as n → ∞.

Remark. Lemma 2 gives explicit bounds on the numbers δn  as well as the probabilities of the
corresponding events  but we have stated the result in this way for the sake of readability.

7

4 Simulations

In four different settings of n  p  and the decay parameter η  we compared the nominal 90% con-
ﬁdence intervals (CIs) of four methods: “oracle”  “ridge”  “normal”  and “OLS”  to be described
below. In each setting  we generated N1 := 100 random designs X with i.i.d. rows drawn from
N (0  Σ)  where λj(Σ) = j−η  j = 1  . . .   p  and the eigenvectors of Σ were drawn randomly by
setting them to be the Q factor in a QR decomposition of a standard p × p Gaussian matrix. Then 
for each realization of X  we generated N2 := 1000 realizations of Y according to the model (1) 
where β = 1/(cid:107)1(cid:107)2 ∈ Rp  and F0 is the centered t distribution on 5 degrees of freedom  rescaled to
have standard deviation σ = 0.1. For each X  and each corresponding Y   we considered the prob-
lem of setting a 90% CI for the mean response value X(cid:62)
i(cid:63) is the row with the highest
leverage score  i.e. i(cid:63) = argmax1≤i≤n Hii and H := X(X(cid:62)X)−1X(cid:62). This problem was shown in
B&F 1983 to be a case where the standard RB method based on least-squares fails when p/n (cid:16) 1.
Below  we refer to this method as “OLS”.
To describe the other three methods  “ridge” refers to the interval [X(cid:62)
Section 2  with B = 1000 and c(cid:62) = X(cid:62)

i(cid:63)(cid:98)βρ −(cid:98)q0.05] 
where (cid:98)qα is the α% quantile of the numbers z1  . . .   zB computed in the proposed algorithm in
we ﬁrst computed(cid:98)r as the value that optimized the MSPE error of a ridge estimator(cid:98)βr with respect
put  = 5(cid:98)r and ρ = 0.1(cid:98)r  as we found the prefactors 5 and 0.1 to work adequately across various
i(cid:63) ((cid:98)βρ−β)|X) ≈ N (0 (cid:98)τ 2)  where
2  ρ = 0.1(cid:98)r  and(cid:98)σ2 is the usual unbiased estimate of σ2 based
(cid:98)τ 2 =(cid:98)σ2(cid:107)X(cid:62)
i(cid:63)(cid:98)βρ − ˜q0.95  X(cid:62)
i(cid:63)(cid:98)βρ − ˜q0.05]  with
i ((cid:98)βρ − β) over all 1000 realizations of Y
ρ = 0.1(cid:98)r  and ˜qα being the empirical α% quantile of X(cid:62)
based on a given X. (This accounts for the randomness in ρ = 0.1(cid:98)r.)

settings. (Optimizing  with respect to MSPE is motivated by Theorems 1  2  and 3. Also  choosing ρ
to be somewhat smaller than  conforms with the constraints on θ and γ in Theorem 4.) The method
“normal” refers to the CI based on the normal approximation L(X(cid:62)

i(cid:63) (X(cid:62)X +ρIp×p)−1X(cid:62)(cid:107)2

on OLS residuals. The “oracle” method refers to the interval [X(cid:62)

i(cid:63) β  where X(cid:62)

i(cid:63)(cid:98)βρ −(cid:98)q0.95  X(cid:62)

to 5-fold cross validation; i.e. cross validation was performed for every distinct pair (X  Y ). We then

i(cid:63). To choose the parameters ρ and  for a given X and Y  

Within a given setting of the triplet (n  p  η)  we refer to the “coverage” of a method as the fraction of
the N1×N2 = 105 instances where the method’s CI contained the parameter X(cid:62)
i(cid:63) β. Also  we refer to
“width” as the average width of a method’s intervals over all of the 105 instances. The four settings of
(n  p  η) correspond to moderate/high dimension and moderate/fast decay of the eigenvalues λi(Σ).
Even in the moderate case of p/n = 0.45  the results show that the OLS intervals are too narrow
and have coverage noticeably less than 90%. As expected  this effect becomes more pronounced
when p/n = 0.95. The ridge and normal intervals perform reasonably well across settings  with
both performing much better than OLS. However  it should be emphasized that our study of RB
is motivated by the desire to gain insight into the behavior of the bootstrap in high dimensions
— rather than trying to outperform particular methods. In future work  we plan to investigate the
relative merits of the ridge and normal intervals in greater detail.

Table 1: Comparison of nominal 90% conﬁdence intervals

setting 1

n = 100  p = 45  η = 0.5

setting 2

n = 100  p = 95  η = 0.5

setting 3

n = 100  p = 45  η = 1

setting 4

n = 100  p = 95  η = 1

width

coverage

width

coverage

width

coverage

width

coverage

oracle
0.21
0.90
0.22
0.90
0.20
0.90
0.21
0.90

ridge
0.20
0.87
0.26
0.88
0.21
0.90
0.26
0.92

normal
0.23
0.91
0.26
0.88
0.22
0.91
0.23
0.87

OLS
0.16
0.81
0.06
0.42
0.16
0.81
0.06
0.42

Acknowledgements. MEL thanks Prof. Peter J. Bickel for many helpful discussions  and grate-
fully acknowledges the DOE CSGF under grant DE-FG02-97ER25308  as well as the NSF-GRFP.

8

References
[1] C.-H. Zhang and S. S. Zhang. Conﬁdence intervals for low dimensional parameters in high
dimensional linear models. Journal of the Royal Statistical Society: Series B  76(1):217–242 
2014.

[2] A. Javanmard and A. Montanari. Hypothesis testing in high-dimensional regression under the

Gaussian random design model: Asymptotic theory. arXiv preprint arXiv:1301.4240  2013.

[3] A. Javanmard and A. Montanari. Conﬁdence intervals and hypothesis testing for high-

linear models.

Bernoulli 

dimensional regression. arXiv preprint arXiv:1306.3171  2013.
Statistical signiﬁcance in high-dimensional

[4] P. B¨uhlmann.

19(4):1212–1242  2013.

[5] S. van de Geer  P. B¨uhlmann  and Y. Ritov. On asymptotically optimal conﬁdence regions and

tests for high-dimensional models. arXiv preprint arXiv:1303.0518  2013.

[6] J. D. Lee  D. L. Sun  Y. Sun  and J. E. Taylor. Exact inference after model selection via the

lasso. arXiv preprint arXiv:1311.6238  2013.

[7] A. Chatterjee and S. N. Lahiri. Rates of convergence of the adaptive lasso estimators to the
oracle distribution and higher order reﬁnements by the bootstrap. The Annals of Statistics 
41(3):1232–1259  2013.

[8] H. Liu and B. Yu. Asymptotic properties of lasso+mls and lasso+ridge in sparse high-

dimensional linear regression. Electronic Journal of Statistics  7:3124–3169  2013.

[9] V. Chernozhukov  D. Chetverikov  and K. Kato. Gaussian approximations and multiplier boot-
strap for maxima of sums of high-dimensional random vectors. The Annals of Statistics 
41(6):2786–2819  2013.

[10] E. L. Lehmann and J. P. Romano. Testing statistical hypotheses. Springer  2005.
[11] D. A. Freedman. Bootstrapping regression models. The Annals of Statistics  9(6):1218–1228 

1981.

[12] P. J. Bickel and D. A. Freedman. Bootstrapping regression models with many parameters. In

Festschrift for Erich L. Lehmann  pages 28–48. Wadsworth  1983.

[13] N. R. Draper and H. Smith. Applied regression analysis. Wiley-Interscience  1998.
[14] P. J. Bickel and D. A. Freedman. Some asymptotic theory for the bootstrap. The Annals of

Statistics  pages 1196–1217  1981.

[15] S. Bobkov and M. Ledoux. One-dimensional empirical measures  order statistics  and Kan-

torovich transport distances. preprint  2014.

[16] A. B. Tsybakov. Introduction to nonparametric estimation. Springer  2009.
[17] L. Wasserman. All of nonparametric statistics. Springer  2006.

9

,Miles Lopes