2019,Efficient Algorithms for Smooth Minimax Optimization,This paper studies first order methods for solving smooth minimax optimization problems $\min_x \max_y g(x y)$ where $g(\cdot \cdot)$ is smooth and $g(x \cdot)$ is concave for each $x$. In terms of $g(\cdot y)$  we consider two settings -- strongly convex and nonconvex -- and improve upon the best known rates in both. For strongly-convex $g(\cdot  y) \ \forall y$  we propose a new direct optimal algorithm combining Mirror-Prox and Nesterov's AGD  and show that it can find global optimum in $\widetilde{O}\left(1/k^2 \right)$ iterations  improving over current state-of-the-art rate of $O(1/k)$. We use this result along with an inexact proximal point method to provide $\widetilde{O}\left(1/k^{1/3} \right)$ rate for finding stationary points in the nonconvex setting where $g(\cdot  y)$ can be nonconvex. This improves over current best-known rate of $O(1/k^{1/5})$. Finally  we instantiate our result for finite nonconvex minimax problems  i.e.  $\min_x \max_{1\leq i\leq m} f_i(x)$  with nonconvex $f_i(\cdot)$  to obtain convergence rate of $O(m^{1/3}\sqrt{\log m}/k^{1/3})$.,EfﬁcientAlgorithmsforSmoothMinimaxOptimizationKiranKoshyThekumprampilUniversityofIllinoisatUrbana-Champaignthekump2@illinois.eduPrateekJainMicrosoftResearch Indiaprajain@microsoft.comPraneethNetrapalliMicrosoftResearch Indiapraneeth@microsoft.comSewoongOhUniversityofWashington Seattlesewoong@cs.washington.eduAbstractThispaperstudiesﬁrstordermethodsforsolvingsmoothminimaxoptimizationproblemsminxmaxyg(x y)whereg(· ·)issmoothandg(x ·)isconcaveforeachx.Intermsofg(· y) weconsidertwosettings–stronglyconvexandnonconvex–andimproveuponthebestknownratesinboth.Forstrongly-convexg(· y) ∀y weproposeanewdirectoptimalalgorithmcombiningMirror-ProxandNesterov’sAGD andshowthatitcanﬁndglobaloptimumineO(cid:0)1/k2(cid:1)iterations improvingovercurrentstate-of-the-artrateofO(1/k).WeusethisresultalongwithaninexactproximalpointmethodtoprovideeO(cid:0)1/k1/3(cid:1)rateforﬁndingstationarypointsinthenonconvexsettingwhereg(· y)canbenonconvex.Thisimprovesovercurrentbest-knownrateofO(1/k1/5).Finally weinstantiateourresultforﬁnitenonconvexminimaxproblems i.e. minxmax1≤i≤mfi(x) withnonconvexfi(·) toobtainconvergencerateofO(m1/3√logm/k1/3).1IntroductionInthispaperwestudysmoothminimaxproblemsoftheform:minx∈Xmaxy∈Yg(x y) g:X×Y→R gissmoothi.e. gradientLipschitz.(1)Theproblemhasapplicationsinseveraldomainssuchasmachinelearning[15 29] optimization[5] statistics[3] mathematics[23] andgametheory[31].Giventheimportanceoftheseproblems thereisanextensivebodyofworkthatstudiesvariousalgorithmsandtheirconvergenceproperties.Thevastmajorityofexistingresultsforthisproblemfocusontheconvex-concavesetting whereg(· y)isconvexforeveryyandg(x ·)isconcaveforeveryx.ThebestknownconvergencerateinthissettingisO(1/k)fortheprimal-dualgap achievedforexamplebyMirror-Prox[34].Thisrateisalsoknowntobeoptimalfortheclassofsmoothconvex-concaveproblems[41].Anaturalquestioniswhetherwecanachieveafasterconvergenceifwehavestrongconvexity(asopposedtojustconvexity)ofg(· y).Weanswerthisintheafﬁrmative byintroducinganalgorithmthatachievesaconvergencerateofeO(cid:0)1/k2(cid:1)forthegeneralsmooth strongly-convex–concaveminimaxproblem.ThealgorithmweproposeisanovelcombinationofMirror-ProxandNesterov’sacceleratedgradientdescent.ThismatchestheknownlowerboundofΩ(1/k2)from[41] closingthegapuptoapoly-logarithmicfactor.Therealsoexistsaconceptuallysimplesmoothingtechniquebasedindirectalgorithm whichpreﬁxesthetoleranceofε.However ourgoalistoﬁndadirectalgorithmwhichdoesnotpreﬁxthetolerance.OtherknownmethodsthatobtainarateofO(1/k2)inthiscontextareforveryspecialcases wherexandyareconnectedthroughabi-lineartermorg(x ·)islineariny[35 20 14 8 49 16 48].33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.SettingOptimalitynotionPreviousstate-of-the-artOurresultsSmoothingschemesLowerboundConvexPrimal-dualgapO(cid:0)k−1(cid:1)[34]--Ω(k−1)[41]StronglyconvexPrimal-dualgapO(cid:0)k−1(cid:1)[34]eO(cid:0)k−2(cid:1)eO(cid:0)k−2(cid:1)Ω(k−2)[41]NonconvexApprox.stat.pointO(cid:0)k−1/5(cid:1)[18]eO(cid:0)k−1/3(cid:1)eO(cid:0)k−1/3(cid:1)[26]-Table1:Comparisonofourresultswithpreviousstate-of-the-art.Weassumethatg(· ·)issmooth(i.e. hasLipschitzgradients)andg(x ·)isconcave∀x∈X.Convexity strongconvexityandnonconvexityintheﬁrstcolumnreferstog(· y)forﬁxedy.Smoothingschemesareindirectmethodsusingthesmoothingtechnique[36].Whilemosttheoreticalresultsfocusontheconvex-concavesetting severalrealworldproblemsfalloutsidethisclass.Aslightlylargerclass whichcapturesseveralmoreapplications istheclassofsmoothnonconvex–concaveminimaxproblems whereg(x ·)isconcaveforeveryxbutg(· y)canbenonconvex.Forexample ﬁniteminimaxproblems i.e. minxmaxmi=1fi(x)=minxmax0(cid:22)y(cid:22)1 Pmi=1yi=1Piyi·fi(x):=g(x y)belongtothisclass andsodosmoothnon-convexconstrainedoptimizationproblems[25].Inaddition severalmachinelearningproblemswithnon-decomposablelossfunctions[22]alsobelongtothisclass.Inthisgeneralnonconvexconcavesettinghowever wecannothopetoﬁndglobaloptimumefﬁcientlyaseventhespecialcaseofnonconvexoptimizationisNP-hard.Similartononconvexoptimization wemighthopetoﬁndanapproximatestationarypoint[37].Oursecondcontributionisanewalgorithmandafasterrateforthegeneralsmoothnonconvex–concaveminimaxproblem.Ouralgorithmisaninexactproximalpointmethodforthenonconvexfunctionf(x):=maxy∈Yg(x y).Thekeyinsightisthattheproximalpointproblemineachiterationresultsinastrongly-convexconcaveminimaxproblem forwhichweuseourimprovedalgorithmtoobtaintheoverallcomputation/iterationcomplexityofeO(cid:0)1/k1/3(cid:1)thusimprovingoverthepreviousbestknownrateofO(1/k1/5)[18]1.Morerecently independenttoourwork asmoothingbasedalgorithmhasalsobeenproposedtoachievethesameO(cid:0)k−1/3(cid:1)rate[26].Finally wespecializeourresulttoﬁniteminimaxproblems i.e. minxmax1≤i≤mfi(x)wherefi(x)canbenonconvexfunctionbuteachfiisasmoothfunction;nonconvexconstrainedopti-mizationproblemscanbereducedtosuchﬁniteminimaxproblems.Forthese weobtainarateofeO(cid:0)m1/3√logm/k1/3(cid:1)totalgradientcomputationswhichimprovesuponthestate-of-the-artrateO(m1/4/k1/4)[11]inthissettingaswell.Summaryofcontributions:SeealsoTable1.1.OptimaleO(cid:0)1/k2(cid:1)convergencerateforsmooth strongly-convex–concaveproblems improvinguponthepreviousbestknownrateofO(1/k)foradirectalgorithmand 2.eO(cid:0)1/k1/3(cid:1)convergencerateforsmooth nonconvex–concaveproblems improvinguponthepreviousbestknownrateofO(cid:0)1/k1/5(cid:1).Relatedworks:Forstrongly-convex-concaveminimaxproblemswithspecialstructures severalalgorithmshavebeenproposed.Inanincreasingorderofgenerality [14 49 50]studyoptimizingastronglyconvexfunctionwithlinearconstraints whichcanbeposedasaspecialcaseofminimaxoptimization [35 8]studyaminimaxproblemwherexandyareconnectedonlythroughabi-lineartermyTAx and[16 20]studyacasewhereg(x ·)islineariny.Inallthesecases itisshownthatO(1/k2)convergencerateisachievableifg(· y)isstrongly-convex∀y.Recently [12]showedlinearconvergenceofgradientdescentascentforstrongly-convex–concaveproblemwithbilinearcouplingwhenAhasfullrowrank.However ithasremainedanopenquestionifthefastrateofO(1/k2)canbeachievedforgeneralstrongly-convex-concaveminimaxproblems.See[32 9 7 17 51 1]1While[18]givesarateofO(cid:0)1/k1/4(cid:1)withanapproximatemaximizationoracleformaxy∈Yg(x y) takingintoaccountthecostofimplementingsuchamaximizationoraclegivesarateofO(cid:0)1/k1/5(cid:1).2fordetailedsurveysontheresultsfortheconvex-concaveminimaxproblems.Forexamplesandapplicationofsaddlepointproblemsrefer[36 19 20 7 43].Fornonconvex-concaveminimaxproblems [42]considersbothdeterministicandstochasticsettings andproposesinexactproximalpointmethodsforsolvingsmoothnonconvex–concaveproblems.Inthedeterministicsetting theirresultguaranteesanerrorofO(1/k1/6).Wenotethattherehavealsobeenothernotionsofstationarityproposedinliteraturefornonconvex-concaveminimaxproblems[28 40].Thesenotionshoweverareweakerthantheoneconsideredinthispaper inthesensethat ournotionofstationarityimpliestheseothernotions(withoutlossinparameters).Foronesuchweakernotion [40]proposesanalgorithmwithaconvergencerateofO(cid:0)k−1/3.5(cid:1).Sincethenotiontheyconsiderisweaker itdoesnotimplythesameconvergencerateinoursetting.Wewouldalsoliketohighlighttheworks[6 13 33 46 34 47 10]designingefﬁcientalgorithmsforsolvingmonotonevariationalinequalitieswhichgeneralizestheconvex-concaveminimaxproblems.Notations:Risthereallineandforanynaturalnumberp Rpistherealvectorspaceofdimensionp.k·kisanormonsomemetricspacewhichwouldbeevidentfromthecontext.ForaconvexsetX⊆Rpandx∈Rp PX(x)=argminx0∈Xkx−x0kistheprojectionofxontoX.Foradifferentiablefunctiong(x y) ∇xg(x y)isitsgradientwithrespecttoxat(x y).Weusethestandardbig-Onotations.ForfunctionsT S:R→Rsuchthat0<liminfx→∞T(x) 0<liminfx→∞S(x) (a)T(x)=O(S(x))meanslimsupx→∞T(x)/S(x)<∞;(b)T(x)=Θ(S(x))meansT(x)=O(S(x))andS(x)=O(T(x));and(c)T(x)=eO(S(x))meansthatT(x)=O(S(x)R(x))forsomepoly-logarithmicfunctionR:R→R.Paperorganization:InSection2 wepresentpreliminariesandallrelevantbackground.InSection3 wepresentourresultsforstrongly-convex–concavesettingandinsection4 resultsfornonconvex–concavesetting.InSection5 wepresentempiricalevaluationofouralgorithmfornonconvex-concavesettingandcompareittoastate-of-the-artalgorithm.WeconcludeinSection6.Severaltechnicaldetailsarepresentedintheappendix.2PreliminariesandbackgroundmaterialInthissection wewillpresentsomepreliminaries describingthesetupandreviewingsomeback-groundmaterialthatwillbeusefulinthesequel.2.1MinimaxproblemsWeareinterestedintheminimaxproblemsoftheform(1)whereg(x y)isasmoothfunction.Deﬁnition1.Afunctiong(x y)issaidtobeL-smoothif:max{k∇xg(x y)−∇xg(x0 y0)k k∇yg(x y)−∇yg(x0 y0)k}≤L(kx−x0k+ky−y0k).Throughout weassumethatg(x .)isconcaveforeveryx∈X.Forg(· y)behaviorintermsofx therearebroadlytwosettings:2.1.1Convex-concavesettingInthissetting g(· y)isconvex∀y∈Y.Givenanygand∀(bx by) thefollowingholdstrivially:minx∈Xg(x by)≤g(bx by)≤maxy∈Yg(bx y) whichthenimpliesthatmaxy∈Yminx∈Xg(x y)≤minx∈Xmaxy∈Yg(x y).Thecelebratedmin-imaxtheoremfortheconvex-concavesetting[44]saysthatifYisacompactsetthentheaboveinequalityisinfactanequality i.e. maxy∈Yminx∈Xg(x y)=minx∈Xmaxy∈Yg(x y).Further-more anypoint(x∗ y∗)isanoptimalsolutionto(1)ifandonlyif:minx∈Xg(x y∗)=g(x∗ y∗)=maxy∈Yg(x∗ y).(2)Hence ourgoalistoﬁndε-primal-dualpair(bx by)withsmallprimal-dualgap:maxy∈Yg(bx y)−minx∈Xg(x by).Deﬁnition2.Foraconvex-concavefunctiong:X×Y→R (ˆx ˆy)isanε-primal-dual-pairofgiftheprimal-dualgapislessthanε:maxy∈Yg(bx y)−minx∈Xg(x by)≤ε.32.1.2Nonconvex-concavesettingInthissettingthefunctiong(· y)neednotbeconvex.Onecannothopetosolvesuchproblemsingeneral sincethespecialcaseofnonconvexoptimizationisalreadyNP-hard[39].Furthermore theminimaxtheoremnolongerholds i.e. maxy∈Yminx∈Xg(x y)canbestrictlysmallerthanminx∈Xmaxy∈Yg(x y) andthereforetheorderofminandmaxmightbeimportantforagivenapplicationi.e. wemightbeinterestedonlyinminimaxbutnotmaximin(orviceversa).So theprimal-dualgapmaynotbeameaningfulquantitytomeasureconvergence.Inthispaperwewillfocusontheminimaxproblem:minx∈Xmaxy∈Yg(x y).Oneapproach inspiredbynonconvexoptimization tomeasureconvergenceistoconsiderthefunctionf(x)=maxy∈Yg(x y)andconsidertheconvergenceratetoapproximateﬁrstorderstationarypoints(i.e. ∇f(x)issmall)[42 18].Butasf(x)couldbenon-smooth ∇f(x)mightnotevenbedeﬁned.Itturnsoutthatwheneverg(x y)issmooth f(x)isweaklyconvex(Deﬁnition4)forwhichﬁrstorderstationaritynotionsarewell-studiedandarediscussedbelow.Approximateﬁrst-orderstationarypointforweaklyconvexfunctions:Weﬁrstneedtogeneral-izethenotionofgradientforanon-smoothfunction.Deﬁnition3.TheFréchetsub-differentialofafunctionf(·)atxisdeﬁnedastheset ∂f(x)={u|liminfx0→xf(x0)−f(x)−hu x0−xi/kx0−xk≥0}.Inordertodeﬁneapproximatestationarypoints wealsoneedthenotionofweaklyconvexfunctionandMoreauenvelope.Deﬁnition4.Afunctionf:X→R∪{∞}isL-weaklyconvexif f(x)+hux x0−xi−L2kx0−xk2≤f(x0) (3)forallFréchetsubgradientsux∈∂f(x) forallx x0∈X.Deﬁnition5.Foraproperlowersemi-continuous(l.s.c.)functionf:X→R∪{∞}andλ>0(X⊆Rp) theMoreauenvelopefunctionisgivenbyfλ(x)=minx0∈Xf(x0)+12λkx−x0k2.(4)Lemma4(inAppendixB.2)providessomeusefulpropertiesoftheMoreauenvelopeforweaklyconvexfunctions.Now ﬁrstorderstationarypointofanon-smoothnonconvexfunctioniswell-deﬁned i.e. x∗isaﬁrstorderstationarypoint(FOSP)ofafunctionf(x)if 0∈∂f(x∗)(seeDeﬁnition3).However unlikesmoothfunctions itisnontrivialtodeﬁneanapproximateFOSP.Forexample ifwedeﬁneanε-FOSPasthepointxwithminu∈∂f(x)kuk≤ε theremayneverexistsuchapointforsufﬁcientlysmallε unlessxisexactlyaFOSP.Incontrast byusingabovepropertiesoftheMoreauenvelopeofaweaklyconvexfunction it’sapproximateFOSPcanbedeﬁnedas[11]:Deﬁnition6.GivenanL-weaklyconvexfunctionf wesaythatx∗isanε-ﬁrstorderstationarypoint(ε-FOSP)if k∇f12L(x∗)k≤ε wheref12ListheMoreauenvelopewithparameter1/2L.UsingLemma4 wecanshowthatforanyε-FOSPx∗ thereexistsˆxsuchthatkˆx−x∗k≤ε/2Landminu∈∂f(ˆx)kuk≤ε.Inotherwords anε-FOSPisO(ε)closetoapointˆxwhichhasasubgradientsmallerthanε.WenotethatothernotionsofFOSPhavealsobeenproposedrecentlysuchasin[40].However itcanbeshownthatanε-FOSPaccordingtotheabovedeﬁnitionisalsoan-FOSPwith[40]’sdeﬁnitionaswell butthereverseisnotnecessarilytrue.2.2Mirror-ProxMirror-Prox[34]isapopularalgorithmproposedforsolvingconvex-concaveminimaxproblems(1).ItachievesaconvergencerateofO(1/k)fortheprimaldualgap.TheoriginalMirror-Proxpaper[34]motivatesthealgorithmthroughaconceptualMirror-Prox(CMP)method whichbringsoutthemainideabehinditsconvergencerateofO(1/k).CMPforEuclideannorm(afterignoringprojectionstoXandY)doesthefollowingupdate:(xk+1 yk+1)=(xk yk)+1β(−∇xg(xk+1 yk+1) ∇yg(xk+1 yk+1)).(5)4ThemaindifferencebetweenCMPandstandardgradientdescentascent(GDA)isthatinthekthstep whileGDAusesgradientsat(xk yk) CMPusesgradientsat(xk+1 yk+1).Thekeyobservationof[34]isthatifg(· ·)issmooth itcanbeimplementedefﬁciently.CMPisanalyzedasfollows:ImplementabilityofCMP:Let(x(0)k y(0)k)=(xk yk).Forβ<1L theiteration(x(i+1)k y(i+1)k)=(xk yk)+1β(cid:16)−∇xg(cid:16)x(i)k y(i)k(cid:17) ∇yg(cid:16)x(i)k y(i)k(cid:17)(cid:17).(6)canbeshowntobe1√2-contraction(wheng(· ·)issmooth)andthatitsﬁxedpointis(xk+1 yk+1).So inlog1iterationsof(6) wecanobtainanaccurateversionoftheupdaterequiredbyCMP.Infact [34]showedthatjusttwoiterationsof(6)sufﬁce[30].ConvergencerateofCMP:UsingCMPupdatewithsimplemanipulationsleadstothefollowing:g(xk+1 y)−g(x yk+1)≤β2(cid:0)kx−xkk2−kx−xk+1k2+ky−ykk2−ky−yk+1k2(cid:1) ∀x∈X y∈Y.O(1/k)convergenceratefollowseasilyusingtheaboveresult.Finally ourmethodandanalysisalsorequiresNesterov’sacceleratedgradientdescentmethod(seeAlgorithm3inAppendixA)andit’sper-stepanalysisby[2](Lemma2inAppendixA).3Strongly-convexconcavesaddlepointproblemWeﬁrststudytheminimaxproblemoftheform:minx∈X[f(x)=maxy∈Yg(x y)] (P1)whereg(x ·)isconcave g(· y)isσ-strongly-convex g(· ·)isL-smooth i.e. 0<σ≤L.X=RpandY⊂Rqisaconvexcompactsub-setofRqandletthefunctionftakeaminimumvaluef∗(>−∞).LetDY=maxy y0∈Yky−y0kbethediameterofY.Ourobjectivehereistoﬁndan-primal-dualpair(bx by)(seeDeﬁnition2).Nowthefactthatf(ˆx)−f∗≤maxy∈Yg(bx y)−minx∈Xg(x by)impliesthatif(ˆx ˆy)isanε-primal-dual-pair thenˆxisalsoanε-approximateminimaoff.Furthermore bySion’sminimaxtheorem[24] strong-convexity–concavityofg(· ·)ensuresthat:minx[f(x):=maxyg(x y)]=maxy[h(y):=minxg(x y)].Hence oneapproachtoefﬁcientlysolvingtheproblemisbyoptimizingthedualproblemmaxyh(y).ByLemma6(inAppendixB.6) h(y)isan(L+L2σ)-smoothfunction.SowecanuseAGDtoensurethath(yk)−h(y∗)=O(1/k2).Now eachstepofAGDrequirescomputingargminxg(x yk)whichcanbedoneefﬁciently(i.e. logarithmicnumberofsteps)asg(· yk)isstrongly-convexandsmooth.So theoverallﬁrst-orderoraclecomplexityish(yk)−h(y∗)=eO(cid:0)1/k2(cid:1).Sodoesthissimpleapproachgiveusourdesiredresult?Unfortunatelythatisnotthecase astheaboveboundonthedualfunctionhdoesnottranslatetothesameerrorrateforprimalfunctionf i.e. thesolutionneednotbeeO(cid:0)1/k2(cid:1)-primal-dualpair.E.g. considerminx∈Rmaxy∈[−1 1][g(x y)=xy+x2/2] whereminxmaxyg(x y)=0 f(x)=x2/2+|x|andh(y)=−y2/2.Ifh(yk)=Θ(k−2) thenxk∈argminxg(x yk)=Θ(1/k)andsof(xk)isΘ(k−1).Thisisduetothenon-smoothnessofargmaxy∈Yg(x y)w.r.t.x.InsteadofusingAGD weintroduceanewmethodtosolvethedualproblemthatwerefertoasDIAG whichstandsforDualImplicitAcceleratedGradient.DIAGcombinesideasfromAGD[38]andNemirovski’soriginalderivationoftheMirror-Proxalgorithm[34] andcanensureafastconvergencerateof˜O(k−2)fortheprimal-dualgap.Wenotethattherealsoexistsaconceptuallysimplersmoothingtechniquebasedindirectalgorithm whichpreﬁxesthetoleranceofε(AppendixD).However ourgoalistoﬁndadirectalgorithmwhichdoesnoterequirepreﬁxingthetoleranceatε.Forbetterexposition weﬁrstpresentaconceptualversionofDIAG(C-DIAG) whichisnotimplementableexactly butbringsoutthemainnewideasinouralgorithm.Wethenpresentadetailederroranalysisfortheinexactversionofthisalgorithm whichisimplementable.3.1Conceptualversion:C-DIAGConsiderthefollowingupdateswhichisamodiﬁedversionofAGD(seeAlgorithm3inAppendixA):5(a)wk=(1−τk)yk+τkzk(b)Choosexk+1 yk+1ensuring:xk+1∈argminxg(x yk+1) andyk+1=PY(wk+1β∇yg(xk+1 wk))(c)zk+1=PY(zk+ηk∇yg(xk+1 wk))CompletepseudocodeforC-DIAGalgorithmispresentedinAlgorithm4inAppendixB.4.ThemainideaofthealgorithmisinStep(b)above(i.e. Step4ofAlgorithm4inAppendixB.4) wherewesimultaneouslyﬁndxk+1andyk+1satisfyingthefollowingrequirements:•xk+1istheminimizerofg(· yk+1) and•yk+1correspondstoanAGDstep(seeAlgorithm3inAppendixA)forg(xk+1 ·)Implementability:Theﬁrstquestioniswhetheritiseasyenoughtoimplementsuchastep?Itturnsoutthatitisindeedpossibletoquicklyﬁndpointsxk+1andyk+1thatapproximatelysatisfytheaboverequirements.Thereasonisthat:•Sinceg(· y)issmoothandstronglyconvexforeveryy∈Y wecanﬁnd-approximateminimizerforagivenyinO(cid:0)log1(cid:1)iterations.•Letx∗(y):=argminx∈Xg(x y).Theiterationyi+1=PY(cid:16)wk+1β∇yg(x∗(yi) wk)(cid:17)isa1/2-contractionwithauniqueﬁxedpointsatisfyingtheupdatesteprequirements(i.e. Step4ofAlgorithm4inAppendixB.4).SeeLemma5inAppendixB.5foraproof.ThismeansthatonlyO(cid:0)log1(cid:1)iterationsagainsufﬁcetoﬁndanupdatethatapproximatelysatisﬁestherequirements.Convergencerate:Sinceyk+1andzk+1correspondtoanAGDupdateforg(xk+1 ·) wecanusethepotentialfunctiondecreaseargumentforAGD(Lemma2inAppendixA)toconcludethat∀y∈Y (k+1)(k+2)(g(xk+1 y)−g(xk+1 yk+1))+2β·ky−zk+1k2≤k(k+1)(g(xk+1 y)−g(xk+1 yk))+2β·ky−zkk2≤k(k+1)(g(xk+1 y)−g(xk y))+k(k+1)(g(xk y)−g(xk yk))+2β·ky−zkk2 wherethelaststepfollowsfromthefactthatxk=argminxg(x yk)andsog(xk yk)≤g(xk+1 yk).Notingthatwecanfurtherrecursivelyboundk(k+1)(g(xk y)−g(xk yk))+2β·ky−zkk2asabove weobtain(k+1)(k+2)(g(xk+1 y)−g(xk+1 yk+1))+2β·ky−zk+1k2≤k(k+1)g(xk+1 y)−kXi=1(2i)·g(xi y)+2β·ky−z0k2⇒k+1Xi=1(2i)·g(xi y)−(k+1)(k+2)g(xk+1 yk+1)≤2β·ky−z0k2.Sinceg(xk+1 yk+1)≤g(x yk+1)foreveryx∈X wehavek+1Xi=1(2i)·g(xi y)−(k+1)(k+2)g(x yk+1)≤2β·ky−z0k2⇒g(¯xk+1 y)−g(x yk+1)≤2β·ky−z0k2(k+1)(k+2) where¯xk+1:=1(k+1)(k+2)Pk+1i=1(2i)·xi.Sincexandyarearbitraryabove thisgivesaO(cid:0)1/k2(cid:1)convergenceratefortheprimaldualgap.3.2ErroranalysisThemainissuewithAlgorithm4isthattheupdatestepisnotexactlyimplementable.However aswenotedintheprevioussection wecanquicklyﬁndupdatesthatalmostsatisfytherequirements.Algorithm1presentsthisinexactversion.ThefollowingtheoremstatesourformalresultandadetailedproofisprovidedinAppendixB.5.6Algorithm1:DualImplicitAcceleratedGradient(DIAG)forstrongly-convex–concaveprogrammingInput:g L σ DY x0 y0 K {ε(k)step}Kk=1Output:¯xK yK1Setβ←2L2σ z0←y02fork=0 1 ... K−1do3τk←2(k+2) ηk←(k+1)2β wk←(1−τk)yk+τkzk4xk+1 yk+1←Imp-STEP(g L σ x0 wk β ε(k+1)step) ensuring:g(xk+1 yk+1)≤minxg(x yk+1)+ε(k+1)step yk+1=PY(cid:18)wk+1β∇yg(xk+1 wk)(cid:19)5zk+1←PY(zk+ηk∇yg(xk+1 wk)) ¯xk+1←2(k+1)(k+2)Pk+1i=1i·xi6return¯xK yK7Imp-STEP(g L σ x0 w β εstep):8Setεmp←2σ5Lq2εstepL R←dlog22DYεmpe εagd←σβ2ε2mp32L2 y0←w9forr=0 1 ... Rdo10Startingatx0useAGD[38]forstrongly-convexg(· yr) tocomputeˆxrsuchthat:g(ˆxr yr)≤minxg(x yr)+εagd (7)11yr+1←PY(cid:0)w+1β∇yg(ˆxr w)(cid:1)12returnˆxR yR+1Theorem1(ConvergencerateofDIAG).Letg:X×Y→RbeaL-smooth σ-strongly-convex–concavefunctiononX=Rpandaconvexcompactsub-setY⊂Rq(withdiameterDY).Then afterKiterations DIAG(Algorithm1)withatolerancescheduleof{ε(k)step}Kk=1foritsImp-STEPsub-routine ﬁnds(¯xK yK)s.t.:max˜y∈Yg(¯xK ˜y)−min˜x∈Xg(˜x yK)≤4L2σD2Y+PKk=1k(k+1)ε(k)stepK(K+1).(8)Inparticular settingε(k)step=L2D2Yσk3(k+1)wehave:max˜y∈Yg(¯xK ˜y)−min˜x∈Xg(˜x yK)≤6L2σD2YK(K+1).Furthermore forthissettingthetotalﬁrstorderoraclecomplexityisgivenby:O(qLσKlog2(K)).Remark1:Theorem1showsthatDIAGneeds˜O((DYL/√σε)·(pL/σ))gradientqueriesforﬁndingaε-primal-dual-pair whilecurrentbest-knownrateisO(1/ε)achievedbyMirror-Prox.ThisdependenceinεandDYisoptimal asitisshownin[41 Theorem10]thatΩ(DY(L−σ)/√σε)gradientqueriesarenecessarytoachieveεerrorintheprimal-dualgap.Remark2:UnlikestandardAGDforh(y) whichonlyupdatesykintheouter-loop DIAG’souter-stepupdatesbothxkandykthusallowingustobettertracktheprimal-dualgap.However DIAG’sdependenceontheconditionnumberL/σseemssub-optimalandcanperhapsbeimprovedifwedonotcomputeImp-STEPnearlyoptimallyallowingforinexactupdates;weleavefurtherinvestigationintoimproveddependenceontheconditionnumberforfuturework.4NonconvexconcavesaddlepointproblemWestudythenonconvexconcaveminimaxproblem(1)whereg(x ·)isconcave g(· y)isnonconvex andg(· ·)isL-smooth X=Rp(suchthatProjX(x)=x)andYisaconvexcompactsub-setofRq.AsmentionedinSection2 wemeasuretheconvergencetoanapproximateFOSPofthisproblem(seeDeﬁnition6)butitrequiresweak-convexityoff(x):=maxy∈Yg(x y).Thefollowinglemmaguaranteesweakconvexityoffgivensmoothnessofg.7Lemma1.Letg(· y)becontinuousandYbecompact.Thenf(x)=maxy∈Yg(x y)isL-weaklyconvex ifgisL-weaklyconvexinx(Deﬁnition1) orifgisL-smoothinx.SeeAppendixB.3fortheproof.Theargumentsof[18]easilyextendtoshowthatapplyingsubgradientmethodonf(x) [11]givesaconvergencerateofO(cid:0)1/k1/5(cid:1).Instead weexploitthesmoothminimaxformoff(·)todesignafasterconvergingscheme.Themainintuitioncomesfromtheproximalviewpointthatgradientdescentcanbeviewedasiterativelyformingandoptimizinglocalquadraticupperbounds.Asfisweaklyconvex addingenoughquadraticregularizationshouldensurethattheresultingsequenceofproblemsareallstrongly-convex–concave.WethenexploitDIAGtoefﬁcientlysolvesuchlocalquadraticproblemstoobtainimprovedconvergencerates.Concretely letbf(x;xk)=maxyg(x y)+Lkx−xkk2.(9)ByL-weak-convexityoff bf(x;xk)isstrongly-convex–concave(Lemma3)thatcanbesolvedusingDIAGuptocertainaccuracytoobtainxk+1.WerefertothisalgorithmasProx-DIAGandprovideapseudo-codeforthesameinAlgorithm2.ThefollowingtheoremgivesconvergenceguaranteesforAlgorithm2:ProximalDualImplicitAcceleratedGradient(Prox-DIAG)fornonconvexconcaveprogrammingInput:g L ε x0 y0Output:xk1Set˜ε←ε264L2fork=0 1 ... Kdo3UsingDIAGforstronglyconvexconcaveminimaxproblem minxmaxy∈Y[bg(x y;xk)=g(x y)+Lkx−xkk2](10)ﬁndxk+1suchthat maxy∈Yg(xk+1 y)+Lkxk+1−xkk2≤minxmaxy∈Yg(x y)+Lkx−xkk2+˜ε4(11)ifmaxy∈Yg(xk y)−3˜ε4≤maxy∈Yg(xk+1 y)+Lkxk+1−xkk2then4returnxkProx-DIAG.Theorem2(ConvergencerateofProx-DIAG).Letg(x y)beL-smooth g(x ·)beconcave XbeRp YbeaconvexcompactsubsetofRq andtheminimumvalueoffunctionf(x)=maxy∈Yg(x y)beboundedbelow i.e.f(x)≥f∗>−∞.ThenProx-DIAG(Algorithm2)after K=(cid:24)44L(f(x0)−f∗)3ε2(cid:25)stepsoutputsanε-FOSP.Thetotalﬁrst-orderoraclecomplexitytooutputε-FOSPis:O(cid:0)L2DY(f(x0)−f∗)ε3log2(cid:0)1ε(cid:1)(cid:1).AproofisprovidedinAppendixB.7.NotethatProx-DIAGsolvesthequadraticapproximationproblemtohigheraccuracyofO(2)whichthenhelpsboundingthegradientoftheMoreauenvelope.Alsoduetothemodularstructureoftheargument afasterinnerloopforspecialsettings e.g. wheng(x y)isaﬁnite-sum canensuremoreefﬁcientalgorithm.Whileouralgorithmisabletosigniﬁcantlyimproveuponexistingstate-of-the-artrateofO(1/ε5)ingeneralnonconvex-concavesetting[18] itisuncleariftheratecanbefurtherimproved.Infact preciselower-boundsforthissettingaremostlyunexploredandweleavefurtherinvestigationintolower-boundsasatopicoffutureresearch.WealsospecializetheProx-DIAGalgorithm asProx-FDIAG(Algorithm5inAppendixC) forthecaseofminimizingaweaklyconvexf(x) withthespecialstructureofﬁnitemax-typefunction:minxhf(x)=max1≤i≤mfi(x)i (P3)8wherefi’scouldbenonconvexbutareL-smooth G-Lipschitzandboundedfrombelow.Forthiscase weimprovethecurrentknownbestrateofO(cid:0)m/ε4(cid:1)andobtainafasterrateofO(mlog3/2m/ε3)usingtheProx-FDIAGalgorithm.PleaserefertoAppendixCformoredetails.5ExperimentsWeempiricallyverifytheperformanceofProx-FDIAG(Algorithm5inAppendixC)onasyn-theticﬁnitemax-typenonconvexminimizationproblem(P3).Weconsiderthefollowingproblem.minx∈R2(cid:2)f(x)=max1≤i≤m=9fi(x)(cid:3)wherefi(x)=q(−1 (X(1)i X(2)i) Ci)(x)forall1≤i≤8 whereq(a b c)(x)=akx−bk22+c X(1)i X(2)i andciarerandomlygenerated.ThuseachfiissmoothwithparameterL=1 whichimpliesthatfisL-weaklyconvex.Weimplementthreealgorithms:Prox-FDIAG(Algorithm5 redcircles) AdaptiveProx-FDIAG(Algorithm6 blackdots) andsubgradientmethod[11](bluetriangles).AdaptiveProx-FDIAGisapracticallyfastervariantofProx-FDIAG withthesameﬁrst-orderoraclecomplexityguarantee(uptoanO(log(1/ε))factor).InFigure1 weplotthenormofgradientofMoreauenvelopek∇f12L(xk)k2againstthenumber100101102103104105106107108106104102100102Sub-gradient methodProx-FDIAG (ours)Adaptive Prox-FDIAG (ours)NormofthegradientofMoreauenvelopek∇f12L(xk)k2numberofgradientoracleaccesseskFigure1:Forsmalltargetaccuracyεregime AdaptiveProx-FDIAG(ours)hasthefastestconvergenceratefollowedbyProx-FDIAG(ours)andsubgradientmethod.ofiterationskinlog-logscale.Weseethat Prox-FDIAGandAdaptiveProx-FDIAGhaveafasterconvergenceratethansubgradientmethod andAdaptiveProx-FDIAGisalmostalwaysfasterthanProx-FDIAG.WeprovidemoredetailsaboutthealgorithmsandtheexperimentsinAppendixE.6ConclusionInthispaper westudysmoothminimaxproblems wherethemaximizationisconcavebuttheminimizationiseitherstronglyconvexornonconvex.Inbothofthesesettings wepresentnewalgorithmsimprovingstate-of-the-art.Thekeyideasarei)anovelwaytocombineMirror-ProxandNesterov’sAGDforstronglyconvexcasethatcantightlyboundprimal-dualgapandii)aninexactproxmethodwithgoodconvergenceratetostationarypointsforthenonconvexcase.WhileweonlypresentourresultsfortheEuclideansetting generalizingittonon-EuclideansettingswiththeframeworkofBregmandivergencesshouldbestraightforward.Finally weshowcasetheempiricalsuperiorityofournonconvexalgorithmoverstate-of-the-artsubgradientmethodforacaseofﬁnitemax-typenonconvexminimizationproblems.Someofthemoreinterestingquestionswouldbetounderstandtheoptimalityoftheratesthatweobtainanddependenceonthestrongconvexityparameter.Furtherextensionsoftheseresultstothestochasticsettingwouldalsobequiteinteresting.AcknowledgementThisworkispartiallysupportedbyNSFawardsCCF-1927712andRI-1929955.9References[1]MohammadAlkousa DarinaDvinskikh FedorStonyakin andAlexanderGasnikov.Acceler-atedmethodsforcompositenon-bilinearsaddlepointproblem.arXivpreprintarXiv:1906.03620 2019.[2]NikhilBansalandAnupamGupta.Potential-functionproofsforﬁrst-ordermethods.arXivpreprintarXiv:1712.04581 2017.[3]JamesOBerger.StatisticaldecisiontheoryandBayesiananalysis.SpringerScience&BusinessMedia 2013.[4]DimitriPBertsekas.Convexoptimizationtheory.AthenaScientiﬁcBelmont 2009.[5]DimitriPBertsekas.ConstrainedoptimizationandLagrangemultipliermethods.Academicpress 2014.[6]RonaldEBruckJr.Ontheweakconvergenceofanergodiciterationforthesolutionofvariationalinequalitiesformonotoneoperatorsinhilbertspace.JournalofMathematicalAnalysisandApplications 61(1):159–164 1977.[7]AntoninChambolleandThomasPock.Anintroductiontocontinuousoptimizationforimaging.ActaNumerica 25:161–319 2016.[8]AntoninChambolleandThomasPock.Ontheergodicconvergenceratesofaﬁrst-orderprimal–dualalgorithm.MathematicalProgramming 159(1-2):253–287 2016.[9]YunmeiChen GuanghuiLan andYuyuanOuyang.Optimalprimal-dualmethodsforaclassofsaddlepointproblems.SIAMJournalonOptimization 24(4):1779–1814 2014.[10]YunmeiChen GuanghuiLan andYuyuanOuyang.Acceleratedschemesforaclassofvariationalinequalities.MathematicalProgramming 165(1):113–149 2017.[11]DamekDavisandDmitriyDrusvyatskiy.StochasticsubgradientmethodconvergesattherateO(k−1/4)onweaklyconvexfunctions.arXivpreprintarXiv:1802.02988 2018.[12]SimonSDuandWeiHu.Linearconvergenceoftheprimal-dualgradientmethodforconvex-concavesaddlepointproblemswithoutstrongconvexity.InThe22ndInternationalConferenceonArtiﬁcialIntelligenceandStatistics pages196–205 2019.[13]JonathanEcksteinandDimitriPBertsekas.Onthedouglas—rachfordsplittingmethodandtheproximalpointalgorithmformaximalmonotoneoperators.MathematicalProgramming 55(1-3):293–318 1992.[14]TomGoldstein BrendanO’Donoghue SimonSetzer andRichardBaraniuk.Fastalternatingdirectionoptimizationmethods.SIAMJournalonImagingSciences 7(3):1588–1623 2014.[15]IanGoodfellow JeanPouget-Abadie MehdiMirza BingXu DavidWarde-Farley SherjilOzair AaronCourville andYoshuaBengio.Generativeadversarialnets.InAdvancesinneuralinformationprocessingsystems pages2672–2680 2014.[16]ErfanYazdandoostHamedaniandNecdetSerhatAybat.Aprimal-dualalgorithmforgeneralconvex-concavesaddlepointproblems.arXivpreprintarXiv:1803.01401 2018.[17]YunlongHeandRenatoDCMonteiro.Anacceleratedhpe-typealgorithmforaclassofcompositeconvex-concavesaddle-pointproblems.SIAMJournalonOptimization 26(1):29–56 2016.[18]ChiJin PraneethNetrapalli andMichaelIJordan.Minmaxoptimization:Stablelimitpointsofgradientdescentascentarelocallyoptimal.arXivpreprintarXiv:1902.00618 2019.[19]AnatoliJuditskyandArkadiNemirovski.Firstordermethodsfornonsmoothconvexlarge-scaleoptimization i:generalpurposemethods.OptimizationforMachineLearning pages121–148 2011.10[20]AnatoliJuditskyandArkadiNemirovski.Firstordermethodsfornonsmoothconvexlarge-scaleoptimization II:utilizingproblemsstructure.OptimizationforMachineLearning 30(9):149–183 2011.[21]ShamKakade ShaiShalev-Shwartz andAmbujTewari.Onthedualityofstrongconvexityandstrongsmoothness:Learningapplicationsandmatrixregularization.UnpublishedManuscript 2009.[22]PurushottamKar HarikrishnaNarasimhan andPrateekJain.Surrogatefunctionsformaximiz-ingprecisionatthetop.arXivpreprintarXiv:1505.06813 2015.[23]DavidKinderlehrerandGuidoStampacchia.Anintroductiontovariationalinequalitiesandtheirapplications volume31.Siam 1980.[24]HidetoshiKomiya.Elementaryproofforsion’sminimaxtheorem.Kodaimathematicaljournal 11(1):5–7 1988.[25]JunpeiKomiyama AkikoTakeda JunyaHonda andHajimeShimao.Nonconvexoptimizationforregressionwithfairnessconstraints.InICML pages2742–2751 2018.[26]WeiweiKongandRenatoDCMonteiro.Anacceleratedinexactproximalpointmethodforsolvingnonconvex-concavemin-maxproblems.arXivpreprintarXiv:1905.13433 2019.[27]AYaKruger.Onfréchetsubdifferentials.JournalofMathematicalSciences 116(3):3325–3358 2003.[28]SongtaoLu IoannisTsaknakis MingyiHong andYongxinChen.Hybridblocksuccessiveapproximationforone-sidednon-convexmin-maxproblems:Algorithmsandapplications.arXivpreprintarXiv:1902.08294 2019.[29]AleksanderMadry AleksandarMakelov LudwigSchmidt DimitrisTsipras andAdrianVladu.Towardsdeeplearningmodelsresistanttoadversarialattacks.arXivpreprintarXiv:1706.06083 2017.[30]AryanMokhtari AsumanOzdaglar andSarathPattathil.Auniﬁedanalysisofextra-gradientandoptimisticgradientmethodsforsaddlepointproblems:Proximalpointapproach.arXivpreprintarXiv:1901.08511 2019.[31]RogerBMyerson.Gametheory.Harvarduniversitypress 2013.[32]AngeliaNedi´candAsumanOzdaglar.Subgradientmethodsforsaddle-pointproblems.Journalofoptimizationtheoryandapplications 142(1):205–228 2009.[33]ArkadiNemirovski.Efﬁcientmethodsforsolvingvariationalinequalities.EkonomikaiMatem.Metody 17:344–359 1981.[34]ArkadiNemirovski.Prox-methodwithrateofconvergenceO(1/t)forvariationalinequali-tieswithlipschitzcontinuousmonotoneoperatorsandsmoothconvex-concavesaddlepointproblems.SIAMJournalonOptimization 15(1):229–251 2004.[35]YuNesterov.Excessivegaptechniqueinnonsmoothconvexminimization.SIAMJournalonOptimization 16(1):235–249 2005.[36]YuNesterov.Smoothminimizationofnon-smoothfunctions.Mathematicalprogramming 103(1):127–152 2005.[37]YuriiNesterov.Introductorylecturesonconvexprogrammingvolumei:Basiccourse.1998.[38]YuriiENesterov.AmethodforsolvingtheconvexprogrammingproblemwithconvergencerateO(1/k2).InDokl.akad.naukSssr volume269 pages543–547 1983.[39]MaherNouiehed JasonDLee andMeisamRazaviyayn.Convergencetosecond-orderstation-arityforconstrainednon-convexoptimization.arXivpreprintarXiv:1810.02024 2018.11[40]MaherNouiehed MaziarSanjabi JasonDLee andMeisamRazaviyayn.Solvingaclassofnon-convexmin-maxgamesusingiterativeﬁrstordermethods.arXivpreprintarXiv:1902.08297 2019.[41]YuyuanOuyangandYangyangXu.Lowercomplexityboundsofﬁrst-ordermethodsforconvex-concavebilinearsaddle-pointproblems.arXivpreprintarXiv:1808.02901 2018.[42]HassanRaﬁque MingruiLiu QihangLin andTianbaoYang.Non-convexmin-maxop-timization:Provablealgorithmsandapplicationsinmachinelearning.arXivpreprintarXiv:1810.02060 2018.[43]MaziarSanjabi JimmyBa MeisamRazaviyayn andJasonDLee.Ontheconvergenceandrobustnessoftrainingganswithregularizedoptimaltransport.InAdvancesinNeuralInformationProcessingSystems pages7091–7101 2018.[44]MauriceSion.Ongeneralminimaxtheorems.PaciﬁcJournalofmathematics 8(1):171–176 1958.[45]SuvritSra SebastianNowozin andStephenJWright.Optimizationformachinelearning.MitPress 2012.[46]PaulTseng.Onlinearconvergenceofiterativemethodsforthevariationalinequalityproblem.JournalofComputationalandAppliedMathematics 60(1-2):237–252 1995.[47]PaulTseng.Onacceleratedproximalgradientmethodsforconvex-concaveoptimization.2008.[48]ZhipengXieandJianwenShi.Acceleratedprimaldualmethodforaclassofsaddlepointproblemwithstronglyconvexcomponent.arXivpreprintarXiv:1906.07691 2019.[49]YangyangXu.Iterationcomplexityofinexactaugmentedlagrangianmethodsforconstrainedconvexprogramming.arXivpreprintarXiv:1711.05812 2017.[50]YangyangXuandShuzhongZhang.Acceleratedprimal–dualproximalblockcoordinateupdatingmethodsforconstrainedconvexoptimization.ComputationalOptimizationandApplications 70(1):91–128 2018.[51]RenboZhao.Optimalalgorithmsforstochasticthree-compositeconvex-concavesaddlepointproblems.arXivpreprintarXiv:1903.01687 2019.12,Kiran Thekumparampil
Prateek Jain
Praneeth Netrapalli
Sewoong Oh