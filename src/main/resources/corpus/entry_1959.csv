2011,Extracting Speaker-Specific Information with a Regularized Siamese Deep Network,Speech conveys different yet mixed information ranging from linguistic to speaker-specific components  and each of them should be exclusively used in a specific task. However  it is extremely difficult to extract a specific information component given the fact that nearly all existing acoustic representations carry all types of speech information. Thus  the use of the same representation in both speech and speaker recognition hinders a system from producing better performance due to interference of irrelevant information. In this paper  we present a deep neural architecture to extract speaker-specific information from MFCCs. As a result  a multi-objective loss function is proposed for learning speaker-specific characteristics and regularization via normalizing interference of non-speaker related information and avoiding information loss. With LDC benchmark corpora and a Chinese speech corpus  we demonstrate that a resultant speaker-specific representation is insensitive to text/languages spoken and environmental mismatches and hence outperforms MFCCs and other state-of-the-art techniques in speaker recognition. We discuss relevant issues and relate our approach to previous work.,Extracting Speaker-Speciﬁc Information with a

Regularized Siamese Deep Network

Ke Chen and Ahmad Salman

School of Computer Science  The University of Manchester

Manchester M13 9PL  United Kingdom

{chen salmana}@cs.manchester.ac.uk

Abstract

Speech conveys different yet mixed information ranging from linguistic to
speaker-speciﬁc components  and each of them should be exclusively used in a
speciﬁc task. However  it is extremely difﬁcult to extract a speciﬁc information
component given the fact that nearly all existing acoustic representations carry
all types of speech information. Thus  the use of the same representation in both
speech and speaker recognition hinders a system from producing better perfor-
mance due to interference of irrelevant information. In this paper  we present a
deep neural architecture to extract speaker-speciﬁc information from MFCCs. As
a result  a multi-objective loss function is proposed for learning speaker-speciﬁc
characteristics and regularization via normalizing interference of non-speaker re-
lated information and avoiding information loss. With LDC benchmark corpora
and a Chinese speech corpus  we demonstrate that a resultant speaker-speciﬁc rep-
resentation is insensitive to text/languages spoken and environmental mismatches
and hence outperforms MFCCs and other state-of-the-art techniques in speaker
recognition. We discuss relevant issues and relate our approach to previous work.

1 Introduction

It is well known that speech conveys various yet mixed information where there are linguistic in-
formation  a major component  and non-verbal information such as speaker-speciﬁc and emotional
components [1]. For human communication  all the information components in speech turn out to
be very useful and exclusively used for different tasks. For example  one often recognizes a speaker
regardless of what is spoken for speaker recognition  while it is effortless for him/her to understand
what is exactly spoken by different speakers for speech recognition. In general  however  there is no
effective way to automatically extract an information component of interest from speech signals so
that the same representation has to be used in different speech information tasks. The interference
of different yet entangled speech information components in most existing acoustic representations
hinders a speech or speaker recognition system from achieving better performance [1].
For speaker-speciﬁc information extraction  two main efforts have been made so far; one is the use
of data component analysis [2]  e.g.  PCA or ICA  and the other is the use of adaptive ﬁltering
techniques [3]. However  the aforementioned techniques either fail to associate extracted data com-
ponents with speaker-speciﬁc information as such information is non-predominant over speech or
obtain features overﬁtting to a speciﬁc corpus since it is unlikely that speaker-speciﬁc information
is statically resided in ﬁxed frequency bands. Hence  the problem is still unsolved in general [4].
Recent studies suggested that learning deep architectures (DAs) provides a new way for tackling
complex AI problems [5]. In particular  representations learned by DAs greatly facilitate various
recognition tasks and constantly lead to the improved performance in machine perception [6]-[9]. On
the other hand  the Siamese architecture originally proposed in [10] uses supervised yet contrastive

1

Figure 1: Regularized Siamese deep network (RSDN) architecture.

learning to explore intrinsic similarity/disimilarity underlying an unknown data space. Incorporated
by DAs  the Siamese architecture has been successfully applied to face recognition [11] and dimen-
sionality reduction [12]. Inspired by the aforementioned work  we present a regularized Siamese
deep network (RSDN) to extract speaker-speciﬁc information from a spectral representation  Mel
Frequency Cepstral Coefﬁcients (MFCCs)  commonly used in both speech and speaker recognition.
A multi-objective loss function is proposed for learning speaker-speciﬁc characteristics  normalizing
interference of non-speaker related information and avoiding information loss. Our RSDN learning
adopts the famous two-phase deep learning strategy [5] [13]; i.e.  greedy layer-wise unsupervised
learning for initializing its component deep neural networks followed by global supervised learning
based on the proposed loss function. With LDC benchmark corpora [14] and a Chinese corpus [15] 
we demonstrate that a generic speaker-speciﬁc representation learned by our RSDN is insensitive
to text and languages spoken and  moreover  applicable to speech corpora unseen during learning.
Experimental results in speaker recognition suggest that a representation learned by the RSDN out-
performs MFCCs and that by the CDBN [9] that learns a generic speech representation without
speaker-speciﬁc information extraction. To our best knowledge  the work presented in this paper is
the ﬁrst attempt on speaker-speciﬁc information extraction with deep learning.
In the reminder of this paper  Sect. 2 describes our RSDN architecture and proposes a loss function.
Sect. 3 presents a two-phase learning algorithm to train the RSDN. Sect. 4 reports our experimental
methodology and results. The last section discusses relevant issues and relates our approach to
previous work in deep learning.

2 Model Description

In this section  we ﬁrst describe our RSDN architecture and then propose a multi-objective loss
function used to train the RSDN for learning speaker-speciﬁc characteristics.

2.1 Architecture

As illustrated in Figure 1  our RSDN architecture consists of two subnets  and each subnet is a fully
connected multi-layered perceptron of 2K+1 layers  i.e.  an input layer  2K-1 hidden layers and a
visible layer at the top. If we stipulate that layer 0 is input layer  there are the same number of
neurons in layers k and 2K-k for k = 0  1 ···   K. In particular  the Kth hidden layer is used as
code layer  and neurons in this layer are further divided into two subsets. As depicted in Figure 1 
those neurons in the box named CS and colored in red constitute one subset for encoding speaker-
speciﬁc information and all remaining neurons in the code layer form the other subset expected to

2

1tx2txˆ1txˆ2txCSCS ;12DCSXCSX4where

µ(i) =

TB(cid:88)

t=1

TB(cid:88)

t=1

accommodate non-speaker related information. The input to each subnet is an MFCC representation
of a frame after a short-term analysis that a speech segment is divided into a number of frames and
the MFCC representation is achieved for each frame. As depicted in Figure 1  xit is the MFCC
feature vector of frame t in Xi  input to subnet i (i=1 2)  where Xi = {xit}TB
t=1 collectively denotes
MFCC feature vectors for a speech segment of TB frames.
During learning  two identical subsets are coupled at their coding layers via neurons in CS with an
incompatibility measure deﬁned on two speech segments of equal length  X1 and X2  input to two
subnets  which will be presented in 2.2. After learning  we achieve two identical subnets and hence
can use either of them to produce a new representation for a speech frame. For input x to a subnet 
only the bottom K layers of the subnet are used and the output of neurons in CS at the code layer or
layer K  denoted by CS(x)  is its new representation  as illustrated by the dash box in Figure 1.

2.2 Loss Function
Let CS(xit) be the output of all neurons in CS of subnet i (i=1 2) for input xit ∈ Xi and CS(Xi) =
{CS(xit)}TB
t=1  which pools output of neurons in CS for TB frames in Xi  as illustrated in Figure 1.
As statistics of speech signals is more likely to capture speaker-speciﬁc information [5]  we deﬁne
the incompatibility measure based on the 1st- and 2nd-order statistics of a new representation to be
learned as

D[CS(X1) CS(X2); Θ] = ||µ(1) − µ(2)||2

2 + ||Σ(1) − Σ(2)||2
F  

(1)

1
TB

CS(xit)  Σ(i) =

1

TB − 1

[CS(xit) − µ(i)][CS(xit) − µ(i)]T   i = 1  2.

TB(cid:88)

t=1

In Eq. (1)  || · ||2 and || · ||F are the L2 norm and the Frobenius norm  respectively. Θ is a collective
notation of all connection weights and biases in the RSDN. Intuitively  two speech segments belong-
ing to different speakers lead to different statistics and hence their incompatibility score measured
by (1) should be large after learning. Otherwise their score is expected to be small.
For a corpus of multiple speakers  we can construct a training set so that an example be in the form:
(X1  X2;I) where I is the label deﬁned as I = 1 if two speech segments  X1 and X2  are spoken
by the same speaker or I = 0 otherwise. Using such training examples  we apply the energy-based
model principle [16] to deﬁne a loss function as

L(X1  X2; Θ) = α[LR(X1; Θ) + LR(X2; Θ)] + (1 − α)LD(X1  X2; Θ) 

(2)

− Dm

1
TB

λm + e

− DS

λS ).

||xit − ˆxit||2

2 and DS = ||Σ(1) − Σ(2)||2

2 (i = 1  2)  LD(X1  X2; Θ) = ID + (1 − I)(e

where
LR(Xi; Θ) =
Here Dm = ||µ(1) − µ(2)||2
F . λm and λS are the tolerance bounds
of incompatibility scores in terms of Dm and DS  which can be estimated from a training set. In
LD(X1  X2; Θ)  we drop explicit parameters of D[CS(X1) CS(X2); Θ] to simplify presentation.
Eq. (2) deﬁnes a multi-objective loss function where α (0 < α < 1) is a parameter used to trade-
off between two objectives LR(Xi; Θ) and LD(X1  X2; Θ). The motivation for two objectives are
as follows. By nature  both speaker-speciﬁc and non-speaker related information components are
entangled over speech [1] [5]. When we tend to extract speaker-speciﬁc information  the interfer-
ence of non-speaker related information is inevitable and appears in various forms. LD(X1  X2; Θ)
measures errors responsible for wrong speaker-speciﬁc statistics on a representation learned by a
Siamese DA in different situations. However  using LD(X1  X2; Θ) only to train a Siamese DA
cannot cope with enormous variations of non-speaker related information  in particular  linguistic
information (a predominant information component in speech)  which often leads to overﬁtting to
a training corpus according to our observations. As a result  we use LR(Xi; Θ) to measure re-
construction errors to monitor information loss during speaker-speciﬁc information extraction. By
minimizing reconstruction errors in two subnets  the code layer leads to a speaker-speciﬁc represen-
tation with the output of neurons in CS while the remaining neurons are used to regularize various
interference by capturing some invariant properties underlying them for good generalization.
In summary  we anticipate that minimizing the multi-objective loss function deﬁned in Eq. (2) will
enable our RSDN to extract speaker-speciﬁc information by encoding it through a generic speaker-
speciﬁc representation.

3

3 Learning Algorithm

(cid:161)

In this section  we apply the two-phase deep learning strategy [5] [13] to derive our learning algo-
rithm  i.e.  pre-training for initializing subnets and discriminative learning for learning a speaker-
speciﬁc representation.
We ﬁrst present the notation system used in our algorithm. Let hkj(xit) denote the output of the
jth neuron in layer k for k=0 1 ···  K ···  2K. hk(xit) =
j=1 is a collective notation
of the output of all neurons in layer k of subnet i (i=1 2) where |hk| is the number of neurons
in layer k. By this notation  k=0 refers to the input layer with h0(xit) = xit  and k=2K refers
In the coding layer  i.e.  layer K  CS(xit) =
to the top layer producing the reconstruction ˆxit.
k denote the
hKj(xit)
connection weight matrix between layers k-1 and k and the bias vector of layer k in subnet i (i=1 2) 
respectively  for k=1 ···  2K. Then output of layer k is hk(xit) = σ[uk(xit)] for k=1 ···  2K-1 
where uk(xit) = W (i)
j=1. Note that we use the
linear transfer function in the top layer  i.e.  layer 2K  to reconstruct the original input.

(cid:162)|CS|
j=1 is a simpliﬁed notation for output of neurons in CS. Let W(i)
(cid:162)|z|

k hk−1(xit) + b(i)

(1 + e−zj )−1

k and σ(z) =

(cid:162)|hk|

k and b(i)

hkj(xit)

(cid:161)

(cid:161)

3.1 Pre-training

For pre-training  we employ the denoising autoencoder [17] as a building block to initialize biases
and connection weight matrices of a subnet. A denoising autoencoder is a three-layered perceptron
where the input  ˜x  is a distorted version of the target output  x. For a training example  (˜x  x)  the
output of the autoencoder is a restored version  ˆx. Since MFCCs fed to the ﬁrst hidden layer and
its intermediate representation input to all other hidden layers are of continuous value  we always
distort input  x  by adding Gaussian noise to form a distorted version  ˜x. The restoration learning
is done by minimizing the MSE loss between x and ˆx with respect to the weight matrix and biases.
We apply the stochastic back-propagation (SBP) algorithm to train denoising autoencoders  and
the greedy layer-wise learning procedure [5] [13] leads to initial weight matrices for the ﬁrst K
hidden layers  as depicted in a dash box in Figure 1  i.e.  W1 ···   WK of a subnet. Then  we set
K−k+1 for k=1 ···  K to initialize WK+1 ···   W2K of the subnet. Finally  the second
WK+k = W T
subnet is created by simply duplicating the pre-trained one.

3.2 Discriminative Learning

For discriminative learning  we minimizing the loss function in Eq. (2) based on pre-trained subnets
for speaker-speciﬁc information extraction. Given our loss function is deﬁned on statistics of TB
frames in a speech segment  we cannot update parameters until we have TB output of neurons in
CS at the code layer. Fortunately  the SBP algorithm perfectly meets our requirement; In the SBP
algorithm  we always set the batch size to the number of frames in a speech segment. To simplify the
presentation  we shall drop explicit parameters in our derivation if doing so causes no ambiguities.
In terms of the reconstruction loss  LR(Xi; Θ)  we have the following gradients. For layer k = 2K 

For all hidden layers  k=2K-1 ···  1  applying the chain rule and (3) leads to

∂LR

∂u2K(xit)

= 2(ˆxit − xit)  i = 1  2.

(cid:181)

∂LR

=

∂LR

hkj(xit)[1−hkj(xit)]

∂LR

 

=

W (i)
k+1

∂hkj(xit)

∂uk(xit)
As the contrastive loss  LD(X1  X2; Θ)  deﬁned on neurons in CS at code layers of two subnets  its
gradients are determined only by parameters related to K hidden layers in two subnets  as depicted
by dash boxes in Figure 1. For layer k=K and subnet i=1  2  after a derivation (see the appendix for
details)  we obtain

∂uk+1(xit)

∂hk(xit)

j=1

(cid:163)

(cid:164)T

∂LR

(3)

. (4)

(cid:182)|hk|

4

(cid:179)(cid:161)
(cid:179)(cid:161)

∂LD

∂uK(xit)

=

[I − λ−1
[I − λ−1

m (1 − I)e
S (1 − I)e

− Dm

λm ]ψj(xit)

− DS

λS ]ξj(xit)

(cid:161)
(cid:161)

(cid:162)|hK|
(cid:162)|hK|

0

(cid:162)|CS|
(cid:162)|CS|

j=1 

j=1 

0

j=|CS|+1

j=|CS|+1

(cid:180)
(cid:180)

.

+

(5)

j

(cid:164)

j

and ξj(xit) = qj(xit)

(cid:161)CS(xit)
(cid:162)

1−(cid:161)CS(xit)
(cid:163)
(cid:162)

Here  ψj(xit) = p(i)
j
where p(i) = 2
TB
and

(cid:161)CS(xit)

(cid:162)
sign(1.5−i)(µ(1)−µ(2))  q(xit) = 4
(cid:181)
j is output of the jth neuron in CS for input xit. For layers k=K-1  ···  1  we have

1−(cid:161)CS(xit)
(cid:163)
(cid:162)
(cid:164)
TB−1 sign(1.5−i)(Σ(1)−Σ(2))[CS(xit)−µ(i)]
(cid:182)|hk|
t=1;I(cid:162)

∂hkj(xit)
j=1
Given a training example 
  we use gradients achieved from Eqs. (3)-(6) to
update all the parameters in the RSDN. For layers k=K+1  ···   2K  their parameters are updated by

(cid:161)CS(xit)
(cid:162)
(cid:164)T
(cid:163)
2(cid:88)
TB(cid:88)

(cid:161){x1t}TB
2(cid:88)

hkj(xit)[1−hkj(xit)]

t=1 {x2t}TB

TB(cid:88)

∂uk+1(xit)

∂uk(xit)

∂hk(xit)

W (i)
k+1

. (6)

∂LD

∂LD

∂LD

∂LR

 

j

=

=

 

j

k − α
TB

k − α
k ← W (i)
W (i)
TB
(cid:180)
For layers k=1  ···   K  their weight matrices and biases are updated with

[hk−1(xrt)]T   b(i)

k ← b(i)

∂uk(xrt)

∂LR

r=1

t=1

TB(cid:88)

∂LR

∂uk(xrt)

. (7)

t=1

r=1

(cid:179)
2(cid:88)
TB(cid:88)

r=1

∂LR

∂uk(xrt)

α

2(cid:88)

(cid:179)

k ← W (i)
W (i)

k − 
TB
k ← b(i)
k − 
b(i)
TB

t=1

+(1 − α)

∂LD

∂uk(xrt)

[hk−1(xrt)]T  

(cid:180)

.

(8a)

(8b)

α

∂LR

∂uk(xrt)

+(1 − α)

∂LD

∂uk(xrt)

t=1

r=1

In Eqs. (7) and (8)   is a learning rate. Here we emphasize that using sum of gradients caused by
two subnets in update rules guarantees that two subsets are always kept identical during learning.

4 Experiment

In this section  we describe our experimental methodology and report experiments results in visual-
ization of vowel distributions  speaker comparison and speaker segmentation.
We employ two LDC benchmark corpora [14]  KING and TIMIT  and a Chinese speech corpus [15] 
CHN  in our experiments. KING  including wide-band and narrow-band sets  consists of 51 speakers
whose utterances were recorded in 10 sessions. By convention  its narrow-band set is called NKING
while KING itself is often referred to its wide-band set. There are 630 speakers in TIMIT and 59
speakers in CHN of three sessions  respectively. All corpora were collected especially for evaluating
a speaker recognition system. The same feature extraction procedure is applied to all three corpora;
i.e.  after a short-term analysis suggested in [18]  including silence removal with an energy-based
method  pre-emphasis with the ﬁlter H(z) = 1−0.95z−1 as well as Hamming windowing with the
size of 20 ms and 10 ms shift  we extract 19-order MFCCs [1] for each frame.
For the RSDN learning  we use utterances of all 49 speakers recorded in sessions 1 and 2 in KING.
Furthermore  we distort all the utterances by the additive white noise channel with SNR of 10dB
and the Rayleigh fading channel with 5 Hz Doppler shift [19] to simulate channel effects. Thus
our training set consists of clean utterances and their corrupted versions. We randomly divide all
utterances into speech segments of a length TB (1 sec≤ TB ≤ 2 sec) and then exhaustively combine
them to form training examples as described in Sect. 2.2. With a validation set of all the utterances
recorded in session 3 in KING  we select a structure of K=4 (100  100  100 and 200 neurons in
layers 1-4 and |CS|=100 in the code layer or layer 4) from candidate models of 2<K<5 and 50-
1000 neurons in a hidden layer. Parameters used in our learning are as follows: Gaussian noise of
N (0  0.1σ) used in denoising autoencoder  α=0.2  λm=100 and λS=2.5 in the loss function deﬁned
in Eq. (2)  and learning rates =0.01 and 0.001 for pre-training and discriminative learning. After
learning  the RSDN is used to yield a 100-dimensional representation  CS  from 19-order MFCCs.
For any speaker recognition tasks  speaker modeling (SM) is inevitable. In our experiments  we use
the 1st- and 2nd-order statistics of a speech segment based on a representation  SM = {µ  Σ}  for
SM. Furthermore  we employ a speaker distance metric: d(SM1 SM2) = tr[(Σ−1
2 )(µ1 −
µ2)(µ1 − µ2)T ]  where SMi = {µi  Σi} (i = 1  2) are two speaker models (SMs). This distance
metric is derived from the divergence metric for two normal distributions [20] by dropping the term
concerning only covariance matrices based on our observation that covariance matrices often vary
considerably for short segments and the original divergence metric often leads to poor performance
for various representations including MFCCs and ours. In contrast  the one deﬁned above is stable
irrespective of utterance lengths and results in good performance for different representations.

1 + Σ−1

5

Figure 2: Visualization of all 20 vowels. (a) CS representation. (b) CS representation. (c) MFCCs.

(a)

(b)

(c)

4.1 Visualization

Vowels have been recognized to be a main carrier of speaker-speciﬁc information [1] [4] [18] [20].
TIMIT [14] provides phonetic transcription of all 10 utterances containing all 20 vowels in English
for every speaker. As all the vowels may appear in 10 different utterances  up to 200 vowel segments
in length of 0.1-0.5 sec are available for a speaker  which enables us to investigate vowel distributions
in a representation space for different speakers. Here  we merely visualize mean feature vectors of
up to 200 segments for a speaker in terms of a speciﬁc representation with the t-SNE method [21] 
which is likely to reﬂect intrinsic manifolds  by projecting them onto a two-dimensional plane.
In the code layer of our RSDN  output of neurons 1-100 forms a speaker-speciﬁc representation  CS 
and that of remaining 100 neurons becomes a non-speaker related representation  dubbed CS. For a
noticeable effect  we randomly choose only ﬁve speakers (four females and one male) and visualize
their vowel distributions in Figure 2 in terms of CS  CS and MFCC representations  respectively 
where a maker/color corresponds to a speaker. It is evident from Figure 2(a) that  by using the CS
representation  most vowels spoken by a speaker are tightly grouped together while vowels spoken
by different speakers are well separated. For the CS representation  close inspection on Figure
2(b) reveals that the same vowels spoken by different speakers are  to a great extent  co-located.
Moreover  most of phonetically correlated vowels  as circled and labeled  are closely located in
dense regions independent of speakers and genders. For comparison  we also visualize the same by
using their original MFCCs in Figure 2(c) and observe that most of phonetically correlated vowels
are also co-located  as circled and labeled  whilst others scatter across the plane and their positions
are determined mainly by vowels but affected by speakers. In particular  most of vowels spoken
by the male  marked by (cid:164) and colored by green  are grouped tightly but isolated from those by all
females. Thus  visualization in Figure 2 demonstrates how our RSDN learning works and could lend
an evidence to justiﬁcation on why MFCCs can be used in both speech and speaker recognition [1].

4.2 Speaker Comparison

Speaker comparison (SC) is an essential process involved in any speaker recognition tasks by com-
paring two speaker models to collect evidence for decision-making  which provides a direct way to
evaluate representations/speaker modeling without addressing decision-making issues [22]. In our
SC experiments  we employ NKING [14]  a narrow-band corpus  of many variabilities. During data
collection  there was a “great divide” between sessions 1-5 and 6-10; both recording device and en-
vironments changed  which alters spectral features of 26 speakers and leads to 10dB SNR reduction
on average. As suggested in [18]  we conduct two experiments: within-divide where SMs built
on utterances in session 1 are compared to SMs on those in sessions 2-5 and cross-divide where
SMs built on utterances in session 1 are compared with those in sessions 6-10. As short utterances
poses a greater challenge for speaker recognition [4] [18] [20]  utterances are partitioned into short
segments of a certain length and SMs built on segments of the same length are always used for SC.
For a thorough evaluation  we apply the SM technique in question to our representation  MFCCs 
and a representation (i.e.  the better one of those yielded by two layers) learned by the CDBN [9]
on all 10 sessions in NKING  and name them SM-RSDN  SM-MFCC and SM-CDBN hereinafter. In
addition  we also compare them to GMMs trained on MFCCs (GMM-MFCC)  a state-of-the-art SM
technique that provides the baseline performance [4] [20]  where for each speaker a GMM-based
SM consisting of 32 Gaussian components is trained on his/her utterances of 60 sec in sessions 1-2
with the EM algorithm [18]. For the CDBN learning [9] and the GMM training [18]  we strictly
follow their suggested parameter settings in our experiments (see [9] [18] for details).

6

/aa/  /iy/  /aw/  /ay//ae/  /aw/  /iy/  /ix//iy/  /ih/  /eh/  /ix//ae/  /aa/  /aw/  /ay/(a)

(b)

(c)

Figure 3: Performance of speaker comparison (DET) in the within-divide (upper row) and the cross-
divide (lower row) experiments for different segment lengths. (a) 1 sec. (b) 3 sec. (d) 5 sec.
Table 1: Performance (mean±std)% of speaker segmentation on TIMIT and CHN audio streams.
Index

TIMIT Audio Stream

CHN Audio Stream

BIC-MFCC Dist-MFCC Dist-RSDN BIC-MFCC Dist-MFCC Dist-RSDN

FAR
MDR
F1

26±09
26±14
67±12

22±11
22±12
74±11

18±11
18±10
79±09

46±04
46±10
44±08

27±11
27±17
68±17

24±11
24±17
72±17

We use Detection Error Trade-off (DET) curves as the performance index in SC. From Figure 3  it is
evident that SM-RSDN outperforms SM-MFCC  SM-CDBN and GMM-MFCC  a baseline system
trained on much longer utterances  as it always yields a smaller operating region  i.e.  all possible
errors  in all the settings. In contrast  SM-MFCC performs better in within-divide settings while
SM-CDBN is always inferior to the baseline system. Relevant issues will be discussed later on.

4.3 Speaker Segmentation

Speaker segmentation (SS) is a task of detecting speaker change points in an audio stream to split
it into acoustically homogeneous segments so that every segment contains only one speaker [23].
Following the same protocol used in previous work [23]  we utilize utterances in TIMIT and CHN
corpora to simulate audio conversations. As a result  we randomly select 250 speakers from TIMIT
to create 25 audio streams where the duration of speakers ranges from 1.6 to 7.0 sec and 50 speakers
from CHN to create 15 audio streams where the duration of speakers is from 3.0 to 8.3 sec. In the
absence of prior knowledge  the distance-based and the BIC techniques are two main approaches
to SS [23]. In our simulations  we apply the distance-based method [23] to our representation and
MFCCs  dubbed Dist-RSDN and Dist-MFCC  where the same parameters  including sliding window
of 1.5 sec and tolerance level of 0.5 sec  are used. In addition  we also apply the BIC method [23] to
MFCCs (BIC-MFCC). Note that the BIC method is inapplicable to our representation since it uses
only covariance information but the high dimensionality of our representation and the use of a small
sliding window in the BIC result in unstable performance  as pointed out early in this section.
For evaluation  we use three common indexes [23]  i.e.  False Alarm Rate (FAR)  Miss Detection
Rate (MDR) and F1 measure deﬁned based on both precision and recall rates. Moreover  we only
report results as FAR equals MDR to avoid addressing decision-making issues [23]. Table 1 tabulates
SS performance where  as boldfaced  results by our representation are superior to those by MFCCs
regardless of SS methods and corpora for creating audio streams used in our simulations.
In summary  visualization of vowels and results in SC and SS suggest that our RSDN successfully
extracts speaker-speciﬁc information; its resultant representation can be generalized to unseen cor-
pora during learning and is insensitive to text and languages spoken and environmental changes.

7

0.10.20.40.60.810.10.20.40.60.81False Alarm ProbabilityMiss ProbabilitySM−MFCCGMM−MFCCSM−CDBNSM−RSDN0.10.20.40.60.810.10.20.40.60.81False Alarm ProbabilityMiss ProbabilitySM−MFCCGMM−MFCCSM−CDBNSM−RSDN0.10.20.40.60.810.10.20.40.60.81False Alarm ProbabilityMiss ProbabilitySM−MFCCGMM−MFCCSM−CDBNSM−RSDN0.10.20.40.60.810.10.20.40.60.81False Alarm ProbabilityMiss ProbabilitySM−MFCCGMM−MFCCSM−CDBNSM−RSDN0.10.20.40.60.810.10.20.40.60.81False Alarm ProbabilityMiss ProbabilitySM−MFCCGMM−MFCCSM−CDBNSM−RSDN0.10.20.40.60.810.10.20.40.60.81False Alarm ProbabilityMiss ProbabilitySM−MFCCGMM−MFCCSM−CDBNSM−RSDN5 Discussion

As pointed out earlier  speech carries different yet mixed information and speaker-speciﬁc informa-
tion is minor in comparison to predominant linguistic information. Our empirical studies suggest
that our success in extracting speaker-speciﬁc information is attributed to both unsupervised pre-
training and supervised discriminative learning with a contrastive loss. In particular  the use of data
regularization in discriminative learning and distorted data in two learning phases plays a critical
role in capturing intrinsic speaker-speciﬁc characteristics and variations caused by miscellaneous
mismatches. Our results not reported here  due to limited space  indicate that without the pre-
training in Sect. 3.1  a randomly initialized RSDN leads to unstable performance often considerably
worse than that of using the pre-training in general. Without discriminative learning  a DA working
on unsupervised learning only  e.g.  the CDBN [9]  tends to yield a new representation that redis-
tributes different information but does not highlight minor speaker-speciﬁc information given the
fact that the CDBN trained on all 10 sessions in NKING leads to a representation that fails to yield
satisfactory SC performance on the same corpus but works well for various audio classiﬁcation tasks
[9]. If we do not use the regularization term  LR(Xi; Θ)  in the loss function in Eq. (2)  our RSDN
is boiled down to a standard Siamese architecture [10]. Our results not reported here show that such
an architecture learns a representation often overﬁtting to the training corpus due to interference
of predominant non-speaker related information  which is not a problem in predominant informa-
tion extraction. The previous work in face recognition [11] could lend an evidence to support our
argument where a Siamese DA without regularization successfully captures predominant identity
characteristics from facial images as  we believe  facial expression and other non-identity informa-
tion are minor in this situation. While the use of distorted data in pre-training is in the same spirit of
self-taught learning [24]  our results including those not reported here reveal that the use of distorted
data in pre-training but not in discriminative learning yields results worse than the baseline perfor-
mance in the cross-divide SC experiment. Hence  sufﬁcient training data reﬂecting mismatches are
also required in discriminative learning for speaker-speciﬁc information extraction.
Our RSDN architecture resembles the one proposed in [12] for dimensionality reduction of hand-
written digits via learning a nonlinear embedding. However  ours distinguishes from theirs in the
use of different building blocks in our DAs  loss functions and motivations. The DA in [12] uses
the RBM [13] as a building block to construct a deep belief subnet in their Siamese DA and the
NCA [25] as their contrastive loss function to minimize the intra-class variability. However  the
NCA does not meet our requirements as there are so many examples in one class. Instead we pro-
pose a contrastive loss to minimize both intra- and inter-class variabilities simultaneously. On the
other hand  intrinsic topological structures of a handwritten digit convey predominant information
given the fact that without using the NCA loss a deep belief autoencoder already yields a good rep-
resentation [7] [12] [13] [26]. Thus  the use of the NCA in [12] simply reinforces the topological
invariance by minimizing other variabilities with a small amount of labeled data [12]. In our work 
however  speaker-speciﬁc information is non-predominant in speech and hence a large amount of la-
beled data reﬂecting miscellaneous variabilities are required during discriminative learning despite
the pre-training. Finally  our code layer yields an overcomplete representation to facilitate non-
predominant information extraction. In contrast  a parsimonious representation seems more suitable
for extracting predominant information since dimensionality reduction is likely to discover “princi-
pal” components that often associate with predominant information  as are evident in [11] [12].
To conclude  we propose a deep neural architecture for speaker-speciﬁc information extraction and
demonstrate that its resultant speaker-speciﬁc representation outperforms the state-of-the-art tech-
niques. It should also be stated that our work presented here is limited to speech corpora available
at present. In our ongoing work  we are employing richer training data towards learning a univer-
sal speaker-speciﬁc representation. In a broader sense  our work presented in this paper suggests
that speech information component analysis (ICA) becomes critical in various speech information
processing tasks; the use of proper speech ICA techniques would result in task-speciﬁc speech rep-
resentations to improve their performance radically. Our work demonstrates that speech ICA is
feasible via learning. Moreover  deep learning could be a promising methodology for speech ICA.

Acknowledgments

Authors would like to thank H. Lee for providing their CDBN code [9] and L. Wang for offering
their SIAT Chinese speech corpus [15] to us; both of which were used in our experiments.

8

References

[1] Huang  X.  Acero  A. & Hon  H. (2001) Spoken Language Processing. New York: Prentice Hall.
[2] Jang  G.  Lee  T. & Oh  Y. (2001) Learning statistically efﬁcient feature for speaker recognition. Proc.
ICASSP  pp. I427-I440  IEEE Press.
[3] Mammone  R.  Zhang  X. & Ramachandran  R. (1996) Robust speaker recognition: a feature-based ap-
proach. IEEE Signal Processing Magazine  13(1): 58-71.
[4] Reynold  D. & Campbell  W. (2008) Text-independent speaker recognition. In J. Benesty  M. Sondhi and
Y. Huang (Eds.)  Handbook of Speech Processing  pp. 763-781  Berlin: Springer.
[5] Bengio  Y. (2009) Learning deep architectures for AI. Foundation and Trends in Machine Learning 2(1):
1-127.
[6] Hinton  G. (2007) Learning multiple layers of representation. Trends in Cognitive Science 11(10): 428-434.
[7] Larochelle  H.  Bengio  Y.  Louradour  J. & Lamblin  P. (2009) Exploring strategies for training deep neural
networks. Journal of Machine Learning Research 10(1): pp. 1-40.
[8] Boureau  Y.  Bach  F.  LeCun  Y. & Ponce  J. (2010) Learning mid-level features for recognition. Proc.
CVPR  IEEE Press.
[9] Lee  H.  Largman  Y.  Pham  P. & Ng  A. (2009) Unsupervised feature learning for audio classiﬁcation using
convolutional deep belief networks. In Advances in Neural Information Processing Systems 22  Cambridge 
MA: MIT Press.
[10] Bromley  J.  Guyon  I.  LeCun  Y.  Sackinger  E. & Shah  R. (1994) Signature veriﬁcation using a Siamese
time delay neural network. In Advances in Neural Information Processing Systems 5  Morgan Kaufmann.
[11] Chopra  S.  Hadsell  R. & LeCun  Y. (2005) Learning a similarity metric discriminatively  with application
to face veriﬁcation. In Proc. CVPR  IEEE Press.
[12] Salakhutdinov  R. & Hinton  G. (2007) Learning a non-linear embedding by preserving class neighborhood
structure. In Proc. AISTATS  Cambridge  MA: MIT Press.
[13] Hinton  G.  Osindero  S. & Teh  Y. (2006) A fast learning algorithm for deep belief nets. Neural Compu-
tation 18(7): 1527-1554.
[14] Linguistic Data Consortium (LDC). [online] www.ldc.upenn.edu
[15] Wang  L. (2008) A Chinese speech corpus for speaker recognition. Tech. Report  SIAT-CAS  China.
[16] LeCun  Y.  Chopra  S. Hadsell  R.  Ranzato  M. & Huang  F. (2007) Energy-based models. In Predicting
Structured Outputs  pp. 191-246  Cambridge  MA: MIT Press.
[17] Vincent  P.  Bengio  Y. & Manzagol  P. (2008) Extracting and composing robust features with denoising
autoencoders. Proc. ICML  pp. 1096-1102  ACM Press.
[18] Reynolds  D. (1995) Speaker Identiﬁcation and veriﬁcation using Gaussian mixture speaker models.
Speech Communication 17(1): 91-108.
[19] Proakis  J. (2001) Digital Communications (4th Edition). New York: McGraw-Hill.
[20] Campbell  J. (1997) Speaker recognition: A tutorial. Proceedings of The IEEE 85(10): 1437-1462.
[21] van der Maaten  L. & Hinton  G. (2008) Visualizing data using t-SNE. Journal of Machine Learning
Research 9: 2579-2605.
[22] Campbell  W. & Karam  Z. (2009) Speaker comparison with inner product discriminant functions. In
Advances in Neural Information Processing Systems 22  Cambridge  MA: MIT Press.
[23] Kotti  M.  Moschou  V. & Kotropoulos  C. (2008) Speaker segmentation and clustering. Signal Processing
88(8): 1091-1124.
[24] Raina  R.  Battle  A.  Lee  H.  Packer  B. & Ng  A. (2007) Self-taught learning: transfer learning from
unlabeled data. Proc. ICML  ACM press.
[25] Goldberger  J.  Roweis  S.  Hinton  G. & Salakhutdinov  R.  (2005) Neighbourhood component analysis.
In Advances in Neural Information Processing Systems 17  Cambridge  MA: MIT Press.
[26] Hinton  G. & Salakhutdinov  R. (2006) Reducing the dimensionality of data with neural networks. Science
313: 504-507.

9

,Konstantinos Bousmalis
George Trigeorgis
Nathan Silberman
Dilip Krishnan
Dumitru Erhan
Rui Costa
Ioannis Alexandros Assael
Brendan Shillingford
Nando de Freitas
TIm Vogels