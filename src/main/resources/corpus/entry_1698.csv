2018,Efficient Convex Completion of Coupled Tensors using Coupled Nuclear Norms,Coupled norms have emerged as a convex method to solve coupled tensor completion. A limitation with coupled norms is that they only induce low-rankness using the multilinear rank of coupled tensors. In this paper  we introduce a new set of coupled norms known as coupled nuclear norms by constraining the CP rank of coupled tensors. We propose new coupled completion models using the coupled nuclear norms as regularizers  which can be optimized using computationally efficient optimization methods. We derive excess risk bounds for proposed coupled completion models and show that proposed norms lead to better performance. Through simulation and real-data experiments  we demonstrate that proposed norms achieve better performance for coupled completion compared to existing coupled norms.,Efﬁcient Convex Completion of Coupled Tensors

using Coupled Nuclear Norms

Kishan Wimalawarne1 and Hiroshi Mamitsuka1 2

1Bioinformatics Center  Kyoto University  Kyoto  Japan

2Department of Computer Science  Aalto University  Espoo  Finland

kishanwn@gmail.com  mami@kuicr.kyoto-u.ac.jp

Abstract

Coupled norms have emerged as a convex method to solve coupled tensor com-
pletion. A limitation with coupled norms is that they only induce low-rankness
using the multilinear rank of coupled tensors. In this paper  we introduce a new
set of coupled norms known as coupled nuclear norms by constraining the CP
rank of coupled tensors. We propose new coupled completion models using the
coupled nuclear norms as regularizers  which can be optimized using computa-
tionally efﬁcient optimization methods. We derive excess risk bounds for pro-
posed coupled completion models and show that proposed norms lead to better
performance. Through simulation and real-data experiments  we demonstrate that
proposed norms achieve better performance for coupled completion compared to
existing coupled norms.

1

Introduction

In this paper  we investigate convex coupled norms for coupled tensor completion. Two tensors
are considered to be coupled when they share a common mode. A well explored problem with
coupled tensors is coupled tensor completion  which studies imputation of partially observed tensors
using coupled tensors as side information (Acar et al.  2014; Bouchard et al.  2013). Coupled tensor
completion is commonly found in many real world applications such as link prediction (Ermis et al. 
2015)  recommendation systems (Acar et al.  2014) and computer vision (Li et al.  2015). Moreover 
the increase in availability of data from multiple sources further makes coupled tensor completion
an important research area requiring thorough investigation.
Over the years  several methods have been proposed to solve coupled tensor completion (Acar et al. 
2014; Ermis et al.  2015). However  many of these methods are non-convex models leading to local
optimal solutions. Additionally  these non-convex models have requirements of specifying ranks
of coupled tensors  which are in many situations unknown. The recent development of coupled
norms (Wimalawarne et al.  2018) has emerged as a convex solution for coupled completion. These
coupled norms are modeled using the trace norm regularization  which eliminates the requirement
of pre-specifying ranks. In spite of favorable qualities  coupled norms only induce low-rankness
with respect to the multilinear rank of coupled tensors. This makes coupled norms sub-optimal for
completion of coupled tensors with other low rank structures.
Until recently  most of the research on convex norms that induces low-rankness of tensors has fo-
cused on constraining the multilinear rank (Tomioka and Suzuki  2013; Wimalawarne et al.  2014).
However  recent studies (Yuan and Zhang  2016) have shown that the tensor nuclear norm  which
is a convex relaxation to minimizing the CANDECOMP/PARAFAC (CP) rank (Carroll and Chang 
1970; Harshman  1970; Hitchcock  1927; Kolda and Bader  2009) has favorable properties com-
pared low rank inducing norms that constrains the multilinear rank (Tomioka and Suzuki  2013;

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

p

Wimalawarne et al.  2014). More speciﬁcally  Yang et al. (2015) showed that tensor comple-
tion using the tensor nuclear norm leads to better sample complexity compared to the over-
lapped norm (Tomioka and Suzuki  2013; Liu et al.  2013)  which was experimentally veriﬁed by
Yuan and Zhang (2016). These advantages are unavailable for coupled norms since they do not
support the tensor nuclear norm nor do they constrain the CP rank of coupled tensors.
In this paper  we investigate coupled completion through constraining the CP ranks of coupled ten-
sors. We propose a set of convex coupled norms by extending the tensor nuclear norm. Additionally 
we propose novel completion models that are regularized by the proposed norms  which obtain
globally optimal solutions. We present theoretical analysis of the proposed completion models us-
ing excess risk bound analysis. Our analysis shows that the excess risk bound for two coupled
K-mode tensors  X 2 Rn(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)n and Y 2 Rn(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)n  both having same CP rank r  is bounded by
n(ln n)K(cid:0)1=2). We show that the obtained excess risk bounds are smaller compared
O(r24KK
to excess risk bounds resulting from multilinear rank based coupled norms. Finally  we verify our
theoretical claims by simulation and real-data experiments.
specify the mode-k unfolding (Kolda and Bader  2009) by T(k) 2 Rnk(cid:2)∏
We use the following notations throughout the paper. Given a K-mode tensor T 2 Rn1(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nK   we
j̸=k nj   which is obtained
by concatenating all slices along the mode-k. Given two matrices  M 2 Rn1(cid:2)n2 and N 2 Rn1(cid:2)n
′
2 
the notation [M ; N ] 2 Rn1(cid:2)(n2+n
′
∑
2) represents their concatenation on the common mode-1. We
indicate the outer product between vectors ui 2 Rni; i = 1; : : : ; N using the notation (cid:10) as (u1 (cid:10)
l=1 ul;il. The k-mode product of a tensor T 2 Rn1(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nk(cid:1)(cid:1)(cid:1)(cid:2)nK and a vector
(cid:1)(cid:1)(cid:1)(cid:10) uN )i1;:::;iN =
v 2 Rnk is deﬁned as T (cid:2)k v =
Ti1;i2;:::;ik;:::;iK vik. Given that rank of the mode-k unfolding
of T is rk  the multilinear rank of T is deﬁned as (r1;(cid:1)(cid:1)(cid:1) ; rK).

∏

nk
ik=1

n

2 Review of Coupled Completion

We brieﬂy review existing coupled completion methods in this section.

2.1 Non-convex Factorization Methods

R

R

∑
Coupled completion models have been mostly investigated through factorization methods.
In
∑
essence  these methods consider explicit factorization of a coupled tensor T 2 Rn1(cid:2)n2(cid:2)n3 and a
i=1 ai(cid:10) bi(cid:10) ci having ai 2 Rn1; bi 2 Rn2 ; ci 2 Rn3; i = 1; : : : ; R
matrix M 2 Rn1(cid:2)m as T =
i=1 ai (cid:10) di having ai 2 Rn1; di 2 Rm; i = 1; : : : ; R  respectively  with a common
and M =
rank R and shared components ai; i = 1; : : : ; R. Many variations of factorization models for cou-
pled completion models have been proposed based on CP decomposition with shared and unshared
components (Acar et al.  2014)  Tucker decomposition (Ermis et al.  2015)  and non-negative factor-
ization (Ermis et al.  2015). However  due to factorization  these coupled completion models are
non-convex that lead to local optimal solutions. Furthermore  these methods require a priori speciﬁ-
cation of rank (R) of each tensor  as well as the number of shared components between the factorized
tensors.

2.2 Convex Coupled Norms

Coupled norms (Wimalawarne et al.  2018) are a set of convex norms designed by combining low
rank tensor and matrix norms. Given a tensor T 2 Rn1(cid:2)n2(cid:2)n3 and a matrix M 2 Rn1(cid:2)n
′
2 coupled
on mode a  coupled norms are deﬁned as

∥T ; M∥a

(b;c;d);

where the subscripts b; c; d 2 fO; L; S;(cid:0)g specify the regularization method to be applied to each
mode and the superscript a speciﬁes the mode in which the tensor and the matrix are coupled. No-
tations O  L  and S indicate that the respective mode is regularized by using the overlapping trace
norm (Tomioka and Suzuki  2013)  latent trace norm (Tomioka and Suzuki  2013)  and scaled latent
trace norm (Wimalawarne et al.  2014)  respectively  and (cid:0) indicates no regularization.

2

An example of a coupled norm that regularizes both coupled tensors using the overlapped trace norm
is

∥T ; M∥1

(O;O;O) := ∥[T(1); M ]∥tr +

3∑

∥T(k)∥tr:
3∑

1p
nk

k=2

The following norm is another example where we consider the T as a summation of latent tensors
T (1)  T (2)  and T (3) and apply the scaled latent norm as
∥[T (1)

∥T ; M∥1

∥T (k)

(

)

inf

k=2

:

(1) ; M ]∥tr +

(S;S;S) =

∥tr

(k)

T (1)+T (2)+T (3)=T

1p
n1

Given two partially observed tensors ^T1 2 Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nK ; K (cid:21) 3 and ^T2 2
′ (cid:21) 2  coupled on their mode-a with observed indexes given by the mappings
Rn
Ω ^T1

′
K′ ; K
  coupled completion is performed by solving

(cid:2)n
(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)n
and Ω ^T2

′
1

′
2

minT1;T2

1
2

∥Ω ^T1

(T1 (cid:0) ^T1)∥2

F +

∥Ω ^T2

1
2

(T2 (cid:0) ^T2)∥2

F + (cid:21)∥T1;T2∥a
cn;

cn is a suitable coupled norm.

where ∥T1;T2∥a
An important property with coupled norms is that the trace norm is applied with respect to each mode
unfolding of tensors. This results in inducing low-rankness only by using the multilinear rank of
coupled tensors. Furthermore  since the deﬁnitions of matrix rank and multilinear rank are different 
concatenated regularization on the coupled mode may not be optimal for sharing information among
the tensors.

3 Proposed Method: Coupled Completion via Coupled Nuclear Norms

{ 1∑

1∑

In this section  we propose a set of convex coupled norms that overcome limitations of existing
coupled completion methods. The main tool we use to build our norms is the tensor nuclear norm
(Yuan and Zhang  2016; Yang et al.  2015; Lim and Comon  2014)  which is deﬁned for a tensor
T 2 Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nK as

∥T ∥(cid:3) = inf

(cid:13)jjT =

(cid:13)ju1j (cid:10) u2j (cid:1)(cid:1)(cid:1) (cid:10) uKj;∥ukj∥2

j=1

j=1

(1)
In practice  we consider that T has a ﬁnite rank R  which is expressed by the notation rank(T ) = R.
When K = 2 and each ukj is orthogonal  the tensor nuclear norm is equivalent to the matrix nuclear
norm.
We now propose coupled norms by only using the tensor nuclear norms  thus low-rankness of both
the coupled tensors are induced using the CP rank. We name our norms coupled nuclear norms.
We introduce the following notation to deﬁne the coupled nuclear norms for two coupled tensors
W 2 Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nK and V 2 Rn

(cid:2)n

′
1

′
2

:

}
2 = 1; (cid:13)j (cid:21) (cid:13)j+1 > 0

(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)n
′
K′ as
∥W;V∥a
ccp;((cid:21)b;b)((cid:21)c;c);

where the superscript a indicates the coupled mode  and each tuple ((cid:21)b; b) and ((cid:21)c; c) indicates the
regualarization method for each tensor. We specify b; c 2 fF; Lg  where F and L indicate that a
tensor is regularized as a whole or as a latent decomposition  respectively. Furthermore  we indicate
(cid:21)b 2 R and (cid:21)c 2 R to specify regularization parameters for nuclear norms of each tensor. The
subscript ccp is used to distinguish the proposed norms from coupled norms in (Wimalawarne et al. 
2018).
Let us now look at a few deﬁnitions of coupled nuclear norms. We start with the following norm
}
(cid:13)ix1i (cid:10) (cid:1)(cid:1)(cid:1) (cid:10) xai (cid:10) (cid:1)(cid:1)(cid:1) xKi;

∥W∥(cid:3) (cid:20) (cid:21)1;∥V∥(cid:3) (cid:20) (cid:21)2

ccp;((cid:21)1;F)((cid:21)2;F) =

∥W;V∥a

{

(cid:12)(cid:12)(cid:12)(cid:12)W =
R∑
R∑

i=1

V =

(cid:23)iy1i (cid:10) (cid:1)(cid:1)(cid:1) (cid:10) xai (cid:10) (cid:1)(cid:1)(cid:1) yK′i

;

(2)

i=1

3

where the subscripts with F lead us to consider W and V as whole tensors without any latent decom-
position. We assume that each tensor has a rank R and all the component vectors xai; i = 1; : : : ; R
on the coupled mode a are common to both the tensors  while the tensor nuclear norm is applied to
W and V to constrain their ranks.
A limitation in the previous norm is that it assumes both W and V have the same rank and all
components along the coupled mode are common. In practice  this can be a strong assumption and
we need to have more freedom for ranks and the amount of sharing among tensors. To incorporate
these features into coupled nuclear norms  we propose to use latent decomposition of tensors  such
that we learn latent tensors that are coupled to other tensors as well as uncoupled. Next  we assume
a latent decomposition for W and deﬁne the following norm

ccp;((cid:21)1;(cid:21)2;L);((cid:21)3;F) =

inf

W (1)+W (2)=W
∥V∥(cid:3) (cid:20) (cid:21)3

∥W;V∥a
R1∑

(cid:12)(cid:12)(cid:12)(cid:12)W (1) =

(cid:13)(1)
i x(1)

1i

i=1

{
∥W (1)∥(cid:3) (cid:20) (cid:21)1;∥W (2)∥(cid:3) (cid:20) (cid:21)2;
}
(cid:10) (cid:1)(cid:1)(cid:1) (cid:10) x(2)
Ki;

R2∑

(cid:13)(2)
i x(2)

i=1

1i

(cid:23)iy1i (cid:10) (cid:1)(cid:1)(cid:1) (cid:10) xai (cid:10) (cid:1)(cid:1)(cid:1) yK′i

;

inf

W (1)+W (2)=W
(cid:10) (cid:1)(cid:1)(cid:1) (cid:10) xai (cid:10) (cid:1)(cid:1)(cid:1) x(1)
Ki; W (2) =
R1∑

V =

i=1

(3)
where the subscript ((cid:21)1; (cid:21)2; L) indicates that the tensor W is considered as two latent tensors and
their nuclear norms are constrained by (cid:21)1 and (cid:21)2. The third subscript ((cid:21)3; F) indicates that V is
considered as a whole without any latent decomposition. Further  the norm considers W (1) to have
common factors with V due to coupling and W (2) is independent from any coupling with V. Due to
the latent decomposition  the rank of W is R1 + R2  however  only R1 components of xa in W are
shared with V.
the
addition
In
deﬁne
ccp;((cid:21)1;F);((cid:21)2;(cid:21)3;L) where the tensor V is considered to have a latent
infV (1)+V (2)=V ∥W;V∥a
decomposition  and infW (1)+W (2)=W;V (1)+V (2)=V ∥W;V∥a
ccp;((cid:21)1;(cid:21)2;L);((cid:21)3;(cid:21)4;L) where both the
tensors are considered to have latent decompositions. Furthermore  our proposed norms can be
extended to deﬁne norms for coupled tensors with more than two coupled tensors.
It is important to note that the deﬁnition of proposed norms do not adhere to all the properties of
the normed space. Rather  they can be considered them as sets constructed by tensor nuclear norms.
However  we refer to our deﬁnitions as norms since they are constructed by constraining the tensor
nuclear norms. Further  we point out that the number of different norms we need for a coupled
tensor using coupled nuclear norms are less compared to multilinear rank based coupled norms
(Wimalawarne et al.  2018).

coupled

nuclear

norms 

further

above

can

we

to

3.1 New Coupled Completion Models

We now propose coupled completion models using coupled nuclear norms. Let us consider two
partially observed tensors X 2 Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nK and Y 2 Rn
(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)n
′
K′ coupled on the mode a.
Let us also consider Ω1 : Rn1(cid:2)n2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nK ! Rm1 and Ω2 : Rn
K′ ! Rm2 as mapping to
(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)n
′
′
observed elements of X and Y  respectively  where m1 and m2 are the number of observed elements.
2
Our objective is to impute missing elements of X and Y by performing coupled completion using
our proposed norms. Let W and V be completed tensors that we want to obtain for X and Y 
respectively. To achieve this using ∥W;V∥a

(cid:2)n
′
2
(cid:2)n
′
1

′
1

ccp;((cid:21)1;F)((cid:21)1;F)  we propose a completion model as
∥W;V∥a

ccp;((cid:21)1;F)((cid:21)1;F)

minW;V

and another completion model by using ∥W;V∥a

s:t Ω1(W) = Ω1(X ); Ω2(V) = Ω2(Y);
ccp;((cid:21)1;(cid:21)2;L)((cid:21)3;F) as

min

W (1)+W (2)=W;V

∥W;V∥a

ccp;((cid:21)1;(cid:21)2;L)((cid:21)1;F)

s:t Ω1(W (1) + W (2)) = Ω1(X );

Ω2(V) = Ω2(Y):

4

(4)

(5)

Similarly  we can deﬁne completion models using infV (1)+V (2)=V ∥W;V∥a
infW (1)+W (2)=W;V (1)+V (2)=V ∥W;V∥a
A key advantage with the proposed coupled nuclear norms is that they do not have overlapping group
structures as in (Wimalawarne et al.  2018) and all tensors are regularized separately. This allows
us to use a computationally feasible method such as the Frank-Wolfe optimization (Jaggi  2013) to
solve the proposed completion models. We provide a Frank-Wolfe based optimization method to
solve above completion models in the Section B of the Appendix.

ccp;((cid:21)1;F);((cid:21)2;(cid:21)3;L) and

ccp;((cid:21)1;(cid:21)2;L);((cid:21)3;(cid:21)4;L).

4 Theoretical Analysis

In this section  we analyze excess risk bounds for proposed coupled completion using coupled nu-
clear norms. We consider a partially observed K-mode tensor X 2 Rn(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)n and a partially ob-
′-mode tensor Y 2 Rn(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)n coupled on their ﬁrst modes. Let us consider two sets S
served K
and P  whose elements contain indexes of arbitrary subsets of elements of X and Y  respectively.
Following (Shamir and Shalev-Shwartz  2014)  we split S and P uniformly at random into training
and test sets; the set S as STrain and STest such that S = STtrain [ STest  the set P as PTrain and
PTest such that P = PTtrain [ PTest. Furthermore  following (Shamir and Shalev-Shwartz  2014)
we consider the special case where jSTrainj = jSTestj = jSj=2 and jPTrainj = jPTestj = jPj=2.
To prove excess risk bounds  we recast each coupled nuclear norm as a hypothesis class for each
completion model. Let us again denote W and V as completed tensors we want to learn from X
and Y  respectively. Given the coupled nuclear norm ∥W;V∥a
ccp;((cid:21)b;F)((cid:21)c;F)  we deﬁne a hypothesis
class as W = fW;V : ∥W;V∥a
ccp;((cid:21)b;F)((cid:21)c;F); rank(W) = rank(V) = rg for some regularization
parameter (cid:21)b and (cid:21)c. Using the hypothesis class and a (cid:3)-Lipschitz continuous and bl bounded loss
function l((cid:1);(cid:1)) that measures the difference between the predicted and actual values  we write the
average loss over training sets of a coupled completion model as
LSTrain;PTrain(W;V) :=

[ ∑

l(Xi1;:::;iK ;Wi1;:::;iK )

1

jSTrain [ PTrainj

(i1;:::;iK )2STrain

∑

+

]
l(Yj1;:::;jK′ ;Vj1;:::;jK′ )

;

the

(j1;:::;jK′ )2PTrain

transductive Rademacher

(6)
and the average loss over test sets can be constructed similarly as LSTest;PTest(W;V) by substituting
STrain and PTrain in (6) with STest and PTest  respectively.
By
Shamir and Shalev-Shwartz  2014) 
ity 1 (cid:0) (cid:14) as
LSTest;PTest(W;V)(cid:0) LSTrain;PTrain(W;V) (cid:20) 4RS;P(l◦W; l◦V) + bl
∑
where RS;P(l ◦ W; l ◦ V)  which is expressed as

2007;
the excess risk can be upper bounded with the probabil-

(El-Yaniv and Pechyony 

complexity

√

theory

(

)

; (7)

[

RS;P(l ◦ W; l ◦ V) =

1jS [ PjE(cid:27)

supW;V2W

i1;::;iK

(cid:6)i1;:::;iK l(Xi1;:::;iK ;Wi1;:::;iK ) +
∑

11 + 4

log 1
(cid:14)

√jSTrain [ PTrainj
]

j1;:::;jK′ l(Yj1;:::;jK′ ;Vj1;:::;jK′ )
′

(cid:6)

;

(8)

j1;:::;jK′

′ 2 Rn(cid:2)n(cid:2)(cid:1)(cid:1)(cid:1)n are K-mode and K

where (cid:6) 2 Rn(cid:2)n(cid:2)(cid:1)(cid:1)(cid:1)n and (cid:6)
′-mode tensors  respectively  and
(cid:6)i1;:::;iK = (cid:27)ϕ 2 f(cid:0)1; 1g with probability 0:5 if (i1; ::; iK) 2 S belonging to an index ϕ 2
j1;:::;jK′ = (cid:27)jSj+ϕ′ 2 f(cid:0)1; 1g with probability 0:5 if
1; : : : ;jSj or (cid:6)i1;:::;iK = 0 otherwise  and (cid:6)
(j1; ::; jK′) 2 P belonging to an index ϕ
In the next two theorems  we show the Rademacher complexities for proposed coupled nuclear
norms ∥W;V∥ccp;((cid:21)1;F);((cid:21)1;F) and ∥W;V∥ccp;((cid:21)1;(cid:21)2;L);((cid:21)3;F) (Detailed proof of these theorems are
given in Section C of appendix.)

′ 2 1; : : : ;jPj or (cid:6)

j1;:::;jK′ = 0 otherwise.

′

′

5

Theorem 1. Let us consider ∥W;V∥a
fW;V : ∥W;V∥a
bounded as

ccp;((cid:21)1;F)((cid:21)2;F) and its associated hypothesis class as W =
ccp;((cid:21)1;F)((cid:21)1;F); rank(W) = rank(V) = rg. Then the Rademacher complexity is
]

n(ln n)K(cid:0)1=2

rBW 23K+K

[

p

K

′

RS;P(l ◦ W; l ◦ V) (cid:20) c(cid:3)jS [ Pj

+ rBV 2K+3K

K

n(ln n)K

′(cid:0)1=2

:

′p

′

where (cid:13)1 (cid:20) BW and (cid:23)1 (cid:20) BV of (2) and c is a constant.
Theorem 2. Let us consider ∥W;V∥a
fW (1);W (2);V :
r1; rank(W (2)) = r2g. Then the Rademacher complexity is bounded as
RS;P(l ◦ W; l ◦ V) (cid:20) c(cid:3)jS [ Pj

ccp;((cid:21)1;(cid:21)2;L)((cid:21)3;F) and its hypothesis class as W =
ccp;((cid:21)1;(cid:21)2;L)((cid:21)3;F); rank(W (1)) = rank(V) =
]

infW (1)+W (2)=W ∥W;V∥a

(r1BW1 + r2BW2)23K+K

[

p

K

′

n(ln n)K(cid:0)1=2
′p

′

+ r2BV 2K+3K

K

n(ln n)K

′(cid:0)1=2

:

1

1

p

p

nK(cid:0)1 +

p
r′K[

(cid:20) BW1  (cid:13)(2)

(cid:20) BW2  (cid:23)1 (cid:20) BV of (3) and c is a constant.

where (cid:13)(1)
The Rademacher complexities in Theorems 1 and 2 show that for K (cid:21) K
′ and r = r1 = r2  excess
n(ln n)K(cid:0)1=2). The excess risk bound for the coupled norm in
risks is bounded by O(r24KK
′
p
p
(Wimalawarne et al.  2018) for the coupled tensors with multilinear rank (r
) are bounded by
; : : : ; r
n(ln n)K(cid:0)1=2 compared
O(
nK(cid:0)1  coupled nuclear norms
to coupled norms (Wimalawarne et al.  2018) that are bounded by
can lead to lower excess risk when coupled tensors have large dimensions (n and K are large).
Though CP rank and multilinear rank cannot be compared directly  when the CP rank is smaller than
the mode dimensions (r < n) our theoretical analysis shows that coupled nuclear norms are more
capable of better performance compared to multilinear rank based coupled norms. Additionally  the
Rademacher complexity is divided by the total number of observed samples from both the coupled
tensors (jS[Pj) leading to a lower Rademacher complexity compared to separate tensor completions.

n]). Since coupled nuclear norms are bounded by

p

′

5 Experiments

We carried out several simulations and real world data experiments to evaluate empirical perfor-
mances of our proposed methods.

5.1 Simulation Experiments

We designed simulation experiments using coupled tensors using both the CP rank and the multilin-
ear rank. For each simulation  we created a tensor T 2 R20(cid:2)20(cid:2)20 and a matrix M 2 R20(cid:2)30 with
∑
speciﬁed ranks and coupled them on their ﬁrst modes (without losing generality) by sharing a certain
amount of singular components along the ﬁrst mode. We chose these dimensions of coupled tensors
in accordance with simulation experiments in (Wimalawarne et al.  2018) for easier comparison. In
order to create a tensor T with CP rank of r  we generated the tensor as T =
a=1 (cid:16)aua (cid:10) va (cid:10) wa
where ua 2 R20  va 2 R20  and wa 2 R20 are randomly generated unit vectors and (cid:16)a 2 R.
To create T with multilinear rank of (r1; r2; r3)  we generated orthogonal matrices U 2 R20(cid:2)r1 
V 2 R20(cid:2)r2  and W 2 R20(cid:2)r3  and a core tensor C 2 Rr1(cid:2)r2(cid:2)r3 with elements randomly sampled
from a Normal distribution and compute T = C (cid:2)1 U (cid:2)2 V (cid:2)3 W . We also created a rank R
⊤ with orthogonal matrices X 2 R20(cid:2)R and Y 2 RR(cid:2)30   and a diagonal matrix
matrix M = XSY
S 2 RR(cid:2)R randomly generated from R+. We coupled the T and M by sharing r
′ components be-
′
′
tween them as X(:; 1 : r
) = [u1; : : : ; ur′] for CP rank based tensors and X(:; 1 : r
)
) = U (:; 1 : r
for multilinear rank based tensors. In order to generate datasets for simulations  we selected training
sets of 30  50  and 70 percentages from total number of elements of the tensor and the matrix  10
percent as validation sets and the rest as test sets. For each simulation we repeated experiments with
10 random selections.

′

r

6

(a) Matrix Completion (M)

(b) Tensor Completion (T )
Figure 1: Performances of completion of the tensor with dimensions of 20 (cid:2) 20 (cid:2) 20 and CP rank
of 5 and matrix with dimensions of 20 (cid:2) 30 and rank of 5 both sharing 5 components.

proposed

coupled

1

norms

experimented with

∥T ; M∥ccp;((cid:21)1;F);((cid:21)1;F) 
We
nuclear
∥T ; M∥ccp;((cid:21)2;(cid:21)3;L);((cid:21)2;F)  and ∥T ; M∥ccp;((cid:21)4;F);((cid:21)4;(cid:21)5;L).
For visual convenience in ﬁg-
ures  we use shortened names for ∥T ; M∥ccp;((cid:21)1;F);((cid:21)1;F)  ∥T ; M∥ccp;((cid:21)2;(cid:21)3;L);((cid:21)2;F)  and
∥T ; M∥ccp;((cid:21)4;F);((cid:21)4;(cid:21)5;L) as ccp-1  ccp-2  and ccp-3  respectively. For all these norms  we used
the regularization parameters (cid:21)1; : : : ; (cid:21)5 in the range from 0:01 to 50 with intervals of 1. As
baseline methods  we performed completion of each individual tensor using the overlapped trace
norm (OTN) and the scaled latent trace norm (SLTN) and individual matrix completion using the
matrix trace norm (MTN). We also used the tensor nuclear norm as a baseline method to evaluate
individual tensor completion. Additionally  we performed coupled completion with coupled norms
(Wimalawarne et al.  2018). However  due to the difﬁculty in plotting all the norms in a single graph
only the result from the best coupled norm is plotted. For all the baseline methods  we selected the
optimal regularization parameters from the range of 0:01 to 5 in divisions of 0:025.
For our ﬁrst experiment we created T by specifying a CP rank of 5 and M with rank of 5. We
coupled T and M by sharing all components on their ﬁrst modes. Figure 1 shows that coupled
nuclear norms have outperformed individual completion of the tensor and the matrix  as well as
coupled completion by the coupled norm (O; O; O).
Next  we give a simulation experiment with coupled tensor using multilinear ranks. We constructed
T with multilinear rank of (5; 5; 5) and M with rank of 5 and shared all components on the
ﬁrst mode. Figure 2 shows that the proposed coupled nuclear norms ∥T ; M∥ccp;((cid:21)1;F);((cid:21)1;F) and
∥T ; M∥ccp;((cid:21)2;(cid:21)3;L);((cid:21)2;F) have outperformed (O; O; O) for both tensor and matrix completion in-
dicating that proposed norms are versatile for coupled tensors with multilinear ranks.

(a) Matrix Completion (M)

(b) Tensor Completion (T )

Figure 2: Performances of completion of the tensor with dimensions of 20(cid:2) 20(cid:2) 20 and multilinear
rank of (5; 5; 5) and matrix with dimensions of 20 (cid:2) 30 and rank of 5 both sharing 5 components.

In all the above experiments  coupled nuclear norms have performed comparable or better than
individual tensor and matrix completion. We give further simulation experiments in Section D of
the Appendix.

1Code and data are available at http://kishan-wimalawarne.com/onewebmedia/NeurIPS_2018_code.rar

7

0.20.30.40.50.60.70.8Fraction of training samples00.020.040.060.080.1MSEMTN(O O O)ccp-1ccp-2ccp-30.20.30.40.50.60.70.8Fraction of training samples0.010.0120.0140.0160.0180.020.0220.024MSEOTNSLTNTNN(O O O)ccp-1ccp-2ccp-30.20.30.40.50.60.70.8Fraction of training samples00.10.20.30.40.5MSEMTN(O O O)ccp-1ccp-2ccp-30.20.30.40.50.60.70.8Fraction of training samples0.010.020.030.040.050.060.070.08MSEOTNSLTNTNN(O O O)ccp-1ccp-2ccp-35.2 Real Data Experiments

We used the UCLAF dataset as our real data experiment.

5.2.1 UCLAF Dataset

The UCLAF dataset (Zheng et al.  2010) is a commonly used benchmark dataset for coupled tensor
completion (Ermis et al.  2015; Wimalawarne et al.  2018). The UCLAF dataset contains GPS data
collected from 164 users in 168 locations performing 5 activities. These GPS data forms a user-
location-activity tensor T 2 R164(cid:2)168(cid:2)5 consisting of only a few observed elements. In order to
learn the unobserved elements  tensor completion can be performed. However  the UCLAF dataset
also contains side information that can be coupled to the tensor to improve the completion procedure.
Similar to (Wimalawarne et al.  2018)  we used the coupling of T with the user-location matrix
X 2 R164(cid:2)168. We used the same random data selection and validation processes as simulation
experiments.

Apart from the coupled nuclear norms  we
experimented with the same baseline meth-
ods for tensors as in the previous section.
For these experiments  we selected regular-
ization parameters from logarithmic linear
scale from 0:01 to 5000 with 200 divisions.
Additionally  we compared our results with
the SDF model (Sorber et al.  2015) by using
a CP rank of 2.
Figure 3 shows that
the coupled nuclear
norm ∥ (cid:1) ∥ccp;((cid:21);F);((cid:21);F) (ccp-1) gives the
best performance.
The coupled norm
(S; O; O) which has given the best perfor-
mance among multilinear rank based cou-
pled norms (Wimalawarne et al.  2018) is
outperformed by all
the coupled nuclear
norms.

Figure 3: Performances on the UCLAF data set

Both simulation and UCLAF data experiments indicate that coupled nuclear norms lead to better
performance compared to existing coupled norms (Wimalawarne et al.  2018).

6 Acknowledgment

This work has been partially supported by MEXT KAKENHI Grant Number 16H02868  Grant Num-
ber JPMJAC1503 ACCEL JST  FiDiPro Tekes (currently Business Finland) and AIPSE Academy of
Finland.

7 Conclusion and Future Work

We introduce coupled nuclear norms by integrating the CP rank into coupled norms. We propose
new coupled completion models regularized by coupled nuclear norms and discuss optimization
procedures to solve them. Our excess risk bounds for coupled completion show that the proposed
norms lead to better performances compared to existing multilinear rank based coupled norms. Our
theoretical analysis is validated through simulation and real world data experiments  where we show
that coupled nuclear norms can give better performance compared to existing methods. We believe
that the proposed coupled nuclear norms should be further investigated to be widely applicable in
real world problems.
Applying coupled nuclear norms to solve large scale problems is an important future research di-
rection. More speciﬁcally  developing computationally feasible optimization methods is important
since computing the coupled nuclear norms can be computationally costly. Future research in this
direction can consider developing globally optimal power methods (Anandkumar et al.  2017) to ap-
proximate coupled nuclear norms. Furthermore  theoretical analysis of coupled nuclear norms with
more than two tensors is another important future research direction.

8

0.20.30.40.50.60.70.8Fraction of training samples0.511.52MSEOTNSLTN(S O O)TNNSDFccp-1ccp-2ccp-3References
Acar  E.  Papalexakis  E. E.  Gürdeniz  G.  Rasmussen  M. A.  Lawaetz  A. J.  Nilsson  M.  and Bro  R. (2014).

Structure-revealing data fusion. BMC Bioinformatics  15:239.

Anandkumar  A.  Ge  R.  and Janzamin  M. (2017). Analyzing tensor power method dynamics in overcomplete

regime. Journal of Machine Learning Research  18:22:1–22:40.

Bouchard  G.  Yin  D.  and Guo  S. (2013). Convex collective matrix factorization. In AISTATS  volume 31 of

JMLR Workshop and Conference Proceedings  pages 144–152. JMLR.org.

Carroll  J. D. and Chang  J.-J. (1970). Analysis of individual differences in multidimensional scaling via an

n-way generalization of “eckart-young” decomposition. Psychometrika  35(3):283–319.

El-Yaniv  R. and Pechyony  D. (2007). Transductive rademacher complexity and its applications. In Learning

Theory  volume 4539  pages 157–171.

Ermis  B.  Acar  E.  and Cemgil  A. T. (2015). Link prediction in heterogeneous data via generalized coupled

tensor factorization. Data Mining and Knowledge Discovery  29(1):203–236.

Harshman  R. A. (1970). Foundations of the PARAFAC procedure: models and conditions for an explanatory

multimodal factor analysis. UCLA Working Papers in Phonetics  16:1–84.

Hitchcock  F. L. (1927). The expression of a tensor or a polyadic as a sum of products. J. Math. Phys  6(1):164–

189.

Jaggi  M. (2013). Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML  volume 28 

pages 427–435. PMLR.

Kolda  T. G. and Bader  B. W. (2009). Tensor Decompositions and Applications. SIAM Review  51(3):455–500.

Ledoux  M. (2001). The Concentration of Measure Phenomenon. Mathematical surveys and monographs.

American Mathematical Society.

Li  C.  Zhao  Q.  Li  J.  Cichocki  A.  and Guo  L. (2015). Multi-tensor completion with common structures. In

AAAI  pages 2743–2749.

Lim  L. and Comon  P. (2014). Blind multilinear identiﬁcation. IEEE Trans. Information Theory  60(2):1260–

1280.

Liu  J.  Musialski  P.  Wonka  P.  and Ye  J. (2013). Tensor completion for estimating missing values in visual

data. IEEE Trans. Pattern Anal. Mach. Intell.  35(1):208–220.

Nguyen  N. H.  Drineas  P.  and Tran  T. D. (2015). Tensor sparsiﬁcation via a bound on the spectral norm of

random tensors. Information and Inference: A Journal of the IMA  4(3):195–229.

Shamir  O. and Shalev-Shwartz  S. (2014). Matrix completion with the trace norm: Learning  bounding  and

transducing. Journal of Machine Learning Research  15:3401–3423.

Sorber  L.  Barel  M. V.  and Lathauwer  L. D. (2015). Structured data fusion. IEEE Journal of Selected Topics

in Signal Processing  9(4):586–600.

Tomioka  R. and Suzuki  T. (2013). Convex tensor decomposition via structured schatten norm regularization.

In NIPS.

Vershynin  R. (2011). Spectral norm of products of random and deterministic matrices. Probability Theory and

Related Fields  150(3):471–509.

Wimalawarne  K.  Sugiyama  M.  and Tomioka  R. (2014). Multitask learning meets tensor factorization: task

imputation via convex optimization. In NIPS.

Wimalawarne  K.  Yamada  M.  and Mamitsuka  H. (2018). Convex coupled matrix and tensor completion.

Neural Computation  30(11):1–33.

Yang  Y.  Feng  Y.  and Suykens  J. A. K. (2015). A rank-one tensor updating algorithm for tensor completion.

IEEE Signal Processing Letters  22(10):1633–1637.

Yuan  M. and Zhang  C.-H. (2016). On tensor completion via nuclear norm minimization. Foundations of

Computational Mathematics  16(4):1031–1068.

Zheng  V. W.  Cao  B.  Zheng  Y.  Xie  X.  and Yang  Q. (2010). Collaborative ﬁltering meets mobile recom-

mendation: A user-centered approach. In AAAI.

9

,Kishan Wimalawarne
Hiroshi Mamitsuka