2019,Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction,Off-policy reinforcement learning aims to leverage experience collected from prior policies for sample-efficient learning. However  in practice  commonly used off-policy approximate dynamic programming methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution  and can make only limited progress without collecting additional on-policy data. As a step towards more robust off-policy algorithms  we study the setting where the off-policy experience is fixed and there is no further interaction with the environment. We identify \emph{bootstrapping error} as a key source of instability in current methods. Bootstrapping error is due to bootstrapping from actions that lie outside of the training data distribution  and it accumulates via the Bellman backup operator. We theoretically analyze bootstrapping error  and demonstrate how carefully constraining action selection in the backup can mitigate it. Based on our analysis  we propose a practical algorithm  bootstrapping error accumulation reduction (BEAR). We demonstrate that BEAR is able to learn robustly from different off-policy distributions  including random data and suboptimal demonstrations  on a range of continuous control tasks.,Stabilizing Off-Policy Q-Learning via Bootstrapping

Error Reduction

Aviral Kumar∗
UC Berkeley

aviralk@berkeley.edu

Justin Fu∗
UC Berkeley

justinjfu@eecs.berkeley.edu

George Tucker
Google Brain

gjt@google.com

Sergey Levine

UC Berkeley  Google Brain

svlevine@eecs.berkeley.edu

Abstract

Off-policy reinforcement learning aims to leverage experience collected from
prior policies for sample-efﬁcient learning. However  in practice  commonly used
off-policy approximate dynamic programming methods based on Q-learning and
actor-critic methods are highly sensitive to the data distribution  and can make only
limited progress without collecting additional on-policy data. As a step towards
more robust off-policy algorithms  we study the setting where the off-policy experi-
ence is ﬁxed and there is no further interaction with the environment. We identify
bootstrapping error as a key source of instability in current methods. Bootstrap-
ping error is due to bootstrapping from actions that lie outside of the training data
distribution  and it accumulates via the Bellman backup operator. We theoretically
analyze bootstrapping error  and demonstrate how carefully constraining action se-
lection in the backup can mitigate it. Based on our analysis  we propose a practical
algorithm  bootstrapping error accumulation reduction (BEAR). We demonstrate
that BEAR is able to learn robustly from different off-policy distributions  including
random and suboptimal demonstrations  on a range of continuous control tasks.

1

Introduction

One of the primary drivers of the success of machine learning methods in open-world perception
settings  such as computer vision [19] and NLP [8]  has been the ability of high-capacity function
approximators  such as deep neural networks  to learn generalizable models from large amounts of
data. Reinforcement learning (RL) has proven comparatively difﬁcult to scale to unstructured real-
world settings because most RL algorithms require active data collection. As a result  RL algorithms
can learn complex behaviors in simulation  where data collection is straightforward  but real-world
performance is limited by the expense of active data collection. In some domains  such as autonomous
driving [38] and recommender systems [3]  previously collected datasets are plentiful. Algorithms
that can utilize such datasets effectively would not only make real-world RL more practical  but also
would enable substantially better generalization by incorporating diverse prior experience.
In principle  off-policy RL algorithms can leverage this data; however  in practice  off-policy al-
gorithms are limited in their ability to learn entirely from off-policy data. Recent off-policy RL
methods (e.g.  [18  29  23  9]) have demonstrated sample-efﬁcient performance on complex tasks
in robotics [23] and simulated environments [36]. However  these methods can still fail to learn
when presented with arbitrary off-policy data without the opportunity to collect more experience

∗Equal Contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

from the environment. This issue persists even when the off-policy data comes from effective expert
policies  which in principle should address any exploration challenge [6  12  11]. This sensitivity to
the training data distribution is a limitation of practical off-policy RL algorithms  and one would hope
that an off-policy algorithm should be able to learn reasonable policies through training on static
datasets before being deployed in the real world.
In this paper  we aim to develop off-policy  value-based RL methods that can learn from large 
static datasets. As we show  a crucial challenge in applying value-based methods to off-policy
scenarios arises in the bootstrapping process employed when Q-functions are evaluated on out of
out-of-distribution action inputs for computing the backup when training from off-policy data. This
may introduce errors in the Q-function and the algorithm is unable to collect new data in order to
remedy those errors  making training unstable and potentially diverging. Our primary contribution is
an analysis of error accumulation in the bootstrapping process due to out-of-distribution inputs and a
practical way of addressing this error. First  we formalize and analyze the reasons for instability and
poor performance when learning from off-policy data. We show that  through careful action selection 
error propagation through the Q-function can be mitigated. We then propose a principled algorithm
called bootstrapping error accumulation reduction (BEAR) to control bootstrapping error in practice 
which uses the notion of support-set matching to prevent error accumulation. Through systematic
experiments  we show the effectiveness of our method on continuous-control MuJoCo tasks  with
a variety of off-policy datasets: generated by a random  suboptimal  or optimal policies. BEAR is
consistently robust to the training dataset  matching or exceeding the state-of-the-art in all cases 
whereas existing algorithms only perform well for speciﬁc datasets.
2 Related Work
In this work  we study off-policy reinforcement learning with static datasets. Errors arising from
inadequate sampling  distributional shift  and function approximation have been rigorously studied as
“error propagation” in approximate dynamic programming (ADP) [4  27  10  33]. These works often
study how Bellman errors accumulate and propagate to nearby states via bootstrapping. In this work 
we build upon tools from this analysis to show that performing Bellman backups on static datasets
leads to error accumulation due to out-of-distribution values. Our approach is motivated as reducing
the rate of propagation of error propagation between states.
Our approach constrains actor updates so that the actions remain in the support of the training dataset
distribution. Several works have explored similar ideas in the context of off-policy learning learning
in online settings. Kakade and Langford [22] shows that large policy updates can be destructive  and
propose a conservative policy iteration scheme which constrains actor updates to be small for provably
convergent learning. Grau-Moya et al. [16] use a learned prior over actions in the maximum entropy
RL framework [25] and justify it as a regularizer based on mutual information. However  none of these
methods use static datasets. Importance Sampling based distribution re-weighting [29  15  30  26]
has also been explored primarily in the context of off-policy policy evaluation.
Most closely related to our work is batch-constrained Q-learning (BCQ) [12] and SPIBB [24]  which
also discuss instability arising from previously unseen actions. Fujimoto et al. [12] show convergence
properties of an action-constrained Bellman backup operator in tabular  error-free settings. We prove
stronger results under approximation errors and provide a bound on the suboptimality of the solution.
This is crucial as it drives the design choices for a practical algorithm. As a consequence  although we
experimentally ﬁnd that [12] outperforms standard Q-learning methods when the off-policy data is
collected by an expert  BEAR outperforms [12] when the off-policy data is collected by a suboptimal
policy  as is common in real-life applications. Empirically  we ﬁnd BEAR achieves stronger and
more consistent results than BCQ across a wide variety of datasets and environments. As we explain
below  the BCQ constraint is too aggressive; BCQ generally fails to substantially improve over the
behavior policy  while our method actually improves when the data collection policy is suboptimal
or random. SPIBB [24]  like BEAR  is an algorithm based on constraining the learned policy to
the support of a behavior policy. However  the authors do not extend safe performance guarantees
from the batch-constrained case to the relaxed support-constrained case  and do not evaluate on
high-dimensional control tasks.

3 Background
We represent
(S A  P  R  ρ0  γ)  where S is the state space  A is the action space  P (s(cid:48)

the environment as a Markov decision process (MDP) deﬁned by a tuple
|s  a) is the transition

2

state marginal of a policy π  deﬁned as the average state visited by the policy (cid:80)∞

distribution  ρ0(s) is the initial state distribution  R(s  a) is the reward function  and γ ∈ (0  1) is the
discount factor. The goal in RL is to ﬁnd a policy π(a|s) that maximizes the expected cumulative
discounted rewards which is also known as the return. The notation µπ(s) denotes the discounted
π(s). P π is
shorthand for the transition matrix from s to s(cid:48) following a certain policy π  p(s(cid:48)
|s  a)].
Q-learning learns the optimal state-action value function Q∗(s  a)  which represents the expected
cumulative discounted reward starting in s taking action a and then acting optimally thereafter. The
optimal policy can be recovered from Q∗ by choosing the maximizing action. Q-learning algorithms
are based on iterating the Bellman optimality operator T   deﬁned as

t=0 γtpt
|s) = Eπ[p(s(cid:48)

(T ˆQ)(s  a) := R(s  a) + γET (s(cid:48)|s a)[max

a(cid:48)

ˆQ(s(cid:48)  a(cid:48))].

When the state space is large  we represent ˆQ as a hypothesis from the set of function approximators
Q (e.g.  neural networks). In theory  the estimate of the Q-function is updated by projecting T ˆQ into
Q (i.e.  minimizing the mean squared Bellman error Eν[(Q − T ˆQ)2]  where ν is the state occupancy
measure under the behaviour policy). This is also referred to a Q-iteration. In practice  an empirical
estimate of T ˆQ is formed with samples  and treated as a supervised (cid:96)2 regression target to form the
next approximate Q-function iterate. In large action spaces (e.g.  continuous)  the maximization
maxa(cid:48) Q(s(cid:48)  a(cid:48)) is generally intractable. Actor-critic methods [35  13  18] address this by additionally
learning a policy πθ that maximizes the Q-function. In this work  we study off-policy learning from a
static dataset of transitions D = {(s  a  s(cid:48)  R(s  a))}  collected under an unknown behavior policy
β(·|s). We denote the distribution over states and actions induced by β as µ(s  a).
4 Out-of-Distribution Actions in Q-Learning

Figure 1: Performance of SAC on HalfCheetah-v2
(return (left) and log Q-values (right)) with off-policy
expert data w.r.t. number of training samples (n). Note
the large discrepancy between returns (which are nega-
tive) and log Q-values (which have large positive values) 
which is not solved with additional samples.

Q-learning methods often fail to learn on static 
off-policy data  as shown in Figure 1. At ﬁrst
glance  this resembles overﬁtting  but increasing
the size of the static dataset does not rectify the
problem  suggesting the issue is more complex.
We can understand the source of this instability
by examining the form of the Bellman backup.
Although minimizing the mean squared Bell-
man error corresponds to a supervised regression
problem  the targets for this regression are them-
selves derived from the current Q-function esti-
mate. The targets are calculated by maximizing
the learned Q-values with respect to the action
at the next state. However  the Q-function esti-
mator is only reliable on inputs from the same
distribution as its training set. As a result  naïvely maximizing the value may evaluate the ˆQ estimator
on actions that lie far outside of the training distribution  resulting in pathological values that incur
large error. We refer to these actions as out-of-distribution (OOD) actions.
Formally  let ζk(s  a) = |Qk(s  a) − Q∗(s  a)| denote the total error at iteration k of Q-learning 
and let δk(s  a) = |Qk(s  a) − T Qk−1(s  a)| denote the current Bellman error. Then  we have
ζk(s  a) ≤ δk(s  a) + γ maxa(cid:48) Es(cid:48)[ζk−1(s(cid:48)  a(cid:48))]. In other words  errors from (s(cid:48)  a(cid:48)) are discounted 
then accumulated with new errors δk(s  a) from the current iteration. We expect δk(s  a) to be high
on OOD states and actions  as errors at these state-actions are never directly minimized while training.
To mitigate bootstrapping error  we can restrict the policy to ensure that it output actions that lie in
the support of the training distribution. This is distinct from previous work (e.g.  BCQ [12]) which
implicitly constrains the distribution of the learned policy to be close to the behavior policy  similarly
to behavioral cloning [31]. While this is sufﬁcient to ensure that actions lie in the training set with
high probability  it is overly restrictive. For example  if the behavior policy is close to uniform  the
learned policy will behave randomly  resulting in poor performance  even when the data is sufﬁcient
to learn a strong policy (see Figure 2 for an illustration). Formally  this means that a learned policy
π(a|s) has positive density only where the density of the behaviour policy β(a|s) is more than a
threshold (i.e.  ∀a  β(a|s) ≤ ε =⇒ π(a|s) = 0)  instead of a closeness constraint on the value of

3

0.0K0.2K0.4K0.6K0.8K1.0KTrainSteps−1000−750−500−25002505007501000HalfCheetah-v2:AverageReturnn=1000n=10000n=100000n=10000000.0K0.2K0.4K0.6K0.8K1.0KTrainSteps051015202530HalfCheetah-v2:log(Q)n=1000n=10000n=100000n=1000000the density π(a|s) and β(a|s). Our analysis instead reveals a tradeoff between staying within the
data distribution and ﬁnding a suboptimal solution when the constraint is too restrictive. Our analysis
motivates us to restrict the support of the learned policy  but not the probabilities of the actions lying
within the support. This avoids evaluating the Q-function estimator on OOD actions  but remains
ﬂexible in order to ﬁnd a performant policy. Our proposed algorithm leverages this insight.

4.1 Distribution-Constrained Backups

In this section  we deﬁne and analyze a backup operator that restricts the set of policies used in the
maximization of the Q-function  and we derive performance bounds which depend on the restricted
set. This provides motivation for constraining policy support to the data distribution. We begin with
the deﬁnition of a distribution-constrained operator:
Deﬁnition 4.1 (Distribution-constrained operators). Given a set of policies Π  the distribution-
constrained backup operator is deﬁned as:

T ΠQ(s  a) def= E(cid:2)R(s  a) + γ max

π∈Π

EP (s(cid:48)|s a) [V (s(cid:48))](cid:3)

V (s) def= max
π∈Π

Eπ[Q(s  a)] .

This backup operator satisﬁes properties of the standard Bellman backup  such as convergence to a
ﬁxed point  as discussed in Appendix A. To analyze the (sub)optimality of performing this backup
under approximation error  we ﬁrst quantify two sources of error. The ﬁrst is a suboptimality bias.
The optimal policy may lie outside the policy constraint set  and thus a suboptimal solution will be
found. The second arises from distribution shift between the training distribution and the policies
used for backups. This formalizes the notion of OOD actions. To capture suboptimality in the ﬁnal
solution  we deﬁne a suboptimality constant  which measures how far π∗ is from Π.
Deﬁnition 4.2 (Suboptimality constant). The suboptimality constant is deﬁned as:

α(Π) = max

s a |T ΠQ∗(s  a) − T Q∗(s  a)|.

Next  we deﬁne a concentrability coefﬁcient [28]  which quantiﬁes how far the visitation distribution
generated by policies from Π is from the training data distribution. This constant captures the degree
to which states and actions are out of distribution.
Assumption 4.1 (Concentrability). Let ρ0 denote the initial state distribution  and µ(s  a) denote
the distribution of the training data over S × A  with marginal µ(s) over S. Suppose there exist
coefﬁcients c(k) such that for any π1  ...πk ∈ Π and s ∈ S:

where P πi is the transition operator on states induced by πi. Then  deﬁne the concentrability
coefﬁcient C(Π) as

To provide some intuition for C(Π)  if µ was generated by a single policy π  and Π = {π} was a
singleton set  then we would have C(Π) = 1  which is the smallest possible value. However  if Π
contained policies far from π  the value could be large  potentially inﬁnite if the support of Π is not
contained in π. Now  we bound the performance of approximate distribution-constrained Q-iteration:
Theorem 4.1. Suppose we run approximate distribution-constrained value iteration with a set
constrained backup T Π. Assume that δ(s  a) ≥ maxk |Qk(s  a) − T ΠQk−1(s  a)| bounds the
Bellman error. Then 

(cid:21)

lim
k→∞

Eρ0[|V πk (s) − V ∗(s)|] ≤

γ

(1 − γ)2

Proof. See Appendix B  Theorem B.1

C(Π)Eµ[max
π∈Π

Eπ[δ(s  a)]] +

1 − γ
γ

α(Π)

This bound formalizes the tradeoff between keeping policies chosen during backups close to the data
(captured by C(Π)) and keeping the set Π large enough to capture well-performing policies (captured
by α(Π)). When we expand the set of policies Π  we are increasing C(Π) but decreasing α(Π). An
example of this tradeoff  and how a careful choice of Π can yield superior results  is given in a tabular

4

ρ0P π1P π2...P πk (s) ≤ c(k)µ(s) 

C(Π) def= (1 − γ)2

kγk−1c(k).

∞(cid:88)

k=1

(cid:20)

Figure 2: Visualized error propagation in Q-learning for various choices of the constraint set Π: unconstrained
(top row) distribution-constrained (middle)  and constrained to the behaviour policy (policy-evaluation  bottom).
Triangles represent Q-values for actions that move in different directions. The task (left) is to reach the bottom-
left corner (G) from the top-left (S)  but the behaviour policy (visualized as arrows in the task image  support
state-action pairs are shown in black on the support set image) travels to the bottom-right with a small amount of
-greedy exploration. Dark values indicate high error  and light values indicate low error. Standard backups
propagate large errors from the low-support regions into the high-support regions  leading to high error. Policy
evaluation reduces error propagation from low-support regions  but introduces signiﬁcant suboptimality bias  as
the data policy is not optimal. A carefully chosen distribution-constrained backup strikes a balance between these
two extremes  by conﬁning error propagation in the low-support region while introducing minimal suboptimality
bias.

(cid:16)

γ

1 +

(cid:17)

gridworld example in Fig. 2  where we visualize errors accumulated during distribution-constrained
Q-iteration for different choices of Π.
Finally  we motivate the use of support sets to construct Π. We are interested in the case where
Π = {π | π(a|s) = 0 whenever β(a|s) < }  where β is the behavior policy (i.e.  Π is the set of
policies that have support in the probable regions of the behavior policy). Deﬁning Π in this way
allows us to bound the concentrability coefﬁcient:
Theorem 4.2. Assume the data distribution µ is generated by a behavior policy β. Let µ(s)
be the marginal state distribution under the data distribution. Deﬁne Π = {π | π(a|s) =
0 whenever β(a|s) < } and let µΠ be the highest discounted marginal state distribution start-
ing from the initial state distribution ρ and following policies π ∈ Π at each time step thereafter.
Then  there exists a concentrability coefﬁcient C(Π) which is bounded:

(1 − )

(1 − γ)f ()

C(Π) ≤ C(β) ·
where f () def= mins∈S µΠ (s)>0[µ(s)] > 0.
Proof. See Appendix B  Theorem B.2
Qualitatively  f () is the minimum discounted visitation marginal of a state under the behaviour policy
if only actions which are more than  likely are executed in the environment. Thus  using support sets
gives us a single lever    which simultaneously trades off the value of C(Π) and α(Π). Not only can
we provide theoretical guarantees  we will see in our experiments (Sec. 6) that constructing Π in this
way provides a simple and effective method for implementing distribution-constrained algorithms.
Intuitively  this means we can prevent an increase in overall error in the Q-estimate by selecting
policies supported on the support of the training action distribution  which would ensure roughly
bounded projection error δk(s  a) while reducing the suboptimality bias  potentially by a large amount.
Bounded error δk(s  a) on the support set of the training distribution is a reasonable assumption when
using highly expressive function approximators  such as deep networks  especially if we are willing
to reweight the transition set [32  11]. We further elaborate on this point in Appendix C.

5 Bootstrapping Error Accumulation Reduction (BEAR)
We now propose a practical actor-critic algorithm (built on the framework of TD3 [13] or SAC [18])
that uses distribution-constrained backups to reduce accumulation of bootstrapping error. The key
insight is that we can search for a policy with the same support as the training distribution  while

5

preventing accidental error accumulation. Our algorithm has two main components. Analogous
to BCQ [13]  we use K Q-functions and use the minimum Q-value for policy improvement  and
design a constraint which will be used for searching over the set of policies Π  which share the
same support as the behaviour policy. Both of these components will appear as modiﬁcations of the
policy improvement step in actor-critic style algorithms. We also note that policy improvement can
be performed with the mean of the K Q-functions  and we found that this scheme works as good in
our experiments.
We denote the set of Q-functions as: ˆQ1 ···   ˆQK. Then  the policy is updated to maximize the
conservative estimate of the Q-values within Π:

πφ(s) := max
π∈Π

Ea∼π(·|s)

min

j=1 .. K

ˆQj(s  a)

(cid:20)

(cid:21)

(cid:20)

(cid:21)

In practice  the behaviour policy β is unknown  so we need an approximate way to constrain π to Π.
We deﬁne a differentiable constraint that approximately constrains π to Π  and then approximately
solve the constrained optimization problem via dual gradient descent. We use the sampled version
of maximum mean discrepancy (MMD) [17] between the unknown behaviour policy β and the
actor π because it can be estimated based solely on samples from the distributions. Given samples
(cid:88)
x1 ···   xn ∼ P and y1 ···   ym ∼ Q  the sampled MMD between P and Q is given by:
MMD2({x1 ···   xn} {y1 ···   ym}) =
Here  k(· ·) is any universal kernel. In our experiments  we ﬁnd both Laplacian and Gaussian kernels
work well. The expression for MMD does not involve the density of either distribution and it can be
optimized directly through samples. Empirically we ﬁnd that  in the low-intermediate sample regime 
the sampled MMD between P and Q is similar to the MMD between a uniform distribution over P ’s
support and Q  which makes MMD roughly suited for constraining distributions to a given support
set. (See Appendix C.3 for numerical simulations justifying this approach).
Putting everything together  the optimization problem in the policy improvement step is

k(xi  xi(cid:48))−

(cid:88)

(cid:88)

k(xi  yj)+

k(yj  yj(cid:48)).

2
nm

1
m2

1
n2

j j(cid:48)

i i(cid:48)

i j

πφ := max
π∈∆|S|

Es∼DEa∼π(·|s)

min

j=1 .. K

ˆQj(s  a)

s.t. Es∼D[MMD(D(s)  π(·|s))] ≤ ε

(1)

where ε is an approximately chosen threshold. We choose a threshold of ε = 0.05 in our experiments.
The algorithm is summarized in Algorithm 1.
How does BEAR connect with distribution-constrained backups described in Section 4.1? Step 5 of
the algorithm restricts πφ to lie in the support of β. This insight is formally justiﬁed in Theorems 4.1
& 4.2 (C(Πε) is bounded). Computing distribution-constrained backup exactly by maximizing over
π ∈ Πε is intractable in practice. As an approximation  we sample Dirac policies in the support of β
(Alg 1  Line 5) and perform empirical maximization to compute the backup. As the maximization
is performed over a narrower set of Dirac policies ({δai} ⊆ Πε)  the bound in Theorem 4.1 still
holds. Empirically  we show in Section 6 that this approximation is sufﬁcient to outperform previous
methods. This connection is brieﬂy discussed in Appendix C.2.
Algorithm 1 BEAR Q-Learning (BEAR-QL)
input : Dataset D  target network update rate τ  mini-batch size N  sampled actions for MMD n  minimum λ
1: Initialize Q-ensemble {Qθi}K
}K
i=1  and a target
i ← θi

i=1  actor πφ  Lagrange multiplier α  target networks {Qθ(cid:48)

actor πφ(cid:48)  with φ(cid:48) ← φ  θ(cid:48)

i

2: for t in {1  . . .   N} do
3:

Sample mini-batch of transitions (s  a  r  s(cid:48)) ∼ D
Q-update:
Sample p action samples  {ai ∼ πφ(cid:48) (·|s(cid:48))}p
Deﬁne y(s  a) := maxai [λ minj=1 .. K Qθ(cid:48)
∀i  θi ← arg minθi (Qθi (s  a) − (r + γy(s  a)))2
Policy-update:
Sample actions {ˆai ∼ πφ(·|s)}m
Update φ  α by minimizing Equation 1 by using dual gradient descent with Lagrange multiplier α
Update Target Networks: θ(cid:48)

i=1 and {aj ∼ D(s)}n
i ← τ θi + (1 − τ )θ(cid:48)

(s(cid:48)  ai) + (1 − λ) maxj=1 .. K Qθ(cid:48)

i; φ(cid:48) ← τ φ + (1 − τ )φ(cid:48)

j=1  n preferably an intermediate integer(1-10)

(s(cid:48)  ai)]

i=1

j

j

4:
5:
6:

7:
8:
9:
10: end for

6

Figure 3: Average performance of BEAR-QL  BCQ  Naïve RL and BC on medium-quality data averaged over
5 seeds. BEAR-QL outperforms both BCQ and Naïve RL. Average return over the training data is indicated by
the magenta line. One step on the x-axis corresponds to 1000 gradient steps.

In summary  the actor is updated towards maximizing the Q-function while still being constrained
to remain in the valid search space deﬁned by Π. The Q-function uses actions sampled from the
actor to then perform distribution-constrained Q-learning  over a reduced set of policies. At test time 
we sample p actions from πφ(s) and the Q-value maximizing action out of these is executed in the
environment. Implementation and other details are present in Appendix D.

6 Experiments
In our experiments  we study how BEAR performs when learning from static off-policy data on a
variety of continuous control benchmark tasks. We evaluate our algorithm in three settings: when the
dataset D is generated by (1) a completely random behaviour policy  (2) a partially trained  medium
scoring policy  and (3) an optimal policy. Condition (2) is of particular interest  as it captures many
common use-cases in practice  such as learning from imperfect demonstration data (e.g.  of the sort
that are commonly available for autonomous driving [14])  or reusing previously collected experience
during off-policy RL. We compare our method to several prior methods: a baseline actor-critic
algorithm (TD3)  the BCQ algorithm [12]  which aims to address a similar problem  as discussed in
Section 4  KL-control [21] (which solves a KL-penalized RL problem similarly to maximum entropy
RL)  a static version of DQfD [20] (where a constraint to upweight Q-values of state-action pairs
observed in the dataset is added as an auxiliary loss on top a regular actor-critic algorithm)  and a
behaviour cloning (BC) baseline  which simply imitates the data distribution. This serves to measure
whether each method actually performs effective RL  or simply copies the data. We report the average
evaluation return over 5 seeds of the policy given by the learned algorithm  in the form of a learning
curve as a function of number of gradient steps taken by the algorithm. These samples are only
collected for evaluation  and are not used for training.
6.1 Performance on Medium-Quality Data

We ﬁrst discuss the evaluation of condition with “mediocre” data (2)  as this condition resembles
the settings where we expect training on ofﬂine data to be most useful. We collected one million
transitions from a partially trained policy  so as to simulate imperfect demonstration data or data from
a mediocre prior policy. In this scenario  we found that BEAR-QL consistently outperforms both
BCQ [12] and a naïve off-policy RL baseline (TD3) by large margins  as shown in Figure 3. This
scenario is the most relevant from an application point of view  as access to optimal data may not
be feasible  and random data might have inadequate exploration to efﬁcient learn a good policy. We
also evaluate the accuracy with which the learned Q-functions predict actual policy returns. These
trends are provided in Appendix E. Note that the performance of BCQ often tracks the performance
of the BC baseline  suggesting that BCQ primarily imitates the data. Our KL-control baseline uses
automatic temperature tuning [18]. We ﬁnd that KL-control usually performs similar or worse to BC 
whereas DQfD tends to diverge often due to cumulative error due to OOD actions and often exhibits
a huge variance across different runs (for example  HalfCheetah-v2 environment).
6.2 Performance on Random and Optimal Datasets

In Figure 5  we show the performance of each method when trained on data from a random policy
(top) and a near-optimal policy (bottom). In both cases  our method BEAR achieves good results 
consistently exceeding the average dataset return on random data  and matching the optimal policy
return on optimal data. Naïve RL also often does well on random data. For a random data policy  all
actions are in-distribution  since they all have equal probability. This is consistent with our hypothesis

7

0.0K0.2K0.4K0.6K0.8K1.0KTrainSteps0100020003000400050006000HalfCheetah-v2BCQBEAR-QLNaive-RLBCDQfDKL-c0.0K0.2K0.4K0.6K0.8K1.0KTrainSteps0500100015002000250030003500Walker2d-v2BCQBEAR-QLNaive-RLBCDQfDKL-c0.0K0.1K0.2K0.3K0.4KTrainSteps050010001500200025003000Hopper-v2BCQBEAR-QLNaive-RLBCDQfDKL-c0.0K0.2K0.4K0.6K0.8K1.0KTrainSteps−500−25002505007501000Ant-v2BCQBEAR-QLNaive-RLBCDQfDKL-cFigure 5: Average performance of BEAR-QL  BCQ  Naïve RL and BC on random data (top row) and optimal
data (bottom row) over 5 seeds. BEAR-QL is the only algorithm capable of learning in both scenarios. Naïve RL
cannot handle optimal data  since it does not illustrate mistakes  and BCQ favors a behavioral cloning strategy
(performs quite close to behaviour cloning in most cases)  causing it to fail on random data. Average return over
the training dataset is indicated by the dashed magenta line.

that OOD actions are one of the main sources of error in off-policy learning on static datasets. The
prior BCQ method [12] performs well on optimal data but performs poorly on random data  where
the constraint is too strict. These results show that BEAR-QL is robust to the dataset composition 
and can learn consistently in a variety of settings. We ﬁnd that KL-control and DQfD can be unstable
in these settings.
Finally  in Figure 4  we show that BEAR outperforms other considered prior methods in the challeng-
ing Humanoid-v2 environment as well  in two cases – Medium-quality data and random data.

6.3 Analysis of BEAR-QL

In this section  we aim to analyze different com-
ponents of our method via an ablation study.
Our ﬁrst ablation studies the support constraint
discussed in Section 5  which uses MMD to mea-
sure support. We replace it with a more standard
KL-divergence distribution constraint  which
measures similarity in density. Our hypothesis
is that this should provide a more conservative
constraint  since matching distributions is not
necessary for matching support. KL-divergence
performs well in some cases  such as with opti-
mal data  but as shown in Figure 6  it performs
worse than MMD on medium-quality data. Even
when KL-divergence is hand tuned fully  so as to prevent instability issues it still performs worse
than a not-well tuned MMD constraint. We provide the results for this setting in the Appendix. We
also vary the number of samples n that are used to compute the MMD constraint. We ﬁnd that
smaller n (≈ 4 or 5) gives better performance. Although the difference is not large  consistently better
performance with 4 samples leans in favour of our hypothesis that an intermediate number of samples
works well for support matching  and hence is less restrictive.

Figure 4: Performance of BEAR-QL  BCQ  Naïve RL
and BC on medium-quality (left) and random (right) data
in the Humanoid-v2 environment. Note that BEAR-QL
outperforms prior methods.

7 Discussion and Future Work
The goal in our work was to study off-policy reinforcement learning with static datasets. We
theoretically and empirically analyze how error propagates in off-policy RL due to the use of out-of-
distribution actions for computing the target values in the Bellman backup. Our experiments suggest
that this source of error is one of the primary issues afﬂicting off-policy RL: increasing the number

8

0.0K0.2K0.4K0.6K0.8K1.0KTrainSteps−1000010002000300040005000HalfCheetah-v2BCQBEARNaive-RLBCDQfDKL-c0.0K0.2K0.4K0.6K0.8K1.0KTrainSteps02000400060008000100001200014000HalfCheetah-v2BCQBEAR-QLNaive-RLBCDQfDKL-c0.0K0.2K0.4K0.6K0.8K1.0KTrainSteps02004006008001000Walker2d-v2BCQBEAR-QLNaive-RLBCDQfDKL-c0.0K0.2K0.4K0.6K0.8K1.0KTrainSteps010002000300040005000Walker2d-v2BCQBEAR-QLNaive-RLBCDQfDKL-c0.0K0.1K0.2K0.3K0.4KTrainSteps0100200300400500600700800Hopper-v2BCQBEAR-QLNaive-RLBCDQfDKL-c0.0K0.1K0.2K0.3K0.4KTrainSteps05001000150020002500300035004000Hopper-v2BCQBEAR-QLNaive-RLBCDQfDKL-c0.0K0.2K0.4K0.6KTrainSteps−1000−5000500100015002000Ant-v2BCQBEAR-QLNaive-RLBCDQfDKL-c0.0K0.2K0.4K0.6K0.8K1.0KTrainSteps−2000−10000100020003000400050006000Ant-v2BCQBEAR-QLNaive-RLBCDQfDKL-c0.0K0.2K0.4K0.6K0.8K1.0KTrainSteps01000200030004000500060007000Humanoid-v2BCQBEAR-QLNaive-RLBCDQfDKL-c0.0K0.2K0.4K0.6K0.8K1.0KTrainSteps0100200300400500600700Humanoid-v2BCQBEAR-QLNaive-RLBCDQfDKL-cof samples does not appear to mitigate the degradation issue (Figure 1)  and training with naïve
RL on data from a random policy  where there are no out-of-distribution actions  shows much less
degradation than training on data from more focused policies (Figure 5). Armed with this insight  we
develop a method for mitigating the effect of out-of-distribution actions  which we call BEAR-QL.
BEAR-QL constrains the backup to use actions that have non-negligible support under the data
distribution  but without being overly conservative in constraining the learned policy. We observe
experimentally that BEAR-QL achieves good performance across a range of tasks  and across a range
of dataset compositions  learning well on random  medium-quality  and expert data.
While BEAR-QL substantially stabilizes off-
policy RL  we believe that this problem merits
further study. One limitation of our current
method is that  although the learned policies
are more performant than those acquired with
naïve RL  performance sometimes still tends
to degrade for long learning runs. An exciting
direction for future work would be to develop
an early stopping condition for RL  perhaps
by generalizing the notion of validation error
to reinforcement learning. A limitation of ap-
proaches that perform constrained-action se-
lection is that they can be overly conservative
when compared to methods that constrain state-
distributions directly  especially with datasets
collected from mixtures of policies. We leave
it to future work to design algorithms that can directly constrain state distributions. A theoretically
robust method for support matching efﬁciently in high-dimensional continuous action spaces is a
question for future research. Perhaps methods from outside RL  predominantly used in domain
adaptation  such as using asymmetric f-divergences [37] can be used for support restriction. Another
promising future direction is to examine how well BEAR-QL can work on large-scale off-policy
learning problems  of the sort that are likely to arise in domains such as robotics  autonomous driving 
operations research  and commerce. If RL algorithms can learn effectively from large-scale off-policy
datasets  reinforcement learning can become a truly data-driven discipline  beneﬁting from the same
advantage in generalization that has been seen in recent years in supervised learning ﬁelds  where
large datasets have enabled rapid progress in terms of accuracy and generalization [7].

Figure 6: Average return (averaged Hopper-v2 and
Walker2d-v2) as a function of train steps for ablation
studies from Section 6.3. (a) MMD constrained optimiza-
tion is more stable and leads to better returns  (b) 4 sample
MMD is more performant than 10.

Acknowledgements

We thank Kristian Hartikainen for sharing implementations of RL algorithms and for help in debug-
ging certain issues. We thank Matthew Soh for help in setting up environments. We thank Aurick
Zhou  Chelsea Finn  Abhishek Gupta and Kelvin Xu for informative discussions. We thank Oﬁr
Nachum for comments on an earlier draft of this paper. We thank Google  NVIDIA  and Amazon for
providing computational resources. This research was supported by Berkeley DeepDrive  JPMorgan
Chase & Co.  NSF IIS-1651843 and IIS-1614653  the DARPA Assured Autonomy program  and
ARL DCIST CRA W911NF-17-2-0181.

References
[1] Andräs Antos  Csaa Szepesvari  and Remi Munos. Value-iteration based ﬁtted policy iteration:
Learning with a single trajectory. In 2007 IEEE International Symposium on Approximate
Dynamic Programming and Reinforcement Learning  pages 330–337  April 2007. doi: 10.1109/
ADPRL.2007.368207.

[2] András Antos  Csaba Szepesvári  and Rémi Munos. Fitted q-iteration in continuous action-
space mdps. In Advances in Neural Information Processing Systems 20  pages 9–16. Curran
Associates  Inc.  2008.

[3] James Bennett  Stan Lanning  et al. The netﬂix prize. 2007.
[4] Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming. Athena Scientiﬁc 

1996.

9

0.0K0.1K0.2K0.3K0.4K0.5KTrainSteps05001000150020002500MMDvsKLconstraintonmediocredataKLMMD0.0K0.1K0.2K0.3K0.4KTrainSteps050010001500200025003000NumberofsamplesforMMDHopper 4Hopper 10Walker2d 4Walker2d 10[5] Jonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning?

In ICML 2019.

[6] Tim de Bruin  Jens Kober  Karl Tuyls  and Robert Babuska. The importance of experience

replay database composition in deep reinforcement learning. 01 2015.

[7] Jia Deng  Wei Dong  Richard S. Socher  Li-Jia Li  Kai Li  and Li Fei-Fei.

Large-Scale Hierarchical Image Database. In CVPR09  2009.

ImageNet: A

[8] Jacob Devlin  Ming-Wei Chang  Kenton Lee  and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 
2018.

[9] Lasse Espeholt  Hubert Soyer  Remi Munos  Karen Simonyan  Volodymir Mnih  Tom Ward 
Yotam Doron  Vlad Firoiu  Tim Harley  Iain Dunning  et al. Impala: Scalable distributed deep-rl
In Proceedings of the International
with importance weighted actor-learner architectures.
Conference on Machine Learning (ICML)  2018.

[10] Amir-massoud Farahmand  Csaba Szepesvári  and Rémi Munos. Error propagation for approxi-
mate policy and value iteration. In Advances in Neural Information Processing Systems  pages
568–576  2010.

[11] Justin Fu  Aviral Kumar  Matthew Soh  and Sergey Levine. Diagnosing bottlenecks in deep

q-learning algorithms. arXiv preprint arXiv:1902.10250  2019.

[12] Scott Fujimoto  David Meger  and Doina Precup. Off-policy deep reinforcement learning

without exploration. arXiv preprint arXiv:1812.02900  2018.

[13] Scott Fujimoto  Herke van Hoof  and David Meger. Addressing function approximation error
in actor-critic methods. In Jennifer Dy and Andreas Krause  editors  Proceedings of the 35th
International Conference on Machine Learning  volume 80 of Proceedings of Machine Learning
Research  pages 1587–1596. PMLR  2018.

[14] Yang Gao  Huazhe Xu  Ji Lin  Fisher Yu  Sergey Levine  and Trevor Darrell. Reinforcement

learning from imperfect demonstrations. In ICLR (Workshop). OpenReview.net  2018.

[15] Carles Gelada and Marc G. Bellemare. Off-policy deep reinforcement learning by bootstrapping

the covariate shift. CoRR  abs/1901.09455  2019.

[16] Jordi Grau-Moya  Felix Leibfried  and Peter Vrancx. Soft q-learning with mutual-information
regularization. In International Conference on Learning Representations  2019. URL https:
//openreview.net/forum?id=HyEtjoCqFX.

[17] Arthur Gretton  Karsten M. Borgwardt  Malte J. Rasch  Bernhard Schölkopf  and Alexander
Smola. A kernel two-sample test. J. Mach. Learn. Res.  13:723–773  March 2012. ISSN
1532-4435. URL http://dl.acm.org/citation.cfm?id=2188385.2188410.

[18] Tuomas Haarnoja  Aurick Zhou  Pieter Abbeel  and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290  2018.

[19] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
pages 770–778  2016.

[20] Todd Hester  Matej Vecerik  Olivier Pietquin  Marc Lanctot  Tom Schaul  Bilal Piot  Dan Horgan 
John Quan  Andrew Sendonaris  Ian Osband  et al. Deep q-learning from demonstrations. In
Thirty-Second AAAI Conference on Artiﬁcial Intelligence  2018.

[21] Natasha Jaques  Asma Ghandeharioun  Judy Hanwen Shen  Craig Ferguson  Àgata Lapedriza 
Noah Jones  Shixiang Gu  and Rosalind W. Picard. Way off-policy batch deep reinforcement
learning of implicit human preferences in dialog. CoRR  abs/1907.00456  2019. URL http:
//arxiv.org/abs/1907.00456.

[22] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning.
In Proceedings of the Nineteenth International Conference on Machine Learning  pages 267–
274. Morgan Kaufmann Publishers Inc.  2002.

[23] Dmitry Kalashnikov  Alex Irpan  Peter Pastor  Julian Ibarz  Alexander Herzog  Eric Jang 
Deirdre Quillen  Ethan Holly  Mrinal Kalakrishnan  Vincent Vanhoucke  and Sergey Levine.
Scalable deep reinforcement learning for vision-based robotic manipulation. In Proceedings

10

of The 2nd Conference on Robot Learning  volume 87 of Proceedings of Machine Learning
Research  pages 651–673. PMLR  2018.

[24] Romain Laroche  Paul Trichelair  and Remi Tachet Des Combes. Safe policy improvement with

baseline bootstrapping. In International Conference on Machine Learning (ICML)  2019.

[25] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and

review. CoRR  abs/1805.00909  2018. URL http://arxiv.org/abs/1805.00909.

[26] A Rupam Mahmood  Huizhen Yu  Martha White  and Richard S Sutton. Emphatic temporal-

difference learning. arXiv preprint arXiv:1507.01569  2015.

[27] Rémi Munos. Error bounds for approximate policy iteration. In Proceedings of the Twentieth
International Conference on International Conference on Machine Learning  pages 560–567.
AAAI Press  2003.

[28] Rémi Munos. Error bounds for approximate value iteration. In Proceedings of the National

Conference on Artiﬁcial Intelligence  2005.

[29] Rémi Munos  Tom Stepleton  Anna Harutyunyan  and Marc Bellemare. Safe and efﬁcient
off-policy reinforcement learning. In Advances in Neural Information Processing Systems 
pages 1054–1062  2016.

[30] Doina Precup  Richard S. Sutton  and Sanjoy Dasgupta. Off-policy temporal-difference learning
with function approximation. In International Conference on Machine Learning (ICML)  2001.

[31] Stefan Schaal. Is imitation learning the route to humanoid robots?  1999.
[32] Tom Schaul  John Quan  Ioannis Antonoglou  and David Silver. Prioritized experience replay.

CoRR  abs/1511.05952  2016.

[33] Bruno Scherrer  Mohammad Ghavamzadeh  Victor Gabillon  Boris Lesner  and Matthieu Geist.
Approximate modiﬁed policy iteration and its application to the game of tetris. Journal of
Machine Learning Research  16:1629–1676  2015. URL http://jmlr.org/papers/v16/
scherrer15a.html.

[34] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust
region policy optimization. In Francis Bach and David Blei  editors  Proceedings of the 32nd
International Conference on Machine Learning  volume 37 of Proceedings of Machine Learning
Research  pages 1889–1897  Lille  France  07–09 Jul 2015. PMLR.

[35] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Second

edition  2018.

[36] Emanuel Todorov  Tom Erez  and Yuval Tassa. MuJoCo: A physics engine for model-based

control. In IROS  pages 5026–5033  2012.

[37] Yifan Wu  Ezra Winston  Divyansh Kaushik  and Zachary Lipton. Domain adaptation with

asymmetrically-relaxed distribution alignment. In ICML 2019.

[38] Fisher Yu  Wenqi Xian  Yingying Chen  Fangchen Liu  Mike Liao  Vashisht Madhavan  and
Trevor Darrell. BDD100K: A diverse driving video database with scalable annotation tooling.
CoRR  abs/1805.04687  2018. URL http://arxiv.org/abs/1805.04687.

11

,Aviral Kumar
Justin Fu
Matthew Soh
George Tucker
Sergey Levine