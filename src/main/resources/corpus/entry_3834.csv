2018,Nonparametric learning from Bayesian models with randomized objective functions,Bayesian learning is built on an assumption that the model space contains a true reflection of the data generating mechanism. This assumption is problematic  particularly in complex data environments. Here we present a Bayesian nonparametric approach to learning that makes use of statistical models  but does not assume that the model is true. Our approach has provably better properties than using a parametric model and admits a Monte Carlo sampling scheme that can afford massive scalability on modern computer architectures. The model-based aspect of learning is particularly attractive for regularizing nonparametric inference when the sample size is small  and also for correcting approximate approaches such as variational Bayes (VB). We demonstrate the approach on a number of examples including VB classifiers and Bayesian random forests.,Nonparametric learning from Bayesian models with

randomized objective functions

Simon Lyddon

Department of Statistics

University of Oxford

Oxford  UK

lyddon@stats.ox.ac.uk

Stephen Walker

Department of Mathematics
University of Texas at Austin

Austin  TX

s.g.walker@math.utexas.edu

Chris Holmes

Department of Statistics

University of Oxford

Oxford  UK

cholmes@stats.ox.ac.uk

Abstract

Bayesian learning is built on an assumption that the model space contains a true
reﬂection of the data generating mechanism. This assumption is problematic  par-
ticularly in complex data environments. Here we present a Bayesian nonparametric
approach to learning that makes use of statistical models  but does not assume
that the model is true. Our approach has provably better properties than using
a parametric model and admits a Monte Carlo sampling scheme that can afford
massive scalability on modern computer architectures. The model-based aspect of
learning is particularly attractive for regularizing nonparametric inference when
the sample size is small  and also for correcting approximate approaches such as
variational Bayes (VB). We demonstrate the approach on a number of examples
including VB classiﬁers and Bayesian random forests.

1

Introduction

Bayesian updating provides a principled and coherent approach to inference for probabilistic models
[23]  but is predicated on the model class being true. That is  for an observation x and a generative
model Fθ(x) parametrized by a ﬁnite-dimensional parameter θ ∈ Θ  then for some parameter value
θ0 ∈ Θ it is that x ∼ Fθ0 (x). In reality  however  all models are false. If the data is simple and
small  and the model space is sufﬁciently rich  then the consequences of model misspeciﬁcation may
not be severe. However  data is increasingly being captured at scale  both in terms of the number
of observations as well as the diversity of data modalities. This poses a risk in conditioning on an
assumption that the model is true.
In this paper we discuss a scalable approach to Bayesian nonparametric learning (NPL) from models
without the assumption that x ∼ Fθ0(x). To do this we use a nonparametric prior that is centered on
a model but does not assume the model to be true. A concentration parameter  c  in the nonparametric
prior quantiﬁes trust in the baseline model and this is subsequently reﬂected in the nonparametric
update  through the relative inﬂuence given to the model-based inference for θ. In particular  c → ∞
recovers the standard model-based Bayesian update while c = 0 leads to a Bayesian bootstrap
estimator for the object of interest.
Our methodology can be applied in a number of situations  including:

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

[S1] Model misspeciﬁcation: where we have used a parametric Bayesian model and we are

concerned that the model may be misspeciﬁed.

[S2] Approximate posteriors: where for expediency we have used an approximate posterior  such

as in variational Bayes (VB)  and we wish to account for the approximation.

[S3] Direct updating from utility-functions: where the sole purpose of the modelling task is to

perform some action or take a decision under a well-speciﬁed utility function.

Our work builds upon previous ideas including [21] who introduced the weighted likelihood bootstrap
(WLB) as a way of generating approximate samples from the posterior of a well-speciﬁed Bayesian
model. [19] highlighted that the WLB in fact provides an exact representation of uncertainty for the
model parameters that minimize the Kullback-Leibler (KL) divergence  dKL(F0  Fθ)  between the
unknown data-generating distribution and the model likelihood fθ(x)  and hence is well motivated
regardless of model validity. These approaches however do not allow for the inclusion of prior
knowledge and do not provide a Bayesian update as we do here.
A major underlying theme behind our paper  and indeed an open ﬁeld for future research  is the idea
of obtaining targeted posterior samples via the maximization of a suitably randomized objective
function. The WLB randomizes the log-likelihood function  effectively providing samples which are
randomized maximum likelihood estimates  whereas we randomize a more general objective function
under a Bayesian nonparametric (NP) posterior. The randomization takes into account knowledge
captured through the choice of a model and parametric prior.

2 Foundations of Nonparametric Learning

We begin with the simplest scenario  namely [S1]  concerning a possibly misspeciﬁed model before
moving on to more complicated situations. It is interesting to note that all of what follows can also be
considered from a viewpoint of NP regularization  using a parametric model to centre a Bayesian NP
analysis in a way that induces stability and parametric structure to the problem.

2.1 Bayesian updating of misspeciﬁed models
Suppose we have a parametric statistical model  FΘ = {fθ(·); θ ∈ Θ)}  where for each θ ∈ Θ ⊆ Rp 
fθ : X → R is a probability density. The conventional approach to Bayesian learning involves
updating a prior distribution to a posterior through Bayes’ theorem. This approach is well studied
and well understood [3]  but formally assumes that the model space contains the true data-generating
mechanism. We will derive a posterior update under weaker assumptions.
Suppose that FΘ has been selected for the purpose of a prediction  or a decision  or some other
modelling task. Consider the thought experiment where the modeller somehow gains access to
Nature’s true sampling distribution for the data  F0(x)  which does not necessarily belong to FΘ.
How should they then update their model?
With access to F0 the modeller can simply request an inﬁnite training set  x1:∞ iid∼ F0  and then
update to the posterior π(θ|x1:∞). Under an inﬁnite sample size all uncertainty is removed and for
regular models the posterior concentrates at a point mass at θ0  the parameter value maximizing the
expected log-likelihood  assuming that the prior has support there; i.e.

(cid:90)

(cid:90)

2

n(cid:88)

i=1

lim

n→∞ n−1
(cid:90)

θ0 = arg max

θ∈Θ

log fθ(xi) = arg max

θ∈Θ

X

log fθ(x) dF0(x).

It is straightforward to see that θ0 minimizes the KL divergence from the true data-generating
mechanism to a density in FΘ

θ0 = arg max

(1)
This is true regardless of whether F0 is in the model space of FΘ and is well-motivated as the target
of statistical model ﬁtting [1  8  27  5].

log fθ(x)dF0(x) = arg min

dF0(x).

θ∈Θ

log

X

θ∈Θ

X

f0(x)
fθ(x)

Uncertainty in this unknown value θ0 ﬂows directly from uncertainty in F0. Of course F0 is unknown 
but being “Bayesian” we can place a prior on it  π(F )  for F ∈ F  that should reﬂect our honest
uncertainty about F0. Typically the prior should have broad support unless we have special knowledge
to hand  which is a problem with a parametric modelling approach that only supports a family of
distribution functions indexed by a ﬁnite-dimensional parameter. The Bayesian NP literature however
provides a range of priors for this sole purpose [14]. Once a prior for F is chosen  the correct
way to propagate uncertainty about θ comes naturally from the posterior distribution for the law
L[θ(F )|x1:n]  via L[F|x1:n]  where θ(F ) = arg maxθ∈Θ
(cid:90)
parameter is then captured in the marginal by treating F as a latent auxiliary probability measure 

(cid:82) log fθ(x)dF (x). The posterior for the

it from the conventional Bayesian posterior π(θ|x1:n) ∝ π(θ)(cid:81)n

(2)
where π(θ|F ) assigns probability 1 to θ = θ(F ). We use ˜π to denote the NP update to distinguish
i=1 fθ(xi)  noting that in general
the nonparametric posterior ˜π(θ | x1:n) will be different to the standard Bayesian update as they are
conditioning on different states of prior knowledge. In particular  as stated above  π(θ|x1:n) assumes
artiﬁcially that F0 ∈ FΘ.

π(θ | F )π(dF | x1:n) 

π(θ  dF | x1:n) =

˜π(θ | x1:n) =

(cid:90)

F

F

2.2 An NP prior using a MDP

For our purposes  the mixture of Dirichlet processes (MDP) [2] is a convenient vehicle for specifying
prior beliefs π(F ) centered on parametric models.1 The MDP prior can be written as

[F | θ] ∼ DP(c  fθ(·));

θ ∼ π(θ).

(3)

This is a mixture of standard Dirichlet processes with mixing distribution or hyper-prior π(θ)  and
concentration parameter c. We write this as F ∼ MDP(π(θ)  c  fθ(·)).
The MDP provides a practical  simple posterior update. From the conjugacy property of the DP
applied to (3)  we have the conditional posterior update given data x1:n  as

[F | θ  x1:n] ∼ DP

c + n 

c

c + n

fθ(·) +

1

c + n

i=1

(4)

where δx denotes the Dirac measure at x. The concentration parameter c is an effective sample size 
governing the trust we have in fθ(x). The marginal posterior distribution for L[F|x1:n] can be written
as

π(dF | x1:n) =

π(dF | θ  x1:n) π(θ | x1:n) dθ 

n(cid:88)

(cid:33)
δxi(·)

(cid:32)

(cid:90)

Θ

(cid:32)

(5)

(cid:33)

n(cid:88)

i.e.

c

1

δxi(·)

fθ(·) +

[F | x1:n] ∼ MDP

π(θ | x1:n)  c + n 

c + n

c + n

(6)
The mixing distribution π(θ|x1:n) coincides with the parametric Bayesian posterior  π(θ|x1:n) 
assuming there are no ties in the data [2]  although as noted above it does not follow that the NP
marginal ˜π(θ|x1:n) is equivalent to the parametric Bayesian posterior π(θ|x1:n).
We can see from the form of the conditional MDP (4) that the sampling distribution of the centering
i=1 δxi(·). The resulting NP posterior
(5) combines the information from the posterior distribution of the centering model π(θ|x1:n) with the
information in the empirical distribution of the data. This leads to a simple and highly parallelizable
Monte Carlo sampling scheme as shown below.

model  fθ(x)  regularizes the inﬂuence of the empirical data(cid:80)n

i=1

.

2.3 Monte Carlo conditional maximization

which we write as G =(cid:82)

The marginal in (2) facilitates a Monte Carlo estimator for functionals of interest under the posterior 
Θ g(θ)˜π(θ|x1:n)dθ. This is achieved by sampling π(θ  dF|x1:n) jointly

1The MDP should not to be confused with the Dirichlet process mixture model (DPM) [18].

3

from the posterior  (cid:90)

Θ

g(θ)˜π(θ | x1:n)dθ ≈ 1
B

B(cid:88)

i=1

g(θ(i))

(cid:90)

θ(i) = θ(F (i)) = arg max

log fθ(x)dF (i)(x)

(7)

X
F (i) ∼ π(dF | x1:n).

θ∈Θ

(8)
This involves an independent Monte Carlo draw (8) from the MDP marginal followed by a condi-
tional maximization of an objective function (7) to obtain each θ(i). This Monte Carlo conditional
maximization (MCCM) sampler is highly amenable to fast implementation on distributed computer ar-
chitectures; given the parametric posterior samples  each NP posterior sample  F (i)  can be computed
independently and in parallel from (8).
We can see from (6) that the parametric posterior samples are not required if c = 0. If c > 0 it may
be computationally intensive to generate samples from the parametric posterior. However  as we will
see next  we do need to sample from this posterior directly. This makes the approach particularly
attractive to fast  tractable approximations for π(θ|x1:n)  such as a variational Bayes (VB) posterior
approximation. The NP update corrects for the approximation in a computationally efﬁcient manner 
leading to a posterior distribution with optimal properties as shown below.

2.4 A more general construction

So far we have assumed  hypothetically  that:

(cid:82) log fθ(x)dF0(x)  rather than α0 = arg maxα

arg maxθ
a utility function u(x  α).

(i) the modeller is interested in learning about the MLE under an inﬁnite sample size  θ0 =

(ii) the parametric mixing distribution π(θ|x1:n) of the MDP posterior in (6) is constructed from

the same centering model that deﬁnes the target parameter  θ0 = arg maxθ

Both of these assumptions can be relaxed. For the latter case  it is valid to use a tractable parametric
mixing distribution π(γ|x1:n) and baseline model fγ  while still learning about θ0 in (1) through the
marginal ˜π(θ|x1:n) as in (2) obtained via θ(F ) and

(cid:82) u(x  α)dF0(x) more generally  for
(cid:82) log fθ(x)dF0(x).
(cid:33)

(cid:88)

i

c + n

1

c + n

c

fγ(·) +

δxi

.

[F | x1:n] ∼ MDP

π(γ | x1:n)  c + n 

For (i)  we can use the mapping α(F ) = arg maxα
actions or parameters maximizing some expected utility under a model-centered MDP posterior.

(cid:82) u(x  α)dF (x) to derive the NPL posterior on
This can be written as ˜π(α|x1:n) =(cid:82) π(α|F )π(dF|x1:n)  where π(α|F ) assigns probability 1 to
(cid:82) u(x  α)dF0(x) 

α = α(F ).
This highlights a major theme of the paper: the idea of obtaining posterior samples via maximization of
a suitably randomized objective function. In generality the target is α0 = arg maxα
obtained by maximizing an objective function  and the randomization arises from the uncertainty in
F0 through π(F|x1:n) that takes into account the information  and any misspeciﬁcation  associated
with a parametric centering model.

(9)

(cid:32)

2.5 The Posterior Bootstrap algorithm

We will use the general construction of Section 2.4 to describe a sampling algorithm. We assume we
have access to samples from the posterior parametric mixing distribution  π(γ|x1:n)  in the MDP. In
the case of model misspeciﬁcation  [S1]  if the data contains no ties  this is simply the parametric
Bayesian posterior under {fγ(x)  π(γ)}  for which there is a large literature of computational methods
available for sampling - see for example [24]. If there are ties then we refer the reader to [2] or note
that we can simply break ties by adding a new pseudo-variable  such as x∗ ∼ N (0  2) for small .
The sampling algorithm  found in Algorithm 1  is a mixture of Bayesian posterior bootstraps. After a
sample γ(i) is drawn from the mixing posterior  π(γ|x1:n)  a posterior pseudo-sample is generated 

4

Algorithm 1: The Posterior Bootstrap
Data: Dataset x1:n = (x1  . . .   xn).
Parameter of interest α0 = α(F0) = arg maxα
Mixing posterior π(γ|x1:n)  concentration parameter c  centering model fγ(x).
Number of centering model samples T .
begin

(cid:82) u(x  α)dF0(x).

for i = 1  . . .   B do

Draw centering model parameter γ(i) ∼ π(γ|x1:n);
iid∼ fγ(i);
Draw posterior pseudo-sample x(i)
Generate weights
(w(i)
1   . . .   w(i)
T(cid:80)
Compute parameter update

(cid:40) n(cid:80)

(n+1):(n+T )

n   w(i)

n+1  . . .   w(i)

˜α(i) = arg maxα

w(i)

j u(xj  α) +

n+ju(x(i)
w(i)

n+j  α)

;

j=1

j=1

n+T ) ∼ Dirichlet(1  . . .   1  c/T  . . .   c/T );

(cid:41)

end
Return NP posterior sample {˜α(i)}B

i=1.

end

iid∼ fγ(i)(x)  and added to the dataset  which is then randomly weighted. The parameter
x(i)
(n+1):(n+T )
under this implicit distribution function is then computed as the solution of an optimization problem.
Note for the special case of correcting model misspeciﬁcation [S1]  we have γ ≡ θ  fγ(·) ≡ fθ(·) 
π(γ|x1:n) ≡ π(θ|x1:n)  α ≡ θ  u(x  α) ≡ log fθ(x)  so that the posterior sample is given by

 .

 n(cid:88)

j=1

T(cid:88)

j=1

˜θ(i) = arg max

θ

w(i)
j

log fθ(xj) +

w(i)
n+j log fθ(x(i)

n+j)

where w(i) ∼ Dirichlet(·) following Algorithm 1 and x(i)
(n+1):(n+T ) are T synthetic observations
drawn from the parametric sampling distribution under θ(i) which itself is drawn from π(θ|x1:n). We
leave the concentration parameter c to be set subjectively by the practitioner  representing faith in
the parametric model. Some further guidance to the setting of c can be found in Section 1 of the
Supplementary Material.

2.6 Adaptive Nonparametric Learning: aNPL

probability measure(cid:81)

Instead of the Dirichlet distribution approximation to the Dirichlet process  we propose an alternative
stick-breaking procedure that has some desirable properties. This procedure entails following the
usual DP stick-breaking construction [25] for the model component of the MDP posterior  by
repeatedly drawing Beta(1  c)-distributed stick breaks  but terminating when the unaccounted for
j(1 − vj)  multiplied by the average mass assigned to the model  c/(c + n) 
drops below some threshold  set by the user. This adaptive nonparametric learning (aNPL) algorithm
is written out in full in Section 2 of the Supplementary Material.
One advantage of this approach is that a number of theoretical results then hold  as for large enough
n  under this adaptive scheme the parametric model is in effect ‘switched off’  and essentially the
MDP with c = 0 is used to generate posterior samples. This is an interesting notion in itself. For
small samples  we prefer the regularization that our model provides  though as n grows the average
probability mass assigned to the model decays like (c + n)−1  as seen in (4). In the adaptive version 
we agree a hard threshold at which point we discard the model entirely and allow the data to speak
for itself. We set this point at a level such that we are a priori comfortable that there is enough
information in the sample alone with which to quantify uncertainty in our parameter of interest. For
example   = 10−4 and c = 1 only utilizes the centering model for n < 10  000. Further  we could
use this idea to set c: this quantity is determined if a tolerance level    and a threshold nmax over
which the parametric model would be discarded  are provided by the practitioner.

5

2.7 Properties of NPL

Bayesian nonparametric learning has a number of important properties that we shall now describe.
Honesty about correctness of model. Uncertainty in the data-generating mechanism is quantiﬁed
via a NP update that takes into account the model likelihood  prior  and concentration parameter c.
Uncertainty about model parameters ﬂows from uncertainty in the data-generating mechanism.
Incorporation of prior information. The prior for θ is naturally incorporated as a mixing distribu-
tion for the MDP. This is in contrast to a number of Bayesian methods with similar computational
properties but that do not admit a prior [21  9].
Parallelized bootstrap computation. As shown in Section 2.5  NPL is trivially parallelizable
through a Bayesian posterior bootstrap and can be coupled with misspeciﬁed models or approximate
posteriors to deliver highly scalable and exact inference.
Consistency. Under mild regularity  all posterior mass concentrates in any neighbourhood of θ0
as deﬁned in (1)  as the number of observations tends to inﬁnity. This follows from an analogous
property of the DP (see  for example [14]).
Standard Bayesian inference is recovered as c → ∞. This follows from the property of the DP
that it converges to the prior degenerate at the base probability distribution in the limit of c → ∞.
Non-informative learning with c = 0. If no model or prior is available  setting c = 0 recovers
the WLB. This has an exact interpretation as an objective NP posterior [19]  where the asymptotic
properties of the misspeciﬁed WLB were studied. [20] demonstrated the suboptimality of a mis-
speciﬁed Bayesian posterior  asymptotically  relative to an asymptotic normal distribution with the
same centering but a sandwich covariance matrix [15]. We will see next that for large samples the
misspeciﬁed Bayesian posterior distribution is predictively suboptimal as well.
A superior asymptotic uncertainty quantiﬁcation to Bayesian updating. A natural way to com-
pare posterior distributions is by measuring their predictive risk  deﬁned as the expected KL divergence
of the posterior predictive to F0. We consider only the situation where there is an absence of strong
prior information  following [26  12].
We say predictive π1 asymptotically dominates π2 up to o(n−k) if for all distributions q there exists
a non-negative and possibly positive real-valued functional K(q(·)) such that:

Ex1:n∼q dKL(q(·)  π2(· | x1:n)) − Ex1:n∼q dKL(q(·)  π1(· | x1:n)) = K(q(·)) + o(n−k).

We have the following theorem about the asymptotic properties of the MDP with c = 0. This result
holds for aNPL  as the model component is ignored for suitably large n.
Theorem 1. The posterior predictive of the MDP with c = 0 asymptotically dominates the standard
Bayesian posterior predictive up to o(n−1).

Proof. In [12] the bootstrap predictive is shown to asymptotically dominate the standard Bayesian
predictive up to o(n−1). In Theorem 1 of [13]  the predictive of the MDP with c = 0 and the
bootstrap predictive are shown to be equal up to op(n−3/2). A Taylor expansion argument shows that
the predictive risk of the MDP with c = 0 has the same asymptotic expansion up to o(n−1) as that of
the bootstrap. Thus Theorem 2 of [12] can be proven with the predictive of the MDP with c = 0 in
place of the bootstrap predictive. Thus the predictive of the MDP with c = 0 must also dominate the
standard Bayesian predictive up to o(n−1).

3

Illustrations

3.1 Exponential family  [S1]

Suppose the centering model is an exponential family with parameter θ and sufﬁcient statistic s(x) 

Under assumed regularity  by differentiating under the integral sign of (1) we ﬁnd that our parameter
of interest must satisfy EF0 s(x) = ∇θK(θ0). For a particular F drawn from the posterior bootstrap 
the expected sufﬁcient statistic is

FΘ =(cid:8)fθ(x) = g(x) exp(cid:8)θT s(x) − K(θ)(cid:9) ; θ ∈ Θ(cid:9) .
 .

∇θK(˜θ) = lim
T→∞

n+T(cid:88)

 n(cid:88)

j=1

6

wjs(xj) +

wjs(xj)

j=n+1

Figure 1: Posterior 95% probability contour for a bivariate Gaussian  comparing VB-NPL with
c ∈ {1  102  103  104} (red  orange  green  blue respectively) to the known Bayes posterior (grey
dashed line) and a VB approximation (black dashed line).

with ˜θ the NP posterior parameter value  weights w1:(n+T ) arising from the Dirichlet distribution
as set out in Algorithm 1  and xj ∼ fθ(·) for j = n + 1  . . .   n + T   with θ drawn from the
parametric posterior. This provides a simple geometric interpretation of our method  as convex
combinations of (randomly-weighted) empirical sufﬁcient statistics and model sufﬁcient statistics
from the parametric posterior. The distribution of the random weights is governed by c and n only.
Our method generates stochastic maps from misspeciﬁed posterior samples to corrected NP posterior
samples  by incorporating information in the data over and above that captured by the model.

3.2 Updating approximate posteriors [S2]: Variational Bayes uncertainty correction

2   1

Variational approximations to Bayesian posteriors are a popular tool for obtaining fast  scalable but
approximate Bayesian posterior distributions [4  6]. The approximate nature of the variational update
can be accounted for using our approach. Figure 1 shows a mean-ﬁeld normal approximation q to a
correlated normal posterior p  an example similar to one from [4]  Section 10.1. We generated 100
observations from a bivariate normal distribution  centered at ( 1
2 )  with dimension-wise variances
both equal to 1 and correlation equal to 0.9  and independent normal priors on each dimension  both
centered at 0 with variance 102. Each posterior contour plotted is based on 10  000 posterior samples.
By applying the posterior bootstrap with a VB posterior (VB-NPL) in place of the Bayesian posterior 
we recover the correct covariance structure for decreasing prior concentration c. If instead of dKL(q  p)
we use dKL(p  q) as the objective  as it is for expectation propagation  the model posterior uncertainty
may be overestimated  but is still corrected by the posterior bootstrap.
We demonstrate this in practice through a VB logistic regression model ﬁt to the Statlog German
Credit dataset  containing 1000 observations and 25 covariates (including intercept)  from the UCI
ML repository [10]  preprocessing via [11]. An independent normal prior with variance 100 was
assigned to each covariate  and 1000 posterior samples were generated for each method. We obtain
a mean-ﬁeld VB sample using automatic differentiation variational inference (ADVI) in Stan [17].
When generating synthetic samples for the posterior bootstrap  both features and classes are needed.
Classes are generated  given features  according to the probability speciﬁed by the logistic distribution.
In this example (and the example in Section 3.3) we repeatedly re-use the features of the dataset for
our pseudo-samples. In Fig. 2 we show that the NP update effectively corrects the VB approximation
for small values of c. Of course  local variational methods can provide more accurate posterior
approximations to Bayesian logistic posteriors [16]  though these too are approximations  that NP
updating can correct.
Comparison with Bayesian logistic regression. The conventional Bayesian logistic regression
assumes the true log-odds of each class is linear in the predictors  and would use MCMC for inference
[22]. The MCMC samples  shown as points in Fig. 2  are a good match to the NPL update but
MCMC requires a user-deﬁned burn-in and convergence checking. The runtime to generate 1 million
samples by MCMC (discarding an equivalent burn-in)  was 33 minutes  compared to 21 seconds with

7

0.40.50.60.70.80.40.50.60.70.8b1b2a) KL(q p)0.40.50.60.70.80.40.50.60.70.8b1b2b) KL(p q)Figure 2: Posterior contour plot for β22 vs β21  for VB-NPL (green) and VB (blue)  for three values
of c. Scatter plot is a Bayesian logistic posterior sample (red) via a Polya-Gamma scheme.

NPL  using an m5.24xlarge AWS instance with 96 vCPUs; a speed-up of 95 times. Additionally NPL
has provably better predictive properties  as detailed in Section 2.7.

3.3 Directly updating the prior: Bayesian Random Forests  using synthetic generated data

Random forests (RF) [7] is an ensemble learning method that is widely used and has demonstrated
excellent general performance [11]. We construct a Bayesian RF (BRF)  via NPL with decision
trees  under a prior mixing distribution (a variant of [S1]). This enables the incorporation of
prior information  via synthetic data generated from a prior prediction function  in a principled
way that is not available to RF. The step-like generative likelihood function arising from the tree
partition structure does not reﬂect our beliefs about the true sampling distribution; the trees are just
a convenient compression of the data. Because of this we simply update the prior in the MDP by
specifying π(γ|x1:n) = π(γ). Details of our implementation of BRF can be found in Section 3 of
the Supplementary Material.
To demonstrate the ability of BRF to incorporate prior information  we conducted the following
experiment. For 13 binary classiﬁcation datasets from the UCI ML repository [10]  we constructed a
prior  training and test dataset of equal size. We measured test dataset predictive accuracy for three
methods (relative to an RF trained on the training dataset only): BRF (c=0) (a non-informative BRF
with c = 0  trained on the training dataset only)  BRF (c>0) (a BRF trained on the training dataset 
incorporating prior pseudo-samples from a non-informative BRF trained on the prior dataset  setting
c equal to the number of observations in the prior dataset)  and RF-all (an RF trained on the combined
training and prior datasets). See Fig. 3 for boxplots of the test accuracy over 100 repetitions.
As detailed in Section 3 of the Supplementary Material  for small c we ﬁnd that BRF and RF have
similar performance  but as c increases  more weight is given to the externally-trained component
and we ﬁnd that BRF outperforms RF. The best performance of our BRF tends to occur when c is set
equal to the number of samples in the external training dataset  in line with our intuition of the role of
c as an effective sample size. Overall  the BRF accuracy is better than that of RF  and close to that of
RF-all. BRF may have privacy beneﬁts over RF-all as it only requires synthetic samples; both the
original data and the model can be kept private.

3.4 Direct updating of utility functions [S3]: population median

(cid:82) |α − x|dF0(x)  and an MDP prior centered at a N (θ  1) with prior π(θ) = N (0  102)

We demonstrate direct inference for a functional of interest using the population median  under a
misspeciﬁed univariate Gaussian model  as an example  where the parameter of interest is α0 =
arg minα
and data generated from a skew-normal distribution. We use the posterior bootstrap to generate
posterior samples that incorporate the prior model information with that from the data. Figure 4
presents histograms of posterior medians given a sample of 20 observations from a skew-normal
distribution with mean 0  variance 1 and median approximately −0.2. The left-most histogram is
sharply peaked at the sample median but does not have support outside of (xmin  xmax). As c grows
smoothness from the parametric model is introduced to a point where the normal location parameter
is used.

8

c = 1c = 1000c = 20000−0.20.00.20.4−0.20.00.20.4−0.20.00.20.4−0.4−0.20.00.2b21b22Figure 3: Box plot of classiﬁcation accuracy minus that of RF  for 13 UCI datasets.

Figure 4: Posterior histogram for median (left to right) c = 0  20  80  1000. Right-most: posterior
expected loss as a function of observation x. Dotted line shows the loss to the sample median.

4 Discussion

We have introduced a new approach for scalable Bayesian nonparametric learning  NPL  for para-
metric models that facilitates prior regularization via a baseline model  and corrects for model
misspeciﬁcation by incorporating an empirical component that has greater inﬂuence as the number
of observations grows. A concentration parameter c encodes subjective beliefs on the validity of
the model; c = ∞ recovers Bayesian updating under the baseline model  and c = 0 ignores the
model entirely. Under regularity conditions  asymptotically  our method closely matches parametric
Bayesian updating if the posited model were indeed true  and will provide an asymptotically superior
predictive if the model is misspeciﬁed. The NP posterior predictive mixes over the parametric model
space as opposed to targeting F0 directly  though this may aid interpretation compared to fully
nonparametric approaches. Additionally  our construction admits a trivially parallelizable sampler
once the parametric posterior samples have been generated (or if c = 0).
Our approach can be seen to blur the boundaries between Bayesian and frequentist inference. Con-
ventionally  the Bayesian approach conditions on data and treats the unknown parameter of interest
as if it were a random variable with some prior on a known model class. The frequentist approach
treats the object of inference as a ﬁxed but unknown constant and characterizes uncertainty through
the ﬁnite sample variability of an estimator targeting this value. Here we randomize an objective
function (an estimator) according to a Bayesian nonparametric prior on the sampling distribution 
leading to a quantiﬁcation of subjective beliefs on the value that would be returned by the estimator
under an inﬁnite sample size.
At the heart of our approach is the notion of Bayesian updating via randomized objective functions
through the posterior bootstrap. The posterior bootstrap acts on an augmented dataset  comprised
of data and posterior pseudo-samples  under which randomized maximum likelihood estimators
provide a well-motivated quantiﬁcation of uncertainty while assuming little about the data-generating
mechanism. The precursor to this is the weighted likelihood bootstrap  which utilized a simpler
form of randomization that ignored prior information. Our approach provides scope for quantifying
uncertainty for more general machine learning models by randomizing their objective functions
suitably.

9

llllllllllllllllllllllllllllllllllsemeionspambasestatlog−australian−creditstatlog−german−creditstatlog−heartstatlog−landsatbankcardiotocography−10clasescardiotocography−3claseshill−valleyilpd−indian−liverlibrasmusk−20.0000.0050.0100.0150.0200.0250.00.10.20.000.010.02−0.040.000.04−0.10−0.050.000.05−0.050.000.05−0.0250.0000.0250.050−0.010.000.010.020.030.04−0.040.000.040.000.020.040.000.01−0.010−0.0050.0000.0050.0100.0000.0250.050AccuracyMethodBRF (c=0)BRF (c>0)RF−all−11q−11q−11q−11q0.000.250.500.75−1.0−0.50.00.5xExpected lossAcknowledgements

SL is funded by the EPSRC OxWaSP CDT  through EP/L016710/1. CH gratefully acknowledges
support for this research from the MRC  The Alan Turing Institute  and the Li Ka Shing foundation.

References
[1] Hirotugu Akaike. Likelihood of a model and information criteria. Journal of Econometrics 

16(1):3–14  1981.

[2] Charles E. Antoniak. Mixtures of Dirichlet Processes with Applications to Bayesian Nonpara-

metric Problems. Annals of Statistics  2(6):1152–1174  1974.

[3] José M. Bernardo and Adrian F. M. Smith. Bayesian Theory. Wiley  Chichester  2006.

[4] Christopher M. Bishop. Pattern Recognition and Machine Learning. Information Science and

Statistics. Springer  2006.

[5] P. G. Bissiri  C. C. Holmes  and S. G. Walker. A general framework for updating belief
distributions. Journal of the Royal Statistical Society. Series B: Statistical Methodology 
78(5):1103–1130  2016.

[6] David M. Blei  Alp Kucukelbir  and Jon D. McAuliffe. Variational Inference: A Review for

Statisticians. Journal of the American Statistical Association  112(518):859–877  2017.

[7] Leo Breiman. Random Forests. Machine Learning  pages 1–33  2001.

[8] K. P. Burnham and D. R. Anderson. Model Selection and Multimodel Inference: A Practical

Information-Theoretic Approach. Springer New York  2003.

[9] Gary Chamberlain and Guido W. Imbens. Nonparametric Applications of Bayesian Inference.

Journal of Business & Economic Statistics  21(1):12–18  2003.

[10] Dua Dheeru and Eﬁ Karra Taniskidou. UCI Machine Learning Repository  2017.

[11] Manuel Fernández-Delgado  Eva Cernadas  Senén Barro  and Dinani Amorim. Do we Need
Hundreds of Classiﬁers to Solve Real World Classiﬁcation Problems? Journal of Machine
Learning Research  15:3133–3181  2014.

[12] Tadayoshi Fushiki. Bootstrap prediction and Bayesian prediction under misspeciﬁed models.

Bernoulli  11(4):747–758  2005.

[13] Tadayoshi Fushiki. Bayesian bootstrap prediction. Journal of Statistical Planning and Inference 

140(1):65–74  2010.

[14] Nils L. Hjort  Chris Holmes  Peter Müller  and Stephen G. Walker. Bayesian Nonparametrics.
Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press 
2010.

[15] Peter J. Huber. The behavior of maximum likelihood estimates under nonstandard conditions.
In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability 
Volume 1: Statistics  pages 221–233  Berkeley  Calif.  1967. University of California Press.

[16] Tommi S. Jaakkola and Michael I. Jordan. A variational approach to Bayesian logistic regression
models and their extensions. International Workshop on Artiﬁcial Intelligence and Statistics 
1997.

[17] Alp Kucukelbir  Rajesh Ranganath  Andrew Gelman  and David M. Blei. Automatic Variational
Inference in Stan. In Advances in Neural Information Processing Systems 28  volume 1  pages
1–9  2015.

[18] Albert Y. Lo. On a Class of Bayesian Nonparametric Estimates: I. Density Estimates. Annals of

Statistics  12(1):351–357  1984.

10

[19] S. P. Lyddon  C. C. Holmes  and S. G. Walker. General Bayesian Updating and the Loss-

Likelihood Bootstrap. arxiv preprint 1709.07616  pages 1–14  2018.

[20] Ulrich K. Müller. Risk of Bayesian Inference in Misspeciﬁed Models  and the Sandwich

Covariance Matrix. Econometrica  81(5):1805–1849  2013.

[21] Michael A. Newton and Adrian E. Raftery. Approximate Bayesian Inference with the Weighted
Likelihood Bootstrap. Journal of the Royal Statistical Society Series B-Statistical Methodology 
56(1):3–48  1994.

[22] Nicholas G. Polson  James G. Scott  and Jesse Windle. Bayesian Inference for Logistic
Models Using Pólya–Gamma Latent Variables. Journal of the American Statistical Association 
108(504):1339–1349  2013.

[23] Christian P. Robert. The Bayesian Choice: From Decision-Theoretic Foundations to Computa-

tional Implementation. Springer Texts in Statistics. Springer New York  2007.

[24] Christian P. Robert and George Casella. Monte Carlo Statistical Methods. Springer Texts in

Statistics. Springer New York  2005.

[25] Jayaram Sethuraman. A constructive deﬁnition of Dirichlet Process Priors. Statistica Sinica 

4:639–650  1994.

[26] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the
log-likelihood function. Journal of Statistical Planning and Inference  90(2):227–244  2000.

[27] Stephen G. Walker. Bayesian inference with misspeciﬁed models. Journal of Statistical

Planning and Inference  143(10):1621–1633  2013.

11

,Behzad Golshan
John Byers
Evimaria Terzi
Chao Qian
Yang Yu
Zhi-Hua Zhou
Minje Jang
Sunghyun Kim
Changho Suh
Sewoong Oh
Simon Lyddon
Stephen Walker
Chris Holmes
Scott Gigante
Adam Charles
Smita Krishnaswamy
Gal Mishne