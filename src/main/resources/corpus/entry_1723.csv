2017,Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes,Often in machine learning  data are collected as a combination of multiple conditions  e.g.  the voice recordings of multiple persons  each labeled with an ID.  How could we build a model that captures the latent information related to  these conditions and generalize to a new one with few data? We present a new model called Latent Variable Multiple Output Gaussian Processes (LVMOGP) that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time. LVMOGP infers the posteriors of Gaussian processes together with a latent space representing the information about different conditions. We derive an efficient variational inference method for LVMOGP for which the computational complexity is as low as sparse Gaussian processes. We show that LVMOGP significantly outperforms related Gaussian process methods on various tasks with both synthetic and real data.,EfÔ¨Åcient Modeling of Latent Information in
Supervised Learning using Gaussian Processes

Zhenwen Dai ‚àó‚Ä°

zhenwend@amazon.com

Mauricio A. √Ålvarez ‚Ä†

mauricio.alvarez@sheffield.ac.uk

Neil D. Lawrence ‚Ä†‚Ä°
lawrennd@amazon.com

Abstract

Often in machine learning  data are collected as a combination of multiple condi-
tions  e.g.  the voice recordings of multiple persons  each labeled with an ID. How
could we build a model that captures the latent information related to these condi-
tions and generalize to a new one with few data? We present a new model called
Latent Variable Multiple Output Gaussian Processes (LVMOGP) that allows to
jointly model multiple conditions for regression and generalize to a new condition
with a few data points at test time. LVMOGP infers the posteriors of Gaussian
processes together with a latent space representing the information about different
conditions. We derive an efÔ¨Åcient variational inference method for LVMOGP for
which the computational complexity is as low as sparse Gaussian processes. We
show that LVMOGP signiÔ¨Åcantly outperforms related Gaussian process methods
on various tasks with both synthetic and real data.

1

Introduction

Machine learning has been very successful in providing tools for learning a function mapping from an
input to an output  which is typically referred to as supervised learning. One of the most pronouncing
examples currently is deep neural networks (DNN)  which empowers a wide range of applications
such as computer vision  speech recognition  natural language processing and machine translation
[Krizhevsky et al.  2012  Sutskever et al.  2014]. The modeling in terms of function mapping assumes
a one/many to one mapping between input and output. In other words  ideally the input should
contain sufÔ¨Åcient information to uniquely determine the output apart from some sensory noise.
Unfortunately  in most cases  this assumption does not hold. We often collect data as a combination
of multiple scenarios  e.g.  the voice recording of multiple persons  the images taken from different
models of cameras. We only have some labels to identify these scenarios in our data  e.g.  we can
have the names of the speakers and the speciÔ¨Åcations of the used cameras. These labels themselves
do not represent the full information about these scenarios. A question therefore is how to use
these labels in a supervised learning task. A common practice in this case would be to ignore the
difference of scenarios  but this will result in low accuracy of modeling  because all the variations
related to the different scenarios are considered as the observation noise  as different scenarios are not
distinguishable anymore in the inputs . Alternatively  we can either model each scenario separately 
which often suffers from too small training data  or use a one-hot encoding to represent each scenario.
In both of these cases  generalization/transfer to new scenario is not possible.

‚àóInferentia Limited.
‚Ä†Dept. of Computer Science  University of ShefÔ¨Åeld  ShefÔ¨Åeld  UK.
‚Ä°Amazon.com. The scientiÔ¨Åc idea and a preliminary version of code were developed prior to joining Amazon.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

(a)

(b)

(c)

(d)

(e)

Figure 1: A toy example about modeling the braking distance of a car. (a) illustrating a car with
the initial speed v0 on a Ô¨Çat road starts to brake due to the friction force Fr. (b) the results of a GP
regression on all the data from 10 different road and tyre conditions. (c) The top plot visualizes the
Ô¨Åtted model with respect to one of the conditions in the training data and the bottom plot shows the
prediction of the trained model for a new condition with only one observation. The model assumes
every condition independently. (d) LVMOGP captures the correlation among different conditions and
the plot shows the curve with respect to one of the conditions. By using the information from all the
conditions  it is able to predict in a new condition with only one observation.(e) The learned latent
variable with uncertainty corresponds to a linear transformation of the inverse of the true friction
coefÔ¨Åcient (¬µ). The blue error bars denote the variational posterior of the latent variables q(H).

In this paper  we address this problem by proposing a probabilistic model that can jointly consider
different scenarios and enables efÔ¨Åcient generalization to new scenarios. Our model is based on
Gaussian Processes (GP) augmented with additional latent variables. The model is able to represent
the data variance related to different scenarios in the latent space  where each location corresponds
to a different scenario. When encountering a new scenario  the model is able to efÔ¨Åcient infer the
posterior distribution of the location of the new scenario in the latent space. This allows the model
to efÔ¨Åciently and robustly generalize to a new scenario. An efÔ¨Åcient Bayesian inference method of
the propose model is developed by deriving a closed-form variational lower bound for the model.
Additionally  with assuming a Kronecker product structure in the variational posterior  the derived
stochastic variational inference method achieves the same computational complexity as a typical
sparse Gaussian process model with independent output dimensions.

2 Modeling Latent Information

2.1 A Toy Problem

Let us consider a toy example where we wish to model the braking distance of a car in a completely
data-driven way. Assuming that we do not know physics about car  we could treat it as a non-
parametric regression problem  where the input is the initial speed read from the speedometer and
the output is the distance from the location where the car starts to brake to the point where the car is
fully stopped. We know that the braking distance depends on the friction coefÔ¨Åcient  which varies
according to the condition of the tyres and road. As the friction coefÔ¨Åcient is difÔ¨Åcult to measure
directly  we can conduct experiments with a set of different tyre and road conditions  each associated
with a condition id  e.g.  ten different conditions  each has Ô¨Åve experiments with different initial
speeds. How can we model the relation between the speed and distance in a data-driven way  so that
we can extrapolate to a new condition with only one experiment?
Denote the speed to be x  the observed braking distance to be y  and the condition id to be d. A
straight-forward modeling choice is to ignore the difference in conditions. Then  the relation between

2

ùë£"ùêπ$0246810Speed‚àí200204060BrakingDistanceMeanDataConÔ¨Ådence010Distancegroundtruthdata0246810Speed010Distance010BrakingDistancegroundtruthdata0246810InitialSpeed010BrakingDistance2.55.07.510.01/¬µ‚àí2‚àí101LatentVariablethe speed and the distance can be modeled as

y = f (x) +  

f ‚àº GP 

(1)

where  represents measurement noise  and the function f is modeled as a Gaussian Process (GP).
Since we do not know the parametric form of the function  we model it non-parametrically. The
drawback of this model is that the accuracy is very low as all the variations caused by different
conditions are modeled as measurement noise (see Figure 1b). Alternatively  we can model each
condition separately  i.e.  fd ‚àº GP  d = 1  . . .   D  where D denotes the number of considered
conditions. In this case  the relation between speed and distance for each condition can be modeled
cleanly if there are sufÔ¨Åcient data in that condition. However  such modeling is not able to generalize
to new conditions (see Figure 1c)  because it does not consider the correlations among conditions.
Ideally  we wish to model the relation together with the latent information associated with different
conditions  i.e.  the friction coefÔ¨Åcient in this example. A probabilistic approach is to assume a latent
variable. With a latent variable hd that represents the latent information associated with the condition
d  the relation between speed and distance for the condition d is  then  modeled as

y = f (x  hd) +  

f ‚àº GP  hd ‚àº N (0  I).

(2)

Note that the function f is shared across all the conditions like in (1)  while for each condition a
different latent variable hd is inferred. As all the conditions are jointly modeled  the correlation
among different conditions are correctly captured  which enables generalization to new conditions
(see Figure 1d for the results of the proposed model).
This model enables us to capture the relation between the speed  distance as well as the latent
information. The latent information is learned into a latent space  where each condition is encoded
as a point in the latent space. Figure 1e shows how the model ‚Äúdiscovers" the concept of friction
coefÔ¨Åcient by learning the latent variable as a linear transformation of the inverse of the true friction
coefÔ¨Åcients. With this latent representation  we are able to infer the posterior distribution of a new
condition given only one observation and it gives reasonable prediction for the speed-distance relation
with uncertainty.

2.2 Latent Variable Multiple Output Gaussian Processes
In general  we denote the set of inputs as X = [x1  . . .   xN ](cid:62)  which corresponds to the speed in
the toy example  and each input xn can be considered in D different conditions in the training data.
For simplicity  we assume that  given an input xn  the outputs associated with all the D conditions
are observed  denoted as yn = [yn1  . . .   ynD](cid:62) and Y = [y1  . . .   yN ](cid:62). The latent variables
representing different conditions are denoted as H = [h1  . . .   hD](cid:62)  hd ‚àà RQH . The dimensionality
of the latent space QH needs to be pre-speciÔ¨Åed like in other latent variable models. The more general
case where each condition has a different set of inputs and outputs will be discussed in Section 4.
Unfortunately  inference of the model in (2) is challenging  because the integral for computing the
analytical intractability  the computation of the likelihood p(Y|X  H) is also very expensive  because
of its cubic complexity O((N D)3). To enable efÔ¨Åcient inference  we propose a new model which
assumes the covariance matrix can be decomposed as a Kronecker product of the covariance matrix
of the latent variables KH and the covariance matrix of the inputs KX. We call the new model Latent
Variable Multiple Output Gaussian Processes (LVMOGP) due to its connection with multiple output
Gaussian processes. The probabilistic distributions of LVMOGP are deÔ¨Åned as

marginal likelihood  p(Y|X) =(cid:82) p(Y|X  H)p(H)dH  is analytically intractable. Apart from the

(3)
where the latent variables H have unit Gaussian priors  hd ‚àº N (0  I)  F = [f1  . . .   fN ](cid:62)  fn ‚àà RD
denote the noise-free observations  the notation ":" represents the vectorization of a matrix  e.g. 
Y: = vec(Y) and ‚äó denotes the Kronecker product. KX denotes the covariance matrix computed
on the inputs X with the kernel function kX and KH denotes the covariance matrix computed on the
latent variable H with the kernel function kH. Note that the deÔ¨Ånition of LVMOGP only introduces
a Kronecker product structure in the kernel  which does not directly avoid the intractability of its
marginal likelihood. In the next section  we will show how the Kronecker product structure can be
used for deriving an efÔ¨Åcient variational lower bound.

p(Y:|F:) = N(cid:0)Y:|F:  œÉ2I(cid:1)  

p(F:|X  H) = N(cid:0)F:|0  KH ‚äó KX(cid:1)  

3

3 Scalable Variational Inference

The exact inference of LVMOGP in (3) is analytically intractable due to an integral of the latent
variable in the marginal likelihood. Titsias and Lawrence [2010] develop a variational inference
method by deriving a closed-form variational lower bound for a Gaussian process model with latent
variables  known as Bayesian Gaussian process latent variable model. Their method is applicable to
a broad family of models including the one in (2)  but is not efÔ¨Åcient for LVMOGP because it has
cubic complexity with respect to D.4 In this section  we derive a variational lower bound that has
the same complexity as a sparse Gaussian process assuming independent outputs by exploiting the
Kronecker product structure of the kernel of LVMOGP.
We augment the model with an auxiliary variable  known as the inducing variable U  following
the same Gaussian process prior p(U:) = N (U:|0  Kuu). The covariance matrix Kuu is deÔ¨Åned
as Kuu = KH
uu following the assumption of the Kronecker product decomposition in (3) 
m ‚àà RQH with
where KH
uu is computed on another set of inducing inputs ZX =
the kernel function kH. Similarly  KX
m has the same dimensionality as
[zX
the inputs xn. We construct the conditional distribution of F as:

uu is computed on a set of inducing inputs ZH = [zH
m ‚àà RQX with the kernel function kX  where zX

uu ‚äó KX

1   . . .   zX

1   . . .   zH

](cid:62)  zX

](cid:62)  zH

MX

MH

p(F|U  ZX   ZH   X  H) = N(cid:0)F:|Kf uK‚àí1

(cid:1)  

uu K(cid:62)

f u

(4)

f u ‚äó KX

f u and Kf f = KH

because p(F|X  H) =(cid:82) p(F|U  ZX   ZH   X  H)p(U|ZX   ZH )dU. Assuming variational posteriors

where Kf u = KH
f u is the cross-covariance computed
between X and ZX with kX and KH
f u is the cross-covariance computed between H and ZH with
kH. Kf f is the covariance matrix computed on X with kX and KH
f f is computed on H with kH.
Note that the prior distribution of F after marginalizing U is not changed with the augmentation 
q(F|U) = p(F|U  X  H) and q(H)  the lower bound of the log marginal likelihood can be derived
as
(5)
where F = (cid:104)log p(Y:|F:)(cid:105)p(F|U X H)q(U)q(H). It is known that the optimal posterior distribution of
q(U) is a Gaussian distribution [Titsias  2009  Matthews et al.  2016]. With an explicit Gaussian

deÔ¨Ånition of q(U) = N(cid:0)U|M  Œ£U(cid:1)  the integral in F has a closed-form solution:

log p(Y|X) ‚â• F ‚àí KL (q(U)(cid:107) p(U)) ‚àí KL (q(H)(cid:107) p(H))  

uu U:  Kf f ‚àí Kf uK‚àí1
f f . KX

f f ‚äó KX

F = ‚àí N D
2
œÉ2 Y(cid:62)
1

log 2œÄœÉ2 ‚àí 1
: Œ®K‚àí1

2œÉ2 Y(cid:62)
uu M: ‚àí 1
2œÉ2

+

: Y: ‚àí 1

2œÉ2 Tr(cid:0)K‚àí1
(cid:0)œà ‚àí tr(cid:0)K‚àí1
uu Œ¶(cid:1)(cid:1)  
(cid:68)

uu Œ¶K‚àí1
uu (M:M(cid:62)
(cid:69)

: + Œ£U )(cid:1)

(6)

where œà = (cid:104)tr (Kf f )(cid:105)q(H)  Œ® = (cid:104)Kf u(cid:105)q(H) and Œ¶ =
.5 Note that the optimal
variational posterior of q(U) with respect to the lower bound can be computed in closed-form.
However  the computational complexity of the closed-form solution is O(N DM 2

f uKf u

K(cid:62)

q(H)

X M 2

H ).

3.1 More EfÔ¨Åcient Formulation

Note that the lower bound in (5-6) does not take advantage of the Kronecker product decomposition.
The computational efÔ¨Åciency could be improved by avoiding directly computing the Kronecker
product of the covariance matrices. Firstly  we reformulate the expectations of the covariance
matrices œà  Œ® and Œ¶  so that the expectation computation can be decomposed 

œà = œàHtr(cid:0)KX
(cid:17)(cid:69)
(cid:16)
(cid:68)

f f

(cid:1)   Œ® = Œ®H ‚äó KX
(cid:69)

(cid:68)

f u  Œ¶ = Œ¶H ‚äó(cid:0)(KX

f u)(cid:1)  
(cid:69)
f u)(cid:62)KX

f u)(cid:62)KH

f u

(cid:68)

tr

KH
f f

where œàH =
. Secondly  we
assume a Kronecker product decomposition of the covariance matrix of q(U)  i.e.  Œ£U = Œ£H ‚äó Œ£X.
Although this decomposition restricts the covariance matrix representation  it dramatically reduces

and Œ¶H =

  Œ®H =

KH
f u

(KH

q(H)

q(H)

q(H)

(7)

4Assume that the number of inducing points is proportional to D.
5The expectation with respect to a matrix (cid:104)¬∑(cid:105)q(H) denotes the expectation with respect to every element of

the matrix.

4

the number of variational parameters in the covariance matrix from M 2
to the above decomposition  the lower bound can be rearranged to speed up the computation 

H to M 2

X + M 2

X M 2

H. Thanks

F = ‚àí N D
2
‚àí 1
‚àí 1
œÉ2 Y(cid:62)
1

+

:

+

uu)‚àí1)M(KH

uu)‚àí1Œ£H(cid:1) tr(cid:0)(KX
uu)‚àí1(Œ®H )(cid:62)(cid:1)
uu)‚àí1Œ¶X(cid:1) .

log 2œÄœÉ2 ‚àí 1

uu)‚àí1)M(KH

uu)‚àí1Œ¶H (KH

2œÉ2 Y(cid:62)
: Y:
uu)‚àí1Œ¶C(KX

2œÉ2 tr(cid:0)M(cid:62)((KX
2œÉ2 tr(cid:0)(KH
(cid:0)(Œ®X (KX
uu)‚àí1Œ¶H(cid:1) tr(cid:0)(KX
2œÉ2 tr(cid:0)(KH
(cid:18)
+ tr(cid:0)(KH

|KH
uu|
|Œ£H| + MH log

uu)‚àí1Œ£H(cid:1) tr(cid:0)(KX

MX log

1
2

1

KL (q(U)(cid:107) p(U)) =

Similarly  the KL-divergence between q(U) and p(U) can also take advantage of the above decom-
position:

uu)‚àí1(cid:1)
uu)‚àí1Œ£X(cid:1)

uu)‚àí1Œ¶H (KH
uu)‚àí1Œ¶X (KX

: ‚àí 1

2œÉ2 œà

(8)

|KX
uu|

|Œ£X| + tr(cid:0)M(cid:62)(KX
(cid:19)
uu)‚àí1M(KH
uu)‚àí1Œ£X(cid:1) ‚àí MH MX

.

uu)‚àí1(cid:1)

(9)

As shown in the above equations 
the direct computation of Kronecker products is com-
pletely avoided. Therefore  the computational complexity of the lower bound is reduced to
O(max(N  MH ) max(D  MX ) max(MX   MH ))  which is comparable to the complexity of sparse
GPs with independent observations O(N M max(D  M )). The new formulation is signiÔ¨Åcantly
more efÔ¨Åcient than the formulation described in the previous section. This enables LVMOGP to be
applicable to real world scenarios. It is also straight-forward to extend this lower bound to mini-batch
learning like in Hensman et al. [2013]  which allows further scaling up.

3.2 Prediction

(cid:90)
=N(cid:0)F‚àó

: |U:  X‚àó  H‚àó)q(U:)dU:
: |Kf‚àóuK‚àí1

After estimating the model parameters and variational posterior distributions  the trained model is
typically used to make predictions. In our model  a prediction can be about a new input x‚àó as well
as a new scenario which corresponds to a new value of the hidden variable h‚àó. Given both a set of
new inputs X‚àó with a set of new scenarios H‚àó  the prediction of noiseless observation F‚àó can be
computed in closed-form 
q(F‚àó
p(F‚àó

: |X‚àó  H‚àó) =

f‚àóf‚àó and KX

f‚àóf‚àó ‚äó KX

f‚àóf‚àó and Kf‚àóu = KH

uu K(cid:62)
f‚àóu . KH

f‚àóu + Kf‚àóuK‚àí1
f‚àóf‚àó and KH

uu M:  Kf‚àóf‚àó ‚àí Kf‚àóuK‚àí1
f‚àóu ‚äó KX

uu Œ£U K‚àí1
where Kf‚àóf‚àó = KH
f‚àóu are the covariance
matrices computed on H‚àó and the cross-covariance matrix computed between H‚àó and ZH. Similarly 
f‚àóu are the covariance matrices computed on X‚àó and the cross-covariance matrix
KX
computed between X‚àó and ZX. For a regression problem  we are often more interested in predicting
for the existing condition from the training data. As the posterior distributions of the existing
conditions have already been estimated as q(H)  we can approximate the prediction by integrating the
: |X‚àó  H)q(H)dH. The above integration
above prediction equation with q(H)  q(F‚àó
is intractable  however  as suggested by Titsias and Lawrence [2010]  the Ô¨Årst and second moment of
F‚àó
: under q(F‚àó

: |X‚àó) can be computed in closed-form.

: |X‚àó) =(cid:82) q(F‚àó

uu K(cid:62)
f‚àóu

(cid:1)  

4 Missing Data

The model described in Section 2.2 assumes that for N different inputs  we observe them in all the
D different conditions. However  in real world problems  we often collect data at a different set of
inputs for each scenario  i.e.  for each condition d  d = 1  . . .   D. Alternatively  we can view the
problem as having a large set of inputs and for each condition only the outputs associated with a

5

subset of the inputs being observed. We refer to this problem as missing data. For the condition d 
](cid:62) and the outputs as Yd = [y1d  . . .   yNdd](cid:62)  and
we denote the inputs as X(d) = [x(d)
optionally a different noise variance as œÉ2
d. The proposed model can be extended to handle this case
by reformulating the F as
‚àí Nd
2

uu Œ¶dK‚àí1

1   . . .   x(d)
Nd

d Yd ‚àí 1
Y(cid:62)
2œÉ2
d

log 2œÄœÉ2

F =

: + Œ£U )(cid:1)
uu (M:M(cid:62)
(cid:17)
(cid:16)

(10)

KX

  in which

D(cid:88)
d ‚äó(cid:16)

d=1

+

d ‚àí 1
2œÉ2
d
(cid:17)
(cid:68)

1
œÉ2
d
(KX

Y(cid:62)
d Œ®dK‚àí1
fdu)(cid:62)KX
fdu)
  Œ®H
d =

(cid:69)

uu M: ‚àí 1
2œÉ2
d

Tr(cid:0)K‚àí1
(cid:0)œàd ‚àí tr(cid:0)K‚àí1

uu Œ¶d

(cid:1)(cid:1)  
(cid:16)
(cid:68)

(cid:68)

where Œ¶d = Œ¶H

  Œ®d = Œ®H

fdu

(KH

d =

fdu)(cid:62)KH

fdfd
. The rest of the
Œ¶H
lower bound remains unchanged because it does not depend on the inputs and outputs. Note that 
although it looks very similar to the bound in Section 3  the above lower bound is computationally
more expensive  because it involves the computation of a different set of Œ¶d  Œ®d  œàd and the
corresponding part of the lower bound for each condition.

KH

q(hd)

q(hd)

q(hd)

fdu

(cid:69)

d ‚äó KX
and œàH

fdu  œàd = œàH
KH
d =

tr

fdfd

(cid:17)(cid:69)
d ‚äó tr

5 Related works

LVMOGP can be viewed as an extension of a multiple output Gaussian process. Multiple output
Gaussian processes have been thoughtfully studied in √Ålvarez et al. [2012]. LVMOGP can be seen as
an intrinsic model of coregionalization [Goovaerts  1997] or a multi-task Gaussian process [Bonilla
et al.  2008]  if the coregionalization matrix B is replaced by the kernel KH. By replacing the
coregionalization matrix with a kernel matrix  we endow the multiple output GP with the ability to
predict new outputs or tasks at test time  which is not possible if a Ô¨Ånite matrix B is used at training
time. Also  by using a model for the coregionalization matrix in the form of a kernel function  we
reduce the number of hyperparameters necessary to Ô¨Åt the covariance between the different conditions 
reducing overÔ¨Åtting when fewer datapoints are available for training. Replacing the coregionalization
matrix by a kernel matrix has also been used in Qian et al. [2008] and more recently by Bussas et al.
[2017]. However  these works do not address the computational complexity problem and their models
can not scale to large datasets. Furthermore  in our model  the different conditions hd are treated as
latent variables  which are not observed  as opposed to these two models where we would need to
provide observed data to compute KH.
Computational complexity in multi-output Gaussian processes has also been studied before for
convolved multiple output Gaussian processes [√Ålvarez and Lawrence  2011] and for the intrinsic
model of coregionalization [Stegle et al.  2011]. In √Ålvarez and Lawrence [2011]  the idea of inducing
inputs is also used and computational complexity reduces to O(N DM 2)  where M refers to a generic
number of inducing inputs. In Stegle et al. [2011]  the covariances KH and KX are replaced by their
respective eigenvalue decompositions and computational complexity reduces to O(N 3 + D3). Our
method reduces computationally complexity to O(max(N  MH ) max(D  MX ) max(MX   MH ))
when there are no missing data. Notice that if MH = MX = M  N > M and D > M  our method
achieves a computational complexity of O(N DM )  which is faster than O(N DM 2) in √Ålvarez and
Lawrence [2011]. If N = D = MH = MX  our method achieves a computational complexity of
O(N 3)  similar to Stegle et al. [2011]. Nonetheless  the usual case is that N (cid:29) MX  improving the
computational complexity over Stegle et al. [2011]. An additional advantage of our method is that it
can easily be parallelized using mini-batches like in Hensman et al. [2013]. Note that we have also
provided expressions for dealing with missing data  a setup which is very common in our days  but
that has not been taken into account in previous formulations.
The idea of modeling latent information about different conditions jointly with the modeling of data
points is related to the style and content model by Tenenbaum and Freeman [2000]  where they
explicitly model the style and content separation as a bilinear model for unsupervised learning.

6 Experiments

We evaluate the performance of the proposed model with both synthetic and real data.

6

(a)

(b)

(c)

Figure 2: The results on two synthetic datasets. (a) The performance of GP-ind  LMC and LVMOGP
evaluated on 20 randomly drawn datasets without missing data. (b) The performance evaluated on 20
randomly drawn datasets with missing data. (c) A comparison of the estimated functions by the three
methods on one of the synthetic datasets with missing data. The plots show the estimated functions
for one of the conditions with few training data. The red rectangles are the noisy training data and the
black crosses are the test data.

Synthetic Data. We compare the performance of the proposed method with GP with independent ob-
servations and the linear model of coregionalization (LMC) [Journel and Huijbregts  1978  Goovaerts 
1997] on synthetic data  where the ground truth is known. We generated synthetic data by sampling
from a Gaussian process  as stated in (3)  and assuming a two-dimensional space for the different
conditions. We Ô¨Årst generated a dataset  where all the conditions of a set of inputs are observed. The
dataset contains 100 different uniformly sampled input locations (50 for training and 50 for testing) 
where each corresponds to 40 different conditions. An observation noise with variance 0.3 is added
onto the training data. This dataset belongs to the case of no missing data  therefore  we can apply
LVMOGP with the inference method presented in Section 3. We assume a 2 dimensional latent
space and set MH = 30 and MX = 10. We compare LVMOGP with two other methods: GP with
independent output dimensions (GP-ind) and LMC (with a full rank coregionalization matrix). We
repeated the experiments on 20 randomly sampled datasets. The results are summarized in Figure
2a. The means and standard deviations of all the methods on 20 repeats are: GP-ind: 0.24 ¬± 0.02 
LMC:0.28¬± 0.11  LVMOGP 0.20¬± 0.02. Note that  in this case  GP-ind performs quite well because
the only gain by modeling different conditions jointly is the reduction of estimation variance from the
observation noise.
Then  we generated another dataset following the same setting  but where each condition had a
different set of inputs. Often  in real data problems  the number of available data in different
conditions is quite uneven. To generate a dataset with uneven numbers of training data in different
conditions  we group the conditions into 10 groups. Within each group  the numbers of training
data in four conditions are generated through a three-step stick breaking procedure with a uniform
prior distribution (200 data points in total). We apply LVMOGP with missing data (Section 4) and
compare with GP-ind and LMC. The results are summarized in Figure 2b. The means and standard
deviations of all the methods on 20 repeats are: GP-ind: 0.43 ¬± 0.06  LMC:0.47 ¬± 0.09  LVMOGP
0.30 ¬± 0.04. In both synthetic experiments  LMC does not perform well because of overÔ¨Åtting
caused by estimating the full rank coregionalization matrix. The Ô¨Ågure 2c shows a comparison of
the estimated functions by the three methods for a condition with few training data. Both LMC and
LVMOGP can leverage the information from other conditions to make better predictions  while LMC
often suffers from overÔ¨Åtting due to the high number of parameters in the coregionalization matrix.
Servo Data. We apply our method to a servo modeling problem  in which the task is to predict the
rise time of a servomechanism in terms of two (continuous) gain settings and two (discrete) choices of
mechanical linkages [Quinlan  1992]. The two choices of mechanical linkages introduce 25 different
conditions in the experiments (Ô¨Åve types of motors and Ô¨Åve types of lead screws). The data in each
condition are scarce  which makes joint modeling necessary (see Figure 3a). We took 70% of the
dataset as training data and the rest as test data  and randomly generated 20 partitions. We applied
LVMOGP with a two-dimensional latent space with an ARD kernel and used Ô¨Åve inducing points
for the latent space and 10 inducing points for the function. We compared LVMOGP with GP with
ignoring the different conditions (GP-WO)  GP with taking each condition as an independent output
(GP-ind)  GP with one-hot encoding of conditions (GP-OH) and LMC. The means and standard
deviations of the RMSE of all the methods on 20 partitions are: GP-WO: 1.03 ¬± 0.20  GP-ind:

7

GP-indLMCLVMOGP0.20.30.40.50.60.7RMSEGP-indLMCLVMOGP0.20.30.40.50.60.7RMSEGP-ind‚àí202testtrainLMC‚àí202‚àí0.20.00.20.40.60.81.01.2LVMOGP‚àí202(a)

(c)

(b)

(d)

Figure 3: The experimental results on servo data and sensor imputation. (a) The numbers of data
points are scarce in each condition. (b) The performance of a list of methods on 20 different train/test
partitions is shown in the box plot. (c) The function learned by LVMOGP for the condition with the
smallest amount of data. With only one training data  the method is able to extrapolate a non-linear
function due to the joint modeling of all the conditions. (d) The performance of three methods on
sensor imputation with 20 repeats.

1.30 ¬± 0.31  GP-OH: 0.73 ¬± 0.26  LMC:0.69 ¬± 0.35  LVMOGP 0.52 ¬± 0.16. Note that in some
conditions the data are very scarce  e.g.  there are only one training data point and one test data
point (see Figure 3c). As all the conditions are jointly modeled in LVMOGP  the method is able to
extrapolate a non-linear function by only seeing one data point.
Sensor Imputation. We apply our method to impute multivariate time series data with massive
missing data. We take a in-house multi-sensor recordings including a list of sensor measurements such
as temperature  carbon dioxide  humidity  etc. [Zamora-Mart√≠nez et al.  2014]. The measurements
are recorded every minute for roughly a month and smoothed with 15 minute means. Different
measurements are normalized to zero-mean and unit-variance. We mimic the scenario of massive
missing data by randomly taking out 95% of the data entries and aim at imputing all the missing
values. The performance is measured as RMSE on the imputed values. We apply LVMOGP with
missing data with the settings: QH = 2  MH = 10 and MX = 100. We compare with LMC and
GP-ind. The experiments are repeated 20 times with different missing values. The results are shown
in a box plot in Figure 3d. The means and standard deviations of all the methods on 20 repeats are:
GP-ind: 0.85 ¬± 0.09  LMC:0.59 ¬± 0.21  LVMOGP 0.45 ¬± 0.02. The high variance of LMC results
are due to the large number of parameters in the coregionalization matrix.

7 Conclusion

In this work  we study the problem of how to model multiple conditions in supervised learning.
Common practices such as one-hot encoding cannot efÔ¨Åciently model the relation among different
conditions and are not able to generalize to a new condition at test time. We propose to solve this
problem in a principled way  where we learn the latent information of conditions into a latent space.
By exploiting the Kronecker product decomposition in the variational posterior  our inference method
is able to achieve the same computational complexity as sparse GPs with independent observations 
when there are no missing data. In experiments on synthetic and real data  LVMOGP outperforms
common approaches such as ignoring condition difference  using one-hot encoding and LMC. In
Figure 3b and 3d  LVMOGP delivers more reliable performance than LMC among different train/test
partitions due to the marginalization of latent variables.

Acknowledgements MAA has been Ô¨Ånanced by the Engineering and Physical Research Council (EPSRC)
Research Project EP/N014162/1.

8

0510152025024681012GP-WOGP-indGP-OHLMCLVMOGP0.51.01.52.0RMSE2.53.03.54.04.55.05.56.06.512345-0.948-0.700-0.452-0.452-0.204-0.2040.0440.0440.2920.2920.5390.5390.7870.787traintestGP-indLMCLVMOGP0.40.50.60.70.80.9RMSEReferences
Mauricio A. √Ålvarez and Neil D. Lawrence. Computationally efÔ¨Åcient convolved multiple output

Gaussian processes. J. Mach. Learn. Res.  12:1459‚Äì1500  July 2011.

Edwin V. Bonilla  Kian Ming Chai  and Christopher K. I. Williams. Multi-task Gaussian process
In John C. Platt  Daphne Koller  Yoram Singer  and Sam Roweis  editors  NIPS 

prediction.
volume 20  2008.

Matthias Bussas  Christoph Sawade  Nicolas K√ºhn  Tobias Scheffer  and Niels Landwehr. Varying-

coefÔ¨Åcient models for geospatial transfer learning. Machine Learning  pages 1‚Äì22  2017.

Pierre Goovaerts. Geostatistics For Natural Resources Evaluation. Oxford University Press  1997.

James Hensman  Nicolo Fusi  and Neil D. Lawrence. Gaussian processes for big data. In UAI  2013.

Andre G. Journel and Charles J. Huijbregts. Mining Geostatistics. Academic Press  1978.

Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiÔ¨Åcation with deep convolu-
tional neural networks. In Advances in Neural Information Processing Systems  pages 1097‚Äì1105 
2012.

Alexander G. D. G. Matthews  James Hensman  Richard E Turner  and Zoubin Ghahramani. On
sparse variational methods and the Kullback-Leibler divergence between stochastic processes. In
AISTATS  2016.

Peter Z. G Qian  Huaiqing Wu  and C. F. Jeff Wu. Gaussian process models for computer experiments

with qualitative and quantitative factors. Technometrics  50(3):383‚Äì396  2008.

J R Quinlan. Learning with continuous classes.

Intelligence  pages 343‚Äì348  1992.

In Australian Joint Conference on ArtiÔ¨Åcial

Oliver Stegle  Christoph Lippert  Joris Mooij  Neil Lawrence  and Karsten Borgwardt. EfÔ¨Åcient
inference in matrix-variate Gaussian models with IID observation noise. In NIPS  pages 630‚Äì638 
2011.

Ilya Sutskever  Oriol Vinyals  and Quoc VV Le. Sequence to sequence learning with neural networks.

In Advances in Neural Information Processing Systems  2014.

JB Tenenbaum and WT Freeman. Separating style and content with bilinear models. Neural

Computation  12:1473‚Äì83  2000.

Michalis K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In

AISTATS  2009.

Michalis K. Titsias and Neil D. Lawrence. Bayesian Gaussian process latent variable model. In

AISTATS  2010.

F. Zamora-Mart√≠nez  P. Romeu  P. Botella-Rocamora  and J. Pardo. On-line learning of indoor
temperature forecasting models towards energy efÔ¨Åciency. Energy and Buildings  83:162‚Äì172 
2014.

Mauricio A. √Ålvarez  Lorenzo Rosasco  and Neil D. Lawrence. Kernels for vector-valued functions:
A review. Foundations and Trends R(cid:13) in Machine Learning  4(3):195‚Äì266  2012. ISSN 1935-8237.
doi: 10.1561/2200000036. URL http://dx.doi.org/10.1561/2200000036.

9

,Zhenwen Dai
Mauricio √Ålvarez
Neil Lawrence