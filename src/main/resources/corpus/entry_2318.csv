2013,Global MAP-Optimality by Shrinking the Combinatorial Search Area with Convex Relaxation,We consider energy minimization for undirected graphical models  also known as MAP-inference problem for Markov random fields. Although combinatorial methods  which return a provably optimal integral solution of the problem  made a big progress in the past decade  they are still typically unable to cope with large-scale datasets. On the other hand  large scale datasets are typically defined on sparse graphs  and convex relaxation methods  such as linear programming relaxations often provide good approximations to integral solutions.   We propose a novel method of combining combinatorial and convex programming techniques to obtain a global solution of the initial combinatorial problem. Based on the information obtained from the solution of the convex relaxation  our method confines application of the combinatorial solver to a small fraction of the initial graphical model  which allows to optimally solve big problems.   We demonstrate the power of our approach on a computer vision energy minimization benchmark.,Global MAP-Optimality by Shrinking the

Combinatorial Search Area with Convex Relaxation

Bogdan Savchynskyy1

J¨org Kappes2

Paul Swoboda2

Christoph Schn¨orr1 2

1Heidelberg Collaboratory for Image Processing  Heidelberg University  Germany

bogdan.savchynskyy@iwr.uni-heidelberg.de

2Image and Pattern Analysis Group  Heidelberg University  Germany
{kappes swoboda schnoerr}@math.uni-heidelberg.de

Abstract

We consider energy minimization for undirected graphical models  also known as
the MAP-inference problem for Markov random ﬁelds. Although combinatorial
methods  which return a provably optimal integral solution of the problem  made a
signiﬁcant progress in the past decade  they are still typically unable to cope with
large-scale datasets. On the other hand  large scale datasets are often deﬁned on
sparse graphs and convex relaxation methods  such as linear programming relax-
ations then provide good approximations to integral solutions.
We propose a novel method of combining combinatorial and convex program-
ming techniques to obtain a global solution of the initial combinatorial problem.
Based on the information obtained from the solution of the convex relaxation  our
method conﬁnes application of the combinatorial solver to a small fraction of the
initial graphical model  which allows to optimally solve much larger problems.
We demonstrate the efﬁcacy of our approach on a computer vision energy mini-
mization benchmark.

1

Introduction

(cid:88)

v∈VG

(cid:88)

The focus of this paper is energy minimization for Markov random ﬁelds. In the most common
pairwise case this problem reads

uv∈EG

min
x∈XG

θv(xv) +

θuv(xu  xv)  

EG θ(x) := min
x∈XG

(1)
where G = (VG EG) denotes an undirected graph with the set of nodes VG (cid:51) v and the set of
edges EG (cid:51) uv; variables xv belong to the ﬁnite label sets Xv  v ∈ VG; potentials θv : Xv → R 
θuv : Xu×Xv → R  v ∈ VG  uv ∈ EG  are associated with the nodes and the edges of G respectively.
We denote by XG the Cartesian product ⊗v∈VGXv.
Problem (1) is known to be NP-hard in general  hence existing methods either consider its convex
relaxations or/and apply combinatorial techniques such as branch-and-bound  combinatorial search 
cutting plane etc. on top of convex relaxations. The main contribution of this paper is a novel
method to combine convex and combinatorial approaches to compute a provably optimal solution.
The method is very general in the sense that it is not restricted to a speciﬁc convex programming
or combinatorial algorithm  although some algorithms are more preferable than others. The main
restriction of the method is the neighborhood structure of the graph G: it has to be sparse. Basic grid
graphs of image data provide examples satisfying this requirement. The method is applicable also to
higher-order problems  deﬁned on so called factor graphs [1]  however we will concentrate mainly
on the pairwise case to keep our exposition simple.
Underlying idea. Fig. 1 demonstrates the main idea of our method. Let A and B be two subgraphs
covering G. Select them so that the only common nodes of these subgraphs lie on their mutual border

1

Solve A and B separately

Check consistency on ∂A

Increase B

A\∂A
B\∂A
∂A ≡ ∂B

label mismatch

A and x∗

A and x∗

A and x∗

A and x∗
B. This process is repeated until either labelings x∗

Figure 1: Underlying idea of the proposed method: the initial graph is split into two subgraphs A
(blue+yellow) and B (red+yellow)  assigned to a convex and a combinatorial solver respectively. If
the integral solutions provided by both solvers do not coincide on the common border ∂A (yellow)
of the two subgraphs  the subgraph B is increased by appending mismatching nodes (green) and the
border is adjusted respectively.
∂A(≡ ∂B) deﬁned in terms of the master-graph G. Let x∗
B be optimal labelings computed
independently on A and B. If these labelings coincide on the border ∂A  then under some additional
conditions the concatenation of x∗
B is an optimal labeling for the initial problem (1)  as we
show in Section 3 (see Theorem 1).
We select the subgraph A such that it contains a ”simple“ part of the problem  for which the convex
relaxation is tight. This part is assigned to the respective convex program solver. The subgraph
B contains in contrast the difﬁcult  combinatorial subproblem and is assigned to a combinatorial
B do not coincide on some border node v ∈ ∂A  we (i) increase the
solver. If the labelings x∗
subgraph B by appending the node v and edges from v to B  (ii) correspondingly decrease A and
(iii) recompute x∗
A and x∗
B coincide on
the border or B equals G. The sparsity of G is required to avoid fast growth of the subgraph B.
We refer to Section 3 for a detailed description of the algorithm  where we in particular specify the
initial selection of the subgraphs A and B and the methods for (i) encouraging consistency of x∗
A
and x∗
B on the boundary ∂A and (ii) providing equivalent results with just a single run of the convex
relaxation solver. These techniques will be described for the local polytope relaxation  known also
as a linear programming relaxation of (1) [2  3].
Related work. The literature on problem (1) is very broad  both regarding convex programming and
combinatorial methods. Here we will concentrate on the local polytope relaxation  that is essential
to our approach.
The local polytope relaxation (LP) of (1) was proposed and analyzed in [4] (see also the recent
review [2]). An alternative view on the same relaxation was proposed in [5]. This view appeared to
be very close to the idea of the Lagrangian or dual decomposition technique (see [6] for applications
to (1)). This idea stimulated development of efﬁcient solvers for convex relaxations of (1). Scalable
solvers for the LP relaxation became a hot topic in recent years [7–14]. The algorithms however 
which guarantee attainment of the optimum of the convex relaxation at least theoretically  are quite
slow in practice  see e.g. comparisons in [11  15]. Remarkably  the fastest scalable algorithms
for convex relaxations are based on coordinate descent: the diffusion algorithm [2] known from
the seventies and especially its dual decomposition based variant TRW-S [16]. There are other
closely related methods [17  18] based on the same principle. Although these algorithms do not
guarantee attainment of the optimum  they converge [19] to points fulﬁlling a condition known as
arc consistency [2] or weak tree agreement [16]. We show in Section 3 that this condition plays a
signiﬁcant role for our approach. It is a common observation that in the case of sparse graphs and/or
strong evidence of the unary terms θv  v ∈ VG  the approximate solutions delivered by such solvers
are quite good from the practical viewpoint. The belief  that these solutions are close to optimal
ones is evidenced by numerical bounds  which these solvers provide as a byproduct.
The techniques used in combinatorial solvers specialized to problem (1) include most of the clas-
sical tools: cutting plane  combinatorial search and branch-and-bound methods were adapted to the
problem (1). The ideas of the cutting plane method form the basis for tightening the LP relaxation
within the dual decomposition framework (see the recent review [20] and references therein) and
for ﬁnding an exact solution for Potts models [21]  which is a special class of problem (1). Com-
binatorial search methods with dynamic programming based heuristics were successfully applied

2

to problems deﬁned on dense and fully connected but small graphs [22]. The specialized branch-
and-bound solvers [23  24] also use convex (mostly LP) relaxations and/or a dynamic programming
technique to produce bounds in the course of the combinatorial search [25]. However the reported
applicability of most combinatorial solvers nowadays is limited to small graphs. Specialized solvers
like [21] scale much better  but are focused on a certain narrow class of problems.
The goal of this work is to employ the fact  that local polytope solvers provide good approximate
solutions and to restrict computational efforts of combinatorial solvers to a relatively small  and
hence tractable part of the initial problem.
Contribution. We propose a novel method for obtaining a globally optimal solution of the energy
minimization problem (1) for sparse graphs and demonstrate its performance on a series of large-
scale benchmark datasets. We were able to

• solve previously unsolved large-scale problems of several different types  and
• attain optimal solutions of hard instances of Potts models an order of magnitude faster than

specialized state of the art algorithms [21].

For an evaluation of our method we use datasets from the very recent benchmark [15].
Paper structure. In Section 2 we provide the deﬁnitions for the local polytope relaxation and arc
consistency. Section 3 is devoted to the speciﬁcation of our algorithm. In Sections 4 and 5 we
provide results of the experimental evaluation and conclusions.

2 Preliminaries
Notation. A vector x with coordinates xv  v ∈ VG  will be called labeling and its coordinates
xv ∈ Xv – labels. The notation x|W  W ⊂ VG stands for the restriction of x to the subset W  i.e.
for the subvector (xv  v ∈ W). To shorten notation we will sometimes write xuv ∈ Xuv in place
of (xv  xu) ∈ Xu × Xv for (v  u) ∈ EG. Let also nb(v)  v ∈ VG  denote the set of neighbors of
node v  that is the set {u ∈ VG : uv ∈ EG}.
LP relaxation. The local polytope relaxation of (1) reads (see e.g. [2])

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:80)
θv(xv)µv(xv) +
(cid:80)
v∈VG
xv∈VG µv(xv) = 1  v ∈ VG
(cid:80)
xv∈VG µuv(xu  xv) = µu(xu)  xu ∈ Xu  uv ∈ EG
xu∈VG µuv(xu  xv) = µv(xv)  xv ∈ Xv  uv ∈ EG .

(xu xv)∈Xuv

xv∈Xv

uv∈EG

min
µ≥0

s.t.

θuv(xu  xv)µuv(xu  xv)

(2)

(3)

This formulation is based on the overcomplete representation of indicator vectors µ constrained
to the local polytope commonly used for discrete graphical models [3]. It is well-known that the
local polytope constitutes an outer bound (relaxation) of the convex hull of all indicator vectors of
labelings (marginal polytope; cf. [3]).
The Lagrange dual of (2) reads

(cid:88)
v∈VG
s.t. γv ≤

max
φ γ

γv +

(cid:88)
v (xv) := θv(xv) −(cid:80)

uv∈EG
˜θφ

γuv

u∈nb(v) φv u(xv) 

v ∈ VG  xv ∈ Xv  

γuv ≤ ˜θφ

to D(φ) := (cid:80)

uv(xu  xv) := θuv(xu  xv) + φv u(xv) + φu v(xu)  uv ∈ EG  (xu  xv) ∈ Xuv .
In the constraints of (3) we introduced the reparametrized potentials ˜θφ. One can see  that for any
values of the dual variables φ the reparametrized energy E˜θφ G(x) is equal to the non-parametrized
one Eθ G(x) for any labeling x ∈ XG. The objective function of the dual problem is equal
˜θφ
w(xw). A
reparametrization  that is reparametrized potentials ˜θφ  will be called optimal  if the corresponding
φ is the solution of the dual problem (3). In general neither the optimal φ is unique nor the optimal
reparametrization.

w ∈ arg minxw∈Xv∪Xuv

v) +(cid:80)

uv)  where x(cid:48)

uv(x(cid:48)
˜θφ

v (x(cid:48)
˜θφ

uv∈EG

v∈VG

3

Deﬁnition 1 (Strict arc consistency). We will call the node v ∈ VG strictly arc consistent w.r.t.
potentials θ if there exist labels x(cid:48)
v) < θv(xv)
for all xv ∈ Xv\{x(cid:48)
u)}. The label
v} and θvu(x(cid:48)
x(cid:48)
v will be called locally optimal.
If all nodes v ∈ VG are strictly arc consistent w.r.t. the potentials ˜θφ  the dual objective value D(φ)
becomes equal to the energy

u ∈ Xu for all u ∈ nb(v)  such that θv(x(cid:48)
v  x(cid:48)

u) < θvu(xv  xu) for all (xv  xu) ∈ Xvu\{(x(cid:48)

v ∈ Xv and x(cid:48)
v  x(cid:48)

D(φ) = EG ˜θφ(x(cid:48)) = EG θ(x(cid:48))

(4)
of the labeling x(cid:48) constructed by the corresponding locally optimal labels. From duality it follows 
that D(φ) is a lower bound for energies of all labelings EG θ(x)  x ∈ XG. Hence attainment of
equality (4) shows that (i) φ is the solution of the dual problem (3) and (ii) x(cid:48) is the solution of both
the energy minimization problem (1) and its relaxation (2).
Strict arc consistency of all nodes is sufﬁcient  but not necessary for attaining the optimum of the
dual objective (3). Its fulﬁllment means that our LP relaxation is tight  which is not always the
case. However  in many practical cases the optimal reparametrization φ corresponds to strict arc
consistency of a signiﬁcant portion of  but not all graph nodes. The remaining non-consistent part is
often much smaller and consists of many separate ”islands“. The strict arc consistency of a certain
node v  even for the optimally reparametrized potentials ˜θφ  does not guarantee global optimality
of the corresponding locally optimal label xv (unless it holds for all nodes)  though it is a good and
widely used heuristic to obtain an approximate solution of the non-relaxed problem (1). In this work
we provide an algorithm  which is able to prove this optimality or discard it. The algorithm applies
combinatorial optimization techniques only to the arc inconsistent part of the model  which is often
much smaller than the whole model in applications.
Remark 1. Efﬁcient dual decomposition based algorithms optimize dual functions  which differ
from (4) (see e.g. [6  13  16])  but are equivalent to it in the sense of equal optimal values. Getting
reparametrizations ˜θφ is less straightforward in these cases  but can be efﬁciently computed (see
e.g. [16  Sec. 2.2]).

3 Algorithm description
The graph A = (VA EA) will be called an (induced) subgraph of the graph G = (VG EG)  if
VA ⊂ VG and EA = {uv ∈ EG : u  v ∈ VA}. The graph G will be called supergraph of A. The
subgraph ∂A induced by a set of nodes V∂A of the graph A  which are connected to VG\VA  is
called its boundary w.r.t. G  i.e. V∂A = {v ∈ VA : ∃uv ∈ EG : u ∈ VG\VA}. The complement B
to A\∂A  given by VB = {v ∈ VG : v ∈ ∂A ∪ (VG\VA)}  EB = {uv ∈ EG : u  v ∈ VB}  is called
boundary complement to A w.r.t. the graph G. Let A be a subgraph of G and potentials θv  v ∈ VG 
and θuv ∈ EG be associated with nodes and edges of G respectively. We assume  that θv  v ∈ VA 
and θuv ∈ EA are associated with the subgraph A. Hence we consider the energy function EA θ to
be deﬁned on A together with an optimal labeling on A  which is the one that minimizes EA θ.
The following theorem formulates conditions necessary to produce an optimal labeling x∗ on the
subgraph G from the optimal labelings on its mutually boundary complement subgraphs A and B.
Theorem 1. Let A be a subgraph of G and B be its boundary complement w.r.t. A. Let x∗
A and
B be labelings minimizing EA θ and EB θ respectively and let all nodes v ∈ VA be strictly arc
x∗
consistent w.r.t. potentials θ. Then from

x∗
A v = x∗

B v for all v ∈ V∂A

(cid:26) x∗

A v 
x∗
B v 

(5)
v ∈ A
v ∈ B\A   v ∈ VG  is optimal on G.

follows that the labeling x∗ with coordinates x∗

v =

(cid:26) 0 

Proof. Let θ denote potentials of
θ(cid:48)
w(xw) :=

w ∈ V∂A ∪ E∂A
θw(xw)  w /∈ V∂A ∪ E∂A

the problem.

Let us deﬁne other potentials θ(cid:48) as
. Then EG θ(x) = EA θ(cid:48)(x|A) + EB θ(x|B). From strict

4

Algorithm 1

(1) Solve LP and reparametrize (G  θ) → (G  ˜θφ).
(2) Initialize: (A  ˜θφ) and x∗
(3) repeat

A v from arc consistent nodes.

Set B as a boundary complement to A.
B on B.
Compute an optimal labeling x∗
B|∂A return.
If x∗
Else set C := {v ∈ V∂A : x∗
A v (cid:54)= x∗
until C = ∅

A|∂A = x∗

B v}  A := A\C

arc consistency of θ over A directly follows that EA θ(cid:48)(x∗

A) = minxA EA θ(cid:48)(xA). From this follows

EG θ(x) = { min

xA xB

min

x

EA θ(cid:48)(xA) + EB θ(xB)

= min
∂A

x(cid:48)

min

xA : xA|∂A=x(cid:48)

∂A

EA θ(cid:48)(xA) +

min

xB : xB|∂A=x(cid:48)

∂A

s.t. xA|∂A = xB|∂A}
EB θ(xB) ≥ min
= EA θ(cid:48)(x∗

xA

EA θ(cid:48)(xA) + min
xB
A) + EB θ(x∗

EB θ(xB)
B) = EG θ(x∗)

Now we are ready to transform the idea described in the introduction into Algorithm 1.
Step (1). As a ﬁrst step of the algorithm we run an LP solver for the dual problem (3) on the
whole graph G. The output of the algorithm is the reparametrization ˜θφ of the initial problem.
Since well-scalable algorithms for the dual problem (3) attain the optimum only in the limit after a
potentially inﬁnite number of iterations  we cannot afford to solve it exactly. Fortunately  it is not
needed to do so and it is enough to get only a sufﬁciently good approximation. We will return to
this point at the end of this section.
Step (2). We assign to the set VA the nodes of the graph G  which satisfy the strict arc consistency
condition. The optimal labeling on A can be trivially computed from the reparametrized unary
v by x∗
potentials ˜θφ
A v := arg minxv
Step (3). We deﬁne B as the boundary complement to A w.r.t. the master graph G and ﬁnd an
B on the subgraph B with a combinatorial solver. If the boundary condition (5)
optimal labeling x∗
holds we have found the optimal labeling according to Theorem 1. Otherwise we remove the nodes
where this condition fails from A and repeat the whole step until either (5) holds or B = G.

v (xv)  v ∈ A.
˜θφ

3.1 Remarks on Algorithm 1

A|∂A obtained based only on the subgraph A coincides with the boundary labeling x∗

Encouraging boundary consistency condition.
It is quite unlikely  that the optimal boundary
B|∂A
labeling x∗
obtained for the subgraph B. To satisfy this condition the unary potentials should be quite strong on
the border. In other words  they should be at least strictly arc consistent. Indeed they are so  since
we consider the reparametrized potentials ˜θφ  obtained at the LP presolve step of the algorithm.
Single run of LP solver. Reparametrization allows also to perform only a single run of the LP
solver  keeping the results as if the subproblem over A has been solved at each iteration. The
following theorem states this property formally.
Theorem 2. Let all nodes of a graph A be strictly arc consistent w.r.t. potentials ˜θφ  x be the
optimum of EA ˜θφ and A(cid:48) be a subgraph of A. Then x|A(cid:48) optimizes EA(cid:48) ˜θφ.
Proof. The proof follows directly from Deﬁnition 1. Equation (4) holds for the labeling x|A(cid:48)
plugged in place of x(cid:48) and graph A(cid:48) in place of G. Hence x|A(cid:48) provides a minimum of EA(cid:48) ˜θφ.
Presolving B for combinatorial solver. Many combinatorial solvers use linear programming re-
laxations as a presolving step. Reparametrization of the subproblem over the subgraph B plays the
role of such a presolver  since the optimal reparametrization corresponds to the solution of the dual
problem and makes solving the primal one easier.
Connected components analysis. It is often the case that the subgraph B consists of several con-
nected components. We apply the combinatorial solver to each of them independently.

5

Dataset
|VG|
name
tsukuba 110592
venus
166222
teddy
168750
family
pano

425632
514080

Step (1) LP (TRWS)
# it
E
250
2000
10000 14763 1345214

Step (3) ILP (CPLEX)
# it
369537
24
3048296 10
1

time  s
186
3083

time  s

|Xv|
16
20
60

5
7

10000 20156
10000 34092

184825
169224

18
1

E

−

369218
3048043

184813

−

36
69
−
2
−

|B|

min max
130
656
66
233
2062 −
11
109
24474 −

Table 1: Results on Middlebury datasets. The column Dataset contains the dataset name  numbers
|VG| of nodes and |Xv| of labels. Columns Step (1) and Step (3) contain number of iterations  time
and attained energy at steps (1) and (3) of Algorithm 1  corresponding to solving the LP relaxation
and use of a combinatorial solver respectively. The column |B| presents starting and ﬁnal sizes
of the ”combinatorial“ subgraph B. Dash ”-” stands for failure of CPLEX  due to the size of the
combinatorial subproblem.

Subgraph B growing strategy. One can consider different strategies for increasing the subgraph B 
if the boundary condition (5) does not hold. Our greedy strategy is just one possible option.
Optimality of reparametrization. As one can see  the reparametrization plays a signiﬁcant role
for our algorithm: it (i) is required for Theorem 1 to hold; (ii) serves as a criterion for the initial
splitting of G into A and B; (iii) makes the local potentials on the border ∂A stronger; (iv) allows
to avoid multiple runs of the LP solver  when the subgraph A shrinks; (v) can speed-up some com-
binatorial solvers by serving as a presolve result. However  there is no real reason to search for an
optimal reparametrization: all its mentioned functionality remains valid also if it is non-optimal. Of
course  one pays a certain price for the non-optimality: (i) the initial subgraph B becomes larger;
(ii) the local potentials – weaker; (iii) the presolve results for the combinatorial solver become less
precise. Note that even for non-optimal reparametrizations Theorem 2 holds and we need to run the
LP solver only once.

4 Experimental evaluation

We tested our approach on problems from the Middlebury energy minimization benchmark [26] and
the recently published discrete energy minimization benchmark [15]  which includes the datasets
from the ﬁrst one. We have selected computer vision benchmarks intentionally  because many prob-
lems in this area fulﬁll our requirements: the underlying graph is sparse (typically it has a grid
structure) and the LP relaxation delivers good practical results.
Since our experiments serve mainly as proof of concept we used general  though not always the
most efﬁcient solvers: TRW-S [16] as the LP-solver and CPLEX [27] as the combinatorial one
within the OpenGM framework [28]. Unfortunately the original version of TRW-S does not provide
information about strict arc consistency and does not output a reparametrization. Therefore we used
our own implementation in the experiments. Depending on the type of the pairwise factors (Potts 
truncated (cid:96)2 or (cid:96)1-norm) we found our implementation up to an order of magnitude slower than the
freely available code of V. Kolmogorov. This fact suggests that the provided processing time can be
signiﬁcantly improved in more efﬁcient future implementations.
In the ﬁrst round of our experiments we considered problems (i.e. graphical models with the spec-
iﬁed unary and pairwise factors) of the Middlebury MRF benchmark  most of which remained un-
solved  to the best of our knowledge.
MRF stereo dataset consists of 3 models: tsukuba  venus and teddy. Since the optimal inte-
gral solution of tsukuba was recently obtained by LP-solvers [11 13]  we used this dataset to show
how our approach performs for clearly non-optimal reparametrizations. For this we run TRW-S for
250 iterations only. The size of the subgraph B grew from 130 to 656 nodes out of more than 100000
nodes of the original problem (see Table 1). On venus we obtained an optimal labeling after 10
iterations of our algorithm. During these iterations the size of the set B grew from 66 to 233 nodes 
which is only 0.14% of the original problem size. The dataset teddy remains unsolved: though

6

EG θ(x∗)

Dataset

Step (1) LP
# it
pfau
24010.44 1000
palm
12253.75
200
clownﬁsh
100
14794.18
crops
100
11853.12
strawberry 11766.34
100

time  s # it
14
276
65
17
8
32
6
32
29
8

14
93
10
6
31

561
328
355
483

700
350
350
350

1579
790
797
697

3701
181
1601
1114

Step (3) ILP MCA
time  s
> 55496 10000

time  s

MPLP
# LP it LP time  s

> 15000

ILP time  s

Table 2: Exemplary Potts model comparison. Datasets taken from the Color segmentation (N8)
set. Column EG θ(x∗) shows the optimal energy value  columns Step (1) LP and Step (3) ILP
contain number of iterations and time spent at the steps (1) and (3) of Algorithm 1  corresponding to
solving the LP relaxation and use of a combinatorial solver respectively. The column MCA stands
for the time of the multiway-cut solver reported in [21]. The MPLP [17] column provides number
of iterations and time of the LP presolve and the time of the tightening cutting plane phase (ILP).

the size of the problem was reduced from the original 168750 to 2062 nodes  they constituted a
non-manageable task for CPLEX  presumably because of the big number of labels  60 in each node.
MRF photomontage models are difﬁcult for dual solvers like TRW-S because their range of values
in pairwise factors is quite large and varies from 0 to more than 500000 in a factor. Hence we used
10000 iterations of TRW-S at the ﬁrst step of Algorithm 1. For the family dataset the algorithm
decreased the size of the problem for CPLEX from originally over 400000 nodes to slightly more
than 100 and found a solution of the whole problem. In contrast to family the initial subgraph B
for the panorama dataset is much larger (about 25000 nodes) and CPLEX gave up.
MRF inpainting. Though applying TRW-S to both datasets penguin and house allows to de-
crease the problem to about 0.5% of its original size  the resulting subgraphs B of respectively 141
and 856 nodes were too large for CPLEX  presumably because of the big number (256) of labels.

(a) Original image

(b) Kovtun’s method

(c) Our approach

(d) Optimal Labeling

Figure 2: Results for the pfau-instance from [15]. Gray pixels in (b) and (c) mark nodes that
need to be labeled by the combinatorial solver. Our approach (c) leads to much smaller combina-
torial problem instances than Kovtun’s method [29] (b) used in [30]. While Kovtun’s method gets
partial optimality for 5% of the nodes only  our approach requires to solve only tiny problems by a
combinatorial solver.

Potts models. Our approach appeared to be especially efﬁcient for Potts models. We tested it on
the following datasets from the benchmark [15]: Color segmentation (N4)  Color segmentation
(N8)  Color segmentation  Brain and managed to solve all 26 problem instances to optimality.
Solving Potts models to optimality is not a big issue anymore due to the recent work [21]  which
related this problems to the multiway-cut problem [31] and adopted a quite efﬁcient solver based on
the cutting plane technique. However  we were able to outperform even this specialized solver on
hard instances  which we collected in Table 2. There is indeed a simple explanation for this phe-
nomenon: the difﬁcult instances are those  for which the optimal labeling contains many small areas
corresponding to different labels  see e.g. Fig. 2. This is not very typical for Potts models  where an
optimal labeling typically consists of a small number of large segments. Since the number of cutting
planes  which have to be processed by the multiway-cut solver  grows with the total length of the
segment borders  the overall performance signiﬁcantly drops on such instances. Our approach is
able to correctly label most of the borders when solving the LP relaxation. Since the resulting sub-
graph B  passed to the combinatorial solver  is quite small  the corresponding subproblems appear

7

easy to solve even for a general-purpose solver like CPLEX. Indeed  we expect an increase in the
overall performance of our method if the multiway-cut solver would be used in place of CPLEX.
For Potts models there exist methods [29 32] providing part of an optimal solution  known as partial
optimality. Often they allow to drastically simplify the problem so that it can be solved to global
optimality on the remaining variables very fast  see [30]. However for hard instances like pfau these
methods can label only a small fraction of graph nodes persistently  hence combinatorial solvers
cannot solve the rest  or require a lot of time. Our method does not provide partially optimal vari-
ables: if it cannot solve the whole problem no node can be labelled as optimal at all. On the upside
the subgraph B which is given to a combinatorial solver is typically much smaller  see Fig. 2.
For comparison we tested the MPLP solver
[17]  which is based on coordinate de-
scent LP iterations and tightens the LP relaxation with the cutting plane approach de-
scribed in [33]. We used its publicly available code [34].
this solver did
not managed to solve any of
the considered difﬁcult problems (marked as unsolved in
the OpenGM Benchmark [15])  such as color-seg-n8/pfau  mrf stereo/{venus 
teddy}  mrf photomontage/{family  pano}. For easier instances of the Potts model 
we found our solver an order of magnitude faster than MPLP (see Table 2 for the exemplary com-
parison)  though we tried different numbers of LP presolve iterations to speed up the MPLP.
Summary. Our experiments show that our method used even with quite general and not always the
most efﬁcient solvers like TRW-S and CPLEX allows to (i) ﬁnd globally optimal solutions of large
scale problem instances  which were previously unsolvable; (ii) solve hard instances of Potts models
an order of magnitude faster than with a modern specialized combinatorial multiway-cut method;
(iii) overcome the cutting-plane based MPLP method on the tested datasets.

However

5 Conclusions and future work

The method proposed in this paper provides a novel way of combining convex and combinatorial
algorithms to solve large scale optimization problems to a global optimum.
It does an efﬁcient
extraction of the subgraph  where the LP relaxation is not tight and combinatorial algorithms have
to be applied. Since this subgraph often corresponds to only a tiny fraction of the initial problem  the
combinatorial search becomes feasible. The method is very generic: any linear programming and
combinatorial solvers can be used to carry out the respective steps of Algorithm 1. It is particularly
efﬁcient for sparse graphs and when the LP relaxation is almost tight.
In the future we plan to generalize the method to higher order models  tighter convex relaxations for
the convex part of our solver and apply alternative and specialized solvers both for the convex and
the combinatorial parts of our approach.
Acknowledgement. This work has been supported by the German Research Foundation (DFG) within the
program Spatio-/Temporal Graphical Models and Applications in Image Analysis  grant GRK 1653. Authors
thank A. Shekhovtsov  B. Flach  T. Werner  K. Antoniuk and V. Franc from the Center for Machine Perception
of the Czech Technical University in Prague for fruitful discussions.

References
[1] D. Koller and N. Friedman. Probabilistic Graphical Models:Principles and Techniques. MIT Press  2009.
[2] T. Werner. A linear programming approach to max-sum problem: A review. IEEE Trans. on PAMI  29(7) 

July 2007.

[3] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational inference.

Found. Trends Mach. Learn.  1(1-2):1–305  2008.

[4] M. Schlesinger. Syntactic analysis of two-dimensional visual signals in the presence of noise. Kibernetika 

(4):113–130  1976.

[5] M. Wainwright  T. Jaakkola  and A. Willsky. MAP estimation via agreement on (hyper)trees: message

passing and linear programming approaches. IEEE Trans. on Inf. Th.  51(11)  2005.

[6] N. Komodakis  N. Paragios  and G. Tziritas. MRF energy minimization and beyond via dual decomposi-

tion. IEEE Trans. on PAMI  33(3):531 –552  march 2011.

[7] B. Savchynskyy  J. H. Kappes  S. Schmidt  and C. Schn¨orr. A study of Nesterov’s scheme for Lagrangian

decomposition and MAP labeling. In CVPR 2011  2011.

8

[8] S. Schmidt  B. Savchynskyy  J. H. Kappes  and C. Schn¨orr. Evaluation of a ﬁrst-order primal-dual algo-

rithm for MRF energy minimization. In EMMCVPR  pages 89–103  2011.

[9] O. Meshi and A. Globerson. An alternating direction method for dual MAP LP relaxation.

ECML/PKDD (2)  pages 470–483  2011.

In

[10] A. F. T. Martins  M. A. T. Figueiredo  P. M. Q. Aguiar  N. A. Smith  and E. P. Xing. An augmented

Lagrangian approach to constrained MAP inference. In ICML  2011.

[11] B. Savchynskyy  S. Schmidt  J. H. Kappes  and C. Schn¨orr. Efﬁcient MRF energy minimization via

adaptive diminishing smoothing. In UAI-2012  pages 746–755.

[12] D. V. N. Luong  P. Parpas  D. Rueckert  and B. Rustem. Solving MRF minimization by mirror descent.

In Advances in Visual Computing  volume 7431  pages 587–598. Springer Berlin Heidelberg  2012.

[13] J. H. Kappes  B. Savchynskyy  and C. Schn¨orr. A bundle approach to efﬁcient MAP-inference by La-

grangian relaxation. In CVPR 2012  2012.

[14] B. Savchynskyy and S. Schmidt. Getting feasible variable estimates from infeasible ones: MRF local

polytope study. Technical report  arXiv:1210.4081  2012.

[15] J. H. Kappes  B. Andres  F. A. Hamprecht  C. Schn¨orr  S. Nowozin  D. Batra  S. Kim  B. X. Kausler 
J. Lellmann  N. Komodakis  and C. Rother. A comparative study of modern inference techniques for
discrete energy minimization problems. In CVPR  2013.

[16] V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. IEEE Trans. on

PAMI  28(10):1568–1583  2006.

[17] A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms for MAP

LP-relaxations. In NIPS  2007.

[18] T. Hazan and A. Shashua. Norm-product belief propagation: Primal-dual message-passing for approxi-

mate inference. IEEE Trans. on Inf. Theory   56(12):6294 –6316  2010.

[19] M. I. Schlesinger and K. V. Antoniuk. Diffusion algorithms and structural recognition optimization prob-

lems. Cybernetics and Systems Analysis  47(2):175–192  2011.

[20] V. Franc  S. Sonnenburg  and T. Werner. Cutting-Plane Methods in Machine Learning  chapter 7  pages

185–218. The MIT Press  Cambridge USA  2012.

[21] J. H. Kappes  M. Speth  B. Andres  G. Reinelt  and C. Schn¨orr. Globally optimal image partitioning by

multicuts. In EMMCVPR  2011.

[22] M. Bergtholdt  J. H. Kappes  S. Schmidt  and C. Schn¨orr. A study of parts-based object class detection

using complete graphs. IJCV  87(1-2):93–117  2010.

[23] M. Sun  M. Telaprolu  H. Lee  and S. Savarese. Efﬁcient and exact MAP-MRF inference using branch

and bound. In AISTATS-2012.

[24] L. Otten and R. Dechter. Anytime AND/OR depth-ﬁrst search for combinatorial optimization. In Pro-

ceedings of the Annual Symposium on Combinatorial Search (SOCS)  2011.

[25] M. C. Cooper  S. de Givry  M. Sanchez  T. Schiex  M. Zytnicki  and T. Werner. Soft arc consistency

revisited. Artiﬁcial Intelligence  174(7-8):449–478  May 2010.

[26] R. Szeliski  R. Zabih  D. Scharstein  O. Veksler  V. Kolmogorov  A. Agarwala  M. Tappen  and C. Rother.
A comparative study of energy minimization methods for Markov random ﬁelds with smoothness-based
priors. IEEE Trans. PAMI.  30:1068–1080  June 2008.

[27] ILOG  Inc. ILOG CPLEX: High-performance software for mathematical programming and optimization.

See http://www.ilog.com/products/cplex/.

[28] B. Andres  T. Beier  and J. H. Kappes. OpenGM: A C++ library for discrete graphical models. ArXiv

e-prints  2012. Projectpage: http://hci.iwr.uni-heidelberg.de/opengm2/.

[29] I. Kovtun. Partial optimal labeling search for a NP-hard subclass of (max  +) problems. In Proceedings

of the DAGM Symposium  2003.

[30] J. H. Kappes  M. Speth  G. Reinelt  and C. Schn¨orr. Towards efﬁcient and exact MAP-inference for large

scale discrete computer vision problems via combinatorial optimization. In CVPR  2013.

[31] S. Chopra and M. R. Rao. On the multiway cut polyhedron. Networks  21(1):51–89  1991.
[32] P. Swoboda  B. Savchynskyy  J. H. Kappes  and C. Schn¨orr. Partial optimality via iterative pruning for

the Potts model. In SSVM  2013.

[33] D. Sontag  T. Meltzer  A. Globerson  Y. Weiss  and T. Jaakkola. Tightening LP relaxations for MAP using

message-passing. In UAI-2008  pages 503–510.

[34] D. Sontag. C++ code for MAP inference in graphical models.

˜dsontag/code/mplp_ver2.tgz.

See http://cs.nyu.edu/

9

,Bogdan Savchynskyy
Jörg Hendrik Kappes
Paul Swoboda
Christoph Schnörr
Simina Branzei
Ruta Mehta
Noam Nisan