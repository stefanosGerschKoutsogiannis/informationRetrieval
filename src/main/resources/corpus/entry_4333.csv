2019,Positional Normalization,A widely deployed method for reducing the training time of deep neural networks is to normalize activations at each layer. Although various normalization schemes have been proposed  they all follow a common theme: normalize across spatial dimensions and discard the extracted statistics.  In this paper  we propose a novel normalization method that deviates from this theme. Our approach  which we refer to as Positional Normalization (PONO)  normalizes exclusively across channels  which allows us to capture structural information of the input image in the first and second moments. Instead of disregarding this information  we inject it into later layers to preserve or transfer structural information in generative networks. We show that PONO significantly improves the performance of deep networks across a wide range of model architectures and image generation tasks.,Positional Normalization

Boyi Li1 2∗  Felix Wu1∗  Kilian Q. Weinberger1  Serge Belongie1 2

1Cornell University 2Cornell Tech

{bl728  fw245  kilian  sjb344}@cornell.edu

Abstract

A popular method to reduce the training time of deep neural networks is to normal-
ize activations at each layer. Although various normalization schemes have been
proposed  they all follow a common theme: normalize across spatial dimensions
and discard the extracted statistics. In this paper  we propose an alternative nor-
malization method that noticeably departs from this convention and normalizes
exclusively across channels. We argue that the channel dimension is naturally ap-
pealing as it allows us to extract the ﬁrst and second moments of features extracted
at a particular image position. These moments capture structural information about
the input image and extracted features  which opens a new avenue along which
a network can beneﬁt from feature normalization: Instead of disregarding the
normalization constants  we propose to re-inject them into later layers to preserve
or transfer structural information in generative networks.

1

Introduction

A key innovation that enabled the undeniable success of
deep learning is the internal normalization of activations.
Although normalizing inputs had always been one of the
“tricks of the trade” for training neural networks [38]  batch
normalization (BN) [28] extended this practice to every
layer  which turned out to have crucial beneﬁts for deep
networks. While the success of normalization methods
was initially attributed to “reducing internal covariate shift”
in hidden layers [28  40]  an array of recent studies [1  2  4 
24  47  58  67  75] has provided evidence that BN changes
the loss surface and prevents divergence even with large
step sizes [4]  which accelerates training [28].
Multiple normalization schemes have been proposed  each
with its own set of advantages: Batch normalization [28]
beneﬁts training of deep networks primarily in computer
vision tasks. Group normalization [72] is often the ﬁrst
choice for small mini-batch settings such as object detec-
tion and instance segmentation tasks. Layer Normaliza-
tion [40] is well suited to sequence models  common in natural language processing. Instance
normalization [66] is widely used in image synthesis owing to its apparent ability to remove style
information from the inputs. However  all aforementioned normalization schemes follow a common
theme: they normalize across spatial dimensions and discard the extracted statistics. The philosophy
behind their design is that the ﬁrst two moments are considered expendable and should be removed.
In this paper  we introduce Positional Normalization (PONO)  which normalizes the activations at
each position independently across the channels. The extracted mean and standard deviation capture

Figure 1: The mean µ and standard devi-
ation σ extracted by PONO at different
layers of VGG-19 capture structural in-
formation from the input images.

∗: Equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Conv1_2Conv2_2Conv3_4Conv4_4InputσµµσFigure 2: Positional Normalization together with previous normalization methods. In the ﬁgure  each
subplot shows a feature map tensor  with B as the batch axis  C as the channel axis  and (H  W ) as
the spatial axis. The entries colored in green or blue (ours) are normalized by the same mean and
standard deviation. Unlike previous methods  our method processes each position independently  and
compute both statistics across the channels.

the coarse structural information of an input image (see Figure 1). Although removing the ﬁrst two
moments does beneﬁt training  it also eliminates important information about the image  which — in
the case of a generative model — would have to be painfully relearned in the decoder. Instead  we
propose to bypass and inject the two moments into a later layer of the network  which we refer to as
Moment Shortcut (MS) connection.
PONO is complementary to previously proposed normalization methods (such as BN) and as such
can and should be applied jointly. We provide evidence that PONO has the potential to substantially
enhance the performance of generative models and can exhibit favorable stability throughout the
training procedure in comparison with other methods. PONO is designed to deal with spatial infor-
mation  primarily targeted at generative [19  29] and sequential models [23  32  56  63]. We explore
the beneﬁts of PONO with MS in several initial experiments across different model architectures and
image generation tasks and provide code online at https://github.com/Boyiliee/PONO.

2 Related Work

Normalization is generally applied to improve convergence speed during training [50]. Normaliza-
tion methods for neural networks can be roughly categorized into two regimes: normalization of
weights [49  53  57  71] and normalization of activations [28  30  36  40  46  48  59  66  72]. In this
work  we focus on the latter.
Given the activations X ∈ RB×C×H×W (where B denotes the batch size  C the number of channels 
H the height  and W the width) in a given layer of a neural net  the normalization methods differ in
the dimensions over which they compute the mean and variance  see Figure 2. In general  activation
normalization methods compute the mean µ and standard deviation (std) σ of the features in their own
manner  normalize the features with these statistics  and optionally apply an afﬁne transformation
with parameters β (new mean) and γ (new std). This can be written as

+ β.

(1)

(cid:18) Xb c h w − µ

(cid:19)

(cid:48)
b c h w = γ

X

σ

Batch Normalization (BN) [28] computes µ and σ across the B  H  and W dimensions. BN increases
the robustness of the network with respect to high learning rates and weight initializations [4]  which
in turn drastically improves the convergence rate. Synchronized Batch Normalization treats features
of mini-batches across multiple GPUs like a single mini-batch. Instance Normalization (IN) [66]
treats each instance in a mini-batch independently and computes the statistics across only spatial
dimensions (H and W). IN aims to make a small change in the stylization architecture results in a
signiﬁcant qualitative improvement in the generated images. Layer Normalization (LN) normalizes
all features of an instance within a layer jointly  i.e.  calculating the statistics over the C  H  and W
dimensions. LN is beneﬁcial in natural language processing applications [40  68]. Notably  none of
the aforementioned methods normalize the information at different spatial position independently.
This limitation gives rise to our proposed Positional Normalization.
Batch Normalization introduces two learned parameters β and γ to allow the model to adjust the mean
and std of the post-normalized features. Speciﬁcally  β  γ ∈ RC are channel-wise parameters. Condi-
tional instance normalization (CIN) [15] keeps a set parameter of pairs {(βi  γi)|i ∈ {1  . . .   N}}

2

BatchNormalizationInstanceNormalizationGroupNormalizationLayerNormalizationPositionalNormalizationBCH WBCH WBCH WBCH WBCH Wwhich enables the model to have N different behaviors conditioned on a style class label i. Adaptive
instance normalization (AdaIN) [26] generalizes this to an inﬁnite number of styles by using the µ
and σ of IN borrowed from another image as the β and γ. Dynamic Layer Normalization (DLN) [35]
relies on a neural network to generate the β and γ. Later works [27  33] reﬁne AdaIN and generate
the β and γ of AdaIN dynamically using a dedicated neural network. Conditional batch normalization
(CBN) [10] follows a similar spirit and uses a neural network that takes text as input to predict the
residual of β and γ  which is shown to be beneﬁcial to visual question answering models.
Notably  all aforementioned methods generate β and γ as vectors  shared across spatial posi-
tions. In contrast  Spatially Adaptive Denormalization (SPADE) [52]  an extension of Synchro-
nized Batch Normalization with dynamically predicted weights  generates the spatially dependent
β  γ ∈ RB×C×H×W using a two-layer ConvNet with raw images as inputs.
Finally  we introduce shortcut connections to transfer the ﬁrst and second moment from early to later
layers. Similar skip connections (with add  concat operations) have been introduced in ResNets [20]
and DenseNets [25] and earlier works [3  23  34  54  62]  and are highly effective at improving
network optimization and convergence properties [43].

3 Positional Normalization and Moment Shortcut

Prior work has shown that feature normalization has a strong ben-
eﬁcial effect on the convergence behavior of neural networks [4].
Although we agree with these ﬁndings  in this paper we claim that
removing the ﬁrst and second order information at multiple stages
throughout the network may also deprive the deep net of poten-
tially useful information — particularly in the context of generative
models  where a plausible image needs to be generated.

Figure 3: PONO statistics of
DenseBlock-3 of a pretrained
DenseNet-161.

PONO. Our normalization scheme  which we refer to as Positional
Normalization (PONO)  differs from prior work in that we normal-
ize exclusively over the channels at any given ﬁxed pixel location
(see Figure 2). Consequently  the extracted statistics are position dependent and reveal structural
information at this particular layer of the deep net. The mean µ can be considered itself an “image” 
where the intensity of pixel i  j represents the average activation at this particular image location in
this layer. The standard deviation σ is the natural second order extension. Formally  PONO computes

C(cid:88)

c=1

(cid:118)(cid:117)(cid:117)(cid:116) 1

C

C(cid:88)

c=1

µb h w =

1
C

Xb c h w  σb h w =

(Xb c h w − µb h w)2 +  

(2)

where  is a small stability constant (e.g.   = 10−5) to avoid divisions by zero and imaginary values
due to numerical inaccuracies.

Properties. As PONO computes the normalization statistics at all spatial positions independently
from each other (unlike BN  LN  CN  and GN) it is translation  scaling  and rotation invariant.
Further  it is complementary to existing normalization methods and  as such  can be readily applied
in combination with e.g. BN.

Visualization. As the extracted mean and standard deviations are themselves images  we can
visualize them to obtain information about the extract features at the various layers of a convolutional
network. Such visualizations can be revealing and could potentially be used to debug or improve
network architectures. Figure 1 shows heat-maps of the µ and σ captured by PONO at several
layers (Conv1_2  Conv2_2  Conv3_4  and Conv4_4) of VGG-19 [60]. The ﬁgure reveals that the
features in lower layers capture the silhouette of a cat while higher layers locate the position of noses 
eyes  and the end points of ears —- suggesting that later layers may focus on higher level concepts
corresponding to essential facial features (eyes  nose  mouth)  whereas earlier layers predominantly
extract generic low level features like edges. We also observe a similar phenomenon from the
features of ResNets [20] and DenseNets [25] (see Figure 3 and Appendix). The resulting images are
reminiscent of related statistics captured in texture synthesis [14  16–18  21  51  70]. We observe that
unlike VGG and ResNet  DenseNet exhibits strange behavior on corners and boundaries which may

3

µσFigure 4: Left: PONO-MS directly uses the extracted mean and standard deviation as β and γ. Right:
Optionally  one may use a (shallow) ConvNet to predict β and γ dynamically based on µ and σ.

degrade performance when ﬁne-tuned on tasks requiring spatial information such as object detection
or segmentation. This suggests that the padding and downsampling procedure of DenseNet should
be revisited and may lead to improvements if ﬁxed  see Figure 3. The visualizations of the PONO
statistics support our hypothesis that the mean µ and the standard deviation σ may indeed capture
structural information of the image and extracted features  similar to the way statistics computed by
IN have the tendency to capture aspects of the style of the input image [26  66]. This extraction of
valuable information motivates the Moment Shortcut described in the subsequent section.

3.1 Moment Shortcut

In generative models  a deep net is trained to generate an output image from some inputs (images).
Typically  generative models follow an encoder-decoder architecture  where the encoder digests an
image into a condensed form and the decoder recovers a plausible image with some desired properties.
For example  Huang et al. [26] try to transfer the style from an image A to an image B  Zhu et al. [77]
“translate” an image from an input distribution (e.g.  images of zebras) to an output distribution
(e.g.  images of horses)  Choi et al. [8] use a shared encoder-decoder with a classiﬁcation loss in the
encoded latent space to enable translation across multiple distributions  and [27  39] combine the
structural information of an image with the attributes from another image to generate a fused output.
U-Nets [55] famously achieve strong results and compelling optimization properties in generative
models through the introduction of skip connections from the encoder to the decoder. PONO gives
rise to an interesting variant of such skip connections. Instead of connecting all channels  we only
“fast-forward” the positional moment information µ and σ extracted from earlier layers. We refer to
this approach as Moment Shortcut (MS).

Autoencoders. Figure 4 (left) illustrates the use of MS in the context of an autoencoder. Here 
we extract the ﬁrst two moments of the activations (µ  σ) in an encoder layer  and send them to a
corresponding decoder layer. Importantly  the mean is added in the encoder  and the std is multiplied 
similar to (β  γ) in the standard BN layer. To be speciﬁc  MS(x) = γF (x) + β  where F is modeled
by the intermediate layers  and the β and γ are the µ and σ extracted from the input x. MS biases
the decoder explicitly so that the activations in the decoder layers give rise to similar statistics than
corresponding layers in the encoder. As MS shortcut connections can be used with and without
normalization  we refer to the combination of PONO with MS as PONO-MS throughout.
Provided PONO does capture essential structural signatures from the input images  we can use
the extracted moments to transfer this information from a source to a target image. This opens an
opportunity to go beyond autoencoders and use PONO-MS in image-to-image translation settings  for
example in the context of CycleGAN [77] and Pix2Pix [29]. Here  we transfer the structure (through
µ and σ) of one image from the encoder to the decoder of another image.

Dynamic Moment Shortcut.
Inspired by Dynamic Layer Normalization and similar works [6  27 
33  35  52]  we propose a natural extension called Dynamic Moment Shortcut (DMS): instead of re-
injecting µ and σ as is  we use a convolutional neural network that takes µ and σ as inputs to generate
the β and γ for MS. This network can either generate one-channel outputs β  γ ∈ RB×1×H×W or
multi-channel outputs β  γ ∈ RB×C×H×W (like [52]). The right part of Figure 4 illustrates DMS
with one-channel output. DMS is particularly helpful when the task involves shape deformation or

4

µ<latexit sha1_base64="jvB1TjRiQTYIrkWuKFk3DCK3Y5w=">AAAB6nicbVDLSgNBEOz1GeMr6tHLYBA8hd0o6DHgxWNE84DNEmYns8mQmdllHkJY8glePCji1S/y5t84SfagiQUNRVU33V1xxpk2vv/tra1vbG5tl3bKu3v7B4eVo+O2Tq0itEVSnqpujDXlTNKWYYbTbqYoFjGnnXh8O/M7T1RplspHM8loJPBQsoQRbJz00BO2X6n6NX8OtEqCglShQLNf+eoNUmIFlYZwrHUY+JmJcqwMI5xOyz2raYbJGA9p6KjEguoon586RedOGaAkVa6kQXP190SOhdYTEbtOgc1IL3sz8T8vtCa5iXImM2uoJItFieXIpGj2NxowRYnhE0cwUczdisgIK0yMS6fsQgiWX14l7XotuKzV76+qjbCIowSncAYXEMA1NOAOmtACAkN4hld487j34r17H4vWNa+YOYE/8D5/AGYxjfM=</latexit><latexit sha1_base64="C7UjCesaYhx1FSd7GVflPgW3EuE=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKexGQY8BL4KXCOYByRJmJ7PJmHksM7NCWPIPXjwo4tX/8ebfOEn2oIkFDUVVN91dUcKZsb7/7RXW1jc2t4rbpZ3dvf2D8uFRy6hUE9okiivdibChnEnatMxy2kk0xSLitB2Nb2Z++4lqw5R8sJOEhgIPJYsZwdZJrZ5hQ4H75Ypf9edAqyTISQVyNPrlr95AkVRQaQnHxnQDP7FhhrVlhNNpqZcammAyxkPadVRiQU2Yza+dojOnDFCstCtp0Vz9PZFhYcxERK5TYDsyy95M/M/rpja+DjMmk9RSSRaL4pQjq9DsdTRgmhLLJ45gopm7FZER1phYF1DJhRAsv7xKWrVqcFGt3V9W6nd5HEU4gVM4hwCuoA630IAmEHiEZ3iFN095L96797FoLXj5zDH8gff5A6BijzA=</latexit> <latexit sha1_base64="HvkQum4Ib4SoQuEiPMdWmEk51UI=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKezGgB4DXgQvEcwDkiXMTmaTIfNYZmaFsOQjvHhQxKvf482/cZLsQRMLGoqqbrq7ooQzY33/2ytsbG5t7xR3S3v7B4dH5eOTtlGpJrRFFFe6G2FDOZO0ZZnltJtoikXEaSea3M79zhPVhin5aKcJDQUeSRYzgq2TOn0lmEzNoFzxq/4CaJ0EOalAjuag/NUfKpIKKi3h2Jhe4Cc2zLC2jHA6K/VTQxNMJnhEe45KLKgJs8W5M3ThlCGKlXYlLVqovycyLIyZish1CmzHZtWbi/95vdTGN2HGZJJaKslyUZxyZBWa/46GTFNi+dQRTDRztyIyxhoT6xIquRCC1ZfXSbtWDa6qtYd6pXGfx1GEMziHSwjgGhpwB01oAYEJPMMrvHmJ9+K9ex/L1oKXz5zCH3ifP5kMj8Q=</latexit>↵<latexit sha1_base64="iXeJn/fyBpfH+INrtxqaVzVduuM=">AAAB7nicbVBNSwMxEJ31s9avqkcvwSJ4KrtV0GPBi+Clgv2AdinZNG1Ds8mSzApl6Y/w4kERr/4eb/4b03YP2vog8HhvZjLzokQKi77/7a2tb2xubRd2irt7+weHpaPjptWpYbzBtNSmHVHLpVC8gQIlbyeG0ziSvBWNb2d+64kbK7R6xEnCw5gOlRgIRtFJra62ktpRr1T2K/4cZJUEOSlDjnqv9NXta5bGXCFzA2wn8BMMM2pQMMmnxW5qeULZmA55x1FFY27DbL7ulJw7pU8G2rinkMzV3x0Zja2dxJGrjCmO7LI3E//zOikObsJMqCRFrtjio0EqCWoyu530heEM5cQRyoxwuxI2ooYydAkVXQjB8smrpFmtBJeV6sNVuXafx1GAUziDCwjgGmpwB3VoAIMxPMMrvHmJ9+K9ex+L0jUv7zmBP/A+fwB/TY+z</latexit><latexit sha1_base64="cD8L8J3nB19ElH0LLgOOstqarUo=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKexGQY8BL4KXCOYByRJmJ7PJmNmZZR5CWPIPXjwo4tX/8ebfOEn2oIkFDUVVN91dUcqZNr7/7RXW1jc2t4rbpZ3dvf2D8uFRS0urCG0SyaXqRFhTzgRtGmY47aSK4iTitB2Nb2Z++4kqzaR4MJOUhgkeChYzgo2TWj2Zcqv75Ypf9edAqyTISQVyNPrlr95AEptQYQjHWncDPzVhhpVhhNNpqWc1TTEZ4yHtOipwQnWYza+dojOnDFAslSth0Fz9PZHhROtJErnOBJuRXvZm4n9e15r4OsyYSK2hgiwWxZYjI9HsdTRgihLDJ45gopi7FZERVpgYF1DJhRAsv7xKWrVqcFGt3V9W6nd5HEU4gVM4hwCuoA630IAmEHiEZ3iFN096L96797FoLXj5zDH8gff5A9QBj1I=</latexit>⌦<latexit sha1_base64="8LoE1Q9tZVqVOnR6FiipD60Tf8s=">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hd0o6DHgRfASwTwgWcLsZDYZMjuzzPQKIeQjvHhQxKvf482/cZLsQRMLGoqqbrq7olQKi77/7a2tb2xubRd2irt7+weHpaPjptWZYbzBtNSmHVHLpVC8gQIlb6eG0ySSvBWNbmd+64kbK7R6xHHKw4QOlIgFo+ikVlejSLjtlcp+xZ+DrJIgJ2XIUe+Vvrp9zbKEK2SSWtsJ/BTDCTUomOTTYjezPKVsRAe846iibkk4mZ87JedO6ZNYG1cKyVz9PTGhibXjJHKdCcWhXfZm4n9eJ8P4JpwIlWbIFVssijNJUJPZ76QvDGcox45QZoS7lbAhNZShS6joQgiWX14lzWoluKxUH67Ktfs8jgKcwhlcQADXUIM7qEMDGIzgGV7hzUu9F+/d+1i0rnn5zAn8gff5A4nuj7o=</latexit>Encoder / Early layerDecoder / Later layer<latexit sha1_base64="Vu/5lpBJ32Zu1aBhs5l8vMhE6Sk=">AAAB7HicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9FjwInipYNpCG8pmu2mXbjZhdyKU0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl3321lb39jc2i7tlHf39g8OK0fHLZNkmnGfJTLRnZAaLoXiPgqUvJNqTuNQ8nY4vp357SeujUjUI05SHsR0qEQkGEUr+b2QI+1Xqm7NnYOsEq8gVSjQ7Fe+eoOEZTFXyCQ1puu5KQY51SiY5NNyLzM8pWxMh7xrqaIxN0E+P3ZKzq0yIFGibSkkc/X3RE5jYyZxaDtjiiOz7M3E/7xuhtFNkAuVZsgVWyyKMkkwIbPPyUBozlBOLKFMC3srYSOqKUObT9mG4C2/vEpa9Zp3Was/XFUb90UcJTiFM7gAD66hAXfQBB8YCHiGV3hzlPPivDsfi9Y1p5g5gT9wPn8Ax9mOsQ==</latexit><latexit sha1_base64="bMDH8eqlty2JplUDmIm8MBVEnZM=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKexGQY8BL4KXCOYByRJ6J7PJmHksM7NCWPIPXjwo4tX/8ebfOEn2oIkFDUVVN91dUcKZsb7/7RXW1jc2t4rbpZ3dvf2D8uFRy6hUE9okiivdicBQziRtWmY57SSagog4bUfjm5nffqLaMCUf7CShoYChZDEjYJ3U6g1BCOiXK37VnwOvkiAnFZSj0S9/9QaKpIJKSzgY0w38xIYZaMsIp9NSLzU0ATKGIe06KkFQE2bza6f4zCkDHCvtSlo8V39PZCCMmYjIdQqwI7PszcT/vG5q4+swYzJJLZVksShOObYKz17HA6YpsXziCBDN3K2YjEADsS6gkgshWH55lbRq1eCiWru/rNTv8jiK6ASdonMUoCtUR7eogZqIoEf0jF7Rm6e8F+/d+1i0Frx85hj9gff5A4rujyI=</latexit>Moment Shortcut with Positional Normalization (PONO-MS)Dynamic Moment Shortcut (DMS) ConvNetµ<latexit sha1_base64="jvB1TjRiQTYIrkWuKFk3DCK3Y5w=">AAAB6nicbVDLSgNBEOz1GeMr6tHLYBA8hd0o6DHgxWNE84DNEmYns8mQmdllHkJY8glePCji1S/y5t84SfagiQUNRVU33V1xxpk2vv/tra1vbG5tl3bKu3v7B4eVo+O2Tq0itEVSnqpujDXlTNKWYYbTbqYoFjGnnXh8O/M7T1RplspHM8loJPBQsoQRbJz00BO2X6n6NX8OtEqCglShQLNf+eoNUmIFlYZwrHUY+JmJcqwMI5xOyz2raYbJGA9p6KjEguoon586RedOGaAkVa6kQXP190SOhdYTEbtOgc1IL3sz8T8vtCa5iXImM2uoJItFieXIpGj2NxowRYnhE0cwUczdisgIK0yMS6fsQgiWX14l7XotuKzV76+qjbCIowSncAYXEMA1NOAOmtACAkN4hld487j34r17H4vWNa+YOYE/8D5/AGYxjfM=</latexit><latexit sha1_base64="C7UjCesaYhx1FSd7GVflPgW3EuE=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKexGQY8BL4KXCOYByRJmJ7PJmHksM7NCWPIPXjwo4tX/8ebfOEn2oIkFDUVVN91dUcKZsb7/7RXW1jc2t4rbpZ3dvf2D8uFRy6hUE9okiivdibChnEnatMxy2kk0xSLitB2Nb2Z++4lqw5R8sJOEhgIPJYsZwdZJrZ5hQ4H75Ypf9edAqyTISQVyNPrlr95AkVRQaQnHxnQDP7FhhrVlhNNpqZcammAyxkPadVRiQU2Yza+dojOnDFCstCtp0Vz9PZFhYcxERK5TYDsyy95M/M/rpja+DjMmk9RSSRaL4pQjq9DsdTRgmhLLJ45gopm7FZER1phYF1DJhRAsv7xKWrVqcFGt3V9W6nd5HEU4gVM4hwCuoA630IAmEHiEZ3iFN095L96797FoLXj5zDH8gff5A6BijzA=</latexit><latexit sha1_base64="Vu/5lpBJ32Zu1aBhs5l8vMhE6Sk=">AAAB7HicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9FjwInipYNpCG8pmu2mXbjZhdyKU0N/gxYMiXv1B3vw3btsctPXBwOO9GWbmhakUBl3321lb39jc2i7tlHf39g8OK0fHLZNkmnGfJTLRnZAaLoXiPgqUvJNqTuNQ8nY4vp357SeujUjUI05SHsR0qEQkGEUr+b2QI+1Xqm7NnYOsEq8gVSjQ7Fe+eoOEZTFXyCQ1puu5KQY51SiY5NNyLzM8pWxMh7xrqaIxN0E+P3ZKzq0yIFGibSkkc/X3RE5jYyZxaDtjiiOz7M3E/7xuhtFNkAuVZsgVWyyKMkkwIbPPyUBozlBOLKFMC3srYSOqKUObT9mG4C2/vEpa9Zp3Was/XFUb90UcJTiFM7gAD66hAXfQBB8YCHiGV3hzlPPivDsfi9Y1p5g5gT9wPn8Ax9mOsQ==</latexit><latexit sha1_base64="bMDH8eqlty2JplUDmIm8MBVEnZM=">AAAB7XicbVDLSgNBEJyNrxhfUY9eBoPgKexGQY8BL4KXCOYByRJ6J7PJmHksM7NCWPIPXjwo4tX/8ebfOEn2oIkFDUVVN91dUcKZsb7/7RXW1jc2t4rbpZ3dvf2D8uFRy6hUE9okiivdicBQziRtWmY57SSagog4bUfjm5nffqLaMCUf7CShoYChZDEjYJ3U6g1BCOiXK37VnwOvkiAnFZSj0S9/9QaKpIJKSzgY0w38xIYZaMsIp9NSLzU0ATKGIe06KkFQE2bza6f4zCkDHCvtSlo8V39PZCCMmYjIdQqwI7PszcT/vG5q4+swYzJJLZVksShOObYKz17HA6YpsXziCBDN3K2YjEADsS6gkgshWH55lbRq1eCiWru/rNTv8jiK6ASdonMUoCtUR7eogZqIoEf0jF7Rm6e8F+/d+1i0Frx85hj9gff5A4rujyI=</latexit>⌦<latexit sha1_base64="8LoE1Q9tZVqVOnR6FiipD60Tf8s=">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hd0o6DHgRfASwTwgWcLsZDYZMjuzzPQKIeQjvHhQxKvf482/cZLsQRMLGoqqbrq7olQKi77/7a2tb2xubRd2irt7+weHpaPjptWZYbzBtNSmHVHLpVC8gQIlb6eG0ySSvBWNbmd+64kbK7R6xHHKw4QOlIgFo+ikVlejSLjtlcp+xZ+DrJIgJ2XIUe+Vvrp9zbKEK2SSWtsJ/BTDCTUomOTTYjezPKVsRAe846iibkk4mZ87JedO6ZNYG1cKyVz9PTGhibXjJHKdCcWhXfZm4n9eJ8P4JpwIlWbIFVssijNJUJPZ76QvDGcox45QZoS7lbAhNZShS6joQgiWX14lzWoluKxUH67Ktfs8jgKcwhlcQADXUIM7qEMDGIzgGV7hzUu9F+/d+1i0rnn5zAn8gff5A4nuj7o=</latexit><latexit sha1_base64="cD8L8J3nB19ElH0LLgOOstqarUo=">AAAB7XicbVDLSgNBEOyNrxhfUY9eBoPgKexGQY8BL4KXCOYByRJmJ7PJmNmZZR5CWPIPXjwo4tX/8ebfOEn2oIkFDUVVN91dUcqZNr7/7RXW1jc2t4rbpZ3dvf2D8uFRS0urCG0SyaXqRFhTzgRtGmY47aSK4iTitB2Nb2Z++4kqzaR4MJOUhgkeChYzgo2TWj2Zcqv75Ypf9edAqyTISQVyNPrlr95AEptQYQjHWncDPzVhhpVhhNNpqWc1TTEZ4yHtOipwQnWYza+dojOnDFAslSth0Fz9PZHhROtJErnOBJuRXvZm4n9e15r4OsyYSK2hgiwWxZYjI9HsdTRgihLDJ45gopi7FZERVpgYF1DJhRAsv7xKWrVqcFGt3V9W6nd5HEU4gVM4hwCuoA630IAmEHiEZ3iFN096L96797FoLXj5zDH8gff5A9QBj1I=</latexit>IntermediateLayersdistortion. We refer to this approach as PONO-DMS in the following sections. In our experiments 
we explore using a ConvNet with either one or two layers.

4 Experiments and Analysis

We conduct our experiments on unpaired and paired image translation tasks using CycleGAN [77] and
Pix2pix [29] as baselines  respectively. Our code is available at https://github.com/Boyiliee/PONO.

4.1 Experimental Setup

We follow the same setup as CycleGAN [77] and Pix2pix [29] using their ofﬁcial code base.2 We
use four datasets: 1) Maps (Maps ↔ aerial photograph) including 1096 training images scraped
from Google Maps and 1098 images in each domain for testing. 2) Horse ↔ Zebra including 1067
horse images and 1334 zebra images downloaded from ImageNet [11] using keywords wild horse
and zebra  and 120 horse images and 140 zebra images for testing. 3) Cityscapes (Semantic labels
↔ photos) [9] including 2975 images from the Cityscapes training set for training and 500 images
in each domain for testing. 4) Day ↔ Night including 17 823 natural scene images from Transient
Attributes dataset [37] for training  and 2 287 images for testing. The ﬁrst  third  and fourth are paired
image datasets; the second is an unpaired image dataset. We use the ﬁrst and second for CycleGAN 
and all the paired-image datasets for Pix2pix.

Evaluation metrics. We use two evaluation metrics  as follows. (1) Fréchet Inception Distance [22]
between the output images and all test images in the target domain. FID uses an Inception [64]
model pretrained on ImageNet [11] to extract image features. Based on the means and covariance
matrices of the two sets of extracted features  FID is able to estimate how different two distributions
are. (2) Average Learned Perceptual Image Patch Similarity distance [76] of all output and target
image pairs. LPIPS is based on pretrained AlexNet [36] features3  which has been shown [76] to be
highly correlated to human judgment.

Baselines. We include four baseline approaches: (1) CycleGAN or Pix2pix baselines; (2) these
baselines with SPADE [52]  which passes the input image through a 2-layer ConvNet and generates
the β and γ for BN in the decoder. (3) the baseline with additive skip connections where encoder
activations are added to decoder activations; (4) the baseline with concatenated skip connections 
where encoder activations are concatenated to decoder activations as additional channels (similar to
U-Nets [55]). For all models  we follow the same setup as CycleGAN [77] and Pix2pix [29] using
their implementations. Throughout we use the hyper-parameters suggested by the original authors.

4.2 Comparison against Baselines

We add PONO-MS and PONO-DMS to the CycleGAN generator; see the Appendix for the model
architecture. Table 1 shows that both cases outperform all baselines at transforming maps into photos 
with the only exception of SPADE (which however performs worse in the other direction).
Although skip connections could help make up for the lost information  we postulate that directly
adding the intermediate features back may introduce too much unnecessary information and might
distract the model. Unlike the skip connections  SPADE uses the input to predict the parameters
for normalization. However  on Photo → Map  the model has to learn to compress the input photos
and extract structural information from it. A re-introduction of the original raw input may disturb
this process and explain the worse performance. In contrast  PONO-MS normalizes exclusively
across channels which allows us to capture structural information of a particular input image and
re-inject/transfer it to later layers.
The Pix2pix model [29] is a conditional adversarial network introduced as a general-purpose solution
for image-to-image translation problems. Here we conduct experiments on whether PONO-MS helps
Pix2pix [29] with Maps [77]  Cityscapes [9] and Day ↔ Night [37]. We train for 200 epochs and
compare the results with/without PONO-MS  under similar conditions with matching number of
parameters. Results are summarized in Table 2.

2https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
3https://github.com/richzhang/PerceptualSimilarity  version 0.1.

5

Map → Photo Photo → Map Horse → Zebra Zebra → Horse

Method
CycleGAN (Baseline)
+Skip Connections
+Concatenation
+SPADE
+PONO-MS
+PONO-DMS
Table 1: FID of CycleGAN and its variants on Map ↔ Photo and Zebra ↔ Horse datasets. CycleGAN
is trained with two directions together  it is essential to have good performance in both directions.

# of param.
2×11.378M
+0M
+0.74M
+0.456M
+0M
+0.018M

FID
155.9
145.5
145.9
159.9
142.2
140.6

FID
58.3
56.0
61.2
59.8
53.2
54.1

FID
57.9
83.7
58.9
48.2
52.8
53.7

FID
86.3
75.9
85.0
71.2
71.2
65.7

Pix2pix (Baseline)
+PONO-MS

Maps [77]

Map → Photo Photo → Map
60.07 / 0.333
68.73 / 0.169
56.88 / 0.333
68.57 / 0.166

Cityscapes [9]

SL → Photo
71.24 / 0.422
60.40 / 0.331

Photo → SL
102.38 / 0.223
97.78 / 0.224

Day ↔ Night [37]

Day → Night
196.58 / 0.608
191.10 / 0.588

Night →Day
131.94 / 0.531
131.83 / 0.534

Table 2: Comparison based on Pix2pix by FID / LPIPS on Maps [77]  Cityscapes [9]  and Day2Night.
Note: for all scores  the lower the better (SL is short for Semantic labels).

4.3 Ablation Study

Table 3 contains the results of several experiments to evaluate the sensitivities and design choices of
PONO-MS and PONO-DMS. Further  we evaluate Moment Shortcut (MS) without PONO  where we
bypass both statistics  µ and σ  without normalizing the features. The results indicate that PONO-
MS outperforms MS alone  which suggests that normalizing activations with PONO is beneﬁcial.
PONO-DMS can lead to further improvements  and some settings (e.g. 1 conv 3 × 3  multi-channel)
consistently outperform PONO-MS. Here  multi-channel predictions are clearly superior over single-
channel predictions but we do not observe consistent improvements from a 5 × 5 rather than a 3 × 3
kernel size.

Normalizations. Unlike previous normalization methods such as BN and GN that emphasize
on accelerating and stabilizing the training of networks  PONO is used to split off part of the
spatial information and re-inject it later. Therefore  PONO-MS can be applied jointly with other
normalization methods. In Table 4 we evaluate four normalization approaches (BN  IN  LN  GN)
with and without PONO-MS  and PONO-MS without any additional normalization (bottom row). In
detail  BN + PONO-MS is simply applying PONO-MS to the baseline model and keep the original
BN modules which have a different purpose: to stabilize and speed up the training. We also show
the models where BN is replaced by LN/IN/GN as well as these models with PONO-MS. The last
row shows PONO-MS can work independently when we remove the original BN in the model. Each
table entry displays the FID score without and with PONO-MS (the lower score is in bold). The ﬁnal
column (very right) contains the average improvement across all four tasks  relative to the default
architecture  BN without PONO-MS. Two clear trends emerge: 1. All four normalization methods
improve with PONO-MS on average and on almost all individual tasks; 2. additional normalization is
clearly beneﬁcial over pure PONO-MS (bottom row).

Map → Photo Photo → Map Horse → Zebra Zebra → Horse

Method
CycleGAN (Baseline)
+Moment Shortcut (MS)
+PONO-MS
+PONO-DMS (1 conv 3 × 3  one-channel)
+PONO-DMS (2 conv 3 × 3  one-channel)
+PONO-DMS (1 conv 3 × 3  multi-channel)
+PONO-DMS (2 conv 5 × 5  multi-channel)
+PONO-DMS (2 conv 3 × 3  5 × 5  multi-channel)
+PONO-DMS (2 conv 3 × 3  multi-channel)
Table 3: Comparisons of ablation study on FID (lower is better). PONO-MS outperforms MS alone.
PONO-DMS can help obtain better performance than PONO-MS.

155.9
146.1
142.2
147.2
144.8
140.6
155.2
148.4
146.1

57.9
54.5
52.8
55.1
56.0
53.7
52.7
48.9
50.3

86.3
79.8
71.2
74.1
81.6
65.7
64.9
74.3
72.2

58.3
56.6
53.2
53.8
53.3
54.1
54.7
57.3
51.4

6

Method
BN (Default) / BN + PONO-MS
IN / IN + PONO-MS
LN / LN + PONO-MS
GN / GN + PONO-MS
PONO-MS

Map → Photo Photo → Map Horse → Zebra Zebra → Horse Avg. Improvement
57.92 / 52.81
1 / 0.890
0.985 / 0.883
67.87 / 47.14
0.964 / 0.853
54.84 / 49.81
0.940 / 0.849
51.31 / 50.12
49.59
0.913

155.91 / 142.21
154.15 / 153.61
154.49 / 142.05
143.56 / 144.99
143.47

58.32 / 53.23
57.93 / 54.18
53.00 / 50.08
50.62 / 50.50
52.21

86.28 / 71.18
67.85 / 69.21
87.26 / 67.63
93.58 / 63.53
84.68

Table 4: FID scores (lower is better) of CycleGAN with different normalization methods.

.
5 Further Analysis and Explorations

In this section  we apply PONO-MS to two state-of-the-art unsupervised image-to-image translation
models: MUNIT [27] and DRIT [39]. Both approaches may arguably be considered concurrent
works and share a similar design philosophy. Both aim to translate an image from a source to a target
domain  while imposing the attributes (or the style) of another target domain image.
As task  we are provided with an image xA in source domain A and an image xB in target domain
B. DRIT uses two encoders  one to extract content features cA from xA  and the other to extract
attribute features aB from xB. A decoder then takes cA and aB as inputs to generate the output
image xA→B. MUNIT follows a similar pipeline. Both approaches are trained on the two directions 
A → B and B → A  simultaneously. We apply PONO to DRIT or MUNIT immediately after the
ﬁrst three convolution layers (convolution layers before the residual blocks) of the content encoders.
We then use MS before the last three transposed convolution layers with matching decoder sizes. We
follow the DRIT and MUNIT frameworks and consider the extracted statistics (µ’s and σ’s) as part
of the content tensors.

5.1 Experimental Setup
We consider two datasets provided by the authors of DRIT: 1) Portrait ↔ Photo [39  44] with 1714
painting images and 6352 human photos for training  and 100 images in each domain for testing and
2) Cat ↔ Dog [39] containing 771 cat images and 1264 dog images for training  and 100 images in
each domain for testing.
In the following experiments  we use the ofﬁcial codebases4  closely follow their proposed hyper-
parameters and train all models for 200K iterations. We use the holdout test images as the inputs
for evaluation. For each image in the source domain  we randomly sample 20 images in the target
domain to extract the attributes and generate 20 output images. We consider four evaluation metrics:
1) FID [22]: Fréchet Inception Distance between the output images and all test images in the target
domain  2) LPIPSattr [76]: average LPIPS distance between each output image and its corresponding
input image in the target domain  3) LPIPScont: average LPIPS distance between each output image
and its input in the source domain  and 4) perceptual loss (VGG) [31  60]: L1 distance between
the VGG-19 Conv4_4 features [7] of each output image and its corresponding input in the source
domain. The FID and LPIPSattr are used to estimate how likely the outputs are to belong to the target
domain  while LPIPScont and VGG loss are adopted to estimate how much the outputs preserve the
structural information in the inputs. All of them are distance metrics where lower is better. The
original implementations of DRIT and MUNIT assume differently sized input images (216x216 and
256x256  respectively)  which precludes a direct comparison across approaches.

5.2 Results of Attribute Controlled Image Translation
Figure 5 shows the qualitative results on the Cat ↔ Dog dataset. (Here we show the results of
MUNIT’ + PONO-MS which will be explained later.) We observe a clear trend that PONO-MS helps
these two models obtain more plausible results. We observe the models with PONO-MS is able to
capture the content features and attributes distributions  which motivates baseline models to digest
different information from both domains. For example  in the ﬁrst row  when translating cat to dog 
DRIT with PONO-MS is able to capture the cat’s facial expression  and MUNIT with PONO-MS
could successfully generate dog images with plausible content  which largely boosts the performance
of the baseline models. More qualitative results of randomly selected inputs are provided in the
Appendix.

4https://github.com/NVlabs/MUNIT/ and https://github.com/HsinYingLee/DRIT

7

Figure 5: PONO-MS improves the quality of both DRIT [39] and MUNIT [27] on Cat ↔ Dog.

DRIT
DRIT + PONO-MS
MUNIT
MUNIT + PONO-MS
MUNIT’
MUNIT’ + PONO-MS

DRIT
DRIT + PONO-MS
MUNIT
MUNIT + PONO-MS
MUNIT’
MUNIT’ + PONO-MS

LPIPScont VGG
1.796
1.744
1.888
1.559
1.662
1.324

Portrait → Photo
FID LPIPSattr
0.470
0.545
131.2
0.457
0.534
127.9
0.578
0.605
220.1
270.5
0.541
0.423
0.455
0.538
245.0
0.424
159.4
0.319
Cat → Dog
FID LPIPSattr
45.8
0.542
0.524
47.5
0.686
315.6
0.632
254.8
361.5
0.699
0.615
80.4

LPIPScont VGG
2.147
2.147
1.952
1.614
1.867
1.610

0.581
0.576
0.674
0.501
0.607
0.406

LPIPScont VGG
2.033
2.022
2.599
2.202
2.434
1.824

Portrait ← Photo
FID LPIPSattr
0.476
0.585
104.5
0.463
0.575
99.5
0.670
0.619
149.6
127.5
0.586
0.477
0.620
0.601
158.1
0.566
125.1
0.312
Cat ← Dog
FID LPIPSattr
42.0
0.524
0.514
41.0
0.629
290.3
0.624
276.2
289.0
0.767
0.477
90.8

LPIPScont VGG
2.026
2.003
2.110
2.119
2.228
1.689

0.576
0.604
0.591
0.585
0.789
0.428

Table 5: PONO-MS can improve the performance of MUNIT [27]  while for DRIT [39] the improve-
ment is marginal. MUNIT’ is MUNIT with one more Conv3x3-LN-ReLU layer before the output
layer in the decoder  which introduces 0.2% parameters into the generator. Note: for all scores  the
lower the better.

Table 5 show the quantitative results on both Cat ↔ Dog and Portrait ↔ Photo datasets. PONO-MS
improves the performance of both models on all instance-level metrics (LPIPSattr  LPIPScont  and
VGG loss). However  the dataset-level metric  FID  doesn’t improve too much. We believe the reason
is that FID is calculated based on the ﬁrst two order statistic of Inception features and may discard
some subtle differences between each output pair.
Interestingly MUNIT  while being larger than DRIT (30M parameters vs. 10M parameters)  doesn’t
perform better on these two datasets. One reason for its relatively poor performance could be that
the model was not designed for these datasets (MUNIT uses a much larger unpublished dogs to big
cats dataset)  the dataset are very small  and the default image resolution is slightly different. To
further improve MUNIT + PONO-MS  we add one more Conv3x3-LN-ReLU layer before the output
layer. Without this  there is only one layer between the outputs and the last re-introduced µ and σ.
Therefore  adding one additional layer allows the model to learn a nonlinear function of these µ and
σ. We call this model MUNIT’ + PONO-MS. Adding this additional layer signiﬁcantly enhances the
performance of MUNIT while introducing only 75K parameters (about 0.2%). We also provide the
numbers of MUNIT’ (MUNIT with one additional layer) as a baseline for a fair comparison.
Admittedly  the state-of-the-art generative models employ complex architecture and a variety of loss
functions; therefore  unveiling the full potential of PONO-MS on these models can be nontrivial and
required further explorations. It is fair to admit that the results of all model variations are still largely
unsatisfactory and the image translation task remains an open research problem.

8

CatDogcat2dog (cat’s content + dog’s attributes) dog2cat: (dog’s content + cat’s attributes) InputsDRITDRIT+ PONO-MSMUNITMUNIT’+ PONO-MSDRITDRIT+ PONO-MSMUNITMUNIT’+ PONO-MSHowever  we hope that our experiments on DRIT and MUNIT may shed some light on the potential
value of PONO-MS  which could open new interesting directions of research for neural architecture
design.

6 Conclusion and Future Work

In this paper  we propose a novel normalization technique  Positional Normalization (PONO)  in
combination with a purposely limited variant of shortcut connections  Moment Shortcut (MS). When
applied to various generative models  we observe that the resulting model is able to preserve structural
aspects of the input  improving the plausibility performance according to established metrics. PONO
and MS can be implemented in a few lines of code (see Appendix). Similar to Instance Normalization 
which has been observed to capture the style of image [26  33  66]  Positional Normalization captures
structural information. As future work we plan to further explore such disentangling of structural and
style information in the design of modern neural architectures.
It is possible that PONO and MS can be applied to a variety of tasks such as image segmentation [45 
55]  denoising [41  73]  inpainting [74]  super-resolution [13]  and structured output prediction [61].
Further  beyond single image data  PONO and MS may also be applied to video data [42  69]  3D
voxel grids [5  65]  or tasks in natural language processing [12].

Acknowledgments

This research is supported in part by the grants from Facebook  the National Science Foundation
(III-1618134  III-1526012  IIS1149882  IIS-1724282  and TRIPODS-1740822)  the Ofﬁce of Naval
Research DOD (N00014-17-1-2175)  Bill and Melinda Gates Foundation. We are thankful for
generous support by Zillow and SAP America Inc.

References
[1] Sanjeev Arora  Zhiyuan Li  and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch normaliza-

tion. In International Conference on Learning Representations  2019.

[2] David Balduzzi  Marcus Frean  Lennox Leary  JP Lewis  Kurt Wan-Duo Ma  and Brian McWilliams. The
shattered gradients problem: If resnets are the answer  then what is the question? In Proceedings of the
34th International Conference on Machine Learning-Volume 70  pages 342–350. JMLR. org  2017.

[3] Christopher M Bishop. Neural networks for pattern recognition. Oxford university press  1995.

[4] Nils Bjorck  Carla P Gomes  Bart Selman  and Kilian Q Weinberger. Understanding batch normalization.

In Advances in Neural Information Processing Systems  pages 7694–7705  2018.

[5] João Carreira and Andrew Zisserman. Quo vadis  action recognition? A new model and the kinetics dataset.
In 2017 IEEE Conference on Computer Vision and Pattern Recognition  CVPR 2017  Honolulu  HI  USA 
July 21-26  2017  pages 4724–4733. IEEE Computer Society  2017.

[6] Ting Chen  Mario Lucic  Neil Houlsby  and Sylvain Gelly. On self modulation for generative adversarial

networks. arXiv preprint arXiv:1810.01365  2018.

[7] Yang Chen  Yu-Kun Lai  and Yong-Jin Liu. Cartoongan: Generative adversarial networks for photo
cartoonization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 9465–9474  2018.

[8] Yunjey Choi  Minje Choi  Munyoung Kim  Jung-Woo Ha  Sunghun Kim  and Jaegul Choo. Stargan:
Uniﬁed generative adversarial networks for multi-domain image-to-image translation. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition  pages 8789–8797  2018.

[9] Marius Cordts  Mohamed Omran  Sebastian Ramos  Timo Rehfeld  Markus Enzweiler  Rodrigo Benenson 
Uwe Franke  Stefan Roth  and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.
In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2016.

[10] Harm De Vries  Florian Strub  Jérémie Mary  Hugo Larochelle  Olivier Pietquin  and Aaron C Courville.
Modulating early visual processing by language. In Advances in Neural Information Processing Systems 
pages 6594–6604  2017.

9

[11] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition  pages 248–255.
Ieee  2009.

[12] Jacob Devlin  Ming-Wei Chang  Kenton Lee  and Kristina Toutanova. Bert: Pre-training of deep bidirec-

tional transformers for language understanding. arXiv preprint arXiv:1810.04805  2018.

[13] Chao Dong  Chen Change Loy  Kaiming He  and Xiaoou Tang. Learning a deep convolutional network for

image super-resolution. In European conference on computer vision  pages 184–199. Springer  2014.

[14] Ian L Dryden. Shape analysis. Wiley StatsRef: Statistics Reference Online  2014.

[15] Vincent Dumoulin  Jonathon Shlens  and Manjunath Kudlur. A learned representation for artistic style.

Proc. of ICLR  2  2017.

[16] Alexei A Efros and William T Freeman. Image quilting for texture synthesis and transfer. In Proceedings
of the 28th annual conference on Computer graphics and interactive techniques  pages 341–346. ACM 
2001.

[17] Alexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In Proceedings of
the seventh IEEE international conference on computer vision  volume 2  pages 1033–1038. IEEE  1999.

[18] William T. Freeman and Edward H Adelson. The design and use of steerable ﬁlters. IEEE Transactions on

Pattern Analysis & Machine Intelligence  (9):891–906  1991.

[19] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair  Aaron
Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems  pages 2672–2680  2014.

[20] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition  pages 770–778  2016.

[21] David J Heeger and James R Bergen. Pyramid-based texture analysis/synthesis. In Proceedings of the
22nd annual conference on Computer graphics and interactive techniques  pages 229–238. Citeseer  1995.

[22] Martin Heusel  Hubert Ramsauer  Thomas Unterthiner  Bernhard Nessler  and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural
Information Processing Systems  pages 6626–6637  2017.

[23] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation  9(8):1735–1780 

1997.

[24] Elad Hoffer  Ron Banner  Itay Golan  and Daniel Soudry. Norm matters: efﬁcient and accurate normaliza-
tion schemes in deep networks. In Advances in Neural Information Processing Systems  pages 2160–2170 
2018.

[25] Gao Huang  Zhuang Liu  Laurens Van Der Maaten  and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 4700–4708  2017.

[26] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization.

In Proceedings of the IEEE International Conference on Computer Vision  pages 1501–1510  2017.

[27] Xun Huang  Ming-Yu Liu  Serge Belongie  and Jan Kautz. Multimodal unsupervised image-to-image
translation. In Proceedings of the European Conference on Computer Vision (ECCV)  pages 172–189 
2018.

[28] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. arXiv preprint arXiv:1502.03167  2015.

[29] Phillip Isola  Jun-Yan Zhu  Tinghui Zhou  and Alexei A Efros. Image-to-image translation with conditional
adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 1125–1134  2017.

[30] Kevin Jarrett  Koray Kavukcuoglu  Yann LeCun  et al. What is the best multi-stage architecture for object
recognition? In 2009 IEEE 12th international conference on computer vision  pages 2146–2153. IEEE 
2009.

[31] Justin Johnson  Alexandre Alahi  and Li Fei-Fei. Perceptual losses for real-time style transfer and

super-resolution. In European conference on computer vision  pages 694–711. Springer  2016.

10

[32] Andrej Karpathy  George Toderici  Sanketh Shetty  Thomas Leung  Rahul Sukthankar  and Li Fei-Fei.
Large-scale video classiﬁcation with convolutional neural networks. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition  pages 1725–1732  2014.

[33] Tero Karras  Samuli Laine  and Timo Aila. A style-based generator architecture for generative adversarial

networks. arXiv preprint arXiv:1812.04948  2018.

[34] Jiwon Kim  Jung Kwon Lee  and Kyoung Mu Lee. Accurate image super-resolution using very deep
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 1646–1654  2016.

[35] Taesup Kim  Inchul Song  and Yoshua Bengio. Dynamic layer normalization for adaptive neural acoustic

modeling in speech recognition. arXiv preprint arXiv:1707.06065  2017.

[36] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In Advances in neural information processing systems  pages 1097–1105  2012.

[37] Pierre-Yves Laffont  Zhile Ren  Xiaofeng Tao  Chao Qian  and James Hays. Transient attributes for
high-level understanding and editing of outdoor scenes. ACM Transactions on Graphics (TOG)  33(4):149 
2014.

[38] Yann A LeCun  Léon Bottou  Genevieve B Orr  and Klaus-Robert Müller. Efﬁcient backprop. In Neural

networks: Tricks of the trade  pages 9–48. Springer  2012.

[39] Hsin-Ying Lee    Hung-Yu Tseng  Jia-Bin Huang  Maneesh Kumar Singh  and Ming-Hsuan Yang. Diverse
image-to-image translation via disentangled representations. In European Conference on Computer Vision 
2018.

[40] Jimmy Lei Ba  Jamie Ryan Kiros  and Geoffrey E Hinton. Layer normalization. arXiv preprint

arXiv:1607.06450  2016.

[41] Boyi Li  Xiulian Peng  Zhangyang Wang  Jizheng Xu  and Dan Feng. Aod-net: All-in-one dehazing
network. In Proceedings of the IEEE International Conference on Computer Vision  pages 4770–4778 
2017.

[42] Boyi Li  Xiulian Peng  Zhangyang Wang  Jizheng Xu  and Dan Feng. End-to-end united video dehazing

and detection. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence  2018.

[43] Hao Li  Zheng Xu  Gavin Taylor  Christoph Studer  and Tom Goldstein. Visualizing the loss landscape
of neural nets. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett 
editors  Advances in Neural Information Processing Systems 31  pages 6389–6399. Curran Associates 
Inc.  2018.

[44] Ziwei Liu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Deep learning face attributes in the wild. In

Proceedings of the IEEE international conference on computer vision  pages 3730–3738  2015.

[45] Jonathan Long  Evan Shelhamer  and Trevor Darrell. Fully convolutional networks for semantic segmenta-
tion. In Proceedings of the IEEE conference on computer vision and pattern recognition  pages 3431–3440 
2015.

[46] Ping Luo  Jiamin Ren  and Zhanglin Peng. Differentiable learning-to-normalize via switchable normaliza-

tion. arXiv preprint arXiv:1806.10779  2018.

[47] Ping Luo  Xinjiang Wang  Wenqi Shao  and Zhanglin Peng. Towards understanding regularization in batch

normalization. In International Conference on Learning Representations  2019.

[48] Siwei Lyu and Eero P Simoncelli. Nonlinear image representation using divisive normalization. In 2008

IEEE Conference on Computer Vision and Pattern Recognition  pages 1–8. IEEE  2008.

[49] Takeru Miyato  Toshiki Kataoka  Masanori Koyama  and Yuichi Yoshida. Spectral normalization for

generative adversarial networks. Proc. of ICLR  2018.

[50] Genevieve B Orr and Klaus-Robert Müller. Neural networks: tricks of the trade. Springer  2003.

[51] Robert Osada  Thomas Funkhouser  Bernard Chazelle  and David Dobkin. Shape distributions. ACM

Transactions on Graphics (TOG)  21(4):807–832  2002.

[52] Taesung Park  Ming-Yu Liu  Ting-Chun Wang  and Jun-Yan Zhu. Semantic image synthesis with spatially-
adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-
tion  2019.

11

[53] Siyuan Qiao  Huiyu Wang  Chenxi Liu  Wei Shen  and Alan Yuille. Weight standardization. arXiv preprint

arXiv:1903.10520  2019.

[54] Brian D Ripley. Pattern recognition and neural networks. Cambridge university press  2007.

[55] Olaf Ronneberger  Philipp Fischer  and Thomas Brox. U-net: Convolutional networks for biomedical
image segmentation. In International Conference on Medical image computing and computer-assisted
intervention  pages 234–241. Springer  2015.

[56] David E. Rumelhart  Geoffrey E. Hinton  and Ronald J. Williams. Learning representations by back-

propagating errors. Nature  323:533–  October 1986.

[57] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett 
editors  Advances in Neural Information Processing Systems 29  pages 901–909. Curran Associates  Inc. 
2016.

[58] Shibani Santurkar  Dimitris Tsipras  Andrew Ilyas  and Aleksander Madry. How does batch normalization

help optimization? In Advances in Neural Information Processing Systems  pages 2483–2493  2018.

[59] Wenqi Shao  Tianjian Meng  Jingyu Li  Ruimao Zhang  Yudian Li  Xiaogang Wang  and Ping Luo. Ssn:

Learning sparse switchable normalization via sparsestmax. arXiv preprint arXiv:1903.03793  2019.

[60] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. Proc. of ICLR  2015.

[61] Kihyuk Sohn  Honglak Lee  and Xinchen Yan. Learning structured output representation using deep
conditional generative models. In Advances in neural information processing systems  pages 3483–3491 
2015.

[62] Rupesh Kumar Srivastava  Klaus Greff  and Jürgen Schmidhuber. Highway networks. arXiv preprint

arXiv:1505.00387  2015.

[63] Ilya Sutskever  Oriol Vinyals  and Quoc V Le. Sequence to sequence learning with neural networks. In

Advances in neural information processing systems  pages 3104–3112  2014.

[64] Christian Szegedy  Wei Liu  Yangqing Jia  Pierre Sermanet  Scott Reed  Dragomir Anguelov  Dumitru
Erhan  Vincent Vanhoucke  and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of
the IEEE conference on computer vision and pattern recognition  pages 1–9  2015.

[65] Du Tran  Lubomir D. Bourdev  Rob Fergus  Lorenzo Torresani  and Manohar Paluri. Learning spatiotempo-
ral features with 3d convolutional networks. In 2015 IEEE International Conference on Computer Vision 
ICCV 2015  Santiago  Chile  December 7-13  2015  pages 4489–4497. IEEE Computer Society  2015.

[66] Dmitry Ulyanov  Andrea Vedaldi  and Victor Lempitsky. Instance normalization: The missing ingredient

for fast stylization. arXiv preprint arXiv:1607.08022  2016.

[67] Twan van Laarhoven. L2 regularization versus batch and weight normalization.

arXiv:1706.05350  2017.

arXiv preprint

[68] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez  Łukasz
Kaiser  and Illia Polosukhin. Attention is all you need. In Advances in neural information processing
systems  pages 5998–6008  2017.

[69] Xiaolong Wang  Ross Girshick  Abhinav Gupta  and Kaiming He. Non-local neural networks. CVPR 

2018.

[70] Li-Yi Wei and Marc Levoy. Fast texture synthesis using tree-structured vector quantization. In Proceedings
of the 27th annual conference on Computer graphics and interactive techniques  pages 479–488. ACM
Press/Addison-Wesley Publishing Co.  2000.

[71] Felix Wu  Angela Fan  Alexei Baevski  Yann Dauphin  and Michael Auli. Pay less attention with lightweight

and dynamic convolutions. In International Conference on Learning Representations  2019.

[72] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer

Vision (ECCV)  pages 3–19  2018.

[73] Junyuan Xie  Linli Xu  and Enhong Chen. Image denoising and inpainting with deep neural networks. In

Advances in neural information processing systems  pages 341–349  2012.

12

[74] Jiahui Yu  Zhe Lin  Jimei Yang  Xiaohui Shen  Xin Lu  and Thomas S Huang. Generative image inpainting
In Proceedings of the IEEE Conference on Computer Vision and Pattern

with contextual attention.
Recognition  pages 5505–5514  2018.

[75] Hongyi Zhang  Yann N. Dauphin  and Tengyu Ma. Residual learning without normalization via better

initialization. In International Conference on Learning Representations  2019.

[76] Richard Zhang  Phillip Isola  Alexei A Efros  Eli Shechtman  and Oliver Wang. The unreasonable

effectiveness of deep features as a perceptual metric. In CVPR  2018.

[77] Jun-Yan Zhu  Taesung Park  Phillip Isola  and Alexei A Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer
vision  pages 2223–2232  2017.

13

,Raman Arora
Andy Cotter
Nati Srebro
Dipan Pal
Ashwin Kannan
Gautam Arakalgud
Marios Savvides
Marylou Gabrié
Andre Manoel
Clément Luneau
jean barbier
Nicolas Macris
Florent Krzakala
Lenka Zdeborová
Boyi Li
Felix Wu
Kilian Weinberger
Serge Belongie