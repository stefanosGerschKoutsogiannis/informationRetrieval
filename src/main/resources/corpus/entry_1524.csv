2013,Dirty Statistical Models,We provide a unified framework for the high-dimensional analysis of “superposition-structured” or “dirty” statistical models: where the model parameters are a “superposition” of structurally constrained parameters. We allow for any number and types of structures  and any statistical model. We consider the general class of $M$-estimators that minimize the sum of any loss function  and an instance of what we call a “hybrid” regularization  that is the infimal convolution of weighted regularization functions  one for each structural component. We provide corollaries showcasing our unified framework for varied statistical models such as linear regression  multiple regression and principal component analysis  over varied superposition structures.,Dirty Statistical Models

Eunho Yang

Department of Computer Science

University of Texas at Austin
eunho@cs.utexas.edu

Pradeep Ravikumar

Department of Computer Science

University of Texas at Austin

pradeepr@cs.utexas.edu

Abstract

We provide a uniﬁed framework for
the high-dimensional analysis of
“superposition-structured” or “dirty” statistical models: where the model param-
eters are a superposition of structurally constrained parameters. We allow for any
number and types of structures  and any statistical model. We consider the gen-
eral class of M-estimators that minimize the sum of any loss function  and an
instance of what we call a “hybrid” regularization  that is the inﬁmal convolution
of weighted regularization functions  one for each structural component. We pro-
vide corollaries showcasing our uniﬁed framework for varied statistical models
such as linear regression  multiple regression and principal component analysis 
over varied superposition structures.

1

Introduction

High-dimensional statistical models have been the subject of considerable focus over the past
decade  both theoretically as well as in practice. In these high-dimensional models  the ambient
dimension of the problem p may be of the same order as  or even substantially larger than the sample
size n. It has now become well understood that even in this type of high-dimensional p  n scal-
ing  it is possible to obtain statistically consistent estimators provided one imposes structural con-
straints on the statistical models. Examples of such structural constraints include sparsity constraints
(e.g. compressed sensing)  graph-structure (for graphical model estimation)  low-rank structure (for
matrix-structured problems)  and sparse additive structure (for non-parametric models)  among oth-
ers. For each of these structural constraints  a large body of work have proposed and analyzed
statistically consistent estimators. For instance  a key subclass leverage such structural constraints
via speciﬁc regularization functions. Examples include `1-regularization for sparse models  nuclear
norm regularization for low-rank matrix-structured models  and so on.
A caveat to this strong line of work is that imposing such “clean” structural constraints such as
sparsity or low-rank structure  is typically too stringent for real-world messy data. What if the
parameters are not exactly sparse  or not exactly low rank? Indeed  over the last couple of years 
there has been an emerging line of work that address this caveat by “mixing and matching” different
structures. Chandrasekaran et al. [5] consider the problem of recovering an unknown low-rank and
an unknown sparse matrix  given the sum of the two matrices; for which they point to applications
in system identiﬁcation in linear time-invariant systems  and optical imaging systems among others.
Chandrasekaran et al. [6] also apply this matrix decomposition estimation to the learning of latent-
variable Gaussian graphical models  where they estimate an inverse covariance matrix that is the sum
of sparse and low-rank matrices. A number of papers have applied such decomposition estimation
to robust principal component analysis: Cand`es et al. [3] learn a covariance matrix that is the sum
of a low-rank factored matrix and a sparse “error/outlier” matrix  while [9  15] learn a covariance
matrix that is the sum of a low-rank matrix and a column-sparse error matrix. Hsu et al. [7] analyze
this estimation of a sum of a low-rank and elementwise sparse matrix in the noisy setting; while
Agarwal et al. [1] extend this to the sum of a low-rank matrix and a matrix with general structure.
Another application is multi-task learning  where [8] learn a multiple-linear-regression coefﬁcient

1

matrix that is the sum of a sparse and a block-sparse matrix. This strong line of work can be seen to
follow the resume of estimating a superposition of two structures; and indeed their results show this
simple extension provides a vast increase in the practical applicability of structurally constrained
models. The statistical guarantees in these papers for the corresponding M-estimators typically
require fairly extensive technical arguments that extend the analyses of speciﬁc single-structured
regularized estimators in highly non-trivial ways.
This long-line of work above on M-estimators and analyses for speciﬁc pairs of super-position
structures for speciﬁc statistical models  lead to the question: is there a uniﬁed framework for study-
ing any general tuple (i.e. not just a pair) of structures  for any general statistical model? This is
precisely the focus of this paper: we provide a uniﬁed framework of “superposition-structured” or
“dirty” statistical models  with any number and any types of structures  for any statistical model.
By such “superposition-structure ” we mean the constraint that the parameter be a superposition of
“clean” structurally constrained parameters. In addition to the motivation above  of unifying the
burgeoning list of works above  as well as to provide guarantees for many novel superpositions (of
for instance more than two structures) not yet considered in the literature; another key motivation is
to provide insights on the key ingredients characterizing the statistical guarantees for such dirty sta-
tistical models. Our uniﬁed analysis allows the following very general class of M-estimators  which
are the sum of any loss function  and an instance of what we call a “hybrid” regularization func-
tion  that is the inﬁmal convolution of any weighted regularization functions  one for each structural
component. As we show  this is equivalent to an M-estimator that is the sum of (a) a loss function
applied to the sum of the multiple parameter vectors  one corresponding to each structural compo-
nent; and (b) a weighted sum of regularization functions  one for each of the parameter vectors. We
stress that our analysis allows for general loss functions  and general component regularization func-
tions. We provide corollaries showcasing our uniﬁed framework for varied statistical models such as
linear regression  multiple regression and principal component analysis  over varied superposition
structures.

2 Problem Setup

We consider the following general statistical modeling setting. Consider a random variable Z with
1 := {Z1  . . .   Zn} drawn i.i.d. from P.
distribution P  and suppose we are given n observations Zn
We are interested in estimating some parameter ✓⇤ 2 Rp of the distribution P. We assume that
the statistical model parameter ✓⇤ is “superposition-structured ” so that it is the sum of parameter
components  each of which is constrained by a speciﬁc structure. For a formalization of the notion
of structure  we ﬁrst review some terminology from [11]. There  they use subspace pairs (M M?) 
where M✓ M  to capture any structured parameter. M is the model subspace that captures
the constraints imposed on the model parameter  and is typically low-dimensional. M? is the
perturbation subspace of parameters that represents perturbations away from the model subspace.
They also deﬁne the property of decomposability of a regularization function  which captures the
suitablity of a regularization function R to particular structure. Speciﬁcally  a regularization function
R is said to be decomposable with respect to a subspace pair (M M?)  if

2

R(u + v) = R(u) + R(v) 

for all u 2M   v 2 M?.

For any structure such as sparsity  low-rank  etc.  we can deﬁne the corresponding low-dimensional
model subspaces  as well as regularization functions that are decomposable with respect to the cor-
responding subspace pairs.
I. Sparse vectors. Given any subset S ✓{ 1  . . .   p} of the coordinates  let M(S) be the subspace
of vectors in Rp that have support contained in S. It can be seen that any parameter ✓ 2M (S)
would be atmost |S|-sparse. For this case  we use M(S) = M(S)  so that M?(S) = M?(S). As
shown in [11]  the `1 norm R(✓) = k✓k1  commonly used as a sparsity-encouraging regularization
function  is decomposable with respect to subspace pairs (M(S) M?(S)).
II. Low-rank matrices. Consider the class of matrices ⇥ 2 Rk⇥m that have rank r  min{k  m}.
For any given matrix ⇥  we let row(⇥) ✓ Rm and col(⇥) ✓ Rk denote its row space and column
space respectively. For a given pair of r-dimensional subspaces U ✓ Rk and V ✓ Rm  we deﬁne
the subspace pairs as follows: M(U  V ) := ⇥ 2 Rk⇥m | row(⇥) ✓ V  col(⇥) ✓ U and

M?(U  V ) :=⇥ 2 Rk⇥m | row(⇥) ✓ V ?  col(⇥) ✓ U? . As [11] show  the nuclear norm
R(✓) = |||✓|||1 is decomposable with respect to the subspace pairs (M(U  V ) M?(U  V )).
In our dirty statistical model setting  we do not just have one  but a set of structures; suppose we
index them by the set I. Our key structural constraint can then be stated as: ✓⇤ =P↵2I ✓⇤↵  where
✓⇤↵ is a “clean” structured parameter with respect to a subspace pair (M↵ M?
↵ )  for M↵ ✓ M↵.
We also assume we are given a set of regularization functions R↵(·)  for ↵ 2 I that are suited to
the respective structures  in the sense that they are decomposable with respect to the subspace pairs
(M↵ M?
↵ ).
Let L :⌦ ⇥Z n 7! R be some loss function that assigns a cost to any parameter ✓ 2 ⌦ ✓ Rp  for a
given set of observations Zn
1 . For ease of notation  in the sequel  we adopt the shorthand L(✓) for
L(✓; Zn

1 ). We are interested in the following “super-position” estimator:

min

(✓↵)↵2I L⇣X↵2I

✓↵⌘ +X↵2I

↵ R↵(✓↵) 

(1)

where (↵)↵2I are the regularization penalties. This optimization problem involves not just one
parameter vector  but multiple parameter vectors  one for each structural component: while the
loss function applies only to the sum of these  separate regularization functions are applied to the
corresponding parameter vectors. We will now see that this can be re-written to a standard M-
estimation problem which minimizes  over a single parameter vector  the sum of a loss function and
a special “dirty” regularization function.
Given a vector c := (c↵)↵2I of convex-combination weights  suppose we deﬁne the following
“dirty” regularization function  that is the inﬁmal convolution of a set of regularization functions:

R(✓; c) = infnX↵2I

c↵R↵(✓↵) : X↵2I

✓↵ = ✓o.

(2)

It can be shown that provided the individual regularization functions R↵(·)  for ↵ 2 I  are norms 
R(·; c) is a norm as well. We discuss this and other properties of this hybrid regularization function
R(·; c) in Appendix A.
Proposition 1. Suppose (b✓↵)↵2I is the solution to the M-estimation problem in (1). Then b✓ :=
P↵2Ib✓↵ is the solution to the following problem:
where c↵ = ↵/. Similarly  if b✓ is the solution to (3)  then there is a solution (b✓↵)↵2I to the
M-estimation problem (1)  such thatb✓ :=P↵2Ib✓↵.

Proposition 1 shows that the optimization problems (1) and (3) are equivalent. While the tuning
parameters in (1) correspond to the regularization penalties (↵)↵2I  the tuning parameters in (3)
correspond to the weights (c↵)↵2I specifying the “dirty” regularization function. In our uniﬁed
analysis theorem  we will provide guidance on setting these tuning parameters as a function of
various model-parameters.

✓2⌦ L(✓) + R(✓; c) 

min

(3)

3 Error Bounds for Convex M-estimators

Our goal is to provide error bounds kb✓  ✓⇤k  between the target parameter ✓⇤  the minimizer of the
population risk  and our M-estimateb✓ from (1)  for any error norm k·k . A common example of an
error norm for instance is the `2 norm k·k 2. We now turn to the properties of the loss function and
regularization function that underlie our analysis. We ﬁrst restate some natural assumptions on the
loss and regularization functions.
(C1) The loss function L is convex and differentiable.
(C2) The regularizers R↵ are norms  and are decomposable with respect to the subspace pairs

(M↵ M?

↵ )  where M↵ ✓ M↵.

3

Our next assumption is a restricted strong convexity assumption [11]. Speciﬁcally  we will require
the loss function L to satisfy:
(C3) (Restricted Strong Convexity) For all ↵ 2 ⌦↵  where ⌦↵ is the parameter space for the

parameter component ↵ 

L(↵; ✓⇤) := L(✓⇤ + ↵) L (✓⇤) ⌦r✓L(✓⇤)  ↵↵  Lk↵k2  g↵R2
where L is a “curvature” parameter  and g↵R2

↵(↵) is a “tolerance” parameter.

↵(↵) 

Note that these conditions (C1)-(C3) are imposed even when the model has a single clean structural
constraint; see [11]. Note that g↵ is usually a function on the problem size decreasing in the sample
size; in the standard Lasso with |I| = 1 for instance  g↵ = log p
n .
Our next assumption is on the interaction between the different structured components.
(C4) (Structural Incoherence) For all ↵ 2 ⌦↵ 

L

2 X↵2I

k↵k2 +X↵2I

h↵R2

↵(↵).

L✓⇤ +X↵2I

↵ + (|I| 1)L(✓⇤) X↵2I

L✓⇤ + ↵ 

Note that for a model with a single clean structural constraint  with |I| = 1  the condition (C4) is
trivially satisﬁed since the LHS becomes 0. We will see in the sequel that for a large collection
of loss functions including all linear loss functions  the condition (C4) simpliﬁes considerably  and
moreover holds with high probability  typically with h↵ = 0. We note that this condition is much
weaker than “incoherence” conditions typically imposed when analyzing speciﬁc instances of such
superposition-structured models (see e.g. references in the introduction)  where the assumptions
typically include (a) assuming that the structured subspaces (M↵)↵2I intersect only at {0}  and (b)
that the sizes of these subspaces are extremely small.
Finally  we will use the notion of subspace compatibility constant deﬁned in [11]  that captures the
relationship between the regularization function R(·) and the error norm k·k   over vectors in the
subspace M: (M k·k ) := supu2M\{0}
Theorem 1. Suppose we solve the M-estimation problem in (3)  with hybrid regularization
R(·; c)  where the convex-combination weights c are set as c↵ = ↵/P↵2I ↵  with ↵ 
2R⇤↵r✓↵L(✓⇤; Zn
1 ). Further  suppose conditions (C1) - (C4) are satisﬁed. Then  the parame-
kb✓  ✓⇤k ✓ 3|I|
2  32¯g2|I|⇣ max
↵⇧M?↵
↵R2

2¯◆ max
↵ ↵(M↵)⌘2
(✓⇤↵) +

↵pg↵ + h↵  
(✓⇤↵)i.

↵ ↵(M↵) + (|I|p⌧L/p¯) 

⌧L :=X↵2Ih32¯g22

2↵

|I| R↵⇧M?↵

ter error bounds are given as:

.

R
kuk

 

¯g := max

L

¯ :=

where

1

↵

↵2I

↵2I

Remarks: (R1) It is instructive to compare Theorem 1 to the main Theorem in [11]  where they
derive parameter error bounds for any M-estimator with a decomposable regularizer  for any
“clean” structure. Our theorem can be viewed as a generalization: we recover their theorem
when we have a single structure with |I| = 1. We cannot derive our result in turn from their
the
theorem applied to the M-estimator (3) with the hybrid regularization function R(·; c):
“superposition” structure is not captured by a pair of subspaces  nor is the hybrid regularization
function decomposable  as is required by their theorem. Our setting as well as analysis is strictly
more general  because of which we needed the additional structural incoherence assumption (C4)
(which is trivially satisﬁed when |I| = 1).
(R2) Agarwal et al. [1] provide Frobenius norm error bounds for the matrix-decomposition problem
of recovering the sum of low-rank and a general structured matrix. In addition to the greater
generality of our theorem and framework  Theorem 1 addresses two key drawbacks of their
theorem even in their speciﬁc setting. First  the proof for their theorem requires the regularization

4

penalty  for the second structure to be strongly bounded away from zero: their convergence rate
does not approach zero even with inﬁnite number of samples n. Theorem 1  in contrast  imposes

the weaker condition ↵  2R⇤↵r✓↵L(✓⇤; Zn

for the convergence rates to go to zero as a function of the samples. Second  they assumed much
stronger conditions for their theorem to hold; in Theorem 1 in contrast  we pose much milder
“local” RSC conditions (C3)  and a structural incoherence condition (C4).

1 )  which as we show in the corollaries  allows

(R3) The statement in the theorem is deterministic for ﬁxed choices of (↵). We also note that
the theorem holds for any set of subspace pairs (M↵ M?
↵ )↵2I with respect to which the cor-
responding regularizers are decomposable. As noted earlier  the M↵ should ideally be set to
the structured subspace in which the true parameter at least approximately lies  and which we
want to be as small as possible (note that the bound includes a term that depends on the size
of this subspace via the subspace compatibility constant). In particular  if we assume that the
(✓⇤↵) = 0 i.e. ✓⇤↵ 2M ↵  then we obtain the simpler bound in
subspaces are chosen so that ⇧M?↵
the following corollary.
Corollary 1. Suppose we solve the M-estimation problem in (1)  with hybrid regularization
R(·; c)  where the convex-combination weights c are set as c↵ = ↵/P↵2I ↵  with ↵ 
1 )  and suppose conditions (C1) - (C4) are satisﬁed. Further  suppose that the
2R⇤↵r✓↵L(✓⇤; Zn
subspace-pairs are chosen so that ✓⇤↵ 2M ↵. Then  the parameter error bounds are given as:

kb✓  ✓⇤k ✓ 3|I|

2¯◆ max

↵2I

↵ ↵(M↵).

It is now instructive to compare the bounds of Theorem 1  and Corollary 1. Theorem 1 has two terms:
the ﬁrst of which is the sole term in the bound in Corollary 1. This ﬁrst term can be thought of as
the “estimation error” component of the error bound  when the parameter has exactly the structure
being modeled by the regularizers. The second term can be thought of as the “approximation error”
component of the error bound  which is the penalty for the parameter not exactly lying in the struc-
tured subspaces modeled by the regularizers. The key term in the “estimation error” component  in
Theorem 1  and Corollary 1  is:

 = max
↵2I

↵ ↵(M↵).

Note that each ↵ is larger than a particular norm of the sample score function (gradient of the loss
at the true parameter): since the expected value of the score function is zero  the magnitude of the
sample score function captures the amount of “noise” in the data. This is in turn scaled by ↵(M↵) 
which captures the size of the structured subspace corresponding to the parameter component ✓⇤↵. 
can thus be thought of as capturing the amount of noise in the data relative to the particular structure
at hand.
We now provide corollaries showcasing our uniﬁed framework for varied statistical models such as
linear regression  multiple regression and principal component analysis  over varied superposition
structures.

4 Convergence Rates for Linear Regression

In this section  we consider the linear regression model:

(4)
where Y 2 Rn is the observation vector  and ✓⇤ 2 Rp is the true parameter. X 2 Rn⇥p is the
“observation” matrix; while w 2 Rn is the observation noise. For this class of statistical models  we
will consider the instantiation of (1) with the loss function L consisting of the squared loss:

Y = X✓⇤ + w 

(✓↵)↵2I( 1

min

nY  XX↵2I

✓↵

2

2

+X↵2I

↵ R↵(✓↵)) .

(5)

For this regularized least squared estimator (5)  conditions (C1-C2) in Theorem 1 trivially hold.
The restricted strong convexity condition (C3) reduces to the following. Noting that L(✓⇤ + ↵) 
L(✓⇤)  hr✓L(✓⇤)  ↵i = 1

2  we obtain the following restricted eigenvalue condition:

nkX↵k2

5

(D3) 1

nkX↵k2

2  Lk↵k2  g↵R2

↵(↵) for all ↵ 2 ⌦↵.

Finally  our structural incoherence condition reduces to the following: Noting that L(✓⇤ +
nP↵<hX↵  Xi in this speciﬁc
P↵2I ↵) + (|I| 1)L(✓⇤) P↵2I L(✓⇤ + ↵) = 2

nP↵<hX↵  Xi  L2 P↵2I k↵k2 +P↵2I h↵R2

4.1 Structural Incoherence with Gaussian Design

case 
(D4) 2

↵(↵).

We now show that the condition (D4) required for Theorem 1  holds with high probability when the
observation matrix is drawn from a so-called ⌃-Gaussian ensemble: where each row Xi is indepen-
dently sampled from N (0  ⌃). Before doing so  we ﬁrst state some assumption on the population
covariance matrix ⌃. Let PM denote the matrix corresponding to the projection operator for the
subspace M. We will then require the following assumption:
(C-Linear) Let ⇤ := max1 2n2 + 31 1 ( ¯M1 )
maxnmax⇣P ¯M↵⌃P ¯M⌘  max⇣P ¯M↵⌃P ¯M?⌘  max⇣P ¯M?↵

2 2 ( ¯M2 )o. For any ↵   2 I 

Proposition 2. Suppose each row Xi of the observation matrix X is independently sampled from
N (0  ⌃)  and the condition (C-Linear) (6) holds. Further  suppose that ⇧M?↵
(✓⇤↵) = 0  for all
↵ 2 I. Then  it holds that with probability at least 1 

⌃P ¯M?⌘o 

8|I|2⇤2|I|

hX↵  Xi 
when the number of samples scales as n  c⇣ (|I|2 )⇤2|I|

nX↵<

for some constant c that depends only on the distribution of X.

2 X↵ k↵k2
⌘2 max↵ ↵(M↵)2 + max{log p  log n} 

max{n p}
L

L

(6)

L

2 

2

 

4

.

Condition (D3) is the usual restricted eigenvalue condition which has been analyzed previously in
“clean-structured” model estimation  so that we can directly appeal to previous results [10  12] to
show that it holds with high probability when the observation matrix is drawn from the ⌃-Gaussian
ensemble.
We are now ready to derive the consequences of the deterministic bound in Theorem 1 for the case
of the linear regression model above.

4.2 Linear Regression with Sparse and Group-sparse structures

We now consider the following superposition structure  comprised of both sparse and group-sparse
structures. Suppose that a set of groups G = {G1  G2  . . .   Gq} are disjoint subsets of the index-
set {1  . . .   p}  each of size at most |Gi| m. Suppose that the linear regression parameter ✓⇤ is
a superposition of a group-sparse component ✓⇤g with respect to this set of groups G  as well as a
sparse component ✓⇤s with respect to the remaining indices {1  . . .   p}\[q
i=1Gi  so that ✓⇤ = ✓⇤g +✓⇤s.
Then  we use the hybrid regularization functionP↵2I ↵ R↵(✓↵) = sk✓sk1 + gk✓gk1 a where
k✓k1 a :=Pq

Corollary 2. Consider the linear model (4) where ✓⇤ is the sum of exact s-sparse ✓⇤s and exact sg
group-sparse ✓⇤g. Suppose that each row Xi of the observation matrix X is independently sampled
from N (0  ⌃). Further  suppose that (6) holds and w is sub-Gaussian with parameter . Then  if we
solve (5) with

t=1 k✓Gtka for a  2.

s = 8r log p

n

24
¯

kb✓  ✓⇤k2 

pn

and g = 8⇢ m11/a
max⇢r s log p

+r log q
n  
s)  c3/q2  we have the error bound:
+r sg log q
n .

psgm11/a

pn

n

 

then  with probability at least 1  c1 exp(c2n2

6

Let us brieﬂy compare the result from Corollary 2 with those from single-structured regularized
estimators. Since the total sparsity of ✓⇤ is bounded by k✓k0  msg + s  “clean” `1 regularized
◆ . On
◆ . We can easily verify that Corollary 2 achieves

least squares  with high probability  gives the bound [11]: kb✓`1  ✓⇤k2 = O✓q (msg+s) log p
kb✓`1/`2  ✓⇤k2 = O✓q (sg+s)m

the other hand  the support of ✓⇤ also can be interpreted as comprising sg + s disjoint groups in the
worst case  so that “clean” `1/`2 group regularization entails  with high probability  the bound [11]:

+q (sg+s) log q

better bounds  considering the fact p  mq.
5 Convergence Rates for Multiple Regression

n

n

n

Y = X⇥⇤ + W 

In this section  we consider the multiple linear regression model  with m linear regressions written
jointly as

(7)
where Y 2 Rn⇥m is the observation matrix: with each column corresponding to a separate linear
regression task  and ⇥⇤ 2 Rp⇥m is the collated set of parameters. X 2 Rn⇥p is the “observation”
matrix; while W 2 Rn⇥m is collated set of observation noise vectors. For this class of statistical
models  we will consider the instantiation of (1) with the loss function L consisting of the squared
loss:
(8)

min

(⇥↵)↵2In 1

n|||Y  XX↵2I

⇥↵|||2

F +X↵2I

↵ R↵(⇥↵)o.

n|||X↵|||2

In contrast to the linear regression model in the previous section  the model (7) has a matrix-
structured parameter; nonetheless conditions (C3-C4) in Theorem 1 reduce to the following con-
ditions that are very similar to those in the previous section  with the Frobenius norm replacing the
`2 norm:
(D3) 1
(D4) 2

↵(↵) for all ↵ 2 ⌦↵.

F  Lk↵k2  g↵R2

nP↵<hhX↵  Xii  L2 P↵2I k↵k2 +P↵2I h↵R2

where the notation hhA  Bii denotes the trace inner product  trace(A>B) =PiPj AijBij.
As in the previous linear regression example  we again impose the assumption (C-Linear) on the
population covariance matrix of a ⌃-Gaussian ensemble  but in this case with the notational change
of P ¯M↵ denoting the matrix corresponding to projection operator onto the row-spaces of matrices in
¯M↵. Thus  with the low-rank matrix structure discussed in Section 2  we would have P ¯M↵ = U U>.
Under the (C-Linear) assumption  the following proposition then extends Proposition 2:
Proposition 3. Consider the problem (8) with the matrix parameter ⇥. Under the same assumptions
as in Proposition 2  we have with probability at least 1 
L

max{n p}

↵(↵).

 

4

2

nX↵<

hhX↵  Xii 

2 X↵ |||↵|||2

F .

Consider an instance of this multiple linear regression model with the superposition structure con-
sisting of row-sparse  column-sparse and elementwise sparse matrices: ⇥⇤ =⇥ ⇤r +⇥⇤c +⇥⇤s. In order
to obtain estimators for this model  we use the hybrid regularization functionP↵2I ↵ R↵(✓↵) =
rk⇥rkr a + ck⇥ckc a + sk⇥sk1 where k·k r a denotes the sum of `a norm of rows for a  2 
and similarly k·k c a is the sum of `a norm of columns  and k·k 1 is entrywise `1 norm for matrix.
Corollary 3. Consider the multiple linear regression model (7) where ⇥⇤ is the sum of ⇥⇤r with sr
nonzero rows  ⇥⇤c with sc nonzero columns  and ⇥⇤s with s nonzero elements. Suppose that the design
matrix X is ⌃-Gaussian ensemble with the properties of column normalization and max(X)  pn.
Further  suppose that (6) holds and W is elementwise sub-Gaussian with parameter . Then  if we
solve (8) with
+r log m
s = 8r log p + log m
n o 

n o  and c = 8n p11/a

r = 8n m11/a

+r log p

pn

pn

 

n

7

with probability at least 1 c1 exp(c2n2
max⇢r s(log p + log m)
kb⇥  ⇥⇤k2 

36
¯

n

 

s) c3

p2  c3
psrm11/a
pn

m2   the error of the estimateb⇥ is bounded as:
+r sr log p
+r sc log m
.

pscp11/a

pn

n

n

 

6 Convergence Rates for Principal Component Analysis

In this section  we consider the robust/noisy principal component analysis problem  where we are
given n i.i.d. random vectors Zi 2 Rp where Zi = Ui + vi. Ui ⇠ N (0  ⇥⇤) is the “uncorrupted” set
of observations  with a low-rank covariance matrix ⇥⇤ = LLT   for some loading matrix L 2 Rp⇥r.
vi 2 Rp is a noise/error vector; in standard factor analysis  vi is a spherical Gaussian noise vector:
vi ⇠ N (0  2Ip⇥p) (or vi = 0); and the goal is to recover the loading matrix L from samples.
In PCA with sparse noise  vi ⇠ N (0  ⇤)  where ⇤ is elementwise sparse. In this case  the covari-
ance matrix of Zi has the form ⌃=⇥ ⇤ + ⇤  where ⇥⇤ is low-rank  and ⇤ is sparse. We can thus
write the sample covariance model as: Y := 1
i =⇥ ⇤ + ⇤ + W   where W 2 Rp⇥p
is a Wishart distributed random matrix. For this class of statistical models  we will consider the
following instantiation of (1):
min

nPn
F + ⇥|||⇥|||1 + kk1 .
(⇥ )|||Y  ⇥  |||2

where |||·|||1 denotes the nuclear norm while k·k 1 does the element-wise `1 norm (we will use |||·|||2
for the spectral norm.).
In contrast to the previous two examples  (9) includes a trivial design matrix  X = Ip⇥p  which al-
lows (D4) to hold under the simpler (C-linear) condition: where ⇤ is max1 2n2+ 31 1 ( ¯M1 )
2 2 ( ¯M2 )o 
maxnmax⇣P ¯M⇥ P ¯M⌘  max⇣P ¯M⇥ P ¯M?⌘  max⇣P ¯M?⇥ P ¯M⌘  max⇣P ¯M?⇥ P ¯M?⌘o 

i=1 ZiZT

1
16⇤2 .

(10)

(9)

Corollary 4. Consider the principal component analysis model where ⇥⇤ has the rank r at most
and ⇤ has s nonzero entries. Suppose that (10) holds. Then  given the choice of

with the additional constraint of k⇥k1  ↵

compute the error bound kb⇥  ⇥⇤k2 ⇣ max{p|||⌃|||2p rp

with probability at least 1  c1 exp(c2 log p).
Remarks. Agarwal et al. [1] also analyze this model  and propose to use the M-estimator in (9) 
p . Under a stricter “global” RSC condition  they
p} where ↵ is a
parameter between 1 and p. This bound is similar to that in Corollary 4  but with an additional
p   so that it does not go to zero as a function of n. It also faces a trade-off: a smaller
term ↵
value of ↵ to reduce error bound would make the assumption on the maximum element of ⇥⇤
stronger as well. Our corollaries do not suffer these lacunae; see also our remarks in (R2) in
Theorem 1. [14] extended the result of [1] to the special case where ⇥⇤ =⇥ ⇤r +⇥ ⇤s using the
notation of the previous section; the remarks above also apply here. Note that our work and [1]
derive Frobenius error bounds under restricted strong convexity conditions; other recent works
such as [7] also derive such Frobenius error bounds but under stronger conditions (see [1] for
details).

n  ⇢ (⌃)q s log p

n + ↵

Acknowledgments
We acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803  DMS-
1264033.

8

where ⇢(⌃) = maxj ⌃jj  the optimal error of (9) is bounded by

n

⇥ = 16p|||⌃|||2r p
kb⇥  ⇥⇤k2 

48
¯

 

n

   = 32⇢(⌃)r log p
  2⇢(⌃)r s log p
n  

max⇢p|||⌃|||2r rp

n

References
[1] A. Agarwal  S. Negahban  and M. J. Wainwright. Noisy matrix decomposition via convex

relaxation: Optimal rates in high dimensions. Annals of Statistics  40(2):1171–1197  2012.

[2] E. J. Cand`es  J. K. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate
measurements. Communications on Pure and Applied Mathematics  59(8):1207–1223  2006.
[3] E. J. Cand`es  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? Journal of

the ACM  58(3)  May 2011.

[4] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of lin-
ear inverse problems. In 48th Annual Allerton Conference on Communication  Control and
Computing  2010.

[5] V. Chandrasekaran  S. Sanghavi  P. A. Parrilo  and A. S. Willsky. Rank-sparsity incoherence

for matrix decomposition. SIAM Journal on Optimization  21(2)  2011.

[6] V. Chandrasekaran  P. A. Parrilo  and A. S. Willsky. Latent variable graphical model selection

via convex optimization. Annals of Statistics (with discussion)  40(4)  2012.

[7] D. Hsu  S. M. Kakade  and T. Zhang. Robust matrix decomposition with sparse corruptions.

IEEE Trans. Inform. Theory  57:7221–7234  2011.

[8] A. Jalali  P. Ravikumar  S. Sanghavi  and C. Ruan. A dirty model for multi-task learning. In

Neur. Info. Proc. Sys. (NIPS)  23  2010.

[9] M. McCoy and J. A. Tropp. Two proposals for robust pca using semideﬁnite programming.

Electron. J. Statist.  5:1123–1160  2011.

[10] S. Negahban and M. J. Wainwright. Estimation of (near) low-rank matrices with noise and

high-dimensional scaling. Annals of Statistics  39(2):1069–1097  2011.

[11] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-
dimensional analysis of M-estimators with decomposable regularizers. Statistical Science  27
(4):538–557  2012.

[12] G. Raskutti  M. J. Wainwright  and B. Yu. Restricted eigenvalue properties for correlated

gaussian designs. Journal of Machine Learning Research (JMLR)  99:2241–2259  2010.

[13] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed

Sensing: Theory and Applications. Cambridge University Press  2012.

[14] H. Xu and C. Leng. Robust multi-task regression with grossly corrupted observations. Inter.

Conf. on AI and Statistics (AISTATS)  2012.

[15] H. Xu  C. Caramanis  and S. Sanghavi. Robust pca via outlier pursuit. IEEE Transactions on

Information Theory  58(5):3047–3064  2012.

9

,Eunho Yang
Pradeep Ravikumar
Xu Jia
Bert De Brabandere
Tinne Tuytelaars
Luc Gool
Bo Dai
Sanja Fidler
Dahua Lin