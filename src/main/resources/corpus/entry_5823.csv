2013,Using multiple samples to learn mixture models,In the mixture models problem it is assumed that there are $K$ distributions $\theta_{1} \ldots \theta_{K}$ and one gets to observe a sample from a mixture of these distributions with unknown coefficients. The goal is to associate instances with their generating distributions  or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same $K$ underlying distributions  but with different mixing weights. As with topic modeling  having multiple samples is often a reasonable assumption.  Instead of pooling the data into one sample  we prove that it is possible to use the differences between the samples to better recover the underlying structure. We present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high.  The methods  when applied to topic modeling  allow generalization to words not present in the training data.,Using multiple samples to learn mixture

models

Stanford University

Jason Lee∗
Stanford  USA

jdl17@stanford.edu

Ran Gilad-Bachrach

Microsoft Research

Redmond  USA

rang@microsoft.com

Rich Caruana
Microsoft Research

Redmond  USA

rcaruana@microsoft.com

Abstract

In the mixture models problem it is assumed that there are K distributions
θ1  . . .   θK and one gets to observe a sample from a mixture of these distri-
butions with unknown coeﬃcients. The goal is to associate instances with
their generating distributions  or to identify the parameters of the hidden
distributions. In this work we make the assumption that we have access to
several samples drawn from the same K underlying distributions  but with
diﬀerent mixing weights. As with topic modeling  having multiple samples
is often a reasonable assumption. Instead of pooling the data into one sam-
ple  we prove that it is possible to use the diﬀerences between the samples
to better recover the underlying structure. We present algorithms that re-
cover the underlying structure under milder assumptions than the current
state of art when either the dimensionality or the separation is high. The
methods  when applied to topic modeling  allow generalization to words not
present in the training data.

1 Introduction

The mixture model has been studied extensively from several directions. In one setting it
is assumed that there is a single sample  that is a single collection of instances  from which
one has to recover the hidden information. A line of studies on clustering theory  starting
from [5] has proposed to address this problem by ﬁnding a projection to a low dimensional
space and solving the problem in this space. The goal of this projection is to reduce the
dimension while preserving the distances  as much as possible  between the means of the
underlying distributions. We will refer to this line as MM (Mixture Models). On the other
end of the spectrum  Topic modeling (TM)  [9  3]  assumes multiple samples (documents)
that are mixtures  with diﬀerent weights of the underlying distributions (topics) over words.
Comparing the two lines presented above shows some similarities and some diﬀerences. Both
models assume the same generative structure: a point (word) is generated by ﬁrst choosing
the distribution θi using the mixing weights and then selecting a point (word) according to
this distribution. The goal of both models is to recover information about the generative
model (see [10] for more on that). However  there are some key diﬀerences:
(a) In MM  there exists a single sample to learn from. In TM  each document is a mixture

of the topics  but with diﬀerent mixture weights.

(b) In MM  the points are represented as feature vectors while in TM the data is represented
as a word-document co-occurrence matrix. As a consequence  the model generated by
TM cannot assign words that did not previously appear in any document to topics.
∗Work done while the author was an intern at Microsoft Resaerch

1

(c) TM assumes high density of the samples  i.e.  that the each word appears multiple times.
However  if the topics were not discrete distributions  as is mostly the case in MM  each
"word" (i.e.  value) would typically appear either zero or one time  which makes the
co-occurrence matrix useless.

In this work we try to close the gap between MM and TM. Similar to TM  we assume that
multiple samples are available. However  we assume that points (words) are presented as
feature vectors and the hidden distributions may be continuous. This allows us to solve
problems that are typically hard in the MM model with greater ease and generate models
that generalize to points not in the training data which is something that TM cannot do.

1.1 Deﬁnitions and Notations
We assume a mixture model in which there are K mixture components θ1  . . .   θK deﬁned
over the space X. These mixture components are probability measures over X. We assume
that there are M mixture models (samples)  each drawn with diﬀerent mixture weights
Φ1  . . .   ΦM such that Φj = (φj
K) where all the weights are non-negative and sum to
1. Therefore  we have M diﬀerent probability measures D1  . . .   DM deﬁned over X such
i θi (A). We will denote

that for a measurable set A and j = 1  . . .   M we have Dj(A) =P

1  . . .   φj

i φj

by φmin the minimal value of φj
i.
In the ﬁrst part of this work  we will provide an algorithm that given samples S1  . . .   SM
from the mixtures D1  . . .   DM ﬁnds a low-dimensional embedding that preserves the dis-
tances between the means of each mixture.
In the second part of this work we will assume that the mixture components have disjoint
supports. Hence we will assume that X = ∪jCj such that the Cj’s are disjoint and for
every j  θj(Cj) = 1. Given samples S1  . . .   SM  we will provide an algorithm that ﬁnds the
supports of the underlying distributions  and thus clusters the samples.

1.2 Examples
Before we dive further in the discussion of our methods and how they compare to prior art 
we would like to point out that the model we assume is realistic in many cases. Consider the
following example: assume that one would like to cluster medical records to identify sub-
types of diseases (e.g.  diﬀerent types of heart disease). In the classical clustering setting
(MM)  one would take a sample of patients and try to divide them based on some similarity
criteria into groups. However  in many cases  one has access to data from diﬀerent hospitals
in diﬀerent geographical locations. The communities being served by the diﬀerent hospitals
may be diﬀerent in socioeconomic status  demographics  genetic backgrounds  and exposure
to climate and environmental hazards. Therefore  diﬀerent disease sub-types are likely to
appear in diﬀerent ratios in the diﬀerent hospital. However  if patients in two hospitals
acquired the same sub-type of a disease  parts of their medical records will be similar.
Another example is object classiﬁcation in images. Given an image  one may break it to
patches  say of size 10x10 pixels. These patches may have diﬀerent distributions based on the
object in that part of the image. Therefore  patches from images taken at diﬀerent locations
will have diﬀerent representation of the underlying distributions. Moreover  patches from
the center of the frame are more likely to contain parts of faces than patches from the
perimeter of the picture. At the same time  patches from the bottom of the picture are
more likely to be of grass than patches from the top of the picture.
In the ﬁrst part of this work we discuss the problem of identifying the mixture component
from multiple samples when the means of the diﬀerent components diﬀer and variances are
bounded. We focus on the problem of ﬁnding a low dimensional embedding of the data that
preserves the distances between the means since the problem of ﬁnding the mixtures in a
low dimensional space has already been address (see  for example [10]). Next  we address a
diﬀerent case in which we assume that the support of the hidden distributions is disjoint.
We show that in this case we can identify the supports of each distribution. Finally we
demonstrate our approaches on toy problems. The proofs of the theorems and lemmas

2

appear in the appendix. Table 1 summarizes the applicability of the algorithms presented
here to the diﬀerent scenarios.

1.3 Comparison to prior art

Low

DSC

MSP

√

High

clusters

dimension

Disjoint Overlapping
clusters
DSC  MSP

There are two common approaches in the
theoretical study of the MM model. The
method of moments [6  8  1] allows the re-
covery of the model but requires exponen-
tial running time and sample sizes. The
other approach  to which we compare our
results  uses a two stage approach. In the
ﬁrst stage  the data is projected to a low
dimensional space and in the second stage
the association of points to clusters is recov-
ered. Most of the results with this approach
assume that the mixture components are
Gaussians. Dasgupta [5]  in a seminal pa-
per  presented the ﬁrst result in this line.
He used random projections to project the points to a space of a lower dimension. This
work assumes that separation is at least Ω(σmax
n). This result has been improved in
a series of papers. Arora and Kannan [10] presented algorithms for ﬁnding the mixture
components which are  in most cases  polynomial in n and K. Vempala and Wang [11]

Table 1: Summary of the scenarios the MSP
(Multi Sample Projection) algorithm and the
DSC (Double Sample Clustering) algorithm

used PCA to reduce the required separation to Ω(cid:16)

σmaxK 1/4 log1/4(cid:0)n/φmin

(cid:1)(cid:17). They use

are designed to address.

dimension

PCA to project on the ﬁrst K principal components  however  they require the Gaussians
to be spherical. Kanan  Salmasian and Vempala [7] used similar spectral methods but
2/3/φ2min. Chaud-
were able to improve the results to require separation of only cσmaxK
huri [4] have suggested using correlations and independence between features under the
assumption that the means of the Gaussians diﬀer on many features. They require sepa-

σmaxpK log(Kσmax log n/φmin)(cid:17)  however they assume that the Gaussians

ration of Ω(cid:16)

are axis aligned and that the distance between the centers of the Gaussians is spread across
Ω (Kσmax log n/φmin) coordinates.
We present a method to project the problem into a space of dimension d∗ which is the
dimension of the aﬃne space spanned by the means of the distributions. We can ﬁnd
this projection and maintain the distances between the means to within a factor of 1 − .
The diﬀerent factors  σmax  n and  will aﬀect the sample size needed  but do not make the
problem impossible. This can be used as a preprocessing step for any of the results discussed
above. For example  combining with [5] yields an algorithm that requires a separation of only

(cid:17). However  using [11] will result in separation requirement
d∗(cid:17) ≤ Ω(cid:16)
σmaxpK log (Kσmax log d∗/φmin)(cid:17). There is also an improvement in terms of the

Ω(cid:16)
of Ω(cid:16)

value of σmax since we need only to control the variance in the aﬃne space spanned by the
means of the Gaussians and do not need to restrict the variance in orthogonal directions 
as long as it is ﬁnite. Later we also show that we can work in a more generic setting
where the distributions are not restricted to be Gaussians as long as the supports of the
distributions are disjoint. While the disjoint assumption may seem too strict  we note that
the results presented above make very similar assumptions. For example  even if the required
separation is σmaxK 1/2 then if we look at the Voronoi tessellation around the centers of the
Gaussians  each cell will contain at least 1 − (2π)−1
K 3/4 exp (−K/2) of the mass of the
Gaussian. Therefore  when K is large  the supports of the Gaussians are almost disjoint.

σmax

σmax

√

√

K

2 Projection for overlapping components

In this section we present a method to use multiple samples to project high dimensional
mixtures to a low dimensional space while keeping the means of the mixture components

3

Algorithm 1 Multi Sample Projection (MSP)
Inputs:
Samples S1  . . .   Sm from mixtures D1  . . .   Dm
Outputs:
Vectors ¯v1  . . .   ¯vm−1 which span the projected space
Algorithm:

1. For j = 1  . . .   m let ¯Ej be the mean of the sample Sj
2. For j = 1  . . .   m − 1 let ¯vj = ¯Ej − ¯Ej+1
3. return ¯v1  . . .   ¯vm−1

well separated. The main idea behind the Multi Sample Projection (MSP) algorithm is
simple. Let µi be the mean of the i’th component θi and let Ej be the mean of the j’th
mixture Dj. From the nature of the mixture  Ej is in the convex-hull of µ1  . . .   µK and
hence in the aﬃne space spanned by them; this is demonstrated in Figure 1. Under mild
assumptions  if we have suﬃciently many mixtures  their means will span the aﬃne space
spanned by µ1  . . .   µK. Therefore  the MSP algorithm estimates the Ej’s and projects to
the aﬃne space they span. The reason for selecting this sub-space is that by projecting on
this space we maintain the distance between the means while reducing the dimension to at
most K − 1. The MSP algorithm is presented in Algorithm 1. In the following theorem we
prove the main properties of the MSP algorithm. We will assume that X = Rn  the ﬁrst two
moments of θj are ﬁnite  and σ2max denotes maximal variance of any of the components in
any direction. The separation of the mixture components is minj6=j0 kµj − µj0k. Finally  we
will denote by d∗ the dimension of the aﬃne space spanned by the µj’s. Hence  d∗ ≤ K − 1.
Theorem 1. MSP Analysis
Let Ej = E [Dj] and let vj = Ej − Ej+1. Let Nj = |Sj|. The following holds for MSP:

1. The computational complexity of the MSP algorithm is nPM
2. For any  > 0  Pr(cid:2)supj

where n is the original dimension of the problem.

.

1
Nj

j

j=1 Nj + 2n (m − 1)

3. Let ¯µi be the projection of µi on the space spanned by ¯v1  . . .   ¯vM−1 and assume that

j αi

jvj and let A = maxi

P(cid:12)(cid:12)αi

j

(cid:12)(cid:12)

X

j

1
Nj

.

2

(cid:13)(cid:13) > (cid:3) ≤ nσ2max
(cid:13)(cid:13)Ej − ¯Ej
P
j be such that µi = P
P
(cid:21)

1
Nj

2

j

∀i  µi ∈ span{vj}. Let αi
then with probability of at least 1 − nσ2max

(cid:20)

Pr

max
i i0

|kµi − µi0k − k¯µi − ¯µi0k| > 

≤ 4nσ2maxA2

2

The MSP analysis theorem shows that with
large enough samples  the projection will
maintain the separation between the centers
of the distributions. Moreover  since this is
a projection  the variance in any direction
cannot increase. The value of A measures
the complexity of the setting. If the mixing
coeﬃcients are very diﬀerent in the diﬀer-
ent samples then A will be small. However 
if the mixing coeﬃcients are very similar  a
larger sample is required. Nevertheless  the
size of the sample needed is polynomial in
the parameters of the problem.
It is also
apparent that with large enough samples 
a good projection will be found  even with

Figure 1: The mean of the mixture compo-
nents will be in the convex hull of their means

demonstrated here by the red line.

4

large variances  high dimensions and close
centroids.
A nice property of the bounds presented here is that they assume only bounded ﬁrst and
second moments. Once a projection to a low dimensional space has been found  it is possible
to ﬁnd the clusters using approaches presented in section 1.3. However  the analysis of the
MSP algorithm assumes that the means of E1  . . .   EM span the aﬃne space spanned by
µ1  . . .   µK. Clearly  this implies that we require that m > d∗. However  when m is much
larger than d∗  we might end-up with a projection on too large a space. This could easily
be ﬁxed since in this case  ¯E1  . . .   ¯Em will be almost co-planar in the sense that there will
be an aﬃne space of dimension d∗ that is very close to all these points and we can project
onto this space.

3 Disjoint supports and the Double Sample Clustering (DSC)

algorithm

In this section we discuss the case where the underlying distributions have disjoint supports.
In this case  we do not make any assumption about the distributions. For example  we do
not require ﬁnite moments. However  as in the mixture of Gaussians case some sort of
separation between the distributions is needed  this is the role of the disjoint supports.
We will show that given two samples from mixtures with diﬀerent mixture coeﬃcients  it
is possible to ﬁnd the supports of the underlying distributions (clusters) by building a tree
of classiﬁers such that each leaf represents a cluster. The tree is constructed in a greedy
fashion. First we take the two samples  from the two distributions  and reweigh the examples
such that the two samples will have the same cumulative weight. Next  we train a classiﬁer
to separate between the two samples. This classiﬁer becomes the root of the tree. It also
splits each of the samples into two sets. We take all the examples that the classiﬁer assign
to the label +1(−1)  reweigh them and train another classiﬁer to separate between the two
samples. We keep going in the same fashion until we can no longer ﬁnd a classiﬁer that
splits the data signiﬁcantly better than random.
To understand why this algorithm works it is easier to look ﬁrst at the case where the
mixture distributions are known. If D1 and D2 are known  we can deﬁne the L1 distance
between them as L1 (D1  D2) = supA |D1 (A) –D2 (A)|.1 It turns out that the supremum is
attained by a set A such that for any i  µi (A) is either zero or one. Therefore  any inner
node in the tree splits the region without breaking clusters. This process proceeds until all
the points associated with a leaf are from the same cluster in which case  no classiﬁer can
distinguish between the classes.
When working with samples  we have to tolerate some error and prevent overﬁtting. One way
to see that is to look at the problem of approximating the L1 distance between D1 and D2
using samples S1 and S2. One possible way to do that is to deﬁne ˆL1 = supA
However  this estimate is almost surely going to be 1 if the underlying distributions are
absolutely continuous. Therefore  one has to restrict the class from which A can be selected
to a class of VC dimension small enough compared to the sizes of the samples. We claim
that asymptotically  as the sizes of the samples increase  one can increase the complexity of
the class until the clusters can be separated.
Before we proceed  we recall a result of [2] that shows the relation between classiﬁcation
and the L1 distance. We will abuse the notation and treat A both as a subset and as a
classiﬁer. If we mix D1 and D2 with equal weights then

(cid:12)(cid:12)(cid:12)(cid:12) A∩S1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).
(cid:12)(cid:12) − A∩S2
(cid:12)(cid:12)S1
(cid:12)(cid:12)S2

err (A) = D1 (X \ A) + D2 (A)
= 1 − D1 (A) + D2 (A)
= 1 − (D1 (A) − D2 (A)) .

Therefore  minimizing the error is equivalent to maximizing the L1 distance.

1the supremum is over all the measurable sets.

5

Algorithm 2 Double Sample Clustering (DSC)
Inputs:

• Samples S1  S2
• A binary learning algorithm L that given samples S1  S2 with weights w1  w2 ﬁnds
• A threshold τ > 0.

a classiﬁer h and an estimator e of the error of h.

Outputs:

• A tree of classiﬁers

Algorithm:

1. Let w1 = 1 and w2 = |S1|/|S2|
2. Apply L to S1 & S2 with weights w1 & w2 to get the classiﬁer h and estimator e.
3. If e ≥ 1

2 − τ 

(a) return a tree with a single leaf.

4. else

j = {x ∈ Sj s.t. h (x) > 0}
j = {x ∈ Sj s.t. h (x) < 0}

(a) For j = 1  2  let S+
(b) For j = 1  2  let S−
(c) Let T + be the tree returned by the DSC algorithm applied to S+
(d) Let T − be the tree returned by the DSC algorithm applied to S−
(e) return a tree in which c is at the root node and T − is its left subtree and T +

1 and S+
2
1 and S−
2

is its right subtree

i 6= φ2

i > φ2

i < φ2

The key observation for the DSC algorithm is that if φ1
i   then a set A that maximizes
the L1 distance between D1 and D2 is aligned with cluster boundaries (up to a measure zero).
Furthermore  A contains all the clusters for
which φ1
i and does not contain all the
clusters for which φ1
i . Hence  if we
split the space to A and ¯A we have few clus-
ters in each side. By applying the same trick
recursively in each side we keep on bisecting
the space according to cluster boundaries
until subspaces that contain only a single
cluster remain. These sub-spaces cannot be
further separated and hence the algorithm
will stop. Figure 2 demonstrates this idea.
The following lemma states this argument
mathematically:

i φj

i θi then

Lemma 1. If Dj =P
i max(cid:0)φ1
P
D2 (A∗) =P
2. If A∗ = ∪i:φ1

1. L1 (D1  D2)

i − φ2
i >φ2

i   0(cid:1).
i max(cid:0)φ1

i

Ci then D1 (A∗) −

i   0(cid:1).

i − φ2

3. If ∀i  φ1

6= φ2

i

i and A is such that
D1 (A)−D2 (A) = L1 (D1  D2) then
∀i  θi (A∆A∗) = 0.

≤

Figure 2: Demonstration of the DSC algo-
rithm. Assume that Φ1 = (0.4  0.3  0.3) for
the orange  green and blue regions respec-
tively and Φ2 = (0.5  0.1  0.4). The green re-
gion maximizes the L1 distance and therefore
will be separated from the blue and orange.
Conditioned on these two regions  the mixture
coeﬃcients are Φ1orange  blue = (4/7  3/7) and
Φ2orange  blue = (5/9  4/9). The region that
maximized this conditional L1 is the orange
regions that will be separated from the blue.

We conclude from Lemma 1 that if D1 and
D2 were explicitly known and one could have found a classiﬁer that best separates between
the distributions  that classiﬁer would not break clusters as long as the mixing coeﬃcients

6

are not identical. In order for this to hold when the separation is applied recursively in the
DSC algorithm it suﬃces to have that for every I ⊆ [1  . . .   K] if |I| > 1 and i ∈ I then

iP

φ1
i0∈I φ1
i0

iP

φ2
i0∈I φ2
i0

6=

to guarantee that at any stage of the algorithm clusters will not be split by the classiﬁer
(but may be sections of measure zero). This is also suﬃcient to guarantee that the leaves
will contain single clusters.
In the case where data is provided through a ﬁnite sample then some book-keeping is
needed. However  the analysis follows the same path. We show that with samples large
enough  clusters are only minimally broken. For this to hold we require that the learning
algorithm L separates the clusters according to this deﬁnition:
Deﬁnition 1. For I ⊆ [1  . . .   K] let cI : X 7→ {±1} be such that cI(x) = 1 if x ∈ ∪i∈I Ci
and cI(x) = −1 otherwise. A learning algorithm L separates C1  . . .   CK if for every   δ > 0
there exists N such that for every n > N and every measure ν over X×{±1} with probability
1 − δ over samples from νn:

1. The algorithm L returns an hypothesis h : X 7→ {±1} and an error estimator

e ∈ [0  1] such that |Prx y∼ν [h (x) 6= y] − e| ≤ 

2. h is such that

∀I 

Pr
x y∼ν

[h (x) 6= y] < Pr
x y∼ν

[cI (x) 6= y] +  .

Before we introduce the main statement  we deﬁne what it means for a tree to cluster the
mixture components:
Deﬁnition 2. A clustering tree is a tree in which in each internal node is a classiﬁer and
the points that end in a certain leaf are considered a cluster. A clustering tree -clusters the
mixture coeﬃcient θ1  . . .   θK if for every i ∈ 1  . . .   K there exists a leaf in the tree such
that the cluster L ⊆ X associated with this leaf is such that θi (L) ≥ 1 −  and θi0 (L) < 
for every i0 6= i.
To be able to ﬁnd a clustering tree  the two mixtures have to be diﬀerent. The following
deﬁnition captures the gap which is the amount of diﬀerence between the mixtures.
Deﬁnition 3. Let Φ1 and Φ2 be two mixture vectors. The gap  g  between them is

g = min

(cid:26)(cid:12)(cid:12)(cid:12)(cid:12)

iP

φ1
i0∈I φ1
i0

iP

φ2
i0∈I φ2
i0

−

(cid:12)(cid:12)(cid:12)(cid:12) : I ⊆ [1  . . .   K] and |I| > 1
(cid:27)

.

We say that Φ is b bounded away from zero if b ≤ mini φi.
Theorem 2. Assume that L separates θ1  . . .   θK  there is a gap g > 0 between Φ1 and Φ2
and both Φ1 and Φ2 are bounded away from zero by b > 0. For every ∗  δ∗ > 0 there exists
N = N (∗  δ∗  g  b  K) such that given two random samples of sizes N < n1  n2 from the two
mixtures  with probability of at least 1 − δ∗ the DSC algorithm will return a clustering tree
which ∗-clusters θ1  . . .   θK when applied with the threshold τ = g/8.

4 Empirical evidence

We conducted several experiments with synthetic data to compare diﬀerent methods when
clustering in high dimensional spaces. The synthetic data was generated from three Gaus-
sians with centers at points (0  0)   (3  0) and (−3  +3). On top of that  we added additional
dimensions with normally distributed noise. In the ﬁrst experiment we used unit variance
for all dimensions. In the second experiment we skewed the distribution so that the variance
in the other features is 5.
Two sets of mixing coeﬃcients for the three Gaussians were chosen at random 100 times by
selecting three uniform values from [0  1] and normalizing them to sum to 1. We generated

7

(a) Accuracy with spherical Gaussians

(b) Average accuracy with skewed Gaussians

Figure 3: Comparison the diﬀerent algorithms: The dimension of the problem is

presented in the X axis and the accuracy on the Y axis.

two samples with 80 examples each from the two mixing coeﬃcients. The DSC and MSP
algorithm received these two samples as inputs while the reference algorithms  which are
not designed to use multiple samples  received the combined set of 160 points as input.
We ran 100 trials. In each trial  each of the algorithms ﬁnds 3 Gaussians. We then measure
the percentage of the points associated with the true originating Gaussian after making the
best assignment of the inferred centers to the true Gaussians.
We compared several algorithms. K-means was used on the data as a baseline. We compared
three low dimensional projection algorithms. Following [5] we used random projections as
the ﬁrst of these. Second  following [11] we used PCA to project on the maximal variance
subspace. MSP was used as the third projection algorithm. In all projection algorithm we
ﬁrst projected on a one dimensional space and then applied K-means to ﬁnd the clusters.
Finally  we used the DSC algorithm. The DSC algorithm uses the classregtree function in
MATLAB as its learning oracle. Whenever K-means was applied  the MATLAB implemen-
tation of this procedure was used with 10 random initial starts.
Figure 3(a) shows the results of the ﬁrst experiment with unit variance in the noise dimen-
sions. In this setting  the Maximal Variance method is expected to work well since the ﬁrst
two dimensions have larger expected variance. Indeed we see that this is the case. However 
when the number of dimensions is large  MSP and DSC outperform the other methods;
this corresponds to the diﬃcult regime of low signal to noise ratio. In 12800 dimensions 
MSP outperforms Random Projections 90% of the time  Maximal Variance 80% of the time 
and K-means 79% of the time. DSC outperforms Random Projections  Maximal Variance
and K-means 84%  69%  and 66% of the time respectively. Thus the p-value in all these
experiments is < 0.01.
Figure 3(b) shows the results of the experiment in which the variance in the noise dimensions
is higher which creates a more challanging problem. In this case  we see that all the reference
methods suﬀer signiﬁcantly  but the MSP and the DSC methods obtain similar results as in
the previous setting. Both the MSP and the DSC algorithms win over Random Projections 
Maximal Variance and K-means more than 78% of the time when the dimension is 400 and
up. The p-value of these experiments is < 1.6 × 10−7.

5 Conclusions

The mixture problem examined here is closely related to the problem of clustering. Most
clustering data can be viewed as points generated from multiple underlying distributions or
generating functions  and clustering can be seen as the process of recovering the structure
of or assignments to these distributions. We presented two algorithms for the mixture
problem that can be viewed as clustering algorithms. The MSP algorithm uses multiple
samples to ﬁnd a low dimensional space to project the data to. The DSC algorithm builds
a clustering tree assuming that the clusters are disjoint. We proved that these algorithms
work under milder assumptions than currently known methods. The key message in this
work is that when multiple samples are available  often it is best not to pool the data into
one large sample  but that the structure in the diﬀerent samples can be leveraged to improve
clustering power.

8

37424752576267020004000600080001000012000Random ProjectionK-MeansMaximal VarianceMSPDSC38404244464850020004000600080001000012000References
[1] Mikhail Belkin and Kaushik Sinha  Polynomial learning of distribution families  Foun-
dations of Computer Science (FOCS)  2010 51st Annual IEEE Symposium on  IEEE 
2010  pp. 103–112.

[2] Shai Ben-David  John Blitzer  Koby Crammer  and Fernando Pereira  Analysis of rep-
resentations for domain adaptation  Advances in neural information processing systems
19 (2007)  137.

[3] David M Blei  Andrew Y Ng  and Michael I Jordan  Latent dirichlet allocation  the

Journal of machine Learning research 3 (2003)  993–1022.

[4] Kamalika Chaudhuri and Satish Rao  Learning mixtures of product distributions using

correlations and independence  Proc. of COLT  2008.

[5] Sanjoy Dasgupta  Learning mixtures of gaussians  Foundations of Computer Science 

1999. 40th Annual Symposium on  IEEE  1999  pp. 634–644.

[6] Adam Tauman Kalai  Ankur Moitra  and Gregory Valiant  Eﬃciently learning mixtures
of two gaussians  Proceedings of the 42nd ACM symposium on Theory of computing 
ACM  2010  pp. 553–562.

[7] Ravindran Kannan  Hadi Salmasian  and Santosh Vempala  The spectral method for

general mixture models  Learning Theory  Springer  2005  pp. 444–457.

[8] Ankur Moitra and Gregory Valiant  Settling the polynomial learnability of mixtures of
gaussians  Foundations of Computer Science (FOCS)  2010 51st Annual IEEE Sympo-
sium on  IEEE  2010  pp. 93–102.

[9] Christos H Papadimitriou  Hisao Tamaki  Prabhakar Raghavan  and Santosh Vem-
pala  Latent semantic indexing: A probabilistic analysis  Proceedings of the seven-
teenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database sys-
tems  ACM  1998  pp. 159–168.

[10] Arora Sanjeev and Ravi Kannan  Learning mixtures of arbitrary gaussians  Proceedings
of the thirty-third annual ACM symposium on Theory of computing  ACM  2001 
pp. 247–257.

[11] Santosh Vempala and Grant Wang  A spectral algorithm for learning mixtures of distri-
butions  Foundations of Computer Science  2002. Proceedings. The 43rd Annual IEEE
Symposium on  IEEE  2002  pp. 113–122.

9

,Jason Lee
Ran Gilad-Bachrach
Rich Caruana
Abir De
Isabel Valera
Niloy Ganguly
Sourangshu Bhattacharya
Manuel Gomez Rodriguez