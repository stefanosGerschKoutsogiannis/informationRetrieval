2011,Similarity-based Learning via Data Driven Embeddings,We consider the problem of classification using similarity/distance functions over data. Specifically  we propose a framework for defining the goodness of a (dis)similarity function with respect to a given learning task and propose algorithms that have guaranteed generalization properties when working with such good functions. Our framework unifies and generalizes the frameworks proposed by (Balcan-Blum 2006) and (Wang et al 2007). An attractive feature of our framework is its adaptability to data - we do not promote a fixed notion of goodness but rather let data dictate it. We show  by giving theoretical guarantees that the goodness criterion best suited to a problem can itself be learned which makes our approach applicable to a variety of domains and problems. We propose a landmarking-based approach to obtaining a classifier from such learned goodness criteria. We then provide a novel diversity based heuristic to perform task-driven selection of landmark points instead of random selection. We demonstrate the effectiveness of our goodness criteria learning method as well as the landmark selection heuristic on a variety of similarity-based learning datasets and benchmark UCI datasets on which our method consistently outperforms existing approaches by a significant margin.,Similarity-based Learning via Data Driven

Embeddings

Purushottam Kar

Indian Institute of Technology

Kanpur  INDIA

purushot@cse.iitk.ac.in

prajain@microsoft.com

Prateek Jain

Microsoft Research India

Bangalore  INDIA

Abstract

We consider the problem of classiﬁcation using similarity/distance functions over
data. Speciﬁcally  we propose a framework for deﬁning the goodness of a
(dis)similarity function with respect to a given learning task and propose algo-
rithms that have guaranteed generalization properties when working with such
good functions. Our framework uniﬁes and generalizes the frameworks proposed
by [1] and [2]. An attractive feature of our framework is its adaptability to data
- we do not promote a ﬁxed notion of goodness but rather let data dictate it. We
show  by giving theoretical guarantees that the goodness criterion best suited to a
problem can itself be learned which makes our approach applicable to a variety of
domains and problems. We propose a landmarking-based approach to obtaining a
classiﬁer from such learned goodness criteria. We then provide a novel diversity
based heuristic to perform task-driven selection of landmark points instead of ran-
dom selection. We demonstrate the effectiveness of our goodness criteria learning
method as well as the landmark selection heuristic on a variety of similarity-based
learning datasets and benchmark UCI datasets on which our method consistently
outperforms existing approaches by a signiﬁcant margin.

1

Introduction

Machine learning algorithms have found applications in diverse domains such as computer vision 
bio-informatics and speech recognition. Working in such heterogeneous domains often involves
handling data that is not presented as explicit features embedded into vector spaces. However in
many domains  for example co-authorship graphs  it is natural to devise similarity/distance functions
over pairs of points. While classical techniques like decision tree and linear perceptron cannot handle
such data  several modern machine learning algorithms such as support vector machine (SVM) can
be kernelized and are thereby capable of using kernels or similarity functions.
However  most of these algorithms require the similarity functions to be positive semi-deﬁnite
(PSD)  which essentially implies that the similarity stems from an (implicit) embedding of the data
into a Hilbert space. Unfortunately in many domains  the most natural notion of similarity does not
satisfy this condition - moreover  verifying this condition is usually a non-trivial exercise. Take for
example the case of images on which the most natural notions of distance (Euclidean  Earth-mover)
[3] do not form PSD kernels. Co-authorship graphs give another such example.
Consequently  there have been efforts to develop algorithms that do not make assumptions about
the PSD-ness of the similarity functions used. One can discern three main approaches in this area.
The ﬁrst approach tries to coerce a given similarity measure into a PSD one by either clipping or
shifting the spectrum of the kernel matrix [4  5]. However  these approaches are mostly restricted to
transductive settings and are not applicable to large scale problems due to eigenvector computation
requirements. The second approach consists of algorithms that either adapt classical methods like

1

k-NN to handle non-PSD similarity/distance functions and consequently offer slow test times [5] 
or are forced to solve non-convex formulations [6  7].
The third approach  which has been investigated recently in a series of papers [1  2  8  9]  uses the
similarity function to embed the domain into a low dimensional Euclidean space. More speciﬁcally 
these algorithms choose landmark points in the domain which then give the embedding. Assuming
a certain “goodness” property (that is formally deﬁned) for the similarity function  these models
offer both generalization guarantees in terms of how well-suited the similarity function is to the
classiﬁcation task as well as the ability to use fast algorithmic techniques such as linear SVM [10]
on the landmarked space. The model proposed by Balcan-Blum in [1] gives sufﬁcient conditions for
a similarity function to be well suited to such a landmarking approach. Wang et al. in [2] on the other
hand provide goodness conditions for dissimilarity functions that enable landmarking algorithms.
Informally  a similarity (or distance) function can be said to be good if points of similar labels
are closer to each other than points of different labels in some sense. Both the models described
above restrict themselves to a ﬁxed goodness criterion  which need not hold for the underlying data.
We observe that this might be too restrictive in many situations and present a framework that al-
lows us to tune the goodness criterion itself to the classiﬁcation problem at hand. Our framework
consequently uniﬁes and generalizes those presented in [1] and [2]. We ﬁrst prove generalization
bounds corresponding to landmarked embeddings under a ﬁxed goodness criterion. We then pro-
vide a uniform-convergence bound that enables us to learn the best goodness criterion for a given
problem. We further generalize our framework by giving the ability to incorporate any Lipschitz
loss function into our goodness criterion which allows us to give guarantees for the use of various
algorithms such as C-SVM and logistic regression on the landmarked space.
Now similar to [1  2]  our framework requires random sampling of training points to create the
embedding space1. However in practice  random sampling is inefﬁcient and requires sampling of a
large number of points to form a useful embedding  thereby increasing training and test time. To
address this issue  [2] proposes a heuristic to select the points that are to be used as landmarks.
However their scheme is tied to their optimization algorithm and is computationally inefﬁcient for
large scale data. In contrast  we propose a general heuristic for selecting informative landmarks
based on a novel notion of diversity which can then be applied to any instantiation of our model.
Finally  we apply our methods to a variety of benchmark datasets for similarity learning as well as
ones from the UCI repository. We empirically demonstrate that our learning model and landmark
selection heuristic consistently offers signiﬁcant improvements over the existing approaches.
In
particular  for small number of landmark points  which is a practically important scenario as it is
expensive to compute similarity function values at test time  our method provides  on an average 
accuracy boosts of upto 5% over existing methods. We also note that our methods can be applied on
top of any strategy used to learn the similarity measure (eg. MKL techniques [11]) or the distance
measure (eg. [12]) itself. Akin to [1]  our techniques can also be extended to learn a combination of
(dis)similarity functions but we do not explore these extensions in this paper.

2 Methodology
Let D be a ﬁxed but unknown distribution over the labeled input domain X and let (cid:96) : X →
{−1  +1} be a labeling over the domain. Given a (potentially non-PSD) similarity function2 K :
X × X → R  the goal is to learn a classiﬁer ˆ(cid:96) : X → {−1  +1} from a ﬁnite number of i.i.d.
samples from D that has bounded generalization error over D.
Now  learning a reasonable classiﬁer seems unlikely if the given similarity function does not have
any inherent “goodness” property. Intuitively  the goodness of a similarity function should be its
suitability to the classiﬁcation task at hand. For PSD kernels  the notion of goodness is deﬁned
in terms of the margin offered in the RKHS [13]. However  a more basic requirement is that the
similarity function should preserve afﬁnities among similarly labeled points - that is to say  a good
similarity function should not  on an average  assign higher similarity values to dissimilarly labeled
points than to similarly labeled points. This intuitive notion of goodness turns out to be rather robust

1Throughout the paper  we use the terms embedding space and landmarked space interchangeably.
2Results described in this section hold for distance functions as well; we present results with respect to

similarity functions for sake of simplicity.

2

in the sense that all PSD kernels that offer a good margin in their respective RKHSs satisfy some
form of this goodness criterion as well [14].
Recently there has been some interest in studying different realizations of this general notion of
goodness and developing corresponding algorithms that allow for efﬁcient learning with similar-
ity/distance functions. Balcan-Blum in [1] present a goodness criteria in which a good similarity
function is considered to be one that  for most points  assigns a greater average similarity to sim-
ilarly labeled points than to dissimilarly labeled points. More speciﬁcally  a similarity function is
(  γ)-good if there exists a weighing function w : X → R such that  at least a (1 − ) probability
mass of examples x ∼ D satisﬁes:

x(cid:48)∼D [w (x(cid:48)) K(x  x(cid:48))|(cid:96)(x(cid:48)) = (cid:96)(x)] ≥ E
E

x(cid:48)∼D [w (x(cid:48)) K(x  x(cid:48))|(cid:96)(x(cid:48)) (cid:54)= (cid:96)(x)] + γ.

(1)

where instead of average similarity  one considers an average weighted similarity to allow the deﬁ-
nition to be more general.
Wang et al in [2] deﬁne a distance function d to be good if a large fraction of the domain is  on
an average  closer to similarly labeled points than to dissimilarly labeled points. They allow these
averages to be calculated based on some distribution distinct from D  one that may be more suited
to the learning problem. However it turns out that their deﬁnition is equivalent to one in which one
again assigns weights to domain elements  as done by [1]  and the following holds

x(cid:48) x(cid:48)(cid:48)∼D×D [w(x(cid:48))w(x(cid:48)(cid:48)) sgn (d(x  x(cid:48)(cid:48)) − d(x  x(cid:48)))|(cid:96)(x(cid:48)) = (cid:96)(x)  (cid:96)(x(cid:48)(cid:48)) (cid:54)= (cid:96)(x)] > γ

E

(2)

Assuming their respective goodness criteria  [1] and [2] provide efﬁcient algorithms to learn clas-
siﬁers with bounded generalization error. However these notions of goodness with a single ﬁxed
criterion may be too restrictive in the sense that the data and the (dis)similarity function may not sat-
isfy the underlying criterion. This is  for example  likely in situations with high intra-class variance.
Thus there is need to make the goodness criterion more ﬂexible and data-dependent.
To this end  we unify and generalize both the above criteria to give a notion of goodness that is more
data dependent. Although the above goodness criteria (1) and (2) seem disparate at ﬁrst  they can
be shown to be special cases of a generalized framework where an antisymmetric function is used
to compare intra and inter-class afﬁnities. We use this observation to deﬁne our novel goodness
criterion using arbitrary bounded antisymmetric functions which we refer to as transfer functions.
This allows us to deﬁne a family of goodness criteria of which (1) and (2) form special cases ((1)
uses the identity function and (2) uses the sign function as transfer function). Moreover  the resulting
deﬁnition of a good similarity function is more ﬂexible and data dependent. In the rest of the paper
we shall always assume that our similarity functions are normalized i.e. for the domain of interest
X   sup
x y∈X
Deﬁnition 1 (Good Similarity Function). A similarity function K : X × X → R is said to be
an (  γ  B)-good similarity for a learning problem where   γ  B > 0 if for some antisymmetric
transfer function f : R → R and some weighing function w : X × X → [−B  B]  at least a (1 − )
probability mass of examples x ∼ D satisﬁes

K(x  y) ≤ 1.

x(cid:48) x(cid:48)(cid:48)∼D×D [w (x(cid:48)  x(cid:48)(cid:48)) f (K(x  x(cid:48)) − K(x  x(cid:48)(cid:48)))|(cid:96)(x(cid:48)) = (cid:96)(x)  (cid:96)(x(cid:48)(cid:48)) (cid:54)= (cid:96)(x)] ≥ Cf γ

E

(3)

where Cf = sup
x x(cid:48)∈X

f (K(x  x(cid:48))) − inf

x x(cid:48)∈X f (K(x  x(cid:48)))

As mentioned before  the above goodness criterion generalizes the previous notions of goodness3
and is adaptive to changes in data as it allows us  as shall be shown later  to learn the best possible
criterion for a given classiﬁcation task by choosing the most appropriate transfer function from a
parameterized family of functions. We stress that the property of antisymmetry for the transfer
function is crucial to the deﬁnition in order to provide a uniform treatment to points of all classes as
will be evident in the proof4 of Theorem 2.
As in [1  2]  our goodness criterion lends itself to a simple learning algorithm which consists of
i=1 (which we refer to

choosing a set of d random pairs of points from the domain P =(cid:8)(cid:0)x+

i   x−
3We refer the reader to the supplementary material (Section 2) for a discussion.
4Due to lack of space we relegate all proofs to the supplementary material

(cid:1)(cid:9)d

i

3

i ))(cid:1)d

landmarks : ΦL : X → Rd  ΦL(x) = (cid:0)f (K(x  x+
positive and negative samples  (cid:8)x+
i=1 ⊂ D+ and(cid:8)x−
(cid:9)d
(cid:80)d

as landmark pairs) and deﬁning an embedding of the domain into a landmarked space using these
i=1 ∈ Rd. The advantage of
i ) − K(x  x−
performing this embedding is the guaranteed existence of a large margin classiﬁer in the landmarked
space as shown below.
(cid:9)d
Theorem 2. If K is an (  γ  B)-good similarity with respect to transfer function f and weight func-
tion w then for any 1 > 0  with probability at least 1 − δ over the choice of d = (8/γ2) ln(2/δ1)
i )f(cid:0)K(x  x+
i=1 ⊂ D− respectively  the classiﬁer

i
h(x) = sgn[g(x)] where g(x) = 1
d
than  + 1 at margin γ
2 .
However  there are two hurdles to obtaining this large margin classiﬁer. Firstly  the existence of this
classiﬁer itself is predicated on the use of the correct transfer function  something which is unknown.
Secondly  even if an optimal transfer function is known  the above formulation cannot be converted
into an efﬁcient learning algorithm for discovering the (unknown) weights since the formulation
seeks to minimize the number of misclassiﬁcations which is an intractable problem in general.
We overcome these two hurdles by proposing a nested learning problem. First of all we assume
that for some ﬁxed loss function L  given any transfer function and any set of landmark pairs  it is
possible to obtain a large margin classiﬁer in the corresponding landmarked space that minimizes L.
Having made this assumption  we address below the issue of learning the optimal transfer function
for a given learning task. However as we have noted before  this assumption is not valid for arbitrary
loss functions. This is why  subsequently in Section 2.2  we shall show it to be valid for a large class
of loss functions by incorporating surrogate loss functions into our goodness criterion.

i )(cid:1) has error no more

i ) − K(x  x−

i=1 w(x+

i   x−

i

2.1 Learning the transfer function

(cid:8)(cid:0)x+

R be a class of antisymmetric functions and W = [−B  B]

In this section we present results that allow us to learn a near optimal transfer function from a family
of transfer functions. We shall assume  for some ﬁxed loss function L  the existence of an efﬁcient
routine which we refer to as TRAIN that shall return  for any landmarked space indexed by a set of
landmark pairs P  a large margin classiﬁer minimizing L. The routine TRAIN is allowed to make
use of additional training data to come up with this classiﬁer.
An immediate algorithm for choosing the best transfer function is to simply search the set of pos-
sible transfer functions (in an algorithmically efﬁcient manner) and choose the one offering lowest
training error. We show here that given enough landmark pairs  this simple technique  which we
refer to as FTUNE (see Algorithm 2) is guaranteed to return a near-best transfer function. For this
we prove a uniform convergence type guarantee on the space of transfer functions.
X×X be a class of weight
Let F ⊂ [−1  1]
functions. For two real valued functions f and g deﬁned on X   let (cid:107)f − g(cid:107)∞ := sup
|f (x) − g(x)|.
(cid:1)(cid:9)d
x∈X
Let B∞(f  r) := { f(cid:48) ∈ F | (cid:107)f − f(cid:48)(cid:107)∞ < r}. Let L be a CL-Lipschitz loss function. Let P =
i=1 be a set of (random) landmark pairs. For any f ∈ F  w ∈ W  deﬁne
x(cid:48) x(cid:48)(cid:48)∼D×D [w (x(cid:48)  x(cid:48)(cid:48)) f (K(x  x(cid:48)) − K(x  x(cid:48)(cid:48)))|(cid:96)(x(cid:48)) = (cid:96)(x)  (cid:96)(x(cid:48)(cid:48)) (cid:54)= (cid:96)(x)]
d(cid:88)
w(cid:0)x+
(cid:2)L(g(f w)(x))(cid:3) ≤ E
(cid:2)L(G(f w)(x))(cid:3) + 1. We now show that a similar result holds even if
(cid:2)L(cid:0)g(f w)(x)(cid:1)(cid:3) 5. Similarly  let w(G f ) be the best weighing function corresponding

Theorem 5 (see Section 2.2) guarantees us that for any ﬁxed f and any 1 > 0  if d is large enough
then E
one is allowed to vary f. Before stating the result  we develop some notation.
For any transfer function f and arbitrary choice of landmark pairs P  let w(g f ) be the best
weighing function for this choice of transfer function and landmark pairs i.e.
let w(g f ) =
arg min
w∈[−B B]d
to G i.e. w(G f ) = arg min

(cid:2)L(cid:0)G(f w)(x)(cid:1)(cid:3). Then we can ensure the following :

(cid:1) f(cid:0)K(x  x+

i   x−
G(f w)(x) =

i ) − K(x  x−

i )(cid:1)

g(f w)(x) =

i   x−

i

1
d

i=1

E
x∼D

x

x

i

E

E
x∼D

w∈W

5Note that the function g(f w)(x) is dictated by the choice of the set of landmark pairs P

4

Theorem 3. Let F be a compact class of transfer functions with respect to the inﬁnity norm and
1  δ > 0. Let N (F  r) be the size of the smallest -net over F with respect to the inﬁnity norm at
scale r = 1
random landmark pairs then
we have the following with probability greater than (1 − δ)

4CLB . Then if one chooses d = 64B2C2

ln

L

(cid:17)

(cid:16) 16B·N (F  r)
(cid:16)
(cid:104)

δ1

L

g(f w(g f ))(x)

L

G(f w(G f ))(x)

(cid:104)

(cid:104)(cid:12)(cid:12)(cid:12) E

x∼D

(cid:16)

sup
f∈F

2
1

(cid:17)(cid:105) − E

x∼D

(cid:17)(cid:105)(cid:12)(cid:12)(cid:12)(cid:105) ≤ 1

This result tells us that in a large enough landmarked space  we shall  for each function f ∈ F 
recover close to the best classiﬁer possible for that transfer function. Thus  if we iterate over the
set of transfer functions (or use some gradient-descent based optimization routine)  we are bound to
select a transfer function that is capable of giving a classiﬁer that is close to the best.

2.2 Working with surrogate loss functions

i

i   x−

The formulation of a good similarity function suggests a simple learning algorithm that involves
the construction of an embedding of the domain into a landmarked space on which the existence
of a large margin classiﬁer having low misclassiﬁcation rate is guaranteed. However  in order to

exploit this guarantee we would have to learn the weights w(cid:0)x+

(cid:1) associated with this classiﬁer

by minimizing the empirical misclassiﬁcation rate on some training set.
Unfortunately  not only is this problem intractable but also hard to solve approximately [15  16].
Thus what we require is for the landmarked space to admit a classiﬁer that has low error with
respect to a loss function that can also be efﬁciently minimized on any training set.
In such a
situation  minimizing the loss on a random training set would  with very high probability  give us
weights that give similar performance guarantees as the ones used in the goodness criterion.
With a similar objective in mind  [1] offers variants of its goodness criterion tailored to the hinge loss
function which can be efﬁciently optimized on large training sets (for example LIBSVM [17]). Here
we give a general notion of goodness that can be tailored to any arbitrary Lipschitz loss function.
Deﬁnition 4. A similarity function K : X × X → R is said to be an (  B)-good similarity for
a learning problem with respect to a loss function L : R → R+ where  > 0 if for some transfer
x∼D [L(G(x))] ≤  where
function f : R → R and some weighing function w : X ×X → [−B  B]  E
G(x) =

x(cid:48) x(cid:48)(cid:48)∼D×D [w (x(cid:48)  x(cid:48)(cid:48)) f (K(x  x(cid:48)) − K(x  x(cid:48)(cid:48)))|(cid:96)(x(cid:48)) = (cid:96)(x)  (cid:96)(x(cid:48)(cid:48)) (cid:54)= (cid:96)(x)]

E

One can see that taking the loss functions as L(x) = 1x<Cf γ gives us Equation 3 which deﬁnes a
good similarity under the 0−1 loss function. It turns out that we can  for any Lipschitz loss function 
give similar guarantees on the performance of the classiﬁer in the landmarked space.
Theorem 5.
If K is an (  B)-good similarity function with respect to a CL-Lipschitz loss
function L then for any 1 > 0  with probability at least 1 − δ over the choice of d =
1) ln(4B/δ1) positive and negative samples from D+ and D− respectively  the ex-
(16B2C 2
[L(g(x))] ≤  + 1 where g(x) =
pected loss of the classiﬁer g(x) with respect to L satisﬁes E

L/2

i=1 w(cid:0)x+
(cid:80)d

1
d

i   x−

i

(cid:1) f(cid:0)K(x  x+

i )(cid:1).

i ) − K(x  x−

x

γ . The 0 − 1 loss function and the loss
If the loss function is hinge loss at margin γ then CL = 1
function L(x) = 1x<γ (implicitly used in Deﬁnition 1 and Theorem 2) are not Lipschitz and hence
this proof technique does not apply to them.

2.3 Selecting informative landmarks

Recall that the generalization guarantees we described in the previous section rely on random se-
lection of landmark pairs from a ﬁxed distribution over the domain. However  in practice  a totally
random selection might require one to select a large number of landmarks  thereby leading to an
inefﬁcient classiﬁer in terms of training as well as test times. For typical domains such as computer
vision  similarity function computation is an expensive task and hence selection of a small number
of landmarks should lead to a signiﬁcant improvement in the test times. For this reason  we pro-
pose a landmark pair selection heuristic which we call DSELECT (see Algorithm 1). The heuristic

5

Algorithm 1 DSELECT
Require: A training set T   landmarking size d.
Ensure: A set of d landmark pairs/singletons.
1: L ← get-random-element(T )  PFTUNE ← ∅
2: for j = 2 to d do
z ← arg min
K(x  x(cid:48)).
3:
L ← L ∪ {z}  T ← T\{z}

(cid:80)

x∈T

x(cid:48)∈L

4:
5: end for
6: for j = 1 to d do
7:

Sample z1  z2 randomly from L with replace-
ment s.t. (cid:96)(z1) = 1  (cid:96)(z2) = −1
PFTUNE ← PFTUNE ∪ {(z1  z2)}

8:
9: end for
10: return L (for BBS)  PFTUNE (for FTUNE)

ity function K and a loss function L

Algorithm 2 FTUNE
Require: A family of transfer functions F  a similar-
Ensure: An optimal transfer function f∗ ∈ F.
1: Select d landmark pairs P .
2: for all f ∈ F do
3:
4: end for
5: f∗ ← arg min
Lf
6: return (f∗  wf∗ ).

wf ← TRAIN(P  L)  Lf ← L (wf )

f∈F

1

|S|(|S|−1)

generalizes naturally to multi-class problems and can also be applied to the classiﬁcation model of
Balcan-Blum that uses landmark singletons instead of pairs.
(cid:80)
At the core of our heuristic is a novel notion of diversity among landmarks. Assuming K is a nor-
malized similarity kernel  we call a set of points S ⊂ X diverse if the average inter-point similarity
x y∈S x(cid:54)=y K(x  y) (cid:28) 1 (in case we are working with a distance kernel we
is small i.e
would require large inter-point distances). The key observation behind DSELECT is that a non-
diverse set of landmarks would cause all data points to receive identical embeddings and linear
separation would be impossible. Small inter-landmark similarity  on the other hand would imply
that the landmarks are well-spread in the domain and can capture novel patterns in the data.
Similar notions of diversity have been used in the past for ensemble classiﬁers [18] and k-NN clas-
siﬁers [5]. Here we use this notion to achieve a better embedding into the landmarked space. Ex-
perimental results demonstrate that the heuristic offers signiﬁcant performance improvements over
random landmark selection (see Figure 1). One can easily extend Although Algorithm 1 to multi-
class problems by selecting a ﬁxed number of landmarks from each class.

3 Empirical results

In this section  we empirically study the performance of our proposed methods on a variety of bench-
mark datasets. We refer to the algorithmic formulation presented in [1] as BBS and its augmentation
using DSELECT as BBS+D. We refer to the formulation presented in [2] as DBOOST. We refer to
our transfer function learning based formulation as FTUNE and its augmentation using DSELECT
as FTUNE+D. In multi-class classiﬁcation scenarios we will use a one-vs-all formulation which
presents us with an opportunity to further exploit the transfer function by learning separate transfer
function per class (i.e. per one-vs-all problem). We shall refer to our formulation using a single
(resp. multiple) transfer function as FTUNE+D-S (resp. FTUNE+D-M). We take the class of ramp
functions indexed by a slope parameter as our set of transfer functions. We use 6 different values
of the slope parameter {1  5  10  50  100  1000}. Note that these functions (approximately) include
both the identity function (used by [1]) and the sign function (used by [2]).
Our goal in this section is two-fold: 1) to show that our FTUNE method is able to learn a more
suitable transfer function for the underlying data than the existing methods BBS and DBOOST and
2) to show that our diversity based heuristic for landmark selection performs better than random
selection. To this end  we perform experiments on a few benchmark datasets for learning with simi-
larity (non-PSD) functions [5] as well as on a variety of standard UCI datasets where the similarity
function used is the Gaussian kernel function.
For our experiments  we implemented our methods FTUNE and FTUNE+D as well as BBS and
BBS+D using MATLAB while using LIBLINEAR [10] for SVM classiﬁcation. For DBOOST  we
use the C++ code provided by the authors of [2]. On all the datasets we randomly selected a ﬁxed
percentage of data for training  validation and testing. Except for DBOOST   we selected the SVM
penalty constant C from the set {1  10  100  1000} using validation. For each method and dataset  we
report classiﬁcation accuracies averaged over 20 runs. We compare accuracies obtained by different
methods using t-test at 95% signiﬁcance level.

6

Dataset/Method
AmazonBinary

AuralSonar

Patrol
Voting
Protein
Mirex07
Amazon47
FaceRec

BBS

DBOOST
0.73(0.13)
0.77(0.10)
0.82(0.08)
0.81(0.08)
0.51(0.06)
0.34(0.11)
0.95(0.03)
0.94(0.03)
1.00(0.01)
0.98(0.02)
0.12(0.01)
0.21(0.03)
0.39(0.06)
0.07(0.04)
0.20(0.04)
0.12(0.03)
(a) 30 Landmarks

FTUNE+D-S
0.84(0.12)
0.80(0.08)
0.58(0.06)
0.94(0.04)
0.98(0.02)
0.28(0.03)
0.61(0.08)
0.63(0.04)

Dataset/Method
AmazonBinary

AuralSonar

Patrol
Voting
Protein
Mirex07
Amazon47
FaceRec

BBS

0.78(0.11)
0.88(0.06)
0.79(0.05)
0.97(0.02)
0.98(0.02)
0.17(0.02)
0.40(0.13)
0.27(0.05)

DBOOST
0.82(0.10)
0.85(0.07)
0.55(0.12)
0.97(0.01)
0.99(0.02)
0.31(0.04)
0.07(0.05)
0.19(0.03)
(b) 300 Landmarks

FTUNE+D-S
0.88(0.07)
0.85(0.07)
0.79(0.07)
0.97(0.02)
0.98(0.02)
0.35(0.02)
0.66(0.07)
0.64(0.04)

Table 1: Accuracies for Benchmark Similarity Learning Datasets for Embedding Dimensional-
ity=30  300. Bold numbers indicate the best performance with 95% conﬁdence level.

Figure 1: Accuracy obtained by various methods on four different datasets as the number of land-
marks used increases. Note that for small number of landmarks (30  50) our diversity based landmark
selection criteria increases accuracy for both BBS and our method FTUNE-S signiﬁcantly.

3.1 Similarity learning datasets

First  we conduct experiments on a few similarity learning datasets [5]; these datasets provide a
(non-PSD) similarity matrix along with class labels. For each of the datasets  we randomly select
70% of the data for training  10% for validation and the remaining for testing purposes. We then
apply our FTUNE-S  FTUNE+D-S  BBS+D methods along with BBS and DBOOST with varying
number of landmark pairs. Note that we do not apply our FTUNE-M method to these datasets as it
overﬁts heavily to these datasets as typically they are small in size.
We ﬁrst compare the accuracy achieved by FTUNE+D-S with the existing methods. Table 1 com-
pares the accuracies achieved by our FTUNE+D-S method with those of BBS and DBOOST over
different datasets when using landmark sets of sizes 30 and 300. Numbers in brackets denote stan-
dard deviation over different runs. Note that in both the tables FTUNE+D-S is one of the best
methods (upto 95% signiﬁcance level) on all but one dataset. Furthermore  for datasets with large
number of classes such as Amazon47 and FaceRec our method outperforms BBS and DBOOST by
at least 20% percent. Also  note that some of the datasets have multiple bold faced methods  which
means that the two sample t-test (at 95% level) rejects the hypothesis that their mean is different.
Next  we evaluate the effectiveness of our landmark selection criteria for both BBS and our method.
Figure 1 shows the accuracies achieved by various methods on four different datasets with increasing
number of landmarks. Note that in all the datasets  our diversity based landmark selection criteria
increases the classiﬁcation accuracy by around 5 − 6% for small number of landmarks.

3.2 UCI benchmark datasets

We now compare our FTUNE method against existing methods on a variety of UCI datasets [19].
We ran experiments with FTUNE and FTUNE+D but the latter did not provide any advantage. So
for lack of space we drop it from our presentation and only show results for FTUNE-S (FTUNE with
a single transfer function) and FTUNE-M (FTUNE with one transfer function per class). Similar
to [2]  we use the Gaussian kernel function as the similarity function for evaluating our method.
We set the “width” parameter in the Gaussian kernel to be the mean of all pair-wise training data
distances  a standard heuristic. For all the datasets  we randomly select 50% data for training  20%
for validation and the remaining for testing. We report accuracy values averaged over 20 runs for
each method with varying number of landmark pairs.

7

501001502002503000.50.60.70.80.91AmazonBinary (Accuracy vs Landmarks)Number of LandmarksAccuracy  FTUNE+DFTUNEBBS+DBBSDBOOST5010015020025030000.10.20.30.40.50.60.7Amazon47 (Accuracy vs Landmarks)Number of LandmarksAccuracy  FTUNE+DFTUNEBBS+DBBSDBOOST501001502002503000.10.150.20.250.30.350.40.45Mirex07 (Accuracy vs Landmarks)Number of LandmarksAccuracy  FTUNE+DFTUNEBBS+DBBSDBOOST010020030000.10.20.30.40.50.6FaceRec (Accuracy vs Landmarks)Number of LandmarksAccuracy  FTUNE+DFTUNEBBS+DBBSDBOOSTDataset/Method

Cod-rna
Isolet
Letters
Magic

Pen-digits
Nursery
Faults

Mfeat-pixel
Mfeat-zernike

Opt-digits
Satellite
Segment

BBS

0.93(0.01)
0.81(0.01)
0.67(0.02)
0.82(0.01)
0.94(0.01)
0.91(0.01)
0.70(0.01)
0.94(0.01)
0.79(0.02)
0.92(0.01)
0.85(0.01)
0.90(0.01)

DBOOST
0.89(0.01)
0.67(0.01)
0.58(0.01)
0.81(0.01)
0.93(0.01)
0.91(0.01)
0.68(0.02)
0.91(0.01)
0.72(0.02)
0.89(0.01)
0.86(0.01)
0.93(0.01)

FTUNE-S
0.93(0.01)
0.84(0.01)
0.69(0.01)
0.84(0.01)
0.97(0.01)
0.90(0.01)
0.70(0.02)
0.95(0.01)
0.79(0.02)
0.94(0.01)
0.86(0.01)
0.92(0.01)

FTUNE-M
0.93(0.01)
0.83(0.01)
0.68(0.02)
0.84(0.01)
0.97(0.00)
0.90(0.00)
0.71(0.02)
0.94(0.01)
0.79(0.02)
0.94(0.01)
0.87(0.01)
0.92(0.01)

Dataset/Method

Cod-rna
Isolet
Letters
Magic

Pen-digits
Nursery
Faults

Mfeat-pixel
Mfeat-zernike

Opt-digits
Satellite
Segment

BBS

0.94(0.00)
0.91(0.01)
0.72(0.01)
0.84(0.01)
0.96(0.00)
0.93(0.01)
0.72(0.02)
0.96(0.01)
0.81(0.01)
0.95(0.01)
0.85(0.01)
0.90(0.01)

DBOOST
0.93(0.00)
0.89(0.01)
0.84(0.01)
0.84(0.00)
0.99(0.00)
0.97(0.00)
0.74(0.02)
0.97(0.01)
0.79(0.01)
0.97(0.00)
0.90(0.01)
0.96(0.01)

FTUNE-S
0.94(0.00)
0.93(0.01)
0.83(0.01)
0.85(0.01)
0.99(0.00)
0.96(0.00)
0.73(0.02)
0.97(0.01)
0.82(0.02)
0.98(0.00)
0.89(0.01)
0.96(0.01)

FTUNE-M
0.94(0.00)
0.93(0.00)
0.83(0.01)
0.85(0.01)
0.99(0.00)
0.97(0.00)
0.73(0.02)
0.97(0.01)
0.82(0.01)
0.98(0.00)
0.89(0.01)
0.96(0.01)

(a) 30 Landmarks

(b) 300 Landmarks

Table 2: Accuracies for Gaussian Kernel for Embedding Dimensionality=30. Bold numbers indicate
the best performance with 95% conﬁdence level. Note that both our methods  especially FTUNE-S 
performs signiﬁcantly better than the existing methods.

Figure 2: Accuracy achieved by various methods on four different UCI repository datasets as the
number of landmarks used increases. Note that both FTUNE-S and FTUNE-M perform signiﬁcantly
better than BBS and DBOOST for small number of landmarks (30  50).

Table 2 compares the accuracies obtained by our FTUNE-S and FTUNE-M methods with those of
BBS and DBOOST when applied to different UCI benchmark datasets. Note that FTUNE-S is one
of the best on most of the datasets for both the landmarking sizes. Also  BBS performs reasonably
well for small landmarking sizes while DBOOST performs well for large landmarking sizes. In
contrast  our method consistently outperforms the existing methods in both the scenarios.
Next  we study accuracies obtained by our method for different landmarking sizes. Figure 2 shows
accuracies obtained by various methods as the number of landmarks selected increases. Note that
the accuracy curve of our method dominates the accuracy curves of all the other methods  i.e. our
method is consistently better than the existing methods for all the landmarking sizes considered.

3.3 Discussion

We note that since FTUNE selects its output by way of validation  it is susceptible to over-ﬁtting on
small datasets but at the same time  capable of giving performance boosts on large ones. We observe
a similar trend in our experiments – on smaller datasets (such as those in Table 1 with average dataset
size 660)  FTUNE over-ﬁts and performs worse than BBS and DBOOST. However  even in these
cases  DSELECT (intuitively) removes redundancies in the landmark points thus allowing FTUNE
to recover the best transfer function. In contrast  for larger datasets like those in Table 2 (average
size 13200)  FTUNE is itself able to recover better transfer functions than the baseline methods
and hence both FTUNE-S and FTUNE-M perform signiﬁcantly better than the baselines. Note that
DSELECT is not able to provide any advantage here since the datasets sizes being large  greedy
selection actually ends up hurting the accuracy.

Acknowledgments

We thank the authors of [2] for providing us with C++ code of their implementation. P. K. is
supported by Microsoft Corporation and Microsoft Research India under a Microsoft Research India
Ph.D. fellowship award. Most of this work was done while P. K. was visiting Microsoft Research
Labs India  Bangalore.

8

501001502002503000.650.70.750.80.850.90.951Isolet (Accuracy vs Landmarks)Number of LandmarksAccuracy  FTUNE (Single)FTUNE (Multiple)BBSDBOOST501001502002503000.50.60.70.80.91Letters (Accuracy vs Landmarks)Number of LandmarksAccuracy  FTUNE (Single)FTUNE (Multiple)BBSDBOOST501001502002503000.930.940.950.960.970.980.991Pen−digits (Accuracy vs Landmarks)Number of LandmarksAccuracy  FTUNE (Single)FTUNE (Multiple)BBSDBOOST501001502002503000.880.90.920.940.960.98Number of LandmarksAccuracyOpt−digits (Accuracy vs Landmarks)  FTUNE (Single)FTUNE (Multiple)BBSDBOOSTReferences

[1] Maria-Florina Balcan and Avrim Blum. On a Theory of Learning with Similarity Functions. In Interna-

tional Conference on Machine Learning  pages 73–80  2006.

[2] Liwei Wang  Cheng Yang  and Jufu Feng. On Learning with Dissimilarity Functions. In International

Conference on Machine Learning  pages 991–998  2007.

[3] Piotr Indyk and Nitin Thaper. Fast Image Retrieval via Embeddings. In International Workshop Statistical

and Computational Theories of Vision  2003.

[4] El˙zbieta Pe¸kalska and Robert P. W. Duin. On Combining Dissimilarity Representations.

Classiﬁer Systems  pages 359–368  2001.

In Multiple

[5] Yihua Chen  Eric K. Garcia  Maya R. Gupta  Ali Rahimi  and Luca Cazzanti. Similarity-based Classiﬁ-

cation: Concepts and Algorithms. Journal of Machine Learning Research  10:747–776  2009.

[6] Cheng Soon Ong  Xavier Mary  St´ephane Canu  and Alexander J. Smola. Learning with non-positive

Kernels. In International Conference on Machine Learning  2004.

[7] Bernard Haasdonk. Feature Space Interpretation of SVMs with Indeﬁnite Kernels. IEEE Transactions on

Pattern Analysis and Machince Intelligence  27(4):482–492  2005.

[8] Thore Graepel  Ralf Herbrich  Peter Bollmann-Sdorra  and Klaus Obermayer. Classiﬁcation on Pairwise

Proximity Data. In Neural Information Processing Systems  page 438444  1998.

[9] Maria-Florina Balcan  Avrim Blum  and Nathan Srebro. Improved Guarantees for Learning via Similarity

Functions. In 21st Annual Conference on Computational Learning Theory  pages 287–298  2008.

[10] Rong-En Fan  Kai-Wei Chang  Cho-Jui Hsieh  Xiang-Rui Wang  and Chih-Jen Lin. LIBLINEAR: A

Library for Large Linear Classiﬁcation. Journal of Machine Learning Research  9:1871–1874  2008.

[11] Manik Varma and Bodla Rakesh Babu. More Generality in Efﬁcient Multiple Kernel Learning. In 26th

Annual International Conference on Machine Learning  pages 1065–1072  2009.

[12] Prateek Jain  Brian Kulis  Jason V. Davis  and Inderjit S. Dhillon. Metric and Kernel Learning using a

Linear Transformation. To appear  Journal of Machine Learning (JMLR)  2011.

[13] Maria-Florina Balcan  Avrim Blum  and Santosh Vempala. Kernels as Features: On Kernels  Margins 

and Low-dimensional Mappings. Machine Learning  65(1):79–94  2006.

[14] Nathan Srebro. How Good Is a Kernel When Used as a Similarity Measure? In 20th Annual Conference

on Computational Learning Theory  pages 323–335  2007.

[15] M. R. Garey and D.S. Johnson. Computers and Intractability: A Guide to the theory of NP-Completeness.

Freeman  San Francisco  1979.

[16] Sanjeev Arora  L´aszl´o Babai  Jacques Stern  and Z. Sweedyk. The Hardness of Approximate Optima in
Lattices  Codes  and Systems of Linear Equations. Journal of Computer and System Sciences  54(2):317–
331  April 1997.

[17] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transac-

tions on Intelligent Systems and Technology  2(3):27:1–27:27  2011.

[18] Krithika Venkataramani and B. V. K. Vijaya Kumar. Designing classiﬁers for fusion-based biometric
veriﬁcation. In Plataniotis Boulgouris and Micheli-Tzankou  editors  Biometrics: Theory  Methods and
Applications. Springer  2009.

[19] A. Frank and Arthur Asuncion. UCI Machine Learning Repository. http://archive.ics.uci.

edu/ml  2010. University of California  Irvine  School of Information and Computer Sciences.

9

,Franz Kiraly
Louis Theran
Xi Chen
Yu Cheng
Bo Tang
Liangpeng Zhang
Ke Tang
Xin Yao