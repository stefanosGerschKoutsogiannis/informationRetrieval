2019,Interior-Point Methods Strike Back: Solving the Wasserstein Barycenter Problem,Computing the Wasserstein barycenter of a set of probability measures under the optimal transport metric can quickly become prohibitive for traditional second-order algorithms  such as interior-point methods  as the support size of the measures increases. In this paper  we overcome the difficulty by developing a new adapted interior-point method that fully exploits the problem's special matrix structure to reduce the iteration complexity and speed up the Newton procedure. Different from regularization approaches  our method achieves a well-balanced tradeoff between accuracy and speed. A numerical comparison on various distributions with existing algorithms exhibits the computational advantages of our approach. Moreover  we demonstrate the practicality of our algorithm on image benchmark problems including MNIST and Fashion-MNIST.,Interior-point Methods Strike Back: Solving the

Wasserstein Barycenter Problem

Dongdong Ge

Research Institute for Interdisciplinary Sciences
Shanghai University of Finance and Economics

ge.dongdong@mail.shufe.edu.cn

Haoyue Wang∗

School of Mathematical Sciences

Fudan University

haoyuewang14@fudan.edu.cn

Zikai Xiong∗

Fudan University

zkxiong16@fudan.edu.cn

School of Mathematical Sciences

Department of Management Science and Engineering

Yinyu Ye

Stanford University
yyye@stanford.edu

Abstract

Computing the Wasserstein barycenter of a set of probability measures under the
optimal transport metric can quickly become prohibitive for traditional second-
order algorithms  such as interior-point methods  as the support size of the measures
increases. In this paper  we overcome the difﬁculty by developing a new adapted
interior-point method that fully exploits the problem’s special matrix structure to
reduce the iteration complexity and speed up the Newton procedure. Different
from regularization approaches  our method achieves a well-balanced tradeoff
between accuracy and speed. A numerical comparison on various distributions
with existing algorithms exhibits the computational advantages of our approach.
Moreover  we demonstrate the practicality of our algorithm on image benchmark
problems including MNIST and Fashion-MNIST.

1

Introduction

To compare  summarize  and combine probability measures deﬁned on a space is a fundamental task
in statistics and machine learning. Given support points of probability measures in a metric space and
a transportation cost function (e.g. the Euclidean distance)  Wasserstein distance deﬁnes a distance
between two measures as the minimal transportation cost between them. This notion of distance
leads to a host of important applications  including text classiﬁcation [30]  clustering [25  26  15  31] 
unsupervised learning [23  13]  semi-supervised learning [47]  supervised-learning[27  19]  statistics
[38  39  48  21]  and others [7  41  1  44  37]. Given a set of measures in the same space  the
2-Wasserstein barycenter is deﬁned as the measure minimizing the sum of squared 2-Wasserstein
distances to all measures in the set. For example  if a set of images (with common structure but
varying noise) are modeled as probability measures  then the Wasserstein barycenter is a mixture
of the images that share this common structure. The Wasserstein barycenter better captures the
underlying geometric structure than the barycenter deﬁned by the Euclidean or other distances. As a
result  the Wasserstein barycenter has applications in clustering [25  26  15]  image retrieval [14] and
others [32  43  11  34].
From the computation point of view  ﬁnding the barycenter of a set of discrete measures can be
formulated by linear programming[6  8]. Nonetheless  state-of-the-art linear programming solvers
do not scale with the immense amount of data involved in barycenter calculations. Current research

∗Haoyue Wang and Zikai Xiong are corresponding authors.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

on computation mainly follows two types of methods. The ﬁrst type attempts to solve the linear
program (or some equivalent problem) with scalable ﬁrst-order methods. J.Ye et al.
[54] use
modiﬁed Bregman ADMM(BADMM) – introduced by [51] – to compute Wasserestein barycenters
for clustering problems. L.Yang et al. [53] adopt symmetric Gauss-Seidel ADMM to solve the dual
linear program  which reduces the computational cost in each iteration. S.Claici et al. [12] introduce a
stochastic alternating algorithm that can handle continuous input measures. However  these methods
are still computationally inefﬁcient when the number of support points of the input measures and the
number of input measures are large. Due to the nature of the ﬁrst-order methods  these algorithms
often converge too slowly to reach high-accuracy solutions.
The second  more mainstream  approach introduces an entropy regularization term to the linear
programming formulation[14  9]. This technique was ﬁrst developed in solving optimal transportation
problem. See [14  3  18  50  33  10  22] for the related works. M. Staib et al. [49] discuss the parallel
computation issue and introduce a sampling method. P.Dvurechenskii et al. [17] study decentralized
and distributed computation for the regularized problem. These methods are indeed suitable for
large-scale problems due to their low computational cost and parsimonious memory usage. However 
this advantage is obtained at the expense of the solution accuracy: especially when the regularization
term is weighted less in order to approximate the original problem more accurately  computational
efﬁciency degenerates and the outputs become unstable [9]. S. Amari et al. [5] propose a entropic
regularization based sharpening technique but their result is not the accurate real barycenter. P.C.
Alvarez-Esteban et al. [4] prove that the barycenter must be the ﬁxed-point of a new operator. See
[42] for a detailed survey of related algorithms.
In this paper  we develop a new interior-point method (IPM)  namely Matrix-based Adaptive Al-
ternating interior-point Method (MAAIPM)  to efﬁciently calculate the Wasserstein barycenters. If
the support is pre-speciﬁed  we apply one step of the Mizuno-Todd-Ye predictor-corrector IPM[36].
The algorithm gains a quadratic convergence rate showed by Y. Ye et al. [56]  which is a distinct
advantage of IPMs over ﬁrst-order methods. In practice  we implement Mehrotra’s predictor-corrector
IPM [35]  and add clever heuristics in choosing step lengths and centering parameters. If the support
is also to be optimized  MAAIPM alternatively updates support and linear program variables in
an adaptive strategy. At the beginning  MAAIPM updates support points X∗ by an unconstrained
quadratic program after a few number of IPM iterations. At the end  MAAIPM updates X∗ after
every IPM iteration and applies the "jump" tricks to escape local minima. Under the framework of
MAAIPM  we present two block matrix-based accelerated algorithms to quickly solve the Newton
equations at each iteration. Despite a prevailing belief that IPMs are inefﬁcient for large-scale cases 
we show that such an inefﬁciency can be overcome through careful manipulation of the block-data
structure of the normal equation. As a result  our stylized IPM has the following advantages.
Low theoretical complexity. The linear programming formulation of
i=1 mi + m variables andredN m +
i=1 mi + 1 constraints  where the integers N  m and mi will be
speciﬁed later. Although MAAIPM is still a second-order method 
in our two block matrix-based accelerated algorithms  every iteration
of solving the Newton direction has a time complexity of merely
i )  where a stan-

the Wasserstein barycenter has m(cid:80)N
(cid:80)N
O(m2(cid:80)N
i=1 mi + N m3) or O(m(cid:80)N
dard IPM would need O(cid:0)(N m +(cid:80)N

i=1 mi + m)(cid:1).

i +(cid:80)N

i=1 mi + 1)2(m(cid:80)N

i=1 m3

i=1 m2

For simplicity  let mi = m  i = 1  2 . . .   N  then the time complexity
of our algorithm in each iteration is O(N m3)  instead of standard IPM’s
complexity O(N 3m4). Note that theoretically  when N m2 = (N m)k for
some 1 < k < 2  the complexity of the standard IPM can be reduced to
O((N m)ω(k)) + O((N m)3) via fast matrix computation methods  where
the speciﬁc value of ω(k) can be found in table 3 of [20])
Practical effectiveness in speed and accuracy. Compared to regular-
ized methods  IPMs gain high-accuracy solutions and high convergence
rate by nature. Numerical experiments show that our algorithm converges
to highly accurate solutions of the original linear program with the least
number of iterations. Figure 1 shows the advantages of our methods in
accuracy in comparison to the well-developed Sinkhorn-type algorithm
[14  9].

a

Figure 1: A comparison
of algorithms for com-
puting the barycenters
between
Sinkhorn
based approach[9](left)
and MAAIPM(right).
Samples of handbag(ﬁrst
4 rows) are from Fashion-
MNIST dataset.

2

There are more advantages of our approaches in real implementation. When the support points of
measures are the same  there are several specially designed highly memory-efﬁcient and thus very
fast Sinkhorn based algorithms such as [46  9]. However  when the support points of measures are
different  the convolutional method in [46] is no longer applicable and the memory usage of our
method is within a constant multiple of the popular memory-efﬁcient ﬁrst-order Sinkhorn method 
IBP[9]  much less than the memory used by a commercial solver. In this case  experiments also show
that our algorithm can perform the best in both accuracy and overall runtime. Our algorithms also
inherits a natural structure potentially ﬁtting parallel computing scheme well. Those merits ensure
that our algorithm is highly suitable for large-scale computation of Wasserstein barycenters.
The rest of the paper is organized as follows. In section 2  we brieﬂy deﬁne the Wasserstein barycenter.
In section 3  we present its linear programming formulation and introduce the IPM framework. In
section 4  we present an IPM implementation that greatly reduces the computational cost of classical
IPMs. In section 5  we present our numerical results.

2 Background and Preliminaries

discrete probability measures [2  16]. Let Σn = {a ∈ Rn|(cid:80)n

In this section  we brieﬂy recall the Wasserstein distance and the Wasserstein barycenter for a set of
i=1 ai = 1  ai ≥ 0 for i = 1  2  . . . n}
be the probability simplex in Rn. For two vectors s(1) ∈ Σn1  s(2) ∈ Σn2  deﬁne the set of matrices
: Π1n2 = s(1)  Π(cid:62)1n1 = s(2)}. Let P = {(ai  qi) : i = 1  . . .   m}
M(s(1)  s(2)) = {Π ∈ Rn1×n2
denote the discrete probability measure supported on m points q1  . . .   qm in Rd with weights
a1  . . .   am respectively. The Wasserstein barycenter of the two measures U = {(ai  qi) : i =
1  . . .   m1} and V = {(bj  pj) : j = 1  . . .   m2} is

+

W2(U V) := min

πij(cid:107)qi − pj(cid:107)2 : Π = [πij] ∈ M(a  b)

(1)

where a = (a1  . . .   am1)(cid:62) and b = (b1  . . .   bm2)(cid:62). Consider a set of probability measures
{P (t)  t = 1 ···   N} where P (t) = {(a(t)
mt)(cid:62).
The Wasserstein barycenter (with m support points) P = {(wi  xi) : i = 1 ···   m} is another
probability measure which is deﬁned as a solution of the problem

1   . . .   a(t)

  q(t)

i

i ) : i = 1  . . .   mt}  and let a(t) = (a(t)
N(cid:88)
1m = a(t)  Π(t) ≥ 0 ∀t = 1 ···   N(cid:9). For a given set of

(W2(P P (t)))2.

× ··· × Rm×mN

(2)

t=1

+

:

minP

Furthermore  deﬁne the simplex S = (cid:8)(w  Π(1)  . . .   Π(N )) ∈ Rm
mw = 1  w ≥ 0; Π(t)1mt = w (cid:0)Π(t)(cid:1)(cid:62)
(cid:68)
D(t)(X)  Π(t)(cid:69)

1(cid:62)
support points X = {x1  . . .   xm}  deﬁne the distance matrices D(t)(X) = [(cid:107)xi−q(t)
j (cid:107)2
for t = 1  . . .   N. Then problem (2) is equivalent to

s.t. (w  Π(1)  . . .   Π(N )) ∈ S  x1  . . .   xm ∈ Rn.

+ × Rm×m1

N(cid:88)

min

+

2] ∈ Rm×mt

1
N

(3)

w X Π(t)

t=1

Problem (3) is a nonconvex problem  where one needs to ﬁnd the optimal support points X and the
optimal weight vector w of a barycenter simultaneously. However  in many real applications  the
support X of a barycenter can be speciﬁed empirically from the support points of {P (t)}N
t=1. Indeed 
in some cases  all measures in {P (t)}N
t=1 have the same set of support points and hence the barycenter
should also take the same set of support points. In view of this  we will also focus on the case when
the support X is given. Consequently  problem (3) reduces to the following problem:


(cid:118)(cid:117)(cid:117)(cid:116) m1(cid:88)

i=1

m2(cid:88)

j=1



(cid:68)

D(t)  Π(t)(cid:69)

N(cid:88)

t=1

min
w Π(t)

s.t. (w  Π(1)  . . .   Π(N )) ∈ S

(4)

where D(t) denotes D(X  Q(t)) for simplicity. In the following sections  we refer to problem (4) as
the Pre-speciﬁed Support Problem  and call problem (3) the Free Support Problem.

3

3 General Framework for MAAIPM

Linear programming formulation and preconditioning. Note that the Pre-speciﬁed Support
Problem is a linear program. In this subsection  we focus on removing redundant constraints. First 

we vectorize the constraints Π(t)1mt = w and(cid:0)Π(t)(cid:1)(cid:62)

1m = a(t) captured in S to become

(1(cid:62)

mt

⊗ Im)vec(Π(t)) = w  (Imt ⊗ 1(cid:62)

m)vec(Π(t)) = a(t)  t = 1 ···   N.

Thus  problem (4) can be formulated into the standard-form linear program:

min c(cid:62)x s.t. Ax = b  x ≥ 0

2

3

mN

0
1m

(cid:20)E(cid:62)

(cid:21)(cid:62)
i=1 mi  nrow := N m +(cid:80)N

(5)
with x = (vec(Π(1)); ...; vec(Π(N )); w)   b = (a(1); a(2); ...; a(N ); 0m; ...; 0m; 1)  c =

m  ...  ImN ⊗ 1(cid:62)

ncol := m(cid:80)N

1 E(cid:62)
E(cid:62)
0
m); E2 is a block diagonal matrix: E2 = diag(1(cid:62)

(vec(D(1)); ...; vec(D(N )); 0) and A =
trix: E1 = diag(Im1 ⊗ 1(cid:62)
Im  ...  1(cid:62)

⊗ Im); and E3 = −1N ⊗ Im. Let M :=(cid:80)N

  where E1 is a block diagonal ma-
⊗
i=1 mi + 1 and
i=1 mi + m. Then A ∈ Rnrow×ncol   b ∈ Rnrow and c ∈ Rncol. We are faced with a
standard form linear program with ncol variables and nrow constraints. In the spacial case where all
mi = m  the number of variables is O(N m)  and the number of constraints is O(N m2).
For efﬁcient implementations of IPMs for this linear program  we need to remove redundant con-
straints.
Lemma 3.1 Let ¯A ∈ R(nrow−N )×ncol be obtained from A by removing the (M +1)-th  (M +m+1)-
th  ···   (M + (N − 1)m + 1)-th rows of A  and ¯b ∈ Rnrow−N be obtained from b by removing the
(M + 1)-th  (M + m + 1)-th  ···   (M + (N − 1)m + 1)-th entries of b. Then 1) ¯A has full row
rank;

2) x satisﬁes Ax = b if and only if x satisﬁes ¯Ax = ¯b.

m1

The proof of this lemma is available in the supplement. With this lemma  the primal problem and
dual problem of problem 5 can be written as
¯Ax = ¯b  x ≥ 0.

¯A(cid:62)λ + s = c  s ≥ 0.

(Primal) min c(cid:62)x s.t.

(cid:62)
(Dual) max ¯b

p s.t.

(6)

Framework of Matrix-based Adaptive Alternating Interior-point Method (MAAIPM). When
the support points are not pre-speciﬁed  we need to solve problem (3). As we just saw  When X is
ﬁxed  the problem becomes a linear program. When (w {Π(t)}) are ﬁxed  the problem is a quadratic
optimization problem with respect to X  and the optimal X∗ can be written in closed form as

i =(cid:0)(cid:80)N

x∗

t=1

(cid:80)mt

(cid:1)−1(cid:80)N

(cid:80)mt

j=1 π(t)

ij

t=1

j=1 π(t)

ij q(t)
j  

i = 1  2 . . .   m.

(7)

In anther word  (3) can be reformulated as

min c(x)(cid:62)x s.t. ¯Ax = ¯b  x ≥ 0.

(8)
Since  as stated above  (3) is a non-convex problem and so it contains saddle points and local minima.
This makes ﬁnding a global optimizer difﬁcult. Examples of local minima and saddle points are
available in the supplement. The alternating minimization strategy used in [16  53  54] alternates
between optimizing X by solving (7) and optimizing (w {Π(t)}) by solving (4). However  this
alternating approach cannot avoid local minima or saddle points. Every iteration may require solving
a linear program (4)  which is expensive when the problem size is large.
To overcome the drawbacks  we propose Matrix-based Adaptive Alternating IPM (MAAIPM). If the
support is pre-speciﬁed  we solve a single linear program by predictor-corrector IPM[35  40  52]. If
the support should be optimized  MAAIPM uses an adaptive strategy. At the beginning  because the
primal variables are far from the optimal solution  MAAIPM updates X∗ of (7) after a few number of
IPM iterations for (w {Π(t)}). Then  MAAIPM updates X∗ after every IPM iteration and applies the
"jump" tricks to escape local minima. Although MAAIPM cannot ensure ﬁnding a globally optimal
solution  it can frequently get a better solution in shorter time. Since at the beginning MAAIPM
updates X∗ after many IPM iterations  primal dual predictor-corrector IPM is more efﬁcient. At the
end  X∗ is updated more often and each update of X∗ changes the linear programming objective

4

function so that dual variables may be infeasible. However  the primal variables always remain
feasible so that the primal IPM is more suitable at the end. Moreover  primal IPM is better for
applying "jump" tricks or other local-minima-escaping techniques  which has been shown in [55].
Details and illustration are available in the supplement.
In predictor-corrector IPM  the main computational cost lies in solving the Newton equations  which
can be reformulated as the normal equations

¯A(Dk)2 ¯A(cid:62)∆λk = f k 

(9)
) and f k is in Rnrow−N . This linear system of matrix ¯A(Dk)2 ¯A(cid:62)
where Dk denotes diag(x(k)
can be efﬁciently solved by the two methods proposed in the next section. In the primal IPM 
MAAIPM combines following the central path with optimizing the support points  i.e.  it contains
three parts in one iteration  taking an Newton step in the logarithmic barrier function

i /s(k)

i

¯Ax = b 

i=1 ln xi 

subject to

(10)
reducing the penalty µ  and updating the support (7). The Newton direction pk at the kth iteration is
calculated by

pk = xk + (X k)2(cid:16) ¯A(cid:62)(cid:0) ¯A(X k)2 ¯A(cid:62)(cid:1)−1(cid:0) ¯A(X k)2c − µ ¯AX k1(cid:1) − c

(11)
). The main cost of primal IPM lies in solving a linear system of ¯A(X k)2 ¯A(cid:62) 
where X k = diag(x(k)
which again can be efﬁciently solved by the two methods described in the following section. Further
more  we also apply the warm-start technique to smartly choose the starting point of the next IPM after
"jump" [45]. Compared with primal-dual IPMs’ warm-start strategies [29  28]  our technique saves
the searching time  and only requires slightly more memory. When we suitably set the termination
criterion  numerical studies show that MAAIPM outperforms previous algorithms in both speed and
accuracy  no matter whether the support is pre-speciﬁed or not.

(cid:17)

/µk 

i

minimize c(cid:62)x − µ(cid:80)n

4 Efﬁcient Methods for Solving the Normal Equations
In this section  we discuss efﬁcient methods for solving normal equations in the format ( ¯AD ¯A(cid:62))z =
f  where D is a diagonal matrix with all diagonal entries being positive. Let d = diag(D)  and
M2 = N (m − 1). First  through simple calculation  we have the following lemma on the structure of
matrix ¯AD ¯A(cid:62)  whose proof is available in the supplement.
Lemma 4.1 ¯AD ¯AT can be written in the following format:

 B1

¯AD ¯AT =

0
B(cid:62)
2 B3 + B4 α
c
0

B2
α(cid:62)



where B1 ∈ RM×M is a diagonal matrix with positive diagonal entries; B2 ∈ RM×M2 is a block-
diagonal matrix with N blocks (the size of the i-th block is (m−1)×mi); B3 ∈ RM2×M2 is a diagonal
matrix with positive diagonal entries; Let y = d(ncol−m+2 : ncol)  then B4 = (1N 1(cid:62)
N )⊗diag(y) 
and α = −1N ⊗ y; c = 1(cid:62)
Single low-rank regularization method (SLRM). Brieﬂy speaking  we will perform several basic
transformations on the matrix ¯AD ¯AT to transform it into an easy-to-solve format. Then we solve the
system with the transformed coefﬁcient matrix and ﬁnally transform the obtained solution back to get
an solution of ( ¯AD ¯A(cid:62))z = f.

md(ncol − m + 1 : ncol).



   V2 :=

(cid:34)IM

(cid:35)
IM2 −α/c

1

Deﬁne V1 :=
A2 := B4 − 1

IM−B(cid:62)
2 B−1
c αα(cid:62). Then 

1

IM2

1

B1

V2V1 ¯AD ¯AT V (cid:62)

1 V (cid:62)

2 =

B3 − B(cid:62)

2 B−1

1 B2 + B4 − 1

c αα(cid:62)

Deﬁne Y = diag(y) − 1

c yy(cid:62)  we have the following lemma.

5

  A1 := B3 − B(cid:62)

2 B−1

1 B2 and

 =

c

(cid:34)B1

A1 + A2

(cid:35)

.

c

Lemma 4.2
a) A1 is a block-diagonal matrix with N blocks. The size of each block is (m− 1)× (m− 1). Further
N )⊗ Y   and Y is positive
more  A1 is positive deﬁnite and strictly diagonal dominant. b) A2 = (1N 1(cid:62)
deﬁnite and strictly diagonal dominant.

2 B−1

1

1 z(4);

Output: z

c yy(cid:62));

2 z(3)   z = V (cid:62)

c z(2)(M + M2 + 1);

8 compute z(4) = V (cid:62)

z(3)(M + 1 : M + M2) = (A1 + A2)−1z(2)(M + 1 : M + M2);

Algorithm 1: Solver for the normal equation ( ¯AD ¯AT )z = f
Input: d = diag(D) ∈ Rncol; f ∈ RM +N (m−1)+1
1 compute B1  B2  B3  vector y = d(ncol − m + 2 : ncol) and c;
2 compute T = B(cid:62)
and matrices V1  V2;
3 compute A1 = B3 − T B2 and A2 = (1N 1(cid:62)
N ) ⊗ (diag(y) − 1
4 compute z(1) = V1f and z(2) = V2z(1);
5 compute z(3)(1 : M ) = B−1
1 z(2)(1 : M );
6 compute z(3)(M + M2 + 1) = 1
7 solve the linear system with coefﬁcient matrix A1 + A2 to get

Since the positive def-
initeness and diagonal
dominance claimed in
this lemma  the com-
putation of
the in-
verse matrices of each
block of A1 and A2
is numerically stable.
Now we introduce the
procedure for solving
( ¯AD ¯AT )z = f  as de-
scried in Algorithm 1
(z(1) - z(4) in the algo-
rithm are intermediate
variables).
In step 7 
we need to solve a lin-
ear system with coefﬁ-
cient matrix of dimension N (m−1)×N (m−1)  which is hard to compute with common methods for
dense symmetric matrices. In view of the low-rank structure of the matrix A2  we introduce a method 
namely Single Low-rank Regularization Method (SLRM)  which requires only O(N m3) ﬂops in
⊗ Im−1.
computation. Assume A1 = diag(A11  A22  ...  AN N ) and deﬁne U =
We can solve the linear system (A1 + A2)x = g by Algorithm 2.
The proof of correctness of Algorithm 2
and other analysis is available in the sup-
plement.

Algorithm 2: SLRM for the system (A1 +
A2)x = g
Input: A1  A2  g
1 compute A−1
11   ...  A−1
2 set A−1
1 = diag(A−1
3 compute x(1) = A−1
1 g;
4 compute x(2) = U T x(1);
5 compute x(3)(end − m + 2 : end) =

Double low-rank regularization method
(DLRM) when m is large.
In many ap-
plications  m is relatively large compared
to mt. For instance  in the area of im-
age identiﬁcation  the pixel support points
of the images at hand are sparse (small
mt) but different. To ﬁnd the "barycenter"
of these images  we need to assume the
"barycenter" image has much more pixel
support points (large m) than all the sam-
ple images. Sometimes  m might be about
5 to 20 times of each mt. In this case  the
computational cost of step 1 in SLRM is
heavy  since we need to solve N linear systems with dimension m × m. In this subsection  we use
the low rank regularization formula to further reduce the computational cost.
In view of lemma 4.1  assume

6 set x(3)(1 : end − m + 1) = 0 ;
7 compute x(4) = U x(3) and x(5) = A−1
8 compute x = x(1) − x(5);
Output: x

(cid:20)IN−1 1N−1

ii )\x(2)(end − m + 2 : end);

(Y −1 +(cid:80)N

ii   i = 1  ..  N;

i=1 A−1

(cid:21)

0

1

N N );

1 x(4);

B1 = diag(B11  ...  B1N )  B2 = diag(B21  ...  B2N )  B3 = diag(B31  ...  B3N ).

where B1i ∈ Rmi×mi  B2i ∈ Rmi×(m−1) and B3i ∈ R(m−1)×(m−1). Recall that A1 = B3 −
2 B−1
B(cid:62)
1i B2i. Since m >> mi 
we can use the following formula:

1 B2 and A1 = diag(A11  ...  AN N )  we have Aii = B3i − B(cid:62)
2i(B1i − B2iB−1
ii = (B3i − B(cid:62)
A−1

(12)
Instead of calculating and storing each Aii explicitly  we can just calculate and store each (B1i −
B2iB−1
2i)−1. When we need to calculate Aiiy for some vector y  we can use (12) and sequentially

1i B2i)−1 = B−1

2i)−1B2iB−1
3i .

3i + B−1

2iB−1

3i B(cid:62)

3i B(cid:62)

2iB−1

3i B(cid:62)

6

multiply each matrix with vectors. As a result  the ﬂops required in step 1 of SLRM reduce to
i=1mi)  which
O(mΣN
is at the same level (except for a constant) of a primal variable.

i )  and the total memory usage of whole MAAIPM is O(mΣN

i + ΣN

i=1m2

i=1m3

Complexity analysis. The following theorem summarizes the time and space complexity of the
aforementioned two methods.

Theorem 4.3 a) For SLRM  the time complexity in terms of ﬂops is O(m2(cid:80)N
memory usage in terms of doubles is O(m(cid:80)N
i +(cid:80)N
in terms of ﬂops is O(m(cid:80)N
O(m(cid:80)N

i=1 mi +N m3)  and the
i=1 mi + N m2); b) For the DLRM  the time complexity
i )  and the memory usage in terms of doubles is

i=1 mi +(cid:80)N

i ).
i=1 m2

i=1 m2

i=1 m3

We can choose between SLRM and DLRM for different cases to achieve lower time and space
complexity. Note that as N  m  mi grows up  the memory usage here is within an constant time of
the representative Sinkhorn type algorithms like IBP[9].

5 Experiments

We conduct three numerical experiments to investigate the real performance of our methods. The ﬁrst
experiment shows the advantages of SLRM and DLRM over traditional approaches in solving Newton
equations with a same structure as barycenter problems. The second experiment fully demonstrates
the merits of MAAIPM: high speed/accuracy and more efﬁcient memory usage. In the last experiment
with real benchmark data  MAAIPM recovers the images better than any other approach implemented.
In different experiments  we compare our methods with state-of-art commercial solvers(MATLAB 
Gurobi  MOSEK)  the iterative Bregman projection (IBP) by [9]  Bregman ADMM (BADMM)
[51  54]. The result also illustrates MAAIPM’s superiority over symmetric Gauss-Seidel ADMM
(sGS-ADMM) [53].
All experiments are run in Matlab R2018b on a workstation with two processors  Intel(R) Xeon(R)
Processor E5-2630@2.40Ghz (8 cores and 16 threads per processor) and 64GB of RAM  equipped
with 64-bit Windows 10 OS. Full experiment details are available in the supplement.
Experiments on solving the normal equations:
For ﬁgure 2  one can see that both SLRM and DLRM
clealy outperform the Matlab solver in in all cases.
For computation time  SLRM increases linearly with
respect to N and m(cid:48)  and DLRM increases linearly
with respect to N and m  which matches the con-
In practice  we select
clusions in Theorem 4.3.
t and DLRM when

SLRM when m2 ≤ 4(cid:80)N
m2 > 4(cid:80)N

t=1 m2

t .
t=1 m2

1   . . .   q(t)
1   . . .   a(t)

Experiments on barycenter problems:
In this ex-
periment  we set d = 3 for convenience. For P (t) 
each entry of (q(t)
m(cid:48)) is generated with i.i.d.
standard Gaussian distribution. The entries of the
weight vectors (a(t)
m(cid:48)) are simulated by uni-
form distribution on (0  1) and then are normalized.
Next we apply the k-means2 method to choose m
points to be the support points. Note that Gurobi and
MOSEK use a crossover strategy when close to the
exact solution to ensure obtaining a highly accurate
solution  we can regard Gurobi’s objective value Fgu
as the exact optimal value of the linear program (4).
Let "normalized obj" denote the normalized objective value deﬁned by |Fmethod − Fgu|/Fgu  where
Fmethod is the objective value respectively obtained by each method. Let "feasibility error" denote

Figure 2: Average computation time of 200
independent trials in solving the linear system.
Entries of diagonal D and f are generated by
uniform distribution in (0  1). In base situa-
tion  N = 50  m = 50  m(cid:48) = 25. Sub-ﬁgures
show the computation times when rescaling
N  m and m1 = ··· = mN = m(cid:48) by respec-
tively αN   αm and αm(cid:48) times.

2We call the Malab function "kmeans" in statistics and machine learning toolbox.

7

5101500.050.10.15runtime(seconds)51015010203040runtime(seconds)SLRMDLRMMatlab5101500.51runtime(seconds)510152002468runtime(seconds)(cid:110) (cid:107){Π(t)1mt−w}(cid:107)F

1+(cid:107)w(cid:107)F +(cid:107){Π(t)}(cid:107)F

 |1(cid:62)w − 1|(cid:111)

(cid:107){(Π(t))(cid:62)1m−a(t)}(cid:107)F
1+(cid:107){a(t)}(cid:107)F +(cid:107){Π(t)}(cid:107)F

 

  as a measure of the distance to the

max
feasible set.
From ﬁgure 3  we see that MAAIPM displays a
super-linear convergence rate for the objective 
which is consistent with the result of [56]. Note
that the feasibility error of MAAIPM increases
a little bit near the end but is still much lower
than BADMM and IBP. Although other methods
may have lower objective values in early stages 
their solutions are not acceptable due to high
feasibility errors.
Then we run numerical experiments to test the
computation time of methods in pre-speciﬁed
support points cases. For MAAIPM  we termi-
λk| +
nate it when (b
|c(cid:62)xk|) is less than 5 × 10−5. For sGS-ADMM  we compare with it indirectly by the bench-
mark claimed in their paper [53]: commercial solver Gurobi 8.1.0 [24] (academic license) with the
default parameter settings. We also compare with another commercial solver MOSEK 9.1.0(aca-
demic license). In our observation  MAAIPM can frequently perform better than other popular
commercial solvers. We use the default parameter setting(optimal for most cases) for Gurobi
and MOSEK so that they can exploit multiple processors (16 threads) while other methods are
(cid:1) 1
implemented with only one thread3. For BADMM  we follow the algorithm 4 in [54] to im-
plement and terminate when (cid:107)Π(k 1) − Π(k 2)(cid:107)F /(1 + (cid:107)Π(k 1)(cid:107)F + (cid:107)Π(k 2)(cid:107)F ) < 10−5. Set
2 . For IBP  we follow the remark 3 in [9] to implement the
k } − {u(n−1)
}(cid:107)F ) < 10−8 and
method  terminate it when (cid:107){u(n)
k }(cid:107)F + (cid:107){v(n−1)
}(cid:107)F ) < 10−8  and choose the regularization
(cid:107){v(n)
parameter  from {0.1  0.01  0.001} in our experiments. For BADMM and IBP  we implement the
Matlab codes4 by J.Ye et al. [54] and set the maximum iterate number respectively 4000 and 105.

Figure 3: Performance of methods in pre-speciﬁed
support cases. N = m = 50 and m1 = ··· =
mN = 50

(cid:107){At}(cid:107)F = (cid:0)(cid:80)N

t=1 (cid:107)At(cid:107)2
}(cid:107)F /(1 + (cid:107){v(n)

}(cid:107)F /(1 + (cid:107){u(n)

k }(cid:107)F + (cid:107){u(n−1)

k } − {v(n−1)

k

(cid:62)

λk − c(cid:62)xk)/(1 + |b

(cid:62)

F

k

k

k

Figure 4: The left 8 ﬁgures are the average computation time  normalized objective value and
feasibility error of Gurobi  MOSEK  MAAIPM  BADMM and IBP( = 0.1  0.01  0.001) in pre-
speciﬁed support cases from 30 independent trials. In the ﬁrst row  m = 100  mt follows an uniform
distribution on (75  125). In the second row  N = 50  m = 100 and m1 = ··· = mN = m(cid:48). The
right ﬁgure is the average computation time of Gurobi and MAAIPM in pre-speciﬁed support cases
from 10 independent trials. mt follows a uniform distribution on (150  250)  and m = 200.
From the left 8 sub-ﬁgures in ﬁgure 4 one can observe that MAAIPM returns a considerably accurate
solution in the second shortest computation time. For IBP  although it returns an objective value in
the shortest time when  = 0.1  the quality of the solution is almost the worst. Because IBP only
solves an approximate problem  if  is set smaller  the computation time sharply increases but the

3We call the Matlab function "maxNumCompThreads(1)"
4Available in https://github.com/bobye/WBC_Matlab

8

00.050.10.150.20.250.30.3510-410-210010200.050.10.150.20.250.30.3510-610-510-410-310-210-11002040608010-11001011021031042040608005101520252040608010-510-310-11011032040608010-510-410-310-210-110010020030040050060010-110010110210310410510020030040050060002040608010010020030040050060010-510-310-110110310020030040050060010-510-410-310-210-110050010001500200025003000020004000600080001000012000140001600018000Figure 5: computation time and normalized objective
value of MAAIPM  BADMM and IBP in the free sup-
port cases from 30 independent trials. "Normalized
obj" denote Fmethod/FM AAIP M − 1  where Fmethod
is the objective value obtained by each method. N
takes different values and m = m(cid:48) = 50.

quality of the solution is still not ensured. For BADMM  it gives a solution close to the exact one  but
requires much more computation time.
For Gurobi and MOSEK  although they can
exploit 16 threads  the computation time
is far more than that of MAAIPM That is
to say  MAAIPM also largely outperforms
sGS-ADMM in speed  according to table 1 
2  3 in [53]. Moreover  because the num-
ber of iterations remains almost indepen-
dent of the problem size  the main compu-
tational cost of MAAIPM is approximately
linear with respect to N and m(cid:48). In fact 
when N = 5000  MAAIPM requires only
3098.23 seconds  while MOSEK uses over
20000 seconds. Although the memory usage
of MAAIPM is within a constant multiple
of that of IBP  the former one is ususaly
larger than the latter one. But the right sub-
ﬁgure in Figure 4 and the case of N = 5000
demonstrate that MAAIPM’s memory usage
is managed more efﬁcient compared to Gurobi and MOSEK. These positive traits are consistent with
the time and memory complexity proved in Theorem 4.3.
Next  we conduct numerical studies to test MAAIPM in free support
cases  i.e.  problem (3). Same as [54]  we implement the version
of BADMM and IBP that can automatically update support points
and set the initial support points in multivariate normal distribution.
We set the maximum number of iterations in BADMM and IBP as
104 and 106. The entries of (q(t)
m(cid:48)) are generated with i.i.d.
uniform distribution in (0  100) and the initial support points fol-
lows a Gaussian distribution. In ﬁgure 6  "Normalized obj" denotes
Fmethod/FM AAIP M − 1  where Fmethod is the objective value ob-
tained by each iteration of methods. From ﬁgure 5 and 6  one can
see that  in the free support cases  MAAIPM can still obtain the
smallest objective value in the second shortest time. That is because
MAAIPM updates support more frequently and adopts "jump" tricks
to avoid the local minima. Although IBP can obtain an approximate
value in the shortest time when  = 0.1  the quality of the barycenter
is too low to be useful.

Figure 6: Performance of
methods in free support cases.
N = 40  m = m1 = m2 =
··· = mN = 50.

1   . . .   q(t)

MNIST

50

time(seconds)

250

Table 1: Experiments on datasets

Experiments on real applica-
tions: We conduct similar ex-
periments to [16  53] on the
MNIST4 and Fashion-MNIST4
datasets.
In MNIST  We ran-
domly select 200 images for digit
8 and resize each image to 0.5 
1  2 times of its original size
28 × 28.
In Fashion-MNIST 
we randomly select 20 images of
handbag  and resize each image
to 0.5  1 time of the original size.
The support points of images are
dense and different. Next  for
each case  we apply MAAIPM  BADMM and IBP( = 0.01) to compute the Wasserstein barycenter
in respectively free support cases and pre-speciﬁed support cases. From table 1  one can see that 
MAAIPM obtained the clearest and sharpest barycenters within the least computation time.

Fashion-MNIST
25
75

IBP( = 0.01)

500

1000

MAAIPM

BADMM

4Available in http://yann.lecun.com/exdb/mnist/ and https://github.com/zalandoresearch/fashion-mnist

9

5010015020001002003004005006007005010015020010-410-21001020510152010-410-2100102Acknowledgments

We thank Tianyi Lin  Simai He  Bo Jiang  Qi Deng and Yibo Zeng for helpful discussions and fruitful
suggestions.

References
[1] S.S. Abadeh  V.A. Nguyen  D. Kuhn  and P.M.M Esfahani. Wasserstein distributionally robust kalman

ﬁltering. In Advances in Neural Information Processing Systems 31  pages 8483–8492  2018.

[2] M. Agueh and G. Carlier. Barycenters in the Wasserstein space. SIAM Journal on Mathematical Analysis 

43(2): 904–924  2011.

[3] J. Altschuler  J. Weed  and P. Rigollet. Near-linear time approximation algorithms for optimal transport via

Sinkhorn iteration. In Advances in Neural Information Processing Systems 30  pages 1964–1974  2017.

[4] P.C. Alvarez-Esteban  E. Barrio  J. Cuesta-Albertos  and C. Matran. A ﬁxed-point approach to barycenters

in Wasserstein space. Journal of Mathematical Analysis and Applications  441(2): 744–762  2016.

[5] S. Amari  R. Karakida  M.Oizumi and M. Cuturi Information geometry for regularized optimal transport

and barycenters of patterns. Neural computation  31(5): 827-848  2019.

[6] E. Anderes  S. Borgwardt  and J. Miller. Discrete Wasserstein barycenters: Optimal transport for discrete
data. Math Meth Oper Res 84(2):389–409 October2016. ISSN 1432-2994  1432-5217. doi: 10.1007/s00186-
016-0549-x.

[7] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein generative adversarial networks. In International

Conference on Machine Learning  pages 214–223  2017.

[8] D. Bertsimas and J.N. Tsitsiklis. Introduction to Linear Optimization. Athena Scientiﬁc  1997.

[9] J.D. Benamou  G. Carlier  M. Cuturi  L. Nenna  and G. Peyr´e. Iterative Bregman projections for regularized

transportation problems. SIAM Journal on Scientiﬁc Computing  37(2):A1111–A1138  2015.

[10] J. Blanchet  A. Jambulapati  C. Kent and A. Sidford. Towards optimal running times for optimal transport.

arXiv:1810.07717.

[11] G. Carlier  A. Oberman  and E. Oudet. Numerical methods for matching for teams and Wasserstein

barycenters. ESAIM: Mathematical Modelling and Numerical Analysis  49(6):1621–1642  2015.

[12] S. Claici  E. Chien  J. Solomon. Stochastic Wasserstein Barycenters. arXiv:1802.05757.

[13] N. Courty  R. Flamary  A. Habrard  and A. Rakotomamonjy. Joint distribution optimal transportation for

domain adaptation. In Advances in Neural Information Processing Systems 30  pages 3730–3739. 2017.

[14] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural

Information Processing Systems 26  pages 2292–2300  2013.

[15] A. Dessein  N. Papadakis  and C.A. Deledalle. Parameter estimation in ﬁnite mixture models by regularized

optimal transport: a uniﬁed framework for hard and soft clustering. arXiv:1711.04366  2017.

[16] M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In the 31st International Conference

on Machine Learning  pages 685–693  2014.

[17] P. Dvurechenskii  D. Dvinskikh  A. Gasnikov  C. Uribe  and A. Nedich. Decentralize and randomize:
Faster algorithm for Wasserstein barycenters. In Advances in Neural Information Processing Systems 30 
pages 10783–10793  2018.

[18] P. Dvurechensky  A. Gasnikov and A. Kroshnin. Computational Optimal Transport: Complexity by
Accelerated Gradient Descent Is Better Than by Sinkhorn’s Algorithm. Proceedings of the 35th International
Conference on Machine Learning  80:1367-1376  2018.

[19] C. Frogner  C. Zhang  H. Mobahi  M. Araya  and T.A. Poggio. Learning with a Wasserstein loss. In

Advances in Neural Information Processing Systems 28  pages 2053–2061  2015.

[20] F. L. Gall and F. Urrutia. Improved Rectangular Matrix Multiplication using Powers of the Coppersmith-

Winograd Tensor. arXiv:1708.05622. 2017

10

[21] R. Gao  L. Xie  Y. Xie  and H. Xu. Robust hypothesis testing using wasserstein uncertainty sets. In

Advances in Neural Information Processing Systems 31  pages 7913–7923. 2018.

[22] A. Genevay  M. Cuturi  G. Peyr´e  and F. Bach. Stochastic optimization for large-scale optimal transport. In

Advances in Neural Information Processing Systems 29  pages 3440–3448  2016.

[23] I. Gulrajani  F. Ahmed  M. Arjovsky  V. Dumoulin  and A.C. Courville. Improved training of Wasserstein

GANs. In Advances in Neural Information Processing Systems 30  pages 5767–5777  2017.

[24] Inc. Gurobi Optimization. Gurobi Optimizer Reference Manual  2018.

[25] N. Ho  V. Huynh  D. Phung  and M.I. Jordan. Probabilistic multilevel clustering via composite transporta-

tion distance. arXiv:1810.11911  2018.

[26] N. Ho  X. Nguyen  M. Yurochkin  H.H. Bui  V. Huynh  and D. Phung. Multilevel clustering via wasserstein

means. In International Conference on Machine Learning  pages 1501–1509  2017.

[27] G. Huang  C. Guo  M.J. Kusner  Y. Sun  F. Sha  and K.Q. Weinberger. Supervised word mover’s distance.

In Advances in Neural Information Processing Systems 29  pages 4862– 4870  2016.

[28] E. John  E. A. Yıldırım. Implementation of warm-start strategies in interior-point methods for linear

programming in ﬁxed dimension. Computational Optimization and Applications  41(2): 151-183  2008.

[29] E. A. Yıldırım  S. J. Wright Warm-start strategies in interior-point methods for linear programming. SIAM

Journal on Optimization  12(3): 782-810  2002.

[30] M.J. Kusner  Y. Sun  N.I. Kolkin  and K.Q. Weinberger. From word embeddings to document distances. In

the 32nd In International Conference on Machine Learning  pages 957–966  2015.

[31] T. Lacombe  M. Cuturi  and S. Oudot. Large scale computation of means and clusters for persistence dia-
grams using optimal transport. In Advances in Neural Information Processing Systems 31  pages 9792–9802 
2018.

[32] J. Lee and M. Raginsky. Minimax statistical learning with wasserstein distances. In Advances in Neural

Information Processing Systems 31  pages 2692–2701. 2018.

[33] T. Lin  N. Ho and M.I. Jordan. On Efﬁcient Optimal Transport: An Analysis of Greedy and Accelerated

Mirror Descent Algorithms. arXiv:1901.06482.

[34] A. Mallasto and A. Feragen. Learning from uncertain curves: The 2-wasserstein metric for gaussian

processes. In Advances in Neural Information Processing Systems 30  pages 5660–5670. 2017.

[35] S. Mehrotra. On the Implementation of a Primal-Dual interior-point Method. SIAM J. Optim.  2(4) 

575–601. 1992.

[36] S. Mizuno  M. J. Todd and Y. Ye. On adaptive-step primal-dual interior-point algorithms for linear

programming. Mathematics of Operations research  18(4): 964-981  1993.

[37] B. Muzellec and M. Cuturi. Generalizing point embeddings using the wasserstein space of elliptical
distributions. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa- Bianchi  and R. Garnett 
editors  Advances in Neural Information Processing Systems 31  pages 10258–10269. 2018.

[38] X. Nguyen. Convergence of latent mixing measures in ﬁnite and inﬁnite mixture models. Annals of

Statistics  4(1):370–400  2013.

[39] X. Nguyen. Borrowing strength in hierarchical Bayes: posterior concentration of the Dirichlet base

measure. Bernoulli  22(3):1535–1571  2016.

[40] J. Nocedal and S.J. Wright Numerical Optimization  2006.

[41] G. Peyr´e  M. Cuturi  and J. Solomon. Gromov-Wasserstein averaging of kernel and distance matrices. In

International Conference on Machine Learning  pages 2664–2672  2016.

[42] G. Peyr´e and M. Cuturi. Computational Optimal Transport. arXiv:1803.00567.

[43] J. Rabin  G. Peyr´e  J. Delon  and M. Bernot. Wasserstein barycenter and its application to texture mixing.
In Scale Space and Variational Methods in Computer Vision  volume 6667 of Lecture Notes in Computer
Science  pages 435–446. Springer  2012.

11

[44] A. Rolet  M. Cuturi  and G. Peyr´e. Fast dictionary learning with a smoothed Wasserstein loss. In

International Conference on Artiﬁcial Intelligence and Statistics  pages 630–638  2016.

[45] A. Skajaa  E. D. Andersen  Y. Ye Warmstarting the homogeneous and self-dual interior-point method for

linear and conic quadratic problems. Mathematical Programming Computation  5(1): 1-25  2013.

[46] J. Solomon  G. D. Goes  G. Peyré  M. Cuturi  A. Butscher  A. Nguyen  D. Tao  L. Guibas  Convolu-
tional wasserstein distances: Efﬁcient optimal transportation on geometric domains. ACM Transactions on
Graphics (TOG)  34(4)  66  2015.

[47] J. Solomon  R.M. Rustamov  L. Guibas  and A. Butscher. Wasserstein propagation for semi-supervised

learning. In the 31st International Conference on Machine Learning  pages 306–314  2014.

[48] S. Srivastava  C. Li  and D. Dunson. Scalable Bayes via barycenter in Wasserstein space. Journal of

Machine Learning Research  19(8):1–35  2018.

[49] M. Staib  S. Claici  J.M. Solomon  and S. Jegelka. Parallel streaming Wasserstein barycenters. In Advances

in Neural Information Processing Systems 30  pages 2647–2658  2017a.

[50] C. Villani. Optimal Transport: Old and New  volume 338. Springer Science & Business Media  2008.

[51] H. Wang and A. Banerjee. Bregman Alternating Direction Method of Multipliers. In Advances in Neural

Information Processing Systems 27  2014  pp. 2816-2824.

[52] S.J. Wright. Primal-dual interior-point methods  1997.

[53] L. Yang  J. Li  D. Sun and K.C. Toh. A Fast Globally Linearly Convergent Algorithm for the Computation

of Wasserstein Barycenters  arXiv:1809.04249  2018.

[54] J. Ye  P. Wu  J.Z. Wang  and J. Li. Fast discrete distribution clustering using Wasserstein barycenter with

sparse support. IEEE Transactions on Signal Processing  65(9):2317–2332  2017.

[55] Y Ye. On afﬁne scaling algorithms for nonconvex quadratic programming. Mathematical Programming 

nL)-iteration algorithm for

56(1-3): 285-300  1992

√
[56] Y. Ye  O. Güler  R. A. Tapia  and Y. Zhang. A quadratically convergent O (

linear programming. Mathematical Programming   59(1):151-162 2014.

12

,Robert Geirhos
Carlos R. M. Temme
Jonas Rauber
Heiko H. Schütt
Matthias Bethge
Felix A. Wichmann
DongDong Ge
Haoyue Wang
Zikai Xiong
Yinyu Ye