2008,A general framework for investigating how far the decoding process in the brain can be simplified,``How is information decoded in the brain?'' is one of the most difficult and important questions in neuroscience. Whether neural correlation is important or not in decoding neural activities is of special interest. We have developed a general framework for investigating how far the decoding process in the brain can be simplified. First  we hierarchically construct simplified probabilistic models of neural responses that ignore more than $K$th-order correlations by using a maximum entropy principle. Then  we compute how much information is lost when information is decoded using the simplified models  i.e.  ``mismatched decoders''. We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. We applied our proposed framework to spike data for vertebrate retina. We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies  pseudo correlations seem to carry a large portion of the information.,A general framework for investigating how far the

decoding process in the brain can be simpliﬁed

Masafumi Oizumi1  Toshiyuki Ishii2  Kazuya Ishibashi1

Toshihiko Hosoya2  Masato Okada1 2
oizumi@mns.k.u-tokyo.ac.jp

tishii@brain.riken.jp kazuya@mns.k.u-tokyo.ac.jp

hosoya@brain.riken.jp  okada@k.u-tokyo.ac.jp

1 University of Tokyo  Kashiwa-shi  Chiba  JAPAN

2 RIKEN Brain Science Institute  Wako-shi  Saitama  JAPAN

Abstract

“How is information decoded in the brain?” is one of the most difﬁcult and im-
portant questions in neuroscience. Whether neural correlation is important or not
in decoding neural activities is of special interest. We have developed a general
framework for investigating how far the decoding process in the brain can be sim-
pliﬁed. First  we hierarchically construct simpliﬁed probabilistic models of neu-
ral responses that ignore more than Kth-order correlations by using a maximum
entropy principle. Then  we compute how much information is lost when infor-
mation is decoded using the simpliﬁed models  i.e.  “mismatched decoders”. We
introduce an information theoretically correct quantity for evaluating the informa-
tion obtained by mismatched decoders. We applied our proposed framework to
spike data for vertebrate retina. We used 100-ms natural movies as stimuli and
computed the information contained in neural activities about these movies. We
found that the information loss is negligibly small in population activities of gan-
glion cells even if all orders of correlation are ignored in decoding. We also found
that if we assume stationarity for long durations in the information analysis of dy-
namically changing stimuli like natural movies  pseudo correlations seem to carry
a large portion of the information.

1 Introduction

An ultimate goal of neuroscience is to elucidate how information is encoded and decoded by neural
activities. To investigate what information is encoded by neurons in certain area of the brain  the
mutual information between stimuli and neural responses is often calculated.
In the analysis of
mutual information  it is implicitly assumed that encoded information is decoded by an optimal
decoder  which exactly matches the encoder.
In other words  the brain is assumed to have full
knowledge of the encoding process. Generally  if the neural activities are correlated  the amount of
data needed for the optimal decoding scales exponentially with the number of neurons. Since a large
amount of data and many complex computations are needed for optimal decoding  the assumption
of an optimal decoder in the brain is doubtful.

The reason mutual information is widely used in neuroscience despite the doubtfulness of the opti-
mal decoder is that we are completely ignorant of how information is decoded in the brain. Thus 
we simply evaluate the maximal amount of information that can be extracted from neural activities
by calculating the mutual information. To address this lack of knowledge  we can ask a different
question: “How much information can be obtained by a decoder that has partial knowledge of the
encoding process?” [10  14] We call this type of a decoder “simpliﬁed decoder” or a “mismatched
decoder”. For example  an independent decoder is a simpliﬁed decoder; it takes only the marginal

1

distribution of the neural responses into consideration and ignores the correlations between neuronal
activities. The independent decoder is of particular importance because several studies have shown
that maximum likelihood estimation can be implemented by a biologically plausible network [2  4].
If it is experimentally shown that a sufﬁciently large portion of information is obtained by the in-
dependent decoder  we can say that the brain may function in a manner similar to the independent
decoder. In this context  Nirenberg et al. computed the amount of information obtained by the in-
dependent decoder in pairs of retinal ganglion cells activities [10]. They showed that no pair of
cells showed a loss of information greater than 11%. Because only pairs of cells were considered
in their analysis  it has not been still elucidated whether correlations are not important in population
activities.

To elucidate whether correlations are important or not in population activities  we have developed
a general framework for investigating the importance of correlation in decoding neural activities.
When population activities are analyzed  we have to deal with not only second-order correlations
but also higher-order correlations in general. Therefore  we need to hierarchically construct simpli-
ﬁed decoders that account of up to Kth-order correlations  where K = 1  2  ...  N. By computing
how much information is obtained by the simpliﬁed decoders  we investigate how many orders of
correlation should be taken into account to extract enough information. To compute the information
obtained by the mismatched decoders  we introduce a information theoretically correct quantity de-
rived by Merhav et al. [8]. Information for mismatched decoders previously proposed by Nirenberg
and Latham is the lower bound on the correct information [5  11]. Because this lower bound can be
very loose and their proposed information can be negative when many cells are analyzed as is shown
in the paper  we need to accurately evaluate the information obtained by mismatched decoders.

The plan of the paper is as follows. In Section 2  we describe a way of computing the information
that can be extracted from neural activities by mismatched decoders using the information derived
by Merhav et al.. Using analytical computation  we demonstrate how information for mismatched
decoders previously proposed by Nirenberg and Latham differs from the correct information derived
by Merhav et al.  especially when many cells are analyzed. In Section 3  we apply our framework to
spike data for ganglion cells in the salamander retina. We ﬁrst describe the method of hierarchically
constructing simpliﬁed decoders by using the maximum entropy principle [12]. We then compute the
information obtained with the simpliﬁed decoders. We ﬁnd that more than 90% of the information
can be extracted from the population activities of ganglion cells even if all orders of correlations
are ignored in decoding. We also describe the problem of previous studies [10  12] in which the
stationarity of stimuli is assumed for a duration that is too long. Using a toy model  we demonstrate
that pseudo correlations seem to carry a large portion of the information because of the stationarity
assumption.

2 Information for mismatched decoders

Let us consider how much information about stimuli can be extracted from neural responses. We
assume that we experimentally obtain the conditional probability distribution p(r|s) that neural re-
sponses r are evoked by stimulus s. We can say that the stimulus is encoded by neural response r 
which obeys the distribution p(r|s). We call p(r|s) the “encoding model”. The maximal amount of
information obtained with the optimal decoder can be evaluated by using the mutual information:

I = −Z drp(r) log2 p(r) +Z drXs

p(s)p(r|s) log2 p(r|s) 

(1)

where p(r) =Ps p(r|s)p(s) and p(s) is the prior probability of stimuli. In the optimal decoder  the

probability distribution q(r|s) that exactly matches the encoding model p(r|s) is used for decoding;
that is  q(r|s) = p(r|s). We call q(r|s) the “decoding model”. We can also compute the maximal
amount of information obtained by a decoder using a decoding model q(r|s) that does not match the
encoding model p(r|s) by using an equation derived by Merhav et al. [8]:

I ∗(β) = −Z drp(r) log2Xs

p(s)q(r|s)β +Z drXs

p(s)p(r|s) log2 q(r|s)β 

(2)

where β takes the value that maximizes I ∗(β). Thus  β is the value that satisﬁes ∂I ∗/∂β = 0. We
call a decoder using the mismatched decoding model a “mismatched decoder”.

2

(cid:14976)(cid:15000)

(cid:15040)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:15040)

(cid:19)

(cid:18)(cid:16)(cid:23)

(cid:18)

(cid:770)(cid:18)(cid:16)(cid:23)

(cid:770)(cid:19)

(cid:20)

(cid:15001)

(cid:15008)(cid:14977)(cid:14982)(cid:15008)
(cid:15008)(cid:15013)(cid:15011)(cid:14982)(cid:15008)

(cid:19)(cid:23)(cid:18)

(cid:15040)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:15040)

(cid:19)

(cid:18)(cid:16)(cid:27)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:25)

(cid:18)(cid:16)(cid:24)

(cid:18)(cid:16)(cid:23)
(cid:20)

(cid:21)

(cid:15008)(cid:14977)
(cid:14984)(cid:14982)(cid:15008)
(cid:15008)(cid:15013)(cid:15011)(cid:14982)(cid:15008)

(cid:25)

(cid:22)

(cid:24)
(cid:15013)(cid:15052)(cid:15044)(cid:15033)(cid:15036)(cid:15049)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15034)(cid:15036)(cid:15043)(cid:15043)(cid:15050)

(cid:23)

(cid:23)(cid:18)

(cid:19)(cid:18)(cid:18)

(cid:15013)(cid:15052)(cid:15044)(cid:15033)(cid:15036)(cid:15049)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15034)(cid:15036)(cid:15043)(cid:15043)(cid:15050)

Figure 1: Comparison between correct information I ∗ derived by Merhav et al. and Nirenberg-
Latham information I N L. A: Difference between I ∗/I (solid line) and I N L/I (dotted line) in
Gaussian model where correlations and derivatives of mean ﬁring rates are uniform. Correlation
1 /I (dotted line) when spike
parameter c = 0.01. B: Difference between I ∗
data in Figure 3A are used. For this spike data and other spike data analyzed  Nirenberg-Latham
information provides a tight lower bound on the correct information  possibly because the number
of cells is small.

1 /I (solid line) and I N L

Previously  Nirenberg and Latham proposed that the information obtained by mismatched decoders
can be evaluated by using [11]

I N L = −Z drp(r) log2Xs

p(s)q(r|s) +Z drXs

p(s)p(r|s) log2 q(r|s).

(3)

We call their proposed information “Nirenberg-Latham information”. If we set β = 1 in Eq. 2 
we obtain Nirenberg-Latham information  I ∗(1) = I N L. Thus  Nirenberg-Latham information
does not give correct information; instead  it simply provides the lower bound on the correct infor-
mation  I ∗(β)  which is the maximum value with respect to β [5  8]. The lower bound provided
by Nirenberg-Latham information can be very loose and the Nirenberg-Latham information can be
negative when many cells are analyzed.

Theoretical evaluation of information I  I ∗  and I N L

We consider the problem where mutual information is computed when stimulus s  which is a single
variable  and slightly different stimulus s + ∆s are presented. We assume the prior probability of
stimuli  p(s) and p(s + ∆s)  are equal: p(s) = p(s + ∆s) = 1/2. Neural responses evoked by the
stimuli are denoted by r  which is considered here to be the neuron ﬁring rate. When the difference
between two stimuli is small  the conditional probability p(r|s + ∆s) can be expanded with respect
to ∆s as p(r|s+∆s) = p(r|s)+p′(r|s)∆s+ 1
2 p′′(r|s)(∆s)2 +...  where ′ represents differentiation
with respect to s. Using the expansion  to leading order of ∆s  we can write mutual information I
as

I =

∆s2

8 Z dr

(p′(r|s))2

p(r|s)

 

(4)

whereR dr p′(r|s)2

is the Fisher information. Thus  we can see that the mutual information is pro-
portional to the Fisher information when ∆s is small. Similarly  the correct information I ∗ for the
mismatched decoders and the Nirenberg-Latham information I N L can be written as

p(r|s)

I ∗ =

I N L =

p(r|s)(q′(r|s))2

q(r|s)2

 

¶−1
q(r|s) ! .

p′(r|s)q(r|s)

∆s2

∆s2

q(r|s)

p′(r|s)q′(r|s)

¶2µZ dr
8 µZ dr
8 Ã−Z drp(r|s)µ q′(r|s)
q(r|s)¶2
´2³R dr p(r|s)(q′(r|s))2

q(r|s)2

+ 2Z dr
´−1

Taking into consideration the proportionality of the mutual information to the Fisher information  we
in Eq. 5 is a Fisher information-like

can interpret that³R dr p′(r|s)q′(r|s)

quantity for mismatched decoders.

q(r|s)

(5)

(6)

3

Let us consider the case in which the encoding model p(r|s) obeys the Gaussian distribution

p(r|s) =

1
Z

expµ−

1
2

(r − f (s))T C−1(r − f (s))¶  

(7)

where T stands for the transpose operation  f (s) is the mean ﬁring rates given stimulus s  and C is
the covariance matrix. We consider an independent decoding model q(r|s) that ignores correlations:

q(r|s) =

1
ZD

expµ−

1
2

(r − f (s))T C−1

D (r − f (s))¶  

where CD is the diagonal covariance matrix obtained by setting the off-diagonal elements of C to
0. If the Gaussian integral is performed for Eqs. 4-5  I  I ∗  and I N L can be written as

(8)

(9)

(10)

(11)

I =

I ∗ =

∆s2

8

∆s2

8

f ′T (s)C−1f ′(s) 

(f ′T (s)C−1
D
CC−1
D

f ′T (s)C−1
D

f ′(s))2

f ′(s)

 

∆s2

I N L =

8 ¡−f ′T (s)C−1

D

CC−1
D

f ′(s) + 2f ′T (s)C−1
D

f ′(s)¢ .

The correct information obtained by the independent decoder for the Gaussian model (Eq. 10) is
inversely proportional to the decoding error of s when the independent decoder is applied  which
was computed from the generalized Cram´er Rao bound by Wu et al. [14].
As a simple example  we consider a uniform correlation model [1  14] in which covariance matrix C
is given by Cij = σ2[δij + c(1 − δij)] and assume that the derivatives of the ﬁring rates are uniform:
that is f ′

i = f ′. In this case  I  I ∗  and I N L can be computed using

I =

I ∗ =

I N L =

∆s2

N f ′2

8

σ2(N c + 1 − c)

 

∆s2

8

∆s2

8

N f ′2

 

σ2(N c + 1 − c)
(−c(N − 1) + 1)N f ′2

σ2

(12)

(13)

(14)

 

where N is the number of cells. We can see that I ∗ is equal to I  which means that information is
not lost even if correlation is ignored in the decoding process. Figure 1A shows I N L/I and I ∗/I
when the degree of correlation c is 0.01. As shown in Figure 1A  the difference between the correct
information I ∗ and Nirenberg-Latham information I N L is very large when the number of cells N is
large. When N > c+1
c   I N L is negative. Analysis showed that using Nirenberg-Latham information
I N L as a lower bound on the correct information I ∗ can lead to wrong conclusions  especially when
many cells are analyzed.

3 Analysis of information in population activities of ganglion cells

3.1 Methods

We analyzed the data obtained when N = 7 retinal ganglion cells were simultaneously recorded
using a multielectrode array. The stimulus was a natural movie  which was 200 s long and repeated
45 times. We divided the movie into many short natural movies and considered them as stimuli over
which information contained in neural activities is computed. For instance  when it was divided into
10-s-long natural movies  there were 20 stimuli. Figure 2A shows the response of the seven retinal
ganglion cells to natural movies from 0 to 10 s in length. To apply information theoretic techniques 
we ﬁrst discretized the time into small time bins ∆τ and indicated whether a spike was emitted or
not in each time bin with a binary variable: σi = 1 means that the cell i spiked and σi = 0 means that
it did not spike. We set the length of the time  ∆τ   to 5 ms so that it was short enough to avoid two
spikes falling into the same bin. In this way  the spike pattern of ganglion cells was transformed into
an N-letter binary word  σ = {σ1  σ2  ...  σN }  as shown in Figure 2B. Then  we determined the

4

(cid:15000)

(cid:15049)
(cid:15036)
(cid:15033)
(cid:15044)
(cid:15052)
(cid:15045)
(cid:14967)
(cid:15043)
(cid:15043)

(cid:15036)
(cid:15002)

(cid:25)

(cid:24)

(cid:23)

(cid:22)

(cid:21)

(cid:20)

(cid:19)
(cid:18)

(cid:15001)

(cid:15049)
(cid:15036)
(cid:15033)
(cid:15044)
(cid:15052)
(cid:15045)
(cid:14967)
(cid:15043)
(cid:15043)

(cid:15036)
(cid:15002)

(cid:25)

(cid:24)

(cid:23)

(cid:22)

(cid:21)

(cid:20)

(cid:19)
(cid:18)(cid:16)(cid:27)

(cid:18)(cid:16)(cid:27)(cid:23)

(cid:15019)(cid:15040)(cid:15044)(cid:15036)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:19)

(cid:23)

(cid:15019)(cid:15040)(cid:15044)(cid:15036)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:19)(cid:18)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14984)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14984)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14984)
(cid:14983)
(cid:14984)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)
(cid:14984)
(cid:14983)

(cid:14983)
(cid:14983)
(cid:14984)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14984)

(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)
(cid:14983)

Figure 2: A: Raster plot of seven retinal ganglion cells responding to a natural movie. B: Transfor-
mation of spike trains into binary words.

frequency with which a particular spike pattern  σ  was observed during each stimulus and estimated
the conditional probability distribution pdata(σ|s) from experimental data. Using these conditional
probabilities  we evaluated the information contained in N-letter binary words σ.
Generally  the joint probability of N binary variables can be written as [9]

pN (σ) =

1
Z

exp
Xi

θiσi +Xi<j

θijσiσj + · · · + θ12...N σ1σ2...σN
 .

(15)

This type of probability distribution is called a log-linear model. Because the number of parameters
in a log-linear model is equal to the number of all possible conﬁgurations of an N-letter binary word
σ  we can determine the values of parameters so that the log-linear model pN (σ) exactly matches
empirical probability distribution pdata(σ): that is  pN (σ) = pdata(σ).
To compute the information for mismatched decoders  we construct simpliﬁed models of neural
responses that partially match the empirical distribution  pdata(σ). The simplest model is an “inde-
pendent model” p1(σ)  where only the average of each σi agrees with the experimental data: that is 
hσiip1(σ) = hσiipdata(σ). There are many possible probability distributions that satisfy these con-
straints. In accordance with the maximum entropy principle [12]  we choose the one that maximizes

entropy H  H = −Pσ p1(σ) log p1(σ). The resulting maximum entropy distribution is

(16)

p1(σ) =

1
Z1

exp"Xi

θ(1)

i σi# .

in which model parameters θ(1) are determined so that the constraints are satisﬁed. This model
corresponds to a log-linear model in which all orders of correlation parameters {θij  θijk  ...  θ12...N }
are omitted. If we perform maximum likelihood estimation of model parameters θ(1) in the log-
linear model  the result is that the average σi under the log-linear model equals the average σi
found in the data: that is  hσiip1(σ) = hσiipdata(σ). This result is identical to the constraints of
the maximum entropy model. Generally  the maximum entropy method is equivalent to maximum
likelihood ﬁtting of a log-linear model [6].
Similarly  we can consider a “second-order correlation model” p2(σ)  which is consistent with not
only the averages of σi but also the averages of all products σiσj found in the data. Maximizing the
entropy with constraints hσiip2(σ) = hσiipdata(σ) and hσiσjip2(σ) = hσiσjipdata(σ)  we obtain

p2(σ) =

1
Z2

exp
Xi

θ(2)

i σi +Xi j

θ(2)

ij σiσj


 

(17)

in which model parameters θ(2) are determined so that the constraints are satisﬁed. The procedure
described above can also be used to construct a “Kth-order correlation model” pK(σ). If we substi-
tute the simpliﬁed models of neural responses pK(σ|s) into mismatched decoding models q(σ|s) in

5

(cid:15000)

(cid:15040)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:15040)

(cid:19)

(cid:18)(cid:16)(cid:27)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:25)

(cid:18)(cid:16)(cid:24)

(cid:18)(cid:16)(cid:23)
(cid:20)

(cid:15001)

(cid:15040)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:15040)

(cid:19)

(cid:18)(cid:16)(cid:27)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:25)

(cid:18)(cid:16)(cid:24)

(cid:18)(cid:16)(cid:23)
(cid:20)

(cid:21)

(cid:15008)(cid:14977)
(cid:14984)(cid:14982)(cid:15008)
(cid:15008)(cid:14977)
(cid:14985)(cid:14982)(cid:15008)

(cid:21)

(cid:22)

(cid:23)

(cid:24)

(cid:25)

(cid:15013)(cid:15052)(cid:15044)(cid:15033)(cid:15036)(cid:15049)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15034)(cid:15036)(cid:15043)(cid:15043)(cid:15050)

(cid:15008)(cid:14977)
(cid:14984)(cid:14982)(cid:15008)
(cid:15008)(cid:14977)
(cid:14985)(cid:14982)(cid:15008)

(cid:25)

(cid:22)

(cid:24)
(cid:15013)(cid:15052)(cid:15044)(cid:15033)(cid:15036)(cid:15049)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15034)(cid:15036)(cid:15043)(cid:15043)(cid:15050)

(cid:23)

Figure 3: Dependence of amount of information obtained by simpliﬁed decoders on number of
ganglion cells analyzed. Same spike data obtained from retinal ganglion cells responding to a natural
movie were used to obtain analysis results shown in panels A and B. A: 10-s-long natural movie B:
100-ms-long natural movie

Eq. 2  we can compute the amount of information that can be obtained when more than Kth-order
correlations are ignored in the decoding 

I ∗

K(β) = −Xσ

pN (σ) log2Xs

p(s)pK(σ|s)β +Xs

p(s)Xσ

pN (σ|s) log2 pK(σ|s)β.

(18)

By evaluating the ratio of information  I ∗
be taken into account to extract enough information.

K/I  we can infer how many orders of correlation should

3.2 Results

1 = I ∗

1 /I and I ∗

1 /I  and that
First  we investigated how the ratio of information obtained by an independent model  I ∗
obtained by a second-order correlation model  I ∗
2 /I  changed when the number of cells analyzed was
changed. We set the length of the stimulus to 10 s. We could obtain 20 kinds of stimuli from a 200-s-
long natural movie (see Methods). In previous studies  comparable length stimuli (7 s for Nirenberg
et al.’s study [10] and 20 s for Schneidman et al.’s study [12]) were used. When two neurons were
analyzed  there were 21 possible combinations for choosing 2 cells out of 7 cells  which is the total
number of cells simultaneously recorded. We computed the average value of I ∗
K/I for K = 1  2 over
2 /I monotonically decreased
all possible combinations of cells. Figure 3A shows that I ∗
when the number of cells was increased. A comparison between the correct information  I ∗
1 /I  and
1 /I where I N L
1 (β = 1)  is shown in Figure 1B. When
Nirenberg-Latham information  I N L
only two cells were considered  I ∗
1 /I exceeded 90%  which means that ignoring correlation leads
to only a small loss of information. This is consistent with the result obtained by Nirenberg et al.
[10]. However  when all cells (N = 7) were used in the analysis  I ∗
1 /I becomes only about 60%.
Thus  correlation seems to be much more important for decoding when population activities are
considered than when only two cells are considered. At least  we can say that qualitatively different
things occur when large populations of cells are analyzed  as Schneidman et al. pointed out [12].
We should be careful about concluding from the results shown in Figure 3A that correlation is
important for decoding. In this analysis  we considered a 10-s-long stimuli and assumed stationarity
during each stimulus. By stationarity we mean that we assumed spikes are generated by a single
process that can be described by a single conditional distribution p(σ|s). Because the natural movies
change much more rapidly and our visual system has much higher time resolution than 10 s [13] 
we also considered shorter stimuli. In Figure 3B  we computed I ∗
2 /I over 100-ms-long
natural movies. In this case  we could obtain 2000 stimuli from the 200-s-long natural movie. When
the length of each stimulus was 100 ms  no spikes occurred while some stimuli were presented. We
removed those stimuli and used the remaining stimuli for the analysis. In this case  the amount of
information obtained by independent model I ∗
1 was more than 90% even when all cells (N = 7)
were considered. Although 100 ms may still be too long to be considered as a single process  the
result shown in Figure 3B reﬂects a situation that our brain has to deal with  that is more realistic than
that reﬂected in Figure 3A. Figure 4A shows the dependence of information obtained by simpliﬁed
decoders on the length of stimulus. In this analysis  we changed the length of the stimulus from 100
ms to 10 s and computed I ∗
2 /I for activities of N = 7 cells. We also analyzed additional
experimental data obtained when N = 6 retinal ganglion cells were simultaneously recorded from

1 /I and I ∗

1 /I and I ∗

6

(cid:15000)

(cid:15040)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:15040)

(cid:19)

(cid:18)(cid:16)(cid:27)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:25)

(cid:18)(cid:16)(cid:24)

(cid:18)(cid:16)(cid:23)
(cid:19)(cid:18)

(cid:770)(cid:19)

(cid:15001)

(cid:15040)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:15040)

(cid:15008)(cid:14977)
(cid:14984)(cid:14982)(cid:15008)
(cid:15008)(cid:14977)
(cid:14985)(cid:14982)(cid:15008)

(cid:18)

(cid:19)(cid:18)

(cid:19)

(cid:19)(cid:18)

(cid:15011)(cid:15036)(cid:15045)(cid:15038)(cid:15051)(cid:15039)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15050)(cid:15051)(cid:15040)(cid:15044)(cid:15052)(cid:15043)(cid:15052)(cid:15050)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:19)

(cid:18)(cid:16)(cid:27)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:25)

(cid:15008)(cid:14977)
(cid:14984)(cid:14982)(cid:15008)
(cid:15008)(cid:14977)
(cid:14985)(cid:14982)(cid:15008)

(cid:770)(cid:19)

(cid:19)(cid:18)

(cid:18)

(cid:19)(cid:18)

(cid:19)

(cid:19)(cid:18)

(cid:15011)(cid:15036)(cid:15045)(cid:15038)(cid:15051)(cid:15039)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15050)(cid:15051)(cid:15040)(cid:15044)(cid:15052)(cid:15043)(cid:15052)(cid:15050)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:15002)

(cid:15040)

(cid:14967)
(cid:15035)
(cid:15036)
(cid:15045)
(cid:15032)
(cid:15051)
(cid:15033)
(cid:15046)
(cid:14967)
(cid:15045)
(cid:15046)
(cid:15040)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15049)
(cid:15046)
(cid:15037)
(cid:15045)
(cid:15040)
(cid:14967)
(cid:15037)
(cid:15046)
(cid:14967)
(cid:15051)
(cid:15045)
(cid:15052)
(cid:15046)
(cid:15044)
(cid:15000)

(cid:14976)
(cid:14972)
(cid:14975)
(cid:14967)
(cid:15049)
(cid:15036)
(cid:15035)
(cid:15046)
(cid:15034)
(cid:15036)
(cid:15035)
(cid:14967)
(cid:15035)
(cid:15036)
(cid:15039)
(cid:15034)
(cid:15051)
(cid:15032)
(cid:15044)
(cid:15050)
(cid:15044)
(cid:14967)
(cid:15056)
(cid:15033)
(cid:14967)
(cid:14967)
(cid:14967)
(cid:14967)

(cid:15040)

(cid:19)

(cid:18)(cid:16)(cid:26)

(cid:18)(cid:16)(cid:24)

(cid:18)(cid:16)(cid:22)

(cid:18)(cid:16)(cid:20)

(cid:18)

(cid:15008)(cid:14977)

(cid:14984)(cid:14982)(cid:15008)

(cid:18)(cid:16)(cid:26)
(cid:15011)(cid:15036)(cid:15045)(cid:15038)(cid:15051)(cid:15039)(cid:14967)(cid:15046)(cid:15037)(cid:14967)(cid:15050)(cid:15051)(cid:15040)(cid:15044)(cid:15052)(cid:15043)(cid:15052)(cid:15050)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:18)(cid:16)(cid:27)

(cid:19)

Figure 4: Dependence of amount of information obtained by simpliﬁed decoders on length of stim-
uli. Stimulus was same natural movie for both panels  but spike data obtained from retinas of
different salamander were used in panels A and B. A: Seven simultaneously recorded ganglion cells
B: Six simultaneously recorded ganglion cells C: Artiﬁcial spike data generated according to the
ﬁring rates shown in Figure 5A

(cid:15001)

(cid:23)(cid:18)

(cid:18)
(cid:18)

(cid:23)(cid:18)

(cid:15000)

(cid:23)(cid:18)
(cid:14970)(cid:14984)

(cid:18)
(cid:18)

(cid:23)(cid:18)
(cid:14970)(cid:14985)

(cid:18)
(cid:18)

(cid:15040)

(cid:14976)
(cid:15050)
(cid:14982)
(cid:15050)
(cid:15036)
(cid:15042)
(cid:15047)
(cid:15050)
(cid:14975)
(cid:14967)
(cid:15036)
(cid:15051)
(cid:15032)
(cid:15049)
(cid:14967)
(cid:15038)
(cid:15045)
(cid:15040)
(cid:15049)
(cid:15040)
(cid:15005)

(cid:19)

(cid:19)

(cid:15019)(cid:15040)(cid:15044)(cid:15036)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:20)

(cid:20)

(cid:15002)

(cid:23)(cid:18)

(cid:18)

(cid:23)(cid:18)

(cid:18)

(cid:15050)(cid:14984)

(cid:15050)(cid:14985)

(cid:15050)(cid:14986)

(cid:15050)(cid:14987)

(cid:23)(cid:18)

(cid:18)

(cid:23)(cid:18)

(cid:18)

(cid:18)(cid:16)(cid:20) (cid:18)(cid:16)(cid:22)

(cid:18)(cid:16)(cid:20) (cid:18)(cid:16)(cid:22)

(cid:23)(cid:18)

(cid:18)

(cid:23)(cid:18)

(cid:18)

(cid:23)(cid:18)

(cid:18)

(cid:23)(cid:18)

(cid:18)

(cid:19)(cid:16)(cid:20) (cid:19)(cid:16)(cid:22)

(cid:19)(cid:16)(cid:20) (cid:19)(cid:16)(cid:22)

(cid:18)(cid:16)(cid:24) (cid:18)(cid:16)(cid:26) (cid:19)

(cid:18)(cid:16)(cid:24) (cid:18)(cid:16)(cid:26) (cid:19)

(cid:15019)(cid:15040)(cid:15044)(cid:15036)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

(cid:19)(cid:16)(cid:24) (cid:19)(cid:16)(cid:26) (cid:20)

(cid:19)(cid:16)(cid:24) (cid:19)(cid:16)(cid:26) (cid:20)

(cid:19)(cid:16)(cid:23)

(cid:20)

(cid:19)(cid:16)(cid:23)

(cid:20)

(cid:15050)(cid:14984)

(cid:15050)(cid:14985)

(cid:23)(cid:18)

(cid:18)
(cid:19)

(cid:23)(cid:18)

(cid:18)(cid:16)(cid:23)

(cid:19)

(cid:18)
(cid:18)

(cid:18)(cid:16)(cid:23)

(cid:18)
(cid:19)

(cid:19)

(cid:15019)(cid:15040)(cid:15044)(cid:15036)(cid:14967)(cid:14975)(cid:15050)(cid:14976)

Figure 5: Firing rates of two model cells. Rate of cell #1 shown in top panel; rate of cell #2 is shown
in bottom panel. A: Firing rates from 0 to 2 s. B: Firing rates (solid line) and mean ﬁring rates
(dashed line) when stimulus was 1 s long. C: Firing rates (solid line) and mean ﬁring rates (dashed
line) when stimulus was 500 ms long.

another salamander retina. The same 200-s-long natural movie was used as a stimulus for Figure 4B
as for Figure 4A  and the activities of N = 6 cells were analyzed. Figure 4B shows the result. We
can clearly see the same tendency as shown in Figures 4A and B: the amount of information decoded
by the simpliﬁed decoders monotonically increased as the length of the stimulus was shortened.

To clarify the reason the correlation becomes less important as the stimulus is shortened  we used
the toy model shown in Figure 5. We considered the case in which two cells ﬁre independently
in accordance with a Poisson process and performed an analysis similar to the one we did for the
actual spike data. We used simulated spike data for the two cells generated in accordance with the
ﬁring rates shown in Figure 5A. The ﬁring rates with a 2-s stimulus sinusoidally change with time.
We divided the 2-s-long stimulus into two 1-s-long stimulus  s1 and s2  as shown in Figure 5B.
Then  we computed mutual information I and the information obtained by independent model I ∗
1
over s1 and s2. Because the two cells ﬁred independently  there were no correlations between two
cells essentially. However  there was pseudo correlation due to the assumption of stationarity for the
dynamically changing stimulus. The pseudo correlation was high for s1 and low for s2. This means
that “correlation” plays an important role in discriminating two stimuli  s1 and s2. In contrast  the
mean ﬁring rates of the two cells during each stimulus were equal for s1 and s2. Therefore  if the
stimulus is 1 s long  we cannot discriminate two stimuli by using the independent model  that is 
1 = 0. We also considered the case in which the stimulus was 0.5 s long  as shown in Figure
I ∗
5C. In this case  pseudo correlations again appeared but there was a signiﬁcant difference in the
mean ﬁring rates between the stimuli. Thus  the independent model can be used to extract almost all
the information. The dependence of I ∗
1 /I on the stimulus length is shown in Figure 4C. Behaviors
similar to those represented in Figure 4C were also observed in the analysis of the actual spike data
for retinal ganglion cells (Figure 4A and 4B). Even if we observe that correlation carries a signiﬁcant
large portion of information for longer stimuli compared with the speed of change in the ﬁring rates 

7

it may simply be caused by meaningless pseudo correlation. To assess the role of correlation in
information processing  the stimuli used should be short enough to think neural responses to these
stimuli generated by a single process.

4 Summary and Discussion

We described a general framework for investigating how far the decoding process in the brain can
be simpliﬁed. We computed the amount of information that can be extracted by using simpliﬁed
decoders constructed using a maximum entropy model  i.e.  mismatched decoders. We showed
that more than 90% of the information encoded in retinal ganglion cells activities can be decoded
by using an independent model that ignores correlation. Our results imply that the brain uses a
simpliﬁed decoding strategy in which correlation is ignored.

When we computed the information obtained by the independent model  we regarded a 100-ms-long
natural movie as one stimulus. However  when we considered stimuli that were long compared with
the speed of the change in the ﬁring rates as one stimulus  correlation carried a large portion of
information. This is due to pseudo correlation  which is observed if stationarity is assumed for long
durations. The human visual system can process visual information in less than 150 ms [13]. We
should set the length of the stimulus appropriately by taking the time resolution of our visual system
into account.

Our results do not imply that any kind of correlation does not carry much information because we
dealt only with correlated spikes within a 5-ms time bin. In our analysis  we did not analyze the
correlation on a longer time scale  which can be observed in the activities of retinal ganglion cells
[7]. We also did not investigate the information carried by the relative timing of spikes [3]. Further
investigations are needed for these types of correlation. Our approach of comparing the mutual
information with the information obtained by simpliﬁed decoders can also be used for studying
other types of correlations.

References

[1] Abbott  L. F.  & Dayan  P. (1999). Neural Comput.  11  91-101.
[2] Deneve  S.  Latham  P. E.  & Pouget  A. (1999). Nature Neurosci.  2  740-745.
[3] Gollish  S.  & Meister  M. (2008). Science  319  1108-1111.
[4] Jazayeri  M. & Movshon  J. A. (2006). Nature Neurosci.  9  690-696.
[5] Latham  P. E.  & Nirenberg  S. (2005). J. Neurosci.  25  5195-5206.
[6] MacKay  D. (2003). Information Theory  Inference and Learning Algorithms (Cambridge Univ.

Press  Cambridge  England).

[7] Meister  M.  & Berry  M. J. II (1999). Neuron  22  435-450.
[8] Merhav  N.  Kaplan  G.  Lapidoth  A.  & Shamai Shitz  S. (1994). IEEE Trans. Inform. Theory 

40  1953-1967.

[9] Nakahara  H.  & Amari  S. (2002). Neural Comput.  14  2269-2316.
[10] Nirenberg  S.  Carcieri  S. M.  Jacobs  A. L.  & Latham  P. E. (2001). Nature  411  698-701.
[11] Nirenberg  S.  & Latham  P. (2003). Proc. Natl. Acad. Sci. USA  100  7348-7353.
[12] Schneidman  E.  Berry  M. J. II  Segev  R.  & Bialek. W. (2006). Nature  440  1007-1012.
[13] Thorpe  S.  Fize  D.  & Marlot  C. (1996). Nature  381  520-522.
[14] Wu  S.  Nakahara  H.  & Amari  S. (2001). Neural Comput.  13  775-797.

8

,Vibhav Vineet
Carsten Rother
Philip Torr
Changyou Chen
Jun Zhu
Xinhua Zhang
Sida Wang
Arun Tejasvi Chaganty
Percy Liang
Gamaleldin Elsayed
Simon Kornblith
Quoc Le