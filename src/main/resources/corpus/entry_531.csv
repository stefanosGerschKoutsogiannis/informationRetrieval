2016,An algorithm for L1 nearest neighbor search via monotonic embedding,Fast algorithms for nearest neighbor (NN) search have in large part focused on L2 distance. Here we develop an approach for L1 distance that begins with an explicit and exact embedding of the points into L2. We show how this embedding can efficiently be combined with random projection methods for L2 NN search  such as locality-sensitive hashing or random projection trees. We rigorously establish the correctness of the methodology and show by experimentation that it is competitive in practice with available alternatives.,An algorithm for (cid:2)1 nearest neighbor search via

monotonic embedding

Xinan Wang∗
UC San Diego

xinan@ucsd.edu

Sanjoy Dasgupta

UC San Diego

dasgupta@cs.ucsd.edu

Abstract

Fast algorithms for nearest neighbor (NN) search have in large part focused on (cid:2)2
distance. Here we develop an approach for (cid:2)1 distance that begins with an explicit
and exactly distance-preserving embedding of the points into (cid:2)2
2. We show how
this can efﬁciently be combined with random-projection based methods for (cid:2)2 NN
search  such as locality-sensitive hashing (LSH) or random projection trees. We
rigorously establish the correctness of the methodology and show by experimen-
tation using LSH that it is competitive in practice with available alternatives.

1

Introduction

Nearest neighbor (NN) search is a basic primitive of machine learning and statistics. Its utility in
practice hinges on two critical issues: (1) picking the right distance function and (2) using algorithms
that ﬁnd the nearest neighbor  or an approximation thereof  quickly.
The default distance function is very often Euclidean distance. This is a matter of convenience
and can be partially justiﬁed by theory: a classical result of Stone [1] shows that k-nearest neigh-
bor classiﬁcation is universally consistent in Euclidean space. This means that no matter what the
distribution of data and labels might be  as the number of samples n goes to inﬁnity  the kn-NN
classiﬁer converges to the Bayes-optimal decision boundary  for any sequence (kn) with kn → ∞
and kn/n → 0. The downside is that the rate of convergence could be slow  leading to poor perfor-
mance on ﬁnite data sets. A more careful choice of distance function can help  by better separating
the different classes. For the well-known MNIST data set of handwritten digits  for instance  the 1-
NN classiﬁer using Euclidean distance has an error rate of about 3%  whereas a more careful choice
of distance function—tangent distance [2] or shape context [3]  for instance—brings this below 1%.
The second impediment to nearest neighbor search in practice is that a naive search through n
candidate neighbors takes O(n) time  ignoring the dependence on dimension. A wide variety of
ingenious data structures have been developed to speed this up. The most popular of these fall into
two categories: hashing-based and tree-based.
Perhaps the best-known hashing approach is locality-sensitive hashing (LSH) [4  5  6  7  8  9  10].
These randomized data structures ﬁnd approximate nearest neighbors with high probability  where
c-approximate solutions are those that are at most c times as far as the nearest neighbor.
Whereas hashing methods create a lattice-like spatial partition  tree methods [11  12  13  14] create
a hierarchical partition that can also be used to speed up nearest neighbor search. There are families
of randomized trees with strong guarantees on the tradeoff between query time and probability of
ﬁnding the exact nearest neighbor [15].
These hashing and tree methods for (cid:2)2 distance both use the same primitive: random projection [16].
For data in Rd  they (repeatedly) choose a random direction u from the multivariate Gaussian

∗

Supported by UC San Diego Jacobs Fellowship

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

√

2.

1/2
1

1/2
1

N (0  Id) and then project points x onto this direction: x (cid:4)→ u · x. Such projections have many
appealing mathematical properties that make it possible to give algorithmic guarantees  and that
also produce good performance in practice.
For distance functions other than (cid:2)2  there has been far less work. In this paper  we develop nearest
neighbor methods for (cid:2)1 distance. This is a more natural choice than (cid:2)2 in many situations  for
instance when the data points are probability distributions: documents are often represented as dis-
tributions over topics  images as distributions over categories  and so on. Earlier works on (cid:2)1 search
are summarized below. We adopt a different approach  based on a novel embedding.
One basic fact is that (cid:2)1 distance is not embeddable in (cid:2)2 [17]. That is  given a set of points
x1  . . .   xn ∈ Rd  it is in general not possible to ﬁnd corresponding points z1  . . .   zn ∈ Rq such that
(cid:6)xi− xj(cid:6)1 = (cid:6)zi− zj(cid:6)2. This can be seen even from the four points at the vertices of a square—any
embedding of these into (cid:2)2 induces a multiplicative distortion of at least
Interestingly  however  the square root of (cid:2)1 distance is embeddable in (cid:2)2 [18]. And the nearest
neighbor with respect to (cid:2)1 distance is the same as the nearest neighbor with respect to (cid:2)
. This
observation is the starting point of our approach. It suggests that we might be able to embed data
into (cid:2)2 and then simply apply well-established methods for (cid:2)2 nearest neighbor search. However 
there are numerous hurdles to overcome.
into (cid:2)2 is an existential  not algorithmic  fact. Indeed  all that is
First  the embeddability of (cid:2)
known for general case is that there exists such an embedding into Hilbert space. For the special
case of data in {0  1  . . .   M}d  earlier work has suggested a unary embedding into a Hamming
space {0  1}M d (where 0 ≤ x ≤ M gets mapped to x 1’s followed by (M − x) 0’s) [19]  but this is
wasteful of space and is inefﬁcient to be used by dimension reduction algorithms [16] when M is
large. Our embedding is general and is more efﬁcient.
Now  given a ﬁnite point set x1  . . .   xn ∈ Rd and the knowledge that an embedding exists  we
could use multidimensional scaling [20] to ﬁnd such an embedding. But this takes O(n3) time 
which is often not viable. Instead  we exhibit an explicit embedding: we give an expression for
points z1  . . .   zn ∈ RO(nd) such that (cid:6)xi − xj(cid:6)1 = (cid:6)zi − zj(cid:6)2
2.
This brings us to the second hurdle. The explicit construction avoids inﬁnite-dimensional space but
is still much higher-dimensional than we would like. The space requirement for writing down the n
embedded points is O(n2d)  which is prohibitive in practice. To deal with this  we recall that the two
popular schemes for (cid:2)2 embedding described above are both based on Gaussian random projections 
and in fact look at the data only through the lens of such projections. We show how to compute these
projections without ever constructing the O(nd)-dimensional embeddings explicitly.
Finally  even if it is possible to efﬁciently build a data structure on the n points  how can queries
be incorporated? It turns out that if a query point is added to the original n points  our explicit
embedding changes signiﬁcantly. Nonetheless  by again exploiting properties of Gaussian random
projections  we show that it is possible to hold on to the random projections of the original n em-
bedded points and to set the projection of the query point so that the correct joint distribution is
achieved. Moreover  this can be done very efﬁciently.
Finally  we run a variety of experiments showing the good practical performance of this approach.

Related work

The k-d tree [11] is perhaps the prototypical tree-based method for nearest neighbor search  and can
be used for (cid:2)1 distance. It builds a hierarchical partition of the data using coordinate-wise splits  and
uses geometric reasoning to discard subtrees during NN search. Its query time can degenerate badly
with increasing dimension  as a result of which several variants have been developed  such as trees
in which the cells are allowed to overlap slightly [21]. Various tree-based methods have also been
developed for general metrics  such as the metric tree and cover tree [14  12].
For k-d tree variants  theoretical guarantees are available for exact (cid:2)2 nearest neighbor search when
the split direction is chosen at random from a multivariate Gaussian [15]. For a data set of n points 
the tree has size O(n) and the query time is O(2d log n)  where d is the intrinsic dimension of the
data. Such analysis is not available for (cid:2)1 distance.

2

Also in wide use is locality-sensitive hashing for approximate nearest neighbor search [22]. For a
data set of n points  this scheme builds a data structure of size O(n1+ρ) and ﬁnds a c-approximate
nearest neighbor in time O(nρ)  for some ρ > 0 that depends on c  on the speciﬁc distance function 
and on the hash family. For (cid:2)2 distance  it is known how to achieve ρ ≈ 1/c2 [23]  although the
scheme most commonly used in practice has ρ ≈ 1/c [8]. This works by repeatedly using the
following hash function:

h(x) = (cid:10)(v · x + b)/R(cid:11) 

where v is chosen at random from a multivariate Gaussian  R > 0 is a constant  and b is uniformly
distributed in [0  R). A similar scheme also works for (cid:2)1  using Cauchy random projection: each
coordinate of v is picked independently from a standard Cauchy distribution. This achieves exponent
ρ ≈ 1/c  although one downside is the high variance of this distribution. Another LSH family
[22  10] uses a randomly shifted grid for (cid:2)1 nearest neighbor search. But it is less used in practice 
due to its restrictions on data. For example  if the nearest neighbor is further away than the width of
the grid  it may never be found.
Besides LSH  random projection is the basis for some other NN search algorithms [24  25]  classiﬁ-
cation methods [26]  and dimension reduction techniques [27  28  29].
There are several impediments to developing NN methods for (cid:2)1 spaces. 1) There is no Johnson-
Lindenstrauss type dimension reduction technique for (cid:2)1 [30]. 2) The Cauchy random projection
does not preserve the (cid:2)1 distance as a norm  which restricts its usage for norm based algorithms [31].
3) Useful random properties [26] cannot be formulated exactly; only approximations exist. Fortu-
nately  all these three problems are absent in (cid:2)2 space  which motivates developing efﬁcient embed-
ding algorithms from (cid:2)1 to (cid:2)2.

2 Explicit embedding

We begin with an explicit isometric embedding from (cid:2)1 to (cid:2)2
immediately to multiple dimensions because both (cid:2)1 and (cid:2)2

2 for 1-dimensional data. This extends

2 distance are coordinatewise additive.

2.1 The 1-dimensional case
First  sort the points x1  . . .   xn ∈ R so that x1 ≤ x2 ≤ ··· ≤ xn. Then  construct the embedding
φ(x1)  φ(x2)  . . .   φ(xn) ∈ Rn−1 as follows:
(cid:5)
⎤
⎥⎥⎥⎥⎦

(cid:3)(cid:4)
x2 − x1
x3 − x2
x4 − x3

(cid:3)(cid:4)
x2 − x1
x3 − x2

(cid:2)
⎡
⎢⎢⎢⎢⎣

(cid:3)(cid:4)
x2 − x1

φ(x2)

(cid:5)
⎤
⎥⎥⎥⎥⎦

(cid:3)(cid:4)

φ(x1)

(cid:2)
⎡
⎢⎢⎢⎢⎣

√
√
√

φ(x3)

√
√

φ(xn)

√

(1)

0
0
0
...
0

(cid:5)
(cid:2)
⎡
⎤
⎢⎢⎢⎢⎣
⎥⎥⎥⎥⎦
j(cid:13)

k=i+1

(cid:12)

0
0
...
0

(cid:14)(cid:15)
xk − xk−1

(cid:2)
⎡
⎢⎢⎢⎢⎣
(cid:16)2

0
...
0

(cid:17)1/2

(cid:12)

(cid:5)
⎤
⎥⎥⎥⎥⎦ . . .
j(cid:13)

√

...
xn − xn−1
(cid:17)1/2

For any 1 ≤ i < j ≤ n  φ(xi) and φ(xj) agree on all coordinates except i to (j − 1). Therefore 

(cid:6)φ(xi) − φ(xj)(cid:6)2 =

xk − xk−1

= |xj − xi|1/2  (2)

=

k=i+1

so the embedding preserves the (cid:2)
restrictions on the range of x1  x2  . . .   xn  it is applicable to any ﬁnite set of points.

distance between these points. Since the construction places no

1/2
1

2.2 Extension to multiple dimensions

We construct an embedding of d-dimensional points by stacking 1-dimensional embeddings.
Consider points x1  x2  . . .   xn ∈ Rd.
Suppose we have a collection of embedding maps
φ1  φ2  . . .   φd  one per dimension. Each of the embeddings is constructed from the values on a
denote the j-th coordinate of xi  for 1 ≤ j ≤ d  then embedding φj
single coordinate: if we let x

(j)
i

3

is based on x

(j)
1   x

(j)
2   . . .   x

(j)

(cid:18)
n ∈ R. The overall embedding is the concatenation

(cid:18)

(cid:19)

(cid:18)

(cid:18)

φ (xi) =

φτ
1

x

  φτ
2

x

(1)
i

  . . .   φτ
d

x

(d)
i

where 1 ≤ i ≤ n  and τ denotes transpose. For any 1 ≤ i < j ≤ n 
(cid:18)
− φk
(cid:17)1/2

(cid:6)φ (xi) − φ (xj)(cid:6)2 =

(cid:12)
(cid:12)

(cid:18)

(cid:19)

(k)
i

k=1

x

x

d(cid:13)
d(cid:13)

(cid:21)(cid:21)(cid:21)

− x

(k)
j

(k)
i

= (cid:6)xi − xj(cid:6)1/2

1

=

k=1

(cid:19)(cid:19)τ ∈ Rd(n−1)
(cid:17)1/2
(cid:19)(cid:20)(cid:20)(cid:20)2

(k)
j

2

(2)
i

(cid:19)
(cid:20)(cid:20)(cid:20)φk
(cid:21)(cid:21)(cid:21)x

(3)

(4)

(5)

(6)

It may be of independent interest to consider the properties of this explicit embedding. We can
represent it by a matrix of n columns with one embedded point per column. The rank of this
matrix—and  therefore  the dimensionality of the embedded points—turns out to be O(n). But we
can show that the “effective rank” [32] of the centered matrix is just O(d log n); see Appendix B.

3

Incorporating a query

Once again  we begin with the 1-dimensional case and then extend to higher dimension.

3.1 The 1-dimensional case
For nearest neighbor search  we need a joint embedding of the data points S = {x1  x2  . . .   xn}
with the subsequent query point q. In fact  we need to embed S ﬁrst and then incorporate q later  but
this is non-trivial since adding q changes the explicit embedding of other points.
We start with an example. Again  assume x1 ≤ x2 ≤ ··· ≤ xn.
Example 1. Suppose query q has x2 ≤ q < x3. Adding q to the original n points changes the
embedding φ(·) ∈ Rn−1 of Eq. 1 to φ(·) ∈ Rn. Notice that the dimension increases by one.

(cid:2)
⎡

⎢⎢⎢⎢⎢⎢⎣

(cid:3)(cid:4)

φ(x1)

0
0
0
0
...
0

(cid:5)
⎤

⎥⎥⎥⎥⎥⎥⎦

(cid:2)
⎡

⎢⎢⎢⎢⎢⎢⎣

(cid:3)(cid:4)
x2 − x1

φ(x2)

√

(cid:5)
⎤

⎥⎥⎥⎥⎥⎥⎦

(cid:2)
⎡

⎢⎢⎢⎢⎢⎢⎣

φ(x3)

φ(xn)

(cid:5)
⎤

(cid:2)
⎡

(cid:3)(cid:4)
√
x2 − x1
√
q − x2
√
x3 − q
√
x4 − x3

⎥⎥⎥⎥⎥⎥⎦ . . .

(cid:3)(cid:4)
√
x2 − x1
√
q − x2
√
x3 − q
0
...
0
q − x2  0  . . .   0)τ ∈ Rn.

⎢⎢⎢⎢⎢⎢⎣

√

√

xn − xn−1

...

(cid:5)
⎤

⎥⎥⎥⎥⎥⎥⎦

√

x2 − x1 

0
0
0
...
0

The query point is mapped to φ(q) = (

From the example above  it is clear what happens when q lies between some xi and xi+1. There are
also two “corner cases” that can occur: q < x1 and q > xn. Fortunately  the embedding of S is
almost unchanged for the corner cases: φ(xi) = (φτ (xi)  0)τ ∈ Rn  appending a zero at the end.
x1 − q)τ ∈ Rn; for q ≥ xn  the query is
For q < x1  the query is mapped to φ(q) = (0  . . .   0 
mapped to φ(q) = (

q − xn)τ ∈ Rn.

x3 − x2  . . .  

xn − xn−1 

x2 − x1 

√

√

√

√

√

3.2 Random projection for the 1-dimensional case

We would like to generate Gaussian random projections of the (cid:2)2 embeddings of the data points. In
this subsection  we mainly focus on the typical case when the query q lies between two data points 
and we leave the treatment of the (simpler) corner cases to Alg. 1. The notation follows section 3.1 
and we assume the xi are arranged in increasing order for i = 1  2  . . .   n.
Setting 1. The query lies between two data points: xα ≤ q < xα+1 for some 1 ≤ α ≤ n − 1.

4

We will consider two methods for randomly projecting the embedding of S ∪{q} and show that they
yield exactly the same joint distribution.
The ﬁrst method applies Gaussian random projection to the embedding φ of S ∪ {q}. Sample a
multivariate Gaussian vector v from N (0  In). For any x ∈ S ∪ {q}  the projection is

pg(x) := v · φ(x)

(7)
This is exactly the projection we want. However  it requires both S and q  whereas in practice  we
will initially have to project just S by itself  and we will only later be given some (arbitrary) q.
The second method starts by projecting the explicitly embedded points S. Later  it receives query
q and ﬁnds a suitable projection for it as well. So  we begin by sampling a multivariate Gaussian
vector u from N (0  In−1)  and for any x ∈ S  use the projection

(8)
where the subindex e stands for embedding. Conditioned on the value (pe (xα+1)−pe (xα))  namely
√
xα+1 − xα · u(α)  the projection of a subsequent query q is taken to be

pe(x) := u · φ(x)

pe(q) = pe (xα) + Δ

(cid:22)

Δ ∼ N

1 · (pe (xα+1) − pe (xα))
σ2

σ2
1 + σ2
2

 

σ2
1σ2
2
σ2
1 + σ2
2

(cid:23)

(9)

1 = q − xα  σ2

2 = xα+1 − q.

where σ2
Theorem 1. Fix any x1  . . .   xn  q ∈ R satisfying Setting 1. Consider the joint distribution of
[pg (x1)   pg (x2)   . . .   pg (xn)   pg(q)] induced by a random choice of v (as per Eq. 7)  and the joint
distribution of [pe (x1)   pe (x2)   . . .   pe (xn)   pe(q)] induced by a random choice of u and Δ (as
per Eqs. 8 and 9). These distributions are identical.

The details are in Appendix A: brieﬂy  we show that both joint distributions are multivariate Gaus-
sians  and that they have the same mean and covariance.
We highlight the advantages of our method. First  projecting the data set using Eq. 8 does not require
advance knowledge of the query  which is crucial for nearest neighbor search; second  generating
the projection for the 1-dimensional query takes O(log n) time  which makes this method efﬁcient.
We describe the 1-dimensional algorithm in Alg. 1  where we assume that a permutation that sorts
the points  denoted Π  is provided  along with the location of q within this ordering  denoted α. We
will resolve this later in Alg. 2.

3.3 Random projection for the higher dimensional case

We will henceforth use ERP (Euclidean random projection) to denote our overall scheme consisting
of embedding (cid:2)1 into (cid:2)2
2  followed by random Gaussian projection (Alg. 2). A competitor scheme 
as described earlier  applies Cauchy random projection directly in the (cid:2)1 space; we refer to this as
CRP. The time and space costs for ERP are shown in Table 1  if we generate k projections for n data
points and m queries in Rd. The costs scale linearly in d  since the constructions and computation
are dimension by dimension. We have a detailed analysis below.
Preprocessing: This involves sorting the points along each coordinate separately and storing the
resulting permutations Π1  . . .   Πd. The time and space costs are acceptable  because reading or
storing the data takes as much as O(nd).
Project data: The time taken by ERP to project the n points is comparable to that of CRP. But
ERP requires a factor O(n) more space  compared to O(kd) for CRP  because it needs to store the
projections of each of the individual coordinates of the data points.
Project query: ERP methods are efﬁcient for query answering. The projection is calculated directly
in the original d-dimensional space. The log n overhead comes from using binary search  coordi-
natewise  to place the query within the ordering of the data points. Once these ranks are obtained 
they can be reused for as many projections as needed.

5

Algorithm 1 Random projection (1-dimensional case)

≤ xπ2

such that xπ1

function project-data (S  Π)
input:
— data set S = (xi : 1 ≤ i ≤ n)
— sorted indices Π = (πi : 1 ≤ i ≤ n)
≤  . . .  ≤ xπn
output:
— projections P = (pi : 1 ≤ i ≤ n) for S
← 0
pπ1
for i = 2  3  . . .   n do
ui ← N (0  1)
pπi
end for
return P

← pπi−1 + ui ·(cid:15)

− xπi−1

xπi

function project-query(q  α S  Π  P )
input:
— query q and its rank α in data set S
— sorted indices Π of S
— projections P of S
output:
— projection pq for q
case: 1 ≤ α ≤ n − 1
1 ← q − xπα
(cid:22)
σ2
2 ← xπα+1 − q
σ2
1 · (pπα+1
σ2
Δ ← N
pq ← pπα + Δ
case: α = 0
− q
r ← N (0  1) 
r ← N (0  1)  pq ← pπn + r · √
q − xπn
return pq

pq ← r · √

σ2
1σ2
2
σ2
1 + σ2
2

case: α = n

− pπα )

σ2
1 + σ2
2

(cid:23)

 

xπ1

Table 1: Efﬁciency of ERP algorithm: Generate k projections for n data points and m queries in Rd.

Time cost
Space cost

Preprocessing
O(dn log n)

O(dn)

Project data

Project query

O(knd)
O(knd)

O(md(k + log n))

NA

4 Experiment

In this section  we demonstrate that ERP can be directly used by existing NN search algorithms 
such as LSH  for efﬁcient (cid:2)1 NN search. We choose commonly used data sets for image retrieval
and text classiﬁcation. Besides our method  we also implement the metric tree (a popular tree-type
data structure) and Cauchy LSH for comparison.
Data sets When data points represent distributions  (cid:2)1 distance is natural. We use four such data
sets. 1) Corel uci [21]  available at [33]  contains 68 040 histograms (32-dimension) for color im-
ages from Corel image collections; 2) Corel hist [34  21]  processed by [21]  contains 19 797 his-
tograms (64-dimension  non-zero dimension is 44) for color images from Corel Stock Library; 3)
Cade [35]  is a collection of documents from Brazilian web pages. Topics are extracted using latent
Dirichlet allocation algorithm [36]. We use 13 016 documents with distributions over the 120 topics
(120-dimension); 4) We download about 35 000 images from ImageNet [37]  and process each of
them into a probabilistic distribution over 1 000 classes using trained convolution neural network
[38]. Furthermore  we collapse the distribution into a 100-dimension representation  summing each
10 consecutive mass of probability. This reduces the training and testing time.
In each data set  we remove duplicates. For either parameter optimization or testing  we randomly
separate out 10% of the data as queries such that the query-to-data ratio is 1 : 9.
Performance evaluation We evaluate performance using query cost. For linear scan or metric
tree  this is the average number of points accessed when answering a query. For LSH  we also need
to add the overhead of evaluating the LSH functions.
The scheme [8  39] of LSH is summarized as follows. Given three parameters k  L and R (k  L
are positive integers  k is even  R is a positive real)  the LSH algorithm uses k-tuple hash functions
of the form g(x) = (h1(x)  h2(x)  . . .   hk(x)) to distribute data or queries to their bins. L is the
total number of such g-functions. The h-functions are of the form h(x) = (cid:10)(v · x + b)/R(cid:11)  each

6

Algorithm 2 Overall algorithm for Random projection  in context of NN search

Starting information:
— data set S = {xi : 1 ≤ i ≤ n} ⊂ Rd
Subsequent arrival:
— query q ∈ Rd

preprocessing:
Sort data along each dimension:
for j ∈ {1  . . .   d} do
Sj = {x
Πj ← index-sort (Sj)  where

: 1 ≤ i ≤ n}

(j)
i

Πj = {πji : 1 ≤ i ≤ n} satisfying
πj1 ≤ x

πj2 ≤ ··· ≤ x

(j)
πjn

(j)

(j)

x
end for
save Π = (Π1  Π2  . . .   Πd)

project data:
for j = 1  2  . . .   d do
Pj ← project-data (Sj  Πj) where

Pj = {pji : 1 ≤ i ≤ n}
(cid:24)

end for
save P = (P1  P2  . . .   Pd)
projection of xi ∈ S is
d
j=1 pji
project query:
for j = 1  2  . . .   d do
αj ← binary-search(q(j)  Sj  Πj) satisfying
≤ q(j) ≤ x

(j)
πjαj
end for
save rank α for use in multiple projections
pq ← 0
for j = 1  2  . . .   d do
pg ← pg+ project-query(q(j)  αj Sj  Πj  Pj)
end if
projection for q is pg

(j)
πj(αj +1)

x

Table 2: Performance evaluation: Query cost = Tr + To.

Linear Scan or Metric Tree

CRP-LSH
ERP-LSH

Retrieval cost: Tr
# Accessed points
# Accessed points
# Accessed points

Overhead: To

0

k/2 · √

2L

k/2 · √

2L + log n

b ∈ [0  R). As suggested in [39]  we implement the reuse of h-functions so that only (k/2·√

either explicitly or implicitly associated with a random vector v and a uniformly distributed variable
2L) of
them are actually evaluated. For ERP-LSH  there is an additional overhead of log n due to the use
of binary search. We summarize these costs in Table 2; for conciseness  we have removed the linear
dependence on d in both the retrieval cost and the overhead.
Implementations The linear scan and the metric tree are for exact NN search. We use the code
[40] for metric tree. For LSH  there is only public code for (cid:2)2 NN search. We implement the LSH
scheme  referring to the manual [39]. In particular  we implement the reuse of the h-functions  such

that the number of actually evaluated h-functions is (k/2 · √

2L)  in contrast to (k · L).

(cid:24)

(cid:15)(cid:6)q − xN N (q)(cid:6)1 is the average (cid:2)

We choose approximation factor c = 1.5 (the results turn out to be much closer to true NN)  and
set the success rate to be 0.9  which means that the algorithm should report c-approximate NN
successfully for at least 90% of the queries. Taking the parameter suggestions [8] into account  we
choose R for CRP-LSH from dN N × {1  5  10  50  100}; we choose R for ERP-LSH from d
×
(cid:3)
(cid:24)
(cid:6)q − xN N (q)(cid:6)1 is the average (cid:2)1 NN distance; d
{1  2  3  4}  where dN N = 1|Q|
(cid:3)
N N
N N =
(cid:3)
1|Q|
N N normalizes
the average NN distance to 1 for LSH. Fixing R  we optimize k and L in the following range:
k ∈ {2  4  . . .   30}  L ∈ {1  2  . . .   40}.
Results Both CRP-LSH and ERP-LSH achieve a competitive efﬁciency over the other two meth-
ods. We list the test results in Table 3  and put parameters in Table 4 in Appendix C.

1/2
1 NN distance. The term dN N or d

q∈Q

q∈Q

7

Table 3: Average query cost and average approximation rate if applicable (in parentheses).

Corel uci
(d = 32)
61220
2575

329 ± 55 (1.07)
330 ± 18 (1.11)

Corel hist
(d = 44)
17809
718

245 ± 43 (1.05)
250 ± 15 (1.08)

Linear scan
Metric tree
CRP-LSH
ERP-LSH

Cade

(d = 120)

11715
9184

292 ± 11 (1.11)
218 ± 8 (1.15)

ImageNet
(d = 100)

31458
12375

548 ± 66 (1.09)
346 ± 15 (1.13)

5 Conclusion

In this paper  we have proposed an explicit embedding from (cid:2)1 to (cid:2)2
2  and we have found an algorithm
to generate the random projections  reducing the time dependence of n from O(n) to O(log n). In
addition  we have observed that the effective rank of the (centered) embedding is as low as O(d ln n) 
compared to its rank O(n). Algorithms remain to be explored  in order to take advantage of such a
low rank.
Our current method takes space O(ndm) to store the parameters of the random vectors  where m is
the number of hash functions. We have implemented one empirical scheme [39] to reuse the hashing
functions. It is still expected to develop other possible schemes.

Acknowledgement

The authors are grateful to the National Science Foundation for support under grant IIS-1162581.

References
[1] C. J. Stone. Consistent nonparametric regression. The Annals of Statistics  5:595–620  1977.
[2] P. Y. Simard  Y. A. LeCun  J. S. Denker  and B. Victorri. Transformation invariance in pattern
In Neural networks: Tricks of the

recognition—Tangent distance and tangent propagation.
trade  volume 1524  pages 239–274. Springer-Verlag  New York  1998.

[3] S. Belongie  J. Malik  and J. Puzicha. Shape matching and object recognition using shape

contexts. IEEE Trans. Pattern Anal. Mach. Intell.  24(4):509–522  2002.

[4] A. Broder. On the resemblance and containment of documents. In Proceedings of Compression

and Complexity of Sequences  pages 21–29  1997.

[5] P. Indyk and R. Motwani. Approximate nearest neighbors: Towards removing the curse of

dimensionality. In STOC  pages 604–613  1998.

[6] A. Broder  M. Charikar  A. Frieze  and M. Mitzenmacher. Min-wise independent permutations.

Journal of Computer and System Sciences  60:630–659  2000.

[7] M. Charikar. Similarity estimation techniques from rounding algorithms.

380–388  2002.

In STOC  pages

[8] M. Datar  N. Immorlica  P. Indyk  and V. Mirrokni. Locality-sensitive hashing scheme based

on p-stable distributions. In SoCG  pages 253–262  2004.

[9] A. Shrivastava and P. Li. Asymmetric LSH (ALSH) for sublinear time maximum inner product

search (MIPS). In NIPS  pages 2321–2329  2014.

[10] A. Andoni and P. Indyk. Efﬁcient algorithms for substring near neighbor problem. In SODA 

pages 1203–1212  2006.

[11] J. L. Bentley. Multidimensional binary search trees used for associative searching. Communi-

cations of the ACM  18(9):509–517  1975.

[12] A. Beygelzimer  S. Kakade  and J. Langford. Cover trees for nearest neighbor. In ICML  pages

97–104  2006.

[13] S. M. Omohundro. Bumptrees for efﬁcient function  constraint  and classiﬁcation learning. In

NIPS  volume 40  pages 175–179  1991.

[14] J. K. Uhlmann. Satisfying general proximity/similarity queries with metric trees. Information

processing letters  40:175–179  1991.

8

[15] S. Dasgupta and K. Sinha. Randomized partition trees for nearest neighbor search. Algorith-

mica  72:237–263  2015.

[16] W. Johnson and J. Lindenstrauss. Extensions of Lipschhitz maps into a Hilbert space. Con-

temporary Math  26:189–206  1984.

[17] J. H. Wells and L. R. Williams. Embeddings and extensions in analysis  volume 84. Springer-

Verlag  New York  1975.

[18] A. Vedaldi and A. Zisserman. Efﬁcient additive kernels via explicit feature maps. IEEE Trans.

Pattern Anal. Mach. Intell.  34:480–492  2012.

[19] N. Linial  E. London  and Y. Rabinovich. The geometry of graphs and some of its algorithmic

applications. In FOCS  pages 577–591  1994.

[20] I. Borg and P. Groenen. Modern multidimensional scaling: Theory and applications. Springer-

Verlag  Berlin  1997.

[21] T. Liu  A. Moore  A. Gray  and K. Yang. An investigation of practical approximate nearest

neighbor algorithms. In NIPS  pages 825–832  2004.

[22] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in

high dimensions. In Communications of the ACM  volume 51  pages 117–122  2008.

[23] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in

high dimensions. In FOCS  pages 459–468  2006.

[24] J. Kleinberg. Two algorithms for nearest-neighbor search in high dimensions. In STOC  pages

599–608  1997.

[25] N. Ailon and B. Chazelle. The fast Johnson-Lindenstrauss transform and approximate nearest

neighbors. SIAM Journal on Computing  39:302–322  2009.

[26] P. Li  G. Samorodnitsk  and J. Hopcroft. Sign cauchy projections and chi-square kernel. In

NIPS  pages 2571–2579  2013.

[27] S. Dasgupta and A. Gupta. An elementary proof of a theorem of Johnson and Lindenstrauss.

Random Structures & Algorithms  22:60–65  2003.

[28] D. Achlioptas. Database-friendly random projections. In Proceedings of the Symposium on

Principles of Database Systems  pages 274–281  2001.

[29] R. I. Arriaga and S. Vempala. An algorithmic theory of learning: Robust concepts and random

projection. In FOCS  pages 616–623  1999.

[30] M. Charikar and A. Sahai. Dimension reduction in the L1 norm. In FOCS  pages 551–560 

2002.

[31] P. Indyk. Stable distributions  pseudorandom generators  embeddings  and data stream compu-

tation. Journal of the ACM  53(3):307–323  2006.

[32] M. Rudelson and R. Vershynin. Sampling from large matrices: An approach through geometric

functional analysis. Journal of the ACM (JACM)  54(4):21  2007.

[33] https://archive.ics.uci.edu/ml/datasets/Corel+Image+Features.
[34] A. Gionis  P. Indyk  and R. Motwani. Similarity search in high dimensions via hashing. In

VLDB  volume 99  pages 518–529  1999.

[35] Ana Cardoso-Cachopo. Improving Methods for Single-label Text Categorization. PdD Thesis 
Instituto Superior Tecnico  Universidade Tecnica de Lisboa. Data avaliable at http://ana.
cachopo.org/datasets-for-single-label-text-categorization  2007.

[36] http://www.cs.columbia.edu/~blei/topicmodeling_software.html.
[37] J. Deng  W. Dong  R. Socher  L. J. Li  K. Li  and F. Li. Imagenet: A large-scale hierarchical

image database. In CVPR  pages 248–255. IEEE  2009.

[38] Y. Jia  E. Shelhamer  J. Donahue  S. Karayev  J. Long  R. Girshick  S. Guadarrama  and
T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093  2014.

[39] A. Andoni and P. Indyk. E2LSH 0.1 user manual. Technical report  2005.
[40] J. K. Uhlmann.

Implementing metric trees to satisfy general proximity/similarity queries.

Manuscript  1991.

9

,Yanshuai Cao
Marcus Brubaker
David Fleet
Aaron Hertzmann
Eran Treister
Javier Turek
Xinan Wang
Sanjoy Dasgupta
Bo Dai
Dahua Lin
Tao Sun
Yuejiao Sun
Dongsheng Li
Qing Liao