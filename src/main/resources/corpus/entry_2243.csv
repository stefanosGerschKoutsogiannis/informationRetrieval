2019,Generalization Bounds for Neural Networks via Approximate Description Length,We investigate the sample complexity of networks with bounds on the magnitude of its weights. 
In particular  we consider the class
\[
\cn = \left\{W_t\circ\rho\circ W_{t-1}\circ\rho\ldots\circ \rho\circ W_{1} : W_1 \ldots W_{t-1}\in M_{d\times d}  W_t\in M_{1 d}  \right\}
\]
where the spectral norm of each $W_i$ is bounded by $O(1)$  the Frobenius norm is bounded by $R$  and $\rho$ is the sigmoid function $\frac{e^x}{1 + e^x}$ or the smoothened ReLU function $ \ln\left(1 + e^x\right)$.
We show that for any depth $t$  if the inputs are in $[-1 1]^d$  the sample complexity of $\cn$ is $\tilde O\left(\frac{dR^2}{\epsilon^2}\right)$. This bound is optimal up to log-factors  and substantially improves over the previous state of the art of $\tilde O\left(\frac{d^2R^2}{\epsilon^2}\right)$  that was established in a recent line of work.

We furthermore show that this bound remains valid if instead of considering the magnitude of the $W_i$'s  we consider the magnitude of $W_i - W_i^0$  where $W_i^0$ are some reference matrices  with spectral norm of $O(1)$. By taking the $W_i^0$ to be the matrices in the onset of the training process  we get sample complexity bounds that are sub-linear in the number of parameters  in many {\em typical} regimes of parameters.  

To establish our results we develop a new technique to analyze the sample complexity of families $\ch$ of predictors. 
We start by defining a new notion of a randomized approximate description of functions $f:\cx\to\reals^d$. We then show that if there is a way to approximately describe functions in a class $\ch$ using $d$ bits  then $\frac{d}{\epsilon^2}$ examples suffices to guarantee uniform convergence. Namely  that the empirical loss of all the functions in the class is $\epsilon$-close to the true loss. Finally  we develop a set of tools for calculating the approximate description length of classes of functions that can be presented as a composition of linear function classes and non-linear functions.,Generalization Bounds for Neural Networks via

Approximate Description Length

Amit Daniely

Hebrew University and Google Research Tel-Aviv

amit.daniely@mail.huji.ac.il

Elad Granot

Hebrew University

elad.granot@mail.huji.ac.il

Abstract

(cid:17)

(cid:16) dR2

(cid:17)

2

(cid:16) d2R2

We investigate the sample complexity of networks with bounds on the magnitude
of its weights. In particular  we consider the class
N = {Wt ◦ ρ ◦ Wt−1 ◦ ρ . . . ◦ ρ ◦ W1 : W1  . . .   Wt−1 ∈ Md×d  Wt ∈ M1 d}
where the spectral norm of each Wi is bounded by O(1)  the Frobenius norm
is bounded by R  and ρ is the sigmoid function ex
1+ex or the smoothened ReLU
function ln (1 + ex). We show that for any depth t  if the inputs are in [−1  1]d 
the sample complexity of N is ˜O
. This bound is optimal up to log-factors 
and substantially improves over the previous state of the art of ˜O
established in a recent line of work [9  4  7  5  2  8].
We furthermore show that this bound remains valid if instead of considering the
magnitude of the Wi’s  we consider the magnitude of Wi − W 0
i are
some reference matrices  with spectral norm of O(1). By taking the W 0
i to be the
matrices at the onset of the training process  we get sample complexity bounds that
are sub-linear in the number of parameters  in many typical regimes of parameters.
To establish our results we develop a new technique to analyze the sample complex-
ity of families H of predictors. We start by deﬁning a new notion of a randomized
approximate description of functions f : X → Rd. We then show that if there is a
way to approximately describe functions in a class H using d bits  then d
2 examples
sufﬁces to guarantee uniform convergence. Namely  that the empirical loss of all
the functions in the class is -close to the true loss. Finally  we develop a set of
tools for calculating the approximate description length of classes of functions
that can be presented as a composition of linear function classes and non-linear
functions.

i   where W 0

  that was

2

1

Introduction

We analyze the sample complexity of networks with bounds on the magnitude of their weights. Let
us consider a prototypical case  where the input space is X = [−1  1]d  the output space is R  the
number of layers is t  all hidden layers has d neurons  and the activation function is ρ : R → R. The
class of functions computed by such an architecture is

N = {Wt ◦ ρ ◦ Wt−1 ◦ ρ . . . ◦ ρ ◦ W1 : W1  . . .   Wt−1 ∈ Md×d  Wt ∈ M1 d}

As the class N is deﬁned by (t − 1)d2 + d = O(d2) parameters  classical results (e.g. [1]) tell
us that order of d2 examples are sufﬁcient and necessary in order to learn a function from N (in a
standard worst case analysis). However  modern networks often succeed to learn with substantially
less examples. One way to provide alternative results  and a potential explanation to the phenomena 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

is to take into account the magnitude of the weights. This approach was a success story in the days
of SVM [3] and Boosting [10]  provided a nice explanation to generalization with sub-linear (in the
number of parameters) number of examples  and was even the deriving force behind algorithmic
progress. It seems just natural to adopt this approach in the context of modern networks. For instance 
it is natural to consider the class

NR = {Wt ◦ ρ ◦ Wt−1 ◦ ρ . . . ◦ ρ ◦ W1 : ∀i (cid:107)Wi(cid:107)F ≤ R (cid:107)Wi(cid:107) ≤ O(1)}

where (cid:107)W(cid:107) = max(cid:107)x(cid:107)=1 (cid:107)W x(cid:107) is the spectral norm and (cid:107)W(cid:107)F =
ij is the Frobenius
norm. This class has been analyzed in several recent works [9  4  7  5  2  8]. Best known results
show a sample complexity of ˜O
(for the sake of simplicity  in the introduction  we ignore the
dependence on the depth in the big-O notation). In this paper we prove  for various activations  a
stronger bound of ˜O

  which is optimal  up to log factors  for constant depth networks.

(cid:16) d2R2

(cid:16) dR2

i j=1 W 2

(cid:17)

(cid:17)

2

2

(cid:113)(cid:80)d

2

2

√

(cid:17)

(cid:17)

(cid:16) d2

(cid:16) dR2

bound  we get a sample complexity of ˜O

d. The Frobenius norm of such a matrix is of order
d. Going back to our ˜O

How good is this bound? Does it ﬁnally provide sub-linear bound in typical regimes of the parameters?
To answer this question  we need to ask how large R is. While this question of course don’t have a
deﬁnite answer  empirical studies (e.g. [12]) show that it is usually the case that the norm (spectral 
Frobenius  and others) of the weight matrices is at the same order of magnitude as the norm of the
matrix in the onset of the training process. In most standard training methods  the initial matrices
are random matrices with independent (or almost independent) entries  with mean zero and variance
d. Hence  the magnitude of R is of
of order 1
√
order
  which is
unfortunately still linear in the number of parameters.
Since our bound is almost optimal  we can ask whether this is the end of the story? Should we
abandon the aforementioned approach to network sample complexity? A more reﬁned examination of
the training process suggests another hope for this approach. Indeed  the training process doesn’t start
from the zero matrix  but rather form a random initialization matrix. Thus  it stands to reason that
instead of considering the magnitude of the weight matrices Wi  we should consider the magnitude
of Wi − W 0
is the initial weight matrix. Indeed  empirical studies [6] show that the
Frobenius norm of Wi − W 0
is often order of magnitude smaller than the Frobenius norm of Wi.
Following this perspective  it is natural to consider the class
NR(W 0
For some ﬁxed matrices  W 0
balls around the initial W 0
at hand. In other words  we can expect that the sample complexity of NR(W 0
approximately the sample complexity of NR. Namely  we expect a sample complexity of ˜O
Such a bound would ﬁnally be sub-linear  as in practice  it is often the case that R2 (cid:28) d.
This approach was pioneered by [4] who considered the class

t of spectral norm1 O(1). It is natural to expect that considering
1   . . .   W 0
i ’s instead of zero  shouldn’t change the sample complexity of the class
t ) should be
.

t ) =(cid:8)Wt ◦ ρ ◦ Wt−1 ◦ ρ . . . ◦ ρ ◦ W1 : (cid:107)Wi − W 0

i (cid:107) ≤ O(1) (cid:107)Wi − W 0

i   where W 0
i

1   . . .   W 0

1   . . .   W 0

i

1   . . .   W 0

N 2 1
R (W 0

where (cid:107)W(cid:107)2 1 = (cid:80)d
(cid:16) dR2

t ) =(cid:8)Wt ◦ ρ . . . ◦ ρ ◦ W1 : (cid:107)Wi − W 0
(cid:113)(cid:80)d
. Since  (cid:107)W(cid:107)2 1 ≤ √
(cid:16) dR2
(cid:17)

˜O
NR(W 0
complexity bound of ˜O

on NR(W 0

1   . . .   W 0

1   . . .   W 0

(cid:17)

t ).

2

2

i=1

ij. For this class they proved a sample complexity bound of
on
t )  which is still not sublinear2. In this paper we ﬁnally prove a sub-linear sample

j=1 W 2
d(cid:107)W(cid:107)F   this implies a sample complexity bound of ˜O

2

i (cid:107)F ≤ R(cid:9)
(cid:17)
(cid:16) dR2
i (cid:107)2 1 ≤ R(cid:9)
(cid:17)
(cid:16) d2R2

2

i (cid:107) ≤ O(1) (cid:107)Wi − W 0

To prove our results  we develop a new technique for bounding the sample complexity of function
classes. Roughly speaking  we deﬁne a notion of approximate description of a function  and count

1The bound of O(1) on the spectral norm of the W 0

neural networks – the spectral norm of W 0
show that the spectral norm of Wi − W 0
(cid:107)W(cid:107)F = Θ(1) (namely  each entry has variance 1

√
2We note that (cid:107)W(cid:107)2 1 = Θ(
d) even if W is a random matrix with variance that is calibrated so that

i is usually very small.

i is again motivated by the practice of
i   with standard initializations  is O(1)  and empirical studies [6  12]

i ’s and Wi − W 0

d2 ).

2

how many bits are required in order to give an approximate description for the functions in the class
under study. We then show that this number  called the approximate description length (ADL)  gives
an upper bound on the sample complexity. The advantage of our method over existing techniques is
that it behaves nicely with compositions. That is  once we know the approximate description length
of a class H of functions from X to Rd  we can also bound the ADL of ρ ◦ H  as well as L ◦ H 
where L is a class of linear functions. This allows us to utilize the compositional structure of neural
networks.

1  . . .   xk

d  . . .   xk

1)  . . .   med(x1

a∈A f (a). We denote Bd

d)(cid:1). We use log to denote

Rd we denote med(x1  . . .   xk) =(cid:0)med(x1
F =(cid:80)

2 Preliminaries
Notation We denote by med(x1  . . .   xk) the median of x1  . . .   xk ∈ R. For vectors x1  . . .   xk ∈
(cid:80)
log2  and ln to denote loge An expression of the form f (n) (cid:46) g(n) means that there is a universal
constant c > 0 for which f (n) ≤ cg(n). For a ﬁnite set A and f : A → R we let Ex∈A f =
Ex∈A f (a) = 1|A|
1. Likewise 
we denote Sd−1 = {x ∈ Rd : (cid:107)x(cid:107) = 1} We denote the Frobenius norm of a matrix W by
(cid:107)W(cid:107)2
ij  while the spectral norm is denoted by (cid:107)W(cid:107) = max(cid:107)x(cid:107)=1 (cid:107)W x(cid:107). For a pair of
vectors x  y ∈ Rd we denote by xy ∈ Rd their point-wise product xy = (x1y1  . . .   xdyd)
Uniform Convergence and Covering Numbers Fix an instance space X   a label space Y and a
loss (cid:96) : Rd × Y → [0 ∞). We say that (cid:96) is Lipschitz / Bounded / etc. if for any y ∈ Y  (cid:96)(·  y)
is. Fix a class H from X to Rd. For a distribution D and a sample S ∈ (X × Y)m we deﬁne the
representativeness of S as
repD(S H) = sup
h∈H

M = {x ∈ Rd : (cid:107)x(cid:107) ≤ M} and Bd = Bd

(cid:96)D(h)− (cid:96)S(h) for (cid:96)D(h) = E

(x y)∼D (cid:96)(h(x)  y) and (cid:96)S(h) =

m(cid:88)

(cid:96)(h(xi)  yi)

ij W 2

1
m

i=1

We note that if repD(S H) ≤  then any algorithm that is guaranteed to return a function ˆh ∈ H
will enjoy a generalization bound (cid:96)D(h) ≤ (cid:96)S(h) + . In particular  the ERM algorithm will return a
function whose loss is optimal  up to an additive factor of . We will focus on bounds on repD(S H)
when S ∼ Dm. To this end  we will rely on the connection between representativeness and the
covering numbers of H.
Deﬁnition 2.1. Fix a class H of functions from X to Rd  an integer m   > 0 and 1 ≤ p ≤ ∞. We
deﬁne Np(H  m  ) as the minimal integer for which the following holds. For every A ⊂ X of size

(cid:12)(cid:12)(cid:12) ˜H(cid:12)(cid:12)(cid:12) ≤ Np(H  m  ) and for any h ∈ H there is ˜h ∈ ˜H with

such that

≤ m there exists ˜H ⊂(cid:0)Rd(cid:1)X
(cid:13)(cid:13)(cid:13)h(x) − ˜h(x)
(cid:13)(cid:13)(cid:13)p
(cid:16)Ex∈A
(cid:17) 1

∞

p ≤ . For p = 2  we denote N (H  m  ) = N2(H  m  )

We conclude with a lemma  which will be useful in this paper. The proof can be found in the
supplementary material.
Lemma 2.2. Let (cid:96) : Rd × Y → R be L-Lipschitz w.r.t. (cid:107) · (cid:107)∞ and B-bounded. Assume that for any
√
0 <  ≤ 1  log (N (H  m  )) ≤ n
2 . Then ES∼Dm repD(S H) (cid:46) (L+B)
√
m log(m). Furthermore 
with probability at least 1 − δ  repD(S H) (cid:46) (L+B)
√
m log(m) + B

(cid:113) 2 ln(2/δ)

√

m

n

n

A Basic Inequality
Lemma 2.3. Let X1  . . .   Xn be independent r.v. with that that are σ-estimators to µ. Then

Pr (|med(X1  . . .   Xn) − µ| > kσ) <(cid:0) 2

(cid:1)n

k

3 Simpliﬁed Approximate Description Length

To give a soft introduction to our techniques  we ﬁrst consider a simpliﬁed version of it. We next
deﬁne the approximate description length of a class H of functions from X to Rd  which quantiﬁes
the number of bits it takes to approximately describe a function from H. We will use the following
notion of approximation

3

Deﬁnition 3.1. A random vector X ∈ Rd is a σ-estimator to x ∈ Rd if

E X = x and ∀u ∈ Sd−1  VAR((cid:104)u  X(cid:105)) = E(cid:104)u  X − x(cid:105)2 ≤ σ2

A random function ˆf : X → Rd is a σ-estimator to f : X → Rd if for any x ∈ X   ˆf (x) is a
σ-estimator to f (x).
A (σ  n)-compressor C for a class H takes as input a function h ∈ H  and outputs a (random) function
Ch such that (i) Ch is a σ-estimator of h and (ii) it takes n bits to describe Ch. Formally 
Deﬁnition 3.2. A (σ  n)-compressor for H is a triplet (C  Ω  µ) where µ is a probability measure on

Ω  and C is a function C : Ω × H →(cid:0)Rd(cid:1)X

such that

1. For any h ∈ H and x ∈ X   (Cωh)(x)  ω ∼ µ is a σ-estimator of h(x).

2. There are functions E : Ω × H → {±1}n and D : {±1}n →(cid:0)Rd(cid:1)X

for which C = D ◦ E
Deﬁnition 3.3. We say that a class H of functions from X to Rd has approximate description length
n if there exists a (1  n)-compressor for H
(cid:80)k
It is not hard to see that if (C  Ω  µ) is a (σ  n)-compressor for H  then
i=1(Cωih)(x)

(cid:17)

(cid:16) σ√
any 1 ≥  > 0 there exists an(cid:0)  n(cid:100)−2(cid:101)(cid:1)-compressor for H.

(Cω1 ... ωk h)(x) :=

is a

  kn

k

k

-compressor for H. Hence  if the approximate description length of H is n  then for

log (N (H  m  )) ≤ n(cid:6)−2(cid:7). Hence  if (cid:96) : Rd × Y → R is L-Lipschitz and B-bounded  then for any

We next connect the approximate description length  to covering numbers and representativeness. We
separate it into two lemmas  one for d = 1 and one for general d  as for d = 1 we can prove a slightly
stronger bound.
Lemma 3.4. Fix a class H of functions from X to R with approximate description length n. Then 
√
distribution D on X × Y  ES∼Dm repD(S H) (cid:46) (L+B)
√
m log(m). Furthermore  with probability
√
at least 1 − δ  repD(S H) (cid:46) (L+B)
√
Lemma 3.5. Fix a class H of functions from X to Rd with approximate description length n. Then 

log (N∞(H  m  )) ≤ log (N (H  m  )) ≤ n(cid:6)16−2(cid:7)(cid:100)log(dm)(cid:101)

(cid:113) 2 ln(2/δ)

m log(m) + B

Hence  if (cid:96) : Rd × Y → R is L-Lipschitz w.r.t. (cid:107) · (cid:107)∞ and B-bounded  then for any distribution D
on X × Y  ES∼Dm repD(S H) (cid:46) (L+B)
log(m). Furthermore  with probability at least
1 − δ  repD(S H) (cid:46) (L+B)

(cid:113) 2 ln(2/δ)

√
√
n log(dm)
m

√
√
n log(dm)
m

log(m) + B

m

m

n

n

3.1 Linear Functions

Theorem 3.6. Let class Ld1 d2 M = (cid:8)x ∈ Bd1 (cid:55)→ W x : W is d2 × d1 matrix with (cid:107)W(cid:107)F ≤ M(cid:9)

We next bound the approximate description length of linear functions with bounded Frobenius norm.

has approximate description length

(cid:25)

(cid:24) 1

4

n ≤

+ 2M 2

2(cid:100)log (2d1d2(M + 1))(cid:101)

Hence  if (cid:96) : Rd2 × Y → R is L-Lipschitz w.r.t. (cid:107) · (cid:107)∞ and B-bounded  then for any distribution D
on X × Y

repD(S Ld1 d2 M ) (cid:46) (L + B)(cid:112)M 2 log(d1d2M ) log(d2m)

log(m)

√

E

S∼Dm

Furthermore  with probability at least 1 − δ 

repD(S Ld1 d2 M ) (cid:46) (L + B)(cid:112)M 2 log(d1d2M ) log(d2m)

√

m

(cid:114)

log(m) + B

2 ln (2/δ)

m

m

4

We remark that the above bounds on the representativeness coincides with standard bounds ([11] for
instance)  up to log factors. The advantage of these bound is that they remain valid for any output
dimension d2.
In order to prove theorem 3.6 we will use a randomized sketch of a matrix.
Deﬁnition 3.7. Let w ∈ Rd be a vector. A random sketch of w is a random vector ˆw that is samples
as follows. Choose i w.p. pi = w2
let b = 1 and otherwise b = 0.
Finally  let ˆw =
ei. A random k-sketch of w is an average of k-independent random
sketches of w. A random sketch and a random k-sketch of a matrix is deﬁned similarly  with the
standard matrix basis instead of the standard vector basis.

−(cid:106) wi

2d . Then  w.p. wi
pi

(cid:16)(cid:106) wi

2(cid:107)w(cid:107)2 + 1

(cid:17)

(cid:107)

(cid:107)

+ b

pi

pi

i

(cid:113) 1
4 + 2(cid:107)w(cid:107)2-estimator of w.

4 + 2(cid:107)w(cid:107)2

sample a k-sketch ˆW of W for k =(cid:6) 1

The following useful lemma shows that an sketch w is a
Lemma 3.8. Let ˆw be a random sketch of w ∈ Rd. Then  (1) E ˆw = w and (2) for any u ∈ Sd−1 
E ((cid:104)u  ˆw(cid:105) − (cid:104)u  w(cid:105))2 ≤ E(cid:104)u  ˆw(cid:105)2 ≤ 1
Proof. (of theorem 3.6) We construct a compressor for Ld1 d2 M as follows. Given W   we will
that that W (cid:55)→ ˆW is a (1  2k (cid:100)log(2d1d2(M + 1))(cid:101))-compressor for Ld1 d2 M . Indeed  to specify a
sketch of W we need (cid:100)log(d1d2)(cid:101) bits to describe the chosen index  as well as log (2d1d2M + 2)
bits to describe the value in that index. Hence  2k (cid:100)log(2d1d2(M + 1))(cid:101) bits sufﬁces to specify a
k-sketch. It remains to show that for x ∈ Bd1  ˆW x is a 1-estimator of W x. Indeed  by lemma 3.8 
E ˆW = W and therefore E ˆW x = W x. Likewise  for u ∈ Sd2−1. We have

4 + 2M 2(cid:7)  and will return the function x (cid:55)→ ˆW x. We claim

(cid:69) − (cid:104)u  W x(cid:105)(cid:17)2

= E(cid:16)(cid:68) ˆW   xuT(cid:69) −(cid:10)W  xuT(cid:11)(cid:17)2 ≤ 1

4 + 2M 2

≤ 1

k

E(cid:16)(cid:68)

u  ˆW x

n=1 |an| = 1. For any W ∈ Md d we deﬁne hW (x) = 1√

(cid:9) In order to build compressors for classes of networks  we will

be Bd. We ﬁx an activation function ρ : R → R that is assumed to be a polynomial ρ(x) =(cid:80)k
with(cid:80)n
let H = (cid:8)hW : ∀i  (cid:107)wi(cid:107) ≤ 1

3.2 Simpliﬁed Depth 2 Networks
To demonstrate our techniques  we consider the following class of functions. We let the domain X to
(cid:80)d
i=0 aixi
i=1 ρ((cid:104)wi  x(cid:105)) Finally  we
utilize to compositional structure of the classes. Speciﬁcally  we have that H = Λ ◦ ρ ◦ F where
F = {x (cid:55)→ W x : W is d × d matrix with (cid:107)wi(cid:107) ≤ 1 for all i} and Λ(x) = 1√
As F is a subset of Ld d 
√
d  we know that there exists a (1  O (d log(d)))-compressor for it. We will
use this compressor to build a compressor to ρ ◦ F  and then to Λ ◦ ρ ◦ F. We will start with the
latter  linear case  which is simpler
Lemma 3.9. Let X be a σ-estimator to x ∈ Rd1. Let A ∈ Md2 d1 be a matrix of spectral norm
≤ r. Then  AX is a (rσ)-estimator to Ax. In particular  if C is a (1  n)-compressor to a class H of
functions from X to Rd. Then

(cid:80)d

i=1 xi.

d

d

2

C(cid:48)
ω(Λ ◦ h) = Λ ◦ Cωh

is a (1  n)-compressor to Λ ◦ H
We next consider the composition of F with the non-linear ρ. As opposed to composition with a linear
function  we cannot just generate a compression version using F’s compressor and then compose
with ρ. Indeed  if X is a σ-estimator to x  it is not true in general that ρ(X) is an estimator of ρ(x).
For instance  consider the case that ρ(x) = x2  and X = (X1  . . .   Xd) is a vector of independent
standard Gaussians. X is a 1-estimator of 0 ∈ Rd. On the other hand  ρ(X) = (X 2
n) is not
an estimator of 0 = ρ(0). We will therefore take a different approach. Given f ∈ F  we will sample
(cid:81)i
i=1 from F’s compressor  and deﬁne the compressed version of
k independent estimators {Cωif}k
σ ◦ h as C(cid:48)
j=0 Cωif. This construction is analyzed in the following lemma

f =(cid:80)d

1   . . .   X 2

i=0 ai

ω1 ... ωk

5

Lemma 3.10. If C is a(cid:0) 1

2   n(cid:1)-compressor of a class H of functions from X to(cid:2)− 1

a (1  n)-compressor of ρ ◦ H
Combining theorem 3.6 and lemmas 3.9  3.10 we have:
Theorem 3.11. H has approximation length (cid:46) d log(d). Hence  if (cid:96) : R × Y → R is L-Lipschitz
and B-bounded  then for any distribution D on X × Y

2

(cid:3)d. Then C(cid:48) is

2   1

repD(S H) (cid:46) (L + B)(cid:112)d log(d)

√

m

E

S∼Dm

Furthermore  with probability at least 1 − δ 

repD(S H) (cid:46) (L + B)(cid:112)d log(d)

√

m

log(m) + B

log(m)

(cid:114)

2 ln (2/δ)

m

Lemma 3.10 is implied by the following useful lemma:
Lemma 3.12.

1. If X is a σ-estimator of x then aX is a (|a|σ)-estimator of aX

2. Suppose that for n = 1  2  3  . . . Xn is a σn-estimator of xn ∈ Rd. Assume furthermore
n=1 Xn is a

that(cid:80)∞
n=1 xn and(cid:80)∞
σ(cid:48)-estimator of(cid:81)k

n=1 σn converge to x ∈ Rd and σ ∈ [0 ∞). Then (cid:80)∞
i=1 are independent σi-estimators of xi ∈ Rd. Then(cid:81)k
i=1 xi for σ(cid:48)2 =(cid:81)k

3. Suppose that {Xi}k

(cid:17) −(cid:81)k

i + (cid:107)xi(cid:107)2∞
σ2

i=1 (cid:107)xi(cid:107)2∞

σ-estimator of x

i=1 Xi is a

(cid:16)

i=1

We note that the bounds in the above lemma are all tight.

4 Approximation Description Length

In this section we reﬁne the deﬁnition of approximate description length that were given in section 3.
We start with the encoding of the compressed version of the functions. Instead of standard strings 
we will use what we call bracketed string. The reason for that often  in order to create a compressed
version of a function  we concatenate compressed versions of other functions. This results with
strings with a nested structure. For instance  consider the case that a function h is encoded by the
concatenation of h1 and h2. Furthermore  assume that h1 is encoded by the string 01  while h2 is
encoded by the concatenation of h3  h4 and h5 that are in turn encoded by the strings 101  0101 and
1110. The encoding of h will then be [[01][[101][0101][1110]]]. We note that in section 3 we could
avoid this issue since the length of the strings and the recursive structure were ﬁxed  and did not
depend on the function we try to compress. Formally  we deﬁne
Deﬁnition 4.1. A bracketed string is a rooted tree S  such that (i) the children of each edge are
ordered  (ii) there are no nodes with a singe child  and (iii) the leaves are labeled by {0  1}. The
length  len(S) of S is the number of its leaves.

Let S be a bracketed string. There is a linear order on its leaves that is deﬁned as follows. Fix a pair
of leaves  v1 and v2  and let u be their LCA. Let u1 (resp. u2) be the child of u that lie on the path to
v1 (resp. v2). We deﬁne v1 < v2 if u1 < u2 and v1 > v2 otherwise (note that necessarily u1 (cid:54)= u2).
Let v1  . . .   vn be the leaves of T   ordered according to the above order  and let b1  . . .   bn be the
corresponding bits. The string associated with T is s = b1 . . . bn. We denote by Sn the collection of
bracketed strings of length ≤ n  and by S = ∪∞
The following lemma shows that in log-scale  the number of bracketed strings of length ≤ n differ
from standard strings of length ≤ n by only a constant factor
Lemma 4.2. |Sn| ≤ 32n
We next revisit the deﬁnition of a compressor for a class H. The deﬁnition of compressor will now
have a third parameter  ns  in addition to σ and n. We will make three changes in the deﬁnition.
The ﬁrst  which is only for the sake of convenience  is that we will use bracketed strings rather than
standard strings. The second change  is that the length of the encoding string will be bounded only

n=1Sn the collection of all bracketed strings.

6

D : T ns × T →(cid:0)Rd(cid:1)X

and E(ω  h) encode a σ-estimator. Namely  there is a function D : Sns × S →(cid:0)Rd(cid:1)X

in expectation. The ﬁnal change is that the compressor can now output a seed. That is  given a
function h ∈ H that we want to compress  the compressor can generate both a non-random seed
Es(h) ∈ Sns and a random encoding E(ω  h) ∈ S with Eω∼µ len(E(ω  h)) ≤ n. Together  Es(h)
such that
D(Es(h)  E(ω  h))  ω ∼ µ is a σ-estimator of h. The advantage of using seeds is that it will
allow us to generate many independent estimators  at a lower cost. In the case that n (cid:28) ns  the
cost of generating k independent estimators of h ∈ H is ns + kn bits (in expectation) instead of
k(ns + n) bits. Indeed  we can encode k estimators by a single seed Es(h) and k independent
“regular" encodings E(ωk  h)  . . .   E(ωk  h). The formal deﬁnition is given next.
Deﬁnition 4.3. A (σ  ns  n)-compressor for H is a 5-tuple C = (Es  E  D  Ω  µ) where µ is a
probability measure on Ω  and Es  E  D are functions Es : H → T ns  E : Ω × H → T   and
such that for any h ∈ H and x ∈ X (1) D(Es(h)  E(ω  h))  ω ∼ µ is a
σ-estimator of h and (2) Eω∼µ len(E(ω  h)) ≤ n
We ﬁnally revisit the deﬁnition of approximate description length. We will add an additional
parameter  to accommodate the use of seeds. Likewise  the approximate description length will
now be a function of m – we will say that H has approximate description length (ns(m)  n(m)) if
there is a (1  ns(m)  n(m))-compressor for the restriction of H to any set A ⊂ X of size at most m.
Formally:
Deﬁnition 4.4. We say that a class H of functions from X to Rd has approximate description length
(ns(m)  n(m)) if for any set A ⊂ X of size ≤ m there exists a (1  ns(m)  n(m))-compressor for
H|A
It is not hard to see that if H has approximate description length (ns(m)  n(m))  then for any

1 ≥  > 0 and a set A ⊂ X of size ≤ m  there exists an(cid:0)  ns(m)  n(m)(cid:100)−2(cid:101)(cid:1)-compressor for H|A.

We next connect the approximate description length  to covering numbers and representativeness.
The proofs are similar the the proofs of lemmas 3.4 and 3.5.
Lemma 4.5. Fix a class H of functions from X to R with approximate description length
(ns(m)  n(m)). Then  log (N (H  m  )) (cid:46) ns(m) + n(m)
2 Hence  if (cid:96) : Rd × Y → R is L-Lipschitz
and B-bounded  then for any distribution D on X × Y
√

repD(S H) (cid:46) (L + B)(cid:112)ns(m) + n(m)

log(m)

E

S∼Dm

Furthermore  with probability at least 1 − δ 
√

repD(S H) (cid:46) (L + B)(cid:112)ns(m) + n(m)

m

(cid:114)

log(m) + B

2 ln (2/δ)

m

Lemma 4.6. Fix a class H of functions from X to Rd with approximate description length
(ns(m)  n(m)). Then  log (N (H  m  )) ≤ log (N∞(H  m  )) (cid:46) ns(m) + n(m) log(dm)
. Hence  if
(cid:96) : Rd × Y → R is L-Lipschitz w.r.t. (cid:107) · (cid:107)∞ and B-bounded  then for any distribution D on X × Y

2

repD(S H) (cid:46) (L + B)(cid:112)ns(m) + n(m) log(dm)

√

E

S∼Dm

Furthermore  with probability at least 1 − δ 
√

repD(S H) (cid:46) (L + B)(cid:112)ns(m) + n(m) log(dm)

m

log(m)

(cid:114)

log(m) + B

2 ln (2/δ)

m

m

m

s(m)  n1(m)) and (n2

We next analyze the behavior of the approximate description length under various operations
Lemma 4.7. Let H1 H2 be classes of functions from X to Rd with approximate description length
s(m)  n2(m)). Then H1 + H2 has approximate description length of
of (n1
(n1
s(m) + n2
Lemma 4.8. Let H be a class of functions from X to Rd with approximate description length
of (ns(m)  n(m)). Let A be d2 × d1 matrix. Then A ◦ H1 has approximate description length

s(m)  2n1(m) + 2n2(m))

(cid:0)ns(m) (cid:6)(cid:107)A(cid:107)2(cid:7) n(m)(cid:1)

7

Figure 1: The functions ln (1 + ex) and ex
1+ex

Deﬁnition 4.9. Denote by Ld1 d2 r R the class of all d2 × d1 matrices of spectral norm at most r and
Frobenius norm at most R.
Lemma 4.10. Let H be a class of functions from X to Rd1 with approximate description length
(ns(m)  n(m)). Assume furthermore that for any x ∈ X and h ∈ H we have that (cid:107)h(x)(cid:107) ≤ B. Then 
Ld1 d2 r R ◦ H has approximate description length

(cid:0)ns(m)  n(m)O(r2 + 1) + O(cid:0)(d1 + B2)(R2 + 1) log(Rd1d2 + 1)(cid:1)(cid:1)

Deﬁnition 4.11. A function f : R → R is B-strongly-bounded if for all n ≥ 1  (cid:107)f (n)(cid:107)∞ ≤ n!Bn.
Likewise  f is strongly-bounded if it is B-strongly-bounded for some B

We note that
Lemma 4.12. If f is B-strongly-bounded then f is analytic and its Taylor coefﬁcients around any
point are bounded by Bn

The following lemma gives an example to a strongly bounded sigmoid function  as well as a strongly
bounded smoothened version of the ReLU (see ﬁgure 1).
Lemma 4.13. The functions ln (1 + ex) and ex
Lemma 4.14. Let H be a class of functions from X to Rd with approximate description length of
(ns(m)  n(m)). Let ρ : R → R be B-strongly-bounded. Then  ρ ◦ H has approximate description
length of

(cid:0)ns(m) + O(cid:0)n(m)B2 log(md)(cid:1)   O(cid:0)n(m)B2 log(d)(cid:1)(cid:1)

1+ex are strongly-bounded

5 Sample Complexity of Neural Networks
Fix the instance space X to be the ball of radius
strongly-bounded activation ρ. Fix matrices W 0
class of depth-t networks
Nr R(W 0
We note that

t ) =(cid:8)Wt ◦ ρ ◦ Wt−1 ◦ ρ . . . ◦ ρ ◦ W1 : (cid:107)Wi − W 0

1   . . .   W 0

√

d in Rd (in particular [−1  1]d ⊂ X ) and a B-
i ∈ Mdi di−1   i = 1  . . .   t. Consider the following

i (cid:107) ≤ r (cid:107)Wi − W 0

i (cid:107)F ≤ R(cid:9)

1   . . .   W 0

t ) = Nr R(W 0

Nr R(W 0
The following lemma analyzes the cost  in terms of approximate description length  when moving
from a class H to Nr R(W 0) ◦ H.
Lemma 5.1. Let H be a class of functions from X to Rd1 with approximate description length
(ns(m)  n(m)) and (cid:107)h(x)(cid:107) ≤ M for any x ∈ X and h ∈ H. Fix W 0 ∈ Md2 d1. Then  Nr R(W 0
t ) ◦
H has approximate description length of

t ) ◦ . . . ◦ Nr R(W 0
1 )

n(cid:48)(m) = n(m)O(r2 + (cid:107)W 0(cid:107)2 + 1) + O(cid:0)(d1 + M 2)(R2 + 1) log(Rd1d2 + 1)(cid:1)

(cid:0)ns(m) + n(cid:48)(m)B2 log(md2)  n(cid:48)(m)B2 log(d2)(cid:1)
(cid:17)
(cid:16)√

The lemma is follows by combining lemmas 4.7  4.8  4.10 and 4.14. We note that in the case that
√
d1  d2 ≤ d  M = O(
) and R ≥ 1 we get that
Nr R(W 0) ◦ H has approximate description length of

(cid:0)ns(m) + O (n(m) log(md))   O (n(m) log(d)) + O(cid:0)d1R2 log2(d)(cid:1)(cid:1)

d1)  B  r (cid:107)W 0(cid:107) = O(1) (and hence R = O

for

d

By induction  the approximate description length of Nr R(W 0

(cid:16)

dR2O (log(d))t log(md)  dR2O (log(d))t+1(cid:17)

1   . . .   W 0

t ) is

8

References
[1] Martin Anthony and Peter Bartlet. Neural Network Learning: Theoretical Foundations. Cam-

bridge University Press  1999.

[2] Sanjeev Arora  Rong Ge  Behnam Neyshabur  and Yi Zhang. Stronger generalization bounds

for deep nets via a compression approach. In ICML  2018.

[3] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3:463–482  2002.

[4] Peter L Bartlett  Dylan J Foster  and Matus J Telgarsky. Spectrally-normalized margin bounds
for neural networks. In Advances in Neural Information Processing Systems  pages 6240–6249 
2017.

[5] Noah Golowich  Alexander Rakhlin  and Ohad Shamir. Size-independent sample complexity of

neural networks. In COLT  2018.

[6] Vaishnavh Nagarajan and J Zico Kolter. Generalization in deep networks: The role of distance

from initialization. arXiv preprint arXiv:1901.01672  2019.

[7] Behnam Neyshabur  Srinadh Bhojanapalli  and Nathan Srebro. A pac-bayesian approach to

spectrally-normalized margin bounds for neural networks. In ICLR  2018.

[8] Behnam Neyshabur  Zhiyuan Li  Srinadh Bhojanapalli  Yann LeCun  and Nathan Srebro. The

role of over-parametrization in generalization of neural networks. In ICLR  2019.

[9] Behnam Neyshabur  Ryota Tomioka  and Nathan Srebro. Norm-based capacity control in neural

networks. In Conference on Learning Theory  pages 1376–1401  2015.

[10] R.E. Schapire  Y. Freund  P. Bartlett  and W.S. Lee. Boosting the margin: A new explanation
for the effectiveness of voting methods. In Machine Learning: Proceedings of the Fourteenth
International Conference  pages 322–330  1997. To appear  The Annals of Statistics.

[11] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge university press  2014.

[12] Ilya Sutskever  James Martens  George Dahl  and Geoffrey Hinton. On the importance of
initialization and momentum in deep learning. In International conference on machine learning 
pages 1139–1147  2013.

9

,Amit Daniely
Elad Granot