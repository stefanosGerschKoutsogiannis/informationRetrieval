2010,Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors,We introduce a new Bayesian nonparametric approach to identification of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint  as defined by the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a definite advantage over a group LAR algorithm and state-of-the-art parametric identification techniques based on prediction error minimization.,Learning sparse dynamic linear systems using

stable spline kernels and exponential hyperpriors

Alessandro Chiuso

University of Padova

Vicenza  Italy

Gianluigi Pillonetto∗

University of Padova

Padova  Italy

Department of Management and Engineering

Department of Information Engineering

alessandro.chiuso@unipd.it

giapi@dei.unipd.it

Abstract

We introduce a new Bayesian nonparametric approach to identiﬁcation of sparse
dynamic linear systems. The impulse responses are modeled as Gaussian pro-
cesses whose autocovariances encode the BIBO stability constraint  as deﬁned by
the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by
placing exponential hyperpriors on the scale factors of such kernels. Numerical
experiments regarding estimation of ARMAX models show that this technique
provides a deﬁnite advantage over a group LAR algorithm and state-of-the-art
parametric identiﬁcation techniques based on prediction error minimization.

1

Introduction

Black-box identiﬁcation approaches are widely used to learn dynamic models from a ﬁnite set of
input/output data [1]. In particular  in this paper we focus on the identiﬁcation of large scale linear
systems that involve a wide amount of variables and ﬁnd important applications in many different
domains such as chemical engineering  economic systems and computer vision [2]. In this scenario
a key point is that the identiﬁcation procedure should be sparsity-favouring  i.e. able to extract from
the large number of subsystems entering the system description just that subset which inﬂuences
signiﬁcantly the system output. Such sparsity principle permeates many well known techniques
in machine learning and signal processing such as feature selection  selective shrinkage and com-
pressed sensing [3  4].
In the classical identiﬁcation scenario  Prediction Error Methods (PEM) represent the most used
approaches to optimal prediction of discrete-time systems [1]. The statistical properties of PEM
(and Maximum Likelihood) methods are well understood when the model structure is assumed to be
known. However  in real applications  ﬁrst a set of competitive parametric models has to be postu-
lated. Then  a key point is the selection of the most adequate model structure  usually performed by
AIC and BIC criteria [5  6]. Not surprisingly  the resulting prediction performance  when tested on
experimental data  may be distant from that predicted by “standard” (i.e. without model selection)
statistical theory  which suggests that PEM should be asymptotically efﬁcient for Gaussian innova-
tions. If this drawback may affect standard identiﬁcation problems  a fortiori it renders difﬁcult the
study of large scale systems where the elevated number of parameters  as compared to the number
of data available  may undermine the applicability of the theory underlying e.g. AIC and BIC.
Some novel estimation techniques inducing sparse models have been recently proposed. They in-
clude the well known Lasso [7] and Least Angle Regression (LAR) [8] where variable selection is
performed exploiting the (cid:96)1 norm. This type of penalty term encodes the so called bi-separation
∗This research has been partially supported by the PRIN Project “Sviluppo di nuovi metodi e algoritmi
per l’identiﬁcazione  la stima Bayesiana e il controllo adattativo e distribuito”  by the Progetto di Ateneo
CPDA090135/09 funded by the University of Padova and by the European Community’s Seventh Framework
Programme under agreement n. FP7-ICT-223866-FeedNetBack.

1

feature  i.e. it favors solutions with many zero entries at the expense of few large components. Con-
sistency properties of this method are discussed e.g.
in [9  10]. Extensions of this procedure for
group selection include Group Lasso and Group LAR (GLAR) [11] where the sum of the Euclidean
norms of each group (in place of the absolute value of the single components) is used. Theoreti-
cal analyses of these approaches and connections with the multiple kernel learning problem can be
found in [12  13]. However  most of the work has been done in the “static” scenario while very little 
with some exception [14  15]  can be found regarding the identiﬁcation of dynamic systems.
In this paper we adopt a Bayesian point of view to prediction and identiﬁcation of sparse linear sys-
tems. Our starting point is the new identiﬁcation paradigm developed in [16] that relies on nonpara-
metric estimation of impulse responses (see also [17] for extensions to predictor estimation). Rather
than postulating ﬁnite-dimensional structures for the system transfer function  e.g. ARX  ARMAX
or Laguerre [1]  the system impulse response is searched for within an inﬁnite-dimensional space.
The intrinsical ill-posed nature of the problem is circumvented using Bayesian regularization meth-
ods. In particular  working under the framework of Gaussian regression [18]  in [16] the system
impulse response is modeled as a Gaussian process whose autocovariance is the so called stable
spline kernel that includes the BIBO stability constraint.
In this paper  we extend this nonparametric paradigm to the design of optimal linear predictors for
sparse systems. Without loss of generality  analysis is restricted to MISO systems so that we inter-
pret the predictor as a system with m + 1 inputs (given by past outputs and inputs) and one output
(output predictions). Thus  predictor design amounts to estimating m + 1 impulse responses mod-
eled as realizations of Gaussian processes. We set their autocovariances to stable spline kernels with
different (and unknown) scale factors which are assigned exponential hyperpriors having a common
hypervariance. In this way  while GLAR uses the sum of the (cid:96)1 norms of the single impulse re-
sponses  our approach favors sparsity through an (cid:96)1 penalty on kernel hyperparameters. Inducing
sparsity by hyperpriors is an important feature of our approach. In fact  this permits to obtain the
marginal posterior of the hyperparameters in closed form and hence also their estimates in a robust
way. Once the kernels are selected  the impulse responses are obtained by a convex Tikhonov-type
variational problem. Numerical experiments involving sparse ARMAX systems show that this ap-
proach provides a deﬁnite advantage over both GLAR and PEM (equipped with AIC or BIC) in
terms of predictive capability on new output data.
The paper is organized as follows. In Section 2  the nonparametric approach to system identiﬁcation
introduced in [16] is brieﬂy reviewed. Section 3 reports the statement of the predictor estimation
problem while Section 4 describes the new Bayesian model for system identiﬁcation of sparse linear
systems. In Section 5  a numerical algorithm which returns the unknown components of the prior
and the estimates of predictor and system impulse responses is derived. In Section 6 we use simu-
lated data to demonstrate the effectiveness of the proposed approach. Conclusions end the paper.

2 Preliminaries: kernels for system identiﬁcation

2.1 Kernel-based regularization
A widely used approach to reconstruct a function from indirect measurements {yt} consists of min-
imizing a regularization functional in a reproducing kernel Hilbert space (RKHS) H associated with
a symmetric and positive-deﬁnite kernel K [19]. Given N data points  least-squares regularization
in H estimates the unknown function as

N(cid:88)

t=1

ˆh = arg min
h

(yt − Γt[h])2 + η(cid:107)h(cid:107)2H

(1)

where {Γt} are linear and bounded functionals on H related to the measurement model while the
positive scalar η trades off empirical error and solution smoothness [20].
Under the stated assumptions and according to the representer theorem [21]  the minimizer of (1)
is the sum of N basis functions deﬁned by the kernel ﬁltered by the operators {Γt}  with coefﬁ-
cients obtainable solving a linear system of equations. Such solution enjoys also an interpretation
in Bayesian terms. It corresponds to the minimum variance estimate of f when f is a zero-mean
Gaussian process with autocovariance K and {yt − Γt[f ]} is white Gaussian noise independent of
f [22]. Often  prior knowledge is limited to the fact that the signal  and possibly some of its deriva-
tives  are continuous with bounded energy. In this case  f is often modeled as the p-fold integral of

2

Figure 1: Realizations of a stochastic process f with autocovariance proportional to the standard
Cubic Spline kernel (left)  the new Stable Spline kernel (middle) and its sampled version enriched
by a parametric component deﬁned by the poles −0.5 ± 0.6

√−1 (right).

white noise. If the white noise has unit intensity  the autocorrelation of f is Wp where

(cid:26) u if u ≥ 0

0

if u < 0

(2)

(cid:90) 1

0

Wp(s  t) =

Gp(s  u)Gp(t  u)du 

Gp(r  u) =

 

(u)+ =

(r − u)p−1
(p − 1)!

+

This is the autocovariance associated with the Bayesian interpretation of p-th order smoothing
splines [23]. In particular  when p = 2  one obtains the cubic spline kernel.

2.2 Kernels for system identiﬁcation

In the system identiﬁcation scenario  the main drawback of the kernel (2) is that it does not account
for impulse response stability. In fact  the variance of f increases over time. This can be easily
appreciated by looking at Fig. 1 (left) which displays 100 realizations drawn from a zero-mean
Gaussian process with autocovariance proportional to W2. One of the key contributions of [16] is
the deﬁnition of a kernel speciﬁcally suited to linear system identiﬁcation leading to an estimator
with favorable bias and variance properties. In particular  it is easy to see that if the autocovariance
of f is proportional to Wp  the variance of f (t) is zero at t = 0 and tends to ∞ as t increases.
However  if f represents a stable impulse response  we would rather let it have a ﬁnite variance at
t = 0 which goes exponentially to zero as t tends to ∞. This property can be ensured by considering
autocovariances proportional to the class of kernels given by

Kp(s  t) = Wp(e−βs  e−βt) 

(3)
where β is a positive scalar governing the decay rate of the variance [16]. In practice  β will be
unknown so that it is convenient to treat it as a hyperparameter to be estimated from data.
In view of (3)  if p = 2 the autocovariance becomes the Stable Spline kernel introduced in [16]:

s  t ∈ R+

K2(t  τ ) =

e−β(t+τ )e−β max(t τ )

2

− e−3β max(t τ )

6

(4)

Proposition 1 [16] Let f be zero-mean Gaussian with autocovariance K2. Then  with probability
one  the realizations of f are continuous impulse responses of BIBO stable dynamic systems.

The effect of the stability constraint is visible in Fig. 1 (middle) which displays 100 realizations
drawn from a zero-mean Gaussian process with autocovariance proportional to K2 with β = 0.4.

3 Statement of the system identiﬁcation problem
In what follows  vectors are column vectors  unless other is speciﬁed. We denote with {yt}t∈Z 
yt ∈ R and {ut}t∈Z  ut ∈ Rm a pair of jointly stationary stochastic processes which represent 

3

Figure 2: Bayesian network describing the new nonparametric model for identiﬁcation of sparse
linear systems where yl := [yl−1  yl−2  . . .] and  in the reduced model  λ := λ1 = . . . = λm+1.

respectively  the output and input of an unknown time-invariant dynamical system. With some
abuse of notation  yt will both denote a random variable (from the random process {yt}t∈Z) and its
sample value. The same holds for ut. Our aim is to identify a linear dynamical system of the form

∞(cid:88)

∞(cid:88)

yt =

fiut−i +

giet−i

(5)

i=1

i=0

(cid:2)(cid:80)∞

yt =(cid:80)m

In (5)  fi ∈ R1×m and gi ∈ R are matrix and scalar coefﬁcients of the

from {ut  yt}t=1 .. N .
unknown system impulse responses while et is the Gaussian innovation sequence.
Following the Prediction Error Minimization framework  identiﬁcation of the dynamical system (5)
t }t∈N denote
is converted in estimation of the associated one-step-ahead predictor. Letting hk := {hk
the predictor impulse response associated with the k-th input {uk
i=1 hm+1

(6)
where hm+1 := {hm+1
}t∈N is the impulse response modeling the autoregressive component of the
predictor. As is well known  if the joint spectrum of {yt} and {ut} is bounded away from zero  each
hk is (BIBO) stable. Under such assumption  our aim is to estimate the predictor impulse responses 
in a scenario where the number of measurements N is not large  as compared with m  and many
measured inputs could be irrelevant for the prediction of yt. We will focus on the identiﬁcation of
ARMAX models  so that the zeta-transforms of {hk} are rational functions all sharing the same
denominator  even if the approach described below immediately extends to general linear systems.

(cid:3) +(cid:80)∞

t }t∈Z  one has

yt−i + et

i

k=1

i=1 hk

i uk

t−i

t

4 A Bayesian model for identiﬁcation of sparse linear systems

4.1 Prior for predictor impulse responses
We model {hk} as independent Gaussian processes whose kernels share the same hyperparameters
apart from the scale factors. In particular  each hk is proportional to the convolution of a zero-
mean Gaussian process  with autocovariance given by the sampled version of K2  with a parametric
impulse response r  used to capture dynamics hardly represented by a smooth process  e.g. high-
frequency oscillations. For instance  the zeta-transform R(z) of r can be parametrized as follows

R(z) =

z2

Pθ(z)

 

Pθ(z) = z2 + θ1z + θ2 

θ ∈ Θ ⊂ R2

(7)

where the feasible region Θ constraints the two roots of Pθ(z) to belong to the open left unit
semicircle in the complex plane. To better appreciate the role of the ﬁnite-dimensional compo-
nent of the model  Fig. 1 (right panel) shows some realizations (with samples linearly interpolated)
drawn from a discrete-time zero-mean normal process with autocovariance given by K2 enriched by
θ = [1 0.61] in (7). Notice that  in this way  an oscillatory behavior is introduced in the realizations

4

by enriching the Stable Spline kernel with the poles −0.5 ± 0.6
The kernel of hk deﬁned by K2 and (7) is denoted by K : N × N (cid:55)→ R and depends on β  θ. Thus 
letting E[·] denote the expectation operator  the prior model on the impulse responses is given by

√−1.

E[hk

j hk

i ] = λ2

kK(j  i; θ  β) 

k = 1  . . .   m + 1 

i  j ∈ N

4.2 Hyperprior for the hyperparameters

The noise variance σ2 will always be estimated via a preliminary step using a low-bias ARX model 
as described in [24]. Thus  this parameter will be assumed known in the description of our Bayesian
model. The hyperparameters β  θ and {λk} are instead modeled as mutually independent random
vectors. β is given a non informative probability density on R+ while θ has a uniform distribution
on Θ. Each λk is an exponential random variable with inverse of the mean (and SD) γ ∈ R+  i.e.

p(λk) = γ exp (−γλk) χ(λk ≥ 0) 

k = 1  . . .   m + 1

with χ the indicator function. We also interpret γ as a random variable with a non informative prior
on R+. Finally  ζ indicates the hyperparameter random vector  i.e. ζ := [λ1  . . .   λm+1  θ1  θ2  β  γ].

4.3 The full Bayesian model
Let Ak ∈ RN×∞ where  for j = 1  . . .   N and i ∈ N  we have:

[Ak]ji = uk

(8)
In view of (6)  using notation of ordinary algebra to handle inﬁnite-dimensional objects with each
hk interpreted as an inﬁnite-dimensional column vector  it holds that

[Am+1]ji = yj−i

for k = 1  . . .   m 

j−i

m(cid:88)

y+ =

Ak(uk)hk + Am+1(y+  y-)hm+1 + e

(9)

k=1

y- = [y0  y−1  y−2  . . .]T  

y+ = [y1  y2  . . .   yN ]T  

where

e = [e1  e2  . . .   eN ]T (10)
In practice  y- is never completely known and a solution is to set its unknown components to zero 
see e.g. Section 3.2 in [1]. Further  the following approximation is exploited:
p(y+ {hk}  y-|ζ) ≈ p(y+|{hk}  y-  ζ)p({hk}|ζ)p(y-)

(11)
the past y- is assumed not to carry information on the predictor impulse responses and the

i.e.
hyperparameters. Our stochastic model is described by the Bayesian network in Fig. 2 (left side).
The dependence on y- is hereafter omitted as well as dependence of the {Ak} on y+ or uk. We
start reporting a preliminary lemma  whose proof can be found in [17]  which will be needed in
propositions 2 and 3.
Lemma 1 Let the roots of Pθ in (7) be stable. Then  if {yt} and {ut} are zero mean  ﬁnite variance
stationary stochastic processes  each operator {Ak} is almost surely (a.s.) continuous in HK.

5 Estimation of the hyper-parameters and the predictor impulse responses

5.1 Estimation of the hyper-parameters

We estimate the hyperparameter vector ζ by optimizing its marginal posterior  i.e. the joint density
of y+  ζ and {hk} where all the {hk} are integrated out. This is described in the next proposition
that derives from simple manipulations of probability densities whose well-posedness is guaranteed
by lemma 1. Below  IN is the N × N identity matrix while  with a slight abuse of notation  K is
now seen as an element of R∞×∞  i.e. its i-th column is the sequence K(·  i)  i ∈ N.
Proposition 2 Let {yt} and {ut} be zero mean  ﬁnite variance stationary stochastic processes.
Then  under the approximation (11)  the maximum a posteriori estimate of ζ given y+ is

ˆζ = arg min

ζ

J(y+; ζ)

s.t.

θ ∈ Θ 

γ  β > 0 

λk ≥ 0

(k = 1  . . .   m + 1)

(12)

5

m+1(cid:88)

where J is almost surely well deﬁned pointwise and given by

log(cid:0)det[2πV [y+]](cid:1) +

1
2

1
2

λk − log(γ)

J(y+; ζ) =

(y+)T (V [y+])−1y+ + γ

with V [y+] = σ2IN +(cid:80)m+1
nected with multiple kernel learning  see Section 3 in [25]. Additional terms are log(cid:0)det[V [y+]](cid:1)

The objective (13)  including the (cid:96)1 penalty on {λk}  is a Bayesian modiﬁed version of that con-

k .
k=1 λkAkKAT

(13)

k=1

and log(γ) that permits to estimate the weight of the (cid:96)1 norm jointly with the other hyperparameters.
An important issue for the practical use of our numerical scheme is the availability of a good start-
ing point for the optimizer. Below  we describe a scheme that achieves a suboptimal solution just
solving an optimization problem in R4 related to the reduced Bayesian model of Fig. 2 (right side).

(cid:34)

(cid:35)

m+1(cid:88)

k=1

i) Obtain {ˆλk}  ˆθ and ˆβ solving the following modiﬁed version of problem (12)

arg min

ζ

J(y+; ζ) − γ

λk + log(γ)

s.t. θ ∈ Θ 

β > 0 

λ1 = . . . = λm+1 ≥ 0

ii) Set ˆγ = 1/ˆλ1 and ˆζ = [ˆλ1  . . .   ˆλm+1  ˆθ  ˆβ  ˆγ]. Then  for k = 1  . . .   m + 1: set ¯ζ = ˆζ

except for the k-th component of ¯ζ which is set to 0; if J(y+; ¯ζ) ≤ J(y+; ˆζ)  set ˆζ = ¯ζ.

5.2 Estimation of the predictor impulse responses for known ζ
Let HK be the RKHS associated with K  with norm (cid:107) · (cid:107)HK . Let also ˆhk = E[hk|y+  ζ]. The
following result comes from the representer theorem whose applicability is guaranteed by lemma 1.

{ˆhk}m+1

m+1(cid:88)
Proposition 3 Under the same assumptions of Proposition 2  almost surely we have
(cid:107)f k(cid:107)2HK
(cid:33)−1

(cid:107)y+ − m+1(cid:88)
(cid:32)

where (cid:107) · (cid:107) is the Euclidean norm. Moreover  almost surely we also have for k = 1  . . .   m + 1

Akf k(cid:107)2 + σ2

{f k∈HK}m+1

k=1 = arg

min

λ2
k

k=1

k=1

k=1

m+1(cid:88)

ˆhk = λ2

kKAT

k c 

c =

σ2IN +

λkAkKAT
k

y+

(14)

After obtaining the estimates of the {hk}  simple formulas can then be used to derive the system
impulse responses f and g in (5) and hence also the k-step ahead predictors  see [1] for details.

k=1

6 Numerical experiments

We consider two Monte Carlo studies of 200 runs where at any run an ARMAX linear system with
15 inputs is generated as follows

• the number of hk different from zero is randomly drawn from the set {0  1  2  ..  8}.
• Then  the order of the ARMAX model is randomly chosen in [1  30] and the model is
generated by the MATLAB function drmodel.m. The system and the predictor poles are
restricted to have modulus less than 0.95 with the (cid:96)2 norm of each hk bounded by 10.

In the ﬁrst Monte Carlo experiment  at any run an identiﬁcation data set of size 500 and a test set
of size 1000 is generated using independent realizations of white noise as input.
In the second
experiment  the prediction on new data is more challenging. In fact  at any run  an identiﬁcation
data set of size 500 and a test set of size 1000 is generated via the MATLAB function idinput.m
using  respectively  independent realizations of a random Gaussian signal with band [0  0.8] and
[0  0.9] (the interval boundaries specify the lower and upper limits of the passband  expressed as
fractions of the Nyquist frequency). We compare the following estimators:

6

Figure 3: Boxplots of the values of COD1 obtained by PEM+Or  Stable Spline  GLAR and
PEM+BIC in the two experiments. The outliers obtained by PEM+BIC are not all displayed.

Experiment

#1
#2

PEM+Oracle

100%
100%

Stable Spline

98.7%
98.4%

Subopt. Stable Spline GLAR
45.6%
52.4%

97.5%
98.2%

Table 1: Percentage of the hk equal to zero correctly set to zero by the employed estimator.

1. GLAR: this is the GLAR algorithm described in [11] applied to ARX models; the order
(between 1 and 30) and the level of sparsity (i.e. the number of null hk) is determined using
the ﬁrst 2/3 of the 500 available data as training set and the remaining part as validation
data (the use of Cp statistics does not provide better results in this case).

2. PEM+Oracle: this is the classical PEM approach  as implemented in the pem.m function
of the MATLAB System Identiﬁcation Toolbox [26]  equipped with an oracle that  at every
run  knows which predictor impulse response are zero and  having access to the test set 
selects those model orders that provide the best prediction performance.

3. PEM+BIC: this is the classical PEM approach that uses BIC for model order selection. The
order of the polynomials in the ARMAX model are not allowed to be different each other
since this would lead to a combinatorial explosion of the number of competitive models.
4. Stable Spline: this is the approach based on the full Bayesian model of Fig. 2. The ﬁrst
40 available input/output pairs enter the {Ak} in (9) so that N = 460. For computational
reasons  the number of estimated predictor coefﬁcients is 40.

5. Suboptimal Stable Spline: the same as above except that we exploit the reduced Bayesian
model of Fig. 2 complemented with the procedure described at the end of subsection 5.1.

The following performance indexes are considered:

1. Percentage of the impulse responses equal to zero correctly set to zero by the estimator.
2. k-step-ahead Coefﬁcient of Determination  denoted by CODk  quantifying how much of

the test set variance is explained by the forecast. It is computed at each run as

CODk := 1−

1

1000

(cid:80)1000

RM S2
k
i=1 (ytest

t − ¯ytest

t

 

)2

RM Sk :=

(cid:118)(cid:117)(cid:117)(cid:116) 1

1000

1000(cid:88)

t − ˆytest
(ytest

t|t−k)2

t=1

(15)

7

PEM+OrStable SplineGLARPEM+BIC−1.5−1−0.500.51COD1#1PEM+OrStable SplineGLARPEM+BIC−2−1.5−1−0.500.51COD1#2Figure 4: CODk  i.e. average coefﬁcient of determination relative to k-step ahead prediction  ob-
tained during the Monte Carlo study #1 (top) and #2 (bottom) using PEM+Oracle (•)  GLAR (∗)
Stable Spline based on the full (◦) and the reduced (+) Bayesian model of Fig. 2.

where ¯ytest is the sample mean of the test set data {ytest
t|t−k is the k-step
ahead prediction computed using the estimated model. The average index obtained during
the Monte Carlo study  as a function of k  is then denoted by CODk.

}1000
t=1 and ˆytest

t

Notice that  in both of the cases  the larger the index  the better is the performance of the estimator.
In every experiment the performance of PEM+BIC has been largely unsatisfactory  providing
strongly negative values for CODk. This is illustrated e.g. in Fig. 3 showing the boxplots of the
200 values of COD1 obtained by 4 of the employed estimators during the two Monte Carlo studies.
We have also assessed that results do not improve using AIC. In view of this  in what follows other
results from PEM+BIC will not be shown.
Table 1 reports the percentage of the predictor impulse responses equal to zero correctly estimated as
zero by the estimators. Remarkably  in all the cases the Stable Spline estimators not only outperform
GLAR but the achieved percentage is close to 99%. This shows that the use of the marginal posterior
permits to effectively detect the subset of the {λk} equal to zero. Finally  Fig. 4 displays CODk as a
function of the prediction horizon obtained during the Monte Carlo study #1 (top) and #2 (bottom).
The performance of Stable Spline appears superior than that of GLAR and is comparable with that
of PEM+Oracle also when the reduced Bayesian model of Fig. 2 is used.

7 Conclusions

We have shown how identiﬁcation of large sparse dynamic systems can beneﬁt from the ﬂexibility
of kernel methods. To this aim  we have extended a recently proposed nonparametric paradigm to
identify sparse models via prediction error minimization. Predictor impulse responses are modeled
as zero-mean Gaussian processes using stable spline kernels encoding the BIBO-stability constraint
and sparsity is induced by exponential hyperpriors on their scale factors. The method compares
much favorably with GLAR  with its performance close to that achievable combining PEM with an
oracle which exploits the test set in order to select the best model order. In the near future we plan to
provide a theoretical analysis characterizing the hyperprior-based scheme as well as to design new
ad hoc optimization schemes for hyperparameters estimation.

8

12345678910111213141516171819200.60.70.80.91Average COD  #11234567891011121314151617181920−0.500.51kAverage COD  #2Stable SplineSuboptimal Stable Spline PEM + OracleGLARReferences
[1] L. Ljung. System Identiﬁcation - Theory For the User. Prentice Hall  1999.
[2] J. Mohammadpour and K.M. Grigoriadis. Efﬁcient Modeling and Control of Large-scale Systems.

Springer  2010.

[3] T. J. Hastie and R. J. Tibshirani. Generalized additive models. In Monographs on Statistics and Applied

Probability  volume 43. Chapman and Hall  London  UK  1990.

[4] D. Donoho. Compressed sensing. IEEE Trans. on Information Theory  52(4):1289–1306  2006.
[5] H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic Control 

19:716–723  1974.

[6] G. Schwarz. Estimating the dimension of a model. The Annals of Statistics  6:461–464  1978.
[7] R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society 

Series B.  58  1996.

[8] B. Efron  T. Hastie  L. Johnstone  and R. Tibshirani. Least angle regression. Annals of Statistics  32:407–

499  2004.

[9] P. Zhao and B. Yu. On model selection consistency of lasso. Journal of Machine Learning Research 

7:2541–2563  2006.

[10] H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association 

101:1418–1429  2006.

[11] Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of

the Royal Statistical Society  Series B  68:49–67  2006.

[12] F.R. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn. Res.  9:1179–

1225  2008.

[13] C. A. Micchelli and M. Pontil. Learning the kernel function via regularization. Journal of Machine

Learning Research  6:1099–1125  2005.

[14] H. Wang  G. Li  and C.L. Tsai. Regression coefﬁcient and autoregressive order shrinkage and selection

via the lasso. Journal Of The Royal Statistical Society Series B  69(1):63–78  2007.

[15] Nan-Jung Hsu  Hung-Lin Hung  and Ya-Mei Chang. Subset selection for vector autoregressive processes

using lasso. Computational Statistics and Data Analysis  52:3645–3657  2008.

[16] G. Pillonetto and G. De Nicolao. A new kernel-based approach for linear system identiﬁcation. Automat-

ica  46(1):81–93  2010.

[17] G. Pillonetto  A. Chiuso  and G. De Nicolao. Prediction error identiﬁcation of linear systems: a nonpara-

metric Gaussian regression approach. Automatica (in press)  2011.

[18] C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning. The MIT Press  2006.
[19] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society 

68:337–404  1950.

[20] G. Wahba. Support vector machines  reproducing kernel Hilbert spaces and randomized GACV. Technical

Report 984  Department of Statistics  University of Wisconsin  1998.

[21] G. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. Journal of Mathematical

Analysis and Applications  33(1):82–95  1971.

[22] A. J. Smola and B. Sch¨olkopf. Bayesian kernel methods.

In S. Mendelson and A. J. Smola  editors 
Machine Learning  Proceedings of the Summer School  Australian National University  pages 65–117 
Berlin  Germany  2003. Springer-Verlag.

[23] G. Wahba. Spline models for observational data. SIAM  Philadelphia  1990.
[24] G.C. Goodwin  M. Gevers  and B. Ninness. Quantifying the error in estimated transfer functions with

application to model order selection. IEEE Transactions on Automatic Control  37(7):913–928  1992.

[25] F. Dinuzzo. Kernel machines with two layers and multiple kernel learning. Technical report  Preprint

arXiv:1001.2709  2010. Available at http://www-dimat.unipv.it/ dinuzzo.

[26] L. Ljung. System Identiﬁcation Toolbox V7.1 for Matlab. Natick  MA: The MathWorks  Inc.  2007.

9

,Jaime Ide
Fábio Cappabianco
Fabio Faria
Chiang-shan Li