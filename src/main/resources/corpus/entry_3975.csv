2018,On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport,Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems  we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that  when initialized correctly and in the many-particle limit  this gradient flow  although non-convex  converges to global minimizers. The proof involves Wasserstein gradient flows  a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles  even in high dimension.,On the Global Convergence of Gradient Descent for
Over-parameterized Models using Optimal Transport

Lénaïc Chizat

Francis Bach

INRIA  ENS  PSL Research University

INRIA  ENS  PSL Research University

Paris  France

lenaic.chizat@inria.fr

Paris  France

francis.bach@inria.fr

Abstract

Many tasks in machine learning and signal processing can be solved by minimizing
a convex function of a measure. This includes sparse spikes deconvolution or
training a neural network with a single hidden layer. For these problems  we study
a simple minimization method: the unknown measure is discretized into a mixture
of particles and a continuous-time gradient descent is performed on their weights
and positions. This is an idealization of the usual way to train neural networks
with a large hidden layer. We show that  when initialized correctly and in the
many-particle limit  this gradient ﬂow  although non-convex  converges to global
minimizers. The proof involves Wasserstein gradient ﬂows  a by-product of optimal
transport theory. Numerical experiments show that this asymptotic behavior is
already at play for a reasonable number of particles  even in high dimension.

1

Introduction

A classical task in machine learning and signal processing is to search for an element in a Hilbert
space F that minimizes a smooth  convex loss function R : F → R+ and that is a linear combination
of a few elements from a large given parameterized set {φ(θ)}θ∈Θ ⊂ F. A general formulation of
this problem is to describe the linear combination through an unknown signed measure µ on the
parameter space and to solve for
J∗ = min
µ∈M(Θ)

(1)
where M(Θ) is the set of signed measures on the parameter space Θ and G : M(Θ) → R is an
optional convex regularizer  typically the total variation norm when sparse solutions are preferred. In
this paper  we consider the inﬁnite-dimensional case where the parameter space Θ is a domain of Rd
and θ (cid:55)→ φ(θ) is differentiable. This framework covers:

(cid:18)(cid:90)

(cid:19)

J(µ) := R

φdµ

+ G(µ)

J(µ) 

loss function  and φ(θ) : x (cid:55)→ σ((cid:80)d−1

• Training neural networks with a single hidden layer  where the goal is to select  within a
speciﬁc class  a function that maps features in Rd−1 to labels in R  from the observation
of a joint distribution of features and labels. This corresponds to F being the space of
square-integrable real-valued functions on Rd−1  R being  e.g.  the quadratic or the logistic
i=1 θixi + θd)  with an activation function σ : R → R.
Common choices are the sigmoid function or the rectiﬁed linear unit [18  14]  see more
details in Section 4.2.
• Sparse spikes deconvolution  where one attempts to recover a signal which is a mixture
of impulses on Θ given a noisy and ﬁltered observation y (a square-integrable function
on Θ). This corresponds to F being the space of square-integrable real-valued functions
on Rd  deﬁning φ(θ) : x (cid:55)→ ψ(x − θ) the translations of the ﬁlter impulse response ψ

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

and R(f ) = (1/2λ)(cid:107)f − y(cid:107)2
L2  for some λ > 0 that depends on the estimated noise level.
Solving (1) allows then to reconstruct the mixture of impulses with some guarantees [12  13].
• Low-rank tensor decomposition [16]  recovering mixture models from sketches [26]  see [6]
for a detailed list of other applications. For example  with symmetric matrices  F = Rd×d
and Φ(θ) = θθ(cid:62)  we recover low-rank matrix decompositions [15].

1.1 Review of optimization methods and previous work

While (1) is a convex problem  ﬁnding approximate minimizers is hard as the variable is inﬁnite-
dimensional. Several lines of work provide optimization methods but with strong limitations.

Conditional gradient / Frank-Wolfe. This approach tackles a variant of (1) where the regulariza-
tion term is replaced by an upper bound on the total variation norm; the associated constraint set
is the convex hull of all Diracs and negatives of Diracs at elements of θ ∈ Θ  and thus adapted to
conditional gradient algorithms [19]. At each iteration  one adds a new particle by solving a linear
minimization problem over the constraint set (which correspond to ﬁnding a particle θ ∈ Θ)  and then
updates the weights. The resulting iterates are sparse and there is a guaranteed sublinear convergence
rate of the objective function to its minimum. However  the linear minimization subroutine is hard to
perform in general : it is for instance NP-hard for neural networks with homogeneous activations [4].
One thus generally resorts to space gridding (in low dimension) or to approximate steps  akin to
boosting [36]. The practical behavior is improved with nonconvex updates [6  7] reminiscent of the
ﬂow studied below.

Semideﬁnite hierarchy. Another approach is to parameterize the unknown measure by its sequence
of moments. The space of such sequences is characterized by a hierarchy of SDP-representable
necessary conditions. This approach concerns a large class of generalized moment problems [22]
and can be adapted to deal with special instances of (1) [9]. It is however restricted to φ which
are combinations of few polynomial moments  and its complexity explodes exponentially with the
dimension d. For d ≥ 2  convergence to a global minimizer is only guaranteed asymptotically 
similarly to the results of the present paper.

Particle gradient descent. A third approach  which exploits the differentiability of φ  consists in
discretizing the unknown measure µ as a mixture of m particles parameterized by their positions and
weights. This corresponds to the ﬁnite-dimensional problem

(cid:32)

1
m

(cid:33)

m(cid:88)

i=1

Jm(w  θ)

where

Jm(w  θ) := J

min
w∈Rm
θ∈Θm

wiδθi

 

(2)

which can then be solved by classical gradient descent-based algorithms. This method is simple
to implement and is widely used for the task of neural network training but  a priori  we may only
hope to converge to local minima since Jm is non-convex. Our goal is to show that this method also
beneﬁts from the convex structure of (1) and enjoys an asymptotical global optimality guarantee.
There is a recent literature on global optimality results for (2) in the speciﬁc task of training neural
networks. It is known that in this context  Jm has less  or no  local minima in an over-parameterization
regime and stochastic gradient descent (SGD) ﬁnds a global minimizer under restrictive assump-
tions [34  35  33  23]; see [33] for an account of recent results. Our approach is not directly
comparable to these works: it is more abstract and nonquantitative—we study an ideal dynamics
that one can only hope to approximate—but also much more generic. Our objective  in the space
of measures  has many local minima  but we build gradient ﬂows that avoids them  relying mainly
on the homogeneity properties of Jm (see [16  20] for other uses of homogeneity in non-convex
optimization). The novelty is to see (2) as a discretization of (1)—a point of view also present in [25]
but not yet exploited for global optimality guarantees.

1.2 Organization of the paper and summary of contributions

Our goal is to explain when and why the non-convex particle gradient descent ﬁnds global minima.
We do so by studying the many-particle limit m → ∞ of the gradient ﬂow of Jm. More speciﬁcally:

2

• In Section 2  we introduce a more general class of problems and study the many-particle
limit of the associated particle gradient ﬂow. This limit is characterized as a Wasserstein
gradient ﬂow (Theorem 2.6)  an object which is a by-product of optimal transport theory.
• In Section 3  under assumptions on φ and the initialization  we prove that if this Wasserstein
gradient ﬂow converges  then the limit is a global minimizer of J. Under the same conditions 
it follows that if (w(m)(t)  θ(m)(t))t≥0 are gradient ﬂows for Jm suitably initialized  then

m t→∞ J(µm t) = J∗

lim

where

µm t =

1
m

w(m)

i

(t)δθ(m)

i

(t).

m(cid:88)

i=1

• Two different settings that leverage the structure of φ are treated: the 2-homogeneous and the
partially 1-homogeneous case. In Section 4  we apply these results to sparse deconvolution
and training neural networks with a single hidden layer  with sigmoid or ReLU activation
function. In each case  our result prescribes conditions on the initialization pattern.
• We perform simple numerical experiments that indicate that this asymptotic regime is
already at play for small values of m  even for high-dimensional problems. The method
behaves incomparably better than simply optimizing on the weights with a very large set of
ﬁxed particles.

Our focus on qualitative results might be surprising for an optimization paper  but we believe that
this is an insightful ﬁrst step given the hardness and the generality of the problem. We suggest to
understand our result as a ﬁrst consistency principle for practical and a commonly used non-convex
optimization methods. While we focus on the idealistic setting of a continuous-time gradient ﬂow
with exact gradients  this is expected to reﬂect the behavior of ﬁrst order descent algorithms  as they
are known to approximate the former: see [31] for (accelerated) gradient descent and [21  Thm. 2.1]
for SGD.
Notation. Scalar products and norms are denoted by · and | · | respectively in Rd  and by (cid:104)· ·(cid:105) and
(cid:107) · (cid:107) in the Hilbert space F. Norms of linear operators are also denoted by (cid:107) · (cid:107). The differential of a
function f at a point x is denoted dfx. We write M(Rd) for the set of ﬁnite signed Borel measures
on Rd  δx is a Dirac mass at a point x and P2(Rd) is the set of probability measures endowed with
the Wasserstein distance W2 (see Appendix A).

Recent related work. Several independent works [24  28  32] have studied the many-particle limit
of training a neural network with a single large hidden layer and a quadratic loss R. Their main focus
is on quantifying the convergence of SGD or noisy SGD to the limit trajectory  which is precisely a
mean-ﬁeld limit in this case. Since in our approach this limit is mostly an intermediate step necessary
to state our global convergence theorems  it is not studied extensively for itself. These papers thus
provide a solid complement to Section 2.4 (a difference is that we do not assume that R is quadratic
nor that V is differentiable). Also  [24] proves a quantitive global convergence result for noisy SGD
to an approximate minimizer: we stress that our results are of a different nature  as they rely on
homogeneity and not on the mixing effect of noise.

2 Particle gradient ﬂows and many-particle limit

2.1 Main problem and assumptions

From now on  we consider the following class of problems on the space of non-negative ﬁnite
measures on a domain Ω ⊂ Rd which  as explained below  is more general than (1):

(cid:18)(cid:90)

(cid:19)

(cid:90)

F ∗ = min

µ∈M+(Ω)

F (µ)

where

F (µ) = R

Φdµ

+

V dµ 

(3)

and we make the following assumptions.
Assumptions 2.1. F is a separable Hilbert space  Ω ⊂ Rd is the closure of a convex open set  and
(i) (smooth loss) R : F → R+ is differentiable  with a differential dR that is Lipschitz on bounded

sets and bounded on sublevel sets 

3

(ii) (basic regularity) Φ : Ω → F is (Fréchet) differentiable  V : Ω → R+ is semiconvex1  and
(iii) (locally Lipschitz derivatives with sublinear growth) there exists a family (Qr)r>0 of nested

nonempty closed convex subsets of Ω such that:
(a) {u ∈ Ω ; dist(u  Qr) ≤ r(cid:48)} ⊂ Qr+r(cid:48) for all r  r(cid:48) > 0 
(b) Φ and V are bounded and dΦ is Lipschitz on each Qr  and
(c) there exists C1  C2 > 0 such that supu∈Qr ((cid:107)dΦu(cid:107) +(cid:107)∂V (u)(cid:107)) ≤ C1 + C2r for all r > 0 

where (cid:107)∂V (u)(cid:107) stands for the maximal norm of an element in ∂V (u).

Assumption 2.1-(iii) reduces to classical local Lipschitzness and growth assumptions on dΦ and ∂V if
the nested sets (Qr)r are the balls of radius r  but unbounded sets Qr are also allowed. These sets are
a technical tool used later to conﬁne the gradient ﬂows in areas where gradients are well-controlled.

By convention  we set F (µ) = ∞ if µ is not concentrated on Ω. Also  the integral(cid:82) Φdµ is a
(cid:82) (cid:107)φ(cid:107)d|µ| < ∞. Otherwise  we also set F (µ) = ∞ by convention.

Bochner integral [10  App. E6]. It yields a well-deﬁned value in F whenever Φ is measurable and

Recovering (1) through lifting.
It is shown in Appendix A.2 that  for a class of admissible regu-
larizers G containing the total variation norm  problem (1) admits an equivalent formulation as (3).
Indeed  consider the lifted domain Ω = R × Θ  the function Φ(w  θ) = wφ(θ) and V (w  θ) = |w|.
Then J∗ equals F ∗ and given a minimizer of one of the problems  one can easily build minimizers for
the other. This equivalent lifted formulation removes the asymmetry between weight and position—
weight becomes just another coordinate of a particle’s position. This is the right point of view for our
purpose and this is why F is our central object of study in the following.

Homogeneity. The functions Φ and V obtained through the lifting share the property of being
positively 1-homogeneous in the variable w. A function f between vector spaces is said positively
p-homogeneous when for all λ > 0 and argument x  it holds f (λx) = λpf (x). This property is
central for our global convergence results (but is not needed throughout Section 2).

2.2 Particle gradient ﬂow

We ﬁrst consider an initial measure which is a mixture of particles—an atomic measure— and deﬁne
the initial object in our construction: the particle gradient ﬂow. For a number m ∈ N of particles 
and a vector u ∈ Ωm of positions  this is the gradient ﬂow of

(cid:33)

(cid:32)

m(cid:88)

i=1

1
m

(cid:32)

m(cid:88)

i=1

1
m

(cid:33)

m(cid:88)

i=1

1
m

Fm(u) := F

δui

= R

Φ(ui)

+

V (ui) 

(4)

or  more precisely  its subgradient ﬂow because V can be non-smooth. We recall that a subgradient
of a (possibly non-convex) function f : Rd → ¯R at a point u0 ∈ Rd is a p ∈ Rd satisfying
f (u) ≥ f (u0) + p · (u − u0) + o(u − u0) for all u ∈ Rd. The set of subgradients at u is a closed
convex set called the subdifferential of f at u denoted ∂f (u) [27].
Deﬁnition 2.2 (Particle gradient ﬂow). A gradient ﬂow for the functional Fm is an absolutely
continuous2 path u : R+ → Ωm which satisﬁes u(cid:48)(t) ∈ −m ∂Fm(u(t)) for almost every t ≥ 0.
This deﬁnition uses a subgradient scaled by m  which is the subgradient relative to the scalar product
on (Rd)m scaled by 1/m: this normalization amounts to assigning a mass 1/m to each particle and
is convenient for taking the many-particle limit m → ∞. We now state basic properties of this object.
Proposition 2.3. For any initialization u(0) ∈ Ωm  there exists a unique gradient ﬂow u : R+ → Ωm
ds Fm(u(s))|s=t = −|u(cid:48)(t)|2 and the velocity of
for Fm. Moreover  for almost every t > 0  it holds d
the i-th particle is given by u(cid:48)

i(t) = vt(ui(t))  where for u ∈ Ω and µm t := (1/m)(cid:80)m
(cid:1)   ∂jΦ(u)(cid:11)(cid:3)d

˜vt(u) = −(cid:2)(cid:10)R(cid:48)(cid:0)(cid:82) Φdµm t

vt(u) = ˜vt(u) − proj∂V (u)(˜vt(u)) with
(5)
1A function f : Rd → R is semiconvex  or λ-convex  if f + λ| · |2 is convex  for some λ ∈ R. On a compact
x(s) =(cid:82) t
2An absolutely continuous function x : R → Rd is almost everywhere differentiable and satisﬁes x(t) −

domain  any smooth fonction is semiconvex.

i=1 δui(t) 

s x(cid:48)(r)dr for all s < t.

j=1 .

4

The expression of the velocity involves a projection because gradient ﬂows select subgradients
of minimal norm [29]. We have denoted by R(cid:48)(f ) ∈ F the gradient of R at f ∈ F and by
∂jΦ(u) ∈ F the differential dΦu applied to the j-th vector of the canonical basis of Rd. Note
that [˜vt(ui)]m
i=1 is (minus) the gradient of the ﬁrst term in (4) : when V is differentiable  we have
vt(u) = ˜vt(u) − ∇V (u) and we recover the classical gradient of (4). When V is non-smooth  this
gradient ﬂow can be understood as a continuous-time version of the forward-backward minimization
algorithm [11].

2.3 Wasserstein gradient ﬂow

The fact that the velocity of each particle can be expressed as the evaluation of a velocity ﬁeld (Eq. (5))
makes it easy  at least formally  to generalize the particle gradient ﬂow to arbitrary measure-valued
initializations—not just atomic ones. On the one hand  the evolution of a time-dependent measure
(µt)t under the action of instantaneous velocity ﬁelds (vt)t≥0 can be formalized by a conservation
of mass equation  known as the continuity equation  that reads ∂tµt = −div(vtµt) where div is the
divergence operator3 (see Appendix B). On the other hand  there is a direct link between the velocity
ﬁeld (5) and the functional F . The differential of F evaluated at µ ∈ M(Ω) is represented by the
function F (cid:48)(µ) : Ω → R deﬁned as

(cid:28)

R(cid:48)(cid:18)(cid:90)

(cid:19)

(cid:29)

F (cid:48)(µ)(u) :=

Φdµ

  Φ(u)

+ V (u).

Thus vt is simply a ﬁeld of (minus) subgradients of F (cid:48)(µm t)—it is in fact the ﬁeld of minimal norm
subgradients. We write this relation vt ∈ −∂F (cid:48)(µm t). The set ∂F (cid:48) is called the Wasserstein subdif-
ferential of F   as it can be interpreted as the subdifferential of F relatively to the Wasserstein metric
on P2(Ω) (see Appendix B.2.1). We thus expect that for initializations with arbitrary probability
distributions  the generalization of the gradient ﬂow coindices with the following object.
Deﬁnition 2.4 (Wasserstein gradient ﬂow). A Wasserstein gradient ﬂow for the functional F on a time
interval [0  T [ is an absolutely continuous path (µt)t∈[0 T [ in P2(Ω) that satisﬁes  distributionally on
[0  T [ × Ωd 

∂tµt = −div(vtµt) where

vt ∈ −∂F (cid:48)(µt).

(6)

This is a proper generalization of Deﬁnition 2.2 since  whenever (u(t))t≥0 is a particle gradient
ﬂow for Fm  then t (cid:55)→ µm t := 1
i=1 δui(t) is a Wasserstein gradient ﬂow for F in the sense of
Deﬁnition 2.4 (see Proposition B.1). By leveraging the abstract theory of gradient ﬂows developed
in [3]  we show in Appendix B.2.1 that these Wasserstein gradient ﬂows are well-deﬁned.
Proposition 2.5 (Existence and uniqueness). Under Assumptions 2.1  if µ0 ∈ P2(Ω) is concentrated
on a set Qr0 ⊂ Ω  then there exists a unique Wasserstein gradient ﬂow (µt)t≥0 for F starting from µ0.
It satisﬁes the continuity equation with the velocity ﬁeld deﬁned in (5) (with µt in place of µm t).

m

(cid:80)m

Note that the condition on the initialization is automatically satisﬁed in Proposition 2.3 because there
the initial measure has a ﬁnite discrete support: it is thus contained in any Qr for r > 0 large enough.

2.4 Many-particle limit

We now characterize the many-particle limit of classical gradient ﬂows  under Assumptions 2.1.
Theorem 2.6 (Many-particle limit). Consider (t (cid:55)→ um(t))m∈N a sequence of classical gradient
ﬂows for Fm initialized in a set Qr0 ⊂ Ω. If µm 0 converges to some µ0 ∈ P2(Ω) for the Wasserstein
distance W2  then (µm t)t converges  as m → ∞  to the unique Wasserstein gradient ﬂow of F
starting from µ0.
Given a measure µ0 ∈ P2(Qr0)  an example for the sequence um(0) is um(0) = (u1  . . .   um)
where u1  u2  . . .   um are independent samples distributed according to µ0. By the law of large
numbers for empirical distributions  the sequence of empirical distributions µm 0 = 1
i=1 δui
m
converges (almost surely  for W2) to µ0. In particular  our proof of Theorem 2.6 gives an alternative
proof of the existence claim in Proposition 2.5 (the latter remains necessary for the uniqueness of the
limit).

(cid:80)m

i=1 : Rd → Rd  its divergence is given by div(E) =(cid:80)d

3For a smooth vector ﬁeld E = (Ei)d

i=1 ∂Ei/∂xi.

5

3 Convergence to global minimizers

3.1 General idea
As can be seen from Deﬁnition 2.4  a probability measure µ ∈ P2(Ω) is a stationary point of a
Wasserstein gradient ﬂow if and only if 0 ∈ ∂F (cid:48)(µ)(u) for µ-a.e. u ∈ Ω. It is proved in [25] that
these stationary points are  in some cases  optimal over probabilities that have a smaller support.
However  they are not in general global minimizers of F over M+(Ω)  even when R is convex. Such
global minimizers are indeed characterized as follows.
Proposition 3.1 (Minimizers). Assume that R is convex. A measure µ ∈ M+(Ω) such that F (µ) <
∞ minimizes F on M+(Ω) iff F (cid:48)(µ) ≥ 0 and F (cid:48)(µ)(u) = 0 for µ-a.e. u ∈ Ω.
Despite these strong differences between stationarity and global optimality  we show in this section
that Wasserstein gradient ﬂows converge to global minimizers  under two main conditions:

deﬁnition of homogeneity)  and

• On the structure: Φ and V must share a homogeneity direction (see Section 2.1 for the
• On the initialization: the support of the initialization of the Wasserstein gradient ﬂow
satisﬁes a “separation” property. This property is preserved throughout the dynamic and 
combined with homogeneity  allows to escape from neighborhoods of non-optimal points.

We turn these general ideas into concrete statements for two cases of interest  that exhibit different
structures and behaviors: (i) when Φ and V are positively 2-homogeneous and (ii) when Φ and V are
positively 1-homogeneous with respect to one variable.

3.2 The 2-homogeneous case
In the 2-homogeneous case a rich structure emerges  where the (d−1)-dimensional sphere Sd−1 ⊂ Rd
plays a special role. This covers the case of lifted problems of Section 2.1 when φ is 1-homogeneous
and neural networks with ReLU activation functions.
Assumptions 3.2. The domain is Ω = Rd with d ≥ 2 and Φ is differentiable with dΦ locally
Lipschitz  V is semiconvex and V and Φ are both positively 2-homogeneous. Moreover 

(i) (smooth convex loss) The loss R is convex  differentiable with differential dR Lipschitz on

bounded sets and bounded on sublevel sets 

(ii) (Sard-type regularity) For all f ∈ F  the set of regular values4 of θ ∈ Sd−1 (cid:55)→ (cid:104)f  Φ(θ)(cid:105) + V (θ)
is dense in its range (it is in fact sufﬁcient that this holds for functions f which are of the form

f = R(cid:48)((cid:82) Φdµ) for some µ ∈ M+(Ω)).

Taking the balls of radius r > 0 as the family (Qr)r>0  these assumptions imply Assumptions 2.1.
We believe that Assumption 3.2-(ii) is not of practical importance: it is only used to avoid some
pathological cases in the proof of Theorem 3.3. By applying Morse-Sard’s lemma [1]  it is anyways
fulﬁlled if the function in question is d − 1 times continuously differentiable. We now state our ﬁrst
global convergence result. It involves a condition on the initialization  a separation property  that can
only be satisﬁed in the many-particle limit. In an ambient space Ω  we say that a set C separates the
sets A and B if any continuous path in Ω with endpoints in A and B intersects C.
Theorem 3.3. Under Assumptions 3.2  let (µt)t≥0 be a Wasserstein gradient ﬂow of F such that 
for some 0 < ra < rb  the support of µ0 is contained in B(0  rb) and separates the spheres raSd−1
and rbSd−1. If (µt)t converges to µ∞ in W2  then µ∞ is a global minimizer of F over M+(Ω). In
particular  if (um(t))m∈N t≥0 is a sequence of classical gradient ﬂows initialized in B(0  rb) such
that µm 0 converges weakly to µ0 then (limits can be interchanged)
F (µ).

lim
t m→∞ F (µm t) = min

µ∈M+(Ω)

A proof and stronger statements are presented in Appendix C. There  we give a criterion for Wasser-
stein gradient ﬂows to escape neighborhoods of non-optimal measures—also valid in the ﬁnite-particle
4For a function g : Θ → R  a regular value is a real number α in the range of g such that g−1(α) is included

in an open set where g is differentiable and where dg does not vanish.

6

setting—and then show that it is always satisﬁed by the ﬂow deﬁned above. We also weaken the
assumption that µt converges: we only need a certain projection of µt to converge weakly. Finally 
the fact that limits in m and t can be interchanged is not anecdotal: it shows that the convergence is
not conditioned on a relative speed of growth of both parameters.
This result might be easier to understand by drawing an informal distinction between (i) the structural
assumptions which are instrumental and (ii) the technical conditions which have a limited practical
interest. The initialization and the homogeneity assumptions are of the ﬁrst kind. The Sard-type
regularity is in contrast a purely technical condition: it is generally hard to check and known counter-
examples involve artiﬁcial constructions such as the Cantor function [37]. Similarly  when there is
compactness  a gradient ﬂow that does not converge is an unexpected (in some sense adversarial)
behavior  see a counter-example in [2]. We were however not able to exclude this possibility under
interesting assumptions (see a discussion in Appendix C.5).

3.3 The partially 1-homogeneous case

Similar results hold in the partially 1-homogeneous setting  which covers the lifted problems of
Section 2.1 when φ is bounded (e.g.  sparse deconvolution and neural networks with sigmoid
activation).
Assumptions 3.4. The domain is Ω = R × Θ with Θ ⊂ Rd−1  Φ(w  θ) = w · φ(θ) and V (w  θ) =
|w| ˜V (θ) where φ and ˜V are bounded  differentiable with Lipschitz differential. Moreover 
(i) (smooth convex loss) The loss R is convex  differentiable with differential dR Lipschitz on

bounded sets and bounded on sublevel sets 

(ii) (Sard-type regularity) For all f ∈ F  the set of regular values of gf : θ ∈ Θ (cid:55)→ (cid:104)f  φ(θ)(cid:105) + ˜V (θ)

is dense in its range  and

(iii) (boundary conditions) The function φ behaves nicely at the boundary of the domain: either

(a) Θ = Rd−1 and for all f ∈ F  θ ∈ Sd−2 (cid:55)→ gf (rθ) converges  uniformly in C 1(Sd−2) as
r → ∞  to a function satisfying the Sard-type regularity  or
(b) Θ is the closure of an bounded open convex set and for all f ∈ F  gf satisﬁes Neumann
boundary conditions (i.e.  for all θ ∈ ∂Θ  d(gf )θ((cid:126)nθ) = 0 where (cid:126)nθ ∈ Rd−1 is the normal
to ∂Θ at θ).

With the family of nested sets Qr := [−r  r] × Θ  r > 0  these assumptions imply Assumptions 2.1.
The following theorem mirrors the statement of Theorem 3.3  but with a different condition on the
initialization. The remarks after Theorem 3.3 also apply here.
Theorem 3.5. Under Assumptions 3.4  let (µt)t≥0 be a Wasserstein gradient ﬂow of F such that
for some r0 > 0  the support of µ0 is contained in [−r0  r0] × Θ and separates {−r0} × Θ from
{r0} × Θ. If (µt)t converges to µ∞ in W2  then µ∞ is a global minimizer of F over M+(Ω). In
particular  if (um(t))m∈N t≥0 is a sequence of classical gradient ﬂows initialized in [−r0  r0] × Θ
such that µm 0 converges to µ0 in W2 then (limits can be interchanged)

lim
t m→∞ F (µm t) = min

µ∈M+(Ω)

F (µ).

4 Case studies and numerical illustrations

In this section  we apply the previous abstract statements to speciﬁc examples and show on synthetic
experiments that the particle-complexity to reach global optimality is very favorable.

4.1 Sparse deconvolution
For sparse deconvolution  it is typical to consider a signal y ∈ F := L2(Θ) on the d-torus Θ = Rd/Zd.
The loss function is R(f ) = (1/2λ)(cid:107)y − f(cid:107)2
L2 for some λ > 0  a parameter that increases with the
noise level and the regularization is V (w  θ) = |w|. Consider a ﬁlter impulse response ψ : Θ → R and
let Φ(w  θ) : x (cid:55)→ w · ψ(x − θ). The object sought after is a signed measure on Θ  which is obtained
R wdµ(w  B)
for all measurable B ⊂ Θ. We show in Appendix D that Theorem 3.5 applies.

from a probability measure on R × Θ by applying a operator deﬁned by h1(µ)(B) =(cid:82)

7

Proposition 4.1 (Sparse deconvolution). Assume that the ﬁlter impulse response ψ is min{2  d}
times continuously differentiable  and that the support of µ0 contains {0} × Θ. If the projection
(h1(µt))t of the Wasserstein gradient ﬂow of F weakly converges to ν ∈ M(Θ)  then ν is a global
minimizer of

min

µ∈M(Θ)

1
2λ

(cid:13)(cid:13)y −(cid:82) ψdµ(cid:13)(cid:13)2

L2 + |µ|(Θ).

We show an example of such a reconstruction on the 1-torus on Figure 1  where the ground truth
consists of m0 = 5 weighted spikes  ψ is an ideal low pass ﬁlter (a Dirichlet kernel of order 7)
and y is a noisy observation of the ﬁltered spikes. The particle gradient ﬂow is integrated with the
forward-backward algorithm [11] and the particles initialized on a uniform grid on {0} × Θ.

Figure 1: Particle gradient ﬂow for sparse deconvolution on the 1-torus (horizontal axis shows
positions  vertical axis shows weights). Failure to ﬁnd a minimizer with 6 particles  success with 10
and 100 particles (an animated plot of this particle gradient ﬂow can be found in Appendix D.5).

4.2 Neural networks with a single hidden layer
We consider a joint distribution of features and labels ρ ∈ P(Rd−2 × R) and ρx ∈ P(Rd−2) the
on F = L2(ρx)  where (cid:96) : R × R → R+ is either the squared loss or the logistic loss. Also  we set
i=1 θixi + θd−1) for an activation function σ : R → R. Depending on the

marginal distribution of features. The loss is the expected risk R(f ) =(cid:82) (cid:96)(f (x)  y)dρ(x  y) deﬁned
Φ(w  θ) : x (cid:55)→ wσ((cid:80)d−2

choice of σ  we face two different situations.

If σ is a sigmoid  say σ(s) = (1 + e−s)−1  then Theorem 3.5  with domain
Sigmoid activation.
Θ = Rd−1 applies. The natural (optional) regularization term is V (w  θ) = |w|  which amounts to
penalizing the (cid:96)1 norm of the weights.
Proposition 4.2 (Sigmoid activation). Assume that ρx has ﬁnite moments up to order min{4  2d− 2} 
that the support of µ0 is {0} × Θ and that boundary condition 3.4-(iii)-(a) holds. If the Wasserstein
gradient ﬂow of F converges in W2 to µ∞  then µ∞ is a global minimizer of F .

Note that we have to explicitly assume the boundary condition 3.4-(iii)-(a) because the Sard-type
regularity at inﬁnity cannot be checked a priori (this technical detail is discussed in Appendix D.3).
ReLU activation. The activation function σ(s) = max{0  s} is positively 1-homogeneous: this
makes Φ 2-homogeneous and corresponds  at a formal level  to the setting of Theorem 3.3. An
admissible choice of regularizer here would be the (semi-convex) function V (w  θ) = |w| · |θ| [4].
However  as shown in Appendix D.4  the differential dΦ has discontinuities: this prevents altogether
from deﬁning gradient ﬂows  even in the ﬁnite-particle regime.
Still  a statement holds for a different parameterization of the same class of functions  which makes Φ
differentiable. To see this  consider a domain Θ which is the disjoint union of 2 copies of Rd. On the
i=1 s(θi)xi + s(θd)) where s(θi) = θi|θi| is the signed square
function. On the second copy  Φ has the same deﬁnition but with a minus sign. This trick allows to
have the same expression power than classical ReLU networks. In practice  it corresponds to simply
putting  say  random signs in front of the activation. The regularizer here can be V (θ) = |θ|2.
Proposition 4.3 (Relu activation). Assume that ρx ∈ P(Rd−1) has ﬁnite second moments  that the
support of µ0 is r0Sd−1 for some r0 > 0 (on both copies of Rd) and that the Sard-type regularity

ﬁrst copy  deﬁne Φ(θ) : x (cid:55)→ σ((cid:80)d−1

8

0.00.20.40.60.81.021012particle gradient flowlimit of the flowoptimal positions0.00.20.40.60.81.0210120.00.20.40.60.81.021012Assumption 3.2-(ii) holds. If the Wasserstein gradient ﬂow of F converges in W2 to µ∞  then µ∞ is a
global minimizer of F .

We display on Figure 2 particle gradient ﬂows for training a neural network with a single hidden
layer and ReLU activation in the classical (non-differentiable) parameterization  with d = 2 (no
regularization). Features are normally distributed  and the ground truth labels are generated with a
similar network with m0 = 4 neurons. The particle gradient ﬂow is “integrated” with mini-batch
SGD and the particles are initialized on a small centered sphere.

Figure 2: Training a neural network with ReLU activation. Failure with 5 particles (a.k.a. neurons) 
success with 10 and 100 particles. We show the trajectory of |w(t)| · θ(t) ∈ R2 for each particle (an
animated plot of this particle gradient ﬂow can be found in Appendix D.5).

4.3 Empirical particle-complexity

Since our convergence results are non-quantitative  one might argue that similar—and much simpler
to prove—asymptotical results hold for the method of distributing particles on the whole of Θ
and simply optimizing on the weights  which is a convex problem. Yet  the comparison of the
particle-complexity shown in Figure 3 stands strongly in favor of particle gradient ﬂows. While
exponential particle-complexity is unavoidable for the convex approach  we observed on several
synthetic problems that particle gradient descent only needs a slight over-parameterization m > m0
to ﬁnd global minimizers within optimization error (see details in Appendix D.5).

e
c
n
e
g
r
e
v
n
o
c

t
a

s
s
o
l

s
s
e
c
x
E

(a) Sparse deconvolution (d = 1)

(b) ReLU activation (d = 100)

(c) Sigmoid activation (d = 100)

Figure 3: Comparison of particle-complexity for particle gradient ﬂow and convex minimization on a
ﬁxed grid: excess loss at convergence vs. number of particles. Simplest minimizer has m0 particles.

5 Conclusion

We have established asymptotic global optimality properties for a family of non-convex gradient
ﬂows. These results were enabled by the study of a Wasserstein gradient ﬂow: this object simpliﬁes
the handling of many-particle regimes  analogously to a mean-ﬁeld limit. The particle-complexity
to reach global optimality turns out very favorable on synthetic numerical problems. This conﬁrms
the relevance of our qualitative results and calls for quantitative ones that would further exploit the
properties of such particle gradient ﬂows. Multiple layer neural networks are also an interesting
avenue for future research.

9

21012321012particle gradient flowoptimal positionslimit measure2101232101221012321012100101102105104103102101100101101102106105104103102101100particle gradient flowconvex minimizationbelow optim. errorm0101102105104103102101100Acknowledgments

We acknowledge supports from grants from Région Ile-de-France and the European Research Council
(grant SEQUOIA 724063).

References
[1] Ralph Abraham and Joel Robbin. Transversal mappings and ﬂows. WA Benjamin New York 

1967.

[2] Pierre-Antoine Absil  Robert Mahony  and Benjamin Andrews. Convergence of the iterates of
descent methods for analytic cost functions. SIAM Journal on Optimization  16(2):531–547 
2005.

[3] Luigi Ambrosio  Nicola Gigli  and Giuseppe Savaré. Gradient ﬂows: in metric spaces and in

the space of probability measures. Springer Science & Business Media  2008.

[4] Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of

Machine Learning Research  18(19):1–53  2017.

[5] Adrien Blanchet and Jérôme Bolte. A family of functional inequalities: Łojasiewicz inequalities
and displacement convex functions. Journal of Functional Analysis  275(7):1650–1673  2018.

[6] Nicholas Boyd  Geoffrey Schiebinger  and Benjamin Recht. The alternating descent conditional
gradient method for sparse inverse problems. SIAM Journal on Optimization  27(2):616–639 
2017.

[7] Kristian Bredies and Hanna Katriina Pikkarainen. Inverse problems in spaces of measures.

ESAIM: Control  Optimisation and Calculus of Variations  19(1):190–218  2013.

[8] Felix E. Browder. Fixed point theory and nonlinear problems. Proc. Sym. Pure. Math  39:49–88 

1983.

[9] Paul Catala  Vincent Duval  and Gabriel Peyré. A low-rank approach to off-the-grid sparse

deconvolution. Journal of Physics: Conference Series  904(1):012015  2017.

[10] Donald L. Cohn. Measure theory  volume 165. Springer  1980.

[11] Patrick L. Combettes and Jean-Christophe Pesquet. Proximal splitting methods in signal
processing. In Fixed-point algorithms for inverse problems in science and engineering  pages
185–212. Springer  2011.

[12] Yohann De Castro and Fabrice Gamboa. Exact reconstruction using Beurling minimal extrapo-

lation. Journal of Mathematical Analysis and applications  395(1):336–354  2012.

[13] Vincent Duval and Gabriel Peyré. Exact support recovery for sparse spikes deconvolution.

Foundations of Computational Mathematics  15(5):1315–1355  2015.

[14] Ian Goodfellow  Yoshua Bengio  and Aaron Courville. Deep Learning. MIT Press  2016.

[15] Suriya Gunasekar  Blake E. Woodworth  Srinadh Bhojanapalli  Behnam Neyshabur  and Nati
Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information
Processing Systems 30  2017.

[16] Benjamin D. Haeffele and René Vidal. Global optimality in neural network training.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
7331–7339  2017.

[17] Daniel Hauer and José Mazón. Kurdyka-Łojasiewicz-Simon inequality for gradient ﬂows in

metric spaces. arXiv preprint arXiv:1707.03129  2017.

[18] Simon Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall  1994.

[19] Martin Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In Proceed-

ings of the International Conference on Machine Learning (ICML)  2013.

10

[20] Michel Journée  Francis Bach  P-A Absil  and Rodolphe Sepulchre. Low-rank optimization on
the cone of positive semideﬁnite matrices. SIAM Journal on Optimization  20(5):2327–2351 
2010.

[21] Harold Kushner and G. George Yin. Stochastic approximation and recursive algorithms and

applications  volume 35. Springer Science & Business Media  2003.

[22] Jean-Bernard Lasserre. Moments  positive polynomials and their applications  volume 1. World

Scientiﬁc  2010.

[23] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU

activation. In Advances in Neural Information Processing Systems  pages 597–607  2017.

[24] Song Mei  Andrea Montanari  and Phan-Minh Nguyen. A mean ﬁeld view of the landscape of
two-layer neural networks. Proceedings of the National Academy of Sciences  115(33):E7665–
E7671  2018.

[25] Atsushi Nitanda and Taiji Suzuki. Stochastic particle gradient descent for inﬁnite ensembles.

arXiv preprint arXiv:1712.05438  2017.

[26] Clarice Poon  Nicolas Keriven  and Gabriel Peyré. A dual certiﬁcates analysis of compressive

off-the-grid recovery. arXiv preprint arXiv:1802.08464  2018.

[27] Ralph T. Rockafellar. Convex Analysis. Princeton University Press  1997.

[28] Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error.
arXiv preprint arXiv:1805.00915  2018.

[29] Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser  NY  2015.
[30] Filippo Santambrogio. {Euclidean  metric  and Wasserstein} gradient ﬂows: an overview.

Bulletin of Mathematical Sciences  7(1):87–154  2017.

[31] Damien Scieur  Vincent Roulet  Francis Bach  and Alexandre d’Aspremont. Integration methods
and optimization algorithms. In Advances in Neural Information Processing Systems  pages
1109–1118  2017.

[32] Justin Sirignano and Konstantinos Spiliopoulos. Mean ﬁeld analysis of neural networks. arXiv

preprint arXiv:1805.01053  2018.

[33] Mahdi Soltanolkotabi  Adel Javanmard  and Jason D. Lee. Theoretical insights into the op-
timization landscape of over-parameterized shallow neural networks. IEEE Transactions on
Information Theory  2018.

[34] Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer

neural networks. arXiv preprint arXiv:1702.05777  2017.

[35] Luca Venturi  Afonso Bandeira  and Joan Bruna. Spurious valleys in two-layer neural network

optimization landscapes. arXiv preprint arXiv:1802.06384  2018.

[36] Chu Wang  Yingfei Wang  Robert Schapire  et al. Functional Frank-Wolfe boosting for general

loss functions. arXiv preprint arXiv:1510.02558  2015.

[37] Hassler Whitney et al. A function not constant on a connected set of critical points. Duke

Mathematical Journal  1(4):514–517  1935.

11

,Shariq Mobin
James Arnemann
Fritz Sommer
Manuel Watter
Jost Springenberg
Joschka Boedecker
Martin Riedmiller
Lénaïc Chizat
Francis Bach