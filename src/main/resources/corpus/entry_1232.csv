2013,Correlations strike back (again): the case of associative memory retrieval,It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied  though equally pernicious  is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations  and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs  such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate.,Correlations strike back (again): the case of

associative memory retrieval

Cristina Savin1

cs664@cam.ac.uk

Peter Dayan2

dayan@gatsby.ucl.ac.uk

M´at´e Lengyel1

m.lengyel@eng.cam.ac.uk

1Computational & Biological Learning Lab  Dept. Engineering  University of Cambridge  UK

2Gatsby Computational Neuroscience Unit  University College London  UK

Abstract

It has long been recognised that statistical dependencies in neuronal activity need
to be taken into account when decoding stimuli encoded in a neural population.
Less studied  though equally pernicious  is the need to take account of dependen-
cies between synaptic weights when decoding patterns previously encoded in an
auto-associative memory. We show that activity-dependent learning generically
produces such correlations  and failing to take them into account in the dynamics
of memory retrieval leads to catastrophically poor recall. We derive optimal net-
work dynamics for recall in the face of synaptic correlations caused by a range of
synaptic plasticity rules. These dynamics involve well-studied circuit motifs  such
as forms of feedback inhibition and experimentally observed dendritic nonlineari-
ties. We therefore show how addressing the problem of synaptic correlations leads
to a novel functional account of key biophysical features of the neural substrate.

1

Introduction

Auto-associative memories have a venerable history in computational neuroscience. However  it is
only rather recently that the statistical revolution in the wider ﬁeld has provided theoretical traction
for this problem [1]. The idea is to see memory storage as a form of lossy compression – information
on the item being stored is mapped into a set of synaptic changes – with the neural dynamics during
retrieval representing a biological analog of a corresponding decompression algorithm. This implies
there should be a tight  and indeed testable  link between the learning rule used for encoding and the
neural dynamics used for retrieval [2].
One issue that has been either ignored or trivialized in these treatments of recall is correlations
among the synapses [1–4] – beyond the perfect (anti-)correlations emerging between reciprocal
synapses with precisely (anti-)symmetric learning rules [5]. There is ample experimental data for
the existence of such correlations: for example  in rat visual cortex  synaptic connections tend to
cluster together in the form of overrepresented patterns  or motifs  with reciprocal connections being
much more common than expected by chance  and the strengths of the connections to and from
each neuron being correlated [6]. The study of neural coding has indicated that it is essential to
treat correlations in neural activity appropriately in order to extract stimulus information well [7–
9]. Similarly  it becomes pressing to examine the nature of correlations among synaptic weights in
auto-associative memories  the consequences for retrieval of ignoring them  and methods by which
they might be accommodated.

1

Here  we consider several well-known learning rules  from simple additive ones to bounded synapses
with metaplasticity  and show that  with a few signiﬁcant exceptions  they induce correlations be-
tween synapses that share a pre- or a post-synaptic partner. To assess the importance of these de-
pendencies for recall  we adopt the strategy of comparing the performance of decoders which either
do or do not take them into account [10]  showing that they do indeed have an important effect on
efﬁcient retrieval. Finally  we show that approximately optimal retrieval involves particular forms
of nonlinear interactions between different neuronal inputs  as observed experimentally [11].

2 General problem formulation

We consider a network of N binary neurons that enjoy all-to-all connectivity.1 As is conventional 
and indeed plausibly underpinned by neuromodulatory interactions [12]  we assume that network
dynamics do not play a role during storage (with stimuli being imposed as patterns of activity on the
neurons)  and that learning does not occur during retrieval.
To isolate the effects of different plasticity rules on synaptic correlations from other sources of
correlations  we assume that the patterns of activity inducing the synaptic changes have no particular
structure  i.e. their distribution factorizes. For further simplicity  we take these activity patterns to
be binary with pattern density f  i.e. a prior over patterns deﬁned as:

(cid:89)

i

Pstore(x) =

Pstore(xi)

Pstore(xi) = f xi · (1 − f )1−xi

(1)

During recall  the network is presented with a cue  ˜x  which is a noisy or partial version of one
of the originally stored patterns. Network dynamics should complete this partial pattern  using the
information in the weights W (and the cue). We start by considering arbitrary dynamics; later we
impose the critical constraint for biological realisability that they be strictly local  i.e. the activity of
neuron i should depend exclusively on inputs through incoming synapses Wi ·.
Since information storage by synaptic plasticity is lossy  recall is inherently a probabilistic inference
problem [1  13] (Fig. 1a)  requiring estimation of the posterior over patterns  given the information
in the weights and the recall cue:

P (x|W  ˜x) ∝ Pstore(x) · Pnoise(˜x|x) · P(W|x)

(2)
This formulation has formed the foundation of recent work on constructing efﬁcient autoassociative
recall dynamics for a range of different learning rules [2–4]. In this paper  we focus on the last term
P(W|x)  which expresses the probability of obtaining W as the synaptic weight matrix when x is
stored along with T − 1 random patterns (sampled from the prior  Eq. 1). Critically  this is where
we diverge from previous analyses that assumed this distribution was factorised  or only trivially
correlated due to reciprocal synapses being precisely (anti-)symmetric [1  2  4]. In contrast  we
explicitly study the emergence and effects of non-trivial correlations in the synaptic weight matrix-
distribtion  because almost all synaptic plasticity rules induce statistical dependencies between the
synaptic weights of each neuron (Fig. 1a  d).
The inference problem expressed by Eq. 2 can be translated into neural dynamics in several ways
– dynamics could be deterministic  attractor-like  converging to the most likely pattern (a MAP
estimate) of the distribution of x [2]  or to a mean-ﬁeld approximate solution [3]; alternatively  the
dynamics could be stochastic  with the activity over time representing samples from the posterior 
and hence implicitly capturing the uncertainty associated with the answer [4]. We consider the latter.
Since we estimate performance by average errors  the optimal response is the mean of the posterior 
which can be estimated by integrating the activity of the network during retrieval.
We start by analysing the class of additive learning rules  to get a sense for the effect of correla-
tions on retrieval. Later  we focus on multi-state synapses  for which learning rules are described
by transition probabilities between the states [14]. These have been used to capture a variety of
important biological constraints such as bounds on synaptic strengths and metaplasticity  i.e. the
fact that synaptic changes induced by a certain activity pattern depend on the history of activity at
the synapse [15]. The two classes of learning rule are radically different; so if synaptic correlations
matter during retrieval in both cases  then the conclusion likely applies in general.

1Complete connectivity simpliﬁes the computation of the parameters for the optimal dynamics for cascade-

like learning rules considered in the following  but is not necessary for the theory.

2

Figure 1: Memory recall as inference and additive learning rules. a. Top: Synaptic weights 
W  arise by storing the target pattern x together with T −1 other patterns  {x(t)}t=1...T−1. During
recall  the cue  ˜x  is a noisy version of the target pattern. The task of recall is to infer x given W
and ˜x (by marginalising out {x(t)}). Bottom: The activity of neuron i across the stored patterns is
a source of shared variability between synapses connecting it to neurons j and k. b-c. Covariance
rule: patterns of synaptic correlations and recall performance for retrieval dynamics ignoring or
considering synaptic correlations; T = 5. d-e. Same for the simple Hebbian learning rule. The
control is an optimal decoder that ignores W.
3 Additive learning rules

i

Wij =(cid:80)

  x(t)

t Ω(x(t)

Local additive learning rules assume that synaptic changes induced by different activity patterns
combine additively; such that storing a sequence of T patterns from Pstore(x)  results in weights
j )  with function Ω(xi  xj) describing the change in synaptic strength induced
by presynaptic activity xj and postsynaptic activity xi. We consider a generalized Hebbian form for
this function  with Ω (xi  xj) = (xi − α)(xj − β). This class includes  for example  the covariance
rule (α = β = f)  classically used in Hopﬁeld networks  or simple Hebbian learning (α = β = 0).
As synaptic changes are deterministic  the only source of uncertainty in the distribution P(W|x)
is the identity of the other stored patterns. To estimate this  let us ﬁrst consider the distribution of
the weights after storing one random pattern from Pstore(x). The mean µ and covariance C of the
weight change induced by this event can be computed as:2

Pstore(x)(cid:0)Ω|(x) · Ω|(x)T(cid:1) dx − µ · µT

µ =

Pstore(x)Ω|(x)dx 

C =

(cid:90)

(cid:90)

Since the rule is additive and the patterns are independent  the mean and covariance scale linearly
with the number of intervening patterns. Hence  the distribution over possible weight values at
recall  given that pattern x is stored along with T − 1 other  random  patterns has mean µW =
Ω(x) + (T − 1) · µ  and covariance CW = (T − 1) · C. Most importantly  because the rule is
additive  in the limit of many stored patterns (and in practice even for modest values of T )  the
distribution P(W|x) approaches a multivariate Gaussian that is characterized completely by these
two quantities; moreover  its covariance is independent of x.
For retrieval dynamics based on Gibbs sampling  the key quantity is the log-odds ratio

(3)

(4)

(cid:18) P(xi = 1|x¬i  W  ˜x)

P(xi = 0|x¬i  W  ˜x)

(cid:19)

Ii = log

for neuron i  which could be represented by the total current entering the unit. This would translate
into a probability of ﬁring given by the sigmoid activation function f (Ii) = 1/(1 + e−Ii ).
The total current entering a neuron is a sum of two terms: one term from the external input of the
form c1 · ˜xi + c2 (with constants c1 and c2 determined by parameters f and r [16])  and one term
from the recurrent input  of the form:

(cid:17)−(cid:16)

(cid:17)T

C−1(cid:16)

W| − µ(0)

W

W| − µ(1)

W

W| − µ(1)

W

(5)

(cid:17)(cid:19)

(cid:18)(cid:16)

(cid:17)T

C−1(cid:16)

I rec
i =

1

2(T −1)

W| − µ(0)

W

2For notational convenience  we use a column-vector form of the matrix of weight changes Ω  and the

weight matrix W  marked by subscript |.

3

00.51cortical data (Song 2005)simple Hebb rulecorrcovariance ruleabde25501000102030error (%)controlN25501000510simple (ignoring correlations)exact (considering correlations)error (%)controlN00.51corrcW = Ω|(x(0/1))+(T−1)µ and x(0/1) is the vector of activities obtained from x in which
where µ(0/1)
the activity of neuron i is set to 0  or 1  respectively.
It is easy to see that for the covariance rule  Ω (xi  xj) = (xi − f )(xj − f )  synapses sharing
a single pre- or post-synaptic partner happen to be uncorrelated (Fig. 1b). Moreover  as for any
(anti-)symmetric additive learning rule  reciprocal connections are perfectly correlated (Wij = Wji).
The (non-degenerate part of the) covariance matrix in this case becomes diagonal  and the total
current in optimal retrieval reduces to simple linear dynamics :

(cid:88)

j

(cid:88)
(cid:123)(cid:122)

j

Wij

(cid:125)

xj

(cid:125)

− f

(cid:124)

− f 2 1 − 2f
(cid:124)
(cid:123)(cid:122)
(cid:125)

2

constant

(cid:19)

(6)

Ii =

1

(T − 1) σ2

W

(cid:18)(cid:88)
(cid:124)

j

− (1 − 2f )2
(cid:123)(cid:122)
(cid:124)

2

(cid:125)

Wijxj

(cid:123)(cid:122)

recurrent input

feedback inhibition

homeostatic term

where σ2
W is the variance of a synaptic weight resulting from storing a single pattern. This term
includes a contribution from recurrent excitatory input  dynamic feedback inhibition (proportional
to the total population activity) and a homeostatic term that reduces neuronal excitability as function
of the net strength of its synapses (a proxy for average current the neuron expects to receive) [17].
Reassuringly  the optimal decoder for the covariance rule recovers a form for the input current that is
closely related to classic Hopﬁeld-like [5] dynamics (with external ﬁeld [1  18]): feedback inhibition
is needed only when the stored patterns are not balanced (f (cid:54)= 0.5); for the balanced case  the
homeostatic term can be integrated in the recurrent current  by rewriting neural activities as spins.
In sum  for the covariance rule  synapses are fortuitously uncorrelated (except for symmetric pairs
which are perfectly correlated)  and thus simple  classical linear recall dynamics sufﬁce (Fig. 1c).
The covariance rule is  however  the exception rather than the rule. For example  for simple Hebbian
learning  Ω (xi  xj) = xi·xj  synapses sharing a pre- or post-synaptic partner are correlated (Fig. 1d)
Interestingly  the ﬁnal expression of the
and so the covariance matrix C is no longer diagonal.
recurrent current to a neuron remains strictly local (because of additivity and symmetry)  and very
similar to Eq. 6  but feedback inhibition becomes a non-linear function of the total activity in the
network [16]. In this case  synaptic correlations have a dramatic effect: using the optimal non-linear
dynamics ensures high performance  but trying to retrieve information using a decoder that assumes
synaptic independence (and thus uses linear dynamics) yields extremely poor performance  which
is even worse than the obvious control of relying only on the information in the recall cue and the
prior over patterns (Fig. 1e).
For the generalized Hebbian case  Ω (xi  xj) = (xi−α)(xj−β) with α(cid:54)= β  the optimal decoder be-
comes even more complex  with the total current including additional terms accounting for pairwise
correlations between any two synapses that have neuron i as a pre- or post-synaptic partner [16].
Hence  retrieval is no longer strictly local3 and a biological implementation will require approximat-
ing the contribution of non-local terms as a function of locally available information  as we discuss
in detail for palimpsest learning below.

4 Palimpsest learning rules

Though additive learning rules are attractive for their analytical tractability  they ignore several im-
portant aspects of synaptic plasticity  e.g. they assume that synapses can grow without bound. We
investigate the effects of bounded weights by considering another class of learning rules  which as-
sumes synaptic efﬁcacies can only take binary values  with stochastic transitions between the two
underpinned by paired cascades of latent internal states [14] (Fig. 2). These learning rules  though
very simple  capture an important aspect of memory – the fact that memory is leaky  and information
about the past is overwritten by newly stored items (usually referred to as the palimpsest property).
Additionally  such rules can account for experimentally observed synaptic metaplasticity [15].

3For additive learning rules  the current to neuron i always depends only on synapses local to a neuron  but
these can also include outgoing synapses of which the weight  W·i  should not inﬂuence its dynamics. We refer
to such dynamics as ‘semi-local’. For other learning rules  the optimal current to neuron i may depend on all
connections in the network  including Wjk with j  k(cid:54)= i (‘non-local’ dynamics).

4

Figure 2: Palimpsest learning. a. The cascade model. Colored circles are latent states (V ) that
belong to two different synaptic weights (W )  arrows are state transitions (blue: depression  red:
potentiation) b. Different variants of mapping pre- and post-synaptic activations to depression (D)
and potentiation (P): R1–postsynaptically gated  R2–presynaptically gated  R3–XOR rule. c. Cor-
relation structure induced by these learning rules. c. Retrieval performance for each rule.
Learning rule

Learning is stochastic and local  with changes in the state of a synapse Vij being determined only by
the activation of the pre- and post-synaptic neurons  xj and xi. In general  one could deﬁne separate
transition matrices for each activity pattern  M(xi  xj)  describing the probability of a synaptic state
transitioning between any two states Vij to V (cid:48)
ij following an activity pattern  (xi  xj). For simplicity 
we deﬁne only two such matrices  for potentiation  M+  and depression  M−  respectively  and then
map different activity patterns to these events. In particular  we assume Fusi’s cascade model [14]4
and three possible mappings (Fig. 2b [16]): 1) a postsynaptically gated learning rule  where changes
occur only when the postsynaptic neuron is active  with co-activation of pre- and post- neuron lead-
ing to potentiation  and to depression otherwise5; 2) a presynaptically gated learning rule  typically
assumed when analysing cascades[20  21]; and 3) an XOR-like learning rule which assumes po-
tentiation occurs whenever the pre- and post- synaptic activity levels are the same  with depression
otherwise. The last rule  proposed by Ref. 22  was speciﬁcally designed to eliminate correlations
between synapses  and can be viewed as a version of the classic covariance rule fashioned for binary
synapses.

Estimating the mean and covariance of synaptic weights

ages over possible neural activity patterns: M = (cid:80)

At the level of a single synapse  the presentation of a sequence of uncorrelated patterns from
Pstore(x) corresponds to a Markov random walk  deﬁned by a transition matrix M  which aver-
Pstore(xi) · Pstore(xj) · M(xi  xj). The
distribution over synaptic states t steps after the initial encoding can be calculated by starting from
the stationary distribution of the weights πV 0 (assuming a large number of other patterns have pre-
viously been stored; formally  this is the eigenvector of M corresponding to eigenvalue λ = 1)  then
storing the pattern (xi  xj)  and ﬁnally t − 1 other patterns from the prior:

xi xj

t−1 · M(xi  xj) · πV 0 

πV (xi  xj  t) = M

(7)
l = P(Vij = l|xi  xj)  l ∈ {1 . . . 2n} 
with the distribution over states given as a column vector  πV
where n is the depth of the cascade. Lastly  the distribution over weights  P(Wij|xi  xj)  can be
derived as πW = MV →W · πV   where MV →W is a deterministic map from states to observed
weights (Fig. 2a).
As in the additive case  the states of synapses sharing a pre- or post- synaptic partner will be corre-
lated (Figs. 1a  2c). The degree of correlations for different synaptic conﬁgurations can be estimated
by generalising the above procedure to computing the joint distribution of the states of pairs of
synapses  which we represent as a matrix ρ. E.g. for a pair of synapses sharing a postsynaptic
partner (Figs. 1b  d  and 2c)  element (u  v) is ρuv = P(Vpost pre1 = u  Vpost pre2 = v). Hence  the
presentation of an activity pattern (xpre1  xpre2  xpost) induces changes in the corresponding pair of

4Other models  e.g. serial [19]  could be used as well without qualitatively affecting the results.
5One could argue that this is the most biologically relevant as plasticity is often NMDA-receptor dependent 

and hence it requires postsynaptic depolarisation for any effect to occur.

5

acorrelation coefficientcortex data (Song 2005)PDPD--PDprepost0011R1R3--PDR2bsimpledynamicscorr-dependentdynamicsd00.20.4c010200102001020error (%) correlated synapsesexactapprox**pseudo-storage00.30.600.20.4where ˆM(xi) = (cid:80)

incoming synapses to neuron post as ρ(1) = M(xpost  xpre1) · ρ(0) · M(xpost  xpre2)T  where ρ(0)
is the stationary distribution corresponding to storing an inﬁnite number of triplets from the pattern
distribution [16].
Replacing πV with ρ (which is now a function of the triplet (xpre1  xpre2  xpost))  and the multipli-
cation by M with the slightly more complicated operator above  we can estimate the evolution of
the joint distribution over synaptic states in a manner very similar to Eq. 7:
Pstore(xi) · ˆM(xi) · ρ(t−1) · ˆM(xi)T 

(cid:88)

ρ(t) =

(8)

xi

xj

Pstore(xj)M(xi  xj). Also as above  the ﬁnal joint distribution over states
V →W . This

can be mapped into a joint distribution over synaptic weights as MV →W · ρ(t) · MT
approach can be naturally extended to all other correlated pairs of synapses [16].
The structure of correlations for different synaptic pairs varies signiﬁcantly as a function of the
learning rule (Fig. 2c)  with the overall degree of correlations depending on a range of factors.
Correlations tend to decrease with cascade depth and pattern sparsity. The ﬁrst two variants of the
learning rule considered are not symmetric  and so induce different patterns of correlations than the
additive rules above. The XOR rule is similar to the covariance rule  but the reciprocal connections
are no longer perfectly correlated (due to metaplasticity)  which means that it is no longer possible
to factorize P(W|x). Hence  assuming independence at decoding seems bound to introduce errors.

Approximately optimal retrieval when synapses are independent

(cid:81)
the evidence from the weights factorizes  P(W|x) =
If we ignore synaptic correlations 
i j P(Wij|xi  xj)  and so the exact dynamics would be semi-local3. We can further approximate
the contribution of the outgoing weights by its mean  which recovers the same simple dynamics
derived for the additive case:

(cid:18) P(xi = 1|x¬i  W  ˜x)

P(xi = 0|x¬i  W  ˜x)

(cid:19)

(cid:88)

j

Ii = log

= c1

Wijxj + c2

Wij + c3

j

j

xj + c4 ˜xi + c5 (9)

(cid:88)

(cid:88)

The parameters c. depend on the prior over x  the noise model  the parameters of the learning rule
and t. Again  the optimal decoder is similar to previously derived attractor dynamics; in particular 
for stochastic binary synapses with presynaptically gated learning the optimal dynamics require
dynamic inhibition only for sparse patterns  and no homeostatic term  as used in [21] .
To validate these dynamics  we remove synaptic correlations by a pseudo-storage procedure in which
synapses are allowed to evolve independently according to transition matrix M  rather than changing
as actual intermediate patterns are stored. The dynamics work well in this case  as expected (Fig. 2d 
blue bars). However  when storing actual patterns drawn from the prior  performance becomes ex-
tremely poor  and often worse than the control (Fig. 2d  gray bars). Moreover  performance worsens
as the network size increases (not shown). Hence  ignoring correlations is highly detrimental for this
class of learning rules too.

(cid:88)

(cid:18)(cid:88)

Approximately optimal retrieval when synapses are correlated
To accommodate synaptic correlations  we approximate P(W|x) with a maximum entropy dis-
tribution with the same marginals and covariance structure  ignoring the higher order moments.6
Speciﬁcally  we assume the evidence from the weights has the functional form:

P(W|x  t) =

1

Z(x  t)

exp

kij(x  t) · Wij +

1
2

ij

J(ij)(kl)(x  t) · WijWkl

ijkl

(10)

We use the TAP mean-ﬁeld method [23] to ﬁnd parameters k and J and the partition function  Z 
for each possible activity pattern x  given the mean and covariance for the synaptic weights matrix 
computed above7 [16].

6This is just a generalisation of the simple dynamics which assume a ﬁrst order max entropy model; more-
over  the resulting weight distribution is a binary analog of the multivariate normal used in the additive case 
allowing the two to be directly compared.

7Here  we ask whether it is possible to accommodate correlations in appropriate neural dynamics at all 

ignoring the issue of how the optimal values for the parameters of the network dynamics would come about.

6

(cid:19)

i

number of coactivated inputs (cid:80)

j Wijxj; lines: different levels of neural excitability(cid:80)

Figure 3: Implications for neural dynamics. a. R1: parameters for I rec
; linear modulation by
network activity  nb. b. R2: nonlinear modulation of pairwise term by network activity (cf. middle
panel in a); other parameters have linear dependences on nb. c. R1: Total current as function of
j Wij  line
widths scale with frequency of occurrence in a sample run. d. Same for R2. e. Nonlinear integration
in dendrites  reproduced from [11]  cf. curves in c.
Exact retrieval dynamics based on Eq. 10  but not respecting locality constraints  work substantially
better in the presence of synaptic correlations  for all rules (Fig. 2d  yellow bars). It is important to
note that for the XOR rule  which was supposed to be the closest analog to the covariance rule and
hence afford simple recall dynamics [22]  error rates stay above control  suggesting that it is actually
a case in which even dependencies beyond 2nd-order correlation would need to be considered.
As in the additive case  exact recall dynamics are biologically implausible  as the total current to
the neuron depends on the full weight matrix. It is possible to approximate the dynamics using
strictly local information by replacing the nonlocal term by its mean  which  however  is no longer a
j(cid:54)=i xj [16]. Under
this approximation  the current from recurrent connections corresponding to the evidence from the
weights becomes:

constant  but rather a linear function of the total activity in the network  nb =(cid:80)

(cid:52)
(ij)(ik)(x)WijWik − Z(cid:52)

J

(11)

(cid:18) P(W|x(1))

P(W|x(0))

(cid:19)

(cid:88)

=

I rec
i = log

(cid:88)

1
2

jk

(cid:52)
k
ij (x)Wij +

j

where i is the index of the neuron to be updated  and x(0/1) activity vector has the to-be-updated
neuron’s activity set to 1 or 0  respectively  and all other components given by the current network
state. The functions k

(cid:0)x(0)(cid:1) 
and Z(cid:52) = log(cid:0)Z(cid:0)x(1)(cid:1)(cid:1) − log(cid:0)Z(cid:0)x(0)(cid:1)(cid:1) depend on the local activity at the indexed synapses 

(cid:0)x(1)(cid:1)−J(ij)(kl)

(cid:52)
ij (x) = kij(x(1))−kij(x(0))  J

(cid:52)
(ij)(kl)(x) = J(ij)(kl)

(formally  quadratically) on the number of co-active inputs  nW 1 = (cid:80)

modulated by the number of active neurons in the network  nb. This approximation is again consis-
tent with our previous analysis  i.e. in the absence of synaptic correlations  the complex dynamics
recover the simple case presented before. Importantly  this approximation also does about as well as
exact dynamics (Fig. 2d  red bars).
For post-synaptically gated learning  comparing the parameters of the dynamics in the case of inde-
pendent versus correlated synapses (Fig. 3a) reveals a modest modulation of the recurrent input by
the total activity. More importantly  the net current to the postsynaptic neuron depends non-linearly
j xjWij  (Fig. 3c)  which
is reminiscent of experimentally observed dendritic non-linearities [11] (Fig. 3e). Conversely  for
the presynaptically gated learning rule  approximately optimal dynamics predict a non-monotonic
modulation of activity by lateral inhibition (Fig. 3b)  but linear neural integration (Fig. 3d).8 Lastly 
retrieval based on the XOR rule has the same form as the simple dynamics derived for the factorized
case [16]. However  the total current has to be rescaled to compensate for the correlations introduced
by reciprocal connections.

8The difference between the two rules emerges exclusively because of the constraint of strict locality of the

approximation  since the exact form of the dynamics is essentially the same for the two.

7

01020−0.500.501020−10−50501020−0.0500.05024681012−20612number of coactive inputspostsynaptic currentacdno corrcorrnumber of coactive inputspostsynaptic current024681012−10−5051001020−0.0100.01beTIPMIDDLEBASEnumber of inputsnormalized EPSP0.00.20.40.60.81.001234567additive

cascade

RULE
covariance
simple Hebbian
generalized Hebbian
presyn. gated
postsyn. gated
XOR

EXACT DYNAMICS
strictly local  linear
strictly local  nonlinear
semi-local  nonlinear
nonlocal  nonlinear
nonlocal  nonlinear
beyond correlations

NEURAL IMPLEMENTATION

linear feedback inh.  homeostasis
nonlinear feedback inh.
nonlinear feedback inh.
nonlinear feedback inh.  linear dendritic integr.
linear feedback inh.  non-linear dendritic integr.
?

Table 1: Results summary: circuit adaptations against correlations for different learning rules.

5 Discussion

Statistical dependencies between synaptic efﬁcacies are a natural consequence of activity dependent
synaptic plasticity  and yet their implications for network function have been unexplored. Here  in
the context of an auto-associative memory network  we investigated the patterns of synaptic corre-
lations induced by several well-known learning rules and their consequent effects on retrieval. We
showed that most rules considered do indeed induce synaptic correlations and that failing to take
them into account greatly damages recall. One fortuitous exception is the covariance rule  for which
there are no synaptic correlations. This might explain why the bulk of classical treatments of auto-
associative memories  using the covariance rule  could achieve satisfying capacity levels despite
overlooking the issue of synaptic correlations [5  24  25].

In general  taking correlations into account optimally during recall requires dynamics in which there
are non-local interactions between neurons. However  we derived approximations that perform well
and are biologically realisable without such non-locality (Table 1). Examples include the modula-
tion of neural responses by the total activity of the population  which could be mediated by feedback
inhibition  and speciﬁc dendritic nonlinearities. In particular  for the post-synaptically gated learn-
ing rule  which may be viewed as an abstract model of hippocampal NMDA receptor-dependent
plasticity  our model predicts a form of non-linear mapping of recurrent inputs into postsynaptic
currents which is similar to experimentally observed dendritic integration in cortical pyramidal cells
[11]. In general  the tight coupling between the synaptic plasticity used for encoding (manifested
in patterns of synaptic correlations) and circuit dynamics offers an important route for experimental
validation [2].
None of the rules governing synaptic plasticity that we considered perfectly reproduced the pattern
of correlations in [6]; and indeed  exactly which rule applies in what region of the brain under which
neuromodulatory inﬂuences is unclear. Furthermore  results in [6] concern the neocortex rather
than the hippocampus  which is a more common target for models of auto-associative memory.
Nonetheless  our analysis has shown that synaptic correlations matter for a range of very different
learning rules that span the spectrum of empirical observations.
Another strategy to handle the negative effects of synaptic correlations is to weaken or eliminate
them. For instance  in the palimpsest synaptic model [14]  the deeper the cascade  the weaker the
correlations  and so metaplasticity may have the beneﬁcial effect of making recall easier. Another 
popular  idea is to use very sparse patterns [21]  although this reduces the information content of
each one. More speculatively  one might imagine a process of off-line synaptic pruning or recoding 
in which strong correlations are removed or the weights adjusted so that simple recall methods will
work.
Here  we focused on second-order correlations. However  for plasticity rules such as XOR  we
showed that this does not sufﬁce. Rather  higher-order correlations would need to be considered 
and thus  presumably higher-order interactions between neurons approximated. Finally  we know
from work on neural coding of sensory stimuli that there are regimes in which correlations either
help or hurt the informational quality of the code  assuming that decoding takes them into account.
Given our results  it becomes important to look at the relative quality of different plasticity rules 
assuming realizable decoding – it is not clear whether rules that strive to eliminate correlations will
be bested by ones that do not.
Acknowledgments This work was supported by the Wellcome Trust (CS  ML)  the Gatsby Chari-
table Foundation (PD)  and the European Union Seventh Framework Programme (FP7/2007–2013)
under grant agreement no. 269921 (BrainScaleS) (ML).

8

References
1. Sommer  F.T. & Dayan  P. Bayesian retrieval in associative memories with storage errors. IEEE transac-

tions on neural networks 9  705–713 (1998).

2. Lengyel  M.  Kwag  J.  Paulsen  O. & Dayan  P. Matching storage and recall: hippocampal spike timing-

dependent plasticity and phase response curves. Nature Neuroscience 8  1677–1683 (2005).

3. Lengyel  M. & Dayan  P. Uncertainty  phase and oscillatory hippocampal recall. Advances in Neural

Information Processing (2007).

4. Savin  C.  Dayan  P. & Lengyel  M. Two is better than one: distinct roles for familiarity and recollection in
retrieving palimpsest memories. in Advances in Neural Information Processing Systems  24 (MIT Press 
Cambridge  MA  2011).

5. Hopﬁeld  J.J. Neural networks and physical systems with emergent collective computational abilities.

Proc. Natl. Acad. Sci. USA 76  2554–2558 (1982).

6. Song  S.  Sj¨ostr¨om  P.J.  Reigl  M.  Nelson  S. & Chklovskii  D.B. Highly nonrandom features of synaptic

connectivity in local cortical circuits. PLoS biology 3  e68 (2005).

7. Dayan  P. & Abbott  L. Theoretical Neuroscience (MIT Press  2001).
8. Averbeck  B.B.  Latham  P.E. & Pouget  A. Neural correlations  population coding and computation.

Nature Reviews Neuroscience 7  358–366 (2006).

9. Pillow  J.W. et al. Spatio-temporal correlations and visual signalling in a complete neuronal population.

Nature 454  995–999 (2008).

10. Latham  P.E. & Nirenberg  S. Synergy  redundancy  and independence in population codes  revisited.

Journal of Neuroscience 25  5195–5206 (2005).

11. Branco  T. & H¨ausser  M. Synaptic integration gradients in single cortical pyramidal cell dendrites. Neuron

69  885–892 (2011).

12. Hasselmo  M.E. & Bower  J.M. Acetylcholine and memory. Trends Neurosci. 16  218–222 (1993).
13. MacKay  D.J.C. Maximum entropy connections: neural networks.

in Maximum Entropy and Bayesian
Methods  Laramie  1990 (eds. Grandy  Jr  W.T. & Schick  L.H.) 237–244 (Kluwer  Dordrecht  The Nether-
lands  1991).

14. Fusi  S.  Drew  P.J. & Abbott  L.F. Cascade models of synaptically stored memories. Neuron 45  599–611

(2005).

15. Abraham  W.C. Metaplasticity: tuning synapses and networks for plasticity. Nature Reviews Neuroscience

9  387 (2008).

16. For details  see Supplementary Information.
17. Zhang  W. & Linden  D. The other side of the engram: experience-driven changes in neuronal intrinsic

excitability. Nature Reviews Neuroscience (2003).

18. Engel  A.  Englisch  H. & Sch¨utte  A. Improved retrieval in neural networks with external ﬁelds. Euro-

physics Letters (EPL) 8  393–397 (1989).

19. Leibold  C. & Kempter  R. Sparseness constrains the prolongation of memory lifetime via synaptic meta-

plasticity. Cerebral cortex (New York  N.Y. : 1991) 18  67–77 (2008).

20. Amit  Y. & Huang  Y. Precise capacity analysis in binary networks with multiple coding level inputs.

Neural computation 22  660–688 (2010).

21. Huang  Y. & Amit  Y. Capacity analysis in multi-state synaptic models: a retrieval probability perspective.

Journal of computational neuroscience (2011).

22. Dayan Rubin  B. & Fusi  S. Long memory lifetimes require complex synapses and limited sparseness.

Frontiers in Computational Neuroscience (2007).

23. Thouless  D.J.  Anderson  P.W. & Palmer  R.G. Solution of ’Solvable model of a spin glass’. Philosophical

Magazine 35  593–601 (1977).

24. Amit  D.  Gutfreund  H. & Sompolinsky  H. Storing inﬁnite numbers of patterns in a spin-glass model of

neural networks. Phys Rev Lett 55  1530–1533 (1985).

25. Treves  A. & Rolls  E.T. What determines the capacity of autoassociative memories in the brain? Network

2  371–397 (1991).

9

,Cristina Savin
Peter Dayan
Mate Lengyel