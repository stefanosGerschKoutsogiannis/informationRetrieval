2012,Accelerated Training for Matrix-norm Regularization: A Boosting Approach,Sparse learning models typically combine a smooth loss with a nonsmooth penalty  such as trace norm. Although recent developments in sparse approximation have offered promising solution methods  current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates. In this paper  we propose a boosting method for regularized learning that guarantees $\epsilon$ accuracy within $O(1/\epsilon)$ iterations. Performance is further accelerated by interlacing boosting with fixed-rank local optimization---exploiting a simpler local objective than previous work. The proposed method yields state-of-the-art performance on large-scale problems. We also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle.,Accelerated Training for Matrix-norm
Regularization: A Boosting Approach

Department of Computing Science  University of Alberta  Edmonton AB T6G 2E8  Canada

Xinhua Zhang∗  Yaoliang Yu and Dale Schuurmans
{xinhua2 yaoliang dale}@cs.ualberta.ca

Abstract

Sparse learning models typically combine a smooth loss with a nonsmooth
penalty  such as trace norm. Although recent developments in sparse approxi-
mation have offered promising solution methods  current approaches either apply
only to matrix-norm constrained problems or provide suboptimal convergence
rates. In this paper  we propose a boosting method for regularized learning that
guarantees  accuracy within O(1/) iterations. Performance is further accelerated
by interlacing boosting with ﬁxed-rank local optimization—exploiting a simpler
local objective than previous work. The proposed method yields state-of-the-art
performance on large-scale problems. We also demonstrate an application to la-
tent multiview learning for which we provide the ﬁrst efﬁcient weak-oracle.

Introduction

1
Our focus in this paper is on unsupervised learning problems such as matrix factorization or latent
subspace identiﬁcation. Automatically uncovering latent factors that reveal important structure in
data is a longstanding goal of machine learning research. Such an analysis not only provides un-
derstanding  it can also facilitate subsequent data storage  retrieval and processing. We focus in
particular on coding or dictionary learning problems  where one seeks to decompose a data matrix
X into an approximate factorization ˆX = U V that minimizes reconstruction error while satisfying
other properties like low rank or sparsity in the factors. Since imposing a bound on the rank or
number of non-zero elements generally makes the problem intractable  such constraints are usually
replaced by carefully designed regularizers that promote low rank or sparse solutions [1–3].
Interestingly  for a variety of dictionary constraints and regularizers  the problem is equivalent to
a matrix-norm regularized problem on the reconstruction matrix ˆX [1  4]. One intensively studied
example is the trace norm  which corresponds to bounding the Euclidean norm of the code vectors in
U while penalizing V via its (cid:96)21 norm. To solve trace norm regularized problems  variational meth-
ods that optimize over U and V only guarantee local optimality  while proximal gradient algorithms
that operate on ˆX [5  6] can achieve an  accurate (global) solutions in O(1/
) iterations  but these
require singular value thresholding [7] at each iteration  preventing application to large problems.
Recently  remarkable promise has been demonstrated for sparse approximation methods. [8] con-
verts the trace norm problem into an optimization over positive semideﬁnite (PSD) matrices  then
solves the problem via greedy sparse approximation [9  10]. [11] further generalizes the algorithm
from trace norm to gauge functions [12]  dispensing with the PSD conversion. However  these
schemes turn the regularization into a constraint. Despite their theoretical equivalence  many practi-
cal applications require the solution to the regularized problem  e.g. when nested in another problem.
In this paper  we optimize the regularized objective directly by reformulating the problem in the
framework of (cid:96)1 penalized boosting [13  14]  allowing it to be solved with a general procedure de-
veloped in Section 2. Each iteration of this procedure calls an oracle to ﬁnd a weak hypothesis

√

∗Xinhua Zhang is now at the National ICT Australia (NICTA)  Machine Learning Group.

1

(typically a rank-one matrix) yielding the steepest local reduction of the (unregularized) loss. The
associated weight is then determined by accounting for the (cid:96)1 regularization. Our ﬁrst key contri-
bution is to establish that  when the loss is convex and smooth  the procedure ﬁnds an  accurate
solution within O(1/) iterations. To the best of our knowledge  this is the ﬁrst O(1/) objective
value rate that has been rigorously established for (cid:96)1 regularized boosting. [15] considered a similar
boosting approach  but required totally corrective updates. In addition  their rate characterizes the
diminishment of the gradient  and is O(1/2) as opposed to O(1/) established here. [9–11  16–18]
establish similar rates  but only for the constrained version of the problem.
We also show in Section 3 how the empirical performance of (cid:96)1 penalized boosting can be greatly im-
proved by introducing an auxiliary rank-constrained local-optimization within each iteration. Inter-
lacing rank constrained optimization with sparse updates has been shown effective in semi-deﬁnite
programming [19–21]. [22] applied the idea to trace norm optimization by factoring the reconstruc-
tion matrix into two orthonormal matrices and a positive semi-deﬁnite matrix. Unfortunately  this
strategy creates a very difﬁcult constrained optimization problem  compelling [22] to resort to man-
ifold techniques. Instead  we use a simpler variational representation of matrix norms that leads to
a new local objective that is both unconstrained and smooth. This allows the application of much
simpler and much more efﬁcient solvers to greatly accelerate the overall optimization.
Underlying standard sparse approximation methods is an oracle that efﬁciently selects a weak hy-
pothesis (using boosting terminology). Unfortunately these oracle problems are extremely challeng-
ing except in limited cases [3  11]. Our next major contribution  in Section 4  is to formulate an
efﬁcient oracle for latent multiview factorization models [2  4]  based on a positive semi-deﬁnite
relaxation that we prove incurs no gap.
Finally  we point out that our focus in this paper is on the optimization of convex problems that relax
the “hard” rank constraint. We do not explicitly minimize the rank  which is different from [23].
Notation We use γK to denote the gauge induced by set K; (cid:107)·(cid:107)∗ to denote the dual norm of (cid:107)·(cid:107);
and (cid:107)·(cid:107)F   (cid:107)·(cid:107)tr and (cid:107)·(cid:107)sp to denote the Frobenius norm  trace norm and spectral norm respectively.
i (cid:107)Xi:(cid:107)R  while (cid:104)X  Y (cid:105) := tr(X(cid:48)Y ) denotes the inner prod-
uct. The notation X (cid:60) 0 will denote positive semi-deﬁnite; X:i and Xi: stands for the i-th column
and i-th row of matrix X; and diag {ci} denotes a diagonal matrix with the (i  i)-th entry ci.
2 The Boosting Framework with (cid:96)1 Regularization
Consider a coding problem where one is presented an n×m matrix Z  whose columns correspond to
m training examples. Our goal is to learn an n×k dictionary matrix U  consisting of k basis vectors 
and a k × m coefﬁcient matrix V   such that U V approximates Z under some loss L(U V ). We sup-
press the dependence on the data Z throughout the paper. To remove the scaling invariance between
U and V   it is customary to restrict the bases  i.e. columns of U  to the unit ball of some norm (cid:107)·(cid:107)C.
Unfortunately  for a ﬁxed k  this coding problem is known to be computationally tractable only for
the squared loss. To retain tractability for a variety of convex losses  a popular and successful recent
approach has been to avoid any “hard” constraint on the number of bases  i.e. k  and instead impose
regularizers on the matrix V that encourage a low rank or sparse solution.
To be more speciﬁc  the following optimization problem lies at the heart of many sparse learning
models [e.g. 1  3  4  24  25]:

(cid:107)X(cid:107)R 1 denotes the row-wise norm(cid:80)

(1)
where λ ≥ 0 speciﬁes the tradeoff between loss and regularization. The (cid:107)·(cid:107)R norm in the block R-1
norm provides the ﬂexibility of promoting useful structures in the solution  e.g. (cid:96)1 norm for sparse
solutions  (cid:96)2 norm for low rank solutions  and block structured norms for group sparsity. To solve
(1)  we ﬁrst reparameterize the rows of ˜V by ˜Vi: = σiVi:  where σi ≥ 0 and (cid:107)Vi:(cid:107)R ≤ 1. Now (1)
can be reformulated by introducing the reconstruction matrix X := U ˜V :
(1) = min
(2)
X
where Σ = diag{σi}  and U and V in the last minimization also carry norm constraints. (2) is
illuminating in two respects. First it reveals that the regularizer essentially seeks a rank-one decom-
position of the reconstruction matrix X  and penalizes the (cid:96)1 norm of the combination coefﬁcients
as a proxy of the “rank”. Second  the regularizer in (2) is now expressed precisely in the form of the

(cid:107) ˜V (cid:107)R 1 = min

L(U ˜V ) + λ(cid:107) ˜V (cid:107)R 1 

U  ˜V :(cid:107)U:i(cid:107)C≤1 U ˜V =X

min

U :(cid:107)U:i(cid:107)C≤1

min

˜V

min

σ U V :σ≥0 U ΣV =X

(cid:88)

i

L(X) + λ

min

L(X) + λ

X

σi 

2

(cid:104)∇L(Xk−1)  H(cid:105).

Algorithm 1: The vanilla boosting algorithm.
Require: The weak hypothesis set A in (3).
1: Set X0 = 0  s0 = 0.
2: for k = 1  2  . . . do
3: Hk ← argmin
H∈A
(ak  bk) ←
4:
argmin
a≥0 b≥0
i ← akσ(k−1)
σ(k)
k ← bk  A(k)
σ(k)
i=1 σ(k)
i=1 σ(k)

  A(k)
k ← Hk.
i A(k)
i = aksk−1 + bk.

6: Xk ←(cid:80)k
sk ←(cid:80)k

L(aXk−1+bHk) + λ(ask +b).
  ∀ i < k

i = akXk−1+bkHk 

i ← A(k−1)

5:

i

i

7: end for

4:

Algorithm 2: Boosting with local search.
Require: A set of weak hypotheses A.
1: Set X0 = 0  U0 = V0 = Λ0 = [ ]  s0 = 0.
2: for k = 1  2  . . . do
(cid:104)∇L(Xk−1)  uv(cid:48)(cid:105).
3:

(uk  vk) ← argmin
uv(cid:48)∈A
(ak  bk) ←
argmin
a≥0 b≥0
Uinit ← ( ˆUk−1

(cid:112)akΛk−1 
Vinit ← ((cid:112)akΛk−1 ˆVk−1 

L(aXk−1+b ukv(cid:48)
k)+λ(ask+b).
√
bkuk) 
√
bkvk)(cid:48).
Locally optimize g(U  V ) with initial
6:
value (Uinit  Vinit). Get a solution (Uk Vk).
7: Xk ← UkVk  Λk ← diag{(cid:107)U:i(cid:107)C(cid:107)Vi:(cid:107)R} 
sk ← 1
8: end for

(cid:80)k
i=1((cid:107)U:i(cid:107)2

C + (cid:107)Vi:(cid:107)2
R).

5:

2

gauge function γK induced by the convex hull K of the set1

(3)
Since K is convex and symmetric (−K = K)  the gauge function γK is in fact a norm  hence the
support function of A deﬁnes the dual norm ||| · ||| (see e.g. [26  Proposition V.3.2.1]):

A = {uv(cid:48) : (cid:107)u(cid:107)C ≤ 1 (cid:107)v(cid:107)R ≤ 1}.

(cid:107)Λ(cid:48)u(cid:107)∗

(cid:107)Λv(cid:107)∗
C  

u(cid:48)Λv = max

|||Λ||| := max

X∈A tr(X(cid:48)Λ) =

max

v:(cid:107)v(cid:107)R≤1

u:(cid:107)u(cid:107)C≤1

R = max

u v:(cid:107)u(cid:107)C≤1 (cid:107)v(cid:107)R≤1

(4)
and the gauge function γK is simply its dual norm |||·|||∗. For example  when (cid:107)·(cid:107)R = (cid:107)·(cid:107)C = (cid:107)·(cid:107)2 
we have ||| · ||| = (cid:107) · (cid:107)sp  so the regularizer (as the dual norm) becomes (cid:107) · (cid:107)tr. Another special
case of this result was found in [4  Theorem 1]  where again (cid:107) · (cid:107)R = (cid:107) · (cid:107)2 but (cid:107) · (cid:107)C is more
complicated than (cid:107) · (cid:107)2. Note that the original proofs in [1  4] are somewhat involved. Moreover 
this gauge function framework is ﬂexible enough to subsume a number of structurally regularized
problems [11  12]  and it is certainly possible to devise other (cid:107) · (cid:107)R and (cid:107) · (cid:107)C norms that would
induce interesting matrix norms.
The gauge function framework also allows us to develop an efﬁcient boosting algorithm for (2)  by
resorting to the following equivalent problem:

σiAi

+ λ

σi.

(5)

i

i

{σ∗

i   A∗

i } := argmin
σi≥0 Ai∈A

The optimal solution X∗ of (2) can be easily recovered as (cid:80)

f ({σi  Ai}) := L
iσ∗
i A∗

f ({σi  Ai})  where

i . Note that in the boosting

terminology  A corresponds to the set of weak hypotheses.
2.1 The boosting algorithm
To solve (5) we propose the boosting strategy presented in Algorithm 1. At each iteration  a weak
hypothesis Hk that yields the most rapid local decrease of the loss L is selected. Then Hk is com-
bined with the previous ensemble by tuning its weights to optimize the regularized objective. Note
that in Step 5 all the weak hypotheses selected in the previous steps are scaled by the same value.
As the (cid:96)1 regularizer requires the sum of all the weights  we introduce a variable sk that recursively
updates this sum in Step 6. In addition  Xk is used only in Step 3 and 4  which do not require its
explicit expansion in terms of the elements of A. Therefore this expansion of Xk does not need to
be explicitly maintained and Step 5 is included only for conceptual clarity.

(cid:16)(cid:88)

(cid:17)

(cid:88)

2.2 Rate of convergence
We prove the convergence rate of Algorithm 1  under the standard assumption:
Assumption 1 L is bounded from below and has bounded sub-level sets. The problem (5) admits
at least one minimizer X∗. L is differentiable and satisﬁes the following inequality for all η ∈
i σiAi = X  Ai∈K  σi ≥ 0}.

1Recall that the gauge function γK is deﬁned as γK(X) := inf{(cid:80)

i σi :(cid:80)

3

[0  1] and A  B in the (smallest) convex set that contains both X∗ and the sub-level set of f (0):
L((1 − η)A + ηB) ≤ L(A) + η (cid:104)B − A ∇L(A)(cid:105) + CLη2
. Here CL > 0 is a ﬁnite constant that
depends only on L and X∗.

2

Theorem 1 (Rate of convergence) Under Assumption 1  Algorithm 1 ﬁnds an  accurate solution
to (5) in O(1/) steps. More precisely  denoting f∗ as the minimum of (5)  then

f ({σ(k)

i

  A(k)

i }) − f∗ ≤ 4CL
k + 2

.

(6)

The proof is given in Appendix A. Note that the rate is independent of the regularization constant λ.
k+2; it should be clear that
In the proof we ﬁx the variable a in Step 4 of Algorithm 1 to be simply 2
setting a by line search will only accelerate the convergence. An even more aggressive scheme is
the totally corrective update [15]  which in Step 4 ﬁnds the weights for all A(k)

’s selected so far:

i

(cid:32) k(cid:88)

min
σi≥0

L

(cid:33)

k(cid:88)

σiA(k)

i

+ λ

σi.

(7)

i=1

i=1

i

i σi with h((cid:80)

where h is a convex non-decreasing function over [0 ∞). In (5)  this replaces(cid:80)

But in this case we will have to explicitly maintain the expansion of Xt in terms of the A(k)
’s.
For boosting without regularization  the 1/ rate of convergence is known to be optimal [27]. We
conjecture that 1/ is also a lower bound for regularized boosting.
Extensions Our proof technique allows the regularizer to be generalized to the form h(γK(X)) 
i σi).
By taking h(x) as the indicator h(x) = 0 if x ≤ 1;∞ otherwise  our rate can be straightforwardly
translated into the constrained setting.
3 Local Optimization with Fixed Rank
In Algorithm 1  Xk is determined by searching in the conic hull of Xk−1 and Hk.2 Suppose there
exists some auxiliary procedure that allows Xk to be further improved somehow to Yk (e.g. by local
greedy search)  then the overall optimization can beneﬁt from it. The only challenge  nevertheless 
is how to restore the “context” from Yk  especially the bases Ai and their weights σi.
In particular  suppose we have an auxiliary function g and the following procedure is feasible:
1. Initialization: given an ensemble {σi  Ai}  there exists a S such that g(S) ≤ f ({σi  Ai}).
2. Local optimization: some (local) optimizer can ﬁnd a T such that g(T ) ≤ g(S).
3. Recovery: one can recover an ensemble {βi  Bi : βi ≥ 0  Bi ∈ A} such that f ({βi  Bi}) ≤ g(T ).
Then obviously the new ensemble {βi  Bi} improves upon {σi  Ai}. This local search scheme can
i }. Perform
be easily embedded into Algorithm 1 as follows. After Step 5  initialize S by {σ(k)
i βi.
The rate of convergence will directly carry over. However  the major challenge here is the potentially
expensive step of recovery because little assumption or constraint is made on T .
Fortunately  a careful examination of Algorithm 1 reveals that a complete recovery of {βi  Bi} is not
required. Indeed  only two “sufﬁcient statistics” are needed: Xk and sk  and therefore it sufﬁces to
recover them only. Next we will show how this can be accomplished efﬁciently in (2) . Two simple
propositions will play a key role. Both proofs can be found in Appendix C.
Proposition 1 For the gauge γK induced by K  the convex hull of A in (3)  we have

local optimization and recover {βi  Bi}. Then replace Step 6 by Xk =(cid:80)

i βiBi and sk =(cid:80)

  A(k)

i

γK(X) = min

U V :U V =X

1
2

(cid:107)U:i(cid:107)2

C + (cid:107)Vi:(cid:107)2

R

.

(8)

(cid:18)

(cid:88)

i

(cid:19)

2 This does not mean Xk is a minimizer of L(X) + λγK(X) in that cone  because the bases are not
optimized simultaneously. Incidentally  this also shows why working with (5) turns out to be more convenient.

4

(cid:19)

(cid:88)

λ
2

(cid:107)U:i(cid:107)2

(cid:18)
k(cid:88)

C + (cid:107)Vi:(cid:107)2
(cid:18)
k(cid:88)

1
2

If (cid:107)·(cid:107)R = (cid:107)·(cid:107)C = (cid:107) · (cid:107)2  then γK becomes the trace norm (as we saw before)  and(cid:80)

R) is simply (cid:107)U(cid:107)2

(cid:107)Vi:(cid:107)2
norm [28]. This motivates us to choose the auxiliary function as

F + (cid:107)V (cid:107)2

C +
F . Then Proposition 1 is a well-known variational form of the trace

i((cid:107)U:i(cid:107)2

k(cid:88)

i=1

(9)
Proposition 2 For any U ∈Rm×k and V∈Rk×n  there exist σi≥ 0  ui∈Rm  and vi∈Rn such that

g(U  V ) = L(U V ) +

R

.

i

i=1

i=1

σi =

U V =

(cid:107)U:i(cid:107)2

σiuiv(cid:48)
i 

(cid:107)vi(cid:107)R ≤ 1 

(cid:107)ui(cid:107)C ≤ 1 

C + (cid:107)Vi:(cid:107)2
Now we can specify concrete details for local optimization in the context of matrix norms:
1. Initialize: given {σi ≥ 0  uiv(cid:48)
σ1u1  . . .  

σkvk)(cid:48).
2. Locally optimize g(U  V ) with initialization (Uinit  Vinit)  to obtain a solution (U∗  V ∗).
3. Recovery: use Proposition 2 to (conceptually) recover {βi  ˆui  ˆvi} from (U∗  V ∗).
The key advantage of this procedure is that Proposition 2 allows Xk and sk to be computed directly
from (U∗  V ∗)  keeping the recovery completely implicit:

i=1  set (Uinit  Vinit) to satisfy g(Uinit  Vinit) = f ({σi  uiv(cid:48)

i ∈ A}k
√
σkuk) 

√
Uinit = (

√
Vinit = (

i}):
(11)

σ1v1  . . .  

.

(10)

and

√

R

(cid:19)

(cid:19)

Xk =

βi ˆui ˆv(cid:48)

i = U∗V ∗ 

and sk =

i=1

i=1

(cid:107)U∗
:i(cid:107)2

C + (cid:107)V ∗
i:(cid:107)2

R

.

(12)

In addition  Proposition 2 ensures that locally improving the solution does not incur an increment
in the number of weak hypotheses. Using the same trick  the (Uinit  Vinit) in (11) for the (k + 1)-th
iteration can also be formulated in terms of (U∗  V ∗). Different from the local optimization for
trace norm in [21] which naturally works on the original objective  our scheme requires a nontrivial
(variational) reformulation of the objective based on Propositions 1 and 2.
The ﬁnal algorithm is summarized in Algorithm 2  where ˆU and ˆV in Step 5 denote the column-wise
and row-wise normalized versions of U and V   respectively. Compared to the local optimization in
[22]  which is hampered by orthogonal and PSD constraints  our (local) objective in (9) is uncon-
strained and smooth for many instances of (cid:107)·(cid:107)C and (cid:107)·(cid:107)R. This is plausible because no other con-
straints (besides the norm constraint)  such as orthogonality  are imposed on U and V in Proposition
2. Thus the local optimization we face  albeit non-convex in general  is more amenable to efﬁcient
solvers such as L-BFGS.
Remark Consider if one performs totally corrective update as in (7). Then all of the coefﬁcients
and weak hypotheses from (U∗  V ∗) have to be considered  which can be computationally expen-
sive. For example  in the case of trace norm  this leads to a full SVD on U∗V ∗. Although U∗ and V ∗
usually have low rank  which can be exploited to ameliorate the complexity  it is clearly preferable
to completely eliminate the recovery step  as in Algorithm 2.
4 Latent Generative Model with Multiple Views
Underlying most boosting algorithms is an oracle that identiﬁes the steepest descent weak hypothesis
(Step 3 of Algorithm 1). Approximate solutions often sufﬁce [8  9]. When (cid:107)·(cid:107)R and (cid:107)·(cid:107)C are both
Euclidean norms  this oracle can be efﬁciently computed via the leading left and right singular vector
pair. However  for most other interesting cases like low rank tensors  such an oracle is intractable
[29]. In this section we discover that for an important problem of multiview learning  the oracle can
be surprisingly solved in polynomial time  yielding an efﬁcient computational strategy.
Multiview learning analyzes multi-modal data  such as heterogeneous descriptions of text  image and
video  by exploiting the implicit conditional independence structure. In this case  beyond a single
dictionary U and coefﬁcient matrix V that model a single view Z (1)  multiple dictionaries U (k) are
needed to reconstruct multiple views Z (k)  while keeping the latent representation V shared across
all views. Formally the problem in multiview factorization is to optimize [2  4]:

Lt(U (t)V ) + λ(cid:107)V (cid:107)R 1 .

(13)

k(cid:88)

k(cid:88)

σi =

1
2

(cid:18)

k(cid:88)

i=1

k(cid:88)

t=1

min
U (1):(cid:107)U (1)
:i (cid:107)C≤1

. . .

min
U (k):(cid:107)U (k)
:i (cid:107)C≤1

min

V

5

We can easily re-express the problem as an equivalent “single” view formulation (1) by stacking all
{U (t)} into the rows of a big matrix U  with a new column norm (cid:107)U:i(cid:107)C := max
:i (cid:107)C. Then
the constraints on U (t) in (13) can be equivalently written as (cid:107)U:i(cid:107)C ≤ 1  and Algorithm 2 can be
directly applied with two specializations. First the auxiliary function g(U  V ) in (9) becomes

(cid:107)U (t)

t=1...k

(cid:19)

λ
2

(cid:18)
(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)

i

t

(cid:18)(cid:18)
(cid:88)

i

(cid:19)2
:i (cid:107)C

(cid:107)U (t)

(cid:19)

g(U  V ) = L(U V )+

λ
2

max
t=1...k

+(cid:107)Vi:(cid:107)2

R

= L(U V )+

(cid:107)U (t)
:i (cid:107)2

C +(cid:107)Vi:(cid:107)2

R

max
t=1...k

which can be locally optimized. The only challenge left is the oracle problem in (4)  which takes the
following form when all norms are Euclidean:
u(cid:48)Λv = max
(cid:107)u(cid:107)C≤1

(cid:107)Λ(cid:48)u(cid:107)2 = max

(cid:107)u(cid:107)C≤1 (cid:107)v(cid:107)≤1

u:∀t (cid:107)ut(cid:107)≤1

Λ(cid:48)
tut

(14)

max

.

(cid:13)(cid:13)(cid:13)(cid:13)2

[4  24] considered the case where k = 2 and showed that exact solutions to (14) can be found efﬁ-
ciently. But their derivation does not seem to extend to k > 2. Fortunately there is still an interesting
and tractable scenario. Consider multilabel classiﬁcation with a small number of classes  and U (1)
and U (2) are two views of features (e.g. image and text). Then each class label corresponds to a
view and the corresponding ut is univariate. Since there must be an optimal solution on the extreme
points of the feasible region  we can enumerate {−1  1} for ut (t ≥ 3) and for each assignment solve
a subproblem of the following form that instantiates (14) (c is a constant vector)

(cid:107)Λ(cid:48)

1u1 + Λ(cid:48)

2u2 + c(cid:107)2   s.t. (cid:107)u1(cid:107) ≤ 1  (cid:107)u2(cid:107) ≤ 1.

(QP ) max
u1 u2

(cid:32) r

(cid:33)

z

(cid:32) 0

Due to inhomogeneity  the technique in [4] is not applicable. Rewrite (15) in matrix form
(cid:104)I00  zz(cid:48)(cid:105) = 1 

(cid:104)M2  zz(cid:48)(cid:105) ≤ 0

(cid:104)M1  zz(cid:48)(cid:105) ≤ 0

(cid:104)M0  zz(cid:48)(cid:105)

(QP ) min

s.t.

(cid:33)

(cid:32)−1

(cid:33)

(cid:32)−1

(15)

(16)

(cid:33)

  M0 = −

c(cid:48)Λ(cid:48)
Λ1c Λ1Λ(cid:48)
Λ2c Λ2Λ(cid:48)

c(cid:48)Λ(cid:48)
1 Λ1Λ(cid:48)
1 Λ2Λ(cid:48)

1

2

u1
u2

where z=
 
and I00 is a zero matrix with only the (1  1)-th entry being 1. Let X = zz(cid:48)  a semi-deﬁnite program-
ming relaxation for (QP ) can be obtained by dropping the rank-one constraint:

  M1=

  M2=

0

0

I

I

2

2

(cid:104)M0  X(cid:105)  

s.t.

(cid:104)M1  X(cid:105) ≤ 0 

(cid:104)M2  X(cid:105) ≤ 0 

(cid:104)I00  X(cid:105) = 1  X (cid:23) 0.

(17)

(SP ) min

X

Its dual problem  which is also the Lagrange dual of (QP )  can be written as

s.t. Z := M0 − y0I00 + y1M1 + y2M2 (cid:23) 0 

y1 ≥ 0 

y2 ≥ 0.

(18)

(SD)

max
y0 y1 y2

y0 

(SD) is a convex problem that can be solved efﬁciently by  e.g.  cutting plane methods. (SP ) is
also a convex semideﬁnite program (SDP) amenable for standard SDP solvers. However further
recovering the solution to (QP ) is not straightforward  because there may be a gap between the
optimal values of (SP ) and (QP ). The gap is zero (i.e. strong duality between (QP ) and (SD))
only if the rank-one constraint that (SP ) dropped from (QP ) is automatically satisﬁed  i.e. if (SP )
has a rank-one optimal solution.
Fortunately  as one of our main results  we prove that strong duality always holds for the particular
problem originating from (15). Our proof utilizes some recent development in optimization [30] 
and is relegated to Appendix D.
5 Experimental Results
We compared our Algorithm 2 with three state-of-the-art solvers for trace norm regularized objec-
tives: MMBS3 [22]  DHM [15]  and JS [8]. JS was proposed for solving the constrained problem:
minX L(X) s.t. (cid:107)X(cid:107)tr ≤ ζ  which makes it hard to compare with solvers for the penalized prob-
lem: minX L(X) + λ(cid:107)X(cid:107)tr. As a workaround  we ﬁrst chose a λ  and found the optimal solution
X∗ for the penalized problem. Then we set ζ = (cid:107)X∗(cid:107)tr and ﬁnally solved the constrained problem
by JS. In this case  it is only fair to compare how fast L(X) (loss) is decreased by various solvers 
rather than L(X) + λ(cid:107)X(cid:107)∗ (objective). DHM is sensitive to the estimate of the Lipschitz constant
of the gradient of L  which we manually tuned for a small value such that DHM still converges.
Since the code for MMBS is specialized to matrix completion  it was used only in this comparison.
Traditional solvers such as proximal methods [6] were not included because they are much slower.

3 http://www.montefiore.ulg.ac.be/˜mishra/softwares/traceNorm.html

6

(a) Objective & loss vs time (loglog)

(a) Objective & loss vs time (loglog)

(a) Objective & loss vs time (loglog)

(b) Test NMAE vs time (semilogx)

(b) Test NMAE vs time (semilogx)

(b) Test NMAE vs time (semilogx)

Figure 1: MovieLens100k.

Figure 2: MovieLens1M.

Figure 3: MovieLens10M.

Comparison 1: Matrix completion We ﬁrst compared all methods on a matrix completion prob-
lem  using the standard datasets MovieLens100k  MovieLens1M  and MovieLens10M [6  8  21] 
which are sized 943× 1682  6040× 3706  and 69878× 10677 respectively (#user × #movie). They
contain 105  106 and 107 movie ratings valued from 1 to 5  and the task is to predict the rating for
a user on a movie. The training set was constructed by randomly selecting 50% ratings for each
user  and the prediction is made on the rest 50% ratings. In Figure 1 to 3  we show how fast various
algorithms drive down the training objective  training loss L (squared Euclidean distance)  and the
normalized mean absolute error (NMAE) on the test data [see  e.g.  6  8]. We tuned the λ to optimize
the test NMAE.
From Figure 1(a)  2(a)  3(a)  it is clear that it takes much less amount of CPU time for our method to
reduce the objective value (solid line) and the loss L (dashed line). This implies that local search and
partially corrective updates in our method are very effective. Not surprisingly MMBS is the closest
to ours in terms of performance because it also adopts local optimization. However it is still slower
because their local search is conducted on a constrained manifold. In contrast  our local search
objective is entirely unconstrained and smooth  which we manage to solve efﬁciently by L-BFGS.4
JS  though applied indirectly  is faster than DHM in reducing the loss. We observed that DHM kept
running coordinate descent with a constant step size  while the totally corrective update was rarely
taken. We tried accelerating it by using a smaller value of the estimate of the Lipschitz constant of
the gradient of L  but it leads to divergence after a rapid decrease of the objective for the ﬁrst few
iterations. A hybrid approach might be useful.
We also studied the evolution of the NMAE performance on the test data. For this we compared the
matrix reconstruction after each iteration against the ground truth. As plotted in Figure 1(b)  2(b) 
3(b)  our approach achieves comparable (or better) NMAE in much less time than all other methods.
Comparison 2: multitask and multiclass learning Secondly  we tested on a multiclass classiﬁ-
cation problem with synthetic dataset. Following [15]  we generated a dataset of D = 250 features
and C = 100 classes. Each class c has 10 training examples and 10 test examples drawn inde-
pendently and identically from a class-speciﬁc multivariate Gaussian N (µc  Σc). µc ∈ R250 has
the last 200 coordinates being 0  and the top 50 coordinates were chosen uniformly random from
{−1  1}. The (i  j)-th element of Σc is 22(0.5)|i−j|. The task is to predict the class membership of
a given example. We used the logistic loss for a model matrix W ∈ RD×C. In particular  for each

4 http://www.cs.ubc.ca/˜pcarbo/lbfgsb-for-matlab.html

7

10−2100102105106MovieLens−100k  λ = 20Running time (seconds)Objective and loss (training) Ours−objOurs−lossMMBS−objMMBS−lossDHM−objDHM−lossJS−loss10−21001020.20.30.40.50.60.70.80.9MovieLens−100k  λ = 20Running time (seconds)Test error OursMMBSDHMJS100102104106107MovieLens−1m  λ = 50Running time (seconds)Objective and loss (training) Ours−objOurs−lossMMBS−objMMBS−lossDHM−objDHM−lossJS−loss1001021040.20.30.40.50.60.70.80.9MovieLens−1m  λ = 50Running time (seconds)Test error OursMMBSDHMJS100102104106106107108109MovieLens−10m  λ = 50Running time (seconds)Objective and loss (training) Ours−objOurs−lossMMBS−objMMBS−lossDHM−objDHM−lossJS−loss1001021041060.10.20.30.40.50.60.70.80.9MovieLens−10m  λ = 50Running time (seconds)Test error OursMMBSDHMJStraining example xi with label
yi ∈ {1  ..  C}  we deﬁned an
individual loss Li(W ) as
Li(W ) = − log p(yi|xi; W ) 
where for any class c 
p(c|xi;W ) = Z−1

(cid:88)
i exp(W (cid:48)
exp(W (cid:48)

:cxi) 
:cxi).

Zi =

c

(a) Objective & loss vs time (loglog)

(a) Objective & loss vs time (loglog)

(b) Test error vs time (semilogx)
Figure 4: Multiclass classiﬁca-
tion with synthetic datset.

(b) Test error vs time (semilogx)
Figure 5: Multitask learning for
school dataset.

Then L(W ) is deﬁned as the
average of Li(W ) over the
whole training set. We found
that λ = 0.01 yielded the
lowest test classiﬁcation er-
ror; the corresponding results
are given in Figure 4. Clearly 
the intermediate models out-
put by our scheme achieve
comparable (or better) train-
ing objective and test error in
orders of magnitude less time
than those generated by DHM
and JS.
We also applied the solvers to a multitask learning problem with
the school dataset [25]. The task is to predict the score of
15362 students from 139 secondary schools based on a number
of school-speciﬁc and student-speciﬁc attributes. Each school is
considered as a task for which a predictor is learned. We used the
ﬁrst random split of training and testing data provided by [25] 5 
and set λ so as to achieve the lowest test squared error. Again 
as shown in Figure 5 our approach is much faster than DHM and
JS in ﬁnding the optimal solution for training objective and test
error. As the problem requires a large λ  the trace norm penalty
Figure 6: Multiview training.
is small  making the loss close to the objective.
Comparison 3: Multiview learning Finally we perform an initial test on our global optimization
technique for learning latent models with multiple views. We used the Flickr dataset from NUS-
WIDE [31]. Its ﬁrst view is a 634 dimensional low-level feature  and the second view consists of
1000 dimensional tags. The class labels correspond to the type of animals and we randomly chose 5
types with 20 examples in each type. The task is to train the model in (13) with λ = 10−3. We used
squared loss for the ﬁrst view  and logistic loss for the other views.
We compared our method with a local optimization approach to solving (13). The local method ﬁrst
ﬁxes all U (t) and minimizes V   which is a convex problem that can be solved by FISTA [32]. Then
it ﬁxes V and optimizes U (t)  which is again convex. We let Alt refer to the scheme that alternates
these updates to convergence. From Figure 6 it is clear that Alt is trapped by a locally optimal
solution  which is inferior to a globally optimal solution that our method is guaranteed to ﬁnd. Our
method also reduces both the objective and the loss slightly faster than Alt.
6 Conclusion and Outlook
We have proposed a new boosting algorithm for a wide range of matrix norm regularized problems.
It is closely related to generalized conditional gradient method [33]. We established the O(1/)
convergence rate  and showed its empirical advantage over state-of-the-art solvers on large scale
problems. We also applied the method to a novel problem  latent multiview learning  for which we
designed a new efﬁcient oracle. We plan to study randomized boosting with (cid:96)1 regularization [34]  
and to extend the framework to more general nonlinear regularization [3].

5http://ttic.uchicago.edu/˜argyriou/code/mtl_feat/school_splits.tar

8

10010210−1100101102Synthetic multiclass  λ = 0.01Running time (seconds)Objective and loss (training) Ours−objOurs−lossDHM−objDHM−lossJS−loss1001020.650.70.750.80.850.90.95Multiclass  λ = 0.01Running time (seconds)Test error OursDHMJS100102101102103104School multitask  λ = 0.1Running time (seconds)Objective and loss (training) Ours−objOurs−lossDHM−objDHM−lossJS−loss10010202004006008001000School Multitask  λ = 0.1Running time (seconds)Test regression error OursDHMJS102103100101102Multiview Flickr  λ=0.001Running time (seconds)Objective and loss (training) Ours−objOurs−lossAlt−objAlt−lossReferences
[1] F. Bach  J. Mairal  and J. Ponce. Convex sparse matrix factorizations. arXiv:0812.1869v1  2008.
[2] H. Lee  R. Raina  A. Teichman  and A. Ng. Exponential family sparse coding with application to self-

taught learning. In IJCAI  2009.

[3] D. Bradley and J. Bagnell. Convex coding. In UAI  2009.
[4] X. Zhang  Y-L Yu  M. White  R. Huang  and D. Schuurmans. Convex sparse coding  subspace learning 

and semi-supervised extensions. In AAAI  2011.

[5] T. K. Pong  P. Tseng  S. Ji  and J. Ye. Trace norm regularization: Reformulations  algorithms  and multi-

task learning. SIAM Journal on Optimization  20(6):3465–3489  2010.

[6] K-C Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least

squares problems. Paciﬁc Journal of Optimization  6:615–640  2010.

[7] J-F Cai  E. J. Cand´es  and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM

Journal on Optimization  20(4):1956–1982  2010.

[8] M. Jaggi and M. Sulovsky. A simple algorithm for nuclear norm regularized problems. In ICML  2010.
[9] E. Hazan. Sparse approximate solutions to semideﬁnite programs. In LATIN  2008.
[10] K. L. Clarkson. Coresets  sparse greedy approximation  and the Frank-Wolfe algorithm. In SODA  2008.
[11] A. Tewari  P. Ravikumar  and I. S. Dhillon. Greedy algorithms for structurally constrained high dimen-

sional problems. In NIPS  2011.

[12] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear inverse

problems. Foundations of Computational Mathematics  12(6):805–849  2012.

[13] Y. Bengio  N.L. Roux  P. Vincent  O. Delalleau  and P. Marcotte. Convex neural networks. In NIPS  2005.
[14] L. Mason  J. Baxter  P. L. Bartlett  and M. Frean. Functional gradient techniques for combining hypothe-

ses. In Advances in Large Margin Classiﬁers  pages 221–246  Cambridge  MA  2000. MIT Press.

[15] M. Dudik  Z. Harchaoui  and J. Malick. Lifted coordinate descent for learning with trace-norm regular-

izations. In AISTATS  2012.

[16] S. Shalev-Shwartz  N. Srebro  and T. Zhang. Trading accuracy for sparsity in optimization problems with

sparsity constraints. SIAM Journal on Optimization  20:2807–2832  2010.

[17] X. Yuan and S. Yan. Forward basis selection for sparse approximation over dictionary. In AISTATS  2012.
IEEE Trans.
[18] T. Zhang. Sequential greedy approximation for certain convex optimization problems.

Information Theory  49(3):682–691  2003.

[19] S. Burer and R. Monteiro. Local minima and convergence in low-rank semideﬁnite programming. Math-

ematical Programming  103(3):427–444  2005.

[20] M. Journee  F. Bach  P.-A. Absil  and R. Sepulchre. Low-rank optimization on the cone of positive

semideﬁnite matrices. SIAM Journal on Optimization  20:2327C–2351  2010.

[21] S. Laue. A hybrid algorithm for convex semideﬁnite optimization. In ICML  2012.
[22] B. Mishra  G. Meyer  F. Bach  and R. Sepulchre. Low-rank optimization with trace norm penalty. Tech-

nical report  2011. http://arxiv.org/abs/1112.2318.

[23] S. Shalev-Shwartz  A. Gonen  and O. Shamir. Large-scale convex minimization with a low-rank con-

straint. In ICML  2011.

[24] M. White  Y. Yu  X. Zhang  and D. Schuurmans. Convex multi-view subspace learning. In NIPS  2012.
[25] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Machine Learning  73(3):

243–272  2008.

[26] J-B Hiriart-Urruty and C. Lemar´echal. Convex Analysis and Minimization Algorithms  I and II  volume

305 and 306. Springer-Verlag  1993.

[27] I. Mukherjee  C. Rudin  and R. Schapire. The rate of convergence of Adaboost. In COLT  2011.
[28] N. Srebro  J. Rennie  and T. Jaakkola. Maximum-margin matrix factorization. In NIPS  2005.
[29] C. Hillar and L-H Lim. Most tensor problems are NP-hard. arXiv:0911.1393v3  2012.
[30] W. Ai and S. Zhang. Strong duality for the CDT subproblem: A necessary and sufﬁcient condition. SIAM

Journal on Optimization  19:1735–1756  2009.

[31] T.S. Chua  J. Tang  R. Hong  H. Li  Z. Luo  and Y.T. Zhang. A real-world web image database from

national university of singapore. In International Conference on Image and Video Retrieval  2009.

[32] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[33] K. Bredies  D. Lorenz  and P. Maass. A generalized conditional gradient method and its connection to an

iterative shrinkage method. Computational Optimization and Applications  42:173–193  2009.

[34] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM

Journal on Optimization  22(2):341–362  2012.

9

,Rakesh Shivanna
Bibaswan Chatterjee
Raman Sankaran
Chiranjib Bhattacharyya
Francis Bach
Ricky T. Q. Chen
Xuechen Li
Roger Grosse
David Duvenaud