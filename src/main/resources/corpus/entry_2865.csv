2019,Learning Bayesian Networks with Low Rank Conditional Probability Tables,In this paper  we provide a method to learn the directed structure of a Bayesian network using data. The data is accessed by making conditional probability queries to a black-box model. We introduce a notion of simplicity of representation of conditional probability tables for the nodes in the Bayesian network  that we call ``low rankness''. We connect this notion to the Fourier transformation of real valued set functions and propose a method which learns the exact directed structure of a `low rank` Bayesian network using very few queries. We formally prove that our method correctly recovers the true directed structure  runs in polynomial time and only needs polynomial samples with respect to the number of nodes. We also provide further improvements in efficiency if we have access to some observational data.,Learning Bayesian Networks with Low Rank

Conditional Probability Tables

Adarsh Barik

Department of Computer Science

Purdue University

West Lafayette  Indiana  USA

abarik@purdue.edu

Jean Honorio

Department of Computer Science

Purdue University

West Lafayette  Indiana  USA

jhonorio@purdue.edu

Abstract

In this paper  we provide a method to learn the directed structure of a Bayesian
network using data. The data is accessed by making conditional probability queries
to a black-box model. We introduce a notion of simplicity of representation of
conditional probability tables for the nodes in the Bayesian network  that we call
“low rankness”. We connect this notion to the Fourier transformation of real valued
set functions and propose a method which learns the exact directed structure of
a ‘low rank‘ Bayesian network using very few queries. We formally prove that
our method correctly recovers the true directed structure  runs in polynomial time
and only needs polynomial samples with respect to the number of nodes. We also
provide further improvements in efﬁciency if we have access to some observational
data.

1

Introduction

Motivation. Real-world systems are made of large number of constituent variables. Understanding
the interactions and relationships of these variables is key to understand the behavior of such systems.
Scientists and researchers from many domains have been using graphs to model and learn relationships
amongst variables of real-world systems for a long time. Bayesian networks are one of the most
important classes of probabilistic graphical models which are used to model complex systems. They
provide a compact representation of joint probability distributions among a set of variables.

Related work. Learning the structure of a Bayesian network from observational data is a well
known but an incredibly difﬁcult problem to solve in the machine learning community. Due to its
popularity and applications  a considerable amount of work has been done in this ﬁeld. Most of
these work use observational data to learn the structure. We can broadly divide these methods in
two categories. The methods in the ﬁrst category use score maximization techniques to learn the
DAG from observational data. In this category  there are some heuristics based approaches such as
Friedman et al. (1999); Tsamardinos et al. (2006); Margaritis and Thrun (2000); Moore and Wong
(2003) which run in polynomial-time without offering any convergence/consistency guarantee. There
are also some exact but exponential time score maximizing exact algorithms such as Koivisto and
Sood (2004); Silander and Myllymäki (2006); Cussens (2008); Jaakkola et al. (2010). The methods
in the second category are independence test based methods such as Spirtes et al. (2000); Cheng et al.
(2002); Yehezkel and Lerner (2005); Xie and Geng (2008).
There have also been some work to learn the structure of a Bayesian network using interventional
data (Murphy  2001; Tong and Koller  2001; Eaton and Murphy  2007; Triantaﬁllou and Tsamardinos 
2015). Most of these works ﬁrst ﬁnd a Markov equivalence class from observational data and then
direct the edges using interventions. Unfortunately  the ﬁrst step of ﬁnding Markov equivalence class

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

remains NP-hard (Chickering  1996). Hausar and Bühlmann (2012)  He and Geng (2008)  Kocaoglu
et al. (2017) have presented polynomial time methods to ﬁnd an optimal set of interventions for
chordal DAGs. Bello and Honorio (2018) have proposed a method to learn a Bayesian network
using interventional path queries with logarithmic sample complexity. However  their method runs in
exponential time in terms of the number of parents.
In this paper  our work takes an intermediate path. We do not use pure observational or interventional
data directly. Rather  we assume that there exists a black-box which answers conditional probability
queries by outputting observational data. Our goal is to limit the number of such queries and learn
the directed structure of a Bayesian network. We propose a novel algorithm to achieve this goal. We
also provide a method to improve our results by having access to some observational data. We intend
to measure our performance based on the following criteria. 1. Correctness - We want to come up
with a method which correctly recovers the directed structure of a Bayesian network with provable
theoretical guarantees. 2. Computational efﬁciency - The method must run fast enough to handle
the high dimensional cases. Ideally  we want to have polynomial time complexity with respect to
the number of nodes. 3. Sample complexity - We would like to use as few samples as possible for
recovering the structure of the Bayesian network. As with the time complexity  we want to achieve
polynomial sample complexity with respect to the size of the network.

Contribution. Consider a binary node i of a Bayesian network with m parents. The conditional
probability table (CPT) of node i has 2m`1 entries. This number quickly becomes very large even
for modest values of m. To handle such large tables while still maintaining the effect of all the
parents  we introduce a notion of simplicity of representation of the CPTs  which we call “low
rankness”. Our intuition is that each CPT can be treated as summation of multiple simple tables 
each of them depending only on a handful of parents (say k parents where k is the rank of the CPT).
We connect this notion of rank of a CPT to the Fourier transformation of a speciﬁc real valued set
function (Stobbe and Krause  2012) and use compressed sensing techniques (Rauhut  2010) to show
that the Fourier coefﬁcients of this set function can be used to learn the structure of the Bayesian
network. While doing so  we provide a method with theoretical guarantees of correctness  and which
works in polynomial time and sample complexity. Our method requires computation of conditional
probabilities from data. We do this by making queries to a black-box. One query consists of two
steps. The ﬁrst step is the selection of variables  i.e.  choosing a target variable and a set of variables
for conditioning. The second step is to assign speciﬁc values to the selected conditioning variables.
This process is similar to the process used in Bello and Honorio (2018); Kocaoglu et al. (2017)  which
consider a particular selection of variables as one intervention. An actual setting of the variables
are considered as one experiment. For example  a selection of k binary variables can be assigned 2k
distinct values and can be queried in 2k different ways. Our setting is similar to an interventional
setting where a selection can be compared to an intervention and an assignment can be compared
to an experiment  although our method never queries the 2k distinct values  but a single random
assignment instead. Thus  we compare our results to the state-of-the-art interventional methods in
Table 1. It should be noted that the number of queries (or experiments in the interventional setting)
are a better metric for comparison than the number of selections (or interventions). This is because a
selection may involve only one node (Bello and Honorio  2018) or multiple nodes (in this paper) and
thus could hide some complexity of the problem. Furthermore  the sample complexity of the problem
depends on the number of queries.

Table 1: Sample and time complexity  number of selections (interventions) and queries (experiments)
required for structure learning of binary Bayesian networks. Here n is the number of nodes  k is the
maximum size of the Markov blanket. The maximum number of parents of a node is Opkq.
Time Complexity Selections Queries
Algorithms
n log nq Opnq
Our Work
(no observational data)
Opnq
Our Work
(with observational data) Blackbox - Opnk3 log5 kq
Bello and Honorio (2018) Interventional - Opn22k log nq Opn22k log nq Opn2q
Kocaoglu et al. (2017)

Sample Complexity
Blackbox - Opnk3 log4 nplog k Opn4k
` log log nqq
Observational - Opnq
Opn4q
Opnk4?

Interventional - no guarantees Op2nkn2 log2 nq Oplog nq Op2n log nq

?

k log kq

Opnk3 log4 nq
Opnk3 log4 kq
Opn22kq

2

n

2 Preliminaries
In this section  we introduce formal deﬁnitions and notations. Let X “ tX1  X2  . . .   Xnu be a set of
random variables. For a set A  XA denotes the set of random variables Xi P X such that i P A. We
use the shorthand notation i to denote V ztiu. We deﬁne a Bayesian network on a directed acyclic
graph G “ pV  Eq where V denotes the set of vertices and E is a set of ordered pair of nodes  each
corresponding to a directed edge  i.e.  if pa  bq P E then there is an edge a Ñ b in G. The parents of
a node i @i P V denoted by πGpiq  are set of all nodes j such that edge pj  iq P E. We also deﬁne
the Markov blanket MBGpiq for a node i as a set of nodes containing parents  children and parents of
children of node i. The nodes with no children are called terminal nodes.
Deﬁnition 1 (Bayesian network). Let G “ pV  Eq be a directed acyclic graph (DAG) and X “
tX1  X2  . . .   Xnu be a set of random variables such that Xi corresponds to a random variable at
node i P V @i “ t1  . . .   nu. Let XπGpiq denote the set of random variables deﬁned on the parents
ś
of node i in DAG G. A Bayesian network B “ pG Pq represents a joint probability distribution P
over the set of random variables X deﬁned on the nodes of DAG G which factorizes according to
i“1 PpXi|XπGpiqq where PpXi|XπGpiqq denotes
the DAG structure  i.e.  PpX1  X2  . . .   Xnq “
conditional probability distribution (CPD) of node i given its parents in DAG G.
We denote the domain of a random variable Xi @i P t1  . . .   nu by dompXiq. The cardinality of a
ś
set is denoted by notation | ¨ |. A Bayesian network B “ pG Pq on discrete nodes is called a binary
Bayesian network if | dompXiq| “ 2 @i P t1  . . .   nu. For discrete nodes  PpXi|XπGpiqq is often
represented as a conditional probability table (CPT) with | dompXiq|
jPπGpiq | dompXjq| entries.
In this work  we will only focus on binary Bayesian networks. Next  we introduce a novel concept of
rank of a conditional probability distribution for a node of Bayesian network.
Deﬁnition 2 (Rank k conditional probability distribution). A node i P V of a Bayesian network
BpG Pq is said to be rank k representable with respect to a set Apiq Ď V ztiu and probability
distribution P if 
QSpXS “ xSq @xi P dompXiq  xApiq P dompXApiqq
PpXi “ xi|XApiq “ xApiqq “

Ś
(1)
jPS dompXjq Ñ R is a function which depends only on the variables XS. A node i is
where QS :
said to have rank k conditional probability table if it is rank k representable but is not rank k ´ 1
representable with respect to Apiq and P.
ř
For example  a node i P V of a Bayesian network BpG Pq is rank 2 representable with respect
to its parents πGpiq and P if we can write PpXi “ xi|XπGpiq “ xπGpiqq “ QipXi “ xiq `
jPπGpiq QijpXi “ xi  Xj “ xjq  where @xi P dompXiq  xj P dompXjq @j P πGpiq. It is easy to
observe that any node i P V is always rank |Apiq|`1 representable with respect to a set Apiq Ă V ztiu
and P. Also  rank k representations for a node i with respect to Apiq and P may not be unique. We
ř
consider real-valued set functions on a set T of cardinality t deﬁned as f : 2T Ñ R where 2T denotes
the power set of T . Let F be the space of all such functions  with corresponding inner product
AP2T fpAqgpAq. The space F has a natural Fourier basis  and in our set function
xf  gy ﬁ 2´t
notation the corresponding Fourier basis vectors are ψBpAq ﬁ p´1q|AXB|. We deﬁne the Fourier
transformation coefﬁcients of function f as ˆfpBq ﬁ xf  ψBy “ 2´t
AP2T fpAqp´1q|AXB|. Using
Fourier coefﬁcients  the function f can be reconstructed as:

SĎtiuYApiq
1ď|S|ďk  iPS

ÿ

ř

ÿ

BP2T

fpAq “

ˆfpBqψBpAq

(2)

The Fourier support of a set function is the collection of subsets with nonzero Fourier coefﬁcient:
supportp ˆfq ﬁ tB P 2T| ˆfpBq ‰ 0u.

3 Method and Theoretical Analysis

In this section  we develop our method for learning the structure of a Bayesian network and provide
theoretical guarantees for correct and efﬁcient learning. First we would like to mention some technical
assumptions.

3

Assumption 1 (Availability of Black-box). For a Bayesian network BpG Pq  we can submit a
conditional probability query BBpi  A  xA  Nq to a black-box on any set of selected nodes i P V  A Ď
i and value xA  and receive N i.i.d. samples from the conditional distribution PpXi|XA “ xAq.
Assumption 2 (Faithfulness). The distribution over the nodes of the Bayesian network BpG Pq
induced by pG Pq exhibits no other independencies beyond those implied by the structure of G.
Assumption 3 (Low rank CPTs). Each node i P V in the Bayesian network BpG Pq has rank 2
conditional probability tables with respect to πGpiq and P.
Assumption 1 implies the availability of observational data for all queries. This is analogous to the
standard assumption of availability of interventional data in interventional setting (Murphy  2001;
He and Geng  2008; Kocaoglu et al.  2017; Tong and Koller  2001; Hausar and Bühlmann  2012).
Assumption 2 is also a standard assumption (Kocaoglu et al.  2017; Tong and Koller  2001; He and
Geng  2008; Spirtes et al.  2000; Triantaﬁllou and Tsamardinos  2015) which ensures that we only
have those independence relations between nodes which come from d-separation. We also introduce
a novel Assumption 3 which ensure that CPTs of nodes have a simple representation. In the later
sections  we relate this to sparsity in the Fourier domain. We note that there is nothing special about
CPTs being rank 2 and our method can be extended for any rank k CPTs.

3.1 Problem Description

In this work  we address the following question:
Problem 1 (Recovering structure of a Bayesian network using black-box queries). Consider we have
access to a black-box which provides observational data for our conditional probability queries for a
faithful Bayesian network BpG Pq with each node i having rank 2 CPT with respect to its parents
πGpiq and P. Can we recover the directed structure of G with theoretical guarantees of correctness
and efﬁciency in terms of time and sample complexity?

We show that it is indeed possible to do. We control the number of samples by controlling the number
of queries. We also show that it is possible to further reduce the sample complexity if we have access
to some observational data.

3.2 Theoretical Result

In this subsection  we state our theoretical results. We start by analyzing terminal nodes.

Analyzing Terminal Nodes. Since terminal nodes do not have any children  their Markov blanket
only contains their parents. Furthermore  if the Bayesian network is faithful then for any terminal
node t P V : PpXt|XπGptqq “ PpXt|XMBGptqq “ PpXt|Xtq. Thus  for any terminal node t P
V  PpXt|XπGptqq can be computed without explicitly knowing its parents. Next  we deﬁne a set
function which computes PpXt|XπGptqq. In particular  for an assignment xπGptq P t0  1u|πGptq|  we
are interested in computing PpXt “ 1|XπGptq “ xπGptqq. Note that PpXt “ 0|XπGptq “ xπGptqq
can simply be computed by subtracting PpXt “ 1|XπGptq “ xπGptqq from 1. Let t denote the set
V zttu. For node t and a set A Ď t  let xA P t0  1un be an assignment such that xA
i “ 1iPA @i ‰ t
t “ 0. We deﬁne a set function ft for each terminal node t P V as follows:
and xA

ÿ

ftpAq “ QtpXt “ xA

t q `

QtjpXt “ xA

t   Xj “ xA
j q 

@A Ď t

jPπGptq

(3)
πGptqq and ftpAq “ ftpA X πtq.
Note that Equation (3) precisely computes PpXt “ 1|XπGptq “ xA
Next  we prove that the Fourier support of ft only contains singleton sets of parents of node t.
Theorem 1. If nodes of a Bayesian network BpG Pq have rank 2 with respect to their parents πGp.q
and P  then the Fourier coefﬁcient ˆftpBq for function ft deﬁned by equation (3) for any terminal
ř
node t and a set B P 2t is given by:

˘
QtjpXt “ 1  Xj “ 0q ´ QtjpXt “ 1  Xj “ 1q

˘
QtjpXt “ 1  Xj “ 0q ` QtjpXt “ 1  Xj “ 1q
  B “ tju @j P πGptq

$’&’%QtpXt “ 1q ` 1
`

1
2
0  Otherwise

  B “ φ

ˆftpBq “

jPπGptq

2

`

4

(4)

(See Appendix A for detailed proofs.)

Analyzing Non-Terminal Nodes. A similar analysis can be done for non-terminal nodes. However 
for a non-terminal node i we can not compute PpXi|XπGpiqq without explicitly knowing the parents
of node i. We will rather focus on computing PpXi|XMBGpiqq for non-terminal nodes which equals
to computing PpXi|Xiq which can be done from data. Similar to the previous case  we deﬁne a set
function gi for each non-terminal node i P V as follows:

gipAq “ QipXi “ xA

i q `

QijpXi “ xA

i   Xj “ xA
j q 

@A Ď i

(5)

ÿ

jPπGpiq

We can deﬁne a corresponding set function fi which computes PpXi|XMBGpiqq for non-terminal
nodes. We do it in the following way:
fipAq “PpXi “ xA

ř
“ PpXi “ xA
PpXi “ xA

ś
i |XMBGpiq “ xMBGpiqq
ś
kPchildGpiq PpXk “ xA
πGpiqq
i |XπGpiq “ xA
πGpiqq
i |XπGpiq “ xA
kPchildGpiq PpXk “ xA

k |XπGpkq “ xA
k |XπGpkq “ xπGpkqAq

πGpkqq

(6)

Xi

where childGpiq is the set of children of node i in DAG G. We can again compute the Fourier support
for fi for each non-terminal node.
Theorem 2. If nodes of a Bayesian network BpG Pq have rank 2 with respect to their parents πGp.q
and P  then the Fourier coefﬁcient ˆfipBq for function fi deﬁned by equation (6) for any non-terminal
node i and a set B P 2i is given by:
ř
|Bz MBGpiq| ě 1
ś
ˆfipBq “
kPchildGpiqgkpAq`gipAYtiuq

ś
ś
kPchildGpiqgkpAq
kPchildGpiqgkpAYtiuq ψBpAq 

AP2V ´i

#

gipAq

gipAq

2n´1

0 

1

otherwise
(7)

3.3 Algorithm

Our algorithm works on the principle that the terminal nodes are rank 2 with respect to their Markov
Blanket and P  while non-terminal nodes are not. This is true if for every non-terminal node there
exists a B P 2V such that |Bz MBGpiq| “ 0 and ˆfipBq is nonzero. This is formalized in what
follows.
Assumption 4 (Non-terminal nodes are not rank 2). There exists a B P 2V for each non-terminal
node i  such that |B| “ 2  |Bz MBGpiq| “ 0 and ˆfipBq as deﬁned by Equation (7) is non-zero.
This distinction helps us to differentiate between terminal and non-terminal nodes. Note that the
set function fi is uniquely determined by its Fourier coefﬁcients. Moreover  the Fourier support for
function fi is sparse. For terminal nodes  ˆfipBq is non-zero only for the empty set or the singleton
nodes  while for the non-terminal nodes  ˆfipBq is non-zero for B Ď MBGpiq. Thus  recovering
Fourier coefﬁcients from the measurements of fi can be treated as recovering a sparse vector in R2i.
However  |2i| could be quite large. We avoid this problem by substituting fi by another function
gi P G2 where Gk “ tgi | @B P supportpgiq |B| ď ku. Note that 

ˆfipBkqψBkpAjq `

ˆfipBkqψBkpAjq `

ˆfipBkqψBkpAjq

(8)

ÿ

|Bk|“1
BkP2i

fipAjq “

and @gi P G2 

gipAjq “

ˆfipBkqψBkpAjq `

ˆfipBkqψBkpAjq

(9)

ÿ

|Bk|“2
BkP2i

ÿ

|Bk|ě3
BkP2i

ÿ

|Bk|“1
BkP2i

ÿ

|Bk|“2
BkP2i

It follows that for a terminal node i  gi “ fi as for terminal nodes fi P G1. For non-terminal nodes 
using results from Theorem 2  if B Ď MBGpiq then ˆfipBq ‰ 0 and therefore gi R G1. Now  let Ai

5

be a collection of mi sets Aj P 2V ´i chosen uniformly at random. We measure gipAjq for each
Aj P Ai and then using equation (2) we can write:

gipAjq “

p´1q|AjXBk| ˆfipBkq @Aj P Ai

(10)

ÿ

BkP2i |Bk|ď2

Let gi P Rmi be a vector whose jth row is gipAjq and ˆgi P Rn`pn´1
form ˆfipBkq@Bk P ρi where

2 q be a vector with elements of

ρi “ tBk | Bk P 2i |Bk| ď 2u

(11)

is a set which contains supportp ˆfiq. Then 

`

˘

jk “ p´1q|AjXBk| .

gi “ Miˆgi where  Mi P t´1  1umiˆn such that Mi

(12)
Also note that for terminal nodes ˆgi is sparse with |πGpiq| ` 1 non-zero elements for terminal
` k ` 1 non-zero elements for non-terminal nodes where k “ | MBGpiq|.
nodes and at max
Equation (12) can be solved by any standard compressed sensing techniques to recover the parents of
the terminal nodes. Using this formulation and the fact that terminal nodes have non-zero Fourier
coefﬁcients on empty or singleton sets  we can provide an algorithm to identify the terminal nodes and
their corresponding parents. We can use this algorithm repeatedly to identify the complete structure
of the Bayesian network until the last two nodes where we can not apply our algorithm. Algorithm 1
identiﬁes the parents for each node and consequently the directed structure of the Bayesian network.

k
2

Algorithm 1: getParentspV q
:Nodes V “ t1  2  . . .   nu

Input
Output :Recovered parent set ˆπ : V Ñ 2V
S Ð V ;
while |S| ě 3 do
T  ˆπ “ getTerminalNodespSq ;
S Ð SzT ;

end
for i P S do
ˆπpiq “ φ;
end

Algorithm 2: getTerminalNodespSq
:Nodes S Ď t1  2  . . .   nu
ˆπ : T Ñ 2S

Input
Output :Set of terminal nodes T and their parents
T Ð φ  ˆπpiq Ð φ @i P S ;
for node i P S  j P t1  . . .   miu do

Choose Aj P 2Sztiu uniformly at random ;
Compute
fipAjq “ PpXi “ 0|XSztiu “ xAj
Sztiuq ;
jk for Bk P ρi (Eq (11) (12)) ;
Compute Mi
Solve for βi using compressed sensing (Eq
(13)) ;
if βipBq “ 0 for all |B| ą 1 then

T Ð T Y tiu ;
ˆπpiq Ð YB:βipBq‰0B ;

end

end

4 Analysis in Finite Sample Regime

So far our results have been in the population setting where we assumed that we had access to the true
conditional probabilities. However  generally this is not the case and we have to work with a ﬁnite
number of samples from the black-box. In this section  we provide theoretical results for different
ﬁnite sample regimes.

4.1 Without access to any observational data

In this setting  we assume that we only have access to a black-box which outputs observational
data for our conditional probability queries. One selection of nodes consists of ﬁxing Xi and then
measuring Xi for each node i. We need only 1 selection for each node. Thus the total number of
selections for all the nodes is n. One query amounts to ﬁxing Xi to a particular xi. Note that while
2n´1 such queries are possible for each selection on each node  we only conduct mi queries for each
node i.

6

Number of Queries. We measure gipAjq by querying for fipAjq. Let |fipAjq ´ gipAjq| ď
j @Aj P Ai for some j ą 0. Once we have the noisy measurements of gipAjq  we can get a good
approximation of ˆgi by solving the following optimization problem for each node i.

βi “ min

ˆgiPR|ρi| }ˆgi}1

s.t.}Miˆgi ´ fi}2 ď  where  “

(13)

d ÿ

2
j .

AjPAj

Theorem 3. Suppose ˆgi is constructed by computing ˆgipBkq using Bk from a ﬁxed collection ρi as
deﬁned in Equation (11). Furthermore  suppose gi is computed by selecting mi sets Aj uniformly at
random from 2i. We deﬁne the matrix Mi as in equation (12). Then there exist universal constants
C1  C2 ą 0 such that if  mi ě maxpC1|supportpˆgiq| log4pn `
δq and
βi is solved using equation (13). Then with probability at least 1 ´ δ  we have }βi ´ ˆgi}2 ď
for some universal constant C3 ą 0. If the minimum non-zero element of |ˆgi| is greater
C3
?
than 2C3
then βi recovers ˆgi up to the signs. Furthermore  if Assumption 4 is satisﬁed then
 @B P ρi |B| “ 2 if and only if i is a terminal node and ˆπpiq “ tB | |B| “
|βipBq| ď C3
mi
?
u correctly recovers the parents of the terminal node i  i.e.  ˆπpiq “ πGpiq.
1  |βipBq| ą C3
mi
?
mi
Applying this recursively shows the correctness of Algorithm 1.

q  C2|supportpˆgiq| log 1

n´1
2

?
mi

`

˘

k
2

` k ` 1. Thus the num-
The sparsity for each node would be less than or equal to
ber of queries needed for each node (using arguments from Theorem 3) would be of order
δqq. At the ﬁrst iteration  we query all the nodes. From the next iteration
Opmaxpk2 log4 n  k2 log 1
onwards  we query for only the nodes which had terminal nodes as their children i.e.  for a maximum
δqq. We
of k nodes. Thus the total number of queries needed would be Opmaxpnk3 log4 n  nk3 log 1
can recover parents for terminal nodes using Theorem 3.
Sample and Time Complexity. The sample complexity is Opmaxp nk3 log4 n
log log nq  nk3
B for details).

δplog k ` log log nqq and the time complexity is Opn4k

plog k `
n log nq (See Appendix

2 log 1

?

2

`

˘

4.2 With Access to Some Observational Data

In this setting  we have access to some observational data as well. We can use the observational data
to ﬁgure out the Markov blanket of each node which helps us reduce number of selected variables in
the conditional probability queries. Once we have the Markov blanket  we only select the nodes in
MBGpiq for each query. We need only 1 selection for each node. Thus the total number of selections
for all nodes does not exceed n.
Using Observational Data. Recall that P is the true joint distribution over the nodes of a Bayesian
network BpG Pq. We deﬁne a collection of distributions P over the nodes of the Bayesian network
as: P “

(cid:32)
(
P is faithful to G. |PpXi|Xlq “ PpXi|Xlq @i  l P t1  . . .   nu

Computing the Markov Blanket from Observational Data. Consider a probability distribution
ˆP P P on the nodes of the Bayesian network such that each node i is rank 2 with respect to MBGpiq
and ˆP . This allows us to recover the Markov blanket of the node using the observational data.
Theorem 4. If there exists a probability distribution ˆP P P such that each node i is rank 2 with
respect to MBGpiq and ˆP   then the Markov blanket of a node i can be recovered by solving the
following system of equations:
PpXi “ 0  Xl “ 0q “ ˜QipXi “ 0qPpXl “ 0q `

˜QijpXi “ 0  Xj “ 0qPpXj “ 0  Xl “ 0q

ÿ

` ˜QilpXi “ 0  Xl “ 0qPpXl “ 0q  @ l “ t1  . . .   nu  l ‰ i

PpXi “ 0q “ ˜QipXi “ 0q `

˜QijpXi “ 0  Xj “ 0qPpXj “ 0q

ÿ

jP´i
j‰l

jP´i
j‰l

7

which can be written in a more compact form:

y “ Aq

(14)
where y P Rn and A P Rnˆn and q P Rn. The entries of y are indexed by l “ t1 . . . nu such that
yl “ PpXi “ 0  Xl “ 0q when l ‰ i and yl “ PpXi “ 0q when l “ i. The entries of A are indexed
by l  j P t1  . . .   nu  where Alj “ PpXl “ 0  Xj “ 0q for l ‰ i  j ‰ i  j ‰ l and   Alj “ PpXl “ 0q
when l “ j  l ‰ i  Alj “ PpXl “ 0q for l ‰ i  j “ i  Alj “ PpXj “ 0q for l “ i  j ‰ i and Alj “ 1
for l “ i  j “ i. The entries of q are indexed by j P t1  . . .   nu such that qj “ ˜QijpXi “ 0  Xj “ 0q
for j ‰ i and qj “ ˜QipXi “ 0q for j “ i.
For terminal nodes  existence of ˆP P P as P P P is guaranteed. To ensure that ˆP P P also exists for
non-terminal nodes  we make the following assumption:
Assumption 5. The population matrix A P Rnˆn as deﬁned in equation (14) is positive deﬁnite.
This assumption is not strong. We can  in fact  show that A is a positive semideﬁnite matrix.
Lemma 1. The population matrix A as deﬁned in equation (14) is a positive semideﬁnite matrix.

We can solve Equation (14) to get ˜Qi and ˜Qij. The Markov blanket of node i is computed by
MBGpiq “ tj| ˜Qij ‰ 0u. To this end  we prove that:
Lemma 2. If ˜Qijp¨ ¨q @j P t1  . . .   nu  j ‰ i is computed by solving system of linear equations (14)
and ˆP P P is faithful to G then ˜Qijp¨ ¨q ‰ 0 @j P t1  . . .   nu  j ‰ i if and only if j P MBGpiq.
Once we know the Markov blanket for each node i  the queries in Algorithm 2 can be changed from
fipAjq “ PpXi “ 0|XSztiu “ xAj
SXMBiq which helps
in reducing the sample and time complexity.
Number of Queries. Again  let | MBGpiq| ď k @i P t1  . . .   nu. The sparsity for each node would
` k ` 1. Thus number of queries needed for each node (using arguments
be less than or equal to
from Theorem 3) would be of order Opmaxpk2 log4 k  k2 log 1
δq. As before  these queries are repeated
nk times. Thus the total number of queries needed would be Opmaxpnk3 log4 k  nk3 log 1

Sztiuq to fipAjq “ PpXi “ 0|XSXMBi “ xAj

˘

`

k
2

δqq.

`

˘

n
2

` 3nq ´ N 2

Sample and Time Complexity. We use the following lemma to get the sample complexity for the
observational data.
2 q i.i.d observations are sufﬁcient to measure elements of A and y   close
Lemma 3. N “ Op log n
to their true value. That is |A ´ ˆA| ď  and |y ´ ˆy| ď   for some  ą 0 with probability at least
1 ´ 2 expplogp
2 q for some  ą 0 where ˆA and ˆy are the empirical measurements of
A and y respectively and | ¨ ´ ¨ | denotes componentwise comparison for matrices.
At this point  it remains to be shown that we can still recover the Markov blanket for the nodes using
the noisy measurements of unary and pairwise marginals. Below  we prove that this is true as long as
A is well conditioned.
Lemma 4. Let ˆA and ˆy be the empirical measurements of A and y as deﬁned in equation (14)
respectively such that |ˆA ´ A| ď  and |ˆy ´ y| ď  for some  ą 0  where | ¨ ´ ¨ | denotes
componentwise comparison for matrices. Let ˆq be the solution to the system of linear equations given
by ˆy “ ˆAˆq and ηκ8pAq ď 1  then ˆq recovers q up to signs as long as N “ Opnq i.i.d. measurements
´1}8 is the condition
are used to measure ˆA and maxi |qi|
number of A and η “ maxp
The time complexity of computing the Markov Blanket is Opn4q. The sample complexity for the
k log kq
black-box queries is Opmaxp nk3 log5 k
(See Appendix C for details).
For synthetic experiments validating our theory  please See Appendix D.

δ log kqq and the time complexity is Opnk4

mini |qi| ď 1´ηκ8pAq
ř
j“1 PpXj“0q`1
n´1

4ηκ8pAq   where κ8pAq ﬁ }A}8}A

PpXn“0qq.

  nk3

2 log 1

?

n

 

2

8

Concluding Remarks.
In this paper  we provide a novel method with theoretical guarantees to
recover directed structure of a Bayesian network using black-box queries. We further improve our
results when we have access to some observational data. We developed a theory for rank 2 CPTs
which can easily be extended to a more general rank k CPTs. It would be interesting to see if we
can provide similar results for a Bayesian network with low rank CPTs using pure observational or
interventional data.

References
Anderson  T. W. (1962). An Introduction to Multivariate Statistical Analysis. Technical report  Wiley

New York.

Bello  K. and Honorio  J. (2018). Computationally and Statistically Efﬁcient Learning of Causal
Bayes Nets Using Path Queries. In Advances in Neural Information Processing Systems  pages
10931–10941.

Cheng  J.  Greiner  R.  Kelly  J.  Bell  D.  and Liu  W. (2002). Learning Bayesian Networks From

Data: An Information-Theory Based Approach. Artiﬁcial intelligence  137(1-2):43–90.

Chickering  D. (1996). Learning Bayesian Networks Is NP-Complete. Learning from Data  pages

121–130.

Cussens  J. (2008). Bayesian Network Learning by Compiling to Weighted MAX-SAT. Uncertainty

in Artiﬁcial Intelligence.

Dvoretzky  A.  Kiefer  J.  and Wolfowitz  J. (1956). Asymptotic Minimax Character of the Sample
Distribution Function and of the Classical Multinomial Estimator. The Annals of Mathematical
Statistics  pages 642–669.

Eaton  D. and Murphy  K. (2007). Exact Bayesian Structure Learning From Uncertain Interventions.

Artiﬁcial Intelligence and Statistics  pages 107–114.

Friedman  N.  Nachman  I.  and Peér  D. (1999). Learning Bayesian Network Structure From
Massive Datasets: The Sparse Candidate Algorithm. In Proceedings of the Fifteenth conference on
Uncertainty in artiﬁcial intelligence  pages 206–215. Morgan Kaufmann Publishers Inc.

Hausar  A. and Bühlmann  P. (2012). Two Optimal Strategies for Active Learning of Causal Models
From Interventions. Proceedings of the 6th European Workshop on Probabilistic Graphical
Models.

He  Y. and Geng  Z. (2008). Active Learning of Causal Networks With Intervention Experiments and

Optimal Designs. Journal of Machine Learning Research.

Higham  N. J. (1994). A Survey of Componentwise Perturbation Theory  volume 48. American

Mathematical Society.

Jaakkola  T.  Sontag  D.  Globerson  A.  and Meila  M. (2010). Learning Bayesian Network Structure
Using LP Relaxations. In Proceedings of the Thirteenth International Conference on Artiﬁcial
Intelligence and Statistics  pages 358–365.

Kocaoglu  M.  Shanmugam  K.  and Bareinboim  E. (2017). Experimental Design for Learning
Causal Graphs With Latent Variables. In Advances in Neural Information Processing Systems 
pages 7018–7028.

Koivisto  M. and Sood  K. (2004). Exact Bayesian Structure Discovery in Bayesian Networks.

Journal of Machine Learning Research  5(May):549–573.

Margaritis  D. and Thrun  S. (2000). Bayesian Network Induction via Local Neighborhoods. In

Advances in neural information processing systems  pages 505–511.

Moore  A. and Wong  W.-K. (2003). Optimal Reinsertion: A New Search Operator for Accelerated
and More Accurate Bayesian Network Structure Learning. In International Conference on Machine
Learning  volume 3  pages 552–559.

9

Murphy  K. P. (2001). Active Learning of Causal Bayes Net Structure. Technical Report.

Rauhut  H. (2010). Compressive Sensing and Structured Random Matrices. Theoretical foundations

and numerical methods for sparse recovery  9:1–92.

Silander  T. and Myllymäki  P. (2006). A Simple Approach for Finding the Globally Optimal Bayesian

Network Structure. In Uncertainty in Artiﬁcial Intelligence  pages 445–452.

Spirtes  P.  Glymour  C. N.  and Scheines  R. (2000). Causation  Prediction  and Search. MIT press.

Stobbe  P. and Krause  A. (2012). Learning Fourier Sparse Set Functions. In Artiﬁcial Intelligence

and Statistics  pages 1125–1133.

Tong  S. and Koller  D. (2001). Active Learning for Structure in Bayesian Networks. International

Join Conference on Artiﬁcial Intelligence  17:863–869.

Triantaﬁllou  S. and Tsamardinos  I. (2015). Constraint-Based Causal Discovery From Multiple

Interventions Over Overlapping Variable Sets. Journal of Machine Learning Research.

Tsamardinos  I.  Brown  L. E.  and Aliferis  C. F. (2006). The Max-Min Hill-Climbing Bayesian

Network Structure Learning Algorithm. Machine learning  65(1):31–78.

Xie  X. and Geng  Z. (2008). A Recursive Method for Structural Learning of Directed Acyclic

Graphs. Journal of Machine Learning Research  9(Mar):459–483.

Yehezkel  R. and Lerner  B. (2005). Recursive Autonomy Identiﬁcation for Bayesian Network

Structure Learning. In Artiﬁcial Intelligence and Statistics  pages 429–436. Citeseer.

10

,Adarsh Barik
Jean Honorio