2019,Provable Gradient Variance Guarantees for Black-Box Variational Inference,Recent variational inference methods use stochastic gradient estimators whose variance is not well understood. Theoretical guarantees for these estimators are important to understand when these methods will or will not work. This paper gives bounds for the common “reparameterization” estimators when the target is smooth and the variational family is a location-scale distribution. These bounds are unimprovable and thus provide the best possible guarantees under the stated assumptions.,Provable Gradient Variance Guarantees for

Black-Box Variational Inference

Justin Domke

College of Information and Computer Sciences

University of Massachusetts Amherst

domke@cs.umass.edu

Abstract

Recent variational inference methods use stochastic gradient estimators whose
variance is not well understood. Theoretical guarantees for these estimators are
important to understand when these methods will or will not work. This paper
gives bounds for the common “reparameterization” estimators when the target is
smooth and the variational family is a location-scale distribution. These bounds
are unimprovable and thus provide the best possible guarantees under the stated
assumptions.

1

Introduction

Take a distribution p(z  x) representing relationships between data x and latent variables z. After
observing x  one might wish to approximate the marginal probability p(x) or the posterior p(z|x).
Variational inference (VI) is based on the simple observation that for any distribution q(z) 

log

p(z  x)
q(z)

+KL (q(z)kp(z|x)) .

(1)

log p(x) = Ez⇠q
|

}

ELBO(q)

{z
VI algorithms typically choose an approximating family qw and maximize ELBO(qw) over w.
Since log p(x) is ﬁxed  this simultaneously tightens a lower-bound on log p(x) and minimizes the
divergence from qw(z) to the posterior p(z|x).
Traditional VI algorithms suppose p and qw are simple enough for certain expectations to have closed
forms  leading to deterministic coordinate-ascent type algorithms [6  1  20]. Recent work has turned
towards stochastic optimization. There are two motivations for this. First  stochastic data subsampling
can give computational savings [7]. Second  more complex distributions can be addressed if p is
treated as a “black box”  with no expectations available [9  15  19]. In both cases  one can still
estimate a stochastic gradient of the ELBO [17] and thus use stochastic gradient optimization. It is
possible to address very complex and large-scale problems using this strategy [10].
These improvements in scale and generality come at a cost: Stochastic optimization is typically less
reliable than deterministic coordinate ascent. Convergence is often a challenge  and methods typically
use heuristics for parameters like step-sizes. Failures do frequently occur in practice [22  11  4].
To help understand when black-box VI can be expected to work  this paper investigates the variance
of gradient estimates. This is a major issue in practice  and many ideas have been proposed to attempt
to reduce the variance [8  5  12  2  18  13  14  16]. Despite all this  few rigorous guarantees on the
variance of gradient estimators seem to be known (Sec. 5.1).

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1.1 Contributions
This paper studies “reparameterization” (RP) or “path” based gradient estimators when qw is in a
multivariate location-scale family. We decompose ELBO(qw) = l(w) + h(w) where h(w) is the
entropy of qw (known in closed-form) and l(w) = Ez⇠qw log p(z  x). The key assumption is that
log p(z  x) is (Lipschitz) smooth as a function of z  meaning that rz log p(z  x) can’t change too
quickly as z changes. Formally f (z) is M-smooth if krf (z)  rf (z0)k2  Mkz  z0k2.
Bound for smooth target distributions: If g is the RP gradient estimator of rl(w) and log p is
M-smooth  then Ekgk2 is bounded by a quadratic function of w (Thm. 3). With a small
relaxation  this is Ekgk2  aM 2kw  ¯wk2 (Eq. (3)) where ¯w are ﬁxed parameters and a
is determined by the location-scale family.
Generalized bound: We extend this result to consider a more general notion of “matrix” smoothness
(Thm. 5) reﬂecting that the sensitivity of rz log p(z  x) to changes in z may depend on the
direction of change.
Data Subsampling: We again extend this result to consider data subsampling (Thm. 6). In particular 

we observe that non-uniform subsampling gives tighter bounds.

In all cases  we show that the bounds are unimprovable. We experimentally compare these bounds to
empirical variance.

2 Setup

Given some “black box” function f  this paper studies estimating gradients of functions l of the form
l(w) = Ez⇠qw f (z). Now  suppose some base distribution s and mapping Tw are known such that if
u ⇠ s  then Tw(u) ⇠ qw. Then  l can be written as
l(w) = Eu⇠s

f (Tw(u)).

If we deﬁne g = rwf (Tw(u))  then g is an unbiased estimate of rl  i.e. E g = rl(w). The same
idea can be used when f is composed as a ﬁnite sum as f (z) =PN
n=1 fn(z). If N is large  even
evaluating f once might be expensive. However  take any positive distribution ⇡ over n 2{ 1 ···   N}
and sample n ⇠ ⇡ independently of u. Then  if we deﬁne g = rw⇡(n)1fn(Tw(u))  this is again an
unbiased estimator with E g = rl(w).
Convergence rates in stochastic optimization depend on the variability of the gradient estimator 
typically either via the expected squared norm (ESN) Ekgk2
2 or the trace of the variance tr V g. These
are closely related  since Ekgk2
The goal of this paper is to bound the variability of g for reparameterization / path estimators of g.
This requires making assumptions about (i) the transformation function Tw and base distribution s
(which determine qw) and (ii) the target function f.
Here  we are interested in the case of afﬁne mappings. We use the mapping[17]

2 = tr V g + k E gk2
2.

Tw(u) = Cu + m 

where w = (m  C) is a single vector of all parameters. This is the most common mapping used
to represent location-scale families. That is  if u ⇠ s then Tw(u) is equal in distribution to a
location-scale family distribution. For example  if s = N (0  I) then Tw(u) is equal in distribution to
N (m  CC>).
We will refer to the base distribution as standardized if the components of u = (u1 ···   ud) ⇠ s are
iid with E u1 = E u3
1 = 0 and V u1 = 1. The bounds will depend on the fourth moment  = E[u4
1] 
but are otherwise independent of s.
To apply these estimators to VI  choose f (z) = log(z  x). Then ELBO(w) = l(w) + h(w)
where h is the entropy of qw. Stochastic estimates of the gradient of l can be employed in a
stochastic gradient method to maximize the ELBO. To model the stochastic setting  suppose that
n=1 p(xn|z). Then  one may choose  e.g. fn(z) =

X = (x1 ···   xN ) are iid and p(z  X) = p(z)QN

2

1

N log p(z) + log p(xn|z). The entropy h is related to the (constant) entropy of the base distribution
as h(w) = Entropy(s) + log |C|.
The main bounds of this paper concern estimators for the gradient of l(w) alone  disregarding h(w).
There are two reasons for this. First  in location-scale families  the exact gradient of h(w) is known.
Second  if one uses a stochastic estimator for h(w)  this can be “absorbed” into l(w) to some degree.
This is discussed further in Sec. 5.

3 Variance Bounds

3.1 Technical Lemmas
We begin with two technical lemmas which will do most of the work in the main results. Both have
(somewhat laborious) proofs in Sec. 7 (Appendix). The ﬁrst lemma relates the norm of the parameter
gradient of f (Tw(u)) (with respect to w) to the norm of the gradient of f (z) itself  evaluated at
z = Tw(u).
Lemma 1. For any w and u  krwf (Tw(u))k2
The proof is tedious but essentially amounts to calculating the derivative with respect to each
component of w (i.e. entries mi and Cij)  summing the square of all entries  and simplifying. The
second lemma gives a closed-form for the expectation of a closely related expression that will appear
in the proof of Thm. 3 as a consequence of applying Lem. 1.
Lemma 2. Let u ⇠ s for s standardized with u 2 Rd and Eu⇠s u4
2 = (d + 1)km  ¯zk2

i = . Then for any ¯z 
2 + (d + )kCk2
F .

Again  the proof is tedious but based on simple ideas: Substitute the deﬁnition of Tw into the left-hand
side and expand all terms. This gives terms between zeroth and fourth order (in u). Calculating the
exact expectation of each and simplifying using the assumption that s is standardized gives the result.

2⇣1 + kuk2
2⌘ .

21 + kuk2

2 = krf (Tw(u))k2

EkTw(u)  ¯zk2

3.2 Basic Variance Bound
Given these two lemmas  we give our major technical result  bounding the variability of a
reparameterization-based gradient estimator. This will be later be extended to consider data subsam-
pling  and a generalized notion of smoothness. Note that we do not require that f be convex.
Theorem 3. Suppose f is M-smooth  ¯z is a stationary point of f  and s is standardized with u 2 Rd
and E u4

i = . Let g = rwf (Tw(u)) for u ⇠ s. Then 
2  M 2⇣(d + 1)km  ¯zk2

Ekgk2

F⌘ .
2 + (d + )kCk2

Moreover  this result is unimprovable without further assumptions.

(2)

Proof. We expand the deﬁnition of g  and use the above lemmas and the smoothness of f.

Ekgk2

2

2 = Ekrwf (Tw(u))k2
= Ekrf (Tw(u))k2
2 (1 + kuk2
2)
= Ekrf (Tw(u))  rf ( ¯z)k2
2 (1 + kuk2
2)
 E M 2 kTw(u)  ¯zk2
= M 2⇣(d + 1)km  ¯zk2

F⌘ .
2 + (d + )kCk2

2 (1 + kuk2
2)

(Deﬁnition of g)
(Lem. 1)
(rf ( ¯z) = 0)
(f is smooth)
(Lem. 2)

To see that this is unimprovable without further assumptions  observe that the only inequality is using
the smoothness on f to bound the norm of the difference of gradients at Tw(u) and at ¯z. But for
2 this inequality is tight. Thus  for any M and ¯z  there is a function f satisfying
f (z) = M
the assumptions of the theorem such that Eq. (2) is an equality.

2 kz  ¯zk2

3

With a small amount of additional looseness  we can cast Eq. (2) into a more intuitive form. Deﬁne
F   so
¯w = ( ¯z  0d⇥d)  where 0d⇥d is a d ⇥ d matrix of zeros. Then  kw  ¯wk2
we can slightly relax Eq. (2) to the more user-friendly form of

2 = km  ¯zk2

2 + kCk2

Ekgk2

2  (d + )M 2 kw  ¯wk2
2 .

(3)
The only additional looseness is bounding d+1  d+. This is justiﬁed since when s is standardized 
i is the kurtosis  which is at least one. Here   is determined by s and does not depend on the
 = u4
dimensionality. For example  if s is Gaussian   = 3. Thus  Eq. (3) will typically not be much looser
than Eq. (2).
Intuitively  ¯w are parameters that concentrate q entirely at a stationary point of f. It is not hard to
show that kw  ¯wk2 = Ez⇠qw kz  ¯zk2. Thus  Eq. (3) intuitively says that Ekgk2 is bounded in
terms of how far far the average point sampled from qw is from ¯z. Since f need not be convex  there
might be multiple stationary points. In this case  Thm. 3 holds simultaneously for all of them.

3.3 Generalized Smoothness

Since the above bound is not improvable  tightening it requires stronger assumptions. The tightness
of Thm. 3 is determined by the smoothness condition that the difference of gradients at two points
is bounded as krf (y)  rf (z)k2  M ky  zk2. For some problems  f may be much smoother
in certain directions than others. In such cases  the smoothness constant M will need to reﬂect the
worst-case direction. To produce a tighter bound for such situations  we generalize the notion of
smoothness to allow M to be a symmetric matrix.
Deﬁnition 4. f is M-matrix-smooth if krf (y)  rf (z)k2  kM (y  z)k2 (for symmetric M).
We can generalize the result in Thm. 3 to functions with this matrix-smoothness condition. The proof
is very similar. The main difference is that after applying the smoothness condition  the matrix M
needs to be “absorbed” into the parameters w = (m  C) before applying Lem. 2.
Theorem 5. Suppose f is M-matrix smooth  ¯z is a stationary point of f  and s is standardized with
u 2 Rd and E u4

i = . Let g = rwf (Tw(u)) for u ⇠ s. Then 

Ekgk2

2  (d + 1)kM (m  ¯z)k2

2 + (d + )kM Ck2
F .

(4)

Moreover  this result is unimprovable without further assumptions.

Proof. The proof closely mirrors that of Thm. 3. Here  given w = (m  C)  we deﬁne v =
(M m  M C)  to be w with M “absorbed” into the parameters.

Ekgk2

2

2 = Ekrwf (Tw(u))k2
= Ekrf (Tw(u))k2
2 (1 + kuk2
2)
= Ekrf (Tw(u))  rf ( ¯z)k2
2 (1 + kuk2
2)
 EkM (Tw(u)  ¯z)k2
= EkTv(u)  M ( ¯z  m)k2
= (d + 1)kM m  M ¯zk2

2 (1 + kuk2
2)

2 (1 + kuk2
2)
2 + (d + )kM Ck2
F .

Deﬁnition of g)
(Lem. 1)
(rf ( ¯z) = 0)
(f is smooth)
(Absorb M into v)
(Lem. 2)

To see that this is unimprovable  observe that the only inequality is the matrix-smoothness condition
2 (z  ¯z)>M (z  ¯z)  the difference of gradients krf (y)  rf (z)k2 =
on f. But for f (z) = 1
kM (y  z)k2 is an equality. Thus  for any M and ¯z  there is an f satisfying the assumptions of the
theorem such that the bound in Eq. (4) is an equality.

It’s easy to see that this reduces to Thm. 3 in the case that f is smooth in the standard sense– this
corresponds to the situation where M is some constant times the identity. Alternatively  one can
simply observe that the two results are the same if M is a scalar. Thus  going forward we will use
Eq. (4) to represent the result with either type of smoothness assumption on f.

4

3.4 Subsampling

Often  the function f (z) takes the form of a sum over other functions fn(z)  typically representing
different data. Write this as

f (z) =

fn(z).

NXn=1

To estimate the gradient of Eu⇠s f (Tw(u))  one can save time by using “subsampling”: That is  draw
a random n  and then estimate the gradient of Eu⇠s fn(Tw(u)). The following result bounds this
procedure. It essentially just takes a set of estimators  one corresponding to each function fn  bounds
their expected squared norm using the previous theorems  and then combines these.
Theorem 6. Suppose fn is Mn-matrix-smooth  ¯zn is a stationary point of fn  and s is standardized
with u 2 Rd and E u4

⇡(n)rfn(Tw(u)) for u ⇠ s and n ⇠ ⇡ independent. Then 

i = . Let g = 1

1

⇡(n)⇣(d + 1)kMn(m  ¯zn)k2

F⌘ .
2 + (d + )kMnCk2

(5)

Moreover  this result is unimprovable without further assumptions.

Ekgk2

2 

NXn=1

n=1 E an and Ekbk2

Proof. Consider a simple lemma: Suppose a1 ··· aN are independent random vectors and ⇡ is any
distribution over 1··· N. Let b = an/⇡(n) for n ⇠ ⇡  where n is independent of an. It is easy to
2 =Pn Ekank2
show that E b =PN
2 /⇡(n). The result follows from applying
this with an = rwfn (Tw(u))  and then bounding Ekank2
2 using Thm. 5.
Again  in this result the only source of looseness is the use of the smoothness bound for the component
functions fn. Accordingly  the result can be shown to be unimprovable: For any set of stationary
points ¯z and smoothness parameters Mn we can construct functions fn (as in Thm. 5) for which the
previous theorems are tight and thus this result is also tight.

This result generalizes all the previous bounds: Thm. 5 is the special case when N = 1  while Thm. 3
is the special-case when N = 1 and f1 is M1-smooth (for a scalar M1). The case where N > 1 but
fn is Mn-smooth (for scalar Mn) is also useful– the bound in Eq. (5) remains valid  but with a scalar
Mn.

4 Empirical Evaluation

4.1 Model and Datasets

We consider Bayesian linear regression and logistic regression models on various datasets (Table 1).
Given data {(x1  y1) ··· (xN   yN )}  let y be a vector of all yn and X a matrix of all xn. We
assume a Gaussian prior so that p(z  y|X) = N (z|0  2I)QN
n=1 p(yn|z  xn). For linear regression 
p(yn|z  xn) = N (yn|z>xi ⇢ 2)  while for logistic regression  p(yn|z  xn) = Sigmoid(ynz>xn).
For both models we use a prior of 2 = 1. For linear regression  we set ⇢2 = 4.
To justify the use of VI  apply the decomposition in Eq. (1) substituting p(z  y|X) in place of p(z  x)
to get that

log p(y|X) = Ez⇠q

log

p(z  y|X)

q(z)

+ KL (q(z)kp(z|y  X)) .

Thus  adjusting the parameters of q to maximize the ﬁrst term on the right tightens a lower-bound
on the conditional log likelihood log p(y|X) and minimizes the divergence from q to the posterior.
So  we again take our goal as maximizing l(w) + h(w). In the batch setting  f (z) = log p(z  y|X) 
while with subsampling  fn(z) = 1

N log p(z) + log p(yn|z  xn).

5

Figure 1: How loose are the bounds compared to reality? Odd Rows: Evolution of the ELBO
during the single optimization trace used to compare all estimators. Even Rows: True and bounded
variance with gradients estimated in “batch” (using the full dataset in each evaluation) and “uniform”
(stochastically with ⇡(n) = 1/N). The ﬁrst two rows are for linear regression models  while the
rest are for logistic regression. Key Observations: (i) Batch estimation is lower-variance but higher
cost (ii) variance with stochastic estimation varies little over time (iii) using matrix smoothness
signiﬁcantly tightens bounds – and is exact for linear regression models.

Sec. 8 shows that if 0  00(t)  ✓  thenPN
✓PN

i=1 aia>i . Applying this1 gives that f (z) and fn(z) are matrix-smooth for

n=1 (a>n z + bn) is M-matrix-smooth for M =

M =

1
2 I + c

NXn=1

xnx>n   and Mn =

1
N 2 I + c xnx>n  

1For linear regression  set (t) = t2/(2⇢2)  an = xn and bn = yn and observe that 00 = 1/⇢2. For
logistic regression  set (t) = log Sigmoid(t)  an = ynxn and bn = 0 and observe that 00  1/4. Adding
the prior and using the triangle inequality gives the result.

6

10.01000.0k1e51e81e111e14g2Grad Estimatorbatchuniformbound (matrix)bound (scalar)truemushroomsFigure 2: Tightening variance bounds reduces true variance. A comparison of the true (vertical
bars) and bounded Ekgk2 values produced using ﬁve different gradient estimators. Batch does
not use subsampling. Uniform uses subsampling ⇡(n) = 1/N  proportional uses ⇡(n) / Mn 
opt (scalar) numerically optimizes ⇡(n) to tighten Eq. (5) with a scalar Mn and opt (matrix)
tightens Eq. (5) with a matrix Mn. For each sampling strategy  we show the variance bound both with
a scalar and matrix Mn. Uniform sampling has true and bounded values of Ekgk2 ranging between
1.5x and 10x higher than those for sampling with ⇡ numerically optimized.

Dataset
boston
ﬁres

cpusmall

a1a

ionosphere
australian

sonar

mushrooms

r
r
r
c
c
c
c
c

Type

# dims

# data
506
517
8192
1695
351
690
208
8124

13
12
13
124
35
15
61
113

where c = 1/⇢2 for linear regression  and c = 1/4
for logistic regression. Taking the spectral norm
of these matrices gives scalar smoothness constants.
With subsampling  this is kMnk2 = 1
2N + ckxnk2.

4.2 Evaluation of Bounds

Table 1: Regression (r) and classiﬁcation (c)
datasets

To enable a clear comparison of of different estima-
tors and bounds  we generate a single optimization
trace of parameter vectors w for each dataset. All
comparisons use this same trace. These use a con-
servative optimization method: Find a maximum ¯z
and then initialize to w = ( ¯z  0). Then  optimization
uses proximal stochastic gradient descent (with the proximal operator reﬂecting h) with a step size of
1/M (the scalar smoothness constant) and 1000 evaluations for each gradient estimate.
Fig. 1 shows the evolution of the ELBO along with the variance of gradient estimation either in batch
or stochastically with a uniform distribution over data. For each iteration and estimator  we plot the
empirical kgk2 along with this paper’s bounds using either scalar or matrix smoothness.

4.3 Sampling distributions

With subsampling  variability depends on the sampling distribution ⇡. We consider uniform sampling
as well as three strategies that attempt to tighten the bound in Thm. 6. In general Pn f (n)2/⇡(n) is
minimized over distributions ⇡ by ⇡(n) /| f (n)|. Thus  the tightest bound is given by

7

1e41e81e12g2batchopt (matrix)opt (scalar)proportionaluniformGrad Estimator1.18e+099.35e+101.15e+111.15e+111.15e+112.42e+125.93e+134.79e+134.79e+134.79e+131.53e+051.03e+089.66e+078.9e+073.93e+071.53e+051.03e+089.66e+078.9e+073.93e+07Bound Typebound (matrix)bound (scalar)mushrooms⇡⇤w(n) /q(d + 1)kMn(m  ¯zn)k2

2 + (d + )kMnCk2
F .

(6)

We call this “opt (scalar)” or “opt (matrix)” when Mn is a scalar or matrix  respectively. We also
consider a “proportional” heuristic with ⇡(n) / Mn for a scalar Mn. Sampling from Eq. (6) appears
to require calculating the right-hand side for each n and then normalizing  which may not be practical
for large datasets. While there are obvious heuristics for recursively approximating ⇡⇤ during an
optimization  to maintain focus we do not pursue these ideas here.
Fig. 2 shows the empirical and true variance at the ﬁnal iteration of the optimization shown in
Fig. 1. The basic conclusion is that using a more careful sampling distribution reduces both true and
empirical variance.

5 Discussion

5.1 Related work

Xu et al. [21] compute the variance of a reparameterization estimator applied to a quadratic function 
when the variational distribution is a fully-factorized Gaussian. This paper can be seen as extending
this result to more general densities (full-rank location-scale families) and more general target
functions (smooth functions).
Fan et al. [4] give an abstract variance bound for RP estimators. Essentially  they argue that if
gi = rwif (Tw(u)) and rwif (Tw(u)) is M-smooth as a function of u  then V[gi]  M 2⇡2/4
when u ⇠N (0  I). While this result is fairly abstract – there is no proof that the smoothness
assumption holds for any particular M with any particular f and Tw – it is similar in spirit to the
results in this paper.

5.2 Variance vs Expected Squared Norms
The above results are on the the expected squared norm (ESN) of the gradient Ekgk2. Some
stochastic gradient convergence rates instead consider (the trace of) the variance V[g]. Since tr V[g] =
Ekgk2  k E gk2  ESN bounds are valid as variance bounds. Still  one can ask if these bounds are
loose. The following (proof in Sec. 7.3) gives a lower-bound that shows that there is not much to gain
from a direct bound on the variance rather than just using the ESN bound from Thm. 6.
Theorem 7. For any symmetric matrices M1 ···   MN and vectors ¯z1 ···   ¯zN   there are functions
f1 ···   fN such that (1) fn is Mn-matrix-smooth and has a stationary point at ¯zn and (2) if s is
standardized with u 2 Rd and E u4
NXn=1
1

F⌘ .
2 + (d +   1)kMnCk2

⇡(n)⇣dkMn(m  ¯zn)k2

tr Vkgk2

2 

i =   then for g = 1

⇡(n)rfn(Tw(u)) 

When d  1 this lower-bound is very close to the upper-bound on Ekgk2 in Thm. 6. Thus  under
this paper’s assumptions  a variance bound cannot be signiﬁcantly better than an ESN bound.

5.3 The Entropy Term

All discussion in this paper has been for gradient estimators for l  while the goal is of course to
optimize l + h. For location-scale families  h is known in closed-form  meaning the exact gradient –
or the proximal operator for h – can be computed exactly. Still  it has been observed that if qw is very
close to p(z|x)  cancellations mean that estimating the gradient of h + l might have lower variance
than the gradient of l alone [12].
With any variational family  it is well-known that the gradient of the entropy can be represented
as rw Ez⇠qw log qv(z)|v=w. That is  the dependence of log qw on w can be neglected under
differentiation. Thus  if one wishes to stochastically estimate the gradient of h  one can treat log qv in
the same way as log p when calculating gradients. Then  one could apply the analysis in this paper to
f (z) = log p(z  x)  log qv(z) rather than f (z) = log p(z  x) as done above. It is easy to imagine

8

situations where subtracting log qv (or a fraction of it) from log p would change Mn and ¯zn in such a
way as to produce a tighter bound. Thus  the bounds in this paper are consistent with practices [5  12]
where using log qv as a control variate can reduce gradient variance.

5.4 Smoothness and Convergence Guarantees

At a very high level  convergence rates for stochastic gradient methods require both (1) control of the
variability of the gradient estimator and (2) either convexity or Lipschitz smoothness of the objective.
This paper is dedicated entirely to the ﬁrst goal. Independent recent work has addressed at the second
issue [3]. The basic summary is that if f (z) is smooth  then l(w) is smooth  and similarly if f (z) is
strongly convex. However  full convergence guarantees for black-box VI remain an open research
problem.

5.5 Prospects for Generalizing Bounds to Other Variational Families

The bounds given in this paper are closely tied to location-scale families: The exact form of the
reparameterization function Tw is used in Lem. 1 and Lem. 2  which underly the main results of
Thm. 3  Thm. 5  and Eq. (4). Thus  extending our proof strategy to other variational families would
require deriving new results analogous to Lem. 1 and Lem. 2 for the reparameterization function
Tw corresponding to those new variational families. Moreover  if the exact entropy is not available
for a variational family  the analysis must address the variance of the entropy gradient estimator  as
discussed in Sec. 5.3.

5.6 Limitations

This work has several limitations. First  it applies only to location-scale families  and requires that
the target objective be smooth. Second  if log p is smooth  it may still be challenging in practice to
establish what the smoothness constant is. Third  we observed that even with our strongest condition
of matrix smoothness  the some looseness remains in the bounds with the logistic regression examples.
Since the ESN bound is unimprovable  this looseness cannot be removed without using more detailed
structure of the target log p. It is not obvious what this structure would be  or how it would be
obtained for practical black-box inference problems.

References
[1] David M. Blei  Alp Kucukelbir  and Jon D. McAuliffe. Variational Inference: A Review for

Statisticians. Journal of the American Statistical Association  112(518):859–877  2017.

[2] Alexander Buchholz  Florian Wenzel  and Stephan Mandt. Quasi-Monte Carlo Variational

Inference. In ICML  2018.

[3] Justin Domke.

Provable Smoothness Guarantees for Black-Box Variational Inference.

arXiv:1901.08431 [cs  stat]  2019.

[4] Kai Fan  Ziteng Wang  Jeff Beck  James Kwok  and Katherine Heller. Fast Second-Order

Stochastic Backpropagation for Variational Inference. In NeurIPS  2015.

[5] Tomas Geffner and Justin Domke. Using Large Ensembles of Control Variates for Variational

Inference. In NeurIPS  2018.

[6] Zoubin Ghahramani and Matthew Beal. Propagation Algorithms for Variational Bayesian

Learning. In NeurIPS  2001.

[7] Matthew D. Hoffman  David M. Blei  Chong Wang  and John Paisley. Stochastic Variational

Inference. Journal of Machine Learning Research  14:1303–1347  2013.

[8] Andrew Miller  Nick Foti  Alexander D’ Amour  and Ryan P Adams. Reducing Reparameteri-

zation Gradient Variance. In NeurIPS  2017.

[9] Rajesh Ranganath  Sean Gerrish  and David M. Blei. Black Box Variational Inference. In

AISTATS  2014.

9

[10] Jeffrey Regier  Kiran Pamnany  Ryan Giordano  Rollin Thomas  David Schlegel  Jon McAuliffe 
and Prabhat. Learning an Astronomical Catalog of the Visible Universe through Scalable
Bayesian Inference. arXiv:1611.03404 [astro-ph  stat]  2016.

[11] Jeffrey Regier  Michael I Jordan  and Jon McAuliffe. Fast Black-box Variational Inference

through Stochastic Trust-Region Optimization. In NeurIPS  page 10  2017.

[12] Geoffrey Roeder  Yuhuai Wu  and David K Duvenaud. Sticking the Landing: Simple  Lower-

Variance Gradient Estimators for Variational Inference. In NeurIPS  2017.

[13] Francisco J. R. Ruiz  Michalis K. Titsias  and David M. Blei. The Generalized Reparameteriza-

tion Gradient. In NeurIPS  2016.

[14] Francisco J. R. Ruiz  Michalis K. Titsias  and David M. Blei. Overdispersed Black-Box

Variational Inference. arXiv:1603.01140 [stat]  2016.

[15] Tim Salimans and David A. Knowles. Fixed-Form Variational Posterior Approximation through

Stochastic Linear Regression. Bayesian Analysis  8(4):837–882  2013.

[16] Linda S. L. Tan and David J. Nott. Gaussian variational approximation with sparse precision

matrices. Statistics and Computing  28(2):259–275  2018.

[17] Michalis Titsias and Miguel Lázaro-gredilla. Doubly Stochastic Variational Bayes for non-

Conjugate Inference. In ICML  2014.

[18] Michalis K. Titsias and Miguel Lázaro-Gredilla. Local Expectation Gradients for Black Box

Variational Inference. In NeurIPS  2015.

[19] David Wingate and Theophane Weber. Automated Variational Inference in Probabilistic Pro-

gramming. arXiv:1301.1299 [cs  stat]  2013.

[20] John Winn and Christopher M Bishop. Variational Message Passing. Journal of Machine

Learning Research  6:661–694  2005.

[21] Ming Xu  Matias Quiroz  Robert Kohn  and Scott A. Sisson. Variance reduction properties of

the reparameterization trick. In AISTATS  2019.

[22] Yuling Yao  Aki Vehtari  Daniel Simpson  and Andrew Gelman. Yes  but Did It Work?:

Evaluating Variational Inference. In ICML  2018.

10

,Justin Domke