2018,Invertibility of Convolutional Generative Networks from Partial Measurements,In this work  we present new theoretical results on convolutional generative neural networks  in particular their invertibility (i.e.  the recovery of input latent code given the network output). The study of network inversion problem is motivated by image inpainting and the mode collapse problem in training GAN. Network inversion is highly non-convex  and thus is typically computationally intractable and without optimality guarantees. However  we rigorously prove that  under some mild technical assumptions  the input of a two-layer convolutional generative network can be deduced from the network output efficiently using simple gradient descent. This new theoretical finding implies that the mapping from the low- dimensional latent space to the high-dimensional image space is bijective (i.e.  one-to-one). In addition  the same conclusion holds even when the network output is only partially observed (i.e.  with missing pixels). Our theorems hold for 2-layer convolutional generative network with ReLU as the activation function  but we demonstrate empirically that the same conclusion extends to multi-layer networks and networks with other activation functions  including the leaky ReLU  sigmoid and tanh.,Invertibility of Convolutional Generative
Networks from Partial Measurements

Fangchang Ma*

MIT

fcma@mit.edu

Ulas Ayaz˚

MIT

uayaz@mit.edu
uayaz@lyft.com

Sertac Karaman

MIT

sertac@mit.edu

Abstract

The problem of inverting generative neural networks (i.e.  to recover the input latent
code given partial network output)  motivated by image inpainting  has recently
been studied by a prior work that focused on fully-connected networks. In this
work  we present new theoretical results on convolutional networks  which are more
widely used in practice. The network inversion problem is highly non-convex  and
hence is typically computationally intractable and without optimality guarantees.
However  we rigorously prove that  for a 2-layer convolutional generative network
with ReLU and Gaussian-distributed random weights  the input latent code can be
deduced from the network output efﬁciently using simple gradient descent. This
new theoretical ﬁnding implies that the mapping from the low-dimensional latent
space to the high-dimensional image space is one-to-one  under our assumptions.
In addition  the same conclusion holds even when the network output is only
partially observed (i.e.  with missing pixels). We further demonstrate  empirically 
that the same conclusion extends to networks with multiple layers  other activation
functions (leaky ReLU  sigmoid and tanh)  and weights trained on real datasets.

1

Introduction

In recent years  generative models have made signiﬁcant progress in learning representations for
complex and multi-modal data distributions  such as those of natural images [10  18]. However 
despite the empirical success  there has been relatively little theoretical understanding into the
mapping itself from the input latent space to the high-dimensional space. In this work  we address the
following question: given a convolutional generative network2  is it possible to “decode” an output
image and recover the corresponding input latent code? In other words  we are interested in the
invertibility of convolutional generative models.
The impact of the network inversion problem is two-fold. Firstly  the inversion itself can be applied
in image in-painting [21  17]  image reconstruction from sparse measurements [14  13]  and image
manipulation [22] (e.g.  vector arithmetic of face images [12]). Secondly  the study of network
inversion provides insight into the mapping from the low-dimensional latent space to the high-
dimensional image space (e.g.  is the mapping one-to-one or many-to-one?). A deeper understanding
of the mapping can potentially help solve the well known mode collapse3 problem [20] during the
training in the generative adversarial network (GAN) [7  16].

˚Both authors contributed equally to this work. Ulas Ayaz is presently afﬁliated with Lyft  Inc.
2Deep generative models typically use transposed convolution (a.k.a. “deconvolution”). With a slight abuse

of notation we refer to transposed convolutional generative models as convolutional models.

3Mode collapse refers to the problem that the Generator characterizes only a few images to fool the
discriminator in GAN. In other words  multiple latent codes are mapped to the same output in the image space.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Recovery of the input latent code z from under-sampled measurements y “ AGpzq where
A is a sub-sampling matrix and G is an expanding generative neural network. We prove that z can be
recovered with guarantees using simple gradient-descent methods under mild technical assumptions.

The challenge of the inversion of a deep neural network lies in the fact that the inversion problem is
highly non-convex  and thus is typically computationally intractable and without optimality guaran-
tees. However  in this work  we show that network inversion can be solved efﬁciently and optimally 
despite being highly non-convex. Speciﬁcally  we prove that with simple ﬁrst-order algorithms like
stochastic gradient descent  we can recover the latent code with guarantees. The sample code is
available at https://github.com/fangchangma/invert-generative-networks.

1.1 Related Work

The network inversion problem has attracted some attention very recently. For instance  Bora et al. [2]
empirically ﬁnd that minimizing the non-convex Problem (3)  which is deﬁned formally in Section 2 
using standard gradient-based optimizer yields good reconstruction results from small number of
Gaussian random measurements. They also provide guarantees on the global minimum of a generative
network with certain structure. However  their work does not analyze how to ﬁnd the global minimum.
Hand and Voroninski [8] further establish that a fully connected generative network with weights
following Gaussian distribution can be inverted given only compressive linear observations of its last
layer. In particular  they show that under mild technical conditions Problem (3) has a favorable global
geometry  in the sense that there are no stationary points outside of neighborhoods around the desired
solution and its negative multiple with high probability. However  most practical generative networks
are deconvolutional rather than fully connected  due to memory and speed constraints. Besides  their
results are proved for Gaussian random measurements  which are rarely encountered in practical
applications. In this work  we build on top of [8] and extend their results to 2-layer deconvolutional
neural networks  as well as uniform random sub-sampling.
We also note the work [6]  which studies a 1-layer network with a special activation function
(Concatenated ReLU  which is essentially linear) and a strong assumption on the latent code (k-
sparsity). In comparison  our results are much stronger than [6]. Speciﬁcally  our results are for
2-layer networks (with empirical evidences for deeper networks)  and they apply to the most common
ReLU activation function. Our result also makes no assumption regarding the sparsity of latent codes.
Another line of research  which focuses on gradient-based algorithms  analyzes the behavior of
(stochastic) gradient descent for Gaussian-distributed input. Soltanolkotabi [19] showed that projected
gradient descent is able to ﬁnd the true weight vector for 1-layer  1-neuron model. More recently 
Du et al. [5] improved this result for a simple convolutional neural network with two unknown
layers. Their assumptions on random input and their problem of weight learning are different than
the problem we study in this paper.
Our problem is also connected to compressive sensing [4  3] which exploits the sparsity of natural
signals to design acquisition schemes where the number of measurements scales linearly with the
sparsity level. The signal is typically assumed to be sparse in a given dictionary  and the objective
function is convex. In comparison  our work does not assume sparsity  and we provide a direct
analysis of gradient descents for the highly non-convex problem.

2

1.2 Contribution

The contribution of this work is three-fold:

• We prove that a convolutional generative neural network is invertible  with high probability 
under the following assumptions: (1) the network consists of two layers of transposed con-
volutions followed by ReLU activation functions; (2) the network is (sufﬁciently) expansive;
(3) the ﬁlter weights follow a Gaussian distribution. When these conditions are satisﬁed 
the input latent code can be recovered from partial output of a generative neural network by
minimizing a L2 empirical loss function using gradient descent.
subset of pixels is observed. This is essentially the image inpainting problem.

• We prove that the same inversion can be achieved with high probability  even when only a
• We validate our theoretical results using both random weights and weights trained on real
data. We further demonstrate empirically that that our theoretical results generalize to (1)
multiple-layer networks; (2) networks with other nonlinear activation functions  including
Leaky ReLU  Sigmoid and Tanh.

Two key ideas of our proof include (a) the concentration bounds of convolutional weight matrices
combined with ReLU operation  and (b) the angle distortion between two arbitrary input vectors
under the transposed convolution and ReLU. In general  our proof follows a similar basic structure
to [8]  where the authors show the invertibility of fully connected networks with Gaussian weights.
However  in fully connected networks  the weight matrix of each layer is a dense Gaussian matrix. In
contrast  in convolutional networks the weight matrices are highly sparse with block structure due to
striding ﬁlters  as in Figure 2(a). Therefore  [8]’s proof does not apply to convolutional networks  and
the extension of concentration bounds for our case is not trivial.
To address such problem  we propose a new permutation technique which shufﬂes the rows and
columns of weight matrices to obtain a block matrix  as depicted in Figure 2(b). With permutation 
each block is now a dense Gaussian matrix  where we can apply existing matrix concentration results.
However  the permutation operation is quite arbitrary  depending on the structure of the convolutional
network. This requires some careful handling  since the second step (b) requires the control of angles.
In addition  Hand and Voroninski [8] assume a Gaussian sub-sampling matrix at the output of the
network  rather than partial sub-sampling (sub-matrix of identity matrix) that we study in this problem.
We observe that sub-sampling operation can be swapped with the last ReLU in the network  since
both are entrywise operations. We handle the sub-sampling by making the last layer more expansive 
and prove that it is the same with no downsampling from a theoretical standpoint.

2 Problem Statement
In this section  we introduce the notation and deﬁne the network inversion problem. Let z˛ P Rn0
denote the latent code of interest  Gp¨q : Rn0 Ñ Rnd pn0 ! ndq be a d-layer generative network
that maps from the latent space to the image space. Then the ground truth output image x˛ P Rnd is
produced by

(1)
In this paper we consider Gp¨q to be a deep neural network4. In particular we assume Gp¨q to be a
two-layer transposed convolutional network  modeled by

x˛ “ Gpz˛q 

Gpzq “ σpW2σpW1zqq

(2)
where σpzq “ maxpz  0q denotes the rectiﬁed linear unit (ReLU) that applies entrywise. W1 P
Rn1ˆn0 and W2 P Rn2ˆn1  are the weight matrices of the convolutional neural network in the ﬁrst
and second layers  respectively. Note that since Gp¨q is a convolutional network  W1 and W2 are
highly sparse with a particular block structure  as illustrated in Figure 2(a).
Let us make the inversion problem a bit more general by assuming that we only have partial
observations of the output image pixels. Speciﬁcally  let A P Rmˆn2 be a sub-sampling matrix
4Note that this network inversion problem happens at the inference stage  and thus is independent of the

training process.

3

(a subset of the rows of an identity matrix)  and then the observed pixels are y˛ “ Ax˛ P Rm.
Consequently  the inversion problem given partial measurements can be described as follows:

z˛ P Rn0  W1 P Rn1ˆn0  W2 P Rn2ˆn1   A P Rmˆn2

Let:
Given: A  W1  W2 and observations y˛ “ AGpz˛q
Find: z˛ and x˛ “ Gpz˛q

Since x˛ is determined completely by the latent representation z˛  we only need to ﬁnd z˛. We
propose to solve the following optimization problem for an estimate ˆz:

ˆz “ arg min

z

Jpzq  where Jpzq “ 1
2

}y˛ ´ AGpzq}2

(3)

This minimization problem is highly non-convex because of G. Therefore  in general a gradient
descent approach is not guaranteed to ﬁnd the global minimum z˛  where Jpz˛q “ 0.

2.1 Notation and Assumptions

(a)

(b)

Figure 2: Illustration of a single transposed convolution operation. fi j stands for ith ﬁlter kernel
for the jth input channel. z and x denote the input and output signals  respectively.
(a) The
standard transposed convolution represented as linear multiplication. (b) With proper row and column
permutations  the permuted weight matrix has a repeating block structure.

We vectorize the input signal to 1D signal. The feature at the ith layer consists of Ci channels  each
of size Di. Therefore  ni “ Ci ¨ Di. At any convolutional layer  let fi j denotes the kernel ﬁlter
(each of size (cid:96)) for the ith input channel and the jth output channel. For simplicity  we assume the
stride to be equal to the kernel size l. All ﬁlters can be concatenated to form a large block matrix Wi.
For instance  an example of such block matrix W1 for the ﬁrst layer is shown in Figure 2(a). Under
our assumptions  the input and output sizes at each deconvolution operation can be associated as
Di`1 “ Di(cid:96).
Let DvJpxq be one-sided directional derivative of the objective function Jp¨q along the direction
v  i.e.  DvJpxq “ limtÑ0` Jpx`tvq´Jpxq
. Let Bpx  rq be the Euclidean ball of radius r centered at
x. We omit some universal constants in the inequalities and use Á (if the constant depends on a
variable ) instead.

t

3 Main Results

In this section  we present our main theoretical results regarding the invertibility of a 2-layer convolu-
tional generative network with ReLUs. Our ﬁrst main theoretical contribution is as follows: although
the problem in (3) is non-convex  under appropriate conditions there is a strict descent direction
everywhere  except in the neighborhood of z˛ and that of a negative multiple of z˛.

4

f1 1z1z2x1x2=·f1 1f1 1f1 2f1 2f1 2f2 2f2 2f2 2f2 1f2 1f2 1D1D0lD1z′x′=·C1D1lf1 1f1 1f1 1f1 2f1 2f1 2f2 1f2 1f2 1f2 2f2 2f2 2C0C1lTheorem 1 (Invertibity of convolutional generative networks). Fix  ą 0. Let W1 P RC0D0ˆC1D1
and W2 P RC1D1ˆC2D2 be deconvolutional weight matrices with ﬁlters in R(cid:96) with i.i.d. entries from
Np0  1{Ci(cid:96)q for layers i “ 1  2 respectively. Let the sampling matrix A “ I be an identity matrix
(meaning there’s no sub-sampling). If C1(cid:96) Á C0 log C0 and C2(cid:96) Á C1 log C1 then with probability
at least 1 ´ κpD1C1 e´γC0 ` D2C2 e´γC1q we have the following. For all nonzero z and z˛  there
exists vz z˛ P Rn0 such that
Dvz z˛ Jpzq ă 0 
DzJp0q ă 0 

@z R Bpz˛  }z˛}2q Y Bp´ρz˛  }z˛}2q Y t0u
@z ‰ 0 

(4)
(5)

where ρ is a positive constant. Both γ ą 0 and κ ą 0 depend only on .
Theorem 1 establishes under some conditions that the landscape of the cost function is not adversarial.
Despite the heavily loaded notation  Theorem 1 simply requires that the weight matrices with Gaussian
ﬁlters should be sufﬁciently expansive (i.e.  output dimension of each layer should increase by at least
a logarithmic factor). Theorem 1 does not provide information regarding the neighborhood centered
at ´ρx˛  which implies the possible existence of a local minimum or a saddle point. However 
empirically we did not observe convergence to a point other than the ground truth. In other words 
gradient descent seems to always ﬁnd the global minimum  see Figure 4.
One assumption we make is the size of stride s being same as the ﬁlter size (cid:96). Although theoretically
convenient  this assumption is not common in the practical choices of transposed convolutional
networks. We believe a further analysis can remove this assumption  which we also leave as a future
work. In practice different activation functions other than ReLU can be used as well  such as sigmoid
function  Tanh and Leaky ReLU. It is also an interesting venue of research to see whether a similar
analysis can be done with those activations. In particular  for Leaky ReLU we brieﬂy explain how
the proof would divert from ours in Section Sup.2. We include landscapes of the cost function when
different activations are used in Figure 4.
Gaussian weight assumption might seem unrealistic at ﬁrst. However  there is some research [1]
indicating that weights of some trained networks follow a normal distribution. We also make a similar
observation on the networks we trained  see Section 4. We also note that Theorem 1 does not require
independence of network weights across layers.

Proof Outline: Due to space limitations  the complete proof of Theorem 1 is given in the supple-
mentary material [15]. Here we give a brief outline of the proof and highlight the main steps. The
theorem is proven by showing two main conditions on the weight matrices.
The ﬁrst condition is on the spatial arrangement of the network weights within each layer.
Lemma Sup.2 [15] provides a concentration bound on the distribution of the effective weight matrices
(after merging the ReLUs into the matrices). It shows that the set of neuron weights within each
layer are distributed approximately like Gaussian. A key idea for the proving Lemma Sup.2 is our
new permutation technique. Speciﬁcally  we rearrange both rows and columns of the sparse weight
matrices  as in Figure 2(a)  into a block diagonal matrix  as in Figure 2(b). Each block in the permuted
matrix is the same Gaussian matrix with independent entries. The permutation into block matrices
helps turns each block in Figure 2(b) into a dense Gaussian matrix  and therefore makes it possible to
utilize existing concentration bounds on Gaussian matrices.
The second condition is on the approximate angle contraction property of an effective weight matrix
Wi (after merging the ReLUs into the matrices). Lemma Sup.4 [15] shows that the angle between two
arbitrary input vectors x and y does not vanish under a transposed convolution layer and the ReLU.
The permutation poses a signiﬁcant challenge on the proof of Lemma Sup.4  since permutation of the
input vectors distorts the angles. The difﬁculty is handled carefully in the proof of Lemma Sup.4 
˝
which deviates from the proof machinery in [8] and hence is a major technical contribution.
Corollary 2 (One-to-one mapping). Under the assumptions of Theorem 1  the mapping Gp¨q :
Rn0 Ñ Rn2 pn0 ! n2q is injective (i.e.  one-to-one) with high probability.
Corollary 2 is a direct implication of Theorem 1. Corollary 2 states that the mapping from the latent
code space to the high-dimensional image space is one-to-one with high probability  when the
assumptions hold. This is interesting from a practical point of view  because mode collapse is a
well-known problem in training of GAN [20] and Corollary 2 provides a sufﬁcient condition to avoid
mode collapses. It remains to be further explored how we can make use of this insight in practice.

5

Conjecture 3. Under the assumptions of Theorem 1  let the network weights follow any zero-mean
  @t ą 0 instead of Gaussian. Then with high
subgaussian distribution Pp|x| ą tq ď ce´γt2
probability the same conclusion holds.

A subgaussian distribution (a.k.a. light-tailed distribution) is one whose tail decays at least as fast as a
Gaussian distribution (i.e.  exponential decay). This includes  for example  any bounded distribution
and the exponential distribution. Empirically  we observe that Theorem 1 holds for a number of zero-
mean subgaussian distributions  including uniform random weights and t`1 ´1u binary random
weights.
Now let us move on to the case where the subsampling matrix A is not an identity matrix. Instead 
consider a ﬁxed sampling rate r P p0  1s.
Theorem 4 (Invertibility under partial measurements). Under the assumptions of Theorem 1  let
A P RmˆC2D2 be an arbitrary subsampling matrix with m{pC2D2q ě r. Then with high probability
the same result as Theorem 1 hold.

Note that the subsampling rate r appears in the dimension of the weight matrix of the second layer.

Proof. Since ReLU operation is pointwise  we have the identity

y “ AGpzq “ AσpW2σpW1zqq “ σpAW2σpW1zqq.

It sufﬁces to show that Theorem 1 still holds with AW2 as the last weight matrix. Note that AW2
selects a row subset of the matrix W2 Figure 2(a). Consequently  after proper permutation  AW2 is
again a block diagonal matrix with each block being a Gaussian matrix with independent entries.
Only this time the blocks are not identical  but instead have different sizes. As a result  Theorem 1
still holds for AW2  since the proof of Theorem 1 does not require the identical blocks. However 
there are certain dimension constraints  which can be met by expanding the last layer with a factor of
r  the sampling rate. This modiﬁcation is reﬂected in the additional dimension assumption on the
weight matrix W2.

The minimal sampling rate r is a constant that depends on both the network architecture (e.g.  how
expansive the networks are) and the sampling matrix A. We made 2 empirical observations. Firstly 
spatially disperse sampling patterns (e.g.  uniform random samples) require a lower r  whilst more
aggressive sampling patterns (e.g.  top half  left half  sampling around image boundaries) demand
more measurements for perfect recovery. Secondly  regardless of the sampling patterns A  the
probability of perfect recovery exhibits a phase transition phenomenon w.r.t. the sampling rate r.
This observation supports Theorem 4 (i.e.  network is invertible given sufﬁcient measurements). A
more rigorous and mathematical characterization of r remains an open question.

4 Experimental Validation

In this section  we verify the gaussian weight assumption of trained generative networks  our main
result Theorem 4 on simulated 2-layer networks  as well as the generalization of Theorem 4 to more
complex multi-layer networks trained on real datasets.

4.1 Gaussian Weight in Trained Networks

We extract the convolutional ﬁlter weights  trained on real data to generate images in Figure 5  from
a 4-layer convolutional generative models. The histogram of the weights in each layer is depicted
in Figure 3. It can be observed that the trained weights highly resembles a zero-mean gaussian
distribution. We also discover similar distributions of weights in other trained convolutional networks 
such as ResNet [9]. Arora et al. [1] also report similar results.

4.2 On 2-layer Networks with Random Weights

As a sanity check on Theorem 4  we construct a generative neural network with 2 transposed
convolution layers  each followed by a ReLU. The ﬁrst layer has 16 channels and the second layer has
1 single channel. Both layers have a kernel size of 5 and a stride of 3. In order to be able to visualize

6

Figure 3: Distribution of the kernel weights from every layer in a trained convolutional generative
network. The trained weights roughly follow a zero-mean gaussian distribution.

the cost function landscape  we set the input latent space to be 2-dimensional. The weights of the
transposed convolution kernels are drawn i.i.d. from a Gaussian distribution with zero mean and unit
standard deviation. Only 50% of the network output is observed. We compute the cost function Jpzq
for every input latent code z on a grid centered at the ground truth. The landscape of the cost function
Jpzq is depicted in Figure 4(a). Although Theorem 4 implies a possibility of a stationary point at the
negative multiple of the ground truth  experimentally we do not observe convergence to any point
other than the global minimum.
Despite the fact that Theorem 1 and Theorem 4 are proved only for the case of 2-layer network
with ReLU  the same conclusion empirically extends to networks with more layers and different
kernel sizes and strides. In addition  the inversion of generative models generalizes to other standard
activation functions including Sigmoid  and Tanh. Speciﬁcally  Sigmoid and Tanh have quasi-convex
landscapes as shown in Figure 4(b) and (c)  which are even more favorable than that of ReLU. Leaky
ReLU has the same landscape as a regular ReLU.

(a) ReLU

(b) Sigmoid

(c) Tanh

(d) mode collapse

Figure 4: The landscape of the cost function Jpzq for deconvolutional networks with (a) ReLU  (b)
Sigmoid  and (c) Tanh as activation functions  respectively. There exists a unique global minimum.
As a counter example  we draw kernel weights uniformly randomly from r0  1s (which violates the
zero-mean Gaussian assumption). Consequently  there is a ﬂat global minimum in the latent space 
as shown in Figure 4(d). In this region  any two latent vectors are mapped to the exact same output 
indicating that mode collapse indeed occurs.

4.3 On Multi-layer Networks Trained with Real Data

In this section  we demonstrate empirically that our ﬁnding holds for multi-layer networks trained on
real data. The ﬁrst network is trained with GAN to generate handwritten digits  and the second for
celebrity faces. In both experiments  the correct latent codes can be recovered perfectly from partial
(but sufﬁciently many) observations.

MNIST: For the ﬁrst network on handwritten digit  we rescale the raw grayscale images from the
MNIST dataset [11] to size of 32 ˆ 32. We used the conditional deep convolutional generative adver-
sarial networks (DCGAN) framework [16  18] to train both a generative model and a discriminator.
Speciﬁcally  the generative network has 4 transposed convolutional layers. The ﬁrst 3 transposed
convolutional layers are followed by a batch normalization and a Leaky ReLU. The last layer is
followed by a Tanh. The discriminator has 4 convolutional layers  with the ﬁrst 3 followed by batch

7

0.20.10.00.1weight value0.02.55.07.510.012.515.017.5Probability densitylayer 1alayer 1blayer 2layer 3layer 4-0.500.5-1.5-11-0.501.54020-400-20-20020-4040-50505-510normalization and Leaky ReLU and the last one followed by a Sigmoid function. We use Adam with
learn rate 0.1 to optimize the latent code z . The optimization process usually converges within 500
iterations. The input noise to the generator is set to have a relatively small dimension 10 to ensure a
sufﬁciently expanding network.

Figure 5: We demonstrate recovery of latent codes on a generative network trained on the MNIST
dataset. From top to bottom: ground truth output images; partial measurements with different
sampling masks; reconstructed image using the recovered latent codes from partial measurements.
The recovery of latent codes in these examples is perfect  using simple gradient descent.

5 different sampling matrices are showcased in Figure 5  including observing uniform random
samples  as well as the top half  bottom half  left half  and right half of the image space. In all cases 
the input latent codes are recovery perfectly. We feed the recovered latent code as input to the network
to obtain the completed image  shown in the 3rd row.

Figure 6: recovery of latent codes on a generative network trained on the CelebA dataset. From
top to bottom: ground truth output images; partial measurements with different sampling masks;
reconstructed image using the recovered latent codes from partial measurements. The recovery of
latent codes in these examples is perfect  using simple gradient descent.

CelebFaces: A similar study is conducted on a generative network trained on the CelebFaces [12]
dataset. We rescale the raw grayscale images from the MNIST dataset [11] to size of 64 ˆ 64. A
similar network architecture to previous MNIST experiment is adopted  but both the generative model
and the discriminator have 4 layers rather than 3. The images are showcased in Figure 6.
Note that the probability of exact recovery increases with the number of measurements. The minimum
number of measurements required for exact recovery  however  depends on the network architecture 
the weights  and the sampling spatial patterns. The mathematical characterization for minimal number
of measurements remains a challenging open question.

5 Conclusion

In this work we prove rigorously that a 2-layer ReLU convolutional generative neural network is
invertible  even when only partial output is observed. This result provides a sufﬁcient condition
for the generator network to be one-to-one  which avoids the mode collapse problem in training of
GAN. We empirically demonstrate that the same conclusion holds even if the generative models
have other nonlinear activation functions (LeakyReLU  Sigmoid and Tanh) and multiple layers. The
same proof technique can be potentially generalized to multi-layer networks. Some interesting
future research directions include rigorous proofs for leaky ReLUs and other activation functions 
subgaussian network weights  as well as inversion under noisy measurements.

8

References
[1] Sanjeev Arora  Yingyu Liang  and Tengyu Ma. Why are deep nets reversible: A simple theory 

with implications for training. ICLR workshop.

[2] Ashish Bora  Ajil Jalal  Eric Price  and Alexandros G Dimakis. Compressed sensing using

generative models. arXiv preprint arXiv:1703.03208  2017.

[3] E. Candès  J. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate

measurements. Comm. Pure Appl. Math.  59(8):1207–1223  2006.

[4] David L. Donoho. For most large underdetermined systems of linear equations the minimal

l1-norm solution is also the sparsest solution. Comm. Pure Appl. Math  59:797–829  2004.

[5] Simon S. Du  Jason D. Lee  Yuandong Tian  Barnabas Poczos  and Aarti Singh. Gradient
descent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. arXiv preprint
arXiv:1712.00779  2017.

[6] Anna C Gilbert  Yi Zhang  Kibok Lee  Yuting Zhang  and Honglak Lee. Towards understanding

the invertibility of convolutional neural networks. arXiv preprint arXiv:1705.08664  2017.

[7] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[8] Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by

empirical risk. arXiv preprint arXiv:1705.07576  2017.

[9] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[10] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

[11] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[12] Ziwei Liu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Deep learning face attributes in the

wild. In Proceedings of International Conference on Computer Vision (ICCV)  2015.

[13] Fangchang Ma and Sertac Karaman. Sparse-to-dense: Depth prediction from sparse depth

samples and a single image. arXiv preprint arXiv:1709.07492  2017.

[14] Fangchang Ma  Luca Carlone  Ulas Ayaz  and Sertac Karaman. Sparse depth sensing for

resource-constrained robots. arXiv preprint arXiv:1703.01398  2017.

[15] Fangchang Ma  Ulas Ayaz  and Sertac Karaman. Supplementary materials - invertibility of

convolutional generative networks from partial measurements. NIPS  2018.

[16] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint

arXiv:1411.1784  2014.

[17] Deepak Pathak  Philipp Krahenbuhl  Jeff Donahue  Trevor Darrell  and Alexei A Efros. Context
encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 2536–2544  2016.

[18] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[19] Mahdi Soltanolkotabi. Learning relus via gradient descent.

In I. Guyon  U. V. Luxburg 
S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in Neural
Information Processing Systems 30  pages 2004–2014. Curran Associates  Inc.  2017.

9

[20] Akash Srivastava  Lazar Valkoz  Chris Russell  Michael U Gutmann  and Charles Sutton.
Veegan: Reducing mode collapse in gans using implicit variational learning. In Advances in
Neural Information Processing Systems  pages 3310–3320  2017.

[21] Raymond A Yeh  Chen Chen  Teck Yian Lim  Alexander G Schwing  Mark Hasegawa-Johnson 
and Minh N Do. Semantic image inpainting with deep generative models. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition  pages 5485–5493  2017.

[22] Jun-Yan Zhu  Philipp Krähenbühl  Eli Shechtman  and Alexei A. Efros. Generative visual
In Proceedings of European Conference on

manipulation on the natural image manifold.
Computer Vision (ECCV)  2016.

10

,Fangchang Ma
Ulas Ayaz
Sertac Karaman