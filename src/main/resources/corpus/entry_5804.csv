2019,Unsupervised learning of object structure and dynamics from videos,Extracting and predicting object structure and dynamics from videos without supervision is a major challenge in machine learning. To address this challenge  we adopt a keypoint-based image representation and learn a stochastic dynamics model of the keypoints. Future frames are reconstructed from the keypoints and a reference frame. By modeling dynamics in the keypoint coordinate space  we achieve stable learning and avoid compounding of errors in pixel space. Our method improves upon unstructured representations both for pixel-level video prediction and for downstream tasks requiring object-level understanding of motion dynamics. We evaluate our model on diverse datasets: a multi-agent sports dataset  the Human3.6M dataset  and datasets based on continuous control tasks from the DeepMind Control Suite. The spatially structured representation outperforms unstructured representations on a range of motion-related tasks such as object tracking  action recognition and reward prediction.,Unsupervised Learning of Object Structure and

Dynamics from Videos

Matthias Minderer∗ Chen Sun Ruben Villegas
Kevin Murphy Honglak Lee

Forrester Cole

{mjlm  chensun  rubville  fcole  kpmurphy  honglak}@google.com

Google Research

Abstract

Extracting and predicting object structure and dynamics from videos without
supervision is a major challenge in machine learning. To address this challenge 
we adopt a keypoint-based image representation and learn a stochastic dynamics
model of the keypoints. Future frames are reconstructed from the keypoints and
a reference frame. By modeling dynamics in the keypoint coordinate space  we
achieve stable learning and avoid compounding of errors in pixel space. Our
method improves upon unstructured representations both for pixel-level video
prediction and for downstream tasks requiring object-level understanding of motion
dynamics. We evaluate our model on diverse datasets: a multi-agent sports dataset 
the Human3.6M dataset  and datasets based on continuous control tasks from
the DeepMind Control Suite. The spatially structured representation outperforms
unstructured representations on a range of motion-related tasks such as object
tracking  action recognition and reward prediction.

1

Introduction

Videos provide rich visual information to understand the dynamics of the world. However  extracting
a useful representation from videos (e.g. detection and tracking of objects) remains challenging and
typically requires expensive human annotations. In this work  we focus on unsupervised learning of
object structure and dynamics from videos.
One approach for unsupervised video understanding is to learn to predict future frames [17  16  9  15 
24  30  8  3  14]. Based on this body of work  we identify two main challenges: First  it is hard to
make pixel-level predictions because motion in videos becomes highly stochastic for horizons beyond
about a second. Since semantically insigniﬁcant deviations can lead to large error in pixel space  it is
often difﬁcult to distinguish good from bad predictions based on pixel losses. Second  even if good
pixel-level prediction is achieved  this is rarely the desired ﬁnal task. The representations of a model
trained for pixel-level reconstruction are not guaranteed to be useful for downstream tasks such as
tracking  motion prediction and control.
Here  we address both of these challenges by using an explicit  interpretable keypoint-based represen-
tation of object structure as the core of our model. Keypoints are a natural representation of dynamic
objects  commonly used for face and pose tracking. Training keypoint detectors  however  generally
requires supervision. We learn the keypoint-based representation directly from video  without any
supervision beyond the pixel data  in two steps: ﬁrst encode individual frames to keypoints  then
model the dynamics of those points. As a result  the representation of the dynamics model is spatially
structured  though the model is trained only with a pixel reconstruction loss. We show that enforcing
spatial structure signiﬁcantly improves video prediction quality and performance for tasks such as
action recognition and reward prediction.

∗Google AI Resident

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

By decoupling pixel generation from dynamics prediction  we avoid compounding errors in pixel
space because we never condition on predicted pixels. This approach has been shown to be beneﬁcial
for supervised video prediction [25]. Furthermore  modeling dynamics in keypoint coordinate
space allows us to sample and evaluate predictions efﬁciently. Errors in coordinate space are
more meaningful than in pixel space  since distance between keypoints is more closely related to
semantically relevant differences than pixel-space distance. We exploit this by using a best-of-many-
samples objective [4] during training to achieve stochastic predictions that are both highly diverse
and of high quality  outperforming the predictions of models lacking spatial structure.
Finally  because we build spatial structure into our model a priori  its internal representation is biased
to contain object-level information that is useful for downstream applications. This bias leads to
better results on tasks such as trajectory prediction  action recognition and reward prediction.
Our contributions are: (1) a novel architecture and optimization techniques for unsupervised video
prediction with a structured internal representation; (2) a model that outperforms recent work [8 
28] and our unstructured baseline in pixel-level video prediction; (3) improved performance vs.
unstructured models on downstream tasks requiring object-level understanding.

2 Related work

Unsupervised learning of keypoints. Previous work explores learning to ﬁnd keypoints in an image
by applying an autoencoding architecture with keypoint-coordinates as a representational bottleneck
[12  33]. The bottleneck forces the image to be encoded in a small number of points. We build on
these methods by extending them to the video setting.
Stochastic sequence prediction. Successful video prediction requires modeling uncertainty. We
adopt the VRNN [6] architecture  which adds latent random variables to the standard RNN archi-
tecture  to sample from possible futures. More sophisticated approaches to stochastic prediction of
keypoints have been recently explored [31  21]  but we ﬁnd the basic VRNN architecture sufﬁcient
for our applications.
Unsupervised video prediction. A large body of work explores learning to predict video frames
using only a pixel-reconstruction loss [18  20  17  9  24  7]. Most similar to our work are approaches
that perform deterministic image generation from a latent sample produced by stochastic sampling
from a prior conditioned on previous timesteps [8  3  14]. Our approach replaces the unstructured
image representation with a structured set of keypoints  improving performance on video prediction
and downstream tasks compared with SVG [8] (Section 5).
Recent methods also apply adversarial training to improve prediction quality and diversity of sam-
ples [22  14]. EPVA [28] predicts dynamics in a high-level feature space and applies an adversarial
loss to the predicted features. We compare against EPVA and show improvement without adversarial
training  but adversarial training is compatible with our method and is a promising future direction.
Video prediction with spatially structured representations. Like our approach  several recent
methods explore explicit  spatially structured representations for video prediction. Xu et al. [29]
proposed to discover object parts and structure by watching how they move in videos. Vid2Vid [27]
proposed a video-to-video translation network from segmentation masks  edge masks and human
pose. The method is also used for predicting a few frames into the future by predicting the structure
representations ﬁrst. Villegas et al. [25] proposed to train a human pose predictor and then use the
predicted pose to generate future frames of human motion. In [26]  a method is proposed where
future human pose is predicted using a stochastic network and the pose is then used to generate future
frames. Recent methods on video generation have used spatially structured representations for video
motion transfer between humans [1  5]. In contrast  our model is able to ﬁnd spatially structured
representation without supervision while using video frames as the only learning signal.

3 Architecture

Our model is composed of two parts: a keypoint detector that encodes each frame into a low-
dimensional  keypoint-based representation  and a dynamics model that predicts dynamics in the
keypoint space (Figure 1).

2

Figure 1: Architecture of our model. Variables are black  functions blue  losses red. Some arrows are
omitted for clarity  see Equations 1 to 4 for details.

3.1 Unsupervised keypoint detector

The keypoint detection architecture is inspired by [12]  which we adapt for the video setting. Let
v1:T ∈ RH×W×C be a video sequence of length T . Our goal is to learn a keypoint detector
ϕdet(vt) = xt that captures the spatial structure of the objects in each frame in a set of keypoints xt.
The detector ϕdet is a convolutional neural network that produces K feature maps  one for each
keypoint. Each feature map is normalized and condensed into a single (x  y)-coordinate by computing
the spatial expectation of the map. The number of heatmaps K is a hyperparameter that represents
the maximum expected number of keypoints necessary to model the data.
For image reconstruction  we learn a generator ϕrec that reconstructs frame vt from its keypoint
representation. The generator also receives the ﬁrst frame of the sequence v1 to capture the static
appearance of the scene: vt = ϕrec(v1  xt). Together  the keypoint detector ϕdet and generator ϕrec
form an autoencoder architecture with a representational bottleneck that forces the structure of each
frame to be encoded in a keypoint representation [12].
The generator is also a convolutional neural network. To supply the keypoints to the network  each
point is converted into a heatmap with a Gaussian-shaped blob at the keypoint location. The K
heatmaps are concatenated with feature maps from the ﬁrst frame v1. We also concatenate the
keypoint-heatmaps for the ﬁrst frame v1 to the decoder input for subsequent frames vt  to help the
decoder to "inpaint" background regions that were occluded in the ﬁrst frame. The resulting tensor
forms the input to the generator. We add skip connections from the ﬁrst frame of the sequence to the
generator output such that the actual task of the generator is to predict vt − v1.
We use the mean intensity µk of each keypoint feature map returned by the detector as a continuous-
valued indicator of the presence of the modeled object. When converting keypoints back into
heatmaps  each map is scaled by the corresponding µk. The model can use µk to encode the presence
or absence of individual objects on a frame-by-frame basis.

3.2 Stochastic dynamics model

To model the dynamics in the video  we use a variational recurrent neural network (VRNN) [6]. The
core of the dynamics model is a latent belief z over keypoint locations x. In the VRNN architecture 
the prior belief is conditioned on all previous timesteps through the hidden state ht−1 of an RNN 
and thus represents a prediction of the current keypoint locations before observing the image:

(1)
We obtain the posterior belief by combining the previous hidden state with the unsupervised keypoint
coordinates xt = ϕdet(vt) detected in the current frame:

p(zt|x<t  z<t) = ϕprior(ht−1)

q(zt|x≤t  z<t) = ϕenc(ht−1  xt)

Predictions are made by decoding the latent belief:

p(xt|z≤t  x<t) = ϕdec(zt  ht−1)

3

(2)

(3)

Stop-gradientFinally  the RNN is updated to pass information forward in time:

ht = ϕRNN(xt  zt  ht−1).

(4)
Note that to compute the posterior (Eq. 2)  we obtain xt from the keypoint detector  but for the
recurrence in Eq. 4  we obtain xt by decoding the latent belief. We can therefore predict into
the future without observing images by decoding xt from the prior belief. Because the model
has both deterministic and stochastic pathways across time  predictions can account for long-term
dependencies as well as future uncertainty [10  6].

4 Training

4.1 Keypoint detector

The keypoint detector is trained with a simple L2 image reconstruction loss Limage =(cid:80)

t ||v − ˆv||2
2 
where v is the true and ˆv is the reconstructed image. Errors from the dynamics model are not
backpropagated into the keypoint detector.2
Ideally  the representation should use as few keypoints as possible to encode each object. To encourage
such parsimony  we add two additional losses to the keypoint detector:
Temporal separation loss. Image features whose motion is highly correlated are likely to belong to
the same object and should ideally be represented jointly by a single keypoint. We therefore add a
separation loss that encourages keypoint trajectories to be decorrelated in time. The loss penalizes
"overlap" between trajectories within a Gaussian radius σsep:
exp(− dkk(cid:48)
2σ2
sep

(cid:88)

(cid:88)

Lsep =

(5)

)

(cid:80)
t ||(xt k − (cid:104)xk(cid:105)) − (xt k(cid:48) − (cid:104)xk(cid:48)(cid:105))||2

k(cid:48)

k

2 is the distance between the trajectories
where dkk(cid:48) = 1
of keypoints k and k(cid:48)  computed after subtracting the temporal mean (cid:104)x(cid:105) from each trajectory.
T
|| · ||2
k |µk| on the keypoint

Keypoint sparsity loss. For similar reasons  we add an L1 penalty Lsparse =(cid:80)

2 denotes the squared Euclidean norm.

scales µ to encourage keypoints to be sparsely active.
In Section 5.3  we show that both Lsep and Lsparse contribute to stable keypoint detection.

4.2 Dynamics model

The standard VRNN [6] is trained to encode the detected keypoints by maximizing the evidence lower
bound (ELBO)  which is composed of a reconstruction loss and a KL term between the Gaussian
prior N prior

= N (zt|ϕprior(ht−1)) and posterior distribution N enc

t = N (zt|ϕenc(ht−1  xt)):

t

(cid:105)

LVRNN = − T(cid:88)

E(cid:104)

log p(xt|z≤t  x<t) − βKL(N enc

t

(cid:107) N prior

t

)

(6)

t=1

The KL term regularizes the latent representation. In the VRNN architecture  it is also responsible for
training the RNN  since it encourages the prior to predict the posterior based on past information. To
balance reliance on predictions with ﬁdelity to observations  we add the hyperparameter β (see also
[2]). We found it essential to tune β for each dataset to achieve a balance between reconstruction
quality (lower β) and prediction diversity.
The KL term only trains the dynamics model for single-step predictions because the model receives
observations after each step [10]. To encourage learning of long-term dependencies  we add a pure
reconstruction loss  without the KL term  for multiple future timesteps:
E [log p(xt|z≤t  x≤T )]

Lfuture = − T +∆T(cid:88)

(7)

2We found this to be necessary to maintain a keypoint-structured representation. If the image model is
trained based on errors from the dynamics model  the image model may adopt the poorly structured code of an
incompletely trained dynamics model  rather than the dynamics model adopting the keypoint-structured code.

t=T +1

4

(a) Basketball

(b) Human3.6M

Figure 2: Main datasets used in our experiments. First row: Ground truth images. Second row:
Decoded coordinates (black dots; ˆxt in Figure 1) and past trajectories (gray lines). Third row:
Reconstructed image. Green borders indicate observed frames  red indicate predicted frames.
The standard approach to estimate log p(xt|z≤t  x≤t) in Eq. 6 and 7 is to sample a single zt. To
further encourage diverse predictions  we instead use the best of a number of samples [4] at each
timestep during training:

(cid:0) log p(xt|zi t  z<t  x<t)(cid:1) 

max

i

(8)

t

t

for observed steps and zi t ∼ N prior

where zi t ∼ N enc
for predicted steps. By giving the model
several chances to make a good prediction  it is encouraged to cover a range of likely data modes 
rather than just the most likely. Sampling and evaluating several predictions at each timestep would
be expensive in pixel space. However  since we learn the dynamics in the low-dimensional keypoint
space  we can evaluate sampled predictions without reconstructing pixels. Due to the keypoint
structure  the L2 distance of samples from the observed keypoints meaningfully captures sample
quality. This would not be guaranteed for an unstructured latent representation. As shown in Section 5 
the best-of-many objective is crucial to the performance of our model.
The combined loss of the whole model is:

Limage + λsepLsep + λsparseLsparse + LVRNN + Lfuture 

(9)
where λsep and λsparse are scale parameters for the keypoint separation and sparsity losses. See Sec-
tion S1 for implementation details  including a list of hyperparameters and tuning ranges (Table S1).

5 Results

We ﬁrst show that the structured representation of our model improves prediction quality on two
video datasets  and then show that it is more useful than unstructured representations for downstream
tasks that require object-level information.

5.1 Structured representation improves video prediction

We evaluate frame prediction on two video datasets (Figure 2). The Basketball dataset consists of a
synthetic top-down view of a basketball court containing ﬁve offensive players and the ball  all drawn
as colored dots. The videos are generated from real basketball player trajectories [32]  testing the
ability of our model to detect and stably represent multiple objects with complex dynamics. The
dataset contains 107 146 training and 13 845 test sequences. The Human3.6 dataset [11] contains
video sequences of human actors performing various actions. We use subjects S1  S5  S6  S7  and
S9 for training (600 videos)  and subjects S9 and S11 for evaluation (239 videos). For both datasets 
ground truth object coordinates are available for evaluation  but are not used by the model. The
Basketball dataset contains the coordinates of each of the 5 players and the ball. The Human dataset
contains 32 motion capture points  of which we select 12 for evaluation.
We compare the full model (Struct-VRNN) to a series of baselines and ablations: the Struct-VRNN
(no BoM) model was trained without the best-of-many objective; the Struct-RNN is deterministic;
the CNN-VRNN architecture uses the same stochastic dynamics model as the Struct-VRNN  but uses
an unstructured deep feature vector as its internal representation instead of structured keypoints. All
structured models use K = 12 for Basketball  and K = 48 for Human3.6M  and were conditioned on

5

Truet=0t=5t=10t=15t=20t=24Coord.Recon.Truet=0t=5t=10t=20t=30t=40Coord.Recon.Figure 3: Video generation quality on Human3.6M. Our stochastic structured model (Struct-VRNN)
outperforms our deterministic baseline (Struct-RNN)  our unstructured baseline (CNN-VRNN) 
and the SVG [8] and SAVP [14] models. Top: Example observed (green borders) and predicted
(red borders) frames (best viewed as video: https://mjlm.github.io/video_structure/).
Example sequences are the closest or furthest samples from ground truth according to VGG cosine
similarity  as indicated. Note that for Struct-VRNN  even the samples furthest from ground truth are
of high visual quality. Bottom left: Mean VGG cosine similarity of the the samples closest to ground
truth (left) and furthest from ground truth (right). Higher is better. Plots show mean performance
across 5 model initializations  with the 95% conﬁdence interval shaded. Bottom right: Fréchet
Video Distance [23]  using all samples. Lower is better. Dots represents separate model initializations.
EPVA [28] is not stochastic  so we compare performance with a single sample from our method on
their test set.

8 frames and trained to predict 8 future frames. For the CNN-VRNN  which lacks keypoint structure 
we use a latent representation with 3K elements  such that its capacity is at least as large as that of
the Struct-VRNN representation. Finally  we compare to three published models: SVG [8]  SAVP
[14] and EPVA [28] (Figure 3).
The Struct-VRNN model matches or outperforms the other models in perceptual image and video
quality as measured by VGG [19] feature cosine similarity and Fréchet Video Distance [23] (Figure 3).
Results for the lower-level metrics SSIM and PSNR are similar (see supplemental material).
The ablations suggest that the structured representation  the stochastic belief  and the best-of-many
objective all contribute to model performance. The full Struct-VRNN model generates the best
reconstructions of ground truth  and also generates the most diverse samples (i.e.  samples that are
furthest from ground truth; Figure 3 bottom left). In contrast  the ablated models and SVG show both
lower best-case accuracy and smaller differences between closest and furthest samples  indicating
less diverse samples. SAVP is closer  performing well on the single-frame metric (VGG cosine
sim.)  but still worse on FVD than the structured model. Qualitatively  Struct-VRNN exhibits sharper
images and longer object permanence than the unstructured models (Figure 3  top; note limb detail
and dynamics). This is true even for the samples that are far from ground truth (Figure 3 top  row
"Struct-VRNN (furthest)")  which suggests that our model produces diverse high-quality samples 
rather than just a few good samples among many diverse but unrealistic ones. This conclusion is

6

Truet=0t=3t=5t=10t=15t=20t=25t=30t=35t=40t=45t=50Struct-VRNN(closest)Struct-VRNN(furthest)CNN-VRNN(closest)SVG(closest)SAVP(closest)1020304050Timestep0.80.91.0VGG cosine sim.Closest sampleStruct-VRNNSVRNN (no BoM)Struct-RNNCNN-VRNNSVGSAVP1020304050TimestepFurthest sampleStruct-VRNNSVRNN (no BoM)Struct-RNNCNN-VRNNSVGSAVP20040060080010001200FVDOur test setStruct-VRNNEPVAEPVA-GANEPVA test setFigure 4: Prediction error for ground-truth trajectories by linear regression from predicted keypoints.
(sup.) indicates supervised baseline.

Figure 5: Ablating either the temporal separation loss or the keypoint sparsity loss reduces model
performance and stability. In the FVD plots  each dot corresponds to a different model initialization.
Coordinate error plots show the prediction error when regressing the ground-truth object coordinates
on the discovered keypoints. Lines show the mean of ﬁve model initializations  with the 95%
conﬁdence intervals shaded.

backed up by the FVD (Figure 3 bottom right)  which measures the overall quality of a distribution
of videos [23].

5.2 The learned keypoints track objects

We now examine how well the learned keypoints track the location of objects. Since we do not expect
the keypoints to align exactly with human-labeled objects  we ﬁt a linear regression from the keypoints
to the ground truth object positions and measure trajectory prediction error on held-out sequences
(Figure 4). The trajectory error is the average distance between true and predicted coordinates at each
timestep. To account for stochasticity  we sample 50 predictions and report the error of the best.3
As a baseline  we train Struct-VRNN and CNN-VRNN models with additional supervision that forces
the learned keypoints to match the ground-truth keypoints. The keypoints learned by the unsupervised
Struct-VRNN model are nearly as predictive as those trained with supervision  indicating that the
learned keypoints represent useful spatial information. In contrast  prediction from the internal
representation of the unsupervised CNN-VRNN is poor. When trained with supervision  however  the
CNN-VRNN reaches similar performance as the supervised Struct-VRNN. In other words  both the
Struct-VRNN and the CNN-VRNN can learn a spatial internal representation  but the Struct-VRNN
learns it without supervision.
As expected  the less diverse predictions of the Struct-VRNN (no BoM) and Struct-RNN perform
worse on the coordinate regression task. Finally  for comparison  we remove the dynamics model
entirely and simply predict the last observed keypoint locations for all future timepoints. All models
except unsupervised CNN-VRNN outperform this baseline.

5.3 Simple inductive biases improve object tracking

In Section 4.1  we described losses intended to add inductive biases such as keypoint sparsity and
uncorrelated object trajectories to the keypoint detector. We ﬁnd that these losses improve object
tracking performance and stability. Figure 5 shows that models without Lsep and Lsparse show reduced
video prediction and tracking performance. The increased variability between model initializations
without Lsep and Lsparse suggests that these losses improve the learnability of a stable keypoint
3For Human3.6M  we choose the best sample based on the average error of all coordinates. For Basketball 

we choose the best sample separately for each player.

7

No dynamicsSVRNN (no BoM)Struct-VRNN (sup.)Figure 6: Unsupervised keypoints allow human-guided exploration of object dynamics. We manipu-
lated the observed coordinates for Player 1 (black arrow) to change the original (blue) trajectory. The
other players were not manipulated. The dynamics were then rolled out into the future to predict how
the players will behave in the manipulated (red) scenario. Black crosses mark initial player positions.
Light-colored parts of the trajectories are observed  dark-colored parts are predicted. Dots indicate
ﬁnal position. Lines of the same color indicate different samples conditioned on the same observed
coordinates.

structure (also see Figure S6). In summary  we ﬁnd that training and ﬁnal performance is most stable
if K is chosen to be larger than the expected number of objects  such that the model can use µ in
combination with Lsparse and Lsep to activate the optimal number of keypoints.

5.4 Manipulation of keypoints allows interaction with the model

Since the learned keypoints track objects  the
model’s predictions can be intuitively manipu-
lated by directly adjusting the keypoints.
On the Basketball dataset  we can explore coun-
terfactual scenarios such as predicting how the
other players react if one player moves left as
opposed to right (Figure 6). We simply manip-
ulate the sequence observed keypoint locations
before they are passed to the RNN  thus condi-
tioning the RNN states and predictions on the
manipulated observations.
For the Human3.6M dataset  we can indepen-
dently manipulate body parts and generate poses
that are not present in the training set (Fig-
ure 7; please see https://mjlm.github.io/
video_structure/for videos). The model
learns to associate keypoints with local areas of the body  such that moving keypoints near an
arm moves the arm without changing the rest of the image.

Figure 7: Keypoints learned by our method may be
manipulated to change peoples’ poses. Note that
both manipulations and effects are spatially local.
Best viewed in video (https://mjlm.github.
io/video_structure/).

5.5 Structured representation retains more semantic information

The learned keypoints are also useful for downstream
tasks such as action recognition and reward prediction in
reinforcement learning.
To test action recognition performance  we train a sim-
ple 3-layer RNN to classify Human3.6M actions from a
sequence of keypoints (see Section S2.2 for model details).
The keypoints learned by the structured models perform
better than the unstructured features learned by the CNN-
VRNN (Figure 8). Future prediction is not needed  so the
RNN and VRNN models perform similarly.
One major application we anticipate for our model is plan-
ning and reinforcement learning of spatially deﬁned tasks.
As a ﬁrst step  we trained our model on a dataset collected

8

Figure 8: Action recognition on the
Human3.6M dataset. Solid line: null
model (predict the most frequent action).
Dashed line: prediction from ground-
truth coordinates. Sup.  supervised.

ManipulationManipulationOriginalLeft legRight legLeft armRight armS9 exampleS11 exampleStruct-VRNN (sup.)Struct-VRNNStruct-RNNCNN-VRNN (sup.)CNN-VRNN0.000.250.50Action recognitionaccuracyFigure 9: Predicting rewards on the DeepMind Control Suite continuous control domains. We chose
domains with dense rewards to ensure the random policy would provide a sufﬁcient reward signal
for this analysis. To make scales comparable across domains  errors are normalized to a null model
which predicts the mean training-set-reward at all timesteps. Lines show the mean across test-set
examples and 5 random model initializations  with the 95% conﬁdence interval shaded.

from six tasks in the DeepMind Control Suite (DMCS) 
a set of simulated continuous control environments (Figure 9). Image observations and rewards
were collected from the DMCS environments using random actions  and we modiﬁed our model
to condition predictions on the agent’s actions by feeding the actions as an additional input to the
RNN. Models were trained without access to the task reward function. We used the latent state of the
dynamics model as an input to a separate reward prediction model for each task (see Section S2.3 for
details). The dynamics learned by the Struct-VRNN give better reward prediction performance than
the unstructured CNN-VRNN baseline  suggesting our architecture may be a useful addition to plan-
ning and reinforcement learning models. Concurrent work that applies a similar keypoint-structured
model to control tasks conﬁrms these results [13].

6 Discussion

A major question in machine learning is to what degree prior knowledge should be built into a
model  as opposed to learning it from the data. This question is especially important for unsupervised
vision models trained on raw pixels  which are typically far removed from the information that is
of interest for downstream tasks. We propose a model with a spatial inductive bias  resulting in a
structured  keypoint-based internal representation. We show that this structure leads to superior results
on downstream tasks compared to a representation derived from a CNN without a keypoint-based
representational bottleneck.
The proposed spatial prior using keypoints represents a middle ground between unstructured repre-
sentations and an explicitly object-centric approach. For example  we do not explicitly model object
masks  occlusions  or depth. Our architecture either leaves these phenomena unmodeled  or learns
them from the data. By choosing to not build this kind of structure into the architecture  we keep our
model simple and achieve stable training (see variability across initializations in Figures 3  4  and 5)
on diverse datasets  including multiple objects and complex  articulated human shapes.
We also note the importance of stochasticity for the prediction of videos and object trajectories. In
natural videos  any sequence of conditioning frames is consistent with an astronomical number of
plausible future frames. We found that methods that increase sample diversity (e.g. the best-of-many
objective [4]) led to large gains in FVD  which measures the similarity of real and predicted videos
on the level of distributions over entire videos. Conversely  due to the diversity of plausible futures 
frame-wise measures of similarity to ground truth (e.g. VGG cosine similarity  PSNR  and SSIM) are
near-meaningless for measuring long-term video prediction quality.
Beyond image-based measures  the most meaningful evaluation of a predictive model is to apply it to
downstream tasks of interest  such as planning and reinforcement learning for control tasks. Because
of its simplicity  our architecture is straightforward to combine with existing architectures for tasks
that may beneﬁt from spatial structure. Applying our model to such tasks is an important future
direction of this work.

9

References
[1] K. Aberman  R. Wu  D. Lischinski  B. Chen  and D. Cohen-Or. Learning Character-Agnostic

Motion for Motion Retargeting in 2D. In SIGGRAPH  2019.

[2] A. A. Alemi  B. Poole  I. Fischer  J. V. Dillon  R. A. Saurous  and K. Murphy. Fixing a Broken

ELBO. In ICML  2018.

[3] M. Babaeizadeh  C. Finn  R. Erhan  Dumitru an Campbell  and S. Levine. Stochastic variational

video prediction. In ICLR  2018.

[4] A. Bhattacharyya  B. Schiele  and M. Fritz. Accurate and Diverse Sampling of Sequences based

on a "Best of Many" Sample Objective. In CVPR  2018.

[5] C. Chan  S. Ginosar  T. Zhou  and A. A. Efros. Everybody Dance Now. In CoRR  volume

abs/1808.07371  2018.

[6] J. Chung  K. Kastner  L. Dinh  K. Goel  A. Courville  and Y. Bengio. A Recurrent Latent

Variable Model for Sequential Data. In NeurIPS  2015.

[7] E. Denton and V. Birodkar. Unsupervised Learning of Disentangled Representations from

Video. In NeurIPS  2017.

[8] E. Denton and R. Fergus. Stochastic Video Generation with a Learned Prior. In ICML  2018.

[9] C. Finn  I. Goodfellow  and S. Levine. Unsupervised learning for physical interaction through

video prediction. In NIPS  2016.

[10] D. Hafner  T. Lillicrap  I. Fischer  R. Villegas  D. Ha  H. Lee  and J. Davidson. Learning Latent

Dynamics for Planning from Pixels. In ICML  2019.

[11] C. Ionescu  D. Papava  V. Olaru  and C. Sminchisescu. Human3.6m: Large scale datasets and

predictive methods for 3d human sensing in natural environments. In PAMI  2014.

[12] T. Jakab  A. Gupta  H. Bilen  and A. Vedaldi. Conditional Image Generation for Learning the

Structure of Visual Objects. In NeurIPS  2018.

[13] T. Kulkarni  A. Gupta  C. Ionescu  S. Borgeaud  M. Reynolds  A. Zisserman  and V. Mnih.
Unsupervised learning of object keypoints for perception and control. In arXiv: 1906.11883 
2019.

[14] A. X. Lee  R. Zhang  F. Ebert  P. Abbeel  C. Finn  and S. Levine. Stochastic Adversarial Video

Prediction. In CoRR  volume abs/1804.01523  2018.

[15] W. Lotter  G. Kreiman  and D. Cox. Deep predictive coding networks for video prediction and

unsupervised learning. In ICLR  2017.

[16] M. Mathieu  C. Couprie  and Y. LeCun. Deep multi-scale video prediction beyond mean square

error. In ICLR  2016.

[17] J. Oh  X. Guo  H. Lee  R. L. Lewis  and S. Singh. Action-conditional video prediction using

deep networks in atari games. In NeurIPS  2015.

[18] M. Ranzato  A. Szlam  J. Bruna  M. Mathieu  R. Collobert  and S. Chopra. Video (language)
modeling: a baseline for generative models of natural videos. arXiv preprint:1412.6604  2014.

[19] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image

recognition. In CoRR  2014.

[20] N. Srivastava  E. Mansimov  and R. Salakhudinov. Unsupervised Learning of Video Represen-

tations using LSTMs. In ICML  2015.

[21] C. Sun  P. Karlsson  J. Wu  J. B. Tenenbaum  and K. Murphy. Stochastic Prediction of Multi-

Agent Interactions From Partial Observations. In ICLR  2019.

10

[22] S. Tulyakov  M.-Y. Liu  X. Yang  and J. Kautz. Mocogan: Decomposing motion and content for

video generation. In CVPR  2018.

[23] T. Unterthiner  S. van Steenkiste  K. Kurach  R. Marinier  M. Michalski  and S. Gelly. Towards

Accurate Generative Models of Video: A New Metric & Challenges. In CoRR  2018.

[24] R. Villegas  J. Yang  S. Hong  X. Lin  and H. Lee. Decomposing Motion and Content for

Natural Video Sequence Prediction. In ICLR  2017.

[25] R. Villegas  J. Yang  Y. Zou  S. Sohn  X. Lin  and H. Lee. Learning to Generate Long-term

Future via Hierarchical Prediction. In ICML  2017.

[26] J. Walker  K. Marino  A. Gupta  and M. Hebert. The Pose Knows: Video Forecasting by

Generating Pose Futures. In NeurIPS  2018.

[27] T.-C. Wang  M.-Y. Liu  J.-Y. Zhu  G. Liu  A. Tao  J. Kautz  and B. Catanzaro. Video-to-Video

Synthesis. In NeurIPS  2018.

[28] N. Wichers  R. Villegas  D. Erhan  and H. Lee. Hierarchical Long-term Video Prediction

without Supervision. In ICML  2018.

[29] Z. Xu  Z. Liu  C. Sun  K. Murphy  W. T. Freeman  J. B. Tenenbaum  and J. Wu. Unsupervised

Discovery of Parts  Structure  and Dynamics. In ICLR  2019.

[30] T. Xue  J. Wu  K. Bouman  and B. Freeman. Visual dynamics: Probabilistic future frame

synthesis via cross convolutional networks. In NeurIPS  2016.

[31] X. Yan  A. Rastogi  R. Villegas  K. Sunkavalli  E. Shechtman  S. Hadap  E. Yumer  and H. Lee.
Mt-vae: Learning motion transformations to generate multimodal human dynamics. In ECCV 
2018.

[32] E. Zhan  S. Zheng  Y. Yue  L. Sha  and P. Lucey. Generating Multi-Agent Trajectories using

Programmatic Weak Supervision. In ICLR  2019.

[33] Y. Zhang  Y. Guo  Y. Jin  Y. Luo  Z. He  and H. Lee. Unsupervised Discovery of Object

Landmarks as Structural Representations. In CVPR  2018.

11

,Matthias Minderer
Chen Sun
Ruben Villegas
Forrester Cole
Kevin Murphy
Honglak Lee