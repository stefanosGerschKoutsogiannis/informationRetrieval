2015,Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems,This paper is concerned with finding a solution x to a quadratic system of equations y_i = |< a_i  x >|^2   i = 1  2  ...  m. We prove that it is possible to solve unstructured quadratic systems in n variables exactly from O(n) equations in linear time  that is  in time proportional to reading and evaluating the data. This is accomplished by a novel procedure  which starting from an initial guess given by a spectral initialization procedure  attempts to minimize a non-convex objective. The proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion  which discard terms bearing too much influence on the initial estimate or search directions. These careful selection rules---which effectively serve as a variance reduction scheme---provide a tighter initial guess  more robust descent directions  and thus enhanced practical performance. Further  this procedure also achieves a near-optimal statistical accuracy in the presence of noise. Finally  we demonstrate empirically that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size.,Solving Random Quadratic Systems of Equations

Is Nearly as Easy as Solving Linear Systems

Yuxin Chen

Department of Statistics

Stanford University
Stanford  CA 94305

yxchen@stanfor.edu

Department of Mathematics and Department of Statistics

Emmanuel J. Candès

Stanford University
Stanford  CA 94305

candes@stanford.edu

Abstract

This paper is concerned with ﬁnding a solution x to a quadratic system of equa-
tions yi = |(cid:104)ai  x(cid:105)|2  i = 1  . . .   m. We demonstrate that it is possible to solve
unstructured random quadratic systems in n variables exactly from O(n) equa-
tions in linear time  that is  in time proportional to reading the data {ai} and {yi}.
This is accomplished by a novel procedure  which starting from an initial guess
given by a spectral initialization procedure  attempts to minimize a nonconvex
objective. The proposed algorithm distinguishes from prior approaches by reg-
ularizing the initialization and descent procedures in an adaptive fashion  which
discard terms bearing too much inﬂuence on the initial estimate or search direc-
tions. These careful selection rules—which effectively serve as a variance reduc-
tion scheme—provide a tighter initial guess  more robust descent directions  and
thus enhanced practical performance. Further  this procedure also achieves a near-
optimal statistical accuracy in the presence of noise. Empirically  we demonstrate
that the computational cost of our algorithm is about four times that of solving a
least-squares problem of the same size.

1

Introduction

Suppose we are given a response vector y = [yi]1≤i≤m generated from a quadratic transformation
of an unknown object x ∈ Rn/Cn  i.e.

yi = |(cid:104)ai  x(cid:105)|2  

i = 1 ···   m 

(1)
where the feature/design vectors ai ∈ Rn/Cn are known. In other words  we acquire measurements
about the linear product (cid:104)ai  x(cid:105) with all signs/phases missing. Can we hope to recover x from this
nonlinear system of equations?
This problem can be recast as a quadratically constrained quadratic program (QCQP)  which sub-
sumes as special cases various classical combinatorial problems with Boolean variables (e.g. the
NP-complete stone problem [1  Section 3.4.1]). In the physical sciences  this problem is commonly
referred to as phase retrieval [2]; the origin is that in many imaging applications (e.g. X-ray crys-
tallography  diffraction imaging  microscopy) it is infeasible to record the phases of the diffraction
patterns so that we can only record |Ax|2  where x is the electrical ﬁeld of interest. Moreover  this
problem ﬁnds applications in estimating the mixture of linear regression  since one can transform the
latent membership variables into missing phases [3]. Despite its importance across various ﬁelds 
solving the quadratic system (1) is combinatorial in nature and  in general  NP complete.
To be more realistic albeit more challenging  the acquired samples are almost always corrupted by
some amount of noise  namely 

yi ≈ |(cid:104)ai  x(cid:105)|2  

i = 1 ···   m.

(2)

1

For instance  in imaging applications the data are best modeled by Poisson random variables

i = 1 ···   m 

(3)

ind.∼ Poisson(cid:0)|(cid:104)ai  x(cid:105)|2(cid:1) 

yi

which captures the variation in the number of photons detected by a sensor. While we shall pay
special attention to the Poisson noise model due to its practical relevance  the current work aims to
accommodate general—or even deterministic—noise structures.

1.1 Nonconvex optimization

Assuming independent samples  the ﬁrst attempt is to seek the maximum likelihood estimate (MLE):
(4)
where (cid:96) (z; yi) represents the log-likelihood of a candidate z given the outcome yi. As an example 
under the Poisson data model (3)  one has (up to some constant offset)

(cid:96) (z; yi)  

i=1

minimizez −(cid:88)m

(cid:96)(z; yi) = yi log(|a∗

i z|2) − |a∗

i z|2.

(5)

Computing the MLE  however  is in general intractable  since (cid:96)(z; yi) is not concave in z.
Fortunately  under unstructured random systems  the problem is not as ill-posed as it might seem 
and is solvable via convenient convex programs with optimal statistical guarantees [4–12]. The
basic paradigm is to lift the quadratically constrained problem into a linearly constrained problem
by introducing a matrix variable X = xx∗ and relaxing the rank-one constraint. Nevertheless 
working with the auxiliary matrix variable signiﬁcantly increases the computational complexity 
which exceeds the order of n3 and is prohibitively expensive for large-scale data.
This paper follows a different route  which attempts recovery by minimizing the nonconvex objec-
tive (4) or (5) directly (e.g. [2  13–19]). The main incentive is the potential computational beneﬁt 
since this strategy operates directly upon vectors instead of lifting decision variables to higher di-
mension. Among this class of procedures  one natural candidate is the family of gradient-descent
type algorithms developed with respect to the objective (4). This paradigm can be regarded as per-
forming some variant of stochastic gradient descent over the random samples {(yi  ai)}1≤i≤m as
an approximation to maximize the population likelihood L(z) := E(y a)[(cid:96)(z; y)]. While in general
nonconvex optimization falls short of performance guarantees  a recently proposed approach called
Wirtinger Flow (WF) [13] promises efﬁciency under random features. In a nutshell  WF initializes
the iterate via a spectral method  and then successively reﬁnes the estimate via the following update
rule:

z(t+1) = z(t) +

∇(cid:96)(z(t); yi) 

where z(t) denotes the tth iterate of the algorithm  and µt is the learning rate. Here  ∇(cid:96)(z; yi)
represents the Wirtinger derivative with respect to z  which reduces to the ordinary gradient in the
real setting. Under Gaussian designs  WF (i) allows exact recovery from O (n log n) noise-free
quadratic equations [13];1 (ii) recovers x up to -accuracy within O(mn2 log 1/) time (or ﬂops)
[13]; and (iii) is stable and converges to the MLE under Gaussian noise [20]. Despite these intriguing
guarantees  the computational complexity of WF still far exceeds the best that one can hope for.
Moreover  its sample complexity is a logarithmic factor away from the information-theoretic limit.

(cid:88)m

i=1

µt
m

1.2 This paper: Truncated Wirtinger Flow

This paper develops a novel linear-time algorithm  called Truncated Wirtinger Flow (TWF)  that
achieves a near-optimal statistical accuracy. The distinguishing features include a careful initializa-
tion procedure and a more adaptive gradient ﬂow. Informally  TWF entails two stages:

subset T0 of data {yi} that do not bear too much inﬂuence on the spectral estimates;

1. Initialization: compute an initial guess z(0) by means of a spectral method applied to a
2. Loop: for 0 ≤ t < T  

z(t+1) = z(t) +

∇(cid:96)(z(t); yi)

(6)

(cid:88)

µt
m

i∈Tt+1

for some index set Tt+1 ⊆ {1 ···   m} over which ∇(cid:96)(z(t); yi) are well-controlled.

1 f (n) = O (g(n)) or f (n) (cid:46) g(n) (resp. f (n) (cid:38) g(n)) means there exists a constant c > 0 such that

|f (n)| ≤ c|g(n)| (resp. |f (n)| ≥ c|g(n)|). f (n) (cid:16) g(n) means f (n) and g(n) are orderwise equivalent.

2

(a)

(b)

Figure 1:
(a) Relative errors of CG and TWF vs. iteration count  where n = 1000 and m = 8n.
(b) Relative MSE vs. SNR in dB  where n = 100. The curves are shown for two settings: TWF for
solving quadratic equations (blue)  and MLE had we observed additional phase information (green).

We highlight three aspects of the proposed algorithm  with details deferred to Section 2.

(a) In contrast to WF and other gradient descent variants  we regularize both the initialization and
the gradient ﬂow in a more cautious manner by operating only upon some iteration-varying
index sets Tt. The main point is that enforcing such careful selection rules lead to tighter
initialization and more robust descent directions.
(b) TWF sets the learning rate µt in a far more liberal fashion (e.g. µt ≡ 0.2 under suitable
conditions)  as opposed to the situation in WF that recommends µt = O(1/n).
(c) Computationally  each iterative step mainly consists in calculating {∇(cid:96)(z; yi)}  which is in-
expensive and can often be performed in linear time  that is  in time proportional to evaluating
the data and the constraints. Take the real-valued Poisson likelihood (5) for example:

(cid:26) yi

|a(cid:62)
i z|2

(cid:18) yi − |a(cid:62)

i z|2
a(cid:62)
i z

(cid:19)

(cid:27)

= 2

(cid:40)

∇(cid:96)(z; yi) = 2

aia(cid:62)

i z − aia(cid:62)
i z

1 ≤ i ≤ m 

ai 

which essentially amounts to two matrix-vector products. To see this  rewrite
i ∈ Tt+1 
otherwise 

2 yi−|a(cid:62)
i z(t)|2
a(cid:62)
i z(t)
0 

∇(cid:96)(z(t); yi) = A(cid:62)v 

vi =

 

(cid:88)

i∈Tt+1

where A := [a1 ···   am](cid:62). Hence  Az(t) gives v and A(cid:62)v the desired truncated gradient.

1.3 Numerical surprises
The power of TWF is best illustrated by numerical examples. Since x and e−jφx are indistinguish-
able given y  we evaluate the solution based on a metric that disregards the global phase [13]:

dist (z  x) := minϕ:∈[0 2π) (cid:107)e−jϕz − x(cid:107).

(7)

In the sequel  TWF operates according to the Poisson log-likelihood (5)  and takes µt ≡ 0.2.
We ﬁrst compare the computational efﬁciency of TWF for solving quadratic systems with that of
conjugate gradient (CG) for solving least square problems. As is well known  CG is among the
most popular methods for solving large-scale least square problems  and hence offers a desired
benchmark. We run TWF and CG respectively over the following two problems:

(a)
(b)

ﬁnd x ∈ Rn
ﬁnd x ∈ Rn

s.t. bi = a(cid:62)
i x 
s.t. bi = |a(cid:62)
i x| 

1 ≤ i ≤ m 
1 ≤ i ≤ m 

where m = 8n  x ∼ N (0  I)  and ai
ind.∼ N (0  I). This yields a well-conditioned design matrix
A  for which CG converges extremely fast [21]. The relative estimation errors of both methods are
reported in Fig. 1(a)  where TWF is seeded by 10 power iterations. The iteration counts are plotted
in different scales so that 4 TWF iterations are tantamount to 1 CG iteration. Since each iteration
of CG and TWF involves two matrix vector products Az and A(cid:62)v  the numerical plots lead to a
suprisingly positive observation for such an unstructured design:

3

Iteration01015Relative error10-810-710-610-510-410-310-210-11000204060 5least squares (CG)truncated WFSNR (dB) (n =100)152025303540455055Relative MSE (dB)-65-60-55-50-45-40-35-30-25-20truncated WFMLE w/ phase Figure 2: Recovery after (top) truncated spectral initialization  and (bottom) 50 TWF iterations.

Even when all phase information is missing  TWF is capable of solving a quadratic system of
equations only about 4 times2 slower than solving a least squares problem of the same size!

The numerical surprise extends to noisy quadratic systems. Under the Poisson data model  Fig. 1(b)
displays the relative mean-square error (MSE) of TWF when the signal-to-noise ratio (SNR) varies;
here  the relative MSE and the SNR are deﬁned as3

MSE := dist2(ˆx  x) / (cid:107)x(cid:107)2

and

SNR := 3(cid:107)x(cid:107)2 

(8)

where ˆx is an estimate. Both SNR and MSE are displayed on a dB scale (i.e. the values of
10 log10(SNR) and 10 log10(MSE) are plotted). To evaluate the quality of the TWF solution  we
compare it with the MLE applied to an ideal problem where the phases (i.e. {ϕi = sign(a(cid:62)
i x)})
are revealed a priori. The presence of this precious side information gives away the phase retrieval
problem and allows us to compute the MLE via convex programming. As illustrated in Fig. 1(b) 
TWF solves the quadratic system with nearly the best possible accuracy  since it only incurs an extra
1.5 dB loss compared to the ideal MLE with all true phases revealed.
To demonstrate the scalability of TWF on real data  we apply TWF on a 320×1280 image. Consider
a type of physically realizable measurements called coded diffraction patterns (CDP) [22]  where

y(l) = |F D(l)x|2 

1 ≤ l ≤ L 

(9)
where m = nL  |z|2 denotes the vector of entrywise squared magnitudes  and F is the DFT matrix.
Here  D(l) is a diagonal matrix whose diagonal entries are randomly drawn from {1 −1  j −j} 
which models signal modulation before diffraction. We generate L = 12 masks for measurements 
and run TWF on a MacBook Pro with a 3 GHz Intel Core i7. We run 50 truncated power iterations
and 50 TWF iterations  which in total cost 43.9 seconds for each color channel. The relative errors
after initialization and TWF iterations are 0.4773 and 2.2 × 10−5  respectively; see Fig. 2.

1.4 Main results

We corroborate the preceding numerical ﬁndings with theoretical support. For concreteness  we
assume TWF proceeds according to the Poisson log-likelihood (5). We suppose the samples (yi  ai)
are independently and randomly drawn from the population  and model the random features ai as

ai ∼ N (0  I n) .

(10)

To start with  the following theorem conﬁrms the performance of TWF under noiseless data.

2Similar phenomena arise in many other experiments we’ve conducted (e.g. when the sample size m ranges
i x)2 and yi − µi 
3To justify the deﬁnition of SNR  note that the signals and noise are captured by µi = (a(cid:62)
m(cid:107)x(cid:107)2 = 3(cid:107)x(cid:107)2.

from 6n to 20n). In fact  this factor seems to improve slightly as m/n increases.
i x|4
i x|2 ≈ 3m(cid:107)x(cid:107)4

respectively. The SNR is thus given by

(cid:80)m
(cid:80)m
i=1 Var[yi] =

i=1 µ2
i

(cid:80)m
(cid:80)m
i=1 |a(cid:62)
i=1 |a(cid:62)

4

Theorem 1 (Exact recovery). Consider the noiseless case (1) with an arbitrary x ∈ Rn. Suppose
that the learning rate µt is either taken to be a constant µt ≡ µ > 0 or chosen via a backtracking
line search. Then there exist some constants 0 < ρ  ν < 1 and µ0  c0  c1  c2 > 0 such that with
probability exceeding 1 − c1 exp (−c2m)  the TWF estimates (Algorithm 1) obey

dist(z(t)  x) ≤ ν(1 − ρ)t(cid:107)x(cid:107) 

∀t ∈ N 

(11)

provided that m ≥ c0n and µ ≤ µ0. As discussed below  we can take µ0 ≈ 0.3.
Theorem 1 justiﬁes two intriguing properties of TWF. To begin with  TWF recovers the ground
truth exactly as soon as the number of equations is on the same order of the number of unknowns 
which is information theoretically optimal. More surprisingly  TWF converges at a geometric rate 
i.e. it achieves -accuracy (i.e. dist(z(t)  x) ≤ (cid:107)x(cid:107)) within at most O (log 1/) iterations. As a
result  the time taken for TWF to solve the quadratic systems is proportional to the time taken to
read the data  which conﬁrms the linear-time complexity of TWF. These outperform the theoretical
guarantees of WF [13]  which requires O(mn2 log 1/) runtime and O(n log n) sample complexity.
Notably  the performance gain of TWF is the result of the key algorithmic changes. Rather than
maximizing the data usage at each step  TWF exploits the samples at hand in a more selective
manner  which effectively trims away those components that are too inﬂuential on either the initial
guess or the search directions  thus reducing the volatility of each movement. With a tighter initial
guess and better-controlled search directions in place  TWF is able to proceed with a more aggressive
learning rate. Taken collectively these efforts enable the appealing convergence property of TWF.
Next  we turn to more realistic noisy data by accounting for a general additive noise model:

yi = |(cid:104)ai  x(cid:105)|2 + ηi 

1 ≤ i ≤ m 

where ηi represents a noise term. The stability of TWF is demonstrated in the theorem below.
Theorem 2 (Stability). Consider the noisy case (12). Suppose that the learning rate µt is either
taken to be a positive constant µt ≡ µ or chosen via a backtracking line search. If

(12)

(13)

(14)

(15)

then with probability at least 1 − c2 exp (−c3m)  the TWF estimates (Algorithm 1) satisfy

(cid:107)η(cid:107)∞ ≤ c1 (cid:107)x(cid:107)2  

dist(z(t)  x) (cid:46)

and

m ≥ c0n  µ ≤ µ0 
(cid:107)η(cid:107)√
m(cid:107)x(cid:107) + (1 − ρ)t(cid:107)x(cid:107) 
|(cid:104)ai  x(cid:105)|4(cid:17)

(cid:16)(cid:88)m

SNR :=

i=1

/ (cid:107)η(cid:107)2 ≈ 3m(cid:107)x(cid:107)4 / (cid:107)η(cid:107)2 

∀t ∈ N

for all x ∈ Rn. Here  0 < ρ < 1 and µ0  c0  c1  c2  c3 > 0 are some universal constants.
Alternatively  if one regards the SNR for the model (12) as follows

then we immediately arrive at another form of performance guarantee stated in terms of SNR:

1√
SNR

dist(z(t)  x) (cid:46)

(cid:107)x(cid:107) + (1 − ρ)t(cid:107)x(cid:107) 

∀t ∈ N.

(16)
As a consequence  the relative error of TWF reaches O(SNR−1/2) within a logarithmic number of
iterations. It is worth emphasizing that the above stability guarantee is deterministic  which holds for
any noise structure obeying (13). Encouragingly  this statistical accuracy is nearly un-improvable 
as revealed by a minimax lower bound that we provide in the supplemental materials.
We pause to remark that several other nonconvex methods have been proposed for solving quadratic
equations  which exhibit intriguing empirical performances. A partial list includes the error reduc-
tion schemes by Fienup [2]  alternating minimization [14]  Kaczmarz method [17]  and generalized
approximate message passing [15]. However  most of them fall short of theoretical support. The
analytical difﬁculty arises since these methods employ the same samples in each iteration  which
introduces complicated dependencies across all iterates. To circumvent this issue  [14] proposes
a sample-splitting version of the alternating minimization method that employs fresh samples in
each iteration. Despite the mathematical convenience  the sample complexity of this approach is
O(n log3 n + n log2 n log 1/)  which is a factor of O(log3 n) from optimal and is empirically
largely outperformed by the variant that reuses all samples. In contrast  our algorithm uses the same
pool of samples all the time and is therefore practically appealing. Besides  the approach in [14]
does not come with provable stability guarantees. Numerically  each iteration of Fienup’s algorithm
(or alternating minimization) involves solving a least squares problem  and the algorithm converges
in tens or hundreds of iterations. This is computationally more expensive than TWF  whose compu-
tational complexity is merely about 4 times that of solving a least squares problem.

5

2 Algorithm: Truncated Wirtinger Flow

we denote A := [a1 ···   am](cid:62) and A (M ) :=(cid:8)a(cid:62)

(cid:9)

This section explains the basic principles of truncated Wirtinger ﬂow. For notational convenience 

i M ai

1≤i≤m for any M ∈ Rn×n.

2.1 Truncated gradient stage

In the case of independent real-valued data  the descent direc-
tion of the WF updates—which is the gradient of the Poisson
log-likelihood—can be expressed as follows:
i z|2
yi − |a(cid:62)
(cid:125)
a(cid:62)
i z

∇(cid:96)(z; yi) =

m(cid:88)

m(cid:88)

(cid:123)(cid:122)

(17)

(cid:124)

ai 

i=1

i=1

2

:=νi

Figure 3: The locus of − 1
2∇(cid:96)i (z)
for all unit vectors ai. The red ar-
rows depict those directions with
large weights.

where νi represents the weight assigned to each feature ai.
Unfortunately  the gradient of this form is non-integrable and
hence uncontrollable. To see this  consider any ﬁxed z ∈ Rn.
m(cid:107)z(cid:107) 
The typical value of min1≤i≤m |a(cid:62)
leading to some excessively large weights νi. Notably  an un-
derlying premise for a nonconvex procedure to succeed is to
ensure all iterates reside within a basin of attraction  that is  a neighborhood surrounding x within
which x is the unique stationary point of the objective. When a gradient is unreasonably large  the
iterative step might overshoot and end up leaving this basin of attraction. Consequently  WF moving
along the preceding direction might not come close to the truth unless z is already very close to x.
This is observed in numerical simulations4.
TWF addresses this challenge by discarding terms having too high of a leverage on the search
direction; this is achieved by regularizing the weights νi via appropriate truncation. Speciﬁcally 

i z| is on the order of 1

z(t+1) = z(t) +

∇(cid:96)tr(z(t)) 

∀t ∈ N 

(18)

µt
m

where ∇(cid:96)tr (·) denotes the truncated gradient given by
yi − |a(cid:62)
i z|2
a(cid:62)
i z
for some appropriate truncation criteria speciﬁed by E i
E i
1 (z) and E i
2 (z) to be two collections of events given by

∇(cid:96)tr (z) :=

(cid:88)m
z (cid:107)z(cid:107) ≤(cid:12)(cid:12)a(cid:62)

i z(cid:12)(cid:12) ≤ αub

i=1

2

:= (cid:8)αlb
(cid:26)

:=

E i
1(z)
E i
2(z)

|yi − |a(cid:62)

i z|2| ≤ αh
m

ai1E i
1 (·) and E i

(19)
1(z)∩E i
2 (·). In our algorithm  we take

2(z)

z (cid:107)z(cid:107)(cid:9);
(cid:13)(cid:13)y − A(cid:0)zz(cid:62)(cid:1)(cid:13)(cid:13)1

(cid:27)

 

|a(cid:62)
i z|
(cid:107)z(cid:107)

(20)

(21)

z   αub

where αlb
z   αz are predetermined truncation thresholds. In words  we drop components whose
size fall outside some conﬁdence range—a range where the magnitudes of both the numerator and
denominator of νi are comparable to their respective mean values.
This paradigm could be counter-intuitive at ﬁrst glance  since one might expect the larger terms
to be better aligned with the desired search direction. The issue  however  is that the large terms
are extremely volatile and could dominate all other components in an undesired way. In contrast 
TWF makes use of only gradient components of typical sizes  which slightly increases the bias but
remarkably reduces the variance of the descent direction. We expect such gradient regularization
and variance reduction schemes to be beneﬁcial for solving a broad family of nonconvex problems.

2.2 Truncated spectral initialization

A key step to ensure meaningful convergence is to seed TWF with some point inside the basin of
attraction  which proves crucial for other nonconvex procedures as well. An appealing initialization

4For complex-valued data  WF converges empirically  as mini |a(cid:62)

i z| is much larger than the real case.

6

z = (3  6)x = (2.7  8)Algorithm 1 Truncated Wirtinger Flow.
Input: Measurements {yi | 1 ≤ i ≤ m} and feature vectors {ai | 1 ≤ i ≤ m}; truncation
thresholds αlb

z = 0.3  αub

z = αh = 5  and αy = 3)

z   αub

and αy ≥ 3.

(25)

i=1 yi and ˜z is the leading eigenvector of

Initialize z(0) to be(cid:113) mn(cid:80)m

0 < αlb

(cid:113) 1

z   αh  and αy satisfying (by default  αlb
z ≤ 0.5  αub
z ≥ 5  αh ≥ 5 
(cid:80)m
(cid:88)m
yi −(cid:12)(cid:12)a∗

i=1(cid:107)ai(cid:107)2 λ˜z  where λ =

yiaia∗

1
m

Y =

i=1

m

i 1{|yi|≤α2

0}.

yλ2

i z(t)(cid:12)(cid:12)2

Loop: for t = 0 : T do

(cid:26)

where
E i
1 :=

z ≤
αlb

(cid:27)

z(t+1) = z(t) +
i z(t)|
|a∗
(cid:107)z(t)(cid:107) ≤ αub

 

z

√
n
(cid:107)ai(cid:107)

and Kt :=

1
m

2µt
m
E i
2 :=

(cid:88)m
(cid:26)
(cid:88)m

i=1

l=1

z(t)∗ai

ai1E i

|yi − |a∗

i z(t)|2| ≤ αhKt

(cid:12)(cid:12)yl − |a∗

l z(t)|2(cid:12)(cid:12).

2

1∩E i
 
√
n
(cid:107)ai(cid:107)

(22)

(23)

  (24)

(cid:27)

i z(t)|
|a∗
(cid:107)z(t)(cid:107)

Output z(T ).

procedure is the spectral method [14] [13]  which initializes z(0) as the leading eigenvector of (cid:101)Y :=
(cid:80)m
i=1 yiaia(cid:62)

i . This is based on the observation that for any ﬁxed unit vector x 

1
m

E[(cid:101)Y ] = I + 2xx(cid:62) 
(cid:0)m−1aka(cid:62)

whose principal component is exactly x with an eigenvalue of 3.
Unfortunately  the success of this method requires a sample complexity exceeding n log n. To see
this  recall that maxi yi ≈ 2 log m. Letting k = arg maxi yi and ˜ak := ak/(cid:107)ak(cid:107)  one can derive

which dominates x(cid:62)(cid:101)Y x ≈ 3 unless m (cid:38) n log m. As a result  ˜ak is closer to the principal
component of (cid:101)Y than x when m (cid:16) n. This drawback turns out to be a substantial practical issue.

k yk

(cid:1) ˜ak ≈ (2n log m)/m 

k (cid:101)Y ˜ak ≥ ˜a(cid:62)

k

˜a(cid:62)

(cid:88)m

This issue can be remedied if we preclude those data yi with large
magnitudes when running the spectral method. Speciﬁcally  we
propose to initialize z(0) as the leading eigenvector of

1
m

yiaia(cid:62)

Y :=

(26)
followed by proper scaling so as to ensure (cid:107)z(0)(cid:107) ≈ (cid:107)x(cid:107). As illus-
trated in Fig. 4  the empirical advantage of the truncated spectral
method is increasingly more remarkable as n grows.

i 1{|yi|≤α2

y( 1

i=1

m

(cid:80)m
l=1 yl)}

2.3 Choice of algorithmic parameters

Figure 4: Relative initializa-
tion error when ai ∼ N (0  I).

One important implementation detail is the learning rate µt. There
are two alternatives that work well in both theory and practice:
1. Fixed size. Take µt ≡ µ for some constant µ > 0. As long as µ is not too large  this strategy
always works. Under the condition (25)  our theorems hold for any positive constant µ < 0.28.
2. Backtracking line search with truncated objective. This strategy performs a line search along
the descent direction and determines an appropriate learning rate that guarantees a sufﬁcient
improvement with respect to the truncated objective. Details are deferred to the supplement.

Another algorithmic details to specify are the truncation thresholds αh  αlb
z   and αy. The
present paper isolates a concrete set of combinations as given in (25). In all theory and numerical
experiments presented in this work  we assume that the parameters fall within this range.

z   αub

7

n: signal dimension (m = 6n)10002000300040005000Relative MSE0.70.80.9 1 spectral method truncated spectral method(a)

(b)

(c)

Figure 5: (a) Empirical success rates for real Gaussian design; (b) empirical success rates for com-
plex Gaussian design; (c) relative MSE (averaged over 100 runs) vs. SNR for Poisson data.

3 More numerical experiments and discussion

z = 0.3  αub

We conduct more extensive numerical experiments to corroborate our main results and verify the
applicability of TWF on practical problems. For all experiments conducted herein  we take a ﬁxed
step size µt ≡ 0.2  employ 50 power iterations for initialization and T = 1000 gradient iterations.
The truncation levels are taken to be the default values αlb
We ﬁrst apply TWF to a sequence of noiseless problems with n = 1000 and varying m. Generate the
ind.∼ N (0  I);
object x at random  and produce the feature vectors ai in two different ways: (1) ai
ind.∼ N (0  I) + jN (0  I). A Monte Carlo trial is declared success if the estimate ˆx obeys
(2) ai
dist (ˆx  x) /(cid:107)x(cid:107) ≤ 10−5. Fig. 5(a) and 5(b) illustrate the empirical success rates of TWF (average
over 100 runs for each m) for noiseless data  indicating that m ≥ 5n are m ≥ 4.5n are often
sufﬁcient under real and complex Gaussian designs  respectively. For the sake of comparison  we
simulate the empirical success rates of WF  with the step size µt = min{1 − e−t/330  0.2} as
recommended by [13]. As shown in Fig. 5  TWF outperforms WF under random Gaussian features 
implying that TWF exhibits either better convergence rate or enhanced phase transition behavior.

z = αh = 5  and αy = 3.

ind.∼
Next  we empirically evaluate the stability of TWF under noisy data. Set n = 1000  produce ai
N (0  I)  and generate yi according to the Poisson model (3). Fig. 5(c) shows the relative mean
square error—on the dB scale—with varying SNR (cf. (8)). As can be seen  the empirical relative
MSE scales inversely proportional to SNR  which matches our stability guarantees in Theorem 2
(since on the dB scale  the slope is about -1 as predicted by the theory (16)).
While this work focuses on the Poisson-type objective for con-
creteness  the proposed paradigm carries over to a variety of
nonconvex objectives  and might have implications in solving
other problems that involve latent variables  e.g. matrix comple-
tion [23–25]  sparse coding [26]  dictionary learning [27]  and
mixture problems (e.g. [28  29]). We conclude this paper with an
example on estimating mixtures of linear regression. Imagine

(cid:26)a(cid:62)

yi ≈

i β1  with probability p 
a(cid:62)
i β2 

else 

1 ≤ i ≤ m 

(27)

Figure 6: Empirical success rate
for mixed regression (p = 0.5).

1 ≤ i ≤ m 

where β1  β2 are unknown. It has been shown in [3] that in the
noiseless case  the ground truth satisﬁes
(cid:62)
2 + β2β

fi(β1  β2) := y2

1 )ai − a(cid:62)
(cid:62)

i + 0.5a(cid:62)

i (β1β

reduces to the form (1)). Running TWF with a nonconvex objective(cid:80)m

which forms a set of quadratic constraints (in particular  if one further knows β1 = −β2  then this
i (z1  z2) (with the
assistance of a 1-D grid search proposed in [29] applied right after truncated initialization) yields
accurate estimation of β1  β2 under minimal sample complexity  as illustrated in Fig. 6.

i (β1 + β2) yi = 0 

i=1 f 2

Acknowledgments

E. C. is partially supported by NSF under grant CCF-0963835 and by the Math + X Award from the
Simons Foundation. Y. C. is supported by the same NSF grant.

8

m: number of measurements (n =1000) 2n 3n 4n 5n 6n Empirical success rate 0 0.5 1 TWF (Poisson objective) WF (Gaussian objective)m: number of measurements (n =1000) 2n 3n 4n 5n 6n Empirical success rate 0 0.5 1 TWF (Poisson objective) WF (Gaussian objective)SNR (dB) (n =1000)152025303540455055Relative MSE (dB)-65-60-55-50-45-40-35-30-25-20 m = 6n m = 8n m = 10nm: number of measurements ( n =1000) 5n 6n 7n 8n 9n 10n Empirical success rate 0 0.5 1 References
[1] A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization  volume 2. 2001.
[2] J. R. Fienup. Phase retrieval algorithms: a comparison. Applied optics  21:2758–2769  1982.
[3] Y. Chen  X. Yi  and C. Caramanis. A convex formulation for mixed regression with two

components: Minimax optimal rates. In Conference on Learning Theory (COLT)  2014.

[4] E. J. Candès  T. Strohmer  and V. Voroninski. Phaselift: Exact and stable signal recovery from
magnitude measurements via convex programming. Communications on Pure and Applied
Mathematics  66(8):1017–1026  2013.

[5] I. Waldspurger  A. d’Aspremont  and S. Mallat. Phase recovery  maxcut and complex semidef-

inite programming. Mathematical Programming  149(1-2):47–81  2015.

[6] Y. Shechtman  Y. C. Eldar  A. Szameit  and M. Segev. Sparsity based sub-wavelength imaging
with partially incoherent light via quadratic compressed sensing. Optics express  19(16)  2011.
[7] E. J. Candès and X. Li. Solving quadratic equations via PhaseLift when there are about as
many equations as unknowns. Foundations of Computational Math.  14(5):1017–1026  2014.
[8] H. Ohlsson  A. Yang  R. Dong  and S. Sastry. Cprl–an extension of compressive sensing to the
phase retrieval problem. In Advances in Neural Information Processing Systems (NIPS)  2012.
[9] Y. Chen  Y. Chi  and A. J. Goldsmith. Exact and stable covariance estimation from quadratic

sampling via convex programming. IEEE Trans. on Inf. Theory  61(7):4034–4059  2015.

[10] T. Cai and A. Zhang. ROP: Matrix recovery via rank-one projections. Annals of Stats.
[11] K. Jaganathan  S. Oymak  and B. Hassibi. Recovery of sparse 1-D signals from the magnitudes

of their Fourier transform. In IEEE ISIT  pages 1473–1477  2012.

[12] D. Gross  F. Krahmer  and R. Kueng. A partial derandomization of phaselift using spherical

designs. Journal of Fourier Analysis and Applications  21(2):229–266  2015.

[13] E. J. Candès  X. Li  and M. Soltanolkotabi. Phase retrieval via Wirtinger ﬂow: Theory and

algorithms. IEEE Transactions on Information Theory  61(4):1985–2007  April 2015.

[14] P. Netrapalli  P. Jain  and S. Sanghavi. Phase retrieval using alternating minimization. NIPS 

2013.

[15] P. Schniter and S. Rangan. Compressive phase retrieval via generalized approximate message

passing. IEEE Transactions on Signal Processing  63(4):1043–1055  Feb 2015.

[16] A. Repetti  E. Chouzenoux  and J.-C. Pesquet. A nonconvex regularized approach for phase

retrieval. International Conference on Image Processing  pages 1753–1757  2014.

[17] K. Wei. Phase retrieval via Kaczmarz methods. arXiv:1502.01822  2015.
[18] C. White  R. Ward  and S. Sanghavi. The local convexity of solving quadratic equations.

arXiv:1506.07868  2015.

[19] Y. Shechtman  A. Beck  and Y. C. Eldar. GESPAR: Efﬁcient phase retrieval of sparse signals.

IEEE Transactions on Signal Processing  62(4):928–938  2014.

[20] M. Soltanolkotabi. Algorithms and Theory for Clustering and Nonconvex Quadratic Program-

ming. PhD thesis  Stanford University  2014.

[21] L. N. Trefethen and D. Bau III. Numerical linear algebra  volume 50. SIAM  1997.
[22] E. J. Candès  X. Li  and M. Soltanolkotabi. Phase retrieval from coded diffraction patterns. to

appear in Applied and Computational Harmonic Analysis  2014.

[23] R. Keshavan  A. Montanari  and S. Oh. Matrix completion from noisy entries. Journal of

Machine Learning Research  11:2057–2078  2010.

[24] P. Jain  P. Netrapalli  and S. Sanghavi. Low-rank matrix completion using alternating mini-

mization. In ACM symposium on Theory of computing  pages 665–674  2013.

[25] R. Sun and Z. Luo. Guaranteed matrix completion via nonconvex factorization. FOCS  2015.
[26] S. Arora  R. Ge  T. Ma  and A. Moitra. Simple  efﬁcient  and neural algorithms for sparse

coding. Conference on Learning Theory (COLT)  2015.

[27] J. Sun  Q. Qu  and J. Wright. Complete dictionary recovery over the sphere. ICML  2015.
[28] S. Balakrishnan  M. J. Wainwright  and B. Yu. Statistical guarantees for the EM algorithm:

From population to sample-based analysis. arXiv preprint arXiv:1408.2156  2014.

[29] X. Yi  C. Caramanis  and S. Sanghavi. Alternating minimization for mixed linear regression.

International Conference on Machine Learning  June 2014.

9

,Pablo Sprechmann
Roee Litman
Tal Ben Yakar
Alexander Bronstein
Guillermo Sapiro
Yuxin Chen
Emmanuel Candes