2014,Optimal rates for k-NN density and mode estimation,We present two related contributions of independent interest: (1) high-probability finite sample rates for $k$-NN density estimation  and (2) practical mode estimators -- based on $k$-NN -- which attain minimax-optimal rates under surprisingly general distributional conditions.,Optimal rates for k-NN density and mode estimation

Sanjoy Dasgupta

University of California  San Diego  CSE

dasgupta@eng.ucsd.edu

Samory Kpotufe ∗

Princeton University  ORFE
samory@princeton.edu

Abstract

We present two related contributions of independent interest: (1) high-probability
ﬁnite sample rates for k-NN density estimation  and (2) practical mode estimators
– based on k-NN – which attain minimax-optimal rates under surprisingly general
distributional conditions.

1

Introduction

We prove ﬁnite sample bounds for k-nearest neighbor (k-NN) density estimation  and subsequently
apply these bounds to the related problem of mode estimation. These two main results  while related 
are interesting on their own.
First  k-NN density estimation [1] is one of the better known and simplest density estimation pro-
cedures. The estimate fk(x) of an unknown density f (see Deﬁnition 1 of Section 3) is a simple
functional of the distance rk(x) from x to its k-th nearest neighbor in a sample X[n] (cid:44) {Xi}n
i=1.
As such it is intimately related to other functionals of rk(x)  e.g. the degree of vertices x in k-NN
graphs and their variants used in modeling communities and in clustering applications (see e.g. [2]).
While this procedure has been known for a long time  its convergence properties are still not fully
understood. The bulk of research in the area has concentrated on establishing its asymptotic con-
vergence  while its ﬁnite sample properties have received little attention in comparison. Our ﬁnite
sample bounds are concisely derived once the proper tools are identiﬁed. The bounds hold with high
probability  under general conditions on the unknown density f. This generality proves quite useful
as shown in our subsequent application to the problem of mode estimation.
The basic problem of estimating the modes (local maxima) of an unknown density f has also been
studied for a while (see e.g. [3] for an early take on the problem). It arises in various unsupervised
problems where modes are used as a measure of typicality of a sample X. In particular  in modern
applications  mode estimation is often used in clustering  with the modes representing cluster centers
(see e.g. [4  5] and general applications of the popular mean-shift procedure).
While there exists a rich literature on mode estimation  the bulk of theoretical work concerns es-
timators of a single mode (highest maximum of f)  and often concentrates on procedures that are
hard to implement in practice. Given the generality of our ﬁrst result on k-NN density estimation 
we can prove that some simple implementable procedures yield optimal estimates of the modes of
an unknown density f  under surprisingly general conditions on f.
Our results are overviewed in the following section  along with an overview of the rich literature on
k-NN density estimation and mode estimation. This is followed by our theoretical setup in Section 3;
our rates for k-NN density estimation are detailed in Section 4  while the results on mode estimation
are given in Section 5.

∗Much of this work was conducted when this author was at TTI-Chicago.

1

2 Overview of results and related Work

2.1 Rates for k-NN density estimates

√

The k-NN density estimator dates back perhaps to the early work of [1] where it is shown to be
consistent when the unknown density f is continuous on Rd. While one of the best known and
simplest procedure for density estimation  it has proved more cumbersome to analyze than its smooth
counterpart  the kernel density estimator.
More general consistency results such as [6  7] have been established since its introduction.
In
particular [6] shows that  for f Lipschitz in a neighborhood of a point x  where f (x) > 0  and
k = k(n) satisfying k → ∞ and k/n2/(2+d) → 0  the estimator is asymptotically normal  i.e.
√
D−→ N (0  1). The recent work of [8]  concerning generalized weighted
k(fk(x) − f (x))/f (x)
variants of k-NN  shows that asymptotic normality holds under the weaker restriction k/n4/(4+d) →
0 if f is twice differentiable at x.
Asymptotic normality as stated above yields some insight into the rate of convergence of fk: we
can expect that |fk(x) − f (x)| (cid:46) f (x)/
k under the stated conditions on k. In fact  [8] shows
that such a result can be obtained in expectation for n = n(x) sufﬁciently large.
In particular 
their conditions on k allows for a setting of k ≈ n4/(4+d) (not allowed under the above conditions)
yielding a minimax-optimal l2 risk E |fk(x) − f (x)|2 (cid:46) f (x)2/k = O(n−4/(4+d)).
While consistency results and bounds on expected error are now well understood  we still don’t have
a clear understanding of the conditions under which high probability bounds on |fk(x) − f (x)| are
possible. This is particularly important given the inherent instability of nearest neighbors estimates
which are based on order-statistics rather than the more stable average statistics at the core of kernel-
density estimates. The recent result of [9] provides an initial answer: they obtain a high-probability
bound uniformly over x taking value in the sample X[n]  however under conditions not allowing for
optimal settings of k (where f is assumed Lipschitz).
The bounds in the present paper hold with high-probability  simultaneously for all x in the support
of f. Rather than requiring smoothness conditions on f  we simply give the bounds in terms of the
modulus of continuity of f at any x  i.e. how much f can change in a neighborhood of x. This
allows for a useful degree of ﬂexibility in applying these bounds. In particular  optimal bounds
under various degrees of smoothness of f at x easily follow. More importantly  for our application
to mode estimation  the bounds allow us to handle |fk(x) − f (x)| at different x ∈ Rd with varying
smoothness in f. As a result we can derive minimax-optimal mode estimation rates for practical
procedures under surprisingly weak assumptions.

2.2 Mode estimation

There is an extensive literature on mode estimation and we unfortunately can only overview some
of the relevant work. Most of the literature covers the case of a unimodal distribution  or one where
there is a single maximizer x0 of f.
Early work on estimating the (single) mode of a distribution focused primarily on understand-
ing the consistency and rates achievable by various approaches  with much less emphasis on the
ease of implementation of these approaches. The common approaches consist of estimating x0 as
ˆx (cid:44) arg supx∈Rd fn(x) where fn is an estimate of f  usually a kernel density estimate. Various
work such as [3  10  11] establish consistency properties of the approach and achievable rates under
various Euclidean settings and regularity assumptions on the distribution F. More recent work such
as [12  13] address the problem of optimal choice of bandwidth and kernel to adaptively achieve
the minimax risk for mode estimation. Essentially  under smoothness κ (e.g. f is κ times differen-
Ef (cid:107)ˆx − x0(cid:107)) is of the form n−(κ−1)/(2κ+d)  as independently
tiable)  the minimax risk (inf ˆx supf
established in [14] and [15].
As noticed early in [16]  the estimator arg supx∈Rd fn(x)  while yielding much insight into the
problem  is hard to implement in practice. Hence  other work  apparently starting with [16  14]
have looked into so-called recursive estimators of the (single) mode which are practical and easy
to update as the sample size increases. These approaches can be viewed as some form of gradient-

2

ascent of fn with carefully chosen step sizes. The later versions of [14] are shown to be minimax-
optimal. Another line of work is that of so-called direct mode estimators which estimate the mode
from practical statistics of the data [17  18]. In particular  [18] shows that the simple and practical
estimator arg maxx∈X[n] fn(x)  where fn is a kernel-density estimator  is a consistent estimator of
the mode. We show in the present paper that arg maxx∈X[n] fk(x)  where fk is a k-NN density
estimator  is not only consistent  but converges at a minimax-optimal rate under surprisingly mild
distributional conditions.
The more general problem of estimating all modes of distribution has received comparatively little
attention. The best known practical approach for this problem is the mean-shift procedure and its
variants [19  4  20  21]  quite related to recursive-mode-estimators  as they essentially consist of
gradient ascent of fn starting from every sample point  where fn is required to be appropriately
smooth to ascend (e.g. a smooth kernel estimate). While mean-shift is popular in practice  it has
proved quite difﬁcult to analyze. A recent result of [22] comes close to establishing the consistency
of mean-shift  as it establishes the convergence of the procedure to the right gradient lines (essen-
tially the ascent path to the mode) if it is seeded from ﬁxed starting points rather than the random
It remains unclear however whether mean-shift produces only true modes 
samples themselves.
given the inherent variability in estimating f from sample. This question was recently addressed by
[23] which proposes a hypothesis test to detect false modes based on conﬁdence intervals around
Hessians estimated at the modes returned by any procedure.
Interestingly  while a k-NN density estimate fk is far from smooth  in fact not even continuous  we
show a simple practical procedure that identiﬁes any mode of the unknown density f under mild
conditions: we mainly require that f is well approximated by a quadratic in a neighborhood of
each mode. Our ﬁnite sample rates (on (cid:107)ˆx − x0(cid:107)  for an estimate ˆx of any mode x0) are of the
form O(k−1/4)  hold with high-probability and are minimax-optimal for an appropriate choice of
k = Θ(n4/(4+d)).
If in addition f is Lipschitz or more generally H¨older-continuous (in principle uniform continuity
of f is enough)  all the modes returned above a level set λ of fk can be optimally assigned to
separate modes of the unknown f. Since λ n→∞−−−−→ 0  the procedure consistently prunes false modes.
This feature is made intrinsic to the procedure by borrowing from insights of [9  24] on identifying
false clusters by inspecting levels sets of fn. These last works concern the related area of level set
estimation  and do not study mode estimation rates.
As alluded to so far  our results are given in terms of local assumptions on modes rather than
global distributional conditions. We show that any mode that is sufﬁciently salient (this is locally
parametrized) w.r.t. the ﬁnite sample size n  is optimally estimated  while false modes are pruned
away. In particular our results allow for f having a countably inﬁnite number of modes.

3 Preliminaries

Throughout the analysis  we assume access to a sample X[n] = {Xi}n
from an
absolutely continuous distribution F over Rd  with Lebesgue-density function f. We let X denote
the support of the density function f.
The k-NN density estimate at a point x is deﬁned as follows.
Deﬁnition 1 (k-NN density estimate). For every x ∈ Rd  let rk(x) denote the distance from x to its
k-th nearest neighbor in X[n]. The density estimate is given as:

i=1 drawn i.i.d.

fk(x) (cid:44)

k

n · vd · rk(x)d  

where vd denotes the volume of the unit sphere in Rd.

All balls considered in the analysis are closed Euclidean balls of Rd.

3

4 k-NN density estimation rates
In this section we bound the error in estimating f (x) as fk(x) at every x ∈ X . The main results of
the section are Lemmas 3 and 4. These lemmas are easily obtained given the right tools: uniform
concentration bounds on the empirical mass of balls in Rd  using relative Vapnik-Chervonenkis
bounds  i.e. Bernstein’s type bounds rather than Chernoff type bounds (see e.g. Theorem 5.1 of
[25]). We next state a form of these bounds for completion.
Lemma 1. Let G be a class of functions from X to {0  1} with VC dimension d < ∞  and P a
probability distribution on X . Let E denote expectation with respect to P. Suppose n points are
drawn independently at random from P; let En denote expectation with respect to this sample. Then
for any δ > 0  with probability at least 1 − δ  the following holds for all g ∈ G:

(cid:112)Eg) ≤ Eg − Eng ≤ min(β2

n + βn

(cid:112)Eng  βn

(cid:112)Eg) 

(cid:112)Eng  β2

− min(βn

where βn =(cid:112)(4/n)(d ln 2n + ln(8/δ)).

n + βn

These sort of relative VC bounds allows for a tighter relation (than Chernoff type bounds) between
empirical and true mass of sets (Eng and Eg) in those situations where these quantities are small 
n = ˜O(1/n) above. This is particularly useful since the balls we have to deal
i.e. of the order of β2
with are those containing approximately k points  and hence of (small) mass approximately k/n.
A direct result of the above lemma is the following lemma of [26]. This next lemma essentially
reworks Lemma 1 above into a form we can use more directly. We re-use Cδ n below throughout
the analysis.
√
Lemma 2 ([26]). Pick 0 < δ < 1. Let Cδ n (cid:44) 16 log(2/δ)
probability at least 1 − δ  for every ball B ⊂ Rd we have 

d log n. Assume k ≥ d log n. With

√
√

k
n

F(B) ≥ Cδ n

F(B) ≥ k
n

+ Cδ n

d log n

=⇒ Fn(B) > 0 

n
=⇒ Fn(B) ≥ k
√
n

  and

F(B) ≤ k
n

− Cδ n

k
n

=⇒ Fn(B) <

k
n

.

The main idea in bounding fk(x) is to bound the random term rk(x) in terms of f (x) using Lemma
2 above. We can deduce from the lemma that if a ball B(x  r) centered has mass roughly k/n  then
its empirical mass is likely to be of the order k/n; hence rk(x) is likely to be close to the radius r
of B(x  r). Now if f does not vary too much in B(x  r)  then we can express the mass of B(x  r) in
terms of f (x)  and thus get our desired bound on rk(x) and fk(x) in terms of f (x).
Our results are given in terms of how f varies in a neighborhood of x  captured as follows.
Deﬁnition 2. For x ∈ Rd and  > 0  deﬁne ˆr(  x) (cid:44) sup
and ˇr(  x) (cid:44) sup

(cid:110)
r : sup(cid:107)x−x(cid:48)(cid:107)≤r f (x(cid:48)) − f (x) ≤ 

(cid:110)
r : sup(cid:107)x−x(cid:48)(cid:107)≤r f (x) − f (x(cid:48)) ≤ 

(cid:111)

(cid:111)

 

.

The continuity parameters ˆr(  x) and ˇr(  x) (related to the modulus of continuity of f at x) are eas-
ily bounded under smoothness assumptions on f at x. Our high-probability bounds on the estimates
fk(x) in terms of f (x) and the continuity parameters are given as follows.
δ n. Then  with probability at least 1 − δ  for all
Lemma 3 (Upper-bound on fk). Suppose k ≥ 4C 2
(cid:19)
x ∈ Rd and all  > 0 

(cid:18)

fk(x) <

1 + 2

(f (x) + )  

Cδ n√
k

provided k satisﬁes vd · ˆr(  x)d · (f (x) + ) ≥ k

n − Cδ n

4

√
k
n .

Lemma 4 (Lower-bound on fk). Then  with probability at least 1− δ  for all x ∈ Rd and all  > 0 

(cid:18)

(cid:19)

fk(x) ≥

1 − Cδ n√
k

(f (x) − )  

√
k
n .

provided k satisﬁes vd · ˇr(  x)d · (f (x) − ) ≥ k
The proof of these results are concise applications of Lemma 2 above. They are given in the appendix
(long version). The trick is in showing that  under the conditions on k  there exists an r ≈ (k/(n ·
f (x)))1/d which is at most ˆr(  x) or ˇr(  x) as appropriate; hence  f does not vary much on B(x  r)
so we must have

n + Cδ n

F (B(x  r)) ≈ volume (B(x  r)) · f (x) = vd · rd · f (x) ≈ k
n

.

√
Using Lemma 2 we get rk(x) ≈ r; plug this value into fk(x) to obtain fk(x) ≈ (1 + 1/
k)f (x).
Lemmas 3 and 4 allow a great deal of ﬂexibility as we will soon see with their application to mode
estimation. In particular we can consider various smoothness conditions simultaneously at different
x for different biases .
Suppose for instance that f is locally H¨older at x 
√
B(x  r) 
√
(/L)1/β; pick  = O(f (x)/
|fk(x) − f (x)| ≤ O(f (x)/

for all x(cid:48) ∈
|f (x) − f (x(cid:48))| ≤ L(cid:107)x − x(cid:48)(cid:107)β. Then for small   both ˆr(  x) and ˇr(  x) are at least
k) for n sufﬁciently large  then by both lemmas we have  w.h.p. 
k)d/βf (x) ≥ Ck/n

∃r  L  β > 0 s.t.

for some constant C. This allows for a setting of k = Θ(cid:0)n2β/(2β+d)(cid:1) for a minimax-optimal rate
of |fk(x) − f (x)| = O(cid:0)n−β/(2β+d(cid:1).

√
k) provided k = Ω(log2 n) and satisﬁes vd(1/L

i.e.

The ability to consider various biases  would prove particularly helpful in the next section on
mode estimation where we have to consider different approximations in different parts of space with
varying smoothness in f. In particular  at a mode x  we will essentially have β = 2 (f is twice
differentiable) while elsewhere on X we might not have much smoothness in f.

5 Mode estimation

We start with the following deﬁnition of modes.
Deﬁnition 3. We denote the set of modes of f by M ≡ {x : ∃r > 0 ∀x(cid:48) ∈ B(x  r)  f (x(cid:48)) < f (x)} .
We need the following assumption at modes.
Assumption 1. f is twice differentiable in a neighborhood of every x ∈ M. We denote the gradient
and Hessian of f by ∇f and ∇2f. Furthermore  ∇2f (x) is negative deﬁnite at all x ∈ M.
Assumption 1 excludes modes at the boundary of the support of f (where f cannot be continuously
differentiable). We note that most work on the subject consider only interior modes as we are
doing here. Modes on the boundary can however be handled under additional boundary smoothness
assumptions to ensure that f puts sufﬁcient mass on any ball around such modes. This however only
complicates the analysis  while the main insights remain the same as for interior modes.
An implication of Assumption 1 is that for all x ∈ M  ∇f is continuous in a neighborhood of x 
with ∇f (x) = 0. Together with ∇2f (x) ≺ 0 (i.e. negative deﬁnite)  f is well-approximated by a
quadratic in a neighborhood of a mode x ∈ M. This is stated in the following lemma.
Lemma 5. Let f satisfy Assumption 1. Consider any x ∈ M. Then there exists a neighborhood
B(x  r)  r > 0  and constants ˆCx  ˇCx > 0 such that  for all x(cid:48) ∈ B(x  r)  we have

ˇCx (cid:107)x(cid:48) − x(cid:107)2 ≤ f (x) − f (x(cid:48)) ≤ ˆCx (cid:107)x(cid:48) − x(cid:107)2 .

(1)

We can therefore parametrize a mode x ∈ M locally as follows:
Deﬁnition 4 (Critical radius rx around mode x). For every mode x ∈ M  there exists rx > 0  such
that B(x  rx) is contained in a set Ax  satisfying the following conditions:
(i) Ax is a connected component of a level set X λ (cid:44) {x(cid:48) ∈ X : f (x(cid:48)) > λ} for some λ > 0.
(ii) ∃ ˆCx  ˇCx > 0  ∀x(cid:48) ∈ Ax  ˇCx (cid:107)x(cid:48) − x(cid:107)2 ≤ f (x) − f (x(cid:48)) ≤ ˆCx (cid:107)x(cid:48) − x(cid:107)2. (So Ax ∩ M = {x}.)

5

Return arg maxx∈X[n] fk(x).

Figure 1: Estimate the mode of a unimodal density f from X[n].

Figure 2: The analysis argues over different regions (depicted) around a mode x.

Finally  we assume that every hill in f corresponds to a mode in M:
Assumption 2. Each connected component of any level set X λ  λ > 0  contains a mode in M.

5.1 Single mode
We start with the simple but common assumption that |M| = 1. This case has been extensively
studied to get a handle on the inherent difﬁculty of mode estimation. The usual procedures in the
statistical literature are known to be minimax-optimal but are not practical: they invariably return the
maximizer of some density estimator (usually a kernel estimate) over the entire space Rd. Instead
we analyze the practical procedure of Figure 1 where we pick the maximizer of fk out of the ﬁnite
sample X[n]. The rates of Theorem 1 are optimal (O(n−1/(4+d))) for a setting of k = O(n4/(4+d)).
Theorem 1. Let δ > 0. Assume f has a single mode x0 and satisﬁes Assumptions 1  2. There exists
Nx0 δ such that the following holds for n ≥ Nx0 δ. Let ˆCx0  ˇCx0 be as in Deﬁnition 4. Suppose k
satisﬁes(cid:32)

(cid:33)4d/(4+d)

(cid:33)2

(cid:32)

(cid:115)

f (x0)(2d+4)/(4+d)(cid:16) vd

(cid:17)4/(4+d)

.

(2)

24Cδ nf (x0)

ˇCx0r2
x0

≤ k ≤

1
2

Cδ n
ˆCx0

n

4

(cid:115)

Let x be the mode returned in the procedure of Figure 1. With probability at least 1 − 2δ we have

(cid:107)x − x0(cid:107) ≤ 5

Cδ n
ˇCx0

f (x0) · 1
k1/4

.

Proof. Let rx0 be the critical radius of Deﬁnition 4. Let rn(x0) ≡ inf(cid:8)r : B(x0  r) ∩ X[n] (cid:54)= ∅(cid:9).

Let 0 < τ < 1 to be later speciﬁed  and assume the event that rn(x0) ≤ τ
2 rx0. We will bound the
probability of this event once the proper setting of τ becomes clear.
Consider ˜r satisfying rx0 ≥ ˜r ≥ 2rn(x0)/τ (see Figure 2). We will ﬁrst upper bound fk for any x
outside B(x0  ˜r)  then lower-bound fk for x ∈ B(x0  rn(x0)).
Recall Ax0 from Deﬁnition 4. By equation (1) we have

sup

x∈Ax0\B(x0 ˜r/2)

f (x) ≤ f (x0) − ˇCx0(˜r/2)2 (cid:44) ˆF .

(3)

The above allows us to apply Lemma 3 as follows. First note that for any x ∈ X\B(x0  ˜r/2)  f (x) ≤
ˆF since Ax0 is a level set of the unimodal f  i.e. supx /∈Ax0
f (x). Therefore  for
= ˆF − f (x). By equation (3) the modulus of continuity ˆr(  x) is at least
any x ∈ X \ B(x0  ˜r) let 
.

f (x) ≤ inf x∈Ax0

6

Initialize: Mn ← ∅.
For λ = maxx∈X[n] fn(x) down to 0:

√
k.

(cid:110) ˜Ai

• Let λ (cid:44) λ · Cδ n/
• Let

(cid:111)m
• Mn ← Mn ∪(cid:110)

i=1

Return the estimated modes Mn.

be the CCs of G (λ − λ − ˜) disjoint from Mn.

xi (cid:44) arg maxx∈ ˜Ai∩Xλ

[n]

fn(x)

(cid:111)m

.

i=1

Figure 3: Estimate the modes of a multimodal f from X[n]. The parameter ˜ serves to prune.

˜r/2. Therefore  if k satisﬁes

vd · (˜r/2)d ·(cid:0)f (x0) − ˇCx0 (˜r/2)2(cid:1) ≥ k

we have with probability at least 1 − δ

(cid:18)

− Cδ n

√

k
n

 

n

(cid:19)(cid:0)f (x0) − ˇCx0 (˜r/2)2(cid:1) .

sup

x∈X\B(x0 ˜r)

fk(x) <

1 + 2

Cδ n√
k

(4)

(5)

Now we turn to x ∈ B(x0  rn(x0)). We have again by equation (1) that inf x∈B(x τ ˜r) f (x) ≥
f (x0) − ˆCx0(τ ˜r)2 (cid:44) ˇF . Therefore  for x ∈ B(x0  rn(x0)) let  = f (x) − ˇF   we have ˇr(  x) ≥
τ ˜r − rn(x0) ≥ τ ˜r/2. It follows that  if k satisﬁes

(6)
we have by Lemma 4 that  with probability at least 1 − δ (under the same event used in Lemma 3)

+ Cδ n

n

 

vd · ((τ /2)˜r)d ·(cid:16)

f (x0) − ˆCx0 (τ ˜r)2(cid:17) ≥ k
(cid:18)

(cid:19)(cid:16)

f (x0) − ˆCx0 (τ ˜r)2(cid:17)

√

k
n

inf

x∈B(x rn(x0))

fk(x) ≥

1 − Cδ n√
k

.

(7)

√

(cid:17)

24f (x0)Cδ n/

(cid:16) ˇCx0

Next  with a bit of algebra  we can pick τ and ˜r so that the l.h.s. of (5) is less than the l.h.s.
of equation (7). It sufﬁces to pick τ 2 = ˇCx0/8 ˆCx0 and ˜r2 ≥ 24f (x0)Cδ n/ ˇCx0
k. Given these
settings  equations (4) and (6) are satisﬁed whenever k satisﬁes equation (2) of the lemma statement.
It follows that  with probability at least 1 − δ  inf x∈B(x rn(x0)) fk(x) > supx∈X\B(x0 ˜r) fk(x).
Therefore  the empirical mode chosen by the procedure is in B(x0  ˆr). We are free to choose ˜r as
small as max
We’ve assumed so far the event that rn(x0) ≤ τ

(cid:26)(cid:114)
follows. Let r (cid:44)(cid:113)
on k imply that r ≤ rx0  and that vd · ((τ /2)r)d ·(cid:16)

2 rx0. We bound the probability of this event as
k. Under the above setting of τ  the Theorem’s assumptions
√
n . Again 
√
by equation (1)  this implies that F(B(x0  (τ /2)r)) ≥ k
n . By Lemma  2  with probability
at least 1 − δ  Fn(B(x0  (τ /2)r)) ≥ k/n and therefore rn(x0) ≤ (τ /2)r ≤ (τ /2)rx0. It now
becomes clear that we can just pick ˜r = r.

f (x0) − ˆCx0 ((τ /2)r)2(cid:17) ≥ k

24f (x0)Cδ n/ ˇCx0

  2rn(x0)/τ

n + Cδ n

n + Cδ n

(cid:27)

√

√

k

.

k

k

5.2 Multiple modes

In this section we turn to the problem of estimating the modes of a more general density f with an
unknown number of modes.
The algorithm of Figure 3 operates on the following set of nested graphs G(λ). These are subgraphs
of a mutual k-NN graph on the sample X[n]  where vertices are connected if they are in each other’s
nearest neighbor sets. The connected components (CCs) of these graphs G(λ) are known to be good
estimates of the CCs of corresponding level sets of the unknown density f [9  26  27].

7

(cid:44)(cid:8)x ∈ X[n] : fn(x) ≥ λ(cid:9)   and where vertices x  x(cid:48) are connected by an edge when and only
Deﬁnition 5 (k-NN level set G(λ)). Given λ ∈ R  let G(λ) denote the graph with vertices in
X λ
when (cid:107)x − x(cid:48)(cid:107) ≤ α · min{rk(x)  rk(x(cid:48))}  for some α ≥ √
[n]
We will show that for a given n  any sufﬁciently salient mode is optimally recovered; furthermore 
if f is uniformly continuous on Rd  then the procedure returns no false mode above a level λn → 0.

2.

5.2.1 Optimal Recovery for Any Mode

The guarantees of this section would be given in terms of salient modes as deﬁned below. Essentially
a mode x0 is salient if it is separated from other modes by a sufﬁciently wide and deep valley.
We deﬁne saliency in a way similar to [9]  but simpler: we only require a wide valley since the
smoothness of f at the mode (as expressed in equation 1) takes care of the depth.
We start with a notion of separation between sets inspired from [26].
Deﬁnition 6 (r-separation). A  A(cid:48) ⊂ X are r-separated if there exists a (separating) set S ⊂ Rd
such that: every path from A to A(cid:48) crosses S  and supx∈S+B(0 r) f (x) < inf x∈A∪A(cid:48) f (x).
Our notion of mode saliency follows: for a mode x  we require the critical set Ax of Deﬁnition 4 to
be well separated from all components at the level where it appears.
Deﬁnition 7 (r-salient Modes). A mode x of f is said to be r-salient for r > 0 if the following
holds. There exist Ax as in Deﬁnition 4 (with the corresponding rx  ˆCx and ˇCx)  which is a CC of
say X λx (cid:44) {x ∈ X : f (x) ≥ λx}. Ax is r-separated from X λx \ Ax.
The next theorem again yields the optimal rates O(n−1/(4+d)) for k = O(n4/(4+d)).
Theorem 2 (Recovery of salient modes). Assume f satisﬁes Assumptions 1  2. Suppose ˜ =
˜(n) n→∞−−−−→ 0. Let x0 be an r-salient mode for some r > 0. Assume k = Ω
. Then
there exist N = N (x0 {˜(n)}) depending on x0 and ˜(n) such that the following holds for n ≥ N.
Let Ax0   ˆCx0  ˇCx0 be as in Deﬁnition 4  and let λx0
f (x). Let δ > 0. Suppose k further
satisﬁes(cid:32)

(cid:44) inf x∈Ax0
(cid:33)4d/(4+d)

(cid:32)

(cid:115)

C 2

δ n

(cid:17)

(cid:16)

(cid:16) vd

(cid:17)4/(4+d)

/4  (r/α)2(cid:9)(cid:33)2

ˇCx0 min(cid:8)r2

x0

24Cδ nf (x0)

λ(2d+4)/(4+d)
x0

n

4

.

Let Mn be the modes returned by the procedure of Figure 3. With probability at least 1 − 2δ  there
exists x ∈ Mn such that

≤ k ≤

Cδ n
ˆCx0

1
2

(cid:115)

(cid:107)x − x0(cid:107) ≤ 5

Cδ n
ˇCx0

f (x0) · 1
k1/4

.

5.2.2 Pruning guarantees

The proof of the main theorem of this section is based on Lemma 7.4 of [24].
Theorem 3. Let Λ (cid:44) supx f (x) and r() (cid:44) supx∈Rd max{ˆr(  x)  ˇr(  x)}. Assume f satisﬁes
Assumption 2. Suppose r(˜) = Ω (k/n)1/d  which is feasible whenever f is uniformly continuous
on Rd. In particular  if f is H¨older continuous  i.e.

∀x  x(cid:48) ∈ Rd 

|f (x) − f (x(cid:48))| ≤ L(cid:107)x − x(cid:48)(cid:107)β   for some L > 0  0 < β ≤ 1 

then we can just let ˜ = Ω (k/n)β/d since r(˜) ≥ (˜/L)1/β. Deﬁne

(cid:40)

(cid:32)

(cid:33)

√

(cid:41)

.

λ0 = max

2˜  8

Λ
k

C 2

δ n 

k
n

+ Cδ n

k
n

2

vdr(˜)d

Assume k ≥ 9C 2
let λf = inf x∈X λ
M ∩ X λf .

[n]

δ n. The following holds with probability at least 1 − δ. Pick any λ ≥ 2λ0  and
[n] can be assigned to distinct modes in

f (x). All estimated modes in Mn ∩ X λ

8

References
[1] Don O Loftsgaarden  Charles P Quesenberry  et al. A nonparametric estimate of a multivariate density

function. The Annals of Mathematical Statistics  36(3):1049–1051  1965.

[2] M. Maier  M. Hein  and U. von Luxburg. Optimal construction of k-nearest neighbor graphs for identify-

ing noisy clusters. Theoretical Computer Science  410:1749–1764  2009.

[3] Emanuel Parzen et al. On estimation of a probability density function and mode. Annals of mathematical

statistics  33(3):1065–1076  1962.

[4] Yizong Cheng. Mean shift  mode seeking  and clustering. Pattern Analysis and Machine Intelligence 

IEEE Transactions on  17(8):790–799  1995.

[5] Fr´ed´eric Chazal  Leonidas J Guibas  Steve Y Oudot  and Primoz Skraba. Persistence-based clustering in

riemannian manifolds. Journal of the ACM (JACM)  60(6):41  2013.

[6] David S Moore and James W Yackel. Large sample properties of nearest neighbor density function

estimators. Technical report  DTIC Document  1976.

[7] L.P. Devroye and T.J. Wagner. The strong uniform consistency of nearest neighbor density estimates. The

Annals of Statistics  5:536–540  1977.

[8] G´erard Biau  Fr´ed´eric Chazal  David Cohen-Steiner  Luc Devroye  Carlos Rodriguez  et al. A weighted
k-nearest neighbor density estimate for geometric inference. Electronic Journal of Statistics  5:204–237 
2011.

[9] S. Kpotufe and U. von Luxburg. Pruning nearest neighbor cluster trees. In International Conference on

Machine Learning  2011.

[10] Herman Chernoff. Estimation of the mode. Annals of the Institute of Statistical Mathematics  16(1):31–

41  1964.

[11] William F Eddy et al. Optimum kernel estimators of the mode. The Annals of Statistics  8(4):870–882 

1980.

[12] Birgit Grund  Peter Hall  et al. On the minimisation of lp error in mode estimation. The Annals of

Statistics  23(6):2264–2284  1995.

[13] Jussi Klemel¨a. Adaptive estimation of the mode of a multivariate density. Journal of Nonparametric

Statistics  17(1):83–105  2005.

[14] Aleksandr Borisovich Tsybakov. Recursive estimation of the mode of a multivariate distribution. Prob-

lemy Peredachi Informatsii  26(1):38–45  1990.

[15] David L Donoho and Richard C Liu. Geometrizing rates of convergence  iii. The Annals of Statistics 

pages 668–701  1991.

[16] Luc Devroye. Recursive estimation of the mode of a multivariate density. Canadian Journal of Statistics 

7(2):159–167  1979.

[17] Ulf Grenander et al. Some direct estimates of the mode. The Annals of Mathematical Statistics  36(1):131–

138  1965.

[18] Christophe Abraham  G´erard Biau  and Benoˆıt Cadre. On the asymptotic properties of a simple estimate

of the mode. ESAIM: Probability and Statistics  8:1–11  2004.

[19] Keinosuke Fukunaga and Larry Hostetler. The estimation of the gradient of a density function  with

applications in pattern recognition. Information Theory  IEEE Transactions on  21(1):32–40  1975.

[20] Dorin Comaniciu and Peter Meer. Mean shift: A robust approach toward feature space analysis. Pattern

Analysis and Machine Intelligence  IEEE Transactions on  24(5):603–619  2002.

[21] Jia Li  Surajit Ray  and Bruce G Lindsay. A nonparametric statistical approach to clustering via mode

identiﬁcation. Journal of Machine Learning Research  8(8)  2007.

[22] Ery Arias-Castro  David Mason  and Bruno Pelletier. On the estimation of the gradient lines of a density

and the consistency of the mean-shift algorithm. Unpublished Manuscript  2013.

[23] Christopher Genovese  Marco Perone-Paciﬁco  Isabella Verdinelli  and Larry Wasserman. Nonparametric

inference for density modes. arXiv preprint arXiv:1312.7567  2013.

[24] K. Chaudhuri  S. Dasgupta  S. Kpotufe  and U. von Luxburg. Consistent procedures for cluster tree

estimation and pruning. Arxiv  2014.

[25] O. Bousquet  S. Boucheron  and G. Lugosi. Introduction to statistical learning theory. Lecture Notes in

Artiﬁcial Intelligence  3176:169–207  2004.

[26] K. Chaudhuri and S. Dasgupta. Rates for convergence for the cluster tree. In Advances in Neural Infor-

mation Processing Systems  2010.

[27] S. Balakrishnan  S. Narayanan  A. Rinaldo  A. Singh  and L. Wasserman. Cluster trees on manifolds. In

Advances in Neural Information Processing Systems  pages 2679–2687  2013.

9

,Sanjoy Dasgupta
Samory Kpotufe
Jessa Bekker
Jesse Davis
Arthur Choi
Adnan Darwiche
Guy Van den Broeck
Hong Chen
Haifeng Xia
Heng Huang
Weidong Cai
Jen Ning Lim
Makoto Yamada
Bernhard Schölkopf
Wittawat Jitkrittum