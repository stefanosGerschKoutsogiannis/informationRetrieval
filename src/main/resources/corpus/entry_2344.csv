2017,Gradient Methods for Submodular Maximization,In this paper  we study the problem of maximizing continuous submodular functions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing  matrix approximations and network inference. Despite the apparent lack of convexity in such functions  we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically  we prove that for monotone continuous DR-submodular functions  all fixed points of projected gradient ascent provide a factor $1/2$ approximation to the global maxima. We also study stochastic gradient methods and show that after $\mathcal{O}(1/\epsilon^2)$ iterations these methods reach solutions which achieve in expectation objective values exceeding $(\frac{\text{OPT}}{2}-\epsilon)$. An immediate application of our results is to maximize submodular functions that are defined stochastically  i.e. the submodular function is defined as an expectation over a family of submodular functions with an unknown distribution. We will show how stochastic gradient methods are naturally well-suited for this setting  leading to a factor $1/2$ approximation when the function is monotone. In particular  it allows us to approximately maximize discrete  monotone submodular optimization problems via projected gradient ascent on a continuous relaxation  directly connecting the discrete and continuous domains. Finally  experiments on real data demonstrate that our projected gradient methods consistently achieve the best utility compared to other continuous baselines while remaining competitive in terms of computational effort.,Gradient Methods for Submodular Maximization

Hamed Hassani
ESE Department

Philadelphia  PA

hassani@seas.upenn.edu

University of Pennsylvania

University of Southern California

Mahdi Soltanolkotabi

EE Department

Los Angeles  CA

soltanol@usc.edu

Amin Karbasi
ECE Department
Yale University
New Haven  CT

amin.karbasi@yale.edu

Abstract

In this paper  we study the problem of maximizing continuous submodular func-
tions that naturally arise in many learning applications such as those involving
utility functions in active learning and sensing  matrix approximations and network
inference. Despite the apparent lack of convexity in such functions  we prove that
stochastic projected gradient methods can provide strong approximation guarantees
for maximizing continuous submodular functions with convex constraints. More
speciﬁcally  we prove that for monotone continuous DR-submodular functions  all

ﬁxed points of projected gradient ascent provide a factor 1~2 approximation to the
O(1~2) iterations these methods reach solutions which achieve in expectation
objective values exceeding( OPT
− ). An immediate application of our results is to
well-suited for this setting  leading to a factor 1~2 approximation when the func-

maximize submodular functions that are deﬁned stochastically  i.e. the submodular
function is deﬁned as an expectation over a family of submodular functions with an
unknown distribution. We will show how stochastic gradient methods are naturally

global maxima. We also study stochastic gradient methods and show that after

tion is monotone. In particular  it allows us to approximately maximize discrete 
monotone submodular optimization problems via projected gradient ascent on a
continuous relaxation  directly connecting the discrete and continuous domains.
Finally  experiments on real data demonstrate that our projected gradient methods
consistently achieve the best utility compared to other continuous baselines while
remaining competitive in terms of computational effort.

2

1

Introduction

Submodular set functions exhibit a natural diminishing returns property  resembling concave functions
in continuous domains. At the same time  they can be minimized exactly in polynomial time (while
can only be maximized approximately)  which makes them similar to convex functions. They have
found numerous applications in machine learning  including viral marketing [1]  dictionary learning
[2] network monitoring [3  4]  sensor placement [5]  product recommendation [6  7]  document and
corpus summarization [8] data summarization [9]  crowd teaching [10  11]  and probabilistic models
[12  13]. However  submodularity is in general a property that goes beyond set functions and can
be deﬁned for continuous functions. In this paper  we consider the following stochastic continuous
submodular optimization problem

whereK is a bounded convex body D is generally an unknown distribution  and Fθ’s are continuous
submodular functions for every θ∈D. We also use OPTá maxx∈K F(x) to denote the optimum
value. We note that the function F(x) is itself also continuous submodular  as a non-negative

combination of submodular functions are still submodular [14]. The formulation covers popular

(1.1)

max

x∈K F(x)࣊ Eθ∼D[Fθ(x)] 

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

instances of submodular optimization. For instance  whenD puts all the probability mass on a single
objective is the ﬁnite-sum continuous submodular optimization whereD is uniformly distributed over
m instances  i.e.  F(x)࣊ 1
θ=1 Fθ(x).
m∑m

function  (1.1) reduces to deterministic continuous submodular optimization. Another common

A natural approach to solving problems of the form (1.1) is to use projected stochastic methods.
As we shall see in Section 5  these local search heuristics are surprisingly effective. However  the
reasons for this empirical success is completely unclear. The main challenge is that maximizing F
corresponds to a nonconvex optimization problem (as the function F is not concave)  and a priori it is
not clear why gradient methods should yield a reliable solution. This leads us to the main challenge
of this paper

Do projected gradient methods lead to provably good solutions for continuous
submodular maximization with general convex constraints?

show that

We answer the above question in the afﬁrmative  proving that projected gradient methods produce a
competitive solution with respect to the optimum. More speciﬁcally  given a general bounded convex

bodyK and a continuous function F that is monotone  smooth  and (weakly) DR-submodular we
• All stationary points of a DR-submodular function F overK provide a 1~2 approximation
(a.k.a. gradient ﬂows) always lead to a solutions with 1~2 approximation guarantees.
 iterations produces a solution with objective value
• Projected gradient ascent after O L2
larger than(OPT~2− ). When calculating the gradient is difﬁcult but an unbiased estimate
2 iterations
+ σ2
can be easily obtained  the stochastic projected gradient ascent in O L2
ﬁnds a solution with objective value exceeding(OPT~2− ). Here  L2 is the smoothness

to the global maximum. Thus  projected gradient methods with sufﬁciently small step sizes





of the continuous submodular function measured in the (cid:96)2-norm  σ2 is the variance of the
stochastic gradient with respect to the true gradient and OPT is the function value at the
global optimum.

in (2.6)) we prove the above results with γ2~(1+ γ2) approximation guarantee.

• More generally  for weakly continuous DR-submodular functions with parameter γ (deﬁne

Our result have some important implications. First  they show that projected gradient methods are an
efﬁcient way of maximizing the multilinear extension of (weakly) submodular set functions for any

submodularity ratio γ (note that γ= 1 corresponds to submodular functions) [2]. Second  in contrast
origin [15  16]  projected gradient methods can start from any initial point in the constraint setK

to conditional gradient methods for submodular maximization that require the initial point to be the

provides a unifying approach for solving the stochastic submodular maximization problem [18]

and still produce a competitive solution. Third  such conditional gradient methods  when applied
to the stochastic setting (with a ﬁxed batch size)  perform poorly and can produce arbitrarily bad
solutions when applied to continuous submodular functions (see [17  Appendix B] in the long version
of this paper for an example and further discussion on why conditional gradient methods do not easily
admit stochastic variants). In contrast  stochastic projected gradient methods are stable by design

and provide a solution with an approximation ratio of at least 1~2 in expectation. Finally  our work
where the functions fθ∶ 2V → R+ are submodular set functions deﬁned over the ground set V . Such
recently introduced and studied in [18]. SinceD is unknown  problem (1.2) cannot be directly solved.
relaxation to reach a solution that is within a factor(1− 1~e) of the optimum. In contrast  our work
provides a general recipe with 1~2 approximation guarantee for problem (1.2) in which fθ’s can be

Instead  [18] showed that in the case of coverage functions  it is possible to efﬁciently maximize f by
lifting the problem to the continuous domain and using stochastic gradient methods on a continuous

objective functions naturally arise in many data summarization applications [19] and have been

f(S)࣊ Eθ∼D[fθ(S)] 

(1.2)

any monotone submodular function.

2

(2.1)

2 Continuous Submodular Maximization

Even though submodularity is mostly considered on discrete domains  the notion can be naturally

F(x)+ F(y)≥ F(x∨ y)+ F(x∧ y) 

f(A)+ f(B)≥ f(A∩ B)+ f(A∪ B).

A set function f ∶ 2V → R+  deﬁned on the ground set V   is called submodular if for all subsets
A  B⊆ V   we have
extended to arbitrary lattices [20]. To this aim  let us consider a subset of Rn+ of the formX=∏n
i=1Xi
where eachXi is a compact subset of R+. A function F ∶X → R+ is submodular [21] if for all
(x  y)∈X×X   we have
where x∨y࣊ max(x  y) (component-wise) and x∧y࣊ min(x  y) (component-wise). A submodular
function is monotone if for any x  y∈X obeying x≤ y  we have F(x)≤ F(y) (here  by x≤ y we
if for any x∈X   any two distinct basis vectors ei  ej∈ Rn  and any two non-negative real numbers
zi  zj∈ R+ obeying xi+ zi∈Xi and xj+ zj∈Xj we have
Clearly  the above deﬁnition includes submodularity over a set (by restrictingXi’s to{0  1}) or over
an integer lattice (by restrictingXi’s to Z+) as special cases. However  in the remainder of this paper
we consider continuous submodular functions deﬁned on product of sub-intervals of R+. We note that

F(x+ ziei)+ F(x+ zjej)≥ F(x)+ F(x+ ziei+ zjej).

mean that every element of x is less than that of y). Like set functions  we can deﬁne submodularity in
an equivalent way  reminiscent of diminishing returns  as follows [14]: the function F is submodular

functions that are both submodular and convex/concave. For instance  for a concave function g

The above expression makes it clear that continuous submodular functions are not convex nor concave

when twice differentiable  F is submodular if and only if all cross-second-derivatives are non-positive
[14]  i.e. 

∂2F(x)
≤ 0.
in general as concavity (convexity) implies that∇2F  0 (resp.▽2F  0). Indeed  we can have
i=1 λixi) is submodular and concave.
and non-negative weights λi≥ 0  the function F(x)= g(∑n
functions are called DR-submodular [16  22] if for any x  y∈X obeying x≤ y  any standard basis
vector ei∈ Rn  and any non-negative number z∈ R+ obeying zei+ x∈X and zei+ y∈X   we have
mapping  i.e.  for all x  y ∈ X such that x ≤ y we have∇F(x) ≥ ∇F(y) [16]. When twice

(2.4)
One can easily verify that for a differentiable DR-submodular functions the gradient is an antitone

F(zei+ x)− F(x)≥ F(zei+ y)− F(y).

Trivially  afﬁne functions are submodular  concave  and convex. A proper subclass of submodular

∀i≠ j ∀x∈X  

∂xi∂xj

(2.2)

(2.3)

∂2F(x)

(2.5)

∂xi∂xj

differentiable  DR-submodularity is equivalent to

The above twice differentiable functions are sometimes called smooth submodular functions in the
literature [23]. However  in this paper  we say a differentiable submodular function F is L-smooth

∀i & j ∀x∈X  
≤ 0.
w.r.t a norm⋅ (and its dual norm⋅∗) if for all x  y∈X we have
∇F(x)−∇F(x)∗≤ Lx− y.
Here ⋅∗ is the dual norm of⋅ deﬁned asg∗= supx∈Rn∶x≤1 gT x. When the function is
i∈[n][∇F(x)]i
γ= inf
[∇F(y)]i
x y∈X
x≤y
(1− xj)f(S).
F(x)= Q
xiM
M
S⊆V
i∈S
j~∈S

See [24] for related deﬁnitions. Clearly  for a differentiable DR-submodular function we have γ= 1.
An important example of a DR-submodular function is the multilinear extension [15] F∶[0  1]n→ R

smooth w.r.t the (cid:96)2-norm we use L2 (note that the (cid:96)2 norm is self-dual). We say that a function is
weakly DR-submodular with parameter γ if

of a discrete submodular function f  namely 

(2.6)

inf

.

3

We note that for set functions  DR-submodularity (i.e.  Eq. 2.4) and submodularity (i.e.  Eq. 2.1) are
equivalent. However  this is not true for the general submodular functions deﬁned on integer lattices
or product of sub-intervals [16  22].
The focus of this paper is on continuous submodular maximization deﬁned in Problem (1.1). More

speciﬁcally  we assume thatK⊂X is a a general bounded convex set (not necessarily down-closed

as considered in [16]) with diameter R. Moreover  we consider Fθ’s to be monotone (weakly)
DR-submodular functions with parameter γ.

3 Background and Related Work

Submodular set functions [25  20] originated in combinatorial optimization and operations research 
but they have recently attracted signiﬁcant interest in machine learning. Even though they are usually
considered over discrete domains  their optimization is inherently related to continuous optimization
methods. In particular  Lovasz [26] showed that the Lovasz extension is convex if and only if
the corresponding set function is submodular. Moreover  minimizing a submodular set-function is
equivalent to minimizing the Lovasz extension.1 This idea has been recently extended to minimization
of strict continuous submodular functions (i.e.  cross-order derivatives in (2.3) are strictly negative)
[14]. Similarly  approximate submodular maximization is linked to a different continuous extension
known as multilinear extension [28]. Multilinear extension (which is an example of DR-submodular
functions studied in this paper) is not concave nor convex in general. However  a variant of conditional
gradient method  called continuous greedy  can be used to approximately maximize them. Recently 
Chekuri et al [23] proposed an interesting multiplicative weight update algorithm that achieves

(1− 1~e− ) approximation guarantee after ˜O(n2~2) steps for twice differentiable monotone DR-
continuous greedy algorithm  achieves(1− 1~e− ) approximation guarantee after O(L2~) iterations

submodular functions (they are also called smooth submodular functions) subject to a polytope
constraint. Similarly  Bian et al [16] proved that a conditional gradient method  similar to the

for maximizing a monotone DR-submodular functions subject to special convex constraints called
down-closed convex bodies. A few remarks are in order. First  the proposed conditional gradient
methods cannot handle the general stochastic setting we consider in Problem (1.1) (in fact  projection
is the key). Second  there is no near-optimality guarantee if conditional gradient methods do not start
from the origin. More precisely  for the continuous greedy algorithm it is necessary to start from
the 0 vector (to be able to remain in the convex constraint set at each iteration). Furthermore  the
0 vector must be a feasible point of the constraint set. Otherwise  the iterates of the algorithm may
fall out of the convex constraint set leading to an infeasible ﬁnal solution. Third  due to the starting
point requirement  they can only handle special convex constraints  called down-closed. And ﬁnally 
the dependency on L2 is very subomptimal as it can be as large as the dimension n (e.g.  for the
multilinear extensions of some submodular set functions  see [17  Appendix B] in the long version of
this paper). Our work resolves all of these issues by showing that projected gradient methods can also
approximately maximize monotone DR-submodular functions subject to general convex constraints 

albeit  with a lower 1~2 approximation guarantee.

Generalization of submodular set functions has lately received a lot of attention. For instance  a line
of recent work considered DR-submodular function maximization over an integer lattice [29  30  22].
Interestingly  Ene and Nguyen [31] provided an efﬁcient reduction from an integer-lattice DR-
submodular to a submodular set function  thus suggesting a simple way to solve integer-lattice
DR-submodular maximization. Note that such reductions cannot be applied to the optimization
problem (1.1) as expressing general convex body constraints may require solving a continuous
optimization problem.

4 Algorithms and Main Results

In this section we discuss our algorithms together with the corresponding theoretical guarantees. In
what follows  we assume that F is a weakly DR-submodular function with parameter γ.

1The idea of using stochastic methods for submodular minimization has recently been used in [27].

4

4.1 Characterizing the quality of stationary points

We begin with the deﬁnition of a stationary point.

Deﬁnition 4.1 A vector x∈K is called a stationary point of a function F ∶X → R+ over the set
K⊂X if maxy∈K∇F(x)  y− x≤ 0.

Stationary points are of interest because they characterize the ﬁxed points of the Gradient Ascent
(GA) method. Furthermore  (projected) gradient ascent with a sufﬁciently small step size is known
to converge to a stationary point for smooth functions [32]. To gain some intuition regarding this
connection  let us consider the GA procedure. Roughly speaking  at any iteration t of the GA

procedure  the value of F increases (to the ﬁrst order) by∇F(xt)  xt+1− xt. Hence  the progress
at time t is at most maxy∈K∇F(xt)  y−xt. If at any time t we have maxy∈K∇F(xt)  y−xt≤ 0 

then the GA procedure will not make any progress and it will be stuck once it falls into a stationary
point.
The next natural question is how small can the value of F be at a stationary point compared to the
global maximum? The following lemma relates the value of F at a stationary point to OPT.

Theorem 4.2 Let F ∶X → R+ be monotone and weakly DR-submodular with parameter γ and
assumeK⊆X is a convex set. Then 
(i) If x is a stationary point of F inK  then F(x)≥ γ2
(ii) Furthermore  if F is L-smooth  gradient ascent with a step size smaller than 1~L will
particular case of γ= 1  i.e.  when F is DR-submodular  asserts that at any stationary point F is at
1+γ2 approximation ratio. The
least OPT~2. This lower bound is in fact tight. In the long version of this paper (in particular [17 
OPT~2 at a stationary point that is also a local maximum.
γ= 1 follows directly from [28  Lemma 3.2]. See the long version of this paper [17  Section 7] for

The theorem above guarantees that all ﬁxed points of the GA method yield a solution whose function
value is at least γ2

We would like to note that our result on the quality of stationary points (i.e.  ﬁrst part of Theorem 4.2
above) can be viewed as a simple extension of the results in [33]. In particular  the special case of

1+γ2 OPT. Thus  all ﬁxed point of GA provide a factor γ2

Appendix A]) we provide a simple instance of a differentiable DR-Submodular function that attains

converge to a stationary point.

1+γ2 OPT.

how this lemma is used in our proofs. However  we note that the main focus of this paper is whether
such a stationary point can be found efﬁciently using stochastic schemes that do not require exact
evaluations of gradients. This is the subject of the next section.

4.2

(Stochastic) gradient methods

constraints  GA iteratively applies the following update

We now discuss our ﬁrst algorithmic approach. For simplicity we focus our exposition on the DR

of this paper ([17  Section 7]). A simple approach to maximizing DR submodular functions is to use

xt+1=PK(xt+ µt∇F(xt)) .

submodular case  i.e.  γ= 1  and discuss how this extends to the more general case in the long version
the (projected) Gradient Ascent (GA) method. Starting from an initial estimate x1∈K obeying the
Here  µt is the learning rate andPK(v) denotes the Euclidean projection of v onto the setK.
leads to the Stochastic Gradient Method (SGM). Starting from an initial estimate x0∈K obeying the
unbiased estimate of the gradient∇F(xt) and µt is the learning rate. The result is then projected onto
the setK. We note that when gt=∇F(xt)  i.e.  when there is no randomness in the updates  then

(4.2)
Speciﬁcally  at every iteration t  the current iterate xt is updated by adding µtgt  where gt is an

However  in many problems of practical interest we do not have direct access to the gradient of F . In
these cases it is natural to use a stochastic estimate of the gradient in lieu of the actual gradient. This

xt+1=PK(xt+ µtgt) .

constraints  SGM iteratively applies the following updates

(4.1)

5

Algorithm 1 (Stochastic) Gradient Method for Maximizing F(x) over a convex setK
Parameters: Integer T> 0 and scalars ηt> 0  t∈[T]
Initialize: x1∈K
for t= 1 to T do
yt+1← xt+ ηtgt 
where gt is a random vector s.t. E[gtxt]=∇F(xt)
xt+1= arg minx∈Kx− yt+12
Pick τ uniformly at random from{1  2  . . .   T}.

end for

Output xτ

the SGM updates (4.2) reduce to the GA updates (4.1). We detail the SGM method in Algorithm 1.
As we shall see in our experiments detained in Section 5  the SGM method is surprisingly effective
for maximizing monotone DR-submodular functions. However  the reasons for this empirical success
was previously unclear. The main challenge is that maximizing F corresponds to a nonconvex
optimization problem (as the function F is not concave)  and a priori it is not clear why gradient
methods should yield a competitive ratio. Thus  studying gradient methods for such nonconvex
problems poses new challenges:

Do (stochastic) gradient methods converge to a stationary point?

oracle gt obeying

(4.3)

2

(cid:96)2

(cid:96)2

1

R

t

(cid:6)≤ σ2.

the elements in place to state our ﬁrst theorem.

≤ Lx− y(cid:96)2

Theorem 4.3 (Stochastic Gradient Method) Let us assume that F is L-smooth w.r.t. the Euclidean
  monotone and DR-submodular. Furthermore  assume that we have access to a stochastic

The next theorem addresses some of these challenges. To be able to state this theorem let us recall the
standard deﬁnition of smoothness. We say that a continuously differentiable function F is L-smooth
.
. We now have all

(in Euclidean norm) if the gradient∇F is L-Lipschitz  that is∇F(x)−∇F(y)(cid:96)2
We also deﬁned the diameter (in Euclidean norm) as R2= supx y∈K 1
x− y2
norm⋅(cid:96)2
and Egt−∇F(xt)2
E[gt]=∇F(xt)
We run stochastic gradient updates of the form (4.2) with µt=
√
taking values in{1  2  . . .   T} with equal probability. Then 
L+ σ
− R2L+ OPT
+ Rσ√
E[F(xτ)]≥ OPT
 .
1(T−1) and 1 and T each with probability
E[F(xτ)]≥ OPT
− R2L
 .
+ R2σ2
2  iterations of the stochastic gradient method
The above results roughly state that T=O R2L
− . Stated differently 
T =O R2L
+ R2σ2
2  iterations of the stochastic gradient method provides in expectation a value
−  approximation ratio for DR-submodular maximization. As explained in Section
4.1  it is not possible to go beyond the factor 1~2 approximation ratio using gradient ascent from an

Remark 4.4 We would like to note that if we pick τ to be a random variable taking values in

{2  . . .   T− 1} with probability

from any initial point  yields a solution whose objective value is at least OPT
2

. Let τ be a random variable

2(T−1) then

1

+ Rσ√

T



that exceeds OPT
2

2

2T

T

2

2T



arbitrary initialization.
An important aspect of the above result is that it only requires an unbiased estimate of the gradient.
This ﬂexibility is crucial for many DR-submodular maximization problems (see  (1.1)) as in many
cases calculating the function F and its derivative is not feasible. However  it is possible to provide a
good un-biased estimator for these quantities.

6

We would like to point out that our results are similar in nature to known results about stochastic
for stochastic

methods for convex optimization. Indeed  this result interpolates between the 1√
smooth optimization  and the 1~T for deterministic smooth optimization. The special case of σ= 0
assumptions of Theorem 4.3  it is possible to show that F(xT)≥ OPT
randomized choice of τ∈[T].
Finally  we would like to note that while the ﬁrst term in (4.3) decreases as 1~T   the pre-factor L

which corresponds to Gradient Ascent deserves particular attention. In this case  and under the
T   without the need for a

− R2L

could be rather large in many applications. For instance  this quantity may depend on the dimension
of the input n (see [17  Appendix C] in the extended version of this paper). Thus  the number of
iterations for reaching a desirable accuracy may be very large. Such a large computational load causes
(stochastic) gradient methods infeasible in some application domains. It is possible to overcome this
deﬁciency by using stochastic mirror methods (see [17  Section 4.3] in the extended version of this
paper).

T

2

5 Experiments

F(x).

S∶S≤k

max

same optimal value [15]:

f(S)= max
x∈Pk

In our experiments  we consider a movie recommendation application [19] consisting of N users and
n movies. Each user i has a user-speciﬁc utility function fi for evaluating sets of movies. The goal is
to ﬁnd a set of k movies such that in expectation over users’ preferences it provides the highest utility 

maximization problem deﬁned in (1.2). We consider a setting that consists of N users and consider
the empirical objective function 1
uniform on the integers between 1 and N. We can then run the (discrete) greedy algorithm on the
empirical objective function to ﬁnd a good set of size k. However  as N is a large number  the greedy
algorithm will require a high computational complexity. Another way of solving this problem is
to evaluate the multilinear extension Fi of any sampled function fi and solve the problem in the

i.e.  maxS≤k f(S)  where f(S)࣊ Ei∼D[fi(S)]. This is an instance of the stochastic submodular
j=1 fi. In other words  the distributionD is assumed to be
N∑N
continuous domain as follows. Let F(x)= Ei∼D[Fi(x)] for x∈[0  1]n and deﬁne the constraint set
i=1 xi≤ k}. The discrete and continuous optimization formulations lead to the
Pk={x∈[0  1]m∶∑n
in the continuous domain that is at least 1~2 approximation to the optimal value. By rounding that
at least 1~2 of the optimum solution set of size k. We note that randomized Pipage rounding does not
need access to the value of f. We also remark that projection ontoPk can be done very efﬁciently in
O(n) time (see [18  34  35]). Therefore  such an approach easily scales to big data scenarios where
(i) Stochastic Gradient Ascent (SG): We use the step size µt= c~√
B to denote mini-batch sizes (we further let α= 1  δ= 0  see Algorithm 1 in [16] for more

the size of the data set (e.g. number of users) or the number of items n (e.g. number of movies) are
very large.
In our experiments  we consider the following baselines:

t and mini-batch size B.
The details for computing an unbiased estimator for the gradient of F are given in [17 
Appendix D] of the extended version of this paper.

fractional solution (for instance via randomized Pipage rounding [15]) we obtain a set whose utility is

Therefore  by running the stochastic versions of projected gradient methods  we can ﬁnd a solution

(ii) Frank-Wolfe (FW) variant of [16]: We use T to denote the total number of iterations and

details).

(iii) Batch-mode Greedy (Greedy): We run the vanilla greedy algorithm (in the discrete domain)
in the following way. At each round of the algorithm (for selecting a new element)  B
random users are picked and the function f is estimated by the average over the B selected
users.

N= 6041 users for n= 4000 movies. Let ri j denote the rating of user i for movie j (if such a rating

To run the experiments we use the MovieLens data set. It consists of 1 million ratings (from 1 to 5) by

does not exist we assign ri j to 0). In our experiments  we consider two well motivated objective
functions. The ﬁrst one is called “facility location" where the valuation function by user i is deﬁned

7

(a) Concave Over Modular

(b) Concave Over Modular

(c) Facility Location

(d) Facility Location

Figure 1: (a) shows the performance of the algorithms w.r.t. the cardinality constraint k for the
concave over modular objective. Each of the continuous algorithms (i.e.  SG and FW) run for

T= 2000 iterations. (b) shows the performance of the SG algorithm versus the number of iterations
for ﬁxed k= 20 for the concave over modular objective. The green dashed line indicates the value
obtained by Greedy (with B = 1000). Recall that the step size of SG is c~√
function. Each of the continuous algorithms (SG and FW) run for T = 2000 iterations. (d) shows

t. (c) shows the
performance of the algorithms w.r.t. the cardinality constraint k for the facility location objective

the performance of different algorithms versus the number of simple function computations (i.e. the
number of fi’s evaluated during the algorithm) for the facility location objective function. For the
greedy algorithm  larger number of function computations corresponds to a larger batch size. For SG
larger time corresponds to larger iterations.

as f(S  i)= maxj∈S ri j. In words  the way user i evaluates a set S is by picking the highest rated

movie in S. Thus  the objective function is equal to

function composed with a modular function  i.e.  f(S  i)=(∑j∈S ri j)1~2. Again  by considering the

In our second experiment  we consider a different user-speciﬁc valuation function which is a concave

uniform distribution over the set of users  we obtain

ffac(S)= 1

N

NQ
i=1

j∈S

max

ri j.

fcon(S)= 1

N

NQ
i=1

Q
j∈S

ri j1~2

.

Note that the multilinear extensions of f1 and f2 are neither concave nor convex.
Figure 1 depicts the performance of different algorithms for the two proposed objective functions. As
Figures 1a and 1c show  the FW algorithm needs a much higher mini-batch size to be comparable

8

1020304050k468SG(B=20)Greedy(B=100)Greedy(B=1000)FW(B=20)FW(B=100)objective value02004006008001000T(numberofiterations)4.85.05.25.45.6SG(B=20 c=1)GreedySG(B=20 c=10)objective value1020304050k3.754.004.254.504.755.00SG(B=20)Greedy(B=100)Greedy(B=1000)FW(B=20)FW(B=100)objective value0.00.20.40.60.8numberoffunctioncomputations⇥1084.654.704.754.80SG(B=20 c=1)SG(B=20 c=2)Greedyobjective valuein performance to our stochastic gradient methods. Note that a smaller batch size leads to less
computational effort (using the same value for B and T   the computational complexity of FW and

SGA is almost the same). Figure 1b shows that after a few hundred iterations SG with B= 20 obtains
almost the same utility as the Greedy method with a large batch size (B= 1000). Finally  Figure 1d

shows the performance of the algorithms with respect to the number of times the single functions
(fi’s) are evaluated. This further shows that gradient based methods have comparable complexity
w.r.t. the Greedy algorithm in the discrete domain.

6 Conclusion

In this paper we studied gradient methods for submodular maximization. Despite the lack of
convexity of the objective function we demonstrated that local search heuristics are effective at
ﬁnding approximately optimal solutions. In particular  we showed that all ﬁxed point of projected

gradient ascent provide a factor 1~2 approximation to the global maxima. We also demonstrated that
2) iterations.
stochastic gradient and mirror methods achieve an objective value of OPT~2−   inO( 1

We further demonstrated the effectiveness of our methods with experiments on real data.
While in this paper we have focused on convex constraints  our framework may allow non-convex
constraints as well. For instance it may be possible to combine our framework with recent results in
[36  37  38] to deal with general nonconvex constraints. Furthermore  in some cases projection onto
the constraint set may be computationally intensive or even intractable but calculating an approximate
projection may be possible with signiﬁcantly less effort. One of the advantages of gradient descent-
based proofs is that they continue to work even when some perturbations are introduced in the updates.
Therefore  we believe that our framework can deal with approximate projections and we hope to
pursue this in future work.

Acknowledgments

This work was done while the authors were visiting the Simon’s Institute for the Theory of Computing.
A. K. is supported by DARPA YFA D16AP00046. The authors would like to thank Jeff Bilmes  Volkan
Cevher  Chandra Chekuri  Maryam Fazel  Stefanie Jegelka  Mohammad-Reza Karimi  Andreas
Krause  Mario Lucic  and Andrea Montanari for helpful discussions.

9

References

[1] D. Kempe  J. Kleinberg  and E. Tardos. Maximizing the spread of inﬂuence through a social network. In

KDD  2003.

[2] A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection  sparse

approximation and dictionary selection. ICML  2011.

[3] J. Leskovec  A. Krause  C. Guestrin  C. Faloutsos  J. Van Briesen  and N. Glance. Cost-effective outbreak

detection in networks. In KDD  2007.

[4] R. M. Gomez  J. Leskovec  and A. Krause. Inferring networks of diffusion and inﬂuence. In Proceedings

of KDD  2010.

[5] C. Guestrin  A. Krause  and A. P. Singh. Near-optimal sensor placements in gaussian processes. In ICML 

2005.

[6] K. El-Arini  G. Veda  D. Shahaf  and C. Guestrin. Turning down the noise in the blogosphere. In KDD 

2009.

[7] B. Mirzasoleiman  A. Badanidiyuru  and A. Karbasi. Fast constrained submodular maximization: Person-

alized data summarization. In ICML  2016.

[8] H. Lin and J. Bilmes. A class of submodular functions for document summarization. In Proceedings of
Annual Meeting of the Association for Computational Linguistics: Human Language Technologies  2011.

[9] B. Mirzasoleiman  A. Karbasi  R. Sarkar  and A. Krause. Distributed submodular maximization: Identifying

representative elements in massive data. In NIPS  2013.

[10] A. Singla  I. Bogunovic  G. Bartok  A. Karbasi  and A. Krause. Near-optimally teaching the crowd to

classify. In ICML  2014.

[11] B. Kim  O. Koyejo  and R. Khanna. Examples are not enough  learn to criticize! criticism for interpretability.

In NIPS  2016.

[12] J. Djolonga and A. Krause. From map to marginals: Variational inference in bayesian submodular models.

In NIPS  2014.

[13] R. Iyer and J. Bilmes. Submodular point processes with applications to machine learning. In Artiﬁcial

Intelligence and Statistics  2015.

[14] F. Bach. Submodular functions: from discrete to continous domains. arXiv preprint arXiv:1511.00394 

2015.

[15] G. Calinescu  C. Chekuri  M. Pal  and J. Vondrak. Maximizing a submodular set function subject to a

matroid constraint. SIAM Journal on Computing  2011.

[16] A. Bian  B. Mirzasoleiman  J. M. Buhmann  and A. Krause. Guaranteed non-convex optimization:

Submodular maximization over continuous domains. arXiv preprint arXiv:1606.05615  2016.

[17] H. Hassani  M. Soltanolkotabi  and A. Karbasi. Gradient methods for submodular maximization. arXiv

preprint arXiv:1708.03949  2017.

[18] M. Karimi  M. Lucic  H. Hassani  and A. Krasue. stochastic submodular maximization: The case for

coverage functions. 2017.

[19] S. A. Stan  M. Zadimoghaddam  A. Krasue  and A. Karbasi. Probabilistic submodular maximization in

sub-linear time. ICML  2017.

[20] S. Fujishige. Submodular functions and optimization  volume 58. Annals of Discrete Mathematics  North

Holland  Amsterdam  2nd edition  2005.

[21] L. A. Wolsey. An analysis of the greedy algorithm for the submodular set covering problem. Combinatorica 

2(4):385–393  1982.

[22] T. Soma and Y. Yoshida. A generalization of submodular cover via the diminishing return property on the

integer lattice. In NIPS  2015.

10

[23] C. Chekuri  T. S. Jayram  and J. Vondrak. On multiplicative weight updates for concave and submodular
function maximization. In Proceedings of the 2015 Conference on Innovations in Theoretical Computer
Science  pages 201–210. ACM  2015.

[24] R. Eghbali and M. Fazel. Designing smoothing functions for improved worst-case competitive ratio in

online optimization. In Advances in Neural Information Processing Systems  pages 3287–3295  2016.

[25] J. Edmonds. Matroids and the greedy algorithm. Mathematical programming  1(1):127–136  1971.

[26] László Lovász. Submodular functions and convexity. In Mathematical Programming The State of the Art 

pages 235–257. Springer  1983.

[27] D. Chakrabarty  Y. T. Lee  Sidford A.  and S. C. W. Wong. Subquadratic submodular function minimization.

In STOC  2017.

[28] C. Chekuri  J. Vondrák  and R.s Zenklusen. Submodular function maximization via the multilinear
relaxation and contention resolution schemes. In Proceedings of the 43rd ACM Symposium on Theory of
Computing (STOC)  2011.

[29] T. Soma  N. Kakimura  K. Inaba  and K. Kawarabayashi. Optimal budget allocation: Theoretical guarantee

and efﬁcient algorithm. In ICML  2014.

[30] C. Gottschalk and B. Peis. Submodular function maximization on the bounded integer lattice.

International Workshop on Approximation and Online Algorithms  2015.

In

[31] A. Ene and H. L. Nguyen. A reduction for optimizing lattice submodular functions with diminishing

returns. arXiv preprint arXiv:1606.08362  2016.

[32] Yurii Nesterov. Introductory lectures on convex optimization: A basic course  volume 87. Springer Science

& Business Media  2013.

[33] J. Vondrak  C. Chekuri  and R. Zenklusen. Submodular function maximization via the multilinear relaxation
and contention resolution schemes. In Proceedings of the forty-third annual ACM symposium on Theory of
computing  pages 783–792. ACM  2011.

[34] P. Brucker. AnO(n) algorithm for quadratic knapsack problems. Operations Research Letters  3(3):163–

166  1984.

[35] P. M. Pardalos and N. Kovoor. An algorithm for a singly constrained class of quadratic programs subject to

upper and lower bounds. Mathematical Programming  46(1):321–328  1990.

[36] S. Oymak  B. Recht  and M. Soltanolkotabi. Sharp time–data tradeoffs for linear inverse problems. arXiv

preprint arXiv:1507.04793  2015.

[37] M. Soltanolkotabi. Structured signal recovery from quadratic measurements: Breaking sample complexity

barriers via nonconvex optimization. arXiv preprint arXiv:1702.06175  2017.

[38] M. Soltanolkotabi. Learning ReLUs via gradient descent. arXiv preprint arXiv:1705.04591  2017.

11

,Hamed Hassani
Mahdi Soltanolkotabi
Amin Karbasi
Ho Chung Law
Dino Sejdinovic
Tim Lucas
Seth Flaxman
Katherine Battle
Kenji Fukumizu