2016,Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent,Matrix completion  where we wish to recover a low rank matrix by observing a few entries from it  is a widely studied problem in both theory and practice with wide applications. Most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously. However  in many applications  the online version  where we observe one entry at a time and dynamically update our estimate  is more appealing. While existing algorithms are efficient for the offline setting  they could be highly inefficient for the online setting.  In this paper  we propose the first provable  efficient online algorithm for matrix completion. Our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (SGD). After every observation  it performs a fast update involving only one row of two tall matrices  giving near linear total runtime. Our algorithm can be naturally used in the offline setting as well  where it gives competitive sample complexity and runtime to state of the art algorithms. Our proofs introduce a general framework to show that SGD updates tend to stay away from saddle surfaces and could be of broader interests to other non-convex problems.,Provable Efﬁcient Online Matrix Completion via

Non-convex Stochastic Gradient Descent

Chi Jin

UC Berkeley

chijin@cs.berkeley.edu

Sham M. Kakade

University of Washington
sham@cs.washington.edu

Praneeth Netrapalli

Microsoft Research India
praneeth@microsoft.com

Abstract

Matrix completion  where we wish to recover a low rank matrix by observing a
few entries from it  is a widely studied problem in both theory and practice with
wide applications. Most of the provable algorithms so far on this problem have
been restricted to the ofﬂine setting where they provide an estimate of the unknown
matrix using all observations simultaneously. However  in many applications  the
online version  where we observe one entry at a time and dynamically update our
estimate  is more appealing. While existing algorithms are efﬁcient for the ofﬂine
setting  they could be highly inefﬁcient for the online setting.
In this paper  we propose the ﬁrst provable  efﬁcient online algorithm for matrix
completion. Our algorithm starts from an initial estimate of the matrix and then
performs non-convex stochastic gradient descent (SGD). After every observation 
it performs a fast update involving only one row of two tall matrices  giving near
linear total runtime. Our algorithm can be naturally used in the ofﬂine setting as
well  where it gives competitive sample complexity and runtime to state of the art
algorithms. Our proofs introduce a general framework to show that SGD updates
tend to stay away from saddle surfaces and could be of broader interests to other
non-convex problems.

1

Introduction

Low rank matrix completion refers to the problem of recovering a low rank matrix by observing the
values of only a tiny fraction of its entries. This problem arises in several applications such as video
denoising [13]  phase retrieval [3] and most famously in movie recommendation engines [15]. In the
context of recommendation engines for instance  the matrix we wish to recover would be user-item
rating matrix where each row corresponds to a user and each column corresponds to an item. Each
entry of the matrix is the rating given by a user to an item. Low rank assumption on the matrix is
inspired by the intuition that rating of an item by a user depends on only a few hidden factors  which
are much fewer than the number of users or items. The goal is to estimate the ratings of all items by
users given only partial ratings of items by users  which would then be helpful in recommending new
items to users.
The seminal works of Candès and Recht [4] ﬁrst identiﬁed regularity conditions under which low
rank matrix completion can be solved in polynomial time using convex relaxation – low rank matrix
completion could be ill-posed and NP-hard in general without such regularity assumptions [9].
Since then  a number of works have studied various algorithms under different settings for matrix
completion: weighted and noisy matrix completion  fast convex solvers  fast iterative non-convex
solvers  parallel and distributed algorithms and so on.
Most of this work however deals only with the ofﬂine setting where all the observed entries are
revealed at once and the recovery procedure does computation using all these observations simultane-
ously. However in several applications [5  18]  we encounter the online setting where observations are

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

only revealed sequentially and at each step the recovery algorithm is required to maintain an estimate
of the low rank matrix based on the observations so far. Consider for instance recommendation
engines  where the low rank matrix we are interested in is the user-item rating matrix. While we make
an observation only when a user rates an item  at any point of time  we should have an estimate of the
user-item rating matrix based on all prior observations so as to be able to continuously recommend
items to users. Moreover  this estimate should get better as we observe more ratings.
Algorithms for ofﬂine matrix completion can be used to solve the online version by rerunning the
algorithm after every additional observation. However  performing so much computation for every
observation seems wasteful and is also impractical. For instance  using alternating minimization 
which is among the fastest known algorithms for the ofﬂine problem  would mean that we take several
passes of the entire data for every additional observation. This is simply not feasible in most settings.
Another natural approach is to group observations into batches and do an update only once for each
batch. This however induces a lag between observations and estimates which is undesirable. To the
best of our knowledge  there is no known provable  efﬁcient  online algorithm for matrix completion.
On the other hand  in order to deal with the online matrix completion scenario in practical applications 
several heuristics (with no convergence guarantees) have been proposed in literature [2  19]. Most
of these approaches are based on starting with an estimate of the matrix and doing fast updates of
this estimate whenever a new observation is presented. One of the update procedures used in this
context is that of stochastic gradient descent (SGD) applied to the following non-convex optimization
problem

(cid:107)M − UV(cid:62)(cid:107)2

s.t. U ∈ Rd1×k  V ∈ Rd2×k 

min
U V

(1)
where M is the unknown matrix of size d1 × d2  k is the rank of M and UV(cid:62) is a low rank
factorization of M we wish to obtain. The algorithm starts with some U0 and V0  and given a
new observation (M)ij  SGD updates the ith-row and the jth-row of the current iterates Ut and Vt
respectively by

F

U(i)

t+1 = U(i)
t+1 = V(j)

t − 2ηd1d2
t − 2ηd1d2

V(j)

(cid:0)UtV(cid:62)
(cid:0)UtV(cid:62)

t − M(cid:1)
t − M(cid:1)

t

ij V(j)
ij U(i)

t

  and 

 

(2)

where η is an appropriately chosen stepsize  and U(i) denote the ith row of matrix U. Note that each
update modiﬁes only one row of the factor matrices U and V  and the computation only involves one
row of U  V and the new observed entry (M)ij and hence are extremely fast. These fast updates
make SGD extremely appealing in practice. Moreover  SGD  in the context of matrix completion  is
also useful for parallelization and distributed implementation [23].

1.1 Our Contributions

In this work we present the ﬁrst provable efﬁcient algorithm for online matrix completion by showing
that SGD (2) with a good initialization converges to a true factorization of M at a geometric rate. Our
main contributions are as follows.



(cid:107)M(cid:107)F

• We provide the ﬁrst provable  efﬁcient  online algorithm for matrix completion. Starting with
a good initialization  after each observation  the algorithm makes quick updates each taking
time O(k3) and requires O(µdkκ4(k + log
) log d) observations to reach  accuracy 
where µ is the incoherence parameter  d = max(d1  d2)  k is the rank and κ is the condition
number of M.
• Moreover  our result features both sample complexity and total runtime linear in d  and is
competitive to even the best existing ofﬂine results for matrix completion. (either improve
over or is incomparable  i.e.  better in some parameters and worse in others  to these results).
See Table 1 for the comparison.
• To obtain our results  we introduce a general framework to show SGD updates tend to stay
away from saddle surfaces. In order to do so  we consider distances from saddle surfaces 
show that they behave like sub-martingales under SGD updates and use martingale conver-
gence techniques to conclude that the iterates stay away from saddle surfaces. While [24]
shows that SGD updates stay away from saddle surfaces  the stepsizes they can handle are

2

Table 1: Comparison of sample complexity and runtime of our algorithm with existing algorithms in

order to obtain Frobenius norm error . (cid:101)O(·) hides log d factors. See Section 1.2 for more discussion.

Algorithm

Nuclear Norm [22]

Alternating

minimization [14]

Alternating

minimization [8]
Projected gradient

descent[12]
SGD [24]
Our result

 )

Sample complexity

(cid:101)O(µdk)
(cid:101)O(µdkκ8 log 1
(cid:101)O(cid:0)µdk2κ2(cid:0)k + log 1
(cid:101)O(µdk5)
(cid:101)O(µ2dk7κ6)
(cid:1)(cid:1)
(cid:101)O(cid:0)µdkκ4(cid:0)k + log 1





(cid:1)(cid:1)

)

 )

√

Total runtime

(cid:101)O(d3/
(cid:101)O(µdk2κ8 log 1
(cid:101)O(cid:0)µdk3κ2(cid:0)k + log 1
(cid:101)O(µdk7 log 1
(cid:1)
(cid:101)O(cid:0)µdk4κ4 log 1

poly(µ  d  k  κ) log 1


 )



(cid:1)(cid:1)

Online?

No

No

No

No

Yes
Yes



quite small (scaling as 1/poly(d1  d2))  leading to suboptimal computational complexity.
Our framework makes it possible to establish the same statement for much larger step sizes 
giving us near-optimal runtime. We believe these techniques may be applicable in other
non-convex settings as well.

1.2 Related Work

In this section we will mention some more related work.
Ofﬂine matrix completion: There has been a lot of work on designing ofﬂine algorithms for matrix
completion  we provide the detailed comparison with our algorithm in Table 1. The nuclear norm
relaxation algorithm [22] has near-optimal sample complexity for this problem but is computationally
expensive. Motivated by the empirical success of non-convex heuristics  a long line of works 
[14  8  12  24] and so on  has obtained convergence guarantees for alternating minimization  gradient
descent  projected gradient descent etc. Even the best of these are suboptimal in sample complexity
by poly(k  κ) factors. Our sample complexity is better than that of [14] and is incomparable to those
of [8  12]. To the best of our knowledge  the only provable online algorithm for this problem is that
of Sun and Luo [24]. However the stepsizes they suggest are quite small  leading to suboptimal
computational complexity by factors of poly(d1  d2). The runtime of our algorithm is linear in d 
which makes poly(d) improvements over it.
Other models for online matrix completion: Another variant of online matrix completion studied
in the literature is where observations are made on a column by column basis e.g.  [16  26]. These
models can give improved ofﬂine performance in terms of space and could potentially work under
relaxed regularity conditions. However  they do not tackle the version where only entries (as opposed
to columns) are observed.
Non-convex optimization: Over the last few years  there has also been a signiﬁcant amount of
work in designing other efﬁcient algorithms for solving non-convex problems. Examples include
eigenvector computation [6  11]  sparse coding [20  1] etc. For general non-convex optimization  an
interesting line of recent work is that of [7]  which proves gradient descent with noise can also escape
saddle point  but they only provide polynomial rate without explicit dependence. Later [17  21] show
that without noise  the space of points from where gradient descent converges to a saddle point is a
measure zero set. However  they do not provide a rate of convergence. Another related piece of work
to ours is [10]  proves global convergence along with rates of convergence  for the special case of
computing matrix squareroot.

1.3 Outline

The rest of the paper is organized as follows. In Section 2 we formally describe the problem and all
relevant parameters. In Section 3  we present our algorithms  results and some of the key intuition

3

behind our results. In Section 4 we give proof outline for our main results. We conclude in Section 5.
All formal proofs are deferred to the Appendix.

2 Preliminaries

In this section  we introduce our notation  formally deﬁne the matrix completion problem and
regularity assumptions that make the problem tractable.

2.1 Notation
We use [d] to denote {1  2 ···   d}. We use bold capital letters A  B to denote matrices and bold
lowercase letters u  v to denote vectors. Aij means the (i  j)th entry of matrix A. (cid:107)w(cid:107) denotes the
(cid:96)2-norm of vector w and (cid:107)A(cid:107)/(cid:107)A(cid:107)F/(cid:107)A(cid:107)∞ denotes the spectral/Frobenius/inﬁnity norm of matrix
A. σi(A) denotes the ith largest singular value of A and σmin(A) denotes the smallest singular
value of A. We also let κ(A) = (cid:107)A(cid:107) /σmin(A) denote the condition number of A (i.e.  the ratio
of largest to smallest singular value). Finally  for orthonormal bases of a subspace W  we also use
PW = WW(cid:62) to denote the projection to the subspace spanned by W.

2.2 Problem statement and assumptions
Consider a general rank k matrix M ∈ Rd1×d2. Let Ω ⊂ [d1]× [d2] be a subset of coordinates  which
are sampled uniformly and independently from [d1] × [d2]. We denote PΩ(M) to be the projection
of M on set Ω so that:

(cid:26) Mij 

0 

if (i  j) ∈ Ω
if (i  j) (cid:54)∈ Ω

[PΩ(M)]ij =

Low rank matrix completion is the task of recovering M by only observing PΩ(M). This task is
ill-posed and NP-hard in general [9]. In order to make this tractable  we make by now standard
assumptions about the structure of M.
Deﬁnition 2.1. Let W ∈ Rd×k be an orthonormal basis of a subspace of Rd of dimension k. The
coherence of W is deﬁned to be
def
=

(cid:13)(cid:13)e(cid:62)
i W(cid:13)(cid:13)2

(cid:107)PWei(cid:107)2 =

µ(W)

Assumption 2.2 (µ-incoherence[4  22]). We assume M is µ-incoherent  i.e.  max{µ(X)  µ(Y)} ≤
µ  where X ∈ Rd1×k  Y ∈ Rd2×k are the left and right singular vectors of M.

d
k

max
1≤i≤d

d
k

max
1≤i≤d

3 Main Results and Intuition

In this section  we present our main result. We will ﬁrst state result for a special case where M is a
symmetric positive semi-deﬁnite (PSD) matrix  where the algorithm and analysis are much simpler.
We will then discuss the general case.

3.1 Symmetric PSD Case

def
Consider the special case where M is symmetric PSD and let d
= d1 = d2. Then  we can parametrize
a rank k symmetric PSD matrix by UU(cid:62) where U ∈ Rd×k. Our algorithm for this case is given
in Algorithm 1. The following theorem provides guarantees on the performance of Algorithm 1.
The algorithm starts by using an initial set of samples Ωinit to construct a crude approximation to
the low rank of factorization of M. It then observes samples from M one at a time and updates its
factorization after every observation.
Theorem 3.1. Let M ∈ Rd×d be a rank k  symmetric PSD matrix with µ-incoherence. There
exist some absolute constants c0 and c such that if |Ωinit| ≥ c0µdk2κ2(M) log d  learning rate
η ≤

d8   we will have for all t ≤ d2 that1:

c

µdkκ3(M)(cid:107)M(cid:107) log d   then with probability at least 1 − 1
η · σmin(M)

(cid:107)UtU(cid:62)

t − M(cid:107)2

F ≤

(cid:19)t(cid:18) 1

σmin(M)

.

(cid:19)2

(cid:18)

1 − 1
2

10

1W.L.O.G  we can always assume t < d2  otherwise we already observed the entire matrix.

4

Initial set of uniformly random samples Ωinit of a symmetric PSD matrix M ∈ Rd×d 

Algorithm 1 Online Algorithm for PSD Matrix Completion.
Input:
Output: U such that UU(cid:62) ≈ M

learning rate η  iterations T
0 ← top k SVD of
U0U(cid:62)
for t = 0 ···   T − 1 do

d2

|Ωinit|PΩinit(M)

Observe Mij where (i  j) ∼ Unif ([d] × [d])
Ut+1 ← Ut − 2ηd2(UtU(cid:62)
t − M)ij(eie(cid:62)

j + eje(cid:62)

i )Ut

end for
Return UT

Remarks:

• The algorithm uses an initial set of observations Ωinit to produce a warm start iterate U0 
• The sample complexity of the warm start phase is O(µdk2κ2(M) log d). The initialization

then enters the online stage  where it performs SGD.

consists of a top-k SVD on a sparse matrix  whose runtime is O(µdk3κ2(M) log d).

• For the online phase (SGD) 

if we choose η =
of observations T required for the error (cid:107)UT U(cid:62)
O(µdkκ(M)4 log d log σmin(M)

the number
T − M(cid:107)F to be smaller than  is
• Since each SGD step modiﬁes two rows of Ut  its runtime is O(k) with a total runtime for

µdkκ3(M)(cid:107)M(cid:107) log d 

).

c



online phase of O(kT ).

Our proof approach is to essentially show that the objective function is well-behaved (i.e.  is smooth
and strongly convex) in a local neighborhood of the warm start region  and then use standard
techniques to show that SGD obtains geometric convergence in this setting. The most challenging and
novel part of our analysis comprises of showing that the iterate does not leave this local neighborhood
while performing SGD updates. Refer Section 4 for more details on the proof outline.

3.2 General Case
Let us now consider the general case where M ∈ Rd1×d2 can be factorized as UV(cid:62) with U ∈ Rd1×k
and V ∈ Rd2×k. In this scenario  we denote d = max{d1  d2}. We recall our remarks from the
previous section that our analysis of the performance of SGD depends on the smoothness and strong
convexity properties of the objective function in a local neighborhood of the iterates. Having U (cid:54)= V
introduces additional challenges in this approach since for any nonsingular k-by-k matrix C  and
U(cid:48) def
= VC−1  we have U(cid:48)V(cid:48)(cid:62) = UV(cid:62). Suppose for instance C is a very small scalar
times the identity i.e.  C = δI for some small δ > 0. In this case  U(cid:48) will be large while V(cid:48) will be
small. This drastically deteriorates the smoothness and strong convexity properties of the objective
function in a neighborhood of (U(cid:48)  V(cid:48)).

= UC(cid:62)  V(cid:48) def

Algorithm 2 Online Algorithm for Matrix Completion (Theoretical)
Input:
Output: U  V such that UV(cid:62) ≈ M

Initial set of uniformly random samples Ωinit of M ∈ Rd1×d2  learning rate η  iterations T
0 ← top k SVD of d1d2
V ← SVD(UtV(cid:62)
t )

U0V(cid:62)
for t = 0 ···   T − 1 do

|Ωinit|PΩinit(M)

˜Vt ← WV D 1

WU DW(cid:62)
˜Ut ← WU D 1
2  
Observe Mij where (i  j) ∼ Unif ([d] × [d])
Ut+1 ← ˜Ut − 2ηd1d2( ˜Ut ˜V(cid:62)
t − M)ijeie(cid:62)
Vt+1 ← ˜Vt − 2ηd1d2( ˜Ut ˜V(cid:62)
t − M)ijeje(cid:62)

j

2

i

˜Vt
˜Ut

end for
Return UT   VT .

5

2   ˜Vt ← WV D 1

2   where WU DW(cid:62)

V is the SVD of matrix UtV(cid:62)

To preclude such a scenario  we would ideally like to renormalize after each step by doing ˜Ut ←
WU D 1
t . This algorithm is described
in Algorithm 2. However  a naive implementation of Algorithm 2  especially the SVD step  would
incur O(min{d1  d2}) computation per iteration  resulting in a runtime overhead of O(d) over both
the online PSD case (i.e.  Algorithm 1) as well as the near linear time ofﬂine algorithms (see Table 1).
It turns out that we can take advantage of the fact that in each iteration we only update a single
row of Ut and a single row of Vt  and do efﬁcient (but more complicated) update steps instead of
doing an SVD on d1 × d2 matrix. The resulting algorithm is given in Algorithm 3. The key idea
is that in order to implement the updates  it sufﬁces to do an SVD of U(cid:62)
t Vt which are
k × k matrices. So the runtime of each iteration is at most O(k3). The following lemma shows the
equivalence between Algorithms 2 and 3.

t Ut and V(cid:62)

U0V(cid:62)
for t = 0 ···   T − 1 do

Algorithm 3 Online Algorithm for Matrix Completion (Practical)
Input:
Output: U  V such that UV(cid:62) ≈ M

Initial set of uniformly random samples Ωinit of M ∈ Rd1×d2  learning rate η  iterations T
0 ← top k SVD of d1d2
U ← SVD(U(cid:62)
t Ut)
V ← SVD(V(cid:62)
t Vt)
V ← SVD(D
U R(cid:62)

PΩinit (M)

V )(cid:62))

Ωinit

1
2

RU DU R(cid:62)
RV DV R(cid:62)
QU DQ(cid:62)
Observe Mij where (i  j) ∼ Unif ([d] × [d])
t − M)ijeie(cid:62)
Ut+1 ← Ut − 2ηd1d2(UtV(cid:62)
t − M)ijeje(cid:62)
Vt+1 ← Vt − 2ηd1d2(UtV(cid:62)

U RV (D

1
2

j VtRV D
i UtRU D

2

− 1
V QV Q(cid:62)
U D
− 1
U QU Q(cid:62)
V D

2

1
2

U

U R(cid:62)
V R(cid:62)

1
2

V

end for
Return UT   VT .

Lemma 3.2. Algorithm 2 and Algorithm 3 are equivalent in the sense that: given same observations
from M and other inputs  the outputs of Algorithm 2  U  V and those of Algorithm 3  U(cid:48)  V(cid:48) satisfy
UV(cid:62) = U(cid:48)V(cid:48)(cid:62).

Since the output of both algorithms is the same  we can analyze Algorithm 2 (which is easier than
that of Algorithm 3)  while implementing Algorithm 3 in practice. The following theorem is the main
result of our paper which presents guarantees on the performance of Algorithm 2.
Theorem 3.3. Let M ∈ Rd1×d2 be a rank k matrix with µ-incoherence and let d
def
= max(d1  d2).
There exist some absolute constants c0 and c such that if |Ωinit| ≥ c0µdk2κ2(M) log d  learning rate
η ≤

d8   we will have for all t ≤ d2 that:

c

µdkκ3(M)(cid:107)M(cid:107) log d   then with probability at least 1 − 1
η · σmin(M)

(cid:107)UtV(cid:62)

t − M(cid:107)2

F ≤

(cid:18)

(cid:19)t(cid:18) 1

σmin(M)

.

(cid:19)2

1 − 1
2

10

Remarks:

• Just as in the case of PSD matrix completion (Theorem 3.1)  Algorithm 2 needs an initial
set of observations Ωinit to provide a warm start U0 and V0 after which it performs SGD.
• The sample complexity and runtime of the warm start phase are the same as in symmetric
PSD case. The stepsize η and the number of observations T to achieve  error in online
phase (SGD) are also the same as in symmetric PSD case.
• However  runtime of each update step in online phase is O(k3) with total runtime for online

phase O(k3T ).

The proof of this theorem again follows a similar line of reasoning as that of Theorem 3.1 by ﬁrst
showing that the local neighborhood of warm start iterate has good smoothness and strong convexity
properties and then use them to show geometric convergence of SGD. Proof of the fact that iterates
do not move away from this local neighborhood however is signiﬁcantly more challenging due to
renormalization steps in the algorithm. Please see Appendix C for the full proof.

6

4 Proof Sketch

In this section we will provide the intuition and proof sketch for our main results. For simplicity and
highlighting the most essential ideas  we will mostly focus on the symmetric PSD case (Theorem
3.1). For the asymmetric case  though the high-level ideas are still valid  a lot of additional effort is
required to address the renormalization step in Algorithm 2. This makes the proof more involved.
First  note that our algorithm for the PSD case consists of an initialization and then stochastic descent
steps. The following lemma provides guarantees on the error achieved by the initial iterate U0.
Lemma 4.1. Let M ∈ Rd×d be a rank-k PSD matrix with µ-incoherence. There exists a constant c0
such that if |Ωinit| ≥ c0µdk2κ2(M) log d  then with probability at least 1 − 1
d10   the top-k SVD of
|Ωinit|PΩinit(M) (denote as U0U(cid:62)
0 (cid:107)F ≤ 1
20

(cid:13)(cid:13)2 ≤ 10µkκ(M)

(cid:107)M − U0U(cid:62)

(cid:13)(cid:13)e(cid:62)

0 ) satisﬁes:

and max

σmin(M)

(cid:107)M(cid:107)

j U0

(3)

d2

d

j

By Lemma 4.1  we know the initialization algorithm already gives U0 in the local region given by
Eq.(3). Intuitively  stochastic descent steps should keep doing local search within this local region.
To establish linear convergence on (cid:107)UtU(cid:62)
F and obtain ﬁnal result  we ﬁrst establish several
important lemmas describing the properties of this local regions. Throughout this section  we always
denote SVD(M) = XSX(cid:62)  where X ∈ Rd×k  and diagnal matrix S ∈ Rk×k. We postpone all the
formal proofs in Appendix.
F and any U1  U2 ∈ {U|(cid:107)U(cid:107) ≤ Γ}  we have:
Lemma 4.2. For function f (U) = (cid:107)M − UU(cid:62)(cid:107)2

t − M(cid:107)2

(cid:107)∇f (U1) − ∇f (U2)(cid:107)F ≤ 16 max{Γ2 (cid:107)M(cid:107)} · (cid:107)U1 − U2(cid:107)F

Lemma 4.3. For function f (U) = (cid:107)M − UU(cid:62)(cid:107)2

F and any U ∈ {U|σmin(X(cid:62)U) ≥ γ}  we have:

(cid:107)∇f (U)(cid:107)2

F ≥ 4γ2f (U)

Lemma 4.2 tells function f is smooth if spectral norm of U is not very large. On the other hand 
σmin(X(cid:62)U) not too small requires both σmin(U(cid:62)U) and σmin(X(cid:62)W) are not too small  where W
is top-k eigenspace of UU(cid:62). That is  Lemma 4.3 tells function f has a property similar to strongly
convex in standard optimization literature  if U is rank k in a robust sense (σk(U) is not too small) 
and the angle between the top k eigenspace of UU(cid:62) and the top k eigenspace M is not large.

Lemma 4.4. Within the region D = {U|(cid:13)(cid:13)M − UU(cid:62)(cid:13)(cid:13)F ≤ 1
Lemma 4.4 tells inside region {U|(cid:13)(cid:13)M − UU(cid:62)(cid:13)(cid:13)F ≤ 1

(cid:107)U(cid:107) ≤(cid:112)2(cid:107)M(cid:107) 

σmin(X(cid:62)U) ≥(cid:112)σk(M)/2

10 σk(M)}  we have:

10 σk(M)}  matrix U always has a good
spectral property which gives preconditions for both Lemma 4.2 and 4.3  where f (U) is both smooth
and has a property very similar to strongly convex.
With above three lemmas  we already been able to see the intuition behind linear convergence in
Theorem 3.1. Denote stochastic gradient

j + eje(cid:62)

SG(U) = 2d2(UU(cid:62) − M)ij(eie(cid:62)

(4)
where SG(U) is a random matrix depends on the randomness of sample (i  j) of matrix M. Then 
the stochastic update step in Algorithm 1 can be rewritten as:
Ut+1 ← Ut − ηSG(Ut)

Let f (U) = (cid:107)M − UU(cid:62)(cid:107)2
F  By easy caculation  we know ESG(U) = ∇f (U)  that is SG(U) is
unbiased. Combine Lemma 4.4 with Lemma 4.2 and Lemma 4.3  we know within region D speciﬁed
by Lemma 4.4  we have function f (U) is 32(cid:107)M(cid:107)-smooth  and (cid:107)∇f (U)(cid:107)2
Let’s suppose ideally  we always have U0  . . .   Ut inside region D  this directly gives:

F ≥ 2σmin(M)f (U).

i )U

Ef (Ut+1) ≤ Ef (Ut) − ηE(cid:104)∇f (Ut)  SG(Ut)(cid:105) + 16η2 (cid:107)M(cid:107) · E(cid:107)SG(Ut)(cid:107)2

F

= Ef (Ut) − ηE(cid:107)∇f (Ut)(cid:107)2
F + 16η2 (cid:107)M(cid:107) · E(cid:107)SG(Ut)(cid:107)2
≤ (1 − 2ησmin(M))Ef (Ut) + 16η2 (cid:107)M(cid:107) · E(cid:107)SG(Ut)(cid:107)2

F

F

7

One interesting aspect of our main result is that we actually show linear convergence under the
presence of noise in gradient. This is true because for the second-order (η2) term above  we can
F ≤ h(U) · f (U)  where h(U) is a factor depends on U and
roughly see from Eq.(4) that (cid:107)SG(U)(cid:107)2
always bounded. That is  SG(U) enjoys self-bounded property — (cid:107)SG(U)(cid:107)2
F will goes to zero  as
objective function f (U) goes to zero. Therefore  by choosing learning rate η appropriately small 
we can have the ﬁrst-order term always dominate the second-order term  which establish the linear
convergence.
Now  the only remaining issue is to prove that “U0  . . .   Ut always stay inside local region D”. In
reality  we can only prove this statement with high probability due to the stochastic nature of the
update. This is also the most challenging part in our proof  which makes our analysis different from
standard convex analysis  and uniquely required due to non-convex setting.
Our key theorem is presented as follows:

F and gi(U) =(cid:13)(cid:13)e(cid:62)

i U(cid:13)(cid:13)2. Suppose initial U0 satisfying:

Theorem 4.5. Let f (U) =(cid:13)(cid:13)UU(cid:62) − M(cid:13)(cid:13)2
(cid:19)2
(cid:18) σmin(M)

f (U0) ≤

gi(U0) ≤ 10µkκ(M)2

d

max

i

(cid:107)M(cid:107)

 

20

Then  there exist some absolute constant c such that for any learning rate η <
with at least 1 − T

d10 probability  we will have for all t ≤ T that:

µdkκ3(M)(cid:107)M(cid:107) log d  

c

(cid:18) σmin(M)

(cid:19)2

10

f (Ut) ≤ (1 − 1
2

ησmin(M))t

 

max

i

gi(Ut) ≤ 20µkκ(M)2

d

(cid:107)M(cid:107)

(5)

Note function maxi gi(U) indicates the incoherence of matrix U. Theorem 4.5 guarantees if inital
U0 is in the local region which is incoherent and U0U(cid:62)
0 is close to M  then with high probability
for all steps t ≤ T   Ut will always stay in a slightly relaxed local region  and f (Ut) has linear
convergence.
It is not hard to show that all saddle points of f (U) satisfy σk(U) = 0  and all local minima are global
minima. Since U0  . . .   Ut automatically stay in region f (U) ≤ ( σmin(M)
)2 with high probability 
we know Ut also stay away from all saddle points. The claim that U0  . . .   Ut stays incoherent is
essential to better control the variance and probability 1 bound of SG(Ut)  so that we can have large
step size and tight convergence rate.
The major challenging in proving Theorem 4.5 is to both prove Ut stays in the local region  and
achieve good sample complexity and running time (linear in d) in the same time. This also requires
the learning rate η in Algorithm 1 to be relatively large. Let the event Et denote the good event where
U0  . . .   Ut satisﬁes Eq.(5). Theorem 4.5 is claiming that P (ET ) is large. The essential steps in
the proof is contructing two supermartingles related to f (Ut)1Et and gi(Ut)1Et (where 1(·) denote
indicator function)  and use Bernstein inequalty to show the concentration of supermartingales. The
1Etterm allow us the claim all previous U0  . . .   Ut have all desired properties inside local region.
Finally  we see Theorem 3.1 as a immediate corollary of Theorem 4.5.

10

5 Conclusion

In this paper  we presented the ﬁrst provable  efﬁcient online algorithm for matrix completion  based
on nonconvex SGD. In addition to the online setting  our results are also competitive with state of the
art results in the ofﬂine setting. We obtain our results by introducing a general framework that helps
us show how SGD updates self-regulate to stay away from saddle points. We hope our paper and
results help generate interest in online matrix completion  and our techniques and framework prompt
tighter analysis for other nonconvex problems.

References
[1] Sanjeev Arora  Rong Ge  Tengyu Ma  and Ankur Moitra. Simple  efﬁcient  and neural algorithms for

sparse coding. arXiv preprint arXiv:1503.00778  2015.

[2] Matthew Brand. Fast online SVD revisions for lightweight recommender systems. In SDM  pages 37–46.

SIAM  2003.

8

[3] Emmanuel J Candes  Yonina C Eldar  Thomas Strohmer  and Vladislav Voroninski. Phase retrieval via

matrix completion. SIAM Review  57(2):225–251  2015.

[4] Emmanuel J. Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations

of Computational Mathematics  9(6):717–772  December 2009.

[5] James Davidson  Benjamin Liebald  Junning Liu  Palash Nandy  Taylor Van Vleet  Ullas Gargi  Sujoy
Gupta  Yu He  Mike Lambert  Blake Livingston  et al. The youtube video recommendation system. In
Proceedings of the fourth ACM conference on Recommender systems  pages 293–296. ACM  2010.

[6] Christopher De Sa  Kunle Olukotun  and Christopher Ré. Global convergence of stochastic gradient

descent for some non-convex matrix problems. arXiv preprint arXiv:1411.1134  2014.

[7] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points—online stochastic gradient

for tensor decomposition. arXiv preprint arXiv:1503.02101  2015.

[8] Moritz Hardt. Understanding alternating minimization for matrix completion. In Foundations of Computer

Science (FOCS)  2014 IEEE 55th Annual Symposium on  pages 651–660. IEEE  2014.

[9] Moritz Hardt  Raghu Meka  Prasad Raghavendra  and Benjamin Weitz. Computational limits for matrix

completion. In COLT  pages 703–725  2014.

[10] Prateek Jain  Chi Jin  Sham M Kakade  and Praneeth Netrapalli. Computing matrix squareroot via non

convex local search. arXiv preprint arXiv:1507.05854  2015.

[11] Prateek Jain  Chi Jin  Sham M Kakade  Praneeth Netrapalli  and Aaron Sidford. Matching matrix
bernstein with little memory: Near-optimal ﬁnite sample guarantees for oja’s algorithm. arXiv preprint
arXiv:1602.06929  2016.

[12] Prateek Jain and Praneeth Netrapalli. Fast exact matrix completion with ﬁnite samples. arXiv preprint

arXiv:1411.1087  2014.

[13] Hui Ji  Chaoqiang Liu  Zuowei Shen  and Yuhong Xu. Robust video denoising using low rank matrix

completion. In CVPR  pages 1791–1798. Citeseer  2010.

[14] Raghunandan Hulikal Keshavan. Efﬁcient algorithms for collaborative ﬁltering. PhD thesis  STANFORD

UNIVERSITY  2012.

[15] Yehuda Koren. The BellKor solution to the Netﬂix grand prize. Netﬂix prize documentation  81:1–10 

2009.

[16] Akshay Krishnamurthy and Aarti Singh. Low-rank matrix and tensor completion via adaptive sampling.

In Advances in Neural Information Processing Systems  pages 836–844  2013.

[17] Jason D Lee  Max Simchowitz  Michael I Jordan  and Benjamin Recht. Gradient descent converges to

minimizers. University of California  Berkeley  1050:16  2016.

[18] G. Linden  B. Smith  and J. York. Amazon.com recommendations: item-to-item collaborative ﬁltering.

IEEE Internet Computing  7(1):76–80  Jan 2003.

[19] Xin Luo  Yunni Xia  and Qingsheng Zhu. Incremental collaborative ﬁltering recommender based on

regularized matrix factorization. Knowledge-Based Systems  27:271–280  2012.

[20] Julien Mairal  Francis Bach  Jean Ponce  and Guillermo Sapiro. Online learning for matrix factorization

and sparse coding. The Journal of Machine Learning Research  11:19–60  2010.

[21] Ioannis Panageas and Georgios Piliouras. Gradient descent converges to minimizers: The case of non-

isolated critical points. arXiv preprint arXiv:1605.00405  2016.

[22] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research 

12(Dec):3413–3430  2011.

[23] Benjamin Recht and Christopher Ré. Parallel stochastic gradient algorithms for large-scale matrix comple-

tion. Mathematical Programming Computation  5(2):201–226  2013.

[24] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization. In Foundations

of Computer Science (FOCS)  2015 IEEE 56th Annual Symposium on  pages 270–289. IEEE  2015.

[25] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational

mathematics  12(4):389–434  2012.

[26] Se-Young Yun  Marc Lelarge  and Alexandre Proutiere. Streaming  memory limited matrix completion

with noise. arXiv preprint arXiv:1504.03156  2015.

9

,Chi Jin
Sham Kakade
Praneeth Netrapalli