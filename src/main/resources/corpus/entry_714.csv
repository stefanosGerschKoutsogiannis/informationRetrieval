2018,Bayesian Control of Large MDPs with Unknown Dynamics in Data-Poor Environments,We propose a Bayesian decision making framework for control of Markov Decision Processes (MDPs) with unknown dynamics and large  possibly continuous  state  action  and parameter spaces in data-poor environments. Most of the existing adaptive controllers for MDPs with unknown dynamics are based on the reinforcement learning framework and rely on large data sets acquired by sustained direct interaction with the system or via a simulator. This is not feasible in many applications  due to ethical  economic  and physical constraints. The proposed framework addresses the data poverty issue by decomposing the problem into an offline planning stage that does not rely on sustained direct interaction with the system or simulator and an online execution stage. In the offline process  parallel Gaussian process temporal difference (GPTD) learning techniques are employed for near-optimal Bayesian approximation of the expected discounted reward over a sample drawn from the prior distribution of unknown parameters. In the online stage  the action with the maximum expected return with respect to the posterior distribution of the parameters is selected. This is achieved by an approximation of the posterior distribution using a Markov Chain Monte Carlo (MCMC) algorithm  followed by constructing multiple Gaussian processes over the parameter space for efficient prediction of the means of the expected return at the MCMC sample. The effectiveness of the proposed framework is demonstrated using a simple dynamical system model with continuous state and action spaces  as well as a more complex model for a metastatic melanoma gene regulatory network observed through noisy synthetic gene expression data.,Bayesian Control of Large MDPs with

Unknown Dynamics in Data-Poor Environments

Mahdi Imani

Texas A&M University
College Station  TX  USA
m.imani88@tamu.edu

Seyede Fatemeh Ghoreishi

Texas A&M University
College Station  TX  USA
f.ghoreishi88@tamu.edu

Ulisses M. Braga-Neto
Texas A&M University
College Station  TX  USA
ulisses@ece.tamu.edu

Abstract

We propose a Bayesian decision making framework for control of Markov Deci-
sion Processes (MDPs) with unknown dynamics and large  possibly continuous 
state  action  and parameter spaces in data-poor environments. Most of the exist-
ing adaptive controllers for MDPs with unknown dynamics are based on the re-
inforcement learning framework and rely on large data sets acquired by sustained
direct interaction with the system or via a simulator. This is not feasible in many
applications  due to ethical  economic  and physical constraints. The proposed
framework addresses the data poverty issue by decomposing the problem into an
ofﬂine planning stage that does not rely on sustained direct interaction with the
system or simulator and an online execution stage. In the ofﬂine process  parallel
Gaussian process temporal difference (GPTD) learning techniques are employed
for near-optimal Bayesian approximation of the expected discounted reward over
a sample drawn from the prior distribution of unknown parameters. In the online
stage  the action with the maximum expected return with respect to the posterior
distribution of the parameters is selected. This is achieved by an approximation
of the posterior distribution using a Markov Chain Monte Carlo (MCMC) algo-
rithm  followed by constructing multiple Gaussian processes over the parameter
space for efﬁcient prediction of the means of the expected return at the MCMC
sample. The effectiveness of the proposed framework is demonstrated using a
simple dynamical system model with continuous state and action spaces  as well
as a more complex model for a metastatic melanoma gene regulatory network
observed through noisy synthetic gene expression data.

1

Introduction

Dynamic programming (DP) solves the optimal control problem for Markov Decision Processes
(MDPs) with known dynamics and ﬁnite state and action spaces. However  in complex applications
there is often uncertainty about the system dynamics. In addition  many practical problems have
large or continuous state and action spaces. Reinforcement learning is a powerful technique widely
used for adaptive control of MDPs with unknown dynamics [1]. Existing RL techniques developed
for MDPs with unknown dynamics rely on data that is acquired via interaction with the system or
via simulation. While this is feasible in areas such as robotics or speech recognition  in other appli-
cations such as medicine  materials science  and business  there is either a lack of reliable simulators
or inaccessibility to the real system due to practical limitations  including cost  ethical  and physical
considerations. For instance  recent advances in metagenomics and neuroscience call for the devel-
opment of efﬁcient intervention strategies for disease treatment. However  these systems are often
modeled with MDPs with continuous state and action spaces  with limited access to expensive data.
Thus  there is a need for control of systems with unknown dynamics and large or continuous state 
action  and parameter spaces in data-poor environments.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Related Work: Approximate dynamic programming (ADP) techniques have been developed for
problems in which the exact DP solution is not achievable. These include parametric and non-
parametric reinforcement learning (RL) techniques for approximating the expected discounted re-
ward over large or continuous state and action spaces. Parametric RL techniques include neural
ﬁtted Q-iteration [2]  deep reinforcement learning [3]  and kernel-based techniques [4]. A popular
class of non-parametric RL techniques is Gaussian process temporal difference (GPTD) learning [5] 
which provides a Bayesian representation of the expected discounted return. However  all afore-
mentioned methods involve approximate ofﬂine planning for MDPs with known dynamics or online
learning by sustained direct interaction with the system or a simulator. The multiple model-based
RL (MMRL) [6] is a framework that allows the extension of the aforementioned RL techniques
to MDPs with unknown dynamics represented over a ﬁnite parameter space  and therefore cannot
handle large or continuous parameter spaces.
In addition  there are several Bayesian reinforcement learning techniques in the literature [7]. For
example  Bayes-adaptive RL methods assume a parametric family for the MDP transition matrix
and simultaneously learn the parameters and policy. A closely related method in this class is the
Beetle algorithm [8]  which converts a ﬁnite-state MDP into a continuous partially-observed MDP
(POMDP). Then  an approximate ofﬂine algorithm is developed to solve the POMDP. The Beetle
algorithm is however capable of handling ﬁnite state and action spaces only. Online tree search
approximations underlie a varied and popular class of Bayesian RL techniques [9–16]. In particular 
the Bayes-adaptive Monte-Carlo planning (BAMCP) algorithm [16] has been shown empirically
to outperform the other techniques in this category. This is due to the fact that BAMCP uses a
rollout policy during the learning process  which effectively biases the search tree towards good
solutions. However  this class of methods applies to ﬁnite-state MDPs with ﬁnite actions; application
to continuous state and action spaces requires discretization of these spaces  rendering computation
intractable in most cases of interest.
Lookahead policies are a well-studied class of techniques that can be used for control of MDPs with
large or continuous state  action  and parameter spaces [17]. However  ignoring the long future hori-
zon in their decision making process often results in poor performance. Other methods to deal with
systems carrying other sources of uncertainty include [18  19].
Main Contributions: The goal of this paper is to develop a framework for Bayesian decision mak-
ing for MDPs with unknown dynamics and large or continuous state  action and parameter spaces
in data-poor environments. The framework consists of ofﬂine and online stages. In the ofﬂine stage 
samples are drawn from a prior distribution over the space of parameters. Then  parallel Gaussian
process temporal difference (GPTD) learning algorithms are applied for Bayesian approximation of
the expected discounted reward associated with these parameter samples. During the online process 
a Markov Chain Monte Carlo (MCMC) algorithm is employed for sample-based approximation of
the posterior distribution. For decision making with respect to the posterior distribution  Gaussian
process regression over the parameter space based on the means and variances of the expected re-
turns obtained in the ofﬂine process is used for prediction of the expected returns at the MCMC
sample points. The proposed framework offers several beneﬁts  summarized as follows:
• Risk Consideration: Most of the existing techniques try to estimate ﬁxed values for approx-
imating the expected Q-function and make a decision upon that  while the proposed method
is capable of Bayesian representation of the Q-function. This allows risk consideration during
action selection  which is required by many real-world applications  such as cancer drug design.
• Fast Online Decision Making: The proposed method is suitable for problems with tight time-
limit constraints  in which the action should be selected relatively fast. Most of the computational
effort spent by the proposed method is in the ofﬂine process. By contrast  the online process used
by Monte-Carlo based techniques is often very slow  especially for large MDPs  in which a large
number of trajectories must be simulated for accurate estimation of the Q-functions.

• Continuous State/Action Spaces: Existing Bayesian RL techniques can handle continuous state
and action spaces to some extent (e.g.  via discretization). However  the difﬁculty in picking a
proper quantization rate  which directly impacts accuracy  and the computational intractability
for large MDPs make the existing methods less attractive.

• Generalization: Another feature of the proposed method is the ability to serve as an initialization
step for Monte-Carlo based techniques. In fact  if the expected error at each time point is large

2

(according to the Bayesian representation of the Q-functions)  Monte-Carlo techniques can be
employed for efﬁcient online search using the available results of the proposed method.
• Anytime Planning: The Bayesian representation of the Q-function allows starting the online
decision making process at anytime to improve the ofﬂine planning results. In fact  while the
online planning is undertaken  the accuracy of the Q-functions at the current ofﬂine samples can
be improved or the Q-functions at new ofﬂine samples from the posterior distribution can be
computed.

2 Background
A Markov decision process (MDP) is formally deﬁned by a 5-tuple (cid:104)S  A  T  R  γ(cid:105)  where S is the
state space  A is the action space  T : S × A × S is the state transition probability function such
that T (s  a  s(cid:48)) = p(s(cid:48) | s  a) represents the probability of moving to state s(cid:48) after taking action a in
state s  R : S × A → R is a bounded reward function such that R(s  a) encodes the reward earned
when action a is taken in state s  and 0 < γ < 1 is a discount factor.
A deterministic stationary policy π for an MDP is a mapping π : S → A from states to actions. The
expected discounted reward function at state s ∈ S after taking action a ∈ A and following policy π
afterward is deﬁned as:

(cid:35)

(cid:34) ∞(cid:88)

Qπ(s  a) = E

γtR(st  at) | s0 = s  a0 = a

.

(1)

t=0

The optimal action-value function  denoted by Q∗  provides the maximum expected return Q∗(s  a)
that can be obtained after executing action a in state s. An optimal stationary policy π∗  which
attains the maximum expected return for all states  is given by π∗(s) = maxa∈A Q∗(s  a).
An MDP is said to have known dynamics if the 5-tuple (cid:104)S  A  T  R  γ(cid:105) is fully speciﬁed  otherwise
it is said to have unknown dynamics. For an MDP with known dynamics and ﬁnite state and action
spaces  planning algorithms such as Value Iteration or Policy Iteration [20] can be used to compute
the optimal policy ofﬂine. Several approximate dynamic programming (ADP) methods have been
developed for approximating the optimal stationary policy over continuous state and action spaces.
However  in this paper  we are concerned with large MDP with unknown dynamics in data-poor
environments.

3 Proposed Bayesian Decision Framework

Let the unknown parts of the dynamics be encoded into a ﬁnite dimensional vector θ  where θ takes
value in a parameter space Θ ⊂ Rm. Notice that each θ ∈ Θ speciﬁes an MDP with known
dynamics. Assuming (a0:k−1  s0:k) be the sequence of taken actions and observed states up to time
step k during the execution process  the proposed method selects an action according to:

Eθ|s0:k a0:k−1[Q∗

θ(sk  a)]  

a∈A

ak = argmax

(2)
where the expectation is taken relative to the posterior distribution p(θ | s0:k  a0:k−1)  and Q∗
characterizes the optimal expected return for the MDP associated with θ.
Two main issues complicate ﬁnding the exact solution in (2). First  computation of the posterior
distribution might not have a closed-form solution  and one needs to use techniques such as Markov-
Chain Monte-Carlo (MCMC) for sample-based approximation of the posterior. Secondly  the exact
computation of Q∗
θ for any given θ is not possible  due to the large or possibly continuous state
and action spaces. However  for any θ ∈ Θ  the expected return can be approximated with one of
the many existing techniques such as neural ﬁtted Q-iteration [2]  deep reinforcement learning [3] 
and Gaussian process temporal difference (GPTD) learning [5]. On the other hand  all the afore-
mentioned techniques can be extremely slow over an MCMC sample that is sufﬁciently large to
achieve accurate results. In sum  computation of the expected returns associated with samples of the
posterior distribution during the execution process is not practical.
In the following paragraphs  we propose efﬁcient ofﬂine and online planning processes capable of
computing an approximate solution to the optimization problem in (2).

θ

3

(cid:34) ∞(cid:88)

r=t

3.1 Ofﬂine Planner
The ofﬂine process starts by drawing a sample Θprior = {θprior
}N prior
i=1 ∼ p(θ) of size N prior from
the parameter prior distribution. For each sample point θ ∈ Θprior  one needs to approximate
the optimal expected return Q∗
θ over the entire state and action spaces. We propose to do this by
using Gaussian process temporal difference (GPTD) learning [5]. The detailed reasoning behind
this choice will be provided when the online planner is discussed.
θ for given θ ∈
GP-SARSA is a GPTD algorithm that provides a Bayesian approximation of Q∗
Θprior. We describe the GP-SARSA algorithm over the next several paragraphs. Given a policy
πθ : S → A for an MDP corresponding to θ  the discounted return at time step t can be written as:

i

(cid:35)

U t πθ

θ

(st  at) = E

γr−t Rθ(sr+1  ar+1)

 

(3)

where sr+1 ∼ p (s(cid:48) | sr  ar = πθ(sr)  θ)  and U t πθ
(st  at) is the expected accumulated reward for
the system corresponding to parameter θ obtained over time if the current state and action are st and
at and policy πθ is followed afterward.
In the GPTD method  the expected discounted return U t πθ

(st  at) is approximated as:

θ

θ

θ (s  a) + ∆Qπθ
θ  
θ (s  a) is a Gaussian process [21] over the space S×A and ∆Qπθ

(st = s  at = a) ≈ Qπθ

U t πθ

θ

(4)
θ is a zero-mean Gaussian

q. A zero-mean Gaussian process is usually considered as a prior:

where Qπθ
residual with variance σ2

θ (s  a) = GP (0  kθ ((s  a)  (s(cid:48)  a(cid:48))))  
Qπθ

(5)
where kθ(· ·) is a real-valued kernel function  which encodes our prior belief on the correlation be-
tween (s  a) and (s(cid:48)  a(cid:48)). One possible choice is considering decomposable kernels over the state and
action spaces: kθ ((s  a)  (s(cid:48)  a(cid:48))) = kS θ (s  s(cid:48))×kU θ (a  a(cid:48)). A proper choice of the kernel function
depends on the nature of the state and action spaces  e.g.  whether they are ﬁnite or continuous.
Let Bθ
by a policy πθ from an MDP corresponding to θ  with the corresponding immediate rewards rθ
[Rθ(s0  a0)  . . .   Rθ(st  at)]T . The posterior distribution of Qπθ

t = [(s0  a0)  . . .   (st  at)]T be the sequence of observed joint state and action pairs simulated
t =

θ (s  a) can be written as [5]:

θ (s  a) | rθ
Qπθ

covθ((s  a)  (s  a)) = kθ((s  a)  (s  a))−K(s a) Bθ

Qθ(s  a) = K(s a) Bθ

where

with

Ht =



1 −γ
0
1 −γ
0
...
...
...

0
0

0
0

0
0

. . .
0
. . . 0
...
. . .
. . .

...
...
1 −γ
1
0

t )−1rθ
t  

t

t

t  Bθ
t

HT

HT

t   Bθ

t + σ2

t + (σq

q HtHT
HT

t  Bθ
t
t (HtKBθ

t (HtKBθ
HT

t ∼ N(cid:0)Qθ(s  a)  covθ ((s  a)  (s  a))(cid:1)  
  KB B(cid:48) =
t ∼ N(cid:16)

kθ((s0  a0)  (s(cid:48)

kθ((sm  am)  (s(cid:48)

q )2ItHT
t

(cid:17)(cid:17)

0  a(cid:48)

0  a(cid:48)

+ (σθ

(cid:16)

...

0
0

...

θ)2HtHT

t )−1HtKT

(s a) Bθ
t

(7)

0)) . . . kθ((s0  a0)  (s(cid:48)

n  a(cid:48)

n))

0)) . . . kθ((sm  am)  (s(cid:48)

n  a(cid:48)

n))

...

 

(6)

(8)

(9)

for B = [(s0  a0)  . . .   (sm  am)]T and B(cid:48) = [(s(cid:48)
The hyper-parameters of the kernel function can be estimated by maximizing the likelihood of the
observed reward [22]:

0)  . . .   (s(cid:48)

n  a(cid:48)

0  a(cid:48)

n)]T .

t | Bθ
rθ

KBθ

0  Ht
where It is the identity matrix of size t × t.
The choice of policy for gathering data has signiﬁcant impact on the proximity of the estimated
discounted return to the optimal one. A well-known option  which uses Bayesian representation of
the expected return and adaptively balances exploration and exploitation  is given by [22]:

qa  qa∼N(cid:0)Qθ(s  a)  covθ ((s  a)  (s  a))(cid:1).

(10)

t  Bθ
t

 

πθ(s) = argmax

a∈A

4

The GP-SARSA algorithm approximates the expected return by simulating several trajectories based
on the above policy. Running N prior parallel GP-SARSA algorithms for each θ ∈ Θprior leads to
N prior near-optimal approximations of the expected reward functions.

(cid:104)E(cid:104) ˆQθ(sk  a)
(cid:105)(cid:105)

3.2 Online Planner
Let ˆQθ(s  a) be the Gaussian process approximating the optimal Q-function for any s ∈ S and a ∈ A
computed by a GP-SARSA algorithm associated with parameter θ ∈ Θprior. One can approximate
(2) as:

Eθ|s0:k a0:k−1

ak ≈ argmax
a∈A

Eθ|s0:k a0:k−1

a∈A

= argmax

(11)
While the value of Qθ(sk  a) at values θ ∈ Θprior drawn from the prior distribution is available  the
expectation in (11) is over the posterior distribution.
Rather than restricting ourselves to parametric families  we compute the expectation in (11) by a
Markov Chain Monte-Carlo (MCMC) algorithm for generating i.i.d. sample values from the pos-
terior distribution. For simplicity  and without loss of generality  we employ the basic Metropolis
Hastings MCMC [23] algorithm. Let the last accepted MCMC sample in the sequence of samples
be θpost
  generated at the j-th iteration. A candidate MCMC sample point θcand is drawn accord-
ing to a symmetric proposal distribution q(θ | θpost
). The candidate MCMC sample point θcand is
accepted with probability α given by:

j

j

(cid:2)Qθ(sk  a)(cid:3) .

α = min

1 

p(s0:k  a0:k−1 | θcand) p(θcand)
p(s0:k  a0:k−1 | θpost
) p(θpost
)

j

j

 

(12)

(cid:40)

(cid:41)

otherwise it is rejected  where p(θ) denotes the prior probability of θ. Accordingly  the (j + 1)th
MCMC sample point is:

(cid:26)θn with probability α

θpost
j

otherwise

θpost
j+1 =

(13)

j

j

1

  . . .   θpost

q(θ | θpost

) > 0  for any θpost

N post) is approximately a sample from the posterior distribution.

Repeating this process leads to a sequence of MCMC sample points. The positivity of the pro-
posal distribution (i.e.
) is a sufﬁcient condition for ensur-
ing an ergodic Markov chain whose steady-state distribution is the posterior distribution p(θ |
s0:k  a0:k−1) [24]. Removing a ﬁxed number of initial “burn-in” sample points  the MCMC sample
Θpost = (θpost
The last step towards the computation of (11) is the approximation of the mean of the predicted
expected return Qθ(.  .) at values of the MCMC sample Θpost. We take advantage of the Bayesian
representation of the expected return computed by the ofﬂine GP-SARSAs for this  as described
next.
((sk  a)  (sk  a))  . . . 
Let f a
= [Qθprior
sk
covθprior
((sk  a)  (sk  a))]T be the means and variances of the predicted expected returns com-
puted based on the results of ofﬂine GP-SARSAs at current state sk for a given action a ∈ A. This
information can be used for constructing a Gaussian process for predicting the expected return over

(sk  a)  . . .   Qθprior
N prior

(sk  a)]T and va
sk

= [covθprior

N prior

1

1

the MCMC sample: Qθpost

1

(sk  a)
...

Qθpost
N post

(sk  a)

where

 = ΣΘpost Θprior
 k(θ1  θ(cid:48)

...

ΣΘm Θn =

(cid:0)ΣΘprior Θprior +Diag(va
k)(cid:1)−1
  

. . .
...
. . . k(θm  θ(cid:48)
n)

k(θ1  θ(cid:48)
n)

1)

...

k(θm  θ(cid:48)
n)

5

f a
sk

 

(14)

for Θm = {θ1  . . .   θm} and Θn = {θ(cid:48)
n}  and k(θ  θ(cid:48)) denotes the correlation between
sample points in the parameter space. The parameters of the kernel function can be inferred by
maximizing the marginal likelihood:

1  . . .   θ(cid:48)

k | Θprior ∼ N(cid:0)0  ΣΘprior Θprior + Diag(va
k)(cid:1) .

f a

(15)
The process is summarized in Figure 1(a). The red vertical lines represent the expected returns at
sample points from the posterior. It can be seen that only a single ofﬂine sample point is in the area
covered by the MCMC samples  which illustrates the advantage of the constructed Gaussian process
for predicting the expected return over the posterior distribution.

Figure 1: (a) Gaussian process for prediction of the expected returns at posterior sample points based
on prior sample points. (b) Proposed framework.
The GP is constructed for any given a ∈ A. For a large or continuous action space  one needs to draw
a ﬁnite set of actions {a1  . . .   aM} from the space  and compute Qθ(sk  a) for a ∈ {a1  . . .   aM}
and θ ∈ Θpost. It should be noted that the uncertainty in the expected return of the ofﬂine sample
points is efﬁciently taken into account for predicting the mean expected error at the MCMC sample
points. Thus  equation (11) can be written as:

(cid:2)Qθ(sk  a)(cid:3) ≈ argmax

(cid:88)

1

ak ≈ argmax
a∈A

Eθ|s0:k a0:k−1

a∈{a1 ... aM}

N post

θ∈Θpost

Qθ(sk  a) .

(16)

It is shown empirically in numerical experiments that as more data are observed during execution  the
proposed method becomes more accurate  eventually achieving the performance of a GP-SARSA
trained on data from the true system model. The entire proposed methodology is summarized in
Algorithm 1 and Figure 1(b) respectively.
Notice that the values of N prior and N post should be chosen based on the size of the MDP  the
availability of computational resources  and presence of time constraints. Indeed  large N prior means
that larger parameter samples must be obtained in the ofﬂine process  while large N post is associated
with larger MCMC samples in the posterior update step.

4 Numerical Experiments

The numerical experiments compare the performance of the proposed framework with two other
methods: 1) Multiple Model-based RL (MMRL) [6]: the parameter space in this method is quan-
tized into a ﬁnite set Θquant according to its prior distribution and the results of ofﬂine parallel
GP-SARSA algorithms associated with this set are used for decision making during the execution
θ∈Θquant Qθ(sk  ak = a)P (θ | s0:k  a0:k−1). 2) One-step
process via: aMMRL
lookahead policy [17]: this method selects the action with the highest expected immediate reward:
k = argmaxa∈A Eθ|s0:k a0:k−1[R(sk  ak = a)]. As a baseline for performance  the results of the
aseq
GP-SARSA algorithm tuned to the true model are also displayed.

= argmaxa∈A(cid:80)

k

6

Offline	PlannerOnline	PlannerpriorposteriorAlgorithm 1 Bayesian Control of Large MDPs with Unknown Dynamics in Data-Poor Environments.

Ofﬂine Planning

1: Draw N prior parameters from the prior distribution: Θprior = {θ1  . . .   θN prior} ∼ p(θ).
2: Run N prior parallel GP-SARSAs:

Online Planning

3: Initial action selection:

ˆQθ ← GP-SARSA(θ)  θ ∈ Θprior.

a0 = arg max
a∈A

1

N prior

(cid:88)

θ∈Θprior

Qθ(s0  a).

4: for k = 1  . . . do
5:
6:
7:
8:

Take action ak−1  record the new state sk.
Given s0:k  a0:k−1  run MCMC and collect Θpost
for a ∈ {a1  . . .   aM} do
Record the means and variances of ofﬂine GPs at (sk  a):

.

k

(sk  a)  . . .   Qθprior
N prior

f a
sk = [Qθ1
va
sk = [covθ1 ((sk  a)  (sk  a))  . . .   covθprior

(sk  a)]T  

N prior

((sk  a)  (sk  a))]T.

9:
10:
11:
12:

sk   va

sk over Θprior.

Construct a GP using f a
Use the constructed GP to compute Qθ(sk  a)  for θ ∈ Θpost.
end for
Action selection:

13: end for

ak = arg max

a∈{a1 ... aM }

(cid:88)

1

N post

θ∈Θpost

Qθ(sk  a).

Simple Continuous State and Action Example: The following simple MDP with unknown dy-
namics is considered in this section:

sk = bound[sk−1 − θsk−1(0.5 − sk−1) + 0.2ak−1 + nk]  

(17)
where sk ∈ S = [0  1] and ak ∈ A = [−1  1] for any k ≥ 0  nk ∼ N (0  0.05)  θ is the unknown
parameter with true value θ∗ = 0.2 and bound maps the argument to the closest point in state space.
The reward function is R(s  a) = −10 δs<0.1 − 10 δs>0.9 − 2|a|  so that the cost is minimum when
the system is in the interval [0.1  0.9]. The prior distribution is p(θ) ∼ N (0  0.2). The decomposable
squared exponential kernel function is used over the state and action spaces. The ofﬂine and MCMC
sample sizes are 10 and 1000  respectively.
Figures 2(a) and (b) plot the optimal actions in the state and parameter spaces and the Q-function
over state and action spaces for the true model θ∗  obtained by GP-SARSA algorithms. It can be seen
that the decision is signiﬁcantly impacted by the parameter  especially in regions of the state space
between 0.5 to 1. The Bayesian approximation of the Q-function is represented by two surfaces
that deﬁne 95%-conﬁdence intervals for the expected return. The average reward per step over
100 independent runs starting from different initial states are plotted in Figure 2(c). As expected 
the maximum average reward is obtained by the GP-SARSA associated with the true model. The
proposed framework signiﬁcantly outperforms both MMRL and one-step lookahead techniques.
One can see that the average reward for the proposed algorithm converges to the true model results
after less than 20 actions while the other methods do not. The very poor performance of the one-step
lookahead method is due to the greedy heuristics involved in its decision making process  which do
not factor in long-term rewards.
Melanoma Gene Regulatory Network: A key goal in genomics is to ﬁnd proper intervention
strategies for disease treatment and prevention. Melanoma is the most dangerous form of skin
cancer  the gene-expression behavior of which can be represented through the Boolean activities of
7 genes displayed in Figure 3. Each gene expression can be 0 or 1  corresponding to gene inactivation
or activation  respectively. The gene states are assumed to be updated at each discrete time through

7

Figure 2: Small example results.

the following nonlinear signal model:

xk = f (xk−1) ⊕ ak−1 ⊕ nk  

(18)
where xk = [WNT5Ak  pirink  S100Pk  RET1k  MART1k  HADHBk  STC2k] is the state vector at
time step k  action ak−1 ∈ A ⊂ {0  1}7  such that ak−1(i) = 1 ﬂips the state of the ith gene  f is
the Boolean function displayed in Table 1  in which the ith binary string speciﬁes the output value
for the given input genes  “⊕” indicates component-wise modulo-2 addition and nk ∈ {0  1}7 is
Boolean transition noise  such that P (nk(i) = 1) = p  for i = 1  . . .   7.

Figure 3: Melanoma regulatory network

Table 1: Boolean functions for the melanoma GRN.

Genes

Input Gene(s)

WNT5A HADHB
pirin
S100P
RET1
MART1
HADHB pirin S100P RET1
STC2

prin  RET1 HADHB
S100P RET1 STC2
RET1 HADHB STC2
pirin MART1 STC2

pirin STC2

Output

10
00010111
10101010
00001111
10101111
01110111
1101

In practice  the gene states are observed through gene expression technologies such as cDNA mi-
croarray or image-based assay. A Gaussian observation model is appropriate for modeling the gene
expression data produced by these technologies:

yk(i) ∼ N (20 xk(i) + θ  10)  

(19)
for i = 1  . . .   7; where parameter θ is the baseline expression in the inactivated state with the true
value θ∗ = 30. Such a model is known as a partially-observed Boolean dynamical system in the
literature [25  26].
It can be shown that for any given θ ∈ R  the partially-observed MDP in (18) and (19) can be
transformed into an MDP in a continuous belief space [27  28]:

sk = g(sk−1  ak−1  θ)

∝ p(yk | xk  θ) P (xk | xk−1  ak) sk−1  

(20)

8

0.60.4-100.20.25state (s)0.500action (a)0.75-0.211(a)																																																																																														(b)																									(c)Proposed	MethodFigure 4: Melanoma gene regulatory network results.

50(cid:80)128

where “∝" indicates that the right-hand side must be normalized to add up to 1. The belief state is a
vector of length 128 in a simplex of size 127.
In [29  30]  the expression of WNT5A was found to be highly discriminatory between cells
with properties typically associated with high metastatic competence versus those with low
metastatic competence. Hence  an intervention that blocked the WNT5A protein from activat-
ing its receptor could substantially reduce the ability of WNT5A to induce a metastatic pheno-
type. Thus  we consider the following immediate reward function in belief space: R(s  a) =
i=1 s(i) δxi(1)=0 − 10||a||1. Three actions are available for controlling the system: A =
{[0  0  0  0  0  0  0]  [0  0  0  1  0  0  0]  [0  0  0  0  0  1  0]}.
The decomposable squared exponential and delta Kronecker kernel functions are used for Gaussian
process regression over the belief state and action spaces  respectively. The ofﬂine and MCMC
sample sizes are 10 and 3000  respectively. The average reward per step over 100 independent runs
for all methods is displayed in Figure 4. Uniform and Gaussian distributions with different variances
are used as prior distributions in order to investigate the effect of prior peakedness. As expected  the
highest average reward is obtained for GP-SARSA tuned to the true parameter θ∗. The proposed
method has higher average reward than the MMRL and one-step lookahead algorithms. In fact 
the expected return produced by the proposed method converges to the GP-SARSA tuned to the
true parameter faster for peaked prior distributions. As more actions are taken  the performance of
MMRL approaches  but not quite reaches  the baseline performance of the GP-SARSA tuned to the
true parameter. The one-step lookahead method performs poorly in all cases as it does not account
for long-term rewards in the decision making process.

5 Conclusion

In this paper  we introduced a Bayesian decision making framework for control of MDPs with un-
known dynamics and large or continuous state  actions and parameter spaces in data-poor environ-
ments. The proposed framework does not require sustained direct interaction with the system or a
simulator  but instead it plans ofﬂine over a ﬁnite sample of parameters from a prior distribution over
the parameter space and transfers this knowledge efﬁciently to sample parameters from the posterior
during the execution process. The methodology offers several beneﬁts  including the possibility of
handling large and possibly continuous state  action  and parameter spaces; data-poor environments;
anytime planning; and dealing with risk in the decision making process.

Acknowledgment

The authors acknowledge the support of the National Science Foundation  through NSF award CCF-
1718924.

9

References
[1] R. S. Sutton and A. G. Barto  Reinforcement learning: An introduction. MIT press  1998.

[2] A. Antos  C. Szepesvári  and R. Munos  “Fitted Q-iteration in continuous action-space MDPs ” in Ad-

vances in neural information processing systems  pp. 9–16  2008.

[3] V. Mnih  K. Kavukcuoglu  D. Silver  A. A. Rusu  J. Veness  M. G. Bellemare  A. Graves  M. Riedmiller 
A. K. Fidjeland  G. Ostrovski  et al.  “Human-level control through deep reinforcement learning ” Nature 
vol. 518  no. 7540  p. 529  2015.

[4] L. Busoniu  R. Babuska  B. De Schutter  and D. Ernst  Reinforcement learning and dynamic programming

using function approximators  vol. 39. CRC press  2010.

[5] Y. Engel  S. Mannor  and R. Meir  “Reinforcement learning with Gaussian processes ” in Proceedings of

the 22nd international conference on Machine learning  pp. 201–208  ACM  2005.

[6] K. Doya  K. Samejima  K.-i. Katagiri  and M. Kawato  “Multiple model-based reinforcement learning ”

Neural computation  vol. 14  no. 6  pp. 1347–1369  2002.

[7] M. Ghavamzadeh  S. Mannor  J. Pineau  A. Tamar  et al.  “Bayesian reinforcement learning: A survey ”

Foundations and Trends R(cid:13) in Machine Learning  vol. 8  no. 5-6  pp. 359–483  2015.

[8] P. Poupart  N. Vlassis  J. Hoey  and K. Regan  “An analytic solution to discrete Bayesian reinforcement
learning ” in Proceedings of the 23rd international conference on Machine learning  pp. 697–704  ACM 
2006.

[9] A. Guez  D. Silver  and P. Dayan  “Efﬁcient Bayes-adaptive reinforcement learning using sample-based

search ” in Advances in Neural Information Processing Systems  pp. 1025–1033  2012.

[10] J. Asmuth and M. L. Littman  “Learning is planning: near Bayes-optimal reinforcement learning via

Monte-Carlo tree search ” arXiv preprint arXiv:1202.3699  2012.

[11] Y. Wang  K. S. Won  D. Hsu  and W. S. Lee  “Monte Carlo Bayesian reinforcement learning ” arXiv

preprint arXiv:1206.6449  2012.

[12] N. A. Vien  W. Ertel  V.-H. Dang  and T. Chung  “Monte-Carlo tree search for Bayesian reinforcement

learning ” Applied intelligence  vol. 39  no. 2  pp. 345–353  2013.

[13] S. Ross and J. Pineau  “Model-based Bayesian reinforcement learning in large structured domains ” in
Uncertainty in artiﬁcial intelligence: proceedings of the... conference. Conference on Uncertainty in Ar-
tiﬁcial Intelligence  vol. 2008  p. 476  NIH Public Access  2008.

[14] T. Wang  D. Lizotte  M. Bowling  and D. Schuurmans  “Bayesian sparse sampling for on-line reward
optimization ” in Proceedings of the 22nd international conference on Machine learning  pp. 956–963 
ACM  2005.

[15] R. Fonteneau  L. Busoniu  and R. Munos  “Optimistic planning for belief-augmented Markov decision
processes ” in Adaptive Dynamic Programming And Reinforcement Learning (ADPRL)  2013 IEEE Sym-
posium on  pp. 77–84  IEEE  2013.

[16] A. Guez  D. Silver  and P. Dayan  “Scalable and efﬁcient Bayes-adaptive reinforcement learning based on

Monte-Carlo tree search ” Journal of Artiﬁcial Intelligence Research  vol. 48  pp. 841–883  2013.

[17] W. B. Powell and I. O. Ryzhov  Optimal learning  vol. 841. John Wiley & Sons  2012.

[18] N. Drougard  F. Teichteil-Königsbuch  J.-L. Farges  and D. Dubois  “Structured possibilistic planning

using decision diagrams. ” in AAAI  pp. 2257–2263  2014.

[19] F. W. Trevizan  F. G. Cozman  and L. N. de Barros  “Planning under risk and Knightian uncertainty. ” in

IJCAI  vol. 2007  pp. 2023–2028  2007.

[20] D. P. Bertsekas  Dynamic programming and optimal control  vol. 1. Athena scientiﬁc Belmont  MA 

1995.

[21] C. E. Rasmussen and C. Williams  Gaussian processes for machine learning. MIT Press  2006.

[22] M. Gasic and S. Young  “Gaussian processes for POMDP-based dialogue manager optimization ”

IEEE/ACM Transactions on Audio  Speech  and Language Processing  vol. 22  no. 1  pp. 28–40  2014.

10

[23] W. K. Hastings  “Monte Carlo sampling methods using Markov chains and their applications ” Biometrika 

vol. 57  no. 1  pp. 97–109  1970.

[24] W. R. Gilks  S. Richardson  and D. Spiegelhalter  Markov chain Monte Carlo in practice. CRC press 

1995.

[25] U. Braga-Neto  “Optimal state estimation for Boolean dynamical systems ” in Signals  Systems and Com-
puters (ASILOMAR)  2011 Conference Record of the Forty Fifth Asilomar Conference on  pp. 1050–1054 
IEEE  2011.

[26] M. Imani and U. Braga-Neto  “Maximum-likelihood adaptive ﬁlter for partially-observed Boolean dy-

namical systems ” IEEE Transactions on Signal Processing  vol. 65  no. 2  pp. 359–371  2017.

[27] M. Imani and U. M. Braga-Neto  “Point-based methodology to monitor and control gene regulatory net-

works via noisy measurements ” IEEE Transactions on Control Systems Technology  2018.

[28] M. Imani and U. M. Braga-Neto  “Finite-horizon LQR controller for partially-observed Boolean dynam-

ical systems ” Automatica  vol. 95  pp. 172–179  2018.

[29] M. Bittner  P. Meltzer  Y. Chen  Y. Jiang  E. Seftor  M. Hendrix  M. Radmacher  R. Simon  Z. Yakhini 
A. Ben-Dor  et al.  “Molecular classiﬁcation of cutaneous malignant melanoma by gene expression pro-
ﬁling ” Nature  vol. 406  no. 6795  pp. 536–540  2000.

[30] A. T. Weeraratna  Y. Jiang  G. Hostetter  K. Rosenblatt  P. Duray  M. Bittner  and J. M. Trent  “Wnt5a
signaling directly affects cell motility and invasion of metastatic melanoma ” Cancer cell  vol. 1  no. 3 
pp. 279–288  2002.

11

,Mahdi Imani
Seyede Fatemeh Ghoreishi
Ulisses M. Braga-Neto