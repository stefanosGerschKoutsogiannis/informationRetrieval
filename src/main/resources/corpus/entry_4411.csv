2008,Privacy-preserving logistic regression,This paper addresses the important tradeoff between privacy and learnability  when designing algorithms for learning from private databases. First we apply an idea of Dwork et al. to design a specific privacy-preserving machine learning algorithm  logistic regression. This involves bounding the sensitivity of logistic regression  and perturbing the learned classifier with noise proportional to the sensitivity. Noting that the approach of Dwork et al. has limitations when applied to other machine learning algorithms  we then present another privacy-preserving logistic regression algorithm. The algorithm is based on solving a perturbed objective  and does not depend on the sensitivity. We prove that our algorithm preserves privacy in the model due to Dwork et al.  and we provide a learning performance guarantee. Our work also reveals an interesting connection between regularization and privacy.,Privacy-preserving logistic regression

Kamalika Chaudhuri

Information Theory and Applications
University of California  San Diego

kamalika@soe.ucsd.edu

Claire Monteleoni∗

Center for Computational Learning Systems

Columbia University

cmontel@ccls.columbia.edu

Abstract

This paper addresses the important tradeoff between privacy and learnability 
when designing algorithms for learning from private databases. We focus on
privacy-preserving logistic regression. First we apply an idea of Dwork et al. [6]
to design a privacy-preserving logistic regression algorithm. This involves bound-
ing the sensitivity of regularized logistic regression  and perturbing the learned
classiﬁer with noise proportional to the sensitivity.
We then provide a privacy-preserving regularized logistic regression algorithm
based on a new privacy-preserving technique: solving a perturbed optimization
problem. We prove that our algorithm preserves privacy in the model due to [6].
We provide learning guarantees for both algorithms  which are tighter for our new
algorithm  in cases in which one would typically apply logistic regression. Ex-
periments demonstrate improved learning performance of our method  versus the
sensitivity method. Our privacy-preserving technique does not depend on the sen-
sitivity of the function  and extends easily to a class of convex loss functions. Our
work also reveals an interesting connection between regularization and privacy.

1 Introduction

Privacy-preserving machine learning is an emerging problem  due in part to the increased reliance on
the internet for day-to-day tasks such as banking  shopping  and social networking. Moreover  pri-
vate data such as medical and ﬁnancial records are increasingly being digitized  stored  and managed
by independent companies. In the literature on cryptography and information security  data privacy
deﬁnitions have been proposed  however designing machine learning algorithms that adhere to them
has not been well-explored. On the other hand  data-mining algorithms have been introduced that
aim to respect other notions of privacy that may be less formally justiﬁed.
Our goal is to bridge the gap between approaches in the cryptography and information security com-
munity  and those in the data-mining community. This is necessary  as there is a tradeoff between
the privacy of a protocol  and the learnability of functions that respect the protocol. In addition to
the speciﬁc contributions of our paper  we hope to encourage the machine learning community to
embrace the goals of privacy-preserving machine learning  as it is still a ﬂedgling endeavor.
In this work  we provide algorithms for learning in a privacy model introduced by Dwork et al. [6].
The -differential privacy model limits how much information an adversary can gain about a par-
ticular private value  by observing a function learned from a database containing that value  even if
she knows every other value in the database. An initial positive result [6] in this setting depends on
the sensitivity of the function to be learned  which is the maximum amount the function value can
change due to an arbitrary change in one input. Using this method requires bounding the sensitivity
of the function class to be learned  and then adding noise proportional to the sensitivity. This might
be difﬁcult for some functions that are important for machine learning.

∗The majority of this work was done while at UC San Diego.

1

The contributions of this paper are as follows. First we apply the sensitivity-based method of design-
ing privacy-preserving algorithms [6] to a speciﬁc machine learning algorithm  logistic regression.
Then we present a second privacy-preserving logistic regression algorithm. The second algorithm is
based on solving a perturbed objective function  and does not depend on the sensitivity. We prove
that the new method is private in the -differential privacy model. We provide learning performance
guarantees for both algorithms  which are tighter for our new algorithm  in cases in which one would
typically apply logistic regression. Finally  we provide experiments demonstrating superior learning
performance of our new method  with respect to the algorithm based on [6]. Our technique may have
broader applications  and we show that it can be applied to certain classes of optimization problems.

1.1 Overview and related work

At the ﬁrst glance  it may seem that anonymizing a data-set – namely  stripping it of identifying
information about individuals  such as names  addresses  etc – is sufﬁcient to preserve privacy.
However  this is problematic  because an adversary may have some auxiliary information  which
may even be publicly available  and which can be used to breach privacy. For more details on such
attacks  see [12].
To formally address this issue  we need a deﬁnition of privacy which works in the presence of
auxiliary knowledge by the adversary. The deﬁnition we use is due to Dwork et al. [6]  and has been
used in several applications [4  11  2]. We explain this deﬁnition and privacy model in more detail
in Section 2.
Privacy and learning. The work most related to ours is [8] and [3]. [8] shows how to ﬁnd classiﬁers
that preserve -differential privacy; however  their algorithm takes time exponential in d for inputs
in Rd. [3] provides a general method for publishing data-sets while preserving -differential privacy
such that the answers to all queries of a certain type with low VC-dimension are approximately
correct. However  their algorithm can also be computationally inefﬁcient.
Additional related work. There has been a substantial amount of work on privacy in the literature 
spanning several communities. Much work on privacy has been done in the data-mining community
[1  7]  [14  10]  however the privacy deﬁnitions used in these papers are different  and some are sus-
ceptible to attacks when the adversary has some prior information. In contrast  the privacy deﬁnition
we use avoids these attacks  and is very strong.

2 Sensitivity and the -differential privacy model

Before we deﬁne the privacy model that we study  we will note a few preliminary points. Both in
that model  and for our algorithm and analyses  we assume that each value in the database is a real
vector with norm at most one. That is  a database contains values x1  . . .   xn  where xi ∈ Rd 
and (cid:107)xi(cid:107) ≤ 1 for all i ∈ {1  . . .   n}. This assumption is used in the privacy model. In addition 
we assume that when learning linear separators  the best separator passes through the origin. Note
that this is not an assumption that the data is separable  but instead an assumption that a vector’s
classiﬁcation is based on its angle  regardless of its norm.
In both privacy-preserving logistic regression algorithms that we state  the output is a parameter
vector  w  which makes prediction SGN(w · x)  on a point x. For a vector x  we use ||x|| to denote
its Euclidean norm. For a function G(x) deﬁned on Rd  we use ∇G to denote its gradient and ∇2G
to denote its Hessian.

Privacy Deﬁnition. The privacy deﬁnition we use is due to Dwork et al. [6  5]. In this model  users
have access to private data about individuals through a sanitization mechanism  usually denoted by
M. The interaction between the sanitization mechanism and an adversary is modelled as a sequence
of queries  made by the adversary  and responses  made by the sanitizer. The sanitizer  which is
typically a randomized algorithm  is said to preserve -differential privacy  if the private value of
any one individual in the data set does not affect the likelihood of a speciﬁc answer by the sanitizer
by more than .
More formally  -differential privacy can be deﬁned as follows.

2

Deﬁnition 1 A randomized mechanism M provides -differential privacy  if  for all databases D1
and D2 which differ by at most one element  and for any t 
≤ e

Pr[M(D1) = t]
Pr[M(D2) = t]

It was shown in [6] that if a mechanism satisﬁes -differential privacy  then an adversary who knows
the private value of all the individuals in the data-set  except for one single individual  cannot ﬁgure
out the private value of the unknown individual  with sufﬁcient conﬁdence  from the responses of
the sanitizer. -differential privacy is therefore a very strong notion of privacy.
[6] also provides a general method for computing an approximation to any function f while preserv-
ing -differential privacy. Before we can describe their method  we need a deﬁnition.

Deﬁnition 2 For any function f with n inputs  we deﬁne the sensitivity S(f) as the maximum  over
all inputs  of the difference in the value of f when one input of f is changed. That is 
|f(x1  . . .   xn−1  xn = a) − f(x1  . . .   xn−1  xn = a(cid:48))|

S(f) = max
(a a(cid:48))

[6] shows that for any input x1  . . .   xn  releasing f(x1  . . .   xn) + η  where η is a random variable
drawn from a Laplace distribution with mean 0 and standard deviation S(f )
preserves -differential
privacy.
In [13]  Nissim et al. showed that given any input x to a function  and a function f  it is sufﬁcient
to draw η from a Laplace distribution with standard deviation SS(f )
  where SS(f) is the smoothed-
sensitivity of f around x. Although this method sometimes requires adding a smaller amount of
noise to preserve privacy  in general  smoothed sensitivity of a function can be hard to compute.





3 A Simple Algorithm

Based on [6]  one can come up with a simple algorithm for privacy-preserving logistic regression 
which adds noise to the classiﬁer obtained by logistic regression  proportional to its sensitivity. From
Corollary 2  the sensitivity of logistic regression is at most 2
nλ. This leads to Algorithm 1  which
obeys the privacy guarantees in Theorem 1.

Algorithm 1:

1. Compute w∗  the classiﬁer obtained by regularized logistic regression on the labelled ex-

amples (x1  y1)  . . .   (xn  yn).

2. Pick a noise vector η according to the following density function: h(η) ∝ e− nλ

To pick such a vector  we choose the norm of η from the Γ(d  2
direction of η uniformly at random.

2 ||η||.
nλ) distribution  and the

3. Output w∗ + η.

Theorem 1 Let (x1  y1)  . . .   (xn  yn) be a set of labelled points over Rd such that ||xi|| ≤ 1 for
all i. Then  Algorithm 1 preserves -differential privacy.

PROOF: The proof follows by a combination of [6]  and Corollary 2  which states that the sensitivity
of logistic regression is at most 2

nλ. (cid:3)

Lemma 1 Let G(w) and g(w) be two convex functions  which are continuous and differentiable at
all points. If w1 = argminwG(w) and w2 = argminwG(w) + g(w)  then  ||w1 − w2|| ≤ g1
. Here 
g1 = maxw ||∇g(w)|| and G2 = minv minw vT∇2G(w)v  for any unit vector v.

G2

The main idea of the proof is to examine the gradient and the Hessian of the functions G and g
around w1 and w2. Due to lack of space  the full proof appears in the full version of our paper.

Corollary 2 Given a set of n examples x1  . . .   xn in Rd  with labels y1  . . .   yn  such that for all i 
||xi|| ≤ 1  the sensitivity of logistic regression with regularization parameter λ is at most 2
nλ .

3

n. (cid:3)

PROOF: We use a triangle inequality and the fact that G2 ≥ λ and g1 ≤ 1
Learning Performance. In order to assess the performance of Algorithm 1  we ﬁrst try to bound
the performance of Algorithm 1 on the training data. To do this  we need to deﬁne some notation.
For a classiﬁer w  we use L(w) to denote the expected loss of w over the data distribution  and
ˆL(w) to denote the empirical average loss of w over the training data. In other words  ˆL(w) =

(cid:80)n
i=1 log(1 + e−yiwT xi)  where  (xi  yi)  i = 1  . . .   n are the training examples.

1
n
Further  for a classiﬁer w  we use the notation fλ(w) to denote the quantity 1
ˆfλ(w) to denote the quantity 1
by the following lemma.

2 λ||w||2 + L(w) and
2 λ||w||2 + ˆL(w). Our guarantees on this algorithm can be summarized

Lemma 3 Given a logistic regression problem with regularization parameter λ  let w1 be the classi-
ﬁer that minimizes ˆfλ  and w2 be the classiﬁer output by Algorithm 1 respectively. Then  with prob-
ability 1− δ over the randomness in the privacy mechanism  ˆfλ(w2) ≤ ˆfλ(w1) + 2d2(1+λ) log2(d/δ)
.

λ2n22

Due to lack of space  the proof is deferred to the full version.
From Lemma 3  we see that performance of Algorithm 1 degrades with decreasing λ  and is poor in
particular when λ is very small. One question is  can we get a privacy-preserving approximation to
logistic regression  which has better performance bounds for small λ? To explore this  in the next
section  we look at a different algorithm.

4 A New Algorithm

In this section  we provide a new privacy-preserving algorithm for logistic regression. The input to
our algorithm is a set of examples x1  . . .   xn over Rd such that ||xi|| ≤ 1 for all i  a set of labels
y1  . . .   yn for the examples  a regularization constant λ and a privacy parameter   and the output is
a vector w∗ in Rd. Our algorithm works as follows.
Algorithm 2:

1. We pick a random vector b from the density function h(b) ∝ e− 

2||b||. To implement this 
 ) distribution  and the direction of b uniformly at

we pick the norm of b from the Γ(d  2
random.

2. Given examples x1  . . .   xn  with labels y1  . . .   yn and a regularization constant λ  we

compute w∗ = argminw

1

2 λwT w + bT w

n + 1

n

(cid:80)n
i=1 log(1 + e−yiwT xi). Output w∗.

We observe that our method solves a convex programming problem very similar to the logistic
regression convex program  and therefore it has running time similar to that of logistic regression.
In the sequel  we show that the output of Algorithm 2 is privacy preserving.

Theorem 2 Given a set of n examples x1  . . .   xn over Rd  with labels y1  . . .   yn  where for each
i  ||xi|| ≤ 1  the output of Algorithm 2 preserves -differential privacy.
PROOF: Let a and a(cid:48) be any two vectors over Rd with norm at most 1  and y  y(cid:48) ∈
{−1  1}. For any such (a  y)  (a(cid:48)  y(cid:48))  consider the inputs (x1  y1)  . . .   (xn−1  yn−1)  (a  y) and
(x1  y1) . . .   (xn−1  yn−1)  (a(cid:48)  y(cid:48)). Then  for any w∗ output by our algorithm  there is a unique
value of b that maps the input to the output. This uniqueness holds  because both the regularization
function and the loss functions are differentiable everywhere.
Let the values of b for the ﬁrst and second cases respectively  be b1 and b2.
Since w∗ is the value that minimizes both the optimization problems  the derivative of both opti-
mization functions at w∗ is 0.
This implies that for every b1 in the ﬁrst case  there exists a b2 in the second case such that: b1 −
1+eyw∗T a = b2 −
1+ey(cid:48)w∗T a(cid:48) ≤ 1

1+ey(cid:48)w∗T a(cid:48) . Since ||a|| ≤ 1  ||a(cid:48)|| ≤ 1  and

1+eyw∗T a ≤ 1  and

y(cid:48)a(cid:48)

ya

1

1

4

for any w∗  ||b1 − b2|| ≤ 2. Using the triangle inequality  ||b1|| − 2 ≤ ||b2|| ≤ ||b1|| + 2. Therefore 
for any pair (a  y)  (a(cid:48)  y(cid:48)) 

Pr[w∗|x1  . . .   xn−1  y1  . . .   yn−1  xn = a  yn = y]
Pr[w∗|x1  . . .   xn−1  y1  . . .   yn−1  xn = a(cid:48)  yn = y(cid:48)]

= h(b1)
h(b2)

= e− 

2 (||b1||−||b2||)

where h(bi) for i = 1  2 is the density of bi. Since −2 ≤ ||b1|| − ||b2|| ≤ 2  this ratio is at most e.
theorem follows. (cid:3)
We notice that the privacy guarantee for our algorithm does not depend on λ; in other words  for any
value of λ  our algorithm is private. On the other hand  as we show in Section 5  the performance of
our algorithm does degrade with decreasing λ in the worst case  although the degradation is better
than that of Algorithm 1 for λ < 1.

Other Applications. Our algorithm for privacy-preserving logistic regression can be generalized to
provide privacy-preserving outputs for more general convex optimization problems  so long as the
problems satisfy certain conditions. These conditions can be formalized in the theorem below.
Theorem 3 Let X = {x1  . . .   xn} be a database containing private data of individuals. Suppose
i=1 l(w  xi) 
over w ∈ Rd for some d  such that all of the following hold:

we would like to compute a vector w that minimizes the function F (w) = G(w) +(cid:80)n

1. G(w) and l(w  xi) are differentiable everywhere  and have continuous derivatives
2. G(w) is strongly convex and l(w  xi) are convex for all i
3. ||∇wl(w  x)|| ≤ κ  for any x.

Let b = B · ˆb  where B is drawn from Γ(d  2κ

dimensional unit sphere. Then  computing w∗  where w∗ = argminwG(w) +(cid:80)n

 )  and ˆb is drawn uniformly from the surface of a d-
i=1 l(w  xi) + bT w 

provides -differential privacy.

5 Learning Guarantees

In this section  we show theoretical bounds on the number of samples required by the algorithms to
learn a linear classiﬁer. For the rest of the section  we use the same notation used in Section 3.
First we show that  for Algorithm 2  the values of ˆfλ(w2) and ˆfλ(w1) are close.
Lemma 4 Given a logistic regression problem with regularization parameter λ  let w1 be the clas-
siﬁer that minimizes ˆfλ  and w2 be the classiﬁer output by Algorithm 2 respectively. Then  with
probability 1 − δ over the randomness in the privacy mechanism  ˆfλ(w2) ≤ ˆfλ(w1) + 8d2 log2(d/δ)
.

λn22

The proof is in the full version of our paper. As desired  for λ < 1  we have attained a tighter bound
using Algorithm 2  than Lemma 3 for Algorithm 1.
Now we give a performance guarantee for Algorithm 2.

Theorem 4 Let w0 be a classiﬁer with expected loss L over the data distribution. If the training ex-
δ )||w0||
amples are drawn independently from the data distribution  and if n > C max(||w0||2
  d log( d
) 
g
for some constant C  then  with probability 1 − δ  the classiﬁer output by Algorithm 2 has loss at
most L + g over the data distribution.
PROOF: Let w∗ be the classiﬁer that minimizes fλ(w) over the data distribution  and let w1 and w2
be the classiﬁers that minimize ˆfλ(w) and ˆfλ(w) + bT w
n over the data distribution respectively. We
can use an analysis as in [15] to write that:

2
g

L(w2) = L(w0) + (fλ(w2) − fλ(w∗)) + (fλ(w∗) − fλ(w0)) + λ
2

||w0||2 − λ
2

||w2||2

(1)

5

λn22

Notice that from Lemma 4  ˆfλ(w2) − ˆfλ(w1) ≤ 8d2 log2(d/δ)
the second quantity in equation 1 as fλ(w2) − fλ(w∗) ≤ 16d2 log2(d/δ)
λn22
w∗  the third quantity in Equation 1 is non-positive. If λ is set to be
in Equation 1 is at most g
if n > C · ||w0||d log( d
output by Algorithm 2 is at most L(w0) + g.
(cid:3)

2 . Now  if n > C · ||w0||2
≤ g
  then  16d2 log2( d
δ )
λn22

. Using this and [16]  we can bound
λn). By deﬁnition of
||w0||2   then  the fourth quantity
λn ≤ g
4 . In addition 
2
g
4 . In either case  the total loss of the classiﬁer w2

for a suitable constant C  1

+ O( 1

g

δ )

g

The same technique can be used to analyze the sensitivity-based algorithm  using Lemma 3  which
yields the following.

Theorem 5 Let w0 be a classiﬁer with expected loss L over the data distribution.
If
the training examples are drawn independently from the data distribution  and if n >
C max(||w0||2
)  for some constant C  then  with probability 1 − δ  the
classiﬁer output by Algorithm 2 has loss at most L + g over the data distribution.

δ )||w0||
  d log( d
g

δ )||w0||2

  d log( d

3/2

g

2
g



It is clear that this bound is never lower than the bound for Algorithm 2. Note that for problems in
which (non-private) logistic regression performs well  (cid:107)w0(cid:107) ≥ 1 if w0 has low loss  since otherwise
one can show that the loss of w0 would be lower bounded by log(1 + 1
e ). Thus the performance
guarantee for Algorithm 2 is signiﬁcantly stronger than for Algorithm 1  for problems in which one
would typically apply logistic regression.

6 Results in Simulation

Sensitivity method
New method
Standard LR

Uniform  margin=0.03 Unseparable (uniform with noise 0.2 in margin 0.1)
0.2962±0.0617
0.1426±0.1284
0±0.0016

0.3257±0.0536
0.1903±0.1105
0.0530±0.1105

Figure 1: Test error: mean ± standard deviation over ﬁve folds. N=17 500.

We include some simulations that compare the two privacy-preserving methods  and demonstrate
that using our privacy-preserving approach to logistic regression does not degrade learning per-
formance terribly  from that of standard logistic regression. Performance degradation is inevitable
however  as in both cases  in order to address privacy concerns  we are adding noise  either to the
learned classiﬁer  or to the objective.
In order to obtain a clean comparison between the various logistic regression variants  we ﬁrst ex-
perimented with artiﬁcial data that is separable through the origin. Because the classiﬁcation of a
vector by a linear separator through the origin depends only its angle  not its norm  we sampled the
data from the unit hypersphere. We used a uniform distribution on the hypersphere in 10 dimensions
with zero mass within a small margin (0.03) from the generating linear separator. Then we experi-
mented on uniform data that is not linearly separable. We sampled data from the surface of the unit
ball in 10 dimensions  and labeled it with a classiﬁer through the origin. In the band of margin ≤ 0.1
with respect to the perfect classiﬁer  we performed random label ﬂipping with probability 0.2. For
our experiments  we used convex optimization software provided by [9].
Figure 1 gives mean and standard deviation of test error over ﬁve-fold cross-validation  on 17 500
points. In both simulations  our new method is superior to the sensitivity method  although incurs
more error than standard (non-private) logistic regression. For both problems  we tuned the logistic
regression parameter  λ  to minimize the test error of standard logistic regression  using ﬁve-fold
cross-validation on a holdout set of 10 000 points (the tuned values are: λ = 0.01 in both cases).
For each test error computation  the performance of each of the privacy-preserving algorithms was
evaluated by averaging over 200 random restarts  since they are both randomized algorithms.
In Figure 2a)-b) we provide learning curves. We graph the test error after each increment of 1000
points  averaged over ﬁve-fold cross validation. The learning curves reveal that  not only does the

6

Figure 2: Learning curves: a) Uniform distribution  margin=0.03  b) Unseparable data.
Epsilon curves: c) Uniform distribution  margin=0.03  d) Unseparable data.

new method reach a lower ﬁnal error than the sensitivity method  but it also has better performance
at most smaller training set sizes.
In order to observe the effect of the level of privacy on the learning performance of the privacy-
preserving learning algorithms  in Figure 2c)-d) we vary   the privacy parameter to the two algo-
rithms  on both the uniform  low margin data  and the unseparable data. As per the deﬁnition of
-differential privacy in Section 2  strengthening the privacy guarantee corresponds to reducing .
Both algorithms’ learning performance degrades in this direction. For the majority of values of 
that we tested  the new method is superior in managing the tradeoff between privacy and learning
performance. For very small   corresponding to extremely stringent privacy requirements  the sen-
sitivity method performs better but also has a predication accuracy close to chance  which is not
useful for machine learning purposes.

7 Conclusion

In conclusion  we show two ways to construct a privacy-preserving linear classiﬁer through logistic
regression. The ﬁrst one uses the methods of [6]  and the second one is a new algorithm. Us-
ing the -differential privacy deﬁnition of Dwork et al. [6]  we prove that our new algorithm is
privacy-preserving. We provide learning performance guarantees for the two algorithms  which are
tighter for our new algorithm  in cases in which one would typically apply logistic regression. In
simulations  our new algorithm outperforms the method based on [6].
Our work reveals an interesting connection between regularization and privacy: the larger the reg-
ularization constant  the less sensitive the logistic regression function is to any one individual ex-
ample  and as a result  the less noise one needs to add to make it privacy-preserving. Therefore 
regularization not only prevents overﬁtting  but also helps with privacy  by making the classiﬁer less

7

246810121400.10.20.30.40.50.60.7N/1000. Learning curve for uniform data. d=10  epsilon=0.1  margin=0.03  lambda=0.01Avg test error over 5−fold cross−valid. 200 random restarts.  Our methodStandard LRSensitivity method24681012140.050.10.150.20.250.30.350.40.450.50.55N/1000. Learning curve for unseparable data. d=10  epsilon=0.1  lambda=0.01Avg test error over 5−fold cross−valid. 200 random restarts.  Our methodStandard LRSensitivity method00.020.040.060.080.10.120.140.160.180.20.10.150.20.250.30.350.40.450.50.550.6Epsilon. Uniform data  d=10  n=10 000  margin=0.03  lambda=0.01Avg test error over 5!fold cross!valid. 200 random restarts.  Our methodSensitivity method00.020.040.060.080.10.120.140.160.180.20.20.250.30.350.40.450.50.55Epsilon. Unseparable data  d=10  n=10 000  lambda=0.01Avg test error over 5−fold cross−valid. 200 random restarts.  Our methodSensitivity methodsensitive. An interesting future direction would be to explore whether other methods that prevent
overﬁtting also have such properties.
Other future directions would be to apply our techniques to other commonly used machine-learning
algorithms  and to explore whether our techniques can be applied to more general optimization
problems. Theorem 3 shows that our method can be applied to a class of optimization problems
with certain restrictions. An open question would be to remove some of these restrictions.
Acknowledgements. We thank Sanjoy Dasgupta and Daniel Hsu for several pointers.

References
[1] R. Agrawal and R. Srikant. Privacy-preserving data mining. SIGMOD Rec.  29(2):439–450  2000.
[2] B. Barak  K. Chaudhuri  C. Dwork  S. Kale  F. McSherry  and K. Talwar. Privacy  accuracy  and consis-

tency too: a holistic solution to contingency table release. In PODS  pages 273–282  2007.

[3] A. Blum  K. Ligett  and A. Roth. A learning theory approach to non-interactive database privacy. In R. E.

Ladner and C. Dwork  editors  STOC  pages 609–618. ACM  2008.

[4] K. Chaudhuri and N. Mishra. When random sampling preserves privacy. In C. Dwork  editor  CRYPTO 

volume 4117 of Lecture Notes in Computer Science  pages 198–213. Springer  2006.

[5] C. Dwork. Differential privacy. In M. Bugliesi  B. Preneel  V. Sassone  and I. Wegener  editors  ICALP

(2)  volume 4052 of Lecture Notes in Computer Science  pages 1–12. Springer  2006.

[6] C. Dwork  F. McSherry  K. Nissim  and A. Smith. Calibrating noise to sensitivity in private data analysis.

In Theory of Cryptography Conference  pages 265–284  2006.

[7] A. Evﬁmievski  J. Gehrke  and R. Srikant. Limiting privacy breaches in privacy preserving data mining.

In PODS  pages 211–222  2003.

[8] S. P. Kasiviswanathan  H. K. Lee  K. Nissim  S. Raskhodnikova  and A. Smith. What can we learn

privately? In Proc. of Foundations of Computer Science  2008.
[9] C. T. Kelley. Iterative Methods for Optimization. SIAM  1999.
[10] A. Machanavajjhala  J. Gehrke  D. Kifer  and M. Venkitasubramaniam.

anonymity. In ICDE  page 24  2006.

l-diversity: Privacy beyond k-

[11] F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS  pages 94–103  2007.
[12] A. Narayanan and V. Shmatikov. Robust de-anonymization of large sparse datasets. In IEEE Symposium

on Security and Privacy  pages 111–125. IEEE Computer Society  2008.

[13] K. Nissim  S. Raskhodnikova  and A. Smith. Smooth sensitivity and sampling in private data analysis. In

D. S. Johnson and U. Feige  editors  STOC  pages 75–84. ACM  2007.

[14] P. Samarati and L. Sweeney. Protecting privacy when disclosing information: k-anonymity and its en-
In Proc. of the IEEE Symposium on Research in

forcement through generalization and suppression.
Security and Privacy  1998.

[15] S. Shalev-Shwartz and N. Srebro. Svm optimization: Inverse dependence on training set size. In Interna-

tional Conference on Machine Learning(ICML)  2008.

[16] K. Sridharan  N. Srebro  and S. Shalev-Schwartz. Fast rates for regularized objectives. In Neural Infor-

mation Processing Systems  2008.

8

,Tamir Hazan
Subhransu Maji
Joseph Keshet
Tommi Jaakkola
Lin Song
Yanwei Li
Zeming Li
Gang Yu
Hongbin Sun
Jian Sun
Nanning Zheng