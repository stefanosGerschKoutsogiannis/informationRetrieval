2016,Clustering Signed Networks with the Geometric Mean of Laplacians,Signed networks allow to model positive and negative relationships. We analyze existing extensions of spectral clustering to signed networks. It turns out that existing approaches do not recover the ground truth clustering in several situations where either the positive or the negative network structures contain no noise. Our analysis shows that these problems arise as existing approaches take some form of arithmetic mean of the Laplacians of the positive and negative part. As a solution we propose to use the geometric mean of the Laplacians of positive and negative part and show that it outperforms the existing approaches. While the geometric mean of matrices is computationally expensive  we show that eigenvectors of the geometric mean can be computed efficiently  leading to a numerical scheme for sparse matrices which is of independent interest.,Clustering Signed Networks with the

Geometric Mean of Laplacians

Pedro Mercado1  Francesco Tudisco2 and Matthias Hein1

1Saarland University  Saarbrücken  Germany

2University of Padua  Padua  Italy

Abstract

Signed networks allow to model positive and negative relationships. We analyze
existing extensions of spectral clustering to signed networks. It turns out that
existing approaches do not recover the ground truth clustering in several situations
where either the positive or the negative network structures contain no noise. Our
analysis shows that these problems arise as existing approaches take some form of
arithmetic mean of the Laplacians of the positive and negative part. As a solution
we propose to use the geometric mean of the Laplacians of positive and negative
part and show that it outperforms the existing approaches. While the geometric
mean of matrices is computationally expensive  we show that eigenvectors of the
geometric mean can be computed efﬁciently  leading to a numerical scheme for
sparse matrices which is of independent interest.

1

Introduction

A signed graph is a graph with positive and negative edge weights. Typically positive edges model
attractive relationships between objects such as similarity or friendship and negative edges model
repelling relationships such as dissimilarity or enmity. The concept of balanced signed networks
can be traced back to [10  3]. Later  in [5]  a signed graph is deﬁned as k-balanced if there exists
a partition into k groups where only positive edges are within the groups and negative edges are
between the groups. Several approaches to ﬁnd communities in signed graphs have been proposed
(see [23] for an overview). In this paper we focus on extensions of spectral clustering to signed
graphs. Spectral clustering is a well established method for unsigned graphs which  based on the
ﬁrst eigenvectors of the graph Laplacian  embeds nodes of the graphs in Rk and then uses k-means
to ﬁnd the partition. In [16] the idea is transferred to signed graphs. They deﬁne the signed ratio
and normalized cut functions and show that the spectrum of suitable signed graph Laplacians yield a
relaxation of those objectives. In [4] other objective functions for signed graphs are introduced. They
show that a relaxation of their objectives is equivalent to weighted kernel k-means by choosing an
appropriate kernel. While they have a scalable method for clustering  they report that they can not
ﬁnd any cluster structure in real world signed networks.
We show that the existing extensions of the graph Laplacian to signed graphs used for spectral
clustering have severe deﬁciencies. Our analysis of the stochastic block model for signed graphs
shows that  even for the perfectly balanced case  recovery of the ground-truth clusters is not guaranteed.
The reason is that the eigenvectors encoding the cluster structure do not necessarily correspond to
the smallest eigenvalues  thus leading to a noisy embedding of the data points and in turn failure
of k-means to recover the cluster structure. The implicit mathematical reason is that all existing
extensions of the graph Laplacian are based on some form of arithmetic mean of operators of the
positive and negative graphs. In this paper we suggest as a solution to use the geometric mean of
the Laplacians of positive and negative part. In particular  we show that in the stochastic block
model the geometric mean Laplacian allows in expectation to recover the ground-truth clusters in

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

any reasonable clustering setting. A main challenge for our approach is that the geometric mean
Laplacian is computationally expensive and does not scale to large sparse networks. Thus a main
contribution of this paper is showing that the ﬁrst few eigenvectors of the geometric mean can still be
computed efﬁciently. Our algorithm is based on the inverse power method and the extended Krylov
subspace technique introduced by [8] and allows to compute eigenvectors of the geometric mean
A#B of two matrices A  B without ever computing A#B itself.
In Section 2 we discuss existing work on Laplacians on signed graphs. In Section 3 we discuss the
geometric mean of two matrices and introduce the geometric mean Laplacian which is the basis of our
spectral clustering method for signed graphs. In Section 4 we analyze our and existing approaches for
the stochastic block model. In Section 5 we introduce our efﬁcient algorithm to compute eigenvectors
of the geometric mean of two matrices  and ﬁnally in Section 6 we discuss performance of our
approach on real world graphs. Proofs have been moved to the supplementary material.

2 Signed graph clustering

Networks encoding positive and negative relations among the nodes can be represented by weighted
signed graphs. Consider two symmetric non-negative weight matrices W + and W −  a vertex set
V = {v1  . . .   vn}  and let G+ = (V  W +) and G− = (V  W −) be the induced graphs. A signed
graph is the pair G± = (G+  G−) where G+ and G− encode positive and the negative relations 
respectively.
The concept of community in signed networks is typically related to the theory of social balance.
This theory  as presented in [10  3]  is based on the analysis of affective ties  where positive ties are a
source of balance whereas negative ties are considered as a source of imbalance in social groups.
Deﬁnition 1 ([5]  k-balance). A signed graph is k-balanced if the set of vertices can be partitioned
into k sets such that within the subsets there are only positive edges  and between them only negative.
The presence of k-balance in G± implies the presence of k groups of nodes being both assortative
in G+ and dissassortative in G−. However this situation is fairly rare in real world networks and
expecting communities in signed networks to be a perfectly balanced set of nodes is unrealistic.
In the next section we will show that Laplacians inspired by Deﬁnition 1 are based on some form of
arithmetic mean of Laplacians. As an alternative we propose the geometric mean of Laplacians and
show that it is able to recover communities when either G+ is assortative  or G− is disassortative  or
both. Results of this paper will make clear that the use of the geometric mean of Laplacians allows to
recognize communities where previous approaches fail.

2.1 Laplacians on Unsigned Graphs

Spectral clustering of undirected  unsigned graphs using the Laplacian matrix is a well established
technique (see [19] for an overview). Given an unsigned graph G = (V  W )  the Laplacian and its
normalized version are deﬁned as

L = D − W

Lsym = D−1/2LD−1/2

(1)

where Dii =(cid:80)n

j=1 wij is the diagonal matrix of the degrees of G. Both Laplacians are positive
semideﬁnite  and the multiplicity k of the eigenvalue 0 is equal to the number of connected compo-
nents in the graph. Further  the Laplacian is suitable in assortative cases [19]  i.e. for the identiﬁcation
of clusters under the assumption that the amount of edges inside clusters has to be larger than the
amount of edges between them.
For disassortative cases  i.e. for the identiﬁcation of clusters where the amount of edges has to be
larger between clusters than inside clusters  the signless Laplacian is a better choice [18]. Given the
unsigned graph G = (V  W )  the signless Laplacian and its normalized version are deﬁned as

Q = D + W 

Qsym = D−1/2QD−1/2

(2)

Both Laplacians are positive semi-deﬁnite  and the smallest eigenvalue is zero if and only if the graph
has a bipartite component [6].

2

2.2 Laplacians on Signed Graphs

and ¯Dii =(cid:80)n

ii =(cid:80)n

Recently a number of Laplacian operators for signed networks have been introduced. Consider the
signed graph G± = (G+  G−). Let D+
ij be the diagonal matrix of the degrees of G+

j=1 w+

j=1 w+

ij + w−

ij the one of the overall degrees in G±.

The following Laplacians for signed networks have been considered so far

LBR = D+ − W ++W −  LBN = ¯D−1LBR 
LSR = ¯D − W ++W −  LSN = ¯D−1/2LSR ¯D−1/2 

(3)
and spectral clustering algorithms have been proposed for G±  based on these Laplacians [16  4].
Let L+ and Q− be the Laplacian and the signless Laplacian matrices of the graphs G+ and G− 
respectively. We note that the matrix LSR blends the informations from G+ and G− into (twice) the
arithmetic mean of L+ and Q−  namely the following identity holds

(balance ratio/normalized Laplacian)

(signed ratio/normalized Laplacian)

(4)
Thus  as an alternative to the normalization deﬁning LSN from LSR  it is natural to consider the
arithmetic mean of the normalized Laplacians LAM = L+
sym. In the next section we
introduce the geometric mean of L+
sym and propose a new clustering algorithm for signed
graphs based on that matrix. The analysis and experiments of next sections will show that blending
the information from the positive and negative graphs trough the geometric mean overcomes the
deﬁciencies showed by the arithmetic mean based operators.

sym and Q−

sym + Q−

LSR = L+ + Q− .

3 Geometric mean of Laplacians

We deﬁne here the geometric mean of matrices and introduce the geometric mean of normalized
Laplacians for clustering signed networks. Let A1/2 be the unique positive deﬁnite solution of the
matrix equation X 2 = A  where A is positive deﬁnite.
Deﬁnition 2. Let A  B be positive deﬁnite matrices. The geometric mean of A and B is the positive
deﬁnite matrix A#B deﬁned by A#B = A1/2(A−1/2BA−1/2)1/2A1/2.
One can prove that A#B = B#A (see [1] for details). Further  there are several useful ways to
represent the geometric mean of positive deﬁnite matrices (see f.i. [1  12])

A#B = A(A−1B)1/2 = (BA−1)1/2A = B(B−1A)1/2 = (AB−1)1/2B

(5)
The next result reveals further consistency with the scalar case  in fact we observe that if A and B have
some eigenvectors in common  then A + B and A#B have those eigenvectors  with eigenvalues given
by the arithmetic and geometric mean of the corresponding eigenvalues of A and B  respectively.
Theorem 1. Let u be an eigenvector of A and B with eigenvalues λ and µ  respectively. Then  u is
an eigenvector of A + B and A#B with eigenvalue λ + µ and

λµ  respectively.

√

3.1 Geometric mean for signed networks clustering
Consider the signed network G± = (G+  G−). We deﬁne the normalized geometric mean Laplacian
of G± as

LGM = L+

(6)
We propose Algorithm 1 for clustering signed networks  based on the spectrum of LGM . By
deﬁnition 2  the matrix geometric mean A#B requires A and B to be positive deﬁnite. As both
the Laplacian and the signless Laplacian are positve semi-deﬁnte  in what follows we shall assume
sym in (6) are modiﬁed by a small diagonal shift  ensuring positive
that the matrices L+
deﬁniteness. That is  in practice  we consider L+
sym + ε2I being ε1 and ε2
small positive numbers. For the sake of brevity  we do not explicitly write the shifting matrices.
Input: Symmetric weight matrices W +  W − ∈ Rn×n  number k of clusters to construct.
Output: Clusters C1  . . .   Ck.

sym + ε1I and Q−

sym and Q−

sym#Q−

sym

1 Compute the k eigenvectors u1  . . .   uk corresponding to the k smallest eigenvalues of LGM .
2 Let U = (u1  . . .   uk).
3 Cluster the rows of U with k-means into clusters C1  . . .   Ck.

Algorithm 1: Spectral clustering with LGM on signed networks

3

(E+)
(E−)
(Ebal)

p+
out < p+
in
p−
in < p−
p−
in + p+

out
out < p+

in + p−

out

(Evol)

(Econf )

(EG)

out < p+

(cid:17)(cid:16)
(cid:16)
in + (k − 1)p−
p−
(cid:17)(cid:16)
(cid:16)

in+(k−1)p+
p+

kp+

out

out

kp+

out

in+(k−1)p+
p+

out

out

(cid:17)
in + (k − 1)p+
(cid:17)

−
−
in+(k−1)p
p
out
−
−
in−p
1 + p
out
−
in+(k−1)p

−
out

−
in

< 1

kp

p

< 1

Table 1: Conditions for the Stochastic Block Model analysis of Section 4

The main bottleneck of Algorithm 1 is the computation of the eigenvectors in step 1. In Section 5 we
propose a scalable Krylov-based method to handle this problem.
Let us brieﬂy discuss the motivating intuition behind the proposed clustering strategy. Algorithm 1 
as well as state-of-the-art clustering algorithms based on the matrices in (3)  rely on the k smallest
eigenvalues of the considered operator and their corresponding eigenvectors. Thus the relative
ordering of the eigenvalues plays a crucial role. Assume the eigenvalues to be enumerated in
ascending order. Theorem 1 states that the functions (A  B) (cid:55)→ A + B and (A  B) (cid:55)→ A#B map
eigenvalues of A and B having the same corresponding eigenvectors  into the arithmetic mean

λi(A) + λj(B) and geometric mean(cid:112)λi(A)λj(B)  respectively  where λi(·) is the ith smallest

eigenvalue of the corresponding matrix. Note that the indices i and j are not the same in general 
as the eigenvectors shared by A and B may be associated to eigenvalues having different positions
in the relative ordering of A and B. This intuitively suggests that small eigenvalues of A + B are
related to small eigenvalues of both A and B  whereas those of A#B are associated with small
eigenvalues of either A or B  or both. Therefore the relative ordering of the small eigenvalues of
LGM is inﬂuenced by the presence of assortative clusters in G+ (related to small eigenvalues of
sym) or by disassortative clusters in G− (related to small eigenvalues in Q−
sym)  whereas the ordering
L+
of the small eigenvalues of the arithmetic mean takes into account only the presence of both those
situations.
In the next section  for networks following the stochastic block model  we analyze in expectation
the spectrum of the normalized geometric mean Laplacian as well as the one of the normalized
Laplacians previously introduced. In this case the expected spectrum can be computed explicitly and
we observe that in expectation the ordering induced by blending the informations of G+ and G−
trough the geometric mean allows to recover the ground truth clusters perfectly  whereas the use of
the arithmetic mean introduces a bias which reverberates into a signiﬁcantly higher clustering error.

4 Stochastic block model on signed graphs

in (p−

out (p−

In this section we present an analysis of different signed graph Laplacians based on the Stochastic
Block Model (SBM). The SBM is a widespread benchmark generative model for networks showing a
clustering  community  or group behaviour [22]. Given a prescribed set of groups of nodes  the SBM
deﬁnes the presence of an edge as a random variable with probability being dependent on which
groups it joins. To our knowledge this is the ﬁrst analysis of spectral clustering on signed graphs
with the stochastic block model. Let C1  . . .  Ck be ground truth clusters  all having the same size |C|.
We let p+
in) be the probability that there exists a positive (negative) edge between nodes in the
same cluster  and let p+
out) denote the probability of a positive (negative) edge between nodes in
different clusters.
Calligraphic letters denote matrices in expectation. In particular W + and W− denote the weight
matrices in expectation. We have W +
in if vi  vj belong to the same cluster 
whereas W +
i j = p−
out if vi  vj belong to different clusters. Sorting nodes according
to the ground truth clustering shows that W + and W− have rank k.
Consider the relations in Table 1. Conditions E+ and E− describe the presence of assortative or
disassortative clusters in expectation. Note that  by Deﬁnition 1  a graph is balanced if and only if
in = 0. We can see that if E+ ∩ E− then G− and G+ give information about the cluster
out = p−
p+
structure. Further  if E+ ∩ E− holds then Ebal holds. Similarly Econf characterizes a graph where
the relative amount of conﬂicts - i.e. positive edges between the clusters and negative edges inside the
clusters - is small. Condition EG is strictly related to such setting. In fact when E− ∩ EG holds then

out and W−

in and W−

i j = p−

i j = p+

i j = p+

4

Econf holds. Finally condition Evol implies that the expected volume in the negative graph is smaller
than the expected volume in the positive one. This condition is therefore not related to any signed
clustering structure.
Let

χ1 = 1 

χi = (k − 1)1Ci − 1Ci

.

The use of k-means on χi  i = 1  . . .   k identiﬁes the ground truth communities Ci. As spectral
clustering relies on the eigenvectors corresponding to the k smallest eigenvalues (see Algorithm 1)
we derive here necessary and sufﬁcient conditions such that in expectation the eigenvectors χi  i =
1  . . .   k correspond to the k smallest eigenvalues of the normalized Laplacians introduced so far. In
particular  we observe that condition EG affects the ordering of the eigenvalues of the normalized
geometric mean Laplacian. Instead  the ordering of the eigenvalues of the operators based on the
arithmetic mean is related to Ebal and Evol. The latter is not related to any clustering  thus introduces
a bias in the eigenvalues ordering which reverberates into a noisy embedding of the data points and in
turn into a signiﬁcantly higher clustering error.
Theorem 2. Let LBN and LSN be the normalized Laplacians deﬁned in (3) of the expected graphs.
The following statements are equivalent:

1. χ1  . . .   χk are the eigenvectors corresponding to the k smallest eigenvalues of LBN .
2. χ1  . . .   χk are the eigenvectors corresponding to the k smallest eigenvalues of LSN .
3. The two conditions Ebal and Evol hold simultaneously.

Theorem 3. Let LGM = L+
sym be the geometric mean of the Laplacians of the expected
graphs. Then χ1  . . .   χk are the eigenvectors corresponding to the k smallest eigenvalues of LGM
if and only if condition EG holds.

sym#Q−

Conditions for the geometric mean Laplacian of diagonally shifted Laplacians are available in the
supplementary material. Intuition suggests that a good model should easily identify clusters when
E+ ∩ E−. However  unlike condition EG  condition Evol ∩ Ebal is not directly satisﬁed under that
regime. Speciﬁcally  we have
Corollary 1. Assume that E+ ∩ E− holds. Then χ1  . . .   χk are eigenvectors corresponding to the
k smallest eigenvalues of LGM . Let p(k) denote the proportion of cases where χ1  . . .   χk are the
eigenvectors of the k smallest eigenvalues of LSN or LBN   then p(k) ≤ 1

6 + 2

3(k−1) + 1

(k−1)2 .

In order to grasp the difference in expectation between LBN   LSN and LGM   in Fig 1 we present the
proportion of cases where Theorems 2 and 3 hold under different contexts. Experiments are done with
all four parameters discretized in [0  1] with 100 steps. The expected proportion of cases where EG
holds (Theorem 3) is far above the corresponding proportion for Evol ∩ Ebal (Theorem 2)  showing
that in expectation the geometric mean Laplacian is superior to the other signed Laplacians.
In
Fig. 2 we present experiments on sampled graphs with k-means on top of the k smallest eigenvectors.
In all cases we consider clusters of size |C| = 100 and present the median of clustering error (i.e. 
error when clusters are labeled via majority vote) of 50 runs. The results show that the analysis
made in expectation closely resembles the actual behavior. In fact  even if we expect only one noisy
eigenvector for LBN and LSN   the use of the geometric mean Laplacian signiﬁcantly outperforms
any other previously proposed technique in terms of clustering error. LSN and LBN achieve good
clustering only when the graph resembles a k-balanced structure  whereas they fail even in the ideal
situation where either the positive or the negative graphs are informative about the cluster structure.
As shown in Section 6  the advantages of LGM over the other Laplacians discussed so far allow us to
identify a clustering structure on the Wikipedia benchmark real world signed network  where other
clustering approaches have failed.

5 Krylov-based inverse power method for small eigenvalues of L+

sym#Q−

sym

The computation of the geometric mean A#B of two positive deﬁnite matrices of moderate size
has been discussed extensively by various authors [20  11  12  13]. However  when A and B have
large dimensions  the approaches proposed so far become unfeasible  in fact A#B is in general a full
matrix even if A and B are sparse. In this section we present a scalable algorithm for the computation
of the smallest eigenvectors of L+
sym. The method is discussed for a general pair of matrices
A and B  to emphasize its general applicability which is therefore interesting in itself. We remark that

sym#Q−

5

Figure 1: Fraction of cases where in expectation χ1  . . .   χk correspond to the k smallest eigenvalues
under the SBM.

Figure 2: Median clustering error under the stochastic block model over 50 runs.

the method takes advantage of the sparsity of A and B and does not require to explicitly compute the
matrix A#B. To our knowledge this is the ﬁrst effective method explicitly built for the computation
of the eigenvectors of the geometric mean of two large and sparse positive deﬁnite matrices.
Given a positive deﬁnite matrix M with eigenvalues λ1 ≤ ··· ≤ λn  let H be any eigenspace of M
associated to λ1  . . .   λt. The inverse power method (IPM) applied to M is a method that converges
to an eigenvector x associated to the smallest eigenvalue λH of M such that λH (cid:54)= λi  i = 1  . . .   t.
The pseudocode of IPM applied to A#B = A(A−1B)1/2 is shown in Algorithm 2. Given a vector
v and a matrix M  the notation solve{M  v} is used to denote a procedure returning the solution
x of the linear system M x = v. At each step the algorithm requires the solution of two linear
systems. The ﬁrst one (line 2) is solved by the preconditioned conjugate gradient method  where the
preconditioner is obtained by the incomplete Cholesky decomposition of A. Note that the conjugate
gradient method is very fast  as A is assumed sparse and positive deﬁnite  and it is matrix-free  i.e. it
requires to compute the action of A on a vector  whereas it does not require the knowledge of A (nor
its inverse). The solution of the linear system occurring in line 3 is the major inner-problem of the
proposed algorithm. Its efﬁcient solution is performed by means of an extended Krylov subspace
technique that we describe in the next section. The proposed implementation ensures the whole IPM
is matrix-free and scalable.

5.1 Extended Krylov subspace method for the solution of the linear system (A−1B)1/2x = y
We discuss here how to apply the technique known as Extended Krylov Subspace Method (EKSM) for
the solution of the linear system (A−1B)1/2x = y. Let M be a large and sparse matrix  and y a given
vector. When f is a function with a single pole  EKSM is a very effective method to approximate
the vector f (M )y without ever computing the matrix f (M ) [8]. Note that  given two positive
deﬁnite matrices A and B and a vector y  the vector we want to compute is x = (A−1B)−1/2y 
so that our problem boils down to the computation of the product f (M )y  where M = A−1B and
f (X) = X−1/2. The general idea of EKSM s-th iteration is to project M onto the subspace

Ks(M  y) = span{y  M y  M−1y  . . .   M s−1y  M 1−sy}  

and solve the problem there. The projection onto Ks(M  y) is realized by means of the Lanczos
process  which produces a sequence of matrices Vs with orthogonal columns  such that the ﬁrst

6

2 5 102550100Numberofclusters00.20.40.60.81PositiveandNegativeInformative:p+out<p+inandp−in<p−outUpperboundLSN LBNLGM(ours)2 5 102550100NumberofclustersPositiveorNegativeInformative:p+out<p+inorp−in<p−out2 5 102550100Numberofclustersp−in+p+out<p+in+p−out-0.050 0.05 PositiveInformation:p+in−p+out00.10.20.30.4MedianClusteringErrorNegativeInformativep−in=0.01 p−out=0.09-0.050 0.05 NegativeInformation:p−in−p−outPositiveInformativep+in=0.09 p+out=0.012 5 7 10Sparsity(%)NegativeInformativep−out/p−in=9/1p+out/p+in=1±0.32 5 7 10Sparsity(%)PositiveInformativep+out/p+in=1/9p−out/p−in=1±0.3L+symQ−symLSNLBNLAMLGM(ours)column of Vs is a multiple of y and range(Vs) = Ks(M  y). Moreover at each step we have

M Vs = VsHs + [us+1  vs+1][e2s+1  e2s+2]T

(7)
where Hs is 2s × 2s symmetric tridiagonal  us+1 and vs+1 are orthogonal to Vs  and ei is the i-th
canonical vector. The solution x is then approximated by xs = Vsf (Hs)e1(cid:107)y(cid:107) ≈ f (M )y. If n
is the order of M  then the exact solution is obtained after at most n steps. However  in practice 
signiﬁcantly fewer iterations are enough to achieve a good approximation  as the error (cid:107)xs − x(cid:107)
decays exponentially with s (Thm 3.4 and Prop. 3.6 in [14]). See the supplementary material for
details.
The pseudocode for the extended Krylov iteration is presented in Algorithm 3. We use the stopping
criterion proposed in [14]. It is worth pointing out that at step 4 of the algorithm we can freely
choose any scalar product (cid:104)· ·(cid:105)  without affecting formula (7) nor the convergence properties of
the method. As M = A−1B  we use the scalar product (cid:104)u  v(cid:105)A = uT Av induced by the positive
deﬁnite matrix A  so that the computation of the tridiagonal matrix Hs in the algorithm simpliﬁes
to V T
s BVs. We refer to [9] for further details. As before  the solve procedure is implemented
by means of the preconditioned conjugate gradient method  where the preconditioner is obtained
by the incomplete Cholesky decomposition of the coefﬁcient matrix. Figure 3 shows that we are
able to compute the smallest eigenvector of L+
sym being just a constant factor worse than
the computation of the eigenvector of the arithmetic mean  whereas the direct computation of the
geometric mean followed by the computation of the eigenvectors is unfeasible for large graphs.

sym#Q−

3

Input: x0  eigenspace H of A#B.
Output: Eigenpair (λH  x) of A#B
1 repeat
2

uk ← solve{A  xk}
vk ← solve{(A−1B)1/2  uk}
yk ← project uk over H⊥
4
xk+1 ← yk/(cid:107)yk(cid:107)2
5
6 until tolerance reached
7 λH ← xT
Algorithm 2:
A#B.1/2

k+1xk  x ← xk+1

IPM applied

to

5

4

Input: u0 = y  V0 = [· ]
Output: x = (A−1B)−1/2y
1 v0 ← solve{B  Au0}
2 for s = 0  1  2  . . .   n do
˜Vs+1 ← [Vs  us  vs]
3
Vs+1 ← Orthogonalize columns of ˜Vs+1 w.r.t. (cid:104)· ·(cid:105)A
Hs+1 ← V T
xs+1 ← H
if tolerance reached then break
us+1 ← solve{A  BVs+1e1}
vs+1 ← solve{B  AVs+1e2}

s+1BVs+1
−1/2
s+1 e1

6
7
8
9
10 end
11 x ← Vs+1xs+1
Algorithm 3: EKSM for
(A−1B)−1/2y

the

computation of

Figure 3: Median execution time of 10 runs for different Lapla-
cians. Graphs have two perfect clusters and 2.5% of edges
among nodes. LGM (ours) uses Algs 2 and 3  whereas we
used Matlab’s eigs for the other matrices. The use of eigs
on LGM is prohibitive as it needs the matrix LGM to be built
(we use the toolbox provided in [2])  destroying the sparsity
of the original graphs. Experiments are performed using one
thread.

6 Experiments

Sociology Networks We evaluate signed Laplacians LSN   LBN   LAM and LGM through three real-
world and moderate size signed networks: Highland tribes (Gahuku-Gama) network [21]  Slovene
Parliamentary Parties Network [15] and US Supreme Court Justices Network [7]. For the sake of
comparison we take as ground truth the clustering that is stated in the corresponding references. We
observe that all signed Laplacians yield zero clustering error.
Experiments on Wikipedia signed network. We consider the Wikipedia adminship election dataset
from [17]  which describes relationships that are positive  negative or non existent. We use Algs. 1−3
and look for 30 clusters. Positive and negative adjacency matrices sorted according to our clustering
are depicted in Figs. 4(a) and 4(b). We can observe the presence of a large relatively empty cluster.

7

246810sizeofgraph×104100102104Mediantime(sec)LSN(eigs)LBN(eigs)LGM(eigs)LGM(ours)Zooming into the denser portion of the graph we can see a k-balanced behavior (see Figs. 4(c) and
4(d))  i.e. the positive adjacency matrix shows assortative groups - resembling a block diagonal
structure - while the negative adjacency matrix shows a disassortative setting. Using LAM and LBN
we were not able to ﬁnd any clustering structure  which corroborates results reported in [4]. This
further conﬁrms that LGM overcomes other clustering approaches. To the knowledge of the authors 
this is the ﬁrst time that clustering structure has been found in this dataset.

(a) W +

(b) W −

(c) W +(Zoom)

(d) W −(Zoom)

Figure 4: Wikipedia weight matrices sorted according to the clustering obtained with LGM (Alg. 1).
Experiments on UCI datasets. We evaluate our method LGM (Algs. 1−3) against LSN   LBN  
and LAM with datasets from the UCI repository (see Table. 2). We build W + from a symmetric
k+-nearest neighbor graph  whereas W − is obtained from the symmetric k−-farthest neighbor
graph. For each dataset we test all clustering methods over all possible choices of k+  k− ∈
{3  5  7  10  15  20  40  60}. In Table 2 we report the fraction of cases where each method achieves
the best and strictly best clustering error over all the 64 graphs  per each dataset. We can see that our
method outperforms other methods across all datasets.
In the ﬁgure on the right of Table 2 we present the clustering error on MNIST dataset ﬁxing k+ = 10.
With Q−
sym one gets the highest clustering error  which shows that the k−-farthest neighbor graph is a
source of noise and is not informative. In fact  we observe that a small subset of nodes is the farthest
neighborhood of a large fraction of nodes. The noise from the k−-farthest neighbor graph is strongly
inﬂuencing the performances of LSN and LBN   leading to a noisy embedding of the datapoints and
in turn to a high clustering error. On the other hand we can see that LGM is robust  in the sense that
its clustering performances are not affected negatively by the noise in the negative edges. Similar be-
haviors have been observed for the other datasets in Table 2  and are shown in supplementary material.

a

MNIST  k+ = 10

iris wine ecoli optdig USPS pendig MNIST
150
70000
3

178
3

310
3

10992

# vertices
# classes

LSN

LBN

LAM

LGM

Best (%)

Best (%)

Str. best (%) 7.8

17.2 21.9
4.7

23.4 40.6 18.8
Str. best (%) 10.9 21.9 14.1
7.8
6.3
12.5 28.1 14.1
Str. best (%) 10.9 14.1 12.5
59.4 42.2 65.6
Str. best (%) 57.8 35.9 60.9

Best (%)

Best (%)

5620
10
28.1
28.1
0.0
0.0
0.0
0.0
71.9
71.9

9298
10
10.9
9.4
1.6
1.6
0.0
0.0
89.1
87.5

10
10.9
10.9
3.1
3.1
1.6
1.6
84.4
84.4

10
12.5
12.5
0.0
0.0
0.0
0.0
87.5
87.5

Table 2: Experiments on UCI datasets. Left: fraction of cases where methods achieve best and strictly
best clustering error. Right: clustering error on MNIST dataset.

a

k−

Acknowledgments. The authors acknowledge support by the ERC starting grant NOLEPRO

8

57101520406000.20.40.6L+symQ-symLSNLBNLAMLGM(ours)References
[1] R. Bhatia. Positive deﬁnite matrices. Princeton University Press  2009.
[2] D. Bini and B. Ianazzo. The Matrix Means Toolbox. http://bezout.dm.unipi.it/

software/mmtoolbox/  May 2015.

[3] D. Cartwright and F. Harary. Structural balance: a generalization of Heider’s theory. Psycholog-

ical Review  63(5):277–293  1956.

[4] K. Chiang  J. Whang  and I. Dhillon. Scalable clustering of signed networks using balance

normalized cut. CIKM  pages 615–624  2012.

[5] J. A. Davis. Clustering and structural balance in graphs. Human Relations  20:181–187  1967.
[6] M. Desai and V. Rao. A characterization of the smallest eigenvalue of a graph. Journal of

Graph Theory  18(2):181–194  1994.

[7] P. Doreian and A. Mrvar. Partitioning signed social networks. Social Networks  31(1):1–11 

2009.

[8] V. Druskin and L. Knizhnerman. Extended Krylov subspaces: approximation of the matrix

square root and related functions. SIAM J. Matrix Anal. Appl.  19:755–771  1998.

[9] M. Fasi and B. Iannazzo. Computing the weighted geometric mean of two large-scale matrices

and its inverse times a vector. MIMS EPrint: 2016.29.

[10] F. Harary. On the notion of balance of a signed graph. Michigan Mathematical Journal 

2:143–146  1953.

[11] N. J. Higham  D. S. Mackey  N. Mackey  and F. Tisseur. Functions preserving matrix groups

and iterations for the matrix square root. SIAM J. Matrix Anal. Appl.  26:849–877  2005.

[12] B. Iannazzo. The geometric mean of two matrices from a computational viewpoint. Numer.

Linear Algebra Appl.  to appear  2015.

[13] B. Iannazzo and M. Porcelli. The Riemannian Barzilai-Borwein method with nonmonotone

line-search and the Karcher mean computation. Optimization online  December 2015.

[14] L. Knizhnerman and V. Simoncini. A new investigation of the extended Krylov subspace

method for matrix function evaluations. Numer. Linear Algebra Appl.  17:615–638  2009.

[15] S. Kropivnik and A. Mrvar. An Analysis of the Slovene Parliamentary Parties Networks.

Development in Statistics and Methodology  pages 209–216  1996.

[16] J. Kunegis  S. Schmidt  A. Lommatzsch  J. Lerner  E. Luca  and S. Albayrak. Spectral analysis
of signed graphs for clustering  prediction and visualization. In ICDM  pages 559–570  2010.
[17] J. Leskovec and A. Krevl. SNAP Datasets: Stanford Large Network Dataset Collection.

http://snap.stanford.edu/data  June 2014.

[18] S. Liu. Multi-way dual cheeger constants and spectral bounds of graphs. Advances in Mathe-

matics  268:306 – 338  2015.

[19] U. Luxburg. A tutorial on spectral clustering. Statistics and Computing  17(4):395–416  Dec.

2007.

[20] M. Raïssouli and F. Leazizi. Continued fraction expansion of the geometric matrix mean and

applications. Linear Algebra Appl.  359:37–57  2003.

[21] K. E. Read. Cultures of the Central Highlands  New Guinea. Southwestern Journal of Anthro-

pology  10(1):pp. 1–43  1954.

[22] K. Rohe  S. Chatterjee  B. Yu  et al. Spectral clustering and the high-dimensional stochastic

blockmodel. The Annals of Statistics  39(4):1878–1915  2011.

[23] J. Tang  Y. Chang  C. Aggarwal  and H. Liu. A survey of signed network mining in social media.

arXiv preprint arXiv:1511.07569  2015.

9

,Pedro Mercado
Francesco Tudisco
Matthias Hein