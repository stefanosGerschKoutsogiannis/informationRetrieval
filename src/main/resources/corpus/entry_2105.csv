2012,Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods,We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients  analyzing their finite-sample convergence rates. We show that if pairs of function values are available  algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\sqrt{\dim}$ in convergence rate over traditional stochastic gradient methods  where $\dim$ is the dimension of the problem. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems  which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors.,Finite Sample Convergence Rates of Zero-Order

Stochastic Optimization Methods

John C. Duchi1 Michael I. Jordan1 2 Martin J. Wainwright1 2
Andre Wibisono1
1Department of Electrical Engineering and Computer Science and 2Department of Statistics

University of California  Berkeley

Berkeley  CA USA 94720

{jduchi jordan wainwrig wibisono}@eecs.berkeley.edu

Abstract

We consider derivative-free algorithms for stochastic optimization problems that
use only noisy function values rather than gradients  analyzing their ﬁnite-sample
convergence rates. We show that if pairs of function values are available  algo-
rithms that use gradient estimates based on random perturbations suffer a factor
of at most √d in convergence rate over traditional stochastic gradient methods 
where d is the problem dimension. We complement our algorithmic develop-
ment with information-theoretic lower bounds on the minimax convergence rate of
such problems  which show that our bounds are sharp with respect to all problem-
dependent quantities: they cannot be improved by more than constant factors.

1

Introduction

Derivative-free optimization schemes have a long history in optimization (see  for example  the
book by Spall [21])  and they have the clearly desirable property of never requiring explicit gradient
calculations. Classical techniques in stochastic and non-stochastic optimization  including Kiefer-
Wolfowitz-type procedures [e.g. 17]  use function difference information to approximate gradients
of the function to be minimized rather than calculating gradients. Researchers in machine learning
and statistics have studied online convex optimization problems in the bandit setting  where a player
and adversary compete  with the player choosing points θ in some domain Θ and an adversary
choosing a point x  forcing the player to suffer a loss F (θ; x)  where F (·; x) :Θ → R is a convex
function [13  5  1]. The goal is to choose optimal θ based only on observations of function values
F (θ; x). Applications including online auctions and advertisement selection in search engine results.
Additionally  the ﬁeld of simulation-based optimization provides many examples of problems in
which optimization is performed based only on function values [21  10]  and problems in which
the objective is deﬁned variationally (as the maximum of a family of functions)  such as certain
graphical model and structured-prediction problems  are also natural because explicit differentiation
may be difﬁcult [23].

Despite the long history and recent renewed interest in such procedures  an understanding of their
ﬁnite-sample convergence rates remains elusive.
In this paper  we study algorithms for solving
stochastic convex optimization problems of the form

min
θ∈Θ

f (θ) := EP [F (θ; X)] =!X

(1)
where Θ ⊆ Rd is a compact convex set  P is a distribution over the space X   and for P -almost every
x ∈X   the function F (·; x) is closed convex. Our focus is on the convergence rates of algorithms
that observe only stochastic realizations of the function values f (θ).

F (θ; x)dP (x) 

Work on this problem includes Nemirovski and Yudin [18  Chapter 9.3]  who develop a randomized
sampling strategy that estimates ∇F (θ; x) using samples from the surface of the "2-sphere  and

1

Flaxman et al. [13]  who build on this approach  applying it to bandit convex optimization problems.
The convergence rates in these works are (retrospectively) sub-optimal [20  2]: Agarwal et al. [2]

provide algorithms that achieve convergence rates (ignoring logarithmic factors) of O(poly(d)/√k) 

where poly(d) is a polynomial in the dimension d  for stochastic algorithms receiving only single
function values  but (as the authors themselves note) the algorithms are quite complicated.

Some of the difﬁculties inherent in optimization using only a single function evaluation can be alle-
viated when the function F (·; x) can be evaluated at two points  as noted independently by Agarwal
et al. [1] and Nesterov [20]. The insight is that for small u  the quantity (F (θ + uZ; x)− F (θ; x))/u
approximates a directional derivative of F (θ; x) and can thus be used in ﬁrst-order optimization
schemes. Such two-sample-based gradient estimators allow simpler analyses  with sharper conver-
gence rates [1  20]  than algorithms that have access to only a single function evaluation in each
iteration. In the current paper  we take this line of work further  ﬁnding the optimal rate of con-
vergence for procedures that are only able to obtain function evaluations  F (·; X)  for samples X.
Moreover  adopting the two-point perspective  we present simple randomization-based algorithms
that achieve these optimal rates.
More formally  we study algorithms that receive paired observations Y (θ  τ ) ∈ R2  where θ and τ
are points the algorithm selects  and the tth sample is

Y t(θt τ t) :="F (θt; X t)
F (τ t; X t)#

(2)

where X t is a sample drawn from the distribution P . After k iterations  the algorithm returns a

vector$θ(k) ∈ Θ. In this setting  we analyze stochastic gradient and mirror-descent procedures [27 
18  6  19] that construct gradient estimators using the two-point observations Y t. By a careful
analysis of the dimension dependence of certain random perturbation schemes  we show that the
convergence rate attained by our stochastic gradient methods is roughly a factor of √d worse than
that attained by stochastic methods that observe the full gradient ∇F (θ; X). Under appropriate
conditions  our convergence rates are a factor of √d better than those attained by Agarwal et al.
[1] and Nesterov [20]. In addition  though we present our results in the framework of stochastic
optimization  our analysis applies to (two-point) bandit online convex optimization problems [13 
5  1]  and we consequently obtain the sharpest rates for such problems. Finally  we show that
the convergence rates we provide are tight—meaning sharp to within constant factors—by using
information-theoretic techniques for constructing lower bounds on statistical estimators.

2 Algorithms

Stochastic mirror descent methods are a class of stochastic gradient methods for solving the problem
minθ∈Θ f (θ). They are based on a proximal function ψ  which is a differentiable convex function
deﬁned over Θ that is assumed (w.l.o.g. by scaling) to be 1-strongly convex with respect to the norm
’·’ over Θ. The proximal function deﬁnes a Bregman divergence Dψ :Θ × Θ → R+ via

Dψ(θ  τ ) := ψ(θ) − ψ(τ ) − )∇ψ(τ ) θ − τ* ≥

1
2 ’θ − τ’2  

(3)

where the inequality follows from the strong convexity of ψ over Θ. The mirror descent (MD)
method proceeds in a sequence of iterations that we index by t  updating the parameter vector θt ∈
Θ using stochastic gradient information to form θt+1. At iteration t the MD method receives a
(subgradient) vector gt ∈ Rd  which it uses to update θt via
1

(4)

θt+1 = argmin

θ∈Θ %&gt θ’ +

Dψ(θ  θ t)(  

α(t)

where {α(t)} is a non-increasing sequence of positive stepsizes.
We make two standard assumptions throughout the paper. Let θ∗ denote a minimizer of the prob-
lem (1). The ﬁrst assumption [18  6  19] describes the properties of ψ and the domain.
Assumption A. The proximal function ψ is strongly convex with respect to the norm ’·’. The
domain Θ is compact  and there exists R < ∞ such that Dψ(θ∗ θ ) ≤ 1

2 R2 for θ ∈ Θ.

2

Our second assumption is standard for almost all ﬁrst-order stochastic gradient methods [19  24  20] 
and it holds whenever the functions F (·; x) are G-Lipschitz with respect to the norm ’·’. We use
’·’∗ to denote the dual norm to ’·’  and let g :Θ ×X → Rd denote a measurable subgradient
selection for the functions F ; that is  g(θ; x) ∈ ∂F (θ; x) with E[g(θ; X)] ∈ ∂f (θ).
Assumption B. There is a constant G < ∞ such that the (sub)gradient selection g satisﬁes
E[’g(θ; X)’2

∗] ≤ G2 for θ ∈ Θ.

When Assumptions A and B hold  the convergence rate of stochastic mirror descent methods is
well understood [6  19  Section 2.3]. Indeed  let the variables X t ∈X be sampled i.i.d. according
to P   set gt = g(θt; X t)  and let θt be generated by the mirror descent iteration (4) with stepsize
α(t) = α/√t. Then one obtains

E[f ($θ(k))] − f (θ∗) ≤

1

2α√k

R2 +

α
√k

G2.

(5)

For the remainder of this section  we explore the use of function difference information to obtain
subgradient estimates that can be used in mirror descent methods to achieve statements similar to
the convergence guarantee (5).

2.1 Two-point gradient estimates and general convergence rates

In this section  we show—under a reasonable additional assumption—how to use two samples of
the random function values F (θ; X) to construct nearly unbiased estimators of the gradient ∇f (θ)
of the expected function f . Our analytic techniques are somewhat different than methods employed
in past work [1  20]; as a consequence  we are able to achieve optimal dimension dependence.
Our method is based on an estimator of ∇f (θ). Our algorithm uses a non-increasing sequence
of positive smoothing parameters {ut} and a distribution µ on Rd (which we specify) satisfying
Eµ[ZZ #] = I. Upon receiving the point X t ∈X   we sample an independent vector Zt and set

gt =

F (θt + utZt; X t) − F (θt; X t)

ut

Zt.

(6)

We then apply the mirror descent update (4) to the quantity gt.
The intuition for the estimator (6) of ∇f (θ) follows from an understanding of the directional deriva-
tives of the random function realizations F (θ; X). The directional derivative f $(θ  z) of the function
f at the point θ in the direction z is f $(θ  z) := limu↓0
. The limit always exists when
f is convex [15  Chapter VI]  and if f is differentiable at θ  then f $(θ  z) = )∇f (θ)  z*. In addition 
we have the following key insight (see also Nesterov [20  Eq. (32)]): whenever ∇f (θ) exists 

f (θ+uz)−f (θ)

u

E[f $(θ  Z)Z] = E[)∇f (θ)  Z* Z] = E[ZZ #∇f (θ)] = ∇f (θ)

if the random vector Z ∈ Rd has E[ZZ #] = I. Intuitively  for ut small enough in the construc-
tion (6)  the vector gt should be a nearly unbiased estimator of the gradient ∇f (θ).
To formalize our intuition  we make the following assumption.
Assumption C. There is a function L : X→ R+ such that for (P -almost every) x ∈X   the func-
tion F (·; x) has L(x)-Lipschitz continuous gradient with respect to the norm ’·’  and the quantity
L(P )2 := E[L(X)2] < ∞.
With Assumption C  we can show that gt is (nearly) an unbiased estimator of ∇f (θt). Furthermore 
for appropriate random vectors Z  we can also show that gt has small norm  which yields better
convergence rates for mirror descent-type methods. (See the proof of Theorem 1.) In order to study
the convergence of mirror descent methods using the estimator (6)  we make the following additional
assumption on the distribution µ.
Assumption D. Let Z be sampled according to the distribution µ  where E[ZZ #] = I. The quantity
M (µ)2 := E[’Z’4 ’Z’2
∗] < ∞  and there is a constant s(d) such that for any vector g ∈ Rd 
E[’)g  Z* Z’2
∗] ≤ s(d)’g’2
∗.

3

As the next theorem shows  Assumption D is somewhat innocuous  the constant M (µ) not even
appearing in the ﬁnal bound. The dimension (and norm) dependent term s(d)  however  is impor-
tant for our results. In Section 2.2 we give explicit constructions of random variables that satisfy
Assumption D. For now  we present the following result.
Theorem 1. Let {ut}⊂ R+ be a non-increasing sequence of positive numbers  and let θt be
generated according to the mirror descent update (4) using the gradient estimator (6). Under As-
sumptions A  B  C  and D  if we set the step and perturbation sizes

α(t) = α

R

2G)s(d)√t
RG)s(d)
√k

and ut = u

L(P )M (µ) ·

G)s(d)
max α  α−1- + αu2 RG)s(d)

1
t

 

+ u

RG)s(d) log k

k

 

k

then

E*f ($θ(k)) − f (θ∗)+ ≤ 2
k.k
where$θ(k) = 1

t=1 θt  and the expectation is taken with respect to the samples X and Z.

The proof of Theorem 1 requires some technical care—we never truly receive unbiased gradients—
and it builds on convergence proofs developed in the analysis of online and stochastic convex op-
timization [27  19  1  12  20] to achieve bounds of the form (5). Though we defer proof to Ap-
pendix A.1  at a very high level  the argument is as follows. By using Assumption C  we see that
for small enough ut  the gradient estimator gt from (6) is close (in expectation with respect to X t)
to f $(θt  Zt)Zt  which is an unbiased estimate of ∇f (θt). Assumption C allows us to bound the
moments of the gradient estimator gt. By carefully showing that taking care to make sure that the
errors in gt as an estimator of ∇f (θt) scale with ut  we given an analysis similar to that used to
derive the bound (5) to obtain Theorem 1.

Before continuing  we make a few remarks. First  the method is reasonably robust to the selection
of the step-size multiplier α (as noted by Nemirovski et al. [19] for gradient-based MD methods).

So long as α(t) ∝ 1/√t  mis-specifying the multiplier α results in a scaling at worst linear in
max{α  α−1}. Perhaps more interestingly  our setting of ut was chosen mostly for convenience
and elegance of the ﬁnal bound. In a sense  we can simply take u to be extremely close to zero (in
practice  we must avoid numerical precision issues  and the stochasticity in the method makes such
choices somewhat unnecessary). In addition  the convergence rate of the method is independent
of the Lipschitz continuity constant L(P ) of the instantaneous gradients ∇F (·; X); the penalty for
nearly non-smooth objective functions comes into the bound only as a second-order term. This
suggests similar results should hold for non-differentiable functions; we have been able to show that
in some cases this is true  but a fully general result has proved elusive thus far. We are currently
investigating strategies for the non-differentiable case.

that E[exp(’g(θ; X)’2

Using similar arguments based on Azuma-Hoeffding-type inequalities  it is possible to give high-
probability convergence guarantees [cf. 9  19] under additional tail conditions on g  for example 
∗ /G2)] ≤ exp(1). Additionally  though we have presented our results as
convergence guarantees for stochastic optimization problems  an inspection of our analysis in Ap-
pendix A.1 shows that we obtain (expected) regret bounds for bandit online convex optimization
problems [e.g. 13  5  1].

2.2 Examples and corollaries

In this section  we provide examples of random sampling strategies that give direct convergence
rate estimates for the mirror descent algorithm with subgradient samples (6). For each corollary 
we specify the norm ’·’  proximal function ψ  and distribution µ  verify that Assumptions A  B  C 
and D hold  and then apply Theorem 1 to obtain a convergence rate.

We begin with a corollary that describes the convergence rate of our algorithm when the expected
function f is Lipschitz continuous with respect to the Euclidean norm ’·’2.
Corollary 1. Given the proximal function ψ(θ) := 1
2] ≤ G2 and
that µ is uniform on the surface of the "2-ball of radius √d. With the step size choices in Theorem 1 

2  suppose that E[’g(θ; X)’2

2 ’θ’2

4

we have

E*f ($θ(k)) − f (θ∗)+ ≤ 2

RG√d
√k

max{α  α−1} + αu2 RG√d

k

+ u

RG√d log k

k

.

Proof Note that ’Z’2 = √d  which implies M (µ)2 = E[’Z’6
see that E[ZZ #] = I. Thus  for g ∈ Rd we have

2] = d3. Furthermore  it is easy to

E[’)g  Z* Z’2

2] = dE[)g  Z*2] = dE[g#ZZ #g] = d’g’2
2  

which gives us s(d) = d.

The rate provided by Corollary 1 is the fastest derived to date for zero-order stochastic optimiza-
tion using two function evaluations. Both Agarwal et al. [1] and Nesterov [20] achieve rates of

convergence of order RGd/√k. Admittedly  neither requires that the random functions F (·; X) be
continuously differentiable. Nonetheless  Assumption C does not require a uniform bound on the
Lipschitz constant L(X) of the gradients ∇F (·; X); moreover  the convergence rate of the method
is essentially independent of L(P ).

In high-dimensional scenarios  appropriate choices for the proximal function ψ yield better scaling
on the norm of the gradients [18  14  19  12]. In online learning and stochastic optimization settings
where one observes gradients g(θ; X)  if the domain Θ is the simplex  then exponentiated gradient

algorithms [16  6] using the proximal function ψ(θ) = .j θj log θj obtain rates of convergence
dependent on the "∞-norm of the gradients ’g(θ; X)’∞. This scaling is more palatable than de-
pendence on Euclidean norms applied to the gradient vectors  which may be a factor of √d larger.
Similar results apply [7  6] when using proximal functions based on "p-norms. Indeed  making the
2(p−1) ’θ’2
choice p = 1 + 1/ log d and ψ(θ) = 1
Corollary 2. Assume that E[’g(θ; X)’2
∞] ≤ G2 and that Θ ⊆{ θ ∈ Rd : ’θ’1 ≤ R}. Set µ to be
uniform on the surface of the "2-ball of radius √d. Use the step sizes speciﬁed in Theorem 1. There
are universal constants C1 < 20e and C2 < 10e such that
/αu2 + u log k0 .

max α  α−1- + C2

E*f ($θ(k)) − f (θ∗)+ ≤ C1

p  we obtain the following corollary.

RG√d log d

√k

The proof of this corollary is somewhat involved. The main argument involves showing

Proof
that the constants M (µ) and s(d) may be taken as M (µ) ≤ d6 and s(d) ≤ 24d log d.
First  we recall [18  7  Appendix 1] that our choice of ψ is strongly convex with respect to the norm
’·’p. In addition  if we deﬁne q = 1 + log d  then we have 1/p + 1/q = 1  and ’v’q ≤ e’v’∞ for
any v ∈ Rd and any d. As a consequence  we see that we may take the norm ’·’ = ’·’1 and the dual
norm ’·’∗ = ’·’∞  and E[’)g  Z* Z’2
q] ≤ e2E[’)g  Z* Z’2
∞]. To apply Theorem 1 with appropriate
values from Assumption D  we now bound E[’)g  Z* Z’2
∞]; see Appendix A.3 for a proof.
Lemma 3. Let Z be distributed uniformly on the "2-sphere of radius √d. For any g ∈ Rd 

RG√d log d

k

E[’)g  Z* Z’2

∞] ≤ C · d log d’g’2
∞  

where C ≤ 24 is a universal constant.
As a consequence of Lemma 3  the constant s(d) of Assumption D satisﬁes s(d) ≤ Cd log d.
Finally  we have the essentially trivial bound M (µ)2 = E[’Z’4
∞] ≤ d6 (we only need the
quantity M (µ) to be ﬁnite to apply Theorem 1). Recalling that the set Θ ⊆{ θ ∈ Rd : ’θ’1 ≤ R} 
our choice of ψ yields [e.g.  14  Lemma 3]

1 ’Z’2

(p − 1)Dψ(θ  τ ) ≤

1
2 ’θ’2

p +

1
2 ’τ’2

p + ’θ’p ’τ’p .

We thus ﬁnd that Dψ(θ  τ ) ≤ 2R2 log d for any θ  τ ∈ Θ  and using the step and perturbation size
choices of Theorem 1 gives the result.

5

Corollary 2 attains a convergence rate that scales with dimension as √d log d. This dependence
on dimension is much worse than that of (stochastic) mirror descent using full gradient informa-
tion [18  19]. The additional dependence on d suggests that while O(1/2) iterations are required to
achieve -optimization accuracy for mirror descent methods (ignoring logarithmic factors)  the two-
point method requires O(d/2) iterations to obtain the same accuracy. A similar statement holds for
the results of Corollary 1. In the next section  we show that this dependence is sharp: except for log-
arithmic factors  no algorithm can attain better convergence rates  including the problem-dependent
constants R and G.

3 Lower bounds on zero-order optimization

We turn to providing lower bounds on the rate of convergence for any method that receives random
function values. For our lower bounds  we ﬁx a norm ’·’ on Rd and as usual let ’·’∗ denote its
dual norm. We assume that Θ= {θ ∈ Rd : ’θ’ ≤ R} is the norm ball of radius R. We study
all optimization methods that receive function values of random convex functions  building on the
analysis of stochastic gradient methods by Agarwal et al. [3].

More formally  let Ak denote the collection of all methods that observe a sequence of data points

(Y 1  . . .   Y k) ⊂ R2 with Y t = [F (θt  X t) F (τ t  X t)] and return an estimate$θ(k) ∈ Θ. The classes
of functions over which we prove our lower bounds consist of those satisfying Assumption B  that is 
for a given Lipschitz constant G > 0  optimization problems over the set FG. The set FG consists of
pairs (F  P ) as described in the objective (1)  and for (F  P ) ∈F G we assume there is a measurable
subgradient selection g(θ; X) ∈ ∂F (θ; X) satisfying EP [’g(θ; X)’2
Given an algorithm A∈ Ak and a pair (F  P ) ∈F G  we deﬁne the optimality gap

∗] ≤ G2 for θ ∈ Θ.

EP [F (θ; X)]  

(7)

k(A  F  P  Θ) := f ($θ(k)) − inf

θ∈Θ

f (θ) = EP1F ($θ(k); X)2 − inf

θ∈Θ

where$θ(k) is the output of A on the sequence of observed function values. The quantity (7) is a
random variable  since the Y t are random and A may use additional randomness. We we are thus
interested in its expected value  and we deﬁne the minimax error

∗
k(FG  Θ) := inf

A∈Ak

sup

(F P )∈FG

E[k(A  F  P  Θ)] 

(8)

where the expectation is over the observations (Y 1  . . .   Y k) and randomness in A.
3.1 Lower bounds and optimality

In this section  we give lower bounds on the minimax rate of optimization for a few speciﬁc settings.
We present our main results  then recall Corollaries 1 and 2  which imply we have attained the min-
imax rates of convergence for zero-order (stochastic) optimization schemes. The following sections
contain proof sketches; we defer technical arguments to appendices.

We begin by providing minimax lower bounds when the expected function f (θ) = E[F (θ; X)] is
Lipschitz continuous with respect to the "2-norm. We have the following proposition.

Proposition 1. Let Θ=  θ ∈ Rd : ’θ’2 ≤ R- and FG consist of pairs (F  P ) for which the sub-
gradient mapping g satisﬁes EP [’g(θ; X)’2
2] ≤ G2 for θ ∈ Θ. There exists a universal constant
c > 0 such that for k ≥ d 

∗
k(FG  Θ) ≥ c

GR√d
√k

.

Combining the lower bounds provided by Proposition 1 with our algorithmic scheme in Section 2
shows that our analysis is essentially sharp  since Corollary 1 provides an upper bound for the
minimax optimality of RG√d/√k. The stochastic gradient descent algorithm (4) coupled with the
sampling strategy (6) is thus optimal for stochastic problems with two-point feedback.

Now we investigate the minimax rates at which it is possible to solve stochastic convex optimization
problems whose objectives are Lipschitz continuous with respect to the "1-norm. As noted earlier 
such scenarios are suitable for high-dimensional problems [e.g. 19].

6

Proposition 2. Let Θ= {θ ∈ Rd : ’θ’1 ≤ R} and FG consist of pairs (F  P ) for which the
subgradient mapping g satisﬁes EP [’g(θ; X)’2
∞] ≤ G2 for θ ∈ Θ. There exists a universal constant
c > 0 such that for k ≥ d 

∗
k(FG  Θ) ≥ c

GR√d
√k

.

We may again consider the optimality of our mirror descent algorithms  recalling Corollary 2. In this
case  the MD algorithm (4) with the choice ψ(θ) = 1
p  where p = 1 + 1/ log d  implies
that there exist universal constants c and C such that

2(p−1) ’θ’2

GR√d
√k ≤ ∗

c

GR√d log d

k(FG  Θ) ≤ C

√k

for the problem class described in Proposition 2. Here the upper bound is again attained by our
two-point mirror-descent procedure. Thus  to within logarithmic factors  our mirror-descent based
algorithm is optimal for these zero-order optimization problems.

When full gradient information is available  that is  one has access to the subgradient selection
g(θ; X)  the √d factors appearing in the lower bounds in Proposition 1 and 2 are not present [3].
The √d factors similarly disappear from the convergence rates in Corollaries 1 and 2 when one
uses gt = g(θ; X) in the mirror descent updates (4); said differently  the constant s(d) = 1 in
Theorem 1 [19  6]. As noted in Section 2  our lower bounds consequently show that in addition to
dependence on the radius R and second moment G2 in the case when gradients are available [3] 

all algorithms must suffer an additional O(√d) penalty in convergence rate. This suggests that for

high-dimensional problems  it is preferable to use full gradient information if possible  even when
the cost of obtaining the gradients is somewhat high.

3.2 Proofs of lower bounds

We sketch proofs for our lower bounds on the minimax error (8)  which are based on the framework
introduced by Agarwal et al. [3]. The strategy is to reduce the optimization problem to a testing
problem: we choose a ﬁnite set of (well-separated) functions  show that optimizing well implies that
one can identify the function being optimized  and then  as in statistical minimax theory [26  25] 
apply information-theoretic lower bounds on the probability of error in hypothesis testing problems.
We begin with a ﬁnite set V⊆ Rd  to be chosen depending on the characteristics of the function
class FG  and a collection of functions and distributions G = {(Fv  Pv) : v ∈V}⊆F G indexed by
V. Deﬁne fv(θ) = EPv [Fv(θ; X)]  and let θ∗
v ∈ argminθ∈Θ fv(θ). We also let δ> 0 be an accuracy
parameter upon which Pv and the following quantities implicitly depend. Following Agarwal et al.
[3]  we deﬁne the separation between two functions as

ρ(fv  fw) := inf

θ∈Θ1fv(θ) + fw(θ)2 − fv(θ∗

v) − fw(θ∗
w) 

and the minimal separation of the set V (this may depend on the accuracy parameter δ) as

ρ∗(V) := min{ρ(fv  fw) : v  w ∈V   v 0= w}.

sampled uniformly from V (see [3  Lemma 2]) 

For any algorithm A∈ Ak  there exists a hypothesis test$v : (Y 1  . . .   Y k) →V such that for V
P($v(Y 1  . . .   Y k) 0= V ) ≤

E[k(A  Fv  Pv  Θ)] 
where the expectation is taken over the observations (Y 1  . . .   Y k). By Fano’s inequality [11] 

E[k(A  FV   PV   Θ)] ≤

ρ∗(V)

ρ∗(V)

max
v∈V

2

2

(9)

P($v 0= V ) ≥ 1 −

I(Y 1  . . .   Y k; V ) + log 2

log |V|

.

(10)

We thus must upper bound the mutual information I(Y 1  . . .   Y k; V )  which leads us to the follow-
ing. (See Appendix B.3 for the somewhat technical proof of the lemma.)

7

Lemma 4. Let X | V = v be distributed as N (δv  σ 2I)  and let F (θ; x) = )θ  x*. Let V be a
uniform random variable on V⊂ Rd  and assume that Cov(V ) 1 λI for some λ ≥ 0. Then

I(Y 1  Y 2  . . .   Y k; V ) ≤

λkδ2
σ2 .

Using Lemma 4  we can obtain a lower bound on the minimax optimization error whenever the
instantaneous objective functions are of the form F (θ; x) = )θ  x*. Combining inequalities (9) 
(10)  and Lemma 4  we ﬁnd that if we choose the accuracy parameter

 

(11)

δ =

σ

2 − log 241/2
√kλ3 log |V|
E[k(A  F  P  Θ)] ≥ ρ∗(V)/4.

(we assume that |V| > 4) we ﬁnd that there exist a pair (F  P ) ∈F G such that

(12)
The inequality (12) can give concrete lower bounds on the minimax optimization error. In our lower
bounds  we use Fv(θ; x) = )θ  x* and set Pv to be the N (δv  σ 2I) distribution  which allows us to
apply Lemma 4. Proving Propositions 1 and 2 thus requires three steps:

1. Choose the set V with the property that Cov(V ) 1 λI when V ∼ Uniform(V).
2. Choose the variance parameter σ2 such that for each v ∈V   the pair (Fv  Pv) ∈F G.
3. Calculate the separation value ρ∗(V) as a function of the accuracy parameter δ.
Enforcing (Fv  Pv) ∈F G amounts to choosing σ2 so that E[’X’2
∗] ≤ G2 for X ∼ N (δv  σ 2I).
By construction fv(θ) = δ )θ  v*  which allows us to give lower bounds on the minimal separation
ρ∗(V) for the choices of the norm constraint ’θ’ ≤ R in Propositions 1 and 2. We defer formal
proofs to Appendices B.1 and B.2  providing sketches here.

For Proposition 1  an argument using the probabilistic method implies that there are universal
constants c1  c2 > 0 for which there is a 1
2 packing V of the "2-sphere of radius 1 with size
at least |V| ≥ exp(c1d) and such that (1/|V|).v∈V vv# 1 c2Id×d/d. By the linearity of fv 
we ﬁnd ρ(fv  fw) ≥ δR/16  and setting σ2 = G2/(2d) and δ as in the choice (11) implies that
E[’X’2
For Proposition 2  we use the packing set V = {±ei : i = 1  . . .   d}. Standard bounds [8] on
the normal distribution imply that for Z ∼ N (0  I)  we have E[’Z’2
∞] = O(log d). Thus we ﬁnd
that for σ2 = O(G2/ log(d)) and suitably small δ  we have E[’X’2
∞] = O(G2); linearity yields
ρ(fv  fw) ≥ δR for v 0= w ∈V . Setting δ as in the expression (11) yields Proposition 2.

2] ≤ G2. Substituting δ and ρ∗(V) into the bound (12) proves Proposition 1.

4 Discussion

We have analyzed algorithms for stochastic optimization problems that use only random function
values—as opposed to gradient computations—to minimize an objective function. As our develop-
ment of minimax lower bounds shows  the algorithms we present  which build on those proposed by
Agarwal et al. [1] and Nesterov [20]  are optimal: their convergence rates cannot be improved (in a
minimax sense) by more than numerical constant factors. As a consequence of our results  we have
attained sharp rates for bandit online convex optimization problems with multi-point feedback. We
have also shown that there is a necessary sharp transition in convergence rates between stochastic
gradient algorithms and algorithms that compute only function values. This result highlights the
advantages of using gradient information when it is available  but we recall that there are many
applications in which gradients are not available.

Finally  one question that this work leaves open  and which we are actively attempting to address 
is whether our convergence rates extend to non-smooth optimization problems. We conjecture that
they do  though it will be interesting to understand the differences between smooth and non-smooth
problems when only zeroth-order feedback is available.

Acknowledgments This material supported in part by ONR MURI grant N00014-11-1-0688 and
the U.S. Army Research Laboratory and the U.S. Army Research Ofﬁce under grant no. W911NF-
11-1-0391. JCD was also supported by an NDSEG fellowship and a Facebook PhD fellowship.

8

References

[1] A. Agarwal  O. Dekel  and L. Xiao. Optimal algorithms for online convex optimization with multi-point
In Proceedings of the Twenty Third Annual Conference on Computational Learning

bandit feedback.
Theory  2010.

[2] A. Agarwal  D. P. Foster  D. Hsu  S. M. Kakade  and A. Rakhlin.

timization with bandit
http://arxiv.org/abs/1107.1744.

feedback.

SIAM Journal on Optimization  To appear  2011.

Stochastic convex op-
URL

[3] A. Agarwal  P. L. Bartlett  P. Ravikumar  and M. J. Wainwright. Information-theoretic lower bounds on the
oracle complexity of convex optimization. IEEE Transactions on Information Theory  58(5):3235–3249 
May 2012.

[4] K. Ball. An elementary introduction to modern convex geometry. In S. Levy  editor  Flavors of Geometry 

pages 1–58. MSRI Publications  1997.

[5] P. L. Bartlett  V. Dani  T. P. Hayes  S. M. Kakade  A. Rakhlin  and A. Tewari. High-probability regret
bounds for bandit online linear optimization. In Proceedings of the Twenty First Annual Conference on
Computational Learning Theory  2008.

[6] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex opti-

mization. Operations Research Letters  31:167–175  2003.

[7] A. Ben-Tal  T. Margalit  and A. Nemirovski. The ordered subsets mirror descent optimization method

with applications to tomography. SIAM Journal on Optimization  12:79–108  2001.

[8] V. Buldygin and Y. Kozachenko. Metric Characterization of Random Variables and Random Processes 

volume 188 of Translations of Mathematical Monographs. American Mathematical Society  2000.

[9] N. Cesa-Bianchi  A. Conconi  and C.Gentile. On the generalization ability of on-line learning algorithms.

In Advances in Neural Information Processing Systems 14  pages 359–366  2002.

[10] A. Conn  K. Scheinberg  and L. Vicente.

Introduction to Derivative-Free Optimization  volume 8 of

MPS-SIAM Series on Optimization. SIAM  2009.

[11] T. M. Cover and J. A. Thomas. Elements of Information Theory  Second Edition. Wiley  2006.
[12] J. C. Duchi  S. Shalev-Shwartz  Y. Singer  and A. Tewari. Composite objective mirror descent. In Pro-

ceedings of the Twenty Third Annual Conference on Computational Learning Theory  2010.

[13] A. D. Flaxman  A. T. Kalai  and H. B. McMahan. Online convex optimization in the bandit setting:
gradient descent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on
Discrete Algorithms (SODA)  2005.

[14] C. Gentile. The robustness of the p-norm algorithms. Machine Learning  53(3)  2002.
[15] J. Hiriart-Urruty and C. Lemar´echal. Convex Analysis and Minimization Algorithms I & II. Springer 

1996.

[16] J. Kivinen and M. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Infor-

mation and Computation  132(1):1–64  Jan. 1997.

[17] H. J. Kushner and G. Yin.

Stochastic Approximation and Recursive Algorithms and Applications.

Springer  Second edition  2003.

[18] A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization. Wiley  1983.
[19] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[20] Y. Nesterov.

of
http://www.ecore.be/DPs/dp_1297333890.pdf  2011.

Random gradient-free minimization

convex

functions.

URL

[21] J. C. Spall. Introduction to Stochastic Search and Optimization: Estimation  Simulation  and Control.

Wiley  2003.

[22] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed Sensing:

Theory and Applications  chapter 5  pages 210–268. Cambridge University Press  2012.

[23] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational inference.

Foundations and Trends in Machine Learning  1(1–2):1–305  2008.

[24] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of

Machine Learning Research  11:2543–2596  2010.

[25] Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. Annals of

Statistics  27(5):1564–1599  1999.

[26] B. Yu. Assouad  Fano  and Le Cam. In Festschrift for Lucien Le Cam  pages 423–435. Springer-Verlag 

1997.

[27] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings

of the Twentieth International Conference on Machine Learning  2003.

9

,Chengyue Gong
Di He
Tao Qin
Liwei Wang
Tie-Yan Liu
Mitsuru Kusumoto
Takuya Inoue
Gentaro Watanabe
Takuya Akiba
Masanori Koyama