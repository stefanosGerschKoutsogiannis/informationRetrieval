2018,Sharp Bounds for Generalized Uniformity Testing,We study the problem of generalized uniformity testing of a discrete probability distribution: Given samples from a probability distribution p over an unknown size discrete domain Ω  we want to distinguish  with probability at least 2/3  between the case that p is uniform on some subset of Ω versus ε-far  in total variation distance  from any such uniform distribution. We establish tight bounds on the sample complexity of generalized uniformity testing. In more detail  we present a computationally efficient tester whose sample complexity is optimal  within constant factors  and a matching worst-case information-theoretic lower bound. Specifically  we show that the sample complexity of generalized uniformity testing is Θ(1/(ε^(4/3) ||p||_3) + 1/(ε^2 ||p||_2 )).,Sharp Bounds for Generalized Uniformity Testing

Ilias Diakonikolas

University of Southern California

diakonik@usc.edu

Daniel M. Kane

University of California  San Diego

dakane@ucsd.edu

Alistair Stewart

University of Southern California

stewart.al@gmail.com

Abstract

We study the problem of generalized uniformity testing of a discrete probability
distribution: Given samples from a probability distribution p over an unknown
size discrete domain Ω  we want to distinguish  with probability at least 2/3 
between the case that p is uniform on some subset of Ω versus -far  in total vari-
ation distance  from any such uniform distribution. We establish tight bounds
on the sample complexity of generalized uniformity testing. In more detail  we
present a computationally efﬁcient tester whose sample complexity is optimal 
within constant factors  and a matching worst-case information-theoretic lower
bound. Speciﬁcally  we show that the sample complexity of generalized unifor-

mity testing is Θ(cid:0)1/(4/3(cid:107)p(cid:107)3) + 1/(2(cid:107)p(cid:107)2)(cid:1).

1

Introduction

Consider the following statistical task: Given independent samples from a distribution over an un-
known size discrete domain Ω  determine whether it is uniform on some subset of the domain versus
signiﬁcantly different from any such uniform distribution. Formally  let CU
def= {uS : S ⊆ Ω}
denote the set of uniform distributions uS over subsets S of Ω. Given sample access to an unknown
distribution p on Ω and a proximity parameter  > 0  we want to correctly distinguish between the
case that p ∈ CU versus dTV (p CU ) def= minS⊆Ω dTV (p  uS) ≥   with probability at least 2/3.
Here  dTV (p  q) = (1/2)(cid:107)p− q(cid:107)1 denotes the total variation distance between distributions p and q.
This natural problem  termed generalized uniformity testing  was recently introduced by Batu and
Canonne [BC17]  who gave the ﬁrst upper and lower bounds on its sample complexity.
Generalized uniformity testing bears a strong resemblance to the familiar task of uniformity testing 
where one is given samples from a distribution p on a domain of known size n and the goal is to
determine  with probability at least 2/3  whether p is the uniform distribution un on this domain
versus dTV (p  un) ≥ . Uniformity testing is arguably the most extensively studied problem in
distribution property testing [GR00  Pan08  VV14  DKN15b  Gol16  DGPP16  DGPP17] and its
sample complexity is well understood. Speciﬁcally  it is known [Pan08  CDVV14  VV14  DKN15b]
that Θ(n1/2/2) samples are necessary and sufﬁcient for this task.
The ﬁeld of distribution property testing [BFR+00] has seen substantial progress in the past decade 
see [Rub12  Can15] for two recent surveys. A large body of the literature has focused on char-
acterizing the sample size needed to test properties of arbitrary distributions of a given support
size. This regime is fairly well understood: for many properties of interest there exist sample-
efﬁcient testers [Pan08  CDVV14  VV14  DKN15b  ADK15  CDGR16  DK16  DGPP16  CDS17 
DGPP18  CDKS18]. Moreover  an emerging body of work has focused on leveraging a pri-

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

ori structure of the underlying distributions to obtain signiﬁcantly improved samples complexi-
ties [BKR04  DDS+13  DKN15b  DKN15a  CDKS17  DP17  DDK16  DKN17].
Perhaps surprisingly  the natural setting where the distribution is arbitrary on a discrete but un-
known domain (of unknown size) does not seem to have been explicitly studied before the recent
work of Batu and Canonne [BC17]. Returning to the speciﬁc problem studied here  at ﬁrst glance
it might seem that generalized uniformity testing and uniformity testing are essentially the same
task. Naively  one might attempt to apply the existing uniformity testers directly without explicit
knowledge of the domain. This nearly works  as standard testers do not need to make use of any
particular information about the names of domain elements. However  these algorithms do make use
of the domain size in a critical way. This difﬁculty is not so easy to overcome. In fact  as was shown
in [BC17]  the sample complexity with an unknown domain size is signiﬁcantly different. Speciﬁ-
cally  [BC17] gave a generalized uniformity tester with expected sample complexity O(1/(6(cid:107)p(cid:107)3))
and showed a lower bound of Ω(1/(cid:107)p(cid:107)3). This should be compared to the O(n1/2/2)-sample tester
for distributions on domains of size n. Of particular interest here is that distributions p with support
size n can have 1/(cid:107)p(cid:107)3 as large as n2/3  making the problem with unknown domain substantially
harder in the worst case.

1.1 Our Results and Techniques

a parameter 0 <  < 1  the algorithm uses O(cid:0)1/(4/3(cid:107)p(cid:107)3) + 1/(2(cid:107)p(cid:107)2)(cid:1) independent samples

An immediate open question arising from the work of [BC17] is to precisely characterize the sample
complexity of generalized uniformity testing. The main result of this paper provides an answer to
this question. In particular  we show the following:
Theorem 1.1 (Main Result). There is an algorithm with the following performance guarantee:
Given sample access to an arbitrary distribution p over an unknown size discrete domain Ω and
from p in expectation  and distinguishes between the case p ∈ CU versus dTV (p CU ) ≥  with
probability at least 2/3. Moreover  for every 0 <  < 1/10 and n > 1  any algorithm that
distinguishes between p ∈ CU and dTV (p CU ) ≥  requires at least Ω(n2/3/4/3 + n1/2/2)
samples  where p is guaranteed to have (cid:107)p(cid:107)3 = Θ(n−2/3) and (cid:107)p(cid:107)2 = Θ(n−1/2).

In the following paragraphs  we provide an intuitive explanation of our algorithm and our matching
sample size lower bound  in tandem with a comparison to the prior work [BC17].

2

Sample-Optimal Generalized Uniformity Tester Our algorithm requires considering two cases
based on the relative size of  and (cid:107)p(cid:107)2
2. This case analysis seems somewhat intrinsic to the problem
as the correct sample complexity branches into these cases.
For large   we use the same overall technique as [BC17]  noting that p is uniform if and only
if (cid:107)p(cid:107)3 = (cid:107)p(cid:107)4/3
  and that for p far from uniform  (cid:107)p(cid:107)3 must be substantially larger. The basic
idea from here is to ﬁrst obtain rough approximations to (cid:107)p(cid:107)2 and (cid:107)p(cid:107)3 in order to ascertain the
2 and (cid:107)p(cid:107)3
correct number of samples to use  and then use standard unbiased estimators of (cid:107)p(cid:107)2
3
to approximate them to appropriate precision  so that their relative sizes can be compared with
appropriate accuracy.
We improve upon the work of [BC17] in this parameter regime in a couple of ways. First  we
obtain more precise lower bounds on the difference (cid:107)p(cid:107)3
2 in the case where p is far from
uniform (Lemma 2.8). This allows us to reduce the accuracy needed in estimating (cid:107)p(cid:107)2 and (cid:107)p(cid:107)3.
Second  we reﬁne the method used for performing the approximations to these moments ((cid:96)r-norms).
In particular  we observe that using the generic estimators for these quantities yields sub-optimal
bounds for the following reason: The error of the unbiased estimators is related to their variance 
which in turn can be expressed in terms of the higher moments of p (Fact 2.1). This implies for
example that the worst case sample complexity for estimating (cid:107)p(cid:107)3 comes when the fourth and ﬁfth
moments of p are large. However  since we are trying to test for the case of uniformity (where
these higher moments are minimal)  we do not need to worry about this worst case. In particular 
after applying sample efﬁcient tests to ensure that the higher moments of p are not much larger than
expected  the standard estimators for the second and third moments of p can be shown to converge
more rapidly than they would in the worst case (Fact 2.1).

3 − (cid:107)p(cid:107)4

2

The above algorithm is not sufﬁcient for small values of . For  sufﬁciently small  we employ a
different  perhaps more natural  algorithm. Here we take m samples (for m appropriately chosen
based on an approximation to (cid:107)p(cid:107)2) and consider the subset S of the domain that appears in the
sample. We then test whether the conditional distribution p on S is uniform  and output the answer
of this tester. The number of samples m drawn in the ﬁrst step is sufﬁciently large so that p(S) 
the probability mass of S under p  is relatively high. Hence  it is easy to sample from the condi-
tional distribution using rejection sampling. Furthermore  we can use a standard uniformity testing

algorithm requiring O((cid:112)|S|/2) samples.
x = Θ(1/n)  with high constant probability  the random variable Z(x) =(cid:80)
implies that(cid:80)

To establish correctness of this algorithm  we need to show that if p is far from uniform  then the
conditional distribution p on S is far from uniform as well. We show (Lemma 2.10) that for any
i∈S |pi − x| is large. It
is not hard to show that this holds with high probability for each ﬁxed x  as p being far from uniform
i∈Ω min(pi |pi − x|) is large. This latter condition can be shown to provide a clean
lower bound for the expectation of Z(x). To conclude the argument  we show that Z(x) is tightly
concentrated around its expectation. Applying an appropriate union bound  allows us to show that
Z(x) is large for all x  and thus that the conditional distribution is far form uniform.

Sample Complexity Lower Bound The lower bound of Ω(n1/2/2) follows directly from the
standard lower bound of [Pan08] for uniformity testing on a given domain of size n. The other
branch of the lower bound  namely Ω(n2/3/4/3)  is more involved. To prove this lower bound 
we use the shared information method of [DK16] for the following family of hard instances: In the
“YES” case  we consider the distribution over (pseudo-)distributions on N bins  where each pi is
(1+2)/n with probability n/(N (1+2))  and 0 otherwise. (Here we assume that the parameter N is
sufﬁciently large compared to the other parameters.) In the “NO” case  we consider the distribution
over (pseudo-)distributions on N bins  where each pi is (1+)/n with probability n/(2N )  (1−)/n
with probability n/(2N )  and 0 otherwise.

Notation. Let Ω denote the unknown discrete domain. Each probability distribution over Ω can be
i∈Ω pi = 1. We will use pi 
instead of p(i)  to denote the probability of element i ∈ Ω in p. For a distribution p and a set S ⊆ Ω 
i∈S pi and by (p|S) the conditional distribution of p on S. For r ≥ 1  the

associated with a probability mass function p : Ω → R+ such that(cid:80)
we denote by p(S) def= (cid:80)
r = (cid:80)

(cid:96)r-norm of a function p : Ω → R is (cid:107)p(cid:107)r
i∈Ω |pi|r. For ∅ (cid:54)= S ⊆ Ω  let uS be the uniform distribution over S. Let
Fr(p) def= (cid:107)p(cid:107)r
CU
def= {uS : ∅ (cid:54)= S ⊆ Ω} be the set of uniform distributions over subsets of Ω. The total variation
distance between distributions p  q on Ω is deﬁned as dTV (p  q) def= maxS⊆Ω |p(S) − q(S)| =
(1/2) · (cid:107)p − q(cid:107)1. Finally  we denote by Poi(λ) the Poisson distribution with parameter λ.

i∈Ω |pi|r(cid:1)1/r. For convenience  we will denote

def= (cid:0)(cid:80)

2 Generalized Uniformity Tester

sums Fr(p) =(cid:80)

Before we describe our algorithm  we summarize a few preliminary results on estimating the power
i∈Ω |pi|r of an unknown distribution p. We present these results in Section 2.1. In
Section 2.2  we present and analyze the algorithm for large values of . In Section 2.3  we do the
same for the small  algorithm. Finally  in Section 2.4  we present the full algorithm.

2.1 Estimating the Power Sums of a Discrete Distribution

We will require various notions of approximation for the power sums of a discrete distribution.
Fact 2.1 ([AOST17]). Let p be a probability distribution on an unknown discrete domain. For any

r ≥ 1  there exists an estimator(cid:98)Fr(p) for Fr(p) that draws Poi(m) samples from p and satisﬁes the
following: E[(cid:98)Fr(p)] = Fr(p) and Var[(cid:98)Fr(p)] = m−2r(cid:80)r−1
The estimator (cid:98)Fr(p) draws Poi(m) samples from p and mr ·(cid:98)Fr(p) equals the number of r-wise

(cid:1)rr−tFr+t(p).

t=0 mr+t(cid:0)r

collisions  i.e.  ordered r-tuples of samples that land in the same bin. We use this fact to get a few
useful algorithms for approximating these moments:

t

3

Lemma 2.2. There exists an algorithm that given an integer r ≥ 1 and sample access to a distribu-
tion p returns a positive real number x so that:

1. With at least 99% probability x is within a constant (depending on r) multiple of (cid:107)p(cid:107)r.
2. The expectation of 1/x is Or(1/(cid:107)p(cid:107)r).
3. The expected number of samples taken by the algorithm is Or(1/(cid:107)p(cid:107)r).

Proof. The algorithm is as follows:

Algorithm 1 Algorithm for Rough Moment Estimation
1: procedure ROUGH-MOMENT-ESTIMATOR(p  r)
input: Sample access to distribution p on unknown discrete domain Ω and an integer r > 0.
output: A value x approximating (cid:107)p(cid:107)r.
2:
3:

Draw samples from p until there is some r-wise collision among these samples.
Return 1/n  where n is the number of samples taken in Step 2.

Firstly  we note that with large constant probability n (cid:29)r 1/(cid:107)p(cid:107)r. This is because after taking
m samples  the expected number of r-wise collisions is at most Fr(p)mr = ((cid:107)p(cid:107)rm)r. Thus  by
Markov’s inequality  if m (cid:28) 1/(cid:107)p(cid:107)r  then with large constant probability  our algorithm will not
have terminated yet. To ﬁnish the proof  it sufﬁces to show that E[n] = Or(1/(cid:107)p(cid:107)r). This implies by
Markov’s inequality that with large constant probability n (cid:28)r 1/(cid:107)p(cid:107)r  and bounds the expectations
of the number of samples and of 1/x. Let m = 1/(cid:107)p(cid:107)r. We note  by Fact 2.1   if we take Poi(m)
samples from p  the expected number of r-wise collisions is 1  and the variance is Or(1). By the
Paley-Zygmund inequality  every time the algorithm takes Poi(m) samples  there is at least a cr > 0
probability of seeing an r-wise collision. Therefore  if we consider our algorithm to take samples
in blocks of size Poi(m)  the probability that we have not found an r-wise collision after t blocks is
at most (1 − cr)t. Thus  the expected number of blocks until we have an r-wise collision is Or(1).
Therefore  the expected number of samples is Or(m) = Or(1/(cid:107)p(cid:107)r) completing the proof.
From the above  we derive an algorithm that approximates (cid:107)p(cid:107)r to a small relative error:
Lemma 2.3. There exists an algorithm that given sample access to a distribution p  a positive

integer r and a 1 > δ > 0  computes a value(cid:98)γr so that with probability at least 19/20 we have that
|(cid:98)γr − Fr(p)| ≤ δ · Fr(p). Furthermore  this algorithm uses an expected Or(

) samples.

δ2(cid:107)p(cid:107)r

1

Proof. The algorithm is as follows:

Algorithm 2 Algorithm for Moment Estimation
1: procedure MOMENT-ESTIMATOR(p  r  δ)
input: Sample access to arbitrary distribution p on unknown discrete domain Ω and an integer

output: A value(cid:98)γr approximating Fr(p).

r > 0  and a 1 > δ > 0.

2:
3:
4:

Run Rough-Moment-Estimator(p  r) returning a value x.
Let m be Cr/(δ2x) for Cr a sufﬁciently large constant in terms of r.
Run the algorithm from Fact 2.1 using Poi(m) samples and return the result.

To show correctness  ﬁrst note that with 99% probability we have that x = Θr((cid:107)p(cid:107)r)  and thus  m is
at least a sufﬁciently large multiple of 1/δ2(cid:107)p(cid:107)r. If this holds  then the output of our algorithm will
be a random variable with mean Fr(p). We need to bound the variance  which we do as follows:

Claim 2.4. If m(cid:107)p(cid:107)r (cid:29) 1  then Var((cid:99)Fr(p)) = Or(Fr(p)2((cid:107)p(cid:107)r/m)).

t=0 mt−r(cid:107)p(cid:107)t+r

r

= Or(m−1(cid:107)p(cid:107)2r−1

r

) = Or(Fr(p)2((cid:107)p(cid:107)r/m)) 

(cid:16)(cid:80)r−1

Proof. The variance is Or
which completes the proof.

(cid:17)

4

If Cr is large enough  this implies that Var((cid:99)Fr(p)) ≤ (Fr(p)2δ2)/100. Given this  our bound on
|(cid:98)γr − Fr(p)| follows from Chebyshev’s inequality. In terms of sample complexity  we note that the

expected number of samples in Step 1 is Or(1/(cid:107)p(cid:107)r)  and the expected number of samples in Step
2 is O(m) = Or(1/(δ2x))  which in expectation is Or(1/(δ2(cid:107)p(cid:107)r)). This completes the proof.

Our algorithm will begin by running Rough-Moment-Estimator to compute rough estimates of
the second and third moments of p. Unless there is some n for which (cid:107)p(cid:107)2 = Θ(n−1/2) and
(cid:107)p(cid:107)3 = Θ(n−2/3)  then we know that p cannot possibly be uniform. Otherwise  we know that if p
is uniform  then its support must have size Θ(n). Our algorithm will thus critically depend on the
following proposition:
Proposition 2.5. There exists an algorithm that given sample access to a distribution p  and n   > 0
takes an expected O(n2/3/4/3 +n1/2/2) samples from p and distinguishes with probability at least
2/3 between the cases: (i) p is the uniform distribution on a domain of size Θ(n)  and (ii) p is -far
from any uniform distribution.
Our algorithm will begin by verifying that (cid:107)p(cid:107)2 = Θ(n−1/2) and (cid:107)p(cid:107)3 = Θ(n−2/3) using Lemma
2.2. Thus  in the second case  we can assume that (cid:107)p(cid:107)2 = Θ(n−1/2) and (cid:107)p(cid:107)3 = Θ(n−2/3). We
will further split our algorithm into cases depending on whether  is bigger than n−1/4  which in
particular determines which term dominates the sample complexity.
We will need the following simple claim giving a useful condition for the soundness case:

Claim 2.6. If dTV (p CU ) ≥   then for all x ∈ [0  1] we have that(cid:80)
Proof. Let Sh be the set of i ∈ Ω on which pi > x/2. Let δ =(cid:80)

i∈Ω min{pi |x − pi|} ≥ /2.
i∈Ω min{pi |x − pi|}. Note that
δ = (cid:107)p− cx Sh(cid:107)1  where cx Sh is the pseudo-distribution that is x on Sh on 0 elsewhere. If (cid:107)cx Sh(cid:107)1
were 1  cx Sh would be the uniform distribution uSh and we would have δ ≥ . However  this need
not be the case. That said  it is easy to see that (cid:107)uSh −cx Sh(cid:107)1 = |1−(cid:107)cx Sh(cid:107)1| ≤ (cid:107)p−cx Sh(cid:107)1 = δ.
Therefore  by the triangle inequality 2δ ≥ (cid:107)p− cx Sh(cid:107)1 +(cid:107)uSh − cx Sh(cid:107)1 ≥ (cid:107)p− uSh(cid:107)1 ≥  .

2.2 Algorithm for Large 

Lemma 2.7. There exists an algorithm that given sample access to a distribution p  and n   > 0
with  ≥ n−1/4 takes an expected O(n2/3/4/3) samples from p and distinguishes with probability
at least 9/10 between the cases: (i) p is the uniform distribution on a domain of size Θ(n). (ii) p
satisﬁes (cid:107)p(cid:107)2 = Θ(n−1/2)  (cid:107)p(cid:107)3 = Θ(n−2/3)  and p is -far from any uniform distribution.
The basic idea of this algorithm is that if p is uniform over any discrete domain then

F3(p) = F2(p)2 .

(1)

We claim that this condition is robust. Namely for p far from uniform  Equation (1) will fail by a lot.
Therefore  we can distinguish between the relevant cases by ﬁnding suitably close approximations
to F2(p) and F3(p). To start with  we need to prove the robust version of Equation (1):
2(p). (ii) If dTV (p CU ) ≥  
Lemma 2.8. We have the following: (i) If p ∈ CU   then F3(p) = F2
then F3(p) − F2
Proof. The proof of (i) is straightforward. Suppose that p = uS for some ∅ (cid:54)= S ⊆ Ω. It then
follows that F2(p) = 1/|S| and F3(p) = 1/|S|2  yielding part (i) of the lemma. We now proceed
to prove part (ii). Suppose that dTV (p CU ) ≥ . First  it will be useful to rewrite the quantity
F3(p) − F2

2(p) > 2F2

2(p)/64.

2(p) as follows:

(cid:88)

i∈Ω

5

F3(p) − F2

2(p) =

pi(pi − F2(p))2 .

(2)

(cid:80)
Note that (2) follows from the identity pi(pi − F2(p))2 = p3
i F2(p) by summing
its complement Sh = Ω\Sl. Note that(cid:80)
over i ∈ Ω. Since dTV (p CU ) ≥   an application of Claim 2.6 for x = F2(p) ∈ [0  1]  gives that
i∈Ω min{pi |F2(p)−pi|} ≥ /2 . We partition Ω into the sets Sl = {i ∈ Ω | pi < F2(p)/2} and
|F2(p)−

i∈Ω min{pi |F2(p)−pi|} =(cid:80)

i + piF2(p)2 − 2p2

pi +(cid:80)

i∈Sh

i∈Sl

pi ≥ /4 or(cid:80)
pi(pi − F2(p))2 > (F2(p)/2)2 ·(cid:88)

pi ≥ /4. Using (2) we can now write

i∈Sh

i∈Sl

|F2(p) − pi| ≥ /4. We analyze each case

pi = F2

2(p)/16 .

i∈Sl

i∈Sl

pi| . It follows that either(cid:80)
separately. First  suppose that(cid:80)
2(p) ≥(cid:88)
Now suppose that(cid:80)
2(p) ≥ (cid:88)

(2) we obtain
F3(p) − F2

F3(p) − F2

i∈Sh

i∈Sl

|F2(p) − pi| ≥ /4. Note that 1 ≤ |Sh| ≤ 2/|F2(p)|. In this case  using

pi(pi − F2(p))2 ≥ (F2(p)/2) · (cid:88)

(pi − F2(p))2

i∈Sh

|F2(p) − pi|)2
|Sh|

i∈Sh
≥ (F2(p)/2)2 · (/4)2 = 2F2

2(p)/64  

i∈Sh

≥ (F2(p)/2) · ((cid:80)

where the second inequality uses the deﬁnition of Sh  and the third is Cauchy-Schwarz.

We are now ready to prove Lemma 2.7. At a high level  the algorithm is simple. Compute ap-
proximations to F2(p) and F3(p) using Fact 2.1 and apply Lemma 2.8. However  there is one
technical problem with this scheme. Namely that the variance in our estimator for F3(p) depends
on the values of F4(p) and F5(p). If either of these are too large  then it will affect the accuracy
of our ﬁnal estimator. However  if p is uniform on a domain of size Θ(n)  it must be the case that
F4(p) = O(n−3) and F5(p) = O(n−4). Se we will ﬁrst perform a pre-processing step where we
verify that neither F4(p) nor F5(p) are too large  before estimating F2(p) and F3(p).

Proof of Lemma 2.7. The pseudocode is described in Algorithm 3.

Algorithm 3 Algorithm for Large 
1: procedure LARGE-EPS-TESTER(p  n  )
input: Sample access to arbitrary distribution p on unknown discrete domain Ω and n   > 0 and

 ≥ n−1/4.
output: “YES” with probability 9/10 if p is uniform on a set of size Θ(n)  “NO” with probability
9/10 if (cid:107)p(cid:107)2 = Θ(n−1/2) and (cid:107)p(cid:107)3 = Θ(n−2/3) and p is -far from any uniform distribution.
Let C  C(cid:48) be a sufﬁciently large constants  with C large enough relative to C(cid:48). Let m =

2:

Cn2/3/4/3.

Draw Poi(O(m)) samples from p and let(cid:98)γ4 denote the value of(cid:98)F4(p) on this sample.
if(cid:98)γ4 > C(cid:48)n−3 then return “NO”.
Draw Poi(O(m)) samples from p and let(cid:98)γ5 denote the value of(cid:98)F5(p) on this sample.
if(cid:98)γ5 > C(cid:48)n−4 then return “NO”.
Compute the estimates(cid:98)F2(p) (cid:98)F3(p) on two separate sets of Poi(m) samples.
(cid:16)(cid:98)F3(p) −(cid:98)F2(p)2 ≤ 2/(300n2)
(cid:17)

then return “YES”.

if
else return “NO”.

3:
4:
5:
6:
7:
8:
9:

with at least 99% probability.
with at least 99% probability unless Fr(p) = O(C(cid:48)(n1−r + m−r)).

In the completeness case  we claim that these steps will reject
In particular  if Fr(p) ≥

Note that the expected number of samples taken by this algorithm is O(m) = O(n2/3/4/3).
We next prove correctness. We start by considering Steps 3 through 6. Firstly  in the complete-

ness case  we note that Fr(p) = Θ(n1−r)  and therefore  by the Markov bound  (cid:98)γr ≤ C(cid:48)n1−r
KC(cid:48)(n1−r + m−r)  then m(cid:107)p(cid:107)r ≥ 1  and therefore  by Claim 2.4 we have that E[(cid:98)γr] = Fr(p)
and Var((cid:98)γr) = O(Fr(p)2/K 2). So  if K is sufﬁciently large  by Chebyshev’s inequality  with 99%
probability we have that(cid:98)(γ)r > Fr(p)/2 ≥ C(cid:48)n1−r. Thus  in the remainder  we can assume that
Var((cid:99)F2(p)) = O(m−2F2(p) + m−1F3(p)) = O(m−2n−1 + m−1n−2) = O(4/n2)/C   where
Var((cid:99)F3(p)) = O(m−3F3(p) + m−2F4(p) + m−1F5(p))

F4(p) = O(C(cid:48)(n−3 + m−4)) and F5(p) = O(C(cid:48)(n−4 + m−5)). To analyze Step 7  we note that
we use that  ≥ n−1/4 and m = Cn2/3/4/3. Similarly  we have

= O(m−3n−2 + C(cid:48)m−2n−3 + C(cid:48)m−6 + C(cid:48)m−1n−4) = O(4/n4)(C(cid:48)/C).

6

√

Therefore  by Chebyshev’s inequality  with 99% probability we have that |(cid:99)F2(p) − F2(p)| =
C   and |(cid:99)F3(p) − F3(p)| = O(2/n2)(cid:112)C(cid:48)/C . Assuming these hold  we have that
(cid:12)(cid:12)(cid:12)(cid:0)F3(p) − F2(p)2(cid:1) −(cid:16)(cid:99)F3(p) −(cid:99)F2(p)2(cid:17)(cid:12)(cid:12)(cid:12) = O(2/n2)(cid:112)C(cid:48)/C.

O(2/n)/

Thus  if C/C(cid:48) is sufﬁciently large  if p is uniform  we accept  and if p is -far from uniform  then by
Lemma 2.8  we reject. This completes the proof.

2.3 Algorithm for Small 
In this section  we give a tester that works for  ≤ n−1/4.
Lemma 2.9. There exists an algorithm that given sample access to a distribution p  and n   > 0
with  ≤ n−1/4 takes an expected O(n1/2/2) samples from p and distinguishes with probability at
least 9/10 between the cases: (i) p is the uniform distribution on a domain of size Θ(n)  and (ii) p
is -far from any uniform distribution.

Proof. The basic idea is that we will take Θ(n) samples from p and let S be the set of distinct
elements seen. We then test uniformity of (p|S) using the standard uniformity tester.

Algorithm 4 Algorithm for Small Epsilon
1: procedure SMALL-EPS-TESTER(p  n  )
input: Sample access to arbitrary distribution p on unknown discrete domain Ω and n   > 0 and

output: “YES” with probability 9/10 if p is uniform on a set of size Θ(n)  “NO” with probability

n−1/4 ≥ .

9/10 p is -far from any uniform distribution.

Let C  C(cid:48) be a sufﬁciently large constants with C large even relative to C(cid:48). Let m = Cn.
Draw Poi(m) samples from p. Let S be the subset of Ω that appears in the sample.
Verify the following conditions: (i) Each i ∈ S appears O(C log n) times; (ii) |S| = Θ(n).
if (either of conditions (i) or (ii)) is violated) then return “NO”.
Draw m(cid:48) = C
if fewer than half of these samples were in S then return “NO”.
Use the ﬁrst m(cid:48)/2 of these samples that landed in S to run the standard uniformity tester for

n/2 samples from p.

√

(p|S) with distance /C(cid:48) and 1% probability of error.

return the answer of the tester in Step 8.

2:
3:
4:
5:
6:
7:
8:

9:

We note that the expected number of samples is O(m + m(cid:48)) = O(n2/3/4/3). It remains to prove
correctness. We begin with the completeness case. If p is uniform over a set of size Θ(n)  with high
probability no bin will see more than O(C log(n)) samples  thus (i) is satisﬁed. Furthermore  we
note that with high probability that Poi(Cn) samples from p will cover more than two thirds of the
bins with high probability and thus (ii) will be satisﬁed. Additionally  this means that p(S) ≥ 2/3 
so again with high probability  at least half of our m(cid:48) samples will lie in S. These ﬁrst m(cid:48)/2
samples from S will be independent samples from (p|S)  which is uniform  and therefore with 99%
probability will pass the uniformity tester. Therefore  in this case  our algorithm will return “YES”
with probability at least 9/10.
For the soundness case  we note that if any bin has probability more than a sufﬁciently large multiple
of log(n)/n  we will fail to satisfy (i) with high probability and reject. We would like to claim next
that (p|S) is likely to be far from uniform  and thus that we will fail the ﬁnal test. Of course  this
may depend on the randomness over our ﬁrst set of samples  but we claim it with high probability.
In particular  we show (see supplementary material for the proof):
Lemma 2.10. If dTV (p CU ) ≥  and p assigns no more than O(log(n)/n) mass to any single bin 
then with high probability over the Poi(m) samples  we have at least one of the following: (i) |S| is
not Θ(n)  (ii) p(S) ≤ 1/3  (iii) dTV ((p|S) CU ) ≥ /C(cid:48).

7

Algorithm 5 The Full Tester
1: procedure GENERALIZED-UNIFORMITY-TESTER(p  )
input: Sample access to arbitrary distribution p on unknown discrete domain Ω and n   > 0.
output: “YES” with probability 2/3 if p is uniform on its support  “NO” with probability 2/3 p is

-far from any uniform distribution.

Let (cid:98)γ2 = Rough-Moment-Estimator(p  2).
Let (cid:98)γ3 = Rough-Moment-Estimator(p  3).
if (cid:98)γ3 is not Θ((cid:98)γ2
Let n = (cid:98)γ3

4/3) then return “NO”.

2:
3:
4:
5:
6:
7:

−3/2.

if  ≥ n−1/4 then return Large-Eps-Tester(p  n  )
if n−1/4 ≥  then return Small-Eps-Tester(p  n  )

2.4 Full Tester

4/3).
Assuming this holds  F2(p) = Θ(n−1/2) and F3(p) = Θ(n−2/3)  so the assumptions necessary for
our Small/Large- testers are satisﬁed  and they will work with appropriate probability.
For sample complexity  we note that the ﬁrst two lines take O(1/(cid:107)p(cid:107)3) samples in expectation. The

First  we verify correctness. With appropriately high probability (cid:98)γ2 and(cid:98)γ3 approximate (cid:107)p(cid:107)2 and
(cid:107)p(cid:107)3 respectively to within constant factors. In this case  p cannot be uniform unless (cid:98)γ3 = Θ((cid:98)γ2
remaining lines use an expected O(n2/3/4/3+n1/2/2) samples. This is O(1/(4/3(cid:98)γ3)+1/(2(cid:98)γ2)).
Our ﬁnal expected sample bound follows from noting by Lemma 2.2 that the expected values of 1/(cid:98)γ3
and 1/(cid:98)γ2 are O(1/(cid:107)p(cid:107)3) and O(1/(cid:107)p(cid:107)2)  respectively. This completes our proof.

3 Sample Complexity Lower Bound

√
In this section  we sketch a sample size lower bound matching our algorithm in Proposition 2.5. One
n/2) samples are
part of the lower bound is fairly easy. In particular  it is known [Pan08] that Ω(
required to test uniformity of a distribution with a known support of size n. It is easy to see that the
hard cases for this lower bound still work when (cid:107)p(cid:107)2 = Θ(n−1/2) and (cid:107)p(cid:107)3 = Θ(n−2/3).
The other half of the lower bound is somewhat more difﬁcult and we rely on the lower bound
techniques of [DK16]. In particular  for n > 0  and 1/10 >  > n−1/4 and for N sufﬁciently large 
we produce a pair of distributions D and D(cid:48) over positive measures on [N ]  so that: 1. A random
sample from D or D(cid:48) has total mass Θ(1) with high probability. 2. A random sample from D or
D(cid:48) has (cid:107)p(cid:107)2 = Θ(n−1/2) and (cid:107)p(cid:107)3 = Θ(n−2/3) with high probability. 3. A sample from µ ∈ D
has µ/(cid:107)µ(cid:107)1 the uniform distribution over some subset of [N ] with probability 1. 4. A sample from
µ ∈ D(cid:48) has µ/||µ(cid:107)1 at least Ω()-far from any uniform distribution with high probability. 5. Given a
measure µ taking randomly from either D or D(cid:48)  no algorithm given the output of a Poisson process
with intensity kµ for k = o(min(n2/3/4/3  n)) can reliably distinguish between a µ taken from D
and µ taken from D(cid:48).
Before we exhibit these families  we ﬁrst discuss why the above is sufﬁcient. This Poissonization
technique has been used previously in various settings [VV14  DK16  WY16  DGPP17]  so we only
provide a sketch here. In particular  suppose that we have such families D and D(cid:48)  but that there is
also an algorithm A that distinguishes between a distribution p being uniform and being -far from
uniform when (cid:107)p(cid:107)2 = Θ(n−1/2) and (cid:107)p(cid:107)3 = Θ(n−2/3) in m = o(n2/3/4/3) samples. We show
that we can use algorithm A to violate property 5 above. In particular  letting p = µ/(cid:107)µ(cid:107)1 for µ a
random measure taken from either D or D(cid:48)  we note that with high probability (cid:107)p(cid:107)2 = Θ(n−1/2)
and (cid:107)p(cid:107)3 = Θ(n−2/3). Therefore  m(cid:48) = o(n2/3/4/3) samples are sufﬁcient to distinguish between
p being uniform and being Ω() far from uniform. However  by properties 3 and 4  this is equivalent
to distinguish between µ being taken from D and being taken from D(cid:48). On the other hand  given the
output of a Poisson process with intensity Cm(cid:48)µ  for C a sufﬁciently large constant  a random m(cid:48)
of these samples (note that there are at least m(cid:48) total samples with high probability) are distributed
identically to m(cid:48) samples from p. Thus  applying A to these samples distinguishes between µ taken
from D and µ taken from D(cid:48)  thus contradicting property 5. Due to space constraints  the technical
details are deferred to the supplementary material.

8

4 Conclusions

In this paper  we gave tight upper and lower bounds on the sample complexity of generalized
uniformity testing – a natural non-trivial generalization of uniformity testing  recently introduced
in [BC17]. The obvious research question is to understand the sample complexity of testing more
general symmetric properties (e.g.  closeness  independence  etc.) for the regime where the domain
of the underlying distributions is discrete but unknown (of unknown size). Is it possible to obtain
sub-learning sample complexities for these problems? And what is the optimal sample complexity
for each of these tasks?

References
[ADK15] J. Acharya  C. Daskalakis  and G. Kamath. Optimal testing for properties of distribu-

tions. In NIPS  pages 3591–3599  2015.

[AOST17] J. Acharya  A. Orlitsky  A. T. Suresh  and H. Tyagi. Estimating renyi entropy of discrete

distributions. IEEE Trans. Information Theory  63(1):38–56  2017.

[BC17] T. Batu and C. Canonne. Generalized uniformity testing. CoRR  abs/1708.04696  2017.

To appear in FOCS’17.

[BFR+00] T. Batu  L. Fortnow  R. Rubinfeld  W. D. Smith  and P. White. Testing that distributions
are close. In IEEE Symposium on Foundations of Computer Science  pages 259–269 
2000.

[BKR04] T. Batu  R. Kumar  and R. Rubinfeld. Sublinear algorithms for testing monotone and
unimodal distributions. In ACM Symposium on Theory of Computing  pages 381–390 
2004.

[BLM13] S. Boucheron  G. Lugosi  and P. Massart. Concentration Inequalities: A Nonasymptotic

Theory of Independence. OUP Oxford  2013.

[Can15] C. L. Canonne. A survey on distribution testing: Your data is big. but is it blue? Elec-

tronic Colloquium on Computational Complexity (ECCC)  22:63  2015.

[CDGR16] C. L. Canonne  I. Diakonikolas  T. Gouleakis  and R. Rubinfeld. Testing shape restric-
tions of discrete distributions. In 33rd Symposium on Theoretical Aspects of Computer
Science  STACS 2016  pages 25:1–25:14  2016.

[CDKS17] C. L. Canonne  I. Diakonikolas  D. M. Kane  and A. Stewart. Testing bayesian networks.
In Proceedings of the 30th Conference on Learning Theory  COLT 2017  pages 370–
448  2017.

[CDKS18] C. L. Canonne  I. Diakonikolas  D. M. Kane  and A. Stewart. Testing conditional in-
dependence of discrete distributions. In Proceedings of the 50th Annual ACM SIGACT
Symposium on Theory of Computing  STOC 2018  pages 735–748  2018.

[CDS17] C. L. Canonne  I. Diakonikolas  and A. Stewart. Fourier-based testing for families of

distributions. CoRR  abs/1706.05738  2017.

[CDVV14] S. Chan  I. Diakonikolas  P. Valiant  and G. Valiant. Optimal algorithms for testing

closeness of discrete distributions. In SODA  pages 1193–1203  2014.

[DDK16] C. Daskalakis  N. Dikkala  and G. Kamath.

abs/1612.03147  2016.

Testing ising models.

CoRR 

[DDS+13] C. Daskalakis  I. Diakonikolas  R. Servedio  G. Valiant  and P. Valiant. Testing k-modal

distributions: Optimal algorithms via reductions. In SODA  pages 1833–1852  2013.

[DGPP16] I. Diakonikolas  T. Gouleakis  J. Peebles  and E. Price. Collision-based testers are opti-
mal for uniformity and closeness. Electronic Colloquium on Computational Complexity
(ECCC)  23:178  2016.

9

[DGPP17] I. Diakonikolas  T. Gouleakis  J. Peebles  and E. Price. Sample-optimal identity testing

with high probability. CoRR  abs/1708.02728  2017.

[DGPP18] I. Diakonikolas  T. Gouleakis  J. Peebles  and E. Price. Sample-optimal identity testing
with high probability. In 45th International Colloquium on Automata  Languages  and
Programming  ICALP 2018  pages 41:1–41:14  2018.

[DK16] I. Diakonikolas and D. M. Kane. A new approach for testing properties of discrete
distributions. In FOCS  pages 685–694  2016. Full version available at abs/1601.05557.

[DKN15a] I. Diakonikolas  D. M. Kane  and V. Nikishkin. Optimal algorithms and lower bounds
for testing closeness of structured distributions. In IEEE 56th Annual Symposium on
Foundations of Computer Science  FOCS 2015  pages 1183–1202  2015.

[DKN15b] I. Diakonikolas  D. M. Kane  and V. Nikishkin. Testing identity of structured distribu-
tions. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete
Algorithms  SODA 2015  pages 1841–1854  2015.

[DKN17] I. Diakonikolas  D. M. Kane  and V. Nikishkin. Near-optimal closeness testing of dis-
In 44th International Colloquium on Automata  Lan-

crete histogram distributions.
guages  and Programming  ICALP 2017  pages 8:1–8:15  2017.

[DP17] C. Daskalakis and Q. Pan. Square hellinger subadditivity for bayesian networks and
its applications to identity testing. In Proceedings of the 30th Conference on Learning
Theory  COLT 2017  pages 697–703  2017.

[Gol16] O. Goldreich. The uniform distribution is complete with respect to testing identity to a

ﬁxed distribution. ECCC  23  2016.

[GR00] O. Goldreich and D. Ron. On testing expansion in bounded-degree graphs. Technical

Report TR00-020  Electronic Colloquium on Computational Complexity  2000.

[Pan08] L. Paninski. A coincidence-based test for uniformity given very sparsely-sampled dis-

crete data. IEEE Transactions on Information Theory  54:4750–4755  2008.

[Rub12] R. Rubinfeld. Taming big probability distributions. XRDS  19(1):24–28  2012.

[VV14] G. Valiant and P. Valiant. An automatic inequality prover and instance optimal identity

testing. In FOCS  2014.

[WY16] Y. Wu and P. Yang. Minimax rates of entropy estimation on large alphabets via best
IEEE Transactions on Information Theory  62(6):3702–

polynomial approximation.
3720  June 2016.

10

,Ilias Diakonikolas
Daniel M. Kane
Alistair Stewart