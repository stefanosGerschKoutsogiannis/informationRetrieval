2015,Efficient Learning by Directed Acyclic Graph For Resource Constrained Prediction,We study the problem of reducing test-time acquisition costs in classification systems. Our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction. We model our system as a directed acyclic graph (DAG) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements. This problem can be naturally posed as an empirical risk minimization over training data. Rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes  we propose an efficient algorithm motivated by dynamic programming. We learn node policies in the DAG by reducing the global objective to a series of cost sensitive learning problems. Our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture. In addition  we present an extension to map other budgeted learning problems with large number of sensors to our DAG architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors.,Efﬁcient Learning by Directed Acyclic Graph For

Resource Constrained Prediction

Joseph Wang

Department of Electrical
& Computer Engineering

Boston University 
Boston  MA 02215
joewang@bu.edu

Kirill Trapeznikov

Systems & Technology Research

Woburn  MA 01801

kirill.trapeznikov@

stresearch.com

Abstract

Venkatesh Saligrama
Department of Electrical
& Computer Engineering

Boston University 
Boston  MA 02215

srv@bu.edu

We study the problem of reducing test-time acquisition costs in classiﬁcation sys-
tems. Our goal is to learn decision rules that adaptively select sensors for each
example as necessary to make a conﬁdent prediction. We model our system as a
directed acyclic graph (DAG) where internal nodes correspond to sensor subsets
and decision functions at each node choose whether to acquire a new sensor or
classify using the available measurements. This problem can be posed as an em-
pirical risk minimization over training data. Rather than jointly optimizing such
a highly coupled and non-convex problem over all decision nodes  we propose an
efﬁcient algorithm motivated by dynamic programming. We learn node policies
in the DAG by reducing the global objective to a series of cost sensitive learning
problems. Our approach is computationally efﬁcient and has proven guarantees of
convergence to the optimal system for a ﬁxed architecture. In addition  we present
an extension to map other budgeted learning problems with large number of sen-
sors to our DAG architecture and demonstrate empirical performance exceeding
state-of-the-art algorithms for data composed of both few and many sensors.

1

Introduction

Many scenarios involve classiﬁcation systems constrained by measurement acquisition budget. In
this setting  a collection of sensor modalities with varying costs are available to the decision system.
Our goal is to learn adaptive decision rules from labeled training data that  when presented with an
unseen example  would select the most informative and cost-effective acquisition strategy for this
example. In contrast  non-adaptive methods [24] attempt to identify a common sparse subset of
sensors that can work well for all data. Our goal is an adaptive method that can classify typical cases
using inexpensive sensors while using expensive sensors only for atypical cases.
We propose an adaptive sensor acquisition system learned using labeled training examples. The
system  modeled as a directed acyclic graph (DAG)  is composed of internal nodes  which contain
decision functions  and a single sink node (the only node with no outgoing edges)  representing
the terminal action of stopping and classifying (SC). At each internal node  a decision function
routes an example along one of the outgoing edges. Sending an example to another internal node
represents acquisition of a previously unacquired sensor  whereas sending an example to the sink
node indicates that the example should be classiﬁed using the currently acquired set of sensors. The
goal is to learn these decision functions such that the expected error of the system is minimized
subject to an expected budget constraint.
First  we consider the case where the number of sensors available is small (as in [19  23  20])  though
the dimensionality of data acquired by each sensor may be large (such as an image taken in different

1

[19] and Wang et al.

modalities). In this scenario  we construct a DAG that allows for sensors to be acquired in any order
and classiﬁcation to occur with any set of sensors. In this regime  we propose a novel algorithm to
learn node decisions in the DAG by emulating dynamic programming (DP). In our approach  we
decouple a complex sequential decision problem into a series of tractable cost-sensitive learning
subproblems. Cost-sensitive learning (CSL) generalizes multi-decision learning by allowing deci-
sion costs to be data dependent [2]. Such reduction enables us to employ computationally efﬁcient
CSL algorithms for iteratively learning node functions in the DAG. In our theoretical analysis  we
show that  given a ﬁxed DAG architecture  the policy risk learned by our algorithm converges to the
Bayes risk as the size of the training set grows.
Next  we extend our formulation to the case where a large number of sensors exist  but the number
of distinct sensor subsets that are necessary for classiﬁcation is small (as in [25  11] where the depth
of the trees is ﬁxed to 5). For this regime  we present an efﬁcient subset selection algorithm based
on sub-modular approximation. We treat each sensor subset as a new “sensor ” construct a DAG
over unions of these subsets  and apply our DP algorithm. Empirically  we show that our approach
outperforms state-of-the-art methods in both small and large scale settings.
Related Work: There is an extensive literature on adaptive methods for sensor selection for reducing
test-time costs. It arguably originated with detection cascades (see [26  4] and references therein) 
a popular method in reducing computation cost in object detection for cases with highly skewed
class imbalance and generic features. Computationally cheap features are used at ﬁrst to ﬁlter out
negative examples and more expensive features are used in later stages.
[23  20].
Our technical approach is closely related to Trapeznikov et al.
Like us they formulate an ERM problem and generalize detection cascades to classiﬁer cascades
and trees and handle balanced and/or multi-class scenarios. Trapeznikov et al.
[19] propose a
similar training scheme for the case of cascades  however restrict their training to cascades and
simple decision functions which require alternating optimization to learn. Alternatively  Wang et
al. [21  22  23  20] attempt to jointly solve the decision learning problem by formulating a linear
upper-bounding surrogate  converting the problem into a linear program (LP).
Conceptually  our work is closely related to Xu et al. [25]
and Kusner et al.[11]  who introduce Cost-Sensitive Trees
of Classiﬁers (CSTC) and Approximately Submodular
Trees of Classiﬁers (ASTC)  respectively  to reducing test
time costs. Like our paper they propose a global ERM
problem. They solve for the tree structure  internal de-
cision rules and leaf classiﬁers jointly using alternative
minimization techniques. Recently  Kusner et al.[11]
propose Approximately Submodular Trees of Classiﬁers
(ASTC)  a variation of CSTC which provides robust per-
formance with signiﬁcantly reduced training time and
greedy approximation  respectively. Recently  Nan et al.
[14] proposed random forests to efﬁciently learn budgeted
systems using greedy approximation over large data sets.
The subject of this paper is broadly related to other
adaptive methods in the literature. Generative methods
[17  8  9  6] pose the problem as a POMDP  learn condi-
tional probability models  and myopically select features
based information gain of unknown features. MDP-based methods [5  10  7  3] encode current obser-
vations as state  unused features as action space  and formulate various reward functions to account
for classiﬁcation error and costs. He et al. [7] apply imitation learning of a greedy policy with a sin-
gle classiﬁcation step as actions. Dulac-Arnold et al. [5] and Karayev et al. [10] apply reinforcement
learning to solve this MDP. Benbouzid et al.[3] propose classiﬁer cascades with an additional skip
action within an MDP framework. Nan et al. [15] consider a nearest neighbor approach to feature
selection  with conﬁdence driven by margin magnitude.

Figure 1: A simple example of a sensor
selection DAG for a three sensor system.
At each state  represented by a binary vec-
tor indicating measured sensors  a policy π
chooses between either adding a new sensor
or stopping and classifying. Note that the
state sSC has been repeated for simplicity.

2

2 Adaptive Sensor Acquisition by DAG

In this section  we present our adaptive sensor acquisition DAG that during test-time sequentially de-
cides which sensors should be acquired for every new example entering the system. Before formally
describing the system and our learning approach  we ﬁrst provide a simple illustration for a 3 sensor
DAG shown in Fig. 1. The state indicating acquired sensors is represented by a binary vector  with
a 0 indicating that a sensor measurement has not been acquired and a 1 representing an acquisition.
Consider a new example that enters the system. Initially  it has a state of [0  0  0]T (as do all samples
during test-time) since no sensors have been acquired. It is routed to the policy function π0  which
makes a decision to measure one of the three sensors or to stop and classify. Let us assume that the
function π0 routes the example to the state [1  0  0]T   indicating that the ﬁrst sensor is acquired. At
this node  the function π1 has to decide whether to acquire the second sensor  acquire the third  or
classifying using only the ﬁrst. If π1 chooses to stop and classify then this example will be classiﬁed
using only the ﬁrst sensor.
Such decision process is performed for every new example. The system adaptively collects sensors
until the policy chooses to stop and classify (we assume that when all sensors have been collected
the decision function has no choice but to stop and classify  as shown for π7 in Fig. 1).
Problem Formulation: A data instance  x ∈ X   consists of M sensor measurements  x =
{x1  x2  . . .   xM}  and belongs to one of L classes indicated by its label y ∈ Y = {1  2  . . . L}.
Each sensor measurement  xm  is not necessarily a scalar but may instead be multi-dimensional. Let
the pair  (x  y)  be distributed according to an unknown joint distribution D. Additionally  associated
with each sensor measurement xm is an acquisition cost  cm.
To model the acquisition process  we deﬁne a state space S = {s1  . . .   sK  sSC}. The states
{s1  . . .   sK} represent subsets of sensors  and the stop-and-classify state sSC represents the action
of stopping and classifying with a current subset. Let Xs correspond to the space of sensor mea-
surements in subset s. We assume that the state space includes all possible subsets1  K = 2M .
For example in Fig. 1  the system contains all subsets of 3 sensors. We also introduce the state
transition function  T : S → S  that deﬁnes a set of actions that can be taken from the current
state. A transition from the current sensor subset to a new subset corresponds to an acquisition of
new sensor measurements. A transition to the state sSC corresponds to stopping and classifying
using the available information. This terminal state  sSC  has access to a classiﬁer bank used to
predict the label of an example. Since classiﬁcation has to operate on any sensor subset  there is one
classiﬁer for every sk: fs1   . . .   fsK such that fs : Xs → Y. We assume the classiﬁer bank is given
and pre-trained. Practically  the classiﬁers can be either unique for each subset or a missing feature
(i.e. sensor) classiﬁcation system as in [13]. We overload notation and use node  subset of sensors 
and path leading up to that subset on the DAG interchangeably. In particular we let S denote the
collection of subsets of nodes. Each subset is associated with a node on the DAG graph. We refer to
each node as a state since it represents the “state-of-information” for an instance at that node.
We deﬁne the loss associated with classifying an example/label pair (x  y) using the sensors in sj as
(1)

Using this convention  the loss is the sum of the empirical risk associated with classiﬁer fsj and the
cost of the sensors in the subset sj. The expected loss over the data is deﬁned

Lsj (x  y) = 1fsj (x)(cid:54)=y +

(cid:88)
LD(π) = Ex y∼D(cid:2)Lπ(x)(x  y)(cid:3) .

k∈sj

ck.

(2)
Our goal is to ﬁnd a policy which adaptively selects subsets for examples such that their average
loss is minimized

min
π∈Π

(3)
where π : X → S is a policy selected from a family of policies Π and π(x) is the state selected by
the policy π for example x. We denote the quantity LD as the value of (3) when Π is the family
of all measurable functions. LD is the Bayes cost  representing the minimum possible cost for any
1While enumerating all possible combinations is feasible for small M  for large M this problem becomes
intractable. We will overcome this limitation in Section 3 by applying a novel sensor selection algorithm. For
now  we remain in the small M regime.

LD(π) 

3

function given the distribution of data. In practice  the distribution D is unknown  and instead we
are given training examples (x1  y1)  . . .   (xn  yn) drawn I.I.D. from D. The problem becomes an
empirical risk minimization:
(4)

n(cid:88)

Lπ(xi)(xi  yi).

min
π∈Π

i=1

Recall that our sensor acquisition system is represented as a DAG. Each node in a graph corresponds
to a state (i.e. sensor subset) in S  and the state transition function  T (sj)  deﬁnes the outgoing edges
from every node sj. We refer to the entire edge set in the DAG as E. In such a system  the policy π is
parameterized by the set of decision functions π1  . . .   πK at every node in the DAG. Each function 
πj : X → T (sj)  maps an example to a new state (node) from the set speciﬁed by outgoing edges.
Rather than directly minimizing the empirical risk in (4)  ﬁrst  we deﬁne a step-wise cost associated
with all edges (sj  sk) ∈ E

(cid:40)(cid:80)

C(x  y  sj  sk) =

(5)
C(·) is either the cost of acquiring new sensors or is the classiﬁcation error induced by classifying
with the current subset if sk = sSC. Using this step-wise cost  we deﬁne the empirical loss of the
system w.r.t a path for an example x:

.

ct

t∈sk\sj
1fsj (x)(cid:54)=y

if sk (cid:54)= sSC
otherwise

R (x  y  π1  ...  πK ) =

C (x  y  sj  sj+1)  

(6)

(sj  sj+1) ∈ path(x π1 ... πK )

(cid:88)

n(cid:88)

i=1

where path (x  π1  . . .   πK) is the path on the DAG induced by the policy functions π1  . . .   πK for
example x. The empirical minimization equivalent to (4) for our DAG system is a sample average
over all example speciﬁc path losses:

R (xi  yi  π1  . . .   πK ) .

(7)

∗
1   . . .   π

∗
K = argmin

π

π1 ... πK∈Π

Next  we present a reduction to learn the functions π1  ...  πK that minimize the loss in (7).

2.1 Learning Policies in a DAG

Learning the functions π1  . . .   πK that minimize the cost in (7) is a highly coupled problem. Learn-
ing a decision function πj is dependent on the other functions in two ways: (a) πj is dependent on
functions at nodes downstream (nodes for which a path exists from πj)  as these determine the cost
of each action taken by πj on an individual example (the cost-to-go)  and (b) πj is dependent on
functions at nodes upstream (nodes for which a path exists to πj)  as these determine the distribution
of examples that πj acts on. Consider a policy πj at a node corresponding to state sj such that all
outgoing edges from j lead to leaves. Also  we assume all examples pass through this node πj (we
are ignoring the effect of upstream dependence b). This yields the following important lemma:
Lemma 2.1. Given the assumptions above  the problem of minimizing the risk in (6) w.r.t a single
policy function  πj  is equivalent to solving a k-class cost sensitive learning (CSL) problem.2
Proof. Consider the risk in (6) with πj such that all outgoing edges from j lead to a leaf. Ignoring
the effect of other policy functions upstream from j  the risk w.r.t πj is:

R(x  y  πj) =

C(x  y  sj  sk)1πj (x)=sk → min
π∈Π

R(xi  yi  πj).

(cid:88)

sk∈T (sj )

n(cid:88)

i=1

Minimizing the risk over training examples yields the optimization problem on the right hand side.
This is equivalent to a CSL problem over the space of “labels” T (sj) with costs given by the transi-
tion costs C(x  y  sj  sk).

In order to learn the policy functions π1  . . .   πK  we propose Algorithm 1  which iteratively learns
policy functions using Lemma 2.1. We solve the CSL problem by using a ﬁlter-tree scheme [2]
for Learn  which constructs a tree of binary classiﬁers. Each binary classiﬁer can be trained using
regularized risk minimization. For concreteness we deﬁne the Learn algorithm as

Learn ((x1  (cid:126)w1)  ...  (xn  (cid:126)wn)) (cid:44)
F ilterT ree((x1  (cid:126)w1)  ...  (xn  (cid:126)wn))

(8)
where the binary classiﬁers in the ﬁlter tree are trained using an appropriately regularized cali-
brated convex loss function. Note that multiple schemes exist that map the CSL problem to binary
classiﬁcation.

2We consider the k-class CSL problem formulated by Beygelzimer et al. [2]  where an instance of the
problem is deﬁned by a distribution D over X ×[0  inf)k  a space of features and associated costs for predicting
each of the k labels for each realization of features. The goal is to learn a function which maps each element of
X to a label {1  . . .   k} s.t. the expected cost is minimized.

4

A single iteration of Algorithm 1 pro-
ceeds as follows: (1) A node j is cho-
sen whose outgoing edges connect only
to leaf nodes. (2) The costs associated
with each connected leaf node are found.
(3) The policy πj is trained on the entire
set of training data according to these
(4)
costs by solving a CSL problem.
The costs associated with taking the ac-
tion πj are computed for each example 
and the costs of moving to state j are
updated. (5) Outgoing edges from node
j are removed (making it a leaf node) 
and (6) disconnected nodes (that were
previously connected to node j) are re-
moved. The algorithm iterates through
these steps until all edges have been re-
moved. We denote the policy functions
trained on the empirical data using Alg.
1 as πn

K.
1   . . .   πn

2.2 Analysis

Algorithm 1 Graph Reduce Algorithm

i=1 

Input: Data: (xi  yi)n
DAG: (nodes S  edges E  costs C(xi  yi  e) ∀e ∈ E) 
CSL alg: Learn ((x1  (cid:126)w1)  . . .   (xn  (cid:126)wn))) → π(·)
while Graph S is NOT empty do

(1) Choose a node  j ∈ S  s.t. all children of j are
leaf nodes
for example i ∈ {1  . . .   n} do

(2) Construct the weight vector (cid:126)wi of edge costs
per action.

end for
(3) πj ← Learn ((x1  (cid:126)w1)  . . .   (xn  (cid:126)wn))
(4) Evaluate πj and update edge costs to node j:
C(xi  yi  sn  sj) ← (cid:126)wj
i (πj(xi)) + C(xi  yi  sn  sj)
(5) Remove all outgoing edges from node j in E
(6) Remove all disconnected nodes from S.

end while
Output: Policy functions  π1  . . .   πK

Our goal is to show that the expected risk of the policy functions π1  . . .   πK learned by Alg. 1
converge to the Bayes risk. We ﬁrst state our main result:
Theorem 2.2. Alg. 1 is universally consistent  that is

(9)
K are the policy functions learned using Alg. (1)  which in turn uses Learn de-

1   . . .   πn

lim

n→∞LD(πn

K) → LD

where πn
1   . . .   πn
scribed by Eq. 8.

Alg. 1 emulates a dynamic program applied in an empirical setting. Policy functions are decoupled
and trained from leaf to root conditioned on the output of descendant nodes.
To adapt to the empirical setting  we optimize at each stage over all examples in the training set. The
key insight is the fact that universally consistent learners output optimal decisions over subsets of the
space of data  that is they are locally optimal. To illustrate this point  consider a standard classiﬁca-
tion problem. Let X (cid:48) ⊂ X be the support (or region) of examples induced by upstream deterministic
decisions. d∗ and f∗  Bayes optimal classiﬁers w.r.t the full space and subset  respectively  are equal
on the reduced support:

E(cid:2)1f (x)(cid:54)=y|x  x ∈ X (cid:48) ⊂ X(cid:3) ∀ x ∈ X (cid:48)

E(cid:2)1d(x)(cid:54)=y|x(cid:3) = f

∗
d

.

∗

(x) = arg min

f

(x) = arg min

d

From this insight  we decouple learning problems while still training a system that converges to the
Bayes risk. This can be achieved by training universally consistent CSL algorithms such as ﬁlter
trees [2] that reduce the problem to binary classiﬁcation. By learning consistent binary classiﬁers
[1  18]  the risk of the cost-sensitive function can be shown to converge to the Bayes risk [2]. Proof
of Theorem 2.2 is included in the Supplementary Material.
Computational Efﬁciency: Alg. 1 reduces the problem to solving a series of O(KM ) binary clas-
siﬁcation problems  where K is the number of nodes in the DAG and M is the number of sensors.
Finding each binary classiﬁer is computationally efﬁcient  reducing to a convex problem with O(n)
variables. In contrast  nearly all previous approaches require solving a non-convex problem and
resort to alternating optimization [25  19] or greedy approximation [11]. Alternatively  convex sur-
rogates proposed for the global problem [23  20] require solving large convex programs with θ(n)
variables  even for simple linear decision functions. Furthermore  existing off-the-shelf algorithms
cannot be applied to train these systems  often leading to less efﬁcient implementation.

2.3 Generalization to Other Budgeted Learning Problems

Although  we presented our algorithm in the context of supervised classiﬁcation and a uniform
linear sensor acquisition cost structure  the above framework holds for a wide range of problems.

5

In particular  any loss-based learning problem can be solved using the proposed DAG approach by
generalizing the cost function

(cid:40)

˜C(x  y  sj  sk) =

c(x  y  sj  sk)
D (x  y  sj)

if sk (cid:54)= sSC
otherwise

 

(10)

where c(x  y  sj  sk) is the cost of acquiring sensors in sk\sj for example (x  y) given the current
state sj and D (x  y  sj) is some loss associated with applying sensor subset sj to example (x  y).
This framework allows for signiﬁcantly more complex budgeted learning problems to be handled.
For example  the sensor acquisition cost  c(x  y  sj  sk)  can be object dependent and non-linear 
such as increasing acquisition costs as time increases (which can arise in image retrieval problems 
where users are less likely to wait as time increases). The cost D (x  y  sj) can include alternative
costs such as (cid:96)2 error in regression  precision error in ranking  or model error in structured learning.
As in the supervised learning case  the learning functions and example labels do not need to be
explicitly known. Instead  the system requires only empirical performance to be provided  allow-
ing complex decision systems (such as humans) to be characterized or systems learned where the
classiﬁers and labels are sensitive information.

3 Adaptive Sensor Acquisition in High-Dimensions

So far  we considered the case where the DAG system allows for any subset of sensors to be ac-
quired  however this is often computationally intractable as the number of nodes in the graph grows
exponentially with the number of sensors. In practice  these complete systems are only feasible for
data generated from a small set of sensors ( 10 or less).

3.1 Learning Sensor Subsets

Figure 2: An example of a DAG system us-
ing the 3 sensor subsets shown on the bot-
tom left. The new states are the union of
these sensor subsets  with the system other-
wise constructed in the same fashion as the
small scale system.

Although constructing an exhaustive DAG for data with
a large number of sensors is computationally intractable 
in many cases this is unnecessary. Motivated by previous
methods [6  25  11]  we assume that the number of “ac-
tive” nodes in the exhaustive graph is small  that is these
nodes are either not visited by any examples or all ex-
amples that visit the node acquire the same next sensor.
Equivalently  this can be viewed as the system needing
only a small number of sensor subsets to classify all ex-
amples with low acquisition cost.
Rather than attempt to build the entire combinatorially
sized graph  we instead use this assumption to ﬁrst ﬁnd
these “active” subsets of sensors and construct a DAG to choose between unions of these subsets.
The step of ﬁnding these sensor subsets can be viewed as a form of feature clustering  with a goal
of grouping features that are jointly useful for classiﬁcation. By doing so  the size of the DAG is re-
duced from exponential in the number of sensors  2M   to exponential in a much smaller user chosen
parameter number of subsets  2t. In experimental results  we limit t = 8  which allows for a diverse
subsets of sensors to be found while preserving computational tractability and efﬁciency.
Our goal is to learn sensor subsets with high classiﬁcation performance and low acquisition cost
(empirically low cost as deﬁned in (1)). Ideally  our goal is to jointly learn the subsets which mini-
mize the empirical risk of the entire system as deﬁned in (4)  however this presents a computationally
intractable problem due to the exponential search space. Rather than attempt to solve this difﬁcult
problem directly  we minimize classiﬁcation error over a collection of sensor subsets σ1  . . .   σt
subject to a cost constraint on the total number of sensors used. We decouple the problem from the
policy learning problem by assuming that each example is classiﬁed by the best possible subset. For
a constant sensor cost  the problem can be expressed as a set constraint problem:
|σj| ≤ B
δ

1fσj (xi)(cid:54)=yi

min

j∈{1 ... t}

N(cid:88)

such that:

(11)

(cid:105)

(cid:104)

min

σ1 ... σt

1
N

i=1

t(cid:88)

j=1

 

where B is the total sensor budget over all sensor subsets and δ is the cost of a single sensor.

6

(cid:104)

N(cid:88)

i=1

(cid:105) → max

σ1 ... σt

t(cid:88)

j=1

Although minimizing this loss is still computationally intractable  consider instead the equivalent
problem of maximizing the “reward” (the event of a correct classiﬁcation) of the subsets  deﬁned as

G =

max

j∈{1 ... t}

1fσj (xi)=yi

1
N

G(c1  . . .   ct) such that:

|σj| ≤ B
δ

.

(12)

This problem is related to the knapsack problem with a non-linear objective. Maximizing the reward
in (12) is still a computationally intractable problem  however the reward function is structured to
allow for efﬁcient approximation.
Lemma 3.1. The objective of the maximization in (12) is sub-modular with respect to the set of
subsets  such that adding any new set to the reward yields diminishing returns.
Theorem 3.2. Given that the empirical risk of each classiﬁer fσk is submodular and monotonically
decreasing w.r.t.
the elements in σk and uniform sensor costs  the strategy in Alg. 2 is an O(1)
approximation of the optimal reward in (12).
Proof of these statements is included in the Supplementary Material and centers on showing that the
objective is sub-modular  and therefore applying a greedy strategy yields a 1 − 1
e approximation of
the optimal strategy [16].

Algorithm 2 Sensor Subset Selection

3.2 Constructing DAG using Sensor Subsets

Alg. 2 requires computation of the reward G for

δ tM(cid:1) sensor subsets  where M is the num-

only O(cid:0) B

Input: Number of Subsets t  Cost Con-
straint B
δ
Output: Feature subsets  σ1  . . .   σt
Initialize: σ1  . . .   σt = ∅
(i  j) = argmaxi∈{1 ... t} argmaxj∈σC
G(σ1  ...  σi ∪ j  ...  σt)
j=1 |σj| ≤ C
δ do

while(cid:80)T

ber of sensors  to return a constant-order approxima-
tion to the NP-hard knapsack-type problem. Given
the set of sensor subsets σ1  . . .   σt  we can now
construct a DAG using all possible unions of these
subsets  where each sensor subset σj is treated as
a new single sensor  and apply the small scale sys-
tem presented in Sec. 2. The result is an efﬁciently
learned system with relatively low complexity yet
strong performance/cost trade-off. Additionally  this
result can be extended to the case of non-uniform
costs  where a simple extension of the greedy algorithm yields a constant-order approximation [12].
A simple case where three subsets are used is shown in Fig. 2. The three learned subsets of sensors
are shown on the bottom left of Fig. 2  and these three subsets are then used to construct the entire
DAG in the same fashion as in Fig. 1. At each stage  the state is represented by the union of sensor
subsets acquired. Grouping the sensors in this fashion reduces the size of the graph to 8 nodes as
opposed to 64 nodes required if any subset of the 6 sensors can be selected. This approach allows
us to map high-dimensional adaptive sensor selection problems to small scale DAG in Sec. 2.

σi = σi ∪ j
(i  j) = argmaxi∈{1 ... t} argmaxj∈σC
G(σ1  ...  σi ∪ j  ...  σt)

end while

i

i

4 Experimental Results

To demonstrate the performance of our DAG sensor acquisition system  we provide experimental
results on data sets previously used in budgeted learning. Three data sets previously used for budget
cascades [19  23] are tested. In these data sets  examples are composed of a small number of sensors
(under 4 sensors). To compare performance  we apply the LP approach to learning sensor trees [20]
and construct trees containing all subsets of sensors as opposed to ﬁxed order cascades [19  23].
Next  we examine performance of the DAG system using 3 higher dimensional sets of data previ-
ously used to compare budgeted learning performance [11]. In these cases  the dimensionality of
the data (between 50 and 400 features) makes exhaustive subset construction computationally infea-
sible. We greedily construct sensor subsets using Alg. 2  then learn a DAG over all unions of these
sensor subsets. We compare performance with CSTC [25] and ASTC [11].
For all experiments  we use cost sensitive ﬁlter trees [2]  where each binary classiﬁer in the tree is
learned using logistic regression. Homogeneous polynomials are used as decision functions in the
ﬁlter trees. For all experiments  uniform sensor cost were were varied in the range [0  M ] achieve
systems with different budgets. Performance between the systems is compared by plotting the aver-
age number of features acquired during test-time vs. the average test error.

7

4.1 Small Sensor Set Experiments

(a) letter

(b) pima

(c) satimage

Figure 3: Average number of sensors acquired vs. average test error comparison between LP tree systems and
DAG systems.
We compare performance of our trained DAG with that of a complete tree trained using an LP
surrogate [20] on the landsat  pima  and letter datasets. To construct each sensor DAG  we include all
subsets of sensors (including the empty set) and connect any two nodes differing by a single sensor 
with the edge directed from the smaller sensor subset to the larger sensor subset. By including the
empty set  no initial sensor needs to be selected. 3rd-order homogeneous polynomials are used for
both the classiﬁcation and system functions in the LP and DAG.
As seen in Fig. 3  the systems learned with a DAG outperform the LP tree systems. Additionally 
the performance of both of the systems is signiﬁcantly better than previously reported performance
on these data sets for budget cascades [19  23]. This arises due to both the higher complexity of the
classiﬁers and decision functions as well as the ﬂexibility of sensor acquisition order in the DAG
and LP tree compared to cascade structures. For this setting  it appears that the DAG approach is
superior approach to LP trees for learning budgeted systems.

4.2 Large Sensor Set Experiments

(a) MiniBooNE

(b) Forest

(c) CIFAR

Figure 4: Comparison between CSTC  ASTC  and DAG of the average number of acquired features (x-axis)
vs. test error (y-axis).
Next  we compare performance of our trained DAG with that of CSTC [25] and ASTC [11] for
the MiniBooNE  Forest  and CIFAR datasets. We use the validation data to ﬁnd the homogeneous
polynomial that gives the best classiﬁcation performance using all features (MiniBooNE: linear 
Forest: 2nd order  CIFAR: 3rd order). These polynomial functions are then used for all classiﬁcation
and policy functions. For each data set  Alg. 2 was used to ﬁnd 7 subsets  with an 8th subset of all
features added. An exhaustive DAG was trained over all unions of these 8 subsets.
Fig. 4 shows performance comparing the average cost vs. average error of CSTC  ASTC  and our
DAG system. The systems learned with a DAG outperform both CSTC and ASTC on the Mini-
BooNE and Forest data sets  with comparable performance on CIFAR at low budgets and superior
performance at higher budgets.

Acknowledgments
This material is based upon work supported in part by the U.S. National Science Foundation Grant 1330008 
by the Department of Homeland Security  Science and Technology Directorate  Ofﬁce of University Programs 
under Grant Award 2013- ST-061-ED0001  by ONR Grant 50202168 and US AF contract FA8650-14-C-1728.
The views and conclusions contained in this document are those of the authors and should not be interpreted as
necessarily representing the social policies  either expressed or implied  of the U.S. DHS  ONR or AF.

8

11.21.41.61.822.22.42.62.830.20.250.30.350.4Average Features UsedAverage Test Error LP TreeDAG11.21.41.61.822.22.42.62.830.180.190.20.210.220.230.240.250.260.27Average Features UsedAverage Test Error LP TreeDAG11.522.533.540.10.150.20.250.30.350.40.45Average Features UsedAverage Test Error LP TreeDAG051015202530354045500.050.10.150.20.250.3 ASTCCSTCDAG051015202530354045500.210.220.230.240.250.260.270.28 ASTCCSTCDAG0501001502002500.30.320.340.360.380.40.420.44 ASTCCSTCDAGReferences
[1] P. Bartlett  M. Jordan  and J. Mcauliffe. Convexity  Classiﬁcation  and Risk Bounds. Journal of American

Statistical Association  101(473):138–156  2006.

[2] A. Beygelzimer  J. Langford  and P. Ravikumar. Multiclass classiﬁcation with ﬁlter trees. 2007.
[3] R. Busa-Fekete  D. Benbouzid  and B. Kégl. Fast classiﬁcation using sparse decision dags. In Proceedings

of the 29th International Conference on Machine Learning  2012.

[4] M. Chen  Z. Xu  K. Weinberger  O. Chapelle  and D. Kedem. Classiﬁer cascade: Tradeoff between
accuracy and feature evaluation cost. In International Conference on Artiﬁcial Intelligence and Statistics 
2012.

[5] G. Dulac-Arnold  L. Denoyer  P. Preux  and P. Gallinari. Datum-wise classiﬁcation: a sequential approach

to sparsity. In Machine Learning and Knowledge Discovery in Databases  pages 375–390. 2011.

[6] T. Gao and D. Koller. Active classiﬁcation based on value of classiﬁer. In Advances in Neural Information

Processing Systems  volume 24  pages 1062–1070  2011.

[7] H. He  H. Daume III  and J. Eisner. Imitation learning by coaching. In Advances In Neural Information

Processing Systems  pages 3158–3166  2012.

[8] S. Ji and L. Carin. Cost-sensitive feature acquisition and classiﬁcation. Pattern Recognition  40(5)  2007.
[9] P. Kanani and P. Melville. Prediction-time active feature-value acquisition for cost-effective customer

targeting. In Advances In Neural Information Processing Systems  2008.

[10] S. Karayev  M. Fritz  and T. Darrell. Dynamic feature selection for classiﬁcation on a budget. In Interna-

tional Conference on Machine Learning: Workshop on Prediction with Sequential Models  2013.

[11] M. Kusner  W. Chen  Q. Zhou  Z. Xu  K. Weinberger  and Y. Chen. Feature-cost sensitive learning with

submodular trees of classiﬁers. In Twenty-Eighth AAAI Conference on Artiﬁcial Intelligence  2014.

[12] J. Leskovec  A. Krause  C. Guestrin  C. Faloutsos  J. VanBriesen  and N. Glance. Cost-effective outbreak

detection in networks. In International Conference on Knowledge Discovery and Data Mining  2007.

[13] L. Maaten  M. Chen  S. Tyree  and K. Q. Weinberger. Learning with marginalized corrupted features. In

Proceedings of the 30th International Conference on Machine Learning  2013.

[14] F. Nan  J. Wang  and V. Saligrama. Feature-budgeted random forest. In Proceedings of the 32nd Interna-

tional Conference on Machine Learning  2015.

[15] F. Nan  J. Wang  K. Trapeznikov  and V. Saligrama. Fast margin-based cost-sensitive classiﬁcation. In

International Conference on Acoustics  Speech and Signal Processing  2014.

[16] G. Nemhauser  L. Wolsey  and M. Fisher. An analysis of approximations for maximizing submodular set

functionsï£¡i. Mathematical Programming  14(1):265–294  1978.

[17] V. S. Sheng and C. X. Ling. Feature value acquisition in testing: A sequential batch test algorithm. In

Proceedings of the 23rd International Conference on Machine Learning  pages 809–816  2006.

[18] I. Steinwart. Consistency of support vector machines and other regularized kernel classiﬁers. Information

Theory  IEEE Transactions on  51(1):128–142  2005.

[19] K. Trapeznikov and V. Saligrama. Supervised sequential classiﬁcation under budget constraints.

International Conference on Artiﬁcial Intelligence and Statistics  pages 581–589  2013.

In

[20] J. Wang  T. Bolukbasi  K. Trapeznikov  and V. Saligrama. Model selection by linear programming. In

European Conference on Computer Vision  pages 647–662  2014.

[21] J. Wang and V. Saligrama. Local supervised learning through space partitioning. In Advances in Neural

Information Processing Systems  pages 91–99. 2012.

[22] J. Wang and V. Saligrama. Locally-linear learning machines (L3M). In Asian Conference on Machine

Learning  pages 451–466  2013.

[23] J. Wang  K. Trapeznikov  and V. Saligrama. An lp for sequential learning under budgets. In International

Conference on Artiﬁcial Intelligence and Statistics  pages 987–995  2014.

[24] Z. Xu  O. Chapelle  and K. Weinberger. The greedy miser: Learning under test-time budgets. In Proceed-

ings of the 29th International Conference on Machine Learning  2012.

[25] Z. Xu  M. Kusner  M. Chen  and K. Weinberger. Cost-sensitive tree of classiﬁers. In Proceedings of the

30th International Conference on Machine Learning  pages 133–141  2013.

[26] C. Zhang and Z. Zhang. A Survey of Recent Advances in Face Detection. Technical report  Microsoft

Research  2010.

9

,Joseph Wang
Kirill Trapeznikov
Venkatesh Saligrama
Sanghack Lee
Elias Bareinboim