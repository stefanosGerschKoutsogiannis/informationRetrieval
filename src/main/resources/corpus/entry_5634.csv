2016,Tractable Operations for Arithmetic Circuits of Probabilistic Models,We consider tractable representations of probability distributions and the polytime operations they support.  In particular  we consider a recently proposed arithmetic circuit representation  the Probabilistic Sentential Decision Diagram (PSDD).  We show that PSDD supports a polytime multiplication operator  while they do not support a polytime operator for summing-out variables.  A polytime multiplication operator make PSDDs suitable for a broader class of applications compared to arithmetic circuits  which do not in general support multiplication.  As one example  we show that PSDD multiplication leads to a very simple but effective compilation algorithm for probabilistic graphical models: represent each model factor as a PSDD  and then multiply them.,Tractable Operations for

Arithmetic Circuits of Probabilistic Models

Yujia Shen and Arthur Choi and Adnan Darwiche

{yujias aychoi darwiche}@cs.ucla.edu

Computer Science Department

University of California
Los Angeles  CA 90095

Abstract

We consider tractable representations of probability distributions and the polytime
operations they support. In particular  we consider a recently proposed arithmetic
circuit representation  the Probabilistic Sentential Decision Diagram (PSDD). We
show that PSDDs support a polytime multiplication operator  while they do not
support a polytime operator for summing-out variables. A polytime multiplication
operator makes PSDDs suitable for a broader class of applications compared to
classes of arithmetic circuits that do not support multiplication. As one example 
we show that PSDD multiplication leads to a very simple but effective compilation
algorithm for probabilistic graphical models: represent each model factor as a
PSDD  and then multiply them.

1

Introduction

Arithmetic circuits (ACs) have been a central representation for probabilistic graphical models 
such as Bayesian networks and Markov networks. On the reasoning side  some state-of-the-art
approaches for exact inference are based on compiling probabilistic graphical models into arithmetic
circuits [Darwiche  2003]; see also Darwiche [2009  chapter 12]. Such approaches can exploit
parametric structure (such as determinism and context-speciﬁc independence)  allowing inference to
scale sometimes to models with very high treewidth  which are beyond the scope of classical inference
algorithms such as variable elimination and jointree. For example  the ace system for compiling
ACs [Chavira and Darwiche  2008] was the only system in the UAI’08 evaluation of probabilistic
reasoning systems to exactly solve all 250 networks in a challenging (very high-treewidth) suite of
relational models [Darwiche et al.  2008].
On the learning side  arithmetic circuits have become a popular representation for learning from
data  as they are tractable for certain probabilistic queries. For example  there are algorithms for
learning ACs of Bayesian networks [Lowd and Domingos  2008]  ACs of Markov networks [Lowd
and Rooshenas  2013  Bekker et al.  2015] and Sum-Product Networks (SPNs) [Poon and Domingos 
2011]  among other related representations.1
Depending on their properties  different classes of ACs are tractable for different queries and opera-
tions. Among these queries are maximum a posteriori (MAP) inference 2 which is an NP-complete
problem  and evaluating the partition function  which is a PP-complete problem (more intractable).
Among operations  the multiplication of two ACs stands out as particularly important  being a primi-
tive operation in some approaches to incremental or adaptive inference [Delcher et al.  1995  Acar
et al.  2008]  bottom-up compilation of probabilistic graphical models [Choi et al.  2013]  and some
search-based approaches to structure learning [Bekker et al.  2015].

1SPNs can be converted into ACs (and vice-versa) with linear size and time [Rooshenas and Lowd  2014].
2This is also known as most probable explanation (MPE) inference [Pearl  1988].

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In this paper  we investigate the tractability of two fundamental operations on arithmetic circuits:
multiplying two ACs and summing out a variable from an AC. We show that both operations are
intractable for some inﬂuential ACs that have been employed in the probabilistic reasoning and
learning literatures. We then consider a recently proposed sub-class of ACs  called the Probabilistic
Sentential Decision Diagram (PSDD) [Kisa et al.  2014]. We show that PSDDs support a polytime
multiplication operation  which makes them suitable for a broader class of applications. We also show
that PSDDs do not support a polytime summing-out operation (a primitive operation for message-
passing inference algorithms). We empirically illustrate the advantages of PSDDs compared to
other AC representations  for compiling probabilistic graphical models. Previous approaches for
compiling probabilistic models into ACs are based on encoding these models into auxiliary logical
representations  such as a Sentential Decision Diagram (SDD) or a deterministic DNNF circuits 
which are then converted to an AC [Chavira and Darwiche  2008  Choi et al.  2013]. PSDDs are
a direct representation of probability distributions  bypassing the overhead of intermediate logical
representations  and leading to more efﬁcient compilations in some cases. Most importantly though 
this approach lends itself to a signiﬁcantly simpler compilation algorithm: represent each factor of a
given model as a PSDD  and then multiply the factors using PSDD multiplication.
This paper is organized as follows. In Section 2  we review arithmetic circuits (ACs) as a representa-
tion of probability distributions  including PSDDs in particular. In Section 3  we introduce a polytime
multiplication operator for PSDDs  and in Section 4  we show that there is no polytime sum-out
operator for PSDDs. In Section 5  we propose a simple compilation algorithm for PSDDs based
on the multiply operator  which we evaluate empirically. We discuss related work in Section 6 and
ﬁnally conclude in Section 7. Proofs of theorems are available in the Appendix.

2 Representing Distributions Using Arithmetic Circuits

We start with the deﬁnition of factors  which include distributions as a special case.

Deﬁnition 1 (Factor) A factor f (X) over variables X maps each instantiation x of variables X

into a non-negative number f (x). The factor represents a distribution whenPx f (x) = 1.
We deﬁne the value of a factor at a partial instantiation y  where Y ✓ X  as f (y) =Pz f (yz) 
where Z = X \ Y. When the factor is a distribution  f (y) corresponds to the probability of evidence
y. We also deﬁne the MAP instantiation of a factor as argmaxx f (x)  which corresponds to the most
likely instantiation when the factor is a distribution.
The classical  tabular representation of a factor f (X) is exponential in the number of variables X.
However  one can represent such factors more compactly using arithmetic circuits.
Deﬁnition 2 (Arithmetic Circuit) An arithmetic circuit AC(X) over variables X is a rooted DAG
whose internal nodes are labeled with + or ⇤ and whose leaf nodes are labeled with either indicator
variables x or non-negative parameters ✓. The value of the circuit at instantiation x  denoted
AC(x)  is obtained by assigning indicator x the value 1 if x is compatible with instantiation x and
0 otherwise  then evaluating the circuit in the standard way. The circuit AC(X) represents factor
f (X) iff AC(x) = f (x) for each instantiation x.
A tractable arithmetic circuit allows one to efﬁciently answer certain queries about the factor it
represents. We next discuss two properties that lead to tractable arithmetic circuits. The ﬁrst is
decomposability [Darwiche  2001b]  which was used for probabilistic reasoning in [Darwiche  2003].

Deﬁnition 3 (Decomposability) Let n be a node in an arithmetic circuit AC(X). The variables of
n  denoted vars(n)  are the variables X 2 X with some indicator x appearing at or under node
n. An arithmetic circuit is decomposable iff every pair of children c1 and c2 of a ⇤-node satisﬁes
vars(c1) \ vars(c2) = ;.
The second property is determinism [Darwiche  2001a]  which was also employed for probabilistic
reasoning in Darwiche [2003].
Deﬁnition 4 (Determinism) An arithmetic circuit AC(X) is deterministic iff each +-node has at
most one non-zero input when the circuit is evaluated under any instantiation x of the variables X.

2

A third property called smoothness is also desirable as it simpliﬁes the statement of certain AC
algorithms  but is less important for tractability as it can be enforced in polytime [Darwiche  2001a].
Deﬁnition 5 (Smoothness) An arithmetic circuit AC(X) is smooth iff it contains at least one indi-
cator for each variable in X  and for each child c of +-node n  we have vars(n) = vars(c).

Decomposability and determinism lead to tractability in the following sense. Let Pr (X) be a
distribution represented by a decomposable  deterministic and smooth arithmetic circuit AC(X).
Then one can compute the following queries in time that is linear in the size of circuit AC(X): the
probability of any partial instantiation  Pr (y)  where Y ✓ X [Darwiche  2003] and the most likely
instantiation  argmaxx Pr (x) [Chan and Darwiche  2006]. The decision problems of these queries
are known to be PP-complete and NP-complete for Bayesian networks [Roth  1996  Shimony  1994].
A number of methods have been proposed for
compiling a Bayesian network into a decompos-
able  deterministic and smooth AC that repre-
sents its distribution [Darwiche  2003]. Figure 1
depicts such a circuit that represents the distribu-
tion of Bayesian network A ! B. One method
ensures that the size of the AC is proportional to
the size of a jointree for the network. Another
method yields circuits that can sometimes be ex-
ponentially smaller  and is implemented in the
publicly available ace system [Chavira and Dar-
wiche  2008]; see also Darwiche et al. [2008].

Figure 1: An AC for a Bayesian network A ! B.
Additional methods are discussed in Darwiche [2009  chapter 12].
This work is motivated by the following limitation of these tractable circuits  which may narrow their
applicability in probabilistic reasoning and learning.
Deﬁnition 6 (Multiplication) The product of two arithmetic circuits AC1(X) and AC2(X) is an
arithmetic circuit AC(X) such that AC(x) = AC1(x)AC2(x) for every instantiation x.
Theorem 1 Computing the product of two decomposable ACs is NP-hard if the product is also
decomposable. Computing the product of two decomposable and deterministic ACs is NP-hard if the
product is also decomposable and deterministic.

We now investigate a newly introduced class of tractable ACs  called the Probabilistic Sentential
Decision Diagram (PSDD) [Kisa et al.  2014]. In particular  we show that this class of circuits admits
a tractable product operation and then explore an application of this operation to exact inference in
probabilistic graphical models.
PSDDs were motivated by the need to represent probability distributions Pr (X) with many instantia-
tions x attaining zero probability  Pr (x) = 0. Consider the distribution Pr (X) in Figure 2(a) for an
example. The ﬁrst step in constructing a PSDD for this distribution is to construct a special Boolean
circuit that captures its zero entries; see Figure 2(b). The Boolean circuit captures zero entries in the
following sense. For each instantiation x  the circuit evaluates to 0 at instantiation x iff Pr (x) = 0.
The second and ﬁnal step of constructing a PSDD amounts to parameterizing this Boolean circuit
(e.g.  by learning them from data)  by including a local distribution on the inputs of each or-gate; see
Figure 2(c).
The Boolean circuit underlying a PSDD is known as a Sentential Decision Diagram (SDD) [Darwiche 
2011]. These circuits satisfy speciﬁc syntactic and semantic properties based on a binary tree  called
a vtree  whose leaves correspond to variables; see Figure 2(d). The following deﬁnition of SDD
circuits is a based on the one given by Darwiche [2011] and uses a different notation.

Deﬁnition 7 (SDD) An SDD normalized for a vtree v is a Boolean circuit deﬁned as follows. If v is
a leaf node labeled with variable X  then the SDD is either X  ¬X  ? or an or-gate with inputs X
and ¬X. If v is an internal vtree node  then the SDD has the structure in Figure 3  where p1  . . .   pn
are SDDs normalized for the left child vl and s1  . . .   sn are SDDs normalized for the right child vr.
Moreover  the circuits p1  . . .   pn are consistent  mutually exclusive and exhaustive.

3

****+++****(cid:79)a(cid:79)a b(cid:79)b(cid:79)a(cid:84)a(cid:84)ab|(cid:84)ab|(cid:84)ab|(cid:84)ab|(cid:84)A B C P r
0.2
0
0.2
0
0
0.0
0.1
0
0.0
1
0.3
1
0.1
1
0.1
1
(a) Distribution

0
1
0
1
0
1
0
1

0
0
1
1
0
0
1
1

3

3

.6

.4

1

4

1

C

C ¬C

1

.33

.67

4

.5
.5
C ¬C

1

C

.75

.25

A B ¬A¬B

A ¬B¬A B

(b) SDD

A B ¬A¬B

A ¬B¬A B

(c) PSDD

3 

1 

4 
C 

0 
A 

2 
B 

(d) Vtree

Figure 2: A probability distribution and its SDD/PSDD representation. Note that the numbers
annotating or-gates in (b) & (c) correspond to vtree node IDs in (d). Further  note that while the
circuit appears to be a tree  the input variables are shared and hence the circuit is not a tree.

↵n

· · ·

↵1
↵2

· · ·

p1 s1 p2 s2

pn sn

Figure 3: Each (pi  si ↵ i)
is called an element of the
or-gate  where the pi’s are
called primes and the si’s
are called subs. Moreover 

Pi ↵i = 1 and exactly one

pi evaluates to 1 under any
circuit input.

SDD circuits alternate between or-gates and and-gates. Their and-
gates have two inputs each. The or-gates of these circuits are such
that at most one input will be high under any circuit input. An SDD
circuit may produce a 1-output for every possible input (i.e.  the circuit
represents the function true). These circuits arise when representing
strictly positive distributions (with no zero entries).
A PSDD is obtained by including a distribution ↵1  . . .  ↵ n on the
inputs of each or-gate; see Figure 3. The semantics of PSDDs are
given in [Kisa et al.  2014].3 We next provide an alternative semantics 
which is based on converting a PSDD into an arithmetic circuit.

Deﬁnition 8 (ACs of PSDDs) The arithmetic circuit of a PSDD is
obtained as follows. Leaf nodes x and ? are converted into x and 0 
respectively. Each and-gate is converted into a ⇤-node. Each or-node
with children c1  . . .   cn and corresponding parameters ↵1  . . .  ↵ n is
converted into a +-node with children ↵1 ⇤ c1  . . .   ↵n ⇤ cn.
Theorem 2 The arithmetic circuit of a PSDD represents the distribu-
tion induced by the PSDD. Moreover  the arithmetic circuit is decom-
posable and deterministic.4

The PSDD is a complete and canonical representation of probability
distributions. That is  PSDDs can represent any distribution  and there is a unique PSDD for that distri-
bution (under some conditions). A variety of probabilistic queries are tractable on PSDDs  including
that of computing the probability of a partial variable instantiation and the most likely instantiation.
Moreover  the maximum likelihood parameter estimates of a PSDD are unique given complete data 
and these parameters can be computed efﬁciently using closed-form estimates; see [Kisa et al. 
2014] for details. Finally  PSDDs have been used to learn distributions over combinatorial objects 
including rankings and permutations [Choi et al.  2015]  paths and games [Choi et al.  2016]. In these
applications  the Boolean circuit underlying a PSDD captures variable instantiations that correspond
to combinatorial objects  while its parameterization induces a distribution over these objects.
As a concrete example  PSDDs were used to induce distributions over the permutations of n items as
follows. We have a variable Xij for each i  j 2{ 1  . . .   n} denoting that item i is at position j in the
permutation. Clearly  not all instantiations of these variables correspond to (valid) permutations. An
SDD circuit is then constructed  which outputs 1 iff the corresponding input corresponds to a valid
permutation. Each parameterization of this SDD circuit leads to a distribution on permutations and
these parameterizations can be learned from data; see Choi et al. [2015].

3Let x be an instantiation of PSDD variables. If the SDD circuit outputs 0 at input x  then Pr (x) = 0.
Otherwise  traverse the circuit top-down  visiting the (unique) high input of each visited or-node  and all inputs
of each visited and-node. Then Pr (x) is the product of parameters visited during the traversal process.

4The arithmetic circuit also satisﬁes a minor weakening of smoothness with the same effect as smoothness.

4

3 Multiplying Two PSDDs

Factors and their operations are fundamental to probabilistic inference  whether exact or approximate
[Darwiche  2009  Koller and Friedman  2009]. Consider two of the most basic operations on factors:
(1) computing the product of two factors and (2) summing out a variable from a factor. With these
operations  one can directly implement various inference algorithms  including variable elimination 
the jointree algorithm  and message-passing algorithms such as loopy belief propagation. Typically 
tabular representations (and their sparse variations) are used to represent factors and implement
the above algorithms; see Larkin and Dechter [2003]  Sanner and McAllester [2005]  Chavira and
Darwiche [2007] for some alternatives.
More generally  factor multiplication is useful for online or incremental reasoning with probabilistic
models. In some applications  we may not have access to all factors of a model beforehand  to
compile as a jointree or an arithmetic circuit. For example  when learning the structure of a Markov
network from data [Bekker et al.  2015]  we may want to introduce and remove candidate factors from
a model  while evaluating the changes to the log likelihood. Certain realizations of generalized belief
propagation also require the multiplication of factors [Yedidia et al.  2005  Choi and Darwiche  2011].
In these realizations  one can use factor multiplication to enforce dependencies between factors that
have been relaxed to make inference more tractable  albeit less accurate.
We next discuss PSDD multiplication  while deferring summing out to the following section.

   {}   0
for all elements (p  s  ↵) of n1 do

Algorithm 1 Multiply(n1  n2  v)
input: PSDDs n1  n2 normalized for vtree v
output: PSDD n and constant 
main:
1: n  k cachem(n1  n2)  cachec(n1  n2)
2: if n 6= null then return (n  k)
3: else if v is a leaf then (n  ) BaseCase(n1  n2)
4: else
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: cachem(n1  n2) n
17: cachec(n1  n2) 
18: return (n  )

for all elements (q  r  ) of n2 do
(m1  k1) Multiply(p  q  vl)
if k1 6= 0 then

(m2  k2) Multiply(s  r  vr)
⌘ k1 · k2 · ↵ · 
  + ⌘
add (m1  m2 ⌘ ) to 

 { (m1  m2 ⌘/ ) | (m1  m2 ⌘ ) 2 }
n unique PSDD node with elements 

. check if previously computed
. return previously cached result
.n 1  n2 are literals  ? or simple or-gates
.n 1 and n2 have the structure in Figure 3
. initialization
. see Figure 3
. see Figure 3
. recursively multiply primes p and q
. if (m1  k1) is not a trivial factor
. recursively multiply subs s and r
. compute weight of element (m1  m2)
. aggregate weights of elements

. normalize parameters of 
. cache lookup for unique nodes

. store results in cache

Our ﬁrst observation is that the product of two distributions is generally not a distribution  but a factor.
Moreover  a factor f (X) can always be represented by a distribution Pr (X) and a constant  such
that f (x) =  · Pr (x). Hence  our proposed multiplication method will output a PSDD together
with a constant  as given in Algorithm 1. This algorithm uses three caches  one for storing constants
(cachec)  another for storing circuits (cachem)  and a third used to implement Line 15.5 This line
ensures that the PSDD has no duplicate structures of the form given in Figure 3. The description
of function BaseCase() on Line 3 is available in the Appendix. It appears inside the proof of the
following theorem  which establishes the soundness and complexity of the given algorithm.

Theorem 3 Algorithm 1 outputs a PSDD n normalized for vtree v. Moreover  if Pr 1(X) and Pr 2(X)
are the distributions of input PSDDs n1 and n2  and Pr (X) is the distribution of output PSDD n 
then Pr 1(x)Pr 2(x) =  · Pr (x) for every instantiation x. Finally  Algorithm 1 takes time O(s1s2) 
where s1 and s2 are the sizes of input PSDDs.

5The cache key of a PSDD node in Figure 3 is based on the (unique) ID’s of nodes pi/si and parameters ↵i.

5

We will later discuss an application of PSDD multiplication to probabilistic inference  in which we
cascade these multiplication operations. In particular  we end up multiplying two factors f1 and f2 
represented by PSDDs n1 and n2 and the corresponding constants 1 and 2. We use Algorithm 1
for this purpose  multiplying PSDDs n1 and n2 (distributions)  to yield a PSDD n (distribution) and
a constant . The factor f1f2 will then correspond to PSDD n and constant  · 1 · 2.

A

A

A

G

G K H D

B

G F K

B J H

H D

E C

I D

Figure 4: A vtree and two of its projections.

Another observation is that Algorithm 1 assumes
that the input PSDDs are over the same vtree and 
hence  same set of variables. A more detailed ver-
sion of this algorithm can multiply two PSDDs
over different sets of variables as long as the PS-
DDs have compatible vtrees. We omit this version
here to simplify the presentation  but mention that
it has the same complexity as Algorithm 1.
Two vtrees over variables X and Y are compatible
iff they can be obtained by projecting some vtree
on variables X and Y  respectively.

Deﬁnition 9 (Vtree Projection) Let v be a vtree
over variables Z. The projection of v on variables
X ✓ Z is obtained as follows. Successively remove every maximal subtree v0 whose variables are
outside X  while replacing the parent of v0 with its sibling.

Figure 4 depicts a vtree and two of its projections. When compiling a probabilistic graphical model
into a PSDD  we ﬁrst construct a vtree v over all variables in the model. We then compile each factor
f (X) into a PSDD  using the projection of v on variables X. We ﬁnally multiply the PSDDs of these
factors. We will revisit these steps later.

4 Summing-Out a Variable in a PSDD

We now discuss the summing out of variables from distributions represented by arithmetic circuits.

Deﬁnition 10 (Sum Out) Summing-out a variable X 2 X from factor f (X) results in another fac-

tor over variables Y = X\{X}  denoted byPX f and deﬁned as:⇣PX f⌘(y) def= Px f (x  y).

When the factor is a distribution (i.e.  normalized)  the sum out operation corresponds to marginaliza-
tion. Together with multiplication  summing out provides a direct implementation of algorithms such
as variable elimination and those based on message passing.
Just like multiplication  summing out is also intractable for a common class of arithmetic circuits.

Theorem 4 The sum-out operation on decomposable and deterministic ACs is NP-hard  assuming
the output is also decomposable and deterministic.

This theorem does not preclude the possibility that the resulting AC is of polynomial size with respect
to the size of the input AC—it just says that the computation is intractable. Summing out is also
intractable on PSDDs  but the result is stronger here as the size of the output can be exponential.

Theorem 5 There exists a class of factors f (X) and variable X 2 X  such that n = |X| can be
arbitrarily large  f (X) has a PSDD whose size is linear in n  while the PSDD ofPX f has size
exponential in n for every vtree.

Only the multiplication operation is needed to compile probabilistic graphical models into arithmetic
circuits. Even for inference algorithms that require summing out variables  such as variable elimina-
tion  summing out can still be useful  even if intractable  since the size of resulting arithmetic circuit
will not be larger than a tabular representation.

6

5 Compiling Probabilistic Graphical Models into PSDDs

Even though PSDDs form a strict subclass of decomposable and deterministic ACs (and satisfy
stronger properties)  one can still provide the following classical guarantee on PSDD size.

Theorem 6 The interaction graph of factors f1(X1)  . . .   fn(Xn) has nodes corresponding to vari-
ables X1 [ . . . [ Xn and an edge between two variables iff they appear in the same factor. There is
a PSDD for the product f1 . . . fn whose size is O(m · exp(w))  where m is the number of variables
and w is its treewidth.

This theorem provides an upper bound on the size of PSDD compilations for both Bayesian and
Markov networks. An analogous guarantee is available for SDD circuits of propositional models 
using a special type of vtree known as a decision vtree [Oztok and Darwiche  2014]. We next discuss
our experiments  which focused on the compilation of Markov networks using decision vtrees.
To compile a Markov network  we ﬁrst construct a decision vtree using a known technique.6 For each
factor of the network  we project the vtree on the factor variables  and then compile the factor into
a PSDD. This can be done in time linear in the factor size  but we omit the details here. We ﬁnally
multiply the obtained PSDDs. The order of multiplication is important to the overall efﬁciency of the
compilation approach. The order we used is as follows. We assign each PSDD to the lowest vtree
node containing the PSDD variables  and then multiply PSDDs in the order that we encounter them
as we traverse the vtree bottom-up (this is analogous to compiling CNFs in Choi et al. [2013]).
Table 1 summarizes our results. We compiled Markov networks into three types of arithmetic circuits.
The ﬁrst compilation (AC1) is to decomposable and deterministic ACs using ace [Chavira and
Darwiche  2008].7 The second compilation (AC2) is also to decomposable and deterministic ACs  but
using the approach proposed in Choi et al. [2013]. The third compilation is to PSDDs as discussed
above. The ﬁrst two approaches are based on reducing the inference problem into a weighted model
counting problem. In particular  these approaches encode the network using Boolean expressions 
which are compiled to logical representations (d-DNNF or SDD)  from which an arithmetic circuit is
induced. The systems underlying these approaches are quite complex and are the result of many years
of engineering. In contrast  the proposed compilation to PSDDs does not rely on an intermediate
representation or additional boxes  such as d-DNNF or SDD compilers.
The benchmarks in Table 1 are from the UAI-14 Inference Competition.8 We selected all networks
over binary variables in the MAR track  and report a network only if at least one approach successfully
compiled it (given time and space limits of 30 minutes and 16GB). We report the size (the number
of edges) and time spent for each compilation. First  we note that for all benchmarks that compiled
to both PSDD and AC2 (based on SDDs)  the PSDD size is always smaller. This can be attributed
in part to the fact that reductions to weighted model counting represent parameters explicitly as
variables  which are retained throughout the compilation process. In contrast  PSDD parameters are
annotated on its edges. More interestingly  when we multiply two PSDD factors  the parameters of
the inputs may not persist in the output PSDD. That is  the PSDD only maintains enough parameters
to represent the resulting distribution  which further explains the size differences.
In the Promedus benchmarks  we also see that in all but 5 cases  the compiled PSDD is smaller than
AC1. However  several Grids benchmarks were compilable to AC1  but failed to compile to AC2
or PSDD  given the time and space limits. On the other hand  we were able to compile some of the
relational benchmarks to PSDD  which did not compile to AC1 and compiled partially to AC2.

6 Related Work

Tabular representations and their sparse variations (e.g.  Larkin and Dechter [2003]) are typically
used to represent factors for probabilistic inference and learning. Rules and decision trees are more
succinct representations for modeling context-speciﬁc independence  although they are not much more
amenable to exact inference compared to tabular representations [Boutilier et al.  1996  Friedman and
Goldszmidt  1998]. Domain speciﬁc representations have been proposed  e.g.  in computer vision

6We used the minic2d package which is available at reasoning.cs.ucla.edu/minic2d/.
7The ace system is publicly available at http://reasoning.cs.ucla.edu/ace/.
8http://www.hlt.utdallas.edu/~vgogate/uai14-competition/index.html

7

network

AC2

201 250

compilation size

AC2

psdd

Alchemy_11
Grids_11
Grids_12
Grids_13
Grids_14

AC1
12 705 213
81 074 816
232 496
81 090 432
83 186 560

compilation time
AC1
- 13 715 906 130.83
- 271.97
-
0.93
457 529
- 273.88
-
-
- 279.12

psdd
- 300.80
-
-
1.68
1.12
-
-
-
-
72.39 204.54 223.60
Segmentation_11 20 895 884 41 603 129 30 951 708
Segmentation_12 15 840 404 41 005 721 34 368 060
51.27 209.03 283.79
Segmentation_13 33 746 511 78 028 443 33 726 812 117.46 388.97 255.29
62.31 279.19 639.07
Segmentation_14 16 965 928 48 333 027 46 363 820
- 273.67
Segmentation_15 29 888 972
65.64 265.07 163.38
Segmentation_16 18 799 112 54 557 867 19 935 308
41 070
10.43
- 594.68
217 696
2.28
30 542
2.46
48 814
3.94
26 100
24.90
749 528
1.52
9 520
29 150
2.06
50.22
1 549 170

relational_3
relational_5
Promedus_11
Promedus_12
Promedus_13
Promedus_14
Promedus_15
Promedus_16
Promedus_17

-
-
67 036
45 119
42 065
2 354 180
14 363
45 935
3 336 316

183 064
-
174 592
349 916
83 701
3 667 740
31 176
154 467
9 849 598

-
-
6.80
0.91
0.80
21.64
0.95
1.35
68.08

- 33 866 332 107.87

1.21

1.88
5.81
0.23
33.27
0.10
0.40
48.47

Table 1: AC compilation size (number of edges) and time (in seconds)

compilation size

compilation time

AC2

9 085

4 774

network

188 322
31 911
39 016

AC1
Promedus_18 3 006 654
Promedus_19
Promedus_20
Promedus_21
Promedus_22
Promedus_23
Promedus_24
Promedus_25
Promedus_26
Promedus_27
Promedus_28
Promedus_29
Promedus_30
Promedus_31
Promedus_32
Promedus_33
Promedus_34
Promedus_35
Promedus_36
Promedus_37
Promedus_38

AC2 psdd
psdd AC1
18.38 21.20
762 247 539 478 20.46
25.01 68.62
6.80
3.46
3.24
0.96
1.78
0.18
0.62
0.10
1.58
0.63
17.77 10.88
3.29
0.80
0.05
0.45
6.78
7.66
32.90
2.72
0.71 198.74
1.16
0.55
0.73
1.59
0.30
1.04
0.54
0.08
1.88
1.23
0.07
0.50
1.96
0.12
0.57
1.77
0.11
0.59
1.57
0.07
0.59
1.78
0.78
0.87
1.79
0.13
0.68
1.22
0.12
1.91
6.15
3.50
1.49
1.67
17.19
8.09

796 928 1 171 288 977 510
70 492
70 422
10 944
17 528
26 010
33 064
329 669 1 473 628 317 514
1 960
556 179 3 614 581 407 974
5 146
57 190
24 578
19 434
33 611
52 698
17 084
24 049
46 364
10 403
4 828
20 600
6 734
9 884
21 230
10 842
17 977
31 754
8 682
15 215
33 064
4 006
10 734
18 535
21 398
38 113
54 214
11 120
18 765
31 792
19 175
11 004
31 792
144 664
77 088
79 210
177 560
593 675 123 552

[Felzenszwalb and Huttenlocher  2006]  to allow for more efﬁcient factor operations. Algebraic
Decision Diagrams (ADDs) and Algebraic Sentential Decision Diagrams (ASDDs) can also be used
to multiply two factors in polytime [Bahar et al.  1993  Herrmann and de Barros  2013]  but their sizes
can grow quickly with repeated multiplications: ADDs have a distinct node for each possible value
of a factor/distribution. Since ADDs also support a polytime summing-out operation  ADDs are more
commonly used in the context of variable elimination [Sanner and McAllester  2005  Chavira and
Darwiche  2007]  and in message passing algorithms [Gogate and Domingos  2013]. Probabilistic
Decision Graphs (PDGs) and AND/OR Multi-Valued Decision Diagrams (AOMDD) support a
polytime multiply operator  and also have treewidth upper bounds when compiling probabilistic
graphical models [Jaeger  2004  Mateescu et al.  2008]. Both PDGs and AOMDDs can be viewed as
sub-classes of PSDDs that branch on variables instead of sentences as is the case with PSDDs—this
distinction can lead to exponential reductions in size [Xue et al.  2012  Bova  2016].

7 Conclusion

We considered the tractability of multiplication and summing-out operators for arithmetic circuits
(ACs)  as tractable representations of factors and distributions. We showed that both operations are
intractable for deterministic and decomposable ACs (under standard complexity theoretic assump-
tions). We also showed that for a sub-class of ACs  known as PSDDs  a polytime multiplication
operator is supported. Moreover  we showed that PSDDs do not support summing-out in polytime
(unconditionally). Finally  we illustrated the utility of PSDD multiplication  providing a relatively
simple but effective algorithm for compiling probabilistic graphical models into PSDDs.

Acknowledgments
This work was partially supported by NSF grant #IIS-1514253 and ONR grant #N00014-15-1-2339.

References
U. A. Acar  A. T. Ihler  R. R. Mettu  and Ö. Sümer. Adaptive inference on general graphical models. In UAI 

pages 1–8  2008.

R. I. Bahar  E. A. Frohm  C. M. Gaona  G. D. Hachtel  E. Macii  A. Pardo  and F. Somenzi. Algebraic decision

diagrams and their applications. In ICCAD  pages 188–191  1993.

J. Bekker  J. Davis  A. Choi  A. Darwiche  and G. Van den Broeck. Tractable learning for complex probability

queries. In NIPS  2015.

C. Boutilier  N. Friedman  M. Goldszmidt  and D. Koller. Context-speciﬁc independence in Bayesian networks.

In UAI  pages 115–123  1996.

S. Bova. SDDs are exponentially more succinct than OBDDs. In AAAI  pages 929–935  2016.
H. Chan and A. Darwiche. On the robustness of most probable explanations. In UAI  2006.
M. Chavira and A. Darwiche. Compiling Bayesian networks using variable elimination. In IJCAI  2007.

8

M. Chavira and A. Darwiche. On probabilistic inference by weighted model counting. AIJ  172(6–7):772–799 

April 2008.

A. Choi and A. Darwiche. Relax  compensate and then recover. In T. Onada  D. Bekki  and E. McCready 

editors  NFAI  volume 6797 of LNCF  pages 167–180. Springer  2011.

A. Choi  D. Kisa  and A. Darwiche. Compiling probabilistic graphical models using sentential decision diagrams.

In ECSQARU  pages 121–132  2013.

A. Choi  G. Van den Broeck  and A. Darwiche. Tractable learning for structured probability spaces: A case

study in learning preference distributions. In IJCAI  2015.

A. Choi  N. Tavabi  and A. Darwiche. Structured features in naive Bayes classiﬁcation. In AAAI  2016.
A. Darwiche. On the tractable counting of theory models and its application to truth maintenance and belief

revision. Journal of Applied Non-Classical Logics  11(1-2):11–34  2001a.

A. Darwiche. Decomposable negation normal form. J. ACM  48(4):608–647  2001b.
A. Darwiche. A differential approach to inference in Bayesian networks. J. ACM  50(3):280–305  2003.
A. Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge University Press  2009.
A. Darwiche. SDD: A new canonical representation of propositional knowledge bases. In IJCAI  pages 819–826 

2011.

A. Darwiche and P. Marquis. A knowledge compilation map. JAIR  17:229–264  2002.
A. Darwiche  R. Dechter  A. Choi  V. Gogate  and L. Otten. Results from the probabilistic inference evaluation

of UAI-08. 2008.

A. L. Delcher  A. J. Grove  S. Kasif  and J. Pearl. Logarithmic-time updates and queries in probabilistic networks.

In UAI  pages 116–124  1995.

P. F. Felzenszwalb and D. P. Huttenlocher. Efﬁcient belief propagation for early vision. IJCV  70(1):41–54 

2006.

N. Friedman and M. Goldszmidt. Learning bayesian networks with local structure. In Learning in graphical

models  pages 421–459. Springer  1998.

V. Gogate and P. M. Domingos. Structured message passing. In UAI  2013.
R. G. Herrmann and L. N. de Barros. Algebraic sentential decision diagrams in symbolic probabilistic planning.

In Proceedings of the Brazilian Conference on Intelligent Systems (BRACIS)  pages 175–181  2013.

M. Jaeger. Probabilistic decision graphs — combining veriﬁcation and AI techniques for probabilistic inference.

International Journal of Uncertainty  Fuzziness and Knowledge-Based Systems  12:19–42  2004.

D. Kisa  G. Van den Broeck  A. Choi  and A. Darwiche. Probabilistic sentential decision diagrams. In KR  2014.
D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press  2009.
D. Larkin and R. Dechter. Bayesian inference in the presence of determinism. In AISTATS  2003.
D. Lowd and P. M. Domingos. Learning arithmetic circuits. In UAI  pages 383–392  2008.
D. Lowd and A. Rooshenas. Learning Markov networks with arithmetic circuits. In AISTATS  pages 406–414 

2013.

R. Mateescu  R. Dechter  and R. Marinescu. AND/OR multi-valued decision diagrams (AOMDDs) for graphical

models. J. Artif. Intell. Res. (JAIR)  33:465–519  2008.

U. Oztok and A. Darwiche. On compiling CNF into decision-DNNF. In CP  pages 42–57  2014.
J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. MK  1988.
H. Poon and P. M. Domingos. Sum-product networks: A new deep architecture. In UAI  pages 337–346  2011.
A. Rooshenas and D. Lowd. Learning sum-product networks with direct and indirect variable interactions. In

ICML  pages 710–718  2014.

D. Roth. On the hardness of approximate reasoning. Artif. Intell.  82(1-2):273–302  1996.
S. Sanner and D. A. McAllester. Afﬁne algebraic decision diagrams (AADDs) and their application to structured

probabilistic inference. In IJCAI  pages 1384–1390  2005.

S. E. Shimony. Finding MAPs for belief networks is NP-hard. Artif. Intell.  68(2):399–410  1994.
D. Sieling and I. Wegener. NC-algorithms for operations on binary decision diagrams. Parallel Processing

Letters  3:3–12  1993.

Y. Xue  A. Choi  and A. Darwiche. Basing decisions on sentences in decision diagrams. In AAAI  pages 842–849 

2012.

J. Yedidia  W. Freeman  and Y. Weiss. Constructing free-energy approximations and generalized belief propaga-

tion algorithms. IEEE Transactions on Information Theory  51(7):2282–2312  2005.

9

,Yujia Shen
Arthur Choi
Adnan Darwiche