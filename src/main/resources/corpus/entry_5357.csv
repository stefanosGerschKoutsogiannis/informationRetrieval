2019,Superset Technique for Approximate Recovery in One-Bit Compressed Sensing,One-bit compressed sensing (1bCS) is a method of signal acquisition under extreme measurement quantization that gives important insights on the limits of signal compression and analog-to-digital conversion. The setting is also equivalent to the problem of learning a sparse hyperplane-classifier. In this paper  we propose a generic  approach for signal recovery in nonadaptive 1bCS that leads to improved sample complexity for approximate recovery for a variety of signal models  including nonnegative signals and binary signals. We construct 1bCS matrices that are universal - i.e. work for all signals under a model - and at the same time recover very general random sparse signals with high probability. In our approach  we divide the set of samples (measurements) into two parts  and use the first part to recover the superset of the support of a sparse vector. The second set of measurements is then used to approximate the signal within the superset. While support recovery in 1bCS is well-studied  recovery of superset of the support requires fewer samples  which then leads to an overall reduction in sample complexity for approximate recovery.,Superset Technique for Approximate Recovery in

One-Bit Compressed Sensing

University of Massachusetts Amherst

University of Massachusetts Amherst

Larkin Flodin

Amherst  MA 01003

lflodin@cs.umass.edu

Venkata Gandikota

Amherst  MA 01003

gandikota.venkata@gmail.com

Arya Mazumdar

University of Massachusetts Amherst

Amherst  MA 01003
arya@cs.umass.edu

Abstract

One-bit compressed sensing (1bCS) is a method of signal acquisition under ex-
treme measurement quantization that gives important insights on the limits of
signal compression and analog-to-digital conversion. The setting is also equiva-
lent to the problem of learning a sparse hyperplane-classiﬁer. In this paper  we
propose a generic approach for signal recovery in nonadaptive 1bCS that leads
to improved sample complexity for approximate recovery for a variety of signal
models  including nonnegative signals and binary signals. We construct 1bCS
matrices that are universal - i.e. work for all signals under a model - and at the
same time recover very general random sparse signals with high probability. In
our approach  we divide the set of samples (measurements) into two parts  and
use the ﬁrst part to recover the superset of the support of a sparse vector. The
second set of measurements is then used to approximate the signal within the
superset. While support recovery in 1bCS is well-studied  recovery of superset
of the support requires fewer samples  which then leads to an overall reduction in
sample complexity for approximate recovery.

Introduction

1
Sparsity is a natural property of many real-world signals. For example  image and speech signals
are sparse in the Fourier basis  which led to the theory of compressed sensing  and more broadly 
sampling theory [12  7]. In some important multivariate optimization problems with many optimal
points  sparsity of the solution is also a measure of ‘simplicity’ and insisting on sparsity is a common
method of regularization [19]. While recovering sparse vectors from linear measurements is a
well-studied topic  technological advances and increasing data size raises new questions. These
include quantized and nonlinear signal acquisition models  such as 1-bit compressed sensing [4]. In
1-bit compressed sensing  linear measurements of a sparse vector are quantized to only 1 bit  e.g.
indicating whether the measurement outcome is positive or not  and the task is to recover the vector
up to a prescribed Euclidean error with minimum number of measurements. Like compressed sensing 
the overwhelming majority of the literature  including this paper  focuses on the nonadaptive setting
for the problem.
One of the ways to approximately recover a sparse vector from 1-bit measurements is to use a subset
of all the measurements to identify the support of the vector. Next  the remainder of the measurements
can be used to approximate the vector within the support. Note that this second set of measurements
is also predeﬁned  and therefore the entire scheme is still nonadaptive. Such a method appears in the

Preprint. Under review.

context of ‘universal’ matrix designs in [9  1]. The resulting schemes are the best known  in some
sense  but still result in a large gap between the upper and lower bounds for approximate recovery of
vectors.
In this paper we take steps to close these gaps  by presenting a simple yet powerful idea. Instead
of using a subset of the measurements to recover the support of the vector exactly  we propose
using a (smaller) set of measurements to recover a superset of the support. The remainder of the
measurements can then be used to better approximate the vector within the superset. It turns out this
idea which we call the “superset technique” leads to optimal number of measurements for universal
schemes for several important classes of sparse vectors (for example  nonnegative vectors). We also
present theoretical results providing a characterization of matrices that would yield universal schemes
for all sparse vectors.

Prior Results. While the compressed sensing framework was introduced in [7]  it was not until [4]
that 1-bit quantization of the measurements was considered as well  to try and combat the fact that
taking real-valued measurements to arbitrary precision may not be practical in applications. Initially 
the focus was primarily on approximately reconstructing the direction of the signal x (the quantization
does not preserve any information about the magnitude of the signal  so all we can hope to reconstruct
is the direction). However  in [10] the problem of support recovery  as opposed to approximate vector
reconstruction  was ﬁrst considered and it was shown that O (k log n) measurements is sufﬁcient to
recover the support of a k-sparse signal in Rn with high probability. This was subsequently shown to
be tight with the lower bound proven in [3].
All the above results assume that a new measurement matrix is constructed for each sparse signal  and
success is deﬁned as either approximately recovering the signal up to error ✏ in the `2 norm (for the
approximate vector recovery problem)  or exactly recovering the support of the signal (for the support
recovery problem)  with high probability. Generating a new matrix for each instance is not practical
in all applications  which has led to interest in the “universal” versions of the above two problems 
where a single matrix must work for support recovery or approximate recovery of all k-sparse real
signals  with high probability.

the entries of the signal to be nonnegative (which is the case for many real-world signals such as

for universal approximate recovery. The dependence on ✏ was then improved signiﬁcantly to

Plan and Vershynin showed in [15] that both O k
k measurements sufﬁce
Ok3 log n
✏ in [9]  who also considered the problem of universal support recovery  and showed
that for that problem  Ok3 log n measurements is sufﬁcient. They showed as well that if we restrict
images)  then Ok2 log n is sufﬁcient for universal support recovery. The constructions of their
(RUFFs) can be used to improve the upper bound on universal support recovery to Ok2 log n
Ok2 log n + k

measurement matrices are based primarily on combinatorial objects  speciﬁcally expanders and Union
Free Families (UFFs).
Most recently  [1] showed that a modiﬁed version of the UFFs used in [9] called “Robust UFFs”

for all real-valued signals  matching the previous upper bound for nonnegative signals  and showed
this is nearly tight with a lower bound of ⌦(k2 log n/ log k) for real signals. They also show that

✏ measurements sufﬁces for universal approximate recovery.

In tandem with the development of these theoretical results providing necessary and sufﬁcient
numbers of measurements for support recovery and approximate vector recovery  there has been a
signiﬁcant body of work in other directions on 1-bit compressed sensing  such as heuristic algorithms
that perform well empirically  and tradeoffs between different parameters. More speciﬁcally  [11]
introduced a gradient-descent based algorithm called Binary Iterative Hard Thresholding (BIHT)
which performs very well in practice; later  [13] gave another heuristic algorithm which performs
comparably well or better  and aims to allow for very efﬁcient decoding after the measurements are
taken. Other papers such as [18] have studied the tradeoff between the amount of quantization of the
signal  and the necessary number of measurements.

k + k

✏6 log n

k and O k

✏5 log2 n

Our Results. We focus primarily on upper bounds in the universal setting  aiming to give construc-
tions that work with high probability for all sparse vectors. In [1]  3 major open questions are given
regarding Universal 1-bit Compressed Sensing  which  paraphrasing  are as follows:

1. How many measurements are necessary and sufﬁcient for a matrix to be used to exactly

recover all k-sparse binary vectors?

2

Table 1: Upper and lower bounds for 1bCS problems with k-sparse signals

Problem
Universal Support Recovery (x 2 Rn)
Universal ✏-approximate Recovery (x 2 Rn)
Universal ✏-approximate Recovery (x 2 Rn
0)
Universal Exact Recovery (x 2{ 0  1}n)
Non-Universal Support Recovery (x 2 Rn)

UB

[1]  [11]

O⇣k2 log n⌘ [1]
eO(min(k2 log n
O⇣k log( n
O⇣k log( n
O (k log n) [3]

k ) + k

✏⌘⇤
k ) + k3/2⌘⇤

k + k

✏   k

✏ log n

k ))

*Bound proved in this work.

Explicit UB

O⇣k2 log n⌘⇤

–

–
–

O⇣k log2 n⌘⇤

LB

⌦(k2 log n/ log k) [1]
⌦(k log n

k + k

✏ ) [1]

⌦(k log n
k )
⌦(k log n
k )
⌦(k log n
k ) [3]

2. What is the correct complexity (in terms of number of measurements) of universal ✏-

approximate vector recovery for real signals?

3. Can we obtain explicit (i.e. requiring time polynomial in n and k) constructions of the
Robust UFFs used for universal support recovery (yielding measurement matrices with

Ok2 log n rows)?

k + k

k + k

✏   k

✏.

In this work we make progress towards solutions to all three Open Questions. Our primary contribu-
tion is the “superset technique” which relies on ideas from the closely related sparse recovery problem
of group testing [8]; in particular  we show in Theorem 6 that for a large class of signals including
all nonnegative (and thus all binary) signals  we can improve the upper bound for approximate
recovery by ﬁrst recovering an O (k)-sized superset of the support rather than the exact support  then
subsequently using Gaussian measurements. The previous best upper bound for binary signals from
[11] was Ok3/2 log n  which we improve to Ok3/2 + k log n
k  and for nonnegative signals was
Omin(k2 log n
✏ log n)  which we improve to Ok log n
of Robust UFFs yielding measurement matrices for support recovery with Ok2 log n rows in time
surements than is optimal (Ok log2 n as opposed to Ok log n
k) in Section 4.2; to our knowledge 

Regarding Open Question 3  using results of Porat and Rothschild regarding weakly explicit construc-
tions of Error-Correcting Codes (ECCs) on the Gilbert-Varshamov bound [16]  we give a construction

that is polynomial in n (though not in k) in Theorem 12. Based on a similar idea  we also give a
weakly explicit construction for non-universal approximate recovery using only sightly more mea-

explicit constructions in the non-universal setting have not been studied previously. Furthermore  this
result gives a single measurement matrix which works for almost all vectors  as opposed to typical
non-universal results which work with high probability for a particular vector and matrix pair.
In Appendix C  we give a sufﬁcient condition generalizing the notion of RUFFs for a matrix to be
used for universal recovery of a superset of the support for all real signals; while we do not provide
constructions  this seems to be a promising direction for resolving Open Question 2.
The best known upper and lower bounds for the various compressed sensing problems considered in
this work are presented in Table 1.
2 Deﬁnitions
We write Mi for the ith row of the matrix M  and Mi j for the entry of M in the ith row and jth
column. We write vectors x in boldface  and write xi for the ith component of the vector x. The set
{1  2  . . .   n} will be denoted by [n]  and for any set S we write P(S) for the power set of S (i.e. the
set of all subsets of S).
We will write supp(x) ✓ [n] to mean the set of indices of nonzero components of x (so supp(x) =
{i : xi 6= 0})  and ||x||0 to denote | supp(x)|.
For a real number y  sign(y) returns 1 if y is strictly positive  1 if y is strictly negative  and 0 if
y = 0. While this technically returns more than one bit of information  if we had instead deﬁned
sign(y) to be 1 when y  0 and 1 otherwise  we could still determine whether y = 0 by looking at
sign(y)  sign(y)  so this affects the numbers of measurements by only a constant factor. We will
not concern ourselves with the constants involved in any of our results  so we have chosen to instead
use the more convenient deﬁnition.
We will sometimes refer to constructions from the similar “group testing” problem in our results.
To this end  we will use the symbol “” to represent the group testing measurement between a
measurement vector and a signal vector. Speciﬁcally  for a measurement m of length n and signal x

3

of length n  m  x is equal to 1 if supp(m) \ supp(x) is nonempty  and 0 otherwise. We will also
make use of the “list-disjunct” matrices used in some group testing constructions.
Deﬁnition 1. An m ⇥ n binary matrix M is (k  l)-list disjunct if for any two disjoint sets S  T ✓
col(M ) with |S| = k |T| = l  there exists a row in M in which some column from T has a nonzero
entry  but every column from S has a zero.

The primary use of such matrices is that in the group testing model  they can be used to recover a
superset of size at most k + l of the support of any k-sparse signal x from applying a simple decoding
to the measurement results M  x.
In the following deﬁnitions  we write S for a generic set that is the domain of the signal. In this paper
we consider signals with domain R  R0 (nonnegative reals)  and {0  1}.
Deﬁnition 2. An m ⇥ n measurement matrix M can be used for Universal Support Recovery of
k-sparse x 2 Sn (in m measurements) if there exists a decoding function f : {1  0  1}m !P ([n])
such that f (sign(M x)) = supp(x) for all x satisfying ||x||0  k.
Deﬁnition 3. An m⇥ n measurement matrix M can be used for Universal ✏-Approximate Recovery
of k-sparse x 2 Sn (in m measurements) if there exists a decoding function f : {1  0  1}m ! Sn
such that

x

||x||2 



f (sign(M x))

||f (sign(M x))||22  ✏ 

for all x with ||x||0  k.
3 Upper Bounds for Universal Approximate Recovery
Here we present our main result  an upper bound on the number of measurements needed to perform
universal ✏-approximate recovery for a large class of real vectors that includes all binary vectors and
all nonnegative vectors. The general technique will be to ﬁrst use what are known as “list-disjunct”
matrices from the group testing literature to recover a superset of the support of the signal  then use
Gaussian measurements to approximate the signal within the superset. Because the measurements in
the second part are Gaussian  we can perform the recovery within the (initially unknown) superset
nonadaptively. When restricting to the class of binary or nonnegative signals  our upper bound
improves on existing results and is close to known lower bounds.
First  we need a lemma stating the necessary and sufﬁcient conditions on a signal vector x in
order to be able to reconstruct the results of a single group testing measurement m  x using sign
measurements. To concisely state the condition  we introduce some notation: for a subset S ✓ [n]
and vector x of length n  we write x|S to mean the restriction of x to the indices of S.
Lemma 1. Let m 2{ 0  1}n and x 2 Rn. Deﬁne S = supp(m) \ supp(x). If either S is empty or
S is nonempty and mT|S x|S 6= 0  we can reconstruct the result of the group testing measurement
m  x from the sign measurement sign(mT x).
Proof. We observe sign(mT x) and based on that must determine the value of m x  or equivalently
whether S is empty or nonempty. If sign(mT x) 6= 0 then mT x 6= 0  so S is nonempty and
m  x = 1. Otherwise we have sign(mT x) = 0  in which case we must have mT x = 0. If S were
nonempty then we would have mT|S x|S = 0  contradicting our assumption. Therefore in this case
we must have S empty and m  x = 0  so for x satisfying the above condition we can reconstruct
the results of a group testing measurement.

For convenience  we use the following property to mean that a signal x has the necessary property
from Lemma 1 with respect to every row of a matrix M.
Property 1. Let M be an m⇥n matrix  and x a signal of length n. Deﬁne Si = supp(Mi)\supp(x).
Then for every row Mi of M  either Si is empty  or M T
Corollary 2. Let M be a (k  l)-list disjunct matrix  and x 2 Rn be a k-sparse real signal. If
Property 1 holds for M and x  then we can use the measurement matrix M to recover a superset of
size at most k + l of the support of x using sign measurements.
k ) rows which we
Combining this corollary with results of [6]  there exist matrices with Ok log( n
can use to recover an O (k)-sized superset of the support of x using sign measurements  provided x

i |Si x|Si 6= 0.

4

satisﬁes the above condition. Strongly explicit constructions of these matrices exist also  although

requiring Ok1+o(1) log n rows [5].

The other result we need is one that tells us how many Gaussian measurements are necessary to
approximately recover a real signal using maximum likelihood decoding. Similar results have
appeared elsewhere  such as [11]  but we include the proof for completeness.
Lemma 3. There exists a measurement matrix A for 1-bit compressed sensing such that for every
pair of k-sparse x  y 2 Rn with ||x||2 = ||y||2 = 1  sign(Ax) 6= sign(Ay) whenever ||x y||2 >✏  
provided that

m = O✓ k

✏

k⌘◆ .
log⇣ n

We will make use of the following facts in the proof.
Fact 4. For all x 2 R  1  x < ex.
Fact 5. For all x 2 [0  1]  cos1(x) p2(1  x).
Proof of Lemma 3. Let A ⇠N m⇥n(0  1). For a measurement to separate x and y  it is necessary
that the hyperplane corresponding to some row a of A lies between x and y. Thus our goal here is to
show that if we take m to be large enough  that all pairs of points at distance >✏ will be separated
with high probability. Since the rows of A are chosen independently and have Gaussian entries  they
are spherically symmetric  and thus the probability that the random hyperplane a lies between x and
y is proportional to the angle between them. Let ||x  y||2 >✏   then we start out by upper bounding
the probability that no measurement separates a particular pair x and y.
Before beginning  recall that for unit vectors 1  xT y = ||x  y||2
2/2  so given that ||x  y||2 >✏  
we have xT y < 1  ✏2/2.

Pr[sign(ax) = sign(ay)] =

⇡

<

1  cos1(xT y)
1  cos1(1✏2/2)
 exp( cos1(1✏2/2)


⇡
exp( ✏
⇡ )

⇡

)

(from Fact 4).
(from Fact 5).

m✏

As there are m independent measurements  the probability that x and y are not separated by any of
the m measurements is at most

so union bounding over alln

⇡ ⌘  
exp⇣
✓n
k◆2
⇡ ⌘ .
exp⇣
This probability becomes less than 1 for m  ⇡
there exists a matrix that can perform ✏-approximate recovery for all pairs of sparse vectors.

k2 pairs of k-sparse x and y  the total probability of error is strictly less

k   so with this number of measurements

✏ (2k) log n

than

m✏

✏ log(O(k)

k )⌘ = O k

✏-approximate recovery within the superset. We can do this even nonadaptively  because the rows of
the matrix for approximate recovery are Gaussian. Combining this with Corollary 2 and the group
testing constructions of [6]  we have the following theorem.

Note that in the case that we already have a superset of the support of size O (k)  the previous result
tells us there exists a matrix with O⇣ k
✏ rows which can be used to perform
M (2) where M (1) is a (k O (k))-list disjunct matrix with Ok log n
Theorem 6. Let M = M (1)
k
rows  and M (2) is a matrix with O k
✏ rows that can be used for ✏-approximate recovery within the
superset as in Lemma 3  so M consists of Ok log( n
✏ rows. Let x 2 Rn be a k-sparse signal.

If Property 1 holds for M (1) and x  then M can be used for ✏-approximate recovery of x.

k ) + k

5

Remark. We note that the class of signal vectors x which satisfy the condition in Theorem 6 is
actually quite large  in the sense that there is a natural probability distribution over all sparse signals
x for which vectors violating the condition occur with probability 0. The details are laid out in
Lemma 14.

As special cases  we have improved upper bounds for nonnegative and binary signals. For ease of
comparison with the other results  we assume the binary signal is rescaled to have unit norm  so has

all entries either 0 or equal to 1/p||x||0.
Corollary 7. Let M =M (1)
rows  and M (2) is a matrix with O k
superset as in Lemma 3  so M consists of Ok log( n

M (2) where M (1) is a (k O (k))-list disjunct matrix with Ok log n
k
✏ rows that can be used for ✏-approximate recovery within the
✏ rows. Let x 2 Rn be a k-sparse signal.

If all entries of x are nonnegative  then M can be used for ✏-approximate recovery of x.

k ) + k

Proof. In light of Theorem 6  we need only note that as all entries of M (1) and x are nonnegative 
Property 1 is satisﬁed for M (1) and x.

M (2) where M (1) is a (k O (k))-list disjunct matrix with Ok log n
Corollary 8. Let M =M (1)
k
rows  and M (2) is a matrix with Ok3/2 rows that can be used for ✏-approximate recovery (with
✏< 1/pk) within the superset as in Corollary 2   so M consists of Ok log( n
k ) + k3/2 rows. Let
x 2 Rn be the k-sparse signal vector. If all nonzero entries of x are equal  then M can be used for
exact recovery of x.
Proof. Here we use the fact that if we perform ✏-approximate recovery using ✏< 1/pk then as
the minimum possible distance between any two k-sparse rescaled binary vectors is 1/pk  we will
recover the signal vector exactly.

4 Explicit Constructions
4.1 Explicit Robust UFFs from Error-Correcting Codes
In this section we explain how to combine several existing results in order to explicitly construct
Robust UFFs that can be used for support recovery of real vectors. This partially answers Open
Problem 3 from [1].
Deﬁnition 4. A family of sets F = {B1  B2  . . .   Bn} with each Bi ✓ [m] is an (n  m  d  k  ↵)-
Robust-UFF if |Bi| = d 8i  and for every distinct j0  j1  . . .   jk 2 [n]  |Bj0 \ (Bj1 [ Bj2 [···[
Bjk )| <↵ |Bj0|.
It is shown in [1] that nonexplicit (n  m  d  k  1/2)-Robust UFFs exist with m = Ok2 log n   d =
O (k log n) which can be used to exactly recover the support of any k-sparse real vector of length n
in m measurements.
The results we will need are the following  where the q-ary entropy function Hq is deﬁned as

Hq(x) = x logq(q  1)  x logq x  (1  x) logq(1  x).

Theorem 9 ([16] Thm. 2). Let q be a prime power  m and k positive integers  and  2 [0  1]. Then if
k  (1  Hq())m  we can construct a q-ary linear code with rate k
m and relative distance  in time
Omqk.
Theorem 10 ([1] Prop. 17). Given a q-ary error correcting code with rate r and relative distance
(1  )  we can construct a (qrd  qd  d  1  )-Robust-UFF.
Theorem 11 ([1] Prop.
(n  m  d  k  ↵)-Robust-UFF.

If F is an (n  m  d  1  ↵/k)-Robust-UFF  then F is also an

15).

By combining the above three results  we have the following.

6

Theorem 12. We can explicitly construct an (n  m  d  k  ↵)-Robust UFF with m = O⇣ k2 log n
d = O⇣ k log n
Proof. First  we instantiate Theorem 9 to obtain a q-ary code C of length d with q = O (k/↵)  relative
distance  = k↵
Applying Theorem 10 to this code results in an (n  m  d  1  )-Robust-UFF F where n = qrd 
m = qd   = 1  . By Theorem 11  F is also an (n  m  d  k  k)-Robust UFF. Plugging back in
the parameters of the original code 

↵ ⌘ in time O(k/↵)k.
k   and rate r = 1  Hq() in time Oqk.

↵2 ⌘ and

m = qd =

q log n
r log q

=

q log n

(1  Hq((k  ↵)/k)) log q

= O✓ k2 log n
↵2 ◆  

k = (1  )k = (1 

)k = k  (k  ↵) = ↵.

k  ↵

k

While the time needed for this construction is not polynomial in k (and therefore the construction is
not strongly explicit) as asked for in Open Question 3 of [1]  this at least demonstrates that there exist

codes with sufﬁciently good parameters to yield Robust UFFs with m = Ok2 log n.

4.2 Non-Universal Approximate Recovery
If instead of requiring our measurement matrices to be able to recover all k-sparse signals simultane-
ously (i.e. to be universal)  we can instead require only that they are able to recover “most” k-sparse
signals. Speciﬁcally  in this section we will assume that the sparse signal is generated in the following
way: ﬁrst a set of k indices is chosen to be the support of the signal uniformly at random. Then  the
signal is chosen to be a uniformly random vector from the unit sphere on those k indices. We relax
the requirement that the supports of all k-sparse signals can be recovered exactly (by some decoding)
to the requirement that we can identify the support of a k-sparse signal with probability at least 1   
where  2 [0  1). Note that even when  = 0  this is a weaker condition than universality  as the
space of possible k-sparse signals is inﬁnite.
It is shown in [3] that a random matrix construction using O (k log n) measurements sufﬁces to
recover the support with error probability approaching 0 as k and n approach inﬁnity. The following
theorem shows that we can explicitly construct a matrix which works in this setting  at the cost of
slightly more measurements (about Ok log2(n)).
 )⌘ rows that can exactly determine the support of a k-sparse
vectors) with m = O⇣k log(n)
signal with probability at least 1    where the signals are generated by ﬁrst choosing the size k
support uniformly at random  then choosing the signal to be a uniformly random vector on the sphere
on those k coordinates.

Theorem 13. We can explicitly construct measurement matrices for Support Recovery (of real

log k log( n

To prove this theorem  we need a lemma which explains how we can use sign measurements to
“simulate” group testing measurements with high probability. Both the result and proof are similar
to Lemma 1  with the main difference being that given the distribution described above  the vectors
violating the necessary condition in Lemma 1 occur with zero probability and so can be safely ignored.
For this lemma  we do not need the further assumption made in Theorem 13 that the distribution over
support sets is uniform. The proof is presented in Appendix A.
Lemma 14. Suppose we have a measurement vector m 2{ 0  1}n  and a k-sparse signal x 2 Rn.
The signal x is generated randomly by ﬁrst picking a subset of size k from [n] (using any distribution)
to be the support  then taking x to be a uniformly random vector on the sphere on those k coordinates.
Then from sign(mT x)  we can determine the value of m  x with probability 1.
As the above argument works with probability 1  we can easily extend it to an entire measurement
matrix M with any ﬁnite number of rows by a union bound  and recover all the group testing
measurement results M  x with probability 1 as well. This means we can leverage the following
result from [14]:

7

log k log(n) log( n

log k log n log( n

k + k

Theorem 15 ([14] Thm. 5). When x 2{ 0  1}n is drawn uniformly at random among all k-
sparse binary vectors  there exists an explicitly constructible group testing matrix M with m =
 )⌘ rows which can exactly identify x from observing the measurement results
O⇣ k
M  x with probability at least 1  .
Combining this with the lemma above  we can use the matrix M from Theorem 15 with m =
O⇣ k
 )⌘ rows (now representing sign measurements) to exactly determine the support
of x with probability at least 1  ; we ﬁrst use Lemma 14 to recover the results of the group testing
tests M  x with probability 1  and can then apply the above theorem using the results of the group
testing measurements.
We can also use this construction for approximate recovery rather than support recovery using
Lemma 3  by appending O k
✏ rows of Gaussian measurements to M  ﬁrst recovering the exact
✏ rows for non-universal approximate recovery of real signals  where the top
Ok log2(n) + k

portion is explicit.
Remark. Above  we have shown that in the non-universal setting  we can use constructions from
group testing to recover the exact support with high probability  and then subsequently perform
approximate recovery within that exact support. If we are interested only in performing approximate
recovery  we can apply our superset technique here as well; Lemma 14 implies also that using
a (k O (k))-list disjunct matrix we can with probability 1 recover an O (k)-sized superset of the
support  and such matrices exist with Ok log n
✏ more
Gaussian measurements to recover the signal within the superset. This gives a non-universal matrix
✏ rows for approximate recovery  the top part of which can be made strongly
with Ok log n
explicit with only slightly more measurements (Ok1+o(1) log n

k rows. Following this  we can use O k

support  then doing approximate recovery within that support. This gives a matrix with about

5 Experiments
In this section  we present some empirical results relating to the use of our superset technique in
approximate vector recovery for real-valued signals. To do so  we compare the average error (in `2
norm) of the reconstructed vector from using an “all Gaussian” measurement matrix to ﬁrst using
a small number of measurements to recover a superset of the support of the signal  then using the
remainder of the measurements to recover the signal within that superset via Gaussian measurements.
We have used the well-known BIHT algorithm of [11] for recovery of the vector both using the all
Gaussian matrix and within the superset  but we emphasize that this superset technique is highly
general  and could just as easily be applied on top of other decoding algorithms that use only Gaussian
measurements  such as the “QCoSaMP” algorithm of [17].

k vs. Ok log n
k).

To generate random signals x  we ﬁrst choose a size k support uniformly at random among then
k
possibilities  then for each coordinate in the chosen support  generate a random value from N (0  1).
The vector is then rescaled so that ||x||2 = 1.
For the dotted lines in Figure 1 labeled “all Gaussian ” for each value of (n  m  k) we performed 500
trials in which we generated an m ⇥ n matrix with all entries in N (0  1). We then used BIHT (run
either until convergence or 1000 iterations  as there is no convergence guarantee) to recover the signal
from the measurement matrix and measurement outcomes.
For the solid lines in Figure 1 labeled “4k log n Superset ” we again performed 500 trials for each

value of (n  m  k) where in each trial we generated a measurement matrix M =M (1)

M (2) with m

rows in total. Each entry of M (1) is a Bernoulli random variable that takes value 1 with probability
k+1 and value 0 with probability k
k+1; there is evidence from the group testing literature [3  2] that
this probability is near-optimal in some regimes  and it appears also to perform well in practice;
see Appendix B for some empirical evidence. The entries of M (2) are drawn from N (0  1). We
use a standard group testing decoding (i.e.  remove any coordinates that appear in a test with result
0) to determine a superset based on y1 = sign(M (1)x)  then use BIHT (again run either until
convergence or 1000 iterations) to reconstruct x within the superset using the measurement results
y2 = sign(M (2)x). The number of rows in M (1) is taken to be m1 = 4k log10(n) based on the

1

8

(a) n = 1000  k = 5

(b) n = 1000  k = 10

(c) n = 1000  k = 20

(d) n = 1000  k = 40

Figure 1: Average error of reconstruction for different sparsity levels with and without use of matrix
for superset of support recovery

fact that with high probability Ck log n rows for some constant C should be sufﬁcient to recover an
O (k)-sized superset  and the remainder m2 = (m  m1) of the measurements are used in M (2).
We display data only for larger values of m  to ensure there are sufﬁciently many rows in both
portions of the measurement matrix. From Figure 1 one can see that in this regime  using a small
number of measurements to ﬁrst recover a superset of the support provides a modest improvement in
reconstruction error compared to the alternative. In the higher-error regime when there are simply
not enough measurements to obtain an accurate reconstruction  as can be seen in the left side of the
graph in Figure 1d  the two methods perform about the same. In the empirical setting  our superset of
support recovery technique can be viewed as a very ﬂexible and low overhead method of extending
other existing 1bCS algorithms which use only Gaussian measurements  which are quite common.
Acknowledgements: This research is supported in part by NSF CCF awards 1618512  1642658  and
1642550 and the UMass Center for Data Science.
References
[1] Jayadev Acharya  Arnab Bhattacharyya  and Pritish Kamath. Improved bounds for universal
one-bit compressive sensing. In 2017 IEEE International Symposium on Information Theory
(ISIT)  pages 2353–2357. IEEE  2017.

[2] Matthew Aldridge  Leonardo Baldassini  and Oliver Johnson. Group testing algorithms: Bounds

and simulations. IEEE Trans. Information Theory  60(6):3671–3687  2014.

[3] George K. Atia and Venkatesh Saligrama. Boolean compressed sensing and noisy group testing.

IEEE Trans. Information Theory  58(3):1880–1901  2012.

[4] Petros Boufounos and Richard G. Baraniuk. 1-bit compressive sensing.

In 42nd Annual
Conference on Information Sciences and Systems  CISS 2008  Princeton  NJ  USA  19-21 March
2008  pages 16–21. IEEE  2008.

[5] Mahdi Cheraghchi. Noise-resilient group testing: Limitations and constructions. Discrete

Applied Mathematics  161(1-2):81–95  2013.

9

[6] Annalisa De Bonis  Leszek Gasieniec  and Ugo Vaccaro. Optimal two-stage algorithms for

group testing problems. SIAM Journal on Computing  34(5):1253–1270  2005.

[7] David L. Donoho. Compressed sensing. IEEE Trans. Information Theory  52(4):1289–1306 

2006.

[8] D. Du and F. Hwang. Combinatorial Group Testing and Its Applications. Applied Mathematics.

World Scientiﬁc  2000.

[9] Sivakant Gopi  Praneeth Netrapalli  Prateek Jain  and Aditya Nori. One-bit compressed sensing:
Provable support and vector recovery. In International Conference on Machine Learning  pages
154–162  2013.

[10] Jarvis D. Haupt and Richard G. Baraniuk. Robust support recovery using sparse compressive
sensing matrices. In 45st Annual Conference on Information Sciences and Systems  CISS 2011 
The John Hopkins University  Baltimore  MD  USA  23-25 March 2011  pages 1–6. IEEE  2011.
[11] Laurent Jacques  Jason N Laska  Petros T Boufounos  and Richard G Baraniuk. Robust 1-bit
compressive sensing via binary stable embeddings of sparse vectors. IEEE Transactions on
Information Theory  59(4):2082–2102  2013.

[12] HJ Landau. Sampling  data transmission  and the nyquist rate. Proceedings of the IEEE 

55(10):1701–1706  1967.

[13] Ping Li. One scan 1-bit compressed sensing. In Arthur Gretton and Christian C. Robert  editors 
Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics 
AISTATS 2016  Cadiz  Spain  May 9-11  2016  volume 51 of JMLR Workshop and Conference
Proceedings  pages 1515–1523. JMLR.org  2016.

[14] Arya Mazumdar. Nonadaptive group testing with random set of defectives.

Information Theory  62(12):7522–7531  2016.

IEEE Trans.

[15] Yaniv Plan and Roman Vershynin. Robust 1-bit compressed sensing and sparse logistic re-
gression: A convex programming approach. IEEE Trans. Information Theory  59(1):482–494 
2013.

[16] Ely Porat and Amir Rothschild. Explicit nonadaptive combinatorial group testing schemes.

IEEE Trans. Information Theory  57(12):7982–7989  2011.

[17] Hao-Jun Michael Shi  Mindy Case  Xiaoyi Gu  Shenyinying Tu  and Deanna Needell. Methods
for quantized compressed sensing. In 2016 Information Theory and Applications Workshop 
ITA 2016  La Jolla  CA  USA  January 31 - February 5  2016  pages 1–9. IEEE  2016.

[18] Martin Slawski and Ping Li. b-bit marginal regression. In Corinna Cortes  Neil D. Lawrence 
Daniel D. Lee  Masashi Sugiyama  and Roman Garnett  editors  Advances in Neural Information
Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015 
December 7-12  2015  Montreal  Quebec  Canada  pages 2062–2070  2015.

[19] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society: Series B (Methodological)  58(1):267–288  1996.

10

,Larkin Flodin
Venkata Gandikota
Arya Mazumdar