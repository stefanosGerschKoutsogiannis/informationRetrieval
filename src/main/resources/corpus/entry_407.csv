2018,Adaptive Negative Curvature Descent with Applications in Non-convex Optimization,Negative curvature descent (NCD) method has been utilized to design deterministic or stochastic algorithms for non-convex optimization aiming at finding second-order stationary points or local minima. In existing studies  NCD needs to approximate the smallest eigen-value of the Hessian matrix with a sufficient precision (e.g.  $\epsilon_2\ll 1$) in order to achieve a sufficiently accurate second-order stationary solution (i.e.  $\lambda_{\min}(\nabla^2 f(\x))\geq -\epsilon_2)$.  One issue  with this approach is that the target precision $\epsilon_2$ is usually set to be very small in order to find a high quality solution  which increases the complexity for computing a negative curvature. To address this issue  we propose an adaptive NCD to allow for an adaptive error dependent on the current gradient's magnitude in approximating the smallest eigen-value of the Hessian  and to encourage competition between  a noisy NCD step and gradient descent step. We consider the applications of the proposed adaptive NCD for both deterministic and stochastic non-convex optimization  and demonstrate that it can help reduce the the overall complexity in computing the negative curvatures during the course of optimization without sacrificing the iteration complexity.,Adaptive Negative Curvature Descent

with Applications in Non-convex Optimization

Mingrui Liu†  Zhe Li†  Xiaoyu Wang‡  Jinfeng Yi(cid:92)  Tianbao Yang†

†Department of Computer Science  The University of Iowa  Iowa City  IA 52242  USA

‡ Intellifusion (cid:92) JD AI Research

mingrui-liu  tianbao-yang@uiowa.edu

Abstract

Negative curvature descent (NCD) method has been utilized to design deterministic
or stochastic algorithms for non-convex optimization aiming at ﬁnding second-order
stationary points or local minima. In existing studies  NCD needs to approximate
the smallest eigen-value of the Hessian matrix with a sufﬁcient precision (e.g. 
2 (cid:28) 1) in order to achieve a sufﬁciently accurate second-order stationary solution
(i.e.  λmin(∇2f (x)) ≥ −2). One issue with this approach is that the target
precision 2 is usually set to be very small in order to ﬁnd a high quality solution 
which increases the complexity for computing a negative curvature. To address
this issue  we propose an adaptive NCD to allow an adaptive error dependent on
the current gradient’s magnitude in approximating the smallest eigen-value of the
Hessian  and to encourage competition between a noisy NCD step and gradient
descent step. We consider the applications of the proposed adaptive NCD for both
deterministic and stochastic non-convex optimization  and demonstrate that it can
help reduce the the overall complexity in computing the negative curvatures during
the course of optimization without sacriﬁcing the iteration complexity.

1

Introduction

In this paper  we consider the following optimization problem:

min
x∈Rd

f (x) 

(1)

and

(cid:107)∇f (x)(cid:107) ≤ 1 

λmin(∇2f (x)) ≥ −2 

where f (x) is a non-convex smooth function with Lipschitz continuous Hessian  which could has
some special structure (e.g.  expectation structure or a ﬁnite-sum structure). A standard measure
of an optimization algorithm is how fast the algorithm converges to an optimal solution. However 
ﬁnding the global optimal solution to a generally non-convex problem is intractable [13] and is even a
NP-hard problem [10]. Therefore  we aim to ﬁnd an approximate second-order stationary point with:
(2)
i.e.  ∇f (x∗) =
which nearly satisfy the second-order necessary optimality conditions 
0  λmin(∇2f (x∗)) ≥ 0  where (cid:107) · (cid:107) denotes the Euclidean norm and λmin(·) denotes the smallest
eigen-value function. In this work  we refer to a solution that satisﬁes (2) as an (1  2)-second-order
stationary solution. When the function is non-degenerate (i.e.  strict saddle or the Hessian at all
saddle points have a strictly negative eigen-value)  then the solution satisfying (2) is close to a local
minimum for sufﬁciently small 0 < 1  2 (cid:28) 1. Please note that in this paper we do not follow the
tradition of [14] that restricts 2 =
1. One reason is for more generality that allows us to compare
several recent results and another reason is that having different accuracy levels for the ﬁrst-order and
the second-order guarantee brings more ﬂexibility in the choice of our algorithms.
Recently  there has emerged a surge of studies interested in ﬁnding an approximate second-order
stationary point that satisfy (2). An effective technique used in many algorithms is negative curvature

√

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

curvature  its complexity (or the number of Hessian-vector products) is in the order of (cid:101)O(1/

descent (NCD)  which utilizes a negative curvature direction to decrease the objective value. NCD
has two additional beneﬁts (i) escaping from non-degenerate saddle points; (ii) searching for a region
where the objective function is almost-convex that enables accelerated gradient methods. It has
been leveraged to design deterministic and stochastic non-convex optimization with state-of-the-art
time complexities for ﬁnding a second-order stationary point [4  17  16  2]. A common feature
of these algorithms is that they need to compute a negative curvature direction that approximates
the eigenvector corresponding to the smallest eigen-value to an accurate level matching the target
precision 2 on the second-order information  i.e.  ﬁnding a unit vector v such that λmin(∇2f (x)) ≥
v(cid:62)∇2f (x)v−2/2. The approximation accuracy has a direct impact on the complexity of computing
the negative curvature. For example  when the Lanczos method is utilized for computing the negative
2).
One potential issue is that the target precision 2 is usually set to be very small in order to ﬁnd a high
quality solution  which increases the complexity for computing a negative curvature  e.g.  the number
of Hessian-vector products used in the Lanczos method.
In this paper  we propose an adaptive NCD step based on full or sub-sampled Hessian that uses a noisy
negative curvature to update the solution with an error of approximating the smallest eigen-value
adaptive to the magnitude of the (stochastic) gradient at the time of invocation. A novel result is that
for an iteration t that requires a negative curvature direction it is enough to compute a noisy negative
curvature that approximates the smallest eigen-vector with a noise level of max(2 (cid:107)g(xt)(cid:107)α)  where
g(xt) is the gradient or mini-batch stochastic gradient at the current solution xt and α ∈ (0  1] is
a parameter that characterizes the relationship between 2 and 1  i.e.  2 = α
1 . It implies that

the Lanczos method only needs (cid:101)O(1/(cid:112)max(2 (cid:107)g(xt)(cid:107)α) number of Hessian-vector products for

√

computing such a noisy negative curvature. Another feature of the proposed adaptive NCD step is
that it encourages the competition between a negative curvature descent and the gradient descent
to guarantee a maximal decrease of the objective value. Building on the proposed adaptive NCD
step  we design two simple algorithms to enjoy a second-order convergence for deterministic and
stochastic non-convex optimization. Furthermore  we demonstrate the applications of the proposed
adaptive NCD steps in existing deterministic and stochastic optimization algorithms to match the
state-of-the-art worst-case complexity for ﬁnding a second-order stationary point. However  the
adaptive nature of the developed algorithms make them perform better than their counterparts using
the standard NCD step.

2 Related Work

There have been several recent studies that explicitly explore the negative curvature direction for
updating the solution. Here  we emphasize the differences between the development in this paper and
previous works. Curtis and Robinson [7] proposed a similar algorithm to one of our deterministic
algorithms except for how to compute the negative curvature. The key difference between our work
and [7] lie at they ignored the computational costs for computing the (approximate) negative curvature.
In addition  they considered a stochastic version of their algorithms but provided no second-order
convergence guarantee. In contrast  we also develop a stochastic algorithm with provable second-order
convergence guarantee.
Royer and Wright [17] proposed an algorithm that utilizes the negative gradient direction  the negative
curvature direction  the Newton direction and the regularized Newton direction together with line
search in a uniﬁed framework  and also analyzed the time complexity of a variant with inexact
calculations of the negative curvature by the Lanczos algorithm and of the (regularized) Newton
directions by conjugate gradient method. The comparison between their algorithm and our algorithms
shows that (i) we only use the gradient and the negative curvature directions; (ii) the time complexity
2);
(iii) the time complexity of one of our deterministic algorithm is at least the same and usually better
than their time complexity. Additionally  their conjugate gradient method could fail due to the
inexact smallest eigen-value computed by the randomized Lanczos method  and their ﬁrst-order and
second-order convergence guarantee could be on different points.
Carmon et al. [4] developed an algorithm that utilizes the negative curvature descent to reach a region
that is almost convex and then switches to an accelerated gradient method to decrease the magnitude
of the gradient. One of our algorithms is built on this development by replacing their negative

for computing an approximate negative curvature in their work is also of the order of (cid:101)O(1/

√

2

curvature descent with our adaptive negative curvature descent  which has the same guarantee on the
smallest eigen-value of the returned solution but uses a much less number of Hessian-vector products.
In addition  we also show that an inexact Hessian can be used in place of the full Hessian to enjoy the
same iteration complexity. Several studies revolve around solving cubic regularization step [1  18] 
which also requires a negative curvature direction.
Recently  several stochastic algorithms use the negative curvature information to derive the state-
of-the-art time complexities for ﬁnding a second-order stationary point for non-convex optimiza-
tion [16  2  19  3]  which combine existing stochastic ﬁrst-order algorithms and a NCD method with
differences lying at how to compute the negative curvature. In this work  we also demonstrate the
applications of the proposed adaptive NCD for stochastic non-convex optimization  and develop
several stochastic algorithms that not only match the state-of-the-art worst-case time complexity but
also enjoy adaptively smaller time complexity for computing the negative curvature. We emphasize
that the proposed adaptive NCD could be used in future developments of non-convex optimization.

3 Preliminaries and Warm-up

In this work  we will consider two types of non-convex optimization problem: deterministic ob-
jective where the gradient ∇f (x) and Hessian ∇2f (x) can be computed  stochastic objective
f (x) = Eξ[f (x; ξ)] where only stochastic gradient ∇f (x; ξ) and stochastic Hessian ∇2f (x; ξ) can
be computed. We note that a ﬁnite-sum objective can be considered as a stochastic objective. The
goal of the paper is to design algorithms that can ﬁnd an (1  2)-second order stationary point x that
satisﬁes (2). For simplicity  we consider 2 = α
A function f (x) is smooth if its gradient is Lipschitz continuous  i.e.  there exists L1 > 0 such that
(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L1(cid:107)x − y(cid:107) hold for all x  y. The Hessian of a twice differentiable function
f (x) is Lipschitz continuous  if there exists L2 > 0 such that (cid:107)∇2f (x)−∇2f (y)(cid:107)2 ≤ L2(cid:107)x−y(cid:107) for
all x  y  where (cid:107)X(cid:107)2 denotes the spectral norm of a matrix X. A function f (x) is called µ-strongly
2(cid:107)y − x(cid:107)2 ∀x  y. If f (x) satisﬁes the above
convex (µ > 0) if f (y) ≥ f (x) + ∇f (x)(cid:62)(y − x) + µ
condition for µ < 0  it is referred to as γ-almost convex with γ = −µ.
Throughout the paper  we make the following assumptions.
Assumption 1. For the optimization problem (1)  we assume:

1 for α ∈ (0  1].

(i) the objective function f (x) is twice differentiable;
(ii) it has L1-Lipschitz continuous gradient and L2-Lipschitz continuous Hessian;
(iii) given an initial solution x0  there exists ∆ < ∞ such that f (x0) − f (x∗) ≤ ∆  where x∗

denotes the global minimum of (1);

(iv) if f (x) is a stochastic objective  we assume each random function f (x; ξ) is twice dif-
ferentiable and has L1-Lipschitz continuous gradient and L2-Lipschitz continuous Hes-
sian  and its stochastic gradient has exponential tail behavior  i.e.  E[exp((cid:107)∇f (x; ξ) −
∇f (x)(cid:107)2/G2)] ≤ exp(1) holds for any x ∈ Rd;

(v) a Hessian-vector product can be computed in O(d) time.

In this paper  we assume there exists an algorithm that can compute a unit-length negative curvature
direction v ∈ Rd of a function f (x) satisfying

λmin(∇2f (x)) ≥ v(cid:62)∇2f (x)v − ε

(3)
with high probability 1 − δ. We refer to such an algorithm as NCS(f  x  ε  δ) and denote its time
complexity by Tn(f  ε  δ  d)  where NCS is short for negative curvature search.
There exist algorithms to implement negative curvature search (NCS) for two different cases: de-
terministic objective and stochastic objective with theoretical guarantee  which we provide in the
supplement. To facilitate the discussion in the following sections  we summarize the results here.
Lemma 1. For a deterministic objective  the Lanczos method ﬁnd a unit vector v satisfying (3) with
. For a stochastic objective f (x) = Eξ[f (x; ξ)]  there
exists a randomized algorithm that produces a unit vector v satisfying (3) with a time complexity

a time complexity of Tn(f  ε  δ  d) = (cid:101)O

(cid:17)

(cid:16) d√

ε

3

  δ) to ﬁnd a unit vector v satisfying (3)

sign(v(cid:62)∇f (x))v

L1

2

2L1

>

L2

3L2
2

(cid:107)∇f (x)(cid:107)2

∇f (x)

x+ = x − 2|v(cid:62)∇2f (x)v|
x+ = x − 1

Algorithm 1 AdaNCDdet(x  α  δ ∇f (x))
1: Apply NCS(f  x  max(2 (cid:107)∇f (x)(cid:107)α)
2: if 2(−v(cid:62)∇2f (x)v)3
then
3:
4: else
5:
6: end if
7: Return x+  v
Algorithm 2 AdaNCDmb(x  α  δ S  g(x)):
1: Apply NCS(fS   x  max(2 (cid:107)g(x)(cid:107)α)
− 2|v(cid:62)HS (x)v|2
2: if 2(−v(cid:62)HS (x)v)3
3:
4: else
5:
6: end if
7: Return x+  v

x+ = x − 2|v(cid:62)HS (x)v|
x+ = x − 1

g(x)

3L2
2

6L2
2

zv

L2

L1

2

(cid:107)g(x)(cid:107)2

  δ) to ﬁnd a unit vector v satisfying (3) for fS
>

− (cid:48)2
(cid:5)z ∈ {1 −1} is a Rademacher random variable

then

4L1

L1

of with Tn(f  ε  δ  d) = (cid:101)O(cid:0) d
Tn(f  ε  δ  d) = (cid:101)O(d(m + m3/4(cid:112)1/ε))  where (cid:101)O suppresses a logarithmic term in δ  d  1/ε.

(cid:1). If f (x) has a ﬁnite-sum structure with m components  then a

randomized algorithm exists that produces a unit vector v satisfying (3) with a time complexity of

ε2

4 Adaptive Negative Curvature Descent Step
In this section  we present several variants of adaptive negative curvature descent (AdaNCD) step
for different objectives and with different available information. We also present their guarantee on
decreasing the objective function.

4.1 Deterministic Objective
For a deterministic objective  when a negative curvature of the Hessian matrix ∇2f (x) at a point x is
required  the gradient ∇f (x) is readily available. We utilize this information to design an AdaNCD
shown in Algorithm 1. First  we compute a noisy negative curvature v that approximates the smallest
eigen-value of the Hessian at the current point x up to a noise level ε = max(2 (cid:107)∇f (x)(cid:107)α). Then
we take either the noisy negative curvature direction or the negative gradient direction depending on
which decreases the objective value more. This is done by comparing the estimations of the objective
decrease for following these two directions as shown in Step 3 in Algorithm 1. Its guarantee on
objective decrease is stated in the following lemma  which will be useful for proving convergence to
a second-order stationary point.
Lemma 2. When v(cid:62)∇2f (x)v ≤ 0  the Algorithm 1 (AdaNCDdet) provides a guarantee that

f (x) − f (x+) ≥ max

(cid:18) 2|v(cid:62)∇2f (x)v|3

3L2
2

(cid:107)∇f (x)(cid:107)2

2L1

 

4.2 Stochastic Objective
For a stochastic objective f (x) = Eξ[f (x; ξ)]  we assume a noisy gradient g(x) that satisﬁes (4)
(with high probability) is available when computing the negative curvature at x:

(cid:107)g(x) − ∇f (x)(cid:107) ≤ (cid:48)

(4)

This can be met by using a mini-batch stochastic gradient g(x) = 1|S1|
(cid:80)
sufﬁciently large batch size (see Lemma 9 in the supplement).
We can use a NCS algorithm to compute a negative curvature based on a mini-batched Hessian. To
ξ∈S ∇2f (x; ξ)  where S denote a set of random samples  satisfy the
this end  let HS (x) = 1|S|

f (x; ξ) with a

ξ∈S1

(cid:19)

(cid:80)

4

following inequality (with high probability):

(cid:107)HS (x) − ∇2f (x)(cid:107)2 ≤ 2/12.

(5)
The inequality (5) holds with high probability when S is sufﬁciently large due to the exponential tail
(cid:80)
behavior of (cid:107)HS (x) − ∇2f (x)(cid:107)2 stated in the Lemma 8 in the supplement.
ξ∈S f (·; ξ). A variant of AdaNCD using such a mini-batched Hessian is
Denote by fS = 1|S|
presented in Algorithm 2  where z is a Rademacher random variable  i.e. z = 1 −1 with equal
probability. Lemma 3 provides objective decrease guarantee of Algorithm 2.
Lemma 3. When v(cid:62)HS (x)v ≤ 0 and (5) holds (with high probability)  the Algorithm 2 (AdaNCDmb)
provides a guarantee (with high probability) that

f (x) − E[f (x+)] ≥ max
If v(cid:62)HS (x)v ≤ −2/2  we have

(cid:26) 2(−v(cid:62)HS (x)v)3
(cid:18) 3

3L2
2

2

 

6L2
2

− 2|v(cid:62)HS (x)v|2
(cid:19)

(cid:107)g(x)(cid:107)2

− (cid:48)2

 

24L2
2

4L1

L1

f (x) − f (x+) ≥ max

(cid:27)

(cid:107)g(x)(cid:107)2

4L1

− (cid:48)2

L1

Remark: When the objective has a ﬁnite-sum structure  Algorithm 2 is also applicable  where
the noise gradient g(x) can be replaced with the full gradient ∇f (x). This is the variant using
sub-sampled Hessian.
We can also use a different variant of Algorithm 2: AdaNCDonline  which uses an online algorithm to
compute the negative curvature and is described in Algorithm 7 (in the supplement) with Lemma 7
(in the supplement) as its theoretical guarantee.
5 Simple Adaptive Algorithms with Second-order Convergence
In this section  we present simple deterministic and stochastic algorithms by employing AdaNCD
presented in the last section. These simple algorithms deserve attention due to several reasons (i) they
are simpler than many previous algorithms but can enjoy a similar time complexity when 2 = 1;
(ii) they guarantee that the objective value can decrease at every iteration  which does not hold for
some complicated algorithms with state-of-the-art complexity results (e.g.  [4  2]).

(cid:19)

2L1
2
1

2L1
2
1

 

2
3α
1

 

2
3α
1

j∗ ≤ 1 + max

(cid:18) 12L2

(cid:18) 12L2

(f (x1) − f (xj∗ )) ≤ 1 + max

5.1 Deterministic Objective
We present a deterministic algorithm for a deterministic objective in Algorithm 3  which is referred
to as AdaNCG (where NCG represents Negative Curvature and Gradient  Ada represents the adaptive
nature of the NCD component).
Theorem 1. For any α ∈ (0  1]  the AdaNCG algorithm terminates at iteration j∗ for some

(cid:19)
with (cid:107)∇f (xj∗ )(cid:107) ≤ 1  and with probability at least 1 − δ  λmin(∇2f (xj∗ )) ≥ −α
1  (cid:107)∇f (xj)(cid:107)α)  δ(cid:48)  d).
the j-th iteration requires time a complexity of Tn(f  max(α
Remark: First  when 2 = 1 =   the iteration complexity of AdaNCG for achieving a point
with max{(cid:107)∇f (x)(cid:107) −λmin(∇2f (x))} ≤  is O(1/3)  which match the results in previous works
(e.g. [17  5  6  18]). However  the number of Hessian-vector products in AdaNCG could be much less
than that in these existing works. For example  the number of Hessian-vector products in [17  18]
2) at each iteration requiring the second-order information. In contrast  when employ-
ing the Lanczos method the number of Hessian-vector products at each iteration of AdaNCG is
2) depending on the
magnitude of the gradient. Second  the worse-case time complexity of AdaNCG is given by
using the worse-case time complexity of each iteration  which is the

is (cid:101)O(1/
(cid:101)O(d/(cid:112)max(2 (cid:107)∇f (xj)(cid:107)α))  which could be much smaller than (cid:101)O(1/
(cid:16)
(cid:101)O
end up with (cid:101)O(d/9/4)  which is worse than the best time complexity (cid:101)O(d/7/4) found in literature

same as the result of Theorem 2 in [18].
One might notice that if we plug 1 =   2 =

 into the worst-case time complexity of AdaNCG  we

−2
1 

−1/2
2

−7/2
  
2

1 . Furthermore 

(cid:111)(cid:17)

∆ 

(6)

√

(cid:110)

d max

√

√

5

(xj+1  vj) = AdaNCDdet(xj  α  δ(cid:48) ∇f (x))
if v(cid:62)

2 and (cid:107)∇f (xj)(cid:107) ≤ 1 then

2

3
2

∆) 

(cid:17)

  2L1
2
1

(cid:16) 12L2

j ∇2f (xj)vj > − 2
Return xj

Algorithm 3 AdaNCG: (x0  1  α  δ)
1: x1 = x0  2 = α
1
2: δ(cid:48) = δ/(1 + max
3: for j = 1  2  . . .   do
4:
5:
6:
end if
7:
8: end for
Algorithm 4 S-AdaNCG: (x0  1  α  δ)
1: x1 = x0  2 = α
2: for j = 1  2  . . .   do
3:
4:
5:
6:
7:
end if
8:
9: end for

1   δ(cid:48) = δ/(cid:101)O(−2
(cid:80)

1   −3
2 )

Generate two random sets S1 S2
let g(xj) = 1|S1|
(xj+1  vj) = AdaNCDmb(xj  α  δ(cid:48) S2  g(xj))
if v(cid:62)

j HS2 (xj)vj > −2/2 and (cid:107)g(xj)(cid:107) ≤ 1 then
Return xj

∇f (x; ξ) satisfy (4)

ξ∈S1

2
1

1

2
2

log( 4d

(1 + 3 log( 2

(e.g.  [1  4]). In next section  we will use AdaNCG as a sub-routine to develop an algorithm that can
match the state-of-the-art time complexity but also enjoy the adaptiveness of AdaNCG.
Before ending this subsection  we would like to point out that an inexact Hessian satisfying (5) can
be used for computing the negative curvature. For example  if the objective has a ﬁnite-sum form 
AdaNCDdet can be replaced by AdaNCDmb using full gradient. Lemma 3 provides a similar guarantee
to Lemma 2 and can be used to derive a similar convergence to Theorem 1.
5.2 Stochastic Objective
We present a stochastic algorithm based on the AdaNCDmb in Algorithm 4  which is referred to as
S-AdaNCG (where S represents stochastic). A similar algorithm based on the AdaNCDonline with
similar worst-case complexity can be developed  which is omitted.
Theorem 2. Set |S1| = 32G2

.
Remark: We can analyze the worst-case time complexity of S-AdaNCG by using randomized

(cid:17)
δ(cid:48) ). With probability 1 − δ  the
(cid:0)∇2f (xj∗ )(cid:1) ≥ −22 with probability 1−3δ. Furthermore  the
(cid:17)(cid:17)
(cid:17)(cid:16) d
) and upon termination it
  1
2
1
(cid:1). Let us
(cid:1)  which matches the time complexity of stochastic gradient descent for ﬁnding a ﬁrst-order
(cid:1) for ﬁnding an (1 

(cid:16) 1
S-AdaNCG algorithm terminates at some iteration j∗ = (cid:101)O(max
δ(cid:48) )) and |S2| = 9216L2
(cid:16) 1
worst-case time complexity of S-AdaNCG is given by (cid:101)O
holds that (cid:107)∇f (xj∗ )(cid:107) ≤ 21 and λmin
algorithms as in Lemma 1 to compute the negative curvature with Tn(f  ε  δ  d) = (cid:101)O(cid:0) d
(cid:101)O(cid:0)d/4
of (cid:101)O(cid:0)d/3.5
matches (cid:101)O(cid:0)d/3.5

stationary point. It is almost linear in the problem’s dimensionality better than that of noisy SGD
methods [9  20]. It is also notable that the worst-case time complexity of S-AdaNCG is worse than that
of a recent algorithm called Natasha2 proposed in [2]  which has a state-of-the-art time complexity
1) second-order stationary point. However  S-AdaNCG is much
simpler than Natasha2  which involves many parameters and switches between several procedures. In
next section  we will present an improved algorithm of S-AdaNCG  whose worst-case time complexity

1) second-order stationary point.
6 Adaptive Algorithms with State-of-the-Art Complexities
In this section  we demonstrate the applications of the presented AdaNCD for deterministic and
stochastic optimization with a state-of-the-art time complexity  aiming for better practical performance
than their counterparts in literature. We will show that how the proposed AdaNCD can reduce the
time complexity of these existing algorithms.

  it is not difﬁcult to show that the worst-case time complexity of S-AdaNCG is

(cid:1) for ﬁnding an (1 

+ Tn(fS2  2  δ(cid:48)  d)

consider 2 = 1/2

(cid:16)

  1
2
1

max

3
2

2
1

1

1

1

1

√

√

3
2

ε2

6

(cid:17)(cid:101)

(cid:16) max(12L2

1

3
2

√

12

10L2

+ 2

2 2L1)

else

3   δ(cid:48))
  2

1   K := (cid:100)1 + ∆
(cid:98)xk = AdaNCG(xk  3α/2
if (cid:107)∇f ((cid:98)xk)(cid:107) ≤ 1 then
Return(cid:98)xk
fk(x) = f (x) + L1 ([(cid:107)x −(cid:98)xk(cid:107) − 2/L2]+)2
xk+1 = Almost-Cvx-AGD(fj (cid:98)xk  1

Algorithm 5 AdaNCG+: (x0  1  α  δ)
1: 2 = α
2: δ(cid:48) := δ/K
3: for k = 1  2  . . .   do
4:
5:
6:
7:
8:
9:
end if
10:
11: end for
Algorithm 6 AdaNCD-SCSG: (x0  1  α  b  δ)
1: Input: x0  1  α  δ
2: for j = 1  2  . . .   do
3:
4:
5:
6:
7:
8:
end if
9:
10: end for

Generate three random sets S S1 S2
yj = SCSG-Epoch(xj S  b)
let g(yj) = ∇fS1 (x; ξ) satisfy (4)
(xj+1  vj) = AdaNCDmb(yj  α  δ S2  g(yj))
if v(cid:62)

j HS2 (yj)vj > −2/2 and (cid:107)g(yj)(cid:107) ≤ 1 then
Return yj

2   32  5L1)

d

(cid:17)

(cid:19)

the Al-

+ 1
12

(cid:20)(cid:18)

(cid:16) 1

let 2 = α

AdaNCD steps in AdaNCG and (cid:101)O

(cid:19)
1 . With probability at least 1 − δ 

6.1 Deterministic Objective
For deterministic objective  we consider the accelerated method proposed in [4]  which relies on
NCD to ﬁnd a point around which the objective function is almost convex and then switches to an
accelerated gradient method. We present an adaptive variant of [4]’s method in Algorithm 5  where
we use our AdaNCG in place of NCD. The procedure Almost-Convex-AGD is the same as in [4].
For completeness  we present it in the supplement. The convergence guarantee is presented below.
Theorem 3. For any α ∈ (0  1] 

gorithm AdaNCG+ returns a vector (cid:98)xk such that (cid:107)∇f ((cid:98)xk)(cid:107) ≤ 1 and λmin(∇2f ((cid:98)xk)) ≥
(cid:21)
(cid:18)(cid:18)

−2 with at most O
+ 1/2
2
2
1
gradient steps in Almost-Convex-AGD  and each step j within AdaNCG+ requires time of
Tn(f  max(2 (cid:107)∇f (xj)(cid:107)2/3)1/2  δ(cid:48)  d)  and the worse-case time complexity of AdaNCG+ is

(cid:19)
1  the worst-case time complexity of AdaNCG+ is (cid:101)O

(cid:101)O
(cid:19)
Remark: First  when 2 ≤ √
1 it reduces to (cid:101)O(d/7/4)  which matches the best time complexity in
Specially  for 2 =
same guarantee as the NCD in [4] (see Corollary 1 in the supplement)  i.e.  returning a solution(cid:98)xj
previous studies. Second  we note that the subroutine AdaNCG(xj  3α/2
satisfying λmin(∇2f ((cid:98)xj)) ≥ −2 with high probability. The number of iterations within AdaNCG
In particular  the number of Hessian-vector products of each NCD step in [4] is (cid:101)O(1/
becomes (cid:101)O(1/(cid:112)max(2 (cid:107)∇f (xj)(cid:107)2/3)) for each AdaNCD step in AdaNCG+. Finally  we note that

is similar to that in NCD employed by [4]  and the number of iterations within Almost-Convex-
AGD is similar to that in [4]. The improvement of AdaNCG+ over [4]’s algorithm is brought
by reducing the number of Hessian-vector products for performing each iteration of AdaNCG.
2)  which

when using the Lanczos method for NCS.

  2/3  δ(cid:48)) provides the

+ d1/2
2
2
1

+ d
7/2
2

+ d
7/2
2

+ 1

(cid:18)

13/2

2

13/2

2

1

7/2
2

13/2

2

d

.

3
2

√

AdaNCG+ has the same worse-case time complexity as AdaNCG for 2 ∈ [1  2/3
over AdaNCG+ for 2 ∈ [2/3

  1/2

].

1

1

1

]  but improves

1

√

7

6.2 Stochastic Objective
Next  we present a stochastic algorithm for tackling a stochastic objective f (x) = E[f (x; ξ)] in order
to achieve a state-of-the-art worse-case complexity for ﬁnding a second-order stationary point. We
consider combining the proposed AdaNCDmb with an existing stochastic variance reduced gradient
method for a stochastic objective  namely SCSG [12].
The detailed steps are shown in Algorithm 6  which is referred to as AdaNCD-SCSG and can be
considered as an improvement of Algorithm 4. The sub-routine SCSG-Epoch is one epoch of SCSG 
which is included in the supplement. It is worth mentioning that Algorithm 6 is based on the design
of [19] that also combined a NCD step with SCSG to prove the second-order convergence. The
difference from [19] is that they studied how to use a ﬁrst-order method without resorting to Hessian-
vector products to extract the negative curvature direction  while we focus on reducing the time
complexity of NCS using the proposed adaptive NCD. Our result below shows AdaNCD-SCSG has a
worst-case time complexity that matches the state-of-the-art time complexity for ﬁnding an (1 
1)
second-order stationary point.
Theorem 4. For any α ∈ (0  1]  let 2 = α

2 b1/2)))  |S1| =
2). With high probability  the Algorithm AdaNCD-SCSG returns a vector
calls of

1) and |S2| = (cid:101)O(1/2

1 . Suppose |S| = (cid:101)O(max(1/2
(cid:101)O(1/2
yj such that (cid:107)∇f (yj)(cid:107) ≤ 21 and λmin(∇2f (xj)) ≥ −22 with at most (cid:101)O
(cid:33)

SCSG-Epoch and AdaNCDmb.
Remark: The worst-case time complexity of AdaNCD-SCSG can be computed as

(cid:32)(cid:32)

1  1/(9/2

(cid:33)

+ 1
3
2

b1/3
4/3
1

(cid:18)

(cid:19)

√

(cid:101)O

b1/3
4/3
1

+

1
3
2

(|S|d + |S1|d + Tn(fS2  2  δ(cid:48)  d))
(cid:19)(cid:18)

(cid:18)

(cid:18)

.

If we consider using randomized algorithms as in Lemma 6 in supplement to implement NCS in
. Let

+

d

b1/3
4/3
1

+ 1
3
2

1
2
1

1
9/2
b1/2
2

+ 1
2
2

AdaNCDmb  the above time complexity reduces to (cid:101)O
(cid:101)O(d/3.5).

. By setting b = 1/1/2

us consider 2 = 1/2

1

1

  the worst-case time complexity of AdaNCD-SCSG is

(cid:19)(cid:19)

7 Empirical Studies
In this section  we report some experimental results to justify effectiveness of AdaNCD for both
deterministic and stochastic non-convex optimization. We consider three problems  namely  the
cubic regularization  regularized non-linear least-square  and one hidden-layer neural network (NN)
problem.
2  where A ∈ R1000×1000. For
The cubic regularization problem is: minw
deterministic optimization  we generate a diagonal A such that 100 randomly selected diagonal entry
is −1 and the rest diagonal entries follow uniform distribution between [1  2]  and set b as a zero
vector. For stochastic optimization  we let A = A(cid:48) +E[diag(ξ)] and b = E[ξ(cid:48)]  where A(cid:48) is generated
similarly  ξ are uniform random variables from [−0.1  0.1] and ξ(cid:48) are uniform random variables from
[−1  1]. The parameter ρ is set to 0.5 for both deterministic and stochastic experiments. It is clear
that zero is a saddle point of the problem. In order to test the capability of escaping from saddle point 
we let each algorithm start from a zero vector.

2 w(cid:62)Aw + b(cid:62)w + ρ

3(cid:107)w(cid:107)3

1

+(cid:80)d

(cid:80)n

(cid:0)yi − σ(w(cid:62)xi)(cid:1)2
(cid:80)n

1
n

i=1

The regularized non-linear least-square problem is: minw
 
where xi ∈ Rd  yi ∈ {0  1}  σ(s) = 1/(1 + exp(−s)) is a sigmoid function  and the second term is
a non-convex regularizer [15]  which is to increase the negative curvature of the problem. We use
w1a data (n = 2477  d = 300) from the libsvm website [8]  and set λ = 1.
Learning a NN with one hidden layer is imposed as: minw
i=1 (cid:96)(W2σ(W1xi + b1) + b2  yi) 
where xi ∈ Rd  yi ∈ {1 −1} are input data  W1  W2  b1  b2 are parameters of the NN with appro-
priate dimensions  and (cid:96)(z  y) is cross-entropy loss. We use 12  665 examples from the MNIST
dataset [11] that belong to two categories 0 and 1 as input data  where the input feature dimensionality
is 784. The number of neurons in hidden layer is set to 10 so that the total number of parameters
including bias terms is 7872.

1+αw2
i

i=1

1
n

λw2
i

8

Figure 1: Comparison of different deterministic algorithms (upper) and stochastic algorithms (lower)
for solving cubic regularization  regularized nonlinear least square  and neural network (from left to
right).
For deterministic experiments  we compare AdaNCG  AdaNCG+ with their non-adaptive counter-
parts. In particular  the non-adaptive counterpart of AdaNCG named NCG uses NCS(f  x  2/2  δ).
The non-adaptive counterpart of AdaNCG+ is the algorithm proposed in [4]  which is referred to
as NCD-AG. For stochastic experiments  we compare S-AdaNCG  AdaNCD-SCSG  and the non-
adaptive version of AdaNCD-SCSG named NCD-SCSG. For all experiments  we choose α = 1
2  i.e. 
1. The parameters L1  L2 are tuned for the non-adaptive algorithm NCG  and the same values
2 =
are used in other algorithms. The searching range for L1 and L2 are from 10−5:1:5. The mini-batch
size used in S-AdaNCG and AdaNCD-SCSG is set to 50 for cubic regularization and 128 for other two
√
tasks. We use the Lanczos method for NCS. For non-adaptive algorithms  the number of iterations in
each call of the Lanczos method is set to min(C log(d)/
2  d); and for adaptive algorithms  the num-

ber of iterations in each call of the Lanczos method is set to min(C log(d)/(cid:112)max(2 (cid:107)g(x)(cid:107)1/2)  d) 

√

√

where g(x) is either a full gradient or a mini-batch stochastic gradient. The value of C is set to
L1.
We set 1 = 10−2 for cubic regularization  and 1 = 10−4 for other two tasks. We report the objective
value v.s. the number of oracle calls (including gradient evaluations and Hessian-vector productions)
in Figure 1. From deterministic optimization results  we can see that the AdaNCD can greatly
improve the convergence of AdaNCG and AdaNCG+ compared to their non-adaptive counterparts. In
addition  AdaNCG performs better than AdaNCG+ on the tested tasks. The reason is that AdaNCG
can guarantee the decrease of the objective values at every iteration  while AdaNCG+ that uses the
AG method to optimize an almost convex functions does not have such guarantee. From stochastic
optimization results  AdaNCD also makes AdaNCD-SCSG converge faster than its non-adaptive
counterpart NCD-SCSG. In addition  AdaNCD-SCSG is faster than S-AdaNCG. Finally  we note
that the ﬁnal solution found by the proposed algorithms satisfy the prescribed optimality condition.
For example  on the solution found by AdaNCG on the cubic regularization problem the gradient
norm is 0.0085 and the minimum eigen-value of the Hessian is −0.0043.
8 Conclusion
In this paper  we have developed several variants of adaptive negative curvature descent step that
employ a noisy negative curvature direction for non-convex optimization. The novelty of the
proposed algorithms lie at that the noise level in approximating the negative curvature is adaptive
to the magnitude of the current gradient instead of a prescribed small noise level  which could
dramatically reduce the number of Hessian-vector products. Building on the adaptive negative
curvature descent step  we have developed several deterministic and stochastic algorithms and
established their complexities. The effectiveness of adaptive negative curvature descent is also
demonstrated by empirical studies.

Acknowledgments

We thank the anonymous reviewers for their helpful comments. M. Liu  T. Yang are partially
supported by National Science Foundation (IIS-1545995).

9

0200400600800100012001400Number of oracle calls-0.7-0.6-0.5-0.4-0.3-0.2-0.10objectiveNCD-AGAdaNCG+NCGAdaNCG00.511.522.53Number of oracle calls×1040.230.2350.240.2450.25ObjectiveNCD-AGAdaNCG+NCGAdaNCG0246810Number of oracle calls×10411.522.533.544.55ObjectiveNCD-AGAdaNCG+NCGAdaNCGNumber of oracle calls×10402468Objective-0.7-0.6-0.5-0.4-0.3-0.2-0.10AdaNCD-SCSGNCD-SCSGS-AdaNCGNumber of oracle calls×10400.511.522.533.5Objective0.2320.2340.2360.2380.240.2420.2440.2460.2480.25AdaNCD-SCSGNCD-SCSGS-AdaNCGNumber of oracle calls×105012345Objective11.522.533.544.55AdaNCD-SCSGNCD-SCSGS-AdaNCGReferences
[1] Naman Agarwal  Zeyuan Allen Zhu  Brian Bullins  Elad Hazan  and Tengyu Ma. Finding approximate
local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on
Theory of Computing (STOC)  pages 1195–1199  2017.

[2] Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. CoRR  /abs/1708.08694  2017.

[3] Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via ﬁrst-order oracles. CoRR 

abs/1711.06673  2017.

[4] Yair Carmon  John C. Duchi  Oliver Hinder  and Aaron Sidford. Accelerated methods for non-convex

optimization. CoRR  abs/1611.00756  2016.

[5] Coralia Cartis  Nicholas I. M. Gould  and Philippe L. Toint. Adaptive cubic regularisation methods
for unconstrained optimization. part i: motivation  convergence and numerical results. Mathematical
Programming  127(2):245–295  Apr 2011.

[6] Coralia Cartis  Nicholas I. M. Gould  and Philippe L. Toint. Adaptive cubic regularisation methods for un-
constrained optimization. part ii: worst-case function- and derivative-evaluation complexity. Mathematical
Programming  130(2):295–319  Dec 2011.

[7] Frank E. Curtis and Daniel P. Robinson. Exploiting negative curvature in deterministic and stochastic

optimization. CoRR  abs/1703.00412  2017.

[8] Rong-En Fan and Chih-Jen Lin. Libsvm data: Classiﬁcation  regression and multi-label. URL: http://www.

csie. ntu. edu. tw/cjlin/libsvmtools/datasets  2011.

[9] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points — online stochastic
gradient for tensor decomposition. In Peter Grünwald  Elad Hazan  and Satyen Kale  editors  Proceedings
of The 28th Conference on Learning Theory (COLT)  volume 40  pages 797–842. PMLR  03–06 Jul 2015.

[10] Christopher J. Hillar and Lek-Heng Lim. Most tensor problems are np-hard. J. ACM  60(6):45:1–45:39 

November 2013.

[11] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[12] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Non-convex ﬁnite-sum optimization via scsg

methods. In Advances in Neural Information Processing Systems  pages 2345–2355  2017.

[13] A. S. Nemirovsky and D. B. Yudin. Problem Complexity and Method Efﬁciency in Optimization. A

Wiley-Interscience publication. Wiley  1983.

[14] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance.

Mathematical Programming  108(1):177–205  2006.

[15] Sashank J Reddi  Suvrit Sra  Barnabás Póczos  and Alex Smola. Fast incremental method for smooth
nonconvex optimization. In Decision and Control (CDC)  2016 IEEE 55th Conference on  pages 1971–
1977. IEEE  2016.

[16] Sashank J. Reddi  Manzil Zaheer  Suvrit Sra  Barnabás Póczos  Francis R. Bach  Ruslan Salakhutdinov 

and Alexander J. Smola. A generic approach for escaping saddle points. CoRR  abs/1709.01434  2017.

[17] Clément W. Royer and Stephen J. Wright. Complexity analysis of second-order line-search algorithms for

smooth nonconvex optimization. CoRR  abs/1706.03131  2017.

[18] Peng Xu  Farbod Roosta-Khorasani  and Michael W. Mahoney. Newton-type methods for non-convex

optimization under inexact hessian information. CoRR  abs/1708.07164  2017.

[19] Yi Xu  Rong Jin  and Tianbao Yang. First-order stochastic algorithms for escaping from saddle points in

almost linear time. CoRR  abs/1711.01944  2017.

[20] Yuchen Zhang  Percy Liang  and Moses Charikar. A hitting time analysis of stochastic gradient langevin
dynamics. In Proceedings of the 30th Conference on Learning Theory (COLT)  pages 1980–2022  2017.

10

,Mingrui Liu
Zhe Li
Xiaoyu Wang
Jinfeng Yi
Tianbao Yang