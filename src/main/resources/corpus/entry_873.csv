2019,Budgeted Reinforcement Learning in Continuous State Space,A Budgeted Markov Decision Process (BMDP) is an extension of a Markov Decision Process to critical applications requiring safety constraints. It relies on a notion of risk implemented in the shape of an upper bound on a constrains violation signal that -- importantly -- can be modified in real-time. So far  BMDPs could only be solved in the case of finite state spaces with known dynamics. This work extends the state-of-the-art to continuous spaces environments and unknown dynamics. We show that the solution to a BMDP is the fixed point of a novel Budgeted Bellman Optimality operator. This observation allows us to introduce natural extensions of Deep Reinforcement Learning algorithms to address large-scale BMDPs. We validate our approach on two simulated applications: spoken dialogue and autonomous driving.,Budgeted Reinforcement Learning in Continuous

State Space

Nicolas Carrara∗

SequeL team  INRIA Lille – Nord Europe†

nicolas.carrara@inria.fr

Romain Laroche

Microsoft Research  Montreal  Canada

romain.laroche@microsoft.com

Odalric-Ambrym Maillard

SequeL team  INRIA Lille – Nord Europe

odalric.maillard@inria.fr

Edouard Leurent∗

SequeL team  INRIA Lille – Nord Europe†

Renault Group  France

edouard.leurent@inria.fr

Tanguy Urvoy

Orange Labs  Lannion  France

tanguy.urvoy@orange.com

Olivier Pietquin

Google Research - Brain Team

SequeL team  INRIA Lille – Nord Europe†

pietquin@google.com

Abstract

A Budgeted Markov Decision Process (BMDP) is an extension of a Markov
Decision Process to critical applications requiring safety constraints. It relies on a
notion of risk implemented in the shape of a cost signal constrained to lie below
an – adjustable – threshold. So far  BMDPs could only be solved in the case of
ﬁnite state spaces with known dynamics. This work extends the state-of-the-art
to continuous spaces environments and unknown dynamics. We show that the
solution to a BMDP is a ﬁxed point of a novel Budgeted Bellman Optimality
operator. This observation allows us to introduce natural extensions of Deep
Reinforcement Learning algorithms to address large-scale BMDPs. We validate
our approach on two simulated applications: spoken dialogue and autonomous
driving3.

1

Introduction

r =(cid:80)∞

Reinforcement Learning (RL) is a general framework for decision-making under uncertainty. It
frames the learning objective as the optimal control of a Markov Decision Process (S A  P  Rr  γ)
with measurable state space S  discrete actions A  unknown rewards Rr ∈ RS×A  and unknown
dynamics P ∈ M(S)S×A   where M(X ) denotes the probability measures over a set X . Formally 
we seek a policy π ∈ M(A)S that maximises in expectation the γ-discounted return of rewards
Gπ
However  this modelling assumption comes at a price: no control is given over the spread of the
performance distribution (Dann et al.  2019). In many critical real-world applications where failures
may turn out very costly  this is an issue as most decision-makers would rather give away some
amount of expected optimality to increase the performances in the lower-tail of the distribution. This

t=0 γtRr(st  at).

∗Both authors contributed equally.
†Univ. Lille  CNRS  Centrale Lille  INRIA UMR 9189 - CRIStAL  Lille  France
3Videos and code are available at https://budgeted-rl.github.io/.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

has led to the development of several risk-averse variants where the optimisation criteria include
other statistics of the performance  such as the worst-case realisation (Iyengar  2005; Nilim and El
Ghaoui  2005; Wiesemann et al.  2013)  the variance-penalised expectation (García and Fernández 
2015; Tamar et al.  2012)  the Value-At-Risk (VaR) (Mausser and Rosen  2003; Luenberger  2013) 
or the Conditional Value-At-Risk (CVaR) (Chow et al.  2015  2018).
Reinforcement Learning also assumes that the performance can be described by a single reward
function Rr. Conversely  real problems typically involve many aspects  some of which can be
contradictory (Liu et al.  2014). For instance  a self-driving car needs to balance between progressing
quickly on the road and avoiding collisions. When aggregating several objectives in a single scalar
signal  as often in Multi-Objectives RL (Roijers et al.  2013)  no control is given over their relative
ratios  as high rewards can compensate high penalties. For instance  if a weighted sum is used to
balance velocity v and crashes c  then for any given choice of weights ω the optimality equation
and the automotive company cannot control where its optimal policy π∗ lies on that line.
Both of these concerns can be addressed in the Constrained Markov Decision Process (CMDP)
setting (Beutler and Ross  1985; Altman  1999). In this multi-objective formulation  task completion
and safety are considered separately. We equip the MDP with a cost signal Rc ∈ RS×A and a cost
budget β ∈ R. Similarly to Gπ
t=0 γtRc(st  at) and the new
cost-constrained objective:

r is the equation of a line in (E[(cid:80) γtvt]  E[(cid:80) γtct]) 

ωv E[(cid:80) γtvt]+ωa E[(cid:80) γtct] = G∗

r   we deﬁne the return of costs Gπ

c =(cid:80)∞

r = maxπ Gπ

max

π∈M(A)S

E[Gπ

r|s0 = s]

s.t. E[Gπ

c |s0 = s] ≤ β

(1)

This constrained framework allows for better control of the performance-safety tradeoff. However  it
suffers from a major limitation: the budget has to be chosen before training  and cannot be changed
afterwards.
To address this concern  the Budgeted Markov Decision Process (BMDP) was introduced in (Boutilier
and Lu  2016) as an extension of CMDPs to enable the online control over the budget β within an
interval B ⊂ R of admissible budgets. Instead of ﬁxing the budget prior to training  the objective is
now to ﬁnd a generic optimal policy π∗ that takes β as input so as to solve the corresponding CMDP
(Eq. (1)) for all β ∈ B. This gives the system designer the ability to move the optimal policy π∗ in
real-time along the Pareto-optimal curve of the different reward-cost trade-offs.
Our ﬁrst contribution is to re-frame the original BMDP formulation in the context of continuous states
and inﬁnite discounted horizon. We then propose a novel Budgeted Bellman Optimality Operator and
prove the optimal value function to be a ﬁxed point of this operator. Second  we use this operator in
BFTQ  a batch Reinforcement Learning algorithm  for solving BMDPs online by interaction with
an environment  through function approximation and a tailored exploration procedure. Third  we
scale this algorithm to large problems by providing an efﬁcient implementation of the Budgeted
Bellman Optimality Operator based on convex programming  a risk-sensitive exploration procedure 
and by leveraging tools from Deep Reinforcement Learning such as Deep Neural Networks and
synchronous parallel computing. Finally  we validate our approach in two environments that display
a clear trade-off between rewards and costs: a spoken dialogue system and a problem of behaviour
planning for autonomous driving. The proofs of our main results are provided in Appendix A.

2 Budgeted Dynamic Programming

We work in the space of budgeted policies  where a policy π both depends on the current budget β
and also outputs a next budget βa. Hence  the budget β is neither ﬁxed nor constant as in the CMDP
setting but instead evolves as part of the dynamics.
We cast the BMDP problem as a multi-objective MDP problem (Roijers et al.  2013) by considering
augmented state and action spaces S = S × B and A = A × B  and equip them with the augmented
dynamics P ∈ M(S)S×A deﬁned as:
(cid:48) | s  a) = P ((s
(cid:48)

) | (s  β)  (a  βa))

(cid:48)|s  a)δ(β

(cid:48) − βa) 

def= P (s

P (s

(cid:48)

  β

(2)

where δ is the Dirac indicator distribution.

2

In other words  in these augmented dynamics  the output budget βa returned at time t by a budgeted
policy π ∈ Π = M(A)S will be used to condition the policy at the next timestep t + 1.
We stack the rewards and cost functions in a single vectorial signal R ∈ (R2)S×A. Given an
augmented transition (s  a) = ((s  β)  (a  βa))  we deﬁne:

(cid:20)Rr(s  a)

(cid:21)

def=

∈ R2.

Likewise  the return Gπ = (Gπ
and the value functions V π  Qπ of a budgeted policy π ∈ Π are deﬁned as:

r   Gπ

t=0 γtR(st  at) 

R(s  a)

Rc(s  a)

c ) of a budgeted policy π ∈ Π refers to: Gπ def=(cid:80)∞

(3)

r   V π
c )

V π(s) = (V π

Qπ(s  a) = (Qπ

def= E [Gπ | s0 = s]
def= E [Gπ | s0 = s  a0 = a] .
(4)
c (s) ≤ β} that we assume is
We restrict S to feasible budgets only: S f
non-empty for the BMDP to admit a solution. We still write S in place of S f for brevity of notations.
(cid:88)
Proposition 1 (Budgeted Bellman Expectation). The value functions V π and Qπ verify:
(cid:48) | s  a) V π(s
(cid:48)

def={(s  β) ∈ S : ∃π ∈ Π  V π

Qπ(s  a) = R(s  a) + γ

π(a|s)Qπ(s  a)

(cid:88)

V π(s) =

r   Qπ
c )

P (s

(5)

)

a∈A

s(cid:48)∈S

(cid:88)

(cid:88)

T πQ(s  a)

def= R(s  a) + γ

Moreover  consider the Budgeted Bellman Expectation operator T π: ∀Q ∈ (R2)SA  s ∈ S  a ∈ A 
(6)

P (s
Then T π is a γ-contraction and Qπ is its unique ﬁxed point.
Deﬁnition 1 (Budgeted Optimality). We now come to the deﬁnition of budgeted optimality. We want
an optimal budgeted policy to: (i) respect the cost budget β  (ii) maximise the γ-discounted return of
rewards Gr  (iii) in case of tie  minimise the γ-discounted return of costs Gc. To that end  we deﬁne
for all s ∈ S:
(i) Admissible policies Πa:

(cid:48)|s  a)π(a

(cid:48)
)Q(s

a(cid:48)∈A

(cid:48)|s
(cid:48)

(cid:48)
  a

s(cid:48)∈S

)

Πa(s)

def={π ∈ Π : V π

c (s) ≤ β} where s = (s  β)

r and candidate policies Πr:

(ii) Optimal value function for rewards V ∗
V π
r (s)

def= max
π∈Πa(s)
c and optimal policies Π∗:
(iii) Optimal value function for costs V ∗
V π
c (s) 

∗
c (s)

∗
r (s)

Πr(s)

(s)

Π

V

V

∗

def= arg max
π∈Πa(s)

def= min
π∈Πr(s)

def= arg min
π∈Πr(s)

V π
r (s)

V π
c (s)

(7)

(8)

(9)

We deﬁne the budgeted action-value function Q∗ similarly:
∗
Q
c (s  a)

∗
r(s  a)
Q
and denote V ∗ = (V ∗
r   V ∗
Theorem 1 (Budgeted Bellman Optimality). The optimal budgeted action-value function Q∗ veriﬁes:
(11)

def= max
Qπ
π∈Πa(s)
c )  Q∗ = (Q∗

r (s  a)
r  Q∗
c ).

πgreedy(a(cid:48)|s(cid:48); Q
∗

(s  a) = T Q
∗

def= min
π∈Πr(s)

def= R(s  a) + γ

P (s(cid:48)|s  a)

(cid:88)

(cid:88)

(s(cid:48)  a(cid:48)) 

c (s  a)

∗
)Q

(s  a)

∗
Q

(10)

Qπ

where the greedy policy πgreedy is deﬁned by: ∀s = (s  β) ∈ S  a ∈ A ∀Q ∈ (R2)A×S  

s(cid:48)∈S

a(cid:48)∈A

πgreedy(a|s; Q) ∈ arg min
ρ∈ΠQ

r

E
a∼ρ

Qc(s  a) 

where ΠQ
r

def= arg max
ρ∈M(A)
s.t. E
a∼ρ

E
Qr(s  a)
a∼ρ
Qc(s  a) ≤ β.

3

(12a)

(12b)

(12c)

Remark 1 (Appearance of the greedy policy). In classical Reinforcement Learning  the greedy policy
takes a simple form πgreedy(s; Q∗) = arg maxa∈A Q∗(s  a)  and the term πgreedy(a(cid:48)|s(cid:48); Q∗)Q∗(s(cid:48)  a(cid:48))
in (11) conveniently simpliﬁes to maxa(cid:48)∈A Q∗(s(cid:48)  a(cid:48)). Unfortunately  in a budgeted setting the greedy
policy requires solving the nested constrained optimisation program (12) at each state and budget in
order to apply this Budgeted Bellman Optimality operator.
Proposition 2 (Optimality of the greedy policy). The greedy policy πgreedy(· ; Q∗) is uniformly
optimal: ∀s ∈ S  πgreedy(· ; Q∗) ∈ Π∗(s). In particular  V πgreedy(·;Q∗) = V ∗ and Qπgreedy(·;Q∗) = Q∗.
Budgeted Value Iteration The Budgeted Bellman Optimality equation is a ﬁxed-point equation 
which motivates the introduction of a ﬁxed-point iteration procedure. We introduce Algorithm 1 
a Dynamic Programming algorithm for solving known BMDPs. If it were to converge to a unique
ﬁxed point  this algorithm would provide a way to compute Q∗ and recover the associated optimal
budgeted policy πgreedy(· ; Q∗).
Theorem 2 (Non-contractivity of T ). For any BMDP (S A  P  Rr  Rc  γ) with |A| ≥ 2  T is not a
contraction. Precisely: ∀ε > 0 ∃Q1  Q2 ∈ (R2)SA : (cid:107)T Q1 − T Q2(cid:107)∞ ≥ 1
Unfortunately  as T is not a contraction  we can guarantee neither the convergence of Algorithm 1
nor the unicity of its ﬁxed points. Despite those theoretical limitations  we empirically observed the
convergence to a ﬁxed point in our experiments (Section 5). We conjecture a possible explanation:
Theorem 3 (Contractivity of T on smooth Q-functions). The operator T is a contraction when
restricted to the subset Lγ of Q-functions such that "Qr is Lipschitz with respect to Qc":

ε(cid:107)Q1 − Q2(cid:107)∞.

Lγ =

Q ∈ (R2)SA s.t. ∃L < 1
γ − 1 : ∀s ∈ S  a1  a2 ∈ A 
|Qr(s  a1) − Qr(s  a2)| ≤ L|Qc(s  a1) − Qc(s  a2)|

(cid:26)

(cid:27)

(13)

Thus  we expect that Algorithm 1 is likely to converge when Q∗ is smooth  but could diverge if the
slope of Q∗ is too high. L2-regularisation can be used to encourage smoothness and mitigate risk of
divergence.

3 Budgeted Reinforcement Learning

In this section  we consider BMDPs with unknown parameters that must be solved by interaction
with an environment.

3.1 Budgeted Fitted-Q

When the BMDP is unknown  we need to adapt Algorithm 1 to work with a batch of samples
i)}i∈[1 N ] collected by interaction with the environment. Applying T in (11)
D = {(si  ai  ri  s(cid:48)
s(cid:48)∼P over next states s(cid:48) and hence an access to the model
would require computing an expectation E
P . We instead use ˆT   a sampling operator  in which this expectation is replaced by:

ˆT Q(s  a  r  s
(cid:48)

def= r + γ

)

πgreedy(a(cid:48)|s(cid:48); Q)Q(s(cid:48)  a(cid:48)).

(cid:88)

a(cid:48)∈A

We introduce in Algorithm 2 the Budgeted-Fitted-Q (BFTQ) algorithm  an extension of the Fitted-Q
(FTQ) algorithm (Ernst et al.  2005; Riedmiller  2005) adapted to solve unknown BMDPs. Because we
work with continuous state space S and budget space B  we need to employ function-approximation
in order to generalise to nearby states and budgets. Precisely  given a parametrized model Qθ  we
2. Any

seek to minimise a regression loss L(Qθ  Qtarget;D) =(cid:80)D ||Qθ(s  a) − Qtarget(s  a  r  s(cid:48))||2

model can be used  such as linear models  regression trees  or neural networks.

Algorithm 1: Budgeted Value Iteration
Data: P  Rr  Rc
Result: Q∗
1 Q0 ← 0
2 repeat
Qk+1 ← T Qk
3
4 until convergence

Algorithm 2: Budgeted Fitted-Q
Data: D
Result: Q∗
1 Qθ0 ← 0
2 repeat
3
4 until convergence

θk+1 ← arg minθ L(Qθ  ˆT Qθk ;D)

4

3.2 Risk-sensitive exploration
In order to run Algorithm 2  we must ﬁrst gather a batch of samples D. The following strategy is
motivated by the intuition that a wide variety of risk levels needs to be experienced during training 
which can be achieved by enforcing the risk constraints during data collection. Ideally we would
need samples from the asymptotic state-budget distribution limt→∞ P (st) induced by an optimal
policy π∗ given an initial distribution P (s0)  but as we are actually building this policy  it is not
possible. Following the same idea of ε-greedy exploration for FTQ (Ernst et al.  2005; Riedmiller 
2005)  we introduce an algorithm for risk-sensitive exploration. We follow an exploration policy: a
mixture between a random budgeted policy πrand and the current greedy policy πgreedy. The batch D is
split into several mini-batches generated sequentially  and πgreedy is updated by running Algorithm 2
on D upon mini-batch completion. πrand should only pick augmented actions that are admissible
candidates for πgreedy. To that extent πrand is designed to obtain trajectories that only explore feasible
budgets: we impose that the joint distribution P (a  βa|s  β) veriﬁes E[βa] ≤ β. This condition
deﬁnes a probability simplex ∆A from which we sample uniformly. Finally  when interacting with an
environment the initial state s0 is usually sampled from a starting distribution P (s0). In the budgeted
setting  we also need to sample the initial budget β0. Importantly  we pick a uniform distribution
P (β0) = U(B) so that the entire range of risk-level is explored  and not only reward-seeking
behaviours as would be the case with a traditional risk-neutral ε-greedy strategy. The pseudo-code of
our exploration procedure is shown in Algorithm 3.

Algorithm 3: Risk-sensitive exploration
Data: An environment  a BFTQ solver  W CPU workers
Result: A batch of transitions D
1 D ← ∅
2 for each intermediate batch do
3
4

split episodes between W workers
for each episode in batch do
parallel

// run this loop on each worker in

sample initial budget β ∼ U(B).
while episode not done do
update ε from schedule.
sample z ∼ U([0  1]).
if z < ε then sample (a  βa) ∼ U(∆AB).
else sample (a  βa) ∼ πgreedy(a  βa|s  β; Q∗).
append transition (s  β  a  βa  R  C  s(cid:48)) to batch D.
step episode budget β ← βa

5
6
7
8
9
10
11
12
13
end
14
πgreedy(· ∼; Q∗) ← BFTQ(D).
15
16 end
17 return the batch of transitions D

end

// Explore
// Exploit

4 A Scalable Implementation

In this section  we introduce an implementation of the BFTQ algorithm designed to operate efﬁciently
and handle large batches of experiences D.

4.1 How to compute the greedy policy?

As stated in Remark 1  computing the greedy policy πgreedy in (11) is not trivial since it requires
solving the nested constrained optimisation program (12). However  it can be solved efﬁciently by
exploiting the structure of the set of solutions with respect to β  that is  concave and increasing.
Proposition 3 (Equality of πgreedy and πhull). Algorithm 1 and Algorithm 2 can be run by replacing
πgreedy in the equation (11) of T with πhull as described in Algorithm 4.

πgreedy(a|s; Q) = πhull(a|s; Q)

5

Algorithm 4: Convex hull policy πhull(a|s; Q)
Data: s = (s  β)  Q
1 Q+ ← {Qc > min{Qc(s  a) s.t. a ∈ arg maxa Qr(s  a)}}
2 F ← top frontier of convex_hull(Q(s A) \ Q+)
3 FQ ← F ∩ Q(s A)
4 for points q = Q(s  a) ∈ FQ in clockwise order do

p ← (β − q1
return the mixture (1 − p)δ(a − a1) + pδ(a − a2)

if ﬁnd two successive points ((q1

5
6
7
8 end
9 return δ(a − arg maxa Qr(s  a))

c − q1
c )

c )/(q2

c   q2

c   q1

r )  (q2

// dominated points

// candidate mixtures

r )) of FQ such that q1

c ≤ β < q2

c then

// budget β always respected

Figure 1: Representation of πhull. When the budget lies between
Q(s  a1) and Q(s  a2)  two points of the top frontier of the convex
hull  then the policy is a mixture of these two points.

The computation of πhull
in Algo-
rithm 4 is illustrated in Figure 1: ﬁrst
we get rid of dominated points. Then
we compute the top frontier of the
convex hull of the Q-function. Next 
we ﬁnd the two closest augmented ac-
tions a1 and a2 with cost-value Qc
surrounding β: Qc(s  a1) ≤ β <
Qc(s  a2). Finally  we mix the two ac-
tions such that the expected spent bud-
get is equal to β. Because of the con-
cavity of the convex hull top frontier 
any other combination of augmented
actions would lead to a lower expected
reward Qr.

4.2 Function approximation

Neural networks are well suited to model Q-functions in Reinforcement Learning algorithms (Ried-
miller  2005; Mnih et al.  2015). We approximate Q = (Qr  Qc) using one single neural network.
Thus  the two components are jointly optimised which accelerates convergence and fosters learning
of useful shared representations. Moreover  as in (Mnih et al.  2015) we are dealing with a ﬁnite
(categorical) action space A  instead of including the action in the input we add the output of the
Q-function for each action to the last layer. Again  it provides a faster convergence toward useful
shared representations and it only requires one forward pass to evaluate all action values. Finally 
beside the state s there is one more input to a budgeted Q-function: the budget βa. This budget is a
scalar value whereas the state s is a vector of potentially large size. To avoid a weak inﬂuence of β
compared to s in the prediction  we include an additional encoder for the budget  whose width and
depth may depend on the application. A straightforward choice is a single layer with the same width
as the state. The overall architecture is shown in Figure 7 in Appendix B.

4.3 Parallel computing

In a simulated environment  a ﬁrst process that can be distributed is the collection of samples in
the exploration procedure of Algorithm 3  as πgreedy stays constant within each mini-batch which
avoids the need of synchronisation between workers. Second  the main bottleneck of BFTQ is the
computation of the target T Q. Indeed  when computing πhull we must perform at each epoch a

Graham-scan of complexity O(|A||(cid:101)B| log |A(cid:101)B|) per sample in D to compute the convex hulls of Q
(where (cid:101)B is a ﬁnite discretisation of B). The resulting total time-complexity is O(
log |A||(cid:101)B|).
model Q(s(cid:48) A(cid:101)B) for each sample s(cid:48) ∈ D  which can be done in a single forward pass. By using

This operation can easily be distributed over several CPUs provided that we ﬁrst evaluate the

multiprocessing in the computations of πhull  we enjoy a linear speedup. The full description of our
scalable implementation of BFTQ is recalled in Algorithm 5 in Appendix B.

|D||A||(cid:101)B|

1−γ

6

Q( )s⎯⎯a⎯⎯⎯2Q( )s⎯⎯a⎯⎯⎯1βQrQcQ+QQ( )s⎯⎯⎯⎯⎯⎯⎯5 Experiments

There are two hypotheses we want to validate.

Exploration strategies We claimed in Section 3.2 that a risk-sensitive exploration was required in
the setting of BMDPs. We test this hypotheses by confronting our strategy to a classical risk-neutral
strategy. The latter is chosen to be a ε-greedy policy slowly transitioning from a random to a greedy
c . The quality of the resulting batches D is
policy4 that aims to maximise Eπ Gπ
assessed by training a BFTQ policy and comparing the resulting performance.

r regardless of Eπ Gπ

Budgeted algorithms We compare our scalable BFTQ algorithm described in Section 4 to an
FTQ(λ) baseline. This baseline consists in approximating the BMDP by a ﬁnite set of CMDPs
problems. We solve each of these CMDP using the standard technique of Lagrangian Relaxation: the
cost constraint is converted to a soft penalty weighted by a Lagrangian multiplier λ in a surrogate
reward function: maxπ Eπ[Gπ
c ]. The resulting MDP can be solved by any RL algorithm  and
we chose FTQ for being closest to BFTQ. In our experiments  a single training of BFTQ corresponds
to 10 trainings of FTQ(λ) policies. Each run was repeated Nseeds times. Parameters of the algorithms
can be found in Appendix D.3.1

r − λGπ

5.1 Environments

We evaluate our method on three different environments involving reward-cost trade-offs. Their
parameters can be found in Appendix D.3.2

Corridors This simple environment is only meant to highlight clearly the speciﬁcity of exploration
in a budgeted setting. It is a continuous gridworld with Gaussian perturbations  consisting in a maze
composed of two corridors: a risky one with high rewards and costs  and a safe one with low rewards
and no cost. In both corridors the outermost cell is the one yielding the most reward  which motivates
a deep exploration.

Spoken dialogue system Our second application is a dialogue-based slot-ﬁlling simulation that
has already beneﬁted from batch RL optimisation in the past (Li et al.  2009; Chandramohan et al. 
2010; Pietquin et al.  2011). The system ﬁlls in a form of slot-values by interacting a user through
speech  before sending them a response. For example  in a restaurant reservation domain  it may
ask for three slots: the area of the restaurant  the price-range and the food type. The user could
respectively provide those three slot-values : Cambridge  Cheap and Indian-food. In this
application  we do not focus on how to extract such information from the user utterances  we rather
focus on decision-making for ﬁlling in the form. To that end  the system can choose among a set of
generic actions. As in (Carrara et al.  2018)  there are two ways of asking for a slot value: a slot value
can be either be provided with an utterance  which may cause speech recognition errors with some
probability  or by requiring the user to ﬁll-in the slots by using a numeric pad. In this case  there are
no recognition errors but a counterpart risk of hang-up: we assume that manually ﬁlling a key-value
form is time-consuming and annoying. The environment yields a reward if all slots are ﬁlled without
errors  and a constraint if the user hang-ups. Thus  there is a clear trade-off between using utterances
and potentially committing a mistake  or using the numeric pad and risking a premature hang-up.

Autonomous driving
In our third application  we use the highway-env environment (Leurent 
2018) for simulated highway driving and behavioural decision-making. We deﬁne a task that displays
a clear trade-off between safety and efﬁciency. The agent controls a vehicle with a ﬁnite set of
manoeuvres implemented by low-lever controllers: A = {no-op  right-lane  left-lane  faster  slower}.
It is driving on a two-lane road populated with other trafﬁc participants: the vehicles in front of
the agent drive slowly  and there are incoming vehicles on the opposite lane. Their behaviours are
randomised  which introduces some uncertainty with respect to their possible future trajectories. The
task consists in driving as fast as possible  which is modelled by a reward proportional to the velocity:
Rr(st  at) ∝ vt. This motivates the agent to try and overtake its preceding vehicles by driving
fast on the opposite lane. This optimal but overly aggressive behaviour can be tempered through a
cost function that embodies a safety objective: Rc(st  at) is set to 1/H whenever the ego-vehicle is

4We train this greedy policy using FTQ.

7

Figure 2: Density of explored states (left) and corresponding policy performances (right) of two exploration
strategies in the corridors environment.

Figure 3: Performance comparison of FTQ(λ) and BFTQ on slot-ﬁlling (left) and highway-env(right)

driving on the opposite lane  where H is the episode horizon. Thus  the constrained signal Gπ
maximum proportion of time that the agent is allowed to drive on the wrong side of the road.

c is the

5.2 Results

r   Gπ

In the following ﬁgures  each patch represents the mean and 95% conﬁdence interval over Nseeds
seeds of the means of (Gπ
c ) over Ntrajs trajectories. That way  we display the variation related to
learning (and batches) rather than the variation in the execution of the policies.
We ﬁrst bring to light the role of risk-sensitive exploration in the corridors environment: Figure 2
shows the set of trajectories collected by each exploration strategy. and the resulting performance
of a budgeted policy trained on each batch. The trajectories (orange) in the risk-neutral batch are
concentrated along the risky corridor (right) and ignore the safe corridor (left)  which results in
bad performances in the low-risk regime. Conversely  trajectories in the risk-sensitive batch (blue)
are well distributed among both corridors and the corresponding budgeted policy achieves good
performance across the whole spectrum of risk budgets.
In a second experiment displayed in Figure 3  we compare the performance of FTQ(λ) to that of
BFTQ in the dialogue and autonomous driving tasks. For each algorithm  we plot the reward-cost
trade-off curve. In both cases  BFTQ performs almost as well as FTQ(λ) despite only requiring a
single model. All budgets are well-respected on slot-ﬁlling  but on highway-env we can observe an
underestimation of Qc  since e.g. E[Gc|β = 0] (cid:39) 0.1. This underestimation can be a consequence
of two approximations: the use of the sampling operator ˆT instead of the true population operator
T   and the use of the neural network function approximation Qθ instead of Q. Still  BFTQ provides
a better control on the expected cost of the policy  than FTQ(λ). In addition  BFTQ behaves more
consistently than FTQ(λ) overall  as shown by its lower extra-seed variance.

8

BFTQ(risk-sensitive)BFTQ(risk-neutral)GπrGπcGπrGπcGπrGπcAdditional material such as videos of policy executions is provided in Appendix D.

6 Discussion

Algorithm 2 is an algorithm for solving large unknown BMDPs with continuous states. To the best of
our knowledge  there is no algorithm in the current literature that combines all those features.
Algorithms have been proposed for CMDPs  which are less ﬂexible sub-problems of the more general
BMDP. When the environment parameters (P   Rr  Rc) are known but not tractable  solutions relying
on function approximation (Undurti et al.  2011) or approximate linear programming (Poupart et al. 
2015) have been proposed. For unknown environments  online algorithms (Geibel and Wysotzki 
2005; Abe and others  2010; Chow et al.  2018; Achiam et al.  2017) and a batch algorithm (Thomas
et al.  2015; Petrik et al.  2016; Laroche and Trichelair  2019; Le et al.  2019) can solve large unknown
CMDPs. Nevertheless  these approaches are limited in that the constraints thresholds are ﬁxed prior
to training and cannot be updated in real-time at policy execution to select the desired level of risk.
To our knowledge  there were only two ways of solving a BMDP. The ﬁrst one is to approximate
it with a ﬁnite set of CMDPs (e.g. see our FTQ(λ) baseline). The solutions of these CMDPs take
the form of mixtures between two deterministic policies (Theorem 4.4  Beutler and Ross  1985). To
obtain these policies  one needs to evaluate their expected cost by interacting with the environment5.
Our solution not only requires one single model but also avoids any supplementary interaction.
The only other existing BMDP algorithm  and closest work to ours  is the Dynamic Programming
algorithm proposed by Boutilier and Lu (2016). However  their work was established for ﬁnite state
spaces only  and their solution relies heavily on this property. For instance  they enumerate and sort
the next states s(cid:48) ∈ S by their expected value-by-cost  which could not be performed in a continuous
state space S. Moreover  they rely on the knowledge of the model (P   Rr  Rc)  and do not address
the question of learning from interaction data.

7 Conclusion

The BMDP framework is a principled framework for safe decision making under uncertainty  which
could be beneﬁcial to the diffusion of Reinforcement Learning in industrial applications. However 
BMDPs could so far only be solved in ﬁnite state spaces which limits their interest in many use-cases.
We extend their deﬁnition to continuous states by introducing of a novel Dynamic Programming
operator  that we build upon to propose a Reinforcement Learning algorithm. In order to scale to large
problems  we provide an efﬁcient implementation that exploits the structure of the value function and
leverages tools from Deep Distributed Reinforcement Learning. We show that on two practical tasks
our solution performs similarly to a baseline Lagrangian relaxation method while only requiring a
single model to train  and relying on an interpretable β instead of the tedious tuning of the penalty λ.

Acknowledgements

This work has been supported by CPER Nord-Pas de Calais/FEDER DATA Advanced data science
and technologies 2015-2020  the French Ministry of Higher Education and Research  INRIA  and the
French Agence Nationale de la Recherche (ANR). We thank Guillaume Gautier  Fabrice Clerot  and
Xuedong Shang for the helpful discussions and valuable insights.

5More details are provided in Appendix C

9

References
Naoki Abe et al. Optimizing debt collections using constrained reinforcement learning. In Special

Interest Group on Knowledge Discovery and Data Mining (SIGKDD)  2010.

Joshua Achiam  David Held  Aviv Tamar  and Pieter Abbeel. Constrained policy optimization. In

Proceedings of the International Conference on Machine Learning (ICML)  2017.

Eitan Altman. Constrained Markov Decision Processes. CRC Press  1999.
Frederick J. Beutler and Keith W. Ross. Optimal policies for controlled markov chains with a

constraint. In Journal of Mathematical Analysis and Applications  1985.

Craig Boutilier and Tyler Lu. Budget allocation using weakly coupled  constrained markov decision

processes. In Uncertainty in Artiﬁcial Intelligence (UAI)  2016.

Nicolas Carrara  Romain Laroche  Jean-Léon Bouraoui  Tanguy Urvoy  and Olivier Pietquin. Safe
transfer learning for dialogue applications. In International Conference on Statistical Language
and Speech Processing (SLSP)  2018.

Senthilkumar Chandramohan  Matthieu Geist  and Olivier Pietquin. Optimizing spoken dialogue
management with ﬁtted value iteration. In Conference of the International Speech Communication
Association (InterSpeech)  2010.

Yinlam Chow  Aviv Tamar  Shie Mannor  and Marco Pavone. Risk-Sensitive and Robust Decision-
Making: a CVaR Optimization Approach. In Advances in Neural Information Processing Systems
(NIPS)  2015.

Yinlam Chow  Mohammad Ghavamzadeh  Lucas Janson  and Marco Pavone. Risk-constrained
reinforcement learning with percentile risk criteria. In Journal of Machine Learning Research
(JMLR)  2018.

Christoph Dann  Lihong Li  Wei Wei  and Emma Brunskill. Policy certiﬁcates: Towards accountable
reinforcement learning. In Proceedings of the International Conference on Machine Learning
(ICML)  2019.

Damien Ernst  Pierre Geurts  and Louis Wehenkel. Tree-Based Batch Mode Reinforcement Learning.

In Journal of Machine Learning Research (JMLR)  2005.

Javier García and Fernando Fernández. A Comprehensive Survey on Safe Reinforcement Learning .

In Journal of Machine Learning Research (JMLR)  2015.

Peter Geibel and Fritz Wysotzki. Risk-sensitive reinforcement learning applied to control under

constraints. In Journal of Artiﬁcial Intelligence Research (JAIR)  2005.

Garud N. Iyengar. Robust Dynamic Programming . In Mathematics of Operations Research  2005.
Hatim Khouzaimi  Romain Laroche  and Fabrice. Lefevre. Optimising turn-taking strategies with
reinforcement learning. . In Special Interest Group on Discourse and Dialogue (SIGDIAL)  2015.
Romain Laroche and Rémi Trichelair  Paul and Tachet des Combes. Safe policy improvement with
baseline bootstrapping. In Proceedings of the International Conference on Machine Learning
(ICML)  2019.

Hoang M. Le  Cameron Voloshin  and Yisong Yue. Batch policy learning under constraints. In

Proceedings of the International Conference on Machine Learning (ICML)  2019.

Edouard Leurent. An environment for autonomous driving decision-making. https://github.com/

eleurent/highway-env  2018.

Lihong Li  Jason D. Williams  and Suhrid Balakrishnan. Reinforcement learning for dialog man-
In Conference of the

agement using least-squares policy iteration and fast feature selection.
International Speech Communication Association (InterSpeech)  2009.

Chunming Liu  Xin Xu  and Dewen Hu. Multiobjective Reinforcement Learning: A Comprehensive

Overview. In IEEE Transactions on Systems  Man  and Cybernetics: Systems  2014.
David G. Luenberger. Investment science. Oxford University Press  Incorporated  2013.
H. Mausser and D. Rosen. Beyond VaR: from measuring risk to managing risk. In Proceedings of

the IEEE Conference on Computational Intelligence for Financial Engineering  2003.

10

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A. Rusu  Joel Veness  Marc G. Belle-
mare  Alex Graves  Martin Riedmiller  Andreas K. Fidjeland  Georg Ostrovski  Stig Petersen 
Charles Beattie  Amir Sadik  Ioannis Antonoglou  Helen King  Dharshan Kumaran  Daan Wierstra 
Shane Legg  and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature  2015.

Arnab Nilim and Laurent El Ghaoui. Robust Control of Markov Decision Processes with Uncertain

Transition Matrices . In Operations Research  2005.

Mohammad Petrik  Marek Ghavamzadeh    and Yinlam Chow. Safe policy improvement by min-
imizing robust baseline regret. In Advances in Neural Information Processing Systems (NIPS) 
2016.

Olivier Pietquin  Matthieu Geist  Senthilkumar Chandramohan  and Hervé Frezza-Buet. Sample-
efﬁcient batch reinforcement learning for dialogue management optimization. ACM Transactions
on Speech and Language Processing (TSLP)  7(3):7  2011.

Pascal Poupart  Aarti Malhotra  Pei Pei  Kee-Eung Kim  Bongseok Goh  and Michael Bowling.
Approximate linear programming for constrained partially observable markov decision processes.
In Proceedings of the Association for the Advancement of Artiﬁcial Intelligence Conference (AAAI) 
2015.

Martin Riedmiller. Neural ﬁtted Q iteration - First experiences with a data efﬁcient neural Reinforce-
ment Learning method. In Lecture Notes in Computer Science (including subseries Lecture Notes
in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics)  2005.

Diederik M. Roijers  Peter Vamplew  Shimon Whiteson  and Richard Dazeley. A survey of multi-
objective sequential decision-making. In Journal of Artiﬁcial Intelligence Research (JAIR)  2013.
Aviv Tamar  Dotan Di Castro   and Shie Mannor. Policy Gradients with Variance Related Risk

Criteria . In Proceedings of the International Conference on Machine Learning (ICML)  2012.

Philip Thomas  Georgios Theocharous  and Mohammad Ghavamzadeh. High conﬁdence policy
improvement. In Proceedings of the International Conference on Machine Learning (ICML)  2015.
Aditya Undurti  Alborz Geramifard  and Jonathan P. How. Function approximation for continuous

constrained mdps. In Tech Report  2011.

Wolfram Wiesemann  Daniel Kuhn  and Berç Rustem. Robust markov decision processes.

Mathematics of Operations Research  2013.

In

11

,Yinlam Chow
Ofir Nachum
Edgar Duenez-Guzman
Mohammad Ghavamzadeh
Nicolas Carrara
Edouard Leurent
Romain Laroche
Tanguy Urvoy
Odalric-Ambrym Maillard
Olivier Pietquin