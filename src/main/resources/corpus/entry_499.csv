2019,An Algorithm to Learn Polytree Networks with Hidden Nodes,Ancestral graphs are a prevalent mathematical tool to take into account latent (hidden) variables in a probabilistic graphical model. In ancestral graph representations  the nodes are only the observed (manifest) variables and the notion of m-separation fully characterizes the conditional independence relations among such variables  bypassing the need to explicitly consider latent variables. However  ancestral graph models do not necessarily represent the actual causal structure of the model  and do not contain information about  for example  the precise number and location of the hidden variables. Being able to detect the presence of latent variables while also inferring their precise location within the actual causal structure model is a more challenging task that provides more information about the actual causal relationships among all the model variables  including the latent ones. In this article  we develop an algorithm to exactly recover graphical models of random variables with underlying polytree structures when the latent nodes satisfy specific degree conditions. Therefore  this article proposes an approach for the full identification of hidden variables in a polytree. We also show that the algorithm is complete in the sense that when such degree conditions are not met  there exists another polytree with fewer number of latent nodes satisfying the degree conditions and entailing the same independence relations among the observed variables  making it indistinguishable from the actual polytree.,An Algorithm to Learn Polytree Networks with

Hidden Nodes

Firoozeh Sepehr
Department of EECS

University of Tennessee Knoxville

1520 Middle Dr  Knoxville  TN 37996

Donatello Materassi
Department of EECS

University of Tennessee Knoxville

1520 Middle Dr  Knoxville  TN 37996

dawn@utk.edu

dmateras@utk.edu

Abstract

Ancestral graphs are a prevalent mathematical tool to take into account latent (hid-
den) variables in a probabilistic graphical model. In ancestral graph representations 
the nodes are only the observed (manifest) variables and the notion of m-separation
fully characterizes the conditional independence relations among such variables 
bypassing the need to explicitly consider latent variables. However  ancestral graph
models do not necessarily represent the actual causal structure of the model  and
do not contain information about  for example  the precise number and location of
the hidden variables. Being able to detect the presence of latent variables while
also inferring their precise location within the actual causal structure model is a
more challenging task that provides more information about the actual causal rela-
tionships among all the model variables  including the latent ones. In this article 
we develop an algorithm to exactly recover graphical models of random variables
with underlying polytree structures when the latent nodes satisfy speciﬁc degree
conditions. Therefore  this article proposes an approach for the full identiﬁcation
of hidden variables in a polytree. We also show that the algorithm is complete
in the sense that when such degree conditions are not met  there exists another
polytree with fewer number of latent nodes satisfying the degree conditions and
entailing the same independence relations among the observed variables  making it
indistinguishable from the actual polytree.

1

Introduction

The presence of unmeasured variables is a fundamental challenge in discovery of causal relationships
[1  2  3]. When the causal diagram is a Directed Acyclic Graph (DAG) with unmeasured variables 
a common approach is to use ancestral graphs to describe the independence relations among the
measured variables [2]. The main advantage of ancestral graphs is that they involve only the measured
variables and successfully encode all their conditional independence relations via m-separation.
Furthermore  complete algorithms have been devised to obtain ancestral graphs from observational
data  e.g.  the work in [3]. However  recovering the actual structure of the original DAG is something
that ancestral graphs somehow circumvent. For example  it might be known that the actual causal
diagram has a polytree structure including the hidden nodes  but the ancestral graph associated with
the measured variables might not even be a polytree [4]. Instead  the recovery of causal diagrams
including the location of their hidden variables is a very challenging task and algorithmic solutions
are available only for speciﬁc scenarios [5  6  7  8]. For example  in the case of speciﬁc distributions
(i.e.  Gaussian and Binomial) when the causal diagram is known to be a rooted tree  the problem
has been solved by exploiting the additivity of a metric along the paths of the tree [6  7  8  9]. In the
case of generic distributions  though  additive metrics might be too diﬃcult to deﬁne or cannot be
deﬁned in general. Furthermore  rooted trees can be considered a rather limiting class of networks

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

since they represent probability distributions which can only be factorized according to second order
conditional distributions [10].
This article makes a novel contribution towards the recovery of more general causal diagrams.
Indeed  it provides an algorithm to learn causal diagrams making no assumptions on the underlying
probability distribution  and considering polytree structures which can represent factorizations
involving conditional distributions of arbitrarily high order. Furthermore  it is shown that a causal
diagram with a polytree structure can be exactly recovered if and only if each hidden node satisﬁes
the following conditions: (i) the node has at least two children; (ii) if the node has exactly one parent 
such a parent is not hidden; (iii) the node has at least degree 3  or each of its two children has at least
another parent. The provided algorithm recovers every polytree structure with hidden nodes satisfying
these conditions  and  remarkably  makes use only of third order statistics. If the degree conditions are
not satisﬁed  then it is shown that there exists another polytree with fewer number of hidden random
variables which entails the same independence relations among the observed variables. Indeed  in
this case  when no additional information/observations are provided  no test can be constructed to
determine the true structure. Another main advantage of this proposed approach lies in the fact that it
follows a form of Occam’s razor principle since in the case where the degree conditions on the hidden
nodes are not met  then a polytree with minimal number of hidden nodes is selected. We ﬁnd this
property quite relevant in application scenarios since Occam’s razor is arguably one of the cardinal
principles in all sciences.

2 Preliminaries  Assumptions and Problem Deﬁnition

In order to formulate our problem  we ﬁrst introduce a generalization of the notions of directed and
undirected graphs (see for example [11  12]) which also considers a partition of the set of nodes into
visible and hidden nodes.
Deﬁnition 1 (Latent partially directed graph). A latent partially directed graph ¯G(cid:96) is a 4-ple
(V  L  E  (cid:126)E) where

• the disjoint sets V and L are named the set of visible nodes and the set of hidden nodes 
• the set E is the set of undirected edges containing unordered pairs of (V ∪ L) × (V ∪ L) 
• the set (cid:126)E is the set of directed edges containing ordered pairs of (V ∪ L) × (V ∪ L).

We denote the unordered pair of two elements yi  y j ∈ V ∪ L as yi − y j  and the ordered pair of yi  y j
(when yi precedes y j) as yi → y j. In a latent partially directed graph the sets E and (cid:126)E do not share
any edges. Namely  yi − y j ∈ E implies that both yi → y j and y j → yi are not in (cid:126)E. (cid:3)
A latent partially directed graph is a fully undirected graph when (cid:126)E = ∅  and we simplify the notation
by writing G(cid:96) = (V  L  E). Similarly  when E = ∅  we have a fully directed graph  and we denote
it by (cid:126)G(cid:96) = (V  L  (cid:126)E). Furthermore  if we drop the distinction between visible and hidden nodes and
consider V ∪ L as the set of nodes  we recover the standard notions of undirected and directed graphs.
Thus  latent partially directed graphs inherit  in a natural way  all notions associated with standard
graphs (e.g.  path  degree  neighbor  etc.  see for example [11]). In the scope of this article  we denote
degree  outdegree  indegree  children  parents  descendants and ancestors of a node y in graph (cid:126)G
using deg(cid:126)G (y)  deg+
(y)  ch(cid:126)G (y)  pa(cid:126)G (y)  de(cid:126)G (y) and an(cid:126)G (y)  respectively (see [11  12] for
(cid:126)G
precise deﬁnitions). Furthermore  the notion of restriction of a graph to a subset of nodes follows
immediately.
Deﬁnition 2 (Restriction of a latent partially directed graph). The restriction of a latent partially
directed graph ¯G(cid:96) = (V  L  E  (cid:126)E) with respect to a set of nodes A ⊆ V ∪ L is the latent partially
directed graph obtained by considering only the nodes in A and the edges linking pairs of nodes
which are both in A. (cid:3)

(y)  deg−

(cid:126)G

Moreover  a latent partially directed graph is called a latent partially directed tree when there exists
exactly one path connecting any pair of nodes.
Deﬁnition 3 (Latent partially directed tree). A latent partially directed tree (cid:126)P(cid:96) is a latent partially
directed graph ¯G(cid:96) = (V  L  E  (cid:126)E) where every pair of nodes yi  y j ∈ V ∪ L is connected by exactly one
path.

2

Trivially  latent partially directed trees generalize the notions of undirected trees and polytrees
(directed trees) [13]. In a latent partially directed tree  we deﬁne a hidden cluster as a group of hidden
nodes that are connected to each other via a path constituted exclusively of hidden nodes.
Deﬁnition 4 (Hidden cluster). A hidden cluster in a latent partially directed tree (cid:126)P(cid:96) = (V  L  E  (cid:126)E)
is a set C ⊆ L such that for each distinct pair of nodes yi  y j ∈ C the unique path connecting them
contains only nodes in C and no node in C is linked to a node which is in L \ C. (cid:3)
Observe that each node in a hidden cluster has neighbors which are either visible or hidden nodes
of the same cluster. Figure 1 (a) depicts a latent directed tree (or a latent polytree) and its hidden
clusters C1 and C2 highlighted by the dotted lines.

(a)

(b)

(c)

Figure 1: A hidden polytree (a) and its collapsed hidden polytree (b)  a minimal hidden polytree (c).

Furthermore  we introduce the set of (visible) neighbors of a hidden cluster  its closure and its degree.
Deﬁnition 5 (Neighbors  closure  and degree of a hidden cluster). In a latent partially directed tree 
the set of all visible nodes linked to any of the nodes of a hidden cluster C is the set of neighbors of C
and is denoted by N(C). We deﬁne the degree of the hidden cluster as |N(C)|  namely  the number of
neighbors of the cluster. We refer to the restriction of a latent polytree to a hidden cluster and its
neighbors as the closure of the hidden cluster. (cid:3)

Observe that the neighbors of C1 are shaded with orange color in Figure 1 (a). We also remind the
notion of a root node and deﬁne the notion of a root of a hidden cluster.
Deﬁnition 6 (Root of a latent polytree  and root of a hidden cluster in a latent polytree). In a latent
polytree (cid:126)P(cid:96) = (V  L  (cid:126)E)  a root is a node yr ∈ V ∪ L with indegree equal to zero. Also  we deﬁne any
root of the restriction of the polytree to one of its hidden clusters as the root of the hidden cluster. (cid:3)

For example  in Figure 1 (a)  node y1 is a root of the latent polytree and node yh3 is a root of the hidden
cluster C1. In this article  we make extensive use of the restriction of a polytree to the descendants of
one of its roots. We deﬁne such a restriction as the rooted subtree of the polytree associated with
that root. Additionally  given a latent partially directed tree  we deﬁne its collapsed representation
by replacing each hidden cluster with a single hidden node. The formal deﬁnition is as follows and
Figure 1 (b) depicts the collapsed representation of the latent polytree of Figure 1 (a).
Deﬁnition 7 (Collapsed representation). We deﬁne the collapsed representation of (cid:126)P(cid:96) = (V  L  E  (cid:126)E)
as the latent partially directed tree (cid:126)Pc = (V  Lc  Ec  (cid:126)Ec) where nc is the number of hidden clusters
C1  ...  Cnc  and Lc := C1 ∪ ... ∪ Cnc  and

Ec := {yi − y j ∈ E | yi  y j ∈ V} ∪ {yi − Ck | ∃y j ∈ Ck  yi − y j ∈ E} ∪ {Ck − y j | ∃yi ∈ Ck  yi − y j ∈ E}
(cid:126)Ec := {yi → y j ∈ (cid:126)E | yi  y j ∈ V} ∪ {yi → Ck | ∃y j ∈ Ck  yi → y j ∈ (cid:126)E} ∪ {Ck → y j | ∃yi ∈ Ck  yi → y j ∈ (cid:126)E}.(cid:3)

In this article  we show the cases where graphical models with polytree structures can be recovered
from the independence relations involving only visible nodes. Speciﬁcally  we assume that a polytree
is a perfect map (see [14  12]) for a probabilistic model deﬁned over the variables V ∪ L where V and
L are disjoint sets. We ﬁnd conditions under which it is possible to recover information about the
perfect map of the probabilistic model considering only independence relations of the form I(yi ∅  y j)
(read yi and y j are independent) and I(yi  yk  y j) (read yi and y j are conditionally independent given
yk) for all nodes yi  y j  yk ∈ V. One of the fundamental requirements of solving this problem is that
all hidden nodes need to satisfy certain degree conditions summarized in the following deﬁnition.
Deﬁnition 8 (Minimal latent polytree). A latent polytree (cid:126)P(cid:96) = (V  L  (cid:126)E) is minimal if every hidden
node yh ∈ L satisﬁes one of the following conditions:

3

h12h710645h631h2h3h4h511C1C2789C221064531C111789h12h710645h631h3h511C1121413C2C378914• deg+
• deg+

(cid:126)P(cid:96)

(yh) ≥ 2 and deg(cid:126)P(cid:96)
(yh) = 2 and deg−

(cid:0)yc2
(cid:1)   deg−
(cid:0)yc1
(yh)| = 1  then pa(cid:126)P(cid:96)
(yh) ≥ 3 and if |pa(cid:126)P(cid:96)
(yh) = 0 and deg−

(cid:1) ≥ 2 where ch(cid:126)P(cid:96)

(yh) ⊆ V;

(cid:126)P(cid:96)

(cid:126)P(cid:96)

(cid:126)P(cid:96)

(cid:126)P(cid:96)

(yh) = {yc1   yc2}. (cid:3)
Note that the nodes yh2  yh4  yh5  yh7 in Figure 1 (a) do not satisfy the minimality conditions and
therefore the hidden polytree is not minimal. Instead  Figure 1 (c) shows a minimal latent polytree.
The algorithm we propose to recover the structure of a latent polytree can be decomposed in several
tasks and the hidden nodes which are roots with outdegree equal to 2 and at least one visible child
require to be dealt with in a special way in the last task of the algorithm. Therefore  we deﬁne the
following two types of hidden nodes to make this distinction.
Deﬁnition 9 (Type-I and type-II hidden nodes). In a minimal latent polytree  we classify a hidden
node yh as type-II when deg(cid:126)G (yh) = 2 with at least one visible child. All other hidden nodes are
classiﬁed as type-I.

In the minimal latent polytree of Figure 2 (a)  the hidden nodes yh2 and yh3 are type-II hidden nodes 
while all the other hidden nodes are type-I.

(a)

(b)

(c)

Figure 2: A minimal latent polytree (cid:126)P(cid:96) with type-II hidden nodes (a)  its quasi-skeleton (b)  and its
collapsed quasi-skeleton (c).

We deﬁne the quasi-skeleton of a minimal latent polytree to deal with type-II hidden nodes separately.
Deﬁnition 10 (Quasi-skeleton of a latent polytree). In a minimal latent polytree (cid:126)P(cid:96) = (V  L  (cid:126)E)  the
quasi-skeleton of (cid:126)P(cid:96) is the undirected graph obtained by removing the orientation of all edges in (cid:126)P(cid:96) 
and removing all the type-II hidden nodes and then linking its two children together. (cid:3)

In Figure 2 (b)  we have the quasi-skeleton of the polytree of Figure 2 (a). Observe that we can
easily deﬁne the collapsed representation of a quasi-skeleton of a latent polytree by ﬁnding the
quasi-skeleton ﬁrst and then ﬁnding its collapsed representation as in Figure 2 (c).
As it is well known in the theory of graphical models  in the general case  from a set of conditional
independence statements (formally  a semi-graphoid) faithful to a Directed Acyclic Graph (DAG)  it
is not possible to recover the full DAG [15  1]. What can be recovered for sure is the pattern of the
DAG  namely the skeleton and the v-structures (i.e.  yi → yk ← y j) of the DAG [15  1]. In this article 
we show that  similarly  in the case of a minimal latent polytree  we are able to recover the pattern of
the polytree from the independence statements involving only the visible variables.
Deﬁnition 11 (Pattern of a polytree). Let (cid:126)P = (N  (cid:126)E) be a polytree. The pattern of (cid:126)P is a partially
directed graph where the orientation of all the v-structures (i.e.  yi → yk ← y j) are known and
as many as the remaining undirected edges are oriented in such a way that the other alternative
orientation would result in a v-structure. (cid:3)

Now we have all the necessary tools to formulate the problem.
Problem Formulation. Assume a semi-graphoid deﬁned over a set of variables V∪L. Let the latent
polytree (cid:126)P(cid:96) = (V  L  (cid:126)E) be faithful to the semi-graphoid and assume that the nodes in L satisfy the
minimality conditions. Recover the pattern of (cid:126)P(cid:96) from conditional independence relations involving
only nodes in V.
Remark 12. The proposed solution makes only use of the conditional independence relations of the
form I(yi ∅  y j) and I(yi  yk  y j) for all yi  y j  yk ∈ V.

4

h259h3h5h6341187h46h1211059h5h6341187h46h1211059C2341187C1621103 An Algorithm to Reconstruct Minimal Hidden Polytrees

Our algorithm for learning the pattern of a minimal latent polytree is made of the following 5 tasks:

1. Using the independence statements involving the visible nodes  determine the number of rooted

subtrees in the latent polytree and their respective sets of visible nodes;

2. Given all the visible nodes belonging to each rooted subtree  determine the collapsed quasi-

skeleton of each rooted subtree;

3. Merge the overlapping hidden clusters in the collapsed quasi-skeleton of each rooted subtree to

obtain the collapsed quasi-skeleton of the latent polytree;

4. Determine the quasi-skeleton of the latent polytree from the collapsed quasi-skeleton of the

latent polytree (recover type-I hidden nodes);

5. Obtain the pattern of the latent polytree from the recovered quasi-skeleton of the latent polytree

(recover type-II hidden nodes and edge orientations).

Figure 3 shows the stage of the recovery of the polytree structure at the end of each task. The
following subsections provide more details about each task  but the most technical results are in the
Supplemental Material. We stress that the ﬁrst two tasks mostly leverage previous work about rooted
trees and the main novelty of this article lies in tasks 3  4 and 5.

(True)

(Task 1)

(Task 2)

(Task 3)

(Task 4)

(Task 5)

Figure 3: The actual minimal latent polytree (True); the lists of visible nodes for each rooted subtree
(Task 1)  collapsed quasi-skeletons of the rooted subtrees (Task 2)  merging of the overlapping hidden
clusters (Task 3)  detection of type-I hidden nodes (Task 4)  detection of type-II hidden nodes along
with orientation of the edges to obtain the pattern of the hidden polytree (Task 5). Observe that the
full polytree is not recovered at the end of task 5 since the edge y9 − y18 is left undirected.

3.1 Task 1: Determine the visible nodes of each rooted subtree

This ﬁrst task can be performed by the Pairwise-Finite Distance Algorithm (PFDA)  presented in [16]
and reported in the Supplementary Material as Algorithm 4. As shown in [16]  PFDA takes as input
the set of visible nodes of a latent polytree and outputs sets of visible nodes with the property that
each set corresponds to the visible descendants of a root of the latent polytree  when the polytree is
minimal. In the following theorem  we show that the output of PFDA applied to the independence
statements is the same as described above. See Supplementary Material for the proof of this theorem.
Theorem 13. Consider a latent polytree (cid:126)P(cid:96) = (V  L  (cid:126)E) faithful to a probabilistic model. Assume that
the hidden nodes in L satisfy the minimality conditions. Then PFDA  applied to the independence
statements of the probabilistic model with the form I(yi ∅  y j) for all yi  y j ∈ V  outputs a collection
of sets  such that each of them is given by all the visible descendants of a root of (cid:126)P(cid:96).

5

2710h451h1h24638h5h7h69h311171516141312h141810514638911171516141312417151614102913121715161413127613127151114171691811115171516141514171691715161491101312131267478CFCACDCHCI1715161492101312CB131263CC181113121411715162478CA(cid:48)531069182710h451h24638h5h7h69111715161413124182710h451h1h24638h5h7h69h311171516141312h14183.2 Task 2: Determine the collapsed quasi-skeleton of each rooted subtree

The second task is performed by the Reconstruction Algorithm for Latent Rooted Trees in [17]. We
report it as Algorithm 5 in the Supplementary Material for completeness. The input of this algorithm
is the set Vr of the visible nodes belonging to a rooted subtree Tr and independence relations of the
form I(yi  yk  y j) or ¬I(yi  yk  y j) for distinct yi  y j  yk ∈ Vr. Its output is the collapsed quasi-skeleton
of Tr. Thus  we can call this algorithm on all of the sets of visible nodes V1  ...  Vnr where nr is
the number of roots  obtained from Task 1  and ﬁnd the collapsed quasi-skeletons of all the rooted
subtrees of the latent polytree. This result is formalized in the following theorem. See Supplementary
Material for the proof of this theorem.

Theorem 14. Let (cid:126)P(cid:96) = (V  L  (cid:126)E) be a minimal latent polytree. Consider a root yr of (cid:126)P(cid:96) and let
Vr = V ∩ de(cid:126)P(cid:96)
(yr). The output of Reconstruction Algorithm for Latent Rooted Trees applied to Vr is
the collapsed quasi-skeleton of the rooted subtree with root node yr.

3.3 Task 3: Merge the overlapping hidden clusters of the collapsed rooted trees

By applying the Reconstruction Algorithm for Latent Rooted Trees on each set of visible nodes in
the same rooted tree  we have  as an output  the collapsed quasi-skeletons of all rooted subtrees in the
original hidden polytree. In the general case  some hidden clusters in the collapsed quasi-skeleton of
the rooted subtrees might overlap  namely  they might share some hidden nodes in the original hidden
polytree. The following theorem provides a test on the sets of visible nodes of the rooted subtrees in
a minimal latent polytree to determine if two hidden clusters in two distinct collapsed quasi-skeletons
of two rooted subtrees belong to the same cluster in the collapsed quasi-skeleton of the polytree. See
Supplementary Material for the proof of this theorem.

Theorem 15. Consider a minimal latent polytree (cid:126)P(cid:96). Let C1 and C2 be two distinct hidden clusters
in the collapsed quasi-skeletons of two rooted subtrees of (cid:126)P(cid:96). If the set of neighbors of C1 and the set
of neighbors of C2 share at least a pair of visible nodes  i.e.  |N(C1) ∩ N(C2)| ≥ 2  then the nodes in
C1 and C2 belong to the same hidden cluster in the collapsed quasi-skeleton of (cid:126)P(cid:96).

This theorem is the enabling result for the Hidden Cluster Merging Algorithm (HCMA)  presented in
Algorithm 1  which merges all the collapsed quasi-skeletons associated with the individual rooted
subtrees  obtained from Task 2  into the collapsed quasi-skeleton of the polytree. This algorithm
starts with the collapsed quasi-skeleton of the rooted subtrees  then ﬁnds pairs of clusters that overlap
by testing if they share at least one pair of visible neighbors (see Theorem 15)  and then merges the
overlapping pairs. This procedure is repeated until no clusters are merged anymore.

Algorithm 1 Hidden Cluster Merging Algorithm

Input the collapsed quasi-skeleton of the rooted subtrees Ti = (Vi  Li  Ei) for i = 1  ...  nr
Output the collapsed quasi-skeleton P of the latent polytree

1: Initialize the set of clusters P with the hidden clusters of all Ti  i.e.  P := {{C1} {C2}  ... {Ck}}
2: while there are two elements Ci  C j ∈ P such that |N(Ci) ∩ N(C j)| ≥ 2 do
3:
4:
5: end while
6: Deﬁne the polytree P = (∪iVi P  E) where E := {{ya  yb} | ∃ i : ya  yb ∈ Vi  ya − yb ∈ Ei} ∪

remove Ci  C j from P and add Ci ∪ C j to P
deﬁne N(Ci ∪ C j) := N(Ci) ∪ N(C j)

{{ya  Cb} | ∃ i  h : ya ∈ Vi  yh ∈ Li  Li ⊆ Cb  Cb ∈ P  ya − yh ∈ Ei}

The following theorem guarantees that  for a minimal latent polytree  the output of HCMA is the
collapsed quasi-skeleton of the polytree. See Supplementary Material for the proof of this theorem.

Theorem 16. Let (cid:126)P(cid:96) = (V  L  (cid:126)E) be a minimal latent polytree and let Ti = (Vi  Li  Ei) for i = 1  ...  nr
be the collapsed quasi-skeletons of the rooted subtrees of (cid:126)P(cid:96). Then HCMA outputs the collapsed
quasi-skeleton of (cid:126)P(cid:96).

6

3.4 Task 4: Determine the quasi-skeleton of the latent polytree from the collapsed

quasi-skeleton of the latent polytree (recover type-I hidden nodes)

After performing the HCMA  the output is the collapsed quasi-skeleton of the latent polytree  thus 
the structure of the hidden nodes within each hidden cluster is not known yet. Note that the restriction
of the original polytree to the closure of a hidden cluster is a smaller polytree. The goal of this task is
to recover the structure of the hidden clusters by focusing on each individual closure (i.e.  recover
Type-I hidden nodes and their connectivities). Given the closure of a hidden cluster  the basic strategy
is to detect one root of the hidden cluster along with the visible nodes (if any) linked to this root.
Then  we label such a root as a visible node  add edges between this node and its visible neighbors 
and subsequently apply the same strategy recursively to the descendants of such a detected root.
Since we focus on the closure of a speciﬁc hidden cluster  say C  we deﬁne the following sets
˜Vr = Vr ∩ N(C) for r = 1  ...  nr where nr is the number of rooted subtrees in the latent polytree and
Vr are the sets of visible nodes in each rooted subtree (obtained from Task 1). A fundamental result
for detection of a root of a hidden cluster is the following theorem. See Supplementary Material for
the proof of this theorem.
Theorem 17. Let (cid:126)P(cid:96) be a minimal latent polytree and let (cid:126)T r = (Vr  Lr  (cid:126)Er) with r = 1  ...  nr be all
the rooted subtrees of (cid:126)P(cid:96). Let C be a hidden cluster in the collapsed quasi-skeleton of (cid:126)P(cid:96). Deﬁne
˜Vr := Vr ∩ N(C) for r = 1  ...  nr where nr is the number of roots in (cid:126)P(cid:96). Then  Tr contains a hidden
root of C if and only if ˜Vr (cid:44) ∅ and for all ˜Vr(cid:48) with r(cid:48) (cid:44) r we have | ˜Vr \ ˜Vr(cid:48)| > 1 or | ˜Vr(cid:48) \ ˜Vr| ≤ 1.
To make the application of this theorem more clear  consider the latent polytree introduced in
Figure 3 (True). After applying the ﬁrst three tasks  we obtain the collapsed quasi-skeleton of the
latent polytree as depicted in Figure 3 (Task 3). Observe that the rooted subtrees (cid:126)T 1 (with root y1)
and (cid:126)T 2 (with root y2) satisfy the conditions of Theorem 17 indicating that they contain a root of the
hidden cluster. The following lemma allows one to ﬁnd the visible nodes linked to a hidden root in
the closure of a hidden cluster. See Supplementary Material for the proof of this lemma.
Lemma 18. Let (cid:126)P(cid:96) be a minimal latent polytree. Consider a hidden root yh of a hidden cluster C in
the collapsed quasi-skeleton of (cid:126)P(cid:96) where yh belongs to the rooted subtree Tr = (Vr  Lr  (cid:126)Er). Deﬁne
˜Vr(cid:48) := Vr(cid:48) ∩ N(C) for r(cid:48) = 1  ...  nr where nr is the number of roots in (cid:126)P(cid:96). The visible nodes linked to
yh are given by the set W \ W where

I := {r} ∪ {r(cid:48)such that | ˜Vr \ ˜Vr(cid:48)| = | ˜Vr(cid:48) \ ˜Vr| = 1} 

W :=

˜Vi 

W :=

˜Vi.

(cid:91)

i∈I

(cid:91)

i(cid:60)I

We follow the example of Figure 3 to show the steps of Task 4 in more details. Without loss
of generality  choose Tr = T1. Consider the closure of CA(cid:48) obtained at the end of Task 3
and then apply Lemma 18 to obtain I = {1  2}  W = {y1  y2  y10  y12  y13  y14  y15  y16  y17}  W =
{y5  y6  y9  y11  y12  y13  y14  y15  y16  y17}  and thus W \ W = {y1  y2  y10}. Therefore  the visible nodes
linked to the hidden root in T1 are y1  y2 and y10. Now we introduce the Hidden Cluster Learning
Algorithm (HCLA)  presented in Algorithm 2  to learn the structure of a hidden cluster.
Again  consider the closure of the hidden cluster CA(cid:48) as depicted in Figure 4 (Task 4a) which we
obtained at the end of Task 3. Then  apply Hidden Node Detection procedure to CA(cid:48) and observe
that the output at the end of Step 23 of Algorithm 2 is in Figure 4 (Task 4b). The output of the
merging in Steps 24-27 is depicted in Figure 4 (Task 4c) and the output of the merging in Step 28 is
depicted in Figure 4 (Task 4d). Now  we can apply the same procedure recursively to the remaining
hidden clusters to obtain the ﬁnal output of Task 4  the quasi-skeleton of the polytree  as depicted in
Figure 3 (Task 4).
Here  we show that the output of HCLA is the quasi-skeleton of the latent polytree. See Supplementary
Material for the proof of this theorem.
Theorem 19. Let (cid:126)P(cid:96) = (V  L  (cid:126)E) be a minimal latent polytree. When HCLA is applied to all hidden
clusters of the collapsed quasi-skeleton of (cid:126)P(cid:96)  the output P = (V  E) is the quasi-skeleton of (cid:126)P(cid:96).
Furthermore  HCLA also outputs  for each pair yi  y j ∈ V  the relation I(yi ∅  y j) if and only if the
path connecting yi and y j in (cid:126)P(cid:96) contains an inverted fork.

7

ya  yb ∈(cid:83)

Algorithm 2 Hidden Cluster Learning Algorithm
Input the collapsed quasi-skeleton of a minimal polytree (cid:126)P(cid:96)  collapsed quasi-skeletons of the
rooted subtrees Ti = (Vi  Li  Ei) for i = 1  ...  nr  and the set of the hidden clusters P = {C1  ...  CnC}
Output P and the independence relations of the form I(ya ∅  yb) or ¬I(ya ∅  yb) for all nodes

i Vi
Call Hidden Node Detection Procedure(C1) where C1 is the ﬁrst element of P

Compute ˜Vi = Vi ∩ N(C)
Find ˜Vr which satisﬁes | ˜Vr \ ˜Vr(cid:48)| > 1 or | ˜Vr(cid:48) \ ˜Vr| ≤ 1 for all r(cid:48) (cid:44) r (as in Theorem 17)
Initialize W := ˜Vr  W := ∅  and I := {r}
for all i = 1  ...  nr with i (cid:44) r do

if | ˜Vr \ ˜Vi| = 1 and | ˜Vi \ ˜Vr| = 1 (as in Lemma 18) then
else

1: while P (cid:44) ∅ do
2:
3: end while
4: procedure Hidden Node Detection(C)
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:

W := W ∪ ˜Vi and I := I ∪ {i}
W := W ∪ ˜Vi

end if

end for
A new hidden node yh is revealed
Add yh to all the rooted trees Ti with i ∈ I  namely Vi := Vi ∪ {yh}
(cid:111)
Add the independence relation ¬I(yh ∅  y) for all y ∈ Vi with i ∈ I  and add the independence

Link all nodes in W \ W to yh in all Ti with i ∈ I  namely Ei := Ei ∪(cid:110){yh  y} | y ∈ W \ W

relation I(yh ∅  y) for all other nodes y

for all i ∈ I do

18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29: end procedure

create nk = |W ∩ W| new clusters: C(i)
link yh to C(i)
1   ...  C(i)
nk
1   ...  C(i)
link each cluster C(i)
end for
while ∃ya  yb ∈ N(C(i)
j ) ∪ N(C(i)

nk to a distinct element in W ∩ W
k ) such that ya  yb ∈ ˜Vm where m (cid:60) I do

1   ...  C(i)
nk

merge the two hidden clusters C(i)
update the structure of Ti with the new hidden clusters

j and C(i)
k

end while
Let P = (V P  E) be the output of HCMA applied to Ti = (Vi  Li  Ei)  for i = 1  ...  nr

3.5 Task 5: Obtain the pattern of the latent polytree from the recovered quasi-skeleton of

the latent polytree (recover type-II hidden nodes and edge orientations)

Once the quasi-skeleton of the latent polytree has been obtained  the only missing nodes to recover
the full skeleton are the type-II hidden nodes of the original polytree. Interestingly  the detection
of such hidden nodes can be performed concurrently with the recovery of the edge orientations. In
particular  we apply Rebane and Pearl’s algorithm in [13] to orient the edges of the quasi-skeleton
of the polytree. Then  we have that the edges receiving double orientations imply the presence of
a type-II hidden node between the two linked nodes. Thus  the Hidden Root Recovery Algorithm
(HRRA)  presented in Algorithm 3  is simply an implementation of Rebane and Pearl’s algorithm
(Steps 1-4)  as depicted in Figure 4 (Task 5a)  with the additional detection of type-II hidden nodes
(Steps 5-10).
As a consequence  we have this ﬁnal result stated in Theorem 20 to prove that HRRA outputs the
pattern of the latent polytree. See Supplementary Material for the proof of this theorem.
Theorem 20. Let (cid:126)P(cid:96) be a minimal latent polytree. When the input is the quasi-skeleton of (cid:126)P(cid:96) with
the independence statements of the form I(yi ∅  y j) or ¬I(yi ∅  y j) for all the pairs of nodes yi and
y j  the output of HRRA is the pattern of (cid:126)P(cid:96).

For a complete step by step example of this algorithm see the Supplementary Material.

8

(Task 4a)

(Task 4b)

(Task 4c)

(Task 4d)

(Task 5a)

(Task 5b)

Figure 4: The closure of the hidden cluster CA(cid:48) of the latent polytree in Figure 3 (True) obtained
after Task 3 (Task 4a)  the hidden clusters obtained after Step 23 of HCLA (Task 4b)  merging of
the overlapping hidden clusters as in Steps 24-27 of HCLA (Task 4c)  merging of the overlapping
hidden clusters as in Step 28 of HCLA (Task 4d)  orienting the edges in the quasi-skeleton of the
latent polytree as in Steps 1-4 of HRRA (Task 5a)  and recovering type-II hidden nodes (Task 5b).

Input P = (V  E)  the quasi-skeleton of a latent polytree  and the independence relations of the

Algorithm 3 Hidden Root Recovery Algorithm
form I(yi ∅  y j) or ¬I(yi ∅  y j) for all nodes yi  y j ∈ V
Output the partially directed polytree ¯P = (V  E  (cid:126)E)

if yi − yk  y j − yk ∈ E and I(yi ∅  y j)  then add yi → yk and y j → yk to (cid:126)E
if yi → yk ∈ (cid:126)E  yk − y j ∈ E and ¬I(yi ∅  y j)  then add yk → y j to (cid:126)E

1: while additional edges are oriented do
2:
3:
4: end while
5: Remove the edges that are oriented in (cid:126)E from E
6: for all yi  y j such that yi → y j  y j → yi ∈ (cid:126)E do
7:
8:
9:
10:
11: end for

a new hidden node of Type-II is detected which is a parent of yi and y j
remove yi → y j  y j → yi from (cid:126)E
add a new node yh to V
add yh → y j  yh → yi to (cid:126)E

4 Conclusions and Discussion

We have provided an algorithm to reconstruct the pattern of a latent polytree graphical model. The
algorithm only requires the second and third order statistics of the observed variables and no prior
information about the number and location of the hidden nodes is assumed. An important property of
the proposed approach is that the algorithm is sound under speciﬁc degree conditions on the hidden
variables. If such degree conditions are not met  it is shown  in the Supplementary Material  that there
exists another latent polytree with fewer number of hidden nodes entailing the same independence
relations. In this sense  the proposed algorithm always recover a minimal graphical model in the
sense of hidden nodes following a form of Occam’s razor principle. Future work will study how
this algorithm performs under limited amount of data and how to deal with situations when the
measurements are not exact.

Acknowledgments

This work has been partially supported by NSF (CNS CAREER #1553504).

9

1113121411715162CA(cid:48)51069115171516141514171691015161412171312CF(cid:48)CH(cid:48)CI(cid:48)13126CC(cid:48)h2C(1)1C(1)2C(1)3C(1)4C(1)5C(1)6115171516141514171691015161412171312CF(cid:48)CH(cid:48)CI(cid:48)13126CC(cid:48)h2C(1)1C(1)2115910151614121713126h2CA(cid:48)(cid:48)CB(cid:48)(cid:48)2710h451h24638h5h7h69h3111715161413124182710h451h1h24638h5h7h69h311171516141312h1418References
[1] Judea Pearl. Causality: Models  Reasoning and Inference. Cambridge University Press  2nd edition  2009.

[2] Thomas Richardson and Peter Spirtes. Ancestral graph markov models. The Annals of Statistics  30(4):962–

1030  2002.

[3] Jiji Zhang. On the completeness of orientation rules for causal discovery in the presence of latent

confounders and selection bias. Artiﬁcial Intelligence  172(16):1873 – 1896  2008.

[4] Guangdi Li  Anne-Mieke Vandamme  and Jan Ramon. Learning ancestral polytrees. 2014. The Workshop
of Learning Tractable Probabilistic Models at The 31st International Conference on Machine Learning
(ICML 2014)  Date: 2014/01/01 - 2014/01/01  Location: Beijing  China.

[5] Péter L Erdös  Michael A Steel  LászlóA Székely  and Tandy J Warnow. A few logs suﬃce to build (almost)

all trees: Part ii. Theoretical Computer Science  221(1-2):77–118  1999.

[6] Animashree Anandkumar and Ragupathyraj Valluvan. Learning loopy graphical models with latent

variables: Eﬃcient methods and guarantees. The Annals of Statistics  41(2):401–435  2013.

[7] Elchanan Mossel. Distorted metrics on trees and phylogenetic forests. IEEE/ACM Trans. Comput. Biol.

Bioinformatics  4(1):108–116  January 2007.

[8] Yacine Jernite  Yonatan Halpern  and David Sontag. Discovering hidden variables in noisy-or networks
using quartet tests. In C. J. C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K. Q. Weinberger 
editors  Advances in Neural Information Processing Systems 26  pages 2355–2363. Curran Associates 
Inc.  2013.

[9] Myung Jin Choi  Vincent YF Tan  Animashree Anandkumar  and Alan S Willsky. Learning latent tree

graphical models. Journal of Machine Learning Research  12:1771–1812  2011.

[10] Raphaël Mourad  Christine Sinoquet  Nevin Lianwen Zhang  Tengfei Liu  and Philippe Leray. A survey on

latent tree models and applications. Journal of Artiﬁcial Intelligence Research  47:157–203  2013.

[11] Reinhard Diestel. Graph Theory. Springer  5th edition  2010.

[12] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. The MIT Press 

2009.

[13] G. Rebane and J. Pearl. The recovery of causal polytrees from statistical data. In Proceedings of the Third

Conference on Uncertainty Artiﬁcial Intelligence  pages 222–228  1987.

[14] T. Verma and J. Pearl. Causal Networks: Semantics and Expressiveness.

Workshop on Uncertainty in Artiﬁcial Intelligence  pages 352–359  1988.

In Proceedings of the 4th

[15] P. Spirtes  C.N. Glymour  and R. Scheines. Causation  prediction  and search  volume 81. The MIT Press 

2000.

[16] F. Sepehr and D. Materassi. Learning networks of linear dynamic systems with tree topologies. IEEE

Transactions on Automatic Control  2019. doi: 10.1109/TAC.2019.2915153.

[17] D. Materassi. Reconstructing tree structures of dynamic systems with hidden nodes under nonlinear
dynamics. In 2016 24th Mediterranean Conference on Control and Automation (MED)  pages 1331–1336 
June 2016.

10

,Firoozeh Sepehr
Donatello Materassi