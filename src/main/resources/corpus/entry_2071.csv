2018,FastGRNN: A Fast  Accurate  Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network,This paper develops the FastRNN and FastGRNN algorithms to address the twin RNN limitations of inaccurate training and inefficient prediction. Previous approaches have improved accuracy at the expense of prediction costs making them infeasible for resource-constrained and real-time applications. Unitary RNNs have increased accuracy somewhat by restricting the range of the state transition matrix's singular values but have also increased the model size as they require a larger number of hidden units to make up for the loss in expressive power. Gated RNNs have obtained state-of-the-art accuracies by adding extra parameters thereby resulting in even larger models. FastRNN addresses these limitations by adding a residual connection that does not constrain the range of the singular values explicitly and has only two extra scalar parameters. FastGRNN then extends the residual connection to a gate by reusing the RNN matrices to match state-of-the-art gated RNN accuracies but with a 2-4x smaller model. Enforcing FastGRNN's matrices to be low-rank  sparse and quantized resulted in accurate models that could be up to 35x smaller than leading gated and unitary RNNs. This allowed FastGRNN to accurately recognize the "Hey Cortana" wakeword with a 1 KB model and to be deployed on severely resource-constrained IoT microcontrollers too tiny to store other RNN models. FastGRNN's code is available at (https://github.com/Microsoft/EdgeML/).,FastGRNN: A Fast  Accurate  Stable and Tiny
Kilobyte Sized Gated Recurrent Neural Network

Aditya Kusupati†  Manish Singh§  Kush Bhatia‡ 
Ashish Kumar‡  Prateek Jain† and Manik Varma†

†Microsoft Research India

§Indian Institute of Technology Delhi
‡University of California Berkeley

{t-vekusu prajain manik}@microsoft.com  singhmanishiitd@gmail.com

kush@cs.berkeley.edu  ashish_kumar@berkeley.edu

Abstract

This paper develops the FastRNN and FastGRNN algorithms to address the twin
RNN limitations of inaccurate training and inefﬁcient prediction. Previous ap-
proaches have improved accuracy at the expense of prediction costs making them
infeasible for resource-constrained and real-time applications. Unitary RNNs have
increased accuracy somewhat by restricting the range of the state transition matrix’s
singular values but have also increased the model size as they require a larger num-
ber of hidden units to make up for the loss in expressive power. Gated RNNs have
obtained state-of-the-art accuracies by adding extra parameters thereby resulting
in even larger models. FastRNN addresses these limitations by adding a residual
connection that does not constrain the range of the singular values explicitly and
has only two extra scalar parameters. FastGRNN then extends the residual connec-
tion to a gate by reusing the RNN matrices to match state-of-the-art gated RNN
accuracies but with a 2-4x smaller model. Enforcing FastGRNN’s matrices to be
low-rank  sparse and quantized resulted in accurate models that could be up to
35x smaller than leading gated and unitary RNNs. This allowed FastGRNN to
accurately recognize the "Hey Cortana" wakeword with a 1 KB model and to be
deployed on severely resource-constrained IoT microcontrollers too tiny to store
other RNN models. FastGRNN’s code is available at [30].

1

Introduction

Objective: This paper develops the FastGRNN (an acronym for a Fast  Accurate  Stable and Tiny
Gated Recurrent Neural Network) algorithm to address the twin RNN limitations of inaccurate
training and inefﬁcient prediction. FastGRNN almost matches the accuracies and training times of
state-of-the-art unitary and gated RNNs but has signiﬁcantly lower prediction costs with models
ranging from 1 to 6 Kilobytes for real-world applications.
RNN training and prediction: It is well recognized that RNN training is inaccurate and unstable
as non-unitary hidden state transition matrices could lead to exploding and vanishing gradients for
long input sequences and time series. An equally important concern for resource-constrained and
real-time applications is the RNN’s model size and prediction time. Squeezing the RNN model and
code into a few Kilobytes could allow RNNs to be deployed on billions of Internet of Things (IoT)
endpoints having just 2 KB RAM and 32 KB ﬂash memory [17  29]. Similarly  squeezing the RNN
model and code into a few Kilobytes of the 32 KB L1 cache of a Raspberry Pi or smartphone  could
signiﬁcantly reduce the prediction time and energy consumption and make RNNs feasible for real-

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

time applications such as wake word detection [27  11  12  42  43]  predictive maintenance [46  1] 
human activity recognition [3  2]  etc.
Unitary and gated RNNs: A number of techniques have been proposed to stabilize RNN training
based on improved optimization algorithms [40  26]  unitary RNNs [5  24  37  47  50  54  25] and
gated RNNs [20  13  14]. While such approaches have increased the RNN prediction accuracy they
have also signiﬁcantly increased the model size. Unitary RNNs have avoided gradients exploding and
vanishing by limiting the range of the singular values of the hidden state transition matrix. This has
led to only limited gains in prediction accuracy as the optimal transition matrix might often not be
close to unitary. Unitary RNNs have compensated by learning higher dimensional representations but 
unfortunately  this has led to larger model sizes. Gated RNNs [20  13  14] have stabilized training by
adding extra parameters leading to state-of-the-art prediction accuracies but with models that might
sometimes be even larger than unitary RNNs.
FastRNN: This paper demonstrates that standard RNN training could be stabilized with the addition
of a residual connection [19  44  22  7] having just 2 additional scalar parameters. Residual con-
nections for RNNs have been proposed in [22] and further studied in [7]. This paper proposes the
FastRNN architecture and establishes that a simple variant of [22  7] with learnt weighted residual
connections (2) can lead to provably stable training and near state-of-the-art prediction accuracies
with lower prediction costs than all unitary and gated RNNs. In particular  FastRNN’s prediction
accuracies could be: (a) up to 19% higher than a standard RNN; (b) could often surpass the ac-
curacies of all unitary RNNs and (c) could be just shy of the accuracies of leading gated RNNs.
FastRNN’s empirical performance could be understood on the basis of theorems proving that for an
input sequence with T steps and appropriate setting of residual connection weights: (a) FastRNN
converges to a stationary point within O(1/2) SGD iterations (see Theorem 3.1)  independent of
T   while the same analysis for a standard RNN reveals an upper bound of O(2T ) iterations and (b)
FastRNN’s generalization error bound is independent of T whereas the same proof technique reveals
an exponential bound for standard RNNs.
FastGRNN: Inspired by this analysis  this paper develops the novel FastGRNN architecture by
converting the residual connection to a gate while reusing the RNN matrices. This allowed FastGRNN
to match  and sometimes exceed  state-of-the-art prediction accuracies of LSTM  GRU  UGRNN and
other leading gated RNN techniques while having 2-4x fewer parameters. Enforcing FastGRNN’s
matrices to be low-rank  sparse and quantized led to a minor decrease in the prediction accuracy but
resulted in models that could be up to 35x smaller and ﬁt in 1-6 Kilobytes for many applications. For
instance  using a 1 KB model  FastGRNN could match the prediction accuracies of all other RNNs at
the task of recognizing the "Hey Cortana" wakeword. This allowed FastGRNN to be deployed on IoT
endpoints  such as the Arduino Uno  which were too small to hold other RNN models. On slightly
larger endpoints  such as the Arduino MKR1000 or Due  FastGRNN was found to be 18-42x faster at
making predictions than other leading RNN methods.
Contributions: This paper makes two contributions. First  it rigorously studies the residual connec-
tion based FastRNN architecture which could often outperform unitary RNNs in terms of training
time  prediction accuracy and prediction cost. Second  inspired by FastRNN  it develops the Fast-
GRNN architecture which could almost match state-of-the-art accuracies and training times but with
prediction costs that could be lower by an order of magnitude. FastRNN and FastGRNN’s code can
be downloaded from [30].

2 Related Work

Residual connections: Residual connections have been studied extensively in CNNs [19  44] as
well as RNNs [22  7]. The Leaky Integration Unit architecture [22] proposed residual connections
for RNNs but were unable to learn the state transition matrix due to the problem of exploding
and vanishing gradients. They therefore sampled the state transition matrix from a hand-crafted
distribution with spectral radius less than one. This limitation was addressed in [7] where the state
transition matrix was learnt but the residual connections were applied to only a few hidden units
and with randomly sampled weights. Unfortunately  the distribution from which the weights were
sampled could lead to an ill-conditioned optimization problem. In contrast  the FastRNN architecture
leads to provably stable training with just two learnt weights connected to all the hidden units.

2

(a) FastRNN - Residual Connection

(b) FastGRNN - Gate

Figure 1: Block diagrams for FastRNN (a) and FastGRNN (b). FastGRNN uses shared matrices W 
U to compute both the hidden state ht as well as the gate zt.

Unitary RNNs: Unitary RNNs [5  50  37  24  47  25] stabilize RNN training by learning only well-
conditioned state transition matrices. This limits their expressive power and prediction accuracy while
increasing training time. For instance  SpectralRNN [54] learns a transition matrix with singular
values in 1 ± . Unfortunately  the training algorithm converged only for small  thereby limiting
accuracy on most datasets. Increasing the number of hidden units was found to increase accuracy
somewhat but at the cost of increased training time  prediction time and model size.
Gated RNNs: Gated architectures [20  13  14  23] achieve state-of-the-art classiﬁcation accuracies
by adding extra parameters but also increase model size and prediction time. This has resulted in a
trend to reduce the number of gates and parameters with UGRNN [14] simplifying GRU [13] which
in turn simpliﬁes LSTM [20]. FastGRNN can be seen as a natural simpliﬁcation of UGRNN where
the RNN matrices are reused within the gate and are made low-rank  sparse and quantized so as to
compress the model.
Efﬁcient training and prediction: Efﬁcient prediction algorithms have often been obtained by
making sparsity and low-rank assumptions. Most unitary methods effectively utilize a low-rank
representation of the state transition matrix to control prediction and training complexity [24  54].
Sparsity  low-rank  and quantization were shown to be effective in RNNs [51  39  48]  CNNs [18] 
trees [29] and nearest neighbour classiﬁers [17]. FastGRNN builds on these ideas to utilize low-rank 
sparse and quantized representations for learning kilobyte sized classiﬁers without compromising
on classiﬁcation accuracy. Other approaches to speed up RNN training and prediction are based on
replacing sequential hidden state transitions by parallelizable convolutions [9] or on learning skip
connections [10] so as to avoid evaluating all the hidden states. Such techniques are complementary
to the ones proposed in this paper and can be used to further improve FastGRNN’s performance.

3 FastRNN and FastGRNN

Notation: Throughout the paper  parameters of an RNN are denoted by matrices W ∈ R ˆD×D  U ∈
R ˆD× ˆD and bias vectors b ∈ R ˆD  often using subscripts if multiple vectors are required to specify the
architecture. a (cid:12) b denotes the Hadamard product between a and b  i.e.  (a (cid:12) b)i = ai  bi. (cid:107) · (cid:107)0
denotes the number of non-zeros entries in a matrix or vector. (cid:107) · (cid:107)F  (cid:107) · (cid:107)2 denotes the Frobenius and
spectral norm of a matrix  respectively. Unless speciﬁed  (cid:107) · (cid:107) denotes (cid:107) · (cid:107)2 of a matrix or vector.

a(cid:62)b =(cid:80)

i aibi denotes the inner product of a and b.

Standard RNN architecture [41] is known to be unstable for training due to exploding or vanishing
gradients and hence is shunned for more expensive gated architectures.
This paper studies the FastRNN architecture that is inspired by weighted residual connections [22  19] 
and shows that FastRNN can be signiﬁcantly more stable and accurate than the standard RNN while
preserving its prediction complexity. In particular  Section 3.1.1 demonstrates parameter settings for
FastRNN that guarantee well-conditioned gradients as well as faster convergence rate and smaller
generalization error than the standard RNN. This paper further strengthens FastRNN to develop the
FastGRNN architecture that is more accurate than unitary methods [5  54] and provides comparable
accuracy to the state-of-the-art gated RNNs at 35x less computational cost (see Table 3).

3

htht-1xtWUσβαxtht-1UhtWtanhf(zt)ζ(1-zt)+=3.1 FastRNN
Let X = [x1  . . .   xT ] be the input data where xt ∈ RD denotes the t-th step feature vector. Then 
the goal of multi-class RNNs is to learn a function F : RD×T → {1  . . .   L} that predicts one of L
classes for the given data point X. Standard RNN architecture has a provision to produce an output
at every time step  but we focus on the setting where each data point is associated with a single label
that is predicted at the end of the time horizon T . Standard RNN maintains a vector of hidden state
ht ∈ R ˆD which captures temporal dynamics in the input data  i.e. 
ht = tanh(Wxt + Uht−1 + b).

(1)

As explained in the next section  learning U  W in the above architecture is difﬁcult as the gradient
can have exponentially large (in T ) condition number. Unitary methods explicitly control the
condition number of the gradient but their training time can be signiﬁcantly larger or the generated
model can be less accurate.
Instead  FastRNN uses a simple weighted residual connection to stabilize the training by generating
well-conditioned gradients. In particular  FastRNN updates the hidden state ht as follows:

˜ht = σ(Wxt + Uht−1 + b) 
ht = α˜ht + βht−1 

(2)
where 0 ≤ α  β ≤ 1 are trainable weights that are parameterized by the sigmoid function. σ : R → R
is a non-linear function such as tanh  sigmoid  or ReLU  and can vary across datasets. Given hT   the
label for a given point X is predicted by applying a standard classiﬁer  e.g.  logistic regression to hT .
Typically  α (cid:28) 1 and β ≈ 1 − α  especially for problems with larger T . FastRNN updates hidden
state in a controlled manner with α  β limiting the extent to which the current feature vector xt
updates the hidden state. Also  FastRNN has only 2 more parameters than RNN and require only
ˆD more computations  which is a tiny fraction of per-step computation complexity of RNN. Unlike
unitary methods [5  23  54]  FastRNN does not introduce expensive structural constraints on U and
hence scales well to large datasets with standard optimization techniques [28].

3.1.1 Analysis
This section shows how FastRNN addresses the issue of ill-conditioned gradients  leading to stable
training and smaller generalization error. For simplicity  assume that the label decision function is
one dimensional and is given by f (X) = v(cid:62)hT . Let L(X  y; θ) = L(f (X)  y; θ) be the logistic
loss function for the given labeled data point (X  y) and with parameters θ = (W  U  v). Then  the
gradient of L w.r.t. W  U  v is given by:

T(cid:88)
T(cid:88)

t=0

t=0

Dt

Dt

(cid:32)T−1(cid:89)
(cid:32)T−1(cid:89)

k=t

k=t

∂L
∂U

= α

∂L
∂W

= α

M (U) =(cid:81)T−1

(cid:62)

(cid:62)

(αU

(αU

Dk+1 + βI)

Dk+1 + βI)

(∇hT L)h

(cid:62)
t−1 

(∇hT L)x

(cid:62)
t  

∂L
∂v

=

−y exp (−y · v(cid:62)hT )
1 + exp (−y · v(cid:62)hT )

hT  

(3)

(4)

where ∇hT L = −c(θ) · y · v  and c(θ) =

1+exp (y·v(cid:62)hT ). A critical term in the above expression is:

1

k=t (αU(cid:62)Dk+1 + βI)  whose condition number  κM (U)  is bounded by:

κM (U) ≤ (1 + α
(1 − α

β maxk (cid:107)U(cid:62)Dk+1(cid:107))T−t
β maxk (cid:107)U(cid:62)Dk+1(cid:107))T−t  

(5)
where Dk = diag(σ(cid:48)(Wxk + Uhk−1 + b)) is the Jacobian matrix of the pointwise nonlinearity.
Also if α = 1 and β = 0  which corresponds to standard RNN  the condition number of M (U) can
(cid:107)U(cid:62)Dk+1(cid:107)
λmin(U(cid:62)Dk+1) )T−t where λmin(A) denotes the minimum singular value of A.
be as large as (maxk
Hence  gradient’s condition number for the standard RNN can be exponential in T . This implies that 
relative to the average eigenvalue  the gradient can explode or vanish in certain directions  leading to
unstable training.
In contrast to the standard RNN  if β ≈ 1 and α ≈ 0  then the condition number  κM (U)  for
FastRNN is bounded by a small term. For example  if β = 1 − α and α =
T maxk (cid:107)U(cid:62)Dk+1(cid:107) 

1

(cid:33)
(cid:33)

4

then κM (U) = O(1). Existing unitary methods are also motivated by similar observation. But they
attempt to control the κM (U) by restricting the condition number  κU  of U which can still lead
to ill-conditioned gradients as U(cid:62)Dk+1 might still be very small in certain directions. By using
residual connections  FastRNN is able to address this issue  and hence have faster training and more
accurate model than the state-of-the-art unitary RNNs.
Finally  by using the above observations and a careful perturbation analysis  we can provide the
following convergence and generalization error bounds for FastRNN:
Theorem 3.1 (Convergence Bound). Let [(X1  y1)  . . .   (Xn  yn)] be the given labeled sequential
training data. Let L(θ) = 1
i L(Xi  yi; θ) be the loss function with θ = (W  U  v) be the
parameters of FastRNN architecture (2) with β = 1 − α and α such that 
n

(cid:80)

α ≤ min

1

1

1

4T · |D(cid:107)U(cid:107)2 − 1|  

4T · RU

 

T · |(cid:107)U(cid:107)2 − 1|

(cid:18)

2(cid:107)] ≤ BM :=

SGD  when applied to the data for a maximum of M iteration outputs a solution(cid:98)θ such that:
where D = supθ k (cid:107)Dθ
E[(cid:107)∇θL((cid:98)θ)(cid:107)2

k(cid:107)2. Then  randomized stochastic gradient descent [15]  a minor variation of

O(αT )L(θ0)

M
where RX = maxX (cid:107)X(cid:107)F for X = {U  W  v}  L(θ0) is the loss of the initial classiﬁer  and
  k ∈ [M ]  ¯D ≥ 0.
the step-size of the k-th SGD iteration is ﬁxed as: γk = min
Maximum number of iterations is bounded by M = O( αT
Theorem 3.2 (Generalization Error Bound). [6] Let Y  ˆY ⊆ [0  1] and let FT denote the class
of FastRNN with (cid:107)U(cid:107)F ≤ RU (cid:107)W(cid:107)F ≤ RW. Let the ﬁnal classiﬁer be given by σ(v(cid:62)hT ) 
(cid:107)v(cid:107)2 ≤ Rv . Let L : Y × ˆY → [0  B] be any 1-Lipschitz loss function. Let D be any distribution on
X × Y such that (cid:107)xit(cid:107)2 ≤ Rx a.s. Let 0 ≤ δ ≤ 1. For all β = 1 − α and α such that 

2 · poly(L(θ0)  RWRURv  ¯D))   ≥ 0.

¯D

(cid:110) 1O(αT )  

4RWRURv

≤  

¯D +

T

M

M

√
¯D

+

 

(cid:19)
(cid:19) O(αT )√
(cid:111)

(cid:18)

(cid:18)

(cid:19)

(cid:115)

1

n(cid:88)

α ≤ min

4T · |D(cid:107)U(cid:107)2 − 1|  

4T · RU

 

T · |(cid:107)U(cid:107)2 − 1|

 

1

1

k(cid:107)2  we have that with probability at least 1 − δ  all functions f ∈ v ◦ FT

where D = supθ k (cid:107)Dθ
satisfy 

ED[L(f (X)  y)] ≤ 1
n

L(f (Xi)  yi) + C O(αT )√
n

+ B

ln( 1
δ )
n

 

i=1

where C = RWRURxRv represents the boundedness of the parameter matrices and the data.
The convergence bound states that if α = O(1/T ) then the algorithm converges to a stationary
point in constant time with respect to T and polynomial time with respect to all the other problem
parameters. Generalization bound states that for α = O(1/T )  the generalization error of FastRNN is
independent of T . In contrast  similar proof technique provide exponentially poor (in T ) error bound
and convergence rate for standard RNN. But  this is an upper bound  so potentially signiﬁcantly better
error bounds for RNN might exist; matching lower bound results for standard RNN is an interesting
research direction. Also  O(T 2) generalization error bound can be argued using VC-dimension style
arguments [4]. But such bounds hold for speciﬁc settings like binary y  and are independent of
problem hardness parameterized by the size of the weight matrices (RW  RU).
Finally  note that the above analysis ﬁxes α = O(1/T )  β = 1 − α  but in practice FastRNN learns
α  β (which is similar to performing cross-validation on α  β). However  interestingly  across datasets
the learnt α  β values indeed display a similar scaling wrt T for large T (see Figure 2).

3.2 FastGRNN

While FastRNN controls the condition number of gradient reasonably well  its expressive power
might be limited for some datasets. This concern is addressed by a novel architecture  FastGRNN 
that uses a scalar weighted residual connection for each and every coordinate of the hidden state ht.
That is 

zt = σ(Wxt + Uht−1 + bz) 
˜ht = tanh(Wxt + Uht−1 + bh) 
ht = (ζ(1 − zt) + ν) (cid:12) ˜ht + zt (cid:12) ht−1 

(6)

5

where 0 ≤ ζ  ν ≤ 1 are trainable parameters that are parameterized by the sigmoid function  and
σ : R → R is a non-linear function such as tanh  sigmoid and can vary across datasets. Note that
each coordinate of zt is similar to parameter β in (2) and ζ(1 − zt) + ν’s coordinates simulate α
parameter; also if ν ≈ 0  ζ ≈ 1 then it satisﬁes the intuition that α + β = 1. It was observed that
across all datasets  this gating mechanism outperformed the simple vector extension of FastRNN
where each coordinate of α and β is learnt (see Appendix G).
FastGRNN computes each coordinate of gate zt using a non-linear function of xt and ht−1. To
minimize the number of parameters  FastGRNN reuses the matrices W  U for the vector-valued
gating function as well. Hence  FastGRNN’s inference complexity is almost same as that of the
standard RNN but its accuracy and training stability is on par with expensive gated architectures like
GRU and LSTM.
Sparse low-rank representation: FastGRNN further compresses the model size by using a low-rank
and a sparse representation of the parameter matrices W  U. That is 

W = W1(W2)(cid:62)  U = U1(U2)(cid:62)  (cid:107)Wi(cid:107)0 ≤ si

(7)
where W1 ∈ R ˆD×rw   W2 ∈ RD×rw  and U1  U2 ∈ R ˆD×ru. Hyperparameters rw  sw  ru  su
provide an efﬁcient way to control the accuracy-memory trade-off for FastGRNN and are typically
set via ﬁne-grained validation. In particular  such compression is critical for FastGRNN model to ﬁt
on resource-constrained devices. Second  this low-rank representation brings down the prediction
time by reducing the cost at each time step from O( ˆD(D + ˆD)) to O(rw(D + ˆD) + ru ˆD). This
enables FastGRNN to provide on-device prediction in real-time on battery constrained devices.

w  (cid:107)Ui(cid:107)0 ≤ si

u  i = {1  2} 

3.2.1 Training FastGRNN

The parameters for FastGRNN: ΘFastGRNN = (Wi  Ui  bh  bz  ζ  ν) are trained jointly using pro-
jected batch stochastic gradient descent (b-SGD) (or other stochastic optimization methods) with
typical batch sizes ranging from 64 − 128. In particular  the optimization problem is given by:

(cid:88)

j

ΘFastGRNN (cid:107)Wi(cid:107)0≤si

min
w  (cid:107)Ui(cid:107)0≤si

u i∈{1 2}

J (ΘFastGRNN) =

1
n

L(Xj  yj; ΘFastGRNN)

(8)

where L denotes the appropriate loss function (typically softmax cross-entropy). The training
procedure for FastGRNN is divided into 3 stages:
(I) Learning low-rank representation (L): In the ﬁrst stage of the training  FastGRNN is trained
for e1 epochs with the model as speciﬁed by (7) using b-SGD. This stage of optimization ignores the
sparsity constraints on the parameters and learns a low-rank representation of the parameters.
(II) Learning sparsity structure (S): FastGRNN is next trained for e2 epochs using b-SGD  pro-
jecting the parameters onto the space of sparse low-rank matrices after every few batches while
maintaining support between two consecutive projection steps. This stage  using b-SGD with Iterative
Hard Thresholding (IHT)  helps FastGRNN identify the correct support for parameters (Wi  Ui).
(III) Optimizing with ﬁxed parameter support: In the last stage  FastGRNN is trained for e3
epochs with b-SGD while freezing the support set of the parameters.
In practice  it is observed that e1 = e2 = e3 = 100 generally leads to the convergence of FastGRNN
to a good solution. Early stopping is often deployed in stages (II) and (III) to obtain the best models.

3.3 Byte Quantization (Q)

FastGRNN further compresses the model by quantizing each element of Wi  Ui  restricting them to
at most one byte along with byte indexing for sparse models. However  simple integer quantization
of Wi  Ui leads to a large loss in accuracy due to gross approximation. Moreover  while such a
quantization reduces the model size  the prediction time can still be large as non-linearities will
require all the hidden states to be ﬂoating point. FastGRNN overcomes these shortcomings by training
Wi and Ui using piecewise-linear approximation of the non-linear functions  thereby ensuring that
all the computations can be performed with integer arithmetic. During training  FastGRNN replaces
the non-linear function in (6) with their respective approximations and uses the above mentioned
training procedure to obtain ΘFastGRNN. The ﬂoating point parameters are then jointly quantized
to ensure that all the relevant entities are integer-valued and the entire inference computation can

6

be executed efﬁciently with integer arithmetic without a signiﬁcant drop in accuracy. For instance 
Tables 4  5 show that on several datasets FastGRNN models are 3-4x faster than their corresponding
FastGRNN-Q models on common IoT boards with no ﬂoating point unit (FPU). FastGRNN-LSQ 
FastGRNN "minus" the Low-rank  Sparse and Quantized components  is the base model with no
compression.

4 Experiments

Datasets: FastRNN and FastGRNN’s performance was benchmarked on the following IoT tasks
where having low model sizes and prediction times was critical to the success of the application: (a)
Wakeword-2 [45] - detecting utterances of the "Hey Cortana" wakeword; (b) Google-30 [49] and
Google-12 - detection of utterances of 30 and 10 commands plus background noise and silence and
(c) HAR-2 [3] and DSA-19 [2] - Human Activity Recognition (HAR) from an accelerometer and
gyroscope on a Samsung Galaxy S3 smartphone and Daily and Sports Activity (DSA) detection
from a resource-constrained IoT wearable device with 5 Xsens MTx sensors having accelerometers 
gyroscopes and magnetometers on the torso and four limbs. Traditional RNN tasks typically do
not have prediction constraints and are therefore not the focus of this paper. Nevertheless  for the
sake of completeness  experiments were also carried out on benchmark RNN tasks such as language
modeling on the Penn Treebank (PTB) dataset [33]  star rating prediction on a scale of 1 to 5 of Yelp
reviews [52] and classiﬁcation of MNIST images on a pixel-by-pixel sequence [32  31].
All datasets  apart from Wakeword-2  are publicly available and their pre-processing and feature
extraction details are provided in Appendix B. The publicly provided training set for each dataset
was subdivided into 80% for training and 20% for validation. Once the hyperparameters had been
ﬁxed  the algorithms were trained on the full training set and results were reported on the publicly
available test set. Table 1 lists the statistics of all datasets.
Baseline algorithms and Implementation: FastRNN and FastGRNN were compared to stan-
dard RNN [41]  leading unitary RNN approaches such as SpectralRNN [54]  Orthogonal RNN
(oRNN) [37]  Efﬁcient Unitary Recurrent Neural Networks (EURNN) [24]  FactoredRNN [47] and
state-of-the-art gated RNNs including UGRNN [14]  GRU [13] and LSTM [20]. Details of these
methods are provided in Section 2. Native Tensorﬂow implementations were used for the LSTM
and GRU architectures. For all the other RNNs  publicly available implementations provided by the
authors were used taking care to ensure that published results could be reproduced thereby verifying
the code and hyper-parameter settings. All experiments were run on an Nvidia Tesla P40 GPU with
CUDA 9.0 and cuDNN 7.1 on a machine with an Intel Xeon 2.60 GHz CPU with 12 cores.
Hyper-parameters: The hyper-parameters of each algorithm were set by a ﬁne-grained validation
wherever possible or according to the settings recommended by the authors otherwise. Adam 
Nesterov Momentum and SGD were used to optimize each algorithm on each dataset and the
optimizer with the best validation performance was selected. The learning rate was initialized to
10−2 for all architectures except for RNNs where the learning rate was initialized to 10−3 to ensure
stable training. Each algorithm was run for 200 epochs after which the learning rate was decreased by
a factor of 10−1 and the algorithm run again for another 100 epochs. This procedure was carried out
on all datasets except for Pixel MNIST where the learning rate was decayed by 1
2 after each pass of
200 epochs. Batch sizes between 64 and 128 training points were tried for most architectures and a
batch size of 100 was found to work well in general except for standard RNNs which required a batch
size of 512. FastRNN used tanh as the non-linearity in most cases except for a few (indicated by +)
where ReLU gave slightly better results. Table 11 in the Appendix lists the non-linearity  optimizer
and hyper-parameter settings for FastGRNN on all datasets.

Table 1: Dataset Statistics

Dataset

#Train

#Features

Google-12
Google-30
Wakeword-2
Yelp-5
HAR-2
Pixel-MNIST-10
PTB-10000
DSA-19

22 246
51 088
195 800
500 000
7 352
60 000
929 589
4 560

3 168
3 168
5 184
38 400
1 152
784
—
5 625

#Time
Steps
99
99
162
300
128
784
300
125

#Test

3 081
6 835
83 915
500 000
2 947
10 000
82 430
4 560

7

Table 2: PTB Language Modeling - 1 Layer
Train

Model

Train

Test

Method

Perplexity

Perplexity

Size (KB)

Time (min)

RNN
FastRNN
FastGRNN-LSQ
FastGRNN
SpectralRNN
UGRNN
LSTM

144.71
127.76+
115.92
116.11
130.20
119.71
117.41

68.11
109.07
89.58
81.31
65.42
65.25
69.44

129
513
513
39
242
256
2052

9.11
11.20
12.53
13.75

—

11.12
13.52

Evaluation criteria: The emphasis in this paper is on designing RNN architectures which can run
on low-memory IoT devices and which are efﬁcient at prediction time. As such  the model size of
each architecture is reported along with its training time and classiﬁcation accuracy (F1 score on the
Wakeword-2 dataset and perplexity on the PTB dataset). Prediction times on some of the popular
IoT boards are also reported. Note that  for NLP applications such as PTB and Yelp  just the model
size of the various RNN architectures has been reported. In a real application  the size of the learnt
word-vector embeddings (10 MB for FastRNN and FastGRNN) would also have to be considered.
Results: Tables 2 and 3 compare the performance of FastRNN  FastGRNN and FastGRNN-LSQ to
state-of-the-art RNNs. Three points are worth noting about FastRNN’s performance. First  FastRNN’s
prediction accuracy gains over a standard RNN ranged from 2.34% on the Pixel-MNIST dataset
to 19% on the Google-12 dataset. Second  FastRNN’s prediction accuracy could surpass leading
unitary RNNs on 6 out of the 8 datasets with gains up to 2.87% and 3.77% over SpectralRNN on the
Google-12 and DSA-19 datasets respectively. Third  FastRNN’s training speedups over all unitary
and gated RNNs could range from 1.2x over UGRNN on the Yelp-5 and DSA-19 datasets to 196x
over EURNN on the Google-12 dataset. This demonstrates that the vanishing and exploding gradient
problem could be overcome by the addition of a simple weighted residual connection to the standard
RNN architecture thereby allowing FastRNN to train efﬁciently and stablely. This also demonstrates
that the residual connection offers a theoretically principled architecture that can often result in
accuracy gains without limiting the expressive power of the hidden state transition matrix.
Tables 2 and 3 also demonstrate that FastGRNN-LSQ could be more accurate and faster to train than
all unitary RNNs. Furthermore  FastGRNN-LSQ could match the accuracies and training times of
state-of-the-art gated RNNs while having models that could be 1.18-4.87x smaller. This demonstrates
that extending the residual connection to a gate which reuses the RNN matrices increased accuracy
with virtually no increase in model size over FastRNN in most cases. In fact  on Google-30 and
Pixel-MNIST FastGRNN-LSQ’s model size was lower than FastRNN’s as it had a lower hidden
dimension indicating that the gate efﬁciently increased expressive power.
Finally  Tables 2 and 3 show that FastGRNN’s accuracy was at most 1.13% worse than the best RNN
but its model could be up to 35x smaller even as compared to low-rank unitary methods such as
SpectralRNN. Figures 3 and 4 in the Appendix also show that FastGRNN-LSQ and FastGRNN’s
classiﬁcation accuracies could be higher than those obtained by the best unitary and gated RNNs
for any given model size in the 0-128 KB range. This demonstrates the effectiveness of making
FastGRNN’s parameters low-rank  sparse and quantized and allows FastGRNN to ﬁt on the Arduino

Table 3: FastGRNN had up to 35x smaller models than leading RNNs with almost no loss in accuracy

Dataset

Method

RNN

d FastRNN

FastGRNN-LSQ
FastGRNN

y SpectralRNN

EURNN
oRNN
FactoredRNN

e
s
o
p
o
r
P

r
a
t
i
n
U

d UGRNN

e
t
a
G

GRU
LSTM

Dataset

Method

RNN

e
s
o
p
o
r
P

r
a
t
i
n
U

d FastRNN

FastGRNN-LSQ
FastGRNN

y SpectralRNN

EURNN
oRNN
FactoredRNN

d UGRNN

e
t
a
G

GRU
LSTM

Accuracy

(%)
73.25
92.21+
93.18
92.10

91.59
76.79
88.18
53.33
92.63
93.15
92.30

Google-12

Model

Size (KB)

56
56
57
5.5

228
210
102
1114
75
248
212

Train

Time (hr)
1.11
0.61
0.63
0.75

Accuracy

(%)
80.05
91.60+
92.03
90.78

19.00
120.00
16.00
7.00
0.78
1.23
1.36

88.73
56.35
86.95
40.57
90.54
91.41
90.31

Accuracy

(%)
47.59
55.38
59.51
59.43

56.56
59.01

—
—

58.67
59.02
59.49

Yelp-5

RNN Model
Size (KB)

Train

Time (hr)

130
130
130
8

89
122
—
—
258
388
516

3.33
3.61
3.91
4.62

4.92
72.00

—
—
4.34
8.12
8.61

Accuracy

(%)
91.31
94.50+
95.38
95.59

95.48
93.11
94.57
78.65
94.53
93.62
93.65

HAR-2
Model

Size (KB)

Train

Time (hr)

29
29
29
3

525
12
22
1
37
71
74

0.11
0.06
0.08
0.10

0.73
0.84
2.72
0.11
0.12
0.13
0.18

8

63
96
45
6.25

128
135
120
1150
260
257
219

Accuracy

(%)
71.68
84.14
85.00
83.73

80.37

—

72.52
73.20
84.74
84.84
84.84

Google-30

Model

Size (KB)

Train

Time (hr)

F1

Score
89.17
97.09
98.19
97.83

96.75
92.22

—
—

98.17
97.63
97.82

Wakeword-2
Model

Size (KB)

Train

Time(hr)

8
8
8
1

17
24
—
—
16
24
32

0.28
0.69
0.83
1.08

7.00
69.00

—
—
1.00
1.38
1.71

Pixel-MNIST-10

2.13
1.30
1.41
1.77

11.00
19.00
35.00
8.52
2.11
2.70
2.63

DSA-19
Model

Size (KB)

Train

Time (min)

20
97
208
3.25

50
—
18
1154
399
270
526

1.11
1.92
2.15
2.10

2.25
—
—
—
2.31
2.33
2.58

Accuracy

(%)
94.10
96.44
98.72
98.20

97.70
95.38
97.20
94.60
97.29
98.70
97.80

Model

Size (KB)

71
166
71
6

25
64
49
125
84
123
265

Train

Time (hr)
45.56
15.10
12.57
16.97

122.00

—

—
—

15.17
23.67
26.57

Table 4: Prediction time in ms on the Arduino MKR1000

Method
FastGRNN
FastGRNN-Q
RNN
UGRNN
SpectralRNN

Google-12 HAR-2 Wakeword-2

537
2282
12028
22875
70902

162
553
2249
4207
—

175
755
2232
6724
10144

Table 5: Prediction time in ms on the Arduino Due
Google-12 HAR-2 Wakeword-2
Method
FastGRNN
FastGRNN-Q
RNN
UGRNN
SpectralRNN

62
172
590
1142
55558

242
779
3472
6693
17766

77
238
653
1823
2691

Uno having just 2 KB RAM and 32 KB ﬂash memory. In particular  FastGRNN was able to recognize
the "Hey Cortana" wakeword just as accurately as leading RNNs but with a 1 KB model.
Prediction on IoT boards: Unfortunately  most RNNs were too large to ﬁt on an Arduino Uno apart
from FastGRNN. On the slightly more powerful Arduino MKR1000 having an ARM Cortex M0+
microcontroller operating at 48 MHz with 32 KB RAM and 256 KB ﬂash memory  Table 4 shows
that FastGRNN could achieve the same prediction accuracy while being 25-45x faster at prediction
than UGRNN and 57-132x faster than SpectralRNN. Results on the even more powerful Arduino Due
are presented in Table 5 while results on the Raspberry Pi are presented in Table 12 of the Appendix.
Ablations  extensions and parameter settings: Enforcing that FastGRNN’s matrices be low-rank
led to a slight increase in prediction accuracy and reduction in prediction costs as shown in the
ablation experiments in Tables 8  9 and 10 in the Appendix. Adding sparsity and quantization
led to a slight drop in accuracy but resulted in signiﬁcantly smaller models. Next  Table 16 in
the Appendix shows that regularization and layering techniques [36] that have been proposed to
increase the prediction accuracy of other gated RNNs are also effective for FastGRNN and can lead
to reductions in perplexity on the PTB dataset. Finally  Figure 2 and Table 7 of the Appendix measure
the agreement between FastRNN’s theoretical analysis and empirical observations. Figure 2 (a) shows
that the α learnt on datasets with T time steps is decreasing function of T and Figure 2 (b) shows
that the learnt α and β follow the relation α/β ≈ O(1/T ) for large T which is one of the settings
in which FastRNN’s gradients stabilize and training converges quickly as proved by Theorems 3.1
and 3.2. Furthermore  β can be seen to be close to 1 − α for large T in Figure 2 (c) as assumed in
Section 3.1.1 for the convergence of long sequences. For instance  the relative error between β and
1 − α for Google-12 with 99 timesteps was 2.15%  for HAR-2 with 128 timesteps was 3.21% and for
MNIST-10 with 112 timesteps was 0.68%. However  for short sequences where there was a lower
likelihood of gradients exploding or vanishing  β was found to deviate signiﬁcantly from 1 − α as
this led to improved prediction accuracy. Enforcing that β = 1 − α on short sequences was found to
drop accuracy by up to 1.5%.

(a)

(b)

(c)

Figure 2: Plots (a) and (b) show the variation of α and α/β of FastRNN with respect to 1/T for three datasets.
Plot (c) shows the relation between β and 1 − α. In accordance with Theorem 3.1  the learnt values of α and
α/β scale as O(1/T ) while β → 1 − α for long sequences.

5 Conclusions

This paper proposed the FastRNN and FastGRNN architectures for efﬁcient RNN training and
prediction. FastRNN could lead to provably stable training by incorporating a residual connection
with two scalar parameters into the standard RNN architecture. FastRNN was demonstrated to have
lower training times  lower prediction costs and higher prediction accuracies than leading unitary
RNNs in most cases. FastGRNN extended the residual connection to a gate reusing the RNN matrices
and was able to match the accuracies of state-of-the-art gated RNNs but with signiﬁcantly lower
prediction costs. FastGRNN’s model could be compressed to 1-6 KB without compromising accuracy
in many cases by enforcing that its parameters be low-rank  sparse and quantized. This allowed
FastGRNN to make accurate predictions efﬁciently on severely resource-constrained IoT devices too
tiny to hold other RNN models.

9

00.050.10.151/T00.20.40.6Google-12HAR-2MNIST-1000.050.10.151/T00.20.40.60.8Google-12HAR-2MNIST-100.40.50.60.70.80.910.80.91Google - 12HAR-2MNIST-10Acknowledgements

We are grateful to Ankit Anand  Niladri Chatterji  Kunal Dahiya  Don Dennis  Inderjit S. Dhillon 
Dinesh Khandelwal  Shishir Patil  Adithya Pratapa  Harsha Vardhan Simhadri and Raghav Somani
for helpful discussions and feedback. KB acknowledges the support of the NSF through grant
IIS-1619362 and of the AFOSR through grant FA9550-17-1-0308.

References
[1] S. Ahmad  A. Lavin  S. Purdy  and Z. Agha. Unsupervised real-time anomaly detection for

streaming data. Neurocomputing  262:134–147  2017.

[2] K. Altun  B. Barshan  and O. Tunçel. Comparative study on classifying human activities with
miniature inertial and magnetic sensors. Pattern Recognition  43(10):3605–3620  2010. URL
https://archive.ics.uci.edu/ml/datasets/Daily+and+Sports+Activities.

[3] D. Anguita  A. Ghio  L. Oneto  X. Parra  and J. L. Reyes-Ortiz. Human activity
recognition on smartphones using a multiclass hardware-friendly support vector machine.
In International Workshop on Ambient Assisted Living  pages 216–223. Springer  2012.
URL https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+
using+smartphones.

[4] M. Anthony and P. L. Bartlett. Neural network learning: Theoretical foundations. Cambridge

University Press  2009.

[5] M. Arjovsky  A. Shah  and Y. Bengio. Unitary evolution recurrent neural networks.

International Conference on Machine Learning  pages 1120–1128  2016.

In

[6] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3(Nov):463–482  2002.

[7] Y. Bengio  N. Boulanger-Lewandowski  and R. Pascanu. Advances in optimizing recurrent
networks. In Acoustics  Speech and Signal Processing (ICASSP)  2013 IEEE International
Conference on  pages 8624–8628. IEEE  2013.

[8] K. Bhatia  K. Dahiya  H. Jain  Y. Prabhu  and M. Varma. The Extreme Classiﬁcation
Repository: Multi-label Datasets & Code. URL http://manikvarma.org/downloads/
XC/XMLRepository.html.

[9] J. Bradbury  S. Merity  C. Xiong  and R. Socher. Quasi-recurrent neural networks. arXiv

preprint arXiv:1611.01576  2016.

[10] V. Campos  B. Jou  X. G. i Nieto  J. Torres  and S.-F. Chang. Skip RNN: Learning to skip state
updates in recurrent neural networks. In International Conference on Learning Representations 
2018.

[11] G. Chen  C. Parada  and G. Heigold. Small-footprint keyword spotting using deep neural
networks. In Acoustics  Speech and Signal Processing (ICASSP)  2015 IEEE International
Conference on  pages 4087–4091. IEEE  2014.

[12] G. Chen  C. Parada  and T. N. Sainath. Query-by-example keyword spotting using long short-
term memory networks. In Acoustics  Speech and Signal Processing (ICASSP)  2015 IEEE
International Conference on  pages 5236–5240. IEEE  2015.

[13] K. Cho  B. Van Merriënboer  D. Bahdanau  and Y. Bengio. On the properties of neural machine

translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259  2014.

[14] J. Collins  J. Sohl-Dickstein  and D. Sussillo. Capacity and trainability in recurrent neural

networks. arXiv preprint arXiv:1611.09913  2016.

[15] S. Ghadimi and G. Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic

programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[16] N. Golowich  A. Rakhlin  and O. Shamir. Size-independent sample complexity of neural

networks. arXiv preprint arXiv:1712.06541  2017.

[17] C. Gupta  A. S. Suggala  A. Gupta  H. V. Simhadri  B. Paranjape  A. Kumar  S. Goyal  R. Udupa 
M. Varma  and P. Jain. Protonn: Compressed and accurate knn for resource-scarce devices. In
Proceedings of the International Conference on Machine Learning  August 2017.

10

[18] S. Han  H. Mao  and W. J. Dally. Deep compression: Compressing deep neural networks with

pruning  trained quantization and huffman coding. In ICLR  2016.

[19] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
770–778  2016.

[20] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation  9(8):

1735–1780  1997.

[21] H. Inan  K. Khosravi  and R. Socher. Tying word vectors and word classiﬁers: A loss framework

for language modeling. arXiv preprint arXiv:1611.01462  2016.

[22] H. Jaeger  M. Lukosevicius  D. Popovici  and U. Siewert. Optimization and applications of

echo state networks with leaky-integrator neurons. Neural Networks  20(3):335–352  2007.

[23] L. Jing  C. Gulcehre  J. Peurifoy  Y. Shen  M. Tegmark  M. Soljaci´c  and Y. Bengio. Gated

orthogonal recurrent units: On learning to forget. arXiv preprint arXiv:1706.02761  2017.

[24] L. Jing  Y. Shen  T. Dubcek  J. Peurifoy  S. Skirlo  M. Tegmark  and M. Soljaci´c. Tunable efﬁ-
cient unitary neural networks (eunn) and their application to RNN. In International Conference
on Machine Learning  2017.

[25] C. Jose  M. Cisse  and F. Fleuret. Kronecker recurrent units. In J. Dy and A. Krause  editors 
International Conference on Machine Learning  volume 80 of Proceedings of Machine Learning
Research  pages 2380–2389  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018. PMLR.
[26] S. Kanai  Y. Fujiwara  and S. Iwamura. Preventing gradient explosions in gated recurrent units.

In Advances in Neural Information Processing Systems  pages 435–444  2017.

[27] V. Këpuska and T. Klein. A novel wake-up-word speech recognition system  wake-up-word
recognition task  technology and evaluation. Nonlinear Analysis: Theory  Methods & Applica-
tions  71(12):e2772–e2789  2009.

[28] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[29] A. Kumar  S. Goyal  and M. Varma. Resource-efﬁcient machine learning in 2 kb ram for the
internet of things. In Proceedings of the International Conference on Machine Learning  August
2017.

[30] A. Kusupati  D. Dennis  C. Gupta  A. Kumar  S. Patil  and H. Simhadri. The EdgeML Li-
brary: An ML library for machine learning on the Edge  2017. URL https://github.com/
Microsoft/EdgeML.

[31] Q. V. Le  N. Jaitly  and G. E. Hinton. A simple way to initialize recurrent networks of rectiﬁed

linear units. arXiv preprint arXiv:1504.00941  2015.

[32] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[33] M. P. Marcus  M. A. Marcinkiewicz  and B. Santorini. Building a large annotated corpus of

english: The penn treebank. Computational linguistics  19(2):313–330  1993.

[34] J. McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating dimensions
with review text. In Proceedings of the 7th ACM conference on Recommender systems  pages
165–172. ACM  2013.

[35] G. Melis  C. Dyer  and P. Blunsom. On the state of the art of evaluation in neural language

models. arXiv preprint arXiv:1707.05589  2017.

[36] S. Merity  N. S. Keskar  and R. Socher. Regularizing and optimizing LSTM language models.

arXiv preprint arXiv:1708.02182  2017.

[37] Z. Mhammedi  A. Hellicar  A. Rahman  and J. Bailey. Efﬁcient orthogonal parametrisation
of recurrent neural networks using householder reﬂections. In International Conference on
Machine Learning  2017.

[38] T. Mikolov and G. Zweig. Context dependent recurrent neural network language model. SLT 

12(234-239):8  2012.

[39] S. Narang  E. Elsen  G. Diamos  and S. Sengupta. Exploring sparsity in recurrent neural

networks. arXiv preprint arXiv:1704.05119  2017.

11

[40] R. Pascanu  T. Mikolov  and Y. Bengio. On the difﬁculty of training recurrent neural networks.

In International Conference on Machine Learning  pages 1310–1318  2013.

[41] D. E. Rumelhart  G. E. Hinton  and R. J. Williams. Learning representations by back-propagating

errors. Nature  323(6088):533  1986.

[42] T. N. Sainath and C. Parada. Convolutional neural networks for small-footprint keyword spotting.
In Sixteenth Annual Conference of the International Speech Communication Association  2015.
[43] Siri Team  Apple. Hey Siri: An on-device dnn-powered voice trigger for apple’s personal
assistant  2017. URL https://machinelearning.apple.com/2017/10/01/hey-siri.
html.

[44] R. K. Srivastava  K. Greff  and J. Schmidhuber. Highway networks.

arXiv:1505.00387  2015.

arXiv preprint

[45] STCI  Microsoft. Wakeword dataset.
[46] G. A. Susto  A. Schirru  S. Pampuri  S. McLoone  and A. Beghi. Machine learning for predictive
maintenance: A multiple classiﬁer approach. IEEE Transactions on Industrial Informatics  11
(3):812–820  2015.

[47] E. Vorontsov  C. Trabelsi  S. Kadoury  and C. Pal. On orthogonality and learning recurrent
networks with long term dependencies. In International Conference on Machine Learning 
2017.

[48] Z. Wang  J. Lin  and Z. Wang. Accelerating recurrent neural networks: A memory-efﬁcient
approach. IEEE Transactions on Very Large Scale Integration (VLSI) Systems  25(10):2763–
2775  2017.
[49] P. Warden.

Speech commands: A dataset for limited-vocabulary speech recognition.
arXiv preprint arXiv:1804.03209  2018. URL http://download.tensorflow.org/data/
speech_commands_v0.01.tar.gz.

[50] S. Wisdom  T. Powers  J. Hershey  J. Le Roux  and L. Atlas. Full-capacity unitary recurrent
neural networks. In Advances in Neural Information Processing Systems  pages 4880–4888 
2016.

[51] J. Ye  L. Wang  G. Li  D. Chen  S. Zhe  X. Chu  and Z. Xu. Learning compact recurrent neural

networks with block-term tensor decomposition. arXiv preprint arXiv:1712.05134  2017.

[52] Yelp Inc. Yelp dataset challenge  2017. URL https://www.yelp.com/dataset/

challenge.

[53] W. Zaremba  I. Sutskever  and O. Vinyals. Recurrent neural network regularization. arXiv

preprint arXiv:1409.2329  2014.

[54] J. Zhang  Q. Lei  and I. S. Dhillon. Stabilizing gradients for deep neural networks via efﬁcient

SVD parameterization. In International Conference on Machine Learning  2018.

12

,Aditya Kusupati
Manish Singh
Kush Bhatia
Ashish Kumar
Prateek Jain
Manik Varma