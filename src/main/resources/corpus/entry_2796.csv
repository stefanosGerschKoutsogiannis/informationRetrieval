2009,Label Selection on Graphs,We investigate methods for selecting sets of labeled vertices for use in predicting the labels of vertices on a graph.  We specifically study methods which choose a single batch of labeled vertices (i.e. offline  non sequential methods).  In this setting  we find common graph smoothness assumptions directly motivate simple label selection methods with interesting theoretical guarantees. These methods bound prediction error in terms of the smoothness of the true labels with respect to the graph.  Some of these bounds give new motivations for previously proposed algorithms  and some suggest new algorithms which we evaluate.  We show improved performance over baseline methods on several real world data sets.,Label Selection on Graphs

Andrew Guillory

Department of Computer Science

University of Washington

Jeff Bilmes

Department of Electrical Engineering

University of Washington

guillory@cs.washington.edu

bilmes@ee.washington.edu

Abstract

We investigate methods for selecting sets of labeled vertices for use in predicting
the labels of vertices on a graph. We speciﬁcally study methods which choose
a single batch of labeled vertices (i.e. ofﬂine  non sequential methods). In this
setting  we ﬁnd common graph smoothness assumptions directly motivate simple
label selection methods with interesting theoretical guarantees. These methods
bound prediction error in terms of the smoothness of the true labels with respect
to the graph. Some of these bounds give new motivations for previously proposed
algorithms  and some suggest new algorithms which we evaluate. We show im-
proved performance over baseline methods on several real world data sets.

1 Introduction

In this work we consider learning on a graph. Assume we have an undirected graph of n nodes
given by a symmetric weight matrix W . The ith node in the graph has a label yi ∈ {0  1} stored in
a vector of labels y ∈ {0  1}n. We want to predict all of y from the labels yL for a labeled subset
L ⊂ V = [n]. V is the set of all vertices. We use ˆy ∈ {0  1}n to denote our predicted labels. The
number of incorrect predictions is ||y − ˆy||2.
Graph-based learning is an interesting alternative to traditional feature-based learning.
In many
problems  graph representations are more natural than feature vector representations. When clas-
sifying web pages  for example  edge weights in the graph may incorporate information about hy-
perlinks. Even when the original data is represented as feature vectors  transforming the data into a
graph (for example using a Gaussian kernel to compute weights between points) can be convenient
(cid:80)
for exploiting properties of a data set.
In order to bound prediction error  we assume that the labels are smoothly varying with respect
i j Wi j|yi − yj| is
to the underlying graph. The simple smoothness assumption we use is that
small. Here || denotes absolute value  but the labels are binary so we can equivalently use squared
difference. This smoothness assumption has been used by graph-based semi-supervised learning
algorithms which compute ˆy using a labeled set L chosen uniformly at random from V [Blum and
Chawla  2001  Hanneke  2006  Pelckmans et al.  2007  Bengio et al.  2006] and by online graph la-
beling methods that operate on an adversarially ordered stream of vertices [Pelckmans and Suykens 
2008  Brautbar  2009  Herbster et al.  2008  2005  Herbster  2008]
In this work we consider methods that make use of the smoothness assumption and structure of the
graph in order to both select L as well as make predictions. Our hope is to achieve higher prediction
accuracy as compared to random label selection and other methods for choosing L. We are particu-
larly interested in batch ofﬂine methods which select L up front  receive yL and then predict ˆy. The
single batch  ofﬂine label selection problem is important in many real-world applications because it
is often the case that problem constraints make requesting more than one batch of labels very costly.
For example  if requesting a label involves a time consuming  expensive experiment (potentially

1

involving human subjects)  it may be signiﬁcantly less costly to run a single batch of experiments in
parallel as compared to running experiments in series.
i j Wi j|yi − yj| is small  guarantee the
We give several methods which  under the assumption
prediction error ||y − ˆy||2 will also be small. Some of the bounds provide interesting justiﬁcations
for previously used methods  and we show improved performance over random label selection and
baseline submodular maximization methods on several real world data sets.

(cid:80)

2 General Worst Case Bound

We ﬁrst give a simple worst case bound on prediction error in terms of label smoothness using few
assumptions about the method used to select labels or make predictions. In fact  the only assumption
we make is that the predictions are consistent with the set of labeled points (i.e. ˆyL = yL). The
bound motivates an interesting method for selecting labeled points and provides a new motivation
for a standard prediction method Blum and Chawla [2001] when used with arbitrarily selected L.
The bound also forms the basis of the other bounds we derive which make additional assumptions.
Deﬁne the graph cut function Γ(A  B) (cid:44)

(cid:80)

i∈A j∈B Wi j. Let

Ψ(L) (cid:44)

min

T⊆(V \L)(cid:54)=0

Γ(T  (V \ T ))

|T|

Note this function is different from normalized cut (also called sparsest cut). In this function  the
denominator is simply |T| while for normalized cut the denominator is min(|T| |V \ T|). This
difference is important: computing normalized cut is NP-hard  but we will show Ψ(L) can be com-
puted in polynomial time. Ψ(L) measures how easily we can cut a large portion of the graph away
from L. If Ψ(L) is small  then we can separate many nodes from L without cutting very many edges.
We show that Ψ(L) where L is the set of labeled vertices measures to what extent prediction error
can be high relative to label smoothness. This makes intuitive sense because if Ψ(L) is small than
there is a large set of unlabeled nodes which are weakly connected to the remainder of the graph
(including L).
Theorem 1. For any ˆy consistent with a labeled set L
||y− ˆy||2 ≤ 1

Wi j(|yi− yj|⊕|ˆyi− ˆyj|) ≤ 1

Wi j|yi− yj|+

Wi j|ˆyi− ˆyj|)

(cid:88)

(cid:88)

(cid:88)

2Ψ(L)

i j

(

2Ψ(L)

i j

i j

where ⊕ is the XOR operator.
Proof. Let I be the set of incorrectly classiﬁed points. First note that I ∩ L = ∅ (none of the labeled
points are incorrectly classiﬁed).

|I| = Γ(I  V \ I)

|I|

Γ(I  V \ I)

≤ Γ(I  V \ I)
Ψ(L)

Note that for all of the edges (i  j) counted in Γ(I  V \ I)  ˆyi = ˆyj implies yi (cid:54)= yj and ˆyi (cid:54)= ˆyj
implies yi = yj. Then

(cid:88)

i j

|I| ≤ 1

2Ψ(L)

Wi j(|yi − yj| ⊕ |ˆyi − ˆyj|)

The 1

2 term is introduced because the sum double counts edges.

This bound is tight when the set of incorrectly classiﬁed points I is one of the sets minimizing
minT⊆(V \L)(cid:54)=0 Γ(T  (V \ T ))/|T|.
This bound provides an interesting justiﬁcation for the algorithm in Blum and Chawla [2001] and
related methods when used with arbitrarily selected labeled sets. The term involving the predicted
i j Wi j|ˆyi − ˆyj|  is the objective function minimized under the constraint ˆyL = yL by the
labels 
algorithm of Blum and Chawla [2001]. When this is used to compute ˆy  the bound simpliﬁes.

(cid:80)

2

Lemma 1. If

for a labeled set L then

(cid:88)

i j

(cid:88)

ˆy = argminˆy∈{0 1}n:ˆyL=yL

Wi j|ˆyi − ˆyj|

||y − ˆy||2 ≤ 1
Ψ(L)

Wi j|yi − yj|

i j

(cid:80)
i j Wi j|ˆyi − ˆyj| ≤ (cid:80)

ComputeCut(L)

T (cid:48) ← V \ L
repeat
T ← T (cid:48)
λ ← Γ(T V \T )
T (cid:48) ← argmin
A⊆(V \L)

|T|

Γ(A  V \ A) − λ|A|

until Γ(T (cid:48)  V \ T (cid:48)) − λ|T (cid:48)| = 0
return T

MaximizeΨ(L  k)

L ← ∅
repeat

T ← ComputeCut(L)
i ← random vertex in T
L ← L ∪ {i}

until |L| = k
return L

Figure 1: Left: Algorithm for computing Ψ(L). Right: Heuristic for maximizing Ψ(L).

Proof. When we choose ˆy in this way
follows from Theorem 1.

i j Wi j|yi − yj| and the lemma

Label propagation solves a version of this problem in which ˆy is real valued [Bengio et al.  2006].
The bound also motivates a simple label selection method. In particular  we would like to select a la-
beled L set that maximizes Ψ(L). We ﬁrst describe how to compute Ψ(L) for a ﬁxed L. Computing
Ψ(L) is related to computing

min

T⊆(V \L)

Γ(T  V \ T ) − λ|T|

(1)

with parameter λ > 0. The following result is paraphrased from Fujishige [2005] (pages 248-249).
Theorem 2. λ(cid:48) = minT

g(T ) if and only if

f (T )

and

∀λ ≤ λ(cid:48) min

T

f(T ) − λg(T ) = 0

∀λ > λ(cid:48) min

T

f(T ) − λg(T ) < 0

We can compute Equation 1 for all λ via a parametric maxﬂow/mincut computation (it is known
there are no more than n − 1 distinct solutions). This gives a polynomial time algorithm for com-
puting Ψ(L). Note this theorem is for unconstrained minimization of T   but restricting T ∩ L = ∅
does not change the result: this constraint simply removes elements from the ground set. In practice 
this constraint can be enforced by contracting the graph used in the ﬂow computations or by giving
certain edges inﬁnite capacity.
As an alternative to solving the parametric ﬂow problem  we can ﬁnd the desired λ value through
an iterative method [Cunningham  1985]. The left of Figure 1 shows this approach. The algorithm
takes in a set L and computes argminT⊆(V \L)(cid:54)=0 Γ(T  (V \T ))/|T|. The correctness proof is simple.
When the algorithm terminates  we know λ ≥ λ(cid:48) = minT⊆(V \L)(cid:54)=0 Γ(T  (V \ T ))/|T| because we
set λ to be Γ(T  (V \ T ))/|T| for a particular T . By Theorem 2 and the termination condition  we
also know λ ≤ λ(cid:48) and can conclude λ = λ(cid:48) and the set T returned achieves this minimum. One can
also show the algorithm terminates in at most |V | iterations [Cunningham  1985].
Having shown how to compute Ψ(L)  we now consider methods for maximizing it. Ψ is neither
submodular nor supermodular. This seems to rule out straightforward set function optimization. In
our experiments  we try a simple heuristic based on the following observation: for any L  if Ψ(L(cid:48)) >
Ψ(L) then it must be the case that L(cid:48) intersects one of the cuts minimizing minT⊆(V \L)(cid:54)=∅ Γ(T  (V \

3

T ))/|T|. In other words  in order to increase Ψ(L) we must necessarily include a point from the
current cut. Our heuristic is then to simply add a random element from this cut to L. The right of
Figure 1 shows this method.
Several issues remain. First  although we have proposed a reasonable heuristic for maximizing
Ψ(L)  we do not have methods for maximizing it exactly or with guaranteed approximation. Aside
from knowing the function is not submodular or supermodular  we also do not know the hardness
of the problem. In the next section  we describe a lower bound on the Ψ function based on a notion
of graph covering. This lower bound can be maximized approximately via a simple algorithm and
has a well understood hardness of approximation. Second  we have found in experimenting with our
heuristic for maximizing Ψ(L) that the function can be prone to imbalanced cuts; the computed cuts
sometimes contain all or most of the unselected points V \ L and other times focus on small sets of
outliers. We give a third bound on error which attempts to address some of this sensitivity.

3 Graph Covering Algorithm

(cid:80)

The method we consider in this section uses a notion of graph covering. We say a set L α-covers
the graph if ∀i ∈ V either i ∈ L or
j∈L Wi j ≥ α. In other words  every node in the graph is
either in L or connected with total weight at least α to nodes in L (or both). This is a simple real
valued extension of dominating sets. A dominating set is a set L ⊆ V such that ∀i ∈ V either
i ∈ L or a neighbor of i is in L (or both). This notion of covering is related to the Ψ function
discussed in the previous section. In particular  if a set L α-covers a graph than it is necessarily the
case that Ψ(L) ≥ α. The converse does not hold  however. In other words  α is a lower bound on
Ψ(L). Then  α can replace Ψ(L) in the bound in the previous section for a looser upper bound on
prediction error. Although the bound is looser  compared to maximizing Ψ(L) we better understand
the complexity of computing an α-cover.
Corollary 1. For any ˆy consistent with a labeled set L that is an α-cover
||y − ˆy||2 ≤ 1
2α

Wi j(|yi − yj| ⊕ |ˆyi − ˆyj|) ≤ 1
2α

Wi j|yi − yj| +

Wi j|ˆyi − ˆyj|)

(cid:88)

(cid:88)

(cid:88)

(

i j

i j

i j

where ⊕ is the XOR operator.
Similar to Lemma 1  by making additional assumptions concerning the prediction method used we
can derive a slightly simpler bound. In particular  for a labeled set L that is an α cover  we assume
unlabeled nodes are labeled with the weighted majority vote of neighbors in L. In other words  set
ˆyi = yi for i ∈ L  and set ˆyi = y(cid:48) for i /∈ L with y(cid:48) such that
j∈L:yj(cid:54)=y(cid:48) Wi j.
With this prediction method we get the following bound.
Lemma 2. If L is an α-cover and V \ L is labeled according to majority vote

j∈L:yj =y(cid:48) Wi j ≥(cid:80)
(cid:80)
(cid:88)

(cid:88)

||y − ˆy||2 ≤ 1
α

Wi j|yi − yj|(1 − |ˆyi − ˆyj|) ≤ 1
α

i j

Wi j|yi − yj|

i j

(cid:80)

Proof. The right hand side follows immediately from the middle expression  so we focus on the ﬁrst
inequality. For every incorrectly labeled node  there is a set of nodes Li = {j ∈ L : ˆyi = ˆyj} which
satisﬁes yi (cid:54)= yj∀j ∈ Li  and
Wi j ≥ α/2. We then have for every incorrectly labeled node
a unique set of edges with total weight at least α/2 included inside the summation in the middle
expression.

j∈Li

|L| : F (L) ≥ α

In computing an α-cover  we want to solve.
min
L⊆V

Where

(cid:88)
i i = ∞. F (cid:48) is the minimum of a set of modular functions. F (cid:48)
where W (cid:48)
is neither supermodular nor submodular. However  we can still compute an approximately minimal

F (L) (cid:44) min
i∈V \L
i j = Wi j for i (cid:54)= j and W (cid:48)

Wi j = F (cid:48)(L) (cid:44) min
i∈V

(cid:88)

W (cid:48)

j∈L

j∈L

i j

4

α-cover using a trick introduced by Krause et al. [2008]. In particular  Krause et al. [2008] point out
that

i j  α) is submodular  and the sum of submodular functions is submodular.

(cid:80)

Also  min(
Then  we can replace F (cid:48) with

j∈L W (cid:48)

(cid:88)

j∈L

min
i∈V

(cid:88)

i

1
n

i j ≥ α ⇔
W (cid:48)
(cid:88)

F (cid:48)
α(L) =

i

(cid:88)

j∈L

(cid:88)

j∈L

1
n

min(

i j  α) ≥ α
W (cid:48)

min(

W (cid:48)

i j  α)

and solve

|L| : F (cid:48)

α(L) ≥ α

min
L⊆V

This is a submodular set cover problem. The greedy algorithm has approximation guarantees for
this problem for integer valued functions [Krause et al.  2008]. For binary weight graphs the ap-
proximation is O(log n). For real valued functions  it’s possible to round the function values to get
an approximation guarantee. In practice  we apply the greedy algorithm directly.
As previously mentioned  α-covers can be seen as real valued generalizations of dominating sets.
In particular  an α-cover is a dominating set for binary weight graphs and α = 1. The hardness
of approximation results for ﬁnding a minimum size dominating set then carry over to the more
general α-cover problem. The next theorem shows that the α-cover problem is NP-hard and in fact
the greedy algorithm for computing an α-cover is optimal up to constant factors for α = 1 and binary
weight graphs. It is based on the well known connection between ﬁnding a minimum dominating
set problem and ﬁnding a minimum set cover.
Theorem 3. Finding the smallest dominating set L in a binary weight graph is N P -complete.
Furthermore  if there is some  > 0 such that a polynomial time algorithm approximates the smallest
dominating set within (1 − ) ln(n/2) then N P ⊂ T IM E(nO(log log n)).
We have so far discussed computing a small α cover for a ﬁxed α. If we instead have a ﬁxed label
budget and want to maximize α  we can do so by performing binary search over α. This is the
approach used by Krause et al. [2008] and gives a bi-approximation.

4 Normalized Cut Algorithm

In this section we consider an algorithm that clusters the data set and replaces the Ψ function with a
normalized cut value. The normalized cut value for a set T ⊂ V is

Γ(T  V \ T )

min(|T| |V \ T|)

In other words  normalized cut is the ratio between the cut value for T and minimum of the size of
T and its complement. Computing the minimum normalized cut for a graph is NP-hard.
Consider the following method: 1) partition the set of nodes V into clusters S1  S2  ...Sk  2) for each
cluster request sufﬁcient labels to estimate the majority class with probability at least 1 − δ/k  and
3) label all nodes in each cluster with the majority label for that cluster. Here the probability 1− δ/k
is with respect to the choice of the labeled nodes used to estimate the majority class for each cluster.
Theorem 4. Let S1  S2  ...Sk be a partition of V   and assume we have estimates of the majority
class of each Sl each of which are accurate with probability at least 1− δ/k. If ˆy labels every i ∈ Sl
according to the estimated majority label for Sl then with probability at least 1 − δ
Wi j|yi − yj|

||y − ˆy||2 ≤

(cid:88)

(cid:88)

(cid:88)

Wi j|yi − yj| ≤ 1
2φ

1
2φl

i j∈Sl

i j

l

where

and

φl = min
T⊂Sl

Γ(T  Sl \ T )

min(|T| |Sl \ T|)

φ = min

l

φl

5

(cid:83)k
Proof. By the union bound  the estimated majority labels for all of the clusters are correct with
probability at least 1 − δ. Let I be the set of incorrectly labeled nodes (errors). We consider the
l=1 Il Note that |Il| ≤ |Sl \ Il|
intersection of I with each of the clusters. Let Il (cid:44) |I ∩ Sl|. I =
since we labeled cluster according to the majority label for the cluster. Then

(cid:88)

|Il| =

Γ(Il  Sl \ Il)

|Il|

Γ(Il  Sl \ Il)

min(|Il| |Sl \ Il|)

Γ(Il  Sl \ Il)

(cid:88)
(cid:88)
(cid:88)

l

l

|I| =

=

≤

l

Γ(Il  Sl \ Il)
Γ(Il  Sl \ Il)

φl

l

For any i  j  with i ∈ Il and j ∈ Sl \ Il  we must have yi (cid:54)= yj. Also  for any i  j with yi (cid:54)= yj and
i  j ∈ Sl  either i ∈ Il or j ∈ Il. In other words  there is a one-to-one correspondence between 1)
edges i  j for which i  j ∈ Sl and either i ∈ Il or j ∈ Il and 2) edges i  j for which i  j ∈ Sl and
yi (cid:54)= yj. The desired result then follows.

Note in practice we only label the unlabeled nodes in each cluster using the majority label estimates.
Using the true labels for the labeled nodes only decreases error  so the theorem still holds.
In this bound  φ is a measure of the density of the clusters. Computing φl for a particular cluster
is NP-hard  but there are approximation algorithms. However  we are not aware of approximation
algorithms for computing a partition such that φ is maximized. This is different from the standard
normalized cut clustering problem; we do not care if clusters are strongly connected to each other
only that each cluster is internally dense. In our experiments  we try several standard clustering
algorithms and achieve good real world performance  but it remains an interesting open question to
design a clustering algorithm for directly maximizing φ. An approach we have not yet tried is to use
the error bound to choose between the results of different clustering algorithms.
We now consider the problem of estimating the majority class for a cluster. If we uniformly sample
labels from a cluster  standard results give that the probability of incorrectly estimating the majority
decreases exponentially with the number of labels if the fraction of nodes in the minority class is
bounded away from 1/2 by a constant. We now show that if the labels are sufﬁciently smooth and
the cluster is sufﬁciently dense then the fraction of nodes in the minority class is small.
Theorem 5. The fraction of nodes in the minority class of S is at most

(cid:80)

where

φ = min
T⊂S

i j∈S Wi j|yi − yj|

φ|S|
Γ(T  S \ T )

min(|T| |S \ T|)

Proof. Let S− be the set of nodes belonging to the minority class and S+ be the set of nodes
belonging to the other class. Let f be the fraction of nodes in the minority class.
min(|S+| |S−|)
i j∈S Wi j|yi − yj|

(cid:80)
i j∈S Wi j|yi − yj|
min(|S+| |S−|)

(cid:80)

f =

|S−|
|S|

|S−|
(cid:80)
|S| =
i j∈S Wi j|yi − yj|
(cid:80)
i j∈S Wi j|yi − yj|

|S|

φ|S|

=

≤

min(|S+| |S−|)

Γ(S+  S−)

If we have an estimate of the smoothness of the labels in a cluster  we can use this bound and an
approximation of φ to determine the number of labels needed to estimate the majority class with
high conﬁdence. In our experiments  we simply request a single label per cluster.

6

Digit1/10
Text/10
BCI/10
USPS/10
g241c/10
g241d/10
Digit1/100
Text/100
BCI/100
USPS/100
g241c/100
g241d/100

Spectral
9.54 (4.42)
37.64 (8.64)
50.13 (2.16)
15.22 (6.22)
39.63 (5.67)
22.31 (7.06)
4.47 (1.35)
31.67 (2.41)
47.37 (2.80)
6.23 (1.49)
44.31 (2.09)
41.70 (2.44)

k-Cut
50.02 (1.04)
50.03 (0.3)
50.16 (0.64)
31.53 (23.65)
50.03 (0.03)
50.02 (0.23)
50.07 (1.46)
50.26 (2.73)
50.14 (0.5)
31.13 (26.31)
50.02 (0.18)
50.03 (0.18)

METIS
4.93 (4.05)
34.76 (6.05)
49.68 (2.63)
8.15 (5.51)
29.18 (7.28)
22.57 (7.26)
3.24 (0.76)
32.57 (1.88)
45.35 (1.91)
9.28 (1.38)
37.47 (2.13)
35.96 (1.99)

Ψ
49.92 (3.18)
50.05 (0.06)
50.32 (0.55)
20.07 (2.70)
50.29 (0.07)
50.01 (0.09)
2.60 (0.83)
48.34 (0.67)
48.17 (1.87)
10.17 (0.39)
52.48 (0.37)
50.33 (0.21)

Baseline
20.90 (15.67)
45.91 (7.96)
50.12 (1.32)
15.87 (4.82)
47.26 (5.19)
48.46 (3.39)
2.57 (0.67)
26.82 (3.88)
47.48 (2.99)
6.33 (2.46)
42.86 (4.50)
41.56 (4.34)

Table 1: Error rate mean (standard deviation) for different data set  label count  method combina-
tions.

Figure 2: Left: Points selected by the Ψ function maximization method. Right: Points selected by
the spectral clustering method.

5 Experiments

We experimented with a method based on Lemma 1. We use the randomized method for maximizing
Ψ and then predict with min-cuts [Blum and Chawla  2001]. We also tried a method based on
Theorem 4. We cluster the data then label each cluster according to a single randomly chosen point.
We chose the number of clusters to be equal to the number of labeled points observing that if a
cluster is split evenly amongst the two classes then we will have a high error rate regardless of
how well we estimate the majority class. We tried three clustering algorithms: a spectral clustering
method [Ng et al.  2001]  the METIS package for graph partitioning [Karypis and Kumar  1999] 
and a k-cut approximation algorithm [Saran and Vazirani  1995  Gusﬁeld  1990]. As a baseline we
use random label selection and prediction using the label propagation method of Bengio et al. [2006]
with  = 10−6 and µ = 10−6 and class mass normalization. We also experimented with a method
motivated by the graph covering bound  but for lack of space we omit these results.
We used six benchmark data sets [Chapelle et al.  2006]. We use graphs constructed with a Gaussian
kernel with standard deviation chosen to be the average distance to the k1th nearest neighbor divided
by 3 (a similar heuristic is used by Chapelle et al. [2006]). We then make this graph sparse by
removing the edge between node i and j unless i is one of j’s k2 nearest neighbors or j is one of i’s
k2 nearest neighbors. We use 10 and 100 labels. We set k1 and k2 for each data set and label count
to be the parameters which give the lowest average error rate for label propagation averaging over
100 trials and choosing from the set {5  10  50  100}. We tune the graph construction parameters to
give low error for the baseline method to ensure any bias is in favor of the baseline as opposed to
the new methods we propose. We then report average error over 1000 trials in the 10 label case and
100 trials in the 100 label case for each combination of data set and algorithm.
Table 1 shows these results. We ﬁnd that the Ψ function method does not perform well. We found
on most of the data sets the cuts found by the method included all or almost all of V \ L. In this case
the points selected are essentially random. However  on the USPS data set and on some synthetic
data sets we have tried  we have also observed the opposite behavior where the cuts are very small
and seem to focus on small sets of outliers. Figure 2 shows an example of this. The k-cut method

7

−3−2−101234567−4−202468−3−2−101234567−4−202468also did not perform well. We’ve found this method has similar problems with outliers. We think
these outlier sensitive methods are impractical for graphs constructed from real world data.
The results for the spectral clustering and METIS clustering methods  however  are quite encourag-
ing. These methods performed well matching or beating the baseline method on the 10 label trials
and in some cases signiﬁcantly improving performance. The METIS method seems particularly ro-
bust. On the 100 label trials  performance was not as good. In general  we expect label selection to
help more when learning from very few labels. The choice in clustering method seems to be of great
practical importance. The clustering methods which work best seem to be methods which minimize
normalize cut like objectives. This is not surprising given the presence of the normalized cut term in
Theorem 4  but it is an open problem to give a clustering method for directly minimizing the bound.
We ﬁnally note that the numbers we report for our baseline method are in some cases signiﬁcantly
different than the published numbers [Chapelle et al.  2006]. This seems to be because of a variety of
factors including differences in implementation as well as signiﬁcant differences in experiment set
up. We have also experimented with several heuristic modiﬁcations to our methods and compared
our methods to simple greedy methods. One modiﬁcation we tried is to use label propagation for
prediction in conjunction with our label selection methods. We omit these results for lack of space.

6 Related Work

Previous work has also used clustering  covering  and other graph properties to guide label selection
on graphs. We are  however  the ﬁrst to our knowledge to give bounds which relate prediction
error to label smoothness for single batch label selection methods. Most previous work on label
(cid:80)
selection methods for learning on graphs has considered active (i.e. sequential) label selection [Zhu
and Lafferty  2003  Pucci et al.  2007  Zhao et al.  2008  Wang et al.  2007  Afshani et al.  2007].
i j Wi j|yi − yj| labels are
Afshani et al. [2007] show in this setting O(c log(n/c)) where c =
sufﬁcient and necessary to learn the labeling exactly under some balance assumptions. Without
balance assumptions they show O(c log(1/) + c log(n/c)) labels are sufﬁcient to achieve an  error
rate. In some cases  our bounds are better despite considering only non sequential label selection.
Consider the case where c grows linearly with n so c/n = a for some constant a > 0. In this case 
with the bound of Afshani et al. [2007] the number of labels required to achieve a ﬁxed error rate
 also grows linearly with n. In comparison  our graph covering bound needs an α-cover with α =
a/. For some graph topologies  the size of such a cover can grow sublinearly with n (for example
if the graph contains large  dense clusters). Afshani et al. [2007] also use a kind of dominating set
in their method  and it could be interesting to see if portions of their analysis could be adapted to the
ofﬂine setting. Zhao et al. [2008] also use a clustering algorithm to select initial labels.
Other work has given generalization error bounds in terms of label smoothness [Pelckmans et al. 
(cid:80)
2007  Hanneke  2006  Blum et al.  2004] for transductive learning from randomly selected L.
These bounds are PAC style which typically show that  roughly  the error rate decreases with
i j Wi j|yi − yj|/(b|L|)) where b is the minimum 2-cut of the graph. Depending on the
O(
(cid:80)
graph structure  our bounds can be signiﬁcantly better. For example  if a binary weight graph con-
tains c cliques of size n/c then  we can ﬁnd an α cover of size cα log(cα) giving an error rate of
i j Wi j|yi − yj|/(nα)). This is better if c log(cα) < n/b.
O(

A line of work has examined mistake bounds in terms of label smoothness for online learning on
graphs [Pelckmans and Suykens  2008  Brautbar  2009  Herbster et al.  2008  2005  Herbster  2008].
These mistake bounds hold no matter how the sequence of vertices are chosen. Herbster [2008]
also considers how cluster structure can improve mistake bounds in this setting and gives a mistake
bound similar to our graph covering bound on prediction error. Herbster et al. [2005] discusses using
an active learning method for the ﬁrst several steps of an online algorithm. Our work differs from
this previous work by considering prediction error bounds for ofﬂine learning as opposed to mistake
bounds for online learning. The mistake bound setting is signiﬁcantly different as the prediction
method receives feedback after every prediction.

Acknowledgments

This material is based upon work supported by the National Science Foundation under grant IIS-
0535100.

8

References
P. Afshani  E. Chiniforooshan  R. Dorrigiv  A. Farzan  M. Mirzazadeh  N. Simjour  and H. Zarrabi-Zadeh. On

the complexity of ﬁnding an unknown cut via vertex queries. In COCOON  2007.

Y. Bengio  O. Delalleau  and N. Le Roux. Label propagation and quadratic criterion.

B. Sch¨olkopf  and A. Zien  editors  Semi-Supervised Learning. MIT Press  2006.

In O. Chapelle 

A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML  2001.
A. Blum  J. Lafferty  M. R. Rwebangira  and R. Reddy. Semi-supervised learning using randomized mincuts.

In ICML  2004.

M. Brautbar. Online Learning a Labeling of a Graph. Mining and Learning with Graphs  2009.
O. Chapelle  B. Sch¨olkopf  and A. Zien. Semi-supervised learning. MIT press  2006.
W. Cunningham. Optimal attack and reinforcement of a network. Journal of the ACM  1985.
S. Fujishige. Submodular Functions and Optimization. Elsevier Science  2005.
D. Gusﬁeld. Very simple methods for all pairs network ﬂow analysis. SIAM Journal on Computing  1990.
S. Hanneke. An analysis of graph cut size for transductive learning. In ICML  2006.
M. Herbster. Exploiting Cluster-Structure to Predict the Labeling of a Graph. In ALT  2008.
M. Herbster  M. Pontil  and L. Wainer. Online learning over graphs. In ICML  2005.
M. Herbster  G. Lever  and M. Pontil. Online Prediction on Large Diameter Graphs. In NIPS  2008.
G. Karypis and V. Kumar. A fast and highly quality multilevel scheme for partitioning irregular graphs. SIAM

Journal on Scientiﬁc Computing  1999.

A. Krause  H. B. McMahan  C. Guestrin  and A. Gupta. Robust submodular observation selection. JMLR 

2008.

A. Y. Ng  M. I. Jordan  and Y. Weiss. On spectral clustering: Analysis and an algorithm. In NIPS  2001.
K. Pelckmans and J. Suykens. An online algorithm for learning a labeling of a graph. In Mining and Learning

with Graphs  2008.

K. Pelckmans  J. Shawe-Taylor  J. Suykens  and B. De Moor. Margin based transductive graph cuts using linear

programming. 2007.

A. Pucci  M. Gori  and M. Maggini. Semi-supervised active learning in graphical domains. In Mining and

Learning With Graphs  2007.

H. Saran and V. V. Vazirani. Finding k cuts within twice the optimal. SIAM Journal on Computing  1995.
M. Wang  X. Hua  Y. Song  J. Tang  and L. Dai. Multi-Concept Multi-Modality Active Learning for Interactive

Video Annotation. In International Conference on Semantic Computing  2007.

W. Zhao  J. Long  E. Zhu  and Y. Liu. A scalable algorithm for graph-based active learning. In Frontiers in

Algorithmics  2008.

X. Zhu and J. Lafferty. Combining active learning and semi-supervised learning using gaussian ﬁelds and

harmonic functions. In ICML  2003.

9

,Benjamin Planche
Xuejian Rong
Ziyan Wu
Srikrishna Karanam
Harald Kosch
YingLi Tian
Jan Ernst
ANDREAS HUTTER