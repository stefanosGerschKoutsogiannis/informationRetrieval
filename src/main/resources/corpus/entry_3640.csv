2012,Multiresolution Gaussian Processes,We propose a multiresolution Gaussian process to capture long-range  non-Markovian dependencies while allowing for abrupt changes.  The multiresolution GP hierarchically couples a collection of smooth GPs  each defined over an element of a random nested partition.  Long-range dependencies are captured by the top-level GP while the partition points define the abrupt changes.  Due to the inherent conjugacy of the GPs  one can analytically marginalize the GPs and compute the conditional likelihood of the observations given the partition tree.  This allows for efficient inference of the partition itself  for which we employ graph-theoretic techniques.  We apply the multiresolution GP to the analysis of Magnetoencephalography (MEG) recordings of brain activity.,Multiresolution Gaussian Processes

Emily B. Fox

Dept of Statistics  University of Washington

David B. Dunson

Dept of Statistical Science  Duke University

ebfox@stat.washington.edu

dunson@stat.duke.edu

Abstract

We propose a multiresolution Gaussian process to capture long-range  non-
Markovian dependencies while allowing for abrupt changes and non-stationarity.
The multiresolution GP hierarchically couples a collection of smooth GPs  each
deﬁned over an element of a random nested partition. Long-range dependen-
cies are captured by the top-level GP while the partition points deﬁne the abrupt
changes. Due to the inherent conjugacy of the GPs  one can analytically marginal-
ize the GPs and compute the marginal likelihood of the observations given the par-
tition tree. This property allows for efﬁcient inference of the partition itself  for
which we employ graph-theoretic techniques. We apply the multiresolution GP to
the analysis of magnetoencephalography (MEG) recordings of brain activity.

1 Introduction
A key challenge in many time series applications is capturing long-range dependencies for which
Markov-based models are insufﬁcient. One method of addressing this challenge is through em-
ploying a Gaussian process (GP) with an appropriate (non-band-limited) covariance function. How-
ever  GPs typically assume smoothness properties that can blur key elements of the signal if abrupt
changes occur. The Mat´ern kernel enables less smooth functions  but assumes a stationary process
that does not adapt to varying levels of smoothness. Likewise  a changepoint [21] or partition [8]
model between smooth functions fails to capture long range dependencies spanning changepoints.

Another long-memory process is the fractional ARIMA process [5  13]. Wavelet methods have also
been proposed  including recently for smooth functions with discontinuities [2]. We take a funda-
mentally different approach based on GPs that allows (i) direct interpretability  (ii) local stationarity 
(iii) irregular grids of observations  and (iv) sharing information across related time series.

As a motivating application  consider magnetoencephalography (MEG) recordings of brain activity
in response to some word stimulus. Due to the low signal-to-noise-ratio (SNR) regime  multiple
trials are often recorded  presenting a functional data analysis scenario. Each trial results in a noisy
trajectory with key discontinuities (e.g.  after stimulus onset). Although there are overall similarities
between the trials  there are also key differences that occur based on various physiological phenom-
ena  as depicted in Fig. 1. We clearly see abrupt changes as well as long-range correlations. Key to
the data analysis is the ability to share information about the overall trajectory between the single
trials without forcing unrealistic smoothness assumptions on the single trials themselves.

In order to capture both long-range dependencies and potential discontinuities  we propose a mul-
tiresolution GP (mGP) that hierarchically couples a collection of smooth GPs  each deﬁned over an
element of a nested partition set. The top-level GP captures a smooth global trajectory  while the
partition points deﬁne abrupt changes in correlation induced by the lower-level GPs. Due to the in-
herent conjugacy of the GPs  conditioned on the partition points the resulting function at the bottom
level is marginally GP-distributed with a partition-dependent (and thus non-stationary) covariance
function. The correlation between any two observations yi and yj generated by the mGP at locations
xi and xj is a function of the distance ||xi − xj || and which partition sets contain both xi and xj.
In a standard regression setting  the marginal GP structure of the mGP allows us to compute the
marginal likelihood of the data conditioned on the partition  enabling efﬁcient inference of the par-
tition itself. We integrate over the hierarchy of GPs and only sample the partition points. For our

1

0.5

0

−0.5

s
n
o

i
t

a
v
r
e
s
b
O

−1
0

50

100

50

100

150

200

250

300

50

100

150

200

250

300

250

300

50

100

150

200

250

300

50

100

150

200

250

300

A1
1

A1
2

A0

A1
1

A1
2

A2
1

A2
2

A2
3

A2
4

A2
1

A2
2

A2

3 A2

4

150

200

Time

1  A1

Figure 2: mGP on a balanced  binary tree
partition: Parent function is split by A1 =
2}. Recursing down the tree  each
{A1
partition has a GP with mean given by its
parent function restricted to that set.

Figure 1: For sensor 1 and word house  Left: Data from three
trials; Middle: Empirical correlation matrix from 20 trials; Right:
Hierarchical segmentation produced by recursive minimization of
normalized cut objective  with color indicating tree level.
proposal distribution  we borrow the graph-theoretic idea of normalized cuts [22] often used in image
segmentation. Our inferences integrate over the partition tree  allowing blurring of discontinuities
and producing functions which can appear smooth when discontinuities are not present in the data.
2 Background
A GP provides a distribution on real-valued functions f : X → ℜ  with the property that the function
evaluated at any ﬁnite collection of points is jointly Gaussian. The GP  denoted GP(m  c)  is uniquely
deﬁned by its mean function m and covariance function c. That is  f ∼ GP(m  c) if and only if for
all n ≥ 1 and x1  . . .   xn  (f (x1)  . . .   f (xn)) ∼ Nn(µ  K)  with µ = [m(x1)  . . .   m(xn)] and
[K]ij = c(xi  xj ). The properties (e.g.  continuity  smoothness  periodicity  etc.) of functions
drawn from a given GP are determined by the covariance function. The squared exponential kernel 
2)  leads to smooth functions. Here  d is a scale hyperparameter and κ
c(x  x′) = d exp(−κ||x − x′||2
is the bandwidth determining the extent of the correlation in f over X . See [18] for further details.
3 Multiresolution Gaussian Process Formulation
Our interest is in modeling a function g that (i) is locally smooth  (ii) exhibits long-range correlations
(i.e.  corr(g(x)  g(x′)) > 0 for ||x − x′|| relatively large)  and (iii) has abrupt changes. We begin by
modeling a single function  but with a speciﬁcation that readily lends itself to modeling a collection
of functions that share a common global trajectory  as explored in Sec. 4.
Generative Model Assume a set of noisy observations y = {y1  . . .   yn}  yi ∈ ℜ  of the function
g at locations {x1  . . .   xn}  xi ∈ X ⊂ ℜp:

yi = g(xi) + ǫi 

(1)
We hierarchically deﬁne g as follows. Let A = {A0  A1  . . .   AL−1} be a nested partition  or tree
for some k. Furthermore 
assume that each Aℓ
i is a contiguous subset of X . Fig. 2 depicts a balanced  binary tree partition. We
deﬁne a global parent function on A0 as f 0 ∼ GP(0  c0). This function captures the overall shape
of g and its long-range dependencies. Then  over each partition set Aℓ

partition  of X with A0 = X   X = Si Aℓ

j = ∅  and Aℓ

ǫi ∼ N (0  σ2).

i we independently draw

i ⊂ Aℓ−1

i ∩ Aℓ

i  Aℓ

k

i ).

i )  cℓ

f ℓ(Aℓ

i ) ∼ GP(f ℓ−1(Aℓ

(2)
That is  the mean of the GP is given by the parent function restricted to the current partition set. Due
to the conditional independence of these draws  f ℓ can have discontinuities at the partition points.
However  due to the coupling of GPs through the tree  f ℓ will maintain aspects of the shape of f 0.
Finally  we set g = f L−1. A pictorial representation of the mGP is shown in Fig. 2.
i ) ∼ GP(0  cℓ
We can equivalently represent the mGP as an additive GP model: φℓ(Aℓ
i )  g = Pℓ φℓ.
Covariance Function We assume a squared exponential kernel cℓ
2) 
i||x − x′||2
i = dℓ
encouraging local smoothness over each partition set Aℓ
ℓ=1(dℓ)2 < 1
for ﬁnite variance regardless of tree depth and additionally encouraging lower levels to vary less from
their parent function  providing regularization and robustness to the choice of L.
2 so that each child function is locally as smooth as
We typically assume bandwidths κℓ
its parent. One can think of this formulation as akin to a fractal process: zooming in on any partition 
the locally deﬁned function has the same smoothness as that of its parent over the larger partition.
Thus  lower levels encode ﬁner-resolution details. We denote the covariance hyperparameters as θ =
{d0  . . .   dL−1  κ}  and omit the dependency in conditional distributions for notational simplicity.
See the Supplementary Material for discussion of other possible covariance speciﬁcations.

i = dℓ withP∞

i . We focus on dℓ

i = κ/||Aℓ

i exp(−κℓ

i||2

2

Induced Marginal GP The conditional independencies of our mGP imply that

p(g | A) = Z p(f 0)

L−1

Yℓ=1

p(f ℓ | f ℓ−1  Aℓ)df 0:L−2.

(3)

Due to the inherent conjugacy of the GPs  one can analytically marginalize the hierarchy of GPs
conditioned on the partition tree A yielding

g | A ∼ GP(0  c∗

A) 

c∗
A =

L−1

Xℓ=0 Xi

cℓ
i

I
Aℓ
i

.

(4)

Aℓ
i

(x  x′) = 1 if x  x′ ∈ Aℓ

Here  I
i and 0 otherwise. Eq. (4) provides an interpretation of the mGP
as a (marginally) partition-dependent GP  where the partition A deﬁnes the discontinuities in the
covariance function c∗
A. The covariance function encodes local smoothness of g and discontinuities
at the partition points. Note that c∗
The correlation between any two observations yi and yj at locations xi and xj generated as in Eq. (1)
is a function of how many tree levels contain both xi and xj and the distance ||xi − xj ||. Let rℓ
i
index the partition set such that xi ∈ Aℓ
and Lij the lowest level for which xi and xj fall into the
rℓ
i
j). Then  for xi 6= xj 
same set (i.e.  the largest ℓ such that rℓ

A deﬁnes a non-stationary covariance function.

i = rℓ

corr(yi  yj | A) =

(xi  xj )

PLij
ℓ=0 cℓ
rℓ
i
Qk∈{i j}(σ2 +PL−1
ℓ=0 cℓ
rℓ
k

(xk  xk))

1

2

= PLij

ℓ=0 dℓ exp(−κ||xi − xj ||2

2/||Aℓ
rℓ
i

σ2 +PL−1

ℓ=0 dℓ

||2
2)

 

(5)

where the second equality follows from assuming the previously described kernels. An example
correlation matrix is shown in Fig. 3(c). κ determines the width of the bands while dℓ controls the
contribution of level ℓ. Since dℓ is square summable  lower levels are less inﬂuential.
Marginal Likelihood Based on a vector of observations y = [y1 · · · yn]′ at
locations
x = [x1 · · · xn]′  we can restrict our attention to evaluating the GPs at x. Let f ℓ(x) =
[f ℓ(x1) · · · f ℓ(xn)]′. By deﬁnition of the GP  we have

f ℓ(x) | f ℓ−1(x)  Aℓ ∼ N (f ℓ−1(x)  Kℓ) 

[Kℓ]i j = (cid:26) cℓ

r(xi  xj) xi  xj ∈ Aℓ
r
otherwise

0

.

(6)

The level-speciﬁc covariance matrix Kℓ is block-diagonal with structure determined by the level-
speciﬁc partition Aℓ. Observations are generated as y | g(x) ∼ N (g(x)  σ2In). Recalling Eq. (3) 
standard results yield

g(x) | A ∼ N(cid:18)0 

L−1

Xℓ=0

Kℓ(cid:19) y | A ∼ N(cid:18)0  σ2In +

L−1

Xℓ=0

Kℓ(cid:19).

(7)

This result can also be derived from the induced mGP of Eq. (4). We see that the marginal likelihood
p(y | A) has a closed form. Alternatively  one can condition on the GP at any level ℓ′:

y | f ℓ′

(x)  A ∼ N(cid:18)f ℓ′

(x)  σ2In +

L−1

Xℓ=ℓ′+1

Kℓ(cid:19).

(8)

A key advantage of the mGP is the conditional conjugacy of the latent GPs that allows us to compute
the likelihood of the data simply conditioned on the hierarchical partition A (see Eq. (7)). This fact
is fundamental to the efﬁciency of the partition inference procedure described in Sec. 5.
4 Multiple Trials
In many applications  such as the motivating MEG application  one has a collection of observations
of an underlying signal. To capture the common global trajectory of these trials while still allowing
for trial-speciﬁc variability  we model each as a realization from an mGP with a shared parent func-
tion f 0. One could trivially allow for alternative structures of hierarchical sharing beyond f 0 if an
application warranted. For simplicity  and due to the motivating MEG application  we additionally
assume shared changepoints between the trials  though this assumption can also be relaxed.

3

s
n
o

i
t

a
v
r
e
s
b
O

8

6

4

2

0

−2

−4
0

10

5

0

−5

s
n
o

i
t

a
v
r
e
s
b
O

50

100
Time

(a)

150

200

0

50

150

200

100
Time
(b)

20

40

60

80

100

120

140

160

180

200

20

40

60

80

100

(c)

20

40

60

80

100

120

140

160

180

200

20

40

60

120

140

160

180

200

20

40

60

80

100

120

140

160

180

200

20

40

60

120

140

160

180

200

120

140

160

180

200

80

100

(e)

80

100

(d)

Figure 3: (a) Three trials and (b) all 100 trials of data generated from a 5-level mGP with a shared parent
function f 0 and partition A (randomly sampled). (c) True correlation matrix. (d) Empirical correlation matrix
from 100 trials. (e) Hierarchical segmentation produced by recursive minimization of normalized cut objective.
Generative Model For each trial y

(j) = {y(j)

i = g(j)(xi) + ǫ(j)
y(j)

i

1   . . .   y(j)
n }  we model
ǫ(j)
i ∼ N (0  σ2) 

 

with g(j) = f L−1 (j) generated from a trial-speciﬁc GP hierarchy f 0 → f 1 (j) → · · · → f L−1 (j)
with shared parent f 0. (Again  alternative structures can be considered.) From Eq. (8) with ℓ′ = 0 
and exploiting the independence of {f ℓ (j)}  independently for each j
L−1

(9)

(10)

(j) | f 0(x)  A ∼ N(cid:18)y

y

(j); f 0(x)  σ2In +

Xℓ=1

Kℓ(cid:19).

Note that with our GP-based formulation  we need not assume coincident observation locations
x1  . . .   xn between the trials. However  for simplicity of exposition  we consider shared locations.

We compactly denote the covariance by Σ = σ2In +PL−1

ℓ=1 Kℓ.

Simulated data generated from a 5-level mGP with shared f 0 and A are shown in Fig. 3. The sample
correlation matrix is also shown. Compare with the MEG data of Fig. 1. Both the qualitative struc-
ture of the raw time series as well as blockiness of the correlation matrix have striking similarities.

Posterior Global Trajectory and Predictions Based on a set of trials {y
interest to infer the posterior of f 0. Standard Gaussian conjugacy results imply that

(1)  . . .   y

(J)}  it is of

p(f 0(x) | y

(1)  . . .   y

(J)  A) = N(cid:18)(cid:0)K −1

0 + JΣ−1(cid:1)−1

˜y (cid:0)K −1

0 + JΣ−1(cid:1)−1(cid:19) 

(11)

(i). Likewise  the predictive distribution of data from a new trial is

where ˜y = Σ−1Pi y

p(y

(J+1) | y

(1)  . . .   y

(J)  A) = Z p(y
= N(cid:18)(cid:0)K −1

0 + JΣ−1(cid:1)−1

(J+1) | f 0(x)  A)p(f 0(x) | y

(1)  . . .   y

(J)  A)df 0

˜y  Σ +(cid:0)K −1

0 + JΣ−1(cid:1)−1(cid:19).

(12)

Marginal Likelihood Since the set of trials Y = {y
parent function f 0  the marginal likelihood does not decompose over trials. Instead 

(1)  . . .   y

(J)} are generated from a shared

p(Y | A) =

|K0|−1/2|Σ|−J/2

(2π)−nJ/2|K −1

0 + JΣ−1|1/2

exp(cid:18) −

1

2 Xi

(i)′

y

Σ−1

(i) +

y

1
2

˜y

′(K −1

0 + JΣ−1)−1 ˜y(cid:19).

(13)

See the Supplementary Material for a derivation. One can easily verify that the above simpliﬁes to
the marginal likelihood of Eq. (7) when J = 1.
5 Inference of the Hierarchical Partition
In the formulation so far  we have assumed that the hierarchical partition A is given. A key question
is to infer the partition from the data. Assume that we have prior p(A) on the hierarchical partition.
Based on the fact that we can analytically compute p(Y | A)  we can use importance sampling or
independence chain Metropolis Hastings to draw samples from the posterior p(A | Y ).
In what follows  we assume a balanced binary tree for A. See the Supplementary Material for a
discussion of how unbalanced trees can be considered via modiﬁcations to the covariance hyperpa-
rameter speciﬁcation or by considering alternative priors p(A) such as the Mondrian process [20].

4

Partition Prior We consider a prior solely on the partition points {z1  . . .   z2L−1−1} rather than
taking tree level into account as well. Because of our time-series analysis focus  we assume X ⊂ ℜ.
We deﬁne a distribution F on X and specify p(A) = Qi F (zi). Generatively  one can think of

drawing 2L−1 − 1 partition points from F and deterministically forming a balanced binary tree
A from these. For multidimensional X   one could use Voronoi tessellation and graph matching to
build the tree from the randomly selected zi. Such a prior allows for trivial speciﬁcation of a uniform
distribution on A (simply taking F uniform on X ) or for eliciting prior information on changepoints 
such as based on physiological information for the MEG data. Eliciting such information in a level-
dependent setup is not straightforward. Also  despite common deployment  taking the partition point
at level ℓ as uniformly distributed over the parent set Aℓ−1
yields high mass on A with small Aℓ
i.
This property is undesirable because it leads to trees with highly unbalanced partitions.

i

Our resulting inferences perform Bayesian model averaging over trees. As such  even though we
specify a prior on partitions with 2L−1 − 1 changepoints  the resulting functions can appear to
adaptively use fewer by averaging over the uncertainty in the discontinuity location.
Partition Proposal Although stochastic tree search algorithms tend to be inefﬁcient in general 
we can harness the well-deﬁned correlation structure associated with a given hierarchical partition
to much more efﬁciently search the tree space. One can think of every observed location xi as a
node in a graph with edge weights between xi and xj deﬁned by the magnitude of the correlation of
yi and yj. Based on this interpretation  the partition points of A correspond to graph cuts that bisect
small edge weights  as graphically depicted in Fig. 4. As such  we seek a method for hierarchically
cutting a graph. Given a cost matrix W with elements wuv deﬁned for all pairs of nodes u  v in a set
V   the normalized cut metric [22] for partitioning V into disjoint sets A and B is given by

ncut(A  B) = cut(A  B)(cid:2)assoc(A  V )−1 + assoc(B  V )−1(cid:3)  

(14)

where cut(A  B) = Pu∈A v∈B wuv and assoc(A  V ) = Pu∈A v∈V wuv. Typically  the cut point

is selected as the minimum of the metric ncut(A  B) computed over all possible subsets A and B.
The normalized cut metric balances between the cost of edge weights cut and the connectivity of the
cut component  thus avoiding cuts that separate small sets. Fig. 1 shows an example of applying a
greedy normalized cuts algorithm (recursively minimizing ncut(A  B)) to MEG data.
Instead of deterministically selecting cut points  we employ the
normalized cut objective as a proposal distribution. Let the cost
matrix W be the absolute value of the empirical correlation matrix
computed from trials {y
(J)} (see Fig. 1). Due to the
natural ordering of our locations xi ∈ X ⊂ ℜ  the algorithm is
straightforwardly implemented. We step down the hierarchy  ﬁrst
proposing a cut of A0 into {A1

(1)  . . .   y

2} with probability

1  A1

TIME 

Figure 4: Illustration of cutpoints
dividing contiguous segments at
points of low correlation.

cut 2 

cut 1 

cut 2 

q({A1

1  A1

2}) ∝ ncut(A1

1  A1

2)−1.

(15)

i is partitioned via a normalized cut proposal based on the submatrix of W corre-
At level ℓ  each Aℓ
i . The probability of any partition A under the speciﬁed proposal
sponding to the locations xi ∈ Aℓ
distribution is simply computed as the product of the sequence of conditional probabilities of each
cut. This procedure generates cut points only at the observed locations xi. More formally  the
partition point in X is proposed as uniformly distributed between xi and xi+1. Extensions to multi-
dimensional X rely on spectral clustering algorithms based on the graph Laplacian [24].
Markov Chain Monte Carlo An importance sampler draws hierarchical partitions A(m) ∼ q 
with the proposal distribution q deﬁned as above  and then weights the samples by p(A(m))/q(A(m))
to obtain posterior draws [19]. Such an approach is naively parallelizable  and thus amenable to
efﬁcient computations  though the effective sample size may be low if q does not adequately match
the posterior p(A | Y ). Alternatively  a straightforward independence chain Metropolis Hastings
algorithm (see Supplementary Material) is deﬁned by iteratively proposing A′ ∼ q which is accepted
with probability min{r(A′ | A)  1} where A is a previous sample of a hierarchical partition and

r(A′ | A) = p(Y | A′)p(A′)q(A)/[p(Y | A)p(A)q(A′)].

(16)

The tailoring of the proposal distribution q to this application based on normalized cuts dramatically
aids in improving the acceptance rate relative to more naive tree proposals. However  the acceptance

5

rate tends to decrease as higher posterior probability partitions A are discovered  especially for trees
with many levels and large input spaces X for which the search space is larger.
One beneﬁt of the MCMC approach over importance sampling is the ability to include more intricate
tree proposals to increase efﬁciency. We choose to interleave both local and global tree proposals. At
i) and then propose a
each iteration  we ﬁrst randomly select a node in the tree (i.e.  a partition set Aℓ
new sequence of cuts for all children of this node. When the root node is selected  corresponding to
A0  the proposal is equivalent to the global proposals previously considered. We adapt the proposal
distribution for node selection to encourage more global searches at ﬁrst and then shift towards a
greater balance between local and global searches as the sampling progresses. Sequential Monte
Carlo methods [4] can also be considered  with particles generated as global proposals.
Computational Complexity The per iteration complexity is O(n3)  equivalent to a typical like-
lihood evaluation under a GP prior. Using dynamic programming  the cost associated with the nor-
malized cuts proposal is O(n2(L − 1)). Standard techniques for more efﬁcient GP computations are
readily applicable  as well as extensions that harness the additive block structure of the covariance.
6 Related Work
Various aspects of the mGP have similarities to other models proposed in the literature that primarily
fall into two main categories: (i) GPs deﬁned over a partitioned input space  and (ii) collections of
GPs deﬁned at tree nodes. The treed GP [8] captures non-stationarities by deﬁning independent GPs
at the leaves of a Bayesian CART-partitioned input space. The related approach of [12] assumes
a Voronoi tessellation. For time series  [21] examines online inference of changepoints with GPs
modeling the data within each segment. These methods capture abrupt changes  but do not allow for
long-range dependencies spanning changepoints nor a functional data hierarchical structure  both
inherent to our multiresolution perspective. A main motivation of the treed GP is the resulting
computational speed-ups of an independently partitioned GP. A two-level hierarchical GP also aimed
at computational efﬁciency is considered by [16]  where the top-level GP is deﬁned at a coarser scale
and provides a piece-wise constant mean for lower-level GPs on a pre-partitioned input space.

[10  11] consider covariance functions deﬁned on a phylogenetic tree such that the covariance be-
tween function-valued traits depends on both their spatial distance and evolutionary time spanned
via a common ancestor. Here  the tree deﬁnes the strength and structure of sharing between a col-
lection of functions rather than abrupt changes within the function. The Bayesian rose tree of [3]
considers a mixture of GP experts  as in [14  17]  but using Bayesian hierarchical clustering with
arbitrary branching structure in place of a Dirichlet process mixture. Such an approach is funda-
mentally different from the mGP: each GP is deﬁned over the entire input space  data result from a
GP mixture  and input points are not necessarily spatially clustered. Alternatively  multiscale pro-
cesses have a long history (cf. [25]): the variables deﬁne a Markov process on a typically balanced 
binary tree and higher-level nodes capture coarser level information about the process. In contrast 
the higher level nodes in the mGP share the same temporal resolution and only vary in smoothness.

At a high level  the mGP differs from previous GP-based tree models in that the nodes of our tree
represent GPs over a contiguous subset of the input space X constrained in a hierarchical fashion.
Thus  the mGP combines ideas of GP-based tree models and GP-based partition models.

As presented in Sec. 3  one can formulate an mGP as an additive GP where each GP in the sum
decomposes independently over the level-speciﬁc partition of the input space X . The additive GPs
of [6] instead focus on coping with multivariate inputs  in a similar vain to hierarchical kernel learn-
ing [1]  thus addressing an inherently different task.
7 Results
7.1 Synthetic Experiments
To assess our ability to infer a hierarchical partition via the proposed MCMC sampler  we generated
100 trials of length 200 from a 5-level mGP with a shared parent function f 0. The hyperparameters
were set to σ2 = 0.1  κ = 10  dℓ = d0 exp(−0.5(ℓ + 1)) for ℓ = 0  . . .   L − 1 with d0 = 5. The
data are shown in Fig. 3  along with the empirical correlation matrix that is used as the cost matrix
for the normalized cuts proposals.

For inference  we set σ2 = ˆσ2/3 and dℓ = (ˆσ2/3) exp(−0.5ℓ)  where ˆσ2 is the average time-
speciﬁc sample variance. κ was as in the simulation. The hyperparameter mismatch demonstrates

6

20

40

60

80

100

120

140

160

180

200

20

40

60

80

100

(a)

20

40

60

80

100

120

140

160

180

200

20

40

60

120

140

160

180

200

80

100

(b)

d
o
o
h

i
l

i

e
k
L
g
o
L

 

−2.88 x 104

−2.9

−2.92

−2.94

−2.96

−2.98

−3
0

120

140

160

180

200

 

mGP
hGP
GP

0.3

0.2

0.1

0

0

f
 

−
 
)

0

f
(
d
e
a
m

t

d
o
o
h

i
l

i

e
k
L
g
o
L

 

 
t

u
o
d
e
H

l

−300

−350

−400

−450

−500

i
t
s
e

−0.1

−0.2

 

0

3000

2000

1000
Iteration
(c)

150

200

100
Time

50
(d)

G P

hG P

L=2
(e)

L=5

L=7

L=10

Figure 5: For the data of Fig. 3  (a) true and (b) MAP partitions. (c) Trace plots of log likelihood versus
MCMC iteration for 10 chains. Log likelihood under the true partition (cyan) and minimized normalized cut
partition of Fig. 3 (magenta) are also shown. (d) Errors between posterior mean f 0 and true f 0 for GP  hGP  and
mGP. (e) Predictive log likelihood of 10 heldout sequences for GP  hGP  and mGP with L = 2  5(true)  7  10.

some robustness to mispeciﬁcation. For a uniform prior p(A)  10 independent MCMC chains were
run for 3000 iterations  thinned by 10. The ﬁrst 1000 iterations used pure global tree searches; the
sampler was then tempered to uniform node proposals. The effects of this choice are apparent in
the likelihood plot of Fig. 5  which also displays the true hierarchical partition and MAP estimate.
Compare to the normalized cuts partition of Fig. 3  especially at the important level 1 cut. The full
simulation study took less than 7 minutes to run on a single 1.8 GHz Intel Core i7 processor.

To assess sensitivity to the choice of L  we compare the predictive log-likelihood of 10 heldout test
sequences under an mGP with 2  5  7  and 10 levels. As shown in Fig. 5(e)  there is a clear gain
going from 2 to 5 levels. However  overestimating L has minimal inﬂuence on predictive likelihood
since lower tree levels capture ﬁner details and have less overall effect. We also compare to a single
GP and a 2-level hierarchical GP (hGP) (see Sec. 7.2). For a direct comparison  both use squared
exponential kernels. Hyperparameters were set as in the mGP for the top-level GP. The total variance
was also matched with the GP taking this as noise and the hGP splitting between level 2 and noise.
In addition to better predictive performance  Fig. 5(d) shows the mGP’s improved estimation of f 0.
7.2 MEG Analysis
We analyzed magnetoencephalography (MEG) recordings of neuronal activity collected from a hel-
met with gradiometers distributed over 102 locations around the head. The gradiometers measure
the spatial gradient of the magnetic activity in Teslas per meter (T/m) [9]. Since the ﬁrings of neu-
rons in the brain only induce a weak magnetic ﬁeld outside of the skull  the signal-to-noise ratio of
the MEG data is very low and typically multiple recordings  or trials  of a given task are collected.
Our MEG data was recorded while a subject viewed 20 stimuli describing concrete nouns (both
the written noun and a representative line drawing)  with 20 interleaved trials per word. See the
Supplementary Material for further details on the data and our analyses presented herein.

Efﬁcient sharing of information between the single trials is important for tasks such as word clas-
siﬁcation [7]. A key insight of [7] was the importance of capturing the time-varying correlations
between MEG sensors for performing classiﬁcation. However  the formulation still necessitates a
mean model. [7] propose a 2-level hierarchical GP (hGP): a parent GP captures the common global
trajectory  as in the mGP  and each trial-speciﬁc GP is centered about the entire parent function1.
This formulation maintains global smoothness at the individual trial level. The mGP instead mod-
els the trial-speciﬁc variability with a multi-level tree of GPs deﬁned as deviations from the parent
function over local partitions  allowing for abrupt changes relative to the smooth global trajectory.

For our analyses  we consider the words associated with the “building” and “tool” categories shown
in Fig. 7.
Independently for each of the 10 words and 102 sensors  we trained a 5-level mGP
using 15 randomly selected trials as training data and the 5 remaining for testing. Each trial was
of length n = 340. We ran 3 independent MCMC chains for 3000 iterations with both global and
local tree searches. We discarded the ﬁrst 1000 samples as burn-in and thinned by 10. The mGP
hyperparameters were set exactly as in the simulated study of Sec. 7.1 for structure learning and
then optimized over a grid to maximize the marginal likelihood of the training data.

We compare the predictive performance of the mGP in terms of MSE of heldout segments relative to
a GP and hGP  each with similarly optimized hyperparameters. The predictive mean conditioned on
data up to the heldout time is straightforwardly derived from Eq. (12). For the mGP  the calculation is
averaged over the posterior samples of A. Fig. 6 displays the MSEs decomposed by cortical region.

1The model of [7] uses an hGP in a latent space. The mGP could be similarly deployed.

7

P
G

 
.
v
 
E
S
M
 
n
i
 
e
s
a
e
r
c
e
D
%

 

25

20

15

10

5

0

−5

 

 

Visual
Frontal
Parietal
Temporal

 

Visual
Frontal
Parietal
Temporal

P
G
h
 
.
v
 
E
S
M
 
n
i
 
e
s
a
e
r
c
e
D
%

 

25

20

15

10

5

0

−5

 

s
n
o

i
t

a
v
r
e
s
b
O

10

0

−10

−20

 

0

MLE
hGP
mGP

1.5

 

x 104

d
o
o
h

i
l

i

e
k
L
 
g
o
L
 
t
u
o
d
e
H

l

−5

−6

−7

−8

wfm m

100

150

250
Conditioning Point

200

300

100

150

250
Conditioning Point

200

300

(a)

(b)

0.5

1

Time (sec)
(c)

m G P

(d)

Visual L=1

Frontal L=1

τ :τ +30 conditioned on y ∗

∗ for the mGP and wavelet-based method of [15].

∗. (d) Boxplots of predictive log likelihood of heldout y

Figure 6: Per-lobe comparison of mGP to (a) GP and (b) hGP: For various values of τ   % decrease in predictive
MSE of heldout y ∗
1:τ −1 and 15 training sequences. (c) For a visual cortex sensor and
word hammer  plots of test data  empirical mean (MLE)  and hGP and mGP predictive mean for entire heldout
y
The results clearly indicate that the mGP consistently bet-
ter captures the features of the data  and particularly for
sensors with large abrupt changes such as in the visual
cortex. The heldout trials for a visual cortex sensor are
displayed in Fig. 6(c). Relative to the hGP  the mGP
much better tracks the early dip in activity right after the
visual stimulus onset (t = 0). The posterior distribu-
tion of inferred changepoints at level 1  also broken down
by cortical region  are displayed in Fig. 7. As expected 
the visual cortex has the earliest changepoints. Similar
trends are seen in the parietal lobe that handles percep-
tion and sensory integration. The temporal lobe  which
is key in semantic processing  has changepoints occur-
ring later. These results concur with the ﬁndings of [23]:
semantic processing starts between 250 and 600 ms and
word length (a visual feature) is decoded most accurately
very near the standard 100ms response time (“n100”).

Figure 7: Inferred changepoints at level 1
aggregated over sensors within each lobe:
visual (top-left)  frontal (top-right)  parietal
(bottom-left)  and temporal (bottom-right).

igloo
house
church
apartment
barn
hammer
saw
screwdriver
pliers
chisel

igloo
house
church
apartment
barn
hammer
saw
screwdriver
pliers
chisel

Temporal L=1

Parietal L=1

Time (sec)

Time (sec)

0.5

Time (sec)

1

0.5

Time (sec)

1

0.5

1

0

0

0.5

1

0

0

We also compare our predictive performance to that of the wavelet-based functional mixed model
(wfmm) of [15]. The wfmm has become a standard approach for functional data analysis since it
allows for spiky trajectories and efﬁcient sharing of information between trials. One limitation  how-
ever  is the restriction to a regular grid of observations. The wfmm enables analysis in a multivariate
setting  but for a direct comparison we simply apply the wfmm to each word and sensor indepen-
dently. Fig. 6(d) shows boxplots of the predictive heldout log likelihood of the test trials under the
mGP and wfmm. The results are over 5 heldout trials  102 sensors  and 10 words. In addition to the
easier interpretability of the mGP  the predictive performance also exceeds that of the wfmm.
8 Discussion
The mGP provides a ﬂexible framework for characterizing the dependence structure of real data 
such as the examined MEG recordings  capturing certain features more accurately than previous
models. In particular  the mGP provides a hierarchical functional data analysis framework for mod-
eling (i) strong  locally smooth sharing of information  (ii) global long-range correlations  and (iii)
abrupt changes. The simplicity of the mGP formulation enables further theoretical analysis  for
example  combining posterior consistency results from changepoint analysis with those for GPs.
Although we focused on univariate time series analysis  our formulation is amenable to multivari-
ate functional data analysis extensions: one can naturally accommodate hierarchical dependence
structures through partial sharing of parents in the tree  or possibly via mGP factor models.

There are many interesting questions relating to the proposed covariance function. Our fractal spec-
iﬁcation represents a particular choice to avoid over-parameterization  although alternatives could
be considered. For hyperparameter inference  we anticipate that joint sampling with the partition
would mix poorly  and consider it a topic for future exploration. Another interesting topic is to
explore proposals for more general tree structures. We believe that the proposed mGP represents a
powerful  broadly applicable new framework for non-stationary analyses  especially in a functional
data analysis setting  and sets the foundation for many interesting possible extensions.
Acknowledgments
The authors thank Alona Fyshe  Gustavo Sudre and Tom Mitchell for their help with data acquisition  prepro-
cessing  and useful suggestions. This work was supported in part by AFOSR Grant FA9550-12-1-0453 and the
National Institute of Environmental Health Sciences (NIEHS) of the NIH under Grant R01 ES017240.

8

References
[1] F. Bach. High-dimensional non-linear variable selection through hierarchical kernel learning. Technical

Report 0909.0844v1  arXiv  2009.

[2] J. Beran and Y. Shumeyko. On asymptotically optimal wavelet estimation of trend functions under long-

range dependence. Bernoulli  18(1):137–176  2012.

[3] C. Blundell  Y. W. Teh  and K. A. Heller. Bayesian rose trees. In Proc. Uncertainty in Artiﬁcial Intelli-

gence  pages 65–72  2010.

[4] P. Del Moral  A. Doucet  and A. Jasra. Sequential Monte Carlo samplers. Journal of the Royal Statistical

Society  Series B  68(3):411–436  2006.

[5] F. X. Diebold and G. D. Rudebusch. Long memory and persistence in aggregate output. Journal of

Monetary Economics  24:189–209  1989.

[6] D. Duvenaud  H. Nickisch  and C. E. Rasmussen. Additive Gaussian processes. In Advances in Neural

Information Processing Systems  volume 24  pages 226–234  2011.

[7] A. Y. Fyshe  E. B. Fox  D. B. Dunson  and T. Mitchell. Hierarchical latent dictionaries for models of
brain activation. In Proc. International Conference on Artiﬁcial Intelligence and Statistics  pages 409–
421  2012.

[8] R. .B. Gramacy and H. K. H. Lee. Bayesian treed Gaussian process models with an application to com-

puter modeling. Journal of the American Statistical Association  103(483):1119–1130  2008.

[9] P. Hansen  M. Kringelbach  and R. Salmelin. MEG: An Introduction to Methods. Oxford University

Press  USA  2010. ISBN 0195307232.

[10] R. Henao and J. E. Lucas. Efﬁcient hierarchical clustering for continuous data. Technical Report

1204.4708v1  arXiv  2012.

[11] N. S. Jones and J. Moriarty. Evolutionary inference for function-valued traits: Gaussian process regression

on phylogenies. Technical Report 1004.4668v2  arXiv  2011.

[12] H. M. Kim  B. K. Mallick  and C. C. Holmes. Analyzing nonstationary spatial data using piecewise

Gaussian processes. Journal of the American Statistical Association  100(470):653–668  2005.

[13] P. S. Kokoszka and M. S. Taqqu. Parameter estimation for inﬁnite variance fractional ARIMA. The

Annals of Statistics  24(5):1880–1913  1996.

[14] E. Meeds and S. Osindero. An alternative mixture of Gaussian process experts. In Advances in Neural

Information Processing Systems  volume 18  pages 883–890  2006.

[15] J. S. Morris and R. J. Carroll. Wavelet-based functional mixed models. Journal of the Royal Statistical

Society  Series B  68(2):179–199  2006.

[16] S. Park and S. Choi. Hierarchical Gaussian process regression. In Proc. Asian Conference on Machine

Learning  pages 95–110  2010.

[17] C. E. Rasmussen and Z. Ghahramani.

Neural Information Processing Systems  volume 2  pages 881–888  2002.

Inﬁnite mixtures of Gaussian process experts.

In Advances in

[18] C. E. Rasmussen and C. K. .I. Williams. Gaussian Processes for Machine Learning. MIT Press  2006.
[19] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer  2005.
[20] D. M. Roy and Y. W. Teh. The Mondrian process. In Advances in Neural Information Processing Systems 

volume 21  pages 1377–1384  2009.

[21] Y. Saatci  R. Turner  and C. E. Rasmussen. Gausssian process change point models. In Proc. International

Conference on Machine Learning  pages 927–934  2010.

[22] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis

and Machine Intelligence  22(8):888–905  2000.

[23] G. Sudre  D. Pomerleaum  M. Palatucci  L. Wehbe  A. Fyshe  R. Salmelin  and T. Mitchell. Tracking
neural coding of perceptual and semantic features of concrete nouns. Neuroimage  62(1):451–463  2012.

[24] U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing  17(4):395–416  2007.
[25] A. S. Willsky. Multiresolution Markov models for signal and image processing. Proceedings of the IEEE 

90(8):1396–1458  2002.

9

,Assaf Glazer
Michael Lindenbaum
Shaul Markovitch
Jun-Yan Zhu
Richard Zhang
Deepak Pathak
Trevor Darrell
Alexei Efros
Oliver Wang