2012,Reducing statistical time-series problems to binary classification,We  show how binary classification methods developed to work on i.i.d. data can be  used for solving  statistical problems that are seemingly unrelated to classification and concern highly-dependent time series.  Specifically  the problems of time-series  clustering  homogeneity testing and the three-sample problem  are addressed. The algorithms that we construct for solving  these problems are based on a new metric between time-series distributions  which can be evaluated using binary classification methods.  Universal consistency of the  proposed algorithms  is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data.,Reducing statistical time-series problems to binary

classiﬁcation

Daniil Ryabko

SequeL-INRIA/LIFL-CNRS 
Universit´e de Lille  France
daniil@ryabko.net

J´er´emie Mary

SequeL-INRIA/LIFL-CNRS 
Universit´e de Lille  France

Jeremie.Mary@inria.fr

Abstract

We show how binary classiﬁcation methods developed to work on i.i.d. data can
be used for solving statistical problems that are seemingly unrelated to classiﬁ-
cation and concern highly-dependent time series. Speciﬁcally  the problems of
time-series clustering  homogeneity testing and the three-sample problem are ad-
dressed. The algorithms that we construct for solving these problems are based
on a new metric between time-series distributions  which can be evaluated using
binary classiﬁcation methods. Universal consistency of the proposed algorithms
is proven under most general assumptions. The theoretical results are illustrated
with experiments on synthetic and real-world data.

1

Introduction

Binary classiﬁcation is one of the most well-understood problems of machine learning and statistics:
a wealth of efﬁcient classiﬁcation algorithms has been developed and applied to a wide range of
applications. Perhaps one of the reasons for this is that binary classiﬁcation is conceptually one of
the simplest statistical learning problems. It is thus natural to try and use it as a building block for
solving other  more complex  newer or just different problems; in other words  one can try to obtain
efﬁcient algorithms for different learning problems by reducing them to binary classiﬁcation. This
approach has been applied to many different problems  starting with multi-class classiﬁcation  and
including regression and ranking [3  16]  to give just a few examples. However  all of these problems
are formulated in terms of independent and identically distributed (i.i.d.) samples. This is also the
assumption underlying the theoretical analysis of most of the classiﬁcation algorithms.
In this work we consider learning problems that concern time-series data for which independence
assumptions do not hold. The series can exhibit arbitrary long-range dependence  and different time-
series samples may be interdependent as well. Moreover  the learning problems that we consider —
the three-sample problem  time-series clustering  and homogeneity testing — at ﬁrst glance seem
completely unrelated to classiﬁcation.
We show how the considered problems can be reduced to binary classiﬁcation methods. The results
include asymptotically consistent algorithms  as well as ﬁnite-sample analysis. To establish the con-
sistency of the suggested methods  for clustering and the three-sample problem the only assumption
that we make on the data is that the distributions generating the samples are stationary ergodic; this
is one of the weakest assumptions used in statistics. For homogeneity testing we have to make some
mixing assumptions in order to obtain consistency results (this is indeed unavoidable [22]). Mixing
conditions are also used to obtain ﬁnite-sample performance guarantees for the ﬁrst two problems.
The proposed approach is based on a new distance between time-series distributions (that is  be-
tween probability distributions on the space of inﬁnite sequences)  which we call telescope distance.
This distance can be evaluated using binary classiﬁcation methods  and its ﬁnite-sample estimates
are shown to be asymptotically consistent. Three main building blocks are used to construct the tele-

1

scope distance. The ﬁrst one is a distance on ﬁnite-dimensional marginal distributions. The distance
we use for this is the following: dH(P  Q) := suph∈H |EP h − EQh| where P  Q are distributions
and H is a set of functions. This distance can be estimated using binary classiﬁcation methods 
and thus can be used to reduce various statistical problems to the classiﬁcation problem. This dis-
tance was previously applied to such statistical problems as homogeneity testing and change-point
estimation [14]. However  these applications so far have only concerned i.i.d. data  whereas we
want to work with highly-dependent time series. Thus  the second building block are the recent
results of [1  2]  that show that empirical estimates of dH are consistent (under certain conditions
on H) for arbitrary stationary ergodic distributions. This  however  is not enough: evaluating dH
for (stationary ergodic) time-series distributions means measuring the distance between their ﬁnite-
dimensional marginals  and not the distributions themselves. Finally  the third step to construct the
distance is what we call telescoping. It consists in summing the distances for all the (inﬁnitely many)
ﬁnite-dimensional marginals with decreasing weights.
We show that the resulting distance (telescope distance) indeed can be consistently estimated based
on sampling  for arbitrary stationary ergodic distributions. Further  we show how this fact can be
used to construct consistent algorithms for the considered problems on time series. Thus we can
harness binary classiﬁcation methods to solve statistical learning problems concerning time series.
To illustrate the theoretical results in an experimental setting  we chose the problem of time-series
clustering  since it is a difﬁcult unsupervised problem which seems most different from the prob-
lem of binary classiﬁcation. Experiments on both synthetic and real-world data are provided. The
real-world setting concerns brain-computer interface (BCI) data  which is a notoriously challenging
application  and on which the presented algorithm demonstrates competitive performance.
A related approach to address the problems considered here  as well some related problems about
stationary ergodic time series  is based on (consistent) empirical estimates of the distributional dis-
tance  see [23  21  13] and [8] about the distributional distance. The empirical distance is based on
counting frequencies of bins of decreasing sizes and “telescoping.” A similar telescoping trick is
used in different problems  e.g. sequence prediction [19]. Another related approach to time-series
analysis involves a different reduction  namely  that to data compression [20].
Organisation. Section 2 is preliminary. In Section 3 we introduce and discuss the telescope dis-
tance. Section 4 explains how this distance can be calculated using binary classiﬁcation methods.
Sections 5 and 6 are devoted to the three-sample problem and clustering  respectively. In Section 7 
under some mixing conditions  we address the problems of homogeneity testing  clustering with
unknown k  and ﬁnite-sample performance guarantees. Section 8 presents experimental evaluation.
Some proofs are deferred to the supplementary material.

2 Notation and deﬁnitions
Let (X  FX ) be a measurable space (the domain). Time-series (or process) distributions are proba-
 FN) of one-way inﬁnite sequences (where FN is the Borel sigma-
bility measures on the space (X
N). We use the abbreviation X1..k for X1  . . .   Xk. All sets and functions introduced
algebra of X
below (in particular  the sets Hk and their elements) are assumed measurable.
A distribution ρ is stationary if ρ(X1..k ∈ A) = ρ(Xn+1..n+k ∈ A) for all A ∈ FX k  k  n ∈ N
(with FX k being the sigma-algebra of X k). A stationary distribution is called (stationary) ergodic
IXi..i+k∈A = ρ(A) ρ-a.s. for every A ∈ FX k  k ∈ N. (This deﬁnition 
if limn→∞ 1
n
which is more suited for the purposes of this work  is equivalent to the usual one expressed in terms
of invariant sets  see e.g. [8].)

i=1..n−k+1

N

(cid:80)

3 A distance between time-series distributions
We start with a distance between distributions on X   and then we will extend it to distributions on
X ∞. For two probability distributions P and Q on (X  F) and a set H of measurable functions on
X   one can deﬁne the distance

dH(P  Q) := sup
h∈H

|EP h − EQh|.

2

Special cases of this distance are Kolmogorov-Smirnov [15]  Kantorovich-Rubinstein [11] and
Fortet-Mourier [7] metrics; the general case has been studied since at least [26].
We will be interested in the cases where dH(P  Q) = 0 implies P = Q. Note that in this case dH
is a metric (the rest of the properties are easy to see). For reasons that will become apparent shortly
(see Remark below)  we will be mainly interested in the sets H that consist of indicator functions.
In this case we can identify each f ∈ H with the set {x : f (x) = 1} ⊂ X and (by a slight abuse
of notation) write dH(P  Q) := suph∈H |P (h) − Q(h)|. It is easy to check that in this case dH is
a metric if and only if H generates F. The latter property is often easy to verify directly. First
of all  it trivially holds for the case where H is the set of halfspaces in a Euclidean X . It is also
easy to check that it holds if H is the set of halfspaces in the feature space of most commonly used
kernels (provided the feature space is of the same or higher dimension than the input space)  such as
polynomial and Gaussian kernels.
Based on dH we can construct a distance between time-series probability distributions. For two
time-series distributions ρ1  ρ2 we take the dH between k-dimensional marginal distributions of ρ1
and ρ2 for each k ∈ N  and sum them all up with decreasing weights.
Deﬁnition 1 (telescope distance D). For two time series distributions ρ1 and ρ2 on the space
(X∞ F∞) and a sequence of sets of functions H = (H1 H2  . . . ) deﬁne the telescope distance

wk sup
h∈Hk

DH(ρ1  ρ2) :=

|Eρ1h(X1  . . .   Xk) − Eρ2h(Y1  . . .   Yk)| 
where wk  k ∈ N is a sequence of positive summable real weights (e.g. wk = 1/k2).
Lemma 1. DH is a metric if and only if dHk is a metric for every k ∈ N.
Proof. The statement follows from the fact that two process distributions are the same if and only if
all their ﬁnite-dimensional marginals coincide.
Deﬁnition 2 (empirical telescope distance ˆD). For a pair of samples X1..n and Y1..m deﬁne empir-
ical telescope distance as
ˆDH(X1..n  Y1..m) :=

(1)

∞(cid:88)

k=1

wk sup
h∈Hk

1

n − k + 1

h(Xi..i+k−1) −

1

m − k + 1

h(Yi..i+k−1)

(2)

min{m n}(cid:88)

k=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

n−k+1(cid:88)

i=1

m−k+1(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

All the methods presented in this work are based on the empirical telescope distance. The key fact
is that it is an asymptotically consistent estimate of the telescope distance  that is  the latter can be
consistently estimated based on sampling.
Theorem 1. Let H = (H1 H2  . . . )  Hk ⊂ X k  k ∈ N be a sequence of separable sets of indicator
functions of ﬁnite VC dimension such that Hk generates FX k. Then  for every stationary ergodic
time series distributions ρX and ρY generating samples X1..n and Y1..m we have

lim

n m→∞

ˆDH(X1..n  Y1..m) = DH(ρX   ρY )

(3)

The proof is deferred to the supplementary material. Note that ˆDH is a biased estimate of DH  and 
unlike in the i.i.d. case  the bias may depend on the distributions; however  the bias is o(n).
Remark. The condition that the sets Hk are sets of indicator function of ﬁnite VC dimension
comes from [2]  where it is shown that for any stationary ergodic distribution ρ  under these
conditions  suph∈Hk
h(Xi..i+k−1) is an asymptotically consistent estimate of
Eρh(X1  . . .   Xk). This fact implies that dH can be consistently estimated  from which the the-
orem is derived.

(cid:80)n−k+1

n−k+1

i=1

1

4 Calculating ˆDH using binary classiﬁcation methods

The methods for solving various statistical problems that we suggest are all based on ˆDH. The main
appeal of this approach is that ˆDH can be calculated using binary classiﬁcation methods. Here we
explain how to do it.

3

m−k+1(cid:88)

m−k+1(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

n−k+1(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

n−k+1(cid:88)

i=1

The deﬁnition (2) of DH involves calculating l summands (where l := min{n  m})  that is

1

n − k + 1

h(Xi..i+k−1) −

1

sup
h∈Hk

(4)
for each k = 1..l. Assuming that h ∈ Hk are indicator functions  calculating each of the summands
amounts to solving the following k-dimensional binary classiﬁcation problem. Consider Xi..i+k−1 
i = 1..n − k + 1 as class-1 examples and Yi..i+k−1  i = 1..m − k + 1 as class-0 examples. The
supremum (4) is attained on h ∈ Hk that minimizes the empirical risk  with examples wighted with
respect to the sample size. Indeed  then we can deﬁne the weighted empirical risk of any h ∈ Hk as

m − k + 1

h(Yi..i+k−1)

i=1

1

n − k + 1

(1 − h(Xi..i+k−1)) +

1

m − k + 1

h(Yi..i+k−1)

which is obviously minimized by any h ∈ Hk that attains (4).
Thus  as long as we have a way to ﬁnd h ∈ Hk that minimizes empirical risk  we have a consistent
estimate of DH(ρX   ρY )  under the mild conditions on H required by Theorem 1. Since the di-
mension of the resulting classiﬁcation problems grows with the length of the sequences  one should
prefer methods that work in high dimensions  such as soft-margin SVMs [6].
A particularly remarkable feature is that the choice of Hk is much easier for the problems that we
consider than in the binary classiﬁcation problem. Speciﬁcally  if (for some ﬁxed k) the classiﬁer
that achieves the minimal (Bayes) error for the classiﬁcation problem is not in Hk  then obviously
the error of an empirical risk minimizer will not tend to zero  no matter how much data we have. In
contrast  all we need to achieve asymptotically 0 error in estimating ˆD (and therefore  in the learning
problems considered below) is that the sets Hk asymptotically generate FX k and have a ﬁnite VC
dimension (for each k). This is the case already for the set of hyperplanes in Rk! Thus  while the
choice of Hk (or  say  of the kernel to use in SVM) is still important from the practical point of view 
it is almost irrelevant for the theoretical consistency results. Thus  we have the following.
Claim 1. The approximation error |DH(P  Q) − ˆDH(X  Y )|  and thus the error of the algorithms
below  can be much smaller than the error of classiﬁcation algorithms used to calculate DH(X  Y ).

Finally  we remark that while in (2) the number of summands is l  it can be replaced with any γl such
that γl → ∞  without affecting any asymptotic consistency results. A practically viable choice is
γl = log l; in fact  there is no reason to choose faster growing γn since the estimates for higher-order
summands will not have enough data to converge. This is also the value we use in the experiments.

5 The three-sample problem

We start with a conceptually simple problem known in statistics as the three-sample problem (some
times also called time-series classiﬁcation). We are given three samples X = (X1  . . .   Xn) 
Y = (Y1  . . .   Ym) and Z = (Z1  . . .   Zl). It is known that X and Y were generated by differ-
ent time-series distributions  whereas Z was generated by the same distribution as either X or Y . It
is required to ﬁnd out which one is the case. Both distributions are assumed to be stationary ergodic 
but no further assumptions are made about them (no independence  mixing or memory assump-
tions). The three sample-problem for dependent time series has been addressed in [9] for Markov
processes and in [23] for stationary ergodic time series. The latter work uses an approach based on
the distributional distance.
Indeed  to solve this problem it sufﬁces to have consistent estimates of some distance between time
series distributions. Thus  we can use the telescope distance. The following statement is a simple
corollary of Theorem 1.
Theorem 2. Let the samples X = (X1  . . .   Xn)  Y = (Y1  . . .   Ym) and Z = (Z1  . . .   Zl) be
generated by stationary ergodic distributions ρX   ρY and ρZ  with ρX (cid:54)= ρY and either (i) ρZ = ρX
or (ii) ρZ = ρY . Assume that the sets Hk ⊂ X k  k ∈ N are separable sets of indicator functions of
ﬁnite VC dimension such that Hk generates FX k. A test that declares (i) if ˆDH(Z  X) ≤ ˆDH(Z  Y )
and (ii) otherwise makes only ﬁnitely many errors with probability 1 as n  m  l → ∞.
It is straightforward to extend this theorem to more than two classes; in other words  instead of X
and Y one can have an arbitrary number of samples from different stationary ergodic distributions.

4

6 Clustering time series

1   . . .   X 1
n1

1   . . .   X N
nN

)  . . .   X N = (X N

) generated by k dif-
We are given N samples X 1 = (X 1
ferent stationary ergodic time-series distributions ρ1  . . .   ρk. The number k is known  but the dis-
tributions are not. It is required to group the N samples into k groups (clusters)  that is  to output
a partitioning of {X1..XN} into k sets. While there may be many different approaches to deﬁne
what is a good clustering (and  in general  deciding what is a good clustering is a difﬁcult problem) 
for the problem of classifying time-series samples there is a natural choice  proposed in [21]: those
samples should be put together that were generated by the same distribution. Thus  deﬁne target
clustering as the partitioning in which those and only those samples that were generated by the same
distribution are placed in the same cluster. A clustering algorithm is called asymptotically consistent
if with probability 1 there is an n(cid:48) such that the algorithm produces the target clustering whenever
maxi=1..N ni ≥ n(cid:48).
Again  to solve this problem it is enough to have a metric between time-series distributions that can
be consistently estimated. Our approach here is based on the telescope distance  and thus we use ˆD.
The clustering problem is relatively simple if the target clustering has what is called the strict sepa-
ration property [4]: every two points in the same target cluster are closer to each other than to any
point from a different target cluster. The following statement is an easy corollary of Theorem 1.
Theorem 3. Assume that the sets Hk ⊂ X k  k ∈ N are separable sets of indicator functions of ﬁnite
VC dimension  such that Hk generates FX k. If the distributions ρ1  . . .   ρk generating the samples
) are stationary ergodic  then with probability 1
X 1 = (X 1
from some n := maxi=1..N ni on the target clustering has the strict separation property with respect
to ˆDH.

1   . . .   X 1
n1

)  . . .   X N = (X N

1   . . .   X N
nN

With the strict separation property at hand  it is easy to ﬁnd asymptotically consistent algorithms.
We will give some simple examples  but the theorem below can be extended to many other distance-
based clustering algorithms.
The average linkage algorithm works as follows. The distance between clusters is deﬁned as the
average distance between points in these clusters. First  put each point into a separate cluster. Then 
merge the two closest clusters; repeat the last step until the total number of clusters is k. The farthest
point clustering works as follows. Assign c1 := X 1 to the ﬁrst cluster. For i = 2..k  ﬁnd the point
X j  j ∈ {1..N} that maximizes the distance mint=1..i ˆDH(X j  ct) (to the points already assigned
to clusters) and assign ci := X j to the cluster i. Then assign each of the remaining points to the
nearest cluster. The following statement is a corollary of Theorem 3.
Theorem 4. Under the conditions of Theorem 3  average linkage and farthest point clusterings are
asymptotically consistent.

Note that we do not require the samples to be independent; the joint distributions of the samples may
be completely arbitrary  as long as the marginal distribution of each sample is stationary ergodic.
These results can be extended to the online setting in the spirit of [13].

7 Speed of convergence

The results established so far are asymptotic out of necessity: they are established under the as-
sumption that the distributions involved are stationary ergodic  which is too general to allow for
any meaningful ﬁnite-time performance guarantees. Moreover  some statistical problems  such as
homogeneity testing or clustering when the number of clusters is unknown  are provably impossible
to solve under this assumption [22].
While it is interesting to be able to establish consistency results under such general assumptions  it
is also interesting to see what results can be obtained under stronger assumptions. Moreover  since
it is usually not known in advance whether the data at hand satisﬁes given assumptions or not  it
appears important to have methods that have both asymptotic consistency in the general setting and
ﬁnite-time performance guarantees under stronger assumptions.
In this section we will look at the speed of convergence of ˆD under certain mixing conditions  and
use it to construct solutions for the problems of homogeneity and clustering with an unknown num-

5

(cid:16)

n−k+1(cid:88)

(cid:17)
h(Xi..i+k−1) − Eρ1 h(X1..k)| > ε

qn(ρ Hk  ε) := ρ

|

1

n − k + 1

sup
h∈Hk

i=1

ber of clusters  as well as to establish ﬁnite-time performance guarantees for the methods presented
in the previous sections.
A stationary distribution on the space of one-way inﬁnite sequences (X N
extended to a stationary distribution on the space of two-way inﬁnite sequences (X Z
form . . .   X−1  X0  X1  . . . .
Deﬁnition 3 (β-mixing coefﬁcients). For a process distribution ρ deﬁne the mixing coefﬁcients

 FN) can be uniquely
 FZ) of the

β(ρ  k) :=

sup

A∈σ(X−∞..0) 
B∈σ(Xk..∞)

|ρ(A ∩ B) − ρ(A)ρ(B)|

where σ(..) denotes the sigma-algebra of the random variables in brackets.
When β(ρ  k) → 0 the process ρ is called absolutely regular; this condition is much stronger than
ergodicity  but is much weaker than the i.i.d. assumption.

7.1 Speed of convergence of ˆD

Assume that a sample X1..n is generated by a distribution ρ that is uniformly β-mixing with coefﬁ-
cients β(ρ  k) Assume further that Hk is a set of indicator functions with a ﬁnite VC dimension dk 
for each k ∈ N.
The general tool that we use to obtain performance guarantees in this section is the following bound
that can be obtained from the results of [12].

≤ nβ(ρ  tn − k) + 8tdk+1

e−lnε2/8 

(5)
where tn are any integers in 1..n and ln = n/tn. The parameters tn should be set according to the
values of β in order to optimize the bound.
One can use similar bounds for classes of ﬁnite Pollard dimension [18] or more general bounds
expressed in terms of covering numbers  such as those given in [12]. Here we consider classes
of ﬁnite VC dimension only for the ease of the exposition and for the sake of continuity with the
previous section (where it was necessary).
Furthermore  for the rest of this section we assume geometric β-mixing distributions  that is 
β(ρ  t) ≤ γt for some γ < 1. Letting ln = tn =

n the bound (5) becomes

√

n

qn(ρ Hk  ε) ≤ nγ

(6)
Lemma 2. Let two samples X1..n and Y1..m be generated by stationary distributions ρX and ρY
whose β-mixing coefﬁcients satisfy β(ρ.  t) ≤ γt for some γ < 1. Let Hk  k ∈ N be some sets of
indicator functions on X k whose VC dimension dk is ﬁnite and non-decreasing with k. Then

nε2/8.

√

n−k + 8n(dk+1)/2e−√

P (| ˆDH(X1..n  Y1..m) − DH(ρX   ρY )| > ε) ≤ 2∆(ε/4  n(cid:48))

where n(cid:48) := min{n1  n2}  the probability is with respect to ρX × ρY and
√
n+log(ε) + 8n(d− log ε+1)/2e−√

∆(ε  n) := − log ε(nγ

nε2/8).

(7)

(8)

7.2 Homogeneity testing

Given two samples X1..n and Y1..m generated by distributions ρX and ρY respectively  the problem
of homogeneity testing (or the two-sample problem) consists in deciding whether ρX = ρY . A test
is called (asymptotically) consistent if its probability of error goes to zero as n(cid:48) := min{m  n} goes
to inﬁnity. In general  for stationary ergodic time series distributions  there is no asymptotically
consistent test for homogeneity [22]  so stronger assumptions are in order.
Homogeneity testing is one of the classical problems of mathematical statistics  and one of the most
studied ones. Vast literature exits on homogeneity testing for i.i.d. data  and for dependent processes

6

as well. We do not attempt to survey this literature here. Our contribution to this line of research is
to show that this problem can be reduced (via the telescope distance) to binary classiﬁcation  in the
case of strongly dependent processes satisfying some mixing conditions.
It is easy to see that under the mixing conditions of Lemma 1 a consistent test for homogeneity exists 
and ﬁnite-sample performance guarantees can be obtained. It is enough to ﬁnd a sequence εn → 0
such that ∆(εn  n) → 0 (see (8). Then the test can be constructed as follows: say that the two se-
quences X1..n and Y1..m were generated by the same distribution if ˆDH(X1..n  Y1..m) < εmin{n m};
otherwise say that they were generated by different distributions. The following statement is an im-
mediate consequence of Lemma 2.
Theorem 5. Under the conditions of Lemma 2 the probability of Type I error (the distributions are
the same but the test says they are different) of the described test is upper-bounded by 4∆(ε/8  n(cid:48)).
The probability of Type II error (the distributions are different but the test says they are the same) is
upper-bounded by 4∆(δ − ε/8  n(cid:48)) where δ := 1/2DH(ρX   ρY ).
The optimal choice of εn may depend on the speed at which dk (the VC dimension of Hk) increases;
however  for most natural cases (recall that Hk are also parameters of the algorithm) this growth is
polynomial so the main term to control is e−√
For example  if Hk is the set of halfspaces in X k = Rk then dk = k + 1 and one can chose
εn := n−1/8. The resulting probability of Type I error decreases as exp(−n1/4).

nε2/8.

7.3 Clustering with a known or unknown number of clusters

If the distributions generating the samples satisfy certain mixing conditions  then we can augment
Theorems 3 and 4 with ﬁnite-sample performance guarantees.
Theorem 6.
1   . . .   X 1
(X 1
n1
δ := mini j=1..N i(cid:54)=j DH(ρi  ρj) and n := mini=1..N ni. Then with probability at least

) satisfy the conditions of Lemma 2.

)  . . .   X N = (X N

samples X 1

Let

the

distributions

=
Deﬁne

generating

the

ρ1  . . .   ρk
1   . . .   X N
nN
1 − N (N − 1)∆(δ/4  n)/2

the target clustering of the samples has the strict separation property. In this case single linkage
and farthest point algorithms output the target clustering.

Proof. Note that a sufﬁcient condition for the strict separation property to hold is that for every one
out of N (N − 1)/2 pairs of samples the estimate ˆDH(X i  X j) i  j = 1..N is within δ/4 of the DH
distance between the corresponding distributions. It remains to apply Lemma 2 to obtain the ﬁrst
statement  and the second statement is obvious (cf. Theorem 4).

As with homogeneity testing  while in the general case of stationary ergodic distributions it is im-
possible to have a consistent clustering algorithm when the number of clusters k is unknown  the
situation changes if the distributions satisfy certain mixing conditions. In this case a consistent clus-
tering algorithm can be obtained as follows. Assign to the same cluster all samples that are at most
εn-far from each other  where the threshold εn is selected the same way as for homogeneity testing:
εn → 0 and ∆(εn  n) → 0. The optimal choice of this parameter depends on the choice of Hk
through the speed of growth of the VC dimension dk of these sets.
Theorem 7. Given N samples generated by k different stationary distributions ρi  i = 1..k (un-
known k) all satisfying the conditions of Lemma 2  the probability of error (misclustering at least
one sample) of the described algorithm is upper-bounded by

2N (N − 1) max{∆(ε/8  n)  ∆(δ − ε/8  n)}

where δ := mini j=1..k i(cid:54)=j DH(ρi  ρj) and n = mini=1..N ni  with ni  i = 1..N being lengths of
the samples.

8 Experiments

For experimental evaluation we chose the problem of time-series clustering. Average-linkage clus-
tering is used  with the telescope distance between samples calculated using an SVM  as described

7

in Section 4. In all experiments  SVM is used with radial basis kernel  with default parameters of
libsvm [5].

8.1 Synthetic data

For the artiﬁcial setting we have chosen highly-dependent time series distributions which have the
same single-dimensional marginals and which cannot be well approximated by ﬁnite- or countable-
state models. The distributions ρ(α)  α ∈ (0  1)  are constructed as follows. Select r0 ∈ [0  1]
uniformly at random; then  for each i = 1..n obtain ri by shifting ri−1 by α to the right  and
removing the integer part. The time series (X1  X2  . . . ) is then obtained from ri by drawing a point
from a distribution law N1 if ri < 0.5 and from N2 otherwise. N1 is a 3-dimensional Gaussian with
mean of 0 and covariance matrix Id×1/4. N2 is the same but with mean 1. If α is irrational1 then the
distribution ρ(α) is stationary ergodic  but does not belong to any simpler natural distribution family
[25]. The single-dimensional marginal is the same for all values of α. The latter two properties
make all parametric and most non-parametric methods inapplicable to this problem.
In our experiments  we use two process distributions ρ(αi)  i ∈ {1  2}  with α1 = 0.31...  α2 =
0.35... . The dependence of error rate on the length of time series is shown on Figure 1. One
clustering experiment on sequences of length 1000 takes about 5 min. on a standard laptop.

8.2 Real data

To demonstrate the applicability of the proposed methods to realistic scenarios  we chose the brain-
computer interface data from BCI competition III [17]. The dataset consists of (pre-processed)
BCI recordings of mental imagery: a person is thinking about one of three subjects (left foot  right
foot  a random letter). Originally  each time series consisted of several consecutive sequences of
different classes  and the problem was supervised: three time series for training and one for testing.
We split each of the original time series into classes  and then used our clustering algorithm in a
completely unsupervised setting. The original problem is 96-dimensional  but we used only the ﬁrst
3 dimensions (using all 96 gives worse performance). The typical sequence length is 300. The
performance is reported in Table 1  labeled TSSVM. All the computation for this experiment takes
approximately 6 minutes on a standard laptop.
The following methods were used for comparison. First  we used dynamic time wrapping (DTW)
[24] which is a popular base-line approach for time-series clustering. The other two methods in
Table 1 are from [10]. The comparison is not fully relevant  since the results in [10] are for different
settings; the method KCpA was used in change-point estimation method (a different but also un-
supervised setting)  and SVM was used in a supervised setting. The latter is of particular interest
since the classiﬁcation method we used in the telescope distance is also SVM  but our setting is
unsupervised (clustering).

s2

s1

s3
TSSVM 84% 81% 61%
46% 41% 36%
DTW
KCpA
79% 74% 61%
76% 69% 60%
SVM

Figure 1: Error of two-class clustering using
TSSVM; 10 time series in each target cluster  av-
eraged over 20 runs.

Table 1: Clustering accuracy in the BCI
dataset.
3 subjects (columns)  4 methods
(rows). Our method is TSSVM.

Acknowledgments. This research was funded by the Ministry of Higher Education and Research  Nord-Pas-
de-Calais Regional Council and FEDER (Contrat de Projets Etat Region CPER 2007-2013)  ANR projects
EXPLO-RA (ANR-08-COSI-004)  Lampada (ANR-09-EMER-007) and CoAdapt  and by the European Com-
munity’s FP7 Program under grant agreements n◦ 216886 (PASCAL2) and n◦ 270327 (CompLACS).

1in the experiments simulated by a longdouble with a long mantissa

8

0200400600800100012000.00.10.20.30.4Time of observationError rateReferences
[1] T. M. Adams and A. B. Nobel. Uniform convergence of Vapnik-Chervonenkis classes under

ergodic sampling. The Annals of Probability  38:1345–1367  2010.

[2] T. M. Adams and A. B. Nobel. Uniform approximation of Vapnik-Chervonenkis classes.

Bernoulli  18(4):1310–1319  2012.

[3] M.-F. Balcan  N. Bansal  A. Beygelzimer  D. Coppersmith  J. Langford  and G. Sorkin. Robust
reductions from ranking to classiﬁcation. In COLT’07  v. 4539 of LNCS  pages 604–619. 2007.
[4] M.F. Balcan  A. Blum  and S. Vempala. A discriminative framework for clustering via simi-

larity functions. In STOC  pp. 671–680. ACM  2008.

[5] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology  2:27:1–27:27  2011. Software available
at http://www.csie.ntu.edu.tw/˜cjlin/libsvm.

[6] C. Cortes and V. Vapnik. Support-vector networks. Mach. Learn.  20(3):273–297  1995.
[7] R. Fortet and E. Mourier. Convergence de la r´epartition empirique vers la r´epartition

th´eoretique. Ann. Sci. Ec. Norm. Super.  III. Ser  70(3):267–285  1953.

[8] R. Gray. Probability  Random Processes  and Ergodic Properties. Springer Verlag  1988.
[9] M. Gutman. Asymptotically optimal classiﬁcation for multiple tests with empirically observed

statistics. IEEE Transactions on Information Theory  35(2):402–408  1989.

[10] Z. Harchaoui  F. Bach  E. Moulines. Kernel change-point analysis. NIPS  pp. 609–616  2008.
[11] L. V. Kantorovich and G. S. Rubinstein. On a function space in certain extremal problems.

Dokl. Akad. Nauk USSR  115(6):1058–1061  1957.

[12] R.L. Karandikara and M. Vidyasagar. Rates of uniform convergence of empirical means with

mixing processes. Statistics and Probability Letters  58:297–307  2002.

[13] A. Khaleghi  D. Ryabko  J. Mary  and P. Preux. Online clustering of processes. In AISTATS 

JMLR W&CP 22  pages 601–609  2012.

[14] D. Kifer  S. Ben-David  J. Gehrke. Detecting change in data streams. VLDB (v.30): 180–191 

2004.

[15] A.N. Kolmogorov. Sulla determinazione empirica di una legge di distribuzione. G. Inst. Ital.

Attuari  pages 83–91  1933.

[16] John Langford  Roberto Oliveira  and Bianca Zadrozny. Predicting conditional quantiles via

reduction to classiﬁcation. In UAI  2006.

[17] Jos´e del R. Mill´an. On the need for on-line learning in brain-computer interfaces. In Proc. of

the Int. Joint Conf. on Neural Networks  2004.

[18] D. Pollard. Convergence of Stochastic Processes. Springer  1984.
[19] B. Ryabko. Prediction of random sequences and universal coding. Problems of Information

Transmission  24:87–96  1988.

[20] B. Ryabko. Compression-based methods for nonparametric prediction and estimation of some
characteristics of time series. IEEE Transactions on Information Theory  55:4309–4315  2009.

[21] D. Ryabko. Clustering processes. In Proc. ICML 2010  pp. 919–926  Haifa  Israel  2010.
[22] D. Ryabko. Discrimination between B-processes is impossible. Journal of Theoretical Proba-

bility  23(2):565–575  2010.

[23] D. Ryabko and B. Ryabko. Nonparametric statistical inference for ergodic processes. IEEE

Transactions on Information Theory  56(3):1430–1435  2010.

[24] H. Sakoe and S. Chiba. Dynamic programming algorithm optimization for spoken word recog-

nition. IEEE Transactions on Acoustics  Speech and Signal Processing  26(1):43–49  1978.

[25] P. Shields. The Ergodic Theory of Discrete Sample Paths. AMS Bookstore  1996.
[26] V. M. Zolotarev. Metric distances in spaces of random variables and their distributions. Math.

USSR-Sb  30(3):373–401  1976.

9

,Xinhua Zhang
Wee Sun Lee
Yee Whye Teh
Jinyan Liu
Zhiyi Huang
Xiangning Wang