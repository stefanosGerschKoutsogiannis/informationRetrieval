2015,On the Optimality of Classifier Chain for Multi-label Classification,To capture the interdependencies between labels in multi-label classification problems  classifier chain (CC)  tries to take the multiple labels of each instance into account under a deterministic high-order Markov Chain model. Since its  performance is sensitive to the choice of label order  the key issue is how to determine the optimal label order for CC. In this work  we first generalize the CC model over a random label order. Then  we present a theoretical analysis of the generalization error for the proposed generalized model. Based on our results  we propose a dynamic programming based classifier chain (CC-DP) algorithm to search the globally optimal label order for CC and a greedy classifier chain (CC-Greedy) algorithm to find a locally optimal CC. Comprehensive experiments on a number of real-world multi-label data sets from various domains demonstrate that our proposed CC-DP algorithm outperforms state-of-the-art approaches and the CC-Greedy algorithm achieves comparable prediction performance with CC-DP.,On the Optimality of Classiﬁer Chain for

Multi-label Classiﬁcation

Weiwei Liu

Ivor W. Tsang∗

Centre for Quantum Computation and Intelligent Systems

University of Technology  Sydney

liuweiwei863@gmail.com  ivor.tsang@uts.edu.au

Abstract

To capture the interdependencies between labels in multi-label classiﬁcation prob-
lems  classiﬁer chain (CC) tries to take the multiple labels of each instance into
account under a deterministic high-order Markov Chain model. Since its perfor-
mance is sensitive to the choice of label order  the key issue is how to determine
the optimal label order for CC. In this work  we ﬁrst generalize the CC model over
a random label order. Then  we present a theoretical analysis of the generaliza-
tion error for the proposed generalized model. Based on our results  we propose
a dynamic programming based classiﬁer chain (CC-DP) algorithm to search the
globally optimal label order for CC and a greedy classiﬁer chain (CC-Greedy)
algorithm to ﬁnd a locally optimal CC. Comprehensive experiments on a num-
ber of real-world multi-label data sets from various domains demonstrate that our
proposed CC-DP algorithm outperforms state-of-the-art approaches and the CC-
Greedy algorithm achieves comparable prediction performance with CC-DP.

1 Introduction

Multi-label classiﬁcation  where each instance can belong to multiple labels simultaneously  has
signiﬁcantly attracted the attention of researchers as a result of its various applications  ranging from
document classiﬁcation and gene function prediction  to automatic image annotation. For example 
a document can be associated with a range of topics  such as Sports  Finance and Education [1]; a
gene belongs to the functions of protein synthesis  metabolism and transcription [2]; an image may
have both beach and tree tags [3].
One popular strategy for multi-label classiﬁcation is to reduce the original problem into many bina-
ry classiﬁcation problems. Many works have followed this strategy. For example  binary relevance
(BR) [4] is a simple approach for multi-label learning which independently trains a binary classiﬁer
for each label. Recently  Dembczynski et al. [5] have shown that methods of multi-label learn-
ing which explicitly capture label dependency will usually achieve better prediction performance.
Therefore  modeling the label dependency is one of the major challenges in multi-label classiﬁca-
tion problems. Many multi-label learning models [5  6  7  8  9  10  11  12] have been developed to
capture label dependency. Amongst them  the classiﬁer chain (CC) model is one of the most popular
methods due to its simplicity and promising experimental results [6].
CC works as follows: One classiﬁer is trained for each label. For the (i + 1)th label  each instance
is augmented with the 1st  2nd  ···   ith label as the input to train the (i + 1)th classiﬁer. Given a
new instance to be classiﬁed  CC ﬁrstly predicts the value of the ﬁrst label  then takes this instance
together with the predicted value as the input to predict the value of the next label. CC proceeds
in this way until the last label is predicted. However  here is the question: Does the label order
affect the performance of CC? Apparently yes  because different classiﬁer chains involve different

(cid:3)

Corresponding author

1

classiﬁers trained on different training sets. Thus  to reduce the inﬂuence of the label order  Read et
al. [6] proposed the ensembled classiﬁer chain (ECC) to average the multi-label predictions of CC
over a set of random chain ordering. Since the performance of CC is sensitive to the choice of label
order  there is another important question: Is there any globally optimal classiﬁer chain which can
achieve the optimal prediction performance for CC? If yes  how can the globally optimal classiﬁer
chain be found?
To answer the last two questions  we ﬁrst generalize the CC model over a random label order. We
then present a theoretical analysis of the generalization error for the proposed generalized model.
Our results show that the upper bound of the generalization error depends on the sum of reciprocal
of square of the margin over the labels. Thus  we can answer the second question: the globally
optimal CC exists only when the minimization of the upper bound is achieved over this CC. To
ﬁnd the globally optimal CC  we can search over q! different label orders1  where q denotes the
number of labels  which is computationally infeasible for a large q. In this paper  we propose the
dynamic programming based classiﬁer chain (CC-DP) algorithm to simplify the search algorithm 
which requires O(q3nd) time complexity. Furthermore  to speed up the training process  a greedy
classiﬁer chain (CC-Greedy) algorithm is proposed to ﬁnd a locally optimal CC  where the time
complexity of the CC-Greedy algorithm is O(q2nd).
Notations: Assume xt ∈ Rd is a real vector representing an input or instance (feature) for t ∈
{1 ···   n}. n denotes the number of training samples. Yt ⊆ {λ1  λ2 ···   λq} is the corresponding
∈ {0  1}q is used to represent the label set Yt  where yt(j) = 1 if and only if
output (label). yt
λj ∈ Yt.

2 Related work and preliminaries

To capture label dependency  Hsu et al. [13] ﬁrst use compressed sensing technique to handle the
multi-label classiﬁcation problem. They project the original label space into a low dimensional label
space. A regression model is trained on each transformed label. Recovering multi-labels from the
regression output usually involves solving a quadratic programming problem [13]  and many works
have been developed in this way [7  14  15]. Such methods mainly aim to use different projection
methods to transform the original label space into another effective label space.
Another important approach attempts to exploit the different orders (ﬁrst-order  second-order and
high-order) of label correlations [16]. Following this way  some works also try to provide a proba-
bilistic interpretation for label correlations. For example  Guo and Gu [8] model the label correla-
tions using a conditional dependency network; PCC [5] exploits a high-order Markov Chain model
to capture the correlations between the labels and provide an accurate probabilistic interpretation of
CC. Other works [6  9  10] focus on modeling the label correlations in a deterministic way  and CC
is one of the most popular methods among them. This work will mainly focus on the deterministic
high-order classiﬁer chain.

2.1 Classiﬁer chain
Similar to BR  the classiﬁer chain (CC) model [6] trains q binary classiﬁers hj (j ∈ {1 ···   q}).
Classiﬁers are linked along a chain where each classiﬁer hj deals with the binary classiﬁcation prob-
lem for label λj. The augmented vector {xt  yt(1) ···   yt(j)}n
t=1 is used as the input for training
classiﬁer hj+1. Given a new testing instance x  classiﬁer h1 in the chain is responsible for predict-
ing the value of y(1) using input x. Then  h2 predicts the value of y(2) taking x plus the predicted
value of y(1) as an input. Following in this way  hj+1 predicts y(j + 1) using the predicted value
of y(1) ···   y(j) as additional input information. CC passes label information between classiﬁers 
allowing CC to exploit the label dependence and thus overcome the label independence problem
of BR. Essentially  it builds a deterministic high-order Markov Chain model to capture the label
correlations.

1! represents the factorial notation.

2

2.2 Ensembled classiﬁer chain

Different classiﬁer chains involve different classiﬁers learned on different training sets and thus the
order of the chain itself clearly affects the prediction performance. To solve the issue of selecting a
chain order for CC  Read et al. [6] proposed the extension of CC  called ensembled classiﬁer chain
(ECC)  to average the multi-label predictions of CC over a set of random chain ordering. ECC ﬁrst
randomly reorders the labels {λ1  λ2 ···   λq} many times. Then  CC is applied to the reordered
labels for each time and the performance of CC is averaged over those times to obtain the ﬁnal
prediction performance.

3 Proposed model and generalization error analysis

3.1 Generalized classiﬁer chain

We generalize the CC model over a random label order  called generalized classiﬁer chain (GCC)
model. Assume the labels {λ1  λ2 ···   λq} are randomly reordered as {ζ1  ζ2 ···   ζq}  where ζj =
λk means label λk moves to position j from k. In the GCC model  classiﬁers are also linked along
a chain where each classiﬁer hj deals with the binary classiﬁcation problem for label ζj (λk). GCC
follows the same training and testing procedures as CC  while the only difference is the label order.
In the GCC model  for input xt  yt(j) = 1 if and only if ζj ∈ Yt.
3.2 Generalization error analysis

In this section  we analyze the generalization error bound of the multi-label classiﬁcation problem
using GCC based on the techniques developed for the generalization performance of classiﬁers with
a large margin [17] and perceptron decision tree [18].
Let X represent the input space. Both s and (cid:22)s are m samples drawn independently according to
an unknown distribution D. We denote logarithms to base 2 by log. If S is a set  |S| denotes its
cardinality. ∥ · ∥ means the l2 norm. We train a support vector machine(SVM) for each label ζj.
Let {xt}n
t=1 as the label  the output parameter of SVM is deﬁned as
[wj  bj] = SV M ({xt  yt(ζ1) ···   yt(ζj−1)}n
t=1). The margin for label ζj is deﬁned
as:

t=1 as the feature and {yt(ζj)}n

t=1 {yt(ζj)}n

(1)

The fat shattering dimension f at(γ) of the set H is a function from the positive real numbers to the
integers which maps a value γ to the size of the largest γ-shattered set  if this is ﬁnite  or inﬁnity
otherwise.
Assume H is the real valued function class and h ∈ H. l(y  h(x)) denotes the loss function. The
expected error of h is deﬁned as erD[h] = E(x y)∼D[l(y  h(x))]  where (x  y) drawn from the
unknown distribution D. Here we select 0-1 loss function. So  erD[h] = P(x y)∼D(h(x) ̸= y).
ers[h] is deﬁned as ers[h] = 1
n
Suppose N (ϵ H  s) is the ϵ-covering number of H with respect to the l∞ pseudo-metric measuring
the maximum discrepancy on the sample s. The notion of the covering number can be referred to
the Supplementary Materials. We introduce the following general corollary regarding the bound of
the covering number:

[yt ̸= h(xt)].2

n∑

t=1

2The expression [yt ̸= h(xt)] evaluates to 1 if yt ̸= h(xt) is true and to 0 otherwise.

3

We begin with the deﬁnition of the fat shattering dimension.
Deﬁnition 1 ([19]). Let H be a set of real valued functions. We say that a set of points P is γ-
shattered by H relative to r = (rp)p∈P if there are real numbers rp indexed by p ∈ P such that for
all binary vectors b indexed by P   there is a function fb ∈ H satisfying

γj =

1||wj||2
{≥ rp + γ

≤ rp − γ

fb(p) =

if bp = 1
otherwise

ϵ2

Corollary 1 ([17]). Let H be a class of functions X → [a  b] and D a distribution over X. Choose
(
0 < ϵ < 1 and let d = f at(ϵ/4) ≤ em. Then
E(N (ϵ H  s)) ≤ 2

)d log(2em(b−a)/(dϵ))

4m(b − a)2

(2)

where the expectation E is over samples s ∈ X m drawn according to Dm.
We study the generalization error bound of the speciﬁed GCC with the speciﬁed number of labels
and margins. Let G be the set of classiﬁers of GCC  G = {h1  h2 ···   hq}. ers[G] denotes the
fraction of the number of errors that GCC makes on s. Deﬁne ^x ∈ X × {0  1}  ^hj(^x) = hj(x)(1 −
y(j))− hj(x)y(j). If an instance x ∈ X is correctly classiﬁed by hj  then ^hj(^x) < 0. Moreover  we
introduce the following proposition:
Proposition 1. If an instance x ∈ X is misclassiﬁed by a GCC model  then ∃hj ∈ G  ^hj(^x) ≥ 0.
Lemma 1. Given a speciﬁed GCC model with q labels and with margins γ1  γ2 ···   γq for each
label satisfying ki = f at(γi/8)  where f at is continuous from the right. If GCC has correctly
classiﬁed m multi-labeled examples s generated independently according to the unknown (but ﬁxed)
distribution D and (cid:22)s is a set of another m multi-labeled examples  then we can bound the following
probability to be less than δ: P 2m{s(cid:22)s : ∃ a GCC model  it correctly classiﬁes s  fraction of (cid:22)s misclas-
siﬁed > ϵ(m  q  δ)} < δ  where ϵ(m  q  δ) = 1
q
).
i=1 ki log( 8em
ki
Proof. (of Lemma 1). Suppose G is a GCC model with q labels and with margins γ1  γ2 ···   γq 
the probability event in Lemma 1 can be described as

m (Q log(32m)+log 2q

δ ) and Q =

∑

A = {s(cid:22)s : ∃G  ki = f at(γi/8)  ers[G] = 0  er(cid:22)s[G] > ϵ}.

Let ^s and ^(cid:22)s denote two different set of m examples  which are drawn i.i.d. from the distribution
D × {0  1}. Applying the deﬁnition of ^x  ^h and Proposition 1  the event can also be written as
^hi(^xt)  2^γi = −ri |{^y ∈ ^(cid:22)s :
A = {^s^(cid:22)s : ∃G  ^γi = γi/2  ki = f at(^γi/4)  ers[G] = 0  ri = maxt
^hi(^xt) means the minimal value of |hi(x)|
∃hi ∈ G  ^hi(^y) ≥ 2^γi + ri}| > mϵ}. Here  −maxt
which represents the margin for label ζi  so 2^γi = −ri. Let γki = min{γ
/4) ≤ ki}  so
γki

≤ ^γi  we deﬁne the following function:

: f at(γ

′

′

if ^h ≥ 0
if ^h ≤ −2γki
otherwise

0

−2γki
^h

π(^h) =

so π(^h) ∈ [−2γki  0]. Let π( ^G) = {π(^h) : h ∈ G}.
^s^(cid:22)s represent the minimal γki-cover set of π( ^G) in the pseudo-metric d^s^(cid:22)s. We have that for
Let Bki
any hi ∈ G  there exists ~f ∈ Bki
^s^(cid:22)s   |π(^hi(^z)) − π( ~f (^z))| < γki  for all ^z ∈ ^s^(cid:22)s. For all ^x ∈ ^s  by
the deﬁnition of ri  ^hi(^x) ≤ ri = −2^γi  and γki
≤ ^γi  ^hi(^x) ≤ −2γki  π(^hi(^x)) = −2γki  so
π( ~f (^x)) < −2γki + γki = −γki. However  there are at least mϵ points ^y ∈ ^(cid:22)s such that ^hi(^y) ≥ 0 
so π( ~f (^y)) > −γki > maxtπ( ~f (^xt)). Since π only reduces separation between output values  we
~f (^xt) holds. Moreover  the mϵ points in ^(cid:22)s with the largest
conclude that the inequality ~f (^y) > maxt
−mϵ of the
~f values must remain for the inequality to hold. By the permutation argument  at most 2
sequences obtained by swapping corresponding points satisfy the conditions for ﬁxed ~f.
As for any hi ∈ G  there exists ~f ∈ Bki
inequality for ki. Note that |Bki
union bound  we get the following inequality:
|) + ··· + E(|Bkq

| possibilities of ~f that satisfy the
| is a positive integer which is usually bigger than 1 and by the

^s^(cid:22)s   so there are |Bki

|) × ··· × E(|Bkq

P (A) ≤ (E(|Bk1

−mϵ ≤ (E(|Bk1

|))2

|))2

−mϵ

^s^(cid:22)s

^s^(cid:22)s

Since every set of points γ-shattered by π( ^G) can be γ-shattered by ^G  so f atπ( ^G)(γ) ≤ f at ^G(γ) 
where ^G = {^h : h ∈ G}. Hence  by Corollary 1 (setting [a  b] to [−2γki   0]  ϵ to γki and m to 2m) 

^s^(cid:22)s

^s^(cid:22)s

^s^(cid:22)s

^s^(cid:22)s

E(|Bki

^s^(cid:22)s

|) = E(N (γki  π( ^G)  ^s^(cid:22)s)) ≤ 2(32m)d log( 8em

d )

4

where d = f atπ( ^G)(γki/4) ≤ f at ^G(γki/4) ≤ ki. Thus E(|Bki
obtain

^s^(cid:22)s

|) ≤ 2(32m)ki log( 8em

ki

)  and we

P (A) ≤ (E(|Bk1
∑

^s^(cid:22)s

where Q =

q
i=1 ki log( 8em
ki

^s^(cid:22)s

|) × ··· × E(|Bkq

|))2
(
). And so (E(|Bk1
ϵ(m  q  δ) ≥ 1
m

^s^(cid:22)s

i=1

)
|))2
|) × ··· × E(|Bkq
^s^(cid:22)s
2q
δ

Q log(32m) + log

2(32m)ki log( 8em

ki

) = 2q(32m)Q

−mϵ < δ provided

−mϵ ≤ q∏

as required.

Lemma 1 applies to a particular GCC model with a speciﬁed number of labels and a speciﬁed margin
for each label. In practice  we will observe the margins after running the GCC model. Thus  we must
bound the probabilities uniformly over all of the possible margins that can arise to obtain a practical
bound. The generalization error bound of the multi-label classiﬁcation problem using GCC is shown
as follows:
Theorem 1. Suppose a random m multi-labeled sample can be correctly classiﬁed using a GCC
model  and suppose this GCC model contains q classiﬁers with margins γ1  γ2 ···   γq for each
label. Then we can bound the generalization error with probability greater than 1− δ to be less than

)

(

130R2

m

′

Q

log(8em) log(32m) + log

2(2m)q

δ

q
i=1

1

(γi)2 and R is the radius of a ball containing the support of the distribution.

∑

where Q

′

=

Before proving Theorem 1  we state one key Symmetrization lemma and Theorem 2.
Lemma 2 (Symmetrization). Let H be the real valued function class. s and (cid:22)s are m samples both
drawn independently according to the unknown distribution D. If mϵ2 ≥ 2  then

Ps(sup
h∈H

|erD[h] − ers[h]| ≥ ϵ) ≤ 2Ps(cid:22)s(sup
h∈H

|er(cid:22)s[h] − ers[h]| ≥ ϵ/2)

The proof details of this lemma can be found in the Supplementary Material.
Theorem 2 ([20]). Let H be restricted to points in a ball of M dimensions of radius R about the
origin  then

{

}
R2
γ2   M + 1

f atH(γ) ≤ min

(3)

(4)

Proof. (of Theorem 1). We must bound the probabilities over different margins. We ﬁrst use Lem-
ma 2 to bound the probability of error in terms of the probability of the discrepancy between the
performance on two halves of a double sample. Then we combine this result with Lemma 1. We
must consider all possible patterns of ki’s for label ζi. The largest value of ki is m. Thus  for ﬁxed q 
we can bound the number of possibilities by mq. Hence  there are mq of applications of Lemma 1.
Let ci = {γ1  γ2 ···   γq} denote the i-th combination of margins varied in {1 ···   m}q. G denotes
a set of GCC models. The generalization error of G can be represented as erD[G] and ers[G] is 0 
where G ∈ G. The uniform convergence bound of the generalization error is

|erD[G] − ers[G]| ≥ ϵ)

Ps(sup
G∈G

Applying Lemma 2 

Ps(sup
G∈G

|erD[G]−ers[G]| ≥ ϵ) ≤ 2Ps(cid:22)s(sup
G∈G

|er(cid:22)s[G] − ers[G]| ≥ ϵ/2)

Let Jci = {s(cid:22)s : ∃ a GCC model G with q labels and with margins ci : ki = f at(γi/8)  ers[G] =
0  er(cid:22)s[G] ≥ ϵ/2}. Clearly 

( mq∪

)

Jci

i=1

|er(cid:22)s[G] − ers[G]| ≥ ϵ/2) ≤ P mq

Ps(cid:22)s(sup
G∈G

5

As ki still satisﬁes ki = f at(γi/8)  Lemma 1 can still be applied to each case of P mq
δk = δ/mq. Applying Lemma 1 (replacing δ by δk/2)  we get:

where ϵ(m  k  δk/2) ≥ 2/m(Q log(32m) + log 2×2q
bound  it sufﬁces to show that P mq
Lemma 2 

P mq

(Jci) < δk/2

∪
i=1 Jci) ≤∑

∑
) and Q =
). By the union
(Jci) < δk/2 × mq = δ/2. Applying
mq
i=1 P mq
|erD[G] − ers[G]| ≥ ϵ) ≤ 2Ps(cid:22)s(sup
|er(cid:22)s[G] − ers[G]| ≥ ϵ/2)
)
( mq∪
G∈G

q
i=1 ki log( 4em
ki

(Jci). Let

Ps(sup
G∈G

mq

δk

(

≤ 2P mq

Jci

< δ

Thus  Ps(supG∈G |erD[G] − ers[G]| ≤ ϵ) ≥ 1 − δ. Let R be the radius of a ball containing the
support of the distribution. Applying Theorem 2  we get ki = f at(γi/8) ≤ 65R2/(γi)2. Note
that we have replaced the constant 82 = 64 by 65 in order to ensure the continuity from the right
required for the application of Lemma 1. We have upperbounded log(8em/ki) by log(8em). Thus 

i=1

)

Q log(32m) + log

2(2m)q

δ

′

Q

log(8em) log(32m) + log

2(2m)q

δ

m

)

(

erD[G] ≤ 2/m
≤ 130R2

(

∑

where Q

′

=

q
i=1

1

(γi)2 .

Given the training data size and the number of labels  Theorem 1 reveals one important factor in re-
ducing the generalization error bound for the GCC model: the minimization of the sum of reciprocal
of square of the margin over the labels. Thus  we obtain the following Corollary:
Corollary 2 (Globally Optimal Classiﬁer Chain). Suppose a random m multi-labeled sample with
q labels can be correctly classiﬁed using a GCC model  this GCC model is the globally optimal
′ in Theorem 1 is achieved over this classiﬁer
classiﬁer chain if and only if the minimization of Q
chain.

Given the number of labels q  there are q! different label orders. It is very expensive to ﬁnd the
′  by searching over all of the label orders. Next  we
globally optimal CC  which can minimize Q
discuss two simple algorithms.

4 Optimal classiﬁer chain algorithm

In this section  we propose two simple algorithms for ﬁnding the optimal CC based on our result
in Section 3. To clearly state the algorithms  we redeﬁne the margins with label order information.
Given label set M = {λ1  λ2 ···   λq}  suppose a GCC model contains q classiﬁers. Let oi(1 ≤
∑
oi ≤ q) denote the order of λi in the GCC model  γoi
represents the margin for label λi  with
previous oi − 1 labels as the augmented input. If oi = 1  then γ1
i represents the margin for label λi 
without augmented input. Then Q
i )2 .
1
(γoi

′ is redeﬁned as Q
′

q
i=1

=

i

4.1 Dynamic programming algorithm

]
∑
To simplify the search algorithm mentioned before  we propose the CC-DP algorithm to ﬁnd the
globally optimal CC. Note that Q
  we
i )2 = 1
1
(γoi
′ over a subset of M with the length of 1  2 ···   q.
explore the idea of DP to iteratively optimize Q
′ over M. Assume i ∈ {1 ···   q}. Let V (i  η) be the optimal
Finally  we can obtain the optimal Q
′ over a subset of M with the length of η(1 ≤ η ≤ q)  where the label order is ending by label λi.
{
Q
Suppose M η
i represent the corresponding label set for V (i  η). When η = q  V (i  q) be the optimal
′ over M  where the label order is ending by label λi. The DP equation is written as:
Q

q )2 + ··· +

k+1 )2 +

∑

}

1
oj
j )2

k
j=1

q
i=1

[

ok+1

=

(γ

(γ

(γ

oq

′

1

V (i  η + 1) = min

j̸=i λi̸∈M (cid:17)

j

1
(γη+1

i

)2

+ V (j  η)

(5)

6

i

i )2 and M 1

i = M 1
j

is the margin for label λi  with M η

i = {λi}. Then  the optimal Q

where γη+1
j as the augmented input. The initial condition of
′ over M can be obtained by solving
DP is: V (i  1) = 1
mini∈{1 ···  q} V (i  q). Assume the training of linear SVM takes O(nd). The CC-DP algorithm is
(γ1
shown as the following bottom-up procedure: from the bottom  we ﬁrst compute V (i  1) = 1
i )2  
(γ1
which takes O(nd). Then we compute V (i  2) = minj̸=i λi̸∈M 1
i )2 + V (j  1)}  which requires
at most O(qnd)  and set M 2
∪ {λi}. Similarly  it takes at most O(q2nd) time complexity to
calculate V (i  q). Last  we iteratively solve this DP Equation  and use mini∈{1 ···  q} V (i  q) to get
the optimal solution  which requires at most O(q3nd) time complexity.
Theorem 3 (Correctness of CC-DP). Q
can ﬁnd the globally optimal CC.

′ can be minimized by CC-DP  which means this Algorithm

{ 1

(γ2

j

The proof can be referred to in the Supplementary Materials.

4.2 Greedy algorithm

We propose a CC-Greedy algorithm to ﬁnd a locally optimal CC to speed up the CC-DP algorithm.
To save time  we construct only one classiﬁer chain with the locally optimal label order. Based on
the training instances  we select the label from {λ1  λ2 ···   λq} as the ﬁrst label  if the maximum
margin can be achieved over this label  without augmented input. The ﬁrst label is denoted by ζ1.
Then we select the label from the remainder as the second label  if the maximum margin can be
achieved over this label with ζ1 as the augmented input. We continue in this way until the last label
is selected. Finally  this algorithm will converge to the locally optimal CC. We present the details
of the CC-Greedy algorithm in the Supplementary Materials  where the time complexity of this
algorithm is O(q2nd).

5 Experiment

In this section  we perform experimental studies on a number of benchmark data sets from different
domains to evaluate the performance of our proposed algorithms for multi-label classiﬁcation. All
the methods are implemented in Matlab and all experiments are conducted on a workstation with a
3.2GHZ Intel CPU and 4GB main memory running 64-bit Windows platform.

5.1 Data sets and baselines

We conduct experiments on eight real-world data sets with various domains from three websites.345
Following the experimental settings in [5] and [7]  we preprocess the LLog  yahoo art  eurlex sm
and eurlex ed data sets. Their statistics are presented in the Supplementary Materials. We compare
our algorithms with some baseline methods: BR  CC  ECC  CCA [14] and MMOC [7]. To perform
a fair comparison  we use the same linear classiﬁcation/regression package LIBLINEAR [21] with
L2-regularized square hinge loss (primal) to train the classiﬁers for all the methods. ECC is averaged
over several CC predictions with random order and the ensemble size in ECC is set to 10 according
to [5  6]. In our experiment  the running time of PCC and EPCC [5] on most data sets  like slashdot
and yahoo art  takes more than one week. From the results in [5]  ECC is comparable with EPCC
and outperforms PCC  so we do not consider PCC and EPCC here. CCA and MMOC are two
state-of-the-art encoding-decoding [13] methods. We cannot get the results of CCA and MMOC on
yahoo art 10  eurlex sm 10 and eurlex ed 10 data sets in one week. Following [22]  we consider
the Example-F1  Macro-F1 and Micro-F1 measures to evaluate the prediction performance of all
methods. We perform 5-fold cross-validation on each data set and report the mean and standard
error of each evaluation measurement. The running time complexity comparison is reported in the
Supplementary Materials.

3http://mulan.sourceforge.net
4http://meka.sourceforge.net/#datasets
5http://cse.seu.edu.cn/people/zhangml/Resources.htm#data

7

Table 1: Results of Example-F1 on the various data sets (mean ± standard deviation). The best
results are in bold. Numbers in square brackets indicate the rank.

Data set

BR

CC

ECC

CCA

0.6109 (cid:6) 0.024[4]
0.5850(cid:6) 0.033[7]
0.6076 (cid:6) 0.019[6]
0.6096(cid:6) 0.018[5]
yeast
0.5947(cid:6) 0.015[4] 0.5947 (cid:6) 0.009[4]
0.5247 (cid:6) 0.025[7]
0.5991(cid:6) 0.021[1]
image
0.5260 (cid:6) 0.021[3]
0.5246(cid:6) 0.028[4] 0.5123(cid:6) 0.027[5]
0.4898 (cid:6) 0.024[6]
slashdot
0.4848(cid:6) 0.014[4]
0.4792 (cid:6) 0.017[7] 0.4799(cid:6) 0.011[6]
0.4812 (cid:6) 0.024[5]
enron
0.3138 (cid:6) 0.022[6]
0.3219(cid:6) 0.028[4] 0.3223(cid:6) 0.030[3]
0.2978 (cid:6) 0.026[7]
LLog 10
0.5013(cid:6) 0.022[4]
0.5070(cid:6) 0.020[3]
yahoo art 10 0.4840 (cid:6) 0.023[5]
-
eurlex sm 10 0.8594 (cid:6) 0.003[5]
0.8609(cid:6) 0.004[1] 0.8606(cid:6) 0.003[3]
-
0.7183(cid:6) 0.013[2]
0.7176(cid:6) 0.012[4]
eurlex ed 10 0.7170 (cid:6) 0.012[5]
-
3.63
3.88
Average Rank

5.88

4.60

CC-DP

MMOC

CC-Greedy
0.6135(cid:6) 0.015[2]
0.6132 (cid:6) 0.021 [3] 0.6144(cid:6) 0.021[1]
0.5939(cid:6) 0.021[6]
0.5960 (cid:6) 0.012[3]
0.5976(cid:6) 0.015[2]
0.5268(cid:6) 0.022[1]
0.5266(cid:6) 0.022[2]
0.4895 (cid:6) 0.022[7]
0.4894 (cid:6) 0.016[2] 0.4880(cid:6) 0.015[3]
0.4940 (cid:6) 0.016[1]
0.3153 (cid:6) 0.026[5]
0.3269(cid:6) 0.023[2]
0.3298(cid:6) 0.025[1]
0.5131(cid:6) 0.015[2]
0.5135(cid:6) 0.020[1]
-
0.8600(cid:6) 0.004[4]
0.8609(cid:6) 0.004[1]
-
0.7190(cid:6) 0.013[1]
0.7183(cid:6) 0.013[2]
-
1.50
2.63

3.80

5.2 Prediction performance

Example-F1 results for our method and baseline approaches in respect of the different data sets
are reported in Table 1. Other measure results are reported in the Supplementary Materials. From
the results  we can see that: 1) BR is much inferior to other methods in terms of Example-F1.
Our experiment provides empirical evidence that the label correlations exist in many real word
data sets and because BR ignores the information about the correlations between the labels  BR
achieves poor performance on most data sets. 2) CC improves the performance of BR  however 
it underperforms ECC. This result veriﬁes the answer to our ﬁrst question stated in Section 1: the
label order does affect the performance of CC; ECC  which averages over several CC predictions
with random order  improves the performance of CC. 3) CC-DP and CC-Greedy outperforms CCA
and MMOC. This studies verify that optimal CC achieve competitive results compared with state-
of-the-art encoding-decoding approaches. 4) Our proposed CC-DP and CC-Greedy algorithms are
successful on most data sets. This empirical result also veriﬁes the answers to the last two questions
stated in Section 1: the globally optimal CC exists and CC-DP can ﬁnd the globally optimal CC
which achieves the best prediction performance; the CC-Greedy algorithm achieves comparable
prediction performance with CC-DP  while it requires lower time complexity than CC-DP. In the
experiment  our proposed algorithms are much faster than CCA and MMOC in terms of both training
and testing time  and achieve the same testing time with CC. Through the training time for our
algorithms is slower than BR  CC and ECC. Our extensive empirical studies show that our algorithms
achieve superior performance than those baselines.

6 Conclusion

To improve the performance of multi-label classiﬁcation  a plethora of models have been developed
to capture label correlations. Amongst them  classiﬁer chain is one of the most popular approaches
due to its simplicity and good prediction performance. Instead of proposing a new learning model 
we discuss three important questions in this work regarding the optimal classiﬁer chain stated in
Section 1. To answer these questions  we ﬁrst propose a generalized CC model. We then provide
a theoretical analysis of the generalization error for the proposed generalized model. Based on our
results  we obtain the answer to the second question: the globally optimal CC exists only if the mini-
mization of the upper bound is achieved over this CC. It is very expensive to search over q! different
label orders to ﬁnd the globally optimal CC. Thus  we propose the CC-DP algorithm to simplify
the search algorithm  which requires O(q3nd) complexity. To speed up the CC-DP algorithm  we
propose a CC-Greedy algorithm to ﬁnd a locally optimal CC  where the time complexity of the CC-
Greedy algorithm is O(q2nd). Comprehensive experiments on eight real-world multi-label data sets
from different domains verify our theoretical studies and the effectiveness of proposed algorithms.

Acknowledgments

This research was supported by the Australian Research Council Future Fellowship FT130100746.

References
[1] Robert E. Schapire and Yoram Singer. BoosTexter: A Boosting-based System for Text Categorization.

Machine Learning  39(2-3):135–168  2000.

8

[2] Zafer Barutc¸uoglu and Robert E. Schapire and Olga G. Troyanskaya. Hierarchical multi-label prediction

of gene function. Bioinformatics  22(7):22–7  2006.

[3] Matthew R. Boutell and Jiebo Luo and Xipeng Shen and Christopher M. Brown. Learning Multi-Label

Scene Classiﬁcation. Pattern Recognition  37(9):1757–1771  2004.

[4] Grigorios Tsoumakas and Ioannis Katakis and Ioannis P. Vlahavas. Mining Multi-label Data. In Data

Mining and Knowledge Discovery Handbook  pages 667–685  2010. Springer US.

[5] Krzysztof Dembczynski and Weiwei Cheng and Eyke H¨ullermeier. Bayes Optimal Multilabel Classiﬁ-
cation via Probabilistic Classiﬁer Chains. Proceedings of the 27th International Conference on Machine
Learning  pages 279–286  Haifa  Israel  2010. Omnipress.

[6] Jesse Read and Bernhard Pfahringer and Geoffrey Holmes and Eibe Frank. Classiﬁer Chains for Multi-
label Classiﬁcation. In Proceedings of the European Conference on Machine Learning and Knowledge
Discovery in Databases: Part II  pages 254–269  Berlin  Heidelberg  2009. Springer-Verlag.

[7] Yi Zhang and Jeff G. Schneider. Maximum Margin Output Coding. Proceedings of the 29th International

Conference on Machine Learning  pages 1575–1582  New York  NY  2012. Omnipress.

[8] Yuhong Guo and Suicheng Gu. Multi-Label Classiﬁcation Using Conditional Dependency Networks.
Proceedings of the Twenty-Second International Joint Conference on Artiﬁcial Intelligence  pages 1300–
1305  Barcelona  Catalonia  Spain  2011. AAAI Press.

[9] Sheng-Jun Huang and Zhi-Hua Zhou. Multi-Label Learning by Exploiting Label Correlations Locally.
Proceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence  Toronto  Ontario  Canada 
2012. AAAI Press.

[10] Feng Kang and Rong Jin and Rahul Sukthankar. Correlated Label Propagation with Application to Multi-
label Learning. 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition 
pages 1719–1726  New York  NY  2006. IEEE Computer Society.

[11] Weiwei Liu and Ivor W. Tsang. Large Margin Metric Learning for Multi-Label Prediction. Proceedings
of the Twenty-Ninth Conference on Artiﬁcial Intelligence  pages 2800–2806  Texas  USA  2015. AAAI
Press.

[12] Mingkui Tan and Qinfeng Shi and Anton van den Hengel and Chunhua Shen and Junbin Gao and Fuyuan
Hu and Zhen Zhang. Learning Graph Structure for Multi-Label Image Classiﬁcation via Clique Genera-
tion. The IEEE Conference on Computer Vision and Pattern Recognition  2015.

[13] Daniel Hsu and Sham Kakade and John Langford and Tong Zhang. Multi-Label Prediction via Com-
pressed Sensing. Advances in Neural Information Processing Systems  pages 772–780  2009. Curran
Associates  Inc.

[14] Yi Zhang and Jeff G. Schneider. Multi-Label Output Codes using Canonical Correlation Analysis. Pro-
ceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics  pages 873–
882  Fort Lauderdale  USA  2011. JMLR.org.

[15] Farbound Tai and Hsuan-Tien Lin. Multilabel Classiﬁcation with Principal Label Space Transformation.

Neural Computation  24(9):2508–2542  2012.

[16] Min-Ling Zhang and Kun Zhang. Multi-label learning by exploiting label dependency. Proceedings
of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages
999–1008  QWashington  DC  USA  2010. ACM.

[17] John Shawe-Taylor and Peter L. Bartlett and Robert C. Williamson and Martin Anthony. Structural Risk
Minimization Over Data-Dependent Hierarchies. IEEE Transactions on Information Theory  44(5):1926–
1940  1998.

[18] Kristin P. Bennett and Nello Cristianini and John Shawe-Taylor and Donghui Wu. Enlarging the Margins

in Perceptron Decision Trees. Machine Learning  41(3):295–313  2000.

[19] Michael J. Kearns and Robert E. Schapire. Efﬁcient Distribution-free Learning of Probabilistic Concept-
s. Proceedings of the 31st Symposium on the Foundations of Computer Science  pages 382–391  Los
Alamitos  CA  1990. IEEE Computer Society Press.

[20] Peter L. Bartlett and John Shawe-Taylor. Generalization Performance of Support Vector Machines and
Other Pattern Classiﬁers. Advances in Kernel Methods - Support Vector Learning  pages 43–54  Cam-
bridge  MA  USA  1998. MIT Press.

[21] Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang and Chih-Jen Lin. LIBLIN-
EAR: A Library for Large Linear Classiﬁcation. Journal of Machine Learning Research  9:1871–1874 
2008.

[22] Qi Mao and Ivor Wai-Hung Tsang and Shenghua Gao. Objective-Guided Image Annotation.

Transactions on Image Processing  22(4):1585–1597  2013.

IEEE

9

,Weiwei Liu
Ivor Tsang