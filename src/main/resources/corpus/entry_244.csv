2019,A Unifying Framework for Spectrum-Preserving Graph Sparsification and Coarsening,How might one ``reduce'' a graph? 
That is  generate a smaller graph that preserves the global structure at the expense of discarding local details?  
There has been extensive work on both graph sparsification (removing edges) and graph coarsening (merging nodes  often by edge contraction); however  these operations are currently treated separately.  
Interestingly  for a planar graph  edge deletion corresponds to edge contraction in its planar dual (and more generally  for a graphical matroid and its dual).  
Moreover  with respect to the dynamics induced by the graph Laplacian (e.g.  diffusion)  deletion and contraction are physical manifestations of two reciprocal limits: edge weights of $0$ and $\infty$  respectively.  
In this work  we provide a unifying framework that captures both of these operations  allowing one to simultaneously sparsify and coarsen a graph while preserving its large-scale structure.  
The limit of infinite edge weight is rarely considered  as many classical notions of graph similarity diverge.  However  its algebraic  geometric  and physical interpretations are reflected in the Laplacian pseudoinverse $\mat{L}^\dagger$  which remains finite in this limit.  
Motivated by this insight  we provide a probabilistic algorithm that reduces graphs while preserving $\mat{L}^\dagger$  using an unbiased procedure that minimizes its variance. 
We compare our algorithm with several existing sparsification and coarsening algorithms using real-world datasets  and demonstrate that it more accurately preserves the large-scale structure.,A Unifying Framework for Spectrum-Preserving

Graph Sparsiﬁcation and Coarsening

Gecia Bravo-Hermsdorff*

Princeton Neuroscience Institute

Princeton University

Princeton  NJ  08544  USA
geciah@princeton.edu

Lee M. Gunderson*

Department of Astrophysical Sciences

Princeton University

Princeton  NJ  08544  USA

leeg@princeton.edu

Abstract

How might one “reduce” a graph? That is  generate a smaller graph that preserves
the global structure at the expense of discarding local details? There has been
extensive work on both graph sparsiﬁcation (removing edges) and graph coarsening
(merging nodes  often by edge contraction); however  these operations are currently
treated separately. Interestingly  for a planar graph  edge deletion corresponds to
edge contraction in its planar dual (and more generally  for a graphical matroid
and its dual). Moreover  with respect to the dynamics induced by the graph
Laplacian (e.g.  diffusion)  deletion and contraction are physical manifestations
of two reciprocal limits: edge weights of 0 and 1  respectively. In this work  we
provide a unifying framework that captures both of these operations  allowing one
to simultaneously sparsify and coarsen a graph while preserving its large-scale
structure. The limit of inﬁnite edge weight is rarely considered  as many classical
notions of graph similarity diverge. However  its algebraic  geometric  and physical
interpretations are reﬂected in the Laplacian pseudoinverse L†  which remains ﬁnite
in this limit. Motivated by this insight  we provide a probabilistic algorithm that
reduces graphs while preserving L†  using an unbiased procedure that minimizes
its variance. We compare our algorithm with several existing sparsiﬁcation and
coarsening algorithms using real-world datasets  and demonstrate that it more
accurately preserves the large-scale structure.

1 Motivation

Many complex structures and phenomena are naturally described as graphs (eg 1 brains  social
networks  the internet  etc). Indeed  graph-structured data are becoming increasingly relevant to
the ﬁeld of machine learning [2  3  4]. These graphs are frequently massive  easily surpassing our
working memory  and often the computer’s relevant cache [5]. It is therefore essential to obtain
smaller approximate graphs to allow for more efﬁcient computation.
Graphs are deﬁned by a set of nodes V and a set of edges E ✓ V ⇥ V between them  and are often
represented as an adjacency matrix A with size |V |⇥| V | and density /| E|. Reducing either of
these quantities is advantageous: graph “coarsening” focuses on the former  aggregating nodes while
respecting the overall structure  and graph “sparsiﬁcation” on the latter  preferentially retaining the
important edges.

⇤Both authors contributed equally to this work.
1The authors agree with the sentiment of the footnote on page xv of [1]  viz  omitting superﬂuous full stops

to obtain a more efﬁcient compression of  eg: videlicet  exempli gratia  etc.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Spectral graph sparsiﬁcation has revolutionized the ﬁeld of numerical linear algebra and is used  eg  in
algorithms for solving linear systems with symmetric diagonally dominant matrices in nearly-linear
time [6  7] (in contrast to the fastest known algorithm for solving general linear systems  taking
O(n!)-time  where ! ⇡ 2.373 is the matrix multiplication exponent [8]).
Graph coarsening appears in many computer science and machine learning applications  eg: as
primitives for graph partitioning [9] and visualization algorithms2 [10]; as layers in graph convolution
networks [3  11]; for dimensionality reduction and hierarchical representation of graph-structured
data [12  13]; and to speed up regularized least square problems on graphs [14]  which arise in a
variety of problems such as ranking [15] and distributed synchronization of clocks [16].
A variety of algorithms  with different objectives  have been proposed for both sparsiﬁcation and
coarsening. However  a frequently recurring theme is to consider the graph Laplacian L = D  A 
where D is the diagonal matrix of node degrees. Indeed  it appears in a wide range of applications 
eg: its spectral properties can be leveraged for graph clustering [17]; it can be used to efﬁciently solve
min-cut/max-ﬂow problems [18]; and for undirected  positively weighted graphs (the focus of this
paper)  it induces a natural quadratic form  which can be used  eg  to smoothly interpolate functions
over the nodes [19].
Work on spectral graph sparsiﬁcation focuses on preserving the Laplacian quadratic form ~x>L~x  a
popular measure of spectral similarity suggested by Spielman & Teng [6]. A key result in this ﬁeld is
that any dense graph can be sparsiﬁed to O(|V | log |V |) edges in nearly linear time using a simple
probabilistic algorithm [20]: start with an empty graph  include edges from the original graph with
probability proportional to their effective resistance  and appropriately reweight those edges so as to
preserve ~x>L~xwithin a reasonable factor.
In contrast to the ﬁrm theoretical footing of spectral sparsiﬁcation  work on graph coarsening
has not reached a similar maturity; while a variety of spectral coarsening schemes have been
recently proposed  algorithms frequently rely on heuristics  and there is arguably no consensus. Eg:
Jin & Jaja [21] use k eigenvectors of the Laplacian as feature vectors to perform k-means clustering
of the nodes; Purohit et al. [22] aim to minimize the change in the largest eigenvalue of the adjacency
matrix; and Loukas & Vandergheynst [23] focuses on a “restricted” Laplacian quadratic form.
Although recent work has combined sparsiﬁcation and coarsening [24]  they used separate algorithmic
primitives  essentially analyzing the serial composition of the above algorithms. The primary contri-
bution of this work is to provide a unifying probabilistic framework that allows one to simultaneously
sparsify and coarsen a graph while preserving its global structure by using a single cost function that
preserves the Laplacian pseudoinverse L†.
Corollary contributions include: 1) Identifying the limit of inﬁnite edge weight with edge contraction 
highlighting how its algebraic  geometric  and physical interpretations are reﬂected in L†  which
remains ﬁnite in this limit (Section 2); 2) Offering a way to quantitatively compare the effects
of edge deletion and edge contraction (Section 2 and 3); 3) Providing a probabilistic algorithm
that reduces graphs while preserving L†  using an unbiased procedure that minimizes its variance
(Sections 3 and 4); 4) Proposing a more sensitive measure of spectral similarity of graphs  inspired
by the Poincaré half-plane model of hyperbolic space (Section 5.3); and 5) Comparing our algorithm
with several existing sparsiﬁcation and coarsening algorithms using synthetic and real-world datasets 
demonstrating that it more accurately preserves the large-scale structure (Section 5).

2 Why the Laplacian pseudoinverse

Many computations over graphs involve solving L~x= ~b for ~x[25]. Thus  the algebraically relevant
operator is arguably the Laplacian pseudoinverse L†. In fact  its connection with random walks
has been used to derive useful measures of distances on graphs  such as the well-known effective
resistance [26]  and the recently proposed resistance perturbation distance [27]. Moreover  taking
the pseudoinverse of L leaves its eigenvectors unchanged  but inverts the nontrivial eigenvalues.
Thus  as the largest eigenpairs of L† are associated with global structure  preserving its action will
preferentially maintain the overall “shape” of the graph (see Appendix Section G for details). For
instance  the Fielder vector [17] (associated with the “algebraic connectivity” of a graph) will be

2For animated examples using our graph reduction algorithm  see the following link:

youtube.com/playlist?list=PLmﬁQcz2q6d3sZutLri4ZAIDLqM_4K1p-.

2

preferentially preserved. We now discuss in further detail why L† is well-suited for both graph
sparsiﬁcation and coarsening.
Attention is often restricted to undirected  positively weighted graphs [28]. These graphs have
many convenient properties  eg  their Laplacians are positive semideﬁnite (~x>L~x 0) and have a
well-understood kernel and cokernel (L~1 = ~1>L = ~0). The edge weights are deﬁned as a mapping
W: E ! R>0. When the weights represent connection strength  it is generally understood that
we ! 0 is equivalent to removing edge e. However  the closure of the positive reals has a reciprocal
limit  namely we ! +1.
This limit is rarely considered  as many classical notions of graph similarity diverge. This includes
the standard notion of spectral similarity  where eG is a -spectral approximation of G if it preserves
the Laplacian quadratic form ~x>LG~xto within a factor of  for all vectors ~x2 R|VG| [6]. Clearly  this
limit yields a graph that does not approximate the original for any choice of : any ~xwith different
values for the two nodes joined by the edge with inﬁnite weight now yields an inﬁnite quadratic form.
This suggests considering only vectors that have the same value for these two nodes  essentially
contracting them into a single “supernode”. Algebraically  this interpretation is reﬂected in L†  which
remains ﬁnite in this limit: the pair of rows (and columns) corresponding to the contracted nodes
become identical (see Appendix Section C).
Physically  consider the behavior of the heat equation @t~x+ L~x= ~0: as we ! +1  the values on
the two nodes immediately equilibrate between themselves  and remain tethered for the rest of the
evolution.3 Geometrically  the reciprocal limits of we ! 0 and we ! +1 have dual interpretations:
consider a planar graph and its planar dual; edge deletion in one graph corresponds to contraction in
the other  and vice versa. This naturally extends to nonplanar graphs via their graphical matroids and
their duals [29].
Finally  while the Laplacian operator is frequently considered in the graph sparsiﬁcation and coarsen-
ing literature  its pseudoinverse also has many important applications in the ﬁeld of machine learning
[30]  eg: online learning over graphs [31]; similarity prediction of network data [32]; determining
important nodes [33]; providing a measure of network robustness to multiple failures [34]; extending
principal component analysis to graphs [35]; and collaborative recommendation systems [36]. Hence 
graph reduction algorithms that preserve L† would be useful to the machine learning community.

3 Our graph reduction framework

We now describe our framework for constructing probabilistic algorithms that generate a reduced

graph eG from an initial graph G  motivated by the following desiderata: 1) Reduce the number of

edges/nodes (Section 3.1); 2) Preserve L† in expectation (Section 3.2); and 3) Minimize the change
in L† (Section 3.3).
We ﬁrst deﬁne these goals more formally. Then  in Section 3.4  we combine these requirements
to deﬁne our cost function and derive the optimal probabilistic action (ie  deletion  contraction  or
reweight) to perform to an edge.

3.1 Reducing edges and nodes

Depending on the application  it might be more important to reduce the number of nodes (eg 
coarsening a sparse network) or the number of edges (eg  sparsifying a dense network). Let r be
the number of prioritized items reduced during a particular iteration. When those items are nodes 
then r = 0 for a deletion  and r = 1 for a contraction. When those items are edges  then r = 1 for a
deletion  however r > 1 for a contraction is possible: if the contracted edge forms a triangle in the
original graph  then the other two edges will become parallel in the reduced graph (see Figure SI 3
in Appendix Section C). With respect to the Laplacian  this is equivalent to a single edge with
weight given by the sum of these now parallel edges. Thus  when edge reduction is prioritized  a
contraction will have r = 1 + ⌧e  where ⌧e is the number of triangles in the original graph G in which
the contracted edge e participates.

3In the spirit of another common analogy (edge weights as conductances of a network of resistors)  breaking

a resistor is equivalent to deleting that edge  while contraction amounts to completely soldering over it.

3

Note that  even when node reduction is prioritized  the number of edges will also necessarily decrease.
Conversely  when edge reduction is prioritized  contraction of an edge is also possible  thereby
reducing the number of nodes as well. For the case of simultaneously sparsifying and coarsening a
graph  we choose to prioritize edge reduction  although nodes could also be a sensible choice.

3.2 Preserving the Laplacian pseudoinverse
Consider perturbing the weight of a single edge e = (v1  v2) by w. The change in the Laplacian is
(1)

L

where L
signed incidence (column) vector associated with edge e  with entries

eG and LG are the perturbed and original Laplacians  respectively  and ~be is the (arbitrarily)

eG  LG = w~be

~b>e  

The change in L† is given by the Woodbury matrix identity4 [39]:

Note that this change can be expressed as a matrix that depends only on the choice of edge e 
multiplied by a scalar term that depends (nonlinearly) on the change to its weight:

(be)i =(+1

1
0

i = v1
i = v2
otherwise.

L†

G = 

eG  L†
L† = f⇣ w

we

w

1 +w~b>e L†

G

~be

L†

G

~b>e L†
~be
G.

  we⌦e⌘
{z
}

⇥ Me

 

|{z}

nonlinear scalar

constant matrix

|

f = 

 

we⌦e

w
we
1 + w
we
~b>e L†
~be
G 
~be.

G

(2)

(3)

(4)

(5)

(10)

where

(6)
(7)
Hence  if the probabilistic reweight of this edge is chosen such that E[f ] = 0  then we have
E[L†

G  as desired. Importantly  f remains ﬁnite in the following relevant limits:

Me = weL†
⌦e = ~b>e L†

G

deletion:
contraction:

w

we ! 1 
we ! +1 

w

f ! (1  we⌦e)1
f !  (we⌦e)1 .

(8)

eG] = L†

Note that f diverges when considering deletion of an edge with we⌦e = 1 (ie  an edge cut). Indeed 
such an action would disconnect the graph and invalidate the use of equation 3 (see footnote 4).
However  this possibility is precluded by the requirement that E[f ] = 0.

3.3 Minimizing the error
Minimizing the magnitude of L† requires a choice of matrix norm  which we take to be the sum of
the squares of its entries (ie  the square of the Frobenius norm). Our motivation is twofold. First  the
algebraically convenient fact that the Frobenius norm of a rank one matrix has a simple form  viz 

(9)
Second  the square of this norm behaves as a variance; to the extent that the Meassociated to different
edges can be treated as (entrywise) uncorrelated one can decompose multiple perturbations as follows:

me ⌘ kMekF = we~b>e L†

GL†

~be.

G

EX L†

2

F ⇡X EhL†2
Fi 

4This expression is only ofﬁcially applicable when the initial and ﬁnal matrices are full-rank; additional care
must be taken when they are not. However  for the case of changing the edge weights of a graph Laplacian  the
original formula remains unchanged [37  38] (so long as the graph remains connected)  provided one uses the
deﬁnitions in Section 3.5 (see also Appendix Sections C and F).

4

which allows the single-edge results from Section 3.4 to be iteratively applied to our reduction
algorithm  which has multiple reductions (Section 4). In Appendix Section A  we empirically validate
this approximation using synthetic and real-world networks  showing that this approximation is either
nearly exact or a conservative estimate.
For subtleties associated with edge contraction (see Appendix Section F  in particular equation 39).

3.4 A cost function for spectral graph reduction

Combining the discussed desiderata  we choose to minimize the following cost function:

(11)

Fi  2E[r]  

C = EhL†2
E⇥L†⇤ = 0  

subject to

(12)
where the parameter  controls the tradeoff between number of prioritized items reduced r and error
incurred in L†. This cost function naturally arises when minimizing the expected squared error for a
given expected amount of reduction (or equivalently maximizing the expected number of reductions
for a given expected squared error).
We desire to minimize this cost function over all possible reduced graphs. As  when reducing
multiple edges  E[r] is additive and the expected squared error is empirically additive  we are able
to decompose this objective into a sequence of minimizations applied to individual edges. Thus 
minimization of this cost function for each edge acted upon can be seen as a probabilistic greedy
algorithm for minimizing the cost function for the ﬁnal reduced graph.
Here  we describe the analytic solution for the optimal action (ie  probabilistically choosing to delete 
contract  or reweight) to be applied to a single edge. We provide the solution in Figure 1  and a
detailed derivation in Appendix Section B.
For a given edge e  the values of me  we⌦e  and ⌧e are ﬁxed  and minimizing the cost function (11)
(given (12)) results in a piecewise solution with three regimes  depending on the value of : 1) When
< 1(me  we⌦e ⌧ e) = min(1d  1c)   is small compared with the error that would be incurred
by acting on this edge  thus it should not be changed; 2) When > 2(me  we⌦e ⌧ e)   is large for
this edge  and the optimal solution is to probabilistically delete or contract this edge (pd + pc = 1;
no reweight is required); and 3) In the intermediate case (1 << 2)  there are two possibilities 
depending on the edge and the choice of prioritized items: if 1d < 1c  the edge is either deleted or
reweighted  and if 1c < 1d  the edge is either contracted or reweighted.

pd = 0 
c pd = 1 

=⇣1  pd

1wee⌘1

d pd = 0 
1

<

pd = 1  we⌦e 

pc = 0  w
we

(1wee)  

pc = 1 

=  pc

wee1+e

2

<

<
1


pc = we⌦e

1

<
d
1


pc = 0 

 > 2

 < 1

 1

w
we

w
we

= 0

wee

me

 

c
1


me

pc = 0  w
we

= 0

prioritizing edges

prioritizing nodes

 < 1

pd = 0 
c pd = 1 

2

<

<
1


1

<
d
1


me

(1wee)  

=⇣1  pd

1wee⌘1

w
we

pc = 0 

 1

pc = 1 

me

wee1+e

d pd = 0 
1

<

c
1


w
we

=  pc

wee

1d

1c

2

 

me

1wee

me

wee

11+e

1

me

wee

me

wee(1wee)

1

1+1+e

me

wee(1wee)

 > 2

pd = 1  we⌦e 

pc = we⌦e

prioritizing edges

prioritizing nodes

Figure 1: Left: Minimizing C for a single edge e. There are three regimes for the solution  depending on the
value of . When node reduction is prioritized  set ⌧e = 0. Right: Values of  dividing the three regimes.
Note that when edge reduction is prioritized  the number of triangles enters the expressions  and when node
reduction is prioritized  there is no deletion in the intermediate regime. However  for either choice  both deletion
and contraction can have ﬁnite probability  and the algorithm does not exclusively reduce one or the other. Thus 
when simultaneously sparsifying and coarsening a graph  the prioritized items may be chosen to be either edges
or nodes. We remark that the values of 1d  1c  and 2 might be of independent interest as measures of edge
importance for analyzing connections in real-world networks.

11+e

1wee

1

wee

wee

1d

1c

me

me

me

2

me

wee(1wee)

1

1+1+e

me

wee(1wee)

5

3.5 Node-weighted Laplacian

Moreover  one must be careful to choose the appropriate pseudoinverse of L

G  one must keep track of the number of original nodes that comprise these “supernodes” and
assign them proportional weights. The appropriate reduced Laplacian L
n B>WeB  where the W are the diagonal matrices of the node weights5 and the edge weights of
W 1

When nodes are merged  one often represents the connectivity of the resulting graph eG by a matrix
of smaller size. To properly compare the spectral properties of eG with those of the original graph
eG (of size |VeG|⇥| VeG|) is then
eG  respectively  and B is its signed incidence matrix with columns given by (2).
eG  which is given by

eG + J1
where ~wn 2 R|VG|>0 is the vector of node weights. Note that L†
eGL†
eG = I  J   the appropriate
eGL
eG = L
node-weighted projection matrix.
To compare the action of the original and reduced Laplacians on a vector ~x2 R|VG| over the nodes
eG to operate on the same space as LG. We thus deﬁne the
of the original graph  one must “lift” L
mapping from original to coarsened nodes as a |VeG|⇥| VG| matrix C   with entries

eG =L

 J  

~1>~wn

~1 ~w>n  

(13)

(14)

(15)

L†

1

J =

cij =n1
eG l = C>L
4 Our graph reduction algorithm

The appropriate lifted Laplacian is L
verse is L†

eG l = C>L†

eGW 1

0

node j in supernode i
otherwise.

n C . Likewise  the lifted Laplacian pseudoin-
n C (see Appendix Section C for a detailed rationale of these deﬁnitions).

eGW 1

Using this framework  we now describe our graph reduction algorithm. Similar to many graph
coarsening methods [41  42]  we obtain the reduced graph by acting on the initial graph (as opposed
to adding edges to an empty graph  as is frequently done in sparsiﬁcation [43  44]).
Care must be taken  however  as simultaneous deletions/contractions may result in undesirable
behavior. Eg  while any edge that is itself a cut-set will never be deleted (as we⌦e = 1)  a collection
of edges that together make a cut-set might all have ﬁnite deletion probability. Hence  if multiple
edges are simultaneously deleted  the graph could become disconnected. In addition  the single-edge
analysis could underestimate the change in L† associated with simultaneous contractions. Eg  consider
two highly-connected nodes that are each the center of a different community  and a third auxiliary
node that happens to be connected to both: contracting the auxiliary node into either of the other two
would be sensible  but performing both contractions would merge the two communities.
Algorithm 1 describes our graph reduction scheme. Its inputs are: G  the original graph; q  the
fraction of sampled edges to act upon per iteration; d  the minimum expected decrease in prioritized
items per edge acted upon; and StopCriterion  a user-deﬁned function. With these inputs  we
implicitly select . Let ? e be the minimum  such that E[r]  d for edge e. For each iteration  we
compute ? e for all sampled edges  and choose a  such that a fraction q of them have ? e < .
We then apply the corresponding probabilistic actions to these edges. The appropriate choice of
StopCriterion depends on the application. Eg  if one desires to bound the accuracy of an algorithm
that uses graph reduction as a primitive  limiting the Frobenius error in L† is a sensible choice (it is
trivial to keep a running total of the estimated error  see Appendix Section A). On the other hand  if
one would like the reduced graph to be no larger than a certain size  then one can simply continue
reducing until this point. While both criteria may also be implicitly implemented via an upper bound
on   the relationship is nontrivial and depends on the structure of the graph.
The aforementioned problems associated with simultaneous deletions/contractions can be eliminated
by taking a conservative approach: acting on only a single edge per iteration. However  this results
in an algorithm that does not scale favorably for large graphs. A more scalable solution involves

5Wn is often referred to as the “mass matrix” [40]. We note that the use of the random walk matrix D1L

can be seen as using the node degrees as a surrogate for the node weights.

6

2: Initialize eG0 G  t 0  stop False

Sample an independent edge set
for (edge e) in (sampled edges) do

Compute ⌦e  me (see equations (7) and (9))
Evaluate ?e  according to d (see Tables in Figure 1)

end for
Choose  such that a fraction q of the sampled edges (those with the lowest ?e) are acted upon
Probabilistically choose to reweight  delete  or contract these edges

Algorithm 1 ReduceGraph
1: Inputs: graph G  fraction of sampled edges to act upon q  minimum E[r] per edge acted upon d  and a

StopCriterion

3: while not (stop) do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end while

Perform reweights and deletions to eGt
Perform contractions to eGt
eGt+1 eGt  t t + 1
stop StopCriterion(eGt)
16: return reduced graph eGt

carefully sampling the candidate set of edges. In particular  we are able to signiﬁcantly ameliorate
these issues by sampling the candidate edges such that they do not have any nodes in common (ie 
the sampled edges form an independent edge set). Not only does this eliminate the possibility of
“accidental” contractions  but  empirically  it also suppresses the occurrence of graph disconnections
(the small fraction that become disconnected are restarted). At each iteration  our algorithm ﬁnds a
random maximal independent edge set in O(|V |) time using a simple greedy algorithm.6 In practice 
the size of such a set scales as O(|V |) (although it is easy to ﬁnd families for which this scaling does
not hold  eg  star graphs). Our algorithm then computes the ⌦e and me of these sampled edges  and
acts on the fraction q with the lowest ?e.
The main computational bottleneck of our algorithm is computing ⌦e and me (equation (9)). However 
we can draw on the work of [20]  which describes a method for efﬁciently computing "-approximate

values of ⌦e for all edges  requiring eO(|E| log |V |/✏2) time. With minimal changes  this procedure
can also be used to compute approximate values of me with similar efﬁciency (in Appendix Section F 
we discuss the details of how to efﬁciently compute approximations of me). As we must compute
these quantities for each iteration  we multiply the running time by the expected number of iterations 
O(|E|/qd|V |). Empirically  we ﬁnd that one is able to set q ⇠ 1/16 and d ⇠ 1/4 with minimal loss
in reduction quality (see Appendix Section E). Thus  we expect that our algorithm could have a
running time of eO(hki|E|)  where hki is the average degree. However  in the following results  we

have used a naive implementation: computing L† at the onset  and updating it using the Woodbury
matrix identity.

5 Experimental results

In this section  we empirically validate our framework and compare it with existing algorithms.
We consider two cases of our general framework  namely graph sparsiﬁcation (excluding regimes
involving edge contraction)  and graph coarsening (prioritizing reduction of nodes). In addition 
as graph reduction is often used in graph visualization  we generated videos of our algorithm
simultaneously sparsifying and coarsening several real-world datasets (see footnote 2 and Appendix
Section I).

5.1 Hyperbolic interlude

When comparing a graph G with its reduced approximation eG  it is natural to consider how relevant
eG l~xis aligned with LG~x  the fractional error

linear operators treat the same input vector. If the vector L
in the quadratic form ~x>L~xis a natural quantity to consider  as it corresponds to the relative change in
the magnitude of these vectors. However  it is not so clear how to compare output vectors that have

6Speciﬁcally  randomly permute the nodes  and sequentially pair them with a random available neighbor (if

there is one). The obtained set contains at least half as many edges as the maximum matching [45].

7

an angular difference. Here  we describe a natural extension of this notion of fractional error  which
draws intuition from the Poincaré half-plane model of hyperbolic geometry. In particular  we choose
the boundary of the half-plane to be perpendicular to ~xand compute the geodesic distance between
LG~xand L

eG l~x  viz 

(16)

(17)

d~x(L0  L1) def= arccosh 1 +(L0  L1)~x2

2~x>L0~x~x>L1~x !  
2~x2

2

where L0 and L1 are positive deﬁnite matrices (for now).
We deﬁne the hyperbolic distance between these matrices as

dh(L0  L1) def= sup
~x

d~x(L0  L1) .

This dimensionless quantity inherits the following standard desirable features of a dis-
tance: symmetry and non-negativity  dh(L0  L1) = dh(L1  L0)  0; identity of indiscernibles 
dh(L0  L1) = 0 () L0 = L1; and subadditivity  dh(L0  L2)  dh(L0  L1) + dh(L1  L2). In ad-
dition  we note that dh(cL0  cL1) = dh(L0  L1) 8c 2 R\{0}  emphasizing its interpretation as a
fractional error.
This notion naturally extends to (positive semideﬁnite) graph Laplacians if one considers only vectors
~xthat are orthogonal to their kernels (ie  require that ~1>~x= 0 when taking the supremum in (17)).
With this modiﬁcation  the connection with the spectral graph sparsiﬁcation can be stated as follows:

Theorem 1. If dhLG  L

eG  ln()  then eG is a -spectral approximation of G.

Here  the notion of -spectral approximation is the same as in Spielman & Teng [6] (see Section 2) 
and thus is restricted to sparsiﬁcation only. The proof is provided in Appendix Section D.
As d~xis analogous to the ratio of quadratic forms with ~x  dh is likewise analogous to the notion of a
-spectral approximation. Moreover  as d~xand dh also consider angular differences between LG~x
and L
In the following sections  we compare our algorithm with other graph reduction methods using d~x 
where we choose ~xto be eigenvectors of the original graph Laplacian. In Appendix Section H  we
replicate our results using more standard measures (eg  quadratic forms and eigenvalues).

eG l~x  they serve as more sensitive measures of graph similarity.

5.2 Comparison with spectral graph sparsiﬁcation
Figure 2 compares our algorithm (prioritizing edge reduction  and excluding the possibility of
contraction) with the standard spectral sparsiﬁcation algorithm of Spielman & Srivastava [20] using
three real-world datasets. We choose to compare with this particular sparsiﬁcation method because it
directly aims to optimally preserve the Laplacian. To the best of our knowledge  other sparsiﬁcation
methods either do not explicitly preserve properties associated with the Laplacian [46  47]  or share
the same spirit as Spielman & Srivastava’s algorithm [48] (often considering other settings  such
as distributed [49] or streaming [50] computation). The results in Figure 2 show that our algorithm
better preserves L† and preferentially preserves its action on eigenvectors associated with global
structure.

5.3 Comparison with graph coarsening algorithms
Figure 3 compares our algorithm (prioritizing node reduction) with several existing coarsening
algorithms using three more real-world datasets. In order to make a fair comparison with these
existing methods  after contracting their prescribed groups of nodes  we appropriately lift the resulting
reduced L†
structure.

eG (see Appendix Section C). We ﬁnd that our algorithm more accurately preserves global

6 Conclusion

In this work  we unify spectral graph sparsiﬁcation and coarsening through the use of a single cost
function that preserves the Laplacian pseudoinverse L†. We describe a probabilistic algorithm for

8

Supplementary Information:

Supplementary Information:

Supplementary Information:

A Unifying Framework for Spectrum-Preserving

A Unifying Framework for Spectrum-Preserving

A Unifying Framework for Spectrum-Preserving

Graph Sparsiﬁcation and Coarsening

Graph Sparsiﬁcation and Coarsening

Graph Sparsiﬁcation and Coarsening

 
r
e
v
u
o
c
n
a
V

Lee M. Gunderson∗

Department of Astrophysical Sciences

Princeton University

Princeton  NJ  08540  USA

leeg@princeton.edu

 
)
9
1
0
2
S
P
I
r
u
e
N

Princeton University

Gecia Bravo-Hermsdorff

Princeton Neuroscience Institute

Gecia Bravo-Hermsdorff
Department of Astrophysical Sciences

graph reduction that employs edge deletion  contraction  and reweighting to keep E⇥L†

Lee M. Gunderson∗
Princeton Neuroscience Institute
Princeton University
G  and
Princeton  NJ  08540  USA
geciah@princeton.edu
uses a new measure of edge importance (?) to minimize its variance. Using synthetic and real-world
datasets  we demonstrate that our algorithm more accurately preserves global structure compared to
existing algorithms. We hope that our framework (or some perturbation of it) will serve as a useful
tool for graph algorithms  numerical linear algebra  and machine learning.

Lee M. Gunderson∗
Princeton Neuroscience Institute
Princeton University

Gecia Bravo-Hermsdorff
Department of Astrophysical Sciences

eG⇤ = L†

Princeton  NJ  08540  USA
geciah@princeton.edu

Princeton  NJ  08540  USA

Princeton  NJ  08540  USA

|E˜G|/|EG| ≈ 1/2

|E˜G|/|EG| ≈ 1/2

|E˜G|/|EG| ≈ 1/2

leeg@princeton.edu

leeg@princeton.edu

Princeton University

Princeton University

G
E
|
/
|

k
r
o
w
s
i
h
t

G
V
|
/
|

o
t

|

|

s

(

.

m
e
Princeton  NJ  08540  USA
t
s
y
geciah@princeton.edu
S
g
n
i
s
s
e
c
o
r
P
n
o
i
t
a
m
r
o
f
n
I

)
s
/
q
(
n
o
p
u
d
e
t
c
a

˜G
V

˜G
E

y
l
l
a
u
q
e

M
R
y
b

d
e
z
i
l
a
m
r
o
n

 

l

†˜G 
L
d
n
a

†G
L

n
e
e
w
t
e
b
)
!x
d
(

r
o
r
r
e

l
a
n
o
i
t
c
a
r
F

)
s
(

|
g
n
i
n
i
a
m
e
r

|
g
|E˜G|/|EG| ≈ 1/12
n
i
n
i
a
Spielman et al
m
Fractional error (d!x) between
e
d
†˜G 
r
e
L†˜G and L†
G  normalized by RM
L
l
p
d
m
n
a
a
s

d
e
l
p
m
a
s

s
e
g
d
e

s
e
g
d
e

s
e
g
d
e

l

Ours

Fractional error (d!x)
†G
L
between L†˜G and L†
n
e
e
w
t
e
b

f
o
n
o
i
t
c
a
r
F

G

f
o
r
e
b
m
u
N

s
e
d
o
n
f
o
n
o
i
t
c
a
r
F

f
o
n
o
i
t
c
a
r
F

Fraction of nodes remaining |V˜G|/|VG|

)
!x
d
(

r
o
r
r
e

†˜G
L
d
n
a

†G
L
n
e
e
w
t
e
b

l
a
n
o
i
t
c
a
r
F

1

)
!x
d
(

0.1

0.01

0.001

r
o
r
r
e

l
a
n
o
i
t
c
a
r
F

1.0

1

0.1

0.01
∞
→
0.001
w

e

Spielman et al

|E˜G|/|EG| ≈ 1/12
Fractional error (d!x) between
L†˜G and L†
G  normalized by RM

d
e
t
u
b
i
r
t
n
o
c

l
a
r
u
e
N
n
o

Ours

s
r
o
h
t
u
a

e
c
n
e
r
e
f
n
o
C
d
r
3
3

h
t
o
B
∗

Fractional error (d!x)
∞
between L†˜G and L†
→
Fraction of nodes remaining |V˜G|/|VG|

e
w

G

1

0.1

|E˜G|/|EG| ≈ 1/12
Fractional error (d!x) between
L†˜G and L†
G  normalized by RM

Spielman et al

Ours

0.01

0.001

Fractional error (d!x)
between L†˜G and L†

G

Fraction of nodes remaining |V˜G|/|VG|

2
1
5
3
2
1

0.5

0.0

1.0

0.5

0.0

1.0

0.5

0.0

.
a
d
a
n
a
C

Fraction of sampled edges acted upon (q/s)

Fraction of edges remaining |E˜G|/|EG|
Number of sampled edges (s)

Fraction of edges remaining |E˜G|/|EG|
Number of sampled edges (s)
Supplementary Information:

Fraction of edges remaining |E˜G|/|EG|
Number of sampled edges (s)
Supplementary Information:

Graph Sparsiﬁcation and Coarsening

A Unifying Framework for Spectrum-Preserving
Fraction of sampled edges acted upon (q/s)

Figure 2: Our sparsiﬁcation algorithm preferentially preserves global structure. We applied our algorithm
without contraction (Ours) and compare with that of Spielman & Srivastava [20] (Spielman et al) using three
A Unifying Framework for Spectrum-Preserving
Fraction of sampled edges acted upon (q/s)
datasets: Left: a collaboration network of Jazz musicians (198 nodes and 2742 edges) from [51]; Middle:
the C. elegans posterior nervous system connectome (269 nodes and 2902 edges) from [52]; and Right: a
weighted social network of face-to-face interactions between primary school students  with initial edge weights
proportional to the number of interactions between pairs of students (236 nodes and 5899 edges) from [53]. For
G~xand L†
the two algorithms  we compute the hyperbolic distance d~x(fractional error) between L†
levels of sparsiﬁcation for two choices of ~x: the smallest non-trivial eigenvector of the original Laplacian (dark
Gecia Bravo-Hermsdorff
Lee M. Gunderson
shading)  which is associated with global structure; and the median eigenvector (light shading). Shading denotes
Department of Astrophysical Sciences
Princeton Neuroscience Institute
Princeton University
one standard deviation about the mean for 16 runs of the algorithms. The curves end at the minimum edge
Princeton  NJ  08540  USA
density for which the sparsiﬁed graph is connected.
geciah@princeton.edu

Graph Sparsiﬁcation and Coarsening

Lee M. Gunderson
Princeton Neuroscience Institute
Princeton University

Gecia Bravo-Hermsdorff
Department of Astrophysical Sciences

eG~xat different

Princeton  NJ  08540  USA
geciah@princeton.edu

Princeton  NJ  08540  USA
geciah@princeton.edu

Princeton Neuroscience Institute

Gecia Bravo-Hermsdorff

Princeton  NJ  08540  USA

Princeton  NJ  08540  USA

leeg@princeton.edu

leeg@princeton.edu

Princeton University

Princeton University

Princeton University

 
)
9
1
0
2
S
P
I
r
u
e
N

 
r
e
v
u
o
c
n
a
V

s

(

.

Supplementary Information:

A Unifying Framework for Spectrum-Preserving

Graph Sparsiﬁcation and Coarsening

Lee M. Gunderson

Department of Astrophysical Sciences

Princeton University

Princeton  NJ  08540  USA

leeg@princeton.edu

M
R
y
b

d
e
z
i
l
a
m
r
o
n

 

†˜G
L
d
n
a

†G
L

n
e
e
w
t
e
b
)
!x
d
(

r
o
r
r
e

l
a
n
o
i
t
c
a
r
F

2
/
1
≈

|

G
E
|
/
|

˜G
E

2
1
/
1
≈

|

G
E
|
/
|

˜G
E

|

|

y
t
i
s
r
e
v
i
n
U
n
o
t
e
c
n
i
r
P

A
S
U

 

0
4
5
8
0

 
J
N

 
n
o
t
e
c
n
i
r
P

u
d
e
.
n
o
t
e
c
n
i
r
p
@
g
e
e
l

y
t
i
s
r
e
v
i
n
U
n
o
t
e
c
n
i
r
P

A
S
U

 
0
4
5
8
0
 
J
N

 
n
o
t
e
c
n
i
r
P

u
d
e
.
n
o
t
e
c
n
i
r
p
@
h
a
i
c
e
g

2
/
1
⇡

|

G
E
|
/
|

˜G
E

2
1
/
1
⇡

|

G
E
|
/
|

˜G
E

|

|

k
r
o
w
s
i
h
t

o
t

y
l
l
a
u
q
e

m
e
t
s
y
S
g
n
i
s
s
e
c
o
r
P
n
o
i
t
a
m
r
o
f
n
I

e
c
n
e
r
e
f
n
o
C
d
r
3
3

)
s
/
q
(

n
o
p
u

d
e
t
c
a

|

G
V
|
/
|

˜G
V

|
g
n
i
n
i
a
m
e
r

|

G
E
|
/
|

˜G
E

|
g
n
i
n
i
a
m
e
r

M
R
y
b

d
e
z
i
l
a
m
r
o
n

 

†˜G
L
d
n
a

†G
L

n
e
e
w
t
e
b

)
~x
d
(

r
o
r
r
e

l
a
n
o
i
t
c
a
r
F

M
R
y
b

d
e
z
i
l
a
m
r
o
n
 

l

†˜G 
L
d
n
a

†G
L

n
e
e
w
t
e
b

)
~x
d
(

r
o
r
r
e

l
a
n
o
i
t
c
a
r
F

|E˜G|/|EG| ⇡ 1/2

∗Both authors contributed equally to this work.
10

)
s
(

KMeans

10

|E˜G|/|EG| ⇡ 1/2

∗Both authors contributed equally to this work.
10

KMeans

|E˜G|/|EG| ⇡ 1/2

∗Both authors contributed equally to this work.

HEM

|E˜G|/|EG| ⇡ 1/12
33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.
RM
1
)
Fractional error (d~x) between
~x
†˜G
L
d
L†˜G and L†
(
G  normalized by RM
d
n
a

|E˜G|/|EG| ⇡ 1/12
33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.
RM
Fractional error (d~x) between
L†˜G and L†
G  normalized by RM
LV

|E˜G|/|EG| ⇡ 1/12
33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.
RM
Fractional error (d~x) between
L†˜G and L†
G  normalized by RM
LV

†˜G 
L
d
n
a

d
e
l
p
m
a
s

d
e
l
p
m
a
s

l
a
r
u
e
N
n
o

s
e
g
d
e

s
e
g
d
e

r
o
r
r
e

s
e
g
d
e

d
e
t
u
b
i
r
t
n
o
c

)
~x
d
(

HCM

HCM

HCM

HEM

HEM

0.1

1

1

l

KMeans

0.1

0.01

r
o
r
r
e

s
e
d
o
n
†G
f
Ours
L
Fractional error (d~x)
o
n
n
between L†˜G and L†
o
e
e
i
t
w
c
a
t
e
r
0.5
F
b

LV
†G
L
n
e
e
w
t
e
b

l
a
n
o
i
t
c
a
r
F

G

l
a
n
o
i
t
c
a
r
1.0
F

f
o

n
o
i
t
c
a
r
F

f
o
r
e
b
m
u
N

0.1
2
1
5
3
2
1.0
1

0.01

f
o

n
o
i
t
c
a
r
F

0.0

s
r
o
h
t
u
Fractional error (d~x)
a
h
Ours
between L†˜G and L†
t
o
B
G
⇤
0.5

1
!

1
!

e
w

w

e

Ours
Fractional error (d~x)
between L†˜G and L†
0.5

G

0.01

0.0

1.0

0.0

Fraction of nodes remaining |V˜G|/|VG|

Fraction of nodes remaining |V˜G|/|VG|

Fraction of nodes remaining |V˜G|/|VG|

Fraction of sampled edges acted upon (q/s)

Fraction of edges remaining |E˜G|/|EG|
Number of sampled edges (s)

Fraction of edges remaining |E˜G|/|EG|
Number of sampled edges (s)

Fraction of edges remaining |E˜G|/|EG|
Number of sampled edges (s)

Figure 3: Our algorithm preserves global structure more accurately than other coarsening algorithms.
We compare our algorithm (prioritizing node reduction) (Ours) to several existing coarsening algorithms: two
classical methods for graph coarsening (heavy-edge matching (HEM) [54] and heavy-clique matching (HCM)
[54])  and two recently proposed spectral coarsening algorithms (local variation by Loukas [55] (LV) and the k-
Fraction of sampled edges acted upon (q/s)
means method by Jin & Jaja [21] (KMeans)). We ran the comparisons using three datasets: Left: a transportation
network of European cities and roads between them (1039 nodes and 1305 edges) from [56]; Middle: a triangular
mesh of the text “NeurIPS” (567 nodes and 1408 edges); and Right: a weighted social network of face-to-face
interactions during an exhibition on infectious diseases  with initial edge weights proportional to the number of
interactions between pairs of people (410 nodes and 2765 edges) from [57]. For all algorithms considered  we
compute the hyperbolic distance d~x(fractional error) between L†
eigenvector of the original Laplacian (associated with global structure). To provide a baseline  we plot their mean
fractional error normalized by that obtained by random matching (RM) [54] for the same level of coarsening.
Shading denotes one standard deviation about the mean for 16 runs of the algorithms.

eG l~x  where ~xis the smallest non-trivial

Fraction of sampled edges acted upon (q/s)

G~xand L†

⇤Both authors contributed equally to this work.

⇤Both authors contributed equally to this work.

⇤Both authors contributed equally to this work.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

9

Acknowledgments

We would like to thank Matthew de Courcy-Ireland for insightful discussions
and Ashlyn Maria Bravo Gundermsdorff for unique perspectives.

References
[1] Chazelle  B. The Discrepancy Method: Randomness and Complexity (Cambridge University Press  2000).
[2] Bronstein  M. M.  Bruna  J.  LeCun  Y.  Szlam  A. & Vandergheynst  P. Geometric deep learning: Going

beyond Euclidean data. IEEE Signal Processing Magazine 34  18–42 (2017).

[3] Bruna  J.  Zaremba  W.  Szlam  A. & LeCun  Y. Spectral networks and locally connected networks on

graphs. International Conference on Learning Representations (2014).

[4] Henaff  M.  Bruna  J. & LeCun  Y.

arXiv:1506.05163 (2015).

Deep convolutional networks on graph-structured data.

[5] Batson  J.  Spielman  D. A.  Srivastava  N. & Teng  S.-H. Spectral sparsiﬁcation of graphs: Theory and

algorithms. Communications of the ACM 56  87–94 (2013).

[6] Spielman  D. A. & Teng  S.-H. Spectral sparsiﬁcation of graphs. SIAM Journal on Computing 40 

981–1025 (2011).

[7] Cohen  M. B. et al. Solving SDD linear systems in nearly m log1/2 n time. Proceedings of the 46th Annual

ACM Symposium on Theory of Computing (2014).

[8] Le Gall  F. Powers of tensors and fast matrix multiplication. Proceedings of the 39th International

Symposium on Symbolic and Algebraic Computation (2014).

[9] Safro  I.  Sanders  P. & Schulz  C. Advanced coarsening schemes for graph partitioning. Journal of

Experimental Algorithmics 19  1.1–1.24 (2015).

[10] Harel  D. & Koren  Y. A fast multi-scale method for drawing large graphs. Graph Drawing 183–196

(2001).

[11] Simonovsky  M. & Komodakis  N. Dynamic edge-conditioned ﬁlters in convolutional neural networks on

graphs. IEEE Conference on Computer Vision and Pattern Recognition 3693–3702 (2017).

[12] Lafon  S. & Lee  A. Diffusion maps and coarse-graining: A uniﬁed framework for dimensionality reduction 
graph partitioning  and data set parameterization. IEEE Transactions on Pattern Analysis and Machine
Intelligence 28  1393–1403 (2006).

[13] Chen  H.  Perozzi  B.  Hu  Y. & Skiena  S. HARP: Hierarchical representation learning for networks. 32nd

AAAI Conference on Artiﬁcial Intelligence (2018).

[14] Hirani  A.  Kalyanaraman  K. & Watts  S. Graph Laplacians and least squares on graphs. IEEE International

Parallel and Distributed Processing Symposium Workshop 812–821 (2015).

[15] Negahban  S.  Oh  S. & Shah  D. Iterative ranking from pairwise comparisons. Advances in Neural

Information Processing Systems 2474–2482 (2012).

[16] Solis  R.  Borkar  V. S. & Kumar  P. A new distributed time synchronization protocol for multihop wireless

networks. Proceedings of the 45th IEEE Conference on Decision and Control 2734–2739 (2006).

[17] Fiedler  M. Algebraic connectivity of graphs. Czechoslovak Mathematical Journal 23  298–305 (1973).
[18] Christiano  P.  Kelner  J. A.  Madry  A.  Spielman  D. A. & Teng  S.-H. Electrical ﬂows  Laplacian systems 
and faster approximation of maximum ﬂow in undirected graphs. Proceedings of the 43rd Annual ACM
Symposium on Theory of Computing 273–282 (2011).

[19] Kyng  R.  Rao  A.  Sachdeva  S. & Spielman  D. A. Algorithms for Lipschitz learning on graphs.

Conference on Learning Theory (2015).

[20] Spielman  D. A. & Srivastava  N. Graph sparsiﬁcation by effective resistances. SIAM Journal on Computing

40  1913–1926 (2011).

[21] Jin  Y. & JaJa  J. F. Network summarization with preserved spectral properties. arXiv:1802.04447 (2018).
[22] Purohit  M.  Prakash  B. A.  Kang  C.  Zhang  Y. & Subrahmanian  V. Fast inﬂuence-based coarsening for
large networks. Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining 1296–1305 (2014).

[23] Loukas  A. & Vandergheynst  P. Spectrally approximating large graphs with smaller graphs. International

Conference on Machine Learning 80  3237–3246 (2018).

10

[24] Zhao  Z.  Wang  Y. & Feng  Z. Nearly-linear time spectral graph reduction for scalable graph partitioning

and data visualization. arXiv:1812.08942 (2018).

[25] Teng  S.-H. The Laplacian paradigm: Emerging algorithms for massive graphs. Theory and Applications

of Models of Computation 2–14 (2010).

[26] Chandra  A. K.  Raghavan  P.  Ruzzo  W. L.  Smolensky  R. & Tiwari  P. The electrical resistance of a

graph captures its commute and cover times. Computational Complexity 6  312–340 (1996).

[27] Monnig  N. D. & Meyer  F. G. The resistance perturbation distance: A metric for the analysis of dynamic

networks. Discrete Applied Mathematics 236  347–386 (2018).

[28] Cohen  M. B. et al. Almost-linear-time algorithms for Markov chains and new spectral primitives for
directed graphs. Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing
410–419 (2017).

[29] Oxley  J. G. Matroid Theory  vol. 3 (Oxford University Press  USA  2006).
[30] Ranjan  G.  Zhang  Z.-L. & Boley  D. Incremental computation of pseudoinverse of Laplacian. Lecture

Notes in Computer Science 729–749 (2014).

[31] Herbster  M.  Pontil  M. & Wainer  L. Online learning over graphs. Proceedings of the 22nd International

Conference on Machine Learning 305–312 (2005).

[32] Gentile  C.  Herbster  M. & Pasteris  S. Online similarity prediction of networked data from known and

unknown graphs. Conference on Learning Theory 662–695 (2013).

[33] Van Mieghem  P.  Devriendt  K. & Cetinay  H. Pseudoinverse of the Laplacian and best spreader node in a

network. Physical Review E 96  032311 (2017).

[34] Ranjan  G. & Zhang  Z.-L. Geometry of complex networks and topological centrality. Physica A: Statistical

Mechanics and its Applications 392  3833–3845 (2013).

[35] Saerens  M.  Fouss  F.  Yen  L. & Dupont  P. The principal components analysis of a graph  and its

relationships to spectral clustering. European Conference on Machine Learning 371–383 (2004).

[36] Pirotte  A.  Renders  J.-M.  Saerens  M. & Fouss  F. Random-walk computation of similarities between
nodes of a graph with application to collaborative recommendation. IEEE Transactions on Knowledge &
Data Engineering 355–369 (2007).

[37] Riedel  K. S. A Sherman–Morrison–Woodbury identity for rank augmenting matrices with application to

centering. SIAM Journal on Matrix Analysis and Applications 13  659–662 (1992).

[38] Meyer  C. D.  Jr. Generalized inversion of modiﬁed matrices. SIAM Journal on Applied Mathematics 24 

315–323 (1973).

[39] Woodbury  M. A.

Inverting Modiﬁed Matrices. Memorandum Rept 42  Statistical Research Group

(Princeton University  Princeton  NJ  1950).

[40] Koren  Y.  Carmel  L. & Harel  D. ACE: A fast multiscale eigenvectors computation for drawing huge

graphs. IEEE Symposium on Information Visualization 137–144 (2002).

[41] Hendrickson  B. & Leland  R. W. A multilevel algorithm for partitioning graphs. Proceedings of the 1995

ACM/IEEE Conference on Supercomputing 95  1–14 (1995).

[42] Ron  D.  Safro  I. & Brandt  A. Relaxation-based coarsening and multiscale graph organization. SIAM

Journal on Multiscale Modeling & Simulation 9  407–423 (2011).

[43] Kyng  R.  Pachocki  J.  Peng  R. & Sachdeva  S. A framework for analyzing resparsiﬁcation algorithms.

Proceedings of the 38th Annual ACM-SIAM Symposium on Discrete Algorithms 2032–2043 (2017).

[44] Lee  Y. T. & Sun  H. An SDP-based algorithm for linear-sized spectral sparsiﬁcation. Proceedings of the

49th Annual ACM SIGACT Symposium on Theory of Computing 678–687 (2017).

[45] Ausiello  G. et al. Complexity and Approximation: Combinatorial Optimization Problems and their

Approximability Properties (Springer Science & Business Media  2012).

[46] Satuluri  V.  Parthasarathy  S. & Ruan  Y. Local graph sparsiﬁcation for scalable clustering. In Proceedings

of the 2011 ACM SIGMOD International Conference on Management of data  721–732 (ACM  2011).
[47] Ahn  K. J.  Guha  S. & McGregor  A. Graph sketches: sparsiﬁcation  spanners  and subgraphs.

In
Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI symposium on Principles of Database Systems 
5–14 (ACM  2012).

[48] Fung  W.-S.  Hariharan  R.  Harvey  N. J. & Panigrahi  D. A general framework for graph sparsiﬁcation.

SIAM Journal on Computing 48  1196–1223 (2019).

[49] Koutis  I. & Xu  S. C. Simple parallel and distributed algorithms for spectral graph sparsiﬁcation. ACM

Transactions on Parallel Computing (TOPC) 3  14 (2016).

11

[50] Kapralov  M.  Lee  Y. T.  Musco  C.  Musco  C. P. & Sidford  A. Single pass spectral sparsiﬁcation in

dynamic streams. SIAM Journal on Computing 46  456–477 (2017).

[51] Gleiser  P. M. & Danon  L. Community structure in jazz. Advances in Complex Systems 6  565–573 (2003).
[52] Jarrell  T. A. et al. The connectome of a decision-making neural network. Science 337  437–444 (2012).
[53] Stehl’e  J. et al. High-resolution measurements of face-to-face contact patterns in a primary school. PloS

One 6  e23176 (2011).

[54] Karypis  G. & Kumar  V. A fast and high quality multilevel scheme for partitioning irregular graphs. SIAM

Journal on Scientiﬁc Computing 20  359–392 (1998).

[55] Loukas  A. Graph reduction with spectral and cut guarantees. arXiv:1808.10650v2 (2018).
[56]

˘Subelj  L. & Bajec  M. Robust network community detection using balanced propagation. The European
Physical Journal B 81  353–362 (2011).

[57] Isella  L. et al. What’s in a crowd? Analysis of face-to-face behavioral networks. Journal of Theoretical

Biology 271  166–180 (2011).

[58] Bell  W.  Olson  L. & Schroder  J. Pyamg: Algebraic multigrid solvers in python v3. 0  2015. URL

http://www. pyamg. org. Release 3 (2015).

12

,Gecia Bravo Hermsdorff
Lee Gunderson