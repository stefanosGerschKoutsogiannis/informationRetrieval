2019,Diffusion Improves Graph Learning,Graph convolution is the core of most Graph Neural Networks (GNNs) and usually approximated by message passing between direct (one-hop) neighbors. In this work  we remove the restriction of using only the direct neighbors by introducing a powerful  yet spatially localized graph convolution: Graph diffusion convolution (GDC). GDC leverages generalized graph diffusion  examples of which are the heat kernel and personalized PageRank. It alleviates the problem of noisy and often arbitrarily defined edges in real graphs. We show that GDC is closely related to spectral-based models and thus combines the strengths of both spatial (message passing) and spectral methods. We demonstrate that replacing message passing with graph diffusion convolution consistently leads to significant performance improvements across a wide range of models on both supervised and unsupervised tasks and a variety of datasets. Furthermore  GDC is not limited to GNNs but can trivially be combined with any graph-based model or algorithm (e.g. spectral clustering) without requiring any changes to the latter or affecting its computational complexity. Our implementation is available online.,Diffusion Improves Graph Learning

Johannes Klicpera  Stefan Weißenberger  Stephan Günnemann

Technical University of Munich

{klicpera stefan.weissenberger guennemann}@in.tum.de

Abstract

Graph convolution is the core of most Graph Neural Networks (GNNs) and usually
approximated by message passing between direct (one-hop) neighbors. In this
work  we remove the restriction of using only the direct neighbors by introducing a
powerful  yet spatially localized graph convolution: Graph diffusion convolution
(GDC). GDC leverages generalized graph diffusion  examples of which are the
heat kernel and personalized PageRank. It alleviates the problem of noisy and often
arbitrarily deﬁned edges in real graphs. We show that GDC is closely related to
spectral-based models and thus combines the strengths of both spatial (message
passing) and spectral methods. We demonstrate that replacing message passing
with graph diffusion convolution consistently leads to signiﬁcant performance
improvements across a wide range of models on both supervised and unsupervised
tasks and a variety of datasets. Furthermore  GDC is not limited to GNNs but
can trivially be combined with any graph-based model or algorithm (e.g. spectral
clustering) without requiring any changes to the latter or affecting its computational
complexity. Our implementation is available online. 1

1

Introduction

When people started using graphs for evaluating chess tournaments in the middle of the 19th
century they only considered each player’s direct opponents  i.e. their ﬁrst-hop neighbors. Only
later was the analysis extended to recursively consider higher-order relationships via A2  A3  etc.
and ﬁnally generalized to consider all exponents at once  using the adjacency matrix’s dominant
eigenvector [38  75]. The ﬁeld of Graph Neural Networks (GNNs) is currently in a similar state. Graph
Convolutional Networks (GCNs) [32]  also referred to as Message Passing Neural Networks (MPNNs)
[23] are the prevalent approach in this ﬁeld but they only pass messages between neighboring nodes
in each layer. These messages are then aggregated at each node to form the embedding for the next
layer. While MPNNs do leverage higher-order neighborhoods in deeper layers  limiting each layer’s
messages to one-hop neighbors seems arbitrary. Edges in real graphs are often noisy or deﬁned using
an arbitrary threshold [70]  so we can clearly improve upon this approach.
Since MPNNs only use the immediate neigborhod information  they are often referred to as spatial
methods. On the other hand  spectral-based models do not just rely on ﬁrst-hop neighbors and capture
more complex graph properties [16]. However  while being theoretically more elegant  these methods
are routinely outperformed by MPNNs on graph-related tasks [32  74  81] and do not generalize
to previously unseen graphs. This shows that message passing is a powerful framework worth
extending upon. To reconcile these two separate approaches and combine their strengths we propose
a novel technique of performing message passing inspired by spectral methods: Graph diffusion
convolution (GDC). Instead of aggregating information only from the ﬁrst-hop neighbors  GDC
aggregates information from a larger neighborhood. This neighborhood is constructed via a new
graph generated by sparsifying a generalized form of graph diffusion. We show how graph diffusion

1https://www.daml.in.tum.de/gdc

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

is expressed as an equivalent polynomial ﬁlter and how GDC is closely related to spectral-based
models while addressing their shortcomings. GDC is spatially localized  scalable  can be combined
with message passing  and generalizes to unseen graphs. Furthermore  since GDC generates a new
sparse graph it is not limited to MPNNs and can trivially be combined with any existing graph-based
model or algorithm in a plug-and-play manner  i.e. without requiring changing the model or affecting
its computational complexity. We show that GDC consistently improves performance across a wide
range of models on both supervised and unsupervised tasks and various homophilic datasets. In
summary  this paper’s core contributions are:
1. Proposing graph diffusion convolution (GDC)  a more powerful and general  yet spatially localized
alternative to message passing that uses a sparsiﬁed generalized form of graph diffusion. GDC is
not limited to GNNs and can be combined with any graph-based model or algorithm.

2. Analyzing the spectral properties of GDC and graph diffusion. We show how graph diffusion is

expressed as an equivalent polynomial ﬁlter and analyze GDC’s effect on the graph spectrum.

3. Comparing and evaluating several speciﬁc variants of GDC and demonstrating its wide applicabil-

ity to supervised and unsupervised learning on graphs.

2 Generalized graph diffusion

We consider an undirected graph G = (V E) with node set V and edge set E. We denote with
N = |V| the number of nodes and A ∈ RN×N the adjacency matrix. We deﬁne generalized graph
diffusion via the diffusion matrix

∞(cid:88)

k=0

S =

θkT k 

(1)

and require that(cid:80)∞

D is the diagonal matrix of node degrees  i.e. Dii =(cid:80)N

with the weighting coefﬁcients θk  and the generalized transition matrix T . The choice of θk and T k
must at least ensure that Eq. 1 converges. In this work we will consider somewhat stricter conditions
k=0 θk = 1  θk ∈ [0  1]  and that the eigenvalues of T are bounded by λi ∈ [0  1] 
which together are sufﬁcient to guarantee convergence. Note that regular graph diffusion commonly
requires T to be column- or row-stochastic.
Transition matrix. Examples for T in an undirected graph include the random walk transition matrix
Trw = AD−1 and the symmetric transition matrix Tsym = D−1/2AD−1/2  where the degree matrix
j=1 Aij. Note that in our deﬁnition Trw is
column-stochastic. We furthermore adjust the random walk by adding (weighted) self-loops to the
original adjacency matrix  i.e. use ˜Tsym = (wloopIN + D)−1/2(wloopIN + A)(wloopIN + D)−1/2 
with the self-loop weight wloop ∈ R+. This is equivalent to performing a lazy random walk with a
probability of staying at node i of pstay i = wloop/Di.
Special cases. Two popular examples of graph diffusion are personalized PageRank (PPR) [57] and
k = α(1 − α)k  with teleport
the heat kernel [36]. PPR corresponds to choosing T = Trw and θPPR
probability α ∈ (0  1) [14]. The heat kernel uses T = Trw and θHK
k!   with the diffusion time
t [14]. Another special case of generalized graph diffusion is the approximated graph convolution
introduced by Kipf & Welling [32]  which translates to θ1 = 1 and θk = 0 for k (cid:54)= 1 and uses
T = ˜Tsym with wloop = 1.
Weighting coefﬁcients. We compute the series deﬁned by Eq. 1 either in closed-form  if possible  or
by restricting the sum to a ﬁnite number K. Both the coefﬁcients deﬁned by PPR and the heat kernel
give a closed-form solution for this series that we found to perform well for the tasks considered.
Note that we are not restricted to using Trw and can use any generalized transition matrix along
with the coefﬁcients θPPR
and the series still converges. We can furthermore choose θk by
repurposing the graph-speciﬁc coefﬁcients obtained by methods that optimize coefﬁcients analogous
to θk as part of their training process. We investigated this approach using label propagation [8  13]
and node embedding models [1]. However  we found that the simple coefﬁcients deﬁned by PPR or
the heat kernel perform better than those learned by these models (see Fig. 7 in Sec. 6).

k = e−t tk

or θHK
k

k

2

.

.

.

.

.

.

.

.

.

Graph diffusion

Density deﬁnes edges

Sparsify edges

New graph

Figure 1: Illustration of graph diffusion convolution (GDC). We transform a graph A via graph
diffusion and sparsiﬁcation into a new graph ˜S and run the given model on this graph instead.

3 Graph diffusion convolution

Essentially  graph diffusion convolution (GDC) exchanges the normal adjacency matrix A with a
sparsiﬁed version ˜S of the generalized graph diffusion matrix S  as illustrated by Fig. 1. This matrix
deﬁnes a weighted and directed graph  and the model we aim to augment is applied to this graph
instead. We found that the calculated edge weights are beneﬁcial for the tasks considered. However 
we even found that GDC works when ignoring the weights after sparsiﬁcation. This enables us to use
GDC with models that only support unweighted edges such as the degree-corrected stochastic block
model (DCSBM). If required  we make the graph undirected by using ( ˜S + ˜ST )/2  e.g. for spectral
clustering. With these adjustments GDC is applicable to any graph-based model or algorithm.
Intuition. The general intuition behind GDC is that graph diffusion smooths out the neighborhood
over the graph  acting as a kind of denoising ﬁlter similar to Gaussian ﬁlters on images. This helps
with graph learning since both features and edges in real graphs are often noisy. Previous works also
highlighted the effectiveness of graph denoising. Berberidis & Giannakis [7] showed that PPR is
able to reconstruct the underlying probability matrix of a sampled stochastic block model (SBM)
graph. Kloumann et al. [35] and Ragain [64] showed that PPR is optimal in recovering the SBM
and DCSBM clusters in the space of landing probabilities under the mean ﬁeld assumption. Li et al.
[40] generalized this result by analyzing the convergence of landing probabilities to their mean ﬁeld
values. These results conﬁrm the intuition that graph diffusion-based smoothing indeed recovers
meaningful neighborhoods from noisy graphs.
Sparsiﬁcation. Most graph diffusions result in a dense matrix S. This happens even if we do not
sum to k = ∞ in Eq. 1 due to the “four/six degrees of separation” in real-world graphs [5]. However 
the values in S represent the inﬂuence between all pairs of nodes  which typically are highly localized
[54]. This is a major advantage over spectral-based models since the spectral domain does not provide
any notion of locality. Spatial localization allows us to simply truncate small values of S and recover
sparsity  resulting in the matrix ˜S. In this work we consider two options for sparsiﬁcation: 1. top-k:
Use the k entries with the highest mass per column  2. Threshold : Set entries below  to zero.
Sparsiﬁcation would still require calculating a dense matrix S during preprocessing. However  many
popular graph diffusions can be approximated efﬁciently and accurately in linear time and space.
Most importantly  there are fast approximations for both PPR [3  77] and the heat kernel [34]  with
which GDC achieves a linear runtime O(N ). Furthermore  top-k truncation generates a regular graph 
which is amenable to batching methods and solves problems related to widely varying node degrees
[15]. Empirically  we even found that sparsiﬁcation slightly improves prediction accuracy (see Fig. 5
in Sec. 6). After sparsiﬁcation we calculate the (symmetric or random walk) transition matrix on the
resulting graph via T ˜S
Limitations. GDC is based on the assumption of homophily  i.e. “birds of a feather ﬂock together”
[49]. Many methods share this assumption and most common datasets adhere to this principle.
However  this is an often overlooked limitation and it seems non-straightforward to overcome. One
way of extending GDC to heterophily  i.e. “opposites attract”  might be negative edge weights

−1/2
˜SD
˜S

.

sym = D

−1/2
˜S

3

[17  44]. Furthermore  we suspect that GDC does not perform well in settings with more complex
edges (e.g. knowledge graphs) or graph reconstruction tasks such as link prediction. Preliminary
experiments showed that GDC indeed does not improve link prediction performance.

4 Spectral analysis of GDC

Even though GDC is a spatial-based method it can also be interpreted as a graph convolution and
analyzed in the graph spectral domain. In this section we show how generalized graph diffusion
is expressed as an equivalent polynomial ﬁlter and vice versa. Additionally  we perform a spectral
analysis of GDC  which highlights the tight connection between GDC and spectral-based models.
Spectral graph theory. To employ the tools of spectral theory to graphs we exchange the regular
Laplace operator with either the unnormalized Laplacian Lun = D− A  the random-walk normalized
Lrw = IN − Trw  or the symmetric normalized graph Laplacian Lsym = IN − Tsym [76]. The
Laplacian’s eigendecomposition is L = U ΛU T   where both U and Λ are real-valued. The graph
Fourier transform of a vector x is then deﬁned via ˆx = U T x and its inverse as x = U ˆx. Using
this we deﬁne a graph convolution on G as x ∗G y = U ((U T x) (cid:12) (U T y))  where (cid:12) denotes the
Hadamard product. Hence  a ﬁlter gξ with parameters ξ acts on x as gξ(L)x = U ˆGξ(Λ)U T x  where
ˆGξ(Λ) = diag(ˆgξ 1(Λ)  . . .   ˆgξ N (Λ)). A common choice for gξ in the literature is a polynomial
ﬁlter of order J  since it is localized and has a limited number of parameters [16  27]:

 J(cid:88)

 U T .

gξ(L) =

ξjLj = U

ξjΛj

j=0

j=0

(2)

J(cid:88)

J(cid:88)

j=0

(−t)j
j!

Graph diffusion as a polynomial ﬁlter. Comparing Eq. 1 with Eq. 2 shows the close relationship
between polynomial ﬁlters and generalized graph diffusion since we only need to exchange L by T
to go from one to the other. To make this relationship more speciﬁc and ﬁnd a direct correspondence
between GDC with θk and a polynomial ﬁlter with parameters ξj we need to ﬁnd parameters that
solve

ξjLj !=

θkT k.

(3)

K(cid:88)

k=0

(cid:18)j

(cid:19)

J(cid:88)

k

j=k

(cid:19)j

(cid:18)

1 − 1
α

To ﬁnd these parameters we choose the Laplacian corresponding to L = In − T   resulting in (see
App. A)

ξj =

(−1)jθk 

θk =

(−1)kξj 

(4)

(cid:18)k

(cid:19)

K(cid:88)

j

k=j

which shows the direct correspondence between graph diffusion and spectral methods. Note that we
need to set J = K. Solving Eq. 4 for the coefﬁcients corresponding to the heat kernel θHK
and PPR
k
θPPR
k

leads to

ξHK
j =

 

ξPPR
j =

 

(5)

j

showing how the heat kernel and PPR are expressed as polynomial ﬁlters. Note that PPR’s cor-
responding polynomial ﬁlter converges only if α > 0.5. This is caused by changing the order of
summation when deriving ξPPR
  which results in an alternating series. However  if the series does
converge it gives the exact same transformation as the equivalent graph diffusion.
Spectral properties of GDC. We will now extend the discussion to all parts of GDC and analyze
how they transform the graph Laplacian’s eigenvalues. GDC consists of four steps: 1. Calculate
the transition matrix T   2. take the sum in Eq. 1 to obtain S  3. sparsify the resulting matrix by
truncating small values  resulting in ˜S  and 4. calculate the transition matrix T ˜S.
1. Transition matrix. Calculating the transition matrix T only changes which Laplacian matrix we
use for analyzing the graph’s spectrum  i.e. we use Lsym or Lrw instead of Lun. Adding self-loops to
obtain ˜T does not preserve the eigenvectors and its effect therefore cannot be calculated precisely.
Wu et al. [78] empirically found that adding self-loops shrinks the graph’s eigenvalues.

4

PPR (α)
0.05
0.15

Heat (t)
3
5

1.0

λ

0.5

20

15

10

5

A
λ
/
S
λ

0
0.0

0.5

1.5

2.0

1.0
λA

0.0

0

λ
λ;  = 10−3
∆λ;  = 10−3
∆λ;  = 10−4

˜S
λ
/

˜S
T
λ

1.00

0.75

0.50

0.25

0.00

 = 10−3
 = 10−4

1000

Index

2000

0.0

0.5
λ˜S

1.0

(a) Graph diffusion as a ﬁlter  PPR
with α and heat kernel with t. Both
act as low-pass ﬁlters.

(b) Sparsiﬁcation with threshold  of
PPR (α = 0.1) on CORA. Eigenval-
ues are almost unchanged.

(c) Transition matrix on CORA’s
sparsiﬁed graph ˜S. This acts as a
weak high-pass ﬁlter.

Figure 2: Inﬂuence of different parts of GDC on the Laplacian’s eigenvalues λ.

∞(cid:88)

k=0

2. Sum over T k. Summation does not affect the eigenvectors of the original matrix  since T kvi =
λiT k−1vi = λk
i vi  for the eigenvector vi of T with associated eigenvalue λi. This also shows that
the eigenvalues are transformed as

˜λi =

θkλk
i .

(6)

expression for PPR  i.e. ˜λi = α(cid:80)∞
exponential series  resulting in ˜λi = e−t(cid:80)∞

Since the eigenvalues of T are bounded by 1 we can use the geometric series to derive a closed-form
k=0(1 − α)kλk
. For the heat kernel we use the
i =
i = et(λi−1). How this transformation affects
tk
k! λk
the corresponding Laplacian’s eigenvalues is illustrated in Fig. 2a. Both PPR and the heat kernel act
as low-pass ﬁlters. Low eigenvalues corresponding to large-scale structure in the graph (e.g. clusters
[55]) are ampliﬁed  while high eigenvalues corresponding to ﬁne details but also noise are suppressed.
3. Sparsiﬁcation. Sparsiﬁcation changes both the eigenvalues and the eigenvectors  which means
that there is no direct correspondence between the eigenvalues of S and ˜S and we cannot analyze
its effect analytically. However  we can use eigenvalue perturbation theory (Stewart & Sun [69] 
Corollary 4.13) to derive the upper bound

k=0

α

1−(1−α)λi

(cid:118)(cid:117)(cid:117)(cid:116) N(cid:88)

(˜λi − λi)2 ≤ ||E||F ≤ N||E||max ≤ N  

(7)

i=1

√

with the perturbation matrix E = ˜S − S and the threshold . This bound signiﬁcantly overestimates
the perturbation since PPR and the heat kernel both exhibit strong localization on real-world graphs
and hence the change in eigenvalues empirically does not scale with N (or  rather 
N). By ordering
the eigenvalues we see that  empirically  the typical thresholds for sparsiﬁcation have almost no effect
on the eigenvalues  as shown in Fig. 2b and in the close-up in Fig. 11 in App. B.2. We ﬁnd that the
small changes caused by sparsiﬁcation mostly affect the highest and lowest eigenvalues. The former
correspond to very large clusters and long-range interactions  which are undesired for local graph
smoothing. The latter correspond to spurious oscillations  which are not helpful for graph learning
either and most likely affected because of the abrupt cutoff at .
4. Transition matrix on ˜S. As a ﬁnal step we calculate the transition matrix on the resulting graph
˜S. This step does not just change which Laplacian we consider since we have already switched to
using the transition matrix in step 1. It furthermore does not preserve the eigenvectors and is thus
again best investigated empirically by ordering the eigenvalues. Fig. 2c shows that  empirically 
this step slightly dampens low eigenvalues. This may seem counterproductive. However  the main
purpose of using the transition matrix is ensuring that sparsiﬁcation does not cause nodes to be treated
differently by losing a different number of adjacent edges. The ﬁltering is only a side-effect.
Limitations of spectral-based models. While there are tight connections between GDC and spectral-
based models  GDC is actually spatial-based and therefore does not share their limitations. Similar to
polynomial ﬁlters  GDC does not compute an expensive eigenvalue decomposition  preserves locality
on the graph and is not limited to a single graph after training  i.e. typically the same coefﬁcients θk

5

can be used across graphs. The choice of coefﬁcients θk depends on the type of graph at hand and
does not change signiﬁcantly between similar graphs. Moreover  the hyperparameters α of PPR and t
of the heat kernel usually fall within a narrow range that is rather insensitive to both the graph and
model (see Fig. 8 in Sec. 6).

5 Related work

Graph diffusion and random walks have been extensively studied in classical graph learning [13  14 
36  37]  especially for clustering [34]  semi-supervised classiﬁcation [12  22]  and recommendation
systems [44]. For an overview of existing methods see Masuda et al. [46] and Fouss et al. [22].
The ﬁrst models similar in structure to current Graph Neural Networks (GNNs) were proposed by
Sperduti & Starita [68] and Baskin et al. [6]  and the name GNN ﬁrst appeared in [24  65]. However 
they only became widely adopted in recent years  when they started to outperform classical models in
many graph-related tasks [19  33  42  82]. In general  GNNs are classiﬁed into spectral-based models
[11  16  28  32  41]  which are based on the eigendecomposition of the graph Laplacian  and spatial-
based methods [23  26  43  52  56  62  74]  which use the graph directly and form new representations
by aggregating the representations of a node and its neighbors. However  this distinction is often rather
blurry and many models can not be clearly attributed to one type or the other. Deep learning also
inspired a variety of unsupervised node embedding methods. Most models use random walks to learn
node embeddings in a similar fashion as word2vec [51] [25  61] and have been shown to implicitly
perform a matrix factorization [63]. Other unsupervised models learn Gaussian distributions instead
of vectors [10]  use an auto-encoder [31]  or train an encoder by maximizing the mutual information
between local and global embeddings [73].
There have been some isolated efforts of using extended neighborhoods for aggregation in GNNs
and graph diffusion for node embeddings. PPNP [33] propagates the node predictions generated by
a neural network using personalized PageRank  DCNN [4] extends node features by concatenating
features aggregated using the transition matrices of k-hop random walks  GraphHeat [79] uses the
heat kernel and PAN [45] the transition matrix of maximal entropy random walks to aggregate over
nodes in each layer  PinSage [82] uses random walks for neighborhood aggregation  and MixHop [2]
concatenates embeddings aggregated using the transition matrices of k-hop random walks before
each layer. VERSE [71] learns node embeddings by minimizing KL-divergence from the PPR matrix
to a low-rank approximation. Attention walk [1] uses a similar loss to jointly optimize the node
embeddings and diffusion coefﬁcients θk. None of these works considered sparsiﬁcation  generalized
graph diffusion  spectral properties  or using preprocessing to generalize across models.

6 Experimental results

Experimental setup. We take extensive measures to prevent any kind of bias in our results. We
optimize the hyperparameters of all models on all datasets with both the unmodiﬁed graph and all
GDC variants separately using a combination of grid and random search on the validation set. Each
result is averaged across 100 data splits and random initializations for supervised tasks and 20 random
initializations for unsupervised tasks  as suggested by Klicpera et al. [33] and Shchur et al. [67]. We
report performance on a test set that was used exactly once. We report all results as averages with
95 % conﬁdence intervals calculated via bootstrapping.
We use the symmetric transition matrix with self-loops ˜Tsym = (IN +D)−1/2(IN +A)(IN +D)−1/2
for GDC and the column-stochastic transition matrix T ˜S
on ˜S. We present two simple
and effective choices for the coefﬁcients θk: The heat kernel and PPR. The diffusion matrix S is
sparsiﬁed using either an -threshold or top-k.
Datasets and models. We evaluate GDC on six datasets: The citation graphs CITESEER [66] 
CORA [48]  and PUBMED [53]  the co-author graph COAUTHOR CS [67]  and the co-purchase
graphs AMAZON COMPUTERS and AMAZON PHOTO [47  67]. We only use their largest connected
components. We show how GDC impacts the performance of 9 models: Graph Convolutional
Network (GCN) [32]  Graph Attention Network (GAT) [74]  jumping knowledge network (JK)
[80]  Graph Isomorphism Network (GIN) [81]  and ARMA [9] are supervised models. The degree-
corrected stochastic block model (DCSBM) [30]  spectral clustering (using Lsym) [55]  DeepWalk

rw = ˜SD−1

˜S

6

CORA

None
Heat
PPR

75
72
69
66
63
60

CITESEER

PUBMED

80

76

72

GCN

JK

GAT
GIN
COAUTHOR CS

ARMA

GCN

JK

GAT
GIN
AMZ COMP

ARMA

GCN

JK

GAT
GIN
AMZ PHOTO

ARMA

80

60

90

75

60

)

%

(

y
c
a
r
u
c
c
A

84

81

78

75

72

)

%

(

y
c
a
r
u
c
c
A

92

90

oom

GIN

JK

40

GCN

GAT

ARMA
Figure 3: Node classiﬁcation accuracy of GNNs with and without GDC. GDC consistently improves
accuracy across models and datasets. It is able to ﬁx models whose accuracy otherwise breaks down.

ARMA

ARMA

GCN

GCN

GAT

GIN

GAT

JK

GIN

JK

CORA

CITESEER

)

%

(

y
c
a
r
u
c
c
A

60

45

30

)

%

(

y
c
a
r
u
c
c
A

60

45

30

DCSBM Spectral DeepWalk
COAUTHOR CS

None
Heat
PPR

DGI

60

45

30

60

45

30

DCSBM Spectral DeepWalk
AMZ COMP

DGI

PUBMED

DCSBM Spectral DeepWalk
AMZ PHOTO

DGI

70

60

50

40

75

60

45

30

DCSBM Spectral DeepWalk

DGI

DCSBM Spectral DeepWalk

DGI

DCSBM Spectral DeepWalk

DGI

Figure 4: Clustering accuracy with and without GDC. GDC consistently improves the accuracy
across a diverse set of models and datasets.

[61]  and Deep Graph Infomax (DGI) [73] are unsupervised models. Note that DGI uses node features
while other unsupervised models do not. We use k-means clustering to generate clusters from node
embeddings. Dataset statistics and hyperparameters are reported in App. B.
Semi-supervised node classiﬁcation. In this task the goal is to label nodes based on the graph  node
features X ∈ RN×F and a subset of labeled nodes y. The main goal of GDC is improving the
performance of MPNN models. Fig. 3 shows that GDC consistently and signiﬁcantly improves the
accuracy of a wide variety of state-of-the-art models across multiple diverse datasets. Note how GDC
is able to ﬁx the performance of GNNs that otherwise break down on some datasets (e.g. GAT). We
also surpass or match the previous state of the art on all datasets investigated (see App. B.2).
Clustering. We highlight GDC’s ability to be combined with any graph-based model by reporting
the performance of a diverse set of models that use a wide range of paradigms. Fig. 4 shows the
unsupervised accuracy obtained by matching clusters to ground-truth classes using the Hungarian
algorithm. Accuracy consistently and signiﬁcantly improves for all models and datasets. Note that
spectral clustering uses the graph’s eigenvectors  which are not affected by the diffusion step itself.
Still  its performance improves by up to 30 percentage points. Results in tabular form are presented
in App. B.2.

7

)

%

(
y
c
a
r
u
c
c
A
∆

0
−1
−2

)

%

(
y
c
a
r
u
c
c
A

80

70

100

CORA
CITESEER
AMZ COMP

101

102

Average degree

103

Figure 5: GCN+GDC accuracy
(using PPR and top-k). Lines in-
dicate original accuracy and de-
gree. GDC surpasses original ac-
curacy at around the same degree
independent of dataset. Sparsiﬁ-
cation often improves accuracy.

0

1

2

3

Self-loop weight
6:
Difference

Figure
in
GCN+GDC accuracy (using
PPR and top-k 
percentage
points) compared to the symmet-
ric Tsym without self-loops. Trw
performs worse and self-loops
have no signiﬁcant effect.

CORA
CITESEER
AMZ COMP

Tsym
Trw

PPR
CORA

AdaDIF
CITESEER

AMZ
COMP

K
J

N
C
G

A
M
R
A

K
J

N
C
G

A
M
R
A

K
J

N
C
G

A
M
R
A

)

%

(

y
c
a
r
u
c
c
A

80

70

4

Figure 7: Accuracy of GDC with
coefﬁcients θk deﬁned by PPR
and learned by AdaDIF. Simple
PPR coefﬁcients consistently per-
form better than those obtained
by AdaDIF  even with optimized
regularization.

) CORA
%

85

(

y
c
a
r
u
c
c
A

GCN
JK
ARMA

80

0.01 0.05
α

PPR

CITESEER

76

85

AMZ COMP

85

80

74

0.4

0.01 0.05
α

82

0.4

0.01 0.05
α

75

0.4

0.5 1 2 5 20

t

CORA

Heat

CITESEER

AMZ COMP

75

72

80

70

0.5 1 2 5 20

t

0.5 1 2 5 20

t

Figure 8: Accuracy achieved by using GDC with varying hyperparameters of PPR (α) and the heat
kernel (t). Optimal values fall within a narrow range that is consistent across datasets and models.

In this work we concentrate on node-level prediction tasks in a transductive setting. However  GDC
can just as easily be applied to inductive problems or different tasks like graph classiﬁcation. In
our experiments we found promising  yet not as consistent results for graph classiﬁcation (e.g. 2.5
percentage points with GCN on the DD dataset [18]). We found no improvement for the inductive
setting on PPI [50]  which is rather unsurprising since the underlying data used for graph construction
already includes graph diffusion-like mechanisms (e.g. regulatory interactions  protein complexes 
and metabolic enzyme-coupled interactions). We furthermore conducted experiments to answer ﬁve
important questions:
Does GDC increase graph density? When sparsifying the generalized graph diffusion matrix S we
are free to choose the resulting level of sparsity in ˜S. Fig. 5 indicates that  surprisingly  GDC requires
roughly the same average degree to surpass the performance of the original graph independent of the
dataset and its average degree (-threshold in App. B.2  Fig. 12). This suggests that the sparsiﬁcation
hyperparameter can be obtained from a ﬁxed average degree. Note that CORA and CITESEER are
both small graphs with low average degree. Graphs become denser with size [39] and in practice we
expect GDC to typically reduce the average degree at constant accuracy. Fig. 5 furthermore shows
that there is an optimal degree of sparsity above which the accuracy decreases. This indicates that
sparsiﬁcation is not only computationally beneﬁcial but also improves prediction performance.
How to choose the transition matrix T ? We found Tsym to perform best across datasets. More
speciﬁcally  Fig. 6 shows that the symmetric version on average outperforms the random walk
transition matrix Trw. This ﬁgure also shows that GCN accuracy is largely insensitive to self-loops
when using Tsym – all changes lie within the estimated uncertainty. However  we did ﬁnd that other
models  e.g. GAT  perform better with self-loops (not shown).
How to choose the coefﬁcients θk? We found the coefﬁcients deﬁned by PPR and the heat kernel
to be effective choices for θk. Fig. 8 shows that their optimal hyperparameters typically fall within
a narrow range of α ∈ [0.05  0.2] and t ∈ [1  10]. We also tried obtaining θk from models that
learn analogous coefﬁcients [1  8  13]. However  we found that θk obtained by these models tend to
converge to a minimal neighborhood  i.e. they converge to θ0 ≈ 1 or θ1 ≈ 1 and all other θk ≈ 0.

8

)

%

(

y
c
a
r
u
c
c
A
∆

4
2
0

2
1
0

CORA
Heat
PPR

1 2 3 4 ≥5

4
5
2
=
¯n

0
1
6
=
¯n

3
3
3
=
¯n

5
1
1
=
¯n

8
.
7
4
=
¯n

Hops

CITESEER

PUBMED

1

0

1 2 3 4 5 ≥6

2
3
2
=
¯n

5
7
4
=
¯n

5
4
3
=
¯n

4
7
1
=
¯n

8
.
2
8
=
¯n

3
.
2
7
=
¯n

Hops

1 2 3 4 5 ≥6

6
.
7
1
=
¯n

7
8
1
=
¯n

0
0
5
=
¯n

9
2
5
=
¯n

2
5
1
=
¯n

7
.
4
5
=
¯n

Hops

COAUTHOR CS
1

0

1 2 3 4 ≥5

7
5
5
=
¯n

9
4
3
=
¯n

2
.
1
7
=
¯n

1
2
1
2
=
¯n

2
0
6
1
=
¯n

Hops

AMZ COMP

2
1
0

1 2 ≥3

1
2
3
=
¯n

7
7
8
=
¯n

2
0
1
=
¯n

Hops

AMZ PHOTO
2
1
0

1 2 3 ≥4

8
8
4
=
¯n

9
6
7
=
¯n

6
.
1
7
=
¯n

1
.
1
1
=
¯n

Hops

Figure 10: Improvement (percentage points) in GCN accuracy by adding GDC depending on distance
(number of hops) from the training set. Nodes further away tend to beneﬁt more from GDC.

85

80

75

5

)

%

(

y
c
a
r
u
c
c
A

None
Heat
PPR
60

GCN
JK
ARMA
20
30

This is caused by their training losses almost always decreasing
when the considered neighborhood shrinks. We were able to
control this overﬁtting to some degree using strong regularization
(speciﬁcally  we found L2 regularization on the difference of
neighboring coefﬁcients θk+1−θk to perform best). However  this
requires hand-tuning the regularization for every dataset  which
defeats the purpose of learning the coefﬁcients from the graph.
Moreover  we found that even with hand-tuned regularization the
coefﬁcients deﬁned by PPR and the heat kernel perform better
than trained θk  as shown in Fig. 7.
How does the label rate affect GDC? When varying the label
rate from 5 up to 60 labels per class we found that the improvement
achieved by GDC increases the sparser the labels are. Still  GDC
improves performance even for 60 labels per class  i.e. 17 %
label rate (see Fig. 9). This trend is most likely due to larger
neighborhood leveraged by GDC.
Which nodes beneﬁt from GDC? Our experiments showed no correlation of improvement with
most common node properties  except for the distance from the training set. Nodes further away
from the training set tend to beneﬁt more from GDC  as demonstrated by Fig. 10. Besides smoothing
out the neighborhood  GDC also has the effect of increasing the model’s range  since it is no longer
restricted to only using ﬁrst-hop neighbors. Hence  nodes further away from the training set inﬂuence
the training and later beneﬁt from the improved model weights.

Figure 9: Accuracy on Cora with
different label rates.
Improve-
ment from GDC increases for
sparser label rates.

10
40
Labels per class

7 Conclusion

We propose graph diffusion convolution (GDC)  a method based on sparsiﬁed generalized graph
diffusion. GDC is a more powerful  yet spatially localized extension of message passing in GNNs 
but able to enhance any graph-based model. We show the tight connection between GDC and
spectral-based models and analyzed GDC’s spectral properties. GDC shares many of the strengths of
spectral methods and none of their weaknesses. We conduct extensive and rigorous experiments that
show that GDC consistently improves the accuracy of a wide range of models on both supervised
and unsupervised tasks across various homophilic datasets and requires very little hyperparameter
tuning. There are many extensions and applications of GDC that remain to be explored. We expect
many graph-based models and tasks to beneﬁt from GDC  e.g. graph classiﬁcation and regression.
Promising extensions include other diffusion coefﬁcients θk such as those given by the methods
presented in Fouss et al. [22] and more advanced random walks and operators that are not deﬁned by
powers of a transition matrix.

Acknowledgments

This research was supported by the German Federal Ministry of Education and Research (BMBF) 
grant no. 01IS18036B  and by the Deutsche Forschungsgemeinschaft (DFG) through the Emmy
Noether grant GU 1409/2-1 and the TUM International Graduate School of Science and Engineering
(IGSSE)  GSC 81. The authors of this work take full responsibilities for its content.

9

References
[1] Sami Abu-El-Haija  Bryan Perozzi  Rami Al-Rfou  and Alex Alemi. Watch Your Step: Learning Node

Embeddings via Graph Attention. In NeurIPS  2018.

[2] Sami Abu-El-Haija  Bryan Perozzi  Amol Kapoor  Nazanin Alipourfard  Kristina Lerman  Hrayr Harutyun-
yan  Greg Ver Steeg  and Aram Galstyan. MixHop: Higher-Order Graph Convolutional Architectures via
Sparsiﬁed Neighborhood Mixing. In ICML  2019.

[3] R. Andersen  F. Chung  and K. Lang. Local Graph Partitioning using PageRank Vectors. In FOCS  2006.

[4] James Atwood and Don Towsley. Diffusion-Convolutional Neural Networks. In NIPS  2016.

[5] Lars Backstrom  Paolo Boldi  Marco Rosa  Johan Ugander  and Sebastiano Vigna. Four degrees of

separation. In ACM Web Science Conference  2012.

[6] Igor I. Baskin  Vladimir A. Palyulin  and Nikolai S. Zeﬁrov. A Neural Device for Searching Direct
Correlations between Structures and Properties of Chemical Compounds. Journal of Chemical Information
and Computer Sciences  37(4):715–721  1997.

[7] Dimitris Berberidis and Georgios B. Giannakis. Node Embedding with Adaptive Similarities for Scalable

Learning over Graphs. CoRR  1811.10797  2018.

[8] Dimitris Berberidis  Athanasios N. Nikolakopoulos  and Georgios B. Giannakis. Adaptive diffusions for

scalable learning over graphs. IEEE Transactions on Signal Processing  67(5):1307–1321  2019.

[9] Filippo Maria Bianchi  Daniele Grattarola  Lorenzo Livi  and Cesare Alippi. Graph Neural Networks with

convolutional ARMA ﬁlters. CoRR  1901.01343  2019.

[10] Aleksandar Bojchevski and Stephan Günnemann. Deep Gaussian Embedding of Graphs: Unsupervised

Inductive Learning via Ranking. ICLR  2018.

[11] Joan Bruna  Wojciech Zaremba  Arthur Szlam  and Yann LeCun. Spectral Networks and Deep Locally

Connected Networks on Graphs. In ICLR  2014.

[12] Eliav Buchnik and Edith Cohen. Bootstrapped Graph Diffusions: Exposing the Power of Nonlinearity.
Proceedings of the ACM on Measurement and Analysis of Computing Systems (POMACS)  2(1):1–19 
2018.

[13] Siheng Chen  Aliaksei Sandryhaila  Jose M. F. Moura  and Jelena Kovacevic. Adaptive graph ﬁltering:
Multiresolution classiﬁcation on graphs. In IEEE Global Conference on Signal and Information Processing
(GlobalSIP)  2013.

[14] F. Chung. The heat kernel as the pagerank of a graph. Proceedings of the National Academy of Sciences 

104(50):19735–19740  2007.

[15] Aurelien Decelle  Florent Krzakala  Cristopher Moore  and Lenka Zdeborová.

Inference and phase
transitions in the detection of modules in sparse networks. Physical Review Letters  107(6):065701  2011.

[16] Michaël Defferrard  Xavier Bresson  and Pierre Vandergheynst. Convolutional Neural Networks on Graphs

with Fast Localized Spectral Filtering. In NIPS  2016.

[17] Tyler Derr  Yao Ma  and Jiliang Tang. Signed Graph Convolutional Networks. In ICDM  2018.

[18] Paul D. Dobson and Andrew J. Doig. Distinguishing enzyme structures from non-enzymes without

alignments. Journal of Molecular Biology  330(4):771–783  2003.

[19] David K. Duvenaud  Dougal Maclaurin  Jorge Aguilera-Iparraguirre  Rafael Gómez-Bombarelli  Timothy
Hirzel  Alán Aspuru-Guzik  and Ryan P. Adams. Convolutional Networks on Graphs for Learning
Molecular Fingerprints. In NIPS  2015.

[20] Radim ˇReh˚uˇrek and Petr Sojka. Software Framework for Topic Modelling with Large Corpora. In LREC

2010 Workshop on New Challenges for NLP Frameworks  2010.

[21] Matthias Fey and Jan E. Lenssen. Fast Graph Representation Learning with PyTorch Geometric. In ICLR

workshop  2019.

[22] François Fouss  Kevin Francoisse  Luh Yen  Alain Pirotte  and Marco Saerens. An experimental investi-
gation of kernels on graphs for collaborative recommendation and semisupervised classiﬁcation. Neural
Networks  31:53–72  2012.

10

[23] Justin Gilmer  Samuel S. Schoenholz  Patrick F. Riley  Oriol Vinyals  and George E. Dahl. Neural Message

Passing for Quantum Chemistry. In ICML  2017.

[24] M. Gori  G. Monfardini  and F. Scarselli. A new model for learning in graph domains. In IEEE International

Joint Conference on Neural Networks  2005.

[25] Aditya Grover and Jure Leskovec. node2vec: Scalable Feature Learning for Networks. In KDD  2016.

[26] William L. Hamilton  Zhitao Ying  and Jure Leskovec. Inductive Representation Learning on Large Graphs.

In NIPS  2017.

[27] David K. Hammond  Pierre Vandergheynst  and Rémi Gribonval. Wavelets on graphs via spectral graph

theory. Applied and Computational Harmonic Analysis  30(2):129–150  2011.

[28] Mikael Henaff  Joan Bruna  and Yann LeCun. Deep Convolutional Networks on Graph-Structured Data.

CoRR  1506.05163  2015.

[29] Eric Jones  Travis Oliphant  Pearu Peterson  and others. SciPy: Open source scientiﬁc tools for Python.

2001.

[30] Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in networks.

Physical review E  83(1):016107  2011.

[31] Thomas N. Kipf and Max Welling. Variational Graph Auto-Encoders. In NIPS workshop  2016.

[32] Thomas N. Kipf and Max Welling. Semi-Supervised Classiﬁcation with Graph Convolutional Networks.

In ICLR  2017.

[33] Johannes Klicpera  Aleksandar Bojchevski  and Stephan Günnemann. Predict then Propagate: Graph

Neural Networks Meet Personalized PageRank. In ICLR  2019.

[34] Kyle Kloster and David F Gleich. Heat kernel based community detection. In KDD  2014.

[35] Isabel M. Kloumann  Johan Ugander  and Jon Kleinberg. Block models and personalized PageRank.

Proceedings of the National Academy of Sciences  114(1):33–38  2017.

[36] Risi Imre Kondor and John Lafferty. Diffusion kernels on graphs and other discrete structures. In ICML 

2002.

[37] Stéphane Lafon and Ann B. Lee. Diffusion Maps and Coarse-Graining: A Uniﬁed Framework for
Dimensionality Reduction  Graph Partitioning  and Data Set Parameterization. IEEE Trans. Pattern Anal.
Mach. Intell.  28(9):1393–1403  2006.

[38] Edmund Landau. Zur relativen Wertbemessung der Turnierresultate. Deutsches Wochenschach  11:

366–369  1895.

[39] Jure Leskovec  Jon Kleinberg  and Christos Faloutsos. Graphs over Time: Densiﬁcation Laws  Shrinking

Diameters and Possible Explanations. In KDD  2005.

[40] Pan Li  Eli Chien  and Olgica Milenkovic. Optimizing generalized pagerank methods for seed-expansion

community detection. In NeurIPS  2019.

[41] Ruoyu Li  Sheng Wang  Feiyun Zhu  and Junzhou Huang. Adaptive Graph Convolutional Neural Networks.

In AAAI  2018.

[42] Yaguang Li  Rose Yu  Cyrus Shahabi  and Yan Liu. Diffusion Convolutional Recurrent Neural Network:

Data-Driven Trafﬁc Forecasting. In ICLR  2018.

[43] Yujia Li  Daniel Tarlow  Marc Brockschmidt  and Richard S. Zemel. Gated Graph Sequence Neural

Networks. In ICLR  2016.

[44] Jeremy Ma  Weiyu Huang  Santiago Segarra  and Alejandro Ribeiro. Diffusion ﬁltering of graph signals

and its use in recommendation systems. In ICASSP  2016.

[45] Zheng Ma  Ming Li  and Yuguang Wang. PAN: Path Integral Based Convolution for Deep Graph Neural

Networks. In ICML workshop  2019.

[46] Naoki Masuda  Mason A Porter  and Renaud Lambiotte. Random walks and diffusion on networks. Physics

reports  716:1–58  2017.

11

[47] Julian J. McAuley  Christopher Targett  Qinfeng Shi  and Anton van den Hengel. Image-Based Recom-

mendations on Styles and Substitutes. In SIGIR  2015.

[48] Andrew Kachites McCallum  Kamal Nigam  Jason Rennie  and Kristie Seymore. Automating the construc-

tion of internet portals with machine learning. Information Retrieval  3(2):127–163  2000.

[49] Miller McPherson  Lynn Smith-Lovin  and James M Cook. Birds of a feather: Homophily in social

networks. Annual review of sociology  27(1):415–444  2001.

[50] Jörg Menche  Amitabh Sharma  Maksim Kitsak  Susan Ghiassian  Marc Vidal  Joseph Loscalzo  and Albert-
László Barabási. Uncovering disease-disease relationships through the incomplete human interactome.
Science  347(6224):1257601  2015.

[51] Tomas Mikolov  Ilya Sutskever  Kai Chen  Gregory S. Corrado  and Jeffrey Dean. Distributed Representa-

tions of Words and Phrases and their Compositionality. In NIPS  2013.

[52] Federico Monti  Davide Boscaini  Jonathan Masci  Emanuele Rodola  Jan Svoboda  and Michael M.
Bronstein. Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs. In CVPR 
2017.

[53] Galileo Namata  Ben London  Lise Getoor  and Bert Huang. Query-driven Active Surveying for Collective

Classiﬁcation. In International Workshop on Mining and Learning with Graphs (MLG)  KDD  2012.

[54] Huda Nassar  Kyle Kloster  and David F. Gleich. Strong Localization in Personalized PageRank Vectors.

In International Workshop on Algorithms and Models for the Web Graph (WAW)  2015.

[55] Andrew Y Ng  Michael I Jordan  and Yair Weiss. On Spectral Clustering: Analysis and an algorithm. In

NIPS  2002.

[56] Mathias Niepert  Mohamed Ahmed  and Konstantin Kutzkov. Learning Convolutional Neural Networks

for Graphs. In ICML  2016.

[57] Lawrence Page  Sergey Brin  Rajeev Motwani  and Terry Winograd. The pagerank citation ranking:

Bringing order to the web. Report  Stanford InfoLab  1998.

[58] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito  Zeming
Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in PyTorch. In NIPS
workshop  2017.

[59] Fabian Pedregosa  Gaël Varoquaux  Alexandre Gramfort  Vincent Michel  Bertrand Thirion  Olivier Grisel 
Mathieu Blondel  Peter Prettenhofer  Ron Weiss  Vincent Dubourg  Jake Vanderplas  Alexandre Passos 
David Cournapeau  Matthieu Brucher  Matthieu Perrot  and Édouard Duchesnay. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research  12:2825–2830  2011.

[60] Tiago P. Peixoto. The graph-tool python library. ﬁgshare  2014.

[61] Bryan Perozzi  Rami Al-Rfou  and Steven Skiena. DeepWalk: online learning of social representations. In

KDD  2014.

[62] Trang Pham  Truyen Tran  Dinh Q. Phung  and Svetha Venkatesh. Column Networks for Collective

Classiﬁcation. In AAAI  2017.

[63] Jiezhong Qiu  Yuxiao Dong  Hao Ma  Jian Li  Kuansan Wang  and Jie Tang. Network Embedding as

Matrix Factorization: Unifying DeepWalk  LINE  PTE  and node2vec. In WSDM  2018.

[64] Stephen Ragain. Community Detection via Discriminant functions for Random Walks in the degree-

corrected Stochastic Block Model. Report  Stanford University  2017.

[65] F. Scarselli  M. Gori  Ah Chung Tsoi  M. Hagenbuchner  and G. Monfardini. The Graph Neural Network

Model. IEEE Transactions on Neural Networks  20(1):61–80  2009.

[66] Prithviraj Sen  Galileo Namata  Mustafa Bilgic  Lise Getoor  Brian Gallagher  and Tina Eliassi-Rad.

Collective Classiﬁcation in Network Data. AI Magazine  29(3):93–106  2008.

[67] Oleksandr Shchur  Maximilian Mumme  Aleksandar Bojchevski  and Stephan Günnemann. Pitfalls of

Graph Neural Network Evaluation. In NIPS workshop  2018.

[68] A. Sperduti and A. Starita. Supervised neural networks for the classiﬁcation of structures. IEEE Transac-

tions on Neural Networks  8(3):714–735  1997.

12

[69] Gilbert Wright Stewart and Ji-guang Sun. Matrix Perturbation Theory. Computer Science and Scientiﬁc

Computing. 1990.

[70] Yu-Hang Tang  Dongkun Zhang  and George Em Karniadakis. An atomistic ﬁngerprint algorithm for

learning ab initio molecular force ﬁelds. The Journal of Chemical Physics  148(3):034101  2018.

[71] Anton Tsitsulin  Davide Mottin  Panagiotis Karras  and Emmanuel Müller. VERSE: Versatile Graph

Embeddings from Similarity Measures. In WWW  2018.

[72] Stefan Van Der Walt  S Chris Colbert  and Gael Varoquaux. The NumPy array: a structure for efﬁcient

numerical computation. Computing in Science & Engineering  13(2):22  2011.

[73] Petar Velickovic  William Fedus  William L. Hamilton  Pietro Liò  Yoshua Bengio  and R. Devon Hjelm.

Deep Graph Infomax. In ICLR  2019.

[74] Petar Veliˇckovi´c  Guillem Cucurull  Arantxa Casanova  Adriana Romero  Pietro Liò  and Yoshua Bengio.

Graph Attention Networks. In ICLR  2018.

[75] Sebastiano Vigna. Spectral ranking. Network Science  CoRR (updated  0912.0238v15)  4(4):433–445 

2016.

[76] Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing  17(4):395–416  2007.

[77] Zhewei Wei  Xiaodong He  Xiaokui Xiao  Sibo Wang  Shuo Shang  and Ji-Rong Wen. TopPPR: Top-k

Personalized PageRank Queries with Precision Guarantees on Large Graphs. In SIGMOD  2018.

[78] Felix Wu  Tianyi Zhang  Amauri Holanda de Souza Jr.  Christopher Fifty  Tao Yu  and Kilian Q. Weinberger.

Simplifying Graph Convolutional Networks. In ICML  2019.

[79] Bingbing Xu  Huawei Shen  Qi Cao  Keting Cen  and Xueqi Cheng. Graph Convolutional Networks using

Heat Kernel for Semi-supervised Learning. In IJCAI  2019.

[80] Keyulu Xu  Chengtao Li  Yonglong Tian  Tomohiro Sonobe  Ken-ichi Kawarabayashi  and Stefanie Jegelka.

Representation Learning on Graphs with Jumping Knowledge Networks. In ICML  2018.

[81] Keyulu Xu  Weihua Hu  Jure Leskovec  and Stefanie Jegelka. How Powerful are Graph Neural Networks?

In ICLR  2019.

[82] Rex Ying  Ruining He  Kaifeng Chen  Pong Eksombatchai  William L. Hamilton  and Jure Leskovec.

Graph Convolutional Neural Networks for Web-Scale Recommender Systems. KDD  2018.

13

,Johannes Klicpera
Stefan Weißenberger
Stephan Günnemann