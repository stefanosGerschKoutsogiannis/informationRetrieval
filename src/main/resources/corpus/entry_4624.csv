2013,Reflection methods for user-friendly submodular optimization,Recently  it has become evident that submodularity naturally captures widely occurring concepts in machine learning  signal processing and computer vision. In consequence  there is need for efficient optimization procedures for submodular functions  in particular for minimization problems. While general submodular minimization is challenging  we propose a new approach that exploits existing decomposability of submodular functions. In contrast to previous approaches  our method is neither approximate  nor impractical  nor does it need any cumbersome parameter tuning. Moreover  it is easy to implement and parallelize. A key component of our approach is a formulation of the discrete submodular minimization problem as a continuous best approximation problem. It is solved through a sequence of reflections and its solution can be  automatically thresholded to obtain an optimal discrete solution. Our method solves both the continuous and discrete formulations of the problem  and therefore has applications in learning  inference  and reconstruction. In our experiments  we show the benefits of our new algorithms for two image segmentation tasks.,Reﬂection methods for user-friendly

submodular optimization

Stefanie Jegelka

UC Berkeley

Berkeley  CA  USA

Francis Bach
INRIA - ENS
Paris  France

Abstract

Suvrit Sra

MPI for Intelligent Systems

T¨ubingen  Germany

Recently  it has become evident that submodularity naturally captures widely
occurring concepts in machine learning  signal processing and computer vision.
Consequently  there is need for efﬁcient optimization procedures for submodu-
lar functions  especially for minimization problems. While general submodular
minimization is challenging  we propose a new method that exploits existing de-
composability of submodular functions. In contrast to previous approaches  our
method is neither approximate  nor impractical  nor does it need any cumbersome
parameter tuning. Moreover  it is easy to implement and parallelize. A key com-
ponent of our method is a formulation of the discrete submodular minimization
problem as a continuous best approximation problem that is solved through a
sequence of reﬂections  and its solution can be easily thresholded to obtain an
optimal discrete solution. This method solves both the continuous and discrete
formulations of the problem  and therefore has applications in learning  inference 
and reconstruction. In our experiments  we illustrate the beneﬁts of our method on
two image segmentation tasks.

1

Introduction

Submodularity is a rich combinatorial concept that expresses widely occurring phenomena such as
diminishing marginal costs and preferences for grouping. A set function F : 2V → R on a set V is
submodular if for all subsets S  T ⊆ V   we have F (S ∪ T ) + F (S ∩ T ) ≤ F (S) + F (T ).
Submodular functions underlie the goals of numerous problems in machine learning  computer vision
and signal processing [1]. Several problems in these areas can be phrased as submodular optimization
tasks: notable examples include graph cut-based image segmentation [7]  sensor placement [30]  or
document summarization [31]. A longer list of examples may be found in [1].
The theoretical complexity of submodular optimization is well-understood: unconstrained mini-
mization of submodular set functions is polynomial-time [19] while submodular maximization is
NP-hard. Algorithmically  however  the picture is different. Generic submodular maximization admits
efﬁcient algorithms that can attain approximate optima with global guarantees; these algorithms are
typically based on local search techniques [16  35]. In contrast  although polynomial-time solvable 
submodular function minimization (SFM) which seeks to solve

min
S⊆V

F (S) 

(1)

poses substantial algorithmic difﬁculties. This is partly due to the fact that one is commonly interested
in an exact solution (or an arbitrarily close approximation thereof)  and “polynomial-time” is not
necessarily equivalent to “practically fast”.
Submodular minimization algorithms may be obtained from two main perspectives: combinatorial
and continuous. Combinatorial algorithms for SFM typically use close connections to matroid and

1

maximum ﬂow methods; the currently theoretically fastest combinatorial algorithm for SFM scales
as O(n6 + n5τ )  where τ is the time to evaluate the function oracle [37] (for an overview of other
algorithms  see e.g.  [33]). These combinatorial algorithms are typically nontrivial to implement.
Continuous methods offer an alternative by instead minimizing a convex extension. This idea exploits
the fundamental connection between a submodular function F and its Lov´asz extension f [32]  which
is continuous and convex. The SFM problem (1) is then equivalent to

min
x∈[0 1]n

f (x).

(2)

The Lov´asz extension f is nonsmooth  so we might have to resort to subgradient methods. While
a fundamental result of Edmonds [15] demonstrates that a subgradient of f can be computed in
√
O(n log n) time  subgradient methods can be sensitive to choices of the step size  and can be slow.
They theoreticaly converge at a rate of O(1/
t) (after t iterations). The “smoothing technique” of
[36] does not in general apply here because computing a smoothed gradient is equivalent to solving
the submodular minimization problem. We discuss this issue further in Section 2.
An alternative to minimizing the Lov´asz extension directly on [0  1]n is to consider a slightly modiﬁed
convex problem. Speciﬁcally  the exact solution of the discrete problem minS⊆V F (S) and of its
(cid:62) 0} of
nonsmooth convex relaxation minx∈[0 1]n f (x) may be found as a level set S0 = {k | x∗
the unique point x∗ that minimizes the strongly convex function [1  10]:

k

2(cid:107)x(cid:107)2.

f (x) + 1

Following [28  29  38  41]  we make the assumption that F (S) =(cid:80)r

(3)
We will refer to the minimization of (3) as the proximal problem due to its close similarity to proximity
operators used in convex optimization [12]. When F is a cut function  (3) becomes a total variation
problem (see  e.g.  [9] and references therein) that also occurs in other regularization problems [1].
2(cid:107)x(cid:107)2; (ii) the
Two noteworthy points about (3) are: (i) addition of the strongly convex component 1
ensuing removal of the box-constraints x ∈ [0  1]n. These changes allow us to consider a convex
dual which is amenable to smooth optimization techniques.
Typical approaches to generic SFM include Frank-Wolfe methods [17] that have cheap iterations
and O(1/t) convergence  but can be quite slow in practice (Section 5); or the minimum-norm-
point/Fujishige-Wolfe algorithm [20] that has expensive iterations but ﬁnite convergence. Other
recent methods are approximate [24]. In contrast to several iterative methods based on convex
relaxations  we seek to obtain exact discrete solutions.
To the best of our knowledge  all generic algorithms that use only submodularity are several orders
of magnitude slower than specialized algorithms when they exist (e.g.  for graph cuts). However 
the submodular function is not always generic and given via a black-box  but has known structure.
i=1 Fi(S) is a sum of sufﬁciently
“simple” functions (see Sec. 3). This structure allows the use of (parallelizable) dual decomposition
techniques for the problem in Eq. (2)  with [11  38] or without [29] Nesterov’s smoothing technique 
or with direct smoothing [41] techniques. But existing approaches typically have two drawbacks: (1)
they use smoothing or step-size parameters whose selection may be critical and quite tedious; and (2)
they still exhibit slow convergence (see Section 5).
These drawbacks arise from working with formulation (2). Our main insight is that  despite seemingly
counter-intuitive  the proximal problem (3) offers a much more user-friendly tool for solving (1)
than its natural convex counterpart (2)  both in implementation and running time. We approach
problem (3) via its dual. This allows decomposition techniques which combine well with orthogonal
projection and reﬂection methods that (a) exhibit faster convergence  (b) are easily parallelizable  (c)
require no extra hyperparameters  and (d) are extremely easy to implement.
The main three algorithms that we consider are: (i) dual block-coordinate descent (equivalently 
primal-dual proximal-Dykstra)  which was already shown to be extremely efﬁcient for total variation
problems [2] that are special cases of Problem (3); (ii) Douglas-Rachford splitting using the careful
variant of [4]  which for our formulation (Section 4.2) requires no hyper-parameters; and (iii)
accelerated projected gradient [5]. We will see these alternative algorithms can offer speedups beyond
known efﬁciencies. Our observations have two implications: ﬁrst  from the viewpoint of solving
Problem (3)  they offers speedups for often occurring denoising and reconstruction problems that
employ total variation. Second  our experiments suggest that projection and reﬂection methods can
work very well for solving the combinatorial problem (1).

2

In summary  we make the following contributions: (1) In Section 3  we cast the problem of minimizing
decomposable submodular functions as an orthogonal projection problem and show how existing
optimization techniques may be brought to bear on this problem  to obtain fast  easy-to-code and
easily parallelizable algorithms. In addition  we show examples of classes of functions amenable
to our approach. In particular  for simple functions  i.e.  those for which minimizing F (S) − a(S)
is easy for all vectors1 a ∈ Rn  the problem in Eq. (3) may be solved in O(log 1
ε ) calls to such
minimization routines  to reach a precision ε (Section 2 3). (2) In Section 5  we demonstrate the
empirical gains of using accelerated proximal methods  Douglas-Rachford and block coordinate
descent methods over existing approaches: fewer hyperparameters and faster convergence.

2 Review of relevant results from submodular analysis
The relevant concepts we review here are the Lov´asz extension  base polytopes of submodular
functions  and relationships between proximal and discrete problems. For more details  see [1  19].

k=1 xσ(k)

Lov´asz extension and convexity. The power set 2V may be naturally identiﬁed with the ver-
tices of the hypercube  i.e.  {0  1}n. The Lov´asz extension f of any set function is deﬁned
by linear interpolation  so that for any S ⊂ V   F (S) = f (1S).
It may be computed in
if xσ(1) (cid:62) ··· (cid:62) xσ(n)  then f (x) =
closed form once the components of x are sorted:

(cid:2)F ({σ(1)  . . .   σ(k)}) − F ({σ(1)  . . .   σ(k − 1)})(cid:3) [32]. For the graph cut function  f

(cid:80)n

is the total variation.
In this paper  we are going to use two important results: (a) if the set function F is submodular  then
its Lov´asz extension f is convex  and (b) minimizing the set function F is equivalent to minimizing
f (x) with respect to x ∈ [0  1]n. Given x ∈ [0  1]n  all of its level sets may be considered and the
function may be evaluated (at most n times) to obtain a set S. Moreover  for a submodular function 
the Lov´asz extension happens to be the support function of the base polytope B(F ) deﬁned as

B(F ) = {y ∈ Rn | ∀S ⊂ V  y(S) (cid:54) F (S) and y(V ) = F (V )} 

that is f (x) = maxy∈B(F ) y(cid:62)x [15]. A maximizer of y(cid:62)x (and hence the value of f (x))  may be
computed by the “greedy algorithm”  which ﬁrst sorts the components of w in decreasing order
xσ(1) (cid:62) ··· (cid:62) xσ(n)  and then compute yσ(k) = F ({σ(1)  . . .   σ(k)}) − F ({σ(1)  . . .   σ(k − 1)}).
In other words  a linear function can be maximized over B(F ) in time O(n log n + nτ ) (note that
the term nτ may be improved in many special cases). This is crucial for exploiting convex duality.
Dual of discrete problem. We may derive a dual problem to the discrete problem in Eq. (1) and
the convex nonsmooth problem in Eq. (2)  as follows:

max
y∈B(F )

y(cid:62)x = max
y∈B(F )

min
x∈[0 1]n

y(cid:62)x = max
y∈B(F )

(y)−(V ) 

µ as x∗

f (x) = min
x∈[0 1]n

F (S) = min
x∈[0 1]n

k = sup{µ ∈ R | k ∈ S∗

(4)
min
S⊆V
where (y)− = min{y  0} applied elementwise. This allows to obtain dual certiﬁcates of optimality
from any y ∈ B(F ) and x ∈ [0  1]n.
2(cid:107)x(cid:107)2  has intricate
Proximal problem. The optimization problem (3)  i.e.  minx∈Rn f (x) + 1
relations to the SFM problem [10]. Given the unique optimal solution x∗ of (3)  the maximal (resp.
minimal) optimizer of the SFM problem is the set S∗ of nonnegative (resp. positive) elements of x∗.
More precisely  solving (3) is equivalent to minimizing F (S) + µ|S| for all µ ∈ R. A solution
Sµ ⊆ V is obtained from a solution x∗ as S∗
(cid:62) µ}. Conversely  x∗ may be obtained
µ} for all k ∈ V . Moreover  if x is an ε-optimal solution
from all S∗
of Eq. (3)  then we may construct
εn-optimal solutions for all Sµ [1; Prop. 10.5]. In practice  the
duality gap of the discrete problem is usually much lower than that of the proximal version of the
same problem  as we will see in Section 5. Note that the problem in Eq. (3) provides much more
information than Eq. (2)  as all µ-parameterized discrete problems are solved.
The dual problem of Problem (3) reads as follows:
2(cid:107)x(cid:107)2
− 1
2(cid:107)y(cid:107)2
2 
min
x∈Rn
where primal and dual variables are linked as x = −y. Observe that this dual problem is equivalent
to ﬁnding the orthogonal projection of 0 onto B(F ).

2(cid:107)x(cid:107)2
1Every vector a ∈ Rn may be viewed as a modular (linear) set function: a(S) (cid:44)(cid:80)

µ = {i | x∗

2 = max
y∈B(F )

2 = max
y∈B(F )

min
x∈Rn

2 = min
x∈Rn

max
y∈B(F )

f (x) + 1

2(cid:107)x(cid:107)2

y(cid:62)x + 1

y(cid:62)x + 1

√

i

i∈S a(i).

3

Divide-and-conquer strategies for the proximal problems. Given a solution x∗ of the proximal
problem  we have seen how to get S∗
µ for any µ by simply thresholding x∗ at µ. Conversely  one can
recover x∗ exactly from at most n well-chosen values of µ. A known divide-and-conquer strategy
[19  21] hinges upon the fact that for any µ  one can easily see which components of x∗ are greater
or smaller than µ by computing S∗
µ. The resulting algorithm makes O(n) calls to the submodular
function oracle. In [25]  we extend an alternative approach by Tarjan et al. [42] for cuts to general
submodular functions and obtain a solution to (3) up to precision ε in O(min{n  log 1
ε}) iterations.
This result is particularly useful if our function F is a sum of functions for each of which by itself
the SFM problem is easy. Beyond squared (cid:96)2-norms  our algorithm equally applies to computing all
j=1 hj(xj) for arbitrary smooth strictly convex functions hj  j = 1  . . .   n.

minimizers of f (x) +(cid:80)p
(cid:80)r

(cid:88)r

(cid:88)r

(cid:88)r
2(cid:107)x− z(cid:107)2
2 subject to y ∈ B(Fj).

j=1

3 Decomposition of submodular functions
Following [28  29  38  41]  we assume that our function F may be decomposed as the sum F (S) =
j=1 Fj(S) of r “simple” functions. In this paper  by “simple” we mean functions G for which
G(S) − a(S) can be minimized efﬁciently for all vectors a ∈ Rn (more precisely  we require that
S (cid:55)→ G(S ∪ T )− a(S) can be minimized efﬁciently over all subsets of V \ T   for any T ⊆ V and a).
Efﬁciency may arise from the functional form of G  or from the fact that G has small support. For
such functions  Problems (1) and (3) become

2(cid:107)x(cid:107)2
2.

(5)

fj(x) + 1

j=1

j=1

fj(x)

min
S⊆V

2(cid:107)y − z(cid:107)2

Fj(S) = min
x∈[0 1]n

2 + fj(x)  or equivalently 

min
x∈Rn
The key to the algorithms presented here is to be able to minimize 1
to orthogonally project z onto B(Fj): min 1
We next sketch some examples of functions F and their decompositions into simple functions Fj. As
shown at the end of Section 2  projecting onto B(Fj) is easy as soon as the corresponding submodular
minimization problems are easy. Here we outline some cases for which specialized fast algorithms
are known.
Graph cuts. A widely used class of submodular functions are graph cuts. Graphs may be decom-
posed into substructures such as trees  simple paths or single edges. Message passing algorithms
apply to trees  while the proximal problem for paths is very efﬁciently solved by [2]. For single edges 
it is solvable in closed form. Tree decompositions are common in graphical models  whereas path
decompositions are frequently used for TV problems [2].
Concave functions. Another important class of submodular functions is that of concave functions of
cardinality  i.e.  Fj(S) = h(|S|) for a concave function h. Problem (3) for such functions may be
solved in O(n log n) time (see [18] and our appendix in [25]). Functions of this class have been used
in [24  27  41]. Such functions also include covering functions [41].
Hierarchical functions. Here  the ground set corresponds to the leaves of a rooted  undirected tree.
Each node has a weight  and the cost of a set of nodes S ⊆ V is the sum of the weights of all nodes
in the smallest subtree (including the root) that spans S. This class of functions too admits to solve
the proximal problem in O(n log n) time [22  23  26].
Small support. Any general  potentially slower algorithm such as the minimum-norm-point algo-
rithm can be applied if the support of each Fj is only a small subset of the ground set.

We ﬁrst review existing dual decomposition techniques for the nonsmooth problem (1). We always
Rn (cid:39) Rn×r. We follow [29] to derive a dual

3.1 Dual decomposition of the nonsmooth problem

assume that F =(cid:80)r
j=1 Fj  and deﬁne Hr :=(cid:81)r
(cid:88)r

j=1

formulation (see appendix in [25]):
Lemma 1. The dual of Problem (1) may be written in terms of variables λ1  . . .   λr ∈ Rn as

s.t. λ ∈(cid:8)(λ1  . . .   λr) ∈ Hr |(cid:88)r

gj(λj)

max

(6)

j=1

where gj(λj) = minS⊂V Fj(S) − λj(S) is a nonsmooth concave function.
The dual is the maximization of a nonsmooth concave function over a convex set  onto which it is
easy to project: the projection of a vector y has j-th block equal to yj − 1
k=1 yk. Moreover  in
our setup  functions gj and their subgradients may be computed efﬁciently through SFM.

r

j=1

λj = 0(cid:9)
(cid:80)r

4

t).

We consider several existing alternatives for the minimization of f (x) on x ∈ [0  1]n  most of which
use Lemma 1. Computing subgradients for any fj means calling the greedy algorithm  which runs in
time O(n log n). All of the following algorithms require the tuning of an appropriate step size.
Primal subgradient descent (primal-sgd): Agnostic to any decomposition properties  we may
√
apply a standard simple subgradient method to f. A subgradient of f may be obtained from the
subgradients of the components fj. This algorithm converges at rate O(1/
√
Dual subgradient descent (dual-sgd) [29]: Applying a subgradient method to the nonsmooth dual
in Lemma 1 leads to a convergence rate of O(1/
t). Computing a subgradient requires minimizing
the submodular functions Fj individually. In simulations  following [29]  we consider a step-size
rule similar to Polyak’s rule (dual-sgd-P) [6]  as well as a decaying step-size (dual-sgd-F)  and use
discrete optimization for all Fj.
Primal smoothing (primal-smooth) [41]: The nonsmooth primal may be smoothed in several ways
2(cid:107)yj(cid:107)2. This
j (xj) = maxyj∈B(Fj ) y(cid:62)
by smoothing the fj individually; one example is ˜f ε
leads to a function that is (1/ε)-smooth. Computing ˜f ε
j means solving the proximal problem for Fj.
The convergence rate is O(1/t)  but  apart from step size which may be set relatively easily  the
smoothing constant ε needs to be deﬁned.
Dual smoothing (dual-smooth): Instead of the primal  the dual (6) may be smoothed  e.g.  by
entropy [8  38] applied to each gj as ˜gε
j (λj) = minx∈[0 1]n fj(x) + εh(x) where h(x) is a negative
entropy. Again  the convergence rate is O(1/t) but there are two free parameters (in particular the
smoothing constant ε which is hard to tune). This method too requires solving proximal problems for
all Fj in each iteration.
Dual smoothing with entropy also admits coordinate descent methods [34] that exploit the decompo-
sition  but we do not compare to those here.

j xj − ε

3.2 Dual decomposition methods for proximal problems
We may also consider Eq. (3) and ﬁrst derive a dual problem using the same technique as in
Section 3.1. Lemma 2 (proved in the appendix in [25]) formally presents our dual formulation as a

best approximation problem. The primal variable can be recovered as x = −(cid:80)
y ∈(cid:89)r

Lemma 2. The dual of Eq. (3) may be written as the best approximation problem
min
λ y

λj = 0(cid:9) 

(cid:107)y − λ(cid:107)2

j yj.

j=1

j=1

2

s.t. λ ∈(cid:8)(λ1  . . .   λr) ∈ Hr |(cid:88)r
(cid:13)(cid:13)(cid:13)(cid:88)r

(cid:13)(cid:13)(cid:13)2

yj

− 1
2

j=1

2

max

y

B(Fj). (7)

(8)

We can actually eliminate the λj and obtain the simpler looking dual problem

s.t. yj ∈ B(Fj)  j ∈ {1  . . .   r}

Such a dual was also used in [40]. In Section 5  we will see the effect of solving one of these duals or
the other. For the simpler dual (8) the case r = 2 is of special interest; it reads

max

y1∈B(F1)  y2∈B(F2)

− 1
2

(cid:107)y1 + y2(cid:107)2

2 ⇐⇒

y1∈B(F1) −y2∈−B(F2)

min

(cid:107)y1 − (−y2)(cid:107)2.

(9)

We write problem (9) in this suggestive form to highlight its key geometric structure: it is  like (7) 
a best approximation problem: i.e.  the problem of ﬁnding the closest point between the polytopes
B(F1) and −B(F2). Notice  however  that (7) is very different from (9)—the former operates in a
product space while the latter does not  a difference that can have impact in practice (see Section 5).
We are now ready to present algorithms that exploit our dual formulations.
4 Algorithms
We describe a few competing methods for solving our smooth dual formulations. We describe the
details for the special 2-block case (9); the same arguments apply to the block dual from Lemma 2.

4.1 Block coordinate descent or proximal-Dykstra
Perhaps the simplest approach to solving (9) (viewed as a minimization problem) is to use a block
coordinate descent (BCD) procedure  which in this case performs the alternating projections:
)(cid:107)2. (10)

2 ← argminy2∈B(F2) (cid:107)y2 − (−yk+1
yk+1

1 ← argminy1∈B(F1) (cid:107)y1 − (−yk
yk+1

2 )(cid:107)2
2;

1

5

The iterations for solving (8) are analogous. This BCD method (applied to (9)) is equivalent to
applying the so-called proximal-Dykstra method [12] to the primal problem. This may be seen by
comparing the iterates. Notice that the BCD iteration (10) is nothing but alternating projections onto
the convex polyhedra B(F1) and B(F2). There exists a large body of literature studying method of
alternating projections—we refer the interested reader to the monograph [13] for further details.
However  despite its attractive simplicity  it is known that BCD (in its alternating projections form) 
can converge arbitrarily slowly [4] depending on the relative orientation of the convex sets onto which
one projects. Thus  we turn to a potentially more effective method.

4.2 Douglas-Rachford splitting

The Douglas-Rachford (DR) splitting method [14] includes algorithms like ADMM as a special
case [12].
It avoids the slowdowns alluded to above by replacing alternating projections with
alternating “reﬂections”. Formally  DR applies to convex problems of the form [3  12]

(11)
subject to the qualiﬁcation ri(dom φ1) ∩ ri(dom φ2) (cid:54)= ∅. To solve (11)  DR starts with some z0 
and performs the three-step iteration (for k ≥ 0):

φ1(x) + φ2(x) 

minx

1. xk = proxφ2(zk);

where γk ∈ [0  2] is a sequence of scalars that satisfy(cid:80)

2. vk = proxφ1 (2xk − zk);

3. zk+1 = zk + γk(vk − zk) 

(12)
k γk(2 − γk) = ∞. The sequence {xk}

produced by iteration (12) can be shown to converge to a solution of (11) [3; Thm. 25.6].
Introducing the reﬂection operator

Rφ := 2 proxφ − I 

and setting γk = 1  the DR iteration (12) may be written in a more symmetric form as

xk = proxφ2 (zk) 

zk+1 = 1

2 [Rφ1Rφ2 + I]zk 

k ≥ 0.

(13)

Applying DR to the duals (7) or (9)  requires ﬁrst putting them in the form (11)  either by introducing
extra variables or by going back to the primal  which is unnecessary. This is where the special
structure of our dual problem proves crucial  a recognition that is subtle yet remarkably important.
Instead of applying DR to (9)  consider the closely related problem

δ1(y) + δ−

miny

2 (y) 

(14)
2 are indicator functions for B(F1) and −B(F2)  respectively. Applying DR directly
where δ1  δ−
to (14) does not work because usually ri(dom δ1) ∩ ri(dom δ2) = ∅. Indeed  applying DR to (14)
generates iterates that diverge to inﬁnity [4; Thm. 3.13(ii)]. Fortunately  even though the DR iterates
for (14) may diverge  Bauschke et al. [4] show how to extract convergent sequences from these
iterates  which actually solve the corresponding best approximation problem; for us this is nothing
but the dual (9) that we wanted to solve in the ﬁrst place. Theorem 3  which is a simpliﬁed version
of [4; Thm. 3.13]  formalizes the above discussion.
Theorem 3. [4] Let A and B be nonempty polyhedral convex sets. Let ΠA (ΠB) denote orthogonal
projection onto A (B)  and let RA := 2ΠA − I (similarly RB) be the corresponding reﬂection
operator. Let {zk} be the sequence generated by the DR method (13) applied to (14). If A ∩ B (cid:54)= ∅ 
then {zk}k≥0 converges weakly to a ﬁxed-point of the operator T := 1
2 [RARB + I]; otherwise
(cid:107)zk(cid:107)2 → ∞. The sequences {xk} and {ΠAΠBzk} are bounded; the weak cluster points of either of
the two sequences

{(ΠARBzk  xk)}k≥0

{(ΠAxk  xk)}k≥0 

(15)

are solutions best approximation problem mina b (cid:107)a − b(cid:107) such that a ∈ A and b ∈ B.
The key consequence of Theorem 3 is that we can apply DR with impunity to (14)  and extract from
its iterates the optimal solution to problem (9) (from which recovering the primal is trivial). The most
important feature of solving the dual (9) in this way is that absolutely no stepsize tuning is required 
making the method very practical and user friendly.

6

pBCD  iter 1

pBCD  iter 7

DR  iter 1

DR  iter 4

smooth gap
discrete gap

νs = 8.05 · 104
νd = 5.9 · 10−1
Figure 1: Segmentation results for the slowest and fastest projection method  with smooth (νs) and discrete
(νd) duality gaps. Note how the background noise disappears only for small duality gaps.

νs = 4.17 · 105
νd = 6.6 · 103

νs = 3.4 · 106
νd = 4.6 · 103

νs = 4.4 · 105
νd = 5.5 · 102

5 Experiments
We empirically compare the proposed projection methods2 to the (smoothed) subgradient methods
discussed in Section 3.1. For solving the proximal problem  we apply block coordinate descent (BCD)
and Douglas-Rachford (DR) to Problem (8) if applicable  and also to (7) (BCD-para  DR-para). In
addition  we use acceleration to solve (8) or (9) [5]. The main iteration cost of all methods except
for the primal subgradient method is the orthogonal projection onto polytopes B(Fj). The primal
subgradient method uses the greedy algorithm in each iteration  which runs in O(n log n). However 
as we will see  its convergence is so slow to counteract any beneﬁt that may arise from not using
projections. We do not include Frank-Wolfe methods here  since FW is equivalent to a subgradient
descent on the primal and converges correspondingly slowly.
As benchmark problems  we use (i) graph cut problems for segmentation  or MAP inference in a
4-neighborhood grid-structured MRF  and (ii) concave functions similar to [41]  but together with
graph cut functions. The functions in (i) decompose as sums over vertical and horizontal paths. All
horizontal paths are independent and can be solved together in parallel  and similarly all vertical
paths. The functions in (ii) are constructed by extracting regions Rj via superpixels and  for each
Rj  deﬁning the function Fj(S) = |S||Rj \ S|. We use 200 and 500 regions. The problems
have size 640 × 427. Hence  for (i) we have r = 640 + 427 (but solve it as r = 2) and for (ii)
r = 640 + 427 + 500 (solved as r = 3). More details and experimental results may be found in [25].
Two functions (r = 2). Figure 2 shows the duality gaps for the discrete and smooth (where
applicable) problems for two instances of segmentation problems. The algorithms working with
the proximal problems are much faster than the ones directly solving the nonsmooth problem. In
particular DR converges extremely fast  faster even than BCD which is known to be a state-of-the-art
algorithms for this problem [2]. This  in itself  is a new insight for solving TV. If we aim for parallel
methods  then again DR outperforms BCD. Figure 3 (right) shows the speedup gained from parallel
processing. Using 8 cores  we obtain a 5-fold speed-up. We also see that the discrete gap shrinks
faster than the smooth gap  i.e.  the optimal discrete solution does not require to solve the smooth
problem to extremely high accuracy. Figure 1 illustrates example results for different gaps.
More functions (r > 2). Figure 3 shows example results for four problems of sums of concave and
cut functions. Here  we can only run DR-para. Overall  BCD  DR-para and the accelerated gradient
method perform very well.
In summary  our experiments suggest that projection methods can be extremely useful for solving
the combinatorial submodular minimization problem. Of the tested methods  DR  cyclic BCD and
accelerated gradient perform very well. For parallelism  applying DR on (9) converges much faster
than BCD on the same problem. Moreover  in terms of running times  running the DR method with a
mixed Matlab/C implementation until convergence on a single core is only 3-8 times slower than the
optimized efﬁcient C code of [7]  and only 2-4 times on 2 cores. These numbers should be read while
considering that  unlike [7]  the projection methods naturally lead to parallel implementations  and
are able to integrate a large variety of functions.

6 Conclusion
We have presented a novel approach to submodular function minimization based on the equivalence
with a best approximation problem. The use of reﬂection methods avoids any hyperparameters
and reduce the number of iterations signiﬁcantly  suggesting the suitability of reﬂection methods

2Code and data corresponding to this paper are available at https://sites.google.com/site/mloptstat/drsubmod

7

Figure 2: Comparison of convergence behaviors. Left: discrete duality gaps for various optimization
schemes for the nonsmooth problem  from 1 to 1000 iterations. Middle: discrete duality gaps for
various optimization schemes for the smooth problem  from 1 to 100 iterations. Right: corresponding
continuous duality gaps. From top to bottom: two different images.

Figure 3: Left two plots: convergence behavior for graph cut plus concave functions. Right: Speedup
due to parallel processing.

for combinatorial problems. Given the natural parallelization abilities of our approach  it would
be interesting to perform detailed empirical comparisons with existing parallel implementations of
graph cuts (e.g.  [39]). Moreover  a generalization beyond submodular functions of the relationships
between combinatorial optimization problems and convex problems would enable the application of
our framework to other common situations such as multiple labels (see  e.g.  [29]).
Acknowledgments. This research was in part funded by the Ofﬁce of Naval Research under contract/grant
number N00014-11-1-0688  by NSF CISE Expeditions award CCF-1139158  by DARPA XData Award FA8750-
12-2-0331  and the European Research Council (SIERRA project)  as well as gifts from Amazon Web Services 
Google  SAP  Blue Goji  Cisco  Clearstory Data  Cloudera  Ericsson  Facebook  General Electric  Hortonworks 
Intel  Microsoft  NetApp  Oracle  Samsung  Splunk  VMware and Yahoo!.
References
[1] F. Bach. Learning with submodular functions: A convex optimization perspective. Arxiv preprint

[2] A. Barbero and S. Sra. Fast Newton-type methods for total variation regularization. In ICML  2011.
[3] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.

arXiv:1111.6453v2  2013.

Springer  2011.

[4] H. H. Bauschke  P. L. Combettes  and D. R. Luke. Finding best approximation pairs relative to two closed

convex sets in Hilbert spaces. J. Approx. Theory  127(2):178–192  2004.

[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

8

2004006008001000−101234iterationlog10(duality gap)         discrete gaps − non−smooth problems − 1  dual−sgd−Pdual−sgd−Fdual−smoothprimal−smoothprimal−sgd20406080100−101234iterationlog10(duality gap)         discrete gaps − smooth problems− 1  grad−accelBCDDRBCD−paraDR−para20406080100−4−20246         iterationlog10(duality gap)smooth gaps − smooth problems − 1  grad−accelBCDDRBCD−paraDR−para2004006008001000−1012345iterationlog10(duality gap)         discrete gaps − non−smooth problems − 4  dual−sgd−Pdual−sgd−Fdual−smoothprimal−smoothprimal−sgd20406080100−1012345iterationlog10(duality gap)         discrete gaps − smooth problems− 4  grad−accelBCDDRBCD−paraDR−para20406080100−2−101234567         iterationlog10(duality gap)smooth gaps − smooth problems − 4  grad−accelBCDDRBCD−paraDR−para50100150200−3−2−10123456iterationlog10(duality gap)discrete gaps − 2  dual−sgd−PDR−paraBCDBCD−paragrad−accel20406080100−3−2−10123456iterationlog10(duality gap)discrete gaps − 3  dual−sgd−PDR−paraBCDBCD−paragrad−accel02468012345640 iterations of DR# coresspeedup factor[6] D. P. Bertsekas. Nonlinear programming. Athena Scientiﬁc  1999.
[7] Y. Boykov  O. Veksler  and R. Zabih. Fast approximate energy minimization via graph cuts. IEEE TPAMI 

[8] B.Savchynskyy  S.Schmidt  J.H.Kappes  and C.Schn¨orr. Efﬁcient MRF energy minimization via adaptive

[9] A. Chambolle. An algorithm for total variation minimization and applications. J Math. Imaging and Vision 

23(11):1222–1239  2001.

diminishing smoothing. In UAI  2012.

20(1):89–97  2004.

[10] A. Chambolle and J. Darbon. On total variation minimization and surface evolution using parametric

maximum ﬂows. Int. Journal of Comp. Vision  84(3):288–307  2009.

[11] F. Chudak and K. Nagano. Efﬁcient solutions to relaxations of combinatorial problems with submodular

penalties via the Lov´asz extension and non-smooth convex optimization. In SODA  2007.

[12] P. L. Combettes and J.-C. Pesquet. Proximal Splitting Methods in Signal Processing. In Fixed-Point

Algorithms for Inverse Problems in Science and Engineering  pages 185–212. Springer  2011.

[13] F. R. Deutsch. Best Approximation in Inner Product Spaces. Springer Verlag  ﬁrst edition  2001.
[14] J. Douglas and H. H. Rachford. On the numerical solution of the heat conduction problem in 2 and 3 space

variables. Tran. Amer. Math. Soc.  82:421–439  1956.

[15] J. Edmonds. Submodular functions  matroids  and certain polyhedra. In Combinatorial optimization -

Eureka  you shrink!  pages 11–26. Springer  2003.

[16] U. Feige  V. S. Mirrokni  and J. Vondrak. Maximizing non-monotone submodular functions. SIAM J Comp 

[17] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quarterly  3:

40(4):1133–1153  2011.

95–110  1956.

[18] S. Fujishige. Lexicographically optimal base of a polymatroid with respect to a weight vector. Mathematics

of Operations Research  pages 186–196  1980.

[19] S. Fujishige. Submodular Functions and Optimization. Elsevier  2005.
[20] S. Fujishige and S. Isotani. A submodular function minimization algorithm based on the minimum-norm

base. Paciﬁc Journal of Optimization  7:3–17  2011.

[21] H. Groenevelt. Two algorithms for maximizing a separable concave function over a polymatroid feasible

region. European Journal of Operational Research  54(2):227–236  1991.

[22] D.S. Hochbaum and S.-P. Hong. About strongly polynomial time algorithms for quadratic optimization

over submodular constraints. Math. Prog.  pages 269–309  1995.

[23] S. Iwata and N. Zuiki. A network ﬂow approach to cost allocation for rooted trees. Networks  44:297–301 

[24] S. Jegelka  H. Lin  and J. Bilmes. On fast approximate submodular minimization. In NIPS  2011.
[25] S. Jegelka  F. Bach  and S. Sra. Reﬂection methods for user-friendly submodular optimization (extended

2004.

version). arXiv  2013.

[26] R. Jenatton  J. Mairal  G. Obozinski  and F. Bach. Proximal methods for hierarchical sparse coding. Journal

of Machine Learning Research  pages 2297–2334  2011.

[27] P. Kohli  L. Ladick´y  and P. Torr. Robust higher order potentials for enforcing label consistency. Int.

Journal of Comp. Vision  82  2009.

[28] V. Kolmogorov. Minimizing a sum of submodular functions. Disc. Appl. Math.  160(15)  2012.
[29] N. Komodakis  N. Paragios  and G. Tziritas. MRF energy minimization and beyond via dual decomposition.

IEEE TPAMI  33(3):531–552  2011.

[30] A. Krause and C. Guestrin. Submodularity and its applications in optimized information gathering. ACM

Transactions on Intelligent Systems and Technology  2(4)  2011.

[31] H. Lin and J. Bilmes. A class of submodular functions for document summarization. In NAACL/HLT 

[32] L. Lov´asz. Submodular functions and convexity. Mathematical programming: the state of the art  Bonn 

[33] S. T. McCormick. Submodular function minimization. Discrete Optimization  12:321–391  2005.
[34] O. Meshi  T. Jaakkola  and A. Globerson. Convergence rate analysis of MAP coordinate minimization

2011.

pages 235–257  1982.

algorithms. In NIPS  2012.

[35] G.L. Nemhauser  L.A. Wolsey  and M.L. Fisher. An analysis of approximations for maximizing submodular

set functions–I. Math. Prog.  14(1):265–294  1978.

[36] Y. Nesterov. Smooth minimization of non-smooth functions. Math. Prog.  103(1):127–152  2005.
[37] J. B. Orlin. A faster strongly polynomial time algorithm for submodular function minimization. Math.

Prog.  118(2):237–251  2009.

[38] B. Savchynskyy  S. Schmidt  J. Kappes  and C. Schn¨orr. A study of Nesterov’s scheme for Lagrangian

decomposition and MAP labeling. In CVPR  2011.

[39] A. Shekhovtsov and V. Hlav´ac. A distributed mincut/maxﬂow algorithm combining path augmentation and

push-relabel. In Energy Minimization Methods in Computer Vision and Pattern Recognition  2011.

[40] P. Stobbe. Convex Analysis for Minimizing and Learning Submodular Set functions. PhD thesis  California

Institute of Technology  2013.

[41] P. Stobbe and A. Krause. Efﬁcient minimization of decomposable submodular functions. In NIPS  2010.
[42] R. Tarjan  J. Ward  B. Zhang  Y. Zhou  and J. Mao. Balancing applied to maximum network ﬂow problems.

In European Symp. on Algorithms (ESA)  pages 612–623  2006.

9

,Stefanie Jegelka
Francis Bach
Suvrit Sra
Will Hamilton
Zhitao Ying
Jure Leskovec