2011,Greedy Model Averaging,This paper considers the problem of combining multiple models to achieve a prediction accuracy not much worse than that of the best single model for least squares regression.  It is known that if the models are mis-specified  model averaging is superior to model selection.  Specifically  let $n$ be the sample size  then the worst case regret of the former decays at the rate of $O(1/n)$   while the worst case regret of the latter decays at the rate of $O(1/\sqrt{n})$.  In the literature  the most important and widely studied model averaging method that achieves the optimal $O(1/n)$ average regret   is the exponential weighted model averaging (EWMA) algorithm. However this method suffers from several limitations.   The purpose of this paper is to present a new greedy model averaging procedure that improves EWMA.  We prove strong theoretical guarantees for the new procedure and illustrate our theoretical results with empirical examples.,Greedy Model Averaging

Department of Statistics Rutgers University  New Jersey  08816

dongdai916@gmail.com

Dong Dai

Department of Statistics  Rutgers University  New Jersey  08816

tzhang@stat.rutgers.edu

Tong Zhang

Abstract

This paper considers the problem of combining multiple models to achieve a
prediction accuracy not much worse than that of the best single model for least
squares regression. It is known that if the models are mis-speciﬁed  model aver-
aging is superior to model selection. Speciﬁcally  let n be the sample size  then
the worst case regret of the former decays at the rate of O(1/n) while the worst
case regret of the latter decays at the rate of O(1/
n). In the literature  the most
important and widely studied model averaging method that achieves the optimal
O(1/n) average regret is the exponential weighted model averaging (EWMA) al-
gorithm. However this method suffers from several limitations. The purpose of
this paper is to present a new greedy model averaging procedure that improves
EWMA. We prove strong theoretical guarantees for the new procedure and illus-
trate our theoretical results with empirical examples.

√

1

Introduction

This paper considers the model combination problem  where the goal is to combine multiple models
in order to achieve improved accuracy. This problem is important for practical applications because
it is often the case that single learning models do not perform as well as their combinations. In
practice  model combination is often achieved through the so-called “stacking” procedure  where
multiple models {f1(x)  . . .   fM (x)} are ﬁrst learned based on a shared “training dataset”. Then
these models are combined on a separate “validation dataset”. This paper is motivated by this sce-
nario. In particular  we assume that M models {f1(x)  . . .   fM (x)} are given a priori (e.g.  we may
regard them as being obtained with a separate training set)  and we are provided with n labeled data
points (validation data) {(X1  Y1)  . . .   (Xn  Yn)} to combine these models.
For simplicity and clarity  our analysis focuses on least squares regression in ﬁxed design although
similar analysis can be extended to random design and to other loss functions. In this setting  for
notation convenience  we can represent the k-th model on the validation data as a vector f k =
[fk(X1)  . . .   fk(Xn)] ∈ Rn  and we let the observation vector y = [Y1  . . .   Yn] ∈ Rn. Let
g = Ey be the mean. Our goal (in the ﬁxed design or denoising setting) is to estimate the mean
vector g from y using the M existing models F = {f 1  . . . f M}. Here  we can write

y = g + ξ 

where we assume that ξ are iid Gaussian noise: ξ ∼ N (0  σ2I n×n) for simplicity. This iid Gaussian
assumption isn’t critical  and the results remain the same for independent sub-Gaussian noise.
We assume that the models may be mis-speciﬁed. That is  let k∗ be the best single model deﬁned as:
(1)

k∗ = argmin

(cid:107)f k − g(cid:107)2
2  

k

1

then f k∗ (cid:54)= g.
We are interested in an estimator ˆf of g that achieves a small regret

R(ˆf ) =

1
n

(cid:13)(cid:13)f k∗ − g(cid:13)(cid:13)2

2 .

− 1
n

(cid:13)(cid:13)(cid:13)ˆf − g
(cid:13)(cid:13)(cid:13)2
M(cid:88)

2

ˆf =

k=1

ˆwkf k 

This paper considers a special class of model combination methods which we refer to as model
averaging  with combined estimators of the form

where ˆwk ≥ 0 and(cid:80)

we choose the model ˆk with the smallest least squares error:

k ˆwk = 1. A standard method for “model averaging” is model selection  where

M(cid:88)

(cid:80)M

qke−λ(cid:107)f k−y(cid:107)2
j=1 qje

−λ(cid:107)f j−y(cid:107)2

2

 

ˆf M S = f ˆk;

ˆk = arg min
k

(cid:107)f k − y(cid:107)2
2 .

This corresponds to the choice of ˆwˆk = 1 and ˆwk = 0 when k (cid:54)= ˆk. However  it is well known that

the worst case regret this procedure can achieve is R(ˆf M S) = O((cid:112)ln M/n) [1]. Another standard

model averaging method is the Exponential Weighted Model Averaging (EWMA) estimator deﬁned
as

ˆf EW M A =

ˆwkf k 

ˆwk =

(2)

2

k=1

favoring some models over some other models. Here we assume that qj ≥ 0 and(cid:80)

with a tuned parameter λ ≥ 0. The extra parameters {qj}j=1 ... M are priors that impose bias
j qj = 1.
In this setting  the most common prior choice is the ﬂat prior qj = 1/M. It should be pointed
out that a progressive variant of (2)  which returns the average of n + 1 EWMA estimators with
Si = {(X1  Y1)  . . .   (Xi  Yi)} for i = 0  1  . . .   n  was often analyzed in the earlier literature
[2  9  5  1]. Nevertheless  the non progressive version presented in (2) is clearly a more natural
estimator  and this is the form that has been studied in more recent work [3  6  8]. Our current paper
does not differentiate these two versions of EWMA because they have similar theoretical properties.
In particular  our experiments only compare to the non-progressive version (2) that performs better
in practice.
It is known that exponential model averaging leads to an average regret of O(ln M/n) which
achieves the optimal rate; however it was pointed out in [1] that the rate does not hold with large

probability. Speciﬁcally  EWMA only leads to a sub-optimal deviation bound of O((cid:112)ln M/n) with

large probability. To remedy this sub-optimality  an empirical star algorithm (which we will refer to
as STAR from now on) was then proposed in [1]; it was shown that the algorithm gives O(ln M/n)
deviation bound with large probability under the ﬂat prior qi = 1/M. One major issue of the STAR
algorithm is that its average performance is often inferior to EWMA  as we can see from our em-
pirical examples. Therefore although theoretically interesting  it is not an algorithm that can be
regarded as a replacement of EWMA for practical purposes. Partly for this reason  a more recent
study [7] re-examined the problem of improving EWMA  where different estimators were proposed
in order to achieve optimal deviation for model averaging. However  the proposed algorithms are
rather complex and difﬁcult to implement. The purpose of this paper is to present a simple greedy
model averaging (GMA) algorithm that gives the optimal O(ln M/n) deviation bound with large
probability  and it can be applied with arbitrary prior qi. Moreover  unlike STAR which has average
performance inferior to EWMA  the average performance of GMA algorithm is generally superior
to EWMA as we shall illustrate with examples. It also has some other advantages which we will
discuss in more details later in the paper.

2 Greedy Model Averaging

This paper studies a new model averaging procedure presented in Algorithm 1. The procedure has L
stages  and each time adds an additional model f ˆk((cid:96)) into the ensemble. It is based on a simple  but

2

(cid:13)(cid:13)(cid:13)2

((cid:96)−1) − f j

depends on the extra term µ((cid:96))(cid:13)(cid:13)(cid:13)ˆf

important modiﬁcation of a classical sequential greedy approximation procedure in the literature [4] 
which corresponds to setting µ((cid:96)) = 0  λ = 0 in Algorithm 1 with α((cid:96)) optimized over [0  1]. The
(2) with the above mentioned classical greedy
STAR algorithm corresponds to the stage-2 estimator ˆf
procedure of [4]. However  in order to prove the desired deviation bound  our analysis critically
which isn’t present in the classical procedure (that
is  our proof does not apply to the procedure of [4]). As we will see in Section 4  this extra term
does have a positive impact under suitable conditions that correspond to Theorem 1 and Theorem 2
below  and thus this term is not only for theoretical interest  but also it leads to practical beneﬁts
under the right conditions.
Another difference between GMA and the greedy algorithm in [4] is that our procedure allows the
use of non-ﬂat priors through the extra penalty term λc((cid:96)) ln(1/qj). This generality can be useful
for some applications. Moreover  it is useful to notice that if we choose the ﬂat prior qj = 1/M 
then the term λc((cid:96)) ln(1/qj) is identical for all models  and thus this term can be removed from the
optimization. In this case  the proposed method has the advantage of being parameter free (with the
default choice of ν = 0.5). This advantage is also shared by the STAR algorithm.

2

((cid:96))

(0)

= 0

: noisy observation y and static models f 1  . . .   f M
: averaged model ˆf

input
output
parameters: prior {qj}j=1 ... M and regularization parameters ν and λ
let ˆf
for (cid:96) = 1  2  . . .   L do
let α((cid:96)) = ((cid:96) − 1)/(cid:96)
let µ(1) = 0; µ(2) = 0.05; µ((cid:96)) = ν((cid:96) − 1)/(cid:96)2 if (cid:96) > 2
let c(1) = 1; c(2) = 0.25; and c((cid:96)) = [20ν(1 − ν)((cid:96) − 1)]−1 if (cid:96) > 2
let ˆk((cid:96)) = argminj Q((cid:96))(j)  where

(cid:20)(cid:13)(cid:13)(cid:13)α((cid:96)) ˆf

((cid:96)−1)

((cid:96)−1)

+ (1 − α((cid:96)))f j − y

((cid:96)−1) − f j

(cid:13)(cid:13)(cid:13)2

2

+ µ((cid:96))(cid:13)(cid:13)(cid:13)ˆf

(cid:13)(cid:13)(cid:13)2

2

+ λc((cid:96)) ln 1
qj

(cid:21)

Q((cid:96))(j) :=

((cid:96))

= α((cid:96)) ˆf

let ˆf

end

+ (1 − α((cid:96)))f ˆk((cid:96))

Algorithm 1: Greedy Model Averaging (GMA)

Observe that the ﬁrst stage of GMA corresponds to the standard model selection procedure:

(cid:104)(cid:13)(cid:13)f j − y(cid:13)(cid:13)2

2

(cid:105)

+ λ ln(1/qj)

 

ˆk(1) = argmin

j

(1)

ˆf

= f ˆk(1).

(cid:34)(cid:13)(cid:13)(cid:13)(cid:13) 1

2

√
As we have pointed out earlier  it is well known that only O(1/
n) regret can be achieved by
any model selection procedure (that is  any procedure that returns a single model ˆf ˆk for some ˆk).
However  a combination of only two models will allow us to achieve the optimal O(1/n) rate. In
fact  ˆf

(2) achieves this rate. For clarity  we rewrite this stage 2 estimator as

(cid:13)(cid:13)(cid:13)(cid:13)2

2

(cid:13)(cid:13)(cid:13)ˆf ˆk(1) − f j

(cid:13)(cid:13)(cid:13)2

2

(cid:35)

ˆk(2) = argmin

j

(f ˆk(1) + f j) − y

+

1
20

+

λ
4

ln(1/qj)

 

(2)

ˆf

=

1
2

(f ˆk(1) + f ˆk(2)).

Theorem 1 shows that this simple stage 2 estimator achieves O(1/n) regret. A similar result was
shown in [1] for the STAR algorithm under the ﬂat prior qj = 1/M  which corresponds to the stage
2 estimator of the classical greedy algorithm in [4]. Theoretically our result has several advantages
over that of the classical EWMA method. First it produces a sparse estimator while exponential
averaging estimator is dense; second the performance bound is scale free in the sense that the bound

3

(cid:13)(cid:13)f j

(cid:13)(cid:13); third the optimal bound

depends only on the noise variance but not the magnitude of maxj
holds with high probability while EWMA only achieves optimal bound on average but not with large
probability; and ﬁnally if we choose a ﬂat prior qj = 1/M  the estimator is parameter free because
we can exclude the term λ ln(1/qj) from the estimators. This result also improves the recent work
of [7] in that the resulting bound is scale free while the algorithm itself is signiﬁcantly simpler. One
disadvantage of this stage-2 estimator (and similarly the STAR estimator of [1]) is that its average
performance is generally inferior to that of EWMA  mainly due to the relatively large constant in
Theorem 1 (the same issue holds for the STAR algorithm). For this reason  the stage-2 estimator is
not a practical replacement of EWMA. This is the main reason why it is necessary to run GMA for
L > 2 stages  which leads to reduced constants (see Theorem 2) below. Our empirical experiments
show that in order to compete with EWMA for average performance  it is important to take L > 2.
However a relatively small L (as small as L = 5) is often sufﬁcient  and in such case the resulting
estimator is still quite sparse.

Theorem 1 Given qj ≥ 0 such that

qj = 1. If λ ≥ 40σ2  then with probability 1 − 2δ we have

M(cid:80)

(2)

R(ˆf

j=1

) ≤ λ
n

(cid:20) 3

4

(cid:21)

ln(1/qk∗ ) +

1
2

ln(1/δ)

.

(2) achieves the optimal rate  running GMA for more more stages
While the stage-2 estimator ˆf
can further improve the performance. The following theorem shows that similar bounds can be
obtained for GMA at stages larger than 2. However  the constant before σ2
qk∗ δ approaches 8
when (cid:96) → ∞ (with default ν = 0.5)  which is smaller than the constant of Theorem 1 which is
about 30. This implies potential improvement when we run more stages  and this improvement is
conﬁrmed in our empirical study. In fact  with relatively large (cid:96)  the GMA method not only has the
theoretical advantage of achieving smaller regret in deviation (that is  the regret bound holds with
large probability) but also achieves better average performance in practice.

n ln 1

Theorem 2 Given qj ≥ 0 such that
then with probability 1 − 2δ we have

((cid:96))

R(ˆf

) ≤ λ
n

qj = 1. If λ ≥ 40σ2 and let 0 < ν < 1 in Algorithm 1 

M(cid:80)
(cid:20) ((cid:96) − 2) + ln((cid:96) − 1) + 30ν(1 − ν)

j=1

(cid:21)

20ν(1 − ν)(cid:96)

ln

1

qk∗ δ

.

Another important advantage of running GMA for (cid:96) > 2 stages is that the resulting estimator not
only competes with the best single estimator f k∗  but also competes with the best estimator in
the convex hull of cov(F) (with the parameter ν appropriately tuned). Note that the latter can be
signiﬁcantly better than the former. Deﬁne the convex hull of F as

 M(cid:88)

j=1

(cid:88)

j

 .

cov(F) =

wjf j : wj ≥ 0;

wj = 1

√
The following theorem shows that as (cid:96) → ∞  the prediction error of ˆf
((cid:96)) is no more than O(1/
√
n)
worse than that of the optimal ¯f ∈ cov(F) when we choose a sufﬁciently small ν = O(1/
n)
in Algorithm 1. Note that in this case  it is beneﬁcial to use a parameter ν smaller than the default
choice of ν = 0.5. This phenomenon is also conﬁrmed by our experiments.

Theorem 3 Given qj ≥ 0 such that

M(cid:80)
(cid:80)
j wj = 1 and wj ≥ 0  and let ¯f =(cid:80)
(cid:13)(cid:13)(cid:13)ˆf
(cid:88)

then with probability 1 − 2δ  when (cid:96) → ∞:
1
n

(cid:13)(cid:13)¯f − g(cid:13)(cid:13)2

((cid:96)) − g

(cid:13)(cid:13)(cid:13)2

≤ 1
n

2+

ν
n

wk

j=1

2

k

qj = 1. Consider any {wj : j = 1  . . .   M} such that
j wjf j. If λ ≥ 40σ2 and let 0 < ν < 1 in Algorithm 1 
(cid:19)

(cid:19)

(cid:18) 1

(cid:18) 1

(cid:88)

(cid:13)(cid:13)f k − ¯f(cid:13)(cid:13)2

wk ln

+O

λ

.

2+

20ν(1 − ν)n

(cid:96)

δqk

k

4

3 Experiments

The point of these experiments is to show that the consequences of our theoretical analysis can be
observed in practice  which support the main conclusions we reach. For this purpose  we consider
the model g = Xw + 0.5∆g  where X = (f 1  . . .   f M ) is an n × M matrix with independent
standard Gaussian entries  and ∆g ∼ N (0  In×n) implies that the model is mis-speciﬁed.
The noise vector is ξ ∼ N (0  σ2I n×n)  independently generated of X. The coefﬁcient vector
j=1 |uj| for i = 1  . . .   s  where u1  . . .   us are

w = (w1  . . .   wM )(cid:62) is given by wi = |ui|/(cid:80)s

independent standard uniform random variables for some ﬁxed s.
The performance of an estimator ˆf measured here is the mean squared error (MSE) deﬁned as

(cid:13)(cid:13)(cid:13)ˆf − g

(cid:13)(cid:13)(cid:13)2

2

MSE(ˆf ) =

1
n

.

We run the Greedy Model Averaging (GMA) algorithm for L stages up to L = 40. The EWMA
parameter is tuned via 10-fold cross-validation. Moreover  we also listed the performance of EWMA
with projection  which is the method that runs EWMA  but with each model f k replaced by model
˜f k = αkf k where αk = arg minα∈R (cid:107)αf k − y(cid:107)2
2. That is  ˜f k is the best linear scaling of f k to
predict y. Note that this is a special case of the class of methods studied in [6] (which considers
more general projections) that leads to non progressive regret bounds  and this is the method of
signiﬁcant current interests [3  8]. However  at least for the scenario considered in our paper  the
projected EWMA method never improves performance in our experiments. Finally  for reference
purpose  we also report the MSE of the best single model (BSM) f k∗  where k∗ is given by (1).
The model f k∗ is clearly not a valid estimator because it depends on the unobserved g; however its
performance is informative  and thus included in the tables. For simplicity  all algorithms use ﬂat
prior qk = 1/M.

4

Illustration of Theorem 1 and Theorem 2

The ﬁrst set of experiments are performed with the parameters n = 50  M = 200 s = 1 and σ = 2.
Five hundred replications are run  and the MSE performance of different algorithms are reported in
Table 1 using the “mean ± standard deviation” format.
Note that with s = 1  the target is g = f 1 + 0.5∆g. Since f 1 and ∆g are random Gaussian vectors 
the best single model is likely f 1. The noise σ = 2 is relatively large. This is thus the situation
that model averaging does not achieve as good a performance as that of the best single model. This
corresponds to the scenario considered in Theorem 1 and Theorem 2.
The results indicate that for GMA  from L = 1 (corresponding to model selection) to L = 2 (stage-2
model averaging of Theorem 1)  there is signiﬁcant reduction of error. The performance of GMA
with L = 2 is comparable to that of the STAR algorithm. This isn’t surprising  because STAR can
be regarded as the stage-2 estimator based on the more classical greedy algorithm of [4]. We also
observe that the error keeps decreasing (but at a slower pace) when L > 2  which is consistent with
Theorem 2. It means that in order to achieve good performance  it is necessary to use more stages
than L = 2 (although this doesn’t change the O(1/n) rate for regret  it can signiﬁcantly reduce
constant). It becomes better than EWMA when L is as small as 5  which still gives a relatively
sparse averaged model. EWMA with projection does not perform as well as the standard EWMA
method in this setting. Moreover  we note that in this scenario  the standard choice of ν = 0.5 in
Theorem 2 is superior to choosing smaller ν = 0.1 or ν = 0.001. This again is consistent with
Theorem 2  which shows that the new term we added into the greedy algorithm is indeed useful in
this scenario.

5

Illustration of Theorem 3

The second set of experiments are performed with the parameters n = 50  M = 200 s = 10 and
σ = 0.5. Five hundred replications are run  and the MSE performance of different algorithms are
reported in Table 2 using the “mean ± standard deviation” format.

5

Table 1: MSE of different algorithms: best single model is superior to averaged models

STAR

0.663 ± 0.4
L = 1

EWMA
0.645 ± 0.5
L = 2

0.735 ± 0.74
0.735 ± 0.74
0.735 ± 0.74

0.689 ± 0.4
0.689 ± 0.4
0.689 ± 0.4

GMA
ν = 0.5
ν = 0.1
ν = 0.01

0.744 ± 0.5
L = 5

0.58 ± 0.39
0.645 ± 0.31
0.663 ± 0.3

EWMA (with projection)

BSM

0.252 ± 0.05

L = 20

0.566 ± 0.37
0.623 ± 0.29
0.638 ± 0.28

L = 40

0.567 ± 0.38
0.622 ± 0.29
0.639 ± 0.28

√

Note that with s = 10  the target is g = ¯f + 0.5∆g for some ¯f ∈ cov(F). The noise σ = 0.5 is
relatively small  which makes it beneﬁcial to compete with the best model ¯f in the convex hull even
n) when competing with ¯f. This is thus the situation
though GMA has a larger regret of O(1/
considered in Theorem 3  which means that model averaging can achieve better performance than
that of the best single model.
The results again show that for GMA  from L = 1 (corresponding to model selection) to L = 2
(stage-2 model averaging of Theorem 1)  there is signiﬁcant reduction of error. The performance
of GMA with L = 2 is again comparable to that of the STAR algorithm. Again we observe that
even with the standard choice of ν = 0.5  the error keeps decreasing (but at a slower pace) when
L > 2  which is consistent with Theorem 2. It becomes better than EWMA when L is as small as 5 
which still gives a relatively sparse averaged model. EWMA with projection again does not perform
as well as the standard EWMA method in this setting. Moreover  we note that in this scenario  the
standard choice of ν = 0.5 in Theorem 2 is inferior to choosing smaller parameter values of ν = 0.1
or ν = 0.001. This is consistent with Theorem 3  where it is beneﬁcial to use a smaller value for ν
in order to compete with the best model in the convex hull.

Table 2: MSE of different algorithms: best single model is inferior to averaged model

EWMA (with projection)

STAR

0.443 ± 0.08
L = 1

0.809 ± 0.12
0.809 ± 0.12
0.809 ± 0.12

EWMA

0.316 ± 0.087
L = 2

0.456 ± 0.081
0.456 ± 0.081
0.456 ± 0.081

GMA
ν = 0.5
ν = 0.1
ν = 0.01

0.364 ± 0.078
L = 5

0.305 ± 0.062
0.269 ± 0.056
0.268 ± 0.053

L = 20

0.266 ± 0.057
0.214 ± 0.046
0.211 ± 0.045

L = 40

0.265 ± 0.057
0.211 ± 0.045
0.207 ± 0.045

BSM

0.736 ± 0.083

6 Conclusion

This paper presents a new model averaging scheme which we call greedy model averaging (GMA).
It is shown that the new method can achieve regret bound of O(ln M/n) with large probability
when competing with the single best model. Moreover  it can also compete with the best combined
model in convex hull. Both our theory and experimental results suggest that the proposed GMA
algorithm is superior to the standard EWMA procedure. Due to the simplicity of our proposal 
GMA may be regarded as a valid alternative to the more widely studied EWMA procedure both for
practical applications and for theoretical purposes. Finally we shall point out that while this work
only considers static model averaging where the models F are ﬁnite  similar results can be obtained
for afﬁne estimators or inﬁnite models considered in recent work [3  6  8]. Such extension will be
left to the extended report.

A Proof Sketches

We only include proof sketches  and leave the details to the supplemental material that accompanies
the submission. First we need the following standard Gaussian tail bounds. The proofs can be found
in the supplemental material.

6

(cid:80)
Proposition 1 Let f j ∈ Rn be a set of ﬁxed vectors (j = 1  . . .   M)  and assume that qj ≥ 0 with

j qj = 1. Let k∗ be a ﬁxed integer between 1 and M. Deﬁne event E1 as

(cid:26)

(cid:113)
∀j : (f j − f k∗ )(cid:62)ξ ≤ σ(cid:107)f j − f k∗(cid:107)2
(cid:113)

∀j  k : (f j − f k)(cid:62)ξ ≤ σ(cid:107)f j − f k(cid:107)2

(cid:26)

E1 =

E2 =

(cid:27)
(cid:27)

2 ln(1/(δqj))

2 ln(1/(δqjqk))

 

and deﬁne event E2 as

then P (E1) ≥ 1 − δ and P (E2) ≥ 1 − δ.

A.1 Proof Sketch of Theorem 1
More detailed proof can be found in the supplemental material. Note that with probability 1 − 2δ 
both event E1 and event E2 of Proposition 1 hold. Moreover we have

(cid:13)(cid:13)(cid:13)ˆf

(2) − g

(cid:13)(cid:13)(cid:13)2

2

+µ(2)

+ λc(2)(ln(1/qk∗ ) − ln(1/qˆk(2))).
In the above derivation  the inequality is equivalent to Q(2)(ˆk(2)) ≤ Q(2)(k∗)  which is a simple fact
of the deﬁnition of ˆk((cid:96)) in the algorithm. Also we can rewrite the fact that Q(1)(ˆk(1)) ≤ Q(1)(k∗) as

(1) − f ˆk(2)

2

2

+ 2(1 − α(2))ξ

(cid:62)

(f ˆk(2) − f k∗ )

(cid:19)

(cid:13)(cid:13)(cid:13)2

By combining the above two inequalities  we obtain

(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2

2

2

(cid:13)(cid:13)(cid:13)ˆf
(cid:13)(cid:13)(cid:13)ˆf

2

(1)

(1)

(cid:62)

=

(cid:13)(cid:13)(cid:13)2

(1) − g

2 ≤ 2ξ

+ (1 − α(2))f ˆk(2) − g
+ (1 − α(2))f k∗ − g
(1) − f k∗

−(cid:13)(cid:13)(cid:13)ˆf

(cid:13)(cid:13)(cid:13)α(2) ˆf
≤ (cid:13)(cid:13)(cid:13)α(2) ˆf
(cid:18)(cid:13)(cid:13)(cid:13)ˆf
(cid:13)(cid:13)(cid:13)2
−(cid:13)(cid:13)f k∗ − g(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2
2 ≤ α(2)(cid:104)
−(cid:13)(cid:13)f k∗ − g(cid:13)(cid:13)2
−µ(2)(cid:13)(cid:13)f ˆk(1) − f ˆk(2)
(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)ˆf
(cid:13)(cid:13)(cid:13)2
−(cid:13)(cid:13)f k∗ − g(cid:13)(cid:13)2
(cid:115)
+2(cid:13)(cid:13)f ˆk(1) − f k∗
(cid:13)(cid:13)2 σ
+(µ(2) − 1/4)(cid:13)(cid:13)f ˆk(1) − f k∗

(2) − g
λc(1) + λc(2)) ln(1/qk∗ ) − 1
2

(2) − g
+2(1 − α(2))ξ

(f ˆk(2) − f k∗ ) +

2 ln

1
2

(cid:62)

2

2

2

≤ (

Since α(2) = 1/2  we obtain

(f ˆk(1) − f k∗ ) + λc(1) ln(qˆk(1)/qk∗ ).

(cid:62)

(cid:105)
(cid:13)(cid:13)2
(f ˆk(1) − f k∗ ) + λc(1) ln(qˆk(1)/qk∗ )
µ(2) − α(2)(1 − α(2))
2 + λc(2)(ln(1/qk∗ ) − ln(1/qˆk(2))).

(cid:105)(cid:13)(cid:13)f ˆk(1) − f k∗

(cid:104)

2ξ

2

1

(cid:115)
λc(1) ln(1/qˆk(1)) − λc(2) ln(1/qˆk(2))

(cid:13)(cid:13)f ˆk(2) − f ˆk(1)
2 − µ(2)(cid:13)(cid:13)f ˆk(1) − f ˆk(2)
(cid:13)(cid:13)2
(cid:13)(cid:13)2

(cid:13)(cid:13)2 σ

+ 2 · 1
2

qˆk(1)δ

2 ln

2

1

qˆk(1)qˆk(2)δ

≤ (

1
2

λc(1) + λc(2)) ln(1/qk∗ ) + (2r1 + 2r2) ln(1/δ).

The ﬁrst inequality above uses the tail probability bounds in the event E1 and E2. We then use the
algebraic inequality 2a1b1 ≤ a2
2 to obtain the last inequality 
which implies the desired bound.

1 and 2a2b2 ≤ a2

2/r2 + r2b2

1/r1 + r1b2

A.2 Proof Sketch of Theorem 2
Again  more detailed proof can be found in the supplemental material. With probability 1− 2δ  both
event E1 and event E2 of Proposition 1 hold. This implies that the claim of Theorem 1 also holds.

7

− λc((cid:96))(ln(qk∗ ) − ln(qˆk((cid:96)) )) + 2(1 − α((cid:96)))ξ

(cid:62)

(f ˆk((cid:96)) − f k∗ )

((cid:96)−1)

(cid:13)(cid:13)(cid:13)2

((cid:96)) − g

(cid:13)(cid:13)(cid:13)ˆf

≤ (cid:13)(cid:13)(cid:13)α((cid:96)) ˆf

Now consider any (cid:96) ≥ 3. We have

(cid:105)
(cid:62)(cid:104)
(cid:18)(cid:13)(cid:13)(cid:13)ˆf
(cid:13)(cid:13)(cid:13)2
(1 − α((cid:96)))f ˆk((cid:96)) − (1 − α((cid:96)))f k∗
((cid:96)−1) − f ˆk((cid:96))
((cid:96)−1) − f k∗
The inequality is equivalent to Q((cid:96))(ˆk((cid:96))) ≤ Q((cid:96))(k∗)  which is a simple fact of the deﬁnition of ˆk((cid:96))
in the algorithm. We can rewrite the above inequality as

+λc((cid:96))(ln(1/qk∗ ) − ln(1/qˆk((cid:96)) )) + µ((cid:96))

+ (1 − α((cid:96)))f k∗ − g

−(cid:13)(cid:13)(cid:13)ˆf

(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)2

+ 2ξ

2

2

2

2

(cid:19)

.

2

(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)f ˆk((cid:96)) − f k∗

(cid:13)(cid:13)2

2

≤ α((cid:96))

((cid:96)−1) − g

2

((cid:96)) − g

(cid:18)(cid:13)(cid:13)(cid:13)ˆf
(cid:13)(cid:13)(cid:13)2
(cid:18)(cid:13)(cid:13)(cid:13)ˆf
−µ((cid:96))(cid:13)(cid:13)(cid:13)f ˆk((cid:96)) − ˆf
(cid:18)(cid:13)(cid:13)(cid:13)ˆf
(cid:13)(cid:13)f ˆk((cid:96)) − f k∗
(cid:18)(cid:13)(cid:13)(cid:13)ˆf

2
(cid:96)

+

≤ α((cid:96))

((cid:96)−1) − g

≤ (cid:96) − 1
(cid:20)
− (cid:96) − 1

((cid:96)−1) − g
(cid:96)2 ν(1 − ν) +

+

(cid:96)

2

2

2

2

2

2

+

(cid:105)(cid:13)(cid:13)(cid:13)ˆf

((cid:96)−1) − f k∗

µ((cid:96)) − α((cid:96))(1 − α((cid:96)))

(cid:19)
−(cid:13)(cid:13)f k∗ − g(cid:13)(cid:13)2
(cid:19)
(cid:13)(cid:13)(cid:13)2
−(cid:13)(cid:13)f k∗ − g(cid:13)(cid:13)2
((cid:96)−1)(cid:13)(cid:13)(cid:13)2
(cid:104)
(cid:19)
(cid:13)(cid:13)(cid:13)2
−(cid:13)(cid:13)f k∗ − g(cid:13)(cid:13)2
(cid:115)
− µ((cid:96))(cid:2)α((cid:96))(1 − α((cid:96))) − µ((cid:96))(cid:3)
+ λc((cid:96))(ln(1/qk∗ ) − ln(1/qˆk((cid:96))))
(cid:13)(cid:13)2 σ
(cid:19)
(cid:13)(cid:13)(cid:13)2
−(cid:13)(cid:13)f k∗ − g(cid:13)(cid:13)2
(cid:21)(cid:13)(cid:13)f ˆk((cid:96)) − f k∗
(cid:2)µ((cid:96)) − α((cid:96))(1 − α((cid:96)))(cid:3)(cid:13)(cid:13)(cid:13)ˆf
(cid:13)(cid:13)2
(cid:13)(cid:13)f ˆk((cid:96)) − f k∗

(cid:13)(cid:13)2
((cid:96)−1) − f k∗

α((cid:96))(1 − α((cid:96)))

+ λc((cid:96))(ln(1/qk∗ ) − ln(1/qˆk((cid:96)) ))

2 + 2r(cid:96) ln

(cid:13)(cid:13)(cid:13)2

σ2
(cid:96)2r(cid:96)

qˆk((cid:96)) δ

qˆk((cid:96))δ

2 ln

1

1

2

2

.

that −p(cid:107)a(cid:107)2 − q (cid:107)b(cid:107)2 ≤ −pq/(p + q)(cid:107)a + b(cid:107)2 
≤

The second inequality uses the fact
which implies
that
− µ((cid:96))[α((cid:96))(1−α((cid:96)))−µ((cid:96))]
2 and uses the Gaussian tail bound in the event E1. The
last inequality uses 2ab ≤ a2/r(cid:96) + r(cid:96)b2  where r(cid:96) > 0 is r(cid:96) = λc((cid:96))/2. Denote by R((cid:96)) =
−1 we
(cid:96) R((cid:96)−1) + λc((cid:96)) ln(1/qk∗ δ) . Solving this recursion for R((cid:96)) leads to the desired

2  then since the choice of parameters c((cid:96)) = [20ν(1 − ν)((cid:96) − 1)]

−(cid:13)(cid:13)f k∗ − g(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)ˆf

α((cid:96))(1−α((cid:96)))

((cid:96)) − g

(cid:13)(cid:13)(cid:13)2

2

2

2

− µ((cid:96))(cid:13)(cid:13)(cid:13)f ˆk((cid:96)) − ˆf

((cid:96)−1)(cid:13)(cid:13)(cid:13)2

obtain R((cid:96)) ≤ (cid:96)−1
bound.

A.3 Proof Sketch of Theorem 3
Again  more detailed proof can be found in the supplemental material. Consider any (cid:96) ≥ 3. We have

(cid:13)(cid:13)(cid:13)ˆf
(cid:88)

2

(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)α((cid:96)) ˆf
(cid:88)

+ µ((cid:96))

2

k

wk

≤

((cid:96)−1)

((cid:96)) − g

+λc((cid:96))(

+ (1 − α((cid:96)))f k − g

(cid:13)(cid:13)(cid:13)2
The inequality is equivalent to Q((cid:96))(ˆk((cid:96))) ≤(cid:80)
(cid:13)(cid:13)(cid:13)ˆf

of ˆk((cid:96)) in the algorithm. Denote by R((cid:96)) =
that of Theorem 2 implies that
R((cid:96)) ≤ (cid:96) − 1

R((cid:96)−1) + λc((cid:96))(cid:88)

k

wk ln(1/qk) − ln(1/qˆk((cid:96)) )) + 2ξ

(cid:62)

(cid:96)

Now by solving the recursion  we obtain the theorem.

8

(cid:33)

(cid:13)(cid:13)(cid:13)2

2

−(cid:13)(cid:13)(cid:13)ˆf
((cid:96)−1) − f ˆk((cid:96))
(cid:35)
(cid:88)

wkf k

.

k

k

wk

(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)ˆf

((cid:96)−1) − f k

(cid:32)(cid:88)
(cid:34)
(1 − α((cid:96)))f ˆk((cid:96)) − (1 − α((cid:96)))
(cid:13)(cid:13)(cid:13)2

−(cid:13)(cid:13)¯f − g(cid:13)(cid:13)2

2

2

k wkQ((cid:96))(k)  which is a simple fact of the deﬁnition
((cid:96)) − g
2  then the same derivation as

(cid:88)

(cid:13)(cid:13)f k − ¯f(cid:13)(cid:13)2

2 .

wk ln(1/(δqk)) + [µ((cid:96)) + (1 − α((cid:96)))2]

wk

k

k

References
[1] Jean-Yves Audibert. Progressive mixture rules are deviation suboptimal. In NIPS’07  2008.
[2] Olivier Catoni. Statistical learning theory and stochastic optimization. Springer-Verlag  2004.
[3] Arnak Dalalyan and Joseph Salmon. Optimal aggregation of afﬁne estimators. In COLT’01 

2011.

[4] L.K. Jones. A simple lemma on greedy approximation in Hilbert space and convergence rates for

projection pursuit regression and neural network training. Ann. Statist.  20(1):608–613  1992.

[5] Anatoli Juditsky  Philippe Rigollet  and Alexandre Tsybakov. Learning by mirror averaging.

The Annals of Statistics  36:2183–2206  2008.

[6] Gilbert Leung and A.R. Barron. Information theory and mixing least-squares regressions. In-

formation Theory  IEEE Transactions on  52(8):3396 –3410  aug. 2006.

[7] Philippe Rigollet. Kullback-leibler aggregation and misspeciﬁed generalized linear models.

arXiv:0911.2919  November 2010.

[8] Pilippe Rigollet and Alexandre Tsybakov. Exponential Screening and optimal rates of sparse

estimation. The Annals of Statistics  39:731–771  2011.

[9] Yuhong Yang. Adaptive regression by mixing. Journal of the American Statistical Association 

96:574–588  2001.

9

,Nathaniel Korda
Emilie Kaufmann
Remi Munos