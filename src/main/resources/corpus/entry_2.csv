2009,$L_1$-Penalized Robust Estimation for a Class of Inverse Problems Arising in Multiview Geometry,We propose a new approach to the problem of robust estimation in multiview geometry. Inspired by recent  advances in the sparse recovery problem of statistics  our estimator is defined as a Bayesian maximum a posteriori  with multivariate Laplace prior on the vector describing the outliers. This leads to an estimator in which  the fidelity to the data is measured by the $L_\infty$-norm while the regularization is done by the $L_1$-norm.  The proposed procedure is fairly fast since the outlier removal is done by solving one linear program (LP).  An important difference compared to existing algorithms is that for our estimator it is not necessary  to specify neither the number nor the proportion of the outliers. The theoretical results  as well as  the numerical example reported in this work  confirm the efficiency of the proposed approach.,L1-Penalized Robust Estimation for a Class of Inverse

Problems Arising in Multiview Geometry

Arnak S. Dalalyan and Renaud Keriven

IMAGINE/LabIGM 

dalalyan keriven@imagine.enpc.fr

Universit´e Paris Est - Ecole des Ponts ParisTech 

Marne-la-Vall´ee  France

Abstract

We propose a new approach to the problem of robust estimation in multiview ge-
ometry. Inspired by recent advances in the sparse recovery problem of statistics 
we deﬁne our estimator as a Bayesian maximum a posteriori with multivariate
Laplace prior on the vector describing the outliers. This leads to an estimator
in which the ﬁdelity to the data is measured by the L∞-norm while the regular-
ization is done by the L1-norm. The proposed procedure is fairly fast since the
outlier removal is done by solving one linear program (LP). An important differ-
ence compared to existing algorithms is that for our estimator it is not necessary
to specify neither the number nor the proportion of the outliers. We present strong
theoretical results assessing the accuracy of our procedure  as well as a numerical
example illustrating its efﬁciency on real data.

1 Introduction

In the present paper  we are concerned with a class of non-linear inverse problems appearing in the
structure and motion problem of multiview geometry. This problem  that have received a great deal
of attention by the computer vision community in last decade  consists in recovering a set of 3D
points (structure) and a set of camera matrices (motion)  when only 2D images of the aforemen-
tioned 3D points by some cameras are available. Throughout this work we assume that the internal
parameters of cameras as well as their orientations are known. Thus  only the locations of camera
centers and 3D points are to be estimated. In solving the structure and motion problem by state-of-
the-art methods  it is customary to start by establishing correspondences between pairs of 2D data
points. We will assume in the present study that these point correspondences have been already
established.
One can think of the structure and motion problem as the inverse problem of inverting the operator O
that takes as input the set of 3D points and the set of cameras  and produces as output the 2D images
of the 3D points by the cameras. This approach will be further formalized in the next section.
Generally  the operator O is not injective  but in many situations (for example  when for each pair
of cameras there are at least ﬁve 3D points in general position that are seen by these cameras [23]) 
there is only a small number of inputs  up to an overall similarity transform  having the same image
by O. In such cases  the solutions to the structure and motion problem can be found using algebraic
arguments.
The main ﬂaw of algebraic solutions is their sensitivity to the noise in the data: very often  thanks
to the noise in the measurements  there is no input that could have generated the observed output.
A natural approach to cope with such situations consists in searching for the input providing the
closest possible output to the observed data. Then  a major issue is how to choose the metric in the
output space. A standard approach [16] consists in measuring the distance between two elements

1

(a)

(b)

(c)

(d)

(e)

Figure 1: (a) One image from the dinosaur sequence. Camera locations and scene points estimated
by the blind L∞-cost minimization (b c) and by the proposed “outlier aware” procedure (d e).
of the output space in the Euclidean L2-norm. In the structure and motion problem with more than
two cameras  this leads to a hard non-convex optimization problem. A particularly elegant way of
circumventing the non-convexity issues inherent to the use of L2-norm consists in replacing it by the
L∞-norm [15  18  24  25  27  13  26]. It has been shown that  for a number of problems  L∞-norm
based estimators can be computed very efﬁciently using  for example  the iterative bisection method
[18  Algorithm 1  p. 1608] that solves a convex program at each iteration. There is however an
issue with the L∞-techniques that dampens the enthusiasm of practitioners: it is highly sensitive to
outliers (c.f . Fig. 1). In fact  among all Lq-metrics with q ≥ 1  the L∞-metric is the most seriously
affected by the outliers in the data. Two procedures have been introduced [27  19] that make the
L∞-estimator less sensitive to outliers. Although these procedures demonstrate satisfactory empir-
ical performance  they suffer from a lack of sufﬁcient theoretical support assessing the accuracy of
produced estimates.
The purpose of the present work is to introduce and to theoretically investigate a new procedure
of estimation in presence of noise and outliers. Our procedure combines L∞-norm for measuring
the ﬁdelity to the data and L1-norm for regularization. It can be seen as a maximum a posteriori
(MAP) estimator under uniformly distributed random noise and a sparsity favoring prior on the
vector of outliers. Interestingly  this study bridges the work on the robust estimation in multiview
geometry [12  27  19  21] and the theory of sparse recovery in statistics and signal processing [10 
2  5  6].
The rest of the paper is organized as follows. The next section gives the precise formulation of the
translation estimation and triangulation problem to which the presented methodology can be applied.
A brief review of the L∞-norm minimization algorithm is presented in Section 3. In Section 4  we
introduce the statistical framework and derive a new procedure as a MAP estimator. The main result
on the accuracy of this procedure is stated and proved in Section 5  while Section 6 contains some
numerical experiments. The methodology of our study is summarized in Section 7.

2 Translation estimation and triangulation

Let us start by presenting a problem of multiview geometry to which our approach can be success-
fully applied  namely the problem of translation estimation and triangulation in the case of known
rotations. For rotation estimation algorithms  we refer the interested reader to [22  14] and the
references therein.
Let P∗
i   i = 1  . . .   m  be a sequence of m cameras that are known up to a translation. Recall that a
camera is characterized by a 3× 4 matrix P with real entries that can be written as P = K[R|t]  where
K is an invertible 3 × 3 matrix called the camera calibration matrix  R is a 3 × 3 rotation matrix and
t ∈ R3. We will refer to t as the translation of the camera P. We can thus write P∗
i = Ki[Ri|t∗
i ] 
i = 1  . . .   m. For a set of unknown scene points U∗
j    j = 1  . . .   n  expressed in homogeneous
coordinates (i.e.  U∗
j is an element of the projective space P3)  we assume that noisy images of each
U∗
j by some cameras P∗

i are observed. Thus  we have at our disposal the measurements

(cid:20)eT

1 P∗
2 P∗
eT

i U∗
i U∗

xij =

1
3 P∗
i U∗
eT

j

(cid:21)

j

j

+ ξij 

j = 1  . . .   n 

i ∈ Ij 

(1)

where e(cid:96)  (cid:96) = 1  2  3  stands for the unit vector of R3 having one as the (cid:96)th coordinate and Ij is the
j} does not
set of indices of cameras for which the point U∗
contain points at inﬁnity: U∗

j is visible. We assume that the set {U∗
j ∈ R3 and for every j = 1  . . .   n.

j |1]T for some X∗

j = [X∗T

2

∗.

3 P∗

i U∗

m   X∗T

∗ = (t∗T

1   . . .   t∗T

1   . . .   X∗T

j is in front of the camera
j ≥ 0. This is termed cheirality condition. Furthermore  we will assume that none
i . This assumption implies that

We are now in a position to state the problem of translation estimation and triangulation in the
i } (translation estimation)
context of multiview geometry. It consists in recovering the 3-vectors {t∗
and the 3D points {X∗
j} (triangulation) from the noisy measurements {xij; j = 1  . . .   n; i ∈ Ij} ⊂
n )T ∈ R3(m+n). Thus 
R2. In what follows  we use the notation θ
we are interested in estimating θ
Remark 1 (Cheirality). It should be noted right away that if the point U∗
P∗
i   then eT
of the true 3D points U∗
3 P∗
eT
Remark 2 (Identiﬁability). The parameter θ we have just deﬁned is  in general  not identiﬁable from
the measurements {xij}. In fact  one easily checks that  for every α (cid:54)= 0 and for every t ∈ R3  the
parameters {t∗
j + t)} generate the same measurements. To cope with
this issue  we assume that t∗
j = 1. Thus  in what follows we assume
∗ ∈ R3(m+n−1). Further assumptions ensuring the identiﬁability
that t∗
of θ

j} and {α(t∗
1 = 03 and that mini j eT
∗ and θ

j lies on the principal plane of a camera P∗

1 is removed from θ
∗ are given below.

j > 0 so that the quotients eT

j   (cid:96) = 1  2  are well deﬁned.

i − Rit)  α(X∗

3 P∗

i U∗

3 P∗

i U∗

(cid:96) P∗

i U∗

i   X∗

i U∗

j /eT

3 Estimation by Sequential Convex Programming

s =(cid:80)
squared REs. This deﬁnes the estimator(cid:98)θ as a minimizer of the cost function C2 2(θ) =(cid:80)

This section presents results on the estimation of θ based on the reprojection error (RE) minimiza-
tion. This material is essential for understanding the results that are at the core of the present work.
In what follows  for every s ≥ 1  we denote by (cid:107)x(cid:107)s the Ls-norm of a vector x  i.e.(cid:107)x(cid:107)s
j |xj|s
if x = (x1  . . .   xd)T. As usual  we extend this to s = +∞ by setting (cid:107)x(cid:107)∞ = maxj |xj|.
A classical method [16] for estimating the parameter θ is based on minimizing the sum of the
i j (cid:107)xij−
3 PiUj is the 2-vector that we would obtain if θ

2  where xij(θ) :=(cid:2)eT

xij(θ)(cid:107)2
were the true parameter. It can also be written as

(cid:3)T/eT

1 PiUj; eT

2 PiUj

xij(θ) =

(2)
The minimization of C2 2 is a hard nonconvex problem. In general  it does not admit closed-form
solution and the existing iterative algorithms may often get stuck in local minima. An ingenious
idea to overcome this difﬁculty [15  17] is based on the minimization of the L∞ cost function

;

.

1 Ki(RiXj + ti)
3 Ki(RiXj + ti)
eT

2 Ki(RiXj + ti)
eT
3 Ki(RiXj + ti)
eT

(cid:20) eT

C∞ s(θ) = max

j=1 ... n

max
i∈Ij

(cid:107)xij − xij(θ)(cid:107)s 

s ∈ [1  +∞].

(3)

Note that the substitution of the L2-cost function by the L∞-cost function has been proved to lead
to improved algorithms in other estimation problems as well  cf.  e.g.  [8]. This cost function has
a clear practical advantage in that all its sublevel sets are convex. This property ensures that all
minima of C∞ s form a convex set and that an element of this set can be computed by solving
a sequence of convex programs [18]  e.g.  by the bisection algorithm. Note that for s = 1 and
s = +∞  the minimization of C∞ s can be recast in a sequence of LPs. The main idea behind the
bisection algorithm can be summarized as follows. We aim to designate an algorithm computing
cheirality condition. Let us introduce the residuals rij(θ) = xij − xij(θ) that can be represented as

(cid:98)θs ∈ arg minθ C∞ s(θ)  for any prespeciﬁed s ≥ 1  over the set of all vectors θ satisfying the

(cid:21)T

(4)
for some vectors aij(cid:96)  cij ∈ R2. Furthermore  as presented in Remark 2  the cheirality conditions
imply the set of linear constraints cT

ijθ ≥ 1. Thus  the problem of computing(cid:98)θs can be rewritten as

rij(θ) =

;

 

(cid:26)(cid:107)rij(θ)(cid:107)s ≤ γ 

minimize

ijθ ≥ 1.
cT
Note that the inequality (cid:107)rij(θ)(cid:107)s ≤ γ can be replaced by (cid:107)AT
ijθ(cid:107)s ≤ γcT
ijθ with Aij = [aij1; aij2].
Although (5) is not a convex problem  its solution can be well approximated by solving a sequence
of convex feasibility problems.

γ

subject to

(5)

(cid:20) aT

ij1θ
cT
ijθ

aT
ij2θ
cT
ijθ

(cid:21)T

3

4 Robust estimation by linear programming

This and the next sections contain the main theoretical contribution of the present work. We start
with the precise formulation of the statistical model. We then exhibit a prior distribution on the
unknown parameters of the model that leads to a MAP estimator.

(cid:20) aT

∗
∗ ;

ij1θ
cT
ijθ

∗
aT
ij2θ
∗
cT
ijθ

(cid:21)T

4.1 The statistical model
Let us ﬁrst observe that  in view of (1) and (4)  the model we are considering can be rewritten as

= ξij 

j = 1  . . .   n; i ∈ Ij.

Let N = 2(cid:80)n

(6)
j=1 Ij be the total number of measurements and let M = 3(n + m − 1) be the size of
∗. Let us denote by A (resp. C) the M × N matrix formed by the concatenation of the
the vector θ
1). Similarly  let us denote by ξ the N-vector formed by concatenating
column-vectors aij(cid:96) (resp. cij
∗)ξp  p = 1  . . .   N. This
the vectors ξij. In these notation  Eq. (6) is equivalent to aT
p θ
equation deﬁnes the statistical model in the case where there is no outlier. To extend this model to
cover the situation where some outliers are present in the measurements  we introduce the vector
ω∗ ∈ RN deﬁned by ω∗
p = 0 if the pth measurement is an inlier and
p| > 0 otherwise. This leads us to the model:
|ω∗

∗ = (cT
p θ

∗ − (cT
p θ

∗)ξp so that ω∗

p = aT
p θ

where diag(v) stands for the diagonal matrix having the components of v as diagonal entries.

ATθ

∗ = ω∗ + diag(CTθ

∗)ξ 

(7)

∗T; ω∗T]T based on the following prior information:

Statement of the problem: Given the matrices A and C  estimate the parameter-vector
∗ = [θ
β
C1 : Eq. (7) holds with some small noise vector ξ 
C2 : minp cT
p θ
C3 : ω∗ is sparse  i.e.  only a small number of coordinates of ω∗ are different from zero.

∗ = 1 

4.2 Sparsity prior and MAP estimator
∗  we place ourselves in the Bayesian framework. To this
To derive an estimator of the parameter β
end  we impose a probabilistic structure on the noise vector ξ and introduce a prior distribution on
the unknown vector β.
Since the noise ξ represents the difference (in pixels) between the measurements and the true image
points  it is naturally bounded and  generally  does not exceeds the level of a few pixels. Therefore 
it is reasonable to assume that the components of ξ are uniformly distributed in some compact set
of R2  centered at the origin. We assume in what follows that the subvectors ξij of ξ are uniformly
distributed in the square [−σ  σ]2 and are mutually independent. Note that this implies that all the
coordinates of ξ are independent. In practice  this assumption can be enforced by decorrelating the
measurements using the empirical covariance matrix [20]. We deﬁne the prior on θ as the uniform
distribution on the polytope P = {θ ∈ RM : CTθ ≥ 1}  where the inequality is understood compo-
nentwise. The density of this distribution is p1(θ) ∝ 1P(θ)  where ∝ stands for the proportionality
relation and 1P(θ) = 1 if θ ∈ P and 0 otherwise. When P is unbounded  this results in an improper
prior  which is however not a problem for deﬁning the Bayes estimator.
The task of choosing a prior on ω is more delicate in that it should reﬂect the information that ω
is sparse. The most natural prior would be the one having a density which is a decreasing function
of the L0-norm of ω  i.e.  of the number of its nonzero coefﬁcients. However  the computation of
estimators based on this type of priors is NP-hard. An approach for overcoming this difﬁculty relies
on using the L1-norm instead of the L0-norm. Following this idea  we deﬁne the prior distribution
on ω by the probability density p2(ω) ∝ f((cid:107)ω(cid:107)1)  where f is some decreasing function2 deﬁned
on [0 ∞). Assuming in addition that θ and ω are independent  we get the following prior on β:

π(β) = π(θ; ω) ∝ 1P(θ) · f((cid:107)ω(cid:107)1).

(8)

1To get a matrix of the same size as A  in the matrix C each column is duplicated two times.
2The most common choice is f (x) = e−x corresponding to the multivariate Laplace density.

4

Theorem 1. Assume that the noise ξ has independent entries which are uniformly distributed in

[−σ  σ] for some σ > 0  then the MAP estimator(cid:98)β = [(cid:98)θT;(cid:98)ωT]T based on the prior π deﬁned by Eq.

(8) is the solution of the optimization problem:

(cid:26)|aT

p θ − ωp| ≤ σcT
p θ ≥ 1  ∀p.
cT

p θ  ∀p

minimize

(cid:107)ω(cid:107)1

subject to

(9)

The proof of this theorem is a simple exercise and is left to the reader.
Remark 3 (Condition C2). One easily checks that any solution of (9) satisﬁes condition C2. Indeed 

if for some solution(cid:98)β it were not the case  then ˜β = (cid:98)β/ minp cT
p(cid:98)θ would satisfy the constraints of
(9) and ˜ω would have a smaller L1-norm than that of(cid:98)ω  which is in contradiction with the fact that
(cid:98)β solves (9).
Remark 4 (The role of σ). In the deﬁnition of(cid:98)β  σ is a free parameter that can be interpreted as
the level of separation of inliers from outliers. The proposed algorithm implicitly assumes that all
the measurements xij for which (cid:107)ξij(cid:107)∞ > σ are outliers  while all the others are treated as inliers.
σ and to deﬁne the estimator(cid:98)β as a MAP estimator based on the prior incorporating the uncertainty
If σ is unknown  a reasonable way of acting is to impose a prior distribution on the possible values of

on σ. When there are no outliers and the prior on σ is decreasing  this approach leads to the estimator
minimizing the L∞ cost function. In the presence of outliers  the shape of the prior on σ becomes
more important for the deﬁnition of the estimator. This is an interesting point for future investigation.

4.3 Two-step procedure
Building on the previous arguments  we introduce the following two-step algorithm.

Step 1: Compute [(cid:98)θT;(cid:98)ωT]T as a solution to (9) and set J = {p :(cid:98)ωp = 0} .

Input: {ap  cp; p = 1  . . .   N} and σ.
Step 2: Apply the bisection algorithm to the reduced data set {xp; p ∈ J}.

Two observations are in order. First  when applying the bisection algorithm at Step 2  we can use

C∞ s((cid:98)θ) as the initial value of γu. The second observation is that a better way of acting would be to
p(cid:98)θ)−1; p = 1  . . .   N}.

minimize the weighted L1-norm of ω  where the weight assigned to ωp is inversely proportional to
∗ is unknown  a reasonable strategy consists in adding a step in between Step
the depth cT
1 and Step 2  which performs the weighted minimization with weights {(cT

∗. Since θ

p θ

5 Accuracy of estimation
Let us introduce some additional notation. Recall the deﬁnition of P and set ∂P = {θ : minp cT
p θ =
1} and ∆P∗ = {θ − θ
(cid:48) ∈ ∂P  θ (cid:54)= θ}. For every subset of indices J ⊂ {1  . . .   N}  we
denote by AJ the M ×N matrix obtained from A by replacing the columns that have an index outside
J by zero. Furthermore  let us deﬁne

(cid:48) : θ  θ

δJ(θ) =

sup

θ(cid:48)∈∂P ATθ(cid:48)(cid:54)=ATθ

(cid:107)AT
J(θ
(cid:107)AT(θ

(cid:48) − θ)(cid:107)2
(cid:48) − θ)(cid:107)2

 

∀J ⊂ {1  . . .   N} 

∀θ ∈ ∂P.

(10)

One easily checks that δJ ∈ [0  1] and δJ ≤ δJ(cid:48) if J ⊂ J(cid:48).
Assumption A: The real number λ deﬁned by λ = ming∈∆P∗ (cid:107)ATg(cid:107)2/(cid:107)g(cid:107)2 is strictly positive.
∗ even in the case without outliers.
Assumption A is necessary for identifying the parameter vector θ
In fact  if ω∗ = 0  and if Assumption A is not fulﬁlled  then3 ∃ g ∈ ∆P∗ such that ATg = 0. That
is  given the matrices A and C  there are two distinct vectors θ1 and θ2 in ∂P such that ATθ1 = ATθ2.
Therefore  if eventually θ1 is the true parameter vector satisfying C1 and C3  then θ2 satisﬁes these
conditions as well. As a consequence  the true vector cannot be accurately estimated.

3We assume for simplicity that ∂P is compact.

5

5.1 The noise free case
To evaluate the quality of estimation  we ﬁrst place ourselves in the case where σ = 0. The estimator

∗ is then deﬁned as a solution to the optimization problem

(cid:98)β of β

(11)

min(cid:107)ω(cid:107)1

over β =

s.t.

(cid:20)θ

(cid:21)

ω

(cid:26)ATθ = ω

CTθ ≥ 1 .

where ω∗

0 ). If δT0(θ

From now on  for every index set T and for every vector h  hT stands for the vector equal to h on
an index set T and zero elsewhere. The complementary set of T will be denoted by T c.
Theorem 2. Let Assumption A be fulﬁlled and let T0 (resp. T1) denote the index set corresponding
∗) + δT0∪T1(θ
∗) < 1 then 

for some constant C0  it holds:

to the locations of S largest entries4 of ω∗ (resp. (ω∗ −(cid:98)ω)T c
S(cid:107)1 

(cid:107)(cid:98)β − β
has no more than S nonzero entries  then the estimation is exact: (cid:98)β = β
Proof. We set h = ω∗ −(cid:98)ω and g = θ

∗(cid:107)2 ≤ C0(cid:107)ω∗ − ω∗

(12)
S stands for the vector ω∗ with all but the S-largest entries set to zero. In particular  if ω∗

∗ −(cid:98)θ. It follows from Remark 3 that g ∈ ∆P. To proceed

with the proof  we need the following auxiliary result  the proof of which can be easily deduced
from [4].
Lemma 1. Let v ∈ Rd be some vector and let S ≤ d be a positive integer. If we denote by T the
indices of S largest entries of the vector |v|  then (cid:107)vT c(cid:107)2 ≤ S−1/2(cid:107)v(cid:107)1.
Applying Lemma 1 to the vector v = hT c

0 and to the index set T = T1  we get

∗.

(cid:107)h(T0∪T1)c(cid:107)2 ≤ S−1/2(cid:107)hT c

0 (cid:107)1.

T c
0

(cid:107)hT c

(cid:107)1 and (cid:107)ω∗

0 (cid:107)1 + (cid:107)ω∗

0 (cid:107)1 ≤ (cid:107)(ω∗−h)T c

T0(cid:107)1 ≤ (cid:107)(cid:98)ω(cid:107)1 + (cid:107)ω∗

(13)
T0(cid:107)1 ≤
0 (cid:107)1 = (cid:107)ω∗ − h(cid:107)1 =
∗ satisﬁes the constraints of the optimization problem (11) a solution of which is(cid:98)β  we have
(cid:107)h(T0∪T1)c(cid:107)2 ≤ S−1/2(cid:107)hT0(cid:107)1 + 2S−1/2(cid:107)ω∗

On the other hand  summing up the inequalities (cid:107)hT c
0 (cid:107)1 +(cid:107)ω∗
(cid:107)(cid:98)ω(cid:107)1  we get
(cid:107)(ω∗ − h)T0(cid:107)1 +(cid:107)hT0(cid:107)1  and using the relation (cid:107)(ω∗ − h)T0(cid:107)1 +(cid:107)(ω∗ − h)T c
(cid:107)1 + (cid:107)hT0(cid:107)1.
(cid:107)(cid:98)ω(cid:107)1 ≤ (cid:107)ω∗(cid:107)1. This inequality  in conjunction with (13) and (14)  implies
both(cid:98)β and β
(cid:107)h(cid:107)2 ≤ (cid:107)hT0∪T1(cid:107)2 + (cid:107)h(T0∪T1)c(cid:107)2 ≤ (cid:107)hT0∪T1(cid:107)2 + (cid:107)hT0(cid:107)2 + 2S−1/2(cid:107)ω∗

where the last step follows from the Cauchy-Schwartz inequality. Using once again the fact that

∗ satisfy the constraints of (11)  we get h = ATg. Therefore 

(cid:107)1 ≤ (cid:107)hT0(cid:107)2 + 2S−1/2(cid:107)ω∗

Since β

(cid:107)1 

(14)

(15)

(cid:107)1

T c
0

T c
0

T c
0

(cid:107)1 ≤ (δ2S + δS)(cid:107)ATg(cid:107)2 + 2S−1/2(cid:107)ω∗

T0∪T1g(cid:107)2 + (cid:107)AT

= (cid:107)AT
= (δ2S + δS)(cid:107)h(cid:107)2 + 2S−1/2(cid:107)ω∗

T0g(cid:107)2 + 2S−1/2(cid:107)ω∗

(cid:107)1.

Since ω∗
To complete the proof  it sufﬁces to observe that

= ω∗ − ωS  the last inequality yields (cid:107)h(cid:107)2 ≤(cid:0)2S−1/2/(1 − δS − δ2S)(cid:1)(cid:107)ω∗ − ω∗
∗(cid:107)2 ≤ (cid:107)g(cid:107)2 + (cid:107)h(cid:107)2 ≤ λ−1(cid:107)Ag(cid:107)2 + (cid:107)h(cid:107)2 =(cid:0)λ−1 + 1(cid:1)(cid:107)h(cid:107)2 ≤ C0(cid:107)ω∗ − ω∗

(cid:107)(cid:98)β − β

S(cid:107)1.

(cid:107)1
(16)
S(cid:107)1.

T c
0

T c
0

T c
0

T c
0

T c
0

∗) < 1 is close in spirit to the restricted isometry
Remark 5. The assumption δT0(θ
assumption (cf.  e.g.  [10  6  3] and the references therein). It is very likely that results similar to
that of Theorem 2 hold under other kind of assumptions recently introduced in the theory of L1-
minimization [11  29  2]. This investigation is left for future research.

∗) + δT0∪T1(θ

then max((cid:107)(cid:98)ω − ω∗(cid:107)2 (cid:107)AT((cid:98)θ − θ

We emphasize that the constant C0 is rather small. For example  if δT0(θ

√
∗)(cid:107)2) ≤ (4/
S)(cid:107)ω∗ − ω∗

S(cid:107)1.

∗) + δT0∪T1(θ

∗) = 0.5 

4in absolute value

6

5.2 The noisy case
The assumption σ = 0 is an idealization of the reality that has the advantage of simplifying the
mathematical derivations. While such a simpliﬁed setting is useful for conveying the main ideas
behind the proposed methodology  it is of major practical importance to discuss the extensions to the

more realistic noisy model. To this end  we introduce the vector(cid:98)ξ of estimated residuals satisfying
AT(cid:98)θ = (cid:98)ω + diag(CT(cid:98)θ)(cid:98)ξ and (cid:107)(cid:98)ξ(cid:107)∞ ≤ σ.
max((cid:107) diag(CT(cid:98)θ)(cid:98)ξ(cid:107)2;(cid:107) diag(CTθ
(cid:107)(cid:98)β − β

Theorem 3. Let the assumptions of Theorem 2 be fulﬁlled.

If for some  > 0 we have

∗(cid:107)2 ≤ C0(cid:107)ω∗ − ω∗

∗)ξ(cid:107)2) ≤   then

S(cid:107)1 + C1

(17)

where C0 and C1 are some constants.

∗)ξ and(cid:98)η = diag(CT(cid:98)θ)(cid:98)ξ. On the one hand  in view of (15) 
(cid:107)1 with h = ω∗ −(cid:98)ω. On the other hand  since

Proof. Let us deﬁne η = diag(CTθ
h = ATg +(cid:98)η − η  we have
we have (cid:107)h(T0∪T1)c(cid:107)2 ≤ (cid:107)hT0(cid:107)2 + 2S−1/2(cid:107)ω∗
(cid:107)h(T0∪T1)c(cid:107)2 ≥ (cid:107)AT
T0g(cid:107)2 + (cid:107)(cid:98)ηT0(cid:107)2 + (cid:107)ηT0(cid:107)2 ≤ (cid:107)AT
and (cid:107)hT0(cid:107)2 ≤ (cid:107)AT
T0∪T1g(cid:107)2 + (cid:107)AT

(T0∪T1)cg(cid:107)2 − (cid:107)(cid:98)η(T0∪T1)c(cid:107)2 − (cid:107)η(T0∪T1)c(cid:107)2 ≥ (cid:107)AT
(T0∪T1)cg(cid:107)2 − 2
T0g(cid:107)2 + 2. These inequalities imply that
T0g(cid:107)2 + 4 + 2S−1/2(cid:107)ω∗
≤ (δT0∪T1 + δT0)(cid:107)ATg(cid:107)2 + 4 + 2S−1/2(cid:107)ω∗

(cid:107)ATg(cid:107)2 ≤ (cid:107)AT

(cid:107)1
(cid:107)1.

T c
0

T c
0

T c
0

To complete the proof  it sufﬁces to remark that

(cid:107)(cid:98)β − β

∗(cid:107)2 ≤ (cid:107)h(cid:107)2 + (cid:107)g(cid:107)2 ≤ (cid:107)AT g(cid:107)2 + (cid:107)g(cid:107)2 + 2 ≤ (1 + λ−1)(cid:107)ATg(cid:107)2 + 2

≤

1+λ−1

1−δT0∪T1−δT0

(4 + 2S−1/2(cid:107)ω∗

(cid:107)1).

T c
0

θ

θ

T c
0

T c
0

∗ = AT

∗)+ δT0∪T1(θ

(cid:48) ∈ ∂P such that AT

∗) < 1 is necessary for θ
∗ and θ

∗ to be consistently estimated. Indeed  if δT0(θ
(cid:48) satisfy (7) with the same number of outliers.

5.3 Discussion
∗) < 1. While this assumption
The main assumption in Theorems 2 and 3 is that δT0(θ
is by no means necessary  it should be recognized that it cannot be signiﬁcantly relaxed. In fact  the
∗) = 1 
condition δT0(θ
(cid:48)  which makes the problem of robust
then it is possible to ﬁnd θ
estimation ill-posed  since both θ
Note also that the mapping J (cid:55)→ δJ(θ) is subadditive  that is δJ ∪ J(cid:48)(θ) ≤ δJ(θ) + δJ(cid:48)(θ).
∗) < 1/3 for every index set J of
Therefore  the condition of Thm. 2 is fulﬁlled as soon as δJ(θ
cardinality ≤ S. Thus  the condition maxJ:|J|≤S δS(θ
∗ in
presence of S outliers  while maxJ:|J|≤S δS(θ
A simple upper bound on δJ  obtained by replacing the sup over ∂P by the sup over RM   is δJ(θ) ≤
J(cid:107)  ∀θ ∈ ∂P  where O = O(A) stands for the Rank(A)×N matrix with orthonormal rows spanning
(cid:107)OT
the image of AT. The matrix norm is understood as the largest singular value. Note that for a given
J  the computation of (cid:107)OT
We emphasize that the model we have investigated comprises the robust linear model as a particular
case. Indeed  if the last row of the matrix A is equal to zero as well as all the rows of C except the
last row which that has all the entries equal to one  then the model described by (7) is nothing else
but a linear model with unknown noise variance.
To close this section  let us stress that other approaches (cf.  for instance  [9  7  1]) recently intro-
duced in sparse learning and estimation may potentially be useful for the problem of robust estima-
tion.

∗) < 1/3 is sufﬁcient for identifying θ

J(cid:107) is far easier than that of δJ(θ).

∗) < 1 is necessary.

6 Numerical illustration

We implemented the algorithm in MatLab  using the SeDuMi package for solving LPs [28]. We
applied our algorithm of robust estimation to the well-known dinosaur sequence 5. which consists

5http://www.robots.ox.ac.uk/˜vgg/data1.html

7

Figure 2: (a)-(c) Overhead view of the scene points estimated by the KK-procedure (a)  by the SH-
procedure (b) and by our procedure. (d) Boxplots of the errors when estimating the camera centers
by our procedure (left) and by the KK-procedure. (e) Boxplots of the errors when estimating the
camera centers by our procedure (left) and by the SH-procedure.
of 36 images of a dinosaur on a turntable  see Fig. 1 (a) for one example. The 2D image points
which are tracked across the image sequence and the projection matrices of 36 cameras are provided
as well. There are 16 432 image points corresponding to 4 983 scene points. This data is severely
affected by outliers which results in a very poor accuracy of the “blind” L∞-cost minimization
procedure. Its maximal RE equals 63 pixel and  as shown in Fig. 1  the estimated camera centers are
not on the same plane and the scatter plot of scene points is inaccurate.
p θ| was larger than σ/4 
We ran our procedure with σ = 0.5 pixel. If for pth measurement |ωp/cT
then the it has been considered is an outlier and removed from the dataset. The corresponding 3D
scene point was also removed if  after the step of outlier removal  it was seen by only one camera.
This resulted in removing 1  306 image points and 297 scene points. The plots (d) and (e) of Fig. 1
show the estimated camera centers and estimated scene points. We see  in particular  that the camera
centers are almost coplanar. Note that in this example  the second step of the procedure described in
Section 4.3 does not improve on the estimator computed at the ﬁrst step. Thus  an accurate estimate
is obtained by solving only one linear program.
We compared our procedure with the procedures proposed by Sim and Hartley [27]  hereafter
referred to as SH-procedure  and by Kanade and Ke [19]  hereafter KK-procedure. For the SH-
procedure  we iteratively computed the L∞-cost minimizer by removing  at each step j  the mea-
surements that had a RE larger than Emax j − 0.5  where Emax j was the largest RE. We have
stopped the SH-procedure when the number of removed measurements exceeded 1 500. This num-
ber has been attained after 53 cycles. Therefore  the execution time was approximately 50 times
larger than for our procedure. The estimator obtained by SH-procedure has a maximal RE equal
to 1.33 pixel  whereas the maximal RE for our estimator is of 0.62 pixel. Concerning the KK-
procedure  we run it with the parameter value m = N − NO = 15  000  which is approximately
the number of inliers detected by our method. Recall that the KK-procedure aims at minimizing the
mth largest RE. As shown in Fig. 2  our procedure performs better than that of [19].

7 Conclusion

In this paper  we presented a rigorous Bayesian framework for the problem of translation estima-
tion and triangulation that have leaded to a new robust estimation procedure. We have formulated
the problem under consideration as a nonlinear inverse problem with a high-dimensional unknown
parameter-vector. This parameter-vector encapsulates the information on the scene points and the
camera locations  as well as the information on the location of outliers in the data. The proposed
estimator exploits the sparse nature of the vector of outliers through L1-norm minimization. We
have given the mathematical proof of the result demonstrating the efﬁciency of the proposed esti-
mator under mild assumptions. Real data analysis conducted on the dinosaur sequence supports our
theoretical results.

Acknowledgments

The work of the ﬁrst author was partially supported by ANR under grants Callisto and Parcimonie.

8

References
[1] F. Bach. Bolasso: model consistent Lasso estimation through the bootstrap. In Twenty-ﬁfth International

Conference on Machine Learning (ICML)  2008. 7

[2] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of lasso and Dantzig selector. Ann.

Statist.  37(4):1705–1732  2009. 2  6

[3] E. Cand`es and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. Ann.

Statist.  35(6):2313–2351  2007. 6

[4] E. J. Cand`es. The restricted isometry property and its implications for compressed sensing. C. R. Math.

Acad. Sci. Paris  346(9-10):589–592  2008. 6

[5] E. J. Cand`es and P. A. Randall. Highly robust error correction by convex programming. IEEE Trans.

Inform. Theory  54(7):2829–2840  2008. 2

[6] E. J. Cand`es  J. K. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate measure-

ments. Comm. Pure Appl. Math.  59(8):1207–1223  2006. 2  6

[7] C. Chesneau and M. Hebiri. Some theoretical results on the grouped variables Lasso. Math. Methods

Statist.  17(4):317–326  2008. 7

[8] A. S. Dalalyan  A. Juditsky  and V. Spokoiny. A new algorithm for estimating the effective dimension-

reduction subspace. Journal of Machine Learning Research  9:1647–1678  Aug. 2008. 3

[9] A. S. Dalalyan and A. B. Tsybakov. Aggregation by exponential weighting  sharp PAC-bayesian bounds

and sparsity. Machine Learning  72(1-2):39–61  2008. 7

[10] D. Donoho  M. Elad  and V. Temlyakov. Stable recovery of sparse overcomplete representations in the

presence of noise. IEEE Trans. Inform. Theory  52(1):6–18  2006. 2  6

[11] D. L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Trans. Inform.

Theory  47(7):2845–2862  2001. 6

[12] O. Enqvist and F. Kahl. Robust optimal pose estimation. In ECCV  pages I: 141–153  2008. 2
[13] R. Hartley and F. Kahl. Optimal algorithms in multiview geometry. In ACCV  volume 1  pages 13 – 34 

Nov. 2007. 2

[14] R. Hartley and F. Kahl. Global optimization through rotation space search. IJCV  2009. 2
[15] R. I. Hartley and F. Schaffalitzky. L∞ minimization in geometric reconstruction problems. In CVPR (1) 

pages 504–509  2004. 2  3

[16] R. I. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University

Press  June 2004. 1  3

[17] F. Kahl. Multiple view geometry and the L∞-norm. In ICCV  pages 1002–1009. IEEE Computer Society 

2005. 3

[18] F. Kahl and R. I. Hartley. Multiple-view geometry under the L∞ norm. IEEE Trans. Pattern Analysis and

Machine Intelligence  30(9):1603–1617  sep 2008. 2  3

[19] T. Kanade and Q. Ke. Quasiconvex optimization for robust geometric reconstruction. In ICCV  pages II:

986–993  2005. 2  8

[20] Q. Ke and T. Kanade. Uncertainty models in quasiconvex optimization for geometric reconstruction. In

CVPR  pages I: 1199–1205  2006. 4

[21] H. D. Li. A practical algorithm for L∞ triangulation with outliers. In CVPR  pages 1–8  2007. 2
[22] D. Martinec and T. Pajdla. Robust rotation and translation estimation in multiview reconstruction. In

CVPR  pages 1–8  2007. 2

[23] D. Nist´er. An efﬁcient solution to the ﬁve-point relative pose problem. IEEE Trans. Pattern Anal. Mach.

Intell  26(6):756–777  2004. 1

[24] C. Olsson  A. P. Eriksson  and F. Kahl. Efﬁcient optimization for L∞ problems using pseudoconvexity.

In ICCV  pages 1–8  2007. 2

[25] Y. D. Seo and R. I. Hartley. A fast method to minimize L∞ error norm for geometric vision problems. In

ICCV  pages 1–8  2007. 2

[26] Y. D. Seo  H. J. Lee  and S. W. Lee. Sparse structures in L-inﬁnity norm minimization for structure and

motion reconstruction. In ECCV  pages I: 780–793  2008. 2

[27] K. Sim and R. Hartley. Removing outliers using the L∞ norm. In CVPR  pages I: 485–494  2006. 2  8
[28] J. F. Sturm. Using SeDuMi 1.02  a MATLAB toolbox for optimization over symmetric cones. Optim.

Methods Softw.  11/12(1-4):625–653  1999. 7

[29] P. Zhao and B. Yu. On model selection consistency of Lasso. J. Mach. Learn. Res.  7:2541–2563  2006.

6

9

,Ohad Shamir
Peng Wang
Xiaohui Shen
Bryan Russell
Scott Cohen
Alan Yuille