2015,Efficient Learning of Continuous-Time Hidden Markov Models for Disease Progression,The Continuous-Time Hidden Markov Model (CT-HMM) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time. However  the lack of an efficient parameter learning algorithm for CT-HMM restricts its use to very small models or requires unrealistic constraints on the state transitions. In this paper  we present the first complete characterization of efficient EM-based learning methods for CT-HMM models. We demonstrate that the learning problem consists of two challenges: the estimation of posterior state probabilities and the computation of end-state conditioned statistics. We solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden Markov model. The second challenge is addressed by adapting three approaches from the continuous time Markov chain literature to the CT-HMM domain. We demonstrate the use of CT-HMMs with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an Alzheimer's disease dataset.,Efﬁcient Learning of Continuous-Time Hidden

Markov Models for Disease Progression

Yu-Ying Liu  Shuang Li  Fuxin Li  Le Song  and James M. Rehg

Atlanta  GA

College of Computing

Georgia Institute of Technology

Abstract

The Continuous-Time Hidden Markov Model (CT-HMM) is an attractive ap-
proach to modeling disease progression due to its ability to describe noisy ob-
servations arriving irregularly in time. However  the lack of an efﬁcient parameter
learning algorithm for CT-HMM restricts its use to very small models or requires
unrealistic constraints on the state transitions. In this paper  we present the ﬁrst
complete characterization of efﬁcient EM-based learning methods for CT-HMM
models. We demonstrate that the learning problem consists of two challenges: the
estimation of posterior state probabilities and the computation of end-state con-
ditioned statistics. We solve the ﬁrst challenge by reformulating the estimation
problem in terms of an equivalent discrete time-inhomogeneous hidden Markov
model. The second challenge is addressed by adapting three approaches from the
continuous time Markov chain literature to the CT-HMM domain. We demon-
strate the use of CT-HMMs with more than 100 states to visualize and predict
disease progression using a glaucoma dataset and an Alzheimer’s disease dataset.

Introduction

1
The goal of disease progression modeling is to learn a model for the temporal evolution of a disease
from sequences of clinical measurements obtained from a longitudinal sample of patients. By dis-
tilling population data into a compact representation  disease progression models can yield insights
into the disease process through the visualization and analysis of disease trajectories. In addition 
the models can be used to predict the future course of disease in an individual  supporting the de-
velopment of individualized treatment schedules and improved treatment efﬁciencies. Furthermore 
progression models can support phenotyping by providing a natural similarity measure between
trajectories which can be used to group patients based on their progression.
Hidden variable models are particularly attractive for modeling disease progression for three rea-
sons: 1) they support the abstraction of a disease state via the latent variables; 2) they can deal with
noisy measurements effectively; and 3) they can easily incorporate dynamical priors and constraints.
While conventional hidden Markov models (HMMs) have been used to model disease progression 
they are not suitable in general because they assume that measurement data is sampled regularly
at discrete intervals. However  in reality patient visits are irregular in time  as a consequence of
scheduling issues  missed visits  and changes in symptomatology.
A Continuous-Time HMM (CT-HMM) is an HMM in which both the transitions between hidden
states and the arrival of observations can occur at arbitrary (continuous) times [1  2]. It is therefore
suitable for irregularly-sampled temporal data such as clinical measurements [3  4  5]. Unfortu-
nately  the additional modeling ﬂexibility provided by CT-HMM comes at the cost of a more com-
plex inference procedure. In CT-HMM  not only are the hidden states unobserved  but the transition
times at which the hidden states are changing are also unobserved. Moreover  multiple unobserved
hidden state transitions can occur between two successive observations. A previous method ad-
dressed these challenges by directly maximizing the data likelihood [2]  but this approach is limited

1

to very small model sizes. A general EM framework for continuous-time dynamic Bayesian net-
works  of which CT-HMM is a special case  was introduced in [6]  but that work did not address the
question of efﬁcient learning. Consequently  there is a need for efﬁcient CT-HMM learning methods
that can scale to large state spaces (e.g. hundreds of states or more) [7].
A key aspect of our approach is to leverage the existing literature for continuous time Markov chain
(CTMC) models [8  9  10]. These models assume that states are directly observable  but retain
the irregular distribution of state transition times. EM approaches to CTMC learning compute the
expected state durations and transition counts conditioned on each pair of successive observations.
The key computation is the evaluation of integrals of the matrix exponential (Eqs. 12 and 13). Prior
work by Wang et. al. [5] used a closed form estimator due to [8] which assumes that the transition
rate matrix can be diagonalized through an eigendecomposition. Unfortunately  this is frequently not
achievable in practice  limiting the usefulness of the approach. We explore two additional CTMC ap-
proaches [9] which use (1) an alternative matrix exponential on an auxillary matrix (Expm method);
and (2) a direct truncation of the inﬁnite sum expansion of the exponential (Unif method). Neither
of these approaches have been previously exploited for CT-HMM learning.
We present the ﬁrst comprehensive framework for efﬁcient EM-based parameter learning in CT-
HMM  which both extends and uniﬁes prior work on CTMC models. We show that a CT-HMM can
be conceptualized as a time-inhomogenous HMM which yields posterior state distributions at the
observation times  coupled with CTMCs that govern the distribution of hidden state transitions be-
tween observations (Eqs. 9 and 10). We explore both soft (forward-backward) and hard (Viterbi de-
coding) approaches to estimating the posterior state distributions  in combination with three methods
for calculating the conditional expectations. We validate these methods in simulation and evaluate
our approach on two real-world datasets for glaucoma and Alzheimer’s disease  including visual-
izations of the progression model and predictions of future progression. Our approach outperforms
a state-of-the-art method [11] for glaucoma prediction  which demonstrates the practical utility of
CT-HMM for clinical data modeling.
2 Continuous-Time Markov Chain
A continuous-time Markov chain (CTMC) is deﬁned by a ﬁnite and discrete state space S  a state
transition rate matrix Q  and an initial state probability distribution π. The elements qij in Q describe
the rate the process transitions from state i to j for i (cid:54)= j  and qii are speciﬁed such that each row of
j(cid:54)=i qij  qii = −qi) [1]. In a time-homogeneous process  in which the qij
are independent of t  the sojourn time in each state i is exponentially-distributed with parameter qi 
which is f (t) = qie−qit with mean 1/qi. The probability that the process’s next move from state i is
to state j is qij/qi. When a realization of the CTMC is fully observed  meaning that one can observe
every transition time (t(cid:48)
0)  ...  yV (cid:48) =
s(t(cid:48)

V (cid:48))}  where s(t) denotes the state at time t  the complete likelihood (CL) of the data is

Q sums to zero (qi =(cid:80)

V (cid:48))  and the corresponding state Y (cid:48) = {y0 = s(t(cid:48)

(qyv(cid:48)  yv(cid:48)+1 /qyv(cid:48) )(qyv(cid:48) e
v(cid:48)+1 − t(cid:48)

v(cid:48) is the time interval between two transitions  nij is the number of transitions

V (cid:48)−1(cid:89)
v(cid:48)=0
where τv(cid:48) = t(cid:48)
from state i to j  and τi is the total amount of time the chain remains in state i.
In general  a realization of the CTMC is observed only at discrete and irregular time points
(t0  t1  ...  tV )  corresponding to a state sequence Y   which are distinct from the switching times.
As a result  the Markov process between two consecutive observations is hidden  with potentially
many unobserved state transitions. Thus both nij and τi are unobserved. In order to express the
likelihood of the incomplete observations  we can utilize a discrete time hidden Markov model by
deﬁning a state transition probability matrix for each distinct time interval t  P (t) = eQt  where
Pij(t)  the entry (i  j) in P (t)  is the probability that the process is in state j after time t given that
it is in state i at time 0. This quantity takes into account all possible intermediate state transitions
and timing between i and j which are not observed. Then the likelihood of the data is

V (cid:48)−1(cid:89)

1  . . .   t(cid:48)

qyv(cid:48)  yv(cid:48)+1 e

v(cid:48) τv(cid:48) ) =

|S|(cid:89)

|S|(cid:89)

v(cid:48) τv(cid:48) =

qnij
ij e

0  t(cid:48)

i=1

j=1 j(cid:54)=i

−qiτi

CL =

v(cid:48)=0

−qy

−qy

(1)

V −1(cid:89)

V −1(cid:89)

|S|(cid:89)

r(cid:89)

|S|(cid:89)

v=0

L =

Pyv  yv+1 (τv) =

(2)
where τv = tv+1 − tv is the time interval between two observations  I(yv = i  yv+1 = j) is an
indicator function that is 1 if the condition is true  otherwise it is 0  τ∆  ∆ = 1  ...  r  represents r
unique values among all time intervals τv  and C(τ = τ∆  yv = i  yv+1 = j) is the total counts

Pij(τ∆)C(τ =τ∆ yv =i yv+1=j)

Pij(τv)

i j=1

i j=1

∆=1

v=0

I(yv =i yv+1=j) =

2

hood takes the form(cid:80)|S|

from all successive visits when the condition is true. Note that there is no analytic maximizer of L 
due to the structure of the matrix exponential  and direct numerical maximization with respect to Q
is computationally challenging. This motivates the use of an EM-based approach.
(cid:80)|S|
An EM algorithm for CTMC is described in [8]. Based on Eq. 1  the expected complete log likeli-
j=1 j(cid:54)=i{log(qij)E[nij|Y  ˆQ0]−qiE[τi|Y  ˆQ0]}  where ˆQ0 is the current
estimate for Q  and E[nij|Y  ˆQ0] and E[τi|Y  ˆQ0] are the expected state transition count and total
duration given the incomplete observation Y and the current transition rate matrix ˆQ0  respectively.
Once these two expectations are computed in the E-step  the updated ˆQ parameters can be obtained
via the M-step as

i=1

ˆqij =

E[nij|Y  ˆQ0]
E[τi|Y  ˆQ0]

  i (cid:54)= j and ˆqii = −(cid:88)

j(cid:54)=i

ˆqij.

(3)

Now the main computational challenge is to evaluate E[nij|Y  ˆQ0] and E[τi|Y  ˆQ0]. By exploiting
the properties of the Markov process  the two expectations can be decomposed as [12]:

E[nij|Y  ˆQ0] =

E[nij|yv  yv+1  ˆQ0] =

I(yv = k  yv+1 = l)E[nij|yv = k  yv+1 = l  ˆQ0]

V −1(cid:88)
V −1(cid:88)

v=0

V −1(cid:88)
|S|(cid:88)
|S|(cid:88)
V −1(cid:88)

v=0

k l=1

E[τi|Y  ˆQ0] =

E[τi|yv  yv+1  ˆQ0] =

I(yv = k  yv+1 = l)E[τi|yv = k  yv+1 = l  ˆQ0]

v=0

v=0

k l=1

where I(yv = k  yv+1 = l) = 1 if the condition is true  otherwise it is 0. Thus  the computation
reduces to computing the end-state conditioned expectations E[nij|yv = k  yv+1 = l  ˆQ0] and
E[τi|yv = k  yv+1 = l  ˆQ0]  for all k  l  i  j ∈ S. These expectations are also a key step in CT-HMM
learning  and Section 4 presents our approach to computing them.

3 Continuous-Time Hidden Markov Model
In this section  we describe the continuous-time hidden Markov model (CT-HMM) for disease pro-
gression and the proposed framework for CT-HMM learning.

3.1 Model Description
In contrast to CTMC  where the states are directly observed  none of the states are directly observed
in CT-HMM. Instead  the available observational data o depends on the hidden states s via the
measurement model p(o|s). In contrast to a conventional HMM  the observations (o0  o1  . . .   oV )
are only available at irregularly-distributed continuous points in time (t0  t1  . . .   tV ). As a conse-
quence  there are two levels of hidden information in a CT-HMM. First  at observation time  the
state of the Markov chain is hidden and can only be inferred from measurements. Second  the state
transitions in the Markov chain between two consecutive observations are also hidden. As a result  a
Markov chain may visit multiple hidden states before reaching a state that emits a noisy observation.
This additional complexity makes CT-HMM a more effective model for event data  in comparison
to HMM and CTMC. But as a consequence the parameter learning problem is more challenging.
We believe we are the ﬁrst to present a comprehensive and systematic treatment of efﬁcient EM
algorithms to address these challenges.
A fully observed CT-HMM contains four sequences of information: the underlying state transition
V (cid:48))} of the hidden
time (t(cid:48)
Markov chain  and the observed data O = (o0  o1  . . .   oV ) at time T = (t0  t1  . . .   tV ). Their joint
complete likelihood can be written as

V (cid:48))  the corresponding state Y (cid:48) = {y0 = s(t(cid:48)
1  . . .   t(cid:48)
|S|(cid:89)
V (cid:48)−1(cid:89)

0)  ...  yV (cid:48) = s(t(cid:48)
V(cid:89)

|S|(cid:89)

V(cid:89)

0  t(cid:48)

−qy

v(cid:48) τv(cid:48)

p(ov|s(tv)) =

−qiτi

qnij
ij e

p(ov|s(tv)).

(4)

v=0

i=1

j=1 j(cid:54)=i

v=0

CL =

qyv(cid:48)  yv(cid:48)+1 e

v(cid:48)=0

We will focus our development on the estimation of the transition rate matrix Q. Estimates for the
parameters of the emission model p(o|s) and the initial state distribution π can be obtained from the
standard discrete time HMM formulation [13]  but with time-inhomogeneous transition probabilities
(described below).

3

3.2 Parameter Estimation
Given a current estimate of the parameter ˆQ0  the expected complete log-likelihood takes the form

L(Q) =

{log(qij)E[nij|O  T  ˆQ0] − qiE[τi|O  T  ˆQ0]} +

E[log p(ov|s(tv))|O  T  ˆQ0]. (5)

|S|(cid:88)

|S|(cid:88)

i=1

j=1 j(cid:54)=i

In the M-step  taking the derivative of L with respect to qij  we have

ˆqij =

E[nij|O  T  ˆQ0]
E[τi|O  T  ˆQ0]

ˆqij.

(6)

V(cid:88)
  i (cid:54)= j and ˆqii = −(cid:88)

v=0

j(cid:54)=i

The challenge lies in the E-step  where we compute the expectations of nij and τi conditioned on the
observation sequence. The statistic for nij can be expressed in terms of the expectations between
successive pairs of observations as follows:

p(s(t1)  ...  s(tV )|O  T  ˆQ0)E[nij|s(t1)  ...  s(tV )  ˆQ0]

p(s(t1)  ...  s(tV )|O  T  ˆQ0)

E[nij|s(tv)  s(tv+1)  ˆQ0]

V −1(cid:88)

v=1

(7)

(8)

E[nij|O  T  ˆQ0] =

=

=

s(t1) ... s(tV )

s(t1) ... s(tV )

V −1(cid:88)

(cid:88)
(cid:88)
|S|(cid:88)

v=1

k l=1

n−1(cid:88)

|S|(cid:88)

v=1

k l=1

p(s(tv) = k  s(tv+1) = l|O  T  ˆQ0)E[nij|s(tv) = k  s(tv+1) = l  ˆQ0].

(9)

In a similar way  we can obtain an expression for the expectation of τi:

E[τi|O  T  ˆQ0] =

p(s(tv) = k  s(tv+1) = l|O  T  ˆQ0)E[τi|s(tv) = k  s(tv+1) = l  ˆQ0].

(10)

In Section 4  we present our approach to computing the end-state conditioned statistics
E[nij|s(tv) = k  s(tv+1) = l  ˆQ0] and E[τi|s(tv) = k  s(tv+1) = l  ˆQ0]. The remaining step
is to compute the posterior state distribution at two consecutive observation times: p(s(tv) =
k  s(tv+1) = l|O  T  ˆQ0).
3.3 Computing the Posterior State Probabilities
The challenge in efﬁciently computing p(s(tv) = k  s(tv+1) = l|O  T  ˆQ0) is to avoid the explicit
enumeration of all possible state transition sequences and the variable time intervals between inter-
mediate state transitions (from k to l). The key is to note that the posterior state probabilities are only
needed at the times where we have observation data. We can exploit this insight to reformulate the
estimation problem in terms of an equivalent discrete time-inhomogeneous hidden Markov model.
Speciﬁcally  given the current estimate ˆQ0  O and T   we will divide the time into V intervals  each
with duration τv = tv − tv−1. We then make use of the transition property of CTMC  and associate
each interval v with a state transition matrix P v(τv) := e ˆQ0τv. Together with the emission model
p(o|s)  we then have a discrete time-inhomogeneous hidden Markov model with joint likelihood:

[P v(τv)](s(tv−1) s(tv ))

p(ov|s(tv)).

(11)

v=1

v=0

The formulation in Eq. 11 allows us to reduce the computation of p(s(tv) = k  s(tv+1) =
l|O  T  ˆQ0) to familiar operations. The forward-backward algorithm [13] can be used to compute the
posterior distribution of the hidden states  which we refer to as the Soft method. Alternatively  the
MAP assignment of hidden states obtained from the Viterbi algorithm can provide an approximate
distribution  which we refer to as the Hard method.
4 EM Algorithms for CT-HMM
Pseudocode for the EM algorithm for CT-HMM parameter learning is shown in Algorithm 1.
Multiple variants of the basic algorithm are possible  depending on the choice of method for
computing the end-state conditioned expectations along with the choice of Hard or Soft decod-
ing for obtaining the posterior state probabilities in Eq. 11. Note that in line 7 of Algorithm 1 

4

V(cid:89)

V(cid:89)

Algorithm 1 CT-HMM Parameter learning (Soft/Hard)
1: Input: data O = (o0  ...  oV ) and T = (t0  . . .   tV )  state set S  edge set L  initial guess of Q
2: Output: transition rate matrix Q = (qij)
3: Find all distinct time intervals t∆  ∆ = 1  ...  r  from T
4: Compute P (t∆) = eQt∆ for each t∆
5: repeat
6:

Compute p(v  k  l) = p(s(tv) = k  s(tv+1) = l|O  T  Q) for all v  and the complete/state-
optimized data likelihood l by using Forward-Backward (soft) or Viterbi (hard)
Create soft count table C(∆  k  l) from p(v  k  l) by summing prob. from visits of same t∆
Use Expm  Unif or Eigen method to compute E[nij|O  T  Q] and E[τi|O  T  Q]
Update qij =

E[τi|O T Q]   and qii = −(cid:80)

7:
8:
E[nij|O T Q]
9:
10: until likelihood l converges

i(cid:54)=j qij

(cid:90) t
(cid:90) t

0

Pk l(t)

1

we group probabilities from successive visits of same time interval and the same speciﬁed end-
states in order to save computation time. This is valid because in a time-homogeneous CT-HMM 
E[nij|s(tv) = k  s(tv+1) = l  ˆQ0] = E[nij|s(0) = k  s(t∆) = l  ˆQ0]  where t∆ = tv+1−tv  so that
the expectations only need to be evaluated for each distinct time interval  rather than each different
visiting time (also see the discussion below Eq. 2).
4.1 Computing the End-State Conditioned Expectations
The remaining step in ﬁnalizing the EM algorithm is to discuss the computation of the end-state
conditioned expectations for nij and τi from Eqs. 9 and 10  respectively. The ﬁrst step is to express
the expectations in integral form  following [14]:
E[nij|s(0) = k  s(t) = l  Q] =

Pk i(x)Pj l(t − x) dx

(12)

qi j

Pk l(t)

0

(cid:21)

k l (t) and τ i i

(cid:20)Q B

Pk i(x)Pi l(t − x) dx.

E[τi|s(0) = k  s(t) = l  Q] =

k l (t) = (cid:82) t

0 Pk i(x)Pj l(t − x)dx = (cid:82) t

It is shown in [15] that (cid:82) t

(13)
0 (eQx)k i(eQ(t−x))j l dx  while
From Eq. 12  we deﬁne τ i j
τ i i
k l(t) can be similarly deﬁned for Eq. 13 (see [6] for a similar construction). Several methods for
computing τ i j
k l(t) have been proposed in the CTMC literature. Metzner et. al. observe
that closed-form expressions can be obtained when Q is diagonalizable [8]. Unfortunately  this
property is not guaranteed to exist  and in practice we ﬁnd that the intermediate Q matrices are
frequently not diagonalizable during EM iterations. We refer to this approach as Eigen.
An alternative is to leverage a classic method of Van Loan [15] for computing integrals of ma-
  where
trix exponentials. In this approach  an auxiliary matrix A is constructed as A =
0 eQxBeQ(t−x)dt =
B is a matrix with identical dimensions to Q.
(eAt)(1:n) (n+1):(2n)  where n is the dimension of Q. Following [9]  we set B = I(i  j)  where
I(i  j) is the matrix with a 1 in the (i  j)th entry and 0 elsewhere. Thus the left hand side reduces to
τ i j
k l (t) for all k  l in the corresponding matrix entries. Thus we can leverage the substantial literature
on numerical computation of the matrix exponential. We refer to this approach as Expm  after the
popular Matlab function. A third approach for computing the expectations  introduced by Hobolth
and Jensen [9] for CTMCs  is called uniformization (Unif ) and is described in the supplementary
material  along with additional details for Expm.
Expm Based Algorithm Algorithm 2 presents pseudocode for the Expm method for computing
end-state conditioned statistics. The algorithm exploits the fact that the A matrix does not change
with time t∆. Therefore  when using the scaling and squaring method [16] for computing matrix
exponentials  one can easily cache and reuse the intermediate powers of A to efﬁciently compute
etA for different values of t.
4.2 Analysis of Time Complexity and Run-Time Comparisons
We conducted asymptotic complexity analysis for all six combinations of Hard and Soft EM with
the methods Expm  Unif  and Eigen for computing the conditional expectations. For both hard and

0 Q

5

Algorithm 2 The Expm Algorithm for Computing End-State Conditioned Statistics
1: for each state i in S do
2:
3:

(cid:20)Q I(i  i)

for ∆ = 1 to r do

(et∆A)(1:n) (n+1):(2n)

  where A =

(cid:21)

Di =

E[τi|O  T  Q] + = (cid:80)

Pkl(t∆)

end for

4:
5:
6: end for
7: for each edge (i  j) in L do
8:
9:

for ∆ = 1 to r do

E[nij|O  T  Q] + =(cid:80)

Nij =

Pkl(t∆)

10:
11:
12: end for

end for

qij (et∆A)(1:n) (n+1):(2n)

0

Q

(k l)∈L C(∆  k  l)(Di)k l

(cid:20)Q I(i  j)

(cid:21)

0

Q

  where A =

(k l)∈L C(∆  k  l)(Nij)k l

soft variants  the time complexity of Expm is O(rS4 + rLS3)  where r is the number of distinct time
intervals between observations  S is the number of states  and L is the number of edges. The soft
version of Eigen has the same time complexity  but since the eigendecomposition of non-symmetric
matrices can be ill-conditioned in any EM iteration [17]  this method is not attractive. Unif is
based on truncating an inﬁnite sum and the truncation point M varies with maxi t∆ qit∆  with the
result that the cost of Unif varies signiﬁcantly with both the data and the parameters. In comparison 
Expm is much less sensitive to these values (log versus quadratic dependency). See the supplemental
material for the details. We conclude that Expm is the most robust method available for the soft EM
case. When the state space is large  hard EM can be used to tradeoff accuracy with time. In the hard
EM case  Unif can be more efﬁcient than Expm  because Unif can evaluate only the expectations
speciﬁed by the required end-states from the best decoded paths  whereas Expm must always produce
results from all end-states.
These asymptotic results are consistent with our experimental ﬁndings. On the glaucoma dataset
from Section 5.2  using a model with 105 states  Soft Expm requires 18 minutes per iteration on a
2.67 GHz machine with unoptimized MATLAB code  while Soft Unif spends more than 105 minutes
per iteration  Hard Unif spends 2 minutes per iteration  and Eigen fails.
5 Experimental results
We evaluated our EM algorithms in simulation (Sec. 5.1) and on two real-world datasets: a glaucoma
dataset (Sec. 5.2) in which we compare our prediction performance to a state-of-the-art method  and
a dataset for Alzheimer’s disease (AD  Sec. 5.3) where we compare visualized progression trends to
recent ﬁndings in the literature. Our disease progression models employ 105 (Glaucoma) and 277
(AD) states  representing a signiﬁcant advance in the ability to work with large models (previous
CT-HMM works [2  7  5] employed fewer than 100 states).

[0  1] and renormalized such that(cid:80)

5.1 Simulation on a 5-state Complete Digraph
We test the accuracy of all methods on a 5-state complete digraph with synthetic data generated
under different noise levels. Each qi is randomly drawn from [1  5] and then qij is drawn from
j(cid:54)=i qij = qi. The state chains are generated from Q  such that
is the largest mean holding time.
each chain has a total duration around T = 100
mini qi
The data emission model for state i is set as N (i  σ2)  where σ varies under different noise level
settings. The observations are then sampled from the state chains with rate
maxi qi
is the smallest mean holding time  which should be dense enough to make the chain identiﬁable.
A total of 105 observations are sampled. The average 2-norm relative error ||ˆq−q||
is used as the
||q||
performance metric  where ˆq is a vector contains all learned qij parameters  and q is the ground
truth.
The simulation results from 5 random runs are listed in Table 1. Expm and Unif produce nearly
identical results so they are combined in the table. Eigen fails at least once for each setting  but
when it works it produces similar results. All Soft methods achieve signiﬁcantly better accuracy

  where

  where

maxi qi

mini qi

0.5

1

1

6

Table 1: The average 2-norm relative error from 5 random runs on a 5-state complete digraph under varying
noise levels. The convergence threshold is ≤ 10−8 on relative data likelihood change.

Error
S(Expm Unif)
H(Expm Unif)

σ = 1/4
0.026±0.008
0.031±0.009

σ = 3/8
0.032±0.008
0.197±0.062

σ = 1/2
0.042±0.012
0.476±0.100

σ = 1
0.199±0.084
0.857±0.080

σ = 2
0.510±0.104
0.925±0.030

Figure 1: (a) The 2D-grid state structure for glaucoma progression modeling. (b) Illustration of the prediction
of future states from s(0) = i. (c) One fold of convergence behavior of Soft(Expm) on the glaucoma dataset.

than Hard methods  especially when the noise level becomes higher. This can be attributed to the
maintenance of the full hidden state distribution which makes it more robust to noise.
5.2 Application of CT-HMM to Predicting Glaucoma Progression
In this experiment we used CT-HMM to visualize a real-world glaucoma dataset and predict glau-
coma progression. Glaucoma is a leading cause of blindness and visual morbidity worldwide [18].
This disease is characterized by a slowly progressing optic neuropathy with associated irreversible
structural and functional damage. There are conﬂicting ﬁndings in the temporal ordering of de-
tectable structural and functional changes  which confound glaucoma clinical assessment and treat-
ment plans [19]. Here  we use a 2D-grid state space model with 105 states  deﬁned by successive
value bands of the two main glaucoma markers  Visual Field Index (VFI) (functional marker) and
average RNFL (Retinal Nerve Fiber Layer) thickness (structural marker) with forwarding edges (see
Fig. 1(a)). More details of the dataset and model can be found in the supplementary material. We
utilize Soft Expm for the following experiments  since it converges quickly (see Fig. 1(c))  has an
acceptable computational cost  and exhibits the best performance.
To predict future continuous measurements  we follow a simple procedure illustrated in Fig. 1(b).
Given a testing patient  Viterbi decoding is used to decode the best hidden state path for the past
visits. Then  given a future time t  the most probable future state is predicted by j = maxj Pij(t)
(blue node)  where i is the current state (black node). To predict the continuous measurements  we
search for the future time t1 and t2 in a desired resolution when the patient enters and leaves a state
having same value range as state j for each disease marker separately. The measurement at time t
can then be computed by linear interpolation between t1 and t2 and the two data bounds of state j for
the speciﬁed marker ([b1  b2] in Fig. 1(b)). The mean absolute error (MAE) between the predicted
values and the actual measurements was used for performance assessment. The performance of CT-
HMM was compared to both conventional linear regression and Bayesian joint linear regression [11].
For the Bayesian method  the joint prior distribution of the four parameters (two intercepts and two
slopes) computed from the training set [11] is used alongside the data likelihood. The results in
Table 2 demonstrate the signiﬁcantly improved performance of CT-HMM.
In Fig. 2(a)  we visualize the model trained using the entire dataset. Several dominant paths can be
identiﬁed: there is an early stage containing RNFL thinning with intact vision (blue vertical path in
the ﬁrst column)  and at around RNFL range [80  85] the transition trend reverses and VFI changes
become more evident (blue horizontal paths). This L shape in the disease progression supports the
ﬁnding in [20] that RNFL thickness of around 77 microns is a tipping point at which functional
deterioration becomes clinically observable with structural deterioration. Our 2D CT-HMM model
can be used to visualize the non-linear relationship between structural and functional degeneration 
yielding insights into the progression process.
5.3 Application of CT-HMM to Exploratory Analysis of Alzheimer’s Disease
We now demonstrate the use of CT-HMM as an exploratory tool to visualize the temporal interaction
of disease markers of Alzheimer’s Disease (AD). AD is an irreversible neuro-degenerative disease
that results in a loss of mental function due to the degeneration of brain tissues. An estimated 5.3

7

s(0)=is(t)=jt1t2b1b2b3Functional deteriorationStructural deterioration......Functional deteriorationStructural deterioration(a)(b)(c)Table 2: The mean absolute error (MAE) of predicting the two glaucoma measures. (∗ represents that CT-
HMM performs signiﬁcantly better than the competing method under student t-test).

MAE
VFI
RNFL

CT-HMM
4.64 ± 10.06
7.05 ± 6.57

Bayesian Joint Linear Regression

5.57 ± 11.11 * (p = 0.005)
9.65 ± 8.42 * (p ≈ 0.000)

Linear Regression

7.00 ± 12.22 *(p ≈ 0.000)
18.13 ± 20.70 * (p ≈ 0.000)

million Americans have AD  yet no prevention or cures have been found [21]. It could be beneﬁcial
to visualize the relationship between clinical  imaging  and biochemical markers as the pathology
evolves  in order to better understand AD progression and develop treatments.
A 277 state CT-HMM model was constructed from a cohort of AD patients (see the supplementary
material for additional details). The 3D visualization result is shown in Fig. 2(b). The state transition
trends show that the abnormality of Aβ level emerges ﬁrst (blue lines) when cognition scores are
still normal. Hippocampus atrophy happens more often (green lines) when Aβ levels are already
low and cognition has started to show abnormality. Most cognition degeneration happens (red lines)
when both Aβ levels and Hippocampus volume are already in abnormal stages. Our quantitative
visualization results supports recent ﬁndings that the decreasing of Aβ level in CSF is an early
marker before detectable hippocampus atrophy in cognition-normal elderly [22]. The CT-HMM
disease model with interactive visualization can be utilized as an exploratory tool to gain insights of
the disease progression and generate hypotheses to be further investigated by medical researchers.

Figure 2: Visualization scheme: (a) The strongest transition among the three instantaneous links from each
state are shown in blue while other transitions are drawn in dotted black. The line width and the node size
reﬂect the expected count. The node color represents the average sojourn time (red to green: 0 to 5 years and
above). (b) similar to (a) but the strongest transition from each state is color coded as follows: Aβ direction
(blue)  hippo (green)  cog (red)  Aβ +hippo (cyan)  Aβ +cog (magenta)  hippo+cog (yellow)  Aβ +hippo+
cog(black). The node color represents the average sojourn time (red to green: 0 to 3 years and above).
6 Conclusion
In this paper  we present novel EM algorithms for CT-HMM learning which leverage recent ap-
proaches [9] for evaluating the end-state conditioned expectations in CTMC models. To our knowl-
edge  we are the ﬁrst to develop and test the Expm and Unif methods for CT-HMM learning. We also
analyze their time complexity and provide experimental comparisons among the methods under soft
and hard EM frameworks. We ﬁnd that soft EM is more accurate than hard EM  and Expm works
the best under soft EM. We evaluated our EM algorithsm on two disease progression datasets for
glaucoma and AD. We show that CT-HMM outperforms the state-of-the-art Bayesian joint linear
regression method [11] for glaucoma progression prediction. This demonstrates the practical value
of CT-HMM for longitudinal disease modeling and prediction.

Acknowledgments

Portions of this work were supported in part by NIH R01 EY13178-15 and by grant U54EB020404 awarded
by the National Institute of Biomedical Imaging and Bioengineering through funds provided by the Big Data
to Knowledge (BD2K) initiative (www.bd2k.nih.gov). Additionally  the collection and sharing of the
Alzheimers data was funded by ADNI under NIH U01 AG024904 and DOD award W81XWH-12-2-0012. The
research was also supported in part by NSF/NIH BIGDATA 1R01GM108341  ONR N00014-15-1-2340  NSF
IIS-1218749  and NSF CAREER IIS-1350983.

8

Functional degeneration (VFI)Structural degeneration (RNFL)structural(Hippocampus)biochemical(A beta)functional(Cognition)(a) Glaucoma progression(b) Alzheimer's disease progressionReferences
[1] D. R. Cox and H. D. Miller  The Theory of Stochastic Processes. London: Chapman and Hall 

1965.

[2] C. H. Jackson  “Multi-state models for panel data: the msm package for R ” Journal of Statis-

tical Software  vol. 38  no. 8  2011.

[3] N. Bartolomeo  P. Trerotoli  and G. Serio  “Progression of liver cirrhosis to HCC: an applica-

tion of hidden markov model ” BMC Med Research Methold.  vol. 11  no. 38  2011.

[4] Y. Liu  H. Ishikawa  M. Chen  and et al.  “Longitudinal modeling of glaucoma progression us-
ing 2-dimensional continuous-time hidden markov model ” Med Image Comput Comput Assist
Interv  vol. 16  no. 2  pp. 444–51  2013.

[5] X. Wang  D. Sontag  and F. Wang  “Unsupervised learning of disease progression models ”

Proceeding KDD  vol. 4  no. 1  pp. 85–94  2014.

[6] U. Nodelman  C. R. Shelton  and D. Koller  “Expectation maximization and complex duration
distributions for continuous time bayesian networks ” in Proc. Uncertainty in AI (UAI 05) 
2005.

[7] J. M. Leiva-Murillo  A. Arts-Rodrguez  and E. Baca-Garca  “Visualization and prediction of

disease interactions with continuous-time hidden markov models ” in NIPS  2011.

[8] P. Metzner  I. Horenko  and C. Schtte  “Generator estimation of markov jump processes based
on incomplete observations nonequidistant in time ” Physical Review E  vol. 76  no. 066702 
2007.

[9] A. Hobolth and J. L. Jensen  “Summary statistics for endpoint-conditioned continuous-time

markov chains ” Journal of Applied Probability  vol. 48  no. 4  pp. 911–924  2011.

[10] P. Tataru and A. Hobolth  “Comparison of methods for calculating conditional expectations of
sufﬁcient statistics for continuous time markov chains ” BMC Bioinformatics  vol. 12  no. 465 
2011.

[11] F. Medeiros  L. Zangwill  C. Girkin  and et al.  “Combining structural and functional measure-
ments to improve estimates of rates of glaucomatous progression ” Am J Ophthalmol  vol. 153 
no. 6  pp. 1197–205  2012.

[12] M. Bladt and M. Srensen  “Statistical inference for discretely observed markov jump pro-

cesses ” J. R. Statist. Soc. B  vol. 39  no. 3  p. 395410  2005.

[13] L. R. Rabinar  “A tutorial on hidden markov models and selected applications in speech recog-

nition ” Proceedings of the IEEE  vol. 77  no. 2  1989.

[14] A. Hobolth and J. L.Jensen  “Statistical inference in evolutionary models of DNA sequences
via the EM algorithm ” Statistical Applications in Genetics and Molecular Biology  vol. 4 
no. 1  2005.

[15] C. Van Loan  “Computing integrals involving the matrix exponential ” IEEE Trans. Automatic

Control  vol. 23  pp. 395–404  1978.

[16] N. Higham  Functions of Matrices: Theory and Computation. SIAM  2008.
[17] P. Metzner  I. Horenko  and C. Schtte  “Generator estimation of markov jump processes ” Jour-

nal of Computational Physics  vol. 227  p. 353375  2007.

[18] S. Kingman  “Glaucoma is second leading cause of blindness globally ” Bulletin of the World

Health Organization  vol. 82  no. 11  2004.

[19] G. Wollstein  J. Schuman  L. Price  and et al.  “Optical coherence tomography longitudinal
evaluation of retinal nerve ﬁber layer thickness in glaucoma ” Arch Ophthalmol  vol. 123 
no. 4  pp. 464–70  2005.

[20] G. Wollstein  L. Kagemann  R. Bilonick  and et al.  “Retinal nerve ﬁbre layer and visual func-

tion loss in glaucoma: the tipping point ” Br J Ophthalmol  vol. 96  no. 1  pp. 47–52  2012.

[21] The Alzheimers Disease Neuroimaging Initiative  “http://adni.loni.usc.edu ”
[22] A. M. Fagan  D. Head  A. R. Shah  and et. al  “Decreased CSF A beta 42 correlates with brain

atrophy in cognitively normal elderly ” Ann Neurol.  vol. 65  no. 2  p. 176183  2009.

9

,Yu-Ying Liu
Shuang Li
Fuxin Li
Le Song
James Rehg