2016,Adversarial Multiclass Classification: A Risk Minimization Perspective,Recently proposed adversarial classification methods have shown promising results for cost sensitive and multivariate losses. In contrast with empirical risk minimization (ERM) methods  which use convex surrogate losses to approximate the desired non-convex target loss function  adversarial methods minimize non-convex losses by treating the properties of the training data as being uncertain and worst case within a minimax game. Despite this difference in formulation  we recast adversarial classification under zero-one loss as an ERM method with a novel prescribed loss function. We demonstrate a number of theoretical and practical advantages over the very closely related hinge loss ERM methods. This establishes adversarial classification under the zero-one loss as a method that fills the long standing gap in multiclass hinge loss classification  simultaneously guaranteeing Fisher consistency and universal consistency  while also providing dual parameter sparsity and high accuracy predictions in practice.,Adversarial Multiclass Classiﬁcation:

A Risk Minimization Perspective

Rizal Fathony

Anqi Liu

Kaiser Asif

Brian D. Ziebart

Department of Computer Science
University of Illinois at Chicago

Chicago  IL 60607

{rfatho2  aliu33  kasif2  bziebart}@uic.edu

Abstract

Recently proposed adversarial classiﬁcation methods have shown promising results
for cost sensitive and multivariate losses. In contrast with empirical risk mini-
mization (ERM) methods  which use convex surrogate losses to approximate the
desired non-convex target loss function  adversarial methods minimize non-convex
losses by treating the properties of the training data as being uncertain and worst
case within a minimax game. Despite this difference in formulation  we recast
adversarial classiﬁcation under zero-one loss as an ERM method with a novel
prescribed loss function. We demonstrate a number of theoretical and practical
advantages over the very closely related hinge loss ERM methods. This establishes
adversarial classiﬁcation under the zero-one loss as a method that ﬁlls the long
standing gap in multiclass hinge loss classiﬁcation  simultaneously guaranteeing
Fisher consistency and universal consistency  while also providing dual parameter
sparsity and high accuracy predictions in practice.

1

Introduction

A common goal for standard classiﬁcation problems in machine learning is to ﬁnd a classiﬁer that
minimizes the zero-one loss. Since directly minimizing this loss over training data via empirical
risk minimization (ERM) [1] is generally NP-hard [2]  convex surrogate losses are employed to
approximate the zero-one loss. For example  the logarithmic loss is minimized by the logistic
regression classiﬁer [3] and the hinge loss is minimized by the support vector machine (SVM) [4  5].
Both are Fisher consistent [6  7] and universally consistent [8  9] for binary classiﬁcation  meaning
they minimize the zero-one loss and are Bayes-optimal classiﬁers when they learn from any true
distribution of data using a rich feature representation. SVMs provide the additional advantage of dual
parameter sparsity so that when combined with kernel methods  extremely rich feature representations
can be efﬁciently considered. Unfortunately  generalizing the hinge loss to classiﬁcation tasks with
more than two labels is challenging and existing multiclass convex surrogates [10–12] tend to lose
their consistency guarantees [13–15] or produce low accuracy predictions in practice [15].
Adversarial classiﬁcation [16  17] uses a different approach to tackle non-convex losses like the
zero-one loss. Instead of approximating the desired loss function and evaluating over the training
data  it adversarially approximates the available training data within a minimax game formulation
with game payoffs deﬁned by the desired (zero-one) loss function [18  19]. This provides promising
empirical results for cost-sensitive losses [16] and multivariate losses such as the F-measure and
the precision-at-k [17]. Conceptually  parameter optimization for the adversarial method forces the
adversary to “behave like” certain properties of the training data sample  making labels easier to
predict within the minimax prediction game. However  a key bottleneck for these methods has been

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

their reliance on zero-sum game solvers for inference  which are computationally expensive relative
to inference in other prediction methods  such as SVMs.
In this paper  we recast adversarial prediction from an empirical risk minimization perspective by
analyzing the Nash equilibrium value of adversarial zero-one classiﬁcation games to deﬁne a new
multiclass loss1. This enables us to demonstrate that zero-one adversarial classiﬁcation ﬁlls the long
standing gap in ERM-based multiclass classiﬁcation by simultaneously: (1) guaranteeing Fisher
consistency and universal consistency; (2) enabling computational efﬁciency via the kernel trick and
dual parameter sparsity; and (3) providing competitive performance in practice. This reformulation
also provides signiﬁcant computational efﬁciency improvements compared to previous adversarial
classiﬁcation training methods [16].

2 Background and Related Work

2.1 Multiclass SVM generalizations

The multiclass support vector machine (SVM) seeks class-based potentials fy(xi) for each input
vector x ∈ X and class y ∈ Y so that the discriminant function  ˆyf (xi) = argmaxy fy(xi) 
minimizes misclassiﬁcation errors  lossf (xi  yi) = I(yi (cid:54)= ˆyf (xi)). Unfortunately  empirical risk
minimization (ERM)  minf E ˜P (x y) [lossf (X  Y )]  for the zero-one loss is NP-hard once the set of
potentials is (parametrically) restricted (e.g.  as a linear function of input features) [2]. Instead  a
hinge loss approximation is employed by the SVM. In the binary setting  yi ∈ {−1  +1}  where the
potential of one class can be set to zero (f−1 = 0) with no loss in generality  the hinge loss is deﬁned
as [1 − yif+1(xi)]+  with the compact deﬁnition [g(.)]+ (cid:44) max(0  g(.)). Binary SVM  which is an
empirical risk minimizer using the hinge loss with L2 regularization 
2||θ||2
2 

E ˜P (x y) [lossfθ (X  Y )] + λ

(1)

min
fθ

j(cid:54)=yi

provides strong theoretical guarantees (Fisher consistency and universal consistency) [8  21] and
computational efﬁciency [1].
Many methods have been proposed to generalize SVM to the multiclass setting. Apart from
for all alternative labels  lossWW(xi  yi) = (cid:80)
the one-vs-all and one-vs-one decomposed formulations [22]  there are three main joint for-
the WW model by Weston et al. [11]  which incorporates the sum of hinge losses
mulations:
[1 − (fyi(xi) − fj(xi))]+; the CS model
employs an absolute hinge loss  lossLLW(xi  yi) = (cid:80)
by Crammer and Singer [10]  which uses the hinge loss of only the largest alternative label 
lossCS(xi  yi) = maxj(cid:54)=yi [1 − (fyi(xi) − fj(xi))]+; and the LLW model by Lee et al. [12]  which
(cid:80)
[1 + fj(xi)]+  and a constraint that
j fj(xi) = 0. The former two models (CS and WW) both utilize the pairwise class-based potential
differences fyi(xi) − fj(xi) and are therefore categorized as relative margin methods. LLW  on the
other hand  is an absolute margin method that only relates to fj(xi)[15]. Fisher consistency  or Bayes
consistency [7  13] guarantees that minimization of a surrogate loss for the true distribution provides
the Bayes-optimal classiﬁer  i.e.  minimizes the zero-one loss. If given any possible distribution of
data  a classiﬁer is Bayes-optimal  it is called universally consistent. Of these  only the LLW method
is Fisher consistent and universally consistent [12–14]. However  as pointed out by Do˘gan et al. [15] 
LLW’s use of an absolute margin in the loss (rather than the relative margin of WW and CS) often
causes it to perform poorly for datasets with low dimensional feature spaces. From the opposite
direction  the requirements for Fisher consistency have been well-characterized [13]  yet this has not
led to a multiclass classiﬁer that is both Fisher consistent and performs well in practice.

j(cid:54)=yi

2.2 Adversarial prediction games

Building on a variety of diverse formulations for adversarial prediction [23–26]  Asif et al. [16]
proposed an adversarial game formulation for multiclass classiﬁcation with cost-sensitive loss
functions. Under this formulation  the empirical training data is replaced by an adversarially chosen
conditional label distribution ˇP (ˇy|x) that must closely approximate the training data  but otherwise
1Farnia & Tse independently and concurrently discovered this same loss function [20]. They provide an

analysis focused on generalization bounds and experiments for binary classiﬁcation.

2

seeks to maximize expected loss  while an estimator player ˆP (ˆy|x) seeks to minimize expected loss.
For the zero-one loss  the prediction game is:

min

ˆP

ˇP :E

max

P (x) ˇP ( ˇy|x)[φ(X  ˇY )]= ˜φ

E ˜P (x) ˆP (ˆy|x) ˇP (ˇy|x)

I( ˆY (cid:54)= ˇY )

.

(2)

(cid:104)

(cid:105)

The vector of feature moments  ˜φ = E ˜P (x y)[φ(X  Y )]  is measured from sample training data.
Using minimax and strong Lagrangian duality  the optimization of Eq. (2) reduces to minimizing the
equilibrium game values of a new set of zero-sum games characterized by matrix L(cid:48)

 ψ1 yi(xi)

...

ψ1 yi(xi) + 1

xi θ:
··· ψ|Y| yi(xi) + 1
...
···

ψ|Y| yi(xi)

...

 ;

(3)

(cid:88)

i

min

θ

max

ˇp

min

ˆp

L(cid:48)
xi θ ˇpxi ; L(cid:48)

xi θ =

ˆpT
xi

where θ is a vector of Lagrangian model parameters  ˆpxi is a vector representation of the conditional
label distribution  ˆP ( ˆY = k|xi)  i.e. ˆpxi = [ ˆP ( ˆY = 1|xi) ˆP ( ˆY = 2|xi) . . .]T  and similarly for
xi θ is a zero-sum game matrix for each example  with ψj yi (xi) = fj(xi) −
ˇpxi. The matrix L(cid:48)
fyi(xi) = θT (φ(xi  j) − φ(xi  yi)). This optimization problem (Eq. (3)) is convex in θ and the
inner zero-sum game can be solved using linear programming [16].

3 Risk Minimization Perspective of Adversarial Multiclass Classiﬁcation

3.1 Nash equilibrium game value

Despite the differences in formulation between adversarial loss minimization and empirical risk
minimization  we now recast the zero-one loss adversarial game as the solution to an empirical
risk minimization problem. Theorem 1 deﬁnes the loss function that provides this equivalence by
considering all possible combinations of the adversary’s label assignments with non-zero probability
in the Nash equilibrium of the game.2
Theorem 1. The model parameters θ for multiclass zero-one adversarial classiﬁcation are equiva-
lently obtained from empirical risk minimization under the adversarial zero-one loss function:

(cid:80)
j∈S ψj yi(xi) + |S| − 1

AL0-1

f (xi  yi) =

max

S⊆{1 ... |Y|}  S(cid:54)=∅

|S|

 

(4)

2

where S is any non-empty member of the powerset of classes {1  2  . . .  |Y|}.
Thus  AL0-1 is the maximum value over 2|Y| − 1 linear hy-
perplanes. For binary prediction tasks  there are three linear
hyperplanes: ψ1 y(x)  ψ2 y(x) and ψ1 y(x)+ψ2 y(x)+1
. Figure
1 shows the loss function in potential difference spaces ψ when
the true label is y = 1. Note that AL0-1 combines two hinge
functions at ψ2 y(x) = −1 and ψ2 y(x) = 1  rather than
SVM’s single hinge at ψ1 y(x) = −1. This difference from
the hinge loss corresponds to the loss that is realized by ran-
domizing label predictions.3 For three classes  the loss function
has seven facets as shown in Figure 2a. Figures 2a  2b  and
2c show the similarities and differences between AL0-1 and the
multiclass SVM surrogate losses based on class potential dif-
ferences. Note that AL0-1 is also a relative margin loss function
that utilizes the pairwise potential difference ψj y(x).

Figure 1: AL0-1 evaluated over
the space of potential differences
(ψj y(x) = fj(x) − fy(x); and
ψj j(x) = 0) for binary prediction
tasks when the true label is y = 1.

3.2 Consistency properties

Fisher consistency is a desirable property for a surrogate loss function that guarantees its minimizer 
given the true distribution  P (x  y)  will yield the Bayes optimal decision boundary [13  14]. For

2The proof of this theorem and others in the paper are contained in the Supplementary Materials.
3We refer the reader to Appendix H for a comparison of the binary adversarial method and the binary SVM.

3

(a)

(b)

(c)

Figure 2: Loss function contour plots over the space of potential differences for the prediction task
with three classes when the true label is y = 1 under AL0-1 (a)  the WW loss (b)  and the CS loss (c).
(Note that ψi in the plots refers to ψj y(x) = fj(x) − fy(x); and ψj j(x) = 0.)

j (x) ⊆ argmaxj Pj(x)  where f∗(x) = [f∗

multiclass zero-one loss  given that we know Pj(x) (cid:44) P (Y = j|x)  Fisher consistency requires
that argmaxj f∗
|Y|(x)]T is the minimizer of
E [lossf (X  Y )|X = x]. Since any constant can be added to all f∗
j (x)
j=1 fj(x) = 0  to remove redundant solutions. We

the same  we employ a sum-to-zero constraint (cid:80)|Y|
Theorem 2. The loss for the minimizer f∗ of E(cid:2)AL0-1

f (X  Y )|X = x(cid:3) resides on the hyperplane

establish an important property of the minimizer for AL0-1 in the following theorem.

j (x) while keeping argmaxj f∗

1 (x)  . . .   f∗

potentials equal to zero. This minimization reduces to the following optimization:

deﬁned (in Eq. 4) by the complete set of labels  S = {1  . . .  |Y|}.
As an illustration for the case of three classes (Figure 2a)  the area described in the theorem
above corresponds to the region in the middle where the hyperplane that supports AL0-1 is
  and  equivalently  where − 1|Y| ≤ fj(x) ≤ |Y|−1
 ∀j ∈ {1  . . .  |Y|}
ψ1 y(x)+ψ2 y(x)+ψ3 y(x)+2
|Y|
j fj(x) = 0. Based on this restriction  we focus on the minimization of
 ∀j ∈ {1  . . .  |Y|} and the sum of

with a constraint that(cid:80)
E(cid:2)AL0-1
f (X  Y )|X = x(cid:3) subject to − 1|Y| ≤ fj(x) ≤ |Y|−1
|Y|(cid:88)
|Y| ≤ fj(x) ≤ |Y| − 1
|Y|
|Y|−1
The solution for this maximization (a linear program) satisﬁes f∗
|Y|
and − 1|Y| otherwise  which therefore implies the Fisher consistency theorem.
Theorem 3. The adversarial zero-one loss  AL0-1  from Eq. (4) is Fisher consistent.

Py(x)fy(x) subject to: − 1

j ∈ {1  . . .  |Y|};

if j = argmaxj Pj(x) 

|Y|(cid:88)

fj(x) = 0.

j (x) =

|Y|

3

max

f

y=1

j=1

Theorem 3 implies that AL0-1 (Eq. (4)) is classiﬁcation calibrated  which indicates minimization
of that loss for all distributions on X × Y also minimizes the zero-one loss [21  13]. As proven in
general by Steinwart and Christmann [2]  Micchelli et al. [27]  since AL0-1 (Eq.(4)) is a Lipschitz
loss with constant 1  the adversarial multiclass classiﬁer is universally consistent under the conditions
speciﬁed in Corollary 1.
Corollary 1. Given a universal kernel and regularization parameter λ in Eq. (1) tending to zero
slower than 1

n   the adversarial multiclass classiﬁer is also universally consistent.

3.3 Optimization

In the learning process for adversarial classiﬁcation  Asif et al. [16] requires a linear program to be
solved that ﬁnds the Nash equilibrium game value and strategy for every training data point in each
gradient update. This requirement is computationally burdensome compared to multiclass SVMs 
which must simply ﬁnd potential-maximizing labels. We propose two approaches with improved

4

efﬁciency by leveraging an oracle for ﬁnding the maximization inside AL0-1 and Lagrange duality in
the quadratic programming formulation.

3.3.1 Primal optimization using stochastic sub-gradient descent
(cid:80)
The sub-gradient in the empirical risk minimization of AL0-1 includes the mean of feature differences 
j∈R [φ(xi  j) − φ(xi  yi)]   where R is the set that maximizes AL0-1. The set R is computed
1|R|
by the oracle using a greedy algorithm. Given θ and a sample (xi  yi)  the algorithm calculates all
potentials ψj yi(xi) for each label j ∈ {1  . . .  |Y|} and sorts them in non-increasing order. Starting
with the empty set R = ∅  it then adds labels to R in sorted order until adding a label would decrease
the value of
Theorem 4. The proposed greedy algorithm used by the oracle is optimal.

j∈R ψj yi (xi)+|R|−1

(cid:80)

|R|

.

3.3.2 Dual optimization

In the next subsections  we focus on the dual optimization technique as it enables us to establish
convergence guarantees. We re-formulate the learning algorithm (with L2 regularization) as a
constrained quadratic program (QP) with ξi specifying the amount of AL0-1 incurred by each of the n
training examples:

min

θ

1
2

(cid:107)θ(cid:107)2 + C

n(cid:88)

i=1

ξi

subject to:

ξi ≥ ∆i k ∀i ∈ {1  . . . n}k ∈ {1  . . .   2|Y| − 1} 

(5)

(cid:80)
where we denote each of the 2|Y|−1 possible constraints for example i corresponding to non-empty el-
j∈Y ψj yi (xi)+|Y|−1
ements of the label powerset as ∆i k (e.g.  ∆i 1 = ψ1 yi(xi)  and ∆i 2|Y|−1 =
).
Note also that non-negativity for ξi is enforced since ∆i yi = ψyi yi(xi) = 0.
Theorem 5. Let Λi k be the partial derivative of ∆i k with respect to θ  i.e.  Λi k = d∆i k
is the constant part of ∆i k (for example if ∆i k = ψ1 yi (xi)+ψ3 yi (xi)+ψ4 yi (xi)+2
then the corresponding dual optimization for the primal minimization (Eq. 5) is:

dθ and νi k
  then νi k = 2
3 ) 

|Y|

3

αi kαj l [Λi k · Λj l]

(6)

n(cid:88)

2|Y|−1(cid:88)

i=1

k=1

max

α

νi k αi k − 1
2

2|Y|−1(cid:88)

i j=1

k l=1

m(cid:88)
2|Y|−1(cid:88)

subject to: αi k ≥ 0 

αi k = C  i ∈ {1  . . .   n}  k ∈ {1  . . .   2|Y| − 1} 

where αi k is the dual variable for the k-th constraint of the i-th sample.

k=1

recovered from the dual variables using the formula: θ = −(cid:80)n

Note that the dual formulation above only depends on the dot product of two constraints’ partial deriva-
tives (with respect to θ) and the constant part of the constraints. The original primal variable θ can be
k=1 αi k Λi k. Given a new

(cid:80)2|Y|−1

datapoint x  de-randomized predictions are obtained from argmaxj fj(x) = argmaxj θTφ(x  j).

i=1

3.3.3 Efﬁciently incorporating rich feature spaces using kernelization

Considering large feature spaces is important for developing an expressive classiﬁer that can learn
from large amounts of training data. Indeed  Fisher consistency requires such feature spaces for its
guarantees to be meaningful. However  naïvely projecting from the original input space  xi  to richer
(or possibly inﬁnite) feature spaces ω(xi)  can be computationally burdensome. Kernel methods
enable this feature expansion by allowing the dot products of certain feature functions to be computed
implicitly  i.e.  K(xi  xj) = ω(xi)· ω(xj). Since our dual formulation only depends on dot products 
we employ kernel methods to incorporate rich feature spaces into our formulation as stated in the
following theorem.
Theorem 6. Let X be the input space and K be a positive deﬁnite real valued kernel on X × X with
a mapping function ω(x) : X → H that maps the input space X to a reproducing kernel Hilbert

5

space H. Then all the values in the dual optimization of Eq. (6) needed to operate in the Hilbert
space H can be computed in terms of the kernel function K(xi  xj) as:

Λi k · Λj l = c(i k) (j l) K(xi  xj)  ∆i k = − n(cid:88)
2|Y|−1(cid:88)
(cid:20)(cid:18) 1(m ∈ Rj l)

fm(xi) = − n(cid:88)

αj l

j=1

l=1

|Rj l|

where c(i k) (j l) =

|Y|(cid:88)

m=1

2|Y|−1(cid:88)
(cid:18) 1(m ∈ Ri k)

j=1

l=1

|Ri k|

− 1(m = yi)

αj l c(j l) (i k) K(xj  xi) + νi k 

(7)

(cid:19)

(cid:21)

K(xj  xi)

 

(cid:19)

− 1(m = yj)

 

(8)

− 1(m = yj)

(cid:19)(cid:18) 1(m ∈ Rj l)

|Rj l|

and Ri k is the set of
ψ1 yi (xi)+ψ3 yi (xi)+ψ4 yi (xi)+2
0 otherwise  and the function 1(j ∈ Ri k) returns 1 if j is a member of set Ri k or 0 otherwise.

labels included in the constraint ∆i k (for example if ∆i k =
  then Ri k = {1  3  4})  the function 1(j = yi) returns 1 if j = yi or

3

3.3.4 Efﬁcient optimization using constraint generation

The number of constraints in the QP formulation above grows exponentially with the number
of classes: O(2|Y|). This prevents the naïve formulation from being efﬁcient for large multi-
class problems. We employ a constraint generation method to efﬁciently solve the dual quadratic
programming formulation that is similar to those used for extending the SVM to multivariate loss
functions [28] and structured prediction settings [29].

Algorithm 1 Constraint generation method
Require: Training data (x1  y1)  . . . (xn  yn)  C  
1: θ ← 0
i ← {∆i k|∆i k = ψyi yi(xi)} ∀i = 1  . . .   n
2: A∗
3: repeat
for i ← 1  n do
4:
5:
6:
7:
8:
9:
10:
end if
11:
end for
12:
13: until no A∗

a ← arg maxk|∆i k∈Ai ∆i k
ξi ← maxk|∆i k∈A∗
∆i k
if ∆i a > ξi +  then
i ∪ {∆i a}

Compute θ from α: θ = −(cid:80)n

i ← A∗
A∗
α ← Optimize dual over A∗ = ∪iA∗

i has changed in the iteration

(cid:80)

i=1

i

(cid:46) Actual label enforces non-negativity

(cid:46) Find the most violated constraint
(cid:46) Compute the example’s current loss estimate

(cid:46) Add it to the enforced constraints set

i
k|∆i k∈A∗

i

αi k Λi k

Algorithm 1 incrementally expands the set of enforced constraints  A∗
i   until no remaining constraint
from the set of all 2|Y| − 1 constraints (in Ai) is violated by more than . To obtain the most
violated constraint  we use the greedy algorithm described in the primal optimization. The constraint
generation algorithm’s stopping criterion ensures that a solution close to the optimal is returned
(violating no constraint by more than ). Theorem 7 provides a polynomial run time convergence
bounds for the Algorithm 1.
Theorem 7. For any  > 0 and training dataset {(x1  y1)  . . .   (xn  yn)} with U = maxi[xi · xi] 
constraint set A∗.

Algorithm 1 terminates after incrementally adding at most max(cid:8) 2n

(cid:9) constraints to the

   4nCU
2

The proof of Theorem 7 follows the procedures developed by Tsochantaridis et al. [28] for bounding
the running time of structured support vector machines. We observe that this bound is quite loose in
practice and the algorithm tends to converge much faster in our experiments.

6

4 Experiments

We evaluate the performance of the AL0-1 classiﬁer and compare with the three most popular
multiclass SVM formulations: WW [11]  CS [10]  and LLW [12]. We use 12 datasets from the UCI
Machine Learning repository [30] with various sizes and numbers of classes (details in Table 1). For
each dataset  we consider the methods using the original feature space (linear kernel) and a kernelized
feature space using the Gaussian radial basis function kernel.

Table 1: Properties of the datasets  the number of constraints considered by SVM models
(WW/CS/LLW)  the average number of constraints added to the constraint set for AL0-1 and the
average number of active constraints at the optima under both linear and Gausssian kernels.

Dataset

iris

redwine

(1)
(2) glass
(3)
(4) ecoli
(5) vehicle
(6) segment
(7) sat
(8) optdigits
(9) pageblocks
(10) libras
(11) vertebral
(12) breasttissue

Properties
# test
45
65
480
101
254
693
2000
1797
1642
108
93
32

# train
105
149
1119
235
592
1617
4435
3823
3831
252
217
74

# class
3
6
10
8
4
7
7
10
5
15
3
6

# feature
4
9
11
7
18
19
36
64
10
90
6
9

SVM

constraints
210
745
10071
1645
1776
9702
26610
34407
15324
3528
434
370

AL0-1 constraints added and active
Linear kernel
Gauss. kernel
13
125
1681
117
311
244
1524
597
427
389
78
65

223
490
3811
821
1201
4312
11860
10072
9155
1165
342
271

213
578
5995
614
1310
4410
11721
7932
9459
1592
344
258

38
252
1783
130
248
469
6269
2315
551
353
86
145

For our experimental methodology  we ﬁrst make 20 random splits of each dataset into training and
testing sets. We then perform two stage  ﬁve-fold cross validation on the training set of the ﬁrst
split to tune each model’s parameter C and the kernel parameter γ under the kernelized formulation.
In the ﬁrst stage  the values for C are 2i  i = {0  3  6  9  12} and the values for γ are 2i  i =
{−12 −9 −6 −3  0}. We select ﬁnal values for C from 2iC0  i = {−2 −1  0  1  2} and values for
γ from 2iγ0  i = {−2 −1  0  1  2} in the second stage  where C0 and γ0 are the best parameters
obtained in the ﬁrst stage. Using the selected parameters  we train each model on the 20 training sets
and evaluate the performance on the corresponding testing set. We use the Shark machine learning
library [31] for the implementation of the three multiclass SVM formulations.
Despite having an exponential number of possible constraints (i.e.  n(2|Y| − 1) for n examples versus
n(|Y| − 1) for SVMs)  a much smaller number of constraints need to be considered by the AL0-1
algorithm in practice to realize a better approximation ( = 0) than Theorem 7 provides. Table 1
shows how the total number of constraints for multiclass SVM compares to the number considered
in practice by our AL0-1 algorithm for linear and Gaussian kernel feature spaces. These range from
a small fraction (0.23) of the SVM constraints for optdigits to a slightly greater number (with a
fraction of 1.06) for iris. More speciﬁcally  of the over 3.9 million (= 210·3823) possible constraints
for optdigits when training the classiﬁer  fewer than 0.3% (7932 or 10072 depending on the feature
representation) are added to the constraint set during the constraint generation process. Fewer still
(597 or 2315 constraints—less than 0.06%) are constraints that are active in the ﬁnal classiﬁer
with non-zero dual parameters. The sparsity of the dual parameters provides a key computational
beneﬁt for support vector machines over logistic regression  which has essentially all non-zero dual
parameters. The small number of active constraints shown in Table 1 demonstrate that AL0-1 induces
similar sparsity  providing efﬁciency when employed with kernel methods.
We report the accuracy of each method averaged over the 20 dataset splits for both linear feature
representations and Gaussian kernel feature representations in Table 2. We denote the results that
are either the best of all four methods or not worse than the best with statistical signiﬁcance (under
paired t-test with α = 0.05) using bold font. We also show the accuracy averaged over all of the
datasets for each method and the number of dataset for which each method is “indistinguishably best”
(bold numbers) in the last row. As we can see from the table  the only alternative model that is Fisher

7

Table 2: The mean and (in parentheses) standard deviation of the accuracy for each model with linear
kernel and Gaussian kernel feature representations. Bold numbers in each case indicate that the result
is the best or not signiﬁcantly worse than the best (paired t-test with α = 0.05).

D

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
avg
#bold

AL0-1

96.3 (3.1)
62.5 (6.0)
58.8 (2.0)
86.2 (2.2)
78.8 (2.2)
94.9 (0.7)
84.9 (0.7)
96.6 (0.6)
96.0 (0.5)
74.1 (3.3)
85.5 (2.9)
64.4 (7.1)
81.59

9

Linear Kernel
CS

WW

96.0 (2.6)
62.2 (3.6)
59.1 (1.9)
85.7 (2.5)
78.8 (1.7)
94.9 (0.8)
85.4 (0.7)
96.5 (0.7)
96.1 (0.5)
72.0 (3.8)
85.9 (2.7)
59.7 (7.8)
81.02

6

96.3 (2.4)
62.5 (3.9)
56.6 (2.0)
85.8 (2.3)
78.4 (2.3)
95.2 (0.8)
84.7 (0.7)
96.3 (0.6)
96.3 (0.5)
71.3 (4.3)
85.4 (3.3)
66.3 (6.9)
81.25

8

LLW

79.7 (5.5)
52.8 (4.6)
57.7 (1.7)
74.1 (3.3)
69.8 (3.7)
75.8 (1.5)
74.9 (0.9)
76.2 (2.2)
92.5 (0.8)
34.0 (6.4)
79.8 (5.6)
58.3 (8.1)
68.80

0

AL0-1

96.7 (2.4)
69.5 (4.2)
63.3 (1.8)
86.0 (2.7)
84.3 (2.5)
96.5 (0.6)
91.9 (0.5)
98.7 (0.4)
96.8 (0.5)
83.6 (3.8)
86.0 (3.1)
68.4 (8.6)
85.14

9

Gaussian Kernel
CS
WW

96.4 (2.4)
66.8 (4.3)
64.2 (2.0)
84.9 (2.4)
84.4 (2.6)
96.6 (0.5)
92.0 (0.6)
98.8 (0.4)
96.6 (0.4)
83.8 (3.4)
85.3 (2.9)
68.1 (6.5)
84.82

6

96.2 (2.3)
69.4 (4.8)
64.2 (1.9)
85.6 (2.4)
83.8 (2.3)
96.3 (0.6)
91.9 (0.5)
98.8 (0.3)
96.7 (0.4)
85.0 (3.9)
85.5 (3.3)
66.6 (8.9)
85.00

6

LLW
95.4 (2.1)
69.2 (4.4)
64.7 (2.1)
86.0 (2.5)
84.4 (2.6)
96.4 (0.5)
91.9 (0.4)
98.9 (0.3)
96.6 (0.4)
83.2 (4.2)
84.4 (2.7)
68.0 (7.2)
84.93

7

consistent—the LLW model—performs poorly on all datasets when only linear features are employed.
This matches with previous experimental results conducted by Do˘gan et al. [15] and demonstrates a
weakness of using an absolute margin for the loss function (rather than the relative margins of all other
methods). The AL0-1 classiﬁer performs competitively with the WW and CS models with a slight
advantages on overall average accuracy and a larger number of “indistinguishably best” performances
on datasets—or  equivalently  fewer statistically signiﬁcant losses to any other method.
The kernel trick in the Gaussian kernel case provides access to much richer feature spaces  improving
the performance of all models  and the LLW model especially. In general  all models provide
competitive results in the Gaussian kernel case. The AL0-1 classiﬁer maintains a similarly slight
advantage and only provides performance that is sub-optimal (with statistical signiﬁcance) in three
of the twelve datasets versus six of twelve and ﬁve of twelve for the other methods. We conclude
that the multiclass adversarial method performs well in both low and high dimensional feature
spaces. Recalling the theoretical analysis of the adversarial method  it is a well-motivated (from
the adversarial zero-one loss minimization) multiclass classiﬁer that enjoys both strong theoretical
properties (Fisher consistency and universal consistency) and empirical performance.

5 Conclusion

Generalizing support vector machines to multiclass settings in a theoretically sound manner remains a
long-standing open problem. Though the loss function requirements guaranteeing Fisher-consistency
are well-understood [13]  the few Fisher-consistent classiﬁers that have been developed (e.g.  LLW)
often are not competitive with inconsistent multiclass classiﬁers in practice.
In this paper  we
have sought to ﬁll this gap between theory and practice. We have demonstrated that multiclass
adversarial classiﬁcation under zero-one loss can be recast from an empirical risk minimization
perspective and its surrogate loss  AL0-1  shown to satisfy the Fisher consistency property  leading
to a universally consistent classiﬁer that also performs well in practice. We believe that this is an
important contribution in understanding both adversarial methods and the generalized hinge loss. Our
future work includes investigating the adversarial methods under the different losses and exploring
other theoretical properties of the adversarial framework  including generalization bounds.

Acknowledgments

This research was supported as part of the Future of Life Institute (futureoﬂife.org) FLI-RFP-AI1
program  grant#2016-158710 and by NSF grant RI-#1526379.

8

References
[1] Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in Neural Information

Processing Systems  pages 831–838  1992.

[2] Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer Publishing Company 

Incorporated  1st edition  2008. ISBN 0387772413.

[3] Peter McCullagh and John A Nelder. Generalized linear models  volume 37. CRC press  1989.
[4] Bernhard E Boser  Isabelle M Guyon  and Vladimir N Vapnik. A training algorithm for optimal margin

classiﬁers. In Proceedings of the Workshop on Computational Learning Theory  pages 144–152  1992.

[5] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning  20(3):273–297  1995.
[6] Yi Lin. Support vector machines and the bayes rule in classiﬁcation. Data Mining and Knowledge

Discovery  6(3):259–275  2002.

[7] Peter L. Bartlett  Michael I. Jordan  and Jon D. McAuliffe. Convexity  classiﬁcation  and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[8] Ingo Steinwart. Support vector machines are universally consistent. J. Complexity  18(3):768–791  2002.
[9] Ingo Steinwart. Consistency of support vector machines and other regularized kernel classiﬁers. IEEE

Trans. Information Theory  51(1):128–142  2005.

[10] Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector

machines. The Journal of Machine Learning Research  2:265–292  2002.

[11] Jason Weston  Chris Watkins  et al. Support vector machines for multi-class pattern recognition. In ESANN 

volume 99  pages 219–224  1999.

[12] Yoonkyung Lee  Yi Lin  and Grace Wahba. Multicategory support vector machines: Theory and application
to the classiﬁcation of microarray data and satellite radiance data. Journal of the American Statistical
Association  99(465):67–81  2004.

[13] Ambuj Tewari and Peter L Bartlett. On the consistency of multiclass classiﬁcation methods. The Journal

of Machine Learning Research  8:1007–1025  2007.

[14] Yufeng Liu. Fisher consistency of multicategory support vector machines. In International Conference on

Artiﬁcial Intelligence and Statistics  pages 291–298  2007.

[15] Ürün Do˘gan  Tobias Glasmachers  and Christian Igel. A uniﬁed view on multi-class support vector

classiﬁcation. Journal of Machine Learning Research  17(45):1–32  2016.

[16] Kaiser Asif  Wei Xing  Sima Behpour  and Brian D. Ziebart. Adversarial cost-sensitive classiﬁcation. In

Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence  2015.

[17] Hong Wang  Wei Xing  Kaiser Asif  and Brian Ziebart. Adversarial prediction games for multivariate

losses. In Advances in Neural Information Processing Systems  pages 2710–2718  2015.

[18] Flemming Topsøe. Information theoretical optimization techniques. Kybernetika  15(1):8–27  1979.
[19] Peter D. Grünwald and A. Phillip Dawid. Game theory  maximum entropy  minimum discrepancy  and

robust Bayesian decision theory. Annals of Statistics  32:1367–1433  2004.

[20] Farzan Farnia and David Tse. A minimax approach to supervised learning.

Information Processing Systems  pages 4233–4241. 2016.

In Advances in Neural

[21] Peter L Bartlett  Michael I Jordan  and Jon D McAuliffe. Large margin classiﬁers: Convex loss  low noise 
and convergence rates. In Advances in Neural Information Processing Systems  pages 1173–1180  2003.
[22] Naiyang Deng  Yingjie Tian  and Chunhua Zhang. Support vector machines: optimization based theory 

algorithms  and extensions. CRC press  2012.

[23] Nilesh Dalvi  Pedro Domingos  Sumit Sanghai  Deepak Verma  et al. Adversarial classiﬁcation.

In
Proceedings of the International Conference on Knowledge Discovery and Data Mining  pages 99–108.
ACM  2004.

[24] Anqi Liu and Brian Ziebart. Robust classiﬁcation under sample selection bias. In Advances in Neural

Information Processing Systems  pages 37–45  2014.

[25] Gert RG Lanckriet  Laurent El Ghaoui  Chiranjib Bhattacharyya  and Michael I Jordan. A robust minimax

approach to classiﬁcation. The Journal of Machine Learning Research  3:555–582  2003.

[26] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair  Aaron
Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing
Systems  pages 2672–2680  2014.

[27] Charles A. Micchelli  Yuesheng Xu  and Haizhang Zhang. Universal kernels. Journal of Machine Learning

Research  6:2651–2667  2006.

[28] Ioannis Tsochantaridis  Thorsten Joachims  Thomas Hofmann  and Yasemin Altun. Large margin methods

for structured and interdependent output variables. In JMLR  pages 1453–1484  2005.

[29] Thorsten Joachims. A support vector method for multivariate performance measures. In Proceedings of

the International Conference on Machine Learning  pages 377–384  2005.

[30] M. Lichman. UCI machine learning repository  2013. URL http://archive.ics.uci.edu/ml.
[31] Christian Igel  Verena Heidrich-Meisner  and Tobias Glasmachers. Shark. Journal of Machine Learning

Research  9:993–996  2008.

9

,Rizal Fathony
Anqi Liu
Kaiser Asif
Brian Ziebart
Andrey Kolobov
Cheng Lu
Eric Horvitz