2013,Memory Limited  Streaming PCA,We consider streaming  one-pass principal component analysis (PCA)  in the high-dimensional regime  with limited memory. Here  $p$-dimensional samples are presented sequentially  and the goal is to produce the $k$-dimensional subspace that best approximates these points. Standard algorithms require $O(p^2)$ memory; meanwhile no algorithm can do better than $O(kp)$ memory  since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the {\em spiked covariance model}  where $p$-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples  $n$  scales proportionally with the dimension  $p$. Yet  all algorithms that provably achieve this  have memory complexity $O(p^2)$. Meanwhile  algorithms with memory-complexity $O(kp)$ do not have provable bounds on sample complexity comparable to $p$. We present an algorithm that achieves both: it uses $O(kp)$ memory (meaning storage of any kind) and is able to compute the $k$-dimensional spike with $O(p \log p)$ sample-complexity -- the first algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model  our simulations show that our algorithm is successful on much more general models for the data.,Memory Limited  Streaming PCA

Ioannis Mitliagkas

Constantine Caramanis

Dept. of Electrical and Computer Engineering

Dept. of Electrical and Computer Engineering

The University of Texas at Austin

ioannis@utexas.edu

The University of Texas at Austin
constantine@utexas.edu

Prateek Jain

Microsoft Research

Bangalore  India

prajain@microsoft.com

Abstract

We consider streaming  one-pass principal component analysis (PCA)  in the high-
dimensional regime  with limited memory. Here  p-dimensional samples are pre-
sented sequentially  and the goal is to produce the k-dimensional subspace that
best approximates these points. Standard algorithms require O(p2) memory;
meanwhile no algorithm can do better than O(kp) memory  since this is what the
output itself requires. Memory (or storage) complexity is most meaningful when
understood in the context of computational and sample complexity. Sample com-
plexity for high-dimensional PCA is typically studied in the setting of the spiked
covariance model  where p-dimensional points are generated from a population
covariance equal to the identity (white noise) plus a low-dimensional perturbation
(the spike) which is the signal to be recovered. It is now well-understood that
the spike can be recovered when the number of samples  n  scales proportionally
with the dimension  p. Yet  all algorithms that provably achieve this  have mem-
ory complexity O(p2). Meanwhile  algorithms with memory-complexity O(kp)
do not have provable bounds on sample complexity comparable to p. We present
an algorithm that achieves both: it uses O(kp) memory (meaning storage of any
kind) and is able to compute the k-dimensional spike with O(p log p) sample-
complexity – the ﬁrst algorithm of its kind. While our theoretical analysis focuses
on the spiked covariance model  our simulations show that our algorithm is suc-
cessful on much more general models for the data.

1

Introduction

Principal component analysis is a fundamental tool for dimensionality reduction  clustering  classi-
ﬁcation  and many more learning tasks. It is a basic preprocessing step for learning  recognition  and
estimation procedures. The core computational element of PCA is performing a (partial) singular
value decomposition  and much work over the last half century has focused on efﬁcient algorithms
(e.g.  Golub & Van Loan (2012) and references therein) and hence on computational complexity.

The recent focus on understanding high-dimensional data  where the dimensionality of the data
scales together with the number of available sample points  has led to an exploration of the sample
complexity of covariance estimation. This direction was largely inﬂuenced by Johnstone’s spiked
covariance model  where data samples are drawn from a distribution whose (population) covariance
is a low-rank perturbation of the identity matrix Johnstone (2001). Work initiated there  and also
work done in Vershynin (2010a) (and references therein) has explored the power of batch PCA in
the p-dimensional setting with sub-Gaussian noise  and demonstrated that the singular value decom-

1

position (SVD) of the empirical covariance matrix succeeds in recovering the principal components
(extreme eigenvectors of the population covariance) with high probability  given n = O(p) samples.

This paper brings the focus on another critical quantity: memory/storage. The only currently avail-
able algorithms with provable sample complexity guarantees either store all n = O(p) samples (note
that for more than a single pass over the data  the samples must all be stored) or explicitly form or
approximate the empirical p × p (typically dense) covariance matrix. All cases require as much as
O(p2) storage for exact recovery. In certain high-dimensional applications  where data points are
high resolution photographs  biometrics  video  etc.  p often is of the order of 1010 − 1012  making
the need for O(p2) memory prohibitive. At many computing scales  manipulating vectors of length
O(p) is possible  when storage of O(p2) is not. A typical desktop may have 10-20 GB of RAM  but
will not have more than a few TB of total storage. A modern smart-phone may have as much as a
GB of RAM  but has a few GB  not TB  of storage. In distributed storage systems  the scalability in
storage comes at the heavy cost of communication.

In this light  we consider the streaming data setting  where the samples xt ∈ Rp are collected
sequentially  and unless we store them  they are irretrievably gone.1 On the spiked covariance model
(and natural generalizations)  we show that a simple algorithm requiring O(kp) storage – the best
possible – performs as well as batch algorithms (namely  SVD on the empirical covariance matrix) 
with sample complexity O(p log p). To the best of our knowledge  this is the only algorithm with
both storage complexity and sample complexity guarantees.

We discuss connections to past work in detail in Section 2  introduce the model in Section 3  and
present the solution to the rank 1 case  the rank k case  and the perturbed-rank-k case in Sections 4.1 
4.2 and 4.3  respectively. In Section 5 we provide experiments that not only conﬁrm the theoretical
results  but demonstrate that our algorithm works well outside the assumptions of our main theorems.

2 Related Work

Memory- and computation-efﬁcient algorithms that operate on streaming data are plentiful in the
literature and many seem to do well in practice. However  there is no algorithm that provably
recovers the principal components in the same noise and sample-complexity regime as the batch
PCA algorithm does and maintains a provably light memory footprint. Because of the practical
relevance  there is renewed interest in this problem. The fact that it is an important unresolved issue
has been pointed out in numerous places  e.g.  Warmuth & Kuzmin (2008); Arora et al. (2012).

Online-PCA for regret minimization is considered in several papers  most recently in Warmuth &
Kuzmin (2008). There the multiplicative weights approach is adapted to this problem  with experts
corresponding to subspaces. The goal is to control the regret  improving on the natural follow-
the-leader algorithm that performs batch-PCA at each step. However  the algorithm can require
O(p2) memory  in order to store the multiplicative weights. A memory-light variant described in
Arora et al. (2012) typically requires much less memory  but there are no guarantees for this  and
moreover  for certain problem instances  its memory requirement is on the order of p2.

Sub-sampling  dimensionality-reduction and sketching form another family of low-complexity and
low-memory techniques  see  e.g.  Clarkson & Woodruff (2009); Nadler (2008); Halko et al. (2011).
These save on memory and computation by performing SVD on the resulting smaller matrix. The
results in this line of work provide worst-case guarantees over the pool of data  and typically require
a rapidly decaying spectrum  not required in our setting  to produce good bounds. More funda-
mentally  these approaches are not appropriate for data coming from a statistical model such as the
spiked covariance model. It is clear that subsampling approaches  for instance  simply correspond to
discarding most of the data  and for fundamental sample complexity reasons  cannot work. Sketch-
ing produces a similar effect: each column of the sketch is a random (+/−) sum of the data points.
If the data points are  e.g.  independent Gaussian vectors  then so will each element of the sketch 
and thus this approach again runs against fundamental sample complexity constraints. Indeed  it is
straightforward to check that the guarantees presented in (Clarkson & Woodruff (2009); Halko et al.
(2011)) are not strong enough to guarantee recovery of the spike. This is not because the results are
weak; it is because they are geared towards worst-case bounds.

1This is similar to what is sometimes referred to as the single pass model.

2

Algorithms focused on sequential SVD (e.g.  Brand (2002  2006)  Comon & Golub (1990) Li (2004)
and more recently Balzano et al. (2010); He et al. (2011)) seek to have the best subspace estimate
at every time (i.e.  each time a new data sample arrives) but without performing full-blown SVD
at each step. While these algorithms indeed reduce both the computational and memory burden of
batch-PCA  there are no rigorous guarantees on the quality of the principal components or on the
statistical performance of these methods.

In a Bayesian mindset  some researchers have come up with expectation maximization approaches
Roweis (1998); Tipping & Bishop (1999)  that can be used in an incremental fashion. The ﬁnite
sample behavior is not known.

Stochastic-approximation-based algorithms along the lines of Robbins & Monro (1951) are also
quite popular  due to their low computational and memory complexity  and excellent performance.
They go under a variety of names  including Incremental PCA (though the term Incremental has been
used in the online setting as well Herbster & Warmuth (2001))  Hebbian learning  and stochastic
power method Arora et al. (2012). The basic algorithms are some version of the following: upon
receiving data point xt at time t  update the estimate of the top k principal components via:

U (t+1) = Proj(U (t) + ηtxtx⊤t U (t)) 

(1)
where Proj(·) denotes the “projection” that takes the SVD of the argument  and sets the top k
singular values to 1 and the rest to zero (see Arora et al. (2012) for discussion). While empirically
these algorithms perform well  to the best of our knowledge - and efforts - there is no associated
ﬁnite sample guarantee. The analytical challenge lies in the high variance at each step  which makes
direct analysis difﬁcult.

In summary  while much work has focused on memory-constrained PCA  there has as of yet been no
work that simultaneously provides sample complexity guarantees competitive with batch algorithms 
and also memory/storage complexity guarantees close to the minimal requirement of O(kp) – the
memory required to store only the output. We present an algorithm that provably does both.

3 Problem Formulation and Notation

We consider the streaming model: at each time step t  we receive a point xt ∈ Rp. Any point that is
not explicitly stored can never be revisited. Our goal is to compute the top k principal components
of the data: the k-dimensional subspace that offers the best squared-error estimate for the points. We
assume a probabilistic generative model  from which the data is sampled at each step t. Speciﬁcally 

xt = Azt + wt 

(2)

where A ∈ Rp×k is a ﬁxed matrix  zt ∈ Rk×1 is a multivariate normal random variable  i.e. 

zt ∼ N (0k×1  Ik×k) 

wt ∼ N (0p×1  σ2Ip×p).

and wt ∈ Rp×1 is the “noise” vector  also sampled from a multivariate normal distribution  i.e. 

Furthermore  we assume that all 2n random vectors (zt  wt ∀1 ≤ t ≤ n) are mutually independent.
In this regime  it is well-known that batch-PCA is asymptotically consistent (hence recovering A up
to unitary transformations) with number of samples scaling as n = O(p) Vershynin (2010b). It is
interesting to note that in this high-dimensional regime  the signal-to-noise ratio quickly approaches
zero  as the signal  or “elongation” of the major axis  kAzk2  is O(1)  while the noise magnitude 
kwk2  scales as O(√p). The central goal of this paper is to provide ﬁnite sample guarantees for a
streaming algorithm that requires memory no more than O(kp) and matches the consistency results
of batch PCA in the sampling regime n = O(p) (possibly with additional log factors  or factors
depending on σ and k).
We denote matrices by capital letters (e.g. A) and vectors by lower-case bold-face letters (x). kxkq
denotes the ℓq norm of x; kxk denotes the ℓ2 norm of x. kAk or kAk2 denotes the spectral norm of
A while kAkF denotes the Frobenius norm of A. Without loss of generality (WLOG)  we assume
that: kAk2 = 1  where kAk2 = maxkxk2=1 kAxk2 denotes the spectral norm of A. Finally  we
write ha  bi = a⊤b for the inner product between a  b. In proofs the constant C is used loosely and
its value may vary from line to line.

3

Algorithm 1 Block-Stochastic Power Method
input {x1  . . .   xn}  Block size: B
1: q0 ∼ N (0  Ip×p) (Initialization)
2: q0 ← q0/kq0k2
3: for τ = 0  . . .   n/B − 1 do

sτ +1 ← 0
for t = Bτ + 1  . . .   B(τ + 1) do

Bhqτ   xtixt

4:
5:
6:
7:
8:
9: end for

sτ +1 ← sτ +1 + 1

end for
qτ +1 ← sτ +1/ksτ +1k2

Block-Stochastic Orthogonal Iteration

H i ∼ N (0  Ip×p)  1 ≤ i ≤ k (Initialization)
H ← Q0R0 (QR-decomposition)
Sτ +1 ← 0
Sτ +1 ← Sτ +1 + 1
Sτ +1 = Qτ +1Rτ +1 (QR-decomposition)

B xtx⊤t Qτ

output

4 Algorithm and Guarantees

In this section  we present our proposed algorithm and its ﬁnite sample analysis. It is a block-wise
stochastic variant of the classical power-method. Stochastic versions of the power method already
exist in the literature; see Arora et al. (2012). The main impediment to the analysis of such stochastic
algorithms (as in (1)) is the large variance of each step  in the presence of noise. This motivates us
to consider a modiﬁed stochastic power method algorithm  that has a variance reduction step built
in. At a high level  our method updates only once in a “block” and within one block we average out
noise to reduce the variance.

Below  we ﬁrst illustrate the main ideas of our method as well as our sample complexity proof for
the simpler rank-1 case. The rank-1 and rank-k algorithms are so similar  that we present them in
the same panel. We provide the rank-k analysis in Section 4.2. We note that  while our algorithm
describes {x1  . . .   xn} as “input ” we mean this in the streaming sense:
the data are no-where
stored  and can never be revisited unless the algorithm explicitly stores them.

4.1 Rank-One Case

We ﬁrst consider the rank-1 case for which each sample xt is generated using: xt = uzt + wt
where u ∈ Rp is the principal component that we wish to recover. Our algorithm is a block-wise
method where all the n samples are divided in n/B blocks (for simplicity we assume that n/B is
an integer). In the (τ + 1)-st block  we compute

sτ +1 = 


1
B

B(τ +1)

Xt=Bτ +1

xtx⊤t 

 qτ .

(3)

Then  the iterate qτ is updated using qτ +1 = sτ +1/ksτ +1k2. Note that  sτ +1 can be computed
online  with O(p) operations per step. Furthermore  storage requirement is also linear in p.

4.1.1 Analysis

the total number of iterations T = Ω(

We now present the sample complexity analysis of our proposed method. Using O(σ4p log(p)/ǫ2)
samples  Algorithm 1 obtains a solution qT of accuracy ǫ  i.e. kqT − uk2 ≤ ǫ.
Theorem 1. Denote the data stream by x1  . . .   xn  where xt ∈ Rp ∀t is generated by (2).
log((σ2+.75)/(σ2+.5)) ) and the block size B =
Set
Ω( (1+3(σ+σ2)√p)2 log(T )
). Then  with probability 0.99  kqT − uk2 ≤ ǫ  where qT is the T -th
iterate of Algorithm 1. That is  Algorithm 1 obtains an ǫ-accurate solution with number of samples
(n) given by:
n = ˜Ω(cid:18) (1 + 3(σ + σ2)√p)2 log(p/ǫ)
ǫ2 log((σ2 + .75)/(σ2 + .5)) (cid:19) .

log(p/ǫ)

ǫ2

Note that in the total sample complexity  we use the notation ˜Ω(·) to suppress the extra log(T ) factor
for clarity of exposition  as T already appears in the expression linearly.

4

Proof. The proof decomposes the current iterate into the component of the current iterate  qτ   in the
direction of the true principal component (the spike) u  and the perpendicular component  showing
that the former eventually dominates. Doing so hinges on three key components: (a) for large enough
B  the empirical covariance matrix Fτ +1 = 1
t=Bτ +1 xtx⊤t is close to the true covariance matrix
M = uu⊤ + σ2I  i.e.  kFτ +1 − Mk2 is small. In the process  we obtain “tighter” bounds for
ku⊤(Fτ +1 − M )uk for ﬁxed u; (b) with probability 0.99 (or any other constant probability)  the
initial point q0 has a component of at least O(1/√p) magnitude along the true direction u; (c) after
τ iterations  the error in estimation is at most O(γτ ) where γ < 1 is a constant.

B PB(τ +1)

There are several results that we use repeatedly  which we collect here  and prove individually in the
full version of the paper (Mitliagkas et al. (2013)).
Lemmas 4  5 and 6. Let B  T and the data stream {xi} be as deﬁned in the theorem. Then:

• (Lemma 4): With probability 1 − C/T   for C a universal constant  we have:

• (Lemma 5): With probability 1 − C/T   for C a universal constant  we have:

1

B Xt

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
xtx⊤t − uu⊤ − σ2I(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
u⊤sτ +1 ≥ u⊤qτ (1 + σ2)(cid:18)1 −

≤ ǫ.

ǫ

4(1 + σ2)(cid:19)  

where st = 1

B PBτ <t≤B(τ +1) xtx⊤t qτ .

• (Lemma 6): Let q0 be the initial guess for u  given by Steps 1 and 2 of Algorithm 1. Then 

w.p. 0.99: |hq0  ui| ≥ C0√p   where C0 > 0 is a universal constant.

Step (a) is proved in Lemmas 4 and 5  while Lemma 6 provides the required result for the initial
vector q0. Using these lemmas  we next complete the proof of the theorem. We note that both (a)
and (b) follow from well-known results; we provide them for completeness.

Let qτ = √1 − δτ u+√δτ gτ   1 ≤ τ ≤ n/B  where gτ is the component of qτ that is perpendicular
to u and √1 − δτ is the magnitude of the component of qτ along u. Note that gτ may well change
at each iteration; we only wish to show δτ → 0.
Now  using Lemma 5  the following holds with probability at least 1 − C/T :
4(1 + σ2)(cid:19) .

u⊤sτ +1 ≥ p1 − δτ (1 + σ2)(cid:18)1 −

(4)

ǫ

Next  we consider the component of sτ +1 that is perpendicular to u:

g⊤τ +1sτ +1 = g⊤τ +1


1
B

B(τ +1)

Xt=Bτ +1

xtx⊤t 

 qτ = g⊤τ +1(M + Eτ )qτ  

where M = uu⊤ +σ2I and Eτ is the error matrix: Eτ = M − 1
kEτk2 ≤ ǫ (w.p. ≥ 1 − C/T ). Hence  w.p. ≥ 1 − C/T :

B PB(τ +1)

t=Bτ +1 xtx⊤t . Using Lemma 4 

g⊤τ +1sτ +1 = σ2g⊤τ +1qτ + kgτ +1k2kEτk2kqτk2 ≤ σ2pδτ + ǫ.

(5)

Now  since qτ +1 = sτ +1/ksτ +1k2 

δτ +1 = (g⊤τ +1qτ +1)2 =

(g⊤τ +1sτ +1)2

(u⊤sτ +1)2 + (g⊤τ +1sτ +1)2

 

(i)

≤

(ii)

≤

(g⊤τ +1sτ +1)2

4(cid:1)2
(1 − δτ )(cid:0)1 + σ2 − ǫ
4(cid:1)2
(1 − δτ )(cid:0)1 + σ2 − ǫ

(σ2√δτ + ǫ)2

+ (g⊤τ +1sτ +1)2

 

+ (σ2√δτ + ǫ)2

5

 

(6)

where  (i) follows from (4) and (ii) follows from (5) along with the fact that x

function in x for c  x ≥ 0. Assuming √δτ ≥ 2ǫ and using (6) and bounding the failure probability
with a union bound  we get (w.p. ≥ 1 − τ · C/T )

c+x is an increasing

δτ (σ2 + 1/2)2

(i)

γ2τ δ0

(ii)

(1 − δτ )(σ2 + 3/4)2 + δτ (σ2 + 1/2)2

δτ +1 ≤
where γ = σ2+1/2
σ2+3/4 and C1 > 0 is a global constant. Inequality (ii) follows from Lemma 6; to prove
(i)  we need the following lemma. It shows that in the recursion given by (7)  δτ decreases at a fast
rate. The rate of decrease in δτ might be initially (for small τ ) sub-linear  but for large enough τ the
rate is linear. We defer the proof to the full version of the paper (Mitliagkas et al. (2013)).

1 − (1 − γ2τ )δ0

≤ C1γ2τ p 

≤

(7)

Lemma 2. If for any τ ≥ 0 and 0 < γ < 1  we have δτ +1 ≤

  then 

γ2δτ

1−δτ +γ2δτ
.

δτ +1 ≤

γ2t+2δ0

1 − (1 − γ2t+2)δ0

Hence  using the above equation after T = O (log(p/ǫ)/ log (1/γ)) updates  with probability at

least 1 − C  √δT ≤ 2ǫ. The result now follows by noting that ku − qTk2 ≤ 2√δT .

Remark: In Theorem 1  the probability of recovery is a constant and does not decay with p. One can
correct this by either paying a price of O(log p) in storage  or in sample complexity: for the former 
we can run O(log p) instances of Algorithm 1 in parallel; alternatively  we can run Algorithm 1
O(log p) times on fresh data each time  using the next block of data to evaluate the old solutions 
always keeping the best one. Either approach guarantees a success probability of at least 1 − 1
pO(1) .
4.2 General Rank-k Case

In this section  we consider the general rank-k PCA problem where each sample is assumed to be
generated using the model of equation (2)  where A ∈ Rp×k represents the k principal components
that need to be recovered. Let A = U ΛV ⊤ be the SVD of A where U ∈ Rp×k  Λ  V ∈ Rk×k.
The matrices U and V are orthogonal  i.e.  U⊤U = I  V ⊤V = I  and Σ is a diagonal matrix with
diagonal elements λ1 ≥ λ2 ··· ≥ λk. The goal is to recover the space spanned by A  i.e.  span(U ).
Without loss of generality  we can assume that kAk2 = λ1 = 1.
Similar to the rank-1 problem  our algorithm for the rank-k problem can be viewed as a streaming
variant of the classical orthogonal iteration used for SVD. But unlike the rank-1 case  we require
a more careful analysis as we need to bound spectral norms of various quantities in intermediate
steps and simple  crude analysis can lead to signiﬁcantly worse bounds. Interestingly  the analysis
is entirely different from the standard analysis of the orthogonal iteration as there  the empirical
estimate of the covariance matrix is ﬁxed while in our case it varies with each block.

For the general rank-k problem  we use the largest-principal-angle-based distance function between
any two given subspaces:

dist (span(U )  span(V )) = dist(U  V ) = kU⊤

⊥ V k2 = kV ⊤

⊥ Uk2 

where U⊥ and V⊥ represent an orthogonal basis of the perpendicular subspace to span(U ) and
span(V )  respectively. For the spiked covariance model  it is straightforward to see that this is
equivalent to the usual PCA ﬁgure-of-merit  the expressed variance.

Theorem 3. Consider a data stream  where xt ∈ Rp for every t is generated by (2)  and the SVD
of A ∈ Rp×k is given by A = U ΛV ⊤. Let  wlog  λ1 = 1 ≥ λ2 ≥ ··· ≥ λk > 0. Let 

T = Ω(cid:18)log(cid:16) p

Then  after T B-size-block-updates  w.p. 0.99  dist(U  QT ) ≤ ǫ. Hence  the sufﬁcient number of
samples for ǫ-accurate recovery of all the top-k principal components is:

(cid:16)(1 + σ)2√k + σ√1 + σ2k√p(cid:17)2

k (cid:19)(cid:19)   B = Ω


kǫ(cid:17) / log(cid:18) σ2 + 0.75λ2

k
σ2 + 0.5λ2

log(T )

λ4
kǫ2

.

n = ˜Ω


(cid:16)(1 + σ)2√k + σ√1 + σ2k√p(cid:17)2
kǫ2 log(cid:16) σ2+0.75λ2
k (cid:17)

k
σ2+0.5λ2

λ4

log(p/kǫ)

.




6

Again  we use ˜Ω(·) to suppress the extra log(T ) factor.

The key part of the proof requires the following additional lemmas that bound the energy of the
current iterate along the desired subspace and its perpendicular space (Lemmas 8 and 9)  and Lemma
10  which controls the quality of the initialization.

Lemmas 8  9 and 10. Let the data stream  A  B  and T be as deﬁned in Theorem 3  σ be the
variance of noise  Fτ +1 = 1

B PBτ <t≤B(τ +1) xtx⊤t and Qτ be the τ -th iterate of Algorithm 1.

• (Lemma 8): ∀ v ∈ Rk and kvk2 = 1  w.p. 1 − 5C/T we have:

kU⊤Fτ +1Qτ vk2 ≥ (λ2

k + σ2 −
• (Lemma 9): With probability at least 1 − 4C/T  
⊥ Fτ +1Qτk2 ≤ σ2kU⊤

kU⊤

⊥ Qτk2 + λ2

kǫ/2.

λ2
kǫ
4

)q1 − kU⊤

⊥

Qτk2
2.

• (Lemma 10): Let Q0 ∈ Rp×k be sampled uniformly at random as in Algorithm 1. Then 
w.p. at least 0.99: σk(U⊤Q0) ≥ Cq 1
kp .

We provide the proof of the lemmas and theorem in the full version (Mitliagkas et al. (2013)).

4.3 Perturbation-tolerant Subspace Recovery

While our results thus far assume A has rank exactly k  and k is known a priori  here we show that
both these can be relaxed; hence our results hold in a quite broad setting.

Let xt = Azt + wt be the t-th step sample  with A = U ΛV T ∈ Rp×r and U ∈ Rp×r  where r ≥ k
is the unknown true rank of A. We run Algorithm 1 with rank k to recover a subspace QT that is
contained in U . The largest principal angle-based distance  from the previous section  can be used
directly in our more general setting. That is  dist(U  QT ) = kU T
QTk2 measures the component of
⊥
QT “outside” the subspace U .

Now  our analysis can be easily modiﬁed to handle this case. Naturally  now the number of samples
we require increases according to r. In particular  if

n = ˜Ω


(cid:0)(1 + σ)2√r + σ√1 + σ2r√p(cid:1)2
rǫ2 log(cid:16) σ2+0.75λ2
r (cid:17)

r
σ2+0.5λ2

λ4

log(p/rǫ)


  

then dist(U  QT ) ≤ ǫ. Furthermore  if we assume r ≥ C · k (or a large enough constant C >
0) then the initialization step provides us better distance  i.e.  dist(U  Q0) ≤ C′/√p rather than
dist(U  Q0) ≤ C′/√kp bound if r = k. This initialization step enables us to give tighter sample
complexity as the r√p in the numerator above can be replaced by √rp.

5 Experiments

In this section  we show that  as predicted by our theoretical results  our algorithm performs close
to the optimal batch SVD. We provide the results from simulating the spiked covariance model 
and demonstrate the phase-transition in the probability of successful recovery that is inherent to the
statistical problem. Then we stray from the analyzed model and performance metric and test our
algorithm on real world–and some very big–datasets  using the metric of explained variance.

In the experiments for Figures 1 (a)-(b)  we draw data from the generative model of (2). Our results
are averaged over at least 200 independent runs. Algorithm 1 uses the block size prescribed in
Theorem 3  with the empirically tuned constant of 0.2. As expected  our algorithm exhibits linear
scaling with respect to the ambient dimension p – the same as the batch SVD. The missing point
on batch SVD’s curve (Figure 1(a))  corresponds to p > 2.4 · 104. Performing SVD on a dense
p × p matrix  either fails or takes a very long time on most modern desktop computers; in contrast 
our streaming algorithm easily runs on this size problem. The phase transition plot in Figure 1(b)

7

106

104

102

l

)
s
e
p
m
a
s
(
 
n

100

 
102

Samples to retrieve spike (σ=0.5  ε=0.05)

 

.
)
p
(
 

i

n
o
s
n
e
m
d

i

 
t

i

n
e
b
m
A

Batch SVD
Our algorithm (streaming)

103
p (dimension)

(a)

104

Probability of success (n=1000  ε=0.05).

 

104

103

102

101

 

10−2

10−1

Noise standard deviation (σ).

100

(b)

1

0.8

0.6

0.4

0.2

0

 

e
c
n
a
i
r
a
v
 

i

d
e
n
a
p
x
E

l

40%

30%

20%

10%

0%

 

2

Our algorithm on large bag−of−words datasets

NY Times: 300K samples  p=103K
PubMed: 8.2M samples  p=140K

NIPS bag−of−words dataset

 

20%

e
c
n
a
i
r
a
v
 

i

d
e
n
a
p
x
E

l

10%

Optimal (batch)
Our algorithm (streaming)
Optimal using B samples

4

8
k (number of components)

6

10

0%
 
1

2

3

4

k (number of components)

5

6

7

(c)

(d)

Figure 1: (a) Number of samples required for recovery of a single component (k = 1) from the
spiked covariance model  with noise standard deviation σ = 0.5 and desired accuracy ǫ = 0.05.
(b) Fraction of trials in which Algorithm 1 successfully recovers the principal component (k = 1)
in the same model  with ǫ = 0.05 and n = 1000 samples  (c) Explained variance by Algorithm 1
compared to the optimal batch SVD  on the NIPS bag-of-words dataset. (d) Explained variance by
Algorithm 1 on the NY Times and PubMed datasets.

shows the empirical sample complexity on a large class of problems and corroborates the scaling
with respect to the noise variance we obtain theoretically.

Figures 1 (c)-(d) complement our complete treatment of the spiked covariance model  with some
out-of-model experiments. We used three bag-of-words datasets from Porteous et al. (2008). We
evaluated our algorithm’s performance with respect to the fraction of explained variance metric:
given the p × k matrix V output from the algorithm  and all the provided samples in matrix X  the
fraction of explained variance is deﬁned as Tr(V T XX T V )/ Tr(XX T ). To be consistent with our
theory  for a dataset of n samples of dimension p  we set the number of blocks to be T = ⌈log(p)⌉
and the size of blocks to B = ⌊n/T⌋ in our algorithm. The NIPS dataset is the smallest  with
1500 documents and 12K words and allowed us to compare our algorithm with the optimal  batch
SVD. We had the two algorithms work on the document space (p = 1500) and report the results in
Figure 1(c). The dashed line represents the optimal using B samples. The ﬁgure is consistent with
our theoretical result: our algorithm performs as well as the batch  with an added log(p) factor in
the sample complexity.

Finally  in Figure 1 (d)  we show our algorithm’s ability to tackle very large problems. Both the
NY Times and PubMed datasets are of prohibitive size for traditional batch methods – the latter
including 8.2 million documents on a vocabulary of 141 thousand words – so we just report the
performance of Algorithm 1. It was able to extract the top 7 components for each dataset in a few
hours on a desktop computer. A second pass was made on the data to evaluate the results  and we
saw 7-10 percent of the variance explained on spaces with p > 104.

8

References

Arora  R.  Cotter  A.  Livescu  K.  and Srebro  N. Stochastic optimization for PCA and PLS. In 50th Allerton

Conference on Communication  Control  and Computing  Monticello  IL  2012.

Balzano  L.  Nowak  R.  and Recht  B. Online identiﬁcation and tracking of subspaces from highly incomplete
information. In Communication  Control  and Computing (Allerton)  2010 48th Annual Allerton Conference
on  pp. 704–711  2010.

Brand  M. Fast low-rank modiﬁcations of the thin singular value decomposition. Linear algebra and its

applications  415(1):20–30  2006.

Brand  Matthew. Incremental singular value decomposition of uncertain data with missing values. Computer

Vision—ECCV 2002  pp. 707–720  2002.

Clarkson  Kenneth L. and Woodruff  David P. Numerical linear algebra in the streaming model. In Proceedings

of the 41st annual ACM symposium on Theory of computing  pp. 205–214  2009.

Comon  P. and Golub  G. H. Tracking a few extreme singular values and vectors in signal processing. Proceed-

ings of the IEEE  78(8):1327–1343  1990.

Golub  Gene H. and Van Loan  Charles F. Matrix computations  volume 3. JHUP  2012.

Halko  Nathan  Martinsson  Per-Gunnar  and Tropp  Joel A. Finding structure with randomness: Probabilistic

algorithms for constructing approximate matrix decompositions. SIAM review  53(2):217–288  2011.

He  J.  Balzano  L.  and Lui  J. Online robust subspace tracking from partial information. arXiv preprint

arXiv:1109.3827  2011.

Herbster  Mark and Warmuth  Manfred K. Tracking the best linear predictor. The Journal of Machine Learning

Research  1:281–309  2001.

Johnstone  Iain M. On the distribution of the largest eigenvalue in principal components analysis.(english. Ann.

Statist  29(2):295–327  2001.

Li  Y. On incremental and robust subspace learning. Pattern recognition  37(7):1509–1518  2004.

Mitliagkas  Ioannis  Caramanis  Constantine  and Jain  Prateek. Memory limited  streaming PCA. arXiv

preprint arXiv:1307.0032  2013.

Nadler  Boaz. Finite sample approximation results for principal component analysis: a matrix perturbation

approach. The Annals of Statistics  pp. 2791–2817  2008.

Porteous  Ian  Newman  David  Ihler  Alexander  Asuncion  Arthur  Smyth  Padhraic  and Welling  Max. Fast
collapsed gibbs sampling for latent dirichlet allocation. In Proceedings of the 14th ACM SIGKDD interna-
tional conference on Knowledge discovery and data mining  pp. 569–577  2008.

Robbins  Herbert and Monro  Sutton. A stochastic approximation method. The Annals of Mathematical Statis-

tics  pp. 400–407  1951.

Roweis  Sam. EM algorithms for PCA and SPCA. Advances in neural information processing systems  pp.

626–632  1998.

Rudelson  Mark and Vershynin  Roman. Smallest singular value of a random rectangular matrix. Communica-

tions on Pure and Applied Mathematics  62(12):1707–1739  2009.

Tipping  Michael E. and Bishop  Christopher M. Probabilistic principal component analysis. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  61(3):611–622  1999.

Vershynin  R. How close is the sample covariance matrix to the actual covariance matrix? Journal of Theoret-

ical Probability  pp. 1–32  2010a.

Vershynin  Roman.

Introduction to the non-asymptotic analysis of random matrices.

arXiv preprint

arXiv:1011.3027  2010b.

Warmuth  Manfred K. and Kuzmin  Dima. Randomized online PCA algorithms with regret bounds that are

logarithmic in the dimension. Journal of Machine Learning Research  9:2287–2320  2008.

9

,Ioannis Mitliagkas
Constantine Caramanis
Prateek Jain