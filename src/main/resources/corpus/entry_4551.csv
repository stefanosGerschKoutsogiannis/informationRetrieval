2011,Randomized Algorithms for Comparison-based Search,This paper addresses the problem of finding the nearest neighbor (or    one of the $R$-nearest neighbors) of a query object $q$ in a    database of $n$ objects  when we can only use a comparison    oracle. The comparison oracle  given two reference objects and a    query object  returns the reference object most similar to the query    object.  The main problem we study is how to search the database for    the nearest neighbor (NN) of a query  while minimizing the    questions.  The difficulty of this problem depends on properties of    the underlying database. We show the importance of a    characterization: \emph{combinatorial disorder} $D$ which defines    approximate triangle inequalities on ranks. We present a lower bound    of $\Omega(D\log \frac{n}{D}+D^2)$ average number of questions in    the search phase for any randomized algorithm  which demonstrates    the fundamental role of $D$ for worst case behavior. We develop a    randomized scheme for NN retrieval in $O(D^3\log^2 n+ D\log^2 n    \log\log n^{D^3})$ questions.  The learning requires asking $O(n    D^3\log^2 n+ D \log^2 n \log\log n^{D^3})$ questions and    $O(n\log^2n/\log(2D))$ bits to store.,Randomized Algorithms for Comparison-based

Search

Dominique Tschopp

AWK Group

Bern  Switzerland

Suhas Diggavi

University of California Los Angeles (UCLA)

Los Angeles  CA 90095

dominique.tschopp@gmail.com

suhasdiggavi@ucla.edu

Payam Delgosha

Sharif University of Technology

Tehran  Iran

Soheil Mohajer

Princeton University
Princeton  NJ 08544

pdelgosha@ee.sharif.ir

smohajer@princeton.edu

Abstract

This paper addresses the problem of ﬁnding the nearest neighbor (or one of the
R-nearest neighbors) of a query object q in a database of n objects  when we can
only use a comparison oracle. The comparison oracle  given two reference objects
and a query object  returns the reference object most similar to the query object.
The main problem we study is how to search the database for the nearest neighbor
(NN) of a query  while minimizing the questions. The difﬁculty of this problem
depends on properties of the underlying database. We show the importance of a
characterization: combinatorial disorder D which deﬁnes approximate triangle
inequalities on ranks. We present a lower bound of Ω(D log n
D + D2) average
number of questions in the search phase for any randomized algorithm  which
demonstrates the fundamental role of D for worst case behavior. We develop
a randomized scheme for NN retrieval in O(D3 log2 n + D log2 n log log nD3
)
questions. The learning requires asking O(nD3 log2 n + D log2 n log log nD3
)
questions and O(n log2 n/ log(2D)) bits to store.

1 Introduction

Consider the situation where we want to search and navigate a database  but the underlying relation-
ships between the objects are unknown and are accessible only through a comparison oracle. The
comparison oracle  given two reference objects and a query object  returns the reference object most
similar to the query object. Such an oracle attempts to model the behavior of human users  capable
of making statements about similarity  but not of assigning meaningful numerical values to distances
between objects. These situations could occur in many tasks  such as recommendation for movies 
restaurants etc.  or a human-assisted search system for image databases among other applications.
Using such an oracle  the best we can hope for is to obtain  for every object u in the database  a
ranking of the other objects according to their similarity to u. However  the use of the oracle to
get complete information about ranking could be costly  since invoking the oracle is to represent
human input to the task (preferences in movies  comparison of images etc). We can pre-process
the database by asking questions during a learning phase  and use the resulting answers to facilitate
the search process. Therefore  the main question we ask in this paper is to design a (approximate)
nearest neighbor retrieval algorithm while minimizing the number of questions to such an oracle.

Clearly the difﬁculty of searching using such an oracle depends critically on the properties of the set
of objects. We demonstrate the importance of a characterization which determines the performance

1

of comparison based search algorithms. Combinatorial disorder (introduced by Goyal et al. [1]) 
deﬁnes approximate triangle inequalities on ranks. Roughly speaking  it deﬁnes a multiplicative
factor D by which the triangle inequality on ranks can be violated. We show our ﬁrst lower bound of
Ω(D log n
D +D2) on average number of questions in the search phase for any randomized algorithm 
and therefore demonstrate the fundamental importance of D for worst case behavior. When the
disorder is known  we can use partial rank information to estimate  or infer the other ranks. This
allows us to design a novel hierarchical scheme which considerably improves the existing bounds
for nearest neighbor search based on a similarity oracle  and performs provably close to the lower
bound. If no characterization of the hidden space can be used as an input  we develop algorithms
that can decompose the space such that dissimilar objects are likely to get separated  and similar
objects have the tendency to stay together; generalizing the notion of randomized k-d-trees [2]. This
is developed in more detail in [3]. Due to space constraints  we give statements of the results along
with an outline of proof ideas in the main text. Additionally we provide proof details in the appendix
[4] as extra material allowed by NIPS.
Relationship to published works: Nearest neighbor (NN) search problem has been very well stud-
ied for metric spaces (see [5]). However  in all these works  it is assumed that one can compute
distances between points in the data set. In [6  7  8  9  10  11]  various approaches to measure simi-
larities between images are presented  which could be used as comparison oracles in our setup. The
algorithmic aspects of searching with a comparison oracle was ﬁrst studied in [1]  where a random
walk algorithm is presented. The main limitation of this algorithm is the fact that all rank relation-
ships need to be known in advance  which amounts to asking the oracle O(n2 log n) questions  in
a database of size n. In [12]  a data structure similar in spirit to ǫ-nets of [13] is introduced. It
is shown that a learning phase with complexity O(D7n log2 n) questions and a space complexity
of O(D5n + Dn log n) allows to retrieve the NN in O(D4 log n) questions. The learning phase
builds a hierarchical structure based on coverings of exponentially decreasing radii. In this paper 
we present what we believe is the ﬁrst lower bound for search through comparisons. This gives
a more fundamental meaning to D as a parameter determining worst case behavior. Based on the
insights gained from this worst case analysis  we then improve (see Section 3) the existing upper
bounds by a poly(D) factor  if we are willing to accept a negligible (less than 1
n ) probability of
failure. Our algorithm is based on random sampling  and can be seen as a form of metric skip list (as
introduced in [14])  but applied to a combinatorial (non-metric) framework. However  the fact that
we do not have access to distances forces us to use new techniques in order to minimize the number
of questions (or ranks we need to compute). In particular  we sample the database at different densi-
ties  and infer the ranks from the density of the sampling  which we believe is a new technique. We
also need to relate samples to each other when building the data structure top down.

A natural question to ask is whether one can develop data structures for NN when a characterization
of the underlying space is unknown.
In [2]  when one has access to metric distances  a binary
tree decomposition of a dataset that adapts to its “intrinsic dimension” [13] has been designed. We
extend the result of [2] to our setup  where we have a comparison oracle but do not have access to
metric distances. This can be used in a manner similar to [2] to ﬁnd (approximate) NN (see [3] for
more details).

To the best of our knowledge  the notion of randomized NN search using similarity oracle is studied
for the ﬁrst time in this paper. Moreover  the hierarchical search scheme proposed is more efﬁcient
than earlier schemes. The lower bound presented appears to be new and demonstrates that our
schemes are (almost) efﬁcient.

2 Deﬁnitions and Problem Statement

We consider a hidden space K  and a database of objects T ⊂ K  with |T | = n. We can only access
this space through a similarity oracle which for any point q ∈ K  and objects u  v ∈ T returns

O(q  u  v) = (cid:26) u if u is more similar to q than v

else.

v

(1)

The goal is to develop and analyse algorithms which for any given q ∈ K  can ﬁnd an object in the
database a ∈ T which is the nearest neighbor (NN) to q  using the smallest number of questions of
type (1). We also relax this goal to ﬁnd the approximate NN with “high probability”. The algorithm

2

may have a learning phase  in which it explores the structure of the database  and stores it using a
certain amount of memory. Note that this phase has to be done prior to knowing the query q ∈ K.
Then  once the query is given  the search phase of the algorithm asks a certain number of questions
of type (1) and ﬁnds the closest object in the database.

The performance of the algorithm is measured by three components among which there could be
a trade-off: the number of questions asked in the learning phase  the number of questions asked in
the searching phase  and the total memory to be stored. The main goal of this work is to design
algorithms for NN search and characterize its performance in terms of these parameters. We will
present some deﬁnitions which are required to state the results of this paper.
Deﬁnition 1. The rank of u in a set S with respect to v  rv(u S) is equal to c  if u is the cth nearest
object to v in S  i.e.  |{w ∈ S : d(w  v) < d(u  v)}| = c − 1  where d(w  v) < d(u  v) could be
interpreted as a distance function. Also the rank ball βx(r) is deﬁned to be {y : rx(y S)) ≤ r}.
Note that we do not need existence of a distance function in Deﬁnition 1. We could replace
d(w  v) < d(u  v) with “v is more similar to w than u” by using the oracle in (1).
To simplify the notation  we only indicate the set if it is unclear from the context i.e.  we write rv(u)
instead of rv(u S) unless there is an ambiguity. Note that rank need not be a symmetric relationship
between objects i.e.  ru(v) 6= rv(u) in general. Further  note that we can rank m objects w.r.t. an
object o by asking the oracle O(m log m) questions  using standard sort algorithms [15].
Our characterization of the space of objects is through a form of approximate triangle inequali-
ties introduced in [1] and [12]. Instead of deﬁning a inequalities between distances  these triangle
inequalities deﬁned over ranks  and depend on a property of the space  called the disorder constant.
Deﬁnition 2. The combinatorial disorder of a set of objects S is the smallest D such that ∀x  y  z ∈
S  we have the following approximate triangle inequalities:

(i) rx(y  S) ≤ D(rz(x  S) + rz(y  S))
(iii) rx(y  S) ≤ D(rx(z  S) + rz(y  S))

(ii) rx(y  S) ≤ D(rx(z  S) + ry(z  S))
(iv) rx(y  S) ≤ D(rz(x  S) + ry(z  S))

In particular  rx(x  S) = 0 and rx(y  S) ≤ Dry(x  S).
3 Contributions

Our contributions are the following: (i) we design a randomized hierarchical data structure with
which we can do NN search using the comparison oracle (ii) we develop the ﬁrst lower bound
for the search complexity in the combinatorial framework of [1  12]  and thereby demonstrate the
importance of combinatorial disorder. The performance of the randomized algorithm (see (i)) is
shown to be close to this lower bound. We also develop a binary tree decomposition that adapts to
the data set in a manner analogous to [2].

More precisely  we prove a lower bound on the average search time to retrieve the nearest neighbor
of a query point for randomized algorithms in the combinatorial framework.
Theorem 1. There exists a space  a conﬁguration of a database of n objects in that space that
for the uniform distribution over placements of the query point q such that no randomized search
algorithm  even if O(n3) questions can be asked in the learning phase  can ﬁnd q’s nearest neighbor
in the database for sure (with a probability of error of 0) by asking less than an expected Ω(D2 +
D log n/D) questions in the worst case when D < √n.

As a consequence of this theorem  there must exist at least one query point in this conﬁguration
which requires asking at least Ω(D log( n
D ) + D2) questions  hence setting a lower bound on the
search complexity. Based on the insights gained from this worst case analysis  we introduce a
conceptually simple randomized hierarchical scheme that allows us to reduce the learning compared
to the existing algorithm (see [12  1]) by a factor D4  memory consumption by a factor D5/ log2 n 
and a factor D/ log n log log nD3 for search.
Theorem 2. We design a randomized algorithm  which for a given query point q  can retrieve
its nearest neighbor with high probability in O(D3 log2 n + D log2 n log log nD3
) questions. The

3

learning requires asking O(nD3 log2 n + D log2 n log log nD3
O(n log2 n/ log(2D)) bits.

) questions and we need to store

Consequently  our schemes are asymptotically (for n) within Dpolylog(n) questions of the optimal
search algorithm.

4 Lower Bounds for NNS

A natural question to ask is whether there are databases and query points for which we need to ask
a minimal number of questions  independent of the algorithm used. In this section  we construct
a database T of n objects  a universe of queries K\T and similarity relationships  for which no
search algorithm can ﬁnd the NN of a query point in less than expected Ω(D log n
D + D2) questions.
We show this even when all possible questions O(u  v  w) related to the n database objects (i.e. 
u  v  w ∈ T ) can be asked during the learning phase. The query is chosen uniformly from the
universe of queries and is unknown during the learning phase.
Database Structure: Consider the weighted graph shown in Fig. 1. It consists of a star with α
branches φ1  φ2  . . .   φα  each composed of n/α2 supernodes (SN). Each of the supernodes in turn
contains α database objects (i.e.  objects in T ). Clearly  in total there are αα n
α2 = n objects.
Note that the database T only includes the set of objects inside the supernodes  and the supernodes 
themselves  are not element of T . We indicate the objects in each branch by numbers from 1 to n/α.
We deﬁne the set of queries  M  as follows: every query point q is attached to one object form T on
each branch of the star with an edge; this object is called a direct node (DN) on the corresponding
branch. Moreover  we assume that the weights of all query edges  the α edges connecting the query
to its DNs  are different. Therefore  the set of all queries  M could be restricted to α!(n/α)α
elements  since there are n/α choices for choosing the direct node in each branch (i.e.  (n/α)α
choices for α branches)  and the weight of the query edges can be ordered in α! different ways.
In this example  distance between two nodes is given by the weighted graph distance  and the oracle
answers queries based on this distance. All edges connecting the SNs to each other have weight 1
expect those α edges emitting from the center of the star and ending at the ﬁrst SNs which have
weight n/(α2). Edges connecting the objects in a supernode to its root are called object edges. We
assume that all n/α object edges in branch φi have weight i/(4α). It remains to ﬁx the weight of
the query edges. We will deﬁne the weight of these edges in the following.
Deﬁnition 3. For a query q ∈ M  deﬁne the α-tuple δq ∈ {1  2  . . .   n/α}α to be the sequence of
DNs of q in α branches  i.e.  δq(i) denotes the indicator of the object on φi which is connected to q
via a query edge. We also represent the rank of the DNs w.r.t. q  by an α-tuple Ψq ∈ {1  . . .   α}α 
i.e.  Ψq(i) denotes the rank of the DN on branch φi among all the other DNs w.r.t. q.
Now we can deﬁne the weight of query edges. For a query q ∈ M  the weight of the query edge
which connects q to δq(i) is given to be 1 + (Ψq(i)/α)ǫ  where ǫ ≪ 1/(4α) is a constant.
As mentioned before  the disorder constant plays an important role in the performance of the algo-
rithm. The following lemma gives the disorder constant for the database introduced. The proof of
this lemma is presented in the appendix [4].
Lemma 1. The star shaped graph introduced above has disorder constant D = Θ(α).

The Lower Bound: In the proof of Theorem 1  we will use Yao’s minimax principle (see [16]) 
which states that  for any distribution on the inputs the expected cost for the best deterministic
algorithm provides a lower bound on the worst case running time of any randomized algorithm. In
the following  we state two lower bounds for the number of questions in the searching phase of any
deterministic algorithm for the database illustrated in Fig. 1.
Proposition 1. The number of questions asked by a deterministic algorithm A  on average w.r.t.
uniform distribution  to solve the NNS problem in star graph  is lower bounded by Ω (α log(n/α)).

To outline the proof of this claim: each question asked by the algorithm involves two database
nodes. Note that the weights of the edges emitting from the center of the graph are chosen so that
the branches become independent  in the sense that questioning nodes on one branch will not reveal

4

Figure 1: The star database: a weighted star graph with α branches  each composed of n/α2 “su-
pernodes”. Each supernode further includes α database objects. Finally  each query points is ran-
domly connected to one object on each branch of the star via a weighted edge. The weights of the
edges are chosen so than the disorder constant be D = Θ(α).

any information about other branches. Therefore  in order to ﬁnd the nearest node to q  the algorithm
has to ﬁnd the direct node on each branch  and then compare them to ﬁnd the NN. For any branch
φi  there are n/α candidates which can be DN of q with equal probability. Hence  roughly speaking 
the algorithm needs to ask Ω(log(n/α)) questions for each branch. This yields to a minimum total
of Ω(α log(n/α)) questions for α independent branches in the graph.
Proposition 2. Any deterministic algorithm A solving nearest neighbor search problem in the input
query set M with uniform distribution should ask on average Ω(α2) questions from the oracle.
To outline the proof of this claim: consider an arbitrary branch φi and assume a genie tells us that
which supernode on φi contains the DN for q. However  we do not know which of p1  p2  . . .   pα 
the nodes inside the revealed supernode  is the DN of q on φi. Since all the edges connecting the
supernode to its children have the same weight  questioning just some of them is not sufﬁcient to
ﬁnd the direct node  and effectively all of them should be asked on average. Since each question
involves at most two of such nodes  an Ω(α) questions is required to ﬁnd the DN on φi. Summing up
the same number over all α branches  we obtain the Ω(α2) lower bound on the number of questions.
Theorem 1 is a direct consequence of the above mentioned propositions.

Proof of Theorem 1. Let A be an arbitrary deterministic algorithm which solves NNS problem in
star shaped graph with uniform distribution. If QA denotes the average number of questions A asks 
according to Proposition 1 and Proposition 2 we have

QA ≥ maxnΩ(cid:16)α log

n

α(cid:17)   Ω(α2)o ≥

1

2 (cid:16)Ω(cid:16)α log

n

α(cid:17)   Ω(α2)(cid:17) = Ω(cid:0)α2 + α log n/α(cid:1) .

(2)

By using the Yao’s Minimax principle  we can conclude Theorem 1.

We can show that this bound is best bound one can ﬁnd for this dataset. Indeed  we present an

algorithm in the appendix [4]  which ﬁnds the query by asking Θ(cid:0)α2 + α log n/α(cid:1) questions.

5

5 Hierarchical Data Structure For Nearest-Neighbor Search

In this section we develop the search algorithm that guarantees the performance stated in Theorem 2.
The learning phase is described in Algorithm 1. The algorithm builds a hierarchical decomposition
level by level  top-down. At each level  we sample objects from the database. The set of samples at
level i is denoted by Si  and we have |Si| = mi = a(2D)i log n  where a is a constant independent1
of n and D. At each level i  every object in T is put in the “bin” of the sample in Si closest to it. To
ﬁnd this sample at level i  for every object p we rank the samples in Si w.r.t. p (by using the oracle
to make pairwise comparisons). However  we show that given that we know D  we only need to
rank those samples that fell in the bin of one of the at most 4aD log n nearest samples to p at level
i − 1. This is a consequence of the fact that we carefully chose the density of the samples at each
level. Further  the fact that we build the hierarchy top-down  allows us to use the answers to the
questions asked at level i  to reduce the number of questions we need to ask at level i + 1. This way 
the number of questions per object does not increase as we go down in the hierarchy  even though
the number of samples increases.
For object p  νp(i) denotes the nearest neighbor to object p in Si. We want to keep the λi =
n/(2D)i−1 closest objects in Si to p in the set Γp(i)  i.e.  all objects o ∈ Si so that rp(o  Si) ≤ λi.
It could be shown that for an object o to be in Γp(i) it is necessary that νo(i − 1) be in Γp(i − 1).
Therefore by taking Λp(i) = {o ∈ Si|νo(i − 1) ∈ Γp(i − 1)} we have Γp(i) ⊆ Λp(i). It could
be veriﬁed that |Γp(i)| ≤ 4aD log n  therefore Γp(i) can be constructed by ﬁnding the 4aD log n
closest objects in Λp(i) to p. Deﬁnitely the ﬁrst object in Γp(i) is νp(i). Therefore we can recursively
build Γp(i)  Λp(i) and νp(i) for 1 ≤ i ≤ log n/ log 2D for any object p  as it is done in the algorithm.
The role of macros BuildHeap and ExtractMin is to build a heap from unordered data  and ex-
tract the minimum element from the heap  respectively. Although they are well-known and standard
algorithms  we will present them in the appendix [4] for completeness.

The search process is described in Algorithm 2. The key idea is that the sample closest to the query
point on the lowest level will be its NN. Hence  by repeating the same process for inserting objects
in the database  we can retrieve the NN w.h.p. We ﬁrst bound the number of questions asked by
Algorithm 1 (w.h.p.)  in Theorem 3. Having this result  the proof of Theorem 2 is then immediate.
Theorem 3. Algorithm 1 succeeds with probability higher than 1 − 1
n   and it requires asking no
more than O(nD3 log2 n + D log2 n log log nD3

) questions w.h.p.

n

(2D)i=1 . For every object p ∈ T ∪{q}  where q is the query

We ﬁrst state a technical lemma that we will need to prove Theorem 3. The proof could be found in
Appendix [4].
Lemma 2. Take a a constant and λi =
point  the following four properties of the data structure are true w.h.p.
1. |Si ∩ βp(λi+1)| ≥ 1
2. |Si ∩ βp(λi)| ≤ 4aD log n
3. |Si+1 ∩ βp(λi−1)| ≤ 16aD3 log n
4. |Si ∩ βp(4λi)| ≥ 4aD log n
5. |Si+1 ∩ βp(4λi−1)| ≤ 64aD3 log n
Proof of Theorem 3. Let mi = a(2D)i log n denote the number of objects we sample at level i  and
let Si be the set of samples at level i i.e.  |Si| = mi. Here  a is an appropriately chosen constant 
independent of D and n. Further  let λi =

n

(2D)i−1 .

From now on  we assume that we are in the situation where Properties (1) to (5) in Lemma 2 are
true for all objects (which is the case w.h.p.). Again  ﬁx an object p. For each object p  we need
to ﬁnd νp(i)  which is the nearest neighbor in Si with respect to p.
In order for being able to
continue this procedure in every level  we keep a wider range of objects: those objects in Si that
have ranks less than λi+1 with respect to p in level i; we store them in Γp(i) (property 1 tells us that
such objects exist)  in this way the ﬁrst object in Γp(i) would be νp(i). In practice our algorithm
stores some redundant objects in Γp(i)  but we claim that totally no more than 4aD log n objects
are stored in Γp(i + 1). To summarize  the properties we want to maintain in each level are: 1-
∀p ∈ T and 1 ≤ i ≤ log n/ log 2D  Si ∩ βp(λi) ⊆ Γp(i) and 2- |Γp(i)| ≤ 4aD log n.

1in fact the value of a is dependent on the value of error we expect  the more accurate we want to be  the

more sample points we need in each level and a would be larger.

6

input : A database with n objects p1  ...  pn  and disorder constant D
output: For each object u  a vector νu of length log n/ log(2D). The list of all samples ∪iSi
Def.: Si: The set of a(2D)i log n random samples at level i  i = 1  . . .   log n/ log(2D);
νo(i) =nearest neighbor to object o in Si; o ∈ T   i = 1  . . .   log n/ log(2D);
contains the λi closest objects to p in Si  possibly with redundant objects;

νo:
Γo(i):
Λo(i): The set of p ∈ Si  for which νp(i − 1) ∈ Γo(i − 1);

log 2D do

for i ← 1 to L = log n
for p ← 1 to n do
if i = 1 then
Λp(1) ← S1
Λp(i) = {o ∈ Si|νo(i − 1) ∈ Γp(i − 1)};
if |Λp(i)| = 0 then
Report Failure
else

else

H ← BuildHeap(Λp(i)) ;
for k ← 1 to 4aD log n do
m ← ExtractMin(H) ;
add m to Γp(i)

end

end
νp(i) ← ﬁrst object in Γp(i);

end

end

end

Algorithm 1: Learning Algorithm

input : A database with n objects and disorder D  the list of samples  the vectors νu for

u ∈ T   a query point q

output: The nearest neighbor of q in the database
Γq(1) = S1;
for i ← 2 to L = log n

log 2D do
Λq(i) ← {p ∈ Si|νp(i − 1) ∈ Γq(i − 1)};
H ← BuildHeap(Λq(i)) ;
for k ← 1 to 4aD log n do
m ← ExtractMin(H) ;
add m to Γq(i)

end

end
return ﬁrst object in Γq( log n

log 2D )

Algorithm 2: Search Algorithm

In the ﬁrst step  for all p  Λp(1) = S1  and since |S1| = 2aD log n < 4aD log n  all the objects in
S1 are extracted from the heap and therefore Γp(i) is S1 ordered with respect to p  as a result both
the properties hold when i = 1. The argument for the maintenance of this property is as follows:
Assume the property holds up to level i; we analyze level i + 1. In fact we want an object s ∈ Si+1
to be in Γp(i + 1) if rp(s) ≤ λi+1 (note that Property 1 guarantees that there is a least one such
sample). Further  let s′ ∈ Si be the sample at level i closest to s i.e.  s′ = minx∈Si rs(s′). Again 
by Property 1  we know that rs(s′) ≤ λi+1. Hence  by the approximate triangle inequality 3 (see
Section 2)  we have:

rp(s T ) ≤ λi+1 and rs(s′ T ) ≤ λi+1 ⇒ rp(s′ T ) ≤ 2Dλi+1 = λi

hence s′ = νs(i) ∈ Si ∩ βp(λi) ⊆ Γp(i) using the ﬁrst property for step i. Therefore νs(i) ∈ Γp(i)
and therefore s ∈ Λp(i + 1). Property 2 tells us that |Si+1 ∩ βp(λi+1)| ≤ 4aD log n. Hence by

7

taking the ﬁrst 4aD log n closest objects to p in Λp(i + 1) and storing them in Γp(i + 1)  we can
make sure than both s ∈ Γp(i + 1) for s ∈ Si+1  s ∈ βp(λi+1) and |Γp(i + 1)| ≤ 4aD log n.
Note that in the last loop of the algorithm when i = log n/ log 2D  according to Property 1  |Si ∩
βp(λi+1)| ≥ 1. But λi+1 in the last step is 1  therefore the closest object to p in the database is
in Slog n/ log 2D  which means that νp(log n/ log 2D) is the nearest neighbor of p in the database.
Repeating this argument for the query point in the Search algorithm shows that after the termination 
the algorithm ﬁnds the nearest neighbor.
To analyze the complexity of the algorithm  we should show that |Λp(i + 1)| is not big. Property 4
tells us that all of the 4aD log n closest samples to p at level i have rank less than 8λi so all objects in
Λp(i) have ranks less than 8λi with respect to p. Consider a sample s ∈ Si such that rp(s T ) ≤ 8λi
and a sample s′′ ∈ Si+1 that falls in the bin of s.
If an object s′′ is in Λp(i + 1)  it means that it falls in the bin of an object s in Γp(i)  i.e. νs′′ (i) ∈
Γp(i). Since s ∈ Γp(i)  we have rp(s  T ) ≤ 8λi.
By property 1  we must have rs′′ (s T ) ≤ λi+1. Thus  by inequality 2  we have:

rs′′ (s T ) ≤ λi+1 and rp(s T ) ≤ 8λi ⇒ rp(s′′ T ) < D(8λi + λi+1) ≤ 4λi−1

By property 5  there are at most O(D3 log n) such samples at level i + 1  i.e. Λp(i + 1) =
O(D3 log n).
To summarize  at each level for each object  we build a heap out of O(D3 log n) objects and ap-
ply O(aD log n) ExtractMin procedures to ﬁnd the ﬁrst 4aD log n objects in the heap. Each
ExtractMin requires O(log(D3 log n)) = O(log log nD3
). Hence the complexity for each level
and for each object is O(D3 log n + D log n log log nD3
). There are O(log n) levels and n objects 
so the overall complexity is O(nD3 log n + nD log2 n log log nD3

).

Proof of Theorem 2. The upper bound on the number of questions to be asked in the learning phase
is immediate from Theorem 3. For each object  we need to store one identiﬁer (the identiﬁer of the
closest object) at every level i in the hierarchy  and one bit to mark it as a member of Si or not;
also one bit if it is in Γq(i − 1) and one bit for being in Λq(i) (we can reuse this memory in the
next level) (note that a heap with size N needs O(N log n) memory  where log n is for storing each
object). Hence  the total memory requirement2 do not exceed O(n log2 n/ log(2D)) bits. Finally 
the properties 1-5 shown in the proof of Theorem 3 are also true for an external query object q.
Hence  to ﬁnd the closest object to q on every level  we build the same heap structure  the only
difference is that instead of repeating this procedure n times in each level  since there is just one
query point  we need to ask at most O(D3 log2 n + D log2 n log log nD3
In
particular  the closest object at level L = log2D(n) will be q’s nearest neighbor w.h.p.

) questions totally.

Note that this scheme can be easily modiﬁed for R-nearest neighbor search. At the i-the level of the
hierarchy  the closest sample to q will  w.h.p.  be one of its
(2D)i nearest neighbors. If we are only
interested in the level of precision  we can stop the hierarchy construction at the desired level.

n

6 Discussion

The use of a comparison oracle is motivated by a human user who can make comparisons between
objects but not assign meaningful numerical values to similarities between objects. There are many
interesting questions raised by studying such a model including fundamental characterizations of the
complexity of search in terms of number of oracle questions. We also believe that ideas of searching
through comparisons form a bridge between many well known search techniques in metric spaces to
perceptually important (non-metric spaces) situations  and could lead to innovative practical appli-
cations. Analogous to locality sensitive hashing  one can develop notions of rank-sensitive hashing 
where “similar” objects based on ranks are given the same hash value. Some preliminary ideas
for it were given in [3]  but we believe this is an interesting line of inquiry. Also in [3]  we have
implemented comparison-based search heuristics to navigate image database.

2Making the assumption that every object can be uniquely identiﬁed with log n bits

8

References

[1] N. Goyal  Y. Lifshits  and H. Schutze  “Disorder inequality: A combinatorial approach to nearest neighbor

search ” in WSDM  2008  pp. 25–32.

[2] S. Dasgupta and Y. Freund  “Random projection trees and low dimensional manifolds ” in STOC  2008 

pp. 537–546.

[3] D. Tschopp  “Routing and search on large scale networks ” Ph.D. dissertation  ´Ecole Polytechnique

F´ed´erale de Lausanne (EPFL)  2010.

[4] D. Tschopp  S. Diggavi  P. Delgosha  and S. Mohajer  “Randomized algorithms for comparison-based

search: Supplementary material ” 2011  submitted to NIPS as supplementary material.

[5] K. Clarkson  “Nearest-neighbor searching and metric space dimensions ” in Nearest-Neighbor Methods
for Learning and Vision: Theory and Practice  G. Shakhnarovich  T. Darrell  and P. Indyk  Eds. MIT
Press  2006  pp. 15–59.

[6] Y. Rubner  C. Tomasi  and L. J. Guibas  “The earth mover’s distance as a metric for image retrieval ”

International Journal of Computer Vision  vol. 40  no. 2  pp. 99–121  2000.

[7] E. Demidenko  “Kolmogorov-smirnov test for image comparison ” in Computational Science and Its

Applications - ICCSA  2004  pp. 933–939.

[8] M. Nachtegael  S. Schulte  V. De Witte  T. Mlange  and E. Kerre  “Image similarity  from fuzzy sets to

color image applications ” in Advances in Visual Information Systems  2007  pp. 26–37.

[9] S. Santini and R. Jain  “Similarity measures ” IEEE transactions on Pattern Analysis and Machine Intel-

ligence  vol. 21  no. 9  pp. 871–883  1999.

[10] G. Chechik  V. Sharma  U. Shalit  and S. Bengio  “Large scale online learning of image similarity through

ranking ” Journal of Machine Learning Research  vol. 11  pp. 1109–1135  2010.

[11] A. Frome  Y. Singer  F. Sha  and J. Malik  “Learning globally-consistent local distance functions for

shape-based image retrieval and classiﬁcation ” in ICCV  2007  pp. 1–8.

[12] Y. Lifshits and S. Zhang  “Combinatorial algorithms for nearest neighbors  near-duplicates and small-

world design ” in SODA  2009  pp. 318–326.

[13] R. Krauthgamer and J. R. Lee  “Navigating nets: simple algorithms for proximity search ” in SODA  2004 

pp. 798–807.

[14] D. R. Karger and M. Ruhl  “Finding nearest neighbors in growth-restricted metrics ” in STOC  2002  pp.

741–750.

[15] T. Cormen  C. Leiserson  R. Rivest  and C. Stein  “Introduction to algorithms ” MIT Press and McGraw-

Hill Book Company  vol. 7  pp. 1162–1171  1976.

[16] R. Motwani and P. Raghavan  Randomized Algorithms. Cambridge University Press  1995.

9

,Hu Ding
Ronald Berezney
Jinhui Xu
Pritish Mohapatra
C.V. Jawahar
M. Pawan Kumar
Yunchen Pu
Zhe Gan
Ricardo Henao
Xin Yuan
Chunyuan Li
Andrew Stevens
Lawrence Carin
Ruoqi Shen
Yin Tat Lee