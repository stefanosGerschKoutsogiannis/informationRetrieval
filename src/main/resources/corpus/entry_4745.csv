2019,Robust Bi-Tempered Logistic Loss Based on Bregman Divergences,We introduce a temperature into the exponential function and replace the softmax output layer of the neural networks by a high-temperature generalization. Similarly  the logarithm in the loss we use for training is replaced by a low-temperature logarithm. By tuning the two temperatures  we create loss functions that are non-convex already in the single layer case. When replacing the last layer of the neural networks by our bi-temperature generalization of the logistic loss  the training becomes more robust to noise. We visualize the effect of tuning the two temperatures in a simple setting and show the efficacy of our method on large datasets. Our methodology is based on Bregman divergences and is superior to a related two-temperature method that uses the Tsallis divergence.,Robust Bi-Tempered Logistic Loss

Based on Bregman Divergences

Ehsan Amid ‹: Manfred K. Warmuth ‹: Rohan Anil : Tomer Koren :§

‹ Department of Computer Science  University of California  Santa Cruz

§ School of Computer Science  Tel Aviv University  Tel Aviv  Israel

{eamid manfred rohananil tkoren}@google.com

: Google Brain

Abstract

We introduce a temperature into the exponential function and replace the softmax
output layer of the neural networks by a high-temperature generalization. Similarly 
the logarithm in the loss we use for training is replaced by a low-temperature
logarithm. By tuning the two temperatures  we create loss functions that are
non-convex already in the single layer case. When replacing the last layer of
the neural networks by our bi-temperature generalization of the logistic loss  the
training becomes more robust to noise. We visualize the effect of tuning the two
temperatures in a simple setting and show the efﬁcacy of our method on large
datasets. Our methodology is based on Bregman divergences and is superior to a
related two-temperature method that uses the Tsallis divergence.

1

Introduction

The logistic loss  also known as the softmax loss  has been the standard choice in training deep
neural networks for classiﬁcation. The loss involves the application of the softmax function on the
activations of the last layer to form the class probabilities followed by the relative entropy (aka
the Kullback-Leibler (KL) divergence) between the true labels and the predicted probabilities. The
logistic loss is known to be a convex function of the activations (and consequently  the weights) of
the last layer.

Although desirable from an optimization standpoint  convex losses have been shown to be prone
to outliers [15] as the loss of each individual example unboundedly increases as a function of
the activations. These outliers may correspond to extreme examples that lead to large gradients  or
misclassiﬁed training examples that are located far away from the classiﬁcation boundary. Requiring a
convex loss function at the output layer thus seems somewhat arbitrary  in particular since convexity in
the last layer’s activations does not guarantee convexity with respect to the parameters of the network
outside the last layer. Another issue arises due to the exponentially decaying tail of the softmax
function that assigns probabilities to the classes. In the presence of mislabeled training examples near
the classiﬁcation boundary  the short tail of the softmax probabilities enforces the classiﬁer to stretch
the decision boundary towards the noisy training examples. In contrast  heavy-tailed alternatives for
the softmax probabilities have been shown to signiﬁcantly improve the robustness of the loss to these
examples [8].

The logistic loss is essentially the negative logarithm of the predicted class probabilities  which are
computed as the normalized exponentials of the inputs. In this paper  we tackle both shortcomings of
the logistic loss  pertaining to its convexity as well as its tail-lightness  by replacing the logarithm
and the exponential functions with their corresponding “tempered” versions. We deﬁne the function

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a)

(b)

(c)

(d)

Figure 1: Tempered logarithm and exponential functions  and the bi-tempered logistic loss: (a) logt
function  (b) expt function  bi-tempered logistic loss when (c) t2 = 1.2 ﬁxed and t1 ď 1  and (d)
t1 = 0.8 ﬁxed and t2 ě 1.

logt : R+ Ñ R with temperature parameter t ě 0 as in [16]:

logt(x) :=

1

1 – t

(x1–t – 1) .

(1)

The logt function is monotonically increasing and concave. The standard (natural) logarithm is
recovered at the limit t Ñ 1. Unlike the standard log  the logt function is bounded from below
by –1/(1 – t) for 0 ď t < 1. This property will be used to deﬁne bounded loss functions that are
signiﬁcantly more robust to outliers. Similarly  our heavy-tailed alternative for the softmax function
is based on the tempered exponential function. The function expt : R Ñ R+ with temperature t ě 0
is deﬁned as the inverse1 of logt  that is 

expt(x) := [1 + (1 – t) x]1/(1–t)

+

 

(2)

where [ ¨ ]+ = max{ ¨   0}. The standard exp function is again recovered at the limit t Ñ 1. Compared
to the exp function  a heavier tail (for negative values of x) is achieved for t > 1. We use this property
to deﬁne heavy-tailed analogues of softmax probabilities at the output layer.

The vanilla logistic loss can be viewed as a logarithmic (relative entropy) divergence that operates on
a “matching” exponential (softmax) probability assignment [11  12]. Its convexity then stems from
classical convex duality  using the fact that the probability assignment function is the gradient of the
dual function to the negative entropy on the simplex. When the logt1
are substituted instead 
this duality still holds whenever t1 = t2  albeit with a different Bregman divergence  and the induced
loss remains convex2. However  for t1 < t2  the loss becomes non-convex in the output activations. In
particular  0 ď t1 < 1 leads to a bounded loss  while t2 > 1 provides tail-heaviness. Figure 1 illustrates
the tempered logt and expt functions as well as examples of our proposed bi-tempered logistic loss
function for a two-class problem expressed as a function of the activation of the ﬁrst class. The true
label is assumed to be class one.

and expt2

Tempered generalizations of the logistic regression have been introduced before [7  8  22  2]. The
most recent two-temperature method [2] is based on the Tsallis divergence and contains all the
previous methods as special cases. However  the Tsallis based divergences do not result in proper
loss functions. In contrast  we show that the Bregman based construction introduced in this paper is
indeed proper  which is a requirement for many real-world applications.

1.1 Our replacement of the softmax output layer in neural networks

Consider an arbitrary classiﬁcation model with multiclass softmax output. We are given training
examples of the form (x  y)  where x is a ﬁxed dimensional input vector and the target y is a probability
vector over k classes. In practice  the targets are often one-hot encoded binary vectors in k dimensions.
Each input x is fed to the model  resulting in a vector z of inputs to the ﬁnal softmax layer. This layer
typically has one trainable weight vector wi per class i and yields the predicted class probability

ˆyi =

exp(ˆai)
j=1 exp(ˆaj)

řk

= exp(cid:16)ˆai – log

k

ÿ

j=1

exp(ˆaj)(cid:17)  for linear activation ˆai = wi ¨ z for class i.

1When 0 ď t < 1  the domain of expt needs to be restricted to –1/(1 – t) ď x for the inverse property to hold.
2In a restricted domain when t1 = t2 < 1  as discussed later.

2

c
i
t
s
i
g
o
L

d
e
r
e
p
m
e
T
-
i

B

bounded & heavy-tail

(0.2  4.0)

only heavy-tail

(1.0  4.0)

only bounded

(0.2  1.0)

bounded & heavy-tail

(0.2  4.0)

(a)

(b)

(c)

(d)

Figure 2: Logistic vs. robust bi-tempered logistic loss: (a) noise-free labels  (b) small-margin label
noise  (c) large-margin label noise  and (d) random label noise. The temperature values (t1  t2) for the
bi-tempered loss are shown above each ﬁgure.

We ﬁrst replace the softmax function by a generalized heavy-tailed version that uses the expt2
with t2 > 1  which we call the tempered softmax function:

function

ˆyi = expt2 (cid:0)ˆai – λt2 (ˆa)(cid:1)  where λt2 (ˆa) P R is s.t.

k

ÿ

j=1

expt2 (cid:0)ˆaj – λt2 (ˆa)(cid:1) = 1 .

This requires computing the normalization value λt2 (ˆa) (for each example) via a binary search or an
iterative procedure like the one given in Appendix A. The relative entropy between the true label y
and prediction ˆy is replaced by the tempered version with temperature range 0 ď t1 < 1 

k

ÿ

i=1

(cid:0)yi (logt1

yi – logt1

ˆyi) – 1
2–t1

(y2–t1

i

– ˆy2–t1

i

)(cid:1) if y one-hot

=

– logt1

ˆyc – 1

2–t1(cid:16)1 –

k

ÿ

i=1

i (cid:17) .
ˆy2–t1

where c = argmaxi yi is the index of the one-hot class. In later sections we prove various properties of
this loss. When t1 = t2 = 1  then it reduces to the vanilla relative entropy loss with softmax. Also
when 0 ď t1 < 1  then the loss is bounded  while t2 > 1 gives the tempered softmax function a heavier
tail.

1.2 An illustration

We provide some intuition on why both boundedness of the loss as well as tail-heaviness of the
tempered softmax are crucial for robustness. For this  we train a small two-layer feed-forward neural
network on a synthetic binary classiﬁcation problem in two dimensions. The network has 10 and 5
units in the ﬁrst and second layer  respectively3. Figure 2(a) shows the results of the logistic and our
bi-tempered logistic loss on the noise-free dataset. The network converges to a desirable classiﬁcation
boundary (the white stripe in the ﬁgure) using both loss functions. In Figure 2(b)  we illustrate the
effect of adding small-margin label noise to the training examples  targeting those examples that
reside near the noise-free classiﬁcation boundary. The logistic loss clearly follows the noisy examples
by stretching the classiﬁcation boundary. On the other hand  using only the tail-heavy tempered
softmax function (t2 = 4 while t1 = 1  i.e. KL divergence as the divergence) can handle the noisy
examples by producing more uniform class probabilities. Next  we show the effect of large-margin
noisy examples in Figure 2(c)  targeting examples that are located far away from the noise-free
classiﬁcation boundary. The convexity of the logistic loss causes the network to be highly affected by
the noisy examples that are located far away from the boundary. In contrast  only the boundedness of
the loss (t1 = 0.2 while t2 = 1  meaning that the outputs are vanilla softmax probabilities) reduces the

3An interactive visualization of the bi-tempered loss is available at: https://google.github.io/

bi-tempered-loss/

3

effect of the outliers by allocating at most a ﬁnite amount of loss to each example. Finally  we show
the effect of random label noise that includes both small-margin and large-margin noisy examples in
Figure 2(d). Clearly  the logistic loss fails to handle the noise  while our bi-tempered logistic loss
successfully recovers the appropriate boundary. Note for random noise  we exploit both boundedness
of the loss (t1 = 0.2 < 1) as well as the tail-heaviness of the probability assignments (t2 = 4 > 1).

The theoretical background as well as our treatment of the softmax layer of the neural networks are
developed in later sections. In particular  we show that special discrete choices of the temperatures
result in a large variety of divergences commonly used in machine learning. As we show in our
experiments  tuning the two temperatures as continuous parameters is crucial.

1.3 Summary of the experiments

We perform experiments by adding synthetic label noise to MNIST and CIFAR-100 datasets and
compare the results of our robust bi-tempered loss to the vanilla logistic loss. Our bi-tempered loss is
signiﬁcantly more robust to label noise (when trained on noisy data and test accuracy is measured w.r.t.
the clean data): It provides 98.56% and 62.55% accuracy on MNIST and CIFAR-100  respectively 
when trained with 40% label noise (compared to 97.64% and 53.17%  respectively  obtained using
logistic loss). The bi-tempered loss also yields improvement over the state-of-the-art results on the
ImageNet-2012 dataset using both the Resnet18 and Resnet50 architectures (see Table 2).

2 Preliminaries

2.1 Convex duality and Bregman divergences on the simplex

We start by brieﬂy reviewing some basic background in convex analysis. For a continuously-
differentiable strictly convex function F : D Ñ R  with convex domain D Ď Rk  the Bregman
divergence [3] between y  ˆy P D induced by F is deﬁned as

∆F(y  ˆy) = F(y) – F(ˆy) – (y – ˆy) ¨ f (ˆy)  

2 }y}2

2) and the Kullback–Leibler divergence ∆F(y  ˆy) = ři(yi log yi

where f (ˆy) := ∇F(ˆy) denotes the gradient of F at ˆy (sometimes called the link function of F). Clearly
∆F(y  ˆy) ě 0 and ∆F(y  ˆy) = 0 iff y = ˆy. Also the Bregman divergence is always convex in the ﬁrst
∆F(y  ˆy) = f (y) – f (ˆy)  but not generally in its second argument. Bregman divergence
argument and ∇y
generalizes many well-known divergences such as the squared Euclidean ∆F(y  ˆy) = 1
2 }y – ˆy}2
2
(with F(y) = 1
– yi + ˆyi) (with
F(y) =ři(yi log yi – yi)). The Bregman divergence is typically not symmetric  i.e. ∆F(y  ˆy) ‰ ∆F(ˆy  y).
Additionally  the Bregman divergence is invariant to adding afﬁne functions to the convex function F:
∆F+A(y  ˆy) = ∆F(y  ˆy)  where A(y) = b + c ¨ y for arbitrary b P R  c P Rk.
For every differentiable strictly convex function F (with domain D Ď Rk
+)  there exists a convex
dual F˚ : D˚ Ñ R function such that for dual parameter pairs (y  a)  a P D˚  the following holds:
a = f (y) and y = f ˚(a) = ∇F˚(a) = f –1(a). However  we are mainly interested in the dual of the
+| řk
function F when the domain is restricted to the probability simplex Sk := {y P Rk
i=1 yi = 1}. Let
ˇF˚ : ˇD˚ Ñ R denote the convex conjugate of the restricted function F : D X Sk Ñ R 

ˆyi

ˇF˚(a) = sup

1PDXSk (cid:0)y

y

1 ¨ a – F(y

1)(cid:1) = sup

1PD
y

λPR(cid:0)y
inf

1 ¨ a – F(y

1) + λ (1 –

k

ÿ

i=1

y1
i)(cid:1)  

where we introduced a Lagrange multiplier λ P R to enforce the linear constraint řk
optimum  the following relationships hold between the primal and dual variables:

i=1 y1

i = 1. At the

where λ(a) is chosen so that it satisﬁes the constraint. Note the dependence of the optimum λ on a.

f (y) = a – λ(a) 1 and y = f –1(cid:0)a – λ(a) 1(cid:1) = ˇf ˚(a)  

(3)

2.2 Matching losses

Next  we recall the notion of a matching loss [11  12  4  17]. It arises as a natural way of deﬁning a
loss function over activations ˆa P Rk  by ﬁrst mapping them to a probability distribution over class
labels using a transfer function s : Rk Ñ Sk  and then computing a divergence ∆F between this

4

distribution and the correct target labels. The idea behind the following deﬁnition is to “match” the
transfer function and the divergence via duality.4
Deﬁnition 1 (Matching Loss). Let F : Sk Ñ R be a continuously-differentiable  strictly convex
function and let s : Rk Ñ Sk be a transfer function such that ˆy = s(ˆa) denotes the predicted probability
distribution based on the activations ˆa. Then the loss function

is called the matching loss for s  if s = ˇf ˚ = ∇ˇF˚.

LF(ˆa | y) := ∆F(y  s(ˆa))  

Note that ˇf ˚ is no longer one-to-one since ˇf ˚(ˆa + R 1) = ˇf ˚(ˆa) (see Appendix D for more details).
However  w.l.o.g. we can constrain the domain of the function to ˆa P dom(ˇf ˚) X {a
1 ¨ 1 = 0}
1 P Rk | a
to obtain a one-to-one mapping. The matching loss is useful due to the following property.
Proposition 1. The matching loss LF(ˆa | y) is convex w.r.t. the activations ˆa P dom(ˇf ˚) X {a
1 ¨ 1 = 0}.
a

1 P Rk |

Proof. Note that ˇF˚ is a strictly convex function and the following relation holds between the
divergences induced by F and ˇF˚ (see proof of Proposition 4 in Appendix D):

∆F(cid:0)y  ˆy(cid:1) = ∆ˇF˚(cid:0)(ˇf ˚)–1(ˆy)  (ˇf ˚)–1(y)(cid:1).

(4)

Thus for any ˆa in the range of (ˇf ˚)–1 

The claim now follows from the convexity of ∆ˇF˚ w.r.t. its ﬁrst argument.

∆F(cid:0)y  ˇf ˚(ˆa)(cid:1) = ∆ˇF˚(cid:0)ˆa  (ˇf ˚)–1(y)(cid:1).

The original motivating example for the matching loss was the logistic loss [11  12]. It can be
obtained as the matching loss for the softmax function

ˆyi = [ˇf ˚(ˆa)]i =

exp(ˆai)
j=1 exp(ˆaj)

řk

 

which corresponds to the relative entropy (KL) divergence

LF(ˆa | y) = ∆F(cid:0)y  ˇf ˚(ˆa)(cid:1) =

k

ÿ

i=1

yi (log yi – log ˆyi) =

k

ÿ

i=1

(cid:0)yi log yi – yi ˆai)(cid:1) + log(cid:0)

k

ÿ

i=1

exp(ˆai)(cid:1)  

induced from the negative entropy function F(y) = řk
i=1(yi log yi – yi). We next deﬁne a family
of convex functions Ft parameterized by a temperature t ě 0. The matching loss LFt (ˆa | y) =
∆Ft(cid:0)y  ˇf ˚
is convex in the activations ˆa. However  by letting the
temperature t2 of ˇf ˚
be larger than the temperature t1 of Ft1   we construct bounded non-convex losses
t2
with heavy-tailed transfer functions.

t (ˆa)(cid:1) for the link function ˇf ˚

t of ˇF˚

t

3 Tempered Matching Loss

We start by introducing a generalization of the relative entropy divergence  denoted by ∆Ft   induced
by a strictly convex function Ft : Rk
+ Ñ R with a temperature parameter t ě 0. The convex function
Ft is chosen so that its gradient takes the form5 ft(y) := ∇Ft(y) = logt y. Via simple integration  we
obtain that

k

ÿ

i=1

Ft(y) =

(cid:0)yi logt yi + 1
Indeed  Ft is a convex function since ∇2Ft(y) = diag(y
convex  for 0 ď t ď 1:
Lemma 1. The function Ft  with 0 ď t ď 1  is B–t–strongly convex over the set {y P Rk
w.r.t. the L2–t-norm.

–t) ľ 0 for any y P Rk

2–t (1 – y2–t

i

)(cid:1) .

+. In fact  Ft is strongly

+ : }y}2–t ď B}

4Originally in [11  12]  the matching loss was deﬁned as a simple integral over the transfer function s = f –1:

LF(ˆa | y) = şˆa

s–1(y)(s(z) – y)¨d z. Our new duality based deﬁnition handles additional linear constraints.

5Here  the logt function is applied elementwise.

5

See Appendix B for a proof. The Bregman divergence induced by Ft is then given by

∆Ft (y  ˆy) =

=

k

ÿ

i=1

k

ÿ

i=1

(cid:0)yi logt yi – yi logt

ˆyi – 1

2–t y2–t

i + 1
2–t

ˆy2–t
i (cid:1)

(5)

(cid:16)

1

(1–t)(2–t) y2–t

i – 1

1–t yiˆy1–t

i + 1
2–t

i (cid:17).
ˆy2–t

The second form may be recognized as β-divergence [5] with parameter β = 2 – t. The divergence (5)
includes many well-known divergences such as squared Euclidean  KL  and Itakura-Saito divergence
as special cases. A list of additional special cases is given in Table 3 of Appendix C.

The following corollary is the direct consequence of the strong convexity of Ft.
Corollary 1. Let max(}y}2–t  }ˆy}2–t) ď B for 0 ď t < 1. Then

1
2Bt }y – ˆy}2

2–t ď ∆Ft (y  ˆy) ď

Bt

2 (1 – t)2 }y

1–t – ˆy

1–t}2

.

2–t
1–t

See Appendix B for a proof. Thus for 0 ď t < 1  ∆Ft (y  ˆy) is upper-bounded by 2 B2–t
(1–t)2 . Note
that boundedness on the simplex also implies boundedness in the L2–t-ball. Thus  Corollary 1
immediately implies the boundedness of the divergence ∆Ft (y  ˆy) with 0 ď t < 1 over the simplex.
Alternate parameterizations of the family {Ft} of convex functions and their corresponding Bregman
divergences are discussed in Appendix C.

3.1 Tempered softmax function

Now  let us consider the convex function Ft(y) when its domain is restricted to the probability simplex
Sk. We denote the constrained dual of Ft(y) by ˇF˚

t (a) 

ˇF˚

t (a) = sup

1PSk (cid:0)y

y

1 ¨ a – Ft(y

1)(cid:1) = sup

1PRk
y
+

inf

λtPR (cid:0)y

1 ¨ a – Ft(y

1) + λt (cid:0)1 –

k

ÿ

i=1

y1
i(cid:1)(cid:1) .

Following our discussion in Section 2.1 and using (3)  the transfer function induced by ˇF˚
t

is6

y = expt (cid:0)a – λt(a) 1(cid:1)  with λt(a) s.t.

k

ÿ

i=1

expt (cid:0)ai – λt(a)(cid:1) = 1.

3.2 Matching loss of tempered softmax

Finally  we derive the matching loss function LFt . Plugging in (7) into (5)  we have

(6)

(7)

Lt(ˆa | y) = ∆Ft(cid:0)y  expt(ˆa – λt(ˆa) 1)(cid:1).

Recall that by Proposition 1  this loss is convex in activations ˆa P dom(ˇf ˚) X {a
1 ¨ 1 = 0}.
In general  λt(a) does not have a closed form solution. However  it can be easily approximated via an
iterative method  e.g.  a binary search. An alternative (ﬁxed-point) algorithm for computing λt(a) for
t > 1 is given in Algorithm 1 of Appendix A.

1 P Rk | a

4 Robust Bi-Tempered Logistic Loss

A more interesting class of loss functions can be obtained by introducing a “mismatch” between
the temperature of the divergence function (5) and the temperature of the probability assignment
function  i.e. the tempered softmax (7). That is  we consider loss functions of the following type:

k

@ 0 ď t1 < 1 < t2 : Lt2
t1

(ˆa | y) := ∆Ft1(cid:0)y  expt2

(ˆa – λt2 (ˆa)1)(cid:1) with λt(ˆa) s.t.

ÿ

i=1

expt (cid:0)ai – λt(a)(cid:1) = 1.

(8)

We call this the Bi-Tempered Logistic Loss. As illustrated in our two-dimensional example in
Section 1  both properties are crucial for handling noisy examples. The derivative of the bi-tempered
loss is given in Appendix E. In the following  we discuss the properties of this loss for classiﬁcation.
t (a) = expt (cid:0)a – λt(a) 1(cid:1) is

6Note that due to the simplex constraint  the link function y = ˇf ˚

t (a) = ∇ ˇF˚

different from f –1

t

(a) = f ˚

t (a) = ∇F˚

t (a) = expt(a)  i.e.  the gradient of the unconstrained dual.

6

4.1 Properness and Monte-Carlo sampling

Let PUK(x  y) denote the (unknown) joint probability distribution of the observed variable x P Rm and
the class label y P [k]. The goal of discriminative learning is to approximate the posterior distribution
of the labels PUK(y | x) via a parametric model P(y | x; Θ) parameterized by Θ. Thus the model ﬁtting
can be expressed as minimizing the following expected loss between the data and the model’s label
probabilities

EPUK(x)h∆(cid:0)PUK(y | x)  P(y | x; Θ)(cid:1)i  

(9)

where ∆(cid:0)PUK(y | x)  P(y | x; Θ)(cid:1) is any divergence measure between PUK(y | x) and P(y | x; Θ). We
(ˆai – λt2 (ˆa))  where ˆa is the
use ∆ := ∆Ft1
activation vector of the last layer given input x and Θ is the set of all weights of the network. Ignoring
the constant terms w.r.t. Θ  our loss (9) becomes

as the divergence and P(i | x; Θ) := P(y = i | x; Θ) = expt2

EPUK(x)h ÿ

i

(cid:0) – PUK(i | x) logt P(i | x; Θ) +

1

2 – t

P(i | x; Θ)2–t(cid:1)i

= –EPUK(x y)h logt P(y | x; Θ)i + EPUK(x)h 1

2 – t ÿ

i

P(i | x; Θ)2–t(cid:1)i

«

1

N ÿ

n (cid:0) – logt P(yn | xn; Θ) +

1

2 – t ÿ

i

P(i | xn; Θ)2–t(cid:1)  

(10a)

(10b)

(10c)

where from (10b) to (10c)  we perform a Monte-Carlo approximation of the expectation w.r.t.
PUK(x  y) using samples {(xn  yn)}N
n=1. Thus  (10c) is an unbiased approximate of the expected loss (9) 
thus is a proper loss [20].

Following the same approximation steps for the Tsallis divergence used in [2]  we have

EPUK(x)h – ÿ

PUK(i | x) logt

P(i | x; Θ)
PUK(i | x)
looooooooooooooooomooooooooooooooooon

i

i « –

1

N ÿ

n

logt

P(yn | xn; Θ)
PUK(yn | xn)

 

∆Tsallis

which  due to the fact that logt
conditional distribution PUK(y | x). In this case the approximation – 1
in [2] by setting PUK(yn | xn) to 1 is not an unbiased estimator of (9) and therefore  not proper.

(cid:0)PUK(y|x)  P(y|x;Θ)(cid:1)
a
b ‰ logt a – logt b in general  requires access to the (unknown)
N řn logt P(yn | xn; Θ) proposed

t

4.2 Bayes-risk consistency

Another important property of a multiclass loss is the Bayes-risk consistency [19]. Bayes-risk
consistency of the two-temperature logistic loss based on the Tsallis divergence was shown in [2]. As
expected  the tempered Bregman loss (8) is also Bayes-risk consistent even in the non-convex case.
Proposition 2. The multiclass bi-tempered logistic loss Lt2

t1 (ˆa | y) is Bayes-risk consistent.

5 Experiments

We demonstrate the practical utility of the bi-tempered logistic loss function on a wide variety of
image classiﬁcation tasks. For moderate-size experiments  we use MNIST dataset of handwritten
digits [14] and CIFAR-100  which contains real-world images from 100 different classes [13]. We
use ImageNet-2012 [6] for large scale image classiﬁcation  having 1000 classes. All experiments are
carried out using the TensorFlow [1] framework. We use P100 GPU’s for small-scale experiments and
Cloud TPU-v2 for larger scale ImageNet-2012 experiments. An implementation of the bi-tempered
logistic loss is available online at: https://github.com/google/bi-tempered-loss.

5.1 Corrupted labels experiments

For our moderate size datasets  i.e. MNIST and CIFAR-100  we introduce noise by artiﬁcially
corrupting a fraction of the labels and producing a new set of labels for each noise level. For all
experiments  we compare our bi-tempered loss function against the logistic loss.

7

Dataset

Loss

Label Noise Level

0.0

0.1

0.2

0.3

0.4

0.5

MNIST

Logistic

99.40

98.96

98.70

98.50

97.64

96.13

Bi-Tempered (0.5  4.0)

99.24

99.13

99.02

98.62

98.56

97.69

CIFAR-100

Logistic

74.03

69.94

66.39

63.00

53.17

52.96

Bi-Tempered (0.8  1.2)

75.30

73.30

70.69

67.45

62.55

57.80

Table 1: Top-1 accuracy on a clean test set for MNIST and CIFAR-100 datasets where a fraction of
the training labels are corrupted.

Model

Logistic

Resnet50

76.332 ˘ 0.105

76.748 ˘ 0.164

71.618 ˘ 0.163

Resnet18

71.333 ˘ 0.069

Bi-tempered (0.9 1.05)

Table 2: Top-1 accuracy on ImageNet-2012 with
Resnet-18 and 50 architectures.

For MNIST  we use a CNN with two convolu-
tional layers of size 32 and 64 with a mask size
of 5  followed by two fully-connected layers
of size 1024 and 10. We apply max-pooling
after each convolutional layer with a window
size equal to 2 and use dropout during training
with keep probability equal to 0.75. We use the
AdaDelta optimizer [21] with 500 epochs and
batch size of 128 for training. For CIFAR-100  we use a Resnet-56 [10] model without batch norm
from [9] with SGD + momentum optimizer trained for 50k steps with batch size of 128 and use the
standard learning rate stair case decay schedule. For both experiments  we report the test accuracy of
the checkpoint which yields the highest accuracy on an identically label-noise corrupted validation
set. We search over a set of learning rates for each experiment. For both experiments  we exhaustively
search over a number of temperatures within the range [0.5  1) and (1.0  4.0] for t1 and t2  respectively.
The results are presented in Table 1 where we report the top-1 accuracy on a clean test set. As can be
seen  the bi-tempered loss outperforms the logistic loss for all noise levels (including the noise-free
case for CIFAR-100). Using our bi-tempered loss function the model is able to continue to perform
well even for high levels of label noise whereas the accuracy of the logistic loss drops immediately
with a much smaller level of noise.

5.2 Large scale experiments

We train state-of-the-art Resnet-18 and Resnet-50 models on the ImageNet-2012 dataset. Note that
the ImageNet-2012 dataset is inherently noisy due to some amount of mislabeling. We train on a
4x4 CloudTPU-v2 device with a batch size of 4096. All experiments were trained for 180 epochs 
and use the SGD + momentum optimizer with staircase learning rate decay schedule. The results are
presented in Table 2. For both architectures we see a signiﬁcant gain in the top-1 accuracy using the
robust bi-tempered loss.

6 Conclusion and Future Work

Neural networks on large standard datasets have been optimized along with a large variety of variables
such as architecture  transfer function  choice of optimizer  and label smoothing to name just a few.
We proposed a new variant by training the network with tunable loss functions. We do this by
ﬁrst developing convex loss functions based on temperature dependent logarithm and exponential
functions. When both temperatures are the same  then a construction based on the notion of “matching
loss” leads to loss functions that are convex in the last layer. However by letting the temperature of
the new tempered softmax function be larger than the temperature of the tempered log function used
in the divergence  we construct tunable losses that are non-convex in the last layer. Our construction
remedies two issues simultaneously: we construct bounded tempered loss functions that can handle
large-margin outliers and introduce heavy-tailedness in our new tempered softmax function that
seems to handle small-margin mislabeled examples. At this point  we simply took a number of
benchmark datasets and networks for these datasets that have been heavily optimized for the logistic
loss paired with vanilla softmax and simply replaced the loss in the last layer by our new construction.
By simply trying a number of temperature pairs  we already achieved signiﬁcant improvements. We
believe that with a systematic “joint optimization” of all commonly tried variables  signiﬁcant further
improvements can be achieved. This is of course a more long-term goal. We also plan to explore the
idea of annealing the temperature parameters over the training process.

8

Acknowledgement

We would like to thank Jerome Rony for pointing out that early stopping improves the accuracy of
the logistic loss on the noisy MNIST experiment. This research was partially supported by the NSF
grant IIS-1546459.

References

[1] Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro 
Greg S. Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  Sanjay Ghemawat  Ian Goodfellow 
Andrew Harp  Geoffrey Irving  Michael Isard  Yangqing Jia  Rafal Jozefowicz  Lukasz Kaiser 
Manjunath Kudlur  Josh Levenberg  Dan Mané  Rajat Monga  Sherry Moore  Derek Murray 
Chris Olah  Mike Schuster  Jonathon Shlens  Benoit Steiner  Ilya Sutskever  Kunal Talwar  Paul
Tucker  Vincent Vanhoucke  Vijay Vasudevan  Fernanda Viégas  Oriol Vinyals  Pete Warden 
Martin Wattenberg  Martin Wicke  Yuan Yu  and Xiaoqiang Zheng. TensorFlow: Large-scale
machine learning on heterogeneous systems  2015. Software available from tensorﬂow.org.

[2] Ehsan Amid  Manfred K. Warmuth  and Sriram Srinivasan. Two-temperature logistic regression
based on the Tsallis divergence. In 22nd International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS 19)  2019.

[3] Lev M Bregman. The relaxation method of ﬁnding the common point of convex sets and
its application to the solution of problems in convex programming. USSR computational
mathematics and mathematical physics  7(3):200–217  1967.

[4] Andreas Buja  Werner Stuetzle  and Yi Shen. Loss functions for binary class probability
estimation and classiﬁcation: Structure and applications. Technical report  University of
Pennsylvania  November 2005.

[5] Andrzej Cichocki and Shun-ichi Amari. Families of alpha-beta-and gamma-divergences:

Flexible and robust measures of similarities. Entropy  12(6):1532–1568  2010.

[6] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei.

ImageNet: A Large-Scale

Hierarchical Image Database. In CVPR09  2009.

[7] Nan Ding. Statistical machine learning in the t-exponential family of distributions. PhD thesis 

Purdue University  2013.

[8] Nan Ding and S. V. N. Vishwanathan.

In Proceedings of the 23th
International Conference on Neural Information Processing Systems  NIPS’10  pages 514–522 
Cambridge  MA  USA  2010.

t-logistic regression.

[9] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. International Conference on

Learning Representations (ICLR)  2017.

[10] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[11] D. P. Helmbold  J. Kivinen  and M. K. Warmuth. Relative loss bounds for single neurons. IEEE

Transactions on Neural Networks  10(6):1291–1304  November 1999.

[12] J. Kivinen and M. K. Warmuth. Relative loss bounds for multidimensional regression problems.

Machine Learning  45(3):301–329  2001.

[13] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report 

Citeseer  2009.

[14] Yann LeCun and Corinna Cortes. The MNIST database of handwritten digits  1999.

[15] Philip M Long and Rocco A Servedio. Random classiﬁcation noise defeats all convex potential
boosters. In Proceedings of the 25th international conference on Machine learning  pages
608–615. ACM  2008.

[16] Jan Naudts. Deformed exponentials and logarithms in generalized thermostatistics. Physica A 

316:323–334  2002.

[17] M. D. Reid and R. C. Williamson. Surrogate regret bounds for proper losses. In Proceedings of

the 26th International Conference on Machine Learning (ICML’09)  pages 897–904  2009.

9

[18] Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and

Trends® in Machine Learning  4(2):107–194  2012.

[19] Ambuj Tewari and Peter L Bartlett. On the consistency of multiclass classiﬁcation methods.

Journal of Machine Learning Research  8(May):1007–1025  2007.

[20] Robert C. Williamson  Elodie Vernet  and Mark D. Reid. Composite multiclass losses. Journal

of Machine Learning Research  17(223):1–52  2016.

[21] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 

2012.

[22] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In Advances in Neural Information Processing Systems  pages 8778–8788 
2018.

10

,Ehsan Amid
Manfred K. Warmuth
Rohan Anil
Tomer Koren