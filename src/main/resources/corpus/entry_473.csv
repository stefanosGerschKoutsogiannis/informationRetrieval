2018,Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced,We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear  ReLU or Leaky ReLU activation. We rigorously prove that gradient flow (i.e. gradient descent with infinitesimal step size) effectively enforces the differences between squared norms across different layers to remain invariant without any explicit regularization. This result implies that if the weights are initially small  gradient flow automatically balances the magnitudes of all layers. Using a discretization argument  we analyze gradient descent with positive step size for the non-convex low-rank asymmetric matrix factorization problem without any regularization. Inspired by our findings for gradient flow  we prove that gradient descent with step sizes $\eta_t=O(t^{−(1/2+\delta)}) (0<\delta\le1/2)$ automatically balances two low-rank factors and converges to a bounded global optimum. Furthermore  for rank-1 asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate. We believe that the idea of examining the invariance imposed by first order algorithms in learning homogeneous models could serve as a fundamental building block for studying optimization for learning deep models.,Algorithmic Regularization in Learning Deep

Homogeneous Models: Layers are Automatically

Balanced˚

Simon S. Du:

Wei Hu;

Jason D. Lee§

Abstract

We study the implicit regularization imposed by gradient descent for learning
multi-layer homogeneous functions including feed-forward fully connected and
convolutional deep neural networks with linear  ReLU or Leaky ReLU activation.
We rigorously prove that gradient ﬂow (i.e. gradient descent with inﬁnitesimal
step size) effectively enforces the differences between squared norms across dif-
ferent layers to remain invariant without any explicit regularization. This result
implies that if the weights are initially small  gradient ﬂow automatically balances
the magnitudes of all layers. Using a discretization argument  we analyze gradient
descent with positive step size for the non-convex low-rank asymmetric matrix
factorization problem without any regularization. Inspired by our ﬁndings for gra-

dient ﬂow  we prove that gradient descent with step sizes ⌘t “ O´t´p 1
2) automatically balances two low-rank factors and converges to a
(0 †  § 1
bounded global optimum. Furthermore  for rank-1 asymmetric matrix factoriza-
tion we give a ﬁner analysis showing gradient descent with constant step size
converges to the global minimum at a globally linear rate. We believe that the
idea of examining the invariance imposed by ﬁrst order algorithms in learning
homogeneous models could serve as a fundamental building block for studying
optimization for learning deep models.

2`q¯

1

Introduction

Modern machine learning models often consist of multiple layers. For example  consider a feed-
forward deep neural network that deﬁnes a prediction function

x ﬁÑ fpx; W p1q  . . .   W pNqq “ W pNqpW pN´1q ¨¨¨ W p2qpW p1qxq¨¨¨q 

where W p1q  . . .   W pNq are weight matrices in N layers  and p¨q is a point-wise homogeneous
activation function such as Rectiﬁed Linear Unit (ReLU) pxq “ maxtx  0u. A simple observa-
tion is that this model is homogeneous: if we multiply a layer by a positive scalar c and divide
another layer by c  the prediction function remains the same  e.g. fpx; cW p1q  . . .   1
c W pNqq “
fpx; W p1q  . . .   W pNqq.
A direct consequence of homogeneity is that a solution can produce small function value while be-
ing unbounded  because one can always multiply one layer by a huge number and divide another

˚The full version of this paper is available at https://arxiv.org/abs/1806.00900.
:Machine Learning Department  School of Computer Science  Carnegie Mellon University. Email:

ssdu@cs.cmu.edu

;Computer Science Department  Princeton University. Email: huwei@cs.princeton.edu
§Department of Data Sciences and Operations  Marshall School of Business  University of Southern Cali-

fornia. Email: jasonlee@marshall.usc.edu

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

layer by that number. Theoretically  this possible unbalancedness poses signiﬁcant difﬁculty in ana-
lyzing ﬁrst order optimization methods like gradient descent/stochastic gradient descent (GD/SGD) 
because when parameters are not a priori constrained to a compact set via either coerciveness5 of
the loss or an explicit constraint  GD and SGD are not even guaranteed to converge [Lee et al.  2016 
Proposition 4.11]. In the context of deep learning  Shamir [2018] determined that the primary barrier
to providing algorithmic results is in that the sequence of parameter iterates is possibly unbounded.
Now we take a closer look at asymmetric matrix factorization  which is a simple two-layer homoge-
neous model. Consider the following formulation for factorizing a low-rank matrix:

min

UPRd1ˆr V PRd2ˆr

f pU   V q “ 1

2››U V J ´ M˚››2

F  

where M˚ P Rd1ˆd2 is a matrix we want to factorize. We observe that due to the homogeneity of
f  it is not smooth6 even in the neighborhood of a globally optimum point. To see this  we compute
the gradient of f:

Bf pU   V q

BU

“`U V J ´ M˚˘ V  

Bf pU   V q

BV

“`U V J ´ M˚˘J U .

Notice that the gradient of f is not homogeneous anymore. Further  consider a globally optimal
solution pU   V q such that }U}F is of order ✏ and }V }F is of order 1{✏ (✏ being very small). A
small perturbation on U can lead to dramatic change to the gradient of U. This phenomenon can
happen for all homogeneous functions when the layers are unbalanced. The lack of nice geometric
properties of homogeneous functions due to unbalancedness makes ﬁrst-order optimization methods
difﬁcult to analyze.
A common theoretical workaround is to artiﬁcially modify the natural objective function as in (1) in
order to prove convergence. In [Tu et al.  2015  Ge et al.  2017a]  a regularization term for balancing
the two layers is added to (1):

(1)

(2)

(3)

min

UPRd1ˆr V PRd2ˆr

1

2››U V J ´ M››2
8››UJU ´ V JV››2
F ` 1

F .

For problem (3)  the regularizer removes the homogeneity issue and the optimal solution becomes
unique (up to rotation). Ge et al. [2017a] showed that the modiﬁed objective (3) satisﬁes (i) every
local minimum is a global minimum  (ii) all saddle points are strict7  and (iii) the objective is smooth.
These imply that (noisy) GD ﬁnds a global minimum [Ge et al.  2015  Lee et al.  2016  Panageas
and Piliouras  2016].
On the other hand  empirically  removing the homogeneity is not necessary. We use GD with random
initialization to solve the optimization problem (1). Figure 1a shows that even without regularization
term like in the modiﬁed objective (3) GD with random initialization converges to a global minimum
and the convergence rate is also competitive. A more interesting phenomenon is shown in Figure 1b
in which we track the Frobenius norms of U and V in all iterations. The plot shows that the ratio
between norms remains a constant in all iterations. Thus the unbalancedness does not occur at all!
In many practical applications  many models also admit the homogeneous property (like deep neural
networks) and ﬁrst order methods often converge to a balanced solution. A natural question arises:

Why does GD balance multiple layers and converge in learning homogeneous functions?

In this paper  we take an important step towards answering this question. Our key ﬁnding is that
the gradient descent algorithm provides an implicit regularization on the target homogeneous func-
tion. First  we show that on the gradient ﬂow (gradient descent with inﬁnitesimal step size) tra-
jectory induced by any differentiable loss function  for a large class of homogeneous models  in-
cluding fully connected and convolutional neural networks with linear  ReLU and Leaky ReLU
activations  the differences between squared norms across layers remain invariant. Thus  as long
as at the beginning the differences are small  they remain small at all time. Note that small
differences arise in commonly used initialization schemes such as
1?d Gaussian initialization or

5A function f is coercive if }x} Ñ 8 implies fpxq Ñ 8.
6A function is said to be smooth if its gradient is -Lipschitz continuous for some ﬁnite  ° 0.
7A saddle point of a function f is strict if the Hessian at that point has a negative eigenvalue.

2

2

0

-2

-4

-6

-8

-10

-12

-14

-16

-18

j

b
O

 
f

 

o
m
h

t
i
r
a
g
o
L

Without Regularization
With Regularization

0

2000

4000

6000

8000

10000

Epochs

(a) Comparison of convergence rates of GD for
objective functions (1) and (3).

s
r
e
y
a
L

 

o
w
T

 
f

o

 
s
m
r
o
N
-
F
n
e
e
w
e
b

t

 

1

0.9

0.8

0.7

0.6

0.5

 

o

i
t

a
R

0.4

0

Without Regularization
With Regularization

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

Epochs

(b) Comparison of quantity }U}2
running GD for objective functions (1) and (3).

F {}V }2

F when

Figure 1: Experiments on the matrix factorization problem with objective functions (1) and (3). Red
lines correspond to running GD on the objective function (1)  and blue lines correspond to running
GD on the objective function (3).

Xavier/Kaiming initialization schemes [Glorot and Bengio  2010  He et al.  2016]. Our result thus
explains why using ReLU activation is a better choice than sigmoid from the optimization point
view. For linear activation  we prove an even stronger invariance for gradient ﬂow: we show
that W phqpW phqqJ ´ pW ph`1qqJW ph`1q stays invariant over time  where W phq and W ph`1q
are weight matrices in consecutive layers with linear activation in between.
Next  we go beyond gradient ﬂow and consider gradient descent with positive step size. We focus on
the asymmetric matrix factorization problem (1). Our invariance result for linear activation indicates
that UJU ´ V JV stays unchanged for gradient ﬂow. For gradient descent  UJU ´ V JV can
change over iterations. Nevertheless we show that if the step size decreases like ⌘t “ O´t´p 1
2`q¯
2)  UJU ´ V JV will remain small in all iterations. In the set where UJU ´ V JV
(0 †  § 1
is small  the loss is coercive  and gradient descent thus ensures that all the iterates are bounded.
Using these properties  we then show that gradient descent converges to a globally optimal solution.
Furthermore  for rank-1 asymmetric matrix factorization  we give a ﬁner analysis and show that
randomly initialized gradient descent with constant step size converges to the global minimum at a
globally linear rate.

Related work. The homogeneity issue has been previously discussed by Neyshabur et al.
[2015a b]. The authors proposed a variant of stochastic gradient descent that regularizes paths in
a neural network  which is related to the max-norm. The algorithm outperforms gradient descent
and AdaGrad on several classiﬁcation tasks.
A line of research focused on analyzing gradient descent dynamics for (convolutional) neural net-
works with one or two unknown layers [Tian  2017  Brutzkus and Globerson  2017  Du et al. 
2017a b  Zhong et al.  2017  Li and Yuan  2017  Ma et al.  2017  Brutzkus et al.  2017]. For one un-
known layer  there is no homogeneity issue. While for two unknown layers  existing work either re-
quires learning two layers separately [Zhong et al.  2017  Ge et al.  2017b] or uses re-parametrization
like weight normalization to remove the homogeneity issue [Du et al.  2017b]. To our knowledge 
there is no rigorous analysis for optimizing multi-layer homogeneous functions.
For a general (non-convex) optimization problem  it is known that if the objective function satisﬁes
(i) gradient changes smoothly if the parameters are perturbed  (ii) all saddle points and local maxima
are strict (i.e.  there exists a direction with negative curvature)  and (iii) all local minima are global
(no spurious local minimum)  then gradient descent [Lee et al.  2016  Panageas and Piliouras  2016]
converges to a global minimum. There have been many studies on the optimization landscapes
of neural networks [Kawaguchi  2016  Choromanska et al.  2015  Du and Lee  2018  Hardt and
Ma  2016  Bartlett et al.  2018  Haeffele and Vidal  2015  Freeman and Bruna  2016  Vidal et al. 
2017  Safran and Shamir  2016  Zhou and Feng  2017  Nguyen and Hein  2017a b  Zhou and Feng 
2017  Safran and Shamir  2017]  showing that the objective functions have properties (ii) and (iii).

3

Nevertheless  the objective function is in general not smooth as we discussed before. Our paper
complements these results by showing that the magnitudes of all layers are balanced and in many
cases  this implies smoothness.

Paper organization. The rest of the paper is organized as follows. In Section 2  we present our
main theoretical result on the implicit regularization property of gradient ﬂow for optimizing neu-
ral networks. In Section 3  we analyze the dynamics of randomly initialized gradient descent for
asymmetric matrix factorization problem with unregularized objective function (1). In Section 4 
we empirically verify the theoretical result in Section 2. We conclude and list future directions in
Section 5. Some technical proofs are deferred to the appendix.
Notation. We use bold-faced letters for vectors and matrices. For a vector x  denote by xris its
i-th coordinate. For a matrix A  we use Ari  js to denote its pi  jq-th entry  and use Ari  :s and
Ar:  js to denote its i-th row and j-th column  respectively (both as column vectors). We use }¨}2 or
}¨} to denote the Euclidean norm of a vector  and use }¨}F to denote the Frobenius norm of a matrix.
We use x¨ ¨y to denote the standard Euclidean inner product between two vectors or two matrices.
Let rns “ t1  2  . . .   nu.
2 The Auto-Balancing Properties in Deep Neural Networks

In this section we study the implicit regularization imposed by gradient descent with inﬁnitesimal
step size (gradient ﬂow) in training deep neural networks. In Section 2.1 we consider fully con-
nected neural networks  and our main result (Theorem 2.1) shows that gradient ﬂow automatically
balances the incoming and outgoing weights at every neuron. This directly implies that the weights
between different layers are balanced (Corollary 2.1). For linear activation  we derive a stronger
auto-balancing property (Theorem 2.2).
In Section 2.2 we generalize our result from fully con-
nected neural networks to convolutional neural networks. In Section 2.3 we present the proof of
Theorem 2.1. The proofs of other theorems in this section follow similar ideas and are deferred to
Appendix A.

2.1 Fully Connected Neural Networks
We ﬁrst formally deﬁne a fully connected feed-forward neural network with N (N • 2) layers. Let
W phq P Rnhˆnh´1 be the weight matrix in the h-th layer  and deﬁne w “ pW phqqN
h“1 as a shorthand
of the collection of all the weights. Then the function fw : Rd Ñ Rp (d “ n0  p “ nN) computed
by this network can be deﬁned recursively: fp1qw pxq “ W p1qx  fphqw pxq “ W phqh´1pfph´1qw
pxqq
(h “ 2  . . .   N)  and fwpxq “ fpNqw pxq  where each h is an activation function that acts coordinate-
wise on vectors.8 We assume that each h (h P rN´1s) is homogeneous  namely  hpxq “ 1hpxq¨x
for all x and all elements of the sub-differential 1hp¨q when h is non-differentiable at x. This
property is satisﬁed by functions like ReLU pxq “ maxtx  0u  Leaky ReLU pxq “ maxtx  ↵xu
(0 † ↵ † 1)  and linear function pxq “ x.
Let ` : Rp ˆ Rp Ñ R•0 be a differentiable loss function. Given a training dataset tpxi  yiqum
i“1 Ä
Rd ˆ Rp  the training loss as a function of the network parameters w is deﬁned as

mÿi“1
We consider gradient descent with inﬁnitesimal step size (also known as gradient ﬂow) applied on
Lpwq  which is captured by the differential inclusion:

`pfwpxiq  yiq .

Lpwq “ 1

(4)

m

dW phq

P ´BLpwq
(5)
BW phq  
where t is a continuous time index  and BLpwq
BW phq is the Clarke sub-differential [Clarke et al.  2008]. If
curves W phq “ W phqptq (h P rNs) evolve with time according to (5) they are said to be a solution
of the gradient ﬂow differential inclusion.
8We omit the trainable bias weights in the network for simplicity  but our results can be directly generalized

h “ 1  . . .   N 

dt

to allow bias weights.

4

Our main result in this section is the following invariance imposed by gradient ﬂow.
Theorem 2.1 (Balanced incoming and outgoing weights at every neuron). For any h P rN ´ 1s and
i P rnhs  we have

d

dt´}W phqri  :s}2 ´ }W ph`1qr:  is}2¯ “ 0.

(6)

Note that W phqri  :s is a vector consisting of network weights coming into the i-th neuron in the h-th
hidden layer  and W ph`1qr:  is is the vector of weights going out from the same neuron. Therefore 
Theorem 2.1 shows that gradient ﬂow exactly preserves the difference between the squared `2-norms
of incoming weights and outgoing weights at any neuron.
Taking sum of (6) over i P rnhs  we obtain the following corollary which says gradient ﬂow pre-
serves the difference between the squares of Frobenius norms of weight matrices.
Corollary 2.1 (Balanced weights across layers). For any h P rN ´ 1s  we have

d

dt´}W phq}2

F ´ }W ph`1q}2

F¯ “ 0.
F ´ }W ph`1q}2

if we use a small initialization  }W phq}2

Corollary 2.1 explains why in practice  trained multi-layer models usually have similar magnitudes
on all the layers:
F is very small at the
beginning  and Corollary 2.1 implies this difference remains small at all time. This ﬁnding also
partially explains why gradient descent converges. Although the objective function like (4) may not
be smooth over the entire parameter space  given that }W phq}2
F is small for all h  the
objective function may have smoothness. Under this condition  standard theory shows that gradient
descent converges. We believe this ﬁnding serves as a key building block for understanding ﬁrst
order methods for training deep neural networks.
For linear activation  we have the following stronger invariance than Theorem 2.1:
Theorem 2.2 (Stronger balancedness property for linear activation). If for some h P rN ´ 1s we
have hpxq “ x  then

F ´}W ph`1q}2

d

dt´W phqpW phqqJ ´ pW ph`1qqJW ph`1q¯ “ 0.

This result was known for linear networks [Arora et al.  2018]  but the proof there relies on the entire
network being linear while Theorem 2.2 only needs two consecutive layers to have no nonlinear
activations in between.
While Theorem 2.1 shows the invariance in a node-wise manner  Theorem 2.2 shows for linear
activation  we can derive a layer-wise invariance. Inspired by this strong invariance  in Section 3 we
prove gradient descent with positive step sizes preserves this invariance approximately for matrix
factorization.

2.2 Convolutional Neural Networks

Now we show that the conservation property in Corollary 2.1 can be generalized to convolutional
neural networks. In fact  we can allow arbitrary sparsity pattern and weight sharing structure within
a layer; convolutional layers are a special case.

Neural networks with sparse connections and shared weights. We use the same notation as in
Section 2.1  with the difference that some weights in a layer can be missing or shared. Formally  the
weight matrix W phq P Rnhˆnh´1 in layer h (h P rNs) can be described by a vector vphq P Rdh and
a function gh : rnhsˆrnh´1s Ñ rdhsYt0u. Here vphq consists of the actual free parameters in this
layer and dh is the number of free parameters (e.g. if there are k convolutional ﬁlters in layer h each
with size r  we have dh “ r ¨ k). The map gh represents the sparsity and weight sharing pattern:

W phqri  js “"0 

vphqrks 

ghpi  jq “ 0 
ghpi  jq “ k ° 0.

5

The following theorem generalizes Corollary 2.1 to neural networks with sparse connections and
shared weights:
Theorem 2.3. For any h P rN ´ 1s  we have

d

dt´}vphq}2 ´ }vph`1q}2¯ “ 0.

h“1 the collection of all the parameters in this network  and we consider

Denote by v “ `vphq˘N

gradient ﬂow to learn the parameters:
dvphq
dt

P ´BLpvq
Bvphq  

h “ 1  . . .   N.

Therefore  for a neural network with arbitrary sparsity pattern and weight sharing structure  gradient
ﬂow still balances the magnitudes of all layers.

2.3 Proof of Theorem 2.1
The proofs of all theorems in this section are similar. They are based on the use of the chain rule
(i.e. back-propagation) and the property of homogeneous activations. Below we provide the proof
of Theorem 2.1 and defer the proofs of other theorems to Appendix A.

dt

k“1

dtpW phqri  jsq2 “ 2W phqri  js¨ dW phqri js
BLkpwq
BW phqri js

Proof of Theorem 2.1. First we note that we can without loss of generality assume L is the loss
In fact  for
associated with one data sample px  yq P Rd ˆ Rp  i.e.  Lpwq “ `pfwpxq  yq.
m∞m
k“1 Lkpwq where Lkpwq “ `pfwpxkq  ykq  for any single weight W phqri  js in the
Lpwq “ 1
BLpwq
“ ´2W phqri  js¨
network we can compute d
BW phqri js “
m∞m
´2W phqri  js ¨ 1
  using the sharp chain rule of differential inclusions for tame
functions [Drusvyatskiy et al.  2015  Davis et al.  2018]. Thus  if we can prove the theorem for
every individual loss Lk  we can prove the theorem for L by taking average over k P rms.
Therefore in the rest of proof we assume Lpwq “ `pfwpxq  yq. For convenience  we denote xphq “
fphqw pxq (h P rNs)  which is the input to the h-th hidden layer of neurons for h P rN ´ 1s and is the
output of the network for h “ N. We also denote xp0q “ x and 0pxq “ x (@x).
Now we prove (6). Since W ph`1qrk  is (k P rnh`1s) can only affect Lpwq through xph`1qrks   we
have for k P rnh`1s 
Bxph`1qrks
BW ph`1qrk  is “ BLpwq
BLpwq
BW ph`1qrk  is “ BLpwq
Bxph`1qrks ¨
BW ph`1qr:  is “ hpxphqrisq ¨ BLpwq
BLpwq
Bxph`1q .

Bxph`1qrks ¨ hpxphqrisq 

which can be rewritten as

It follows that

d

dt}W ph`1qr:  is}2 “ 2BW ph`1qr:  is 

d
dt

W ph`1qr:  isF “ ´2BW ph`1qr:  is 

BLpwq

BW ph`1qr:  isF

“ ´2hpxphqrisq ¨BW ph`1qr:  is  BLpwq
Bxph`1qF .
Bxphqris ¨ h´1pxph´1qq “B BLpwq

On the other hand  W phqri  :s only affects Lpwq through xphqris. Using the chain rule  we get
BLpwq
BW phqri  :s “ BLpwq

Bxph`1q   W ph`1qr:  isF ¨ 1hpxphqrisq ¨ h´1pxph´1qq 

where 1 is interpreted as a set-valued mapping whenever it is applied at a non-differentiable point.9
9More precisely  the equalities should be an inclusion whenever there is a sub-differential  but as we see in

(7)

the next display the ambiguity in the choice of sub-differential does not affect later calculations.

6

It follows that10

d

d
dt

dt}W phqri  :s}2 “ 2BW phqri  :s 
“ ´ 2B BLpwq
“ ´ 2B BLpwq

W phqri  :sF “ ´2BW phqri  :s 
Bxph`1q   W ph`1qr:  isF ¨ 1hpxphqrisq ¨AW phqri  :s  h´1pxph´1qqE
Bxph`1q   W ph`1qr:  isF ¨ 1hpxphqrisq ¨ xphqris “ ´2B BLpwq

Comparing the above expression to (7)  we ﬁnish the proof.

BLpwq

BW phqri  :sF

Bxph`1q   W ph`1qr:  isF ¨ hpxphqrisq.

3 Gradient Descent Converges to Global Minimum for Asymmetric Matrix

Factorization

In this section we constrain ourselves to the asymmetric matrix factorization problem and analyze the
gradient descent algorithm with random initialization. Our analysis is inspired by the auto-balancing
properties presented in Section 2. We extend these properties from gradient ﬂow to gradient descent
with positive step size.
Formally  we study the following non-convex optimization problem:

min

UPRd1ˆr V PRd2ˆr

fpU   V q “ 1

2››U V J ´ M˚››2

F  

where M˚ P Rd1ˆd2 has rank r. Note that we do not have any explicit regularization in (8). The
gradient descent dynamics for (8) have the following form:

(8)

›› ¯U ¯V J ´ M˚››F § ✏.

Ut`1 “ Ut ´ ⌘tpUtV Jt ´ M˚qVt 

Vt`1 “ Vt ´ ⌘tpUtV Jt ´ M˚qJUt.

(9)

3.1 The General Rank-r Case
First we consider the general case of r • 1. Our main theorem below says that if we use a random
small initialization pU0  V0q  and set step sizes ⌘t to be appropriately small  then gradient descent
(9) will converge to a solution close to the global minimum of (8). To our knowledge  this is the ﬁrst
result showing that gradient descent with random initialization directly solves the un-regularized
asymmetric matrix factorization problem (8).
Theorem 3.1. Let 0 † ✏ †} M˚}F . Suppose we initialize the entries in U0 and V0 i.i.d. from
(t “ 0  1  . . .).11
Np0 
Then with high probability over the initialization  limtÑ8pUt  Vtq “ p ¯U   ¯V q exists and satisﬁes

polypdqq (d “ maxtd1  d2u)  and run (9) with step sizes ⌘t “

100pt`1q}M˚}3{2

?✏{r

✏

F

Proof sketch of Theorem 3.1. First let’s imagine that we are using inﬁnitesimal step size in GD.
Then according to Theorem 2.2 (viewing problem (8) as learning a two-layer linear network where
the inputs are all the standard unit vectors in Rd2)  we know that UJU ´ V JV will stay invariant
throughout the algorithm. Hence when U and V are initialized to be small  UJU ´ V JV will stay
small forever. Combined with the fact that the objective fpU   V q is decreasing over time (which
means U V J cannot be too far from M˚)  we can show that U and V will always stay bounded.
Now we are using positive step sizes ⌘t  so we no longer have the invariance of UJU ´ V JV .
Nevertheless  by a careful analysis of the updates  we can still prove that UJt Ut ´ V Jt Vt is small 
the objective fpUt  Vtq decreases  and Ut and Vt stay bounded. Formally  we have the following
lemma:
Lemma 3.1. With high probability over the initialization pU0  V0q  for all t we have:
10This holds for any choice of element of the sub-differential  since 1pxqx “ pxq holds at x “ 0 for any
11The dependency of ⌘t on t can be ⌘t “ ⇥´t´p1{2`q¯ for any constant  P p0  1{2s.

choice of sub-differential.

7

F § 5?r }M˚}F .

(i) Balancedness:››UJt Ut ´ V Jt Vt››F § ✏;
F § 5?r }M˚}F  }Vt}2

(ii) Decreasing objective: fpUt  Vtq§ fpUt´1  Vt´1q§¨¨¨§ fpU0  V0q§ 2}M˚}2
F ;
(iii) Boundedness: }Ut}2
Now that we know the GD algorithm automatically constrains pUt  Vtq in a bounded region  we
can use the smoothness of f in this region and a standard analysis of GD to show that pUt  Vtq
converges to a stationary point p ¯U   ¯V q of f (Lemma B.2). Furthermore  using the results of [Lee
et al.  2016  Panageas and Piliouras  2016] we know that p ¯U   ¯V q is almost surely not a strict saddle
point. Then the following lemma implies that p ¯U   ¯V q has to be close to a global optimum since we

know›› ¯UJ ¯U ´ ¯V J ¯V››F § ✏ from Lemma 3.1 (i). This would complete the proof of Theorem 3.1.
Lemma 3.2. Suppose pU   V q is a stationary point of f such that››UJU ´ V JV››F § ✏. Then
either››U V J ´ M˚››F § ✏  or pU   V q is a strict saddle point of f.

The full proof of Theorem 3.1 and the proofs of Lemmas 3.1 and 3.2 are given in Appendix B.

3.2 The Rank-1 Case
We have shown in Theorem 3.1 that GD with small and diminishing step sizes converges to a global
minimum for matrix factorization. Empirically  it is observed that a constant step size ⌘t ” ⌘ is
enough for GD to converge quickly to global minimum. Therefore  some natural questions are
how to prove convergence of GD with a constant step size  how fast it converges  and how the
discretization affects the invariance we derived in Section 2.
While these questions remain challenging for the general rank-r matrix factorization  we resolve
them for the case of r “ 1. Our main ﬁnding is that with constant step size  the norms of two layers
are always within a constant factor of each other (although we may no longer have the stronger
balancedness property as in Lemma 3.1)  and we utilize this property to prove the linear convergence
of GD to a global minimum.
When r “ 1  the asymmetric matrix factorization problem and its GD dynamics become

and

min

uPRd1  vPRd2

ut`1 “ ut ´ ⌘putvJt ´ M˚qvt 

F

1

2››uvJ ´ M˚››2
vt`1 “ vt ´ ⌘`vtuJt ´ M˚J˘ ut.

Here we assume M˚ has rank 1  i.e.  it can be factorized as M˚ “ 1u˚v˚J where u˚ and v˚ are
unit vectors and 1 ° 0.
Our main theoretical result is the following.
Theorem 3.2 (Approximate balancedness and linear convergence of GD for rank-1 matrix factor-
ization). Suppose u0 „ Np0  Iq  v0 „ Np0  Iq with  “ cinita 1
d (d “ maxtd1  d2u) for some
sufﬁciently small constant cinit ° 0  and ⌘ “ cstep
for some sufﬁciently small constant cstep ° 0.
Then with constant probability over the initialization  for all t we have c0 § |uJt u˚|
|vJt v˚| § C0 for some
universal constants c0  C0 ° 0. Furthermore  for any 0 † ✏ † 1  after t “ O`log d
✏˘ iterations  we
have››utvJt ´ M˚››F § ✏1.
Theorem 3.2 shows for ut and vt  their strengths in the signal space ˇˇuJt u˚ˇˇ andˇˇvJt v˚ˇˇ  are of the

same order. This approximate balancedness helps us prove the linear convergence of GD. We refer
readers to Appendix C for the proof of Theorem 3.2.

1

4 Empirical Veriﬁcation

We perform experiments to verify the auto-balancing properties of gradient descent in neural net-
works with ReLU activation. Our results below show that for GD with small step size and small

8

i

m
r
o
N
 
s
u
n
e
b
o
r
F
d
e
r
a
u
q
S
e
h

 

 

t
 
f

 

o
e
c
n
e
r
e

f
f
i

D

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

0

Between 1st and 2nd Layer
Between 2nd and 3rd Layer

2000

4000

6000

8000

10000

Epochs

s
o

i
t

i

 

a
R
m
r
o
N
 
s
u
n
e
b
o
r
F
d
e
r
a
u
q
S

 

1.006

1.004

1.002

1

0.998

0.996

0.994

0.992

i

m
r
o
N
 
s
u
n
e
b
o
r
F
d
e
r
a
u
q
S
e
h

 

 

Between 1st and 2nd Layer
Between 2nd and 3rd Layer

0

2000

4000

6000

8000

10000

Epochs

t
 
f

o

 

e
c
n
e
r
e

f
f
i

D

10

8

6

4

2

0

0

Between 1st and 2nd Layer
Between 2nd and 3rd Layer

2000

4000

6000

8000

10000

Epochs

s
o

i
t

i

 

a
R
m
r
o
N
 
s
u
n
e
b
o
r
F
d
e
r
a
u
q
S

 

12

10

8

6

4

2

0

Between 1st and 2nd Layer
Between 2nd and 3rd Layer

0

2000

4000

6000

8000

10000

Epochs

(a) Balanced initializa-
tion  squared norm dif-
ferences.

(b) Balanced initializa-
tion  squared norm ra-
tios.

(c) Unbalanced Initial-
ization 
squared norm
differences.

(d) Unbalanced initial-
ization 
squared norm
ratios.

Figure 2: Balancedness of a 3-layer neural network.

initialization: (1) the difference between the squared Frobenius norms of any two layers remains
small in all iterations  and (2) the ratio between the squared Frobenius norms of any two layers
becomes close to 1. Notice that our theorems in Section 2 hold for gradient ﬂow (step size Ñ 0)
but in practice we can only choose a (small) positive step size  so we cannot hope the difference
between the squared Frobenius norms to remain exactly the same but can only hope to observe that
the differences remain small.
We consider a 3-layer fully connected network of the form fpxq “ W3pW2pW1xqq where
x P R1 000 is the input  W1 P R100ˆ1 000  W2 P R100ˆ100  W3 P R10ˆ100  and p¨q is ReLU
activation. We use 1 000 data points and the quadratic loss function  and run GD. We ﬁrst test a bal-
anced initialization: W1ri  js „ Np0  10´4
10 q and W3ri  js „ Np0  10´4q 
which ensures }W1}2
F “ 42.90 
F « }W3}2
F “ 43.76 and }W3}2
}W2}2
F ´ }W2}2
F ´ }W3}2
F . Figures 2b shows that the ratios between norms approach 1. We then test an unbalanced
}Wh}2
initialization: W1ri  js „ Np0  10´4q  W2ri  js „ Np0  10´4q and W3ri  js „ Np0  10´4q. Af-
ter 10 000 iterations we have }W1}2
F “ 45.46. Figure 2c
little throughout the process)  and Figures 2d shows that the ratios become close to 1 after about
1 000 iterations.

Fˇˇ
Fˇˇ are bounded by 0.14 which is much smaller than the magnitude of each
Fˇˇ are bounded by 9 (and indeed change very

andˇˇ}W2}2
shows thatˇˇ}W1}2

100 q  W2ri  js „ Np0  10´4

F « }W2}2

F . After 10 000 iterations we have }W1}2

F “ 43.68. Figure 2a shows that in all iterationsˇˇ}W1}2
Fˇˇ andˇˇ}W2}2

F “ 55.50  }W2}2
F ´ }W3}2

F “ 45.65 and }W3}2

F ´ }W2}2

5 Conclusion and Future Work

In this paper we take a step towards characterizing the invariance imposed by ﬁrst order algorithms.
We show that gradient ﬂow automatically balances the magnitudes of all layers in a deep neural net-
work with homogeneous activations. For the concrete model of asymmetric matrix factorization  we
further use the balancedness property to show that gradient descent converges to global minimum.
We believe our ﬁndings on the invariance in deep models could serve as a fundamental building
block for understanding optimization in deep learning. Below we list some future directions.

Other ﬁrst-order methods.
In this paper we focus on the invariance induced by gradient descent.
In practice  different acceleration and adaptive methods are also used. A natural future direction is
how to characterize the invariance properties of these algorithms.

From gradient ﬂow to gradient descent: a generic analysis? As discussed in Section 3  while
strong invariance properties hold for gradient ﬂow  in practice one uses gradient descent with positive
step sizes and the invariance may only hold approximately because positive step sizes discretize the
dynamics. We use specialized techniques for analyzing asymmetric matrix factorization. It would be
very interesting to develop a generic approach to analyze the discretization. Recent ﬁndings on the
connection between optimization and ordinary differential equations [Su et al.  2014  Zhang et al. 
2018] might be useful for this purpose.

9

Acknowledgements

We thank Phil Long for his helpful comments on an earlier draft of this paper. JDL acknowledges
support from ARO W911NF-11-1-0303.

References
Pierre-Antoine Absil  Robert Mahony  and Benjamin Andrews. Convergence of the iterates of de-
scent methods for analytic cost functions. SIAM Journal on Optimization  16(2):531–547  2005.
Sanjeev Arora  Nadav Cohen  and Elad Hazan. On the optimization of deep networks: Implicit

acceleration by overparameterization. arXiv preprint arXiv:1802.06509  2018.

Peter L Bartlett  David P Helmbold  and Philip M Long. Gradient descent with identity initialization
efﬁciently learns positive deﬁnite linear transformations by deep residual networks. arXiv preprint
arXiv:1802.06093  2018.

Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian

inputs. arXiv preprint arXiv:1702.07966  2017.

Alon Brutzkus  Amir Globerson  Eran Malach  and Shai Shalev-Shwartz.

parameterized networks that provably generalize on linearly separable data.
arXiv:1710.10174  2017.

Sgd learns over-
arXiv preprint

Anna Choromanska  Mikael Henaff  Michael Mathieu  G´erard Ben Arous  and Yann LeCun. The
In Artiﬁcial Intelligence and Statistics  pages 192–204 

loss surfaces of multilayer networks.
2015.

Francis H Clarke  Yuri S Ledyaev  Ronald J Stern  and Peter R Wolenski. Nonsmooth analysis and

control theory  volume 178. Springer Science & Business Media  2008.

Damek Davis  Dmitriy Drusvyatskiy  Sham Kakade  and Jason D Lee. Stochastic subgradient

method converges on tame functions. arXiv preprint arXiv:1804.07795  2018.

Dmitriy Drusvyatskiy  Alexander D Ioffe  and Adrian S Lewis. Curves of descent. SIAM Journal

on Control and Optimization  53(1):114–138  2015.

Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with

quadratic activation. arXiv preprint arXiv:1803.01206  2018.

Simon S Du  Jason D Lee  and Yuandong Tian. When is a convolutional ﬁlter easy to learn? arXiv

preprint arXiv:1709.06129  2017a.

Simon S Du  Jason D Lee  Yuandong Tian  Barnabas Poczos  and Aarti Singh. Gradient de-
scent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. arXiv preprint
arXiv:1712.00779  2017b.

C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectiﬁed network optimization.

arXiv preprint arXiv:1611.01540  2016.

Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points ´ online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory 
pages 797–842  2015.

Rong Ge  Chi Jin  and Yi Zheng. No spurious local minima in nonconvex low rank problems:
A uniﬁed geometric analysis. In Proceedings of the 34th International Conference on Machine
Learning  pages 1233–1242  2017a.

Rong Ge  Jason D Lee  and Tengyu Ma. Learning one-hidden-layer neural networks with landscape

design. arXiv preprint arXiv:1711.00501  2017b.

Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and
statistics  pages 249–256  2010.

10

Benjamin D Haeffele and Ren´e Vidal. Global optimality in tensor factorization  deep learning  and

beyond. arXiv preprint arXiv:1506.07540  2015.

Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231 

2016.

Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition  pages
770–778  2016.

Kenji Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information

Processing Systems  pages 586–594  2016.

Jason D Lee  Max Simchowitz  Michael I Jordan  and Benjamin Recht. Gradient descent only

converges to minimizers. In Conference on Learning Theory  pages 1246–1257  2016.

Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activa-

tion. arXiv preprint arXiv:1705.09886  2017.

Siyuan Ma  Raef Bassily  and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of sgd in modern over-parametrized learning. arXiv preprint arXiv:1712.06559 
2017.

Behnam Neyshabur  Ruslan R Salakhutdinov  and Nati Srebro. Path-SGD: Path-normalized opti-
mization in deep neural networks. In Advances in Neural Information Processing Systems  pages
2422–2430  2015a.

Behnam Neyshabur  Ryota Tomioka  Ruslan Salakhutdinov  and Nathan Srebro. Data-dependent

path normalization in neural networks. arXiv preprint arXiv:1511.06747  2015b.

Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv

preprint arXiv:1704.08045  2017a.

Quynh Nguyen and Matthias Hein. The loss surface and expressivity of deep convolutional neural

networks. arXiv preprint arXiv:1710.10928  2017b.

Ioannis Panageas and Georgios Piliouras. Gradient descent only converges to minimizers: Non-

isolated critical points and invariant regions. arXiv preprint arXiv:1605.00405  2016.

Itay Safran and Ohad Shamir. On the quality of the initial basin in overspeciﬁed neural networks.

In International Conference on Machine Learning  pages 774–782  2016.

Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.

arXiv preprint arXiv:1712.08968  2017.

O. Shamir. Are resnets provably better than linear predictors? arXiv preprint arXiv:1804.06739 

2018.

Weijie Su  Stephen Boyd  and Emmanuel Candes. A differential equation for modeling nesterovs
accelerated gradient method: Theory and insights. In Advances in Neural Information Processing
Systems  pages 2510–2518  2014.

Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its
applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560  2017.

Stephen Tu  Ross Boczar  Max Simchowitz  Mahdi Soltanolkotabi  and Benjamin Recht. Low-rank
solutions of linear matrix equations via procrustes ﬂow. arXiv preprint arXiv:1507.03566  2015.

Rene Vidal  Joan Bruna  Raja Giryes  and Stefano Soatto. Mathematics of deep learning. arXiv

preprint arXiv:1712.04741  2017.

Jingzhao Zhang  Aryan Mokhtari  Suvrit Sra  and Ali Jadbabaie. Direct runge-kutta discretization

achieves acceleration. arXiv preprint arXiv:1805.00521  2018.

11

Kai Zhong  Zhao Song  Prateek Jain  Peter L Bartlett  and Inderjit S Dhillon. Recovery guarantees

for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175  2017.
The landscape of deep learning algorithms.

Pan Zhou and Jiashi Feng.

arXiv:1705.07038  2017.

arXiv preprint

12

,Simon Du
Wei Hu
Jason Lee