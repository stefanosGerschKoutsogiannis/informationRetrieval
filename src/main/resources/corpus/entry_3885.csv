2016,On Graph Reconstruction via Empirical Risk Minimization: Fast Learning Rates and Scalability,The problem of predicting connections between a set of data points finds many applications  in systems biology and social network analysis among others. This paper focuses on the \textit{graph reconstruction} problem  where the prediction rule is obtained by minimizing the average error over all n(n-1)/2 possible pairs of the n nodes of a training graph. Our first contribution is to derive learning rates of order O(log n / n) for this problem  significantly improving upon the slow rates of order O(1/√n) established in the seminal work of Biau & Bleakley (2006). Strikingly  these fast rates are universal  in contrast to similar results known for other statistical learning problems (e.g.  classification  density level set estimation  ranking  clustering) which require strong assumptions on the distribution of the data. Motivated by applications to large graphs  our second contribution deals with the computational complexity of graph reconstruction. Specifically  we investigate to which extent the learning rates can be preserved when replacing the empirical reconstruction risk by a computationally cheaper Monte-Carlo version  obtained by sampling with replacement B << n² pairs of nodes. Finally  we illustrate our theoretical results by numerical experiments on synthetic and real graphs.,On Graph Reconstruction via Empirical Risk

Minimization: Fast Learning Rates and Scalability

Guillaume Papa  Stéphan Clémençon

LTCI  CNRS  Télécom ParisTech  Université Paris-Saclay

75013  Paris  France

first.last@telecom-paristech.fr

Aurélien Bellet

INRIA

59650 Villeneuve d’Ascq  France
aurelien.bellet@inria.fr

Abstract

The problem of predicting connections between a set of data points ﬁnds many
applications  in systems biology and social network analysis among others. This
paper focuses on the graph reconstruction problem  where the prediction rule is
obtained by minimizing the average error over all n(n − 1)/2 possible pairs of
the n nodes of a training graph. Our ﬁrst contribution is to derive learning rates of
√
order OP(log n/n) for this problem  signiﬁcantly improving upon the slow rates
of order OP(1/
n) established in the seminal work of Biau and Bleakley (2006).
Strikingly  these fast rates are universal  in contrast to similar results known for
other statistical learning problems (e.g.  classiﬁcation  density level set estimation 
ranking  clustering) which require strong assumptions on the distribution of the
data. Motivated by applications to large graphs  our second contribution deals with
the computational complexity of graph reconstruction. Speciﬁcally  we investigate
to which extent the learning rates can be preserved when replacing the empirical
reconstruction risk by a computationally cheaper Monte-Carlo version  obtained
by sampling with replacement B (cid:28) n2 pairs of nodes. Finally  we illustrate our
theoretical results by numerical experiments on synthetic and real graphs.

1

Introduction

√

Although statistical learning theory mainly focuses on establishing universal rate bounds (i.e. 
which hold for any distribution of the data) for the accuracy of a decision rule based on training
observations  reﬁned concentration inequalities have recently helped understanding conditions on
the data distribution under which learning paradigms such as Empirical Risk Minimization (ERM)
In binary classiﬁcation  i.e.  the problem of learning to predict a random
lead to faster rates.
binary label Y ∈ {−1  +1} from on an input random variable X based on independent copies
(X1  Y1)  . . .   (Xn  Yn) of the pair (X  Y )  rates faster than 1/
n are achieved when little mass in
the vicinity of 1/2 is assigned by the distribution of the random variable η(X) = P{Y = +1 | X}.
This condition and its generalizations are referred to as the Mammen-Tsybakov noise conditions (see
Mammen and Tsybakov  1999; Tsybakov  2004; Massart and Nédélec  2006). It has been shown
that a similar phenomenon occurs for various other statistical learning problems. Indeed  speciﬁc
conditions under which fast rate results hold have been exhibited for density level set estimation
(Rigollet and Vert  2009)  (bipartite) ranking (Clémençon et al.  2008; Clémençon and Robbiano 
2011; Agarwal  2014)  clustering (Antos et al.  2005; Clémençon  2014) and composite hypothesis
testing (Clémençon and Vayatis  2010).
In this paper  we consider the supervised learning problem on graphs referred to as graph reconstruc-
tion  rigorously formulated by Biau and Bleakley (2006). The objective of graph reconstruction is to
predict the possible occurrence of connections between a set of objects/individuals known to form the
nodes of an undirected graph. Precisely  each node is described by a random vector X which deﬁnes

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

a form of conditional preferential attachment: one predicts whether two nodes are connected based
on their features X and X(cid:48). This statistical learning problem is motivated by a variety of applications
such as systems biology (e.g.  inferring protein-protein interactions or metabolic networks  see Jansen
et al.  2003; Kanehisa  2001) and social network analysis (e.g.  predicting future connections between
users  see Liben-Nowell and Kleinberg  2003). It has recently been the subject of a good deal of
attention in the machine learning literature (see Vert and Yamanishi  2004; Biau and Bleakley  2006;
Shaw et al.  2011)  and is also known as supervised link prediction (Lichtenwalter et al.  2010;
Cukierski et al.  2011). The learning task is formulated as the minimization of a reconstruction risk 
whose natural empirical version is the average prediction error over the n(n − 1)/2 pairs of nodes in
√
a training graph of size n. Under standard complexity assumptions on the set of candidate prediction
rules  excess risk bounds of the order OP(1/
n) for the empirical risk minimizers have been estab-
lished by Biau and Bleakley (2006) based on a representation of the objective functional very similar
to the ﬁrst Hoeffding decomposition for second-order U-statistics (see Hoeffding  1948). However 
Biau & Bleakley ignored the computational complexity of ﬁnding an empirical risk minimizer  which
scales at least as O(n2) since the empirical graph reconstruction risk involves summing up over
n(n − 1)/2 terms. This makes the approach impractical when dealing with large graphs commonly
found in many applications.
Building up on the above work  our contributions to statistical graph reconstruction are two-fold:
Universal fast rates. We prove that a fast rate of order OP(log n/n) is always achieved by empirical
√
reconstruction risk minimizers  in absence of any restrictive condition imposed on the data distribution.
This is much faster than the OP(1/
n) rate established by Biau and Bleakley (2006). Our analysis is
based on a different decomposition of the excess of reconstruction risk of any decision rule candidate 
involving the second Hoeffding representation of a U-statistic approximating it  as well as appropriate
maximal/concentration inequalities.
Scaling-up ERM. We investigate the performance of minimizers of computationally cheaper Monte-
Carlo estimates of the empirical reconstruction risk  built by averaging over B (cid:28) n2 pairs of vertices
drawn with replacement. The rate bounds we obtain highlight that B plays the role of a tuning
parameter to achieve an effective trade-off between statistical accuracy and computational cost.
Numerical results based on simulated graphs and real-world networks are presented in order to
support these theoretical ﬁndings.
The paper is organized as follows. In Section 2  we present the probabilistic setting for graph
reconstruction and recall state-of-the-art results. Section 3 provides our fast rate bound analysis 
while Section 4 deals with the problem of scaling-up reconstruction risk minimization to large graphs.
Numerical experiments are displayed in Section 5  and a few concluding remarks are collected
in Section 6. The technical proofs can be found in the Supplementary Material  along with some
additional remarks and results.

2 Background and Preliminaries

We start by describing at length the probabilistic framework we consider for statistical inference on
graphs  as introduced by Biau and Bleakley (2006). We then brieﬂy recall the related theoretical
results documented in the literature.

2.1 A Probabilistic Setup for Preferential Attachment
In this paper  G = (V  E) is an undirected random graph with a set V = {1  . . .   n} of n ≥ 2
vertices and a set E = {ei j : 1 ≤ i (cid:54)= j ≤ n} ∈ {0  1}n(n−1) describing its edges: for all i (cid:54)= j  we
have ei j = ej i = +1 if the vertices i and j are connected by an edge and ei j = ej i = 0 otherwise.
We assume that G is a Bernoulli graph  i.e. the random variables ei j  1 ≤ i < j ≤ n  are independent
labels drawn from a Bernoulli distribution Ber(p) with parameter p = P{ei j = +1}  the probability
that two vertices of G are connected by an edge. The degree of each vertex is thus distributed as a
binomial with parameters n and p  which can be classically approximated by a Poisson distribution
of parameter λ > 0 in the limit of large n  when np → λ.
Whereas the marginal distribution of the graph G is that of a Bernoulli graph (also sometimes
abusively referred to as a random graph)  a form of conditional preferential attachment is also
speciﬁed in the framework considered here. Precisely  we assume that  for all i ∈ V   a continuous r.v.

2

Xi  taking its values in a separable Banach space X   describes some features related to vertex i. The
Xi’s are i.i.d. with common distribution µ(dx) and  for any i (cid:54)= j  the random pair (Xi  Xj) models
some information useful for predicting the occurrence of an edge connecting the vertices i and j.
Conditioned upon the features (X1  . . .   Xn)  any binary variables ei j and ek l are independent
only if {i  j} ∩ {k  l} = ∅. The conditional distribution of ei j  i (cid:54)= j  is supposed to depend on
(Xi  Xj) solely  described by the posterior preferential attachment probability:

η (Xi  Xj) = P{ei j = +1 | (Xi  Xj)} .

x∈X η(Xi  x)µ(dx) (respectively  (cid:80)

is thus (n − 1)(cid:82)
equipped with these notations  p = (cid:82)

(1)
For instance  ∀(x1  x2) ∈ X 2  η (x1  x2) can be a certain function of a speciﬁc distance or similarity
measure between x1 and x2  as in the synthetic graphs described in Section 5.
The conditional average degree of the vertex i ∈ V given Xi (respectively  given (X1  . . .   Xn))
j(cid:54)=i η(Xi  Xj)). Observe incidentally that 
the 3-tuples
(Xi  Xj  ei j)  1 ≤ i < j ≤ n  are non-i.i.d. copies of a generic random vector (X1  X2  e1 2)
whose distribution L is given by the tensorial product µ(dx1) ⊗ µ(dx2) ⊗ Ber(η(x1  x2))  which
is fully described by the pair (µ  η). Observe also that the function η is symmetric by construction:
∀(x1  x2) ∈ X 2  η(x1  x2) = η(x2  x1).
In this framework  the learning problem introduced by Biau and Bleakley (2006)  referred to as graph
reconstruction  consists in building a symmetric reconstruction rule g : X 2 → {0  1}  from a training
graph G  with nearly minimum reconstruction risk

(x x(cid:48))∈X 2 η(x  x(cid:48))µ(dx)µ(dx(cid:48)). Hence 

R(g) = P{g(X1  X2) (cid:54)= e1 2}  

(2)
thus achieving a comparable performance to that of the Bayes rule g∗(x1  x2) = I{η(x1  x2) > 1/2} 
whose risk is given by R∗ = E[min{η(X1  X2)  1 − η(X1  X2)}] = inf g R(g).
Remark 1 (EXTENDED FRAMEWORK) The results established in this paper can be straightforwardly
extended to a more general framework  where L = L(n) may depend on the number n of vertices.
This allows to consider a general class of models  accounting for possible accelerating properties
exhibited by various non scale-free real networks (Mattick and Gagen  2005). An asymptotic study can
be then carried out with the additional assumption that  as n → +∞  L(n) converges in distribution
to a probability measure L(∞) on X × X × {0  1}  see (Biau and Bleakley  2006). For simplicity  we
restrict our study to the stationary case  i.e. L(n) = L for all n ≥ 2.

2.2 Related Results on Empirical Risk Minimization

A paradigmatic approach in statistical learning  referred to as Empirical Risk Minimization (ERM) 
consists in replacing (2) by its empirical version based on the labeled sample Dn = {(Xi  Xj  ei j) :
1 ≤ i < j ≤ n} related to G:1

(cid:98)Rn(g) =

2

n(n − 1)

(cid:88)

I{g(Xi  Xj) (cid:54)= ei j} .

An empirical risk minimizer(cid:98)gn is a solution of the optimization problem ming∈G (cid:98)Rn(g)  where G
bias inf g∈G R(g)−R∗. The performance of(cid:98)gn is measured by its excess risk R((cid:98)gn)− inf g∈G R(g) 

is a class of reconstruction rules of controlled complexity  hopefully rich enough to yield a small

which can be bounded if we can derive probability inequalities for the maximal deviation

1≤i<j≤n

|(cid:98)Rn(g) − R(g)|.

sup
g∈G

(3)

(4)

In the framework of classiﬁcation  the ﬂagship problem of statistical learning theory  the empirical
risk is of the form of an average of i.i.d. r.v.’s  so that results pertaining to empirical process theory can
be readily used to obtain bounds for the performance of empirical error minimization. Unfortunately 
the empirical risk (3) is a sum of dependent variables. Following in the footsteps of Clémençon et al.

1A classical Lehmann-Scheffé argument shows that (3) is the estimator of (2) with smallest variance among

all unbiased estimators.

3

(2008)  the work of Biau and Bleakley (2006) circumvents this difﬁculty by means of a representation

of (cid:98)Rn(g) as an average of sums of i.i.d. r.v.’s  namely

(cid:88)

σ∈Sn

1
n!

2 (cid:99)(cid:88)

(cid:98) n

i=1

1

(cid:98)n/2(cid:99)

I{g(Xσ(i)  Xσ(i+(cid:98) n

2 (cid:99))) (cid:54)= eσ(i) σ(i+(cid:98) n

2 (cid:99))} 

where the sum is taken over all permutations of Sn  the symmetric group of order n  and (cid:98)u(cid:99) denotes
the integer part of any u ∈ R. Very similar to the ﬁrst Hoeffding decomposition for U-statistics
(see Lee  1990)  this representation reduces the ﬁrst order analysis of the concentration properties of
(4) to the study of a basic empirical process (see Biau and Bleakley  2006  Lemma 3.1). Biau and
n) for the excess of reconstruction
Bleakley (2006) thereby establish rate bounds of the order OP(1/

risk of(cid:98)gn under appropriate complexity assumptions (namely  G is of ﬁnite VC-dimension). Note

incidentally that (3) is a U-statistic only when the variable η(X1  X2) is almost-surely constant (see
Janson and Nowicki  1991  for an asymptotic study of graph reconstruction in this restrictive context).

√

Remark 2 (ALTERNATIVE LOSS FUNCTIONS) For simplicity  all our results are stated for the case
of the 0-1 loss I{g(Xi  Xj) (cid:54)= ei j}  but they straightforwardly extend to more practical alternatives
such as the convex surrogate and cost-sensitive variants used in our numerical experiments. See the
Supplementary Material for more details.

3 Empirical Reconstruction is Always Fast!

In this section  we show that the rate bounds established by Biau and Bleakley (2006) can be largely
improved without any additional assumptions. Precisely  we prove that fast learning rates of order
OP(log n/n) are always attained by the minimizers of the empirical reconstruction risk (3)  as
revealed by the following theorem.

Theorem 1 (FAST RATES) Let(cid:98)gn be any minimizer of the empirical reconstruction risk (3) over a

class G of ﬁnite VC-dimension V < +∞. For all δ ∈ (0  1)  we have w.p. at least 1 − δ: ∀n ≥ 2 

R((cid:98)gn) − R∗ ≤ 2

(cid:18)

g∈G R(g) − R∗(cid:19)

inf

+ C × V log(n/δ)

 

n

where C < +∞ is a universal constant.2
Remark 3 (ON THE BIAS TERM) Apart from its remarkable universality  Theorem 1 takes the same
form as in the case of empirical minimization of U-statistics (Clémençon et al.  2008  Corollary 6) 
with the same constant 2 in front of the bias term inf g∈G R(g) − R∗. As can be seen from the proof 
√
this constant has no special meaning and can be replaced by any constant strictly larger than 1 at the
cost of increasing the constant C. Note that the OP(1/
n) rate obtained by Biau and Bleakley (2006)
has a factor 1 in front of the bias term. Therefore  Theorem 1 provides a signiﬁcant improvement
unless the bias overly dominates the second term of the bound (i.e.  the complexity of G is too small).
Remark 4 (ON COMPLEXITY ASSUMPTIONS) We point out that a similar result can be established
under weaker complexity assumptions involving Rademacher averages (refer to the Supplementary
Material for more details). As may be seen by carefully examining the proof of Theorem 1  this would
require to use the moment inequality for degenerate U-processes stated in (Clémençon et al.  2008 
Theorem 11) instead of that proved by Arcones and Giné (1994).

In the rest of this section  we outline the main ideas used to obtain this result (the detailed proofs can
be found in the Supplementary Material). We rely on some arguments used in the fast rate analysis
for empirical minimization of U-statistics (Clémençon et al.  2008)  although these results only
hold true under restrictive distributional assumptions. Whereas the quantity (3) is not a U-statistic 
one may decompose the difference between the excess of reconstruction risk of any candidate rule
g ∈ G and its empirical counterpart as the sum of its conditional expectation given the Xi’s  which
is a U-statistic  plus a residual term. In order to explain the main argument underlying the present
analysis  additional notation is required. Set
Hg(x1  x2  e1 2) = I{g(x1  x2) (cid:54)= e1 2} and qg(x1  x2  e1 2) = Hg(x1  x2  e1 2)−Hg∗ (x1  x2  e1 2)

2Note that  throughout the paper  the constant C is not necessarily the same at each appearance.

4

qg(Xi  Xj  ei j).

(5)

for any (x1  x2  e1 2) ∈ X × X × {0  1}. Denoting by Λ(g) = R(g) − R∗ = E[qg(X1  X2  e1 2)]
the excess reconstruction risk with respect to the Bayes rule  its empirical estimate is given by

Λn(g) = (cid:98)Rn(g) − (cid:98)Rn(g∗) =

2

n(n − 1)

(cid:88)
Λn(g) − Λ(g) = Un(g) +(cid:99)Wn(g) 
(cid:88)

1≤i<j≤n

2

For all g ∈ G  one may write:

where

Un(g) = E [Λn(g) − Λ(g) | X1  . . .   Xn] =

is a U-statistic of degree 2 with symmetric kernel(cid:101)qg(X1  X2)−Λ(g)  where we denote(cid:101)qg(X1  X2) =
E[qg(X1  X2  e1 2) | X1  X2]  and(cid:99)Wn(g) = 2

(cid:101)qg(Xi  Xj) − Λ(g)
(cid:80)
i<j{qg(Xi  Xj  ei j) −(cid:101)qg(Xi  Xj)}.

n(n − 1)

1≤i<j≤n

n(n−1)

Equipped with this notation  we can now sketch the main steps of the proof of the fast rate bound
stated in Theorem 1. As shown in the Supplementary Material  it is based on Eq. (5) combined with
two intermediary results  each providing a control of one of the terms involved in it. The second order
analysis carried out by Clémençon et al. (2008) shows that the small variance property of U-statistics
may yield fast learning rates for empirical risk minimizers when U-statistics are used to estimate the
risk  under a certain “low-noise” condition (see Assumption 4 therein). One of our main ﬁndings is
that this condition is always fulﬁlled for the speciﬁc U-statistic Un(g) involved in the decomposition
(5) of the excess of reconstruction risk of any rule candidate g  as shown by the following lemma.
Lemma 2 (VARIANCE CONTROL) For any distribution L and any reconstruction rule g  we have

Var (E [qg(X1  X2  e1 2) | X1]) ≤ Λ(g).

The fundamental reason for the universal character of this result lies in the fact that the empirical
reconstruction risk is not an average over all pairs (i.e.  a U-statistic of order 2) but an average over
randomly selected pairs (random selection being ruled by the function η). The resulting smoothness
is the key ingredient allowing us to establish the desired property.
Empirical reconstruction risk minimization over a class G being equivalent to minimization of
Λn(g) − Λ(g)  the result below  combined with (5)  proves that it also boils down to minimizing
Un(g) under appropriate conditions on G  so that the fast rate analysis of Clémençon et al. (2008) can
be extended to graph reconstruction.
Lemma 3 (UNIFORM APPROXIMATION) Under the same assumptions as in Theorem 1  for any
δ ∈ (0  1)  we have with probability larger than 1 − δ: ∀n ≥ 2 

(cid:12)(cid:12)(cid:12)(cid:99)Wn(g)

(cid:12)(cid:12)(cid:12) ≤ C × V log(n/δ)

sup
g∈G

 

n

where C < +∞ is a universal constant.
The proof relies on classical symmetrization and randomization tricks combined with the decou-
pling method  in order to cope with the dependence structure of the variables and apply maxi-
mal/concentration inequalities for sums of independent random variables (see De la Pena and Giné 
1999).
Based on the above results  Theorem 1 can then be derived by relying on the second Hoeffding
decomposition (see Hoeffding  1948). This allows us to write Un(g) as a leading term taking the form
of a sum of i.i.d r.v.’s with variance 4V ar(E[qg(X1  X2  e1 2) | X1])  plus a degenerate U-statistic
(i.e.  a U-statistic of symmetric kernel h(x1  x2) such that E[h(x1  X2)] = 0 for all x1 ∈ X ). The
latter can be shown to be of order OP(1/n) uniformly over the class G by means of concentration
results for degenerate U-processes.
We conclude this section by observing that  instead of estimating the reconstruction risk by (3)  one
could split the training dataset into two halves and consider the unbiased estimate of (2) given by

I{g(Xi  Xi+(cid:98)n/2(cid:99)) (cid:54)= ei i+(cid:98)n/2(cid:99)}.

(6)

(cid:98)n/2(cid:99)(cid:88)

1

(cid:98)n/2(cid:99)

i=1

5

The analysis of the generalization ability of minimizers of this empirical risk functional is simpler 
insofar as only independent r.v.’s are involved in the sum (6). However  this estimate does not share
the reduced variance property of (3) and although one could show that rate bounds of the same
order as those stated in Theorem 1 may be attained by means of results pertaining to ERM theory
for binary classiﬁcation (see e.g. Section 5 in Boucheron et al.  2005)  this would require a very
restrictive assumption on distribution L  namely to suppose that the posterior preferential attachment
probability η stays bounded away from 1/2 with probability one (cf Massart and Nédélec  2006).
This is illustrated in the Supplementary Material.

4 Scaling-up Empirical Risk Minimization

The results of the previous section  as well as those of Biau and Bleakley (2006)  characterize the

excess risk achieved by minimizers of the empirical reconstruction risk (cid:98)Rn(g) but do not consider the
merely computing (cid:98)Rn(g) is prohibitive as the number of terms involved in the summation is O(n2).

computational complexity of ﬁnding such minimizers. For large training graphs  the complexity of

In this section  we introduce a sampling-based approach to build approximations of the reconstruction
risk with much fewer terms than O(n2)  so as to scale-up risk minimization to large graphs.
The strategy we propose  inspired from the notion of incomplete U-statistic (see Lee  1990)  is of
disarming simplicity: instead of the empirical reconstruction risk (3)  we will consider an incomplete
approximation obtained by sampling pairs of vertices (and not vertices) with replacement. Formally 
we deﬁne the incomplete graph reconstruction risk based on B ≥ 1 pairs of vertices as

I{g(Xi  Xj) (cid:54)= ei j}  

(7)

(cid:101)RB(g) =

1
B

(cid:88)

(i j)∈PB

where PB is a set of cardinality B built by sampling with replacement in the set Θn = {(i  j) :
1 ≤ i < j ≤ n} of all pairs of vertices of the training graph G. For any b ∈ {1 
. . .   B}
and all (i  j) ∈ Θn  denote by b(i  j) the variable indicating whether the pair (i  j) has been
picked at the b-th draw (b(i  j) = +1) or not (b(i  j) = +0). The (multinomial) random vectors
b(i  j) = +1 for 1 ≤ b ≤ B) and the

b = (b(i  j))(i j)∈Θn are i.i.d. (notice that(cid:80)
(cid:88)

incomplete risk can be then rewritten as

B(cid:88)

(i j)∈Θn

b(i  j) · I{g(Xi  Xj) (cid:54)= ei j} .

(cid:101)RB(g) =

(8)

1
B

b=1

(i j)∈Θn

Var

(cid:16)

= Var

(cid:16)(cid:98)R1(g)

(cid:16)(cid:98)Rn(g)
(cid:17)

(cid:16)(cid:101)RB(g)
(cid:17)

Observe that the statistic (7) is an unbiased estimate of the true risk (2) and that  given the Xi’s 
its conditional expectation is equal to (3). Considering (7) with B = o(n2) as our empirical risk
estimate signiﬁcantly reduces the computational cost  at the price of a slightly increased variance:

for any reconstruction rule g. Note in particular that the above variance is in general much smaller
B(cid:99) vertices drawn at random

(cid:17) − Var
than that of the complete reconstruction risk based on a subsample of (cid:98)√
We are thus interested in characterizing the performance of solutions(cid:101)gB to the computationally
simpler problem ming∈G (cid:101)RB(g). The following theorem shows that  when the class G is of ﬁnite VC-
dimension  the concentration properties of the incomplete reconstruction risk process {(cid:101)RB(g)}g∈G
can be deduced from those of the complete version {(cid:98)Rn(g)}g∈G.

(thus involving O(B) pairs as well). We refer to the Supplementary Material for more details.

(cid:16)(cid:98)Rn(g)
(cid:17)(cid:17)

+

1
B

Var

 

Theorem 4 (UNIFORM DEVIATIONS) Suppose that the class G is of ﬁnite VC-dimension V < +∞.
For all δ > 0  n ≥ 1 and B ≥ 1  we have with probability at least 1 − δ: 

|(cid:101)RB(g) − (cid:98)Rn(g)| ≤

sup
g∈G

(cid:114)

log 2 + V log ((1 + n(n − 1)/2)/δ)

2B

.

The ﬁnite VC-dimension hypothesis can be relaxed and a bound of the same order can be proved
to hold true under weaker complexity assumptions involving Rademacher averages (see Remark 4).

6

(a) True graph

(b) Graph with scrambled features

(c) Reconstructed graph

Figure 1: Illustrative experiment with n = 50  q = 2  τ = 0.27 and p = 0. Figure 1(a) shows
the training graph  where the position of each node is given by its 2D feature vector. Figure 1(b)
depicts the same graph after applying a random transformation R to the features. On this graph  the
Euclidean distance with optimal threshold achieves a reconstruction error of 0.1311. In contrast  the
reconstruction rule learned from B = 100 pairs of nodes (out of 1225 possible pairs) successfully
inverts R and accurately recovers the original graph (Figure 1(c)). Its reconstruction error is 0.008 on
the training graph and 0.009 on a held-out graph generated with the same parameters.

Remarkably  with only B = O(n) pairs  the rate in Theorem 4 is of the same order (up to a log factor)

as that obtained by Biau and Bleakley (2006) for the maximal deviation supg∈G |(cid:98)Rn(g) − R(g)|
related to the complete reconstruction risk (cid:98)Rn(g) with O(n2) pairs. From Theorem 4  one can get a

√
n) for the minimizer of the incomplete risk involving only O(n) pairs.
learning rate of order OP(1/
Unfortunately  such an analysis does not exploit the relationship between conditional variance
and expectation formulated in Lemma 2  and is thus not sufﬁcient to show that reconstruction rules
minimizing the incomplete risk (7) can achieve learning rates comparable to those stated in Theorem 1.
In contrast  the next theorem provides sharper statistical guarantees. We refer to the Supplementary
Material for the proof.

Theorem 5 Let(cid:101)gB be any minimizer of the incomplete reconstruction risk (7) over a class G of ﬁnite

VC-dimension V < +∞. Then  for all δ ∈ (0  1)  we have with probability at least 1 − δ: ∀n ≥ 2 

R((cid:101)gB) − R∗ ≤ 2

(cid:18)

g∈G R(g) − R∗(cid:19)

inf

(cid:18) 1

(cid:19)

 

+ CV log(n/δ) ×

+

n

1√
B

where C < +∞ is a universal constant.
This bound reveals that the number B ≥ 1 of pairs of vertices plays the role of a tuning parameter 
ruling a trade-off between statistical accuracy (taking B(n) = O(n2) fully preserves the convergence
rate) and computational complexity. This will be conﬁrmed numerically in Section 5.
The above results can be extended to other sampling techniques  such as Bernoulli sampling and
sampling without replacement. We refer to the Supplementary Material for details.

5 Numerical Experiments

In this section  we present some numerical experiments on large-scale graph reconstruction to
illustrate the practical relevance of the idea of incomplete risk introduced in Section 4. Following a
well-established line of work (Vert and Yamanishi  2004; Vert et al.  2007; Shaw et al.  2011)  we
formulate graph reconstruction as a distance metric learning problem (Bellet et al.  2015): we learn
a distance function such that we predict an edge between two nodes if the distance between their
features is smaller than some threshold. Assuming X ⊆ Rq  let Sq
+ be the cone of symmetric PSD
q × q real-valued matrices. The reconstruction rules we consider are parameterized by M ∈ Sq
+ and
have the form

gM (x1  x2) = I{DM (x1  x2) ≤ 1}  

where DM (x1  x2) = (x1 − x2)T M (x1 − x2) is a (pseudo) distance equivalent to the Euclidean
distance after a linear transformation L ∈ Rq×q  with M = LT L. Note that gM (x1  x2) can be
seen as a linear separator operating on the pairwise representation vec((x1 − x2)(x1 − x(cid:48)
2)T ) ∈ Rq2 

7

Table 1: Results (averaged over 10 runs) on synthetic graph with n = 1  000  000  q = 100  p = 0.05.

Reconstruction error
Relative improvement
Training time (seconds)

–
21

B = 0.01n B = 0.1n B = n B = 5n B = 10n
0.1159

0.1185

0.2272

0.1543
32%
398

0.1276
17%
5 705

7%

20 815

2%

42 574

hence the class of learning rules we consider has VC-dimension bounded by q2 + 1. We deﬁne the

reconstruction risk as:(cid:98)Sn(gM ) =

(cid:88)
[(2ei j − 1)(DM (Xi  Xj) − 1)]+  

2

n(n − 1)

i<j

where [·]+ = max(0 ·) is a convex surrogate for the 0-1 loss. In earlier work  ERM has only been
applied to graphs with at most a few hundred or thousand nodes due to scalability issues. Thanks to
our results  we are able to scale it up to much larger networks by sampling pairs of nodes and solve
the resulting simpler optimization problem.
∈ Rq sampled
We create a synthetic graph with n nodes as follows. Each node i has features X true
uniformly over [0  1]. We then add an edge between nodes that are at Euclidean distance smaller than
some threshold τ  and introduce some noise by ﬂipping the value of ei j for each pair of nodes (i  j)
independently with probability p. We then apply a random linear transformation R ∈ Rq×q to each
node to generate a “scrambled” version Xi = RX true
of the nodes’ features. The learning algorithm
is only allowed to observe the scrambled features and must ﬁnd a rule which accurately recovers the
graph by solving the ERM problem above. Note that  denoting Dij = (cid:107)R−1Xi − R−1Xj(cid:107)2  the
posterior preferential attachment probability is given by

i

i

η (Xi  Xj) = (1 − p) · I{Dij ≤ τ} + p · I{Dij > τ}.

The process is illustrated in Figure 1. Using this procedure  we generate a training graph with
n = 1  000  000 and q = 100. We set the threshold τ such that there is an edge between about 20%
of the node pairs  and set p = 0.05. We also generate a test graph using the same parameters. We
then sample uniformly with replacement B pairs of nodes from the training graph to construct our
incomplete reconstruction risk. The reconstruction error of the resulting empirical risk minimizer
is estimated on 1 000 000 pairs of nodes drawn from the test graph. Table 1 shows the test error
(averaged over 10 runs) as well as the training time for several values of B. Consistently with our
theoretical ﬁndings  B implements a trade-off between statistical accuracy and computational cost.
For this dataset  sampling B = 5  000  000 pairs (out of 1012 possible pairs!) is sufﬁcient to ﬁnd an
accurate reconstruction rule. A larger B would result in increased training time for negligible gains
in reconstruction error.
Additional results. In the Supplementary Material  we present comparisons to a node sampling
scheme and to the “dataset splitting” strategy given by (6)  as well as experiments on a real network.

6 Conclusion

In this paper  we proved that the learning rates for ERM in the graph reconstruction problem are
always of order OP(log n/n). We also showed how sampling schemes applied to the population
of edges (not nodes) can be used to scale-up such ERM-based predictive methods to very large
graphs by means of a detailed rate bound analysis  further supported by empirical results. A ﬁrst
possible extension of this work would naturally consist in considering more general sampling designs 
such as Poisson sampling (which generalizes Bernoulli sampling) used in graph sparsiﬁcation (cf
Spielman  2005)  and investigating the properties of minimizers of Horvitz-Thompson versions of the
reconstruction risk (see Horvitz and Thompson  1951). Another challenging line of future research is
to extend the results of this paper to more complex unconditional graph structures in order to account
for properties shared by some real-world graphs (e.g.  graphs with a power law degree distribution).

Acknowledgments This work was partially supported by the chair “Machine Learning for Big
Data” of Télécom ParisTech and by a grant from CPER Nord-Pas de Calais/FEDER DATA Advanced
data science and technologies 2015-2020.

8

References
Agarwal  S. (2014). Surrogate regret bounds for bipartite ranking via strongly proper losses. JMLR  15:1653–

1674.

Antos  A.  Györﬁ  L.  and György  A. (2005). Individual convergence rates in empirical vector quantizer design.

IEEE Transactions on Information Theory  51(11):4013–4023.

Arcones  M. and Giné  E. (1994). U-processes indexed by Vapnik-Chervonenkis classes of functions with
applications to asymptotics and bootstrap of U-statistics with estimated parameters. Stochastic Processes and
their Applications  52:17–38.

Bellet  A.  Habrard  A.  and Sebban  M. (2015). Metric Learning. Morgan & Claypool Publishers.
Biau  G. and Bleakley  K. (2006). Statistical Inference on Graphs. Statistics & Decisions  24:209–232.
Boucheron  S.  Bousquet  O.  Lugosi  G.  and Massart  P. (2005). Moment inequalities for functions of

independent random variables. Ann. Stat.  33(2):514–560.

Clémençon  S. (2014). A statistical view of clustering performance through the theory of U-processes. Journal

of Multivariate Analysis  124:42 – 56.

Clémençon  S. and Robbiano  S. (2011). Minimax learning rates for bipartite ranking and plug-in rules. In

ICML.

Clémençon  S. and Vayatis  N. (2010). Overlaying classiﬁers: a practical approach to optimal scoring. Construc-

tive Approximation  32(3):619–648.

Clémençon  S.  Lugosi  G.  and Vayatis  N. (2008). Ranking and Empirical Minimization of U-statistics. Ann.

Stat.  36(2):844–874.

Cukierski  W.  Hamner  B.  and Yang  B. (2011). Graph-based features for supervised link prediction. In IJCNN.
De la Pena  V. and Giné  E. (1999). Decoupling : from dependence to independence. Springer.
Hoeffding  W. (1948). A class of statistics with asymptotically normal distribution. The Annals of Mathematical

Statistics  19:293–325.

Horvitz  D. and Thompson  D. (1951). A generalization of sampling without replacement from a ﬁnite universe.

Journal of the American Statistical Association  47:663–685.

Jansen  R.  Yu  H.  Greenbaum  D.  Kluger  Y.  Krogan  N.  Chung  S.  Emili  A.  Snyder  M.  Greenblatt  J.  and
Gerstein  M. (2003). A Bayesian networks approach for predicting protein-protein interactions from genomic
data. Science  302(5644):449–453.

Janson  S. and Nowicki  K. (1991). The asymptotic distributions of generalized U-statistics with applications to

random graphs. Probability Theory and Related Fields  90:341–375.

Kanehisa  M. (2001). Prediction of higher order functional networks from genomic data. Pharmacogenomics 

2(4):373–385.

Lee  A. J. (1990). U-statistics: Theory and practice. Marcel Dekker  Inc.  New York.
Liben-Nowell  D. and Kleinberg  J. (2003). The link prediction problem for social networks. In CIKM.
Lichtenwalter  R.  Lussier  J.  and Chawla  N. (2010). New perspectives and methods in link prediction. In KDD.
Mammen  E. and Tsybakov  A. (1999). Smooth discrimination analysis. Ann. Stat.  27(6):1808–1829.
Massart  P. and Nédélec  E. (2006). Risk bounds for statistical learning. Ann. Stat.  34(5).
Mattick  J. and Gagen  M. (2005). Accelerating networks. Science  307(5711):856–858.
Rigollet  P. and Vert  R. (2009). Fast rates for plug-in estimators of density level sets. Bernoulli  14(4):1154–1178.
Shaw  B.  Huang  B.  and Jebara  T. (2011). Learning a Distance Metric from a Network. In NIPS.
Spielman  D. (2005). Fast Randomized Algorithms for Partitioning  Sparsiﬁcation  and Solving Linear Systems.

Lecture notes from IPCO Summer School 2005.

Tsybakov  A. (2004). Optimal aggregation of classiﬁers in statistical learning. Ann. Stat.  32(1):135–166.
Vert  J.-P.  Qiu  J.  and Noble  W. S. (2007). A new pairwise kernel for biological network inference with support

vector machines. BMC Bioinformatics  8(10).

Vert  J.-P. and Yamanishi  Y. (2004). Supervised graph inference. In NIPS  pages 1433–1440.

9

,Parikshit Ram
Alexander Gray
Guillaume Papa
Aurélien Bellet
Stephan Clémençon
Xiaoxiao Guo
Hui Wu
Yu Cheng
Steven Rennie
Gerald Tesauro
Rogerio Feris
Xuanyi Dong
Yi Yang