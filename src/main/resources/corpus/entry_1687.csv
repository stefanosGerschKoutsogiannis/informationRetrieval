2010,Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks,When animals repeatedly choose actions from multiple alternatives  they can allocate their choices stochastically depending on past actions and outcomes. It is commonly assumed that this ability is achieved by modifications in synaptic weights related to decision making. Choice behavior has been empirically found to follow Herrnstein‚Äôs matching law. Loewenstein &amp; Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities. However  their proof did not take into account the change in entire synaptic distributions. In this study  we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufficiently strong so that the fluctuations in input from individual sensory neurons influence the net input to output neurons. This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. This effect causes an undermatching phenomenon  which has been observed in many behavioral experiments. We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.,Effects of Synaptic Weight Diffusion on Learning in

Decision Making Networks

Kentaro Katahira1;2;3  Kazuo Okanoya1;3 and Masato Okada1;2;3

1ERATO Okanoya Emotional Information Project  Japan Science Technology Agency

2Graduate School of Frontier Sciences  The University of Tokyo  Kashiwa  Chiba 277-8561  Japan

3RIKEN Brain Science Institute  Wako  Saitama 351-0198  Japan

katahira@mns.k.u-tokyo.ac.jp okanoya@brain.riken.jp

okada@k.u-tokyo.ac.jp

Abstract

When animals repeatedly choose actions from multiple alternatives  they can al-
locate their choices stochastically depending on past actions and outcomes.
It
is commonly assumed that this ability is achieved by modiÔ¨Åcations in synaptic
weights related to decision making. Choice behavior has been empirically found
to follow Herrnstein‚Äôs matching law. Loewenstein & Seung (2006) demonstrated
that matching behavior is a steady state of learning in neural networks if the synap-
tic weights change proportionally to the covariance between reward and neural ac-
tivities. However  their proof did not take into account the change in entire synap-
tic distributions. In this study  we show that matching behavior is not necessarily
a steady state of the covariance-based learning rule when the synaptic strength is
sufÔ¨Åciently strong so that the Ô¨Çuctuations in input from individual sensory neu-
rons inÔ¨Çuence the net input to output neurons. This is caused by the increasing
variance in the input potential due to the diffusion of synaptic weights. This effect
causes an undermatching phenomenon  which has been observed in many behav-
ioral experiments. We suggest that the synaptic diffusion effects provide a robust
neural mechanism for stochastic choice behavior.

1 Introduction

Decision making has often been studied in experiments in which a subject repeatedly chooses actions
and rewards are given depending on the action. The choice behavior of subjects in such experiments
is known to obey Herrnstein‚Äôs matching law [1]. This law states that the proportional allocation of
choices matches the relative reinforcement obtained from those choices. The neural correlates of
matching behavior have been investigated [2] and the computational models that explain them have
been developed [3  4  5  6  7](cid:637)
Previous studies have shown that the learning rule in which the weight update is made proportionally
to the covariance between reward and neural activities lead to matching behavior (we simply refer
to this learning rule as the covariance rule) [3  7]. In this study  by means of a statistical mechanical
approach [8  9  10  11]  we analyze the properties of the covariance rule in a limit where the num-
ber of plastic synapses is inÔ¨Ånite . We demonstrate that matching behavior is not a steady state of
the covariance rule under three conditions: (1) learning is achieved through the modiÔ¨Åcation of the
synaptic weights from sensory neurons to the value-encoding neurons; (2) individual Ô¨Çuctuations in
sensory input neurons are so large that they can affect the potential of value-coding neurons (possi-
bly via sufÔ¨Åciently strong synapses); (3) the number of plastic synapses that are involved in learning
is large. This result is caused by the diffusion of synaptic weights. The term ‚Äúdiffusion‚Äù refers
to a phenomenon where the distributions over the population of synaptic weights broadens. This
diffusion increases the variance in the potential of output units since the broader synaptic weight
distributions are  the more they amplify Ô¨Çuctuations in individual inputs. This makes the choice

1

behavior of the network more random and moves the probabilities of choosing alternatives to equal
probabilities  than that predicted by the matching law. This outcome corresponds to the under-
matching phenomenon  which has been observed in behavioral experiments.

Our results suggest that when we discuss the learning processes in a decision making network  it may
be insufÔ¨Åcient to only consider a steady state for individual weight updates  and we should therefore
consider the dynamics of the weight distribution and the network architecture. This proceeding is a
short version of our original paper [12]  with the model modiÔ¨Åed and new results included.

2 Matching Law
First  let us formulate the matching law. We will consider a case with two alternatives (each denoted
as A and B)  which has generally been studied in animal experiments. Here  we consider stochastic
choice behavior  where at each time step  a subject chooses alternative a with probability pa. We
‚àë
denote the reward as r. For the sake of simplicity  we restrict r to a binary variable: r = 0 represents
the absence of a reward  and r = 1 means that a reward is given. The expected return  ‚ü®r|a‚ü©  refers
‚àë
‚àë
to the average reward per choice a  and the income  Ia  refers to the total amount of reward resulting
na
from the choice a and Ia/ (
a‚Ä≤ Ia‚Ä≤) is a fractional income from choice a. For a large number of
a‚Ä≤ ‚ü®r|a
‚Ä≤‚ü©pa‚Ä≤ is an average reward per trial over possible choice
trials  this equals ‚ü®r|a‚ü©pa. ‚ü®r‚ü© =
a‚Ä≤ Ia‚Ä≤) = pa for all a with pa Ã∏= 0. For a large
behavior. The matching law states that Ia/ (
‚ü®r|a‚ü©pa
number of trials  the fraction of income from an alternative a is expressed as
‚ü®r‚ü©
Then  the matching law states that this quantity equals pa for all a. To make this hold  it should
satisfy
(1)
if pA Ã∏= 0 and pB Ã∏= 0. Note that ‚ü®r|a‚ü© is the average reward given the current choice  and this is
a function of the past choice. Equation 1 is a condition for the matching law  and we will often use
this identity.

‚ü®r|A‚ü© = ‚ü®r|B‚ü© = ‚ü®r‚ü© 

‚ü®r|a‚ü©pa
a‚Ä≤‚ü®r|a‚Ä≤‚ü©pa‚Ä≤ =

‚àë

na

na

3 Model
Decision Making Network: The decision making network we study consists of sensory-input neu-
rons and output neurons that represent the subjective value of each alternative (we call the output
neurons value-encoding neurons). The network is divided into two groups (A and B)  which par-
ticipate in choosing each alternative. Sensory cues from both targets are given simultaneously via
N ) 1 Each component of input
the N-neuron population  xA = (xA
vectors xA and xB independently obeys a gaussian distribution with mean X0 and variance one
(these quantities can be spike counts during stimulus presentation).
The choice is made in such a way that alternative a is chosen if the potential of output unit ua 
which will be speciÔ¨Åed below  is higher than that of the other alternative. Although we do not model
this comparison process explicitly  it can be carried out via a winner-take-all competition mediated
by feedback inhibition  as has been commonly assumed in decision making networks [3  13]. In
this competition  the ‚Äúwinner‚Äù group gains a high Ô¨Åring rate while the ‚Äúloser‚Äù enters a low Ô¨Åring
state [13]. Let yA and yB denote the Ô¨Ånal output of an output neuron after competition and this is
determined as

N ) and xB = (xB

1   ...  xB

1   ...  xA

yA = 1  yB = 0 
yA = 0  yB = 1 

if uA ‚â• uB 
if uA < uB.

With the synaptic efÔ¨Åcacies (or weights) J A = (J A
to the output units are given by

1   ...  J A

N ) and J B = (J B

1   ...  J B

N )  the net input

N‚àë

ha =

J a
i xa

i   a = A  B.

(2)

1This assumption might be the case when the sensory input for each alternative is completely different  e.g. 
in position  and in color such as those in Sugrue et al.‚Äôs experiment [2]. The case that output neurons share the
inputs from sensory neurons are analyzed in [12].

i=1

2

‚àö
i is scaled as O(1/

N). This means that the mean of ha is O(

N)  thus diverges
We assume that J a
for large N  while the variance is kept of order unity. This is a key assumption of our models. If J a
i
is scaled as O(1/N) instead  the individual Ô¨Çuctuations in xa
i are averaged out. It has been shown
that the mean of the potential are kept of order unity while Ô¨Çuctuations in external sources (xa
i ) that
are of order unity affect the potential in output neuron  under the condition that recurrent inputs
from inhibitory interneurons  excitatory recurrent inputs  and input from external sources (xa
i ) are
balanced [14]. We do not explicitly model this recurrent balancing mechanism  but phenomenolog-
ically incorporate it as follows.

‚àö

Using the order parameters

i=1

J a
i  

1‚àö
N

N X0 (cid:22)Ja  l2

la = ||J a||  (cid:22)Ja =
(3)
‚àö
a) where N (¬µ  œÉ2) denotes the gaussian distribution with mean ¬µ
we Ô¨Ånd ha ‚àº N (
and variance œÉ2. We assume ua obeys a gaussian distribution of mean Caua/
N  and variance
CaVar[ua] + œÉ2
p are constants
that are determined according to the speciÔ¨Åc model architecture of reccurent network  but we set
‚àö
CA = CB = 1 since they do not affect the qualitative properties of the model. Then  ua is computed
as ua = ha ‚àí (cid:22)harec + œÉpœµ with (cid:22)harec = (1 ‚àí 1/
N X0 (cid:22)Ja and œµ is a
N)E[ha] where E[ha] =
gaussian random variable with unit mean and unit variance. Then  ua obey the independent Gaussian
distributions whose means and variances are respectively given by (cid:22)Ja and l2
p. From this  the
probability that the network will choose alternative A can be described as

p due to the reccurent balancing mechanism [14]. CA  CB and œÉ2

a + œÉ2

‚àö

‚àö

N‚àë

Ô£±Ô£≤Ô£≥‚àí X0( (cid:22)JA ‚àí (cid:22)JB)
‚àö

pA =

erfc

1
2

Ô£ºÔ£ΩÔ£æ .
‚à´ ‚àû

(4)

2(l2

A + l2

B + 2œÉ2
p)
where erfc(¬∑) is the complementary error function  erfc(x) = 2‚àö
dt. This expression is
in a closed form of the order parameters. Thus  if we can describe the evolution of these order
parameters  we can completely describe how the behavior of the model changes as a consequence of
learning. In the following  we will often use an additional order parameter  the variance of weight 
œÉ2
a. This parameter is more convenient for gaining insights into the evolution of the weight than
the weight norm  la. The diffusion of weight distributions is reÔ¨Çected by increases in œÉ2
a  i.e.  the
differences between the growth of the second order moment of weight distribution l2
a and that of the
square of its mean (cid:22)J 2
a.
Learning Rules: We consider following two learning rules that belong to the class of the covariance
learning rule:

x e

‚àít2

(cid:25)

Reward-modulated (RM) Hebb rule:

i (t + 1) = J a
J a

i (t) + Œ∑
N

[r(t) ‚àí (cid:22)r(t)] ya(t)(xa

i (t) ‚àí cx) 

(5)

Delta rule:

i (t) + Œ∑
N

[r(t) ‚àí (cid:22)r(t)] (xa

i (t) ‚àí cx) 

i (t + 1) = J a
J a

‚àí cx) for RM-Hebb rule  and xa

(6)
where Œ∑ is the learning rate  (cid:22)¬∑ denotes the expected value and cx is a constant. The expectation of
these updates is proportional to covariance between the reward  r  and a measure of neural activity
‚àí cx for the delta rule). Variants of the RM-Hebb rule
(ya(xa
i
have recently been studied intensively [4  15  16  17  18  19  20]. The delta rule has been used as
an example of the covariance rule [3  7] and has also been used for the learning rule in the model of
‚àö
perceptual learning [21]. The expected reward  (cid:22)r  can be estimated  e.g.  with an exponential kernel
such as (cid:22)r(t + 1) = (1 ‚àí Œ≥)r(t) + Œ≥(cid:22)r(t) with a constant Œ≥. We assume that cx = (1 ‚àí 1/
N)X0 to
simplify the following analysis 2.
‚àë

2From this assumption  this model can be transformed into a simple mathematical equivalent form that the
N ; 1) and the potential in output is replaced with ua =

p
i is replaced with N (X0=

distribution of input xa

i

N

i=1 J a

i xa

i + (cid:27)p(cid:24)a  where (cid:24)a (cid:24) N (0; 1).

3

4 Macroscopic Description of Learning Processes

Here  following the statistical mechanical analysis of on-line learning [8  9  10  11]  we derive
equations that describe the evolution of the order parameters. To do this  we Ô¨Årst rewrite the learning
rule in a vector form:

Fa (xa ‚àí cx) 

1
N

‚àë

‚àë

J a(t + 1) = J a(t) +

(7)
where for the RM-Hebb rule  Fa = Œ∑(rt‚àí (cid:22)rt)ya(cid:636)and for the delta rule  Fa = Œ∑(rt‚àí (cid:22)rt). Taking the
N Fa(t)2 +
square norm of each side of equation 7  we obtain la(t + 1)2 = la(t)2 + 2
‚àí cx). Summing up over all components
O(1/N 2)  where we have deÔ¨Åned ~ha =
on both sides of equation 7  we obtain (cid:22)Ja(t + 1) = (cid:22)Ja(t) + 1
N Fa(t)~xa  where we have deÔ¨Åned
‚àí cx). In both these equations  the magnitude of each update is of order 1/N.
~xa =
Hence  to change the order parameters of order one  O(N) updates are needed. Within this short
period that spans the O(N) updates  the weight change in O(1/N) can be neglected  and the self-
averaging property holds. By using this property and introducing continuous ‚Äútime‚Äù scaled by N 
i.e.  Œ± = t/N  the evolutions of the order parameters obey ordinary differential equations:

N Fa(t) ~ha + 1

i=1(xa

N
i=1 J a

i (xa

N

i

i

(8)
where ‚ü®¬∑‚ü© denotes the ensemble average over all possible inputs and arrivals of rewards. The speciÔ¨Åc
form of the ensemble averages are obtained for reward-dependent Hebbian learning as

a

= 2‚ü®Fa

dl2
a
dŒ±

‚ü© 

d (cid:22)Ja
dŒ±

~ha‚ü© + ‚ü®F 2
{

= ‚ü®Fa ~xa‚ü© 
}

‚ü© = Œ∑2pa

~ha‚ü© = Œ∑ pa {‚ü®r|a‚ü© ‚àí ‚ü®r‚ü©}‚ü®~ha|a‚ü© 
‚ü®Fa
‚ü®F 2
‚ü®Fa ~xa‚ü© = Œ∑ pa {‚ü®r|a‚ü© ‚àí ‚ü®r‚ü©}‚ü®~xa|a‚ü© 
{

(1 ‚àí 2‚ü®r‚ü©)‚ü®r|a‚ü© + (‚ü®r‚ü©)2

a

 

}

 

‚Ä≤‚ü©)‚ü®~ha|a‚ü© + (‚ü®r|a

‚Ä≤‚ü© ‚àí ‚ü®r‚ü©) (cid:22)Ja

‚Ä≤‚ü©)‚ü®~xa|a‚ü© + ‚ü®r|a

‚Ä≤‚ü© ‚àí ‚ü®r‚ü©} .

and for the delta rule 
‚ü®Fa
~ha‚ü© = Œ∑
‚ü®F 2

pa(‚ü®r|a‚ü© ‚àí ‚ü®r|a
‚ü© = Œ∑2 {‚ü®r‚ü©(1 ‚àí ‚ü®r‚ü©)}  
‚ü®Fa ~xa‚ü© = Œ∑ { pa (‚ü®r|a‚ü© ‚àí ‚ü®r|a
)

(

a

The conditional averages ‚ü®~ha|a‚ü© and ‚ü®~xa|a‚ü© in these equations are computed as
‚ü®~ha|a‚ü© = (cid:22)JaX0 +

  ‚ü®~xa|a‚ü© = X0 +

exp

‚àí X 2
0 D2
(cid:22)J
2L2

(cid:22)Ja
‚àö
2œÄL2

pa

exp

(

)

 

‚àí X 2
0 D2
(cid:22)J
2L2

pa

‚àö
l2
a
2œÄL2

‚àö

B + 2œÉ2
where we have deÔ¨Åned L =
given in the supplementary material and [12].

A + l2
l2

(9)
p and D (cid:22)J = (cid:22)JB ‚àí (cid:22)JA. The details on the derivation are

Next  we consider weight normalization in which the total length of the weight vector is kept con-
stant. We adopted this weight normalization because of analytical convenience rather than taking
biological realism into account. Other weight constraints would produce no clear differences in
the following results. SpeciÔ¨Åcally  we constrained the norm of the weight as ||J||2 = 2  where
J = (J A
B = 2. This is achieved by
modifying the learning rule in the following way [22]:
N Fa xa)

N ). This is equivalent to keeping l2

1   ...  J B

1   ...  J A

A + l2

N   J B

J a(t + 1) =

‚àö
= J a(t) + Fa xa
1 + F/N

 

‚àö
‚àö
2(J a(t) + 1
||J A(t) + 1
N FA xA||2 + ||J B(t) + 1
A + F 2
2(F 2

B)  provided that ||J||2 = 2 holds at trial t. Expanding the
with F ‚â° FAuA + FBuB + 1
right-hand side to Ô¨Årst order in 1/N  we can obtain the differential equations similarly to Equation 8:

N FB xB||2

= 2‚ü®Fa

dl2
a
dŒ±

With ‚ü®F‚ü© = ‚ü®FAuA‚ü© + ‚ü®FBuB‚ü© + 1
when l2

A + l2

B = 2; thus  the length of the weight is kept constant.

~ha‚ü© + ‚ü®F 2
2(‚ü®F 2

a

A

d (cid:22)Ja
dŒ±

‚ü© ‚àí ‚ü®F‚ü©l2
= ‚ü®Fa ~xa‚ü© ‚àí 1
‚ü®F‚ü© (cid:22)Ja.
a 
2
‚ü© + ‚ü®F 2
‚ü©)  we can Ô¨Ånd that d(l2
A + l2

B

B)/dŒ± becomes zero

(10)

(11)

4

Figure 1: Evolution of choice probability and order parameters for RM-Hebb rules (A  B  E  F)
and delta rule (C  D  G  H)  without weight normalization (A-D) and with normalization (E-H).
Parameters were X0 = 2  Œ∑ = 0.1 and œÉp = 1  and the reward schedule was a VI schedule (see
main text) with ŒªA = 0.2  ŒªB = 0.1. Lines represent results of theory and symbols plot mean
of ten trials with computer simulation. Simulations were done for N = 1  000. Error bars indicate
standard deviation (s.d.). Error bars are almost invisible for choice probability since s.d.
is very
small.

) is pmatch

5 Results
To demonstrate the behavior of the model  we used a time-discrete version of a variable-interval
(VI) reward schedule  which is commonly used for studying the matching law. In a VI schedule  a
reward is assigned to two alternatives stochastically and independently  with a constant probability 
Œªa for alternative a (a = A  B). The reward remains until it is harvested by choosing the alternative.
Here  we use ŒªA = 0.2  ŒªB = 0.1. For this task setting  the choice probability that yields matching
behavior (denoted as pmatch
= 0.6923. Figure 1(A-D) plots the evolution of choice
probability and order parameters in two learning rules without a weight normalization constraint.
The lines represent the results for theory and the symbols plot the results for simulations. The results
for theory agree well with those for the computer simulations (N = 1  000)  indicating the validity of
our theory. We can see that the choice probability approaches a value that yields matching behavior
)  while the order parameters (cid:22)Ja and œÉa continue to change without becoming saturated.
(pmatch
The weight standard deviation  œÉa  always increases (the synaptic weight diffusion).
Figure 1(E-H) plots the results with weight normalization. Again  the results for theory agree well
with those for computer simulations. For the RM-Hebb rule  the choice probability saturates at a
value below pmatch
  but without
reaching pmatch
. It then returns to the uniform choice probability (pA = 0.5) due to its larger
diffusion effect than that of the RM-Hebb rule.

. For the delta rule  the choice probability Ô¨Årst approaches pmatch

A

A

A

A

A

A

5.1 Matching Behavior Is Not Necessarily Steady State of Learning

From Figure 1  the choice probability seems to asymptotically approach matching behavior for the
case without wight normalization. However  matching behavior is not necessarily a steady state
and then
of learning.
Equations 8 and 11 are numerically solved. We see that pA does not remain at pmatch
but changes
toward the uniform choice (pA = 0.5) for both learning rules. Then  for the RM-Hebb rule  pA
evolves toward pmatch
  but not do so for the delta rule. To understand the mechanism for this

In Figure 2  the order parameters are initialized so that pA(0) = pmatch

A

A

A

5

 "$& "$& "$& !"#$% !"# "$& !"# !"#$%=J?DEC=J?DEC)*+ H@AHF=H=AJAHIH@AHF=H=AJAHIF)F) "$& "$& "$&  "$&  !"# "$& !"####-./0F)F)H@AHF=H=AJAHIH@AHF=H=AJAHI=J?DEC=J?DEC40A>>HKA AJ=HKAH=HE=JE9EJDH=HE=JE5?=A@JEAFigure 2: Strict matching behavior is not equilibrium point. We set initial value of order parameters
to derive perfect matching for (A) no normalization condition and (B) normalization condition.
In both cases  choice probability that yields perfect matching is repulsive. For no normalization
condition  initial conditions were Ô¨Årst set at (cid:22)JB = 1.0  œÉA = œÉB = 1.0 and then (cid:22)JA was determined
so that pA = pmatch
. For normalization condition  these values were rescaled so that normalization
condition was met.

A

a

a

‚àí (cid:22)J 2

a/dŒ± = d(l2

repulsive property of matching behavior  let us substitute the condition of the matching law  ‚ü®r|A‚ü© =
‚ü®r|B‚ü© = ‚ü®r‚ü© into Equations 11  for the no normalization condition. We then Ô¨Ånd that ‚ü®Fa
~ha‚ü©
and ‚ü®Fa ~xa‚ü© are zero but ‚ü®F 2
‚ü© is non-zero and positive except for the non-interesting case where r
always takes the same value. Therefore  when pA = pmatch
  the variance in the weight increases 
a)/dŒ± > 0. This moves the choice probabilities toward unbiased choice
i.e.  dœÉ2
behavior  pA = 0.5 (see Equation 4). This is the reason that pmatch
is repulsive. This result is in
contrast with the N = 1 case [7] where the average changes stop when pA converges to pmatch
B) in Equation 4 is always two; thus  the only factor that
With weight normalization 
determines choice probability is the difference between (cid:22)JA and (cid:22)JB. Substituting ‚ü®r|a‚ü© = ‚ü®r‚ü© ‚àÄa into
Equation 11  only term ‚ü®F 2
2(‚ü®F 2
‚ü©) ( (cid:22)JB ‚àí
‚ü© > 0 holds; thus  the
(cid:22)JA) Except for uninteresting cases where r is always 0 or 1  ‚ü®F 2
absolute difference  | (cid:22)JB ‚àí (cid:22)JA|  always decreases. Hence  again  the choice probability at pmatch
approaches unbiased choice behavior due to the diffusion effect.

‚àö
‚ü© remains  and we obtain d( (cid:22)JB ‚àí (cid:22)JA)/(dŒ±) = ‚àí 1
‚ü© + ‚ü®F 2

‚ü© +‚ü®F 2

A + l2

2(l2

B

B

A

A

A

A

A

A

a

Nevetheless  the choice probability of the RM-Hebb rule without weight normalization asymptoti-
cally converges to pmatch
. The reason for this can be explained as follows. First  we rewrite the
A
choice probability as

.

(12)

Ô£±Ô£≤Ô£≥‚àí

‚àö

pA =

erfc

1
2

X0( (cid:22)JA ‚àí (cid:22)JB)
B + œÉ2
A + œÉ2

2( (cid:22)J 2

A + (cid:22)J 2

B + 2œÉ2
p)

Ô£ºÔ£ΩÔ£æ .

a

A

‚ü©  which moves pA away from pmatch

From this expression  we Ô¨Ånd that the larger the magnitude of (cid:22)Ja is  the weaker the effect of increases
in œÉa. The ‚Äúdiffusion term‚Äù  ‚ü®F 2
depends on pA but not on
the magnitude of (cid:22)Ja‚Äôs. Thus  within the order parameter set satisfying pA = pmatch
  the larger the
magnitudes of Ja‚Äôs are  the weaker is the repulsive effect. If | (cid:22)JB‚àí (cid:22)JA| ‚Üí ‚àû while œÉA  œÉB are Ô¨Ånite 
. Because | (cid:22)JB ‚àí (cid:22)JA| can increase faster than œÉA and œÉB in the RM-Hebb rule
pA stays at pmatch
without any weight constraints  the network approaches such situations. This is the reason that in
Figure 2A the pA returned to pmatch
. When weight normalization
is imposed  the magnitude of (cid:22)Ja‚Äôs are limited as | (cid:22)JB ‚àí (cid:22)JA| < 2. Thus  the diffusion effect prevents
. In the delta rule  the magnitude of (cid:22)Ja‚Äôs cannot increase independently
pA from approaching pmatch
  where the increase in | (cid:22)JB ‚àí (cid:22)JA| and those in
of œÉa‚Äôs. Thus  pA saturates before it reaches pmatch
œÉa‚Äôs are balanced.

after it was repulsed from pmatch

A

A

A

A

A

A

5.2 Learning Rate Dependence of Learning Behavior

Next  we investigate how the learning rate  Œ∑  affects the choice behavior. In the ‚Äúdiffusion term‚Äù 
‚ü®F 2
‚ü©  is a quadratic term in the learning rate Œ∑. In contrast  only the Ô¨Årst order terms of Œ∑ appear

a

6

###$$#%%#H=HE=JE40A>>HKA AJ=HKA##$%&9EJDH=HE=JE=J?DECF)F))*Figure 3: Evolution of choice probability for various learning rates  Œ∑. Top rows are for non-weight
normalization condition and bottom rows are for normalization condition. Columns at left are for
RM-Hebb rule and those at right are for delta rule. Parameters for model and task schedules are
same as those in Figure 1. Initial conditions were set at œÉa = 0.0  (a = A  B)  (cid:22)Ja = 5.0 for
non-normalization condition and (cid:22)Ja = 1.0 for normalization condition.

in the other terms. Therefore  if Œ∑ is small  the repulsive effect from matching behavior due to the
diffusion effect is expected to weaken. Figure 3 plots the dependence of the evolution of pA on Œ∑. As
a whole  as Œ∑ is decreased  the asymptotic value  pA  approaches matching behavior  but relaxation
slows down due to the diffusion of synaptic weights. As we previously discussed  the diffusion
effect is more evident for the delta rule than for the RM-Hebb rule  and for the weight-normalization
condition than the non- normalization condition. This tendency becomes evident as Œ∑ increases.

a

For the RM-Hebb rule without normalization  networks approach matching behavior even for a very
large learning rate (Œ∑ = 1000). At the beginning of learning when (cid:22)Ja is of small magnitude  the dif-
fusion term  ‚ü®F 2
‚ü©  has a large impact so that it greatly impedes learning for a large Œ∑ case. However 
as the magnitude of the differences (cid:22)JA ‚àí (cid:22)JB increases  this effect weakens and the dependence of
pA on Œ∑ becomes quite small. Although there is still a deviation from perfect matching (see inset of
Figure 3A)  the asymptotic value is almost unaffected in the RM-Hebb rule. For the delta rule with-
out normalization  the asymptotic values gradually depend on Œ∑. With normalization constraints  the
RM-Hebb rule also demonstrate graded dependence of asymptotic probability on Œ∑. These results
reÔ¨Çect the fact that the greater learning rate Œ∑ is  the larger the diffusion effect.

5.3 Deviation from Matching Law

Choices by animals in many experiments deviate slightly from matching behavior toward unbi-
ased random choice  a phenomenon called undermatching [2  23]. The synaptic diffusion effects
reproduces this phenomenon. Figure 4A B plots choice probability for option A as a function of
the fraction income from the option.
If this function lies at the diagonal line  it corresponds to
matching behavior. For the RM-rule with weight normalization  as the learning rate Œ∑ increases  the
choice probabilities deviate from matching behavior towards unbiased random choice  pA = 0.5
(Figure 4A). Similar results are obtained for another weight constraint  the hard bound condition
‚àö
(Figure 4B). In this condition  if the updates makes J a
is set to
N (or 0). We see that the larger the Œ∑ is  the broader the weight distributions due the the
Jmax/
synaptic diffusion effects (Figure 4A). This result suggests that the weight diffusion effect causes un-
dermathing regardless of the way of weight constraint  as long as the synaptic weights are conÔ¨Åned
to a Ô¨Ånite range  as predicted by our theory.

‚àö
i > Jmax/

N (or J a

i < 0)  J a

i

7

"  "$"####$$#%%#&"  "$"####$$#%%#&"  "$"####$$#%%#&"  "$"####$$#%%#& 40A>>HKA9EJDH=E=JE)*+ H=E=JE AJ=HKADDDDDDDDDD=J?DEC=J?DEC2H>=>EEJOB?DIEC)2H>=>EEJOB?DIEC)2H>=>EEJOB?DIEC)2H>=>EEJOB?DIEC)Figure 4: Constraints on synaptic weights leads to the undermatching behavior through synaptic
diffusion effects. (A) Choice probability for A as a function of the fraction income for A for the
RM-rule with weight normalization. We used VI schedules with ŒªA = 0.3a and ŒªB = 0.3(1 ‚àí a) 
varying the constant a (0 ‚â§ a ‚â§ 1). The results were obtained using stationaly points of the
macroscopic equations. The diagonal line indicates the perfect matching behavior. As the learning
rate Œ∑ increases  the choice probabilities deviate from matching behavior towards unbiased random
‚àö
choice  pA = 0.5.
(B) The same plot with (A) for the RM-rule with the hard bound condition
(the synaptic weights are restricted to the interval [0  Jmax/
N] where Jmax = 5.0) obtained by
numerical simulations. Simulations were done for N = 500. (C) The weight distribution after
convergence for the simulations in (B) indicated by the gray arrows.

6 Discussion

In this study  we analyzed the reward-based learning procedure in simple  large-scale decision mak-
ing networks. To achieve this  we employed techniques from statistical mechanics. Although sta-
tistical mechanical analysis has been successively applied to analyze the dynamics of learning the
in neural networks  we applied it to reward-modulated learning in decision making networks for
the Ô¨Årst time  to the best of our knowledge. We have assumed the activities of sensory neurons
are independent. In realistic cases  there may be correlations among sensory neurons. The exis-
tence of correlation weakens the diffusion effects. However  if there are independent Ô¨Çuctuations  as
observed in many physiological studies  the diffusion effects are at play here as well.

If only a single plastic synapse is taken into consideration  covariance learning rules seem to make
matching behavior a steady state of learning. However  under certain situations where a large number
of synapses simultaneously modify their efÔ¨Åcacy  matching behavior cannot be a steady state. This
is because the randomness in weight modiÔ¨Åcations affects the choice probability of the network  and
the effect returns to the learning process. These results may offer suggestions for discussing learning
behavior in large-scale neural circuits.

Choice behavior in many experiments deviates slightly from matching behavior toward unbiased
choice behavior  a phenomenon called undermatching [23  2]. There are several possible explana-
tions for this phenomenon. The learning rule employed by Soltani & Wang [4] is equivalent to the
state-less Q-learning in the literature on reinforcement learning [15]. Sakai & Fukai [5  6] proved
that Q-learning does not lead to matching behavior. Thus  Soltani-Wang‚Äôs model is intrinsically
incapable of reproducing matching behavior. The authors interpreted that the departure from match-
ing behavior due to limitations in the learning rule was a possible mechanism for undermatching.
Loewenstein [7] suggested that the mistuning of parameters in the covariance learning rule could
cause undermatching. However  we found that in some task settings  the mistuning can cause over-
matching  rather than undermatching [12]. Our Ô¨Åndings in this study add one possible mechanism
for undermatching  i.e.  undermatching can be caused by the diffusion of synaptic efÔ¨Åcacies. The
diffusion effects provide a robust mechanism for undermatching: It reproduces undermatching be-
havior  regardless of speciÔ¨Åc task settings.

To achieve random choice behavior  it is thought to require Ô¨Åne-tuning of network parameters [16] 
whereas random choice behavior is often observed in behavioral experiments. Our results suggest
that the broad distributions of synaptic weights observed in experiments [24] can make it easier to
realize stochastic random choice behavior perhaps than previously thought.

8

References

[1] R. J. Herrnstein  H. Rachlin  and D. I. Laibson. The Matching Law. Russell Sage Foundation

New York  1997.

[2] L. P. Sugrue  G. S. Corrado  and W. T. Newsome. Matching behavior and the representation of

value in the parietal cortex. Science  304(5678):1782‚Äì1787  2004.

[3] Y. Loewenstein and H. S. Seung. Operant matching is a generic outcome of synaptic plasticity
based on the covariance between reward and neural activity. Proceedings of the National
Academy of Sciences  103(41):15224‚Äì15229  2006.

[4] A. Soltani and X. J. Wang. A biophysically based neural model of matching law behavior:

melioration by stochastic synapses. Journal of Neuroscience  26(14):3731‚Äì3744  2006.

[5] Y. Sakai and T. Fukai. The actor-critic learning is behind the matching law: Matching versus

optimal behaviors. Neural Computation  20(1):227‚Äì251  2008.

[6] Y. Sakai and T. Fukai. When does reward maximization lead to matching law? PLoS ONE 

3(11):e3795  2008.

[7] Y. Loewenstein. Robustness of learning that is based on covariance-driven synaptic plasticity.

PLoS Computational Biology  4(3):e1000007  2008.

[8] W. Kinzel and P. Rujan. Improving a network generalization ability by selecting examples.

Europhysics Letters  13(5):473‚Äì477  1990.

[9] D. Saad. On-line learning in neural networks. Cambridge University Press  1998.
[10] G. Reents and R. Urbanczik. Self-averaging and on-line learning. Physical Review Letters 

80(24):5445‚Äì5448  1998.

[11] M. Biehl  N. Caticha  and P. Riegler. Statistical mechanics of on-line learning. Similarity-

Based Clustering  pages 1‚Äì22  2009.

[12] K. Katahira  K. Okanoya  and M. Okada. Statistical mechanics of reward-modulated learning

in decision making networks. under review.

[13] X. J. Wang. Probabilistic decision making by slow reverberation in cortical circuits. Neuron 

36(5):955‚Äì968  2002.

[14] C. van Vreeswijk and H. Sompolinsky. Chaotic balanced state in a model of cortical circuits.

Neural Computation  10(6):1321‚Äì1371  1998.

[15] A. Soltani  D. Lee  and X. J. Wang. Neural mechanism for stochastic behaviour during a

competitive game. Neural Networks  19(8):1075‚Äì1090  2006.

[16] S. Fusi  W. F. Asaad  E. K. Miller  and X. J. Wang. A neural circuit model of Ô¨Çexible sen-
sorimotor mapping: learning and forgetting on multiple timescales. Neuron  54(2):319‚Äì333 
2007.

[17] E. M. Izhikevich. Solving the distal reward problem through linkage of STDP and dopamine

signaling. Cerebral Cortex  17:2443‚Äì2452  2007.

[18] R. V. Florian. Reinforcement learning through modulation of spike-timing-dependent synaptic

plasticity. Neural Computation  19(6):1468‚Äì1502  2007.

[19] M. A. Farries and A. L. Fairhall. Reinforcement Learning With Modulated Spike Timing

Dependent Synaptic Plasticity. Journal of Neurophysiology  98(6):3648‚Äì3665  2007.

[20] R. Legenstein  D. Pecevski  and W. Maass. A learning theory for reward-modulated spike-
timing-dependent plasticity with application to biofeedback. PLoS Computational Biology 
4(10):e1000180  2008.

[21] C. T. Law and J. I. Gold. Reinforcement learning can account for associative and perceptual

learning on a visual-decision task. Nature Neuroscience  12(5):655‚Äì663  2009.

[22] M. Biehl. An exactly solvable model of unsupervised learning. Europhysics Letters 

25(5):391‚Äì396  1994.

[23] W. M. Baum. On two types of deviation from the matching law: Bias and undermatching.

Journal of the Experimental Analysis of Behavior  22(1):231‚Äì242  1974.

[24] B. Barbour  N. Brunel  V. Hakim  and J. P. Nadal. What can we learn from synaptic weight

distributions? TRENDS in Neurosciences  30(12):622‚Äì629  2007.

9

,Long Jin
Justin Lazarow
Zhuowen Tu