2017,Principles of Riemannian Geometry  in Neural Networks,This study deals with neural networks in the sense of geometric transformations acting on the coordinate representation of the underlying data manifold which the data is sampled from. It forms part of an attempt to construct a formalized general theory of neural networks in the setting of Riemannian geometry. From this perspective  the following theoretical results are developed and proven for feedforward networks. First it is shown that residual neural networks are finite difference approximations to dynamical systems of first order differential equations  as opposed to ordinary networks that are static. This implies that the network is learning systems of differential equations governing the coordinate transformations that represent the data. Second it is shown that a closed form solution of the metric tensor on the underlying data manifold can be found by backpropagating the coordinate representations learned by the neural network itself. This is formulated in a formal abstract sense as a sequence of Lie group actions on the metric fibre space in the principal and associated bundles on the data manifold. Toy experiments were run to confirm parts of the proposed theory  as well as to provide intuitions as to how neural networks operate on data.,Principles of Riemannian Geometry

in Neural Networks

Michael Hauser

Department of Mechanical Engineering

Pennsylvania State University

State College  PA 16801
mzh190@psu.edu

Asok Ray

Department of Mechanical Engineering

Pennsylvania State University

State College  PA 16801

axr2@psu.edu

Abstract

This study deals with neural networks in the sense of geometric transformations
acting on the coordinate representation of the underlying data manifold which
the data is sampled from. It forms part of an attempt to construct a formalized
general theory of neural networks in the setting of Riemannian geometry. From
this perspective  the following theoretical results are developed and proven for
feedforward networks. First it is shown that residual neural networks are (cid:27)-
nite di(cid:29)erence approximations to dynamical systems of (cid:27)rst order di(cid:29)erential
equations  as opposed to ordinary networks that are static. This implies that the
network is learning systems of di(cid:29)erential equations governing the coordinate
transformations that represent the data. Second it is shown that a closed form
solution of the metric tensor on the underlying data manifold can be found by
backpropagating the coordinate representations learned by the neural network
itself. This is formulated in a formal abstract sense as a sequence of Lie group
actions on the metric (cid:27)bre space in the principal and associated bundles on the
data manifold. Toy experiments were run to con(cid:27)rm parts of the proposed theory 
as well as to provide intuitions as to how neural networks operate on data.

1 Introduction

The introduction is divided into two parts. Section 1.1 attempts to succinctly describe ways in
which neural networks are usually understood to operate. Section 1.2 articulates a more minority
perspective. It is this minority perspective that this study develops  showing that there exists a rich
connection between neural networks and Riemannian geometry.

1.1 Latent variable perspectives
Neural networks are usually understood from a latent variable perspective  in the sense that
successive layers are learning successive representations of the data. For example  convolution
networks [10] are understood quite well as learning hierarchical representations of images [19].
Long short-term memory networks [9] are designed such that input data act on a memory cell to
avoid problems with long term dependencies. More complex devices like neural Turing machines
are designed with similar intuitions for reading and writing to a memory [6].
Residual networks were designed [7] with the intuition that it is easier to learn perturbations from
the identity map than it is to learn an unreferenced map. Further experiments then suggest that
residual networks work well because  during forward propagation and back propagation  the signal
from any block can be mapped to any other block [8]. After unraveling the residual network  this
attribute can be seen more clearly. From this perspective  the residual network can be understood
as an ensemble of shallower networks [17].

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

x(0)

x(1)

M

x(2)

x(L)

x(3)

ϕ(1)

x(2) (M )

x(3) (M )

ϕ(2)

ϕ(L)

x(L) (M )

x(0) (M )

x(1) (M )

ϕ(0)

Figure 1: Coordinate systems x(l+1) := ϕ(l) ◦ ... ◦ ϕ(1) ◦ ϕ(0) ◦ x(0) induced by the coordinate

transformations ϕ(l) : x(l) (M ) →(cid:0)ϕ(l) ◦ x(l)(cid:1) (M ) learned by the neural network. The pullback
ϕ(l)∗ : T x(l) (M ) → T(cid:0)ϕ(l) ◦ x(l)(cid:1) (M ) between tangent spaces.

metric gx(l)(M ) (X  Y ) := g(ϕ(l)◦x(l))(M )
pulls-back (i.e. backpropagates) the co-
ordinate representation of the metric tensor from layer l + 1 to layer l  via the pushforward map

ϕ(l)∗ X  ϕ(l)∗ Y

(cid:17)

(cid:16)

1.2 Geometric perspectives

These latent variable perspectives are a powerful tool for understanding and designing neural
networks. However  they often overlook the fundamental process taking place  where successive
layers successively warp the coordinate representation of the data manifold with nonlinear trans-
formations into a form where the classes in the data manifold are linearly separable by hyperplanes.
These nested compositions of a(cid:28)ne transformations followed by nonlinear activations can be seen
by work done by C. Olah (http://colah.github.io/) and published by LeCun et al. [11].
Research in language modeling has shown that the word embeddings learned by the network
preserve vector o(cid:29)sets[13]  with an example given as xapples − xapple ≈ xcars − xcar for the
word embedding vector xi. This suggests the network is learning a word embedding space with
some resemblance to group closure  with group operation vector addition. Note that closure is
generally not a property of data  for if instead of word embeddings one had images of apples and
cars  preservation of these vector o(cid:29)sets would certainly not hold at the input [3]. This is because
the input images are represented in Cartesian coordinates  but are not sampled from a (cid:30)at data
manifold  and so one should not measure vector o(cid:29)sets by Euclidean distance. In Locally Linear
Embedding [14]  a coordinate system is learned in which Euclidean distance can be used. This work
shows that neural networks are also learning a coordinate system in which the data manifold can
be measured by Euclidean distance  and the coordinate representation of the metric tensor can be
backpropagated through to the input so that distance can be measured in the input coordinates.

2 Mathematical notations

.b  are placeholders to keep track of which index comes (cid:27)rst  second  etc.

Einstein notation is used throughout this paper. A raised index in parenthesis  such as x(l)  means
it is the lth coordinate system while ϕ(l) means it is the lth coordinate transformation. If the index
is not in parenthesis  a superscript free index means it is components of a vector  a subscript free
index means it is components of a covector  and a repeated index means implied summation. The .
in tensors  such as Aa.
A (topological) manifold M of dimension dimM is a Hausdor(cid:29)  paracompact topological space that
is locally homeomorphic to RdimM [18]. This homeomorphism x : U → x (U ) ⊆ RdimM is called
a coordinate system on U ⊆ M. Non-Euclidean manifolds  such as S1  can be created by taking
an image and rotating it in a circle. A feedforward network learns coordinate transformations
x(l+1) (M )  and is initialized in Cartesian coordinates x(0) : M → x(0) (M )  as seen in Figure 1.
A data point q ∈ M can only be represented as numbers with respect to some coordinate system;
with the coordinates at layer l + 1  q is represented as the layerwise composition x(l+1) (q) :=

ϕ(l) : x(l) (M ) → (cid:0)ϕ(l) ◦ x(l)(cid:1) (M )  where the new coordinates x(l+1) := ϕ(l)(cid:0)x(l)(cid:1) : M →
(cid:0)ϕ(l) ◦ ... ◦ ϕ(1) ◦ ϕ(0) ◦ x(0)(cid:1) (q). The output coordinate representation is x(L) (M ) ⊆ Rd.
coordinates as x(l+1) := ϕ(l)(cid:0)x(l)(cid:1) := f (x(l); l). Note ReLu is not a bijection and thus not a proper
coordinate transformation. A residual network transforms coordinates as x(l+1) := ϕ(l)(cid:0)x(l)(cid:1) :=

For an activation function f  such as ReLU or tanh  a standard feedforward network transforms

x(l) + f (x(l); l). Note that these are global coordinates over the entire manifold. A residual network
with ReLu activation is bijective  and is piecewise linear with kinks of in(cid:27)nite curvature.

2

softmax(cid:0)W (L) · x(L)(cid:1)j

/(cid:80)K

With the Softmax coordinate transformation de(cid:27)ned as
eW (L)j x(L)

:=
k=1 eW (L)kx(L) the probability of q ∈ M being from class j is P (Y = j|X = q) =

softmax(cid:0)W (L) · x(L) (q)(cid:1)j.
3 Neural networks as Ck di(cid:29)erentiable coordinate transformations
One can de(cid:27)ne entire classes of coordinate transformations. The following formulation also has the
form of di(cid:29)erentiable curves/trajectories  but because the number of dimensions often changes as
one moves through the network  it is di(cid:28)cult to interpret a trajectory traveling through a space of
changing dimensions. A standard feedforward neural network is a C0 function:

(1)
A residual network has the form x(l+1) = x(l) + f (x(l); l). However  because of eventually taking
the limit as L → ∞ and l ∈ [0  1] ⊂ R  as opposed to l being only a (cid:27)nitely countable index  the
equivalent form of the residual network is as follows:

x(l+1) := f (x(l); l)

x(l+1) (cid:39) x(l) + f (x(l); l)∆l

(2)
where ∆l = 1/L for a uniform partition of the interval [0  1] and is implicit in the weight matrix.
One can de(cid:27)ne entire classes of coordinate transformations inspired by (cid:27)nite di(cid:29)erence approxima-
tions of di(cid:29)erential equations. These can be used to impose kth order di(cid:29)erentiable smoothness:
(3)
(4)
Each of these de(cid:27)ne a di(cid:29)erential equation  but of di(cid:29)erent orders of smoothness on the coordinate
transformations. Written in this form the residual network in Equation 3 is a (cid:27)rst-order forward
di(cid:29)erence approximation to a C1 coordinate transformation and has O (∆l) error. Network architec-
tures with higher order accuracies can be constructed  such as central di(cid:29)erencing approximations

δx(l) := x(l+1) − x(l) (cid:39) f (x(l); l)∆l
δ2x(l) := x(l+1) − 2x(l) + x(l−1) (cid:39) f (x(l); l)∆l2

of a C1 coordinate transformation to give O(cid:0)∆l2(cid:1) error.

Note that the architecture of a standard feedforward neural network is a static equation  while the
others are dynamic. Also note that Equation 4 can be rewritten x(l+1) = x(l) + f (x(l); l)∆l2 +
δx(l−1)  where δx(l−1) = x(l) − x(l−1)  and in this form one sees that this is a residual network
with an extra term δx(l−1) acting as a sort of momentum term on the coordinate transformations.
This momentum term is explored in Section 7.1.
By the de(cid:27)nitions of the Ck networks given by Equations 3-4  the right hand side is both continuous
and independent of ∆l (after dividing)  and so the limit exists as ∆l → 0. Convergence rates and
error bounds of (cid:27)nite di(cid:29)erence approximations can be applied to these equations. By the standard
de(cid:27)nition of the derivative  the residual network de(cid:27)nes a system of di(cid:29)erentiable transformations.

dx(l)

x(l+∆l) − x(l)

:= lim
∆l→0

dl
x(l+∆l) − 2x(l) + x(l−∆l)

∆l

= f (x(l); l)

(5)

d2x(l)

dl2

(6)
Notations are slightly changed  by taking l = n∆l for n ∈ {0  1  2  ..  L − 1} and indexing the
layers by the fractional index l instead of the integer index n. This de(cid:27)nes a partitioning:

= f (x(l); l)

:= lim
∆l→0

∆l2

P = {0 = l(0) < l(1) < l(2) < ... < l(n) < ... < l(L) = 1}

(7)
where ∆l(n) := l(n + 1) − l(n) can in general vary with n as the maxn ∆l(n) still goes to zero as
L → ∞. To reduce notation  this paper will write ∆l := ∆l (n) for all n ∈ {0  1  2  ...  L − 1}.
In [4]  a deep residual convolution network was trained on ImageNet in the usual fashion except
parameter weights between residual blocks at the same dimension were shared  at a cost to the
accuracy of only 0.2%. This is the di(cid:29)erence between learning an inhomogeneous (cid:27)rst order
equation dx(l)
:= f (x(l)).
dl

:= f (x(l); l) and a (piecewise) homogeneous (cid:27)rst order equation dx(l)
dl

3

(a) A C0 network with sharply changing layer-wise particle trajectories.

(b) A C1 network with smooth layer-wise particle trajectories.

(c) A C2 network also exhibits smooth layer-wise particle trajectories.

(d) A combination C0 and C1 network  where the identity connection is left out in layer 6.

Figure 2: Untangling the same spiral with 2-dimensional neural networks with di(cid:29)erent constraints
on smoothness. The x and y axes are the two nodes of the neural network at a given layer l  where
layer 0 is the input data. The C0 network is a standard network  while the C1 network is a residual
network and the C2 network also exhibits smooth layerwise transformations. All networks achieve
0.0% error rates. The momentum term in the C2 network allows the red and blue sets to pass over
each other in layers 3  4 and 5. Figure 2d has the identity connection for all layers other than layer 6.

4 The Riemannian metric tensor learned by neural networks

From the perspective of di(cid:29)erentiable geometry  as one moves through the layers of the neural
network  the data manifold stays the same but the coordinate representation of the data manifold
changes with each successive a(cid:28)ne transformation and nonlinear activation. The objective of the
neural network is to (cid:27)nd a coordinate representation of the data manifold such that the classes are
linearly separable by hyperplanes.
De(cid:27)nition 4.1. (Riemannian manifold [18]) A Riemannian manifold (M  g) is a real smooth
manifold M with an inner product  de(cid:27)ned by the positive de(cid:27)nite metric tensor g  varying
smoothly on the tangent space of M.

If the network has been well trained as a classi(cid:27)er  then by Euclidean distance two input points
of the same class may be far apart when represented by the input coordinates but close together
in the output coordinates. Similarly  two points of di(cid:29)erent classes may be near each other when
represented by the input coordinates but far apart in the output coordinates. These ideas form the
basis of Locally Linear Embeddings [14]. The intuitive way to measure distances is in the output
coordinates  which even in the unsupervised case tends to be a (cid:30)attened representation of the data
manifold [3]. Accordingly  the metric in the output coordinates is the Euclidean metric:

The elements of the metric tensor transforms as a tensor with coordinate transformations:

g(x(l))albl =

g(x(l+1))al+1bl+1

(cid:16)

g

x(L)(cid:17)
(cid:19)al+1.

(cid:18) ∂x(l+1)

:= ηaLbL

aLbL

(cid:18) ∂x(l+1)

(cid:19)bl+1.

∂x(l)

.al

∂x(l)

.bl

4

(8)

(9)

layer 0layer 1layer 2layer 3layer 4layer 5layer 6layer 7layer 8layer 9layer 10layer 0layer 1layer 2layer 3layer 4layer 5layer 6layer 7layer 8layer 9layer 10layer 0layer 1layer 2layer 3layer 4layer 5layer 6layer 7layer 8layer 9layer 10layer 0layer 1layer 2layer 3layer 4layer 5layer 6layer 7layer 8layer 9layer 10Figure 3: A C1 (residual) network with a hyperbolic tangent activation function separating the

spiral manifold. Additionally  balls of constant radius ds =(cid:112)galbl (x(l))dx(l)al dx(l)bl at di(cid:29)erent

points are shown. In the output coordinates  distances are measured by the standard Euclidean
metric in Equation 8  and so the circles are "round". The coordinate representation of the metric
tensor is pulled-back (backpropagated) through the network to the input by Equations 9 and 10.
Distances on the data manifold can then be measured with the input Cartesian coordinates  and so
the circles are not round. These balls can also be interpreted as forming an  − δ relationship across
layers of the network  where an -ball at one layer corresponds to a δ-ball at the previous layer.

The above recursive formula is solved from the output layer to the input  i.e. the coordinate
representation of the metric tensor is backpropagated through the network from output to input:

(cid:32)

l(cid:89)

l(cid:48)=L−1

(cid:33)al(cid:48)+1.

(cid:32)

∂x(l(cid:48)+1)
∂x(l(cid:48))

.al(cid:48)

∂x(l(cid:48)+1)
∂x(l(cid:48))

(cid:33)bl(cid:48)+1.

 ηaLbL

.bl(cid:48)

g(x(l))albl =

If the network is taken to be residual as in Equation 2  then the Jacobian of the coordinate transfor-
mation is found  with δal+1.

the Kronecker delta:

.al

(cid:18) ∂x(l+1)

(cid:19)al+1.

∂x(l)

.al

= δal+1.

.al

+

(cid:32)

∂f(cid:0)x(l); l(cid:1)

(cid:33)al+1.

∂x(l)

∆l

.al

(10)

(11)

(12)

Backpropagating the coordinate representation of the metric tensor requires the sequence of matrix
products from output to input  and can be de(cid:27)ned for any layer l:

(cid:32)

(cid:33)al(cid:48)+1.

(cid:32)

P aL.
.al

:=

al(cid:48)+1.
.al(cid:48) +

∂f (z(l(cid:48)+1); l(cid:48))

∂z(l(cid:48)+1)

∂z(l(cid:48)+1)
∂x(l(cid:48))

.el(cid:48)+1

.al(cid:48)

δ

L−1(cid:89)

l(cid:48)=l

(cid:33)el(cid:48)+1.



∆l

where z(l+1) := W (l) · x(l) + b(l). With this  taking the output metric to be the standard Euclidean
metric ηab  the linear element can be represented in the coordinate space for any layer l:

P b.
.bl

dxal dxbl

ds2 = ηabP a.
.al

(13)
The data manifold is independent of coordinate representation. At the output where distances
are measured by the standard Euclidean metric an -ball can be de(cid:27)ned. The linear element in
Equation 13 de(cid:27)nes the corresponding δ-ball at layer l. This can be used to see what in the input
space the neural network encodes as similar in the output space.
As L → ∞  Equation 12 becomes an in(cid:27)nite product of matricies (from our in(cid:27)nite applications of
the chain rule) and these transformations act smoothly along the (cid:27)bres of the tensor bundle. The
proof that this sequence converges in the limit can be found in the appendix.
This analysis has so far assumed a constant layerwise dimension  which is not how most neural
networks are used in practice  where the number of nodes often changes. This is handled by the
pullback metric [18]. Manifolds can be submersed and immersed into lower and higher dimensional
spaces so long as the rank of the pushforward Jacobian matrix is constant for every p ∈ M [12]. The
dimension of the underlying data manifold is de(cid:27)ned as the dimension of the smallest  bottleneck
layer of the neural network  i.e. dimM := minl dimx(l) (M )  and all other higher dimensional
layers are immersion/embedding representations of this lowest dimensional representation.

5

layer 0layer 1layer 2layer 3layer 4layer 5De(cid:27)nition 4.2. (Pushforward map) Let M and N be topological manifolds  ϕ(l) : M → N a
smooth map and T M and T N be their respective tangent spaces. Also let X ∈ T M where
X : C∞ (M ) → R  and f ∈ C∞ (N ). The pushforward is the linear map ϕ(l)∗ : T M → T N that

takes an element X (cid:55)→ ϕ(l)∗ X and is de(cid:27)ned by its action on f as(cid:0)ϕ(l)∗ X(cid:1) (f ) := X (f ◦ ϕ(l)).
(cid:0)ϕ(l)∗ X  ϕ(l)∗ Y(cid:1) ∀X  Y ∈ T M.

De(cid:27)nition 4.3. (Pullback metric) Let (M  gM ) and (N  gN ) be Riemannian manifolds  ϕ(l) : M →
N a smooth map and ϕ(l)∗ : T M → T N the pushforward between their tangent spaces T M and
T N. Then the pullback metric on M is given by gM (X  Y ) := gN
In practice being able to change dimensions in the neural network is important for many reasons.
One reason is that neural networks usually have access to a limited number of types of nonlinear
coordinate transformations  for example tanh  σ and ReLU. This severely limits the ability of the
network to separate the wide variety of manifolds that exist. For example  the networks have
di(cid:28)culty linearly separating the simple toy spirals in Figures 2 because they only have access to
coordinate transformations of the form tanh. If instead they had access to a coordinate system that
was more appropriate for spirals  such as polar coordinates  they could very easily separate the data.
This is the reason why Locally Linear Embeddings [14] could very easily discover the coordinate
charts for the underlying manifold  because k-nearest neighbors is an extremely (cid:30)exible type of
nonlinearity. Allowing the network to go into higher dimensions makes it easier to separate data.

5 Lie Group actions on the metric (cid:27)bre bundle

This section will abstractly formulate Section 4 as neural networks learning sequences of left Lie
Group actions on the metric ((cid:27)bre) space over the data manifold to make the metric representation of
the underlying data manifold Euclidean. Several de(cid:27)nitions  which can be found in the appendix in
the full version of this paper  are needed to formulate Lie group actions on principal and associated
(cid:27)bre bundles  namely of bundles  (cid:27)bre bundles  Lie Groups and their actions on manifolds [18].
De(cid:27)nition 5.1. (Principal (cid:27)bre bundle) A bundle (E  π  M ) is called a principal G-bundle if:
(i.) E is equipped with a right G-action (cid:67): E × G → E.
(ii.) The right G-action (cid:67) is free.
(iii.) (E  π  M ) is (bundle) isomorphic to (E  ρ  E/G) where the surjective projection map ρ : E →
E/G is de(cid:27)ned by ρ () := [] as the equivalence class of points of 
Remark. (Principal bundle) The principal (cid:27)bre bundle can be thought of (locally) as a (cid:27)bre bundle
with (cid:27)bres G over the base manifold M.
De(cid:27)nition 5.2. (Associated (cid:27)bre bundle) Given a G principal bundle and a smooth manifold F on
which exists a left G-action (cid:66): G × F → F   the associated (cid:27)bre bundle (PF   πF   M ) is de(cid:27)ned as
follows:
(i.) let ∼G be the relation on P × F de(cid:27)ned as follows:
(p  f ) ∼G (p(cid:48)  f(cid:48)) : ⇐⇒ ∃h ∈ G : p(cid:48) = p (cid:67) h and f(cid:48) = h−1 (cid:66) f  and thus PF := (P × F ) / ∼G.
(ii.) de(cid:27)ne πF : PF → M by πF ([(p  f )]) := π (p)
Neural network actions on the manifold M are a (layerwise) sequence of left G-actions on the
associated (metric space) (cid:27)bre bundle. Let the dimension of the manifold d := dim M.
The structure group G is taken to be the general linear group of dimension d over R  i.e.
G = GL (d  R) := {φ : Rd → Rd| det φ (cid:54)= 0}.
The principal bundle P is taken to be the frame bundle  i.e. P = LM := ∪p∈M LpM :=
∪p∈M{(e1  ...  ed) ∈ TpM| (e1  ...  ed) is a basis of TpM}  where TpM is the tangent space of
M at the point p ∈ M.
The right G-action (cid:67): LM × GL (d  R) → LM is de(cid:27)ned by e (cid:67) h = (e1  ...  ed) (cid:67) h :=
.1 eal   ...  hal.
(hal.

The (cid:27)bre F in the associated bundle will be the metric tensor space  and so F =(cid:0)Rd(cid:1)∗ ×(cid:0)Rd(cid:1)∗ 
the inverse of the left  namely(cid:0)h−1 (cid:66) g(cid:1)

where the ∗ denotes the cospace. With this  the left G-action (cid:66): GL (d  R) × F → F is de(cid:27)ned as

.d eal )  which is the standard transformation law of linear algebra.

:= (g (cid:67) h)albl

= gal+1bl+1hal+1.

.al hbl+1.

.

.bl

albl

6

(cid:0)h−1

0

(cid:66) h−1

1

(cid:66) ... (cid:66) h−1

L

Layerwise sequential applications of the left G-action from output to input is thus simply understood:

(cid:66) g(cid:1)

a0b0

=(cid:0)h−1

0 • ... • h−1

L

(cid:1) (cid:66) gaLbL =

0(cid:89)

(cid:16)

l(cid:48)=L−1

h

al(cid:48)+1.
.a(cid:48)

l

h

bl(cid:48)+1.
.bl(cid:48)

(cid:17)

gaLbL

(14)

This is equivalent to Equation 10  only formulated in a formal  abstract sense.

6 Backpropagation as a sequence of right Lie Group actions

A similar analysis that has been performed in Sections 4 and 5 can be done to generalize error
backpropagation as a sequence of right Lie Group actions on the output error (or more generally
pull-back the frame bundle). The discrete layerwise error backpropagation algorithm [15] is derived
using the chain rule on graphs. The closed form solution of the gradient of the output error
E with respect to any layer weight W (l−1) can be solved for recursively from the output  by
backpropagating errors:

∂E

∂W (l−1)

=

aL

∂x(L)

(cid:19)

l(cid:48)=L−1

l(cid:89)

∂x(l(cid:48)+1)
∂x(l(cid:48))

(cid:18) ∂E

(cid:32)
(cid:16) ∂x(l)
on the output frame bundle(cid:0) ∂

∂W (l−1)

.al(cid:48)

(cid:33)al(cid:48)+1.
(cid:17)al
(cid:1)

=

∂x(L)

aL

(cid:18) ∂x(l)
(cid:17)al.
(cid:16) ∂x(l)

∂W (l−1)

(cid:19)al
(cid:16) ∂z(l)

In practice  one further applies the chain rule
that W (l−1) is a coordinate chart on the parameter manifold [1]  not the data manifold.
In
this form it is immediately seen that error backpropagation is a sequence of right G-actions
. This pulls-back the frame bundle

(cid:16) ∂x(l(cid:48)+1)

(cid:17)al(cid:48)+1.

(cid:81)l

∂W (l−1)

∂z(l)

.bl

l(cid:48)=L−1

∂x(l(cid:48))

.al(cid:48)

acting on E to the coordinate system at layer l  and thus puts it in the same space as
For the residual network  the transformation matrix Equation 11 can be inserted into Equation 15.By
the same logic as before  the in(cid:27)nite tensor product in Equation 15 converges in the limit L → ∞ in
the same way as in Equation 12  and so it is not rewritten here. In the limit this becomes a smooth
right G-action on the frame bundle  which itself is acting on the error cost function.

∂W (l−1)

(15)

(cid:17)bl. Note
(cid:17)al.
(cid:16) ∂x(l)

7 Numerical experiments

This section presents the results of numerical experiments used to understand the proposed theory.
The C∞ hyperbolic tangent has been used for all experiments  with weights initialized according
to [5]. For all of the experiments  layer 0 is the input Cartesian coordinate representation of the
data manifold  and the (cid:27)nal layer L is the last hidden layer before the linear softmax classi(cid:27)er. GPU
implementations of the neural networks are written in the Python library Theano [2  16].

7.1 Neural networks with Ck di(cid:29)erentiable coordinate transformations
As described in Section 3  kth order smoothness can be imposed on the network by considering
network structures de(cid:27)ned by e.g. Equations 3-4. As seen in Figure 2a  the standard C0 network
with no impositions on di(cid:29)erentiability has very sharp layerwise transformations and separates
the data in an unintuitive way. The C1 residual network and C2 network can be seen in Figures 2b
and 2c  and exhibit smooth layerwise transformations and separate the data in a more intuitive way.
Forward di(cid:29)erencing is used for the C1 network  while central di(cid:29)erencing was used for the C2
network  except at the output layer where backward di(cid:29)erencing was used  and at the input (cid:27)rst
order smoothness was used as forward di(cid:29)erencing violates causality.
In Figure 2c one can see that for the C2 network the red and blue data sets pass over each other
in layers 4  5 and 6. This can be understood as the C2 network has the same form as a residual
network  with an additional momentum term pushing the data past each other.

7

(a) A batch size of 300 for untangling data. As early as layer 4 the input connected sets have been disconnected
and the data are untangled in an unintuitive way. This means a more complex coordinate representation of
the data manifold was learned.

(b) A batch size of 1000 for untangling data. Because the large batch size can well-sample the data manifold  the
spiral sets stay connected and are untangled in an intuitive way. This means a simple coordinate representation
of the data manifold was learned.
Figure 4: The e(cid:29)ect of batch size on coordinate representation learned by the same 2-dimensional
C1 network  where layer 0 is the input representation  and both examples achieve 0% error. A basic
theorem in topology says continuous functions map connected sets to connected sets. A small batch
size of 300 during training sparsely samples from the connected manifold and the network learns
over(cid:27)tted coordinate representations. With a larger batch size of 1000 during training the network
learns a simpler coordinate representation and keeps the connected input connected throughout.

7.2 Coordinate representations of the data manifold and metric tensor

As described in Sections 4 and 5  the network is learning a sequence of non-linear coordinate
transformations  beginning with Cartesian coordinates  to (cid:27)nd a coordinate representation of the
data manifold that well represents the data  and this representation tends to be (cid:30)at. This process
can be visualized in Figure 3. This experiment used a C1 (residual) network and so the group actions
on the principal and associated bundles act approximately smoothly along the (cid:27)bres of the bundles.
In the forward direction  beginning with Cartesian coordinates  a sequence of C1 di(cid:29)erential coordi-
nate transformations is applied to (cid:27)nd a nonlinear coordinate representation of the data manifold
such that in the output coordinates the classes satisfy the cost restraint. In the reverse direction 
starting with a standard Euclidean metric at the output  Equation 8  the coordinate representation
of the metric tensor is backpropagated through the network to the input by Equations 9-10 to (cid:27)nd
the metric tensor representation in the input Cartesian coordinates. The principal components of
the metric tensor are used to draw the ellipses in Figure 3.

7.3 E(cid:29)ect of batch size on set connectedness and topology

A basic theorem in topology says that continuous functions map connected sets to connected sets.
However  in Figure 4a it is seen that as early as layer 4 the continuous neural network is breaking
the connected input set into disconnected sets. Additionally  and although it achieves 0% error 
it is learning very complicated and unintuitive coordinate transformations to represent the data
in a linearly separable form. This is because during training with a small batch size of 300 in
the stochastic gradient descent search  the underlying manifold was not su(cid:28)ciently sampled to
represent the entire connected manifold and so it seemed disconnected.
This is compared to Figure 4b in which a larger batch size of 1000 was used and was su(cid:28)ciently
sampled to represent the entire connected manifold  and the network was also able to achieve 0%
error. The coordinate transformations learned by the neural network with the larger batch size
seem to more intuitively untangle the data in a simpler way than that of Figure 4a. Note that this
experiment is in 2-dimensions  and with higher dimensional data the issue of batch size and set
connectedness becomes exponentially more important by the curse of dimensionality.

8

layer 0layer 2layer 4layer 6layer 8layer 10layer 12layer 14layer 16layer 18layer 20layer 22layer 24layer 26layer 28layer 30layer 32layer 34layer 36layer 38layer 40layer 42layer 44layer 46layer 48layer 50layer 0layer 2layer 4layer 6layer 8layer 10layer 12layer 14layer 16layer 18layer 20layer 22layer 24layer 26layer 28layer 30layer 32layer 34layer 36layer 38layer 40layer 42layer 44layer 46layer 48layer 50(a) A 10 layer C1 network struggles to separate the spirals and has 1% error rate.

(b) A 20 layer C1 network is able to separate the spirals and has 0% error rate.

(c) A 40 layer C1 network is able to separate the spirals and has 0% error rate.

Figure 5: The e(cid:29)ect of number of layers on the separation process of a C1 neural network. In
Figure 5a it is seen that the ∆l is too large to properly separate the data. In Figures 5b and 5c the
∆l is su(cid:28)ciently small to separate the data. Interestingly  the separation process is not as simple as
merely doubling the parameterization and halving the partitioning in Equation 7 because this is
a nonlinear system of ODE’s. This is seen in Figures 5b and 5c; the data are at di(cid:29)erent levels of
separation at the same position of layer parameterization  for example by comparing layer 18 in
Figure 5b to layer 36 in Figure 5c.

7.4 E(cid:29)ect of number of layers on the separation process
This experiment compares the process in which 2-dimensional C1 networks with 10  20 and 40
layers separate the same data  thus experimenting on the ∆l in the partitioning of Equation 7  as
seen in Figure 5. The 10 layer network is unable to properly separate the data and achieves a 1%
error rate  whereas the 20 and 40 layer networks both achieve 0% error rates. In Figures 5b and 5c
it is seen that at same positions of layer parameterization  for example layers 18 and 36 respectively 
the data are at di(cid:29)erent levels of separation. This implies that the partitioning cannot be interpreted
as simply as halving the ∆l when doubling the number of layers. This is because the system of
ODE’s are nonlinear and the ∆l is implicit in the weight matrix.

8 Conclusions

This paper forms part of an attempt to construct a formalized general theory of neural networks as a
branch of Riemannian geometry. In the forward direction  and starting in Cartesian coordinates  the
network is learning a sequence of coordinate transformations to (cid:27)nd a coordinate representation of
the data manifold that well encodes the data  and experimental results suggest this imposes a (cid:30)atness
constraint on the metric tensor in this learned coordinate system. One can then backpropagate the
coordinate representation of the metric tensor to (cid:27)nd its form in Cartesian coordinates. This can be
used to de(cid:27)ne an − δ relationship between the input and output data. Coordinate backpropagation
was formulated in a formal  abstract sense in terms of Lie Group actions on the metric (cid:27)bre bundle.
The error backpropagation algorithm was then formulated in terms of Lie group actions on the
frame bundle. For a residual network in the limit  the Lie group acts smoothly along the (cid:27)bres of the
bundles. Experiments were conducted to con(cid:27)rm and better understand aspects of this formulation.

9 Acknowledgements

This work has been supported in part by the U.S. Air Force O(cid:28)ce of Scienti(cid:27)c Research (AFOSR)
under Grant No. FA9550-15-1-0400. The (cid:27)rst author has been supported by PSU/ARL Walker Fel-
lowship. Any opinions  (cid:27)ndings and conclusions or recommendations expressed in this publication
are those of the authors and do not necessarily re(cid:30)ect the views of the sponsoring agencies.

9

layer 0layer 1layer 2layer 3layer 4layer 5layer 6layer 7layer 8layer 9layer 10layer 0layer 2layer 4layer 6layer 8layer 10layer 12layer 14layer 16layer 18layer 20layer 0layer 4layer 8layer 12layer 16layer 20layer 24layer 28layer 32layer 36layer 40References
[1] Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry  volume 191. American

Mathematical Soc.  2007.

[2] Frédéric Bastien  Pascal Lamblin  Razvan Pascanu  James Bergstra  Ian Goodfellow  Arnaud
Bergeron  Nicolas Bouchard  David Warde-Farley  and Yoshua Bengio. Theano: new features
and speed improvements. arXiv preprint arXiv:1211.5590  2012.

[3] Yoshua Bengio  Grégoire Mesnil  Yann Dauphin  and Salah Rifai. Better mixing via deep
In Proceedings of the 30th International Conference on Machine Learning

representations.
(ICML-13)  pages 552–560  2013.

[4] Alexandre Boulch. Sharesnet: reducing residual network parameter number by sharing

weights. arXiv preprint arXiv:1702.08782  2017.

[5] Xavier Glorot and Yoshua Bengio. Understanding the di(cid:28)culty of training deep feedforward

neural networks. In Aistats  volume 9  pages 249–256  2010.

[6] Alex Graves  Greg Wayne  and Ivo Danihelka. Neural turing machines. arXiv preprint

arXiv:1410.5401  2014.

[7] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 770–778  2016.

[8] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Identity mappings in deep residual

networks. In European Conference on Computer Vision  pages 630–645. Springer  2016.

[9] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation 

9(8):1735–1780  1997.

[10] Alex Krizhevsky  Ilya Sutskever  and Geo(cid:29)rey E Hinton. Imagenet classi(cid:27)cation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[11] Yann LeCun  Yoshua Bengio  and Geo(cid:29)rey Hinton. Deep learning. Nature  521(7553):436–444 

2015.

[12] John M Lee. Smooth manifolds. In Introduction to Smooth Manifolds  pages 1–29. Springer 

2003.

[13] Tomas Mikolov  Wen-tau Yih  and Geo(cid:29)rey Zweig. Linguistic regularities in continuous space

word representations. In Hlt-naacl  volume 13  pages 746–751  2013.

[14] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear

embedding. science  290(5500):2323–2326  2000.

[15] David E Rumelhart  Geo(cid:29)rey E Hinton  and Ronald J Williams. Learning internal representa-

tions by error propagation. Technical report  DTIC Document  1985.

[16] Theano Development Team. Theano: A Python framework for fast computation of mathemat-

ical expressions. arXiv e-prints  abs/1605.02688  May 2016.

[17] Andreas Veit  Michael J Wilber  and Serge Belongie. Residual networks behave like ensembles
of relatively shallow networks. In Advances in Neural Information Processing Systems  pages
550–558  2016.

[18] Gerard Walschap. Metric structures in di(cid:29)erential geometry  volume 224. Springer Science &

Business Media  2012.

[19] Matthew D Zeiler  Dilip Krishnan  Graham W Taylor  and Rob Fergus. Deconvolutional
networks. In Computer Vision and Pattern Recognition (CVPR)  2010 IEEE Conference on  pages
2528–2535. IEEE  2010.

10

,Kenji Kawaguchi
Leslie Kaelbling
Michael Hauser
Asok Ray
Hamid JALALZAI
Stephan Clémençon
Anne Sabourin