2019,Explicit Planning for Efficient Exploration in Reinforcement Learning,Efficient exploration is crucial to achieving good performance in reinforcement learning. Existing systematic exploration strategies (R-MAX  MBIE  UCRL  etc.)  despite being promising theoretically  are essentially greedy strategies that follow some predefined heuristics. When the heuristics do not match the dynamics of Markov decision processes (MDPs) well  an excessive amount of time can be wasted in travelling through already-explored states  lowering the overall efficiency. We argue that explicit planning for exploration can help alleviate such a problem  and propose a Value Iteration for Exploration Cost (VIEC) algorithm which computes the optimal exploration scheme by solving an augmented MDP. We then present a detailed analysis of the exploration behaviour of some popular strategies  showing how these strategies can fail and spend O(n^2 md) or O(n^2 m + nmd) steps to collect sufficient data in some tower-shaped MDPs  while the optimal exploration scheme  which can be obtained by VIEC  only needs O(nmd)  where n  m are the numbers of states and actions and d is the data demand. The analysis not only points out the weakness of existing heuristic-based strategies  but also suggests a remarkable potential in explicit planning for exploration.,Explicit Planning for Efﬁcient Exploration in

Reinforcement Learning

Liangpeng Zhang1  Ke Tang2  and Xin Yao2 1∗

1CERCIA  School of Computer Science  University of Birmingham  U.K.

2Shenzhen Key Laboratory of Computational Intelligence 

University Key Laboratory of Evolving Intelligent Systems of Guangdong Province 

Department of Computer Science and Engineering 

Southern University of Science and Technology  Shenzhen 518055  China

L.Zhang.7@pgr.bham.ac.uk  tangk3@sustc.edu.cn  xiny@sustc.edu.cn

Abstract

Efﬁcient exploration is crucial to achieving good performance in reinforcement
learning. Existing systematic exploration strategies (R-MAX  MBIE  UCRL  etc.) 
despite being promising theoretically  are essentially greedy strategies that follow
some predeﬁned heuristics. When the heuristics do not match the dynamics of
Markov decision processes (MDPs) well  an excessive amount of time can be
wasted in travelling through already-explored states  lowering the overall efﬁciency.
We argue that explicit planning for exploration can help alleviate such a problem 
and propose a Value Iteration for Exploration Cost (VIEC) algorithm which com-
putes the optimal exploration scheme by solving an augmented MDP. We then
present a detailed analysis of the exploration behaviour of some popular strategies 
showing how these strategies can fail and spend O(n2md) or O(n2m + nmd)
steps to collect sufﬁcient data in some tower-shaped MDPs  while the optimal
exploration scheme  which can be obtained by VIEC  only needs O(nmd)  where
n  m are the numbers of states and actions and d is the data demand. The analysis
not only points out the weakness of existing heuristic-based strategies  but also
suggests a remarkable potential in explicit planning for exploration.

1

Introduction

In reinforcement learning (RL)  exploration plays a key role in deciding the quality of data and thus
has a direct impact to the overall performance. Simple exploration strategies such as ε-greedy may
need exponentially many steps to ﬁnd a (near-)optimal policy [1]. On the other hand  more systematic
exploration strategies (R-MAX  UCRL  MBIE and their variants) have far promising theoretical
performance guarantees (see e.g. [2  3  4  5]). Recently  some of these systematic strategies have been
successfully generalised and applied to deep reinforcement learning  achieving good performance in
domains that are known to be hard to explore  such as Montezuma’s Revenge [6  7].
Systematic exploration strategies are carefully designed to ensure that sufﬁcient data is collected for
every unknown states  so that the chance of converging to undesirable policies due to ignorance is
controlled. Unfortunately  the actual data collection process is less carefully executed  in the sense
that these strategies choose actions simply by maximising some predeﬁned heuristics. When the
design of such heuristics does not match the properties of the learning problem well  an excessive
amount of less useful data will be collected due to revisiting well-explored states/actions.

∗The corresponding author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

A straightforward example is as follows. Suppose both a nearby Area1 and a distant Area2 need to be
explored. The transition dynamics makes it easy to travel from Area2 to Area1  but trying to move
from Area1 to Area2 sends the agent back to Area1 with high probability. Clearly  exploring in the
order of Area2→ Area1 is better than Area1→ Area2  since the latter wastes additional time in trying
to travel to Area2 from Area1 which leads to excessive data being collected in Area1. However  most
systematic strategies choose to explore Area1 ﬁrst because it is nearer than Area2 and thus has a
higher heuristic score. We call this a distance trap.
Our analysis in this paper points out that there exist cases where these heuristic-based strategies need
either O(n2md) or O(n2m + nmd) steps to collect sufﬁcient data  while an optimal exploration
scheme only needs O(nmd)  where n  m  and d denote number of states  number of actions  and the
minimum amount of data to be obtained at each state-action pair  respectively. Since n is usually very
large in real-world problems  this result indicates that a signiﬁcant amount of steps can be wasted by
the heuristic-based strategies due to their careless execution of data collection. It also suggests that
explicit planning for exploration can be highly beneﬁcial for improving learning efﬁciency.
The contributions of this paper are as follows.

1. Formulate the planning for exploration problem as an augmented undiscounted Markov
decision process and show that the optimal exploration scheme can be discovered by solving
the Bellman optimality equations for exploration costs.

2. Propose a Value Iteration for Exploration Cost (VIEC) algorithm for ﬁnding the optimal

exploration scheme.

3. Point out two weaknesses of existing systematic exploration strategies: (a) distance traps and
(b) reward traps  and use tower MDPs as examples to give a concrete explanation about how
existing strategies can fail and need O(n2md) or O(n2m + nmd) steps while the optimal
exploration scheme needs only O(nmd) steps to fulﬁl the same exploration demand.

2 Preliminaries

In this paper we follow the common formulation of reinforcement learning [8] in which M =
(S A  P  R  γ) represents a ﬁnite discounted Markov decision process (MDP) with set of states S 
set of actions A  transition probability function P   reward function R  and discount factor γ. Unless
otherwise stated  we use n and m to denote the number of states and actions of an MDP. A policy is
denoted π and its value functions are denoted V π(s) and Qπ(s  a)  while for optimal policy we write
π∗  V ∗ and Q∗  which by deﬁnition satisfy V ∗(s) = maxπ V π(s) and Q∗(s  a) = maxπ Qπ(s  a)
for all s ∈ S and a ∈ A. If exact information about M is available  then π∗ can be obtained by
s(cid:48) P (s(cid:48)|s  a)V ∗(s(cid:48))) or Q∗(s  a) =

solving the Bellman equations V ∗(s) = maxa(E[R(s  a)] + γ(cid:80)
E[R(s  a)] + γ(cid:80)

s(cid:48) P (s(cid:48)|s  a) maxa(cid:48) Q∗(s(cid:48)  a(cid:48)) using Value Iteration algorithm [9].

In reality M is often unknown and needs to be estimated from the data collected during learning. A
straightforward way is to use ˆP (s(cid:48)|s  a) = N (s  a  s(cid:48))/N (s  a) and ˆR(s  a) = C(s  a)/N (s  a) as
estimates of P (s(cid:48)|s  a) and E[R(s  a)]  where N (s  a) and N (s  a  s(cid:48)) indicate the occurrences of
choice (s  a) and transition (s  a  s(cid:48)) and C(s  a) is the sum of the rewards collected at (s  a). As
N (s  a) → ∞ at all (s  a)  this model ˆM of M converges in probability to the true M  and thus we
can eventually obtain π∗ of M from ˆM. Such process is called model-based RL.
Researches on systematic exploration are often based on model-based RL  so that the quality of
learning is mostly decided by their exploration strategies. This paper follows this idea and limits its
scope to the model-based case  but its general suggestion (explicit planning for exploration can be
beneﬁcial) is also applicable to model-free RL.

3 Formulation of the Planning for Exploration Problem

3.1 Data demands

Since the goal of learning is to ﬁnd out a sufﬁciently good policy rather than to have an extremely
accurate estimate of V or Q  a ﬁnite amount of data is often sufﬁcient for the purpose. Various
researches have shown that by applying Hoeffding’s or Chernoff’s inequalities  the minimum amount

2

1

ε2(1−γ)4 (n+ln nm

of data needed at each state-action pair for guaranteeing certain learning quality can be derived. For
δ )) data for each state-action pair is sufﬁcient
example  [2  10] proved that some O(
for R-MAX to be (ε  δ)-PAC  while [4] proved that for MBIE it is O(
ε(1−γ)δ )) 
where n and m are the number of states and actions.
In practice  the theoretical demands of this kind are still likely to be excessive (see e.g. [11  12  13]) 
and users usually have to specify how much data to be collected based on their domain knowledge or
trial-and-error. Whichever the case  the main idea is that such data demands are given (either directly
or indirectly) by the parameter settings prior to the actual learning process  and thus can be used to
make plans for more efﬁcient exploration. The formal deﬁnition of data demands is as follows.
Deﬁnition 3.1 In an MDP with n states and m actions  a demand matrix D is an n × m matrix in
which entry D[s  a] = k ≥ 0 indicates that at least k more data should be collected for state-action
pair (s  a) during learning.

nm

1

ε2(1−γ)4 (n + ln

We write Dt to indicate the demand matrix at time t during learning. After some action At is executed
at some state St  the corresponding entry in the demand matrix should be subtracted by 1 unless it is
already 0  while other entries remain unchanged  that is 

(cid:26)max{0  Dt[s  a] − 1}

Dt+1[s  a] =

Dt[s  a]

(s  a) = (St  At)
otherwise.

For convenience  we deﬁne the demand reduction function H as follows:

(cid:26)D − es a D[s  a] > 0

D

D[s  a] = 0 

H(D; s  a) :=

where es a is an n × m matrix ﬁlled with 0 except for the only nonzero entry es a[s  a] = 1. Then
we can express the change of Dt after (St  At) simply as Dt+1 = H(Dt; St  At).
The demand space (the set of all possible demand matrix) of an MDP is denoted D. It is reasonable
to assume that the demands at every state-action never exceed some sufﬁciently large positive integer
d  thus the size of demand space is at most (d + 1)nm.
Remark. Readers may wonder how to ﬁnd out the “optimal” demand matrix (that has e.g. the least
total demand) for a given learning task. Such matrix can only be obtained with full knowledge of
the MDP  and thus is impractical to obtain in reality. Our point is that given any demand matrix  the
exploration efﬁciency can be improved via planning. It is achieved by minimising the amount of data
collected beyond the speciﬁed demand (i.e. optimal exploration scheme  see next section) rather than
choosing a better demand matrix  and thus the optimality of demand matrices is not the main concern
of this paper.

3.2 Planning for exploration

Demand matrix D indicates how much data is sufﬁcient for obtaining a good policy  and we are
interested in collecting all this required amount of data with the number of steps as small as possible 
since this means that the least amount of unnecessary data is collected beyond D. The exploration
behaviour of a learning agent can be described as an exploration scheme  while its exploration cost is
the expected number of steps needed to fulﬁl all the demands  deﬁned formally as follows.
Deﬁnition 3.2 An exploration scheme ψ is a mapping D × S (cid:55)→ A  where ψ(D; s) = a indicates
that action a should be taken at state s when the demand matrix is D.

Deﬁnition 3.3 The exploration cost C ψ(D; s  a) is the expected time t that the current demand Dt
ﬁrst becomes the all-zero matrix 0 by starting from (s  a) and following ψ  i.e. C ψ(D; s  a) :=
E[inf{t : Dt+1 = 0|D1 = D  S1 = s  A1 = a  Ak = ψ(Dk; Sk) ∀k > 1}].
Given MDP M and exploration scheme ψ  the interaction process becomes a Markov process with
augmented state space D × S and transition probability Pr(D(cid:48)  s(cid:48)|D  s) = P (s(cid:48)|s  ψ(D; s)) for
D(cid:48) = H(D; s  ψ(D; s)) and 0 otherwise. As for the exploration cost  by deﬁnition when D = 0
we have C ψ(D; s  a) = 0 for any (s  a). Any step after Dt = 0 will not result in any exploration

3

cost  while each step before reaching Dt = 0 will increase the cost by 1 uniformly. Therefore  the
planning for exploration problem is an augmented undiscounted MDP  and the following Bellman
equation holds for the exploration cost:

s(cid:48)∈S P (s(cid:48)|s  a) C ψ(cid:0)H(D; s  a); s(cid:48)  ψ(cid:0)H(D; s  a); s(cid:48)(cid:1)(cid:1) D(cid:54)=0

(cid:26)1 +(cid:80)

C ψ(D; s  a) =

0

D=0.

(1)

Let Ψ be the set of all possible exploration schemes for a given MDP. Since less exploration cost is
more desirable  the deﬁnition for the optimal scheme is as follows.
Deﬁnition 3.4 An optimal exploration scheme ψ∗ ∈ Ψ is the one that satisﬁes C ψ∗
minψ∈Ψ C ψ(D; s  a) for any D ∈ D  s ∈ S and a ∈ A.
simply as C∗. In strongly connected
For convenience we write the optimal exploration cost C ψ∗
MDPs  it can be shown that similar to optimal value functions Q∗ and V ∗  the optimal exploration
cost C∗ exists and is unique. In MDPs that are not strongly connected  on the other hand  there exist
cases where some demands are not satisﬁable. For example  in an MDP with two states {s1  s2}
and one action a which transits the agent to s2 with probability 1 from both s1 and s2  a demand
D[s1  a] > 1 can never be satisﬁed and will lead to an inﬁnite exploration cost. However  as discussed
in Section 3.1  since users more or less have control to the exploration demands  in the rest of this
paper we assume that they do not assign unsatisﬁable demands and thus C∗ exists.

(D; s  a) =

3.3 Computing ψ∗
By combining Equation 1 with Deﬁnition 3.4 we get the Bellman optimality equation for C∗:

(cid:26)1 +(cid:80)

0

C∗(D; s  a) =

s(cid:48)∈S P (s(cid:48)|s  a) mina(cid:48)∈A C∗(H(D; s  a); s(cid:48)  a(cid:48)) D(cid:54)=0
D=0.

(2)

an input demand matrix Din  we can easily arrange all k =(cid:81)

Since this equation has structure similar to the original Bellman optimality equation for Q∗ and V ∗ 
we can modify Value Iteration to compute C∗. Note that H(D; s  a) ≤ D for any D  s and a  given
s a(Din[s  a] + 1) demand matrices
satisfying D ≤ Din by topological ordering  i.e. D(0) = (0...0; ...; 0...0)  D(1) = (0...0; ...; 0...1)  ... 
D(k−1) = Din  and compute C∗ from D(0) to D(k−1) to avoid extra iterations on D.
The pseudocode of the Value Iteration for Exploration Cost (VIEC) is presented in Algorithm 1 
where U (D; s) := mina C(D; s  a)  which plays a role similar to V (s) in computing Q(s  a).

repeat

Algorithm 1 Value Iteration for Exploration Cost (VIEC)
Input: Demand matrix Din  transition P
Output: Exploration scheme ψ
1: Initialise all C(D; s  a) = 0  U (D; s) = 0
s a(Din[s  a] + 1) − 1 do

2: for i = 1 to(cid:81)

∆ = 0
for s ∈ S do

for a ∈ A do

c = 1 +(cid:80)

s(cid:48) P (s(cid:48)|s  a) U(cid:0)H(D(i); s  a); s(cid:48)(cid:1)

3:
4:
5:
6:
7:
8:
9:
10:
11:
12: Output ψ such that ψ(D; s) = argminaC(D; s  a)

∆ = max{∆  |C(D(i); s  a) − c|}
C(D(i); s  a) = c

U (D(i); s) = mina C(D(i); s  a)

until ∆ < threshold

Similar to the original Value Iteration  with a sufﬁciently small stopping threshold in Line 11  C
converges to C∗ and thus the output ψ → ψ∗. The proof can be obtained straightforwardly from the
convergence proof of original Value Iteration and will not be elaborated here.

4

VIEC needs to iterate over(cid:81)

s a(Din[s  a] + 1) = O(dnm) demand matrices and is not computation-
ally efﬁcient in practice. Unfortunately  this is unavoidable for computing ψ∗ because even in the
simplest case with deterministic transitions and demands no more than 1  it is a Rural Postman Prob-
lem [14] which is NP-hard  thus solving in polynomial time is impossible unless P=NP. Nevertheless 
an approximation to ψ∗ might be sufﬁcient for the purpose  and we leave this to the future work.
In most RL settings  the transition function P is not known to the learning algorithm in prior. In
this case  one possible choice is to use estimated transition ˆP instead  following an iterative process
shown in Algorithm 2. In this case  output ψ of VIEC is an optimal exploration scheme for the
environment model ˆM rather than the true environment M. With more data been collected  ˆM gets
closer to M and thus ψ becomes closer to the true ψ∗. While a ψ improving over time is surely not
as good as ψ∗  it is still better than never conduct planning  and thus Algorithm 2 should provide a
relatively efﬁcient way of exploration in general.

Algorithm 2 Model-based RL with Planning for Exploration
Input: Initial demand D1
Output: Policy π
1: Initialise ˆP   ˆR randomly or based on prior knowledge
2: ψ = VIEC(D1  ˆP )
3: repeat
4:
5:
6:
7:
8: until Dt = 0 or π is sufﬁciently good

Collect data by following ψ
Update ˆP   ˆR  Dt using collected data
Update ψ using VIEC(Dt  ˆP )
Update π using Value Iteration( ˆP   ˆR)

4 When and How Heuristics Fail and Explicit Planning Helps

Systematic exploration strategies choose action by maximising some predeﬁned heuristics ˜Q(s  a) 
which is prone to the traps as follows. Suppose at current state St and demand Dt there are actions
a1  a2 satisfying C(Dt; St  a1) < C(Dt; St  a2)  so one should choose a1 over a2.
Distance traps. Let the nearest (in terms of expected number of steps to arrive) to-be-explored state
after taking a1 and a2 be s(cid:48) and s(cid:48)(cid:48) respectively. If s(cid:48)(cid:48) is closer to St than s(cid:48)  then the uncertainty of
s(cid:48)(cid:48) is less discounted than s(cid:48) in ˜Q  resulting in ˜Q(St  a1) < ˜Q(St  a2) and thus a2 is picked.
Reward traps. Let the reward (or expected return) of taking a1 and a2 be r(cid:48) and r(cid:48)(cid:48)  respectively.
Then r(cid:48) < r(cid:48)(cid:48) can lead to ˜Q(St  a1) < ˜Q(St  a2) and thus a2 is picked.
These traps can appear in any MDP and signiﬁcantly reduce the efﬁciency of heuristic-based strategies.
To present this more clearly and intuitively  we introduce a class of MDPs called tower MDPs 
analyse the behaviours and exploration costs of several typical exploration strategies and the optimal
exploration scheme in tower MDPs  and then discuss the implication of the results.

4.1 Tower MDPs

1  ...  s(cid:48)

A tower MDP of height h has two groups of states  namely upward states s1  ...  sh and downward
states s(cid:48)
h. The total number of states is n = 2h. The agent always starts interaction from s1.
An example with height h = 5 is shown in Figure 1.
The transitions are deterministic in tower MDPs. Each upward state sk has an action a(cid:48) that transits
the agent to s(cid:48)
k (dashed arrows in Figure 1)  and also an action a that transits to sk+1 if k < h (solid
arrows). Each downward state s(cid:48)
k is an m-armed bandit  which has m actions a1  ...  am that yield
rewards following some predeﬁned distributions and transit the agent to s(cid:48)
k−1 (k > 1) or s1 (k = 1)
(collectively drawn as the double arrows in Figure 1).
To ﬁnd out the optimal policy  the agent has to collect data in these bandits for information about their
reward distributions. For simplicity we assume that the initial demands at each of these m-armed

5

a(cid:48)

a(cid:48)

a(cid:48)

a(cid:48)

a(cid:48)

a

a

a

a

5

4

3

2

1

a1  ...  am

5(cid:48)

4(cid:48)

3(cid:48)

2(cid:48)

1(cid:48)

a1  ...  am

a1  ...  am

a1  ...  am

a1  ...  am

Figure 1: A tower MDP of height h = 5. Each double arrow represents an m-armed bandit.

bandits are uniformly set to some d > 0. As for a and a(cid:48) in upward states  since there is no uncertainty
at all  their initial demands are set to 0.

4.2 Optimal exploration scheme

In a tower MDP of height h with m-armed bandits in downward states  it is easy to see that the
optimal scheme to collect d data at each arm is to repeatedly take the closed path [s1s2...shs(cid:48)
1s1].
Each time taking this path  the demand of one arm at every downward state is reduced by 1  and thus
it needs to be repeated md times to collect all the data required. Since the length of this path is 2h 
the optimal exploration scheme needs 2hmd = O(nmd) steps to fully satisfy the initial demands.

h...s(cid:48)

4.3

ε-greedy

Although ε-greedy is already well-known for its lack of efﬁciency  it is nevertheless interesting to see
how it performs in tower MDPs. Let the bandit in s(cid:48)
1 gives a reward of 1 with probability 1 on all of
its m arms  am in s(cid:48)
h gives reward 1010 with probability 0.01 and reward 0 otherwise  while all other
bandits/arms give zero reward. At the beginning of learning  ε-greedy does not know any of these
rewards  and thus has a 50-50 chance to choose between going to state s2 and s(cid:48)
1. If it chooses s2 
then it has another 50-50 chance between s3 and s(cid:48)
2  and so on. Therefore  the probability it arrives at
sh without visiting any of s(cid:48)
1  ...  s(cid:48)
h−1 is 0.5h−1. If it ever goes to any of state s(cid:48)
1  ...  s(cid:48)
h−1 before
arriving at sh  which happens with probability 1 − 0.5h−1  it will be aware of the reward at s(cid:48)
1  and
thereafter be trapped to going to s(cid:48)
1 as often as possible. Whenever it gets back to s1  it only has
probability (0.5ε)h−1 to randomly wanders into s(cid:48)
h.
Therefore  the average number of steps ε-greedy spends to visit s(cid:48)
0.5h−1)O(
fully fulﬁl the demands  the exploration cost of ε-greedy is O(nmd 2n).

(0.5ε)h−1 ) = O(n2n) if ε is seen as a constant. Since it needs to visit s(cid:48)

h

h once is 0.5h−1 · 2h + (1 −
h (md) times to

4.4 R-MAX

R-MAX [15] is one of the ﬁrst systematic strategies that are proved to have polynomial sample
complexity upper bounds [2  10]. Many exploration strategies are designed based on R-MAX and
have similar performance guarantees  including Delayed Q-learning [16]  MoR-MAX [17]  V-MAX
[18]  and ICR [13]  just to name a few.
R-MAX works as follows. When a state-action pair has a positive demand to fulﬁl  it is labelled
“unknown” and its estimated value ˜Q(s  a) is set to Vmax := Rmax
1−γ   where Rmax is the maximum
possible reward. If its demand is already 0  then it is labelled “known” and the algorithm uses the
Bellman equation to estimate its ˜Q(s  a). R-MAX always chooses the action with maximum ˜Q(s  a).
In tower MDPs  all actions in downward states are initially “unknown” and thus their ˜Q = Vmax at the
beginning of learning. Let the bandits at all states except s(cid:48)
h give zero reward  while the bandit at s(cid:48)
h
gives reward Rmax = 1 with probability 0.1 and reward 0 otherwise for all arms. Under such setting 
R-MAX will not be aware of any positive rewards until s(cid:48)
h is explored. It can be shown recursively

6

that at this stage of learning  at any upward state sk  R-MAX will choose a(cid:48) to go to s(cid:48)
k rather than a
that goes to sk+1. Concretely  at sh  the only choice is a(cid:48) which leads to “unknown” actions in s(cid:48)
h  thus
has ˜Q(sh  a(cid:48)) = γVmax. At state sh−1  going to sh has value ˜Q(sh−1  a) = γ ˜Q(sh  a(cid:48)) = γ2Vmax 
while going to s(cid:48)
h−1 has ˜Q(sh−1  a(cid:48)) = γVmax > ˜Q(sh−1  a)  thus R-MAX will choose a(cid:48) at sh−1
as well. The same happens at every state from sh−1 down to s1. Since the agent starts from state s1 
R-MAX will stick to [s1s(cid:48)
After collecting sufﬁcient data at state s(cid:48)
starts choosing a at s1. Since ˜Q at states other than s1 and s(cid:48)
of exploration due to having the least discount in ˜Q. This leads to a behaviour of taking [s1s2s(cid:48)
to collect at s(cid:48)
3  and so on  and ﬁnally s(cid:48)
process is 2md + 4md + ... + (2h)md = h(h + 1)md = O(n2md).

1  ˜Q(s1  a) drops greatly from γVmax to γ4Vmax and R-MAX
2 is the next target
1s1]
h. The exploration cost of such

1 are tried d times and become “known”.

1s1] until all a1  ...  am at s(cid:48)

1 remain unchanged  s(cid:48)

2  then [s1...s3s(cid:48)

1s1] for s(cid:48)

3...s(cid:48)

2s(cid:48)

4.5

Interval estimation

β√

N (s a)

β√

N (s a)

γ(cid:80)

s(cid:48) ˆP (s(cid:48)|s  a) maxa(cid:48) ˜Q(s(cid:48)  a(cid:48))  where ˜R(s  a) := ˆR(s  a) +

Interval estimation (IE) based exploration strategies utilise statistical methods to create conﬁdence
intervals (CIs) for the estimated models or state/action values. CIs computed by this type of strategies
usually take the form of X(s  a) ±
  where X(s  a) is the variable being estimated  β is a
parameter  and N (s  a) is the amount of data collected at (s  a). Clearly  state-action pairs with less
data have longer CIs  and vice versa. Estimated variable X(s  a) can be transition probability  reward 
or state/action values. When choosing actions  the action with highest estimated value among all
possible MDP models that lie within the CIs is selected.
In this section we take MBIE-EB as example to show how IE-based strategies can be tricked to
make inferior decisions. In MBIE-EB  action values are estimated using ˜Q(s  a) = ˜R(s  a) +
. Since N (s  a) = 0 leads to
division by zero  in the following analysis we assume that they all start with 1. At each step the action
with highest ˜Q(St  a) is executed  thus ˜Q is the heuristic used in MBIE-EB.
We start our analysis with the simplest case m = 1 where all bandits in the tower MDP is one-armed.
The expression of ˜Q can be obtained by solving the Bellman equation. Note that although max
operator is involved on all state-action pairs  the algorithm is essentially choosing between paths
[s1...sjs(cid:48)
j  and
Nj be N (s  a) at that bandit  then we have ˜Qj = γj
1−γ2j
Let the actual reward of the bandits be the same as the settings used in the analysis of R-MAX. At the
1+γj ). Clearly ˜Q1 > ˜Q2 > ... > ˜Qh 

beginning of learning ˜Rj = β/(cid:112)Nj = β  thus ˜Qj = β

1s1] with different j. Let ˜Qj be ˜Q for the j-th path  ˜Rj be ˜R for the bandit at s(cid:48)

(cid:80)j
i=1 γj−i ˜Ri.
1−γ (1− 1

1s1]  which increases N1 and reduces ˜R1.

(cid:80)j
thus MBIE-EB starts with path [s1s(cid:48)
i=1 γj−i ˜Ri shows that ˜Qj with larger j has a greater discount γj−i
The expression ˜Qj = γj
1−γ2j
on ˜Ri  and thus exploring s(cid:48)
1 reduces ˜Qj less for larger j. Therefore  ˜Q2 will eventually surpass ˜Q1
and MBIE-EB moves to exploring s(cid:48)
3  and so on  leading to an exploration behaviour similar
to R-MAX  but lingers less at the same state than R-MAX. A smaller discount factor γ leads to a
larger gap between different ˜Qj  which then leads to a slower pace for MBIE-EB to move upward.
In the case where MBIE-EB only lingers exactly once at each level of the tower  it will take path
[(s1s(cid:48)
1)...] until sh is reached. Thereafter ˜Qh will always be the largest 
and thus the remaining demand will be fulﬁlled through repeating [s1...shs(cid:48)
1s1]. Such behaviour
has exploration cost (2 + 4 + ... + 2h) + 2h(d − 1) = h(h + 1) + 2h(d − 1) = O(n2 + nd) steps.
2 ≈ 0.618 is sufﬁcient to make
5−1
For the sake of space we skip the full derivation here2  but a γ <
sure that MBIE-EB will perform as bad as this3.

1)(s1s2s3s(cid:48)

1)(s1s2s(cid:48)

2  then s(cid:48)

h...s(cid:48)

j...s(cid:48)

2s(cid:48)

3s(cid:48)

2s(cid:48)

√

2The proof will be given in the online supplementary material.
3Note that γ < 0.618 can effectively be achieved by inserting additional dummy states into all transitions 

e.g. if γ = 0.9  by inserting 4 states between all transitions the discount becomes 0.95 = 0.59 < 0.618.

7

Strategy
Optimal scheme
ε-greedy
R-MAX
Interval estimation O(n2m+nmd)

Exploration cost Weakness
O(nmd)
O(nmd 2n)
O(n2md)

-
Distance  reward
Distance
Distance  reward

Table 1: Summary of results on tower MDPs.

Nj k

β√

where Nj k is the number of data at the k-th arm at state s(cid:48)

In the case of m ≥ 2 where there are 2 or more arms in each bandit  ˜Rj in the expression of ˜Qj
becomes the maximum of
j. As a
result  the same pattern as in m = 1 is repeated m times for the case of m ≥ 2  and thus the total
exploration cost is O(n2m + nmd).
Note that MBIE-EB and other IE-based exploration strategies also take ˆR(s  a) into consideration
when choosing actions  and thus can be further tricked by a deceiving setting of true reward R(s  a).
For example  if the setting of rewards in Section 4.3 is used  then more weight will be put into ˜R1 
which gives ˜Qj with smaller j more advantage due to having smaller discount on ˜R1. As a result 
MBIE-EB will stay at lower levels more often and thus will have a worse exploration cost than above.

4.6 Discussion

Table 1 sums up the results of the analysis. As can be seen from the exploration cost column  ε-greedy
is clearly inferior to the rest for being exponential to the number of states n. MBIE-EB is seemingly
better than R-MAX  but since in reality it often happens that n (cid:29) d  the difference between the two
can be small  and both are far worse than the optimal scheme which is only O(nmd). Such results
suggest that explicit planning for exploration can be highly beneﬁcial when the state space is large.
It is interesting to compare the exploration costs with sample complexity bounds  a well-studied
exploration efﬁciency metric. R-MAX and MBIE-EB have sample complexity upper bounds O(n2m)
(ignoring other factors and logarithms) [2  4]  which is similar to the exploration costs O(n2md) on
n and m. However  a variant of R-MAX called MoR-MAX is known to have sample complexity
O(nm) [17]  yet its exploration cost in tower MDPs is still O(n2md) due to having exactly the same
behaviour as R-MAX. This might explain why sample complexity is usually not a good indicator of
practical exploration efﬁciency.
The “distance” and “reward” in the weakness column of Table 1 refers to the distance traps and
reward traps mentioned at the beginning of Section 4. A longer distance makes ε-greedy visit states
in the higher levels via random walk less often  while for R-MAX and IE algorithms a longer distance
leads to more discount and thus a lower heuristic score ˜Q for the states in the higher levels. Reward
traps lure both ε-greedy and IE to the lower-level states  while R-MAX is more resistant to it due to
using Vmax in computing ˜Q. The optimal scheme is the result of minimising undiscounted exploration
cost and is affected by neither traps.
Tower MDPs in the above analysis only use deterministic transitions for simplicity.
In non-
deterministic cases  the negative impact of distance traps can be even more severe due to transition
probabilities amplifying the gaps in average distances. For example  if transiting from state s1 to s(cid:48)
by taking a(cid:48) has probability 1  while taking a at s1 has probability 0.5 to go to s2 and probability
1
0.5 to stay in s1  then the gap between ˜Q1 and ˜Q2 becomes larger and IE algorithms will take path
[s1s(cid:48)
MDPs in reality may not have the same structure as tower MDPs  but the distance traps and the
reward traps discussed above can happen in any types of MDPs. It is possible that in some easier
cases the difference between the optimal scheme and heuristic-based strategies is not as large as
O(nmd) vs. O(n2md)  but in domains where millions of data is required for obtaining an acceptable
policy  even a difference in constant factor can be practically signiﬁcant.
Remark on the reward trap. One may argue that whether or not the reward function actually acts
as a trap is problem-dependent  and there are cases where being trapped by the rewards is actually
desirable due to it leading to an early convergence to good policies.

1s1] more often  increasing the exploration cost.

8

This is partly true  but one should also consider the fact that it is very difﬁcult  if not impossible 
to design a reward function that simultaneously leads to both good policies and good exploration
behaviours. If the way being trapped does not coincide with the (near-)optimal policies  algorithms
like MBIE eventually deviate from the current behaviour and restart exploration. In the early stages
of learning  since the data is still lacking  such situation can occur frequently  resulting in the whole
learning process being prolonged and the total reward reduced.
Therefore  even in the case where the total reward during learning is of concern  ignoring it in the
early stages of the learning process and seeking for a more efﬁcient exploration behaviour can be
beneﬁcial in the long run.

5 Conclusion and Future Work

In this paper we have formulated the planning for exploration problem as solving augmented MDPs 
and provided the Bellman optimality equation for exploration costs. We have proposed a Value
Iteration for Exploration Cost (VIEC) algorithm which computes the optimal exploration scheme
given full knowledge of MDP  and a model-based RL method with planning-for-exploration com-
ponent integrated. We have presented a detailed study of exploration behaviours of several popular
exploration strategies. The analysis exposes the weakness of these heuristic-based strategies and
suggests a remarkable potential in planning for exploration.
A possible direction for future work is to ﬁnd a fast and sufﬁciently good approximate to VIEC. As
we pointed out in Section 3.3  since the demand space is exponential in the number of states  applying
VIEC directly can be computationally expensive in practice. Techniques such as Prioritized Sweeping
[19] may help reduce the computation involved  thus make VIEC more practically useful.
Another direction is to design better heuristic-based exploration strategies that can handle the distance
and reward traps discussed in Section 4 better. Although by No Free Lunch theorem [20] no heuristic
can perform universally better than others  it is nevertheless useful to have a larger toolbox of
easy-to-compute heuristics that can cope with different types of MDPs.

Acknowledgments

This work was supported by EPSRC (Grant Nos. EP/J017515/1 and EP/P005578/1)  the Royal
Society (through a Newton Advanced Fellowship to Ke Tang and hosted by Xin Yao)  the Program
for Guangdong Introducing Innovative and Enterpreneurial Teams (Grant No. 2017ZT07X386) 
Shenzhen Peacock Plan (Grant No. KQTD2016112514355531) and the Program for University Key
Laboratory of Guangdong Province (Grant No. 2017KSYS008).

References
[1] Steven D. Whitehead. A complexity analysis of cooperative mechanisms in reinforcement
learning. In Proceedings of AAAI 1991  pages 607–613  Palo Alto  CA  USA  1991. AAAI
Press.

[2] Sham M. Kakade. On the sample complexity of reinforcement learning. PhD thesis  University

College London  London  U.K.  2003.

[3] Peter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted reinforcement
learning. In Advances in Neural Information Processing Systems 19  pages 49–56. MIT Press 
Cambridge  MA  USA  2006.

[4] Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for
markov decision processes. Journal of Computer and System Sciences  74(8):1309–1331  2008.

[5] Tor Lattimore and Marcus Hutter. Near-optimal PAC bounds for discounted MDPs. Theoretical

Computer Science  558:125–143  2014.

[6] Marc Bellemare  Sriram Srinivasan  Georg Ostrovski  Tom Schaul  David Saxton  and Remi
Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural
Information Processing Systems  pages 1471–1479  New York  USA  2016. Curran Associates 
Inc.

9

[7] Haoran Tang  Rein Houthooft  Davis Foote  Adam Stooke  Xi Chen  Yan Duan  John Schulman 
Filip DeTurck  and Pieter Abbeel. # Exploration: A study of count-based exploration for
deep reinforcement learning. In Advances in Neural Information Processing Systems  pages
2753–2762  2017.

[8] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press 

2018.

[9] Martin Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

Wiley-Interscience  Hoboken  NJ  USA  1994.

[10] Alexander L. Strehl  Lihong Li  and Michael L. Littman. Reinforcement learning in ﬁnite

MDPs: PAC analysis. Journal of Machine Learning Research  10:2413–2444  2009.

[11] Alexander L. Strehl and Michael L. Littman. An empirical evaluation of interval estimation for
markov decision processes. In Tools with Artiﬁcial Intelligence (ICTAI)  pages 128–135  New
York  USA  2004. IEEE.

[12] J. Zico Kolter and Andrew Y. Ng. Near-Bayesian exploration in polynomial time. In Proceedings
of the 26th International Conference on Machine Learning  pages 513–520  New York  USA 
2009. ACM.

[13] Liangpeng Zhang  Ke Tang  and Xin Yao. Increasingly cautious optimism for practical PAC-
MDP exploration. In Proceedings of the 24th International Joint Conference on Artiﬁcial
Intelligence  pages 4033–4040  Palo Alto  CA  USA  2015. AAAI Press.

[14] Horst A Eiselt  Michel Gendreau  and Gilbert Laporte. Arc routing problems  part ii: The rural

postman problem. Operations research  43(3):399–414  1995.

[15] Ronen I. Brafman and Moshe Tennenholtz. R-max–a general polynomial time algorithm for
near-optimal reinforcement learning. Journal of Machine Learning Research  3:213–231  2002.

[16] Alexander L. Strehl  Lihong Li  Eric Wiewiora  John Langford  and Michael L. Littman. PAC
model-free reinforcement learning. In Proceedings of the 23rd International Conference on
Machine learning  pages 881–888  New York  USA  2006. ACM.

[17] István Szita and Csaba Szepesvári. Model-based reinforcement learning with nearly tight
In Proceedings of the 27th International Conference on

exploration complexity bounds.
Machine Learning  pages 1031–1038  New York  USA  2010. ACM.

[18] Karun Rao and Shimon Whiteson. V-max: tempered optimism for better PAC reinforcement
learning. In Proceedings of the 11th International Conference on Autonomous Agents and
Multiagent Systems  volume 1  pages 375–382  2012.

[19] Harm Van Seijen and Rich Sutton. Planning by prioritized sweeping with small backups. In
Proceedings of the 30th International Conference on Machine Learning  volume 28  pages
361–369  Atlanta  Georgia  USA  17–19 Jun 2013. PMLR.

[20] David H Wolpert and William G Macready. No free lunch theorems for optimization. IEEE

Transactions on Evolutionary Computation  1(1):67–82  1997.

10

,Liangpeng Zhang
Ke Tang
Xin Yao