2010,Tight Sample Complexity of Large-Margin Learning,We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L2 regularization: We introduce the gamma-adapted-dimension  which is a simple function of the spectrum of a distribution's covariance matrix  and show distribution-specific upper and lower bounds on the sample complexity  both governed by the gamma-adapted-dimension of the source distribution. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. The bounds hold for a rich family of sub-Gaussian distributions.,Tight Sample Complexity of Large-Margin Learning

1 School of Computer Science & Engineering  The Hebrew University  Jerusalem 91904  Israel

Sivan Sabato1 Nathan Srebro2 Naftali Tishby1

2 Toyota Technological Institute at Chicago  Chicago  IL 60637  USA
{sivan sabato tishby}@cs.huji.ac.il  nati@ttic.edu

Abstract

We obtain a tight distribution-speciﬁc characterization of the sample complex-
ity of large-margin classiﬁcation with L2 regularization: We introduce the
γ-adapted-dimension  which is a simple function of the spectrum of a distribu-
tion’s covariance matrix  and show distribution-speciﬁc upper and lower bounds
on the sample complexity  both governed by the γ-adapted-dimension of the
source distribution. We conclude that this new quantity tightly characterizes the
true sample complexity of large-margin classiﬁcation. The bounds hold for a rich
family of sub-Gaussian distributions.

1

Introduction

In this paper we tackle the problem of obtaining a tight characterization of the sample complexity
which a particular learning rule requires  in order to learn a particular source distribution. Specif-
ically  we obtain a tight characterization of the sample complexity required for large (Euclidean)
margin learning to obtain low error for a distribution D(X  Y )  for X ∈ Rd  Y ∈ {±1}.
Most learning theory work focuses on upper-bounding the sample complexity. That is  on pro-
viding a bound m(D  ǫ) and proving that when using some speciﬁc learning rule  if the sample
size is at least m(D  ǫ)  an excess error of at most ǫ (in expectation or with high probability) can
be ensured. For instance  for large-margin classiﬁcation we know that if PD[kXk ≤ B] = 1 
then m(D  ǫ) can be set to O(B2/(γ2ǫ2)) to get true error of no more than ℓ∗
γ + ǫ  where
ℓ∗
γ = minkwk≤1 PD(Y hw  Xi ≤ γ) is the optimal margin error at margin γ.
Such upper bounds can be useful for understanding positive aspects of a learning rule. But it is
difﬁcult to understand deﬁciencies of a learning rule  or to compare between different rules  based
on upper bounds alone. After all  it is possible  and often the case  that the true sample complexity 
i.e. the actual number of samples required to get low error  is much lower than the bound.

Of course  some sample complexity upper bounds are known to be “tight” or to have an almost-
matching lower bound. This usually means that the bound is tight as a worst-case upper bound for
a speciﬁc class of distributions (e.g. all those with PD[kXk ≤ B] = 1). That is  there exists some
source distribution for which the bound is tight. In other words  the bound concerns some quantity
of the distribution (e.g. the radius of the support)  and is the lowest possible bound in terms of this
quantity. But this is not to say that for any speciﬁc distribution this quantity tightly characterizes the
sample complexity. For instance  we know that the sample complexity can be much smaller than the

radius of the support of X  if the average normpE[kXk2] is small. However  E[kXk2] is also not

a precise characterization of the sample complexity  for instance in low dimensions.

The goal of this paper is to identify a simple quantity determined by the distribution that does
precisely characterize the sample complexity. That is  such that the actual sample complexity for the
learning rule on this speciﬁc distribution is governed  up to polylogarithmic factors  by this quantity.

1

In particular  we present the γ-adapted-dimension kγ(D). This measure reﬁnes both the dimension
and the average norm of X  and it can be easily calculated from the covariance matrix of X. We show
that for a rich family of “light tailed” distributions (speciﬁcally  sub-Gaussian distributions with
independent uncorrelated directions – see Section 2)  the number of samples required for learning
by minimizing the γ-margin-violations is both lower-bounded and upper-bounded by ˜Θ(kγ). More
precisely  we show that the sample complexity m(ǫ  γ  D) required for achieving excess error of no
more than ǫ can be bounded from above and from below by:
Ω(kγ(D)) ≤ m(ǫ  γ  D) ≤ ˜O(

kγ(D)

).

ǫ2

As can be seen in this bound  we are not concerned about tightly characterizing the dependence of
the sample complexity on the desired error [as done e.g. in 1]  nor with obtaining tight bounds for
very small error levels. In fact  our results can be interpreted as studying the sample complexity
needed to obtain error well below random  but bounded away from zero. This is in contrast to
classical statistics asymptotic that are also typically tight  but are valid only for very small ǫ. As was
recently shown by Liang and Srebro [2]  the quantities on which the sample complexity depends on
for very small ǫ (in the classical statistics asymptotic regime) can be very different from those for
moderate error rates  which are more relevant for machine learning.

Our tight characterization  and in particular the distribution-speciﬁc lower bound on the sample
complexity that we establish  can be used to compare large-margin (L2 regularized) learning to other
learning rules. In Section 7 we provide two such examples: we use our lower bound to rigorously
establish a sample complexity gap between L1 and L2 regularization previously studied in [3]  and to
show a large gap between discriminative and generative learning on a Gaussian-mixture distribution.
In this paper we focus only on large L2 margin classiﬁcation. But in order to obtain the distribution-
speciﬁc lower bound  we develop novel tools that we believe can be useful for obtaining lower
bounds also for other learning rules.

Related work

Most work on “sample complexity lower bounds” is directed at proving that under some set of
assumptions  there exists a source distribution for which one needs at least a certain number of
examples to learn with required error and conﬁdence [4  5  6]. This type of a lower bound does
not  however  indicate much on the sample complexity of other distributions under the same set of
assumptions.

As for distribution-speciﬁc lower bounds  the classical analysis of Vapnik [7  Theorem 16.6] pro-
vides not only sufﬁcient but also necessary conditions for the learnability of a hypothesis class with
respect to a speciﬁc distribution. The essential condition is that the ǫ-entropy of the hypothesis
class with respect to the distribution be sub-linear in the limit of an inﬁnite sample size. In some
sense  this criterion can be seen as providing a “lower bound” on learnability for a speciﬁc distribu-
tion. However  we are interested in ﬁnite-sample convergence rates  and would like those to depend
on simple properties of the distribution. The asymptotic arguments involved in Vapnik’s general
learnability claim do not lend themselves easily to such analysis.

Benedek and Itai [8] show that if the distribution is known to the learner  a speciﬁc hypothesis
class is learnable if and only if there is a ﬁnite ǫ-cover of this hypothesis class with respect to the
distribution. Ben-David et al. [9] consider a similar setting  and prove sample complexity lower
bounds for learning with any data distribution  for some binary hypothesis classes on the real line.
In both of these works  the lower bounds hold for any algorithm  but only for a worst-case target
hypothesis. Vayatis and Azencott [10] provide distribution-speciﬁc sample complexity upper bounds
for hypothesis classes with a limited VC-dimension  as a function of how balanced the hypotheses
are with respect to the considered distributions. These bounds are not tight for all distributions  thus
this work also does not provide true distribution-speciﬁc sample complexity.

2 Problem setting and deﬁnitions

Let D be a distribution over Rd × {±1}. DX will denote the restriction of D to Rd. We are
interested in linear separators  parametrized by unit-norm vectors in Bd
  {w ∈ Rd | kwk2 ≤ 1}.
1

2

1

1

γ(D)   minw∈Bd

ℓγ(w  D). For a sample S = {(xi  yi)}m

For a predictor w denote its misclassiﬁcation error with respect to distribution D by ℓ(w  D)  
P(X Y )∼D[Y hw  Xi ≤ 0]. For γ > 0  denote the γ-margin loss of w with respect to D by
ℓγ(w  D)   P(X Y )∼D[Y hw  Xi ≤ γ]. The minimal margin loss with respect to D is denoted
by ℓ∗
i=1 such that (xi  yi) ∈ Rd × {±1} 
the margin loss with respect to S is denoted by ˆℓγ(w  S)   1
m|{i | yihxi  wi ≤ γ}| and the misclas-
siﬁcation error is ˆℓ(w  S)   1
m|{i | yihxi  wi ≤ 0}|. In this paper we are concerned with learning by
minimizing the margin loss. It will be convenient for us to discuss transductive learning algorithms.
Since many predictors minimize the margin loss  we deﬁne:
Deﬁnition 2.1. A margin-error minimization algorithm A is an algorithm whose input is a
i=1 and an unlabeled test sample ˜SX = {˜xi}m
margin γ  a training sample S = {(xi  yi)}m
i=1 
ˆℓγ(w  S). We denote the output of the algorithm by
which outputs a predictor ˜w ∈ argminw∈Bd
˜w = Aγ(S  ˜SX ).
We will be concerned with the expected test loss of the algorithm given a random training sample and
S  ˜S∼Dm[ˆℓ(A(S  ˜SX )  ˜S)]  where
a random test sample  each of size m  and deﬁne ℓm(Aγ  D)   E
S  ˜S ∼ Dm independently. For γ > 0  ǫ ∈ [0  1]  and a distribution D  we denote the distribution-
speciﬁc sample complexity by m(ǫ  γ  D): this is the minimal sample size such that for any margin-
error minimization algorithm A  and for any m ≥ m(ǫ  γ  D)  ℓm(Aγ  D) − ℓ∗
Sub-Gaussian distributions

γ(D) ≤ ǫ.

We will characterize the distribution-speciﬁc sample complexity in terms of the covariance of X ∼
DX. But in order to do so  we must assume that X is not too heavy-tailed. Otherwise  X can
have even inﬁnite covariance but still be learnable  for instance if it has a tiny probability of having
an exponentially large norm. We will thus restrict ourselves to sub-Gaussian distributions. This
ensures light tails in all directions  while allowing a sufﬁciently rich family of distributions  as we
presently see. We also require a more restrictive condition – namely that DX can be rotated to a
product distribution over the axes of Rd. A distribution can always be rotated so that its coordinates
are uncorrelated. Here we further require that they are independent  as of course holds for any
multivariate Gaussian distribution.
Deﬁnition 2.2 (See e.g. [11  12]). A random variable X is sub-Gaussian with moment B (or
B-sub-Gaussian) for B ≥ 0 if
We further say that X is sub-Gaussian with relative moment ρ = B/pE[X 2].

∀t ∈ R  E[exp(tX)] ≤ exp(B2t2/2).

The sub-Gaussian family is quite extensive: For instance  any bounded  Gaussian  or Gaussian-
mixture random variable with mean zero is included in this family.
Deﬁnition 2.3. A distribution DX over X ∈ Rd is independently sub-Gaussian with relative
moment ρ if there exists some orthonormal basis a1  . . .   ad ∈ Rd  such that hX  aii are independent
sub-Gaussian random variables  each with a relative moment ρ.
We will focus on the family Dsg
ρ of all independently ρ-sub-Gaussian distributions in arbitrary di-
mension  for a small ﬁxed constant ρ. For instance  the family Dsg
3/2 includes all Gaussian distribu-
tions  all distributions which are uniform over a (hyper)box  and all multi-Bernoulli distributions 
in addition to other less structured distributions. Our upper bounds and lower bounds will be tight
up to quantities which depend on ρ  which we will regard as a constant  but the tightness will not
depend on the dimensionality of the space or the variance of the distribution.

(1)

3 The γ-adapted-dimension

As mentioned in the introduction  the sample complexity of margin-error minimization can be upper-
bounded in terms of the average norm E[kXk2] by m(ǫ  γ  D) ≤ O(E[kXk2]/(γ2ǫ2)) [13]. Alter-
natively  we can rely only on the dimensionality and conclude m(ǫ  γ  D) ≤ ˜O(d/ǫ2) [7]. Thus 

3

although both of these bounds are tight in the worst-case sense  i.e. they are the best bounds that
rely only on the norm or only on the dimensionality respectively  neither is tight in a distribution-
speciﬁc sense: If the average norm is unbounded while the dimensionality is small  an arbitrarily
large gap is created between the true m(ǫ  γ  D) and the average-norm upper bound. The converse
happens if the dimensionality is arbitrarily high while the average-norm is bounded.

Seeking a distribution-speciﬁc tight analysis  one simple option to try to tighten these bounds is to
consider their minimum  min(d  E[kXk2]/γ2)/ǫ2  which  trivially  is also an upper bound on the
sample complexity. However  this simple combination is also not tight: Consider a distribution in
which there are a few directions with very high variance  but the combined variance in all other
directions is small. We will show that in such situations the sample complexity is characterized not
by the minimum of dimension and norm  but by the sum of the number of high-variance dimensions
and the average norm in the other directions. This behavior is captured by the γ-adapted-dimension:
Deﬁnition 3.1. Let b > 0 and k a positive integer.

(a). A subset X ⊆ Rd is (b  k)-limited if there exists a sub-space V ⊆ Rd of dimension d − k

such that X ⊆ {x ∈ Rd | kx′Pk2 ≤ b}  where P is an orthogonal projection onto V .

sion d − k such that EX∼DX [kX ′Pk2] ≤ b  with P an orthogonal projection onto V .

(b). A distribution DX over Rd is (b  k)-limited if there exists a sub-space V ⊆ Rd of dimen-
Deﬁnition 3.2. The γ-adapted-dimension of a distribution or a set  denoted by kγ  is the minimum
k such that the distribution or set is (γ2k  k) limited.
It is easy to see that kγ(DX ) is upper-bounded by min(d  E[kXk2]/γ2). Moreover  it can be much
smaller. For example  for X ∈ R1001 with independent coordinates such that the variance of the
ﬁrst coordinate is 1000  but the variance in each remaining coordinate is 0.001 we have k1 = 1 but
d = E[kXk2] = 1001. More generally  if λ1 ≥ λ2 ≥ ··· λd are the eigenvalues of the covariance
matrix of X  then kγ = min{k | Pd
i=k+1 λi ≤ γ2k}. A quantity similar to kγ was studied
previously in [14]. kγ is different in nature from some other quantities used for providing sample
complexity bounds in terms of eigenvalues  as in [15]  since it is deﬁned based on the eigenvalues
of the distribution and not of the sample. In Section 6 we will see that these can be quite different.

In order to relate our upper and lower bounds  it will be useful to relate the γ-adapted-dimension for
different margins. The relationship is established in the following Lemma   proved in the appendix:
Lemma 3.3. For 0 < α < 1  γ > 0 and a distribution DX  kγ(DX ) ≤ kαγ(DX ) ≤ 2kγ (DX )
We proceed to provide a sample complexity upper bound based on the γ-adapted-dimension.

α2 + 1.

4 A sample complexity upper bound using γ-adapted-dimension

In order to establish an upper bound on the sample complexity  we will bound the fat-shattering
dimension of the linear functions over a set in terms of the γ-adapted-dimension of the set. Recall
that the fat-shattering dimension is a classic quantity for proving sample complexity upper bounds:
Deﬁnition 4.1. Let F be a set of functions f : X → R  and let γ > 0. The set {x1  . . .   xm} ⊆ X is
γ-shattered by F if there exist r1  . . .   rm ∈ R such that for all y ∈ {±1}m there is an f ∈ F such
that ∀i ∈ [m]  yi(f (xi) − ri) ≥ γ. The γ-fat-shattering dimension of F is the size of the largest
set in X that is γ-shattered by F.
The sample complexity of γ-loss minimization is bounded by ˜O(dγ/8/ǫ2) were dγ/8 is the γ/8-
fat-shattering dimension of the function class [16  Theorem 13.4]. Let W(X ) be the class of linear
functions restricted to the domain X . For any set we show:
Theorem 4.2. If a set X is (B2  k)-limited  then the γ-fat-shattering dimension of W(X ) is at most
2 (B2/γ2 + k + 1). Consequently  it is also at most 3kγ(X ) + 1.
Proof. Let X be a m × d matrix whose rows are a set of m points in Rd which is γ-shattered.
For any ǫ > 0 we can augment X with an additional column to form the matrix ˜X of dimensions
1+ǫ such that eXwy = y (the details
m× (d + 1)  such that for all y ∈ {−γ  +γ}m  there is a wy ∈ Bd+1

3

4

can be found in the appendix). Since X is (B2  k)-limited  there is an orthogonal projection matrix
˜P of size (d + 1) × (d + 1) such that ∀i ∈ [m] k ˜X ′
iPk2 ≤ B2 where ˜Xi is the vector in row i of
˜X. Let ˜V be the sub-space of dimension d − k spanned by the columns of ˜P . To bound the size of
the shattered set  we show that the projected rows of ˜X on V are ‘shattered’ using projected labels.
We then proceed similarly to the proof of the norm-only fat-shattering bound [17].
We have ˜X = ˜X ˜P + ˜X(I − ˜P ). In addition  ˜Xwy = y. Thus y − ˜X ˜P wy = ˜X(I − ˜P )wy.
I − ˜P is a projection onto a k + 1-dimensional space  thus the rank of ˜X(I − ˜P ) is at most k + 1.
Let T be an m × m orthogonal projection matrix onto the subspace orthogonal to the columns
of ˜X(I − ˜P ). This sub-space is of dimension at most l = m − (k + 1)  thus trace(T ) = l.
T (y − ˜X ˜P wy) = T ˜X(I − ˜P )wy = 0(d+1)×1. Thus T y = T ˜X ˜P wy for every y ∈ {−γ  +γ}m.
Denote row i of T by ti and row i of T ˜X ˜P by zi. We have ∀i ≤ m  hzi  w1
Pj≤m ti[j]y[j]. Therefore hPi ziy[i]  w1
yi = Pi≤mPj≤(l+k) ti[j]y[i]y[j]. Since kw1
∀x ∈ Rd+1  (1 + ǫ)kxk ≥ kxkkw1
Pi≤mPj≤m ti[j]y[i]y[j]. Taking the expectation of y chosen uniformly at random  we have
ziy[i]k] ≥Xi j
E[ti[j]y[i]y[j]] = γ2Xi
E[kPi ziy[i]k2] =Pl
i=1 kzik2 = trace( ˜P ′ ˜X ′T 2 ˜X ˜P ) ≤ trace( ˜P ′ ˜X ′ ˜X ˜P ) ≤ B2m.
γ 2 m. Since this holds for any
γ 2 (k + 1) ≤

In addition  1
γ 2
From the inequality E[X 2] ≤ E[X]2  it follows that l2 ≤ (1 + ǫ)2 B2
ǫ > 0  we can set ǫ = 0 and solve for m. Thus m ≤ (k + 1) + B2
(k + 1) + B2

yi = tiy =
yk ≤ 1 + ǫ 
yi. Thus ∀y ∈ {−γ  +γ}m  (1 + ǫ)kPi ziy[i]k ≥

(1 + ǫ)E[kXi

2γ 2 +q B4

ti[i] = γ2trace(T ) = γ2l.

yk ≥ hx  w1

4γ 4 + B2

γ 2 (k + 1) ≤ 3

2 ( B2

γ 2 + k + 1).

γ 2 +q B2

Corollary 4.3. Let D be a distribution over X × {±1}  X ⊆ Rd. Then

m(ǫ  γ  D) ≤ eO(cid:18) kγ/8(X )

ǫ2

(cid:19) .

The corollary above holds only for distributions with bounded support. However  since sub-Gaussian
variables have an exponentially decaying tail  we can use this corollary to provide a bound for
independently sub-Gaussian distributions as well (see appendix for proof):
Theorem 4.4 (Upper Bound for Distributions in Dsg
that DX ∈ Dsg
ρ  

ρ ). For any distribution D over Rd ×{±1} such
ρ2kγ(DX )

m(ǫ  γ  D) = ˜O(

).

ǫ2

This new upper bound is tighter than norm-only and dimension-only upper bounds. But does the
γ-adapted-dimension characterize the true sample complexity of the distribution  or is it just another
upper bound? To answer this question  we need to be able to derive sample complexity lower bounds
as well. We consider this problem in following section.

5 Sample complexity lower bounds using Gram-matrix eigenvalues

We wish to ﬁnd a distribution-speciﬁc lower bound that depends on the γ-adapted-dimension  and
matches our upper bound as closely as possible. To do that  we will link the ability to learn with
a margin  with properties of the data distribution. The ability to learn is closely related to the
probability of a sample to be shattered  as evident from Vapnik’s formulations of learnability as a
function of the ǫ-entropy. In the preceding section we used the fact that non-shattering (as captured
by the fat-shattering dimension) implies learnability. For the lower bound we use the converse fact 
presented below in Theorem 5.1: If a sample can be fat-shattered with a reasonably high probability 
then learning is impossible. We then relate the fat-shattering of a sample to the minimal eigenvalue
of its Gram matrix. This allows us to present a lower-bound on the sample complexity using a lower
bound on the smallest eigenvalue of the Gram-matrix of a sample drawn from the data distribution.
We use the term ‘γ-shattered at the origin’ to indicate that a set is γ-shattered by setting the bias
r ∈ Rm (see Def. 4.1) to the zero vector.

5

Theorem 5.1. Let D be a distribution over Rd × {±1}. If the probability of a sample of size m
drawn from Dm
X to be γ-shattered at the origin is at least η  then there is a margin-error minimization
algorithm A  such that ℓm/2(Aγ  D) ≥ η/2.
Proof. For a given distribution D  let A be an algorithm which  for every two input samples S and
[ˆℓγ(w  ˜S)].
ˆℓγ(w  S) that maximizes E ˜SY ∈Dm
˜SX  labels ˜SX using the separator w ∈ argminw∈Bd
For every x ∈ Rd there is a label y ∈ {±1} such that P(X Y )∼D[Y 6= y | X = x] ≥ 1
2 . If the set of
examples in SX and ˜SX together is γ-shattered at the origin  then A chooses a separator with zero
margin loss on S  but loss of at least 1

2 on ˜S. Therefore ℓm/2(Aγ  D) ≥ η/2.

1

Y

The notion of shattering involves checking the existence of a unit-norm separator w for each label-
vector y ∈ {±1}m. In general  there is no closed form for the minimum-norm separator. However 
the following Theorem provides an equivalent and simple characterization for fat-shattering:
Theorem 5.2. Let S = (X1  . . .   Xm) be a sample in Rd  denote X the m×d matrix whose rows are
y′(XX ′)−1y ≤ 1.
the elements of S. Then S is 1-shattered iff X is invertible and ∀y ∈ {±1}m 
The proof of this theorem is in the appendix. The main issue in the proof is showing that if a set is
shattered  it is also shattered with exact margins  since the set of exact margins {±1}m lies in the
convex hull of any set of non-exact margins that correspond to all the possible labelings. We can now
use the minimum eigenvalue of the Gram matrix to obtain a sufﬁcient condition for fat-shattering 
after which we present the theorem linking eigenvalues and learnability. For a matrix X  λn(X)
denotes the n’th largest eigenvalue of X.
Lemma 5.3. Let S = (X1  . . .   Xm) be a sample in Rd  with X as above. If λm(XX ′) ≥ m then
S is 1-shattered at the origin.

Proof. If λm(XX ′) ≥ m then XX ′ is invertible and λ1((XX ′)−1) ≤ 1/m. For any y ∈ {±1}m
we have kyk = √m and y′(XX ′)−1y ≤ kyk2λ1((XX ′)−1) ≤ m(1/m) = 1. By Theorem 5.2 the
sample is 1-shattered at the origin.
Theorem 5.4. Let D be a distribution over Rd×{±1}  S be an i.i.d. sample of size m drawn from D 
and denote XS the m × d matrix whose rows are the points from S. If P[λm(XSX ′
S) ≥ mγ2] ≥ η 
then there exists a margin-error minimization algorithm A such that ℓm/2(Aγ  D) ≥ η/2.
Theorem 5.4 follows by scaling XS by γ  applying Lemma 5.3 to establish γ-fat shattering with
probability at least η  then applying Theorem 5.1. Lemma 5.3 generalizes the requirement for linear
independence when shattering using hyperplanes with no margin (i.e. no regularization). For unreg-
ularized (homogeneous) linear separation  a sample is shattered iff it is linearly independent  i.e. if
λm > 0. Requiring λm > mγ2 is enough for γ-fat-shattering. Theorem 5.4 then generalizes the
simple observation  that if samples of size m are linearly independent with high probability  there
is no hope of generalizing from m/2 points to the other m/2 using unregularized linear predictors.
Theorem 5.4 can thus be used to derive a distribution-speciﬁc lower bound. Deﬁne:

mγ(D)   1
2

min(cid:26)m(cid:12)(cid:12)(cid:12)(cid:12) PS∼Dm[λm(XSX ′

S) ≥ mγ2] <

1

2(cid:27)

Then for any ǫ < 1/4− ℓ∗
γ(D)  we can conclude that m(ǫ  γ  D) ≥ mγ(D)  that is  we cannot learn
within reasonable error with less than mγ examples. Recall that our upper-bound on the sample
complexity from Section 4 was ˜O(kγ). The remaining question is whether we can relate mγ and
kγ  to establish that the our lower bound and upper bound tightly specify the sample complexity.

6 A lower bound for independently sub-Gaussian distributions

As discussed in the previous section  to obtain sample complexity lower bound we require a bound
on the value of the smallest eigenvalue of a random Gram-matrix. The distribution of this eigenvalue
has been investigated under various assumptions. The cleanest results are in the case where m  d →
∞ and m

d → β < 1  and the coordinates of each example are identically distributed:

6

Theorem 6.1 (Theorem 5.11 in [18]). Let Xi be a series of mi × di matrices whose entries are i.i.d.
random variables with mean zero  variance σ2 and ﬁnite fourth moments. If limi→∞
= β < 1 
then limi→∞ λm( 1

mi
di

d XiX ′

i) = σ2(1 − √β)2.

This asymptotic limit can be used to calculate mγ and thus provide a lower bound on the sample
complexity: Let the coordinates of X ∈ Rd be i.i.d. with variance σ2 and consider a sample of size
m. If d  m are large enough  we have by Theorem 6.1:

λm(XX ′) ≈ dσ2(1 −pm/d)2 = σ2(√d − √m)2

Solving σ2(√d −p2mγ)2 = 2mγγ2 we get mγ ≈ 1
2 d/(1 + γ/σ)2. We can also calculate the γ-
adapted-dimension for this distribution to get kγ ≈ d/(1 + γ2/σ2)  and conclude that 1
4 kγ ≤ mγ ≤
1
2 kγ. In this case  then  we are indeed able to relate the sample complexity lower bound with kγ  the
same quantity that controls our upper bound. This conclusion is easy to derive from known results 
however it holds only asymptotically  and only for a highly limited set of distributions. Moreover 
since Theorem 6.1 holds asymptotically for each distribution separately  we cannot deduce from it
any ﬁnite-sample lower bounds for families of distributions.

For our analysis we require ﬁnite-sample bounds for the smallest eigenvalue of a random Gram-
matrix. Rudelson and Vershynin [19  20] provide such ﬁnite-sample lower bounds for distributions
with identically distributed sub-Gaussian coordinates. In the following Theorem we generalize re-
sults of Rudelson and Vershynin to encompass also non-identically distributed coordinates. The
proof of Theorem 6.2 can be found in the appendix. Based on this theorem we conclude with Theo-
rem 6.3  stated below  which constitutes our ﬁnal sample complexity lower bound.
Theorem 6.2. Let B > 0. There is a constant β > 0 which depends only on B  such that for any
δ ∈ (0  1) there exists a number L0  such that for any independently sub-Gaussian distribution with
covariance matrix Σ ≤ I and trace(Σ) ≥ L0  if each of its independent sub-Gaussian coordinates
has moment B  then for any m ≤ β · trace(Σ)
P[λm(XmX ′

m) ≥ m] ≥ 1 − δ 

Where Xm is an m × d matrix whose rows are independent draws from DX.
Theorem 6.3 (Lower bound for distributions in Dsg
and an integer L0 such that for any D such that DX ∈ Dsg
γ > 0 and any ǫ < 1
m(ǫ  γ  D) ≥ βkγ(DX ).

4 − ℓ∗

γ(D) 

ρ ). For any ρ > 0  there are a constant β > 0
ρ and kγ(DX ) > L0  for any margin

Proof. The covariance matrix of DX is clearly diagonal. We assume w.l.o.g.
that Σ =
diag(λ1  . . .   λd) where λ1 ≥ . . . ≥ λd > 0. Let S be an i.i.d. sample of size m drawn from
D. Let X be the m × d matrix whose rows are the unlabeled examples from S. Let δ be ﬁxed  and
set β and L0 as deﬁned in Theorem 6.2 for δ. Assume m ≤ β(kγ − 1).
We would like to use Theorem 6.2 to bound the smallest eigenvalue of XX ′ with high probability 
so that we can then apply Theorem 5.4 to get the desired lower bound. However  Theorem 6.2
holds only if all the coordinate variances are bounded by 1  and it requires that the moment  and not
the relative moment  be bounded. Thus we divide the problem to two cases  based on the value of
λkγ +1  and apply Theorem 6.2 separately to each case.
Case I: Assume λkγ +1 ≥ γ2. Then ∀i ∈ [kγ]  λi ≥ γ2. Let Σ1 = diag(1/λ1  . . .   1/λkγ   0  . . .   0).
The random matrix X√Σ1 is drawn from an independently sub-Gaussian distribution  such that
each of its coordinates has sub-Gaussian moment ρ and covariance matrix Σ · Σ1 ≤ Id. In addition 
trace(Σ · Σ1) = kγ ≥ L0. Therefore Theorem 6.2 holds for X√Σ1  and P[λm(XΣ1X ′) ≥ m] ≥
1 − δ. Clearly  for any X  λm( 1
Case II: Assume λkγ +1 < γ2. Then λi < γ2 for all i ∈ {kγ + 1  . . .   d}. Let Σ2 =
diag(0  . . .   0  1/γ2  . . .   1/γ2)  with kγ zeros on the diagonal. Then the random matrix X√Σ2
is drawn from an independently sub-Gaussian distribution with covariance matrix Σ· Σ2 ≤ Id  such
that all its coordinates have sub-Gaussian moment ρ. In addition  from the properties of kγ (see
discussion in Section 2)  trace(Σ · Σ2) = 1
i=kγ +1 λi ≥ kγ − 1 ≥ L0 − 1. Thus Theorem 6.2
holds for X√Σ2  and so P[λm( 1

γ 2 XX ′) ≥ λm(XΣ1X ′). Thus P[λm( 1

γ 2 XX ′) ≥ m] ≥ 1 − δ.

γ 2 XX ′) ≥ m] ≥ P[λm(XΣ2X ′) ≥ m] ≥ 1 − δ.

γ 2 Pd

7

In both cases P[λm( 1
an algorithm A such that for any m ≤ β(kγ − 1) − 1  ℓm(Aγ  D) ≥ 1
ǫ < 1

γ 2 XX ′) ≥ m] ≥ 1 − δ for any m ≤ β(kγ − 1). By Theorem 5.4  there exists
2 − δ/2. Therefore  for any

γ(D)  we have m(ǫ  γ  D) ≥ β(kγ − 1). We get the theorem by setting δ = 1
4 .

2 − δ/2 − ℓ∗

7 Summary and consequences

Theorem 4.4 and Theorem 6.3 provide an upper bound and a lower bound for the sample complexity
of any distribution D whose data distribution is in Dsg
ρ for some ﬁxed ρ > 0. We can thus draw the
following bound  which holds for any γ > 0 and ǫ ∈ (0  1

4 − ℓ∗

γ(D)):

Ω(kγ(DX )) ≤ m(ǫ  γ  D) ≤ ˜O(

kγ(DX )

ǫ2

).

(2)

In both sides of the bound  the hidden constants depend only on the constant ρ. This result shows
that the true sample complexity of learning each of these distributions is characterized by the γ-
adapted-dimension. An interesting conclusion can be drawn as to the inﬂuence of the conditional
distribution of labels DY |X: Since Eq. (2) holds for any DY |X  the effect of the direction of the best
separator on the sample complexity is bounded  even for highly non-spherical distributions. We can
use Eq. (2) to easily characterize the sample complexity behavior for interesting distributions  and
to compare L2 margin minimization to learning methods.
Gaps between L1 and L2 regularization in the presence of irrelevant features. Ng [3] considers
learning a single relevant feature in the presence of many irrelevant features  and compares using
L1 regularization and L2 regularization. When kXk∞ ≤ 1  upper bounds on learning with L1
regularization guarantee a sample complexity of O(log(d)) for an L1-based learning rule [21]. In
order to compare this with the sample complexity of L2 regularized learning and establish a gap 
one must use a lower bound on the L2 sample complexity. The argument provided by Ng actually
assumes scale-invariance of the learning rule  and is therefore valid only for unregularized linear
learning. However  using our results we can easily establish a lower bound of Ω(d) for many speciﬁc
distributions with kXk∞ ≤ 1 and Y = X[1] ∈ {±1}. For instance  when each coordinate is an
independent Bernoulli variable  the distribution is sub-Gaussian with ρ = 1  and k1 = ⌈d/2⌉.
Gaps between generative and discriminative learning for a Gaussian mixture. Consider two
classes  each drawn from a unit-variance spherical Gaussian in a high dimension Rd and with a
large distance 2v >> 1 between the class means  such that d >> v4. Then PD[X|Y = y] =
N (yv · e1  Id)  where e1 is a unit vector in Rd. For any v and d  we have DX ∈ Dsg
1 . For large
values of v  we have extremely low margin error at γ = v/2  and so we can hope to learn the
classes by looking for a large-margin separator. Indeed  we can calculate kγ = ⌈d/(1 + v2
4 )⌉  and
conclude that the sample complexity required is ˜Θ(d/v2). Now consider a generative approach:
ﬁtting a spherical Gaussian model for each class. This amounts to estimating each class center as
the empirical average of the points in the class  and classifying based on the nearest estimated class
center. It is possible to show that for any constant ǫ > 0  and for large enough v and d  O(d/v4)
samples are enough in order to ensure an error of ǫ. This establishes a rather large gap of Ω(v2)
between the sample complexity of the discriminative approach and that of the generative one.

To summarize  we have shown that the true sample complexity of large-margin learning of a rich
family of speciﬁc distributions is characterized by the γ-adapted-dimension. This result allows true
comparison between this learning algorithm and other algorithms  and has various applications  such
as semi-supervised learning and feature construction. The challenge of characterizing true sample
complexity extends to any distribution and any learning algorithm. We believe that obtaining an-
swers to these questions is of great importance  both to learning theory and to learning applications.

Acknowledgments

The authors thank Boaz Nadler for many insightful discussions  and Karthik Sridharan for pointing
out [14] to us. Sivan Sabato is supported by the Adams Fellowship Program of the Israel Academy
of Sciences and Humanities. This work was supported by the NATO SfP grant 982480.

8

References
[1] I. Steinwart and C. Scovel. Fast rates for support vector machines using Gaussian kernels. Annals of

Statistics  35(2):575–607  2007.

[2] P. Liang and N. Srebro. On the interaction between norm and dimensionality: Multiple regimes in learn-

ing. In ICML  2010.

[3] A.Y. Ng. Feature selection  l1 vs. l2 regularization  and rotational invariance. In ICML  2004.
[4] A. Antos and G. Lugosi. Strong minimax lower bounds for learning. Mach. Learn.  30(1):31–56  1998.
[5] A. Ehrenfeucht  D. Haussler  M. Kearns  and L. Valiant. A general lower bound on the number of ex-
amples needed for learning. In Proceedings of the First Anuual Workshop on Computational Learning
Theory  pages 139–154  August 1988.

[6] C. Gentile and D.P. Helmbold. Improved lower bounds for learning from noisy examples: an information-

theoretic approach. In COLT  pages 104–115  1998.

[7] V.N. Vapnik. The Nature of Statistical Learning Theory. Springer  1995.
[8] Gyora M. Benedek and Alon Itai. Learnability with respect to ﬁxed distributions. Theoretical Computer

Science  86(2):377–389  September 1991.

[9] S. Ben-David  T. Lu  and D. P´al. Does unlabeled data provably help? In Proceedings of the Twenty-First

Annual Conference on Computational Learning Theory  pages 33–44  2008.

[10] N. Vayatis and R. Azencott. Distribution-dependent vapnik-chervonenkis bounds.

pages 230–240  London  UK  1999. Springer-Verlag.

In EuroCOLT ’99 

[11] D.J.H. Garling. Inequalities: A Journey into Linear Analysis. Cambrige University Press  2007.
[12] V.V. Buldygin and Yu. V. Kozachenko. Metric Characterization of Random Variables and Random Pro-

cesses. American Mathematical Society  1998.

[13] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural

results. In COLT 2001  volume 2111  pages 224–240. Springer  Berlin  2001.

[14] O. Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Analysis of

Learning Algorithms. PhD thesis  Ecole Polytechnique  2002.

[15] B. Sch¨olkopf  J. Shawe-Taylor  A. J. Smola  and R.C. Williamson. Generalization bounds via eigenvalues

of the gram matrix. Technical Report NC2-TR-1999-035  NeuroCOLT2  1999.

[16] M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University

Press  1999.

[17] N. Christianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University

Press  2000.

[18] Z. Bai and J.W. Silverstein. Spectral Analysis of Large Dimensional Random Matrices. Springer  second

edition edition  2010.

[19] M. Rudelson and R. Vershynin. The smallest singular value of a random rectangular matrix. Communi-

cations on Pure and Applied Mathematics  62:1707–1739  2009.

[20] M. Rudelson and R. Vershynin. The littlewoodofford problem and invertibility of random matrices. Ad-

vances in Mathematics  218(2):600–633  2008.

[21] T. Zhang. Covering number bounds of certain regularized linear function classes. Journal of Machine

Learning Research  2:527–550  2002.

[22] G. Bennett  V. Goodman  and C. M. Newman. Norms of random matrices. Paciﬁc J. Math.  59(2):359–

365  1975.

[23] F.L. Nazarov and A. Podkorytov. Ball  haagerup  and distribution functions. Operator Theory: Advances

and Applications  113 (Complex analysis  operators  and related topics):247–267  2000.

[24] R.E.A.C. Paley and A. Zygmund. A note on analytic functions in the unit circle. Proceedings of the

Cambridge Philosophical Society  28:266272  1932.

9

,Jin-Hwa Kim
Sang-Woo Lee
Donghyun Kwak
Min-Oh Heo
Jeonghee Kim
Jung-Woo Ha
Byoung-Tak Zhang
Amo Tong
Ding-Zhu Du
Weili Wu