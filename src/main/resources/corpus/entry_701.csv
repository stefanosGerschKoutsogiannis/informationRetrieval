2010,Batch Bayesian Optimization via Simulation Matching,Bayesian optimization methods are often used to optimize unknown functions that are costly to evaluate. Typically  these methods sequentially select inputs to be evaluated one at a time based on a posterior over the unknown function that is updated after each evaluation. There are a number of effective sequential policies for selecting the individual inputs. In many applications  however  it is desirable to perform multiple evaluations in parallel  which requires selecting batches of multiple inputs to evaluate at once. In this paper  we propose a novel approach to batch Bayesian optimization  providing a policy for selecting batches of inputs with the goal of optimizing the function as efficiently as possible. The key idea is to exploit the availability of high-quality and efficient sequential policies  by using Monte-Carlo simulation to select input batches that closely match their expected behavior. To the best of our knowledge  this is the first batch selection policy for Bayesian optimization. Our experimental results on six benchmarks show that the proposed approach significantly outperforms two baselines and can lead to large advantages over a top sequential approach in terms of performance per unit time.,Batch Bayesian Optimization

via Simulation Matching

Javad Azimi  Alan Fern  Xiaoli Z. Fern
School of EECS  Oregon State University

{azimi  afern  xfern}@eecs.oregonstate.edu

Abstract

Bayesian optimization methods are often used to optimize unknown functions that
are costly to evaluate. Typically  these methods sequentially select inputs to be
evaluated one at a time based on a posterior over the unknown function that is
updated after each evaluation. In many applications  however  it is desirable to
perform multiple evaluations in parallel  which requires selecting batches of mul-
tiple inputs to evaluate at once. In this paper  we propose a novel approach to
batch Bayesian optimization  providing a policy for selecting batches of inputs
with the goal of optimizing the function as efﬁciently as possible. The key idea is
to exploit the availability of high-quality and efﬁcient sequential policies  by using
Monte-Carlo simulation to select input batches that closely match their expected
behavior. Our experimental results on six benchmarks show that the proposed ap-
proach signiﬁcantly outperforms two baselines and can lead to large advantages
over a top sequential approach in terms of performance per unit time.

Introduction

1
We consider the problem of maximizing an unknown function f (x) when each evaluation of the
function has a high cost. In such cases  standard optimization techniques such as empirical gradient
methods are not practical due to the high number of function evaluations that they demand. Rather 
Bayesian optimization (BO) methods [12  4] have demonstrated signiﬁcant promise in their ability
to effectively optimize a function given only a small number of evaluations. BO gains this efﬁciency
by leveraging Bayesian models that take into account all previously observed evaluations in order
to better inform future evaluation choices. In particular  typical BO methods continually maintain a
posterior over f (x) that is used to select the next input to evaluate. The result of the evaluation is
then used to update the posterior and the process repeats. There are a number of well established
policies for selecting the next input to evaluate given the current posterior. We will refer to such
policies as sequential policies to stress the fact that they select one input at a time.
In many applications it is possible and desirable to run multiple function evaluations in parallel.
This is the case  for example  when the underlying function corresponds to a controlled laboratory
experiment where multiple experimental setups are examined simultaneously  or when the underly-
ing function is the result of a costly computer simulation and multiple simulations can be run across
different processors in parallel. In such cases  existing sequential policies are not sufﬁcient. Rather 
batch mode BO is more appropriate  where policies select a batch of multiple inputs to be evaluated
at once. To the best of our knowledge and as noted in [4]  there is no established work on BO that
considers the batch selection problem  except for a brief treatment in [21]. The main contribution of
this work is to propose an approach to batch BO and to demonstrate its effectiveness.
The key motivation behind our approach comes from the fact that the sequential mode of BO has a
fundamental advantage over BO in batch mode. This is because in sequential mode  each function
evaluation is immediately used to obtain a more accurate posterior of f (x)  which in turn will allow

1

a selection policy to make more informed choices about the next input. Given an effective sequential
selection policy  our goal is then to design a batch policy that approximates its behavior.
In particular  our batch policy attempts to select a batch that “matches” the expected behavior of a
sequential policy as closely as possible. The approach generates Monte-Carlo simulations of a se-
quential policy given the current posterior  and then derives an optimization problem over possible
batches aimed at minimizing the loss between the sequential policy and the batch. We consider two
variants of this optimization problem that yield a continuous weighted k-means problem and a com-
binatorial weighted k-medoid problem. We solve the k-means variant via k-means clustering and
show that the k-medoid variant corresponds to minimizing a non-increasing supermodular function 
for which there is an efﬁcient approximation algorithm [9].
We evaluate our approach on a collection of six functions and compare it to random and another
baseline batch policy based on submodular maximization. The results show that our approach signif-
icantly outperforms these baselines and can lead to large advantages over a top sequential approach
in terms of performance per unit time.

2 Problem Setup
Let X ⊆ Rn be an n-dimensional input space  where we will often refer to elements of X as an
experiment and assume that each dimension i is bounded in [Ai  Bi]. We assume an unknown real-
valued function f : X → R  which represents the expected value of the dependent variable after
running an experiment. For example  f (x) might correspond to the result of a wet-lab experiment
or a computer simulation with input parameters x. Conducting an experiment x produces a noisy
outcome y = f (x) +   where  is a noise term that might be 0 in some applications.
Our objective is to ﬁnd an experiment x ∈ X that approximately maximizes f by requesting a
limited number of experiments and observing their outcomes. Furthermore we are interested in
applications where (1) running experiments is costly (e.g. in terms of laboratory or simulation time);
and (2) it is desirable to run k > 1 experiments in parallel. This motivates the problem of selecting
a sequence of batches  each containing k experiments  where the choice of a batch can depend on
the results observed from all previous experiments. We will refer to the rule for selecting a batch
based on previous experiments as the batch policy. The main goal of this paper is to develop a batch
policy that optimizes the unknown function as efﬁciently as possible.
Due to the high cost of experiments  traditional optimization techniques such as empirical gradient
ascent are not practical for our setting  due to their high demands on the number of experiments.
Rather  we build on Bayesian optimization (BO) [10  12  4]  which leverages Bayesian modeling
in an attempt to achieve more efﬁcient optimization. In particular  BO maintains a posterior over
the unknown function based on previously observed experiments  e.g. represented via a Gaussian
Process (GP) [19]. This posterior is used to select the next experiment to be run in a way that attempts
to trade-off exploring new parts of the experimental space and exploiting parts that look promising.
While the BO literature has provided a number of effective policies  they are all sequential policies 
where only a single experiment is selected and run at a time. Thus  the main novelty of our work is
in deﬁning a batch policy in the context of BO  which is described in the next section.

3 Simulation Matching for Batch Selection
Given a data set D of previously observed experiments  which induces a posterior distribution over
the unknown function  we now consider how to select the next batch of k experiments. A key issue in
making this choice is to manage the trade-off between exploration and exploitation. The policy must
attempt to explore by requesting experiments from unexplored parts of the input space  at the same
time also attempt to optimize the unknown function via experiments that look promising given the
current data. While  under most measures  optimizing this trade-off is computationally intractable 
there are a number of heuristic sequential policies from the BO literature that are computationally
efﬁcient and perform very well in practice. For example  one such policy selects the next experiment
to be the one that has the “maximum expected improvement” according to the current posterior
[14  10]. The main idea behind our approach is to leverage such sequential policies by selecting a
batch of k > 1 experiments that “closely matches” the sequential policy’s expected behavior.
More formally  let π be a sequential policy. Given a data set D of prior experimental results  π returns
the next experiment x ∈ X to be selected. As is standard in BO  we assume we have a posterior

2

π.1 Our batch policy is based on generating a number of samples of Sk

density P (f | D) over the unknown function f  such as a Gaussian Process. Given this density we
can deﬁne a density over the outcomes of executing policy π for k steps  each outcome consisting
of a set of k selected experiments. Let Sk
π be the random variable denoting the set of k experiments
resulting from such k-step executions  which has a well deﬁned density over all possible sets given
the posterior of f. Importantly  it is generally straightforward to use Monte Carlo simulation to
sample values of Sk
π  which
are used to deﬁne an objective for optimizing a batch of k experiments. Below we describe this
objective and a variant  followed by a description of how we optimize the proposed objectives.
3.1 Batch Objective Function
Our goal is to select a batch B of k experiments that best “matches the expected behavior” of a base
sequential policy π conditioned on the observed data D. More precisely  we consider a batch B to
be a good match for a policy execution if B contains an experiment that is close to the best of the k
experiments selected by the policy. To specify this objective we ﬁrst introduce some notation. Given
a function f and a set of experiments S  we deﬁne x∗(f  S) = arg maxx∈S f (x) to be the maximizer
of f in S. Also  for any experiment x and set B we deﬁne nn(x  B) = arg minx(cid:48)∈B (cid:107) x − x(cid:48) (cid:107) to
be the nearest neighbor of x in set B. Our objective can now be written as selecting a batch B that
minimizes

π)  B) (cid:107)2| D(cid:3) | D(cid:3) .

π) − nn(x∗(f  Sk

(cid:2)Ef|Sk

π

(cid:2)(cid:107) x∗(f  Sk

OBJ(B) = ESk

π

π D) · P (Sk

π | D) = P (f | Sk

π and f as
Note that this nested expectation is the result of decomposing the joint posterior over Sk
π | D). If we assume that the unknown function f (x) is
P (f  Sk
Lipschitz continuous then minimizing this objective can be viewed as minimizing an upper bound
on the expected performance difference between the sequential policy and the selected batch. Here
the performance of a policy or a batch is equal to the output value of the best selected experiment.
π with a sample average
We will approximate this objective by replacing the outer expectation over Sk
over n samples {S1  . . .   Sn} of Sk

π as follows  recalling that each Si is a set of k experiments:
Ef|Si

(cid:2)(cid:107) x∗(f  Si) − nn(x∗(f  Si)  B) (cid:107)2 | D(cid:3)

OBJ(B) ≈ 1
n

=

=

1
n

1
n

(cid:88)
(cid:88)
(cid:88)

i

i

(cid:88)
(cid:88)

x∈Si

x∈Si

i

Pr(x = x∗(f  Si) | D  Si)· (cid:107) x − nn(x  B) (cid:107)2

αi x· (cid:107) x − nn(x  B) (cid:107)2

(1)

the constraint that B is restricted to experiments in the simulations  i.e. B ⊆ (cid:83)

The second step follows by noting that x∗(f  Si) must be one of the k experiments in Si.
We now deﬁne our objective as minimizing (1) over batch B. The objective corresponds to a
weighted k-means clustering problem  where we must select B to minimize the weighted distor-
tion between the simulated points and their closest points in B. The weight on each simulated
experiment αi x corresponds to the probability that the experiment x ∈ Si achieves the maximum
value of the unknown f among the experiments in Si  conditioned on D and the fact that Sk
π = Si.
We refer to this objective as the k-means objective.
We also consider a variant of this objective where the goal is to ﬁnd a B that minimizes (1) under
i Si s.t. |B| = k.
This objective corresponds to the weighted k-medoid clustering problem  which is often considered
to improve robustness to outliers in clustering. Accordingly we will refer to this objective as the
k-medoid objective and note that given a ﬁxed set of simulations this corresponds to a discrete
optimization problem.
3.2 Optimization Approach
The above k-means and k-medoid objectives involve the weights αi x = P (x = x∗
π =
Si)  for each x ∈ Si. In general these weights will be difﬁcult to compute exactly  particularly
1For example  this can be done by starting with D and selecting the ﬁrst experiment x1 using π and then
using P (f | D) to simulate the result y1 of experiment x1. This simulated experiment is added to D and the
process repeats for k − 1 additional experiments.

i (f ) | D  Sk

3

Algorithm 1 Greedy Weighted k-Medoid Algorithm
Input:S = {(x1  w1)  . . .   (xm  wm)}  k
Output:B

B ← {x1  . . .   xm} // initialize batch to all data points
while |B| > k do
x ← arg minx∈B
B ← B \ x

(cid:80)m
j=1 wj· (cid:107) xj − nn(xj  B \ x) (cid:107) // point that inﬂuences objective the least

end while
return B

due to the conditioning on the set Si. In this work  we approximate those weights by dropping the
conditioning on Si  for which it is then possible to derive a closed form when the posterior over f is
represented as a Gaussian Process (GP). We have found that this approach leads to good empirical
performance. In particular  instead of using the weights αi x we use the weights ˆαi x = P (x =
i (f ) | D). When the posterior over f is represented as a GP  as in our experiments  the joint
x∗
distribution over experimental outcomes in Si = {xi 1  . . .   xi k} is normally distributed. That is 
the random vector (cid:104)f (xi 1)  . . .   f (xi k)(cid:105) ∼ N (µ  Σ)  where the mean µ and covariance Σ have
standard closed forms given by the GP conditioned on D. From this  it is clear that for a GP the
computation of ˆαi x is equivalent to computing the probability that the ith component of a normally
distributed vector is larger than the other components. A closed form solution for this probability is
given by the following proposition.

Proposition 1. If (y1  y2  . . .   yk) ∼ N(cid:0)µy  Σy

(cid:1) then for any i ∈ {1  . . .   k} 

k−1(cid:89)

P (yi ≥ y1  yi ≥ y2  . . .   yi ≥ yk) =

(1 − Φ(−µj))

(2)

j=1

where Φ(.) is standard normal cdf  µ = (µ1  µ2 · · ·  µk−1) = (cid:0)AΣyA(cid:48)(cid:1)− 1

2 Aµy  such that A ∈
R(k−1)×k is a sparse matrix that for any j = 1  2 ···  k− 1 we have Aj i = 1  and for any 1 ≤ p < i
we have Ap p = −1   and for any i < p ≤ k we have Ap−1 p = −1.
Using this approach to compute the weights we can now consider optimizing the k-means and k-
medoid objectives from (1)  both of which are known to be NP-hard problems. For the k-means
objective we solve for the set B by simply applying the k-means clustering algorithm [13] to the

{(x  ˆαi x)}. The k cluster centers are returned as our batch B.

weighted data set(cid:83)

(cid:83)

x∈Si

i

The k-medoid objective is well known [22] and the weighted k-medoid clustering algorithm [11]
has been shown to perform well and be robust to outliers in the data. While we have experimented
with this algorithm and obtained good results  we have achieved results that are as good or better
using an alternative greedy algorithm that provides certain approximation guarantees. Pseudo-code
for this algorithm is shown in Figure 1. The input to the algorithm is the set of weighted experiments
and the batch size k. The algorithm initializes the batch B to include all of the input experiments 
which achieves the minimum objective value of zero. The algorithm then iteratively removes one
experiment from B at a time until |B| = k  each time removing the element whose removal results
in the smallest increase in the k-medoid objective.
This greedy algorithm is motivated by theoretical results on the minimization of non-increasing 
supermodular set functions.
Deﬁnition 1. Suppose S is a ﬁnite set  f : 2S → R+ is a supermodular set function if for all
B1 ⊆ B2 ⊆ S and {x} ∈ S \ B2  it holds that f (B1) − f (B1 ∪ {x}) ≥ f (B2) − f (B2 ∪ {x}).
Thus  a set function is supermodular if adding an element to a smaller set provides no less improve-
ment than adding the element to a larger set. Also  a set function is non-increasing if for any set
S and element x if f (S) ≥ f (S ∪ {x}). It can be shown that our k-medoid objective function of
(1) is both a non-increasing and supermodular function of B and achieves a minimum value of zero
i Si. It follows that we can obtain an approximation guarantee for the described greedy

for B =(cid:83)

algorithm in [9].

4

Theorem 1. [9] Let f be a monotonic non-increasing supermodular function over subsets of the
ﬁnite set S  |S| = m and f (S) = 0. Let B be the set of the elements returned by the greedy
algorithm 1 s.t |B| = k  q = m − k and B∗ = argminB(cid:48)⊆S |B(cid:48)|=k f (B(cid:48))  then

(cid:20)(cid:18) q + t

(cid:21)
(cid:19)q − 1

q

f (B) ≤ 1
t

f (B∗) ≤ et − 1

t

f (B∗)

(3)

where t is the steepness parameter [9] of function f.

Notice that the approximation bound involves the steepness parameter t of f  which characterizes
the rate of decrease of f. This is unavoidable since it is known that achieving a constant factor
approximation guarantee is not possible unless P=NP [17]. Further this bound has been shown to be
tight for any t [9]. Note that this is in contrast to guarantees for greedy maximization of submodular
functions [7] for which there are constant factor guarantees. Also note that the greedy algorithm
we use is qualitatively different from the one used for submodular maximization  since it greedily
removes elements from B rather than greedily adding elements to B.
4
GP Posterior. Our batch selection approach described above requires that we maintain a posterior
over the unknown function f. For this purpose we use a zero-mean GP prior with a zero-mean
Gaussian noise model with variance equal to 0.01. The GP covariance is speciﬁed by a Gaussian

2w (cid:107) x − x(cid:48) (cid:107)2(cid:1)  with signal variance σ = y2max where ymax is the

Implementation Details and Baselines

kernel K(x  x(cid:48)) = σ exp(cid:0)− 1
to set the kernel width w to 0.01(cid:80)d

maximum value of the unknown function. In all of our experiments we used a simple rule of thumb
i=1 li where li is the input space length in dimension i. We have
found this rule to work well for a variety of problems. An alternative would be to use a validation-
based approach for selecting the kernel parameters. In the BO setting  however  we have found this
to be unreliable since the number of data points is relatively small.
Base Sequential Policy. Our batch selection approach also requires a base sequential policy π to be
used for simulation matching. This policy must be able to select the next experiment given any set
of prior experimental observations D. In our experiments  we use a policy based on the Maximum
Expected Improvement (MEI) heuristic [14  10] which is a very successful sequential policy for BO
and has been shown to converge in the limit to the global optimum. Given data D the MEI policy
simply selects the next experiment to be the one that maximizes the expected improvement over the
current set of experiments with respect to maximizing the unknown function. More formally  let y∗
be the value of the best/largest experimental outcome observed so far in D. The MEI value of an
experiment x is given by MEI(x) = Ef [max{f (x) − y∗  0} | D]. For our GP posterior over f we
can derive a closed form for this given by: u = y∗−µ(x)
σ(x) where y∗ is our best currently observed
value. For any given example x  the MEI can be computed as follows:

MEI(x) = σ(x) [−uΦ(−u) + φ(u)]   u =

y∗ − µ(x)

σ(x)

where Φ and φ are the standard normal cumulative distribution and density functions and µ(x) and
σ(x) are the mean and variance of f (x) according to the GP given D  which have simple closed
forms. Note that we have also evaluated our simulation-matching approach with an alternative
sequential policy known as Maximum Probability of Improvement [16  10]. The results (not shown
in this paper) are similar to those obtained from MEI  showing that our general approach works well
for different base policies.
The computation of the MEI policy requires maximizing MEI(x) over the input space X . In gen-
eral  this function does not have a unique local maximum and various strategies have been tried for
maximizing it. In our experiments  we (approximately) maximize the MEI function using the DI-
RECT black-box optimization procedure  which has shown good optimization performance as well
as computational efﬁciency in practice.
Baseline Batch Policies. To the best of our knowledge there is no well-known batch policy for
Bayesian optimization. However  in our experiments we will compare against two baselines. The
ﬁrst baseline is random selection  where a batch of k random experiments is returned at each step. In-
terestingly  in the case of batch active learning for classiﬁcation  the random batch selection strategy

5

Function
Cosines
Rosenbrock
Michalewicz

1 − (u2 + v2 − 0.3cos(3πu) − 0.3cos(3πv))

u = 1.6x − 0.5  v = 1.6y − 0.5

Mathematical representation
10 − 100(y − x2)2 − (1 − x)2

(cid:16)

(cid:17)(cid:17)20

(cid:16) i.x2

i

π

Table 1: Benchmark Functions.

−(cid:80)5

i=1 sin(xi).

sin

has been surprisingly effective and is often difﬁcult to outperform with more sophisticated strategies
[8]. However  as our experiments will show  our approach will dominate random.
Our second  more sophisticate  baseline is based on selecting a batch of experiments whose expected
maximum output is the largest. More formally  we consider selecting a size k batch B that max-
imizes the objective Ef [maxx∈B f (x) | D]  which we will refer to as the EMAX objective. For
our GP prior  each set B = {x1  . . .   xk} can be viewed as deﬁning a normally distributed vec-
tor (cid:104)f (x1)  . . .   f (xk)(cid:105) ∼ N (µ  Σ). Even in this case  ﬁnding the optimal set B is known to be
NP-hard. However  for the case where f is assumed to be non-negative  the EMAX objective is
a non-negative  submodular  non-decreasing function of B. Together these properties imply that a
simple greedy algorithm can achieve an approximation ratio of 1 − e−1 [7]. The algorithm starts
with an empty B and greedily adds experiments to B  each time selecting the one that improves the
EMAX objective the most. Unfortunately  in general there is no closed form solution for evaluating
the EMAX objective  even in our case of normally distributed vectors [20]. Therefore  to imple-
ment the greedy algorithm  which requires many evaluations of the EMAX objective  we use Monte
Carlo sampling  where for a given set B we sample the corresponding normally distributed vector
and average the maximum values across the samples.
5 Experimental Results
In this section we evaluate our proposed batch BO approach and the baseline approaches on six
different benchmarks.
5.1 Benchmark Functions
We consider three well-known synthetic benchmark functions: Cosines and Rosenbrock [1  5] 
which are over [0  1]2  and Michalewicz [15]  which is over [0  π]5. Table 1 gives the formulas
for each of these functions. Two additional benchmark functions Hydrogen and FuelCell  which
range over [0  1]2  are derived from real-world experimental data sets. In both cases  the bench-
mark function was created by ﬁtting regression models to data sets resulting from real experiments.
The Hydrogen data set is the result of data collected as part of a study on biosolar hydrogen pro-
duction [6]  where the goal was to maximize the hydrogen production of a particular bacteria by
optimizing the PH and Nitrogen levels of the growth medium. The FuelCell data set was collected
as part of a study investigating the inﬂuence of anodes’ nano-structure on the power output of mi-
crobial fuel cells [3]. The experimental inputs include the average area and average circularity of
the nano-particles [18]. Contour plots of the four 2-d functions are shown in Figure 1.
The last benchmark function is derived from the Cart-Pole [2] problem  which is a commonly used
reinforcement learning problem. The goal is to optimize the parameters of a controller for a wheeled
cart with the objective of balancing a pole. The controller is parameterized by four parameters
giving a 4-d space of experiments in [1 −1]4. Given a setting for these parameters  the benchmark
function is implemented by using the standard Cart-Pole simulator to return the reward received for
the controller.
5.2 Results
Figures 2 and 3 show the performance of our methods on all six benchmark functions for batch sizes
5 and 10 respectively. Each graph contains 5 curves  each corresponding to a different BO approach
(see below). Each curve is the result of taking an average of 100 independent runs. The x-axis of
each graph represents the total number of experiments and the y-axis represents the regret values 
where the regret of a policy at a particular point is the difference between the best possible output
value (or an upper bound if the value is not known) and the best value found by the policy. Hence the
regret is always positive and smaller values are preferred. Each run of a policy initializes the data set
to contain 5 randomly selected experiments for the 2-d functions and 20 random initial experiments
for the higher dimensional functions.

6

Fuel Cell

Hydrogen

Cosines

Rosenbrock

Figure 1: The contour plots for the four 2−dimension proposed test functions.

Fuel Cell

Hydrogen

Cosines

Rosenbrock

Cart-Pole

Michalewicz

Figure 2: Performance evaluation with batch size 5.

Fuel Cell

Hydrogen

Cosines

Rosenbrock

Cart-Pole

Michalewicz

Figure 3: Performance evaluation with batch size 10.

7

00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.911015202530350.20.250.30.350.40.450.50.550.60.65# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandom10152025303500.050.10.150.20.250.3# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandom1015202530350.10.20.30.40.50.6# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandom10152025303500.050.10.150.20.250.30.350.40.450.5# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandom255075100125150175200200300400500600700800900# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandom25303540455055606570758022.12.22.32.42.52.62.72.82.93# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandom15202530350.20.250.30.350.40.450.50.550.60.65# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandom152025303500.050.10.150.20.25# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandom15202530350.10.150.20.250.30.350.40.450.50.550.6# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandom152025303500.050.10.150.20.250.30.35# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandom306090120150180200200300400500600700800900# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandom3040506070802.12.22.32.42.52.62.72.82.93# of ExperimetsRegret  Sequentialk−medoidk−meansEMAXRandomEach graph gives curves for four batch approaches including our baselines Random and EMAX 
along with our proposed approaches based on the k-means and k-medoid objectives  which are
optimized by weighted k-means clustering and the greedy Algorithm 1 respectively. In addition  for
reference we plot the performance of the base Sequential MEI BO policy (k = 1) on each graph.
Note that since the batch approaches request either 5 or 10 experiments at a time  their curves only
contain data points at those intervals. For example  for the batch size 5 results the ﬁrst point on a
batch curve corresponds to 10 experiments  including the initial 5 experiments and the ﬁrst requested
batch. The next point on the batch curve is for 15 experiments which includes the next requested
batch and so on. Rather the Sequential policy has a point at every step since it requests experiments
one at a time. It is important to realize that we generally expect a good sequential policy to do better 
or no worse  than a batch policy with respect to performance per number of experiments. Thus  the
Sequential curve can be typically viewed as an upper performance bound and provides an indication
of how much loss is incurred when moving to a batch setting in terms of efﬁciency per experiment.
Comparison to Baselines. The major observation from our results is that for all benchmarks and
for both batch sizes the proposed k-means and k-medoid approaches signiﬁcantly outperform the
baselines. This provides strong validation for our proposed simulation-matching approach to batch
selection.
k-means vs. k-medoid. In most cases  the k-means and k-medoid approaches perform similarly.
However  for both batch sizes k-medoid often does shows a small improvement over k-means and
appears to have a signiﬁcant advantage in FuelCell. The only exception is in Hydrogen where k-
means shows a small advantage over k-medoid for small numbers of experiments. Overall  both
approaches appear to be effective and in these domains k-medoid has a slight edge.
Batch vs. Sequential. The advantage of Sequential over our batch approaches varies with the bench-
mark. However  in most cases  our proposed batch approaches catch up to Sequential in a relatively
small number of experiments and in some cases  the batch policies are similar to Sequential from
the start. The main exception is Cart-Pole for batch size 10  where the batch policies appear to be
signiﬁcantly less efﬁcient in terms of performance versus number of experiments. Generally  we see
that the difference between our batch policies and Sequential is larger for batch size 10 than batch
size 5  which is expected  since larger batch sizes imply that less information per experiment is used
in making decisions.
It is clear  however  that if we evaluate the performance of our batch policies in terms of experi-
mental time  then there is a very signiﬁcant advantage over Sequential. In particular  the amount of
experimental time for a policy is approximately equal to the number of requested batches  assuming
that the batch size is selected to allow for all selected experiments to be run in parallel. This means 
for example  that for the batch size 5 results  5 time steps for the batch approaches correspond to
30 total experiments (5 initial + 5 batches). We can compare this point to the ﬁrst point on the
Sequential curve  which also corresponds to 5 time steps (5 experiments beyond the initial 5). In all
cases  the batch policies yield a very large improvement in regret reduction per unit time  which is
the primary motivation for batch selection.

6 Summary and Future Work

In this paper we introduced a novel approach to batch BO based on the idea of simulation matching.
The key idea of our approach is to design batches of experiments that approximately match the
expected performance of high-quality sequential policies for BO. We considered two variants of
the matching problem and showed that both approaches signiﬁcantly outperformed two baselines
including random batch selection on six benchmark functions. For future work we plan to consider
the general idea of simulation matching for other problems  such as active learning  where there are
also good sequential policies and batch selection is often warranted. In addition  we plan to consider
less myopic approaches for selecting each batch and the problem of batch size selection  where there
is a choice about batch size that must take into account the current data and experimental budget.

Acknowledgments

The authors acknowledge the support of the NSF under grants IIS-0905678.

8

References
[1] B. S. Anderson  A. W. More  and D. Cohn. A nonparametric approach to noisy and costly optimization.

In ICML  2000.

[2] A. G. Barto  R. S. Sutton  and C. W. Anderson. Neuronlike adaptive elements that can solve difﬁcult

learning control problems. 13:835–846  1983.

[3] D. Bond and D. Lovley. Electricity production by geobacter sulfurreducens attached to electrodes. Appli-

cations of Environmental Microbiology  69:1548–1555  2003.

[4] E. Brochu  M. Cora  and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions 
with application to active user modeling and hierarchical reinforcement learning. Technical Report TR-
2009-23  Department of Computer Science  University of British Columbia  2009.

[5] M. Brunato  R. Battiti  and S. Pasupuleti. A memory-based rash optimizer. In AAAI-06 Workshop on

Heuristic Search  Memory Based Heuristics and Their applications  2006.

[6] E. H. Burrows  W.-K. Wong  X. Fern  F. W. Chaplen  and R. L. Ely. Optimization of ph and nitrogen
for enhanced hydrogen production by synechocystis sp. pcc 6803 via statistical and machine learning
methods. Biotechnology Progress  25:1009–1017  2009.

[7] M. F. G Nemhauser  L Wolsey. An analysis of the approximations for maximizing submodular set func-

tions. Mathematical Programmingn  14:265–294  1978.

[8] Y. Guo and D. Schuurmans. Discriminative batch mode active learning. Proceedings of Advances in

Neural Information Processing Systems (NIPS2007)  6  2007.

[9] V. P. Il’ev. An approximation guarantee of the greedy descent algorithm for minimizing a supermodular

set function. Discrete Applied Mathematics  114(1-3):131–146  2001.

[10] D. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global

Optimization  21:345–383  2001.

[11] L. Kaufman and P. J. Rousseeuw. Clustering by means of medoids. Statistical data analysis based on L1

norm  pages 405–416  1987.

[12] D. Lizotte. Practical Bayesian optimization. PhD thesis  University of Alberta  2008.
[13] S. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory  28(2):129–137 

1982.

[14] M. Locatelli. Bayesian algorithms for one-dimensional globaloptimization. J. of Global Optimization 

10(1):57–76  1997.

[15] Z. Michalewicz. Genetic algorithms + data structures = evolution programs (2nd  extended ed.).

Springer-Verlag New York  Inc.  New York  NY  USA  1994.

[16] A. Moore and J. Schneider. Memory-based stochastic optimization. In NIPS  1995.
[17] G. Nemhauser and L. Wolsey. Integer and combinatorial optimization. Wiley New York  1999.
[18] D. Park and J. Zeikus. Improved fuel cell and electrode designs for producing electricity from microbial

degradation. Biotechnol.Bioeng.  81(3):348–355  2003.

[19] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT  2006.
[20] A. M. Ross. Computing Bounds on the Expected Maximum of Correlated Normal Variables . Methodol-

ogy and Computing in Applied Probability  2008.

[21] M. Schonlau. Computer Experiments and Global Optimization. PhD thesis  University of Waterloo  1997.
Integer programming and the theory of grouping. Journal of the American Statistical
[22] H. D. Vinod.

Association  64(326):506–519  1969.

9

,Kush Bhatia
Prateek Jain
Parameswaran Kamalaruban
Purushottam Kar