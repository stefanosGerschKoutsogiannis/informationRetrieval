2011,Maximum Margin Multi-Label Structured Prediction,We study multi-label prediction for structured output spaces  a problem that occurs  for example  in object detection in images  secondary structure prediction in computational biology  and graph matching with symmetries. Conventional multi-label classification techniques are typically not applicable in this situation  because they require explicit enumeration of the label space  which is infeasible in case of structured outputs. Relying on techniques originally designed for single- label structured prediction  in particular structured support vector machines  results in reduced prediction accuracy  or leads to infeasible optimization problems.  In this work we derive a maximum-margin training formulation for multi-label structured prediction that remains computationally tractable while achieving high prediction accuracy. It also shares most beneficial properties with single-label maximum-margin approaches  in particular a formulation as a convex optimization problem  efficient working set training  and PAC-Bayesian generalization bounds.,Maximum Margin Multi-Label Structured Prediction

Christoph H. Lampert

IST Austria (Institute of Science and Technology Austria)

Am Campus 1  3400 Klosterneuburg  Austria

http://www.ist.ac.at/∼chl

chl@ist.ac.at

Abstract

We study multi-label prediction for structured output sets  a problem that occurs 
for example  in object detection in images  secondary structure prediction in com-
putational biology  and graph matching with symmetries. Conventional multi-
label classiﬁcation techniques are typically not applicable in this situation  be-
cause they require explicit enumeration of the label set  which is infeasible in case
of structured outputs. Relying on techniques originally designed for single-label
structured prediction  in particular structured support vector machines  results in
reduced prediction accuracy  or leads to infeasible optimization problems.

In this work we derive a maximum-margin training formulation for multi-label
structured prediction that remains computationally tractable while achieving high
prediction accuracy.
It also shares most beneﬁcial properties with single-label
maximum-margin approaches  in particular formulation as a convex optimization
problem  efﬁcient working set training  and PAC-Bayesian generalization bounds.

1

Introduction

The recent development of conditional random ﬁelds (CRFs) [1]  max-margin Markov networks
(M3Ns) [2]  and structured support vector machines (SSVMs) [3] has triggered a wave of interest in
the prediction of complex outputs. Typically  these are formulated as graph labeling or graph match-
ing tasks in which each input has a unique correct output. However  not all problems encountered
in real applications are reﬂected well by this assumption: machine translation in natural language
processing  secondary structure prediction in computational biology  and object detection in com-
puter vision are examples of tasks in which more than one prediction can be “correct” for each data
sample  and that are therefore more naturally formulated as multi-label prediction tasks.
In this paper  we study multi-label structured prediction  deﬁning the task and introducing the nec-
essary notation in Section 2. Our main contribution is a formulation of a maximum-margin training
problem  named MLSP  which we introduce in Section 3. Once trained it allows the prediction of
multiple structured outputs from a single input  as well as abstaining from a decision. We study
the generalization properties of MLSP in form of a generalization bound in Section 3.2  and we
introduce a working set optimization procedure in Section 3.3. The main insights from these is that
MLSP behaves similarly to a single-label SSVM in terms of efﬁcient use of training data and compu-
tational effort during training  despite the increased complexity of the problem setting. In Section 4
we discuss MLSP’s relation to existing methods for multi-label prediction with simple label sets 
and to single-label structured prediction. We furthermore compare MLSP to a multi-label structured
prediction methods within the SSVM framework in Section 4.1. In Section 5 we compare the dif-
ferent approaches experimentally  and we conclude in Section 6 by summarizing and discussing our
contribution.

1

2 Multi-label structured prediction
We ﬁrst recall some background and establish the notation necessary to discuss multi-label classi-
ﬁcation and structured prediction in a maximum margin framework. Our overall task is predicting
outputs y ∈ Y for inputs x ∈ X in a supervised learning setting.
In ordinary (single-label) multi-class prediction we use a prediction function  g : X → Y  for this 
which we learn from i.i.d. example pairs {(xi  yi)}i=1 ... n ⊂ X ×Y. Adopting a maximum-margin
setting  we set

g(x) := argmaxy∈Y f (x  y)

for a compatibility function f (x  y) := (cid:104)w  ψ(x  y)(cid:105).

(1)
The joint feature map ψ : X ×Y → H maps input-output pairs into a Hilbert space H with inner
product (cid:104)·  ·(cid:105). It is deﬁned either explicitly  or implicitly through a joint kernel function k : (X ×
Y) × (X ×Y) → R. We measure the quality of predictions by a task-dependent loss function
∆ : Y × Y → R+  where ∆(y  ¯y) speciﬁes what cost occurs if we predict an output ¯y while the
correct prediction is y.
Structured output prediction can be seen as a generalization of the above setting  where one wants to
make not only one  but several dependent decisions at the same time  for example  deciding for each
pixel of an image to which out of several semantic classes it belongs. Equivalently  one can interpret
the same task as a special case of supervised single-label prediction  where inputs and outputs consist
of multiple parts. In the above example  a whole image is one input sample  and a segmentation mask
with as many entries as the image has pixels is an output. Having a choice of M ≥ 2 classes per pixel
of a (w×h)-sized image leads to an output set of M w·h elements. Enumerating all of these is out of
question  and collecting training examples for each of them even more so. Consequently  structured
output prediction requires specialized techniques that avoid enumerating all possible outputs  and
that can generalize between labels in the output set. A popular technique for this task is the structured
(output) support vector machine (SSVM) [3]. To train it  one has to solve a quadratic program
subject to n|Y| linear constraints. If an efﬁcient separation oracle is available  i.e. a technique for
identifying the currently most violated linear constraints  working set training  in particular cutting
plane [4] or bundle methods [5] allow SSVM training to arbitrary precision in polynomial time.
Multi-label prediction is a generalization of single-label prediction that gives up the condition of a
functional relation between inputs and outputs. Instead  each input object can be associated with
any (ﬁnite) number of outputs  including none. Formally  we are given pairs {(xi  Y i)}i=1 ... n ⊂
X ×P(Y)  where P denotes the power set operation  and we want to determine a set-valued function
G : X → P(Y). Often it is convenient to use indicator vectors instead of variable size subsets.
We say that v ∈ {±1}Y represents the subset Y ∈ P(Y) if vy = +1 for y ∈ Y and vy = −1
otherwise. Where no confusion arises  we use both representations interchangeably  e.g.  we write
either Y i or vi for a label set in the training data. To measure the quality of a predicted set we use a
set loss function ∆ML : P(Y) × P(Y) → R. Note that multi-label prediction can also be interpreted
as ordinary single-output prediction with P(Y) taking the place of the original output set Y. We will
come back to this view in Section 4.1 when discussing related work.
Multi-label structured prediction combines the aspects of multi-label prediction and structured out-
put sets: we are given a training set {(xi  Y i)}i=1 ... n ⊂ X × P(Y)  where Y is a structured output
set of potentially very large size  and we would like to learn a prediction function: G : X → P(Y)
with the ability to generalize also in the output set. In the following  we will take the view of struc-
tured prediction point of view  deriving expressions for predicting multiple structured outputs instead
of single ones. Alternatively  the same conclusions could be reached by interpreting the task as per-
forming multi-label predicting with binary output vectors that are too large to store or enumerate
explicitly  but that have an internal structure allowing generalization between the elements.

3 Maximum margin multi-label structured prediction

In this section we propose a learning technique designed for multi-label structure prediction that we
call MLSP. It makes set-valued prediction by1 

G(x) := {y ∈ Y : f (x  y) > 0}

for

f (x  y) := (cid:104)w  ψ(x  y)(cid:105).

(2)

1More complex prediction rules exist in the multi-label literature  see  e.g.  [6]. We restrict ourselves to per-
label thresholding  because more advanced rules complicate the learning and prediction problem even further.

2

Note that the compatibility function  f (x  y)  acts on individual inputs and outputs  as in single-label
prediction (1)  but the prediction step consists of collecting all outputs of positive scores instead of
ﬁnding the outputs of maximal score. By including a constant entry into the joint feature map ψ(x  y)
we can model a bias term  thereby avoiding the need of a threshold during prediction (2). We can
also add further ﬂexibility by a data-independent  but label-dependent term. Note that our setup
differs from SSVMs training in this regard. There  a bias term  or a constant entry of the feature
map  would have no inﬂuence  because during training only pairwise differences of function values
are considered  and during prediction a bias does not affect the argmax-decision in Equation (1).
We learn the weight vector w for the MLSP compatibility function in a maximum-margin framework
that is derived from regularized risk minimization. As the risk depends on the loss function chosen 
we ﬁrst study the possibilities we have for the set loss ∆ML : P(Y) × P(Y) → R+. There are no
established functions for this in the structured prediction setting  but it turns out that two canonical
set losses are consistent with the following ﬁrst principles. Positivity: ∆ML(Y  ¯Y ) ≥ 0  with equality
only if Y = ¯Y   Modularity: ∆ML should decompose over the elements of Y (in order to facilitate
efﬁcient computation)  Monotonicity: ∆ML should reﬂect that making a wrong decision about some
element y ∈ Y can never reduce the loss. The last criterion we formalize as
for any ¯y (cid:54)∈ Y   and
for any y (cid:54)∈ ¯Y .

∆ML(Y  ¯Y ∪ {¯y}) ≥ ∆ML(Y  ¯Y )
∆ML(Y ∪ {y}  ¯Y ) ≥ ∆ML(Y  ¯Y )

Two candidates that fulﬁll these criteria are the sum loss  ∆sum(Y  ¯Y ) :=(cid:80)

(3)
(4)
y∈Y (cid:127) ¯Y λ(Y  y)  and the
max loss  ∆max(Y  ¯Y ) := maxy∈Y (cid:127) ¯Y λ(Y  y)  where Y (cid:127) ¯Y := (Y\¯Y )∪( ¯Y\Y ) is the symmetric set
difference  and λ : P(Y) × Y → R+ is a task-dependent per-label misclassiﬁcation cost. Assuming
that a set Y is the correct prediction  λ(Y  ¯y) speciﬁes either the cost of predicting ¯y  although ¯y (cid:54)∈ Y  
or of not predicting ¯y  when really ¯y ∈ Y . In the special case of λ ≡ 1 the sum loss is known as
symmetric difference loss  and it coincides with the Hamming loss of the binary indicator vector
representation. The max loss becomes the 0/1-loss between sets in this case. In a general case  λ
typically expresses partial correctness  generalizing the single-label structured loss ∆(y  ¯y). Note
that in evaluating λ(Y  ¯y) one has access to the whole set Y   not just single elements. Therefore  a
ﬂexible penalization of multiple errors is possible  e.g.  submodular behavior.
While in the small-scale multi-label situation  the sum loss is more common  we argue in this work
that that the max loss has advantages in the structured prediction situation. For once  the sum loss has
a scaling problem. Because it adds potentially exponentially many terms  the ratio in loss between
making few mistakes and making many mistakes is very large. If used in the unnormalized form
given above this can result in impractically large values. Normalizing the expression by multiplying
with 1/|Y| stabilizes the upper value range  but it leads to a situation where ∆sum(Y  ¯Y ) ≈ 0 in
the common situation that ¯Y differs from Y in only a few elements. The value range of the max
loss  on the other hand  is the same as the value range of λ and therefore easy to keep reasonable.
A second advantage of the max loss is that it leads to an efﬁcient constraint generation technique
during training  as we will see in Section 3.3.

1

3.1 Maximum margin multi-label structured prediction (MLSP)
(cid:80)
To learn the parameters w of the compatibility function f (x  y) we follow a regularized risk mini-
mization framework: given i.i.d. training examples {(xi  Y i)}i=1 ... n  we would like to minimize
(cid:80)
2(cid:107)w(cid:107)2 + C
i ∆max(Y i  G(xi)). Using the deﬁnition of ∆max this is equivalent to minimizing
2(cid:107)w(cid:107)2 + C
yf (xi  y) ≤ 0. Upper bounding the
i ξi  subject to ξi ≥ λ(Y i  y) for all y ∈ Y with vi
1
n(cid:88)
inequalities by a Hinge construction yields the following maximum-margin training problem:

n

n

(w∗  ξ∗) =

(5)

argmin

w∈H ξ1 ... ξn∈R+

(cid:107)w(cid:107)2 +

1
2

C
n

ξi

i=1

subject to  for i = 1  . . .   n 

ξi ≥ λ(Y i  y)[1 − vi

yf (xi  y)]  for all y ∈ Y.

(6)

Note that making per-label decisions through thresholding does not rule out the sharing of information between
labels. In the terminology of [7]  Equation (2) corresponds to a conditional label independence assumption.
Through the joint feature function ψ(x  y) te proposed model can still learn unconditional dependence between
labels  which relates closer to an intuition of the form “Label A tends to co-occur with label B”.

3

Besides this slack rescaled variant  one can also form margin rescaled training using the constraints

ξi ≥ λ(Y i  y) − vi

yf (xi  y) 

(7)
Both variants coincide in the case of 0/1 set loss  λ(Y i  y) ≡ 1. The main difference between slack
and margin rescaled training is how they treat the case of λ(Y i  y) = 0 for some y ∈ Y. In slack
rescaling  the corresponding outputs have no effect on the training at all  whereas for margin rescal-
ing  no margin is enforced for such examples  but a penalization still occurs whenever f (xi  y) > 0
for y (cid:54)∈ Y i  or if f (xi  y) < 0 for y ∈ Y i.

for all y ∈ Y.

3.2 Generalization Properties
Maximum margin structured learning has become successful not only because it provides a pow-
erful framework for solving practical prediction problems  but also because it comes with certain
theoretical guarantees  in particular generalization bounds. We expect that many of these results
will have multi-label analogues. As an initial step  we formulate and prove a generalization bound
for slack-rescaled MLSP similar to the single-label SSVM analysis in [8].
Let Gw(x) := {y ∈ Y : fw(x  y) > 0} for fw(x  y) = (cid:104)w  ψ(x  y)(cid:105). We assume |Y| < r and
(cid:107)ψ(x  y)(cid:107) < s for all (x  y) ∈ X × Y  and λ(Y  y) ≤ Λ for all (Y  y) ∈ P(Y) × Y.
For any distribution Qw over weight vectors  that may depend on w  we denote by L(Qw  P ) the
expected ∆max-risk for P -distributed data 

(cid:8)RP ∆max(G ¯w)(cid:9) = E ¯w∼Qw (x Y )∼P

(cid:8)∆max(Y  G ¯w(x))(cid:9).

L(Qw  P ) = E ¯w∼Qw

(8)

The following theorem bounds the expected risk in terms of the total margin violations.
Theorem 1. With probability at least 1 − σ over the sample S of size n  the following inequality
holds simultaneously for all weight vectors w.

(cid:16) s2||w||2 ln(rn/||w||2) + ln n

(cid:17)1/2

σ

2(n − 1)

||w||2
n

+

(9)

L(Qw D) ≤ 1
n

(cid:96)(xi  Y i  f ) +

n(cid:88)

for (cid:96)(xi  Y i  f ) := max

i=1

y∈Y λ(Y i  y)(cid:74)vi

yf (xi  y) < 1(cid:75)  where vi is the binary indicator vector of Y i.

Proof. The argument follows [8  Section 11.6]. It can be found in the supplemental material.

A main insight from Theorem 1 is that the number of samples needed for good generalization grows
only logarithmically with r  i.e. the size of Y. This is the same complexity as for single-label
prediction using SSVMs  despite the fact that multi-label prediction formally maps into P(Y)  i.e.
an exponentially larger output set.

3.3 Numeric Optimization
The numeric solution of MLSP training resembles SSVM training. For explicitly given joint fea-
ture maps  ψ(x  y)  we can solve the optimization problem (5) in the primal  for example using
subgradient descent. To solve MLSP in a kernelized setup we introduce Lagrangian multipliers
(αi

y)i=1 ... n;y∈Y for the constraints (7)/(6). For the margin-rescaled variant we obtain the dual

¯y k(cid:0)(xi  y)  (x¯ı  ¯y)(cid:1) +

(cid:88)

λi
yαi
y

For slack-rescaled MLSP  the dual is computed analogously as

max
y∈R+
αi

(cid:88)
subject to (cid:88)
(cid:88)
subject to (cid:88)

max
y∈R+
αi

− 1
vi
yv¯ı
2
(i y) (¯ı ¯y)

¯yαi

yα¯ı

y ≤ C
αi
n

y

− 1
yv¯ı
vi
2
(i y) (¯ı ¯y)

¯yαi

yα¯ı

 

for i = 1  . . .   n.

¯y k(cid:0)(xi  y)  (x¯ı  ¯y)(cid:1) +

(i y)

(cid:88)

(i y)

αi
y

αi
y
λi
y

y

≤ C
n

 

for i = 1  . . .   n 

4

(10)

(11)

(12)

(13)

with the convention that only terms with λi
bility function becomes

(cid:88)

f (x  y) =

(14)

y (cid:54)= 0 enter the summation. In both cases  the compati-

¯y k(cid:0)(xi  ¯y)  (x  y)(cid:1).

αi
¯yvi

(i ¯y)

Comparing the optimization problems (10)/(11) and (12)/(13) to the ordinary SVM dual  we see
that MLSP couples |Y| binary SVM problems by the joint kernel function and the summed-over box
constraints. In particular  whenever only a feasibly small subset of variables has to be considered  we
can solve the problem using a general purpose QP solver  or a slightly modiﬁed SVM solver. Overall 
however  there are infeasibly many constraints in the primal  or variables in the dual. Analogously
to the SSVM situation we therefore apply iterative working set training  which we explain here
using the terminology of the primal. We start with an arbitrary  e.g. empty  working set S. Then  in
each step we solve the optimization using only the constraints indicated by the working set. For the
resulting solution (wS  ξS) we check whether any constraints of the full set (6)/(7) are violated up
to a target precision . If not  we have found the optimal parameters. Otherwise  we add the most
violated constraint to S and start the next iteration. The same monotonicity argument as in [3] shows
that we reach an objective value -close to the optimal one within O( 1
 ) steps. Consequently  MLSP
training is roughly comparable in computational complexity to SSVM training.
The crucial step in working set training is the identiﬁcation of violated constraints. Note that con-
straints in MLSP are determined by pairs of samples and single labels  not pairs of samples and
sets of labels. This allows us to reuse existing methods for loss augmented single label inference.
In practice  it is safe to assume that the sets Y i are feasibly small  since they are given to us ex-
plicitly. Consequently  we can identify violated “positive” constraints by explicitly checking the
inequalities (7)/(6) for y ∈ Y i. Identifying violated “negative” constraint requires loss-augmented
prediction over Y\Y i. We are not aware of a general purpose solution for this task  but at least all
problems that allow efﬁcient K-best MAP prediction can be handled by iteratively performing loss-
augmented prediction within Y until a violating example from Y\Y i is found  or it is conﬁrmed that
no such example exists. Note that K-best versions of most standard MAP prediction methods have
been developed  including max-ﬂow [9]  loopy BP [10]  LP-relaxations [11]  and Sampling [12].

3.4 Prediction problem
After training  Equation (2) speciﬁes the rule to predict output sets for new input data. In contrast
to single-label SSVM prediction this requires not only a maximization over all elements of Y  but
the collection of all elements y ∈ Y of positive score. The structure of the output set is not as
immediately helpful for this as it is  e.g.  in MAP prediction. Task-speciﬁc solutions exist  however 
for example branch-and-bound search for object detection [13]. Also  it is often possible to establish
an upper bound on the number of desired outputs  and then  K-best prediction techniques can again
be applied. This makes MLSP of potential use for several classical tasks  such as parsing and
chunking in natural language processing  secondary structured prediction in computational biology 
or human pose estimation in computer vision. In general situations  evaluating (2) might require
approximate structured prediction techniques  e.g. iterative greedy selection [14]. Note that the use
of approximation algorithms is little problematic here  because  in contrast to training  the prediction
step is not performed in an iterative manner  so errors do not accumulate.
4 Related Work
Multi-label classiﬁcation is an established ﬁeld of research in machine learning and several es-
tablished techniques are available  most of which fall into one of three categories: 1) Multi-class
reformulations [15] treat every possible label subset  Y ∈ P(Y)  as a new class in an independent
multi-class classiﬁcation scenario. 2) Per-label decomposition [16] trains one classiﬁer for each out-
put label and makes independent decision for each of those. 3) Label ranking [17] learns a function
that ranks all potential labels for an input sample. Given the size of Y  1) is not a promising direction
for multi-label structured prediction. A straight-forward application of 2) and 3) are also infeasible
if Y is too large to enumerate. However  MLSP resembles both approaches by sharing their predic-
tion rule (2). MLSP can be seen as a way to make a combination of approaches applicable to the
situation of structured prediction by incorporating the ability to generalize in the label set.
Besides the general concepts above  many speciﬁc techniques for multi-label prediction have been
proposed  several of them making use of structured prediction techniques: [18] introduces an SSVM

5

formulation that allows direct optimization of the average precision ranking loss when the label
set can be enumerated. [19] relies on a counting framework for this purpose  and [20] proposes
an SSVM formulation for enforcing diversity between the labels.
[21] and [22] identify shared
subspaces between sets of labels  [23] encodes linear label relations by a change of the SSVM
regularizer  and [24] handles the case of tree- and DAG-structured dependencies between possible
outputs. All these methods work in the multi-class setup and require an explicit enumerations of the
label set. They use a structured prediction framework to encode dependencies between the individual
output labels  of which there are relatively few. MLSP  on the other hand  aims at predicting multiple
structured object  i.e. the structured prediction framework is not just a tool to improve multi-class
classiﬁcation with multiple output labels  but it is required as a core component for predicting even
a single output.
Some previous methods targeting multi-label prediction with large output sets  in particular using
label compression [25] or a label hierarchy [26]. This allows handling thousands of potential output
classes  but a direct application to the structured prediction situation is not possible  because the
methods still require explicit handling of the output label vectors  or cannot predict labels that were
not part of the training set.
The actual task of predicting multiple structured outputs has so far not appeared explicitly in the
literature. The situation of multiple inputs during training has  however  received some attention:
[27] introduces a one-class SVM based training technique for learning with ambiguous ground truth
data. [13] trains an SSVM for the same task by deﬁning a task-adapted loss function ∆min(Y  ¯y) =
miny∈Y ∆(y  ¯y). [28] uses a similar min-loss in a CRF setup to overcome problems with incomplete
annotation. Note that ∆min(Y  ¯y) has the right signature to be used as a misclassiﬁcation cost λ(Y  ¯y)
in MLSP. The compatibility functions learned by the maximum-margin techniques [13  27] have
the same functional form as f (x  y) in MLSP  so they can  in principle  be used to predict multiple
outputs using Equation (2). However  our experiments of Section 5 show that this leads to low multi-
label prediction accuracy  because the training setup is not designed for this evaluation procedure.

4.1 Structured Multilabel Prediction in the SSVM Framework
At ﬁrst sight  it appears unnecessary to go beyond the standard structured prediction framework at
all in trying to predict subsets of Y. As mentioned in Section 3  multi-label prediction into Y can
be interpreted as single-label prediction into P(Y)  so a straight-forward approach to multi-label
structured prediction would be to use an ordinary SSVM with output set P(Y). We will call this
setup P-SSVM. It has previously been proposed for classical multi-label prediction  for example
in [23]. Unfortunately  as we will show in this section  the P-SSVM setup is not well suited to the
structured prediction situation.
A P-SSVM learns a prediction function  G(x) := argmaxY ∈P(Y) F (x  Y )  with linearly parameter-
ized compatibility function  F (x  Y ) := (cid:104)w  ψ(x  Y )(cid:105)  by solving the optimization problem

argmin

(cid:107)w(cid:107)2 +

1
2

C
n

ξi 

subject to ξi ≥ ∆(yi  Y ) + F (xi  Y ) − F (xi  Y i)  (15)

i=1

w∈H ξ1 ... ξn∈R+
for i = 1  . . .   n  and for all Y ∈ P(Y). The main problem with this general form is that identifying
violated constraints of (15) requires loss-augmented maximization of F over P(Y)  i.e. an exponen-
tially larger set than Y. To better understand this problem  we analyze what happens when making
the same simplifying assumptions as for MLSP in Section 3.1. First  we assume additivity of F over
y∈Y f (x  y) for f (x  y) := (cid:104)w  ψ(x  y)(cid:105). This turns the argmax-evaluation

for G(x) exactly into the prediction rule (2)  and the constraint set in (15) simpliﬁes to

Y  i.e. F (x  Y ) := (cid:80)
ξi ≥ ∆ML(Y i  Y ) −(cid:88)
sum loss does: with ∆ML(Y i  Y ) = (cid:80)

y∈Y (cid:127)Y i

n(cid:88)

vi
yf (xi  y) 

for i = 1  . . .   n  and for all Y ∈ P(Y) 

(16)

Choosing ∆ML as max loss does not allow us to further simplify this expression  but choosing the
y∈Y (cid:127)Y i λ(Y i  y)  we obtain an explicit expression for the

label set maximizing the right hand side of the constraint (16)  namely

viol ={y ∈ Y i : f (xi  y) < λ(Y i  y)} ∪ {y ∈ Y \ Y i : f (xi  y) > −λ(Y i  y)}.
Y i
Thus  we avoid having to maximize a function over P(Y). Unfortunately  the set Y i
tion (17) can contain exponentially many terms  rendering a numeric computation of F (xi  Y i

(17)
viol in Equa-
viol) or

6

viol. In fact  starting the optimization with w = 0 would already lead to Y i

its gradient still infeasible in general. Note that this is not just a rare  easily avoidable case. Because
w  and thereby f  are learned iteratively  they typically go through phases of low prediction qual-
viol = Y
ity  i.e. large Y i
for all i = 1  . . .   n. Consequently  we presume that P-SSVM training is intractable for structured
prediction problems  except for the case of a small label set.
Note that while computational complexity is the most prominent problem of P-SSVM training  it is
not the only one. For example  even if we did ﬁnd a polynomial-time training algorithm to solve (15)
the generalization ability of the resulting predictor would be unclear:
the SSVM-generalization
bounds [8] suggest that training sets of size O(log |P(Y)|) = O(|Y|) will be required  compared to
the O(log |Y|) bound we established for MLSP in Section 3.2.
5 Experimental Evaluation
To show the practical use of MLSP we performed experiments on multi-label hierarchical classi-
ﬁcation and object detection in natural images. The complete protocol of training a miniature toy
example can be found in the supplemental material (available from the author’s homepage).

5.1 Multi-label hierarchical classiﬁcation
We use hierarchical classiﬁcation as an illustrative example that in particular allows us to compare
MLSP to alternative  less scalable  methods. On the one hand  it is straight-forward to model as a
structured prediction task  see e.g. [3  29  30  31]. On the other hand  its output set is small enough
such that we can compare MLSP also against other approaches that cannot handle very large output
sets  in particular P-SSVM and independent per-label training.
The task in hierarchical classiﬁcation is to classify samples into a number of discrete classes  where
each class corresponds to a path in a tree. Classes are considered related if they share a path in the
tree  and this is reﬂected by sharing parts of the joint feature representations. In our experiments 
we use the PASCAL VOC2006 dataset that contains 5304 images  each belonging to between 1
and 4 out of 10 classes. We represent each image x by 960-dimensional GIST features φ(x) and
use the same 19-node hierarchy κ and joint feature function  ψ(x  y) = vec(φ(x) ⊗ κ(y))  as in
[30]. As baselines we use P-SSVM [23]  JKSE [27]  and an SSVM trained with the normal  single-
label objective  but evaluated by Equation (2). We follow the pre-deﬁned data splits  doing model
selection using the train and val parts to determine C ∈ {2−1  . . .   214} (MLSP  P-SSVM  SSVM) 
or ν ∈ {0.05  0.10  . . .   0.95} (JKSE). We then retrain on the combination of train and val and we
test on the test part of the dataset. As the label set is small  we use exhaustive search over Y to
identify violated constraints during training and to perform the ﬁnal predictions.
We report results in Table 1a). As there is no single established multi-label error measure  and be-
cause it illustrates the effect of training with different loss function  we report several common mea-
sures. The results show nicely how the assumptions made during training inﬂuence the prediction
characteristics. Qualitatively  MLSP achieves best prediction accuracy in the max loss  P-SSVM is
better if we judge by the sum loss. This exactly reﬂects the loss functions they are trained with. In-
dependent training achieves very good results with respect to both measures  justifying its common
use for multi-label prediction with small label sets and many training examples per label2 Ordinary
SSVM training does not achieve good max- or sum-loss scores  but it performs well if quality is
measured by the average of the area under the precision-recall curves across labels for each indi-
vidual test example. This is also plausible  as SSVM training uses a ranking-like loss: all potential
labels for each input are enforced to be in the right order (correct labels have higher score than in-
correct ones)  but nothing in the objective encourages a cut-off point at 0. As a consequence  too
few or too many labels are predicted by Equation (2). In Table 1a) it appears to be too many  visible
as high recall but low precision. JKSE does not achieve competitive results in max loss  mAUC loss
or F1-score. Potentially this is because we use it with a linear kernel to stay comparable with the
other methods  whereas [27] reported good results mainly for nonlinear kernels.
Qualitatively  MLSP and P-SSVM show comparable prediction quality. We take this as an indication
that both  training with sum loss and training with max loss  make sense conceptually. However  of

2For ∆sum this is not surprising: independent training is known to be the optimal setup  if enough data is
available [32]. For ∆sum  the multi-class reformulation would be the optimal setup. The problem in multi-label
structured prediction is solely that |Y| is too large  and training data too scarce  to use either of these setups.

7

Figure 1: Multi-label structured prediction results. ∆max/∆sum: max/sum loss (lower is better) 
mAUC: mean area under per-sample precision-recall curve  prec/rec/F1: precision  recall  F1-score
(higher is better). Methods printed in italics are infeasible for general structured output sets.

∆max ∆sum mAUC F1 ( prec / rec )
0.42 ( 0.40 / 0.46 )
0.73
MLSP
JKSE
0.23 ( 0.14 / 0.76 )
1.00
0.37 ( 0.24 / 0.88 )
0.88
SSVM
P-SSVM 0.75
0.44 ( 0.48 / 0.41 )
indep.
0.73
0.46 ( 0.61 / 0.38 )

1.59
1.91
3.86
1.11
1.07

0.82
0.54
0.84
0.83
0.84

(a) Hierarchical classiﬁcation results.

MLSP
JKSE
SSVM
P-SSVM
indep.

∆max ∆sum
1.31
0.66
7.29
0.99
0.93
3.71

F1 ( prec / rec )
0.46 ( 0.60 / 0.52 )
0.09 ( 0.60 / 0.16 )
0.21 ( 0.79 / 0.33 )

infeasible
infeasible

(b) Object detection results.

the ﬁve methods  only MLSP  JKSE and SSVM generalize to more general structured prediction
setting  as they do not require exhaustive enumeration of the label set. Amongst these  MLSP is
preferable  except if one is only interested in ranking the labels  for which SSVM also works well.

5.2 Object class detection in natural images
Object detection can be solved as a structured prediction problem where natural images are the inputs
and coordinate tuples of bounding boxes are the outputs. The label set is of quadratic size in the num-
ber of image pixels and thus cannot be searched exhaustively. However  efﬁcient (loss-augmented)
argmax-prediction can be performed by branch-and-bound search [33]. Object detection is also in-
herently a multi-label task  because natural images contain different numbers of objects. We perform
experiments on the public UIUC-Cars dataset [34]. Following the experimental setup of [27] we use
the multiscale part of the dataset for training and the singlescale part for testing. The additional set
of pre-cropped car and background images serves as validation set for model selection. We use the

localization kernel  k(cid:0)(x  y)  (¯x  ¯y)(cid:1) = φ(x|y)tφ(¯x|¯y) where φ(x|y) is a 1000-dimensional bag of

visual words representation of the region y within the image x [13]. As misclassiﬁcation cost we
use λ(Y  y) := 1 for y ∈ Y   and λ(Y  y) := min¯y∈Y A(¯y  y) otherwise  where A(¯y  y) := 0 if
area(¯y∩y)
area(¯y∪y) ≥ 0.5  and A(¯y  y) := 1 otherwise. This is a common measure in object detection  which
reﬂects the intuition that all objects in an image should be identiﬁed  and that an object’s position is
acceptable if it overlaps sufﬁciently with at least one ground truth object. P-SSVM and independent
training are not applicable in this setup  so we compare MLSP against JKSE and SSVM. For each
method we train models on the training set and choose the C or ν value that maximizes the F1 score
over the validation set of precropped object and background images. Prediction is performed using
branch and bound optimization with greedy non-maximum suppression [35]. Table 1b) summarizes
the results on the test set (we do not report the mAUC measure  as computing this would require
summing over the complete output set). One sees that MLSP achieves the best results amongst the
three method. SSVM as well as JKSE suffer particularly from low recall  and their predictions also
have higher sum loss as well as max loss.

6 Summary and Discussion
We have studied multi-label classiﬁcation for structured output sets. Existing multi-label techniques
cannot directly be applied to this task because of the large size of the output set  and our analysis
showed that formulating multi-label structured prediction set a set-valued structured support vec-
tor machine framework also leads to infeasible training problems. Instead  we proposed an new
maximum-margin formulation  MLSP  that remains computationally tractable by use of the max
loss instead of sum loss between sets  and shows several of the advantageous properties known
from other maximum-margin based techniques  in particular a convex training problem and PAC-
Bayesian generalization bounds. Our experiments showed that MLSP has higher prediction accuracy
than baseline methods that remain applied in structured output settings. For small label sets  where
both concepts are applicable  MLSP performs comparable to the set-valued SSVM formulation.
Besides these promising initial results  we believe that there are still several aspects of multi-label
structured prediction that need to be better understood  in particular the prediction problem at test
time. Collecting all elements of positive score is a natural criterion  but it is costly to perform
exactly if the output set is very large. Therefore  it would be desirable to develop sparsity enforcing
variations of Equation (2)  for example by adopting ideas from compressed sensing [25].

8

References
[1] J. D. Lafferty  A. McCallum  and F. C. N. Pereira. Conditional random ﬁelds: Probabilistic models for

segmenting and labeling sequence data. In ICML  2001.

[2] B. Taskar  C. Guestrin  and D. Koller. Max-margin Markov networks. In NIPS  2003.
[3] I. Tsochantaridis  T. Joachims  T. Hofmann  and Y. Altun. Large margin methods for structured and

interdependent output variables. JMLR  6  2006.

[4] T. Joachims  T. Finley  and C. N. J. Yu. Cutting-plane training of structural SVMs. Machine Learning 

[5] C. H. Teo  SVN Vishwanathan  A. Smola  and Q. V. Le. Bundle methods for regularized risk minimiza-

[6] G. Tsoumakas and I. Katakis. Multi-label classiﬁcation: An overview. International Journal of Data

[7] K. Dembczynski  W. Cheng  and E. H¨ullermeier. Bayes optimal multilabel classiﬁcation via probabilistic

77(1)  2009.

tion. JMLR  11  2010.

Warehousing and Mining  3(3)  2007.

classiﬁer chains. In ICML  2011.

[8] D. McAllester. Generalization bounds and consistency for structured labeling. In G. Bakır  T. Hofmann 

B. Sch¨olkopf  A.J. Smola  and B. Taskar  editors  Predicting Structured Data. MIT Press  2007.

[9] D. Nilsson. An efﬁcient algorithm for ﬁnding the M most probable conﬁgurations in probabilistic expert

systems. Statistics and Computing  8(2)  1998.

[10] C. Yanover and Y. Weiss. Finding the M most probable conﬁgurations using loopy belief propagation. In

[11] M. Fromer and A. Globerson. An LP View of the M-best MAP problem. In NIPS  2009.
[12] J. Porway and S.-C. Zhu. C 4: Exploring multiple solutions in graphical models by cluster sampling.

[13] M. B. Blaschko and C. H. Lampert. Learning to localize objects with structured output regression. In

[14] A. Bordes  N. Usunier  and L. Bottou. Sequence labelling SVMs trained in one pass. ECML PKDD  2008.
[15] M. R. Boutell  J. Luo  X. Shen  and C.M. Brown. Learning multi-label scene classiﬁcation. Pattern

NIPS  2004.

PAMI  33(9)  2011.

ECCV  2008.

Recognition  37(9)  2004.

ECML  1998.

Learning  39(2–3)  2000.

sion. In ACM SIGIR  2007.

Learning  76(2):227–242  2009.

SIGKDD  2008.

[16] T. Joachims. Text categorization with support vector machines: Learning with many relevant features. In

[17] R. E. Schapire and Y. Singer. Boostexter: A boosting-based system for text categorization. Machine

[18] Y. Yue  T. Finley  F. Radlinski  and T. Joachims. A support vector method for optimizing average preci-

[19] T. G¨artner and S. Vembu. On structured output training: Hard cases and an efﬁcient alternative. Machine

[20] Y. Yue and T. Joachims. Predicting diverse subsets using structural SVMs. In ICML  2008.
[21] S. Ji  L. Tang  S. Yu  and J. Ye. Extracting shared subspaces for multi-label classiﬁcation.

In ACM

[22] P. Rai and H. Daum´e III. Multi-label prediction via sparse inﬁnite CCA. In NIPS  2009.
[23] B. Hariharan  L. Zelnik-Manor  S. V. N. Vishwanathan  and M. Varma. Large scale max-margin multi-

label classiﬁcation with priors. In ICML  2010.

[24] W. Bi and J. Kwok. Multi-label classiﬁcation on tree- and DAG-structured hierarchies. In ICML  2011.
[25] D. Hsu  S. Kakade  J. Langford  and T. Zhang. Multi-label prediction via compressed sensing. In NIPS 

2009.

2011.

2004.

[26] G. Tsoumakas  I. Katakis  and I. Vlahavas. Effective and efﬁcient multilabel classiﬁcation in domains

with large number of labels. In ECML PKDD  2008.

[27] C. H. Lampert and M. B. Blaschko. Structured prediction by joint kernel support estimation. Machine

[28] J. Petterson  T. S. Caetano  J. J. McAuley  and J. Yu. Exponential family graph matching and ranking. In

Learning  77(2–3)  2009.

NIPS  2009.

[29] J. Rousu  C. Saunders  S. Szedmak  and J. Shawe-Taylor. Kernel-based learning of hierarchical multilabel

classiﬁcation models. JMLR  7  2006.

[30] A. Binder  K.-R. M¨uller  and M. Kawanabe. On taxonomies for multi-class image categorization. IJCV 

[31] L. Cai and T. Hofmann. Hierarchical document categorization with support vector machines. In ICKM 

[32] K. Dembczynski  W. Cheng  and E. H¨ullermeier. Bayes optimal multilabel classiﬁcation via probabilistic

classiﬁer chains. In ICML  2010.

[33] C. H. Lampert  M. B. Blaschko  and T. Hofmann. Efﬁcient subwindow search: A branch and bound

framework for object localization. PAMI  31(12)  2009.

[34] S. Agarwal  A. Awan  and D. Roth. Learning to detect objects in images via a sparse  part-based repre-

sentation. PAMI  26(11)  2004.

[35] C. H. Lampert. An efﬁcient divide-and-conquer cascade for nonlinear object detection. In CVPR  2010.

9

,Jamie Shotton
Toby Sharp
Pushmeet Kohli
Sebastian Nowozin
John Winn
Antonio Criminisi
Elias Bareinboim
Judea Pearl
Tong Yang
Xiangyu Zhang
Zeming Li
Wenqiang Zhang
Jian Sun