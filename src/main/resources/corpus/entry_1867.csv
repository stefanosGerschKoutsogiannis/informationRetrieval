2016,Estimating the class prior and posterior from noisy positives and unlabeled data,We develop a classification algorithm for estimating posterior distributions from positive-unlabeled data  that is robust to noise in the positive labels and effective for high-dimensional data. In recent years  several algorithms have been proposed to learn from positive-unlabeled data; however  many of these contributions remain theoretical  performing poorly on real high-dimensional data that is typically contaminated with noise. We build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers. We prove that these univariate transforms preserve the class prior  enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data. The theoretical development and parametric and nonparametric algorithms proposed here constitute an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data.,Estimating the class prior and posterior from noisy

positives and unlabeled data

Shantanu Jain  Martha White  Predrag Radivojac

Department of Computer Science

Indiana University  Bloomington  Indiana  USA

{shajain  martha  predrag}@indiana.edu

Abstract

We develop a classiﬁcation algorithm for estimating posterior distributions from
positive-unlabeled data  that is robust to noise in the positive labels and effective
for high-dimensional data. In recent years  several algorithms have been proposed
to learn from positive-unlabeled data; however  many of these contributions re-
main theoretical  performing poorly on real high-dimensional data that is typically
contaminated with noise. We build on this previous work to develop two practical
classiﬁcation algorithms that explicitly model the noise in the positive labels and
utilize univariate transforms built on discriminative classiﬁers. We prove that these
univariate transforms preserve the class prior  enabling estimation in the univari-
ate space and avoiding kernel density estimation for high-dimensional data. The
theoretical development and parametric and nonparametric algorithms proposed
here constitute an important step towards wide-spread use of robust classiﬁcation
algorithms for positive-unlabeled data.

1

Introduction

Access to positive  negative and unlabeled examples is a standard assumption for most semi-
supervised binary classiﬁcation techniques. In many domains  however  a sample from one of the
classes (say  negatives) may not be available  leading to the setting of learning from positive and
unlabeled data (Denis et al.  2005). Positive-unlabeled learning often emerges in sciences and com-
merce where an observation of a positive example (say  that a protein catalyzes reactions or that a
social network user likes a particular product) is usually reliable. Here  however  the absence of a
positive observation cannot be interpreted as a negative example. In molecular biology  for example 
an attempt to label a data point as positive (say  that a protein is an enzyme) may be unsuccessful for
a variety of experimental and biological reasons  whereas in social networks an explicit dislike of a
product may not be possible. Both scenarios lead to a situation where negative examples cannot be
actively collected.
Fortunately  the absence of negatively labeled examples can be tackled by incorporating unlabeled
examples as negatives  leading to the development of non-traditional classiﬁers. Here we follow the
terminology by Elkan and Noto (2008) that a traditional classiﬁer predicts whether an example is
positive or negative  whereas a non-traditional classiﬁer predicts whether the example is positive or
unlabeled. Positive vs. unlabeled (non-traditional) training is reasonable because the class posterior
— and also the optimum scoring function for composite losses (Reid and Williamson  2010) — in the
traditional setting is monotonically related to the posterior in the non-traditional setting. However 
the true posterior can be fully recovered from the non-traditional posterior only if we know the class
prior; i.e.  the proportion of positives in unlabeled data. The knowledge of the class prior is also
necessary for estimation of the performance criteria such as the error rate  balanced error rate or
F-measure  and also for ﬁnding the right threshold for the non-traditional scoring function that leads
to an optimal classiﬁer with respect to some criteria (Menon et al.  2015).

Class prior estimation in a nonparametric setting has been actively researched in the past decade
offering an extensive theory of identiﬁability (Ward et al.  2009; Blanchard et al.  2010; Scott et al. 
2013; Jain et al.  2016) and a few practical solutions (Elkan and Noto  2008; Ward et al.  2009;
du Plessis and Sugiyama  2014; Sanderson and Scott  2014; Jain et al.  2016; Ramaswamy et al. 
2016). Application of these algorithms to real data  however  is limited in that none of the pro-
posed algorithms simultaneously deals with noise in the labels and practical estimation for high-
dimensional data.
Much of the theory on learning class priors relies on the assumption that either the distribution of
positives is known or that the positive sample is clean. In practice  however  labeled data sets con-
tain class-label noise  where an unspeciﬁed amount of negative examples contaminates the positive
sample. This is a realistic scenario in experimental sciences where technological advances enabled
generation of high-throughput data at a cost of occasional errors. One example for this comes from
the studies of proteins using analytical chemistry technology; i.e.  mass spectrometry. For example 
in the process of peptide identiﬁcation (Steen and Mann  2004)  bioinformatics methods are usually
set to report results with speciﬁed false discovery rate thresholds (e.g.  1%). Unfortunately  statis-
tical assumptions in these experiments are sometimes violated thereby leading to substantial noise
in reported results  as in the case of identifying protein post-translational modiﬁcations. Similar
amounts of noise might appear in social networks such as Facebook  where some users select ‘like’ 
even when they do not actually like a particular post. Further  the only approach that does consider
similar such noise (Scott et al.  2013) requires density estimation  which is known to be problematic
for high-dimensional data.
In this work  we propose the ﬁrst classiﬁcation algorithm  with class prior estimation  designed
particularly for high-dimensional data with noise in the labeling of positive data. We ﬁrst formalize
the problem of class prior estimation from noisy positive and unlabeled data. We extend the existing
identiﬁability theory for class prior estimation from positive-unlabeled data to this noise setting.
We then show that we can practically estimate class priors and the posterior distributions by ﬁrst
transforming the input space to a univariate space  where density estimation is reliable. We prove
that these transformations preserve class priors and show that they correspond to training a non-
traditional classiﬁer. We derive a parametric algorithm and a nonparametric algorithm to learn the
class priors. Finally  we carry out experiments on synthetic and real-life data and provide evidence
that the new approaches are sound and effective.

2 Problem formulation

Consider a binary classiﬁcation problem of mapping an input space X to an output space Y = {0  1}.
Let f be the true distribution of inputs. It can be represented as the following mixture

f (x) = ↵f1(x) + (1− ↵)f0(x) 

(1)
where x 2 X   y 2 Y  fy are distributions over X for the positive (y = 1) and negative (y = 0)
class  respectively; and ↵ 2 [0  1) is the class prior or the proportion of the positive examples in f.
We will refer to a sample from f as unlabeled data.
Let g be the distribution of inputs for the labeled data. Because the labeled sample contains some
mislabeled examples  the corresponding distribution is also a mixture of f1 and a small proportion 
say 1 − β  of f0. That is 

g(x) = βf1(x) + (1 − β)f0(x) 

(2)
where β 2 (0  1]. Observe that both mixtures have the same components but different mixing
proportions. The simplest scenario is that the mixing components f0 and f1 correspond to the class-
conditional distributions p(x|Y = 0) and p(x|Y = 1)  respectively. However  our approach also
permits transformations of the input space X   thus resulting in a more general setup.
The objective of this work is to study the estimation of the class prior ↵ = p(Y = 1) and propose
practical algorithms for estimating ↵. The efﬁcacy of this estimation is clearly tied to β  where as β
gets smaller  the noise in the positive labels becomes larger. We will discuss identiﬁability of ↵ and
β and give a practical algorithm for estimating ↵ (and β). We will then use these results to estimate
the posterior distribution of the class variable  p(y|x)  despite the fact that the labeled set does not
contain any negative examples.

2

3

Identiﬁability

The class prior is identiﬁable if there is a unique class prior for a given pair (f  g). Much of the
identiﬁability characterization in this section has already been considered as the case of asymmetric
noise (Scott et al.  2013); see Section 7 on related work. We recreate these results here  with the aim
to introduce required notation  to highlight several important results for later algorithm development
and to include a few missing results needed for our approach. Though the proof techniques are
themselves quite different and could be of interest  we include them in the appendix due to space.
There are typically two aspects to address with identiﬁability. First  one needs to determine if a
problem is identiﬁable  and  second  if it is not  propose a canonical form that is identiﬁable. In
this section we will see that class prior is not identiﬁable in general because f0 can be a mixture
containing f1 and vice versa. To ensure identiﬁability  it is necessary to choose a canonical form
that prefers a class prior that makes the two components as different as possible; this canonical form
was introduced as the mutual irreducibility condition (Scott et al.  2013) and is related to the proper
novelty distribution (Blanchard et al.  2010) and the max-canonical form (Jain et al.  2016).
We discuss identiﬁability in terms of measures. Let µ  ⌫  µ0 and µ1 be probability measures deﬁned
on some σ-algebra A on X   corresponding to f  g  f0 and f1  respectively. It follows that

µ = ↵µ1 + (1 − ↵)µ0
⌫ = βµ1 + (1 − β)µ0.

(3)
(4)

Consider a family of pairs of mixtures having the same components
F(⇧) = {(µ  ⌫) : µ = ↵µ1 + (1 − ↵)µ0  ⌫ = βµ1 + (1 − β)µ0  (µ0  µ1) 2 ⇧  0  ↵ < β  1} 
where ⇧ is some set of pairs of probability measures deﬁned on A. The family is parametrized
by the quadruple (↵  β  µ0  µ1). The condition β > ↵ means that ⌫ has a greater proportion of
µ1 compared to µ. This is consistent with our assumption that the labeled sample mainly contains
positives. The most general choice for ⇧ is

⇧all = Pall ⇥ Pall \�(µ  µ) : µ 2 Pall  

where Pall is the set of all probability measures deﬁned on A and�(µ  µ) : µ 2 Pall is the set of

pairs with equal distributions. Removing equal pairs prevents µ and ⌫ from being identical.
We now deﬁne the maximum proportion of a component λ1 in a mixture λ  which is used in the
results below and to specify the criterion that enables identiﬁability; more speciﬁcally 

aλ1

λ = max�↵ 2 [0  1] : λ = ↵λ1 + (1 − ↵)λ0  λ0 2 Pall .

Of particular interest is the case when aλ1
λ = 0  which should be read as “λ is not a mixture contain-
ing λ1”. We ﬁnally deﬁne the set all possible (↵  β) that generate µ and ⌫ when (µ0  µ1) varies in
⇧:
A+(µ  ⌫  ⇧) = {(↵  β) : µ = ↵µ1 + (1 − ↵)µ0  ⌫ = βµ1 + (1 − β)µ0  (µ0  µ1) 2 ⇧  0  ↵ < β  1}.
If A+(µ  ⌫  ⇧) is a singleton set for all (µ  ⌫) 2 F(⇧)  then F(⇧) is identiﬁable in (↵  β).
First  we show that the most general choice for ⇧  ⇧all  leads to unidentiﬁability (Lemma 1). Fortu-
nately  however  by choosing a restricted set

(5)

⇧res =�(µ0  µ1) 2 ⇧all : aµ1

µ0 = 0  aµ0

µ1 = 0 

as ⇧  we do obtain identiﬁability (Theorem 1). In words  ⇧res contains pairs of distributions  where
each distribution in a pair cannot be expressed as a mixture containing the other. The proofs of the
results below are in the Appendix.
Lemma 1 (Unidentiﬁability) Given a pair of mixtures (µ  ⌫) 2 F(⇧all) 
(↵  β  µ0  µ1) generate (µ  ⌫) and ↵+ = a⌫

let parameters

µ  β+ = aµ

⌫ . It follows that

1. There is a one-to-one relation between (µ0  µ1) and (↵  β) and

µ0 =

βµ − ↵⌫
β − ↵

 

µ1 =

(1 − ↵)⌫ − (1 − β)µ

β − ↵

.

(6)

3

2. Both expressions on the right-hand side of Equation 6 are well deﬁned probability measures

if and only if ↵/β  ↵+ and (1−β)/(1−↵)  β+.

3. A+(µ  ⌫  ⇧all) = {(↵  β) : ↵/β  ↵+  (1−β)/(1−↵)  β+}.
4. F(⇧all) is unidentiﬁable in (↵  β); i.e.  (↵  β) is not uniquely determined from (µ  ⌫).
5. F(⇧all) is unidentiﬁable in ↵ and β  individually; i.e.  neither ↵ nor β is uniquely deter-

mined from (µ  ⌫).

Observe that the deﬁnition of aλ1
A+(µ  ⌫  ⇧all) satisﬁes ↵ < β  as expected.
Theorem 1 (Identiﬁablity) Given (µ  ⌫) 2 F(⇧all)  let ↵+ = a⌫
(µ−↵+⌫)/(1−↵+)  µ⇤1 = (⌫−β+µ)/(1−β+) and

λ and µ 6= ⌫ imply ↵+ < 1 and  consequently  any (↵  β) 2
⌫ . Let µ⇤0 =

µ and β+ = aµ

It follows that

↵⇤ = ↵+(1−β+)/(1−↵+β+) 

β⇤ = (1−β+)/(1−↵+β+).

(7)

1. (↵⇤  β⇤  µ⇤0  µ⇤1) generate (µ  ⌫)
2. (µ⇤0  µ⇤1) 2 ⇧res and  consequently  ↵⇤ = aµ⇤1
3. F(⇧res) contains all pairs of mixtures in F(⇧all).
4. A+(µ  ⌫  ⇧res) = {(↵⇤  β⇤)}.
5. F(⇧res) is identiﬁable in (↵  β); i.e.  (↵  β) is uniquely determined from (µ  ⌫).

µ   β⇤ = aµ⇤1
⌫ .

We refer to the expressions of µ and ⌫ as mixtures of components µ0 and µ1 as a max-canonical form
when (µ0  µ1) is picked from ⇧res. This form enforces that µ1 is not a mixture containing µ0 and vice
versa  which leads to µ0 and µ1 having maximum separation  while still generating µ and ⌫. Each
pair of distributions in F(⇧res) is represented in this form. Identiﬁability of F(⇧res) in (↵  β) occurs
precisely when A+(µ  ⌫  ⇧res) = {(↵⇤  β⇤)}  i.e.  (↵⇤  β⇤) is the only pair of mixing proportions
that can appear in a max-canonical form of µ and ⌫. Moreover  Statement 1 in Theorem 1 and
Statement 1 in Lemma 1 imply that the max-canonical form is unique and completely speciﬁed
by (↵⇤  β⇤  µ⇤0  µ⇤1)  with ↵⇤ < β⇤ following from Equation 7. Thus  using F(⇧res) to model the
unlabeled and labeled data distributions makes estimation of not only ↵  the class prior  but also
β  µ0  µ1 a well-posed problem. Moreover  due to Statement 3 in Theorem 1  there is no loss in the
modeling capability by using F(⇧res) instead of F(⇧all). Overall  identiﬁability  absence of loss of
modeling capability and maximum separation between µ0 and µ1 combine to justify estimating ↵⇤
as the class prior.

4 Univariate Transformation

The theory and algorithms for class prior estimation are agnostic to the dimensionality of the data;
in practice  however  this dimensionality can have important consequences. Parametric Gaussian
mixture models trained via expectation-maximization (EM) are known to strongly suffer from co-
linearity in high-dimensional data. Nonparametric (kernel) density estimation is also known to have
curse-of-dimensionality issues  both in theory (Liu et al.  2007) and in practice (Scott  2008).
We address the curse of dimensionality by transforming the data to a single dimension. The trans-
formation ⌧ : X ! R  surprisingly  is simply an output of a non-traditional classiﬁer trained to
separate labeled sample  L  from unlabeled sample  U. The transform is similar to that in (Jain
et al.  2016)  except that it is not required to be calibrated like a posterior distribution; as shown
below  a good ranking function is sufﬁcient. First  however  we introduce notation and formalize the
data generation steps (Figure 1).
Let X be a random variable taking values in X   capturing the true distribution of inputs  µ  and Y
be an unobserved random variable taking values in Y  giving the true class of the inputs. It follows
that X|Y = 0 and X|Y = 1 are distributed according to µ0 and µ1  respectively. Let S be a
selection random variable  whose value in S = {0  1  2} determines the sample to which an input x
is added (Figure 1). When S = 1  x is added to the noisy labeled sample; when S = 0  x is added
to the unlabeled sample; and when S = 2  x is not added to either of the samples. It follows that

4

Input

Select for
labeling

yes

Success of
labeling

yes

Y = 0 w.p. 0
Y = 1 w.p. 1

Noisy positive

S = 1

no

Unlabeled

S = 0

no

Y = 0 w.p. 1 0
Y = 1 w.p. 1 1

Dropped
S = 2

Figure 1: The labeling procedure  with S taking values from S = {0  1  2}. In the ﬁrst step  the sample is
randomly selected to attempt labeling  with some probability independent of X or Y . If it is not selected  it is
added to the “Unlabeled” set. If it is selected  then labeling is attempted. If the true label is Y = 1  then with
probability γ1 2 (0  1)  the labeling will succeed and it is added to “Noisy positives”. Otherwise  it is added
to the “Dropped” set. If the true label is Y = 0  then the attempted labeling is much more likely to fail  but
because of noise  could succeed. The attempted label of Y = 0 succeeds with probability γ0  and is added to
“Noisy positives”  even though it is actually a negative instance. γ0 = 0 leads to the no noise case and the noise
increases as γ0 increases. β = γ1↵/(γ1↵+γ0(1↵))  gives the proportion of positives in the “Noisy positives”.
X u = X|S = 0 and X l = X|S = 1 are distributed according to µ and ⌫  respectively. We make
the following assumptions which are consistent with the statements above:
(8)
(9)
(10)
Assumptions 8 and 9 states that the proportion of positives in the unlabeled sample and the labeled
sample matches the true proportion in µ and ⌫  respectively. Assumption 10 states that the distribu-
tion of the positive inputs (and the negative inputs) in both the unlabeled and the labeled samples is
equal and unbiased. Lemma 2 gives the implications of these assumptions. Statement 3 in Lemma 2
is particularly interesting and perhaps counter-intuitive as it states that with non-zero probability
some inputs need to be dropped.
Lemma 2 Let X  Y and S be random variables taking values in X   Y and S  respectively  and
X u = X|S = 0 and X l = X|S = 1. For measures µ  ⌫  µ0  µ1  satisfying Equations 3 and 4 and
µ1 6= µ0  let µ  µ0  µ1 give the distribution of X  X|Y = 0 and X|Y = 1  respectively. If X  Y and
S satisfy assumptions 8  9 and 10  then

p(y|S = 0) = p(y) 
p(y = 1|S = 1) = β 
p(x|s  y) = p(x|y).

1. X is independent of S = 0; i.e.  p(x|S = 0) = p(x)
2. X u and X l are distributed according to µ and ⌫  respectively.
3. p(S = 2) 6= 0.

The proof is in the Appendix. Next  we highlight the conditions under which the score function ⌧
preserves ↵⇤. Observing that S serves as the pseudo class label for labeled vs. unlabeled classiﬁca-
tion as well  we ﬁrst give an expression for the posterior:

⌧p(x) = p(S = 1|x  S 2 {0  1})  8x 2 X .

(11)

Theorem 2 (↵⇤-preserving transform) Let random variables X  Y  S  X u  X l and measures
µ  ⌫  µ0  µ1 be as deﬁned in Lemma 2. Let ⌧p be the posterior as deﬁned in Equation 11 and
⌧ = H ◦ ⌧p  where H is a 1-to-1 function on [0  1] and ◦ is the composition operator. Assume

1. (µ0  µ1) 2 ⇧res 
2. X u and X l are continuous with densities f and g  respectively 
3. µ⌧   ⌫⌧   µ⌧ 1 are the measures corresponding to ⌧ (X u)  ⌧ (X l)  ⌧ (X1)  respectively 
4. (↵+  β+  ↵⇤  β⇤) = (a⌫

⌧   ↵⇤⌧   β⇤⌧ ) = (a⌫⌧

⌫ ) and (↵+

µ   aµ1

⌧   β+

⌫   aµ1

µ  aµ

µ⌧   aµ⌧

⌫⌧   aµ⌧ 1

µ⌧   aµ⌧ 1

⌫⌧ ).

Then

(↵+

⌧   β+

⌧   ↵⇤⌧   β⇤⌧ ) = (↵+  β+  ↵⇤  β⇤)

and so ⌧ is an ↵⇤-preserving transformation.
Moreover  ⌧p can also be used to compute the true posterior probability:
1 − ⌧p(x) −

β⇤ − ↵⇤ ✓ p(S = 0)

p(Y = 1|x) =

↵⇤(1 − ↵⇤)

p(S = 1)

⌧p(x)

1 − β⇤

1 − ↵⇤◆.

(12)

5

The proof is in the Appendix. Theorem 2 shows that the ↵⇤ is the same for the original data and
the transformed data  if the transformation function ⌧ can be expressed as a composition of ⌧p and
a one-to-one function  H  deﬁned on [0  1]. Trivially  ⌧p itself is one such function. We emphasize 
however  that ↵⇤-preservation is not limited by the efﬁcacy of the calibration algorithm; uncalibrated
scoring that ranks inputs as ⌧p(x) also preserves ↵⇤. Theorem 2 further demonstrates how the true
posterior  p(Y = 1|x)  can be recovered from ⌧p by plugging in estimates of ⌧p  p(S=0)/p(S=1) 
↵⇤ and β⇤ in Equation 12. The posterior probability ⌧p can be estimated directly by using a prob-
abilistic classiﬁer or by calibrating a classiﬁer’s score (Platt  1999; Niculescu-Mizil and Caruana 
2005); |U|/|L| serves as an estimate of p(S=0)/p(S=1); section 5 gives parametric and nonparametric
approaches for estimation of ↵⇤ and β⇤.

5 Algorithms

In this section  we derive a parametric and a nonparametric algorithm to estimate ↵⇤ and β⇤ from the
i}. In theory  both approaches
unlabeled sample  U = {X u
can handle multivariate samples; in practice  however  to circumvent the curse of dimensionality  we
exploit the theory of ↵⇤-preserving univariate transforms to transform the samples.
Parametric approach. The parametric approach is derived by modeling each sample as a two
component Gaussian mixture  sharing the same components but having different mixing proportions:

i }  and the noisy positive sample  L = {X l

X u
i ⇠ ↵N (u1  ⌃1) + (1 − ↵)N (u0  ⌃0)
X l
i ⇠ βN (u1  ⌃1) + (1 − β)N (u0  ⌃0)

where u1  u0 2 Rd and ⌃1  ⌃0 2 Sd
++  the set of all d⇥d positive deﬁnite matrices. The algorithm is
an extension to the EM approach for Gaussian mixture models (GMMs) where  instead of estimating
the parameters of a single mixture  the parameters of both mixtures (↵  β  u0  u1  ⌃0  ⌃1) are esti-
mated simultaneously by maximizing the combined likelihood over both U and L. This approach 
which we refer to as a multi-sample GMM (MSGMM)  exploits the constraint that the two mixtures
share the same components. The update rules and their derivation are given in the Appendix.
Nonparametric approach. Our nonparametric strategy directly exploits the results of Lemma 1 and
Theorem 1  which give a direct connection between (↵+ = a⌫
⌫ ) and (↵⇤  β⇤). Therefore 
for a two-component mixture sample  M  and a sample from one of the components  C  it only
requires an algorithm to estimate the maximum proportion of C in M. For this purpose  we use
the AlphaMax algorithm (Jain et al.  2016)  brieﬂy summarized in the Appendix. Speciﬁcally  our
two-step approach for estimating ↵⇤ and β⇤ is as follows: (i) Estimate ↵+ and β+ as outputs of
AlphaMax(U  L) and AlphaMax(L  U )  respectively; (ii) Estimate (↵⇤  β⇤) from the estimates of
(↵+  β+) by applying Equation 7. We refer to our nonparametric algorithm as AlphaMax-N.

µ  β+ = aµ

6 Empirical investigation

In this section we systematically evaluate the new algorithms in a controlled  synthetic setting as
well as on a variety of data sets from the UCI Machine Learning Repository (Lichman  2013).
Experiments on synthetic data: We start by evaluating all algorithms in a univariate setting where
both mixing proportions  ↵ and β  are known. We generate unit-variance Gaussian and unit-scale
Laplace-distributed i.i.d. samples and explore the impact of mixing proportions  the size of the
component sample  and the separation and overlap between the mixing components on the accuracy
of estimation. The class prior ↵ was varied from {0.05  0.25  0.50} and the noise component β from
{1.00  0.95  0.75}. The size of the labeled sample L was varied from {100  1000}  whereas the size
of the unlabeled sample U was ﬁxed at 10000.
Experiments on real-life data: We considered twelve real-life data sets from the UCI Machine
Learning Repository. To adjust these data to our problems  categorical features were transformed
into numerical using sparse binary representation  the regression data sets were transformed into
classiﬁcation based on mean of the target variable  and the multi-class classiﬁcation problems were
converted into binary problems by combining classes. In each data set  a subset of positive and
negative examples was randomly selected to provide a labeled sample while the remaining data
(without class labels) were used as unlabeled data. The size of the labeled sample was kept at 1000
(or 100 for small data sets) and the maximum size of unlabeled data was set 10000.

6

Algorithms: We compare the AlphaMax-N and MSGMM algorithms to the Elkan-Noto algorithm
(Elkan and Noto  2008) as well as the noiseless version of AlphaMax (Jain et al.  2016). There
are several versions of the Elkan-Noto estimator and each can use any underlying classiﬁer. We
used the e1 alternative estimator combined with the ensembles of 100 two-layer feed-forward neural
networks  each with ﬁve hidden units. The out-of-bag scores of the same classiﬁer were used as
a class-prior preserving transformation that created an input to the AlphaMax algorithms.
It is
important to mention that neither Elkan-Noto nor AlphaMax algorithm was developed to handle
noisy labeled data. In addition  the theory behind the Elkan-Noto estimator restricts its use to class-
conditional distributions with non-overlapping supports. The algorithm by du Plessis and Sugiyama
(2014) minimizes the same objective as the e1 Elkan-Noto estimator and  thus  was not implemented.
Evaluation: All experiments were repeated 50 times to be able to draw conclusions with statistical
signiﬁcance. In real-life data  the labeled sample was created randomly by choosing an appropriate
number of positive and negative examples to satisfy the condition for β and the size of the labeled
sample  while the remaining data was used as the unlabeled sample. Therefore  the class prior in
the unlabeled data varies with the selection of the noise parameter β. The mean absolute difference
between the true and estimated class priors was used as a performance measure. The best performing
algorithm on each data set was determined by multiple hypothesis testing using the P-value of 0.05
and Bonferroni correction.
Results: The comprehensive results for synthetic data drawn from univariate Gaussian and Laplace
distributions are shown in Appendix (Table 2). In these experiments no transformation was applied
prior to running any of the algorithms. As expected  the results show excellent performance of the
MSGMM model on the Gaussian data. These results signiﬁcantly degrade on Laplace-distributed
data  suggesting sensitivity to the underlying assumptions. On the other hand  AlphaMax-N was
accurate over all data sets and also robust to noise. These results suggest that new parametric and
nonparametric algorithms perform well in these controlled settings.
Table 1 shows the results on twelve real data sets. Here  AlphaMax and AlphaMax-N algorithms
demonstrate signiﬁcant robustness to noise  although the parametric version MSGMM was compet-
itive in some cases. On the other hand  the Elkan-Noto algorithm expectedly degrades with noise.
Finally  we investigated the practical usefulness of the ↵⇤-preserving transform. Table 3 (Appendix)
shows the results of AlphaMax-N and MSGMM on the real data sets  with and without using the
transform. Because of computational and numerical issues  we reduced the dimensionality by us-
ing principal component analysis (the original data caused matrix singularity issues for MSGMM
and density estimation issues for AlphaMax-N). MSGMM deteriorates signiﬁcantly without the
transform  whereas AlphaMax-N preserves some signal for the class prior. AlphaMax-N with the
transform  however  shows superior performance on most data sets.

7 Related work

Class prior estimation in a semi-supervised setting  including positive-unlabeled learning  has been
extensively discussed previously; see Saerens et al. (2002); Cortes et al. (2008); Elkan and Noto
(2008); Blanchard et al. (2010); Scott et al. (2013); Jain et al. (2016) and references therein. Re-
cently  a general setting for label noise has also been introduced  called the mutual contamination
model. The aim under this model is to estimate multiple unknown base distributions  using multi-
ple random samples that are composed of different convex combinations of those base distributions
(Katz-Samuels and Scott  2016). The setting of asymmetric label noise is a subset of this more
general setting  treated under general conditions by Scott et al. (2013)  and previously investigated
under a more restrictive setting as co-training (Blum and Mitchell  1998). A natural approach is
to use robust estimation to learn in the presence of class noise; this strategy  however  has been
shown to be ineffective  both theoretically (Long and Servedio  2010; Manwani and Sastry  2013)
and empirically (Hawkins and McLachlan  1997; Bashir and Carter  2005)  indicating the need to
explicitly model the noise. Generative mixture model approaches have also been developed  which
explicitly model the noise (Lawrence and Scholkopf  2001; Bouveyron and Girard  2009); these al-
gorithms  however  assume labeled data for each class. As the most related work  though Scott et al.
(2013) did not explicitly treat the positive-unlabeled learning with noisy positives  their formulation
can incorporate this setting by using ⇡0 = ↵ and β = 1 − ⇡1. The theoretical and algorithmic
treatment  however  is very different. Their focus is on identiﬁability and analyzing convergence
rates and statistical properties  assuming access to some ⇤ function which can obtain proportions

7

Table 1: Mean absolute difference between estimated and true mixing proportion over twelve data sets from
the UCI Machine Learning Repository. Statistical signiﬁcance was evaluated by comparing the Elkan-Noto
algorithm  AlphaMax  AlphaMax-N  and the multi-sample GMM after applying a multivariate-to-univariate
transform (MSGMM-T). The bold font type indicates the winner and the asterisk indicates statistical signiﬁ-
cance. For each data set  shown are the true mixing proportion (↵)  true proportion of the positives in the labeled
sample (β)  sample dimensionality (d)  the number of positive examples (n1)  the total number of examples
(n)  and the area under the ROC curve (AUC) for a model trained between labeled and unlabeled data.
Data

Elkan-Noto AlphaMax AlphaMax-N MSGMM-T

n

AUC d
0.842 13
0.819 13
0.744 13
1030
0.685 8
1030
0.662 8
0.567 8
1030
0.825 127 2565 5574
0.795 127 2565 5574
0.672 127 2565 5574
506
0.810 13
209
506
0.777 13
209
0.651 13
506
209
1508 6435
0.933 36
1508 6435
0.904 36
0.788 36
1508 6435
0.792 126 3916 8124
0.766 126 3916 8124
0.648 126 3916 8124
5473
0.885 10
5473
0.858 10
0.768 10
5473
0.875 16
0.847 16
0.738 16
0.735 8
0.710 8
0.623 8
0.929 9
0.903 9
0.802 9
0.842 57
0.812 57
0.695 57
0.626 11
0.610 11
0.531 11

n1
5188 45000 0.241
5188 45000 0.284
5188 45000 0.443
0.329
490
0.363
490
0.531
490
0.017
0.078
0.396
0.159
0.226
0.501
0.074
0.110
0.302
0.029
0.087
0.370
0.116
560
0.137
560
560
0.256
3430 10992 0.030
3430 10992 0.071
3430 10992 0.281
0.351
268
0.408
268
268
0.586
8903 58000 0.024*
8903 58000 0.052
8903 58000 0.199
0.184
1813 4601
1813 4601
0.246
0.515
1813 4601
0.290
4113 6497
0.322
4113 6497
4113 6497
0.420

768
768
768

0.070
0.079
0.124
0.141
0.174
0.212
0.011
0.016
0.137
0.087
0.094
0.125
0.009
0.015
0.063
0.015*
0.015
0.140
0.026*
0.031*
0.041*
0.006*
0.011
0.093
0.120
0.118
0.144
0.027
0.004*
0.047
0.046
0.059
0.155
0.083
0.113
0.322

0.037*
0.036*
0.040*
0.181
0.231
0.272
0.017
0.006
0.009
0.094
0.110
0.134
0.007*
0.008*
0.012*
0.022
0.008*
0.020
0.044
0.052
0.064
0.009
0.005*
0.007*
0.111
0.110
0.156
0.029
0.007
0.004*
0.041
0.042*
0.044*
0.060
0.063
0.353

0.163
0.155
0.127
0.077*
0.095*
0.233
0.008*
0.006
0.006*
0.209
0.204
0.172
0.157
0.152
0.143
0.037
0.037
0.024
0.129
0.125
0.111
0.081
0.074
0.062
0.171
0.168
0.175
0.157
0.157
0.148
0.059
0.063
0.059
0.070
0.076
0.293

Bank

Concrete

Gas

Housing

Landsat

Mushroom

Pageblock

Pendigit

Pima

Shuttle

Spambase

Wine

β

↵
0.095 1.00
0.096 0.95
0.101 0.75
0.419 1.00
0.425 0.95
0.446 0.75
0.342 1.00
0.353 0.95
0.397 0.75
0.268 1.00
0.281 0.95
0.330 0.75
0.093 1.00
0.103 0.95
0.139 0.75
0.409 1.00
0.416 0.95
0.444 0.75
0.086 1.00
0.087 0.95
0.090 0.75
0.243 1.00
0.248 0.95
0.268 0.75
0.251 1.00
0.259 0.95
0.289 0.75
0.139 1.00
0.140 0.95
0.143 0.75
0.226 1.00
0.240 0.95
0.295 0.75
0.566 1.00
0.575 0.95
0.612 0.75

between samples. They do not explicitly address issues with high-dimensional data nor focus on
algorithms to obtain ⇤. In contrast  we focus primarily on the univariate transformation to handle
high-dimensional data and practical algorithms for estimating ↵⇤. Supervised learning used for class
prior-preserving transformation provides a rich set of techniques to address high-dimensional data.

8 Conclusion

In this paper  we developed a practical algorithm for classiﬁcation of positive-unlabeled data with
noise in the labeled data set.
In particular  we focused on a strategy for high-dimensional data 
providing a univariate transform that reduces the dimension of the data  preserves the class prior so
that estimation in this reduced space remains valid and is then further useful for classiﬁcation. This
approach provides a simple algorithm that simultaneously improves estimation of the class prior and
provides a resulting classiﬁer. We derived a parametric and a nonparametric version of the algorithm
and then evaluated its performance on a wide variety of learning scenarios and data sets. To the best
of our knowledge  this algorithm represents one of the ﬁrst practical and easy-to-use approaches to
learning with high-dimensional positive-unlabeled data with noise in the labels.

8

Acknowledgements
We thank Prof. Michael W. Trosset for helpful comments. Grant support: NSF DBI-1458477  NIH
R01MH105524  NIH R01GM103725  and the Indiana University Precision Health Initiative.

References
S. Bashir and E. M. Carter. High breakdown mixture discriminant analysis. J Multivar Anal  93(1):102–111 

2005.

G. Blanchard  G. Lee  and C. Scott. Semi-supervised novelty detection. J Mach Learn Res  11:2973–3009 

2010.

A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. COLT 1998  pages 92–100 

1998.

C. Bouveyron and S. Girard. Robust supervised classiﬁcation with mixture models: learning from data with

uncertain labels. Pattern Recognit  42(11):2649–2658  2009.

C. Cortes  M. Mohri  M. Riley  and A. Rostamizadeh. Sample selection bias correction theory. ALT 2008 

pages 38–53  2008.

F. Denis  R. Gilleron  and F. Letouzey. Learning from positive and unlabeled examples. Theor Comput Sci 

348(16):70–83  2005.

M. C. du Plessis and M. Sugiyama. Class prior estimation from positive and unlabeled data. IEICE Trans Inf

& Syst  E97-D(5):1358–1362  2014.

C. Elkan and K. Noto. Learning classiﬁers from only positive and unlabeled data. KDD 2008  pages 213–220 

2008.

D. M. Hawkins and G. J. McLachlan. High-breakdown linear discriminant analysis. J Am Stat Assoc  92(437):

136–143  1997.

S. Jain  M. White  M. W. Trosset  and P. Radivojac. Nonparametric semi-supervised learning of class propor-

tions. arXiv preprint arXiv:1601.01944  2016. URL http://arxiv.org/abs/1601.01944.

J. Katz-Samuels and C. Scott. A mutual contamination analysis of mixed membership and partial label models.

arXiv preprint arXiv:1602.06235  2016. URL http://arxiv.org/abs/1602.06235.

N. D. Lawrence and B. Scholkopf. Estimating a kernel Fisher discriminant in the presence of label noise. ICML

2001  pages 306–313  2001.

M. Lichman. UCI Machine Learning Repository  2013. URL http://archive.ics.uci.edu/ml.
H. Liu  J. D. Lafferty  and L. A. Wasserman. Sparse nonparametric density estimation in high dimensions using

the rodeo. AISTATS 2007  pages 283–290  2007.

P. M. Long and R. A. Servedio. Random classiﬁcation noise defeats all convex potential boosters. Mach Learn 

78(3):287–304  2010.

N. Manwani and P. S. Sastry. Noise tolerance under risk minimization. IEEE T Cybern  43(3):1146–1151 

2013.

A. K. Menon  B. van Rooyen  C. S. Ong  and R. C. Williamson. Learning from corrupted binary labels via

class-probability estimation. ICML 2015  pages 125–134  2015.

A. Niculescu-Mizil and R. Caruana. Obtaining calibrated probabilities from boosting. UAI 2005  pages 413–

420  2005.

J. C. Platt. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods 

pages 61–74. MIT Press  1999.

H. G. Ramaswamy  C. Scott  and A. Tewari. Mixture proportion estimation via kernel embedding of distribu-

tions. arXiv preprint arXiv:1603.02501  2016. URL https://arxiv.org/abs/1603.02501.

M. D. Reid and R. C. Williamson. Composite binary losses. J Mach Learn Res  11:2387–2422  2010.
M. Saerens  P. Latinne  and C. Decaestecker. Adjusting the outputs of a classiﬁer to new a priori probabilities:

a simple procedure. Neural Comput  14:21–41  2002.

T. Sanderson and C. Scott. Class proportion estimation with application to multiclass anomaly rejection. AIS-

TATS 2014  pages 850–858  2014.

C. Scott  G. Blanchard  and G. Handy. Classiﬁcation with asymmetric label noise: consistency and maximal

denoising. J Mach Learn Res W&CP  30:489–511  2013.

D. W. Scott. The curse of dimensionality and dimension reduction. Multivariate Density Estimation: Theory 

Practice  and Visualization  pages 195–217  2008.

H. Steen and M. Mann. The ABC’s (and XYZ’s) of peptide sequencing. Nat Rev Mol Cell Biol  5(9):699–711 

2004.

G. Ward  T. Hastie  S. Barry  J. Elith  and J.R. Leathwick. Presence-only data and the EM algorithm. Biometrics 

65(2):554–563  2009.

9

,Bo Xie
Yingyu Liang
Le Song
Shantanu Jain
Martha White
Predrag Radivojac
Zeyuan Allen-Zhu