2016,Cooperative Inverse Reinforcement Learning,For an autonomous system to be helpful to humans and to pose no unwarranted risks  it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative  partial- information game with two agents  human and robot; both are rewarded according to the human’s reward function  but the robot does not initially know what this is. In contrast to classical IRL  where the human is assumed to act optimally in isolation  optimal CIRL solutions produce behaviors such as active teaching  active learning  and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP  prove that optimality in isolation is suboptimal in CIRL  and derive an approximate CIRL algorithm.,Cooperative Inverse Reinforcement Learning

Dylan Hadﬁeld-Menell∗

University of California at Berkeley

Berkeley  CA 94709

Anca Dragan

Pieter Abbeel
Electrical Engineering and Computer Science

Stuart Russell

Abstract

For an autonomous system to be helpful to humans and to pose no unwarranted
risks  it needs to align its values with those of the humans in its environment in
such a way that its actions contribute to the maximization of value for the humans.
We propose a formal deﬁnition of the value alignment problem as cooperative
inverse reinforcement learning (CIRL). A CIRL problem is a cooperative  partial-
information game with two agents  human and robot; both are rewarded according
to the human’s reward function  but the robot does not initially know what this
is. In contrast to classical IRL  where the human is assumed to act optimally in
isolation  optimal CIRL solutions produce behaviors such as active teaching  active
learning  and communicative actions that are more effective in achieving value
alignment. We show that computing optimal joint policies in CIRL games can be
reduced to solving a POMDP  prove that optimality in isolation is suboptimal in
CIRL  and derive an approximate CIRL algorithm.

1

Introduction

“If we use  to achieve our purposes  a mechanical agency with whose operation we cannot interfere
effectively . . . we had better be quite sure that the purpose put into the machine is the purpose which
we really desire.” So wrote Norbert Wiener (1960) in one of the earliest explanations of the problems
that arise when a powerful autonomous system operates with an incorrect objective. This value
alignment problem is far from trivial. Humans are prone to mis-stating their objectives  which can
lead to unexpected implementations. In the myth of King Midas  the main character learns that
wishing for ‘everything he touches to turn to gold’ leads to disaster. In a reinforcement learning
context  Russell & Norvig (2010) describe a seemingly reasonable  but incorrect  reward function for
a vacuum robot: if we reward the action of cleaning up dirt  the optimal policy causes the robot to
repeatedly dump and clean up the same dirt.
A solution to the value alignment problem has long-term implications for the future of AI and its
relationship to humanity (Bostrom  2014) and short-term utility for the design of usable AI systems.
Giving robots the right objectives and enabling them to make the right trade-offs is crucial for
self-driving cars  personal assistants  and human–robot interaction more broadly.
The ﬁeld of inverse reinforcement learning or IRL (Russell  1998; Ng & Russell  2000; Abbeel
& Ng  2004) is certainly relevant to the value alignment problem. An IRL algorithm infers the
reward function of an agent from observations of the agent’s behavior  which is assumed to be
optimal (or approximately so). One might imagine that IRL provides a simple solution to the value
alignment problem: the robot observes human behavior  learns the human reward function  and
behaves according to that function. This simple idea has two ﬂaws. The ﬁrst ﬂaw is obvious: we
don’t want the robot to adopt the human reward function as its own. For example  human behavior
(especially in the morning) often conveys a desire for coffee  and the robot can learn this with IRL 
but we don’t want the robot to want coffee! This ﬂaw is easily ﬁxed: we need to formulate the value

∗{dhm  anca  pabbeel  russell}@cs.berkeley.edu

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

alignment problem so that the robot always has the ﬁxed objective of optimizing reward for the
human  and becomes better able to do so as it learns what the human reward function is.
The second ﬂaw is less obvious  and less easy to ﬁx. IRL assumes that observed behavior is optimal
in the sense that it accomplishes a given task efﬁciently. This precludes a variety of useful teaching
behaviors. For example  efﬁciently making a cup of coffee  while the robot is a passive observer  is a
inefﬁcient way to teach a robot to get coffee. Instead  the human should perhaps explain the steps in
coffee preparation and show the robot where the backup coffee supplies are kept and what do if the
coffee pot is left on the heating plate too long  while the robot might ask what the button with the
puffy steam symbol is for and try its hand at coffee making with guidance from the human  even if
the ﬁrst results are undrinkable. None of these things ﬁt in with the standard IRL framework.
Cooperative inverse reinforcement learning. We propose  therefore  that value alignment should
be formulated as a cooperative and interactive reward maximization process. More precisely  we
deﬁne a cooperative inverse reinforcement learning (CIRL) game as a two-player game of partial
information  in which the “human”  H  knows the reward function (represented by a generalized
parameter θ)  while the “robot”  R  does not; the robot’s payoff is exactly the human’s actual reward.
Optimal solutions to this game maximize human reward; we show that solutions may involve active
instruction by the human and active learning by the robot.
Reduction to POMDP and Sufﬁcient Statistics. As one might expect  the structure of CIRL games
is such that they admit more efﬁcient solution algorithms than are possible for general partial-
information games. Let (πH  πR) be a pair of policies for human and robot  each depending  in
general  on the complete history of observations and actions. A policy pair yields an expected sum of
rewards for each player. CIRL games are cooperative  so there is a well-deﬁned optimal policy pair
that maximizes value.2 In Section 3 we reduce the problem of computing an optimal policy pair to the
solution of a (single-agent) POMDP. This shows that the robot’s posterior over θ is a sufﬁcient statistic 
in the sense that there are optimal policy pairs in which the robot’s behavior depends only on this
statistic. Moreover  the complexity of solving the POMDP is exponentially lower than the NEXP-hard
bound that (Bernstein et al.  2000) obtained by reducing a CIRL game to a general Dec-POMDP.
Apprenticeship Learning and Suboptimality of IRL-Like Solutions. In Section 3.3 we model
apprenticeship learning (Abbeel & Ng  2004) as a two-phase CIRL game. In the ﬁrst phase  the
learning phase  both H and R can take actions and this lets R learn about θ. In the second phase 
the deployment phase  R uses what it learned to maximize reward (without supervision from H).
We show that classic IRL falls out as the best-response policy for R under the assumption that the
human’s policy is “demonstration by expert” (DBE)  i.e.  acting optimally in isolation as if no robot
exists. But we show also that this DBE/IRL policy pair is not  in general  optimal: even if the robot
expects expert behavior  demonstrating expert behavior is not the best way to teach that algorithm.
We give an algorithm that approximately computes H’s best response when R is running IRL under
the assumption that rewards are linear in θ and state features. Section 4 compares this best-response
policy with the DBE policy in an example game and provides empirical conﬁrmation that the best-
response policy  which turns out to “teach” R about the value landscape of the problem  is better than
DBE. Thus  designers of apprenticeship learning systems should expect that users will violate the
assumption of expert demonstrations in order to better communicate information about the objective.

2 Related Work

Our proposed model shares aspects with a variety of existing models. We divide the related work into
three categories: inverse reinforcement learning  optimal teaching  and principal–agent models.

Inverse Reinforcement Learning. Ng & Russell (2000) deﬁne inverse reinforcement learning
(IRL) as follows: “Given measurements of an [actor]’s behavior over time.
. . . Determine the
reward function being optimized.” The key assumption IRL makes is that the observed behavior is
optimal in the sense that the observed trajectory maximizes the sum of rewards. We call this the
demonstration-by-expert (DBE) assumption. One of our contributions is to prove that this may be
suboptimal behavior in a CIRL game  as H may choose to accept less reward on a particular action
in order to convey more information to R. In CIRL the DBE assumption prescribes a ﬁxed policy

2A coordination problem of the type described in Boutilier (1999) arises if there are multiple optimal policy

pairs; we defer this issue to future work.

2

Figure 1: The difference between demonstration-by-expert and instructive demonstration in the
mobile robot navigation problem from Section 4. Left: The ground truth reward function. Lighter
grid cells indicates areas of higher reward. Middle: The demonstration trajectory generated by the
expert policy  superimposed on the maximum a-posteriori reward function the robot infers. The robot
successfully learns where the maximum reward is  but little else. Right: An instructive demonstration
generated by the algorithm in Section 3.4 superimposed on the maximum a-posteriori reward function
that the robot infers. This demonstration highlights both points of high reward and so the robot learns
a better estimate of the reward.
for H. As a result  many IRL algorithms can be derived as state estimation for a best response to
different πH  where the state includes the unobserved reward parametrization θ.
Ng & Russell (2000)  Abbeel & Ng (2004)  and Ratliff et al. (2006) compute constraints that
characterize the set of reward functions so that the observed behavior maximizes reward. In general 
there will be many reward functions consistent with this constraint. They use a max-margin heuristic
to select a single reward function from this set as their estimate. In CIRL  the constraints they compute
characterize R’s belief about θ under the DBE assumption.
Ramachandran & Amir (2007) and Ziebart et al. (2008) consider the case where πH is “noisily
expert ” i.e.  πHis a Boltzmann distribution where actions or trajectories are selected in proportion
to the exponent of their value. Ramachandran & Amir (2007) adopt a Bayesian approach and place
an explicit prior on rewards. Ziebart et al. (2008) places a prior on reward functions indirectly by
assuming a uniform prior over trajectories. In our model  these assumptions are variations of DBE
and both implement state estimation for a best response to the appropriate ﬁxed H.
Natarajan et al. (2010) introduce an extension to IRL where R observes multiple actors that cooperate
to maximize a common reward function. This is a different type of cooperation than we consider 
as the reward function is common knowledge and R is a passive observer. Waugh et al. (2011) and
Kuleshov & Schrijvers (2015) consider the problem of inferring payoffs from observed behavior in a
general (i.e.  non-cooperative) game given observed behavior. It would be interesting to consider an
analogous extension to CIRL  akin to mechanism design  in which R tries to maximize collective
utility for a group of Hs that may have competing objectives.
Fern et al. (2014) consider a hidden-goal MDP  a special case of a POMDP where the goal is an
unobserved part of the state. This can be considered a special case of CIRL  where θ encodes a
particular goal state. The frameworks share the idea that R helps H. The key difference between the
models lies in the treatment of the human (the agent in their terminology). Fern et al. (2014) model
the human as part of the environment. In contrast  we treat H as an actor in a decision problem that
both actors collectively solve. This is crucial to modeling the human’s incentive to teach.

Optimal Teaching. Because CIRL incentivizes the human to teach  as opposed to maximizing
reward in isolation  our work is related to optimal teaching: ﬁnding examples that optimally train
a learner (Balbach & Zeugmann  2009; Goldman et al.  1993; Goldman & Kearns  1995). The key
difference is that efﬁcient learning is the objective of optimal teaching  while it emerges as a property
of optimal equilibrium behavior in CIRL.
Cakmak & Lopes (2012) consider an application of optimal teaching where the goal is to teach the
learner the reward function for an MDP. The teacher gets to pick initial states from which an expert
executes the reward-maximizing trajectory. The learner uses IRL to infer the reward function  and
the teacher picks initial states to minimize the learner’s uncertainty. In CIRL  this approach can be
characterized as an approximate algorithm for H that greedily minimizes the entropy of R’s belief.
Beyond teaching  several models focus on taking actions that convey some underlying state  not
necessarily a reward function. Examples include ﬁnding a motion that best communicates an agent’s
intention (Dragan & Srinivasa  2013)  or ﬁnding a natural language utterance that best communicates

3

GroundTruthExpertDemonstrationInstructiveDemonstrationa particular grounding (Golland et al.  2010). All of these approaches model the observer’s inference
process and compute actions (motion or speech) that maximize the probability an observer infers the
correct hypothesis or goal. Our approximate solution to CIRL is analogous to these approaches  in
that we compute actions that are informative of the correct reward function.

Principal–agent models. Value alignment problems are not intrinsic to artiﬁcial agents. Kerr
(1975) describes a wide variety of misaligned incentives in the aptly titled “On the folly of rewarding
A  while hoping for B.” In economics  this is known as the principal–agent problem: the principal
(e.g.  the employer) speciﬁes incentives so that an agent (e.g.  the employee) maximizes the principal’s
proﬁt (Jensen & Meckling  1976).
Principal–agent models study the problem of generating appropriate incentives in a non-cooperative
setting with asymmetric information. In this setting  misalignment arises because the agents that
economists model are people and intrinsically have their own desires. In AI  misalignment arises
entirely from the information asymmetry between the principal and the agent; if we could characterize
the correct reward function  we could program it into an artiﬁcial agent. Gibbons (1998) provides a
useful survey of principal–agent models and their applications.

3 Cooperative Inverse Reinforcement Learning

This section formulates CIRL as a two-player Markov game with identical payoffs  reduces the
problem of computing an optimal policy pair for a CIRL game to solving a POMDP  and characterizes
apprenticeship learning as a subclass of CIRL games.

3.1 CIRL Formulation

Deﬁnition 1. A cooperative inverse reinforcement learning (CIRL) game M is a two-player Markov
game with identical payoffs between a human or principal  H  and a robot or agent  R. The
game is described by a tuple  M = (cid:104)S {AH AR}  T (·|· · ·) {Θ  R(· · ·;·)}  P0(· ·)  γ(cid:105)  with the
following deﬁnitions:

both agents: T (s(cid:48)

S a set of world states: s ∈ S.
AH a set of actions for H: aH ∈ AH.
AR a set of actions for R: aR ∈ AR.
T (·|· · ·) a conditional distribution on the next world state  given previous state and action for
Θ a set of possible static reward parameters  only observed by H: θ ∈ Θ.
R(· · ·;·) a parameterized reward function that maps world states  joint actions  and reward
P0(· ·) a distribution over the initial state  represented as tuples: P0(s0  θ)
γ a discount factor: γ ∈ [0  1].

parameters to real numbers. R : S × AH × AR × Θ → R.

|s  aH  aR).

t . Both actors receive reward rt = R(st  aH

We write the reward for a state–parameter pair as R(s  aH  aR; θ) to distinguish the static reward
parameters θ from the changing world state s. The game proceeds as follows. First  the initial
state  a tuple (s  θ)  is sampled from P0. H observes θ  but R does not. This observation model
captures the notion that only the human knows the reward function  while both actors know a prior
distribution over possible reward functions. At each timestep t  H and R observe the current state st
and select their actions aH
t ; θ) and observe
each other’s action selection. A state for the next timestep is sampled from the transition distribution 
st+1 ∼ PT (s(cid:48)
histories; πH :(cid:2)
Behavior in a CIRL game is deﬁned by a pair of policies  (πH  πR)  that determine action selection
for H and R respectively. In general  these policies can be arbitrary functions of their observation
→ AR. The optimal joint
policy is the policy that maximizes value. The value of a state is the expected sum of discounted
rewards under the initial distribution of reward parameters and world states.
Remark 1. A key property of CIRL is that the human and the robot get rewards determined by the
same reward function. This incentivizes the human to teach and the robot to learn without explicitly
encoding these as objectives of the actors.

× Θ → AH  πR :(cid:2)

AH × AR × S

AH × AR × S

|st  aH

t   aR

t )  and the process repeats.

t   aR

(cid:3)∗

t   aR

(cid:3)∗

4

3.2 Structural Results for Computing Optimal Policy Pairs
The analogue in CIRL to computing an optimal policy for an MDP is the problem of computing an
optimal policy pair. This is a pair of policies that maximizes the expected sum of discounted rewards.
This is not the same as ‘solving’ a CIRL game  as a real world implementation of a CIRL agent must
account for coordination problems and strategic uncertainty (Boutilier  1999). The optimal policy pair
represents the best H and R can do if they can coordinate perfectly before H observes θ. Computing
an optimal joint policy for a cooperative game is the solution to a decentralized-partially observed
Markov decision process (Dec-POMDP). Unfortunately  Dec-POMDPs are NEXP-complete (Bernstein
et al.  2000) so general Dec-POMDP algorithms have a computational complexity that is doubly
exponential. Fortunately  CIRL games have special structure that reduces this complexity.
Nayyar et al. (2013) shows that a Dec-POMDP can be reduced to a coordination-POMDP. The actor in
this POMDP is a coordinator that observes all common observations and speciﬁes a policy for each
actor. These policies map each actor’s private information to an action. The structure of a CIRL game
implies that the private information is limited to H’s initial observation of θ. This allows the reduction
to a coordination-POMDP to preserve the size of the (hidden) state space  making the problem easier.
Theorem 1. Let M be an arbitrary CIRL game with state space S and reward space Θ. There exists
a (single-actor) POMDP MC with (hidden) state space SC such that |SC| = |S| · |Θ| and  for any
policy pair in M  there is a policy in MC that achieves the same sum of discounted rewards.
Theorem proofs can be found in the supplementary material. An immediate consequence of this
result is that R’s belief about θ is a sufﬁcient statistic for optimal behavior.
Corollary 1. Let M be a CIRL game. There exists an optimal policy pair (πH∗
depends on the current state and R’s belief.
Remark 2. In a general Dec-POMDP  the hidden state for the coordinator-POMDP includes each
actor’s history of observations. In CIRL  θ is the only private information so we get an exponential
decrease in the complexity of the reduced problem. This allows one to apply general POMDP
algorithms to compute optimal joint policies in CIRL.

) that only

  πR∗

It is important to note that the reduced problem may still be very challenging. POMDPs are difﬁcult
in their own right and the reduced problem still has a much larger action space. That being said 
this reduction is still useful in that it characterizes optimal joint policy computation for CIRL as
signiﬁcantly easier than Dec-POMDPs. Furthermore  this theorem can be used to justify approximate
methods (e.g.  iterated best response) that only depend on R’s belief state.

3.3 Apprenticeship Learning as a Subclass of CIRL Games
A common paradigm for robot learning from humans is apprenticeship learning. In this paradigm 
a human gives demonstrations to a robot of a sample task and the robot is asked to imitate it in a
subsequent task. In what follows  we formulate apprenticeship learning as turn-based CIRL with a
learning phase and a deployment phase. We characterize IRL as the best response (i.e.  the policy
that maximizes reward given a ﬁxed policy for the other player) to a demonstration-by-expert policy
for H. We also show that this policy is  in general  not part of an optimal joint policy and so IRL is
generally a suboptimal approach to apprenticeship learning.
Deﬁnition 2. (ACIRL) An apprenticeship cooperative inverse reinforcement learning (ACIRL) game
is a turn-based CIRL game with two phases: a learning phase where the human and the robot take
turns acting  and a deployment phase  where the robot acts independently.

Example. Consider an example apprenticeship task where R needs to help H make ofﬁce supplies.
H and R can make paperclips and staples and the unobserved θ describe H’s preference for paperclips
vs staples. We model the problem as an ACIRL game in which the learning and deployment phase
each consist of an individual action. The world state in this problem is a tuple (ps  qs  t) where ps
and qs respectively represent the number of paperclips and staples H owns. t is the round number.
An action is a tuple (pa  qa) that produces pa paperclips and qa staples. The human can make 2
items total: AH = {(0  2)  (1  1)  (2  0)}. The robot has different capabilities. It can make 50
units of each item or it can choose to make 90 of a single item: AR = {(0  90)  (50  50)  (90  0)}.
We let Θ = [0  1] and deﬁne R so that θ indicates the relative preference between paperclips and
staples:R(s  (pa  qa); θ) = θpa + (1 − θ)qa. R’s action is ignored when t = 0 and H’s is ignored
when t = 1. At t = 2  the game is over  so the game transitions to a sink state  (0  0  2).

5

Deployment phase — maximize mean reward estimate.
It is simplest to analyze the deployment
phase ﬁrst. R is the only actor in this phase so it get no more observations of its reward. We have
shown that R’s belief about θ is a sufﬁcient statistic for the optimal policy. This belief about θ induces
a distribution over MDPs. A straightforward extension of a result due to Ramachandran & Amir
(2007) shows that R’s optimal deployment policy maximizes reward for the mean reward function.
Theorem 2. Let M be an ACIRL game. In the deployment phase  the optimal policy for R maximizes
reward in the MDP induced by the mean θ from R’s belief.
3 ] and (2  0) otherwise.
In our example  suppose that πH selects (0  2) if θ ∈ [0  1
R begins with a uniform prior on θ so observing  e.g.  aH = (0  2) leads to a posterior distribution
that is uniform on [0  1
3 ). Theorem 2 shows that the optimal action maximizes reward for the mean θ
so an optimal R behaves as though θ = 1

3 )  (1  1) if θ ∈ [ 1

6 during the deployment phase.

3   2

Learning phase — expert demonstrations are not optimal. A wide variety of apprenticeship
learning approaches assume that demonstrations are given by an expert. We say that H satisﬁes the
demonstration-by-expert (DBE) assumption in ACIRL if she greedily maximizes immediate reward
on her turn. This is an ‘expert’ demonstration because it demonstrates a reward maximizing action
but does not account for that action’s impact on R’s belief. We let πE represent the DBE policy.
Theorem 2 enables us to characterize the best response for R when πH = πE: use IRL to compute
the posterior over θ during the learning phase and then act to maximize reward under the mean θ in
the deployment phase. We can also analyze the DBE assumption itself. In particular  we show that
πE is not H’s best response when πR is a best response to πE.
Theorem 3. There exist ACIRL games where the best-response for H to πR violates the expert
demonstrator assumption. In other words  if br(π) is the best response to π  then br(br(πE)) (cid:54)= πE.
The supplementary material proves this theorem by computing the optimal equilibrium for our
example. In that equilibrium  H selects (1  1) if θ ∈ [ 41
92 ]. In contrast  πE only chooses (1  1) if
θ = 0.5. The change arises because there are situations (e.g.  θ = 0.49) where the immediate loss of
reward to H is worth the improvement in R’s estimate of θ.
Remark 3. We should expect experienced users of apprenticeship learning systems to present
demonstrations optimized for fast learning rather than demonstrations that maximize reward.
Crucially  the demonstrator is incentivized to deviate from R’s assumptions. This has implications
for the design and analysis of apprenticeship systems in robotics. Inaccurate assumptions about user
behavior are notorious for exposing bugs in software systems (see  e.g.  Leveson & Turner (1993)).

92   51

3.4 Generating Instructive Demonstrations
Now  we consider the problem of computing H’s best response when R uses IRL as a state estimator.
For our toy example  we computed solutions exhaustively  for realistic problems we need a more
efﬁcient approach. Section 3.2 shows that this can be reduced to an POMDP where the state is a
tuple of world state  reward parameters  and R’s belief. While this is easier than solving a general
Dec-POMDP  it is a computational challenge. If we restrict our attention to the case of linear reward
functions we can develop an efﬁcient algorithm to compute an approximate best response.
Speciﬁcally  we consider the case where the reward for a state (s  θ) is deﬁned as a linear combination
of state features for some feature function φ : R(s  aH  aR; θ) = φ(s)(cid:62)θ. Standard results from the
IRL literature show that policies with the same expected feature counts have the same value (Abbeel
& Ng  2004). Combined with Theorem 2  this implies that the optimal πR under the DBE assumption
computes a policy that matches the observed feature counts from the learning phase.
This suggests a simple approximation scheme. To compute a demonstration trajectory τ H  ﬁrst
compute the feature counts R would observe in expectation from the true θ and then select actions
that maximize similarity to these target features. If φθ are the expected feature counts induced by θ
then this scheme amounts to the following decision rule:

τ H ← argmax

(1)
This rule selects a trajectory that trades off between the sum of rewards φ(τ )(cid:62)θ and the feature
dissimilarity ||φθ − φ(τ )||2. Note that this is generally distinct from the action selected by the
demonstration-by-expert policy. The goal is to match the expected sum of features under a distribution
of trajectories with the sum of features from a single trajectory. The correct measure of feature

φ(τ )(cid:62)θ − η||φθ − φ(τ )||2.

τ

6

Figure 2: Left  Middle: Comparison of ‘expert’ demonstration (πE) with ‘instructive’ demonstration (br).
Lower numbers are better. Using the best response causes R to infer a better distribution over θ so it does a
better job of maximizing reward. Right: The regret of the instructive demonstration policy as a function of how
optimal R expects H to be. λ = 0 corresponds to a robot that expects purely random behavior and λ = ∞
corresponds to a robot that expects optimal behavior. Regret is minimized for an intermediate value of λ: if λ is
too small  then R learns nothing from its observations; if λ is too large  then R expects many values of θ to lead
to the same trajectory so H has no way to differentiate those reward functions.
similarity is regret: the difference between the reward R would collect if it knew the true θ and the
reward R actually collects using the inferred θ. Computing this similarity is expensive  so we use an
(cid:96)2 norm as a proxy measure of similarity.

4 Experiments
4.1 Cooperative Learning for Mobile Robot Navigation
Our experimental domain is a 2D navigation problem on a discrete grid. In the learning phase of
the game  H teleoperates a trajectory while R observes. In the deployment phase  R is placed in a
random state and given control of the robot. We use a ﬁnite horizon H  and let the ﬁrst H
2 timesteps
be the learning phase. There are Nφ state features deﬁned as radial basis functions where the centers
are common knowledge. Rewards are linear in these features and θ. The initial world state is in the
middle of the map. We use a uniform distribution on [−1  1]Nφ for the prior on θ. Actions move in
one of the four cardinal directions {N  S  E  W} and there is an additional no-op ∅ that each actor
executes deterministically on the other agent’s turn.
Figure 1 shows an example comparison between demonstration-by-expert and the approximate best
response policy in Section 3.4. The leftmost image is the ground truth reward function. Next to
it are demonstration trajectories produce by these two policies. Each path is superimposed on the
maximum a-posteriori reward function the robot infers from the demonstration. We can see that the
demonstration-by-expert policy immediately goes to the highest reward and stays there. In contrast 
the best response policy moves to both areas of high reward. The robot reward function the robot
infers from the best response demonstration is much more representative of the true reward function 
when compared with the reward function it infers from demonstration-by-expert.

4.2 Demonstration-by-Expert vs Best Responder
Hypothesis. When R plays an IRL algorithm that matches features  H prefers the best response
policy from Section 3.4 to πE: the best response policy will signiﬁcantly outperform the DBE policy.
Manipulated Variables. Our experiment consists of 2 factors: H-policy and num-features. We
make the assumption that R uses an IRL algorithm to compute its estimate of θ during learn-
ing and maximizes reward under this estimate during deployment. We use Maximum-Entropy
IRL (Ziebart et al.  2008) to implement R’s policy. H-policy varies H’s strategy πHand has two
levels: demonstration-by-expert (πE) and best-responder (br). In the πE level H maximizes reward
during the demonstration. In the br level H uses the approximate algorithm from Section 3.4 to
compute an approximate best response to πR. The trade-off between reward and communication η is
set by cross-validation before the game begins. The num-features factor varies the dimensionality of
φacross two levels: 3 features and 10 features. We do this to test whether and how the difference
between experts and best-responders is affected by dimensionality. We use a factorial design that
leads to 4 distinct conditions. We test each condition against a random sample of N = 500 different
reward parameters. We use a within-subjects design with respect to the the H-policy factor so the
same reward parameters are tested for πE and br.

7

RegretKL||θGT−ˆθ||2036912num-features=3brπERegretKL||θGT−ˆθ||20481216num-features=10brπE10−310−1101λ00.250.50.751RegretRegretforbrDependent Measures. We use the regret with respect to a fully-observed setting where the robot
knows the ground truth θ as a measure of performance. We let ˆθ be the robot’s estimate of the reward
parameters and let θGT be the ground truth reward parameters. The primary measure is the regret of
R’s policy: the difference between the value of the policy that maximizes the inferred reward ˆθ and
the value of the policy that maximizes the true reward θGT . We also use two secondary measures.
The ﬁrst is the KL-divergence between the maximum-entropy trajectory distribution induced by ˆθ
and the maximum-entropy trajectory distribution induced by θ. Finally  we use the (cid:96)2-norm between
the vector or rewards deﬁned by ˆθ and the vector induced by θGT .
Results. There was relatively little correlation between the measures (Cronbach’s α of .47)  so
we ran a factorial repeated measures ANOVA for each measure. Across all measures  we found a
signiﬁcant effect for H-policy  with br outperforming πE on all measures as we hypothesized (all
with F > 962  p < .0001). We did ﬁnd an interaction effect with num-features for KL-divergence
and the (cid:96)2-norm of the reward vector but post-hoc Tukey HSD showed br to always outperform πE.
The interaction effect arises because the gap between the two levels of H-policy is larger with fewer
reward parameters; we interpret this as evidence that num-features = 3 is an easier teaching problem
for H. Figure 2 (Left  Middle) shows the dependent measures from our experiment.

4.3 Varying R’s Expectations
Maximum-Entropy IRL includes a free parameter λ that controls how optimal R expects H to behave.
If λ = 0  R will update its belief as if H’s observed behavior is independent of her preferences θ. If
λ = ∞  R will update its belief as if H’s behavior is exactly optimal. We ran a followup experiment
to determine how varying λ changes the regret of the br policy.
Changing λ changes the forward model in R’s belief update: the mapping R hypothesizes between
a given reward parameter θ and the observed feature counts φθ. This mapping is many-to-one for
extreme values of λ. λ ≈ 0 means that all values of θ lead to the same expected feature counts
because trajectories are chosen uniformly at random. Alternatively  λ >> 0 means that almost all
probability mass falls on the optimal trajectory and many values of θ will lead to the same optimal
trajectory. This suggests that it is easier for H to differentiate different values of θ if R assumes she
is noisily optimal  but only up until a maximum noise level. Figure 2 plots regret as a function of λ
and supports this analysis: H has less regret for intermediate values of λ.

5 Conclusion and Future Work

In this work  we presented a game-theoretic model for cooperative learning  CIRL. Key to this model
is that the robot knows that it is in a shared environment and is attempting to maximize the human’s
reward (as opposed to estimating the human’s reward function and adopting it as its own). This leads
to cooperative learning behavior and provides a framework in which to design HRI algorithms and
analyze the incentives of both actors in a reward learning environment.
We reduced the problem of computing an optimal policy pair to solving a POMDP. This is a useful
theoretical tool and can be used to design new algorithms  but it is clear that optimal policy pairs
are only part of the story. In particular  when it performs a centralized computation  the reduction
assumes that we can effectively program both actors to follow a set coordination policy. This is
clearly infeasible in reality  although it may nonetheless be helpful in training humans to be better
teachers. An important avenue for future research will be to consider the coordination problem: the
process by which two independent actors arrive at policies that are mutual best responses. Returning
to Wiener’s warning  we believe that the best solution is not to put a speciﬁc purpose into the machine
at all  but instead to design machines that provably converge to the right purpose as they go along.

Acknowledgments

This work was supported by the DARPA Simplifying Complexity in Scientiﬁc Discovery (SIMPLEX)
program  the Berkeley Deep Drive Center  the Center for Human Compatible AI  the Future of Life
Institute  and the Defense Sciences Ofﬁce contract N66001-15-2-4048. Dylan Hadﬁeld-Menell is
also supported by a NSF Graduate Research Fellowship.

8

References
Abbeel  P and Ng  A. Apprenticeship learning via inverse reinforcement learning. In ICML  2004.
Balbach  F and Zeugmann  T. Recent developments in algorithmic teaching. In Language and

Automata Theory and Applications. Springer  2009.

Bernstein  D  Zilberstein  S  and Immerman  N. The complexity of decentralized control of Markov

decision processes. In UAI  2000.

Bostrom  N. Superintelligence: Paths  dangers  strategies. Oxford  2014.
Boutilier  Craig. Sequential optimality and coordination in multiagent systems. In IJCAI  volume 99 

pp. 478–485  1999.

Cakmak  M and Lopes  M. Algorithmic and human teaching of sequential decision tasks. In AAAI 

2012.

Dragan  A and Srinivasa  S. Generating legible motion. In Robotics: Science and Systems  2013.
Fern  A  Natarajan  S  Judah  K  and Tadepalli  P. A decision-theoretic model of assistance. JAIR  50

(1):71–104  2014.

Gibbons  R. Incentives in organizations. Technical report  National Bureau of Economic Research 

1998.

Goldman  S and Kearns  M. On the complexity of teaching. Journal of Computer and System

Sciences  50(1):20–31  1995.

Goldman  S  Rivest  R  and Schapire  R. Learning binary relations and total orders. SIAM Journal on

Computing  22(5):1006–1034  1993.

Golland  D  Liang  P  and Klein  D. A game-theoretic approach to generating spatial descriptions. In

EMNLP  pp. 410–419  2010.

Jensen  M and Meckling  W. Theory of the ﬁrm: Managerial behavior  agency costs and ownership

structure. Journal of Financial Economics  3(4):305–360  1976.

Kerr  S. On the folly of rewarding A  while hoping for B. Academy of Management Journal  18(4):

769–783  1975.

Kuleshov  V and Schrijvers  O. Inverse game theory. Web and Internet Economics  2015.
Leveson  N and Turner  C. An investigation of the Therac-25 accidents. IEEE Computer  26(7):

18–41  1993.

Natarajan  S  Kunapuli  G  Judah  K  Tadepalli  P  and Kersting  Kand Shavlik  J. Multi-agent inverse

reinforcement learning. In Int’l Conference on Machine Learning and Applications  2010.

Nayyar  A  Mahajan  A  and Teneketzis  D. Decentralized stochastic control with partial history
sharing: A common information approach. IEEE Transactions on Automatic Control  58(7):
1644–1658  2013.

Ng  A and Russell  S. Algorithms for inverse reinforcement learning. In ICML  2000.
Ramachandran  D and Amir  E. Bayesian inverse reinforcement learning. In IJCAI  2007.
Ratliff  N  Bagnell  J  and Zinkevich  M. Maximum margin planning. In ICML  2006.
Russell  S. and Norvig  P. Artiﬁcial Intelligence. Pearson  2010.
Russell  Stuart J. Learning agents for uncertain environments (extended abstract). In COLT  1998.
Waugh  K  Ziebart  B  and Bagnell  J. Computational rationalization: The inverse equilibrium

problem. In ICML  2011.

Wiener  N. Some moral and technical consequences of automation. Science  131  1960.
Ziebart  B  Maas  A  Bagnell  J  and Dey  A. Maximum entropy inverse reinforcement learning. In

AAAI  2008.

9

,Dylan Hadfield-Menell
Stuart Russell
Pieter Abbeel
Anca Dragan