2010,Copula Processes,We define a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions. As an example  we develop a stochastic volatility model  Gaussian Copula Process Volatility (GCPV)  to predict the latent standard deviations of a sequence of random variables. To make predictions we use Bayesian inference  with the Laplace approximation  and with Markov chain Monte Carlo as an alternative. We find our model can outperform GARCH on simulated and financial data. And unlike GARCH  GCPV can easily handle missing data  incorporate covariates other than time  and model a rich class of covariance structures.,Copula Processes

Andrew Gordon Wilson∗
Department of Engineering
University of Cambridge
agw38@cam.ac.uk

Zoubin Ghahramani†
Department of Engineering
University of Cambridge

zoubin@eng.cam.ac.uk

Abstract

We deﬁne a copula process which describes the dependencies between arbitrarily
many random variables independently of their marginal distributions. As an exam-
ple  we develop a stochastic volatility model  Gaussian Copula Process Volatility
(GCPV)  to predict the latent standard deviations of a sequence of random vari-
ables. To make predictions we use Bayesian inference  with the Laplace approxi-
mation  and with Markov chain Monte Carlo as an alternative. We ﬁnd our model
can outperform GARCH on simulated and ﬁnancial data. And unlike GARCH 
GCPV can easily handle missing data  incorporate covariates other than time  and
model a rich class of covariance structures.

Imagine measuring the distance of a rocket as it leaves Earth  and wanting to know how these mea-
surements correlate with one another. How much does the value of the measurement at ﬁfteen
minutes depend on the measurement at ﬁve minutes? Once we’ve learned this correlation structure 
suppose we want to compare it to the dependence between measurements of the rocket’s velocity.
To do this  it is convenient to separate dependence from the marginal distributions of our measure-
ments. At any given time  a rocket’s distance from Earth could have a Gamma distribution  while its
velocity could have a Gaussian distribution. And separating dependence from marginal distributions
is precisely what a copula function does.
While copulas have recently become popular  especially in ﬁnancial applications [1  2]  as Nelsen
[3] writes  “the study of copulas and the role they play in probability  statistics  and stochastic
processes is a subject still in its infancy. There are many open problems. . . ” Typically only bivariate
(and recently trivariate) copulas are being used and studied. In our introductory example  we are
interested in learning the correlations in different stochastic processes  and comparing them.
It
would therefore be useful to have a copula process  which can describe the dependencies between
arbitrarily many random variables independently of their marginal distributions. We deﬁne such a
process. And as an example  we develop a stochastic volatility model  Gaussian Copula Process
Volatility (GCPV). In doing so  we provide a Bayesian framework for the learning the marginal
distributions and dependency structure of what we call a Gaussian copula process.
The volatility of a random variable is its standard deviation. Stochastic volatility models are used
to predict the volatilities of a heteroscedastic sequence – a sequence of random variables with dif-
ferent variances  like distance measurements of a rocket as it leaves the Earth. As the rocket gets
further away  the variance on the measurements increases. Heteroscedasticity is especially impor-
tant in econometrics; the returns on equity indices  like the S&P 500  or on currency exchanges  are
heteroscedastic. Indeed  in 2003  Robert Engle won the Nobel Prize in economics “for methods of
analyzing economic time series with time-varying volatility”. GARCH [4]  a generalized version of
Engle’s ARCH  is arguably unsurpassed for predicting the volatility of returns on equity indices and
currency exchanges [5  6  7]. GCPV can outperform GARCH  and is competitive on ﬁnancial data
that especially suits GARCH [8  9  10]. Before discussing GCPV  we ﬁrst introduce copulas and the
copula process. For a review of Gaussian processes  see Rasmussen and Williams [11].

∗http://mlg.eng.cam.ac.uk/andrew
†Also at the machine learning department at Carnegie Mellon University.

1

1 Copulas

Copulas are important because they separate the dependency structure between random variables
from their marginal distributions.
Intuitively  we can describe the dependency structure of any
multivariate joint distribution H(x1  . . .   xn) = P (X1 ≤ x1  . . . Xn ≤ xn) through a two step
process. First we take each univariate random variable Xi and transform it through its cumulative
distribution function (cdf) Fi to get Ui = Fi(Xi)  a uniform random variable. We then express
the dependencies between these transformed variables through the n-copula C(u1  . . .   un).
Formally  an n-copula C : [0  1]n → [0  1] is a multivariate cdf with uniform univariate marginals:
C(u1  u2  . . .   un) = P (U1 ≤ u1  U2 ≤ u2  . . .   Un ≤ un)  where U1  U2  . . .   Un are standard
uniform random variables. Sklar [12] precisely expressed our intuition in the theorem below.

Theorem 1.1. Sklar’s theorem
Let H be an n-dimensional distribution function with marginal distribution functions F1  F2  . . .   Fn.
Then there exists an n-copula C such that for all (x1  x2  . . .   xn) ∈ [−∞ ∞]n 

H(x1  x2  . . .   xn) = C(F1(x1)  F2(x2)  . . .   Fn(xn)) = C(u1  u2  . . .   un).

(1)
If F1  F2  . . .   Fn are all continuous then C is unique; otherwise C is uniquely determined on
Range F1 × Range F2 × ··· × Range Fn. Conversely  if C is an n-copula and F1  F2  . . .   Fn are
distribution functions  then the function H is an n-dimensional distribution function with marginal
distribution functions F1  F2  . . .   Fn.
As a corollary  if F (−1)
u1  u2  . . .   un ∈ [0  1]n 

(u) = inf{x : F (x) ≥ u}  the quasi-inverse of Fi  then for all

i

C(u1  u2  . . .   un) = H(F (−1)

(u1)  F (−1)

1

(u2)  . . .   F (−1)

n

(un)).

(2)

2

C(u  v) = Φρ(Φ−1(u)  Φ−1(v)) 

In other words  (2) can be used to construct a copula. For example  the bivariate Gaussian copula is
deﬁned as

(3)
where Φρ is a bivariate Gaussian cdf with correlation coefﬁcient ρ  and Φ is the standard univariate
Gaussian cdf. Li [2] popularised the bivariate Gaussian copula  by showing how it could be used to
study ﬁnancial risk and default correlation  using credit derivatives as an example.
By substituting F (x) for u and G(y) for v in equation (3)  we have a bivariate distribution H(x  y) 
with a Gaussian dependency structure  and marginals F and G. Regardless of F and G  the resulting
H(x  y) can still be uniquely expressed as a Gaussian copula  so long as F and G are continuous. It is
then a copula itself that captures the underlying dependencies between random variables  regardless
of their marginal distributions. For this reason  copulas have been called dependence functions
[13  14]. Nelsen [3] contains an extensive discussion of copulas.

2 Copula Processes

Imagine choosing a covariance function  and then drawing a sample function at some ﬁnite number
of points from a Gaussian process. The result is a sample from a collection of Gaussian random
variables  with a dependency structure encoded by the speciﬁed covariance function. Now  suppose
we transform each of these values through a univariate Gaussian cdf  such that we have a sample
from a collection of uniform random variables. These uniform random variables also have this
underlying Gaussian process dependency structure. One might call the resulting values a draw from
a Gaussian-Uniform Process. We could subsequently put these values through an inverse beta cdf 
to obtain a draw from what could be called a Gaussian-Beta Process: the values would be a sample
from beta random variables  again with an underlying Gaussian process dependency structure. We
could also transform the uniform values with different inverse cdfs  which would give a sample from
different random variables  with dependencies encoded by the Gaussian process.
The above procedure is a means to generate samples from arbitrarily many random variables  with
arbitrary marginal distributions  and desired dependencies. It is an example of how to use what we
call a copula process – in this case  a Gaussian copula process  since a Gaussian copula describes
the dependency structure of a ﬁnite number of samples. We now formally deﬁne a copula process.

2

Deﬁnition 2.1. Copula Process
Let {Wt} be a collection of random variables indexed by t ∈ T   with marginal distribution functions
Ft  and let Qt = Ft(Wt). Further  let µ be a stochastic process measure with marginal distribution
functions Gt  and joint distribution function H. Then Wt is copula process distributed with base
measure µ  or Wt ∼ CP(µ)  if and only if for all n ∈ N  ai ∈ R 

n(cid:92)

P (

{G(−1)

ti

(Qti ) ≤ ai}) = Ht1 t2 ... tn (a1  a2  . . .   an).

(4)

i=1

ti

is the quasi-inverse of Gti  as previously deﬁned.

Each Qti ∼ Uniform(0  1)  and G(−1)
Deﬁnition 2.2. Gaussian Copula Process
Wt is Gaussian copula process distributed if it is copula process distributed and the base measure
µ is a Gaussian process. If there is a mapping Ψ such that Ψ(Wt) ∼ GP(m(t)  k(t  t(cid:48)))  then we
write Wt ∼ GCP(Ψ  m(t)  k(t  t(cid:48))).
For example  if we have Wt ∼ GCP with m(t) = 0 and k(t  t) = 1  then in the deﬁnition of a copula
process  Gt = Φ  the standard univariate Gaussian cdf  and H is the usual GP joint distribution
function. Supposing this GCP is a Gaussian-Beta process  then Ψ = Φ−1 ◦ FB  where FB is a
univariate Beta cdf. One could similarly deﬁne other copula processes.
We described generally how a copula process can be used to generate samples of arbitrarily many
random variables with desired marginals and dependencies. We now develop a speciﬁc and practical
application of this framework. We introduce a stochastic volatility model  Gaussian Copula Process
Volatility (GCPV)  as an example of how to learn the joint distribution of arbitrarily many random
variables  the marginals of these random variables  and to make predictions. To do this  we ﬁt a
Gaussian copula process by using a type of Warped Gaussian Process [15]. However  our method-
ology varies substantially from Snelson et al. [15]  since we are doing inference on latent variables
as opposed to observations  which is a much greater undertaking that involves approximations  and
we are doing so in a different context.

3 Gaussian Copula Process Volatility
Assume we have a sequence of observations y = (y1  . . .   yn)(cid:62) at times t = (t1  . . .   tn)(cid:62). The
observations are random variables with different latent standard deviations. We therefore have n
unobserved standard deviations  σ1  . . .   σn  and want to learn the correlation structure between
these standard deviations  and also to predict the distribution of σ∗ at some unrealised time t∗.
We model the standard deviation function as a Gaussian copula process:

σt ∼ GCP(g−1  0  k(t  t(cid:48))).

(5)

Speciﬁcally 

f (t) ∼ GP(m(t) = 0  k(t  t(cid:48)))
σ(t) = g(f (t)  ω)
y(t) ∼ N (0  σ2(t)) 

(6)
(7)
(8)
where g is a monotonic warping function  parametrized by ω. For each of the observations y =
(y1  . . .   yn)(cid:62) we have corresponding GP latent function values f = (f1  . . .   fn)(cid:62)  where σ(ti) =
g(fi  ω)  using the shorthand fi to mean f (ti).
σt ∼ GCP  because any ﬁnite sequence (σ1  . . .   σp) is distributed as a Gaussian copula:
P (σ1 ≤ a1  . . .   σp ≤ ap) = P (g−1(σ1) ≤ g−1(a1)  . . .   g−1(σp) ≤ g−1(ap))
= ΦΓ(g−1(a1)  . . .   g−1(ap)) = ΦΓ(Φ−1(F (a1))  . . .   Φ−1(F (ap)))
= ΦΓ(Φ−1(u1)  . . .   Φ−1(up)) = C(u1  . . .   up) 

(9)

where Φ is the standard univariate Gaussian cdf (supposing k(t  t) = 1)  ΦΓ is a multivariate Gaus-
sian cdf with covariance matrix Γij = cov(g−1(σi)  g−1(σj))  and F is the marginal distribution of

3

each σi. In (5)  we have Ψ = g−1  because it is g−1 which maps σt to a GP. The speciﬁcation in
(5) is equivalently expressed by (6) and (7). With GCPV  the form of g is learned so that g−1(σt)
is best modelled by a GP. By learning g  we learn the marginal of each σ: F (a) = Φ(g−1(a)) for
a ∈ R. Recently  a different sort of ‘kernel copula process’ has been used  where the marginals
of the variables being modelled are not learned [16].1 Further  we also consider a more subtle and
ﬂexible form of our model  where the function g itself is indexed by time: g = gt(f (t)  ω). We only
assume that the marginal distributions of σt are stationary over ‘small’ time periods  and for each of
these time periods (5)-(7) hold true. We return to this in the ﬁnal discussion section.
Here we have assumed that each observation  conditioned on knowing its variance  is normally
distributed with zero mean. This is a common assumption in heteroscedastic models. The zero
mean and normality assumptions can be relaxed and are not central to this paper.

4 Predictions with GCPV
Ultimately  we wish to infer p(σ(t∗)|y  z)  where z = {θ  ω}  and θ are the hyperparameters of the
GP covariance function. To do this  we sample from

p(f∗|y  z) =

p(f∗|f   θ)p(f|y  z)df

(10)

(cid:90)

and then transform these samples by g. Letting (Cf )ij = δijg(fi  ω)2  where δij is the Kronecker
delta  Kij = k(ti  tj)  (k∗)i = k(t∗  ti)  we have

p(f|y  z) = N (f ; 0  K)N (y; 0  Cf )/p(y|z) 
∗ K−1k∗).

p(f∗|f   θ) = N (k(cid:62)

∗ K−1f   k(t∗  t∗) − k(cid:62)
(cid:90)

p(y|f   ω)p(f|θ)df .

p(y|z) =

(11)
(12)

(13)

We also wish to learn z  which we can do by ﬁnding the ˆz that maximizes the marginal likelihood 

Unfortunately  for many functions g  (10) and (13) are intractable. Our methods of dealing with
this can be used in very general circumstances  where one has a Gaussian process prior  but an
(optionally parametrized) non-Gaussian likelihood. We use the Laplace approximation to estimate
p(f|y  z) as a Gaussian. Then we can integrate (10) for a Gaussian approximation to p(f∗|y  z) 
which we sample from to make predictions of σ∗. Using Laplace  we can also ﬁnd an expression
for an approximate marginal likelihood  which we maximize to determine z. Once we have found
z with Laplace  we use Markov chain Monte Carlo to sample from p(f∗|y  z)  and compare that to
using Laplace to sample from p(f∗|y  z). In the supplement we relate this discussion to (9).

4.1 Laplace Approximation

The goal is to approximate (11) with a Gaussian  so that we can evaluate (10) and (13) and make
predictions. In doing so  we follow Rasmussen and Williams [11] in their treatment of Gaussian
process classiﬁcation  except we use a parametrized likelihood  and modify Newton’s method.
First  consider as an objective function the logarithm of an unnormalized (11):

s(f|y  z) = log p(y|f   ω) + log p(f|θ).

(14)

The Laplace approximation uses a second order Taylor expansion about the ˆf which maximizes
(14)  to ﬁnd an approximate objective ˜s(f|y  z). So the ﬁrst step is to ﬁnd ˆf  for which we use
Newton’s method. The Newton update is f new = f − (∇∇s(f ))−1∇s(f ). Differentiating (14) 

∇s(f|y  z) = ∇ log p(y|f   ω) − K−1f
∇∇s(f|y  z) = ∇∇ log p(y|f   ω) − K−1 = −W − K−1 

(15)
(16)

where W is the diagonal matrix −∇∇ log p(y|f   ω).

1Note added in proof : Also  for a very recent related model  see Rodr´ıguez et al. [17].

4

If the likelihood function p(y|f   ω) is not log concave  then W may have negative entries. Vanhat-
alo et al. [18] found this to be problematic when doing Gaussian process regression with a Student-t
likelihood. They instead use an expectation-maximization (EM) algorithm for ﬁnding ˆf  and iterate
ordered rank one Cholesky updates to evaluate the Laplace approximate marginal likelihood. But
EM can converge slowly  especially near a local optimum  and each of the rank one updates is vul-
nerable to numerical instability. With a small modiﬁcation of Newton’s method  we often get close to
quadratic convergence for ﬁnding ˆf  and can evaluate the Laplace approximate marginal likelihood
in a numerically stable fashion  with no approximate Cholesky factors  and optimal computational
requirements. Some comments are in the supplementary material but  in short  we use an approxi-
mate negative Hessian  −∇∇s ≈ M + K−1  which is guaranteed to be positive deﬁnite  since M
is formed on each iteration by zeroing the negative entries of W . For stability  we reformulate our
2   b = M f + ∇ log p(y|f ) 
optimization in terms of B = I + M 1
a = b − QKb. Since (K−1 + M )−1 = K − KQK  the Newton update becomes f new = Ka.
With these updates we ﬁnd ˆf and get an expression for ˜s which we use to approximate (13) and

(11). The approximate marginal likelihood q(y|z) is given by(cid:82) exp(˜s)df. Taking its logarithm 

2   and let Q = M 1

2 B−1M 1

2 KM 1

log q(y|z) = − 1
2

ˆf(cid:62)a ˆf + log p(y| ˆf ) − 1
2

log |B ˆf| 

(17)

where B ˆf is B evaluated at ˆf  and a ˆf is a numerically stable evaluation of K−1 ˆf.
To learn the parameters z  we use conjugate gradient descent to maximize (17) with respect to z.
Since ˆf is a function of z  we initialize z  and update ˆf every time we vary z. Once we have found
an optimum ˆz  we can make predictions. By exponentiating ˜s  we ﬁnd a Gaussian approximation to
the posterior (11)  q(f|y  z) = N ( ˆf   K − KQK). The product of this approximate posterior with
p(f∗|f ) is Gaussian. Integrating this product  we approximate p(f∗|y  z) as
∗ Qk∗).

(18)
Given n training observations  the cost of each Newton iteration is dominated by computing the
cholesky decomposition of B  which takes O(n3) operations. The objective function typically
changes by less than 10−6 after 3 iterations. Once Newton’s method has converged  it takes only
O(1) operations to draw from q(f∗|y  z) and make predictions.

∗ ∇ log p(y| ˆf )  k(t∗  t∗) − k(cid:62)

q(f∗|y  z) = N (k(cid:62)

4.2 Markov chain Monte Carlo

We use Markov chain Monte Carlo (MCMC) to sample from (11)  so that we can later sample from
p(σ∗|y  z) to make predictions. Sampling from (11) is difﬁcult  because the variables f are strongly
coupled by a Gaussian process prior. We use a new technique  Elliptical Slice Sampling [19]  and
ﬁnd it extremely effective for this purpose. It was speciﬁcally designed to sample from posteriors
with correlated Gaussian priors. It has no free parameters  and jointly updates every element of f.
For our setting  it is over 100 times as fast as axis aligned slice sampling with univariate updates.
To make predictions  we take J samples of p(f|y  z)  {f 1  . . .   f J}  and then approximate (10) as
a mixture of J Gaussians:

p(f∗|y  z) ≈ 1
J

(19)
Each of the Gaussians in this mixture have equal weight. So for each sample of f∗|y  we uniformly
choose a random p(f∗|f i  θ) and draw a sample. In the limit J → ∞  we are sampling from the
exact p(f∗|y  z). Mapping these samples through g gives samples from p(σ∗|y  z). After one O(n3)
and one O(J) operation  a draw from (19) takes O(1) operations.

i=1

J(cid:88)

p(f∗|f i  θ).

4.3 Warping Function

The warping function  g  maps fi  a GP function value  to σi  a standard deviation. Since fi can take
any value in R  and σi can take any non-negative real value  g : R → R+. For each fi to correspond
to a unique deviation  g must also be one-to-one. We use

g(x  ω) =

aj log[exp[bj(x + cj)] + 1] 

aj  bj > 0.

(20)

K(cid:88)

j=1

5

tends to ((cid:80)K

This is monotonic  positive  inﬁnitely differentiable  asymptotic towards zero as x → −∞  and
j=1 ajbj)x as x → ∞. In practice  it is useful to add a small constant to (20)  to avoid
rare situations where the parameters ω are trained to make g extremely small for certain inputs  at
the expense of a good overall ﬁt; this can happen when the parameters ω are learned by optimizing
a likelihood. A suitable constant could be one tenth the absolute value of the smallest nonzero
observation.
By inferring the parameters of the warping function  or distributions of these parameters  we are
learning a transformation which will best model σt with a Gaussian process. The more ﬂexible the
warping function  the more potential there is to improve the GCPV ﬁt – in other words  the better
we can estimate the ‘perfect’ transformation. To test the importance of this ﬂexibility  we also try
a simple unparametrized warping function  g(x) = ex. In related work  Goldberg et al. [20] place
a GP prior on the log noise level in a standard GP regression model on observations  except for
inference they use Gibbs sampling  and a high level of ‘jitter’ for conditioning.
Once g is trained  we can infer the marginal distribution of each σ: F (a) = Φ(g−1(a))  for a ∈ R.
This suggests an alternate way to initialize g: we can initialize F as a mixture of Gaussians  and
then map through Φ−1 to ﬁnd g−1. Since mixtures of Gaussians are dense in the set of probability
distributions  we could in principle ﬁnd the ‘perfect’ g using an inﬁnite mixture of Gaussians [21].

5 Experiments

(cid:80)q

j=1 bjσ2

i=1 aiy2

In our experiments  we predict the latent standard deviations σ of observations y at times t  and
also σ∗ at unobserved times t∗. To do this  we use two versions of GCPV. The ﬁrst variant  which
we simply refer to as GCPV  uses the warping function (20) with K = 1  and squared exponential
covariance function  k(t  t(cid:48)) = A exp(−(t−t(cid:48))2/l2)  with A = 1. The second variant  which we call
GP-EXP  uses the unparametrized warping function ex  and the same covariance function  except
the amplitude A is a trained hyperparameter. The other hyperparameter l is called the lengthscale of
the covariance function. The greater l  the greater the covariance between σt and σt+a for a ∈ R.
We train hyperparameters by maximizing the Laplace approximate log marginal likelihood (17).
We then sample from p(f∗|y) using the Laplace approximation (18). We also do this using MCMC
(19) with J = 10000  after discarding a previous 10000 samples of p(f|y) as burn-in. We pass
these samples of f∗|y through g and g2 to draw from p(σ∗|y) and p(σ2∗|y)  and compute the sample
mean and variance of σ∗|y. We use the sample mean as a point predictor  and the sample variance
for error bounds on these predictions  and we use 10000 samples to compute these quantities. For
GCPV we use Laplace and MCMC for inference  but for GP-EXP we only use Laplace. We compare
predictions to GARCH(1 1)  which has been shown in extensive and recent reviews to be competitive
with other GARCH variants  and more sophisticated models [5  6  7]. GARCH(p q) speciﬁes y(t) ∼
N (0  σ2(t))  and lets the variance be a deterministic function of the past: σ2
t−i +
t−j. We use the Matlab Econometrics Toolbox implementation of GARCH  where the

t = a0 +(cid:80)p

parameters a0  ai and bj are estimated using a constrained maximum likelihood.
We make forecasts of volatility  and we predict historical volatility. By ‘historical volatility’ we
mean the volatility at observed time points  or between these points. Uncovering historical volatility
is important. It could  for instance  be used to study what causes ﬂuctuations in the stock market  or
to understand physical systems.
To evaluate our model  we use the Mean Squared Error (MSE) between the true variance  or proxy
for the truth  and the predicted variance. Although likelihood has advantages  we are limited in
space  and we wish to harmonize with the econometrics literature  and other assessments of volatility
models  where MSE is the standard. In a similar assessment of volatility models  Brownlees et al.
[7] found that MSE and quasi-likelihood rankings were comparable.
When the true variance is unknown we follow Brownlees et al. [7] and use squared observations
as a proxy for the truth  to compare our model to GARCH.2 The more observations  the more
reliable these performance estimates will be. However  not many observations (e.g. 100) are needed
for a stable ranking of competing models; in Brownlees et al. [7]  the rankings derived from high
frequency squared observations are similar to those derived using daily squared observations.

2Since each observation y is assumed to have zero mean and variance σ2  E[y2] = σ2.

6

5.1 Simulations
We simulate observations from N (0  σ2(t))  using σ(t) = sin(t) cos(t2) + 1  at t =
(0  0.02  0.04  . . .   4)(cid:62). We call this data set TRIG. We also simulate using a standard deviation
that jumps from 0.1 to 7 and back  at times t = (0  0.1  0.2  . . .   6)(cid:62). We call this data set JUMP.
To forecast  we use all observations up until the current time point  and make 1  7  and 30 step
ahead predictions. So  for example  in TRIG we start by observing t = 0  and make forecasts at
t = 0.02  0.14  0.60. Then we observe t = 0  0.02 and make forecasts at t = 0.04  0.16  0.62  and
so on  until all data points have been observed. For historical volatility  we predict the latent σt at
the observation times  which is safe since we are comparing to the true volatility  which is not used
in training; the results are similar if we interpolate. Figure 1 panels a) and b) show the true volatil-
ity for TRIG and JUMP respectively  alongside GCPV Laplace  GCPV MCMC  GP-EXP Laplace 
and GARCH(1 1) predictions of historical volatility. Table 1 shows the results for forecasting and
historical volatility.
In panel a) we see that GCPV more accurately captures the dependencies between σ at different
times points than GARCH: if we manually decrease the lengthscale in the GCPV covariance func-
tion  we can replicate the erratic GARCH behaviour  which inaccurately suggests that the covariance
between σt and σt+a decreases quickly with increases in a. We also see that GCPV with an un-
parametrized exponential warping function tends to overestimates peaks and underestimate troughs.
In panel b)  the volatility is extremely difﬁcult to reconstruct or forecast – with no warning it will
immediately and dramatically increase or decrease. This behaviour is not suited to a smooth squared
exponential covariance function. Nevertheless  GCPV outperforms GARCH  especially in regions
of low volatility. We also see this in panel a) for t ∈ (1.5  2). GARCH is known to respond slowly
to large returns  and to overpredict volatility [22]. In JUMP  the greater the peaks  and the smaller
the troughs  the more GARCH suffers  while GCPV is mostly robust to these changes.

5.2 Financial Data

The returns on the daily exchange rate between the Deutschmark (DM) and the Great Britain
Pound (GBP) from 1984 to 1992 have become a benchmark for assessing the performance of
GARCH models [8  9  10]. This exchange data  which we refer to as DMGBP  can be obtained
from www.datastream.com  and the returns are calculated as rt = log(Pt+1/Pt)  where Pt is
the number of DM to GBP on day t. The returns are assumed to have a zero mean function.
We use a rolling window of the previous 120 days of returns to make 1  7  and 30 day ahead volatility
forecasts  starting at the beginning of January 1988  and ending at the beginning of January 1992
(659 trading days). Every 7 days  we retrain the parameters of GCPV and GARCH. Every time
we retrain parameters  we predict historical volatility over the past 120 days. The average MSE
for these historical predictions is given in Table 1  although they should be observed with caution;
unlike with the simulations  the DMGBP historical predictions are trained using the same data they
are assessed on. In Figure 1c)  we see that the GARCH one day ahead forecasts are lifted above
the GCPV forecasts  but unlike in the simulations  they are now operating on a similar lengthscale.
This suggests that GARCH could still be overpredicting volatility  but that GCPV has adapted its
estimation of how σt and σt+a correlate with one another. Since GARCH is suited to this ﬁnancial
data set  it is reassuring that GCPV predictions have a similar time varying structure. Overall  GCPV
and GARCH are competitive with one another for forecasting currency exchange returns  as seen
in Table 1. Moreover  a learned warping function g outperforms an unparametrized one  and a full
Laplace solution is comparable to using MCMC for inference  in accuracy and speed. This is also
true for the simulations. Therefore we recommend whichever is more convenient to implement.

6 Discussion

We deﬁned a copula process  and as an example  developed a stochastic volatility model  GCPV 
which can outperform GARCH. With GCPV  the volatility σt is distributed as a Gaussian Copula
Process  which separates the modelling of the dependencies between volatilities at different times
from their marginal distributions – arguably the most useful property of a copula. Further  GCPV ﬁts
the marginals in the Gaussian copula process by learning a warping function. If we had simply cho-
sen an unparametrized exponential warping function  we would incorrectly be assuming that the log

7

Data set
TRIG

JUMP
×103

DMGBP
×10−9

Table 1: MSE for predicting volatility.

Model Historical

1 step

7 step

30 step

GCPV (LA)
GCPV (MCMC)
GP-EXP
GARCH
GCPV (LA)
GCPV (MCMC)
GP-EXP
GARCH
GCPV (LA)
GCPV (MCMC)
GP-EXP
GARCH

0.0953
0.0760
0.193
0.938

0.588
1.21
1.43
1.88

2.43
2.39
2.52
2.83

0.588
0.622
0.646
1.04

0.891
0.951
1.76
1.58

3.00
3.00
3.20
3.03

0.951
0.979
1.36
1.79

1.38
1.37
6.95
3.43

3.08
3.08
3.46
3.12

1.71
1.76
1.15
5.12

1.35
1.35
14.7
5.65

3.17
3.17
5.14
3.32

Figure 1: Predicting volatility and learning its marginal pdf. For a) and b)  the true volatility  and GCPV
(MCMC)  GCPV (LA)  GP-EXP  and GARCH predictions  are shown respectively by a thick green line  a
dashed thick blue line  a dashed black line  a cyan line  and a red line. a) shows predictions of historical
volatility for TRIG  where the shade is a 95% conﬁdence interval about GCPV (MCMC) predictions. b) shows
predictions of historical volatility for JUMP. In c)  a black line and a dashed red line respectively show GCPV
(LA) and GARCH one day ahead volatility forecasts for DMGBP. In d)  a black line and a dashed blue line
respectively show the GCPV learned marginal pdf of σt in DMGBP and a Gamma(4.15 0.00045) pdf.

volatilities are marginally Gaussian distributed. Indeed  for the DMGBP data  we trained the warping
function g over a 120 day period  and mapped its inverse through the univariate standard Gaussian
cdf Φ  and differenced  to estimate the marginal probability density function (pdf) of σt over this
period. The learned marginal pdf  shown in Figure 1d)  is similar to a Gamma(4.15 0.00045) distri-
bution. However  in using a rolling window to retrain the parameters of g  we do not assume that the
marginals of σt are stationary; we have a time changing warping function.
While GARCH is successful  and its simplicity is attractive  our model is also simple and has a
number of advantages. We can effortlessly handle missing data  we can easily incorporate covariates
other than time (like interest rates) in our covariance function  and we can choose from a rich class
of covariance functions – squared exponential  Brownian motion  Mat´ern  periodic  etc. In fact  the
volatility of high frequency intradaily returns on equity indices and currency exchanges is cyclical
[23]  and GCPV with a periodic covariance function is uniquely well suited to this data. And the
parameters of GCPV  like the covariance function lengthscale  or the learned warping function 
provide insight into the underlying source of volatility  unlike the parameters of GARCH.
Finally  copulas are rapidly becoming popular in applications  but often only bivariate copulas are
being used. With our copula process one can learn the dependencies between arbitrarily many ran-
dom variables independently of their marginal distributions. We hope the Gaussian Copula Process
Volatility model will encourage other applications of copula processes. More generally  we hope
our work will help bring together the machine learning and econometrics communities.
Acknowledgments: Thanks to Carl Edward Rasmussen and Ferenc Husz´ar for helpful conversa-
tions. AGW is supported by an NSERC grant.

8

012340123Time(a)VolatilityTRIG 024605101520Time(b)VolatilityJUMP 020040060000.0050.010.015Days(c)VolatilityDMGBP 00.0050.010200400600σ(d)Probability DensityDMGBP References
[1] Paul Embrechts  Alexander McNeil  and Daniel Straumann. Correlation and dependence in risk manage-
ment: Properties and pitfalls. In Risk Management: Value at risk and beyond  pages 176–223. Cambridge
University Press  1999.

[2] David X. Li. On default correlation: A copula function approach. Journal of Fixed Income  9(4):43–54 

2000.

[3] Roger B. Nelsen. An Introduction to Copulas. Springer Series in Statistics  second edition  2006.
[4] Tim Bollerslev. Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics  31

(3):307–327  1986.

[5] Ser-Huang Poon and Clive W.J. Granger. Practical issues in forecasting volatility. Financial Analysts

Journal  61(1):45–56  2005.

[6] Peter Reinhard Hansen and Asger Lunde. A forecast comparison of volatility models: Does anything

beat a GARCH(1 1). Journal of Applied Econometrics  20(7):873–889  2005.

[7] Christian T. Brownlees  Robert F. Engle  and Bryan T. Kelly. A practical guide to volatility forecasting

through calm and storm  2009. Available at SSRN: http://ssrn.com/abstract=1502915.

[8] T. Bollerslev and E. Ghysels. Periodic autoregressive conditional heteroscedasticity. Journal of Business

and Economic Statistics  14:139–151  1996.

[9] B.D. McCullough and C.G. Renfro. Benchmarks and software standards: A case study of GARCH

procedures. Journal of Economic and Social Measurement  25:59–71  1998.

[10] C. Brooks  S.P. Burke  and G. Persand. Benchmarks and the accuracy of GARCH model estimation.

International Journal of Forecasting  17:45–56  2001.

[11] Carl Edward Rasmussen and Christopher K.I. Williams. Gaussian processes for Machine Learning. The

MIT Press  2006.

[12] Abe Sklar. Fonctions de r´epartition `a n dimensions et leurs marges. Publ. Inst. Statist. Univ. Paris  8:

229–231  1959.

[13] P Deheuvels. Caract´eisation compl`ete des lois extrˆemes multivari´es et de la convergence des types

extrˆemes. Publications de l’Institut de Statistique de l’Universit´e de Paris  23:1–36  1978.

[14] G Kimeldorf and A Sampson. Uniform representations of bivariate distributions. Communications in

Statistics  4:617–627  1982.

[15] Edward Snelson  Carl Edward Rasmussen  and Zoubin Ghahramani. Warped Gaussian Processes. In

NIPS  2003.

[16] Sebastian Jaimungal and Eddie K.H. Ng. Kernel-based Copula processes. In ECML PKDD  2009.
[17] A. Rodr´ıguez  D.B. Dunson  and A.E. Gelfand. Latent stick-breaking processes. Journal of the American

Statistical Association  105(490):647–659  2010.

[18] Jarno Vanhatalo  Pasi Jylanki  and Aki Vehtari. Gaussian process regression with Student-t likelihood. In

NIPS  2009.

[19] Iain Murray  Ryan Prescott Adams  and David J.C. MacKay. Elliptical Slice Sampling. In AISTATS 

2010.

[20] Paul W. Goldberg  Christopher K.I. Williams  and Christopher M. Bishop. Regression with input-

dependent noise: A Gaussian process treatment. In NIPS  1998.

[21] Carl Edward Rasmussen. The Inﬁnite Gaussian Mixture Model. In NIPS  2000.
[22] Ruey S. Tsay. Analysis of Financial Time Series. John Wiley & Sons  2002.
[23] Torben G. Andersen and Tim Bollerslev. Intraday periodicity and volatility persistence in ﬁnancial mar-

kets. Journal of Empirical Finance  4(2-3):115–158  1997.

9

,Leonid Boytsov
Bilegsaikhan Naidan
Julien Mairal