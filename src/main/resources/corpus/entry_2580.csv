2015,Improved Iteration Complexity Bounds of Cyclic Block Coordinate Descent for Convex Problems,The iteration complexity of the block-coordinate descent (BCD) type algorithm has been under extensive investigation. It was recently shown that for convex problems the classical cyclic BCGD (block coordinate gradient descent) achieves an O(1/r) complexity (r is the number of passes of all blocks). However  such bounds are at least linearly depend on $K$ (the number of variable blocks)  and are at least $K$ times worse than those of the gradient descent (GD) and proximal gradient (PG) methods.In this paper  we close such theoretical performance gap between cyclic BCD and GD/PG. First we show that for a family of quadratic nonsmooth problems  the complexity bounds for cyclic Block Coordinate Proximal Gradient (BCPG)  a popular variant of BCD  can match those of the GD/PG in terms of dependency on $K$ (up to a \log^2(K) factor). Second  we establish an improved complexity bound for Coordinate Gradient Descent (CGD) for general convex problems which can match that of GD in certain scenarios. Our bounds are sharper than the known bounds as they are always at least $K$ times worse than GD. {Our analyses do not depend on the update order of block variables inside each cycle  thus our results also apply to BCD methods with random permutation (random sampling without replacement  another popular variant).,Improved Iteration Complexity Bounds of Cyclic
Block Coordinate Descent for Convex Problems

Ruoyu Sun∗  Mingyi Hong†‡

Abstract

The iteration complexity of the block-coordinate descent (BCD) type algorithm
has been under extensive investigation. It was recently shown that for convex
problems the classical cyclic BCGD (block coordinate gradient descent) achieves
an O(1/r) complexity (r is the number of passes of all blocks). However  such
bounds are at least linearly depend on K (the number of variable blocks)  and
are at least K times worse than those of the gradient descent (GD) and proximal
gradient (PG) methods. In this paper  we close such theoretical performance gap
between cyclic BCD and GD/PG. First we show that for a family of quadratic
nonsmooth problems  the complexity bounds for cyclic Block Coordinate Proxi-
mal Gradient (BCPG)  a popular variant of BCD  can match those of the GD/PG
(K) factor). Second  we establish an
in terms of dependency on K (up to a log
improved complexity bound for Coordinate Gradient Descent (CGD) for general
convex problems which can match that of GD in certain scenarios. Our bounds
are sharper than the known bounds as they are always at least K times worse than
GD. Our analyses do not depend on the update order of block variables inside
each cycle  thus our results also apply to BCD methods with random permutation
(random sampling without replacement  another popular variant).

2

1 Introduction
Consider the following convex optimization problem

K(cid:2)

min f (x) = g(x1 ···   xK ) +

hk(xk) 

s.t.

xk ∈ Xk  ∀ k = 1 ··· K 

(1)

k=1

where g : X → R is a convex smooth function; h : X → R is a convex lower semi-continuous
possibly nonsmooth function; xk ∈ Xk ⊆ RN is a block variable. A very popular method for
solving this problem is the so-called block coordinate descent (BCD) method [5]  where each time
a single block variable is optimized while the rest of the variables remain ﬁxed. Using the classical
cyclic block selection rule  the BCD method can be described below.

Algorithm 1: The Cyclic Block Coordinate Descent (BCD)

At each iteration r + 1  update the variable blocks by:

(cid:3)

(cid:4)

(r)
x
k

∈ min
xk∈Xk

g

xk  w

(r)−k

+ hk(xk)  k = 1 ···   K.

(2)

ruoyu@stanford.edu

∗Department of Management Science
and Engineering  Stanford University  Stanford  CA.
†Department of Industrial & Manufacturing Systems Engineering and Department of Electrical & Computer
‡The authors contribute equally to this work.

Engineering  Iowa State University  Ames  IA  mingyi@iastate.edu

1

where we have used the following short-handed notations:

(cid:6)

(cid:5)
(cid:5)

(r)

(cid:6)
k+1  ···   x
(r−1)
(r−1)
(r)
(r)
k−1  x
:=
w
k
k
k+1  ···   x
(r−1)
(r−1)
(r)
(r)−k :=
k−1  x
w
K
x−k := [x1 ···   xk−1  xk+1 ···   xK] .

1  ···   x
x
1  ···   x
x

  x

(r)

  k = 1 ···   K 

(r−1)
K
  k = 1 ···   K 

The convergence analysis of the BCD has been extensively studied in the literature  see [5  14 
19  15  4  7  6  10  20]. For example it is known that for smooth problems (i.e. f is continuous
differentiable but possibly nonconvex  h = 0)  if each subproblem has a unique solution and g is
non-decreasing in the interval between the current iterate and the minimizer of the subproblem (one
special case is per-block strict convexity)  then every limit point of {x(r)} is a stationary point [5 
Proposition 2.7.1]. The authors of [6  19] have derived relaxed conditions on the convergence of
BCD. In particular  when problem (1) is convex and the level sets are compact  the convergence of
the BCD is guaranteed without requiring the subproblems to have unique solutions [6]. Recently
Razaviyayn et al [15] have shown that the BCD converges if each subproblem (2) is solved inexactly 
by way of optimizing certain surrogate functions.
Luo and Tseng in [10] have shown that when problem (1) satisﬁes certain additional assumptions
such as having a smooth composite objective and a polyhedral feasible set  then BCD converges lin-
early without requiring the objective to be strongly convex. There are many recent works on showing
iteration complexity for randomized BCGD (block coordinate gradient descent)  see [17  12  8  16  9]
and the references therein. However the results on the classical cyclic BCD is rather scant. Saha
and Tewari [18] show that the cyclic BCD achieves sublinear convergence for a family of special
LASSO problems. Nutini et al [13] show that when the problem is strongly convex  unconstrained
and smooth  BCGD with certain Gauss-Southwell block selection rule could be faster than the ran-
domized rule. Recently Beck and Tetruashvili show that cyclic BCGD converges sublinearly if the
objective is smooth. Subsequently Hong et al in [7] show that such sublinear rate not only can
be extended to problems with nonsmooth objective  but is true for a large family of BCD-type al-
gorithm (with or without per-block exact minimization  which includes BCGD as a special case).
When each block is minimized exactly and when there is no per-block strong convexity  Beck [2]
proves the sublinear convergence for certain 2-block convex problem (with only one block having
Lipschitzian gradient). It is worth mentioning that all the above results on cyclic BCD can be used
to prove the complexity for a popular randomly permuted BCD in which the blocks are randomly
sampled without replacement.
(cid:7)
To illustrate the rates developed for the cyclic BCD algorithm  let us deﬁne X
solution set for problem (1)  and deﬁne the constant
(cid:6)x − x

(3)
Let us assume that hk(xk) ≡ 0  Xk = RN   ∀ k for now  and assume that g(·) has Lipschitz
continuous gradient:

(cid:8)
∗(cid:6) | f (x) ≤ f (x(0))

∗ to be the optimal

R0 := max
x∈X

max
x∗∈X ∗

.

(cid:6)∇g(x) − ∇g(z)(cid:6) ≤ L(cid:6)x − z(cid:6)  ∀ x  z ∈ X.

(4)

Also assume that g(·  x−k) has Lipschitz continuous gradient with respect to each xk  i.e. 

(cid:6)∇kg(xk  x−k) − ∇kg(vk  x−k)(cid:6) ≤ Lk(cid:6)xk − vk(cid:6)  ∀ x  v ∈ X  ∀ k.

(5)
Let Lmax := maxk Lk and Lmin := mink Lk. It is known that the cyclic BCPG has the following
iteration complexity [4  7] 1

BCD := f (x(r)) − f
Δ

(r)

∗ ≤ CLmax(1 + KL2/L2

min)R2
0

∀ r ≥ 1 

1
r

 

(6)

where C > 0 is some constant independent of problem dimension. Similar bounds are provided
for cyclic BCD in [7  Theorem 6.1]. In contrast  it is well known that when applying the classical

1Note that the assumptions made in [4] and [7] are slightly different  but the rates derived in both cases have

similar dependency on the problem dimension K.

2

gradient descent (GD) method to problem (1) with the constant stepsize 1/L  we have the following
rate estimate [11  Corollary 2.1.2]
GD := f (x(r)) − f (x
∗

) ≤ 2(cid:6)x(0) − x

∀ r ≥ 1  ∀ x

∗ ∈ X

∗(cid:6)2L

(7)

(r)

Δ

∗

.

 

≤ 2R2
0L
r + 4

r + 4

Note that unlike (6)  here the constant in front of the 1/(r + 4) term is independent of the problem
dimension. In fact  the ratio of the bound given in (6) and (7) is

CLmax

L

(1 + KL2/L2

min)

r + 4

r

which is at least in the order of K. For big data related problems with over millions of variables  a
multiplicative constant in the order of K can be a serious issue. In a recent work by Saha and Tewari
[18]  the authors show that for a LASSO problem with special data matrix  the rate of cyclic BCD
(with special initialization) is indeed K-independent. Unfortunately  such a result has not yet been
extended to any other convex problems. An open question posed by a few authors [4  3  18] are: is
such a K factor gap intrinsic to the cyclic BCD or merely an artifact of the existing analysis?

(cid:9)(cid:9)(cid:9)(cid:9)(cid:9) K(cid:2)

1
2

K(cid:2)

(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)2

2 Improved Bounds of Cyclic BCPG for Nonsmooth Quadratic Problem

In this section  we consider the following nonsmooth quadratic problem

Akxk − b

+

k=1

hk(xk) 

min f (x) :=

(8)
where Ak ∈ RM×N; b ∈ RM; xk ∈ RN is the kth block coordinate; hk(·) is the same as in
(1). Note the blocks are assumed to have equal dimension for simplicity of presentation. Deﬁne
A := [A1 ···   Ak] ∈ RM×KN. For simplicity  we have assumed that all the blocks have the same
size. Problem (8) includes for example LASSO and group LASSO as special cases.
We consider the following cyclic BCPG algorithm.

k=1

s.t. xk ∈ Xk  ∀ k

Algorithm 2: The Cyclic Block Coordinate Proximal Gradient (BCPG)

At each iteration r + 1  update the variable blocks by:

(r+1)
x
k

= arg min
xk∈Xk

(r+1)
g(w
k

) +

∇kg

(r+1)
w
k

  xk − x

(r)
k

+

Pk
2

(cid:11)

(cid:9)(cid:9)(cid:9)xk − x

(r)
k

(cid:9)(cid:9)(cid:9)2

+ hk(xk)
(9)

(cid:10)

(cid:3)

(cid:12)

(cid:4)

(cid:13)

Here Pk is the inverse of the stepsize for xk  which satisﬁes

Pk ≥ λmax

= Lk  ∀ k.

AT

k Ak

(10)
Deﬁne Pmax := maxk Pk and Pmin = mink Pk. Note that for the least square problem (smooth
quadratic minimization  i.e. hk ≡ 0 ∀ k)  BCPG reduces to the widely used BCGD method.
(cid:10)
The optimality condition for the kth subproblem is given by
∇kg(w
In what follows we show that the cyclic BCPG for problem (8) achieves a complexity bound that
only dependents on log2(N K)  and apart from such log factor it is at least K times better than those
known in the literature. Our analysis consists of the following three main steps:

) ≥ 0  ∀ xk ∈ Xk.

+ hk(xk) − hk(x

k )  xk − x

(r+1)
) + Pk(x
k

(r+1)
k

(r+1)
k

(r+1)
k

− x

(cid:11)

(11)

(r)

1. Estimate the descent of the objective after each BCPG iteration;
2. Estimate the cost yet to be minimized (cost-to-go) after each BCPG iteration;
3. Combine the above two estimates to obtain the ﬁnal bound.

First we show that the BCPG achieves the sufﬁcient descent.

3

Lemma 2.1. We have the following estimate of the descent when using the BCPG:

f (x(r)) − f (x(r+1)) ≥ K(cid:2)

Proof. We have the following series of inequalities

k=1

(cid:6)x

(r+1)
k

− x

(r)
k

(cid:6)2.

Pk
2

(12)

k=1

g(w

=

k=1

(r+1)

)

(r)

f (x

(r+1)
k

) +

(r+1)
k

(r+1)
k+1 )

) + hk(x

(r+1)
f (w
k

(r+1)
f (w
k

) − f (w
(cid:3)
)−

(cid:4)
∇kg(w
(cid:9)

) − f (x
K(cid:2)
≥ K(cid:2)
K(cid:2)
≥ K(cid:2)
To proceed  let us introduce two matrices (cid:14)P and (cid:14)A given below  which have dimension K × K and

where the second inequality uses the optimality condition (11).

(cid:5)
(cid:6)(cid:6)(cid:6)x

k ) − hk(x

− x
(cid:7)

(cid:6)(cid:6)(cid:6)x

Q.E.D.

(cid:3)(cid:4)

(cid:6)(cid:6)(cid:6)2

(cid:6)(cid:6)(cid:6)2

k (cid:4)2

∇kg

(r+1)
k

(r+1)
k

− x

  x

(r+1)
k

)  x

(r+1)
k

− x

(r)
k

− x

(r)
k

− x

(r)
k

(r+1)
k

(cid:4)x

(r+1)
k

(r+1)
k

) −

(r)

hk(x

+

Pk
2

+

Pk
2

(r+1)
k

(r)

.

(cid:8)

w

=

k=1

Pk
2

(cid:7)

(cid:5)

(r)
k

k=1

M K × N K  respectively

(cid:10)P :=

⎡
⎢⎢⎣ P1
...

0

0

0
P2

...

0

0
0

...

0

···
···
···
···

0
0

...

0
0

...

0 PK

(cid:14)P ⊗ IN (cid:12) (cid:14)A

0

0
A2

⎤
⎡
⎥⎥⎦   (cid:10)A :=
⎢⎢⎣ A1
...
T (cid:14)A (cid:12) AT A
T (cid:14)A  K(cid:14)A

...

0

0

···
···
···
···

0
0

...

0

0
0

...

0
0

...

0 AK

By utilizing the deﬁnition of Pk in (10) we have the following inequalities (the second inequality
comes from [12  Lemma 1])

where IN is the N × N identity matrix and the notation “⊗” denotes the Kronecker product.
Next let us estimate the cost-to-go.
Lemma 2.2. We have the following estimate of the optimality gap when using the BCPG:

(cid:3)
Δ(r+1) : = f (x(r+1)) − f (x
∗

(cid:15)

)

≤ R0log(2N K)

L/

Pmin +

Pmax

(cid:4)(cid:9)(cid:9)(cid:9)(cid:9)(x(r+1) − x(r))((cid:14)P

1/2 ⊗ IN )

(13)

(14)

Our third step combines the previous two steps and characterizes the iteration complexity. This is
the main result of this section.
Theorem 2.1. The iteration complexity of using BCPG to solve (8) is given below.

(cid:17) R2

Δ(r+1) ≤ 3 max

2. When the stepsizes are chosen as Pk = λmax(AT

1. When the stepsizes are chosen conservatively as Pk = L  ∀ k  we have
(cid:20)(cid:21)
(cid:20)(cid:21)

(cid:19)
k Ak) = Lk  ∀ k. Then we have
(cid:19)

(16)
In particular  if the problem is smooth and unconstrained  i.e.  when h ≡ 0  and Xk =
RN  ∀ k  then we have

Δ(r+1) ≤ 3 max

Δ0  4 log2(2N K)L

Δ0  2 log2(2N K)

(cid:18)
(cid:18)

R2
0
r + 1

L2
Lmin

Lmax +

(15)

r + 1

0

Δ(r+1) ≤ 3 max

L2
Lmin

R2
0
r + 1

.

(17)

⎤
⎥⎥⎦ .

(cid:9)(cid:9)(cid:9)(cid:9)

(cid:15)

(cid:16)

L  2 log2(2N K)

Lmax +

4

2

We comment on the bounds derived in the above theorem. The bound for BCPG with uniform
“conservative” stepsize 1/L has the same order as the GD method  except for the log
(2N K) factor
(cf. (7)). In [4  Corollary 3.2]  it is shown that the BCGD with the same “conservative” stepsize
0  which is about K/(3 log2(2N K)) times
achieves a sublinear rate with a constant of 4L(1 + K)R2
worse than our bound. Further  our bound has the same dependency on L (i.e.  12L v.s. L/2) as
the one derived in [18] for BCPG with a “conservative” stepsize to solve an (cid:3)1 penalized quadratic
problem with special data matrix  but our bound holds true for a much larger class of problems (i.e. 
all quadratic nonsmooth problem in the form of (8)). However  in practice such conservative stepsize
is slow (compared with BCPG with Pk = Lk  for all k) hence is rarely used.
The rest of the bounds derived in Theorem 2.1 is again at least K/ log2(2N K) times better than
existing bounds of cyclic BCPG. For example  when the problem is smooth and unconstrained  the
ratio between our bound (17) and the bound (6) is given by
6R2

≤ 6 log2(2N K)(1 + L2/(LminLmax))

2

0 log2(2N K)(L2/Lmin + Lmax)
CLmax(1 + KL2/L2

min)R2
0

C(1 + KL2/L2

min)

= O(log

(2N K)/K)
(18)

where in the last inequality we have used the fact that Lmax/Lmin ≥ 1.
For unconstrained smooth problems  let us compare the bound derived in the second part of The-
orem 2.1 (stepsize Pk = Lk ∀k) with that of the GD (7). If L = KLk for all k (problem badly
conditioned)  our bound is about K log2(2N K) times worse than that of the GD. This indicates a
counter-intuitive phenomenon: by choosing conservative stepsize Pk = L ∀k the iteration complex-
ity of BCGD is K times better compared with choosing a more aggressive stepzise Pk = Lk ∀k. It
also indicates that the factor L/Lmin may hide an additional factor of K.

3 Iteration Complexity for General Convex Problems

In this section  we consider improved iteration complexity bounds of BCD for general unconstrained
smooth convex problems. We prove a general iteration complexity result  which includes a result of
Beck et al. [4] as a special case. Our analysis for the general case also applies to smooth quadratic
problems  but is very different from the analysis in previous sections for quadratic problems. For
simplicity  we only consider the case N = 1 (scalar blocks); the generalization to the case N > 1 is
left as future work.
(x).
Let us assume that the smooth objective g has second order derivatives Hij(x) := ∂ 2g
When each block is just a coordinate  we assume |Hij(x)| ≤ Lij ∀i  j. Then Li = Lii and
∂xi∂xj
Lij ≤ √
Lj. For unconstrained smooth convex problems with scalar block variables  the BCPG
iteration reduces to the following coordinate gradient descent (CGD) iteration:

(cid:15)

Li

(r)
x(r) = w
1

d1−→ w

(r)
2

d2−→ w

(r)

3 −→ . . . dK−→ w

(r)
K+1 = x(r+1) 

(19)

(r)
k

(r)
k

dk−→ w

(r)
k ) and w

(r)
k+1 means that w

(r)
k+1 is a linear combination of w

where dk = ∇kg(w
dkek (ek is the k-th block unit vector).
In the following theorem  we provide an iteration complexity bound for the general convex problem.
The proof framework follows the standard three-step approach that combines sufﬁcient descent and
cost-to-go estimate; nevertheless  the analysis of the sufﬁcient descent is very different from the
methods used in the previous sections. The intuition is that CGD can be viewed as an inexact
gradient descent method  thus the amount of descent can be bounded in terms of the norm of the full
gradient. It would be difﬁcult to further tighten this bound if the goal is to obtain a sufﬁcient descent
based on the norm of the full gradient. Having established the sufﬁcient descent in terms of the
full gradient ∇g(x(r))  we can easily prove the iteration complexity result  following the standard
analysis of GD (see  e.g. [11  Theorem 2.1.13]).
Theorem 3.1. For CGD with Pk ≥ Lmax ∀k  we have

and

(cid:19)

g(x(r)) − g(x
∗

) ≤ 2

Pmax +

  ∀ r ≥ 1.

R2
0
r

(20)

(cid:20)

k Lk)2}

(cid:22)
min{KL2  (
Pmin

5

k only differ by the k-th block  and ∇kg is Lipschitz continuous with

k+1 and wr

Proof. Since wr
Lipschitz constant Lk  we have 2
k+1) ≤g(wr

g(wr

(cid:15) +

(cid:6)wr

k+1 − wr

k

(cid:6)2

Lk
2

k

k+1 − wr
k)(cid:6)2

k)  wr
(cid:6)∇kg(wr
k)(cid:6)2 

2P 2
k
(cid:6)∇kg(wr

=g(wr
≤g(wr

k) + (cid:14)∇kg(wr
k) − 2Pk − Lk
k) − 1
2Pk
r(cid:2)
(cid:5)

[g(wr

k=1

k+1)] ≥ r(cid:2)

k=1

k = xr −
wr

1
P1

d1  . . .  

1

Pk−1

dk−1  0  . . .   0

 

(cid:6)∇kg(wr

k)(cid:6)2.

1
2Pk

(cid:6)T

where the last inequality is due to Pk ≥ Lk.
The amount of decrease can be estimated as

g(xr) − g(xr+1) =

k) − g(wr

Since

by the mean-value theorem  there must exist ξk such that
(cid:18)(cid:19)
k) = ∇(∇kg)(ξk) · (xr − wr
k)

(cid:17)
∇kg(xr) − ∇kg(wr
(cid:17)

∂2g

=

(ξk)  . . .  

∂xk∂x1
1√
P1

∂2g

∂xk∂xk−1
1√

Pk−1

=

Hk1(ξk)  . . .  

Hk k−1(ξk)  0  . . .   0

(ξk)  0  . . .   0

(cid:20)T

d1  . . .  

1

Pk−1

1
P1

(cid:18)(cid:19)

1√

P1

d1  . . .  

dk−1  0  . . .   0
1√

(cid:20)T

dK

 

PK

where Hij (x) = ∂ 2g
∂xi∂xj

(x) is the second order derivative of g. Then

∇kg(xr) = ∇kg(xr) − ∇kg(wr

(cid:17)
(cid:17)

=

=

1√
P1
1√
P1

k) + ∇kg(wr
k)
1√

Pk−1

Hk1(ξk)  . . .  

Hk k−1(ξk)  0  . . .   0

Hk1(ξk)  . . .  

1√

Pk−1

Hk k−1(ξk) 

(cid:18)(cid:19)

d1  . . .  

1√
P1

(cid:18)(cid:19)

1√

PK

1√
P1

d1  . . .  

√

Pk  0  . . .   0

(cid:20)T
dK
1√

PK

+ dk

(cid:20)T

dK

= vT

k d 

where we have deﬁned

d :=

vk :=

(cid:5)
(cid:23)

(cid:6)T
1(cid:15)

 

Pk−1

1√
P1

d1  . . .  

1√

PK

dK

1√
P1

Hk1(ξk)  . . .  

(cid:15)

(cid:24)

Pk  . . .   0

.

Hk k−1(ξk) 

Let

V :=

⎤
⎦ =

⎡

⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣

⎡
⎣vT

1
. . .
vT

K

√

P1

H21(ξ2)
H31(ξ3)

H41(ξ4)

...

1√
P1
1√
P1
1√
P1

1√
P1

√
0
P2

H32(ξ3)

H42(ξ4)

...

1√
P2
1√
P2

1√

P2

0
0
0

0
√
0
P3

1√

P3

1√

P3

H43(ξ4)

...

HK3(ξK)

. . .
. . .
. . .
...
...

. . .

1√

PK−1

HK1(ξK)

HK2(ξK)

2 A stronger bound is g(wr

k+1) ≤ g(wr

k) − 1

(cid:4)∇kg(wr

k)(cid:4)2  where ˆPk =

Pk ≤ 2Pk − Lk ≤ 2Pk  the improvement ratio of using this stronger bound is no more than a factor of 2.

2Pk

P 2
k

2Pk−Lk

0
...
HK K−1(ξK)

√

PK
(26)
≤ Pk  but since

(21)

(22)

(23)

(24)

(25)

⎤

⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦

0
0
0

0

...

6

(cid:2)

Therefore  we have
(cid:6)∇g(xr)(cid:6)2 =

(cid:6)∇kg(xr)(cid:6)2 (24)
=

k

Combining with (22)  we get

g(xr) − g(xr+1) ≥

(cid:2)
(cid:6)vT
k d(cid:6)2 = (cid:6)V d(cid:6)2 ≤ (cid:6)V (cid:6)2(cid:6)d(cid:6)2 = (cid:6)V (cid:6)2
(cid:2)

k

(cid:6)∇kg(wr

k)(cid:6)2 ≥ 1

2(cid:6)V (cid:6)2

(cid:6)∇g(xr)(cid:6)2.

1
2Pk

k

(cid:2)

k

(cid:6)∇kg(wr

k)(cid:6)2.

1
Pk

⎡
⎢⎢⎢⎢⎣

Let D (cid:2) Diag(P1  . . .   PK) and let H(ξ) be deﬁned as

H(ξ) :=

0

H21(ξ2)
H31(ξ3)

...

0
0

H32(ξ3)

...

0
0
0

...

. . .
. . .
. . .

Then V = D1/2 + H(ξ)D
(cid:6)V (cid:6)2 = (cid:6)D1/2 + H(ξ)D

HK1(ξK) HK2(ξK) HK3(ξK )
−1/2  which implies
−1/2(cid:6)2 ≤ 2((cid:6)D1/2(cid:6)2 + (cid:6)H(ξ)D

⎤
⎥⎥⎥⎥⎦ .

0
0
0

...

0

...
. . . HK K−1(ξK)

0
0
0
...

(cid:19)

−1/2(cid:6)2) ≤ 2

Pmax +

Plugging into (27)  we obtain

(27)

(28)

(cid:20)

.

(29)

(cid:6)H(ξ)(cid:6)2
Pmin

Pmax +

1
(cid:5)H(ξ)(cid:5)2
Pmin

g(x(r)) − g(x(r+1)) ≥ 1
2

(cid:6)∇g(x(r))(cid:6)2.
From the fact that Hkj(ξk) is a scalar bounded above by |Hkj (ξk)| ≤ Lkj ≤(cid:15)
(cid:2)
(cid:2)
LkLj ≤ (
(cid:2)

(30)
We provide the second bound of (cid:6)H(cid:6) below. Let Hk denote the k-th row of H  then (cid:6)Hk(cid:6) ≤ L.
Therefore  we have

|Hkj(ξk)|2 ≤
(cid:2)

(cid:6)H(cid:6)2 ≤ (cid:6)H(cid:6)2

LkLj  thus

(cid:2)

Lk)2.

F =

k<j

k<j

k

(cid:6)H(cid:6)2 ≤ (cid:6)H(cid:6)2

(cid:22)
Combining this bound and (30)  we obtain that (cid:6)H(cid:6)2 ≤ min{KL2  (
Denote ω = 1
2

  then (29) becomes

(cid:6)Hk(cid:6)2 ≤

F =

k

k

1

L2 = KL2.

Pmax+ β2
Pmin

k Lk)2} (cid:2) β2.

g(x(r)) − g(x(r+1)) ≥ ω(cid:6)∇g(x(r))(cid:6)2  ∀r.

(31)
This relation also implies g(x(r)) ≤ g(x(0))  thus by the deﬁnition of R0 in (3) we have (cid:6)x(r) −
∗(cid:6) ≤ R0. By the convexity of g and the Cauchy-Schwartz inequality  we have
x
∗(cid:15) ≤ (cid:6)∇g(x(r))(cid:6)R0.

) ≤ (cid:14)∇g(x(r))  x(r) − x

g(x(r)) − g(x
∗
Combining with (31)  we obtain

g(x(r)) − g(x(r+1)) ≥ ω
R2
0
Let Δ(r) = g(x(r)) − g(x
∗

)  we obtain

(g(x(r)) − g(x
∗

))2.

Then we have

Δ(r) − Δ(r+1) ≥ ω
R2
0

Δ(r).

1

Δ(r+1)

≥ 1

Δ(r) +

ω
R2
0

Δ(r)
Δ(r+1)

≥ 1

Δ(r) +

ω
R2
0

.

7

Summarizing the inequalities  we get

1

Δ(r+1)

≥ 1

Δ(0) +

ω
R2
0

(r + 1) ≥ ω
R2
0

(r + 1) 

which leads to

Δ(r+1) = g(x(r+1)) − g(x
∗

(cid:22)
where β2 = min{KL2  (

k Lk)2}. This completes the proof.

) ≤ 1
ω

R2
0
r + 1

= 2(Pmax +

β2
Pmin

R2
0
r + 1

 

)

Q.E.D.

(cid:19)

(cid:20)

Let us compare this bound with the bound derived in [4  Theorem 3.1] (replacing the denominator
r + 8/K by r)  which is

(32)

g(xr) − g(x
∗

) ≤ 4

Pmax +

Pmax
Pmin

KL2
Pmin

R2
r

.

(cid:22)

r )  while our bound is O( L

(cid:22)
k Lk)2}. Neither of the two bounds KL2 and (

(cid:22)
(cid:22)
  we
In our new bound  besides reducing the coefﬁcient from 4 to 2 and removing the factor Pmax
improve KL2 to min{KL2  (
Pmin
k Lk)2 implies
the other: when L = Lk ∀k the new bound (
k Lk)2 is K times larger; when L = KLk ∀k or
L = L1 > L2 = ··· = LK = 0 the new bound is K times smaller. In fact  when L = KLk ∀k 
our new bound is K times better than the bound in [4] for either Pk = Lk or Pk = L. For example 
when Pk = L ∀k  the bound in [4] becomes O( KL
r )  which matches GD
(listed in Table 1 below). Another advantage of the new bound (
k Lk)2 is that it does not increase
if we add an artiﬁcial block xK+1 and perform CGD for function ˜g(x  xk+1) = g(x); in contrast 
the existing bound KL2 will increase to (K + 1)L2  even though the algorithm does not change at
all.
We have demonstrated that our bound can match GD in some cases  but can possibly be K times
worse than GD. An interesting question is: for general convex problems can we obtain an O( L
r )
bound for cyclic BCGD  matching the bound of GD? Removing the K-factor in (32) will lead to an
O( L
r ) bound for conservative stepsize Pk = L no matter how large Lk and L are. We conjecture that
an O( L
r ) bound for cyclic BCGD cannot be achieved for general convex problems. That being said 
we point out that the iteration complexity of cyclic BCGD may depend on other intrinsic parameters
of the problem such as {Lk}k and  possibly  third order derivatives of g. Thus the question of ﬁnding
the best iteration complexity bound of the form O(h(K) L
r )  where h(K) is a function of K  may
not be the right question to ask for BCD type algorithms.
4 Conclusion
In this paper  we provide new analysis and improved complexity bounds for cyclic BCD-type meth-
ods. For convex quadratic problems  we show that the bounds are O( L
r )  which is independent of
K (except for a mild log2(2K) factor) and is about Lmax/L + L/Lmin times worse than those
for GD/PG. By a simple example we show that it is not possible to obtain an iteration complexity
O(L/(Kr)) for cyclic BCPG. For illustration  the main results of this paper in several simple set-
tings are summarized in the table below. Note that different ratios of L over Lk can lead to quite
different comparison.

Table 1: Comparison of Various Iteration Complexity Results

Lip-constant
1/Stepsize

Diagonal Hessian Li = L

Pi = L

Full Hessian Li = L
Large stepsize Pi = L

K

K

GD

Random BCGD
Cyclic BCGD [4]

Cyclic CGD  Cor 3.1
Cyclic BCGD (QP)

L/r
L/r
KL/r
KL/r

log2(2K)L/r

N/A

L/(Kr)
K 2L/r
KL/r

log2(2K)KL/r

8

Full Hessian Li = L
Small stepsize Pi = L

K

L/r
L/r
KL/r
L/r

log2(2K)L/r

References
[1] J. R. Angelos  C. C. Cowen  and S. K. Narayan. Triangular truncation and ﬁnding the norm of

a hadamard multiplier. Linear Algebra and its Applications  170:117 – 135  1992.

[2] A. Beck. On the convergence of alternating minimization with applications to iterative-
ly reweighted least squares and decomposition schemes. SIAM Journal on Optimization 
25(1):185–209  2015.

[3] A. Beck  E. Pauwels  and S. Sabach. The cyclic block coordinate gradient method for convex

optimization problems. 2015. Preprint  available on arXiv:1502.03716v1.

[4] A. Beck and L. Tetruashvili. On the convergence of block coordinate descent type methods.

SIAM Journal on Optimization  23(4):2037–2060  2013.

[5] D. P. Bertsekas. Nonlinear Programming  2nd ed. Athena Scientiﬁc  Belmont  MA  1999.
[6] L. Grippo and M. Sciandrone. On the convergence of the block nonlinear Gauss-Seidel method

under convex constraints. Operations Research Letters  26:127–136  2000.

[7] M. Hong  X. Wang  M. Razaviyayn  and Z.-Q. Luo. Iteration complexity analysis of block

coordinate descent methods. 2013. Preprint  available online arXiv:1310.6957.

[8] Z. Lu and L. Xiao. On the complexity analysis of randomized block-coordinate descent meth-

ods. 2013. accepted by Mathematical Programming.

[9] Z. Lu and L. Xiao. Randomized block coordinate non-monotone gradient method for a class

of nonlinear programming. 2013. Preprint.

[10] Z.-Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex
differentiable minimization. Journal of Optimization Theory and Application  72(1):7–35 
1992.

[11] Y. Nesterov. Introductory lectures on convex optimization: A basic course. Springer  2004.
[12] Y. Nesterov. Efﬁciency of coordiate descent methods on huge-scale optimization problems.

SIAM Journal on Optimization  22(2):341–362  2012.

[13] J. Nutini  M. Schmidt  I. H. Laradji  M. Friedlander  and H. Koepke. Coordinate descent
converges faster with the Gauss-Southwell rule than random selection. In the Proceeding of
the 30th International Conference on Machine Learning (ICML)  2015.

[14] M. J. D. Powell. On search directions for minimization algorithms. Mathematical Program-

ming  4:193–201  1973.

[15] M. Razaviyayn  M. Hong  and Z.-Q. Luo. A uniﬁed convergence analysis of block suc-
cessive minimization methods for nonsmooth optimization. SIAM Journal on Optimization 
23(2):1126–1153  2013.

[16] M. Razaviyayn  M. Hong  Z.-Q. Luo  and J. S. Pang. Parallel successive convex approxima-
tion for nonsmooth nonconvex optimization. In the Proceedings of the Neural Information
Processing (NIPS)  2014.

[17] P. Richt´arik and M. Tak´aˇc. Iteration complexity of randomized block-coordinate descent meth-

ods for minimizing a composite function. Mathematical Programming  144:1–38  2014.

[18] A. Saha and A. Tewari. On the nonasymptotic convergence of cyclic coordinate descent

method. SIAM Journal on Optimization  23(1):576–601  2013.

[19] P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimiza-

tion. Journal of Optimization Theory and Applications  103(9):475–494  2001.

[20] Y. Xu and W. Yin. A block coordinate descent method for regularized multiconvex optimiza-
tion with applications to nonnegative tensor factorization and completion. SIAM Journal on
Imaging Sciences  6(3):1758–1789  2013.

9

,Ruoyu Sun
Yonghong Luo
Xiangrui Cai
Ying ZHANG
Jun Xu
Yuan xiaojie