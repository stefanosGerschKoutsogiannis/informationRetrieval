2015,Biologically Inspired Dynamic Textures for Probing Motion Perception,Perception is often described as a predictive process based on an optimal inference with respect to a generative model. We study here the principled construction of a generative model specifically crafted to probe motion perception. In that context  we first provide an axiomatic  biologically-driven derivation of the model. This model synthesizes random dynamic textures which are defined by stationary Gaussian distributions obtained by the random aggregation of warped patterns. Importantly  we show that this model can equivalently be described as a stochastic partial differential equation. Using this characterization of motion in images  it allows us to recast motion-energy models into a principled Bayesian inference framework. Finally  we apply these textures in order to psychophysically probe speed perception in humans. In this framework  while the likelihood is derived from the generative model  the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion.,Biologically Inspired Dynamic Textures

for Probing Motion Perception

Jonathan Vacher

CNRS UNIC and Ceremade

Univ. Paris-Dauphine

75775 Paris Cedex 16  FRANCE

vacher@ceremade.dauphine.fr

Andrew Isaac Meso

Institut de Neurosciences de la Timone

UMR 7289 CNRS/Aix-Marseille Universit´e

13385 Marseille Cedex 05  FRANCE

andrew.meso@univ-amu.fr

Laurent Perrinet

Institut de Neurosciences de la Timone

UMR 7289 CNRS/Aix-Marseille Universit´e

13385 Marseille Cedex 05  FRANCE

laurent.perrinet@univ-amu.fr

Gabriel Peyr´e

CNRS and Ceremade
Univ. Paris-Dauphine

75775 Paris Cedex 16  FRANCE

peyre@ceremade.dauphine.fr

Abstract

Perception is often described as a predictive process based on an optimal inference
with respect to a generative model. We study here the principled construction
of a generative model speciﬁcally crafted to probe motion perception.
In that
context  we ﬁrst provide an axiomatic  biologically-driven derivation of the model.
This model synthesizes random dynamic textures which are deﬁned by stationary
Gaussian distributions obtained by the random aggregation of warped patterns.
Importantly  we show that this model can equivalently be described as a stochastic
partial differential equation. Using this characterization of motion in images  it
allows us to recast motion-energy models into a principled Bayesian inference
framework. Finally  we apply these textures in order to psychophysically probe
speed perception in humans. In this framework  while the likelihood is derived
from the generative model  the prior is estimated from the observed results and
accounts for the perceptual bias in a principled fashion.

1 Motivation

A normative explanation for the function of perception is to infer relevant hidden parameters from
the sensory input with respect to a generative model [7]. Equipped with some prior knowledge
about this representation  this corresponds to the Bayesian brain hypothesis  as has been perfectly
illustrated by the particular case of motion perception [19]. However  the Gaussian hypothesis
related to the parameterization of knowledge in these models —for instance in the formalization
of the prior and of the likelihood functions— does not always ﬁt with psychophysical results [17].
As such  a major challenge is to reﬁne the deﬁnition of generative models so that they conform to
the widest variety of results.
From this observation  the estimation problem inherent to perception is linked to the deﬁnition of an
adequate generative model. In particular  the simplest generative model to describe visual motion
It states that luminance I(x  t) for (x  t) ∈ R2 × R is
is the luminance conservation equation.
approximately conserved along trajectories deﬁned as integral lines of a vector ﬁeld v(x  t) ∈ R2 ×
R. The corresponding generative model deﬁnes random ﬁelds as solutions to the stochastic partial
differential equation (sPDE) 

= W 

(1)

∂I
∂t

(cid:104)v  ∇I(cid:105) +

1

where (cid:104)·  ·(cid:105) denotes the Euclidean scalar product in R2  ∇I is the spatial gradient of I. To match
the statistics of natural scenes or some category of textures  the driving term W is usually deﬁned
as a colored noise corresponding to some average spatio-temporal coupling  and is parameterized
by a covariance matrix Σ  while the ﬁeld is usually a constant vector v(x  t) = v0 accounting for a
full-ﬁeld translation with constant speed.
Ultimately  the application of this generative model is essential for probing the visual system  for
instance to understand how observers might detect motion in a scene. Indeed  as shown by [9  19] 
the negative log-likelihood corresponding to the luminance conservation model (1) and deter-
mined by a hypothesized speed v0 is proportional to the value of the motion-energy model [1]
||(cid:104)v0  ∇(K (cid:63) I)(cid:105) + ∂(K(cid:63)I)
||2  where K is the whitening ﬁlter corresponding to the inverse of Σ 
and (cid:63) is the convolution operator. Using some prior knowledge on the distribution of motions  for
instance a preference for slow speeds  this indeed leads to a Bayesian formalization of this inference
problem [18]. This has been successful in accounting for a large class of psychophysical observa-
tions [19]. As a consequence  such probabilistic frameworks allow one to connect different models
from computer vision to neuroscience with a uniﬁed  principled approach.
However the model deﬁned in (1) is obviously quite simplistic with respect to the complexity of natu-
ral scenes. It is therefore useful here to relate this problem to solutions proposed by texture synthesis
methods in the computer vision community. Indeed  the literature on the subject of static textures
synthesis is abundant (see [16] and the references therein for applications in computer graphics).
Of particular interest for us is the work of Galerne et al. [6]  which proposes a stationary Gaussian
model restricted to static textures. Realistic dynamic texture models are however less studied  and
the most prominent method is the non-parametric Gaussian auto-regressive (AR) framework of [3] 
which has been reﬁned in [20].

∂t

Contributions. Here  we seek to engender a better understanding of motion perception by im-
proving generative models for dynamic texture synthesis. From that perspective  we motivate the
generation of optimal stimulation within a stationary Gaussian dynamic texture model. We base our
model on a previously deﬁned heuristic [10  11] coined “Motion Clouds”. Our ﬁrst contribution is

Figure 1: Parameterization of the class of Motion Clouds stimuli. The illustration relates the
parametric changes in MC with real world (top row) and observer (second row) movements.
(A) Orientation changes resulting in scene rotation are parameterized through θ as shown in
the bottom row where a horizontal a and obliquely oriented b MC are compared. (B) Zoom
movements  either from scene looming or observer movements in depth  are characterised by
scale changes reﬂected by a scale or frequency term z shown for a larger or closer object b
compared to more distant a. (C) Translational movements in the scene characterised by V
using the same formulation for static (a) slow (b) and fast moving MC  with the variability in
these speeds quantiﬁed by σV . (ξ and τ ) in the third row are the spatial and temporal frequency
scale parameters. The development of this formulation is detailed in the text.

2

an axiomatic derivation of this model  seen as a shot noise aggregation of dynamically warped “tex-
tons”. This formulation is important to provide a clear understanding of the effects of the model’s
parameters manipulated during psychophysical experiments. Within our generative model  they
correspond to average translation speed and orientation of the “textons” and standard deviations
of random ﬂuctuations around this average. Our second contribution (proved in the supplemen-
tary materials) is to demonstrate an explicit equivalence between this model and a class of linear
stochastic partial differential equations (sPDE). This shows that our model is a generalization of the
well-known luminance conservation equation. This sPDE formulation has two chief advantages: it
allows for a real-time synthesis using an AR recurrence and it allows one to recast the log-likelihood
of the model as a generalization of the classical motion energy model  which in turn is crucial to
allow for a Bayesian modeling of perceptual biases. Our last contribution is an illustrative appli-
cation of this model to the psychophysical study of motion perception in humans. This application
shows how the model allows us to deﬁne a likelihood  which enables a simple ﬁtting procedure to
determine the prior driving the perceptual bias.

In the following  we will denote (x  t) ∈ R2 × R the space/time variable  and (ξ  τ ) ∈
Notations.
R2 × R the corresponding frequency variables. If f (x  t) is a function deﬁned on R3  then ˆf (ξ  τ )
denotes its Fourier transform. For ξ ∈ R2  we denote ξ = ||ξ||(cos(∠ξ)  sin(∠ξ)) ∈ R2 its polar
coordinates. For a function g in R2  we denote ¯g(x) = g(−x). In the following  we denote with
a capital letter such as A a random variable  a we denote a a realization of A  we let PA(a) be the
corresponding distribution of A.

2 Axiomatic Construction of a Dynamic Texture Stimulation Model

Solving a model-based estimation problem and ﬁnding optimal dynamic textures for stimulating an
instance of such a model can be seen as equivalent mathematical problems. In the luminance con-
servation model (1)  the generative model is parameterized by a spatio-temporal coupling function 
which is encoded in the covariance Σ of the driving noise and the motion ﬂow v0. This coupling
(covariance) is essential as it quantiﬁes the extent of the spatial integration area as well as the in-
tegration dynamics  an important issue in neuroscience when considering the implementation of
integration mechanisms from the local to the global scale. In particular  it is important to understand
modular sensitivity in the various lower visual areas with different spatio-temporal selectivities such
as Primary Visual Cortex (V1) or ascending the processing hierarchy  Middle Temple area (MT).
For instance  by varying the frequency bandwidth of such dynamic textures  distinct mechanisms
for perception and action have been identiﬁed [11]. However  such textures were based on a heuris-
tic [10]  and our goal here is to develop a principled  axiomatic deﬁnition.

2.1 From Shot Noise to Motion Clouds

We propose a mathematically-sound derivation of a general parametric model of dynamic textures.
This model is deﬁned by aggregation  through summation  of a basic spatial “texton” template g(x).
The summation reﬂects a transparency hypothesis  which has been adopted for instance in [6]. While
one could argue that this hypothesis is overly simplistic and does not model occlusions or edges  it
leads to a tractable framework of stationary Gaussian textures  which has proved useful to model
static micro-textures [6] and dynamic natural phenomena [20]. The simplicity of this framework
allows for a ﬁne tuning of frequency-based (Fourier) parameterization  which is desirable for the
interpretation of psychophysical experiments.
We deﬁne a random ﬁeld as

(cid:88)

p∈N

Iλ(x  t) def.=

1
√λ

g(ϕAp (x − Xp − Vpt))

(2)

where ϕa : R2 → R2 is a planar warping parameterized by a ﬁnite dimensional vector a. Intuitively 
this model corresponds to a dense mixing of stereotyped  static textons as in [6]. The originality is
two-fold. First  the components of this mixing are derived from the texton by visual transformations
ϕAp which may correspond to arbitrary transformations such as zooms or rotations  illustrated in
Figure 1. Second  we explicitly model the motion (position Xp and speed Vp) of each individual
texton. The parameters (Xp  Vp  Ap)p∈N are independent random vectors. They account for the

3

variability in the position of objects or observers and their speed  thus mimicking natural motions in
an ambient scene. The set of translations (Xp)p∈N is a 2-D Poisson point process of intensity λ > 0.
The following section instantiates this idea and proposes canonical choices for these variabilities.
The warping parameters (Ap)p are distributed according to a distribution PA. The speed parameters
(Vp)p are distributed according to a distribution PV on R2. The following result shows that the
model (2) converges to a stationary Gaussian ﬁeld and gives the parameterization of the covariance.
Its proof follows from a specialization of [5  Theorem 3.1] to our setting.
(cid:90) (cid:90)
Proposition 1. Iλ is stationary with bounded second order moments.
Σ(x  t  x(cid:48)  t(cid:48)) = γ(x − x(cid:48)  t − t(cid:48)) where γ satisﬁes

Its covariance is

∀ (x  t) ∈ R3 

γ(x  t) =

cg(ϕa(x − νt))PV (ν)PA(a)dνda

R2

(3)

where cg = g (cid:63) ¯g is the auto-correlation of g. When λ → +∞  it converges (in the sense of ﬁnite
dimensional distributions) toward a stationary Gaussian ﬁeld I of zero mean and covariance Σ.

2.2 Deﬁnition of “Motion Clouds”

We detail this model here with warpings as rotations and scalings (see Figure 1). These account for
the characteristic orientations and sizes (or spatial scales) in a scene with respect to the observer

∀ a = (θ  z) ∈ [−π  π) × R∗

+  ϕa(x) def.= zR−θ(x) 

where Rθ is the planar rotation of angle θ. We now give some physical and biological motivation
underlying our particular choice for the distributions of the parameters. We assume that the distribu-
tions PZ and PΘ of spatial scales z and orientations θ  respectively (see Figure 1)  are independent
and have densities  thus considering ∀ a = (θ  z) ∈ [−π  π) × R∗
+  PA(a) = PZ(z) PΘ(θ). The
speed vector ν is assumed to be randomly ﬂuctuating around a central speed v0  so that

∀ ν ∈ R2  PV (ν) = P||V −v0||(||ν − v0||).

(4)
In order to obtain “optimal” responses to the stimulation (as advocated by [21])  it makes sense to
deﬁne the texton g to be equal to an oriented Gabor acting as an atom  based on the structure of
a standard receptive ﬁeld of V1. Each would have a scale σ and a central frequency ξ0. Since the
orientation and scale of the texton is handled by the (θ  z) parameters  we can impose without loss of
generality the normalization ξ0 = (1  0). In the special case where σ → 0  g is a grating of frequency
ξ0  and the image I is a dense mixture of drifting gratings  whose power-spectrum has a closed form
expression detailed in Proposition 2. Its proof can be found in the supplementary materials. We call
this Gaussian ﬁeld a Motion Cloud (MC)  and it is parameterized by the envelopes (PZ  PΘ  PV ) and
has central frequency and speed (ξ0  v0). Note that it is possible to consider any arbitrary textons
g  which would give rise to more complicated parameterizations for the power spectrum ˆg  but we
decided here to stick to the simple case of gratings.
Proposition 2. When g(x) = ei(cid:104)x  ξ0(cid:105)  the image I deﬁned in Proposition 1 is a stationary Gaussian
ﬁeld of covariance having the power-spectrum
PZ (||ξ||)
||ξ||2

where the linear transform L is such that ∀u ∈ R L(f )(u) =(cid:82) π

∀ (ξ  τ ) ∈ R2 × R  ˆγ(ξ  τ ) =

PΘ (∠ξ)L(P||V −v0||)

−π f (−u/ cos(ϕ))dϕ.

Remark 1. Note that the envelope of ˆγ is shaped along a cone in the spatial and temporal domains.
This is an important and novel contribution when compared to a Gaussian formulation like a clas-
sical Gabor. In particular  the bandwidth is then constant around the speed plane or the orientation
line with respect to spatial frequency. Basing the generation of the textures on all possible transla-
tions  rotations and zooms  we thus provide a principled approach to show that bandwidth should be
proportional to spatial frequency to provide a better model of moving textures.

τ + (cid:104)v0  ξ(cid:105)

 

(5)

(cid:18)

−

(cid:19)

||ξ||

2.3 Biologically-inspired Parameter Distributions
We now give meaningful specialization for the probability distributions (PZ  PΘ  P||V −v0||)  which
are inspired by some known scaling properties of the visual transformations relevant to dynamic
scene perception.

4

First  small  centered  linear movements of the observer along the axis of view (orthogonal to the
plane of the scene) generate centered planar zooms of the image. From the linear modeling of the
observer’s displacement and the subsequent multiplicative nature of zoom  scaling should follow a
Weber-Fechner law stating that subjective sensation when quantiﬁed is proportional to the logarithm
of stimulus intensity. Thus  we choose the scaling z drawn from a log-normal distribution PZ 
deﬁned in (6). The bandwidth σZ quantiﬁes the variance in the amplitude of zooms of individual
textons relative to the set characteristic scale z0. Similarly  the texture is perturbed by variation in the
global angle θ of the scene: for instance  the head of the observer may roll slightly around its normal
position. The von-Mises distribution – as a good approximation of the warped Gaussian distribution
around the unit circle – is an adapted choice for the distribution of θ with mean θ0 and bandwidth
σΘ  see (6). We may similarly consider that the position of the observer is variable in time. On ﬁrst
order  movements perpendicular to the axis of view dominate  generating random perturbations to
the global translation v0 of the image at speed ν − v0 ∈ R2. These perturbations are for instance
described by a Gaussian random walk: take for instance tremors  which are constantly jittering 
small ((cid:54) 1 deg) movements of the eye. This justiﬁes the choice of a radial distribution (4) for
PV . This radial distribution P||V −v0|| is thus selected as a bell-shaped function of width σV   and we
choose here a Gaussian function for simplicity  see (6). Note that  as detailed in the supplementary
a slightly different bell-function (with a more complicated expression) should be used to obtain an
exact equivalence with the sPDE discretization mentioned in Section 4.
The distributions of the parameters are thus chosen as

− ln( z
e

z0 )2
Z)   PΘ(θ) ∝ e
2 ln(1+σ2

z0
z

PZ(z) ∝

Remark 2. Note that in practice we have parametrized PZ by its mode mZ = argmaxz
standard deviation dZ =

and P||V −v0||(r) ∝ e
(cid:113)(cid:82) z2PZ(z)dz  see the supplementary material and [4].

4σ2
Θ

cos(2(θ−θ0))

− r2
2σ2

(6)
V .
PZ(z) and

τ
Slope: ∠v0

ξ2

z0

σΘ
σZ

ξ1

θ0

σV

σZ

z0

ξ1

Two different projections of ˆγ in Fourier space

t

MC of two different spatial frequencies z

Figure 2: Graphical representation of the covariance γ (left) —note the cone-like shape of the
envelopes– and an example of synthesized dynamics for narrow-band and broad-band Motion
Clouds (right).

Plugging these expressions (6) into the deﬁnition (5) of the power spectrum of the motion cloud 
one obtains a parameterization which is very similar to the one originally introduced in [11]. The
following table gives the speed v0 and frequency (θ0  z0) central parameters in terms of amplitude
and orientation  each one being coupled with the relevant dispersion parameters. Figure 1 and 2
shows a graphical display of the inﬂuence of these parameters.

(mean  dispersion)

Speed
(v0  σV )

Freq. orient.

(θ0  σΘ)

Freq. amplitude

(z0  σZ) or (mZ  dZ)

Remark 3. Note that the ﬁnal envelope of ˆγ is in agreement with the formulation that is used in [10].
However  that previous derivation was based on a heuristic which intuitively emerged from a long
interaction between modelers and psychophysicists. Herein  we justiﬁed these different points from
ﬁrst principles.
Remark 4. The MC model can equally be described as a stationary solution of a stochastic partial
differential equation (sPDE). This sPDE formulation is important since we aim to deal with dynamic
stimulation  which should be described by a causal equation which is local in time. This is crucial
for numerical simulations  since  this allows us to perform real-time synthesis of stimuli using an

5

auto-regressive time discretization. This is a signiﬁcant departure from previous Fourier-based im-
plementation of dynamic stimulation [10  11]. This is also important to simplify the application
of MC inside a bayesian model of psychophysical experiments (see Section 3)The derivation of an
equivalent sPDE model exploits a spectral formulation of MCs as Gaussian Random ﬁelds. The full
proof along with the synthesis algorithm can be found in the supplementary material.

3 Psychophysical Study: Speed Discrimination

To exploit the useful features of our MC model and provide a generalizable proof of concept based
on motion perception  we consider here the problem of judging the relative speed of moving dy-
namical textures and the impact of both average spatial frequency and average duration of temporal
correlations.

3.1 Methods
The task was to discriminate the speed v ∈ R of MC stimuli moving with a horizontal central
speed v = (v  0). We assign as independent experimental variable the most represented spatial
frequency mZ  that we denote in the following z for easier reading. The other parameters are
and dZ = 1.0 c/◦. Note
set to the following values σV = 1
that σV is thus dependent of the value of z (that is computed from mZ and dZ  see Remark 2
and the supplementary ) to ensure that t(cid:63) = 1
σV z stays constant. This parameter t(cid:63) controls the
temporal frequency bandwidth  as illustrated on the middle of Figure 2. We used a two alternative
forced choice (2AFC) paradigm. In each trial a grey ﬁxation screen with a small dark ﬁxation spot
was followed by two stimulus intervals of 250 ms each  separated by a grey 250 ms inter-stimulus
interval. The ﬁrst stimulus had parameters (v1  z1) and the second had parameters (v2  z2). At the
end of the trial  a grey screen appeared asking the participant to report which one of the two intervals
was perceived as moving faster by pressing one of two buttons  that is whether v1 > v2 or v2 > v1.
Given reference values (v(cid:63)  z(cid:63))  for each trial  (v1  z1) and (v2  z2) are selected so that

σΘ = π
12  

θ0 = π
2  

t(cid:63)z  

(cid:26) vi = v(cid:63)  zi ∈ z(cid:63) + ∆Z

vj ∈ v(cid:63) + ∆V   zj = z(cid:63)

(cid:26) ∆V = {−2 −1  0  1  2} 

where

∆Z = {−0.48 −0.21  0  0.32  0.85} 

where (i  j) = (1  2) or (i  j) = (2  1) (i.e. the ordering is randomized across trials)  and where z
values are expressed in cycles per degree (c/◦) and v values in ◦/s. Ten repetitions of each of the 25
possible combinations of these parameters are made per block of 250 trials and at least four such
blocks were collected per condition tested. The outcome of these experiments are summarized by
psychometric curves ˆϕv(cid:63) z(cid:63)  where for all (v − v(cid:63)  z − z(cid:63)) ∈ ∆V × ∆Z  the value ˆϕv(cid:63) z(cid:63) (v  z) is
the empirical probability (each averaged over the typically 40 trials) that a stimulus generated with
parameters (v(cid:63)  z) is moving faster than a stimulus with parameters (v  z(cid:63)).
To assess the validity of our model  we tested four different scenarios by considering all possible
choices among z(cid:63) = 1.28 c/◦ 
and t(cid:63) ∈ {0.1s  0.2s}  which corresponds
to combinations of low/high speeds and a pair of temporal frequency parameters. Stimuli were gen-
erated on a Mac running OS 10.6.8 and displayed on a 20” Viewsonic p227f monitor with resolution
1024 × 768 at 100 Hz. Routines were written using Matlab 7.10.0 and Psychtoolbox 3.0.9 con-
trolled the stimulus display. Observers sat 57 cm from the screen in a dark room. Three observers
with normal or corrected to normal vision took part in these experiments. They gave their informed
consent and the experiments received ethical approval from the Aix-Marseille Ethics Committee in
accordance with the declaration of Helsinki.

v(cid:63) ∈ {5◦/s  10◦/s} 

3.2 Bayesian modeling

To make full use of our MC paradigm in analyzing the obtained results  we follow the methodology
of the Bayesian observer used for instance in [13  12  8]. We assume the observer makes its deci-
[− log(PM|V Z(m|v  z)) −
sion using a Maximum A Posteriori (MAP) estimator ˆvz(m) = argmin
log(PV |Z(v|z))] computed from some internal representation m ∈ R of the observed stimulus. For
simplicity  we assume that the observer estimates z from m without bias. To simplify the numerical
analysis  we assume that the likelihood is Gaussian  with a variance independent of v. Furthermore 

v

6

we assume that the prior is Laplacian as this gives a good description of the a priori statistics of
speeds in natural images [2]:
PM|V Z(m|v  z) =

(7)
where vmax > 0 is a cutoff speed ensuring that PV |Z is a well deﬁned density even if az > 0.
Both az and σz are unknown parameters of the model  and are obtained from the outcome of the
experiments by a ﬁtting process we now explain.

and PV |Z(v|z) ∝ eazv1[0 vmax](v).

√2πσz

− |m−v|2
2σ2
z

1

e

3.3 Likelihood and Prior Estimation

Following for instance [13  12  8]  the theoretical psychophysical curve obtained by a Bayesian
decision model is

ϕv(cid:63) z(cid:63) (v  z) def.= E(ˆvz(cid:63) (Mv z(cid:63) ) > ˆvz(Mv(cid:63) z))

z ) is a Gaussian variable having the distribution PM|V Z(·|v  z).

where Mv z ∼ N (v  σ2
The following proposition shows that in our special case of Gaussian prior and Laplacian likelihood 
it can be computed in closed form. Its proof follows closely the derivation of [12  Appendix A]  and
can be found in the supplementary materials.
Proposition 3. In the special case of the estimator (3.2) with a parameterization (7)  one has

(cid:32)

(cid:33)

z(cid:63) + σ2
z

(cid:112)σ2
(cid:16) v−v(cid:63)−µz z(cid:63)

λz z(cid:63)

.

(8)

v − v(cid:63) − az(cid:63) σ2

z(cid:63) + azσ2
z

ϕv(cid:63) z(cid:63) (v  z) = ψ

(cid:82) t
−∞ e−s2/2ds is a sigmoid function.

where ψ(t) = 1√
2π
One can ﬁt the experimental psychometric function to compute the perceptual bias term µz z(cid:63) ∈ R
and an uncertainty λz z(cid:63) such that ˆϕv(cid:63) z(cid:63) (v  z) ≈ ψ
Remark 5. Note that in practice we perform a ﬁt in a log-speed domain ie we consider ϕ˜v(cid:63) z(cid:63) (˜v  z)
where ˜v = ln(1 + v/v0) with v0 = 0.3◦/s following [13].
By comparing the theoretical and experimental psychopysical curves (8) and (3.3)  one thus obtains
. The only remaining
the following expressions σ2
unknown is az(cid:63)  that can be set as any negative number based on previous work on low speed priors
or  alternatively estimated in future by performing a wiser ﬁtting method.

z − µz z(cid:63)

and az = az(cid:63)

z z(cid:63) − 1

z = λ2

(cid:17)

σ2
z(cid:63)
σ2

2 λ2

z(cid:63) z(cid:63)

σ2
z

3.4 Psychophysic Results

The main results are summarized in Figure 3 showing the parameters µz z(cid:63) in Figure 3(a) and the
parameters σz in Figure 3(b). Spatial frequency has a positive effect on perceived speed; speed is
systematically perceived as faster as spatial frequency is increased  moreover this shift cannot simply
be explained to be the result of an increase in the likelihood width (Figure 3(b)) at the tested spatial
frequency  as previously observed for contrast changes [13  12]. Therefore the positive effect could
be explained by a negative effect in prior slopes az as the spatial frequency increases. However  we
do not have any explanation for the observed constant likelihood width as it is not consistent with
the speed width of the stimuli σV = 1

t(cid:63)z which is decreasing with spatial frequency.

3.5 Discussion

We exploited the principled and ecologically motivated parameterization of MC to ask about the ef-
fect of scene scaling on speed judgements. In the experimental task  MC stimuli  in which the spatial
scale content was systematically varied (via frequency manipulations) around a central frequency of
1.28 c/◦ were found to be perceived as slightly faster at higher frequencies slightly slower at lower
frequencies. The effects were most prominent at the faster speed tested  of 10 ◦/s relative to those at
5 ◦/s. The ﬁtted psychometic functions were compared to those predicted by a Bayesian model in
which the likelihood or the observer’s sensory representation was characterised by a simple Gaus-
sian. Indeed  for this small data set intended as a proof of concept  the model was able to explain

7

(a)

(b)

Figure 3: 2AFC speed discrimination results. (a) Task generates psychometric functions which
show shifts in the point of subjective equality for the range of test z. Stimuli of lower frequency
with respect to the reference (intersection of dotted horizontal and vertical lines gives the refer-
ence stimulus) are perceived as going slower  those with greater mean frequency are perceived
as going relatively faster. This effect is observed under all conditions but is stronger at the
highest speed and for subject 1. (b) The estimated σz appear noisy but roughly constant as a
function of z for each subject. Widths are generally higher for v = 5 (red) than v = 10 (blue)
traces. The parameter t(cid:63) does not show a signiﬁcant effect across the conditions tested.

these systematic biases for spatial frequency as shifts in our a priori on speed during the perceptual
judgements as the likelihood width are constant across tested frequencies but lower at the higher of
the tested speeds. Thus having a larger measured bias given the case of the smaller likelihood width
(faster speed) is consistent with a key role for the prior in the observed perceptual bias.
A larger data set  including more standard spatial frequencies and the use of more observers  is
needed to disambiguate the models predicted prior function.

4 Conclusions

We have proposed and detailed a generative model for the estimation of the motion of images based
on a formalization of small perturbations from the observer’s point of view during parameterized
rotations  zooms and translations. We connected these transformations to descriptions of ecolog-
ically motivated movements of both observers and the dynamic world. The fast synthesis of nat-
uralistic textures optimized to probe motion perception was then demonstrated  through fast GPU
implementations applying auto-regression techniques with much potential for future experimenta-
tion. This extends previous work from [10] by providing an axiomatic formulation. Finally  we
used the stimuli in a psychophysical task and showed that these textures allow one to further under-
stand the processes underlying speed estimation. By linking them directly to the standard Bayesian
formalism  we show that the sensory representations of the stimulus (the likelihoods) in such mod-
els can be described directly from the generative MC model. In our case we showed this through
the inﬂuence of spatial frequency on speed estimation. We have thus provided just one example
of how the optimized motion stimulus and accompanying theoretical work might serve to improve
our understanding of inference behind perception. The code associated to this work is available at
https://jonathanvacher.github.io.

Acknowledgements

We thank Guillaume Masson for useful discussions during the development of the experiments. We
also thank Manon Bouy´e and ´Elise Amfreville for proofreading. LUP was supported by EC FP7-
269921  “BrainScaleS”. The work of JV and GP was supported by the European Research Council
(ERC project SIGMA-Vision). AIM and LUP were supported by SPEED ANR-13-SHS2-0006.

8

0.81.01.21.41.61.82.0−0.20−0.15−0.10−0.050.000.050.100.15PSEbias(µz z∗)Subject1v∗=5 t∗=100v∗=5 t∗=200v∗=10 t∗=100v∗=10 t∗=2000.81.01.21.41.61.82.0−0.2−0.10.00.10.20.3Subject20.81.01.21.41.61.82.0Spatialfrequency(z)incycles/deg−0.050.000.050.100.150.200.25Likehoodwidth(σz)0.81.01.21.41.61.82.0Spatialfrequency(z)incycles/deg−0.4−0.20.00.20.40.60.8References
[1] Adelson  E. H. and Bergen  J. R. (1985). Spatiotemporal energy models for the perception of

motion. Journal of Optical Society of America  A.  2(2):284–99.

[2] Dong  D. (2010). Maximizing causal information of natural scenes in motion. In Ilg  U. J. and

Masson  G. S.  editors  Dynamics of Visual Motion Processing  pages 261–282. Springer US.

[3] Doretto  G.  Chiuso  A.  Wu  Y. N.  and Soatto  S. (2003). Dynamic textures. International

Journal of Computer Vision  51(2):91–109.

[4] Field  D. J. (1987). Relations between the statistics of natural images and the response properties

of cortical cells. J. Opt. Soc. Am. A  4(12):2379–2394.

[5] Galerne  B. (2011). Stochastic image models and texture synthesis. PhD thesis  ENS de Cachan.
[6] Galerne  B.  Gousseau  Y.  and Morel  J. M. (2011). Micro-Texture synthesis by phase random-

ization. Image Processing On Line  1.

[7] Gregory  R. L. (1980). Perceptions as hypotheses. Philosophical Transactions of the Royal

Society B: Biological Sciences  290(1038):181–197.

[8] Jogan  M. and Stocker  A. A. (2015). Signal integration in human visual speed perception. The

Journal of Neuroscience  35(25):9381–9390.

[9] Nestares  O.  Fleet  D.  and Heeger  D. (2000). Likelihood functions and conﬁdence bounds for
total-least-squares problems. In IEEE Conference on Computer Vision and Pattern Recognition.
CVPR 2000  volume 1  pages 523–530. IEEE Comput. Soc.

[10] Sanz-Leon  P.  Vanzetta  I.  Masson  G. S.  and Perrinet  L. U. (2012). Motion clouds: model-
based stimulus synthesis of natural-like random textures for the study of motion perception. Jour-
nal of Neurophysiology  107(11):3217–3226.

[11] Simoncini  C.  Perrinet  L. U.  Montagnini  A.  Mamassian  P.  and Masson  G. S. (2012). More
is not always better: adaptive gain control explains dissociation between perception and action.
Nature Neurosci  15(11):1596–1603.

[12] Sotiropoulos  G.  Seitz  A. R.  and Seri`es  P. (2014). Contrast dependency and prior expecta-

tions in human speed perception. Vision Research  97(0):16 – 23.

[13] Stocker  A. A. and Simoncelli  E. P. (2006). Noise characteristics and prior expectations in

human visual speed perception. Nature Neuroscience  9(4):578–585.

[14] Unser  M. and Tafti  P. (2014). An Introduction to Sparse Stochastic Processes. Cambridge

University Press  Cambridge  UK. 367 p.

[15] Unser  M.  Tafti  P. D.  Amini  A.  and Kirshner  H. (2014). A uniﬁed formulation of gaus-
sian versus sparse stochastic processes - part II: Discrete-Domain theory. IEEE Transactions on
Information Theory  60(5):3036–3051.

[16] Wei  L. Y.  Lefebvre  S.  Kwatra  V.  and Turk  G. (2009). State of the art in example-based
texture synthesis. In Eurographics 2009  State of the Art Report  EG-STAR. Eurographics Asso-
ciation.

[17] Wei  X.-X. and Stocker  A. A. (2012). Efﬁcient coding provides a direct link between prior
and likelihood in perceptual bayesian inference. In Bartlett  P. L.  Pereira  F. C. N.  Burges  C.
J. C.  Bottou  L.  and Weinberger  K. Q.  editors  NIPS  pages 1313–1321.

[18] Weiss  Y. and Fleet  D. J. (2001). Velocity likelihoods in biological and machine vision. In In

Probabilistic Models of the Brain: Perception and Neural Function  pages 81–100.

[19] Weiss  Y.  Simoncelli  E. P.  and Adelson  E. H. (2002). Motion illusions as optimal percepts.

Nature Neuroscience  5(6):598–604.

[20] Xia  G. S.  Ferradans  S.  Peyr´e  G.  and Aujol  J. F. (2014). Synthesizing and mixing stationary

gaussian texture models. SIAM Journal on Imaging Sciences  7(1):476–508.

[21] Young  R. A. and Lesperance  R. M. (2001). The gaussian derivative model for spatial-temporal

vision: II. cortical data. Spatial vision  14(3):321–390.

9

,Jonathan Vacher
Andrew Isaac Meso
Laurent Perrinet
Gabriel Peyré