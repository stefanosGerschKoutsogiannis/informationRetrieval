2017,Structured Bayesian Pruning via Log-Normal Multiplicative Noise,Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves gener- alization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However  this sparsity can hardly be used for acceleration since it is unstructured. In the paper  we propose a new Bayesian model that takes into account the computational structure of neural net- works and provides structured sparsity  e.g. removes neurons and/or convolutional channels in CNNs. To do this we inject noise to the neurons outputs while keeping the weights unregularized. We establish the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the KL-term in the evidence lower bound is com- puted in closed-form. The model leads to structured sparsity by removing elements with a low SNR from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is easy to implement as it can be formulated as a separate dropout-like layer.,Structured Bayesian Pruning via Log-Normal

Multiplicative Noise

Kirill Neklyudov 1 2
k.necludov@gmail.com

Dmitry Molchanov 1 3
dmolchanov@hse.ru

Arsenii Ashukha 1 2
aashukha@hse.ru

Dmitry Vetrov 1 2
dvetrov@hse.ru

1National Research University Higher School of Economics 2Yandex

3Skolkovo Institute of Science and Technology

Abstract

Dropout-based regularization methods can be regarded as injecting random noise
with pre-deﬁned magnitude to different parts of the neural network during training.
It was recently shown that Bayesian dropout procedure not only improves gener-
alization but also leads to extremely sparse neural architectures by automatically
setting the individual noise magnitude per weight. However  this sparsity can hardly
be used for acceleration since it is unstructured. In the paper  we propose a new
Bayesian model that takes into account the computational structure of neural net-
works and provides structured sparsity  e.g. removes neurons and/or convolutional
channels in CNNs. To do this we inject noise to the neurons outputs while keeping
the weights unregularized. We establish the probabilistic model with a proper
truncated log-uniform prior over the noise and truncated log-normal variational
approximation that ensures that the KL-term in the evidence lower bound is com-
puted in closed-form. The model leads to structured sparsity by removing elements
with a low SNR from the computation graph and provides signiﬁcant acceleration
on a number of deep neural architectures. The model is easy to implement as it can
be formulated as a separate dropout-like layer.

1

Introduction

Deep neural networks are a ﬂexible family of models which provides state-of-the-art results in many
machine learning problems [14  20]. However  this ﬂexibility often results in overﬁtting. A common
solution for this problem is regularization. One of the most popular ways of regularization is Binary
Dropout [19] that prevents co-adaptation of neurons by randomly dropping them during training. An
equally effective alternative is Gaussian Dropout [19] that multiplies the outputs of the neurons by
Gaussian random noise. In recent years several Bayesian generalizations of these techniques have
been developed  e.g. Variational Dropout [8] and Variational Spike-and-Slab Neural Networks [13].
These techniques provide theoretical justiﬁcation of different kinds of Dropout and also allow for
automatic tuning of dropout rates  which is an important practical result.
Besides overﬁtting  compression and acceleration of neural networks are other important challenges 
especially when memory or computational resources are restricted. Further studies of Variational
Dropout show that individual dropout rates for each weight allow to shrink the original network
architecture and result in a highly sparse model [16]. General sparsity provides a way of neural
network compression  while the time of network evaluation may remain the same  as most modern
DNN-oriented software can’t work with sparse matrices efﬁciently. At the same time  it is possible
to achieve acceleration by enforcing structured sparsity in convolutional ﬁlters or data tensors. In
the simplest case it means removing redundant neurons or convolutional ﬁlters instead of separate
weights; but more complex patterns can also be considered. This way Group-wise Brain Damage

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

[10] employs group-wise sparsity in convolutional ﬁlters  Perforated CNNs [3] drop redundant rows
from the intermediate dataframe matrices that are used to compute convolutions  and Structured
Sparsity Learning [24] provides a way to remove entire convolutional ﬁlters or even layers in residual
networks. These methods allow to obtain practical acceleration with little to no modiﬁcations of
the existing software. In this paper  we propose a tool that is able to induce an arbitrary pattern
of structured sparsity on neural network parameters or intermediate data tensors. We propose a
dropout-like layer with a parametric multiplicative noise and use stochastic variational inference to
tune its parameters in a Bayesian way. We introduce a proper analog of sparsity-inducing log-uniform
prior distribution [8  16] that allows us to formulate a correct probabilistic model and avoid the
problems that come from using an improper prior. This way we obtain a novel Bayesian method of
regularization of neural networks that results in structured sparsity. Our model can be represented
as a separate dropout-like layer that allows for a simple and ﬂexible implementation with almost no
computational overhead  and can be incorporated into existing neural networks.
Our experiments show that our model leads to high group sparsity level and signiﬁcant acceleration
of convolutional neural networks with negligible accuracy drop. We demonstrate the performance of
our method on LeNet and VGG-like architectures using MNIST and CIFAR-10 datasets.

2 Related Work

Deep neural networks are extremely prone to overﬁtting  and extensive regularization is crucial.
The most popular regularization methods are based on injection of multiplicative noise over layer
inputs  parameters or activations [8  19  22]. Different kinds of multiplicative noise have been used
in practice; the most popular choices are Bernoulli and Gaussian distributions. Another type of
regularization of deep neural networks is based on reducing the number of parameters. One approach
is to use low-rank approximations  e.g. tensor decompositions [4  17]  and the other approach is
to induce sparsity  e.g. by pruning [5] or L1 regularization [24]. Sparsity can also be induced by
using the Sparse Bayesian Learning framework with empirical Bayes [21] or with sparsity-inducing
priors [12  15  16].
High sparsity is one of the key factors for the compression of DNNs [5  21]. However  in addition to
compression it is beneﬁcial to obtain acceleration. Recent papers propose different approaches to
acceleration of DNNs  e.g. Spatial Skipped Convolutions [3] and Spatially Adaptive Computation
Time [2] that propose different ways to reduce the number of computed convolutions  Binary
Networks [18] that achieve speedup by using only 1 bit to store a single weight of a DNN  Low-Rank
Expansions [6] that use low-rank ﬁlter approximations  and Structured Sparsity Learning [24] that
allows to remove separate neurons or ﬁlters. As reported in [24] it is possible to obtain acceleration of
DNNs by introducing structured sparsity  e.g. by removing whole neurons  ﬁlters or layers. However 
non-adaptive regularization techniques require tuning of a huge number of hyperparameters that
makes it difﬁcult to apply in practice. In this paper we apply the Bayesian learning framework to
obtain structured sparsity and focus on acceleration of neural networks.

3 Stochastic Variational Inference

Given a probabilistic model p(y | x  ✓) we want to tune parameters ✓ of the model using training
dataset D = {(xi  yi)}N
i=1. The prior knowledge about parameters ✓ is deﬁned by prior distribution
p(✓). Using the Bayes rule we obtain the posterior distribution p(✓ |D) = p(D | ✓)p(✓)/p(D).
However  computing posterior distribution using the Bayes rule usually involves computation of
intractable integrals  so we need to use approximation techniques.
One of the most widely used approximation techniques is Variational Inference. In this approach the
unknown distribution p(✓ |D) is approximated by a parametric distribution q(✓) by minimization
of the Kullback-Leibler divergence KL(q(✓)k p(✓ |D)). Minimization of the KL divergence is
equivalent to maximization of the variational lower bound L().

L() = LD()  KL(q(✓)k p(✓)) 
Eq(✓) log p(yi | xi ✓ )

where LD() =

NXi=1

2

(1)

(2)

LD() is a so-called expected log-likelihood function which is intractable in case of complex
probabilistic model p(y | x  ✓). Following [8] we use the Reparametrization trick to obtain an
unbiased differentiable minibatch-based Monte Carlo estimator of the expected log-likelihood. Here
N is the total number of objects  M is the minibatch size  and f (  ") provides samples from the
approximate posterior q(✓) as a deterministic function of a non-parametric noise " ⇠ p(").

log p(yik | xik   wik = f (  "ik ))

(3)

D

LD() ' LSGV B

MXk=1
L() 'L SGV B() = LSGV B

() =

N
M

D

()  KL(q(w)k p(w))

rLD() ' rLSGV B

(4)
(5)
This way we obtain a procedure of approximate Bayesian inference where we solve optimization
problem (4) by stochastic gradient ascent w.r.t. variational parameters . This procedure can be
efﬁciently applied to Deep Neural Networks and usually the computational overhead is very small  as
compared to ordinary DNNs.
If the model p(y | x  ✓  w) has another set of parameters w that we do not want to be Bayesian about 
we can still use the same variational lower bound objective:

()

D

L(  w) = LD(  w)  KL(q(✓)k p(✓)) ! max

 w

 

where LD(  w) =

Eq(✓) log p(yi | xi ✓  w )

NXi=1

(6)

(7)

This objective corresponds the maximum likelihood estimation wM L of parameters w  while ﬁnding
the approximate posterior distribution q(✓) ⇡ p(✓ |D  wM L). In this paper we denote the weights of
the neural networks  the biases  etc. as w and ﬁnd their maximum likelihood estimation as described
above. The parameters ✓ that undergo the Bayesian treatment are the noisy masks in the proposed
dropout-like layer (SBP layer). They are described in the following section.

4 Group Sparsity with Log-normal Multiplicative Noise

Variational Inference with a sparsity-inducing log-uniform prior over the weights of a neural network
is an efﬁcient way to enforce general sparsity on weight matrices [16]. However  it is difﬁcult to
apply this approach to explicitly enforce structured sparsity. We introduce a dropout-like layer with a
certain kind of multiplicative noise. We also make use of the sparsity-inducing log-uniform prior  but
put it over the noise variables rather than weights. By sharing those noise variables we can enforce
group-wise sparsity with any form of groups.

4.1 Variational Inference for Group Sparsity Model
We consider a single dropout-like layer with an input vector x 2 RI that represents one object
with I features  and an output vector y 2 RI of the same size. The input vector x is usually
supposed to come from the activations of the preceding layer. The output vector y would then
serve as an input vector for the following layer. We follow the general way to build dropout-
like layers (8). Each input feature xi is multiplied by a noise variable ✓i that comes from some
distribution pnoise(✓). For example  for Binary Dropout pnoise(✓) would be a fully factorized
Bernoulli distribution with pnoise(✓i) = Bernoulli(p)  and for Gaussian dropout it would be a
fully-factorized Gaussian distribution with pnoise(✓i) = N (1 ↵ ).

yi = xi · ✓i

(8)
Note that if we have a minibatch X M⇥I of M objects  we would independently sample a separate
noise vector ✓m for each object xm. This would be the case throughout the paper  but for the sake of
simplicity we would consider a single object x in all following formulas. Also note that the noise ✓ is
usually only sampled during the training phase. A common approximation during the testing phase
is to use the expected value E✓ instead of sampling ✓. All implementation details are provided and
discussed in Section 4.5.

✓ ⇠ pnoise(✓)

3

We follow a Bayesian treatment of the variable ✓  as described in Section 3. In order to obtain a
sparse solution  we choose the prior distribution p(✓) to be a fully-factorized improper log-uniform
distribution. We denote this distribution as LogU1(·) to stress that it has inﬁnite domain. This
distribution is known for its sparsiﬁcation properties and works well in practice for deep neural
networks [16].

p(✓) =

p(✓i)

p(✓i) = LogU1(✓i) /

1
✓ i

✓i > 0

(9)

IYi=1

In order to train the model  i.e. perform variational inference  we need to choose an approximation
family q for the posterior distribution p(✓ |D) ⇡ q(✓).
IYi=1

(11)
A common choice of variational distribution q(·) is a fully-factorized Gaussian distribution. However 
for this particular model we choose q(✓) to be a fully-factorized log-normal distribution (10–11). To
make this choice  we were guided by the following reasons:

i ) () log ✓i ⇠N (log ✓i | µi  2
i )

IYi=1
✓i ⇠ LogN(✓i | µi  2

LogN(✓i | µi  2
i )

q(✓i | µi  i) =

q(✓) =

(10)

• The log-uniform distribution is a speciﬁc case of the log-normal distribution when the parameter
 goes to inﬁnity and µ remains ﬁxed. Thus we can guarantee that in the case of no data our
variational approximation can be made exact. Hence this variational family has no "prior gap".

• We consider a model with multiplicative noise. The scale of this noise corresponds to its shift in
the logarithmic space. By establishing the log-uniform prior we set no preferences on different
scales of this multiplicative noise. The usual use of a Gaussian as a posterior immediately implies
very asymmetric skewed distribution in the logarithmic space. Moreover log-uniform and Gaussian
distributions have different supports and that will require establishing two log-uniform distributions
for positive and negative noises. In this case Gaussian variational approximation would have
quite exotic bi-modal form (one mode in the log-space of positive noises and another one in the
log-space of negative noises). On the other hand  the log-normal posterior for the multiplicative
noise corresponds to a Gaussian posterior for the additive noise in the logarithmic scale  which is
much easier to interpret.

• Log-normal noise is always non-negative both during training and testing phase  therefore it does
not change the sign of its input. This is in contrast to Gaussian multiplicative noise N (✓i | 1 ↵ )
that is a standard choice for Gaussian dropout and its modiﬁcations [8  19  23]. During the training
phase Gaussian noise can take negative values  so the input to the following layer can be of arbitrary
sign. However  during the testing phase noise ✓ is equal to 1  so the input to the following layer is
non-negative with many popular non-linearities (e.g. ReLU  sigmoid  softplus). Although Gaussian
dropout works well in practice  it is difﬁcult to justify notoriously different input distributions
during training and testing phases.

• The log-normal approximate posterior is tractable. Speciﬁcally 
KL(LogN(✓ | µ  2)k LogU1(✓)) can be computed analytically.
The ﬁnal loss function is presented in equation (12) and is essentially the original variational lower
bound (4).

the KL divergence term

D

LSGV B() = LSGV B

(12)
where µ and  are the variatianal parameters  and W denotes all other trainable parameters of the
neural network  e.g. the weight matrices  the biases  batch normalization parameters  etc.
Note that we can optimize the variational lower bound w.r.t. the parameters µ and  of the log-normal
noise ✓. We do not ﬁx the mean of the noise thus making our variational approximation more tight.

(µ    W )  KL(q(✓ | µ  )k p(✓)) ! max

µ  W

 

4.2 Problems of Variational Inference with Improper Log-Uniform Prior
The log-normal posterior in combination with a log-uniform prior has a number of attractive features.
However  the maximization of the variational lower bound with a log-uniform prior and a log-normal

4

posterior is an ill-posed optimization problem. As the log-uniform distribution is an improper prior 
the KL-divergence between a log-normal distribution LogN(µ  2) and a log-uniform distribution
LogU1 is inﬁnite for any ﬁnite value of parameters µ and .
KLLogN(x| µ  2)k LogU1(x) = C  log C

(13)
A common way to tackle this problem is to consider the density of the log-uniform distribution
to be equal to C
✓ and to treat C as some ﬁnite constant. This trick works well for the case of a
Gaussian posterior distribution [8  16]. The KL divergence between a Gaussian posterior and a
log-uniform prior has an inﬁnite gap  but can be calculated up to this inﬁnite constant in a meaningful
way [16]. However  for the case of the log-normal posterior the KL divergence is inﬁnite for any
ﬁnite values of variational parameters  and is equal to zero for a ﬁxed ﬁnite µ and inﬁnite . As
the data-term (3) is bounded for any value of variational parameters  the only global optimum of
the variational lower bound is achieved when µ is ﬁnite and ﬁxed  and  goes to inﬁnity. In this
case the posterior distribution collapses into the prior distribution and the model fails to extract any
information about the data. This effect is wholly caused by the fact that the log-uniform prior is an
improper (non-normalizable) distribution  which makes the whole probabilistic model ﬂawed.

= +1

4.3 Variational Inference with Truncated Approximation Family
Due to the improper prior the optimization problem becomes ill-posed. But do we really need to
use an improper prior distribution? The most common number format that is used to represent
the parameters of a neural network is the ﬂoating-point format. The ﬂoating-point format is only
able to represent numbers from a limited range. For example  a single-point precision variable
can only represent numbers from the range 3.4 ⇥ 1038 to +3.4 ⇥ 1038  and the smallest possible
positive number is equal to 1.2 ⇥ 1038. All of probability mass of the improper log-uniform prior is
concentrated beyond the single-point precision (and essentially any practical ﬂoating point precision) 
not to mention that the actual relevant range of values of neural network parameters is much smaller. It
means that in practice this prior is not a good choice for software implementation of neural networks.
We propose to use a truncated log-uniform distribution (14) as a proper analog of the log-uniform
distribution. Here I[a b](x) denotes the indicator function for the interval x 2 [a  b]. The posterior
distribution should be deﬁned on the same support as the prior distribution  so we also need to use a
truncated log-normal distribution (14).
LogU[a b](✓i) / LogU1(✓i) · I[a b](log ✓i)

LogN[a b](✓i) / LogN(✓i | µi  2

i ) · I[a b](log ✓i)
(14)

Our ﬁnal model then can be formulated as follows.

yi = xi · ✓i

p(✓i) = LogU[a b](✓i)

(15)
Note that all the nice facts about the log-normal posterior distribution from the Section 4.1 are also
true for the truncated log-normal posterior. However  now we have a proper probabilistic model and
the Stochastic Variational Inference can be preformed correctly. Unlike (13)  now the KL divergence
term (16–17) can be calculated correctly for all valid values of variational parameters (see Appendix
A for details).

q(✓i | µi  i) = LogN[a b](✓i | µi  2
i )

KL(q(✓ | µ  )k p(✓)) =

KL(q(✓i | µi  i)k p(✓i))

IXi=1
i  log((i)  (↵i)) 

b  a

(16)

(17)

↵i(↵i)  i(i)
2((i)  (↵i))

KL(q(✓i | µi  i)k p(✓i)) = log

p2⇡e 2
where ↵i = aµi
  (·) and (·) are the density and the CDF of the standard normal
i
distribution.
The reparameterization trick also can still be performed (18) using the inverse CDF of the truncated
normal distribution (see Appendix B).

  i = bµi
i

 

✓i = expµi + i1 ( (↵i) + (( i)  ( ↵i)) yi)   where yi ⇠U (y | 0  1)

(18)
The ﬁnal loss and the set of parameters is the same as described in Section 4.1  and the training
procedure remains the same.

5

4.4 Sparsity
Log-uniform prior is known to lead to a sparse solution [16]. In the variational dropout paper authors
interpret the parameter ↵ of the multiplicative noise N (1 ↵ ) as a Gaussian dropout rate and use it as
a thresholding criterion for weight pruning. Unlike the binary or Gaussian dropout  in the truncated
log-normal model there is no "dropout rate" variable. However  we can use the signal-to-noise ratio
E✓/pVar(✓) (SNR) for thresholding.

((i  ↵i)  (i  i))/p(i)  (↵i)

i )((2i  ↵i)  (2i  i))  ((i  ↵i)  (i  i))2

The SNR can be computed analytically  the derivation can be found in the appendix. It has a simple
interpretation. If the SNR is low  the corresponding neuron becomes very noisy and its output no
longer contains any useful information. If the SNR is high  it means that the neuron output contains
little noise and is important for prediction. Therefore we can remove all neurons or ﬁlters with a low
SNR and set their output to constant zero.

pexp(2

SNR(✓i) =

(19)

Implementation details

4.5
We perform a minibatch-based stochastic variational inference for training. The training procedure
looks as follows. On each training step we take a minibatch of M objects and feed it into the neural
network. Consider a single SBP layer with input X M⇥I and output Y M⇥I. We independently sample
a separate noise vector ✓m ⇠ q(✓) for each object xm and obtain a noise matrix ✓M⇥I. The output
matrix Y M⇥I is then obtained by component-wise multiplication of the input matrix and the noise
matrix: ymi = xmi · ✓m
i .
To be fully Bayesian  one would also sample and average over different dropout masks ✓ during
testing  i.e. perform Bayesian ensembling. Although this procedure can be used to slightly improve
the ﬁnal accuracy  it is usually avoided. Bayesian ensembling essentially requires sampling of
different copies of neural networks  which makes the evaluation K times slower for averaging over
K samples. Instead  during the testing phase in most dropout-based techniques the noise variable
✓ is replaced with its expected value. In this paper we follow the same approach and replace all
non-pruned ✓i with their expectations (20) during testing. The derivation of the expectation of the
truncated log-normal distribution is presented in Appendix C.

E✓i =

exp(µi + 2

(i)  (↵i)✓ 2

i /2)

i + µi  a

i

◆  ✓ 2

i + µi  b

i

◆

(20)

We tried to use Bayesian ensembling with this model  and experienced almost no gain of accuracy. It
means that the variance of the learned approximate posterior distribution is low and does not provide
a rich ensemble.
Throughout the paper we introduced the SBP dropout layer for the case when input objects are
represented as one-dimensional vectors x. When deﬁned like that  it would induce general sparsity
on the input vector x. It works as intended for fully-connected layers  as a single input feature
corresponds to a single output neuron of a preceding fully-connected layer and a single output neuron
of the following layer. However  it is possible to apply the SBP layer in a more generic setting. Firstly 
if the input object is represented as a multidimensional tensor X with shape I1 ⇥ I2 ⇥···⇥ Id  the
noise vector ✓ of length I = I1 ⇥ I2 ⇥···⇥ Id can be reshaped into a tensor with the same shape.
Then the output tensor Y can be obtained as a component-wise product of the input tensor X and
the noise tensor ✓. Secondly  the SBP layer can induce any form of structured sparsity on this input
tensor X. To do it  one would simply need to use a single random variable ✓i for the group of input
features that should be removed simultaneously. For example  consider an input tensor X H⇥W⇥C
that comes from a convolutional layer  H and W being the size of the image  and C being the number
of channels. Then  in order to remove redundant ﬁlters from the preceding layer (and at the same
time redundant channels from the following layer)  one need to share the random variables ✓ in the
following way:

(21)
Note that now there is one sample ✓ 2 RC for one object X H⇥W⇥C on each training step. If
the signal-to-noise ratio becomes lower than 1 for a component ✓c  that would mean that we can

✓c ⇠ LogN[a b](✓c | µc  2
c )

yhwc = xhwc · ✓c

6

Figure 1: The value of the SGVB for the case of
ﬁxed variational parameter µ = 0 (blue line) and
for the case when both variational parameters µ
and  are trained (green line)

Figure 2: The learned signal-to-noise ratio for
image features on the MNIST dataset.

permanently remove the c-th channel of the input tensor  and therefore delete the c-th ﬁlter from the
preceding layer and the c-th channel from the following layer. All the experiments with convolutional
architectures used this formulation of SBP. This is a general approach that is not limited to reducing
the shape of the input tensor. It is possible to obtain any ﬁxed pattern of group-wise sparsity using
this technique.
Similarly  the SBP layer can be applied in a DropConnect fashion. One would just need to multiply
the weight tensor W by a noise tensor ✓ of similar shape. The training procedure remains the same.
It is still possible to enforce any structured sparsity pattern for the weight tensor W by sharing the
random variables as described above.

5 Experiments
We perform an evaluation on different supervised classiﬁcation tasks and with different architectures
of neural networks including deep VGG-like architectures with batch normalization layers. For each
architecture  we report the number of retained neurons and ﬁlters  and obtained acceleration. Our
experiments show that Structured Bayesian Pruning leads to a high level of structured sparsity in
convolutional ﬁlters and neurons of DNNs without signiﬁcant accuracy drop. We also demonstrate
that optimization w.r.t. the full set of variational parameters (µ  ) leads to improving model quality
and allows us to perform sparsiﬁcation in a more efﬁcient way  as compared to tuning of only one
free parameter that corresponds to the noise variance. As a nice bonus  we show that Structured
Bayesian Pruning network does not overﬁt on randomly labeled data  that is a common weakness of
non-bayesian dropout networks. The source code is available in Theano [7] and Lasagne  and also in
TensorFlow [1] (https://github.com/necludov/group-sparsity-sbp).
5.1 Experiment Setup

The truncation parameters a and b are the hyperparameters of our model. As our layer is meant for
regularization of the model  we would like our layer not to amplify the input signal and restrict the
noise ✓ to an interval [0  1]. This choice corresponds to the right truncation threshold b set to 0. We
ﬁnd empirically that the left truncation parameter a does not inﬂuence the ﬁnal result much. We use
values a = 20 and b = 0 in all experiments.
We deﬁne redundant neurons by the signal-to-noise ratio of the corresponding multiplicative noise ✓.
See Section 4.4 for more details. By removing all neurons and ﬁlters with the SNR < 1 we experience
no accuracy drop in all our experiments. SBP dropout layers were put after each convolutional layer
to remove its ﬁlters  and before each fully-connected layer to remove its input neurons. As one ﬁlter
of the last convolutional layer usually corresponds to a group of neurons in the following dense layer 
it means that we can remove more input neurons in the ﬁrst dense layer. Note that it means that we
have two consecutive dropout layers between the last convolutional layer and the ﬁrst fully-connected
layer in CNNs  and a dropout layer before the ﬁrst fully-connected layer in FC networks (see Fig. 2).

7

Table 1: Comparison of different structured sparsity inducing techniques on LeNet-5-Caffe and
LeNet-500-300 architectures. SSL [24] is based on group lasso regularization  SparseVD [16])
is a Bayesian model with a log-uniform prior that induces weight-wise sparsity. For SparseVD a
neuron/ﬁlter is considered pruned  if all its weights are set to 0. Our method provides the highest
speed-up with a similar accuracy. We report acceleration that was computed on CPU (Intel Xeon
E5-2630)  GPU (Tesla K40) and in terms of Floating Point Operations (FLOPs).

LeNet-500-300 SSL

Network Method
Original
SparseVD

Error % Neurons per Layer
1.54
784  500  300  10
1.57
537  217  130  10
1.49
434  174  78  10
(ours) StructuredBP 1.55
245  160  55  10
0.80
20  50  800  500
0.75
17  32  329  75
1.00
3  12  800  500
(ours) StructuredBP 0.86
3  18  284  283

Original
SparseVD

LeNet5-Caffe SSL

FLOPs
CPU
GPU
1.00⇥
1.00⇥ 1.00⇥
3.73⇥
1.19⇥ 1.03⇥
6.06⇥
2.21⇥ 1.04⇥
2.33⇥ 1.08⇥ 11.23⇥
1.00⇥
1.00⇥ 1.00⇥
1.48⇥ 1.41⇥
2.19⇥
5.17⇥ 1.80⇥
3.90⇥
5.41⇥ 1.91⇥ 10.49⇥

Table 2: Comparison of different structured sparsity inducing techniques (SparseVD [16]) on VGG-
like architectures on CIFAR-10 dataset. StructuredBP stands for the original SBP model  and
StructuredBPa stands for the SBP model with KL scaling. k is a width scale factor that determines
the number of neurons or ﬁlters on each layer of the network (width(k) = k ⇥ original width)
CPU
FLOPs
1.00⇥ 1.00⇥ 1.00⇥
2.50⇥ 1.69⇥ 2.27⇥
2.71⇥ 1.74⇥ 2.30⇥
3.68⇥ 2.06⇥ 3.16⇥
1.00⇥ 1.00⇥ 1.00⇥
3.35⇥ 2.16⇥ 3.27⇥
3.63⇥ 2.17⇥ 3.32⇥
4.47⇥ 2.47⇥ 3.93⇥

64  64  128  128  256  256  256  512  512  512  512  512  512  512
64  62  128  126  234  155  31  81  76  9  138  101  413  373
64  62  128  126  234  155  31  79  73  9  59  73  56  27
44  54  92  115  234  155  31  76  55  9  34  35  21  280
96  96  192  192  384  384  384  768  768  768  768  768  768  768
96  78  191  146  254  126  27  79  74  9  137  100  416  479
96  77  190  146  254  126  26  79  70  9  71  82  79  49
77  74  161  146  254  125  26  78  66  9  47  55  54  237

Original
SparseVD
StructuredBP
StructuredBPa
Original
SparseVD
StructuredBP
StructuredBPa

7.2
7.2
7.5
9.0
6.8
7.0
7.2
7.8

Units per Layer

(ours)
(ours)
1.5

(ours)
(ours)

Method

Error %

k
1.0

GPU

5.2 More Flexible Variational Approximation

Usually during automatic training of dropout rates the mean of the noise distribution remains ﬁxed.
In the case of our model it is possible to train both mean and variance of the multiplicative noise. By
using a more ﬂexible distribution we obtain a tighter variational lower bound and a higher sparsity
level. In order to demonstrate this effect  we performed an experiment on MNIST dataset with a fully
connected neural network that contains two hidden layers with 1000 neurons each. The results are
presented in Fig. 1.

5.3 LeNet5 and Fully-Connected Net on MNIST

We compare our method with other sparsity inducing methods on the MNIST dataset using a
fully connected architecture LeNet-500-300 and a convolutional architecture LeNet-5-Caffe. These
networks were trained with Adam without any data augmentation. The LeNet-500-300 network
was trained from scratch  and the LeNet-5-Caffe1 network was pretrained with weight decay. An
illustration of trained SNR for the image features for the LeNet-500-3002 network is shown in
Fig. 2. The ﬁnal accuracy  group-wise sparsity levels and speedup for these architectures for different
methods are shown in Table 1.
5.4 VGG-like on CIFAR-10

To prove that SBP scales to deep architectures  we apply it to a VGG-like network [25] that was
adapted for the CIFAR-10 [9] dataset. The network consists of 13 convolutional and two fully-
connected layers  trained with pre-activation batch normalization and Binary Dropout. At the start of
the training procedure  we use pre-trained weights for initialization. Results with different scaling
of the number of units are presented in Table 2. We present results for two architectures with
different scaling coefﬁcient k 2{ 1.0  1.5} . For smaller values of scaling coefﬁcient k 2{ 0.25  0.5}
we obtain less sparse architecture since these networks have small learning capacities. Besides
the results for the standard StructuredBP procedure  we also provide the results for SBP with KL
scaling (StructuredBPa). Scaling the KL term of the variational lower bound proportional to the
computational complexity of the layer leads to a higher sparsity level for the ﬁrst layers  providing

1A modiﬁed version of LeNet5 from [11]. Caffe Model speciﬁcation: https://goo.gl/4yI3dL
2Fully Connected Neural Net with 2 hidden layers that contains 500 and 300 neurons respectively.

8

more acceleration. Despite the higher error values  we obtain the higher value of true variational
lower bound during KL scaling  hence  we ﬁnd its another local maximum.

5.5 Random Labels
A recent work shows that Deep Neural Networks have so much capacity that they can easily memorize
the data even with random labeling [26]. Binary dropout as well as other standard regularization
techniques do not prevent the networks from overﬁtting in this scenario. However  recently it was
shown that Bayesian regularization may help [16]. Following these works  we conducted similar
experiments. We used a Lenet5 network on the MNIST dataset and a VGG-like network on CIFAR-10.
Although Binary Dropout does not prevent these networks from overﬁtting  SBP decides to remove
all neurons of the neural network and provides a constant prediction. In other words  in this case SBP
chooses the simplest model that achieves the same testing error rate. This is another conﬁrmation that
Bayesian regularization is more powerful than other popular regularization techniques.
6 Conclusion
We propose Structured Bayesian Pruning  or SBP  a dropout-like layer that induces multiplicative
random noise over the output of the preceding layer. We put a sparsity-inducing prior over the noise
variables and tune the noise distribution using stochastic variational inference. SBP layer can induce
an arbitrary structured sparsity pattern over its input and provides adaptive regularization. We apply
SBP to cut down the number of neurons and ﬁlters in convolutional neural networks and report
signiﬁcant practical acceleration with no modiﬁcation of the existing software implementation of
these architectures.

Acknowledgments
We would like to thank Christos Louizos and Max Welling for valuable discussions. Kirill Neklyudov
and Arsenii Ashukha were supported by HSE International lab of Deep Learning and Bayesian Meth-
ods which is funded by the Russian Academic Excellence Project ’5-100’. Dmitry Molchanov was
supported by the Ministry of Education and Science of the Russian Federation (grant 14.756.31.0001).
Dmitry Vetrov was supported by the Russian Science Foundation grant 17-11-01027.

References
[1] Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro  Greg S
Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  et al. Tensorﬂow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint arXiv:1603.04467  2016.

[2] Michael Figurnov  Maxwell D Collins  Yukun Zhu  Li Zhang  Jonathan Huang  Dmitry Vetrov  and Ruslan
Salakhutdinov. Spatially adaptive computation time for residual networks. arXiv preprint arXiv:1612.02297 
2016.

[3] Mikhail Figurnov  Aizhan Ibraimova  Dmitry P Vetrov  and Pushmeet Kohli. Perforatedcnns: Acceleration
through elimination of redundant convolutions. In Advances in Neural Information Processing Systems 
pages 947–955  2016.

[4] Timur Garipov  Dmitry Podoprikhin  Alexander Novikov  and Dmitry Vetrov. Ultimate tensorization:

compressing convolutional and fc layers alike. arXiv preprint arXiv:1611.03214  2016.

[5] Song Han  Huizi Mao  and William J Dally. Deep compression: Compressing deep neural networks with

pruning  trained quantization and huffman coding. arXiv preprint arXiv:1510.00149  2015.

[6] Max Jaderberg  Andrea Vedaldi  and Andrew Zisserman. Speeding up convolutional neural networks with

low rank expansions. arXiv preprint arXiv:1405.3866  2014.

[7] Bergstra James  Breuleux Olivier  Bastien Frédéric  Lamblin Pascal  and Pascanu Razvan. Theano: a cpu
and gpu math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference
(SciPy).

[8] Diederik P Kingma  Tim Salimans  and Max Welling. Variational dropout and the local reparameterization
trick. In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and R. Garnett  editors  Advances in Neural
Information Processing Systems 28  pages 2575–2583. Curran Associates  Inc.  2015.

[9] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.

9

[10] Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Proceedings of

the IEEE Conference on Computer Vision and Pattern Recognition  pages 2554–2564  2016.

[11] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[12] Ekaterina Lobacheva  Nadezhda Chirkova  and Dmitry Vetrov. Bayesian sparsiﬁcation of recurrent neural

networks. arXiv preprint arXiv:1708.00077  2017.

[13] Christos Louizos. Smart regularization of deep architectures. Master’s thesis  University of Amsterdam 

2015.

[14] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan Wierstra 
and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 
2013.

[15] Dmitry Molchanov  Arseniy Ashuha  and Dmitry Vetrov. Dropout-based automatic relevance determination.

In Bayesian Deep Learning workshop  NIPS  2016.

[16] Dmitry Molchanov  Arsenii Ashukha  and Dmitry Vetrov. Variational dropout sparsiﬁes deep neural

networks. arXiv preprint arXiv:1701.05369  2017.

[17] Alexander Novikov  Dmitrii Podoprikhin  Anton Osokin  and Dmitry P Vetrov. Tensorizing neural networks.

In Advances in Neural Information Processing Systems  pages 442–450  2015.

[18] Mohammad Rastegari  Vicente Ordonez  Joseph Redmon  and Ali Farhadi. Xnor-net: Imagenet classiﬁ-
cation using binary convolutional neural networks. In European Conference on Computer Vision  pages
525–542. Springer  2016.

[19] Nitish Srivastava  Geoffrey E Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov. Dropout:
a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research 
15(1):1929–1958  2014.

[20] Christian Szegedy  Wei Liu  Yangqing Jia  Pierre Sermanet  Scott Reed  Dragomir Anguelov  Dumitru
Erhan  Vincent Vanhoucke  and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition  pages 1–9  2015.

[21] Karen Ullrich  Edward Meeds  and Max Welling. Soft weight-sharing for neural network compression.

arXiv preprint arXiv:1702.04008  2017.

[22] Li Wan  Matthew Zeiler  Sixin Zhang  Yann L Cun  and Rob Fergus. Regularization of neural networks
using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) 
pages 1058–1066  2013.

[23] Sida I Wang and Christopher D Manning. Fast dropout training. In ICML (2)  pages 118–126  2013.

[24] Wei Wen  Chunpeng Wu  Yandan Wang  Yiran Chen  and Hai Li. Learning structured sparsity in deep

neural networks. In Advances in Neural Information Processing Systems  pages 2074–2082  2016.

[25] Sergey Zagoruyko. 92.45 on cifar-10 in torch  2015.

[26] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding deep

learning requires rethinking generalization. arXiv preprint arXiv:1611.03530  2016.

10

,Kirill Neklyudov
Dmitry Molchanov
Dmitry Vetrov