2018,Bilevel Distance Metric Learning for Robust Image Recognition,Metric learning  aiming to learn a discriminative Mahalanobis distance matrix M that can effectively reflect the similarity between data samples  has been widely studied in various image recognition problems. Most of the existing metric learning methods input the features extracted directly from the original data in the preprocess phase. What's worse  these features usually take no consideration of the local geometrical structure of the data and the noise existed in the data  thus they may not be optimal for the subsequent metric learning task. In this paper  we integrate both feature extraction and metric learning into one joint optimization framework and propose a new bilevel distance metric learning model. Specifically   the lower level characterizes the intrinsic data structure using graph regularized sparse coefficients  while the upper level forces the data samples from the same class to be close to each other and pushes those from different classes far away. 
 In addition  leveraging the KKT conditions and the alternating direction method (ADM)  we derive an efficient algorithm to solve the proposed new model. Extensive experiments on various occluded datasets demonstrate the effectiveness and robustness of our method.,Bilevel Distance Metric Learning for

Robust Image Recognition

1 School of Electronic Engineering  Xidian University  Xi’an  Shaanxi  China

2 Electrical and Computer Engineering  University of Pittsburgh  USA  3 JDDGlobal.com

Jie Xu1 2  Lei Luo2  Cheng Deng1 ∗  Heng Huang2 3∗

jie.xu@pitt.edu leiluo2017@pitt.edu

chdeng.xd@gmail.com heng.huang@pitt.edu

Abstract

Metric learning  aiming to learn a discriminative Mahalanobis distance matrix M
that can effectively reﬂect the similarity between data samples  has been widely
studied in various image recognition problems. Most of the existing metric learning
methods input the features extracted directly from the original data in the prepro-
cess phase. What’s worse  these features usually take no consideration of the local
geometrical structure of the data and the noise that exists in the data  thus they
may not be optimal for the subsequent metric learning task. In this paper  we
integrate both feature extraction and metric learning into one joint optimization
framework and propose a new bilevel distance metric learning model. Speciﬁcally 
the lower level characterizes the intrinsic data structure using graph regularized
sparse coefﬁcients  while the upper level forces the data samples from the same
class to be close to each other and pushes those from different classes far away.
In addition  leveraging the KKT conditions and the alternating direction method
(ADM)  we derive an efﬁcient algorithm to solve the proposed new model. Exten-
sive experiments on various occluded datasets demonstrate the effectiveness and
robustness of our method.

1

Introduction

Metric learning problem is concerned with learning an optimal distance matrix M that captures the
important relationships among data for a given task  i.e.  assigning smaller distances between similar
items and larger distances between dissimilar items. Generally  metric learning can be formulated
as a minimal optimization about the objective function: µReg(M) + Loss(M A)  where Reg(M)
is a regularization term on the distance matrix M and Loss(M A) is a loss function that penalizes
constraints. Different choices of regularization terms and constraints result in various metric learning
methods  e.g.  large-margin nearest neighbor (LMNN) [16]  information-theoretic metric learning
(ITML) [4]  FANTOPE [7]  CAP [5]  etc. More recent works focus on using maximum correntropy
criterion [18]  smoothed wasserstein distance [19]  matrix variate Gaussian mixture distribution [11]
for metric learning formulations to improve the robustness. Although these methods achieve great
success  they all mainly focus on improving the discriminability of the distance matrix M but ignore
the discriminating power of input features. Especially  the descriptors of the sample pairs they address
are usually extracted directly from the original data in the preprocess phase without considering the
local geometrical structure of the data  thus such descriptors may not be optimal for the subsequent
metric learning task.
Besides metric learning methods  many other machine learning tasks such as clustering and dictionary
learning also suffer from the above limitation. To address this issue  the recently proposed solution

∗J.X. and L.L. made equal contributions. C.D. and H.H. are corresponding authors.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

is to adopt the strategy of joint learning or bilevel model  and fortunately  great achievements have
been made by many researchers. Wang et al. [15] propose a joint optimization framework in terms of
both feature extraction and discriminative clustering. They utilize graph regularized sparse codes as
the features  and formulate sparse coding as the constraint for clustering. Zhou et al. [23] present a
novel bilevel model-based discriminative dictionary learning method for recognition tasks. The upper
level directly minimizes the classiﬁcation error  while the lower level uses the sparsity term and the
Laplacian term to characterize the intrinsic data structure. Yang et al. [20] propose a bilevel sparse
coding model for coupled feature spaces  where they aim to learn dictionaries for sparse modeling
in both spaces while enforcing some desired relationships between the two signal spaces. All these
models beneﬁt from the joint learning strategy or the bilevel model and achieve an overall optimality
to a great extent. Inspired by these works  we propose to extract features and learn the Mahalanobis
distance matrix M through a uniﬁed joint optimization model.
How to choose the feature extraction model is also an important problem. Although metric learning
task aims to learn a discriminative M  it would be better if the input features are also of discriminating
power. The common choice is principal component analysis (PCA) feature  which is able to reduce the
data dimension and identify the most important features [1]. However  PCA feature is not necessarily
discriminative and also may loss the useful information. More recently  sparse coefﬁcients prove to be
an effective feature which is not only robust to noise but also scalable to high dimensional data [17].
Furthermore  motivated by recent progress in manifold learning  Zheng et al. [22] incorporate
the graph Laplacian into the sparse coding objective function as a regularizer  achieving more
discriminating power compared with traditional sparse coding algorithms.
In this paper  we integrate the graph regularized sparse coding model into the distance metric learning
framework and propose our new bilevel model. The lower level focus on detecting the underlying
data structure  while the upper level directly forces the data samples from the same class to be close to
each other and pushes those samples from different classes far away. Note that the input data samples
of the upper level are represented by the sparse coefﬁcients learnt from the lower level model. And
beneﬁting from the feature extraction operation of the lower level model  the new features become
more robust to noise with the sparsity norm and more discriminative with the Laplacian graph term.
In addition  to solve our bilevel model  we transform the lower level problem of the proposed model
into equality and inequality constraints and then apply ADM to solve it. Extensive experiments
on various occluded datasets indicate that the proposed bilevel model can achieve more promising
performance than other related methods.
Notations: Let S+ denotes the set of real-valued symmetric positive semi-deﬁnite (PSD) matrices.
For matrices A and B  denote the Frobenius inner product by (cid:104)A  B(cid:105) = T r(A(cid:62)B)  where ‘T r’
denotes the trace of a matrix. For a given vector a = (a1  a2  ...  ad)(cid:62)  diag(a) = A corresponds to
a squared diagonal matrix such that ∀i  Ai i = ai. ek ∈ Rk represents a unit vector of length k  and I
is a unit matrix. Finally  for x ∈ R  let [x]+ = max(0  x).

2 Bilevel Distance Metric Learning

2.1 Large Margin Nearest Neighbor
Let {(x1  y1)  ...  (xn  yn)} ∈ Rd × C be a set of labeled training data with discrete labels C =
{1  ...  c}  where n is the number of samples. Most of the metric learning methods aim to learn

a metric  such as widely used Mahalanobis distance dM(xi  xj) =(cid:112)(xi − xj)(cid:62)M(xi − xj)  to

effectively reﬂect the similarity between data.
Large margin nearest neighbor (LMNN) [16]  as one of the most widely used metric learning methods 
requires the learned Mahalanobis distance to satisfy two objectives  i.e.  samples from the same class
are forced to be close to each other and those from different classes are pushed far away. If we denote
the similar pairs by S and triplet constraints by T as:

S = {(xi  xj) : yi = yj and xj belongs to the k-neighborhood of xi} 
T = {(xi  xj  xk) : (xi  xj) ∈ S  yi (cid:54)= yk} 

(1)

(2)

then LMNN model can be formulated as:

(cid:88)

(i j)∈S

(1 − λ)

min
M∈S+

d2
M(xi  xj) + λ

[1 + d2

M(xi  xj) − d2

M(xi  xk)]+ 

(cid:88)

(i j k)∈T

2

(a) Original data X

(b) Sparse coefﬁcients A

Figure 1: Original data X and the corresponding sparse coefﬁcients A learned from the proposed
bilevel distance metric learning model. The x-axis represents different samples belonged to eight
classes.

where λ ∈ [0  1] controls the relative weight between two terms. The objective function in Eq. (2)
pulls the “target” neighbors whose labels are the same as xi’s toward xi and pushes away the
“impostor” neighbors whose labels are different from xi’s.
Although LMNN achieves good results  it learns the distance matrix to characterize the point-to-
point distance which is sensitive to the noise. Furthermore  the descriptors of the sample pairs
it addresses are usually extracted directly from the original data in the preprocess phase without
considering the local geometrical structure of the data. Thus such features may not be optimal for the
subsequent metric learning task. To address these problems  in this paper  we propose a bilevel model
which jointly learns the distance matrix M and extracts features under a sparse representation-based
framework.

2.2 Bilevel Distance Metric Learning

(i j)∈S
1
2

(i j k)∈T

(3)

Sparse representations prove to be an effective feature for classiﬁcation. Also  some researchers
suggest that the contribution of one sample to the reconstruction of another sample is a good indicator
of similarity between these two samples [3]. Thus the reconstruction coefﬁcients can be used to
constitute the similarity graph. Inspired by these ﬁndings  we integrate both sparse representation and
graph regularization into a metric learning framework and propose our new bilevel distance metric
learning model.
We assume all the data samples X = [x1  x2  ...  xn] ∈ Rd×n are represented by their cor-
responding sparse coefﬁcients A = [a1  a2  ...  an] ∈ Rk×n based on a learned dictionary
U = [u1  u2  ...  uk] ∈ Rd×k. Then the proposed bilevel distance metric learning model can
be expressed as follows:
(1 − λ)

(cid:88)

(cid:88)

d2
M(ai  aj) + λ

[ξ + d2

M(ai  aj) − d2

M(ai  ak)]+

min
M∈Sd

+ U

s.t. A = arg min
A

||X − UA||2

F + α||A||1 +

β
2

T r(ALA(cid:62)) 

||ui||2

2 ≤ 1 ∀i 

where the Laplacian term T r(ALA(cid:62)) is introduced to guarantee the sparse coefﬁcients can capture
the geometric structure of the data. L is the graph Laplacian matrix constructed from the label vector
Y = [y1  y2  ...  yn] ∈ Rn. λ  α and β are three regularization parameters.
In our bilevel model (3)  the upper level feeds the representation (ai  aj  ak) of the triplet constraint
(xi  xj  xk) into the LMNN model and directly minimizes the loss function. The lower level tries to
capture the intrinsic data structure. Note that the Laplacian matrix L is constructed in a supervised
way  thus the data structure can be well preserved even if there exists noise in the data. By solving
the above optimization problem (3)  a recognition-driven dictionary U can be learnt and accordingly
leading to a well representative sparse coefﬁcients A. In the meantime  we can also obtain a good
Mahalanobis distance matrix M with the new discriminative feature A.
It is worth mentioning that the sparsity penalty and Laplacian regularization encourage the group
sparsity of coefﬁcients  thus the samples from the same class are forced to have similar sparse

3

12345678Class labelOriginal feature-0.1-0.0500.050.10.1512345678Class labelCoefficients-0.200.20.4representations and those from different classes are to have dissimilar sparse codes. For clarity  we
show the original data (including eight classes) and its corresponding sparse coefﬁcients learnt by
our bilevel model in Fig. 1. The coefﬁcients equipped with this useful property make the upper level
easier to fulﬁll its mission which is to force the data samples from the same class to be close to each
other and pushes those samples from different classes far away.

2.3 Optimization

We use the alternating direction method (ADM) to solve the optimization problem (3) after some
delicate reformulations.
Let A = B − C  where B ∈ Rk×n and C ∈ Rk×n are two nonnegative matrices such that B takes
all the positive elements in A and the remaining elements of B are set to 0  while C does the same for
the negative elements in A (after negation). Then the lower level optimization problem of model (3)
can be transformed into the following problem:

1
2

||X − UPZ||2

F + αe(cid:62)

min

(4)
where Z = [B; C] ∈ R2k×n and P = [I −I] ∈ Rk×2k. Obviously  problem (4) is a convex problem 
which can be replaced by its KKT conditions [23]. Then we obtain the following equivalent model:

2kZen +

Z

s.t. Z ≥ 0 

β
2

T r(PZLZ(cid:62)P(cid:62)) 
(cid:88)

[ξ + d2

M∈Sd

min
+ Z B U

(1 − λ)

d2
P(cid:62)MP(zi  zj) + λ

(i j k)∈T

P(cid:62)MP(zi  zj) − d2

P(cid:62)MP(zi  zk)]+

(cid:88)

(i j)∈S
UPZ − P

s.t. P

(cid:62)

U

(cid:62)
B (cid:12) Z = 0  Z ≥ 0  B ≤ 0 

(cid:62)
X + αE + βP
PZL + B = 0 
2 ≤ 1 
||ui||2

U

(cid:62)

(cid:62)

∀i ∈ {1  2  ...  k}.

(5)

where B ∈ R2k×n is the Lagrange multiplier matrix and B satisﬁes the constraint B ≤ 0. E ∈
R2k×n is an all-one matrix.
With all these steps  the proposed bilevel distance metric learning model (3) is reformulated to a
unilevel optimization problem which can be solved by ADM. We introduce two auxiliary variables
W and S and relax (5) to the following problem:

min

M∈Sd

+ Z B W S U

(1 − λ)

d2
P(cid:62)MP(zi  zj) + λ

[ξ + d2

P(cid:62)MP(zi  zj) − d2

P(cid:62)MP(zi  zk)]+

(cid:88)

(i j k)∈T

s.t. P

(cid:62)

U

(cid:62)
Z − S = 0  S ≥ 0  B ≤ 0 

X + αE + βWL + B = 0  B (cid:12) S = 0  P
∀i ∈ {1  2  ...  k}.

||ui||2

2 ≤ 1 

U

(cid:62)

(cid:62)

(cid:62)

PZ − W = 0 

(6)

(cid:88)

(i j)∈S
UPZ − P

(cid:88)

(i j)∈S
(cid:62)

(cid:62)

(cid:88)

(i j k)∈T

The augmented Lagrangian function of problem (6) is:

L(Z  B  W  S  U  M  R1  R2  R3  R4  µ)
=(1 − λ)

d2
P(cid:62)MP(zi  zj) + λ

[ξ + d2

P(cid:62)MP(zi  zj) − d2

P(cid:62)MP(zi  zk)]+

X + αE + βWL + B(cid:105) + (cid:104)R2  B (cid:12) S(cid:105) + (cid:104)R3  P

(cid:62)

PZ − W(cid:105)

(7)

(cid:62)

U

+(cid:104)R1  P
+(cid:104)R4  Z − S(cid:105) +
(||B (cid:12) S||2

(cid:62)
U
UPZ − P
(cid:62)

UPZ − P
(cid:62)
U
PZ − W||2

||P
F + ||P
(cid:62)

µ
2

+

µ
2

F + ||Z − S||2

F ) 

(cid:62)

(cid:62)

U

X + αE + βWL + B||2

F

where R1 ∼ R4 are Lagrange multipliers  and µ ≥ 0 is the penalty parameter.
We alternately update the variables Z  B  W  S  U and M in each iteration by minimizing the
augmented Lagrangian function of problem (6) with other variables ﬁxed. We initialize the Maha-
lanobis distance matrix M as a unit matrix. The initialization processes of the dictionary U and the
coefﬁcients A are same as in FDDL [21]. More speciﬁcally  the iterations go as follows:
Step 1: Update Z by ﬁxing B  W  S  U and M. For each zi ∈ Z  we have

zi = G−1

1 (qi + (1 − λ)P(cid:62)MP

zj + λP(cid:62)MP

(zj − zk)) 

(8)

(cid:88)

(i j k)∈T

(cid:88)

(i j)∈S

4

where G1 = 2µP(cid:62)U(cid:62)UU(cid:62)UP + 2µP(cid:62)P + µI + (1− λ)(cid:80)

(i j)∈S P(cid:62)MP. qi is the i-th column
of Q  Q = µP(cid:62)U(cid:62)UP(P(cid:62)U(cid:62)X − αE − βWL − B − R1/µ) − P(cid:62)P(R3 − µW) − R4 + µS.
Step 2: Update B by ﬁxing Z  W  S  U and M.

B = −Π+ ((S (cid:12) R2/µ + G2 + βWL + R1/µ) (cid:11) (S (cid:12) S + E))  

(9)
where G2 = P(cid:62)U(cid:62)UPZ − P(cid:62)U(cid:62)X + αE. Π+(·) is an operator that projects a matrix onto the
nonnegative cone  which can be deﬁned as follows:

(cid:26)Xij 

0 

if Xij ≥ 0;
otherwise.

Π+(Xij) =

(10)

(11)

(12)

Step 3: Update W by ﬁxing Z  B  S  U and M.

W =(cid:2)P(cid:62)PZ + R3/µ − β(G2 + B + R1/µ)L(cid:62)(cid:3)(cid:0)β2LL(cid:62) + I(cid:1)−1

.

Step 4: Update S by ﬁxing Z  B  W  U and M.

S = Π+ ((Z + R4/µ − B (cid:12) R2/µ) (cid:11) (B (cid:12) B + E)) .

Step 5: Update U by ﬁxing Z  B  W  S and M. We need to solve the following problem:

with (cid:79)U = 2(cid:0)U(G(cid:62)

(13)
U = arg min
U∈Ω
2 ≤ 1  i = 1  ...  k}. The problem (13) is a quartic polynomial minimization
where Ω = {U | ||Ui||2
problem. It is difﬁcult to compute its exact solution. So we use the projected gradient descent method
to update U:

||G2 + βWL + B + R1/µ||2
F  

6 + G6) + G(cid:62)

4 + G4) + U(G(cid:62)

(14)
8 + G8) −
3 P(cid:62) + 4XX(cid:62)U  where G3 = αE + βWL + B + R1/µ  G4 = PZZ(cid:62)P(cid:62)U(cid:62)U 
1 P(cid:62). ΠΩ(U) is

2XG(cid:62)
G5 = U(cid:62)UPZZ(cid:62)P(cid:62)  G6 = PZX(cid:62)U  G7 = U(cid:62)UPZX(cid:62)  G8 = PZG(cid:62)
the projection of the matrix U onto Ω and η1 is a step size.
Step 6: Update M by ﬁxing Z  B  W  S and U. The objective function is linear with respect to M  
we directly adopt subgradient descent to update M in each iteration. As before  set zij = zi − zj 
then the subgradient of problem (6) with respect to M can be calculated as follows:
ijP(cid:62) − Pzikz(cid:62)

(cid:79)M = (1 − λ)

ijP(cid:62) + λ

(Pzijz(cid:62)

Pzijz(cid:62)

ikP(cid:62)) 

(cid:88)

(cid:88)

U = ΠΩ(U − η1(cid:79)U) 

5 + G5)(cid:1) − 4(cid:0)U(G(cid:62)

(cid:1) + 2U(G(cid:62)

7

(15)

(i j)∈S

(i j k)∈T +

where T + denotes the subset of constraints in T that is larger than 0 in function (6). After each
iteration  M is projected onto the positive semideﬁnite cone:
(M − η2(cid:79)M) 

(16)
(M) is the orthogonal projection of the matrix M ∈ Sd onto the

M = ΠSd

+

+. The speciﬁc procedures are summarized in Algorithm 1.

where η2 is a step size  and ΠSd
positive semideﬁnite cone Sd

+

2.4 Classiﬁcation Scheme

When problem (6) is solved  we obtain a dictionary U and the sparse coefﬁcients A = PZ of training
samples. In the testing phase  given a testing sample x  we ﬁrst compute its sparse coefﬁcient by the
vector form of the lower level optimization model:

a∗ = arg min

a

1
2

||x − Ua||2

F + α||a||1 +

β
2

qi||a − ai||2
2 

(17)

(cid:88)

i∈Ns(x)

where Ns(x) denotes the set of s nearest neighbors of x and the s nearest neighbors are chosen from
training samples X. ai is the coefﬁcient of the i-th training sample xi. qi is the weight between the

5

Algorithm 1 Algorithm to solve Eq. (6)
1: Input: S T   X ∈ Rd×n  L  λ  α  β
2: Output: M ∈ Sd
3: Initialization: M0  U0  A0  Z0 = P†A0  S0 = Z0  W0 = P(cid:62)PZ0  and B0 =
P(cid:62)(U0)(cid:62)X − P(cid:62)(U0)(cid:62)U0PZ0 − αE − βW0L. Set R1 = 0d  R2 = 0d  R3 = 0d 
R4 = 0d  µ0 = 1e − 3  µmax = 1e + 8  ρ = 1.3  ε1 = 1e − 4  ε2 = 1e − 5  and t = 0.

+  U  A

4: repeat
5:
6:

1+µ(cid:0)P(cid:62)(Ut+1)(cid:62)Ut+1PZt+1 − P(cid:62)(Ut+1)(cid:62)X + αE + βWt+1L + Bt+1(cid:1) 

Steps 1∼6;
Update Lagrange multipliers and µt+1:
Rt+1
2 + µ(Bt+1 (cid:12) St+1)  Rt+1
Rt+1
4 + µ(Zt+1 − St+1)  µt+1 = min(ρµt  µmax);
Rt+1
t ← t + 1;
7:
8: until Converge

3 + µ(P(cid:62)PZt+1 − Wt+1) 

1 = Rt
2 = Rt
4 = Rt

3 = Rt

training sample xi and the test sample x. Note that in the training phase  we construct the weight
matrix Q as follows:

(cid:26) 1  if samples xi and xj belong to the same class 

(18)

Qij =

0  otherwise.

and Tii =(cid:80)

Then we compute the corresponding Laplacian matrix L = T − Q  where T is a diagonal matrix
j Qij. In the testing phase  we ﬁnd s nearest neighbors from training set for each test
sample. In the experiment  we set s = 5 and the weight qi = 1 (∀i ∈ Ns(·)).
After the coefﬁcient a∗ of the test sample x is obtained  the squared Mahalanobis distance between
the test sample x and the training sample xi can be calculated as:

M(a∗  ai) = (a∗ − ai)(cid:62)M(a∗ − ai) 
d2

(19)

where M is the learned optimal distance matrix. The test sample x is then classiﬁed to the class
where its nearest training sample belongs.

2.5 Convergence Analysis

There are lots of researchers focusing on the convergence of ADM with two blocks of variables.
However  there is still no afﬁrmative convergence proof for multi-block convex minimization problem
where the objective function consists of more than two separable convex functions. The recent
solution is to use an additional dual step-size parameter µ in updating Lagrange multipliers (as shown
in Algorithm 1). This scheme is simple and effective because it not only requires no additional
assumptions associated with the objective function but also guarantees the convergence of ADM with
multi-block variable under mild assumptions [9]. For these reasons  it is enough that we only need to
choose a proper step-size parameter µ and termination conditions.
To further illustrate the convergence of ADM in solving the proposed model (6)  we conduct
several experiments on three datasets  including NUST Robust Face database (NUST-RF) [2]  OSR
dataset [13] and PubFig database [6]. Note that there are two environments in NUST-RG database 
i.e.  indoor and outdoor. The objective function values versus number of iterations are shown in Fig. 2.
From the ﬁgure we can see that the objective values reduce reasonably well.

3 Experimental Results

We evaluate the proposed algorithm over different classiﬁcation databases  including real-world
malicious occlusion datasets  contiguous occlusion and corruption datasets. There are two main goals
in our experiments: ﬁrst  we will show that our bilevel model is more robust to be applied to solve
real-world occlusion problems; second  our model is able to outperform the related metric learning
methods.

6

(a) Indoor

(b) Outdoor

(c) OSR

(d) PubFig

Figure 2: Objective value vs. the number of iterations.

(a) Indoor

(b) Outdoor

Figure 3: Cropped images of one subject captured in two environments in NUST-RF database  i.e. 
(a) indoor  and (b) outdoor.

3.1 Compared Methods

We compare the proposed bilevel distance metric learning model with the following methods: the
base-line KNN [14]  LMNN [16]  FANTOPE [7]  CAP [5] and RML [10]. Speciﬁcally  as baselines 
we consider the most relevant technique from the literature  i.e.  k-nearest neighbor method (KNN).
KNN computes Euclidean distance to measure the similarity between any two images. LMNN is
one of the most widely-used Mahalanobis distance metric learning methods  which uses labeled
information to generate triplet constraints. FANTOPE method is based on LMNN  and it utilizes
a fantope regularization which minimizes sum of k smallest singular values of distance matrix M.
Same as FANTOPE method  CAP method is also based on LMNN  and it uses a capped trace norm
to penalize the singular values of distance matrix M that are less than a threshold adaptively learned
in the optimization. RML learns the discriminative distance matrix by enforcing a margin between
the inter-class sparse reconstruction residual and intra-class sparse reconstruction residual.
For all metric learners  we use 5-fold cross validation and gauge the average accuracy and stan-
dard deviation as ﬁnal performance. All the regularization parameters are tuned from range
{10−4  10−3  10−2  10−1  1  10  102}. For CAP and FANTOPE methods  the parameter rank of
distance matrix M is tuned from [10 : 5 : 30]. For a fair comparison  we specify 1 “target” neighbor
for each training sample for all LMNN related methods. In testing phase  we use 1-NN method.

3.2 Real-World Malicious Occlusion

First we consider the NUST Robust Face database (NUST-RF) [2]. It is mainly designed for robust
face recognition under various occlusions. Except occlusion  it also includes variations of illumination 
expression and pose. We use a subset face images of NUST-RF database  and there are 50 subjects
captured in two environments (indoor and outdoor). We manually crop the face portion of the image
and then normalize it to 80 × 60 pixels. Fig. 3 shows an example of several selected images of one
subject. We extracted LOMO features for each image [8]  which not only achieve some invariance to
viewpoint changes  but also capture local region characteristics of a person. PCA is further applied to
reduce the feature dimension to 30.
Table 1 shows the recognition performance of different methods on NUST-RF database of two
environments. Obviously  our method outperforms other competing methods in indoor case and gets

7

05101520# of iterations00.511.52Objective value10401020# of iterations020004000600080001000012000Objective value05101520# of iterations0246810Objective value10401020# of iterations02004006008001000Objective valueTable 1: Recognition accuracy (%) and standard deviation of different methods on NUST-RF database
in two environments.
KNN [14]
36.14 ± 2.70
45.24 ± 1.51

FANTOPE [7]
41.87 ± 2.50
58.72 ± 1.33

RML [10]
35.56 ± 3.04
42.81 ± 2.16

LMNN [16]
36.20 ± 3.30
46.01 ± 2.06

Indoor
Outdoor

CAP [5]

41.70 ± 2.86
58.34 ± 1.33

Proposed
47.84 ± 1.18
58.21 ± 1.68

(a) OSR dataset

(b) PubFig dataset

Figure 4: Example pairs of images from two datasets  i.e.  (a) OSR dataset  (b) PubFig dataset.
For each subﬁgure  from left to right: original image  its noisy versions (with sparse noise  regular
occlusion and irregular occlusion  respectively).

comparable results with FANTOPE method in outdoor case. This is because the proposed model
jointly extracts features under a sparse-representation model and performs distance metric learning
task at the same time. In this way  our features are more robust to noise  thus we can get better results
than LMNN which is only based on extracted features. For FANTOPE and CAP methods  they
also achieve relatively good results because the low-rank regularization on M ﬁts this face database
just right. Moreover  as shown in Fig. 3  if occlusions exist  it is unlikely that the test image will
be very close to any single training image of the same class  so that the KNN classiﬁer performs
poorly. Although LMNN can improve the recognition rates compared to KNN  their improvements
are limited. RML also performs poorly because it is based on the MSE criterion which is sensitive to
outliers.

3.3 Sparse Noise and Contiguous Occlusion

Next we did three groups of occlusion experiments associated with two datasets  i.e.  OSR dataset [13]
and PubFig database [6]  to validate the robustness of the proposed algorithm. There are 2688 images
from 8 scene categories in OSR dataset. We extract gist features as representation [12]. For PubFig
database  we use a subset face images and there are 771 images from 8 face categories [6]. Similarly
with NUST-RF database  we extract LOMO features as representation. We simulate various types
of contiguous occlusion by adding sparse noise to both training and testing data or by replacing a
randomly selected local region in each image with an unrelated square block of the “baboon” image
for regular occlusion and a randomly located “tiger” image for irregular occlusion. Sparse noise
is simulated by 20 adding Gaussian noise with zero mean and 0.01 variance to both training and
testing data. And the size of the added image is 60% of the size of the original image. Fig. 4 shows a
clean image and its noisy versions from two datasets. Since the differences between the pixels of
the unrelated “baboon” image or “tiger” image and the pixels of the images from two datasets are
relatively small  the contiguous occlusion caused by these unrelated images is much more challenging
than by random black or white dots.
Table 2 and Table 3 show the classiﬁcation accuracy and the standard derivation of different methods
on two datasets  i.e.  OSR dataset and PubFig dataset. It is obvious our method consistently outper-
forms other competing methods in most cases  especially on the occlusion data. This is because our
bilevel model jointly performs metric learning and extracts features at the same time. And since we
use the sparsity penalty and graph regularization in the lower level model  the new features is not
only more robust to noise but also discriminative. For this classiﬁcation task  both FANTOPE and
CAP methods are based on LMNN method. Since they all have similar results  which indicates the
low-rank regularization on M for Mahalanobis distance metric learning is not particularly effective in
this case. Especially for regular occlusion that replaces a randomly selected local region with “baboon”
image and irregular occlusion that replaces local region with “tiger” image  LMNN  FANTOPE and
CAP achieve almost the same result. For RML method  due to the limitation that RML is based on
the MSE criterion  it still performs poorly.

8

Table 2: Recognition accuracy (%) and standard deviation of different methods on OSR dataset 
where sparse noise  regular and irregular occlusions are added.

KNN [14]
69.01 ± 1.96
61.83 ± 1.75
55.34 ± 2.72
52.25 ± 1.74

LMNN [16]
74.41 ± 1.20
66.67 ± 1.70
58.66 ±1.31
57.02 ± 1.80

FANTOPE [7]
74.97 ± 0.88
66.70 ± 1.68
58.73 ± 1.43
57.10 ± 1.74

CAP [5]

74.45 ± 1.19
66.67 ± 1.70
58.70 ± 1.27
57.13 ± 1.68

RML [10]
61.34 ± 1.62
56.57 ± 2.60
54.38 ± 3.26
50.45 ± 2.17

Proposed
74.43 ± 1.14
68.72 ± 2.72
64.77 ± 1.46
62.72 ± 3.06

Original
Sparse
Regular
Irregular

Table 3: Recognition accuracy (%) and standard deviation of different methods on PubFig dataset 
where sparse noise  regular and irregular occlusions are added.

KNN [14]
56.73 ± 1.12
48.46 ± 1.35
35.30 ± 1.14
40.94 ± 2.30

LMNN [16]
61.65 ± 1.63
51.35 ± 1.55
37.48 ± 1.64
41.73 ± 3.39

FANTOPE [7]
61.69 ± 1.60
51.39 ± 1.69
37.71 ± 1.56
41.88 ± 2.78

CAP [5]

61.80 ± 1.72
51.39 ± 1.99
38.05 ± 1.57
42.33 ± 2.35

RML [10]
55.86 ± 1.54
48.23 ± 1.26
35.80 ± 1.59
40.23 ± 2.48

Proposed
63.46 ± 1.65
54.40 ± 1.59
44.29 ± 0.54
49.10 ± 1.09

Original
Sparse
Regular
Irregular

To discuss the inﬂuences of individual parameters on the performance of the proposed model  we
take PubFig dataset with regular occlusion as an example. We test the inﬂuence of parameters λ  α 
β on the recognition accuracy as shown in Fig. 5.

Figure 5: The inﬂuence of parameters λ  α  β on the recognition accuracy of PubFig dataset with
regular occlusion.

4 Conclusion

We propose a new bilevel distance metric learning model for robust image recognition task. Different
from conventional metric learning methods which learn a Mahalanobis distance matrix based on
extracted features  we dig the intrinsic data structures using the Laplacian graph regularized sparse
coefﬁcients and jointly perform distance metric learning at the same time. Due to the feature
extraction operation of the lower level model  the new descriptors become more robust to noise with
the sparsity norm and more discriminative with the Laplacian graph term  leading to good recognition
performance. Moreover  we also derive an efﬁcient algorithm to solve the proposed new model.
Extensive experiments on several occluded datasets verify the remarkable performance improvements
led by the proposed bilevel model.

Acknowledgments

J.X. and C.D. were partially supported by the National Natural Science Foundation of China 61572388 
the National Key Research and Development Program of China (2017YFE0104100)  and the Key
R&D Program-The Key Industry Innovation Chain of Shaanxi under Grants 2017ZDCXL-GY-05-04-
02 and 2018ZDXM-GY-176.
L.L. and H.H. were partially supported by U.S. NSF-IIS 1836945  NSF-IIS 1836938  NSF-DBI
1836866  NSF-IIS 1845666  NSF-IIS 1852606  NSF-IIS 1838627  NSF-IIS 1837956.

9

0.20.40.60.81 (=1e-3  =1e-2)404244464850Recognition accuracy (%)10-410-3 (=0.5  =1e-2)3035404550Recognition accuracy (%)10-410-310-210-1 (=0.5  = 1e-3)3035404550Recognition accuracy (%)References
[1] Hervé Abdi and Lynne J Williams. Principal component analysis. Wiley interdisciplinary reviews:

computational statistics  2(4):433–459  2010.

[2] Shuo Chen  Jian Yang  Lei Luo  Yang Wei  Kaihua Zhang  and Ying Tai. Low-rank latent pattern
approximation with applications to robust image classiﬁcation. IEEE transactions on image processing 
26(11):5519–5530  2017.

[3] Bin Cheng  Jianchao Yang  Shuicheng Yan  Yun Fu  and Thomas S Huang. Learning with l1-graph for

image analysis. IEEE transactions on image processing  19(4):858–866  2010.

[4] Jason V Davis  Brian Kulis  Prateek Jain  Suvrit Sra  and Inderjit S Dhillon. Information-theoretic metric

learning. In ICML  pages 209–216. ACM  2007.

[5] Zhouyuan Huo  Feiping Nie  and Heng Huang. Robust and effective metric learning using capped trace

norm: Metric learning via capped trace norm. In SIGKDD  pages 1605–1614. ACM  2016.

[6] Neeraj Kumar  Alexander C Berg  Peter N Belhumeur  and Shree K Nayar. Attribute and simile classiﬁers

for face veriﬁcation. In ICCV  pages 365–372. IEEE  2009.

[7] Marc T Law  Nicolas Thome  and Matthieu Cord. Fantope regularization in metric learning. In CVPR 

pages 1051–1058. IEEE  2014.

[8] Shengcai Liao  Yang Hu  Xiangyu Zhu  and Stan Z Li. Person re-identiﬁcation by local maximal occurrence

representation and metric learning. In CVPR  pages 2197–2206. IEEE  2015.

[9] Zhouchen Lin  Risheng Liu  and Zhixun Su. Linearized alternating direction method with adaptive penalty

for low-rank representation. In NIPS  pages 612–620  2011.

[10] Jiwen Lu  Gang Wang  Weihong Deng  and Kui Jia. Reconstruction-based metric learning for unconstrained

face veriﬁcation. IEEE transactions on information forensics and security  10(1):79–89  2015.

[11] Lei Luo and Heng Huang. Matrix variate gaussian mixture distribution steered robust metric learning. In

AAAI  pages 3722–3729  2018.

[12] Aude Oliva and Antonio Torralba. Modeling the shape of the scene: A holistic representation of the spatial

envelope. International journal of computer vision  42(3):145–175  2001.

[13] Devi Parikh and Kristen Grauman. Relative attributes. In ICCV  pages 503–510. IEEE  2011.

[14] Leif E Peterson. K-nearest neighbor. Scholarpedia  4(2):1883  2009.

[15] Zhangyang Wang  Yingzhen Yang  Shiyu Chang  Jinyan Li  Simon Fong  and Thomas S Huang. A joint
optimization framework of sparse coding and discriminative clustering. In IJCAI  pages 3932–3938  2015.

[16] Kilian Q Weinberger and Lawrence K Saul. Distance metric learning for large margin nearest neighbor

classiﬁcation. JMLR  10(Feb):207–244  2009.

[17] John Wright  Allen Y Yang  Arvind Ganesh  S Shankar Sastry  and Yi Ma. Robust face recognition via

sparse representation. TPAMI  31(2):210–227  2009.

[18] Jie Xu  Lei Luo  Cheng Deng  and Heng Huang. Robust metric learning model using maximum correntropy

criterion. In SIGKDD  pages 2555–2564. ACM  2018.

[19] Jie Xu  Lei Luo  and Heng Huang. Multi-level metric learning via smoothed wasserstein distance. In

IJCAI  pages 2919–2925  2018.

[20] Jianchao Yang  Zhaowen Wang  Zhe Lin  Xianbiao Shu  and Thomas Huang. Bilevel sparse coding for

coupled feature spaces. In CVPR  pages 2360–2367. IEEE  2012.

[21] Meng Yang  Lei Zhang  Xiangchu Feng  and David Zhang. Fisher discrimination dictionary learning for

sparse representation. In ICCV  pages 543–550. IEEE  2011.

[22] Miao Zheng  Jiajun Bu  Chun Chen  Can Wang  Lijun Zhang  Guang Qiu  and Deng Cai. Graph regularized
sparse coding for image representation. IEEE transactions on image processing  20(5):1327–1336  2011.

[23] Pan Zhou  Chao Zhang  and Zhouchen Lin. Bilevel model-based discriminative dictionary learning for

recognition. IEEE transactions on image processing  26(3):1173–1187  2017.

10

,Michalis Titsias RC AUEB
Jie Xu
Lei Luo
Cheng Deng
Heng Huang