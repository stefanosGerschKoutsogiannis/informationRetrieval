2019,Sparse Variational Inference: Bayesian Coresets from Scratch,The proliferation of automated inference algorithms in Bayesian statistics has provided practitioners newfound access to fast  reproducible data analysis and powerful statistical models.  Designing automated methods that are also both computationally scalable and theoretically sound  however  remains a significant challenge.  Recent work on Bayesian coresets takes the approach of compressing the dataset before running a standard inference algorithm  providing both scalability and guarantees on posterior approximation error.  But the automation of past coreset methods is limited because they depend on the availability of a reasonable coarse posterior approximation  which is difficult to specify in practice.  In the present work we remove this requirement by formulating coreset construction as sparsity-constrained variational inference within an exponential family.  This perspective leads to a novel construction via greedy optimization  and also provides a unifying information-geometric view of present and past methods.  The proposed Riemannian coreset construction algorithm is fully automated  requiring no problem-specific inputs aside from the probabilistic model and dataset.  In addition to being significantly easier to use than past methods  experiments demonstrate that past coreset constructions are fundamentally limited by the fixed coarse posterior approximation; in contrast  the proposed algorithm is able to continually improve the coreset  providing state-of-the-art Bayesian dataset summarization with orders-of-magnitude reduction in KL divergence to the exact posterior.,Sparse Variational Inference:
Bayesian Coresets from Scratch

Trevor Campbell

Department of Statistics

University of British Columbia

Vancouver  BC V6T 1Z4
trevor@stat.ubc.ca

Boyan Beronov

Department of Computer Science
University of British Columbia

Vancouver  BC V6T 1Z4
beronov@cs.ubc.ca

Abstract

The proliferation of automated inference algorithms in Bayesian statistics has pro-
vided practitioners newfound access to fast  reproducible data analysis and powerful
statistical models. Designing automated methods that are also both computationally
scalable and theoretically sound  however  remains a signiﬁcant challenge. Recent
work on Bayesian coresets takes the approach of compressing the dataset before
running a standard inference algorithm  providing both scalability and guarantees
on posterior approximation error. But the automation of past coreset methods is
limited because they depend on the availability of a reasonable coarse posterior
approximation  which is difﬁcult to specify in practice. In the present work we re-
move this requirement by formulating coreset construction as sparsity-constrained
variational inference within an exponential family. This perspective leads to a novel
construction via greedy optimization  and also provides a unifying information-
geometric view of present and past methods. The proposed Riemannian coreset
construction algorithm is fully automated  requiring no problem-speciﬁc inputs
aside from the probabilistic model and dataset. In addition to being signiﬁcantly
easier to use than past methods  experiments demonstrate that past coreset con-
structions are fundamentally limited by the ﬁxed coarse posterior approximation;
in contrast  the proposed algorithm is able to continually improve the coreset  pro-
viding state-of-the-art Bayesian dataset summarization with orders-of-magnitude
reduction in KL divergence to the exact posterior.

1

Introduction

Bayesian statistical models are powerful tools for learning from data  with the ability to encode
complex hierarchical dependence and domain expertise  as well as coherently quantify uncertainty in
latent parameters. In practice  however  exact Bayesian inference is typically intractable  and we must
use approximate inference algorithms such as Markov chain Monte Carlo (MCMC) [1; 2  Ch. 11 12]
and variational inference (VI) [3  4]. Until recently  implementations of these methods were created
on a per-model basis  requiring expert input to design the MCMC transition kernels or derive VI
gradient updates. But developments in automated tools—e.g.  automatic differentiation [5  6]  “black-
box” gradient estimates [7]  and Hamiltonian transition kernels [8  9]—have obviated much of this
expert input  greatly expanding the repertoire of Bayesian models accessible to practitioners.
In modern data analysis problems  automation alone is insufﬁcient; inference algorithms must also
be computationally scalable—to handle the ever-growing size of datasets—and provide theoretical
guarantees on the quality of their output such that statistical pracitioners may conﬁdently use them
in failure-sensitive settings. Here the standard set of tools falls short. Designing correct MCMC
schemes in the large-scale data setting is a challenging  problem-speciﬁc task [10–12]; and despite

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

recent results in asymptotic theory [13–16]  it is difﬁcult to assess the effect of the variational family
on VI approximations for ﬁnite data  where a poor choice can result in severe underestimation
of posterior uncertainty [17  Ch. 21]. Other scalable Bayesian inference algorithms have largely
been developed by modifying standard inference algorithms to handle distributed or streaming data
processing [10  11  18–29]  which tend to have no guarantees on inferential quality and require
extensive model-speciﬁc expert tuning.
Bayesian coresets (“core of a dataset”) [30–32] are an alternative approach—based on the notion that
large datasets often contain a signiﬁcant fraction of redundant data—that summarize and sparsify the
data as a preprocessing step before running a standard inference algorithm such as MCMC or VI. In
contrast to other large-scale inference techniques  Bayesian coreset construction is computationally
inexpensive  simple to implement  and provides theoretical guarantees relating coreset size to posterior
approximation quality. However  state-of-the-art algorithms formulate coreset construction as a sparse
regression problem in a Hilbert space  which involves the choice of a weighted L2 inner product [31].
If left to the user  the choice of weighting distribution signiﬁcantly reduces the overall automation
of the approach; and current methods for ﬁnding the weighting distribution programatically are
generally as expensive as posterior inference on the full dataset itself. Further  even if an appropriate
inner product is speciﬁed  computing it exactly is typically intractable  requiring the use of ﬁnite-
dimensional projections for approximation [31]. Although the problem in ﬁnite-dimensions can be
studied using well-known techniques from sparse regression  compressed sensing  random sketching 
boosting  and greedy approximation [33–51]  these projections incur an unknown error in the
construction process in practice  and preclude asymptotic consistency as the coreset size grows.
In this work  we provide a new formulation of coreset construction as exponential family variational
inference with a sparsity constraint. The fact that coresets form a sparse subset of an exponential
family is crucial in two regards. First  it enables tractable unbiased Kullback-Leibler (KL) divergence
gradient estimation  which is used in the development of a novel coreset construction algorithm
based on greedy optimization. In contrast to past work  this algorithm is fully automated  with
no problem-speciﬁc inputs aside from the probabilistic model and dataset. Second  it provides a
unifying view and strong theoretical underpinnings of both the present and past coreset constructions
through Riemannian information geometry. In particular  past methods are shown to operate in a
single tangent space of the coreset manifold; our experiments show that this fundamentally limits the
quality of the coreset constructed with these methods. In contrast  the proposed method proceeds
along the manifold towards the posterior target  and is able to continually improve its approximation.
Furthermore  new relationships between the optimization objective of past approaches and the coreset
posterior KL divergence are derived. The paper concludes with experiments demonstrating that 
compared with past methods  Riemannian coreset construction is both easier to use and provides
orders-of-magnitude reduction in KL divergence to the exact posterior.

2 Background

In the problem setting of the present paper  we are given a probability density π(θ) for variables
θ ∈ Θ that decomposes into N potentials (fn(θ))N

n=1 and a base density π0(θ) 

(cid:33)

(cid:32) N(cid:88)

n=1

π(θ) :=

1
Z

exp

fn(θ)

π0(θ) 

(1)

where Z is the (unknown) normalization constant. Such distributions arise frequently in a number of
scenarios: for example  in Bayesian statistical inference problems with conditionally independent
data given θ  the functions fn are the log-likelihood terms for the N data points  π0 is the prior
density  and π is the posterior; or in undirected graphical models  the functions fn and log π0 might
represent N + 1 potentials. The algorithms and analysis in the present work are agnostic to their
particular meaning  but for clarity we will focus on the setting of Bayesian inference throughout.
As it is often intractable to compute expectations under π exactly  practitioners have turned to
approximate algorithms. Markov chain Monte Carlo (MCMC) methods [1  8  9]  which return
approximate samples from π  remain the gold standard for this purpose. But since each sample
typically requires at least one evaluation of a function proportional to π with computational cost Θ(N ) 
in the large N setting it is expensive to obtain sufﬁciently many samples to provide high conﬁdence in
empirical estimates. To reduce the cost of MCMC  we can instead run it on a small  weighted subset

2

of data known as a Bayesian coreset [30]  a concept originating from the computational geometry
and optimization literature [52–57]. Let w ∈ RN≥0 be a sparse vector of nonnegative weights such
1 [wn > 0] ≤ M. Then we approximate the
full log-density with a w-reweighted sum with normalization Z(w) > 0 and run MCMC on the
approximation1 

that only M (cid:28) N are nonzero  i.e. (cid:107)w(cid:107)0 :=(cid:80)N
(cid:32) N(cid:88)

(cid:33)

n=1

1

wnfn(θ)

π0(θ) 

(2)

πw(θ) :=

exp

Z(w)

n=1

where π1 = π corresponds to the full density. If M (cid:28) N  evaluating a function proportional to πw is
much less expensive than doing so for the original π  resulting in a signiﬁcant reduction in MCMC
computation time. The major challenge posed by this approach  then  is to ﬁnd a set of weights w that
renders πw as close as possible to π while maintaining sparsity. Past work [31  32] formulated this as
a sparse regression problem in a Hilbert space with the L2(ˆπ) norm for some weighting distribution
ˆπ and vectors2 gn := (fn − Eˆπ [fn]) 

(cid:32) N(cid:88)

gn − N(cid:88)

n=1

n=1

(cid:33)2 s.t. w ≥ 0  (cid:107)w(cid:107)0 ≤ M.

wngn

w(cid:63) = arg min
w∈RN

Eˆπ

(3)

As the expectation is generally intractable to compute exactly  a Monte Carlo approximation is used in
S −1 [gn(θ1) − ¯gn  . . .   gn(θS) − ¯gn]T ∈
its place: taking samples (θs)S
s=1 gn(θs) yields a linear ﬁnite-dimensional sparse regression problem in RS 
RS where ¯gn = 1

i.i.d.∼ ˆπ and setting ˆgn =

√

s=1

(cid:80)S

S

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) N(cid:88)

n=1

ˆgn − N(cid:88)

n=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

2

w(cid:63) = arg min
w∈RN

wnˆgn

s.t. w ≥ 0  (cid:107)w(cid:107)0 ≤ M 

(4)

which can be solved with sparse optimization techniques [31  32  34–36  45  46  48  58–60]. However 
there are two drawbacks inherent to the Hilbert space formulation. First  the use of the L2(ˆπ) norm
requires the selection of the weighting function ˆπ  posing a barrier to the full automation of coreset
construction. There is currently no guidance on how to select ˆπ  or the effect of different choices
in the literature. We show in Sections 4 and 5 that using such a ﬁxed weighting ˆπ fundamentally
limits the quality of coreset construction. Second  the inner products typically cannot be computed
exactly  requiring a Monte Carlo approximation. This adds noise to the construction and precludes
asymptotic consistency (in the sense that πw (cid:54)→ π1 as the sparsity budget M → ∞). Addressing
these drawbacks is the focus of the present work.

3 Bayesian coresets from scratch

In this section  we provide a new formulation of Bayesian coreset construction as variational inference
over an exponential family with sparse natural parameters  and develop an iterative greedy algorithm
for optimization.

3.1 Sparse exponential family variational inference

We formulate coreset construction as a sparse variational inference problem 

w(cid:63) = arg min
w∈RN

DKL (πw||π1)

s.t. w ≥ 0  (cid:107)w(cid:107)0 ≤ M.

Expanding the objective and denoting expectations under πw as Ew 

DKL (πw||π1) = log Z(1) − log Z(w) − N(cid:88)

(1 − wn)Ew [fn(θ)] .

(5)

(6)

1Throughout  [N ] := {1  . . .   N}  1 and 0 are the constant vectors of all 1s / 0s respectively (the dimension
will be clear from context)  1A is the indicator vector for A ⊆ [N ]  and 1n is the indicator vector for n ∈ [N ].
2In [31]  the Eˆπ [fn] term was missing; it is necessary to account for the shift-invariance of potentials.

n=1

3

Eq. (6) illuminates the major challenges with the variational approach posed in Eq. (5). First  the
normalization constant Z(w) of πw—itself a function of the weights w—is unknown; typically  the
form of the approximate distribution is known fully in variational inference. Second  even if the
constant were known  computing the objective in Eq. (5) requires taking expectations under πw 
which is in general just as difﬁcult as the original problem of sampling from the true posterior π1.
Two key insights in this work address these issues and lead to both the development of a new
coreset construction algorithm (Algorithm 1) and a more comprehensive understanding of the coreset
construction literature (Section 4). First  the coresets form a sparse subset of an exponential family:
the nonnegative weights form the natural parameter w ∈ RN≥0  the component potentials (fn(θ))N
n=1
form the sufﬁcient statistic  log Z(w) is the log partition function  and π0 is the base density 
fN (θ) ]T .

πw(θ) := exp(cid:0)wT f (θ) − log Z(w)(cid:1) π0(θ)

(7)
Using the well-known fact that the gradient of an exponential family log-partition function is the
mean of the sufﬁcient statistic  Ew [f (θ)] = ∇w log Z(w)  we can rewrite the optimization Eq. (5) as
(8)
w(cid:63) = arg min
w∈RN

log Z(1) − log Z(w) − (1 − w)T∇w log Z(w)

s.t. w ≥ 0  (cid:107)w(cid:107)0 ≤ M.

f (θ) := [ f1(θ)

. . .

(cid:2)f  f T (1 − w)(cid:3)  

Taking the gradient of this objective function and noting again that  for an exponential family  the
Hessian of the log-partition function log Z(w) is the covariance of the sufﬁcient statistic 

∇wDKL (πw||π1) = −∇2

w log Z(w)(1 − w) = − Covw

n=1 fn(θ) −(cid:80)N

fn(θ) with the residual error(cid:80)N

(9)
where Covw denotes covariance under πw. In other words  increasing the weight wn by a small
amount decreases DKL (πw||π1) by an amount proportional to the covariance of the nth potential
n=1 wnfn(θ) under πw. If required  it is not difﬁcult
to use the connection between derivatives of log Z(w) and moments of the sufﬁcient statistic under
πw to derive 2nd and higher order derivatives of DKL (πw||π1).
This provides a natural tool for optimizing the coreset construction objective in Eq. (5)—Monte
Carlo estimates of sufﬁcient statistic moments—and enables coreset construction without both the
problematic selection of a Hilbert space (i.e.  ˆπ) and ﬁnite-dimensional projection error from past
approaches. But obtaining Monte Carlo estimates requires sampling from πw; the second key insight
in this work is that as long as we build up the sparse approximation w incrementally  the iterates
will themselves be sparse. Therefore  using a standard Markov chain Monte Carlo algorithm [9] to
obtain samples from πw for gradient estimation is actually not expensive—with cost O(M ) instead
of O(N )—despite the potentially complicated form of πw.

3.2 Greedy selection

One option to build up a coreset incrementally is to use a greedy approach (Algorithm 1) to select and
subsequently reweight a single potential function at a time. For greedy selection  the naïve approach
is to select the potential that provides the largest local decrease in KL divergence around the current
weights w  i.e.  selecting the potential with the largest covariance with the residual error per Eq. (9).
However  since the weight wn(cid:63) will then be optimized over [0 ∞)  the selection of the next potential
to add should be invariant to scaling each potential fn by any positive constant. Thus we propose the
use of the correlation—rather than the covariance—between fn and the residual error f T (1 − w) as
the selection criterion:

(cid:26) (cid:12)(cid:12)Corrw

Corrw

(cid:2)fn  f T (1 − w)(cid:3)(cid:12)(cid:12) wn > 0
(cid:2)fn  f T (1 − w)(cid:3) wn = 0

n(cid:63)= arg max

n∈[N ]

.

(10)

Although seemingly ad-hoc  this modiﬁcation will be placed on a solid information-geometric
theoretical foundation in Proposition 1 (see also Eq. (34) in Appendix A). Note that since we do not
have access to the exact correlations  we must use Monte Carlo estimates via sampling from πw for
greedy selection. Given S samples (θs)S

i.i.d.∼ πw  these are given by the N-dimensional vector

(cid:34)

S(cid:88)

s=1

1
S

(cid:35)− 1
2(cid:32)

1
S

s=1

S(cid:88)

ˆgsˆgT

(cid:33)
s (1 − w)

(cid:91)Corr = diag

ˆgsˆgT
s

(cid:34) f1(θs)

(cid:35)

ˆgs :=

...

− 1
S

(cid:34) f1(θr)

(cid:35)

S(cid:88)

r=1

fN (θr)

...

 

(11)

s=1

fN (θs)

4

where diag [·] returns a diagonal matrix with the same diagonal entries as its argument. The details of
using the correlation estimate (Eq. (11)) in the greedy selection rule (Eq. (10)) to add points to the
coreset are shown in lines 4–9 of Algorithm 1. Note that this computation has cost O(N S). If N is
large enough that computing the entire vectors ˆgs ∈ RN is cost-prohibitive  one may instead compute
ˆgs in Eq. (11) only for indices in I ∪ U—where I = {n ∈ [N ] : wn > 0} is the set of active indices 
and U is a uniformly selected subsample of U ∈ [N ] indices—and perform greedy selection only
within these indices.

3.3 Weight update
After selecting a new potential function n(cid:63)  we add it to the active set of indices I ⊆ [N ] and update
the weights by optimizing

w(cid:63) = arg min

v∈RN

DKL (πv||π)

s.t.

v ≥ 0 

(1 − 1I)T v = 0.

(12)

In particular  we run T steps of generating S samples (θs)S
estimate D of the gradient ∇wDKL (πw||π1) based on Eq. (9) 

s=1

i.i.d.∼ πw  computing a Monte Carlo

S(cid:88)

s=1

D := − 1
S

s (1 − w) ∈ RN ˆgs as in Eq. (11) 

ˆgsˆgT

(13)

and taking a stochastic gradient step wn ← wn − γtDn at step t ∈ [T ] for each n ∈ I  using a
typical learning rate γt ∝ t−1. The details of the weight update step are shown in lines 10–15 of
Algorithm 1. As in the greedy selection step  the cost of each gradient step is O(N S)  due to the ˆgT
s 1
term in the gradient. If N is large enough that this computation is cost-prohibitive  one can use ˆgs
computed only for indices in I ∪ U  where U is a uniformly selected subsample of U ∈ [N ] indices.

4 The information geometry of coreset construction

(cid:115)

(cid:90) 1

0

The perspective of coresets as a sparse exponential family also enables the use of information geometry
to derive a unifying connection between the variational formulation and previous constructions.
In particular  the family of coreset posteriors deﬁnes a Riemannian statistical manifold M =
with chart M → RN≥0  endowed with the Fisher information metric G [61  p. 33 34] 
{πw}w∈RN≥0

G(w) =

πw(θ)∇w log πw(θ)∇w log πw(θ)T dθ = ∇2

w log Z(w) = Covw [f ] .
For any differentiable curve γ : [0  1] → RN≥0  the metric deﬁnes a notion of path length 

(14)

(cid:90)

T

dγ(t)

dγ(t)

dt

dt

dt 

L(γ) =

G(γ(t))

(15)
and a constant-speed curve of minimal length between any two points w  w(cid:48) ∈ RN≥0 is referred to as a
geodesic [61  Thm. 5.2]. The geodesics are the generalization of straight lines in Euclidean space to
curved Riemannian manifolds  such as M. Using this information-geometric view  Proposition 1
shows that both Hilbert coreset construction (Eq. (3)) and the proposed greedy sparse variational
inference procedure (Algorithm 1) attempt to directionally align the ˆw → w and ˆw → 1 geodesics
on M for ˆw  w  1 ∈ RN≥0 (reference  coreset  and true posterior weights  respectively) as illustrated
in Fig. 1. The key difference is that Hilbert coreset construction uses a ﬁxed reference point ˆw—
corresponding to ˆπ in Eq. (3)—and thus operates entirely in a single tangent space of M  while
the proposed greedy method uses ˆw = w and thus improves its tangent space approximation
as the algorithm iterates. For this reason  we refer to the method in Section 3 as a Riemannian
coreset construction algorithm. In addition to this uniﬁcation of coreset construction methods  the
geometric perspective also provides the means to show that the Hilbert coresets objective bounds the
symmetrized coreset KL divergence DKL (πw||π) + DKL (π||πw) if the Riemannian metric does not
vary too much  as shown in Proposition 2. Incidentally  Lemma 3 in Appendix A—which is used to
prove Proposition 2—also provides a nonnegative unbiased estimate of the symmetrized coreset KL
divergence  which may be used for performance monitoring in practice.

5

Algorithm 1 Greedy sparse stochastic variational inference
1: procedure SPARSEVI(f  π0  S  T   (γt)∞
2:
3:

w ← 0 ∈ RN   I ← ∅
for m = 1  . . .   M do

t=1  M)

4:

5:
6:

8:
9:

10:

11:
12:
13:
14:

7: (cid:91)Corr ← diag

(cid:17) ∈ RN

s=1

i.i.d.∼ πw ∝ exp(wT f (θ))π0(θ)

(cid:46) Take S samples from the current coreset posterior approximation πw
(θs)S
(cid:46) Compute the N-dimensional potential vector for each sample
ˆfs ← f (θs) ∈ RN for s ∈ [S]  and ¯f ← 1
ˆgs ← ˆfs − ¯f for s ∈ [S]
(cid:46) Estimate correlations between the potentials and the residual error

(cid:80)S

ˆfs

s=1

S

(cid:104) 1

S

(cid:80)S

(cid:105)− 1
2(cid:16)1

S

(cid:80)S

s=1 ˆgsˆgT

s=1 ˆgsˆgT
s

s (1 − w)
(cid:46) Add the best next potential to the coreset
n(cid:63) ← arg maxn∈[N ] |(cid:91)Corrn|1 [n ∈ I] + (cid:91)Corrn1 [n /∈ I]
I ← I ∪ {n(cid:63)}
(cid:46) Update all the active weights in I via stochastic gradient descent on DKL (πw||π)
for t = 1  . . .   T do
(cid:46) Use samples from πw to estimate the gradient
(θs)S
ˆfs ← f (θs) ∈ RN for s ∈ [S]  and ¯f ← 1
ˆgs ← ˆfs − ¯f for s ∈ [S]
D ← − 1
(cid:46) Take a stochastic gradient step for active indices in I

i.i.d.∼ πw ∝ exp(wT f (θ))π0(θ)

w ← w − γtIID where II :=(cid:80)

n is the diagonal indicator matrix for I

s (1 − w)

(cid:80)S

(cid:80)S

s=1 ˆgsˆgT

n∈I 1n1T

ˆfs

s=1

s=1

S

S

end for

15:
16:
17:
18:
19: end procedure

end for
return w

Proposition 1. Suppose ˆπ in Eq. (3) satisﬁes ˆπ = π ˆw for a set of weights ˆw ∈ RN≥0. For u  v ∈ RN≥0 
let ξu→v denote the initial tangent of the u → v geodesic on M  and (cid:104)· ·(cid:105)u denote the inner product
under the Riemannian metric G(u) with induced norm (cid:107) · (cid:107)u. Then Hilbert coreset construction in
Eq. (3) is equivalent to

w(cid:63) = arg min
w∈RN

(cid:107)ξ ˆw→1 − ξ ˆw→w(cid:107) ˆw

s.t. w ≥ 0  (cid:107)w(cid:107)0 ≤ M 

and each greedy selection step of Riemannian coreset construction in Eq. (10) is equivalent to

n(cid:63) = arg min
n∈[N ] tn∈R

(cid:107)ξw→1 − ξw→w+tn1n(cid:107)w

s.t. ∀ n /∈ I  tn > 0.

Proposition 2. Suppose ˆπ in Eq. (3) satisﬁes ˆπ = π ˆw for a set of weights ˆw ∈ RN≥0. Then if Jˆπ(w)
is the objective function in Eq. (3) 

DKL (π||πw) + DKL (πw||π) ≤ Cˆπ(w) · Jˆπ(w) 

(cid:0)G( ˆw)−1/2G((1 − U )w + U 1)G( ˆw)−1/2(cid:1)(cid:3). In particular  if

(cid:2)λmax

(18)

where Cˆπ(w) := EU∼Unif[0 1]
∇2
w log Z(w) is constant in w ∈ RN≥0  then Cˆπ(w) = 1.

(16)

(17)

5 Experiments

In this section  we compare the quality of coresets constructed via the proposed SparseVI greedy
coreset construction method  uniform random subsampling  and Hilbert coreset construction (GIGA
[32]). In particular  for GIGA we used a 100-dimensional random projection generated from a
Gaussian ˆπ with two parametrizations: one with mean and covariance set using the moments of the
exact posterior (Optimal) which is a benchmark but is not possible to achieve in practice; and one

6

Figure 1: Information-geometric view of greedy coreset construction on the coreset manifold M.
(1a): Hilbert coreset construction  with weighting distribution π ˆw  full posterior π  coreset posterior
πw  and arrows denoting initial geodesic directions from ˆw towards new datapoints. (1b): Riemannian
coreset construction  with the path of posterior approximations πwt  t = 0  . . .   3  and arrows denoting
initial geodesic directions towards new datapoints to add within each tangent plane.

(a)

(b)

Figure 2: (2a): Synthetic comparison of coreset construction methods. Solid lines show the median
KL divergence over 10 trials  with 25th and 75th percentiles shown by shaded areas. (2b): 2D
projection of coresets after 0  1  5  20  50  and 100 iterations via SparseVI. True/coreset posterior
and 2σ-predictive ellipses are shown in black/blue respectively. Coreset points are black with radius
denoting weight.

with mean and covariance uniformly distributed between the prior and the posterior with 75% relative
noise added (Realistic) to simulate the choice of ˆπ without exact posterior information. Experiments
were performed on a machine with an Intel i7 8700K processor and 32GB memory; code is available
at www.github.com/trevorcampbell/bayesian-coresets.

5.1 Synthetic Gaussian posterior inference

We ﬁrst compared the coreset construction algorithms on a synthetic example involving posterior
inference for the mean of a d-dimensional Gaussian with Gaussian observations 
i.i.d.∼ N (θ  Σ)  n = 1  . . .   N.

θ ∼ N (µ0  Σ0)

(19)

xn

We selected this example because it decouples the evaluation of the coreset construction methods from
the concerns of stochastic optimization and approximate posterior inference: the coreset posterior
πw is a Gaussian πw = N (µw  Σw) with closed-form expressions for the parameters as well as
covariance (see Appendix B for the derivation) 

Σw =(cid:0)Σ−1

0 +(cid:80)N

n=1wnΣ−1(cid:1)−1

(cid:0)Σ−1
0 µ0 + Σ−1(cid:80)N

n=1wnxn

(cid:1)

µw = Σw

Covw [fn  fm] = 1/2 tr ΨT Ψ + νT

mΨνn 

7

(20)
(21)

(a)

(b)

Figure 3: (3a): Coreset construction on regression of housing prices using radial basis functions in
the UK Land Registry data. Solid lines show the median KL divergence over 10 trials  with 25th and
75th percentiles shown by shaded areas. (3b): Posterior mean contours with coresets of size 0–300
via SparseVI compared with the exact posterior. Posterior and ﬁnal coreset highlighted in the top
row. Coreset points are black with radius denoting weight.

where Σ = QQT   νn := Q−1(xn − µw) and Ψ := Q−1ΣwQ−T . Thus the greedy selection and
weight update can be performed without Monte Carlo estimation. We set Σ0 = Σ = I  µ0 = 0 
d = 200  and N = 1  000. We used a learning rate of γt = t−1  T = 100 weight update optimization
iterations  and M = 200 greedy iterations  although note that this is an upper bound on the size of
the coreset as the same data point may be selected multiple times. The results in Fig. 2 demonstrate
that the use of a ﬁxed weighting function ˆπ (and thus  a ﬁxed tangent plane on the coreset manifold)
fundamentally limits the quality coresets via past algorithms. In contrast  the proposed greedy
algorithm is “manifold-aware” and is able to continually improve the approximation  resulting in
orders-of-magnitude improvements in KL divergence to the true posterior.

5.2 Bayesian radial basis function regression

n

n α + n

i.i.d.∼ N (0  σ2)

Next  we compared the coreset construction algorithms on Bayesian basis function regression for
N = 10  000 records of house sale log-price yn ∈ R as a function of latitude / longitude coordinates
xn ∈ R2 in the UK.3 The regression problem involved inference for the coefﬁcients α ∈ RK in a
linear combination of radial basis functions bk(x) = exp(−1/2σ2
(22)
yn = bT
We generated 50 basis functions for each of 6 scales σk ∈ {0.2  0.4  0.8  1.2  1.6  2.0} by generating
means µk uniformly from the data  and added one additional near-constant basis with scale 100
and mean corresponding to the mean latitude and longitude of the data. This resulted in K = 301
total basis functions and thus a 301-dimensional regression problem. We set the prior and noise
parameters µ0  σ2
0  σ2 equal to the empirical mean  second moment  and variance of the price paid
n=1 across the whole dataset  respectively. As in Section 5.1  the posterior and log-likelihood
(yn)N
covariances are available in closed form  and all algorithmic steps can be performed without Monte
Carlo. In particular  πw = N (µw  Σw)  where (see Appendix B for the derivation)

k(x − µk)2)  k = 1  . . .   K 
α ∼ N (µ0  σ2

bn = [ b1(xn)

bK(xn) ]T

···

0I).

N(cid:88)

N(cid:88)

n=1

Σw = (Σ−1

wnbnbT

0 + σ−2

Covw [fn  fm] = σ−4(cid:0)νnνmβT

n )−1

n=1

n βm)2(cid:1) .

and µw = Σw(Σ−1

0 µ0 + σ−2

wnynbn)

(23)

(24)
where νn := yn − µT
wbn  Σw = LLT   and βn := LT bn. We used a learning rate of γt = t−1 
T = 100 optimization steps  and M = 300 greedy iterations  although again note that this is an upper

n βm + 1/2(βT

3This dataset was constructed by merging housing prices from the UK land registry data https://www.gov.
uk/government/statistical-data-sets/price-paid-data-downloads with latitude & longitude co-
ordinates from the Geonames postal code data http://download.geonames.org/export/zip/.

8

(a)

(b)

Figure 4: The results of the logistic (4a) and Poisson (4b) regression experiments. Plots show the
median KL divergence (estimated using the Laplace approximation [62] and normalized by the value
for the prior) across 10 trials  with 25th and 75th percentiles shown by shaded areas. From top to
bottom  (4a) shows the results for logistic regression on synthetic  chemical reactivities  and phishing
websites data  while (4b) shows the results for Poisson regression on synthetic  bike trips  and airport
delays data. See Appendix C for details.

bound on the coreset size. The results in Fig. 3 generally align with those from the previous synthetic
experiment. The proposed sparse variational inference formulation builds coresets of comparable
quality to Hilbert coreset construction (when given the exact posterior for ˆπ) up to a size of about 150.
Beyond this point  past methods become limited by their ﬁxed tangent plane approximation while
the proposed method continues to improve. This experiment also highlights the sensitivity of past
methods to the choice of ˆπ: uniform subsampling outperforms GIGA with a realistic choice of ˆπ.

5.3 Bayesian logistic and Poisson regression

Finally  we compared the methods on logistic and Poisson regression applied to six datasets (details
may be found in Appendix C) with N = 500 and dimension ranging from 2-15. We used M = 100
greedy iterations  S = 100 samples for Monte Carlo covariance estimation  and T = 500 optimization
iterations with learning rate γt = 0.5t−1. Fig. 4 shows the result of this test  demonstrating that
the proposed greedy sparse VI method successfully recovers a coreset with divergence from the
exact posterior as low or lower than GIGA with without having the beneﬁt of a user-speciﬁed
weighting function. Note that there is a computational price to pay for this level of automation;
Fig. 5  Appendix C shows that SparseVI is signiﬁcantly slower than Hilbert coreset construction via
GIGA [32]  primarily due to the expensive gradient descent weight update. However  if we remove
GIGA (Optimal) from consideration due to its unrealistic use of ˆπ ≈ π1  SparseVI is the only
practical coreset construction algorithm that reduces the KL divergence to the posterior appreciably
for reasonable coreset sizes. We leave improvements to computational cost for future work.

6 Conclusion

This paper introduced sparse variational inference for Bayesian coreset construction. By exploiting
the fact that coreset posteriors form an exponential family  a greedy algorithm as well as a unifying
Riemannian information-geometric view of present and past coreset constructions were developed.
Future work includes extending sparse VI to improved optimization techniques beyond greedy
methods  and reducing computational cost.

Acknowledgments T. Campbell and B. Beronov are supported by National Sciences and Engineer-
ing Research Council of Canada (NSERC) Discovery Grants. T. Campbell is additionally supported
by an NSERC Discovery Launch Supplement.

9

References
[1] Christian Robert and George Casella. Monte Carlo Statistical Methods. Springer  2nd edition  2004.

[2] Andrew Gelman  John Carlin  Hal Stern  David Dunson  Aki Vehtari  and Donald Rubin. Bayesian data

analysis. CRC Press  3rd edition  2013.

[3] Michael Jordan  Zoubin Ghahramani  Tommi Jaakkola  and Lawrence Saul. An introduction to variational

methods for graphical models. Machine Learning  37:183–233  1999.

[4] Martin Wainwright and Michael Jordan. Graphical models  exponential families  and variational inference.

Foundations and Trends in Machine Learning  1(1–2):1–305  2008.

[5] Aılım Güne¸s Baydin  Barak Pearlmutter  Alexey Radul  and Jeffrey Siskind. Automatic differentiation in

machine learning: a survey. Journal of Machine Learning Research  18:1–43  2018.

[6] Alp Kucukelbir  Dustin Tran  Rajesh Ranganath  Andrew Gelman  and David Blei. Automatic differentia-

tion variational inference. Journal of Machine Learning Research  18:1–45  2017.

[7] Rajesh Ranganath  Sean Gerrish  and David Blei. Black box variational inference. In International

Conference on Artiﬁcial Intelligence and Statistics  2014.

[8] Radford Neal. MCMC using Hamiltonian dynamics. In Steve Brooks  Andrew Gelman  Galin Jones  and

Xiao-Li Meng  editors  Handbook of Markov chain Monte Carlo  chapter 5. CRC Press  2011.

[9] Matthew Hoffman and Andrew Gelman. The No-U-Turn Sampler: adaptively setting path lengths in

Hamiltonian Monte Carlo. Journal of Machine Learning Research  15:1351–1381  2014.

[10] Rémi Bardenet  Arnaud Doucet  and Chris Holmes. On Markov chain Monte Carlo methods for tall data.

Journal of Machine Learning Research  18:1–43  2017.

[11] Steven Scott  Alexander Blocker  Fernando Bonassi  Hugh Chipman  Edward George  and Robert McCul-
loch. Bayes and big data: the consensus Monte Carlo algorithm. International Journal of Management
Science and Engineering Management  11:78–88  2016.

[12] Michael Betancourt. The fundamental incompatibility of Hamiltonian Monte Carlo and data subsampling.

In International Conference on Machine Learning  2015.

[13] Pierre Alquier and James Ridgway. Concentration of tempered posteriors and of their variational approxi-

mations. The Annals of Statistics  2018 (to appear).

[14] Yixin Wang and David Blei. Frequentist consistency of variational Bayes. Journal of the American

Statistical Association  0(0):1–15  2018.

[15] Yun Yang  Debdeep Pati  and Anirban Bhattacharya. α-variational inference with statistical guarantees.

The Annals of Statistics  2018 (to appear).

[16] Badr-Eddine Chérief-Abdellatif and Pierre Alquier. Consistency of variational Bayes inference for

estimation and model selection in mixtures. Electronic Journal of Statistics  12:2995–3035  2018.

[17] Kevin Murphy. Machine learning: a probabilistic perspective. The MIT Press  2012.

[18] Matthew Hoffman  David Blei  Chong Wang  and John Paisley. Stochastic variational inference. The

Journal of Machine Learning Research  14:1303–1347  2013.

[19] Maxim Rabinovich  Elaine Angelino  and Michael Jordan. Variational consensus Monte Carlo. In Advances

in Neural Information Processing Systems  2015.

[20] Tamara Broderick  Nicholas Boyd  Andre Wibisono  Ashia Wilson  and Michael Jordan. Streaming

variational Bayes. In Advances in Neural Information Processing Systems  2013.

[21] Trevor Campbell  Julian Straub  John W. Fisher III  and Jonathan How. Streaming  distributed variational

inference for Bayesian nonparametrics. In Advances in Neural Information Processing Systems  2015.

[22] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics. In

International Conference on Machine Learning  2011.

[23] Sungjin Ahn  Anoop Korattikara  and Max Welling. Bayesian posterior sampling via stochastic gradient

Fisher scoring. In International Conference on Machine Learning  2012.

10

[24] Rémi Bardenet  Arnaud Doucet  and Chris C Holmes. Towards scaling up Markov chain Monte Carlo: an
adaptive subsampling approach. In International Conference on Machine Learning  pages 405–413  2014.

[25] Anoop Korattikara  Yutian Chen  and Max Welling. Austerity in MCMC land: cutting the Metropolis-

Hastings budget. In International Conference on Machine Learning  2014.

[26] Dougal Maclaurin and Ryan Adams. Fireﬂy Monte Carlo: exact MCMC with subsets of data. In Conference

on Uncertainty in Artiﬁcial Intelligence  2014.

[27] Sanvesh Srivastava  Volkan Cevher  Quoc Dinh  and David Dunson. WASP: scalable Bayes via barycenters

of subset posteriors. In International Conference on Artiﬁcial Intelligence and Statistics  2015.

[28] Reihaneh Entezari  Radu Craiu  and Jeffrey Rosenthal. Likelihood inﬂating sampling algorithm.

arXiv:1605.02113  2016.

[29] Elaine Angelino  Matthew Johnson  and Ryan Adams. Patterns of scalable Bayesian inference. Foundations

and Trends in Machine Learning  9(1–2):1–129  2016.

[30] Jonathan Huggins  Trevor Campbell  and Tamara Broderick. Coresets for Bayesian logistic regression. In

Advances in Neural Information Processing Systems  2016.

[31] Trevor Campbell and Tamara Broderick. Automated scalable Bayesian inference via Hilbert coresets.

Journal of Machine Learning Research  20(15):1–38  2019.

[32] Trevor Campbell and Tamara Broderick. Bayesian coreset construction via greedy iterative geodesic ascent.

In International Conference on Machine Learning  2018.

[33] Kenneth Clarkson. Coresets  sparse greedy approximation  and the Frank-Wolfe algorithm. ACM

Transactions on Algorithms  6(4)  2010.

[34] Simon Lacoste-Julien and Martin Jaggi. On the global linear convergence of Frank-Wolfe optimization

variants. In Advances in Neural Information Processing Systems  2015.

[35] Francesco Locatello  Michael Tschannen  Gunnar Rätsch  and Martin Jaggi. Greedy algorithms for cone
constrained optimization with convergence guarantees. In Advances in Neural Information Processing
Systems  2017.

[36] Andrew Barron  Albert Cohen  Wolfgang Dahmen  and Ronald DeVore. Approximation and learning by

greedy algorithms. The Annals of Statistics  36(1):64–94  2008.

[37] Yutian Chen  Max Welling  and Alex Smola. Super-samples from kernel herding. In Uncertainty in

Artiﬁcial Intelligence  2010.

[38] Robert Schapire. The strength of weak learnability. Machine Learning  5(2):197–227  1990.

[39] Ferenc Huszar and David Duvenaud. Optimally-weighted herding is Bayesian quadrature. In Uncertainty

in Artiﬁcial Intelligence  2012.

[40] Yoav Freund and Robert Schapire. A decision-theoretic generalization of on-line learning and an application

to boosting. Journal of Computer and System Sciences  55:119–139  1997.

[41] Emmanuel Candès and Terence Tao. Decoding by linear programming. IEEE Transactions on Information

Theory  51(12):4203–4215  2005.

[42] Emmanual Candès and Terence Tao. The Dantzig selector: statistical estimation when p is much larger

than n. The Annals of Statistics  35(6):2313–2351  2007.

[43] David Donoho. Compressed sensing. IEEE Transactions on Information Theory  52(4):1289–1306  2006.

[44] Holger Boche  Robert Calderbank  Gitta Kutyniok  and Jan Vybíral. A survey of compressed sensing. In
Holger Boche  Robert Calderbank  Gitta Kutyniok  and Jan Vybíral  editors  Compressed Sensing and its
Applications: MATHEON Workshop 2013. Birkhäuser  2015.

[45] Stéphane Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transac-

tions on Signal Processing  41(12):3397–3415  1993.

[46] Sheng Chen  Stephen Billings  and Wan Luo. Orthogonal least squares methods and their application to

non-linear system identiﬁcation. International Journal of Control  50(5):1873–1896  1989.

11

[47] Scott Chen  David Donoho  and Michael Saunders. Atomic decomposition by basis pursuit. SIAM Review 

43(1):129–159  1999.

[48] Joel Tropp. Greed is good: algorithmic results for sparse approximation. IEEE Transactions on Information

Theory  50(10):2231–2242  2004.

[49] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society

Series B  58(1):267–288  1996.

[50] Leo Geppert  Katja Ickstadt  Alexander Munteanu  Jesn Quedenfeld  and Christian Sohler. Random

projections for Bayesian regression. Statistics and Computing  27:79–101  2017.

[51] Daniel Ahfock  William Astle  and Sylvia Richardson. Statistical properties of sketching algorithms.

arXiv:1706.03665  2017.

[52] Pankaj Agarwal  Sariel Har-Peled  and Kasturi Varadarajan. Geometric approximation via coresets.

Combinatorial and computational geometry  52:1–30  2005.

[53] Michael Langberg and Leonard Schulman. Universal -approximators for integrals. In Proceedings of the

21st Annual ACM–SIAM Symposium on Discrete Algorithms  pages 598–607  2010.

[54] Dan Feldman and Michael Langberg. A uniﬁed framework for approximating and clustering data. In

Proceedings of the 43rd Annual ACM Symposium on Theory of Computing  pages 569–578  2011.

[55] Dan Feldman  Melanie Schmidt  and Christian Sohler. Turning big data into tiny data: constant-size
In Proceedings of the 24th Annual ACM–SIAM

coresets for k-means  pca and projective clustering.
Symposium on Discrete Algorithms  pages 1434–1453  2013.

[56] Olivier Bachem  Mario Lucic  and Andreas Krause. Practical coreset constructions for machine learning.

arXiv:1703.06476  2017.

[57] Vladimir Braverman  Dan Feldman  and Harry Lang. New frameworks for ofﬂine and streaming coreset

constructions. arXiv:1612.00889  2016.

[58] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Research Logistics

Quarterly  3:95–110  1956.

[59] Jacques Guélat and Patrice Marcotte. Some comments on Wolfe’s ‘away step’. Mathematical Programming 

35:110–119  1986.

[60] Martin Jaggi. Revisiting Frank-Wolfe: projection-free sparse convex optimization. In International

Conference on Machine Learning  2013.

[61] Shun ichi Amari. Information Geometry and its Applications. Springer  2016.

[62] Luke Tierney and Joseph Kadane. Accurate approximations for posterior moments and marginal densities.

Journal of the American Statistical Association  81(393):82–86  1986.

12

,Trevor Campbell
Boyan Beronov