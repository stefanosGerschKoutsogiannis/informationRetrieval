2014,Spectral Learning of Mixture of Hidden Markov Models,In this paper  we propose a learning approach for the Mixture of Hidden Markov Models (MHMM) based on the Method of Moments (MoM). Computational advantages of MoM make MHMM learning amenable for large data sets. It is not possible to directly learn an MHMM with existing learning approaches  mainly due to a permutation ambiguity in the estimation process. We show that it is possible to resolve this ambiguity using the spectral properties of a global transition matrix even in the presence of estimation noise. We demonstrate the validity of our approach on synthetic and real data.,Spectral Learning of Mixture of Hidden Markov

Models

Y. Cem S¨ubakan(cid:91)  Johannes Traa(cid:93)  Paris Smaragdis(cid:91) (cid:93) (cid:92)

(cid:91)Department of Computer Science  University of Illinois at Urbana-Champaign

(cid:93)Department of Electrical and Computer Engineering  University of Illinois at Urbana-Champaign

{subakan2  traa2  paris}@illinois.edu

(cid:92)Adobe Systems  Inc.

Abstract

In this paper  we propose a learning approach for the Mixture of Hidden Markov
Models (MHMM) based on the Method of Moments (MoM). Computational ad-
vantages of MoM make MHMM learning amenable for large data sets. It is not
possible to directly learn an MHMM with existing learning approaches  mainly
due to a permutation ambiguity in the estimation process. We show that it is pos-
sible to resolve this ambiguity using the spectral properties of a global transition
matrix even in the presence of estimation noise. We demonstrate the validity of
our approach on synthetic and real data.

1

Introduction

Method of Moments (MoM) based algorithms [1  2  3] for learning latent variable models have
recently become popular in the machine learning community. They provide uniqueness guarantees
in parameter estimation and are a computationally lighter alternative compared to more traditional
maximum likelihood approaches. The main reason behind the computational advantage is that once
the moment expressions are acquired  the rest of the learning work amounts to factorizing a moment
matrix whose size is independent of the number of data items. However  it is unclear how to use these
algorithms for more complicated models such as Mixture of Hidden Markov Models (MHMM).
MHMM [4] is a useful model for clustering sequences  and has various applications [5  6  7]. The
E-step of the Expectation Maximization (EM) algorithm for an MHMM requires running forward-
backward message passing along the latent state chain for each sequence in the dataset in every EM
iteration. For this reason  if the number of sequences in the dataset is large  EM can be computa-
tionally prohibitive.
In this paper  we propose a learning algorithm based on the method of moments for MHMM. We
use the fact that an MHMM can be expressed as an HMM with block diagonal transition matrix.
Having made that observation  we use an existing MoM algorithm to learn the parameters up to a
permutation ambiguity. However  this doesn’t recover the parameters of the individual HMMs. We
exploit the spectral properties of the global transition matrix to estimate a de-permutation mapping
that enables us to recover the parameters of the individual HMMs. We also specify a method that
can recover the number of HMMs under several spectral conditions.

2 Model Deﬁnitions

2.1 Hidden Markov Model
In a Hidden Markov Model (HMM)  an observed sequence x = x1:T = {x1  . . .   xt  . . .   xT} with
xt ∈ RL is generated conditioned on a latent Markov chain r = r1:T = {r1  . . .   rt  . . .   rT}  with

1

rt ∈ {1  . . . M}. The HMM is parameterized by an emission matrix O ∈ RL×M   a transition matrix
A ∈ RM×M and an initial state distribution ν ∈ RM . Given the model parameters θ = (O  A  ν) 
the likelihood of an observation sequence x1:T is deﬁned as follows:

(cid:88)

(cid:88)

T(cid:89)

p(x1:T|θ) =

p(x1:T   r1:T|θ) =

p(xt|rt  O) p(rt|rt−1  A)

r1:T

r1:T

t=1

=1(cid:62)

M A diag(p(xT| :  O))··· A diag(p(x1| :  O)) ν = 1(cid:62)

M

(cid:32) T(cid:89)

(cid:33)

Adiag(O(xt))

ν 

(1)

t=1

where 1M ∈ RM is a column vector of ones  we have switched from index notation to matrix
notation in the second line such that summations are embedded in matrix multiplications  and we
use the MATLAB colon notation to pick a row/column of a matrix. Note that O(xt) := p(xt| :  O).
The model parameters are deﬁned as follows:
• ν(u) = p(r1 = u|r0) = p(r1 = u)
• A(u  v) = p(rt = u|rt−1 = v)  t ≥ 2
• O(:  u) = E[xt|rt = u]

initial latent state distribution
latent state transition matrix
emission matrix

The choice of the observation model p(xt|rt) determines what the columns of O correspond to:
⇒ O(:  u) = E[xt|rt = u] = µu.
• Gaussian: p(xt|rt = u) = N (xt; µu  σ2)
• Poisson: p(xt|rt = u) = PO(xt; λu)
⇒ O(:  u) = E[xt|rt = u] = λu.
• Multinomial: p(xt|rt = u) = Mult(xt; pu  S) ⇒ O(:  u) = E[xt|rt = u] = pu.

The ﬁrst model is a multivariate  isotropic Gaussian with mean µu ∈ RL and covariance σ2I ∈
RL×L. The second distribution is Poisson with intensity parameter λu ∈ RL. This choice is partic-
ularly useful for counts data. The last density is a multinomial distribution with parameter pu ∈ RL
and number of draws S.

2.2 Mixture of HMMs

The Mixture of HMMs (MHMM) is a useful model for clustering sequences where each sequence
is modeled by one of K HMMs. It is parameterized by K emission matrices Ok ∈ RL×M   K
transition matrices1 Ak ∈ RM×M   and K initial state distributions νk ∈ RM as well as a cluster
prior probability distribution π ∈ RK. Given the model parameters θ1:K = (O1:K  A1:K  ν1:K  π) 
the likelihood of an observation sequence xn = {x1 n  x2 n  . . .   xTn n} is computed as a convex
combination of the likelihood of K HMMs:

k=1

K(cid:88)
K(cid:88)
K(cid:88)

k=1

p(xn|θ1:K) =

=

=

(cid:88)
Tn(cid:89)
(cid:32) Tn(cid:89)
(cid:40)

r1:Tn n
1(cid:62)

t=1

J

πk

πk

p(hn = k)p(xn|hn = k  θk) =

πk

p(xn  rn|hn = k  θk)

K(cid:88)

(cid:88)

k=1

r1:Tn  n

p(xt n|rt n  hn = k  Ok)p(rt n|rt−1 n  hn = k  Ak)

(cid:33)

(cid:41)

Akdiag(Ok(xt n))

νk

 

(2)

k=1

t=1

where hn ∈ {1  2  . . .   K} is the latent cluster indicator  rn = {r1 n  r2 n  . . .   rTn n} is the latent
state sequence for the observed sequence xn  and Ok(xt n) is a shorthand for p(xt n| :  hn = k  Ok).
Note that if a sequence is assigned to the kth cluster (hn = k)  the corresponding HMM parameters
θk = (Ak  Ok  νk) are used to generate it.

1Without loss of generality  the number of hidden states for each HMM is taken to be M to keep the notation

uncluttered.

2

3 Spectral Learning for MHMMs

Traditionally  the parameters of an MHMM are learned with the Expectation-Maximization (EM)
algorithm. One drawback of EM is that it requires a good initialization. Another issue is its com-
putational requirements. In every iteration  one has to perform forward-backward message passing
for every sequence  resulting in a computationally expensive process  especially when dealing with
large datasets.
The proposed MoM approach avoids the issues associated with EM by leveraging the information in
various moments computed from the data. Given these moments  which can be computed efﬁciently 
the computation time of the learning algorithm is independent of the amount of data (number of
sequences and their lengths).
Our approach is mainly based on the observation that an MHMM can be seen as a single HMM with a
block-diagonal transition matrix. We will ﬁrst establish this proposition and discuss its implications.
Then  we will describe the proposed learning algorithm.

3.1 MHMM as an HMM with a special structure

Lemma 1:
An MHMM with local parameters θ1:K = (O1:K  A1:K  ν1:K  π) is an HMM with global parame-
ters ¯θ = ( ¯O  ¯A  ¯ν)  where:

¯O = [O1 O2 . . . OK]

 

¯A =

 .

0

0
0

¯ν =

0 . . . AK

π2ν2
...

0 A2 . . .
...

 π1ν1

  

A1 0 . . .
(cid:33)
 diag([O1 O2 . . . OK] (xt))


(cid:41)

0
0

πKνK

 π1ν1

π2ν2
...



πKνK

(3)

(4)

Proof: Consider the MHMM likelihood for a sequence xn:

p(xn|θ1:K) =

πk

1(cid:62)

M

Ak diag(Ok(xt))

νk

(cid:32) Tn(cid:89)
A1 0 . . .

0 A2 . . .
...

t=1

(cid:40)
 Tn(cid:89)
(cid:32) Tn(cid:89)

t=1

K(cid:88)

k=1

=1(cid:62)

M K

=1(cid:62)

M K

(cid:33)

0

0 . . . AK

¯A diag( ¯O(xt))

¯ν 

t=1

where [O1 O2 . . . OK] (xt) := ¯O(xt). We conclude that the MHMM and an HMM with param-
(cid:3)
eters ¯θ describe equivalent probabilistic models.
We see that the state space of an MHMM consists of K disconnected regimes. For each sequence
sampled from the MHMM  the ﬁrst latent state r1 determines what region the entire latent state
sequence lies in.

3.2 Learning an MHMM by learning an HMM

In the previous section  we showed the equivalence between the MHMM and an HMM with a block-
diagonal transition matrix. Therefore  it should be possible to use an HMM learning algorithm such
as spectral learning for HMMs [1  2] to ﬁnd the parameters of an MHMM. However  the true global
parameters ¯θ are recovered inexactly due to noise : ¯θ → ¯θ and state indexing ambiguity via a
permutation mapping P: ¯θ → ¯θP
 ) obtained
from the learning algorithm are in the following form:

 . Consequently  the parameters ¯θP

 = ( ¯OP

   ¯AP

   ¯νP

¯OP
 = ¯OP (cid:62) 

¯AP
 = P ¯AP (cid:62) 

¯νP
 = P ¯ν  

(5)

3

where P is the permutation matrix corresponding to the permutation mapping P.
The presence of the permutation is a fundamental nuisance for MHMM learning since it causes
parameter mixing between the individual HMMs. The global parameters are permuted such that it
becomes impossible to identify individual cluster parameters. A brute force search to ﬁnd P requires
(M K)! trials  which is infeasible for anything but very small M K. Nevertheless  it is possible to

efﬁciently ﬁnd a depermutation mapping (cid:101)P using the spectral properties of the global transition
matrix ¯A. Our ultimate goal in this section is to undo the effect of P by estimating a (cid:101)P that makes

¯AP
 block diagonal despite the presence of the estimation noise .

3.2.1 Spectral properties of the global transition matrix
Lemma 2:
Assuming that each of the local transition matrices A1:K has only one eigenvalue which is 1  the
global transition matrix ¯A has K eigenvalues which are 1.
Proof:

V1Λ1V −1

1

0
0

¯A =

0

. . .
...
0 VKΛKV −1

0

K

 =

V1 . . .
(cid:124)

0
0

0

...
0
0 VK



Λ1 . . .
(cid:123)(cid:122)

0
0

0

...
0
0 ΛK

¯V ¯Λ ¯V −1



V1 . . .

0
0

0

...
0
0 VK

−1
(cid:125)

 

k

where VkΛkV −1
is the eigenvalue decomposition of Ak with Vk as eigenvectors  and Λk as a di-
agonal matrix with eigenvalues on the diagonal. The eigenvalues of A1:K appear unaltered in the
(cid:3)
eigenvalue decomposition of ¯A  and consequently ¯A has K eigenvalues which are 1.
Corollary 1:

¯Ae =(cid:2)¯v11(cid:62)

lim
e→∞

M . . . ¯vk1(cid:62)

(6)

where ¯vk = [0(cid:62) . . . v(cid:62)

k . . . 0(cid:62)](cid:62) and vk is the stationary distribution of Ak  ∀k ∈ {1  . . .   K}.

M

M . . . ¯vK1(cid:62)

(cid:3)  
 V −1
1 0 . . . 0

0 0 . . . 0

...

Proof:

e→∞(VkΛkV −1

lim

k

)e = lim

e→∞ VkΛe

kV −1

k = Vk

k = vk1(cid:62)
M .

0 0 . . . 0

The third step follows because there is only one eigenvalue with magnitude 1. Since multiplying ¯A
by itself amounts to multiplying the corresponding diagonal blocks  we have the structure in (6). (cid:3)
Note that equation (6) points out that the matrix lime→∞ ¯Ae consists of K blocks of size M × M
where the k’th block is vk1(cid:62)
M . A straightforward algorithm can now be developed for making
¯AP block diagonal. Since the eigenvalue decomposition is invariant under permutation  ¯A and
¯AP have the same eigenvalues and eigenvectors. As e → ∞  K clusters of columns appear in
( ¯AP )e. Thus  ¯AP can be made block-diagonal by clustering the columns of ( ¯AP )∞. This idea
is illustrated in the middle row of Figure 1. Note that  in an actual implementation  one would
use a low-rank reconstruction by zeroing-out the eigenvalues that are not equal to 1 in ¯Λ to form
( ¯AP )r := ¯V P (¯ΛP )r( ¯V P )−1 = ( ¯AP )∞  where (¯ΛP )r ∈ RM K×M K is a diagonal matrix with only
K non-zero entries  corresponding to the eigenvalues which are 1.
This algorithm corresponds to the noiseless case ¯AP. In practice  the output of the learning algorithm
 )e  as e → ∞  as illustrated in
is ¯AP
the bottom row of Figure 1. We can see that the three-cluster structure no longer holds for large e.
Instead  the columns of the transition matrix converge to a global stationary distribution.

 and the clear structure in Equation (6) no longer holds in ( ¯AP

3.2.2 Estimating the permutation in the presence of noise

In the general case with noise   we lose the spectral property that the global transition matrix
has K eigenvalues which are 1. Consequently  the algorithm described in Section 3.2.1 cannot be

4

Figure 1: (Top left) Block-diagonal transition matrix after e-fold exponentiation. Each block con-
verge to its own stationary distribution. (Top right) Same as above with permutation. (Bottom)
Corrupted and permuted transition matrix after exponentiation. The true number K = 3 of HMMs
is clear for intermediate values of e  but as e → ∞  the columns of the matrix converge to a global
stationary distribution.

applied directly to make ¯AP
one eigenvalue with unit magnitude and lime→∞( ¯AP

 block diagonal. In practice  the estimated transition matrix has only
 )e converges to a global stationary distribution.

However  if the noise  is sufﬁciently small  a depermutation mapping (cid:101)P and the number of HMM
clusters K can be successfully estimated. We now specify the spectral conditions for this.
k := αkλ1 k for k ∈ {1  . . .   K} as the global  noisy eigenvalues with
Deﬁnition 1: We denote λG
|λG
k+1|  ∀k ∈ {1  . . .   K − 1}  where λ1 k is the original eigenvalue of the kth cluster
k| ≥ |λG
with magnitude 1 and αk is the noise that acts on that eigenvalue (note that α1 = 1). We denote
j k := βj kλj k for j ∈ {2  . . .   M} and k ∈ {1  . . .   K} as the local  noisy eigenvalues with
λL
|λL
j+1 k|  ∀k ∈ {1  . . .   K} and ∀j ∈ {1  . . .   M − 1}  where λj k is the original eigenvalue
j k| ≥ |λL
with the jth largest magnitude in the kth cluster  and βj k is the noise that acts on that eigenvalue.
Deﬁnition 2: The low-rank eigendecomposition of the estimated transition matrix ¯AP
 is deﬁned as
 := V ΛrV −1  where V is a matrix with eigenvectors in the columns and Λr is a diagonal matrix
Ar
with eigenvalues λG
Conjecture 1:
|λL
2 k|  then Ar can be formed using the eigen-decomposition of ¯AP
If |λG
√
 − Ar(cid:107)F ≤ O(1/
with high probability  (cid:107)Ar
vectors.
Justiﬁcation:

 . Then 
T N )  where T N is the total number of observed

1:K in the ﬁrst K entries.

K| > max

k∈{1 ... K}

(cid:107)Ar

 − Ar(cid:107)F = (cid:107)Ar

 − A + A − Ar(cid:107)F ≤(cid:107)Ar

 − A(cid:107)F + (cid:107)A − Ar(cid:107)F

=(cid:107)A − Ar(cid:107)F + (cid:107)A − A + A¯r
≤(cid:107)A − Ar(cid:107)F + (cid:107)A¯r
√
≤2KM + O(1/

(cid:107)F + (cid:107)A − A(cid:107)F
T N ) = O(1/

(cid:107)F
√

T N )  w.h.p. 

where A is used for ¯AP to reduce the notation clutter (and similarly Ar for ( ¯AP )r and so on)  we
 = V Λ¯rV −1  where Λ¯r is a
used the triangle inequality for the ﬁrst and second inequalities and A¯r
diagonal matrix of eigenvalues with the ﬁrst K diagonal entries equal to zero (complement of Λr).
For the last inequality  we used the fact that A ∈ RM K×M K has entries in the interval [0  1] and we
used the sample complexity result from [1]. The bound speciﬁed in [1] is for a mixture model  but
since the two models are similar and the estimation procedure is almost identical  we are reusing it.
We believe that further analysis of the spectral learning algorithm is out of the scope of this paper 
(cid:3)
so we leave this proposition as a conjecture.
Conjecture 1 asserts that  if we have enough data we should obtain an estimate Ar
 close to Ar in the
squared error sense. Furthermore  if the following mixing rate condition is satisﬁed  we will be able
to identify the number of clusters K from the data.

5

 e: 1 rt rt+1 e: 5 rt e: 10 rt e: 20 rt e: 1 rt rt+1 e: 5 rt e: 10 rt e: 20 rt e: 1 rt rt+1 e: 5 rt e: 10 rt e: 20 rt (Left) Number of signiﬁcant eigenvalues across exponentiations.

Figure 2:
Longevity L˜λK(cid:48) with respect to the eigenvalue index K(cid:48).
Deﬁnition 3: Let(cid:101)λk denote the kth largest eigenvalue (in decreasing order) of the estimated transi-

(Right) Spectral

tion matrix ¯AP

 . We deﬁne the quantity 

> 1 − γ

−

> 1 − γ

 

(7)

(cid:32)(cid:34) (cid:80)K(cid:48)
(cid:80)M K
l=1 |˜λl|e
l(cid:48)=1 |˜λl(cid:48)|e

∞(cid:88)

e=1

L˜λK(cid:48) :=

(cid:35)

(cid:34)(cid:80)K(cid:48)−1
(cid:80)M K
|˜λl|e
l(cid:48)=1 |˜λl(cid:48)|e

l=1

(cid:35)(cid:33)

max

If |λG

K| >

as the spectral longevity of ˜λK(cid:48). The square brackets [.] denote an indicator function which outputs
1 if the argument is true and 0 otherwise  and γ is a small number such as machine epsilon.
|(cid:101)λK(cid:48)+1||(cid:101)λK(cid:48)−1| = K  for K(cid:48) ∈

Lemma 3:
{2  3  . . .   M K − 1}  then arg maxK(cid:48) L˜λK(cid:48) = K.
Proof: The ﬁrst condition ensures that the top K eigenvalues are global eigenvalues. The second
condition is about the convergence rates of the two ratios in equation (7). The ﬁrst indicator function
(cid:80)K(cid:48)−1
has the following summation inside:

|λL
2 k| and arg maxK(cid:48)

|(cid:101)λK(cid:48)|2

k∈{1 ... K}

(cid:80)K(cid:48)−1
l(cid:48)=1 |˜λl(cid:48)|e + |˜λK(cid:48)|e + |˜λK(cid:48)+1|e +(cid:80)M K

|˜λl|e + |˜λK(cid:48)|e

l=1

=

(cid:80)K(cid:48)
(cid:80)M K
l=1 |˜λl|e
l(cid:48)=1 |˜λl(cid:48)|e

l(cid:48)=K(cid:48)+2 |˜λl(cid:48)|e

.

The rate at which this term goes to 1 is determined by the spectral gap |λK(cid:48)|/|λK(cid:48)+1|. The smaller
this ratio is  the faster the term (it is non-decreasing w.r.t. e) converges to 1. For the second indi-
cator function inside L˜λK(cid:48)   we can do the same analysis and see that the convergence rate is again
determined by the gap |λK(cid:48)−1|/|λK(cid:48)|. The ratio of the two spectral gaps determines the spectral
|(cid:101)λK(cid:48)+1||(cid:101)λK(cid:48)−1|  we have arg maxK(cid:48) L˜λK(cid:48) = K. (cid:3)
longevity. Hence  for the K(cid:48) with largest ratio
is not too noisy  we can
Lemma 3 tells us the following.
determine the number of clusters by choosing the value of K(cid:48) such that it maximizes L˜λK(cid:48) . This
corresponds to exponentiating the sorted eigenvalues in a ﬁnite range  and recording the number of
non-negligible eigenvalues. This is depicted in Figure 2.

If the estimated transition matrix ¯AP

|(cid:101)λK(cid:48)|2



3.3 Proposed Algorithm

In previous sections  we have shown that the permutation caused by the MoM estimation procedure
can be undone  and we have proposed a way to estimate the number of clusters K. We summarize
the whole procedure in Algorithm 1.

4 Experiments

4.1 Effect of noise on depermutation algorithm

We have tested the algorithm’s performance with respect to amount of data. We used the parameters
K = 3  M = 4  L = 20  and we have 2 sequences with length T for each cluster. We used a
Gaussian observation model with unit observation variance and the columns of the emission matrices
O1:K were drawn from zero mean spherical Gaussian with variance 2. Results for 10 uniformly

6

1020123456789 e K0 No. of Significant Eigenvalues 1234567890246810 Eigenvalue Index Spectral Longevity Spectral Longevity of Eigenvalues Method of Moments Parameter Estimation

 ) = HMM MethodofMoments (x1:N   M K)

( ¯OP

   ¯AP

Depermutation

Algorithm 1 Spectral Learning for Mixture of Hidden Markov Models
Inputs: x1:N : Sequences  M K : total number of states of global HMM.

Output: (cid:98)θ =

(cid:16)(cid:98)O1:(cid:98)K  (cid:98)A1:(cid:98)K

(cid:17)

: MHMM parameters



Find eigenvalues of ¯AP
Exponentiate eigenvalues for each discrete value e in a sufﬁciently large range.

Identify (cid:98)K as the eigenvalue with largest longevity.
Compute rank-(cid:98)K reconstruction Ar
 with (cid:98)K clusters to ﬁnd a depermutation mapping (cid:101)P via cluster labels.
 according to (cid:101)P.
Form(cid:98)θ by choosing corresponding blocks from depermuted ¯OP
Return(cid:98)θ.

Cluster the columns of Ar
 and ¯AP
Depermute ¯OP

 via eigendecomposition.

 and ¯AP
 .

Figure 3: Top row: Euclidean distance vs T . Second row: Noisy input matrix. Third row: Noisy
reconstruction Ar
. Bottom row: Depermuted matrix  numbers at the bottom indicate the estimated
number of clusters.

spaced sequence lengths from 10 to 1000 are shown in Figure 3. On the top row  we plot the total
error (from centroid to point) obtained after ﬁtting k-means with true number of HMM clusters. We
can see that the correct number of clusters K = 3 as well as the block-diagonal structure of the
transition matrix is correctly recovered even in the case where T = 20.

4.2 Amount of data vs accuracy and speed

We have compared clustering accuracies of EM and our approach on data sampled from a Gaussian
emission MHMM. Means of each state of each cluster is drawn from a zero mean unit variance
Gaussian  and observation covariance is spherical with variance 2. We set L = 20  K = 5  M =
3. We used uniform mixing proportions and uniform initial state distribution. We evaluated the
clustering accuracies for 10 uniformly spaced sequence lengths (every sequence has the same length)
between 20 and 200  and 10 uniformly spaced number of sequences between 1 and 100 for each
cluster. The results are shown in Figure 4. Although EM seems to provide higher accuracy on

7

101202303404505606707808901000012Euclidean Distance vs Sequence LengthTEuc. Dist.3333333333Figure 4: Clustering accuracy and run time results for synthetic data experiments.

Algorithm
Spectral

Table 1: Clustering accuracies for handwritten digit dataset.
2v5
99
100
100

EM init. w/ Spectral
EM init. at Random

1v3
70
99
99

1v4
54
100
98

1v2
100
100
96

2v3
83
96
83

2v4
99
100
100

regions where we have less data  spectral algorithm is much faster. Note that  in spectral algorithm
we include the time spent in moment computation. We used four restarts for EM  and take the result
with highest likelihood  and used an automatic stopping criterion.

4.3 Real data experiment

We ran an experiment on the handwritten character trajectory dataset from the UCI machine learn-
ing repository [8]. We formed pairs of characters and compared the clustering results for three
algorithms: the proposed spectral learning approach  EM initialized at random  and EM initialized
with MoM algorithm as explored in [9]. We take the maximum accuracy of EM over 5 random ini-
tializations in the third row. We set the algorithm parameters to K = 2 and M = 4. There are 140
sequences of average length 100 per class. In the original data  L = 3  but to apply MoM learning 
we require that M K < L. To achieve this  we transformed the data vectors with a cubic polyno-
mial feature transformation such that L = 10 (this is the same transformation that corresponds to
a polynomial kernel). The results from these trials are shown in Table 1. We can see that although
spectral learning doesn’t always surpass randomly initialized EM on its own  it does serve as a very
good initialization scheme.

5 Conclusions and future work

We have developed a method of moments based algorithm for learning mixture of HMMs. Our
experimental results show that our approach is computationally much cheaper than EM  while being
comparable in accuracy. Our real data experiment also show that our approach can be used as a
good initialization scheme for EM. As future work  it would be interesting to apply the proposed
approach on other hierarchical latent variable models.
Acknowledgements: We would like to thank Taylan Cemgil  David Forsyth and John Hershey for
valuable discussions. This material is based upon work supported by the National Science Founda-
tion under Grant No. 1319708.

References
[1] A. Anandkumar  D. Hsu  and S.M. Kakade. A method of moments for mixture models and

hidden markov models. In COLT  2012.

[2] A. Anandkumar  R. Ge  D. Hsu  S.M. Kakade  and M. Telgarsky. Tensor decompositions for

learning latent variable models. arXiv:1210.7559v2  2012.

8

20208080808020806040536582838077629510082687397668082861008810058766579858110098811007378619780601001001001007977696984100100100100100767769100100887810010010088788880100100100100100755863827810010010079100100788680100877780100100100 T N/K Accuracy (%) of spectral algorithm 1031731161582001123456781006080604080608060604010010010010010010010080801001001001001007110010080100801001001001001001001001001001001001001001001001001001001001001001001001001001001008010010010010010010010010010010080100100100100100100100100100808010010010010010010080100100100100100100100100100100100100100 T N/K Accuracy (%) of EM algorithm 10317311615820011234567810022111121322333344455233455667733456678910335678911121334578101113141534689111314161735791113141618203571012141618202236811131618202325 T N/K Run time (s) of spectral algorithm 1031731161582001123456781001516341933564756752713817818731336760684657361454235296370724550969109310561616894273785297031241187314341418243316529075473413011323164618513074242317244466297014771098189232582030240422958810401106168319431861239636033332266791133520202457266223114330513758492338651664259737614431391441334247491521685520461879187539203609362987196890 T N/K Run time (s) of EM algorithm 103173116158200112345678100[3] Daniel Hsu  Sham M. Kakade  and Tong Zhang. A spectral algorithm for learning hidden
markov models a spectral algorithm for learning hidden markov models. Journal of Computer
and System Sciences  (1460-1480)  2009.

[4] P. Smyth. Clustering sequences with hidden markov models. In Advances in neural information

processing systems  1997.

[5] Yuting Qi  J.W. Paisley  and L. Carin. Music analysis using hidden markov mixture models.

Signal Processing  IEEE Transactions on  55(11):5209 –5224  nov. 2007.

[6] A. Jonathan  S. Sclaroff  G. Kollios  and V. Pavlovic. Discovering clusters in motion time-series

data. In CVPR  2003.

[7] Tim Oates  Laura Firoiu  and Paul R. Cohen. Clustering time series with hidden markov models
and dynamic time warping. In In Proceedings of the IJCAI-99 Workshop on Neural  Symbolic
and Reinforcement Learning Methods for Sequence Learning  pages 17–21  1999.

[8] K. Bache and M. Lichman. UCI machine learning repository  2013.
[9] Arun Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear regressions.

In International Conference on Machine Learning (ICML)  2013.

9

,Cem Subakan
Johannes Traa
Paris Smaragdis