2015,Sparse Linear Programming via Primal and Dual Augmented Coordinate Descent,Over the past decades  Linear Programming (LP) has been widely used in different areas and considered as one of the mature technologies in numerical optimization. However  the complexity offered by state-of-the-art algorithms (i.e. interior-point method and primal  dual simplex methods) is still unsatisfactory for problems in machine learning with huge number of variables and constraints. In this paper  we investigate a general LP algorithm based on the combination of Augmented Lagrangian and Coordinate Descent (AL-CD)  giving an iteration complexity of $O((\log(1/\epsilon))^2)$ with $O(nnz(A))$ cost per iteration  where $nnz(A)$ is the number of non-zeros in the $m\times n$ constraint matrix $A$  and in practice  one can further reduce cost per iteration to the order of non-zeros in columns (rows) corresponding to the active primal (dual) variables through an active-set strategy. The algorithm thus yields a tractable alternative to standard LP methods for large-scale problems of sparse solutions and $nnz(A)\ll mn$. We conduct experiments on large-scale LP instances from $\ell_1$-regularized multi-class SVM  Sparse Inverse Covariance Estimation  and Nonnegative Matrix Factorization  where the proposed approach finds solutions of $10^{-3}$ precision orders of magnitude faster than state-of-the-art implementations of interior-point and simplex methods.,Sparse Linear Programming via

Primal and Dual Augmented Coordinate Descent

Ian E.H. Yen ∗ Kai Zhong ∗ Cho-Jui Hsieh †
∗ {ianyen pradeepr inderjit}@cs.utexas.edu

∗ University of Texas at Austin

† chohsieh@ucdavis.edu

zhongkai@ices.utexas.edu

Pradeep Ravikumar ∗

† University of California at Davis

Inderjit S. Dhillon ∗

Abstract

Over the past decades  Linear Programming (LP) has been widely used in different
areas and considered as one of the mature technologies in numerical optimization.
However  the complexity offered by state-of-the-art algorithms (i.e. interior-point
method and primal  dual simplex methods) is still unsatisfactory for problems in
machine learning with huge number of variables and constraints. In this paper 
we investigate a general LP algorithm based on the combination of Augmented
Lagrangian and Coordinate Descent (AL-CD)  giving an iteration complexity of
O((log(1/))2) with O(nnz(A)) cost per iteration  where nnz(A) is the number
of non-zeros in the m× n constraint matrix A  and in practice  one can further re-
duce cost per iteration to the order of non-zeros in columns (rows) corresponding
to the active primal (dual) variables through an active-set strategy. The algorithm
thus yields a tractable alternative to standard LP methods for large-scale problems
of sparse solutions and nnz(A) (cid:28) mn. We conduct experiments on large-scale
LP instances from (cid:96)1-regularized multi-class SVM  Sparse Inverse Covariance Es-
timation  and Nonnegative Matrix Factorization  where the proposed approach
ﬁnds solutions of 10−3 precision orders of magnitude faster than state-of-the-art
implementations of interior-point and simplex methods. 1

1

Introduction

Linear Programming (LP) has been studied since the early 19th century and has become one of
the representative tools of numerical optimization with wide applications in machine learning such
as (cid:96)1-regularized SVM [1]  MAP inference [2]  nonnegative matrix factorization [3]  exemplar-
based clustering [4  5]  sparse inverse covariance estimation [6]  and Markov Decision Process [7].
However  as the demand for scalability keeps increasing  the scalability of existing LP solvers has
become unsatisfactory. In particular  most algorithms in machine learning targeting large-scale data
have a complexity linear to the data size [8  9  10]  while the complexity of state-of-the-art LP
solvers (i.e. Interior-Point method and Primal  Dual Simplex methods) is still at least quadratic in
the number of variables or constraints [11].
The quadratic complexity comes from the need to solve each linear system exactly in both simplex
and interior point method. In particular  the simplex method  when traversing from one corner point
to another  requires solution to a linear system that has dimension linear to the number of variables
or constraints  while in an Interior-Point method  ﬁnding the Newton direction requires solving a
linear system of similar size. While there are sparse variants of LU and Cholesky decomposition that
can utilize the sparsity pattern of matrix in a linear system  the worst-case complexity for solving
such system is at least quadratic to the dimension except for very special cases such as a tri-diagonal
or band-structured matrix.

1Our solver has been released here: http://www.cs.utexas.edu/˜ianyen/LPsparse/

1

For interior point method (IPM)  one remedy to the high complexity is employing an iterative method
such as Conjugate Gradient (CG) to solve each linear system inexactly. However  this can hardly
tackle the ill-conditioned linear systems produced by IPM when iterates approach boundary of con-
straints [12]. Though substantial research has been devoted to the development of preconditioners
that can help iterative methods to mitigate the effect of ill-conditioning [12  13]  creating a precon-
ditioner of tractable size is a challenging problem by itself [13]. Most commercial LP software thus
still relies on exact methods to solve the linear system.
On the other hand  some dual or primal (stochastic) sub-gradient descent methods have cheap cost
for each iteration  but require O(1/2) iterations to ﬁnd a solution of  precision  which in practice
can even hardly ﬁnd a feasible solution satisfying all constraints [14].
Augmented Lagrangian Method (ALM) was invented early in 1969  and since then there have been
several works developed Linear Program solver based on ALM [15  16  17]. However  the challenge
of ALM is that it produces a series of bound-constrained quadratic problems that  in the traditional
sense  are harder to solve than linear system produced by IPM or Simplex methods [17]. Speciﬁcally 
in a Projected-CG approach [18]  one needs to solve several linear systems via CG to ﬁnd solution
to the bound-constrained quadratic program  while there is no guarantee on how many iterations
it requires. On the other hand  Projected Gradient Method (PGM)  despite its guaranteed iteration
complexity  has very slow convergence in practice. More recently  Multi-block ADMM [19  20]
was proposed as a variant of ALM that  for each iteration  only updates one pass (or even less)
blocks of primal variables before each dual update  which however  requires a much smaller step
size in the dual update to ensure convergence [20  21] and thus requires large number of iterations
for convergence to moderate precision. To our knowledge  there is still no report on a signiﬁcant
improvement of ALM-based methods over IPM or Simplex method for Linear Programming.
In the recent years  Coordinate Descent (CD) method has demonstrated efﬁciency in many machine
learning problems with bound constraints or other non-smooth terms [9  10  22  23  24  25] and has
solid analysis on its iteration complexity [26  27]. In this work  we show that CD algorithm can
be naturally combined with ALM to solve Linear Program more efﬁciently than existing methods
on large-scale problems. We provide an O((log(1/))2) iteration complexity of the Augmented
Lagrangian with Coordinate Descent (AL-CD) algorithm that bounds the total number of CD up-
dates required for an -precise solution  and describe an implementation of AL-CD that has cost
O(nnz(A)) for each pass of CD. In practice  an active-set strategy is introduced to further reduce
cost of each iteration to the active size of variables and constraints for primal-sparse and dual-sparse
LP respectively  where a primal-sparse LP has most of variables being zero  and a dual-sparse LP
has few binding constraints at the optimal solution. Note  unlike in IPM  the conditioning of each
subproblem in ALM does not worsen over iterations [15  16]. The AL-CD framework thus provides
an alternative to interior point and simplex methods when it is infeasible to exactly solving an n× n
(or m × m) linear system.

2 Sparse Linear Program

We are interested in solving linear programs of the form

min
x∈Rn
s.t.

f (x) = cT x
AI x ≤ bI   AEx = bE
xj ≥ 0  j ∈ [nb]

(1)

where AI is mI by n matrix of coefﬁcients and AE is mE by n. Without loss of generality  we
assume non-negative constraints are imposed on the ﬁrst nb variables  denoted as xb  such that
x = [xb; xf ] and c = [cb; cf ]. The inequality and equality coefﬁcient matrices can then be
partitioned as AI = [AI b AI f ] and AE = [AE b AE f ]. The dual problem of (1) then takes the
form

min
y∈Rm
s.t.

g(y) = bT y
− AT
yi ≥ 0 

b y ≤ cb   −AT
i ∈ [mI ].

f y = cf

2

(2)

where m = mI + mE  b = [bI ; bE]  Ab = [AI b; AE b]  Af = [AI f ; AE f ]  and y = [yI ; yE]. In
most of LP occur in machine learning  m and n are both at scale in the order 105 ˜106  for which an
algorithm with cost O(mn)  O(n2) or O(m2) is unacceptable. Fortunately  there are usually various
types of sparsity present in the problem that can be utilized to lower the complexity.
First  the constraint matrix A = [AI ; AE] are usually pretty sparse in the sense that nnz(A) (cid:28) mn 
and one can compute matrix-vector product Ax in O(nnz(A)). However  in most of current LP
solvers  not only matrix-vector product but also a linear system involving A needs to be solved 
which in general  has cost much more than O(nnz(A)) and can be up to O(min(n3  m3)) in the
worst case.
In particular  the simplex-type methods  when moving from one corner to another 
requires solving a linear system that involves a sub-matrix of A with columns corresponding to the
basic variables [11]  while in an interior point method (IPM)  one also needs to solve a normal
equation system of matrix ADtAT to obtain the Newton direction  where Dt is a diagonal matrix
that gradually enforces complementary slackness as IPM iteration t grows [11]. While one remedy
to the high complexity is to employ iterative method such as Conjugate Gradient (CG) to solve the
system inexactly within IPM  this approach can hardly handle the ill-conditionedness occurs when
IPM iterates approaches boundary [12]. On the other hand  the Augmented Lagrangian approach
does not have such asymptotic ill-conditionedness and thus an iterative method with complexity
linear to O(nnz(A)) can be used to produce sufﬁciently accurate solution for each sub-problem.
Besides sparsity in the constraint matrix A  two other types of structures  which we termed primal
and dual sparsity  are also prevalent in the context of machine learning. A primal-sparse LP refers
to an LP with optimal solution x∗ comprising only few non-zero elements  while a dual-sparse LP
refers to an LP with few binding constraints at optimal  which corresponds to the non-zero dual
variables. In the following  we give two examples of sparse LP.

L1-Regularized Support Vector Machine The problem of L1-regularized multi-class Support
Vector Machine [1]

k(cid:88)

m=1

(cid:107)wm(cid:107)1 +

l(cid:88)
ξi
mxi ≥ em
i − ξi  ∀(i  m)

i=1

min
wm ξi

λ

xi − wT

wT
yi

(3)

i = 0 if yi = m  em

s.t.
where em
i = 1 otherwise. The task is dual-sparse since among all samples i and
class k  only those leads to misclassiﬁcation will become binding constraints. The problem (3) is
also primal-sparse since it does feature selection through (cid:96)1-penalty. Note the constraint matrix in
(3) is also sparse since each constraint only involves two weight vectors  and the pattern xi can be
also sparse.

Sparse Inverse Covariance Estimation The Sparse Inverse Covariance Estimation aims to ﬁnd
a sparse matrix Ω that approximate the inverse of Covariance matrix. One of the most popular
approach to this solves a program of the form [6]
(cid:107)Ω(cid:107)1
(cid:107)SΩ − Id(cid:107)max ≤ λ

(4)
which is primal-sparse due to the (cid:107).(cid:107)1 penalty. The problem has a dense constraint matrix  which
however  has special structure where the coefﬁcient matrix S can be decomposed into a product of
two low-rank and (possibly) sparse n by d matrices S = Z T Z. In case Z is sparse or n (cid:28) d  this
decomposition can be utilized to solve the Linear Program much more efﬁciently. We will discuss
on how to utilize such structure in section 4.3.

min
Ω∈Rd×d
s.t.

3 Primal and Dual Augmented Coordinate Descent

In this section  we describe an Augmented Lagrangian method (ALM) that carefully tackles the
sparsity in a LP. The choice between Primal and Dual ALM depends on the type of sparsity present
in the LP. In particular  a primal AL method can solve a problem of few non-zero variables more
efﬁciently  while dual ALM will be more efﬁcient for problem with few binding constraints. In the
following  we describe the algorithm only from the primal point of view  while the dual version can
be obtained by exchanging the roles of primal (1) and dual (2).

3

Algorithm 1 (Primal) Augmented Lagrangian Method

Initialization: y0 ∈ Rm and η0 > 0.
repeat

(cid:20) AI xt+1 − bI + ξt+1

1. Solve (6) to obtain (xt+1  ξt+1) from yt.
AExt+1 − bE

2. Update yt+1 = yt + ηt
3. t = t + 1.
4. Increase ηt by a constant factor if necessary.

until (cid:107)[AI xt − bI ]+(cid:107)∞ ≤ p and (cid:107)AExt − bE(cid:107)∞ ≤ .

(cid:21)

.

3.1 Augmented Lagrangian Method (Dual Proximal Method)
Let g(y) be the dual objective function (2) that takes ∞ if y is infeasible. The primal AL algorithm
can be interpreted as a dual proximal point algorithm [16] that for each iteration t solves

yt+1 = argmin

g(y) +

y

(cid:107)y − yt(cid:107)2.

1
2ηt

(5)

Since g(y) is nonsmooth  (5) is not easier to solve than the original dual problem. However  the dual
of (5) takes the form:

(cid:13)(cid:13)(cid:13)(cid:13) AI x − bI + ξ

AEx − bE

(cid:20) yt

I
yt
E

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)2

+

1
ηt

(6)

min
x  ξ

s.t.

F (x  ξ) = cT x +
xb ≥ 0  ξ ≥ 0 

ηt
2

which is a bound-constrained quadratic problem. Note given (x  ξ) as Lagrangian Multipliers of (5) 
the corresponding y minimizing Lagrangian L(x  ξ  y) is

(cid:20) AI x − bI + ξ

AEx − bE

(cid:21)

(cid:21)

(cid:20) yt

I
yt
E

y(x  ξ) = ηt

+

 

(7)

and thus one can solve (x∗  ξ∗) from (6) and ﬁnd yt+1 through (7). The resulting algorithm is
sketched in Algorithm 1. For problem of medium scale  (6) is not easier to solve than a linear
system due to non-negative constraints  and thus an ALM is not preferred to IPM in the traditional
sense. However  for large-scale problem with m × n (cid:29) nnz(A)  the ALM becomes advantageous
since: (i) the conditioning of (6) does not worsen over iterations  and thus allows iterative methods
to solve it approximately in time proportional to O(nnz(A)). (ii) For a primal-sparse (dual-sparse)
problem  most of primal (dual) variables become binding at zero as iterates approach to the optimal
solution  which yields a potentially much smaller subproblem.

3.2 Solving Subproblem via Coordinate Descent

Given a dual solution yt  we employ a variant of Randomized Coordinate Descent (RCD) method
to solve subproblem (6). First  we note that  given x  the part of variables in ξ can minimized in
closed-form as

(8)
where function [v]+ truncates each element of vector v to be non-negative as [v]+i = max{vi  0}.
Then (6) can be re-written as

I /ηt]+ 

ξ(x) = [bI − AI x − yt

(cid:13)(cid:13)(cid:13)(cid:13) [AI x − bI + yt

AEx − bE + yt

I /ηt]+
E/ηt

(cid:13)(cid:13)(cid:13)(cid:13)2

(9)

min

x

s.t.

ˆF (x) = cT x +
xb ≥ 0.

ηt
2

4

Algorithm 2 RCD for subproblem (6)

Algorithm 3 PN-CG for subproblem (6)

INPUT: ηt > 0 and (xt 0  wt 0  vt 0) satisfying
relation (11)  (12).
OUTPUT: (xt k  wt k  vt k)
repeat

j

1. Pick a coordinate j uniformly at random.
2. Compute ∇j ˆF (x)  ∇2
ˆF (x).
3. Obtain Newton direction d∗
j .
4. Do line search (15) to ﬁnd step size.
5. Update xt k+1 ← xt k + βrd∗
j .
6. Maintain relation (11)  (12).
7. k ← k + 1.

INPUT: ηt > 0 and (xt 0  wt 0  vt 0) satisfying
relation (11)  (12).
OUTPUT: (xt k  wt k  vt k)
repeat

1. Identify active variables At k.
2. Compute [∇jF (x)]At k and set Dt k.
3. Find Newton direction d∗
At k with CG.
4. Find step size via projected line search.
5. Update xt k+1 ← (xt k + βrd∗
6. Maintain relation (11)  (12).
7. k ← k + 1.

j )+.

until (cid:107)d∗(x)(cid:107)∞ ≤ t.

At k(cid:107)∞ ≤ t.
Denote the objective function as ˆF (x). The gradient of (9) can be expressed as

until (cid:107)d∗

∇ ˆF (x) = c + ηtAT

I [w]+ + ηtAT

Ev

where

w = AI x − bI + yt
v = AEx − bE + yt

I /ηt
E/ηt 

and the (generalized) Hessian of (9) is

∇2 ˆF (x) = ηtAT

I D(w)AI + ηtAT

EAE 

(10)

(11)
(12)

(13)

where D(w) is an mI by mI diagonal matrix with Dii(w) = 1 if wi > 0 and Dii = 0 otherwise.
The RCD algorithm then proceeds as follows. In each iteration k  it picks a coordinate from j ∈
{1  ..  n} uniformly at random and minimizes w.r.t. the coordinate. The minimization is conducted
by a single-variable Newton step  which ﬁrst ﬁnds the Newton direction d∗
j through minimizing a
quadratic approximation

d∗
j = argmin

d

s.t.

∇j ˆF (xt k)d +
j + d ≥ 0 
xt k

∇2

j

1
2

ˆF (xt k)d2

(14)

and then conducted a line search to ﬁnd the smallest r ∈ {0  1  2  ...} satisfying

ˆF (xt k + βrd∗

(15)
for some line-search parameter σ ∈ (0  1/2]  β ∈ (0  1)  where ej denotes a vector with only jth
element equal to 1 and all others equal to 0. Note the single-variable problem (14) has closed-form
solution

j ej) − ˆF (xt k) ≤ σβr(∇j ˆF (xt k)d∗
j ).

d∗
j =

j − ∇j ˆF (xt k
xt k

j )/∇2

j

ˆF (xt k
j )

− xt k

j

 

(16)

(cid:104)

(cid:105)

+

which in a naive implementation  takes O(nnz(A)) time due to the computation of (11) and (12).
However  in a clever implementation  one can maintain the relation (11)  (12) as follows whenever
a coordinate xj is updated by βrd∗

(cid:21)

(cid:20) wt k+1

j

vt k+1

=

(cid:21)

(cid:20) wt k

vt k

+ βrd∗

j

(cid:21)

(cid:20) aI

j
aE
j

 

(17)

where aj = [aI
second-derivative of jth coordinate

j ; aE

j ] denotes the jth column of AI and AE. Then the gradient and (generalized)

∇j ˆF (x) = cj + ηt(cid:104)aI
∇2

(cid:32) (cid:88)

ˆF (x) = ηt

j

j   v(cid:105)

(cid:88)
j   [w]+(cid:105) + ηt(cid:104)aE

(aI

i j)2 +

(aE

i j)2

(cid:33)

(18)

i:wi>0

i

5

can be computed in O(nnz(aj)) time. Similarly  for each coordinate update  one can evaluate the
j ej) − ˆF (xt k) in O(nnz(aj)) by only computing terms
difference of function value ˆF (xt k + d∗
related to the jth variable.
The overall procedure for solving subproblem is summarized in Algorithm 2. In practice  a random
permutation is used instead of uniform sampling to ensure that every coordinate is updated once
before proceeding to the next round  which can speed up convergence and ease the checking of stop-
ping condition (cid:107)d∗(x)(cid:107)∞ ≤ t  and an active-set strategy is employed to avoid updating variables
with d∗

j = 0. We describe details in section 4

3.3 Convergence Analysis

In this section  we prove the iteration complexity of AL-CD method. Existing analysis [26  27]
shows that Randomized Coordinate Descent can be up to n times faster than Gradient-based methods
in certain conditions. However  to prove a global linear rate of convergence the analysis requires
objective function to be strongly convex  which is not true for our sub-problem (6). Here we follow
the approach in [28  29] to show global linear convergence of Algorithm 2 by utilizing the fact that 
when restricted to a constant subspace  (6) is strongly convex. All proofs will be included in the
appendix.
Theorem 1 (Linear Convergence). Denote F ∗ as the optimum of (6) and ¯x = [x; ξ]. The iterates
{¯xk}∞

k=0 of the RCD Algorithm 2 has

E[F (¯xk+1)] − F ∗ ≤

γ = max(cid:8)16ηtM θ(F 0 − F ∗)   2M θ(1 + 4L2

where
M = maxj∈[¯n] (cid:107)¯aj(cid:107)2 is an upper bound on coordinate-wise second derivative  and Lg is local
Lipschitz-continuous constant of function g(z) = ηt(cid:107)z− b + yt/ηt(cid:107)2  and θ is constant of Hoffman’s
bound that depends on the polyhedron formed by the set of optimal solutions.

(19)

(cid:19)(cid:0)E[F (¯xk)] − F ∗(cid:1)  
g)   6(cid:9)  

(cid:18)

1 − 1
γn

Then the following theorem gives a bound on the number of iterations required to ﬁnd an 0-precise
solution in terms of the proximal minimization (5).
Theorem 2 (Inner Iteration Complexity). Denote y(¯xk) as the dual solution (7) corresponding to
the primal iterate ¯xk. To guarantee

with probability 1 − p  it sufﬁces running RCD Algorithm 2 for number of iterations

(cid:107)y(¯xk) − yt+1(cid:107) ≤ 0

(cid:32)(cid:115)

2(F (¯x0) − F ∗)

ηtp

1
0

(cid:33)

.

k ≥ 2γn log

Now we prove the overall iteration complexity of AL-CD. Note that existing linear convergence
analysis of ALM on Linear Program [16] assumes exact solutions of subproblem (6)  which is
not possible in practice. Our next theorem extends the linear convergence result to cases when
subproblems are solved inexactly  and in particular  shows the total number of coordinate descent
updates required to ﬁnd an -accurate solution.
Theorem 3 (Iteration Complexity). Denote {ˆyt}∞
act dual proximal updates  {yt}∞
of y to the set of optimal dual solutions. To guarantee (cid:107)ˆyt − ˆyt
sufﬁces to run Algorithm 1 for

t=1 as the sequence of iterates obtained from inex-
t=1 as that generated by exact updates  and yS∗ as the projection
S∗(cid:107) ≤ 2 with probability 1 − p  it

(20)

(21)

(22)

(cid:19)

(cid:18) LR
(cid:18)



(cid:18)

T = (1 +

) log

1
α

(cid:16) ω

(cid:17)



+

3
2

k ≥ 2γn

log

log

(1 +

1
α

) log

LR



(cid:19)(cid:19)

outer iterations with ηt = (1 + α)L  and solve each sub-problem (6) by running Algorithm 2 for

(cid:113) 2(1+α)L(F 0−F ∗)

inner iterations  where L is a constant depending on the polyhedral set of optimal solutions  ω =
  R = (cid:107)proxηtg(y0) − y0(cid:107)  and F 0  F ∗ are upper and lower bounds on the

p

initial and optimal function values of subproblem respectively.

6

3.4 Fast Asymptotic Convergence via Projected Newton-CG

The RCD algorithm converges to a solution of moderate precision efﬁciently  but in some problems
a higher precision might be required. In such case  we transfer the subproblem solver from RCD
to a Projected Newton-CG (PN-CG) method after iterates are close enough to the optimum. Note
the Projected Newton method does not have global iteration complexity but has fast convergence for
iterates very close to the optimal.
Denote F (x) as the objective in (9). Each iterate of PN-CG begins by ﬁnding the set of active
variables deﬁned as

At k = {j|xt k

j > 0 ∨ ∇jF (xt k) < 0}.

(23)

Then the algorithm ﬁxes xt k

j = 0 ∀j /∈ At k and solves a Newton linear system w.r.t. j ∈ At k

[∇2At k F (xt k)]d = −[∇At k F (xt k)]

(24)
to obtain direction d∗ for the current active variables. Let dAt k denotes a size-n vector taking value
in d∗ for j ∈ At k and taking value 0 for j /∈ At k. The algorithm then conducts a projected line
search to ﬁnd smallest r ∈ {0  1  2  ...} satisfying

F ([xt k + βrdAt k ]+) − F (xt k) ≤ σβr(∇jF (xt k)dAt k ) 

(25)
and update x by xt k+1 ← (xt k + βrd∗
j )+. Compared to interior point method  one key to the
tractability of this approach lies on the conditioning of linear system (24)  which does not worsen
as outer iteration t increases  so an iterative Conjugate Gradient (CG) method can be used to obtain
accurate solution without factorizing the Hessian matrix. The only operation required within CG is
the Hessian-vector product

[∇2At k F (xt k)]s = ηt [AT

(26)
where the operator [.]At k takes the sub-matrix with row and column indices belonging to At k. For
a primal or dual-sparse LP  the product (26) can be evaluated very efﬁciently  since it only involves
non-zero elements in columns of AI  AE belonging to the active set  and rows of AI corresponding
to the binding constraints for which Dii(wt k) > 0. The overall cost of the product (26) is only

I D(wt k)AI + AT

EAE]At k s 

O(cid:0)nnz([AI ]Dt k At k ) + nnz([AE]: At k )(cid:1)  

where Dt k = {i|wt k
i > 0} is the set of current binding constraints. Considering that the com-
putational bottleneck of PN-CG is on the CG iterations for solving linear system (24)  the efﬁcient
computation of product (26) reduces the overall complexity of PN-CG signiﬁcantly. The whole
procedure is summarized in Algorithm 3.

4 Practical Issues

4.1 Precision of Subproblem Minimization

In practice  it is unnecessary to solve subproblem (6) to high precision  especially for iterations
of ALM in the beginning. In our implementation  we employ a two-phase strategy  where in the
ﬁrst phase we limit the cost spent on each sub-problem (6) to be a constant multiple of nnz(A) 
while in the second phase we dynamically increment the AL parameter ηt and inner precision t to
ensure sufﬁcient decrease in the primal and dual infeasibility respectively. The two-phase strategy
is particularly useful for primal or dual-sparse problem  where sub-problem in the latter phase has
smaller active set that results in less computation cost even when solved to high precision.

4.2 Active-Set Strategy
Our implementation of Algorithm 2 maintains an active set of variables A  which initially contains
all variables  but during the RCD iterates  any variable xj binding at 0 with gradient ∇jF greater
than a threshold δ will be excluded from A till the end of each subproblem solving. A will be
re-initialized after each dual proximal update (7). Note in the initial phase  the cost spent on each
subproblem is a constant multiple of nnz(A)  so if |A| is small one would spend more iterations on
the active variables to achieve faster convergence.

7

4.3 Dealing with Decomposable Constraint Matrix

When we have a m by n constraint matrix A = U V T that can be decomposed into product of an
m × r matrix U and a r × n matrix V T   if r (cid:28) min{m  n} or nnz(U ) + nnz(V ) (cid:28) nnz(A)  we
can re-formulate the constraint Ax ≤ b as U z ≤ b   V T x = z with auxiliary variables z ∈ Rr.
This new representation reduce the cost of Hessian-vector product in Algorithm 3 and the cost of
each pass of CD in Algorithm 2 from O(nnz(A)) to O(nnz(U ) + nnz(V )).

5 Numerical Experiments

Table 1: Timing Results (in sec. unless speciﬁed o.w.) on Multiclass L1-regularized SVM

Data
rcv1
news
sector
mnist

cod-rna.rf
vehicle
real-sim

nb

4 833 738
2 498 415
11 597 992

75 620
69 537
79 429
114 227

mI

778 200
302 765
666 848
540 000
59 535
157 646
72 309

P-Simp. D-Simp.
> 48hr
> 48hr
37 912
> 48hr
9 282
> 48hr
2 556
6 454
5 738
86 130
3 296
143.33
49 405
> 48hr

Barrier D-ALCD P-ALCD
> 48hr
> 48hr
> 48hr
73 036
> 48hr
8 858
89 476

3 155
395
2 029
7 207
2 676
598
297

3 452
148
1 419
146
3 130
31
179

Table 2: Timing Results (in sec. unless speciﬁed o.w.) on Sparse Inverse Covariance Estimation

Data

textmine
E2006
dorothea

nb

60 876
55 834
47 232

mI

60 876
55 834
47 232

mE
43 038
32 174
1 600

nf

43 038
32 174
1 600

P-Simp D-Simp
> 48hr > 48hr > 48hr
> 48hr > 48hr
94623
3 980

Barrier D-ALCD P-ALCD
18 507
4 207
38

103

43 096
> 48hr

47

82

Table 3: Timing Results (in sec. unless speciﬁed o.w.) for Nonnegative Matrix Factorization.

Data

micromass

ocr

nb

2 896 770
6 639 433

mI

4 107 438
13 262 864

P-Simp. D-Simp.
> 96hr
> 96hr
> 96hr
> 96hr

Barrier
280 230
284 530

D-ALCD P-ALCD
12 119
12 966
40 242
> 96hr

In this section  we compare the AL-CD algorithm with state-of-the-art implementation of interior
point and primal  dual Simplex methods in commercial LP solver CPLEX  which is of top efﬁciency
among many LP solvers as investigated in [30]. For all experiments  the stopping criteria is set to
require both primal and dual infeasibility (in the (cid:96)∞-norm) smaller than 10−3 and set the initial sub-
problem tolerance t = 10−2 and ηt = 1. The LP instances are generated from L1-SVM (3)  Sparse
Inverse Covariance Estimation (4) and Nonnegative Matrix Factorization [3]. For the Sparse Inverse
Covariance Estimation problem  we use technique introduced in section 4.3 to decompose the low-
rank matrix S  and since (4) results in d independent problems for each column of the estimated
matrix  we report result on only one of them. The data source and statistics are included in the
appendix.
Among all experiments  we observe that the proposed primal  dual AL-CD methods become partic-
ularly advantageous when the matrix A is sparse. For example  for text data set rcv1  real-sim and
news in Table 1  the matrix A is particularly sparse and AL-CD can be orders of magnitude faster
than other approaches by avoiding solving n × n linear system exactly. In addition  the dual-ALCD
(also dual simplex) is more efﬁcient in L1-SVM problem due to the problem’s strong dual sparsity 
while the primal-ALCD is more efﬁcient on the primal-sparse Inverse Covariance estimation prob-
lem. For the Nonnegative Matrix Factorization problem  both the dual and primal LP solutions are
not particularly sparse due to the choice of matrix approximation tolerance (1% of #samples)  but
the AL-CD approach is still comparably more efﬁcient.
Acknowledgement We acknowledge the support of ARO via W911NF-12-1-0390  and the support
of NSF via grants CCF-1320746  CCF-1117055  IIS-1149803  IIS-1320894  IIS-1447574  DMS-
1264033  and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support
Research at the Interface of the Biological and Mathematical Sciences.

8

References
[1] J. Zhu  S. Rosset  T. Hastie  and R. Tibshirani. 1-norm support vector machines. NIPS  2004.
[2] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press  2009.
[3] N. Gillis and R. Luce. Robust near-separable nonnegative matrix factorization using linear optimization.

JMLR  2014.

[4] A. Nellore and R. Ward. Recovery guarantees for exemplar-based clustering. arXiv.  2013.
[5] I. Yen  X. Lin  K. Zhong  P. Ravikumar  and I. Dhillon. A convex exemplar-based approach to MAD-

Bayes dirichlet process mixture models. In ICML  2015.

[6] M. Yuan. High dimensional inverse covariance matrix estimation via linear programming. JMLR  2010.
In Systems and
[7] D. Bello and G. Riano. Linear programming solvers for Markov decision processes.

Information Engineering Design Symposium  pages 90–95  2006.

[8] T. Joachims. Training linear svms in linear time. In KDD. ACM  2006.
[9] C. Hsieh  K. Chang  C. Lin  S.S. Keerthi  and S. Sundararajan. A dual coordinate descent method for

large-scale linear SVM. In ICML  volume 307. ACM  2008.

[10] G. Yuan  C. Hsieh K. Chang  and C. Lin. A comparison of optimization methods and software for large-

scale l1-regularized linear classiﬁcation. JMLR  11  2010.

[11] J. Nocedal and S.J. Wright. Numerical Optimization. Springer  2006.
[12] J. Gondzio. Interior point methods 25 years later. EJOR  2012.
[13] J. Gondzio. Matrix-free interior point method. Computational Optimization and Applications  2012.
[14] V.Eleuterio and D.Lucia. Finding approximate solutions for large scale linear programs. Thesis  2009.
[15] Evtushenko  Yu. G  Golikov  AI  and N. Mollaverdy. Augmented lagrangian method for large-scale linear

programming problems. Optimization Methods and Software  20(4-5):515–524  2005.

[16] F. Delbos and J.C. Gilbert. Global linear convergence of an augmented lagrangian algorithm for solving

convex quadratic optimization problems. 2003.

[17] O. G¨uler. Augmented lagrangian algorithms for linear programming. Journal of optimization theory and

applications  75(3):445–470  1992.

[18] J. Mor´e J and G. Toraldo. On the solution of large quadratic programming problems with bound con-

straints. SIAM Journal on Optimization  1(1):93–113  1991.

[19] M. Hong and Z. Luo. On linear convergence of alternating direction method of multipliers. arXiv  2012.
[20] H. Wang  A. Banerjee  and Z. Luo. Parallel direction method of multipliers. In NIPS  2014.
[21] C.Chen  B.He  Y.Ye  and X.Yuan. The direct extension of admm for multi-block convex minimization

problems is not necessarily convergent. Mathematical Programming  2014.

[22] I.Dhillon  P.Ravikumar  and A.Tewari. Nearest neighbor based greedy coordinate descent. In NIPS  2011.
Indexed block coordinate descent for large-scale linear
[23] I. Yen  C. Chang  T. Lin  S. Lin  and S. Lin.

classiﬁcation with limited memory. In KDD. ACM  2013.

[24] I. Yen  S. Lin  and S. Lin. A dual-augmented block minimization framework for learning with limited

memory. In NIPS  2015.

[25] K. Zhong  I. Yen  I. Dhillon  and P. Ravikumar. Proximal quasi-Newton for computationally intensive

l1-regularized m-estimators. In NIPS  2014.

[26] P. Richt´arik and M. Tak´aˇc.

Iteration complexity of randomized block-coordinate descent methods for

minimizing a composite function. Mathematical Programming  144(1-2):1–38  2014.

[27] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM

Journal on Optimization  22(2):341–362  2012.

[28] P. Wang and C. Lin.

Iteration complexity of feasible descent methods for convex optimization. The

Journal of Machine Learning Research  15(1):1523–1548  2014.

[29] I. Yen  C. Hsieh  P. Ravikumar  and I.S. Dhillon. Constant nullspace strong convexity and fast convergence

of proximal methods under high-dimensional settings. In NIPS  2014.

[30] B. Meindl and M. Templ. Analysis of commercial and free and open source solvers for linear optimization

problems. Eurostat and Statistics Netherlands  2012.

[31] A.J. Hoffman. On approximate solutions of systems of linear inequalities. Journal of Research of the

National Bureau of Standards  49(4):263–265  1952.

[32] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS  2007.
[33] I. Yen  T. Lin  S. Lin  P. Ravikumar  and I. Dhillon. Sparse random feature algorithm as coordinate descent

in Hilbert space. In NIPS  2014.

9

,Jasper De Bock
Cassio de Campos
Alessandro Antonucci
Ian En-Hsu Yen
Kai Zhong
Cho-Jui Hsieh
Pradeep Ravikumar
Inderjit Dhillon
Hossein Esfandiari
Nitish Korula
Vahab Mirrokni