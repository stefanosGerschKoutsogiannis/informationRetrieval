2016,Pruning Random Forests for Prediction on a Budget,We propose to prune a random forest (RF) for resource-constrained prediction. We first construct a RF and then prune it to optimize expected feature cost & accuracy. We pose pruning RFs as a novel 0-1 integer program with linear constraints that encourages feature re-use. We establish total unimodularity of the constraint set to prove that the corresponding LP relaxation solves the original integer program. We then exploit connections to combinatorial optimization and develop an efficient primal-dual algorithm  scalable to large datasets. In contrast to our bottom-up approach  which benefits from good RF initialization  conventional methods are top-down acquiring features based on their utility value and is generally intractable  requiring heuristics.  Empirically  our pruning algorithm outperforms existing state-of-the-art resource-constrained algorithms.,Pruning Random Forests for Prediction on a Budget

Feng Nan

Systems Engineering

Boston University
fnan@bu.edu

Joseph Wang

Electrical Engineering

Boston University
joewang@bu.edu

Venkatesh Saligrama
Electrical Engineering

Boston University

srv@bu.edu

Abstract

We propose to prune a random forest (RF) for resource-constrained prediction. We
ﬁrst construct a RF and then prune it to optimize expected feature cost & accuracy.
We pose pruning RFs as a novel 0-1 integer program with linear constraints that
encourages feature re-use. We establish total unimodularity of the constraint set
to prove that the corresponding LP relaxation solves the original integer program.
We then exploit connections to combinatorial optimization and develop an efﬁcient
primal-dual algorithm  scalable to large datasets. In contrast to our bottom-up
approach  which beneﬁts from good RF initialization  conventional methods are
top-down acquiring features based on their utility value and is generally intractable 
requiring heuristics. Empirically  our pruning algorithm outperforms existing
state-of-the-art resource-constrained algorithms.

1

Introduction

Many modern classiﬁcation systems  including internet applications (such as web-search engines 
recommendation systems  and spam ﬁltering) and security & surveillance applications (such as wide-
area surveillance and classiﬁcation on large video corpora)  face the challenge of prediction-time
budget constraints [21]. Prediction-time budgets can arise due to monetary costs associated with
acquiring information or computation time (or delay) involved in extracting features and running the
algorithm. We seek to learn a classiﬁer by training on fully annotated training datasets that maintains
high-accuracy while meeting average resource constraints during prediction-time. We consider a
system that adaptively acquires features as needed depending on the instance(example) for high
classiﬁcation accuracy with reduced feature acquisition cost.
We propose a two-stage algorithm. In the ﬁrst stage  we train a random forest (RF) of trees using
an impurity function such as entropy or more specialized cost-adaptive impurity [16]. Our second
stage takes a RF as input and attempts to jointly prune each tree in the forest to meet global resource
constraints. During prediction-time  an example is routed through all the trees in the ensemble to the
corresponding leaf nodes and the ﬁnal prediction is based on a majority vote. The total feature cost
for a test example is the sum of acquisition costs of unique features1 acquired for the example in the
entire ensemble of trees in the forest. 2
We derive an efﬁcient scheme to learn a globally optimal pruning of a RF minimizing the
empirical error and incurred average costs. We formulate the pruning problem as a 0-1 inte-
ger linear program that incorporates feature-reuse constraints. By establishing total unimod-
ularity of the constraint set  we show that solving the linear program relaxation of the in-
teger program yields the optimal solution to the integer program resulting in a polynomial

1When an example arrives at an internal node  the feature associated with the node is used to direct the
example. If the feature has never been acquired for the example an acquisition cost is incurred. Otherwise  no
acquisition cost is incurred as we assume that feature values are stored once computed.

2For time-sensitive cases such as web-search we parallelize the implementation by creating parallel jobs

across all features and trees. We can then terminate jobs based on what features are returned.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

No Usage

1–7
> 7 Cost Error
7.3%
91.7% 1% 42.0 6.6%
68.3% 31.5% 0.2% 24.3 6.7%

Unpruned RF
BudgetPrune

Table 1: Typical feature usage in a 40 tree RF before and after
pruning (our algorithm) on the MiniBooNE dataset. Columns 2-4
list percentage of test examples that do not use the feature  use it
1 to 7 times  and use it greater than 7 times  respectively. Before
pruning  91% examples use the feature only a few (1 to 7) times 
paying a signiﬁcant cost for its acquisition; after pruning  68% of
the total examples no longer use this feature  reducing cost with
minimal error increase. Column 5 is the average feature cost (the
average number of unique features used by test examples). Column
6 is the test error of RFs. Overall  pruning dramatically reduces
average feature cost while maintaining the same error level.

time algorithm for optimal pruning. We develop a primal-dual algorithm by leveraging re-
sults from network-ﬂow theory for scaling the linear program to large datasets. Empirically 
this pruning outperforms state-of-the-art resource efﬁcient algorithms on benchmarked datasets.
Our approach is motivated by the fol-
lowing considerations:
(i) RFs are scalable to large datasets
and produce ﬂexible decision bound-
aries yielding high prediction-time ac-
curacy. The sequential feature usage
of decision trees lends itself to adap-
(ii) RF fea-
tive feature acquisition.
ture usage is superﬂuous  utilizing fea-
tures with introduced randomness to
increase diversity and generalization.
Pruning can yield signiﬁcant cost re-
duction with negligible performance
loss by selectively pruning features
sparsely used across trees  leading to
cost reduction with minimal accuracy
degradation (due to majority vote). See Table 1. (iii) Optimal pruning encourages examples to use
features either a large number of times  allowing for complex decision boundaries in the space of
those features  or not to use them at all  avoiding incurring the cost of acquisition. It enforces the
fact that once a feature is acquired for an example  repeated use incurs no additional acquisition cost.
Intuitively  features should be repeatedly used to increase discriminative ability without incurring
further cost. (iv) Resource constrained prediction has been conventionally viewed as a top-down
(tree-growing) approach  wherein new features are acquired based on their utility value. This is often
an intractable problem with combinatorial (feature subsets) and continuous components (classiﬁers)
requiring several relaxations and heuristics. In contrast  ours is a bottom-up approach that starts with
good initialization (RF) and prunes to realize optimal cost-accuracy tradeoff. Indeed  while we do not
pursue it  our approach can also be used in conjunction with existing approaches.
Related Work: Learning decision rules to minimize error subject to a budget constraint during
prediction-time is an area of recent interest  with many approaches proposed to solve the prediction-
time budget constrained problem [9  22  19  20  12]. These approaches focus on learning complex
adaptive decision functions and can be viewed as orthogonal to our work. Conceptually  these are
top-down “growing” methods as we described earlier (see (iv)). Our approach is bottom-up that seeks
to prune complex classiﬁers to tradeoff cost vs. accuracy.
Our work is based on RF classiﬁers [3]. Traditionally  feature cost is not incorporated when construct-
ing RFs  however recent work has involved approximation of budget constraints to learn budgeted
RFs [16]. The tree-growing algorithm in [16] does not take feature re-use into account. Rather
than attempting to approximate the budget constraint during tree construction  our work focuses on
pruning ensembles of trees subject to a budget constraint. Methods such as traditional ensemble
learning and budgeted random forests can be viewed as complementary.
Decision tree pruning has been studied extensively to improve generalization performance  we are not
aware of any existing pruning method that takes into account the feature costs. A popular method for
pruning to reduce generalization error is Cost-Complexity Pruning (CCP)  introduced by Breiman et
al. [4]. CCP trades-off classiﬁcation ability for tree size  however it does not account for feature costs.
As pointed out by Li et al. [15]  CCP has undesirable “jumps" in the sequence of pruned tree sizes.
To alleviate this  they proposed a Dynamic-Program-based Pruning (DPP) method for binary trees.
The DPP algorithm is able to obtain optimally pruned trees of all sizes; however  it faces the curse
of dimensionality when pruning an ensemble of decision trees and taking feature cost into account.
[23  18] proposed to solve the pruning problem as a 0-1 integer program; again  their formulations
do not account for feature costs that we focus on in this paper. The coupling nature of feature usage
makes our problem much harder. In general pruning RFs is not a focus of attention as it is assumed
that overﬁtting can be avoided by constructing an ensemble of trees. While this is true  it often leads
to extremely large prediction-time costs. Kulkarni and Sinha [11] provide a survey of methods to
prune RFs in order to reduce ensemble size. However  these methods do not explicitly account for
feature costs.

2

2 Learning with Resource Constraints

In this paper  we consider solving the Lagrangian relaxed problem of learning under prediction-time
resource constraints  also known as the error-cost tradeoff problem:

f∈F E(x y)∼P [err (y  f (x))] + λEx∼Px [C (f  x)]  
min

(1)
where example/label pairs (x  y) are drawn from a distribution P; err(y  ˆy) is the error function;
C(f  x) is the cost of evaluating the classiﬁer f on example x; λ is a tradeoff parameter. A larger λ
places a larger penalty on cost  pushing the classiﬁer to have smaller cost. By adjusting λ we can
obtain a classiﬁer satisfying the budget constraint. The family of classiﬁers F in our setting is the
space of RFs  and each RF f is composed of T decision trees T1  . . .  TT .
Our approach: Rather than attempting to construct the optimal ensemble by solving Eqn. (1)
directly  we instead propose a two-step algorithm that ﬁrst constructs an ensemble with low prediction
error  then prunes it by solving Eqn. (1) to produce a pruned ensemble given the input ensemble. By
adopting this two-step strategy  we obtain an ensemble with low expected cost while simultaneously
preserving the low prediction error.
There are many existing methods to construct RFs  however the focus of this paper is on the second
step  where we propose a novel approach to prune RFs to solve the tradeoff problem Eqn.(1). Our
pruning algorithm is capable of taking any RF as input  offering the ﬂexibility to incorporate any
state-of-the-art RF algorithm.

3 Pruning with Costs

In this section  we treat the error-cost tradeoff problem Eqn. (1) as an RF pruning problem. Our key
contribution is to formulate pruning as a 0-1 integer program with totally unimodular constraints.
We ﬁrst deﬁne notations used throughout the paper. A training sample S = {(x(i)  y(i)) :
i = 1  . . .   N} is generated i.i.d. from an unknown distribution  where x(i) ∈ (cid:60)K is the feature
vector with a cost assigned to each of the K features and y(i) is the label for the ith example. In
the case of multi-class classiﬁcation y ∈ {1  . . .   M}  where M is the number of classes. Given a
decision tree T   we index the nodes as h ∈ {1  . . .  |T |}  where node 1 represents the root node. Let
˜T denote the set of leaf nodes of tree T . Finally  the corresponding deﬁnitions for T can be extended
to an ensemble of T decision trees {Tt : t = 1  . . .   T} by adding a subscript t.
Pruning Parametrization: In order to model ensemble pruning as an optimization problem  we
parametrize the space of all prunings of an ensemble. The process of pruning a decision tree T at
an internal node h involves collapsing the subtree of T rooted at h  making h a leaf node. We say
a pruned tree T (p) is a valid pruned tree of T if (1) T (p) is a subtree of T containing root node 1
and (2) for any h (cid:54)= 1 contained in T (p)  the sibling nodes (the set of nodes that share the same
immediate parent node as h in T ) must also be contained in T (p). Specifying a pruning is equivalent
to specifying the nodes that are leaves in the pruned tree. We therefore introduce the following binary
variable for each node h ∈ T

(cid:26) 1

0

zh =

if node h is a leaf in the pruned tree 
otherwise.

We call the set {zh ∀h ∈ T } the node variables as they are associated with each node in the tree.
Consider any root-to-leaf path in a tree T   there should be exactly one node in the path that is a leaf
node in the pruned tree. Let p(h) denote the set of predecessor nodes  the set of nodes (including h)
that lie on the path from the root node to h. The set of valid pruned trees can be represented as the
u∈p(h) zu = 1 ∀h ∈ ˜T . Given a

set of node variables satisfying the following set of constraints:(cid:80)

valid pruning for a tree  we now seek to parameterize the error of the pruning.
Pruning error: As in most supervised empirical risk minimization problems  we aim to minimize
the error on training data as a surrogate to minimizing the expected error. In a decision tree T   each
node h is associated with a predicted label corresponding to the majority label among the training
examples that fall into the node h. Let Sh denote the subset of examples in S routed to or through
node h on T and let Predh denote the predicted label at h. The number of misclassiﬁed examples

3

at h is therefore eh =(cid:80)

i∈Sh

1

(cid:80)
[y(i)(cid:54)=Predh]. We can thus estimate the error of tree T in terms of the
h∈ ˜T eh  where N = |S| is the total number

h∈ ˜Tt

(cid:80)

(cid:80)

t=1
ehzh.

(cid:80)T

(cid:80)T

number of misclassiﬁed examples in the leaf nodes: 1
N
of examples.
Our goal is to minimize the expected test error of the trees in the random forest  which we
empirically approximate based on the aggregated probability distribution in Step (6) of Algo-
rithm 1 with 1
eh. We can express this error in terms of the node variables:
T N
1
h∈Tt

Pruning cost: Assume the acquisition costs for the K features  {ck : k = 1  . . .   K}  are given. The
feature acquisition cost incurred by an example is the sum of the acquisition costs of unique features
acquired in the process of running the example through the forest. This cost structure arises due to
the assumption that an acquired feature is cached and subsequent usage by the same example incurs
no additional cost. Formally  the feature cost of classifying an example i on the ensemble T[T ] is
k=1 ckwk i  where the binary variables wk i serve as the indicators:
if feature k is used by x(i) in any Tt  t = 1  . . .   T
otherwise.

given by Cfeature(T[T ]  x(i)) =(cid:80)K

(cid:26) 1

wk i =

t=1

T N

0

i=1

t=1

h∈Tt

(cid:80)

(cid:80)T

k=1 ckwk i.

|Sh|dhzh  where dh is the depth of node h.

constraint wk i +(cid:80)

The expected feature cost of a test example can be approximated as 1
N
In some scenarios  it is useful to account for computation cost along with feature acquisition cost
during prediction-time.
In an ensemble  this corresponds to the expected number of Boolean
operations required running a test through the trees  which is equal to the expected depth of the trees.
This can be modeled as 1
N
Putting it together: Having modeled the pruning constraints  prediction performance and costs 
we formulate the problem of pruning using the relationship between the node variables zh’s and
feature usage variables wk i’s. Given a tree T   feature k  and example x(i)  let uk i be the ﬁrst node
associated with feature k on the root-to-leaf path the example follows in T . Feature k is used by
x(i) if and only if none of the nodes between the root and uk i is a leaf. We represent this by the
h∈p(uk i) zh = 1 for every feature k used by example x(i) in T . Recall wk i
indicates whether or not feature k is used by example i and p(uk i) denotes the set of predecessor
nodes of uk i. Intuitively  this constraint says that either the tree is pruned along the path followed
by example i before feature k is acquired  in which case zh = 1 for some node h ∈ p(uk i) and
wk i = 0; or wk i = 1  indicating that feature k is acquired for example i. We extend the notations to
h indicates whether node h in Tt is a leaf after pruning; w(t)
ensemble pruning with tree index t: z(t)
indicates whether feature k is used by the ith example in Tt; wk i indicates whether feature k is used
by the ith example in any of the T trees T1  . . .  TT ; ut k i is the ﬁrst node associated with feature k
on the root-to-leaf path the example follows in Tt; Kt i denotes the set of features the ith example
uses on tree Tt. We arrive at the following integer program.

k i

(cid:80)N

(cid:80)K

(cid:123)



(cid:122)
K(cid:88)

k=1

feature acquisition cost

computational cost

(cid:125)(cid:124)
N(cid:88)

i=1

(cid:123)

(cid:122)

(cid:125)(cid:124)
(cid:88)

T(cid:88)

t=1

h∈Tt

e(t)
h z(t)

h +λ

ck(

1
N

wk i) +

1
N

|Sh|dhzh

(IP)

(cid:123)



min
z(t)
h  w(t)
k i 
wk i∈{0 1}

(cid:122)
s.t. (cid:80)

1
N T

error

(cid:125)(cid:124)
(cid:88)
T(cid:88)
k i +(cid:80)

u∈p(h) z(t)

t=1

h∈Tt

w(t)
k i ≤ wk i 
w(t)

u = 1 
h∈p(ut k i) z(t)

∀h ∈ ˜Tt ∀t ∈ [T ] 

(feasible prunings)

h = 1  ∀k ∈ Kt i ∀i ∈ S ∀t ∈ [T ]  (feature usage/ tree)
∀k ∈ [K] ∀i ∈ S ∀t ∈ [T ]. (global feature usage)

Totally Unimodular constraints: Even though integer programs are NP-hard to solve in general 
we show that (IP) can be solved exactly by solving its LP relaxation. We prove this in two steps:
ﬁrst  we examine the special structure of the equality constraints; then we examine the inequality
constraint that couples the trees. Recall that a network matrix is one with each column having exactly
one element equal to 1  one element equal to -1 and the remaining elements being 0. A network
matrix deﬁnes a directed graph with the nodes in the rows and arcs in the columns. We have the
following lemma.

4



z1
1
1
1
1
1

r1
r2
r3
r4
r5

z2
1
0
0
0
0

z3
0
1
1
1
0

z4
0
1
0
0
0

1 1 w(1)
z5 w(1)
2 1
0
0
0
0
0
0
0
0
1
1
0
0
0
1
0





−r1
r1−r2
r2−r3
r3−r4
r4−r5

r5

11

2

32

4

5

z1

z2

2 1

z3
0

−1 −1
0
0
0
0
1

1 1 w(1)
w(1)
z5
z4
0
0
0
0
1 −1 −1
0
0
0
1 −1
0
0
0
−1
0
1
0
0
0 −1
1
0
0
1
0
0
0
0

0
0
1
0



Figure 1: A decision tree example with node numbers and associated feature in subscripts together
with the constraint matrix and its equivalent network matrix form.

Lemma 3.1 The equality constraints in (IP) can be turned into an equivalent network matrix form
for each tree.

Proof We observe the ﬁrst constraint(cid:80)

u = 1 requires the sum of the node variables along
a path to be 1. The second constraints w(t)
h = 1 has a similar sum except the
variable w(t)
k i as yet another node variable for a ﬁctitious child node of ut k i and the
two equations are essentially equivalent. The rest of proof follows directly from the construction in
Proposition 3 of [18].

k i. Imagine w(t)

h∈p(ut k i) z(t)

u∈p(h) z(t)

k i +(cid:80)

Figure 1 illustrates such a construction. The nodes are numbered 1 to 5. The subscripts at node 1
and 3 are the feature index used in the nodes. Since the equality constraints in (IP) can be separated
based on the trees  we consider only one tree and one example being routed to node 4 on the tree for
simplicity. The equality constraints can be organized in the matrix form as shown in the middle of
Figure 1. Through row operations  the constraint matrix can be transformed to an equivalent network
matrix. Such transformation always works as long as the leaf nodes are arranged in a pre-order
manner. Next  we deal with the inequality constraints and obtain our main result.

Theorem 3.2 The LP relaxation of (IP)  where the 0-1 integer constraints are relaxed to interval
constraints [0  1] for all integer variables  has integral optimal solutions.

Due to space limit the proof can be found in the Suppl. Material. The main idea is to show the
constraints are still totally unimodular even after adding the coupling constraints and the LP relaxed
polyhedron has only integral extreme points [17]. As a result  solving the LP relaxation results in the
optimal solution to the integer program (IP)  allowing for polynomial time optimization. 3

Algorithm 1 BUDGETPRUNE
During Training: input - ensemble(T1  . . .  TT )  training/validation data with labels  λ
1: initialize dual variables β(t)
2: update z(t)
k i ← [β(t)
3: β(t)
4: go to Step 2 until duality gap is small enough.

k i − wk i)]+ for step size γ  where [·]+ = max{0 ·}.

h   w(t)
k i + γ(w(t)

k i for each tree t (shortest-path algo). wk i = 0 if µk i > 0  wk i = 1 if µk i < 0.

k i ← 0.

During Prediction: input - test example x

(cid:80)T

5: Run x on each tree to leaf  obtain the probability distribution over label classes pt at leaf.
6: Aggregate p = 1
T

t=1 pt. Predict the class with the highest probability in p.

4 A Primal-Dual Algorithm

Even though we can solve (IP) via its LP relaxation  the resulting LP can be too large in practical
applications for any general-purpose LP solver. In particular  the number of variables and constraints
is roughly O(T × |Tmax| + N × T × Kmax)  where T is the number of trees; |Tmax| is the maximum
3The nice result of totally unimodular constraints is due to our speciﬁc formulation. See Suppl. Material for

an alternative formulation that does not have such a property.

5

number of nodes in a tree; N is the number of examples; Kmax is the maximum number of features
an example uses in a tree. The runtime of the LP thus scales O(T 3) with the number of trees in the
ensemble  limiting the application to only small ensembles. In this section we propose a primal-dual
approach that effectively decomposes the optimization into many sub-problems. Each sub-problem
corresponds to a tree in the ensemble and can be solved efﬁciently as a shortest path problem. The
p (|Tmax| + N × Kmax) log(|Tmax| + N × Kmax))  where p is the number of
runtime per iteration is O( T
processors. We can thus massively parallelize the optimization and scale to much larger ensembles as
the runtime depends only linearly on T
k i for the inequality
constraints w(t)

p . To this end  we assign dual variables β(t)

k i ≤ wk i and derive the dual problem.

(cid:32) K(cid:88)

k=1

N(cid:88)

i=1

ck(

1
N

(cid:33)

T(cid:88)

N(cid:88)

(cid:88)

t=1

i=1

k∈Kt i

ˆe(t)
h z(t)

h + λ

wk i)

+

β(t)
k i(w(t)

k i − wk i)

max
k i≥0
β(t)

min
h ∈[0 1]
z(t)
k i∈[0 1]
w(t)
wk i∈[0 1]

(cid:88)

h∈Tt

1
N T

T(cid:88)
s.t. (cid:88)

t=1

u∈p(h)
w(t)
k i +

z(t)
u = 1 

(cid:88)

h∈p(ut k i)

∀h ∈ ˜Tt ∀t ∈ [T ] 

h = 1  ∀k ∈ Kt i ∀i ∈ S ∀t ∈ [T ] 
z(t)

where for simplicity we have combined coefﬁcients of z(t)
h . The
primal-dual algorithm is summarized in Algorithm 1. It alternates between updating the primal
and the dual variables. The key is to observe that given dual variables  the primal problem (inner
minimization) can be decomposed for each tree in the ensemble and solved in parallel as shortest path
problems due to Lemma 3.1. (See also Suppl. Material). The primal variables wk i can be solved in
β(t)
k i  where Tk i is the set of trees in which

closed form: simply compute µk i = λck/N −(cid:80)

h in the objective of (IP) to ˆe(t)

example i encounters feature k. So wk i should be set to 0 if µk i > 0 and wk i = 1 if µk i < 0.
Note that our prediction rule aggregates the leaf distributions from all trees instead of just their
predicted labels. In the case where the leaves are pure (each leaf contains only one class of examples) 
this prediction rule coincides with the majority vote rule commonly used in random forests. Whenever
the leaves contain mixed classes  this rule takes into account the prediction conﬁdence of each tree
in contrast to majority voting. Empirically  this rule consistently gives lower prediction error than
majority voting with pruned trees.

t∈Tk i

5 Experiments

We test our pruning algorithm BUDGETPRUNE on four benchmark datasets used for prediction-time
budget algorithms. The ﬁrst two datasets have unknown feature acquisition costs so we assign costs
to be 1 for all features; the aim is to show that BUDGETPRUNE successfully selects a sparse subset of
features on average to classify each example with high accuracy. 4 The last two datasets have real
feature acquisition costs measured in terms of CPU time. BUDGETPRUNE achieves high prediction
accuracy spending much less CPU time in feature acquisition.
For each dataset we ﬁrst train a RF and apply BUDGETPRUNE on it using different λ’s to obtain
various points on the accuracy-cost tradeoff curve. We use in-bag data to estimate error probability at
each node and the validation data for the feature cost variables wk i’s. We implement BUDGETPRUNE
using CPLEX [1] network ﬂow solver for the primal update step. The running time is signiﬁcantly
reduced (from hours down to minutes) compared to directly solving the LP relaxation of (IP) using
standard solvers such as Gurobi [10]. Futhermore  the standard solvers simply break trying to solve
the larger experiments whereas BUDGETPRUNE handles them with ease. We run the experiments for
10 times and report the means and standard deviations. Details of datasets and parameter settings of
competing methods are included in the Suppl. Material.
Competing methods: We compare against four other approaches.
(i) BUDGETRF[16]:
the recursive node splitting process for each tree is stopped as soon as node impu-

4In contrast to traditional sparse feature selection  our algorithm allows adaptivity  meaning different examples

use different subsets of features.

6

attempts

(c) Yahoo! Rank

(d) Scene15

Cost-

(a) MiniBooNE

(b) Forest Covertype

The threshold is a measure of impu-
This can be considered as a naive pruning method
reduces feature acquisition cost while maintaining low impurity in the leaves.

rity (entropy or Pairs) falls below a threshold.
rity tolerated in the leaf nodes.
as it
(ii)
Complexity
Pruning
(CCP)
[4]: it iteratively
subtrees
prunes
such
that
the
resulting tree has
low error
and
small size. We
perform
CCP
on
individual
trees to different
levels to obtain
various points on
the accuracy-cost
tradeoff
curve.
CCP does not
take into account
feature costs. (iii)
GREEDYPRUNE:
is a greedy global
feature pruning
strategy
that
we propose; at
each
iteration
it
to
remove all nodes
corresponding
to one
feature
from the RF such
that the resulting
pruned RF has the lowest training error and average feature cost. The process terminates in at most K
iterations  where K is the number of features. The idea is to reduce feature costs by successively
removing features that result in large cost reduction yet small accuracy loss. We also compare against
the state-of-the-art methods in budgeted learning (iv) GREEDYMISER [22]: it is a modiﬁcation of
gradient boosted regression tree [8] to incorporate feature cost. Speciﬁcally  each weak learner (a
low-depth decision tree) is built to minimize squared loss with respect to current gradient at the
training examples plus feature acquisition cost. To build each weak learner the feature costs are set
to zero for those features already used in previous weak learners. Other prediction-time budget
algorithms such as ASTC [12]  CSTC [21] and cost-weighted l-1 classiﬁers are shown to perform
strictly worse than GREEDYMISER by a signiﬁcant amount [12  16] so we omit them in our plots.
Since only the feature acquisition costs are standardized  for fair comparison we do not include the
computation cost term in the objective of (IP) and focus instead on feature acquisition costs.
MiniBooNE Particle Identiﬁcation and Forest Covertype Datasets:[7] Feature costs are uniform
in both datasets. Our base RF consists of 40 trees using entropy split criteria and choosing from
the full set of features at each split. As shown in (a) and (b) of Figure 2  BUDGETPRUNE (in red)
achieves the best accuracy-cost tradeoff. The advantage of BUDGETPRUNE is particularly large in (b).
GREEDYMISER has lower accuracy in the high budget region compared to BUDGETPRUNE in (a)
and signiﬁcantly lower accuracy in (b). The gap between BUDGETPRUNE and other pruning methods
is small in (a) but much larger in (b). This indicates large gains from globally encouraging feature
sharing in the case of (b) compared to (a). In both datasets  BUDGETPRUNE successfully prunes
away large number of features while maintaining high accuracy. For example in (a)  using only 18
unique features on average instead of 40  we can get essentially the same accuracy as the original RF.
Yahoo! Learning to Rank:[6] This ranking dataset consists of 473134 web documents and 19944
queries. Each example in the dataset contains features of a query-document pair together with the

Figure 2: Comparison of BUDGETPRUNE against CCP  BUDGETRF with early stopping 
GREEDYPRUNE and GREEDYMISER on 4 real world datasets. BUDGETPRUNE (red)
outperforms competing state-of-art methods. GREEDYMISER dominates ASTC [12] 
CSTC [21] and DAG [20] signiﬁcantly on all datasets. We omit them in the plots to clearly
depict the differences between competing methods.

7

5101520253035400.880.890.90.910.920.93Test AccuracyAverage Feature Cost BudgetPruneCCP [Breiman et al. 1984]BudgetRF [Nan et al. 2015]GreedyPruneGreedyMiser [Xu et al. 2012]8101214161820220.780.80.820.840.860.880.90.92Test AccuracyAverage Feature Cost BudgetPruneCCP [Breiman et al. 1984]BudgetRF [Nan et al. 2015]GreedyPruneGreedyMiser [Xu et al. 2012]4060801001201401601802000.120.1250.130.1350.14Average Precision@5Average Feature Cost BudgetPruneCCP [Breiman et al. 1984]BudgetRF [Nan et al. 2015]GreedyPruneGreedyMiser [Xu et al. 2012]51015202530350.720.740.760.780.80.820.84Test AccuracyAverage Feature Cost BudgetPruneCCP [Breiman et al. 1984]BudgetRF [Nan et al. 2015]GreedyPruneGreedyMiser [Xu et al. 2012]relevance rank of the document to the query. There are 141397/146769/184968 examples in the
training/validation/test sets. There are 519 features for each example; each feature is associated with
an acquisition cost in the set {1  5  20  50  100  150  200}  which represents the units of CPU time
required to extract the feature and is provided by a Yahoo! employee. The labels are binarized so that
the document is either relevant or not relevant to the query. The task is to learn a model that takes a
new query and its associated set of documents to produce an accurate ranking using as little feature
cost as possible. As in [16]  we use the Average Precision@5 as the performance metric  which gives
a high reward for ranking the relevant documents on top. Our base RF consists of 140 trees using
cost weighted entropy split criteria as in [16] and choosing from a random subset of 400 features
at each split. As shown in (c) of Figure 2  BUDGETPRUNE achieves similar ranking accuracy as
GREEDYMISER using only 30% of its cost.
Scene15 [13]: This scene recognition dataset contains 4485 images from 15 scene classes (labels).
Following [22] we divide it into 1500/300/2685 examples for training/validation/test sets. We use
a diverse set of visual descriptors and object detectors from the Object Bank [14]. We treat each
individual detector as an independent descriptor so we have a total of 184 visual descriptors. The
acquisition costs of these visual descriptors range from 0.0374 to 9.2820. For each descriptor we train
15 one-vs-rest kernel SVMs and use the output (margins) as features. Once any feature corresponding
to a visual descriptor is used for a test example  an acquisition cost of the visual descriptor is incurred
and subsequent usage of features from the same group is free for the test example. Our base RF
consists of 500 trees using entropy split criteria and choosing from a random subset of 20 features at
each split. As shown in (d) of Figure 2  BUDGETPRUNE and GREEDYPRUNE signiﬁcantly outperform
other competing methods. BUDGETPRUNE has the same accuracy at the cost of 9 as at the full cost of
32. BUDGETPRUNE and GREEDYPRUNE perform similarly  indicating the greedy approach happen
to solve the global optimization in this particular initial RF.
5.1 Discussion & Concluding Comments

We have empirically evaluated several resource constrained learning algorithms including BUDGET-
PRUNE and its variations on benchmarked datasets here and in the Suppl. Material. We highlight
key features of our approach below. (i) STATE-OF-THE-ART METHODS. Recent work has estab-
lished that GREEDYMISER and BUDGETRF are among the state-of-the-art methods dominating
a number of other methods [12  21  20] on these benchmarked datasets. GREEDYMISER requires
building class-speciﬁc ensembles and tends to perform poorly and is increasingly difﬁcult to tune
in multi-class settings. RF  by its nature  can handle multi-class settings efﬁciently. On the other
hand  as we described earlier  [12  20  21] are fundamentally "tree-growing" approaches  namely
they are top-down methods acquiring features sequentially based on a surrogate utility value. This
is a fundamentally combinatorial problem that is known to be NP hard [5  21] and thus requires a
number of relaxations and heuristics with no guarantees on performance. In contrast our pruning
strategy is initialized to realize good performance (RF initialization) and we are able to globally
optimize cost-accuracy objective. (ii) VARIATIONS ON PRUNING. By explicitly modeling feature
costs  BUDGETPRUNE outperforms other pruning methods such as early stopping of BUDGETRF and
CCP that do not consider costs. GREEDYPRUNE performs well validating our intuition (see Table. 1)
that pruning sparsely occurring feature nodes utilized by large fraction of examples can improve
test-time cost-accuracy tradeoff. Nevertheless  the BUDGETPRUNE outperforms GREEDYPRUNE 
which is indicative of the fact that apart from obvious high-budget regimes  node-pruning must
account for how removal of one node may have an adverse impact on another downstream one. (iii)
SENSITIVITY TO IMPURITY  FEATURE COSTS  & OTHER INPUTS. We explore these issues in Suppl.
Material. We experiment BUDGETPRUNE with different impurity functions such as entropy and Pairs
[16] criteria. Pairs-impurity tends to build RFs with lower cost but also lower accuracy compared
to entropy and so has poorer performance. We also explored how non-uniform costs can impact
cost-accuracy tradeoff. An elegant approach has been suggested by [2]  who propose an adversarial
feature cost proportional to feature utility value. We ﬁnd that BUDGETPRUNE is robust with such
costs. Other RF parameters including number of trees and feature subset size at each split do impact
cost-accuracy tradeoff in obvious ways with more trees and moderate feature subset size improving
prediction accuracy while incurring higher cost.
Acknowledgment: We thank Dr Kilian Weinberger for helpful discussions and Dr David Castanon
for the insights on the primal dual algorithm. This material is based upon work supported in part by
NSF Grants CCF: 1320566  CNS: 1330008  CCF: 1527618  DHS 2013-ST-061-ED0001  ONR Grant
50202168 and US AF contract FA8650-14-C-1728.

8

References
[1] IBM ILOG CPLEX Optimizer.

integration/optimization/cplex-optimizer/  2010.

http://www-01.ibm.com/software/

[2] Djalel Benbouzid. Sequential prediction for budgeted learning : Application to trigger design.

Theses  Université Paris Sud - Paris XI  February 2014.

[3] L. Breiman. Random forests. Machine Learning  45(1):5–32  2001.
[4] L. Breiman  J. Friedman  C. J. Stone  and R A Olshen. Classiﬁcation and regression trees. CRC

press  1984.

[5] Venkatesan T. Chakaravarthy  Vinayaka Pandit  Sambuddha Roy  Pranjal Awasthi  and
Mukesh K. Mohania. Decision trees for entity identiﬁcation: Approximation algorithms
and hardness results. ACM Trans. Algorithms  7(2):15:1–15:22  March 2011.

[6] O. Chapelle  Y. Chang  and T. Liu  editors. Proceedings of the Yahoo! Learning to Rank

Challenge  held at ICML 2010  Haifa  Israel  June 25  2010  2011.
[7] A. Frank and A. Asuncion. UCI machine learning repository  2010.
[8] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of

Statistics  29:1189–1232  2000.

[9] T. Gao and D. Koller. Active classiﬁcation based on value of classiﬁer. In Advances in Neural

Information Processing Systems (NIPS)  2011.

[10] Gurobi Optimization Inc. Gurobi optimizer reference manual  2015.
[11] V.Y. Kulkarni and P.K. Sinha. Pruning of random forest classiﬁers: A survey and future

directions. In International Conference on Data Science Engineering (ICDSE)  2012.

[12] M. Kusner  W. Chen  Q. Zhou  E. Zhixiang  K. Weinberger  and Y. Chen. Feature-cost sensitive

learning with submodular trees of classiﬁers. In AAAI  2014.

[13] S. Lazebnik  C. Schmid  and J. Ponce. Beyond bags of features: Spatial pyramid matching for

recognizing natural scene categories. In IEEE CVPR  2006.

[14] L. J. Li  H. Su  E. P. Xing  and L. Fei-Fei. Object Bank: A High-Level Image Representation

for Scene Classiﬁcation and Semantic Feature Sparsiﬁcation. In NIPS. 2010.

[15] X. Li  J. Sweigart  J. Teng  J. Donohue  and L. Thombs. A dynamic programming based pruning

method for decision trees. INFORMS J. on Computing  13(4):332–344  September 2001.

[16] F. Nan  J. Wang  and V. Saligrama. Feature-budgeted random forest. In Proceedings of the 32nd

International Conference on Machine Learning (ICML-15)  2015.

[17] G. L. Nemhauser  L. A. Wolsey  and M. L. Fisher. An analysis of approximations for maximizing

submodular set functions. Mathematical Programming  14(1):265–294  1978.

[18] H. D. Sherali  A. G. Hobeika  and C. Jeenanunta. An optimal constrained pruning strategy for

decision trees. INFORMS Journal on Computing  21(1):49–61  2009.

[19] K. Trapeznikov and V. Saligrama. Supervised sequential classiﬁcation under budget constraints.

In International Conference on Artiﬁcial Intelligence and Statistics  pages 581–589  2013.

[20] J. Wang  K. Trapeznikov  and V. Saligrama. Efﬁcient learning by directed acyclic graph for
resource constrained prediction. In Advances in Neural Information Processing Systems. 2015.
In

[21] Z. Xu  M. Kusner  M. Chen  and K. Q. Weinberger. Cost-sensitive tree of classiﬁers.

Proceedings of the 30th International Conference on Machine Learning  2013.

[22] Z. E. Xu  K. Q. Weinberger  and O. Chapelle. The greedy miser: Learning under test-time
budgets. In Proceedings of the International Conference on Machine Learning  ICML  2012.
[23] Yi Zhang and Huang Huei-chuen. Decision tree pruning via integer programming. Working

paper  2005.

9

,Suvrit Sra
Reshad Hosseini
Siqi Nie
Denis Maua
Cassio de Campos
Qiang Ji
Feng Nan
Joseph Wang
Venkatesh Saligrama