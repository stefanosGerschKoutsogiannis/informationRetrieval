2019,LCA: Loss Change Allocation for Neural Network Training,Neural networks enjoy widespread use  but many aspects of their training  representation  and operation are poorly understood. In particular  our view into the training process is limited  with a single scalar loss being the most common viewport into this high-dimensional  dynamic process. We propose a new window into training called Loss Change Allocation (LCA)  in which credit for changes to the network loss is conservatively partitioned to the parameters. This measurement is accomplished by decomposing the components of an approximate path integral along the training trajectory using a Runge-Kutta integrator. This rich view shows which parameters are responsible for decreasing or increasing the loss during training  or which parameters "help" or "hurt" the network's learning  respectively. LCA may be summed over training iterations and/or over neurons  channels  or layers for increasingly coarse views. This new measurement device produces several insights into training. (1) We find that barely over 50% of parameters help during any given iteration. (2) Some entire layers hurt overall  moving on average against the training gradient  a phenomenon we hypothesize may be due to phase lag in an oscillatory training process. (3) Finally  increments in learning proceed in a synchronized manner across layers  often peaking on identical iterations.,LCA: Loss Change Allocation for

Neural Network Training

Janice Lan

Uber AI

janlan@uber.com

Rosanne Liu

Uber AI

rosanne@uber.com

Hattie Zhou

Uber

hattie@uber.com

yosinski@uber.com

Jason Yosinski

Uber AI

Abstract

Neural networks enjoy widespread use  but many aspects of their training  represen-
tation  and operation are poorly understood. In particular  our view into the training
process is limited  with a single scalar loss being the most common viewport into
this high-dimensional  dynamic process. We propose a new window into training
called Loss Change Allocation (LCA)  in which credit for changes to the network
loss is conservatively partitioned to the parameters. This measurement is accom-
plished by decomposing the components of an approximate path integral along the
training trajectory using a Runge-Kutta integrator. This rich view shows which
parameters are responsible for decreasing or increasing the loss during training 
or which parameters “help” or “hurt” the network’s learning  respectively. LCA
may be summed over training iterations and/or over neurons  channels  or layers
for increasingly coarse views. This new measurement device produces several
insights into training. (1) We ﬁnd that barely over 50% of parameters help during
any given iteration. (2) Some entire layers hurt overall  moving on average against
the training gradient  a phenomenon we hypothesize may be due to phase lag in
an oscillatory training process. (3) Finally  increments in learning proceed in a
synchronized manner across layers  often peaking on identical iterations.

Introduction

1
In the common stochastic gradient descent (SGD) training setup  a parameterized model is iteratively
updated using gradients computed from mini-batches of data chosen from some training set. Unfortu-
nately  our view into the high-dimensional  dynamic training process is often limited to watching
a scalar loss quantity decrease over time. There has been much research attempting to understand
neural network training  with some work studying geometric properties of the objective function
[7  20  28  24  21]  properties of whole networks and individual layers at convergence [4  7  15  35] 
and neural network training from an optimization perspective [30  4  5  3  19]. This body of work in
aggregate provides rich insight into the loss landscape arising from typical combinations of neural
network architectures and datasets. Literature on the dynamics of the training process itself is more
sparse  but a few salient works examine the learning phase through the diagonal of the Hessian 
mutual information between input and output  and other measures [1  25  14].
In this paper we propose a simple approach to inspecting training in progress by decomposing changes
in the overall network loss into a per-parameter Loss Change Allocation or LCA. The procedure for
computing LCA is straightforward  but to our knowledge it has not previously been employed for
investigating network training. We begin by deﬁning this measure in more detail  and then apply it to
reveal several interesting properties of neural network training. Our contributions are as follows:

1. We deﬁne the Loss Change Allocation as a per-parameter  per-iteration decomposition
of changes to the overall network loss (Section 2). Exploring network training with this
measurement tool uncovers the following insights.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: (a) Illustration of this paper’s method on a toy two-dimensional loss surface. We allocate
credit for changes to the model’s training loss to individual parameters (b) θ dim-1 and (c) θ dim-2
by multiplying parameter motion with the corresponding individual component of the gradient of
the training set. This partitions changes to the loss into individual Loss Change Allocation (LCA)
components allows us to measure which parameters learn at each timestep  providing a rich view into
the training process. In the example depicted  although both parameters move  the second parameter
captures all the credit  as only its component of the gradient is non-zero.

2. Learning is very noisy  with only slightly over half of parameters helping to reduce loss on

any given iteration (Section 3).

3. Some entire layers consistently drift in the wrong direction during training  on average
moving against the gradient. We propose and test an explanation that these layers are
slightly out of phase  lagging behind other layers during training (Section 4).

4. We contribute new evidence to suggest that the learning progress is  on a microscopic level 
synchronized across layers  with small peaks of learning often occurring at the same iteration
for all layers (Section 5).

2 The Loss Change Allocation approach
We begin by deﬁning the Loss Change Allocation approach in more detail. Consider a parameterized
training scenario where a model starts at parameter value θ0 and ends at parameter value θT after
training. The training process entails traversing some path P along the surface of a loss landscape
from θ0 to θT . There are several loss landscapes one might consider; in this paper we analyze the
training process  so we measure motion along the loss with respect to the entire training set  here
denoted simply L(θ). We analyze the loss landscape of the training set instead of the validation
set because we aim to measure training  not training confounded with issues of memorization vs.
generalization (though the latter certainly should be the topic of future studies).
The approach in this paper derives from a straightforward application of the fundamental theorem of
calculus to a path integral along the loss landscape:
L(θT ) − L(θ0) =

(1)
where C is any path from θ0 to θT and (cid:104)· ·(cid:105) is the dot product. This equation states that the change in
loss from θ0 to θT may be calculated by integrating the dot product of the loss gradient and parameter
motion along a path from θ0 to θT . Because ∇θL(θ) is the gradient of a function and thus is a
conservative ﬁeld  any path from θ0 to θT may be used; in this paper we consider the path taken by
the optimizer during the course of training. We may approximate this path integral from θ0 to θT by
using a series of ﬁrst order Taylor approximations along the training path. If we index training steps
by t ∈ [0  1  ...  T ]  the ﬁrst order approximation for the change in loss during one step of training is
the following  rewritten as a sum of its individual components:

(cid:104)∇θL(θ)  dθ(cid:105)

(cid:90)

C

L(θt+1) − L(θt) ≈ (cid:104)∇θL(θt)  θt+1 − θt(cid:105)
(∇θL(θt))(i)(θ(i)

K−1(cid:88)

=

t+1 − θ(i)

t ) :=

K−1(cid:88)

At i

(2)

(3)

where ∇θL(θt) represents the gradient of the loss of the whole training set w.r.t. θ evaluated at θt 
v(i) represents the i-th element of a vector v  and the parameter vector θ contains K elements. Note
that while we evaluate model learning by tracking progress along the training set loss landscape

i=0

i=0

2

✓dim-1<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>✓dim-2<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>loss<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>(a)<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>(b)<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>pathtakenfromθ0toθTcumulLCAgradientθdim-1θdim-2LCALCALCAcumulθ0θTL(θ)  training itself is accomplished using stochastic gradient approaches in which noisy gradients
from mini-batches of data drive parameter updates via some optimizer like SGD or Adam. As shown
in Equation 3  the difference in loss produced by one training iteration t may be decomposed into
K individual Loss Change Allocation  or LCA  components  denoted At i. These K components
represent the LCA for a single iteration of training  and over the course of T iterations of training we
will collect a large T × K matrix of At i values.
The total loss over the course of training will often decrease  and the above decomposition allows
us to allocate credit for loss decreases on a per-parameter  per-timestep level. Intuitively  when the
optimizer increases the value of a parameter and its component of the gradient on the whole training
set is negative  the parameter has a negative LCA and is “helping” or “learning”. Positive LCA is
“hurting” the learning process  which may result from several causes: a noisy mini-batch with the
gradient of that step going the wrong way  momentum  or a step size that is too large for a curvy or
rugged loss landscape as seen in [14  32]. If the parameter has a non-zero gradient but does not move 
it does not affect the loss. Similarly  if a parameter moves but has zero gradient  it does not affect the
loss. The sum of the K components is the overall change in loss at that iteration. Figure 1 depicts a
toy example using two parameters. Throughout the paper we use “helping” to indicate negative LCA
(a contribution to the reduction of total loss)  and “hurting” for positive LCA.
An important property of this decomposition is that it is grounded: the sum of individual components
equals the total change in loss  and each contribution has the same fundamental units as the loss
overall (e.g. nats or bits in the case of cross-entropy). This is in contrast to approaches that measure
quantities like parameter motion or approximate elements of the Fisher information (FI) [16  1] 
which also produce per-parameter measurements but depend heavily on the parameterization chosen.
For example  the FI metric is sensitive to scale (e.g. multiply one relu layer weights by 2 and next by
0.5: loss stays the same but FI of each layer changes and total FI changes). Further  LCA has the
beneﬁt of being signed  allowing us to make measurements and interpretations when training goes
backwards (Sections 3 and 4).
Ideally  summing up the K components should equal L(θt+1) − L(θt). In practice  the ﬁrst order
Taylor approximation is often inaccurate due to the curvature of the loss landscape. We can improve
on our LCA approximation from Equation 2 by replacing ∇θL(θt) with 1
2 θt +
2 θt+1) + ∇θL(θt+1))  with the (1  4  1) coefﬁcients coming from the fourth-order Runge–Kutta
1
method (RK4) [23  17] or equivalently from Simpson’s rule [31]. Using a midpoint gradient doubles
computation but shrinks accumulated error drastically  from ﬁrst order to fourth order. If the error is
still too large  we can halve the step size with composite Simpson’s rule by calculating gradients at
4 θt+1 as well. We halve the step size until the absolute error of change in
3
4 θt + 1
loss per iteration is less than 0.001  and we ensure that the cumulative error at the end of training is
less than 1%. First order and RK4 errors can be found in Table S1 in Supplementary Information.
Note that the approach described may be applied to any parameterized model trained via gradient
descent  but for the remainder of the paper we assume the case of neural network training.

6 (∇θL(θt) + 4∇θL( 1

4 θt+1 and 1

4 θt + 3

2.1 Experiments

We employ the LCA approach to examine training on two tasks: MNIST and CIFAR-10  with
architectures including a 3-layer fully connected (FC) network and LeNet [18] on MNIST  and
AllCNN [29] and ResNet-20 [9] on CIFAR-10. Throughout this paper we refer to training runs as
“dataset–network”  e.g.  MNIST–FC  MNIST–LeNet  CIFAR–AllCNN  CIFAR–ResNet  followed
by further conﬁguration details (such as the optimizer) when needed.
For each dataset–network conﬁguration  we train with both SGD and Adam optimizers  and conduct
multiple runs with identical hyperparameter settings. Momentum of 0.9 is used for all SGD runs 
except for one set of “no-momentum” MNIST–FC experiments. Learning rates are manually chosen
between 0.001 to 0.5. See Section S7 in Supplementary Information for more details on architectures
and hyperparameters. We also make our code available at https://github.com/uber-research/
loss-change-allocation. Note that we use standard network architectures to demonstrate use
cases of our tool; we strive for simplicity and interpretability of results rather than state-of-the-art
performance. Thus we do not incorporate techniques such as L2 regularization  data augmentation 
and learning rate decay. Since our method requires calculating gradients of the loss over the entire
training set  it is considerably slower than the regular training process  but remains tractable for small
to medium models; see Section S8 for more details on computation.

3

Figure 2: Frames from an animation of the learning process for two training runs. (left) The 1st layer
of an MNIST–FC (full shape is 100×784  but only the upper left quarter is shown for better clarity).
(right) The 2nd convolutional layer of an MNIST–LeNet (full shape is 40×20 of 5×5 blocks; only
upper left quarter is shown). Each pixel represents one parameter. The LeNet layer shows 5×5 grids
representing each ﬁlter  laid out by input channels (columns) and output channels (rows). Parameters
that help (decrease the loss) at a given time are shown as shades of green. Parameters that hurt
(increase the loss) are shown as shades of red. Larger magnitudes of LCA are darker and white
indicates zero LCA. Iteration 20 is partly through the main drop in loss  and 220 is one full epoch. In
MNIST–FC  we can see clusters spaced at intervals of 28 pixels  because these parameters connect
to the ﬂattened MNIST images. Learning is strongest in early iterations with mostly negative LCA 
remains strong for many iterations but with more variance in LCA across parameters  and has greatly
diminished by iteration 220  where much of learning is complete. The complete animations may be
viewed at: https://youtu.be/xcnoRnoVyXQ and https://youtu.be/EY3LoXmdkYU.

2.2 Direct visualization

We calculate LCA for every parameter at every iteration and animate the LCA values through all
the iterations in the whole training process. Figure 2 shows snapshots of frames from the video
visualization. In such videos  we arrange parameters ﬁrst by layer and then for each layer as two-
dimensional matrices (1-D vectors for biases)  and overlay LCA values as a heatmap. This animation
enables a granular view of the training process.
We can also directly visualize each parameter versus time  granting each parameter its own training
curve. We can optionally aggregate over neurons  channels  layers  etc. (see Section S2 for examples).
A beneﬁt of these visualizations is that they convey a large volume of data directly to the viewer 
surfacing subtle patterns and bugs that can be further investigated. Observed patterns also suggest
more quantitative metrics that surface traits of training. The rest of the paper is dedicated to such
metrics and traits.

3 Learning is very noisy
Although it is a commonly held view that the inherent noise in SGD-based neural network training
exists and is even considered beneﬁcial [15]  this noise is often loosely deﬁned as a deviation in
gradient estimation. While the minibatch gradient serves as as a suggested direction for parameter
movement  it is still one step away from the actual impact on decreasing loss over the whole training
set  which LCA represents precisely. By aggregating a population of per-parameter  per-iteration
LCAs along different axes  we present numerical results that shed light into the noisy learning
behavior. We ﬁnd it surprising that on average almost half of parameters are hurting in every training
iteration. Moreover  each parameter  including ones that help in total  hurt almost half of the time.

Table 1: Percentage of helping parameters (ignoring those with zero LCA) for various networks and
optimizers  averaged across all iterations and 3 independent runs per per conﬁguration.

SGD
Adam

53.72 ± 0.05

N/A

MNIST-FC  mom=0 MNIST-FC MNIST-LeNet CIFAR-ResNet CIFAR-AllCNN
51.09 ± 0.23
50.19 ± 0.01

50.66 ± 0.14
50.30 ± 0.004

57.79 ± 0.16
55.82 ± 0.09

53.97 ± 0.48
51.77 ± 0.21

4

050100150200250300350025iteration 1050100150200250300350025iteration 20050100150200250300350025iteration 220MNIST FC  SGD  1st layer02040020406080iteration 102040020406080iteration 2002040020406080iteration 220MNIST LeNet  SGD  2nd layer(a) Visualization of the percentage of parameters that helped  hurt  or had zero effect
Figure 3:
through training  overlaid with the loss curve of that run. (b) The distribution of helping and hurting
LCA (zeros ignored) over the entire training  zoomed in to ignore 1% of tails. (c) Average percent of
weights helping for each layer in network  curiously near 50% for all. (d) Histogram of the fraction
of iterations each weight helped  showing that most weights swing back and forth between helping
and hurting evenly. In every column the ﬁrst row is MNIST–FC and second row CIFAR–ResNet 
both trained with SGD. Notable facts: MNIST–FC shows a signiﬁcant percent of weights with zero
effect. Because MNIST has pixels that are never on  any ﬁrst layer weights connected to those pixels
cannot help or hurt. CIFAR–ResNet exhibits barely over 50% of parameters helping over the course
of training  even during the period of signiﬁcantly learning (loss reduction) from iteration 0 to 2000.
Averaged over the entire run  only 50.66% of parameters helped (see Table 1). Note that in both runs
we can see that in the earliest iterations  the percent of weights helping is higher  but only slightly.

Barely over 50% of parameters help during training. According to our deﬁnition  for each
iteration of training  parameters can help  hurt  or not impact the overall loss. With that in mind  we
count the number of parameters that help  hurt  or neither  across all training iterations and for various
networks; two examples of networks are shown in Figure 3 (all other networks shown in Section S3).
The data show that in a typical training iteration  close to half of parameters are helping and close
to half are hurting! This ratio is slightly skewed towards helping in early iterations but stays fairly
constant during training. Averaged across all iterations  the percentage of helping parameters for
various network conﬁgurations is reported in Table 1. We see that it varies within a small range of
50% to 58%  with CIFAR networks even tighter at 50% to 51%. This observation also holds true
when we look at each layer separately in a network; Figure 3(c) shows that all layers have similar
ratios of helpful parameters.
Parameters alternate helping. Now that we can tell if each parameter is “helpful”  “hurtful”  or
“neither”1  we wonder if parameters predictably stay in the same category throughout training. In
other words  is there a consistent elite group of parameters that always help? When we measure the
percentage of helpful iterations per parameter throughout a training run  histograms in Figure 3(d)
show that parameters help approximately half of the time  and therefore the training of a network is
achieved by parameters alternating to make helpful contribution to the loss.
Additionally  we can measure the oscillations of individual parameters. Figure S7 shows a high num-
ber of oscillations in weight movement for CIFAR–ResNet on SGD: on average  weight movements
change direction once every 6.7 iterations  and gradients change signs every 9.5 iterations. Section S3
includes these measures for all networks  as well as detailed views in Figure S8 suggesting that many
of these oscillations happen around local minima. While oscillations have been previously observed
for the overall network [32  14]  thanks to LCA  we’re able to more precisely quantify the individual
and net effects of these oscillations. As we’ll see in Section 4  we can also use LCA to identify when
a network is damaged not by oscillations themselves  but by their precise phase.
Noise persists across various hyperparameters. Changing the learning rate  momentum  or batch
size (within reasonable ranges such that the network still trains well) only have a slight effect on the
percent of parameters helping. See Section S3 for a set of experiments on CIFAR–ResNet with SGD 
where percent helped always stays within 50.3% to 51.6% for reasonable hyperparameters.

1We rarely see “neither”  or zero-impact parameters in CIFAR networks  but it can be of a noticable amount

for MNIST (around 20% for MNIST–FC; see Figure 3)  mostly due to the many dead pixels in MNIST.

5

MNIST-FC  SGDCIFAR-ResNet  SGD(b)(a)(c)(d)Figure 4: (left) LCA summed over all of training  for each layer  in CIFAR–ResNet trained with SGD.
Bias and batch norm layers are combined into their corresponding kernel layers. Blue represents
regular runs. Orange is with the last layer frozen at initialization. Note that the other layers  especially
the adjacent few  do not help as much  but the difference in LCA of the last layer is greater than
the total differences of the other layers helping less. Green is with the last layer at a 10x smaller
learning rate than the rest of the network  showing similar layer LCAs as when the layer is frozen.
(right) Resulting train loss and standard deviations for each run conﬁguration. Means and standard
deviations are over 10 runs for each experiment conﬁguration.

Learning is heavy-tailed. A reasonable mental model of the distribution of LCA might be a narrow
Gaussian around the mean. However  we ﬁnd that this is far from reality. Instead  the LCA of both
helping and hurting parameters follow a heavy-tailed distribution  as seen in Figure 3(b). Figure S10
goes into more depth in this direction  showing that contributions from the tail are about three times
larger than would be expected if learning were Gaussian distributed. More precisely  a better model
of LCA would be the Weibull distribution with k < 1. The measurements suggest that the view of
learning as a Wiener process [25] should be reﬁned to reﬂect the heavy tails.

4 Some layers hurt overall
Although our method is used to study low-level  per-parameter LCA  we can also aggregate these
over higher level breakdowns for different insights; individually there is a lot of noise  but on the
whole  the network learns. The behavior of individual layers during learning has been of interest to
many researchers [35  22]  so a simple and useful aggregation is to sum LCA over all parameters
within each layer and sum over all time  measuring how much each layer contributes to total learning.
We see an expected pattern for MNIST–FC and MNIST–Lenet (all layers helping; Figure S11) 
but CIFAR–ResNet with SGD shows a surprising pattern: the ﬁrst and last layers consistently hurt
training (positive total LCA). Over ten runs  the ﬁrst and last layer in ResNet hurt statistically
signiﬁcantly (p-values < 10−4 for both)  whereas all other layers consistently help (p-values < 10−4
for all). Blue bars in Figure 4 shows this distinct effect. Such a surprising observation calls for further
investigation. The following experiments shed light on why this might be happening.
Freezing the ﬁrst layer stops it from hurting but causes others to help less. We try various
experiments freezing the ﬁrst layer at its random initialization. Though we can prevent this layer from
hurting  the overall performance is not any better because the other layers  especially the neighboring
ones  start to help less; see Figure S13 for details. Nonetheless  this can be useful for reducing
compute resources during training as you can freeze the ﬁrst layer without impairing performance.
Freezing the last layer results in signiﬁcant improvement.
In contrast to the ﬁrst layer  freezing
the last layer at its initialization (Figure 4) improves training performance (and test performance
curiously; not shown)  with p-values < 0.001 for both train loss and test loss  over 10 runs! We also
observe other layers  especially neighboring ones  not helping as much  but this time the change in the
last layer’s LCA more than compensates. Decreasing the learning rate of the last layer by 10x (0.01
as opposed to 0.1 for other layers) results in similar behavior as freezing it. These experiments are
consistent with ﬁndings in [12] and [8]  which demonstrate that you can freeze the last layer in some
networks without degrading performance. With LCA  we are now able to provide an explanation for
when and why this phenomenon happens. The instability of the last layer at the start of training in [8]
can also be measured by LCA  as the LCA of the last layer is typically high in the ﬁrst few iterations.

6

036912151821layer1.00.50.00.51.01.52.02.5sum of LCA across layerTotal LCA per layer  ResNet SGDregularlast layer lr=0.01last layer frozenstdev (10 runs)0.000.010.020.030.040.050.060.070.08lossResulting lossFigure 5: CIFAR–ResNet SGD with varying momentum for the last layer (and a ﬁxed 0.9 for all
other layers). Selected momentum values are derived from linear values of delay [0  1  2  ...  9] in a
control system  where momentum = delay/(delay + 1)  and a delay of 9 corresponds to regular
runs of 0.9 momentum. (left) LCA per layer (only the second half of the network is shown for better
visibility; ﬁrst half follows a similar trend  but less pronounced). As the last layer helps more  the
other layers hurt more because they are relatively more delayed. (right) LCA of the last layer is fairly
linear with respect to the delay.

Phase shift hypothesis: is the last layer phase-lagged? While it is interesting to see that de-
creasing the learning rate by 10x or to zero changes the last layer’s behavior  this on its own does
not explain why the layer would end up going backwards. The mini-batch gradient is an unbiased
estimator of the whole training set gradient  so on average the dot product of the mini-batch gradient
with the training set gradient is positive. Thus we must look beyond noise and learning rate for
explanation. We hypothesize that the last layer may be phase lagged with respect to other layers
during learning. Intuitively  it may be that while all layers are oscillating during learning  the last
layer is always a bit behind. As each parameter swings back and forth across its valley  the shape of
its valley is affected by the motion of all other parameters. If one parameter is frozen and all other
parameters trained inﬁnitesimally slowly  that parameters valley will tend to ﬂatten out. This means if
it had climbed a valley (hurting the loss)  it will not be able to fully recover the LCA in the negative
direction  as the steep region has been ﬂattened. If the last layer reacts slower than others  its own
valley walls may tend to be ﬂattened before it can react.
A simple test for this hypothesis is as follows. We note that training with momentum 0.9 corresponds
to an information lag of 9 steps (the mean of an exponential series with exponent .9)—each update
applied uses information 9 steps old. To give the last layer an advantage  we train it with momentum
corresponding to a delay of n for n ∈ {9  8  ...  0} while training all other layers as usual. As shown
in Figure 5  this works  and the transition from hurting to helping (a lot) is almost linear with respect
to delay! As we give the last layer an information freshness advantage  it begins to “steal progress”
from other layers  eventually forcing the neighboring layers into positive LCA. These results suggest
that it may be proﬁtable to view training as a fundamentally oscillatory process upon which much
research in phase-space representations and control system design may come to bear.
Beyond CIFAR–Resnet  other networks also show intriguingly heterogeneous layer behaviors. As we
noted before  in the case of MNIST–FC and MNIST–LeNet trained with SGD  all layers help with
varying quantities. An MNIST–ResNet (added speciﬁcally to see if the effect we see above is due to
the data or the network) shows the last layer hurting as well. We also observe the last layer hurting
for CIFAR–AllCNN with SGD (Figure S14) and multiple layers hurting for a couple of VGG-like
networks (Figure S12). When using Adam instead of SGD  CIFAR–ResNet has a consistently hurting
ﬁrst layer and an inconsistently hurting last two layers. CIFAR–AllCNN trained with Adam does
not have any hurting layers. We note that layers hurting is not a universal phenomenon that will be
observed in all networks  but when it does occur  LCA can identify it and suggest potential candidates
to freeze. Further  viewing training through the lens of information delay seems valid  which suggests
that per-layer optimization adjustments may be beneﬁcial.

5 Learning is synchronized across layers
We learned that layers tend to have their own distinct  consistent behaviors regarding hurting or
helping from per-layer LCA summed across all iterations. In this section we further examine the
per-layer LCA during training  equivalent to studying individual “loss curves” for each layer  and

7

10121416182022layer86420sum of LCA across layermomentum = 0.9momentum variesTotal LCA per layer  ResNet SGD with varied last layer momentumdelay = 9  momentum = 0.9 (regular)delay = 8  momentum = 0.889delay = 7  momentum = 0.875delay = 6  momentum = 0.857delay = 5  momentum = 0.833delay = 4  momentum = 0.8delay = 3  momentum = 0.75delay = 2  momentum = 0.667delay = 1  momentum = 0.5delay = 0  momentum = 002468last layer delay86420last layer LCALast layer: LCA vs. delayregular runFigure 6: Peak learning iterations by layer by class on MNIST–FC. The same LCA data as in
Figure S17 but seperated by class. We plot the top 20 iterations by LCA for each class and each layer 
where that iteration represents a local minimum for LCA. The layers are ordered from bottom to
top. Points highlighted in red represent iterations where all three layers had peak learning for that
particular class. To measure the statistical signiﬁcance of these vertical line structures in red  we
simulate a baseline by shifting each layer in each class randomly by -2  -1  0  or 1  2 iteration. We
ﬁnd that the average number of vertical lines is 0.4 in the baseline and 9.4 in the actual network  and
this difference is signiﬁcant with a p-value < 0.001.

discover that the exact moments where learning peaks are curiously synchronized across layers. And
such synchronization is not driven by only gradients or parameter motion  but both.
We deﬁne “moments of learning” as temporal spikes in an instantaneous LCA curve  local minima
where loss decreased more on that iteration than on the iteration before or after  and show the top 20
such moments (highest magnitude of LCA) for each layer in Figure S17. We further decompose this
metric by class (10 for both MNIST and CIFAR)  where the same moments of learning are identiﬁed
on per-class  per-layer LCAs  shown in Figure 6. Whenever learning is synchronized across layers
(dots that are vertically aligned) they are marked in red. Additional ﬁgures on CIFAR–ResNet can be
seen in Section S5. The large proportion of red aligned stacks suggests that learning is very locally
synchronized across layers.
To gauge statistical signiﬁcance  we compare the number of synchronized moments in these networks
to a simple baseline: the number we would observe if each layer had been randomly shifted to one or
two iterations earlier or later. We ﬁnd that the number synchronized moments is signiﬁcantly more
than that of such a baseline (p-value < 1−6). See details on this experiment in Section S5. Thus 
we conclude that for these networks we’ve measured  learning happens curiously synchronously
across layers throughout the network. We might ﬁnd different behavior in other architectures such as
transformer models or recurrent neural nets  which could be of interest for future work.
But what drives such synchronization? Since learning is deﬁned as the product of parameter motion
and parameter gradient  we further examine whether one of them is synchronized in the ﬁrst place.
By plotting in the same fashion of identiﬁed local peaks  we observe the synchronization pattern in
gradients per layer is clearly different from that in LCA  either in terms of the total loss (Figure S18)
or per-class loss (Figure S16). Since parameter motion (Figure S19) is the same across all classes  it
alone doesn’t drive the per-class LCA. We therefore conclude that the synchronization of learning 
demonstrated by synchronized behavior in LCA movement (Figure 6)  is strong  and comes from
both parameter motion and gradient.

8

0255075100125150175200Training IterationClass: 0Class: 1Class: 2Class: 3Class: 4Class: 5Class: 6Class: 7Class: 8Class: 96 Conclusion

The Loss Change Allocation method acts as a microscope into the training process  allowing us to
examine the inner workings of training with much more ﬁne-grained analysis. When applied to
various tasks  networks and training runs  we observe many interesting patterns in neural network
training that induce better understanding of training dynamics  and bring about practical model
improvements.

6.1 Related work

We note additional connections to existing literature here. The common understanding is that learning
in networks is sparse; a subnetwork [6]  or a random subspace of parameters [19] is sufﬁcient for
optimization and generalization. Our method provides an additional  more accurate  measure of
usefulness to characterize per-parameter contribution. A similar work [34] deﬁnes per-parameter
importance in the same vein but is computed locally with the mini-batch gradient  which overestimates
the true per-parameter contribution to the decrease of loss of the whole training set.
Several previous works have increased our understanding of the training process. Alain and Bengio [2]
measured and tracked over time the ability to linearly predict the ﬁnal class output given intermediate
layers representations. Raghu et al. [22] found that networks converge to ﬁnal representations from
the bottom up  and class-speciﬁc information in networks is formed at various places. Shwartz-Ziv
and Tishby [25] visualized the training process through the information plane  where two phases are
identiﬁed as empirical error minimization of each followed by a slow representation compression.
There measurements are developed but none have examine the process each individual parameter
undergoes.
Methods like saliency maps [27]  DeepVis [33]  and others allow interpretation of representations or
loss surfaces. But these works only approach the end result of the model  not the training process in
progress. LCA can be seen as a new tool that specializes on the microscopic level of details  and such
inspection follows through the whole training process to reveal interesting facts about learning. Some
of our ﬁndings resonate with and complement other work. For example  in [35] it is also observed
that layers have heterogeneous characteristics; in that work layers are denoted as either “robust” or
“critical”  and robust layers can even be reset to their initial value with no negative consequence.

6.2 Future work

There are many potential directions to expand this work. Due to the expensive computation and the
amount of analyses  we have only tested vision classiﬁcation tasks on relatively small datasets so far.
In the future we would like to run this on larger datasets and tasks beyond supervised learning  since
the LCA method directly works on any parameterized model. An avenue to get past the expensive
computation is to analyze how well this method can be approximated with gradients of loss of a
subset of the training set. We are interested to see if the observations we made hold beyond the vision
task and the range of hyperparameters used.
Since per-weight LCA can be seen as a measurement of weight importance  an simple extension is to
perform weight pruning with it  as done in [6  36] (where weight’s ﬁnal value is used as an importance
measure). Further  if there are strong correlations between underperforming hyperparameters and
patterns of LCA  this may help in architecture search or identifying better hyperparameters.
We are also already able to identify which layers or parameters overﬁt by comparing their LCA on
the training set and LCA on the validation or test set  which motions towards future work on targeted
regularization. Finally  the observations about the noise  oscillations  and phase delays can potentially
lead to improved optimization methods.

Acknowledgements

We would like to acknowledge Joel Lehman  Richard Murray  and members of the Deep Collective
research group at Uber AI for conversations  ideas  and feedback on experiments.

9

References
[1] Alessandro Achille  Matteo Rovere  and Stefano Soatto. Critical learning periods in deep neural

networks. CoRR  abs/1711.08856  2017. URL http://arxiv.org/abs/1711.08856.

[2] G. Alain and Y. Bengio. Understanding intermediate layers using linear classiﬁer probes. ArXiv

e-prints  October 2016.

[3] Léon Bottou  Frank E Curtis  and Jorge Nocedal. Optimization methods for large-scale machine

learning. Siam Review  60(2):223–311  2018.

[4] Anna Choromanska  Mikael Henaff  Michael Mathieu  Gérard Ben Arous  and Yann LeCun.
The loss surfaces of multilayer networks. In Artiﬁcial Intelligence and Statistics  pages 192–204 
2015.

[5] Yann N Dauphin  Razvan Pascanu  Caglar Gulcehre  Kyunghyun Cho  Surya Ganguli  and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-
convex optimization. In Advances in neural information processing systems  pages 2933–2941 
2014.

[6] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse  trainable
neural networks. In International Conference on Learning Representations (ICLR)  volume
abs/1803.03635  2019. URL http://arxiv.org/abs/1803.03635.

[7] Ian J Goodfellow  Oriol Vinyals  and Andrew M Saxe. Qualitatively characterizing neural

network optimization problems. arXiv preprint arXiv:1412.6544  2014.

[8] Akhilesh Gotmare  Nitish Shirish Keskar  Caiming Xiong  and Richard Socher. A closer look
at deep learning heuristics: Learning rate restarts  warmup and distillation. In International
Conference on Learning Representations  2019. URL https://openreview.net/forum?
id=r14EOsCqKX.

[9] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image

recognition. CoRR  abs/1512.03385  2015. URL http://arxiv.org/abs/1512.03385.

[10] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
international conference on computer vision  pages 1026–1034  2015.

[11] Geoffrey E Hinton  Nitish Srivastava  Alex Krizhevsky  Ilya Sutskever  and Ruslan R Salakhut-
dinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv
preprint arXiv:1207.0580  2012.

[12] Elad Hoffer  Itay Hubara  and Daniel Soudry. Fix your classiﬁer: the marginal value of training
the last weight layer. CoRR  abs/1801.04540  2018. URL http://arxiv.org/abs/1801.
04540.

[13] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. CoRR  abs/1502.03167  2015. URL http://arxiv.org/
abs/1502.03167.

[14] Stanisław Jastrz˛ebski  Zachary Kenton  Nicolas Ballas  Asja Fischer  Yoshua Bengio  and
Amos Storkey. On the Relation Between the Sharpest Directions of DNN Loss and the
SGD Step Length. In International Conference on Learning Representations (ICLR)  page
arXiv:1807.05031  Jul 2019.

[15] Nitish Shirish Keskar  Dheevatsa Mudigere  Jorge Nocedal  Mikhail Smelyanskiy  and Ping
Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.
arXiv preprint arXiv:1609.04836  2016.

[16] James Kirkpatrick  Razvan Pascanu  Neil Rabinowitz  Joel Veness  Guillaume Desjardins  An-
drei A. Rusu  Kieran Milan  John Quan  Tiago Ramalho  Agnieszka Grabska-Barwinska  Demis
Hassabis  Claudia Clopath  Dharshan Kumaran  and Raia Hadsell. Overcoming catastrophic
forgetting in neural networks. Proceedings of the National Academy of Sciences  114(13):
3521–3526  2017. doi: 10.1073/pnas.1611835114. URL http://www.pnas.org/content/
114/13/3521.abstract.

10

[17] Wilhelm Kutta. Beitrag zur näherungweisen integration totaler differentialgleichungen. 1901.

[18] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[19] Chunyuan Li  Heerad Farkhoor  Rosanne Liu  and Jason Yosinski. Measuring the Intrinsic
Dimension of Objective Landscapes. In International Conference on Learning Representations 
April 2018.

[20] Hao Li  Zheng Xu  Gavin Taylor  Christoph Studer  and Tom Goldstein. Visualizing the
loss landscape of neural nets. In Advances in Neural Information Processing Systems  pages
6389–6399  2018.

[21] Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages
2603–2612. JMLR. org  2017.

[22] M. Raghu  J. Gilmer  J. Yosinski  and J. Sohl-Dickstein. Svcca: Singular vector canonical
correlation analysis for deep learning dynamics and interpretability. ArXiv e-prints  June 2017.

[23] Carl Runge. Über die numerische auﬂösung von differentialgleichungen. Mathematische

Annalen  46(2):167–178  1895.

[24] Itay Safran and Ohad Shamir. On the quality of the initial basin in overspeciﬁed neural networks.

In International Conference on Machine Learning  pages 774–782  2016.

[25] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via

information. CoRR  abs/1703.00810  2017. URL http://arxiv.org/abs/1703.00810.

[26] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. CoRR  abs/1409.1556  2014. URL http://arxiv.org/abs/1409.1556.

[27] Karen Simonyan  Andrea Vedaldi  and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034 
presented at ICLR Workshop 2014  2013.

[28] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error

guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361  2016.

[29] Jost Tobias Springenberg  Alexey Dosovitskiy  Thomas Brox  and Martin A. Riedmiller. Striving
for simplicity: The all convolutional net. CoRR  abs/1412.6806  2014. URL http://arxiv.
org/abs/1412.6806.

[30] Ilya Sutskever  James Martens  George Dahl  and Geoffrey Hinton. On the importance of
initialization and momentum in deep learning. In International conference on machine learning 
pages 1139–1147  2013.

[31] Eric W Weisstein. Simpson’s rule. 2003.

[32] Chen Xing  Devansh Arpit  Christos Tsirigotis  and Yoshua Bengio. A walk with sgd. 2018.

[33] J. Yosinski  J. Clune  A. Nguyen  T. Fuchs  and H. Lipson. Understanding Neural Networks

Through Deep Visualization. ArXiv e-prints  June 2015.

[34] Friedemann Zenke  Ben Poole  and Surya Ganguli.

Improved multitask learning through
synaptic intelligence. CoRR  abs/1703.04200  2017. URL http://arxiv.org/abs/1703.
04200.

[35] Chiyuan Zhang  Samy Bengio  and Yoram Singer. Are all layers created equal? arXiv preprint

arXiv:1902.01996  2019.

[36] Hattie Zhou  Janice Lan  Rosanne Liu  and Jason Yosinski. Deconstructing lottery tickets:

Zeros  signs  and the supermask. arXiv preprint arXiv:1905.01067  2019.

11

,Farzaneh Mirzazadeh
Siamak Ravanbakhsh
Nan Ding
Dale Schuurmans
Janice Lan
Rosanne Liu
Hattie Zhou
Jason Yosinski