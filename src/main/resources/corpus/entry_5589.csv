2013,Top-Down Regularization of Deep Belief Networks,Designing a principled and effective algorithm for learning deep architectures is a challenging problem. The current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization. We suggest a deep learning strategy that bridges the gap between the two phases  resulting in a three-phase learning procedure. We propose to implement the scheme using a method to regularize deep belief networks with top-down information. The network is constructed from building blocks of restricted Boltzmann machines learned by combining bottom-up and top-down sampled signals. A global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used. Experiments on the MNIST dataset show improvements over the existing algorithms for deep belief networks. Object recognition results on the Caltech-101 dataset also yield competitive results.,Top-Down Regularization of Deep Belief Networks

Hanlin Goh∗  Nicolas Thome  Matthieu Cord

Laboratoire d’Informatique de Paris 6

UPMC – Sorbonne Universit´es  Paris  France
{Firstname.Lastname}@lip6.fr

Joo-Hwee Lim†

joohwee@i2r.a-star.edu.sg

Institute for Infocomm Research

A*STAR  Singapore

Abstract

Designing a principled and effective algorithm for learning deep architectures is a
challenging problem. The current approach involves two training phases: a fully
unsupervised learning followed by a strongly discriminative optimization. We
suggest a deep learning strategy that bridges the gap between the two phases  re-
sulting in a three-phase learning procedure. We propose to implement the scheme
using a method to regularize deep belief networks with top-down information. The
network is constructed from building blocks of restricted Boltzmann machines
learned by combining bottom-up and top-down sampled signals. A global op-
timization procedure that merges samples from a forward bottom-up pass and a
top-down pass is used. Experiments on the MNIST dataset show improvements
over the existing algorithms for deep belief networks. Object recognition results
on the Caltech-101 dataset also yield competitive results.

1

Introduction

Deep architectures have strong representational power due to their hierarchical structures. They
are capable of encoding highly varying functions and capture complex relationships and high-level
abstractions among high-dimensional data [1]. Traditionally  the multilayer perceptron is used to
optimize such hierarchical models based on a discriminative criterion that models P (y|x) using a
error backpropagating gradient descent [2  3]. However  when the architecture is deep  it is challeng-
ing to train the entire network through supervised learning due to the large number of parameters 
the non-convex optimization problem and the dilution of the error signal through the layers. This
optimization may even lead to worse performances as compared to shallower networks [4].
Recent developments in unsupervised feature learning and deep learning algorithms have made it
possible to learn deep feature hierarchies. Deep learning  in its current form  typically involves two
consecutive learning phases. The ﬁrst phase greedily learns unsupervised modules layer-by-layer
from the bottom-up [1  5]. Some common criteria for unsupervised learning include the maxi-
mum likelihood that models P (x) [1] and the input reconstruction error of vector x [5–7]. This is
subsequently followed by a supervised phase that ﬁne-tunes the network using a supervised  usu-
ally discriminative algorithm  such as supervised error backpropagation. The unsupervised learning
phase initializes the parameters without taking into account the ultimate task of interest  such as
classiﬁcation. The second phase assumes the entire burden of modifying the model to ﬁt the task.
In this work  we propose a gradual transition from the fully-unsupervised learning to the highly-
discriminative optimization. This is done by adding an intermediate training phase between the two
existing deep learning phases  which enhances the unsupervised representation by incorporating
top-down information. To realize this notion  we introduce a new global (non-greedy) optimization
∗Hanlin Goh is also with the Institute for Infocomm Research  A*STAR  Singapore and the Image and
†Joo-Hwee Lim is also with the Image and Pervasive Access Lab  CNRS UMI 2955  Singapore – France.

Pervasive Access Lab  CNRS UMI 2955  Singapore – France.

1

that regularizes the deep belief network (DBN) from the top-down. We retain the same gradient
descent procedure of updating the parameters of the DBN as the unsupervised learning phase. The
new regularization method and deep learning strategy are applied to handwritten digit recognition
and dictionary learning for object recognition  with competitive empirical results.

2 Related Work

Restricted Boltzmann Machines. A restricted Boltzmann
machine (RBM) [8] is a bipartite Markov random ﬁeld with an
input layer x ∈ RI and a latent layer z ∈ RJ (see Figure 1). The
layers are connected by undirected weights W ∈ RI×J. Each
unit also receives input from a bias parameter bj or ci. The joint
conﬁguration of binary states {x  z} has an energy given by:
(cid:88)

E(x  z) = −z(cid:62)Wx − b(cid:62)z − c(cid:62)x.

The probability assigned to x is given by:

(cid:88)

(1)

(cid:88)

P (x) =

exp(−E(x  z)) 

Z =

x

z

Figure 1: Structure of the RBM.

exp(−E(x  z)) 

(2)

1
Z

z

(cid:89)
(cid:89)

j

i

where Z is known as the partition function  which normalizes P (x) to a valid distribution. The units
in a layer are conditionally independent with distributions given by logistic functions:

P (z|x) =

P (x|z) =

P (zj|x) 

P (xi|z) 

P (zj|x) = 1/(1 + exp(−w(cid:62)
P (xi|z) = 1/(1 + exp(−wiz − ci)).

j x − bj)) 

(3)

(4)

This enables the model to be sampled via alternating Gibbs sampling between the two layers. To
estimate the maximum likelihood of the data distribution P (x)  the RBM is trained by taking the
gradient of the log probability of the input data with respect to the parameters:

∂ log P (x)

∂wij

≈ (cid:104)xizj(cid:105)0 − (cid:104)xizj(cid:105)N  

(5)

where (cid:104)·(cid:105)t denotes the expectation under the distribution at the t-th sampling of the Markov chain.
The ﬁrst term samples the data distribution at t = 0  while the second term approximates the equi-
librium distribution at t = ∞ using the contrastive divergence method [9] by using a small and ﬁnite
number of sampling steps N to obtain a distribution of reconstructed states at t = N. RBMs have
also been regularized to produce sparse representations [10  11].

Supervised Restricted Boltzmann Machines. To introduce
class labels to the RBM  a one-hot coded output vector y ∈ RC
is deﬁned  where yc = 1 iff c is the class index. Another set of
weights V ∈ RC×J connects y with z. The two vectors are con-
catenated to form a new input vector [x  y] for the RBM  which
is linked to z through [W(cid:62)  V(cid:62)]  as shown in Figure 2. This
supervised RBM models the joint distribution P (x  y). The en-
ergy function of this model can be extended to
E(x  y  z) = −z(cid:62)Wx − z(cid:62)Vy − b(cid:62)z − c(cid:62)x − d(cid:62)y (6)

The conditional distribution of the concatenated vector is now:
(7)

P (x  y|z) = P (x|z)P (y|z) =
where P (xi|z) is given in Equation 4 and the outputs yc may
either be logistic units or the softmax units. The RBM may
again be trained using contrastive divergence algorithm [9] to
approximate the maximum likelihood of joint distribution.

P (yc|z) 

P (xi|z)

c

i

(cid:89)

(cid:89)

2

Figure 2: A supervised RBM
jointly models inputs and outputs.
Biases are omitted for simplicity.

1z!Latent layer!I input units!J latent units!x!Input layer!W!bc!z!Latent layer!I input units!J latent units!x!Inputs!W!C output units!y!Classes!V!Concatenated!layer!During inference  only x is given and y is set at a neutral value  which makes this part of the RBM
‘noisy’. The objective is to use x to ‘denoise’ y and obtain the prediction. This can be done by
several iterations of alternating Gibbs sampling. If the number of classes is huge  the number of
input units need to be huge to maintain a high signal to noise ratio. Larochelle and Bengio [12]
suggested to couple this generative model P (x  y) with a discriminative model P (y|x)  which can
help alleviate this issue. However  if the objective is to train a deep network  then with ever new
layer  the previous V has to be discarded and retrained.
It may also not be desirable to use a
discriminative criterion directly from the outputs  especially in the initial layers of the network.

Deep Belief Networks. Deep belief networks (DBN) [1] are probabilistic graphical models made
up of a hierarchy of stochastic latent variables. Being universal approximators [13]  they have been
applied to a variety of problems such as image and video recognition [1  14]  dimension reduc-
tion [15]. It follows a two-phase training strategy of unsupervised greedy pre-training followed by
supervised ﬁne-tuning.
For unsupervised pre-training  a stack of RBMs is trained greedily from the bottom-up  with the
latent activations of each layer used as the inputs for the next RBM. Each new layer RBM models the
data distribution P (x)  such that when higher-level layers are sufﬁciently large  the variational bound
on the likelihood always improves [1]. A popular method for supervised ﬁne-tuning backpropagates
the error given by P (y|x) to update the network’s parameters. It has been shown to perform well
when initialized by ﬁrst learning a model of input data using unsupervised pre-training [15].
An alternative supervised method is a generative model that implements a supervised RBM (Fig-
ure 2) that models P (x  y) at the top layer. For training  the network employs the up-down back-
ﬁtting algorithm [1]. The algorithm is initialized by untying the network’s recognition and generative
weights. First  a stochastic bottom-up pass is performed and the generative weights are adjusted to
be good at reconstructing the layer below. Next  a few iterations of alternating sampling using the
respective conditional probabilities are done at the top-level supervised RBM between the concate-
nated vector and the latent layer. Using contrastive divergence the RBM is updated by ﬁtting to its
posterior distribution. Finally  a stochastic top-down pass adjusts bottom-up recognition weights to
reconstruct the activations of the layer above.
In this work  we extend the existing DBN training strategy by having an additional supervised train-
ing phase before the discriminative error backpropagation. A top-down regularization of the net-
work’s parameters is proposed. The network is optimized globally so that the inputs gradually map
to the output through the layers. We also retain the simple method of using gradient descent to
update the weights of the RBMs and retain the same convention for generative RBM learning.

3 Top-Down RBM Regularization: The Building Block

We regularize RBM learning with targets obtained by sampling from higher-level representations.

Generic Cross-Entropy Regularization. The aim is to construct a top-down regularized building
block for deep networks  instead of combining the optimization criteria directly [12]  which is done
for the supervised RBM model (Figure 2). To give control over individual elements in the latent
vector  one way to manipulate the representations is to point-wise bias the activations for each latent
variable j [11]. Given a training dataset Dtrain  a regularizer based on the cross-entropy loss can be
deﬁned to penalize the difference between the latent vector z and a target vector ˆz:

LRBM +reg(Dtrain) = −

log P (xk) − α

The update rule of the cross-entropy-regularized RBM can be modiﬁed to:

|Dtrain|(cid:88)

k=1

|Dtrain|(cid:88)

J(cid:88)

k=1

j=1

log P (ˆzjk|zjk).

(8)

(9)

∆wij ∝ (cid:104)xisj(cid:105)0 − (cid:104)xizj(cid:105)N  

where

(10)
is the merger of the latent and target activations used to update the parameters. Here  the inﬂuences
of ˆzj and zj are regulated by parameter λ. If λ = 0 or when the activationes match (i.e. zj = ˆzj) 
then the parameter update is exactly that the original contrastive divergence learning algorithm.

sj = (1 − λ) zj + λˆzj

3

Building Block. The same principle of regularizing the latent activations can be used to combine
signals from the bottom-up and top-down. This forms the building block for optimizing a DBN
with top-down regularization. The basic building block is a three-layer structure consisting of three
consecutive layers: the previous zl−1 ∈ RI  current zl ∈ RJ and next zl+1 ∈ RH layers. The
layers are connected by two sets of weight parameters Wl−1 and Wl to the previous and next
layers respectively. For the current layer zl  the bottom-up representations zl l−1 are sampled from
the previous layer zl−1 through weighted connections Wl−1 with:
P (zl l−1 j | zl−1; Wl−1) = 1/(1 + exp(−w(cid:62)

l−1 jzl−1 − bl j)) 

where the two terms in the subscripts of a sampled representation zdest src refer to the destination
(dest) and source (src) layers respectively. Meanwhile  sampling from the next layer zl+1 via
weights Wl drives the top-down representations zl l+1:

(11)

P (zl l+1 j | zl+1; Wl) = 1/(1 + exp(−wl jzl+1 − cl j)).

(12)

The objective is to learn the RBM parameters Wl−1 that map from the previous layer zl−1 to
the current latent layer zl l−1  by maximizing the likelihood of the previous layer P (zl−1) while
considering the top-down samples zl l+1 from the next layer zl+1 as target representations. The loss
function for a network with L layers can be broken down as:

LDBN +topdown =

Ll RBM +topdown

where the cross-entropy regularization the loss function for the layer is

Ll RBM +topdown = −

log P (zl−1 k) − α

log P (zl l+1 jk|zl l−1 jk).

This results in the following gradient descent:

where

L(cid:88)

l=2

|Dtrain|(cid:88)

J(cid:88)

k=1

j=1

|Dtrain|(cid:88)
∆wl−1 ij = ε(cid:0)

k=1

(cid:1)  

(cid:104)zl−1 l−2 isl j(cid:105)0 − (cid:104)zl−1 l izl l−1 j(cid:105)N

(cid:124) (cid:123)(cid:122) (cid:125)
sl jk = (1 − λl) zl l−1 jk

Bottom-up

+λl zl l+1 jk

(cid:124) (cid:123)(cid:122) (cid:125)

Top-down

 

(13)

(14)

(15)

(16)

is the merged representation from the bottom-up and top-down signals (see Figure 3)  weighted by
hyperparameter λl. The bias towards one source of signal can be adjusted by selecting an appropriate
λl. Additionally  the alternating Gibbs sampling  necessary for the contrastive divergence updates 
is performed from the unbiased bottom-up samples using Equation 11 and a symmetric decoder:

P (zl−1 l j = 1 | zl l−1; Wl−1) = 1/(1 + exp(−wl−1 izl l−1 − cl−1 j)).

(17)

Figure 3: The basic building block learns a bottom-up latent representation regularized by top-
down signals. Bottom-up zl l−1 and top-down zl l+1 latent activations are sampled from zl−1 and
zl+1 respectively. They are merged to get the modiﬁed activations sl used for parameter updates.
Reconstructions independently driven from the input signals form the Gibbs sampling Markov chain.

4

Bottom-up!Top-down!Merged!zl l1zl+1zl1zl1 lzl l+1zl l11-step CD!slWl1WlPrevious layer!Next layer!Intermediate layer!4 Globally-Optimized Deep Belief Networks

Forward-Backward Learning Strategy.
In the DBN  RBMs are stacked from the bottom-up in
a greedy layer-wise manner  with each new layer modeling the posterior distribution of the previous
layer. Similarly  regularized building blocks can also be used to construct the regularized DBN
(Figure 4). The network  as illustrated in Figure 4(a)  comprises of a total of L − 1 RBMs. The
network can be trained with a forward and backward strategy (Figure 4(b)). It integrates top-down
regularization with contrastive divergence learning  which is given by alternating Gibbs sampling
between the layers (Figure 4(c)).

(a) Top-down regularized deep belief network.

(b) Forward and backward passes for top-down regularization.

(c) Alternating Gibbs sampling chains for contrastive divergence learning.

Figure 4: Constructing a top-down regularized deep belief network (DBN). All the restricted Boltz-
mann machines (RBM) that make up the network are concurrently optimized.
(a) The building
blocks are connected layer-wise. Both bottom-up and top-down activations are used for training the
network. (b) Activations for the top-down regularization are obtained by sampling and merging the
forward pass and the backward pass. (c) From the activations of the forward pass  the reconstructions
can be obtained by performing alternating Gibbs sampling with the previous layer.

In the forward pass  given the input features  each layer zl is sampled from the bottom-up  based on
the representation of the previous layer zl−1 (Equation 11). The top-level vector zL is activated with
the softmax function. Upon reaching the output layer  the backward pass begins. The activations zL
are combined with the output labels y to produce sL given by

(18)
The merged activations sl (Equation 16)  which besides being used for parameter updates  have a
second role of activating the lower layer zl−1 from the top-down:

sL ck = (1 − λL)zL L−1 ck + λLyck 

P (zl−1 l j | sl; Wl) = 1/1 + exp(−wl−1 jsl − cl−1 j).
This is repeated until the second layer is reached (l = 2) and s2 is computed.

(19)

5

Input!Output!Layer 2!Layer 4!Layer 3!xz3 2z4 5z2 1z2 3z3 4z4 3yz5 4z2 3z3 4z2 1z3 2z4 3z1 2s2s3s4s5z4 5z5 4xz3 2z4 5z2 1z2 3z3 4Merged!z4 3yz5 4s2s3s4s5Forward pass!Backward pass!xz3 2z2 1z4 3z5 4z2 3z3 4z2 1z3 2z4 3z1 2z4 5z5 41-step CD!Top-down sampling encourages the class-based invariance of the bottom-up representations. How-
ever  sampling from the top-down  with the output vector y as the only source will result in only
one activation pattern per class. This is undesirable  especially for the bottom layers  which should
have representations more heavily inﬂuenced by bottom-up data. By merging the top-down repre-
sentations with the bottom-up ones  the representations will encode both instance-based variations
and class-based variations. In the last layer  we typically set λL as 1  so that the ﬁnal RBM given by
WL−1 learns to map to the class labels y. Backward activation of zL−1 L is a class-based invariant
representation obtained from y and used to regularize WL−2. All other backward activations from
this point onwards are based on the merged representation from instance- and class-based represen-
tations.

Three-Phase Learning Procedure. After greedy learning models P (x) and the top-down regu-
larized forward-backward learning is executed. The eventual goal of the network is to be able to give
a prediction of P (y|x). This suggest that the network can adopt a three-phase strategy for training 
whereby the parameters learned in one phase initializes the next  as follows:

• Phase 1 – Unsupervised Greedy. The network is constructed by greedily learning a new
unsupervised RBM on top of the existing network. To enhance the representations  various
regularizations  such as sparsity [10]  can be applied. The stacking process is repeated for
L − 2 RBMs  until layer L − 1 is added to the network.
• Phase 2 – Supervised Regularized. This phase begins by connecting the L − 1 to a ﬁnal
layer  which is activated by the softmax activation function for a classiﬁcation problem.
Using the one-hot coded output vector y ∈ RC as its target activations and setting λL to 1 
the RBM is learned as an associative memory with the following update:
(20)
∆wL−1 ic ∝ (cid:104)zL−1 L−2 i yc(cid:105)0 − (cid:104)zL−1 L i zL L−1 c(cid:105)N .
This ﬁnal RBM  together with the other RBMs learned from Phase 1  form the initialization
for the top-down regularized forward-backward learning algorithm. This phase is used to
ﬁne-tune the network using generative learning  and binds the layers together by aligning
all the parameters of the network with the outputs.

• Phase 3 – Supervised Discriminative. Finally  the supervised error backpropagation al-
gorithm is used to improve class discrimination in the representations. Backpropagation
can also be described in two passes. In the forward pass  each layer is activated from the
bottom-up to obtain the class predictions. The classiﬁcation error is then computed based
on the groundtruth and the backward pass performs gradient descent on the parameters by
backpropagating the errors through the layers from the top-down.

From Phase 1 to Phase 2  the form of the parameter update rule based on gradient descent does not
change. Only that top-down signals are also taken into account. Essentially  the two phases are
performing a variant of the contrastive divergence algorithm. Meanwhile  from Phase 2 to Phase 3 
the inputs to the phases (x and y) do not change  while the optimization function is modiﬁed from
performing regularization to being completely discriminative.

5 Empirical Evaluation

In this work  the proposed deep learning strategy and top-down regularization method were eval-
uated and analyzed using the MNIST handwritten digit dataset [16] and the Caltech-101 object
recognition dataset [17].

5.1 MNIST Handwritten Digit Recognition

The MNIST dataset contains images of handwritten digits. The task is to recognize a digit from
0 to 9 given a 28 × 28 pixel image. The dataset is split into 60  000 images used for training
and 10  000 test images. Many different methods have used this dataset to perform evaluation on
classiﬁcation performances  speciﬁcally the DBNN [1]. The basic version of this dataset  with
neither preprocessing nor enhancements  was used for the evaluation. A ﬁve-layer DBN was setup
to have the same topography as evaluated in [1]. The number of units in each layer  from the ﬁrst to
the last layer  were 784  500  500  2000 and 10  in that order. Five architectural setups were tested:

6

1. Stacked RBMs with up-down learning (original DBN reported in [1]) 
2. Stacked RBMs with forward-backward learning and backpropagation 
3. Stacked sparse RBMs [11] with forward-backward learning and backpropagation  and
4. Stacked sparse RBMs [11] with backpropagation  and
5. Forward-backward learning from random weights.

In the phases 1 and 2  we followed the evaluation procedure of Hinton et al. [1] by initially using
44  000 training and 10  000 validation images to train the network before retraining it with the
full training set. In phase 3  sets of 50  000 and 10  000 images were used as the initial training
and validation sets. After model selection  the network was retrained on the training set of 60  000
images.
To simplify the parameterization for the forward-backward learning in phase 2  the top-down mod-
ulation parameter λl across the layers were controlled by a single parameter γ using the function:

λl = |l − 1|γ/(|l − 1|γ − |L − l|γ).

(21)
where γ > 0. The top-down inﬂuence for a layer l is also dependent on its relative position in the
network. The function assigns λl such that the layers nearer to the input will have stronger inﬂuences
from the input  while the layers near the output will be biased towards the output. This distance-
based modulation of their inﬂuences enables a gradual mapping between the input and output layers.
Our best performance was obtained using setting 3  which got an error rate of 0.91% on the test
set. Figure 5 shows the 91 wrongly classiﬁed test examples for this setting. When initialized with
the conventional RBMs but ﬁne-tuned with forward-backward learning and error backpropagation 
the score was 0.98%. As a comparison  the conventional DBN obtained an error rate of 1.25%.
Directly optimizing the network from random weights produced an error of 1.61%  which is still
fairly decent  considering that the network was optimized globally from scratch. For each setup  the
intermediate results for each training phase are reported in Table 1.
Overall  the results achieved are very competitive for methods with the same complexity that rely on
neither convolution nor image distortions and normalization. A variant of the DBN  which focused
on learning nonlinear transformations of the feature space for nearest neighbor classiﬁcation [18] 
had an error rate of 1.0%. The deep convex net [19]  which utilized more complex convex-optimized
modules as building blocks but did not perform ﬁne-tuning on a global network level  got a score
of 0.83%. At the time of writing  the best performing model on the dataset gave an error rate of
0.23% and used a heavy architecture of a committee of 35 deep convolutional neural nets with
elastic distortions and image normalization [20].
From Table 1  we can observe that each of the three learning phases helped to improve the overall
performance of the networks. The forward-backward algorithm outperforms the up-down learn-
ing of the original DBN. Using sparse RBMs [11] and backpropagation  it was possible to further
improve the recognition performances. The forward-backward learning was effective as a bridge
between the other two phases  with an improvement of 0.17% over the setup without phase 2. The
method was even as a standalone algorithm  demonstrating its potential by learning from randomly
initialized weights.

Table 1: Results on MNIST after various phases of the training process.
Classiﬁcation error rate
Setup / Learning algorithm*

Phase 1

Phase 2

Phase 1

Phase 2

Phase 3

Up-down

Deep belief network (reported in [1])
1. RBMs
2.49%
Proposed top-down regularized deep belief network
2. RBMs
2.49%
3.
2.14%
4.
2.14%
5. Random weights
*Phase 3 runs the error backpropagation algorithm whenever employed.

Forward-backward
Forward-backward
–
Forward-backward

Sparse RBMs
Sparse RBMs

–

1.25%

1.14%
1.06%

–

1.61%

–

0.98%
0.91%
1.08%

–

7

Figure 5: The 91 wrongly classiﬁed test examples from the MNIST dataset.

5.2 Caltech-101 Object Recognition

The Caltech-101 dataset [17] is one of the most popular datasets for object recognition evaluation.
It contains 9  144 images belonging to 101 object categories and one background class. The images
were ﬁrst resized while retaining their original aspect ratios  such that the longer spatial dimension
was at most 300 pixels. SIFT descriptors [21] were extracted from densely sampled patches of
16 × 16 at 4 pixel intervals. The SIFT descriptors were (cid:96)1-normalized by constraining each de-
scriptor vector to sum to a maximum of one  resulting in a quasi-binary feature. Additionally  SIFT
descriptors from a spatial neighborhood of 2 × 2 were concatenated to form a macrofeature [22].
A DBN setup was used to learn a dictionary to map local macrofeatures to a mid-level representa-
tion. Two layers of RBMs were stacked to model the macrofeatures. Both RBMs were regularized
with population and lifetime sparseness during training [23]. First a single RBM  which had 1024
latent variables  was trained from macrofeature. A set of 200  000 randomly selected macrofea-
tures was used for training this ﬁrst layer. The resulting representations of the ﬁrst RBM were then
concatenated within each spatial neighborhood of 2 × 2. The second RBM modeled this spatially
aggregated representation into a higher-level representation. Another set of 200  000 randomly se-
lected spatially aggregated representations was used for training this RBM.
The higher-level RBM representation was associated to the image label. For each experimental
trial  a set of 30 training examples per class (totaling to 3060) was randomly selected for supervised
learning. The forward-backward learning algorithm was used to regularize the learning while ﬁne-
tuning the network. Finally  error backpropagation was performed to further optimize the dictionary.
From these representations  max-pooling within spatial regions deﬁned by a spatial pyramid was
employed [22  24] to obtain a single vector representing the whole image. It is also possible to
employ more advanced pooling schemes [25]. A linear SVM classiﬁer was then trained  using the
same train-test split from the previous supervised learning phase.
Table 2 shows the average class-wise clas-
siﬁcation accuracy  averaged across 102
classes and 10 experimental trials. The re-
sults demonstrate a consistent improvement
moving from Phase 1 to phase 3. The ﬁnal
accuracy obtained was 79.7%. This outper-
forms all existing dictionary learning meth-
ods based on a single image descriptors 
with a 0.8% improvement over the previous
state-of-the-art results [23  28]. As a com-
parison  other existing reported dictionary
learning methods that encode SIFT-based lo-
cal descriptors are also included in Table 2.

Table 2: Classiﬁcation accuracy on Caltech-101.
Method / Training phase
Accuracy
Proposed top-down regularized DBN
Phase 1: Unsupervised stacking
Phase 2: Top-down regularization
Phase 3: Error backpropagation

Sparse coding & max-pooling [22]
Extended HMAX [26]
Convolutional RBM [27]
Unsupervised & supervised RBM [23]
Gated Convolutional RBM [28]

72.8%
78.2%
79.7%

73.4%
76.3%
77.8%
78.9%
78.9%

6 Conclusion

We proposed the notion of deep learning by gradually transitioning from being fully unsupervised to
strongly discriminative. This is achieved through the introduction of an intermediate phase between
the unsupervised and supervised learning phases. This notion is implemented by incorporating
top-down information to DBNs through regularization. The method is easily integrated into the
intermediate learning phase based on simple building blocks. It can be performed to complement
greedy layer-wise unsupervised learning and discriminative optimization using error backpropaga-
tion. Empirical evaluation show that the method leads to competitive results for handwritten digit
recognition and object recognition datasets.

8

References
[1] G. E. Hinton  S. Osindero  and Y.-W. Teh  “A fast learning algorithm for deep belief networks ” Neural

Computation  vol. 18  no. 7  pp. 1527–1554  2006.

[2] Y. LeCun  “Une proc´edure d’apprentissage pour r´eseau a seuil asymmetrique (a learning scheme for

asymmetric threshold networks) ” in Cognitiva 85  1985.

[3] D. E. Rumelhart  G. E. Hinton  and R. J. Williams  “Learning representations by back-propagating errors ”

Nature  vol. 323  pp. 533 – 536  October 1986.

[4] Y. Bengio  “Learning deep architectures for AI ” Foundations and Trends in Machine Learning  vol. 2 

no. 1  pp. 1–127  2009.

[5] Y. Bengio  P. Lamblin  D. Popovici  and H. Larochelle  “Greedy layer-wise training of deep networks ” in

NIPS  2006.

[6] M. Ranzato  C. Poultney  S. Chopra  and Y. LeCun  “Efﬁcient learning of sparse representations with an

energy-based model ” in NIPS  2006.

[7] P. Vincent  H. Larochelle  Y. Bengio  and P.-A. Manzagol  “Extracting and composing robust features

with denoising autoencoders ” in ICML  2008.

[8] P. Smolensky  “Information processing in dynamical systems: Foundations of harmony theory ” in Paral-

lel Distributed Processing: Volume 1: Foundations  ch. 6  pp. 194–281  MIT Press  1986.

[9] G. E. Hinton  “Training products of experts by minimizing contrastive divergence ” Neural Computation 

vol. 14  no. 8  p. 1771–1800  2002.

[10] H. Lee  C. Ekanadham  and A. Ng  “Sparse deep belief net model for visual area V2 ” in NIPS  2008.
[11] H. Goh  N. Thome  and M. Cord  “Biasing restricted Boltzmann machines to manipulate latent selectivity

and sparsity ” in NIPS Workshop  2010.

[12] H. Larochelle and Y. Bengio  “Classiﬁcation using discriminative restricted Boltzmann machines ” in

ICML  2008.

[13] N. Le Roux and Y. Bengio  “Representational power of restricted Boltzmann machines and deep belief

networks ” Neural Computation  vol. 20  pp. 1631–1649  June 2008.

[14] I. Sutskever and G. E. Hinton  “Learning multilevel distributed representations for high-dimensional se-

quences ” in AISTATS  2007.

[15] G. E. Hinton and R. Salakhutdinov  “Reducing the dimensionality of data with neural networks ” Science 

vol. 28  pp. 504–507  2006.

[16] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner  “Gradient-based learning applied to document recogni-

tion ” Proceedings of the IEEE  vol. 86  pp. 2278–2324  November 1998.

[17] L. Fei-Fei  R. Fergus  and P. Perona  “Learning generative visual models from few training examples: An

incremental bayesian approach tested on 101 object categories ” CVPR Workshop  2004.

[18] R. Salakhutdinov and G. E. Hinton  “Learning a nonlinear embedding by preserving class neighbourhood

structure ” in AISTATS  2007.

[19] L. Deng and D. Yu  “Deep convex net: A scalable architecture for speech pattern classiﬁcation ” in Inter-

speech  2011.

[20] D. C. Cires¸an  U. Meier  and J. Schmidhuber  “Multi-column deep neural networks for image classiﬁca-

tion ” in CVPR  2012.

[21] D. Lowe  “Object recognition from local scale-invariant features ” in CVPR  1999.
[22] Y. Boureau  F. Bach  Y. LeCun  and J. Ponce  “Learning mid-level features for recognition ” in CVPR 

2010.

[23] H. Goh  N. Thome  M. Cord  and J.-H. Lim  “Unsupervised and supervised visual codes with restricted

Boltzmann machines ” in ECCV  2012.

[24] S. Lazebnik  C. Schmid  and J. Ponce  “Beyond bags of features: Spatial pyramid matching for recogniz-

ing natural scene categories ” in CVPR  2006.

[25] S. Avila  N. Thome  M. Cord  E. Valle  and A. Ara´ujo  “Pooling in image representation:

codeword point of view ” Computer Vision and Image Understanding  pp. 453–465  May 2013.

the visual

[26] C. Theriault  N. Thome  and M. Cord  “Extended coding and pooling in the HMAX model ” IEEE Trans-

action on Image Processing  2013.

[27] K. Sohn  D. Y. Jung  H. Lee  and A. Hero III  “Efﬁcient learning of sparse  distributed  convolutional

feature representations for object recognition ” in ICCV  2011.

[28] K. Sohn  G. Zhou  C. Lee  and H. Lee  “Learning and selecting features jointly with point-wise gated

boltzmann machines ” in ICML  2013.

9

,Hanlin Goh
Nicolas Thome
Matthieu Cord
Joo-Hwee Lim
Anastasia Pentina
Christoph Lampert
Josip Djolonga
Sebastian Tschiatschek
Andreas Krause