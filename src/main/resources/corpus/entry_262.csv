2017,A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control,We propose an alternative framework to existing setups for controlling false alarms when multiple A/B tests are run over time. This setup arises in many practical applications  e.g. when pharmaceutical companies test new treatment options against control pills for different diseases  or when internet companies test their default webpages versus various alternatives over time. Our framework proposes to replace a sequence of A/B tests by a sequence of best-arm MAB instances  which can be continuously monitored by the data scientist. When interleaving the MAB tests with an online false discovery rate (FDR) algorithm  we can obtain the best of both worlds: low sample complexity and any time online FDR control. Our main contributions are: (i) to propose reasonable definitions of a null hypothesis for MAB instances; (ii) to demonstrate how one can derive an always-valid sequential p-value that allows continuous monitoring of each MAB test; and (iii) to show that using rejection thresholds of online-FDR algorithms as the confidence levels for the MAB algorithms results in both sample-optimality  high power and low FDR at any point in time. We run extensive simulations to verify our claims  and also report results on real data collected from the New Yorker Cartoon Caption contest.,A framework for Multi-A(rmed)/B(andit) Testing

with Online FDR Control

Fanny Yang

Dept. of EECS  U.C. Berkeley
fanny-yang@berkeley.edu

Kevin Jamieson

Allen School of CSE  U. of Washington

jamieson@cs.washington.edu

Aaditya Ramdas

Dept. of EECS and Statistics  U.C. Berkeley

ramdas@berkeley.edu

Martin Wainwright

Dept. of EECS and Statistics  U.C. Berkeley

wainwrig@berkeley.edu

Abstract

We propose an alternative framework to existing setups for controlling false alarms
when multiple A/B tests are run over time. This setup arises in many practical
applications  e.g. when pharmaceutical companies test new treatment options
against control pills for different diseases  or when internet companies test their
default webpages versus various alternatives over time. Our framework proposes to
replace a sequence of A/B tests by a sequence of best-arm MAB instances  which
can be continuously monitored by the data scientist. When interleaving the MAB
tests with an online false discovery rate (FDR) algorithm  we can obtain the best of
both worlds: low sample complexity and any time online FDR control. Our main
contributions are: (i) to propose reasonable deﬁnitions of a null hypothesis for
MAB instances; (ii) to demonstrate how one can derive an always-valid sequential
p-value that allows continuous monitoring of each MAB test; and (iii) to show that
using rejection thresholds of online-FDR algorithms as the conﬁdence levels for
the MAB algorithms results in both sample-optimality  high power and low FDR
at any point in time. We run extensive simulations to verify our claims  and also
report results on real data collected from the New Yorker Cartoon Caption contest.

Introduction

1
Randomized trials are the default option to determine whether potential improvements of an alternative
method (e.g. website design for a tech company  or medication in clinical trials for pharmaceutical
companies) are signiﬁcant compared to a well-established default. In the applied domain  this is often
colloquially referred to as A/B testing or A/B/n testing for several alternatives. The standard practice
is to divert a small amount of the trafﬁc or patients to the alternative and control. If an alternative
appears to be signiﬁcantly better  it is implemented; otherwise  the default setting is maintained.
At ﬁrst glance  this procedure seems intuitive and simple. However  in cases where the aim is to
optimize over one particular metric  one can do better. In particular  this common tool suffers from
several downsides. (1) First  one may wish to allocate more trafﬁc to a better treatment if it is clearly
better. Yet typical A/B/n testing frameworks split the trafﬁc uniformly over alternatives. Adaptive
techniques should help to detect better alternatives faster. (2) Second  companies often desire to
continuously monitor an ongoing A/B test as they may adjust their termination criteria as time goes
by and possibly stop earlier or later than originally intended. However  this practice may result in
many more false alarms if not properly accounted for. This is one of the reasons for the lack of
reproducibility of scientiﬁc results  an issue recently receiving increased attention from the public
media. (3) Third  the lack of sufﬁcient evidence or an insigniﬁcant improvement of the metric may
make it undesirable from a practical or ﬁnancial perspective to replace the default. Therefore  when a

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

company runs hundreds to thousands of A/B tests within a year  ideally the number of statistically
insigniﬁcant changes that it made should be small relative to the total number of changes made. While
controlling the false alarm rate of each individual test does not achieve this type of false discovery
rate (FDR) control  there are known procedures in the multiple testing literature that are tailored to
this problem.
In this paper  we provide a novel framework that addresses the above shortcomings of A/B or A/B/n
testing. The ﬁrst concern is tackled by employing recent advances in adaptive sampling like the pure-
exploration multi-armed bandit (MAB) algorithm. For the second concern  we adopt the notion of
any-time p-values for guilt-free continuous monitoring. Finally  we handle the third issue using recent
results in online FDR control. Hence the combined framework can be described as doubly-sequential
(sequences of MAB tests  each of which is itself sequential). Although each of those problems
has been studied in hitherto disparate communities  how to leverage the best of all worlds  if at all
possible  has remained an open problem. The main contributions of this paper are in successfully
merging these ideas in a meta framework and presenting the conditions under which it can be shown
to yield near-optimal sample complexity and FDR control.
The remainder of this paper is organized as follows. In Section 2  we lay out the conceptual challenges
that we address in the paper  and describe a meta-algorithm that combines adaptive sampling strategies
with FDR control procedures. Section 3 is devoted to the description of a concrete procedure  along
with some theoretical guarantees on its properties. In Section 4  we discuss some results of our
extensive experiments on both simulated and real-world data sets available to us.
2 Formal experimental setup and a meta-algorithm
In this section provide a high-level overview of our proposed combined framework aimed at address-
ing the shortcomings mentioned in the introduction. A speciﬁc instantiation of this meta-algorithm
along with detailed theoretical guarantees are speciﬁed in Section 3.
For concreteness  we refer to the system designer  whether a tech company or a pharmaceutical
company  as a (data) scientist. We assume that the scientist needs to possibly conduct an inﬁnite
number of experiments sequentially  indexed by j. Each experiment has one default setting  referred
to as the control  and K = K(j) alternative settings  called the treatments or alternatives. The
scientist must return one of the K + 1 options that is the “best” according to some predeﬁned metric 
before the next experiment is started. Such a setup is a simple mathematical model both for clinical
trials run by pharmaceutical labs  and A/B/n testing used at scale by tech companies.
One full experiment consists of a sequence of steps. In each step  the scientist assigns a new person to
one of the K + 1 options and observes an outcome. In practice  the role of the scientist could be taken
by an adaptive algorithm  which determines the assignment at time step j by careful consideration
of all previous outcomes. Borrowing terminology from the multi-armed bandit (MAB) literature 
we refer to each of the K + 1 options as an arm  and each assignment to arm i is termed “pulling
arm i”. For concreteness  we assign the index 0 to the control arm and note that it is known to the
algorithm. Furthermore  we assume that the observable metric from each pull of arm i = 0  1  . . .   K
corresponds to an independent draw from an unknown probability distribution with expectation µi.
In the sequel we use µi(cid:63) := max
µi to denote the mean of the best arm. We refer the reader to
i=1 ... K
Table 1 in Appendix A for a glossary of the notation used throughout this paper.

2.1 Some desiderata and difﬁculties
Given the setup above  how can we mathematically describe the guarantees that the companies might
desire from an improved multiple-A/B/n testing framework? For which parts can we leverage known
results and what challenges remain?
For the purpose of addressing the ﬁrst question  let us adopt terminology from the hypothesis testing
literature and view each experiment as a test of a null hypothesis. Any claim that an alternative arm is
the best is called a discovery  and when such a claim is erroneous  it is called a false discovery. When
multiple hypotheses are to be tested  the scientist needs to deﬁne the quantity it wants to control.
While we may desire that the probability of even a single false discovery is small  this is usually
far too stringent for a large and unknown number of tests and results in low power. For this reason 
[1] proposed that it may be more useful to control the expected ratio of false discoveries to the total
number of discoveries (called the False Discovery Rate  or FDR for short) or the ratio of expected
number of false discoveries to the expected number of total discoveries (called the modiﬁed FDR

2

or mFDR for short). Over the past decades  the FDR and its variants like the mFDR have become
standard quantities for multiple testing applications. In the following  if not otherwise speciﬁed  we
use the term FDR to denote both measures in order to simplify the presentation. In Section 3  we
show that both mFDR and FDR can be controlled for different choices of procedures.
2.1.1 Challenges in viewing an MAB instance as a hypothesis test
In our setup  we want to be able to control the FDR at any time in an online manner. Online FDR
procedures were ﬁrst introduced by Foster and Stine [2]  and have since been studied by other authors
(e.g.  [3  4]). They are based on comparing a valid p-value P j with carefully-chosen levels αj for
each hypothesis test1. We reject the null hypothesis  represented as Rj = 1  when P j ≤ αj and we
set Rj = 0 otherwise.
As mentioned  we want to use adaptive MAB algorithms to test each hypothesis  since they can ﬁnd a
best arm among K + 1 with near-optimal sample complexity. However the traditional MAB setup
does not account for the asymmetry between the arms as is the case in a testing setup  with one being
the default (control) and others being alternatives (treatments). This is the standard scenario in A/B/n
testing applications  as e.g. a company might prefer wrong claims that the control is the best (false
negative)  rather than wrong claims that an alternative is the best (false positive)  simply because new
system-wide adoption of selected alternatives might involve high costs. What would be a suitable
null hypothesis in this hybrid setting? For the sake of continuous monitoring  is it possible to deﬁne
and compute always-valid p-values that are super-uniformly distributed under the null hypothesis
when computed at any time t?
In addition to asymmetry  the practical scientist might have a different incentive than the ideal outcome
for MAB algorithms as he/she might not want to ﬁnd the best alternative if it is not substantially
better than the control. Indeed  if the net gain is small  it might be offset by the cost of implementing
the change from the existing default choice. By similar reasoning  we may not require identifying
the single best arm if there is a set of arms with similar means all larger than the rest. We propose a
sensible null-hypothesis for each experiment which incorporates the approximation and minimum
improvement requirement as described above  and provide an always valid p-value which can be
easily calculated at each time step in the experiment. We show that a slight modiﬁcation of the usual
LUCB algorithm caters to this speciﬁc null-hypothesis while still maintaining near-optimal sample
complexity.

Figure 1: Diagram of our MAB-FDR meta algorithm. The green solid arrows symbolize interaction
between the MAB and FDR procedures via the FDR test levels αj and rejection indicator variables Rj.
Notice that the P j-values are now dependent as each αj depends on the past rejections R1  . . .   Rj−1.
The eyes represent possible continuous monitoring by the scientist.

2.1.2 Interaction between MAB and FDR
In order to take advantage of the sample efﬁciency of best-arm bandit algorithms  it is crucial to set the
conﬁdence levels close to what is needed. Given a user-deﬁned level α  at each hypothesis j  online

1A valid P j must be stochastically dominated by a uniform distribution on [0  1]  which we henceforth refer

to as super-uniformly distributed.

3

MAB-FDR meta algorithm𝛼𝛼𝑗𝑗𝑅𝑅𝑗𝑗(𝛼𝛼𝑗𝑗)ExpjMABTest𝑝𝑝𝑗𝑗<𝛼𝛼𝑗𝑗𝑝𝑝𝑗𝑗(𝛼𝛼𝑗𝑗)𝛼𝛼j+1𝑅𝑅j+1(𝛼𝛼j+1)Expj+1MABTest𝑝𝑝j+1 <𝛼𝛼j+1𝑝𝑝j+1 (𝛼𝛼j+1)Online FDR procedure……desired FDR level 𝛼𝛼FDR procedures automatically output the signiﬁcance level αj which are sufﬁcient to guarantee FDR
control  based on past decisions. Can we directly set the MAB conﬁdence levels to these output levels
αj? If we do  our p-values are not independent across different hypotheses anymore: P j directly
depends on the FDR levels αj and each αj in turn depends on past MAB rejections  thus on past
MAB p-values (see Figure 1). Does the new interaction compromise FDR guarantees?
Although known procedures as in [2  4] guarantee FDR control for independent p-values  this does
not hold for dependent p-values in general. Hence FDR control guarantees cannot simply be obtained
out of the box. A key insight that emerges from our analysis is that an appropriate bandit algorithm
actually shapes the p-value distribution under the null in a “good” way that allows us to control FDR.
2.2 A meta-algorithm
Procedure 1 summarizes our doubly-sequential procedure  with a corresponding ﬂowchart in Figure 1.
We will prove theoretical guarantees after instantiating the separate modules. Note that our framework
allows the scientist to plug in their favorite best-arm MAB algorithm or online FDR procedure. The
choice for each of them determines which guarantees can be proven for the entire setup. Any
independent improvement in either of the two parts would immediately lead to an overall performance
boost of the overall framework.

Procedure 1 MAB-FDR Meta algorithm skeleton

1. The scientist sets a desired FDR control rate α.
2. For each j = 1  2  . . . :
• Experiment j receives a designated control arm and some number of alternative arms.
• An online-FDR procedure returns an αj that is some function of the past values {P (cid:96)}j−1
(cid:96)=1.
• An MAB procedure is executed with inputs (a) the control arm and K(j) alternative arms 
(b) conﬁdence level αj  maintains an always valid p-value for each t and if the procedure
self-terminates  returns a recommended arm.

• When the MAB procedure is terminated at time t by itself or the user  if the arm with the
t ≤ αj  then we return P j := P j
t  

highest empirical mean is not the control arm and P j
and the control arm is rejected in favor of this empirically best arm.

3 A concrete procedure with guarantees
We now take the high-level road map given in Procedure 1  and show that we can obtain a concrete 
practically implementable framework with FDR control and power guarantees. We ﬁrst discuss the
key modeling decisions we have to make in order to seamlessly embed MAB algorithms into an
online FDR framework. We then outline a modiﬁed version of a commonly used best-arm algorithm 
before we ﬁnally prove FDR and power guarantees for the concrete combined procedure.
3.1 Deﬁning null hypotheses and constructing p-values
Our ﬁrst task is to deﬁne a null hypothesis for each experiment. As mentioned before  the choice of
the null is not immediately obvious  since we sample from multiple distributions adaptively instead
of independently. In particular  we will generally not have the same number of samples for all arms.
i=1  we propose that the null hypothesis for the
Given a default mean µ0 and alternatives means {µi}K
j-th experiment should be deﬁned as

H j
0 : µ0 ≥ µi − 

for all i = 1  . . .   K 

(1)
where we usually omit the index j for simplicity. It remains to deﬁne an always valid p-value
(previously deﬁned by Johari et al. [5]) for each experiment for the purpose of continuous monitoring.
It is deﬁned as a stochastic process {Pt}∞t=1 such that for all ﬁxed and random stopping times T  
under any distribution P0 over the arm rewards such that the null hypothesis is true  we have
(2)
When all arms are drawn independently an equal number of times  by linearity of expectation one can
regard the distance of each pair of samples as a random variable drawn i.i.d. from a distribution with
mean ˜µ := µ0 − µi. We can then view the problem as testing the standard hypothesis H j
0 : ˜µ > −.
However  when the arms are pulled adaptively  a different solution needs to be found—indeed  in this

P0(PT ≤ α) ≤ α.

4

case  the sample means are not unbiased estimators of the true means  since the number of times an
arm was pulled now depends on the empirical means of all the arms.
Our strategy is to construct always valid p-values by using the fact that p-values can be obtained
by inverting conﬁdence intervals. To construct always-valid conﬁdence bounds  we resort to the
fundamental concept of the law of the iterated logarithm (LIL)  for which non-asymptotic versions
have been recently derived and used for both bandits and testing problems (see [6]  [7]).
To elaborate  deﬁne the function

log( 1

δ ) + 3 log(log( 1
δ )) + 3
n

2 log(log(en))

ϕn(δ) =

If(cid:98)µi n is the empirical average of independent samples from a sub-Gaussian distribution  then it is
known (see  for instance  [8  Theorem 8]) that for all δ ∈ (0  1)  we have

.

(cid:110)P(cid:16) ∞(cid:91)

(cid:17)
{(cid:98)µi n − µi > ϕn(δ ∧ 0.1)}

  P(cid:16) ∞(cid:91)

{(cid:98)µi n − µi < −ϕn(δ ∧ 0.1)}

(4)

≤ δ 

(cid:17)(cid:111)

(3)

max

n=1

(cid:115)

n=1

where δ ∧ 0.1 := min{δ  0.1}.
We are now ready to propose single arm p-values of the form

(cid:110)
γ ∈ [0  1] | (cid:98)µi ni(t) − ϕni(t)( γ
(cid:110)
γ ∈ [0  1] | LCBi(t) ≤ UCB0(t) + 

(cid:111)

Pi t : = sup

= sup

2K ) ≤ (cid:98)µ0 n0(t) + ϕn0(t)( γ

2 ) + 

(cid:111)

(5)

Here we set Pi t = 1 if the supremum is taken over an empty set. Given these single arm p-values 
the always-valid p-value for the experiment is deﬁned as

Pt := min
s≤t

min

i=1 ... K

Pi s.

(6)

i=0 1 ... K

MAB algorithms can identify i(cid:63) with probability at least 1−δ based on at most2(cid:80)

We claim that this procedure leads to an always valid p-value (with proof in Appendix C).
Proposition 1. The sequence {Pt}∞t=1 deﬁned via equation (6) is an always valid p-value.
3.2 Adaptive sampling for best-arm identiﬁcation
In the traditional A/B testing setting described in the introduction  samples are allocated uniformly
to the different alternatives. But by allowing adaptivity  decisions can be made with the same
statistical signiﬁcance using far fewer samples. Suppose moreover that there is a unique maximizer
µi  so that ∆i := µi(cid:63) − µi > 0 for all i (cid:54)= i(cid:63). Then for any δ ∈ (0  1)  best-arm
i(cid:63) := arg max
log(1/δ)
total samples (see the paper [9] for a brief survey and [10] for an application to clinical trials). In
contrast  if samples are allocated uniformly to the alternatives under the same conditions  then the
most natural procedures require K max
log(K/δ) samples before returning i(cid:63) with probability
i(cid:54)=i(cid:63)
at least 1 − δ.
However  standard best-arm bandit algorithms do not incorporate asymmetry as induced by null-
hypotheses as in deﬁnition (1) by default. Furthermore  recall that a practical scientist might desire
the ability to incorporate approximation and a minimum improvement requirement. More precisely 
it is natural to consider the requirement that the returned arm ib satisﬁes the bounds µib ≥ µ0 + 
and µib ≥ µi(cid:63) −  for some  > 0. In Algorithm 1 we present a modiﬁed MAB algorithm based on
the common LUCB algorithm (see [11  12]) which incorporates the above desiderata. We provide a
visualization of how  affects the usual stopping condition in Figure 4 in Appendix A.1.
The following proposition applies to Algorithm 1 run with a control arm indexed by i = 0 with mean
µ0 and alternative arms indexed by i = 1  . . .   K with means µi  respectively. Let ib denote the
random arm returned by the algorithm assuming that it exits  and deﬁne the set
and µi(cid:63) > µ0 + }.

S (cid:63) := {i(cid:63) (cid:54)= 0 | µi(cid:63) ≥ max

µi − 

∆−2

i

∆−2

i

i=1 ... K

i(cid:54)=i(cid:63)

(7)

2Here we have ignored some doubly-logarithmic factors.

5

(cid:80)ni(t)

Algorithm 1 Best-arm identiﬁcation with a control arm for conﬁdence δ and precision  ≥ 0
i let(cid:98)µi(t) = 1
For all t let ni(t) be the number of times arm i has been pulled up to time t. In addition  for each arm
UCBi(t) :=(cid:98)µi ni(t) + ϕni(t)( δ

LCBi(t) :=(cid:98)µi ni(t) − ϕni(t)( δ

τ =1 ri(τ )  deﬁne

2K )

and

2 ).

ni(t)

1. Set t = 1 and sample every arm once.
2. Repeat: Compute ht = arg max

i=0 1 ... K(cid:98)µi(t)  and (cid:96)t = arg

max

UCBi(t)

i=0 1 ... K i(cid:54)=ht
(a) If LCB0(t) > UCBi(t) −   for all i (cid:54)= 0  then output 0 and terminate.

Else if LCBht(t) > UCB(cid:96)t(t) −  and LCBht(t) > UCB0(t) +   then output ht and
terminate.
(b) If  > 0  let ut = arg maxi(cid:54)=0 UCBi(t) and pull all distinct arms in {0  ut  ht  (cid:96)t} once.

If  = 0  pull arms ht and (cid:96)t and set t = t + 1.

Note that the mean associated with any index i(cid:63) ∈ S (cid:63)  assuming that the set is non-empty  is
guaranteed to be -superior to the control mean  and at most -inferior to the maximum mean over all
arms.
Proposition 2. The algorithm 1 terminates in ﬁnite time with probability one. Furthermore  suppose
that the samples from each arm are independent and sub-Gaussian with scale 1. Then for any
δ ∈ (0  1) and  ≥ 0  Algorithm 1 has the following guarantees:
(cid:16)(cid:80)K
i=0(cid:101)∆−2
log(K log((cid:101)∆−2
(a) Suppose that µ0 > max
i=1 ... K
(cid:101)∆0 = (µ0 + ) − max
(cid:101)∆i = (µ0 + ) − µi.
(cid:17)

µi − . Then with probability at least 1 − δ  the algorithm exits with
time steps with effective gaps

(b) Otherwise  suppose that the set S (cid:63) as deﬁned in equation (7) is non-empty. Then with probability

at least 1 − δ  the algorithm exits with ib ∈ S (cid:63) after taking at most
time steps with effective gaps
O

ib = 0 after taking at most O

(cid:16)(cid:80)K
i=0(cid:101)∆−2

µj and

j=1 ... K

(cid:17)

)/δ)

)/δ)

i

i

i

i

log(K log((cid:101)∆−2
(cid:26)
(cid:101)∆0 = min
(cid:26)
(cid:101)∆i = max

(cid:26)

max

j=1 ... K

µj − (µ0 + )  max{∆0  }

and

∆i  min

max

j=1 ... K

µj − (µ0 + )  

.

(cid:27)
(cid:27)(cid:27)

See Appendix D for the proof of this claim. Part (a) of Proposition 2 guarantees that when no
alternative arm is -superior to the control arm (i.e. under the null hypothesis)  the algorithm stops
and returns the control arm with probability at least 1 − δ. Part (b) guarantees that if there is in fact at
least one alternative that is -superior to the control arm (i.e. under the alternative)  then the algorithm
will ﬁnd at least one of them that is at most -inferior to the best of all possible arms.
As our algorithm is a slight modiﬁcation of the LUCB algorithm  the results of [11  12] pro-
vide insight into the number of samples taken before the algorithm terminates.
Indeed  when
 = 0 and i(cid:63) = arg maxi=0 1 ... K µi is a unique maximizer  the nearly optimal sample complex-
ity result of [12] implies that the algorithm terminates under settings (a) and (b) after at most
)/δ) samples are taken (ignoring con-
maxj(cid:54)=i(cid:63) ∆−2
log(K log(∆−2
stants)  where ∆i = µi(cid:63) − µi.
In our development to follow  we now bring back the index for experiment j  in particular using P j
to denote the quantity P j
T at any stopping time T . Here the stopping time can either be deﬁned by the
scientist  or in an algorithmic manner.

j )/δ) +(cid:80)

log(log(∆−2

∆−2

i(cid:54)=i(cid:63)

j

i

i

6

3.3 Best-arm MAB interacting with online FDR
After having established null hypotheses and p-values in the context of best-arm MAB algorithms  we
are now ready to embed them into an online FDR procedure. In the following  we consider p-values
for the j-th experiment P j := P j
which is just the p-value as deﬁned in equation (6) at the stopping
Tj
time Tj  which depends on αj.
We denote the set of true null and false null hypotheses up to experiment J as H0(J) and H1(J)
respectively  where we drop the argument whenever it’s clear from the context. The variable
P j≤αj indicates whether a the null hypothesis of experiment j has been rejected  where
Rj = 1
Rj = 1 denotes a claimed discovery that an alternative was better than the control. The false
discovery rate (FDR) and modiﬁed FDR up to experiment J are then deﬁned as

FDR(J) := E

and

mFDR(J) :=

.

(8)

E(cid:80)
E(cid:80)J

j∈H0 Rj
i=1 Ri + 1

(cid:80)
(cid:80)J

j∈H0 Rj
i=1 Ri ∨ 1

(cid:80)J

j=1

7

Here the expectations are taken with respect to distributions of the arm pulls and the respective
sampling algorithm. In general  it is not true that control of one quantity implies control of the other.
Nevertheless  in the long run (when the law of large numbers is a good approximation)  one does not
expect a major difference between the two quantities in practice.
The set of true nulls H0 thus includes all experiments where H j
0 is true  and the FDR and mFDR are
well-deﬁned for any number of experiments J  since we often desire to control FDR(J) or mFDR(J)
for all J ∈ N. In order to measure power  we deﬁne the -best-arm discovery rate as

BDR(J) :=

j∈H1 Rj 1µib≥µi(cid:63)−1µib≥µ0+

(9)

E(cid:80)

|H1(J)|

We provide a concrete procedure 2 for our doubly sequential framework  where we use a particular
online FDR algorithm due to Javanmard and Montanari [4] known as LORD; the reader should note
that other online FDR procedure could be used to obtain essentially the same set of guarantees. Given
a desired level α  the LORD procedure starts off with an initial “α-wealth” of W (0) < α. Based on
a iniﬁnite sequence {γi}∞i=1 that sums to one  and the time of the most recent discovery τj  it uses up
a fraction γj−τj of the remaining α-wealth to test. Whenever there is a rejection  we increase the
α-wealth by α − W (0). A feasible choice for a stopping time in practice is Tj := min{T (αj)  TS} 
where TS is a maximal number of samples the scientist wants to pull and T (αj) is the stopping time
of the best-arm MAB algorithm run at conﬁdence αj.

Procedure 2 MAB-LORD: best-arm identiﬁcation with online FDR control

1. Initialize W (0) < α  set τ0 = 0  and choose a sequence {γi} s.t.(cid:80)∞i=1 γi = 1

W (j + 1) = W (j) − αj + Rj(α − W (0))

2. At each step j  compute αj = γj−τj W (τj) and
3. Output αj and run Algorithm 1 using αj-conﬁdence and stop at a stopping time Tj.
4. Algorithm 1 returns P j and we reject the null hypothesis if P j ≤ αj.
5. Set Rj = 1

P j≤αj   τj = τj−1 ∨ jRj  update j = j + 1 and go back to step 2.

The following theorem provides guarantees on mFDR and power for the MAB-LORD procedure.
Theorem 1 (Online mFDR control for MAB-LORD).
(a) Procedure 2 achieves mFDR control at level α for stopping times Tj = min{T (αj)  TS}.
(b) Furthermore  if we set TS = ∞  Procedure 2 satisﬁes

BDR(J) ≥

1j∈H1 (1 − αj)
|H1(J)|

.

(10)

See Appendix E for the proof of this claim. Note that by the arguments in the proof of Theorem 1 
mFDR control itself is actually guaranteed for any generalized α-investing procedure [3] combined
with any best-arm MAB algorithm. In fact we could use any adaptive stopping time Tj which depend
on the history only via the rejections R1  . . .   Rj−1. Furthermore  using a modiﬁed LORD proposed

by Javanmard and Montanari [13]  we can also guarantee FDR control– a result we moved to the
Appendix F due to space constraints. It is noteworthy that small values of α do not only guarantee
smaller FDR error but also higher BDR. However  there is no free lunch — a smaller α implies a
smaller αj at each experiment  resulting in a larger required number of pulls for the the best-arm
MAB algorithm.
4 Experimental results
In the following  we brieﬂy describe some results of our experiments3 on both simulated and real-
world data sets  which illustrate that  apart from FDR control  MAB-FDR (used interchangeably with
MAB-LORD here) is highly advantageous in terms of sample complexity and power compared to a
straightforward embedding of A/B testing in online FDR procedures. Unless otherwise noted  we set
 = 0 in all of our simulations to focus on the main ideas and keep the discussion concise.
Competing procedures There are two natural frameworks to compare against MAB-FDR. The
ﬁrst  called AB-FDR or AB-LORD  swaps the MAB part for an A/B (i.e. A/B/n) test (uniformly
sampling all alternatives until termination). The second comparator exchanges the online FDR
control for independent testing at α for all hypotheses – we call this MAB-IND. Formally  AB-FDR
swaps step 3 in Procedure 2 with “Output αj and uniformly sample each arm until stopping time Tj.”
while MAB-IND swaps step 4 in Procedure 2 with “The algorithm returns P j and we reject the null
hypothesis if P j ≤ α.”. In order to compare the performances of these procedures  we ran three sets
of simulations using Procedure 2 with  = 0 and γj = 0.07 log(j∨2)
Our experiments are run on artiﬁcial data with Gaussian/Bernoulli draws and real-world Bernoulli
draws from the New Yorker Cartoon Caption Contest. Recall that the sample complexity of the
best-arm MAB algorithm is determined by the gaps ∆j = µi(cid:63) − µj. One of the main relevant
differences to consider between an experiment of artiﬁcial or real-world nature is thus the distribution
of the means µi for i = 1  . . .   K. The artiﬁcial data simulations are run with a ﬁxed gap ∆ := ∆2
while the means of the other arms are set uniformly in [0  µi(cid:63) − ∆]. For our real-world simulations 
we use empirical means computed from the cartoon caption contest (see details in Appendix B.1.1).
In addition  the contests actually follow a natural chronological order  which makes this dataset highly
relevant to our purposes. In all simulations  60% of all the hypotheses are true nulls  and their indices
are chosen uniformly. Due to space constraints  the experimental results for artiﬁcial and real-world
Bernoulli draws are deferred to Appendix B.

log j as in [4].
√

je

(a)

(b)

Figure 2: (a) Power vs. truncation time TS (per hypothesis) for 50 arms and (b) Sample complexity
vs. # arms for truncation time TS = 300 for Gaussian draws with ﬁxed µi(cid:63) = 8  ∆ = 3 over 500
hypotheses with 200 non-nulls  averaged over 100 runs and α = 0.1.
Power and sample complexity In this section we include ﬁgures on artiﬁcial Gaussian trials which
conﬁrm that the total number of necessary pulls to determine signiﬁcance is much smaller for MAB-
FDR than for AB-FDR. In Fig. 2 (a) we ﬁx the number of arms and plot the BDR with  = 0 (BDR
for short) for both procedures over different choices of truncation times TS. Low BDR indicates that
the algorithm often reaches truncation time before it could stop. For Fig. 2 (b) we ﬁx TS and show
how the sample complexity varies with the number of arms.

3The code for

https://github.com/fanny-yang/MABFDR

reproducing all experiments and plots in this paper

is publicly available at

8

100200300400500600700800Truncation time TS0.00.20.40.60.81.0BDRMAB-LORDAB-LORD20406080100120Number of arms020406080100120140160Total number of samples /1000MAB-LORDAB-LORDObserve in Fig. 2 (a) that the power at any given truncation time is much higher for MAB-FDR than
AB-FDR. This is because the best-arm MAB is more likely to satisfy the stopping criterion before
any given truncation time than the uniform sampling algorithm. Fig. 2(b) qualitatively shows how
the total number of necessary arm pulls for AB-FDR increases much faster with the number of arms
than for MAB-FDR before it plateaus due to the truncation. Recall that whenever the best-arm MAB
stops before the truncation time in each hypothesis  the stopping criterion is met  i.e. the best arm is
identiﬁed with probability at least 1 − αj  so that the power is bound to be close to one whenever
Tj = T (αj).
mFDR control For Fig. 3  we again consider Gaussian draws as in Fig. 2. This time however  for
each true null hypothesis we skip the bandit experiment and directly draw P j ∼ [0  1] to compare
with the signiﬁcance levels αj from our online FDR procedure 2 (see App. B.2 for motivation of this
setting). By Theorem 1  mFDR should still be controlled as it only requires the p-values to be super-
j∈H0J Rj
uniform. In Fig. 3(a) we plot the instantaneous false discovery proportion FDP(J) =
j=1 Rj
over the hypothesis index for different runs with the same settings. Apart from initial ﬂuctuations due
to the relatively small denominator  observe how the guarantee for the FDR(J) = E FDP(J) with
the red line showing its empirical value  transfers to the control of each individual run (blue lines).

(cid:80)
(cid:80)T

(a)

(b)

π2j2 such that(cid:80)∞j=1 αj ≤ α

Figure 3: (a) Single runs of MAB-LORD (blue) and their average (red) with uniformly drawn p-values
for null hypotheses and Gaussian draws as in Figure 2. (b) mFDR over different proportions of
non-nulls π1  with same settings  averaged over 80 runs.
In Figure 3 (b)  we compare the mFDR of MAB-FDR against MAB-IND and a Bonferroni type
correction. The latter uses a simple union bound and chooses αj = 6α
and thus trivially allows for any time FWER  implying FDR control. As expected  Bonferroni is too
conservative and barely makes any rejections whereas the naive MAB-IND approach does not control
FDR. LORD avoids both extremes and controls FDR while having reasonable power.
5 Discussion
The recent focus in popular media about the lack of reproducibility of scientiﬁc results erodes the
public’s conﬁdence in published scientiﬁc research. To maintain credibility of claimed discoveries 
simply decreasing the statistical signiﬁcance levels α of each individual experimental work (e.g. 
reject at level 0.001 rather than 0.05) would drastically hurt power. A common approach is instead
to control the ratio of false discoveries to claimed discoveries at some desired value over many
sequential experiments  requiring the statistical signiﬁcances αj to change from experiment to
experiment. Unlike earlier works on online FDR control  our framework synchronously interacts
with adaptive sampling methods like MABs to make the overall sampling procedure per experiment
much more efﬁcient than uniform sampling. To the best of our knowledge  it is the ﬁrst work that
successfully combines the beneﬁts of adaptive sampling and FDR control. It is worthwhile to note that
any improvement  theoretical or practical  to either online FDR algorithms or best-arm identiﬁcation
in MAB  immediately results in a corresponding improvement for our MAB-FDR framework.
More general notions of FDR with corresponding online procedures have recently been developed by
Ramdas et al [14]. In particular  they incorporate the notion of memory and a priori importance of
each hypothesis. This could prove to be a valuable extension for our setting  especially in cases when
only the percentage of wrong rejections in the recent past matters. It would be useful to establish
FDR control for these generalized notions of FDR as well.

9

0.10.30.50.70.9Proportion of alternatives π10.00.10.20.30.40.5mFDRMAB-LORDMAB-INDMAB-Bonf.Acknowledgements

This work was partially supported by Ofﬁce of Naval Research MURI grant DOD-002888  Air Force
Ofﬁce of Scientiﬁc Research Grant AFOSR-FA9550-14-1-001  and National Science Foundation
Grants CIF-31712-23800 and DMS-1309356.

References
[1] Y. Benjamini and Y. Hochberg  “Controlling the false discovery rate: a practical and powerful
approach to multiple testing ” Journal of the Royal Statistical Society. Series B (Methodological) 
pp. 289–300  1995.

[2] D. P. Foster and R. A. Stine  “α-investing: a procedure for sequential control of expected false
discoveries ” Journal of the Royal Statistical Society: Series B (Statistical Methodology)  vol. 70 
no. 2  pp. 429–444  2008.

[3] E. Aharoni and S. Rosset  “Generalized α-investing: deﬁnitions  optimality results and ap-
plication to public databases ” Journal of the Royal Statistical Society: Series B (Statistical
Methodology)  vol. 76  no. 4  pp. 771–794  2014.

[4] A. Javanmard and A. Montanari  “Online rules for control of false discovery rate and false

discovery exceedance ” The Annals of Statistics  2017.

[5] R. Johari  L. Pekelis  and D. J. Walsh  “Always valid inference: Bringing sequential analysis to

A/B testing ” arXiv preprint arXiv:1512.04922  2015.

[6] K. G. Jamieson  M. Malloy  R. D. Nowak  and S. Bubeck  “lil’ucb: An optimal exploration

algorithm for multi-armed bandits ” in COLT  vol. 35  2014  pp. 423–439.

[7] A. Balsubramani and A. Ramdas  “Sequential nonparametric testing with the law of the iter-
ated logarithm ” in Proceedings of the Thirty-Second Conference on Uncertainty in Artiﬁcial
Intelligence. AUAI Press  2016  pp. 42–51.

[8] E. Kaufmann  O. Cappé  and A. Garivier  “On the complexity of best arm identiﬁcation in

multi-armed bandit models ” The Journal of Machine Learning Research  2015.

[9] K. Jamieson and R. Nowak  “Best-arm identiﬁcation algorithms for multi-armed bandits in
the ﬁxed conﬁdence setting ” in Information Sciences and Systems (CISS)  2014 48th Annual
Conference on.

IEEE  2014  pp. 1–6.

[10] S. S. Villar  J. Bowden  and J. Wason  “Multi-armed bandit models for the optimal design of
clinical trials: beneﬁts and challenges ” Statistical science: a review journal of the Institute of
Mathematical Statistics  vol. 30  no. 2  p. 199  2015.

[11] S. Kalyanakrishnan  A. Tewari  P. Auer  and P. Stone  “Pac subset selection in stochastic multi-
armed bandits ” in Proceedings of the 29th International Conference on Machine Learning
(ICML-12)  2012  pp. 655–662.

[12] M. Simchowitz  K. Jamieson  and B. Recht  “The simulator: Understanding adaptive sampling

in the moderate-conﬁdence regime ” arXiv preprint arXiv:1702.05186  2017.

[13] A. Javanmard and A. Montanari  “On online control of false discovery rate ” arXiv preprint

arXiv:1502.06197  2015.

[14] A. Ramdas  F. Yang  M. J. Wainwright  and M. I. Jordan  “Online control of the false discovery
rate with decaying memory ” in Advances in Neural Information Processing Systems (NIPS)
2017  arXiv preprint arXiv:1710.00499  2017.

10

,Fanny Yang
Aaditya Ramdas
Kevin Jamieson
Martin Wainwright