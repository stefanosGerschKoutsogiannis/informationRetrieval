2012,Multiple Operator-valued Kernel Learning,Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels  and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces.,Multiple Operator-valued Kernel Learning

Hachem Kadri

LIF - CNRS / INRIA Lille - Sequel Project

Universit´e Aix-Marseille

Marseille  France

Alain Rakotomamonjy

LITIS EA 4108

Universit´e de Rouen

St Etienne du Rouvray  France

hachem.kadri@lif.univ-mrs.fr

alain.rakotomamony@insa-rouen.fr

Francis Bach

INRIA - Sierra Project

Ecole Normale Sup´erieure

Paris  France

francis.bach@inria.fr

Philippe Preux

INRIA Lille - Sequel Project

LIFL - CNRS  Universit´e de Lille

Villeneuve d’Ascq  France

philippe.preux@inria.fr

Abstract

Positive deﬁnite operator-valued kernels generalize the well-known notion of
reproducing kernels  and are naturally adapted to multi-output learning situa-
tions. This paper addresses the problem of learning a ﬁnite linear combination
of inﬁnite-dimensional operator-valued kernels which are suitable for extending
functional data analysis methods to nonlinear contexts. We study this problem in
the case of kernel ridge regression for functional responses with an (cid:96)r-norm con-
straint on the combination coefﬁcients (r ≥ 1). The resulting optimization prob-
lem is more involved than those of multiple scalar-valued kernel learning since
operator-valued kernels pose more technical and theoretical issues. We propose a
multiple operator-valued kernel learning algorithm based on solving a system of
linear operator equations by using a block coordinate-descent procedure. We ex-
perimentally validate our approach on a functional regression task in the context
of ﬁnger movement prediction in brain-computer interfaces.

1

Introduction

During the past decades  a large number of algorithms have been proposed to deal with learning
problems in the case of single-valued functions (e.g.  binary-output function for classiﬁcation or real
output for regression). Recently  there has been considerable interest in estimating vector-valued
functions [21  5  7]. Much of this interest has arisen from the need to learn tasks where the target is
a complex entity  not a scalar variable. Typical learning situations include multi-task learning [11] 
functional regression [12]  and structured output prediction [4].
In this paper  we are interested in the problem of functional regression with functional responses in
the context of brain-computer interface (BCI) design. More precisely  we are interested in ﬁnger
movement prediction from electrocorticographic signals [23]. Indeed  from a set of signals measur-
ing brain surface electrical activity on d channels during a given period of time  we want to predict 
for any instant of that period whether a ﬁnger is moving or not and the amplitude of the ﬁnger ﬂex-
ion. Formally  the problem consists in learning a functional dependency between a set of d signals
and a sequence of labels (a step function indicating whether a ﬁnger is moving or not) and between
the same set of signals and vector of real values (the amplitude function). While  it is clear that this
problem can be formalized as functional regression problem  from our point of view  such problem
can beneﬁt from the multiple operator-valued kernel learning framework. Indeed  for these prob-
lems  one of the difﬁculties arises from the unknown latency between the signal related to the ﬁnger

1

movement and the actual movement [23]. Hence  instead of ﬁxing in advance some value for this
latency in the regression model  our framework allows to learn it from the data by means of several
operator-valued kernels.
If we wish to address functional regression problem in the principled framework of reproducing
kernel Hilbert spaces (RKHS)  we have to consider RKHSs whose elements are operators that map
a function to another function space  possibly source and target function spaces being different.
Working in such RKHSs  we are able to draw on the important core of work that has been per-
formed on scalar-valued and vector-valued RKHSs [28  21]. Such a functional RKHS framework
and associated operator-valued kernels have been introduced recently [12  13]. A basic question
with reproducing kernels is how to build these kernels and what is the optimal kernel choice for a
given application. In order to overcome the need for choosing a kernel before the learning process 
several works have tried to address the problem of learning the scalar-valued kernel jointly with
the decision function [18  29]. Since these seminal works  many efforts have been carried out in
order to theoretically analyze the kernel learning framework [9  3] or in order to provide efﬁcient
algorithms [24  1  15]. While many works have been devoted to multiple scalar-valued kernel learn-
ing  this problem of kernel learning have been barely investigated for operator-valued kernels. One
motivation of this work is to bridge the gap between multiple kernel learning (MKL) and operator-
valued kernels by proposing a framework and an algorithm for learning a ﬁnite linear combination
of operator-valued kernels. While each step of the scalar-valued MKL framework can be extended
without major difﬁculties to operator-valued kernels  technical challenges arise at all stages because
we deal with inﬁnite dimensional spaces. It should be pointed out that in a recent work [10]  the
problem of learning the output kernel was formulated as an optimization problem over the cone
of positive semideﬁnite matrices  and a block-coordinate descent method was proposed to solve it.
However  they did not focus on learning the input kernel. In contrast  our multiple operator-valued
kernel learning formulation can be seen as a way of learning simultaneously input and output ker-
nels  although we consider a linear combination of kernels that are ﬁxed in advance.
In this paper  we make the following contributions: 1) we introduce a novel approach to inﬁnite-
dimensional multiple operator-valued kernel learning (MovKL) suitable for learning the functional
dependencies and interactions between continuous data; 2) we extend the original formulation of
ridge regression in dual variables to the functional data analysis domain  showing how to perform
nonlinear functional regression with functional responses by constructing a linear regression opera-
tor in an operator-valued kernel feature space (Section 2); 3) we derive a dual form of the MovKL
problem with functional ridge regression  and show that a solution of the related optimization prob-
lem exists (Section 2); 4) we propose a block-coordinate descent algorithm to solve the MovKL
optimization problem which involves solving a challenging linear system with a sum of block op-
erator matrices (Section 3); 5) we provide an empirical evaluation of MovKL performance which
demonstrates its effectiveness on a BCI dataset (Section 4).

2 Problem Setting

Before describing the multiple operator-valued kernel learning algorithm that we will study and ex-
periment with in this paper  we ﬁrst review notions and properties of reproducing kernel Hilbert
spaces with operator-valued kernels  show their connection to learning from multiple response
data (multiple outputs; see [21] for discrete data and [12] for continuous data)  and describe the
optimization problem for learning kernels with functional response ridge regression.

2.1 Notations and Preliminaries

We start by some standard notations and deﬁnitions used all along the paper. Given a Hilbert
space H  (cid:104)· ·(cid:105)H and (cid:107) · (cid:107)H refer to its inner product and norm  respectively. We denote by Gx
and Gy the separable real Hilbert spaces of input and output functional data  respectively. In func-
tional data analysis domain  continuous data are generally assumed to belong to the space of square
integrable functions L2. In this work  we consider that Gx and Gy are the Hilbert space L2(Ω) which
consists of all equivalence classes of square integrable functions on a ﬁnite set Ω. Ω being poten-
tially different for Gx and Gy. We denote by F(Gx Gy) the vector space of functions f : Gx −→ Gy 
and by L(Gy) the set of bounded linear operators from Gy to Gy.

2

n(cid:88)

i=1

We consider the problem of estimating a function f such that f (xi) = yi when observed functional
data (xi  yi)i=1 ... n ∈ (Gx Gy). Since Gx and Gy are spaces of functions  the problem can be thought
of as an operator estimation problem  where the desired operator maps a Hilbert space of factors to
a Hilbert space of targets. We can deﬁne the regularized operator estimate of f ∈ F as:

fλ (cid:44) arg min
f∈F

1
n

(cid:107)yi − f (xi)(cid:107)2Gy

+ λ(cid:107)f(cid:107)2F .

(1)

In this work  we are looking for a solution to this minimization problem in a function-valued repro-
ducing kernel Hilbert space F. More precisely  we mainly focus on the RKHS F whose elements
are continuous linear operators on Gx with values in Gy. The continuity property is obtained by
considering a special class of reproducing kernels called Mercer kernels [7  Proposition 2.2]. Note
that in this case  F is separable since Gx and Gy are separable [6  Corollary 5.2].
We now introduce (function) Gy-valued reproducing kernel Hilbert spaces and show the correspon-
dence between such spaces and positive deﬁnite (operator) L(Gy)-valued kernels. These extend the
traditional properties of scalar-valued kernels.

Deﬁnition 1 (function-valued RKHS)
A Hilbert space F of functions from Gx to Gy is called a reproducing kernel Hilbert space if there is
a positive deﬁnite L(Gy)-valued kernel KF (w  z) on Gx × Gx such that:
i. the function z (cid:55)−→ KF (w  z)g belongs to F  ∀z ∈ Gx  w ∈ Gx  g ∈ Gy 
ii. ∀f ∈ F  w ∈ Gx  g ∈ Gy  (cid:104)f  KF (w ·)g(cid:105)F = (cid:104)f (w)  g(cid:105)Gy
Deﬁnition 2 (operator-valued kernel)
An L(Gy)-valued kernel KF (w  z) on Gx is a function KF (· ·) : Gx ×Gx −→ L(Gy); furthermore:
i. KF is Hermitian if KF (w  z) = KF (z  w)∗  where ∗ denotes the adjoint operator 
ii. KF is positive deﬁnite on Gx if it is Hermitian and for every natural number r and all

{(wi  ui)i=1 ... r} ∈ Gx × Gy (cid:80)

i j(cid:104)KF (wi  wj)uj  ui(cid:105)Gy ≥ 0.

(reproducing property).

Theorem 1 (bijection between function-valued RKHS and operator-valued kernel)
An L(Gy)-valued kernel KF (w  z) on Gx is the reproducing kernel of some Hilbert space F  if and
only if it is positive deﬁnite.

The proof of Theorem 1 can be found in [21]. For further reading on operator-valued kernels and
their associated RKHSs  see  e.g.  [5  6  7].

2.2 Functional Response Ridge Regression in Dual Variables

We can write the ridge regression with functional responses optimization problem (1) as follows:

n(cid:88)

i=1

1

1
2

(cid:107)f(cid:107)2F +

min
f∈F
with ξi = yi − f (xi).

2nλ

(cid:107)ξi(cid:107)2Gy

(2)

Now  we introduce the Lagrange multipliers αi  i = 1  . . .   n which are functional variables since
the output space is the space of functions Gy. For the optimization problem (2)  the Lagrangian
multipliers exist and the Lagrangian function is well deﬁned. The method of Lagrange multipliers on
Banach spaces  which is a generalization of the classical (ﬁnite-dimensional) Lagrange multipliers
method suitable to solve certain inﬁnite-dimensional constrained optimization problems  is applied
here. For more details  see [16]. Let α = (αi)i=1 ... n ∈ Gn
y the vector of functions containing the
Lagrange multipliers  the Lagrangian function is deﬁned as

L(f  α  ξ) =
where α = (α1  . . .   αn) ∈ Gn
ξ = (ξ1  . . .   ξn) ∈ Gn

y   and ∀a  b ∈ Gn

y   (cid:104)a  b(cid:105)Gn

y

(cid:104)ai  bi(cid:105)Gy.

1

(cid:107)f(cid:107)2F +

1
2
y   y = (y1  . . .   yn) ∈ Gn

2nλ

+ (cid:104)α  y − f (x) − ξ(cid:105)Gn

(3)
y   f (x) = (f (x1)  . . .   f (xn)) ∈ Gn
y  

 

y

y

(cid:107)ξ(cid:107)2Gn
n(cid:80)

=

i=1

3

Differentiating (3) with respect to f ∈ F and setting to zero  we obtain

f (.) =

K(xi  .)αi 

(4)

where K : Gx × Gx −→ L(Gy) is the operator-valued kernel of F.
Substituting this into (3) and minimizing with respect to ξ  we obtain the dual of the functional
response kernel ridge regression (KRR) problem
− 1
2

(cid:104)Kα  α(cid:105)Gn

+ (cid:104)α  y(cid:105)Gn

− nλ
2

(cid:107)α(cid:107)2Gn

max

(5)

α

 

y

y

y

where K = [K(xi  xj)]n
ing the dual formulation of functional KRR are derived in Appendix B of [14].

i j=1 is the block operator kernel matrix. The computational details regrad-

M(cid:88)

n(cid:88)

2.3 MovKL in Dual Variables
Let us now consider that the function f (·) is sum of M functions {fk(·)}M
k=1 where each fk belongs
to a Gy-valued RKHS with kernel Kk(· ·). Similarly to scalar-valued multiple kernel learning  we
can cast the problem of learning these functions fk as

with ξi = yi −(cid:80)M
where d = [d1 ···   dM ]  D = {d : ∀k  dk ≥ 0 and(cid:80)

d∈D min
min
fk∈Fk

i=1
k=1 fk(xi) 
k dr

2nλ

k=1

+

1

(cid:107)fk(cid:107)2Fk
2dk

k ≤ 1} and 1 ≤ r ≤ ∞. Note that this
problem can equivalently be rewritten as an unconstrained optimization problem. Before deriving
the dual of this problem  it can be shown by means of the generalized Weierstrass theorem [17] that
this problem admits a solution. We report the proof in Appendix A of [14].
Now  following the lines of [24]  a dualization of this problem leads to the following equivalent one

(cid:107)ξi(cid:107)2Gy

(6)

d∈D max
min
α∈Gn

y

− nλ
2

(cid:107)α(cid:107)2Gn

y

− 1
2

(cid:104)Kα  α(cid:105)Gn

y

+ (cid:104)α  y(cid:105)Gn

y

 

(7)

n(cid:88)

i=1

M(cid:80)

dkKk and Kk is the block operator kernel matrix associated to the operator-valued

where K =
kernel Kk. The KKT conditions also state that at optimality we have fk(·) =

dkKk(xi ·)αi.

k=1

3 Solving the MovKL Problem

After having presented the framework  we now devise an algorithm for solving this MovKL problem.

3.1 Block-coordinate descent algorithm

i=1

n(cid:80)

Since the optimization problem (6) has the same structure as a multiple scalar-valued kernel learning
problem  we can build our MovKL algorithm upon the MKL literature. Hence  we propose to
borrow from [15]  and consider a block-coordinate descent method. The convergence of a block
coordinate descent algorithm which is related closely to the Gauss-Seidel method was studied in
works of [30] and others. The difference here is that we have operators and block operator matrices
rather than matrices and block matrices  but this doesn’t increase the complexity if the inverse of
the operators are computable (typically analytically or by spectral decomposition). Our algorithm
iteratively solves the problem with respect to α with d being ﬁxed and then with respect to d with α
being ﬁxed (see Algorithm 1). After having initialized {dk} to non-zero values  this boils down to
the following steps :
1. with {dk} ﬁxed  the resulting optimization problem with respect to α has the following

form:

(K + λI)α = y 

(8)

4

where K =(cid:80)M

k=1 dkKk. While the form of solution is rather simple  solving this linear
system is still challenging in the operator setting and we propose below an algorithm for its
resolution.

2. with {fk} ﬁxed  according to problem (6)  we can rewrite the problem as

(cid:107)fk(cid:107)2Fk
dk

min
d∈D

M(cid:88)

k=1

((cid:80)

dk =

r+1

(cid:107)fk(cid:107) 2
k (cid:107)fk(cid:107) 2r

.

r+1 )1/r

(9)

(10)

which has a closed-form solution and for which optimality occurs at [20]:

This algorithm is similar to that of [8] and [15] both being based on alternating optimization. The
difference here is that we have to solve a linear system involving a block-operator kernel matrix
which is a combination of basic kernel matrices associated to M operator-valued kernels. This
makes the system very challenging  and we present an algorithm for solving it in the next paragraph.
We also report in Appendix C of [14] a convergence proof of a modiﬁed version of the MovKL
algorithm that minimizes a perturbation of the objective function (6) with a small positive parameter
required to guarantee convergence [2].

3.2 Solving a linear system involving multiple operator-valued kernel matrices

One common way to construct operator-valued kernels is to build scalar-valued ones which are
carried over to the vector-valued (resp. function-valued) setting by a positive deﬁnite matrix (resp.
operator). In this setting an operator-valued kernel has the following form:

K(w  z) = G(w  z)T 

where G is a scalar-valued kernel and T is a positive operator in L(Gy). In multi-task learning 
T is a ﬁnite dimensional matrix that is expected to share information between tasks [11  5]. More
recently and for supervised functional output learning problems  T is chosen to be a multiplica-
tion or an integral operator [12  13]. This choice is motivated by the fact that functional linear
models for functional responses [25] are based on these operators and then such kernels provide
an interesting alternative to extend these models to nonlinear contexts. In addition  some works on
functional regression and structured-output learning consider operator-valued kernels constructed
from the identity operator as in [19] and [4]. In this work we adopt a functional data analysis point
of view and then we are interested in a ﬁnite combination of operator-valued kernels constructed
from the identity  multiplication and integral operators. A problem encountered when working with
operator-valued kernels in inﬁnite-dimensional spaces is that of solving the system of linear operator
equations (8). In the following we show how to solve this problem for two cases of operator-valued
kernel combinations.
Case 1: multiple scalar-valued kernels and one operator. This is the simpler case where the
combination of operator-valued kernels has the following form

K(w  z) =

dkGk(w  z)T 

Kronecker product between the multiple scalar-valued kernel matrix G = (cid:80)M

(11)
where Gk is a scalar-valued kernel  T is a positive operator in L(Gy)  and dk are the combi-
In this setting  the block operator kernel matrix K can be expressed as a
nation coefﬁcients.
k=1 dkGk  where
i j=1  and the operator T . Thus we can compute an analytic solution of the
Gk = [Gk(xi  xj)]n
system of equations (8) by inverting K + λI using the eigendecompositions of G and T as in [13].
Case 2: multiple scalar-valued kernels and multiple operators. This is the general case where
multiple operator-valued kernels are combined as follows

K(w  z) =

dkGk(w  z)Tk 

(12)

M(cid:88)

k=1

M(cid:88)

k=1

5

Algorithm 2 Gauss-Seidel Method

choose an initial vector of functions α(0)
repeat

for i = 1  2  . . .   n

i ←− sol. of (13):
α(t)

[K(xi  xi) + λI]α(t)

i = si

end for

until convergence

Algorithm 1 (cid:96)r-norm MovKL
Input Kk for k = 1  . . .   M
k ←− 1
d1
M
α ←− 0
for t = 1  2  . . . do

for k = 1  . . .   M

α(cid:48) ←− α

K ←−(cid:80)

k dt

kKk

α ←− solution of (K + λI)α = y
if (cid:107)α − α(cid:48)(cid:107) <  then

break

end if
k ←−
dt+1

end for

((cid:80)

r+1

(cid:107)fk(cid:107) 2
k (cid:107)fk(cid:107) 2r

r+1 )1/r

for k = 1  . . .   M

where Gk is a scalar-valued kernel  Tk is a positive operator in L(Gy)  and dk are the combination
coefﬁcients. Inverting the associated block operator kernel matrix K is not feasible in this case 
that is why we propose a Gauss-Seidel iterative procedure (see Algorithm 2) to solve the system
of linear operator equations (8). Starting from an initial vector of functions α(0)  the idea is to
iteratively compute  until a convergence condition is satisﬁed  the functions αi according to the
following expression

[K(xi  xi) + λI]α(t)

(13)
where t is the iteration index. This problem is still challenging because the kernel K(· ·) still
involves a positive combination of operator-valued kernels. Our algorithm is based on the idea
that instead of inverting the ﬁnite combination of operator-valued kernels [K(xi  xi) + λI]  we can
consider the following variational formulation of this system

K(xi  xj)α(t)

j=i+1

j=1

 

j

K(xi  xj)α(t−1)

i = yi − i−1(cid:88)

j − n(cid:88)

1
2

min
α(t)

i

(cid:104)M +1(cid:88)
j − n(cid:80)

k=1

K(xi  xj)α(t)

j=1

j=i+1

where si = yi − i−1(cid:80)

Kk(xi  xi)α(t)
i

  α(t)

i (cid:105)Gy − (cid:104)si  α(t)

i (cid:105)Gy  

K(xi  xj)α(t−1)

j

  Kk = dkGkTk  ∀k ∈ {1  . . .   M} 

and KM +1 = λI.
Now  by means of a variable-splitting approach  we are able to decouple the role of the different
kernels. Indeed  the above problem is equivalent to the following one :
i 1 = α(t)

i k for k = 2  . . .   M + 1 

(cid:104) ˆK(xi  xi)α(t)

− (cid:104)si  α(t)

with α(t)

i (cid:105)GM

i (cid:105)GM

  α(t)

i

y

y

1
2

min
α(t)

i

where ˆK(xi  xi) is the (M + 1) × (M + 1) diagonal matrix [Kk(xi  xi)]M +1
is the vector
(α(t)
i M +1) and the (M + 1)-dimensional vector si = (si  0  . . .   0). We now have to deal
with a quadratic optimization problem with equality constraints. Writing down the Lagrangian
of this optimization problem and then deriving its ﬁrst-order optimality conditions leads us to the

i 1  . . .   α(t)

k=1 . α(t)

i

following set of linear equations K1(xi  xi)αi 1 − si +(cid:80)M

Kk(xi  xi)αi k − γk
αi 1 − αi k

k=1 γk = 0
= 0
= 0

where k = 2  . . .   M + 1 and {γk} are the Lagrange multipliers related to the M equality con-
straints. Finally  in this set of equations  the operator-valued kernels have been decoupled and thus 
if their inversion can be easily computed (which is the case in our experiments)  one can solve the
problem (14) with respect to {αi k} and γk by means of another Gauss-Seidel algorithm after simple
reorganization of the linear system.

(14)

6

Figure 1: Example of a couple of input-output signals in our BCI task. (left) Amplitude modula-
tion features extracted from ECoG signals over 5 pre-deﬁned channels. (middle) Signal of labels
denoting whether the ﬁnger is moving or not. (right) Real amplitude movement of the ﬁnger.

4 Experiments

In order to highlight the beneﬁt of our multiple operator-valued kernel learning approach  we have
considered a series of experiments on a real dataset  involving functional output prediction in a
brain-computer interface framework. The problem we addressed is a sub-problem related to ﬁn-
ger movement decoding from Electrocorticographic (ECoG) signals. We focus on the problem of
estimating if a ﬁnger is moving or not and also on the direct estimation of the ﬁnger movement
amplitude from the ECoG signals. The development of the full BCI application is beyond the scope
of this paper and our objective here is to prove that this problem of predicting ﬁnger movement can
beneﬁt from multiple kernel learning.
To this aim  the fourth dataset from the BCI Competition IV [22] was used. The subjects were 3
epileptic patients who had platinium electrode grids placed on the surface of their brains. The num-
ber of electrodes varies between 48 to 64 depending on the subject  and their position on the cortex
was unknown. ECoG signals of the subject were recorded at a 1KHz sampling using BCI2000 [27].
A band-pass ﬁlter from 0.15 to 200Hz was applied to the ECoG signals. The ﬁnger ﬂexion of the
subject was recorded at 25Hz and up-sampled to 1KHz by means of a data glove which measures
the ﬁnger movement amplitude. Due to the acquisition process  a delay appears between the ﬁnger
movement and the measured ECoG signal [22]. One of our hopes is that this time-lag can be prop-
erly learnt by means of multiple operator-valued kernels. Features from the ECoG signals are built
by computing some band-speciﬁc amplitude modulation features  which is deﬁned as the sum of the
square of the band-speciﬁc ﬁltered ECoG signal samples during a ﬁxed time window.
For our ﬁnger movement prediction task  we have kept 5 channels that have been manually selected
and split ECoG signals in portions of 200 samples. For each of these time segments  we have the
label of whether at each time sample  the ﬁnger is moving or not as well as the real movement
amplitudes. The dataset is composed of 487 couples of input-output signals  the output signals
being either the binary movement labels or the real amplitude movement. An example of input-
output signals are depicted in Figure 1. In a nutshell  the problem boils down to be a functional
regression task with functional responses.
To evaluate the performance of the multiple operator-valued kernel learning approach  we use both:
(1) the percentage of labels correctly recognized (LCR) deﬁned by (Wr/Tn) × 100%  where Wr
is the number of well-recognized labels and Tn the total number of labels to be recognized; (2) the
residual sum of squares error (RSSE) as evaluation criterion for curve prediction

(cid:90) (cid:88)

{yi(t) −(cid:98)yi(t)}2dt 

RSSE =

where (cid:98)yi(t) is the prediction of the function yi(t) corresponding to real ﬁnger movement or the

i

ﬁnger movement state.
For the multiple operator-valued kernels having the form (12)  we have used a Gaussian kernel
with 5 different bandwidths and a polynomial kernel of degree 1 to 3 combined with three oper-
ators T : identity T y(t) = y(t)  multiplication operator associated with the function e−t2 deﬁned
by T y(t) = e−t2
y(t)  and the integral Hilbert-Schmidt operator with the kernel e−|t−s| proposed

in [13]  T y(t) = (cid:82) e−|t−s|y(s)ds. The inverses of these operators can be computed analytically.

(15)

7

020406080100120140160180200−20020Ch. 1020406080100120140160180200−10010Ch. 2020406080100120140160180200−505Ch. 3020406080100120140160180200−505Ch. 4020406080100120140160180200−505Ch. 5Time samples050100150200−1.5−1−0.500.511.5Time samplesFinger Movement State050100150200−10123456Time samplesFinger Movement Table 1: (Left) Results for the movement state prediction. Residual Sum of Squares Error (RSSE)
and the percentage number of Labels Correctly Recognized (LCR) of : (1) baseline KRR with the
Gaussian kernel  (2) functional response KRR with the integral operator-valued kernel  (3) MovKL
with (cid:96)∞  (cid:96)1 and (cid:96)2-norm constraint. (Right) Residual Sum of Squares Error (RSSE) results for
ﬁnger movement prediction.

Algorithm

RSSE LCR(%)

Algorithm

KRR - scalar-valued -
KRR - functional response -
MovKL - (cid:96)∞ norm -
MovKL - (cid:96)1 norm -
MovKL - (cid:96)2 norm -

68.32
49.40
45.44
48.12
39.36

72.91
80.20
81.34
80.66
84.72

KRR - scalar-valued -
KRR - functional response -
MovKL - (cid:96)∞ norm -
MovKL - (cid:96)1 norm -
MovKL - (cid:96)2 norm -

RSSE

88.21
79.86
76.52
78.24
75.15

While the inverses of the identity and the multiplication operators are easily and directly computable
from the analytic expressions of the operators  the inverse of the integral operator is computed from
its spectral decomposition as in [13]. The number of eigenfunctions as well as the regularization
parameter λ are ﬁxed using “one-curve-leave-out cross-validation” [26] with the aim of minimizing
the residual sum of squares error.
Empirical results on the BCI dataset are summarized in Table 1. The dataset was randomly parti-
tioned into 65% training and 35% test sets. We compare our approach in the case of (cid:96)1 and (cid:96)2-norm
constraint on the combination coefﬁcients with: (1) the baseline scalar-valued kernel ridge regres-
sion algorithm by considering each output independently of the others  (2) functional response ridge
regression using an integral operator-valued kernel [13]  (3) kernel ridge regression with an evenly-
weighted sum of operator-valued kernels  which we denote by (cid:96)∞-norm MovKL.
As in the scalar case  using multiple operator-valued kernels leads to better results. By directly com-
bining kernels constructed from identity  multiplication and integral operators we could reduce the
residual sum of squares error and enhance the label classiﬁcation accuracy. Best results are obtained
using the MovKL algorithm with (cid:96)2-norm constraint on the combination coefﬁcients. RSSE and
LCR of the baseline kernel ridge regression are signiﬁcantly outperformed by the operator-valued
kernel based functional response regression. These results conﬁrm that by taking into account the
relationship between outputs we can improve performance. This is due to the fact that an operator-
valued kernel induces a similarity measure between two pairs of input/output.

5 Conclusion

In this paper we have presented a new method for learning simultaneously an operator and a ﬁ-
nite linear combination of operator-valued kernels. We have extended the MKL framework to deal
with functional response kernel ridge regression and we have proposed a block coordinate descent
algorithm to solve the resulting optimization problem. The method is applied on a BCI dataset to
predict ﬁnger movement in a functional regression setting. Experimental results show that our algo-
rithm achieves good performance outperforming existing methods. It would be interesting for future
work to thoroughly compare the proposed MKL method for operator estimation with previous re-
lated methods for multi-class and multi-label MKL in the contexts of structured-output learning and
collaborative ﬁltering.

Acknowledgments

We would like to thank the anonymous reviewers for their valuable comments. This research was
funded by the Ministry of Higher Education and Research  Nord-Pas-de-Calais Regional Council
and FEDER (Contrat de Projets Etat Region CPER 2007-2013)  ANR projects LAMPADA (ANR-
09-EMER-007) and ASAP (ANR-09-EMER-001)  and by the IST Program of the European Com-
munity under the PASCAL2 Network of Excellence (IST-216886). This publication only reﬂects the
authors’ views. Francis Bach was partially supported by the European Research Council (SIERRA
Project).

8

References

[1] J. Aﬂalo  A. Ben-Tal  C. Bhattacharyya  J. Saketha Nath  and S. Raman. Variable sparsity kernel learning.

JMLR  12:565–592  2011.

[2] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Machine Learning 

73(3):243–272  2008.

[3] F. Bach. Consistency of the group Lasso and multiple kernel learning. JMLR  9:1179–1225  2008.
[4] C. Brouard  F. d’Alch´e-Buc  and M. Szafranski. Semi-supervised penalized output kernel regression for

link prediction. In Proc. ICML  2011.

[5] A. Caponnetto  C. A. Micchelli  M. Pontil  and Y. Ying. Universal multi-task kernels. JMLR  68:1615–

1646  2008.

[6] C. Carmeli  E. De Vito  and A. Toigo. Vector valued reproducing kernel Hilbert spaces of integrable

functions and mercer theorem. Analysis and Applications  4:377–408  2006.

[7] C. Carmeli  E. De Vito  and A. Toigo. Vector valued reproducing kernel Hilbert spaces and universality.

Analysis and Applications  8:19–61  2010.

[8] C. Cortes  M. Mohri  and A. Rostamizadeh. L2 regularization for learning kernels. In Proc. UAI  2009.
[9] C. Cortes  M. Mohri  and A. Rostamizadeh. Generalization bounds for learning kernels. In ICML  2010.
[10] F. Dinuzzo  C. S. Ong  P. Gehler  and G. Pillonetto. Learning output kernels with block coordinate descent.

In Proc. ICML  2011.

[11] T. Evgeniou  C. A. Micchelli  and M. Pontil. Learning multiple tasks with kernel methods. JMLR 

6:615–637  2005.

[12] H. Kadri  E. Duﬂos  P. Preux  S. Canu  and M. Davy. Nonlinear functional regression: a functional RKHS

approach. In Proc. AISTATS  pages 111–125  2010.

[13] H. Kadri  A. Rabaoui  P. Preux  E. Duﬂos  and A. Rakotomamonjy. Functional regularized least squares

classiﬁcation with operator-valued kernels. In Proc. ICML  2011.

[14] H. Kadri  A. Rakotomamonjy  F. Bach  and P. Preux. Multiple operator-valued kernel learning. Technical

Report 00677012  INRIA  2012.

[15] M. Kloft  U. Brefeld  S. Sonnenburg  and A. Zien. (cid:96)p-norm multiple kernel learning. JMLR  12:953–997 

2011.

[16] S. Kurcyusz. On the existence and nonexistence of lagrange multipliers in Banach spaces. Journal of

Optimization Theory and Applications  20:81–110  1976.

[17] A. Kurdila and M. Zabarankin. Convex Functional Analysis. Birkhauser Verlag  2005.
[18] G. Lanckriet  N. Cristianini  L. El Ghaoui  P. Bartlett  and M. Jordan. Learning the kernel matrix with

semi-deﬁnite programming. JMLR  5:27–72  2004.

[19] H. Lian. Nonlinear functional models for functional responses in reproducing kernel Hilbert spaces. The

Canadian Journal of Statistics  35:597–606  2007.

[20] C. Micchelli and M. Pontil. Learning the kernel function via regularization. JMLR  6:1099–1125  2005.
[21] C. A. Micchelli and M. Pontil. On learning vector-valued functions. Neural Comput.  17:177–204  2005.
[22] K. J. Miller and G. Schalk. Prediction of ﬁnger ﬂexion: 4th brain-computer interface data competition.

BCI Competition IV  2008.

[23] T. Pistohl  T. Ball  A. Schulze-Bonhage  A. Aertsen  and C. Mehring. Prediction of arm movement
trajectories from ECoG-recordings in humans. Journal of Neuroscience Methods  167(1):105–114  2008.

[24] A. Rakotomamonjy  F. Bach  Y. Grandvalet  and S. Canu. SimpleMKL. JMLR  9:2491–2521  2008.
[25] J. O. Ramsay and B. W. Silverman. Functional Data Analysis  2nd ed. Springer Verlag  New York  2005.
[26] John A. Rice and B. W. Silverman. Estimating the mean and covariance structure nonparametrically when

the data are curves. Journal of the Royal Statistical Society. Series B  53(1):233–243  1991.

[27] G. Schalk  D. J. McFarland  T. Hinterberger  N. Birbaumer  and J. R. Wolpaw. BCI2000: a general-
purpose brain-computer interface system. Biomedical Engineering  IEEE Trans. on  51:1034–1043  2004.
[28] B. Sch¨olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines  Regularization  Opti-

mization  and Beyond. MIT Press  Cambridge  MA  USA  2002.

[29] S. Sonnenburg  G. R¨atsch  C. Sch¨afer  and B. Sch¨olkopf. Large scale multiple kernel learning. JMLR 

7:1531–1565  2006.

[30] P. Tseng. Convergence of block coordinate descent method for nondifferentiable minimization. J. Optim.

Theory Appl.  109:475–494  2001.

9

,Gang Wang
Georgios Giannakis
Yousef Saad
Jie Chen