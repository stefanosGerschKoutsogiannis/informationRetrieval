2018,Spectral Filtering for General Linear Dynamical Systems,We give a polynomial-time algorithm for learning latent-state linear dynamical systems without system identification  and without assumptions on the spectral radius of the system's transition matrix. The algorithm extends the recently introduced technique of spectral filtering  previously applied only to systems with a symmetric transition matrix  using a novel convex relaxation to allow for the efficient identification of phases.,Spectral Filtering for

General Linear Dynamical Systems

Elad Hazan

Princeton University & Google AI Princeton

ehazan@cs.princeton.edu

Holden Lee

Princeton University

holdenl@princeton.edu

Karan Singh

Princeton University & Google AI Princeton

karans@cs.princeton.edu

Cyril Zhang

Princeton University & Google AI Princeton

cyril.zhang@cs.princeton.edu

Yi Zhang

Princeton University & Google AI Princeton

y.zhang@cs.princeton.edu

Abstract

We give a polynomial-time algorithm for learning latent-state linear dynamical
systems without system identiï¬cation  and without assumptions on the spectral
radius of the systemâ€™s transition matrix. The algorithm extends the recently in-
troduced technique of spectral ï¬ltering  previously applied only to systems with
a symmetric transition matrix  using a novel convex relaxation to allow for the
efï¬cient identiï¬cation of phases.

1

Introduction

Linear dynamical systems (LDSs) are a cornerstone of signal processing and time series analysis.
The problem of predicting the response signal arising from a LDS is a fundamental problem in
machine learning  with a history of more than half a century.
An LDS is given by matrices (ğ´  ğµ  ğ¶  ğ·). Given a sequence of inputs {ğ‘¥ğ‘¡}  the output {ğ‘¦ğ‘¡} of the
system is governed by the linear equations

â„ğ‘¡ = ğ´â„ğ‘¡âˆ’1 + ğµğ‘¥ğ‘¡ + ğœ‚ğ‘¡
ğ‘¦ğ‘¡ = ğ¶â„ğ‘¡ + ğ·ğ‘¥ğ‘¡ + ğœ‰ğ‘¡ 

(1)

where ğœ‚ğ‘¡  ğœ‰ğ‘¡ are noise vectors  and â„ğ‘¡ is a hidden (latent) state.
Roweis and Ghahramani [RG99] show that special cases of this formulation capture a host of ma-
chine learning models  including hidden Markov models  Gaussian mixture models  principal com-
ponent analysis  and linear Gaussian models. It has been observed numerous times in the literature
that if there is no hidden state  or if the transition matrices are known  then the formulation is essen-
tially convex and amenable to efï¬cient optimization.
In this paper we are concerned with the general and more challenging case  arguably the one which
is more applicable as well  in which the hidden state is not observed  and the system dynamics
are unknown to the learner. In this setting  despite the vast literature on the subject from various
communities  there is a lack of provably efï¬cient methods for learning the LDS without strong
generative or other assumptions.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  MontrÃ©al  Canada.

Building on recent advances in spectral ï¬ltering  we develop a novel convex relaxation for LDSs 
resulting in an efï¬cient algorithm for the LDS prediction problem in the general setting. Our al-
gorithm makes online predictions which are close (in terms of mean squared error) to those of the
optimal LDS in hindsight.

1.1 Problem statement and our results

An LDS prediction problem is deï¬ned as follows. Iteratively for ğ‘¡ = 1  2  ...  ğ‘‡   the learner observes
the input to the system ğ‘¥ğ‘¡ âˆˆ Rğ‘›. The learner then makes a prediction Ë†ğ‘¦ğ‘¡ âˆˆ Rğ‘š  observes true
outcome ğ‘¦ğ‘¡ âˆˆ Rğ‘š  and suffers a loss â„“(Ë†ğ‘¦ğ‘¡  ğ‘¦ğ‘¡). For simplicity we consider the mean square error
â„“(Ë†ğ‘¦ğ‘¡  ğ‘¦ğ‘¡) = â€–Ë†ğ‘¦ğ‘¡ âˆ’ ğ‘¦ğ‘¡â€–2  even though our techniques can handle any Lipschitz convex loss.
The goal of the online learner is to minimize its regret  or difference in loss between its prediction 
and the prediction of the best LDS in hindsight that predicts with ğ‘¦*

1  . . .   ğ‘¦*
ğ‘‡ :

ğ‘‡âˆ‘ï¸
ğ‘¡=1 â€–Ë†ğ‘¦ğ‘¡ âˆ’ ğ‘¦ğ‘¡â€–2 âˆ’

ğ‘‡âˆ‘ï¸
ğ‘¡=1 â€–ğ‘¦

*
ğ‘¡ âˆ’ ğ‘¦ğ‘¡â€–2.

Regret(ğ‘‡ ) :=

ğ‘¡ are not ï¬xed vectors  but rather evolve according to a hidden state and equation

We emphasize that ğ‘¦*
(1) according to the best possible transition matrices  in terms of mean square error ï¬t to the data.
Our main result is a polynomial-time algorithm that predicts Ë†ğ‘¦ğ‘¡ given all previous input and feedback
(ğ‘¥1:ğ‘¡  ğ‘¦1:ğ‘¡âˆ’1)  and attains a near-optimal regret bound of

Regret(ğ‘‡ ) â‰¤ Ëœğ‘‚(âˆšğ‘‡ ) + ğ¾ Â· ğ¿.

Here  ğ¿ denotes the inevitable loss incurred by perturbations to the system which cannot be antici-
pated by the learner  which are allowed to be adversarial. This ğ¿ can grow with time  and is usually
assumed to be proportional to a small constant  say ğœ€ğ‘‡ .
The constant in the Ëœğ‘‚(Â·)  as well as ğ¾  depend polynomially on the dimensionality of the system 
the norms of the inputs and outputs  and certain natural quantities related to the transition matrix
ğ´. Additionally  the running time of our algorithm is polynomial in all natural parameters of the
problem.
In comparison to previous approaches  we note:

âˆ™ Our algorithm is the ï¬rst sample-efï¬cient and polynomial-time algorithm with this guaran-
tee. In the next section  we survey local search algorithms that either only converge to local
optima or require an exponential number of iterations in the worst case.

âˆ™ The main feature is that the regret does not depend on the spectral radius ğœŒ(ğ´) of the sys-
temâ€™s hidden-state transition matrix. If one allows a dependence on the condition number 
then simple linear regression-based algorithms are known to obtain the same result  with
time and sample complexity polynomial in

1âˆ’ğœŒ(ğ´). (See Section 6 of [HMR16].)

1

1.2 Related work

The prediction problems of time series for linear dynamical systems was deï¬ned in the seminal work
of Kalman [Kal60]  who introduced the Kalman ï¬lter as a recursive least-squares solution for max-
imum likelihood estimation (MLE) of Gaussian perturbations to the system. For more background
see the classic survey [Lju98]  and the extensive overview of recent literature in [HMR16].
For a linear dynamical system with no hidden state  the system is identiï¬able by a convex program
and thus well understood (see [DMM+17  AYS11]  who address sample complexity issues and
regret for system identiï¬cation and linear-quadratic control in this setting).
Various exponential-time approaches have been proposed to learn the system in the case that the sys-
tem is unknown. Regret bounds similar to ours are obtainable using the continuous multiplicative-
weights algorithm (see [CBL06]  as well as the EWOO algorithm in [HAK07]). These methods 
mentioned brieï¬‚y in [HSZ17]  basically amount to discretizing the entire parameter space of LDSs 
and take time exponential in the system dimensions. Stronger guarantees are obtained in [KM17] 
though still in exponential time.

2

Ghahramani and Roweis [RG99] suggest using the EM algorithm to learn the parameters of an
LDS. This approach remains widely used  but is inherently non-convex and can get stuck in local
minima. Recently [HMR16] show that for a restricted class of systems  gradient descent (also widely
used in practice  perhaps better known in this setting as backpropagation) guarantees polynomial
convergence rates and sample complexity in the batch setting. Their result applies essentially only
to the SISO case  depends polynomially on the spectral gap  and requires the signal to be generated
by an LDS.
In recent work  [HSZ17] show how to efï¬ciently learn an LDS in the online prediction setting 
without any generative assumptions  and without dependence on the condition number. Their
new methodology  however  was restricted to LDSs with symmetric transition matrices. For
the structural result  we use the same results from the spectral theory of Hankel matrices; see
[BT17  Hil94  Cho83]. Obtaining provably efï¬cient algorithms for the general case is signiï¬cantly
more challenging.
We make use of linear ï¬ltering  or linear regression on the past observations as well as inputs  as a
subroutine for future prediction. This technique is well-established in the context of autoregressive
models for time-series prediction that have been extensively studied in the learning and signal-
processing literature  see e.g. [Ham94  BJR94  BD09  KM16  AHMS13  MW07].
The recent success of recurrent neural networks (RNNs) for tasks such as speech and language
modeling has inspired a resurgence of interest in linear dynamical systems [HMR16  BK15].

2 Preliminaries

2.1 Setting
A linear dynamical system Î˜ = (ğ´  ğµ  ğ¶  ğ·)  with initial hidden state â„0 âˆˆ Rğ‘‘  speciï¬es a map
from inputs ğ‘¥1  . . .   ğ‘¥ğ‘‡ âˆˆ Rğ‘› to outputs (responses) ğ‘¦1  . . .   ğ‘¦ğ‘‡ âˆˆ Rğ‘š  given by the recursive
equations
(2)
(3)

â„ğ‘¡ = ğ´â„ğ‘¡âˆ’1 + ğµğ‘¥ğ‘¡ + ğœ‚ğ‘¡
ğ‘¦ğ‘¡ = ğ¶â„ğ‘¡ + ğ·ğ‘¥ğ‘¡ + ğœ‰ğ‘¡ 

where ğ´  ğµ  ğ¶  ğ· are matrices of appropriate dimension  and ğœ‚ğ‘¡  ğœ‰ğ‘¡ are noise vectors.
We make the following assumptions to characterize the â€œsizeâ€ of an LDS we are competing against:

1. Inputs and outputs and bounded: â€–ğ‘¥ğ‘¡â€–2 â‰¤ ğ‘…ğ‘¥ â€–ğ‘¦ğ‘¡â€–2 â‰¤ ğ‘…ğ‘¦.1
2. The system is Lyapunov stable  i.e.  the largest singular value of ğ´ is at most 1: ğœŒ(ğ´) â‰¤ 1.
3. ğ´ is diagonalizable by a matrix with small entries: ğ´ = Î¨Î›Î¨âˆ’1  with â€–Î¨â€–ğ¹

Note that we do not need this parameter to be bounded away from 1.

ğ‘…Î¨. Intuitively  this holds if the eigenvectors corresponding to larger eigenvalues arenâ€™t
close to linearly dependent.

âƒ¦âƒ¦Î¨âˆ’1âƒ¦âƒ¦ğ¹ â‰¤

4. ğµ  ğ¶  ğ· have bounded spectral norms: â€–ğµâ€–2  â€–ğ¶â€–2  â€–ğ·â€–2 â‰¤ ğ‘…Î˜.
5. Let ğ‘† =

{ï¸ ğ›¼|ğ›¼| : ğ›¼ is an eigenvalue of ğ´

be the set of phases of all eigenvalues of ğ´. There
exists a monic polynomial ğ‘(ğ‘¥) of degree ğœ such that ğ‘(ğœ”) = 0 for all ğœ” âˆˆ ğ‘†  the ğ¿1 norm
of its coefï¬cients is at most ğ‘…1  and the ğ¿âˆ norm is at most ğ‘…âˆ. We will explain this
condition in Section 4.1.

}ï¸

In our regret model  the adversary chooses an LDS (ğ´  ğµ  ğ¶  ğ·)  and has a budget ğ¿. The dynamical
system produces outputs given by the above equations  where the noise vectors ğœ‚ğ‘¡  ğœ‰ğ‘¡ are chosen

adversarially  subject to a budget constraint:âˆ‘ï¸€ğ‘‡

ğ‘¡=1 â€–ğœ‚ğ‘¡â€–2 + â€–ğœ‰ğ‘¡â€–2 â‰¤ ğ¿.

Then  the online prediction setting is identical to that proposed in [HSZ17]. For each iteration
ğ‘¡ = 1  . . .   ğ‘‡   the input ğ‘¥ğ‘¡ is revealed  and the learner must predict a response Ë†ğ‘¦ğ‘¡. Then  the true ğ‘¦ğ‘¡
is revealed  and the learner suffers a least-squares loss of â€–ğ‘¦ğ‘¡ âˆ’ Ë†ğ‘¦ğ‘¡â€–2. Of course  if ğ¿ scales with
1Note that no bound on â€–ğ‘¦ğ‘¡â€– is required for the approximation theorem; ğ‘…ğ‘¦ only appears in the regret

bound.

3

â€–ğ‘€â€–2 ğ‘ :=

and the limiting case

ââ ğ‘/2â¤â¥â¦1/ğ‘

 

ğ‘€ (ğ‘  â„  ğ‘—  ğ‘–)2

â¡â¢â£âˆ‘ï¸

ğ‘

â›ââˆ‘ï¸
âˆšï¸ƒâˆ‘ï¸

â„ ğ‘– ğ‘—

â„ ğ‘– ğ‘—

the time horizon ğ‘‡   it is information-theoretically impossible for an online algorithm to incur a loss
sublinear in ğ‘‡   even under non-adversarial (e.g. Gaussian) perturbations. Thus  our end-to-end goal
is to track the LDS with loss that scales with the total magnitude of the perturbations  independently
of ğ‘‡ .
This formulation is fundamentally a min-max problem: given a limited budget of perturbations  an
adversary tries to maximize the error of the algorithmâ€™s predictions  while the algorithm seeks to be
robust against any such adversary. This corresponds to the ğ»âˆ notion of robustness in the control
theory literature; see Section 15.5 of [ZDG+96].

2.2 Spectral ï¬ltering for time series

The spectral ï¬ltering technique is introduced in [HSZ17]  which considers a spectral decomposition
of the derivative of the impulse response function of an LDS with a symmetric transition matrix. A
crucial object of consideration in spectral ï¬ltering is the set of wave-ï¬lters ğœ‘1  . . .   ğœ‘ğ‘˜  which are
the top ğ‘˜ eigenvectors of the deterministic Hankel matrix ğ‘ğ‘‡ âˆˆ Rğ‘‡Ã—ğ‘‡   whose entries are given by
(ğ‘–+ğ‘—)3âˆ’(ğ‘–+ğ‘—). Bounds on the ğœ€-rank of positive semideï¬nite Hankel matrices can be found
ğ‘(ğ‘–  ğ‘—) =
in [BT17]. Our algorithm will â€œcompressâ€ the input time series using a time-domain convolution of
the input time series with ï¬lters derived from these eigenvectors.

2

2.3 Notation for matrix norms

We will consider a few â€œmixedâ€ â„“ğ‘ matrix norms of a 4-tensor ğ‘€  whose elements are indexed
by ğ‘€ (ğ‘  â„  ğ‘—  ğ‘–) (the roles and bounds of these indices will be introduced later). For conciseness 
whenever the norm of such a 4-tensor is taken  we establish the notation for the mixed matrix norm

â€–ğ‘€â€–2 âˆ := max

ğ‘

ğ‘€ (ğ‘  â„  ğ‘—  ğ‘–)2.

These are the straightforward analogues of the matrix norms deï¬ned in [KSST12]  and appear in the
regularization of the online prediction algorithm.

3 Algorithm and main theorem

We begin by describing the algorithm in terms of a linear model ğ‘¦( Ë†Î˜ğ‘¡; ğ‘¥1:ğ‘¡; ğ‘¦ğ‘¡âˆ’1:ğ‘¡âˆ’ğœ )  the details of
which occur in Deï¬nition 2.

ğ‘—=1  the top ğ‘˜ eigenpairs of ğ‘ğ‘‡ .

Algorithm 1 Phased wave-ï¬ltered regression
1: Input: time horizon ğ‘‡   parameters ğ‘˜  ğ‘Š  ğœ  ğ‘… ^Î˜  regularization weight ğœ‚.
2: Compute {(ğœğ‘—  ğœ‘ğ‘—)}ğ‘˜
3: Initialize Ë†Î˜1 âˆˆ ğ’¦ arbitrarily.
4: for ğ‘¡ = 1  . . .   ğ‘‡ do
5:
6:
7:

Predict Ë†ğ‘¦ğ‘¡ := ğ‘¦( Ë†Î˜ğ‘¡; ğ‘¥1:ğ‘¡; ğ‘¦ğ‘¡âˆ’1:ğ‘¡âˆ’ğœ ).
Observe ğ‘¦ğ‘¡. Suffer loss â€–ğ‘¦ğ‘¡ âˆ’ Ë†ğ‘¦ğ‘¡â€–2.
ğ‘¡âˆ’1âˆ‘ï¸
Solve FTRL convex program:
ğ‘¢=0â€–ğ‘¦( Ë†Î˜; ğ‘¥1:ğ‘¢  ğ‘¦ğ‘¢âˆ’1:ğ‘¢âˆ’ğœ ) âˆ’ ğ‘¦ğ‘¢â€–2 +

Ë†Î˜ğ‘¡+1 â† arg min
^Î˜âˆˆğ’¦

ğ‘…( Ë†Î˜).

1
ğœ‚

8: end for

The central result in the paper is stated below.

4

Theorem 1 (Main; informal). Consider a LDS with noise (given by (2) and (3)) satisfying the
assumptions in Section 2.1  where total noise is bounded by ğ¿. Then there is a choice of parameters
such that Algorithm 1 learns a linear model Ë†Î˜ whose predictions Ë†ğ‘¦ğ‘¡ satisfy

poly(ğ‘…  ğ‘‘

â€²

)âˆšğ‘‡ + ğ‘…2âˆğœ 3ğ‘…2

Î˜ğ‘…2

Î¨ğ¿

(4)

)ï¸

ğ‘‡âˆ‘ï¸
ğ‘¡=1 â€–Ë†ğ‘¦ğ‘¡ âˆ’ ğ‘¦ğ‘¡â€–2 â‰¤ Ëœğ‘‚

(ï¸

where ğ‘…1  ğ‘…ğ‘¥  ğ‘…ğ‘¦  ğ‘…Î˜  ğ‘…Î¨ â‰¤ ğ‘…  ğ‘š  ğ‘›  ğ‘‘ â‰¤ ğ‘‘â€².
To deï¬ne the algorithm  we specify a reparameterization of linear dynamical systems. To this end 
we deï¬ne a pseudo-LDS  which pairs a subspace-restricted linear model of the impulse response
with an autoregressive model:
Deï¬nition 2. A pseudo-LDS Ë†Î˜ = (ğ‘€  ğ‘  ğ›½  ğ‘ƒ ) is given by two 4-tensors ğ‘€  ğ‘ âˆˆ Rğ‘ŠÃ—ğ‘˜Ã—ğ‘›Ã—ğ‘š a
vector ğ›½ âˆˆ Rğœ   and matrices ğ‘ƒ0  . . .   ğ‘ƒğœâˆ’1 âˆˆ Rğ‘šÃ—ğ‘›. Let the prediction made by Ë†Î˜  which depends
on the entire history of inputs ğ‘¥1:ğ‘¡ and ğœ past outputs ğ‘¦ğ‘¡âˆ’1:ğ‘¡âˆ’ğœ be given by

ğœâˆ‘ï¸

ğ‘¢=1

[ï¸ƒ(ï¸ƒ

ğœâˆ’1âˆ‘ï¸
(ï¸‚ 2ğœ‹ğ‘¢ğ‘

ğ‘—=0

ğ‘Š

ğ‘¦( Ë†Î˜; ğ‘¥1:ğ‘¡  ğ‘¦ğ‘¡âˆ’1:ğ‘¡âˆ’ğœ )(:) :=

ğ›½ğ‘¢ğ‘¦ğ‘¡âˆ’ğ‘¢ +

ğ‘ƒğ‘—ğ‘¥ğ‘¡âˆ’ğ‘—

ğ‘Šâˆ’1âˆ‘ï¸

ğ‘›âˆ‘ï¸

ğ‘˜âˆ‘ï¸

ğ‘¡âˆ‘ï¸

ğ‘=0

ğ‘–=1

â„=1

ğ‘¢=ğœ

+

ğ‘€ (ğ‘  â„  ğ‘–  :) cos

)ï¸‚

(ï¸‚ 2ğœ‹ğ‘¢ğ‘

)ï¸‚)ï¸ƒ

ğ‘Š

+ ğ‘ (ğ‘  â„  ğ‘–  :) sin

]ï¸ƒ

1
4

â„ ğœ‘â„(ğ‘¢)ğ‘¥ğ‘¡âˆ’ğ‘¢(ğ‘–)
ğœ

Here  ğœ‘1  . . .   ğœ‘ğ‘˜ âˆˆ Rğ‘‡ are the top ğ‘˜ eigenvectors  with eigenvalues ğœ1  . . .   ğœğ‘˜  of ğ‘ğ‘‡ . These
can be computed using specialized methods [BLV98]. Some of the dimensions of these tensors are
parameters to the algorithm  which we list here:

âˆ™ Number of ï¬lters ğ‘˜.
âˆ™ Phase discretization parameter ğ‘Š .
âˆ™ Autoregressive parameter ğœ.
Additionally  we deï¬ne the following:

ğ‘â€² +âˆ‘ï¸€ğœ

ğ¹   where ğ‘ =

ğ‘—=1 â€–ğ‘ƒğ‘—â€–2

âˆšï¸âˆ‘ï¸€ğœ

âˆ™ Regularizer ğ‘…(ğ‘€  ğ‘  ğ›½  ğ‘ƒ ) := â€–ğ‘€â€–2

ln(ğ‘Š )âˆ’1  and ğ‘â€² = ln(ğœ )
ln(ğœ )âˆ’1.

ln(ğ‘Š )

2 ğ‘ + â€–ğ‘â€–2

2 ğ‘ + â€–ğ›½â€–2

âˆ™ Composite norm â€–(ğ‘€  ğ‘  ğ›½  ğ‘ƒ )â€– := â€–ğ‘€â€–2 1 + â€–ğ‘â€–2 1 + â€–ğ›½â€–1 +
âˆ™ Composite norm constraint ğ‘… ^Î˜  and the corresponding set of pseudo-LDSs ğ’¦ = { Ë†Î˜ :

ğ¹ .
ğ‘—=1 â€–ğ‘ƒğ‘—â€–2

â€– Ë†Î˜â€– â‰¤ ğ‘… ^Î˜}.

Crucially  ğ‘¦( Ë†Î˜; ğ‘¥1:ğ‘¡  ğ‘¦ğ‘¡âˆ’1:ğ‘¡âˆ’ğ‘‘) is linear in each of ğ‘€  ğ‘  ğ‘ƒ  ğ›½; consequently  the least-squares loss
â€–ğ‘¦( Ë†Î˜; ğ‘¥1:ğ‘¡) âˆ’ ğ‘¦â€–2 is convex  and can be minimized in polynomial time. To this end  our online pre-
diction algorithm is follow-the-regularized-leader (FTRL)  which requires the solution of a convex
program at each iteration. We choose this regularization to obtain the strongest theoretical guarantee 
and provide a brief note in Section 5 on alternatives to address performance issues.
At a high level  our algorithm works by ï¬rst approximating the response of an LDS by an au-
toregressive model of order (ğœ  ğœ )  then reï¬ning the approximation using wave-ï¬lters with a phase
component. Speciï¬cally  the blocks of ğ‘€ and ğ‘ corresponding to ï¬lter index â„ and phase index ğ‘
specify the linear dependence of ğ‘¦ğ‘¡ on a certain convolution of the input time series  whose kernel
is the pointwise product of ğœ‘â„ and a sinusoid with period ğ‘Š/ğ‘. The structural result which drives
the theorem is that the dynamics of any true LDS are approximated by such a pseudo-LDS  with
reasonably small parameters and coefï¬cients.
Note that the autoregressive component in our deï¬nition of a pseudo-LDS is slightly more restricted
than multivariate autoregressive models: the coefï¬cients ğ›½ğ‘— are scalar  rather than allowed to be

5

arbitrary matrices. These options are interchangeable for our purposes  without affecting the asymp-
totic regret; we choose to use scalar coefï¬cients for a more streamlined analysis.
The online prediction algorithm is fully speciï¬ed in Algorithm 1; the parameter choices that give the
best asymptotic theoretical guarantees are speciï¬ed in the appendix  while typical realistic settings
are outlined in Section 5.

4 Analysis

There are three parts to the analysis  which we outline in the following subsections: proving the ap-
proximability of an LDS by a pseudo-LDS  bounding the regret incurred by the algorithm against the
best pseudo-LDS  and ï¬nally analyzing the effect of noise ğ¿. The full proofs are in Appendices A  B 
and C  respectively.

4.1 Approximation theorem for general LDSs

We develop a more general analogue of the structural result from [HSZ17]  which holds for systems
with asymmetric transition matrix ğ´.
Theorem 3 (Approximation theorem; informal). Consider an noiseless LDS (given by (2) and (3)
with ğœ‚ğ‘¡  ğœ‰ğ‘¡ = 0) satisfying the assumptions in Section 2.1.

)ï¸€)ï¸€  ğ‘Š = ğ‘‚ (poly(ğœ  ğ‘…Î˜  ğ‘…Î¨  ğ‘…1  ğ‘…ğ‘¥  ğ‘‡ )) and a

There is ğ‘˜ = ğ‘‚(ï¸€poly log(ï¸€ğ‘‡  ğ‘…Î˜  ğ‘…Î¨  ğ‘…1  ğ‘…ğ‘¥  1

pseudo-LDS Ë†Î˜ of norm ğ‘‚(poly(ğ‘…Î˜  ğ‘…Î¨  ğ‘…1  ğœ  ğ‘˜)) such that Ë†Î˜ approximates ğ‘¦ğ‘¡ to within ğœ€ for
1 â‰¤ ğ‘¡ â‰¤ ğ‘‡ :

ğœ€

âƒ¦âƒ¦âƒ¦ğ‘¦( Ë†Î˜; ğ‘¥1:ğ‘¡  ğ‘¦ğ‘¡âˆ’1:ğ‘¡âˆ’ğœ ) âˆ’ ğ‘¦ğ‘¡

âƒ¦âƒ¦âƒ¦ â‰¤ ğœ€.

(5)

ğ‘‘

ğ‘‘

For the formal statement (with precise bounds) and proof  see Appendix A.2. In this section we give
some intuition for the conditions and an outline of the proof.
First  we explain the condition on the polynomial ğ‘. As we show in Appendix A.1 we can predict
using a pure autoregressive model  without waveï¬lters  if we require ğ‘ to have all eigenvalues of
ğ´ as roots (i.e.  it is divisible by the minimal polynomial of ğ´). However  the coefï¬cients of this
polynomial could be very large. The size of these coefï¬cients will appear in the bound for the main
theorem  as using large coefï¬cients in the predictor will make it sensitive to noise.
Requiring ğ‘ only to have the phases of eigenvalues of ğ´ as roots can decrease the coefï¬cients
signiï¬cantly. As an example  consider if ğ´ has many ğ‘‘/3 distinct eigenvalues with phase 1  and
similarly for ğœ”  and ğœ”  and suppose their absolute values are close to 1. Then the minimal polynomial
is approximately (ğ‘¥âˆ’ 1)
3 which can have coefï¬cients as large as exp(â„¦(ğ‘‘)). On
the other hand  for the theorem we can take ğ‘(ğ‘¥) = (ğ‘¥ âˆ’ 1)(ğ‘¥ âˆ’ ğœ”)(ğ‘¥ âˆ’ ğœ”) which has degree 3 and
coefï¬cients bounded by a constant. Intuitively  the waveï¬lters help if there are few distinct phases 
or they are well-separated (consider that if the phases were exactly the ğ‘‘th roots of unity  that ğ‘ can
be taken to be ğ‘¥ğ‘‘ âˆ’ 1). Note that when the roots are real  we can take ğ‘ = ğ‘¥ âˆ’ 1 and the analysis
reduces to that of [HSZ17].
We now sketch a proof of Theorem 3. Motivation is given by the Cayley-Hamilton Theorem  which
says that if ğ‘ is the characteristic polynomial of ğ´  then ğ‘(ğ´) = ğ‘‚. This fact tells us that the
ğ‘¡=1 ğ›½ğ‘—ğ‘¥ğœâˆ’ğ‘—  then

â„ğ‘¡ = ğ´ğ‘¡â„0 satisï¬es a linear recurrence of order ğœ = deg ğ‘: if ğ‘(ğ‘¥) = ğ‘¥ğœ +âˆ‘ï¸€ğœ
â„ğ‘¡ +âˆ‘ï¸€ğœ
If ğ‘ has only the phases as the roots  then â„ğ‘¡ +âˆ‘ï¸€ğœ
ğ›¼ = ğ‘Ÿğœ” with |ğœ”| = 1. Suppose ğ‘(ğ‘¥) = ğ‘¥ğœ +âˆ‘ï¸€ğœ

ğ‘¡=1 ğ›½ğ‘—â„ğ‘¡âˆ’ğ‘— Ì¸= 0 but can be written in terms of
the waveï¬lters. Consider for simplicity the 1-dimensional (complex) LDS ğ‘¦ğ‘¡ = ğ›¼ğ‘¦ğ‘¡âˆ’1 + ğ‘¥ğ‘¡  and let
ğ‘¡=1 ğ›½ğ‘—ğ‘¥ğœâˆ’ğ‘— = 0 and ğ‘(ğœ”) = 0. In general the LDS
is a â€œsumâ€ of LDSâ€™s that are in this form. Summing the past ğœ terms with coefï¬cients given by ğ›½ 

ğ‘¡=1 ğ›½ğ‘—â„ğ‘¡âˆ’ğ‘— = 0.

3 (ğ‘¥âˆ’ ğœ”)

3 (ğ‘¥âˆ’ ğœ”)

ğ‘‘

ğ‘¦ğ‘¡ = ğ‘¥ğ‘¡ +ğ›¼ğ‘¥ğ‘¡âˆ’1 +Â·Â·Â·

+ğ›¼ğœ ğ‘¥ğ‘¡âˆ’ğœ

+Â·Â·Â·
+Â·Â·Â· +ğ›¼ğœâˆ’1ğ‘¥ğ‘¡âˆ’ğœ +Â·Â·Â· )
...

...
ğ‘¥ğ‘¡âˆ’ğœ

+Â·Â·Â· )

+ğ›½1(ğ‘¦ğ‘¡âˆ’1 =
...
+ğ›½ğœ (ğ‘¦ğ‘¡âˆ’ğœ =

ğ‘¥ğ‘¡âˆ’1

6

The terms ğ‘¥ğ‘¡  . . .   ğ‘¥ğ‘¡âˆ’ğœ +1 can be taken care of by linear regression. Consider a term ğ‘¥ğ‘—  ğ‘— < ğ‘¡ âˆ’ ğœ
in this sum. The coefï¬cient is ğ›¼ğ‘—âˆ’(ğ‘¡âˆ’ğœ )(ğ›¼ğœ + ğ›½1ğ›¼ğœâˆ’1 + Â·Â·Â· + ğ›½ğœ ). Because ğ‘(ğœ”) = 0  this can be
written as
(6)
Factoring out 1âˆ’ ğ‘Ÿ from each of these terms show that ğ‘¦ğ‘¡ + ğ›½1ğ‘¦ğ‘¡âˆ’1 +Â·Â·Â· + ğ›½ğœ ğ‘¦ğ‘¡âˆ’ğœ can be expressed
as a function of a convolution of the vector ((1 âˆ’ ğ‘Ÿ)ğ‘Ÿğ‘¡âˆ’1ğœ”ğ‘¡âˆ’1) with ğ‘¥1:ğ‘‡ . The waveï¬lters were
designed precisely to approximate the vector ğœ‡(ğ‘Ÿ) = ((1âˆ’ ğ‘Ÿ)ğ‘Ÿğ‘¡âˆ’1)1â‰¤ğ‘¡â‰¤ğ‘‡ well  hence ğ‘¦ğ‘¡ + ğ›½1ğ‘¦ğ‘¡âˆ’1 +
Â·Â·Â· + ğ›½ğœ ğ‘¦ğ‘¡âˆ’ğœ can be approximated using the waveï¬lters multiplied by phase and convolved with ğ‘¥.
Note that the 1 âˆ’ ğ‘Ÿ is necessary in order to make the ğ¿2 norm of ((1 âˆ’ ğ‘Ÿ)ğ‘Ÿğ‘¡âˆ’1)1â‰¤ğ‘¡â‰¤ğ‘‡ bounded  and
hence ensure the waveï¬lters have bounded coefï¬cients.

ğ›¼ğ‘—âˆ’(ğ‘¡âˆ’ğœ )((ğ›¼ğœ âˆ’ ğœ”ğœ ) + ğ›½1(ğ›¼ğœâˆ’1 âˆ’ ğœ”ğœâˆ’1) + Â·Â·Â· ).

4.2 Regret bound for pseudo-LDSs

As an intermediate step toward the main theorem  we show a regret bound on the total least-squares
prediction error made by Algorithm 1  compared to the best pseudo-LDS in hindsight.
Theorem 4 (FTRL regret bound; informal). Let Ë†ğ‘¦*
ğ‘‡ denote the predictions made by the
ï¬xed pseudo-LDS minimizing the total squared-norm error. Then  there is a choice of parameters
for which the decision set ğ’¦ contains all LDSs which obey the assumptions from Section 2.1  for
which the predictions Ë†ğ‘¦1  . . .   Ë†ğ‘¦ğ‘‡ made by Algorithm 1 satisfy

1  . . .   Ë†ğ‘¦*

ğ‘‡âˆ‘ï¸
ğ‘¡=1 â€–Ë†ğ‘¦ğ‘¡ âˆ’ ğ‘¦ğ‘¡â€–2 âˆ’

ğ‘‡âˆ‘ï¸
*
ğ‘¡ âˆ’ ğ‘¦ğ‘¡â€–2 â‰¤ Ëœğ‘‚
ğ‘¡=1 â€–Ë†ğ‘¦

(ï¸

)ï¸

poly(ğ‘…  ğ‘‘

â€²

)âˆšğ‘‡

.

where ğ‘…1  ğ‘…ğ‘¥  ğ‘…ğ‘¦  ğ‘…Î˜  ğ‘…Î¨ â‰¤ ğ‘…  ğ‘š  ğ‘›  ğ‘‘ â‰¤ ğ‘‘â€².
The regret bound follows by applying the standard regret bound of follow-the-regularized-leader
(see  e.g. [Haz16]). However  special care must be taken to ensure that the gradient and diameter
factors incur only a poly log(ğ‘‡ ) factor  noting that the discretization parameter ğ‘Š (one of the di-
mensions of ğ‘€ and ğ‘) must depend polynomially on ğ‘‡ /ğœ€ in order for the class of pseudo-LDSs
to approximate true LDSs up to error ğœ€. To this end  we use a modiï¬cation of the strongly convex
matrix regularizer found in [KSST12]  resulting in a regret bound with logarithmic dependence on
ğ‘Š .
Intuitively  this is possible due to the ğ‘‘-sparsity (and thus â„“1 boundedness) of the phases of true
LDSs  which transfers to an â„“1 bound (in the phase dimension only) on pseudo-LDSs that compete
with LDSs of the same size. This allows us to formulate a second convex relaxation  on top of that of
wave-ï¬ltering  for simultaneous identiï¬cation of eigenvalue phase and magnitude. For the complete
theorem statement and proof  see Appendix B.
We note that the regret analysis can be used directly with the approximation result for autoregressive
models (Theorem 1)  without wave-ï¬ltering. This way  one can straightforwardly obtain a sublinear
regret bound against autoregressive models with bounded coefï¬cients. However  for the reasons
discussed in Section 4.1  the wave-ï¬ltering technique affords us a much stronger end-to-end result.

4.3 Pseudo-LDSs compete with true LDSs

Appendix C (Lemma 14) that if the noise is bounded (âˆ‘ï¸€ğ‘‡

Theorem 3 shows that there exists a pseudo-LDS approximating the actual LDS to within ğœ€ in the
noiseless case. We next need to analyze the best approximation when there is noise. We show in
2 â‰¤ ğ¿)  we incur an
additional term equal to the size of the perturbation âˆšğ¿ times a competitive ratio depending on the
2 ğ‘…Î˜ğ‘…Î¨âˆšğ¿. We show this by showing that any noise has a
dynamical system  for a total of ğ‘…âˆğœ
bounded effect on the predictions of the pseudo-LDS.2
Letting Ë†ğ‘¦*

ğ‘¡ be the predictions of the best pseudo-LDS  we have

ğ‘¡=1 â€–ğœ‚ğ‘¡â€–2

2 + â€–ğœ‰ğ‘¡â€–2

3

ğ‘‡âˆ‘ï¸
ğ‘¡=1 â€–Ë†ğ‘¦ğ‘¡ âˆ’ ğ‘¦ğ‘¡â€–2
2 =

(ï¸ƒ ğ‘‡âˆ‘ï¸
ğ‘¡=1 â€–Ë†ğ‘¦ğ‘¡ âˆ’ ğ‘¦ğ‘¡â€–2
2 âˆ’

)ï¸ƒ

ğ‘‡âˆ‘ï¸
*
ğ‘¡ âˆ’ Ë†ğ‘¦ğ‘¡â€–2
ğ‘¡=1 â€–Ë†ğ‘¦

2

ğ‘‡âˆ‘ï¸
*
ğ‘¡ âˆ’ Ë†ğ‘¦ğ‘¡â€–2
ğ‘¡=1 â€–Ë†ğ‘¦
2 .

+

(7)

2In other words  the prediction error of the pseudo-LDS is stable to noise  and we bound its ğ»âˆ norm.

7

The ï¬rst term is the regret  bounded by Theorem 4 and the second term is bounded by the discussion
above  giving the bound in the Theorem 1.
For the complete proof  see Appendix C.2.

5 Experiments

We exhibit two experiments on synthetic time series  which are generated by randomly-generated
ill-conditioned LDSs. In both cases  ğ´ âˆˆ R10Ã—10 is a block-diagonal matrix  whose 2-by-2 blocks
are rotation matrices [cos ğœƒ âˆ’ sin ğœƒ; sin ğœƒ cos ğœƒ] for phases ğœƒ drawn uniformly at random. This
comprises a hard case for direct system identiï¬cation: long-term time dependences between input
and output  and the optimization landscape is non-convex  with many local minima. Here  ğµ âˆˆ
R10Ã—10 and ğ¶ âˆˆ R2Ã—10 are random matrices of standard i.i.d. Gaussians. In the ï¬rst experiment 
the inputs ğ‘¥ğ‘¡ are i.i.d. spherical Gaussians; in the second  the inputs are Gaussian block impulses.

Figure 1: Performance of Algorithm 1 on synthetic 10-dimensional LDSs. For clarity  error plots
are smoothed by a median ï¬lter. Blue = ours  yellow = EM  red = SSID  black = true responses 
green = inputs  dotted lines = â€œguess the previous outputâ€ baseline. Horizontal axis is time. Left:
Gaussian inputs; SSID fails to converge  while EM ï¬nds a local optimum. Right: Block impulse
inputs; both baselines ï¬nd local optima.

We make a few straightforward modiï¬cations to Algorithm 1  for practicality. First  we replace
the scalar autoregressive parameters with matrices ğ›½ğ‘— âˆˆ Rğ‘šÃ—ğ‘š. Also  for performance reasons 
we use ridge regularization instead of the prescribed pseudo-LDS regularizer with composite norm
constraint. We choose an autoregressive parameter of ğœ = ğ‘‘ = 10 (in accordance with the theory) 
and ğ‘Š = 100.
As shown in Figure 1  our algorithm signiï¬cantly outperforms the baseline methods of system iden-
tiï¬cation followed by Kalman ï¬ltering. The EM and subspace identiï¬cation (SSID; see [VODM12])
algorithms ï¬nds a local optimum; in the experiment with Gaussian inputs  the latter failed to con-
verge (left).
We note that while the main online algorithm from [HSZ17]  Algorithm 1 is signiï¬cantly faster
than baseline methods  ours is not. The reason is that we incur at least an extra factor of ğ‘Š to
compute and process the additional convolutions. To remove this phase discretization bottleneck 
many heuristics are available for phase identiï¬cation; see Chapter 6 of [Lju98].

6 Conclusion

We gave the ï¬rst  to the best of our knowledge  polynomial-time algorithm for prediction in the
general LDS setting without dependence on the spectral radius parameter of the underlying system.
Our algorithm combines several techniques  namely the recently introduced wave-ï¬ltering method 
as well as convex relaxation and linear ï¬ltering.

8

System1:MIMOwithGaussianinputsâˆ’10010Timeseries(xt yt)ytxt0200400600800100010âˆ’3100103106Error||Ë†ytâˆ’yt||2EMSSIDoursË†yt=ytâˆ’1System2:MIMOwithblockinputsâˆ’10010xt(1)yt(1)0200400600800100010âˆ’410âˆ’2100102EMSSIDoursË†yt=ytâˆ’1One important future direction is to improve the regret in the setting of (non-adversarial) Gaussian
noise. In this setting  if the LDS is explicitly identiï¬ed  the best predictor is the Kalman ï¬lter  which 
when unrolled  depends on feedback for all previous time steps  and only incurs a cost ğ‘‚(ğ¿) from
noise in (4). It is of great theoretical and practical interest to compete directly with the Kalman ï¬lter
without system identiï¬cation.

References
[AHMS13] Oren Anava  Elad Hazan  Shie Mannor  and Ohad Shamir. Online learning for time
series prediction. In COLT 2013 - The 26th Annual Conference on Learning Theory 
June 12-14  2013  Princeton University  NJ  USA  pages 172â€“184  2013.

[AYS11] Yasin Abbasi-Yadkori and Csaba SzepesvÃ¡ri. Regret bounds for the adaptive control of
linear quadratic systems. In Proceedings of the 24th Annual Conference on Learning
Theory  pages 1â€“26  2011.

[BD09] P. Brockwell and R. Davis. Time Series: Theory and Methods. Springer  2 edition 

2009.

[BJR94] G. Box  G. Jenkins  and G. Reinsel. Time Series Analysis: Forecasting and Control.

Prentice-Hall  3 edition  1994.

[BK15] David Belanger and Sham Kakade. A linear dynamical system model for text.

International Conference on Machine Learning  pages 833â€“842  2015.

In

[BLV98] Daniel L Boley  Franklin T Luk  and David Vandevoorde. A fast method to diagonalize

a Hankel matrix. Linear algebra and its applications  284(1-3):41â€“52  1998.

[BT17] Bernhard Beckermann and Alex Townsend. On the singular values of matrices
with displacement structure. SIAM Journal on Matrix Analysis and Applications 
38(4):1227â€“1248  2017.

[CBL06] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge

University Press  New York  NY  USA  2006.

[Cho83] Man-Duen Choi. Tricks or treats with the hilbert matrix. The American Mathematical

Monthly  90(5):301â€“312  1983.

[DMM+17] Sarah Dean  Horia Mania  Nikolai Matni  Benjamin Recht  and Stephen Tu. On the
sample complexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688 
2017.

[HAK07] Elad Hazan  Amit Agarwal  and Satyen Kale. Logarithmic regret algorithms for online

convex optimization. Mach. Learn.  69(2-3):169â€“192  December 2007.

[Ham94] J. Hamilton. Time Series Analysis. Princeton Univ. Press  1994.

[Haz16] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in

Optimization  2(3-4):157â€“325  2016.

[Hil94] David Hilbert. Ein beitrag zur theorie des legendreâ€™schen polynoms. Acta mathemat-

ica  18(1):155â€“159  1894.

[HMR16] Moritz Hardt  Tengyu Ma  and Benjamin Recht. Gradient descent learns linear dynam-

ical systems. arXiv preprint arXiv:1609.05191  2016.

[HSZ17] Elad Hazan  Karan Singh  and Cyril Zhang. Learning linear dynamical systems via
spectral ï¬ltering. In Advances in Neural Information Processing Systems  pages 6705â€“
6715  2017.

[Kal60] Rudolph Emil Kalman. A new approach to linear ï¬ltering and prediction problems.

Journal of Basic Engineering  82.1:35â€“45  1960.

9

[KM16] Vitaly Kuznetsov and Mehryar Mohri. Time series prediction and online learning. In
Vitaly Feldman  Alexander Rakhlin  and Ohad Shamir  editors  29th Annual Confer-
ence on Learning Theory  volume 49 of Proceedings of Machine Learning Research 
pages 1190â€“1213  Columbia University  New York  New York  USA  23â€“26 Jun 2016.

[KM17] Vitaly Kuznetsov and Mehryar Mohri. Discriminative state space models. In I. Guyon 
U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett 
editors  Advances in Neural Information Processing Systems 30  pages 5671â€“5679.
Curran Associates  Inc.  2017.

[KSST12] Sham M Kakade  Shai Shalev-Shwartz  and Ambuj Tewari. Regularization techniques
for learning with matrices. Journal of Machine Learning Research  13(Jun):1865â€“
1890  2012.

[Lju98] Lennart Ljung. System identiï¬cation: Theory for the User. Prentice Hall  Upper Saddle

Riiver  NJ  2 edition  1998.

[MW07] Taesup Moon and Tsachy Weissman. Competitive on-line linear ï¬r mmse ï¬ltering. In
IEEE International Symposium on Information Theory - Proceedings  pages 1126 â€“
1130  07 2007.

[RG99] Sam Roweis and Zoubin Ghahramani. A unifying review of linear gaussian models.

Neural computation  11(2):305â€“345  1999.

[VODM12] Peter Van Overschee and BL De Moor. Subspace Identiï¬cation for Linear Systems.

Springer Science & Business Media  2012.

[ZDG+96] Kemin Zhou  John Comstock Doyle  Keith Glover  et al. Robust and optimal control 

volume 40. Prentice hall New Jersey  1996.

10

,Elad Hazan
Holden Lee
Karan Singh
Cyril Zhang
Yi Zhang
Tianbo Li
Yiping Ke