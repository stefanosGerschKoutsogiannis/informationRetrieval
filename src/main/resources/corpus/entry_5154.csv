2018,Spectral Filtering for General Linear Dynamical Systems,We give a polynomial-time algorithm for learning latent-state linear dynamical systems without system identification  and without assumptions on the spectral radius of the system's transition matrix. The algorithm extends the recently introduced technique of spectral filtering  previously applied only to systems with a symmetric transition matrix  using a novel convex relaxation to allow for the efficient identification of phases.,Spectral Filtering for

General Linear Dynamical Systems

Elad Hazan

Princeton University & Google AI Princeton

ehazan@cs.princeton.edu

Holden Lee

Princeton University

holdenl@princeton.edu

Karan Singh

Princeton University & Google AI Princeton

karans@cs.princeton.edu

Cyril Zhang

Princeton University & Google AI Princeton

cyril.zhang@cs.princeton.edu

Yi Zhang

Princeton University & Google AI Princeton

y.zhang@cs.princeton.edu

Abstract

We give a polynomial-time algorithm for learning latent-state linear dynamical
systems without system identiﬁcation  and without assumptions on the spectral
radius of the system’s transition matrix. The algorithm extends the recently in-
troduced technique of spectral ﬁltering  previously applied only to systems with
a symmetric transition matrix  using a novel convex relaxation to allow for the
efﬁcient identiﬁcation of phases.

1

Introduction

Linear dynamical systems (LDSs) are a cornerstone of signal processing and time series analysis.
The problem of predicting the response signal arising from a LDS is a fundamental problem in
machine learning  with a history of more than half a century.
An LDS is given by matrices (𝐴  𝐵  𝐶  𝐷). Given a sequence of inputs {𝑥𝑡}  the output {𝑦𝑡} of the
system is governed by the linear equations

ℎ𝑡 = 𝐴ℎ𝑡−1 + 𝐵𝑥𝑡 + 𝜂𝑡
𝑦𝑡 = 𝐶ℎ𝑡 + 𝐷𝑥𝑡 + 𝜉𝑡 

(1)

where 𝜂𝑡  𝜉𝑡 are noise vectors  and ℎ𝑡 is a hidden (latent) state.
Roweis and Ghahramani [RG99] show that special cases of this formulation capture a host of ma-
chine learning models  including hidden Markov models  Gaussian mixture models  principal com-
ponent analysis  and linear Gaussian models. It has been observed numerous times in the literature
that if there is no hidden state  or if the transition matrices are known  then the formulation is essen-
tially convex and amenable to efﬁcient optimization.
In this paper we are concerned with the general and more challenging case  arguably the one which
is more applicable as well  in which the hidden state is not observed  and the system dynamics
are unknown to the learner. In this setting  despite the vast literature on the subject from various
communities  there is a lack of provably efﬁcient methods for learning the LDS without strong
generative or other assumptions.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Building on recent advances in spectral ﬁltering  we develop a novel convex relaxation for LDSs 
resulting in an efﬁcient algorithm for the LDS prediction problem in the general setting. Our al-
gorithm makes online predictions which are close (in terms of mean squared error) to those of the
optimal LDS in hindsight.

1.1 Problem statement and our results

An LDS prediction problem is deﬁned as follows. Iteratively for 𝑡 = 1  2  ...  𝑇   the learner observes
the input to the system 𝑥𝑡 ∈ R𝑛. The learner then makes a prediction ˆ𝑦𝑡 ∈ R𝑚  observes true
outcome 𝑦𝑡 ∈ R𝑚  and suffers a loss ℓ(ˆ𝑦𝑡  𝑦𝑡). For simplicity we consider the mean square error
ℓ(ˆ𝑦𝑡  𝑦𝑡) = ‖ˆ𝑦𝑡 − 𝑦𝑡‖2  even though our techniques can handle any Lipschitz convex loss.
The goal of the online learner is to minimize its regret  or difference in loss between its prediction 
and the prediction of the best LDS in hindsight that predicts with 𝑦*

1  . . .   𝑦*
𝑇 :

𝑇∑︁
𝑡=1 ‖ˆ𝑦𝑡 − 𝑦𝑡‖2 −

𝑇∑︁
𝑡=1 ‖𝑦

*
𝑡 − 𝑦𝑡‖2.

Regret(𝑇 ) :=

𝑡 are not ﬁxed vectors  but rather evolve according to a hidden state and equation

We emphasize that 𝑦*
(1) according to the best possible transition matrices  in terms of mean square error ﬁt to the data.
Our main result is a polynomial-time algorithm that predicts ˆ𝑦𝑡 given all previous input and feedback
(𝑥1:𝑡  𝑦1:𝑡−1)  and attains a near-optimal regret bound of

Regret(𝑇 ) ≤ ˜𝑂(√𝑇 ) + 𝐾 · 𝐿.

Here  𝐿 denotes the inevitable loss incurred by perturbations to the system which cannot be antici-
pated by the learner  which are allowed to be adversarial. This 𝐿 can grow with time  and is usually
assumed to be proportional to a small constant  say 𝜀𝑇 .
The constant in the ˜𝑂(·)  as well as 𝐾  depend polynomially on the dimensionality of the system 
the norms of the inputs and outputs  and certain natural quantities related to the transition matrix
𝐴. Additionally  the running time of our algorithm is polynomial in all natural parameters of the
problem.
In comparison to previous approaches  we note:

∙ Our algorithm is the ﬁrst sample-efﬁcient and polynomial-time algorithm with this guaran-
tee. In the next section  we survey local search algorithms that either only converge to local
optima or require an exponential number of iterations in the worst case.

∙ The main feature is that the regret does not depend on the spectral radius 𝜌(𝐴) of the sys-
tem’s hidden-state transition matrix. If one allows a dependence on the condition number 
then simple linear regression-based algorithms are known to obtain the same result  with
time and sample complexity polynomial in

1−𝜌(𝐴). (See Section 6 of [HMR16].)

1

1.2 Related work

The prediction problems of time series for linear dynamical systems was deﬁned in the seminal work
of Kalman [Kal60]  who introduced the Kalman ﬁlter as a recursive least-squares solution for max-
imum likelihood estimation (MLE) of Gaussian perturbations to the system. For more background
see the classic survey [Lju98]  and the extensive overview of recent literature in [HMR16].
For a linear dynamical system with no hidden state  the system is identiﬁable by a convex program
and thus well understood (see [DMM+17  AYS11]  who address sample complexity issues and
regret for system identiﬁcation and linear-quadratic control in this setting).
Various exponential-time approaches have been proposed to learn the system in the case that the sys-
tem is unknown. Regret bounds similar to ours are obtainable using the continuous multiplicative-
weights algorithm (see [CBL06]  as well as the EWOO algorithm in [HAK07]). These methods 
mentioned brieﬂy in [HSZ17]  basically amount to discretizing the entire parameter space of LDSs 
and take time exponential in the system dimensions. Stronger guarantees are obtained in [KM17] 
though still in exponential time.

2

Ghahramani and Roweis [RG99] suggest using the EM algorithm to learn the parameters of an
LDS. This approach remains widely used  but is inherently non-convex and can get stuck in local
minima. Recently [HMR16] show that for a restricted class of systems  gradient descent (also widely
used in practice  perhaps better known in this setting as backpropagation) guarantees polynomial
convergence rates and sample complexity in the batch setting. Their result applies essentially only
to the SISO case  depends polynomially on the spectral gap  and requires the signal to be generated
by an LDS.
In recent work  [HSZ17] show how to efﬁciently learn an LDS in the online prediction setting 
without any generative assumptions  and without dependence on the condition number. Their
new methodology  however  was restricted to LDSs with symmetric transition matrices. For
the structural result  we use the same results from the spectral theory of Hankel matrices; see
[BT17  Hil94  Cho83]. Obtaining provably efﬁcient algorithms for the general case is signiﬁcantly
more challenging.
We make use of linear ﬁltering  or linear regression on the past observations as well as inputs  as a
subroutine for future prediction. This technique is well-established in the context of autoregressive
models for time-series prediction that have been extensively studied in the learning and signal-
processing literature  see e.g. [Ham94  BJR94  BD09  KM16  AHMS13  MW07].
The recent success of recurrent neural networks (RNNs) for tasks such as speech and language
modeling has inspired a resurgence of interest in linear dynamical systems [HMR16  BK15].

2 Preliminaries

2.1 Setting
A linear dynamical system Θ = (𝐴  𝐵  𝐶  𝐷)  with initial hidden state ℎ0 ∈ R𝑑  speciﬁes a map
from inputs 𝑥1  . . .   𝑥𝑇 ∈ R𝑛 to outputs (responses) 𝑦1  . . .   𝑦𝑇 ∈ R𝑚  given by the recursive
equations
(2)
(3)

ℎ𝑡 = 𝐴ℎ𝑡−1 + 𝐵𝑥𝑡 + 𝜂𝑡
𝑦𝑡 = 𝐶ℎ𝑡 + 𝐷𝑥𝑡 + 𝜉𝑡 

where 𝐴  𝐵  𝐶  𝐷 are matrices of appropriate dimension  and 𝜂𝑡  𝜉𝑡 are noise vectors.
We make the following assumptions to characterize the “size” of an LDS we are competing against:

1. Inputs and outputs and bounded: ‖𝑥𝑡‖2 ≤ 𝑅𝑥 ‖𝑦𝑡‖2 ≤ 𝑅𝑦.1
2. The system is Lyapunov stable  i.e.  the largest singular value of 𝐴 is at most 1: 𝜌(𝐴) ≤ 1.
3. 𝐴 is diagonalizable by a matrix with small entries: 𝐴 = ΨΛΨ−1  with ‖Ψ‖𝐹

Note that we do not need this parameter to be bounded away from 1.

𝑅Ψ. Intuitively  this holds if the eigenvectors corresponding to larger eigenvalues aren’t
close to linearly dependent.

⃦⃦Ψ−1⃦⃦𝐹 ≤

4. 𝐵  𝐶  𝐷 have bounded spectral norms: ‖𝐵‖2  ‖𝐶‖2  ‖𝐷‖2 ≤ 𝑅Θ.
5. Let 𝑆 =

{︁ 𝛼|𝛼| : 𝛼 is an eigenvalue of 𝐴

be the set of phases of all eigenvalues of 𝐴. There
exists a monic polynomial 𝑝(𝑥) of degree 𝜏 such that 𝑝(𝜔) = 0 for all 𝜔 ∈ 𝑆  the 𝐿1 norm
of its coefﬁcients is at most 𝑅1  and the 𝐿∞ norm is at most 𝑅∞. We will explain this
condition in Section 4.1.

}︁

In our regret model  the adversary chooses an LDS (𝐴  𝐵  𝐶  𝐷)  and has a budget 𝐿. The dynamical
system produces outputs given by the above equations  where the noise vectors 𝜂𝑡  𝜉𝑡 are chosen

adversarially  subject to a budget constraint:∑︀𝑇

𝑡=1 ‖𝜂𝑡‖2 + ‖𝜉𝑡‖2 ≤ 𝐿.

Then  the online prediction setting is identical to that proposed in [HSZ17]. For each iteration
𝑡 = 1  . . .   𝑇   the input 𝑥𝑡 is revealed  and the learner must predict a response ˆ𝑦𝑡. Then  the true 𝑦𝑡
is revealed  and the learner suffers a least-squares loss of ‖𝑦𝑡 − ˆ𝑦𝑡‖2. Of course  if 𝐿 scales with
1Note that no bound on ‖𝑦𝑡‖ is required for the approximation theorem; 𝑅𝑦 only appears in the regret

bound.

3

‖𝑀‖2 𝑞 :=

and the limiting case

⎞⎠𝑞/2⎤⎥⎦1/𝑞

 

𝑀 (𝑝  ℎ  𝑗  𝑖)2

⎡⎢⎣∑︁

𝑝

⎛⎝∑︁
√︃∑︁

ℎ 𝑖 𝑗

ℎ 𝑖 𝑗

the time horizon 𝑇   it is information-theoretically impossible for an online algorithm to incur a loss
sublinear in 𝑇   even under non-adversarial (e.g. Gaussian) perturbations. Thus  our end-to-end goal
is to track the LDS with loss that scales with the total magnitude of the perturbations  independently
of 𝑇 .
This formulation is fundamentally a min-max problem: given a limited budget of perturbations  an
adversary tries to maximize the error of the algorithm’s predictions  while the algorithm seeks to be
robust against any such adversary. This corresponds to the 𝐻∞ notion of robustness in the control
theory literature; see Section 15.5 of [ZDG+96].

2.2 Spectral ﬁltering for time series

The spectral ﬁltering technique is introduced in [HSZ17]  which considers a spectral decomposition
of the derivative of the impulse response function of an LDS with a symmetric transition matrix. A
crucial object of consideration in spectral ﬁltering is the set of wave-ﬁlters 𝜑1  . . .   𝜑𝑘  which are
the top 𝑘 eigenvectors of the deterministic Hankel matrix 𝑍𝑇 ∈ R𝑇×𝑇   whose entries are given by
(𝑖+𝑗)3−(𝑖+𝑗). Bounds on the 𝜀-rank of positive semideﬁnite Hankel matrices can be found
𝑍(𝑖  𝑗) =
in [BT17]. Our algorithm will “compress” the input time series using a time-domain convolution of
the input time series with ﬁlters derived from these eigenvectors.

2

2.3 Notation for matrix norms

We will consider a few “mixed” ℓ𝑝 matrix norms of a 4-tensor 𝑀  whose elements are indexed
by 𝑀 (𝑝  ℎ  𝑗  𝑖) (the roles and bounds of these indices will be introduced later). For conciseness 
whenever the norm of such a 4-tensor is taken  we establish the notation for the mixed matrix norm

‖𝑀‖2 ∞ := max

𝑝

𝑀 (𝑝  ℎ  𝑗  𝑖)2.

These are the straightforward analogues of the matrix norms deﬁned in [KSST12]  and appear in the
regularization of the online prediction algorithm.

3 Algorithm and main theorem

We begin by describing the algorithm in terms of a linear model 𝑦( ˆΘ𝑡; 𝑥1:𝑡; 𝑦𝑡−1:𝑡−𝜏 )  the details of
which occur in Deﬁnition 2.

𝑗=1  the top 𝑘 eigenpairs of 𝑍𝑇 .

Algorithm 1 Phased wave-ﬁltered regression
1: Input: time horizon 𝑇   parameters 𝑘  𝑊  𝜏  𝑅 ^Θ  regularization weight 𝜂.
2: Compute {(𝜎𝑗  𝜑𝑗)}𝑘
3: Initialize ˆΘ1 ∈ 𝒦 arbitrarily.
4: for 𝑡 = 1  . . .   𝑇 do
5:
6:
7:

Predict ˆ𝑦𝑡 := 𝑦( ˆΘ𝑡; 𝑥1:𝑡; 𝑦𝑡−1:𝑡−𝜏 ).
Observe 𝑦𝑡. Suffer loss ‖𝑦𝑡 − ˆ𝑦𝑡‖2.
𝑡−1∑︁
Solve FTRL convex program:
𝑢=0‖𝑦( ˆΘ; 𝑥1:𝑢  𝑦𝑢−1:𝑢−𝜏 ) − 𝑦𝑢‖2 +

ˆΘ𝑡+1 ← arg min
^Θ∈𝒦

𝑅( ˆΘ).

1
𝜂

8: end for

The central result in the paper is stated below.

4

Theorem 1 (Main; informal). Consider a LDS with noise (given by (2) and (3)) satisfying the
assumptions in Section 2.1  where total noise is bounded by 𝐿. Then there is a choice of parameters
such that Algorithm 1 learns a linear model ˆΘ whose predictions ˆ𝑦𝑡 satisfy

poly(𝑅  𝑑

′

)√𝑇 + 𝑅2∞𝜏 3𝑅2

Θ𝑅2

Ψ𝐿

(4)

)︁

𝑇∑︁
𝑡=1 ‖ˆ𝑦𝑡 − 𝑦𝑡‖2 ≤ ˜𝑂

(︁

where 𝑅1  𝑅𝑥  𝑅𝑦  𝑅Θ  𝑅Ψ ≤ 𝑅  𝑚  𝑛  𝑑 ≤ 𝑑′.
To deﬁne the algorithm  we specify a reparameterization of linear dynamical systems. To this end 
we deﬁne a pseudo-LDS  which pairs a subspace-restricted linear model of the impulse response
with an autoregressive model:
Deﬁnition 2. A pseudo-LDS ˆΘ = (𝑀  𝑁  𝛽  𝑃 ) is given by two 4-tensors 𝑀  𝑁 ∈ R𝑊×𝑘×𝑛×𝑚 a
vector 𝛽 ∈ R𝜏   and matrices 𝑃0  . . .   𝑃𝜏−1 ∈ R𝑚×𝑛. Let the prediction made by ˆΘ  which depends
on the entire history of inputs 𝑥1:𝑡 and 𝜏 past outputs 𝑦𝑡−1:𝑡−𝜏 be given by

𝜏∑︁

𝑢=1

[︃(︃

𝜏−1∑︁
(︂ 2𝜋𝑢𝑝

𝑗=0

𝑊

𝑦( ˆΘ; 𝑥1:𝑡  𝑦𝑡−1:𝑡−𝜏 )(:) :=

𝛽𝑢𝑦𝑡−𝑢 +

𝑃𝑗𝑥𝑡−𝑗

𝑊−1∑︁

𝑛∑︁

𝑘∑︁

𝑡∑︁

𝑝=0

𝑖=1

ℎ=1

𝑢=𝜏

+

𝑀 (𝑝  ℎ  𝑖  :) cos

)︂

(︂ 2𝜋𝑢𝑝

)︂)︃

𝑊

+ 𝑁 (𝑝  ℎ  𝑖  :) sin

]︃

1
4

ℎ 𝜑ℎ(𝑢)𝑥𝑡−𝑢(𝑖)
𝜎

Here  𝜑1  . . .   𝜑𝑘 ∈ R𝑇 are the top 𝑘 eigenvectors  with eigenvalues 𝜎1  . . .   𝜎𝑘  of 𝑍𝑇 . These
can be computed using specialized methods [BLV98]. Some of the dimensions of these tensors are
parameters to the algorithm  which we list here:

∙ Number of ﬁlters 𝑘.
∙ Phase discretization parameter 𝑊 .
∙ Autoregressive parameter 𝜏.
Additionally  we deﬁne the following:

𝑞′ +∑︀𝜏

𝐹   where 𝑞 =

𝑗=1 ‖𝑃𝑗‖2

√︁∑︀𝜏

∙ Regularizer 𝑅(𝑀  𝑁  𝛽  𝑃 ) := ‖𝑀‖2

ln(𝑊 )−1  and 𝑞′ = ln(𝜏 )
ln(𝜏 )−1.

ln(𝑊 )

2 𝑞 + ‖𝑁‖2

2 𝑞 + ‖𝛽‖2

∙ Composite norm ‖(𝑀  𝑁  𝛽  𝑃 )‖ := ‖𝑀‖2 1 + ‖𝑁‖2 1 + ‖𝛽‖1 +
∙ Composite norm constraint 𝑅 ^Θ  and the corresponding set of pseudo-LDSs 𝒦 = { ˆΘ :

𝐹 .
𝑗=1 ‖𝑃𝑗‖2

‖ ˆΘ‖ ≤ 𝑅 ^Θ}.

Crucially  𝑦( ˆΘ; 𝑥1:𝑡  𝑦𝑡−1:𝑡−𝑑) is linear in each of 𝑀  𝑁  𝑃  𝛽; consequently  the least-squares loss
‖𝑦( ˆΘ; 𝑥1:𝑡) − 𝑦‖2 is convex  and can be minimized in polynomial time. To this end  our online pre-
diction algorithm is follow-the-regularized-leader (FTRL)  which requires the solution of a convex
program at each iteration. We choose this regularization to obtain the strongest theoretical guarantee 
and provide a brief note in Section 5 on alternatives to address performance issues.
At a high level  our algorithm works by ﬁrst approximating the response of an LDS by an au-
toregressive model of order (𝜏  𝜏 )  then reﬁning the approximation using wave-ﬁlters with a phase
component. Speciﬁcally  the blocks of 𝑀 and 𝑁 corresponding to ﬁlter index ℎ and phase index 𝑝
specify the linear dependence of 𝑦𝑡 on a certain convolution of the input time series  whose kernel
is the pointwise product of 𝜑ℎ and a sinusoid with period 𝑊/𝑝. The structural result which drives
the theorem is that the dynamics of any true LDS are approximated by such a pseudo-LDS  with
reasonably small parameters and coefﬁcients.
Note that the autoregressive component in our deﬁnition of a pseudo-LDS is slightly more restricted
than multivariate autoregressive models: the coefﬁcients 𝛽𝑗 are scalar  rather than allowed to be

5

arbitrary matrices. These options are interchangeable for our purposes  without affecting the asymp-
totic regret; we choose to use scalar coefﬁcients for a more streamlined analysis.
The online prediction algorithm is fully speciﬁed in Algorithm 1; the parameter choices that give the
best asymptotic theoretical guarantees are speciﬁed in the appendix  while typical realistic settings
are outlined in Section 5.

4 Analysis

There are three parts to the analysis  which we outline in the following subsections: proving the ap-
proximability of an LDS by a pseudo-LDS  bounding the regret incurred by the algorithm against the
best pseudo-LDS  and ﬁnally analyzing the effect of noise 𝐿. The full proofs are in Appendices A  B 
and C  respectively.

4.1 Approximation theorem for general LDSs

We develop a more general analogue of the structural result from [HSZ17]  which holds for systems
with asymmetric transition matrix 𝐴.
Theorem 3 (Approximation theorem; informal). Consider an noiseless LDS (given by (2) and (3)
with 𝜂𝑡  𝜉𝑡 = 0) satisfying the assumptions in Section 2.1.

)︀)︀  𝑊 = 𝑂 (poly(𝜏  𝑅Θ  𝑅Ψ  𝑅1  𝑅𝑥  𝑇 )) and a

There is 𝑘 = 𝑂(︀poly log(︀𝑇  𝑅Θ  𝑅Ψ  𝑅1  𝑅𝑥  1

pseudo-LDS ˆΘ of norm 𝑂(poly(𝑅Θ  𝑅Ψ  𝑅1  𝜏  𝑘)) such that ˆΘ approximates 𝑦𝑡 to within 𝜀 for
1 ≤ 𝑡 ≤ 𝑇 :

𝜀

⃦⃦⃦𝑦( ˆΘ; 𝑥1:𝑡  𝑦𝑡−1:𝑡−𝜏 ) − 𝑦𝑡

⃦⃦⃦ ≤ 𝜀.

(5)

𝑑

𝑑

For the formal statement (with precise bounds) and proof  see Appendix A.2. In this section we give
some intuition for the conditions and an outline of the proof.
First  we explain the condition on the polynomial 𝑝. As we show in Appendix A.1 we can predict
using a pure autoregressive model  without waveﬁlters  if we require 𝑝 to have all eigenvalues of
𝐴 as roots (i.e.  it is divisible by the minimal polynomial of 𝐴). However  the coefﬁcients of this
polynomial could be very large. The size of these coefﬁcients will appear in the bound for the main
theorem  as using large coefﬁcients in the predictor will make it sensitive to noise.
Requiring 𝑝 only to have the phases of eigenvalues of 𝐴 as roots can decrease the coefﬁcients
signiﬁcantly. As an example  consider if 𝐴 has many 𝑑/3 distinct eigenvalues with phase 1  and
similarly for 𝜔  and 𝜔  and suppose their absolute values are close to 1. Then the minimal polynomial
is approximately (𝑥− 1)
3 which can have coefﬁcients as large as exp(Ω(𝑑)). On
the other hand  for the theorem we can take 𝑝(𝑥) = (𝑥 − 1)(𝑥 − 𝜔)(𝑥 − 𝜔) which has degree 3 and
coefﬁcients bounded by a constant. Intuitively  the waveﬁlters help if there are few distinct phases 
or they are well-separated (consider that if the phases were exactly the 𝑑th roots of unity  that 𝑝 can
be taken to be 𝑥𝑑 − 1). Note that when the roots are real  we can take 𝑝 = 𝑥 − 1 and the analysis
reduces to that of [HSZ17].
We now sketch a proof of Theorem 3. Motivation is given by the Cayley-Hamilton Theorem  which
says that if 𝑝 is the characteristic polynomial of 𝐴  then 𝑝(𝐴) = 𝑂. This fact tells us that the
𝑡=1 𝛽𝑗𝑥𝜏−𝑗  then

ℎ𝑡 = 𝐴𝑡ℎ0 satisﬁes a linear recurrence of order 𝜏 = deg 𝑝: if 𝑝(𝑥) = 𝑥𝜏 +∑︀𝜏
ℎ𝑡 +∑︀𝜏
If 𝑝 has only the phases as the roots  then ℎ𝑡 +∑︀𝜏
𝛼 = 𝑟𝜔 with |𝜔| = 1. Suppose 𝑝(𝑥) = 𝑥𝜏 +∑︀𝜏

𝑡=1 𝛽𝑗ℎ𝑡−𝑗 ̸= 0 but can be written in terms of
the waveﬁlters. Consider for simplicity the 1-dimensional (complex) LDS 𝑦𝑡 = 𝛼𝑦𝑡−1 + 𝑥𝑡  and let
𝑡=1 𝛽𝑗𝑥𝜏−𝑗 = 0 and 𝑝(𝜔) = 0. In general the LDS
is a “sum” of LDS’s that are in this form. Summing the past 𝜏 terms with coefﬁcients given by 𝛽 

𝑡=1 𝛽𝑗ℎ𝑡−𝑗 = 0.

3 (𝑥− 𝜔)

3 (𝑥− 𝜔)

𝑑

𝑦𝑡 = 𝑥𝑡 +𝛼𝑥𝑡−1 +···

+𝛼𝜏 𝑥𝑡−𝜏

+···
+··· +𝛼𝜏−1𝑥𝑡−𝜏 +··· )
...

...
𝑥𝑡−𝜏

+··· )

+𝛽1(𝑦𝑡−1 =
...
+𝛽𝜏 (𝑦𝑡−𝜏 =

𝑥𝑡−1

6

The terms 𝑥𝑡  . . .   𝑥𝑡−𝜏 +1 can be taken care of by linear regression. Consider a term 𝑥𝑗  𝑗 < 𝑡 − 𝜏
in this sum. The coefﬁcient is 𝛼𝑗−(𝑡−𝜏 )(𝛼𝜏 + 𝛽1𝛼𝜏−1 + ··· + 𝛽𝜏 ). Because 𝑝(𝜔) = 0  this can be
written as
(6)
Factoring out 1− 𝑟 from each of these terms show that 𝑦𝑡 + 𝛽1𝑦𝑡−1 +··· + 𝛽𝜏 𝑦𝑡−𝜏 can be expressed
as a function of a convolution of the vector ((1 − 𝑟)𝑟𝑡−1𝜔𝑡−1) with 𝑥1:𝑇 . The waveﬁlters were
designed precisely to approximate the vector 𝜇(𝑟) = ((1− 𝑟)𝑟𝑡−1)1≤𝑡≤𝑇 well  hence 𝑦𝑡 + 𝛽1𝑦𝑡−1 +
··· + 𝛽𝜏 𝑦𝑡−𝜏 can be approximated using the waveﬁlters multiplied by phase and convolved with 𝑥.
Note that the 1 − 𝑟 is necessary in order to make the 𝐿2 norm of ((1 − 𝑟)𝑟𝑡−1)1≤𝑡≤𝑇 bounded  and
hence ensure the waveﬁlters have bounded coefﬁcients.

𝛼𝑗−(𝑡−𝜏 )((𝛼𝜏 − 𝜔𝜏 ) + 𝛽1(𝛼𝜏−1 − 𝜔𝜏−1) + ··· ).

4.2 Regret bound for pseudo-LDSs

As an intermediate step toward the main theorem  we show a regret bound on the total least-squares
prediction error made by Algorithm 1  compared to the best pseudo-LDS in hindsight.
Theorem 4 (FTRL regret bound; informal). Let ˆ𝑦*
𝑇 denote the predictions made by the
ﬁxed pseudo-LDS minimizing the total squared-norm error. Then  there is a choice of parameters
for which the decision set 𝒦 contains all LDSs which obey the assumptions from Section 2.1  for
which the predictions ˆ𝑦1  . . .   ˆ𝑦𝑇 made by Algorithm 1 satisfy

1  . . .   ˆ𝑦*

𝑇∑︁
𝑡=1 ‖ˆ𝑦𝑡 − 𝑦𝑡‖2 −

𝑇∑︁
*
𝑡 − 𝑦𝑡‖2 ≤ ˜𝑂
𝑡=1 ‖ˆ𝑦

(︁

)︁

poly(𝑅  𝑑

′

)√𝑇

.

where 𝑅1  𝑅𝑥  𝑅𝑦  𝑅Θ  𝑅Ψ ≤ 𝑅  𝑚  𝑛  𝑑 ≤ 𝑑′.
The regret bound follows by applying the standard regret bound of follow-the-regularized-leader
(see  e.g. [Haz16]). However  special care must be taken to ensure that the gradient and diameter
factors incur only a poly log(𝑇 ) factor  noting that the discretization parameter 𝑊 (one of the di-
mensions of 𝑀 and 𝑁) must depend polynomially on 𝑇 /𝜀 in order for the class of pseudo-LDSs
to approximate true LDSs up to error 𝜀. To this end  we use a modiﬁcation of the strongly convex
matrix regularizer found in [KSST12]  resulting in a regret bound with logarithmic dependence on
𝑊 .
Intuitively  this is possible due to the 𝑑-sparsity (and thus ℓ1 boundedness) of the phases of true
LDSs  which transfers to an ℓ1 bound (in the phase dimension only) on pseudo-LDSs that compete
with LDSs of the same size. This allows us to formulate a second convex relaxation  on top of that of
wave-ﬁltering  for simultaneous identiﬁcation of eigenvalue phase and magnitude. For the complete
theorem statement and proof  see Appendix B.
We note that the regret analysis can be used directly with the approximation result for autoregressive
models (Theorem 1)  without wave-ﬁltering. This way  one can straightforwardly obtain a sublinear
regret bound against autoregressive models with bounded coefﬁcients. However  for the reasons
discussed in Section 4.1  the wave-ﬁltering technique affords us a much stronger end-to-end result.

4.3 Pseudo-LDSs compete with true LDSs

Appendix C (Lemma 14) that if the noise is bounded (∑︀𝑇

Theorem 3 shows that there exists a pseudo-LDS approximating the actual LDS to within 𝜀 in the
noiseless case. We next need to analyze the best approximation when there is noise. We show in
2 ≤ 𝐿)  we incur an
additional term equal to the size of the perturbation √𝐿 times a competitive ratio depending on the
2 𝑅Θ𝑅Ψ√𝐿. We show this by showing that any noise has a
dynamical system  for a total of 𝑅∞𝜏
bounded effect on the predictions of the pseudo-LDS.2
Letting ˆ𝑦*

𝑡 be the predictions of the best pseudo-LDS  we have

𝑡=1 ‖𝜂𝑡‖2

2 + ‖𝜉𝑡‖2

3

𝑇∑︁
𝑡=1 ‖ˆ𝑦𝑡 − 𝑦𝑡‖2
2 =

(︃ 𝑇∑︁
𝑡=1 ‖ˆ𝑦𝑡 − 𝑦𝑡‖2
2 −

)︃

𝑇∑︁
*
𝑡 − ˆ𝑦𝑡‖2
𝑡=1 ‖ˆ𝑦

2

𝑇∑︁
*
𝑡 − ˆ𝑦𝑡‖2
𝑡=1 ‖ˆ𝑦
2 .

+

(7)

2In other words  the prediction error of the pseudo-LDS is stable to noise  and we bound its 𝐻∞ norm.

7

The ﬁrst term is the regret  bounded by Theorem 4 and the second term is bounded by the discussion
above  giving the bound in the Theorem 1.
For the complete proof  see Appendix C.2.

5 Experiments

We exhibit two experiments on synthetic time series  which are generated by randomly-generated
ill-conditioned LDSs. In both cases  𝐴 ∈ R10×10 is a block-diagonal matrix  whose 2-by-2 blocks
are rotation matrices [cos 𝜃 − sin 𝜃; sin 𝜃 cos 𝜃] for phases 𝜃 drawn uniformly at random. This
comprises a hard case for direct system identiﬁcation: long-term time dependences between input
and output  and the optimization landscape is non-convex  with many local minima. Here  𝐵 ∈
R10×10 and 𝐶 ∈ R2×10 are random matrices of standard i.i.d. Gaussians. In the ﬁrst experiment 
the inputs 𝑥𝑡 are i.i.d. spherical Gaussians; in the second  the inputs are Gaussian block impulses.

Figure 1: Performance of Algorithm 1 on synthetic 10-dimensional LDSs. For clarity  error plots
are smoothed by a median ﬁlter. Blue = ours  yellow = EM  red = SSID  black = true responses 
green = inputs  dotted lines = “guess the previous output” baseline. Horizontal axis is time. Left:
Gaussian inputs; SSID fails to converge  while EM ﬁnds a local optimum. Right: Block impulse
inputs; both baselines ﬁnd local optima.

We make a few straightforward modiﬁcations to Algorithm 1  for practicality. First  we replace
the scalar autoregressive parameters with matrices 𝛽𝑗 ∈ R𝑚×𝑚. Also  for performance reasons 
we use ridge regularization instead of the prescribed pseudo-LDS regularizer with composite norm
constraint. We choose an autoregressive parameter of 𝜏 = 𝑑 = 10 (in accordance with the theory) 
and 𝑊 = 100.
As shown in Figure 1  our algorithm signiﬁcantly outperforms the baseline methods of system iden-
tiﬁcation followed by Kalman ﬁltering. The EM and subspace identiﬁcation (SSID; see [VODM12])
algorithms ﬁnds a local optimum; in the experiment with Gaussian inputs  the latter failed to con-
verge (left).
We note that while the main online algorithm from [HSZ17]  Algorithm 1 is signiﬁcantly faster
than baseline methods  ours is not. The reason is that we incur at least an extra factor of 𝑊 to
compute and process the additional convolutions. To remove this phase discretization bottleneck 
many heuristics are available for phase identiﬁcation; see Chapter 6 of [Lju98].

6 Conclusion

We gave the ﬁrst  to the best of our knowledge  polynomial-time algorithm for prediction in the
general LDS setting without dependence on the spectral radius parameter of the underlying system.
Our algorithm combines several techniques  namely the recently introduced wave-ﬁltering method 
as well as convex relaxation and linear ﬁltering.

8

System1:MIMOwithGaussianinputs−10010Timeseries(xt yt)ytxt0200400600800100010−3100103106Error||ˆyt−yt||2EMSSIDoursˆyt=yt−1System2:MIMOwithblockinputs−10010xt(1)yt(1)0200400600800100010−410−2100102EMSSIDoursˆyt=yt−1One important future direction is to improve the regret in the setting of (non-adversarial) Gaussian
noise. In this setting  if the LDS is explicitly identiﬁed  the best predictor is the Kalman ﬁlter  which 
when unrolled  depends on feedback for all previous time steps  and only incurs a cost 𝑂(𝐿) from
noise in (4). It is of great theoretical and practical interest to compete directly with the Kalman ﬁlter
without system identiﬁcation.

References
[AHMS13] Oren Anava  Elad Hazan  Shie Mannor  and Ohad Shamir. Online learning for time
series prediction. In COLT 2013 - The 26th Annual Conference on Learning Theory 
June 12-14  2013  Princeton University  NJ  USA  pages 172–184  2013.

[AYS11] Yasin Abbasi-Yadkori and Csaba Szepesvári. Regret bounds for the adaptive control of
linear quadratic systems. In Proceedings of the 24th Annual Conference on Learning
Theory  pages 1–26  2011.

[BD09] P. Brockwell and R. Davis. Time Series: Theory and Methods. Springer  2 edition 

2009.

[BJR94] G. Box  G. Jenkins  and G. Reinsel. Time Series Analysis: Forecasting and Control.

Prentice-Hall  3 edition  1994.

[BK15] David Belanger and Sham Kakade. A linear dynamical system model for text.

International Conference on Machine Learning  pages 833–842  2015.

In

[BLV98] Daniel L Boley  Franklin T Luk  and David Vandevoorde. A fast method to diagonalize

a Hankel matrix. Linear algebra and its applications  284(1-3):41–52  1998.

[BT17] Bernhard Beckermann and Alex Townsend. On the singular values of matrices
with displacement structure. SIAM Journal on Matrix Analysis and Applications 
38(4):1227–1248  2017.

[CBL06] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge

University Press  New York  NY  USA  2006.

[Cho83] Man-Duen Choi. Tricks or treats with the hilbert matrix. The American Mathematical

Monthly  90(5):301–312  1983.

[DMM+17] Sarah Dean  Horia Mania  Nikolai Matni  Benjamin Recht  and Stephen Tu. On the
sample complexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688 
2017.

[HAK07] Elad Hazan  Amit Agarwal  and Satyen Kale. Logarithmic regret algorithms for online

convex optimization. Mach. Learn.  69(2-3):169–192  December 2007.

[Ham94] J. Hamilton. Time Series Analysis. Princeton Univ. Press  1994.

[Haz16] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in

Optimization  2(3-4):157–325  2016.

[Hil94] David Hilbert. Ein beitrag zur theorie des legendre’schen polynoms. Acta mathemat-

ica  18(1):155–159  1894.

[HMR16] Moritz Hardt  Tengyu Ma  and Benjamin Recht. Gradient descent learns linear dynam-

ical systems. arXiv preprint arXiv:1609.05191  2016.

[HSZ17] Elad Hazan  Karan Singh  and Cyril Zhang. Learning linear dynamical systems via
spectral ﬁltering. In Advances in Neural Information Processing Systems  pages 6705–
6715  2017.

[Kal60] Rudolph Emil Kalman. A new approach to linear ﬁltering and prediction problems.

Journal of Basic Engineering  82.1:35–45  1960.

9

[KM16] Vitaly Kuznetsov and Mehryar Mohri. Time series prediction and online learning. In
Vitaly Feldman  Alexander Rakhlin  and Ohad Shamir  editors  29th Annual Confer-
ence on Learning Theory  volume 49 of Proceedings of Machine Learning Research 
pages 1190–1213  Columbia University  New York  New York  USA  23–26 Jun 2016.

[KM17] Vitaly Kuznetsov and Mehryar Mohri. Discriminative state space models. In I. Guyon 
U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett 
editors  Advances in Neural Information Processing Systems 30  pages 5671–5679.
Curran Associates  Inc.  2017.

[KSST12] Sham M Kakade  Shai Shalev-Shwartz  and Ambuj Tewari. Regularization techniques
for learning with matrices. Journal of Machine Learning Research  13(Jun):1865–
1890  2012.

[Lju98] Lennart Ljung. System identiﬁcation: Theory for the User. Prentice Hall  Upper Saddle

Riiver  NJ  2 edition  1998.

[MW07] Taesup Moon and Tsachy Weissman. Competitive on-line linear ﬁr mmse ﬁltering. In
IEEE International Symposium on Information Theory - Proceedings  pages 1126 –
1130  07 2007.

[RG99] Sam Roweis and Zoubin Ghahramani. A unifying review of linear gaussian models.

Neural computation  11(2):305–345  1999.

[VODM12] Peter Van Overschee and BL De Moor. Subspace Identiﬁcation for Linear Systems.

Springer Science & Business Media  2012.

[ZDG+96] Kemin Zhou  John Comstock Doyle  Keith Glover  et al. Robust and optimal control 

volume 40. Prentice hall New Jersey  1996.

10

,Elad Hazan
Holden Lee
Karan Singh
Cyril Zhang
Yi Zhang
Tianbo Li
Yiping Ke