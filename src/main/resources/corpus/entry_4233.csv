2018,Paraphrasing Complex Network: Network Compression via Factor Transfer,Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods  a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper  we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules  which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods.,Paraphrasing Complex Network: Network

Compression via Factor Transfer

Jangho Kim

SeongUk Park

Nojun Kwak

Seoul National University

Seoul National University

Seoul National University

Seoul  Korea

kjh91@snu.ac.kr

Seoul  Korea

swpark0703@snu.ac.kr

Seoul  Korea

nojunk@snu.ac.kr

Abstract

Many researchers have sought ways of model compression to reduce the size of
a deep neural network (DNN) with minimal performance degradation in order
to use DNNs in embedded systems. Among the model compression methods  a
method called knowledge transfer is to train a student network with a stronger
teacher network. In this paper  we propose a novel knowledge transfer method
which uses convolutional operations to paraphrase teacher’s knowledge and to
translate it for the student. This is done by two convolutional modules  which are
called a paraphraser and a translator. The paraphraser is trained in an unsupervised
manner to extract the teacher factors which are deﬁned as paraphrased information
of the teacher network. The translator located at the student network extracts the
student factors and helps to translate the teacher factors by mimicking them. We
observed that our student network trained with the proposed factor transfer method
outperforms the ones trained with conventional knowledge transfer methods.

1

Introduction

In recent years  deep neural nets (DNNs) have shown their remarkable capabilities in various parts
of computer vision and pattern recognition tasks such as image classiﬁcation  object detection 
localization and segmentation. Although many researchers have studied DNNs for their application
in various ﬁelds  high-performance DNNs generally require a vast amount of computational power
and storage  which makes them difﬁcult to be used in embedded systems that have limited resources.
Given the size of the equipment we use  tremendous GPU computations are not generally available in
real world applications.
To deal with this problem  many researchers studied DNN structures to make DNNs smaller and
more efﬁcient to be applicable for embedded systems. These studies can be roughly classiﬁed into
four categories: 1) network pruning  2) network quantization  3) building efﬁcient small networks 
and 4) knowledge transfer. First  network pruning is a way to reduce network complexity by pruning
the redundant and non-informative weights in a pretrained model [26  17  7]. Second  network
quantization compresses a pretrained model by reducing the number of bits used to represent the
weight parameters of the pretrained model [20  27]. Third  Iandola et al. [13] and Howard et al. [11]
proposed efﬁcient small network models which ﬁt into the restricted resources. Finally  knowledge
transfer (KT) method is to transfer large model’s information to a smaller network [22  30  10].
Among the four approaches  in this paper  we focus on the last method  knowledge transfer. Previous
studies such as attention transfer (AT) [30] and knowledge distillation (KD) [10] have achieved
meaningful results in the ﬁeld of knowledge transfer  where their loss function can be collectively
summarized as the difference between the attention maps or softened distributions of the teacher and
the student networks. These methods directly transfer the teacher network’s softened distribution [10]
or its attention map [30] to the student network  inducing the student to mimic the teacher.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Overview of the factor transfer. In the teacher network  feature maps are transformed to
the ‘teacher factors’ by a paraphraser. The number of feature maps of a teacher network (m) are
resized to the number of feature maps of teacher factors (m × k) by a paraphrase rate k. The feature
maps of the student network are also transformed to the ‘student factors’ with the same dimension
as that of the teacher factor using a translator. The factor transfer (FT) loss is used to minimize the
difference between the teacher and the student factors in the training of the translator that generates
student factors. Factors are drawn in blue. Note that before the FT  the paraphraser is already trained
unsupervisedly by a reconstruction loss.

While these methods provide fairly good performance improvements  directly transferring the
teacher’s outputs overlooks the inherent differences between the teacher network and the student
network  such as the network structure  the number of channels  and initial conditions. Therefore 
we need to re-interpret the output of the teacher network to resolve these differences. For example 
from the perspective of a teacher and a student  we came up with a question that simply providing the
teacher’s knowledge directly without any explanation can be somewhat insufﬁcient for teaching the
student. In other words  when teaching a child  the teacher should not use his/her own term because
the child cannot understand it. On the other hand  if the teacher translates his/her terms into simpler
ones  the child will much more easily understand.
In this respect  we sought ways for the teacher network to deliver more understandable information
to the student network  so that the student comprehends that information more easily. To address
this problem  we propose a novel knowledge transferring method that leads both the student and
teacher networks to make transportable features  which we call ‘factors’ in this paper. Contrary to the
conventional methods  our method is not simply to compare the output values of the network directly 
but to train neural networks that can extract good factors and to match these factors. The neural
network that extracts factors from a teacher network is called a paraphraser  while the one that extracts
factors from a student network is called a translator. We trained the paraphraser in an unsupervised
way  expecting it to extract knowledges different from what can be obtained with supervised loss
term. At the student side  we trained the student network with the translator to assimilate the factors
extracted from the paraphraser. The overview of our proposed method is provided in Figure 1. With
various experiments  we succeeded in training the student network to perform better than the ones
with the same architecture trained by the conventional knowledge transfer methods.
Our contributions can be summarized as follows:
• We propose a usage of a paraphraser as a means of extracting meaningful features (factors) in an
unsupervised manner.
• We propose a convolutional translator in the student side that learns the factors of the teacher
network.
• We experimentally show that our approach effectively enhances the performance of the student
network.

2 Related Works

A wide variety of methods have been studied to use conventional networks more efﬁciently. In
network pruning and quantization approaches  Srinivas et al. [26] proposed a data-free pruning

2

(a) Knowledge Distillation

(b) Attention Transfer

(c) Factor Transfer (Proposed)

Figure 2: The structure of (a) KD [10]  (b) AT [30] and (c) the proposed method FT. Unlike KD and
AT  our method does not directly compare the softened distribution (KD) or the attention map (AT)
which is deﬁned as the sum of feature maps of the teacher and the student networks. Instead  we
extract factors from both the teacher and the student  whose difference is tried to be minimized.

method to remove redundant neurons. Han et al. [7] removed the redundant connection and then used
Huffman coding to quantize the weights. Gupta et al. [6] reduced ﬂoat point operation and memory
usage by using the 16 bit ﬁxed-point representation. There are also many studies that directly train
convolutional neural networks (CNN) using binary weights [3  4  20]. However  the network pruning
methods require many iterations to converge and the pruning threshold is manually set according to
the targeted amount of degradation in accuracy. Furthermore  the accuracies of binary weights are
very poor  especially in large CNNs. There are many ways to directly design efﬁcient small networks
such as SqueezeNet [13]  Mobile-Net [11] and Condense-Net [12]  which showed a vast amount of
reduction in the number of parameters compared to the original network sacriﬁcing some accuracies.
Also  there are methods of designing a network using a reinforcement learning algorithm such as
MetaQNN [2] and Neural Architecture Search [33]. Using the reinforcement learning algorithm 
the network itself searches for an efﬁcient structure without human assistance. However  they only
focused on performance without considering the number of parameters. In addition  it takes a lot of
GPU memories and time to learn.
Another method is the ‘knowledge transfer’. This is a method of training a student network with a
stronger teacher network. Knowledge distillation (KD) [10] is the early work of knowledge transfer
for deep neural networks. The main idea of KD is to shift knowledge from a teacher network to
a student network by leaning the class distribution via softened softmax. The student network can
capture not only the information provided by the true labels  but also the information from the teacher.
Yim et al. [28] deﬁned the ﬂow of solution procedure (FSP) matrix calculated by Gram matrix of
feature maps from two layers in order to transfer knowledge. In FitNet [22]  they designed the student
network to be thinner and deeper than the teacher network  and provided hints from the teacher
network for improving performance of the student network by learning intermediate representations
of the teacher network. FitNet attempts to mimic the intermediate activation map directly from the
teacher network. However  it can be problematic since there are signiﬁcant capacity differences
between the teacher and the student. Attention transfer (AT) [30]  in contrast to FitNet  trains a
less deep student network such that it mimics the attention maps of the teacher network which are
summations of the activation maps along the channel dimension. Therefore  an attention map for
a layer is of its the spatial dimensions. Figure 2 visually shows the difference of KD [10]  AT [30]
and the proposed method  factor transfer (FT). Unlike other methods  our method does not directly
compare the teacher and student networks’ softend distribution  or attention maps.
As shown in Figure 1  our paraphraser is similar to the convolutional autoencoder [18] in that it
is trained in an unsupervised manner using the reconstruction loss and convolution layers. Hinton
et al.[9] proved that autoencoders produce compact representations of images that contain enough
information for reconstructing the original images. In [16]  a stacked autoencoder on the MNIST
dataset achieved great results with a greedy layer-wise approach. Many studies show that autoencoder
models can learn meaningful  abstract features and thus achieve better classiﬁcation results in high-
dimensional data  such as images [19  24]. The architecture of our paraphraser is different from
convolutional autoencoders in that convolution layers do not downsample the spatial dimension of an
input since the paraphraser uses sufﬁciently downsampled feature maps of a teacher network as the
input.

3

3 Proposed Method

It is said that if one fully understands a thing  he/she should be able to explain it by himself/herself.
Correspondingly  if the student network can be trained to replicate the extracted information  this
implies that the student network is well informed of that knowledge. In this section  we deﬁne
the output of paraphraser’s middle layer  as ‘teacher factors’ of the teacher network  and for the
student network  we use the translator made up of several convolution layers to generate ‘student
factors’ which are trained to replicate the ‘teacher factors’ as shown in Figure 1. With these modules 
our knowledge transfer process consists of the following two main steps: 1) In the ﬁrst step  the
paraphraser is trained by a reconstruction loss. Then  teacher factors are extracted from the teacher
network by a paraphraser. 2) In the second step  these teacher factors are transferred to the student
factors such that the student network learns from them.

3.1 Teacher Factor Extraction with Paraphraser

ResNet architectures [8] have stacked residual blocks and in [30] they call each stack of residual
blocks as a ‘group’. In this paper  we will also denote each stacked convolutional layers as a ‘group’.
Yosinski et al.[29] veriﬁed lower layer features are more general and higher layer features have a
greater speciﬁcity. Since the teacher network and the student network are focusing on the same task 
we extracted factors from the feature maps of the last group as clearly can be seen in Figure 1 because
the last layer of a trained network must contain enough information for the task.
In order to extract the factor from the teacher network  we train the paraphraser in an unsupervised
way by assigning the reconstruction loss between the input feature maps x and the output feature
maps P (x) of the paraphraser. The unsupervised training act on the factor to be more meaningful 
extracting different kind of knowledge from what can be obtained with supervised cross-entropy
loss function. This approach can also be found in EBGAN [32]  which uses an autoencoder as
discriminator to give the generator different kind of knowledge from binary output.
The paraphraser uses several convolution layers to produce the teacher factor FT which is further pro-
cessed by a number of transposed convolution layers in the training phase. Most of the convolutional
autoencoders are designed to downsample the spatial dimension in order to increase the receptive
ﬁeld. On the contrary  the paraphraser maintains the spatial dimension while adjusting the number of
factor channels because it uses the feature maps of the last group which has a sufﬁciently reduced
spatial dimension. If the teacher network produces m feature maps  we resize the number of factor
channels as m × k. We refer to hyperparameter k as a paraphrase rate.
To extract the teacher factors  an adequately trained paraphraser is needed. The reconstruction loss
function used for training the paraphraser is quite simple as

(1)
where the paraphraser network P (·) takes x as an input. After training the paraphraser  it can extract
the task speciﬁc features (teacher factors) as can be seen in the supplementary material.

Lrec = (cid:107)x − P (x)(cid:107)2 

3.2 Factor Transfer with Translator

Once the teacher network has extracted the factors which are the paraphrased teacher’s knowledge 
the student network should be able to absorb and digest them on its own way. In this paper  we name
this procedure as ‘Factor Transfer’. As depicted in Figure 1  while training the student network  we
inserted the translator right after the last group of student convolutional layers.
The translator is trained jointly with the student network so that the student network can learn the
paraphrased information from the teacher network. Here  the translator plays a role of a buffer that
relieves the student network from the burden of directly learning the output of the teacher network by
rephrasing the feature map of the student network.
The student network is trained with the translator using the sum of two loss terms  i.e. the classiﬁcation
loss and the factor transfer loss:

Lstudent = Lcls + βLF T  

(2)

4

Lcls = C(S(Ix)  y) 
− FS(cid:107)FS(cid:107)2

LF T = (cid:107) FT(cid:107)FT(cid:107)2

(cid:107)p.

(3)

(4)

With (4)  the student’s translator is trained to output the student factors that mimic the teacher factors.
Here  FT and FS denote the teacher and the student factors  respectively. We set the dimension of FS
to be the same as that of FT . We also apply an l2 normalization on the factors as [30]. In this paper 
the performances using l1 loss (p = 1) is reported  but the performance difference between l1 (p = 1)
and l2 (p = 2) losses is minor (See the supplementary material)  so we consistently used l1 loss for
all experiments.
In addition to the factor transfer loss (4)  the conventional classiﬁcation loss (3) is also used to train
student network as in (2). Here  β is a weight parameter and C(S(Ix)  y) denotes the cross entropy
between ground-truth label y and the softmax output S(Ix) of the student network for an input image
Ix  a commonly used term for classiﬁcation tasks.
The translator takes the output features of the student network  and with (2)  it sends the gradient back
to the student networks  which lets the student network absorb and digest the teacher’s knowledge
in its own way. Note that unlike the training of the teacher paraphraser  the student network and its
translator are trained simultaneously in an end-to-end manner.

4 Experiments

In this section  we evaluate the proposed FT method on several datasets. First  we verify the effective-
ness of FT through the experiments with CIFAR-10 [14] and CIFAR-100 [15] datasets  both of which
are the basic image classiﬁcation datasets  because many works that tried to solve the knowledge
transfer problem used CIFAR in their base experiments [22  30]. Then  we evaluate our method
on ImageNet LSVRC 2015 [23] dataset. Finally  we applied our method to object detection with
PASCAL VOC 2007 [5] dataset.
To verify our method  we compare the proposed FT with several knowledge transfer methods such
as KD [10] and AT [30]. There are several important hyperparameters that need to be consistent.
For KD  we ﬁx the temperature for softened softmax to 4 as in [10]  and for β of AT  we set it to
103 following [30]. In the whole experiments  AT used multiple group losses. Alike AT  β of FT is
set to 103 in ImageNet and PASCAL VOC 2007. However  we set it to 5 × 102 in CIFAR-10 and
CIFAR-100 because a large β hinders the convergence.
We conduct experiments for different k values from 0.5 to 4. To show the effectiveness of the
proposed paraphraser architecture  we also used two convolutional autoencoders as paraphrasers
because the autoencoder is well known for extracting good features which contain compressed
information for reconstruction. One is an undercomplete convolutional autoencoder (CAE)  the other
is an overcomplete regularized autoencoder (RAE) which imposes l1 penalty on factors to learn the
size of factors needed by itself [1]. Details of these autoencoders and overall implementations of
experiments are explained in the supplementary material.
In some experiments  we also tested KD in combination with AT or FT because KD transfers output
knowledge while AT and FT delivers knowledge from intermediate blocks and these two different
methods can be combined into one (KD+AT or KD+FT).

4.1 CIFAR-10

The CIFAR-10 dataset consists of 50K training images and 10K testing images with 10 classes.
We conducted several experiments on CIFAR-10 with various network architectures  including
ResNet [8]  Wide ResNet (WRN) [31] and VGG [25]. Then  we made four conditions to test various
situations. First  we used ResNet-20 and ResNet-56 which are used in CIFAR-10 experiments of [8].
This condition is for the case where the teacher and the student networks have same width (number
of channels) and different depths (number of blocks). Secondly  we experimented with different types
of residual networks using ResNet-20 and WRN-40-1. Thirdly  we intended to see the effect of the
absence of shortcut connections that exist in Resblock on knowledge transfer by using VGG13 and
WRN-46-4. Lastly  we used WRN-16-1 and WRN-16-2 to test the applicability of knowledge transfer
methods for the architectures with the same depth but different widths.

5

Student

Teacher

ResNet-20 (0.27M)
ResNet-56 (0.85M)
ResNet-20 (0.27M) WRN-40-1 (0.56M)
WRN-46-4 (10M)
WRN-16-1 (0.17M) WRN-16-2 (0.69M)

VGG-13 (9.4M)

Student

Teacher

Student

7.78
7.78
5.99
8.62

k = 0.5

KD
7.19
7.09
5.71
7.64

AT
7.13
7.34
5.54
8.10
k = 0.75

VGG-13 (9.4M)

ResNet-20 (0.27M)
ResNet-56 (0.85M)
ResNet-20 (0.27M) WRN-40-1 (0.56M)
WRN-46-4 (10M)
WRN-16-1 (0.17M) WRN-16-2 (0.69M)
Table 1: Mean classiﬁcation error (%) on CIFAR-10 dataset (5 runs). All the numbers are the results
of our implementation. AT and KD are implemented according to [30].

6.92
7.05
5.09
7.83

6.85
7.16
4.84
7.64

k = 4
7.08
7.05
4.98
7.95

CAE RAE
7.07
7.24
7.33
7.26
5.53
5.85
8.48
8.00

FT
6.85
6.85
4.84
7.64
k = 1
6.89
7.04
5.04
7.74

6.89
7.00
5.30
7.52
k = 2
6.87
6.85
5.01
7.87

AT+KD FT+KD Teacher

7.04
6.95
4.65
7.59

6.39
6.84
4.44
6.27

Student

Teacher

Student

WRN-16-1 (0.17M) WRN-40-1 (0.56M)
WRN-16-2 (0.69M) WRN-40-2 (2.2M)
Table 2: Median classiﬁcation error (%) on CIFAR-10 dataset (5 runs). The ﬁrst 6 columns are from
Table 1 of [30]  while the last two columns are from our implementation.

8.01
5.71

8.77
6.31

8.12
5.51

6.55
5.09

AT
8.25
5.85

F-ActT KD AT+KD Teacher
8.62
6.24

8.39
6.08

6.58
5.23

FT (k = 0.5) Teacher

In the ﬁrst experiment  we wanted to show that our algorithm is applicable to various networks.
Result of FT and other knowledge transfer algorithms can be found in Table 1. In the table  ‘Student’
column provides the performance of student network trained from scratch. The ‘Teacher’ column
provides the performance of the pretrained teacher network. The numbers in the parentheses are
the sizes of network parameters in Millions. The performances of AT and KD are better than those
of ‘Student’ trained from scratch and the two show better or worse performances than the other
depending on the type of network used. For FT  we chose the best performance among the different k
values shown in the bottom rows in the table. The proposed FT shows better performances than AT
and KD consistently  regardless of the type of network used.
In the cases of hybrid knowledge transfer methods such as AT+KD and FT+KD  we could get
interesting result that AT and KD make some sort of synergy  because for all the cases  AT+KD
performed better than standalone AT or KD. It sometimes performed even better than FT  but FT
model trained together with KD loses its power in some cases.
As stated before in section 3.1  to check if having a paraphraser per group in FT is beneﬁcial  we
trained a ResNet-20 as student network with paraphrasers and translators combined in group1  group2
and group3  using the ResNet-56 as teacher network with k = 0.75. The classiﬁcation error was
7.01%  which is 0.06% higher than that from the single FT loss for the last group. This indicates
that the combined FT loss does not improve the performance thus we have used the single FT loss
throughout the paper. In terms of paraphrasing the information of the teacher network  the paraphraser
which maintains the spatial dimension outperformed autoencoders based methods which use CAE or
RAE.
As a second experiment  we compared FT with transferring FitNets-style hints which use full
activation maps as in [30]. Table 2 shows the results which veriﬁy that using the paraphrased
information is more beneﬁcial than directly using the full activation maps (full feature maps). In
the table  FT gives better accuracy improvement than full-activation transfer (F-ActT). Note that we
trained a teacher network from scratch for factor transfer (the last column) with the same experimental
environment of [30] because there is no pretrained model of the teacher networks.

4.2 CIFAR-100

For further analysis  we wanted to apply our algorithm to more difﬁcult tasks to prove generality of
the proposed FT by adopting CIFAR-100 dataset. CIFAR-100 dataset contains the same number of
images as CIFAR-10 dataset  50K (train) and 10K (test)  but has 100 classes  containing only 500
images per classes. Since the training dataset is more complicated  we thought the number of blocks
(depth) in the network has much more impact on the classiﬁcation performance because deeper
and stronger networks will better learn the boundaries between classes. Thus  the experiments on
CIFAR-100 were designed to observe the changes depending on the depths of networks. The teacher
network was ﬁxed as ResNet-110  and the two networks ResNet-20 and ResNet-56  that have the

6

Teacher

Teacher

Student

Student

ResNet-56 (0.85M)
ResNet-20 (0.27M)

ResNet-110 (1.73M)
ResNet-110 (1.73M)

Teacher
26.91
26.91
RAE
26.29
30.11
Table 3: Mean classiﬁcation error (%) on CIFAR-100 dataset (5 runs). All the numbers are from our
implementation.

AT+KD FT+KD
28.01
26.93
32.19
34.78

Student
28.04
31.24
k = 0.5
25.62
29.20

KD
27.96
33.14

AT
27.28
31.04
k = 0.75

ResNet-56 (0.85M)
ResNet-20 (0.27M)

ResNet-110 (1.73M)
ResNet-110 (1.73M)

CAE
26.41
29.84

k = 1
25.85
29.28

k = 2
25.63
29.19

k = 4
25.87
29.08

FT
25.62
29.08

25.78
29.25

Paraphraser

Yes
No
Yes

Translator

No
Yes
Yes

CIFAR-10 CIFAR-100 Number of layers in Paraphraser CIFAR-10 CIFAR-100

Student (WRN-40-1[0.6M])
Table 4: Left: Ablation study with and without the paraphraser (k = 0.5) and the Translator. (Mean
classiﬁcation error (%) of 5 runs). Right: Effect of number of layers in the paraphraser.

6.18
6.12
5.71
7.02

27.61
27.39
26.91
28.81

1 Layer [0.07M]
2 Layers [0.22M]
3 Layers [0.26M]
Teacher (WRN-40-2[2.2M])

6.09
5.99
5.71
4.96

27.07
27.03
26.91
24.10

same width (number of channels) but different depth (number of blocks) with the teacher  were used
as student networks. As can be seen in Table 3  we got an impressive result that the student network
ResNet-56 trained with FT even outperforms the teacher network. The student ResNet-20 did not
work that well but it also outperformed other knowledge transfer methods.
Additionally  in line with the experimental result in [30]  we also got consistent result that KD suffers
from the gap of depths between the teacher and the student  and the accuracy is even worse compared
to the student network in the case of training ResNet-20. For this dataset  the hybrid methods (AT+KD
and FT+KD) was worse than the standalone AT or FT. This also indicates that KD is not suitable for
a situation where the depth difference between the teacher and the student networks is large.

4.3 Ablation Study

In the introduction  we have described that the teacher network provides more paraphrased information
to the student network via factors  and described a need for a translator to act as a buffer to better
understand factors in the student network. To further analyze the role of factor  we performed an
ablation experiment on the presence or absence of a paraphraser and a translator. The result is shown
in Table 4. The student network and the teacher network are selected with different number of output
channels. One can adjust the number of student and teacher factors by adjusting the paraphrase
rate k of the paraphraser. As described above  since the role of the paraphraser (making FT with
unsupervised training loss) and the translator (trained jointly with student network to ease the learning
of Factor Transfer) are not the same  we can conﬁrm that the synergy of two modules maximizes the
performance of the student network. Also  we report the performance of different number of layers in
the paraphraser. As the number of layers increases  the performance also increases.

4.4

ImageNet

The ImageNet dataset is a image classiﬁcation dataset which consists of 1.2M training images and
50K validation images with 1 000 classes. We conducted large scale experiments on the ImageNet
LSVRC 2015 in order to show our potential availability to transfer even more complex and detailed
informations. We chose ResNet-18 as a student network and ResNet-34 as a teacher network same as
in [30] and validated the performance based on top-1 and top-5 error rates as shown in Table 5.
As can be seen in Table 5  FT consistently outperforms the other methods. The KD  again  suffers
from the depth difference problem  as already conﬁrmed in the result of other experiments. It shows
just adding the FT loss helps to lower about 1.34% of student network’s (ResNet-18) Top-1 error on
ImageNet.

4.5 Object Detection

In this experiment  we wanted to verify the generality of FT  and decided to apply it on detection task 
other than classiﬁcations. We used Faster-RCNN pipeline [21] with PASCAL VOC 2007 dataset [5]

7

Method
Student

KD
AT

FT (k = 0.5)

Teacher

Network
Resnet-18
Resnet-18
Resnet-18
Resnet-18
Resnet-34

Table 5: Top-1 and Top-5 classiﬁcation error (%) on Ima-
geNet dataset. All the numbers are from our implementation.

Table 6: Mean average precision on
PASCAL VOC 2007 test dataset.

Top-1
29.91
33.83
29.36
28.57
26.73

Top-5
10.68
12.55
10.23
9.71
8.57

Method

Student(VGG-16)

FT(VGG-16  k = 0.5)
Teacher(ResNet-101)

mAP
69.5
70.3
75.0

Figure 3: Factor transfer applied to Faster-RCNN framework

for object detection. We used PASCAL VOC 2007 trainval as training data and PASCAL VOC 2007
test as testing data. Instead of using our own ImageNet FT pretrained model as a backbone network
for detection  we tried to apply our method for transferring knowledges about object detection. Here 
we set a hypothesis that since the factors are extracted in an unsupervised manner  the factors not only
can connote the core knowledge of classiﬁcation  but also can convey other types of representations.
In the Faster-RCNN  the shared convolution layers contain knowledges of both classiﬁcation and
localization  so we applied factor transfer to the last layer of shared convolution layer. Figure 3 shows
where we applied FT in the Faster-RCNN framework. We set VGG-16 as a student network and
ResNet-101 as a teacher network. Both networks are ﬁne-tuned at PASCAL VOC 2007 dataset with
ImageNet pretrained model. For FT  we used ImageNet pretrained VGG-16 model and ﬁxed the
layers before conv3 layer during training phase. Then  by the factor transfer  the gradient caused by
the LF T loss back-propagates to the student network passing by the student translator.
As can be seen in Table 6  we could get performance enhancement of 0.8 in mAP (mean average
precision) score by training Faster-RCNN with VGG-16. As mentioned earlier  we have strong belief
that the latter layer we apply the factor transfer  the higher the performance enhances. However  by
the limit of VGG-type backbone network we have used  we tried but could not apply FT else that the
backbone network. Experiment on the capable case where the FT can be applied to the latter layers
like region proposal network (RPN) or other types of detection network will be our future work.

4.6 Discussion

In this section  we compare FitNet [22] and FT. FitNet transfers information of an intermediate
layer while FT uses the last layer  and the purpose of the regressor in FitNet is somewhat different
from our translator. More speciﬁcally  Romero et al. [22] argued that giving hints from deeper layer
over-regularizes the student network. On the contrary  we chose the deeper layer to provide more
speciﬁc information as mentioned in the paper. Also  FitNet does not use the paraphraser as well.
Note that FitNet is actually a 2-stage algorithm in that they initialize the student weights with hints
and then train the student network using Knowledge Distillation.

5 Conclusion

In this work  we propose the factor transfer which is a novel method for knowledge transfer. Unlike
previous methods  we introduce factors which contain paraphrased information of the teacher network 
extracted from the paraphraser. There are mainly two reasons that the student can understand
information from the teacher network more easily by the factor transfer than other methods. One

8

reason is that the factors can relieve the inherent differences between the teacher and student network.
The other reason is that the translator of the student can help the student network to understand
teacher factors by mimicking the teacher factors. A downside of the proposed method is that the
factor transfer requires the training of a paraphraser to extract factors and needs more parameters of
the paraphraser and the translator. However  the convergence of the training for the paraphraser is very
fast and additional parameters are not needed after training the student network. In our experiments 
we showed the effectiveness of the factor transfer on various image classiﬁcation datasets. Also  we
veriﬁed that factor transfer can be applied to other domains than classiﬁcation. We think that our
method will help further researches in knowledge transfer.

Acknowledgments

This work was supported by Next-Generation Information Computing Development Program through
the NRF of Korea (2017M3C4A7077582) and ICT R&D program of MSIP/IITP  Korean Government
(2017-0-00306).

References
[1] Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-
generating distribution. The Journal of Machine Learning Research  15(1):3563–3593  2014.
[2] Bowen Baker  Otkrist Gupta  Nikhil Naik  and Ramesh Raskar. Designing neural network

architectures using reinforcement learning. arXiv preprint arXiv:1611.02167  2016.

[3] Matthieu Courbariaux  Yoshua Bengio  and Jean-Pierre David. Binaryconnect: Training deep
neural networks with binary weights during propagations. In Advances in neural information
processing systems  pages 3123–3131  2015.

[4] Matthieu Courbariaux  Itay Hubara  Daniel Soudry  Ran El-Yaniv  and Yoshua Bengio. Binarized
neural networks: Training deep neural networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830  2016.

[5] M. Everingham  L. Van Gool  C. K. I. Williams  J. Winn  and A. Zisserman.

The
PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal-
network.org/challenges/VOC/voc2007/workshop/index.html.

[6] Suyog Gupta  Ankur Agrawal  Kailash Gopalakrishnan  and Pritish Narayanan. Deep learning
with limited numerical precision. In International Conference on Machine Learning  pages
1737–1746  2015.

[7] Song Han  Huizi Mao  and William J Dally. Deep compression: Compressing deep neural net-
works with pruning  trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 
2015.

[8] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 770–778  2016.

[9] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.

Science  313(5786):504–507  2006.

[10] Geoffrey Hinton  Oriol Vinyals  and Jeff Dean. Distilling the knowledge in a neural network.

arXiv preprint arXiv:1503.02531  2015.

[11] Andrew G Howard  Menglong Zhu  Bo Chen  Dmitry Kalenichenko  Weijun Wang  Tobias
Weyand  Marco Andreetto  and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural
networks for mobile vision applications. arXiv preprint arXiv:1704.04861  2017.

[12] Gao Huang  Shichen Liu  Laurens van der Maaten  and Kilian Q. Weinberger. Condensenet: An

efﬁcient densenet using learned group convolutions. CoRR  abs/1711.09224  2017.

[13] Forrest N Iandola  Song Han  Matthew W Moskewicz  Khalid Ashraf  William J Dally  and Kurt
Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model
size. arXiv preprint arXiv:1602.07360  2016.

[14] Alex Krizhevsky  Vinod Nair  and Geoffrey Hinton. Cifar-10 (canadian institute for advanced

research).

9

[15] Alex Krizhevsky  Vinod Nair  and Geoffrey Hinton. Cifar-100 (canadian institute for advanced

research).

[16] Hugo Larochelle  Dumitru Erhan  Aaron Courville  James Bergstra  and Yoshua Bengio. An
empirical evaluation of deep architectures on problems with many factors of variation. In
Proceedings of the 24th international conference on Machine learning  pages 473–480. ACM 
2007.

[17] Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In
Computer Vision and Pattern Recognition (CVPR)  2016 IEEE Conference on  pages 2554–
2564. IEEE  2016.

[18] Jonathan Masci  Ueli Meier  Dan Cire¸san  and Jürgen Schmidhuber. Stacked convolutional
auto-encoders for hierarchical feature extraction. In International Conference on Artiﬁcial
Neural Networks  pages 52–59. Springer  2011.

[19] Wing WY Ng  Guangjun Zeng  Jiangjun Zhang  Daniel S Yeung  and Witold Pedrycz. Dual
autoencoders features for imbalance classiﬁcation problem. Pattern Recognition  60:875–889 
2016.

[20] Mohammad Rastegari  Vicente Ordonez  Joseph Redmon  and Ali Farhadi. Xnor-net: Imagenet
classiﬁcation using binary convolutional neural networks. In European Conference on Computer
Vision  pages 525–542. Springer  2016.

[21] Shaoqing Ren  Kaiming He  Ross Girshick  and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in neural information processing systems 
pages 91–99  2015.

[22] Adriana Romero  Nicolas Ballas  Samira Ebrahimi Kahou  Antoine Chassang  Carlo Gatta  and

Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550  2014.

[23] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng
Huang  Andrej Karpathy  Aditya Khosla  Michael Bernstein  Alexander C. Berg  and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV)  115(3):211–252  2015.

[24] Hoo-Chang Shin  Matthew R Orton  David J Collins  Simon J Doran  and Martin O Leach.
Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot
study using 4d patient data. IEEE transactions on pattern analysis and machine intelligence 
35(8):1930–1943  2013.

[25] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556  2014.

[26] Suraj Srinivas and R Venkatesh Babu. Data-free parameter pruning for deep neural networks.

arXiv preprint arXiv:1507.06149  2015.

[27] Jiaxiang Wu  Cong Leng  Yuhang Wang  Qinghao Hu  and Jian Cheng. Quantized convolutional
neural networks for mobile devices. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 4820–4828  2016.

[28] Junho Yim  Donggyu Joo  Jihoon Bae  and Junmo Kim. A gift from knowledge distillation:
Fast optimization  network minimization and transfer learning. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)  2017.

[29] Jason Yosinski  Jeff Clune  Yoshua Bengio  and Hod Lipson. How transferable are features in
deep neural networks? In Advances in neural information processing systems  pages 3320–3328 
2014.

[30] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving
the performance of convolutional neural networks via attention transfer. arXiv preprint
arXiv:1612.03928  2016.

[31] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks.

arXiv:1605.07146  2016.

arXiv preprint

[32] Junbo Zhao  Michael Mathieu  and Yann LeCun. Energy-based generative adversarial network.

arXiv preprint arXiv:1609.03126  2016.

[33] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv

preprint arXiv:1611.01578  2016.

10

,Kishan Wimalawarne
Masashi Sugiyama
Ryota Tomioka
Greg Van Buskirk
Benjamin Raichel
Nicholas Ruozzi
Jangho Kim
Seonguk Park
Nojun Kwak