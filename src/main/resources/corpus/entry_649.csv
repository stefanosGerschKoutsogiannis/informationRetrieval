2014,Automated Variational Inference for Gaussian Process Models,We develop an automated variational method for approximate inference in Gaussian process (GP) models whose posteriors are often intractable. Using a mixture of Gaussians as the variational distribution  we show that (i) the variational objective and its gradients can be approximated efficiently via sampling from univariate Gaussian distributions and (ii) the gradients of the GP hyperparameters can be obtained analytically regardless of the model likelihood. We further propose two instances of the variational distribution whose covariance matrices can be parametrized linearly in the number of observations. These results allow gradient-based optimization to be done efficiently in a black-box manner. Our approach is thoroughly verified on 5 models using 6 benchmark datasets  performing as well as the exact or hard-coded implementations while running orders of magnitude faster than the alternative MCMC sampling approaches. Our method can be a valuable tool for practitioners and researchers to investigate new models with minimal effort in deriving model-specific inference algorithms.,Automated Variational Inference

for Gaussian Process Models

Trung V. Nguyen
ANU & NICTA

VanTrung.Nguyen@nicta.com.au

Edwin V. Bonilla

The University of New South Wales

e.bonilla@unsw.edu.au

Abstract

We develop an automated variational method for approximate inference in Gaus-
sian process (GP) models whose posteriors are often intractable. Using a mixture
of Gaussians as the variational distribution  we show that (i) the variational objec-
tive and its gradients can be approximated efﬁciently via sampling from univari-
ate Gaussian distributions and (ii) the gradients wrt the GP hyperparameters can
be obtained analytically regardless of the model likelihood. We further propose
two instances of the variational distribution whose covariance matrices can be
parametrized linearly in the number of observations. These results allow gradient-
based optimization to be done efﬁciently in a black-box manner. Our approach is
thoroughly veriﬁed on ﬁve models using six benchmark datasets  performing as
well as the exact or hard-coded implementations while running orders of magni-
tude faster than the alternative MCMC sampling approaches. Our method can be
a valuable tool for practitioners and researchers to investigate new models with
minimal effort in deriving model-speciﬁc inference algorithms.

1

Introduction

Gaussian processes (GPs  [1]) are a popular choice in practical Bayesian non-parametric modeling.
The most straightforward application of GPs is the standard regression model with Gaussian like-
lihood  for which the posterior can be computed in closed form. However  analytical tractability is
no longer possible when having non-Gaussian likelihoods  and inference must be carried out via ap-
proximate methods  among which Markov chain Monte Carlo (MCMC  see e.g. [2]) and variational
inference [3] are arguably the two techniques most widely used.
MCMC algorithms provide a ﬂexible framework for sampling from complex posterior distributions
of probabilistic models. However  their generality comes at the expense of very high computational
cost as well as cumbersome convergence analysis. Furthermore  methods such as Gibbs sampling
may perform poorly when there are strong dependencies among the variables of interest. Other
algorithms such as the elliptical slice sampling (ESS) developed in [4] are more effective at drawing
samples from strongly correlated Gaussians. Nevertheless  while improving upon generic MCMC
methods  the sampling cost of ESS remains a major challenge for practical usages.
Alternative to MCMC is the deterministic approximation approach via variational inference  which
has been used in numerous applications with some empirical success ( see e.g. [5  6  7  8  9  10  11]).
The main insight from variational methods is that optimizing is generally easier than integrating.
Indeed  they approximate a posterior by optimizing a lower bound of the marginal likelihood  the
so-called evidence lower bound (ELBO). While variational inference can be considerably faster
than MCMC  it lacks MCMC’s broader applicability as it requires derivations of the ELBO and its
gradients on a model-by-model basis.
This paper develops an automated variational inference technique for GP models that not only re-
duces the overhead of the tedious mathematical derivations inherent to variational methods but also

1

allows their application to a wide range of problems. In particular  we consider Gaussian process
models that satisfy the following properties: (i) factorization across latent functions and (ii) factor-
ization across observations. The former assumes that  when there is more than one latent function 
they are generated from independent GPs. The latter assumes that  given the latent functions  the
observations are conditionally independent. Existing GP models  such as regression [1]  binary and
multi-class classiﬁcation [6  12]  warped GPs [13]  log Gaussian Cox process [14]  and multi-output
regression [15]  all fall into this class of models. We note  however  that our approach goes beyond
standard settings for which elaborate learning machinery has been developed  as we only require
access to the likelihood function in a black-box manner.
Our automated deterministic inference method uses a mixture of Gaussians as the approximating
posterior distribution and exploits the decomposition of the ELBO into a KL divergence term and
an expected log likelihood term. In particular  we derive an analytical lower bound for the KL term;
and we show that the expected log likelihood term and its gradients can be computed efﬁciently by
sampling from univariate Gaussian distributions  without explicitly requiring gradients of the likeli-
hood. Furthermore  we optimize the GP hyperparameters within the same variational framework by
using their analytical gradients  irrespective of the speciﬁcs of the likelihood models.
Additionally  we exploit the efﬁcient parametrization of the covariance matrices in the models  which
is linear in the number of observations  along with variance-reduction techniques in order to pro-
vide an automated inference framework that is useful in practice. We verify the effectiveness of
our method with extensive experiments on 5 different GP settings using 6 benchmark datasets. We
show that our approach performs as well as exact GPs or hard-coded deterministic inference imple-
mentations  and that it can be up to several orders of magnitude faster than state-of-the-art MCMC
approaches.
Related work

Black box variational inference (BBVI  [16]) has recently been developed for general latent variable
models. Due to this generality  it under-utilizes the rich amount of information available in GP mod-
els that we previously discussed. For example  BBVI approximates the KL term of the ELBO  but
this is computed analytically in our method. A clear disadvantage of BBVI is that it does not provide
an analytical or practical way of learning the covariance hyperparameters of GPs – in fact  these are
set to ﬁxed values. In principle  these values can be learned in BBVI using stochastic optimiza-
tion  but experimentally  we have found this to be problematic  ineffectual  and time-consuming. In
contrast  our method optimizes the hyperparameters using their exact gradients.
An approach more closely related to ours is in [17]  which investigates variational inference for
GP models with one latent function and factorial likelihood. Their main result is an efﬁcient
parametrization when using a standard variational Gaussian distribution. Our method is more gen-
eral in that it allows multiple latent functions  hence being applicable to settings such as multi-class
classiﬁcation and multi-output regression. Furthermore  our variational distribution is a mixture of
Gaussians  with the full Gaussian distribution being a particular case. Another recent approach to
deterministic approximate inference is the Integrated Nested Laplace Approximation (INLA  [18]).
INLA uses numerical integration to approximate the marginal likelihood  which makes it unsuitable
for GP models that contain a large number of hyperparameters.

2 A family of GP models
We consider supervised learning problems with a dataset of N training inputs x = {xn}N
n=1 and
their corresponding targets y = {yn}N
n=1. The mapping from inputs to outputs is established via
Q underlying latent functions  and our objective is to reason about these latent functions from the
observed data. We specify a class of GP models for which the priors and the likelihoods have the
following structure:

Q(cid:89)

Q(cid:89)
N(cid:89)

j=1

p(f|θ0) =

p(y|f   θ1) =

p(f•j|θ0) =

j=1

p(yn|fn•  θ1) 

N (f•j; 0  Kj) 

(1)

(2)

n=1

2

where f is the set of all latent function values; f•j = {fj(xn)}N
n=1 denotes the values of the latent
function j; fn• = {fj(xn)}Q
j=1 is the set of latent function values which yn depends upon; Kj is
the covariance matrix evaluated at every pair of inputs induced by the covariance function kj(· ·);
and θ0 and θ1 are covariance hyperparameters and likelihood parameters  respectively.
In other words  the class of models speciﬁed by Equations (1) and (2) satisfy the following two cri-
teria: (a) factorization of the prior over the latent functions and (b) factorization of the conditional
likelihood over the observations. Existing GP models including GP regression [1]  binary classiﬁca-
tion [6  12]  warped GPs [13]  log Gaussian Cox processes [14]  multi-class classiﬁcation [12]  and
multi-output regression [15] all belong to this family of models.

3 Automated variational inference for GP models

This section describes our automated inference framework for posterior inference of the latent func-
tions for the given family of models. Apart from Equations (1) and (2)  we only require access to
the likelihood function in a black-box manner  i.e. speciﬁc knowledge of its shape or its gradient is
not needed. Posterior inference for general (non-Gaussian) likelihoods is analytically intractable.
We build our posterior approximation framework upon variational inference principles. This entails
positing a tractable family of distributions and ﬁnding the member of the family that is “closest”
to the true posterior in terms of their KL divergence. Herein we choose the family of mixture of
Gaussians (MoG) with K components  deﬁned as

q(f|λ) =

1
K

qk(f|mk  Sk) =

1
K

N (f•j; mkj  Skj)  λ = {mkj  Skj} 

(3)

where qk(f|mk  Sk) is the component k with variational parameters mk = {mkj}Q
j=1 and Sk =
{Skj}Q
j=1. Less general MoG with isotropic covariances have been used with variational inference
in [7  19]. Note that within each component  the posteriors over the latent functions are independent.
Minimizing the divergence KL[q(f|λ)||p(f|y)] is equivalent to maximizing the evidence lower
bound (ELBO) given by:

K(cid:88)

Q(cid:89)

k=1

j=1

K(cid:88)

k=1

log p(y) ≥ Eq[− log q(f|λ)] + Eq[log p(f )]

Eqk [log p(y|f )] ∆= L.

(4)

(cid:124)

(cid:123)(cid:122)

−KL[q(f|λ)||p(f )]

K(cid:88)

k=1

(cid:125)

+

1
K

Observe that the KL term in Equation (4) does not depend on the likelihood. The remaining term 
called the expected log likelihood (ELL)  is the only contribution of the likelihood to the ELBO. We
can thus address the technical difﬁculties regarding each component and their derivatives separately
using different approaches. In particular  we can obtain a lower bound of the ﬁrst term (KL) and
approximate the second term (ELL) via sampling. Due to the limited space  we only show the main
results and refer the reader to the supplementary material for derivation details.
3.1 A lower bound of −KL[q(f|λ)||p(f )]
The ﬁrst component of the KL divergence term is the entropy of a Gaussian mixture which is not
analytically tractable. However  a lower bound of this entropy can be obtained using Jensen’s in-
equality (see e.g. [20]) giving:

Eq[− log q(f|λ)] ≥ − K(cid:88)

k=1

K(cid:88)

l=1

1
K

log

N (mk; ml  Sk + Sl).

1
K

(5)

The second component of the KL term is a negative cross-entropy between a Gaussian mixture and
a Gaussian  which can be computed analytically giving:

K(cid:88)

Q(cid:88)

k=1

j=1

(cid:2)N log 2π + log |Kj| + mT

kjK−1

j mkj + tr (K−1

(6)

j Skj)(cid:3).

Eq[log p(f )] = − 1
2K

The gradients of the two terms in Equations (5) and (6) wrt the variational parameters can be com-
puted analytically and are given in the supplementary material.

3

3.2 An approximation to the expected log likelihood (ELL)

N(cid:88)

It is clear from Equation (4) that the ELL can be obtained via the ELLs of the individual mixture
components Eqk [log p(y|f )]. Due to the factorial assumption of p(y|f )  the expectation becomes:

Eqk [log p(y|f )] =

Eqk(n) [log p(yn|fn•)] 

(7)

n=1

where qk(n) = qk(n)(fn•|λk(n)) is the marginal posterior with variational parameters λk(n) that
correspond to fn•. The gradients of these individual ELL terms wrt the variational parameters λk(n)
are given by:

∇λk(n)

Eqk(n)[log p(yn|fn•)] =Eqk(n)∇λk(n) log qk(n)(fn•|λk(n)) log p(yn|fn•).

(8)

Using Equations (7) and (8) we establish the following theorem regarding the computation of the
ELL and its gradients.
Theorem 1. The expected log likelihood and its gradients can be approximated using samples from
univariate Gaussian distributions.

The proof is in Section 1 of the supplementary material. A less general result  for the case of
one latent function and the variational Gaussian posterior  was obtained in [17] using a different
derivation. Note that when Q > 1  qk(n) is not a univariate marginal. Nevertheless  it has a diagonal
covariance matrix due to the factorization of the latent posteriors so the theorem still holds.

3.3 Learning of the variational parameters and other model parameters

In order to learn the parameters of the model we use gradient-based optimization of the ELBO. For
this we require the gradients of the ELBO wrt all model parameters.

Variational parameters. The noisy gradients of the ELBO w.r.t. the variational means mk(n) and
variances Sk(n) corresponding to data point n are given by:

k(n) ◦ S(cid:88)

s−1

i=1

1
KS

ˆ∇mk(n)L ≈ ∇mk(n)Lent + ∇mk(n)Lcross +
ˆ∇Sk(n)L ≈ ∇Sk(n)Lent + ∇Sk(n)Lcross
k(n) ◦ (f i

k(n) ◦ s−1
s−1

S(cid:88)

(cid:18)

dg

+

1

2KS

i=1

n• − mk(n)) log p(yn|f i
(f i

n•) 

(9)

(cid:19)

log p(yn|f i

n•)

(10)

n• − mk(n)) ◦ (f i

n• − mk(n)) − s−1

k(n)

where ◦ is the entrywise Hadamard product; {f i
i=1 are samples from qk(n)(fn•|mk(n)  sk(n));
sk(n) is the diagonal of Sk(n) and s−1
k(n) is the element-wise inverse of sk(n); dg turns a vector to a
diagonal matrix; and Lent = Eq[− log q(f|λ)] and Lcross = Eq[log p(f )] are given by Equations (5)
and (6). The control variates technique described in [16] is also used to further reduce the variance
of these estimators.

n•}S

Covariance hyperparameters. The ELBO in Equation (4) reveals a remarkable property: the hy-
perparameters depend only on the negative cross-entropy term Eq[log p(f )] whose exact expression
was derived in Equation (6). This has a signiﬁcant practical implication: despite using black-box
inference  the hyperparameters are optimized wrt the true evidence lower bound (given ﬁxed varia-
tional parameters). This is an additional and crucial advantage of our automated inference method
over other generic inference techniques [16] that seem incapable of hyperparameter learning  in
part because there are not yet techniques for reducing the variance of the gradient estimators. The
gradient of the ELBO wrt any hyperparameter θ of the j-th covariance function is given by:

j ∇θKj − K−1

j ∇θKjK−1

j (mkjmT

(11)

kj + Sj)(cid:1).

K(cid:88)

tr(cid:0)K−1

∇θL = − 1
2K

k=1

4

Likelihood parameters The noisy gradients w.r.t. the likelihood parameters can also be estimated
via samples from univariate marginals:

∇θ1 log p(yn|f k i

(n)  θ1)  where f k i

(n) ∼ qk(n)(fn•|mk(n)  sk(n)).

(12)

K(cid:88)

N(cid:88)

S(cid:88)

k=1

n=1

i=1

∇θ1L ≈ 1
KS

3.4 Practical variational distributions

The gradients from the previous section may be used for automated variational inference for GP
models. However  the mixture of Gaussians (MoG) requires O(N 2) variational parameters for each
covariance matrix  i.e. we need to estimate a total of O(QKN 2) parameters. This causes difﬁculties
for learning when these parameters are optimized simultaneously. This section introduces two spe-
cial members of the MoG family that improve the practical tractability of our inference framework.

Full Gaussian posterior. This instance is the mixture with only 1 component and is thus a Gaus-
sian distribution.
Its covariance matrix has block diagonal structure  where each block is a full
covariance corresponding to that of a single latent function posterior. We thus refer to it as the
full Gaussian posterior. As stated in the following theorem  full Gaussian posteriors can still be
estimated efﬁciently in our variational framework.
Theorem 2. Only O(QN ) variational parameters are required to parametrize the latent posteriors
with full covariance structure.

The proof is given Section 2 of the supplementary material. This result has been stated previously
(see e.g. [6  7  17]) but for speciﬁc models that belong to the class of GP models considered here.

Mixture of diagonal Gaussians posterior. Our second practical variational posterior is a Gaus-
sian mixture with diagonal covariances  yielding two immediate beneﬁts. Firstly  only O(QN )
parameters are required for each mixture component. Secondly  computation is more efﬁcient as
inverting a diagonal covariance can be done in linear time. Furthermore  as a result of the following
theorem  optimization will typically converge faster when using a mixture of diagonal Gaussians.
Theorem 3. The estimator of the gradients wrt the variational parameters using the mixture of
diagonal Gaussians has a lower variance than the full Gaussian posterior’s.

The proof is in Section 3 of the supplementary material and is based on the Rao-Blackwellization
technique [21]. We note that this result is different to that in [16]. In particular  our variational
distribution is a mixture  thus multi-modal. The theorem is only made possible due to the analytical
tractability of the KL term in the ELBO.
Given the noisy gradients  we use off-the-shelf  gradient-based optimizers  such as conjugate gradi-
ent  to learn the model parameters. Note that stochastic optimization may also be used  but it may
require signiﬁcant time and effort in tuning the learning rates.

3.5 Prediction

Given the MoG posterior  the predictive distribution for new test points x∗ is given by:

(cid:90)

K(cid:88)

k=1

(cid:90)

p(Y∗|x∗) =

1
K

p(Y∗|f∗)

p(f∗|f )qk(f )df df∗.

(13)

The inner integral is the predictive distribution of the latent values f∗ and it is a Gaussian since
both qk(f ) and p(f∗|f ) are Gaussian. The probability of the test points taking values y∗ (e.g. in
classiﬁcation) can thus be readily estimated via Monte Carlo sampling. The predictive means and
variances of a MoG can be obtained from that of the individual mixture components as described in
Section 6 of the supplementary material.

5

Mining disasters 811
Boston housing 300
Creep 800
Abalone 1000
Breast cancer 300
USPS 1233

0
206
1266
3177
383
1232

Model
1
Log Gausian Cox process
13 N (y; f  σ2)
Standard regression
30 ∇yt(y)N (t(y); f  σ2) Warped Gaussian processes
Warped Gaussian processes
8
Binary classiﬁcation
9

same as above
1/(1 + exp(−f ))

256 exp(fc)/(cid:80)

Table 1: Datasets  their statistics  and the corresponding likelihood functions and models used in the
experiments  where Ntrain  Ntest  and D are the training size  testing size  and the input dimension 
respectively. See text for detailed description of the models.
Dataset Ntrain Ntest D Likelihood p(y|f )
λy exp(−λ)/y!

i=1 exp(fi) Multi-class classiﬁcation

4 Experiments

We perform experiments with ﬁve GP models: standard regression [1]  warped GPs [13]  binary
classiﬁcation [6  12]  multi-class classiﬁcation [12]  and log Gaussian Cox processes [14] on six
datasets (see Table 1) and repeat the experiments ﬁve times using different data subsets.
Experimental settings. The squared exponential covariance function with automatic relevance de-
termination (see Ch. 4 in [1]) is used with the GP regression and warped GPs. The isotropic co-
variance is used with all other models. The noisy gradients of the ELBO are approximated with
2000 samples and 200 samples are used with control variates to reduce the variance of the gradient
estimators. The model parameters (variational  covariance hyperparameters and likelihood parame-
ters) are learned by iteratively optimizing one set while ﬁxing the others until convergence  which is
determined when changes are less than 1e-5 for the ELBO or 1e-3 for the variational parameters.
Evaluation metrics. To assess the predictive accuracy  we use the standardized squared error (SSE)
for the regression tasks and the classiﬁcation error rates for the classiﬁcation tasks. The negative log
predictive density (NLPD) is also used to evaluate the conﬁdence of the prediction. For all of the
metrics  smaller ﬁgures are better.
Notations. We call our method AGP and use AGP-FULL  AGP-MIX and AGP-MIX2 when
using the full Gaussian and the mixture of diagonal Gaussians with 1 and 2 components  respectively.
Details of these two posteriors were given in Section 3.4. On the plots  we use the shorter notations 
FULL  MIX  and MIX2 due to the limited space.
Reading the box plots. We used box plots to give a more complete picture of the predictive per-
formance. Each plot corresponds to the distribution of a particular metric evaluated at all test points
for a given task. The edges of a box are the q1 = 25th and q3 = 75th percentiles and the central
mark is the median. The dotted line marks the limit of extreme points that are greater than the 97.5th
percentile. The whiskers enclose the points in the range (q1− 1.5(q3− q1)  q3 +1.5(q3− q1))  which
amounts to approximately ±2.7σ if the data is normally distributed. The points outside the whiskers
and below the dotted line are outliers and are plotted individually.

4.1 Standard regression

First we consider the standard Gaussian process regression for which the predictive distribution can
be computed analytically. We compare with this exact inference method (GPR) using the Boston
housing dataset [22]. The results in Figure 1 show that AGP-FULL achieves nearly identical per-
formance as GPR. This is expected as the analytical posterior is a full Gaussian. AGP-MIX and
AGP-MIX2 also give comparable performance in terms of the median SSE and NLPD.

4.2 Warped Gaussian processes (WGP)

The WGP allows for non-Gaussian processes and non-Gaussian noises. The likelihood for each
target yn is attained by warping it through a nonlinear monotonic transformation t(y) giving
p(yn|fn) = ∇yn t(yn)N (t(yn)|fn  σ2). We used the same neural net style transformation as in
[13]. We ﬁxed the warp parameters and used the same procedure for making analytical approxima-
tions to the predicted means and variances for all methods.

6

Figure 1: The distributions of SSE and NLPD of all methods on the regression task. Compared to the
exact inference method GPR  the performance of AGP-FULL is identical while that of AGP-MIX
and AGP-MIX2 are comparable.

Figure 2: The distributions of SSE and NLPD of all methods on the regression task with warped
GPs. The AGP methods (FULL  MIX and MIX2) give comparable performance to exact inference
with WGP and slightly outperform GPR which has narrower ranges of predictive variances.

We compare with the exact implementation of [13] and the standard GP regression (GPR) on the
Creep [23] and Abalone [22] datasets. The results in Figure 2 show that the AGP methods give
comparable performance to the exact method WGP and slightly outperform GPR. The prediction
by GPR exhibits characteristically narrower ranges of predictive variances which can be attributed
to its Gaussian noise assumption.

4.3 Classiﬁcation

For binary classiﬁcation  we use the logistic likelihood and experiment with the Wisconsin breast
cancer dataset [22]. We compare with the variational bounds (VBO) and the expectation propagation
(EP) methods. Details of VBO and EP can be found in [6]. All methods use the same analytical
approximations when making prediction.
For multi-class classiﬁcation  we use the softmax likelihood and experiment with a subset of the
USPS dataset [1] containing the digits 4  7  and 9. We compare with a variational inference method
(VQ) which constructs the ELBO via a quadratic lower bound to the likelihood terms [5]. Prediction
is made by squashing the samples from the predictive distributions of the latent values at test points
through the softmax likelihood for all methods.

Figure 3: Left plot: classiﬁcation error rates averaged over 5 runs (the error bars show two standard
deviations). The AGP methods have classiﬁcation errors comparable to the hard-coded implementa-
tions. Middle and right plots: the distribution of NLPD of all methods on the binary and multi-class
classiﬁcation tasks  respectively. The hard-coded methods are slightly better than AGP.

7

FULLMIXMIX2GPR00.20.40.60.8Boston housingSSEFULLMIXMIX2GPR2345678Boston housingNLPDFULLMIXMIX2GPRWGP00.10.20.30.4CreepSSEFULLMIXMIX2GPRWGP234567CreepNLPDFULLMIXMIX2GPRWGP00.511.522.53AbaloneSSEFULLMIXMIX2GPRWGP12345AbaloneNLPDBreast cancerUSPS00.010.020.030.040.050.06Error rates VQFULLMIXMIX2VBOEPFULLMIXMIX2VBOEP00.20.40.60.81Breast cancerNLPDFULLMIXMIX2VQ00.20.40.60.81USPSNLPDFigure 4: Left plot: the true event counts during the given time period. Middle plot: the posteriors
(estimated intensities) inferred by all methods. For each method  the middle line is the posterior
mean and the two remaining lines enclose 90% interval. AGP-FULL infers the same posterior as
HMC and ESS while AGP-MIX obtains the same mean but underestimates the variance. Right
plot: speed-up factors against the HMC method. The AGP methods run more than 2 orders of
magnitude faster than the sampling methods.

The classiﬁcation error rates and the NLPD are shown in Figure 3 for both tasks. For binary classi-
ﬁcation  the AGP methods give comparable performance to the hard-coded implementations  VBO
and EP. The latter is often considered the best approximation method for this task [6]. Similar
results can be observed for the multi-class classiﬁcation problem.
We note that the running times of our methods are comparable to that of the hard-coded methods.
For example  the average training times for VBO  EP  MIX  and FULL are 76s  63s  210s  and 480s
respectively  on the Wisconsin dataset.

4.4 Log Gaussian Cox process (LGCP)

yn!

The LGCP is an inhomogeneous Poisson process with the log-intensity function being a shifted
n exp(−λn)
draw from a Gaussian process. Following [4]  we use the likelihood p(yn|fn) = λyn
 
where λn = exp(fn + m) is the mean of a Poisson distribution and m is the offset to the log mean.
The data concerns coal-mining disasters taken from a standard dataset for testing point processes
[24]. The offset m and the covariance hyperparameters are set to the same values as in [4].
We compare AGP with the Hybrid Monte Carlo (HMC  [25]) and elliptical slice sampling (ESS 
[4]) methods  where the latter is designed speciﬁcally for GP models. We collected every 100th
sample for a total of 10k samples after a burn-in period of 5k samples; the Gelman-Rubin potential
scale reduction factors [26] are used to check for convergence. The middle plot of Figure 4 shows
the posteriors learned by all methods. We see that the posterior by AGP-FULL is similar to that
by HMC and ESS. AGP-MIX obtains the same posterior mean but it underestimates the variance.
The right plot shows the speed-up factors of all methods against the slowest method HMC. The
AGP methods run more than two orders of magnitude faster than HMC  thus conﬁrming the com-
putational advantages of our method to the sampling approaches. Training time was measured on a
desktop with Intel(R) i7-2600 3.40GHz CPU with 8GB of RAM using Matlab R2012a.

5 Discussion

We have developed automated variational inference for Gaussian process models (AGP). AGP per-
forms as well as the exact or hard-coded implementations when testing on ﬁve models using six real
world datasets. AGP has the potential to be a powerful tool for GP practitioners and researchers
when devising models for new or existing problems for which variational inference is not yet avail-
able. In the future we will address the scalability of AGP to deal with very large datasets.

Acknowledgements

NICTA is funded by the Australian Government through the Department of Communications and
the Australian Research Council through the ICT Centre of Excellence Program.

8

18601880190019201940196001234TimeEvent counts18601880190019201940196000.10.20.30.40.50.6TimeIntensityPosteriors of the latent intensity FULLMIXHMC & ESSTime comparison against HMC00.511.522.5Log10 speed−up factor FULLMIXESSReferences
[1] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning. The

MIT Press  2006.

[2] Radford M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical report 

Department of Computer Science  University of Toronto  1993.

[3] Michael I Jordan  Zoubin Ghahramani  Tommi S Jaakkola  and Lawrence K Saul. An introduction to

variational methods for graphical models. Springer  1998.

[4] Iain Murray  Ryan Prescott Adams  and David J.C. MacKay. Elliptical slice sampling. In AISTATS  2010.
[5] Mohammad E. Khan  Shakir Mohamed  Benjamin M. Marlin  and Kevin P. Murphy. A stick-breaking
likelihood for categorical data analysis with latent Gaussian models. In AISTATS  pages 610–618  2012.
[6] Hannes Nickisch and Carl Edward Rasmussen. Approximations for binary Gaussian process classiﬁca-

tion. Journal of Machine Learning Research  9(10)  2008.

[7] Trung V. Nguyen and Edwin Bonilla. Efﬁcient variational inference for Gaussian process regression

networks. In AISTATS  pages 472–480  2013.

[8] Mohammad E. Khan  Shakir Mohamed  and Kevin P. Murphy. Fast Bayesian inference for non-conjugate

Gaussian process regression. In NIPS  pages 3149–3157  2012.

[9] Miguel L´azaro-Gredilla. Bayesian warped Gaussian processes. In NIPS  pages 1628–1636  2012.
[10] Mark Girolami and Simon Rogers. Variational Bayesian multinomial probit regression with Gaussian

process priors. Neural Computation  18(8):1790–1817  2006.

[11] Miguel L´azaro-Gredilla and Michalis Titsias. Variational heteroscedastic Gaussian process regression. In

ICML  2011.

[12] Christopher K.I. Williams and David Barber. Bayesian classiﬁcation with Gaussian processes. Pattern

Analysis and Machine Intelligence  IEEE Transactions on  20(12):1342–1351  1998.

[13] Edward Snelson  Carl Edward Rasmussen  and Zoubin Ghahramani. Warped Gaussian processes.

NIPS  2003.

In

[14] Jesper Møller  Anne Randi Syversveen  and Rasmus Plenge Waagepetersen. Log Gaussian Cox processes.

Scandinavian journal of statistics  25(3):451–482  1998.

[15] Andrew G. Wilson  David A. Knowles  and Zoubin Ghahramani. Gaussian process regression networks.

In ICML  2012.

[16] Rajesh Ranganath  Sean Gerrish  and David M. Blei. Black box variational inference. In AISTATS  2014.
[17] Manfred Opper and C´edric Archambeau. The variational Gaussian approximation revisited. Neural

Computation  21(3):786–792  2009.

[18] H˚avard Rue  Sara Martino  and Nicolas Chopin. Approximate Bayesian inference for latent Gaussian
models by using integrated nested Laplace approximations. Journal of the royal statistical society: Series
b (statistical methodology)  71(2):319–392  2009.

[19] Samuel J. Gershman  Matthew D. Hoffman  and David M. Blei. Nonparametric variational inference. In

ICML  2012.

[20] Marco F. Huber  Tim Bailey  Hugh Durrant-Whyte  and Uwe D. Hanebeck. On entropy approximation
In IEEE International Conference on Multisensor Fusion and

for Gaussian mixture random vectors.
Integration for Intelligent Systems  2008.

[21] George Casella and Christian P. Robert. Rao-Blackwellisation of sampling schemes. Biometrika  1996.
[22] K. Bache and M. Lichman. UCI machine learning repository  2013.
[23] D. Cole  C. Martin-Moran  A.G. Sheard  H.K.D.H. Bhadeshia  and D.J.C. MacKay. Modelling creep
rupture strength of ferritic steel welds. Science and Technology of Welding & Joining  5(2):81–89  2000.

[24] R.G. Jarrett. A note on the intervals between coal-mining disasters. Biometrika  66(1):191–193  1979.
[25] Simon Duane  Anthony D. Kennedy  Brian J. Pendleton  and Duncan Roweth. Hybrid Monte Carlo.

Physics letters B  195(2):216–222  1987.
[26] Andrew Gelman and Donald B Rubin.
Statistical science  pages 457–472  1992.

Inference from iterative simulation using multiple sequences.

9

,Trung Nguyen
Edwin Bonilla
Yunwen Lei
Ke Tang