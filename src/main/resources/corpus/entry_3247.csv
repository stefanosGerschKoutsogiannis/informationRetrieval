2015,Local Causal Discovery of Direct Causes and Effects,We focus on the discovery and identification of direct causes and effects of a target variable in a causal network. State-of-the-art algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs in order to identify the direct causes and effects of a target variable. While these algorithms are effective  it is often unnecessary and wasteful to find the global structures when we are only interested in one target variable (such as class labels). We propose a new local causal discovery algorithm  called Causal Markov Blanket (CMB)  to identify the direct causes and effects of a target variable based on Markov Blanket Discovery. CMB is designed to conduct causal discovery among multiple variables  but focuses only on finding causal relationships between a specific target variable and other variables. Under standard assumptions  we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency  often by more than one order of magnitude.,Local Causal Discovery of Direct Causes and Effects

Tian Gao

Qiang Ji

Department of ECSE

Rensselaer Polytechnic Institute  Troy  NY 12180

{gaot  jiq}@rpi.edu

Abstract

We focus on the discovery and identiï¬cation of direct causes and effects of a target
variable in a causal network. State-of-the-art causal learning algorithms generally
need to ï¬nd the global causal structures in the form of complete partial directed
acyclic graphs (CPDAG) in order to identify direct causes and effects of a target
variable. While these algorithms are effective  it is often unnecessary and wasteful
to ï¬nd the global structures when we are only interested in the local structure of
one target variable (such as class labels). We propose a new local causal discov-
ery algorithm  called Causal Markov Blanket (CMB)  to identify the direct causes
and effects of a target variable based on Markov Blanket Discovery. CMB is de-
signed to conduct causal discovery among multiple variables  but focuses only on
ï¬nding causal relationships between a speciï¬c target variable and other variables.
Under standard assumptions  we show both theoretically and experimentally that
the proposed local causal discovery algorithm can obtain the comparable identiï¬-
cation accuracy as global methods but signiï¬cantly improve their efï¬ciency  often
by more than one order of magnitude.

1

Introduction

Causal discovery is the process to identify the causal relationships among a set of random variables.
It not only can aid predictions and classiï¬cations like feature selection [4]  but can also help pre-
dict consequences of some given actions  facilitate counter-factual inference  and help explain the
underlying mechanisms of the data [13]. A lot of research efforts have been focused on predict-
ing causality from observational data [13  18]. They can be roughly divided into two sub-areas:
causal discovery between a pair of variables and among multiple variables. We focus on multivari-
ate causal discovery  which searches for correlations and dependencies among variables in causal
networks [13]. Causal networks can be used for local or global causal prediction  and thus they can
be learned locally and globally. Many causal discovery algorithms for causal networks have been
proposed  and the majority of them belong to global learning algorithms as they seek to learn global
causal structures. The Spirtes-Glymour-Scheines (SGS) [18] and Peter-Clark (P-C) algorithm [19]
test for the existence of edges between every pair of nodes in order to ï¬rst ï¬nd the skeleton  or
undirected edges  of causal networks and then discover all the V-structures  resulting in a partially
directed acyclic graph (PDAG). The last step of these algorithms is then to orient the rest of edges
as much as possible using Meek rules [10] while maintaining consistency with the existing edges.
Given a causal network  causal relationships among variables can be directly read off the structure.
Due to the complexity of the P-C algorithm and unreliable high order conditional independence tests
[9]  several works [23  15] have incorporated the Markov Blanket (MB) discovery into the causal
discovery with a local-to-global approach. Growth and Shrink (GS) [9] algorithm uses the MBs
of each node to build the skeleton of a causal network  discover all the V-structures  and then use
the Meek rules to complete the global causal structure. The max-min hill climbing (MMHC) [23]
algorithm also ï¬nds MBs of each variable ï¬rst  but then uses the MBs as constraints to reduce the
search space for the score-based standard hill climbing structure learning methods. In [15]  authors

1

use Markov Blanket with Collider Sets (CS) to improve the efï¬ciency of the GS algorithm by com-
bining the spouse and V-structure discovery. All these local-to-global methods rely on the global
structure to ï¬nd the causal relationships and require ï¬nding the MBs for all nodes in a graph  even
if the interest is the causal relationships between one target variable and other variables. Differ-
ent MB discovery algorithms can be used and they can be divided into two different approaches:
non-topology-based and topology-based. Non-topology-based methods [5  9]  used by CS and GS
algorithms  greedily test the independence between each variable and the target by directly using the
deï¬nition of Markov Blanket. In contrast  more recent topology-based methods [22  1  11] aim to
improve the data efï¬ciency while maintaining a reasonable time complexity by ï¬nding the parents
and children (PC) set ï¬rst and then the spouses to complete the MB.
Local learning of causal networks generally aims to identify a subset of causal edges in a causal
network. Local Causal Discovery (LCD) algorithm and its variants [3  17  7] aim to ï¬nd causal edges
by testing the dependence/independence relationships among every four-variable set in a causal
network. Bayesian Local Causal Discovery (BLCD) [8] explores the Y-structures among MB nodes
to infer causal edges [6]. While LCD/BLCD algorithms aim to identify a subset of causal edges via
special structures among all variables  we focus on ï¬nding all the causal edges adjacent to one target
variable. In other words  we want to ï¬nd the causal identities of each node  in terms of direct causes
and effects  with respect to one target node. We ï¬rst use Markov Blankets to ï¬nd the direct causes
and effects  and then propose a new Causal Markov Blanket (CMB) discovery algorithm  which
determines the exact causal identities of MB nodes of a target node by tracking their conditional
independence changes  without ï¬nding the global causal structure of a causal network. The proposed
CMB algorithm is a complete local discovery algorithm and can identify the same direct causes and
effects for a target variable as global methods under standard assumptions. CMB is more scalable
than global methods  more efï¬cient than local-to-global methods  and is complete in identifying
direct causes and effects of one target while other local methods are not.

2 Backgrounds
We use V to represent the variable space  capital letters (such as X  Y ) to represent variables  bold
letters (such as Z  MB) to represent variable sets  and use |Z| to represent the size of set Z. X âŠ¥âŠ¥ Y
and X âŠ¥\âŠ¥ Y represent independence and dependence between X and Y   respectively. We assume
readers are familar with related concepts in causal network learning  and only review a few major
ones here. In a causal network or causal Bayesian Network [13]  nodes correspond to the random
variables in a variable set V. Two nodes are adjacent if they are connected by an edge. A directed
edge from node X to node Y   (X  Y ) âˆˆ V  indicates X is a parent or direct cause of Y and Y is
a child or direct effect of X [12]. Moreover  If there is a directed path from X to Y   then X is an
ancestor of Y and Y is a descendant of X. If nonadjacent X and Y have a common child  X and
Y are spouses. Three nodes X  Y   and Z form a V-structure [12] if Y has two incoming edges from
X and Z  forming X â†’ Y â† Z  and X is not adjacent to Z. Y is a collider in a path if Y has two
incoming edges in this path. Y with nonadjacent parents X and Z is an unshielded collider. A path
J from node X and Y is blocked [12] by a set of nodes Z  if any of following holds true: 1) there is
a non-collider node in J belonging to Z. 2) there is a collider node C on J such that neither C nor
any of its descendants belong to Z. Otherwise  J is unblocked or active.
A PDAG is a graph which may have both undirected and directed edges and has at most one edge
between any pair of nodes [10]. CPDAGs [2] represent Markov equivalence classes of DAGs  captur-
ing the same conditional independence relationships with the same skeleton but potentially different
edge orientations. CPDAGs contain directed edges that has the same orientation for every DAG in
the equivalent class and undirected edges that have reversible orientations in the equivalent class.
Let G be the causal DAG of a causal network with variable set V and P be the joint probability dis-
tribution over variables in V. G and P satisfy Causal Markov condition [13] if and only if  âˆ€X âˆˆ V 
X is independent of non-effects of X given its direct causes. The causal faithfulness condition [13]
states that G and P are faithful to each other  if all and every independence and conditional indepen-
dence entailed by P is present in G. It enables the recovery of G from sampled data of P . Another
widely-used assumption by existing causal discovery algorithms is causal sufï¬ciency [12]. A set of
variables X âŠ† V is causally sufï¬cient  if no set of two or more variables in X shares a common
cause variable outside V. Without causal sufï¬ciency assumption  latent confounders between adja-
cent nodes would be modeled by bi-directed edges [24]. We also assume no selection bias [20] and

2

we can capture the same independence relationships among variables from the sampled data as the
ones from the entire population.
Many concepts and properties of a DAG hold in causal networks  such as d-separation and MB.
A Markov Blanket [12] of a target variable T   MBT   in a causal network is the minimal set of
nodes conditioned on which all other nodes are independent of T   denoted as X âŠ¥âŠ¥ T|MBT  âˆ€X âŠ†
{V\ T}\ MBT . Given an unknown distribution P that satisï¬ed the Markov condition with respect
to an unknown DAG G0  Markov Blanket Discovery is the process used to estimate the MB of a
target node in G0  from independently and identically distributed (i.i.d) data D of P . Under the
causal faithfulness assumption between G0 and P   the MB of a target node T is unique and is the
set of parents  children  and spouses of T (i.e.  other parents of children of T ) [12]. In addition  the
parents and children set of T   PCT   is also unique. Intuitively  the MB can directly facilitate causal
discovery. If conditioning on the MB of a target variable T renders a variable X independent of
T   then X cannot be a direct cause or effect of T . From the local causal discovery point of view 
although MB may contain nodes with different causal relationships with the target  it is reasonable
to believe that we can identify their relationships exactly  up to the Markov equivalence  with further
tests.
Lastly  exiting causal network learning algorithms all use three Meek rules [10]  which we assume
the readers are familiar with  to orient as many edges as possible given all V-structures in PDAGs to
obtain CPDAG. The basic idea is to orient the edges so that 1) the edge directions do not introduce
new V-structures  2) preserve the no-cycle property of a DAG  and 3) enforce 3-fork V-structures.

3 Local Causal Discovery of Direct Causes and Effects

Existing MB discovery algorithms do not directly offer the exact causal identities of the learned MB
nodes of a target. Although the topology-based methods can ï¬nd the PC set of the target within
the MB set  they can only provide the causal identities of some children and spouses that form v-
structures. Nevertheless  following existing works [4  15]  under standard assumptions  every PC
variable of a target can only be its direct cause or effect:
Theorem 1. Causality within a MB. Under the causal faithfulness  sufï¬ciency  correct indepen-
dence tests  and no selection bias assumptions  the parent and child nodes within a targetâ€™s MB set
in a causal network contains all and only the direct causes and effects of the target variable.

The proof can be directly derived from the PC set deï¬nition of a causal network. Therefore  using
the topology-based MB discovery methods  if we can discover the exact causal identities of the PC
nodes within the MB  causal discovery of direct causes and effects of the target can therefore be
successfully accomplished.
Building on MB discovery  we propose a new local causal discovery algorithm  Causal Markov
Blanket (CMB) discovery as shown in Algorithm 1. It identiï¬es the direct causes and effects of a
target variable without the need of ï¬nding the global structure or the MBs of all other variables in
a causal network. CMB has three major steps: 1) to ï¬nd the MB set of the target and to identify
some direct causes and effects by tracking the independence relationship changes among a targetâ€™s
PC nodes before and after conditioning on the target node  2) to repeat Step 1 but conditioned on
one PC nodeâ€™s MB set  and 3) to repeat Step 1 and 2 with unidentiï¬ed neighboring nodes as new
targets to identify more direct causes and effects of the original target.
Step 1: Initial identiï¬cation. CMB ï¬rst ï¬nds the MB nodes of a target T   MBT   using a topology-
based MB discovery algorithm that also ï¬nds PCT . CMB then uses the CausalSearch subroutine 
shown in Algorithm 2  to get an initial causal identities of variables in PCT by checking every
variable pair in PCT according to Lemma 1.
Lemma 1. Let (X  Y ) âˆˆ PCT   the PC set of the target T âˆˆ V in a causal DAG. The independence
relationships between X and Y can be divided into the following four conditions:
C1 X âŠ¥âŠ¥ Y and X âŠ¥âŠ¥ Y |T ; this condition can not happen.
C2 X âŠ¥âŠ¥ Y and X âŠ¥\âŠ¥ Y |T â‡’ X and Y are both the parents of T .
C3 X âŠ¥\âŠ¥ Y and X âŠ¥âŠ¥ Y |T â‡’ at least one of X and Y is a child of T .
C4 X âŠ¥\âŠ¥ Y and X âŠ¥\âŠ¥ Y |T â‡’ their identities are inconclusive and need further tests.

3

Algorithm 1 Causal Markov Blanket Discovery Algorithm
1: Input: D: Data; T : target variable
2: Output: IDT :

the causal identities of all

13:

nodes with respect to T
{Step 1: Establish initial ID }
3: IDT = zeros(|V|  1);
4: (MBT   PCT ) â† F indM B(T D);
5: Z â† âˆ…;
6: IDT â† CausalSearch(D  T  PCT   Z  IDT )
7: for one X in each pair (X  Y ) with idT = 4 do
8: MBX â† F indM B(X D);
9:
10:

{Step 2: Further test variables with idT = 4}

Z â† {MBX \ T} \ Y ;
IDT â† CausalSearch(D  T  PCT   Z  IDT );
if no element of IDT is equal to 4  break;

11:
12: for every pair of parents (X  Y ) of T do

if âˆƒZ s.t. (X  Z) and (Y  Z) are idT = 4 pairs
then

IDT (Z) = 1;

14:
15: IDT (X) â† 3 âˆ€X that IDT (X) = 4;
16: for each X with idT = 3 do
17:

{Step 3: Resolve variable set with idT = 3}

Recursively ï¬nd IDX  without going back to
the already queried variables;
update IDT according to IDX;
if IDX (T ) = 2 then

IDT (X) = 1;
for every Y in idT = 3 variable pairs
(X  Y ) do

18:
19:
20:
21:

IDT (Y ) = 2;

22:
23:
24: Return: IDT

if no element of IDT is equal to 3  break;

13:
14:
15:
16:
17:
18:
19:
20:
21:

Algorithm 2 CausalSearch Subroutine
1: Input: D: Data; T : target variable; PCT :
the PC set of T ; Z: the conditioned variable
set; ID: current ID

2: Output: IDT : the new causal identities of

all nodes with respect to T
{Step 1: Single PC }
3: if |PCT| = 1 then
IDT (PCT ) â† 3;
4:
{Step 2: Check C2 & C3}
5: for every X  Y âˆˆ PCT do
6:
7:
8:
9:
10:
11:
12:

IDT (X) â† 1; IDT (Y ) â† 1;
if IDT (X) = 1 then
else if IDT (Y ) (cid:54)= 2 then

IDT (Y ) â† 2
IDT (Y ) â† 3

if X âŠ¥âŠ¥ Y |Z and X âŠ¥\âŠ¥ Y |T âˆª Z then
else if X âŠ¥\âŠ¥ Y |Z and X âŠ¥âŠ¥ Y |TâˆªZ then

if IDT (Y ) = 1 then
else if IDT (X) (cid:54)= 2 then

IDT (X) â† 2
IDT (X) â† 3

add (X  Y ) to pairs with idT = 3

else

if IDT (X) & IDT (Y ) = 0 or 4 then

IDT (X) â† 4; IDT (Y ) â† 4
add (X  Y ) to pairs with idT = 4

{Step 3: identify idT = 3 pairs with known
parents}

22: for every X such that IDT (X) = 1 do
23:

for every Y in idT = 3 variable pairs
(X  Y ) do

IDT (Y ) â† 2;

24:
25: Return: IDT

C1 does not happen because the path X âˆ’ T âˆ’ Y is unblocked either not given T or given T   and
the unblocked path makes X and Y dependent on each other. C2 implies that X and Y form a
V-structure with T as the corresponding collider  such as node C in Figure 1a which has two parents
A and B. C3 indicates that the paths between X and Y are blocked conditioned on T   which means
that either one of (X  Y ) is a child of T and the other is a parent  or both of (X  Y ) are children of
T . For example  node D and F in Figure 1a satisfy this condition with respect to E. C4 shows that
there may be another unblocked path from X and Y besides X âˆ’ T âˆ’ Y . For example  in Figure
1b  node D and C have multiple paths between them besides D âˆ’ T âˆ’ C. Further tests are needed
to resolve this case.
Notation-wise  we use IDT to represent the causal identities for all the nodes with respect to T  
IDT (X) as variable Xâ€™s causal identity to T   and the small case idT as the individual ID of a node
to T . We also use IDX to represent the causal identities of nodes with respect to node X. To avoid
changing the already identiï¬ed PCs  CMB establishes a priority system1. We use the idT = 1 to
represent nodes as the parents of T   idT = 2 children of T   idT = 3 to represent a pair of nodes that
cannot be both parents (and/or ambiguous pairs from Markov equivalent structures  to be discussed
at Step 2)  and idT = 4 to represent the inconclusiveness. A lower number id cannot be changed

1Note that the identiï¬cation number is slightly different from the condition number in Lemma 1.

4

Figure 1: a) A Sample Causal Network. b) A Sample Network with C4 nodes. The only active path
between D and C conditioned on MBC \ {T  D} is D âˆ’ T âˆ’ C.

into a higher number (shown by Line 11âˆ¼15 of Algorithm 2). If a variable pair satisï¬es C2  they
will both be labeled as parents (Line 7 of Algorithm 2). If a variable pair satisï¬es C3  one of them
is labeled as idT = 2 only if the other variable within the pair is already identiï¬ed as a parent;
otherwise  they are both labeled as idT = 3 (Line 9âˆ¼12 and 15âˆ¼17 of Algorithm 2). If a PC node
remains inconclusive with idT = 0  it is labeled as idT = 4 in Line 20 of Algorithm 2. Note that
if T has only one PC node  it is labeled as idT = 3 (Line 4 of Algorithm 2). Non-PC nodes always
have idT = 0.
Step 2: Resolve idT = 4. Lemma 1 alone cannot identify the variable pairs in PCT with idT = 4
due to other possible unblocked paths  and we have to seek other information. Fortunately  by
deï¬nition  the MB set of one of the targetâ€™s PC node can block all paths to that PC node.
Lemma 2. Let (X  Y ) âˆˆ PCT   the PC set of the target T âˆˆ V in a causal DAG. The independence
relationships between X and Y   conditioned on the MB of X minus {Y  T}  MBX \ {Y  T}  can
be divided into the following four conditions:
C1 X âŠ¥âŠ¥ Y |MBX \ {Y  T} and X âŠ¥âŠ¥ Y |T âˆª MBX \ Y ; this condition can not happen.
C2 X âŠ¥âŠ¥ Y |MBX \ {Y  T} and X âŠ¥\âŠ¥ Y |T âˆª MBX \ Y â‡’ X and Y are both the parents of T .
C3 X âŠ¥\âŠ¥ Y |MBX \{Y  T} and X âŠ¥âŠ¥ Y |T âˆª MBX \ Y â‡’ at least one of X and Y is a child of T .
C4 X âŠ¥\âŠ¥ Y |MBX \ {Y  T} and X âŠ¥\âŠ¥ Y |T âˆª MBX \ Y â‡’ then X and Y is directly connected.
C1âˆ¼3 are very similar to those in Lemma 1. C4 is true because  conditioned on T and the MB of X
minus Y   the only potentially unblocked paths between X and Y are X âˆ’ T âˆ’ Y and/or X âˆ’ Y . If
C4 happens  then the path Xâˆ’T âˆ’Y has no impact on the relationship between X and Y   and hence
X âˆ’ Y must be directly connected. If X and Y are not directly connected and the only potentially
unblocked path between X and Y is X âˆ’ T âˆ’ Y   and X and Y will be identiï¬ed by Line 10 of
Algorithm 1 with idT âˆˆ {1  2  3}. For example in Figure 1b  conditioned on MBC \ {T  D}  i.e. 
{A  B}  the only path between C and D is through T. However  if X and Y are directly connected 
they will remain with idT = 4 (such as node D and E from Figure 1b). In this case  X  Y   and
T form a fully connected clique  and edges among the variables that form a fully connected clique
can have many different orientation combinations without affecting the conditional independence
relationships. Therefore  this case needs further tests to ensure Meek rules are satisï¬ed. The third
Meek rule (enforcing 3-fork V-structures) is ï¬rst enforced by Line 14 of Algorithm 1. Then the rest
of idT = 4 nodes are changed to have idT = 3 by Line 15 of Algorithm 1 and to be further processed
(even though they could be both parents at the same time) with neighbor nodesâ€™ causal identities.
Therefore  Step 2 of Algorithm 1 makes all variable pairs with idT = 4 to become identiï¬ed either
as parents  children  or with idT = 3 after taking some neighborsâ€™ MBs into consideration. Note
that Step 2 of CMB only needs to ï¬nd the MBâ€™s for a small subset of the PC variables (in fact only
one MB for each variable pair with idT = 4).
Step 3: Resolve idT = 3. After Step 2  some PC variables may still have idT = 3. This could
happen because of the existence of Markov equivalence structures. Below we show the condition
under which the CMB can resolve the causal identities of all PC nodes.

5

ABCDGEFTCBDAE(ğ‘)(ğ‘)Lemma 3. The Identiï¬ability Condition. For Algorithm 1 to fully identify all the causal relation-
ships within the PC set of a target T   1) T must have at least two nonadjacent parents  2) one of T â€™s
single ancestors must contain at least two nonadjacent parents  or 3) T has 3 parents that form a
3-fork pattern as deï¬ned in Meeks rules.

We use single ancestors to represent ancestor nodes that do not have a spouse with a mutual child that
is also an ancestor of T . If the target does not meet any of the conditions in Lemma 2  C2 will never
be satisï¬ed and all PC variables within a MB will have idT = 3. Without a single parent identiï¬ed 
it is impossible to infer the identities of children nodes using C3. Therefore  all the identities of the
PC nodes are uncertain  even though the resulting structure could be a CPDAG.
Step 3 of CMB searches for a non-single ancestor of T to infer the causal directions. For each node
X with idT = 3  CMB tries to identify its local causal structure recursively. If Xâ€™s PC nodes are
all identiï¬ed  it would return to the target with the resolved identities; otherwise  it will continue
to search for a non-single ancestor of X. Note that CMB will not go back to already-searched
variables with unresolved PC nodes without providing new information. Step 3 of CMB checks the
identiï¬ability condition for all the ancestors of the target. If a graph structure does not meet the
conditions of Lemma 3  the ï¬nal IDT will contain some idT = 3  which indicates reversible edges
in CPDAGs. The found causal graph using CMB will be a PDAG after Step 2 of Algorithm 1  and
it will be a CPDAG after Step 3 of Algorithm 1.
Case Study. The procedure using CMB to identify the direct causes and effects of E in Figure 1a
has the following 3 steps. Step 1: CMB ï¬nds the MB and PC set of E. The PC set contains node
D and F . Then  IDE(D) = 3 and IDE(F ) = 3. Step 2: to resolve the variable pair D and F
with idE = 3  1) CMB ï¬nds the PC set of D  containing C  E  and G. Their idD are all 3â€™s  since
D contains only one parent. 2) To resolve IDD  CMB checks causal identities of node C and G
(without going back to E). The PC set of C contains A  B  and D. CMB identiï¬es IDC(A) = 1 
IDC(B) = 1  and IDC(D) = 2. Since C resolves all its PC nodes  CMB returns to node D
with IDD(C) = 1. 3) With the new parent C  IDD(G) = 2  IDD(E) = 2  and CMB returns to
node E with IDE(D) = 1. Step 3: the IDE(D) = 1  and after resolving the pair with idE = 3 
IDE(F ) = 2.
Theorem 2. The Soundness and Completeness of CMB Algorithm. If the identiï¬ability condition
is satisï¬ed  using a sound and complete MB discovery algorithm  CMB will identify the direct causes
and effects of the target under the causal faithfulness  sufï¬ciency  correct independence tests  and
no selection bias assumptions.

Proof. A sound and complete MB discovery algorithm ï¬nd all and only the MB nodes of a target.
Using it and under the causal sufï¬ciency assumption  the learned PC set contains all and only the
cause-effect variables by Theorem 1. When Lemma 3 is satisï¬ed  all parent nodes are identiï¬able
through V-structure independence changes  either by Lemma 1 or by Lemma 2. Also since children
cannot be conditionally independent of another PC node given its MB minus the target node (C2) 
all parents identiï¬ed by Lemma 1 and 2 will be the true positive direct causes. Therefore  all and
only the true positive direct causes will be correctly identiï¬ed by CMB. Since PC variables can only
be direct causes or direct effects  all and only the direct effects are identiï¬ed correctly by CMB.

In the cases where CMB fails to identify all the PC nodes  global causal discovery methods cannot
identify them either. Speciï¬cally  structures failing to satisfy Lemma 3 can have different orien-
tations on some edges while preserving the skeleton and v-structures  hence leading to Markov
equivalent structures. For the cases where T has all single ancestors  the edge directions among all
single ancestors can always be reversed without introducing new V-structures and DAG violations 
in which cases the Meek rules cannot identify the causal directions either. For the cases with fully
connected cliques  these fully connected cliques do not meet the nonadjacent-parents requirement
for the ï¬rst Meek rule (no new V-structures)  and the second Meek rule (preserving DAGs) can
always be satisï¬ed within a clique by changing the direction of one edge. Since CMB orients the
3-fork V-structure in the third Meek rule correctly by Line 12âˆ¼14 of Algorithm 1  CMB can identify
the same structure as the global methods that use the Meek rules.
Theorem 3. Consistency between CMB and Global Causal Discovery Methods. For the same
DAG G  Algorithm 1 will correctly identify all the direct causes and effects of a target variable T

6

as the global and local-to-global causal discovery methods2 that use the Meek rules [10]  up to Gâ€™s
CPDAG under the causal faithfulness  sufï¬ciency  correct independence tests  and no selection bias
assumptions.

Proof. It has been shown that causal methods using Meek rules [10] can identify up to a graphâ€™s
CPDAG. Since Meek rules cannot identify the structures that fail Lemma 3  the global and local-to-
global methods can only identify the same structures as CMB. Since CMB is sound and complete in
identifying these structures by Theorem 2  CMB will identify all direct causes and effects up to Gâ€™s
CPDAG.

3.1 Complexity

The complexity of CMB algorithm is dominated by the step of ï¬nding the MB  which can have an
exponential complexity [1  16]. All other steps of CMB are trivial in comparison. If we assume a
uniform distribution on the neighbor sizes in a network with N nodes  then the expected time com-
plexity of Step 1 of CMB is O( 1
N )  while local-to-global methods are O(2N ).
N
In later steps  CMB also needs to ï¬nd MBs for a small subset of nodes that include 1) one node
between every pair of nodes that meet C4  and 2) a subset of the targetâ€™s neighboring nodes that
provide additional clues for the target. Let l be the total size of these nodes  then CMB reduces the
cost by N
l

times asymptotically.

i=1 2i) = O( 2N

(cid:80)N

4 Experiments

We use benchmark causal learning datasets to evaluate the accuracy and efï¬ciency of CMB with
four other causal discovery algorithms discussed: P-C  GS  MMHC  CS  and the local causal dis-
covery algorithm LCD2 [7]. Due to page limit  we show the results of the causal algorithms on four
medium-to-large datasets: ALARM  ALARM3  CHILD3  and INSUR3. They contain 37 to 111
nodes. We use 1000 data samples for all datasets. For each global or local-to-global algorithm  we
ï¬nd the global structure of a dataset and then extract causal identities of all nodes to a target node.
CMB ï¬nds causal identities of every variable with respect to the target directly. We repeat the dis-
covery process for each node in the datasets  and compare the discovered causal identities of all the
algorithms to all the Markov equivalent structures with the known ground truth structure. We use the
edge scores [15] to measure the number of missing edges  extra edges  and reversed edges3 in each
nodeâ€™s local causal structure and report average values along with its standard deviation  for all the
nodes in a dataset. We use the existing implementation [21] of HITON-MB discovery algorithm to
ï¬nd the MB of a target variable for all the algorithms. We also use the existing implementations [21]
for P-C  MMHC  and LCD2 algorithms. We implement GS  CS  and the proposed CMB algorithms
in MATLAB on a machine with 2.66GHz CPU and 24GB memory. Following the existing proto-
col [15]  we use the number of conditional independence tests needed (or scores computed for the
score-based search method MMHC) to ï¬nd the causal structures given the MBs4  and the number
of times that MB discovery algorithms are invoked to measure the efï¬ciency of various algorithms.
We also use mutual-information-based conditional independence tests with a standard signiï¬cance
level of 0.02 for all the datasets without worrying about parameter tuning.
As shown in Table 1  CMB consistently outperforms the global discovery algorithms on benchmark
causal networks  and has comparable edge accuracy with local-to-global algorithms. Although CMB
makes slightly more total edge errors in ALARM and ALARM3 datasets than CS  CMB is the best
method on CHILD3 and INSUR3. Since LCD2 is an incomplete algorithm  it never ï¬nds extra or
reversed edges but misses the most amount of edges. Efï¬ciency-wise  CMB can achieve more than
one order of magnitude speedup  sometimes two orders of magnitude as shown in CHILD3 and
INSUR3  than the global methods. Compared to local-to-global methods  CMB also can achieve

2We specify the global and local-to-global causal methods to be P-C [19]  GS [9] and CS [15].
3If an edge is reversible in the equivalent class of the original graph but are not in the equivalent class of the

learned graph  it is considered as reversed edges as well.

4For global methods  it is the number of tests needed or scores computed given the moral graph of the global

structure. For LCD2  it would be the total number of tests since it does not use moral graph or MBs.

7

Table 1: Performance of Various Causal Discovery Algorithms on Benchmark Networks

Dataset Method
Alarm

GS
CS

GS
CS

Errors:
Extra
1.59Â±0.19
P-C
MMHC 1.29Â±0.18
0.39Â±0.44
0.42Â±0.10
0.00Â±0.00
LCD2
0.69Â±0.13
CMB
3.71Â±0.57
P-C
MMHC 2.36Â±0.11
1.24Â±0.23
1.26Â±0.16
0.00Â±0.00
LCD2
1.41Â±0.13
CMB
4.32Â±0.68
P-C
MMHC 1.98Â±0.10
0.88Â±0.04
0.94Â±0.20
0.00Â±0.00
LCD2
0.92Â±0.12
CMB
4.76Â±1.33
P-C
MMHC 2.39Â±0.18
1.94Â±0.06
1.92Â±0.08
0.00Â±0.00
1.72Â±0.07

LCD2
CMB

GS
CS

GS
CS

Alarm3

Child3

Insur3

Edges
Missing
2.19Â±0.14
1.94Â±0.09
0.87Â±0.48
0.64Â±0.10
2.49Â±0.00
0.61Â±0.11
2.21Â±0.25
2.45Â±0.08
1.41Â±0.05
1.47Â±0.08
3.85Â±0.00
1.55Â±0.27
2.69Â±0.08
1.57Â±0.04
0.75Â±0.08
0.91Â±0.14
2.63Â±0.00
0.84Â±0.16
2.50Â±0.11
2.53Â±0.06
1.44Â±0.05
1.56Â±0.06
5.03Â±0.00
1.39Â±0.06

Reversed
0.32Â±0.10
0.24Â±0.06
1.13Â±0.23
0.38Â±0.08
0.00Â±0.0
0.51Â±0.10
1.37Â±0.04
0.72Â±0.08
0.99Â±0.14
0.63Â±0.14
0.00Â±0.0
0.78Â±0.25
0.84Â±0.10
0.43Â±0.04
1.03Â±0.08
0.53Â±0.08
0.00Â±0.0
0.60Â±0.10
1.29Â±0.11
0.76Â±0.07
1.19Â±0.10
0.89Â±0.09
0.00Â±0.0
1.19Â±0.05

Total

4.10Â±0.19
3.46Â±0.23
2.39Â±0.44
1.43Â±0.10
2.49Â±0.00
1.81Â±0.11
7.30Â±0.68
5.53Â±0.27
3.64Â±0.13
3.38Â±0.13
3.85Â±0.00
3.73Â±0.11
7.76Â±0.98
4.00Â±0.93
2.66Â±0.33
2.37Â±0.33
2.63Â±0.00
2.36Â±0.31
8.55Â±0.81
5.68Â±0.43
4.57Â±0.33
4.37Â±0.23
5.03Â±0.00
4.30Â±0.21

Efï¬ciency
No. Tests
4.0e3Â±4.0e2
1.8e3Â±1.7e3
586.5Â±72.2
331.4Â±61.9
1.4e3Â±0
53.7Â±4.5
1.6e4Â±4.0e2
3.7e3Â±6.1e2
2.1e3Â±1.2e2
699.1Â±60.4
1.2e4Â±0
50.3Â±6.2
8.3e4Â±2.9e3
6.6e3Â±8.2e2
2.1e3Â±2.5e2
1.0e3Â±4.8e2
3.6e3Â±0
78.2Â±15.2
2.5e5Â±1.2e4
3.1e4Â±5.2e2
4.5e4Â±2.2e3
2.6e4Â±3.9e3
6.6e3Â±0
159.8Â±38.5

-

No. MB
37Â±0
37Â±0
37Â± 0

-

-

2.61 Â± 0.12
111 Â± 0
111 Â± 0
111Â±0

2.58 Â± 0.09

-

-

-

-

-

60 Â±0
60Â±0
60Â± 0

81 Â± 0
81Â±0
81Â±0

2.53 Â± 0.15

2.46 Â± 0.11

more than one order of speedup on ALARM3  CHILD3  and INSUR3. In addition  on these datasets 
CMB only invokes MB discovery algorithms between 2 to 3 times  drastically reducing the MB calls
of local-to-global algorithms. Since independence test comparison is unfair to LCD2 who does not
use MB discovery or ï¬nd moral graphs  we also compared time efï¬ciency between LCD2 and CMB.
CMB is 5 times faster on ALARM  4 times faster on ALARM3 and CHILD3  and 8 times faster on
INSUR3 than LCD2.
In practice  the performance of CMB depends on two factors: the accuracy of independence tests
and MB discovery algorithms. First  independence tests may not always be accurate and could
introduce errors while checking the four conditions of Lemma 1 and 2  especially under insufï¬cient
data samples. Secondly  causal discovery performance heavily depends on the performance of the
MB discovery step  as the error could propagate to later steps of CMB. Improvements on both areas
could further improve CMBâ€™s accuracy. Efï¬ciency-wise  CMBâ€™s complexity can still be exponential
and is dominated by the MB discovery phrase  and thus its worst case complexity could be the same
as local-to-global approaches for some special structures.

5 Conclusion

We propose a new local causal discovery algorithm CMB. We show that CMB can identify the
same causal structure as the global and local-to-global causal discovery algorithms with the same
identiï¬cation condition  but uses a fraction of the cost of the global and local-to-global approaches.
We further prove the soundness and completeness of CMB. Experiments on benchmark datasets
show the comparable accuracy and greatly improved efï¬ciency of CMB for local causal discovery.
Possible future works could study assumption relaxations  especially without the causal sufï¬ciency
assumption  such as by using a similar procedure as FCI algorithm and the improved CS algorithm
[14] to handle latent variables in CMB.

8

References
[1] Constantin Aliferis  Ioannis Tsamardinos  Alexander Statnikov  C. F. Aliferis M. D  Ph. D  I. Tsamardi-
nos Ph. D  and Er Statnikov M. S. Hiton  a novel markov blanket algorithm for optimal variable selection 
2003.

[2] David Maxwell Chickering. Optimal structure identiï¬cation with greedy search. Journal of Machine

Learning Research  2002.

[3] Gregory F Cooper. A simple constraint-based algorithm for efï¬ciently mining observational databases for

causal relationships. Data Mining and Knowledge Discovery  1(2):203â€“224  1997.

[4] Isabelle Guyon  Andre Elisseeff  and Constantin Aliferis. Causal feature selection. 2007.
[5] Daphne Koller and Mehran Sahami. Toward optimal feature selection. In ICML 1996  pages 284â€“292.

Morgan Kaufmann  1996.

[6] Subramani Mani  Constantin F Aliferis  Alexander R Statnikov  and MED NYU. Bayesian algorithms for

causal data mining. In NIPS Causality: Objectives and Assessment  pages 121â€“136  2010.

[7] Subramani Mani and Gregory F Cooper. A study in causal discovery from population-based infant birth
and death records. In Proceedings of the AMIA Symposium  page 315. American Medical Informatics
Association  1999.

[8] Subramani Mani and Gregory F Cooper. Causal discovery using a bayesian local causal discovery algo-

rithm. Medinfo  11(Pt 1):731â€“735  2004.

[9] Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. In Ad-

vances in Neural Information Processing Systems 12  pages 505â€“511. MIT Press  1999.

[10] Christopher Meek. Causal inference and causal explanation with background knowledge. In Proceedings
of the Eleventh conference on Uncertainty in artiï¬cial intelligence  pages 403â€“410. Morgan Kaufmann
Publishers Inc.  1995.

[11] Teppo Niinimaki and Pekka Parviainen. Local structure disocvery in bayesian network. In Proceedings

of Uncertainy in Artiï¬cal Intelligence  Workshop on Causal Structure Learning  pages 634â€“643  2012.

[12] Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan

Kaufmann Publishers  Inc.  2 edition  1988.

[13] Judea Pearl. Causality: models  reasoning and inference  volume 29. Cambridge Univ Press  2000.
[14] Jean-Philippe Pellet and AndrÂ´e Elisseeff. Finding latent causes in causal networks: an efï¬cient approach
based on markov blankets. In Advances in Neural Information Processing Systems  pages 1249â€“1256 
2009.

[15] Jean-Philippe Pellet and Andre Ellisseeff. Using markov blankets for causal structure learning. Journal

of Machine Learning  2008.

[16] Jose M. Pe`oa  Roland Nilsson  Johan BjÂ¨orkegren  and Jesper TegnÂ´er. Towards scalable and data efï¬cient

learning of markov boundaries. Int. J. Approx. Reasoning  45(2):211â€“232  July 2007.

[17] Craig Silverstein  Sergey Brin  Rajeev Motwani  and Jeff Ullman. Scalable techniques for mining causal

structures. Data Mining and Knowledge Discovery  4(2-3):163â€“192  2000.

[18] P. Spirtes  C. Glymour  and R. Scheines. Causation  Prediction  and Search. The MIT Press  2nd edition 

2000.

[19] Peter Spirtes  Clark Glymour  Richard Scheines  Stuart Kauffman  Valerio Aimale  and Frank Wimberly.

Constructing bayesian network models of gene expression networks from microarray data  2000.

[20] Peter Spirtes  Christopher Meek  and Thomas Richardson. Causal inference in the presence of latent
variables and selection bias. In Proceedings of the Eleventh conference on Uncertainty in artiï¬cial intel-
ligence  pages 499â€“506. Morgan Kaufmann Publishers Inc.  1995.

[21] Alexander Statnikov  Ioannis Tsamardinos  Laura E. Brown  and Constatin F. Aliferis. Causal explorer:
A matlab library for algorithms for causal discovery and variable selection for classiï¬cation. In Causation
and Prediction Challenge at WCCI  2008.

[22] Ioannis Tsamardinos  Constantin F. Aliferis  and Alexander Statnikov. Time and sample efï¬cient discov-
ery of markov blankets and direct causal relations. In Proceedings of the ninth ACM SIGKDD interna-
tional conference on Knowledge discovery and data mining  KDD â€™03  pages 673â€“678  New York  NY 
USA  2003. ACM.

[23] Ioannis Tsamardinos  LauraE. Brown  and ConstantinF. Aliferis. The max-min hill-climbing bayesian

network structure learning algorithm. Machine Learning  65(1):31â€“78  2006.

[24] Jiji Zhang. On the completeness of orientation rules for causal discovery in the presence of latent con-

founders and selection bias. Artiï¬cial Intelligence  172(16):1873â€“1896  2008.

9

,Tian Gao
Qiang Ji
Mikhail Yurochkin
XuanLong Nguyen
nikolaos Vasiloglou
Zhuoning Yuan
Yan Yan
Rong Jin
Tianbao Yang