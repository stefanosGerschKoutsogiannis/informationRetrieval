2011,Quasi-Newton Methods for Markov Chain Monte Carlo,The performance of Markov chain Monte Carlo methods is often sensitive to the scaling and correlations between the random variables of interest. An important source of information about the local correlation and scale is given by the Hessian matrix of the target distribution  but this is often either computationally expensive or infeasible. In this paper we propose MCMC samplers that make use of quasi-Newton approximations from the optimization literature  that approximate the Hessian of the target distribution from previous samples and gradients generated by the sampler. A key issue is that MCMC samplers that depend on the history of previous states are in general not valid. We address this problem by using limited memory quasi-Newton methods  which depend only on a fixed window of previous samples. On several real world datasets  we show that the quasi-Newton sampler is a more effective sampler than standard Hamiltonian Monte Carlo at a fraction of the cost of MCMC methods that require higher-order derivatives.,Quasi-Newton Methods

for Markov Chain Monte Carlo

Yichuan Zhang and Charles Sutton

School of Informatics
University of Edinburgh

Y.Zhang-60@sms.ed.ac.uk  csutton@inf.ed.ac.uk

Abstract

The performance of Markov chain Monte Carlo methods is often sensitive to the
scaling and correlations between the random variables of interest. An important
source of information about the local correlation and scale is given by the Hessian
matrix of the target distribution  but this is often either computationally expensive
or infeasible. In this paper we propose MCMC samplers that make use of quasi-
Newton approximations  which approximate the Hessian of the target distribution
from previous samples and gradients generated by the sampler. A key issue is that
MCMC samplers that depend on the history of previous states are in general not
valid. We address this problem by using limited memory quasi-Newton methods 
which depend only on a ﬁxed window of previous samples. On several real world
datasets  we show that the quasi-Newton sampler is more effective than standard
Hamiltonian Monte Carlo at a fraction of the cost of MCMC methods that require
higher-order derivatives.

1

Introduction

The design of effective approximate inference methods for continuous variables often requires con-
sidering the curvature of the target distribution. This is especially true of Markov chain Monte Carlo
(MCMC) methods. For example  it is well known that the Gibbs sampler mixes extremely poorly
on distributions that are strongly correlated. In a similar way  the performance of a random walk
Metropolis-Hastings algorithm is sensitive to the variance of the proposal distribution. Many sam-
plers can be improved by incorporating second-order information about the target distribution. For
example  several authors have used a Metropolis-Hastings algorithm in which the Hessian is used
to form a covariance for a Gaussian proposal [3  11]. Recently  Girolami and Calderhead [5] have
proposed a Hamiltonian Monte Carlo method that can require computing higher-order derivatives of
the target distribution.
Unfortunately  second derivatives can be inconvenient or infeasible to obtain and the quadratic cost
of manipulating a d× d Hessian matrix can also be prohibitive. An appealing idea is to approximate
the Hessian matrix using the sequence of ﬁrst order information of previous samples  in a manner
similar to quasi-Newton methods from the optimization literature. However  samplers that depend on
the history of previous samples must be carefully designed in order to guarantee the chain converges
to the target distribution.
In this paper  we present quasi-Newton methods for MCMC that are based on approximations to the
Hessian from ﬁrst-order information. In particular  we present a Hamiltonian Monte Carlo algorithm
in which the variance of the momentum variables is based on a BFGS approximation. The key point
is that we use a limited memory approximation  in which only a small window of previous samples
are used to the approximate the Hessian. This makes it straightforward to show that our samplers are
valid  because the samples are distributed as a order-k Markov chain. Second  by taking advantage

1

of the special structure in the Hessian approximation  the samplers require only linear time and
linear space in the dimensionality of the problem. Although this is a very natural approach  we are
unaware of previous MCMC methods that use quasi-Newton approximations. In general we know
of very few MCMC methods that make use of the rich set of approximations from the numerical
optimization literature (some exceptions include [7  11]). On several logistic regression data sets 
we show that the quasi-Newton samplers produce samples of higher quality than standard HMC  but
with signiﬁcantly less computation time than methods that require higher-order derivatives.

2 Background

In this section we provide background on Hamiltonian Monte Carlo. An excellent recent tutorial
is given by Neal [9]. Let x be a random variable on state space X = Rd with a target probability
distribution π(x) ∝ exp(L(x)) and p be a Gaussian random variable on P = Rd with density
p(p) = N (p|0  M) where M is the covariance matrix.
In general  Hamiltonian Monte Carlo
(HMC) deﬁnes a stationary Markov chain on the augmented state space X × P with invariant dis-
tribution p(x  p) = π(x)p(p). The sampler is deﬁned using a Hamiltonian function  which up to a
constant is the negative log density of (x  p)  given as follows:

H(x  p) = − L(x) +

pT M−1p.

(1)
In an analogy to physical systems  the ﬁrst term on the RHS is called the potential energy  the second
term is called the kinetic energy  the state x is called the position variable  and p the momentum
variable. Finally  we will call the covariance M the mass matrix. The most common mass matrix
is the identity matrix I. Samples in HMC are generated as following. First  the state p is resampled
from its marginal distribution N (p|0  M). Then  given the current state (x  p)  a new state (x∗  p∗)
is generated by a deterministic simulation of Hamiltonian dynamics:

1
2

˙x = M−1p;

˙p = − ∇xL(x).

(2)
One common approximation to this dynamical system is given by the leapfrog algorithm. One single
iteration of leapfrog algorithm is given by the recursive formula
∇xL(x(τ )) 
p(τ +
x(τ + ) = x(τ ) + M−1p(τ +

2

) = p(τ ) +

(4)

(3)


2


2

) 


2

∇xL(x(τ + )) 


2

) +

p(τ + ) = p(τ +

(5)
where  is the step size and τ is a discrete time variable. The leapfrog algorithm is initialised by
the current sample  that is (x(0)  p(0)) = (x  p). After L leapfrog steps (3)-(5)  the ﬁnal state
(x(L)  p(L)) is used as the proposal (x∗  p∗) in Metropolis-Hastings correction with acceptance
probability min[1  exp(H(x  p) − H(x∗  p∗))]. The step size  and the number of leapfrog steps L
are two parameters of HMC.
In many applications  different components of x may have different scale and be highly correlated.
Tuning HMC in such a situation can be very difﬁcult. However  the performance of HMC can be
improved by multiplying the state x by a non-singular matrix A. If A is chosen well  the transformed
state x(cid:48) = Ax may at least locally be better conditioned  i.e.  the new variables x(cid:48) may be less
correlated and have similar scale  so that sampling can be easier.
In the context of HMC  this
transformation is equivalent to changing mass matrix M. This is because the Hamiltonian dynamics
of the system (Ax  p) with mass matrix M are isomorphic to the dynamics on (x  AT p)  which
is equivalent to deﬁning the state as (x  p) and using the mass matrix M(cid:48) = AT MA. For a more
detailed version of this argument  see the tutorial of Neal [9]. So in this paper we will concentrate
on tuning M on the ﬂy during sampling.
Now  if L has a constant Hessian B (or nearly so)  then a reasonable choice of transformation is to
choose A so that B = AAT   because then the Hessian of the log density over x(cid:48) will be nearly the
identity. This corresponds to a choice of M = B. For more general functions without a constant
Hessian  this argument suggests the idea of employing a mass matrix M(x) that is a function of the
position. In this case the Hamiltonian function can be

H(x  p) = − L(x) +

log(2π)d|M(x)| +

1
2

pT M(x)

−1p 

1
2

(6)

2

where the second term on the RHS is from the normalisation factor of Gaussian momentum variable.

3 Quasi-Newton Approximations for Sampling

In this section  we describe the Hessian approximation that is used in our samplers. It is based on
the well-known BFGS approximation [10]  but there are several customizations that we must make
to use it within a sampler. First we explain quasi-Newton methods in the context of optimization.
Consider minimising the function f : Rd → R  quasi-Newton methods search for the minimum of
f (x) by generating a sequence of iterates xk+1 = xk−αkHk∇f (xk) where Hk is an approximation
to the inverse Hessian at xk  which is computed from the previous function values and gradients.
One of the most popular large scale quasi-Newton methods is limited-Memory BFGS (L-BFGS) [10].
Given the previous m iterates xk−m+1  xk−m+2  . . . xk  the L-BFGS approximation Hk+1 is

Hk+1 = (I − yksT
k
sT
k yk

)Hk(I − skyT
k
sT
k yk

) + sksT
k

(7)
where sk = xk+1 − xk and yk = ∇fk+1 − ∇fk. The base case of the recursion is typically chosen
as Hk−m = γI for some γ ∈ R. If m = k  then this is called the BFGS formula  and typically it is
implemented by storing the full d × d matrix Hk. If m < k  however  this is called limited-memory
BFGS  and can be implemented much more efﬁciently. It can be seen that the BFGS formula (7) is
a rank-two update to the previous Hessian approximation Hk. Therefore Hk+1 is a diagonal matrix
plus a rank 2m matrix  so the matrix vector product Hk∇f (xk) can be computed in linear time
O(md). Typically the product Hv is implemented by a special two-loop recursive algorithm [10].
In contrast to optimization methods  most sampling methods need a factorized form of Hk to draw
samples from N (0  Hk). More precisely  we adopt the factorisation Hk = SkST
k   so that we can
generate a sample as p = Skz where z ∼ N (0  I). The matrix operations to obtain Sk  e.g. the
Cholesky decomposition cost O(d3). To avoid this cost  we need a way to compute Sk that does not
require constructing the matrix Hk explicitly. Fortunately there is a variant of the BFGS formula
that maintains Sk directly [2]  which is

k+1; Sk+1 = (cid:0)I − pkqT
k+1; Ck+1 = (cid:0)I − uktT

k

k

(cid:1) Sk
(cid:1) Ck

Hk+1 = Sk+1ST
Bk+1 = Ck+1CT

i=k−m−1(I− piqT

k denotes the Hessian matrix approximation. Again  we will use a limited-memory

computed by a sequence of inner products Sk+1z =(cid:81)k

where Bk = H−1
version of these updates  in which the recursion is stopped at Hk−m = γI.
As for the running time of the above approximation  computing Sk requires O(m2d) time and
O(md) space  so it is still linear in the dimensionality. The matrix vector product Sk+1z can be
i )Sk−mz  in time O(md).
A second issue is that we need Hk to be positive deﬁnite if it is to be used as a covariance matrix. It
can be shown [10] that Hk is positive deﬁnite if for all i ∈ (k − m + 1  k)  we have sT
i yi > 0. For a
convex function f  an optimizer can be arranged so that this condition always holds  but we cannot
do this in a sampler. Instead  we ﬁrst sort the previous samples {xi} in ascending order with respect
to L(x)  and then check if there are any adjacent pairs (xi  xi+1) such that the resulting si and yi
i yi ≤ 0. If this happens  we remove the point xi+1 from the memory and recompute si  yi
have sT
using xi+2  and so on. In this way we can ensure that Hk is always positive deﬁnite.
Although we have described BFGS as relying on a memory of ”previous” points  e.g.  previous
iterates of an optimization algorithm  or previous samples of an MCMC chain  in principle the
BFGS equations could be used to generate a Hessian approximation from any set of points X =
{x1 . . . xm}. To emphasize this  we will write HBFGS : X (cid:55)→ Hk for the function that maps a
“pseudo-memory” X to the inverse Hessian Hk. This function ﬁrst sorts x ∈ X by L(xi)  then
computes si = xi+1 − xi and yi = ∇L(xi+1) − ∇L(xi)  then ﬁlters xi as described above so that
i yi > 0∀i  and ﬁnally computes the Hessian approximation Hk using the recursion (8)–(11).
sT

3

(cid:115)
(cid:115)

pk =

sk
sT
k yk

sk

; qk =

; uk =

tk =

sT
k Bksk

sT
k yk
sT
k Bkyk

Bksk − yk

sT
k Bksk
sT
k yk

yk + Bksk

(8)
(9)

(10)

(11)

4 Quasi-Newton Markov Chain Monte Carlo

In this section  we describe two new quasi-Newton samplers. They will both follow the same struc-
ture  which we describe now. Intuitively  we want to use the characteristics of the target distribution
to accelerate the exploration of the region with high probability mass. The previous samples pro-
vide information about the target distribution  so it is reasonable to use them to adapt the kernel.
However  naively tuning the sampling parameters using all previous samples may lead to an invalid
chain  that is  a chain that does not have π as its invariant distribution.
Our samplers will use a simple solution to this problem. Rather than adapting the kernel using all
of the previous samples in the Markov chain  we will adapt using a limited window of K previous
samples. The chain as a whole will then be an order K Markov chain.
It is easiest to analyze
this chain by converting it into a ﬁrst-order Markov chain over an enlarged space. Speciﬁcally  we
build a Markov chain in a K-fold product space X K with the stationary distribution p(x1:K) =
i=1:K π(xi). We denote a state of this chain by xt−K+1  xt−K+2  . . .   xt. We use the short-hand

(cid:81)

1:K per iteration  in a Gibbs-like fashion. We

notation x(t)

1:K\i for the subset of x(t)

1:K excluding the x(t)
Our samplers will then update one component of x(t)
deﬁne a transition kernel Ti that only updates the ith component of x(t)
1:K  that is:
i|x(t)

1:K\i)B(xi  x(cid:48)

1:K  x(cid:48)

1:K\i  x(cid:48)

1:K\i) 

Ti(x(t)

1:K) = δ(x(t)

(12)
i|x1:K\i) is called the base kernel that is a MCMC kernel in X and adapts with
where B(xi  x(cid:48)
x(t)
1:K\i. If B leaves π(xi) invariant for all ﬁxed values of x1:K\i  it is straightforward to show that
Ti leaves p invariant. Then  the sampler as a whole updates each of the components x(t)
in sequence 
i
so that the method as a whole is described by the kernel

.

i

T (x1:K  x(cid:48)

1:K) = T1 ◦ T2 . . . ◦ TK(x1:K  x(cid:48)

(13)
where Ti ◦ Tj denotes composition of kernels Ti and Tj. Because the each kernel Ti leaves p(x1:K)
invariant  the composition kernel T also leaves p(x1:K) invariant. Such an adaptive scheme is
equivalent to using an ensemble of K chains and changing the kernel of each chain with the state
of the others. It is called the ensemble-chain adaptation (ECA) in this paper. One early example of
ECA is found in [4]. To simplify the analysis of the validity of the chain  we assume the base kernel
B is irreducible in one iteration. This assumption can be satisﬁed by many popular MCMC kernels.

1:K) 

4.1 Using BFGS within Metropolis-Hastings

1:K) = N (x(cid:48); µ  Σ)  where the proposal mean µ = µ(x(t)

A simple way to incorporate quasi-Newton approximations within MCMC is to use the Metropolis-
Hastings (M-H) algorithm. The intuition is to ﬁt the Gaussian proposal distribution to the target
distribution  so that points in a high probability region are more likely to be proposed. We will
call this algorithm MHBFGS. Speciﬁcally  the proposal distribution of MHBFGS is deﬁned as
q(x(cid:48)|x(t)
1:K)
depend on the state of all K chains.
Several choices for the mean function are possible. One simple choice is to use one of the samples
in the window as the mean  e.g.  µ(x(t)
1 . Another potentially better choice is a Newton
step from xt. For the covariance function at µ  we will use the BFGS approximation Σ(x1:K) =
HBFGS(x1:K). The proposal x(cid:48) of T1 is accepted with probability

1:K) and covariance Σ = Σ(x(t)

1:K) = x(t)

(cid:33)

.

(14)

(cid:32)

α(x(t)

1   x(cid:48)) = min

1 

q(x(t)

1 |x(t)
2:K  x(cid:48))
π(x(t)
1 )

π(x(cid:48))
q(x(cid:48)|x(t)
1   x(t)

2:K)

is duplicated as the new sample.

is rejected  x(t)
1
1   x(t)

If x(cid:48)
Because the Gaussian proposal
q(A|x(t)
2:K) has positive probability for all A ∈ X   the M-H kernel is irreducible within one
iteration. Because the M-H algorithm with acceptance ratio deﬁned as (14) leaves π(x) invariant 
MHBFGS is a valid method that leaves p(x1:K) invariant. Although MHBFGS is simple and intu-
itive  in preliminary experiments we have found that MHBFGS sampler may converge slowly in high

4

1   x(t)

2   . . .   x(t)
K )
  x(t+1)

2

  . . .   x(t+1)
K )

1   p) using (3)-(5)

1   p|x2:K) − H(x∗  p∗|x2:K)) then

Algorithm 1
HMCBFGS
Input: Current memory (x(t)
Output: Next memory (x(t+1)
1
1: p ∼ N (0  BBFGS(x(t)
2:K))
2: (x∗  p∗) ←Leapfrog(x(t)
3: u ∼ Unif[0  1]
4: if u ≤ exp(H(x(t)
K ← x∗
x(t+1)
5:
6: else
K ← x(t)
x(t+1)
7:
8: end if
1:K−1 ← x(t)
9: x(t+1)
2:K
10: return (x(t+1)

  x(t+1)

K

1

2

  . . .   x(t+1)
K )

dimensions. In general Metropolis Hastings with a Gaussian proposal can suffer from random walk
behavior  even if the true Hessian is used. For this reason  next we incorporate the BFGS into a more
sophisticated sampling algorithm.

4.2 Using BFGS within Hamiltonian Monte Carlo

Better convergence speed can be achieved by incorporating BFGS within the HMC kernel. The
high-level idea is to start with the MHBFGS algorithm  but to replace the Gaussian proposal with a
simulation of Hamiltonian dynamics. However  we will need to be a bit careful in order to ensure that
the Hamiltonian is separable  because otherwise we would need to employ a generalized leapfrog
integrator [5] which is signiﬁcantly more expensive.
The new samples in HMCBFGS are generated as follows. As before we update one component of
x(t)
1:K at a time. Say that we are updating component i. First we sample a new value of the momentum
variable p ∼ N (0  BBFGS(x(t)
1:K\i)). It is important that when constructing the BFGS approximation 
we not use the value x(t)
that we are currently resampling. Then we simulate the Hamiltonian
i
dynamics starting at the point (x(t)
  p) using the leapfrog method (3)–(5). The Hamiltonian energy
i
used for this dynamics is simply

(15)

Hi(x(t)

1:K  p) = − L(x(t)

1
i ) +
2
Finally 

pT HBFGS(x(t)
the proposal

1:K\i)−1p 

(cid:90)

This yields a proposed value (x∗  p∗).
min[1  exp(H(xi  p) − H(x∗
procedure is summarized in Algorithm 1.
This procedure is an instance of the general ECA scheme described above  with base kernel

is accepted with probability
i   p∗)]  for H in (15) and p∗ is discarded after M-H correction. This

i  p(cid:48)

i  p(cid:48)

i|x1:K\i) =

B(xi  x(cid:48)
i|x1:K\i) is a standard HMC kernel with mass matrix BBFGS(x1:K\i) that in-
where ˆB(xi  pi  x(cid:48)
cludes sampling pi. The Hamiltonian energy function of ˆB given by (15) is separable  that means
xi only appear in potential energy. It is easy to see that B is a valid kernel in X   so as a ECA method 

i|x1:K\i)dpidp(cid:48)
i.

HMCBFGS leaves p(x1:K) =(cid:81)

ˆB(xi  pi  x(cid:48)

i π(xi) invariant.

It is interesting to consider if the method is valid in the augmented space X K × P K  i.e.  whether
Algorithm 1 leaves the distribution

K(cid:89)

p(x1:K  p1:K) =

π(xi)N (pi; 0  BBFGS(x(t)

1:K\i))

Interestingly  this is not true  because every update to xi changes the Gaussian factors for the mo-
mentum variables pj for j (cid:54)= i in a way that the Metropolis Hastings correction in lines 4–8 does
not consider. So despite the auxiliary variables  it is easiest to establish validity in the original space.

i=1

5

HMCBFGS has the advantages of being a simple approach that only uses gradient and the compu-
tational efﬁciency that the cost of all matrix operations (namely in lines 1 and 2 of Algorithm 1)
is at the scale of O(Kd). But  being an ECA method  HMCBFGS has the disadvantage that the
larger the number of chains K  the updates are “spread across” the chains  so that each chain gets a
small number of updates during a ﬁxed amount of computation time. In Section 6 we will evaluate
empirically whether this potential drawback is outweighed by the advantages of using approximate
second-order information.

5 Related Work

Girolami and Calderhead [5] propose a new HMC method called Riemannian manifold Hamiltonian
Monte Carlo (RMHMC) where M(x) can be any positive deﬁnite matrix. In their work  M(x) is
chosen to be the expected Fisher information matrix and the experimental results show that RMHMC
can converge much faster than many other MCMC methods. Girolami and Calderhead adopted
a generalised leapfrog method that is a reversible and volume-preserving approximation to non-
separable Hamiltonian. However  such a method may require computing third-order derivatives of
L  which can be infeasible in many applications.
Barthelme and Chopin [1] pointed out the possibility to use approximate BFGS Hessian in RMHMC
for computational efﬁciency. Similarly  Roy [14] suggested iteratively updating the local metric
approximation. Roy also emphasized the potential effect of such an iterative approximation to the
validity  a main problem that we address here. An early example of ECA is adaptive direction
sampling (ADS) [4]  in which each sample is taken along a random direction that is chosen based
on the samples from a set of chains. However  the validity of ADS can be established only when the
size of ensemble is greater than the number of dimensions  otherwise the samples are trapped in a
subspace. HMCBFGS avoids this problem because the BFGS Hessian approximation is full rank.
There has been a large amount of interest in adaptive MCMC methods that accumulate informa-
tion from all previous samples. These methods must be designed carefully because if the kernel is
adapted with the full sampling history in a naive way  the sampler can be invalid [13]. A well known
example of a correct adaptive algorithm is the Adaptive Metropolis [6] algorithm  which adapts the
Gaussian proposal of a Metropolis Hastings algorithm based on the empirical covariance of previ-
ous samples in a way that maintains ergodicity. Being a valid method  the adaptation of kernel must
keep decreasing over time. In practice  the parameters of the kernel in many diminishing adaptive
methods converge to a single value over the entire state space. This could be problematic if we want
the sampler to adapt to local characteristics of the target distribution  e.g.  if different regions of the
target distribution have different curvature. Using a ﬁnite memory of recent samples  our method
avoids such a problem.

6 Experiments

We test HMCBFGS on two different models  Bayesian logistic regression and Bayesian conditional
random ﬁelds (BCRFs). We compare HMCBFGS to the standard HMC which uses identity mass
matrix and RMHMC which requires computing the Hessian matrix. All methods are implemented in
Java 1. We do not report results from MHBFGS because preliminary experiments showed that it was
much worse than either HMC or HMCBFGS. The datasets for Bayesian logistic regression is used
for RMHMC in [5]. For HMC and HMCBFGS we employ the random step size  ∼ Unif[0.9ˆ  ˆ] 
where ˆ is the maximum step size. For RMHMC  we used the ﬁxed  = 0.5 for all datasets that
follows the setting in [5].
For HMC and HMCBFGS we tuned L on one data set (the German data set) and used that value
on all datasets. We chose the smallest number of leaps that did not degrade the performance of the
sampler. L was chosen to be 40 for HMC and to be 20 for HMCBFGS. For RMHMC  we employed
L = 6 leaps in RMHMC  following Girolami and Calderhead [5]. For HMCBFGS  we heuristically
chose the number of ensemble chains K to be slightly higher than d/2.

1Our implementation was based on the Matlab code of RMHMC of Girolami and Calderhead and checked

against the original Matlab version

6

For each method  we drew 5000 samples after 1000 burn-in samples. The convergence speed is
measured by effective sample size (ESS) [5]  which summaries the amount of autocorrelation across
different lags over all dimensions2. The more detailed description of ESS can be found in [5]. Be-
cause HMCBFGS displays more correlation within individual chain than across chains  we calculate
the ESS separately for individual chains in the ensemble and the overall ESS is simply the sum of
that from individual chains. All the ﬁnal ESS on each data set is obtained by averaging over 10 runs
using different initialisations.

ESS

HMC HMCBFGS RMHMC

Min
Mean
Max
Time (s)
ES/s

3312
3862
4445
7.56
739

3643
4541
4993
4.74
1470

4819
4950
5000
483.00
107

Table 1: Performance of MCMC samplers on Bayesian logistic regression  as measured by Effective
Sample Size (ESS). Higher is better. Averaged over ﬁve datasets. ES/s is the number of effective
samples per second

Dataset
Australian
German
Heart
Pima
Ripley

D
15
25
14
8
7

N HMC HMCBFGS RMHMC
18
3
54
118
344

818
397
2009
1383
2745

690
1000
532
270
250

396
255
1054
591
1396

Table 2: Effective samples per second on Bayesian logistic regression. D is the number of regression
coefﬁcients and N is the size of training data set

The results on ESS averaged over ﬁve datasets on Bayesian logistic regression are given by Table 1.
Our ESS number of HMC and RMHMC basically replicates the results in [5]. RMHMC achieves
the highest minimum and mean and maximum ESS and that are all very close to the total number of
samples 5000. However  because HMC and our method only require computing the gradient  they
outperforms RMHMC in terms of mean ESS per second. HMCBFGS gains a 10%  17% and 12%
increase in minimum  mean and maximum ESS than HMC  but only needs half number of leaps for
HMC. A detailed performance of methods over datasets is shown in Table 2.
The second model that we use is a Bayesian CRF on a small natural language dataset of FAQs from
Usenet [8]. A linear-chain CRF is used with Gaussian prior on the parameters. The model has 120
parameters. This model has been used previously [12  15]. In a CRF it is intractable to compute
the Hessian matrix exactly  so RMHMC is infeasible. For HMCBFGS we use K = 5 ensemble
chains. Each method is also tested 10 times with different initial points. For each chain we draw
8000 samples with 1000 burn-in. We use the step size  = 0.02 and the number of leaps L = 10
for both HMC and HMCBFGS. This parameter setting gives 84% acceptance rate on both HMC and
HMCBFGS (averaged over the 10 runs).
Figure 1 shows the sample trajectory plots for HMC and HMCBFGS on seven randomly selected
dimensions. It is clear that HMCBFGS demonstrates remarkably less autocorrelation than HMC.
The statistics of ESS in Table 3 gives a quantitative evaluation of the performance of HMC and
HMCBFGS. The results suggest that BFGS approximation dramatically reduces the sample autocor-
relation with a small increase of computational overhead on this dataset.
Finally  we evaluate the scalability of the methods on the highly correlated 1000 dimensional Gaus-
sian N (0  11T + 4). Using an ensemble of K = 5 chains  the samples from HMCBFGS are less
correlated than HMC along the largest eigenvalue direction (Figure 2).

2We use the code from [5] to compute ESS of samples

7

ESS

HMC HMCBFGS

Min
Mean
Max
Time (s)
ES/h

3
9
25
35743
1

26
438
5371
37387
42

Table 3: Performance of MCMC samplers on Bayesian CRFs  as measured by Effective Sample
Size (ESS). Higher is better. ES/h is the number of effective samples per hour

Figure 1: Sample trace plot of 7000 samples from the posterior of a Bayesian CRF using HMC
(left) and our method HMCBFGS (rigt) from a single run of each sampler (each line represents a
dimension)

Figure 2: ACF plot of samples projected on to the direction of largest eigenvector of 1000 dimen-
sional Gaussian using HMC(left) and HMCBFGS(right)

7 Discussion

To the best of our knowledge  this paper presents the ﬁrst adaptive MCMC methods to employ quasi-
Newton approximations. Naive attempts at combining these ideas (such as MHBFGS) do not work
well. On the other hand  HMCBFGS is more effective than the state-of-the-art sampler on several
real world data sets. Furthermore  HMCBFGS works well on a high dimensional model  where full
second-order methods are infeasible  with little extra overhead over regular HMC.
As far as future work  our current method may not work well in regions where the density is not
convex  because the true Hessian is not positive deﬁnite. Another potential issue  the asymptotic
independence between the chains in ECA methods may lead to poor Hessian approximations. On
a brighter note  our work raises the interesting possibility that quasi-Newton methods  which are
almost exclusively used within the optimization literature  may be useful more generally.

Acknowledgments

We thank Iain Murray for many useful discussions  and Mark Girolami for detailed comments on an
earlier draft.

8

01000200030004000500060007000−30−25−20−15−10−50510152001000200030004000500060007000−15−10−5051015020406080100120140160180200−0.500.51LagSample AutocorrelationSample Autocorrelation Function020406080100120140160180200−0.200.20.40.60.8LagSample AutocorrelationSample Autocorrelation FunctionReferences
[1] S. Barthelme and N. Chopin. Discussion on Riemannian Manifold Hamiltonian Monte Carlo.
Journal of the Royal Statistical Society  B (Statistical Methodology)  73:163–164  2011. doi:
10.1111/j.1467-9868.2010.00765.x.

[2] K. Brodlie  A. Gourlay  and J. Greenstadt. Rank-one and rank-two corrections to positive
IMA Journal of Applied Mathematics  11(1):

deﬁnite matrices expressed in product form.
73–82  1973.

[3] S. Chib  E. Greenberg  and R. Winkelmann. Posterior simulation and bayes factors in
panel count data models. Journal of Econometrics  86(1):33–54  June 1998. URL http:
//ideas.repec.org/a/eee/econom/v86y1998i1p33-54.html.

[4] W. R. Gilks  G. O. Roberts  and E. I. George. Adaptive direction sampling. The Statistician 

43(1):179–9  1994.

[5] M. Girolami and B. Calderhead. Riemannian manifold Hamiltonian Monte Carlo (with discus-
sion). Journal of the Royal Statistical Society  B (Statistical Methodology)  73:123–214  2011.
doi: 10.1111/j.1467-9868.2010.00765.x.

[6] H. Haario  E. Saksman  and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli  7(2):

223–242  2001.

[7] J. S. Liu  F. Liang  and W. H. Wong. The multiple-try method and local optimization in
Metropolis sampling. Journal of the American Statistical Association  95(449):pp. 121–134 
2000.

[8] A. McCallum. Frequently asked questions data set. http://www.cs.umass.edu/

˜mccallum/data/faqdata.

[9] R. M. Neal. MCMC using Hamiltonian dynamics. In S. Brooks  A. Gelman  G. Jones  and
X.-L. Meng  editors  Handbook of Markov Chain Monte Carlo. Chapman & Hall / CRC Press 
2010.

[10] J. Nocedal and S. J. Wright. Numerical Optimization. Springer-Verlag  New York  1999. ISBN

0-387-98793-2.

[11] Y. Qi and T. P. Minka. Hessian-based Markov chain Monte Carlo algorithms. In First Cape

Cod Workshop on Monte Carlo Methods  September 2002.

[12] Y. Qi  M. Szummer  and T. P. Minka. Bayesian conditional random ﬁelds. In Artiﬁcial Intelli-

gence and Statistics (AISTATS). Barbados  January 2005.

[13] G. O. Roberts and J. S. Rosenthal. Coupling and ergodicity of adaptive mcmc. Journal of

Applied Probability  44(2):458–475  2007.

[14] D. M. Roy. Discussion on Riemannian Manifold Hamiltonian Monte Carlo. Journal of the
Royal Statistical Society  B (Statistical Methodology)  73:194–195  2011. doi: 10.1111/j.
1467-9868.2010.00765.x.

[15] M. Welling and S. Parise. Bayesian random ﬁelds: The Bethe-Laplace approximation.

Uncertainty in Artiﬁcial Intelligence (UAI)  2006.

In

9

,Kush Bhatia
Prateek Jain
Parameswaran Kamalaruban
Purushottam Kar