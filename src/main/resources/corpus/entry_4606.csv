2013,Recurrent linear models of simultaneously-recorded neural   populations,Population neural recordings with long-range temporal structure are often best understood in terms of a shared underlying low-dimensional dynamical process. Advances in recording technology provide access to an ever larger fraction of the population  but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset. Here we describe a new  scalable approach to discovering the low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population. Our method is based on recurrent linear models (RLMs)  and relates closely to timeseries models based on recurrent neural networks. We formulate RLMs for neural data by generalising the Kalman-filter-based likelihood calculation for latent linear dynamical systems (LDS) models to incorporate a generalised-linear observation process. We show that RLMs describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations. We also introduce the cascaded linear model (CLM) to capture low-dimensional instantaneous correlations in neural populations. The CLM describes the cortical recordings better than either Ising or Gaussian models and  like the RLM  can be fit exactly and quickly. The CLM can also be seen as a generalization of a low-rank Gaussian model  in this case factor analysis. The computational tractability of the RLM and CLM allow both to scale to very high-dimensional neural data.,Recurrent linear models of

simultaneously-recorded neural populations

Marius Pachitariu  Biljana Petreska  Maneesh Sahani

Gatsby Computational Neuroscience Unit

{marius biljana maneesh}@gatsby.ucl.ac.uk

University College London  UK

Abstract

Population neural recordings with long-range temporal structure are often best un-
derstood in terms of a common underlying low-dimensional dynamical process.
Advances in recording technology provide access to an ever-larger fraction of the
population  but the standard computational approaches available to identify the
collective dynamics scale poorly with the size of the dataset. We describe a new 
scalable approach to discovering low-dimensional dynamics that underlie simul-
taneously recorded spike trains from a neural population. We formulate the Re-
current Linear Model (RLM) by generalising the Kalman-ﬁlter-based likelihood
calculation for latent linear dynamical systems to incorporate a generalised-linear
observation process. We show that RLMs describe motor-cortical population data
better than either directly-coupled generalised-linear models or latent linear dy-
namical system models with generalised-linear observations. We also introduce
the cascaded generalised-linear model (CGLM) to capture low-dimensional in-
stantaneous correlations in neural populations. The CGLM describes the cortical
recordings better than either Ising or Gaussian models and  like the RLM  can be
ﬁt exactly and quickly. The CGLM can also be seen as a generalisation of a low-
rank Gaussian model  in this case factor analysis. The computational tractability
of the RLM and CGLM allow both to scale to very high-dimensional neural data.

1

Introduction

Many essential neural computations are implemented by large populations of neurons working in
concert  and recent studies have sought both to monitor increasingly large groups of neurons [1  2]
and to characterise their collective behaviour [3  4]. In this paper we introduce a new computational
tool to model coordinated behaviour in very large neural data sets. While we explicitly discuss only
multi-electrode extracellular recordings  the same model can be readily used to characterise 2-photon
calcium-marker image data  EEG  fMRI or even large-scale biologically-faithful simulations.
Populational neural data may be represented at each time point by a vector yt with as many dimen-
sions as neurons  and as many indices t as time points in the experiment. For spiking neurons  yt
will have positive integer elements corresponding to the number of spikes ﬁred by each neuron in
the time interval corresponding to the t-th bin. As others have before [5  6]  we assume that the
coordinated activity reﬂected in the measurement yt arises from a low-dimensional set of processes 
collected into a vector xt  which is not directly observed. However  unlike the previous studies 
we construct a recurrent model in which the hidden processes xt are driven directly and explicitly
by the measured neural signals y1 . . . yt−1. This assumption simpliﬁes the estimation process. We
assume for simplicity that xt evolves with linear dynamics and affects the future state of the neural
signal yt in a generalised-linear manner  although both assumptions may be relaxed. As in the latent
dynamical system  the resulting model enforces a “bottleneck”  whereby predictions of yt based on
y1 . . . yt−1 must be carried by the low-dimensional xt.

1

State prediction in the RLM is related to the Kalman ﬁlter [7] and we show in the next section a
formal equivalence between the likelihoods of the RLM and the latent dynamical model when ob-
servation noise is Gaussian distributed. However  spiking data is not well modelled as Gaussian 
and the generalisation of our approach to Poisson noise leads to a departure from the latent dynam-
ical approach. Unlike latent linear models with conditionally Poisson observations  the parameters
of our model can be estimated efﬁciently and without approximation. We show that  perhaps in
consequence  the RLM can provide superior descriptions of neural population data.

2 From the Kalman ﬁlter to the recurrent linear model (RLM)

Consider a latent linear dynamical system (LDS) model with linear-Gaussian observations.
Its
graphical model is shown in Fig. 1A. The latent process is parametrised by a dynamics matrix
A and innovations covariance Q that describe the evolution of the latent state xt:

P (xt|xt−1) = N (xt|Axt−1  Q)  

where N (x|µ  Σ) represents a normal distribution on x with mean µ and (co)variance Σ. For brevity 
we omit here and below the special case of the ﬁrst time-step  in which x1 is drawn from a multivari-
ate Gaussian. The output distribution is determined by an observation loading matrix C and a noise
covariance R often taken to be diagonal so that all covariance is modelled by the latent process:

P (yt|xt) = N (yt|Cxt  R) .

In the LDS  the joint likelihood of the observations {yt} can be written as the product:

P (y1 . . . yT ) = P (y1)

P (yt|y1 . . . yt−1)

T(cid:89)

t=2

(cid:90)
(cid:90)

and in the Gaussian case can be computed using the usual Kalman ﬁlter approach to ﬁnd the condi-
tional distributon at time t iteratively:

P (yt+1|y1 . . . yt) =

dxt+1 P (yt+1|xt+1)P (xt+1|y1 . . . yt)
dxt+1 N (yt+1|Cxt+1  R) N (xt+1|Aˆxt  Vt+1)

=
= N (yt+1|CAˆxt  CVt+1C(cid:62) + R)  

certainty Vt+1 = E(cid:2)(xt+1 − Aˆxt)2|y1 . . . yt

where we have introduced the (ﬁltered) state estimate ˆxt = E [xt|y1 . . . yt] and (predictive) un-
Kalman gain Kt = VtC(cid:62)(CVtC(cid:62) + R)−1  giving the following recursive recipe to calculate the
conditional likelihood of yt+1:

(cid:3). Both quantities are computed recursively using the

ˆxt = Aˆxt−1 + Kt(yt − ˆyt)
Vt+1 = A(I − KtC)VtA(cid:62) + Q
ˆyt+1 = CAˆxt

P (yt+1|y1 . . . yt) = N (yt+1| ˆyt+1  CVt+1C(cid:62) + R)

For the Gaussian LDS  the Kalman gain Kt and state uncertainty Vt+1 (and thus the output covari-
ance CVt+1C(cid:62) + R) depend on the model parameters (A  C  R  Q) and on the time step—although
as time grows they both converge to stationary values. Neither depends on the observations.
Thus  we might consider a relaxation of the Gaussian LDS model in which these matrices are taken
to be stationary from the outset  and are parametrised independently so that they are no longer
constrained to take on the “correct” values as computed for Kalman inference. Let us call this
parametric form of the Kalman gain W and the parametric form of the output covariance S. Then
the conditional likelihood iteration becomes

ˆxt = Aˆxt−1 + W (yt − ˆyt)

ˆyt+1 = CAˆxt

P (yt+1|y1 . . . yt) = N (yt+1| ˆyt+1  S) .

2

A x1

A

C

y1

B x0

η1

y1

C x0

A

A

η2

A

x2

C

y2

x1

y2

x1

x3

C

y3

x2

y3

x2

CA

W

CA

W

CA

A

• • •

A

• • •

η3

ηT -1

 LDS

xT

C

yT

xT -1

yT

A

W

• • •

A

xT -1

W

CA

RLM

y1

y2

y3

yT

Figure 1: Graphical representa-
tions of the latent linear dynamical
system (LDS: A  B) and recurrent
linear model (RLM: C). Shaded
variables are observed  unshaded
circles are latent random variables
and squares are variables that de-
pend deterministically on their par-
ents. In B the LDS is redrawn in
terms of the random innovations
ηt = xt − Axt−1  facilitating the
transition towards the RLM. The
RLM is then obtained by replacing
ηt with a deterministically derived
estimate W (yt − ˆyt).

The parameters of this new model are A  C  W and S. This is a relaxation of the Gaussian latent
LDS model because W has more degrees of freedom than Q  as does S than R (at least if R is
constrained to be diagonal). The new model has a recurrent linear structure in that the random
observation yt is fed back linearly to perturb the otherwise deterministic evolution of the state ˆxt.
A graphical representation of this model is shown in Fig. 1C  along with a redrawn graph of the LDS
model. The RLM can be viewed as replacing the random innovation variables ηt = xt − Axt−1
with data-derived estimates W (yt − ˆyt); estimates which are made possible by the fact that ηt
contributes to the variability of yt around ˆyt.

3 Recurrent linear models with Poisson observations

The discussion above has transformed a stochastic-latent LDS model with Gaussian output to an
RLM with deterministic latent  but still with Gaussian output. Our goal  however  is to ﬁt a model
with an output distribution better suited to the binned point-processes that characterise neural spik-
ing. Both linear Kalman-ﬁltering steps above and the eventual stationarity of the inference param-
eters depend on the joint Gaussian structure of the assumed LDS model. They would not apply
if we were to begin a similar derivation from an LDS with Poisson output. However  a tractable
approach to modelling point-process data with low-dimensional temporal structure may be provided
by introducing a generalised-linear output stage directly to the RLM. This model is given by:

ˆxt = Aˆxt−1 + W (yt − ˆyt)

g( ˆyt+1) = CAˆxt

P (yt+1|y1 . . . yt) = ExpFam(yt+1| ˆyt+1)

(1)

where ExpFam is an exponential-family distribution such as Poisson  and the element-wise link
function g allows for a nonlinear mapping from xt to the predicted mean ˆyt+1. In the following  we
will write f for the inverse-link as is more common for neural models  so that ˆyt+1 = f(CAˆxt).
The simplest Poisson-based generalised-linear RLM might take as its output distribution

P (yt| ˆyt) =(cid:89)

Poisson(yti|ˆyti);

ˆyt = f(CAˆxt−1))  

i

where yti is the spike count of the ith cell in bin t and the function f is non-negative. However 
comparison with the output distribution derived for the Gaussian RLM suggests that this choice
would fail to capture the instantaneous covariance that the LDS formulation transfers to the output
distribution (and which appears in the low-rank structure of S above). We can address this concern
in two ways. One option is to bin the data more ﬁnely  thus diminishing the inﬂuence of the instan-
taneous covariance. The alternative is to replace the independent Poissons with a correlated output
distribution on spike counts. The cascaded generalised-linear model introduced below is a natural
choice  and we will show that it captures instantaneous correlations faithfully with very few hidden
dimensions.

3

In practice  we also sometimes add a ﬁxed input µt to equation 1 that varies in time and determines
the average behavior of the population or the peri-stimulus time histogram (PSTH).

ˆyt+1 = f (µt + CAxt)

Note that the matrices A and C retain their interpretation from the LDS models. The matrix A
controls the evolution of the dynamical process xt. The phenomenology of its dynamics is deter-
mined by the complex eigenvalues of A. Eigenvalues with moduli close to 1 correspond to long
timescales of ﬂuctuation around the PSTH. Eigenvalues with non-zero imaginary part correspond
to oscillatory components. Finally  the dynamics will be stable iff all the eigenvalues lie within the
unit disc. The matrix C describes the dependence of the high-dimensional neural signals on the low-
dimensional latent processes xt. In particular  equation 2 determines the ﬁring rate of the neurons.
This generalised-linear stage ensures that the ﬁring rates are positive through the link function f  and
the observation process is Poisson. For other types of data  the generalised-linear stage might be
replaced by other appropriate link functions and output distributions.

3.1 Relationship to other models

RLMs are related to recurrent neural networks [8]. The differences lie in the state evolution  which
in the neural network is nonlinear: xt = h (Axt−1 + W yt−1); and in the recurrent term which
depends on the observation rather than the prediction error. On the data considered here  we found
that using sigmoidal or threshold-linear functions h resulted in models comparable in likelihood
to the RLM  and so we restricted our attention to simple linear dynamics. We also found that
using the prediction error term W (yt−1 − ˆyt) resulted in better models than the simple neural-net
formulation  and we attribute this difference to the link between the RLM and Kalman inference.
It is also possible to work within the stochatic latent LDS framework  replacing the Gaussian out-
put distribution with a generalised-linear Poisson output (e.g. [6]). The main difﬁculty here is the
intractability of the estimation procedure. For an unobserved latent process xt  an inference pro-
cedure needs to be devised to estimate the posterior distribution on the entire sequence x1 . . . xt.
For linear-Gaussian observations  this inference is tractable and is provided by Kalman smoothing.
However  with generalised-linear observations  inference becomes intractable and the necessary ap-
proximations [6] are computationally intense and can jeopardize the quality of the ﬁtted models. By
contrast  in the RLM xt is a deterministic function of data. In effect  the Kalman ﬁlter has been built
into the model as the accurate estimation procedure  and efﬁcient ﬁtting is possible by direct gradient
ascent on the log-likelihood. Empirically we did not encounter difﬁculties with local minima during
optimization  as has been reported for LDS models ﬁt by approximate EM [9]. Multiple restarts
from different random values of the parameters always led to models with similar likelihoods.
Note that to estimate the matrices A and W the gradient must be backpropagated through succes-
sive iterations of equation 1. This technique  known as backpropagation-through-time  was ﬁrst
described by [10] as a technique to ﬁt recurrent neural network models. Recent implementations
have demonstrated state-of-the-art language models [11]. Backpropagation-through-time is thought
to be inherently unstable when propagated past many timesteps and often the gradient is truncated
prematurely [11]. We found that using large values of momentum in the gradient ascent alleviated
these instabilities and allowed us to use backpropagation without the truncation.

4 The cascaded generalised-linear model (CGLM)

The link between the RLM and the LDS raises the possibility that a model for simultaneously-
recorded correlated spike counts might be derived in a similar way  starting from a non-dynamical 
but low-dimensional  Gaussian model. Stationary models of population activity have attracted recent
interest for their own sake (e.g. [1])  and would also provide a way model correlations introduced
by common innovations that were neglected by the simple Poisson form of the RLM. Thus  we
consider vectors y of spike counts from N neurons  without explicit reference to the time at which
they were collected. A Gaussian model for y can certainly describe correlations between the cells 
but is ill-matched to discrete count observations. Thus  as with the derivation of the RLM from the
Kalman ﬁlter  we derive here a new generalisation of a low-dimensional  structured Gaussian model
to spike count data.

4

The distribution of any multivariate variable y can be factorized into a “cascaded” product of multi-
ple one-dimensional distributions:

P (y) =

P (yn|y<n) .

(2)

N(cid:89)

n=1

Here n indexes the neurons up to the last neuron N  and y<n is the (n–1)-vector [y1 . . . yn−1]. For
a Gaussian-distributed y  the conditionals P (yn|y<n) would be linear-Gaussian. Thus  we propose
the “cascaded generalised linear model” (CGLM) in which each such one-dimensional conditional
distribution is a generalised-linear model:

ˆyn = f(cid:0)µn + ST

n y<n

(cid:1)

P (yn|y<n) = ExpFam (ˆyn)

(3)
(4)

and in which the linear weights Sn take on a structured form developed below.
The equations 3 and 4 subsume the Gaussian distribution with arbitrary covariance in the case that
f is linear  and the ExpFam conditionals are Gaussian. In this case  for a joint covariance of Σ  it is
straightforward to derive the expression

1

(Σ≤n ≤n)−1

n n

Sn =

n <n .

(Σ≤n ≤n)−1

(5)
where the subscripts < n and ≤ n restrict the matrix to the ﬁrst (n − 1) and n rows and/or columns
respectively. Thus  we might construct suitably structured linear weights for the CGLM by applying
this result to the covariance matrix induced by the low-dimensional Gaussian model known as factor
analysis [12]. Factor analysis assumes that data are generated from a K-dimensional latent process
x ∼ N (0  I)  where I is the K×K identity matrix  and y has the conditional distribution P (y|x) =
N (Λx  Ψ) with Ψ a diagonal matrix and Λ an N × K loading matrix. This leads to a covariance
of y given by Σ = Ψ + ΛΛT . If we repeat the derivation of equations 3  4 and 5 for this covariance
matrix  we obtain an expression for Sn via the matrix inversion lemma:

Sn =

=

1

(Σ≤n ≤n)−1

n n

1

(Σ≤n ≤n)−1

n n

= −

1

(Σ≤n ≤n)−1

n n

(cid:0)Ψ≤n ≤n + Λ≤n ·ΛT≤n ·(cid:1)−1
(cid:16)
(cid:16)(cid:0)Ψ−1Λ(cid:1)
≤n · (···)(cid:0)ΛΨ−1(cid:1)T

Ψ−1≤n ≤n − Ψ−1≤n ≤nΛ<n · (···) ΛT
<n ·Ψ−1
(cid:17)

≤n ·

n <n

n <n

(cid:17)

n <n

(6)

<n <n

if we arrange all Sn as the upper columns of an N × N matrix S  we can write S = upper(cid:0)zwT(cid:1)

where the omitted factor (··· ) is a K × K matrix. The ﬁrst term in equation 6 vanishes because it
involves only the off-diagonal entries of Ψ. The surviving factor shows that Sn is formed by taking
a linear combination of the columns of Ψ−1Λ and then truncating to the ﬁrst n − 1 elements. Thus 
for some low-dimensional matrices z = Ψ−1Λ and w  where the operation upper extracts the
strictly upper triangular part of a matrix. This is the natural structure imposed on the cascaded
conditionals by factor analysis. Thus  we adopt the same constraint on S in the case of generalised-
linear observations. The resulting (CGLM) is shown below to provide better ﬁts to binarized neural
data than standard Ising models (see the Results section)  even with as few as three latent dimensions.
Another useful property of the CGLM is that it allows stimulus-dependent inputs in equation 3. The
CGLM can also be used in combination with the generalised-linear RLM  with the CGLM replacing
the otherwise independent observation model. This approach can be useful when large bins are used
to discretize spike trains. In both cases the model can be estimated quickly with standard gradient
ascent techniques.

5 Alternative models

5.1 Alternative for temporal interactions: causally-coupled generalised linear model

One popular and simple model of simultaneously recorded neuronal populations [3] constructs tem-
poral dependencies between units by directly coupling each neuron’s probability of ﬁring to the past

5

spikes in the entire population:

yt ∝ Poisson(f(µt +

N(cid:88)

i=1

Bi (hi (cid:63) yt)))

Here  hi (cid:63) yt are convolutions of the spike trains with a set of basis functions hi  and Bi are pairwise
interaction weights. Each matrix Bi has N 2 parameters where N is the number of neurons  so the
number of parameters grows quadratically with the population size. This type of scaling makes the
model prohibitive to use with very large-scale array recordings. Even with aggresive regularization
techniques  the model’s parameters are difﬁcult to identify with limited amounts of data. Perhaps
more importantly  the model does not have a physical interpretation. Neurons recorded in cortex
are rarely directly-connected and retinal ganglion cells almost never directly connect to each other.
Instead  such directly-coupled GLMs are used to describe so-called ’functional’ interactions between
neurons [3]. We believe a much better interpretation for the correlations observed between pairs of
neurons is that they are caused by common inputs to these neurons which seem often to be conﬁned
to a small number of dimensions. The models we propose here  the RLM and the CGLM  are aimed
at discovering such inputs.

5.2 Alternative for instantaneous interactions: the Ising model

Instantaneous interactions between binary data (as would be obtained by counting spikes in short
intervals) can be modelled in terms of their pairwise interactions [1] embodied in the Ising model:

P (y) =

1
Z

eyT Jy.

(7)

where J is a pairwise interaction matrix and Z is the partition function  or the normalization constant
of the model. The model’s attractiveness is that for a given covariance structure it makes the weakest
possible assumptions about the distribution of y  that is  like a Gaussian for continuous data  it
has the largest possible entropy under the covariance constraint. However  the Ising model and
the so-called functional interactions J have no physical interpretation when applied to neural data.
Furthermore  Ising models are difﬁcult to ﬁt as they require estimates of the gradients of the partition
function Z; they also suffer from the same quadratic scaling in number of paramters as does the
directly-coupled GLM. Ising models are even harder to estimate when stimulus-dependent inputs
are added in equation 7  but for data collected in the retina or other sensory areas [1]  much of the
covariation in y may be expected to arise from common stimulus input. Another short-coming of
the Ising model is that it can only model binarized data and cannot be normalized for integer y-s [6] 
so either the time bins need to be reduced to ensure no neuron ﬁres more than one spike in a single
bin or the spike counts must be capped at 1.

6 Results

6.1 Simulated data

We began by evaluating RLM models ﬁt to simulated data where the true generative parameters were
known. Two aspects of the estimated models were of particular interest: the phenomenology of the
dynamics (captured by the eigenvalues of the dynamics matrix A) and the relationship between the
dynamical subspace and measured neural activity (captured by the output matrix C). We evaluated
the agreement between the estimated and generative output matrices by measuring the principal
angles between the corresponding subspaces. These report  in succession  the smallest angle achiev-
able between a line in one subspace and a line in the second subspace  once all previous such vectors
of maximal agreement have been projected out. Exactly aligned n-dimensional subspaces have all
n principal angles equal to 0◦. Unrelated low-dimensional subspaces embedded in high dimensions
are close to orthogonal and so have principal angles near 90◦.
We ﬁrst veriﬁed the robustness of maximisation of the generalised-linear RLM likelihood by ﬁtting
models to simulated data generated by a known RLM. Fig. 2(a) shows eigenvalues from several sim-
ulated RLMs and the eigenvalues recovered by ﬁtting parameters to simulated data. The agreement
is generally good. In particular  the qualitative aspects of the dynamics reﬂected in the absolute val-
ues and imaginary parts of the eigenvalues are well characterised. Fig. 2(d) shows that the RLM ﬁts

6

(a)

(d)

(b)

(e)

(c)

(f)

Figure 2: Experiments on 100-dimensional simulated data generated from a 5-dimensional latent
process. Generating models were Poisson RLM (ad)  Poisson LDS with random parameters (cf) and
Poisson LDS model with parameters ﬁt to neural data (cf). The models ﬁt were PCA  LDS with
Gaussian (LDS/GLDS) or Poisson (PLDS) output  and RLM with Poisson output (RLM). In the
upper plots  eigenvalues from different runs are shown in different colors.

also recover the subspace deﬁned by the loading matrix C  and do so substantially more accurately
than either principal components analysis (PCA) or GLDS models. It is important to note that the
likelihoods of LDS models with Poisson observations are difﬁcult to optimise  and so may yield
poor results even when ﬁt to within-class data. In practice we did not observe local optima with the
RLM or CGLM.
We also asked whether the RLM could recover the dynamical properties and latent subspace of data
generated by a latent LDS model with Poisson observations. Fig. 2(b) shows that the dynamical
eigenvalues of the maximum-likelihood RLM are close to the eigenvalues of generative LDS dy-
namics  whilst Fig. 2(e) shows that the dynamical subspace is also correctly recovered. Parameters
for these simulations were chosen randomly. We then asked whether the quality of parameter identi-
ﬁcation extended to Poisson-output LDS models with realistic parameters  by generating data from
a Poisson-output LDS model that had been ﬁt to a neural recording. As seen in ﬁgs. 2(c) and 2(f) 
the RLM ﬁts remain accurate in this regime  yielding better subspace estimates than either PCA or
a Gaussian LDS.

6.2 Array recorded data

We next compared the performance of the novel models on neural data. The RLM was compared
to the directed-coupled GLM (ﬁt by gradient-based likelihood optimisation) as well as LDS models
with Gaussian or Poisson outputs (ﬁt by EM  with a Laplace approximation E-step). The CGLM
was compared to the Ising model. We used a dataset of 92 neurons recorded with a Utah array
implanted in the premotor and motor cortices of a rhesus macaque monkey performing a delayed
center-out reach task. For all comparisons below we use datasets of 108 trials in which the monkey
made movements to the same target.
We discretized spike trains into time bins of 10ms. The directed-coupled GLM needed substantial
regularization in order to make good predictions on held-out test data. Figure 3(a) shows only
the best cross-validation result for the GLM  but results without regularization for models with

7

00.20.40.60.81−0.4−0.200.20.4RealImaginaryRLM recovers eigenvaluesof simulated dynamics  Ground truthIdentified0.20.40.60.81−0.4−0.200.20.4RealImaginaryRLM identifies the eigenvaluesof diverse PLDS models  Generative PLDSIdentified by PLDSIdentified by RLM0.850.90.951−0.1−0.0500.050.1RealImaginaryRLM identifies the eigenvaluesof a PLDS model fit to real data  Generative PLDSIdentified by RLMPCAGLDSRLM04590Principal anglesbetween ground truth (RLM)and identified subspacesDegreesPCALDSRLMPLDS04590Principal anglesbetween true and identified subspacesDegreesPCAGLDSRLM04590Principal anglesbetween PLDS fit to dataand identified subspacesDegrees(a)

(b)

Figure 3: a. Predictive performance of various models on test data (higher is better). GLM-type
models are helped greatly by self-coupling ﬁlters (which the other models do not have). The best
model is an RLM with three latent dimensions and a low-rank model of the PSTH (see the supple-
mentary material for more information about this model). Adding self-coupling ﬁlters to this model
further increases its predictive performance by 5 (not shown). b. The likelihood per spike of Ising
models as well as CGLM models with small numbers of hidden dimensions. The CGLM saturates
at three dimensions and performs better than Ising models.
low-dimensional parametrisation. Performance was measured by the causal mean-squared-error in
prediction subtracted from the error of a low-rank smoothed PSTH model (based on a singular-
value decomposition of the matrix of all smoothed PSTHs). The number of dimensions (5) and the
standard deviation of the Gaussian smoothing ﬁlter (20 ms) were cross-validated to ﬁnd the best
possible PSTH performance. Thus  our evaluation is focuses on each model’s ability to predict
trial-to-trial co-variation in ﬁring around the mean.
A second measure of performance for the RLM was obtained by studying probabilistic samples
obtained from the ﬁtted model. Figure 4 in the supplemental material shows averaged noise cross-
correlograms obtained from a large set of samples. Note that the PSTHs have been subtracted from
each trial to reveal only the extra correlation structure that is not repeated amongst trials. Even with
few hidden dimensions  the model captures well the full temporal structure of the noise correlations.
In the case of the Ising model we binarized the data by replacing all spike counts larger than 1 with
1. The log-likelihood of the Ising model could only be estimated for small numbers of neurons  so
for comparison we took only the 30 most active neurons. The measure of performance reported in
ﬁgure 3(b) is the extra log-likelihood per spike obtained above that of a model that makes constant
predictions equal to the mean ﬁring rate of each neuron. The CGLM model with only three hidden
dimensions achieves the best generalisation performance  surpassing the Ising model. Similar results
for the performance of the CGLM can be seen on the full dataset of 92 neurons with non-binarized
data  indicating that three latent dimensions sufﬁce to describe the full space visited by the neuronal
population on a trial-by-trial basis.

7 Discussion

The generalised-linear RLM model  while sharing motivation with latent LDS model  can be ﬁt more
efﬁciently and without approximation to non-Gaussian data. We have shown improved performance
on both simulated data and on population recordings from the motor cortex of behaving monkeys.
The model is easily extended to other output distributions (such as Bernoulli or negative binomial) 
to mixed continuous and discrete data  to nonlinear outputs  and to nonlinear dynamics. For the
motor data considered here  the generalised-linear model performed as well as models with further
non-linearites. However  preliminary results on data from sensory cortical areas suggests that non-
linear models may be of greater value in other settings.

8 Acknowledgments

We thank Krishna Shenoy and members of his lab for generously providing access to data. Funding
from the Gatsby Charitable Foundation and DARPA REPAIR N66001-10-C-2010.

8

GLM − SCGLMPLDS10 LDS10LDS20RLM10RLM20             RLM3+PSTH5678910111213MSEbaseline − MSEbaseline = PSTH (low rank)Filtering prediction on test dataIsingrank=1r=2r=3r=4r=500.010.020.030.040.050.060.07Likelihoodperspike−baseline(bits)References
[1] E Schneidman  MJ Berry  R Segev  and W Bialek. Weak pairwise correlations imply strongly correlated

network states in a neural population. Nature  440:1007–1012  2005.

[2] Gyorgy Buzsaki. Large-scale recording of neuronal ensembles. NatNeurosci  7(5):446–51  2004.
[3] J. W. Pillow  J. Shlens  L. Paninski  A. Sher  A. M. Litke  E. J. Chichilnisky  and E. P. Simoncelli. Spatio-
temporal correlations and visual signalling in a complete neuronal population. Nature  454(7207):995–
999  2008.

[4] Mark M. Churchland  Byron M. Yu  Maneesh Sahani  and Krishna V. Shenoy. Techniques for extract-
ing single-trial activity patterns from large-scale neural recordings. CurrOpinNeurobiol  17(5):609–618 
2007.

[5] BM Yu  A Afshar  G Santhanam  SI Ryu  KV Shenoy  and M Sahani. Extracting dynamical structure
embedded in neural activity. Advances in Neural Information Processing Systems  18:1545–1552  2006.
[6] JH Macke  L Bsing  JP Cunningham  BM Yu  KV Shenoy  and M Sahani. Empirical models of spiking in

neural populations. Advances in Neural Information Processing Systems  24:1350–1358  2011.

[7] R.E. Kalman. A new approach to linear ﬁltering and prediction problems. Journal of Basic Engineering 

82(1):35–45  1960.

[8] JL Elman. Finding structure in time. Cognitive Science  14:179–211  1990.
[9] L Buesing  JH Macke  and M Sahani. Spectral learning of linear dynamics from generalised-linear obser-
vations with application to neural population data. Advances in Neural Information Processing Systems 
25  2012.

[10] DE Rumelhart  GE Hinton  and RJ Williams. Learning internal representations by error propagation. Mit

Press Computational Models Of Cognition And Perception Series  pages 318–462  1986.

[11] T Mikolov  A Deoras  S Kombrink  L Burget  and JH Cernocky. Empirical evaluation and combination
of advanced language modeling techniques. Conference of the International Speech Communication
Association  2011.

[12] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer  2006.

9

,Marius Pachitariu
Biljana Petreska
Maneesh Sahani
Atsutoshi Kumagai
Tomoharu Iwata
Yasuhiro Fujiwara