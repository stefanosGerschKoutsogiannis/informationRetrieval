2019,Multilabel reductions: what is my loss optimising?,Multilabel classification is a challenging problem arising in applications ranging from information retrieval to image tagging. A popular approach to this problem is to employ a reduction to a suitable series of binary or multiclass problems (e.g.  computing a softmax based cross-entropy over the relevant labels). While such methods have seen empirical success  less is understood about how well they approximate two fundamental performance measures: precision@$k$ and recall@$k$. In this paper  we study five commonly used reductions  including the one-versus-all reduction  a reduction to multiclass classification  and normalised versions of the same  wherein the contribution of each instance is normalised by the number of relevant labels. Our main result is a formal justification of each reduction: we explicate their underlying risks  and show they are each consistent with respect to either precision or recall. Further  we show that in general no reduction can be optimal for both measures. We empirically validate our results  demonstrating scenarios where normalised reductions yield recall gains over unnormalised counterparts.,Multilabelreductions:whatismylossoptimising?AdityaKrishnaMenon AnkitSinghRawat SashankJ.Reddi andSanjivKumarGoogleResearchNewYork NY10011{adityakmenon sashank ankitsrawat sanjivk}@google.comAbstractMultilabelclassiﬁcationisachallengingproblemarisinginapplicationsrangingfrominformationretrievaltoimagetagging.Apopularapproachtothisproblemistoemployareductiontoasuitableseriesofbinaryormulticlassproblems(e.g. computingasoftmaxbasedcross-entropyovertherelevantlabels).Whilesuchmethodshaveseenempiricalsuccess lessisunderstoodabouthowwelltheyap-proximatetwofundamentalperformancemeasures:precision@kandrecall@k.Inthispaper westudyﬁvecommonlyusedreductions includingtheone-versus-allreduction areductiontomulticlassclassiﬁcation andnormalisedversionsofthesame whereinthecontributionofeachinstanceisnormalisedbythenumberofrelevantlabels.Ourmainresultisaformaljustiﬁcationofeachreduction:weexpli-catetheirunderlyingrisks andshowtheyareeachconsistentwithrespecttoeitherprecisionorrecall.Further weshowthatingeneralnoreductioncanbeoptimalforbothmeasures.Weempiricallyvalidateourresults demonstratingscenarioswherenormalisedreductionsyieldrecallgainsoverunnormalisedcounterparts.1IntroductionMultilabelclassiﬁcationistheproblemofpredictingmultiplelabelsforagiveninstance[TsoumakasandKatakis 2007 ZhangandZhou 2014].Forexample ininformationretrieval onemaypredictwhetheranumberofdocumentsarerelevanttoagivenquery[Manningetal. 2008];inimagetagging onemaypredictwhetherseveralindividuals’facesarecontainedinanimage[Xiaoetal. 2010].Intheextremeclassiﬁcationscenariowherethenumberofpotentiallabelsislarge naïvemodellingofallpossiblelabelcombinationsisprohibitive.Thishasmotivatedanumberofalgorithmstargettingthissetting[Agrawaletal. 2013 Yuetal. 2014 Bhatiaetal. 2015 Jainetal. 2016 BabbarandSchölkopf 2017 Yenetal. 2017 Prabhuetal. 2018 Jainetal. 2019 Reddietal. 2019].Onepopularstrategyistoemployareductiontoaseriesofbinaryormulticlassproblems andthereaftertreatlabelsindependently.Suchreductionssigniﬁcantlyreducethecomplexityoflearning andhaveseenempiricalsuccess.Forexample intheone-versus-allreduction onereducestheproblemtoaseriesofindependentbinaryclassiﬁcationtasks[Brinkeretal. 2006 Dembczy´nskietal. 2010 2012].Similarly inthepick-one-labelreduction onereducestheproblemtomulticlassclassiﬁcationwitharandomlydrawnpositivelabel[Boutelletal. 2004 Jerniteetal. 2017 Joulinetal. 2017].Thetheoreticalaspectsofthesereductionsarelessclear however.Inparticular preciselywhatpropertiesoftheoriginalmultilabelproblemdothesereductionspreserve?Whilesimilarquestionsarewell-studiedforreductionsofmulticlasstobinaryclassiﬁcation[Zhang 2004 TewariandBartlett 2007 Ramaswamyetal. 2014] reductionsformultilabelproblemshavereceivedlessattention despitetheirwideuseinpractice.Recently Wydmuchetal.[2018]establishedthatthepick-one-labelisinconsistentwithrespecttotheprecision@k akeymeasureofretrievalperformance.Ontheotherhand theyshowedthataprobabilisticlabeltree-basedimplementationofone-versus-allisconsistent.Thisintriguingobservationraisestwonaturalquestions:whatcanbesaidaboutconsistencyofotherreductions?Anddoesthepicturechangeifweconsideradifferentmetric suchastherecall@k?33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.ReductionNotationBayes-optimalf∗i(x)Prec@kRec@kConsistencyanalysisOne-versus-all‘OVAP(yi=1|x)X×[Wydmuchetal. 2018]Pick-all-labels‘PALP(yi=1|x)/N(x)X×ThispaperOne-versus-allnormalised‘OVA−NP(y0i=1|x)×XThispaperPick-all-labelsnormalised‘PAL−NP(y0i=1|x)×XThispaperPick-one-label‘POLP(y0i=1|x)×XThispaperTable1:Summaryofreductionsofmultilabeltobinaryormulticlassclassiﬁcationstudiedin§4.1ofthispaper.Foreachreduction wespecifythenotationusedforthelossfunction;theBayes-optimalscorerfortheithlabel assumingrandom(instance label)pairs(x y);whetherthereductionisconsistentfortothemultilabelprecision@kandrecall@k;andwherethisanalysisisprovided.Here P(yi=1|x)denotesthemarginalprobabilityoftheithlabel;P(y0i=1|x)denotesanonlineartransformationofthisprobability per(5);andN(x)istheexpectednumberofrelevantlabelsforx alsoper(12).Inthispaper weprovideasystematicstudyofthesequestionsbyinvestigatingtheconsistencyofﬁvemultilabelreductionswithrespecttobothprecision@kandrecall@k:(a)theone-versus-allreduction(OVA)toindependentbinaryproblems[Dembczy´nskietal. 2010];(b)apick-all-labels(PAL)multiclassreduction whereinaseparatemulticlassexampleiscreatedforeachpositivelabel[Reddietal. 2019];(c)anormalisedone-versus-allreduction(OVA-N) wherethecontributionofeachinstanceisnormalisedbythenumberofrelevantlabels;(d)anormalisedpick-all-labelsreduction(PAL-N) withthesamenormalisationasabove;and (e)thepick-one-label(POL)multiclassreduction whereinasinglemulticlassexampleiscreatedbyrandomlysamplingapositivelabel[Joulinetal. 2017 Jerniteetal. 2017].Ourmainresultisaformaljustiﬁcationofeachreduction:weexplicatethemultilabelrisksunderpin-ningeachofthem(Proposition5) andusethistoshowtheyareeachconsistentwithrespecttoeitherprecisionorrecall(Proposition9 10).Further thisdichotomyisinescapable:theBayes-optimalscorersforthetwomeasuresarefundamentallyincompatible(Corollary2 4) exceptintrivialcaseswherealllabelsareconditionallyindependent orthenumberofrelevantlabelsisconstant.Thisﬁndinghastwoimportantimplications.First whiletheabovereductionsappearsuperﬁciallysimilar theytargetfundamentallydifferentperformancemeasures.Consequently whenrecallisofprimaryinterest suchaswhenthereareonlyafewrelevantlabelsforaninstance[Lapinetal. 2018] employingthe“wrong”reductioncanpotentiallyleadtosuboptimalperformance.Second theprobabilityscoresobtainedfromeachreductionmustbeinterpretedwithcaution:exceptfortheone-versus-allreduction theydonotcoincidewiththemarginalprobabilitiesforeachlabel.Naïvelyusingtheseprobabilitiesfordownstreamdecision-makingmaythusbesub-optimal.Insummary ourcontributionsarethefollowing(seealsoTable1):(1)weformalisetheimplicitmultilabellossandriskunderpinningﬁvedistinctmultilabellearningreductions(§4.1)toasuitablebinaryormulticlassproblem(Proposition5).(2)weestablishsuitableconsistencyofeachreduction:theunnormalisedreductionsareconsistentforprecision whilethenormalisedreductionsareconsistentforrecall(Proposition9 10).(3)weempiricallyconﬁrmthatnormalisedreductionscanyieldrecallgainsoverunnormalisedcounterparts whilethelattercanyieldprecisiongainsovertheformer.2BackgroundandnotationWeformalisethemultilabelclassiﬁcationproblem anditsspecialcaseofmulticlassclassiﬁcation.2.1MultilabelclassiﬁcationSupposewehaveaninstancespaceX(e.g. queries)andlabelspaceY.={0 1}L(e.g. documents)forsomeL∈N+.Here Lrepresentsthetotalnumberofpossiblelabels.Givenaninstancex∈Xwithlabelvectory∈Y weinterpretyi=1tomeanthatthelabeliis“relevant”totheinstancex.Importantly theremaybemultiplerelevantlabelsforagiveninstance.Ourgoalis informally toﬁndarankingoverlabelsgivenaninstance(e.g. rankthemostrelevantdocumentsforaquery).Moreprecisely letPbeadistributionoverX×Y whereP(y|x)denotesthesuitabilityoflabelvectoryforinstancex andP(0L|x)=0(i.e. eachinstancemusthaveatleastonerelevant2label).1Ourgoalistolearnascorerf:X→RLthatorderslabelsaccordingtotheirsuitability(e.g. scoresdocumentsbasedontheirrelevanceforagivenquery).Weevaluateascoreraccordingtotheprecision-at-kandrecall-at-kforgivenk∈[L].={1 2 ... L}[Lapinetal. 2018]:2Prec@k(f).=E(x y)(cid:20)|rel(y)∩Topk(f(x))|k(cid:21)Rec@k(f).=E(x y)(cid:20)|rel(y)∩Topk(f(x))||rel(y)|(cid:21) (1)whereTopk(f)returnsthetopkscoringlabelsaccordingtof(assumingnoties) andrel(y)denotestheindicesoftherelevant(positive)labelsofy.Inaretrievalcontext therecall@kmaybefavourablewhenk(cid:29)rel(y)(i.e. weretrievealargenumberofdocuments butthereareonlyafewrelevantdocumentsforaquery) sincetheprecisionwilldegradeaskincreases[Lapinetal. 2018].Optimisingeitherofthesemeasuresdirectlyisintractable andsotypicallyonepicksamultil-abelsurrogateloss‘ML:{0 1}L×RL→R+ andminimisesthemultilabelriskRML(f).=E(x y)[‘ML(y f(x))].ABayes-optimalscorerf∗isanyminimiserofRML.Forseveralperformancemeasures f∗isanymonotonetransformationofthemarginallabelprobabilitiesP(yi=1|x)[Dem-bczy´nskietal. 2010 Koyejoetal. 2015 WuandZhou 2017].Thus accurateestimationofthesemarginalssufﬁcesforgoodperformanceonthesemeasures.Thisgivescredencetotheexistenceofefﬁcientreductionsthatpreservemultilabelclassiﬁcationperformance atopicweshallstudyin§4.2.2MulticlassclassiﬁcationMulticlassclassiﬁcationisaspecialcaseofmultilabelclassiﬁcationwhereeachinstancehasonlyonerelevantlabel.Concretely ourlabelspaceisnowZ.=[L].SupposethereisanunknowndistributionPoverX×Z whereP(z|x)denotesthesuitabilityoflabelzforinstancex.Wemaynowevaluateacandidatescorerf:X→RLaccordingtothetop-kriskforgivenk∈[L]:Rtop−k(f).=P(z/∈Topk(f(x))).(2)Thismeasureisnaturalwhenwecanmakekguessesastoaninstance’slabel andareonlypenalisedifallguessesareincorrect[Lapinetal. 2018].Thisisequivalenttotheexpectedtop-kloss givenby‘top−k(z f).=Jz/∈Topk(f)K.(3)Whenk=1 weobtainthezero-oneormisclassiﬁcationloss.Forcomputationaltractability ratherthanminimise(2)directly oneoftenoptimisesasurrogateloss‘MC:[L]×RL→R+ withriskRMC(f).=E(x z)[‘MC(z f(x))].Severalsurrogatelosseshavebeenstudied[Zhang 2004 ÁvilaPiresandSzepesvári 2016] themostpopularbeingthesoftmaxcross-entropy‘SM(i f).=−fi+logPj∈[L]efj withfibeingtheithcoordinateofthevectorf.Consistencyofsuchsurrogateswithrespecttothetop-kerrorhavebeenconsideredinseveralrecentworks(seee.g. [Lapinetal. 2015 Lapinetal. 2018 YangandKoyejo 2019]).Wesaythat‘MCisconsistentforthetop-kerrorifdrivingtheexcessriskfor‘MCtozeroalsodrivestheexcessriskforthe‘top−ktozero;thatis foranysequence(fn)∞n=1ofscorers reg(fn;‘MC)→0=⇒reg(fn;‘top−k)→0 (4)wheretheregretofascorerfwithrespectto‘MCisreg(f;‘MC).=RMC(f)−infg:X→RLRMC(g).3Optimalscorersformultilabelprecisionandrecall@kOurfocusinthispaperisonmultilabelclassiﬁcationperformanceaccordingtotheprecision@kandrecall@k(cf.(1)).Itisthereforeprudenttoask:whataretheBayes-optimalpredictionsforeachmeasure?Answeringthisgivesinsightintowhataspectsofascorerthesemeasuresfocuson.Recently Wydmuchetal.[2018]studiedthisquestionforprecision@k establishingthatitisoptimisedbyanyorder-preservingtransformationofthemarginalprobabilitiesP(yi=1|x).Lemma1([Wydmuchetal. 2018]).Themultilabelprecision@kofascorerf:X→RLisPrec@k(f)=ExXi∈Topk(f(x))1k·P(yi=1|x).1Withoutthisassumption onemaytaketheconventionthat0/0=1indeﬁningtherecall@k.2SimilarmetricsmaybedeﬁnedwhenL=1[Karetal. 2014 2015 Liuetal. 2016 Tasche 2018].3Corollary2([Wydmuchetal. 2018]).AssumingtherearenotiesintheprobabilitiesP(yi=1|x) f∗∈argmaxf:X→RPrec@k(f)⇐⇒(∀x∈X)Topk(f∗(x))=Topk(cid:16)[P(yi=1|x)]Li=1(cid:17).Wenowshowthat bycontrast therecall@kwillingeneralnotencourageorderingbythemarginalprobabilities.Indeed itcanbeexpressedinanearlyidenticalformtotheprecison@k butwithacrucialdifference:themarginalprobabilitiesaretransformedbyanadditionalnonlinearweighting.Lemma3.Themultilabelrecall@kofascorerf:X→RLisRec@k(f)=ExXi∈Topk(f(x))P(y0i=1|x)P(y0i=1|x).=P(yi=1|x)·Ey¬i|x yi=1"11+Pj6=iyj# (5)wherey¬idenotesthevectorofallbuttheithlabel i.e. (y1 ... yi−1 yi+1 ... yL)∈{0 1}L−1.The“transformed”probabilitiesP(y0i=1|x)willingeneralnotpreservetheorderingofthemarginalprobabilitiesP(yi=1|x) owingtothemultiplicationbyanon-constantterm.Wethushavethefollowing whichisimplicitinWydmuchetal.[2018 Proposition1] whereinitwasshownthepick-one-labelreductionisinconsistentwithrespecttoprecision.Corollary4.AssumingtherearenotiesintheprobabilitiesP(y0i=1|x) f∗∈argmaxf:X→RRec@k(f)⇐⇒(∀x∈X)Topk(f∗(x))=Topk(cid:16)[P(y0i=1|x)]Li=1(cid:17).Further theorderofP(y0i=1|x)andP(yi=1|x)donotcoincideingeneral.OneimplicationofCorollary4isthat whendesigningreductionsformultilabellearning onemustcarefullyassesswhichofthesetwomeasures(ifany)thereductionisoptimalfor.Wecannothopeforareductiontobeoptimalforboth sincetheirBayes-optimalscorersaregenerallyincompatible.WeremarkhoweverthatonespecialcasewhereP(y0i=1|x)=P(yi=1|x)iswhenPi∈[L]yiisaconstantforeveryinstance whichmayhappenifthelabelsareconditionallyindependent.Havingobtainedahandleontheseperformancemeasures weproceedwithourcentralobjectofinquiry:aretheywellapproximatedbyexistingmultilabelreductions?4Reductionsfrommulti-tosingle-labelclassiﬁcationWestudyﬁvedistinctreductionsofmultilabeltobinaryormulticlassclassiﬁcation.Ourparticularinterestisinwhatmultilabelperformancemeasure(ifany)thesereductionsimplicitlyaimtooptimise.Asaﬁrststeptoansweringthis weexplicatethemultilabellossandriskunderpinningeachofthem.4.1Multilabelreductions:lossfunctionsRecallfrom§2.1thatastandardapproachtomultilabelclassiﬁcationisminimisingasuitablemultilabellossfunction‘ML:{0 1}L×RL→R+.Toconstructsuchaloss apopularapproachistodecomposeitintoasuitablecombinationofbinaryormulticlasslosses;implicitly thisisareductionofmultilabellearningtoasuitablebinaryormulticlassproblem.Weconsiderﬁvedistinctdecompositions(seeTable2).Foreach weexplicatetheirunderlyingmultilabellossinordertocomparethemonanequalfooting.Despitethewidespreaduseofthesereductions toourknowledge theyhavenotbeenexplicitlycomparedinthismannerbypriorwork.Ourgoalistothusprovideauniﬁedanalysisofdistinctmethods similartotheanalysisinDembczy´nskietal.[2012]ofthelabeldependenceassumptionsunderpinningmultilabelalgorithms.One-versus-all(OVA).Theﬁrstapproachisarguablythesimplest:wetrainLindependentbinaryclassiﬁcationmodelstopredicteachyi∈{0 1}.Thiscanbeinterpretedasusingthemultilabelloss‘OVA(y f).=Xi∈[L]‘BC(yi fi)=Xi∈[L]{yi·‘BC(1 fi)+(1−yi)·‘BC(0 fi)} (6)4ReductionExampleinstantiationOne-versus-allPi∈[L]n−yi·logefi1+efi−(1−yi)·log11+efioPick-all-labelsPi∈[L]−yi·logefiPj∈[L]efjOne-versus-allnormalisedPi∈[L]n−yiPj∈[L]yj·logefi1+efi−(cid:16)1−yiPj∈[L]yj(cid:17)·log11+efioPick-all-labelsnormalisedPi∈[L]−yiPj∈[L]yj·logefiPj∈[L]efjPick-one-label−yi0·logefi0Pj∈[L]efj i0∼Discrete(cid:16)nyiPj∈[L]yjo(cid:17)Table2:Examplesofmultilabellossesunderpinningvariousreductions givenlabelsy∈{0 1}Landpredictionsf∈RL.Weassumeasigmoidorsoftmaxcross-entropyfortherelevantbaselosses‘BC ‘MC.where‘BC:{0 1}×R→R+issomebinaryclassiﬁcationloss(e.g. logisticloss).Inwords weconverteach(x y)intoapositiveexampleforeachlabelwithyi=1 andanegativeexampleforeachlabelwithyi=0.Thisisalsoknownasthebinaryrelevancemodel[Brinkeretal. 2006 TsoumakasandVlahavas 2007 Dembczy´nskietal. 2010].Pick-all-labels(PAL).Anothernaturalapproachinvolvesamulticlassratherthanbinaryloss:weconverteach(x y)fory∈Yintomulti-classobservations{(x i):i∈[L] yi=1} withoneobservationperpositivelabel[Reddietal. 2019].Thiscanbeinterpretedasusingthemultilabelloss‘PAL(y f).=Xi∈[L]yi·‘MC(i f) (7)where‘MC:L×RL→R+issomemulticlassloss(e.g. softmaxwithcross-entropyorBOWL per§2.2).Whilethebaselossismulticlass–which inherently assumesthereisonlyonerelevantlabel–wecomputethesumofmanysuchmulticlasslosses oneforeachpositivelabeliny.3Observethateachlossinthesuminvolvestheentirevectorofscoresf;thisisincontrasttotheOVAloss whereineachlossinthesumonlydependsonthescoresfortheithlabel.Further notethatwhenLislarge onemaydesignefﬁcientstochasticapproximationstosuchaloss[Reddietal. 2019].One-versus-allnormalised(OVA-N).Anaturalvariantoftheapproachadoptedintheabovetworeductionsistonormalisethecontributionofeachlossbythenumberofpositivelabels.FortheOVAmethod ratherthanindependentlymodeleachlabelyi wethusmodelnormalisedlabels:‘OVA−N(y f).=Xi∈[L](yiPj∈[L]yj·‘BC(1 fi)+ 1−yiPj∈[L]yj!·‘BC(0 fi)).(8)Togainsomeintuitionforthisloss takethespecialcaseofsquareloss ‘BC(yi fi)=(yi−fi)2.Onemayverifythat‘OVA−N(y f)=Pi∈[L](y0i−fi)2plusaconstant fory0i.=yiPyj.Thus thelossencouragesfitoestimatethe“normalisedlabels”y0i ratherthantherawlabelsyiasinOVA.Pick-all-labelsnormalised(PAL-N).SimilartotheOVA-Nmethod wecannormalisePALto:‘PAL−N(y f).=Xi∈[L]yiPj∈[L]yj·‘MC(i f)=1Pj∈[L]yj·‘PAL(y f).(9)Suchareductionappearstobefolk-knowledgeamongstpractitioners(inparticular beingallowedbypopularlibraries[Abadietal. 2016]) butasfarasweareaware hasnotbeenpreviouslystudied.Togainsomeintuitionforthisloss observethaty0i.=yiPj∈[L]yjformsadistributionoverthelabels.Supposeourscoresf∈RLareconvertedtoaprobabilitydistributionviaasuitablelinkfunctionσ(e.g. thesoftmax) andweapplythelog-lossasour‘MC.Then ‘PAL−Ncorrespondstominimisingthecross-entropybetweenthetrueandmodeldistributionsoverlabels:‘PAL−N(y f)=Xi∈[L]−y0i·logσ(fi)=KL(y0kσ(f))+Constant.3Thisistobecontrastwiththelabelpowersetapproach[Boutelletal. 2004] whichtreatseachdistinctlabelvectorasaseparateclass andthuscreatesamulticlassproblemwith2Lclasses.5Pick-one-label(POL).Inthisreduction givenanexample(x y) weselectasinglerandompositivelabelfromyasthetruelabelforx[Jerniteetal. 2017 Joulinetal. 2017].Thiscanbeunderstoodasastochasticversionof(9) whichconsidersaweightedcombinationofallpositivelabels.4.2Multilabelreductions:populationrisksEachoftheabovereductionsisintuitivelyplausible.Forexample theOVAreductionisaclassicalapproach whichhasseensuccessinmulticlasstobinarycontexts;itisnaturaltoconsideritsuseinmultilabelcontexts.Ontheotherhand thePALreductionexplicitlyencourages“competition”amongstthevariouslabelsif e.g. usedwithasoftmaxcross-entropyloss.Finally thenormalisedreductionsintuitivelypreventinstanceswithmanyrelevantlabelsfromdominatingourmodelling.Inordertomakesuchintuitionsprecise amorecarefulanalysisisneeded.Todoso weconsiderwhatunderlyingmultilabelrisk(i.e. expectedloss)eachreductionimplicitlyoptimises.Proposition5.Givenascorerf:X→R themultilabelrisksforeachoftheabovereductionsare:ROVA(f)=Xi∈[L]E(x yi)[‘BC(yi fi(x))]ROVA−N(f)=Xi∈[L]E(x y0i)[‘BC(y0i fi(x))]RPAL(f)=E(x z)[N(x)·‘MC(z f(x))]RPAL−N(f)=RPOL(f)=E(x z0)[‘MC(z0 f(x))] whereP(y0i=1|x)isper(5) andwehavedeﬁneddiscreterandomvariablesz z0over[L]byP(z0=i|x).=P(y0i=1|x)(10)P(z=i|x).=N(x)−1·P(yi=1|x)(11)N(x).=Xi∈[L]P(yi=1|x).(12)Proposition5explicatesthat asexpected theOVAandOVA-Nmethodsdecomposeintosumsofbinaryclassiﬁcationrisks whiletheotherreductionsdecomposeintosumsofmulticlassrisks.Therearethreemoreinterestingimplications.First normalisationhasanon-trivialeffect:forbothOVAandPAL theirnormalisedcounterpartsinvolvemodiﬁedbinaryandmulticlasslabeldistributionsrespectively.Inparticular whilePALinvolvesP(z|x)constructedfromthemarginallabelprobabilities PAL-NinvolvesP(z0|x)constructedfromthe“transformed”probabilitiesin(5).Second PALyieldsaweightedmulticlassrisk wheretheweightN(x)istheexpectednumberofrelevantlabelsforx.SincePALtreatsthemultilabelproblemasaseriesofmulticlassproblemsforeachpositivelabel instanceswithmanyrelevantlabelshaveagreatercontributiontotheloss.Theweightcanalsobeseenasnormalisingthemarginallabelprobabilities[P(yi=1|x)]i∈[L]toavalidmulticlassdistributionovertheLlabels.Bycontrast theriskinPAL-Nisunweighted despitethelossesforeachbeingrelatedbyascalingfactorper(9).Intuitively thisisaconsequenceofthefactthatthenormalisercanvaryacrossdrawsfromP(y|x) i.e. Ey|x[‘PAL−N(y f(x))]=Ey|x"1Pj∈[L]yj·‘PAL(y f(x))#6=Ey|x"1Pj∈[L]yj#·Ey|x[‘PAL(y f(x))].Third thereisasubtledistinctionbetweenPALandOVA.Theformerallowsfortheuseofanarbitrarymulticlassloss‘MC;inthesimplestcase forabasebinaryclassiﬁcationloss‘BC wemaychoosealosswhichtreatsthegivenlabelasapositive andallotherlabelsasnegative:‘MC(i f)=‘BC(1 fi)+Pj6=i‘BC(0 fj).Thisisamulticlassversionoftheone-versus-allreduction[RifkinandKlautau 2004].However evenwiththischoice thePALandOVArisksdonotagree.Lemma6.TheriskofthePALreductionusingthemulticlassone-verus-alllossisRPAL(f)=ROVA(f)+Xi∈[L]Ex[(N(x)−1)·‘BC(0 fi(x))].Tounderstandthisintuitively thePALlossdecomposesintoonetermforeachrelevantlabel;foreachsuchterm weapplytheloss‘MC whichconsidersallotherlabelstobenegative.Consequently everyirrelevantlabeliscountedmultipletimes whichmanifestsintheextraweightingtermabove.Wewillshortlyseehowthisinﬂuencestheoptimalscorersforthereduction.65OptimalscorersformultilabelreductionsHavingexplicatedtherisksunderpinningeachofthereductions wearenowinapositiontoanswerthequestionofwhatpreciselytheyareoptimising.Weshowthatinfacteachreductionisconsistentforeithertheprecision@korrecall@k;following§3 theycannotbeoptimalforbothingeneral.5.1Multilabelreductions:Bayes-optimalpredictionsWebeginouranalysisbycomputingtheBayes-optimalscorersforeachreduction.Doingsorequireswecommittoaparticularfamilyoflossfunctions‘BCand‘MC.Weconsiderthefamilyofstrictlyproperlosses[Savage 1971 Bujaetal. 2005 ReidandWilliamson 2010] whoseBayes-optimalsolutioninabinaryormulticlasssettingistheunderlyingclass-probability.Canonicalexamplesarethecross-entropyandsquareloss.Inthefollowing weshallassumeourscorersareoftheformf:X→[0 1] sothattheyoutputvalidprobabilitiesratherthanarbitraryrealnumbers;inpracticethisistypicallyachievedbycouplingascorerwithalinkfunction e.g. thesoftmaxorsigmoid.Corollary7.Suppose‘BCand‘MCarestrictlyproperlosses andweusescorersf:X→[0 1].Then foreveryx∈Xandi∈[L] theBayes-optimalf∗i(x)fortheOVAandPALreductionsaref∗OVA i(x)=P(yi=1|x)f∗PAL i(x)=N(x)−1·P(yi=1|x) whiletheBayes-optimalf∗i(x)foreachofthe“normalised”reductions(OVA-N PAL-N POL)aref∗i(x)=P(y0i=1|x).Inwords theunnormalisedreductionsresultinsolutionsthatpreservetheorderingofthemarginalprobabilities whilethenormalisedreductionsresultinsolutionsthatpreservetheorderingofthe“transformed”marginalprobabilities.RecallingCorollary2and4 weseethattheunnormalisedreductionsimplicitlyoptimiseforprecision whilethenormalisedreductionsimplicitlyoptimiseforrecall.WhilethisfactiswellknownforOVA[Dembczy´nskietal. 2010 Wydmuchetal. 2018] toourknowledge thequestionofBayes-optimalityhasnotbeenexploredfortheotherreductions.5.2Multilabelreductions:consistencyCorollary7showsthatthevariousreductions’asymptotictargetscoincidewiththoseforprecisionorrecall.Butwhatcanbesaidabouttheirconsistency(inthesenseofEquation4)?Forthemulticlassreductions wenowshowthisfollowsowingtoastrongerversionofCorollary7:PALandPAL-Nhaveidenticalrisks(uptoscalingandtranslation)totheprecisionandrecall@k respectively.Corollary8.Suppose‘MCisthetop-klossin(2).Then usingthislosswithPALandPAL-N RPAL(f)=Constant−k·Prec@k(f)RPAL−N(f)=Constant−Rec@k(f).Despitetheprioruseofthesereductions theaboveconnectionhas toourknowledge notbeennotedhitherto.Itexplicatesthattwosuperﬁciallysimilarreductionsoptimiseforfundamentallydifferentquantities andtheirusageshouldbemotivatedbywhichoftheseisusefulforaparticularapplication.Buildingonthis wenowshowthatwhenusedwithsurrogatelossesthatareconsistentfortop-kerror(cf.(4)) thereductionsareconsistentwithrespecttotheprecisionandrecall respectively.Inthefollowing denotetheregretofascorerf:X→RLwithrespecttoamultilabelloss‘MLbyreg(f;‘ML).=E(x y)[‘ML(y f(x))]−infg:X→RLE(x y)[‘ML(y g(x))].Wesimilarlydenotetheregretfortheprecision@kandrecall@kbyreg(f;P@k)andreg(f;R@k).Proposition9.Suppose‘MCisconsistentforthetop-kerror.Foranysequence(fn)∞n=1ofscorers reg(fn;‘PAL)→0=⇒reg(fn;P@k)→0reg(fn;‘PAL−N)→0=⇒reg(fn;R@k)→0.7OurﬁnalanalysisisregardingtheOVA-Nmethod whichdoesnothaveariskequivalencetotherecall@k.4Nonetheless itsBayes-optimalscorerswereseentocoincidewiththatoftherecall;asaconsequence similartotheconsistencyanalysisofOVAinWydmuchetal.[2018] wemayshowthataccurateestimationofthetransformedprobabilitiesP(y0i=1|x)impliesgoodrecallperformance.ThisconsequentlyimpliesconsistencyofOVA-N asthelattercanguaranteegoodprobabilityestimateswhenequippedwithastronglyproperloss[Agarwal 2014] suchasforexamplethecross-entropyorsquareloss.Proposition10.Suppose‘BCisaλ-stronglyproperloss.Foranyscorerf:X→[0 1] reg(f;R@k)≤2·Ex(cid:20)maxi∈[L]|P(y0i=1|x)−fi(x)|(cid:21)≤p2/λ·reg(f;‘OVA−N).5.3ImplicationsandfurtherconsiderationsWeconcludeouranalysiswithsomeimplicationsoftheaboveresults.OVAversusPAL.Corollary7suggeststhatforoptimisingprecision(orrecall) thereisnoasymptoticdifferencebetweenusingOVAandPAL(ortheirnormalisedcounterparts).However apotentialadvantageofthePALapproachisthatitallowsforuseoftightsurrogatestothetop-kloss(2) whereinonlythetop-kscoringnegativesareconsideredintheloss.InsettingswhereLislarge onecanefﬁcientlyoptimisesuchalossviathestochasticnegativeminingapproachofReddietal.[2019].Interpretingmodelscores.OnesubtleimplicationofCorollary7isthatforallmethodsbutOVA thelearnedscoresdonotreﬂectthemarginallabelprobabilities.Inparticular whiletheoptimalscorerfortheOVAandPALbothpreservetheorderofthemarginalprobabilities thelatteradditionallyinvolvesaninstance-dependentscalingbyN(x).Consider then anextremescenariowhereforsomex∈X alllabelshavemarginalP(yi=1|x)=1.UndertheOVAreduction wewouldassigneachlabelascoreof1 indicating“perfectrelevance”.However underthePALreduction wewouldassignthemascoreof1L whichwouldnaïvelyindicate“lowrelevance”.ForPAL onecanrectifythisbypositingaformgi(x)forP(yi=1|x) e.g. gi(x)=σ(wTix).Then ifoneusesfi(x)=gi(x)Pj∈[L]gj(x) thelearnedgi’swillmodelthemarginals uptoscaling.Learningfrommulticlasssamples.Ourfocusinthepaperhasbeenonsettingswherewehaveaccesstomultilabelsamples(x y) andchoosetoconvertthemtosuitablebinaryormulticlasssamples e.g. (x z)forPAL.Anaturalquestioniswhetheritispossibletolearnwhenweonlyhavepartialknowledgeofthetruemultilabelvectory.Forexample inaninformationretrievalsetup wewouldideallyliketoobservethemultilabelvectorofallrelevantdocumentsforaquery.However inpractice wemayonlyobserveasinglerelevantdocument(e.g. theﬁrstdocumentclickedbyauserissuingaquery) whichisrandomlysampledaccordingtothemarginallabeldistribution(11).InthenotationofProposition5 suchasettingcorrespondstoobservingmulticlasssamplesfromP(x z)directly ratherthanmultilabelsamplesfromP(x y).Anaturalthoughtistothenminimisethetop-krisk(2)directlyonsuchsamples inhopesofoptimisingforprecision.Surprisingly thisdoesnotcorrespondtooptimisingforprecisionorrecall aswenowexplicate.Lemma11.PickanyP(x y)overX×{0 1}L inducingadistributionP(x z)asin(12).Then Rtop−k(f)=P(z/∈Topk(f(x)))=ExXi/∈Topk(f(x))N(x)−1·P(yi=1|x).Thisriskaboveissimilartotheprecision@k exceptfortheN(x)−1term.Proposition5revealsacrucialmissingingredientthatexplainsthislackofequivalence:whenlearningfrom(x z) oneneedstoweightthesamplesbyN(x) thenumberofrelevantlabels.ThisisachievedimplicitlybythePALreduction sincethelossinvolvesatermforeachrelevantlabel.OnemayfurthercontrasttheabovetothePOLreduction:whilethisreductiondoesnotweighsamples itsampleslabelsaccordingtothetransformeddistribution(10) ratherthanthemarginaldistribution(11).ThisdistinctioniscrucialinensuringthatthePOLmatchestherecall@k.4ConsistencyofOVAforprecision@kwasdoneinWydmuchetal.[2018].8Figure1:Precision@kandrecall@kforthePALandPAL-Nreductions.Aspredictedbythetheory theformeryieldssuperiorprecision@kperformance whilethelatteryieldssuperiorrecall@kperformance.TheOVAandOVA-Nreductionsshowthesametrend andareomittedforclarity.Themaximalnumberoflabelsforanyinstanceisk=5 whichisthepointatwhichbothmethodshaveoverlappingcurves.Insum whenweonlyobserveasinglerelevantlabelforaninstance careisneededtounderstandwhatdistributionthislabelissampledfrom andwhetheradditionalinstanceweightingisnecessary.6ExperimentalvalidationWenowpresentempiricalresultsvalidatingtheprecedingtheory.Speciﬁcally weillustratethatusingthepick-all-labels(PAL)reductionversusitsnormalisedcounterpartcanhavesigniﬁcantdifferencesintermsofprecisionandrecallperformance.Sincethereductionsstudiedhereareallextantintheliterature ouraimisnottoproposeoneofthemasbeing“best”formultilabelclassiﬁcation;rather wewishtoverifythattheysimplyoptimisefordifferentquantities.Toremovepotentialconfoundingfactors weconstructasyntheticdatasetwherewehavecompletecontroloverthedatadistribution.Ourconstructionisinspiredbyaninformationretrievalscenario whereintherearetwodistinctgroupsofqueries(e.g. issuedbydifferentuserbases) whichhavealargelydisjointsetofrelevantdocuments.Withinagroup queriesmaybeeithergenericorspeciﬁc i.e. havemanyorfewmatchingdocuments.Formally wesetX=R2andL=10labels.Wedrawinstancesfromaequal-weightedmixtureoftwoGaussians wheretheGaussiansarecenteredat(1 1)and(−1 −1)respectively.Fortheﬁrstmixturecomponent wedrawlabelsaccordingtoP(y|x)whichisuniformovertwopossibley:eithery=(1K−1 0 0K) ory=(0K−1 1 0K) whereK.=L/2.Forthesecondmixturecomponent they’saresupportedonthe“swapped”labelsy=(0K 0 1K−1) andy=(0K 1 0K−1).Withthissetup wegenerateatrainingsampleof104(instance label)pairs.WetraintheOVA PAL OVA-NandPAL-Nmethods andcomputetheirprecisionandrecallonatestsampleof103(instance label)pairs.Weusealinearmodelforourscorerf andthesoftmaxcross-entropylossfor‘MC.Figure1showstheprecisionandrecall@kcurvesaskisvaried.Aspredictedbythetheory thereisasigniﬁcantgapinperformancebetweenthetwomethodsoneachmetric;e.g. thePALmethodperformssigniﬁcantlybetterthanitsnormalisedcounterpartintermsofprecision.Thisillustratestheimportanceofchoosingthecorrectreductionbasedontheultimateperformancemeasureofinterest.7ConclusionandfutureworkWehavestudiedﬁvecommonlyusedmultilabelreductionsinauniﬁedframework explicatingtheunderlyingmultilabellosseachofthemoptimises.Wethenshowedthateachreductionisprovablyconsistentwithrespecttoeitherprecisionorrecall butnotboth.Further weestablishedthattheBayes-optimalscorersfortheprecisionandrecallonlycoincideinspecialcases(e.g. whenthelabelsareconditionallyindependent) andsonoreductioncanbeoptimalforboth.Weempiricallyvalidatedthatnormalisedlossfunctionscanyieldrecallgainsoverunnormalisedcounterparts.Consistencyanalysisforothermultilabelmetricsandgeneralisationanalysisarenaturaldirectionsforfuturework.9ReferencesMartínAbadi PaulBarham JianminChen ZhifengChen AndyDavis JeffreyDean MatthieuDevin SanjayGhemawat GeoffreyIrving MichaelIsard ManjunathKudlur JoshLevenberg RajatMonga SherryMoore DerekG.Murray BenoitSteiner PaulTucker VijayVasudevan PeteWarden MartinWicke YuanYu andXiaoqiangZheng.Tensorﬂow:Asystemforlarge-scalemachinelearning.InProceedingsofthe12thUSENIXConferenceonOperatingSystemsDesignandImplementation OSDI’16 pages265–283 Berkeley CA USA 2016.USENIXAssociation.ShivaniAgarwal.Surrogateregretboundsforbipartiterankingviastronglyproperlosses.JournalofMachineLearningResearch 15:1653–1674 2014.RahulAgrawal ArchitGupta YashotejaPrabhu andManikVarma.Multi-labellearningwithmillionsoflabels:Recommendingadvertiserbidphrasesforwebpages.InProceedingsofthe22NdInternationalConferenceonWorldWideWeb WWW’13 pages13–24 NewYork NY USA 2013.ACM.ISBN978-1-4503-2035-1.BernardoÁvilaPiresandCsabaSzepesvári.MulticlassClassiﬁcationCalibrationFunctions.arXive-prints art.arXiv:1609.06385 Sep2016.RohitBabbarandBernhardSchölkopf.Dismec:Distributedsparsemachinesforextrememulti-labelclassiﬁca-tion.InProceedingsoftheTenthACMInternationalConferenceonWebSearchandDataMining WSDM’17 pages721–729 NewYork NY USA 2017.ACM.ISBN978-1-4503-4675-7.KushBhatia HimanshuJain PurushottamKar ManikVarma andPrateekJain.Sparselocalembeddingsforextrememulti-labelclassiﬁcation.InAdvancesinNeuralInformationProcessingSystems28 pages730–738.CurranAssociates Inc. 2015.MatthewR.Boutell JieboLuo XipengShen andChristopherM.Brown.Learningmulti-labelsceneclassiﬁca-tion.PatternRecognition 37(9):1757–1771 2004.KlausBrinker JohannesFürnkranz andEykeHüllermeier.Auniﬁedmodelformultilabelclassiﬁcationandranking.InProceedingsofthe2006ConferenceonECAI2006 pages489–493 Amsterdam TheNetherlands TheNetherlands 2006.IOSPress.ISBN1-58603-642-4.A.Buja W.Stuetzle andY.Shen.Lossfunctionsforbinaryclassprobabilityestimationandclassiﬁcation:Structureandapplications.Technicalreport UPenn 2005.KrzysztofDembczy´nski WeiweiCheng andEykeHüllermeier.Bayesoptimalmultilabelclassiﬁcationviaprobabilisticclassiﬁerchains.InProceedingsofthe27thInternationalConferenceonInternationalConferenceonMachineLearning ICML’10 pages279–286 USA 2010.Omnipress.ISBN978-1-60558-907-7.KrzysztofDembczy´nski WillemWaegeman WeiweiCheng andEykeHüllermeier.Onlabeldependenceandlossminimizationinmulti-labelclassiﬁcation.MachineLearning 88(1-2):5–45 July2012.ISSN0885-6125.HimanshuJain YashotejaPrabhu andManikVarma.Extrememulti-labellossfunctionsforrecommendation tagging ranking&othermissinglabelapplications.InProceedingsofthe22NdACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining KDD’16 pages935–944 NewYork NY USA 2016.ACM.ISBN978-1-4503-4232-2.HimanshuJain VenkateshBalasubramanian BhanuChunduri andManikVarma.Slice:Scalablelinearextremeclassiﬁerstrainedon100millionlabelsforrelatedsearches.InProceedingsoftheTwelfthACMInternationalConferenceonWebSearchandDataMining WSDM’19 pages528–536 NewYork NY USA 2019.ACM.ISBN978-1-4503-5940-5.YacineJernite AnnaChoromanska andDavidSontag.Simultaneouslearningoftreesandrepresentationsforextremeclassiﬁcationanddensityestimation.InProceedingsofthe34thInternationalConferenceonMachineLearning volume70ofProceedingsofMachineLearningResearch pages1665–1674 InternationalConventionCentre Sydney Australia 06–11Aug2017.PMLR.ArmandJoulin EdouardGrave PiotrBojanowski andTomasMikolov.Bagoftricksforefﬁcienttextclassiﬁca-tion.InProceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics:Volume2 ShortPapers pages427–431 Valencia Spain April2017.AssociationforComputa-tionalLinguistics.PurushottamKar HarikrishnaNarasimhan andPrateekJain.Onlineandstochasticgradientmethodsfornon-decomposablelossfunctions.InProceedingsofthe27thInternationalConferenceonNeuralInformationProcessingSystems-Volume1 NIPS’14 pages694–702 Cambridge MA USA 2014.MITPress.10PurushottamKar HarikrishnaNarasimhan andPrateekJain.Surrogatefunctionsformaximizingprecisionatthetop.InProceedingsofthe32ndInternationalConferenceonInternationalConferenceonMachineLearning ICML’15 pages189–198.JMLR.org 2015.OluwasanmiKoyejo PradeepRavikumar NagarajanNatarajan andInderjitS.Dhillon.Consistentmultilabelclassiﬁcation.InProceedingsofthe28thInternationalConferenceonNeuralInformationProcessingSystems-Volume2 NIPS’15 pages3321–3329 Cambridge MA USA 2015.MITPress.M.Lapin M.Hein andB.Schiele.Analysisandoptimizationoflossfunctionsformulticlass top-k andmultilabelclassiﬁcation.IEEETransactionsonPatternAnalysisandMachineIntelligence 40(7):1533–1554 July2018.MaksimLapin MatthiasHein andBerntSchiele.Top-kmulticlasssvm.InProceedingsofthe28thInternationalConferenceonNeuralInformationProcessingSystems-Volume1 NIPS’15 pages325–333 Cambridge MA USA 2015.MITPress.Li-PingLiu ThomasG.Dietterich NanLi andZhi-HuaZhou.Transductiveoptimizationoftopkprecision.InProceedingsoftheTwenty-FifthInternationalJointConferenceonArtiﬁcialIntelligence IJCAI’16 pages1781–1787.AAAIPress 2016.ISBN978-1-57735-770-4.ChristopherD.Manning PrabhakarRaghavan andHinrichSchütze.IntroductiontoInformationRetrieval.CambridgeUniversityPress NewYork NY USA 2008.YashotejaPrabhu AnilKag ShrutendraHarsola RahulAgrawal andManikVarma.Parabel:Partitionedlabeltreesforextremeclassiﬁcationwithapplicationtodynamicsearchadvertising.InProceedingsofthe2018WorldWideWebConference WWW’18 pages993–1002 RepublicandCantonofGeneva Switzerland 2018.InternationalWorldWideWebConferencesSteeringCommittee.ISBN978-1-4503-5639-8.HarishG.Ramaswamy BalajiSrinivasanBabu ShivaniAgarwal andRobertC.Williamson.Ontheconsistencyofoutputcodebasedlearningalgorithmsformulticlasslearningproblems.InProceedingsofThe27thConferenceonLearningTheory volume35ofProceedingsofMachineLearningResearch pages885–902 Barcelona Spain 13–15Jun2014.PMLR.SashankJ.Reddi SatyenKale FelixYu DanielHoltmann-Rice JiecaoChen andSanjivKumar.Stochasticnegativeminingforlearningwithlargeoutputspaces.InProceedingsofMachineLearningResearch volume89ofProceedingsofMachineLearningResearch pages1940–1949.PMLR 16–18Apr2019.MarkD.ReidandRobertC.Williamson.Compositebinarylosses.JournalofMachineLearningResearch 11:2387–2422 December2010.RyanRifkinandAldebaroKlautau.Indefenseofone-vs-allclassiﬁcation.J.Mach.Learn.Res. 5:101–141 December2004.ISSN1532-4435.LeonardJ.Savage.Elicitationofpersonalprobabilitiesandexpectations.JournaloftheAmericanStatisticalAssociation 66(336):783–801 1971.DirkTasche.Aplug-inapproachtomaximisingprecisionatthetopandrecallatthetop.CoRR abs/1804.03077 2018.URLhttp://arxiv.org/abs/1804.03077.AmbujTewariandPeterL.Bartlett.Ontheconsistencyofmulticlassclassiﬁcationmethods.J.Mach.Learn.Res. 8:1007–1025 December2007.ISSN1532-4435.GrigoriosTsoumakasandIoannisKatakis.Multi-labelclassiﬁcation:Anoverview.IntJDataWarehousingandMining 2007:1–13 2007.GrigoriosTsoumakasandIoannisVlahavas.Randomk-labelsets:Anensemblemethodformultilabelclas-siﬁcation.InMachineLearning:ECML2007 pages406–417 Berlin Heidelberg 2007.SpringerBerlinHeidelberg.Xi-ZhuWuandZhi-HuaZhou.Auniﬁedviewofmulti-labelperformancemeasures.InProceedingsofthe34thInternationalConferenceonMachineLearning volume70ofProceedingsofMachineLearningResearch pages3780–3788 InternationalConventionCentre Sydney Australia 06–11Aug2017.PMLR.MarekWydmuch KalinaJasinska MikhailKuznetsov RóbertBusa-Fekete andKrzysztofDembczynski.Ano-regretgeneralizationofhierarchicalsoftmaxtoextrememulti-labelclassiﬁcation.InAdvancesinNeuralInformationProcessingSystems31 pages6355–6366.CurranAssociates Inc. 2018.J.Xiao J.Hays K.A.Ehinger A.Oliva andA.Torralba.Sundatabase:Large-scalescenerecognitionfromabbeytozoo.In2010IEEEComputerSocietyConferenceonComputerVisionandPatternRecognition pages3485–3492 June2010.11ForestYangandSanmiKoyejo.Ontheconsistencyoftop-ksurrogatelosses.CoRR abs/1901.11141 2019.URLhttp://arxiv.org/abs/1901.11141.IanE.H.Yen XiangruHuang WeiDai PradeepRavikumar InderjitDhillon andEricXing.Ppdsparse:Aparallelprimal-dualsparsemethodforextremeclassiﬁcation.InProceedingsofthe23rdACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining KDD’17 pages545–553 NewYork NY USA 2017.ACM.ISBN978-1-4503-4887-4.Hsiang-FuYu PrateekJain PurushottamKar andInderjitS.Dhillon.Large-scalemulti-labellearningwithmissinglabels.InProceedingsofthe31stInternationalConferenceonInternationalConferenceonMachineLearning-Volume32 ICML’14 pagesI–593–I–601.JMLR.org 2014.M.ZhangandZ.Zhou.Areviewonmulti-labellearningalgorithms.IEEETransactionsonKnowledgeandDataEngineering 26(8):1819–1837 Aug2014.TongZhang.Statisticalbehaviorandconsistencyofclassiﬁcationmethodsbasedonconvexriskminimization.Ann.Statist. 32(1):56–85 022004.12,Aditya Menon
Ankit Singh Rawat
Sashank Reddi
Sanjiv Kumar