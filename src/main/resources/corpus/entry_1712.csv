2014,Efficient Inference of Continuous Markov Random Fields with Polynomial Potentials,In this paper  we prove that every multivariate polynomial with even degree can be decomposed into a sum of convex and concave polynomials. Motivated by this property  we exploit the concave-convex procedure to perform inference on continuous Markov random fields with polynomial potentials. In particular  we show that the concave-convex decomposition of polynomials can be expressed as a sum-of-squares optimization  which can be efficiently solved via semidefinite programming. We demonstrate the effectiveness of our approach in the context of 3D reconstruction  shape from shading and image denoising  and show that our approach significantly outperforms existing approaches in terms of efficiency as well as the quality of the retrieved solution.,Efﬁcient Inference of Continuous Markov Random

Fields with Polynomial Potentials

Shenlong Wang

University of Toronto

Alexander G. Schwing
University of Toronto

Raquel Urtasun

University of Toronto

slwang@cs.toronto.edu

aschwing@cs.toronto.edu

urtasun@cs.toronto.edu

Abstract

In this paper  we prove that every multivariate polynomial with even degree can
be decomposed into a sum of convex and concave polynomials. Motivated by
this property  we exploit the concave-convex procedure to perform inference on
continuous Markov random ﬁelds with polynomial potentials. In particular  we
show that the concave-convex decomposition of polynomials can be expressed as
a sum-of-squares optimization  which can be efﬁciently solved via semideﬁnite
programing. We demonstrate the effectiveness of our approach in the context
of 3D reconstruction  shape from shading and image denoising  and show that
our method signiﬁcantly outperforms existing techniques in terms of efﬁciency as
well as quality of the retrieved solution.

1

Introduction

Graphical models are a convenient tool to illustrate the dependencies among a collection of random
variables with potentially complex interactions. Their widespread use across domains from com-
puter vision and natural language processing to computational biology underlines their applicability.
Many algorithms have been proposed to retrieve the minimum energy conﬁguration  i.e.  maximum
a-posteriori (MAP) inference  when the graphical model describes energies or distributions deﬁned
on a discrete domain. Although this task is NP-hard in general  message passing algorithms [16] and
graph-cuts [4] can be used to retrieve the global optimum when dealing with tree-structured models
or binary Markov random ﬁelds composed out of sub-modular energy functions.
In contrast  graphical models with continuous random variables are much less well understood. A
notable exception is Gaussian belief propagation [31]  which retrieves the optimum when the poten-
tials are Gaussian for arbitrary graphs under certain conditions of the underlying system. Inspired
by discrete graphical models  message-passing algorithms based on discrete approximations in the
form of particles [6  17] or non-linear functions [27] have been developed for general potentials.
They are  however  computationally expensive and do not perform well when compared to dedi-
cated algorithms [20]. Fusion moves [11] are a possible alternative  but they rely on the generation
of good proposals  a task that is often difﬁcult in practice. Other related work focuses on representing
relations on pairwise graphical models [24]  or marginalization rather than MAP [13].
In this paper we study the case where the potentials are polynomial functions. This is a very general
family of models as many applications such as collaborative ﬁltering [8]  surface reconstruction [5]
and non-rigid registration [30] can be formulated in this way. Previous approaches rely on either
polynomial equation system solvers [20]  semi-deﬁnite programming relaxations [9  15] or approxi-
mate message-passing algorithms [17  27]. Unfortunately  existing methods either cannot cope with
large-scale graphical models  and/or do not have global convergence guarantees.
In particular  we exploit the concave-convex procedure (CCCP) [33] to perform inference on con-
tinuous Markov random ﬁelds (MRFs) with polynomial potentials. Towards this goal  we ﬁrst show
that an arbitrary multivariate polynomial function can be decomposed into a sum of a convex and

1

a concave polynomial. Importantly  this decomposition can be expressed as a sum-of-squares opti-
mization [10] over polynomial Hessians  which is efﬁciently solvable via semideﬁnite programming.
Given the decomposition  our inference algorithm proceeds iteratively as follows: at each iteration
we linearize the concave part and solve the resulting subproblem efﬁciently to optimality. Our algo-
rithm inherits the global convergence property of CCCP [25].
We demonstrate the effectiveness of our approach in the context of 3D reconstruction  shape from
shading and image denoising. Our method proves superior in terms of both computational cost and
the energy of the solutions retrieved when compared to approaches such as dual decomposition [20] 
fusion moves [11] and particle belief propagation [6].

2 Graphical Models with Continuous Variables and Polynomial Functions

In this section we ﬁrst review inference algorithms for graphical models with continuous random
variables  as well as the concave-convex procedure. We then prove existence of a concave-convex
decomposition for polynomials and provide a construction. Based on this decomposition and con-
struction  we propose a novel inference algorithm for continuous MRFs with polynomial potentials.

The MRFs we consider represent distributions deﬁned over a continuous domain X =(cid:81)
the system as a sum of local scoring functions  i.e.  f (x) = (cid:80)
which is speciﬁed by the restriction often referred to as region r ⊆ {1  . . .   n}  i.e.  Xr =(cid:81)

2.1 Graphical Models with Polynomial Potentials
i Xi  which
is a product-space assembled by continuous sub-spaces Xi ⊂ R. Let x ∈ X be the output conﬁg-
uration of interest  e.g.  a 3D mesh or a denoised image. Note that each output conﬁguration tuple
x = (x1 ···   xn) subsumes a set of random variables. Graphical models describe the energy of
r∈R fr(xr). Each local function
fr(xr) : Xr → R depends on a subset of variables xr = (xi)i∈r deﬁned on a domain Xr ⊆ X  
i∈r Xi.
We refer to R as the set of all restrictions required to compute the energy of the system.
We tackle the problem of maximum a-posteriori (MAP) inference  i.e.  we want to ﬁnd the conﬁgu-
ration x∗ having the minimum energy. This is formally expressed as

fr(xr).

(1)

(cid:88)

r∈R

x∗ = arg min

x

Solving this program for general functions is hard. In this paper we focus on energies composed of
polynomial functions. This is a fairly general case  as the energies employed in many applications
obey this assumption. Furthermore  for well-behaved continuous non-polynomial functions (e.g. 
k-th order differentiable) polynomial approximations could be used (e.g.  via a Taylor expansion).
Let us deﬁne polynomials more formally:
Deﬁnition 1. A d-degree multivariate polynomial f (x) : Rn → R is a ﬁnite linear combination of
monomials  i.e. 

where we let the coefﬁcient cm ∈ R and the tuple m = (m1  . . .   mn) ∈ M ⊆ Nn with(cid:80)n

d ∀m ∈ M. The set M subsumes all tuples relevant to deﬁne the function f.

1 xm2

cmxm1

f (x) =

··· xmn
n  

2

(cid:88)

m∈M

i=1 mi ≤

We are interested in minimizing Eq. (1) where the potential functions fr are polynomials with arbi-
trary degree. This is a difﬁcult problem as polynomial functions are in general non-convex. More-
over  for many applications of interest we have to deal with a large number of variables  e.g.  more
than 60 000 when reconstructing shape from shading of a 256 × 256 image. Optimal solutions ex-
ist under certain conditions when the potentials are Gaussian [31]  i.e.  polynomials of degree 2.
Message passing algorithms have not been very successful for general polynomials due to the fact
that the messages are continuous functions. Discrete [6  17] and non-parametric [27] approxima-
tions have been employed with limited success. Furthermore  polynomial system solvers [20]  and
moment-based methods [9] cannot scale up to such a large number of variables. Dual-decomposition
provides a plausible approach for tackling large-scale problems by dividing the task into many small
sub-problems [20]. However  solving a large number of smaller systems is still a bottleneck  and
decoding the optimal solution from the sub-problems might be difﬁcult. In contrast  we propose to
use the Concave-Convex Procedure (CCCP) [33]  which we now brieﬂy review.

2

Inference via CCCP

2.2
CCCP is a majorization-minimization framework for optimizing non-convex functions that can be
written as the sum of a convex and a concave part  i.e.  f (x) = fvex(x) + fcave(x). This frame-
work has recently been used to solve a wide variety of machine learning tasks  such as learning in
structured models with latent variables [32  22]  kernel methods with missing entries [23] and sparse
principle component analysis [26]. In CCCP  f is optimized by iteratively computing a linearization
of the concave part at the current iterate x(i) and solving the resulting convex problem

x(i+1) = arg min

x

fvex(x) + xT∇fcave(x(i)).

(2)

This process is guaranteed to monotonically decrease the objective and it converges globally  i.e. 
for any point x (see Theorem 2 of [33] and Theorem 8 [25]). Moreover  Salakhutdinov et al. [19]
showed that the convergence rate of CCCP  which is between super-linear and linear  depends on
the curvature ratio between the convex and concave part. In order to take advantage of CCCP to
solve our problem  we need to decompose the energy function into a sum of convex and concave
parts. In the next section we show that this decomposition always exists. Furthermore  we provide a
procedure to perform this decomposition given general polynomials.

2.3 Existence of a Concave-Convex Decomposition of Polynomials
Theorem 1 in [33] shows that for all arbitrary continuous functions with bounded Hessian a decom-
position into convex and concave parts exists. However  Hessians of polynomial functions are not
bounded in Rn. Furthermore 
[33] did not provide a construction for the decomposition. In this
section we show that for polynomials this decomposition always exists and we provide a construc-
tion. Note that since odd degree polynomials are unbounded from below  i.e.  not proper  we only
focus on even degree polynomials in the following. Let us therefore consider the space spanned by
polynomial functions with an even degree d.
Proposition 1. The set of polynomial functions f (x) : Rn → R with even degree d  denoted P n
a topological vector space. Furthermore  its dimension dim(P n

(cid:18) n + d − 1

(cid:19)

d   is

d ) =

.

d

Proof. (Sketch) According to the deﬁnition of vector spaces  we know that the set of polynomial
functions forms a vector space over R. We can then show that addition and multiplication over the
polynomial ring P n
d ) is equivalent to computing a d-combination
with repetition from n elements [3].

d is continuous. Finally  dim(P n

Next we investigate the geometric properties of convex even degree polynomials.
Lemma 1. Let the set of convex polynomial functions c(x) : Rn → R with even degree d be Cn
d .
This subset of P n
Proof. Given two arbitrary convex polynomial functions f and g ∈ Cn
scalars a  b ∈ R+. ∀x  y ∈ Rn ∀λ ∈ [0  1]  we have:

d   let h = af +bg with positive

d is a convex cone.

h(λx + (1 − λ)y) = af (λx + (1 − λ)y) + bg(λx + (1 − λ)y)

≤ a(λf (x) + (1 − λ)f (y)) + b(λh(x) + (1 − λ)h(y))
= λh(x) + (1 − λ)h(y).
d  ∀a  b ∈ R+  we have af + bg ∈ Cn

d is a convex cone.

d   i.e.  Cn

Therefore  ∀f  g ∈ Cn

We now show that the eigenvalues of the Hessian of f (hence the smallest one) continuously depend
on f ∈ P n
d .
Proposition 2. For any polynomial function f ∈ P n
d with d ≥ 2  the eigenvalues of its Hessian
eig(∇2f (x)) are continuous w.r.t. f in the polynomial space P n
d .
Proof. ∀f ∈ P n
i cigi  linear in
the coefﬁcients ci. It is easy to see that ∀f ∈ P n
d   the Hessian ∇2f (x) is a polynomial matrix 
i ci∇2gi(x) deﬁne
the Hessian as a function of the coefﬁcients (c1 ···   cn). The eigenvalues eig(M (c1 ···   cn)) are

d   we obtain the representation f = (cid:80)
i ci∇2gi(x). Let M (c1 ···   cn) = ∇2f (x) =(cid:80)

linear in ci  i.e.  ∇2f (x) =(cid:80)

d   given a basis {gi} of P n

3

equivalent to the root of the characteristic polynomial of M (c1 ···   cn)  i.e.  the set of solutions for
det(M − λI) = 0. All the coefﬁcients of the characteristic polynomial are polynomial expressions
w.r.t. the entries of M  hence they are also polynomial w.r.t. (c1 ···   cn) since each entry of M is
linear on (c1 ···   cn). Therefore  the coefﬁcients of the characteristic polynomial are continuously
dependent on (c1 ···   cn). Moreover  the root of a polynomial is continuously dependent on the
coefﬁcients of the polynomial [28]. Based on these dependencies  eig(M (c1 ···   cn)) are continu-
ously dependent on (c1 ···   cn)  and eig(M (c1 ···   cn)) are continuous w.r.t. f in the polynomial
space P n
d .

The following proposition illustrates that the relative interior of the convex cone of even degree
polynomials is not empty.
Proposition 3. For an even degree function space P n
d   such that
∀x ∈ Rn  the Hessian is strictly positive deﬁnite  i.e.  ∇2f (x) (cid:31) 0. Hence the relative interior of
Cn
d is not empty.

d   there exists a function f (x) ∈ P n

Proof. Let f (x) =(cid:80)
i +(cid:80)
∇2f (x) = diag(cid:0)(cid:2)d(d − 1)xd−2

i xd

d . It follows trivially that

i ∈ P n

i x2
1 + 2  d(d − 1)xd−2

2 + 2 ···   d(d − 1)xd−2

n + 2(cid:3)(cid:1) (cid:31) 0 ∀x.

d is P n

d

d ) = dim(P n
d ).

d and P n

d is identical.

d . This concludes the proof.

Given the above two propositions it follows that the dimensionality of Cn
Lemma 2. The dimension of the polynomial vector space is equal to the dimension of the convex
even degree polynomial cone having the same degree d and the same number of variables n  i.e. 
dim(Cn
Proof. According to Proposition 3  there exists a function f ∈ P n
d   with strictly positive deﬁnite
Hessian  i.e.  ∀x ∈ Rn  eig(∇2f (x)) > 0. Consider a polynomial basis {gi} of P n
d . Consider
the vector of eigenvalues E(ˆci) = eig(∇2(f (x) + ˆcigi)). According to Proposition 2  E(ˆci) is
continuous w.r.t. ˆci  and E(0) is an all-positive vector. According to the deﬁnition of continuity 
there exists an  > 0  such that E(ˆci) > 0  ∀ˆci ∈ {c : |c| < }. Hence  there exists a nonzero
constant ˆci such that the polynomial f + ˆcigi is also strictly convex. We can construct such a strictly
convex polynomial ∀gi. Therefore the polynomial set f + ˆcigi is linearly independent and hence a
basis of Cn
Lemma 3. The linear span of the basis of Cn
Proof. Suppose P n
{g1  g2 ··· gN} a basis of Cn
by {g1  g2 ··· gN}. We have {g1  g2 ···   gN   h} are N +1 linear independent vectors in P n
is in contradiction with P n
Theorem 1. ∀f ∈ P n
Proof. Let the basis of Cn
d be {g1  g2 ···   gN}. According to Lemma 3  there exist coefﬁcients
c1 ···   cN   such that f = c1g1 + c2g2 + ··· + cN gN . We can partition the coefﬁcients into
ci≥0 cigi and

d is also N-dimensional. Denote
d such that h cannot be linearly represented
d   which

d . Assume there exists h ∈ P n
d being N-dimensional.

cj <0 cjgj. Let h = (cid:80)

d is N-dimensional. According to Lemma 2  Cn

two sets  according to their sign  i.e.  f = (cid:80)
g = −(cid:80)
(cid:18) n + d − 1

ci≥0 cigi +(cid:80)
(cid:19)

cj <0 cjgj. We have f = h − g  while both h and g are convex polynomials.

According to Theorem 1 there exists a concave-convex decomposition given any polynomial  where
both the convex and concave parts are also polynomials with degree no greater than the original
polynomial. As long as we can ﬁnd
linearly independent convex polynomial basis
functions for any arbitrary polynomial function f ∈ P n
d   we obtain a valid decomposition by looking
at the sign of the coefﬁcients. It is however worth noting that the concave-convex decomposition
is not unique. In fact  there is an inﬁnite number of decompositions  trivially seen by adding and
subtracting an arbitrary convex polynomial to an existing decomposition.
Finding a convex basis is however not an easy task  mainly due to the difﬁculties on checking
convexity and the exponentially increasing dimension. Recently  Ahmadi et al. [1] proved that even
deciding on the convexity of quartic polynomials is NP-hard.

d

d   there exist convex polynomials h  g ∈ Cn

d such that f = h − g.

4

Algorithm 1 CCCP Inference on Continuous MRFs with Polynomial Potentials
Input: Initial estimation x0
∀r ﬁnd fr(xr) = fr vex(xr) + fr cave(xr) via Eq. (4) or via a polynomial basis (Theorem 1)
repeat

r fr vex(xr) + xT∇x((cid:80)
(cid:80)

r∈R fr cave(x(i)

r )) with L-BFGS.

solve x(i+1) = arg minx

until convergence

Output: x∗

2.4 Constructing a Concave-Convex Decomposition of Polynomials
In this section we derive an algorithm to construct the concave-convex decomposition of arbitrary
polynomials. Our algorithm ﬁrst constructs the convex basis of the polynomial vector space P n
d
before extracting a convex polynomial containing the target polynomial via a sum-of-squares (SOS)
program. More formally  given a non-convex polynomial f (x) we are interested in constructing
i cigi(x)  with gi(x)  i = {1  . . .   m}  the set of all con-
vex monomials with degree no grater than deg(f (x)). From this it follows that fvex = h(x) and
i cigi(x). In particular  we want a convex function h(x)  with coefﬁcients ci as small

a convex function h(x) = f (x) +(cid:80)
fcave = −(cid:80)

as possible:

(cid:88)

i

wT c s.t. ∇2f (x) +

min

c

ci∇2gi(x) (cid:31) 0 ∀x ∈ Rn 

(3)

with the objective function being a weighted sum of coefﬁcients. The weight vector w can encode
preferences in the minimization  e.g.  smaller coefﬁcients for larger degrees. This minimization
problem is NP-hard. If it was not  we could decide whether an arbitrary polynomial f (x) is convex
by solving such a program  which contradicts the NP-hardness result of [1]. Instead  we utilize a
tighter set of constraints  i.e.  sum-of-square constraints  which are easier to solve [14].
Deﬁnition 2. For an even degree polynomial f (x) ∈ P n
and only if there exist g1  . . .   gk ∈ P n
Thus  instead of solving the NP-hard program stated in Eq. (3)  we optimize:
ci∇2gi(x) ∈ SOS.

m such that f (x) =(cid:80)k

d   with d = 2m  f is an SOS polynomial if

wT c s.t. ∇2f (x) +

i=1 gi(x)2.

(4)

min

c

(cid:88)

i

The set of SOS Hessians is a subset of the positive deﬁnite Hessians [9]. Hence  every solution of
this problem can be considered a valid construction. Furthermore  the sum-of-squares optimization
in Eq. (4) can be formulated as an efﬁciently solvable semi-deﬁnite program (SDP) [10  9]. It is im-
portant to note that the gap between the SOS Hessians and the positive deﬁnite Hessians increases
as the degree of the polynomials grows. Hence using SOS constraints we might not ﬁnd a solution 
even though there exists one for the original program given in Eq. (3). In practice  SOS optimization
works well for monomials and low-degree polynomials. For pairwise graphical models with arbi-
trary degree polynomials  as well as for graphical models of order up to four with maximum fourth
order degree polynomials  we are guaranteed to ﬁnd a decomposition. This is due to the fact that
SOS convexity and polynomial convexity coincide (Theorem 5.2 in [2]). Most practical graphical
models are within this set. Known counter-examples [2] are typically found using speciﬁc tools.
We summarize our algorithm in Alg. 1. Given a graphical model with polynomial potentials with
degree at most d  we obtain a concave-convex decomposition by solving Eq. (4). This can be done
for the full polynomial or for each non-convex monomial. We then apply CCCP in order to perform
inference  where we solve a convex problem at each iteration. In particular  we employ L-BFGS 
mainly due to its super-linear convergence and its storage efﬁciency [12]. In each L-BFGS step  we
apply a line search scheme based on the Wolfe conditions [12].

2.5 Extensions
Dealing with very large graphs: Motivated by recent progress on accelerating graphical model
inference [7  21  20]  we can handle large-scale problems by employing dual decomposition and
using our approach to solve the sub-problems.
Non-polynomial cases: We have described our method in the context of graphical models with
polynomial potentials. It can be extended to the non-polynomial case if the involved functions have

5

Energy

RMSE (mm)
Time (second)

L-BFGS
10736.4

4.98
0.11

PCBP
6082.7
4.50
56.60

FusionMove ADMM-Poly

4317.7
2.95
0.12

3221.1
3.82
18.32

Ours
3062.8
3.07
8.70 (×2)

Table 1: 3D Reconstruction on 3 × 3 meshes with noise variance σ = 2.

(a) Synthetic meshes

(b) Cardboard meshes

(c) Shape-from-Shading

(d) Denoising

Figure 1: Average energy evolution curve for different applications.

bounded Hessians  since we can still construct the concave-convex decomposition. For instance  for
8 } − x2
the Lorentzian regularizer ρ(x) = log(1 + x2
8
is a valid concave-convex decomposition. We refer the reader to the supplementary material for
a detailed proof. Alternatively  we can approximate any continuous function with polynomials by
employing a Taylor expansion around the current iterate  and updating the solution via one CCCP
step within a trust region.

2 )  we note that ρ(x) = {log(1 + x2

2 ) + x2

3 Experimental Evaluation

We demonstrate the effectiveness of our approach using three different applications: non-rigid 3D
reconstruction  shape from shading and image denoising. We refer the reader to the supplementary
material for more ﬁgures as well as an additional toy experiment on a densely connected graph with
box constraints.

3.1 Non-rigid 3D Reconstruction
We tackle the problem of deformable surface reconstruction from a single image. Following [30] 
we parameterize the 3D shape via the depth of keypoints. Let x ∈ RN be the depth of N points.
We follow the locally isometric deformation assumption [20]  i.e.  the distance between neighboring
keypoints remains constant as the non-rigid surface deforms. The 3D reconstruction problem is then
formulated as

(cid:0)(cid:107)xiqi − xjqj(cid:107)2 − d2

(cid:1)2

 

i j

(cid:88)

min

x

(i j)∈N

(5)

where di j is the distance between keypoints (given as input)  N is the set of all neighboring pixels 
xi is the unknown depth of point i  qi = A−1(ui  vi  1)T is the line-of-sight of pixel i with A
denoting the known internal camera parameters. We consider a six-neighborhod system  i.e.  up 
down  left  right  upper-left and lower-right. Note that each pairwise potential is a four-degree non-
convex polynomial with two random variables. We can easily decompose it into 15 monomials 
and perform a concave-convex decomposition given the corresponding convex polynomials (see
supplementary material for an example).
We ﬁrst conduct reconstruction experiments on the 100 randomly generated 3 × 3 meshes of [20] 
where zero-mean Gaussian noise with standard deviation σ = 2 is added to each observed keypoint
coordinate. We compare our approach to Fusion Moves [30]  particle convex belief propagation
(PCBP) [17]  L-BFGS as well as dual decomposition with the alternating direction method of mul-
tipliers using a polynomial solver (ADMM-Poly) [20]. We employ three different metrics  energy at
convergence  running time and root mean square error (RMSE). For L-BFGS and our method  we
use a ﬂat mesh as initialization with two rotation angles (0  0  0) and (π/4  0  0). The convergence
criteria is an energy decrease of less than 10−5 or a maximum of 500 iterations is reached. As
shown in Table 1 our algorithm achieves lower energy  lower RMSE  and faster running time than
ADMM-Poly and PCBP. Furthermore  as shown in Fig. 1(a) the time for running our algorithm to
convergence is similar to a single iteration of ADMM-Poly  while we achieve much lower energy.

6

100102910111213141516Time (seconds)Log−scale EnergyReal Data 3D Reconstruction Energy Evolution ADMM−PolyOurs1001011010.51111.51212.513Time (seconds)Log−scale EnergySynthetic 3× 3 Mesh Energy Evolution ADMM−PolyOurs10−5100105−10−505Time (seconds)Log−scale EnergyShape−from−shading Energy Evolution Curve ADMM−PolyLBFGSOurs10110210314.8914.914.9114.9214.9314.9414.95Time (seconds)Log−scale EnergyFoE Energy Evolution GradDescLBFGSOursEnergy

RMSE (mm)
Time (second)

L-BFGS CLVM ADMM-Poly
736.98
4.16
0.3406

905.37
5.68
314.8

N/A
7.23
N/A

Ours
687.21
3.29
10.16

Table 2: 3D Reconstruction on Cardboard sequences.

Figure 2: 3D reconstruction results on Cardboard. Left to right: sample comparison  energy curve 
groundtruth  ADMM-Poly and our reconstruction.

Figure 3: Shape-from-Shading results on Penny. Left to right: energy curve  inferred shape  rendered
image with inferred shape  groundtruth image.
We next reconstruct the real-world 9×9 Cardboard sequence [20]. We compare with both ADMM-
Poly and L-BFGS in terms of energy  time and RMSE. We also compare with the constrained latent
variable model of [29]  in terms of RMSE. We cannot compare the energy value since the energy
function is different. Again  we use a ﬂat mesh as initialization. As shown in Table 2  our algorithm
outperforms all baselines. Furthermore  it is more than 20 times faster than ADMM-Poly  which is
the second best algorithm. Average energy as a function of time is shown in Fig. 1(b). We refer
the reader to Fig. 2 and the video in the supplementary material for a visual comparison between
ADMM-Poly and our method. From the ﬁrst subﬁgure we observe that our method achieves lower
energy for most samples. The second subﬁgure illustrate the fact that our approach monotonically
decreases the energy  as well as our method being much faster than ADMM-Poly.

functions.

3.2 Shape-from-Shading
Following [5  20]  we formulate the shape from shading problem with 3rd-order 4-th de-
Let xi j = (ui j  vi j  wi j)T be the 3D coordinates of
gree polynomial
each triangle vertex. Under the Lambertian model assumption 
the intensity of a trian-
gle r is represented as:
  where l = (l1  l2  l3)T is the direction of
the light  pr and qr are the x and y coordinates of normal vector nr = (pr  qr  1)T  
which is computed as pr = (vi j+1−vi j )(wi+1 j−wi j )−(vi+1 j−vi j )(wi j+1−wi j )
and pr =
(ui j+1−ui j )(vi+1 j−vi j )−(ui+1 j−ui j )(vi j+1−vi j )
(ui j+1−ui j )(wi+1 j−wi j )−(ui+1 j−ui j )(wi j+1−wi j )
(ui j+1−ui j )(vi+1 j−vi j )−(ui+1 j−ui j )(vi j+1−vi j )   respectively. Each clique r represents a trian-
gle  which is constructed by three neighboring points on the grid  i.e.  either (xi j  xi j+1  xi+1 j)
or (xi j  xi j−1  xi+1 j). Given the rendered image and lighting direction  shape from shading is
formulated as

Ir = l1pr+l2qr+l3
r +1

p2
r+q2

√

r − (l1pr + l2qr + l3)2(cid:1)2

.

(6)

(cid:0)(p2

(cid:88)

r∈R

min

w

r + q2

r + 1)I 2

We tested our algorithm on the Vase  Penny and Mozart datasets  where Vase and Penny are 128×128
images and Mozart is a 256 × 256 image with light direction l = (0  0  1)T . The energy evolution
curve  the inferred shape as well as the rendered and groud-truth images are illustrated in Fig. 3.
See the supplementary material for more ﬁgures on Penny and Mozart. Our algorithm achieves very
low energy  producing very accurate results in only 30 seconds. ADMM-Poly hardly runs on such
large-scale data due to the computational cost of the polynomial system solver (more than 2 hours

7

05101520500100015002000Sample IndexEnergyConvergent Energy for Samples OursADMM−Poly1001021046810121416Log−Energy Evolution Curve (4th Sample)Time (log scale)Log−Energy OursADMM−Poly−50050−50050100150200ADMM−Poly  Error: 4.9181 mm−50050−50050100150200Ours  Error: 2.1997 mm−50050−50050100150200GroundTruth010203005101520Time (sceonds)Log−EnergyLog−Energy evolution curve204060801001202040608010012001020Iteration: 98  Energy: 81.564  Time: 28.549Iteration: 98  RMSE: 0.012595  Time: 28.549GroundTruthEnergy
PSNR

Ours
29413
31.43
Time (sec)
384.5
Table 3: FoE Energy Minimization Results.

L-BFGS GradDesc
29547
30.96
189.5

29598
31.56
1122.5

Figure 4: FoE based image denoising results on Cameraman  σ = 15.

per iteration).
In order to compare with ADMM-Poly  we also conduct the shape from shading
experiment on a scaled 16 × 16 version of the Vase data. Both methods retrieve a shape that is very
close to the global optimum (0.00027 for ADMM-Poly and 0.00032 for our approach)  however 
our algorithm is over 500 times faster than ADMM-Poly (2250 seconds for ADMM-Poly and 13.29
seconds for our proposed method). The energy evolution curve on the 16 × 16 re-scaled image in
shown in Fig. 1(c).

Image Denoising

3.3
We formulate image denoising via minimizing the Fields-of-Experts (FoE) energy [18]. The
data term encodes the fact that the recovered image should be close to the noisy input  where
closeness is weighted by the noise level σ. Given a pre-learned linear ﬁlterbank of ‘experts’
{Ji}i=1 ... K  the image prior term encodes the fact that natural images are Gibbs distributed via
p(x) = 1

i xr)2)αi ). Thus we formulate denoising as

r∈R(cid:81)K

Z exp((cid:81)

i=1(1 + 1

2 (JT
σ2(cid:107)x − y(cid:107)2

λ

2 +

min

x

(cid:88)

K(cid:88)

r∈R

i=1

αi log(1 +

1
2

(JT

i xr)2) 

(7)

i Ji
8

to obtain the concave-convex decomposition log(1+ 1

(proof in the supplementary material). Therefore  we simply add an extra term γxT
2 (JT

where y is the noisy image input  x is the clean image estimation  r indexes 5 × 5 cliques and i is
the index for each FoE ﬁlter. Note that this energy function is not a polynomial function. However 
for each FoE model  the Hessian of the energy function log(1 + 1
i xr)2) is lower bounded by
− J T
i Ji
r xr with
8
γ > J T
i xr)2)+
r xr. We utilize a pre-trained 5× 5 ﬁlterbank with 24 ﬁlters  and conduct experiments
γxT
on the BM3D benchmark 1 with noise level σ = 15. In addition to the other baselines  we compare
to the original FoE inference algorithm  which essentially is a ﬁrst-order gradient descent method
with ﬁxed gradient step [18]. For L-BFGS  we set the maximum number of iterations to 10 000  to
make sure that the algorithm converges. As shown in Table 3 and Fig. 1(d)  our algorithm achieves
lower energy than L-BFGS and ﬁrst-order gradient descent. Furthermore  we see that lower energy
does not translate to higher PSNR  showing the limitation of FoE as an image prior.

i xr)2) = {log(1+ 1

r xr}− γxT

2 (JT

2 (JT

4 Conclusions
We investigated the properties of polynomials  and proved that every multivariate polynomial with
even degree can be decomposed into a sum of convex and concave polynomials with degree no
greater than the original one. Motivated by this property  we exploited the concave-convex proce-
dure to perform inference on continuous Markov random ﬁelds with polynomial potentials. Our al-
gorithm is especially ﬁt for solving inference problems on continuous graphical models  with a large
number of variables. Experiments on non-rigid reconstruction  shape-from-shading and image de-
noising validate the effectiveness of our approach. We plan to investigate continuous inference with
arbitrary differentiable functions  by making use of polynomial approximations as well as tighter
concave-convex decompositions.

1http://www.cs.tut.fi/˜foi/GCF-BM3D/

8

Clean ImageNoisy Image  PSNR: 24.5952GradDesc  PSNR: 31.0689Ours  PSNR: 30.9311L−BFGS  PSNR: 30.769505010015011.4111.4211.4311.4411.4511.46Time (seconds)Energy (log−scale)Energy evolution curve for FoE GradDescLBFGSOursReferences
[1] A. A. Ahmadi  A. Olshevsky  P. A. Parrilo  and J. N. Tsitsiklis. Np-hardness of deciding convexity of

quartic polynomials and related problems. Mathematical Programming  2013.

[2] A. A. Ahmadi and P. A. Parrilo. A complete characterization of the gap between convexity and sos-

convexity. SIAM J. on Optimization  2013.

[3] K. Batselier  P. Dreesen  and B. D. Moor. The geometry of multivariate polynomial division and elimina-

tion. SIAM Journal on Matrix Analysis and Applications  2013.

[4] Y. Boykov  O. Veksler  and R. Zabih. Fast approximate energy minimization via graph cuts. PAMI  2001.
[5] A. Ecker and A. D. Jepson. Polynomial shape from shading. In CVPR  2010.
[6] A. T. Ihler and D. A. McAllester. Particle belief propagation. In AISTATS  2009.
[7] N. Komodakis  N. Paragios  and G. Tziritas. Mrf energy minimization and beyond via dual decomposition.

PAMI  2011.

[8] Y. Koren  R. Bell  and C. Volinsky. Matrix factorization techniques for recommender systems. Computer 

2009.

[9] J. B. Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on

Optimization  2001.

[10] J. B. Lasserre. Convergent sdp-relaxations in polynomial optimization with sparsity. SIAM Journal on

Optimization  2006.

[11] V. Lempitsky  C. Rother  S. Roth  and A. Blake. Fusion moves for markov random ﬁeld optimization.

PAMI  2010.

[12] J. Nocedal and S. J. Wright. Numerical optimization 2ed. Springer-Verlag  2006.
[13] N. Noorshams and M. J. Wainwright. Belief propagation for continuous state spaces: Stochastic message-

passing with quantitative guarantees. JMLR  2013.

[14] A. Papachristodoulou  J. Anderson  G. Valmorbida  S. Prajna  P. Seiler  and P. Parrilo. Sostools version

3.00 sum of squares optimization toolbox for matlab. arXiv:1310.4716  2013.

[15] P. A. Parrilo. Structured semideﬁnite programs and semialgebraic geometry methods in robustness and

optimization. PhD thesis  Caltech  2000.

[16] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kauf-

mann  1988.

[17] J. Peng  T. Hazan  D. McAllester  and R. Urtasun. Convex max-product algorithms for continuous mrfs

with applications to protein folding. In ICML  2011.

[18] S. Roth and M. J. Black. Fields of experts. IJCV  2009.
[19] R. Salakhutdinov  S. Roweis  and Z. Ghahramani. On the convergence of bound optimization algorithms.

In UAI  2002.

[20] M. Salzmann. Continuous inference in graphical models with polynomial energies. In CVPR  2013.
[21] A. G. Schwing  T. Hazan  M. Pollefeys  and R. Urtasun. Distributed Message Passing for Large Scale

Graphical Models. In CVPR  2011.

[22] A. G. Schwing  T. Hazan  M. Pollefeys  and R. Urtasun. Efﬁcient Structured Prediction with Latent

Variables for General Graphical Models. In ICML  2012.

[23] A. Smola  S. Vishwanathan  and T. Hofmann. Kernel methods for missing variables. AISTATS  2005.
[24] L. Song  A. Gretton  D. Bickson  Y. Low  and C. Guestrin. Kernel belief propagation. In AISTATS  2011.
[25] B. Sriperumbudur and G. Lanckriet. On the convergence of the concave-convex procedure. In NIPS  ’09.
[26] B. Sriperumbudur  D. Torres  and G. Lanckriet. Sparse eigen methods by dc programming. In ICML  ’07.
[27] E. B. Sudderth  A. T. Ihler  M. Isard  W. T. Freeman  and A. S. Willsky. Nonparametric belief propagation.

Communications of the ACM  2010.

[28] D. J. Uherka and A. M. Sergott. On the continuous dependence of the roots of a polynomial on its

coefﬁcients. American Mathematical Monthly  1977.

[29] A. Varol  M. Salzmann  P. Fua  and R. Urtasun. A constrained latent variable model. In CVPR  2012.
[30] S. Vicente and L. Agapito. Soft inextensibility constraints for template-free non-rigid reconstruction. In

ECCV  2012.

[31] Y. Weiss and W. T. Freeman. Correctness of belief propagation in gaussian graphical models of arbitrary

topology. Neural computation  2001.

[32] C. N. Yu and T. Joachims. Learning structural svms with latent variables. In ICML  2009.
[33] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation  2003.

9

,Shenlong Wang
Alex Schwing
Raquel Urtasun
Leonidas Guibas
Qixing Huang
Zhenxiao Liang