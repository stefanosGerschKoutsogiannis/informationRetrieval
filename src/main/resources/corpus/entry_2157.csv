2019,FreeAnchor: Learning to Match Anchors for Visual Object Detection,Modern CNN-based object detectors assign anchors for ground-truth objects under the restriction of object-anchor Intersection-over-Unit (IoU). In this study  we propose a learning-to-match approach to break IoU restriction  allowing objects to match anchors in a flexible manner. Our approach  referred to as FreeAnchor  updates hand-crafted anchor assignment to "free" anchor matching by formulating detector training as a maximum likelihood estimation (MLE) procedure. FreeAnchor targets at learning features which best explain a class of objects in terms of both classification and localization. FreeAnchor is implemented by optimizing detection customized likelihood and can be fused with CNN-based detectors in a plug-and-play manner. Experiments on MS-COCO demonstrate that FreeAnchor consistently outperforms the counterparts with significant margins.,FreeAnchor: Learning to Match Anchors for Visual

Object Detection

Xiaosong Zhang1 

Fang Wan1  Chang Liu1  Rongrong Ji2  Qixiang Ye1 3∗

1University of Chinese Academy of Sciences  Beijing  China

2Xiamen University  Xiamen  China

3Peng Cheng Laboratory  Shenzhen  China

zhangxiaosong18@mails.ucas.ac.cn  qxye@ucas.ac.cn

Abstract

Modern CNN-based object detectors assign anchors for ground-truth objects under
the restriction of object-anchor Intersection-over-Unit (IoU). In this study  we
propose a learning-to-match approach to break IoU restriction  allowing objects
to match anchors in a ﬂexible manner. Our approach  referred to as FreeAnchor 
updates hand-crafted anchor assignment to “free" anchor matching by formulating
detector training as a maximum likelihood estimation (MLE) procedure. FreeAn-
chor targets at learning features which best explain a class of objects in terms of
both classiﬁcation and localization. FreeAnchor is implemented by optimizing
detection customized likelihood and can be fused with CNN-based detectors in
a plug-and-play manner. Experiments on COCO demonstrate that FreeAnchor
consistently outperforms the counterparts with signiﬁcant margins1.

1

Introduction

Over the past few years we have witnessed the success of convolution neural network (CNN) for
visual object detection [1  2  3  4  5  6  7]. To represent objects with various appearance  aspect ratios 
and spatial layouts with limited convolution features  most CNN-based detectors leverage anchor
boxes at multiple scales and aspect ratios as reference points for object localization [3  4  5  6  7]. By
assigning each object to a single or multiple anchors  features can be determined and two fundamental
procedures  classiﬁcation and localization (i.e.  bounding box regression)  are carried out.
Anchor-based detectors leverage spatial alignment  i.e.  Intersection over Unit (IoU) between objects
and anchors  as the criterion for anchor assignment. Each assigned anchor independently supervises
network learning for object prediction  based upon the intuition that the anchors aligned with object
bounding boxes are most appropriate for object classiﬁcation and localization. However  we argue
that such intuition is implausible and the hand-crafted IoU criterion is not the best choice.
On the one hand  for objects of acentric features  e.g.  slender objects  the most representative
features are not close to object centers. A spatially aligned anchor might correspond to fewer
representative features  which deteriorate classiﬁcation and localization capabilities. On the other
hand  it is infeasible to match proper anchors/features for objects using IoU when multiple objects
come together.
It is hard to design a generic rule which can optimally match anchors/features with objects of various
geometric layouts. The widely used hand-crafted assignment could fail when facing acentric  slender 
and/or crowded objects. A learning-based approach requires to be explored to solve this problem in a
systematic way  which is the focus of this study.

*Corresponding Author.
1Code is available at https://github.com/zhangxiaosong18/FreeAnchor

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

We propose a learning-to-match approach for object detection  and target at discarding hand-crafted
anchor assignment while optimizing learning procedures of visual object detection from three speciﬁc
aspects. First  to achieve a high recall rate  the detector is required to guarantee that for each
object at least one anchor’s prediction is close to the ground-truth. Second  in order to achieve high
detection precision  the detector needs to classify anchors with poor localization (large bounding box
regression error) into background. Third  the predictions of anchors should be compatible with the
non-maximum suppression (NMS) procedure  i.e.  the higher the classiﬁcation score is  the more
accurate the localization is. Otherwise  an anchor with accurate localization but low classiﬁcation
score could be suppressed when using the NMS process.
To fulﬁll these objectives  we formulate object-anchor matching as a maximum likelihood estimation
(MLE) procedure [8  9]  which selects the most representative anchor from a “bag" of anchors for
each object. We deﬁne the likelihood probability of each anchor bag as the largest anchor conﬁdence
within it. Maximizing the likelihood probability guarantees that there exists at least one anchor 
which has high conﬁdence for both object classiﬁcation and localization. Meanwhile  most anchors 
which have large classiﬁcation or localization error  are classiﬁed as background. During training 
the likelihood probability is converted into a loss function  which then drives CNN-based detector
training and object-anchor matching.
The contributions of this work are concluded as follows:

• We formulate detector training as an MLE procedure and update hand-crafted an-
chor assignment to free anchor matching. The proposed approach breaks the IoU restric-
tion  allowing objects to ﬂexibly select anchors under the principle of maximum likelihood.

• We deﬁne a detection customized likelihood  and implement joint optimization of ob-
ject classiﬁcation and localization in an end-to-end mechanism. Maximizing the likeli-
hood drives network learning to match optimal anchors and guarantees the comparability of
with the NMS procedure.

2 Related Work

Object detection requires generating a set of bounding boxes along with their classiﬁcation labels
associated with objects in an image. However  it is not trivial for a CNN-based detector to directly
predict an order-less set of arbitrary cardinals. One widely-used workaround is to introduce anchors 
which employs a divide-and-conquer process to match objects with features. This approach has
been successfully demonstrated in Faster R-CNN [3]  SSD [5]  FPN [6]  RetinaNet [7]  DSSD [10]
and YOLOv2 [11]. In these detectors  dense anchors need to be conﬁgured over convolutional
feature maps so that features extracted from anchors can match object windows and the bounding
box regression can be well initialized. Anchors are then assigned to objects or backgrounds by
thresholding their IoUs with ground-truth bounding boxes [3].
Although effective  these approaches are restricted by heuristics that spatially aligned anchors are
compatible for both object classiﬁcation and localization. For objects of acentric features  however 
the detector could miss the best anchors and features.
To break this limitation imposed by pre-assigned anchors  recent anchor-free approaches employ
pixel-level supervision [12] and center-ness bounding box regression [13]. CornerNet [14] and
CenterNet [15] replace bounding box supervision with key-point supervision. MetaAnchor [16]
approach learns to produce anchors from the arbitrary customized prior boxes with a sub-network.
GuidedAnchoring [17] leverages semantic features to guide the prediction of anchors while replacing
dense anchors with predicted anchors. IoU-Net [18] incorporates IoU-guided NMS  which helps
eliminating the suppression failure caused by the misleading classiﬁcation conﬁdences.
Existing approaches have taken a step towards learnable anchor customization. Nevertheless  to the
best of our knowledge  there still lacks a systematic approach to model the correspondence between
anchors and objects during detector training  which inhibits the optimization of feature selection and
feature learning.

2

Figure 1: Comparison of hand-crafted anchor assignment (top) and FreeAnchor (bottom). FreeAnchor
allows each object to ﬂexibly match the best anchor from a “bag" of anchors during detector training.

3 The Proposed Approach

To model the correspondence between objects and anchors  we propose to formulate detector training
as an MLE procedure. We then deﬁne the detection customized likelihood  which simultaneously
facilitates object classiﬁcation and localization. During detector training  we convert detection
customized likelihood into detection customized loss and jointly optimizing object classiﬁcation 
object localization  and object-anchor matching in an end-to-end mechanism.

3.1 Detector Training as Maximum Likelihood Estimation

i

i

j ∈ Rk after the Sigmoid activation  and a location prediction aloc

Let’s begin with a CNN-based one-stage detector [7]. Given an input image I  the ground-truth
annotations are denoted as B  where a ground-truth box bi ∈ B is made up of a class label bcls
and a
. During the forward propagation procedure of the network  each anchor aj ∈ A obtains a
location bloc
j = {x  y  w  h}
class prediction acls
after the bounding box regression. k denotes the number of object classes.
During training  hand-crafted criterion based on IoU is used to assign anchors for objects  Fig. 1 
and a matrix Cij ∈ {0  1} is deﬁned to indicate whether object bi matches anchor aj or not. When
the IoU of bi and aj is greater than a threshold  bi matches aj and Cij = 1. Otherwise  Cij = 0.
i Cij ∈ {0  1} ∀aj ∈ A. By deﬁning A+ ⊆ A as {aj |(cid:80)
Specially  when multiple objects’ IoU are greater than this threshold  the object of the largest IoU
will successfully match this anchor  which guarantees that each anchor is matched by a single object
i Cij = 1} and A− ⊆ A
i Cij = 0}  the loss function L(θ) of the detector is written as follows:
(cid:88)
(cid:88)
Lbg
L(θ) =
j (θ) 

at most  i.e. (cid:80)
as {aj |(cid:80)

CijLloc

CijLcls

(cid:88)

(cid:88)

(cid:88)

ij (θ) + β

ij (θ) +

(1)

aj∈A+

bi∈B

aj∈A+

bi∈B

where θ denotes the network parameters to be learned. Lcls
ij (θ) =
 (cid:126)0  θ) respectively denote the Binary Cross
SmoothL1(aloc
Entropy loss (BCE) for classiﬁcation and the SmoothL1 loss deﬁned for localization [2]. β is a
regularization factor and “bg" indicates “background".

j (θ) = BCE(acls
j

  θ) and Lbg

  θ)  Lloc

  bloc

  bcls

j

i

i

aj∈A−
ij (θ) = BCE(acls
j

3

From the MLE perspective  the training loss L(θ) is converted into a likelihood probability  as follows:

P(θ) = e−L(θ)

(cid:89)
(cid:89)

aj∈A+

=

=

(cid:0)(cid:88)
(cid:0)(cid:88)

bi∈B

bi∈B

ij (θ)(cid:1) (cid:89)
ij (θ)(cid:1) (cid:89)

aj∈A+

(cid:0)(cid:88)
(cid:0)(cid:88)

bi∈B

aj∈A+

bi∈B

Cije−Lcls

CijP cls

Cije−βLloc

ij (θ)(cid:1) (cid:89)
ij (θ)(cid:1) (cid:89)

CijP loc

aj∈A−
P bg
j (θ) 

aj∈A−

e−Lbg

j (θ)

(2)

j (θ) denote classiﬁcation conﬁdence and P loc

where P cls
ij (θ) denotes localization conﬁ-
dence. Minimizing the loss function L(θ) deﬁned in Eq. 1 is equal to maximizing the likelihood
probability P(θ) deﬁned in Eq. 2.
Eq. 2 strictly considers the optimization of classiﬁcation and localization of anchors from the MLE
perspective. However  it unfortunately ignores how to learn the matching matrix Cij. Existing
CNN-based detectors [3  5  6  7  11] solve this problem by empirically assigning anchors using the
IoU criterion  Fig. 1  but ignoring the optimization of object-anchor matching.

aj∈A+
ij (θ) and P bg

3.2 Detection Customized Likelihood

To achieve the optimization of object-anchor matching  we extend the CNN-based detection frame-
work by introducing detection customized likelihood. Such likelihood intends to incorporate the
objectives of recall and precision while guaranteeing the compatibility with NMS.
To implement the likelihood  we ﬁrst construct a bag of candidate anchors for each object bi by
selecting (n) top-ranked anchors Ai ⊂ A in terms of their IoU with the object. We then learns to
match the best anchor while maximizing the detection customized likelihood.
To optimize the recall rate  for each object bi ∈ B we requires to guarantee that there exists at least
one anchor aj ∈ Ai  whose prediction (acls
) is close to the ground-truth. The objective
function can be derived from the ﬁrst two terms of Eq. 2  as follows:

and aloc

j

Precall(θ) =

j

max
aj∈Ai

ij (θ)P loc

(cid:0)P cls

(cid:89)
ij (θ)(cid:1).
(cid:89)
(cid:0)1 − P{aj ∈ A−}(1 − P bg

i

j (θ))(cid:1) 

To achieve increased detection precision  detectors need to classify the anchors of poor localization
into the background class. This is fulﬁlled by optimizing the following objective function:

Pprecision(θ) =

j

(4)
where P{aj ∈ A−} = 1 − maxi P{aj → bi} is the probability that aj misses all objects and
P{aj → bi} denotes the probability that anchor aj correctly predicts object bi.
To be compatible with the NMS procedure  P{aj → bi} should have the following three properties:
(1) P{aj → bi} is a monotonically increasing function of the IoU between aloc
ij . (2)
is smaller than a threshold t  P{aj → bi} is close to 0. (3) For an object bi  there
When IoU loc
ij
exists one and only one aj satisfying P{aj → bi} = 1. These properties can be satisﬁed with a
saturated linear function  as

and bi  IoU loc

j

(3)

0 
x − t1
t2 − t1
1 

 

x ≤ t1
t1 < x < t2 
x ≥ t2

Saturated linear(x  t1  t2) =

which is shown in Fig. 2  and we have P{aj → bi} = Saturated linear(cid:0)IoU loc
(cid:0)1 − P{aj ∈ A−}(1 − P bg

P(cid:48)(θ) = Precall(θ) × Pprecision(θ)

Implementing the deﬁnitions provided above  the detection customized likelihood is deﬁned as
follows:

ij   t  maxj(IoU loc

ij )(cid:1).

j (θ))(cid:1) 

ij (θ)P loc

(cid:89)

(P cls

(5)

=

max
aj∈Ai

i


ij (θ)) ×(cid:89)

j

4

Figure 2: Saturated linear function.

Figure 3: Mean-max function.

which incorporates the objectives of recall  precision and compatibility with NMS. By optimizing
this likelihood  we simultaneously maximize the probability of recall Precall(θ) and precision
Pprecision(θ) and then achieve free object-anchor matching during detector training.

3.3 Anchor Matching Mechanism

To implement this learning-to-match approach in a CNN-based detector  the detection customized
likelihood deﬁned by Eq. 5 is converted to a detection customized loss function  as follows:

L(cid:48)(θ) = − log P(cid:48)(θ)

= −(cid:88)

log(cid:0) max

aj∈Ai

i

ij (θ))(cid:1) −(cid:88)

log(cid:0)1 − P{aj ∈ A−}(1 − P bg

j (θ))(cid:1) 

(P cls

ij (θ)P loc

(6)

where the max function is used to select the best anchor for each object. During training  a single
anchor is selected from a bag of anchors Ai  which is then used to update the network parameter θ.
At early training epochs  the conﬁdence of all anchors is small for randomly initialized network
parameters. The anchor with the highest conﬁdence is not suitable for detector training. We therefore
propose using the Mean-max function  deﬁned as:

j

(cid:80)
(cid:80)

Mean-max(X) =

xj∈X

xj∈X

xj
1 − xj
1 − xj

1

 

which is used to select anchors. When training is insufﬁcient  the Mean-max function  as shown
in Fig. 3  will be close to the mean function  which means almost all anchors in bag are used for
training. Along with training  the conﬁdence of some anchors increases and the Mean-max function
moves closer to the max function. When sufﬁcient training has taken place  a single best anchor can
be selected from a bag of anchors to match each object.
Replacing the max function in Eq. 6 with Mean-max  adding balance factor w1 w2  and applying
focal loss [7] to the second term of Eq. 6  the detection customized loss function of an FreeAnchor
detector is concluded  as follows:

(cid:88)

log(cid:0)Mean-max(Xi)(cid:1) + w2

(cid:88)

F L(cid:0)P{aj ∈ A−}(1 − P bg

j (θ))(cid:1) 

L(cid:48)(cid:48)(θ) = −w1

(7)

i

j

ij (θ)P loc
where Xi = {P cls
By using the parameters α and γ from focal loss [7]  we set w1 = α||B||
F L(x) = −xγ log (1 − x).
With the detection customized loss deﬁned  we implement the training procedure as Algorithm 1.

ij (θ)| aj ∈ Ai} is a likelihood set corresponding to the anchor bag Ai.
n||B||  and

  w2 = 1−α

5

0.00.20.40.60.81.0x0.00.20.40.60.81.0Saturated linear(x t1 t2)t1t2t1=0.5  t2=0.90.00.20.40.60.81.0x10.00.20.40.60.81.0x2maxmean0.2000.3500.5000.6500.800Figure 4: Comparison of learning-to-match anchors (left) with hand-crafted anchor assignment
(right) for the “laptop” object. Red dots denote anchor centers. Darker (redder) dots denote higher
conﬁdence to be matched. For clarity  we select 16 anchors of aspect-ratio 1:1 from all 50 anchors
for illustration. (Best viewed in color)

Algorithm 1 Detector training with FreeAnchor.
Input:

I: Input image.
B: A set of ground-truth bounding boxes bi.
A: A set of anchors aj in image.
n: Hyper-parameter about anchor bag size .
Output: θ: Detection network parameters.
1: θ ← initialize network parameters.
2: for i=1:MaxIter do
3:

Forward propagation:

4:

5:

6:

7: end for
8: return θ

j

for each anchor aj ∈ A.

j

Loss calculation:

Anchor bag construction:

and location aloc

Predict class acls
Ai ← Select n top-ranked anchors aj in terms of their IoU with bi.
Calculate L(cid:48)(cid:48)(θ) with Eq. 7.
θt+1 = θt − λ∇θtL(cid:48)(cid:48)(θt) using a stochastic gradient descent algorithm.

Backward propagation:

4 Experiments

In this section  we present the implementation of an FreeAnchor detector to appraise the effect
of the proposed learning-to-match approach. We also compare the FreeAnchor detector with the
counterpart and the state-of-the-art approaches. Experiments were carried out on COCO 2017[19] 
which contains ∼118k images for training  5k for validation (val) and ∼20k for testing without
provided annotations (test-dev). Detectors were trained on COCO training set  and evaluated on the
val set. Final results were reported on the test-dev set.

4.1

Implementation Details

FreeAnchor is implemented upon a state-of-the-art one-stage detector  RetinaNet [7]  using
ResNet [20] and ResNeXt [21] as the backbone networks. By simply replacing the loss deﬁned in
RetinaNet with the proposed detection customized loss  Eq. 7  we updated the RetinaNet detector
to an FreeAnchor detector. For the last convolutional layer of the classiﬁcation subnet  we set the
bias initialization to b = − log ((1 − ρ)/ρ) with ρ = 0.02. Training used synchronized SGD over 8
Tesla V100 GPUs with a total of 16 images per mini-batch (2 images per GPU). Unless otherwise
speciﬁed  all models were trained for 90k iterations with an initial learning rate of 0.01  which is then
divided by 10 at 60k and again at 80k iterations.

4.2 Model Effect

Learning-to-match: The proposed learning-to-match approach can select proper anchors to represent
the object of interest  Fig. 4. As analyzed in the introduction section  hand-crafted anchor assignment
often fails in two situations: Firstly  slender objects with acentric features; and secondly when multiple

6

laptoplaptoplaptoplaptoplaptopanchor bag initilizaion10k iterations50k iterations90k (the last) iterationshand-crafted assignmentFigure 5: Performance comparison on square and slender
objects.

Figure 6: Performance comparison
on object crowdedness.

objects are provided in crowded scenes. FreeAnchor effectively alleviated these two problems. Over
slender objects  FreeAnchor signiﬁcantly outperformed the RetinaNet baseline  Fig. 5. For other
square objects FreeAnchor reported comparable performance with RetinaNet. The reason for this
is that the learning-to-match procedure drives network activating at least one anchor within each
object’s anchor bag in order to predict correct category and location. The anchor is not necessary
spatially aligned with the object  but has the most representative features for object classiﬁcation and
localization.
We further compared the performance of RetinaNet and FreeAnchor in scenarios of various crowd-
edness  Fig. 6. As the number of objects in each image increased  the FreeAnchor’s advantage
over RetinaNet became more and more obvious. This demonstrated that our approach  with the
learning-to-match mechanism  can select more suitable anchors to objects in crowded scenes.
Compatibility with NMS: To assess the compatibility of anchors’ predictions with NMS  we deﬁned
the NMS Recall (NRτ ) as the ratio of the recall rates after and before NMS for a given IoU thresholds
τ. Following the COCO-style AP metric [19]  NR was deﬁned as the averaged NRτ when τ changes
from 0.50 to 0.90 with an interval of 0.05  Table 1. We compared RetinaNet and FreeAnchor in terms
of their NRτ . It can be seen that FreeAnchor reported higher NRτ   which means higher compatibility
with NMS. This validated that the detection customized likelihood  deﬁned in Section 3.2  can drive
joint optimization of classiﬁcation and localization.

Table 1: Comparison of NMS recall (%) on COCO val set.

backbone

detector

ResNet-50

RetinaNet [7]

FreeAnchor (ours)

NR NR50 NR60 NR70 NR80 NR90
51.3
81.8
83.8
53.1

87.0
89.5

71.8
74.3

98.3
99.2

95.7
97.5

4.3 Parameter Setting

Anchor bag size n: We evaluated anchor bag sizes in {40  50  60  100} and observed that the bag
size 50 reported the best performance.
Background IoU threshold t: A threshold was used in P{aj → bi} during training. We tried
background IoU thresholds in {0.5  0.6  0.7} and validated that 0.6 worked best.
Focal loss parameter: FreeAnchor introduced a bag of anchors to replace independent anchors
and therefore faced more serious sample imbalance. To handle the imbalance  we experimented the
parameters in Focal Loss [7] as α in {0.25  0.5  0.75} and γ in {1.5   2.0  2.5}  and set α = 0.5 and
γ = 2.0.
Loss regularization factor β: The regularization factor β in Eq. 1  which balances the loss of
classiﬁcation and localization  was experimentally validated to be 0.75.

7

clocktraffic lightsports balltoothbrushskiscouchtieobjects category020406080100AP (%)49.127.644.810.415.636.728.248.727.344.814.219.641.433.0square objectsslender objectsRetinaNetFreeAnchor[0  10][11  20][21  30][31  inf)number of objects per image2530354045AP (%)RetinaNetFreeAnchor4.4 Detection Performance

In Table 2  FreeAnchor was compared with the RetinaNet baseline. FreeAnchor consistently
improved the AP up to ∼3.0%  which is a signiﬁcant margin in terms of the challenging object
detection task. Note that the performance gain was achieved with negligible cost of training time.

Table 2: Performance comparison of FreeAnchor and RetinaNet (baseline).

Backbone

Detector

ResNet-50

ResNet-101

RetinaNet [7]

FreeAnchor (ours)

RetinaNet [7]

FreeAnchor (ours)

Training

time
5.02h
5.27h
6.96h
7.26h

AP AP50 AP75 APS APM APL

35.7
38.7
37.8
40.9

55.0
57.3
57.5
59.9

38.5
41.6
40.8
43.8

18.9
20.2
20.2
21.7

38.9
41.3
41.1
43.8

46.3
50.1
49.2
53.0

FreeAnchor was compared with state-of-the-art one-stage detectors in Table 3  used scale jitter and
2× longer training than the same model from Table 2. It outperformed the baseline RetinaNet [7]
and the anchor-free approaches including FoveaBox [22]  FSAF [23]  FCOS [13] and CornerNet [14].
With a litter ResNeXt-64x4d-101 backbone network and fewer training iterations  FreeAnchor was
comparable with CenterNet in AP (44.9% vs. 44.9%) and reported higher AP50  which is a more
widely used metric in many applications.
“FreeAnchor*" refers to extending the scale range from [640  800] to [480  960]  achieving 46.0%
AP. “FreeAnchor**" further utilized multi-scale testing over scales {480  640  800  960  1120  1280} 
and increased AP up to 47.3%  which outperformed most state-of-the-art detectors with the same
backbone network.

Table 3: Performance comparison with state-of-the-art one-stage detectors.

Backbone
Detector
ResNet-101
RetinaNet [7]
ResNet-101
FoveaBox [22]
ResNet-101
FSAF [23]
ResNet-101
FCOS [13]
ResNeXt-101
RetinaNet [7]
ResNeXt-101
FoveaBox [22]
ResNeXt-101
FSAF [23]
FCOS [13]
ResNeXt-101
CornerNet [14] Hourglass-104
CenterNet [15] Hourglass-104
FreeAnchor
FreeAnchor
FreeAnchor*
FreeAnchor**

ResNet-101
ResNeXt-101
ResNeXt-101
ResNeXt-101

Iter. AP AP50 AP75 APS APM APL
50.2
135k
54.5
135k
135k
51.3
51.6
180k
51.2
135k
55.6
135k
135k
52.7
53.3
180k
54.3
500k
57.4
480k
180k
54.8
55.9
180k
57.7
180k
59.0
180k

59.1
60.1
61.5
60.7
61.1
61.9
63.8
62.8
56.4
62.4
62.2
64.3
65.6
66.3

39.1
40.6
40.9
41.5
40.8
42.1
42.9
43.2
40.6
44.9
43.1
44.9
46.0
47.3

42.3
43.5
44.0
45.0
44.1
45.2
46.3
46.6
43.2
48.1
46.4
48.5
49.8
51.5

21.8
23.3
24.0
24.4
24.1
24.9
26.6
26.5
19.1
25.6
24.5
26.8
27.8
30.6

42.7
45.2
44.2
44.8
44.2
46.8
46.2
46.2
42.8
47.4
46.1
48.3
49.5
50.4

5 Conclusion

We proposed an elegant and effective approach  referred to as FreeAnchor  for visual object detection.
FreeAnchor updated the hand-crafted anchor assignment to “free" object-anchor correspondence
by formulating detector training as a maximum likelihood estimation (MLE) procedure. With
FreeAnchor implemented  we signiﬁcantly improved the performance of object detection  in striking
contrast with the baseline detector. The underlying reality is that the MLE procedure with the
detection customized likelihood facilitates learning convolutional features that best explain a class of
objects. This provides a fresh insight for the visual object detection problem.
Acnkowledgement. This work was supported in part by the NSFC under Grant 61836012  61671427 
and 61771447 and Post Doctoral Innovative Talent Support Program under Grant 119103S304.

8

References
[1] Ross B. Girshick  Jeff Donahue  Trevor Darrell  and Jitendra Malik. Rich feature hierarchies for accurate

object detection and semantic segmentation. In IEEE CVPR  pages 580–587  2014.

[2] Ross B. Girshick. Fast R-CNN. In IEEE ICCV  pages 1440–1448  2015.

[3] Shaoqing Ren  Kaiming He  Ross B. Girshick  and Jian Sun. Faster R-CNN: towards real-time object

detection with region proposal networks. In NIPS  pages 91–99  2015.

[4] Joseph Redmon  Santosh Kumar Divvala  Ross B. Girshick  and Ali Farhadi. You only look once: Uniﬁed 

real-time object detection. In IEEE CVPR  pages 779–788  2016.

[5] Wei Liu  Dragomir Anguelov  Dumitru Erhan  Christian Szegedy  Scott E. Reed  Cheng-Yang Fu  and

Alexander C. Berg. SSD: single shot multibox detector. In ECCV  pages 21–37  2016.

[6] Tsung-Yi Lin  Piotr Dollár  Ross B. Girshick  Kaiming He  Bharath Hariharan  and Serge J. Belongie.

Feature pyramid networks for object detection. In IEEE CVPR  pages 936–944  2017.

[7] Tsung-Yi Lin  Priya Goyal  Ross B. Girshick  Kaiming He  and Piotr Dollár. Focal loss for dense object

detection. In IEEE ICCV  pages 2999–3007  2017.

[8] Oded Maron and Tomás Lozano-Pérez. A framework for multiple-instance learning. In NIPS  pages

570–576  1997.

[9] Ronald A Fisher. On the mathematical foundations of theoretical statistics. Philosophical Transactions
of the Royal Society of London. Series A  Containing Papers of a Mathematical or Physical Character 
222(594-604):309–368  1922.

[10] Cheng-Yang Fu  Wei Liu  Ananth Ranga  Ambrish Tyagi  and Alexander C Berg. Dssd: Deconvolutional

single shot detector. arXiv:1701.06659  2017.

[11] Joseph Redmon and Ali Farhadi. YOLO9000: better  faster  stronger. In IEEE CVPR  pages 6517–6525 

2017.

[12] Xinyu Zhou  Cong Yao  He Wen  Yuzhi Wang  Shuchang Zhou  Weiran He  and Jiajun Liang. EAST: an

efﬁcient and accurate scene text detector. In IEEE CVPR  pages 2642–2651  2017.

[13] Zhi Tian  Chunhua Shen  Hao Chen  and Tong He. Fcos: Fully convolutional one-stage object detection.

arXiv:1904.01355  2019.

[14] Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints. In ECCV  pages 765–781  2018.

[15] Kaiwen Duan  Song Bai  Lingxi Xie  Honggang Qi  Qingming Huang  and Qi Tian. Centernet: Object

detection with keypoint triplets. In IEEE CVPR  2019.

[16] Tong Yang  Xiangyu Zhang  Zeming Li  Wenqiang Zhang  and Jian Sun. Metaanchor: Learning to detect

objects with customized anchors. In NIPS  pages 320–330  2018.

[17] Jiaqi Wang  Kai Chen  Shuo Yang  Chen Change Loy  and Dahua Lin. Region proposal by guided

anchoring. In IEEE CVPR  pages 2965–2974  2019.

[18] Borui Jiang  Ruixuan Luo  Jiayuan Mao  Tete Xiao  and Yuning Jiang. Acquisition of localization

conﬁdence for accurate object detection. In ECCV  pages 784–799  2018.

[19] Tsung-Yi Lin  Michael Maire  Serge J. Belongie  Lubomir D. Bourdev  Ross B. Girshick  James Hays 
Pietro Perona  Deva Ramanan  Piotr Dollár  and C. Lawrence Zitnick. Microsoft coco: Common objects in
context. In ECCV  pages 740–755  2014.

[20] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

In IEEE CVPR  pages 770–778  2016.

[21] Saining Xie  Ross Girshick  Piotr Dollar  Zhuowen Tu  and Kaiming He. Aggregated residual transforma-

tions for deep neural networks. In IEEE CVPR  pages 1492–1500  2017.

[22] Tao Kong  Fuchun Sun  Huaping Liu  Yuning Jiang  and Jianbo Shi. Foveabox: Beyond anchor-based

object detector. arXiv:1904.03797  2019.

[23] Chenchen Zhu  Yihui He  and Marios Savvides. Feature selective anchor-free module for single-shot object

detection. In IEEE CVPR  pages 840–849  2019.

9

,Xiaosong Zhang
Fang Wan
Chang Liu
Rongrong Ji
Qixiang Ye