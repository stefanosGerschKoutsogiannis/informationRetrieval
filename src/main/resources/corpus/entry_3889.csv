2009,On Invariance in Hierarchical Models,A goal of central importance in the study of hierarchical models for object recognition -- and indeed the visual cortex -- is that of understanding quantitatively the trade-off between invariance and selectivity  and how invariance and discrimination properties contribute towards providing an improved representation useful for learning from data. In this work we provide a general group-theoretic framework for characterizing and understanding invariance in a family of hierarchical models.  We show that by taking an algebraic perspective  one can provide a concise set of conditions which must be met to establish invariance  as well as a constructive prescription for meeting those conditions. Analyses in specific cases of particular relevance to computer vision and text processing are given  yielding insight into how and when invariance can be achieved. We find that the minimal sets of transformations intrinsic to the hierarchical model needed to support a particular invariance can be clearly described  thereby encouraging efficient computational implementations.,On Invariance in Hierarchical Models

Jake Bouvrie  Lorenzo Rosasco  and Tomaso Poggio

Center for Biological and Computational Learning

Massachusetts Institute of Technology

{jvb lrosasco}@mit.edu  tp@ai.mit.edu

Cambridge  MA USA

Abstract

A goal of central importance in the study of hierarchical models for object recogni-
tion – and indeed the mammalian visual cortex – is that of understanding quantita-
tively the trade-off between invariance and selectivity  and how invariance and dis-
crimination properties contribute towards providing an improved representation
useful for learning from data. In this work we provide a general group-theoretic
framework for characterizing and understanding invariance in a family of hierar-
chical models. We show that by taking an algebraic perspective  one can provide
a concise set of conditions which must be met to establish invariance  as well
as a constructive prescription for meeting those conditions. Analyses in speciﬁc
cases of particular relevance to computer vision and text processing are given 
yielding insight into how and when invariance can be achieved. We ﬁnd that the
minimal intrinsic properties of a hierarchical model needed to support a particular
invariance can be clearly described  thereby encouraging efﬁcient computational
implementations.

Introduction

1
Several models of object recognition drawing inspiration from visual cortex have been developed
over the past few decades [3  8  6  12  10  9  7]  and have enjoyed substantial empirical success. A
central theme found in this family of models is the use of Hubel and Wiesel’s simple and complex
cell ideas [5]. In the primary visual cortex  simple units compute features by looking for the occur-
rence of a preferred stimulus in a region of the input (“receptive ﬁeld”). Translation invariance is
then explicitly built into the processing pathway by way of complex units which pool locally over
simple units. The alternating simple-complex ﬁltering/pooling process is repeated  building increas-
ingly invariant representations which are simultaneously selective for increasingly complex stimuli.
In a computer implementation  the ﬁnal representation can then be presented to a supervised learning
algorithm.
Following the ﬂow of processing in a hierarchy from the bottom upwards  the layerwise representa-
tions gain invariance while simultaneously becoming selective for more complex patterns. A goal of
central importance in the study of such hierarchical architectures and the visual cortex alike is that of
understanding quantitatively this invariance-selectivity tradeoff  and how invariance and selectivity
contribute towards providing an improved representation useful for learning from examples. In this
paper  we focus on hierarchical models incorporating an explicit attempt to impose transformation
invariance  and do not directly address the case of deep layered models without local transformation
or pooling operations (e.g. [4]).
In a recent effort  Smale et al. [11] have established a framework which makes possible a more pre-
cise characterization of the operation of hierarchical models via the study of invariance and discrim-
ination properties. However  Smale et al. study invariance in an implicit  rather than constructive 
fashion. In their work  two cases are studied: invariance with respect to image rotations and string
reversals  and the analysis is tailored to the particular setting. In this paper  we reinterpret and ex-
tend the invariance analysis of Smale et al. using a group-theoretic language towards clarifying and
unifying the general properties necessary for invariance in a family of hierarchical models. We show
that by systematically applying algebraic tools  one can provide a concise set of conditions which
must be met to establish invariance  as well as a constructive prescription for meeting those condi-
tions. We additionally ﬁnd that when one imposes the mild requirement that the transformations of
interest have group structure  a broad class of hierarchical models can only be invariant to orthog-

1

Invariance of a Hierarchical Feature Map

onal transformations. This result suggests that common architectures found in the literature might
need to be rethought and modiﬁed so as to allow for broader invariance possibilities. Finally  we
show that our framework automatically points the way to efﬁcient computational implementations
of invariant models.
The paper is organized as follows. We ﬁrst recall important deﬁnitions from Smale et al. Next  we
extend the machinery of Smale et al. to a more general setting allowing for general pooling func-
tions  and give a proof for invariance of the corresponding family of hierarchical feature maps. This
contribution is key because it shows that several results in [11] do not depend on the particular choice
of pooling function. We then establish a group-theoretic framework for characterizing invariance in
hierarchical models expressed in terms of the objects deﬁned here. Within this framework  we turn
to the problem of invariance in two speciﬁc domains of practical relevance: images and text strings.
Finally  we conclude with a few remarks summarizing the contributions and relevance of our work.
All proofs are omitted here  but can be found in the online supplementary material [2]. The reader
is assumed to be familiar with introductory concepts in group theory. An excellent reference is [1].
2
We ﬁrst review important deﬁnitions and concepts concerning the neural response feature map pre-
sented in Smale et al. The reader is encouraged to consult [11] for a more detailed discussion. We
will draw attention to the conditions needed for the neural response to be invariant with respect
to a family of arbitrary transformations  and then generalize the neural response map to allow for
arbitrary pooling functions. The proof of invariance given in [11] is extended to this generalized
setting. The proof presented here (and in [11]) hinges on a technical “Assumption” which must be
veriﬁed to hold true  given the model and the transformations to which we would like to be invariant.
Therefore the key step to establishing invariance is veriﬁcation of this Assumption. After stating the
Assumption and how it ﬁgures into the overall picture  we explore its veriﬁcation in Section 3. There
we are able to describe  for a broad range of hierarchical models (including a class of convolutional
neural networks [6])  the necessary conditions for invariance to a set of transformations.
2.1 Deﬁnition of the Feature Map and Invariance
First consider a system of patches of increasing size associated to successive layers of the hierarchy 
v1 ⊂ v2 ⊂ ··· ⊂ vn ⊆ S  with vn taken to be the size of the full input. Here layer n is the
top-most layer  and the patches are pieces of the domain on which the input data are deﬁned. The
set S could contain  for example  points in R2 (in the case of 2D graphics) or integer indices (the
case of strings). Until Section 4  the data are seen as general functions  however it is intuitively
helpful to think of the special case of images  and we will use a notation that is suggestive of this
particular case. Next  we’ll need spaces of functions on the patches  Im(vi). In many cases it will
only be necessary to work with arbitrary successive pairs of patches (layers)  in which case we will
denote by u the smaller patch  and v the next larger patch. We next introduce the transformation
sets Hi  i = 1  . . .   n intrinsic to the model. These are abstract sets in general  however here we
will take them to be comprised of translations with h ∈ Hi deﬁned by h : vi → vi+1. Note that by
construction  the functions h ∈ Hi implicitly involve restriction. For example  if f ∈ Im(v2) is an
image of size v2 and h ∈ H1  then f ◦ h is a piece of the image of size v1. The particular piece is
determined by h. Finally  to each layer we also associate a dictionary of templates  Qi ⊆ Im(vi).
The templates could be randomly sampled from Im(vi)  for example.

Given the ingredients above  the neural response Nm(f) and associated derived kernel (cid:98)Km are
nel (cid:98)K1  the m-th derived kernel (cid:98)Km  for m = 2  . . .   n  is obtained by normalizing Km(f  g) =
(cid:104)Nm(f)  Nm(g)(cid:105)L2(Qm−1) where Nm(f)(q) = maxh∈H (cid:98)Km−1(f ◦ h  q) 
Here a kernel is normalized by taking (cid:98)K(f  g) = K(f  g)/(cid:112)K(f  f)K(g  g). Note that the neural

deﬁned as follows.
Deﬁnition 1 (Neural Response). Given a non-negative valued  normalized  initial reproducing ker-
q ∈ Qm−1 with

response decomposes the input into a hierarchy of parts  analyzing sub-regions at different scales.
The neural response and derived kernels describe in compact  abstract terms the core operations built
into the many related hierarchical models of object recognition cited above.
We next deﬁne a set of transformations  distinct from the Hi above  to which we would like to be
invariant. Let r ∈ Ri  i ∈ {1  . . .   n − 1}  be transformations that can be viewed as mapping
either vi to itself or vi+1 to itself (depending on the context in which it is applied). We rule out the
degenerate translations and transformations  h or r mapping their entire domain to a single point.
When it is necessary to identify transformations deﬁned on a speciﬁc domain v  we will use the
notation rv : v → v. Invariance of the neural response feature map can now be deﬁned.

H = Hm−1.

2

rv ◦ h = π(h) ◦ ru

(1)

Deﬁnition 2 (Invariance). The feature map Nm is invariant to the domain transformation r ∈ R if

Nm(f) = Nm(f ◦ r)  for all f ∈ Im(vm)  or equivalently  (cid:98)Km(f ◦ r  f) = 1  for all f ∈ Im(vm).

Invariance and Generalized Pooling

In order to state the invariance properties of a given feature map  a technical assumption is needed.
Assumption 1 (from [11]). Fix any r ∈ R. There exists a surjective map π : H → H satisfying
for all h ∈ H.
This technical assumption is best described by way of an example. Consider images and rotations:
the assumption stipulates that rotating an image and then taking a restriction must be equivalent to
ﬁrst taking a (different) restriction and then rotating the resulting image patch. As we will describe
below  establishing invariance will boil down to verifying Assumption 1.
2.2
We next provide a generalized proof of invariance of a family of hierarchical feature maps  where
the properties we derive do not depend on the choice of the pooling function. Given the above
assumption  invariance can be established for general pooling functions of which the max is only
one particular choice. We will ﬁrst deﬁne such general pooling functions  and then describe the
corresponding generalized feature maps. The ﬁnal step will then be to state an invariance result for
the generalized feature map  given that Assumption 1 holds.
Let H = Hi  with i ∈ {1  . . .   n − 1}  and let B(R) denote the Borel algebra of R. As in Assump-
tion 1  we deﬁne π : H → H to be a surjection  and let Ψ : B(R++) → R++ be a bounded pooling
function deﬁned for Borel sets B ∈ B(R) consisting of only positive elements. Here R++ denotes
the set of strictly positive reals. Given a positive functional F acting on elements of H  we deﬁne
the set F (H) ∈ B(R) as
Note that since π is surjective  π(H) = H  and therefore (F ◦ π)(H) = F (H).
With these deﬁnitions in hand  we can deﬁne a more general neural response as follows. For H =
Hm−1 and all q ∈ Q = Qm−1  let the neural response be given by

F (H) = {F [h] | h ∈ H}.

where

Nm(f)(q) = (Ψ ◦ F )(H)

F [h] = (cid:98)Km−1(f ◦ h  q).

Nm(f) = Nm(f ◦ r) 

Given Assumption 1  we can now prove invariance of a neural response feature map built from the
general pooling function Ψ.

Theorem 1. Given any function Ψ : B(R++) → R++  if the initial kernel satisﬁes (cid:98)K1(f  f ◦ r) = 1

Averaging: We can consider average pooling by setting Ψ(B) = (cid:82)

for all r ∈ R  f ∈ Im(v1)  then
for all r ∈ R  f ∈ Im(vm) and m ≤ n.
We give a few practical examples of the pooling function Ψ.
Maximum: The original neural response is recovered setting Ψ(B) = sup B .
x∈B xdµ . If H has a measure
ρH  then a natural choice for µ is the induced push-forward measure ρH ◦ F −1. The measure ρH
may be simply uniform  or in the case of a ﬁnite set H  discrete. Similarly  we may consider more
general weighted averages.
3 A Group-Theoretic Invariance Framework
This section establishes general deﬁnitions and conditions needed to formalize a group-theoretic
concept of invariance. When Assumption 1 holds  then the neural response map can be made in-
variant to the given set of transformations. Proving invariance thus reduces to verifying that the
Assumption actually holds  and is valid. A primary goal of this paper is to place this task within
an algebraic framework so that the question of verifying the Assumption can be formalized and
explored in full generality with respect to model architecture  and the possible transformations. For-
malization of Assumption 1 culminates in Deﬁnition 3 below  where purely algebraic conditions
are separated from conditions stemming from the mechanics of the hierarchy. This separation re-
sults in a simpliﬁed problem because one can then tackle the algebraic questions independent of and
untangled from the model architecture.
Our general approach is as follows. We will require that R is a subset of a group and then use
algebraic tools to understand when and how Assumption 1 can be satisﬁed given different instances

3

of R. If R is ﬁxed  then the assumption can only be satisﬁed by placing requirements on the sets of
built-in translations Hi  i = 1  . . .   n. Therefore  we will make quantitative  constructive statements
about the minimal sets of translations associated to a layer required to support invariance to a set of
transformations. Conversely  one can ﬁx Hi and then ask whether the resulting feature map will be
invariant to any transformations. We explore this perspective as well  particularly in the examples
of Section 4  where speciﬁc problem domains are considered.
3.1 Formulating Conditions for Invariance
Recall that vi ⊂ S. Because it will be necessary to translate in S  it is assumed that an appropriate
notion of addition between the elements of S is given. If G is a group  we denote the (left) action of
G on S by A : G×S → S. Given an element g ∈ G  the notation Ag : S → S will be utilized. Since
A is a group action  it satisﬁes (Ag ◦ Ag(cid:48))(x) = Agg(cid:48)(x) for all x ∈ S and all g  g(cid:48) ∈ G. Consider
an arbitrary pair of successive layers with associated patch sizes u and v  with u ⊂ v ⊂ S. Recall
that the deﬁnition of the neural response involves the “built-in” translation functions h : u → v 
for h ∈ H = Hu. Since S has an addition operation  we may parameterize h ∈ H explicitly as
ha(x) = x + a for x ∈ u and parameter a ∈ v such that (u + a) ⊂ v. The restriction behavior
of the translations in H prevents us from simply generating a group out of the elements of H. To
get around this difﬁculty  we will decompose the h ∈ H into a composition of two functions: a
translation group action and an inclusion.
Let S generate a group of translations T by deﬁning the injective map

Arv ◦ Ata = Atb ◦ Aru  

(4)

4

S → T
a (cid:55)→ ta.

ha = Ata ◦ ιu

(2)
That is  to every element of a ∈ S we associate a member of the group T whose action corresponds
to translation in S by a: Ata(x) = x + a for x  a ∈ S. (Although we assume the speciﬁc case of
translations throughout  the sets of intrinsic operations Hi may more generally contain other kinds
of transformations. We assume  however  that T is abelian.) Furthermore  because the translations
H can be parameterized by an element of S  one can apply Equation (2) to deﬁne an injective map
τ : H → T by ha (cid:55)→ ta. Finally  we deﬁne ιu : u (cid:44)→ S to be the canonical inclusion of u into S.
We can now rewrite ha : u → v as
Note that because a satisﬁes (u + a) ⊂ v by deﬁnition  im(Ata ◦ ιu) ⊂ v automatically.
In the statement of Assumption 1  the transformations r ∈ R can be seen as maps from u to itself 
or from v to itself  depending on which side of Equation (1) they are applied. To avoid confusion
we denoted the former case by ru and the latter by rv. Although ru and rv are the same “kind”
of transformation  one cannot in general associate to each “kind” of transformation r ∈ R a single
element of some group as we did in the case of translations above. The group action could very
well be different depending on the context. We will therefore consider ru and rv to be distinct
transformations  loosely associated to r. In our development  we will make the important assumption
that the transformations ru  rv ∈ R can be expressed as actions of elements of some group  and
denote this group by R. More precisely  for every ru ∈ R  there is assumed to be a corresponding
element ρu ∈ R whose action satisﬁes Aρu(x) = ru(x) for all x ∈ u  and similarly  for every rv ∈
R  there is assumed to be a corresponding element ρv ∈ R whose action satisﬁes Aρv(x) = rv(x)
for all x ∈ v. The distinction between ρu and ρv will become clear in the case of feature maps
deﬁned on functions whose domain is a ﬁnite set (such as strings). In the case of images  we will
see that ρu = ρv.
Assumption 1 requires that rv ◦ h = h(cid:48) ◦ ru for h  h(cid:48) ∈ H  with the map π : h (cid:55)→ h(cid:48) onto. We
now restate this condition in group-theoretic terms. Deﬁne ˜T = τ(Hu) ⊆ T to be the set of group
elements corresponding to Hu. Set h = ha  h(cid:48) = hb  and denote also by ru  rv the elements of
the group R corresponding to the given transformation r ∈ R. The Assumption says in part that
rv ◦ h = h(cid:48) ◦ ru for some h(cid:48) ∈ H. This can now be expressed as

Arv ◦ Ata ◦ ιu = Atb ◦ ιu ◦ Aru ◦ ιu

(3)
for some tb ∈ ˜T . In order to arrive at a purely algebraic condition for invariance  we will need
to understand and manipulate compositions of group actions. However on the right-hand side of
Equation (3) the translation Atb is separated from the transformation Aru by the inclusion ιu. We
will therefore need to introduce an additional constraint on R. This constraint leads to our ﬁrst
condition for invariance: If x ∈ u  then we require that Aru(x) ∈ u for all r ∈ R. One can now see
that if this condition is met  then verifying Equation (3) reduces to checking that

and that the map ta (cid:55)→ tb is onto.
The next step is to turn compositions of actions Ax ◦ Ay into an equivalent action of the form Axy.
Do do this  one needs R and T to be subgroups of the same group G so that the associativity property
of group actions applies. A general way to accomplish this is to form the semidirect product

G = T (cid:111) R.

(5)
Recall that the semidirect product G = X (cid:111) Y is a way to put two subgroups X  Y together where
X is required to be normal in G  and X ∩ Y = {1} (the usual direct product requires both subgroups
to be normal). In our setting G is easily shown to be isomorphic to a group with normal subgroup T
and subgroup R where each element may be written in the form g = tr for t ∈ T  r ∈ R. We will
see below that we do not loose generality by requiring T to be normal. Note that although this con-
struction precludes R from containing the transformations in T   allowing R to contain translations
is an uninteresting case.
Consider now the action Ag for g ∈ G = T (cid:111) R. Returning to Equation (4)  we can apply the
associativity property of actions and see that Equation (4) will hold as long as

rv ˜T = ˜T ru

(6)
for every r ∈ R. This is our second condition for invariance  and is a purely algebraic requirement
concerning the groups R and T   distinct from the restriction related conditions involving the patches
u and v.
The two invariance conditions we have described thus far combine to capture the content of Assump-
tion 1  but in a manner that separates group related conditions from constraints due to restriction and
the nested nature of an architecture’s patch domains. We can summarize the invariance conditions
in the form of a concise deﬁnition that can be applied to establish invariance of the neural response
feature maps Nm(f)  2 ≤ m ≤ n with respect to a set of transformations. Let ˜R ⊆ R be the set of
transformations for which we would like to prove invariance  in correspondence with R.
Deﬁnition 3 (Compatible Sets). The subsets ˜R ⊂ R and ˜T ⊂ T are compatible if all of the following
conditions hold:

1. For each r ∈ ˜R  rv ˜T = ˜T ru. When ru = rv for all r ∈ R  this means that normalizer of

˜T in ˜R is ˜R.

2. Left transformations rv never take a point in v outside of v  and right transformations ru

never take a point in u/v outside of u/v (respectively):
imAru ◦ ιu ⊆ u 

imArv ◦ ιv ⊆ v 

imAru ◦ ιv ⊆ v 

for all r ∈ ˜R.

3. Translations never take a point in u outside of v:

for all t ∈ ˜T .

imAt ◦ ιu ⊆ v

The ﬁnal condition above has been added to ensure that any set of translations ˜T we might construct
satisfy the implicit assumption that the hierarchy’s translation functions h ∈ H are maps which
respect the deﬁnition h : u → v.
If ˜R and ˜T are compatible  then for each ta ∈ ˜T Equation 3 holds for some tb ∈ ˜T   and the map
ta (cid:55)→ tb is surjective from ˜T → ˜T (by Condition (1) above). So Assumption 1 holds.
As will become clear in the following section  the tools available to us from group theory will
provide insight into the structure of compatible sets.
3.2 Orbits and Compatible Sets
Suppose we assume that ˜R is a subgroup (rather than just a subset)  and ask for the smallest com-
patible ˜T . We will show that the only way to satisfy Condition (1) in Deﬁnition 3 is to require that
˜T be a union of ˜R-orbits  under the action

(7)
for t ∈ T   r ∈ ˜R. This perspective is particularly illuminating because it will eventually allow us
to view conjugation by a transformation r as a permutation of ˜T   thereby establishing surjectivity of

u

(t  r) (cid:55)→ rvtr−1

5

the map π deﬁned in Assumption 1. For computational reasons  viewing ˜T as a union of orbits is
also convenient.
If rv = ru = r  then the action (7) is exactly conjugation and the ˜R-orbit of a translation t ∈ T
is the conjugacy class C ˜R(t) = {rtr−1 | r ∈ ˜R}. Orbits of this form are also equivalence classes
under the relation s ∼ s(cid:48) if s(cid:48) ∈ C ˜R(s)  and we will require ˜T to be partitioned by the conjugacy
classes induced by ˜R.
The following Proposition shows that  given set of candidate translations in H  we can construct a
set of translations compatible with ˜R by requiring ˜T to be a union of ˜R-orbits under the action of
conjugation.
Proposition 1. Let Γ ⊆ T be a given set of translations  and assume the following: (1) G ∼= T (cid:111) R 
(2) For each r ∈ R  r = ru = rv  (3) ˜R is a subgroup of R. Then Condition (1) of Deﬁnition 3 is
satisﬁed if and only if ˜T can be expressed as a union of orbits of the form

C ˜R(t) .

(8)

˜T = (cid:91)

t∈Γ

An interpretation of the above Proposition  is that when ˜T is a union of ˜R-orbits  conjugation by
r can be seen as a permutation of ˜T . In general  a given ˜T may be decomposed into several such
orbits and the conjugation action of ˜R on ˜T may not necessarily be transitive.
4 Analysis of Speciﬁc Invariances
We continue with speciﬁc examples relevant to image processing and text analysis.
4.1
Consider the case where G is the group M of planar isometries  u ⊂ v ⊂ S = R2  and H involves
translations in the plane. Let O2 be the group of orthogonal operators  and let ta ∈ T denote a
translation represented by the vector a ∈ R2. In this section we assume the standard basis and work
with matrix representations of G when it is convenient.
We ﬁrst need that T (cid:67)M  a property that will be useful when verifying Condition (1) of Deﬁnition 3.
Indeed  from the First Isomorphism Theorem [1]  the quotient space M/T is isomorphic to O2 
giving the following commutative diagram:

Isometries of the Plane

where the isomorphism ˜π : M/T → O2 is given by ˜π(mT ) = π(m) and φ(m) = mT . We recall
that the kernel of a group homomorphism π : G → G(cid:48) is a normal subgroup of G  and that normal
subgroups N of G are invariant under the operation of conjugation by elements g of G. That is 
gN g−1 = N for all g ∈ G. With this picture in mind  the following Lemma establishes that T (cid:67) M 
and further shows that M is isomorphic to T (cid:111) R with R = O2  and T a normal subgroup of M.
Lemma 1. For each m ∈ M  ta ∈ T   mta = tbm for some unique element tb ∈ T .
We are now in a position to verify the Conditions of Deﬁnition 3 for the case of planar isometries.
Proposition 2. Let H be the set of translations associated to an arbitrary layer of the hierarchical
feature map and deﬁne the injective map τ : H → T by ha (cid:55)→ ta  where a is a parameter char-
acterizing the translation. Set Γ = {τ(h) | h ∈ H}. Take G = M ∼= T (cid:111) O2 as above. The
sets

˜R = O2 

˜T = (cid:91)

t∈Γ

C ˜R(t)

are compatible.
This proposition states that the hierarchical feature map may be made invariant to isometries  how-
ever one might reasonably ask whether the feature map can be invariant to other transformations.
The following Proposition conﬁrms that isometries are the only possible transformations  with group
structure  to which the hierarchy may be made invariant in the exact sense of Deﬁnition 2.
Proposition 3. Assume that the input spaces {Im(vi)}n−1
i=1 are endowed with a norm inherited from
Im(vn) by restriction. Then at all layers  the group of orthogonal operators O2 is the only group of
transformations to which the neural response can be invariant.

M

π- O2
-
φ ? ˜π
M/T

6

Figure 1: Example illustrating construction of an appro-
priate H. Suppose H initially contains the translations
Γ = {ha  hb  hc}. Then to be invariant to rotations  the
condition on H is that H must also include translations
deﬁned by the ˜R-orbits O ˜R(ta)  O ˜R(tb) and O ˜R(tc). In
this example ˜R = SO2  and the orbits are translations
to points lying on a circle in the plane.

The following Corollary is immediate:

Corollary 1. The neural response cannot be scale invariant  even if (cid:98)K1 is.

We give a few examples illustrating the application of the Propositions above.
Example 1. If we choose the group of rotations of the plane by setting ˜R = SO2 (cid:67) O2  then the
orbits O ˜R(a) are circles of radius (cid:107)a(cid:107). See Figure 1. Therefore rotation invariance is possible as
long as the set ˜T (and therefore H  since we can take H = τ−1( ˜T )) includes translations to all
points along the circle of radius a  for each element ta ∈ ˜T . In particular if H includes all possible
rotations as long as (cid:98)K1 is. A similar argument can be made for reﬂection invariance  as any rotation
translations  then Assumption 1 is veriﬁed  and we can apply Theorem 1: Nm will be invariant to

can be built out of the composition of two reﬂections.
Example 2. Analogous to the previous example  we may also consider ﬁnite cyclical groups Cn
describing rotations by θ = 2π/n. In this case the construction of an appropriate set of translations
is similar: we require that ˜T include at least the conjugacy classes with respect to the group Cn 
CCn(t) for each t ∈ Γ = τ(H).
Example 3. Consider a simple convolutional neural network [6] consisting of two layers  one ﬁlter
at the ﬁrst convolution layer  and downsampling at the second layer deﬁned by summation over all
distinct k × k blocks. In this case  Proposition 2 and Theorem 1 together say that if the ﬁlter kernel
is rotation invariant  then the output representation will be invariant to global rotation of the input
image. This is so because convolution implies the choice K1(f  g) = (cid:104)f  g(cid:105)L2  average pooling 
and H = H1 containing all possible translations. If the convolution ﬁlter z is rotation invariant 
z ◦ r = z for all rotations r  and K1(f ◦ r  z) = K1(f  z ◦ r−1) = K1(f  z). So we can conclude
invariance of the initial kernel.
4.2 Strings  Reﬂections  and Finite Groups
We next consider the case of ﬁnite length strings deﬁned on a ﬁnite alphabet. One of the advantages
group theory provides in the case of string data is that we need not work with permutation repre-
sentations. Indeed  we may equivalently work with group elements which act on strings as abstract
objects. The deﬁnition of the neural response given in Smale et al. involves translating an analysis
window over the length of a given string. Clearly translations over a ﬁnite string do not constitute a
group as the law of composition is not closed in this case. We will get around this difﬁculty by ﬁrst
considering closed words formed by joining the free ends of a string. Following the case of circular
data where arbitrary translations are allowed  we will then consider the original setting described in
Smale et al. in which strings are ﬁnite non-circular objects.
Taking a geometric standpoint sheds light on groups of transformations applicable to strings. In
particular  one can interpret the operation of the translations in H as a circular shift of a string
followed by truncation outside of a ﬁxed window. The cyclic group of circular shifts of an n-string
is readily seen to be isomorphic to the group of rotations of an n-sided regular polygon. Similarly 
reversal of an n-string is isomorphic to reﬂection of an n-sided polygon  and describes a cyclic group
of order two. As in Equation (5)  we can combine rotation and reﬂection via a semidirect product

(9)

∼= Cn (cid:111) C2

Dn

7

vivi+1taOR~(tc)OR~(tb)OR~(ta)tbtcDn = (cid:104)t  r | tn  r2

where Ck denotes the cyclic group of order k. The resulting product group has a familiar presen-
tation. Let t  r be the generators of the group  with r corresponding to reﬂection (reversal)  and t
corresponding to a rotation by angle 2π/n (leftward circular shift by one character). Then the group
of symmetries of a closed n-string is described by the relations
v  rvtrvt(cid:105).

(10)
These relations can be seen as describing the ways in which an n-string can be left unchanged. The
ﬁrst says that circularly shifting an n-string n times gives us back the original string. The second says
that reﬂecting twice gives back the original string  and the third says that left-shifting then reﬂecting
is the same as reﬂecting and then right-shifting. In describing exhaustively the symmetries of an
n-string  we have described exactly the dihedral group Dn of symmetries of an n-sided regular
polygon. As manipulations of a closed n-string and an n-sided polygon are isomorphic  we will use
geometric concepts and terminology to establish invariance of the neural response deﬁned on strings
with respect to reversal. In the following discussion we will abuse notation and at times denote by u
and v the largest index associated with the patches u and v.
In the case of reﬂections of strings  ru is quite distinct from rv. The latter reﬂection  rv  is the
usual reﬂection of an v-sided regular polygon  whereas we would like ru to reﬂect a smaller u-sided
polygon. To build a group out of such operations  however  we will need to ensure that ru and rv
both apply in the context of v-sided polygons. This can be done by extending Aru to v by deﬁning
ru to be the composition of two operations: one which reﬂects the u portion of a string and leaves
the rest ﬁxed  and another which reﬂects the remaining (v − u)-substring while leaving the ﬁrst
u-substring ﬁxed. In this case  one will notice that ru can be written in terms of rotations and the
usual reﬂection rv:
This also implies that for any x ∈ T  

ru = rvt−u = turv .

(11)

{rxr−1 | r ∈ (cid:104)rv(cid:105)} = {rxr−1 | r ∈ (cid:104)rv  ru(cid:105)} 

where we have used the fact that T is abelian  and applied the relations in Equation (10). We can
now make an educated guess as to the form of ˜T by starting with Condition (1) of Deﬁnition 3 and
applying the relations appearing in Equation (10). Given x ∈ ˜T   a reasonable requirement is that
there must exist an x(cid:48) ∈ ˜T such that rvx = x(cid:48)ru. In this case

˜R = R 

x(cid:48) = rvxru = rvxrvt−u = x−1rvrvt−u = x−1t−u 

(12)
where the second equality follows from Equation (11)  and the remaining equalities follow from
the relations (10). The following Proposition conﬁrms that this choice of ˜T is compatible with the
reﬂection subgroup of G = Dv  and closely parallels Proposition 2.
Proposition 4. Let H be the set of translations associated to an arbitrary layer of the hierarchical
feature map and deﬁne the injective map τ : H → T by ha (cid:55)→ ta  where a is a parameter charac-
∼= T (cid:111) R  with T = Cn = (cid:104)t(cid:105) and
terizing the translation. Set Γ = {τ(h) | h ∈ H}. Take G = Dn
R = C2 = {r  1}. The sets
˜T = Γ ∪ Γ−1t−u

are compatible.
One may also consider non-closed strings  as in Smale et al.  in which case substrings which would
wrap around the edges are disallowed. Proposition 4 in fact points to the minimum ˜T for reversals in
this scenario as well  noticing that the set of allowed translations is the same set above but with the
illegal elements removed. If we again take length u substrings of length v strings  this reduced set
of valid transformations in fact describes the symmetries of a regular (v − u + 1)-gon. We can thus
apply Proposition 4 working with the Dihedral group G = Dv−u+1 to settle the case of non-closed
strings.
5 Conclusion
We have shown that the tools offered by group theory can be proﬁtably applied towards understand-
ing invariance properties of a broad class of deep  hierarchical models. If one knows in advance the
transformations to which a model should be invariant  then the translations which must be built into
the hierarchy can be described. In the case of images  we showed that the only group to which a
model in the class of interest can be invariant is the group of planar orthogonal operators.
Acknowledgments
This research was supported by DARPA contract FA8650-06-C-7632  Sony  and King Abdullah
University of Science and Technology.

8

References
[1] M. Artin. Algebra. Prentice-Hall  1991.
[2] J. Bouvrie  L. Rosasco  and T. Poggio.

Supplementary material for “On Invariance
in Hierarchical Models”. NIPS  2009. Available online: http://cbcl.mit.edu/
publications/ps/978_supplement.pdf.

[3] K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of

pattern recognition unaffected by shift in position. Biol. Cyb.  36:193–202  1980.

[4] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of data with neural net-

works. Science  313(5786):504–507  2006.

[5] D.H. Hubel and T.N. Wiesel. Receptive ﬁelds and functional architecture of monkey striate

cortex. J. Phys.  195:215–243  1968.

[6] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proc. of the IEEE  86(11):2278–2324  November 1998.

[7] H. Lee  R. Grosse  R. Ranganath  and A. Ng. Convolutional deep belief networks for scal-
able unsupervised learning of hierarchical representations. In Proceedings of the Twenty-Sixth
International Conference on Machine Learning  2009.

[8] B.W. Mel. SEEMORE: Combining color  shape  and texture histogramming in a neurally

inspired approach to visual object recognition. Neural Comp.  9:777–804  1997.

[9] T. Serre  A. Oliva  and T. Poggio. A feedforward architecture accounts for rapid categorization.

Proceedings of the National Academy of Science  104:6424–6429  2007.

[10] T. Serre  L. Wolf  S. Bileschi  M. Riesenhuber  and T. Poggio. Robust object recognition with
cortex-like mechanisms. IEEE Trans. on Pattern Analysis and Machine Intelligence  29:411–
426  2007.

[11] S. Smale  L. Rosasco  J. Bouvrie  A. Caponnetto  and T. Poggio. Mathematics of the neu-
available online 

ral response. Foundations of Computational Mathematics  June 2009.
DOI:10.1007/s10208-009-9049-1.

[12] H. Wersing and E. Korner. Learning optimized features for hierarchical models of invariant

object recognition. Neural Comput.  7(15):1559–1588  July 2003.

9

,Odalric-Ambrym Maillard
Timothy Mann
Shie Mannor