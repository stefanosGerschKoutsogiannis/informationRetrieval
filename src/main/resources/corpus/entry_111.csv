2019,A Linearly Convergent Proximal Gradient Algorithm for Decentralized  Optimization,Decentralized optimization is a powerful  paradigm that finds applications in engineering and learning design. 	This work studies decentralized composite optimization problems with  non-smooth regularization terms. Most existing gradient-based proximal decentralized methods are known to converge to the optimal solution with sublinear rates  and it remains unclear whether this family of methods can achieve global linear convergence. To tackle this problem  this work assumes the non-smooth regularization term is common across all networked agents  which is the case for many machine learning problems. Under this condition  we design a proximal gradient decentralized algorithm whose fixed point coincides with the desired minimizer. We then provide a concise proof that establishes its linear convergence. In the absence of the non-smooth term  our analysis technique covers the well known   EXTRA algorithm and provides useful bounds on the convergence rate and step-size.,A Linearly Convergent Proximal Gradient Algorithm

for Decentralized Optimization

Sulaiman A. Alghunaim  Kun Yuan

Ali H. Sayed

Electrical and Computer Engineering Department

Ecole Polytechnique Fédérale de Lausanne

University of California Los Angeles

Los Angeles  CA  90095

{salghunaim kunyuan}@ucla.edu

CH-1015 Lausanne  Switzerland

ali.sayed@epfl.ch

Abstract

Decentralized optimization is a powerful paradigm that ﬁnds applications in engi-
neering and learning design. This work studies decentralized composite optimiza-
tion problems with non-smooth regularization terms. Most existing gradient-based
proximal decentralized methods are known to converge to the optimal solution
with sublinear rates  and it remains unclear whether this family of methods can
achieve global linear convergence. To tackle this problem  this work assumes the
non-smooth regularization term is common across all networked agents  which is
the case for many machine learning problems. Under this condition  we design a
proximal gradient decentralized algorithm whose ﬁxed point coincides with the
desired minimizer. We then provide a concise proof that establishes its linear
convergence. In the absence of the non-smooth term  our analysis technique covers
the well known EXTRA algorithm and provides useful bounds on the convergence
rate and step-size.

1

Introduction

Many machine learning problems can be cast as composite optimization problems of the form

N(cid:88)

n=1

min
w∈RM

J(w) + R(w)  where J(w) =

1
N

Q(w; xn)

(1)

where w is the optimization variable  xn is the n-th data  and N is the size of the dataset. The loss
function Q(w; xn) is assumed to be smooth  and R(w) is a regularization term possibly non-smooth.
Typical examples of R(w) can be the (cid:96)1-norm  the elastic-net norm  and indicator functions of convex
sets (e.g.  non-negative orthants). Problems of the form (1) arise in different settings including  among
others  in model ﬁtting [1] and economic dispatch problems in power systems [2].
When the data size N is very large  it becomes intractable or inefﬁcient to solve problem (1) with
a single machine. To relieve this difﬁculty  one solution is to divide the N data samples across
multiple machines and solve problem (1) in a cooperative manner. Many useful distributed algorithms
exist that solve problem (1) across multiple computing agents such as the distributed alternating
direction method of multipliers (ADMM) [1  3]  parallel SGD methods [4  5]  distributed second-order
methods [6–8]  and parallel dual coordinate methods [9  10]. All these methods are designed for a
centralized network topology  e.g.  parameter servers [11]  where there is a central node connected to
all computing agents that is responsible for aggregating local variables and updating model parameters.
The potential bottleneck of the centralized network is the communication trafﬁc jam on the central
node [12–14]. The performance of these non-decentralized methods can be signiﬁcantly degraded
when the bandwidth around the central node is low.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In contrast  decentralized optimization methods are designed for any connected network topology
such as line  ring  grid  and random graphs. There exists no central node for this family of methods 
and each computing agent will exchange information with their immediate neighbors rather than
a remote central server. Decentralized methods to solve problem (1) have been widely studied for
some time in the signal processing  control  and optimization communities [14–28]. More recently 
there have been works in the machine learning community with interest in these problems due to their
advantages over centralized methods [12  13  29  30]. Speciﬁcally  since the communication can be
evenly distributed across each link between nodes  the decentralized algorithms converge faster than
centralized ones when the network has limited bandwidth or high latency [12  13].
For the smooth case  the convergence rates of decentralized methods are comparable to centralized
methods. For example  the decentralized methods in [15  27  31] are shown to converge at the
sublinear rate O(1/i) (where i is the iteration index) for smooth and convex objective functions 
and at the linear rate O(γi) (where γ ∈ (0  1)) for smooth and strongly-convex objective functions.
These convergence rates match the convergence rates of centralized gradient descent. However 
some gap between decentralized and centralized proximal gradient methods continues to exist in the
presence of a composite non-smooth term. While centralized proximal gradient methods have been
shown to converge linearly when the objective is strongly convex [32]  it remains an open question to
establish the linear convergence of decentralized proximal gradient methods. This work closes this
gap by proposing a proximal gradient decentralized algorithm that is shown to converge linearly to
the desired solution. Next we explain the problem set-up and comment on existing related works.

1.1 Problem Set-up

Consider a network of K agents (e.g.  machines  processors) connected over some graph. Through
only local interactions (i.e.  agents only communicate with their immediate neighbors)  each node is
interested in ﬁnding a consensual vector  denoted by w(cid:63)  that minimizes the following aggregate cost:

Jk(w) + R(w)

(2)

(cid:80)L
n=1 Q(w; xk n) where {xk n}L

The cost function Jk(w) : RM → R is privately known by agent k and R(w) : RM → R ∪ {∞}
is a proper1 and lower-semicontinuous convex function (not necessarily differentiable). When
n=1 is the local data assigned or collected by agent k 
Jk(w) = 1
L
and L is the size of the local data  it is easy to see that problem (2) is equivalent to its centralized
counterpart (1) for N = KL. We adopt the following assumption throughout this work.
Assumption 1. (Cost function): There exists a solution w(cid:63) to problem (2). Moreover  each cost
function Jk(w) is convex and ﬁrst-order differentiable with δ-Lipschitz continuous gradients:

w(cid:63) = arg min
w∈RM

1
K

K(cid:88)

k=1

(cid:107)∇Jk(wo) − ∇Jk(w•)(cid:107) ≤ δ(cid:107)wo − w•(cid:107) 

for any wo and w•

and the aggregate cost function ¯J(w) = 1
K

(wo − w•)T(cid:0)∇ ¯J(wo) − ∇ ¯J(w•)(cid:1) ≥ ν(cid:107)wo − w•(cid:107)2

k=1 Jk(w) is ν-strongly-convex:

(cid:80)K

(3)

(4)

for any wo and w•. The constants ν and δ satisfy 0 < ν ≤ δ.
Note that from the strong-convexity condition (4)  we know the objective function in (2) is also
strongly convex and  thus  the global solution w(cid:63) is unique.

2

1.2 Related Works and Contribution

Research on decentralized/distributed optimization and computation dates back several decades (see 
e.g.  [33–36] and the references therein). In recent years  various centralized optimization methods
such as (sub-)gradient descent  proximal gradient descent  (quasi-)Newton method  dual averaging 
alternating direction method of multipliers (ADMM)  and other primal-dual methods have been
extended to the decentralized setting. The core problem in decentralized optimization is to design
methods with convergence rates that are comparable to their centralized counterparts. For the smooth

1The function f (.) is proper if −∞ < f (x) for all x in its domain and f (x) < ∞ for at least one x.

2

case (R(w) = 0)  the decentralized primal methods from [16  37–39] converge linearly to a biased
solution and not the exact solution. For convergence to the exact solution  these primal methods
require employing a decaying step-size that slows down the convergence rate making it sublinear at
O(1/i) in general. The works [18–22] established linear convergence to the exact solution albeit for
decentralized primal-dual methods based on ADMM or inexact augmented Lagrangian techniques.
Other works established linear convergence for simpler implementations including EXTRA [15] 
gradient tracking methods [26  27]  exact diffusion [28]  NIDS [40]  and others. The work [30] study
the problem from the dual domain and propose accelerated dual gradient descent to reach an optimal
linear convergence rate for smooth strongly-convex problems.
There also exist many works on decentralized composite optimization problems with non-smooth
regularization terms. The work [41] considered a similar set-up to this work and proposed a proximal
gradient method combined with Nesterov’s acceleration that can achieve O(1/i2) convergence rate;
however  it requires an increasing number of inner loop consensus steps with each iteration leading to
an expensive solution. Other works focused on the case where each agent k has a local regularizer
Rk(w) possibly different from other agents. For example  a proximal decentralized linearized ADMM
(DL-ADMM) approach is proposed in [22] to solve such composite problems with convergence
guarantees  while the work [42] establishes a sublinear convergence rate O(1/i) for DL-ADMM
when each Jk(w) is smooth with Lipschitz continuous gradient. PG-EXTRA [23] extends EXTRA
[15] to handle non-smooth regularization local terms and it establishes an improved rate o(1/i). The
NIDS algorithm [40] also has an o(1/i) rate and can use larger step-sizes compared to PG-EXTRA.
Based on existing results  there is still a clear gap between decentralized algorithms and centralized
algorithms for problem (2) when using proximal gradient methods.
The work [43] established the asymptotic linear convergence2 of a proximal decentralized algorithm
for the special case when all functions {Jk(w)  Rk(w)} (possibly different regularizers) are piecewise
linear-quadratic (PLQ) functions. While this result is encouraging  it does not cover the global linear
convergence rate we seek in this work since their linear rate occurs only after a sufﬁciently large
number of iterations and requires all costs to be PLQ. Another useful work [29] extends the CoCoA
algorithm [9] to the COLA algorithm for decentralized settings and shows linear convergence in
the presence of a non-differentiable regularizer. Like most other dual coordinate methods  COLA
considers decentralized learning for generalized linear models (e.g.  linear regression  logistic
regression  SVM  etc). This is because COLA requires solving (2) from the dual domain and the
linear model facilitates the derivation of the dual functions. Additionally  different from this work 
COLA is not a proximal gradient-based method; it requires solving an inner minimization problem
to a satisfactory accuracy  which is often computationally expensive but necessary for the linear
convergence analysis.
Note ﬁnally that decentralized optimization problems of the form (2) can be reformulated into a
consensus equality constrained optimization problem (see equation (7)). The consensus constraint
can then be added to the objective function using an indicator function. Several works have proposed
general solutions based on this construction using proximal primal-dual methods – see [44–46]
and references therein. Linear convergence for these methods have been established under certain
conditions that do not cover decentralized composite optimization problems of the form (2). For
example  the works [44  45] require a smoothness assumption  which does not cover the indicator
function needed for the consensus constraint. The work [46] requires the coefﬁcient matrix for the
non-smooth terms to be full-row rank  which is not the case for decentralized optimization problems
even when R(w) = 0.
Contribution. This paper considers the composite optimization problem (2) and has two main
contributions. First  for the case of a common non-smooth regularizer R(w) across all computing
agents  we propose a proximal decentralized algorithm whose ﬁxed point coincides with the desired
global solution w(cid:63). We then provide a short proof to establish its linear convergence when the
k=1 Jk(w) is strongly convex. This result closes the existing
gap between decentralized proximal gradient methods and centralized proximal gradient methods.
The second contribution is in our convergence proof technique. Speciﬁcally  we provide a concise
proof that is applicable to general decentralized primal-dual gradient methods such as EXTRA [15]
when R(w) = 0. Our proof provides useful bounds on the convergence rate and step-sizes.

aggregate of the smooth functions(cid:80)K

2A sequence {xi}∞

(cid:107)xi − x(cid:63)(cid:107) ≤ γiC for some C > 0 and all i ≥ io.

i=0 has asymptotic linear convergence to x(cid:63) if there exists a sufﬁciently large io such that

3

Notation. For a matrix A ∈ RM×N   σmax(A) (σmin(A)) denotes the maximum (minimum) singular
value of A  and σ(A) denotes the minimum non-zero singular value. For a vector x ∈ RM and a
positive semi-deﬁnite matrix C ≥ 0  we let (cid:107)x(cid:107)2
C = xTCx. The N × N identity matrix is denoted by
IN . We let 1N be a vector of size N with all entries equal to one. The Kronecker product is denoted
by ⊗. We let col{xn}N
n=1 denote a column vector (matrix) that stacks the vector (matrices) xn of
appropriate dimensions on top of each other. The subdifferential ∂f (x) of a function f (.) : RM → R
at some x ∈ RM is the set of all subgradients ∂f (x) = {g | gT(y − x) ≤ f (y) − f (x) ∀ y ∈ RM}.
The proximal operator with parameter µ > 0 of a function f (x) : RM → R is

proxµf (x) = arg min

z

f (z) +

(cid:107)z − x(cid:107)2

1
2µ

(5)

2 Proximal Decentralized Algorithm

In this section  we derive the algorithm and list its decentralized implementation.

2.1 Algorithm Derivation

We start by introducing the network weights that are used to implement the algorithm in a decentral-
ized manner. Thus  we let ask denote the weight used by agent k to scale information arriving from
agent s with ask = 0 if s is not a direct neighbor of agent k  i.e.  there is no edge connecting them.
Let A = [ask] ∈ RK×K denote the weight matrix associated with the network. Then  we assume A
to be symmetric and doubly stochastic  i.e.  A1K = 1K and 1T
K. We also assume that A is
primitive  i.e.  there exists an integer p such that all entries of Ap are positive. Note that as long as
the network is connected  there exist many ways to generate such weight matrices in a decentralized
fashion – [14  47  48]. Under these conditions  it holds from the Perron-Frobenius theorem [49] that
A has a single eigenvalue at one with all other eigenvalues being strictly less than one. Therefore 
(IK − A)x = 0 if  and only if  x = c1K for any c ∈ R. If we let wk ∈ RM denote a local copy of
the global variable w available at agent k and introduce the network quantities:

KA = 1T

W ∆= col{w1 ···   wK} ∈ RKM   B ∆=

(6)
then  it holds that BW = 0 if  and only if  wk = ws for all k  s. Note that since A is symmetric
with eigenvalues between (−1  1]  the matrix B is positive semi-deﬁnite with eigenvalues in [0  1).
Problem (2) is equivalent to the following constrained problem:
s.t. B 1

J (W) + R(W) 

(IKM − A ⊗ IM )

2 W = 0

1
2

(7)

where J (W) ∆= (cid:80)K

minimize
W∈RKM

k=1 Jk(wk)  R(W) ∆= (cid:80)K

2 is the square root of the positive
semi-deﬁnite matrix B. To solve problem (7)  we introduce ﬁrst the following equivalent saddle-point
problem:

k=1 R(wk) and B 1

min
W

max

Y

Lµ(W  Y) ∆= J (W) + R(W) + YTB 1

2 W +

(cid:107)B 1

2 W(cid:107)2

1
2µ

(8)

where Y ∈ RM K is the dual variable and µ > 0 is the coefﬁcient for the augmented Lagrangian. By
introducing Jµ(W) = J (W) + 1/2µ(cid:107)B 1

2 W(cid:107)2  it holds that

To solve the saddle point problem in (8)  we propose the following recursion. For i ≥ 0:

Lµ(W  Y) = Jµ(W) + R(W) + YTB 1

 Zi = Wi−1 − µ∇Jµ(Wi−1) − B 1

Yi = Yi−1 + αB 1
Wi = proxµR(Zi)

2 Zi

2 W.

2 Yi−1

(9)

(10a)
(10b)
(10c)

where α > 0 is the dual step-size (a tunable parameter). We will next show that with the initialization
Y0 = 0  we can implement this algorithm in a decentralized manner.

4

Remark 1 (CONVENTIONAL UPDATE). When R(w) = 0 and α = 1  recursions (10a)–(10c) recover
the primal-dual form of the EXTRA algorithm from [15]. However  when R(w) (cid:54)= 0  recursions
(10a)–(10c) differ from PG-EXTRA [23] in the dual update (10b). Different from conventional dual
updates that use Wi (e.g.  see [50] for the primal-dual form of PG-EXTRA)  we use Zi instead of
Wi. This subtle difference changes the complexity of the algorithm and allows us to close the linear
convergence gap between centralized and decentralized algorithms for problems of the form (2). 2

2.2 The Decentralized Implementation
From the deﬁnition of Jµ(W)  we have ∇Jµ(W) = ∇J (W) + 1/µ BW. Substituting ∇Jµ(W) into
(10a)  we have

(11)

(12)

Zi = (IKM − B)Wi−1 − µ∇J (Wi−1) − B 1

With the above relation  we have for i ≥ 1

Zi−Zi−1 = (I−B)(Wi−1−Wi−2)−µ(cid:0)∇J (Wi−1)−∇J (Wi−2)(cid:1)−B 1

2 Yi−1.

From (10b) we have Yi−1 − Yi−2 = αB 1

2 (Yi−1 − Yi−2)
2 Zi−1. Substituting this relation into (12)  we reach

Zi = (I − αB)Zi−1 + (I − B)(Wi−1−Wi−2)−µ(cid:0)∇J (Wi−1)−∇J (Wi−2)(cid:1)
Zi = (I − αB)Zi−1 + (I − B)(Wi−1−Wi−2)−µ(cid:0)∇J (Wi−1)−∇J (Wi−2)(cid:1)

(13)
for i ≥ 1. For initialization  we can repeat a similar argument to show that the proximal primal-dual
method (10a)–(10c) with Y0 = 0 is equivalent to the following algorithm. Let Z0 = W−1 = 0  set
∇J (W−1) ← 0  and W0 to any arbitrary value. Repeat for i = 1 ···

(14a)
(14b)
Since B has network structure  recursion (14) can be implemented in a decentralized way. This
algorithm only requires each agent to share one vector at each iteration; a per agent implementation
of resulting proximal primal-dual diffusion (P2D2) algorithm is listed in (15).

Wi = proxµR(Zi)

(cid:88)

Algorithm (Proximal Primal-Dual Diffusion – P2D2)
Setting: Let B = 0.5(I − A) = [bsk] and choose step-sizes µ and α. Set all initial variables to zero
and repeat for i = 1  2 ···

bsk(αzs i−1 + ws i−1 − ws i−2)

s∈Nk

φk i =
ψk i = wk i−1 − µ∇Jk(wk i−1)
zk i = zk i−1 + ψk i − ψk i−1 − φk i
wk i = proxµR(zk i)

(Communication Step)

(15a)

(15b)
(15c)
(15d)

3 Main Results

In this section  we establish the linear convergence of the proximal primal-dual diffusion (P2D2)
algorithm (10a)–(10c)  which is equivalent to (15). To this end  we establish auxiliary lemmas leading
to the main convergence result.

3.1 Optimality condition

We start by showing the existence and properties of a ﬁxed point for recursions (10a)–(10c).
Lemma 1 (FIXED POINT OPTIMILATY). Under Assumption 1  a ﬁxed point (W(cid:63)  Y(cid:63)  Z(cid:63)) exists for
recursions (10a)–(10c)  i.e.  it holds that

 Z(cid:63) = W(cid:63) − µ∇Jµ(W(cid:63)) − B 1

0 = B 1
W(cid:63) = proxµR(Z(cid:63))

2 Z(cid:63)

2 Y(cid:63)

5

(16a)
(16b)
(16c)

Moreover  W(cid:63) and Z(cid:63) are unique and each block element of W(cid:63) = col{w(cid:63)
the unique solution w(cid:63) to problem (2)  i.e.  w(cid:63)

k = w(cid:63) for all k.

1 ···   w(cid:63)

K} coincides with

Proof. The existence of a ﬁxed point is shown in Section A in the supplementary material. We now
establish the optimality of W(cid:63). Since Z(cid:63) satisﬁes (16b)  it holds that the block elements of Z(cid:63) are
K  and we denote each block element by z(cid:63). Therefore  from
equal to each other  i.e. z(cid:63)
(16c) and the deﬁnition of the proximal operator it holds that

1 = ··· = z(cid:63)

where we used z(cid:63)
any k. It is easy to verify that (17) implies

k = z(cid:63) for each k. Thus  we must have w(cid:63)

K. We denote w(cid:63)

k = w(cid:63) for

w(cid:63)

k = arg min

wk

{R(wk) + (cid:107)wk − z(cid:63)(cid:107)2/2µ}
1 = ··· = w(cid:63)

0 ∈ ∂R(w(cid:63)) + (w(cid:63) − z(cid:63))/µ.

(17)

(18)

K(cid:88)

Multiplying (1K ⊗ IM )T from the left to both sides of equation (16a)  we get

Kz(cid:63) = Kw(cid:63) − µ

∇Jk(w(cid:63)).

(19)

K

k=1

(cid:80)K
Combining (18) and (19)  we get 0 ∈ 1
k=1 ∇Jk(w(cid:63)) + ∂R(w(cid:63))  which implies that w(cid:63) is the
unique solution to problem (2). Due to the uniqueness of w(cid:63)  we see from (19) that z(cid:63) is unique.
Consequently  W(cid:63) = 1K ⊗ w(cid:63) and Z(cid:63) = 1K ⊗ z(cid:63) must be unique.
Remark 2 (PARTICULAR FIXED POINT). From Lemma 1  we see that although W(cid:63) and Z(cid:63) are
unique  there can be multiple ﬁxed points. This is because from (16a)  Y(cid:63) is not unique due the
rank deﬁciency of B 1
2 . However  by following similar arguments to the ones from [18]  it can be
veriﬁed that there exists a particular ﬁxed point (W(cid:63)  Y(cid:63)
b is a
unique vector that belongs to the range space of B 1
2 . In the following we will show that the iterates
(Wi  Yi  Zi) converge linearly to this particular ﬁxed point (W(cid:63)  Y(cid:63)
2

b   Z(cid:63)) satisfying (16a)–(16c) where Y(cid:63)

b   Z(cid:63)).

3.2 Linear Convergence



∆= Wi − W(cid:63) 

To establish the linear convergence of the proximal primal-dual diffusion (P2D2) (10a)–(10c) we
introduce the error quantities:

(cid:101)Wi
(cid:101)Zi = Zi − Z(cid:63)
(cid:101)Yi
(cid:101)Zi = (cid:101)Wi−1 − µ(cid:0)∇Jµ(Wi−1) − ∇Jµ(W(cid:63))(cid:1) − B 1
2(cid:101)Yi−1
(cid:101)Yi =(cid:101)Yi−1 + αB 1
2(cid:101)Zi
(cid:101)Wi = proxµR(Zi) − proxµR(Z(cid:63))

∆= Yi − Y(cid:63)
(20)
b  
By subtracting (16a)–(16c) from (10a)–(10c) with Y(cid:63) = Y(cid:63)
b  we reach the following error recursions
(21a)
(21b)
(21c)
We let σmax and σ denote the maximum singular value and minimum non-zero singular value of
the matrix B. Notice that from (6)  B is symmetric and  thus  its singular values are equal to its
eigenvalues and are in [0  1) (i.e.  σmin = 0 < σ ≤ σmax < 1). The following result follows from
[15  Proposition 3.6].
Lemma 2 (AUGMENTED COST). Under Assumption 1  the penalized augmented cost J (W) +
2(cid:107)W(cid:107)2B with any ρ > 0 is restricted strongly-convex with respect to W(cid:63):

(W − W(cid:63))T(cid:0)∇J (W) − ∇J (W(cid:63))(cid:1) + ρ(cid:107)W − W(cid:63)(cid:107)2B ≥ νρ(cid:107)W − W(cid:63)(cid:107)2
(cid:17)

where

(cid:26)

(cid:27)

ρ

for any c ∈(cid:16)

νρ = min

0 
for any W with W(cid:63) = 1 ⊗ w(cid:63) and where w(cid:63) denotes the minimizer of (2).
Using the previous result  the following lemma establishes a useful inequality for later use.

> 0 

(22)

(23)

ν − 2δc 

ρσ(B)c2
4(c2 + 1)

ν
2δ

6

Lemma 3 (DESCENT INEQUALITY). Under Assumption 1 and step-size conditions µ < (1−σmax)

and α ≤ 1  it holds that(cid:13)(cid:13)(cid:101)Wi−1 − µ(cid:0)∇Jµ(Wi−1) − ∇Jµ(W(cid:63))(cid:1)(cid:13)(cid:13)2 ≤ γ1(cid:107)(cid:101)Wi−1(cid:107)2Q

(24)
where Q = I − αB and γ1 = 1 − µνρ(2 − σmax − µδ) < 1 for some ρ > 0 with νρ given in (23).

δ

Proof. Since ∇Jµ(W) = ∇J (W) + 1

(cid:13)(cid:13)(cid:101)Wi−1 − µ(cid:0)∇Jµ(Wi−1) − ∇Jµ(W(cid:63))(cid:1)(cid:13)(cid:13)2
= (cid:107)(cid:101)Wi−1(cid:107)2−2µ(cid:101)WT

i−1

(cid:0)∇J (Wi−1)−∇J (W(cid:63))(cid:1)−2(cid:107)(cid:101)Wi−1(cid:107)2B +µ2(cid:107)∇Jµ(Wi−1)−∇Jµ(W(cid:63))(cid:107)2

µBW  it holds that

Note that ∇J (W) + 1

µBW is δµ = δ + 1

µ σmax-Lipschitz  thus it holds that [51  Theorem 2.1.5]:

1
µ

B(cid:101)Wi−1(cid:107)2
(cid:0)∇J (Wi−1) − ∇J (W(cid:63)) +

(cid:107)∇Jµ(Wi−1)−∇Jµ(W(cid:63))(cid:107)2 = (cid:107)∇J (Wi−1) − ∇J (W(cid:63)) +

≤ δµ(cid:101)WT
B(cid:101)Wi−1
(cid:13)(cid:13)(cid:101)Wi−1 − µ(cid:0)∇Jµ(Wi−1) − ∇Jµ(W(cid:63))(cid:1)(cid:13)(cid:13)2
(cid:0)∇J (Wi−1)−∇J (W(cid:63))(cid:1)−(2 − µδµ)(cid:107)(cid:101)Wi−1(cid:107)2B
≤ (cid:107)(cid:101)Wi−1(cid:107)2−µ(2 − µδµ)(cid:101)WT
≤(cid:0)1 − µνρ(2 − µδµ)(cid:1)(cid:107)(cid:101)Wi−1(cid:107)2−(2 − µδµ)(1 − ρµ)(cid:107)(cid:101)Wi−1(cid:107)2B

(cid:1)

i−1

i−1

1
µ

Substituting the previous inequality into (25) we get

(27)
where the last inequality follows from (22) and 2 − µδµ = 2 − σmax − µδ > 0 for µ < 2−σmax

Letting γ1 = 1 − µνρ(2 − µδµ) and adding and subtracting αγ1(cid:107)(cid:101)Wi−1(cid:107)B to the right hand side of
(cid:13)(cid:13)(cid:101)Wi−1−µ(cid:0)∇Jµ(Wi−1)−∇Jµ(W(cid:63))(cid:1)(cid:13)(cid:13)2 ≤ γ1(cid:107)(cid:101)Wi−1(cid:107)2Q−(cid:0)(2 − µδµ)(1 − ρµ)−αγ1
(cid:1)(cid:107)(cid:101)Wi−1(cid:107)2B (28)

the previous inequality gives:

δ

.

where Q = I − αB. If we can ensure that

−(cid:0)(2 − µδµ)(1 − µρ) − αγ1

(cid:1)(cid:107)(cid:101)Wi−1(cid:107)2B ≤ 0

then inequality (28) can be upper bounded by (24). To ensure inquality (29)  it is sufﬁcient to ﬁnd µ
and ρ such that

(2 − µδµ)(1 − µρ) − γ1α = (2 − σmax − µδ)(1 − µρ) − γ1α ≥ 0

(25)

(26)

(29)

(30)
and using α ≤ 1  the above

By noting that γ1 = 1 − µνρ(2 − σmax − µδ) < 1 for µ < 1−σmax
inequality is guaranteed to hold if

δ

0 < ρ ≤ 1 − σmax − µδ
µ(2 − σmax − µδ)

(31)

The previous lemma will be used to establish the following primal-dual error bound.
Lemma 4 (ERROR BOUND). Under Assumption 1  if Y0 = 0 and the step-sizes satisfy µ < (1−σmax)
and α ≤ 1  it holds that

δ

(cid:107)(cid:101)Zi(cid:107)2Q + (cid:107)(cid:101)Yi(cid:107)2

α I ≤ γ1(cid:107)(cid:101)Wi−1(cid:107)2Q + γ2(cid:107)(cid:101)Yi−1(cid:107)2

(32)
where Q = I − αB > 0  γ1 = 1 − µνρ(2 − σmax − µδ) < 1  and γ2 = 1 − ασ < 1 for some ρ > 0
with νρ given in (23).

1
α I

1

7

and

Proof. Squaring both sides of (21a) and (21b) we get

(cid:107)(cid:101)Yi(cid:107)2 = (cid:107)(cid:101)Yi−1 + αB 1

− 2(cid:101)YT
2(cid:101)Zi(cid:107)2 = (cid:107)(cid:101)Yi−1(cid:107)2 + α2(cid:107)B 1

(cid:107)(cid:101)Zi(cid:107)2 = (cid:107)(cid:101)Wi−1 − µ(cid:0)∇Jµ(Wi−1) − ∇Jµ(W(cid:63))(cid:1)(cid:107)2 + (cid:107)B 1
2(cid:101)Yi−1(cid:107)2
2(cid:0)(cid:101)Wi−1 − µ(cid:0)∇Jµ(Wi−1) − ∇Jµ(W(cid:63))(cid:1)(cid:1)
i−1B 1
2(cid:101)Zi(cid:107)2 + 2α(cid:101)YT
2(cid:101)Zi
i−1B 1
= (cid:107)(cid:101)Yi−1(cid:107)2 + α2(cid:107)(cid:101)Zi(cid:107)2B − 2α(cid:107)B 1
2(cid:101)Yi−1(cid:107)2
2(cid:0)(cid:101)Wi−1 − µ(cid:0)∇Jµ(Wi−1) − ∇WJµ(W(cid:63))(cid:1)(cid:1)
+ 2α(cid:101)YT
α I =(cid:107)(cid:101)Wi−1−µ(cid:0)∇Jµ(Wi−1)−∇Jµ(W(cid:63))(cid:1)(cid:107)2 +(cid:107)(cid:101)Yi−1(cid:107)2
2(cid:101)Yi−1(cid:107)2
α I ≤ (cid:107)(cid:101)Wi−1 − µ(cid:0)∇Jµ(Wi−1) − ∇Jµ(W(cid:63))(cid:1)(cid:107)2 +(1 − ασ)(cid:107)(cid:101)Yi−1(cid:107)2

α I−α(cid:107)B 1
2   it holds that (cid:107)B 1

b lie in the range space3 of B 1

Multiplying equation (34) by 1

(cid:107)(cid:101)Zi(cid:107)2Q +(cid:107)(cid:101)Yi(cid:107)2
σ(cid:107)(cid:101)Yi−1(cid:107)2 – see [18]. Thus  we can bound (35) by
(cid:107)(cid:101)Zi(cid:107)2Q + (cid:107)(cid:101)Yi(cid:107)2

where Q = I − αB. Since both Yi and Y(cid:63)

α and adding to (33)  we get

i−1B 1

(21a)

1

1

1

(36)
Under the conditions given in Lemma 3  we can substitute inequality (24) in the above relation
and get (32). Note that that γ1 = 1 − µνρ(2 − σmax − µδ) < 1 if µ < (1 − σmax)/δ. Moreover 
Q = I − αB > 0 and γ2 = 1 − ασ < 1 for α ≤ 1 since σmax < 1.

1
α I

1
α I

(35)

2(cid:101)Yi−1(cid:107)2 ≥

(33)

(34)

The next Theorem establishes the linear convergence of our proposed algorithm.
Theorem 1 (LINEAR CONVERGENCE). Under Assumption 1  Y0 = 0  and if step-sizes satisfy

(1 − σmax)

It holds that (cid:107)(cid:101)Wi(cid:107)2 ≤ Cγi where C > 0 and

µ <

δ

  α ≤ min{1  µνρ(2 − σmax − µδ)} .

γ ∆= max{1 − µνρ(2 − σmax − µδ)/(1 − ασmax)  1 − ασ} < 1

for some ρ > 0 with νρ given in (23).
Proof. Assume α ≤ 1 and note that Q = I − αB. Thus  it holds that σmin(Q) = 1 − ασmax and
σmax(Q) = 1. This implies that (1 − ασmax)(cid:107)x(cid:107)2 ≤ (cid:107)x(cid:107)2Q ≤ (cid:107)x(cid:107)2 for any x ∈ RKM . Therefore 
it holds from Lemma 4 that

when µ < (1−σmax)

δ

. Dividing by β ∆= 1 − ασmax both sides of the above inequality  we have

(1 − ασmax)(cid:107)(cid:101)Zi(cid:107)2 + (cid:107)(cid:101)Yi(cid:107)2

(cid:107)(cid:101)Zi(cid:107)2 + (cid:107)(cid:101)Yi(cid:107)2

αβ I ≤ γ1

1

β

1

α I ≤ γ1(cid:107)(cid:101)Wi−1(cid:107)2 + γ2(cid:107)(cid:101)Yi−1(cid:107)2
(cid:107)(cid:101)Wi−1(cid:107)2 + γ2(cid:107)(cid:101)Yi−1(cid:107)2

αβ I .

1

1
α I

Clearly  β ∈ (0  1) when α ≤ 1. Now we evaluate γ1/β. It is easy to verify that

when α ≤ µνρ(2 − σmax − µδ) < µνρ(2−σmax−µδ)
the proximal operator we have:

σmax

. Next  from the non-expansiveness property of

By substituting (42) into (40) and letting γ ∆= max{γ1/β  γ2}  we reach

= (1 − µνρ(2 − σmax − µδ)) /(1 − ασmax) < 1

γ1
β

(cid:107)(cid:101)Wi(cid:107)2 = (cid:107)proxµR(Zi) − proxµR(Z(cid:63))(cid:107)2 ≤ (cid:107)(cid:101)Zi(cid:107)2.
(cid:17)
(cid:16)(cid:107)(cid:101)Wi−1(cid:107)2 + (cid:107)(cid:101)Yi−1(cid:107)2
(cid:107)(cid:101)Wi(cid:107)2 + (cid:107)(cid:101)Yi(cid:107)2
αβ I ≤ γ
αβ I ≤ γi((cid:107)(cid:101)W0(cid:107)2 + (cid:107)(cid:101)Y0(cid:107)2
(cid:107)(cid:101)Wi(cid:107)2 ≤ (cid:107)(cid:101)Wi(cid:107)2 + (cid:107)(cid:101)Yi(cid:107)2

1
αβ I

1

1

αβ I ) 

1

(37)

(38)

(39)

(40)

(41)

(42)

(43)

(44)

when step-sizes µ and α satisfy condition (37). We iterate the above inequality and get

which concludes the proof.

3Since Y0 = 0 and Yi = Yi−1 + αB 1

2 Zi  we know Yi ∈ range(B 1

2 ) for any i.

8

Next we show that when R(w) = 0  we can have a better upper bound for the dual step-size  which
covers the EXTRA algorithm [15].
Theorem 2 (LINEAR CONVERGENCE WHEN R(w) = 0). Under Assumption 1  if R(w) = 0 
Y0 = 0  and the step-sizes satisfy µ < (1−σmax)
C > 0  Q = I − αB > 0  and

and α ≤ 1  it holds that (cid:107)(cid:101)Wi(cid:107)2Q ≤ Cγi where

γ = max(cid:8)1 − µνρ(2 − σmax − µδ)  1 − ασ(cid:9) < 1

δ

for some ρ > 0 with νρ given in (23).
Proof. From lemma 4  we know when µ < (1−σmax)

R(w) = 0  we know Wi = Zi from recursion (10c) and hence(cid:101)Wi =(cid:101)Zi. By letting γ = max{γ1  γ2} 
and α ≤ 1 that inequality (32) holds. Since
inequality (32) becomes (cid:107)(cid:101)Wi(cid:107)2Q + (cid:107)(cid:101)Yi(cid:107)2
α I ≤ γ((cid:107)(cid:101)Wi−1(cid:107)2Q + (cid:107)(cid:101)Yi−1(cid:107)2
positive deﬁnite when α ≤ 1  we reach the linear convergence of (cid:101)Wi.
α I ). Since Q = I − αB is

δ

1

1

In the above Theorem  we see that the convergence rate bound is upper bounded by two terms  one
term is from the cost function and the other is from the network. This bound shows how the network
affects the convergence rate of the algorithm. For example  in Theorem 2  assume that α = 1 and the
network term dominates the convergence rate so that γ = 1 − ασ = 1 − σ. Recall that σ = σ(B)
is the smallest non-zero singular value (or eigenvalue) of the matrix 0.5(I − A). Thus  the effect
of the network on the convergence rate is evident through the term 1 − σ  which becomes close to
one as the network becomes more sparse. Note when α = 1  the algorithm recovers EXTRA as
highlighted in Remark 1. In this case  our step-size condition is on the order of O((1 − σmax)/δ).
Note that the in the original EXTRA proof in [15  Theorem 3.7]  the step-size bound is on the order
of O(νρ(1 − σmax)/δ2))  which scales badly for ill-conditioned problems  i.e.  if δ is much larger
than νρ. We remark that simulations of the proposed algorithm are provided in Section B in the
supplementary material.

Acknowledgments

This work was supported in part by NSF grant CCF-1524250. We would like to thank the anonymous
reviewers for their insightful comments.

References
[1] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning

via alternating direction method of multipliers. Found. Trends Mach. Lear.  3(1):1–122  Jan. 2011.

[2] A. D. Dominguez-Garcia  S. T. Cady  and C. N. Hadjicostis. Decentralized optimal dispatch of distributed
energy resources. In 51st IEEE Conference on Decision and Control (CDC)  pages 3688–3693  Maui  HI 
USA  Dec. 2012.

[3] W. Deng  M.-J. Lai  Z. Peng  and W. Yin. Parallel multi-block ADMM with o (1/k) convergence. Journal

of Scientiﬁc Computing  71(2):712–736  2017.

[4] M. Zinkevich  M. Weimer  L. Li  and A. J. Smola. Parallelized stochastic gradient descent. In Advances in

Neural Information Processing Systems (NIPS)  pages 2595–2603  Vancouver  Canada  2010.

[5] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information

Processing Systems (NIPS)  pages 873–881  Granada Spain  2011.

[6] O. Shamir  N. Srebro  and T. Zhang. Communication-efﬁcient distributed optimization using an approxi-
mate Newton-type method. In International Conference on Machine Learning (ICML)  pages 1000–1008 
Beijing  China  2014.

[7] Y. Zhang and X. Lin. Disco: Distributed optimization for self-concordant empirical loss. In International

Conference on Machine Learning (ICML)  pages 362–370  Lille  France  2015.

[8] C.-P. Lee  C. H. Lim  and S. J. Wright. A distributed quasi-newton algorithm for empirical risk minimization
with nonsmooth regularization. In Proc. ACM SIGKDD  pages 1646–1655  London  United Kingdom 
2018.

9

[9] V. Smith  S. Forte  M. Chenxin  M. Takac  M. I. Jordan  and M. Jaggi. CoCoA: A general framework for
communication-efﬁcient distributed optimization. Journal of Machine Learning Research  18(230):1–49 
2018.

[10] Z. Peng  Y. Xu  M. Yan  and W. Yin. ARock: an algorithmic framework for asynchronous parallel

coordinate updates. SIAM Journal on Scientiﬁc Computing  38(5):A2851–A2879  2016.

[11] M. Li  D. G. Andersen  J. W. Park  A. J. Smola  A. Ahmed  V. Josifovski  J. Long  E. J. Shekita  and B.-Y.
Su. Scaling distributed machine learning with the parameter server. In 11th Symposium on Operating
Systems Design and Implementation (OSDI)  pages 583–598  Broomﬁeld  Denver  Colorado  2014.

[12] X. Lian  C. Zhang  H. Zhang  C.-J. Hsieh  W. Zhang  and J. Liu. Can decentralized algorithms outperform
centralized algorithms? A case study for decentralized parallel stochastic gradient descent. In Advances in
Neural Information Processing Systems (NIPS)  pages 5330–5340  Long Beach  CA  USA  2017.

[13] X. Lian  W. Zhang  C. Zhang  and J. Liu. Asynchronous decentralized parallel stochastic gradient descent.

In International Conference on Machine Learning (ICML)  pages 1–10  Stockholm  Sweden  2018.

[14] A. H. Sayed. Adaptation  learning  and optimization over neworks. Foundations and Trends in Machine

Learning  7(4-5):311–801  2014.

[15] W. Shi  Q. Ling  G. Wu  and W. Yin. EXTRA: An exact ﬁrst-order algorithm for decentralized consensus

optimization. SIAM Journal on Optimization  25(2):944–966  2015.

[16] A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transac-

tions on Automatic Control  54(1):48–61  2009.

[17] A. H. Sayed. Adaptive networks. Proceedings of the IEEE  102(4):460–497  Apr. 2014.

[18] W. Shi  Q. Ling  K. Yuan  G. Wu  and W. Yin. On the linear convergence of the ADMM in decentralized

consensus optimization. IEEE Trans. Signal Process.  62(7):1750–1761  2014.

[19] D. Jakoveti´c  J. M. Moura  and J. Xavier. Linear convergence rate of a class of distributed augmented

Lagrangian algorithms. IEEE Transactions on Automatic Control  60(4):922–936  2015.

[20] F. Iutzeler  P. Bianchi  P. Ciblat  and W. Hachem. Explicit convergence rate of a distributed alternating

direction method of multipliers. IEEE Transactions on Automatic Control  61(4):892–904  2016.

[21] Q. Ling  W. Shi  G. Wu  and A. Ribeiro. DLM: Decentralized linearized alternating direction method of

multipliers. IEEE Transactions on Signal Processing  63:4051–4064  2015.

[22] T.-H. Chang  M. Hong  and X. Wang. Multi-agent distributed optimization via inexact consensus ADMM.

IEEE Transactions on Signal Processing  63(2):482–497  Jan. 2015.

[23] W. Shi  Q. Ling  G. Wu  and W. Yin. A proximal gradient algorithm for decentralized composite

optimization. IEEE Transactions on Signal Processing  63(22):6013–6023  2015.

[24] P. Di Lorenzo and G. Scutari. Next: In-network nonconvex optimization. IEEE Transactions on Signal

and Information Processing over Networks  2(2):120–136  2016.

[25] J. Xu  S. Zhu  Y. C. Soh  and L. Xie. Augmented distributed gradient methods for multi-agent optimization
under uncoordinated constant stepsizes. In Proc. 54th IEEE Conference on Decision and Control (CDC) 
pages 2055–2060  Osaka  Japan  2015.

[26] A. Nedic  A. Olshevsky  and W. Shi. Achieving geometric convergence for distributed optimization over

time-varying graphs. SIAM Journal on Optimization  27(4):2597–2633  2017.

[27] G. Qu and N. Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on

Control of Network Systems  5(3):1245–1260  Sept. 2018.

[28] K. Yuan  B. Ying  X. Zhao  and A. H. Sayed. Exact diffusion for distributed optimization and learning-Part

I: Algorithm development. IEEE Transactions on Signal Processing  67(3):708–723  Feb. 2019.

[29] L. He  A. Bian  and M. Jaggi. COLA: Decentralized linear learning. In Advances in Neural Information

Processing Systems (NeurIPS)  pages 4536–4546  Montreal  Canada  2018.

[30] K. Scaman  F. Bach  S. Bubeck  Y. T. Lee  and L. Massoulie. Optimal algorithms for smooth and strongly
convex distributed optimization in networks. In International Conference on Machine Learning (ICML) 
pages 3027–3036  Stockholm  Sweden  2017.

10

[31] K. Yuan  B. Ying  X. Zhao  and A. H. Sayed. Exact diffusion for distributed optimization and learning-Part

II: Convergence analysis. IEEE Transactions on Signal Processing  67(3):724–739  Feb. 2019.

[32] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM

Journal on Optimization  24(4):2057–2075  2014.

[33] D. Chazan and W. Miranker. Chaotic relaxation. Linear Algebra and its Applications  2(2):199–222  1969.

[34] G. M. Baudet. Asynchronous iterative methods for multiprocessors. Journal of the ACM (JACM)  25(2):

226–244  1978.

[35] D. P. Bertsekas. Distributed asynchronous computation of ﬁxed points. Mathematical Programming  27

(1):107–120  1983.

[36] J. Tsitsiklis  D. Bertsekas  and M. Athans. Distributed asynchronous deterministic and stochastic gradient

optimization algorithms. IEEE Transactions on Automatic Control  31(9):803–812  1986.

[37] J. C. Duchi  A. Agarwal  and M. J. Wainwright. Dual averaging for distributed optimization: Convergence

analysis and network scaling. IEEE Transactions on Automatic Control  57(3):592–606  2012.

[38] K. Yuan  Q. Ling  and W. Yin. On the convergence of decentralized gradient descent. SIAM Journal on

Optimization  26(3):1835–1854  2016.

[39] J. Chen and A. H. Sayed. Distributed Pareto optimization via diffusion strategies. IEEE J. Sel. Topics

Signal Process.  7(2):205–220  April 2013.

[40] Z. Li  W. Shi  and M. Yan. A decentralized proximal-gradient method with network independent step-sizes
and separated convergence rates. IEEE Transactions on Signal Processing  67(17):4494–4506  Sept. 2019.

[41] A. I. Chen and A. Ozdaglar. A fast distributed proximal-gradient method. In Annual Allerton Conference

on Communication  Control  and Computing  pages 601–608  Monticello  IL  USA  Oct. 2012.

[42] N. S. Aybat  Z. Wang  T. Lin  and S. Ma. Distributed linearized alternating direction method of multipliers
for composite convex consensus optimization. IEEE Transactions on Automatic Control  63(1):5–20  2018.

[43] P. Latafat  N. M. Freris  and P. Patrinos. A new randomized block-coordinate primal-dual proximal
algorithm for distributed optimization. IEEE Transactions on Automatic Control  64(10):4050–4065  Oct.
2019.

[44] R. I. Bot  E. R. Csetnek  A. Heinrich  and C. Hendrich. On the convergence rate improvement of a
primal-dual splitting algorithm for solving monotone inclusion problems. Mathematical Programming 
150(2):251–279  2015.

[45] A. Chambolle and T. Pock. On the ergodic convergence rates of a ﬁrst-order primal–dual algorithm.

Mathematical Programming  159(1-2):253–287  Sept. 2016.

[46] P. Chen  J. Huang  and X. Zhang. A primal-dual ﬁxed point algorithm for convex separable minimization

with applications to image restoration. Inverse Problems  29(2):025011  Jan. 2013.

[47] N. Metropolis  A. W. Rosenbluth  M. N. Rosenbluth  A. H. Teller  and E. Teller. Equation of state

calculations by fast computing machines. The Journal of Chemical Physics  21(6):1087–1092  1953.

[48] L. Xiao and S. Boyd. Fast linear iterations for distributed averaging. Systems & Control Letters  53(1):

65–78  2004.

[49] S. U. Pillai  T. Suel  and S. Cha. The Perron-Frobenius theorem: Some of its applications. IEEE Signal

Processing Magazine  22(2):62–75  2005.

[50] Z. Li and M. Yan. A primal-dual algorithm with optimal stepsizes and its application in decentralized

consensus optimization. available on arXiv:1711.06785  Nov. 2017.

[51] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Volume 87  Springer  2013.

11

,Tatiana Shpakova
Francis Bach
Jiantao Jiao
Weihao Gao
Yanjun Han
Sulaiman Alghunaim
Kun Yuan
Ali Sayed