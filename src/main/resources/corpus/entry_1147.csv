2018,Continuous-time Value Function Approximation in Reproducing Kernel Hilbert Spaces,Motivated by the success of reinforcement learning (RL) for discrete-time tasks such as AlphaGo and Atari games  there has been a recent surge of interest in using RL for continuous-time control of physical systems (cf. many challenging tasks in OpenAI Gym and DeepMind Control Suite).
Since discretization of time is susceptible to error  it is methodologically more desirable to handle the system dynamics directly in continuous time.
However  very few techniques exist for continuous-time RL and they lack flexibility in value function approximation.
In this paper  we propose a novel framework for model-based continuous-time value function approximation in reproducing kernel Hilbert spaces.
The resulting framework is so flexible that it can accommodate any kind of kernel-based approach  such as Gaussian processes and kernel adaptive filters  and it allows us to handle uncertainties and nonstationarity without prior knowledge about the environment or what basis functions to employ.
We demonstrate the validity of the presented framework through experiments.,Continuous-time Value Function Approximation

in Reproducing Kernel Hilbert Spaces

Motoya Ohnishi

Keio Univ.  KTH  RIKEN

motoya.ohnishi@riken.jp

Mikael Johansson

KTH

mikaelj@ee.kth.se

Masahiro Yukawa
Keio Univ.  RIKEN

yukawa@elec.keio.ac.jp

Masashi Sugiyama
RIKEN  Univ. Tokyo

masashi.sugiyama@riken.jp

Abstract

Motivated by the success of reinforcement learning (RL) for discrete-time tasks
such as AlphaGo and Atari games  there has been a recent surge of interest in using
RL for continuous-time control of physical systems (cf. many challenging tasks
in OpenAI Gym and DeepMind Control Suite). Since discretization of time is
susceptible to error  it is methodologically more desirable to handle the system
dynamics directly in continuous time. However  very few techniques exist for
continuous-time RL and they lack ﬂexibility in value function approximation.
In this paper  we propose a novel framework for model-based continuous-time
value function approximation in reproducing kernel Hilbert spaces. The resulting
framework is so ﬂexible that it can accommodate any kind of kernel-based approach 
such as Gaussian processes and kernel adaptive ﬁlters  and it allows us to handle
uncertainties and nonstationarity without prior knowledge about the environment
or what basis functions to employ. We demonstrate the validity of the presented
framework through experiments.

1

Introduction

Reinforcement learning (RL) [37  20  35] has been successful in a variety of applications such as
AlphaGo and Atari games  particularly for discrete stochastic systems. Recently  application of RL to
physical control tasks has also been gaining attention  because solving an optimal control problem
(or the Hamilton-Jacobi-Bellman-Isaacs equation) [21] directly is computationally prohibitive for
complex nonlinear system dynamics and/or cost functions.
In the physical world  states and actions are continuous  and many dynamical systems evolve in
continuous time. OpenAI Gym [7] and DeepMind Control Suite [40] offer several representative
examples of such physical tasks. When handling continuous-time (CT) systems  CT formulations
are methodologically desirable over the use of discrete-time (DT) formulations with the small time
intervals  since such discretization is susceptible to errors. In terms of computational complexities
and the ease of analysis  CT formulations are also more advantageous over DT counterparts for
control-theoretic analyses such as stability and forward invariance [14]  which are useful for safety-
critical applications. As we will show in this paper  our framework allows to constrain control inputs
and/or states in a computationally efﬁcient way.
One of the early examples of RL for CT systems [4] pointed out that Q learning is incabable of
learning in continuous time and proposed advantage updating. Convergence proofs were given
in [25] for systems described by stochastic differential equations (SDEs) [28] using a grid-based

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Table 1: Relations to the existing approaches

Non kernel-based
Kernel-based

DT
(e.g. [5])
(e.g. [9])

DT stochastic (MDP) CT
(e.g. [44])
(e.g. [13])

(e.g. [8])
(This work)

CT stochastic
(e.g. [25])
(This work)

discretization of states and time. Stochastic differential dynamic programming and RL have also been
studied in  for example  [43  30  42]. For continuous states and actions  function approximators are
often employed instead of ﬁnely discretizing the state space to avoid the explosion of computational
complexities. The work in [8] presented an application of CT-RL by function approximators such
as Gaussian networks with ﬁxed number of basis functions. In [45]  it was mentioned that any
continuously differentiable value function (VF) can be approximated by increasing the number of
independent basis functions to inﬁnity in CT scenarios  and a CT policy iteration was proposed.
However  without resorting to the theory of reproducing kernels [3]  determining the number of
basis functions and selecting the suitable basis function class cannot be performed systematically
in general. Nonparametric learning is often desirable when no a priori knowledge about a suitable
set of basis functions for learning is available. Kernel-based methods have many non-parametric
learning algorithms  ranging from Gaussian processes (GPs) [32] to kernel adaptive ﬁlters (KFs)
[22]  which can provably deal with uncertainties and nonstationarity. While DT kernel-based
RL was studied in [29  49  41  36  26  13  27]  for example  and the Gaussian process temporal
difference (GPTD) algorithm was presented in [9]  no CT kernel-based RL has been proposed to
our knowledge. Moreover  there is no uniﬁed framework in which existing kernel methods and their
convergence/tracking analyses are straightforwardly applied to model-based VF approximation.
In this paper  we present a novel theoretical framework of model-based CT-VF approximation
in reproducing kernel Hilbert spaces (RKHSs) [3] for systems described by SDEs. The RKHS
for learning is deﬁned through one-to-one correspondence to a user-deﬁned RKHS in which the
VF being obtained is lying. We then obtain the associated kernel to be used for learning. The
resulting framework renders any kind of kernel-based methods applicable in model-based CT-VF
approximation  including GPs [32] and KFs [22]. In addition  we propose an efﬁcient barrier-certiﬁed
policy update for CT systems  which implicitly enforces state constraints. Relations of our framework
to the existing approaches for DT  DT stochastic (the Markov decision process (MDP))  CT  and
CT stochastic systems are shown in Table 1. Our proposed framework covers model-based VF
approximation working in RKHSs  including those for CT and CT stochastic systems. We verify the
validity of the framework on the classical Mountain Car problem and a simulated inverted pendulum.

2 Problem setting

Throughout  R  Z0  and Z>0 are the sets of real numbers  nonnegative integers  and strictly positive
integers  respectively. We suppose that the system dynamics described by the SDE [28] 

dx = h(x(t)  u(t))dt + ⌘(x(t)  u(t))dw 

(1)
is known or learned  where x(t) 2 Rnx  u(t) 2U⇢ Rnu  and w are the state  control  and a Brownian
motion of dimensions nx 2 Z>0  nu 2 Z>0  and nw 2 Z>0  respectively  h : Rnx ⇥U! Rnx is
the drift  and ⌘ : Rnx ⇥U! Rnx⇥nw is the diffusion. A Brownian motion can be considered as a
process noise  and is known to satisfy the Markov property [28]. Given a policy  : Rnx !U   we
deﬁne h(x) := h(x  (x)) and ⌘(x) := ⌘(x  (x))  and make the following two assumptions.
Assumption 1. For any Lipschitz continuous policy   both h(x) and ⌘(x) are Lipschitz continu-
ous  i.e.  the stochastic process deﬁned in (1) is an Itô diffusion [28  Deﬁnition 7.1.1]  which has a
pathwise unique solution for t 2 [0 1).
Assumption 2. The set X⇢ Rnx is compact with nonempty interior int(X )  and int(X ) is invariant
under the system (1) with any Lipschitz continuous policy   i.e. 
(2)
where Px(x(t) 2 int(X )) denotes the probability that x(t) lies in int(X ) when starting from
x(0) = x.

Px(x(t) 2 int(X )) = 1  8x 2 int(X )  8t  0 

2

Figure 1: An illustration of the main ideas of our proposed framework. Given a system dynamics and
an RKHS HV for the VF V   deﬁne HR under one-to-one correspondence to estimate an observable
immediate cost function in HR  and obtain V  by bringing it back to HV .

Assumption 2 implies that a solution of the system (1) stays in int(X ) with probability one. We refer
the readers to [15] for stochastic stability and invariance for SDEs.
In this paper  we consider the immediate cost function1 R : Rnx ⇥U! R  which is continuous and
satisﬁes Ex⇥R 1
0 et|R(x(t)  u(t))|dt⇤ < 1  where Ex is the expectation for all trajectories (time
evolutions of x(t)) starting from x(0) = x  and   0 is the discount factor. Note this boundedness
implies that > 0 or that there exists a zero-cost state which is stochastically asymptotically stable
[15]. Speciﬁcally  we consider the case where the immediate cost is not known a priori but is
sequentially observed. Now  the VF associated with a policy  is given by

V (x) := ExZ 1

0

etR(x(t))dt < 1 

(3)

where R(x(t)) := R(x(t)  (x(t))).
The advantages of using CT formulations include a smooth control performance and an efﬁcient
policy update  and CT formulations require no elaborative partitioning of time [8]. In addition  our
work shows that CT formulations make control-theoretic analyses easier and computationally more
efﬁcient and are more advantageous in terms of susceptibility to errors when the time interval is small.
We mention that the CT formulation can still be considered in spite of the fact that the algorithm is
implemented in discrete time.
With these problem settings in place  our goal is to estimate the CT-VF in an RKHS and improve
policies. However  since the output V (x) is unobservable and the so-called double-sampling
problem exists when approximating VFs (see e.g.  [38  16])  kernel-based supervised learning and its
analysis cannot be directly applied to VF approximation in general. Motivated by this fact  we propose
a novel model-based CT-VF approximation framework which enables us to conduct kernel-based VF
approximation as supervised learning.

3 Model-based CT-VF approximation in RKHSs

In this section  we brieﬂy present an overview of our framework; We take the following steps:

1. Select an RKHS HV which is supposed to contain V  as one of its elements.
2. Construct another RKHS HR under one-to-one correspondence to HV through a certain
bijective linear operator U : HV !H R to be deﬁned later in the next section.
3. Estimate the immediate cost function R in the RKHS HR by kernel-based supervised
learning  and return its estimate ˆR.

4. An estimate of the VF V  is immediately obtained by U1( ˆR).

An illustration of our framework is depicted in Figure 1. Note we can avoid the double-sampling
problem because the operator U is deterministic even though the system dynamics is stochastic.
Therefore  under this framework  model-based CT-VF approximation in RKHSs can be derived 
and convergence/tracking analyses of kernel-based supervised learning can also be applied to VF
approximation.

3

Algorithm 1 Model-based CT-VF Approximation in RKHSs with Barrier-Certiﬁed Policy Updates

Estimate of the VF: ˆV 
for n 2 Z0 do

n = U1( ˆR
n)

- Receive xn 2X   (xn) 2U   and R(xn  (xn)) 2 R
- Update the estimate ˆR
- Update the policy with barrier certiﬁcates when V  is well estimated

n of R by using some kernel-based method in HR

. e.g.  Section 6
. e.g.  (11)

end for

Policy update while restricting certain regions of the state space As mentioned above  one of
the advantages of a CT framework is its afﬁnity for control-theoretic analyses such as stability and
forward invariance  which are useful for safety-critical applications. For example  suppose that we
need to restrict the region of exploration in the state space to some set C := {x 2X | b(x)  0} 
where b : X! R is smooth. This is often required for safety-critical applications.
To this end  control inputs must be properly constrained so that
the state trajectory remains inside the set C. In the safe RL
context  there exists an idea of considering a smaller space of
allowable policies (see [11] and references therein). To effec-
tively constrain policies  we employ control barrier certiﬁcates
(cf. [50  48  12  46  2  1]). Without explicitly calculating the
state trajectory over a long time horizon  it is known that any
Lipschitz continuous policy satisfying control barrier certiﬁ-
cates renders the set C forward invariant [50]  i.e.  the state
trajectory remains inside the set C. In other words  we can im-
plicitly enforce state constraints by satisfying barrier certiﬁcates
when updating policies. Barrier-certiﬁed policy update was ﬁrst
introduced in [27] for DT systems  but is computationally more
efﬁcient in our CT scenario. This concept is illustrated in Fig-
ure 2  where  is the space of Lipschitz continuous policies

Figure 2: An illustration of barrier-
certiﬁed policy updates. State con-
straints are implicitly enforced via
barrier certiﬁcates.

 : X!U   and  is the space of barrier-certiﬁed allowable policies.
A brief summary of the proposed model-based CT-VF approximation in RKHSs is given in Algorithm
1. In the next section  we present theoretical analyses of our framework.

4 Theoretical analyses

We presented the motivations and an overview of our framework in the previous section. In this
section  we validate the proposed framework from theoretical viewpoints. Because the output V (x)
of the VF is unobservable  we follow the strategy presented in the previous section. First  by properly
identifying the RKHS HV which is supposed to contain the VF  we can implicitly restrict the class of
the VF. If the VF V  is twice continuously differentiable2 over int(X ) ⇢X   we obtain the following
Hamilton-Jacobi-Bellman-Isaacs equation [28]:

V (x) = G(V )(x) + R(x)  x 2 int(X ) 

where the inﬁnitesimal generator G is deﬁned as
@x2 A(x) 

tr @2V (x)

G(V )(x) := 

1
2

@V (x)

@x

h(x)  x 2 int(X ).

(4)

(5)

:= A(x  (x)) 2 Rnx⇥nx  where A(x  u) =
Here  tr stands for the trace  and A(x)
⌘(x  u)⌘(x  u)T. By employing a suitable RKHS such as a Gaussian RKHS for HV   we can guarantee
twice continuous differentiability of an estimated VF. Note that functions in a Gaussian RKHS are
smooth [23]  and any continuous function on every compact subset of Rnx can be approximated with
an arbitrary accuracy [34] in a Gaussian RKHS.

1The cost function might be obtained by the negation of the reward function.
2 See  for example  [10  Chapter IV] [19]  for more detailed arguments about the conditions under which

twice continuous differentiability is guaranteed.

4

Next  we need to construct another RKHS HR which contains the immediate cost function R as one
of its element. The relation between the VF and the immediate cost function is given by rewriting (4)
as

R(x) = [Iop + G] (V )(x)  x 2 int(X ) 

(6)
where Iop is the identity operator. To deﬁne the operator Iop + G over the whole X   we use the
following deﬁnition.
Deﬁnition 1 ([52  Deﬁnition 1]). Let Is := {↵ := [↵1 ↵ 2  . . .  ↵ nx]T 2 Znx
j=1 ↵j 
s} for s 2 Z0  nx 2 Z>0. Deﬁne D↵'(x) =
@(x1)↵1 @(x2)↵2 ...@(xnx )↵nx '(x)  where x :=
[x1  x2  . . .   xnx]T 2 Rnx. If X⇢ Rnx is compact with nonempty interior int(X )  Cs(int(X )) is
the space of functions ' over int(X ) such that D↵' is well deﬁned and continuous over int(X )
for each ↵ 2 Is. Deﬁne Cs(X ) to be the space of continuous functions ' over X such that
'|int(X ) 2 Cs(int(X )) and that D↵('|int(X )) has a continuous extension D↵' to X for each ↵ 2 Is.
If  2 C2s(X⇥X )  deﬁne (D↵)x(y) =
Now  suppose that HV is an RKHS associated with the reproducing kernel V (· ·) 2 C2⇥2(X⇥X ).
Then  we deﬁne the operator U : HV !H R := {' | '(x) = U ('V )(x)  9'V 2H V   8x 2X} as

@(x1)↵1 @(x2)↵2 ...@(xnx )↵nx (y  x)  8x  y 2 int(X ).

0 | Pnx

@Pnx

@Pnx

j=1 ↵j

j=1 ↵j

U ('V )(x) := 'V (x)  [De1'V (x)  De2'V (x)  . . .   Denx 'V (x)]h(x)
8'V 2H V   8x 2X  

m n(x)Dem+en'V (x) 

A



1
2

nxXm n=1

(7)

m n(x) is the (m  n) entry of A(x). Note here that U ('V )(x) = [Iop + G] ('V )(x) over
where A
int(X ). We emphasize here that the expected value and the immediate cost are related through the
deterministic operator U. The following main theorem states that HR is indeed an RKHS under
Assumptions 1 and 2  and its corresponding reproducing kernel is obtained.
Theorem 1. Under Assumptions 1 and 2  suppose that HV is an RKHS associated with the repro-
ducing kernel V (· ·) 2 C2⇥2(X⇥X ). Suppose also that (i) > 0  or that (ii) HV is a Gaussian
RKHS  and there exists a point xt!1 2 int(X ) which is stochastically asymptotically stable over
x(t) = xt!1⌘ = 1 for any starting point x 2 int(X ). Then  the following
int(X )  i.e.  Px⇣ lim
statements hold.
(a) The space HR := {' | '(x) = U ('V )(x)  9'V 2H V   8x 2X} is an isomorphic Hilbert
space of HV equipped with the inner product deﬁned by
 ' i(x) := U ('V

t!1

(8)

1  ' V

i )(x)  8x 2X   i 2{ 1  2} 

h'1 ' 2iHR

:=⌦'V

2↵HV

where the operator U is deﬁned in (7).
(b) The Hilbert space HR has the reproducing kernel given by

(x  y) := U (K(·  y))(x)  x  y 2X  

where

(9)

(10)

K(x  y) = V (x  y)  [(De1V )y(x)  (De2V )y(x)  . . .   (Denx V )y(x)]h(y)

1
2



nxXm n=1

A

m n(y)(Dem+enV )y(x).

Proof. See Appendices A and B in the supplementary document.
Under Assumptions 1 and 2  Theorem 1 implies that the VF V  can be uniquely determined by the
immediate cost function R for a policy  if the VF is in an RKHS of a particular class. In fact  the
relation between the VF and the immediate cost function in (4) is based on the assumption that the
VF is twice continuously differentiable over int(X )  and the veriﬁcation theorem (cf. [10]) states
that  when the immediate cost function and a twice continuously differentiable function satisfying
the relation (4) are given under certain conditions  the twice continuously differentiable function is
indeed the VF. In Theorem 1  on the other hand  we ﬁrst restrict the class of the VF by identifying

5

n (x) = U1( ˆR

T

T

T

n)(x) =Pr

an RKHS HV   and then approximate the immediate cost function in the RKHS HR any element of
which satisﬁes the relation (4). Because the immediate cost R(x(t)) is observable  we can employ
any kernel-based supervised learning to estimate the function R in the RKHS HR  such as GPs and
KFs  as elaborated later in Section 6.
In the RKHS HR  an estimate of R at time instant n 2 Z0 is given by ˆR
n(x) =
Pr
i ci(x  xi)  ci 2 R  r 2 Z0  where {xi}i2{1 2 ... r} ⇢X is the set of samples  and the
reproducing kernel  is deﬁned in (9). An estimate of the VF V  at time instant n 2 Z0 is thus
immediately obtained by ˆV 
i=1 ciK(x  xi)  where K is deﬁned in (10).
Note  when the system dynamics is described by an ordinary differential equation (i.e.  ⌘ = 0) 
the assumptions that V  is twice continuously differentiable and that V (· ·) 2 C2⇥2(X⇥X ) are
relaxed to that V  is continuously differentiable and that V (· ·) 2 C2⇥1(X⇥X )  respectively.
As an illustrative example of Theorem 1  we show the case of the linear-quadratic regulator (LQR)
below.
Special case: linear-quadratic regulator Consider a linear feedback LQR  i.e.  LQR(x) =
FLQRx  FLQR 2 Rnu⇥nx  and a linear system ˙x := dx
dt = ALQRx + BLQRu  where ALQR 2
Rnx⇥nx and BLQR 2 Rnx⇥nu are matrices. In this case  we know that the value function V LQR
becomes quadratic with respect to the state variable (cf. [53]). Therefore  we employ an RKHS
with a quadratic kernel for HV   i.e.  V (x  y) = (xTy)2. If we assume that the input space X is so
large that the set span{Asym|Asym = xxT  9x 2X} accommodates any real symmetric matrix  we
obtain HV = {X 3 x 7! xTAsymx|Asym is symmetric}.
Moreover  it follows from the product rule of the directional derivative [6] that K(x  y) =
LQR)x  where ALQR := ALQR 
xTALQRyxTy  xTyxTALQRy = xT(ALQRyyT  yyTA
LQR is symmetric  implying K(·  y) 2H V  
BLQRFLQR. Note Avalue(y) := ALQRyyT  yyTA
T
and we obtain (x  y) = xT(A
LQRAvalue(y) + Avalue(y)ALQR)x. Because Acost(y) :=
LQRAvalue(y)  Avalue(y)ALQR is symmetric  it follows that (·  y) 2H V . If ALQR is stable
A
(Hurwitz)  from Theorem 1  the one-to-one correspondence between HV and HR thus implies that
HV = HR. Therefore  we can fully approximate the immediate cost function RLQR in HR if RLQR
is quadratic with respect to the state variable.
Suppose that
i=1 ci(x  xi) =
xTAcostx. Then  the estimated value function will be given by V LQR(x) = U1(RLQR)(x) =
T
LQRAvalue + AvalueALQR + Acost = 0  which is indeed
the well-known Lyapunov equation [53]. Unlike Gaussian-kernel cases  we only require a ﬁnite
number of parameters to fully approximate the immediate cost function  and hence is analytically
obtainable.
Barrier-certiﬁed policy updates under CT formulation Next  we show that the CT formula-
tion makes barrier-certiﬁed policy updates computationally more efﬁcient under certain condi-
tions. Assume that the system dynamics is afﬁne in the control  i.e.  h(x  u) = f (x) + g(x)u 
and ⌘ = 0  where f : Rnx ! Rnx and g : Rnx ! Rnx⇥nu  and that the immediate cost
R(x  u) is given by Q(x) + 1
2 uTM u  where Q : Rnx ! R  and M 2 Rnu⇥nu is a positive
deﬁnite matrix. Then  any Lipschitz continuous policy  : X!U
satisfying (x) 2S (x) :=
@x g(x)u + ↵(b(x))  0o renders the set C forward invariant [50]  i.e.  the
nu 2U | @b(x)
state trajectory remains inside the set C  where ↵ : R ! R is strictly increasing and ↵(0) = 0. Taking
this constraint into account  the barrier-certiﬁed greedy policy update is given by

the immediate cost function is given by RLQR(x) = Pr

Pr
i=1 ciK(x  xi) = xTAvaluex  where A

@x f (x) + @b(x)

+(x) = argmin

u2S(x)  1

2

uTM u +

@V (x)

@x

g(x)u  

(11)

which is  by virtue of the CT formulation  a quadratic programming (QP) problem at x when U⇢ Rnu
deﬁnes afﬁne constraints (see Appendix C in the supplementary document). The space of allowable
policies is thus given by := { 2  | (x) 2S (x)  8x 2X} . When ⌘ 6= 0 and the dynamics is
learned by GPs as in [30]  the work in [47] provides a barrier certiﬁcate for uncertain dynamics. Note 
one can also employ a function approximator or add noises to the greedily updated policy to avoid

6

unstable performance and promote exploration (see e.g.  [8]). To see if the updated policy + remains
in the space of Lipschitz continuous policies   i.e.   ⇢   we present the following proposition.
Proposition 1. Assume the conditions in Theorem 1. Assume also that U⇢ Rnu deﬁnes afﬁne
constraints  and that f  g  ↵  and the derivative of b are Lipschitz continuous over X . Then  the policy
+ deﬁned in (11) is Lipschitz continuous over X if the width of a feasible set3 is strictly larger than
zero over X .
Proof. See Appendix D in the supplementary document.
Note  if U⇢ Rnx deﬁnes the bounds of each entry of control inputs  it deﬁnes afﬁne constraints.
Lastly  the width of a feasible set is strictly larger than zero if U is sufﬁciently large and @b(x)
@x g(x) 6= 0.
We will further clarify the relations of the proposed theoretical framework to existing works below.

5 Relations to existing works

First  our proposed framework takes advantage of the capability of learning complicated functions
and nonparametric ﬂexibility of RKHSs  and reproduces some of the existing model-based DT-VF
approximation techniques (see Appendix E in the supplementary document). Note that some of the
existing DT-VF approximations in RKHSs  such as GPTD [9]  also work for model-free cases (see
[27] for model-free adaptive DT action-value function approximation  for example). Second  since
the RKHS HR for learning is explicitly deﬁned in our framework  any kernel-based method and its
convergence/tracking analyses are directly applicable. While  for example  the work in [17]  which
aims at attaining a sparse representation of the unknown function in an online fashion in RKHSs  was
extended to the policy evaluation [18] by addressing the double-sampling problem  our framework
does not suffer from the double-sampling problem  and hence any kernel-based online learning (e.g. 
[17  51  39]) can be straightforwardly applied. Third  when the time interval is small  DT formulations
become susceptible to errors  while CT formulations are immune to the choice of the time interval.
Note  on the other hand  a larger time interval poorly represents the system dynamics evolving in
continuous time. Lastly  barrier certiﬁcates are efﬁciently incorporated in our CT framework through
QPs under certain conditions  and state constraints are implicitly taken into account. Stochastic
optimal control such as the work in [43  42] requires sample trajectories over predeﬁned ﬁnite time
horizons and the value is computed along the trajectories while the VF is estimated in an RKHS even
without having to follow the trajectory in our framework.

6 Applications and practical implementation

Rnx

(2⇡2)L/2 exp kx  yk2

22

1

We apply the theory presented in the previous section to the Gaussian kernel case and introduce CTGP
as an example  and present a practical implementation. Assume that A(x  u) 2 Rnx⇥nx is diagonal 
! 
for simplicity. The Gaussian kernel is given by V (x  y) :=
x  y 2X   > 0. Given Gaussian kernel V (x  y)  the reproducing kernel (x  y) deﬁned in (9) is
derived as (x  y) = a(x  y)V (x  y)  where a : X⇥X! R is a real-valued function (see Appendix
F in the supplementary document for the explicit form of a(x  y)).
CTGP One of the celebrated properties of GPs is their Bayesian formulation  which enables us
to deal with uncertainty through credible intervals. Suppose that the observation d at time instant
n 2 Z0 contains some noise ✏ 2 R  i.e.  d(x) = R(x) + ✏  ✏ ⇠N (0  µ2
o)  µo  0. Given data
dN := [d(x0)  d(x1)  . . .   d(xN )]T for some N 2 Z0  we can employ GP regression to obtain the
mean m(x⇤) and the variance µ2(x⇤) of ˆR(x⇤) at a point x⇤ 2X as
µ2(x⇤) = (x⇤  x⇤)  kT

(12)
where I is the identity matrix  k⇤ := [(x⇤  x0)  (x⇤  x1)  . . .   (x⇤  xN )]T  and the (i  j) entry of
G 2 R(N +1)⇥(N +1) is (xi1  xj1). Then  by the existence of the inverse operator U1  the mean
mV (x⇤) and the variance µV 2(x⇤) of ˆV (x⇤) at a point x⇤ 2X can be given by
(G + µ2

m(x⇤) = kT

⇤ (G + µ2

⇤ (G + µ2

T

(G + µ2

oI)1dN   µV 2

oI)1k⇤ 

oI)1dN  

mV (x⇤) = KV
⇤

(x⇤) = V (x⇤  x⇤)  KV
⇤

T

oI)1KV
⇤  

(13)

3Width indicates how much control margin is left for the strictest constraint  as deﬁned in [24  Equation 21].

7

Table 2: Comparisons of the cumulative costs and numbers of times the observed velocities became
lower than 0.05 with and without barrier certiﬁcates

Cumulative cost
With barrier
Without barrier

CTKF
114.2
0 (times)
0 (times)

GPTD_1 DTKF_1 CTGP
299.1
0 (times)
0 (times)

299.1
0 (times)
0 (times)

82.2
0 (times)
10 (times)

GPTD_20 DTKF_20
89.2
0 (times)
20 (times)

90.4
0 (times)
20 (times)

where KV
⇤
document for more details).

:= [K(x⇤  x0)  K(x⇤  x1)  . . .   K(x⇤  xN )]T (see Appendix G in the supplementary

7 Numerical Experiment

0

v(t)

0.0025 cos (3x(t)) dt +

In this section  we ﬁrst show the validity of the proposed CT framework and its advantage over
DT counterparts when the time interval is small  and then compare CTGP and GPTD for RL on a
simulated inverted pendulum. In both of the experiments  the coherence-based sparsiﬁcation [33] in
the RKHS HR is employed to curb the growth of the dictionary size.
Policy evaluations:
comparison of CT and DT approaches We show that our CT ap-
proaches are advantageous over DT counterparts in terms of susceptibility to errors  by using
MountainCarContinuous-v0 in OpenAI Gym [7] as the environment. The state is given by
x(t) := [x(t)  v(t)]T 2 R2  where x(t) and v(t) are the position and the velocity of the car  and the dy-
0.0015 u(t)dt  where u(t) 2 [1.0  1.0].
namics is given by dx =
The position and the velocity are clipped to [0.07  0.07] and [1.2  0.6]  respectively  and the
goal is to reach the position x = 0.45. In the simulation  the control cycle (i.e.  the frequency
of applying control inputs and observing the states and costs) is set to 1.0 second. The ob-
served immediate cost is given by R(x(t)  u(t)) + ✏ = 1 + 0.001u2(t) + ✏ for x(t) < 0.45 and
R(x(t)  u(t)) + ✏ = 0.001u2(t) + ✏ for x(t)  0.45  where ✏ ⇠N (0  0.12). Note the immediate
cost for the DT cases is given by (R(x(t)  u(t)) + ✏)t  where t is the time interval. For policy
evaluations  we use the policy obtained by RL based on the cross-entropy methods4  and the four meth-
ods  CTGP  KF-based CT-VF approximation (CTKF)  GPTD  and KF-based DT-VF approximation
(DTKF)  are used to learn value functions associated with the policy by executing the current policy
for ﬁve episodes  each of which terminates whenever t = 300 or x(t)  0.45. GP-based techniques
are expected to handle the random component ✏ added to the immediate cost. The new policies are
then obtained by the barrier-certiﬁed policy updates under CT formulations  and these policies are
evaluated for ﬁve times. Here  the barrier function is given by b(x) = 0.05 + v  which prevents the
velocity from becoming lower than 0.05. Figure 3 compares the value functions5 learned by each
method for the time intervals t = 20.0 and t = 1.0. We observe that the DT approaches are
sensitive to the choice of t. Table 2 compares the cumulative costs averaged over ﬁve episodes for
each method and for different time intervals and the numbers of times we observed the velocity being
lower than 0.05 when the barrier certiﬁcate is employed and unemployed. (Numbers associated
with the algorithm names indicate the lengths of the time intervals.) Note that the cumulative costs
are calculated by summing up the immediate costs multiplied by the duration of each control cycle 
i.e.  we discretized the immediate cost based on the control cycle. It is observed that the CT approach
is immune to the choice of t while the performance of the DT approach degrades when the time
interval becomes small  and that the barrier-certiﬁed policy updates work efﬁciently.
Reinforcement learning: inverted pendulum We show the advantage of CTGP over GPTD when
the time interval for the estimation is small. Let the state x(t) := [✓(t) ! (t)]T 2 R2 consists of the
angle ✓(t) and the angular velocity !(t) of an inverted pendulum  and we consider the dynamics:
m`2 u(t)dt + 0.01Idw  where g = 9.8  m = 1 ` =
dx = 

m`2 !(t) dt + 0

1 ⇢ = 0.01. The Brownian motion may come from outer disturbances and/or modeling error. In
the simulation  the time interval t is set to 0.01 seconds  and the simulated dynamics evolves

g

` sin(✓(t))  ⇢

!(t)

1

4We used the

code

in https://github.com/udacity/deep-reinforcement-learning/blob/master/cross-

entropy/CEM.ipynb offered by Udacity. The code is based on PyTorch [31].

5We used "jet colormaps" in Python Matplotlib for illustrating the value functions.

8

(a) GPTD for t = 20.0

(b) GPTD for t = 1.0

(c) CTGP

(d) DTKF for t = 20.0

(e) DTKF for t = 1.0

(f) CTKF

Figure 3: Illustrations of the value functions obtained by CTGP  CTKF  GPTD  and DTKF for time
intervals t = 20.0 and t = 1.0. The policy is obtained by RL based on the cross-entropy method.
CT approaches are not affected by the choice of t.

by x = h(x(t)  u(t))t + pt⌘(x(t)  u(t))✏w  where ✏w ⇠N (0  I). In this experiment  the

task is to stabilize the inverted pendulum at ✓ = 0. The observed immediate cost is given by
R(x(t)  u(t)) + ✏ = 1/(1 + e10(✓(t)⇡/16)) + 100/(1 + e10(✓(t)⇡/6)) + 0.05u2(t) + ✏  where
✏ ⇠N (0  0.12). A trajectory associated with the current policy is generated to learn the VF. The
trajectory terminates when |✓(t)| >⇡/ 4 and restarts from a random initial angle. After 10 seconds 
the policy is updated. To evaluate the current policy  average time over ﬁve episodes in which the
pendulum stays up (|✓(t)| ⇡/4) when initialized as ✓(0) 2 [⇡/6 ⇡/ 6] is used. Figure 4 compares
this average time of CTGP and GPTD up to ﬁve updates with standard deviations until when stable
policy improvement becomes difﬁcult without some heuristic techniques such as adding noises to
policies. Note that the same seed for the random number generator is used for the initializations of
both of the two approaches. It is observed that GPTD fails to improve policies. The large credible
interval of CTGP is due to the random initialization of the state.

8 Conclusion and future work

We presented a novel theoretical framework that renders the
CT-VF approximation problem simultaneously solvable in an
RKHS by conducting kernel-based supervised learning for the
immediate cost function in the properly deﬁned RKHS. Our CT
framework is compatible with rich theories of control  including
control barrier certiﬁcates for safety-critical applications. The
validity of the proposed framework and its advantage over
DT counterparts when the time interval is small were veriﬁed
experimentally on the classical Mountain Car problem and a
simulated inverted pendulum.
There are several possible directions to explore as future works;
First  we can employ the state-of-the-art kernel methods within
our theoretical framework or use other variants of RL  such as
actor-critic methods  to improve practical performances. Sec-
ond  we can consider uncertainties in value function approxima-
tion by virtue of the RKHS-based formulation  which might be
used for safety veriﬁcations. Lastly  it is worth further explorations of advantages of CT formulations
for physical tasks.

Figure 4: Comparison of time up
to which the pendulum stays up be-
tween CTGP and GPTD for the in-
verted pendulum (± std. deviation).

9

Acknowledgments
This work was partially conducted when M. Ohnishi was at the GRITS Lab  Georgia Institute of
Technology; M. Ohnishi thanks the members of the GRITS Lab  including Dr. Li Wang  and Prof.
Magnus Egerstedt for discussions regarding barrier functions. M. Yukawa was supported in part by
KAKENHI 18H01446 and 15H02757  M. Johansson was supported in part by the Swedish Research
Council and by the Knut and Allice Wallenberg Foundation  and M. Sugiyama was supported in part
by KAKENHI 17H00757. Lastly  the authors thank all of the anonymous reviewers for their very
insightful comments.

References

[1] A. Agrawal and K. Sreenath. “Discrete control barrier functions for safety-critical control of

discrete systems with application to bipedal robot navigation”. In: Proc. RSS. 2017.

[2] A. D. Ames et al. “Control barrier function based quadratic programs with application to

automotive safety systems”. In: arXiv preprint arXiv:1609.06408 (2016).

[3] N. Aronszajn. “Theory of reproducing kernels”. In: Trans. Amer. Math. Soc. 68.3 (1950) 

pp. 337–404.

[4] L. Baird. “Reinforcement

learning in continuous time: Advantage updating”.

In:

Proc. IEEE ICNN. Vol. 4. 1994  pp. 2448–2453.

[5] L. Baird. “Residual algorithms: Reinforcement learning with function approximation”. In:

Proc. ICML. 1995  pp. 30–37.
J. Bonet and R. D. Wood. Nonlinear continuum mechanics for ﬁnite element analysis. Cam-
bridge University Press  1997.

[6]

[7] G. Brockman et al. “OpenAI Gym”. In: arXiv preprint arXiv:1606.01540 (2016).
[8] K. Doya. “Reinforcement learning in continuous time and space”. In: Neural Computation

[9] Y. Engel  S. Mannor  and R. Meir. “Reinforcement learning with Gaussian processes”. In:

12.1 (2000)  pp. 219–245.

Proc. ICML. 2005  pp. 201–208.

[10] W. H. Fleming and H. M. Soner. Controlled Markov processes and viscosity solutions. Vol. 25.

Springer Science & Business Media  2006.
J. Garcıa and F. Fernández. “A comprehensive survey on safe reinforcement learning”. In: J.
Mach. Learn. Res. 16.1 (2015)  pp. 1437–1480.

[11]

[12] P. Glotfelter  J. Cortés  and M. Egerstedt. “Nonsmooth barrier functions with applications to

multi-robot systems”. In: IEEE Control Systems Letters 1.2 (2017)  pp. 310–315.

[13] S. Grunewalder et al. “Modelling transition dynamics in MDPs with RKHS embeddings”. In:

Proc. ICML. 2012.

Business Media  2011.

[14] H. K Khalil. “Nonlinear systems”. In: Prentice-Hall 3 (1996).
[15] R. Khasminskii. Stochastic stability of differential equations. Vol. 66. Springer Science &

[16] V. R. Konda  J. N. Tsitsiklis  et al. “Convergence rate of linear two-time-scale stochastic

approximation”. In: The Annals of Applied Probability 14.2 (2004)  pp. 796–819.

[17] A. Koppel et al. “Parsimonious online learning with kernels via sparse projections in function

space”. In: Proc. IEEE ICASSP. 2017  pp. 4671–4675.

[18] A. Koppel et al. “Policy evaluation in continuous MDPs with efﬁcient kernelized gradient

temporal difference”. In: IEEE Trans. Automatic Control (submitted) (2017).

[19] N. V. Krylov. Controlled diffusion processes. Vol. 14. Springer Science & Business Media 

2008.

[20] F. L. Lewis and D. Vrabie. “Reinforcement learning and adaptive dynamic programming for

feedback control”. In: IEEE Circuits and Systems Magazine 9.3 (2009).

[21] D. Liberzon. Calculus of variations and optimal control theory: a concise introduction. Prince-

ton University Press  2011.

[22] W. Liu  J. Príncipe  and S. Haykin. Kernel adaptive ﬁltering. New Jersey: Wiley  2010.
[23] H. Q. Minh. “Some properties of Gaussian reproducing kernel Hilbert spaces and their impli-
cations for function approximation and learning theory”. In: Constructive Approximation 32.2
(2010)  pp. 307–338.

10

[24] B. Morris  M. J. Powell  and A. D. Ames. “Sufﬁcient conditions for the Lipschitz continuity of
QP-based multi-objective control of humanoid robots”. In: Proc. CDC. 2013  pp. 2920–2926.
[25] R. Munos and P. Bourgine. “Reinforcement learning for continuous stochastic control prob-

lems”. In: Proc. NIPS. 1998  pp. 1029–1035.

[26] Y. Nishiyama et al. “Hilbert space embeddings of POMDPs”. In: arXiv preprint

arXiv:1210.4887 (2012).

[27] M. Ohnishi et al. “Barrier-certiﬁed adaptive reinforcement learning with applications to

brushbot navigation”. In: arXiv preprint arXiv:1801.09627 (2018).

[28] B. Øksendal. Stochastic differential equations. Springer  2003.
[29] D. Ormoneit and ´S. Sen. “Kernel-based reinforcement learning”. In: Machine Learning 49.2

(2002)  pp. 161–178.

[30] Y. Pan and E. Theodorou. “Probabilistic differential dynamic programming”. In: Proc. NIPS.

2014  pp. 1907–1915.

[31] A. Paszke et al. “Automatic differentiation in PyTorch”. In: (2017).
[32] C. E. Rasmussen and C. K. Williams. Gaussian processes for machine learning. Vol. 1. MIT

press Cambridge  2006.

[33] C. Richard  J. Bermudez  and P. Honeine. “Online prediction of time series data with kernels”.

In: IEEE Trans. Signal Process. 57.3 (2009)  pp. 1058–1067.
I. Steinwart. “On the inﬂuence of the kernel on the consistency of support vector machines”.
In: J. Mach. Learn. Res. 2 (2001)  pp. 67–93.

[34]

[35] M. Sugiyama. Statistical reinforcement learning: modern machine learning approaches. CRC

Press  2015.

[36] W. Sun and J. A. Bagnell. “Online Bellman residual and temporal difference algorithms with

predictive error guarantees”. In: Proc. IJCAI. 2016.

[37] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT Press  1998.
[38] R. S. Sutton  H. R. Maei  and C. Szepesvári. “A convergent O(n) temporal-difference algorithm
for off-policy learning with linear function approximation”. In: Advances in Neural Information
Processing Systems. 2009  pp. 1609–1616.

[39] M. Takizawa and M. Yukawa. “Adaptive nonlinear estimation based on parallel projection
along afﬁne subspaces in reproducing kernel Hilbert space”. In: IEEE Trans. Signal Processing
63.16 (2015)  pp. 4257–4269.

[40] Y. Tassa et al. “DeepMind Control Suite”. In: arXiv preprint arXiv:1801.00690 (2018).
[41] G. Taylor and R. Parr. “Kernelized value function approximation for reinforcement learning”.

In: Proc. ICML. 2009  pp. 1017–1024.

[42] E. Theodorou  J. Buchli  and S. Schaal. “Reinforcement learning of motor skills in high

dimensions: A path integral approach”. In: Proc. IEEE ICRA. 2010  pp. 2397–2403.

[43] E. Theodorou  Y. Tassa  and E. Todorov. “Stochastic differential dynamic programming”. In:

Proc. IEEE ACC. 2010  pp. 1125–1132.
J. N. Tsitsiklis and B. Van R. “Analysis of temporal-diffference learning with function approxi-
mation”. In: Proc. NIPS. 1997  pp. 1075–1081.

[44]

[45] K. G. Vamvoudakis and F. L. Lewis. “Online actor-critic algorithm to solve the continuous-time

inﬁnite horizon optimal control problem”. In: Automatica 46.5 (2010)  pp. 878–888.

[46] L. Wang  A. D. Ames  and M. Egerstedt. “Safety barrier certiﬁcates for collisions-free multi-

robot systems”. In: IEEE Trans. Robotics (2017).

[47] L. Wang  E. A. Theodorou  and M. Egerstedt. “Safe learning of quadrotor dynamics using

barrier certiﬁcates”. In: arXiv preprint arXiv:1710.05472 (2017).

[48] P. Wieland and F. Allgöwer. “Constructive safety using control barrier functions”. In:

Proc. IFAC 40.12 (2007)  pp. 462–467.

[49] X. Xu  D. Hu  and X. Lu. “Kernel-based least squares policy iteration for reinforcement

learning”. In: IEEE Trans. Neural Networks 18.4 (2007)  pp. 973–992.

[50] X. Xu et al. “Robustness of control barrier functions for safety critical control”. In: Proc. IFAC

48.27 (2015)  pp. 54–61.

[51] M. Yukawa. “Multikernel Adaptive Filtering”. In: IEEE Trans. Signal Processing 60.9 (2012) 

pp. 4672–4682.

11

[52] DX. Zhou. “Derivative reproducing properties for kernel methods in learning theory”. In:

Journal of Computational and Applied Mathematics 220.1-2 (2008)  pp. 456–463.

[53] K. Zhou  J. C. Doyle  K. Glover  et al. Robust and optimal control. Vol. 40. Prentice Hall 

1996.

12

,Moritz Hardt
Eric Price
Motoya Ohnishi
Masahiro Yukawa
Mikael Johansson
Masashi Sugiyama