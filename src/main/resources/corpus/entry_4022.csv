2013,One-shot learning by inverting a compositional causal process,People can learn a new visual class from just one example  yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts  generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classification task  where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also used a visual Turing test" to show that our model produces human-like performance on other conceptual tasks  including generating new examples and parsing.",One-shot learning by inverting a compositional causal

process

Brenden M. Lake

Ruslan Salakhutdinov

Dept. of Brain and Cognitive Sciences

Dept. of Statistics and Computer Science

MIT

brenden@mit.edu

University of Toronto

rsalakhu@cs.toronto.edu

Joshua B. Tenenbaum

Dept. of Brain and Cognitive Sciences

MIT

jbt@mit.edu

Abstract

People can learn a new visual class from just one example  yet machine learn-
ing algorithms typically require hundreds or thousands of examples to tackle the
same problems. Here we present a Hierarchical Bayesian model based on com-
positionality and causality that can learn a wide range of natural (although sim-
ple) visual concepts  generalizing in human-like ways from just one image. We
evaluated performance on a challenging one-shot classiﬁcation task  where our
model achieved a human-level error rate while substantially outperforming two
deep learning models. We also tested the model on another conceptual task  gen-
erating new examples  by using a “visual Turing test” to show that our model
produces human-like performance.

Introduction

1
People can acquire a new concept from only the barest of experience – just one or a handful of
examples in a high-dimensional space of raw perceptual input. Although machine learning has
tackled some of the same classiﬁcation and recognition problems that people solve so effortlessly 
the standard algorithms require hundreds or thousands of examples to reach good performance.
While the standard MNIST benchmark dataset for digit recognition has 6000 training examples per
class [19]  people can classify new images of a foreign handwritten character from just one example
(Figure 1b) [23  16  17]. Similarly  while classiﬁers are generally trained on hundreds of images per
class  using benchmark datasets such as ImageNet [4] and CIFAR-10/100 [14]  people can learn a

Figure 1: Can you learn a new concept from just one example? (a & b) Where are the other examples of the
concept shown in red? Answers for b) are row 4 column 3 (left) and row 2 column 4 (right). c) The learned
concepts also support many other abilities such as generating examples and parsing.

1

123Human drawerscanonical12351235.11235.31236.21236.22137.12137.43128.21238.413212123Simple drawerscanonical1326.5213241231732120213113121012323321171231631220b)c)a)Figure 2: Four alphabets from Omniglot  each with ﬁve characters drawn by four different people.

new visual object from just one example (e.g.  a “Segway” in Figure 1a). These new larger datasets
have developed along with larger and “deeper” model architectures  and while performance has
steadily (and even spectacularly [15]) improved in this big data setting  it is unknown how this
progress translates to the “one-shot” setting that is a hallmark of human learning [3  22  28].
Additionally  while classiﬁcation has received most of the attention in machine learning  people
can generalize in a variety of other ways after learning a new concept. Equipped with the concept
“Segway” or a new handwritten character (Figure 1c)  people can produce new examples  parse an
object into its critical parts  and ﬁll in a missing part of an image. While this ﬂexibility highlights the
richness of people’s concepts  suggesting they are much more than discriminative features or rules 
there are reasons to suspect that such sophisticated concepts would be difﬁcult if not impossible
to learn from very sparse data. Theoretical analyses of learning express a tradeoff between the
complexity of the representation (or the size of its hypothesis space) and the number of examples
needed to reach some measure of “good generalization” (e.g.  the bias/variance dilemma [8]). Given
that people seem to succeed at both sides of the tradeoff  a central challenge is to explain this
remarkable ability: What types of representations can be learned from just one or a few examples 
and how can these representations support such ﬂexible generalizations?
To address these questions  our work here offers two contributions as initial steps. First  we introduce
a new set of one-shot learning problems for which humans and machines can be compared side-by-
side  and second  we introduce a new algorithm that does substantially better on these tasks than
current algorithms. We selected simple visual concepts from the domain of handwritten characters 
which offers a large number of novel  high-dimensional  and cognitively natural stimuli (Figure
2). These characters are signiﬁcantly more complex than the simple artiﬁcial stimuli most often
modeled in psychological studies of concept learning (e.g.  [6  13])  yet they remain simple enough
to hope that a computational model could see all the structure that people do  unlike domains such
as natural scenes. We used a dataset we collected called “Omniglot” that was designed for studying
learning from a few examples [17  26]. While similar in spirit to MNIST  rather than having 10
characters with 6000 examples each  it has over 1600 character with 20 examples each – making it
more like the “transpose” of MNIST. These characters were selected from 50 different alphabets on
www.omniglot.com  which includes scripts from natural languages (e.g.  Hebrew  Korean  Greek)
and artiﬁcial scripts (e.g.  Futurama and ULOG) invented for purposes like TV shows or video
games. Since it was produced on Amazon’s Mechanical Turk  each image is paired with a movie
([x y time] coordinates) showing how that drawing was produced.
In addition to introducing new one-shot learning challenge problems  this paper also introduces
Hierarchical Bayesian Program Learning (HBPL)  a model that exploits the principles of composi-
tionality and causality to learn a wide range of simple visual concepts from just a single example. We
compared the model with people and other competitive computational models for character recog-
nition  including Deep Boltzmann Machines [25] and their Hierarchical Deep extension for learning
with very few examples [26]. We ﬁnd that HBPL classiﬁes new examples with near human-level
accuracy  substantially beating the competing models. We also tested the model on generating new
exemplars  another natural form of generalization  using a “visual Turing test” to evaluate perfor-
mance. In this test  both people and the model performed the same task side by side  and then other
human participants judged which result was from a person and which was from a machine.
2 Hierarchical Bayesian Program Learning
We introduce a new computational approach called Hierarchical Bayesian Program Learning
(HBPL) that utilizes the principles of compositionality and causality to build a probabilistic gen-
erative model of handwritten characters.
It is compositional because characters are represented
as stochastic motor programs where primitive structure is shared and re-used across characters at
multiple levels  including strokes and sub-strokes. Given the raw pixels  the model searches for a

2

Figure 3: An illustration of the HBPL model generating two character types (left and right)  where the dotted
line separates the type-level from the token-level variables. Legend: number of strokes κ  relations R  primitive
id z (color-coded to highlight sharing)  control points x (open circles)  scale y  start locations L  trajectories T  
transformation A  noise  and θb  and image I.
“structural description” to explain the image by freely combining these elementary parts and their
spatial relations. Unlike classic structural description models [27  2]  HBPL also reﬂects abstract
causal structure about how characters are actually produced. This type of causal representation
is psychologically plausible  and it has been previously theorized to explain both behavioral and
neuro-imaging data regarding human character perception and learning (e.g.  [7  1  21  11  12  17]).
As in most previous “analysis by synthesis” models of characters  strokes are not modeled at the
level of muscle movements  so that they are abstract enough to be completed by a hand  a foot  or
an airplane writing in the sky. But HBPL also learns a signiﬁcantly more complex representation
than earlier models  which used only one stroke (unless a second was added manually) [24  10] or
received on-line input data [9]  sidestepping the challenging parsing problem needed to interpret
complex characters.
The model distinguishes between character types (an ‘A’  ‘B’  etc.) and tokens (an ‘A’ drawn by a
particular person)  where types provide an abstract structural speciﬁcation for generating different
tokens. The joint distribution on types ψ  tokens θ(m)  and binary images I (m) is given as follows 

M(cid:89)

m=1

κ(cid:89)

i=1

P (ψ  θ(1)  ...  θ(M )  I (1)  ...  I (M )) = P (ψ)

P (I (m)|θ(m))P (θ(m)|ψ).

(1)

Pseudocode to generate from this distribution is shown in the Supporting Information (Section SI-1).

2.1 Generating a character type
A character type ψ = {κ  S  R} is deﬁned by a set of κ strokes S = {S1  ...  Sκ} and spatial relations
R = {R1  ...  Rκ} between strokes. The joint distribution can be written as

P (ψ) = P (κ)

P (Si)P (Ri|S1  ...  Si−1).

(2)

The number of strokes is sampled from a multinomial P (κ) estimated from the empirical frequencies
(Figure 4b)  and the other conditional distributions are deﬁned in the sections below. All hyperpa-
rameters  including the library of primitives (top of Figure 3)  were learned from a large “background
set” of character drawings as described in Sections 2.3 and SI-4.
Strokes. Each stroke is initiated by pressing the pen down and terminated by lifting the
pen up.
In between  a stroke is a motor routine composed of simple movements called sub-
strokes Si = {si1  ...  sini} (colored curves in Figure 3)  where sub-strokes are separated by

3

x(m)11x11y11}}y12x12R1x21y21R2R1R2y(m)11L(m)1y(m)12L(m)2T(m)1T(m)2{A ✏ b}(m)I(m)x(m)21y(m)21x(m)12{A ✏ b}(m)I(m)L(m)1T(m)1L(m)2T(m)2=independent=alongs11=independent=startofs11z11=17z12=17z21=42z11=5z21=17charactertype1(=2)charactertype2(=2)...172542157primitivesx11y11x(m)11y(m)11x21y21x(m)21y(m)21tokenlevel✓(m)typelevel R(m)1R(m)2R(m)1R(m)2P (zi)(cid:81)ni
brief pauses of the pen. Each sub-stroke sij is modeled as a uniform cubic b-spline  which
tive motor elements (top of Figure 3)  and its distribution P (zi) = P (zi1)(cid:81)ni
can be decomposed into three variables sij = {zij  xij  yij} with joint distribution P (Si) =
j=1 P (xij|zij)P (yij|zij). The discrete class zij ∈ N is an index into the library of primi-
j=2 P (zij|zi(j−1)) is a
ﬁrst-order Markov Process that adds sub-strokes at each step until a special “stop” state is sampled
that ends the stroke. The ﬁve control points xij ∈ R10 (small open circles in Figure 3) are sampled
from a Gaussian P (xij|zij) = N (µzij   Σzij )   but they live in an abstract space not yet embedded
in the image frame. The type-level scale yij of this space  relative to the image frame  is sampled
from P (yij|zij) = Gamma(αzij   βzij ).
Relations. The spatial relation Ri speciﬁes how the beginning of stroke Si connects to the pre-
vious strokes {S1  ...  Si−1}. The distribution P (Ri|S1  ...  Si−1) = P (Ri|z1  ...  zi−1)  since it
only depends on the number of sub-strokes in each stroke. Relations can come in four types with
probabilities θR  and each type has different sub-variables and dimensionalities:

• Independent relations  Ri = {Ji  Li}  where the position of stroke i does not depend on previ-
ous strokes. The variable Ji ∈ N is drawn from P (Ji)  a multinomial over a 2D image grid that
depends on index i (Figure 4c). Since the position Li ∈ R2 has to be real-valued  P (Li|Ji) is
then sampled uniformly at random from within the image cell Ji.
• Start or End relations  Ri = {ui}  where stroke i starts at either the beginning or end of a
previous stroke ui  sampled uniformly at random from ui ∈ {1  ...  i − 1}.
• Along relations  Ri = {ui  vi  τi}  where stroke i begins along previous stroke ui ∈ {1  ...  i −
1} at sub-stroke vi ∈ {1  ...  nui} at type-level spline coordinate τi ∈ R  each sampled uni-
formly at random.

2.2 Generating a character token

(cid:89)

i

The token-level variables  θ(m) = {L(m)  x(m)  y(m)  R(m)  A(m)  σ(m)
P (θ(m)|ψ) = P (L(m)|θ(m)
|yi)P (x(m)

|Ri)P (y(m)

\L(m)   ψ)

P (R(m)

b

i

i

i

  (m)}  are distributed as
|xi)P (A(m)  σ(m)
  (m))

b

i

(3)
with details below. As before  Sections 2.3 and SI-4 describe how the hyperparameters were learned.
Pen trajectories. A stroke trajectory T (m)
that represents the path of the pen. Each trajectory T (m)
function of a starting location L(m)
scale y(m)
i
P (x(m)
0. To construct the trajectory T (m)
control points y(m)
to begin at L(m)
the previous sub-stroke’s trajectory  and so on until all sub-strokes are placed.

(Figure 3) is a sequence of points in the image plane
  y(m)
) is a deterministic
∈ R10  and token-level
∈ R. The control points and scale are noisy versions of their type-level counterparts 
y)  where the scale is truncated below
(see illustration in Figure 3)  the spline deﬁned by the scaled
1 ∈ R10 is evaluated to form a trajectory 1 which is shifted in the image plane
is evaluated and placed to begin at the end of

  x(m)
∈ R2  token-level control points x(m)

1 x(m)
. Next  the second spline y(m)

|xij) = N (xij  σ2

|yij) ∝ N (yij  σ2

xI) and P (y(m)

= f (L(m)

2 x(m)

ij

ij

2

i

i

i

i

i

i

i

i

i

|Ri) =
− Ri)  except for the “along” relation which allows for token-level variability for
τ ). Given
is sampled from
i−1 )  ΣL)  where g(·) = Li when R(m)
is
is start or end  and

Token-level relations must be exactly equal to their type-level counterparts  P (R(m)
δ(R(m)
the attachment along the spline using a truncated Gaussian P (τ (m)
the pen trajectories of the previous strokes 
i−1 ) = N (g(R(m)
P (L(m)
independent (Section 2.1)  g(·) = end(T (m)
g(·) is the proper spline evaluation when R(m)

the start position of L(m)
  ...  T (m)

  T (m)
) or g(·) = start(T (m)

|τi) ∝ N (τi  σ2

) when R(m)

  ...  T (m)

|R(m)

is along.

  T (m)

ui

ui

1

1

i

i

i

i

i

i

i

i

i

1The number of spline evaluations is computed to be approximately 2 points for every 3 pixels of distance

along the spline (with a minimum of 10 evaluations).

4

Figure 4: Learned hyper-
parameters. a) A subset of
primitives  where the top
row shows the most com-
mon ones. The ﬁrst con-
trol point (circle) is a ﬁlled.
b&c) Empirical distribu-
tions where the heatmap
c) show how starting point
differs by stroke number.
Image. An image transformation A(m) ∈ R4 is sampled from P (A(m)) = N ([1  1  0  0]  ΣA) 
where the ﬁrst two elements control a global re-scaling and the second two control a global transla-
tion of the center of mass of T (m). The transformed trajectories can then be rendered as a 105x105
grayscale image  using an ink model adapted from [10] (see Section SI-2). This grayscale image
is then perturbed by two noise processes  which make the gradient more robust during optimiza-
tion and encourage partial solutions during classiﬁcation. These processes include convolution with
a Gaussian ﬁlter with standard deviation σ(m)
and pixel ﬂipping with probability (m)  where the
amount of noise σ(m)
and (m) are drawn uniformly on a pre-speciﬁed range (Section SI-2). The
grayscale pixels then parameterize 105x105 independent Bernoulli distributions  completing the full
model of binary images P (I (m)|θ(m)) = P (I (m)|T (m)  A(m)  σ(m)
2.3 Learning high-level knowledge of motor programs

  (m)).

b

b

b

The Omniglot dataset was randomly split into a 30 alphabet “background” set and a 20 alphabet
“evaluation” set  constrained such that the background set included the six most common alphabets
as determined by Google hits. Background images  paired with their motor data  were used to learn
the hyperparameters of the HBPL model  including a set of 1000 primitive motor elements (Figure
4a) and position models for a drawing’s ﬁrst  second  and third stroke  etc. (Figure 4c). Wherever
possible  cross-validation (within the background set) was used to decide issues of model complexity
within the conditional probability distributions of HBPL. Details are provided in Section SI-4 for
learning the models of primitives  positions  relations  token variability  and image transformations.

2.4

Inference

Posterior inference in this model is very challenging  since parsing an image I (m) requires exploring
a large combinatorial space of different numbers and types of strokes  relations  and sub-strokes. We
developed an algorithm for ﬁnding K high-probability parses  ψ[1]  θ(m)[1]  ...  ψ[K]  θ(m)[K]  which
are the most promising candidates proposed by a fast  bottom-up image analysis  shown in Figure
5a and detailed in Section SI-5. These parses approximate the posterior with a discrete distribution 

K(cid:88)

i=1

P (ψ  θ(m)|I (m)) ≈

wiδ(θ(m) − θ(m)[i])δ(ψ − ψ[i]) 

(4)

(5)

where each weight wi is proportional to parse score  marginalizing over shape variables x 

and constrained such that(cid:80)

wi ∝ ˜wi = P (ψ[i]\x  θ(m)[i]  I (m))

i wi = 1. Rather than using just a point estimate for each parse  the
approximation can be improved by incorporating some of the local variance around the parse. Since
the token-level variables θ(m)  which closely track the image  allow for little variability  and since it
is inexpensive to draw conditional samples from the type-level P (ψ|θ(m)[i]  I (m)) = P (ψ|θ(m)[i]) as
it does not require evaluating the likelihood of the image  just the local variance around the type-level
is estimated with the token-level ﬁxed. Metropolis Hastings is run to produce N samples (Section
SI-5.5) for each parse θ(m)[i]  denoted by ψ[i1]  ...  ψ[iN ]  where the improved approximation is

P (ψ  θ(m)|I (m)) ≈ Q(ψ  θ(m)  I (m)) =

wiδ(θ(m) − θ(m)[i])

1
N

δ(ψ − ψ[ij]).

(6)

N(cid:88)

j=1

K(cid:88)

i=1

5

a)b)c)12≥ 4number of strokesstroke start positionslibrary of motor primitives024680200040006000Number of strokesfrequency12341234123412343Figure 5: Parsing a raw image. a) The raw image (i) is processed by a thinning algorithm [18] (ii) and then
analyzed as an undirected graph [20] (iii) where parses are guided random walks (Section SI-5). b) The ﬁve
best parses found for that image (top row) are shown with their log wj (Eq. 5)  where numbers inside circles
denote stroke order and starting position  and smaller open circles denote sub-stroke breaks. These ﬁve parses
were re-ﬁt to three different raw images of characters (left in image triplets)  where the best parse (top right)
and its associated image reconstruction (bottom right) are shown above its score (Eq. 9).
Given an approximate posterior for a particular image  the model can evaluate the posterior predic-
tive score of a new image by re-ﬁtting the token-level variables (bottom Figure 5b)  as explained in
Section 3.1 on inference for one-shot classiﬁcation.
3 Results
3.1 One-shot classiﬁcation

People  HBPL  and several alternative models were evaluated on a set of 10 challenging one-shot
classiﬁcation tasks. The tasks tested within-alphabet classiﬁcation on 10 alphabets  with examples
in Figure 2 and detailed in Section SI-6 . Each trial (of 400 total) consists of a single test image of
a new character compared to 20 new characters from the same alphabet  given just one image each
produced by a typical drawer of that alphabet. Figure 1b shows two example trials.
People. Forty participants in the USA were tested on one-shot classiﬁcation using Mechanical Turk.
On each trial  as in Figure 1b  participants were shown an image of a new character and asked to
click on another image that shows the same character. To ensure classiﬁcation was indeed “one
shot ” participants completed just one randomly selected trial from each of the 10 within-alphabet
classiﬁcation tasks  so that characters never repeated across trials. There was also an instructions
quiz  two practice trials with the Latin and Greek alphabets  and feedback after every trial.
Hierarchial Bayesian Program Learning. For a test image I (T ) and 20 training images I (c) for
c = 1  ...  20  we use a Bayesian classiﬁcation rule for which we compute an approximate solution
(7)

argmax

log P (I (T )|I (c)).

c

Intuitively  the approximation uses the HBPL search algorithm to get K = 5 parses of I (c)  runs
K MCMC chains to estimate the local type-level variability around each parse  and then runs K
gradient-based searches to re-optimizes the token-level variables θ(T ) (all are continuous) to ﬁt the
test image I (T ). The approximation can be written as (see Section SI-7 for derivation)

(cid:90)
K(cid:88)
P (I (T )|θ(T ))P (θ(T )|ψ)Q(θ(c)  ψ  I (c)) dψ dθ(c) dθ(T )

N(cid:88)

log P (I (T )|I (c)) ≈ log

≈ log

wi max
θ(T )

P (I (T )|θ(T ))

P (θ(T )|ψ[ij]) 

(8)

(9)

i=1

1
N

j=1

where Q(· · ·) and wi are from Eq. 6. Figure 5b shows examples of this classiﬁcation score. While
inference so far involves parses of I (c) reﬁt to I (T )  it also seems desirable to include parses of I (T )
reﬁt to I (c)  namely P (I (c)|I (T )). We can re-write our classiﬁcation rule (Eq. 7) to include just the
reverse term (Eq. 10 center)  and then to include both terms (Eq. 10 right)  which is the rule we use 

argmax

c

log P (I (T )|I (c)) = argmax

c

log

P (I (c)|I (T ))

P (I (c))

= argmax

log

c

P (I (c)|I (T ))

P (I (c))

P (I (T )|I (c)) 
(10)

6

ImageThinnedTraced graph (raw)traced graph (cleaned)Binary imageThinned imageplanningplanning cleanedBinary imageThinned imageplanningplanning cleaned0-60-89-159-168Binary imageThinned imageplanningplanning cleaneda)b)iiiiii12train012−59.61−88.912−1591−16812test−2.12e+031212−1.98e+03121−2.07e+03112−2.09e+03121−2.12e+03112train012−59.61−88.912−1591−16812test−2.12e+031212−1.98e+03121−2.07e+03112−2.09e+03121−2.12e+03112train012−59.61−88.912−1591−16812test−2.12e+031212−1.98e+03121−2.07e+03112−2.09e+03121−2.12e+03112train012−59.61−88.912−1591−16812test−2.12e+031212−1.98e+03121−2.07e+03112−2.09e+03121−2.12e+03112train012−59.61−88.912−1591−16812test−2.12e+031212−1.98e+03121−2.07e+03112−2.09e+03121−2.12e+031-127312train012−59.61−88.912−1591−16812test−2.12e+031212−1.98e+03121−2.07e+03112−2.09e+03121−2.12e+03112train012−59.61−88.912−1591−16812test−2.12e+031212−1.98e+03121−2.07e+03112−2.09e+03121−2.12e+031-83112train012−59.61−88.912−1591−16812test−8311212−881121−983112−979121−1.17e+03112train012−59.61−88.912−1591−16812test−8311212−881121−983112−979121−1.17e+03112train012−59.61−88.912−1591−16812test−1.41e+031212−1.22e+03121−1.18e+03112−1.72e+03121−1.54e+03112train012−59.61−88.912−1591−16812test−1.41e+031212−1.22e+03121−1.18e+03112−1.72e+03121−1.54e+031-2041(cid:80)

b

b

  (T )}).

i ˜wi from Eq. 5. These three rules are equivalent if inference is exact  but due

where P (I (c)) ≈
to our approximation  the two-way rule performs better as judged by pilot results.
Afﬁne model. The full HBPL model is compared to a transformation-based approach that models
the variance in image tokens as just global scales  translations  and blur  which relates to congealing
models [23]. This HBPL model “without strokes” still beneﬁts from good bottom-up image analysis
(Figure 5) and a learned transformation model. The Afﬁne model is identical to HBPL during search 
but during classiﬁcation  only the warp A(m)  blur σ(m)
  and noise (m) are re-optimized to a new
image (change the argument of “max” in Eq. 9 from θ(T ) to {A(T )  σ(T )
Deep Boltzmann Machines (DBMs). A Deep Boltzmann Machine  with three hidden layers of
1000 hidden units each  was generatively pre-trained on an enhanced background set using the
approximate learning algorithm from [25]. To evaluate classiﬁcation performance  ﬁrst the approx-
imate posterior distribution over the DBMs top-level features was inferred for each image in the
evaluation set  followed by performing 1-nearest neighbor in this feature space using cosine similar-
ity. To speed up learning of the DBM and HD models  the original images were down-sampled  so
that each image was represented by 28x28 pixels with greyscale values from [0 1]. To further reduce
overﬁtting and learn more about the 2D image topology  which is built in to some deep models like
convolution networks [19]  the set of background characters was artiﬁcially enhanced by generating
slight image translations (+/- 3 pixels)  rotations (+/- 5 degrees)  and scales (0.9 to 1.1).
Hierarchical Deep Model (HD). A more elaborate Hierarchical Deep model is derived by com-
posing hierarchical nonparametric Bayesian models with Deep Boltzmann Machines [26]. The HD
model learns a hierarchical Dirichlet process (HDP) prior over the activities of the top-level fea-
tures in a Deep Boltzmann Machine  which allows one to represent both a layered hierarchy of
increasingly abstract features and a tree-structured hierarchy of super-classes for sharing abstract
knowledge among related classes. Given a new test image  the approximate posterior over class
assignments can be quickly inferred  as detailed in [26].
Simple Strokes (SS). A much simpler variant of HBPL that infers rigid “stroke-like” parts [16].
Nearest neighbor (NN). Raw images are directly compared using cosine similarity and 1-NN.
Results. Performance is summarized in Table 1. As predicted  peo-
ple were skilled one-shot learners  with an average error rate of 4.5%.
HBPL achieved a similar error rate of 4.8%  which was signiﬁcantly
better than the alternatives. The Afﬁne model achieved an error rate
of 18.2% with the classiﬁcation rule in Eq. 10 left  while perfor-
mance was 31.8% error with Eq. 10 right. The deep learning models
performed at 34.8% and 38% error  although performance was much
lower without pre-training (68.3% and 72%). The Simple Strokes and
Nearest Neighbor models had the highest error rates.

Table 1: One-shot classiﬁers
Learner
Error rate
Humans
HBPL
Afﬁne
HD
DBM
SS
NN

4.5%
4.8%

18.2 (31.8%)
34.8 (68.3%)

38 (72%)
62.5%
78.3%

3.2 One-shot generation of new examples

Not only can people classify new examples  they can generate new examples – even from just one
image. While all generative classiﬁers can produce examples  it can be difﬁcult to synthesize a range
of compelling new examples in their raw form  especially since many models generate only features
of raw stimuli (e.g  [5]). While DBMs [25] can generate realistic digits after training on thousands
of examples  how well do these and other models perform from just a single training image?
We ran another Mechanical Turk task to produce nine new examples of 50 randomly selected hand-
written character images from the evaluation set. Three of these images are shown in the leftmost
column of Figure 6. After correctly answering comprehension questions  18 participants in the USA
were asked to “draw a new example” of 25 characters  resulting in nine examples per character.
To simulate drawings from nine different people  each of the models generated nine samples after
seeing exactly the same images people did  as described in Section SI-8 and shown in Figure 6.
Low-level image differences were minimized by re-rendering stroke trajectories in the same way for
the models and people. Since the HD model does not always produce well-articulated strokes  it
was not quantitatively analyzed  although there are clear qualitative differences between these and
the human produced images (Figure 6).

7

Figure 6: Generating new
examples from just a single
“target” image (left). Each
grid shows nine new exam-
ples synthesized by peo-
ple and the three computa-
tional models.

Visual Turing test. To compare the examples generated by people and the models  we ran a visual
Turing test using 50 new participants in the USA on Mechanical Turk. Participants were told that
they would see a target image and two grids of 9 images (Figure 6)  where one grid was drawn
by people with their computer mice and the other grid was drawn by a computer program that
“simulates how people draw a new character.” Which grid is which? There were two conditions 
where the “computer program” was either HBPL or the Afﬁne model. Participants were quizzed
on their comprehension and then they saw 50 trials. Accuracy was revealed after each block of
10 trials. Also  a button to review the instructions was always accessible. Four participants who
reported technical difﬁculties were not analyzed.
Results. Participants who tried to label drawings from people vs. HBPL were only 56% percent cor-
rect  while those who tried to label people vs. the Afﬁne model were 92% percent correct. A 2-way
Analysis of Variance showed a signiﬁcant effect of condition (p < .001)  but no signiﬁcant effect of
block and no interaction. While both group means were signiﬁcantly better than chance  a subject
analysis revealed only 2 of 21 participants were better than chance for people vs. HBPL  while 24
of 25 were signiﬁcant for people vs. Afﬁne. Likewise  8 of 50 items were above chance for people
vs. HBPL  while 48 of 50 items were above chance for people vs. Afﬁne. Since participants could
easily detect the overly consistent Afﬁne model  it seems the difﬁculty participants had in detecting
HBPL’s exemplars was not due to task confusion. Interestingly  participants did not signiﬁcantly
improve over the trials  even after seeing hundreds of images from the model. Our results suggest
that HBPL can generate compelling new examples that fool a majority of participants.

4 Discussion

Hierarchical Bayesian Program Learning (HBPL)  by exploiting compositionality and causality  de-
parts from standard models that need a lot more data to learn new concepts. From just one example 
HBPL can both classify and generate compelling new examples  fooling judges in a “visual Turing
test” that other approaches could not pass. Beyond the differences in model architecture  HBPL was
also trained on the causal dynamics behind images  although just the images were available at eval-
uation time. If one were to incorporate this compositional and causal structure into a deep learning
model  it could lead to better performance on our tasks. Thus  we do not see our model as the ﬁnal
word on how humans learn concepts  but rather  as a suggestion for the type of structure that best
captures how people learn rich concepts from very sparse data. Future directions will extend this
approach to other natural forms of generalization with characters  as well as speech  gesture  and
other domains where compositionality and causality are central.

Acknowledgments
We would like to thank MIT CoCoSci for helpful feedback. This work was supported by ARO MURI
contract W911NF-08-1-0242 and a NSF Graduate Research Fellowship held by the ﬁrst author.

8

PeopleHBPLAfﬁneHDExampleReferences
[1] M. K. Babcock and J. Freyd. Perception of dynamic information in static handwritten forms. American

Journal of Psychology  101(1):111–130  1988.

[2] I. Biederman. Recognition-by-components: a theory of human image understanding. Psychological

[3] S. Carey and E. Bartlett. Acquiring a single new word. Papers and Reports on Child Language Develop-

Review  94(2):115–47  1987.

ment  15:17–29  1978.

[4] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. ImageNet: A large-scale hierarchical image

database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2009.

[5] L. Fei-Fei  R. Fergus  and P. Perona. One-shot learning of object categories. IEEE Transactions on Pattern

Analysis and Machine Intelligence  28(4):594–611  2006.

[6] J. Feldman. The structure of perceptual categories. Journal of Mathematical Psychology  41:145–170 

[7] J. Freyd. Representing the dynamics of a static form. Memory and Cognition  11(4):342–346  1983.
[8] S. Geman  E. Bienenstock  and R. Doursat. Neural Networks and the Bias/Variance Dilemma. Neural

1997.

Computation  4:1–58  1992.

[9] E. Gilet  J. Diard  and P. Bessi`ere. Bayesian action-perception computational model: interaction of pro-

duction and recognition of cursive letters. PloS ONE  6(6)  2011.

[10] G. E. Hinton and V. Nair. Inferring motor programs from images of handwritten digits. In Advances in

[11] K. H. James and I. Gauthier. Letter processing automatically recruits a sensory-motor brain network.

Neural Information Processing Systems 19  2006.

Neuropsychologia  44(14):2937–2949  2006.

[12] K. H. James and I. Gauthier. When writing impairs reading: letter perception’s susceptibility to motor

interference. Journal of Experimental Psychology: General  138(3):416–31  Aug. 2009.

[13] C. Kemp and A. Jern. Abstraction and relational learning. In Advances in Neural Information Processing

Systems 22  2009.

2009.

[14] A. Krizhevsky. Learning multiple layers of features from tiny images. PhD thesis  Unviersity of Toronto 

[15] A. Krizhevsky  I. Sutskever  and G. E. Hinton. ImageNet Classiﬁcation with Deep Convolutional Neural

Networks. In Advances in Neural Information Processing Systems 25  2012.

[16] B. M. Lake  R. Salakhutdinov  J. Gross  and J. B. Tenenbaum. One shot learning of simple visual concepts.

In Proceedings of the 33rd Annual Conference of the Cognitive Science Society  2011.

[17] B. M. Lake  R. Salakhutdinov  and J. B. Tenenbaum. Concept learning as motor program induction:
A large-scale empirical study. In Proceedings of the 34th Annual Conference of the Cognitive Science
Society  2012.

[18] L. Lam  S.-W. Lee  and C. Y. Suen. Thinning Methodologies - A Comprehensive Survey. IEEE Transac-

tions of Pattern Analysis and Machine Intelligence  14(9):869–885  1992.

[19] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-Based Learning Applied to Document Recog-

nition. Proceedings of the IEEE  86(11):2278–2323  1998.

[20] K. Liu  Y. S. Huang  and C. Y. Suen. Identiﬁcation of Fork Points on the Skeletons of Handwritten Chinese
Characters. IEEE Transactions of Pattern Analysis and Machine Intelligence  21(10):1095–1100  1999.
[21] M. Longcamp  J. L. Anton  M. Roth  and J. L. Velay. Visual presentation of single letters activates a

premotor area involved in writing. Neuroimage  19(4):1492–1500  2003.

[22] E. M. Markman. Categorization and Naming in Children. MIT Press  Cambridge  MA  1989.
[23] E. G. Miller  N. E. Matsakis  and P. A. Viola. Learning from one example through shared densities on
transformations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
2000.

[24] M. Revow  C. K. I. Williams  and G. E. Hinton. Using Generative Models for Handwritten Digit Recog-

nition. IEEE Transactions on Pattern Analysis and Machine Intelligence  18(6):592–606  1996.

[25] R. Salakhutdinov and G. E. Hinton. Deep Boltzmann Machines. In 12th Internationcal Conference on

Artiﬁcial Intelligence and Statistics (AISTATS)  2009.

[26] R. Salakhutdinov  J. B. Tenenbaum  and A. Torralba. Learning with Hierarchical-Deep Models. IEEE

Transactions on Pattern Analysis and Machine Intelligence  35(8):1958–71  2013.

[27] P. H. Winston. Learning structural descriptions from examples. In P. H. Winston  editor  The Psychology

of Computer Vision. McGraw-Hill  New York  1975.

[28] F. Xu and J. B. Tenenbaum. Word Learning as Bayesian Inference. Psychological Review  114(2):245–

272  2007.

9

,Brenden Lake
Russ Salakhutdinov
Josh Tenenbaum
Mehrdad Farajtabar
Yichen Wang
Manuel Gomez Rodriguez
Shuang Li
Hongyuan Zha
Le Song
Raymond Yeh
Jinjun Xiong
Minh Do
Alexander Schwing