2019,Optimistic Regret Minimization for Extensive-Form Games via Dilated Distance-Generating Functions,We study the performance of optimistic regret-minimization algorithms for both minimizing regret in  and computing Nash equilibria of  zero-sum extensive-form games. In order to apply these algorithms to extensive-form games  a distance-generating function is needed. We study the use of the dilated entropy and dilated Euclidean distance functions. For the dilated Euclidean distance function we prove the first explicit bounds on the strong-convexity parameter for general treeplexes. Furthermore  we show that the use of dilated distance-generating functions enable us to decompose the mirror descent algorithm  and its optimistic variant  into local mirror descent algorithms at each information set. This decomposition mirrors the structure of the counterfactual regret minimization framework  and enables important techniques in practice  such as distributed updates and pruning of cold parts of the game tree. Our algorithms provably converge at a rate of $T^{-1}$  which is superior to prior counterfactual regret minimization algorithms. We experimentally compare to the popular algorithm CFR+  which has a theoretical convergence rate of $T^{-0.5}$ in theory  but is known to often converge at a rate of $T^{-1}$  or better  in practice. We give an example matrix game where CFR+ experimentally converges at a relatively slow rate of $T^{-0.74}$  whereas our optimistic methods converge faster than $T^{-1}$. We go on to show that our fast rate also holds in the Kuhn poker game  which is an extensive-form game. For games with deeper game trees however  we find that CFR+ is still faster. Finally we show that when the goal is minimizing regret  rather than computing a Nash equilibrium  our optimistic methods can outperform CFR+  even in deep game trees.,Optimistic Regret Minimization for Extensive-Form
Games via Dilated Distance-Generating Functions∗

Gabriele Farina

Computer Science Department
Carnegie Mellon University

Christian Kroer
IEOR Department
Columbia University

gfarina@cs.cmu.edu

christian.kroer@columbia.edu

Tuomas Sandholm

Computer Science Department  CMU

Strategic Machine  Inc.

Strategy Robot  Inc.

Optimized Markets  Inc.
sandholm@cs.cmu.edu

Abstract

We study the performance of optimistic regret-minimization algorithms for both
minimizing regret in  and computing Nash equilibria of  zero-sum extensive-form
games. In order to apply these algorithms to extensive-form games  a distance-
generating function is needed. We study the use of the dilated entropy and dilated
Euclidean distance functions. For the dilated Euclidean distance function we prove
the ﬁrst explicit bounds on the strong-convexity parameter for general treeplexes.
Furthermore  we show that the use of dilated distance-generating functions enable
us to decompose the mirror descent algorithm  and its optimistic variant  into local
mirror descent algorithms at each information set. This decomposition mirrors
the structure of the counterfactual regret minimization framework  and enables
important techniques in practice  such as distributed updates and pruning of cold
parts of the game tree. Our algorithms provably converge at a rate of T −1  which is
superior to prior counterfactual regret minimization algorithms. We experimentally
compare to the popular algorithm CFR+  which has a theoretical convergence rate
of T −0.5 in theory  but is known to often converge at a rate of T −1  or better  in
practice. We give an example matrix game where CFR+ experimentally converges
at a relatively slow rate of T −0.74  whereas our optimistic methods converge faster
than T −1. We go on to show that our fast rate also holds in the Kuhn poker game 
which is an extensive-form game. For games with deeper game trees however  we
ﬁnd that CFR+ is still faster. Finally we show that when the goal is minimizing
regret  rather than computing a Nash equilibrium  our optimistic methods can
outperform CFR+  even in deep game trees.

1

Introduction

Extensive-form games (EFGs) are a broad class of games that can model sequential interaction 
imperfect information  and stochastic outcomes. To operationalize them they must be accompanied
by techniques for computing game-theoretic equilibria such as Nash equilibrium. A notable success
story of this is poker: Bowling et al. [1] computed a near-optimal Nash equilibrium for heads-up

∗The full version of this paper is available on arXiv.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

limit Texas hold’em  while Brown and Sandholm [3] beat top human specialist professionals at the
larger game of heads-up no-limit Texas hold’em. Solving extremely large EFGs relies on many
methods for dealing with the scale of the problem: abstraction methods are sometimes used to create
smaller games [16  26  20  14  6  21]  endgame solving is used to compute reﬁned solutions to the
end of the game in real time [9  15  27]  and recently depth-limited subgame solving has been very
successfully used in real time [28  8  5]. At the core of all these methods is a reliance on a fast
algorithm for computing approximate Nash equilibria of the abstraction  endgame  and/or depth-
limited subgame [28  8  5]. In practice the most popular method has been the CFR+ algorithm [38  35] 
which was used within all three two-player poker breakthroughs [1  28  3]. CFR+ has been shown to
converge to a Nash equilibrium at a rate of T −0.5  but in practice it often performs much better  even
outperforming faster methods that have a guaranteed rate of T −1 [7  24  23  4].
Recently  another class of optimization algorithms has been shown to have appealing theoretical
properties. Online convex optimization (OCO) algorithms are online variants of ﬁrst-order methods:
at each timestep t they receive some loss function (cid:96)t (often a linear loss which is a gradient of some
underlying loss function)  and must then recommend a point from some convex set based on the
series of past points and losses. While these algorithms are generally known to have a T −0.5 rate of
convergence when solving static problems  a recent series of papers showed that when two optimistic
OCO algorithms are faced against each other  and they have some estimate of the next loss faced 
a rate of T −1 can be achieved [30  31  34]. In this paper we investigate the application of these
algorithms to EFG solving  both in the regret-minimization setting  and for computing approximate
Nash equilibria at the optimal rate of O(T −1). The only prior attempt at using optimistic OCO
algorithm in extensive-form games is due to Farina et al. [13]. In that paper  the authors show that by
restricting to the weaker notion of stable-predictive optimism  one can mix and match local stable-
predictive optimistic algorithm at every decision point in the game as desired and obtain an overall
stable-predictive optimistic algorithm that enables O(T −0.75) convergence to Nash equilibrium. The
approach we adopt in this paper is different from that of Farina et al. [13] in that our construction
does not allow one to pick different regret minimizers for different decision points; however  our
algorithms converge to Nash equilibrium at the improved rate O(T −1).
The main hurdle to overcome is that in all known OCO algorithms a distance-generating function
(DGF) is needed to maintain feasibility via proximal operators and ensure that the stepsizes of
the algorithms are appropriate for the convex set at hand. For the case of EFGs  the convex set
is known as a treeplex  and the so-called dilated DGFs are known to have appealing properties 
including closed-form iterate updates and strong convexity properties [18  24]. In particular  the
dilated entropy DGF  which applies the negative entropy at each information set  is known to lead
to the state-of-the-art theoretical rate on convergence for iterative methods [24]. Another potential
DGF is the dilated Euclidean DGF  which applies the (cid:96)2 norm as a DGF at each information set.
We show the ﬁrst explicit bounds on the strong-convexity parameter for the dilated Euclidean DGF
when applied to the strategy space of an EFG. We go on to show that when a dilated DGF is paired
with the online mirror descent (OMD) algorithm  or its optimistic variant  the resulting algorithm
decomposes into a recursive application of local online mirror descent algorithms at each information
set of the game. This decomposition is similar to the decomposition achieved in the counterfactual
regret minimization framework  where a local regret minimizer is applied on the counterfactual regret
at each information set. This localization of the updates along the tree structure enables further
techniques  such as distributing the updates [3  6] or skipping updates on cold parts of the game
tree [2].
It is well-known that the entropy DGF is the theoretically superior DGF when applied to optimization
over a simplex [18]. For the treeplex case where the entropy DGF is used at each information set 
Kroer et al. [24] showed that the strong theoretical properties of the simplex entropy DGF generalize
to the dilated entropy DGF on a treeplex (with earlier weaker results shown by Kroer et al. [22]).
Our results on the dilated Euclidean DGF conﬁrm this ﬁnding  as the dilated Euclidean DGF has a
similar strong convexity parameter  but with respect to the (cid:96)2 norm  rather than the (cid:96)1 norm for dilated
entropy (having strong convexity with respect to the (cid:96)1 norm leads to a tighter convergence-rate
bound because it gives a smaller matrix norm  another important constant in the rate).
In contrast to these theoretical results  for the case of computing a Nash equilibrium in matrix games
it has been found experimentally that the Euclidean DGF often performs much better than the entropy
DGF. This was shown by Chambolle and Pock [11] when using a particular accelerated primal-dual
algorithm [10  11] and using the last iterate (as opposed to the uniformly-averaged iterate as the

2

theory suggests). Kroer [19] recently showed that this extends to the theoretically-sound case of using
linear or quadratic averaging in the same primal-dual algorithm  or in mirror prox [29] (the ofﬂine
variant of optimistic OMD). In this paper we replicate these results when using OCO algorithms: ﬁrst
we show it on a particular matrix game  where we also exhibit a slow T −0.74 convergence rate of
CFR+ (the slowest CFR+ rate seen to the best of our knowledge). We show that for the Kuhn poker
game the last iterate of optimistic OCO algorithms with the dilated Euclidean DGF also converges
extremely fast. In contrast to this  we show that for deeper EFGs CFR+ is still faster. Finally we
compare the performance of CFR+ and optimistic OCO algorithms for minimizing regret  where we
ﬁnd that OCO algorithms perform better.

2 Regret Minimization Algorithms

In this section we present the regret-minimization algorithms that we will work with. We will operate
within the framework of online convex optimization [37]. In this setting  a decision maker repeatedly
plays against an unknown environment by making decision x1  x2  . . . ∈ X for some convex compact
set X . After each decision xt at time t  the decision maker faces a linear loss xt (cid:55)→ (cid:104)(cid:96)t  xt(cid:105)  where
(cid:96)t is a vector in X . Summarizing  the decision maker makes a decision xt+1 based on the sequence
of losses (cid:96)1  . . .   (cid:96)t as well as the sequence of past iterates x1  . . .   xt.
The quality metric for a regret minimizer is its cumulative regret  which is the difference between the
loss cumulated by the sequence of decisions x1  . . .   xT and the loss that would have been cumulated
by playing the best-in-hindsight time-independent decision ˆx. Formally  the cumulative regret up to
time T is

T(cid:88)

t=1

RT :=

(cid:104)(cid:96)t  xt(cid:105) − min
ˆx∈X

(cid:26) T(cid:88)

t=1

(cid:27)
(cid:104)(cid:96)t  ˆx(cid:105)

.

A “good” regret minimizer is such that the cumulative regret grows sublinearly in T .
The algorithms we consider assume access to a distance-generating function d : X → R  which
is 1-strongly convex (with respect to some norm) and continuously differentiable on the inte-
rior of X . Furthermore d should be such that the gradient of the convex conjugate ∇d(g) =
argmaxx∈X(cid:104)g  x(cid:105) − d(x) is easy to compute. Following Hoda et al. [18] we say that a DGF sat-
isfying these properties is a nice DGF for X . From d we also construct the Bregman divergence
D(x (cid:107) x(cid:48)) := d(x) − d(x(cid:48)) − (cid:104)∇d(x(cid:48))  x − x(cid:48)(cid:105).
First we present two classical regret minimization algorithms. The online mirror descent (OMD)
(cid:26)
algorithm produces iterates according to the rule

The follow the regularized leader (FTRL) algorithm produces iterates according to the rule [32]

OMD and FTRL satisfy regret bounds of the form RT ≤ O
The optimistic variants of the classical regret minimization algorithms take as input an additional
vector mt+1  which is an estimate of the loss faced at time t + 1 [12  30]. Optimistic OMD produces
iterates according to the rule [30] (note that xt+1 is produced before seeing (cid:96)t+1  while zt+1 is
produced after)

(e.g. Hazan [17]).

D(x∗(cid:107)x1)L√T

(cid:26)
(cid:104)mt+1  x(cid:105) +

(cid:27)
D(x (cid:107) zt)

1
η

xt+1 = argmin

x∈X

(cid:26)

(cid:27)
D(z (cid:107) zt)

. (3)

  zt+1 = argmin

z∈X

(cid:104)(cid:96)t+1  z(cid:105) +

1
η

Thus it is like OMD  except that xt+1 is generated by an additional step taken using the loss estimate.
This additional step is transient in the sense that xt+1 is not used as a center for the next iterate.

3

xt+1 = argmin

x∈X

(cid:104)(cid:96)t  x(cid:105) +

1
η

(cid:26)(cid:28) t(cid:88)

x∈X

τ =1

xt+1 = argmin

(cid:96)τ   x

d(x)

.

(cid:27)
D(x (cid:107) xt)
(cid:29)

(cid:27)

.

1
η

+

(cid:16)

(cid:17)

(1)

(2)

OFTRL produces iterates according to the rule [30  34]

(cid:26)(cid:28)

(cid:29)

+

1
η

(cid:27)

d(x)

.

(cid:96)τ   x

(4)

t(cid:88)

τ =1

xt+1 = argmin

x∈X

mt+1 +

Again the loss estimate is used in a transient way: it is used as if we already saw the loss at time t + 1 
but then discarded and not used in future iterations.

(cid:8)x(cid:62)Ay(cid:9) where X  Y

2.1 Connection to Saddle Points

A bilinear saddle-point problem is a problem of the form minx∈X maxy∈Y
are closed convex sets. This general formulation allows us to capture  among other settings  several
game-theoretical applications such as computing Nash equilibria in two-player zero-sum games. In
that setting  X and Y are convex polytopes whose description is provided by the sequence-form
constraints  and A is a real payoff matrix [36].
The error metric that we use is the saddle-point residual (or gap) ξ of ( ¯x  ¯y)  deﬁned as ξ( ¯x  ¯y) :=
maxˆy∈Y(cid:104) ¯x  A ˆy(cid:105) − min ˆx∈X(cid:104) ˆx  A ¯y(cid:105). A well-known folk theorem shows that the average of a se-
quence of regret-minimizing strategies for the choice of losses (cid:96)t
: Y (cid:51)
X
y (cid:55)→ (A(cid:62)xt)(cid:62)y leads to a bounded saddle-point residual  since one has

: X (cid:51) x (cid:55)→ (−Ayt)(cid:62)x  (cid:96)t
Y

ξ( ¯x  ¯y) =

(RT

X + RT
Y ).

(5)

1
T

When X  Y are the players’ sequence-form strategy spaces  this implies that the average strategy
proﬁle produced by the regret minimizers is a 1/T (RT
Y )-Nash equilibrium. This also implies
that by using online mirror descent or follow-the-regularizer-leader  one obtains an anytime algorithm
for computing a Nash equilibrium. In particular  at each time T   the average strategy output by each
of the two regret minimizers forms a -Nash equilibrium  where  = O(T −0.5).

X + RT

2.2 RVU Property and Fast Convergence to Saddle Points

Both optimistic OMD and optimistic FTRL satisfy the Regret bounded by Variation in Utilities (RVU)
property  as given by Syrgkanis et al.:
Deﬁnition 1 (RVU property  [34]). We say that a regret minimizer satisﬁes the RVU property if there
exist constants α > 0 and 0 < β ≤ γ  as well as a pair of dual norms ((cid:107) · (cid:107) (cid:107) · (cid:107)∗) such that  no
matter what the loss functions (cid:96)1  . . .   (cid:96)T are 

T(cid:88)

t=1

T(cid:88)

t=1

RT ≤ α + β

(cid:107)(cid:96)t − mt(cid:107)2

∗ − γ

(cid:107)xt − xt−1(cid:107)2.

(RVU)

The deﬁnition given here is slightly more general than that of Syrgkanis et al. [34]: we allow a general
estimate mt of (cid:96)t  whereas their deﬁnition requires using mt = (cid:96)t−1. While the choice mt = (cid:96)t−1
is often reasonable  in some cases other deﬁnitions of the loss prediction are more natural [13]. In
practice  both optimistic OMD and optimistic FTRL satisfy a parametric notion of the RVU property 
which depends on the value of the step-size parameter that was chosen to set up either algorithm.
Theorem 1 (Syrgkanis et al. [34]). For all step-size parameters η > 0  Optimistic OMD satisﬁes
the RVU conditions with respect to the primal-dual norm pair ((cid:107) · (cid:107)1 (cid:107) · (cid:107)∞) with parameters
α = R/η  β = η  γ = 1/(8η)  where R is a constant that scales with the maximum allowed norm of
any loss function (cid:96).
Theorem 2. For all step-size parameters η > 0  OFTRL satisﬁes the RVU conditions with respect
to any primal-dual norm pair ((cid:107) · (cid:107) (cid:107) · (cid:107)∗) with parameters α = ∆d/η  β = η  γ = 1/(4η)  where
∆d := maxx y∈X{d(x) − d(y)}.
Our proof  available in the appendix of the full paper  generalizes the work by Syrgkanis et al. [34]
by extending the proof beyond simplex domains and beyond the ﬁxed choice mt = (cid:96)t−1.
It turns out that this is enough to accelerate the convergence to a saddle point in the construction of
Section 2.1. In particular  by letting the predictions be deﬁned as mt
Y   we
X

X   mt
Y

:= (cid:96)t−1

:= (cid:96)t−1

4

obtain that the residual ξ of the average decisions ( ¯x  ¯y) satisﬁes

T ξ( ¯x  ¯y) ≤

2α(cid:48)
η

+ η

(cid:107)−Ayt + Ayt−1(cid:107)2

(cid:18)

T(cid:88)

t=1

(cid:19)

(cid:18)

T(cid:88)

∗ + (cid:107)A(cid:62)xt − A(cid:62)xt−1(cid:107)2
∗
γ (cid:48)
η

−

t=1

(cid:19)

(cid:18)
η(cid:107)A(cid:107)2

op −

(cid:19)(cid:32) T(cid:88)

t=1

γ (cid:48)
η

2α(cid:48)
η

≤

+

(cid:107)xt − xt−1(cid:107)2 + (cid:107)yt − yt−1(cid:107)2
T(cid:88)

(cid:33)

(cid:107)yt − yt−1(cid:107)2

 

t=1

(cid:107)xt − xt−1(cid:107)2 +

where the ﬁrst inequality holds by plugging (RVU) into (5)  and the second inequality by noting
that the operator norm (cid:107) · (cid:107)op of a linear function is equal to the operator norm of its transpose.
  the saddle-point gap ξ( ¯x  ¯y)
This implies that when the step-size parameter is chosen as η =
satisﬁes ξ( ¯x  ¯y) ≤ 2α(cid:48)
3 Treeplexes and Sequence Form

(cid:107)A(cid:107)op
T√γ (cid:48) = O(T −1).

√γ (cid:48)
(cid:107)A(cid:107)op

We formalize a sequential decision process as follows. We assume that we have a set of decision
points J . Each decision point j ∈ J has a set of actions Aj of size nj. Given a speciﬁc action at j 
the set of possible decision points that the agent may next face is denoted by Cj a. It can be an empty
set if no more actions are taken after j  a. We assume that the decision points form a tree  that is 
Cj a ∩ Cj(cid:48) a(cid:48) = ∅ for all other convex sets and action choices j(cid:48)  a(cid:48). This condition is equivalent to
the perfect-recall assumption in extensive-form games  and to conditioning on the full sequence of
actions and observations in a ﬁnite-horizon partially-observable decision process. In our deﬁnition 
the decision space starts with a root decision point  whereas in practice multiple root decision points
may be needed  for example in order to model different starting hands in card games. Multiple root
decision points can be modeled by having a dummy root decision point with only a single action.
The set of possible next decision points after choosing action a ∈ Aj at decision point j ∈ J  
denoted Cj a  can be thought of as representing the different decision points that an agent may
face after taking action a and then making an observation on which she can condition her next
action choice. In addition to games  our model of sequential decision process captures  for example 
partially-observable Markov decision processes and Markov decision processes where we condition
on the entire history of observations and actions.

As an illustration  consider
the game of Kuhn
poker [25]. Kuhn poker consists of a three-card deck:
king  queen  and jack. The action space for the
ﬁrst player is shown in Figure 1. For instance  we
have: J = {0  1  2  3  4  5  6}; n0 = 1; nj = 2
for all j ∈ J \ {0}; A0 = {start}  A1 = A2 =
A3 = {check  raise}  A4 = A5 = A6 = {fold  call};
C0 start = {1  2  3}  C1 raise = ∅  C3 check = {6}; etc.
The expected loss for a given strategy is non-linear
in the vectors of probability masses for each decision
point j. This non-linearity is due to the probability of
reaching each j  which is computed as the product of
the probabilities of all actions on the path to from the
root to j. An alternative formulation which preserves
linearity is called the sequence form. In the sequence-
form representation  the simplex strategy space at a generic decision point j ∈ J is scaled by the
decision variable associated with the last action in the path from the root of the process to j. In this
formulation  the value of a particular action represents the probability of playing the whole sequence
of actions from the root to that action. This allows each term in the expected loss to be weighted only
by the sequence ending in the corresponding action. The sequence form has been used to instantiate
linear programming [36] and ﬁrst-order methods [18  22  24] for computing Nash equilibria of
zero-sum EFGs. Formally  the sequence-form representation X of a sequential decision process can

Figure 1: Sequential action space for the
ﬁrst player in the game of Kuhn poker.
denotes an observation point;
the end of the decision process.

represents

5

X0X3X6X2X5X1X4startfoldcallfoldcallfoldcallcheckraisecheckraisecheckraisejackqueenkingcheckraisecheckraisecheckraisej(cid:48)∈Cj a X↓j(cid:48)  where

be obtained recursively  as follows: for every j ∈ J   a ∈ Aj  we let X↓j a :=(cid:81)

Π denotes Cartesian product; at every decision point j ∈ J   we let

X↓j := {(λ1  . . .   λnj   λ1xa1  . . .   λnj xanj

) : (λ1  . . .   λn) ∈ ∆nj   xa ∈ X↓j a ∀ a ∈ Aj} 

where we assumed Aj = {a1  . . .   anj}.
The sequence form strategy space for the whole sequential decision process is then X := {1} × X↓r 
where r is the root of the process. The ﬁrst entry  identically equal to 1 for any point in X   corresponds
to what is called the empty sequence. Crucially  X is a convex and compact set  and the expected
loss of the process is a linear function over X . With the sequence-form representation the problem
of computing a Nash equilibrium in an EFG can be formulated as a bilinear saddle-point problem
(see Section 2.1)  where X and Y are the sequence-form strategy spaces of the sequential decision
processes faced by the two players  and A is a sparse matrix encoding the leaf payoffs of the game.
As we have already observed  vectors that pertain to the sequence form have one entry for each
sequence of the decision process. We denote with vφ the entry in v corresponding to the empty
sequence  and vja the entry corresponding to any other sequence (j  a) where j ∈ J   a ∈ Aj.
Sometimes  we will need to slice a vector v and isolate only those entries that refer to all decision
points j(cid:48) and actions a(cid:48) ∈ Aj(cid:48) that are at or below some j ∈ J ; we will denote such operation as v↓j.
Similarly  we introduce the syntax vj to denote the subset of nj = |Aj| entries of v that pertain to
all actions a ∈ Aj at decision point j ∈ J . Finally  note that for any j ∈ J − {r} there is a unique
sequence (j(cid:48)  a(cid:48))  denoted pj and called the parent sequence of decision point j  such that j ∈ Cj(cid:48)a(cid:48).
When j = r is the root decision point  we let pr := φ  the empty sequence.

4 Dilated Distance Generating Functions

(cid:19)

(cid:18)

xpj dj

j∈J

xj
xpj

point: d(x) =(cid:80)

We will be interested in a particular type of DGF which is suitable for sequential decision-making
problems: a dilated DGF. A dilated DGF is constructed by taking a sum over suitable local DGFs for
each decision point  where each local DGF is dilated by the parent variable leading to the decision
. Each “local” DGF dj is given the local variable xj divided by
xpj   so that xj
xpj ∈ ∆nj . The idea is that dj can be any DGF suitable for ∆nj ; by multiplying dj by
xpj and taking a sum over J we construct a DGF for the whole treeplex from these local DGFs.
Hoda et al. [18] showed that dilated DGFs have many of the desired properties of a DGF for an
optimization problem over a treeplex.
We now present two local DGFs for simplexes  that are by far the most common in practice. In
the following we let b be a vector in the n-dimensional simplex ∆n. First  the Euclidean DGF
2  which is 1-strongly convex with respect to the (cid:96)2 norm; secondly  the negative entropy
d(b) = (cid:107)b(cid:107)2
i=1 bi log(bi) (we will henceforth drop the “negative” and simply refer to it as
the entropy DGF)  which is 1-strongly convex with respect to the (cid:96)1 norm. The strong convexity
properties of the dilated entropy DGF were shown by Kroer et al. [24] (with earlier weaker results
shown by Kroer et al. [22]). However  for the dilated Euclidean DGF a setup for achieving a strong-
convexity parameter of 1 was unknown until now; Hoda et al. [18] show that a strong-convexity
parameter exists  but do not show what it is for the general case (they give speciﬁc results for a
particular class of uniform treeplexes). We now show how to achieve this.
We are now ready to state our ﬁrst result on dilated regularizers that are strongly convex with respect
to the Euclidean norm:

DGF d(b) = (cid:80)n

(cid:80)
xpj dj(xj/xpj ) where for all j  dj is µj-strongly convex with
j(cid:48)∈Cja µj(cid:48)  and

respect to the Euclidean norm over ∆nj . Furthermore  deﬁne σja := µj
¯σ := minja σja. Then  d is ¯σ-strongly convex with respect to the Euclidean norm over X .
We can immediately use Theorem 3 to prove the following corollary:
Corollary 1. Let ¯σ > 0 be arbitrary  and for all j let dj be a µj-strongly convex function over ∆nj
with respect to the Euclidean norm  where the µj’s satisfy

Theorem 3. Let d(x) = (cid:80)

2 −

j∈J

(cid:88)

j(cid:48)∈Cja

6

Then  d(x) =(cid:80)

j∈J

µj = 2¯σ + 2 max
a∈Aj

µj(cid:48).

(6)

xpj dj(xj/xpj ) is ¯σ-strongly convex over X with respect to the Euclidean norm.

5 Local Regret Minimization

We now show that OMD and Optimistic OMD run on a treeplex X with a dilated DGF can
both be interpreted as locally minimizing a modiﬁed variant of loss at each information set  with
correspondingly-modiﬁed loss predictions. The modiﬁed local loss at a given information set j takes
into account the loss and DGF below j by adding the expectation with respect to the next iterate xt
↓j.
In practice this modiﬁed loss is easily handled by computing xt bottom-up  thereby visiting j after
having visited the whole subtree below.
We ﬁrst show that the problem of computing the prox mapping  the minimizer of a linear term
plus the Bregman divergence  decomposes into local prox mappings at each simplex of a treeplex.
This will then be used to show that OMD and Optimistic OMD can be viewed as a tree of local
simplex-instantiations of the respective algorithms.

5.1 Decomposition into Local Prox Mappings with a Dilated DGF

We will be interested in solving the following prox mapping  which takes place in the sequence form:
(7)

Prox(g  ˆx) = argmin

(cid:8)
(cid:104)g  x(cid:105) + D(x (cid:107) ˆx)(cid:9).

The reason is that the update applied at each iteration of several OCO algorithms run on the sequence-
form polytope of X can be described as an instantiation of this prox mapping. We now show that this
update can be interpreted as a local prox mapping at each decision point  but with a new loss ˆgj that
depends on the update applied in the subtree beneath j.
Proposition 1 (Decomposition into local prox mappings). A prox mapping (7) on a treeplex with a
Bregman divergence constructed from a dilated DGF decomposes into local prox mappings at each
decision point j where the solution is as follows:

x∈X

(cid:26)

where

ˆgj a = gj a +

(cid:88)

j(cid:48)∈Cj a

(cid:104)ˆgj  bj(cid:105) + Dj

x∗j = xpj · argmin
bj∈∆nj
(cid:34)

− g↓j(cid:48) + ∇d↓j(cid:48)( ˆx↓j(cid:48))(cid:1)

↓j(cid:48)(cid:0)

d∗

bj

(cid:13)(cid:13)(cid:13)(cid:13) ˆxj
(cid:18)
(cid:18) ˆxj
(cid:19)

ˆxpj

(cid:19)(cid:27)
(cid:42)

 

(cid:32)

− dj(cid:48)

ˆxpj

+

∇dj(cid:48)

(cid:33)

ˆxj(cid:48)
ˆxpj(cid:48)

 

ˆxj(cid:48)
ˆxpj(cid:48)

(cid:43)(cid:35)

.

Hoda et al. [18] and Kroer et al. [23] gave variations on a similar result: that the convex conjugate
↓j(−g) can be computed in bottom-up fashion similar to the recursion we show here. Proposition 1
d∗
is slightly different in that we additionally show that the Bregman divergence also survives the
decomposition and can be viewed as a local Bregman divergence. This latter difference will be
necessary for showing that OMD can be interpreted as a local RM.

5.2 Decomposition into Local Regret Minimizers

With Proposition 1 it follows almost directly that OMD and Optimistic OMD can be seen as a set of
local regret minimizers  one for each simplex. Each produces iterates from their respective simplex 
with the overall strategy produced by then applying the sequence-form transformation to these local
iterates.
Theorem 4. OMD with a dilated DGF for a treeplex X corresponds to running OMD locally at each
simplex j  with the local loss ˆ(cid:96)t constructed according to Proposition 1. Optimistic OMD corresponds
to the optimistic variant of this local OMD with local loss predictions ˆ(cid:96)t  ˆmt+1
again constructed
according to Proposition 1 using xt as Bregman divergence center and xt+1 for aggregating losses
below each simplex. Here the modiﬁed loss uses zt
↓j(cid:48) and xt+1 as Bregman divergence center and
aggregating loss below  respectively. The prediction ˆmt+1

↓j(cid:48) and zt+1.
(cid:80)t
Unlike OMD and its optimistic variant  it is not the case that FTRL has a nice interpretation as a local
regret minimizer. The reason is that the prox mapping in (2) or (4) minimizes the sum of losses  rather
than the most recent loss. Because of this  the expected value (cid:104)
↓j (cid:105) at simplex j  which

↓j  xt+1

τ =1 (cid:96)τ

uses zt

j

j

7

inﬂuences the modiﬁed loss at parent simplexes  is computed based on xt+1 for all t losses. Thus
there is no local modiﬁed loss that could be received at rounds 1 through t that accurately reﬂects the
modiﬁed loss needed in Proposition 1.

6 Experimental Evaluation

We experimentally evaluate the performance of optimistic regret minimization methods instantiated
with dilated distance-generating functions. We experiment on three games:
• Smallmatrix  a small 2 × 2 matrix game. Given a mixed strategy x = (x1  x2) ∈ ∆2 for Player
1 and a mixed strategy y = (y1  y2) ∈ ∆2 for Player 2  the payoff function for player 1 is
u(x  y) = 5x1y1 − x1y2 + x2y2.
• Kuhn poker  already introduced in Section 3. In Kuhn poker  each player ﬁrst has to put a payment
of 1 into the pot. Each player is then dealt one of the three cards  and the third is put aside unseen.
A single round of betting then occurs: ﬁrst  Player 1 can check or bet 1. Then 

– If Player 1 checks Player 2 can check or raise 1.

∗ If Player 2 checks a showdown occurs; if Player 2 raises Player 1 can fold or call.

· If Player 1 folds Player 2 takes the pot; if Player 1 calls a showdown occurs.

– If Player 1 raises Player 2 can fold or call.

∗ If Player 2 folds Player 1 takes the pot; if Player 2 calls a showdown occurs.

If no player has folded  a showdown occurs where the player with the higher card wins.

• Leduc poker  a standard benchmark in imperfect-information game solving [33]. The game is
played with a deck consisting of 5 unique cards with 2 copies of each  and consists of two rounds.
In the ﬁrst round  each player places an ante of 1 in the pot and receives a single private card. A
round of betting then takes place with a two-bet maximum  with Player 1 going ﬁrst. A public
shared card is then dealt face up and another round of betting takes place. Again  Player 1 goes
ﬁrst  and there is a two-bet maximum. If one of the players has a pair with the public card  that
player wins. Otherwise  the player with the higher card wins. All bets in the ﬁrst round are 1  while
all bets in the second round are 2. This game has 390 decision points and 911 sequences per player.

Fast Last-Iterate Convergence. In the ﬁrst set of experiments (Figure 2  top row)  we compare
the saddle-point gap of the strategy proﬁles produced by optimistic OMD and optimistic FTRL to
that produced by CFR and CFR+. Optimistic OMD and optimistic FTRL were set up with the
step-size parameter η = 0.1 in Smallmatrix and η = 2 in Kuhn Poker  and the plots show the
last-iterate convergence for the optimistic algorithms  which has recently received attention in the
works by Chambolle and Pock [11] and Kroer [19]. Finally  we instantiated optimistic OMD and
optimistic FTRL with the Euclidean distance generating function as constructed in Corollary 1. The
plots show that—at least in these shallow games—optimistic methods are able to produce even up to
12 orders of magnitude better-approximate saddle-points than CFR and CFR+.
Interestingly  Smallmatrix appears to be a hard instance for CFR+: linear regression on the ﬁrst 20 000
iterations of CFR+ shows  with a coefﬁcient of determination of roughly 0.96  that log ξ(xT
∗ ) ≈
−0.7375 · log(T ) − 2.1349  where (xT
∗ ) is the average strategy proﬁle (computed using linear
averaging  as per CF R+’s construction) up to time T . In other words  we have evidence of at least
one game in which the approximate saddle-point computed by CFR+ experimentally has residual
bounded below by Ω(T −0.74). This observation suggests that the analysis of CFR+ might actually
be quite tight  and that CFR+ is not an accelerated method.
Figure 2 (bottom left) shows the performance of OFTRL in Leduc Poker  compared to CFR and
CFR+ (we do not show optimistic OMD  which we found to have worse performance than OFTRL).
Here OFTRL performs worse than CFR+. This shows that in deeper games  more work has to be
done to fully exploit the accelerated bounds of optimistic regret minimization methods.

∗   yT

∗   yT

Comparing the Cumulative Regret. We also compared the algorithms based on the sum of cumula-
tive regrets (again we omit optimistic OMD  which performed worse than OFTRL). In all three games 
OFTRL leads to lower sum of cumulative regrets. Figure 2 (bottom right) shows the performance of
t=1 xt (note that the

OFTRL in Leduc Poker. Here  we used the usual average of iterates ¯x := 1/T(cid:80)T

choice of averaging strategy has no effect on the bottom right plot.)

8

Figure 2: (Left and upper right) Saddle-point gap as a function of the number of iterations. The plots
show the last-iterate convergence for OOMD and OFTRL.(Lower right) Sum of cumulative regret for
both players in Leduc. Optimistic OMD (OOMD) and OFTRL use step-size parameter η = 0.1 in
Smallmatrix and η = 2 in Kuhn. OFTRL uses step-size parameter η = 200 in Leduc.

OFTRL’s performance matches the theory from Theorem 2 and Section 2.2. In particular  we observe
that while OFTRL does not beat the state-of-the-art CFR+ in terms of saddle-point gap  it beats it
according to the regret sum metric. The fact that CFR+ performs worse with respect to the regret sum
metric is somewhat surprising: the entire derivation of CFR and CFR+ is based on showing bounds
on the regret sum. However  the connection between regret and saddle-point gap (or exploitability) is
one-way: if the two regret minimizers (one per player) have regret R1 and R2  then the saddle point
gap can be easily shown to be less than or equal to (R1 + R2)/T . However  nothing prevents it from
being much smaller than (R1 + R2)/T . What we empirically ﬁnd is that for CFR+ this bound is
very loose. We are not sure why this is the case  and it potentially warrants further investigation in
the future.

7 Conclusions

We studied how optimistic regret minimization can be applied in the context of extensive-form games 
and introduced the ﬁrst instantiations of regret-based techniques that achieve T −1 convergence to
Nash equilibrium in extensive-form games. These methods rely crucially on having a tractable
regularizer to maintain feasibility and control the stepsizes on the domain at hand—in our case  the
sequence-form polytope. We provided the ﬁrst explicit bound on the strong convexity properties
of dilated distance-generating functions with respect to the Euclidean norm. We also showed
that when optimistic regret minimization methods are instantiated with dilated distance-generating
functions  the regret updates are local to each information set in the game  mirroring the structure of
the counterfactual regret minimization framework. This localization of the updates along the tree
structure enables further techniques  such as distributing the updates or skipping updates on cold
parts of the game tree. Finally  when used in self play  these optimistic regret minimization methods
guarantee an optimal T −1 convergence rate to Nash equilibrium.
We demonstrate that in shallow games  methods based on optimistic regret minimization can signiﬁ-
cantly outperform CFR and CFR+—even up to 12 orders of magnitude. In deeper games  more work
has to be done to fully exploit the accelerated bounds of optimistic regret minimization methods.
However  while the strong CFR+ performance in large games remains a mystery  we elucidate some
points about its performance—including showing that its theoretically slow convergence bound is
somewhat tight. Finally  we showed that when the goal is minimizing regret  rather than computing a
Nash equilibrium  optimistic methods can outperform CFR+ even in deep game trees.

9

10010110210310−1610−1210−810−4100OFTRLOOMDCFRCFR+Iterationnumber(T)Saddle-pointgap(ξ)Smallmatrix10010110210310−1610−1210−810−4100OFTRLOOMDCFRCFR+Iterationnumber(T)Saddle-pointgap(ξ)Kuhn10010110210310−310−210−1100101OFTRLCFRCFR+Iterationnumber(T)Saddle-pointgap(ξ)Leduc100101102103101101.5OFTRLCFRCFR+Iterationnumber(T)CumulativeregretLeducAcknowledgments

This material is based on work supported by the National Science Foundation under grants IIS-
1718457  IIS-1617590  and CCF-1733556  and the ARO under award W911NF-17-1-0082. Gabriele
Farina is supported by a Facebook fellowship.

References
[1] Michael Bowling  Neil Burch  Michael Johanson  and Oskari Tammelin. Heads-up limit

hold’em poker is solved. Science  347(6218)  January 2015.

[2] Noam Brown and Tuomas Sandholm. Reduced space and faster convergence in imperfect-
information games via pruning. In International Conference on Machine Learning (ICML) 
2017.

[3] Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus

beats top professionals. Science  page eaao1733  Dec. 2017.

[4] Noam Brown and Tuomas Sandholm. Solving imperfect-information games via discounted re-
gret minimization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence  volume 33 
pages 1829–1836  2019.

[5] Noam Brown and Tuomas Sandholm. Superhuman AI for multiplayer poker. Science  365
(6456):885–890  2019. ISSN 0036-8075. doi: 10.1126/science.aay2400. URL https://
science.sciencemag.org/content/365/6456/885.

[6] Noam Brown  Sam Ganzfried  and Tuomas Sandholm. Hierarchical abstraction  distributed
equilibrium computation  and post-processing  with application to a champion no-limit Texas
Hold’em agent. In International Conference on Autonomous Agents and Multi-Agent Systems
(AAMAS)  2015.

[7] Noam Brown  Christian Kroer  and Tuomas Sandholm. Dynamic thresholding and pruning for

regret minimization. In AAAI Conference on Artiﬁcial Intelligence (AAAI)  2017.

[8] Noam Brown  Tuomas Sandholm  and Brandon Amos. Depth-limited solving for imperfect-

information games. arXiv preprint arXiv:1805.08195  2018.

[9] Neil Burch  Michael Johanson  and Michael Bowling. Solving imperfect information games

using decomposition. In AAAI Conference on Artiﬁcial Intelligence (AAAI)  2014.

[10] Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm for convex problems

with applications to imaging. Journal of Mathematical Imaging and Vision  2011.

[11] Antonin Chambolle and Thomas Pock. On the ergodic convergence rates of a ﬁrst-order

primal–dual algorithm. Mathematical Programming  159(1-2):253–287  2016.

[12] Chao-Kai Chiang  Tianbao Yang  Chia-Jung Lee  Mehrdad Mahdavi  Chi-Jen Lu  Rong Jin 
and Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning
Theory  pages 6–1  2012.

[13] Gabriele Farina  Christian Kroer  Noam Brown  and Tuomas Sandholm. Stable-predictive
optimistic counterfactual regret minimization. In International Conference on Machine Learning
(ICML)  2019.

[14] Sam Ganzfried and Tuomas Sandholm. Potential-aware imperfect-recall abstraction with earth
mover’s distance in imperfect-information games. In AAAI Conference on Artiﬁcial Intelligence
(AAAI)  2014.

[15] Sam Ganzfried and Tuomas Sandholm. Endgame solving in large imperfect-information games.
In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)  2015.
[16] Andrew Gilpin and Tuomas Sandholm. Lossless abstraction of imperfect information games.

Journal of the ACM  54(5)  2007.

[17] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimiza-

tion  2(3-4):157–325  2016.

[18] Samid Hoda  Andrew Gilpin  Javier Peña  and Tuomas Sandholm. Smoothing techniques for
computing Nash equilibria of sequential games. Mathematics of Operations Research  35(2) 
2010.

10

[19] Christian Kroer. First-order methods with increasing iterate averaging for solving saddle-point

problems. arXiv preprint arXiv:1903.10646  2019.

[20] Christian Kroer and Tuomas Sandholm. Extensive-form game abstraction with bounds. In

Proceedings of the ACM Conference on Economics and Computation (EC)  2014.

[21] Christian Kroer and Tuomas Sandholm. Imperfect-recall abstractions with bounds in games. In

Proceedings of the ACM Conference on Economics and Computation (EC)  2016.

[22] Christian Kroer  Kevin Waugh  Fatma Kılınç-Karzan  and Tuomas Sandholm. Faster ﬁrst-order
methods for extensive-form game solving. In Proceedings of the ACM Conference on Economics
and Computation (EC)  2015.

[23] Christian Kroer  Gabriele Farina  and Tuomas Sandholm. Solving large sequential games with
the excessive gap technique. In Proceedings of the Annual Conference on Neural Information
Processing Systems (NIPS)  2018.

[24] Christian Kroer  Kevin Waugh  Fatma Kılınç-Karzan  and Tuomas Sandholm. Faster algo-
rithms for extensive-form game solving via improved smoothing functions. Mathematical
Programming  pages 1–33  2018.

[25] H. W. Kuhn. A simpliﬁed two-person poker.

In H. W. Kuhn and A. W. Tucker  editors 
Contributions to the Theory of Games  volume 1 of Annals of Mathematics Studies  24  pages
97–103. Princeton University Press  Princeton  New Jersey  1950.

[26] Marc Lanctot  Richard Gibson  Neil Burch  Martin Zinkevich  and Michael Bowling. No-regret
learning in extensive-form games with imperfect recall. In International Conference on Machine
Learning (ICML)  2012.

[27] Matej Moravcik  Martin Schmid  Karel Ha  Milan Hladik  and Stephen Gaukrodger. Reﬁning
subgames in large imperfect information games. In AAAI Conference on Artiﬁcial Intelligence
(AAAI)  2016.

[28] Matej Moravˇcík  Martin Schmid  Neil Burch  Viliam Lisý  Dustin Morrill  Nolan Bard  Trevor
Davis  Kevin Waugh  Michael Johanson  and Michael Bowling. Deepstack: Expert-level
artiﬁcial intelligence in heads-up no-limit poker. Science  356(6337)  May 2017.

[29] Arkadi Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequali-
ties with Lipschitz continuous monotone operators and smooth convex-concave saddle point
problems. SIAM Journal on Optimization  15(1)  2004.

[30] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In

Conference on Learning Theory  pages 993–1019  2013.

[31] Sasha Rakhlin and Karthik Sridharan. Optimization  learning  and games with predictable
sequences. In Advances in Neural Information Processing Systems  pages 3066–3074  2013.

[32] Shai Shalev-Shwartz and Yoram Singer. A primal-dual perspective of online learning algorithms.

Machine Learning  69(2-3):115–142  2007.

[33] Finnegan Southey  Michael Bowling  Bryce Larson  Carmelo Piccione  Neil Burch  Darse
Billings  and Chris Rayner. Bayes’ bluff: Opponent modelling in poker. In Proceedings of the
21st Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  July 2005.

[34] Vasilis Syrgkanis  Alekh Agarwal  Haipeng Luo  and Robert E Schapire. Fast convergence of
regularized learning in games. In Advances in Neural Information Processing Systems  pages
2989–2997  2015.

[35] Oskari Tammelin  Neil Burch  Michael Johanson  and Michael Bowling. Solving heads-up
limit Texas hold’em. In Proceedings of the 24th International Joint Conference on Artiﬁcial
Intelligence (IJCAI)  2015.

[36] Bernhard von Stengel. Efﬁcient computation of behavior strategies. Games and Economic

Behavior  14(2):220–246  1996.

[37] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.
In International Conference on Machine Learning (ICML)  pages 928–936  Washington  DC 
USA  2003.

[38] Martin Zinkevich  Michael Bowling  Michael Johanson  and Carmelo Piccione. Regret mini-
mization in games with incomplete information. In Proceedings of the Annual Conference on
Neural Information Processing Systems (NIPS)  2007.

11

,Caglar Gulcehre
Francis Dutil
Adam Trischler
Yoshua Bengio
Gabriele Farina
Christian Kroer
Tuomas Sandholm