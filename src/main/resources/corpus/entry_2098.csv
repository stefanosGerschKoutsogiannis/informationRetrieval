2017,Filtering Variational Objectives,When used as a surrogate objective for maximum likelihood estimation in latent variable models  the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this  we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood  the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO  but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally  we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data.,Filtering Variational Objectives

Chris J. Maddison1 3 *  Dieterich Lawson 2 * George Tucker2 *

Nicolas Heess1  Mohammad Norouzi2  Andriy Mnih1  Arnaud Doucet3  Yee Whye Teh1

1DeepMind  2Google Brain  3University of Oxford
{cmaddis  dieterichl  gjt}@google.com

Abstract

When used as a surrogate objective for maximum likelihood estimation in latent
variable models  the evidence lower bound (ELBO) produces state-of-the-art results.
Inspired by this  we consider the extension of the ELBO to a family of lower bounds
deﬁned by a particle ﬁlter’s estimator of the marginal likelihood  the ﬁltering
variational objectives (FIVOs). FIVOs take the same arguments as the ELBO 
but can exploit a model’s sequential structure to form tighter bounds. We present
results that relate the tightness of FIVO’s bound to the variance of the particle ﬁlter’s
estimator by considering the generic case of bounds deﬁned as log-transformed
likelihood estimators. Experimentally  we show that training with FIVO results
in substantial improvements over training the same model architecture with the
ELBO on sequential data.

1

Introduction

Learning in statistical models via gradient descent is straightforward when the objective function
and its gradients are tractable. In the presence of latent variables  however  many objectives become
intractable. For neural generative models with latent variables  there are currently a few dominant
approaches: optimizing lower bounds on the marginal log-likelihood [1  2]  restricting to a class of
invertible models [3]  or using likelihood-free methods [4  5  6  7].
In this work  we focus on the
ﬁrst approach and introduce ﬁltering variational objectives (FIVOs)  a tractable family of objectives
for maximum likelihood estimation (MLE) in latent variable models with sequential structure.
Speciﬁcally  let x denote an observation of an X -valued random variable. We assume that the
process generating x involves an unobserved Z-valued random variable z with joint density p(x  z)
in some family P. The goal of MLE is to recover p ∈ P that maximizes the marginal log-likelihood 

log p(x) = log(cid:0)(cid:82) p(x  z) dz(cid:1)1. The difﬁculty in carrying out this optimization is that the log-

likelihood function is deﬁned via a generally intractable integral. To circumvent marginalization 
a common approach [1  2] is to optimize a variational lower bound on the marginal log-likelihood
[8  9]. The evidence lower bound L(x  p  q) (ELBO) is the most common such bound and is deﬁned
by a variational posterior distribution q(z|x) whose support includes p’s 

(cid:20)

(cid:21)

log

p(x  z)
q(z|x)

L(x  p  q) = E
q(z|x)

(1)
L(x  p  q) lower-bounds the marginal log-likelihood for any choice of q  and the bound is tight when
q is the true posterior p(z|x). Thus  the joint optimum of L(x  p  q) in p and q is the MLE. In practice 
it is common to restrict q to a tractable family of distributions (e.g.  a factored distribution) and to

= log p(x) − KL(q(z|x) (cid:107) p(z|x)) ≤ log p(x) .

*Equal contribution.
1We reuse p to denote the conditionals and marginals of the joint density.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

jointly optimize the ELBO over p and q with stochastic gradient ascent [1  2  10  11]. Because of
the KL penalty from q to p  optimizing (1) under these assumptions tends to force p’s posterior to
satisfy the factorizing assumptions of the variational family which reduces the capacity of the model
p. One strategy for addressing this is to decouple the tightness of the bound from the quality of q.
For example  [12] observed that Eq. (1) can be interpreted as the log of an unnormalized importance
weight with the proposal given by q  and that using N samples from the same proposal produces a
tighter bound  known as the importance weighted auto-encoder bound  or IWAE.
Indeed  it follows from Jensen’s inequality that the log of any unbiased positive Monte Carlo estimator
of the marginal likelihood results in a lower bound that can be optimized for MLE. The ﬁltering
variational objectives (FIVOs) build on this idea by treating the log of a particle ﬁlter’s likelihood
estimator as an objective function. Following [13]  we call objectives deﬁned as log-transformed
likelihood estimators Monte Carlo objectives (MCOs). In this work  we show that the tightness
of an MCO scales like the relative variance of the estimator from which it is constructed. It is
well-known that the variance of a particle ﬁlter’s likelihood estimator scales more favourably than
simple importance sampling for models with sequential structure [14  15]. Thus  FIVO can potentially
form a much tighter bound on the marginal log-likelihood than IWAE.
The main contributions of this work are introducing ﬁltering variational objectives and a more
careful study of Monte Carlo objectives. In Section 2  we review maximum likelihood estimation via
maximizing the ELBO. In Section 3  we study Monte Carlo objectives and provide some of their basic
properties. We deﬁne ﬁltering variational objectives in Section 4  discuss details of their optimization 
and present a sharpness result. Finally  we cover related work and present experiments showing that
sequential models trained with FIVO outperform models trained with ELBO or IWAE in practice.

2 Background

We brieﬂy review techniques for optimizing the ELBO as a surrogate MLE objective. We restrict our
focus to latent variable models in which the model pθ(x  z) factors into tractable conditionals pθ(z)
and pθ(x|z) that are parameterized differentiably by parameters θ. MLE in these models is then the
problem of optimizing log pθ(x) in θ. The expectation-maximization (EM) algorithm is an approach
to this problem which can be seen as coordinate ascent  fully maximizing L(x  pθ  q) alternately in q
and θ at each iteration [16  17  18]. Yet  EM rarely applies in general  because maximizing over q for
a ﬁxed θ corresponds to a generally intractable inference problem.
Instead  an approach with mild assumptions on the model is to perform gradient ascent following a
Monte Carlo estimator of the ELBO’s gradient [19  10]. We assume that q is taken from a family of
distributions parameterized differentiably by parameters φ. We can follow an unbiased estimator of the
ELBO’s gradient by sampling z ∼ qφ(z|x) and updating the parameters by θ(cid:48) = θ + η∇θ log pθ(x  z)
and φ(cid:48) = φ + η(log pθ(x  z) − log qφ(z|x))∇φ log qφ(z|x)  where the gradients are computed
conditional on the sample z and η is a learning rate. Such estimators follow the ELBO’s gradient in
expectation  but variance reduction techniques are usually necessary [10  20  13].
A lower variance gradient estimator can be derived if qφ is a reparameterizable distribution [1  2  21].
Reparameterizable distributions are those that can be simulated by sampling from a distribution
 ∼ d()  which does not depend on φ  and then applying a deterministic transformation z =
fφ(x  ). When pθ  qφ  and fφ are differentiable  an unbiased estimator of the ELBO gradient consists
of sampling  and updating the parameter by (θ(cid:48)  φ(cid:48)) = (θ  φ) + η∇(θ φ)(log pθ(x  fφ(x  )) −
log qφ(fφ(x  )|x)). Given   the gradients of the sampling process can ﬂow through z = fφ(x  ).
following gradients of
Unfortunately  when the variational
−KL(qφ(z|x) (cid:107) pθ(z|x)) tends to reduce the capacity of the model pθ to match the assumptions
of the variational family. This KL penalty can be “removed” by considering generalizations of the
ELBO whose tightness can be controlled by means other than the closenesss of p and q  e.g.  [12].
We consider this in the next section.

family of qφ is restricted 

3 Monte Carlo Objectives (MCOs)

Monte Carlo objectives (MCOs) [13] generalize the ELBO to objectives deﬁned by taking the log
of a positive  unbiased estimator of the marginal likelihood. The key property of MCOs is that

2

(cid:20)

(cid:21)

(cid:90) p(x  z)

q(z|x)

they are lower bounds on the marginal log-likelihood  and thus can be used for MLE. Motivated
by the previous section  we present results on the convergence of generic MCOs to the marginal
log-likelihood and show that the tightness of an MCO is closely related to the variance of the estimator
that deﬁnes it.
One can verify that the ELBO is a lower bound by using the concavity of log and Jensen’s inequality 

E

q(z|x)

log

p(x  z)
q(z|x)

≤ log

q(z|x) dz = log p(x).

(2)

This argument only relies only on unbiasedness of p(x  z)/q(z|x) when z ∼ q(z|x). Thus  we
can generalize this by considering any unbiased marginal likelihood estimator ˆpN (x) and treating
E[log ˆpN (x)] as an objective function over models p. Here N ∈ N indexes the amount of computation
needed to simulate ˆpN (x)  e.g.  the number of samples or particles.
Deﬁnition 1. Monte Carlo Objectives. Let ˆpN (x) be an unbiased positive estimator of p(x) 
E[ˆpN (x)] = p(x)  then the Monte Carlo objective LN(x  p) over p ∈ P deﬁned by ˆpN (x) is

LN(x  p) = E[log ˆpN (x)]

(3)

For example  the ELBO is constructed from a single unnormalized importance weight ˆp(x) =
p(x  z)/q(z|x). The IWAE bound [12] takes ˆpN (x) to be N averaged i.i.d. importance weights 

LIWAE

N

(x  p  q) =

E

q(zi|x)

log

1
N

p(x  zi)
q(zi|x)

(4)

(cid:34)

(cid:32)

N(cid:88)

i=1

(cid:33)(cid:35)

We consider additional examples in the Appendix. To avoid notational clutter  we omit the arguments
to an MCO  e.g.  the observations x or model p  when the default arguments are clear from context.
Whether we can compute stochastic gradients of LN efﬁciently depends on the speciﬁc form of the
estimator and the underlying random variables that deﬁne it.
Many likelihood estimators ˆpN (x) converge to p(x) almost surely as N → ∞ (known as strong
consistency). The advantage of a consistent estimator is that its MCO can be driven towards log p(x)
by increasing N. We present sufﬁcient conditions for this convergence and a description of the rate:
Proposition 1. Properties of Monte Carlo Objectives. Let LN(x  p) be a Monte Carlo objective
deﬁned by an unbiased positive estimator ˆpN (x) of p(x). Then 

(a) (Bound) LN(x  p) ≤ log p(x).
(b) (Consistency) If log ˆpN (x) is uniformly integrable (see Appendix for deﬁnition) and ˆpN (x)

is strongly consistent  then LN(x  p) → log p(x) as N → ∞.

(c) (Asymptotic Bias) Let g(N ) = E[(ˆpN (x) − p(x))6] be the 6th central moment. If the 1st

inverse moment is bounded  lim supN→∞ E[ˆpN (x)−1] < ∞  then

(cid:18) ˆpN (x)

(cid:19)

p(x)

+ O((cid:112)g(N )).

(5)

log p(x) − LN(x  p) =

1
2

var

Proof. See the Appendix for the proof and a sufﬁcient condition for controlling the ﬁrst inverse
moment when ˆpN (x) is the average of i.i.d. random variables.

In some cases  convergence of the bound to log p(x) is monotonic  e.g.  IWAE [12]  but this is not
true in general. The relative variance of estimators  var(ˆpN (x)/p(x))  tends to be well studied  so
property (c) gives us a tool for comparing the convergence rate of distinct MCOs. For example 
[14  15] study marginal likelihood estimators deﬁned by particle ﬁlters and ﬁnd that the relative
variance of these estimators scales favorably in comparison to naive importance sampling. This
suggests that a particle ﬁlter’s MCO  introduced in the next section  will generally be a tighter bound
than IWAE.

3

N (x1:T   p  q)

Algorithm 1 Simulating LFIVO
1: FIVO(x1:T   p  q  N ):
0}N
i=1 = {1/N}N
2: {wi
3: for t ∈ {1  . . .   T} do
i=1
for i ∈ {1  . . .   N} do
4:
t ∼ qt(zt|x1:t  zi
zi
5:
1:t = CONCAT(zi
zi
6:
t−1αt(zi
7:
8:
9:

(cid:17)
1:t−1)
1:t−1  zi
t)
i=1 wi
1:t)
ˆpt =
ˆpN (x1:t) = ˆpN (x1:t−1)ˆpt
1:t)/ˆpt}N
t}N
i=1 = {wi
{wi

(cid:16)(cid:80)N

t−1αt(zi

i=1

if resampling criteria satisﬁed by {wi
t  zi

i=1 = RSAMP({wi

{wi

t}N
i=1 then
1:t}N
i=1)

t  zi

10:
1:t}N
11:
12: return log ˆpN (x1:T )
13: RSAMP({wi  zi}N
i=1):
14: for i ∈ {1  . . .   N} do
a ∼ Categorical({wi}N
15:
yi = za
16:
17: return { 1

N   yi}N

i=1

i=1)

4 Filtering Variational Objectives (FIVOs)

The ﬁltering variational objectives (FIVOs) are a family of MCOs deﬁned by the marginal likelihood
estimator of a particle ﬁlter. For models with sequential structure  e.g.  latent variable models of audio
and text  the relative variance of a naive importance sampling estimator tends to scale exponentially
in the number of steps. In contrast  the relative variance of particle ﬁlter estimators can scale more
favorably with the number of steps—linearly in some cases [14  15]. Thus  the results of Section 3
suggest that FIVOs can serve as tighter objectives than IWAE for MLE in sequential models.
Let our observations be sequences of T X -valued random variables denoted x1:T   where xi:j ≡
(xi  . . .   xj). We also assume that the data generation process relies on a sequence of T unobserved
Z-valued latent variables denoted z1:T . We focus on sequential latent variable models that factor as a

series of tractable conditionals  p(x1:T   z1:T ) = p1(x1  z1)(cid:81)T

t=2 pt(xt  zt|x1:t−1  z1:t−1).

A particle ﬁlter is a sequential Monte Carlo algorithm  which propagates a population of N weighted
particles for T steps using a combination of importance sampling and resampling steps  see Alg. 1.
In detail  the particle ﬁlter takes as arguments an observation x1:T   the number of particles N  the
model distribution p  and a variational posterior q(z1:T|x1:T ) factored over t 

T(cid:89)

t=1

q(z1:T|x1:T ) =

qt(zt|x1:t  z1:t−1) .

(6)

The particle ﬁlter maintains a population {wi
1:t−1}N
At step t  the ﬁlter independently proposes an extension zi
trajectory zi

1:t−1. The weights wi

t−1  zi

t−1 are multiplied by the incremental importance weights 

i=1 of particles zi
t ∼ qt(zt|x1:t  zi

1:t−1 with weights wi
t−1.
1:t−1) to each particle’s

αt(zi

1:t) =

pt(xt  zi
qt(zi

t|x1:t−1  zi
t|x1:t  zi
1:t−1)

1:t−1)

 

(7)

if the effective sample size (ESS) of the population ((cid:80)N

t satisfy a resampling criteria  then a resampling step is
and renormalized. If the current weights wi
1:t are sampled in proportion to their weights from the current population
performed and N particles zi
with replacement. Common resampling schemes include resampling at every step and resampling
t)2)−1 drops below N/2 [22]. After
1:t are copied to the next step along

i=1(wi
resampling the weights are reset to 1. Otherwise  the particles zi
with the accumulated weights. See Fig. 1 for a visualization.
Instead of viewing Alg. 1 as an inference algorithm  we treat the quantity E[log ˆpN (x1:T )] as an
objective function over p. Because ˆpN (x1:T ) is an unbiased estimator of p(x1:T )  proven in the
Appendix and in [23  24  25  26]  it deﬁnes an MCO  which we call FIVO:
Deﬁnition 2. Filtering Variational Objectives. Let log ˆpN (x1:T ) be the output of Alg. 1 with inputs
(x1:T   p  q  N )  then LFIVO
ˆpN (x1:T ) is a strongly consistent estimator [23  24]. So if log ˆpN (x1:T ) is uniformly integrable  then
LFIVO
N (x1:T   p  q) → log p(x1:T ) as N → ∞. Resampling is the distinguishing feature of LFIVO
N ; if
resampling is removed  then FIVO reduces to IWAE. Resampling does add an amount of immediate
variance  but it allows the ﬁlter to discard low weight particles with high probability. This has the

N (x1:T   p  q) = E[log ˆpN (x1:T )] is a ﬁltering variational objective.

4

log ˆp1

log ˆp2

log ˆp3

log ˆp1

log ˆp2

log ˆp3

log ˆp4

log ˆp4∇ log wi

3

∇ log ˆp4

z1
1

z3
1

z2
1

z1
2

z2
2

z3
2

z1
3

z2
3

z3
3

resample {zi

1:3}3

i=1 ∼ wi

3

z2
1

z2
4

z2
1

z2
4

z2
2

z2
3

propose zi

4 ∼ q4(z4|x1:4  zi

1:3)

z2
2

z2
3

gradients

Figure 1: Visualizing FIVO; (Left) Resample from particle trajectories to determine inheritance in next
step  (middle) propose with qt and accumulate loss log ˆpt  (right) gradients (in the reparameterized
case) ﬂow through the lattice  objective gradients in solid red and resampling gradients in dotted blue.

effect of refocusing the distribution of particles to regions of higher mass under the posterior  and in
some sequential models can reduce the variance from exponential to linear in the number of time
steps [14  15]. Resampling is a greedy process  and it is possible that a particle discarded at step t 
could have attained a high mass at step T . In practice  the best trade-off is to use adaptive resampling
schemes [22]. If for a given x1:T   p  q a particle ﬁlter’s likelihood estimator improves over simple
importance sampling in terms of variance  we expect LFIVO
to be a tighter bound than L or LIWAE
.

N

N

4.1 Optimization

The FIVO bound can be optimized with the same stochastic gradient ascent framework used for
the ELBO. We found in practice it was effective simply to follow a Monte Carlo estimator of the
biased gradient E[∇(θ φ) log ˆpN (x1:T )] with reparameterized zi
t. This gradient estimator is biased 
as the full FIVO gradient has three kinds of terms: it has the term E[∇θ φ log ˆpN (x1:T )]  where
∇θ φ log ˆpN (x1:T ) is deﬁned conditional on the random variables of Alg. 1; it has gradient terms for
every distribution of Alg. 1 that depends on the parameters; and  if adaptive resampling is used  then
it has additional terms that account for the change in FIVO with respect to the decision to resample.
In this section  we derive the FIVO gradient when zi
t are reparameterized and a ﬁxed resampling
schedule is followed. We derive the full gradient in the Appendix.
In more detail  we assume that p and q are parameterized in a differentiable way by θ and φ. Assume
that q is from a reparameterizable family and that zi
t of Alg. 1 are reparameterized. Assume that we
use a ﬁxed resampling schedule  and let I(resampling at step t) be an indicator function indicating
whether a resampling occured at step t. Now  LFIVO
depends on the parameters via log ˆpN (x1:T ) and
the resampling probabilities wi

N

(cid:20)

E

∇(θ φ) log ˆpN (x1:T ) +

t in the density. Thus  ∇(θ φ) LFIVO
(cid:88)T

(cid:88)N

I(resampling at step t) log

N =

t=1

i=1

(cid:21)

ˆpN (x1:T )
ˆpN (x1:t)

∇(θ φ) log wi

t

(8)

t  the terms inside the expectation form
Given a single forward pass of Alg. 1 with reparameterized zi
a Monte Carlo estimator of Eq. (8). However  the terms from resampling events contribute to the
majority of the variance of the estimator. Thus  the gradient estimator that we found most effective
in practice consists only of the gradient ∇(θ φ) log ˆpN (x1:T )  the solid red arrows of Figure 1. We
explore this experimentally in Section 6.3.

4.2 Sharpness

As with the ELBO  FIVO is a variational objective taking a variational posterior q as an argument.
An important question is whether FIVO achieves the marginal log-likelihood at its optimal q. We can
only guarantee this for models in which z1:t−1 and xt are independent given x1:t−1.
Proposition 2. Sharpness of Filtering Variational Objectives. Let LFIVO
q∗(x1:T   p) = argmaxq LFIVO
p(z1:t−1|x1:t−1) for t ∈ {2  . . .   T}  then

N (x1:T   p  q) be a FIVO  and
N (x1:T   p  q). If p has independence structure such that p(z1:t−1|x1:t) =

q∗(x1:T   p)(z1:T ) = p(z1:T|x1:T ) and LFIVO

N (x1:T   p  q∗(x1:T   p)) = log p(x1:T ) .

Proof. See Appendix.

5

Most models do not satisfy this assumption  and deriving the optimal q in general is complicated by
the resampling dynamics. For the restricted the model class in Proposition 2  the optimal qt does
not condition on future observations xt+1:T . We explored this experimentally with richer models
in Section 6.4  and found that allowing qt to condition on xt+1:T does not reliably improve FIVO.
This is consistent with the view of resampling as a greedy process that responds to each intermediate
distribution as if it were the ﬁnal. Still  we found that the impact of this effect was outweighed by the
advantage of optimizing a tighter bound.

5 Related Work

The marginal log-likelihood is a central quantity in statistics and probability  and there has long been
an interest in bounding it [27]. The literature relating to the bounds we call Monte Carlo objectives
has typically focused on the problem of estimating the marginal likelihood itself. [28  29] use Jensen’s
inequality in a forward and reverse estimator to detect the failure of inference methods. IWAE [12] is
a clear inﬂuence on this work  and FIVO can be seen as an extension of this bound. The ELBO enjoys
a long history [8] and there have been efforts to improve the ELBO itself. [30] generalize the ELBO
by considering arbitrary operators of the model and variational posterior. More closely related to
this work is a body of work improving the ELBO by increasing the expressiveness of the variational
posterior. For example  [31  32] augment the variational posterior with deterministic transformations
with ﬁxed Jacobians  and [33] extend the variational posterior to admit a Markov chain.
Other approaches to learning in neural latent variable models include [34]  who use importance
sampling to approximate gradients under the posterior  and [35]  who use sequential Monte Carlo
to approximate gradients under the posterior. These are distinct from our contribution in the sense
that for them inference for the sake of estimation is the ultimate goal. To our knowledge the idea
of treating the output of inference as an objective in and of itself  while not completely novel  has
not been fully appreciated in the literature. Although  this idea shares inspiration with methods that
optimize the convergence of Markov chains [36].
We note that the idea to optimize the log estimator of a particle ﬁlter was independently and
concurrently considered in [37  38]. In [37] the bound we call FIVO is cast as a tractable lower bound
on the ELBO deﬁned by the particle ﬁlter’s non-parameteric approximation to the posterior. [38]
additionally derive an expression for FIVO’s bias as the KL between the ﬁlter’s distribution and a
certain target process. Our work is distinguished by our study of the convergence of MCOs in N 
which includes FIVO  our investigation of FIVO sharpness  and our experimental results on stochastic
RNNs.

6 Experiments

In our experiments  we sought to: (a) compare models trained with ELBO  IWAE  and FIVO bounds
in terms of ﬁnal test log-likelihoods  (b) explore the effect of the resampling gradient terms on FIVO 
(c) investigate how the lack of sharpness affects FIVO  and (d) consider how models trained with
FIVO use the stochastic state. To explore these questions  we trained variational recurrent neural
networks (VRNN) [39] with the ELBO  IWAE  and FIVO bounds using TensorFlow [40] on two
benchmark sequential modeling tasks: natural speech waveforms and polyphonic music. These
datasets are known to be difﬁcult to model without stochastic latent states [41].
The VRNN is a sequential latent variable model that combines a deterministic recurrent neu-
ral network (RNN) with stochastic latent states zt at each step.
The observation distri-
bution over xt is conditioned directly on zt and indirectly on z1:t−1 via the RNN’s state
ht(zt−1  xt−1  ht−1). For a length T sequence  the model’s posterior factors into the condition-
t=1 pt(zt|ht(zt−1  xt−1  ht−1))gt(xt|zt  ht(zt−1  xt−1  ht−1))  and the variational posterior
t=1 qt(zt|ht(zt−1  xt−1  ht−1)  xt). All distributions over latent variables are factorized
Gaussians  and the output distributions gt depend on the dataset. The RNN is a single-layer LSTM
and the conditionals are parameterized by fully connected neural networks with one hidden layer
of the same size as the LSTM hidden layer. We used the residual parameterization [41] for the
variational posterior.

als(cid:81)T
factors as(cid:81)T

6

N Bound Nottingham

4

8

16

ELBO
IWAE
FIVO
ELBO
IWAE
FIVO
ELBO
IWAE
FIVO

-3.00
-2.75
-2.68
-3.01
-2.90
-2.77
-3.02
-2.85
-2.58

JSB MuseData
-8.60
-7.86
-6.90
-8.61
-7.40
-6.79
-8.63
-7.41
-6.72

-7.15
-7.20
-6.20
-7.19
-7.15
-6.12
-7.18
-7.13
-5.89

Piano-midi.de

-7.81
-7.86
-7.76
-7.83
-7.84
-7.45
-7.85
-7.79
-7.43

4

8

N Bound
ELBO
IWAE
FIVO
ELBO
IWAE
FIVO
ELBO
IWAE
FIVO

16

TIMIT

64 units
0
-160
5 691
2 771
3 977
6 023
1 676
3 236
8 630

256 units
10 438
11 054
17 822
9 819
11 623
21 449
9 918
13 069
21 536

Table 1: Test set marginal log-likelihood bounds for models trained with ELBO  IWAE  and FIVO.
For ELBO and IWAE models  we report max{L LIWAE
128 }. For FIVO models  we report LFIVO
128 .
Pianoroll results are in nats per timestep  TIMIT results are in nats per sequence relative to ELBO
with N = 4. For details on our evaluation methodology and absolute numbers see the Appendix.

128  LFIVO

For FIVO we resampled when the ESS of the particles dropped below N/2. For FIVO and IWAE we
used a batch size of 4  and for the ELBO  we used batch sizes of 4N to match computational budgets
(resampling is O(N ) with the alias method). For all models we report bounds using the variational
posterior trained jointly with the model. For models trained with FIVO we report LFIVO
128 . To provide
strong baselines  we report the maximum across bounds  max{L LIWAE
128 }  for models trained
with ELBO and IWAE. Additional details in the Appendix.

128  LFIVO

6.1 Polyphonic Music

We evaluated VRNNs trained with the ELBO  IWAE  and FIVO bounds on 4 polyphonic music
datasets: the Nottingham folk tunes  the JSB chorales  the MuseData library of classical piano and
orchestral music  and the Piano-midi.de MIDI archive [42]. Each dataset is split into standard train 
valid  and test sets and is represented as a sequence of 88-dimensional binary vectors denoting the
notes active at the current timestep. We mean-centered the input data and modeled the output as a set
of 88 factorized Bernoulli variables. We used 64 units for the RNN hidden state and latent state size
for all polyphonic music models except for JSB chorales models  which used 32 units. We report
bounds on average log-likelihood per timestep in Table 1. Models trained with the FIVO bound
signiﬁcantly outperformed models trained with either the ELBO or the IWAE bounds on all four
datasets. In some cases  the improvements exceeded 1 nat per timestep  and in all cases optimizing
FIVO with N = 4 outperformed optimizing IWAE or ELBO for N = {4  8  16}.

6.2 Speech

The TIMIT dataset is a standard benchmark for sequential models that contains 6300 utterances
with an average duration of 3.1 seconds spoken by 630 different speakers. The 6300 utterances are
divided into a training set of size 4620 and a test set of size 1680. We further divided the training
set into a validation set of size 231 and a training set of size 4389  with the splits exactly as in
[41]. Each TIMIT utterance is represented as a sequence of real-valued amplitudes which we split
into a sequence of 200-dimensional frames  as in [39  41]. Data preprocessing was limited to mean
centering and variance normalization as in [41]. For TIMIT  the output distribution was a factorized
Gaussian  and we report the average log-likelihood bound per sequence relative to models trained
with ELBO. Again  models trained with FIVO signiﬁcantly outperformed models trained with IWAE
or ELBO  see Table 1.

6.3 Resampling Gradients

All models in this work (except those in this section) were trained with gradients that did not include
the term in Eq. (8) that comes from resampling steps. We omitted this term because it has an outsized
effect on gradient variance  often increasing it by 6 orders of magnitude. To explore the effects of this
term experimentally  we trained VRNNs with and without the resampling gradient term on the TIMIT
and polyphonic music datasets. When using the resampling term  we attempted to control its variance

7

Figure 2: (Left) Graph of LFIVO
128 over training comparing models trained with and without the
resampling gradient terms on TIMIT with N = 4. (Right) KL divergence from q(z1:T|x1:T ) to
p(z1:T ) for models trained on the JSB chorales with N = 16.

Bound
ELBO
ELBO+s
IWAE
IWAE+s
FIVO
FIVO+s

Nottingham

-2.40
-2.59
-2.52
-2.37
-2.29
-2.34

JSB MuseData
-5.48
-5.53
-5.77
-4.63
-4.08
-3.83

-6.54
-6.48
-6.54
-6.47
-5.80
-5.87

Piano-midi.de

-6.68
-6.77
-6.74
-6.74
-6.41
-6.34

TIMIT
0
-925
1 469
2 630
6 991
9 773

Table 2: Train set marginal log-likelihood bounds for models comparing smoothing (+s) and non-
smoothing variational posteriors. We report max{L LIWAE
128 } for ELBO and IWAE models
and LFIVO
128 for FIVO models. All models were trained with N = 4. Pianoroll results are in nats per
timestep  TIMIT results are in nats per sequence relative to non-smoothing ELBO. For details on our
evaluation methodology and absolute numbers see the Appendix.

128  LFIVO

using a moving-average baseline linear in the number of timesteps. For all datasets  models trained
without the resampling gradient term outperformed models trained with the term by a large margin
on both the training set and held-out data. Many runs with resampling gradients failed to improve
beyond random initialization. A representative pair of train log-likelihood curves is shown in Figure
2 — gradients without the resampling term led to earlier convergence and a better solution. We stress
that this is an empirical result — in principle biased gradients can lead to divergent behaviour. We
leave exploring strategies to reduce the variance of the unbiased estimator to future work.

6.4 Sharpness
FIVO does not achieve the marginal log-likelihood at its optimal variational posterior q∗  because the
optimal q∗ does not condition on future observations (see Section 4.2). In contrast  ELBO and IWAE
are sharp  and their q∗s depend on future observations. To investigate the effects of this  we deﬁned a
smoothing variant of the VRNN in which q takes as additional input the hidden state of a deterministic
RNN run backwards over the observations  allowing q to condition on future observations. We trained
smoothing VRNNs using ELBO  IWAE  and FIVO  and report evaluation on the training set (to
isolate the effect on optimization performance) in Table 2 . Smoothing helped models trained with
IWAE  but not enough to outperform models trained with FIVO. As expected  smoothing did not
reliably improve models trained with FIVO. Test set performance was similar  see the Appendix for
details.

6.5 Use of Stochastic State

A known pathology when training stochastic latent variable models with the ELBO is that stochastic
states can go unused. Empirically  this is associated with the collapse of variational posterior
q(z|x) network to the model prior p(z) [43]. To investigate this  we plot the KL divergence from
q(z1:T|x1:T ) to p(z1:T ) averaged over the dataset (Figure 2). Indeed  the KL of models trained with

8

01234561M Gradient Updates86420246Train Log-likelihood1e4Without Resampling Gradient TermWith Resampling Gradient Term01020304050601k Gradient Updates10-510-410-310-210-1100101KL DivergenceFIVOIWAEELBOELBO collapsed during training  whereas the KL of models trained with FIVO remained high  even
while achieving a higher log-likelihood bound.

7 Conclusions

We introduced the family of ﬁltering variational objectives  a class of lower bounds on the log
marginal likelihood that extend the evidence lower bound. FIVOs are suited for MLE in neural latent
variable models. We trained models with the ELBO  IWAE  and FIVO bounds and found that the
models trained with FIVO signiﬁcantly outperformed other models across four polyphonic music
modeling tasks and a speech waveform modeling task. Future work will include exploring control
variates for the resampling gradients  FIVOs deﬁned by more sophisticated ﬁltering algorithms  and
new MCOs based on differentiable operators like leapfrog operators with deterministically annealed
temperatures. In general  we hope that this paper inspires the machine learning community to take a
fresh look at the literature of marginal likelihood estimators—seeing them as objectives instead of
algorithms for inference.

Acknowledgments

We thank Matt Hoffman  Matt Johnson  Danilo J. Rezende  Jascha Sohl-Dickstein  and Theophane
Weber for helpful discussions and support in this project. A. Doucet was partially supported by the
EPSRC grant EP/K000276/1. Y. W. Teh’s research leading to these results has received funding
from the European Research Council under the European Union’s Seventh Framework Programme
(FP7/2007-2013) ERC grant agreement no. 617071.

References
[1] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. ICLR  2014.
[2] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation

and approximate inference in deep generative models. ICML  2014.

[3] Laurent Dinh  Jascha Sohl-Dickstein  and Samy Bengio. Density estimation using real nvp.

arXiv preprint arXiv:1605.08803  2016.

[4] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil

Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In NIPS  2014.

[5] Sebastian Nowozin  Botond Cseke  and Ryota Tomioka. f-gan: Training generative neural
samplers using variational divergence minimization. arXiv preprint arXiv:1606.00709  2016.
[6] Dustin Tran  Rajesh Ranganath  and David M Blei. Deep and hierarchical implicit models.

arXiv preprint arXiv:1702.08896  2017.

[7] Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv

preprint arXiv:1610.03483  2016.

[8] Michael I Jordan  Zoubin Ghahramani  Tommi S Jaakkola  and Lawrence K Saul. An in-
troduction to variational methods for graphical models. Machine learning  37(2):183–233 
1999.

[9] Matthew J. Beal. Variational algorithms for approximate Bayesian inference. 2003.
[10] Rajesh Ranganath  Sean Gerrish  and David Blei. Black box variational inference. In AISTATS 

2014.

[11] Alp Kucukelbir  Dustin Tran  Rajesh Ranganath  Andrew Gelman  and David M Blei. Automatic

differentiation variational inference. arXiv preprint arXiv:1603.00788  2016.

[12] Yuri Burda  Roger Grosse  and Ruslan Salakhutdinov. Importance weighted autoencoders.

ICLR  2016.

[13] Andriy Mnih and Danilo J Rezende. Variational inference for Monte Carlo objectives. arXiv

preprint arXiv:1602.06725  2016.

[14] Frédéric Cérou  Pierre Del Moral  and Arnaud Guyader. A nonasymptotic theorem for unnor-

malized Feynman–Kac particle models. Ann. Inst. H. Poincaré B  47(3):629–649  2011.

9

[15] Jean Bérard  Pierre Del Moral  and Arnaud Doucet. A lognormal central limit theorem for

particle approximations of normalizing constants. Electron. J. Probab.  19(94):1–28  2014.

[16] Arthur P Dempster  Nan M Laird  and Donald B Rubin. Maximum likelihood from incomplete

data via the EM algorithm. J. R. Stat. Soc. Ser. B Stat. Methodol.  pages 1–38  1977.

[17] CF Jeff Wu. On the convergence properties of the EM algorithm. Ann. Stat.  pages 95–103 

1983.

[18] Radford M Neal and Geoffrey E Hinton. A view of the EM algorithm that justiﬁes incremental 

sparse  and other variants. In Learning in graphical models  pages 355–368. Springer  1998.

[19] Matthew D Hoffman  David M Blei  Chong Wang  and John William Paisley. Stochastic

variational inference. Journal of Machine Learning Research  14(1):1303–1347  2013.

[20] Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks.

arXiv preprint arXiv:1402.0030  2014.

[21] Yarin Gal. Uncertainty in Deep Learning. PhD thesis  University of Cambridge  2016.
[22] Arnaud Doucet and Adam M. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen
years later. In D. Crisan and B. Rozovsky  editors  The Oxford Handbook of Nonlinear Filtering 
pages 656–704. Oxford University Press  2011.

[23] Pierre Del Moral. Feynman-Kac formulae: genealogical and interacting particle systems with

applications. Springer Verlag  2004.

[24] Pierre Del Moral. Mean ﬁeld simulation for Monte Carlo integration. CRC Press  2013.
[25] Christophe Andrieu  Arnaud Doucet  and Roman Holenstein. Particle Markov chain Monte

Carlo methods. J. R. Stat. Soc. Ser. B Stat. Methodol.  72(3):269–342  2010.

[26] Michael K Pitt  Ralph dos Santos Silva  Paolo Giordani  and Robert Kohn. On some properties
of Markov chain Monte Carlo simulation methods based on the particle ﬁlter. J. Econometrics 
171(2):134–151  2012.

[27] Martin J Wainwright  Michael I Jordan  et al. Graphical models  exponential families  and

variational inference. Foundations and Trends in Machine Learning  1(1–2):1–305  2008.

[28] Roger B Grosse  Zoubin Ghahramani  and Ryan P Adams. Sandwiching the marginal likelihood

using bidirectional Monte Carlo. arXiv preprint arXiv:1511.02543  2015.

[29] Yuri Burda  Roger Grosse  and Ruslan Salakhutdinov. Accurate and conservative estimates of

MRF log-likelihood using reverse annealing. In AISTATS  2015.

[30] Rajesh Ranganath  Dustin Tran  Jaan Altosaar  and David Blei. Operator variational inference.

In NIPS  2016.

[31] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows.

ICML  2015.

[32] Diederik P Kingma  Tim Salimans  Rafal Jozefowicz  Xi Chen  Ilya Sutskever  and Max Welling.

Improved variational inference with inverse autoregressive ﬂow. In NIPS  2016.

[33] Tim Salimans  Diederik Kingma  and Max Welling. Markov chain Monte Carlo and variational

inference: Bridging the gap. In ICML  2015.

[34] Jörg Bornschein and Yoshua Bengio. Reweighted wake-sleep. ICLR  2015.
[35] Shixiang Gu  Zoubin Ghahramani  and Richard E Turner. Neural adaptive sequential Monte

Carlo. In NIPS  2015.

[36] Yoshua Bengio  Li Yao  Guillaume Alain  and Pascal Vincent. Generalized denoising auto-

encoders as generative models. In NIPS  2013.

[37] Christian A Naesseth  Scott W Linderman  Rajesh Ranganath  and David M Blei. Variational

sequential Monte Carlo. arXiv preprint arXiv:1705.11140  2017.

[38] Tuan Anh Le  Maximilian Igl  Tom Jin  Tom Rainforth  and Frank Wood. Auto-encoding

sequential Monte Carlo. arXiv preprint arXiv:1705.10306  2017.

[39] Junyoung Chung  Kyle Kastner  Laurent Dinh  Kratarth Goel  Aaron C Courville  and Yoshua

Bengio. A recurrent latent variable model for sequential data. In NIPS  2015.

10

[40] Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro 
Greg S Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  et al. Tensorﬂow: Large-scale
machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 
2016.

[41] Marco Fraccaro  Søren Kaae Sønderby  Ulrich Paquet  and Ole Winther. Sequential neural

models with stochastic layers. In NIPS  2016.

[42] Nicolas Boulanger-Lewandowski  Yoshua Bengio  and Pascal Vincent. Modeling temporal
dependencies in high-dimensional sequences: Application to polyphonic music generation and
transcription. ICML  2012.

[43] Samuel R Bowman  Luke Vilnis  Oriol Vinyals  Andrew M Dai  Rafal Jozefowicz  and Samy
Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349 
2015.

11

,Chris Maddison
John Lawson
George Tucker
Nicolas Heess
Andriy Mnih
Arnaud Doucet
Yee Teh