2018,A Dual Framework for Low-rank Tensor Completion,One of the popular approaches for low-rank tensor completion is to use the latent trace norm regularization. However  most existing works in this direction learn a sparse combination of tensors. In this work  we fill this gap by proposing a variant of the latent trace norm that helps in learning a non-sparse combination of tensors. We develop a dual framework for solving the low-rank tensor completion problem. We first show a novel characterization of the dual solution space with an interesting factorization of the optimal solution. Overall  the optimal solution is shown to lie on a Cartesian  product of Riemannian manifolds. Furthermore  we exploit the versatile Riemannian optimization framework for proposing computationally efficient trust region algorithm. The experiments illustrate the efficacy of the proposed algorithm on several real-world datasets across applications.,A dual framework for low-rank tensor completion

Madhav Nimishakavi∗  Pratik Jawanpuria†  Bamdev Mishra†

∗Indian Institute of Science  India

† Microsoft  India

madhav@iisc.ac.in  {pratik.jawanpuria bamdevm}@microsoft.com

Abstract

One of the popular approaches for low-rank tensor completion is to use the latent
trace norm regularization. However  most existing works in this direction learn a
sparse combination of tensors. In this work  we ﬁll this gap by proposing a variant
of the latent trace norm that helps in learning a non-sparse combination of tensors.
We develop a dual framework for solving the low-rank tensor completion problem.
We ﬁrst show a novel characterization of the dual solution space with an interesting
factorization of the optimal solution. Overall  the optimal solution is shown to lie on
a Cartesian product of Riemannian manifolds. Furthermore  we exploit the versatile
Riemannian optimization framework for proposing computationally efﬁcient trust
region algorithm. The experiments illustrate the efﬁcacy of the proposed algorithm
on several real-world datasets across applications.

1

Introduction

Tensors are multidimensional or K-way arrays  which provide a natural way to represent multi-modal
data [10  11]. Low-rank tensor completion problem  in particular  aims to recover a low-rank tensor
from partially observed tensor [2]. This problem has numerous applications in image/video inpainting
[27  26]  link-prediction [14]  and recommendation systems [39]  to name a few.
In this work  we focus on trace norm regularized low-rank tensor completion problem of the form

(cid:107)WΩ − YΩ(cid:107)2

F +

R(W) 

1
λ

min

W∈Rn1×n2×...×nK

(1)
where YΩ ∈ Rn1×...×nK is a partially observed K- mode tensor  whose entries are only known for a
subset of indices Ω. (WΩ)(i1 ... iK ) = W(i1 ... iK )  if (i1  . . .   iK) ∈ Ω and 0 otherwise  (cid:107)·(cid:107)F is the
Frobenius norm   R(·) is a low-rank promoting regularizer  and λ > 0 is the regularization parameter.
Similar to the matrix completion problem  the trace norm regularization has been used to enforce the
low-rank constraint for the tensor completion problem. The works [41  42] discuss the overlapped
and latent trace norm regularizations for tensors. In particular  [42  45] show that the latent trace
norm has certain better tensor reconstruction bounds. The latent trace norm regularization learns the
tensor as a sparse combination of different tensors. In our work  we empirically motivate the need
for learning non-sparse combination of tensors and propose a variant of the latent trace norm that
learns a non-sparse combination of tensors. We show a novel characterization of the solution space
that allows for a compact storage of the tensor  thereby allowing to develop scalable optimization
formulations. Concretely  we make the following contributions in this paper.

• We propose a novel trace norm regularizer for low-rank tensor completion problem  which
learns a tensor as a non-sparse combination of tensors. In contrast  the more popular latent
trace norm regularizer [41  42  45] learns a highly sparse combination of tensors. Non-sparse
combination helps in capturing information along all the modes.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

• We propose a dual framework for analyzing the problem formulation. This provides
interesting insights into the solution space of the tensor completion problem  e.g.  how the
solutions along different modes are related  allowing a compact representation of the tensor.
• Exploiting the characterization of the solution space  we develop a ﬁxed-rank formulation.
Our optimization problem is on Riemannian spectrahedron manifolds and we propose
computationally efﬁcient trust-region algorithm for our formulation.

Numerical comparisons on real-world datasets for different applications such as video and
hyperspectral-image completion  link prediction  and movie recommendation show that the pro-
posed algorithm outperforms state-of-the-art latent trace norm regularized algorithms. The proofs of
all the theorems and lemmas and additional experimental details are provided in the longer version of
the paper [32]. Our codes are available at https://pratikjawanpuria.com/.

2 Related work

norm is deﬁned as: R(W) := (cid:80)K

Trace norm regularized tensor completion formulations. The works [27  42  37  34  9] discuss
the overlapped trace norm regularization for tensor learning. The overlapped trace norm is motivated
as a convex proxy for minimizing the Tucker (multilinear) rank of a tensor. The overlapped trace
k=1 (cid:107)Wk(cid:107)∗  where Wk is the mode-k matrix unfolding of the
tensor W [25] and (cid:107)·(cid:107)∗ denotes the trace norm regularizer. Wk is a nk × Πj(cid:54)=k nj matrix obtained
by concatenating mode-k ﬁbers (column vectors) of the form W(i1 ... ik−1 : ik+1 ... iK ) [25].
Latent trace norm is another convex regularizer used for low-rank tensor learning [41  43  42  45  17].
In this setting  the tensor W is modeled as sum of K (unknown) tensors W (1)  . . .  W (K) such that
W (k)

are low-rank matrices. The latent trace norm is deﬁned as:

k

√
k (cid:107)∗ scaled by 1/

A variant of the latent trace norm ((cid:107)W (k)
nk) is analyzed in [45]. Latent trace norm
and its scaled variant achieve better recovery bounds than overlapped trace norm [42  45]. Recently 
[17] proposed a scalable latent trace norm based Frank-Wolfe algorithm for tensor completion.
The latent trace norm (2) corresponds to the sparsity inducing (cid:96)1-norm penalization across (cid:107)W (k)
k (cid:107)∗.
Hence  it learns W as a sparse combination of W (k). In case of high sparsity  it may result in
selecting only one of the tensors W (k) as W  i.e.  W = W (k) for some k  in which case W is
essentially learned as a low-rank matrix. In several real-world applications  tensor data cannot be
mapped to a low-rank matrix structure and they require a higher order structure. Therefore  we
propose a regularizer which learns a non-sparse combination of W (k). Non-sparse norms have led to
better generalization performance in other machine learning settings [12  38  22].
We show the beneﬁt of learning a non-sparse mixture of tensors as against a sparse mixture on two
datasets: Ribeira and Baboon (refer Section 5 for details). Figures 1(a) and 1(b) show the relative
sparsity of the optimally learned tensors in the mixture as learned by the (cid:96)1-regularized latent trace
norm based model (2) [42  45  17] versus the proposed (cid:96)2-regularized model (discussed in Section 3).
k (cid:107)W (k)(cid:107)F . In both
the datasets  our model learns a non-sparse combination of tensors  whereas the latent trace norm

The relative sparsity for each W (k) in the mixture is computed as (cid:107)W (k)(cid:107)F /(cid:80)

R(W) :=

(cid:80)K
k=1 W (k)=W; W (k)∈Rn1×...×nK

inf

(cid:80)K
k=1 (cid:107)W (k)

k (cid:107)∗ 

(2)

(a) Ribeira

(b) Baboon

(c) Ribeira

(d) Baboon

Figure 1: (a) & (b) Relative sparsity of each tensor in the mixture of tensors for Ribeira and Baboon datasets.
Our proposed formulation learns a (cid:96)2-norm based non-sparse combination of tensors; (c) & (d) show that the
proposed non-sparse combination obtain better generalization performance on both the datasets.

2

based model learns a highly skewed mixture of tensors. The proposed non-sparse tensor combination
also leads to better generalization performance  as can be observed in the Figures 1(c) and 1(d). In
the particular case of Baboon dataset  the latent trace norm essentially learns W as a low-rank matrix
(W = W (3)) and consequently obtains poor generalization.
Other tensor completion formulations. Other approaches for low-rank tensor completion include
tensor decomposition methods like Tucker and CP [25  10  11]. They generalize the notion of singular
value decomposition of matrices to tensors. Recently  [26] exploits the Riemannian geometry of ﬁxed
multilinear rank to learn factor matrices and the core tensor. They propose a computationally efﬁcient
non-linear conjugate gradient method for optimization over manifolds of tensors of ﬁxed multilinear
rank. [24] further propose an efﬁcient preconditioner for low-rank tensor learning with the Tucker
decomposition. [49] propose a Bayesian probabilistic CP model for performing tensor completion.
Tensor completion algorithms based on tensor tubal-rank have been recently proposed in [48  28].

k (cid:107)2∗ 

k

k

(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:80)

3 Non-sparse latent trace norm and duality
We propose the following formulation for learning the low-rank tensor W
(cid:107)W (k)

+(cid:80)

W (k)

Ω − YΩ

F

1
λk

min

k (cid:107)∗.

W (k)∈Rn1×...×nK

where W =(cid:80)

(3)
k W (k) is the learned tensor. It should be noted that the proposed regularizer in (3)
k (cid:107)∗. In contrast  the latent trace norm regularizer (2) has the (cid:96)1-norm

employs the (cid:96)2-norm over (cid:107)W (k)
over (cid:107)W (k)
While the existing tensor completion approaches [24  17  26  27  42  37] mostly discuss a primal
formulation similar to (1)  we propose a novel dual framework for our analysis. The use of dual
framework for learning low-rank matrices [46  20]  multi-task problems [33  21  19]  etc.  often leads
to novel insights into the solution space of the primal problem.
We begin by discussing how to obtain the dual formulation of (3). Later  we explain how the insights
from the dual framework motivate us to propose a novel ﬁxed-rank formulation. As a ﬁrst step  we
exploit the following variational characterization of the trace norm studied in [3  Theorem 4.1].
Given X ∈ Rd×T   the following result holds:

min

(cid:113)

(cid:104)Θ†  XX(cid:62)(cid:105) 

(cid:107)X(cid:107)2∗ =
(cid:112)

XX(cid:62)/trace(

Θ∈P d range(X)⊆range(Θ)

(4)
where P d denotes the set of d × d positive semi-deﬁnite matrices with unit trace  Θ† denotes the
pseudo-inverse of Θ  range(Θ) = {Θz : z ∈ Rd}  and (cid:104)· ·(cid:105) is the inner product. The expression
XX(cid:62)) [3]  and hence the ranks of Θ and X are equal at
for optimal Θ∗ is Θ∗ =
optimality. Thus  (4) implicitly transfers the low-rank constraint on X (due to trace norm) to an
auxiliary variable Θ ∈ P d. It is well known that positive semi-deﬁnite matrix Θ with unit trace
constraint implies the (cid:96)1-norm constraint on the eigenvalues of Θ  leading to low-rankedness of Θ.
The result (4) has also been recently employed to obtain new factorization insights for structured
low-rank matrices [20].
Using the result (4) in (3) leads to K auxiliary matrices  one Θk ∈ P nk corresponding to every W (k)
(mode-k matrix unfolding of the tensor W (k)). It should also be noted that Θk ∈ P nk are low-rank
matrices. We now present the following theorem that states an equivalent minimax formulation of (3).

k

F −(cid:88)

min

Θ1∈P n1  ... ΘK∈P nK

Theorem 1 An equivalent minimax formulation of the problem (3) is
λk
2

maxZ∈C (cid:104)Z YΩ(cid:105) − 1
The optimal solution of (3) is given by W∗ =(cid:80)

(5)
where Z is the dual tensor variable corresponding to the primal problem (3) and Zk is the mode-k
unfolding of Z. The set C := {Z ∈ Rn1×...×nK : Z(i1 ... iK ) = 0  (i1  . . .   iK) /∈ Ω} constrains Z
K Z∗} be the optimal solution of (5).
to be a sparse tensor with |Ω| non-zero entries. Let {Θ∗
k) ∀k and
×k denotes the tensor-matrix multiplication along mode k.

k W (k)∗  where W (k)∗ = λk(Z∗ ×k Θ∗

(cid:104)Θk ZkZ(cid:62)
k (cid:105) 

1  . . .   Θ∗

(cid:107)Z(cid:107)2

4

k

3

Algorithm 1 Proposed Riemannian trust-region algorithm for (7).
Input: YΩ  rank (r1  . . .   rK)  regularization parameter λ  and tolerance .
Initialize : u ∈ M.
repeat

1: Compute the gradient ∇u(cid:96) for (7) as given in Lemma 1.
2: Compute the search direction which minimizes the trust-region subproblem.
3: Update x with the retraction step to maintain strict feasibility on M. Speciﬁcally for the

It makes use of ∇u(cid:96) and its directional derivative presented in Lemma 1 for (7).
spectrahedron manifold  Uk ← (Uk + Vk)/(cid:107)Uk + Vk(cid:107)F   where Vk is the search direction.

until (cid:107)∇u(cid:96)(cid:107)F < .
Output: u∗

Remark 1: Theorem 1 shows that the optimal solutions W (k)∗ for all k in (3) are completely charac-
terized by a single sparse tensor Z∗ and K low-rank positive semi-deﬁnite matrices {Θ∗
K}.
It should be noted that such a novel relationship of W (k)∗ (for all k) with each other is not evident
from the primal formulation (3).
We next present the following result related to the form of the optimal solution of (3).

1  . . .   Θ∗

Corollary 1 (Representer theorem) The optimal solution of the primal problem (3) admits a repre-
sentation of the form: W (k)∗ = λk(Z ×k Θk) ∀k  where Z ∈ C and Θk ∈ P nk.
k ∈ P nk is a low-rank positive semi-deﬁnite
As discussed earlier in the section  the optimal Θ∗
matrix for all k. In spite of the low-rankness of the optimal solution  an algorithm for (5) need not
produce intermediate iterates that are low rank. From the perspective of large-scale applications  this
observation as well as other computational efﬁciency concerns discussed below motivate to exploit a
ﬁxed-rank parameterization of Θk for all k.
Fixed-rank parameterization. We propose to explicitly constrain the rank of Θk to rk as follows:
(6)
r := {U ∈ Rn×r : (cid:107)U(cid:107)F = 1}. In large-scale tensor completion problems  it
where Uk ∈ S nk
is common to set rk (cid:28) nk  where the ﬁxed-rank parameterization (6) of Θk has a two-fold advantage.
First  the search space dimension drastically reduces from nk((nk + 1)/2 − 1)  which is quadratic in
tensor dimensions  to nkrk − 1 − rk(rk − 1)/2  which is linear in tensor dimensions [23]. Second 
enforcing the constraint Uk ∈ S nk
rk costs O(nkrk)  which is linear in tensor dimensions and is
computationally much cheaper than enforcing Θk ∈ P nk that costs O(n3
k).
Employing the proposed ﬁxed-rank parameterization (6)  we propose a scalable tensor completion
dual formulation.
Fixed-rank dual formulation. The ﬁrst formulation is obtained by employing the parameterization
(6) directly in (5). We subsequently solve the resulting problem as a minimization problem as follows:

Θk = UkU(cid:62)
k  

rk and S n

where u = (U1  . . .   UK) and g : S n1

r1

min
u∈S n1
r1 ×...×S nK
× . . . × S nK

rK

rK

g(u) := maxZ∈C (cid:104)Z YΩ(cid:105) − 1

4

(cid:107)Z(cid:107)2

g(u) 

→ R is the function
k Zk

F −(cid:88)

(cid:13)(cid:13)(cid:13)U(cid:62)

λk
2

k

(cid:13)(cid:13)(cid:13)2

F

.

(7)

(8)

It should be noted that though (7) is a non-convex problem in u  the optimization problem in (8) is
strongly convex in Z for a given u and has unique solution.

4 Optimization algorithm

The optimization problem (7) is of the form

where (cid:96) : M → R is a smooth loss and M := S n1

× . . . × S nK

rK

min
x∈M (cid:96)(x) 

r1

4

× C is the constraint set.

(9)

In order to propose numerically efﬁcient algorithms for optimization over M  we exploit the particular
structure of the set S n
r   which is known as the spectrahedron manifold [23]. The spectrahedron mani-
fold has the structure of a compact Riemannian quotient manifold [23]. Consequently  optimization
on the spectrahedron manifold is handled in the Riemannian optimization framework. This allows to
exploit the rotational invariance of the constraint (cid:107)U(cid:107)F = 1 naturally. The Riemannian manifold
optimization framework embeds the constraint (cid:107)U(cid:107)F = 1 into the search space  thereby translating
the constrained optimization problem into unconstrained optimization problem over the spectrahedron
manifold. The Riemannian framework generalizes a number of classical ﬁrst- and second-order
(e.g.  the conjugate gradient and trust-region algorithms) Euclidean algorithms to manifolds and
provides concrete convergence guarantees [13  1  36  47  35]. The work [1]  in particular  shows a
systematic way of implementing trust-region (TR) algorithms on quotient manifolds. A full list of
optimization-related ingredients and their matrix characterizations for the spectrahedron manifold S n
is in the supplementary material. Overall  the constraint M is endowed a Riemannian structure.
r
We implement the Riemannian TR (second-order) algorithm for (9). To this end  we require the
notions of the Riemannian gradient (the ﬁrst-order derivative of the objective function on the mani-
fold)  the Riemannian Hessian along a search direction (the covariant derivative of the Riemannian
gradient along a tangential direction on the manifold)  and the retraction operator which ensures
that we always stay on the manifold (i.e.  maintain strict feasibility). The Riemannian gradient and
Hessian notions require computations of the standard (Euclidean) gradient ∇x(cid:96)(x) and the directional
derivative of this gradient along a given search direction v denoted by D∇x(cid:96)(x)[v]. The expressions
of both for (7) are given in Lemma 1.
Lemma 1 Let ˆZ be the optimal solution of the convex problem (8) at u ∈ S n1
. Let
∇ug denote the gradient of g(u) at u  D∇ug[v] denote the directional derivative of the gradient ∇ug
along v ∈ Rn1×r1 × . . .× RnK×rK   and ˙Zk be the directional derivative of Zk along v at ˆZk. Then 
∇ug = (−λ1 ˆZ1 ˆZ(cid:62)
KUK)  and D∇ug[v] = (−λ1A1  . . .  −λKAK)  where
Ak = ˆZk ˆZ(cid:62)
A key requirement in Lemma 1 is to efﬁciently solve (8) for a given u = (U1  . . .   UK). It should be
noted that (8) has a closed-form sparse solution  which is equivalent to solving the linear system

k )Uk and symm(∆) = (∆ + ∆(cid:62))/2.

1 U1  . . .  −λK ˆZK ˆZ(cid:62)

k Vk + symm( ˙Zk ˆZ(cid:62)

× . . . × S nK

rK

r1

k λk( ˆZΩ ×k UkU(cid:62)

k )Ω = YΩ.

(10)
Solving the linear system (10) in a single step is computationally expensive (it involves the use of
Kronecker products  vectorization of a sparse tensor  and a matrix inversion). Instead  we use an
iterative solver that exploits the sparsity in the variable Z and the factorization form UkU(cid:62)
k efﬁciently.
Similarly  given ˆZ and v 

˙Z can be computed by solving

ˆZΩ +(cid:80)

λk( ˆZΩ ×k (VkU(cid:62)

k + UkV(cid:62)

k ))Ω.

(11)

(cid:88)

˙ZΩ +

λk( ˙ZΩ ×k UkU(cid:62)

k )Ω = −(cid:88)

k

k

The Riemannian TR algorithm solves a Riemannian trust-region sub-problem in every iteration [1 
Chapter 7]. The TR sub-problem is a second-order approximation of the objective function in a
neighborhood  solution to which does not require inverting the full Hessian of the objective function.
It makes use of the gradient ∇x(cid:96) and its directional derivative along a search direction. The TR
sub-problem is approximately solved with an iterative solver  e.g.  the truncated conjugate gradient
algorithm. The TR sub-problem outputs a potential update candidate for x  which is then accepted or
rejected based on the amount of decrease in the function (cid:96). Algorithm 1 summarizes the key steps of
the TR algorithm for solving (9).
Computational complexity: the per-iteration computational complexity of Algorithm 1 scales
linearly with the number of known entries YΩ  denoted by |Ω|. In particular  the per-iteration
computational cost depends on the following ingredients.

• U(cid:62)

k Zk: it involves computation of nk × rk matrix Ukwith mode-k unfolding of a sparse
Z with |Ω| non-zero entries. This costs O(|Ω|rk). It should be noted that although the
i=1 i(cid:54)=k ni  only a maximum of |Ω| columns have non-zero

dimension of Zk is nk ×(cid:81)K
computing the left hand side of (10) for a given candidate Z. This costs O(|Ω|(cid:80)

entries. We exploit this property of Zk and have a compact memory storage of U(cid:62)

• Computing the solution ˆZ of the linear system (10): an iterative solver for (10) requires

k Zk.

k rk).

5

Table 1: Summary of the baseline low-rank tensor completion algorithms.

Trace norm regularized algorithms

Other algorithms

FFW

Hard

Scaled latent trace norm + Frank Wolfe
optimization + basis size reduction
Scaled overlapped trace norm + proxi-
mal gradient

HaLRTC Scaled

overlapped

trace

norm

Latent

+ ADMM
Latent trace norm + ADMM

Topt

Fixed multilinear rank + conjugate gradi-
ents (CG)

BayesCP Bayesian CP algorithm with rank tuning
geomCG Riemannian CG + ﬁxed multilinear rank
Rprecon Riemannian CG with preconditioning

T-svd

+ ﬁxed multilinear rank
Tensor tubal-rank + ADMM

objective function in (8). This costs O(|Ω|(cid:80)

• Computation of g(u): it relies on the solution of (10) and then explicitly computing the

k rk + K|Ω|).
• ∇ug(u): it requires the computation of terms like ˆZk( ˆZ(cid:62)
k rk).
• Computing the solution ˙Zk of the linear system (11): similar to (10)  (11) is solved with
an iterative solver. The computational cost of solving (11) requires computing terms like
˙Zk  which costs O(|Ω|rk). It should be noted that both ˙Z and ˆZ share the
k Zk and U(cid:62)
U(cid:62)
same sparsity pattern.

k Uk)  which cost O(|Ω|(cid:80)

• D∇ug(u)[v]: it costs O(|Ω|(cid:80)

k

k rk).

• Retraction on S nk

rk : it projects a matrix of size nk × rk on to the set S nk

rk   which costs

k r3

k + r3

k).

• S nk

Overall 
k nkr2

O(nkrk).
rk manifold-related ingredients cost O(nkr2

(cid:80)
The memory cost for our algorithm is O(|Ω| +(cid:80)

the per-iteration computational complexities of our algorithm is O(m(|Ω|(cid:80)
k +(cid:80)

k rk +
k))  where m is the number of iterations needed to solve (10) and (11) approximately.
k nkrk). We observe that both computational and
memory cost scales linearly with the number of observed entries (|Ω|)  which makes our algorithms
scalable to large datasets.
Convergence. The Riemannian TR algorithms come with rigorous convergence guarantees. [1]
discuss the rate of convergence analysis of manifold algorithms  which directly apply in our case. For
trust regions  the global convergence to a ﬁrst-order critical point is discussed in [1  Section 7.4.1]
and the local convergence to local minima is discussed in [1  Section 7.4.2]. From an implementation
perspective  we follow the existing approaches [26  24  17] and bound the number of TR iterations.
Numerical implementation. Our algorithm is implemented using the Manopt toolbox [7] in Matlab 
which has off-the-shelf generic TR implementation.

5 Experiments

We evaluate the generalization performance and efﬁciency of our proposed TR algorithm against
state-of-the-art algorithms in several tensor completion applications.
Trace norm regularized algorithms. Scaled latent trace norm regularized algorithms such as FFW
[17] and Latent [42]  and overlapped trace norm based algorithms such as HaLRTC [27] and Hard
[37] are the closest to our approach. FFW is a recently proposed state-of-the-art large scale tensor
completion algorithm. Table 1 summarizes the trace norm regularized baseline algorithms.
We denote our algorithm as TR-MM (Trust-Region algorithm for MiniMax tensor completion
formulation). We set λk = λnk ∀k in (7). Hence  we tune only one hyper-parameter λ  from the set
{10−3  10−2  . . .   103}  via ﬁve-fold cross-validation of the training data.
Video and image completion
We work with the following datasets for predicting missing values in multi-media data: a) Ribeira is
a hyperspectral image [16] of size 1017× 1340× 33  where each slice represents the image measured
at a particular wavelength. We re-size it to 203 × 268 × 33 [37  26  24]; b) Tomato is a video
sequence dataset [27  8] of size 242 × 320 × 167; and c) Baboon is an RGB image [49]  modeled as

6

Table 2: Generalization performance across several applications: hyperspectral-image/video/image
completion  movie recommendation  and link prediction. Our algorithm  TR-MM  performs signiﬁ-
cantly better than other trace norm based algorithms and obtain the best overall performance. The
symbol ‘−’ denotes the dataset is too large for the algorithm to generate result.

TR-MM FFW Rprecon geomCG Hard Topt HaLRTC Latent T-svd BayesCP

RMSE reported
Ribeira
Tomato
Baboon
ML10M
AUC reported
YouTube (subset)
YouTube (full)
FB15k-237

0.067
0.041
0.121
0.840

0.957
0.932
0.823

0.088
0.045
0.133
0.895

0.954
0.929
0.764

0.083
0.052
0.128
0.831

0.941
0.926
0.821

0.156
0.052
0.128
0.844

0.941
0.926
0.785

0.114 0.127
0.060 0.102
0.126 0.130
−
−

0.954 0.941
−
−
−
−

0.095
0.202
0.247
−

0.783
−
−

0.087 0.064
0.042
0.046
0.459
0.146
−
−

0.945
−
−

0.941
−
−

0.154
0.103
0.159
−

0.950
−
−

(a) Ribeira

(b) FB15k-237

(c) YouTube (full)

(d) FB15k-237

Figure 2: (a) Evolution of test RMSE on Ribeira; (b) & (c) Evolution of test AUC on FB15k-237 and
YouTube (full)  respectively. Our algorithm  TR-MM  obtains the best generalization performance
in all the three datasets. In addition  TR-MM converges to a good solution is fairly quick time; (d)
Variation of test AUC as the amount of training data changes on FB15k-237. TR-MM performs
signiﬁcantly better than baselines when the amount of training data is less.

a 256 × 256 × 3 tensor. Following [24]  we train on a random sample of 10% of the entries and test
on another 10% of the entries for all the three datasets. Each experiment is repeated ten times.
Results. Table 2 reports the root mean squared error (RMSE) on the test set  averaged over ten splits.
Our algorithm  TR-MM  obtains the best results  outperforming other trace norm based algorithms on
all the three datasets. Figure 2(a) shows the trade-off between the test RMSE and the training time of
all the algorithms on Ribeira. It can be observed that TR-MM converges to the lowest RMSE at a
signiﬁcantly faster rate compared to the other baselines. It is evident from the results that learning
a mixture of non-sparse tensors  as learned by the proposed algorithm  helps in achieving better
generalization performance compared to the algorithms that learn a sparse mixture of tensors.
Link prediction
The aim in link prediction setting is to predict missing or new links in knowledge graphs  social
networks  etc. We consider FB15k-237 and YouTube datasets  discussed below.
FB15k-237: this is a subset of FB15k dataset [6  44]  containing facts of the form subject-predicate-
object (RDF) triples from Freebase knowledge graph. FB15k-237 contains 14 541 entities and 237
relationships. The task is to predict the relationships (from a given set of relations) between a pair of
entities in the knowledge graph. It has 310 116 observed relationships (links) between pairs of entities 
which are the positive samples. In addition  516 606 negative samples are generated following the
procedure described in [6]. We model this task as a 14 541× 14 541× 237 tensor completion problem.
Y(a b c) = 1 implies that relationshipb exists between entitya and entityc  and Y(a b c) = 0 implies
otherwise. We keep 80% of the observed entries for training and the remaining 20% for testing.
YouTube: this is a link prediction dataset [40] having 5 types of interactions between 15 088 users.
The task is to predict the interaction (from a given set of interactions) between a pair of users. We
model it as a 15 088 × 15 088 × 5 tensor completion problem. All the entries are known in this case.
We randomly sample 0.8% of the data for training [17] and another 0.8% for testing.

7

10-2100102Time (in seconds)0.10.150.20.250.30.35Test RMSETR-MMFFWRprecongeomCGHardToptHaLRTCLatentT-svd100102Time (in seconds)0.40.50.60.70.8Test AUCTR-MMFFWRprecongeomCG100101102103Time (in seconds)0.50.60.70.80.9Test AUCTR-MMFFWRprecongeomCG20406080 Percentage of full data as training set0.60.650.70.750.80.85Test AUCTR-MMFFWRprecongeomCGTable 3: Rank sets at which the proposed TR-MM algorithm and the Tucker decomposition based
tensor completion algorithms (Rprecon  geomGC  Topt) achieve best results across datasets. It should
be noted that the notion of rank in trace norm regularized approaches (such as TR-MM) differs from
the Tucker rank.

Ribeira
Tomato
Baboon
ML10M
YouTube (subset)
YouTube (full)
FB15k-237

TR-MM rank Tucker rank
(5  5  5)
(10  10  10)
(4  4  3)
(20  10  1)
(3  3  1)
(3  3  1)
(20  20  1)

(15  15  6)
(15  15  15)
(4  4  3)
(4  4  4)
(5  5  5)
(5  5  5)
(5  5  5)

It should be noted that Hard  HaLRTC  and Latent do not scale to the full FB15k-237 and YouTube
datasets as they need to store full tensor in memory. Hence  we follow [17] to create a subset of the
YouTube dataset of size 1509 × 1509 × 5 in which 1509 users with most number of links are chosen.
We randomly sample 5% of the data for training and another 5% for testing.
Each experiment is repeated on ten random train-test splits. Following [29  17]  the generalization
performance for link prediction task is measured by computing the area under the ROC curve on the
test set (test AUC) for each algorithm.
Results. Table 2 reports the average test AUC on YouTube (subset)  Youtube (full) and FB15k-237
datasets. The TR-MM algorithm achieves the best performance in all the link prediction tasks.
This shows that the non-sparse mixture of tensors learned by TR-MM helps in achieving better
performance. Figures 2(b) & 2(c) plots the trade-off between the test AUC and the training time for
FB15k-237 and YouTube  respectively. We observe that TR-MM is the fastest to converge to a good
AUC and take only a few iterations.
We also conduct experiments to evaluate the performance of different algorithms in challenging
scenarios when the amount of training data available is less. On the FB15k-237 dataset  we vary the
size of training data from 20% to 80% of the observed entries  and the remaining 20% of the observed
entries is kept as the test set. Figure 2(d) plots the results of this experiment. We can observe that
TR-MM does signiﬁcantly better than the baselines in data scarce regimes.
Movie recommendation
We evaluate the algorithms on the MovieLens10M (ML10M) dataset [18]. This is a movie recom-
mendation task — predict the ratings given to movies by various users. MovieLens10M contains
10 000 054 ratings of 10 681 movies given by 71 567 users. Following [24]  we split the time into
7-days wide bins  forming a tensor of size 71 567 × 10 681 × 731. For our experiments  we generate
ten random train-test splits  where 80% of the observed entries is kept for training and the rest 20%
for testing.
Results. Table 2 reports the average test RMSE on this task. It can be observed that our algorithm 
TR-MM  outperforms state-of-the-art scaled latent trace norm based algorithm FFW.
Results compared to other baseline algorithms
In addition to the trace norm based algorithms  we also compare against algorithms that model tensor
via Tucker decomposition with ﬁxed multilinear ranks: Rprecon [24]  geomCG [26]  and Topt [15].
Large scale state-of-the-art algorithms in this multilinear framework include Rprecon and geomCG.
We also compare against tensor tubal-rank based algorithm T-svd [48] and CP decomposition based
algorithm BayesCP [49]. Table 1 summarizes these baselines.
As can be observed from Table 2  TR-MM obtains better overall generalization performance than
the above discussed baselines. In the movie recommendation problem  Rprecon achieves better
results than TR-MM. It should be noted that Topt  T-svd  and BayesCP are not scalable to large scale
datasets.
Rank of solutions of TR-MM algorithm
Table 3 shows the rank sets at which the proposed TR-MM and Tucker decomposition based tensor
completion algorithms (Rprecon  geomGC  Topt) achieve best results across datasets. The latent

8

Table 4: Results on outlier robustness experiments. Our algorithm  TR-MM  is more robust to outliers
than the competing baselines. The symbol ‘−’ denotes the dataset is too large for the algorithm to
generate result.

x

Ribeira (RMSE)

0.05
0.10
FB15k-237 (AUC) 0.05
0.10

TR-MM FFW Rprecon geomCG Hard Topt HaLRTC Latent T-svd BayesCP
0.081
0.111

0.157
0.172

0.095
0.112

0.258
0.373

0.142 0.169
0.158 0.188
−
−
−
−

0.121
0.135
−
−

0.103 0.146
0.120 0.182
−
−
−
−

0.201
0.204
−
−

0.803
0.772

0.734
0.711

0.794
0.765

0.764
0.739

trace norm based algorithms (TR-MM  FFW  Latent) model the tensor completion by approximating
the input tensor as a combination of tensors. Each tensor in this combination is constrained to be
low-ranked along a given mode. In contrast  Tucker-decomposition based algorithms model the
tensor completion problem as a factorization problem with the given Tucker rank (also known as the
multilinear rank). Due to this fundamental difference in modeling  the concept of rank in TR-MM
algorithm is different from the multilinear rank of Tucker decomposition based algorithms.
Results on outlier robustness
We also evaluate TR-MM and the baselines considered in Table 1 for outlier robustness on hyper-
spectal image completion and link prediction problems. In the Ribeira dataset  we add the standard
Gaussian noise (N (0  1)) to randomly selected x fraction of the entries in the training set. The
minimum and the maximum value of entries in the (original) Ribeira are 0.01 and 2.09  respectively.
In FB15k-237 dataset  we ﬂip randomly selected x fraction of the entries in the training set  i.e.  the
link is removed if present and vice-versa. We experiment with x = 0.05 and x = 0.10.
The results are reported in Table 4. We observe that our algorithm  TR-MM  obtains the best
generalization performance and  hence  is the most robust to outliers. We also observe that trace
norm regularized algorithms are relatively more robust to outliers than Tucker-decomposition  CP-
decomposition  and tensor tubal-rank based algorithms.

6 Discussion and conclusion

In this paper  we introduce a novel regularizer for low-rank tensor completion problem which learns
the tensor as a non-sparse combination of K tensors  where K is the number of modes. Existing
works [41  42  45  17] learn a sparse combination of tensors  essentially learning the tensor as a
low-rank matrix and losing higher order information in the available data. Hence  we recommend
learning a non-sparse combination of tensors in trace norm regularized setting  especially since K
is typically a small integer in most real-world applications. In our experiments  we observe better
generalization performance with the proposed regularization. Theoretically  we provide the following
result on the reconstruction error in the context of recovering an unknown tensor W∗ from noisy
observation (a similar result on the latent trace norm is presented in [42]).
Lemma 2 Let W∗ be the true tensor to be recovered from observed Y  which is obtained as
Y = W∗ + E  where E ∈ Rn1×...×nK is the noise tensor. Assume that the regularization constant λ

satisﬁes λ ≤ 1/((cid:80)K

k=1 (cid:107)Ek(cid:107)2∞)1/2 then the estimator
(cid:107)Y − W(cid:107)2

ˆW = argmin
1
(
2
satisﬁes the inequality (cid:107) ˆW − W∗(cid:107)F ≤ 2
right hand side also approaches zero.

W

λ

(cid:113)min

k

(cid:88)

k

F +

1
λ

(cid:107)W (k)

k (cid:107)2∗) 

nk. When noise approaches zero  i.e.  E → 0  the

We present a dual framework to analyze the proposed tensor completion formulation. This leads
to a novel ﬁxed-rank formulation  for which we exploit the Riemannian framework to develop
scalable trust region algorithm. In experiments  our algorithm TR-MM obtains better generalization
performance and is more robust to outliers than state-of-the-art low-rank tensor completion algorithms.
In future  optimization algorithms for the proposed formulation can be developed for online or
distributed frameworks. Recent works [4  5  30  31] have explored optimization over Riemannian
manifolds in such learning settings.

9

Acknowledgement

Most of this work was done when MN (as an intern)  PJ  and BM were at Amazon.

References
[1] P.-A. Absil  R. Mahony  and R. Sepulchre. Optimization Algorithms on Matrix Manifolds.

Princeton University Press  2008.

[2] E. Acar  D. M. Dunlavy  T. G. Kolda  and M. Mørup. Scalable tensor factorizations for

incomplete data. Chemometrics and Intelligent Laboratory Systems  106(1):41–56  2011.

[3] A. Argyriou  T. Evgeniou  and M. Pontil. Multi-task feature learning. In NIPS  2006.

[4] L. Balzano  R. Nowak  and B. Recht. Online identiﬁcation and tracking of subspaces from
highly incomplete information. In the 48th Annual Allerton Conference on Communication 
Control  and Computing  2010.

[5] S. Bonnabel. Stochastic gradient descent on Riemannian manifolds. IEEE Transactions on

Automatic Control  58(9):2217–2229  2013.

[6] A. Bordes  N. Usunier  A. Garcia-Duran  J. Weston  and O. Yakhnenko. Translating embeddings

for modeling multi-relational data. In NIPS  2013.

[7] N. Boumal  B. Mishra  P.-A. Absil  and R. Sepulchre. Manopt  a Matlab toolbox for optimization

on manifolds. Journal of Machine Learning Research  15(Apr):1455–1459  2014.

[8] C. F. Caiafa and A. Cichocki. Stable  robust  and super fast reconstruction of tensors using

multi-way projections. IEEE Transactions on Signal Processing  63(3):780–793  2014.

[9] H. Cheng  Y. Yu  X. Zhang  E. Xing  and D. Schuurmans. Scalable and sound low-rank tensor

learning. In AISTATS  2016.

[10] A. Cichocki  H.-A. Phan  Q. Zhao  N. Lee  I. Oseledets  and D. P. Mandic. Tensor networks for
dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions.
Foundations and Trends in Machine Learning  9(4–5):249–429  2017.

[11] A. Cichocki  H.-A. Phan  Q. Zhao  N. Lee  I. Oseledets  and D. P. Mandic. Tensor networks
for dimensionality reduction and large-scale optimization: Part 2 applications and future
perspectives. Foundations and Trends in Machine Learning  9(6):431–673  2017.

[12] C. Cortes  M. Mohri  and A. Rostamizadeh. L2 regularization for learning kernels. In UAI 

2009.

[13] A. Edelman  T.A. Arias  and S.T. Smith. The geometry of algorithms with orthogonality

constraints. SIAM Journal on Matrix Analysis and Applications  20(2):303–353  1998.

[14] B. Ermi¸s  E. Acar  and A. T. Cemgil. Link prediction in heterogeneous data via generalized

coupled tensor factorization. In KDD  2015.

[15] M. Filipovi´c and A. Juki´c. Tucker factorization with missing data with application to low-n-rank

tensor completion. Multidimensional Systems and Signal Processing  2015.

[16] D. H. Foster  S. M. C. Nascimento  and K. Amano. Information limits on neural identiﬁcation
of colored surfaces in natural scenes. Visual Neuroscience  21(3):331–336  2004. URL:
https://personalpages.manchester.ac.uk/staff/d.h.foster/.

[17] X. Guo  Q. Yao  and J. T. Kwok. Efﬁcient sparse low-rank tensor completion using the

frank-wolfe algorithm. In AAAI  2017. URL: https://github.com/quanmingyao/FFWTensor.

[18] F. M. Harper and J. A. Konstan.

The movielens datasets: History and context.
ACM Transactions on Interactive Intelligent Systems  5(4):19:1–19:19  2015. URL:
http://ﬁles.grouplens.org/datasets/movielens/ml-10m-README.html.

10

[19] P. Jawanpuria  M. Lapin  M. Hein  and B. Schiele. Efﬁcient output kernel learning for multiple

tasks. In NIPS  2015.

[20] P. Jawanpuria and B. Mishra. A uniﬁed framework for structured low-rank matrix learning. In

ICML  2018.

[21] P. Jawanpuria and J. S. Nath. Multi-task multiple kernel learning. In SDM  2011.

[22] P. Jawanpuria  M. Varma  and J. S. Nath. On p-norm path following in multiple kernel learning

for non-linear feature selection. In ICML  2014.

[23] M. Journée  F. Bach  P.-A. Absil  and R. Sepulchre. Low-rank optimization on the cone of

positive semideﬁnite matrices. SIAM Journal on Optimization  20(5):2327–2351  2010.

[24] H. Kasai and B. Mishra. Low-rank tensor completion: a Riemannian manifold preconditioning

approach. In ICML  2016. URL: https://bamdevmishra.in/codes/tensorcompletion/.

[25] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review  51(3):455–

500  2009.

[26] D. Kressner  M. Steinlechner  and B. Vandereycken. Low-rank tensor completion by Riemannian
optimization. BIT Numerical Mathematics  54(2):447–468  2014. URL: anchp.epﬂ.ch/geomCG.

[27] J. Liu  P. Musialski  P. Wonka  and J. Ye. Tensor completion for estimating missing values in
visual data. IEEE Transactions on Pattern Analysis and Machine Intelligence  35(1):208–220 
2013. URL: http://www.cs.rochester.edu/u/jliu/code/TensorCompletion.zip.

[28] X.-Y. Liu  S. Aeron  V. Aggarwal  and X. Wang. Low-tubal-rank tensor completion using

alternating minimization. In SPIE Conference on Defense and Security  2016.

[29] L. Lü and T. Zhou. Link prediction in complex networks: A survey. Physica A: statistical

mechanics and its applications  390(6):1150–1170  2011.

[30] M. Meghawanshi  P. Jawanpuria  A. Kunchukuttan  H. Kasai  and B. Mishra. Mctorch  a
manifold optimization library for deep learning. In the NeurIPS workshop on Machine Learning
Open Source Software  2018.

[31] B. Mishra  H. Kasai  P. Jawanpuria  and A. Saroop. A Riemannian gossip approach to subspace

learning on Grassmann manifold. Machine Learning  (to appear in) 2019.

[32] M. Nimishakavi  P. Jawanpuria  and B. Mishra. A dual framework for low-rank tensor comple-

tion. Technical report  arXiv preprint arXiv:1712.01193  2017.

[33] T. K. Pong  P. Tseng  S. Ji  and J. Ye. Trace norm regularization: Reformulations  algorithms 

and multi-task learning. SIAM Journal on Optimization  20(6):3465–3489  2010.

[34] B. Romera-paredes  H. Aung  N. Bianchi-berthouze  and M. Pontil. Multilinear multitask

learning. In ICML  2013.

[35] H. Sato and T. Iwai. A new  globally convergent Riemannian conjugate gradient method.
Optimization: A Journal of Mathematical Programming and Operations Research  64(4):1011–
1031  2013.

[36] H. Sato  H. Kasai  and B. Mishra. Riemannian stochastic variance reduced gradient. Technical

report  arXiv preprint arXiv:1702.05594  2017.

[37] M. Signoretto  Q. T. Dinh  L. D. Lathauwer  and J. A. K. Suykens. Learning with tensors:
a framework based on convex optimization and spectral regularization. Machine Learning 
94(3):303–351  2014.

[38] T. Suzuki. Unifying framework for fast learning rate of non-sparse multiple kernel learning. In

NIPS  2011.

[39] P. Symeonidis  A. Nanopoulos  and Y. Manolopoulos. Tag recommendations based on tensor

dimensionality reduction. In RecSys  2008.

11

[40] L. Tang  X. Wang  and H. Liu. Uncovering groups via heterogeneous interaction analysis. In

ICDM  2009. URL: http://leitang.net/heterogeneous_network.html.

[41] R. Tomioka  K. Hayashi  and H. Kashima. Estimation of low-rank tensors via convex optimiza-

tion. Technical report  arXiv preprint arXiv:1010.0789  2010.

[42] R. Tomioka and T. Suzuki. Convex tensor decomposition via structured schatten norm regular-

ization. In NIPS  2013. URL: http://tomioka.dk/softwares/.

[43] R. Tomioka  T. Suzuki  K. Hayashi  and H. Kashima. Statistical performance of convex tensor

decomposition. In NIPS  2011.

[44] K. Toutanova  D. Chen  P. Pantel  H. Poon  P. Choudhury  and M. Gamon. Represent-
In EMNLP  2015. URL:

ing text for joint embedding of text and knowledge bases.
http://kristinatoutanova.com/.

[45] K. Wimalawarne  M. Sugiyama  and R. Tomioka. Multitask learning meets tensor factorization:

task imputation via convex optimization. In NIPS  2014.

[46] Y. Xin and T. Jaakkola. Primal-dual methods for sparse constrained matrix completion. In

AISTATS  2012.

[47] H. Zhang  S. J. Reddi  and S. Sra. Riemannian svrg: Fast stochastic optimization on Riemannian

manifolds. In NIPS  2016.

[48] Z. Zhang  G. Ely  S. Aeron  N. Hao  and M. E. Kilmer. Novel methods for multi-
In CVPR  2014. URL:

linear data completion and de-noising based on tensor-svd.
http://www.ece.tufts.edu/ shuchin/software.html.

[49] Q. Zhao  L. Zhang  and A. Cichocki. Bayesian CP factorization of incomplete tensors with
automatic rank determination. IEEE Transactions on Pattern Analysis and Machine Intelligence 
37(9):1751–1763  2015. URL: https://github.com/qbzhao/BCPF.

12

,Madhav Nimishakavi
Pratik Kumar Jawanpuria
Bamdev Mishra