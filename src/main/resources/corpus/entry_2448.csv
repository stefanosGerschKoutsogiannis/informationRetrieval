2012,Hierarchical Optimistic Region Selection driven by Curiosity,This paper aims to take a step forwards making the term ``intrinsic motivation'' from reinforcement learning theoretically well founded  focusing on curiosity-driven learning. To that end  we consider the setting where  a fixed partition P of a continuous space X being given  and a process \nu defined on X being unknown  we are asked to sequentially decide which cell of the partition to select as well as where to sample \nu in that cell  in order to minimize a loss function that is inspired from previous work on curiosity-driven learning. The loss on each cell consists of one term measuring a simple worst case quadratic sampling error  and a penalty term proportional to the range of the variance in that cell. The corresponding problem formulation extends the setting known as active learning for multi-armed bandits to the case when each arm is a continuous region  and we show how an adaptation of recent algorithms for that problem and of hierarchical optimistic sampling algorithms for optimization can be used in order to solve this problem. The resulting procedure  called Hierarchical Optimistic Region SElection driven by Curiosity (HORSE.C) is provided together with a finite-time regret analysis.,Hierarchical Optimistic Region Selection driven by

Curiosity

Odalric-Ambrym Maillard

Lehrstuhl f¨ur Informationstechnologie

Montanuniversit¨at Leoben
Leoben  A-8700  Austria

odalricambrym.maillard@gmail.com

Abstract

This paper aims to take a step forwards making the term “intrinsic motivation”
from reinforcement learning theoretically well founded  focusing on curiosity-
driven learning. To that end  we consider the setting where  a ﬁxed partition P of
a continuous space X being given  and a process ν deﬁned on X being unknown 
we are asked to sequentially decide which cell of the partition to select as well as
where to sample ν in that cell  in order to minimize a loss function that is inspired
from previous work on curiosity-driven learning. The loss on each cell consists of
one term measuring a simple worst case quadratic sampling error  and a penalty
term proportional to the range of the variance in that cell. The corresponding
problem formulation extends the setting known as active learning for multi-armed
bandits to the case when each arm is a continuous region  and we show how an
adaptation of recent algorithms for that problem and of hierarchical optimistic
sampling algorithms for optimization can be used in order to solve this problem.
The resulting procedure  called Hierarchical Optimistic Region SElection driven
by Curiosity (HORSE.C) is provided together with a ﬁnite-time regret analysis.

1

Introduction

In this paper  we focus on the setting of intrinsically motivated reinforcement learning (see Oudeyer
and Kaplan [2007]  Baranes and Oudeyer [2009]  Schmidhuber [2010]  Graziano et al. [2011]) 
which is an important emergent topic that proposes new difﬁcult and interesting challenges for the
theorist. Indeed  if some formal objective criteria have been proposed to implement speciﬁc notions
of intrinsic rewards (see Jung et al. [2011]  Martius et al. [2007])  so far  many - and only - experi-
mental work has been carried out for this problem  often with interesting output (see Graziano et al.
[2011]  Mugan [2010]  Konidaris [2011]) but unfortunately no performance guarantee validating a
proposed approach. Thus proposing such an analysis may have great immediate consequences for
validating some experimental studies.
Motivation. A typical example is the work of Baranes and Oudeyer [2009] about curiosity-driven
learning (and later on Graziano et al. [2011]  Mugan [2010]  Konidaris [2011])  where a precise
algorithm is deﬁned together with an experimental study  yet no formal goal is deﬁned  and no
analysis is performed as well. They consider a so-called sensory-motor space X def= S×M ⊂ [0  1]d
where S is a (continuous) state space and M is a (continuous) action space. There is no reward  yet
one can consider that the goal is to actively select and sample subregions of X for which a notion of
“learning progress” - this intuitively measures the decay of some notion of error when successively
sampling into one subregion - is maximal. Two key components are advocated in Baranes and
Oudeyer [2009]  in order to achieve successful results (despite that success is a fuzzy notion):

• The use of a hierarchy of regions  where each region is progressively split into sub-regions.

1

• Splitting leaf-regions in two based on the optimization of the dissimilarity  amongst the
regions  of the learning progress. The idea is to identify regions with a learning complex-
ity that is a globally constant in that region  which also provides better justiﬁcation for
allocating samples between identiﬁed regions.

We believe it is possible to go one step towards a full performance analysis of such algorithms  by
relating the corresponding active sampling problem to existing frameworks.
Contribution. This paper aims to take a step forwards making the term “intrinsic motivation” from
reinforcement learning theoretically well founded  focusing on curiosity-driven learning. We in-
troduce a mathematical framework in which a metric space (which intuitively plays the role of the
state-action space) is divided into regions and a learner has to sample from an unknown random func-
tion in a way that reduces a notion of error measure the most. This error consists of two terms  the
ﬁrst one is a robust measure of the quadratic error between the observed samples and their unknown
mean  the second one penalizes regions with non constant learning complexity  thus enforcing the
notion of curiosity. The paper focuses on how to choose the region to sample from  when a partition
of the space is provided.
The resulting problem formulation can be seen as a non trivial extension of the setting of active
learning in multi-armed bandits (see Carpentier et al. [2011] or Antos et al. [2010])  where the main
idea is to estimate the variance of each arm and sample proportionally to it  to the case when each
arm is a region as opposed to a point. In order to deal with this difﬁculty  the maximal and minimal
variance inside each region is tracked by means of a hierarchical optimization procedure  in the spirit
of the HOO algorithm from Bubeck et al. [2011]. This leads to a new procedure called Hierarchical
Optimistic Region SElection driven by Curiosity (HORSE.C) for which we provide a theoretical
performance analysis.
Outline. The outline of the paper is the following. In Section 2 we introduce the precise setting and
deﬁne the objective function. Section 3 deﬁnes our assumptions. Then in Section 4 we present the
HORSE.C algorithm. Finally in Section 5  we provide the main Theorem 1 that gives performance
guarantees for the proposed algorithm.

2 Setting: Robust region allocation with curiosity-inducing penalty.
Let X assumed to be a metric space and let Y ⊂ Rd be a normed space  equipped with the Euclidean
norm || · ||. We consider an unknown Y-valued process deﬁned on X   written ν : X → M+
1 (Y) 
where M+
1 (Y) refers to the set of all probability measures on Y  such that for all x ∈ X   the random
variable Y ∼ ν(x) has mean µ(x) ∈ Rd and covariance matrix Σ(x) ∈ Md d(R) assumed to be
diagonal. We thus introduce for convenience the notation ρ(x) def= trace(Σ(x))  where trace is
the trace operator (this corresponds to the variance in dimension 1). We call X the input space or
sampling space  and Y the output space or value space.
Intuition Intuitively when applied to the setting of Baranes and Oudeyer [2009]  then X def= S × A
is the space of state-action pairs  where S is a continuous state space and A a continuous action
space  ν is the transition kernel of an unknown MDP  and ﬁnally Y def= S. This is the reason why
we consider Y ⊂ Rd and not only Y ⊂ R as would seem more natural. One difference is that
we assume (see Section 3) that we can sample anywhere in X   which is a restrictive yet common
assumption in the reinforcement learning literature. How to get rid of this assumption is an open
and challenging question that is left for future work.
Sampling error and robustness Let us consider a sequential sampling process on X   i.e. a process
that samples at time t a value Yt ∼ ν(Xt) at point Xt  where Xt ∈ F<t is a measurable function of
the past inputs and outputs {(Xs  Ys)}s<t. It is natural to look at the following quantity  that we call
average noise vector ηt:

One interesting property is that this is a martingale difference sequence  which means that the norm
of this vector enjoys a concentration property. More precisely (see [Maillard  2012  Lemma 1] in
the extended version of the paper)  we have for all deterministic t > 0

def=

ηt

1
t

t�s=1

Ys − µ(Xs) ∈ Rd .

E[||ηt||2 ] =

1
t

E� 1

t

t�s=1

ρ(Xs)� .

2

A similar property holds for a region R ⊂ X that has been sampled nt(R) times  and in order to be
robust against a bad sampling strategy inside a region  it is natural to look at the worst case error 
that we deﬁne as

eR(nt) def=

supx∈R ρ(x)

.

nt(R)

One reason for looking at robustness is that for instance  in the case we work with an MDP  we are
generally not completely free to choose the sample Xs ∈ S ×A: we can only choose the action and
the next state is generally given by Nature. Thus  it is important to be able to estimate this worst
case error so as to prevent from bad situations.
Goal Now let P be a ﬁxed  known partition of the space X and consider the following game. The
goal of an algorithm is  at each time step t  to propose one point xt where to sample the space
X   so that its allocation of samples {nt(R)}R∈P (that is  the number of points sampled in each
region) minimizes some objective function. Thus  the algorithm is free to sample everywhere in
each region  with the goal that the total number of points chosen in each region is optimal in some
sense. A simple candidate for this objective function would be the following

LP (nt) def= max�eR(nt) ; R ∈ P�  

however  in order to incorporate a notion of curiosity  we would also like to penalize regions that
have a variance term ρ that is non homogeneous (i.e. the less homogeneous  the more samples we
allocate). Indeed  if a region has constant variance  then we do not really need to understand more
its internal structure  and thus its better to focus on an other region that has very heterogeneous
variance. For instance  one would like to split such a region in several homogeneous parts  which
is essentially the idea behind section C.3 of Baranes and Oudeyer [2009]. We thus add a curiosity-
penalization term to the previous objective function  which leads us to deﬁne the pseudo-loss of an
allocation nt

def= {nt(R)}R∈P in the following way:
LP (nt) def= max� eR(nt) + λ|R|(max

x∈R

(1)
Indeed  this means that we do not want to focus just on regions with high variance  but also trade-off
with highly heterogeneous regions  which is coherent with the notion of curiosity (see Oudeyer and
Kaplan [2007]). For convenience  we also deﬁne the pseudo-loss of a region R by

ρ(x) − min
x∈R

ρ(x)) ; R ∈ P � .

LR(nt) def= eR(nt) + λ|R|(max
x∈R

ρ(x) − min
x∈R

ρ(x)) .

Regret The regret (or loss) of an allocation algorithm at time T is deﬁned as the difference between
the cumulated pseudo-loss of the allocations nt = {nR t}R∈P proposed by the algorithm and that
of the best allocation strategy n�

R t}R∈P at each time steps; we deﬁne
t = {n�
def=
RT

LP (nt) − LP (n�
t )  

T�t=|P|

where an optimal allocation at time t is deﬁned by

n�

t ∈ argmin� LP (nt) ; {nt(R)}R∈P is such that �R∈P

nt(R) = t� .

Note that the sum starts at t = |P| for a technical reason  since for t < |P|  whatever the allocation 
there is always at least one region with no sample  and thus LP (nt) = ∞.
Example 1 In the special case when X = {1  . . .   K} is ﬁnite with K � T   and when P is the
complete partition (each cell corresponds to exactly one point)  the penalization term is canceled.
Thus the problem reduces to the choice of the quantities nt(i) for each arm i  and the loss of an
allocation simply becomes

L(nt) def= max� ρ(i)

nt(i)

; 1 ≤ i ≤ K� .

This almost corresponds to the already challenging setting analyzed for instance in Carpentier et al.
[2011] or Antos et al. [2010]. The difference is that we are interested in the cumulative regret of
our allocation instead of only the regret suffered for the last round as considered in Carpentier et al.
[2011] or Antos et al. [2010]. Also we directly target ρ(i)
nt(i) whereas they consider the mean sampling
error (but both terms are actually of the same order). Thus the setting we consider can be seen as
a generalization of these works to the case when each arm corresponds to a continuous sampling
domain.

3

3 Assumptions

In this section  we introduce some mild assumptions. We essentially assume that the unknown
distribution is such that it has a sub-Gaussian noise  and a smooth mean and variance functions.
These are actually very mild assumptions. Concerning the algorithm  we consider it can use a
partition tree of the space  and that this one is essentially not degenerated (a typical binary tree that
satisﬁes all the following assumptions is such that each cell is split in two children of equal volume).
Such assumptions on trees have been extensively discussed for instance in Bubeck et al. [2011].
Sampling At any time  we assume that we are able to sample at any point in X   i.e. we assume we
have a generative model1 of the unknown distribution ν.
Unknown distribution We assume that ν is sub-Gaussian  meaning that for all ﬁxed x ∈ X

∀λ ∈ Rd ln E exp[�λ  Y − µ(X)�] ≤

λT Σ(x)λ

 

2

and has diagonal covariance matrix in each point2.
The function µ is assumed to be Lipschitz w.r.t a metric �1  i.e. it satisﬁes

Similarly  the function ρ is assumed to be Lipschitz w.r.t a metric �2  i.e. it satisﬁes

∀x  x� ∈ X ||µ(x) − µ(x�)|| ≤ �1(x  x�) .

∀x  x� ∈ X |ρ(x) − ρ(x�)| ≤ �2(x  x�) .

R(h  i) = R(h + 1  2i) ∪ R(h + 1  2i + 1) .

Hierarchy We assume that Y is a convex and compact subset of [0  1]d. We consider an inﬁnite
binary tree T whose nodes correspond to regions of X . A node is indexed by a pair (h  i)  where
h ≥ 0 is the depth of the nodes in T and 0 ≤ i < 2h is the position of the node at depth h. We write
R(h  i) ⊂ X the region associated with node (h  i). The regions are ﬁxed in advance  are all assumed
to be measurable with positive measure  and must satisfy that for each h ≥ 1  {R(h  i)}0≤i<2h is a
partition of X that is compatible with depth h − 1  where R(0  0) def= X ; in particular for all h ≥ 0 
for all 0 ≤ i < 2h  then
In dimension d  a standard way to deﬁne such a tree is to split each parent node in half along the
largest side of the corresponding hyper-rectangle  see Bubeck et al. [2011] for details.
For a ﬁnite sub-tree Tt of T   we write Leaf (Tt) for the set of all leaves of Tt. For a region (h  i) ∈
Tt  we denote by Ct(h  i) the set of its children in Tt  and by Tt(h  i) the subtree of Tt starting with
root node (h  i).
Algorithm and partition The partition P is assumed to be such that each of its regions R corre-
sponds to one region R(h  i) ∈ T ; equivalently  there exists a ﬁnite sub-tree T0 ⊂ T such that
Leaf (T0) = P. An algorithm is only allowed to expand one node of Tt at each time step t. In the
sequel  we write indifferently P ∈ T and (h  i) ∈ T or P and R(h  i) ⊂ X to refer to the partition
or one of its cell.

Exponential decays Finally  we assume that the �1 and �2 diameters of the region R(h  i) as well as
its volume |R(h  i)| decay at exponential rate in the sense that there exists positive constants γ  γ1 
γ2 and c  c1  c2 such that for all h ≥ 0  then |R(h  i)| ≤ cγh 
1 and max
x� x∈R(h i)

�2(x  x�) ≤ c2γh
2 .

�1(x  x�) ≤ c1γh

Similarly  we assume that there exists positive constants c� ≤ c  c�1 ≤ c1 and c�2 ≤ c2 such that for
all h ≥ 0  then |R(h  i)| ≥ c�γh 

max

x� x∈R(h i)

max

x� x∈R(h i)

�1(x  x�) ≥ c�1γh

1 and max

x� x∈R(h i)

�2(x  x�) ≥ c�2γh
2 .

This assumption is made to avoid degenerate trees and for general purpose only. It actually holds
for any reasonable binary tree.

1using the standard terminology in Reinforcement Learning.
2this assumption is only here to make calculations easier and avoid nasty technical considerations that

anyway do not affect the order of the ﬁnal regret bound but only concern second order terms.

4

4 Allocation algorithm

In this section  we now introduce the main algorithm of this paper in order to solve the problem
considered in Section 2. It is called Hierarchical Optimistic Region SElection driven by Curiosity.
Before proceeding  we need to deﬁne some quantities.

4.1 High-probability upper-bound and lower-bound estimations

Let us consider the following (biased) estimator

1

t�s=1

t (R) def=
ˆσ2

1

Nt(R)

t�s=1

||Ys||2I{Xs ∈ R} − ||

YsI{Xs ∈ R}||2 .

Nt(R)
Apart from a small multiplicative biased by a factor Nt(R)−1
  it has more importantly a positive bias
Nt(R)
due to the fact that the random variables do not share the same mean; this phenomenon is the same
as the estimation of the average variance for independent but non i.i.d variables with different means
j=1 µj]2 (see Lemma 5). In our case 
{µi}i≤n  where the bias would be given by 1
it is thus always non negative  and under the assumption that µ is Lipschitz w.r.t the metric �1  it is
fortunately bounded by d1(R)2  the diameter of R w.r.t the metric �1.
We then introduce the two following key quantities  deﬁned for all x ∈ R and δ ∈ [0  1] by
Ut(R  x  δ) def= ˆσ2

n�n
i=1[µi − 1
t (R) + (1 + 2√d)� d ln(2d/δ)
t (R) − (1 + 2√d)� d ln(2d/δ)

2Nt(R) − d1(R)2 −

�2(Xs  x)I{Xs ∈ R} .

Lt(R  x  δ) def= ˆσ2

n�n

�2(Xs  x)I{Xs ∈ R} 

2Nt(R)

Nt(R)

d ln(2d/δ)

1

t�s=1

+

2Nt(R)

+

1

Nt(R)

t�s=1

Note that we would have preferred to replace the terms involving ln(2d/δ) with a term depending
on the empirical variance  in the spirit of Carpentier et al. [2011] or Antos et al. [2010]. However 
contrary to the estimation of the mean  extending the standard results valid for i.i.d data to the case
of a martingale difference sequence is non trivial for the estimation of the variance  especially due
to the additive bias resulting from the fact that the variables may not share the same mean  but also
to the absence of such results for U-statistics (up to the author’s knowledge). For that reason such
an extension is left for future work.
The following results (we provide the proof in [Maillard  2012  Appendix A.3]) show that
Ut(R  x  δ) is a high probability upper bound on ρ(x) while Lt(R  x  δ) is a high probability lower
bound on ρ(x).
Proposition 1 Under the assumptions that Y is a convex subset of [0  1]d  ν is sub-Gaussian  ρ is
Lipschitz w.r.t. �2 and R ⊂ X is compact and convex  then

Similarly  under the same assumptions  then

P�∀x ∈ X ; Ut(R  x  δ) ≤ ρ(x)� ≤ tδ .

P�∀x ∈ X ; Lt(R  x  δ) ≤ ρ(x) − b(x  R  Nt(R)  δ)� ≤ tδ  
�2(x  x�) + d1(R)2 + 2(1 + 2√d)� d ln(2d/δ)

def= 2 max
x�∈R

2n

where we introduced for convenience the quantity

b(x  R  n  δ)

+

d ln(2d/δ)

2n

.

Now on the other other hand  we have that (see the proof in [Maillard  2012  Appendix A.3])
Proposition 2 Under the assumptions that Y is a convex subset of [0  1]d  ν is sub-Gaussian  µ is
Lipschitz w.r.t. �1  ρ is Lipschitz w.r.t. �2 and R ⊂ X is compact and convex  then
P�∀x ∈ X ; Ut(R  x  δ) ≥ ρ(x) + b(x  R  Nt(R)  δ)� ≤ tδ .

Similarly  under the same assumptions  then

P�∀x ∈ X ; Lt(R  x  δ) ≥ ρ(x)� ≤ tδ .

5

4.2 Hierarchical Optimistic Region SElection driven by Curiosity (HORSE.C).

The pseudo-code of the HORSE.C algorithm is presented in Figure 1 below. This algorithm relies
on the estimation of the quantities maxx∈R ρ(x) and minx∈R ρ(x) in order to deﬁne which point
Xt+1 to sample at time t + 1. It is chosen by expanding a leaf of a hierarchical tree Tt ⊂ T   in an
optimistic way  starting with a tree T0 with leaves corresponding to the partition P.
The intuition is the following: let us consider a node (h  i) of the tree Tt expanded by the algorithm
at time t. The maximum value of ρ in R(h  i) is thus achieved for one of its children node (h�  i�) ∈
Ct(h  i). Thus if we have computed an upper bound on the maximal value of ρ in each child  then
we have an upper bound on the maximum value of ρ in R(h  i). Proceeding in a similar way for the
lower bound  this motivates the following two recursive deﬁnitions:

ˆρ+
t (h  i; δ)

ˆρ−t (h  i; δ)

def= min� max
def= max� min

x∈R(h i)

x∈R(h i)

t (h�  i�; δ) ; (h�  i�) ∈ Ct(h  i)��  
Ut(R(h  i)  x  δ)   max� ˆρ+
Lt(R(h  i)  x  δ)   min� ˆρ−t (h�  i�; δ) ; (h�  i�) ∈ Ct(h  i)�� .

t (step 7 8 9)  or according to ˆρ−t (step 11.)

These values are used in order to build an optimistic estimate of the quantity LR(h i)(Nt) in region
(h  i) (step 4)  and then to select in which cell of the partition we should sample (step 5). Then the
algorithm chooses where to sample in the selected region so as to improve the estimations of ˆρ+
t and
ˆρ−t . This is done by alternating (step 6.) between expanding a leaf following a path that is optimistic
according to ˆρ+
Thus  at a high level  the algorithm performs on each cell (h  i) ∈ P of the given partition two
hierarchical searches  one for the maximum value of ρ in region R(h  i) and one for its minimal
value. This can be seen as an adaptation of the algorithm HOO from Bubeck et al. [2011] with the
main difference that we target the variance and not just the mean (this is more difﬁcult). On the other
hand  there is a strong link between step 5  where we decide to allocate samples between regions
{R(h  i)}(h i)∈P  and the CH-AS algorithm from Carpentier et al. [2011].
5 Performance analysis of the HORSE.C algorithm

In this section  we are now ready to provide the main theorem of this paper  i.e. a regret bound on
the performance of the HORSE.C algorithm  which is the main contribution of this work. To this
end  we make use of the notion of near-optimality dimension  introduced in Bubeck et al. [2011] 
and that measures a notion of intrinsic dimension of the maximization problem.
Deﬁnition (Near optimality dimension) For c > 0  the c-optimality dimension of ρ restricted to
the region R with respect to the pseudo-metric �2 is deﬁned as

max� lim sup

�→0

ln(N (Rc�  �2  �))

ln(�−1)

  0� where Rc�

def= �x ∈ R ; ρ(x) ≥ max

x∈R

ρ(x) − ��  

and where N (Rc�  �2  �) is the �-packing number of the region Rc�.
Let d+(h0  i0) be the c-optimality dimension of ρ restricted to the region R(h0  i0) (see e.g. Bubeck
et al. [2011])  with the constant c def= 4(2c2 + c2
1)/c�2. Similarly  let d−(h0  i0) be the c-optimality
dimension of −ρ restricted to the region R(h0  i0). Let us ﬁnally deﬁne the biggest near-optimality
dimension of ρ over each cell of the partition P to be

dρ

def= max� max�d+(h0  i0)  d−(h0  i0)� ; (h0  i0) ∈ P� .

Theorem 1 (Regret bound for HORSE.C) Under the assumptions of Section 3 and if moreover
1 ≤ γ2  then for all δ ∈ [0  1]  the regret of the Hierarchical Optimistic Region SElection driven by
γ2
Curiosity procedure parameterized with δ is bounded with probability higher than 1− 2δ as follows.

RT ≤

(h0 i0)∈P�

max

T�t=|P|

1

n�

t (h0  i0)

+ 2λcγh0�B�h0  n�

t (h0  i0)  δt� 

6

Algorithm 1 The HORSE.C algorithm.
Require: An inﬁnite binary tree T   a partition P ⊂ T   δ ∈ [0  1]  λ ≥ 0
1: Let T0 be such that Leaf (T0) = P  and δi t =
π2i2(2t+1)|P|t3   t := 0.
2: while true do
3:

6δ

deﬁne for each region (h  i) ∈ Tt the estimated loss
+ λ|R(h  i)|�ˆρ+

ˆρ+
t (h  i; δ)
Nt(R(h  i))

ˆLt(h  i) def=

where δ = δNt(R(h i)) t  where by convention ˆLt(h  i) if it is undeﬁned.
choose the next region of the current partition P ⊂ T to sample
(Ht+1  It+1) def= argmax� ˆLt(h  i) ; (h  i) ⊂ P� .

if Nt(R(h  i)) = n is odd then

t (h  i; δ) − ˆρ−t (h  i; δ)�  

4:

5:
6:

7:

8:

else

9:
10:
11:
12:
13: end while

end if
t := t + 1.

sequentially select a path of children of (Ht+1  It+1) in Tt deﬁned by the initial node
(H 0

t+1  I 0

until j = jt+1 is such that (H jt+1
expand the node (H jt+1

t+1   I j+1

t+1   I jt+1

t+1) def= (Ht+1  It+1)  and then
(H j+1

t+1 ) ∈ Leaf (Tt).

t (h  i; δn t) ; (h  i) ∈ Ct(H j

t+1 ) def= argmax�ˆρ+
(ht+1  it+1) def= argmax�ˆρ+

t+1)�  
t+1 )� .
def= argmax�Ut(R(ht+1  it+1)  x  δn t) ; x ∈ R(ht+1  it+1)�  

t (h  i; δn t) ; (h  i) ∈ Ct+1(H jt+1

t+1   I jt+1

t+1   I jt+1

sample at point Xt+1 and receive the value Yt+1 ∼ ν(Xt+1)  where

t+1  I j

Xt+1

t+1 ) in order to deﬁne Tt+1 and then deﬁne the candidate child

proceed similarly than steps 6 7 8 with ˆρ+

t replaced with ˆρ−t .

+

def=

1γ2h

2 + c2

2(2c2γh

Nh0 (h  k)

2Nh0 (h  k)

def= min

d ln(2d/δk t)

1
C(c�2γh

t (h0  i0) is the optimal

d ln(2d/δk t)
1γ2h

t (h0 i0) t−1  where n�

in which we have used the following quantity

where δt is a shorthand notation for the quantity δn�
allocation at round t for the region (h0  i0) ∈ P and where
B(h0  k  δk t)

1 + 2(1 + 2√d)� d ln(2d/δk t)
h0≤h� 2c2γh
2 )−dρ�k − 2h−h0 [2 + 4√d +�d ln(2d/δk t)/2]2

2Nh0 (h  k) � 
1 )2� .
Note that the assumption γ2
1 ≤ γ2 is only here so that dρ can be deﬁned w.r.t the metric �2 only.
We can remove it at the price of using instead a metric mixing �1 and �2 together and of much
more technical considerations. Similarly  we could have expressed the result using the local values
d+(h0  i0) instead of the less precise dρ (neither those  nor dρ need to be known by the algorithm).
The full proof of this theorem is reported in the appendix. The main steps of the proof are as follows.
First we provide upper and lower conﬁdence bounds for the estimation of the quantities Ut(R  x  δ)
and Lt(R  x  δ). Then  we lower-bound the depth of the subtree of each region (h0  i0) ∈ P that
contains a maximal point argmaxx∈R(h0 i0) ρ(x)  and proceed similarly for a minimal point. This
uses the near-optimality dimension of ρ and −ρ in the region R(h0  i0)  and enables to provide an
t (h  i; δ) as well as a lower bound on ˆρ−t (h  i; δ). This then enables us to deduce
upper bound on ˆρ+
bounds relating the estimated loss ˆLt(h  i) to the true loss LR(h i)(Nt). Finally  we relate the true
t+1(h0  i0) by discussing whether a
loss of the current allocation to the one using the optimal one n�
region has been over or under sampled. This ﬁnal part is closed in spirit to the proof of the regret
bound for CH-AS in Carpentier et al. [2011].
In order to better understand the gain in Theorem 1  we provide the following corollary that gives
more insights about the order of magnitude of the regret.

2 + c2

7

2

Corollary 1 Let β def= 1+ln� max{2  γ−dρ
the partition P of the space X is well behaved  i.e. that for all (h0  i0) ∈ P  then n�
at least at speed O� ln(t)� 1
RT = O� T�t=|P|

}�. Under the assumptions of Theorem 1  assuming that
γ2�2h0β�  then for all δ ∈ [0  1]  with probability higher than 1 − 2δ we
(h0 i0)∈P�

+ 2λcγh0�� ln(t)

2β�.
t (h0  i0)� 1

t+1(h0  i0) grows

t (h0  i0)

have

max

n�

n�

1

(h0 i0)

1

ln(t)

n�

n�

max

LP (n�

t ) =

This regret term has to be compared with the typical range of the cumulative loss of the optimal
allocation strategy  that is given by

+ 2λcγh0 (ρ+

t (h0  i0)− 1

T�t=|P|

T�t=|P|

(h0 i0)
t (h0  i0)

2β   i.e. decays at speed n�

ρ+(h0 i0)�

t (h0 i0)� 1

(h0 i0)∈P� ρ+

def= maxx∈R(h0 i0) ρ(x)  and similarly ρ−(h0 i0)

(h0 i0) − ρ−(h0 i0))� 
def= minx∈R(h0 i0) ρ(x). Thus 
where ρ+
this shows that  after normalization  the relative regret on each cell (h0  i0) is roughly of order
2β . This shows that we are not only able
to compete with the performance of the best allocation strategy  but we actually achieve the exact
same performance with multiplicative factor 1  up to a second order term. Note also that  when
speciﬁed to the case of Example 1  the order of this regret is competitive with the standard results
from Carpentier et al. [2011].
The lost of the variance term ρ+(h0  i0)−1 (that is actually a constant) here comes from the fact
that we are only able to use Hoeffding’s like bounds for the estimation of the variance. In order
to remove it  one would need empirical Bernstein’s bounds for variance estimation in the case of
martingale difference sequences. This is postponed to future work.
6 Discussion
In this paper  we have provided an algorithm together with a regret analysis for a problem of online
allocation of samples in a ﬁxed partition  where the objective is to minimize a loss that contains a
penalty term that is driven by a notion of curiosity. A very speciﬁc case (ﬁnite state space) already
corresponds to a difﬁcult question known as active learning in the multi-armed bandit setting and
has been previously addressed in the literature (e.g. Antos et al. [2010]  Carpentier et al. [2011]). We
have considered an extension of this problem to a continuous domain where a ﬁxed partition of the
space as well as a generative model of the unknown dynamic are given  using our curiosity-driven
loss function as a measure of performance. Our main result is a regret bound for that problem 
that shows that our procedure is ﬁrst order optimal  i.e. achieves the same performance as the best
possible allocation (thus with multiplicative constant 1).
We believe this result contributes to ﬁlling the important gap that exists between existing algorithms
for the challenging setting of intrinsic reinforcement learning and a theoretical analysis of such  the
HORSE.C algorithm being related in spirit to  yet simpler and less ambitious the RIAC algorithm
from Baranes and Oudeyer [2009]. Indeed  in order to achieve the objective that tries to address
RIAC  one should ﬁrst remove the assumption that the partition is given: One trivial solution is to
run the HORSE.C algorithm in episodes of doubling length  starting with the trivial partition  and to
select at the end of each a possibly better partition based on computed conﬁdence intervals  however
making efﬁcient use of previous samples and avoiding a blow-up of candidate partitions happen to
be a challenging question; then one should relax the generative model assumption (i.e. that we can
sample wherever we want)  a question that shares links with a problem called autonomous explo-
ration. Thus  even if the regret analysis of the HORSE.C algorithm is already a strong  new result
that is interesting independently of such difﬁcult speciﬁc goals and of the reinforcement learning
framework (no MDP structure is required)  those questions are naturally left for future work.

Acknowledgements The research leading to these results has received funding from the European
Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no 270327
(CompLACS) and no 216886 (PASCAL2).

8

References
Andr`as Antos  Varun Grover  and Csaba Szepesv`ari. Active learning in heteroscedastic noise. The-

oretical Computer Science  411(29-30):2712–2728  2010.

A. Baranes and P.-Y. Oudeyer. R-IAC: Robust Intrinsically Motivated Exploration and Active Learn-

ing. IEEE Transactions on Autonomous Mental Development  1(3):155–169  October 2009.

S´ebastien Bubeck  R´emi Munos  Gilles Stoltz  and Csaba Szepesv`ari. X-armed bandits. Journal of

Machine Learning Research  12:1655–1695  2011.

Alexandra Carpentier  Alessandro Lazaric  Mohammad Ghavamzadeh  R´emi Munos  and Peter
Auer. Upper-conﬁdence-bound algorithms for active learning in multi-armed bandits. In Jyrki
Kivinen  Csaba Szepesv`ari  Esko Ukkonen  and Thomas Zeugmann  editors  Algorithmic Learn-
ing Theory  volume 6925 of Lecture Notes in Computer Science  pages 189–203. Springer Berlin
/ Heidelberg  2011.

Vincent Graziano  Tobias Glasmachers  Tom Schaul  Leo Pape  Giuseppe Cuccu  J. Leitner  and
J. Schmidhuber. Artiﬁcial Curiosity for Autonomous Space Exploration. Acta Futura (in press) 
(1)  2011.

Tobias Jung  Daniel Polani  and Peter Stone. Empowerment for continuous agent-environment sys-
tems. Adaptive Behavior - Animals  Animats  Software Agents  Robots  Adaptive Systems  19(1):
16–39  2011.

G.D. Konidaris. Autonomous robot skill acquisition. PhD thesis  University of Massachusetts

Amherst  2011.

Odalric-Ambrym Maillard. Hierarchical optimistic region selection driven by curiosity. HAL  2012.

URL http://hal.archives-ouvertes.fr/hal-00740418.

Georg Martius  J. Michael Herrmann  and Ralf Der. Guided self-organisation for autonomous
In Proceedings of the 9th European conference on Advances in artiﬁcial

robot development.
life  ECAL’07  pages 766–775  Berlin  Heidelberg  2007. Springer-Verlag.

Jonathan Mugan. Autonomous Qualitative Learning of Distinctions and Actions in a Developing

Agent. PhD thesis  University of Texas at Austin  2010.

Pierre-Yves Oudeyer and Frederic Kaplan. What is Intrinsic Motivation? A Typology of Computa-

tional Approaches. Frontiers in neurorobotics  1(November):6  January 2007.

J. Schmidhuber. Formal theory of creativity  fun  and intrinsic motivation (1990-2010). Autonomous

Mental Development  IEEE Transactions on  2(3):230–247  2010.

9

,Zhengdong Lu
Hang Li
Chao Qu
Shie Mannor
Huan Xu
Yuan Qi
Le Song
Junwu Xiong