2018,FRAGE: Frequency-Agnostic Word Representation,Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space  we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space  and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective  especially for rare words  and consequently limits the performance of these neural network models. In order to mitigate the issue  in this paper  we propose a neat  simple yet effective adversarial training method to blur the boundary between the embeddings of high-frequency words and low-frequency words. We conducted comprehensive studies on ten datasets across four natural language processing tasks  including word similarity  language modeling  machine translation and text classification. Results show that we achieve higher performance than the baselines in all tasks.,FRAGE: Frequency-Agnostic Word Representation

Chengyue Gong1

Di He2

Xu Tan3

cygong@pku.edu.cn

di_he@pku.edu.cn

xu.tan@microsoft.com

Tao Qin3

Liwei Wang2 4

Tie-Yan Liu3

taoqin@microsoft.com

wanglw@cis.pku.edu.cn

tie-yan.liu@microsoft.com

1Peking University

2Key Laboratory of Machine Perception  MOE  School of EECS  Peking University

4Center for Data Science  Peking University  Beijing Institute of Big Data Research

3Microsoft Research Asia

Abstract

Continuous word representation (aka word embedding) is a basic building block
in many neural network-based models used in natural language processing tasks.
Although it is widely accepted that words with similar semantics should be close
to each other in the embedding space  we ﬁnd that word embeddings learned in
several tasks are biased towards word frequency: the embeddings of high-frequency
and low-frequency words lie in different subregions of the embedding space  and
the embedding of a rare word and a popular word can be far from each other even
if they are semantically similar. This makes learned word embeddings ineffective 
especially for rare words  and consequently limits the performance of these neural
network models. In this paper  we develop FRequency-AGnostic word Embedding
(FRAGE) which is a neat  simple yet effective way to learn word representation
using adversarial training. We conducted comprehensive studies on ten datasets
across four natural language processing tasks  including word similarity  language
modeling  machine translation  and text classiﬁcation. Results show that with
FRAGE  we achieve higher performance than the baselines in all tasks.

1

Introduction

Word embeddings  which are distributed and continuous vector representations for word tokens 
have been one of the basic building blocks for many neural network-based models used in natural
language processing (NLP) tasks  such as language modeling [18  16]  text classiﬁcation [24  7] and
machine translation [4  5  40  38  11]. Different from classic one-hot representation  the learned word
embeddings contain semantic information which can measure the semantic similarity between words
[28]  and can also be transferred into other learning tasks [29  3].
In deep learning approaches for NLP tasks  word embeddings act as the inputs of the neural network
and are usually trained together with neural network parameters. As the inputs of the neural network 
word embeddings carry all the information of words that will be further processed by the network 
and the quality of embeddings is critical and highly impacts the ﬁnal performance of the learning task
[15]. Unfortunately  we ﬁnd the word embeddings learned by many deep learning approaches are far
from perfect. As shown in Figure 1(a) and 1(b)  in the embedding space learned by word2vec model 
the nearest neighbors of word “Peking” includes “quickest”  “multicellular”  and “epigenetic”  which

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

are not semantically similar  while semantically related words such as “Beijing” and “China” are far
from it. Similar phenomena are observed from the word embeddings learned from translation tasks.
With a careful study  we ﬁnd a more general problem which is rooted in low-frequency words in
the text corpus. Without any confusion  we also call high-frequency words as popular words and
call low-frequency words as rare words. As is well known [23]  the frequency distribution of words
roughly follows a simple mathematical form known as Zipf’s law. When the size of a text corpus
grows  the frequency of rare words is much smaller than popular words while the number of unique
rare words is much larger than popular words. Interestingly  the learned embeddings of rare words
and popular words behave differently. (1) In the embedding space  a popular word usually has
semantically related neighbors  while a rare word usually does not. Moreover  the nearest neighbors
of more than 85% rare words are rare words. (2) Word embeddings encode frequency information.
As shown in Figure 1(a) and 1(b)  the embeddings of rare words and popular words actually lie in
different subregions of the space. Such a phenomenon is also observed in [29].
We argue that the different behaviors of the embeddings of popular words and rare words are
problematic. First  such embeddings will affect the semantic understanding of words. We observe
more than half of the rare words are nouns or variants of popular words. Those rare words should
have similar meanings or share the same topics with popular words. Second  the neighbors of a large
number of rare words are semantically unrelated rare words. To some extent  those word embeddings
encode more frequency information than semantic information which is not good from the view
of semantic understanding. It will consequently limit the performance of down-stream tasks using
the embeddings. For example  in text classiﬁcation  it cannot be well guaranteed that the label of a
sentence does not change when you replace one popular/rare word in the sentence by its rare/popular
alternatives.
To address this problem  in this paper  we propose an adversarial training method to learn FRequency-
AGnostic word Embedding (FRAGE). For a given NLP task  in addition to minimizing the task-speciﬁc
loss by optimizing the task-speciﬁc parameters together with word embeddings  we introduce another
discriminator  which takes a word embedding as input and classiﬁes whether it is a popular/rare
word. The discriminator optimizes its parameters to maximize its classiﬁcation accuracy  while word
embeddings are optimized towards a low task-dependent loss as well as fooling the discriminator
to misclassify the popular and rare words. When the whole training process converges and the
system achieves an equilibrium  the discriminator cannot well differentiate popular words from rare
words. Consequently  rare words lie in the same region as and are mixed with popular words in the
embedding space. Then FRAGE will catch better semantic information and help the task-speciﬁc
model to perform better.
We conduct experiments on four types of NLP tasks  including three word similarity tasks  two
language modeling tasks  three sentiment classiﬁcation tasks  and two machine translation tasks to
test our method. In all tasks  FRAGE outperforms the baselines. Speciﬁcally  in language modeling
and machine translation  we achieve better performance than the state-of-the-art results on PTB  WT2
and WMT14 English-German datasets.

2 Background

2.1 Word Representation

Words are the basic units of natural languages  and distributed word representations (i.e.  word
embeddings) are the basic units of many models in NLP tasks including language modeling [18  16]
and machine translation [4  5  40  38  11]. It has been demonstrated that word representations learned
from one task can be transferred to other tasks and achieve competitive performance [3].
While word embeddings play an important role in neural network-based models in NLP and achieve
great success  one technical challenge is that the embeddings of rare words are difﬁcult to train due
to their low frequency of occurrences. [35] develops a novel way to split each word into sub-word
units which is widely used in neural machine translation. However  the low-frequency sub-word units
are still difﬁcult to train: [31] provides a comprehensive study which shows that the rare (sub)words
are usually under-estimated in neural machine translation: during inference step  the model tends to
choose popular words over their rare alternatives.

2

2.2 Adversarial Training

The basic idea of our work to address the above problem is adversarial training  in which two or
more models learn together by pursuing competing goals. A representative example of adversarial
training is Generative Adversarial Networks (GANs) [13  34] for image generation [33  42  2]  in
which a discriminator and a generator compete with each other: the generator aims to generate images
similar to the natural ones  and the discriminator aims to detect the generated ones from the natural
ones. Recently  adversarial training has been successfully applied to NLP tasks [6  22  21]. [6  22]
introduce an additional discriminator to differentiate the semantics learned from different languages
in non-parallel bilingual data. [21] develops a discriminator to classify whether a sentence is created
by human or generated by a model.
Our proposed method is under the adversarial training framework but not exactly the conventional
generator-discriminator approach since there is no generator in our scenario. For an NLP task and its
neural network model (including word embeddings)  we introduce a discriminator to differentiate
embeddings of popular words and rare words; while the NN model aims to fool the discriminator and
minimize the task-speciﬁc loss simultaneously.
Our work is also weakly related to adversarial domain adaptation which attempts to mitigate the
negative effects of domain shift between training and testing [9  36]. The difference between this
work and adversarial domain adaptation is that we do not target at the mismatch between training and
testing; instead  we aim to improve the effectiveness of word embeddings and consequently improve
the performance of end-to-end NLP tasks.

3 Empirical Study

In this section  we study the embeddings of popular words and rare words based on the models trained
from Google News corpora using word2vec 1 and trained from WMT14 English-German translation
task using Transformer [38]. The implementation details can be found in [12].

Experimental Design In both tasks  we simply set the top 20% frequent words in vocabulary as
popular words and denote the rest as rare words (roughly speaking  we set a word as a rare word if
its relative frequency is lower than 10−6 in WMT14 dataset and 10−7 in Google News dataset). We
have tried other thresholds such as 10% or 25% and found the observations are similar.
We study whether the semantic relationship between two words is reasonable. To achieve this  we
randomly sampled some rare/popular words and checked the embeddings trained from different
tasks. For each sampled word  we determined its nearest neighbors based on the cosine similarity
between its embeddings and others’.2 We also manually chose words which are semantically similar
to it. For simplicity  for each word  we call the nearest words predicted from the embeddings as
model-predicted neighbors  and call our chosen words as semantic neighbors.

Observation To visualize word embeddings  we reduce their dimensionalities by SVD and plot
two cases in Figure 1. More cases and other studies without dimensionality reduction can be found in
Section 5.
We ﬁnd that the embeddings trained from different tasks share some common patterns. For both tasks 
more than 90% of model-predicted neighbors of rare words are rare words. For each rare word  the
model-predicted neighbor is usually not semantically related to this word  and semantic neighbors we
chose are far away from it in the embedding space. In contrast  the model-predicted neighbors of
popular words are very reasonable.
As the patterns in rare words are different from that of popular words  we further check the whole
embedding matrix to make a general understanding. We also visualize the word embeddings using
SVD by keeping the two directions with top-2 largest eigenvalues as in [28  30] and plot them in
Figure 1(c) and 1(d). From the ﬁgure  we can see that the embeddings actually encode frequencies to
a certain degree: the rare words and popular words lie in different regions after this linear projection 

1https://code.google.com/archive/p/word2vec/
2Cosine distance is the most popularly used metric in literature to measure semantic similarity [28  32  29].

We also have tried other metrics  e.g.  Euclid distance  and the phenomena still exist.

3

(a) WMT En→De Case

(b) Word2vec Case

(c) WMT En→De

(d) Word2vec

Figure 1: Case study of the embeddings trained from WMT14 translation task using Transformer
and trained from Google News dataset using word2vec is shown in (a) and (b). (c) and (d) show the
visualization of embeddings trained from WMT14 translation task using Transformer and trained
from Google News dataset using word2vec. Red points represent rare words and blue points represent
popular words. In (a) and (b)  we highlight the semantic neighbors in bold.

and thus they occupy different regions in the original embedding space. This strange phenomenon is
also observed in other learned embeddings (e.g.CBOW and GLOVE) and mentioned in [30].

Explanation From the empirical study above  we can see that the occupied spaces of popular
words and rare words are different and here we intuitively explain a possible reason. We simply
take word2vec as an example which is trained by stochastic gradient descent. During training  the
sample rate of a popular word is high and the embedding of a popular word updates frequently. For
a rare word  the sample rate is low and its embedding rarely updates. According to our study  on
average  the moving distance of the embedding for a popular word is twice longer than that of a rare
word during training. As all word embeddings are usually initialized around the origin with a small
variance  we observe in the ﬁnal model  the embeddings of rare words are still around the origin and
the popular words have moved far away.

Discussion We have strong evidence that the current phenomena are problematic. First  according
to our study 3 in both tasks  more than half of the rare words are nouns  e.g.  company names  city
names. They may share some similar topics to popular entities  e.g.  big companies and cities; around
10% percent of rare words include a hyphen (which is usually used to join popular words)  and over
30% rare words are different PoSs of popular words. These words should have mixed or similar
semantics to some popular words. These facts show that rare words and popular words should lie
in the same region of the embedding space  which is different from what we observed. Second  as
we can see from the cases  for rare words  model-predicted neighbors are usually not semantically
related words but frequency-related words (rare words). This shows  for rare words  the embeddings
encode more frequency information than semantic information. It is not good to use such word
embeddings into semantic understanding tasks  e.g.  text classiﬁcation  language modeling  language
understanding  and translation.

4 Our Method

In this section  we present our method to improve word representations. As we have a strong prior that
many rare words should share the same region in the embedding space as popular words  the basic
idea of our algorithm is to train the word embeddings in an adversarial framework: We introduce
a discriminator to categorize word embeddings into two classes: popular ones or rare ones. We
hope the learned word embeddings not only minimize the task-speciﬁc training loss but also fool the
discriminator. By doing so  the frequency information is removed from the embedding and we call
our method frequency-agnostic word embedding (FRAGE).
We ﬁrst deﬁne some notations and then introduce our algorithm. We develop three types of notations:
embeddings  task-speciﬁc parameters/loss  and discriminator parameters/loss.
Denote θemb ∈ Rd×|V | as the word embedding matrix to be learned  where d is the dimension of
the embedding vectors and |V | is the vocabulary size. Let Vpop denote the set of popular words and
Vrare = V \ Vpop denote the set of rare words. Then the embedding matrix θemb can be divided

3We use the POS tagger from Natural Language Toolkit  https://github.com/nltk.

4

dairyunattachedwartimeappendixcyberwarcowmilkPekingBeijingChinadiktatorenquickestepigeneticmulticellularFigure 2: The proposed learning framework includes a task-speciﬁc predictor and a discriminator 
whose function is to classify rare and popular words. Both modules use word embeddings as the
input.

rare for rare words. Let θemb

w

pop for popular words and θemb

into two parts: θemb
denote the embedding
of word w. Let θmodel denote all the other task-speciﬁc parameters except word embeddings. For
instance  for language modeling  θmodel is the parameters of the RNN or LSTM; for neural machine
translation  θmodel is the parameters of the encoder  attention module  and decoder.
Let LT (S; θmodel  θemb) denote the task-speciﬁc loss over a dataset S. Taking language modeling as
an example  the loss LT (S; θmodel  θemb) is deﬁned as the negative log likelihood of the data:

LT (S; θmodel  θemb) = − 1
|S|

log P (y; θmodel  θemb) 

(1)

(cid:88)

y∈S

where y is a sentence.
Let fθD denote a discriminator with parameters θD   which takes a word embedding as input and
outputs a conﬁdence score between 0 and 1 indicating how likely the word is a rare word. Let
LD(V ; θD  θemb) denote the loss of the discriminator:

LD(V ; θD  θemb) =

1

|Vpop|

log fθD (θemb

w ) +

1

|Vrare|

log(1 − fθD (θemb

w )).

(2)

(cid:88)

w∈Vpop

(cid:88)

w∈Vrare

Following the principle of adversarial training  we develop a minimax objective to train the task-
speciﬁc model (θmodel and θemb) and the discriminator (θD) as below:

min

θmodel θemb

max
θD

LT (S; θmodel  θemb) − λLD(V ; θD  θemb) 

(3)

where λ is a coefﬁcient to trade off the two loss terms. We can see that when the model parameter
θmodel and the embedding θemb are ﬁxed  the optimization of the discriminator θD becomes

−λLD(V ; θD  θemb) 

max
θD

(4)

which is to minimize the classiﬁcation error of popular and rare words. When the discriminator θD is
ﬁxed  the optimization of θmodel and θemb becomes

min

θmodel θemb

LT (S; θmodel  θemb) − λLD(V ; θD  θemb) 

(5)

i.e.  to optimize the task performance as well as fooling the discriminator. We train θmodel  θemb and
θD iteratively by stochastic gradient descent or its variants. The general training process is shown in
Algorithm 1.

5 Experiment

We test our method on a wide range of tasks  including word similarity  language modeling  machine
translation  and text classiﬁcation. For each task  we choose the state-of-the-art architecture together
with the state-of-the-art training method as our baseline 4.

4Code for our implementation is available at https://github.com/ChengyueGongR/FrequencyAgnostic

5

Input TokensWord EmbeddingsTask-specific OutputsTask-specific ModelLoss 𝐿𝑇Rare/Popular LabelsDiscriminatorLoss 𝐿𝐷predictpredictAlgorithm 1 Proposed Algorithm
1: Input: Dataset S  vocabulary V = Vpop ∪ Vrare  θmodel  θemb  θD.
2: repeat
3:
4:
5:
6:
7: until Converge
8: Output: θmodel  θemb  θD.

Sample a minibatch ˆS from S.
Sample a minibatch ˆV = ˆVpop ∪ ˆVrare from V .
Update θmodel  θemb by gradient descent according to Eqn. (5) with data ˆS.
Update θD by gradient ascent according to Eqn. (4) with vocabulary ˆV .

For fair comparisons  for each task  our method shares the same model architecture as the baseline.
The only difference is that we use the original task-speciﬁc loss function with an additional adversarial
loss as in Eqn. (3). Dataset description and hyper-parameter conﬁgurations can be found in [12].

5.1 Settings

We conduct experiments on the following tasks.
Word Similarity evaluates the performance of the learned word embeddings by calculating the word
similarity: it evaluates whether the most similar words of a given word in the embedding space are
consistent with the ground-truth  in terms of Spearman’s rank correlation. We use the skip-gram
model as our baseline model [28]5  and train the embeddings using Enwik96. We test the baseline
and our method on three datasets: RG65  WS  and RW. The RW dataset is a dataset for the evaluation
of rare words. Following common practice [28  1  32  29]  we use cosine distance while computing
the similarity between two word embeddings.
Language Modeling is a basic task in natural language processing. The goal is to predict the next
word conditioned on previous words and the task is evaluated by perplexity. We do experiments on
two widely used datasets [25  26  41]  Penn Treebank (PTB) [27] and WikiText-2 (WT2) [26]. We
choose two recent works as our baselines: the AWD-LSTM model7 [25] and the AWD-LSTM-MoS
model 8 [41]. AWD-LSTM [25] is a weight-dropped LSTM which uses Drop Connect on hidden-to-
hidden weights as a means of recurrent regularization. The model is trained by NT-ASGD  which is a
variant of the averaged stochastic gradient method. The training process has two steps  in the second
step  the model is ﬁnetuned using another conﬁguration of NT-ASGD. AWD-LSTM-MoS [41] uses
the Mixture of Softmaxes structure to the vanilla AWD-LSTM and achieves the state-of-the-art result
on PTB and WT2.
Machine Translation is a popular task in both deep learning and natural language processing. We
choose two datasets: WMT14 English-German and IWSLT14 German-English datasets  which are
evaluated in terms of BLEU score9. We use Transformer [38] as the baseline model. Transformer [38]
is a recently developed architecture in which the self-attention network is used during encoding and
decoding step. It achieves the best performances on several machine translation tasks  e.g. WMT14
English-German  WMT14 English-French datasets. We use transformer_base and transformer_big
conﬁgurations following tensor2tensor [37]10.
Text Classiﬁcation is a conventional machine learning task and is evaluated by accuracy. Following
the setting in [20]  we implement a Recurrent CNN-based model11 and test it on AG’s news corpus
(AGs)  IMDB movie review dataset (IMDB) and 20 Newsgroups (20NG). RCNN [20] contains both

5https://github.com/tensorﬂow/models/blob/master/tutorials/embedding
6http://mattmahoney.net/dc/textdata.html
7https://github.com/salesforce/awd-lstm-lm
8https://github.com/zihangdai/mos
9https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl
10To improve the training for imbalanced labeled data  a common method is to adjust loss function by
reweighting the training samples; To regularize the parameter space  a common method is to use l2 regularization.
We tested these methods in machine translation and found the performance is not good. Detailed analysis is
provided in [12]

11https://github.com/brightmart/text_classiﬁcation

6

recurrent and convolutional layers to catch the key components in texts and is widely used in text
classiﬁcation tasks.
In all tasks  we simply set the top 20% frequent words in vocabulary as popular words and denote the
rest as rare words  which is the same as our empirical study. For all the tasks except training skip-gram
model  we use full-batch gradient descent to update the discriminator. For training skip-gram model 
mini-batch stochastic gradient descent is used to update the discriminator with a batch size 3000 
since the vocabulary size is large. For language modeling and machine translation tasks  we use
logistic regression as the discriminator. For other tasks  we ﬁnd using a shallow neural network with
one hidden layer is more efﬁcient and we set the number of nodes in the hidden layer as 1.5 times
embedding size. In all tasks  we set the hyper-parameter λ to 0.1.

RG65

WS

RW

Orig. with FRAGE Orig. with FRAGE Orig. with FRAGE
75.63

78.78

58.12

66.74

69.35

52.67

Table 1: Results on three word similarity datasets.

Paras

Orig.

with FRAGE
Validation Test Validation Test

PTB

WT2

AWD-LSTM w/o ﬁnetune[25]
AWD-LSTM[25]
AWD-LSTM + continuous cache pointer[25]
AWD-LSTM-MoS w/o ﬁnetune[41]
AWD-LSTM-MoS[41]
AWD-LSTM-MoS + dynamic evaluation[41]

AWD-LSTM w/o ﬁnetune[25]
AWD-LSTM[25]
AWD-LSTM + continuous cache pointer[25]
AWD-LSTM-MoS w/o ﬁnetune[41]
AWD-LSTM-MoS[41]
AWD-LSTM-MoS + dynamic evaluation[41]

24M
24M
24M
24M
24M
24M

33M
33M
33M
35M
35M
35M

60.7
60.0
53.9
58.08
56.54
48.33

69.1
68.6
53.8
66.01
63.88
42.41

58.8
57.3
52.8
55.97
54.44
47.69

67.1
65.8
52.0
63.33
61.45
40.68

60.2
58.1
52.3
57.55
55.52
47.38

67.9
66.5
51.0
64.86
62.68
40.85

58.0
56.1
51.8
55.23
53.31
46.54

64.8
63.4
49.3
62.12
59.73
39.14

Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the
perplexity  better the result. Baseline results are obtained from [25  41]. “Paras” denotes the number
of model parameters.

5.2 Results

In this subsection  we provide the experimental results of all tasks. For simplicity  we use “with
FRAGE” as our proposed method in the tables.
Word Similarity The results on three word similarity tasks are listed in Table 1. From the table 
we can see that our method consistently outperforms the baseline on all datasets. In particular  we
outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our
method improves the representation of words  especially the rare words.
Language Modeling The results of language modeling on PTB and WT2 datasets are presented in Ta-
ble 2. We test our model and the baselines at several checkpoints used in the baseline papers: without
ﬁnetune  with ﬁnetune  with post-process (continuous cache pointer [14] or dynamic evaluation [19]).
In all these settings  our method outperforms the two baselines. On PTB dataset  our method improves

7

the AWD-LSTM and AWD-LSTM-MoS baseline by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set
at different checkpoints. On WT2 dataset  which contains more rare words  our method achieves
larger improvements. We improve the results of AWD-LSTM and AWD-LSTM-MoS by 2.3/2.4/2.7
and 1.15/1.72/1.54 in terms of test perplexity  respectively.

WMT En→De

IWSLT De→En

Method
ByteNet[17]
ConvS2S[11]
Transformer Base[38]
Transformer Base with FRAGE 28.36 ConvS2S+Risk [8]
Transformer Big[38]
Transformer Big with FRAGE

BLEU
BLEU Method
30.04
23.75 DeepConv[10]
25.16 Dual transfer learning [39] 32.35
32.68
27.30 ConvS2S+SeqNLL [8]
32.93
33.12
28.40 Transformer
29.11 Transformer with FRAGE 33.97

Table 3: BLEU scores on test set on WMT2014 English-German and IWSLT German-English tasks.

Machine Translation The results of neural machine translation on WMT14 English-German and
IWSLT14 German-English tasks are shown in Table 3. We outperform the baselines for 1.06/0.71 in
the term of BLEU in transformer_base and transformer_big settings in WMT14 English-German
task  respectively. The model learned from adversarial training also outperforms the original one
in IWSLT14 German-English task by 0.85. These results show improving word embeddings can
achieve better results in more complicated tasks and larger datasets.

AG’s

IMDB

20NG

Orig. with FRAGE Orig. with FRAGE
90.47%

91.73% 92.41%

Orig.

with FRAGE

93.07% 96.49%[20]

96.93%

Table 4: Accuracy on test sets of AG’s news corpus (AG’s)  IMDB movie review dataset (IMDB)
and 20 Newsgroups (20NG) for text classiﬁcation.

Text Classiﬁcation The results are listed in Table 4. Our method outperforms the baseline method
for 1.26%/0.66%/0.44% on three different datasets.
As a summary  our experiments on four different tasks with 10 datasets verify the effectiveness of our
method. We provide case study and qualitative analysis of the model with and without our method in
Table 5 and Figure 3. By comparing the cases  we ﬁnd that  with our method  the word similarities
are improved and popular/rare words are better mixed together. More cases are shown in [12].

(a)

(b)

Figure 3: These ﬁgures show that  in different tasks  the embeddings of rare and popular words are
better mixed together after applying our method.

8

Orig.

Orig.

Word: citizens Word: citizenship* Word: accepts* Word: bacterial*

clinicians*
astronomers*
westliche
adults

Model-predicted neighbor
announces*
digs*
externally*
empowers*

bliss*
pakistanis*
dismiss*
reinforces*

multicellular*
epigenetic*
isotopic*
conformational*

Semantic neighbor + Model-predicted Ranking

citizen*:771
citizenship*:832

citizen*:10745
citizens:11706

accepted*:21109 bacteria*:116
accept:30612
chemical:233

Orig. with FRAGE

Orig. with FRAGE

Word: citizens Word: citizenship* Word: accepts* Word: bacterial*

Model-predicted neighbor

homes
citizen*
bürger
population

population
städtischen*
dignity
bürger

registered
tolerate*
recognizing*
accepting*

myeloproliferative*
metabolic*
bacteria*
apoptotic*

Semantic neighbor + Model-predicted Ranking

citizen*:2
citizenship*:40

citizen*:79
citizens:7

accepted*:26
accept:29

bacteria* : 3
chemical: 8

Table 5: Case study for the original model and our method. Rare words are marked by “*”. For each
word  we list its model-predicted neighbors. Moreover  we also show the ranking positions of the
semantic neighbors based on cosine similarity. As we can see  the ranking positions of the semantic
neighbors are very low for the original model.

6 Conclusion

In this paper  we ﬁnd that word embeddings learned in several tasks are biased towards word
frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of
the embedding space. This makes learned word embeddings ineffective  especially for rare words 
and consequently limits the performance of these neural network models. We propose a neat  simple
yet effective adversarial training method to improve the model performance which is veriﬁed in a
wide range of tasks.
We will explore several directions in the future. First  we will investigate the theoretical aspects
of word embedding learning and our adversarial training method. Second  we will study more
applications which have the similar problem even beyond NLP.

Acknowledgement

This work is supported by National Basic Research Program of China (973 Program) (grant no.
2015CB352502)  NSFC (61573026) and BJNSF (L172037) and a grant from Microsoft Research
Asia. We would like to thank the anonymous reviewers for their valuable comments on our paper.

9

References
[1] R. Al-Rfou  B. Perozzi  and S. Skiena. Polyglot: Distributed word representations for multilin-
gual nlp. In Proceedings of the Seventeenth Conference on Computational Natural Language
Learning  pages 183–192  Soﬁa  Bulgaria  August 2013. Association for Computational Lin-
guistics.

[2] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875 

2017.

[3] S. Arora  Y. Liang  and T. Ma. A simple but tough-to-beat baseline for sentence embeddings.

2016.

[4] D. Bahdanau  K. Cho  and Y. Bengio. Neural machine translation by jointly learning to align

and translate. arXiv preprint arXiv:1409.0473  2014.

[5] K. Cho  B. Van Merriënboer  C. Gulcehre  D. Bahdanau  F. Bougares  H. Schwenk  and
Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078  2014.

[6] A. Conneau  G. Lample  M. Ranzato  L. Denoyer  and H. Jégou. Word translation without

parallel data. arXiv preprint arXiv:1710.04087  2017.

[7] A. M. Dai and Q. V. Le. Semi-supervised sequence learning. In Advances in Neural Information

Processing Systems  pages 3079–3087  2015.

[8] S. Edunov  M. Ott  M. Auli  D. Grangier  and M. Ranzato. Classical structured prediction losses

for sequence to sequence learning. arXiv preprint arXiv:1711.04956  2017.

[9] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In Interna-

tional Conference on Machine Learning  pages 1180–1189  2015.

[10] J. Gehring  M. Auli  D. Grangier  and Y. N. Dauphin. A convolutional encoder model for neural

machine translation. arXiv preprint arXiv:1611.02344  2016.

[11] J. Gehring  M. Auli  D. Grangier  D. Yarats  and Y. N. Dauphin. Convolutional sequence to

sequence learning. arXiv preprint arXiv:1705.03122  2017.

[12] C. Gong  D. He  X. Tan  T. Qin  L. Wang  and T. Liu. FRAGE: frequency-agnostic word

representation. CoRR  abs/1809.06858  2018.

[13] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and
Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems 
pages 2672–2680  2014.

[14] E. Grave  A. Joulin  and N. Usunier. Improving neural language models with a continuous

cache. CoRR  abs/1612.04426  2016.

[15] E. Hoffer  I. Hubara  and D. Soudry. Fix your classiﬁer: the marginal value of training the last

weight layer. ICLR  2018.

[16] R. Jozefowicz  O. Vinyals  M. Schuster  N. Shazeer  and Y. Wu. Exploring the limits of language

modeling. arXiv preprint arXiv:1602.02410  2016.

[17] N. Kalchbrenner  L. Espeholt  K. Simonyan  A. v. d. Oord  A. Graves  and K. Kavukcuoglu.

Neural machine translation in linear time. arXiv preprint arXiv:1610.10099  2016.

[18] Y. Kim  Y. Jernite  D. Sontag  and A. M. Rush. Character-aware neural language models. In

AAAI  pages 2741–2749  2016.

[19] B. Krause  E. Kahembwe  I. Murray  and S. Renals. Dynamic evaluation of neural sequence

models. CoRR  abs/1709.07432  2017.

[20] S. Lai  L. Xu  K. Liu  and J. Zhao. Recurrent convolutional neural networks for text classiﬁcation.

In AAAI  volume 333  pages 2267–2273  2015.

10

[21] A. M. Lamb  A. G. A. P. GOYAL  Y. Zhang  S. Zhang  A. C. Courville  and Y. Bengio. Professor
forcing: A new algorithm for training recurrent networks. In Advances In Neural Information
Processing Systems  pages 4601–4609  2016.

[22] G. Lample  L. Denoyer  and M. Ranzato. Unsupervised machine translation using monolingual

corpora only. arXiv preprint arXiv:1711.00043  2017.

[23] R. R. Larson.

Introduction to information retrieval. Journal of the American Society for

Information Science and Technology  61(4):852–853  2010.

[24] A. L. Maas  R. E. Daly  P. T. Pham  D. Huang  A. Y. Ng  and C. Potts. Learning word
vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies-Volume 1  pages 142–150.
Association for Computational Linguistics  2011.

[25] S. Merity  N. S. Keskar  and R. Socher. Regularizing and optimizing LSTM language models.

CoRR  abs/1708.02182  2017.

[26] S. Merity  C. Xiong  J. Bradbury  and R. Socher. Pointer sentinel mixture models. CoRR 

abs/1609.07843  2016.

[27] T. Mikolov  M. Karaﬁát  L. Burget  J. Cernocký  and S. Khudanpur. Recurrent neural network
based language model. In INTERSPEECH 2010  11th Annual Conference of the International
Speech Communication Association  Makuhari  Chiba  Japan  September 26-30  2010  pages
1045–1048  2010.

[28] T. Mikolov  I. Sutskever  K. Chen  G. S. Corrado  and J. Dean. Distributed representations of
words and phrases and their compositionality. In Advances in neural information processing
systems  pages 3111–3119  2013.

[29] J. Mu  S. Bhat  and P. Viswanath. All-but-the-top: simple and effective postprocessing for word

representations. arXiv preprint arXiv:1702.01417  2017.

[30] J. Mu  S. Bhat  and P. Viswanath. All-but-the-top: Simple and effective postprocessing for word

representations. CoRR  abs/1702.01417  2017.

[31] M. Ott  M. Auli  D. Granger  and M. Ranzato. Analyzing uncertainty in neural machine

translation. arXiv preprint arXiv:1803.00047  2018.

[32] J. Pennington  R. Socher  and C. D. Manning. Glove: Global vectors for word representation.
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing 
EMNLP 2014  October 25-29  2014  Doha  Qatar  A meeting of SIGDAT  a Special Interest
Group of the ACL  pages 1532–1543  2014.

[33] A. Radford  L. Metz  and S. Chintala. Unsupervised representation learning with deep convolu-

tional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[34] T. Salimans  I. Goodfellow  W. Zaremba  V. Cheung  A. Radford  and X. Chen. Improved
techniques for training gans. In Advances in Neural Information Processing Systems  pages
2234–2242  2016.

[35] R. Sennrich  B. Haddow  and A. Birch. Neural machine translation of rare words with subword

units. arXiv preprint arXiv:1508.07909  2015.

[36] E. Tzeng  J. Hoffman  K. Saenko  and T. Darrell. Adversarial discriminative domain adaptation.
In 2017 IEEE Conference on Computer Vision and Pattern Recognition  CVPR 2017  Honolulu 
HI  USA  July 21-26  2017  pages 2962–2971  2017.

[37] A. Vaswani  S. Bengio  E. Brevdo  F. Chollet  A. N. Gomez  S. Gouws  L. Jones  L. Kaiser 
N. Kalchbrenner  N. Parmar  R. Sepassi  N. Shazeer  and J. Uszkoreit. Tensor2tensor for neural
machine translation. CoRR  abs/1803.07416  2018.

[38] A. Vaswani  N. Shazeer  N. Parmar  J. Uszkoreit  L. Jones  A. N. Gomez  Ł. Kaiser  and
I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 
pages 6000–6010  2017.

11

[39] Y. Wang  Y. Xia  L. Zhao  J. Bian  T. Qin  G. Liu  and L. Tie-Yan. Dual transfer learning for

neural machine translation with marginal distribution regularization.

[40] Y. Wu  M. Schuster  Z. Chen  Q. V. Le  M. Norouzi  W. Macherey  M. Krikun  Y. Cao  Q. Gao 
K. Macherey  et al. Google’s neural machine translation system: Bridging the gap between
human and machine translation. arXiv preprint arXiv:1609.08144  2016.

[41] Z. Yang  Z. Dai  R. Salakhutdinov  and W. W. Cohen. Breaking the softmax bottleneck: A

high-rank RNN language model. CoRR  abs/1711.03953  2017.

[42] J.-Y. Zhu  T. Park  P. Isola  and A. A. Efros. Unpaired image-to-image translation using

cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593  2017.

12

,Chengyue Gong
Di He
Tao Qin
Liwei Wang
Tie-Yan Liu
Mitsuru Kusumoto
Takuya Inoue
Gentaro Watanabe
Takuya Akiba
Masanori Koyama