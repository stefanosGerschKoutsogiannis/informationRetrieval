2015,Efficient and Robust Automated Machine Learning,The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice  such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand  and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. In this work we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers  14 feature preprocessing methods  and 4 data preprocessing methods  giving rise to a structured hypothesis space with 110 hyperparameters). This system  which we dub auto-sklearn  improves on existing AutoML methods by automatically taking into account past performance on similar datasets  and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge  and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto-sklearn.,Efﬁcient and Robust Automated Machine Learning

Matthias Feurer

Jost Tobias Springenberg

Aaron Klein
Manuel Blum

Katharina Eggensperger

Frank Hutter

Department of Computer Science
University of Freiburg  Germany

{feurerm kleinaa eggenspk springj mblum fh}@cs.uni-freiburg.de

Abstract

The success of machine learning in a broad range of applications has led to an
ever-growing demand for machine learning systems that can be used off the shelf
by non-experts. To be effective in practice  such systems need to automatically
choose a good algorithm and feature preprocessing steps for a new dataset at hand 
and also set their respective hyperparameters. Recent work has started to tackle this
automated machine learning (AutoML) problem with the help of efﬁcient Bayesian
optimization methods. Building on this  we introduce a robust new AutoML system
based on scikit-learn (using 15 classiﬁers  14 feature preprocessing methods  and
4 data preprocessing methods  giving rise to a structured hypothesis space with
110 hyperparameters). This system  which we dub AUTO-SKLEARN  improves on
existing AutoML methods by automatically taking into account past performance
on similar datasets  and by constructing ensembles from the models evaluated
during the optimization. Our system won the ﬁrst phase of the ongoing ChaLearn
AutoML challenge  and our comprehensive analysis on over 100 diverse datasets
shows that it substantially outperforms the previous state of the art in AutoML. We
also demonstrate the performance gains due to each of our contributions and derive
insights into the effectiveness of the individual components of AUTO-SKLEARN.

1

Introduction

Machine learning has recently made great strides in many application areas  fueling a growing
demand for machine learning systems that can be used effectively by novices in machine learning.
Correspondingly  a growing number of commercial enterprises aim to satisfy this demand (e.g. 
BigML.com  Wise.io  SkyTree.com  RapidMiner.com  Dato.com  Prediction.io  DataRobot.com  Microsoft’s Azure Machine
Learning  Google’s Prediction API  and Amazon Machine Learning). At its core  every effective machine learning
service needs to solve the fundamental problems of deciding which machine learning algorithm to
use on a given dataset  whether and how to preprocess its features  and how to set all hyperparameters.
This is the problem we address in this work.
More speciﬁcally  we investigate automated machine learning (AutoML)  the problem of automatically
(without human input) producing test set predictions for a new dataset within a ﬁxed computational
budget. Formally  this AutoML problem can be stated as follows:
Deﬁnition 1 (AutoML problem). For i = 1  . . .   n+m  let xi ∈ Rd denote a feature vector and yi ∈
Y the corresponding target value. Given a training dataset Dtrain = {(x1  y1)  . . .   (xn  yn)} and
the feature vectors xn+1  . . .   xn+m of a test dataset Dtest = {(xn+1  yn+1)  . . .   (xn+m  yn+m)}
drawn from the same underlying data distribution  as well as a resource budget b and a loss metric
L(· ·)  the AutoML problem is to (automatically) produce test set predictions ˆyn+1  . . .   ˆyn+m. The
loss of a solution ˆyn+1  . . .   ˆyn+m to the AutoML problem is given by 1
m

(cid:80)m
j=1 L(ˆyn+j  yn+j).

1

In practice  the budget b would comprise computational resources  such as CPU and/or wallclock time
and memory usage. This problem deﬁnition reﬂects the setting of the ongoing ChaLearn AutoML
challenge [1]. The AutoML system we describe here won the ﬁrst phase of that challenge.
Here  we follow and extend the AutoML approach ﬁrst introduced by AUTO-WEKA [2] (see
http://automl.org). At its core  this approach combines a highly parametric machine learning
framework F with a Bayesian optimization [3] method for instantiating F well for a given dataset.
The contribution of this paper is to extend this AutoML approach in various ways that considerably
improve its efﬁciency and robustness  based on principles that apply to a wide range of machine
learning frameworks (such as those used by the machine learning service providers mentioned above).
First  following successful previous work for low dimensional optimization problems [4  5  6] 
we reason across datasets to identify instantiations of machine learning frameworks that perform
well on a new dataset and warmstart Bayesian optimization with them (Section 3.1). Second  we
automatically construct ensembles of the models considered by Bayesian optimization (Section 3.2).
Third  we carefully design a highly parameterized machine learning framework from high-performing
classiﬁers and preprocessors implemented in the popular machine learning framework scikit-learn [7]
(Section 4). Finally  we perform an extensive empirical analysis using a diverse collection of datasets
to demonstrate that the resulting AUTO-SKLEARN system outperforms previous state-of-the-art
AutoML methods (Section 5)  to show that each of our contributions leads to substantial performance
improvements (Section 6)  and to gain insights into the performance of the individual classiﬁers and
preprocessors used in AUTO-SKLEARN (Section 7).

2 AutoML as a CASH problem

We ﬁrst review the formalization of AutoML as a Combined Algorithm Selection and Hyperparameter
optimization (CASH) problem used by AUTO-WEKA’s AutoML approach. Two important problems
in AutoML are that (1) no single machine learning method performs best on all datasets and (2) some
machine learning methods (e.g.  non-linear SVMs) crucially rely on hyperparameter optimization.
The latter problem has been successfully attacked using Bayesian optimization [3]  which nowadays
forms a core component of an AutoML system. The former problem is intertwined with the latter since
the rankings of algorithms depend on whether their hyperparameters are tuned properly. Fortunately 
the two problems can efﬁciently be tackled as a single  structured  joint optimization problem:
Deﬁnition 2 (CASH). Let A = {A(1)  . . .   A(R)} be a set of algorithms  and let the hyperparameters
of each algorithm A(j) have domain Λ(j). Further  let Dtrain = {(x1  y1)  . . .   (xn  yn)} be a train-
ing set which is split into K cross-validation folds {D(1)
train}
such that D(i)
valid) denote the
loss that algorithm A(j) achieves on D(i)
train with hyperparameters λ. Then 
the Combined Algorithm Selection and Hyperparameter optimization (CASH) problem is to ﬁnd the
joint algorithm and hyperparameter setting that minimizes this loss:

valid  . . .   D(K)
valid for i = 1  . . .   K. Finally  let L(A(j)

train = Dtrain\D(i)

valid} and {D(1)
train  D(i)

λ   D(i)

valid when trained on D(i)

train  . . .   D(K)

K(cid:88)

i=1

A(cid:63)  λ(cid:63) ∈

argmin

A(j)∈A λ∈Λ(j)

1
K

L(A(j)

λ   D(i)

train  D(i)

valid).

(1)

This CASH problem was ﬁrst tackled by Thornton et al. [2] in the AUTO-WEKA system using the
machine learning framework WEKA [8] and tree-based Bayesian optimization methods [9  10]. In
a nutshell  Bayesian optimization [3] ﬁts a probabilistic model to capture the relationship between
hyperparameter settings and their measured performance; it then uses this model to select the most
promising hyperparameter setting (trading off exploration of new parts of the space vs. exploitation
in known good regions)  evaluates that hyperparameter setting  updates the model with the result 
and iterates. While Bayesian optimization based on Gaussian process models (e.g.  Snoek et al. [11])
performs best in low-dimensional problems with numerical hyperparameters  tree-based models have
been shown to be more successful in high-dimensional  structured  and partly discrete problems [12] –
such as the CASH problem – and are also used in the AutoML system HYPEROPT-SKLEARN [13].
Among the tree-based Bayesian optimization methods  Thornton et al. [2] found the random-forest-
based SMAC [9] to outperform the tree Parzen estimator TPE [10]  and we therefore use SMAC
to solve the CASH problem in this paper. Next to its use of random forests [14]  SMAC’s main
distinguishing feature is that it allows fast cross-validation by evaluating one fold at a time and
discarding poorly-performing hyperparameter settings early.

2

Figure 1: Our improved AutoML approach. We add two components to Bayesian hyperparameter optimization
of an ML framework: meta-learning for initializing the Bayesian optimizer and automated ensemble construction
from conﬁgurations evaluated during optimization.

3 New methods for increasing efﬁciency and robustness of AutoML

We now discuss our two improvements of the AutoML approach. First  we include a meta-learning
step to warmstart the Bayesian optimization procedure  which results in a considerable boost in
efﬁciency. Second  we include an automated ensemble construction step  allowing us to use all
classiﬁers that were found by Bayesian optimization.
Figure 1 summarizes the overall AutoML workﬂow  including both of our improvements. We note
that we expect their effectiveness to be greater for ﬂexible ML frameworks that offer many degrees of
freedom (e.g.  many algorithms  hyperparameters  and preprocessing methods).

3.1 Meta-learning for ﬁnding good instantiations of machine learning frameworks

Domain experts derive knowledge from previous tasks: They learn about the performance of machine
learning algorithms. The area of meta-learning [15] mimics this strategy by reasoning about the
performance of learning algorithms across datasets. In this work  we apply meta-learning to select
instantiations of our given machine learning framework that are likely to perform well on a new
dataset. More speciﬁcally  for a large number of datasets  we collect both performance data and a set
of meta-features  i.e.  characteristics of the dataset that can be computed efﬁciently and that help to
determine which algorithm to use on a new dataset.
This meta-learning approach is complementary to Bayesian optimization for optimizing an ML
framework. Meta-learning can quickly suggest some instantiations of the ML framework that are
likely to perform quite well  but it is unable to provide ﬁne-grained information on performance.
In contrast  Bayesian optimization is slow to start for hyperparameter spaces as large as those of
entire ML frameworks  but can ﬁne-tune performance over time. We exploit this complementarity by
selecting k conﬁgurations based on meta-learning and use their result to seed Bayesian optimization.
This approach of warmstarting optimization by meta-learning has already been successfully applied
before [4  5  6]  but never to an optimization problem as complex as that of searching the space
of instantiations of a full-ﬂedged ML framework. Likewise  learning across datasets has also
been applied in collaborative Bayesian optimization methods [16  17]; while these approaches are
promising  they are so far limited to very few meta-features and cannot yet cope with the high-
dimensional partially discrete conﬁguration spaces faced in AutoML.
More precisely  our meta-learning approach works as follows. In an ofﬂine phase  for each machine
learning dataset in a dataset repository (in our case 140 datasets from the OpenML [18] repository) 
we evaluated a set of meta-features (described below) and used Bayesian optimization to determine
and store an instantiation of the given ML framework with strong empirical performance for that
dataset. (In detail  we ran SMAC [9] for 24 hours with 10-fold cross-validation on two thirds of the
data and stored the resulting ML framework instantiation which exhibited best performance on the
remaining third). Then  given a new dataset D  we compute its meta-features  rank all datasets by
their L1 distance to D in meta-feature space and select the stored ML framework instantiations for
the k = 25 nearest datasets for evaluation before starting Bayesian optimization with their results.
To characterize datasets  we implemented a total of 38 meta-features from the literature  including
simple  information-theoretic and statistical meta-features [19  20]  such as statistics about the number
of data points  features  and classes  as well as data skewness  and the entropy of the targets. All
meta-features are listed in Table 1 of the supplementary material. Notably  we had to exclude the
prominent and effective category of landmarking meta-features [21] (which measure the performance
of simple base learners)  because they were computationally too expensive to be helpful in the online
evaluation phase. We note that this meta-learning approach draws its power from the availability of

3

AutoMLsystemMLframework{Xtrain Ytrain Xtest b L}meta-learningdatapre-processorfeaturepreprocessorclassiﬁerbuildensembleˆYtestBayesianoptimizera repository of datasets; due to recent initiatives  such as OpenML [18]  we expect the number of
available datasets to grow ever larger over time  increasing the importance of meta-learning.

3.2 Automated ensemble construction of models evaluated during optimization

While Bayesian hyperparameter optimization is data-efﬁcient in ﬁnding the best-performing hyperpa-
rameter setting  we note that it is a very wasteful procedure when the goal is simply to make good
predictions: all the models it trains during the course of the search are lost  usually including some
that perform almost as well as the best. Rather than discarding these models  we propose to store them
and to use an efﬁcient post-processing method (which can be run in a second process on-the-ﬂy) to
construct an ensemble out of them. This automatic ensemble construction avoids to commit itself to a
single hyperparameter setting and is thus more robust (and less prone to overﬁtting) than using the
point estimate that standard hyperparameter optimization yields. To our best knowledge  we are the
ﬁrst to make this simple observation  which can be applied to improve any Bayesian hyperparameter
optimization method.
It is well known that ensembles often outperform individual models [22  23]  and that effective
ensembles can be created from a library of models [24  25]. Ensembles perform particularly well if
the models they are based on (1) are individually strong and (2) make uncorrelated errors [14]. Since
this is much more likely when the individual models are different in nature  ensemble building is
particularly well suited for combining strong instantiations of a ﬂexible ML framework.
However  simply building a uniformly weighted ensemble of the models found by Bayesian optimiza-
tion does not work well. Rather  we found it crucial to adjust these weights using the predictions of
all individual models on a hold-out set. We experimented with different approaches to optimize these
weights: stacking [26]  gradient-free numerical optimization  and the method ensemble selection [24].
While we found both numerical optimization and stacking to overﬁt to the validation set and to be
computationally costly  ensemble selection was fast and robust. In a nutshell  ensemble selection
(introduced by Caruana et al. [24]) is a greedy procedure that starts from an empty ensemble and then
iteratively adds the model that maximizes ensemble validation performance (with uniform weight 
but allowing for repetitions). Procedure 1 in the supplementary material describes it in detail. We
used this technique in all our experiments – building an ensemble of size 50.

4 A practical automated machine learning system

To design a robust AutoML system  as
our underlying ML framework we chose
scikit-learn [7]  one of the best known
and most widely used machine learning
libraries. It offers a wide range of well es-
tablished and efﬁciently-implemented ML
algorithms and is easy to use for both ex-
perts and beginners. Since our AutoML
system closely resembles AUTO-WEKA 
but – like HYPEROPT-SKLEARN – is based
on scikit-learn  we dub it AUTO-SKLEARN.
Figure 2 depicts AUTO-SKLEARN’s overall
components. It comprises 15 classiﬁcation
algorithms  14 preprocessing methods  and
4 data preprocessing methods. We param-
eterized each of them  which resulted in a
space of 110 hyperparameters. Most of these are conditional hyperparameters that are only active
if their respective component is selected. We note that SMAC [9] can handle this conditionality
natively.
All 15 classiﬁcation algorithms in AUTO-SKLEARN are listed in Table 1a (and described in detail in
Section A.1 of the supplementary material). They fall into different categories  such as general linear
models (2 algorithms)  support vector machines (2)  discriminant analysis (2)  nearest neighbors
(1)  na¨ıve Bayes (3)  decision trees (1) and ensembles (4). In contrast to AUTO-WEKA [2]  we

Figure 2: Structured conﬁguration space. Squared boxes
denote parent hyperparameters whereas boxes with rounded
edges are leaf hyperparameters. Grey colored boxes mark
active hyperparameters which form an example conﬁguration
and machine learning pipeline. Each pipeline comprises one
feature preprocessor  classiﬁer and up to three data prepro-
cessor methods plus respective hyperparameters.

4

datapreprocessorestimatorfeaturepreprocessorclassiﬁerAdaBoost···RFkNN#estimatorslearningratemax.depthpreprocessing···NonePCAfastICArescaling···min/maxstandardonehotenc.···imputationmean···medianbalancingweightingNone#λ
name
4
AdaBoost (AB)
Bernoulli na¨ıve Bayes
2
4
decision tree (DT)
5
extreml. rand. trees
Gaussian na¨ıve Bayes
-
6
gradient boosting (GB)
3
kNN
4
LDA
4
linear SVM
7
kernel SVM
multinomial na¨ıve Bayes 2
3
passive aggressive
2
QDA
5
random forest (RF)
Linear Class. (SGD)
10

cat (cond)

cont (cond)

1 (-)
1 (-)
1 (-)
2 (-)

-
-

2 (-)
1 (-)
2 (-)
2 (-)
1 (-)
1 (-)

-

2 (-)
4 (-)

3 (-)
1 (-)
3 (-)
3 (-)

-

6 (-)
1 (-)
3 (1)
2 (-)
5 (2)
1 (-)
2 (-)
2 (-)
3 (-)
6 (3)

(a) classiﬁcation algorithms

name
#λ
extreml. rand. trees prepr. 5
4
fast ICA
4
feature agglomeration
5
kernel PCA
rand. kitchen sinks
2
3
linear SVM prepr.
-
no preprocessing
5
nystroem sampler
2
PCA
3
polynomial
4
random trees embed.
select percentile
2
3
select rates

cat (cond)

cont (cond)

2 (-)
3 (-)
3 ()
1 (-)

1 (-)
2 (-)

1 (-)

1 (-)
1 (-)
2 (-)

-

-

-

3 (-)
1 (1)
1 (-)
4 (3)
2 (-)
2 (-)

-

4 (3)
1 (-)
1 (-)
4 (-)
1 (-)
1 (-)

one-hot encoding
imputation
balancing
rescaling

2
1
1
1

1 (-)
1 (-)
1 (-)
1 (-)

1 (1)

-
-
-

(b) preprocessing methods

Table 1: Number of hyperparameters for each possible classiﬁer (left) and feature preprocessing method
(right) for a binary classiﬁcation dataset in dense representation. Tables for sparse binary classiﬁcation and
sparse/dense multiclass classiﬁcation datasets can be found in the Section E of the supplementary material 
Tables 2a  3a  4a  2b  3b and 4b. We distinguish between categorical (cat) hyperparameters with discrete values
and continuous (cont) numerical hyperparameters. Numbers in brackets are conditional hyperparameters  which
are only relevant when another parameter has a certain value.

focused our conﬁguration space on base classiﬁers and excluded meta-models and ensembles that
are themselves parameterized by one or more base classiﬁers. While such ensembles increased
AUTO-WEKA’s number of hyperparameters by almost a factor of ﬁve (to 786)  AUTO-SKLEARN
“only” features 110 hyperparameters. We instead construct complex ensembles using our post-hoc
method from Section 3.2. Compared to AUTO-WEKA  this is much more data-efﬁcient: in AUTO-
WEKA  evaluating the performance of an ensemble with 5 components requires the construction and
evaluation of 5 models; in contrast  in AUTO-SKLEARN  ensembles come largely for free  and it is
possible to mix and match models evaluated at arbitrary times during the optimization.
The preprocessing methods for datasets in dense representation in AUTO-SKLEARN are listed in
Table 1b (and described in detail in Section A.2 of the supplementary material). They comprise data
preprocessors (which change the feature values and are always used when they apply) and feature
preprocessors (which change the actual set of features  and only one of which [or none] is used). Data
preprocessing includes rescaling of the inputs  imputation of missing values  one-hot encoding and
balancing of the target classes. The 14 possible feature preprocessing methods can be categorized into
feature selection (2)  kernel approximation (2)  matrix decomposition (3)  embeddings (1)  feature
clustering (1)  polynomial feature expansion (1) and methods that use a classiﬁer for feature selection
(2). For example  L1-regularized linear SVMs ﬁtted to the data can be used for feature selection by
eliminating features corresponding to zero-valued model coefﬁcients.
As with every robust real-world system  we had to handle many more important details in AUTO-
SKLEARN; we describe these in Section B of the supplementary material.

5 Comparing AUTO-SKLEARN to AUTO-WEKA and HYPEROPT-SKLEARN

As a baseline experiment  we compared the performance of vanilla AUTO-SKLEARN (without our
improvements) to AUTO-WEKA and HYPEROPT-SKLEARN  reproducing the experimental setup
with 21 datasets of the paper introducing AUTO-WEKA [2]. We describe this setup in detail in
Section G in the supplementary material.
Table 2 shows that AUTO-SKLEARN performed statistically signiﬁcantly better than AUTO-WEKA
in 6/21 cases  tied it in 12 cases  and lost against it in 3. For the three datasets where AUTO-
WEKA performed best  we found that in more than 50% of its runs the best classiﬁer it chose is not
implemented in scikit-learn (trees with a pruning component). So far  HYPEROPT-SKLEARN is more
of a proof-of-concept – inviting the user to adapt the conﬁguration space to her own needs – than
a full AutoML system. The current version crashes when presented with sparse data and missing
values. It also crashes on Cifar-10 due to a memory limit which we set for all optimizers to enable a

5

e
n
o
l
a
b
A

n
o
z
a
m
A

0
1
-
r
a
f
i

C

r
a
C

0
1
-
r
a
f
i

C

l
l
a
m
S

x
e
v
n
o
C

r
e
t
x
e
D

a
e
h
t
o
r
o
D

n
a
m
r
e
G

t
i
d
e
r
C

e
t
t
e
s
i
G

73.50 16.00 0.39
AS
AW 73.50 30.00 0.00
HS
76.21 16.22 0.39

51.70 54.81 17.53 5.56
56.95 56.20 21.80 8.33
-

57.95 19.18 -

5.51
6.38
-

27.00 1.62
28.33 2.29
27.67 2.29

9
0
D
D
K

y
c
n
e
t
e
p
p
A
1.74
1.74
-

P
K
-
s
v
-
R
K

n
o
l
e
d
a
M

T
S
I
N
M

c
i
s
a
B

I

B
R
M

m
o
c
e
S

n
o
i
e
m
e
S

e
l
t
t
u
h
S

m
r
o
f
e
v
a
W

y
t
i
l
a
u
Q

e
n
i
W

t
s
a
e
Y

0.42
0.31
0.42

12.44 2.84
18.21 2.84
14.74 2.82

46.92 7.87
60.34 8.09
55.79 -

5.24
5.24
5.87

0.01
0.01
0.05

14.93 33.76 40.67
14.13 33.36 37.75
14.07 34.72 38.45

Table 2: Test set classiﬁcation error of AUTO-WEKA (AW)  vanilla AUTO-SKLEARN (AS) and HYPEROPT-
SKLEARN (HS)  as in the original evaluation of AUTO-WEKA [2]. We show median percent error across
100 000 bootstrap samples (based on 10 runs)  simulating 4 parallel runs. Bold numbers indicate the best result.
Underlined results are not statistically signiﬁcantly different from the best according to a bootstrap test with
p = 0.05.

Figure 3: Average rank of all four AUTO-SKLEARN variants (ranked by balanced test error rate (BER)) across
140 datasets. Note that ranks are a relative measure of performance (here  the rank of all methods has to add up
to 10)  and hence an improvement in BER of one method can worsen the rank of another. The supplementary
material shows the same plot on a log-scale to show the time overhead of meta-feature and ensemble computation.

fair comparison. On the 16 datasets on which it ran  it statistically tied the best optimizer in 9 cases
and lost against it in 7.

6 Evaluation of the proposed AutoML improvements

In order to evaluate the robustness and general applicability of our proposed AutoML system on
a broad range of datasets  we gathered 140 binary and multiclass classiﬁcation datasets from the
OpenML repository [18]  only selecting datasets with at least 1000 data points to allow robust
performance evaluations. These datasets cover a diverse range of applications  such as text classiﬁ-
cation  digit and letter recognition  gene sequence and RNA classiﬁcation  advertisement  particle
classiﬁcation for telescope data  and cancer detection in tissue samples. We list all datasets in Table 7
and 8 in the supplementary material and provide their unique OpenML identiﬁers for reproducibility.
Since the class distribution in many of these datasets is quite imbalanced we evaluated all AutoML
methods using a measure called balanced classiﬁcation error rate (BER). We deﬁne balanced error
rate as the average of the proportion of wrong classiﬁcations in each class. In comparison to standard
classiﬁcation error (the average overall error)  this measure (the average of the class-wise error)
assigns equal weight to all classes. We note that balanced error or accuracy measures are often used
in machine learning competitions (e.g.  the AutoML challenge [1] uses balanced accuracy).
We performed 10 runs of AUTO-SKLEARN both with and without meta-learning and with and without
ensemble prediction on each of the datasets. To study their performance under rigid time constraints 
and also due to computational resource constraints  we limited the CPU time for each run to 1 hour; we
also limited the runtime for a single model to a tenth of this (6 minutes). To not evaluate performance
on data sets already used for meta-learning  we performed a leave-one-dataset-out validation: when
evaluating on dataset D  we only used meta-information from the 139 other datasets.
Figure 3 shows the average ranks over time of the four AUTO-SKLEARN versions we tested. We
observe that both of our new methods yielded substantial improvements over vanilla AUTO-SKLEARN.
The most striking result is that meta-learning yielded drastic improvements starting with the ﬁrst

6

500100015002000250030003500time [sec]1.82.02.22.42.62.83.0average rankvanilla auto-sklearnauto-sklearn + ensembleauto-sklearn + meta-learningauto-sklearn + meta-learning + ensembleD

I

t
e
s
a
t
a
d

L
M
n
e
p
O

38
46
179
184
554
772
917
1049
1111
1120
1128
293
389

N
R
A
E
L
K
S

-
O
T
U
A

2.15
3.76
16.99
10.32
1.55
46.85
10.22
12.93
23.70
13.81
4.21
2.86
19.65

t
s
o
o
B
a
d
A

2.68
4.65
17.03
10.52
2.42
49.68
9.11
12.53
23.16
13.54
4.89
4.07
22.98

s
e
y
a
B
e
v
¨ı
a
n

i
l
l
u
o
n
r
e
B

50.22
-
19.27
-
-
47.90
25.83
15.50
28.40
18.81
4.71
24.30
-

n
o
i
s
i
c
e
d

e
e
r
t

2.15
5.62
18.31
17.46
12.00
47.75
11.00
19.31
24.40
17.45
9.30
5.03
33.14

s
e
e
r
t

.
d
n
a
r

.
l

m
e
r
t
x
e

18.06
4.74
17.09
11.10
2.91
45.62
10.22
17.18
24.47
13.86
3.89
3.59
19.38

s
e
y
a
B
e
v
¨ı
a
n

n
a
i
s
s
u
a
G

11.22
7.88
21.77
64.74
10.52
48.83
33.94
26.23
29.59
21.50
4.77
32.44
29.18

t
n
e
i
d
a
r
g

g
n
i
t
s
o
o
b

1.77
3.49
17.00
10.42
3.86
48.15
10.11
13.38
22.93
13.61
4.58
24.48
19.20

N
N
k

A
D
L

50.00
7.57
22.23
31.10
2.68
48.00
11.11
23.80
50.30
17.23
4.59
4.86
30.87

8.55
8.67
18.93
35.44
3.34
46.74
34.22
25.12
24.11
15.48
4.58
24.40
19.68

M
V
S
r
a
e
n
i
l

16.29
8.31
17.30
15.76
2.23
48.38
18.67
17.28
23.99
14.94
4.83
14.16
17.95

M
V
S

l
e
n
r
e
k

17.89
5.36
17.57
12.52
1.50
48.66
6.78
21.44
23.56
14.17
4.59
100.00
22.04

l
a
i
m
o
n
i
t
l
u
m

s
e
y
a
B
e
v
¨ı
a
n

46.99
7.55
18.97
27.13
10.37
47.21
25.50
26.40
27.67
18.33
4.46
24.20
20.04

e
v
i
s
e
r
g
g
a

e
v
i
s
s
a
p

50.00
9.23
22.29
20.01
100.00
48.75
20.67
29.25
43.79
16.37
5.65
21.34
20.14

A
D
Q

8.78
7.57
19.06
47.18
2.75
47.67
30.44
21.38
25.86
15.62
5.59
28.68
39.57

t
s
e
r
o
f

m
o
d
n
a
r

2.34
4.20
17.24
10.98
3.08
47.71
10.83
13.75
28.06
13.70
3.83
2.57
20.66

.
s
s
a
l
C

r
a
e
n
i
L

)

D
G
S
(

15.82
7.31
17.01
12.76
2.50
47.93
18.33
19.92
23.36
14.66
4.33
15.54
17.99

Table 3: Median balanced test error rate (BER) of optimizing AUTO-SKLEARN subspaces for each classiﬁcation
method (and all preprocessors)  as well as the whole conﬁguration space of AUTO-SKLEARN  on 13 datasets.
All optimization runs were allowed to run for 24 hours except for AUTO-SKLEARN which ran for 48 hours.
Bold numbers indicate the best result; underlined results are not statistically signiﬁcantly different from the best
according to a bootstrap test using the same setup as for Table 2.

D

I

t
e
s
a
t
a
d

L
M
n
e
p
O

38
46
179
184
554
772
917
1049
1111
1120
1128
293
389

N
R
A
E
L
K
S

-
O
T
U
A

2.15
3.76
16.99
10.32
1.55
46.85
10.22
12.93
23.70
13.81
4.21
2.86
19.65

r
e
ﬁ

i
s
n
e
d

-
-
-
-
-
-
-
-
-
-
-
24.40
20.63

.

d
n
a
r

.
l

m
e
r
t
x
e

.
r
p
e
r
p
s
e
e
r
t

4.03
4.98
17.83
55.78
1.56
47.90
8.33
20.36
23.36
16.29
4.90
3.41
21.40

n
o
i
t
a
r
e
m
o
l
g
g
a

e
r
u
t
a
e
f

2.24
4.40
16.92
11.31
1.65
48.62
10.33
13.14
23.73
13.73
4.76
-
-

A
C

I

t
s
a
f

7.27
7.95
17.24
19.96
2.52
48.65
16.06
19.92
24.69
14.22
4.96
-
-

A
C
P

l
e
n
r
e
k

5.84
8.74
100.00
36.52
100.00
47.59
20.94
19.57
100.00
14.57
4.21
100.00
17.50

s
k
n
i
s
n
e
h
c
t
i
k

.

d
n
a
r

8.57
8.41
17.34
28.05
100.00
47.68
35.44
20.06
25.25
14.82
5.08
19.30
19.66

.
r
p
e
r
p
M
V
S

r
a
e
n
i
l

2.28
4.25
16.84
9.92
2.21
47.72
8.67
13.28
23.43
14.02
4.52
3.01
19.89

.
c
o
r
p
e
r
p

o
n

2.28
4.52
16.97
11.43
1.60
48.34
9.44
15.84
22.27
13.85
4.59
2.66
20.87

m
e
o
r
t
s
y
n

r
e
l
p
m
a
s

7.70
8.48
17.30
25.53
2.21
48.06
37.83
18.96
23.95
14.66
4.08
20.94
18.46

l
a
i
m
o
n
y
l
o
p

2.90
4.21
16.94
10.54
100.00
48.00
9.11
12.95
26.94
13.22
50.00
-
-

A
C
P

7.23
8.40
17.64
21.15
1.65
47.30
22.33
17.22
23.25
14.23
4.59
-
-

e
l
i
t
n
e
c
r
e
p
t
c
e
l
e
s

n
o
i
t
a
c
ﬁ

i
s
s
a
l
c

.

d
e
b
m
e

s
e
e
r
t

m
o
d
n
a
r

18.50
7.51
17.05
12.68
3.48
47.84
17.67
18.52
26.68
15.03
9.23
8.05
44.83

2.20
4.17
17.09
45.03
1.46
47.56
10.00
11.94
23.53
13.65
4.33
2.86
20.17

D
V
S
d
e
t
a
c
n
u
r
t

-
-
-
-
-
-
-
-
-
-
-
4.05
21.58

s
e
t
a
r

t
c
e
l
e
s

2.28
4.68
16.86
10.47
1.70
48.43
10.44
14.38
23.33
13.67
4.08
2.74
19.18

Table 4: Like Table 3  but instead optimizing subspaces for each preprocessing method (and all classiﬁers).

conﬁguration it selected and lasting until the end of the experiment. We note that the improvement
was most pronounced in the beginning and that over time  vanilla AUTO-SKLEARN also found good
solutions without meta-learning  letting it catch up on some datasets (thus improving its overall rank).
Moreover  both of our methods complement each other: our automated ensemble construction
improved both vanilla AUTO-SKLEARN and AUTO-SKLEARN with meta-learning. Interestingly  the
ensemble’s inﬂuence on the performance started earlier for the meta-learning version. We believe
that this is because meta-learning produces better machine learning models earlier  which can be
directly combined into a strong ensemble; but when run longer  vanilla AUTO-SKLEARN without
meta-learning also beneﬁts from automated ensemble construction.

7 Detailed analysis of AUTO-SKLEARN components

We now study AUTO-SKLEARN’s individual classiﬁers and preprocessors  compared to jointly
optimizing all methods  in order to obtain insights into their peak performance and robustness. Ideally 
we would have liked to study all combinations of a single classiﬁer and a single preprocessor in
isolation  but with 15 classiﬁers and 14 preprocessors this was infeasible; rather  when studying the
performance of a single classiﬁer  we still optimized over all preprocessors  and vice versa. To obtain
a more detailed analysis  we focused on a subset of datasets but extended the conﬁguration budget for
optimizing all methods from one hour to one day and to two days for AUTO-SKLEARN. Speciﬁcally 
we clustered our 140 datasets with g-means [27] based on the dataset meta-features and used one
dataset from each of the resulting 13 clusters (see Table 6 in the supplementary material for the list of
datasets). We note that  in total  these extensive experiments required 10.7 CPU years.
Table 3 compares the results of the various classiﬁcation methods against AUTO-SKLEARN. Overall 
as expected  random forests  extremely randomized trees  AdaBoost  and gradient boosting  showed

7

(a) MNIST (OpenML dataset ID 554)

(b) Promise pc4 (OpenML dataset ID 1049)

Figure 4: Performance of a subset of classiﬁers compared to AUTO-SKLEARN over time. We show median test
error rate and the ﬁfth and 95th percentile over time for optimizing three classiﬁers separately with optimizing
the joint space. A plot with all classiﬁers can be found in Figure 4 in the supplementary material. While
AUTO-SKLEARN is inferior in the beginning  in the end its performance is close to the best method.

the most robust performance  and SVMs showed strong peak performance for some datasets. Besides
a variety of strong classiﬁers  there are also several models which could not compete: The decision
tree  passive aggressive  kNN  Gaussian NB  LDA and QDA were statistically signiﬁcantly inferior
to the best classiﬁer on most datasets. Finally  the table indicates that no single method was the best
choice for all datasets. As shown in the table and also visualized for two example datasets in Figure
4  optimizing the joint conﬁguration space of AUTO-SKLEARN led to the most robust performance.
A plot of ranks over time (Figure 2 and 3 in the supplementary material) quantiﬁes this across all
13 datasets  showing that AUTO-SKLEARN starts with reasonable but not optimal performance and
effectively searches its more general conﬁguration space to converge to the best overall performance
over time.
Table 4 compares the results of the various preprocessors against AUTO-SKLEARN. As for the
comparison of classiﬁers above  AUTO-SKLEARN showed the most robust performance: It performed
best on three of the datasets and was not statistically signiﬁcantly worse than the best preprocessor on
another 8 of 13.

8 Discussion and Conclusion

We demonstrated that our new AutoML system AUTO-SKLEARN performs favorably against the
previous state of the art in AutoML  and that our meta-learning and ensemble improvements for
AutoML yield further efﬁciency and robustness. This ﬁnding is backed by the fact that AUTO-
SKLEARN won the auto-track in the ﬁrst phase of ChaLearn’s ongoing AutoML challenge. In this
paper  we did not evaluate the use of AUTO-SKLEARN for interactive machine learning with an expert
in the loop and weeks of CPU power  but we note that that mode has also led to a third place in
the human track of the same challenge. As such  we believe that AUTO-SKLEARN is a promising
system for use by both machine learning novices and experts. The source code of AUTO-SKLEARN is
available under an open source license at https://github.com/automl/auto-sklearn.
Our system also has some shortcomings  which we would like to remove in future work. As one
example  we have not yet tackled regression or semi-supervised problems. Most importantly  though 
the focus on scikit-learn implied a focus on small to medium-sized datasets  and an obvious direction
for future work will be to apply our methods to modern deep learning systems that yield state-of-
the-art performance on large datasets; we expect that in that domain especially automated ensemble
construction will lead to tangible performance improvements over Bayesian optimization.

Acknowledgments

This work was supported by the German Research Foundation (DFG)  under Priority Programme Autonomous
Learning (SPP 1527  grant HU 1900/3-1)  under Emmy Noether grant HU 1900/2-1  and under the BrainLinks-
BrainTools Cluster of Excellence (grant number EXC 1086).

8

101102103104time [sec]0246810Balanced Error Rateauto-sklearngradient boostingkernel SVMrandom forest101102103104time [sec]1520253035404550Balanced Error Rateauto-sklearngradient boostingkernel SVMrandom forestReferences

[1] I. Guyon  K. Bennett  G. Cawley  H. Escalante  S. Escalera  T. Ho  N.Maci`a  B. Ray  M. Saeed  A. Statnikov 

and E. Viegas. Design of the 2015 ChaLearn AutoML Challenge. In Proc. of IJCNN’15  2015.

[2] C. Thornton  F. Hutter  H. Hoos  and K. Leyton-Brown. Auto-WEKA: combined selection and hyperpa-

rameter optimization of classiﬁcation algorithms. In Proc. of KDD’13  pages 847–855  2013.

[3] E. Brochu  V. Cora  and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions 
with application to active user modeling and hierarchical reinforcement learning. CoRR  abs/1012.2599 
2010.

[4] M. Feurer  J. Springenberg  and F. Hutter. Initializing Bayesian hyperparameter optimization via meta-

learning. In Proc. of AAAI’15  pages 1128–1135  2015.

[5] Reif M  F. Shafait  and A. Dengel. Meta-learning for evolutionary parameter optimization of classiﬁers.

Machine Learning  87:357–380  2012.

[6] T. Gomes  R. Prudˆencio  C. Soares  A. Rossi  and A. Carvalho. Combining meta-learning and search

techniques to select parameters for support vector machines. Neurocomputing  75(1):3–13  2012.

[7] F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel  P. Prettenhofer 
R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher  M. Perrot  and E. Duchesnay.
Scikit-learn: Machine learning in Python. JMLR  12:2825–2830  2011.

[8] M. Hall  E. Frank  G. Holmes  B. Pfahringer  P. Reutemann  and I. Witten. The WEKA data mining

software: An update. SIGKDD  11(1):10–18  2009.

[9] F. Hutter  H. Hoos  and K. Leyton-Brown. Sequential model-based optimization for general algorithm

conﬁguration. In Proc. of LION’11  pages 507–523  2011.

[10] J. Bergstra  R. Bardenet  Y. Bengio  and B. K´egl. Algorithms for hyper-parameter optimization. In Proc.

of NIPS’11  pages 2546–2554  2011.

[11] J. Snoek  H. Larochelle  and R. P. Adams. Practical Bayesian optimization of machine learning algorithms.

In Proc. of NIPS’12  pages 2960–2968  2012.

[12] K. Eggensperger  M. Feurer  F. Hutter  J. Bergstra  J. Snoek  H. Hoos  and K. Leyton-Brown. Towards
an empirical foundation for assessing Bayesian optimization of hyperparameters. In NIPS Workshop on
Bayesian Optimization in Theory and Practice  2013.

[13] B. Komer  J. Bergstra  and C. Eliasmith. Hyperopt-sklearn: Automatic hyperparameter conﬁguration for

scikit-learn. In ICML workshop on AutoML  2014.
[14] L. Breiman. Random forests. MLJ  45:5–32  2001.
[15] P. Brazdil  C. Giraud-Carrier  C. Soares  and R. Vilalta. Metalearning: Applications to Data Mining.

Springer  2009.

[16] R. Bardenet  M. Brendel  B. K´egl  and M. Sebag. Collaborative hyperparameter tuning. In Proc. of

ICML’13 [28]  pages 199–207.

[17] D. Yogatama and G. Mann. Efﬁcient transfer learning method for automatic hyperparameter tuning. In

Proc. of AISTATS’14  pages 1077–1085  2014.

[18] J. Vanschoren  J. van Rijn  B. Bischl  and L. Torgo. OpenML: Networked science in machine learning.

SIGKDD Explorations  15(2):49–60  2013.

[19] D. Michie  D. Spiegelhalter  C. Taylor  and J. Campbell. Machine Learning  Neural and Statistical

Classiﬁcation. Ellis Horwood  1994.

[20] A. Kalousis. Algorithm Selection via Meta-Learning. PhD thesis  University of Geneve  2002.
[21] B. Pfahringer  H. Bensusan  and C. Giraud-Carrier. Meta-learning by landmarking various learning

algorithms. In Proc. of (ICML’00)  pages 743–750  2000.

[22] I. Guyon  A. Saffari  G. Dror  and G. Cawley. Model selection: Beyond the Bayesian/Frequentist divide.

JMLR  11:61–87  2010.

[23] A. Lacoste  M. Marchand  F. Laviolette  and H. Larochelle. Agnostic Bayesian learning of ensembles. In

Proc. of ICML’14  pages 611–619  2014.

[24] R. Caruana  A. Niculescu-Mizil  G. Crew  and A. Ksikes. Ensemble selection from libraries of models. In

Proc. of ICML’04  page 18  2004.

[25] R. Caruana  A. Munson  and A. Niculescu-Mizil. Getting the most out of ensemble selection. In Proc. of

ICDM’06  pages 828–833  2006.

[26] D. Wolpert. Stacked generalization. Neural Networks  5:241–259  1992.
[27] G. Hamerly and C. Elkan. Learning the k in k-means. In Proc. of NIPS’04  pages 281–288  2004.
[28] Proc. of ICML’13  2014.

9

,Matthias Feurer
Aaron Klein
Katharina Eggensperger
Jost Springenberg
Manuel Blum
Frank Hutter