2013,Reinforcement Learning in Robust Markov Decision Processes,An important challenge in Markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system. We consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic. We devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case.,Reinforcement Learning in Robust Markov Decision

Processes

Shiau Hong Lim

Huan Xu

Department of Mechanical Engineering

National University of Singapore

Department of Mechanical Engineering

National University of Singapore

Singapore

mpelsh@nus.edu.sg

Singapore

mpexuh@nus.edu.sg

Department of Electrical Engineering

Shie Mannor

Technion  Israel

shie@ee.technion.ac.il

Abstract

An important challenge in Markov decision processes is to ensure robustness with
respect to unexpected or adversarial system behavior while taking advantage of
well-behaving parts of the system. We consider a problem setting where some
unknown parts of the state space can have arbitrary transitions while other parts
are purely stochastic. We devise an algorithm that is adaptive to potentially ad-
versarial behavior and show that it achieves similar regret bounds as the purely
stochastic case.

1

Introduction

Markov decision processes (MDPs) [Puterman  1994] have been widely used to model and solve
sequential decision problems in stochastic environments. Given the parameters of an MDP  namely 
the rewards and transition probabilities  an optimal policy can be computed.
In practice  these
parameters are often estimated from noisy data and furthermore  they may change during the exe-
cution of a policy. Hence  the performance of the chosen policy may deteriorate signiﬁcantly; see
[Mannor et al.  2007] for numerical experiments.
The robust MDP framework has been proposed to address this issue of parameter uncertainty (e.g. 
[Nilim and El Ghaoui  2005] and [Iyengar  2005]). The robust MDP setting assumes that the true
parameters fall within some uncertainty set U and seeks a policy that performs the best under the
worst realization of the parameters. These solutions  however  can be overly conservative since they
are based on worst-case realization. Variants of robust MDP formulations have been proposed to
mitigate the conservativeness when additional information on parameter distribution [Strens  2000 
Xu and Mannor  2012] or coupling among the parameters [Mannor et al.  2012] are known. A major
drawback of previous work on robust MDPs is that they all focused on the planning problem with
no effort to learn the uncertainty. Since in practice it is often difﬁcult to accurately quantify the
uncertainty  the solutions to the robust MDP can be conservative if a too large uncertainty set is
used.
In this work  we make the ﬁrst attempt to perform learning in robust MDPs. We assume that some of
the state-action pairs are adversarial in the sense that their parameters can change arbitrarily within
U from one step to another. However  others are benign in the sense that they are ﬁxed and behave
purely stochastically. The learner  however  is given only the uncertainty set U and knows neither
the parameters nor the true nature of each state-action pair.

1

In this setting  a traditional robust MDP approach would be equivalent to assuming that all parame-
ters are adversarial and therefore would always execute the minimax policy. This is too conservative
since it could be the case that most of the parameters are stochastic. Alternatively  one could use an
existing online learning algorithm such as UCRL2 [Jaksch et al.  2010] and assume that all parame-
ters are stochastic. This  as we show in the next section  may lead to suboptimal performance when
some of the states are adversarial.
Instead  we propose an online learning approach to robust MDPs. We show that the cumulative
reward obtained from this method is as good as the minimax policy that knows the true nature of
each state-action pair. This means that by incorporating learning in robust MDPs  we can effectively
resolve the “conservativeness due to not knowing the uncertainty” effect.
The rest of the paper is structured as follows. Section 2 discusses the key difﬁculties in our setting
and explains why existing solutions are not applicable.
In subsequent sections  we present our
algorithm  its theoretical performance bound and its analysis. Sections 3 and 4 cover the ﬁnite-
horizon case while Section 5 deals with the inﬁnite-horizon case. We present some experiment
results in Section 6 and conclude in Section 7.

2 Problem setting

We consider an MDP M with a ﬁnite state space S and a ﬁnite action space A. Let S = |S| and
A = |A|. Executing action a in state s results in a random transition according to a distribution
ps a(·) where ps a(s(cid:48)) gives the probability of transitioning to state s(cid:48)  and accumulate an immediate
reward r(s  a).
A robust MDP considers the case where the transition probability is determined in an adversarial
way. That is  when action a is taken at state s  the transition probability ps a(·) can be an arbitrary
element of the uncertainty set U(s  a). In particular  for different visits of same (s  a)  the realization
of ps a can be different  possibly depends on the history. This can model cases where the system
dynamics are inﬂuenced by competitors or exogeneous factors that are hard to model  or the MDP
is a simpliﬁcation of a complicated dynamic system.
Previous research in robust MDPs focused exclusively on the planning problem. Here  the power of
the adversary – the uncertainty set of the parameter – is precisely known  and the goal is to ﬁnd the
minimax policy – the policy with the best performance under the worst admissible parameters.
This paper considers the learning problem of robust MDPs. We ask the following question: suppose
the power of the adversary (the extent to which it can affect the system) is not completely revealed
to the decision maker  if we are allowed to play the MDP many times  can we still obtain an optimal
policy as if we knew the true extent of its power? Or to put it that way  can we develop a procedure
that provides the exact amount of protection against the unknown adversary?
Our speciﬁc setup is as follows: for each (s  a) ∈ S×A an uncertainty set U(s  a) is given. However 
not all states are adversarial. Only a subset F ⊂ S ×A is truly adversarial while all the other state-
action pairs behave purely stochastically  i.e.  with a ﬁxed unknown ps a. Moreover  the set F is not
known to the algorithm.
This setting differs from existing setups  and is challenging for the following reasons:

1. The adversarial actions ps a are not directly observable.
2. The adversarial behavior is not constrained  except it must belong to the uncertainty set.
3. Ignoring the adversarial component results in sub-optimal behavior.

The ﬁrst challenge precludes the use of algorithm based on stochastic games such as R-Max
[Brafman and Tennenholtz  2002]. The R-Max algorithm deals with stochastic games where the
opponent’s action-set for each state is known and the opponent’s actions are always observable. In
our setting  only the outcome (i.e.  the next-state and the reward) of each transition is observable.
The algorithm does not observe the action ps a taken by the adversary. Indeed  because the set F is
unknown  even the action set of the adversary is unknown to the algorithm.
The second challenge is due to unconstrained adversarial behavior. For state-action pairs (s  a) ∈ F 
the opponent is free to choose any ps a ∈ U(s  a) for each transition  possibly depends on the his-

2

tory and the strategy of the decision maker (i.e.  non-oblivious). This affects the sort of performance
guarantee one can reasonably expect from any algorithms. In particular  when considering the regret
against the best stationary policy “in hindsight”  [Yu and Mannor  2009] show that small change in
transition probabilities can cause large regret. Even with additional constraints on the allowed ad-
versarial behavior  they showed that the regret bound still does not vanish with respect to the number
of steps. Indeed  most results for adversarial MDPs [Even-Dar et al.  2005  Even-Dar et al.  2009 
Yu et al.  2009  Neu et al.  2010  Neu et al.  2012] only deal with adversarial rewards while the tran-
sitions are assumed stochastic and ﬁxed  which is considerably simpler than our setting.
Since it is not possible to achieve vanishing regret against best stationary policy in hindsight  we
choose to measure the regret against the performance of a minimax policy that knows exactly which
state-actions are adversarial (i.e.  the set F) as well as the true ps a for all stochastic state-action
pairs. Intuitively  this means that if the adversary chooses to play “nicely”  we are not constrained
to exploit this.
Finally  given that we are competing against the minimax policy  one might ask whether we could
simply apply existing algorithms such as UCRL2 [Jaksch et al.  2010] and treat every state-action
pair as stochastic. The following example shows that ignoring any adversarial behavior may lead to
large regret compared to the minimax policy.

Figure 1: Example MDP with adversarial transitions.

Consider the MDP in Figure 1. Suppose that a UCRL2-like algorithm is used  where all transitions
are assumed purely stochastic. There are 3 alternative policies  each corresponds to choosing action
a1  a2 and a3 respectively in state s0. Action a1 leads to the optimal minimax average reward of
g∗. State s2 leads to average reward of g∗ + β for some β > 0. State s1 has adversarial transition 
where both s2 and s4 are possible next states. s4 has a similar behavior  where it may either lead to
g∗ + β or a “bad” region with average reward g∗
We consider two phases. In phase 1  the adversary behaves “benignly” by choosing all solid-line
transitions. Since both a2 and a3 lead to similar outcome  we assume that in phase 1  both a2 and a3
are chosen for T steps each. In phase 2  the adversary chooses the dashed-line transitions in both s1
and s4. Due to a2 and a3 having similar values (both g∗ + β > g∗) we can assume that a2 is always
chosen in phase 2 (if a3 is ever chosen in phase 2 its value will quickly drop below that of a2).
Suppose that a2 also runs for T steps in phase 2. A little algebra (see the supplementary material
for details) shows that at the end of phase 2 the expected value of s4 (from the learner’s point of
view) is g4 = g∗ + β−α
4 > g∗. The total
accumulated rewards over both phases is however 3T g∗ + T (2β − α). Let c = α − 2β > 0. This
means that the overall total regret is cT which is linear in T .
Note that in the above example  the expected value of a2 remains greater than the minimax value
g∗ throughout phase 2 and therefore the algorithm will continue to prefer a2  even though the actual
accumulated average value is already way below g∗. The reason behind this is that the Markov
property  which is crucial for UCRL2-like algorithms to work  has been violated due to s1 and s4
behaving in a non-independent way caused by the adversary.

and therefore the expected value of s1 is g1 = g∗ + 3β−α

− α for some 2β < α < 3β.

2

3 Algorithm and main result

In this section  we present our algorithm and the main result for the ﬁnite-horizon case with the total
reward as the performance measure. Section 5 provides the corresponding algorithm and result for
the inﬁnite-horizon average-reward case.

3

s0s1s3s2s4g∗g∗+βg∗−αg∗+βa1a2a3For simplicity  we assume without loss of generality a deterministic and known reward function
r(s  a). We also assume that rewards are bounded such that r(s  a) ∈ [0  1]. It is straight-forward 
by introducing additional states  to extend the algorithm and analysis to the case where the reward
function is random  unknown and even adversarial.
In the ﬁnite horizon case  we consider an episodic setting where each episode has a ﬁxed and known
length T . The algorithm starts at a (possibly random) state s0 and executes T stages. After that 
a new episode begins  with an arbitrarily chosen start state (it can simply be the last state of the
previous episode). This goes on indeﬁnitely.
Let π be a ﬁnite-horizon (non-stationary) policy where πt(s) gives the action to be executed in state
s at step t in an episode  where t = 0  . . .   (T − 1). Let Pt be a particular choice of ps a ∈ U(s  a)
for every (s  a) ∈ F at step t. For each t = 0  . . .   (T − 1)  we deﬁne

V π
t (s) = min

Pt ... PT −2

EPt ... PT −2

r(st(cid:48)  πt(cid:48)(st(cid:48)))

and

V ∗
t (s) = max

π

V π
t (s) 

T−1(cid:88)

t(cid:48)=t

where st = s and st+1  . . .   sT−1 are random variables due to the random transitions. We as-
sume that U is such that the minimum above exists (e.g.  compact set). It is not hard to show that
given state s  there exists a policy π with V π
0 (s) and we can compute such a minimax
policy if the algorithm knows F and ps a for all (s  a) /∈ F  from literature of robust MDP (e.g. 
[Nilim and El Ghaoui  2005] and [Iyengar  2005]).
The main message of this paper is that we can determine a policy as good as the minimax policy
without knowing either F or ps a for (s  a) /∈ F. To make this formal  we deﬁne the regret (against
the minimax performance) in episode i  for i = 1  2  . . . as

0 (s) = V ∗

where si
regret for m episodes  which we want to minimize  is thus deﬁned as

t denote the actual state visited and action taken at step t of episode i.1 The total

t and ai

∆i = V ∗

0 (si

0) −

r(si

t  ai

t) 

T−1(cid:88)

t=0

m(cid:88)

i=1

∆(m) =

∆i.

in

scenario

the multi-armed

The main algorithm is given in Figure 2. OLRM is basically UCRL2 [Jaksch et al.  2010] with an
additional stochastic check to detect adversarial state-action pairs. Like UCRL2  the algorithm em-
ploys the “optimism under uncertainty” principle. We start by assuming that all states are stochastic.
If the adversary plays “nicely”  nothing else would have to be done. The key challenge  however  is
to successfully identify the adversarial state-action pairs when they start to behave maliciously.
A similar
by
addressed
[Bubeck and Slivkins  2012]. They show that it is possible to achieve near-optimal regret without
knowing a priori whether a bandit is stochastic or adversarial. In [Bubeck and Slivkins  2012]  the
key is to check some consistency conditions that would be satisﬁed if the behavior is stochastic. We
use the same strategy and the question is then  which condition? We discuss this in section 3.2.
Note that the index k = 1  2  . . . tracks the number of policies. A policy is executed until either a
new pair (s  a) fails the stochastic check  and hence deemed to be adversarial  or some state-action
pair has been executed too many times. In either case  we need to re-compute the current optimistic
policy (see Section 3.1 for the detail). Every time a new policy is computed we call it a new epoch.
While each episode has the same length (T )  each epoch can span multiple episodes  and an epoch
can begin in the middle of an episode.

setting

bandit

been

has

3.1 Computing an optimistic policy

Figure 3 shows the algorithm for computing the optimistic minimax policy  where we treat all state-
action pairs in the set F as adversarial  and (similar to UCRL2) use optimistic values for other
state-action pairs.

1We provide high-probability regret bounds for any single trial  from which the expected regret can be

readily derived  if desired.

4

Input: S  A  T   δ  and for each (s  a)  U(s  a)

1. Initialize the set F ← {}.
2. Initialize k ← 1.
3. Compute an optimistic policy ˜π  assuming all state-action pairs in F are adversarial (Section

3.1).

4. Execute ˜π until one of the followings happen:

• The execution count of some state-action (s  a) has been doubled.
• The executed state-action pair (s  a) fails the stochastic check (Section 3.2). In this case

(s  a) is added to F .

5. Increment k. Go back to step 3.

Figure 2: The OLRM algorithm

particular  we use p(·)V (·) to mean the dot product between two such vectors  i.e. (cid:80)

Here  to simplify notations  we frequently use V (·) to mean the vector whose elements are V (s)
for each s ∈ S. This applies to both value functions as well as probability distributions over S. In
s p(s)V (s).
We use Nk(s  a) to denote the total number of times the state-action pair (s  a) has been executed
before epoch k. The corresponding empirical next-state distribution based on these transitions is
denoted as ˆPk(·|s  a). If (s  a) has never been executed before epoch k  we deﬁne Nk(s  a) = 1 and
assume ˆPk(·|s  a) to be arbitrarily deﬁned.
Input: S  A  T   δ  F   k  and for each (s  a)  U(s  a)  ˆPk(·|s  a) and Nk(s  a).

T−1(s) = maxa r(s  a) for all s.

1. Set ˜V k
2. Repeat  for t = T − 2  . . .   0:
• For each (s  a) ∈ F   set
• For each (s  a) /∈ F   set

(cid:40)

˜Qk

t (s  a) = min

• For each s  set

3. Output ˜π.

T − t 

r(s  a) + ˆPk(·|s  a) ˜V k

t+1(·) + T

2

Nk(s  a)

log

2SAT k2

δ

˜V k
t (s) = maxa ˜Qk

t (s  a)

and

˜πt(s) = arg maxa ˜Qk

t (s  a).

˜Qk

t (s  a) = min

T − t  min
p∈U (s a)

r(s  a) + p(·) ˜V k

t+1(·)

(cid:26)

(cid:115)

(cid:27)

.

(cid:41)

.

Figure 3: Algorithm for computing an optimistic minimax policy.

3.2 Stochasticity check
Every time a state-action (s  a) /∈ F is executed  the outcome is recorded and subjected to a “stochas-
ticity check”. Let n be the total number of times (s  a) has been executed (including the latest one)
and s(cid:48)
n are the next-states for each of these transitions. Let k1  . . .   kn be the epochs in which
each of these transitions happens. Let t1  . . .   tn be the step within the episodes (i.e. episode stage)
where these transitions happen. Let τ be the total number of steps executed by the algorithm (from
the beginning) so far. The stochastic check fails if:

1  . . .   s(cid:48)

(cid:114)

ˆPkj (·|s  a) ˜V kj

tj +1(·) −

tj +1(s(cid:48)
˜V kj

j) > 5T

nS log

4SAT τ 2

δ

.

n(cid:88)

j=1

n(cid:88)

j=1

The stochastic check follows the intuitive saying “if it is not broke  don’t ﬁx it”  by checking whether
the value of actual transition from (s  a) is below what is expected from the parameter estimation.

5

One can show that with high probability  all stochastic state-action pairs will always pass the stochas-
tic check. Now consider an adversarial (s  a) pair: if the adversary plays “nicely”  the current policy
accumulates satisfactory reward and hence nothing needs to be changed  even if the transitions them-
selves fail to “look” stochastic; if the adversary plays “nasty”  then the stochastic check will detect
it  and subsequently protect against it.

3.3 Main result
The following theorem summarizes the performance of OLRM. Here and in the sequel  we use ˜O
when the log terms are omitted. Our result for the inﬁnite-horizon case is similar (see Section 5).
Theorem 1. Given δ  T   S  A  the total regret of OLRM is

∆(m) ≤ ˜O(ST 3/2√Am)

for all m  with probability at least 1 − δ.
Note that the above is with respect to the total number of episodes m. Since the total number of
steps is τ = mT   the regret bound in terms of τ is therefore ˜O(ST√Aτ ). This gives the familiar
√τ regret as in UCRL2. Also  the bound has the same dependencies on S and A as in UCRL2. The
horizon length T plays the role of the “diameter” in the inﬁnite-horizon case and again it has the
same dependency as its counterpart in UCRL2.
The result shows that even though the algorithm deals with unknown stochastic and potentially
adversarial states  it achieves the same regret bound as in the fully stochastic case. In the case where
all states are in fact stochastic  this reduces to the same UCRL2 result.

4 Analysis of OLRM

We brieﬂy explain the roadmap of the proof of Theorem 1. The complete proof can be found in the
supplementary material.
Our proof starts with the following technical Lemma.
Lemma 1. The following holds for all state-action pair (s  a) /∈ F and for t = 0  . . .   (T − 1) in
all epochs k ≥ 1  with probability at least 1 − δ:
t+1(·) − ps a(·) ˜V k

ˆPk(·|s  a) ˜V k

t+1(·) ≤ T

4SAT k2

(cid:115)

Nk(s  a)

2S

log

.

δ

Proof sketch. Since (s  a) /∈ F is stochastic  we apply the bound from [Weissman et al.  2003] for
the 1-norm deviation between ˆPk(·|s  a) and ps a. The bound follows from (cid:107) ˜V k
Using Lemma 1  we show the following lemma that with high probability  all purely stochastic
state-action pairs will always pass the stochastic check.
Lemma 2. The probability that any state-action pair (s  a) /∈ F gets added into set F while running
the algorithm is at most 2δ.

t+1(·)(cid:107)∞ ≤ T .

Proof sketch. Each (s  a) /∈ F is purely stochastic. Suppose (s  a) has been executed n times and
1  . . .   s(cid:48)
s(cid:48)

n are the next-states for these transitions. Recall that the check fails if

(cid:114)

tj +1(s(cid:48)
˜V kj

j) > 5T

nS log

4SAT τ 2

δ

.

n(cid:88)

j=1

ˆPkj (·|s  a) ˜V kj

tj +1(·) −

n(cid:88)

j=1

We can derive a high-probability bound that satisﬁes the stochastic check by applying the Azuma-
Hoeffding inequality on the martingale difference sequence
tj +1(·) − ˜V kj

Xj = ps a(·) ˜V kj

tj +1(s(cid:48)
j)

followed by an application of Lemma 1.

6

We then show that all value estimates ˜V k
Lemma 3. With probability at least 1 − δ  and assume that no state-action pairs (s  a) /∈ F have
been added to F   the following holds for every state s ∈ S  every t ∈ {0  . . .   T − 1} and every
k ≥ 1:

t are always optimistic.

˜V k

t (s) ≥ V ∗

t (s).

Proof sketch. The key challenge is to prove that state-actions in F (adversarial) that have not been
identiﬁed (i.e. all past transitions passed the test) would have optimistic ˜Q values. This can be done
by  again  applying the Azuma-Hoeffding inequality.

Equipped with the previous three lemmas  we are now able to establish Theorem 1.

Proof sketch. Lemma 3 established that all value estimates ˜V k
t are always optimistic. We can there-
fore bound the regret by bounding the difference between ˜V k
t and the actual rewards received by the
algorithm. The “optimistic gap” shrinks in an expected manner as the number of steps executed by
the algorithm grows if all state-actions are stochastic.
For an adversarial state-action (s  a) ∈ F  we use the following facts to ensure the above: (i) If
(s  a) has been added to F (i.e.  it failed the stochastic check) then all policies afterwards would
correctly evaluate its value; (ii) All transitions before (s  a) is added to F (if ever) must have passed
the stochastic check and the check condition ensures that its behavior is consistent with what one
would expect if (s  a) was stochastic.

5

Inﬁnite horizon case

In the inﬁnite horizon case  let P be a particular choice of ps a ∈ U(s  a) for every (s  a) ∈ F.
Given a (stationary) policy π  its average undiscounted reward (or “gain”) is deﬁned as follows:

(cid:35)

gπ
P (s) = lim
τ→∞

EP

1
τ

r(si  π(si))

(cid:34) τ(cid:88)

t=1

τ(cid:88)

t=1

where s1 = s. The limit always exists for ﬁnite MDPs [Puterman  1994]. We make the assumption
that regardless of the choice of P   the resulting MDP is communicating and unichain. 2 In this case
P (s) is a constant and independent of s so we can drop the argument s.
gπ
We deﬁne the worst-case average reward of π over all possible P as gπ = minP gπ
minimax policy π∗ is any policy whose gain gπ∗
executing the MDP M for τ steps as

P . An optimal
= g∗ = maxπ gπ. We deﬁne the regret after

∆(τ ) = τ g∗

−

r(st  at).

The main algorithm for the inﬁnite-horizon case  which we refer as OLRM2  is essentially iden-
tical to OLRM. The main difference is in computing the optimistic policy and the corresponding
stochastic check. The detailed algorithm is presented in the supplementary material.
The algorithms from [Tewari and Bartlett  2007] can be used to compute an optimistic minimax
policy.
In particular  for each (s  a) ∈ F   its transition function is chosen pessimistically from
U(s  a). For each (s  a) /∈ F   its transition function is chosen optimistically from the following set:

(cid:115)

{p : (cid:107)p(·) − ˆPk(·|s  a)(cid:107)1 ≤ σ} where σ =

2S

Nk(s  a)

log

4SAk2

δ

.

2 In more general settings  such as communicating or weakly communicating MDPs  although the optimal
policies (for a ﬁxed P ) always have constant gain  the optimal minimax policies (over all possible P ) might
have non-constant gain. Additional assumptions on U  as well as a slight change in the deﬁnition of the regret
are needed to deal with these cases. This is left for future research.

7

Let ˜Pk(·|s  ˜πk(s)) be the minimax choice of transition functions for each s where the minimax gain
g ˜πk is attained. The bias hk can be obtained by solving the following system of equations for h(·)
(see [Puterman  1994]):
(1)

+ h(s) = r(s  ˜πk(s)) + ˜Pk(·|s  ˜πk(s))h(·).

∀s ∈ S 

g ˜πk

The stochastic check for the inﬁnite-horizon case is mostly identical to the ﬁnite-horizon case  except
that we replace T with the maximal span ˜H of the bias  deﬁned as follows:

(cid:17)

hk(s)

.

The stochastic check fails if:

˜H =

(cid:16)
˜Pkj (·|s  a)hkj (·) − n(cid:88)

k∈{k1 ... kn}

max

n(cid:88)

j=1

j=1

max

s

hk(s) − min
(cid:114)

s

(cid:48)
j) > 5 ˜H
hkj (s

nS log

4SAτ 2

δ

.

Let H be the maximal span of the bias of any optimal minimax policies. The following summa-
rizes the performance of OLRM2. The proof  deferred in the supplementary material  is similar to
Theorem 1.
Theorem 2. Given δ  S  A  the total regret of OLRM2 is
for all τ  with probability at least 1 − δ.
6 Experiment

∆(τ ) ≤ ˜O(SH√Aτ )

Figure 4: Total accumulated rewards. The vertical line marks the start of “breakdown”.

We run both our algorithm as well as UCRL2 on the example MDP in Figure 1 for the inﬁnite-
horizon case. Figure 4 shows the result for g∗ = 0.18  β = 0.07 and α = 0.17. It shows that
UCRL2 accumulates smaller total rewards than the optimal minimax policy while our algorithm
actually accumulates larger total rewards than the minimax policy. We also include the result for a
standard robust MDP that treats all state-action pairs as adversarial and therefore performs poorly.
Additional details are provided in the supplementary material.

7 Conclusion

We presented an algorithm for online learning of robust MDPs with unknown parameters  some
can be adversarial. We show that it achieves similar regret bound as in the fully stochastic case. A
natural extension is to allow the learning of the uncertainty sets in adversarial states  where the true
uncertainty set is unknown. Our preliminary results show that very similar regret bounds can be
obtained for learning from a class of nested uncertainty sets.

Acknowledgments

This work is partially supported by the Ministry of Education of Singapore through AcRF Tier
Two grant R-265-000-443-112 and NUS startup grant R-265-000-384-133. The research leading to
these results has received funding from the European Research Council under the European Union’s
Seventh Framework Programme (FP/2007-2013)/ ERC Grant Agreement n.306638.

8

02468x 10600.511.522.5x 106Time stepsTotal reward  OLRM2UCRL2Standard robust MDPOptimal minimax policyReferences

[Brafman and Tennenholtz  2002] Brafman  R. I. and Tennenholtz  M. (2002). R-max - a general
polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning
Research  3:213–231.

[Bubeck and Slivkins  2012] Bubeck  S. and Slivkins  A. (2012). The best of both worlds: Stochas-
tic and adversarial bandits. Journal of Machine Learning Research - Proceedings Track  23:42.1–
42.23.

[Even-Dar et al.  2005] Even-Dar  E.  Kakade  S. M.  and Mansour  Y. (2005). Experts in a markov
decision process. In Saul  L. K.  Weiss  Y.  and Bottou  L.  editors  Advances in Neural Informa-
tion Processing Systems 17  pages 401–408. MIT Press  Cambridge  MA.

[Even-Dar et al.  2009] Even-Dar  E.  Kakade  S. M.  and Mansour  Y. (2009). Online markov

decision processes. Math. Oper. Res.  34(3):726–736.

[Iyengar  2005] Iyengar  G. N. (2005). Robust dynamic programming. Math. Oper. Res.  30(2):257–

280.

[Jaksch et al.  2010] Jaksch  T.  Ortner  R.  and Auer  P. (2010). Near-optimal regret bounds for

reinforcement learning. J. Mach. Learn. Res.  99:1563–1600.

[Mannor et al.  2012] Mannor  S.  Mebel  O.  and Xu  H. (2012). Lightning does not strike twice:

Robust mdps with coupled uncertainty. In ICML.

[Mannor et al.  2007] Mannor  S.  Simester  D.  Sun  P.  and Tsitsiklis  J. N. (2007). Bias and

variance approximation in value function estimates. Manage. Sci.  53(2):308–322.

[McDiarmid  1989] McDiarmid  C. (1989). On the method of bounded differences. In Surveys in
Combinatorics  number 141 in London Mathematical Society Lecture Note Series  pages 148–
188. Cambridge University Press.

[Neu et al.  2012] Neu  G.  Gy¨orgy  A.  and Szepesv´ari  C. (2012). The adversarial stochastic short-
est path problem with unknown transition probabilities. Journal of Machine Learning Research
- Proceedings Track  22:805–813.

[Neu et al.  2010] Neu  G.  Gy¨orgy  A.  Szepesv´ari  C.  and Antos  A. (2010). Online markov deci-

sion processes under bandit feedback. In NIPS  pages 1804–1812.

[Nilim and El Ghaoui  2005] Nilim  A. and El Ghaoui  L. (2005). Robust control of markov deci-

sion processes with uncertain transition matrices. Oper. Res.  53(5):780–798.

[Puterman  1994] Puterman  M. L. (1994). Markov Decision Processes: Discrete Stochastic Dy-

namic Programming. Wiley-Interscience.

[Strens  2000] Strens  M. (2000). A bayesian framework for reinforcement learning. In In Proceed-
ings of the Seventeenth International Conference on Machine Learning  pages 943–950. ICML.

[Tewari and Bartlett  2007] Tewari  A. and Bartlett  P. (2007). Bounded parameter markov decision

processes with average reward criterion. Learning Theory  pages 263–277.

[Weissman et al.  2003] Weissman  T.  Ordentlich  E.  Seroussi  G.  Verdu  S.  and Weinberger 
Inequalities for the l1 deviation of the empirical distribution. Technical report 

M. J. (2003).
Information Theory Research Group  HP Laboratories.

[Xu and Mannor  2012] Xu  H. and Mannor  S. (2012). Distributionally robust markov decision

processes. Math. Oper. Res.  37(2):288–300.

[Yu and Mannor  2009] Yu  J. Y. and Mannor  S. (2009). Arbitrarily modulated markov decision

processes. In CDC  pages 2946–2953.

[Yu et al.  2009] Yu  J. Y.  Mannor  S.  and Shimkin  N. (2009). Markov decision processes with

arbitrary reward processes. Math. Oper. Res.  34(3):737–757.

9

,Shiau Hong Lim
Huan Xu
Shie Mannor
Ian Osband
Benjamin Van Roy
Weifeng Chen
Zhao Fu
Dawei Yang
Jia Deng