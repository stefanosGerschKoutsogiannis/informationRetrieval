2010,Adaptive Multi-Task Lasso: with Application to eQTL Detection,To understand the relationship between genomic variations among population and complex diseases  it is essential to detect eQTLs which are associated with phenotypic effects. However  detecting eQTLs remains a challenge due to complex underlying mechanisms and the very large number of genetic loci involved compared to the number of samples. Thus  to address the problem  it is desirable to take advantage of the structure of the data and prior information about genomic locations such as conservation scores and transcription factor binding sites.  In this paper  we propose a novel regularized regression approach for detecting eQTLs which takes into account related traits simultaneously while incorporating many regulatory features. We first present a Bayesian network for a multi-task learning problem that includes priors on SNPs  making it possible to estimate the significance of each covariate adaptively. Then we find the maximum a posteriori (MAP) estimation of regression coefficients and estimate weights of covariates jointly. This optimization procedure is efficient since it can be achieved by using convex optimization and a coordinate descent procedure iteratively. Experimental results on simulated and real yeast datasets confirm that our model outperforms previous methods for finding eQTLs.,Adaptive Multi-Task Lasso: with Application to

eQTL Detection

Seunghak Lee  Jun Zhu and Eric P. Xing

School of Computer Science  Carnegie Mellon University

{seunghak junzhu epxing}@cs.cmu.edu

Abstract

To understand the relationship between genomic variations among population and
complex diseases  it is essential to detect eQTLs which are associated with phe-
notypic effects. However  detecting eQTLs remains a challenge due to complex
underlying mechanisms and the very large number of genetic loci involved com-
pared to the number of samples. Thus  to address the problem  it is desirable to
take advantage of the structure of the data and prior information about genomic
locations such as conservation scores and transcription factor binding sites.
In this paper  we propose a novel regularized regression approach for detecting
eQTLs which takes into account related traits simultaneously while incorporating
many regulatory features. We ﬁrst present a Bayesian network for a multi-task
learning problem that includes priors on SNPs  making it possible to estimate the
signiﬁcance of each covariate adaptively. Then we ﬁnd the maximum a posteriori
(MAP) estimation of regression coefﬁcients and estimate weights of covariates
jointly. This optimization procedure is efﬁcient since it can be achieved by us-
ing a projected gradient descent and a coordinate descent procedure iteratively.
Experimental results on simulated and real yeast datasets conﬁrm that our model
outperforms previous methods for ﬁnding eQTLs.

1 Introduction

One of the fundamental problems in computational biology is to understand associations between
genomic variations and phenotypic effects. The most common genetic variations are single nu-
cleotide polymorphisms (SNPs)  and many association studies have been conducted to ﬁnd SNPs
that cause phenotypic variations such as diseases or gene-expression traits [1]. However  association
mapping of causal QTLs or eQTLs remains challenging as the variation of complex traits is a result
of contributions of many genomic variations. In this paper  we focus on two important problems to
detect eQTLs. First  we need to ﬁnd methods to take advantage of the structure of data for ﬁnding
association SNPs from high dimensional eQTL datasets when p ≫ N   where p is the number of
SNPs and N is the sample size. Second  we need techniques to take advantage of prior biological
knowledge to improve the performance of detecting eQTLs.

To address the ﬁrst problem  Lasso is a widely used technique for high-dimensional association
mapping problems  which can yield a sparse and easily interpretable solution via an ℓ1 regularization
[2]. However  despite the success of Lasso  it is limited to considering each trait separately. If we
have multiple related traits it would be beneﬁcial to estimate eQTLs jointly since we can share
information among related traits. For the second problem  Fig. 1 shows some prior knowledge on
SNPs in a genome including transcription factor binding sites (TFBS)  5’ UTR and exon  which play
important roles for the regulation of genes. For example  TFBS controls the transcription of DNA
sequences to mRNAs. Intuitively  if SNPs are located on these regions  they are more likely to be
true eQTLs compared to those on regions without such annotations since they are related to genes or
gene regulations. Thus  it would be desirable to penalize regression coefﬁcients less corresponding

1

Transcription factor binding site

5’ UTR

Exon 

SNPs

Chromosome

Annotation

Figure 1: Examples of prior knowledge on SNPs including transcription factor binding sites  5’ UTR and
exon. Arrows represent SNPs and we indicate three genomic annotations on the chromosome. Here association
SNPs are denoted by red arrows (best viewed in color)  showing that SNPs on regions with regulatory features
are more likely to be associated with traits.

to SNPs having signiﬁcant annotations such as TFBS in a regularized regression model. Again  the
widely used Lasso is limited to treating all SNPs equally.

This paper presents a novel regularized regression approach  called adaptive multi-task Lasso  to
effectively incorporate both the relatedness among multiple gene-expression traits and useful prior
knowledge for challenging eQTL detection. Although some methods have been developed for either
adaptive or multi-task learning  to the best of our knowledge  adaptive multi-task Lasso is the ﬁrst
method that can consider prior information on SNPs and multi-task learning simultaneously in one
single framework. For example  Lirnet uses prior knowledge on SNPs such as conservation scores 
non-synonymous coding and UTR regions for a better search of association mappings [3]. However 
Lirnet considers the average effects of SNPs on gene modules by assuming that association SNPs are
shared in a module. This approach is different from multi-task learning where association SNPs are
found for each trait while considering group effects over multiple traits. To ﬁnd genetic markers that
affect correlated traits jointly  the graph-guided fused Lasso [4] was proposed to consider networks
over multiple traits within an association analysis. However  graph-guided fused Lasso does not
incorporate prior knowledge of genomic locations.

Unlike other methods  we deﬁne the adaptive multi-task Lasso as ﬁnding a MAP estimate of a
Bayesian network  which provides an elegant Bayesian interpretation of our approach; the resultant
optimization problem is efﬁciently solved with an alternating minimization procedure. Finally  we
present empirical results on both simulated and real yeast eQTL datasets  which demonstrates the
advantages of adaptive multi-task Lasso over many other competitors.

2 Problem Deﬁnition: Adaptive Multi-task Lasso

Let Xij ∈ {0  1  2} denote the number of minor alleles at the j-th SNP of i-th individual for
i = 1  . . .   N and j = 1  . . .   p. We have K related gene traits and Y k
represents the gene
i
In our setting  we assume
expression level of k-th gene of i-th individual for k = 1  . . .   K.
that the K traits are related to each other and we explore the relatedness in a multi-task learning
framework. To achieve the relatedness among tasks via grouping effects [5]  we can use any
clustering algorithms such as spectral clustering or hierarchical clustering. In association mapping
problems  these clusters can be viewed as clusters of genes which consist of regulatory networks or
pathways [4]. We treat the problem of detecting eQTLs as a linear regression problem. The general
setting includes one design matrix X and multiple tasks (genes) for k = 1  . . .   K 

Y k = Xβk + ǫ

(1)
where ǫ is a standard Gaussian noise. We further assume that Xij’s are standardized such that
Pi Xij/N = 0 and Pi X 2
Now  the open question is how we can devise an appropriate objective function over β that could ef-
fectively consider the desirable group effects over multiple traits and incorporate useful prior knowl-
edge  as we have stated. To explain the motivation of our work and provide a useful baseline that
grounds the proposed approach  we ﬁrst brieﬂy review the standard Lasso and multi-task Lasso.

ij/N = 1  and consider a model without an intercept.

2.1 Lasso and Multi-task Lasso

Lasso [2] is a technique for estimating the regression coefﬁcients β and has been widely used
for association mapping problems. Mathematically  it solves the ℓ1-regularized least square problem 

ˆβ = argmin

β

1
2

kY − Xβk2

2 + λ

p

Xj=1

δj |βj |

(2)

2

where λ determines the degree of regularization of nonzero βj. The scaling parameters δj ∈ [0  1]
are usually ﬁxed (e.g.  unit ones) or set by cross-validation  which can be very difﬁcult when p is
large. Due to the singularity at the origin  the ℓ1 regularization (Lasso penalty) can yield a stable and
sparse solution  which is desirable for association mapping problems because in most cases we have
p ≫ N and there exists only a small number of eQTLs. It is worth mentioning that Lasso estimates
are posterior mode estimates under a multivariate independent Laplace prior for β [2].
As we can see from problem (2)  the standard Lasso does not distinguish the inputs and regression
coefﬁcients from different tasks.
In order to capture some desirable properties (e.g.  shared
structures or sparse patterns) among multiple related tasks  the multi-task Lasso was proposed [5] 
which solves the problem 

min

β

1
2

K

Xk=1

kY k − Xβkk2

2 + λ

p

Xj=1

δj kβj k2

(3)

where kβjk2 = qPk(βk
2 is the ℓ2-norm. This model encourages group-wise sparsity across
j )
related tasks via the ℓ1/ℓ2 regularization. Again  the solution of Eq. (3) can be interpreted as a MAP
estimate under appropriate priors with ﬁxed scaling parameters.

Multi-task Lasso has been applied (with some extensions) to perform association analysis [4]. How-
ever  as we have stated  the limitation of current approaches is that they do not incorporate the useful
prior knowledge. The proposed adaptive multi-task Lasso  as to be presented  is an extension of the
multi-task Lasso to perform joint group-wise and within-group feature selection and incorporate the
useful prior knowledge for effective association analysis.

2.2 Adaptive Multi-task Lasso

Now  we formally introduce the adaptive multi-task Lasso. For clarity  we ﬁrst deﬁne the sparse
multi-task Lasso with ﬁxed scaling parameters  which will be a sub-problem of the adaptive
multi-task Lasso  as we shall see. Speciﬁcally  sparse multi-task Lasso solves the problem 

min

β

1
2

K

Xk=1

kY k − Xβkk2

2 + λ1

p

Xj=1

θj

K

Xk=1

|βk

j | + λ2

p

Xj=1

ρjkβj k2

(4)

where θ and ρ are the scaling parameters for the ℓ1 and ℓ1/ℓ2-norm  respectively. The regularization
parameters λ1 and λ2 can be determined by cross or holdout validation. Obviously  this model sub-
sumes the standard Lasso and multi-task Lasso  and it has three advantages over previous models.
First  unlike the multi-task Lasso  which contains the ℓl/ℓ2-norm only to achieve group-wise spar-
sity  the ℓ1-norm in Eq. (4) can achieve sparsity among SNPs within a group. This property is useful
when K tasks are not perfectly related and we need additional sparsity in each block of kβjk2. In
section 4  we demonstrate the usefulness of the blended regularization. The hierarchical penaliza-
tion [6] can achieve a smooth shrinkage effect for variables within a group  but it cannot achieve
within-group sparsity. Second  unlike Lasso we induce group sparsity across multiple related traits.
Finally  as to be extended  unlike Lasso and multi-task Lasso which treat βj equally or with a ﬁxed
scaling parameter  we can adaptively penalize each βj according to prior knowledge on covariates
in such a way that SNPs having desirable features are less penalized (see Fig. 1 for details of prior
knowledge on SNPs).

To incorporate the prior knowledge as we have stated  we propose to automatically learn the scaling
parameters (θ  ρ) from data. To that end  we deﬁne θ and ρ as mixtures of features on j-th SNP  i.e.
(5)

ωtf j

t is t-th feature for j-th SNP. For example f j

where f j
t can be a conservation score of j-th SNP or one
if the SNP is located on TFBS  zero otherwise. To avoid scaling issues  we assume each feature is
standardized  i.e.  Pj f j
t = 1  ∀t. Since we are interested in the relative contributions from different
features  we further add the constraints that Pt ωt = 1 and Pt νt = 1. These constraints can be
interpreted as a regularization on the feature weights ω ≥ 0 and ν ≥ 0.
Although using the deﬁnitions (5) in problem (4) and jointly estimating β and feature weights (ω  ν)
can give a solution of adaptive multi-task learning  the resultant method would be lack of an el-
egant Bayesian interpretation  which is a desirable property that can make the framework more

θj = Xt

t and ρj = Xt

νtf j
t  

3

ﬂexible and easily extensible. Recall that the Lasso
estimates can be interpreted as MAP estimates under
Laplace priors. Similarly  to achieve a framework
that enjoys an elegant Bayesian interpretation  we
deﬁne a Bayesian network and treat the adaptive
multi-task learning problem as ﬁnding its MAP
estimate. Speciﬁcally  we build a Bayesian network
as shown in Fig. 2 in order to compute the MAP
estimate of β under adaptive scaling parameters 
{θ  ρ}. We deﬁne the conditional probability of β
given scaling parameters as 

f1

fT

ω

θ

ρ

ν

X

Y

β

Figure 2: Graphical model representation of
adaptive multi-task Lasso.

P (β|θ  ρ) =

1

Z(θ  ρ)

p

K

Yj=1

Yk=1

exp (−θj|βk

j |) ×

p

Yj=1

exp (−ρjkβj k2)

where Z(θ  ρ) is a normalization factor  and P (Y |X  β) ∼ N (Xβ  Σ)  where Σ is the identity
matrix. Although in principle we can treat θ and ρ as random variables and deﬁne a fully Bayesian
approach  for simplicity  we deﬁne θ and ρ as deterministic functions of ω and ν as in Eq. (5).
Extension to a fully Bayesian approach is our future work.

Now we deﬁne the adaptive multi-task Lasso as ﬁnding the MAP estimation of β and simultane-
ously estimating the feature weights (ω  ν)  which is equivalent to solving the optimization problem 

min
β ω ν

1
2

K

Xk=1

kY k − Xβkk2

2 + λ1

p

Xj=1

θj

K

Xk=1

|βk

j | + λ2

p

Xj=1

ρjkβj k2 + log Z(θ  ρ) 

(6)

where ω and ν are related to θ and ρ through Eq. (5) and subject to the constraints as deﬁned above.

Remark 1 Although we can interpret problem (4) as a MAP estimate of β under appropriate priors when
scaling parameters (θ  ρ) are ﬁxed  it does not enjoy an elegant Bayesian interpretation if we perform joint
estimation of β and the scaling parameters (ω  ν) because it ignores normalization factors of the appropriate
priors. Lee et al. [3] used this approach where a regularized regression model is optimized over scaling
parameters and β jointly. Therefore  their method does not have an elegant Bayesian interpretation. Moreover 
as we have stated  Lee et al. [3] did not consider grouping effects over multiple traits.

Remark 2 Our method also differs from the adaptive Lasso [7]   transfer learning with meta-priors [8] and
the Bayesian Lasso [9]. First  although both adaptive Lasso and our method use adaptive parameters for
penalizing regression coefﬁcients  we learn adaptive parameters from prior knowledge on covariates in a multi-
task setting while adaptive Lasso uses ordinary least square solutions for adaptive parameters in a single task
setting. Second  the method of transfer learning with meta-priors [8] is similar to our method in a sense that
both use prior knowledge with multiple related tasks. However  we couple related tasks via ℓ1/ℓ2 penalty while
they couple tasks via transferring hyper-parameters among them. Thus we have group sparsity across tasks
as well as sparsity in each group but they cannot induce group sparsity across different tasks. Finally  the
Bayesian Lasso [9] does not have the grouping effects in multiple traits and the priors used usually do not
consider domain knowledge.

3 Optimization: an Alternating Minimization Approach

Now  we solve the adaptive multi-task Lasso problem (6). First  since the normalization factor Z is
hard to compute  we use its upper bound  as given by 

Z ≤

p

Yj=1ZRK

exp (−kρjk2)dρYj

θj(cid:19)K
(cid:18) 2

=

π

p

Yj=1

K−1

2 Γ( K+1

2 )2K

(ρjK)K Yj

θj(cid:19)K
(cid:18) 2

.

(7)

This integral result is due to normalization constant of K dimensional multivariate Laplace distri-
bution [10  11]. Using this upper bound  the learning problem is to minimize an upper bound of the
objective function in problem (6)  which will be denoted by L(β  ω  ν) henceforth. Although L is
not joint convex over β  ω and ν  it is convex over β given {ω  ν} and convex over {ω  ν} given β.
We use an alternating optimization procedure which (1) minimizes the upper bound L of problem (6)
over {ω  ν} by ﬁxing β; and (2) minimizes L over β by ﬁxing {ω  ν} iteratively until convergence
[12]. Both sub-problems are convex and can be solved efﬁciently via a projected gradient descent
method and a coordinate descent method  respectively.

4

For the ﬁrst step of optimizing L over ω and ν  the sub-problem is to solve

min

ω∈Pω ν∈Pν Xj Xk (cid:16)− log θj + θj |βk

j |(cid:17) +Xj

(−K log ρj + ρjkβj k2)  

where Pω   {ω : Pt ωt = 1  ωt ≥ 0  ∀t} is a simplex over ω  likewise for Pν. θ and ρ are
functions of ω and ν as deﬁned in Eq. (5). This constrained problem is convex and can be solved by
using a gradient descent algorithm combined with a projection onto a simplex sub-space  which can
be efﬁciently done [13]. Since ω and ν are not coupled  we can learn each of them separately.
For the second sub-problem that optimizes L over β given ﬁxed feature weights (ω  ν)  it is exactly
the optimization problem (4). We can solve it using a coordinate descent procedure  which has been
used to optimize the sparse group Lasso [14]. Our problem is different from the sparse group Lasso
in the sense that the sparse group Lasso includes group penalty over multiple covariates for a single
trait  while adaptive multi-task Lasso considers group effects over multiple traits. Here we solve
problem (4) using a modiﬁed version of the algorithm proposed for the sparse group Lasso.

j = 0 for each k. If it is true that βk

As summarized in Algorithm 1  the general optimization procedure is as follows: for each j  we
check the group sparsity condition that βj = 0. If it is true  no update is needed for βj. Otherwise 
we check whether βk
j ; otherwise 
we optimize problem (4) over βk
j with all other coefﬁcients ﬁxed. This one-dimensional optimiza-
tion problem can be efﬁciently solved by using a standard optimization method. This procedure is
continued until a convergence condition is met.
More speciﬁcally  we ﬁrst obtain the optimal conditions for problem (4) by computing the subgra-
dient of its objective function with respect to βk

j = 0  no update is needed for βk

−X T

j (Y k − Xβk) + λ2ρjgk

j and set it to zero:
j + λ1θj hk

j = 0 

where g and h are sub-gradients of the ℓ1/ℓ2-norm and the ℓ1-norm  respectively. Note that gk

j =

βk
j

if βj 6= 0  otherwise kgjk2 ≤ 1; and hk

j = sign(βk

j ) if βk

j 6= 0  otherwise hk

j ∈ [−1  1].

kβj k2
Then  we check the group sparsity that βj = 0. To do that  we set βj = 0 in Eq. (8)  and we have 

X T

j Y k−X T

j Xr6=j

Xrβk

r = λ2ρjgk

j +λ1θjhk

j   and ||gj ||2

2 =

1
2ρ2
λ2
j

K

Xk=1

(X T

j Y k − X T

j Xr6=j

Xrβk

r − λ1θj hk

j )2.

According to subgradient conditions  we need to have a gj that satisﬁes the less than inequality
2 < 1; otherwise  βj will be non-zero. Since gj is a function of hj  it sufﬁces to check whether
kgjk2
the minimal square ℓ2-norm of gj is less than 1. Therefore  we solve the minimization problem of
kgjk2

2 w.r.t hj  which gives the optimal hj as 

(8)

(9)

ck
j

λ1θj

hk

if |

j = 

r . If the minimal kgjk2
j Pr6=jXrβk

ck
j
λ1θj

sign(

)

ck
j
λ1θj

otherwise

| ≤ 1

j =X T

where ck
update is needed; otherwise  we continue to the next step of checking whether βk

2 is less than 1  then βj is zero and no
j =0  ∀k  as follows.

j Y k − X T

Again  we start by assuming βk
j Xr6=j

j Y k − X T

X T

j is zero. By setting βk
j   and hk
Xrβk

r = λ1θj hk

j = 0 in Eq. (8)  we have 
j Xr6=j

j Y k − X T

(X T

λ1θj

j =

1

Xrβk

r ).

j | < 1;
According to the deﬁnition of the subgradient hk
j will be non-zero. This checking step can be easily done. After the check  if we have
otherwise  βk
j   and
j 6= 0  the problem (4) becomes an one-dimensional optimization problem with respect to βk
βk
the solution can be obtained using existing optimization algorithms (e.g. optimize function in the
R). We used majorize-minimize algorithm with gradient descent [15].

j   it needs to satisfy the condition that |hk

With the above two steps  we iteratively optimize (ω  ν) by ﬁxing β and optimize β by ﬁxing feature
weights until convergence. Note that the parameters λ1 and λ2 in Eq. (4)  which determine sparsity
levels  are determined by cross or hold-out validation.

5

Input : X ∈ RN ×p; Y ∈ RN ×K; θ ∈ Rp; ρ ∈ Rp; and βinit ∈ Rp×K
Output: β ∈ Rp×K
β ← βinit;
Iterate this procedure until convergence;
for j ← 1 to p do

m ← 1
λ2

2ρ2
j

PK

k=1 (ck

j − λ1θj hk

j )2 where ck

j and hk

j are computed as in Eq. (9);

if m < 1 then βk
else for k ← 1 to K do

j = 0  for all k = 1  . . . K;

q ← 1

λ1θj

j Xj βk

j |;

|X T

j (Y k − Xβk) + X T
j = 0;

if q < 1 then βk
else Solve the following one-dimensional optimization problem:
βk

2 kY k − Xβkk2

2 + λ1θj|βk

j ← argmin

j | + λ2ρj kβj k2;

1

βk
j

end

end

Algorithm 1: Optimization algorithm for Equation (4) with ﬁxed scaling parameters.

4 Simulation Study

To conﬁrm the behavior of our model  we run the adaptive multi-task Lasso and other methods on
our simulated dataset (p=100  K=10). We ﬁrst randomly select 100 SNPs from 114 yeast genotypes
from the yeast eQTL dataset [16]. Following the simulation study in Kim et al. [4]  we assume that
some SNPs affect biological networks including multiple traits  and true causal SNPs are selected
by the following procedure. Three sets of randomly selected four SNPs are associated with three
trait clusters (1 − 3)  (4 − 6)  (7 − 10)  respectively. One SNP is associated with two clusters
(1 − 3) and (4 − 6)  and one causal SNP is for all traits (1 − 10). For all association SNPs we
set identical association strength from 0.3 to 1. Traits are generated by Y k = Xβk + ǫ  for all
k = 1  . . .   10 where ǫ follows the standard normal distribution. We make 10 features (f1 − f10) 
of which six are continuous and four are discrete. For the ﬁrst three continuous features (f1 − f3) 
the feature value is drawn from s(N (2  1)) if a SNP is associated with any traits; otherwise from
s(N (1  1))  where s(x) =
1+exp(x) is the sigmoid function. For the other three continuous features
(f4 −f6)  the value is drawn from s(N (2  0.5)) if a SNP is associated with any traits; otherwise from
s(N (1  0.5)). Finally  for the discrete features (f7 − f10)  the value is set to s(2) with probability
0.8 if a SNP is associated with any traits; otherwise to s(1). We standardize all the features.

1

True β

AML

SML

/l
A + l
1
2

Single SNP

Lasso

l
/l∞
1

 

10

20

30

40

50

60

70

80

90

10

20

30

40

50

60

70

80

90

10

20

30

40

50

60

70

80

90

10

20

30

40

50

60

70

80

90

10

20

30

40

50

60

70

80

90

10

20

30

40

50

60

70

80

90

10

20

30

40

50

60

70

80

90

100

2 4 6 8 10

100

2 4 6 8 10

100

2 4 6 8 10

100

2 4 6 8 10

100

2 4 6 8 10

100

2 4 6 8 10

100

 

2 4 6 8 10

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Figure 3: Results of the β matrix estimated by different methods. For visualization  we present normalized
absolute values of regression coefﬁcients and darker colors imply stronger association with traits. For each
matrix  X-axis represents traits (1-10) and Y-axis represents SNPs (1-100). True β is shown in the left.

Fig. 3 shows the estimated β matrix by various methods including AML (adaptive multi-task Lasso) 
SML (sparse multi-task Lasso which is AML without adaptive weights)  A+ℓ1/ℓ2 (AML without
Lasso penalty)  Single SNP [17]  Lasso and ℓ1/ℓ∞ (multi-task learning with ℓ1/ℓ∞ norm). In this
ﬁgure  X-axis represents traits (1-10) and Y-axis represents SNPs (1-100). Note that regression
parameters (e.g. λ1 and λ2 for AML) were determined by holdout validation  and we set association
strength to 0.3. We also used hierarchical clustering with cutoff criterion 0.8 prior to run AML 
SML  A+ℓ1/ℓ2 and ℓ1/ℓ∞  and Single SNP and Lasso were analyzed for each trait separately.
We investigate the effect of Lasso penalty in our model by comparing the results of AML and
A+ℓ1/ℓ2. While AML is slightly more efﬁcient than A+ℓ1/ℓ2 in ﬁnding association SNPs  both

6

work very well for this task. It is not surprising since hierarchical clustering reproduced true trait
clusters and true β could be detected without considering single SNP level sparsity in each group.
To further validate the effectiveness of Lasso penalty  we run AML and A+ℓ1/ℓ2 without a priori
clustering step. Interestingly  AML could pick correct SNP-traits associations due to Lasso penalty 
however  A+ℓ1/ℓ2 failed to do so (see Fig. 5c d for the comparison of performance). While Lasso
penalty did not show signiﬁcant contribution for this task when we generated a priori clusters  it is
good to include it when the quality of a clustering is not guaranteed. Comparing the results of AML
and SML in Fig. 3  we could observe that adaptive weights improve the performance signiﬁcantly.
Adaptive weights help not only reduce false positives but also increase true positives.

Fig. 4 shows the learned feature weights of ω (ν is al-
most identical to ω and not shown here). The results are
based on 100 simulations for each association strength
0.3  0.5  0.8 and 1  and half of error bar represents one
standard deviation from the mean. We could observe that
discrete features f7 −f10 have highest weights while low-
est weights are assigned to f1 − f3. These weights are
reasonable because f1 −f3 are drawn from Gaussian with
large standard deviation (STD: 1) compared to that of fea-
tures f4 − f6 (STD: 0.5). Also  discrete features are the
most important since they discriminate true association
SNPs with a high probability 0.8.

0.16

0.14

0.12

t

ω

0.1

0.08

0.06

0.04

0.02

f
1

f
2

f
3

f
4

f
5

f
6

f
7

f
8

f
9

f
10

Features

Figure 4: Learned feature weights of ω.

a
1

0.8

0.6

0.4

0.2

0

0

y
t
i
v
i
t
i
s
n
e
S

b
1

0.8

0.6

0.4

0.2

0

0

y
t
i
v
i
t
i
s
n
e
S

c
1

0.8

0.6

0.4

0.2

0

 
0

y
t
i
v
i
t
i
s
n
e
S

 

y
t
i
v
i
t
i
s
n
e
S

0.5

1 − Specificity

1

d
1

0.8

0.6

0.4

0.2

0

 
0

 

AML
SML
A+l1/l2
l1/l∞
Lasso
Single SNP

0.5

1 − Specificity

1

0.5

1 − Specificity

1

0.5

1 − Specificity

1

Figure 5: ROC curves of various methods as association strength varies (a) 0.3  (b) 0.5 on clustered data  (c)
0.3  and (d) 0.5 on input dataset. (a b) Results on clustered data  where correct groups of gene traits are found
using hierarchical clustering (cutoff = 0.8). (c d) Results on input dataset without using clustering algorithm.

We compare the sensitivity and speciﬁcity of our model with other methods. In Fig. 5  we generated
ROC curves for association strength of 0.3 and 0.5. Fig. 5a b show the results with a priori hierar-
chical clustering and Fig. 5c d is with no such preprocessing steps. Using hierarchical clustering we
could correctly ﬁnd three clusters of gene traits at cutoff 0.8. In Fig. 5  when association strength
is small (i.e.  0.3)  AML and A+ℓ1/ℓ2 signiﬁcantly outperformed other methods. As association
strength increased  the performance of multi-task learning methods improved quickly while meth-
ods based on a single trait such as Lasso and Single SNP showed gradual increase of performance.

We computed test errors on 100 simulated dataset using 30 samples for test and 84 samples for
training. On average  AML achieved the best test error rate of 0.9427  and the order of other methods
in terms of test errors is: A + ℓ1/ℓ2 (0.9506)  SML (1.0436)  ℓ1/ℓ∞ (1.0578) and Lasso (1.1080).

5 Yeast eQTL dataset

We analyze the yeast eQTL dataset [16] that contains expression levels of 5 637 genes and 2 956
SNPs. The genotype data include genetic variants of 114 yeast strains that are progenies of the
standard laboratory strain (BY) and a wild strain (RM). We used 141 modules given by Lee et al.
[3] as groups of gene traits  and extracted unique 1 260 SNPs from 2 956 SNPs for our analysis. For
prior biological knowledge on SNPs used for adaptive multi-task Lasso  we downloaded 12 features
from Saccharomyces Genome Database (http://www.yeastgenome.org) including 11 discrete and 1
continuous feature (conservation score). For a discrete feature  we set its value as f j
t = s(2) if the
feature is found on the j-th SNP  f j
t = s(1) otherwise. For conservation score  we set f j
t = s(score).
All the features are then standardized.

7

0.2

0.18

0.16

0.14

Fig. 6 represents ω learned from the yeast eQTL dataset
(ν is almost identical to ω). The features are ncRNA (f1) 
noncoding exon (f2)  snRNA (f3)  tRNA (f4)  intron (f5) 
binding site (f6)  5’ UTR intron (f7)  LTR retrotranspo-
son (f8)  ARS (f9)  snoRNA (f10)  transposable element
gene (f11) and conservation score (f12). Five discrete fea-
tures turn out to be important including ncRNA  snRNA 
binding site  5’ UTR intron and snoRNA as well as one
continuous feature  i.e.  conservation score. These re-
sults agree with biological insights. For example  ncRNA 
snRNA and snoRNA are potentially important for gene
regulation since they are functional RNA molecules hav-
ing a variety of roles such as transcriptional regulation [18]. Also  conservation score would be
signiﬁcant since mutation in conserved region is more likely to result in phenotypic effects.

Figure 6: Learned weights of ω on the yeast
eQTL dataset.

f5 f6 f7 f8 f9
Features

f1 f2 f3 f4

f10 f11

f12

t

ω

0.1

0.12

0.08

0.06

0.04

0.02

0

3.5

2

0
2

3

 
*
 
s
t
i

a
r
t
 

d
e

t

i

a
c
o
s
s
a

 
f

o

 
r
e
b
m
u
N

2.5

2

1.5

1

0.5

0

 
0

 

β
ncRNA
snRNA
binding sites
five prime UTR intron
conservation scores

10

20

30

40

50

60

70

80

90

100

110

120

SNPs

Figure 7: Plot of 121 SNPs on chromosome 1 and 2 vs the number of genes affected by the SNPs from the
yeast eQTL analysis (blue bar). Five signiﬁcant prior knowledge on SNPs are overlapped with the plot. For
the four discrete priors (ncRNA  snRNA  binding site  5’ UTR intron) we set the value to 1 if annotated  0
otherwise. Binding sites and regions with no associated traits are denoted by long green and short blue arrows.

Fig. 7 shows the number of associated genes for SNPs on chromosome 1 and 2  superimposed on 5
signiﬁcant features. We see that association mapping results were affected by both priors and data.
For example  genomic region indicated by blue arrow shows weak association with traits  where
conservation score is low and no other annotations exist. Also we can see that three SNPs located on
binding sites affect a larger number of gene traits (see green arrows). As an example of biological
analysis  we investigate these three association SNPs. The three SNPs are located on telomeres
(chr1:483  chr1:229090  chr2:9425 (chromosome:coordinate))  and these genomic locations are in
cis to Abf1p (autonomously replicating sequence binding factor-1) binding sites. In biology  it is
known that Abf1p acts as a global transcriptional regulator in yeast [19]. Thus  the genomic regions
in telomeres would be good candidates for novel putative eQTL hotspots that regulate the expression
levels of many genes. They were not reported as eQTL hotspots in Yvert et al. [20].

6 Conclusions

In this paper  we proposed a novel regularized regression model  referred to as adaptive multi-task
Lasso  which takes into account multiple traits simultaneously while weights of different covariates
are learned adaptively from prior knowledge and data. Our simulation results support that our model
outperforms other methods via ℓ1 and ℓ1/ℓ2 penalty over multiple related genes  and especially
adaptively learned regularization signiﬁcantly improved the performance. In our experiments on the
yeast eQTL dataset  we could identify putative three eQTL hotspots with biological supports where
SNPs are associated with a large number of genes.

Acknowledgments

This work was done under a support from NIH 1 R01 GM087694-01  NIH 1RC2HL101487-01
(ARRA)  AFOSR FA9550010247  ONR N0001140910758  NSF Career DBI-0546594  NSF IIS-
0713379 and Alfred P. Sloan Fellowship awarded to E.X.

8

References
[1] R. Sladek  G. Rocheleau  J. Rung  C. Dina  L. Shen  D. Serre  P. Boutin  D. Vincent  A. Belisle 
S. Hadjadj  et al. A genome-wide association study identiﬁes novel risk loci for type 2 diabetes.
Nature  445(7130):881–885  2007.

[2] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society. Series B (Methodological)  58(1):267–288  1996.

[3] S.I. Lee  A.M. Dudley  D. Drubin  P.A. Silver  N.J. Krogan  D. Pe’er  and D. Koller. Learning

a prior on regulatory potential from eQTL data. PLoS Genetics  5(1):e1000358  2009.

[4] S. Kim and E. P. Xing. Statistical estimation of correlated genome associations to a quantitative

trait network. PLoS Genetics  5(8):e1000587  2009.

[5] G. Obozinski  B. Taskar  and M. Jordan. Multi-task feature selection. In Technical Report 

Department of Statistics  University of California  Berkeley  2006.

[6] M. Szafranski  Y. Grandvalet  and P. Morizet-Mahoudeaux. Hierarchical penalization. Ad-

vances in Neural Information Processing Systems  20:1457–1464  2007.

[7] H. Zou. The adaptive Lasso and its oracle properties. Journal of the American Statistical

Association  101(476):1418–1429  2006.

[8] S.I. Lee  V. Chatalbashev  D. Vickrey  and D. Koller. Learning a meta-level prior for feature
relevance from multiple related tasks. In Proceedings of the 24th International Conference on
Machine Learning  pages 489–496  2007.

[9] T. Park and G. Casella. The bayesian Lasso. Journal of the American Statistical Association 

103(482):681–686  2008.

[10] B. M. Marlin  M. Schmidt  and K. P. Murphy. Group sparse priors for covariance estimation. In
Proceedings of the 25th Conference on Uncertainty in Artiﬁcial Intelligence  pages 383–392 
2009.

[11] E. G´omez  M. A. Gomez-Viilegas  and J. M. Marin. A multivariate generalization of the
power exponential family of distributions. Communications in Statistics-Theory and Methods 
27(3):589–600  1998.

[12] H. Lee  A. Battle  R. Raina  and A. Y. Ng. Efﬁcient sparse coding algorithms. Advances in

Neural Information Processing Systems  19:801–808  2007.

[13] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the ℓ1-
ball for learning in high dimensions. In Proceedings of the 25th International Conference on
Machine Learning  pages 272–279  2008.

[14] J. Friedman  T. Hastie  and R. Tibshirani. A note on the group Lasso and a sparse group Lasso.

arXiv:1001.0736v1 [math.ST]  2010.

[15] T. T. Wu and K. Lange. Coordinate descent algorithms for Lasso penalized regression. Ann.

Appl. Stat  2(1):224–244  2008.

[16] R. B. Brem and L. Kruglyak. The landscape of genetic complexity across 5 700 gene expres-
sion traits in yeast. Proceedings of the National Academy of Sciences of the United States of
America  102(5):1572–1577  2005.

[17] S. Purcell  B. Neale  K. Todd-Brown  L. Thomas  M. A. R. Ferreira  D. Bender  J. Maller 
P. Sklar  P. I. W. De Bakker  M. J. Daly  et al. PLINK: a tool set for whole-genome association
and population-based linkage analyses. The American Journal of Human Genetics  81(3):559–
575  2007.

[18] G. Storz. An expanding universe of noncoding RNAs. Science  296(5571):1260–1263  2002.
[19] T. Miyake  J. Reese  C. M. Loch  D. T. Auble  and R. Li. Genome-wide analysis of ARS (au-
tonomously replicating sequence) binding factor 1 (Abf1p)-mediated transcriptional regulation
in Saccharomyces cerevisiae. Journal of Biological Chemistry  279(33):34865–34872  2004.
[20] G. Yvert  R. B. Brem  J. Whittle  J. M. Akey  E. Foss  E. N. Smith  R. Mackelprang 
L. Kruglyak  et al. Trans-acting regulatory variation in Saccharomyces cerevisiae and the
role of transcription factors. Nature Genetics  35(1):57–64  2003.

9

,Devansh Arpit
Ifeoma Nwogu
Venu Govindaraju