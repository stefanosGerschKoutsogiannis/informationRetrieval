2018,FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification,Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset  given one image of the person of interest. For learning robust person features  the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment  or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue  a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators  a novel same-pose loss is also integrated  which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance  no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets  which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.,FD-GAN: Pose-guided Feature Distilling GAN for

Robust Person Re-identiﬁcation

Yixiao Ge1∗

Zhuowan Li2 3∗†

Haiyu Zhao2

Shuai Yi2

Xiaogang Wang1

Guojun Yin2 4†

Hongsheng Li1 ‡

1CUHK-SenseTime Joint Laboratory  The Chinese University of Hong Kong

2SenseTime Research

3Johns Hopkins University

4University of Science and Technology of China

{yxge@link  hsli@ee  xgwang@ee}.cuhk.edu.hk

{zhaohaiyu  yishuai}@sensetime.com

zli110@jhu.edu

gjyin@mail.ustc.edu.cn

Abstract

Person re-identiﬁcation (reID) is an important task that requires to retrieve a
person’s images from an image dataset  given one image of the person of interest.
For learning robust person features  the pose variation of person images is one of
the key challenges. Existing works targeting the problem either perform human
alignment  or learn human-region-based representations. Extra pose information
and computational cost is generally required for inference. To solve this issue 
a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for
learning identity-related and pose-unrelated representations. It is a novel framework
based on a Siamese structure with multiple novel discriminators on human poses
and identities. In addition to the discriminators  a novel same-pose loss is also
integrated  which requires appearance of a same person’s generated images to
be similar. After learning pose-unrelated person features with pose guidance  no
auxiliary pose information and additional computational cost is required during
testing. Our proposed FD-GAN achieves state-of-the-art performance on three
person reID datasets  which demonstrates that the effectiveness and robust feature
distilling capability of the proposed FD-GAN. ‡‡

1

Introduction

Person re-identiﬁcation (reID) is a challenging task  with the purpose of matching pedestrian images
with the same identity across multiple cameras. With the wide usage of deep learning methods 
reID performances by different algorithms increase rapidly. There are various attempts on learning
representations with deep neural networks  however  posture variations  blur and occlusion still pose
great challenges for learning discriminative features. Two types of methods were used for addressing
the issues  aligning pedestrian images [1] or integrating human pose information by learning body-
region features [2]. However  these works also require auxiliary pose information in the inference
stage  which limits the generalization of the algorithms to new images without pose information.
Meanwhile  the computational cost increases due to more complicated inference of pose estimation.

∗The ﬁrst two authors contribute equally to this work.
†This work was done when they were interns at SenseTime Research.
‡Hongsheng Li is the corresponding author.
‡‡The code is now available. https://github.com/yxgeee/FD-GAN

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: The image encoder in the FD-GAN is trained to learn robust identity-related and pose-
unrelated representations with assistance of pose-guided image generator and discriminators. During
inference  it does not need pose information and additional computational cost.

Generative adversarial network (GAN) is gaining increasing attention for image generation. Recently 
some works exploited GANs’ potential on aiding current person reID algorithms. Zheng et al. [3]
proposed a semi-supervised structure which learns generative images with label smoothing regulariza-
tion for outliers (LSRO) regularization. PTGAN [4] was proposed to bridge the domain gap between
separate datasets. In addition to image synthesis  GAN can be used for representation learning as
well. In this work  we propose a novel identity-related representation learning framework for robust
person re-identiﬁcation.
The proposed Feature Distilling Generative Adversarial Network (FD-GAN) maintains identity
feature consistency under pose variation without increasing the complexity of inference (illustrated in
Figure 1). It adopts a Siamese structure for feature learning. Each of the branch consists of an image
encoder and an image generator. The image encoder embeds person visual features given the input
images. The image generator generates new person images conditioned on the pose information and
the input person features by the encoder. Multiple discriminators are integrated in the framework to
distinguish inter-branch and intra-branch relations between generated images by the two branches.
The proposed identity discriminator  the pose discriminator  and the veriﬁcation classiﬁer together
with a reconstruction loss and a novel same-pose loss jointly regularizes the feature learning process
for achieving robust person reID. With the adversarial losses  identity-irrelevant information  such
as pose and background appearance  in the input image is mitigated from the visual features by
the image encoder. More importantly  during inference  additional pose information is no longer
needed and saves additional computational cost. Our method outperforms previous works in three
widely-used reID datasets  i.e. Market-1501 [5]  CUHK03 [6] and DukeMTMC-reID [7] datasets.
Overall  this paper has the following contributions. 1) We propose a novel framework  FD-GAN 
to learn identity-related and pose-unrelated representations for person re-identiﬁcation with pose-
variation. Unlike existing alignment or region-based learning methods  our framework does not
require extra auxiliary pose information or increase the computational complexity during inference.
2) Although person image generation is an auxiliary task for our framework  the generated person
images by our proposed method show better quality than existing speciﬁc person-generation methods.
3) The proposed FD-GAN achieves state-of-the-art re-identiﬁcation performance on Market-1501 [5] 
CUHK03 [6]  and DukeMTMC-reID [7] datasets.

2 Related Work

Generative Adversarial Network (GAN). Goodfellow et al. [8] ﬁrst introduced the adversarial
process to learn generative models. The GAN is generally composed of a generator and a discrimi-
nator  where the discriminator attempts to distinguish the generated images from real distribution
and the generator learns to fool the discriminator. A set of constraints are proposed in previous
works [9  10  11  12] to improve the training process of GANs  e.g.  interpretable representations are
learned by using additional latent code in [12]. GAN-based algorithms shows excellent performance
in image generation [13  14  15  16  17]. In terms of person image generation  PG2 was proposed
to synthesize person images in arbitrary poses in [18]. Siarohin et al. [19] designed a single-stage
approach with deformable skip connections in the generator for better deformable human generation.
Zanﬁr et al. [20] transferred the appearance from the source image onto the target image while
preserving the target shape and clothing segmentation layout. In contrast  our method aims at learn-
ing person features for person reID with assistance of GANs. Apart from person image synthesis 
pose-disentangled representations were learned for face recognition by DR-GAN [21]  which has key
differences with our method. Our experimental results show that our proposed FD-GAN performs

2

EncoderDiscriminatorsVerificationGeneratorPerson ImagesPose & NoiseInferenceTrainPerson FeaturesFigure 2: The Siamese structure of the proposed FD-GAN. Robust identity-related and pose-unrelated
features are learned by the image encoder E with a veriﬁcation loss and the auxiliary task of
generating fake images to fool identity and pose discriminators. A novel same-pose loss term is
introduced to further encourage learning identity-related and pose-unrelated visual features.

better than DR-GAN on person reID. Our main goal is to decompose the pose information from the
image features via adversarial training for learning identity-related and pose-unrelated representation.
Person Re-identiﬁcation (ReID). Person reID [22  23  24  25  26  27  28  29] is a challenging task
due to various human poses  domain differences  occlusions  etc. Two main types of methods were
adopted in previous works  i.e. learning discriminative person representations [30  31  32] and metric
learning [25  33  34  35]. PAN [1] aligns pedestrians and learn person features simultaneously without
any extra annotation. Zhao et al. [2] proposed SpindleNet for learning person features of different
body regions with additional human pose information. Most recent methods [1  25  33  34  35  36  37 
38  39] designed more complicated frameworks to learn more robust representations with increasing
computational cost or requiring extra information during inference.
Inspired by the excellent performances of GAN-based structures for image generation  there were
previous works [3  21  4] begining to design GAN-based algorithms to improve veriﬁcation perfor-
mance of person reID. Zheng et al. [3] introduced a semi-supervised pipeline for jointly training
generated images and real images from training dataset by the proposed LSRO method for regulariz-
ing unlabelled data. PTGAN [4] was proposed to bridge the domain gap between separate person
reID datasets. Due to the challenges from pose diversity for person reID datasets  we propose an
novel GAN-based framework for distilling identity-related features.

3 Feature Distilling Generative Adversarial Network

Our proposed Feature Distilling Generative Adversarial Network (FD-GAN) aims at learning identity-
related and pose-unrelated person representations  in order to handle large pose variations across
images in person reID.
The overall framework of our proposed method is shown in Fig. 2. The proposed FD-GAN adopts
a Siamese structure  including an image encoder E  an image generator G  an identity veriﬁcation
classiﬁer V and two adversarial discriminators  i.e.  the identity discriminator Did and the pose
discriminator Dpd. For each branch of the network  it takes a person image and a target pose
landmark map as inputs. The image encoder E at each branch ﬁrst transforms the input person
image into feature representations. An identity veriﬁcation classiﬁer is utilized to supervise the
feature learning for person reID. However  using only the veriﬁcation classiﬁer makes the encoder
generally encode not only person identity information but also person pose information  which makes
the learned features sensitive to person pose variation. To make the learned features robust and

3

noise zIdentity lossIdentity lossPose lossVerification lossPose lossInput Image 1Input Image 2Target PoseFake Image 1Fake Image 2ImageEncoderPoseEncoderGeneratorGeneratorPoseDiscriminatorIdentityDiscriminatorPoseDiscriminatorVerificationClassifierImageEncoderIdentityDiscriminatorSame-pose lossVisual FeaturesVisualFeaturesFigure 3: Network structures of (a) the generator G and the image encoder E  (b) the veriﬁcation
classiﬁer V   (c) the identity discriminator Did  (d) the pose discriminator Dpd.

eliminate pose-related information  we added an image generator G conditioned on the features from
the encoder and a target pose map. The assumption is intuitive  if the learned person features are
pose-unrelated and identity-related  then it can be used to accurately generate the same person’s
image but with different target poses. An identity discriminator Did and a pose discriminator
Dpd are integrated to regularize the image generation process. Both Did and Dpd are conditional
discriminators that classify whether the input image is real or fake conditioned on the input identity
or pose. They are not used to classify different identities and poses. The image generator together
with the image encoder are encouraged to fool the discriminators with fake generated images. Taking
advantages of the Siamese structure  a novel same-pose loss minimizing the difference between
the fake generated images of the two branches is also utilized  which is shown to further distill
pose-unrelated information from input images. The entire framework is joint trained in an end-to-end
manner. For inference  only the image encoder E is used without auxiliary pose information.

3.1

Image encoder and image generator

The structures of the image encoder E and image generator G are illustrated in Figure 3(a). Given
an input image x  the image encoder E utilizes ResNet-50 as backbone network to encode the input
image into a 2048-dimensional feature vector. The image generator G takes the encoded person
features and target pose map as inputs  and aims at generating another image of the same person
speciﬁed by the target pose. The target pose map is represented by an 18-channel map  where
each channel represents the location of one pose landmark’s location and the one-dot landmark
location is converted to a Gaussian-like heat map. It is encoded by a 5-block Convolution-BN-ReLU
sub-network to obtain a 128-dimensional pose feature vector. The visual features  target pose features 
and an additional 256-dimensional noise vector sampled from standard Gaussian distribution are then
concatenated and input into a series of 5 convolution-BN-dropout-ReLU upsampling blocks to output
the generated person images.

3.2

Identity veriﬁcation classiﬁer

Given the visual features of the two input images from the image encoder  the identity veriﬁcation
classiﬁer V determines whether the two images belong to the same person. Person identity veriﬁcation
is the main task for person re-identiﬁcation and ensures learned features to capture identity information
of person images. The structure of the classiﬁer is shown in Figure 3(b)  which takes visual features of
two person images as inputs and feeds them through element-wise subtraction  element-wise square 
a batch normalization layer  a fully-connected layer  and ﬁnally a sigmoid non-linearity function to
output the probability that the input image pair belongs to the same person. This classiﬁer is trained
with binary cross-entropy loss. Let x1  x2 represent the two input person images  and d(x1  x2)
represents the output same-person conﬁdence score by our sub-network. The identity veriﬁcation
classiﬁer V is trained with the following binary cross-entropy loss 

Lv = −C log d(x1  x2) − (1 − C)(1 − log d(x1  x2)) 

(1)

4

ResNetpxzCypyCPatchGAN0/1ResNetResNetxy-BN0/1Sigmoid(a) Image Encoder  Pose Encoder  Generator(c) Identity Discriminator(d) Pose DiscriminatorConv-BN-ReLU blockConv-BN-Dropout-ReLU blockImage EncoderPose EncoderGenerator-BN0/1Sigmoid(b) Verification ClassifierSquareSquareInput ImageTarget PoseNoise VectorFake ImageVisual Features 1Visual Features 2Input ImageFake/Real ImageFake/Real ImageTarget PoseImage-pose Matching MapSamePerson?FCFCSamePerson?where C is the ground-truth label. C = 1 if x1  x2 belong to the same person and C = 0 otherwise.

3.3

Image generation with identity and pose discriminators

(cid:16)Ey(cid:48)

2(cid:88)

k=1

Lid = max

Did

k∈Y [log Did(xk  y(cid:48)

To regularize the image encoder E to learn only identity-related information  the following person
image generator G is trained with the identity discriminator Did and the pose discriminator Dpd to
generate person images with target poses. Given the input image xk (k = 1 or 2 for two branches)
and the target pose p  the generated image yk is required to have the same person identity with xk but
with the target pose p. The identity discriminator is utilized to maintain identity-related information in
the encoded visual features  while the pose discriminator aims to eliminate pose-related information
from the features.
Identity discriminator Did is trained to distinguish whether the generated person image and the
input person image of the same branch belong to the same person. The image generator would try to
fool the identity discriminator to ensure the encoded visual feature contains sufﬁcient identity-related
information. The identity discriminator sub-network has a similar network structure (see Figure 3(c))
to the identity veriﬁcation classiﬁer V . However  its ResNet-50 sub-network for visual feature
encoding does not share weights with that of our image encoder E  because the identity discriminator
Did aims at distinguishing the identity between the real/fake images  while our image encoder targets
at learning pose-unrelated person features. There is domain gap between the two tasks and sharing
weights hinders feature learning process of the image encoder. Such an argument is supported by our
experiments. Let y(cid:48)
k represent the real person image having the same identity with input image xk
and the target pose p. The adversarial loss of the identity discriminator Di can then be deﬁned as

(cid:17)
k)] + Eyk∈Z [log(1 − Did(xk  yk))]

 

(2)

where Y and Z represent the true data distribution and generated data distribution by the image
generator G.
Pose discriminator Dpd is proposed to distinguish whether the generated person image yk (for k = 1
or 2) matches the given target pose p. The sub-network structure of pose discriminator is shown in
Figure 3(d). It adopts the PatchGAN [40] structure. The input image and pose map (after Gaussian-
like heat-map transformation) is ﬁrst concatenated along the channel dimension and then processed by
4 convolution-ReLU blocks and a sigmoid non-linearity to obtain an image-pose matching conﬁdence
map with values between 0 and 1. Each location of the conﬁdence map represents the matching
degree between the input person image and the pose landmark map. The image generator G would try
to fool the pose discriminator Dpd to obtain high matching conﬁdences with fake generated images.
The adversarial loss of Ddp is then formulated as
k∈Y [log Dpd([p  y(cid:48)

(cid:17)
k])] + Eyk∈Z [log(1 − Dpd([p  yk]))]

(cid:16)Ey(cid:48)

Lpd = max

(3)

 

Dpd

2(cid:88)

k=1

where Dpd utilizes the concatenated person image and pose landmark map as inputs.
However  we observe that the pose discriminator Dpd might overﬁt the poses  i.e.  Dpd might
remember the correspondences between speciﬁc poses and person appearances  because each image’s
pose is generally unique. For instance  if we use a blue-top person’s pose as the target pose  the
generated image of a red-top person might end up having blue top. To solve this problem  we propose
an online pose map augmentation scheme. During training  for each pose landmark  its 1-channel
Gaussian-like heat-map is obtained with a random Gaussian bandwidth in some speciﬁc range. In
this way  we can create many pose maps for the same pose and mitigate the pose overﬁtting problem.
Reconstruction loss. The responsibility of G is not only confusing the discriminators  but also
generating images that are similar to the ground-truth images. However  the discriminators alone
cannot guarantee generating human-perceivable images. Therefore  a reconstruction loss is introduced
to minimize the L1 differences between the generated image yk and its corresponding real image y(cid:48)
k 
which is shown to be helpful for more stable convergence of training the generator.

2(cid:88)

k=1

Lr =

(cid:107)yk − y(cid:48)

k(cid:107)1 

1
mn

5

(4)

k for an input image xk and a target pose p  this loss is not utilized.

where mn is the number of pixels in the real/fake images. When there is no corresponding ground-
truth image y(cid:48)
Same-pose loss. The purpose of the image generator G is to help the image encoder distill only
pose-unrelated information. We input the same person’s two different images and the same target pose
to both branches of our Siamese network  if the conditioning visual features in the two branches are
truly only identity-related  then the two generated images should be similar in appearance. Therefore 
we propose a same-pose loss to minimize the differences between the two generated images of the
same person and with the target pose 

Lsp =

1
mn

(cid:107)y1 − y2(cid:107)1 

(5)

which encourages the learned visual features from E of the two input images to only be identity-
related while ignoring other factors.
The overall training objective. The above mentioned classiﬁer loss  discriminator losses and recon-
struction losses work collaboratively for learning identity-related and pose-unrelated representations.
The overall loss function could be deﬁned by

L = Lv + λidLid + λpdLpd + λrLr + λspLsp 

(6)

where λid  λpd  λr  λsp are the weighting factors for the auxiliary image generation task.

3.4 Training scheme

There are three stages for training our proposed framework. In the ﬁrst stage  our Siamese baseline
model  which includes only the image encoder E and identity veriﬁcation classiﬁer V   is pretrained
on a person reID dataset with only identity cross-entropy loss Lv in Eq. (1). The pre-trained network
weights are then used to initialize E  V   and identity discriminator Did in stage-II. In the second stage 
parameters of E and V are ﬁxed. We then train G  identity discriminator Did  and pose discriminator
Dpd with the overall objective L in Eq. (6). Finally  the whole network is ﬁnetuned jointly in an
end-to-end manner. For each training mini-batch  it contains 128 person image pairs  with 32 of them
belonging to same persons (positive pairs) and 96 of them belonging to different persons (negative
pairs). All images are resized to 256 × 128. The Gaussian bandwidth for obtaining pose landmark
heat-map is uniformly sampled in [4  6].
In training stages II and III  the discriminators and other parts of the network are alternatively
optimized. When jointly optimizing the generator G  the image encoder E and the veriﬁcation
classiﬁer V   the overall objective Eq. (6) is used. When optimizing the discriminators Did and Dpd 
only adversarial losses Lid and Lpd are adopted.
Stage I: ReID baseline pretraining. Our Siamese baseline only includes the image encoder E
and identity veriﬁcation classiﬁer V . The ResNet-50 sub-network is ﬁrst initialized with ImageNet-
pretrained weights [41]. The network is optimized by Stochastic Gradient Descent (SGD) with
momentum 0.9. The initial learning rates are set to 0.01 for E and 0.1 for V   and they are decreased
to 0.1 of their previous values every 40 epochs. The stage-I training process iterates for 80 epochs.
Stage II: FD-GAN pretraining. With E and V ﬁxed  We integrate G  Did  and Dpd into the
framework in stage-II. Adam optimizer is adopted for optimizing G and SGD for Did and Dpd.
The initial learning rates for G  Did  Dpd are set as 10−3  10−4  10−2  respectively. Learning rates
maintain the same for the ﬁrst 50 epochs  and then gradually decrease to 0 in the following 50 epochs.
The loss weights are set as λid = 0.1  λpd = 0.1  λr = 10  λsp = 1. We took the label smoothness
scheme [42] for better balancing between the generator and the discriminator.
Stage III: Global ﬁnetuning. For ﬁnetuning the whole framework end-to-end  we use Adam for
optimizing E  G and V   and SGD for Did  Dpd after loading the pre-trained weights from stage-II.
Speciﬁcally  the initial learning rates are set to 10−6  10−6  10−5  10−4  10−4 for E  G  V   Did  Dpd 
respectively. Learning rates remain the same for the ﬁrst 25 epochs  and then gradually decay to 0 in
the following 25 epochs. Batch normalization layers in E is ﬁxed to achieve better performance. For
loss weights  λid = 0.1  λpd = 0.1  λr = 10  λsp = 1 are set as the weights for different loss terms.
§

§We tune hyperparameters on the validation set of Market-1501 [5]  and directly use the same hyperparame-

ters for DukeMTMC-reID [7] and CUHK03 [6] datasets.

6

Table 1: Component analysis of the proposed FD-GAN on Market-1501 [5] and DukeMTMC-
reID [7] datasets in terms of top-1 accuracy (%) and mAP (%)

Networks

not share E

baseline (single)
baseline (Siamese)
Siamese DR-GAN[21]
FD-GAN (share E)
FD-GAN (no sp.)
FD-GAN (no veri.)
FD-GAN (no sp. & no veri.)
FD-GAN (no Dpd)
FD-GAN (no Did)
FD-GAN (no Did & Dpd)
FD-GAN (no pose aug.)
FD-GAN

n/a
n/a
×
×
√
√
√
√
√
√
√
√

Components

Lsp Lv
√
n/a
n/a
√
n/a
×
√
√
√
×
√
×
×
×
√
√
√
√
√
√
√
√
√
√

Lpd Lid
n/a
n/a
√
√
n/a
n/a
√
√
√
√
√
√
√
√
√
×
√
×
×
×
√
√
√
√

pose map aug.

n/a
√
n/a
√
√
√
√
√
√
√
×
√

Market-1501[5]
top-1
mAP
81.4
59.8
88.2
72.5
73.2
86.7
86.8
73.5
88.9
75.8
89.5
75.7
88.7
74.4
88.0
73.0
72.8
89.2
84.6
71.6
89.5
77.2
77.7
90.5

DukeMTMC-reID[7]
mAP
40.7
61.3
60.2

top-1
62.5
78.2
76.9

62.6
62.4

78.8
78.6

63.9
64.5

79.5
80.0

-
-

-
-
-

-
-

-
-
-

3.5 Comparison to DR-GAN [21]

There is an existing work  DR-GAN [21] based on conditional GAN [43]  which tries to learn pose-
invariant identity representations for face recognition. It also adopts an encoder-decoder structure
with a discriminator for classifying both identity. Comparison results in Section 4.2 demonstrate the
advantages of our proposed method over DR-GAN on the person reID task.
This is because there are three key differences between the proposed FD-GAN and DR-GAN  which
make our algorithm superior. 1) We adopt a Siamese network structure  which enables us to use the
same-pose loss to encourage encoding only learning identity-related information  while DR-GAN
does not have such a loss term. 2) We do not share the weights between the ResNet-50 networks in
the image encoder and in the identity discriminator. We observe that identity veriﬁcation and real/fake
image identity discrimination are two tasks in different domains and therefore their weights should
not be shared. 3) Our Siamese structure utilizes a veriﬁcation classiﬁer instead of a cross-entropy
classiﬁer  which shows better person reID performance than a single-branch network does.

4 Experiments

4.1 Datasets and evaluation metrics

In this paper  three datasets are used for performance evaluation  including Market-1501 [5] 
CUHK03 [6]  and DukeMTMC-reID [7]. The Market-1501 dataset [5] consists of 12 936 im-
ages of 751 identities for training and 19 281 images of 750 identities in the gallery set for testing.
The CUHK03 dataset [6] contains 14 097 training images of 1 467 identities captured from two
cameras. The original training and testing protocol is used. The DukeMTMC-reID dataset [7] is
a subset of the pedestrian tracking dataset DukeMTMC for image-based reID. It contains 16 522
images of 702 identities for training. Mean average precision (mAP) and CMC top-1 accuracy are
adopted for performance evaluation on all the three datasets.

4.2 Component analysis of the proposed FD-GAN

In this section  component analysis is conducted to demonstrate the effectiveness of components
in the FD-GAN framework  including the Siamese structure  and the use of veriﬁcation classiﬁer
and same-pose loss. We also compare with DR-GAN [21]  which also proposes to learning pose
disentangled features. Our Siamese baseline model is only the ResNet-50 image encoder E with our
identity veriﬁcation classiﬁer V . The analysis is conducted on Market-1501 [5] and DukeMTMC-reID
[7] datasets and the results are shown in Table 1.
Siamese structure. We ﬁrst compare the our Siamese reID baseline (denoted as baseline (Siamese))
with the single branch ResNet-50 [44] baseline trained with cross-entropy loss on person IDs (denoted
as baseline (single)). The Siamese baseline outperforms single-branch baseline by 12.7% and 20.6%
in terms of mAP on the two datasets.
Proposed FD-GAN  with online pose map augmentation and adversarial discriminators. Based
on the Siamese structure  we build our proposed FD-GAN framework. We can observe that the

7

Table 2: Experimental comparison of the proposed approach with state-of-the-art methods on
Market-1501 [5]  CUHK03 [6]  and DukeMTMC-reID [7] datasets. Top-1 accuracy(%) and mAP(%)
are reported.

Methods

BoW+KISSME [5]
LOMO+XQDA [37]
OIM Loss [45]
MSCAN [39]
DCA [39]
SpindleNet [2]
k-reciprocal [46]
VI+LSRO [3]
Basel+LSRO [3]
OL-MANS [47]
PA [48]
SVDNet [49]
JLML [50]
Proposed FD-GAN

Market-1501 [5]

mAP

top-1

-
-

-

-
-

60.9
53.1
57.5

63.6
66.1

63.4
62.1
65.5
77.7

-
-

82.1
76.3
80.3
76.9
77.1
84.0

-

60.7
81.0
82.3
85.1
90.5

CUHK03 [6]

mAP

72.5

-
-

-
-
-

-
-
-

-

67.6
87.4

84.8

91.3

top-1

-
-

77.5
74.2
74.2
88.5
61.6
84.6

-

61.7
85.4
81.8
83.2
92.6

DukeMTMC-reID [7]
top-1
mAP
12.1
25.1
30.8
17.0
47.4
68.1

-
-
-
-
-

-
-

-

47.1

56.8

64.5

-
-
-
-
-

-
-

-

67.7

76.7

80.0

proposed FD-GAN gains signiﬁcant improvements from our Siamese baseline in terms of both mean
AP and top-1 accuracy on both two reID datasets. There are 5.2% and 3.2% mAP improvements
in terms of mAP on the two datasets. To show the effectiveness of our proposed online pose map
augmentation  we test removing it when training our FD-GAN (denoted as FD-GAN w/o pose aug.
in Table 1). It results in a .5% performance drop for both datasets. In order to validate the effects
of the two discriminators Did and Dpd  we test removing them separately and together (denoted as
FD-GAN w/o Did or Dpd. in Table 1). It results in not only obviously performance drop  but also
poorer generated images.
DR-GAN [21]  veriﬁcation loss  same-pose loss  and not sharing image encoder. We also study
the effectiveness of using veriﬁcation loss and same-pose loss  and not sharing image encoder weights
to identity discriminator. Original DR-GAN’s pose discriminator classiﬁes each face image into
one of 13 poses. For fair comparison  we ﬁrst test integrating DR-GAN into our Siamese baseline
(denoted as Siamese DR-GAN)  which could be viewed our FD-GAN without the same-pose loss and
also sharing weights between E and Did. Since our network uses pose map as input condition  we
use our conditional pose discriminator Dpd to replace DR-GAN’s pose discriminator. The Siamese
DR-GAN even performs worse than our Siamese baseline on the DukeMTMC-reID dataset. Our
proposed FD-GAN outperforms it by over 4% mAP on both datasets. We also try removing both
veriﬁcation classiﬁer and same-pose loss (denoted as FD-GAN w/o sp. & veri.)  removing only
identity veriﬁcation classiﬁer (denoted as FD-GAN w/o veri.)  removing only same-pose loss (denoted
as FD-GAN w/o sp.) from our proposed FD-GAN and only sharing weights between E and Did
(denoted as FD-GAN share E) . Results in Table 1 show that both the veriﬁcation loss and same-pose
loss are indispensable to achieve superior performance on person reID. Also  not sharing weights
between E and Did results in better performance.

4.3 Comparison with state-of-the-arts

We compare our proposed FD-GAN with the state-of-the-art person reID methods including
VI+LSRO [3]  JLML [50]  PA [48]  etc. on the three datasets  Market-1501 [5]   CUHK03 [6]
  and DukeMTMC-reID [7]. The results are listed in Table 2. Note that only single query results from
published papers are compared in order to make a fair comparison.
By ﬁnetuning the FD-GAN based on ResNet-50 [44] baseline network structure  our proposed FD-
GAN outperforms previous approaches and achieves state-of-the-art performance. We can achieve
90.5% top-1 accuracy and 77.7% mAP on the Market-1501 dataset [5]  92.6% top-1 accuracy and
91.3% mAP on CUHK03 dataset [6]  and 80.0% top-1 accuracy and 64.5% mAP on the DukeMTMC-
reID dataset [7]  which demonstrates the effectiveness of the proposed feature distilling FD-GAN.

4.4 Person image generation and visual analysis

Comparison of person image generation [18  19]. Although generating person images is only an
auxiliary task in our FD-GAN to learn more robust person features. We are interested in comparing

8

Input Pose GT

[18]

[19] Ours

Input Pose GT

[18]

[19] Ours

Input Pose GT

[18]

[19] Ours

(a)

(b)

Figure 4: (a) Generated person images by our proposed method and [18  19] on test images of Market-
1501 dataset [5]. (b) Two examples of the generated images from the Market-1501 dataset [5]. (First
row) the ground-truth images of target poses. (Second-third rows) input images and the generated
images with different target poses on training images of Market-1501 dataset [5].

the generated images with images by other speciﬁcally designed person generation methods [18  19].
Figure 4(a) shows the generated person images by state-of-the-art person generation methods [18  19]
and our FD-GAN. One can clearly see that our proposed method better understand the concept of
“backpack” and could generate correct upper and lower body clothes. We argue that the key is using
person identity supervisions to make the encoder learn better identity-related features. Our Siamese
structure and the same-pose loss also contribute to achieving consistent generation results.
Visualization for learned features. The proposed FD-GAN framework not only improves the
discriminative capability of visual features but could also be used as a visualization tool for manually
examining learned feature representations. The quality of learned person features have direct impact
on the generated person images. We can therefore tell what aspects of person appearances are
captured by the features. For instance  for “input 1_b” in Figure 4(b)  its generated frontal images
do not show colored pattern on the upper body but only the general colors and shapes of the upper
and lower bodies  which might demonstrate that the learned image encoder focus on embedding the
overall appearances of persons but fail to capture the distinguishable details in appearance.

5 Conclusion

In this paper  we proposed the novel FD-GAN for learning identity-related and pose-unrelated
person representations with human pose guidance. Novel Siamese network structure as well as
novel losses ensure the framework learns more pose-invariant features for robust person reID. Our
proposed framework achieves state-of-the-art performance on person reID without using additional
computational cost or extra pose information during inference. The generated person images also
show higher quality than existing speciﬁc person-generation methods.

Acknowledgements

This work is supported by SenseTime Group Limited  the General Research Fund sponsored
by the Research Grants Council of Hong Kong (Nos. CUHK14213616  CUHK14206114 
CUHK14205615  CUHK14203015  CUHK14239816  CUHK419412  CUHK14207814 
CUHK14208417  CUHK14202217) 
the Hong Kong Innovation and Technology Support
Program (No. ITS/121/15FX).

9

References
[1] Zheng  Z.  Zheng  L.  Yang  Y.: Pedestrian alignment network for large-scale person re-

identiﬁcation. TCSVT (2018)

[2] Zhao  H.  Tian  M.  Sun  S.  Shao  J.  Yan  J.  Yi  S.  Wang  X.  Tang  X.: Spindle net: Person
re-identiﬁcation with human body region guided feature decomposition and fusion. In: CVPR.
(2017)

[3] Zheng  Z.  Zheng  L.  Yang  Y.: Unlabeled samples generated by gan improve the person

re-identiﬁcation baseline in vitro. ICCV (2017)

[4] Wei  L.  Zhang  S.  Gao  W.  Tian  Q.: Person transfer gan to bridge domain gap for person

re-identiﬁcation. CVPR (2018)

[5] Zheng  L.  Shen  L.  Tian  L.  Wang  S.  Wang  J.  Tian  Q.: Scalable person re-identiﬁcation: A

benchmark. In: CVPR. (2015)

[6] Li  W.  Zhao  R.  Xiao  T.  Wang  X.: Deepreid: Deep ﬁlter pairing neural network for person

re-identiﬁcation. In: CVPR. (2014)

[7] Ristani  E.  Solera  F.  Zou  R.  Cucchiara  R.  Tomasi  C.: Performance measures and a data set

for multi-target  multi-camera tracking. In: ECCV. (2016)

[8] Goodfellow  I.  Pouget-Abadie  J.  Mirza  M.  Xu  B.  Warde-Farley  D.  Ozair  S.  Courville  A. 

Bengio  Y.: Generative adversarial nets. In: NIPS. (2014) 2672–2680

[9] Huang  X.  Li  Y.  Poursaeed  O.  Hopcroft  J.  Belongie  S.: Stacked generative adversarial

networks. In: CVPR. (2017)

[10] Nowozin  S.  Cseke  B.  Tomioka  R.:

f-gan: Training generative neural samplers using

variational divergence minimization. In: NIPS. (2016)

[11] Radford  A.  Metz  L.  Chintala  S.: Unsupervised representation learning with deep convolu-

tional generative adversarial networks. arXiv preprint arXiv:1511.06434 (2015)

[12] Chen  X.  Duan  Y.  Houthooft  R.  Schulman  J.  Sutskever  I.  Abbeel  P.: Infogan: Interpretable
representation learning by information maximizing generative adversarial nets. In: NIPS. (2016)
2172–2180

[13] Li  J.  Liang  X.  Wei  Y.  Xu  T.  Feng  J.  Yan  S.: Perceptual generative adversarial networks

for small object detection. In: CVPR. (2017)

[14] Bousmalis  K.  Silberman  N.  Dohan  D.  Erhan  D.  Krishnan  D.: Unsupervised pixel-level

domain adaptation with generative adversarial networks. In: CVPR. (2017)

[15] Kaneko  T.  Hiramatsu  K.  Kashino  K.: Generative attribute controller with conditional ﬁltered

generative adversarial networks. In: CVPR. (2017)

[16] Nguyen  V.  Yago Vicente  T.F.  Zhao  M.  Hoai  M.  Samaras  D.: Shadow detection with

conditional generative adversarial networks. In: ICCV. (2017)

[17] Zhang  H.  Xu  T.  Li  H.  Zhang  S.  Wang  X.  Huang  X.  Metaxas  D.N.: Stackgan: Text to
photo-realistic image synthesis with stacked generative adversarial networks. In: ICCV. (2017)

[18] Ma  L.  Jia  X.  Sun  Q.  Schiele  B.  Tuytelaars  T.  Van Gool  L.: Pose guided person image

generation. In: NIPS. (2017) 405–415

[19] Siarohin  A.  Sangineto  E.  Lathuiliere  S.  Sebe  N.: Deformable gans for pose-based human

image generation. CVPR (2017)

[20] Zanﬁr  M.  Popa  A.I.  Zanﬁr  A.  Sminchisescu  C.: Human appearance transfer. In: CVPR.

(2018)

[21] Tran  L.  Yin  X.  Liu  X.: Disentangled representation learning gan for pose-invariant face

recognition. In: CVPR. (2017)

10

[22] Wu  A.  Zheng  W.S.  Yu  H.X.  Gong  S.  Lai  J.: Rgb-infrared cross-modality person re-

identiﬁcation. In: ICCV. (2017)

[23] Su  C.  Li  J.  Zhang  S.  Xing  J.  Gao  W.  Tian  Q.: Pose-driven deep convolutional model for

person re-identiﬁcation. In: ICCV. (2017)

[24] Chung  D.  Tahboub  K.  Delp  E.J.: A two stream siamese convolutional neural network for

person re-identiﬁcation. In: ICCV. (Oct 2017)

[25] Yu  H.X.  Wu  A.  Zheng  W.S.: Cross-view asymmetric metric learning for unsupervised

person re-identiﬁcation. In: ICCV. (2017)

[26] Shen  Y.  Li  H.  Xiao  T.  Yi  S.  Chen  D.  Wang  X.: Deep group-shufﬂing random walk for

person re-identiﬁcation. In: CVPR. (2018)

[27] Shen  Y.  Xiao  T.  Li  H.  Yi  S.  Wang  X.: End-to-end deep kronecker-product matching for

person re-identiﬁcation. In: CVPR. (2018)

[28] Shen  Y.  Li  H.  Yi  S.  Chen  D.  Wang  X.: Person re-identiﬁcation with deep similarity-guided

graph neural network. (2018)

[29] Chen  D.  Xu  D.  Li  H.  Sebe  N.  Wang  X.: Group consistent similarity learning via deep crf

for person re-identiﬁcation. In: CVPR. (2018)

[30] Xiao  T.  Li  H.  Ouyang  W.  Wang  X.: Learning deep feature representations with domain

guided dropout for person re-identiﬁcation. In: CVPR. (2016)

[31] Cheng  D.  Gong  Y.  Zhou  S.  Wang  J.  Zheng  N.: Person re-identiﬁcation by multi-channel

parts-based cnn with improved triplet loss function. In: CVPR. (2016)

[32] Shi  Z.  Hospedales  T.M.  Xiang  T.: Transferring a semantic representation for person

re-identiﬁcation and search. In: CVPR. (2015)

[33] Zhang  L.  Xiang  T.  Gong  S.: Learning a discriminative null space for person re-identiﬁcation.

In: CVPR. (2016)

[34] Liu  Z.  Wang  D.  Lu  H.: Stepwise metric promotion for unsupervised video person re-

identiﬁcation. In: ICCV. (2017)

[35] Bak  S.  Carr  P.: One-shot metric learning for person re-identiﬁcation. In: CVPR. (2017)

[36] Chen  D.  Yuan  Z.  Chen  B.  Zheng  N.: Similarity learning with spatial constraints for person

re-identiﬁcation. In: CVPR. (2016) 1268–1277

[37] Liao  S.  Hu  Y.  Zhu  X.  Li  S.Z.: Person re-identiﬁcation by local maximal occurrence

representation and metric learning. In: CVPR. (2015)

[38] Li  W.  Zhao  R.  Xiao  T.  Wang  X.: Deepreid: Deep ﬁlter pairing neural network for person

re-identiﬁcation. In: CVPR. (2014)

[39] Li  D.  Chen  X.  Zhang  Z.  Huang  K.: Learning deep context-aware features over body and

latent parts for person re-identiﬁcation. In: CVPR. (2017)

[40] Isola  P.  Zhu  J.Y.  Zhou  T.  Efros  A.A.:

adversarial networks. CVPR (2017)

Image-to-image translation with conditional

[41] Deng  J.  Dong  W.  Socher  R.  Li  L.J.  Li  K.  Fei-Fei  L.: Imagenet: A large-scale hierarchical

image database. In: CVPR. (2009)

[42] Salimans  T.  Goodfellow  I.  Zaremba  W.  Cheung  V.  Radford  A.  Chen  X.:

techniques for training gans. In: NIPS. (2016)

Improved

[43] Mirza  M.  Osindero  S.:

arXiv:1411.1784 (2014)

Conditional generative adversarial nets.

arXiv preprint

11

[44] K. He  K.  Zhang  X.  Ren  S.  Sun  J.: Deep residual learning for image recognition. In: CVPR.

(2016)

[45] Xiao  T.  Li  S.  Wang  B.  Lin  L.  Wang  X.: Joint detection and identiﬁcation feature learning

for person search. In: CVPR. (2017)

[46] Zhong  Z.  Zheng  L.  Cao  D.  Li  S.: Re-ranking person re-identiﬁcation with k-reciprocal

encoding. In: CVPR. (2017)

[47] Zhou  J.  Yu  P.  Tang  W.  Wu  Y.: Efﬁcient online local metric adaptation via negative samples

for person reidentiﬁcation. In: CVPR. (2017)

[48] Zhao  L.  Li  X.  Wang  J.  Zhuang  Y.: Deeply-learned part-aligned representations for person

re-identiﬁcation. arXiv preprint arXiv:1707.07256 (2017)

[49] Sun  Y.  Zheng  L.  Deng  W.  Wang  S.: Svdnet for pedestrian retrieval. ICCV (2017)

[50] Li  W.  Zhu  X.  Gong  S.: Person re-identiﬁcation by deep joint learning of multi-loss

classiﬁcation. IJCAI (2017)

12

,Yixiao Ge
Zhuowan Li
Haiyu Zhao
Guojun Yin
Shuai Yi
Xiaogang Wang
hongsheng Li