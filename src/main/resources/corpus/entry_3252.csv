2014,Local Linear Convergence of Forward--Backward under Partial Smoothness,In this paper  we consider the Forward--Backward proximal splitting algorithm to minimize the sum of two proper closed convex functions  one of which having a Lipschitz continuous gradient and the other being partly smooth relatively to an active manifold $\mathcal{M}$. We propose a generic framework in which we show that the Forward--Backward (i) correctly identifies the active manifold $\mathcal{M}$ in a finite number of iterations  and then (ii) enters a local linear convergence regime that we characterize precisely. This gives a grounded and unified explanation to the typical behaviour that has been observed numerically for many problems encompassed in our framework  including the Lasso  the group Lasso  the fused Lasso and the nuclear norm regularization to name a few. These results may have numerous applications including in signal/image processing processing  sparse recovery and machine learning.,Local Linear Convergence of Forward–Backward

under Partial Smoothness

Jingwei Liang and Jalal M. Fadili

GREYC  CNRS  ENSICAEN  Université de Caen

{Jingwei.Liang Jalal.Fadili}@greyc.ensicaen.fr

Gabriel Peyré

CNRS  Ceremade  Université Paris-Dauphine
Gabriel.Peyre@ceremade.dauphine.fr

Abstract

In this paper  we consider the Forward–Backward proximal splitting algorithm to
minimize the sum of two proper convex functions  one of which having a Lip-
schitz continuous gradient and the other being partly smooth relative to an ac-
tive manifold M. We propose a generic framework under which we show that
the Forward–Backward (i) correctly identiﬁes the active manifold M in a ﬁnite
number of iterations  and then (ii) enters a local linear convergence regime that
we characterize precisely. This gives a grounded and uniﬁed explanation to the
typical behaviour that has been observed numerically for many problems encom-
passed in our framework  including the Lasso  the group Lasso  the fused Lasso
and the nuclear norm regularization to name a few. These results may have numer-
ous applications including in signal/image processing processing  sparse recovery
and machine learning.

1 Introduction

1.1 Problem statement

Convex optimization has become ubiquitous in most quantitative disciplines of science. A common
trend in modern science is the increase in size of datasets  which drives the need for more efﬁcient
optimization methods. Our goal is the generic minimization of composite functions of the form

(cid:8)Φ(x) = F (x) + J(x)(cid:9) 

where

min
x∈Rn

(1.1)

(A.1) J ∈ Γ0(Rn)  the set of proper  lower semi-continuous and convex functions;
(A.2) F is a convex and C 1 1(Rn) function whose gradient is β–Lipschitz continuous;
(A.3) Argmin Φ (cid:54)= ∅.

The class of problems (1.1) covers many popular non-smooth convex optimization problems en-
countered in various ﬁelds throughout science and engineering  including signal/image processing 
2λ||y − A · ||2 for some A ∈ Rm×n
machine learning and classiﬁcation. For instance  taking F = 1
and λ > 0  we recover the Lasso problem when J = || · ||1  the group Lasso for J = || · ||1 2  the
fused Lasso for J = ||D∗ · ||1 with D = [DDIF  Id] and DDIF is the ﬁnite difference operator 
anti-sparsity regularization when J = || · ||∞  and nuclear norm regularization when J = || · ||∗.

1

The standard (non relaxed) version of Forward–Backward (FB) splitting algorithm [18] for solving
(1.1) updates to a new iterate xk+1 according to

(cid:0)xk − γk∇F (xk)(cid:1)  γk ∈ [  2/β − ] 

(1.2)
with x0 ∈ Rn arbitrarily chosen and 0 <  ≤ 1/β. Recall that the proximity operator is deﬁned  for
γ > 0  as

xk+1 = proxγkJ

proxγJ (x) = argminz∈Rn

1.2 Contributions

1

2γ||z − x||2 + J(z).

In this paper  we present a uniﬁed local linear convergence analysis for the FB algorithm to solve
(1.1) when J is in addition partly smooth relative to a manifold M (see Deﬁnition 2.1). The class of
partly smooth functions is very large and encompasses all previously discussed examples as special
cases. More precisely  we ﬁrst show that FB has a ﬁnite identiﬁcation property  meaning that after
a ﬁnite number of iterations  say K  all iterates obey xk ∈ M for k ≥ K. Exploiting this property 
we then show that after such a large enough number of iterations  xk converges locally linearly. We
characterize this regime and the rates precisely depending on the structure of the active manifold M.
In general  xk converges locally Q-linearly  and when M is an linear subspace  the convergence be-
comes R-linear. Several experimental results on some of the problems discussed above are provided
to support our theoretical ﬁndings.

1.3 Related work

Finite support identiﬁcation and local R-linear convergence of FB to solve the Lasso problem 
though in inﬁnite-dimensional setting  is established in [3] under either a very restrictive injectiv-
ity assumption  or a non-degeneracy assumption which is a specialization of ours (see (3.1)) to the
(cid:96)1-norm. A similar result is proved in [12]  for F being a smooth convex and locally C 2 function
and J the (cid:96)1-norm  under restricted injectivity and non-degeneracy assumptions. The (cid:96)1-norm is a
partly smooth function and hence covered by our results. [1] proved Q-linear convergence of FB to
solve (1.1) for F satisfying restricted smoothness and strong convexity assumptions  and J being
a so-called convex decomposable regularizer. Again  the latter is a small subclass of partly smooth
functions  and their result is then covered by ours. For example  our framework covers the total
variation (TV) semi-norm and (cid:96)∞-norm regularizers which are not decomposable.
In [13  14]  the authors have shown ﬁnite identiﬁcation of active manifolds associated to partly
smooth functions for various algorithms  including the (sub)gradient projection method  Newton-
like methods  the proximal point algorithm. Their work extends that of e.g. [28] on identiﬁable
surfaces from the convex case to a general non-smooth setting. Using these results  [15] considered
the algorithm [25] to solve (1.1) where J is partly smooth  but not necessarily convex and F is
C 2(Rn)  and proved ﬁnite identiﬁcation of the active manifold. However  the convergence rate
remains an open problem in all these works.

1.4 Notations
For a nonempty convex set C ⊂ Rn  ri(C) denotes its relative interior  aﬀ(C) is its afﬁne hull 
par(C) is the subspace parallel to it. We denote PC the orthogonal projector onto C  and for a matrix
A ∈ Rm×n  AC = A ◦ PC.
Suppose M ⊂ Rn is a C 2-manifold around x ∈ Rn  denote TM(x) the tangent space of M at
x ∈ Rn. The tangent model subspace is deﬁned as

(cid:0)∂J(x)(cid:1) is single-valued  we therefore deﬁne the generalized sign vector

.

It is easy to see that PTx

Tx = par(cid:0)∂J(x)(cid:1)⊥
(cid:0)∂J(x)(cid:1).

ex = PTx

It is straightforward to show that ex = Paﬀ(∂J(x))(0).

2

2 Partial smoothness

Besides (A.1)  our central assumption is that J is a partly smooth function. Partial smoothness of
functions is originally deﬁned in [16]. Our deﬁnition hereafter specializes it to functions in Γ0(Rn).
Deﬁnition 2.1. Let J ∈ Γ0(Rn)  and x ∈ Rn such that ∂J(x) (cid:54)= ∅. J is partly smooth at x relative
to a set M containing x if

(1) (Smoothness) M is a C 2-manifold around x and J restricted to M is C 2 around x.
(2) (Sharpness) The tangent space TM(x) is Tx.
(3) (Continuity) The set–valued mapping ∂J is continuous at x relative to M.

In the following  the class of partly smooth functions at x relative to M is denoted as PSx(M).
When M is an afﬁne manifold  then M = x + Tx  and we denote this subclass as PSAx(x + Tx).
When M is a linear manifold  then M = Tx  and we denote this subclass as PSLx(Tx).
Capitalizing on the results of [16]  it can be shown that under mild transversality assumptions  the
set of continuous convex partly smooth functions is closed under addition and pre-composition by a
linear operator. Moreover  absolutely permutation-invariant convex and partly smooth functions of
the singular values of a real matrix  i.e. spectral functions  are convex and partly smooth spectral
functions of the matrix [9].
It then follows that all the examples discussed in Section 1  including (cid:96)1  (cid:96)1 2  (cid:96)∞ norms  TV semi-
norm and nuclear norm  are partly smooth. In fact  the nuclear norm is partly smooth at a matrix x

relative to the manifold M =(cid:8)x(cid:48) : rank(x(cid:48)) = rank(x)(cid:9). The ﬁrst three regularizers are all part

of the class PSLx(Tx)  see Section 4 and [27] for details.
We now deﬁne a subclass of partly smooth functions where the active manifold is actually a subspace
and the generalized sign vector ex is locally constant.
Deﬁnition 2.2. J belongs to the class PSSx(Tx) if and only if J ∈ PSAx(x + Tx) (resp. J ∈
PSLx(Tx))  and there exists a neighbourhood U of x such that ∀x(cid:48) ∈ (x + Tx) ∩ U (resp. ∀x(cid:48) ∈
Tx ∩ U)

ex(cid:48) = ex.

A typical family of functions that comply with this deﬁnition is that of partly polyhedral func-
tions [26  Section 6.5]  which includes the (cid:96)1 and (cid:96)∞ norms  and TV semi-norm.

3 Local linear convergence of the FB method

In this section  we state our main result on ﬁnite identiﬁcation and local linear convergence of FB.
Theorem 3.1. Assume that (A.1)-(A.3) hold. Suppose that the FB scheme is used to create a se-
quence xk which converges to x(cid:63) ∈ Argmin Φ such that J ∈ PSx(cid:63) (Mx(cid:63) )  F is C 2 near x(cid:63) and

−∇F (x(cid:63)) ∈ ri(cid:0)∂J(x(cid:63))(cid:1).

Then we have the following holds 

(1) The FB scheme (1.2) has the ﬁnite identiﬁcation property  i.e. there exists K ≥ 0  such that

for all k ≥ K  xk ∈ Mx(cid:63).

(2) Suppose moreover there exists α > 0 such that

PT∇2F (x(cid:63))PT (cid:23) αId 
where T := Tx(cid:63). Then for all k ≥ K  the following holds.

(i) Q-linear convergence: if 0 < γ ≤ γk ≤ ¯γ < min(cid:0)2αβ−2  2β−1(cid:1)  then given any
1 > ρ >(cid:101)ρ 
where(cid:101)ρ2 = max(cid:8)q(γ)  q(¯γ)(cid:9) ∈ [0  1[ and q(γ) = 1 − 2αγ + β2γ2.

||xk+1 − x(cid:63)|| ≤ ρ||xk − x(cid:63)|| 

(3.1)

(3.2)

3

(ii) R-linear convergence: if J ∈ PSAx(cid:63) (x(cid:63) + T ) or J ∈ PSLx(cid:63) (T )  then for 0 < γ ≤

γk ≤ ¯γ < min(cid:0)2αν−2  2β−1(cid:1)  where ν ≤ β is the Lipschitz constant of PT∇F PT  

then

k = 1 − 2αγk + ν2γ2

where ρ2
the optimal linear rate can be achieved is

||xk+1 − x(cid:63)|| ≤ ρk||xk − x(cid:63)|| 
k ∈ [0  1[. Moreover  if α

ρ∗ =(cid:112)1 − α2/ν2.

ν2 ≤ ¯γ and set γk ≡ α

ν2   then

Remark 3.2.

• The non-degeneracy assumption in (3.1) can be viewed as a geometric gener-
alization of strict complementarity of non-linear programming. Building on the arguments
of [14]  it turns out that it is almost a necessary condition for ﬁnite identiﬁcation of Mx(cid:63).
• Under the non-degeneracy and restricted strong convexity assumptions (3.1)-(3.2)  one can

actually show that x(cid:63) is unique by extending the reasoning in [26].

• For F = G◦ A  where G satisﬁes (A.2) and A is a linear operator  assumption (3.2) and the
constant α can be restated in terms of local strong convexity of G and restricted injectivity
of A on T   i.e. Ker(A) ∩ T = {0}.

• When F = 1

rem 3.1 can be reﬁned further as the gradient operator ∇F becomes linear.

2||y − A · ||2  not only the minimizer x(cid:63) is unique  but also the rates in Theo-
• Partial smoothness guarantees that xk arrives the active manifold in ﬁnite time  hence rais-
ing the hope of acceleration using second-order information. For instance  one can think
of turning to geometric methods along the manifold Mx(cid:63)  where faster convergence rates
can be achieved. This is also the motivation behind the work of e.g. [19].

When J ∈ PSSx(cid:63) (T )  it turns out that the restricted convexity assumption (3.2) of Theorem 3.1 can
be removed in some cases  but at the price of less sharp rates.
Theorem 3.3. Assume that (A.1)-(A.3) hold. For x(cid:63) ∈ Argmin Φ  suppose that J ∈ PSSx(cid:63) (Tx(cid:63) ) 
B(x(cid:63))   > 0. Let the FB scheme be used to create a sequence xk that converges to x(cid:63) with
C > 0 and ρ ∈ [0  1[ such that for all k large enough

(3.1) is fulﬁlled  and there exists a subspace V such that Ker(cid:0)PT∇2F (x)PT
(cid:1) = V for any x ∈
0 < γ ≤ γk ≤ ¯γ < min(cid:0)2αβ−2  2β−1(cid:1)  where α > 0 (see the proof). Then there exists a constant

||xk − x(cid:63)|| ≤ Cρk.

A typical example where this result applies is for F = G ◦ A with G locally strongly convex  in
which case V = Ker(AT ).

4 Numerical experiments

In this section  we describe some examples to demonstrate the applicability of our results. More
precisely  we consider solving

(4.1)
where y ∈ Rm is the observation  A : Rn → Rm  λ is the tradeoff parameter  and J is either the
(cid:96)1-norm  the (cid:96)∞-norm  the (cid:96)1 2-norm  the TV semi-norm or the nuclear norm.
Example 4.1 ((cid:96)1-norm). For x ∈ Rn  the sparsity promoting (cid:96)1-norm [7  23] is

min
x∈Rn

1

2||y − Ax||2 + λJ(x)

J(x) =(cid:80)n

i=1|xi|.

It can veriﬁed that J is a polyhedral norm  and thus J ∈ PSSx(Tx) for the model subspace

M = Tx =(cid:8)u ∈ Rn : supp(u) ⊆ supp(x)(cid:9)  and ex = sign(x).

The proximity operator of the (cid:96)1-norm is given by a simple soft-thresholding.

4

Let the support of x ∈ Rn be divided into non-overlapping blocks B such that(cid:83)

Example 4.2 ((cid:96)1 2-norm). The (cid:96)1 2-norm is usually used to promote group-structured sparsity [29].
b∈B b = {1  . . .   n}.

The (cid:96)1 2-norm is given by

J(x) = ||x||B =(cid:80)

b∈B||xb|| 

where xb = (xi)i∈b ∈ R|b|. || · ||B in general is not polyhedral  yet partly smooth relative to the
linear manifold

M = Tx =(cid:8)u ∈ Rn : suppB(u) ⊆ suppB(x)(cid:9)  and ex =(cid:0)N (xb)(cid:1)

where suppB(x) =(cid:83)(cid:8)b : xb (cid:54)= 0(cid:9)  N (x) = x/||x|| and N (0) = 0. The proximity operator of the

(cid:96)1 2-norm is given by a simple block soft-thresholding.
Example 4.3 (Total Variation). As stated in the introduction  partial smoothness is preserved under
pre-composition by a linear operator. Let J0 be a closed convex function and D is a linear operator.
Popular examples are the TV semi-norm in which case J0 = || · ||1 and D∗ = DDIF is a ﬁnite
difference approximation of the derivative [22]  or the fused Lasso for D = [DDIF  Id] [24].
If J0 ∈ PSD∗x(M0)  then it is shown in [16  Theorem 4.2] that under an appropriate transversality
condition  J ∈ PSx(M) where

b∈B 

M =(cid:8)u ∈ Rn : D∗u ∈ M0

(cid:9).

In particular  for the case of the TV semi-norm  we have J ∈ PSSx(Tx) with

M = Tx =(cid:8)u ∈ Rn : supp(D∗u) ⊆ I(cid:9) and ex = PTx Dsign(D∗x)

where I = supp(D∗x). The proximity operator for the 1D TV  though not available in closed form 
can be obtained efﬁciently using either the taut string algorithm [10] or the graph cuts [6].
Example 4.4 (Nuclear norm). Low-rank is the spectral extension of vector sparsity to matrix-
valued data x ∈ Rn1×n2  i.e. imposing the sparsity on the singular values of x. Let x = U ΛxV ∗ a
reduced singular value decomposition (SVD) of x. The nuclear norm of a x is deﬁned as

J(x) = ||x||∗ =(cid:80)r
M =(cid:8)z ∈ Rn1×n2 : rank(z) = r(cid:9).

i=1(Λx)i 

where rank(x) = r. It has been used for instance as SDP convex relaxation for many problems
including in machine learning [2  11]  matrix completion [21  4] and phase retrieval [5].
It can be shown that the nuclear norm is partly smooth relative to the manifold [17  Example 2]

The tangent space to M at x and ex are given by

TM(x) =(cid:8)z ∈ Rn1×n2 : z = U L∗ + M V ∗  ∀L ∈ Rn2×r  M ∈ Rn1×r(cid:9)  and ex = U V ∗.

The proximity operator of the nuclear norm is just soft–thresholding applied to the singular values.

Recovery from random measurements

In these examples  the forward observation model is

(4.2)
where A ∈ Rm×n is generated uniformly at random from the Gaussian ensemble with i.i.d. zero-
mean and unit variance entries. The tested experimental settings are

y = Ax0 + ε 

ε ∼ N (0  δ2) 

(a) (cid:96)1-norm m = 48 and n = 128  x0 is 8-sparse;
(b) Total Variation m = 48 and n = 128  (DDIFx0) is 8-sparse;
(c) (cid:96)∞-norm m = 123 and n = 128  x0 has 10 saturating entries;
(d) (cid:96)1 2-norm m = 48 and n = 128  x0 has 2 non-zero blocks of size 4;
(e) Nuclear norm m = 1425 and n = 2500  x0 ∈ R50×50 and rank(x0) = 5.

5

The number of measurements is chosen sufﬁciently large  δ small enough and λ of the order of
δ so that [27  Theorem 1] applies  yielding that the minimizer of (4.1) is unique and veriﬁes the
non-degeneracy and restricted strong convexity assumptions (3.1)-(3.2).
The convergence proﬁle of ||xk−x(cid:63)|| are depicted in Figure 1(a)-(e). Only local curves after activity
identiﬁcation are shown. For (cid:96)1  TV and (cid:96)∞  the predicted rate coincides exactly with the observed
one. This is because these regularizers are all partly polyhedral gauges  and the data ﬁdelity is
quadratic  hence making the predictions of Theorem 3.1(ii) exact. For the (cid:96)1 2-norm  although its
active manifold is still a subspace  the generalized sign vector ek is not locally constant  which
entails that the the predicted rate of Theorem 3.1(ii) slightly overestimates the observed one. For the
nuclear norm  whose active manifold is not linear. Thus Theorem 3.1(i) applies  and the observed
and predicted rates are again close.

TV deconvolution In this image processing example  y is a degraded image generated accord-
ing to the same forward model as (4.1)  but now A is a convolution with a Gaussian kernel. The
anisotropic TV regularizer is used. The convergence proﬁle is shown in Figure 1(f). Assumptions
(3.1)-(3.2) are checked a posteriori. This together with the fact that the anisotropic TV is polyhedral
justiﬁes that the predicted rate is again exact.

(a) (cid:96)1 (Lasso)

(b) TV semi-norm

(c) (cid:96)∞-norm

(d) (cid:96)1 2-norm

(e) Nuclear norm

(f) TV deconvolution

Figure 1: Observed and predicted local convergence proﬁles of the FB method (1.2) in terms of
||xk − x(cid:63)|| for different types of partly smooth functions. (a) (cid:96)1-norm; (b) TV semi-norm; (c) (cid:96)∞-
norm; (d) (cid:96)1 − (cid:96)2-norm; (e) Nuclear norm; (f) TV deconvolution.

Acknowledgment

This work was partly supported by the European Research Council (ERC project SIGMA-Vision).

5 Proofs
Lemma 5.1. Suppose that J ∈ PSx(M). Then for any x(cid:48) ∈ M ∩ U  where U is a neighbourhood
of x  the projector PM(x(cid:48)) is uniquely valued and C 1 around x  and thus

x(cid:48) − x = PTx (x(cid:48) − x) + o(cid:0)||x(cid:48) − x||(cid:1).

If J ∈ PSAx(x + Tx) or J ∈ PSLx(Tx)  then x(cid:48) − x = PTx (x(cid:48) − x).

6

38040042044046048050010−1010−810−610−410−2kxk−x⋆kk theoreticalpractical45050055060065070075080010−1010−810−610−410−2kxk−x⋆kk theoreticalpractical1000200030004000500060007000800010−1010−810−610−410−2100kxk−x⋆kk theoreticalpractical35040045050010−1010−810−610−410−2kxk−x⋆kk theoreticalpractical25030035040045050010−1010−810−610−410−2100kxk−x⋆kk theoreticalpractical5010015020025030010−1010−810−610−410−2100102kxk−x⋆kk theoreticalpracticalProof. Partial smoothness implies that M is a C 2–manifold around x  then PM(x(cid:48)) is uniquely
valued [20] and moreover C 1 near x [17  Lemma 4]. Thus  continuous differentiability shows

x(cid:48) − x = PM(x(cid:48)) − PM(x) = DPM(x)(x − x(cid:48)) + o(||x − x(cid:48)||).

where DPM(x) is the derivative of PM at x. By virtue of [17  Lemma 4] and the sharpness property
of J  this derivative is given by

The case where M is afﬁne or linear is immediate. This concludes the proof.

DPM(x) = PTM(x) = PTx  

Proof of Theorem 3.1.
1. Classical convergence results of the FB scheme  e.g. [8]  show that xk converges to some x(cid:63) ∈
Argmin Φ (cid:54)= ∅ by assumption (A.3). Assumptions (A.1)-(A.2) entail that (3.1) is equivalent
functions [16  Corollary 4.7] ensures that Φ ∈ PSx(cid:63) (M). By deﬁnition of xk+1  we have

to 0 ∈ ri ∂(cid:0)Φ(x(cid:63))(cid:1). Since F ∈ C 2 around x(cid:63)  the smooth perturbation rule of partly smooth
where Gk =(cid:0)Id − γk∇F(cid:1). By Baillon-Haddad theorem  Gk is non-expansive  hence
Since lim inf γk = γ > 0  we obtain dist(cid:0)0  ∂Φ(xk+1)(cid:1) → 0. Owing to assumptions (A.1)-

(cid:0)Gk(xk) − Gk(xk+1)(cid:1) ∈ ∂Φ(xk+1).

dist(cid:0)0  ∂Φ(xk+1)(cid:1) ≤ 1

||Gk(xk) − Gk(xk+1)|| ≤ 1

(A.2)  Φ is sub-differentially continuous and thus Φ(xk) → Φ(x(cid:63)). Altogether  this shows that
the conditions of [13  Theorem 5.3] are fulﬁlled  whence the claim follows.

||xk − xk+1||.

1
γk

γk

γk

2. Take K > 0 sufﬁciently large such that for all k ≥ K  xk ∈ Mx(cid:63) and xk ∈ B(x(cid:63)).

(i) Since proxγkJ is ﬁrmly non-expansive  hence non-expansive  we have

||xk+1 − x(cid:63)|| = ||proxγkJ Gkxk − proxγkJ Gkx(cid:63)|| ≤ ||Gkxk − Gkx(cid:63)||.

(5.1)
By virtue of Lemma 5.1  we have xk − x(cid:63) = PT (xk − x(cid:63)) + o(||xk − x(cid:63)||). This  together
with local C 2 smoothness of F and Lipschitz continuity of ∇F entails

(cid:104)xk − x(cid:63) ∇F (xk) − ∇F (x(cid:63))(cid:105) =(cid:82) 1
= (cid:82) 1
0 (cid:104)PT (xk − x(cid:63)) ∇2F (x(cid:63) + t(xk − x(cid:63)))PT (xk − x(cid:63))(cid:105)dt + o(cid:0)||xk − x(cid:63)||2(cid:1)
≥ α||xk − x(cid:63)||2 + o(cid:0)||xk − x(cid:63)||2(cid:1) .

(5.2)
Since (3.2) holds and ∇2F (x) depends continuously on x  there exists  > 0 such that
PT∇2F (x)PT (cid:31) αId  ∀x ∈ B(x(cid:63)). Thus  classical development of the right hand side of
(5.1) yields

0(cid:104)xk − x(cid:63) ∇2F (x(cid:63) + t(xk − x(cid:63)))(xk − x(cid:63))(cid:105)dt

||xk+1 − x(cid:63)||2 ≤ ||Gkxk − Gkx(cid:63)||2 = ||(xk − x(cid:63)) − γk(∇F (xk) − ∇F (x(cid:63)))||2
k||∇F (xk) − ∇F (x(cid:63))||2

= ||xk − x(cid:63)||2 − 2γk(cid:104)xk − x(cid:63) ∇F (xk) − ∇F (x(cid:63))(cid:105) + γ2
≤ ||xk − x(cid:63)||2 − 2γkα||xk − x(cid:63)||2 + γ2

kβ2||xk − x(cid:63)||2 + o(cid:0)||xk − x(cid:63)||2(cid:1)

(cid:1)||xk − x(cid:63)||2 + o(cid:0)||xk − x(cid:63)||2(cid:1).

=(cid:0)1 − 2αγk + β2γ2

k

(5.3)

Taking the lim sup in this inequality gives

lim sup
k→+∞

||xk+1 − x(cid:63)||2/||xk − x(cid:63)||2 ≤ q(γk) = 1 − 2αγk + β2γ2
k.

It is clear that for 0 < γ ≤ γ ≤ ¯γ < min(cid:0)2αβ−2  2β−1(cid:1)  q(γ) ∈ [0  1[  and q(γ) ≤ (cid:101)ρ2 =
max(cid:8)q(γ)  q(¯γ)(cid:9). Inserting this in (5.4)  and using classical arguments yields the result.

(5.4)

7

(ii) We give the proof for M = T   that for M = x(cid:63) + T is similar. Since xk and x(cid:63) belong to

T   from xk+1 = proxγkJ (Gkxk) we have
Gkxk − xk+1 ∈ γk∂J(xk+1) ⇒ xk+1 = PT
Similarly  we have x(cid:63) = PT Gkx(cid:63) − γke(cid:63). We then arrive at
(xk+1 − x(cid:63)) + γk(ek+1 − e(cid:63)) = (xk − x(cid:63)) − γk
Moreover  maximal monotonicity of γk∂J gives

(cid:0)Gkxk − γk∂J(xk+1)(cid:1) = PT Gkxk − γkek+1.
(cid:0)PT∇F (PT xk) − PT∇F (PT x(cid:63))(cid:1). (5.5)

||(xk+1 − x(cid:63)) + γk(ek+1 − e(cid:63))||2

= ||xk+1 − x(cid:63)||2 + 2(cid:104)xk+1 − x(cid:63)  γk(ek+1 − e(cid:63))(cid:105) + γk||ek+1 − e(cid:63)||2 ≥ ||xk+1 − x(cid:63)||2.

It is straightforward to see that now  (5.2) becomes

(cid:104)xk − x(cid:63)  PT∇F (PT xk) − PT∇F (PT x(cid:63))(cid:105) ≥ α||xk − x(cid:63)||2.

Let ν be the Lipschitz constant of PT∇F PT . Obviously ν ≤ β. Developing ||PT (Gkxk −
Gkx(cid:63))||2 similarly to (5.3) we obtain

||xk+1 − x(cid:63)||2 ≤(cid:0)1 − 2αγk + ν2γ2

(cid:1)||xk − x(cid:63)||2 = ρ2

where ρk ∈ [0  1[ for 0 < γ ≤ γk ≤ ¯γ < min(cid:0)2α/ν2  2/β(cid:1). ρk is minimized at α

k||xk − x(cid:63)||2 

k

proposed optimal rate whenever it obeys the given upper-bound.

ν2 with the

Proof of Theorem 3.3. Arguing similarly to the proof of Theorem 3.1(ii)  and using in addition that
e(cid:63) = ex(cid:63) is locally constant  we get

xk+1 − x(cid:63) = (xk − x(cid:63)) − γk
= (xk − x(cid:63)) − γk

0 PT∇2F (x(cid:63) + t(xk − x(cid:63)))PT (xk − x(cid:63))dt 
Denote Ht = PT∇2F (x(cid:63) + t(xk − x(cid:63)))PT (cid:23) 0. Using that Ht is self-adjoint  we have

(cid:0)PT∇F (PT xk) − PT∇F (PT x(cid:63))(cid:1)
(cid:82) 1

PV xk+1 = PV xk.

Since xk → x(cid:63)  it follows that PV xk = PV x(cid:63) for all k sufﬁciently large. Observing that xk − x(cid:63) =
PV ⊥(xk − x(cid:63)) for all large k  we get

xk+1 − x(cid:63) = xk − x(cid:63) − γk
Observe that V ⊥ ⊂ T . By deﬁnition  Bt = H 1/2
||Btx||2 > σ||x||2 for all x (cid:54)= 0 and t ∈ [0  1]. We then have

0 PV ⊥ HtPV ⊥(xk − x(cid:63))dt.

t PV ⊥ is injective  and therefore  ∃σ > 0 such that

(cid:82) 1

||xk+1 − x(cid:63)||2

(cid:82) 1

= ||xk − x(cid:63)||2 − 2γk
0 (cid:104)xk − x(cid:63)  BT
= ||xk − x(cid:63)||2 − 2γkσ||xk − x(cid:63)||2 + γ2
≤ ||xk − x(cid:63)||2 − 2γkσ||xk − x(cid:63)||2 + γ2
≤ ||xk − x(cid:63)||2 − 2γkσ||xk − x(cid:63)||2 + γ2

k||PV ⊥ PT

t Bt(xk − x(cid:63))(cid:105)dt + γ2
k||PT PV ⊥||2||∇F (xk) − ∇F (x(cid:63))||2
kβ2||PV ⊥||2||PV ⊥ (xk − x(cid:63))||2
kβ2||xk − x(cid:63)||2 = ρ2

k||xk − x(cid:63)||2.

It is easy to see again that ρk ∈ [0  1[ whenever 0 < γ ≤ γk ≤ ¯γ < min(cid:0)2β−1  2σβ−2(cid:1).

(cid:0)∇F (xk) − ∇F (x(cid:63))(cid:1)||2

References

[1] A. Agarwal  S. Negahban  and M. Wainwright. Fast global convergence of gradient methods for high-

dimensional statistical recovery. The Annals of Statistics  40(5):2452–2482  10 2012.

[2] F. Bach. Consistency of trace norm minimization. The Journal of Machine Learning Research  9:1019–

1048  2008.

8

[3] K. Bredies and D. A. Lorenz. Linear convergence of iterative soft-thresholding. Journal of Fourier

Analysis and Applications  14(5-6):813–837  2008.

[4] E. J. Candès and B. Recht. Exact matrix completion via convex optimization. Foundations of Computa-

tional Mathematics  9(6):717–772  2009.

[5] E. J. Candès  T. Strohmer  and V. Voroninski. Phaselift: Exact and stable signal recovery from mag-
nitude measurements via convex programming. Communications on Pure and Applied Mathematics 
66(8):1241–1274  2013.

[6] A. Chambolle and J. Darbon. A parametric maximum ﬂow approach for discrete total variation regular-

ization. In Image Processing and Analysis with Graphs. CRC Press  2012.

[7] S. S. Chen  D. L. Donoho  and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM journal

on scientiﬁc computing  20(1):33–61  1999.

[8] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale

Modeling & Simulation  4(4):1168–1200  2005.

[9] A. Daniilidis  D. Drusvyatskiy  and A. S. Lewis. Orthogonal invariance and identiﬁability. to appear in

SIAM J. Matrix Anal. Appl.  2014.

[10] P. L. Davies and A. Kovac. Local extremes  runs  strings and multiresolution. Ann. Statist.  29:1–65 

2001.

[11] E. Grave  G. Obozinski  and F. Bach. Trace lasso: a trace norm regularization for correlated designs.

arXiv preprint arXiv:1109.1990  2011.

[12] E. Hale  W. Yin  and Y. Zhang. Fixed-point continuation for (cid:96)1-minimization: Methodology and conver-

gence. SIAM Journal on Optimization  19(3):1107–1130  2008.

[13] W. L. Hare and A. S. Lewis. Identifying active constraints via partial smoothness and prox-regularity.

Journal of Convex Analysis  11(2):251–266  2004.

[14] W. L. Hare and A. S. Lewis. Identifying active manifolds. Algorithmic Operations Research  2(2):75–82 

2007.

[15] WL Hare. Identifying active manifolds in regularization problems. In Fixed-Point Algorithms for Inverse

Problems in Science and Engineering  pages 261–271. Springer  2011.

[16] A. S. Lewis. Active sets  nonsmoothness  and sensitivity. SIAM Journal on Optimization  13(3):702–725 

2003.

[17] A. S. Lewis and J. Malick. Alternating projections on manifolds. Mathematics of Operations Research 

33(1):216–234  2008.

[18] P. L. Lions and B. Mercier. Splitting algorithms for the sum of two nonlinear operators. SIAM Journal

on Numerical Analysis  16(6):964–979  1979.

[19] S. A. Miller and J. Malick. Newton methods for nonsmooth convex minimization: connections among-
lagrangian  riemannian newton and sqp methods. Mathematical programming  104(2-3):609–633  2005.
[20] R. A. Poliquin  R. T. Rockafellar  and L. Thibault. Local differentiability of distance functions. Trans.

Amer. Math. Soc.  352:5231–5249  2000.

[21] B. Recht  M. Fazel  and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via

nuclear norm minimization. SIAM review  52(3):471–501  2010.

[22] L. I. Rudin  S. Osher  and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica

D: Nonlinear Phenomena  60(1):259–268  1992.

[23] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society.

Series B. Methodological  58(1):267–288  1996.

[24] R. Tibshirani  M. Saunders  S. Rosset  J. Zhu  and K. Knight. Sparsity and smoothness via the fused

lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology)  67(1):91–108  2004.

[25] P. Tseng and S. Yun. A coordinate gradient descent method for nonsmooth separable minimization. Math.

Prog. (Ser. B)  117  2009.

[26] S. Vaiter  M. Golbabaee  M. J. Fadili  and G. Peyré. Model selection with low complexity priors. Avail-

able at arXiv:1304.6033  2013.

[27] S. Vaiter  G. Peyré  and M. J. Fadili. Model consistency of partly smooth regularizers. Available

arXiv:1405.1004  2014.

[28] S. J. Wright. Identiﬁable surfaces in constrained optimization. SIAM Journal on Control and Optimiza-

tion  31(4):1063–1079  1993.

[29] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  68(1):49–67  2005.

9

,Jingwei Liang
Jalal Fadili
Gabriel Peyré
Etai Littwin
Lior Wolf