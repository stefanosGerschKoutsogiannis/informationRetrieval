2009,Nonlinear directed acyclic structure learning with weakly additive noise models,The recently proposed \emph{additive noise model} has advantages over previous structure learning algorithms  when attempting to recover some true data generating mechanism  since it (i) does not assume linearity or Gaussianity and (ii) can recover a unique DAG rather than an equivalence class. However  its original extension to the multivariate case required enumerating all possible DAGs  and for some special distributions  e.g. linear Gaussian  the model is invertible and thus cannot be used for structure learning. We present a new approach which combines a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the equivalence class. This results in a more computationally efficient approach that is useful for arbitrary distributions even when additive noise models are invertible. Experiments with synthetic and real data show that this method is more accurate than previous methods when data are nonlinear and/or non-Gaussian.,Nonlinear directed acyclic structure learning

with weakly additive noise models

Robert E. Tillman

Arthur Gretton

Peter Spirtes

Carnegie Mellon University

Carnegie Mellon University 

Carnegie Mellon University

Pittsburgh  PA

MPI for Biological Cybernetics

Pittsburgh  PA

rtillman@cmu.edu

Pittsburgh  PA

ps7z@andrew.cmu.edu

arthur.gretton@gmail.com

Abstract

The recently proposed additive noise model has advantages over previous
directed structure learning approaches since it (i) does not assume linearity
or Gaussianity and (ii) can discover a unique DAG rather than its Markov
equivalence class. However  for certain distributions  e.g. linear Gaussians 
the additive noise model is invertible and thus not useful for structure
learning  and it was originally proposed for the two variable case with a
multivariate extension which requires enumerating all possible DAGs. We
introduce weakly additive noise models  which extends this framework to
cases where the additive noise model is invertible and when additive noise
is not present. We then provide an algorithm that learns an equivalence
class for such models from data  by combining a PC style search using recent
advances in kernel measures of conditional dependence with local searches
for additive noise models in substructures of the Markov equivalence class.
This results in a more computationally eﬃcient approach that is useful for
arbitrary distributions even when additive noise models are invertible.

1 Introduction

Learning probabilistic graphical models from data serves two primary purposes: (i) ﬁnd-
ing compact representations of probability distributions to make inference eﬃcient and (ii)
modeling unknown data generating mechanisms and predicting causal relationships. Until
recently  most constraint-based and score-based algorithms for learning directed graphical
models from continuous data required assuming relationships between variables are linear
with Gaussian noise. While this assumption may be appropriate in many contexts  there are
well known contexts  such as fMRI images  where variables have nonlinear dependencies and
data do not tend towards Gaussianity. A second major limitation of the traditional algo-
rithms is they cannot identify a unique structure; they reduce the set of possible structures
to an equivalence class which entail the same Markov properties. The recently proposed ad-
ditive noise model [1] for structure learning addresses both limitations; by taking advantage
of observed nonlinearity and non-Gaussianity  a unique directed acyclic structure can be
identiﬁed in many contexts. However  it too suﬀers from limitations: (i) for certain distri-
butions  e.g. linear Gaussians  the model is invertible and not useful for structure learning;
(ii) it was originally proposed for two variables with a multivariate extension that requires
enumerating all possible DAGs  which is super-exponential in the number of variables.

In this paper  we address the limitations of the additive noise model. We introduce weakly
additive noise models  which have the advantages of additive noise models  but are still
useful when the additive noise model is invertible and in most cases when additive noise is
not present. Weakly additive noise models allow us to express greater uncertainty about the

1

data generating mechanism  but can still identify a unique structure or a smaller equivalence
class in most cases. We also provide an algorithm for learning an equivalence class for such
models from data that is more computationally eﬃcient in the more than two variables case.
Section 2 reviews the appropriate background; section 3 introduces weakly additive noise
models; section 4 describes our learning algorithm; section 5 discusses some related research;
section 6 presents some experimental results; ﬁnally  section 7 oﬀers conclusions..

2 Background

G denotes the parents of Vi and ChVi

Let G = hV  Ei be a directed acyclic graph (DAG)  where V denotes the set of vertices and
Eij ∈ E denotes a directed edge Vi → Vj. Vi is a parent of Vj and Vj is a child of Vi. For
Vi ∈ V  PaVi
G denotes the children of Vi. The degree of Vi
is the number of edges with an endpoint at Vi. A v-structure is a triple hVi  Vj  Vki ⊆ V such
Vj
G . A v-structure is immoral  or an immorality  if Eik /∈ E and Eki /∈ E.
that {Vi  Vk} ⊆ Pa
A joint distribution P over variables corresponding to nodes in V is Markov with respect to
G (cid:17). P is faithful to G if every conditional independence true
G if PP (V) = Y
in P is entailed by the above factorization. A partially directed acyclic graph (PDAG) H for
G is a mixed graph  i.e. consisting of directed and undirected edges  representing all DAGs
Markov equivalent to G  i.e. DAGs entailing exactly the same conditional independencies.
If Vi → Vj is a directed edge in H  then all DAGs Markov equivalent to G have this directed
edge; if Vi − Vj is an undirected edge in H  then some DAGs that are Markov equivalent to
G have the directed edge Vi → Vj while others have the directed edge Vi ← Vj.

PP (cid:16)Vi | PaVi

Vi∈V

The PC algorithm is a well known constraint-based  or conditional independence based 
structure learning algorithm. It is an improved greedy version of the SGS [2] and IC [3]
Instead of searching all subsets of V\{Vi  Vj} for an S such
algorithms  shown below.

Input : Observed data for variables in V
Output: PDAG G over nodes V
G ← the complete undirected graph over the variables in V
For {Vi  Vj} ⊆ V  if ∃S ⊆ V\{Vi  Vj}  such that Vi ⊥⊥ Vj | S  remove the Vi − Vj edge
For {Vi  Vj  Vk} ⊆ V such that Vi − Vj and Vj − Vk remain as edges  but Vi − Vk does
not remain  if ∄S ⊆ V\{Vi  Vj  Vk}  such that Vi ⊥⊥ Vk | {S ∪ Vj}  orient Vi → Vj ← Vk
Orient edges to prevent additional immoralities and cycles using the Meek rules [4]

1

2

3

4

Algorithm 1: SGS/IC algorithm

that Vi ⊥⊥ Vj | S  PC (i) initially sets S = ∅ for all {Vi  Vj} pairs  (ii) checks to see if any
edges can be removed based on the results of conditional independence tests with these S
sets  and (iii) iteratively increases the cardinality of S considered until ∄Vk ∈ V with degree
greater than |S|. S is only considered if it is a subset of nodes connected to Vi or Vj at the
current iteration. PC learns the correct PDAG in the large sample limit when the Markov 
faithfulness  and causal suﬃciency (that there are no unmeasured common causes of two
or more measured variables) assumptions hold [2]. The partial correlation based Fisher
Z-transformation test  which assumes linear Gaussian distributions  is used for conditional
independence testing with continuous variables. The statistical advantage of PC is it limits
the number of tests performed  particularly those with large conditioning sets. This also
yields a computational advantage since the number of possible tests is exponential in |V|.

The recently proposed additive noise model approach to structure learning [1] assumes only
that each variable can be represented as a (possibly nonlinear) function f of its parents
plus additive noise ǫ with some arbitrary distribution  and that the noise components are

n

mutually independent  i.e. P(ǫ1  . . .   ǫn) =

Y

i=1

P(ǫi). Consider the two variable case where

X → Y is the true DAG  X = ǫX   Y = sin(πX) + ǫY   ǫX ∼ U nif (−1  1)  and ǫY ∼
U nif (−1  1). If we regress Y on X (nonparametrically)  the forward model  ﬁgure 1a  and

2

Y

2
1.5
1
0.5
0
−0.5
−1
−1.5
−2
−1

1

0.5

X

0

−0.5

0.5

1

−0.5

0
X
(a)

−1
−2−1.5−1−0.5 0 0.5 1 1.5 2

Y
(b)

Z

10
8
6
4
2
0
−2
−4
−6
−8
−1

1

0.5

X

0

−0.5

0.5

1

−0.5

0
X
(c)

−1
−8 −6 −4 −2 0 2 4 6 8 10

Z
(d)

Figure 1: Nonparametric regressions with data overlayed for (a) Y regressed on X  (b) X
regressed on Y   (c) Z regressed on X  and (d) X regressed on Z

regress X on Y   the backward model  ﬁgure 1b  we observe the residuals ˆǫY ⊥⊥ X and
ˆǫX /⊥⊥ Y . This provides a criterion for distinguishing X → Y from X ← Y in many cases 
but there are counterexamples such as the linear Gaussian case  where the forward model
is invertible so we ﬁnd ˆǫY ⊥⊥ X and ˆǫX ⊥⊥ Y .
[1  5] show  however  that whenever f is
nonlinear  the forward model is noninvertible  and when f is linear  the forward model
is only invertible when ǫ is Gaussian and a few other special cases. Another limitation
of this approach is that it is not closed under marginalization of intermediary variables
for X → Y → Z with X = ǫX   Y = X 3 + ǫY   Z = Y 3 + ǫZ  
when f is nonlinear  e.g.
ǫX ∼ U nif (−1  1)  ǫY ∼ U nif (−1  1)  and ǫZ ∼ U nif (0  1)  observing only X and Z 
ﬁgures 1c and 1d  causes us to reject both the forward and backward models. [5] shows this
method can be generalized to more variables. To test whether a DAG is compatible with
the data  we regress each variable on its parents and test whether the resulting residuals are
mutually independent. This procedure is impractical even for a few variables  however  since
the number of possible DAGs grows super-exponentially with the number of variables  e.g.
there are ≈ 4.2 × 1018 DAGs with 10 nodes. Since we do not assume linearity or Gaussianity
in this framework  a suﬃciently powerful nonparametric independence test must be used.
Typically  the Hilbert Schmidt Independence Criterion [6] is used  which we now deﬁne.
Let X be a random variable with domain X . A Hilbert space HX of functions from X to R
is a reproducing kernel Hilbert space (RKHS) if for some kernel k(·  ·) (the reproducing kernel
for HX )  for every f (·) ∈ HX and x ∈ X   the inner product hf (·)  k(x  ·)iHX = f (x). We may
treat k(x  ·) as a mapping of x to the feature space HX . For x  x′ ∈ X   hk(x  ·)  k(x′  ·)iHX =
k(x  x′)  so we can compute inner products eﬃciently in this high dimensional space. The
Moore-Aronszajn theorem shows that all symmetric positive deﬁnite kernels (most popular
kernels) are reproducing kernels that uniquely deﬁne corresponding RKHSs [7]. Let Y be
a random variable with domain Y and l(·  ·) the reproducing kernel for HY . We deﬁne the
mean map µX and cross covariance CXY as follows  using ⊗ to denote the tensor product.

µX = EX [k(x  ·)]

CXY = ([k(x  ·) − µX ] ⊗ [l(y  ·) − µY ])

If the kernels are characteristic  e.g. Gaussian and Laplace kernels  the mean map is injective
[8  9  10] so distinct probability distributions have diﬀerent mean maps. The Hilbert Schmidt
Independence Criteria (HSIC) HXY = kCXY k2
HS measures the dependence of X and Y  
where k · kHS denotes the Hilbert Schmidt norm. [9] shows HXY = 0 if and only if X ⊥⊥ Y
for characteristic kernels. For m paired i.i.d. samples  let K and L be Gram matrices for
N   let ˜K = HKH and ˜L = HLH
k(·  ·) and l(·  ·)  i.e. kij = k(xi  xj). For H = IN − 1
m2 tr (cid:16) ˜K ˜L(cid:17)  where tr denotes the trace  is an empirical
be centered Gram matrices. ˆHXY =
estimator for HXY [6]. To determine the threshold of a level-α statistical test  we can use
the permutation approach (where we compute ˆHXY for multiple random assignments of the
Y samples to X  and use the 1 − α quantile of the resulting empirical distribution over
ˆHXY )  or a Gamma approximation to the null distribution of m ˆHXY (see [6] for details).

N 1N 1T

1

3 Weakly additive noise models

We now extend the additive noise model framework to account for cases where additive
noise models are invertible and cases where additive noise may not be present.

3

G E is a local additive noise model for a distribution P over V

Deﬁnition 3.1. ψ = DVi  PaVi
that is Markov to a DAG G = hV  Ei if Vi = f (cid:16)PaVi
Deﬁnition 3.2. A weakly additive noise model M = hG  Ψi for a distribution P over V is a
DAG G = hV  Ei and set of local additive noise models Ψ  such that P is Markov to G  ψ ∈ Ψ
if and only if ψ is a local additive noise model for P  and ∀DVi  PaVi
G E ∈ Ψ  ∄Vj ∈ PaVi
G
Vj
such that there exists some graph G′ (not necessarily related to P) such that Vi ∈ Pa
G ′ and
DVj  Pa

G ′E is a local additive noise model for P.

G (cid:17) + ǫ is an additive noise model.

Vj

When we assume a data generating process has a weakly additive noise model representation 
we assume only that there are no cases where X → Y can be written X = f (Y ) + ǫX   but
not Y = f (X) + ǫY .
In other words  the data cannot appear as though it admits an
additive noise model representation  but only in the incorrect direction. This representation
is still appropriate when additive noise models are invertible  and when additive noise is
not present: such cases only lead to weakly additive noise models which express greater
underdetermination of the true data generating process.

We now deﬁne the notion of distribution-equivalence for weakly additive noise models.

Deﬁnition 3.3. A weakly additive noise model M = hG  Ψi is distribution-equivalent to
N = hG′  Ψ′i if and only if G and G′ are Markov equivalent and ψ ∈ Ψ if and only if ψ ∈ Ψ′.

Distribution-equivalence deﬁnes what can be discovered about the true data generating
mechanism using observational data. We now deﬁne a new structure to partition data
generating processes which instantiate distribution-equivalent weakly additive noise models.

Deﬁnition 3.4. A weakly additive noise partially directed acyclic graph (WAN-PDAG) for
M = hG  Ψi is a mixed graph H = hV  Ei such that for {Vi  Vj} ⊆ V 

1. Vi → Vj is a directed edge in H if and only if Vi → Vj is a directed edge in G and

in all G′ such that N = hG′  Ψ′i is distribution-equivalent to M

2. Vi − Vj is an undirected edge in H if and only if Vi → Vj is a directed edge in G and
there exists a G′ and N = hG′  Ψ′i distribution-equivalent to M such that Vi ← Vj
is a directed edge in G′

We now get the following results.
Lemma 3.1. Let M = hG  Ψi be a weakly additive noise model  DVi  PaVi
G ′ and ChVi
N = hG′  Ψ′i be distribution equivalent to M. Then PaVi

G = PaVi

G E ∈ Ψ  and
G = ChVi
G ′.

Proof. Since M and N are distribution-equivalent  PaVi

G = PaVi

G ′. Thus  ChVi

G = ChVi
G ′

Theorem 3.1. The WAN-PDAG for M = hG  Ψi is constructed by (i) adding all directed
and undirected edges in the PDAG instantiated by M  (ii) ∀DVi  PaVi
G E ∈ Ψ  directing all
Vj ∈ PaVi
G as Vi → Vk  and (iii) applying the extended Meek
rules [4]  treating orientions made using Ψ as background knowledge.

G as Vj → Vi and all Vk ∈ ChVi

Proof. (i) This is correct because of Markov equivalence [2]. (ii) This is correct by lemma
3.1. (iii) These rules are correct and complete [4].

WAN-PDAGs can used to identify the same information about the data generating mech-
anism as additive noise models  when additive noise models are identiﬁable  but provide
a more powerful representation of uncertainty and can be used to discover more informa-
tion when additive noise models are unidentiﬁable. The next section describes an eﬃcient
algorithm for learning WAN-PDAGs from data.

4

4 The Kernel PC (kPC) algorithm

We now describe the Kernel PC (kPC) algorithm1  which consists of two stages: (i) a
constraint-based search using the PC algorithm with a nonparametric conditional inde-
pendence test (the Fisher Z test is inappropriate since we want to allow nonlinearity and
non-Gaussianity) to identify the Markov equivalence class and (ii) a “PC-style” search for
noninvertible additive noise models in submodels of the Markov equivalence class.

In the ﬁrst stage  we use a kernel-based conditional dependence measure similar to HSIC
[9] (see also [11  Section 2.2] for a related quantity with a diﬀerent normalization). For
a conditioning variable Z with centered Gram matrix ˜M for a reproducing kernel m(·  ·) 
ZZCZ ¨Y   where ¨X = (X  Z) and
we deﬁne the conditional cross covariance CXY |Z = C ¨XZC−1
¨Y = (Y  Z). Let HXY |Z = kCXY |Zk2
HS. It follows from [9  Theorem 3] that HXY |Z = 0 if
and only if X ⊥⊥ Y |Z when kernels are characteristic. [9] provides the empirical estimator:

ˆHXY |Z =

1
m2 tr( ˜K ˜L − 2 ˜K ˜M ( ˜M + ǫIN )−2 ˜M ˜L + ˜K ˜M ( ˜M + ǫIN )−2 ˜M ˜L ˜M ( ˜M + ǫIN )−2 ˜M )
The null distribution of ˆHXY |Z is unknown and diﬃcult to derive so we must use the
permutation approach described in section 2. This is not straightforward since permuting
X or Y while leaving Z ﬁxed changes the marginal distribution of X given Z or Y given Z.
We thus (making analogy to the discrete case) must cluster Z and then permute elements
only within clusters for the permutation test  as in [12].
This ﬁrst stage is not computational eﬃcient  however  since each evaluation of ˆHXY |Z is
naively O (cid:0)N 3(cid:1) and we need to evaluate ˆHXY |Z approximately 1000 times for each per-
mutation test. Fortunately  we see from [13  Appendix C] that the eigenspectra of Gram
matrices for Gaussian kernels decay very rapidly  so low rank approximations of these ma-
trices can be obtained even when using a very conservative threshold. We implemented the
incomplete Cholesky factorization [14]  which can be used to obtain an m × p matrix G 
where p ≪ m  and an m × m permutation matrix P such that K ≈ P GG⊤P ⊤  where K is
an m × m Gram matrix. A clever implementation after replacing Gram matrices in ˆHXY |Z
with their incomplete Cholesky factorizations and using an appropriate equivalence to invert
G⊤G + ǫIp (cid:16)for ˜M(cid:17) instead of GG⊤ + ǫIm results in a straightforward O (cid:0)mp3(cid:1) operation.

Unfortunately  this is not numerically stable unless a relatively large regularizer ǫ is chosen
or only a small number of columns are used in the incomplete Cholesky factorizations.

A more stable (and faster) approach is to obtain incomplete Cholesky factorizations GX   GY  
and GZ with permutation matrices PX   PY   and PZ  and then obtain the thin SVDs for
HPX GX   HPY GY   and HPZGZ  e.g HP G = U SV   where U is m × p  S is the p × p
diagonal matrix of singular values  and V is p × p. Now deﬁne matrices ¯SX   ¯SY   and ¯SZ
and ¯GX   ¯GY   and ¯GZ as follows:

2

ii = (cid:0)sX
¯sX
ii (cid:1)

2

ii = (cid:0)sY
¯sY
ii(cid:1)

ii(cid:1)2
ii = (cid:0)sZ
¯sZ
ii(cid:1)2
(cid:0)sZ
¯GZ = U Z ¯SZU Z ⊤

+ ǫ

¯GX = U X ¯SX U X ⊤

¯GY = U Y ¯SY U Y ⊤

1
We can compute ˆHXY |Z =
m2 tr (cid:0) ¯GX ¯GY − 2 ¯GX ¯GZ ¯GY + ¯GX ¯GZ ¯GY ¯GZ(cid:1) stably and eﬃ-
ciently in O (cid:0)mp3(cid:1) by choosing an appropriate associative ordering of matrix multiplications.
Figure 2 shows that this method leads to a signiﬁcant increase in speed when used with a
permutation test for conditional independence without signiﬁcantly aﬀecting the empirically
observed type I error rate for a level-.05 test.

In the second stage  we look for additive noise models in submodels of the Markov equiv-
alence class because (i) it may be more eﬃcient to do so and require fewer tests since
orientations implied by an additive noise model may imply further orientations and (ii) we

1MATLAB code may be obtained from http://www.andrew.cmu.edu/∼rtillman/kpc

5

)
s
e

t

u
n
m

i

(
 

e
m
T

i

20

15

10

5

0

 

Naive
Incomplete Cholesky + SVD

 

e

t

a
R

 
r
o
r
r

E

 
I
 

e
p
y
T

 
l

a
c
i
r
i
p
m
E

200

400

600

800

1000

Sample Size

Naive
Incomplete Cholesky + SVD

 

1

0.8

0.6

0.4

0.2

0

 

200

400

600

800

1000

Sample Size

Figure 2: Runtime and Empirical Type I Error Rate. Results are over the generation of 20
3-node DAGs for which X ⊥⊥ Y |Z and the generating distribution was Gaussian.

may ﬁnd more orientations by considering submodels  e.g. if all relations are linear and only
one variable has a non-Gaussian noise term. The basic strategy used is a“PC-style” greedy
search where we look for undirected edges in the current mixed graph (starting with the
PDAG resulting from the ﬁrst stage) adjacent to the fewest other undirected edges. If these
edges can be oriented using additive noise models  we make the implied orientations  apply
the extended Meek rules  and then iterate until no more edges can be oriented. Algorithm
2 provides pseudocode. Let G = hV  Ei be the resulting PDAG and ∀Vi ∈ V  let UVi
G denote
the nodes connected to Vi in G by an undirected edge. We get the following results.

Input : PDAG G = hV  Ei
Output: WAN-PDAG G = hV  Ei
s ← 1

while max

UVi

≥ s do

G (cid:12)(cid:12)(cid:12)

Vi∈V (cid:12)(cid:12)(cid:12)
foreach Vi ∈ V such that (cid:12)(cid:12)(cid:12)

s′ ← s
while s′ > 0 do

UVi

G (cid:12)(cid:12)(cid:12)

= s or (cid:12)(cid:12)(cid:12)

UVi

G (cid:12)(cid:12)(cid:12)

< s and UVi

G was updated do

foreach S ⊆ UVi
not create an immorality do

G such that |S| = s′ and ∀Sk ∈ S  orienting Sk → Vi  does

Nonparametrically regress Vi on PaVi
G ∪ S and compute the residual ˆǫiS
Vj
if ˆǫiS ⊥⊥ S and ∄Vj ∈ S and S′ ⊆ U
G such that. regressing Vj on
∪ S′ ∪ Vi results in the residual ˆǫjS′∪{Vi} ⊥⊥ S′ ∪ {Vi} then
PaG
Vj
∀Sk ∈ S  orient Sk → Vi  and ∀Ul ∈ UVi
Apply the extended Meek rules
∀Vm ∈ V  update UVm

G \S orient Vi → Ul

G   set s′ = 1  and break

end

end
s′ ← s′ − 1;

end

end
s ← s + 1

end

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

Algorithm 2: Second Stage of kPC

Lemma 4.1. If an edge is oriented in the second stage of kPC  it is implied by a noninvertible
local additive noise model.

Proof. If the condition at line 8 is true then DVi  PaVi
noise model. All Ul ∈ UVi

G \S must be children of Vi by lemma 3.1.

G ∪ SE is a noninvertible local additive

6

kPC
PC
GES
LiNGAM
400

200

600

Sample Size

1

0.8

0.6

0.4

0.2

0

 

1

0.8

0.6

0.4

0.2

i

i

n
o
s
c
e
r
P

l
l

a
c
e
R

0

 

200

400

600

Sample Size

 

i

i

n
o
s
c
e
r
P

800

1000

 

kPC
PC
GES
LiNGAM
800

1000

l
l

a
c
e
R

kPC
PC
GES
LiNGAM
400

200

600

Sample Size

1

0.8

0.6

0.4

0.2

0

 

1

0.8

0.6

0.4

0.2

0

 

200

400

600

Sample Size

 

i

i

n
o
s
c
e
r
P

800

1000

 

kPC
PC
GES
LiNGAM
800

1000

l
l

a
c
e
R

1

0.8

0.6

0.4

0.2

0

 

1

0.8

0.6

0.4

0.2

kPC
PC
GES
LiNGAM
400

200

600

Sample Size

kPC
PC
GES
LiNGAM

0

 

200

400

600

Sample Size

 

800

1000

 

800

1000

Linear Gaussian

Linear Non-Gaussian

Nonlinear Non-Gaussian

Figure 3: Precision and Recall

Lemma 4.2. Suppose ψ = hVi  Wi is a noninvertible local additive noise model. Then kPC
will make all orientations implied by ψ.

Proof. Let ˜S = W\PaG
Vi
since |˜S| ≤ |UVi
additive noise model  line 8 is satisﬁed so all edges connected to Vi are oriented.

at the current iteration. kPC must terminate with s > |˜S|
G ∪ ˜SE is a noninvertible local

G | so S = ˜S at some iteration. Since DVi  PaVi

for PaG
Vi

Theorem 4.1. Assume data is generated according to some weakly additive noise model
M = hG  Ψi. Then kPC will return the WAN-PDAG instantiated by M assuming perfect
conditional independence information  Markov  faithfulness  and causal suﬃciency.

Proof. The PC algorithm is correct and complete with respect to conditional independence
[2]. Orientations made with respect to additive noise models are correct by lemma 4.1 and
all such orientations that can be made are made by lemma 4.2. The Meek rules  which are
correct and complete [4]  are invoked after each orientation made with respect to additive
noise models so they are invoked after all such orientations are made.

5 Related research

kPC is similar in spirit to the PC-LiNGAM structure learning algorithm [15]  which assumes
dependencies are linear with either Gaussian or non-Gaussian noise. PC-LiNGAM combines
the PC algorithm with LiNGAM to learn structures referred to as ngDAGs. KCL [11] is
a heuristic search for a mixed graph that uses the same kernel-based dependence measures
as kPC (while not determining signiﬁcance threhsholds via a hypothesis test)  but does
not take advantage of additive noise models.
[16] provides a more eﬃcient algorithm for
learning additive noise models  by ﬁrst ﬁnding a causal ordering after doing a series of
high dimensional regressions and HSIC independence tests and then pruning the resulting
DAG implied by this ordering. Finally  [17] proposes a two-stage procedure for learning
additive noise models from data that is similar to kPC  but requires the additive noise
model assumptions in the ﬁrst stage where the Markov equivalence class is identiﬁed.

6 Experimental results

To evaluate kPC  we generated 20 random 7-nodes DAGs using the MCMC algorithm in [18]
and sampled 1000 data points from each DAG under three conditions: linear dependencies

7

I

LOCC

LACC

LIFG

LIPL

LMTG

I

LOCC

LACC

LIFG

LIPL

LMTG

kPC

iMAGES

Figure 4: Structures learned by kPC and iMAGES

with Gaussian noise  linear dependencies with non-Gaussian noise  and nonlinear dependen-
cies with non-Gaussian noise. We generated non-Gaussian noise using the same procedure
as [19] and used polynomial and trigonometric functions for nonlinear dependencies.

We compared kPC to PC  the score-based GES with the BIC-score [20]  and the ICA-based
LiNGAM [19]  which assumes linear dependencies and non-Gaussian noise. We applied two
metrics in measuring performance vs sample size: precision  i.e. proportion of directed edges
in the resulting graph that are in the true DAG  and recall  i.e. proportion of directed edges
in the true DAG that are in the resulting graph. Figure 3 reports the results. In the linear
Gaussian case  we see PC shows slightly better performance than kPC in precision  which is
unsurprising since PC assumes linear Gaussian distributions. Only LiNGAM shows better
recall  but worse precision. LiNGAM performs signiﬁcantly better than the other algorithms
in the linear non-Gaussian case. kPC performs about the same as PC in precision and recall 
which again is unsurprising since previous simulation results have shown that nonlinearity 
but not non-Gaussianity can signiﬁcantly aﬀect the performance of PC. In the nonlinear
non-Gaussian case  kPC performs slightly better than PC in precision. We note  however 
that in some of these cases the performance of kPC was signiﬁcantly better.2

We also ran kPC on data from an fMRI experiment that is analyzed in [21] where nonlinear
dependencies can be observed. Figure 4 shows the structure that kPC learned  where each
of the nodes corresponds to a particular brain region. This structure is the same as the one
learned by the (GES-style) iMAGES algorithm in [21] except for the absence of one edge.
However  iMAGES required background knowledge to direct the edges. kPC successfully
found the same directed edges without using any background knowledge. Domain experts
in neuroscience have conﬁrmed the plausibility of the observed relationships.

7 Conclusion

We introduced weakly additive noise models  which extend the additive noise model frame-
work to cases such as the linear Gaussian  where the additive noise model is invertible and
thus unidentiﬁable  as well as cases where additive noise is not present. The weakly additive
noise framework allows us to identify a unique DAG when the additive noise model assump-
tions hold  and a structure that is at least as speciﬁc as a PDAG (possibly still a unique
DAG) when some additive noise assumptions fail. We deﬁned equivalence classes for such
models and introduced the kPC algorithm for learning these equivalence classes from data.
Finally  we found that the algorithm performed well on both synthetic and real data.

Acknowledgements

We thank Dominik Janzing and Bernhard Sch¨olkopf for helpful comments. RET was funded
by a grant from the James S. McDonnel Foundation. AG was funded by DARPA IPTO
FA8750-09-1-0141  ONR MURI N000140710747  and ARO MURI W911NF0810242.

2When simulating nonlinear data  we must be careful to ensure that variances do not blow up
and result in data for which no ﬁnite sample method can show adequate performance. This has the
unfortunate side eﬀect that the nonlinear data generated may be well approximated using linear
methods. Future research will consider more sophisticated methods for simulating data that is more
appropriate when comparing kPC to linear methods.

8

References

[1] P. O. Hoyer  D. Janzing  J. M. Mooij  J. Peters  and B. Sch¨olkopf. Nonlinear causal
discovery with additive noise models. In Advances in Neural Information Processing
Systems 21  2009.

[2] P. Spirtes  C. Glymour  and R. Scheines. Causation  Prediction  and Search. 2nd

edition  2000.

[3] J. Pearl. Causality: Models  Reasoning  and Inference. 2000.
[4] C. Meek. Causal inference and causal explanation with background knowledge.
Proceedings of the 11th Conference on Uncertainty in Artiﬁcial Intelligence  1995.

In

[5] K. Zhang and A. Hyv¨arinen. On the identiﬁability of the post-nonlinear causal model.

In Proceedings of the 26th Conference on Uncertainty in Artiﬁcial Intelligence  2009.

[6] A. Gretton  K. Fukumizu  C. H. Teo  L. Song  B. Sch¨olkopf  and A. J. Smola. A kernel
statistical test of independence. In Advances in Neural Information Processing Systems
20  2008.

[7] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American

Mathematical Society  68(3):337404  1950.

[8] A. Gretton  K. Borgwardt  M. Rasch  B. Sch¨olkopf  and A. Smola. A kernel method
for the two-sample-problem. In Advances in Neural Information Processing Systems
19  2007.

[9] K. Fukumizu  A. Gretton  X. Sun  and B. Sch¨olkopf. Kernel measures of conditional

dependence. In Advances in Neural Information Processing Systems 20  2008.

[10] B. Sriperumbudur  A. Gretton  K. Fukumizu  G. Lanckriet  and B. Sch¨olkopf. Injective
hilbert space embeddings of probability measures. In Proceedings of the 21st Annual
Conference on Learning Theory  2008.

[11] X. Sun  D. Janzing  B. Scholk¨opf  and K. Fukumizu. A kernel-based causal learning
algorithm. In Proceedings of the 24th International Conference on Machine Learning 
2007.

[12] X. Sun. Causal inference from statistical data. PhD thesis  Max Plank Institute for

Biological Cybernetics  2008.

[13] F. R. Bach and M. I. Jordan. Kernel independent component analysis. Journal of

Machine Learning Research  3:1–48  2002.

[14] S. Fine and K. Scheinberg. Eﬃcient SVM training using low-rank kernel representa-

tions. Journal of Machine Learning Research  2:243–264  2001.

[15] P. O. Hoyer  A. Hyv¨arinen  R. Scheines  P. Spirtes  J. Ramsey  G. Lacerda  and
S. Shimizu. Causal discovery of linear acyclic models with arbitrary distributions.
In Proceedings of the 24th Conference on Uncertainty in Artiﬁcial Intelligence  2008.

[16] J. M. Mooij  D. Janzing  J. Peters  and B. Scholk¨opf. Regression by dependence mini-
mization and its application to causal inference in additive noise models. In Proceedings
of the 26th International Conference on Machine Learning  2009.

[17] K. Zhang and A. Hyv¨arinen. Acyclic causality discovery with additive noise: An
information-theoretical perspective. In Proceedings of the European Conference on Ma-
chine Learning and Principles and Practice of Knowledge Discovery in Databases 2009 
2009.

[18] G. Melan¸con  I. Dutour  and M. Bousquet-M´elou. Random generation of dags for
graph drawing. Technical Report INS-R0005  Centre for Mathematics and Computer
Sciences  2000.

[19] S. Shimizu  P. Hoyer  A. Hyv¨arinen  and A. Kerminen. A linear non-gaussian acyclic
model for causal discovery. Journal of Machine Learning Research  7:1003–2030  2006.
[20] D. M. Chickering. Optimal structure identiﬁcation with greedy search. Journal of

Machine Learning Research  3:507–554  2002.

[21] J. D. Ramsey  S. J. Hanson  C. Hanson  Y. O. Halchenko  R. A. Poldrack  and C. Gly-

mour. Six problems for causal inference from fMRI. NeuroImage  2009. In press.

9

,Jonas Mueller
Tommi Jaakkola
Bo Li
Yining Wang
Aarti Singh
Yevgeniy Vorobeychik