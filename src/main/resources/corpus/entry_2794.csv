2010,Label Embedding Trees for Large Multi-Class Tasks,Multi-class classification becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a tree-structure of classifiers which  by optimizing the overall tree loss  provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster.,Label Embedding Trees for Large Multi-Class Tasks

Samy Bengio(1)

Jason Weston(1) David Grangier(2)

(1) Google Research  New York  NY

{bengio  jweston}@google.com
(2)NEC Labs America  Princeton  NJ
{dgrangier}@nec-labs.com

Abstract

Multi-class classiﬁcation becomes challenging at test time when the number of
classes is very large and testing against every possible class can become compu-
tationally infeasible. This problem can be alleviated by imposing (or learning)
a structure over the set of classes. We propose an algorithm for learning a tree-
structure of classiﬁers which  by optimizing the overall tree loss  provides superior
accuracy to existing tree labeling methods. We also propose a method that learns
to embed labels in a low dimensional space that is faster than non-embedding ap-
proaches and has superior accuracy to existing embedding approaches. Finally
we combine the two ideas resulting in the label embedding tree that outperforms
alternative methods including One-vs-Rest while being orders of magnitude faster.

1

Introduction

Datasets available for prediction tasks are growing over time  resulting in increasing scale in all their
measurable dimensions: separate from the issue of the growing number of examples m and features
d  they are also growing in the number of classes k. Current multi-class applications such as web
advertising [6]  textual document categorization [11] or image annotation [12] have tens or hundreds
of thousands of classes  and these datasets are still growing. This evolution is challenging traditional
approaches [1] whose test time grows at least linearly with k.
At training time  a practical constraint is that learning should be feasible  i.e.
it should not take
more than a few days  and must work with the memory and disk space requirements of the available
hardware. Most algorithms’ training time  at best  linearly increases with m  d and k; algorithms
that are quadratic or worse with respect to m or d are usually discarded by practitioners working on
real large scale tasks. At testing time  depending on the application  very speciﬁc time constraints
are necessary  usually measured in milliseconds  for example when a real-time response is required
or a large number of records need to be processed. Moreover  memory usage restrictions may also
apply. Classical approaches such as One-vs-Rest are at least O(kd) in both speed (of testing a single
example) and memory. This is prohibitive for large scale problems [6  12  26].
In this work  we focus on algorithms that have a classiﬁcation speed sublinear at testing time in k as
well as having limited dependence on d with best-case complexity O(de(log k + d)) with de (cid:28) d
and de (cid:28) k. In experiments we observe no loss in accuracy compared to methods that are O(kd) 
further  memory consumption is reduced from O(kd) to O(kde). Our approach rests on two main
ideas: ﬁrstly  an algorithm for learning a label tree: each node makes a prediction of the subset of
labels to be considered by its children  thus decreasing the number of labels k at a logarithmic rate
until a prediction is reached. We provide a novel algorithm that both learns the sets of labels at each
node  and the predictors at the nodes to optimize the overall tree loss  and show that this approach is
superior to existing tree-based approaches [7  6] which typically lose accuracy compared to O(kd)
approaches. Balanced label trees have O(d log k) complexity as the predictor at each node is still

1

Algorithm 1 Label Tree Prediction Algorithm

Input: test example x  parameters T .
Let s = 0.
repeat
until |‘s| = 1
Return ‘s.

Let s = argmax{c:(s c)∈E}fc(x).

- Start at the root node

- Traverse to the most conﬁdent child.
- Until this uniquely deﬁnes a single label.

linear in d. Our second main idea is to learn an embedding of the labels into a space of dimension
de that again still optimizes the overall tree loss. Hence  we are required at test time to: (1) map
the test example in the label embedding space with cost O(dde) and then (2) predict using the label
tree resulting in our overall cost O(de(log k + d)). We also show that our label embedding approach
outperforms other recently proposed label embedding approaches such as compressed sensing [17].
The rest of the paper is organized as follows. Label trees are discussed and label tree learning
algorithms are proposed in Section 2. Label embeddings are presented in Section 3. Related prior
work is presented in Section 4. An experimental study on three large tasks is given in Section 5
showing the good performance of our proposed techniques. Finally  Section 6 concludes.

a subset of its parent label set with ‘p =S

2 Label Trees
A label tree is a tree T = (N  E  F  L) with n+1 indexed nodes N = {0  . . . n}  a set of edges E =
{(p1  c1)  (p|E|  c|E|)} which are ordered pairs of parent and child node indices  label predictors
F = {f1  . . .   fn} and label sets L = {‘0  . . .   ‘n} associated to each node. The root node is
labeled with index 0. The edges E are such that all other nodes have one parent  but they can have
an arbitrary number of children (but still in all cases |E| = n). The label sets indicate the set of
labels to which a point should belong if it arrives at the given node  and progress from generic to
speciﬁc along the tree  i.e. the root label set contains all classes |‘0| = k and each child label set is
(p c)∈E ‘c. We differentiate between disjoint label trees
where there are only k leaf nodes  one per class  and hence any two nodes i and j at the same depth
cannot share any labels  ‘i ∩ ‘j = ∅  and joint label trees that can have more than k leaf nodes.
Classifying an example with the label tree is achieved by applying Algorithm 1. Prediction begins
at the root node (s = 0) and for each edge leading to a child (s  c) ∈ E one computes the score
of the label predictor fc(x) which predicts whether the example x belongs to the set of labels ‘c.
One takes the most conﬁdent prediction  traverses to that child node  and then repeats the process.
Classiﬁcation is complete when one arrives at a node that identiﬁes only a single label  which is the
predicted class.
Instances of label trees have been used in the literature before with various methods for choosing
the parameters (N  E  F  L). Due to the difﬁculty of learning  many methods make approximations
such as a random choice of E and optimization of F that does not take into account the overall loss
of the entire system leading to suboptimal performance (see [7] for a discussion). Our goal is to
provide an algorithm to learn these parameters to optimize the overall empirical loss (called the tree
loss) as accurately as possible for a given tree size (speed).
We can deﬁne the tree loss we wish to minimize as:

Z

Z

R(ftree) =

I(ftree(x) 6= y)dP (x  y) =

i∈B(x)={b1(x) ...bD(x)(x)} I(y /∈ ‘i)dP (x  y)

max

(1)

where I is the indicator function and

bj(x) = argmax{c : (bj−1(x) c)∈E}fc(x)

is the index of the winning (“best”) node at depth j  b0(x) = 0  and D(x) is the depth in the tree
of the ﬁnal prediction for x  i.e.
the number of loops plus one of the repeat block when running
Algorithm 1. The tree loss measures an intermediate loss of 1 for each prediction at each depth j of
the label tree where the true label is not in the label set ‘bj (x). The ﬁnal loss for a single example
is the max over these losses  because if any one of these classiﬁers makes a mistake then regardless

2

of the other predictions the wrong class will still be predicted. Hence  any algorithm wishing to
optimize the overall tree loss should train all the nodes jointly with respect to this maximum.
We will now describe how we propose to learn the parameters T of our label tree.
In the next
subsection we show how to minimize the tree loss for a given ﬁxed tree (N  E and L are ﬁxed  F is
to be learned). In the following subsection  we will describe our algorithm for learning N  E and L.

2.1 Learning with a Fixed Label Tree

Let us suppose we are given a ﬁxed label tree N  E  L chosen in advance. Our goal is simply to
minimize the tree loss (1) over the variables F   given training data {(xi  yi)}i=1 ... m. We follow the
standard approach of minimizing the empirical loss over the data  while regularizing our solution.
We consider two possible algorithms for solving this problem.
Relaxation 1: Independent convex problems The simplest (and poorest) procedure is to consider
the following relaxation to this problem:

Remp(ftree) =

1
m

max
j∈B(x)

I(yi /∈ ‘j) ≤ 1
m

I(sgn(fj(xi)) = Cj(yi))

where Cj(y) = 1 if y ∈ ‘j and -1 otherwise. The number of errors counted by the approximation
cannot be less than the empirical tree loss Remp as when  for a particular example  the loss is zero
for the approximation it is also zero for Remp. However  the approximation can be much larger
because of the sum.
One then further approximates this by replacing the indicator function with the hinge loss and choos-
ing linear (or kernel) models of the form fi(x) = w>
i φ(x). We are then left with the following
convex problem: minimize

 

nX

j=1

!

mX

i=1

1
m

γ||wj||2 +

s.t. ∀i  j 

ξij

(cid:26) Cj(yi)fj(xi) ≥ 1 − ξij

ξij ≥ 0

mX

i=1

mX

nX

i=1

j=1

where we also added a classical 2-norm regularizer controlled by the hyperparameter γ. In fact  this
can be split into n independent convex problems because the hyperplanes wi  i = 1  . . .   n  do not
interact in the objective function. We consider this simple relaxation as a baseline approach.
Relaxation 2: Tree Loss Optimization (Joint convex problem) We propose a tighter minimization
of the tree loss with the following:

mX

i=1

1
m

ξα
i

ξi ≥ 0  i = 1  . . .   m.

s.t. fr(xi) ≥ fs(xi) − ξi  ∀r  s : yi ∈ ‘r ∧ yi /∈ ‘s ∧ (∃p : (p  r) ∈ E ∧ (p  s) ∈ E)

(2)
(3)
When α is close to zero  the shared slack variables simply count a single error if any of the pre-
dictions at any depth of the tree are incorrect  so this is very close to the true optimization of the
tree loss. This is measured by checking  out of all the nodes that share the same parent  if the one
containing the true label in its label set is highest ranked. In practice we set α = 1 and arrive at a
convex optimization problem. Nevertheless  unlike relaxation (1) the max is not approximated with
a sum. Again  using the hinge loss and a 2-norm regularizer  we arrive at our ﬁnal optimization
problem:

ξi

(4)

nX

γ

||wj||2 +

1
m

mX

i=1

subject to constraints (2) and (3).

j=1

2.2 Learning Label Tree Structures

The previous section shows how to optimize the label predictors F while the nodes N  edges E and
label sets L which specify the structure of the tree are ﬁxed in advance. However  we want to be
able to learn speciﬁc tree structures dependent on our prediction problem such that we minimize the

3

Algorithm 2 Learning the Label Tree Structure

Train k One-vs-Rest classiﬁers ¯f1  . . .   ¯fk independently (no tree structure is used).
Compute the confusion matrix ¯Cij = |{(x  yi) ∈ V : argmaxr
¯fr(x) = j}| on validation set V.
For each internal node l of the tree  from root to leaf  partition its label set ‘l between its chil-
dren’s label sets Ll = {‘c : c ∈ Nl}  where Nl = {c ∈ N : (l  c) ∈ E} and ∪c∈Nl ‘c = ‘l  by
maximizing:

Apq  where A =

1
2

( ¯C + ¯C>) is the symmetrized confusion matrix 

Rl(Ll) = X

X

c∈Nl

yp yq∈‘c

subject to constraints preventing trivial solutions  e.g. putting all labels in one set (see [4]).
This optimization problem (including the appropriate constraints) is a graph cut problem and it
can be solved with standard spectral clustering  i.e. we use A as the afﬁnity matrix for step 1 of
the algorithm given in [21]  and then apply all of its other steps (2-6).
Learn the parameters f of the tree by minimizing (4) subject to constraints (2) and (3).

overall tree loss. This section describes an algorithm for learning the parameters N  E and L  i.e.
optimizing equation (1) with respect to these parameters.
The key to the generalization ability of a particular choice of tree structure is the learnability of the
label sets ‘. If some classes are often confused but are in different label sets the functions f may not
be easily learnable  and the overall tree loss will hence be poor. For example for an image labeling
task  a decision in the tree between two label sets  one containing tiger and jaguar labels versus one
containing frog and toad labels is presumably more learnable than (tiger  frog) vs. (jaguar  toad).
In the following  we consider a learning strategy for disjoint label trees (the methods in the previous
section were for both joint and disjoint trees). We begin by noticing that Remp can be rewritten as:

mX

i=1

I(yi ∈ ‘j)X

¯y /∈‘j



C(xi  ¯y)

Remp(ftree) =

1
m

max

j

where C(xi  ¯y) = I(ftree(xi) = ¯y) is the confusion of labeling example xi (with true label yi) with
label ¯y instead. That is  the tree loss for a given example is 1 if there is a node j in the tree containing
yi  but we predict a different node at the same depth leading to a prediction not in the label set of j.
Intuitively  the confusion of predicting node i instead of j comes about because of the class confusion
between the labels y ∈ ‘i and the labels ¯y ∈ ‘j. Hence  to provide the smallest tree loss we
want to group together labels into the same label set that are likely to be confused at test time.
Unfortunately we do not know the confusion matrix of a particular tree without training it ﬁrst  but
as a proxy we can use the class confusion matrix of a surrogate classiﬁer with the supposition that
the matrices will be highly correlated. This motivates the proposed Algorithm 2. The main idea is
to recursively partition the labels into label sets between which there is little confusion (measuring
confusion using One-vs-Rest as a surrogate classiﬁer) solving at each step a graph cut problem
where standard spectral clustering is applied [20  21]. The objective function of spectral clustering
penalizes unbalanced partitions  hence encouraging balanced trees. (To obtain logarithmic speed-
ups the tree has to be balanced; one could also enforce this constraint directly in the k-means step.)
The results in Section 5 show that our learnt trees outperform random structures and in fact match
the accuracy of not using a tree at all  while being orders of magnitude faster.

3 Label Embeddings

An orthogonal angle of attack of the solution of large multi-class problems is to employ shared
representations for the labelings  which we term label embeddings. Introducing the function φ(y) =
(0  . . .   0  1  0  . . .   0) which is a k-dimensional vector with a 1 in the yth position and 0 otherwise 
we would like to ﬁnd a linear embedding E(y) = V φ(y) where V is a de × k matrix assuming that
labels y ∈ {1  . . .   k}. Without a tree structure  multi-class classiﬁcation is then achieved with:

fembed(x) = argmaxi=1 ... k S (W x  V φ(i))

(5)

4

where W is a de × d matrix of parameters and S(· ·) is a measure of similarity  e.g. an inner
product or negative Euclidean distance. This method  unlike label trees  is unfortunately still linear
with respect to k. However  it does have better behavior with respect to the feature dimension d 
with O(de(d + k)) testing time  compared to methods such as One-vs-Rest which is O(kd). If the
embedding dimension de is much smaller than d this gives a signiﬁcant saving.
There are several ways we could train such models. For example  the method of compressed sensing
[17] has a similar form to (5)  but the matrix V is not learnt but chosen randomly  and only W
is learnt.
In the next section we will show how we can train such models so that the matrix V
captures the semantic similarity between classes  which can improve generalization performance
over random choices of V in an analogous way to the improvement of label trees over random
trees. Subsequently  we will show how to combine label embeddings with label trees to gain the
advantages of both approaches.

3.1 Learning Label Embeddings (Without a Tree)

We consider two possibilities for learning V and W .
Sequence of Convex Problems Firstly  we consider learning the label embedding by solving a se-
quence of convex problems using the following method. First  train independent (convex) classiﬁers
fi(x) for each class 1  . . .   k and compute the k×k confusion matrix ¯C over the data (xi  yi)  i.e. the
same as the ﬁrst two steps of Algorithm 2. Then  ﬁnd the label embedding vectors Vi that minimize:

( ¯C + ¯C>) is the symmetrized confusion matrix 

kX

Aij||Vi − Vj||2  where A =

subject to the constraint V >DV = I where Dii =P

i j=1

1
2

j Aij (to prevent trivial solutions) which is the
same problem solved by Laplacian Eigenmaps [4]. We then obtain an embedding matrix V where
similar classes i and j should have small distance between their vectors Vi and Vj. All that remains is
to learn the parameters W of our model. To do this  we can then train a convex multi-class classiﬁer
utilizing the label embedding V : minimize

γ||W||F RO +

1
m

mX

i=1

ξi

where ||.||F RO is the Frobenius norm  subject to constraints:

||W xi − V φ(i)||2 ≤ ||W xi − V φ(j)||2 + ξi 

∀j 6= i

(6)

Note that the constraint (6) is linear as we can multiply out and subtract ||W xi||2 from both sides.
At test time we employ equation (5) with S(z  z0) = −||z − z0||.
Non-Convex Joint Optimization The second method is to learn W and V jointly  which requires
non-convex optimization. In that case we wish to directly minimize:

ξi ≥ 0 

i = 1  . . .   m.

γ||W||F RO +

1
m

mX

i=1

ξi

subject to (W xi)>V φ(i) ≥ (W xi)>V φ(j) − ξi 

∀j 6= i

and ||Vi|| ≤ 1   ξi ≥ 0  i = 1  . . .   m. We optimize this using stochastic gradient descent (with
randomly initialized weights) [8]. At test time we employ equation (5) with S(z  z0) = z>z0.

3.2 Learning Label Embedding Trees

In this work  we also propose to combine the use of embeddings and label trees to obtain the ad-
vantages of both approaches  which we call the label embedding tree. At test time  the resulting
label embedding tree prediction is given in Algorithm 3. The label embedding tree has potentially
O(de(d + log(k))) testing speed  depending on the structure of the tree (e.g. being balanced).

5

Algorithm 3 Label Embedding Tree Prediction Algorithm

Input: test example x  parameters T .
Compute z = W x.
Let s = 0.
repeat
until |‘s| = 1
Return ‘s.

Let s = argmax{c:(s c)∈E}fc(x) = argmax{c:(s c)∈E}z>E(c).

- Cache prediction on example
- Start at the root node
- Traverse to the most
conﬁdent child.
- Until this uniquely deﬁnes a single label.

To learn a label embedding tree we propose the following minimization problem:

γ||W||F RO +

1
m

ξi

mX

i=1

subject to constraints:
(W xi)>V φ(r) ≥ (W xi)>V φ(s) − ξi  ∀r  s : yi ∈ ‘r ∧ yi /∈ ‘s ∧ (∃p : (p  r) ∈ E ∧ (p  s) ∈ E)

||Vi|| ≤ 1  ξi ≥ 0  i = 1  . . .   m.

This is essentially a combination of the optimization problems deﬁned in the previous two Sections.
Learning the tree structure for these models can still be achieved using Algorithm 2.

4 Related Work

Multi-class classiﬁcation is a well studied problem. Most of the prior approaches build upon binary
classiﬁcation and have a classiﬁcation cost which grows at least linearly with the number of classes
k. Common multi-class strategies include one-versus-rest  one-versus-one  label ranking and Deci-
sion Directed Acyclic Graph (DDAG). One-versus-rest [25] trains k binary classiﬁers discriminating
each class against the rest and predicts the class whose classiﬁer is the most conﬁdent  which yields
a linear testing cost O(k). One-versus-one [16] trains a binary classiﬁer for each pair of classes
and predicts the class getting the most pairwise preferences  which yields a quadratic testing cost
O(k · (k − 1)/2). Label ranking [10] learns to assign a score to each class so that the correct class
should get the highest score  which yields a linear testing cost O(k). DDAG [23] considers the same
k · (k − 1)/2 classiﬁers as one-versus-one but achieves a linear testing cost O(k). All these methods
are reported to perform similarly in terms of accuracy [25  23].
Only a few prior techniques achieve sub-linear testing cost. One way is to simply remove labels the
classiﬁer performs poorly on [11]. Error correcting code approaches [13] on the other hand represent
each class with a binary code and learn a binary classiﬁer to predict each bit. This means that
the testing cost could potentially be O(log k). However  in practice  these approaches need larger
redundant codes to reach competitive performance levels [19]. Decision trees  such as C4.5 [24]  can
also yield a tree whose depth (and hence test cost) is logarithmic in k. However  testing complexity
also grows linearly with the number of training examples making these methods impractical for
large datasets [22].
Filter tree [7] and Conditional Probability Tree (CPT) [6] are logarithmic approaches that have been
introduced recently with motivations similar to ours  i.e. addressing large scale problems with a
thousand classes or more. Filter tree considers a random binary tree in which each leaf is associated
with a class and each node is associated with a binary classiﬁer. A test example traverses the tree
from the root. At each node  the node classiﬁer decides whether the example is directed to the
right or to the left subtree  each of which are associated to half of the labels of the parent node.
Finally  the label of the reached leaf is predicted. Conditional Probability Tree (CPT) relies on a
similar paradigm but builds the tree during training. CPT considers an online setup in which the
set of classes is discovered during training. Hence  CPT builds the tree greedily: when a new class
is encountered  it is added by splitting an existing leaf. In our case  we consider that the set of
classes are available prior to training and propose to tessellate the class label sets such that the node
classiﬁers are likely to achieve high generalization performance. This contribution is shown to have
a signiﬁcant advantage in practice  see Section 5.

6

Finally  we should mention that a related active area of research involves partitioning the feature
space rather than the label space  e.g. using hierarchical experts [18]  hashing [27] and kd-trees [5].
Label embedding is another key aspect of our work when it comes to efﬁciently handling thousands
of classes. Recently  [26] proposed to exploit class taxonomies via embeddings by learning to project
input vectors and classes into a common space such that the classes close in the taxonomy should
have similar representations while  at the same time  examples should be projected close to their
class representation. In our case  we do not rely on a pre-existing taxonomy: we also would like
to assign similar representations to similar classes but solely relying on the training data. In that
respect  our work is closer to work in information retrieval [3]  which proposes to embed documents
– not classes – for the task of document ranking. Compressed sensing based approaches [17] do
propose to embed class labels  but rely on a random projection for embedding the vector representing
class memberships  with the added advantages of handling problems for which multiple classes are
active for a given example. However  relying on a random projection does not allow for the class
embedding to capture the relation between classes. In our experiments  this aspect is shown to be a
drawback  see Section 5. Finally  the authors of [2] do propose an embedding approach over class
labels  but it is not clear to us if their approach is scalable to our setting.

5 Experimental Study

We consider three datasets: one publicly available image annotation dataset and two proprietary
datasets based on images and textual descriptions of products.
ImageNet Dataset ImageNet [12] is a new image dataset organized according to WordNet [14]
where quality-controlled human-veriﬁed images are tagged with labels. We consider the task of
annotating images from a set of about 16 thousand labels. We split the data into 2.5M images for
training  0.8M for validation and 0.8M for testing  removing duplicates between training  validation
and test sets by throwing away test examples which had too close a nearest neighbor training or
validation example in feature space. Images in this database were represented by a large but sparse
vector of color and texture features  known as visual terms  described in [15].
Product Datasets We had access to a large proprietary database of about 0.5M product descriptions.
Each product is associated with a textual description  an image  and a label. There are ≈18 thousand
unique labels. We consider two tasks: predicting the label given the textual description  and predict-
ing the label given the image. For the text task we extracted the most frequent set of 10 thousand
words (discounting stop words) to yield a textual dictionary  and represented each document by a
vector of counts of these words in the document  normalized using tf-idf. For the image task  images
were represented by a dense vector of 1024 real values of texture and color features.
Table 1 summarizes the various datasets. Next  we describe the approaches that we compared.
Flat versus Tree Learning Approaches In Table 2 we compare label tree predictor training meth-
ods from Section 2.1: the baseline relaxation 1 (“Independent Optimization”) versus our proposed
relaxation 2 (“Tree Loss Optimization”)  both of which learn the classiﬁers for ﬁxed trees; and we
compare our “Learnt Label Tree” structure learning algorithm from Section 2.2 to random struc-
tures. In all cases we considered disjoint trees of depth 2 with 200 internal nodes. The results show
that learnt structure performs better than random structure and tree loss optimization is superior
to independent optimization. We also compare to three other baselines: One-vs-Rest large margin
classiﬁers trained using the passive aggressive algorithm [9]  the Filter Tree [7] and the Conditional
Probability Tree (CPT) [6]. For all algorithms  hyperparameters are chosen using the validation set.
The combination of Learnt Label Tree structure and Tree Loss Optimization for the label predictors
is the only method that is comparable to or better than One-vs-Rest while being around 60× faster
to compute at test time.
For ImageNet one could wonder how well using WordNet (a graph of human annotated label sim-
ilarities) to build a tree would perform instead. We constructed a matrix C for Algorithm 2 where
Cij = 1 if there is an edge in the WordNet graph  and 0 otherwise  and used that to learn a label
tree as before  obtaining 0.99% accuracy using “Independent Optimization”. This is better than a
random tree but not as good as using the confusion matrix  implying that the best tree to use is the
one adapted to the supervised task of interest.

7

Table 1: Summary Statistics of the Three Datasets Used in the Experiments.

Statistics
Task
Number of Training Documents
Number of Test Documents
Validation Documents
Number of Labels
Type of Documents
Type of Features
Number of Features
Average Feature Sparsity

ImageNet
image annotation
2518604
839310
837612
15952
images
visual terms
10000
97.5%

Product Descriptions
product categorization
417484
60278
105572
18489
texts
words
10000
99.6%

Product Images
image annotation
417484
60278
105572
18489
images
dense image features
1024
0.0%

Table 2: Flat versus Tree Learning Results Test set accuracies for various tree and non-tree meth-
ods on three datasets. Speed-ups compared to One-vs-Rest are given in brackets.
Product Desc.
Classiﬁer
37.0% [1×]
One-vs-Rest
14.4% [1285×]
Filter Tree
26.3% [45×]
Conditional Prob. Tree (CPT) CPT
21.3% [59×]
Independent Optimization
27.1% [59×]
Independent Optimization
39.6% [59×]
Tree Loss Optimization

Product Images
12.6% [1×]
0.73% [1320×]
2.20% [115×]
1.35% [61×]
5.95% [61×]
10.6% [61×]

ImageNet
2.27% [1×]
0.59% [1140×]
0.74% [41×]
0.72% [60×]
1.25% [60×]
2.37% [60×]

Random Tree
Learnt Label Tree
Learnt Label Tree

Tree Type
None (ﬂat)
Filter Tree

Table 3: Label Embeddings and Label Embedding Tree Results

ImageNet

Product Images

Tree Type
Classiﬁer
None (ﬂat)
One-vs-Rest
Compressed Sensing
None (ﬂat)
Seq. Convex Embedding None (ﬂat)
Non-Convex Embedding None (ﬂat)
Label Tree
Label Embedding Tree

Accuracy
2.27%
0.6%
2.23%
2.40%
2.54%

Speed Memory Accuracy
1×
3×
3×
3×
85×

1.2 GB
18 MB
18 MB
18 MB
18 MB

12.6%
2.27%
3.9%
14.1%
13.3%

Speed Memory
1×
170 MB
10×
20 MB
10×
20 MB
10×
20 MB
142× 20 MB

Embedding and Embedding Tree Approaches In Table 3 we compare several label embedding
methods: (i) the convex and non-convex methods from Section 5; (ii) compressed sensing; and
(iii) the label embedding tree from Section 3.2. In all cases we ﬁxed the embedding dimension
de = 100. The results show that the random embeddings given by compressed sensing are inferior
to learnt embeddings and Non-Convex Embedding is superior to Sequential Convex Embedding 
presumably as the overall loss which is dependent on both W and V is jointly optimized. The latter
gives results as good or superior to One-vs-Rest with modest computational gain (3× or 10× speed-
up). Note  we do not detail results on the product descriptions task because no speed-up is gained
there from embedding as the sparsity is already so high  however the methods still gave good test
accuracy (e.g. Non-Convex Embedding yields 38.2%  which should be compared to the methods in
Table 2). Finally  combining embedding and label tree learning using the “Label Embedding Tree”
of Section 3.2 yields our best method on ImageNet and Product Images with a speed-up of 85× or
142× respectively with accuracy as good or better than any other method tested. Moreover  memory
usage of this method (and other embedding methods) is signiﬁcantly less than One-vs-Rest.

6 Conclusion

We have introduced an approach for fast multi-class classiﬁcation by learning label embedding trees
by (approximately) optimizing the overall tree loss. Our approach obtained orders of magnitude
speedup compared to One-vs-Rest while yielding as good or better accuracy  and outperformed
other tree-based or embedding approaches. Our method makes real-time inference feasible for very
large multi-class tasks such as web advertising  document categorization and image annotation.

Acknowledgements

We thank Ameesh Makadia for very useful discussions.

8

References
[1] E. Allwein  R. Schapire  and Y. Singer. Reducing multiclass to binary: a unifying approach for margin

classiﬁers. Journal of Machine Learning Research (JMLR)  1:113–141  2001.

[2] Y. Amit  M. Fink  N. Srebro  and S. Ullman. Uncovering shared structures in multiclass classiﬁcation. In

Proceedings of the 24th international conference on Machine learning  page 24. ACM  2007.

[3] B. Bai  J. Weston  D. Grangier  R. Collobert  C. Cortes  and M. Mohri. Half transductive ranking. In

Artiﬁcial Intelligence and Statistics (AISTATS)  2010.

[4] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering.

Advances in neural information processing systems  1:585–592  2002.

[5] J.L. Bentley. Multidimensional binary search trees used for associative searching. Communications of the

ACM  18(9):517  1975.

[6] A. Beygelzimer  J. Langford  Y. Lifshits  G. Sorkin  and A. Strehl. Conditional probability tree estimation

analysis and algorithm. In Conference in Uncertainty in Artiﬁcial Intelligence (UAI)  2009.

[7] A. Beygelzimer  J. Langford  and P. Ravikumar. Error-correcting tournaments. In International Confer-

ence on Algorithmic Learning Theory (ALT)  pages 247–262  2009.

[8] L´eon Bottou. Stochastic learning. In Olivier Bousquet and Ulrike von Luxburg  editors  Advanced Lec-
tures on Machine Learning  Lecture Notes in Artiﬁcial Intelligence  LNAI 3176  pages 146–168. Springer
Verlag  Berlin  2004.

[9] K. Crammer  O. Dekel  J. Keshet  S. Shalev-Shwartz  and Y. Singer. Online passive-aggressive algorithms.

Journal of Machine Learning Research  7:551–585  2006.

[10] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector ma-

chines. Journal of Machine Learning Research (JMLR)  2:265–292  2002.

[11] O. Dekel and O. Shamir. Multiclass-Multilabel Learning when the Label Set Grows with the Number of

Examples. In Artiﬁcial Intelligence and Statistics (AISTATS)  2010.

[12] J. Deng  W. Dong  R. Socher  Li-Jia Li  K. Li  and Fei-Fei Li. Imagenet: A large-scale hierarchical image

database. In Conference on Computer Vision and Pattern Recognition (CVPR)  pages 248–255  2009.

[13] T. Dietterich and G. Bakiri. On the algorithmic implementation of multiclass kernel-based vector ma-

chines. Journal of Artiﬁcial Intelligence Research (JAIR)  2:263–286  1995.

[14] C. Fellbaum  editor. WordNet: An Electronic Lexical Database. MIT Press  1998.
[15] David Grangier and Samy Bengio. A discriminative kernel-based model to rank images from text queries.

Transactions on Pattern Analysis and Machine Intelligence  30(8):1371–1384  2008.

[16] T. Hastie and R. Tibshirani. Classication by pairwise coupling. The Annals of Statistics  26(2):451–471 

2001.

[17] D. Hsu  S. Kakade  J. Langford  and T. Zhang. Multi-label prediction via compressed sensing. In Neural

Information Processing Systems (NIPS)  2009.

[18] M.I. Jordan and R.A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural computation 

6(2):181–214  1994.

[19] J. Langford and A. Beygelzimer. Sensitive error correcting output codes. In Conference on Learning

Theory (COLT)  pages 158–172  2005.

[20] U. Luxburg. A tutorial on spectral clustering. Statistics and Computing  17(4):416  2007.
[21] A.Y. Ng  M.I. Jordan  and Y. Weiss. On spectral clustering: Analysis and an algorithm. Advances in

neural information processing systems  2:849–856  2002.

[22] T. Oates and D. Jensen. The effects of training set size on decision tree complexity. In International

Conference on Machine Learning (ICML)  pages 254–262  1997.

[23] J. Platt  N. Cristianini  and J. Shawe-Taylor. Large margin dags for multiclass classiﬁcation. In NIPS 

pages 547–553  2000.

[24] J. Quinlan. C4.5 : programs for machine learning. Morgan Kaufmann  1993.
[25] R. Rifkin and A. Klautau. In defense of one-vs-all classiﬁcation. Journal of Machine Learning Research

(JMLR)  5:101–141  2004.

[26] K. Weinberger and O. Chapelle. Large margin taxonomy embedding for document categorization. In

NIPS  pages 1737–1744  2009.

[27] P.N. Yianilos. Data structures and algorithms for nearest neighbor search in general metric spaces. In
Proceedings of the fourth annual ACM-SIAM Symposium on Discrete algorithms  page 321. Society for
Industrial and Applied Mathematics  1993.

9

,Kirthevasan Kandasamy
Akshay Krishnamurthy
Barnabas Poczos
Larry Wasserman
james robins
Maja Rudolph
Francisco Ruiz
Stephan Mandt
David Blei