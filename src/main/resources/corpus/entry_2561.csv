2019,Implicit Regularization of Accelerated Methods in Hilbert Spaces,We study learning properties of accelerated gradient descent methods for linear least-squares in Hilbert spaces. We analyze the implicit regularization properties of Nesterov acceleration and a variant of heavy-ball in terms of corresponding learning error bounds. Our results show that acceleration can provides faster bias decay  than gradient descent  but also suffers of a more unstable behavior. As a result acceleration cannot be in general expected to improve learning accuracy with respect to gradient descent  but rather to achieve the same accuracy with reduced computations. Our theoretical results are validated by numerical simulations. Our analysis is based on studying suitable polynomials induced by the accelerated dynamics and combining spectral techniques with concentration inequalities.,Implicit Regularization of Accelerated Methods in

Hilbert Spaces

Nicolò Pagliana

University of Genoa
DIMA & MaLGa

pagliana@dima.unige.it

Lorenzo Rosasco
University of Genoa

DIBRIS  MaLGa  IIT & MIT

lrosasco@mit.edu

Abstract

We study learning properties of accelerated gradient descent methods for linear
least-squares in Hilbert spaces. We analyze the implicit regularization properties
of Nesterov acceleration and a variant of heavy-ball in terms of corresponding
learning error bounds. Our results show that acceleration can provides faster bias
decay than gradient descent  but also suffers of a more unstable behavior. As a
result acceleration cannot be in general expected to improve learning accuracy with
respect to gradient descent  but rather to achieve the same accuracy with reduced
computations. Our theoretical results are validated by numerical simulations.
Our analysis is based on studying suitable polynomials induced by the accel-
erated dynamics and combining spectral techniques with concentration inequalities.

1

Introduction

The focus on optimization is a major trend in modern machine learning  where efﬁciency is mandatory
in large scale problems [4]. Among other solutions  ﬁrst order methods have emerged as methods of
choice. While these techniques are known to have potentially slow convergence guarantees  they also
have low iteration costs  ideal in large scale problems. Consequently the question of accelerating
ﬁrst order methods while keeping their small iteration costs have received much attention  see e.g.
[33]. Since machine learning solutions are typically derived minimizing an empirical objective (the
training error)  most theoretical studies have focused on the error estimated for this latter quantity.
However  it has recently become clear that optimization can play a key role from a statistical point of
view when the goal is to minimize the expected (test) error. On the one hand  iterative optimization
implicitly bias the search for a solution  e.g. converging to suitable minimal norm solutions [27]. On
the other hand  the number of iterations parameterize paths of solutions of different complexity [31].
The idea that optimization can implicitly perform regularization has a long history. In the context
of linear inverse problems  it is known as iterative regularization [11]. It is also an old trick for
training neural networks where it is called early stopping [15]. The question of understanding the
generalization properties of deep learning applications has recently sparked a lot of attention on
this approach  which has be referred to as implicit regularization  see e.g. [13]. Establishing the
regularization properties of iterative optimization requires the study of the corresponding expected
error by combining optimization and statistical tools. First results in this sense focused on linear
least squares with gradient descent and go back to [6  31]  see also [25] and references there in for
improvements. Subsequent works have started considering other loss functions [16]  multi-linear
models [13] and other optimization methods  e.g. stochastic approaches [26  18  14].
In this paper  we consider the implicit regularization properties of acceleration. We focus on linear
least squares in Hilbert space  because this setting allows to derive sharp results and working in
inﬁnite dimension magnify the role of regularization. Unlike in ﬁnite dimension learning bounds are

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

possible only if some form of regularization is considered. In particular  we consider two of the most
popular accelerated gradient approaches  based on Nesterov acceleration [22] and (a variant of) the
heavy-ball method [24]. Both methods achieve acceleration by exploiting a so called momentum
term  which uses not only the previous  but the previous two iterations at each step. Considering
a suitable bias-variance decomposition  our results show that accelerated methods have a behavior
qualitatively different from basic gradient descent. While the bias decays faster with the number of
iterations  the variance increases faster too. The two effect balance out  showing that accelerated
methods achieve the same optimal statistical accuracy of gradient descent but they can indeed do
this with less computations. Our analysis takes advantage of the linear structures induced by least
squares to exploit tools from spectral theory. Indeed  the characterization of both convergence and
stability rely on the study of suitable spectral polynomials deﬁned by the iterates. While the idea that
accelerated methods can be more unstable  this has been pointed out in [10] in a pure optimization
context. Our results quantify this effect from a statistical point of view. Close to our results is the
study in [9]  where a stability approach is considered to analyze gradient methods for different loss
functions [5].

2 Learning with (accelerated) gradient methods
Let the input space X be a separable Hilbert space (with scalar product (cid:104)· ·(cid:105) and induced norm (cid:107)·(cid:107))
and the output space be R 1. Let ρ be a unknown probability measure on the input-output space
X × R  ρX the induced marginal probability on X  and ρ(·|x) the conditional probability measure on
R given x ∈ X. We make the following standard assumption: there exist κ > 0 such that

(cid:104)x  x(cid:48)(cid:105) ≤ κ2

∀x  x(cid:48) ∈ X  ρX-almost surely.

The goal of least-squares linear regression is to solve the expected risk minimization problem

(cid:90)

E(w)  E(w) =

inf
w∈X

X×R

((cid:104)w  x(cid:105) − y)2 dρ(x  y) 

(1)

(2)

n(cid:88)

i=1

where ρ is known only through the n i.i.d. samples (x1  y1)  . . .   (xn  yn). In the following  we
measure the quality of an approximate solution ˆw with the excess risk

E( ˆw) − inf

E .

X

The search of a solution is often based on replacing (2) with the empirical risk minimization (ERM)

ˆE(w) 

min
w∈X

ˆE(w) =

1
n

((cid:104)w  xi(cid:105) − yi)2 .

(3)

For least squares an ERM solution can be computed in closed form using a direct solver. However 
for large problems  iterative solvers are preferable and we next describe the approaches we consider.
First  it is useful to rewrite the ERM with vectors notation. Let y ∈ Rn with (y)i = yi and
X : X → Rn s.t. (X w)i = (cid:104)w  xi(cid:105) for i = 1 . . .   n. Here the norm (cid:107)·(cid:107)n is norm in Rn multiplied by
√
i=1 xiyi. Then  ERM becomes
1/
(4)

n. Let X∗ : Rn → X be the adjoint of X deﬁned by X∗ y = 1
ˆE(w) = (cid:107)X w − y(cid:107)2
n .

(cid:80)n

n

min
w∈X

2.1 Gradient descent and accelerated methods

Gradient descent serves as a reference approach throughout the paper. For problem (4) it becomes
ˆwt+1 = ˆwt − α X∗ (X ˆwt − y)
(5)
2 The progress made by gradient
with initial point ˆw0 = 0 and the step-size α that satisfy α < 1
κ2
descent at each iteration can be slow and the idea behind acceleration is to use the information of the
previous directions in order to improves the convergence rate of the algorithm.

1As shown in Appendix this choice allows to recover nonparametric kernel learning as a special case.
2 The step-size α is the step-size at the t-th iteration and satisﬁes the condition 0 < α(cid:107)X(cid:107)2

op < 1   where
(cid:107)·(cid:107)op denotes the operatorial norm. Since the operator X is bounded by κ (which means (cid:107)X(cid:107)op ≤ κ) it is
sufﬁcient to assume α < 1
κ2 .

2

Heavy-ball
Heavy-ball is a popular accelerated method that adds the momentum ˆwt − ˆwt−1 at each iteration

ˆwt+1 = ˆwt − α X∗ (X ˆwt − y) + β( ˆwt − ˆwt−1)

(6)
with α  β ≥ 0  the case β = 0 reduces to gradient descent. In the quadratic case we consider it is
also called Chebyshev iterative method. The optimization properties of heavy-ball have been studied
extensively [24  32]. Here  we consider the following variant. Let ν > 1  consider the varying
parameter heavy-ball replacing α  β in (6) with αt+1  βt+1 deﬁned as:

αt =

4
κ2

(2t + 2ν − 1)(t + ν − 1)
(t + 2ν − 1)(2t + 4ν − 1)

(t − 1)(2t − 3)(2t + 2ν − 1)

βt =

(t + 2ν − 1)(2t + 4ν − 1)(2t + 2ν − 3)

 

for t > 0 and with initialization ˆw−1 = ˆw0 = 0  α1 = 1
4ν+1   β1 = 0. With this choice and
considering the least-squares problem this algorithm is known as ν−method in the inverse problem
κ2
literature (see e.g. [11]). This seemingly complex parameters’ choice allows to relates the approach
to suitable orthogonal polynomials recursion as we discuss later.

4ν+2

Nesterov acceleration

The second form of gradient acceleration we consider is the popular Nesterov acceleration [22]. In
our setting  it corresponds to the iteration

ˆwt+1 = ˆvt − α X∗ (X ˆvt − y)  

ˆvt = ˆwt + βt ( ˆwt − ˆwt−1)

with the two initial points ˆw−1 = ˆw0 = 0  and the sequence βt chosen as

t − 1
t + β

βt =

β ≥ 1 .

 

(7)

(8)

Differently from heavy-ball  Nesterov acceleration uses the momentum term also in the evaluation of
the gradient. Also in this case optimization results are well known [1  29].
Here  as above  optimization results refer to solving ERM (3)  (4)  whereas in the following we study
to which extent the above iterations can used to minimize the expected error (2). In the next section 
we discuss a spectral approach which will be instrumental towards this goal.

3 Spectral ﬁltering for accelerated methods

Least squares allows to consider spectral approaches to study the properties of gradient methods for
learning. We illustrate these ideas for gradient descent before considering accelerated methods.

Gradient descent as spectral ﬁltering

Note that by a simple (and classical) induction argument  gradient descent can be written as

t−1(cid:88)

j=0

ˆwt = α

(I − α ˆΣ)j X∗ y .

with

ˆΣ = X∗ X 

j=0(I − ασ)j for all σ ∈ (0  κ2] and t ∈ N. Note that  the
polynomials gt are bounded by αt. A ﬁrst observation is that gt(σ)σ converges to 1 as t → ∞  since
σ . A second observation is that the residual polynomials rt(σ) = 1 − σgt(σ) 
gt(σ) converges to 1
which are all bounded by 1  control ERM convergence since 

Equivalently using spectral calculus

ˆwt = gt( ˆΣ) X∗ y  

where gt are the polynomials gt(σ) = α(cid:80)t−1
(cid:13)(cid:13)(cid:13)n
|rt(σ)σq| ≤(cid:16) q

(cid:13)(cid:13)(cid:13)X gt( ˆΣ) X

(cid:107)X ˆwt − y(cid:107)n =

∗ y − y

=

(cid:13)(cid:13)(cid:13)n
(cid:13)(cid:13)(cid:13)gt( ˆΣ) ˆΣy − y
(cid:19)q
(cid:17)q(cid:18) 1

=

.

In particular  if y is in the range of ˆΣr for some r > 0 (source condition on y) improved convergence
rates can be derived noting that by an easy calculation

(cid:13)(cid:13)(cid:13)n
(cid:13)(cid:13)(cid:13)rt( ˆΣ)y

(cid:13)(cid:13)(cid:13)op
≤(cid:13)(cid:13)(cid:13)rt( ˆΣ)

(cid:107)y(cid:107)n .

α

t

3

As we show in Section 4  considering the polynomials gt and rt allows to study not only ERM
but also expected risk minimization (2)  by relating gradient methods to their inﬁnite sample limit.
Further  we show how similar reasoning hold for accelerated methods. In order to do so  it useful to
ﬁrst deﬁne the characterizing properties of gt and rt.

3.1 Spectral ﬁltering

The following deﬁnition abstracts the key properties of the function gt and rt often called spectral
ﬁltering function [2]. Following the classical deﬁnition we replace t with a generic parameter λ.
Deﬁnition 1.
The family {gλ}λ∈(0 1] is called spectral ﬁltering function if the following conditions hold:
(i) There exist a constant E < +∞ such that  for any λ ∈ (0  1]
|gλ(σ)| ≤ E
λ

sup

(9)

σ∈(0 κ2]

.

(ii) Let rλ(σ) = 1 − σ gλ(σ) there exist a constant F0 such that  for any λ ∈ (0  1]

|rλ(σ)| ≤ F0 .

sup

σ∈(0 κ2]

(10)

Deﬁnition 2. (Qualiﬁcation)
The qualiﬁcation of the spectral ﬁltering function {gλ}λ is the maximum parameter q such that for
any λ ∈ (0  1] there exist a constant Fq such that

|rλ(σ)σq| ≤ Fqλq .

sup

σ∈(0 κ2]

(11)

Moreover we say that a ﬁltering function has qualiﬁcation ∞ if (11) holds for every q > 0.
Methods with ﬁnite qualiﬁcation might have slow convergence rates in certain regimes. The smallest
the qualiﬁcation the worse the rates can be.
The discussion in the previous section shows that gradient descent deﬁnes a spectral ﬁltering function
where λ = 1/t. More precisely  the following holds.
for t ∈ N  then the polynomials gt related to the gradient descent
Proposition 1. Assume λ = 1
t
iterates  deﬁned in (5)  are a ﬁltering function with parameters E = α and F0 = 1. Moreover it has
qualiﬁcation ∞ with parameters Fq = (q/α)q.
The above result is classical and we report a proof in the appendix for completeness. Next  we discuss
analogous results for accelerate methods and then compare the different spectral ﬁltering functions.

3.2 Spectral ﬁltering for accelerated methods

For the heavy-ball (6) the following result holds
Proposition 2. Assume κ ≤ 1  let ν > 0 and λ = 1
t2 for t ∈ N  then the polynomials gt related to
heavy-ball method (6) are a ﬁltering function with parameters E = 2 and F0 = 1. Moreover there
exist a positive constant cν < +∞ such that the ν-method has qualiﬁcation ν.
The proof of the above proposition follows combining several intermediate results from [11]. The
key idea is to show that the residual polynomials deﬁned by heavy-ball iteration form a sequence of
orthogonal polynomials with respect to the weight function

ων(σ) =

σ2ν

2 (1 − σ)
σ 1

 

1
2

which is a so called shifted Jacobi weight. Results from orthogonal polynomials can then be used to
characterize the corresponding spectral ﬁltering function.
The following proposition considers Nesterov acceleration.

4

Proposition 3. Assume λ = 1/t2  then the polynomials gt related to Nesterov iterates (7) are a
ﬁltering function with constants E = 2α and F0 = 1. Moreover the qualiﬁcation of this method is at
least 1/2 with constants Fq =

.

(cid:17)q

(cid:16) β2

α

Filtering properties of the Nesterov iteration (7) have been studied recently in the context of inverse
problems [23]. In the appendix 7.3 we provide a simpliﬁed proof based on studying the properties of
suitable discrete dynamical systems deﬁned by the Nesterov iteration (7).

3.3 Comparing the different ﬁlter functions

We summarize the properties of the spectral ﬁltering function of the various methods for κ = 1.

Method

Gradien descent

Heavy-ball
Nesterov

E F0
1
1
1
2
2
1

Fq
qq
(q = ν)
β2q

cν

Qualiﬁcation

∞
ν
≥ 1/2

The main observation is that the properties of the spectral ﬁltering functions corresponding to the
different iterations depend on λ = 1/t for gradient descent  but on λ = 1/t2 for the accelerated
methods. As we see in the next section this leads to substantially different learning properties. Further
we can see that gradient descent is the only algorithm with qualiﬁcation ∞  even if the parameter
Fq = qq can be very large. The accelerated methods seem to have smaller qualiﬁcation. In particular 
the heavy-ball method can attain a high qualiﬁcation  depending on ν  but the constant cν is unknown
and could be large. For Nesterov accelerated method  the qualiﬁcation is at least 1/2 and it’s an open
question whether this bound is tight or higher qualiﬁcation can be attained.
In the next section  we show how the properties of the spectral ﬁltering functions can be exploited to
study the excess risk of the corresponding iterations.

4 Learning properties for accelerated methods

We ﬁrst consider a basic scenario and then a more reﬁned analysis leading to a more general setting
and potentially faster learning rates.

4.1 Attainable case

Consider the following basic assumption.
Assumption 1. Assume there exist M > 0 such that |y| < M ρ-almost surely and w∗ ∈ X such that
E(w∗) = inf X E.
Then the following result can be derived.
Theorem 1. Under Assumption 1  let ˆwGD
be the t-th iterations respectively of gradient
descent (5) and an accelerated version given by (6) or (7). Assuming the sample-size n to be
large enough and let δ ∈ (0  1/2) then there exist two positive constant C1 and C2 such that with
probability at least 1 − δ

and ˆwacc

t

t

E( ˆwGD

t

) − inf

H

E ≤ C1

E( ˆwacc

t

) − inf

H

E ≤ C2

(cid:18) 1
(cid:18) 1

t

+

t2 +

(cid:19)
(cid:19)

t
n
t2
n

log2 2
δ
log2 2
δ

.

where the constants C1 and C2 do not depend on n  t  δ  but depend on the chosen optimization
method.
Moreover by choosing the stopping rules tGD = O(n1/2) and tacc = O(n1/4) both algorithms have
learning rate of order 1√
n .

The proof of the above results is given in the appendix and the novel part is the one concerning
accelerated methods  particularly Nesterov acceleration. The result shows how the number of iteration

5

controls the learning properties both for gradient descent and accelerated gradient. In this sense
implicit regularization occurs in all these approaches. For any t the error is split in two contributions.
Inspecting the proof it is easy to see that  the ﬁrst term in the bound comes from the convergence
properties of the algorithm with inﬁnite data. Hence the optimization error translates into a bias
term. The decay for accelerated method is much faster than for gradient descent. The second term
arises from comparing the empirical iterates with their inﬁnite sample (population) limit. It is a
variance term depending on the sampling in the data and hence decreases with the sample size.
For all methods  this term increases with the number of iterations  indicating that the empirical
and population iterations are increasingly different. However  the behavior is markedly worse for
accelerated methods. The beneﬁt of acceleration seems to be balanced out by this more unstable
behavior. In fact  the beneﬁt of acceleration is apparent balancing the error terms to obtain a ﬁnal
bound. The obtained bound is the same for gradient descent and accelerated methods  and is indeed
optimal since it matches corresponding lower bounds [3  7]. However  the number of iterations
needed by accelerated methods is the square root of those needed by gradient descent  indicating a
substantial computational gain can be attained. Next we show how these results can be generalized to
a more general setting  considering both weaker and stronger assumptions  corresponding to harder
or easier learning problems.

4.2 More reﬁned result

Theorem 1 is a simpliﬁed version of the more general result that we discuss in this section. We
are interested in covering also the non-attainable case  that is when there is no w∗ ∈ X such that
E(w∗) = inf X E. In order to cover this case we have to introduce several more deﬁnitions and
notations. In Appendix 8.2 we give a more detailed description of the general setting. Consider
X f (x)2 dρX(x) and
the space L2
extend the expected risk to L2
ρX be the
hypothesis space of functions such that f (x) = (cid:104)w  x(cid:105) ρX almost surely. Recall that  the minimizer
X y dρ(y|x). The projection fH over
of the expected risk over L2
the closure of the hypothesis space H is deﬁned as

ρX deﬁning E(f ) =(cid:82)
ρX is the regression function fρ =(cid:82)

ρX of the square integrable functions with the norm (cid:107)f(cid:107)2

X×R(f (x) − y)2 dρ(x  y). Let H ⊆ L2

ρX = (cid:82)

Let L : L2

ρX → L2

ρX be the integral operator

fH = arg min

g∈H

(cid:107)g − fρ(cid:107)ρX

.

Lf (x) =

f (x(cid:48))(cid:104)x  x(cid:48)(cid:105) dρX(x(cid:48)) .

(cid:90)

X

The ﬁrst assumption we consider concern the moments of the output variable and is more general
than assuming the output variable y to be bounded as assumed before.
Assumption 2. There exist positive constant Q and M such that for all N (cid:51) l ≥ 2 

(cid:90)

|y|l dρ(y|x) ≤ 1
2

l!M l−2Q2

ρX almost surely.

R

This assumption is standard and satisﬁed in classiﬁcation or regression with well behaved noise.
Under this assumption the regression function fρ is bounded almost surely

(cid:90)

R

(cid:18)(cid:90)

R

(cid:19)1/2 ≤ Q .

|fρ(x)| ≤

|y| dρ(y|x) ≤

|y|2 dρ(y|x)

(12)

The next assumptions are related to the regularity of the target function fH.
Assumption 3.
There exist a positive constant B such that the target function fH satisfy
(fH(x) − fρ(x))2 x ⊗ x dρX(x) (cid:22) B2Σ .

(cid:90)

X

This assumption is needed to deal with the misspeciﬁcation of the model. The last assumptions
quantify the regularity of fH and the size (capacity) of the space H.

6

Assumption 4.
There exist g0 ∈ L2

ρX and r > 0 such that

Moreover we assume that there exist γ ≥ 1 and a positive constant cγ such that the effective dimension

fH = Lrg0  

with (cid:107)g0(cid:107)ρX ≤ R.

N(λ) = Tr

L (L + λI)

− 1

γ .

(cid:16)

−1(cid:17) ≤ cγλ
(cid:1) .

ρX

The assumption on N(λ) is always true for γ = 1 and c1 = κ2 and it’s satisﬁed when the eigenvalues
σi of L decay as i−γ. We recall that  the space H can be characterized in terms of the operator L 
indeed

H = L1/2(cid:0)L2

Hence  the non-attainable corresponds to considering r < 1/2.
Theorem 2. Under Assumption 2  3  4  let ˆwGD
be the t-th iterations of gradient descent
(5) and an accelerated version given by (6) or (7) respectively. Assuming the sample-size n to be
large enough  let δ ∈ (0  1/2) and assuming r to be smaller than the qualiﬁcation of the considered
algorithm (and equal to 1/2 in the case of Nesterov accelerated methods)  then there exist two positive
constant C1 and C2 such that with probability at least 1 − δ

and ˆwacc

t

t

(cid:32)
(cid:32)

(cid:33)
(cid:33)

E( ˆwGD

t

) − inf

H

E ≤ C1

E( ˆwacc

t

) − inf

H

E ≤ C2

1
t2r +

1
t4r +

1
γ

t
n

2
γ

t
n

log2 2
δ

log2 2
δ

.

where the constants C1 and C2 do not depend on n  t  δ  but depend on the chosen optimization.
Choosing the stopping rules tGD = O(n
accelerated methods achieve a learning rate of order O

4γr+2 ) both gradient descent and
.

2γr+1 ) and tacc = O(n

−2γr
2γr+1

n

γ

γ

(cid:16)

(cid:17)

The only reason why we do not consider r < 1/2 in the analysis of Nesterov accelerated methods is
that our proof require the qualiﬁcation of the method to be larger than 1 for technical reasons. However
we think that our result can be extended to that case  furthermore we think Nesterov qualiﬁcation
to be larger than 1  however it’s an open question whether higher qualiﬁcation can be attained. The
proof of the above result is given in the appendix. The general structure of the bound is the same
as in the basic setting  which is now recovered as a special case. However  in this more general
form  the various terms in the bound depend now on the regularity assumptions on the problem. In
particular  the variance depends on the effective dimension behavior  e.g. on the eigenvalue decay 
while the bias depend on the regularity assumption on fH. The general comparison between gradient
descent and accelerated methods follows the same line as in the previous section. Faster bias decay of
accelerated methods is contrasted by a more unstable behavior. As before  the beneﬁt of accelerated
methods becomes clear when deriving optimal stopping time and corresponding learning bound:
they achieve the accuracy of gradient methods but in considerable less time. While heavy-ball and
Nesterov have again similar behaviors  here a subtle difference resides in their different qualiﬁcations 
which in principle lead to different behavior for easy problems  that is for large r and γ. In this
regime  gradient descent could work better since it has inﬁnite qualiﬁcation. For problems in which
r < 1/2 and γ = 1 the rates are worse than in the basic setting  hence these problems are hard.

4.3 Related work

In the convex optimization framework a similar phenomenon was pointed out in [10] where they
introduce the notion of inexact ﬁrst-order oracle and study the behaviour of several ﬁrst-order methods
of smooth convex optimization with such oracle. In particular they show that the superiority of
accelerated methods over standard gradient descent is no longer absolute when an inexact oracle
is used. This because acceleration suffer from the accumulation of the errors committed by the
inexact oracle. A relevant result on the generalization properties of learning algorithm is [5] in
which they introduce the notion of uniform stability and use it to obtain generalization error bounds

7

for regularization based learning algorithms. Recently  to show the effectiveness of commonly
used optimization algorithms in many large-scale learning problems  algorithmic stability has been
established for stochastic gradient methods [14]  as well as for any algorithm in situations where
global minima are approximately achieved [8]. For Nesterov’s accelerated gradient descent and heavy-
ball method  [9] provide stability upper bounds for quadratic loss function in a ﬁnite dimensional
setting. All these approaches focus on the deﬁnition of uniform stability given in [5]. Our approach
to the stability of a learning algorithm is based on the study of ﬁltering properties of accelerated
methods together with concentration inequalities  we obtain upper bounds on the generalization error
for quadratic loss in a inﬁnite dimensional Hilbert space and generalize the bounds obtained in [9]
by considering different regularity assumptions and by relaxing the hypothesis of the existence of a
minimizer of the expected risk on the hypothesis space.

5 Numerical simulation

In this section we show some numerical simulations to validate our results. We want to simulate the
case in which the eigenvalues σi of the operator L are σi = i−γ for some γ ≤ 1 and the non-attainable
case r < 1/2. In order to do this we observe that if we consider the kernel setting over a ﬁnite space
Z = {z1  . . .   zn} of size N with the uniform probability distribution ρZ  then the space L2(Z  ρZ)
becomes RN with the usual scalar product multiplied by 1/N. the operator L becomes a N × N
matrix which entries are Li j = K(zi  zj) for every i  j ∈ {1  . . .   N}  where K is the kernel  which
is ﬁxed by the choice of the matrix L. We build the matrix L = U DU T with U ∈ RN×N orthogonal
matrix and D diagonal matrix with entries Di i = i−γ. The source condition becomes fH = Lrg0
for some g0 ∈ RN   r > 0. We simulate the observed output as y = fH + N(0  σ) where N(0  σ) is
the standardx normal distribution of variance σ2. The sampling operation can be seen as extracting n
indices i1  . . .   in and building the kernel matrix ˆKj k = K(zij   zik ) and the noisy labels ˆyj = yij
for every j  k ∈ {1  . . .   n}. The Representer Theorem ensure that we can built our estimator ˆf ∈ RN
j=1 K(z  zij )cj where the vector c depends on the chosen optimization algorithm and

as ˆf (z) =(cid:80)n

L2
Z

takes the form c = gt( ˆK)y. The excess risk of the estimator ˆf is given by (cid:107) ˆf − fH(cid:107)2
For every algorithm considered  we run 50 repetitions  in which we sample the data-space and
compute the error (cid:107) ˆft − fH(cid:107)2
  where ˆft represents the estimator related to the t-th iteration of one
of the considered algorithms  and in the end we compute the mean and the variance of those errors.
In Figure 1 we simulate the error of all the algorithms considered for both attainable and non-attainable
case. We observe that both heavy-ball and Nesterov acceleration provides faster convergence rates
with respect to gradient descent method  but the learning accuracy is not improved. We observe
that the accelerated methods considered show similar behavior and that for “easy problem” (large r)
that gradient descent can exploits its higher qualiﬁcation and perform similarly to the accelerated
methods.
In Figure 2 we show the test error related to the real dataset pumadyn8nh (available at
https://www.dcc.fc.up.pt/ ltorgo/Regression/puma.html). Even in this case we can observe the
behaviors shown in our theoretical results.

.

L2
Z

Fig. 1: Mean and variance of error (cid:107) ˆft − fH(cid:107)2
N for the t-th iteration of gradient descent (GD)  Nesterov
accelerated algorithm and heavy-ball (ν = 1). Black dots shows the absolute minimum of the curves. The
parameters are chosen N = 104  n = 102  γ = 1  σ = 0.5. We show the attainable case (r = 1/2) in the left 
the “hard case” (r = 0.1 < 1/2) in the center and the “easy case” (r=2>1/2) in the right.

8

Fig. 2: Test error on the real dataset pumadyn8nh using gradient descent (GD)  Nesterov accelerated algorithm
and heavy-ball. In the left we use a gaussian kernel with σ = 1.2 and in the right a polynomial kernel of degree
9.

6 Conclusion

In this paper  we have considered the implicit regularization properties of accelerated gradient methods
for least squares in Hilbert space. Using spectral calculus we have characterized the properties of
the different iterations in terms of suitable polynomials. Using the latter  we have derived error
bounds in terms of suitable bias and variance terms. The main conclusion is that under the considered
assumptions accelerated methods have smaller bias but also larger variance. As a byproduct they
achieve the same accuracy of vanilla gradient descent but with much fewer iterations. Our study
opens a number of potential theoretical and empirical research directions. From a theory point of
view  it would be interesting to consider other learning regimes  for examples classiﬁcation problems 
different loss functions or other regularity assumptions beyond classical nonparametric assumptions 
e.g. misspeciﬁed models and fast eigenvalues decays (Gaussian kernel). From an empirical point of
view it would be interesting to do a more thorough investigation on a larger number of simulated and
real data-sets of varying dimension.

Acknowledgments

This material is based upon work supported by the Center for Brains  Minds and Machines (CBMM) 
funded by NSF STC award CCF-1231216  and the Italian Institute of Technology. We gratefully
acknowledge the support of NVIDIA Corporation for the donation of the Titan Xp GPUs and the
Tesla k40 GPU used for this research. L. R. acknowledges the ﬁnancial support of the AFOSR
projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007 (European Ofﬁce of Aerospace
Research and Development)  and the EU H2020-MSCA-RISE project NoMADS - DLV-777826. N.P.
would like to thank Murata Tomoya for the useful observations.

References
[1] Hedy Attouch  Zaki Chbani  and Hassan Riahi. Rate of convergence of the nesterov accelerated
gradient method in the subcritical case α ≤ 3. ESAIM: Control  Optimisation and Calculus of
Variations  25:2  2019.

[2] Luca Baldassarre  Lorenzo Rosasco  Annalisa Barla  and Alessandro Verri. Multi-output

learning via spectral ﬁltering. Machine learning  87(3):259–301  2012.

[3] Gilles Blanchard and Nicole Mücke. Optimal rates for regularization of statistical inverse
learning problems. Foundations of Computational Mathematics  18(4):971–1013  Aug 2018.

[4] Léon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural

information processing systems  pages 161–168  2008.

[5] Olivier Bousquet and André Elisseeff. Stability and generalization. Journal of machine learning

research  2(Mar):499–526  2002.

9

[6] Peter Bühlmann and Bin Yu. Boosting with the l 2 loss: regression and classiﬁcation. Journal

of the American Statistical Association  98(462):324–339  2003.

[7] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares

algorithm. Foundations of Computational Mathematics  7(3):331–368  2007.

[8] Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms

that converge to global optima. arXiv preprint arXiv:1710.08402  2017.

[9] Yuansi Chen  Chi Jin  and Bin Yu. Stability and convergence trade-off of iterative optimization

algorithms. arXiv preprint arXiv:1804.01619  2018.

[10] Olivier Devolder  François Glineur  and Yurii Nesterov. First-order methods of smooth convex

optimization with inexact oracle. Mathematical Programming  146(1-2):37–75  2014.

[11] Heinz Werner Engl  Martin Hanke  and Andreas Neubauer. Regularization of inverse problems 

volume 375. Springer Science & Business Media  1996.

[12] Junichi Fujii  Masatoshi Fujii  Takayuki Furuta  and Ritsuo Nakamoto. Norm inequalities
equivalent to heinz inequality. Proceedings of the American Mathematical Society  118(3):827–
830  1993.

[13] Suriya Gunasekar  Jason D Lee  Daniel Soudry  and Nati Srebro. Implicit bias of gradient
In Advances in Neural Information Processing

descent on linear convolutional networks.
Systems  pages 9461–9471  2018.

[14] Moritz Hardt  Benjamin Recht  and Yoram Singer. Train faster  generalize better: Stability of

stochastic gradient descent. arXiv preprint arXiv:1509.01240  2015.

[15] Yann A LeCun  Léon Bottou  Genevieve B Orr  and Klaus-Robert Müller. Efﬁcient backprop.

In Neural networks: Tricks of the trade  pages 9–48. Springer  2012.

[16] Junhong Lin  Raffaello Camoriano  and Lorenzo Rosasco. Generalization properties and implicit
regularization for multiple passes sgm. In International Conference on Machine Learning  pages
2340–2348  2016.

[17] Junhong Lin and Volkan Cevher. Optimal convergence for distributed learning with stochastic

gradient methods and spectral-regularization algorithms. stat  1050:22  2018.

[18] Junhong Lin and Lorenzo Rosasco. Optimal learning for multi-pass stochastic gradient methods.

In Advances in Neural Information Processing Systems  pages 4556–4564  2016.

[19] Junhong Lin  Alessandro Rudi  Lorenzo Rosasco  and Volkan Cevher. Optimal rates for spectral
algorithms with least-squares regression over hilbert spaces. Applied and Computational
Harmonic Analysis  2018.

[20] Peter Mathé and Sergei Pereverzev. Regularization of some linear ill-posed problems with

discretized random noisy data. Mathematics of Computation  75(256):1913–1929  2006.

[21] Peter Mathé and Sergei V Pereverzev. Moduli of continuity for operator valued functions.

Numerical Functional Analysis and Optimization  23(5-6):623–631  2002.

[22] Yurii E Nesterov. A method for solving the convex programming problem with convergence

rate o (1/kˆ 2). In Dokl. akad. nauk Sssr  volume 269  pages 543–547  1983.

[23] Andreas Neubauer. On nesterov acceleration for landweber iteration of linear ill-posed problems.

Journal of Inverse and Ill-posed Problems  25(3):381–390  2017.

[24] Boris T Polyak. Introduction to optimization. Technical report  1987.

10

[25] Garvesh Raskutti  Martin J Wainwright  and Bin Yu. Early stopping and non-parametric regres-
sion: an optimal data-dependent stopping rule. The Journal of Machine Learning Research 
15(1):335–366  2014.

[26] Lorenzo Rosasco and Silvia Villa. Learning with incremental iterative regularization.

Advances in Neural Information Processing Systems  pages 1630–1638  2015.

In

[27] Daniel Soudry  Elad Hoffer  Mor Shpigel Nacson  Suriya Gunasekar  and Nathan Srebro. The
implicit bias of gradient descent on separable data. The Journal of Machine Learning Research 
19(1):2822–2878  2018.

[28] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science &

Business Media  2008.

[29] Weijie Su  Stephen Boyd  and Emmanuel Candes. A differential equation for modeling nes-
terov’s accelerated gradient method: Theory and insights. In Advances in Neural Information
Processing Systems  pages 2510–2518  2014.

[30] Gabor Szeg. Orthogonal polynomials  volume 23. American Mathematical Soc.  1939.

[31] Yuan Yao  Lorenzo Rosasco  and Andrea Caponnetto. On early stopping in gradient descent

learning. Constructive Approximation  26(2):289–315  2007.

[32] SK Zavriev and FV Kostyuk. Heavy-ball method in nonconvex optimization problems.

Computational Mathematics and Modeling  4(4):336–341  1993.

[33] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Under-
standing deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 
2016.

11

,Nicolò Pagliana
Lorenzo Rosasco