2017,Regret Analysis for Continuous Dueling Bandit,The dueling bandit is a learning framework where the feedback information in the learning process is restricted to noisy comparison between a pair of actions. In this paper  we address a dueling bandit problem based on a cost function over a continuous space.   We propose a stochastic mirror descent algorithm  and show that  the algorithm achieves an $O(\sqrt{T\log T})$-regret bound under strong convexity and smoothness assumptions for the cost function. Then  we clarify the equivalence between regret minimization in dueling bandit and convex optimization for the cost function.  Moreover  considering a lower bound in convex optimization  it is turned out that our algorithm achieves the optimal convergence rate in convex optimization and the optimal regret in dueling bandit except for a logarithmic factor.,Regret Analysis for Continuous Dueling Bandit

Wataru Kumagai

Center for Advanced Intelligence Project

RIKEN

1-4-1  Nihonbashi  Chuo  Tokyo 103-0027  Japan

wataru.kumagai@riken.jp

Abstract

p

The dueling bandit is a learning framework wherein the feedback information in
the learning process is restricted to a noisy comparison between a pair of actions.
In this research  we address a dueling bandit problem based on a cost function over
a continuous space. We propose a stochastic mirror descent algorithm and show
that the algorithm achieves an O(
T log T )-regret bound under strong convexity
and smoothness assumptions for the cost function. Subsequently  we clarify the
equivalence between regret minimization in dueling bandit and convex optimiza-
tion for the cost function. Moreover  when considering a lower bound in convex
optimization  our algorithm is shown to achieve the optimal convergence rate in
convex optimization and the optimal regret in dueling bandit except for a logarith-
mic factor.

1

Introduction

Information systems and computer algorithms often have many parameters which should be tuned.
When cost or utility are explicitly given as numerical values or concrete functions  the system pa-
rameters can be appropriately determined depending on the the values or the functions. However 
in a human-computer interaction system  it is difﬁcult or impossible for users of the system to pro-
vide user preference as numerical values or concrete functions. Dueling bandit is introduced to
model such situations in Yue and Joachims (2009) and enables us to appropriately tune the param-
eters based only on comparison results on two parameters by the users. In the learning process of
a dueling bandit algorithm  the algorithm chooses a pair of parameters called actions (or arms) and
receives only the corresponding comparison result. Since dueling bandit algorithms do not require
an individual evaluation value for each action  they can be applied for wider areas that cannot be
formulated using the conventional bandit approach.
When action cost (or user utility) implicitly exists  the comparison between two actions is modeled
via a cost (or utility) function  which represents the degree of the cost (or utility)  and a link function 
which determines the noise in the comparison results. We refer to such a modeling method as cost-
based (or utility-based) approach and employ it in this research. Yue and Joachims (2009) ﬁrst
introduced the utility-based approach as a model for a dueling bandit problem.
relates to function optimization with noisy comparisons
The cost-based dueling bandit
(Jamieson et al.  2012; Matsui et al.  2016) because in both frameworks an oracle compare two ac-
tions and the feedback from the oracle is represented by binary information. In particular  the same
algorithm can be applied to both frameworks. However  as different performance measures are ap-
plied to the algorithms in function optimization and dueling bandit  it has not been demonstrated that
an algorithm that works efﬁciently in one framework will also perform well in the other framework.
This study clariﬁes relation between function optimization and dueling bandit thorough their regret
analysis.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

1.1 Problem Setup

{

In the learning process of the dueling bandit problem  a learner presents two points  called actions in
a space A  to an oracle and the oracle returns one-bit feedback to the learner based on which action
wins (i.e.  which action is more preferable for the oracle). Here  we denote by a ≻ a
′ the event that
′ happens. In other words  we assume that the
a wins a
feedback from the oracle follows the following two-valued random variable:

) the probability that a ≻ a

′ and by P (a ≻ a
′

′

1 w:p: P (a ≻ a
′
0 w:p:

1 (cid:0) P (a ≻ a
′

)

);

) :=

F (a; a
where the probability P (a ≻ a
′
) is determined by the oracle. We refer to this type of feedback as
noisy comparison feedback. Unlike conventional bandit problems  the leaner has to make a decision
that is based only on the noisy comparison feedback and cannot access the individual values of
the cost (or utility) function. We further assume that each comparison between a pair of actions is
independent of other comparisons.
The learner makes a sequence of decisions based on the noisy comparisons provided by the oracle.
′
After receiving F (at; a
t+1). As a
performance measure for an action a  we introduce the minimum win probability:

′
t) at time t  the learner chooses the next two action (at+1; a

(1)

We next quantify the performance of the algorithm using the expected regret as follows:1)

RegDB

T = sup
a2A

E

f(P

(cid:3)

(a) (cid:0) P

(cid:3)

(at)) + (P

(cid:3)

(a) (cid:0) P

(cid:3)

:

(2)

]
t))g
′
(a

(cid:3)

P

(a) = inf

a′2A P (a ≻ a

′

):

[
T∑

t=1

1.2 Modeling Assumption
In this section  we clarify some of the notations and assumptions. Let an action space A (cid:26) Rd be
compact convex set with non-empty interior. We denote the Euclidean norm by ∥ (cid:1) ∥.
Assumption 1. There exist functions f : A ! R and (cid:27) : R ! [0; 1] such that the probability in
noisy comparison feedback can be represented as follows:

P (a ≻ a
′

′
) = (cid:27)(f (a

) (cid:0) f (a)):

(3)

In the following  we call f in Assumption 1 a cost function and and (cid:27) a link function. Here  the
cost function and the link function are ﬁxed for each query to the oracle. In this sense  our setting is
different from online optimization where the objective function changes.
Deﬁnition 1. (Strong Convexity) A function f : Rd ! R is (cid:11)-strongly convex over the set A (cid:26) Rd
if for all x; y 2 A it holds that

f (y) (cid:21) f (x) + ∇f (x)

⊤

(y (cid:0) x) +

∥y (cid:0) x∥2:

Deﬁnition 2. (Smoothness) A function f : Rd ! R is (cid:12)-smooth over the set A (cid:26) Rd if for all
x; y 2 A it holds that

f (y) (cid:20) f (x) + ∇f (x)
⊤

(y (cid:0) x) +

∥y (cid:0) x∥2:

Assumption 2. The cost function f : A ! R is twice continuously differentiable  L-Lipschitz 
(cid:11)-strongly convex and (cid:12)-smooth with respect to the Euclidean norm.

(cid:3) of the cost function f since f is strictly
From Assumption 2  there exists a unique minimizer a
′
convex. We set B := supa;a′2A f (a
Assumption 3. The link function (cid:27) : R ! [0; 1] is three times differentiable and rotation-symmetric
(i.e.  (cid:27)((cid:0)x) = 1(cid:0) (cid:27)(x)). Its ﬁrst derivative is positive and monotonically non-increasing on [0; B].
1) Although the regret in (2) appears superﬁcially different from that in Yue and Joachims (2009)  two regrets

) (cid:0) f (a).

can be shown to coincide with each other under Assumptions 1-3 in Subsection 1.2.

2

(cid:11)
2

(cid:12)
2

For examples  the standard logistic distribution function  the cumulative standard Gaussian distri-
bution function and the linear function (cid:27)(x) = (1 + x)=2 can be taken to be link functions that
satisfy Assumption 3. We note that link functions often behave like cumulative probability distribu-
tion functions. This is because the sign of the difference between two noisy function values can be
regarded as the feedback (1) which satisﬁes Assumption 1  and then  the link function (cid:27) coincides
with the cumulative probability distribution function of the noise (see Section 2 of Jamieson et al.
(2012) for more details). We will discuss the relation of noisy comparison feedback to noisy function
values in Section 5.
1.3 Related Work and Our Contributions

p

√

Dueling bandit on the continuous action space relates with various optimization methods. We sum-
marize related studies in the following.
Dueling bandit problem: Yue and Joachims (2009) formulated information retrieval systems as
a dueling bandit problem. They reduced this to a problem of optimizing an “almost"-concave
function and presented a stochastic gradient ascent algorithm based on one-point bandit feedback.
Subsequently  they showed that their algorithm achieves an O(T 3=4)-regret bound under the dif-
ferentiability and the strict concavity for a utility function. Ailon et al. (2014) presented reduction
methods from dueling bandit to the conventional bandit under the strong restriction that the link
T log3 T )-regret bound. We
function is linear and showed that their algorithm achieves an O(
note that dueling bandit has a number of other formulations (Yue and Joachims  2011; Yue et al. 
2012; Busa-Fekete et al.  2013  2014; Urvoy et al.  2013; Zoghi et al.  2014; Jamieson et al.  2015).
Optimization with one-point bandit feedback: In conventional bandit settings  various convex op-
timization methods have been studied. Flaxman et al. (2005) showed that the gradient of smoothed
version of a convex function can be estimated from a one-point bandit feedback and proposed a
stochastic gradient descent algorithm which achieves an O(T 3=4) regret bound under the Lipschitz-
ness condition. Moreover  assuming the strong convexity and the smoothness for the convex function
p
such as (2)  Hazan and Levy (2014) proposed a stochastic mirror descent algorithm which achieves
T log T ) regret bound and showed that the algorithm is near optimal because the upper
an O(
bound matched the lower bound of Ω(
T ) derived by Shamir (2013) up to a logarithmic factor in
bandit convex optimization.
Optimization with two-point bandit feedback: Dueling bandit algorithms require two actions at
each round in common with two-point bandit optimization. In the context of online optimization 
Agarwal et al. (2010) ﬁrst considered convex optimization with two-point feedback. They proposed
a gradient descent-based algorithm and showed that the algorithm achieves the regret bounds of un-
der the Lipschitzness condition and O(log T ) under the strong convexity condition. In stochastic
p
convex optimization  Duchi et al. (2015) showed that a stochastic mirror descent algorithm achieves
T ) regret bound under the Lipschitzness (or the smoothness) condition and proved the up-
an O(
per bound to be optimal deriving a matching lower bound Ω(
T ). Moreover  in both of online
and stochastic convex optimization  Shamir (2017) showed that a gradient descent-based algorithm
achieves an O(
T ) regret bound with optimal dependence on the dimension under the Lipschitz-
ness condition. However  those two-point bandit algorithms strongly depend on the availability of
the difference of function values and cannot be directly applied to the case of dueling bandit where
the difference of function values is compressed to one bit in noisy comparison feedback.
Optimization with noisy comparison feedback: The cost-based dueling bandit relates to function
optimization with noisy comparisons (Jamieson et al.  2012; Matsui et al.  2016) because in both
frameworks  the feedback is represented by preference information. Jamieson et al. (2012) proposed
a coordinate descent algorithm and proved that the convergence rate of the algorithm achieved an
optimal order.2) Matsui et al. (2016) proposed a Newton method-based algorithm and proved that its
convergence rate was almost equivalent to that of Jamieson et al. (2012). They further showed that
their algorithm could easily be parallelized and performed better numerically than the dueling bandit
algorithm in Yue and Joachims (2009). However  since they considered only the unconstrained case
in which A = Rd  it is not possible to apply their algorithm to the setting considered here  in which
the action space is compact.

p

p

2)The optimal order changes depending on the model parameter (cid:20) (cid:21) 1 of the pairwise comparison oracle in

Jamieson et al. (2012).

3

Optimization with one-bit feedback: The optimization method of the dueling bandit algorithm is
based on one-bit feedback. In related work  Zhang et al. (2016) considered stochastic optimization
under one-bit feedback. However  since their approach was restricted to the problem of linear opti-
mization with feedback generated by the logit model  it cannot be applied to the problem addressed
in the current study.
Our contributions: In this paper  we consider the cost-based dueling bandit under Assumptions
1-3. While the formulation is similar to that of Yue and Joachims (2009)  Assumptions 2 and 3 are
stronger than those used in that study. On the other hand  we impose the weaker assumption on
the link function than that of Ailon et al. (2014). Yue and Joachims (2009) showed that a stochastic
gradient descent algorithm can be applied to dueling bandit. Thus  it is naturally expected that a
stochastic mirror descent algorithm  which achieves the (near) optimal order in convex optimiza-
tion with one/two-point bandit feedback  can be applied to dueling bandit setting and achieves good
performance. Following this intuition  we propose a mirror descent-based algorithm. Our key con-
tributions can be summarized as follows:

p

T log T )-regret bound for our algorithm in dueling bandit.

(cid:15) We propose a stochastic mirror descent algorithm with noisy comparison feedback.
(cid:15) We provide an O(
(cid:15) We clarify the relation between the cost-based dueling bandit and convex optimization in terms
(cid:15) We show that the convergence rate of our algorithm is O(
log T =T ) in convex optimization.
(cid:15) We derive a lower bound in convex optimization with noisy comparison feedback and show

of their regrets and show that our algorithm can be applied to convex optimization.

√

our algorithm to be near optimal in both dueling bandit and convex optimization.

2 Algorithm and Main Result

2.1 Stochastic Mirror Descent Algorithm

We ﬁrst prepare the notion of a self-concordant function on which our algorithm is constructed (see
e.g.  Nesterov et al. (1994)  Appendix F in Griva et al. (2009)).
Deﬁnition 3. A function R : int(A) ! R is considered self-concordant if the following two condi-
tions hold:

1. R is three times continuously differentiable and convex  and approaches inﬁnity along any

sequence of points approaching the boundary of int(A).

(cid:12)(cid:12)

2. For every h 2 Rd and x 2 int(A)  j∇3R(x)[h; h; h]j (cid:20) 2(h
t1=t2=t3=0:

∇3R(x)[h; h; h] := @3R

(x + t1h + t2h + t3h)

@t1@t2@t3

⊤∇2R(x)h) 3

2 holds  where

In addition to these two conditions  if R satisﬁes the following condition for a positive real number
(cid:23)  it is called a (cid:23)-self-concordant function:

3. For every h 2 Rd and x 2 int(A)  j∇R(x)
⊤

hj (cid:20) (cid:23) 1

⊤∇2R(x)h) 1
2 .

2 (h

In this paper  we assume the Hessian ∇2R(a) of a (cid:23)-self-concordant function to be full-rank over
A and ∇R(int(A)) = Rd. Bubeck and Eldan (2014) showed that such a (cid:23)-self-concordant function
satisfying (cid:23) = (1 + o(1))d will always exist as long as the dimension d is sufﬁciently large. We next
propose Algorithm 1  which we call NC-SMD. This can be regarded as stochastic mirror descent
with noisy comparison feedback.
√
We make three remarks on Algorithm 1. First  the function Rt is self-concordant though not (cid:23)-
self-concordant. The second remark is as follows. Let us denote the local norms by ∥a∥w =
a⊤∇2R(w)a. Then  if R is a self-concordant function for A  the Dikin Ellipsoid fa
′ (cid:0)
′ 2 Aj ∥a
a∥a (cid:20) 1g centered at a is entirely contained in int(A) for any a 2 int(A). Thus  a
′
t := at +
2 ut in Algorithm 1 is contained in int(A) for any at 2 int(A) and a unit vector ut.
∇2Rt(at)
(cid:0) 1
′
This shows a comparison between actions at and a
t to be feasible. Our third remark is as follows.
Since the self-concordant function Rt at round t depends on the past actions faigt
i=1  it may be
thought that those past actions are stored during the learning process. However  note that only ∇Rt

4

Algorithm 1 Noisy Comparison-based Stochastic Mirror Descent (NC-SMD)
Input: Learning rate (cid:17)  (cid:23)-self-concordant function R  time horizon T   tuning parameters (cid:21); (cid:22)
∑
Initialize: a1 = argmina2AR(a).
for t = 1 to T do
Update Rt(a) = R(a) + (cid:21)(cid:17)
Pick a unit vector ut uniformly at random
t := at + ∇2Rt(at)
(cid:0) 1
′
Compare at and a
t; at)d∇2Rt(at) 1
′
Set ^gt = F (a
2 ut
t (∇Rt(at) (cid:0) (cid:17)^gt)
Set at+1 = ∇R(cid:0)1
end for
Output: aT +1

′
2 ut and receive F (a
t; at)

∥a (cid:0) ai∥2 + (cid:22)∥a∥2

t
i=1

2

and ∇2Rt are used in the algorithm; ∇Rt depends only on
the past actions. Thus  only the sum of past actions must be stored  rather than all past actions.

∑
i=1 at and ∇2Rt does not depend on

t

′

p

′ of the link function is bounded as l0 (cid:20) (cid:27)

2.2 Main Result: Regret Bound
From Assumption 2 and the compactness of A  the diameter R and B := supa;a′2A f (a
) (cid:0) f (a)
are ﬁnite. From Assumption 3  there are exist positive constants l0  L0  B2 and L2 such that the ﬁrst
′ (cid:20) L0 on [(cid:0)B; B] and the second derivative
derivative (cid:27)
′′ is bounded above by B2 and L2-Lipschitz on [(cid:0)B; B]. We use the constants below.
(cid:27)
The following theorem shows that with appropriate parameters  NC-SMD (Algorithms 1) achieves
an O(
. When the tuning parameters satisfy (cid:21) (cid:20) l0(cid:11)=2 
Theorem 4. We set C := (cid:23) + B2L2+(L+1)L0(cid:12)
√

)2 and the total number T of rounds satisﬁes T (cid:21) C log T . Algorithm 1 with a (cid:23)-

self-concordant function and the learning parameter (cid:17) = 1
2d
bound under Assumptions 1-3:

achieves the following regret

(cid:22) (cid:21) (

T log T )-regret bound.

√

0L2=(cid:21)

C log T

L3

2(cid:21)

T

RegDB

T

(cid:20) 4d

CT log T + 2LL0R:

(4)

3 Regret Analysis

We prove Theorem 4 in this section. The proofs of lemmas in this section are provided in supple-
mentary material.

3.1 Reduction to Locally-Convex Optimization

We ﬁrst reduce the dueling bandit problem to a locally-convex optimization problem. We deﬁne
Pb(a) := (cid:27)(f (a) (cid:0) f (b)) for a; b 2 A and Pt(a) := Pat(a). For a cost function f and a self-
concordant function R  we set a
:= argmina2Af (a)  a1 := argmina2AR(a) and a
(cid:3)
(cid:3)
T := 1
T a1 +
(1 (cid:0) 1
[
T∑
Lemma 5. The regret of Algorithm 1 is bounded as follows:

(cid:3). The regret of dueling bandit is bounded as follows.

]

T )a

RegDB

T

(cid:20) 2E

(Pt(at) (cid:0) Pt(a
(cid:3)
T ))

+

LL0(cid:12)

(cid:21)(cid:17)

t=1

log T + 2LL0R:

(5)

The following lemma shows that Pb inherits the smoothness of f globally.
Lemma 6. The function Pb is (L0(cid:12) + B2L2)-smooth for an arbitrary b 2 A.
Let B be the unit Euclidean ball  B(a; (cid:14)) the ball centered at a with radius (cid:14) and L(a; b) the line
; (cid:14)) \ A. The
segment between a and b. In addition  for a; b 2 A  let A(cid:14)(a; b) := [a′2L(a;b)B(a
′
following lemma guarantees the local strong convexity of Pb.
2 l0(cid:11)-strongly convex on A(cid:14)(a
(cid:3)
Lemma 7. The function Pb is 1

; b) when (cid:14) (cid:20) l0(cid:11)

.

2L3

0L2

5

3.2 Gradient Estimation
2 x for x 2 B is included in A due to the properties of the Dikin
We note that at + ∇2Rt(at)
(cid:0) 1
ellipsoid. We introduce the smoothed version of Pt over int(A):
:= Ex2B[Pt(a + ∇2Rt(at)
(cid:0) 1
2 x)]
2 x) (cid:0) f (at))]:
= Ex2B[(cid:27)(f (a + ∇2Rt(at)
(cid:0) 1
Next  we adopt the following estimator for the gradient of ^Pt:

(6)
(7)

^Pt(a)

^gt := F (at + ∇2Rt(at)

2 ut; at)d∇2Rt(at)
(cid:0) 1

1

2 ut;

where ut is drawn from the unit surface S uniformly. We then derive the unbiasedness of ^gt as
follows.
Lemma 8.

E[^gtjat] = ∇ ^Pt(at):

3.3 Regret Bound with Bregman Divergence

From Lemma 5  the regret analysis in dueling bandit is reduced to the minimization problem of the
regret-like value of Pt. Since Pt is globally smooth and locally strongly convex from Lemmas 6
and 7  we can employ convex-optimization methods. Moreover  since ^gt is an unbiased estimator
for the gradient of the smoothed version of Pt from Lemma 8  it is expected that stochastic mirror
descent (Algorithm 1) with ^gt is effective to the minimization problem. In the following  making use
of the property of stochastic mirror descent  we bound the regret-like value of Pt by the Bregman
divergence  and subsequently prove Theorem 4.
Deﬁnition 9. Let R be a continuously differentiable strictly convex function on int(A). Then  the
Bregman divergence associated with R is deﬁned by

)2  the regret of Algorithm 1 is bounded for any

(a (cid:0) b):

E

L3

0L2=(cid:21)

]
[

DR(a; b) = R(a) (cid:0) R(b) (cid:0) ∇R(b)
⊤

Lemma 10. When (cid:21) (cid:20) l0(cid:11)=2 and (cid:22) (cid:21)(

(Pt(at) (cid:0) Pt(a))

a 2 int(A) as follows:

[
T∑
(
(∇R(at) (cid:0) (cid:17)^gt;∇R(at))
R(a) (cid:0) R(a1) + E
t (a) := supx2Rd⟨x; a⟩ (cid:0) Rt(a) is the Fenchel dual of Rt.

(cid:20) 1
(cid:17)
where R(cid:3)
The Bregman divergence in Lemma 10 is bounded as follows.
Lemma 11. When (cid:17) (cid:20) 1

T∑

DR(cid:3)

t=1

t=1

t

])

+

L0(cid:12) + B2L2

(cid:21)(cid:17)

log T;

(8)

2d   the sequence at output by Algorithm 1 satisﬁes
DR(cid:3)

(∇Rt(at) (cid:0) (cid:17)^gt;∇Rt(at)) (cid:20) 4d2(cid:17)2:

t

(9)
[Proof of Theorem 4] From Lemma 4 of Hazan and Levy (2014)  the (cid:23)-self-concordant function R
satisﬁes

R(a
T ) (cid:0) R(a1) (cid:20) (cid:23) log
(cid:3)
′(cid:0) a)g is the Minkowsky function. Since (cid:25)a1 (a
) := inffr (cid:21) 0ja + r
T ) (cid:20) 1(cid:0) T
(cid:0)1(a
(cid:3)
where (cid:25)a(a
(cid:3)
T   we obtainR(a
from the deﬁnition of a
T ) (cid:0) R(a1) (cid:20) (cid:23) log T:
(cid:3)
Note that the condition (cid:17) (cid:20) 1
(
5  10 and 11  we have
(

2d in Lemma 11 is satisﬁed due to T (cid:21) C log T . Combining Lemmas

1 (cid:0) (cid:25)a1 (a
(cid:3)
T )

(cid:23) log T + 4d2(cid:17)2T

L0(cid:12) + D(cid:27)′′L2

(cid:20) 2
(cid:17)

LL0(cid:12)
l0(cid:11)(cid:17)

+ 2LL0R

RegDB

log T +

)

)

(cid:0)1

+

1

T

′

;

(cid:20)

2(cid:23) +

B2L2 + (L + 1)L0(cid:12)

(cid:21)

(cid:17)

(cid:21)(cid:17)
log T

+ 8d2T (cid:17) + 2LL0R:

Thus  when (cid:17) is deﬁned in Theorem 4  the regret bound (4) is obtained.

6

4 Convergence Rate in Convex Optimization

In the previous sections  we considered the minimization problem for the regret of dueling bandit.
In this section  as an application of the approach  we show that the averaged action of NC-SMD
(Algorithm 1) minimize the cost function f in (3).
To derive the convergence rate of our algorithm  we introduce the regret of function optimization and
establish a connection between the regrets of dueling bandit and function optimization. In convex
′
optimization with noisy comparison feedback  the learner chooses a pair (at; a
t) of actions in the
′
learning process and suffers a loss f (at) + f (a
t). Then  the regret of the algorithms in function
optimization is deﬁned as follows:

]

[
T∑

RegF O

T

:= E

(f (at) (cid:0) f (a

(cid:3)

t) (cid:0) f (a
(cid:3)
′
)) + (f (a

))

;

(10)

t=1

= argmina2Af.

(cid:3)
where a
Recalling that the positive constants l0 and L0 satisfy l0 (cid:20) (cid:27)
′
supa;a′2A f (a
as follows:
Lemma 12.

′ (cid:20) L0 on [(cid:0)B; B]  where B :=
) (cid:0) f (a)  the regrets of function optimization (10) and dueling bandit (2) are related

RegDB

T
L0

(cid:20) RegF O

T

(cid:20) RegDB

T
l0

:

(11)

p
Theorem 4 and Lemma 12 give an O(
T log T )-upper bound of the regret of function optimization
in Algorithm 1 under the same conditions as Theorem 4. Given the convexity of f  the average of
the chosen actions of any dueling bandit algorithm (cid:22)aT := 1
2T
)] (cid:20) RegF O

′
T
t) satisﬁes
t=1(at + a

E[f ((cid:22)aT ) (cid:0) f (a

∑

(12)

(cid:3)

T
2T

:

Thus  if an optimization algorithm has a sub-linear regret bound  the above online-to-batch conver-
sion guarantees convergence to the optimal point.
Theorem 13. Under Assumptions 1-3  the averaged action (cid:22)aT satisﬁes the following when T (cid:21)
C log T :

(

√

E[f ((cid:22)aT ) (cid:0) f (a
(cid:3)

)] (cid:20) 1
l0

2d

(cid:23) log T + C

T

+

LL0R

T

;

)

√

where C is the constant deﬁned in Theorem 4.

Theorem 13 shows the convergence rate of NC-SMD (Algorithm 1) to be O(d

log T =T ).

5 Lower Bound

We next derive a lower bound in convex optimization with noisy comparison feedback. To do so  we
employ a lower bound of convex optimization with noisy function feedback. In a setting where the
function feedback is noisy  we query a point a 2 A and obtain a noisy function value f (a)+(cid:24)  where
(cid:24) is a zero-mean random variable with a ﬁnite second moment and independent for each query. 3)
Theorem 14. Assume that the action space A is the d-dimensional unit Euclidean ball Bd and
that the link function (cid:27)G is the cumulative distribution function of the zero-mean Gaussian random
variable with variance 2. Let the number of rounds T be ﬁxed. Then  for any algorithm with noisy
comparison feedback  there exists a function f over Bd which is twice continuously differentiable 
0:5-strongly convex and 3:5-smooth such that the output aT of the algorithm satisﬁes

E[f (aT ) (cid:0) f (a
(cid:3)

)] (cid:21) 0:004 min

:

(13)

}

{

1;

dp
2T

3)In general  the noise (cid:24) can depend on the action a. See e.g. Shamir (2013) for more details.

7

′

) + (cid:24)

′

) + (cid:24)

) for arbitrary a; a

′
) + ((cid:24) (cid:0) (cid:24)

′

sign(f (a) + (cid:24) (cid:0) (f (a
′

)) = sign(f (a) (cid:0) f (a
′

[Proof] The probability distribution of noisy comparison feedback F (a; a
) with the link function
(cid:27)G can be realized by noisy function feedback with thestandard Gaussian noise as follows. Two
′ can be obtained using the noisy function feedback
′
noisy function values f (a) + (cid:24) and f (a
′ are independent standard Gaussian random variables. Then  the probability
twice  where (cid:24) and (cid:24)
′ 2 A:
distribution of the following random variable coincide with that of F (a; a
(14)
Here  note that (cid:24) (cid:0) (cid:24)
′ is the zero-mean Gaussian random variable with variance 2. Thus  single
noisy comparison feedback with the link function (cid:27)G for any actions can be obtained by using
noisy function feedback with standard Gaussian noise twice. This means that if any algorithm with
2T -times noisy function feedback is unable to achieve a certain performance  any algorithm with
T -times noisy comarison feedback is similarly unable to achieve that performance. Thus  to derive
Theorem 14  it is sufﬁcient to show a lower bound of convergence rate with noisy function feedback.
The following lower bound is derived by Theorem 7 of Shamir (2013).
Theorem 15. (Shamir  2013) Let the number of rounds T be ﬁxed. Suppose that the noise (cid:24) at
each round is a standard Gaussian random variable. Then  for any algorithm with noisy function
feedback  there exists a function f over Bd which is twice continuously differentiable  0:5-strongly
convex and 3:5-smooth such that the output aT satisﬁes
)] (cid:21) 0:004 min

E[f (aT ) (cid:0) f (a
(cid:3)

}

:

)):

{

1;

dp
T

By the above discussion and from Theorem 15  we obtain Theorem 14.
Combining Theorem 13 and Theorem 14  the convergence rate of NC-SMD (Algorithm 1) is near
optimal with respect to the number of rounds T . In addition  when the parameter (cid:23) of the self-
concordant function is of constant order with respect to the dimension d of the space A  the conver-
gence rate of NC-SMD is optimal with respect to d. However  it should be noted that the parameter
(cid:23) of a self-concordant function is often of the order of (cid:2)(d) for compact convex sets including the
simplex and the hypercube.
As a consequece of Lemma 12  (12)  and Theorems 4 and 14  the optimal regrets of dueling ban-
dit and function optimization are of the order
T except for the logarithmic factor and NC-SMD
achieves the order. To the best of our knowledge  this is the ﬁrst algorithm with the optimal order in
the continuous dueling bandit setting with the non-linear link function.
p
Finally  we provide an interesting observation on convex optimization. When noisy function feed-
T ) under strong
back is available  the optimal regret of function optimization is of the order (cid:2)(
convexity and smoothness conditions (Shamir  2013). However  even when noisy function feedback
p
is "compressed" into one-bit information as in (14)  our results show that NC-MSD (Algorithm 1)
achieves almost the same order O(
T log T ) about the regret of function optimization as long as
the cumulative probability distribution of the noise satisﬁes Assumption 3.4)

p

6 Conclusion

√

We considered a dueling bandit problem over a continuous action space and proposed a stochas-
p
tic mirror descent. By introducing Assumptions 1-3  we proved that our algorithm achieves an
T log T )-regret bound. We further considered convex optimization under noisy comparison
O(
feedback and showed that the regrets of dueling bandit and function optimization are essentially
equivalent. Using the connection between the two regrets  it was shown that our algorithm achieves
a convergence rate of O(
log T =T ) in the framework of function optimization with noisy compar-
ison feedback. Moreover  we derived a lower bound of the convergence rate in convex optimization
and showed that our algorithm achieves near optimal performance in dueling bandit and convex op-
timization. Some open questions still remain. While we have only dealt with bounds which hold
in expectation  the derivation of the high-probability bound is a problem that has not been solved.
While the analysis of our algorithm relies on strong convexity and smoothness  a regret bound with-
out these conditions is also important.

4)Jamieson et al. (2012) provided a similar observation. However  their upper bound of the regret was derived
only when the action space is the whole of Euclidean space (i.e.  A = Rd) and the assumption for noisy
comparison feedback is different from ours (Assumption 1).

8

Acknowledgment

We would like to thank Professor Takafumi Kanamori for helpful comments. This work was sup-
ported by JSPS KAKENHI Grant Number 17K12653.

References
[1] A. Agarwal  O. Dekel  and L. Xiao (2010) “Optimal Algorithms for Online Convex Optimization with

Multi-Point Bandit Feedback. ” in COLT  pp. 28–40  Citeseer.

[2] N. Ailon  T. Joachims  and Z. Karnin (2014) “Reducing dueling bandits to cardinal bandits ” arXiv

preprint arXiv:1405.3396.

[3] S. Bubeck and R. Eldan (2014) “The entropic barrier: a simple and optimal universal self-concordant

barrier ” arXiv preprint arXiv:1412.1587.

[4] R. Busa-Fekete  E. Hüllermeier  and B. Szörényi (2014) “Preference-based rank elicitation using statis-
tical models: The case of Mallows ” in Proceedings of the 31st International Conference on Machine
Learning (ICML-14)  pp. 1071–1079.

[5] R. Busa-Fekete  B. Szorenyi  W. Cheng  P. Weng  and E. Hüllermeier (2013) “Top-k selection based on
adaptive sampling of noisy preferences ” in International Conference on Machine Learning  pp. 1094–
1102.

[6]

J. C. Duchi  M. I. Jordan  M. J. Wainwright  and A. Wibisono (2015) “Optimal rates for zero-order
convex optimization: The power of two function evaluations ” IEEE Transactions on Information Theory 
Vol. 61  pp. 2788–2806.

[7] A. D. Flaxman  A. T. Kalai  and H. B. McMahan (2005) “Online convex optimization in the bandit set-
ting: gradient descent without a gradient ” in Proceedings of the sixteenth annual ACM-SIAM symposium
on Discrete algorithms  pp. 385–394  Society for Industrial and Applied Mathematics.

[8]

I. Griva  S. G. Nash  and A. Sofer (2009) Linear and nonlinear optimization: Siam
Appendix
http://math.gmu.edu/~igriva/book/topics.html.

available

contains

F

which

(F.2)

is

at

the

following

URL:

[9] E. Hazan and K. Levy (2014) “Bandit convex optimization: Towards tight bounds ” in Advances in

Neural Information Processing Systems  pp. 784–792.

[10] K. G. Jamieson  S. Katariya  A. Deshpande  and R. D. Nowak (2015) “Sparse Dueling Bandits. ” in

AISTATS.

[11] K. G. Jamieson  R. Nowak  and B. Recht (2012) “Query complexity of derivative-free optimization ” in

Advances in Neural Information Processing Systems  pp. 2672–2680.

[12] K. Matsui  W. Kumagai  and T. Kanamori (2016) “Parallel distributed block coordinate descent methods

based on pairwise comparison oracle ” Journal of Global Optimization  pp. 1–21.

[13] Y. Nesterov  A. Nemirovskii  and Y. Ye (1994) Interior-point polynomial algorithms in convex program-

ming  Vol. 13: SIAM.

[14] O. Shamir (2013) “On the Complexity of Bandit and Derivative-Free Stochastic Convex Optimization. ”

in COLT  pp. 3–24.

[15]

(2017) “An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with Two-

Point Feedback ” The Journal of Machine Learning Research  Vol. 18  p. 1–11.

[16] T. Urvoy  F. Clerot  R. Féraud  and S. Naamane (2013) “Generic Exploration and K-armed Voting Ban-

dits. ” in ICML (2)  pp. 91–99.

[17] Y. Yue  J. Broder  R. Kleinberg  and T. Joachims (2012) “The k-armed dueling bandits problem ” Journal

of Computer and System Sciences  Vol. 78  pp. 1538–1556.

[18] Y. Yue and T. Joachims (2009) “Interactively optimizing information retrieval systems as a dueling ban-
dits problem ” in Proceedings of the 26th Annual International Conference on Machine Learning  pp.
1201–1208  ACM.

9

[19]

(2011) “Beat the mean bandit ” in Proceedings of the 28th International Conference on Ma-

chine Learning (ICML-11)  pp. 241–248.

[20] L. Zhang  T. Yang  R. Jin  Y. Xiao  and Z.-H. Zhou (2016) “Online stochastic linear optimization under

one-bit feedback ” in International Conference on Machine Learning  pp. 392–401.

[21] M. Zoghi  S. Whiteson  R. Munos  M. d. Rijke et al. (2014) “Relative upper conﬁdence bound for the
k-armed dueling bandit problem ” in JMLR Workshop and Conference Proceedings  No. 32  pp. 10–18 
JMLR.

10

,David Woodruff
Wataru Kumagai