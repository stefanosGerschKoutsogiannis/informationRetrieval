2011,Metric Learning with Multiple Kernels,Metric learning has become a very active research field. The most popular representative--Mahalanobis metric learning--can be seen as learning a linear transformation and then computing the Euclidean metric in the transformed space. Since a linear transformation might not always be appropriate for a given learning problem  kernelized versions of various metric learning algorithms exist. However  the problem then becomes finding the appropriate kernel function. Multiple kernel learning addresses this limitation by learning a linear combination of a number of predefined kernels; this approach can be also readily used in the context of multiple-source learning to fuse different data sources. Surprisingly  and despite the extensive work on multiple kernel learning for SVMs  there has been no work in   the area of metric learning with multiple kernel learning. In this paper we fill this gap and present a general approach for metric learning with multiple kernel learning. Our approach can be instantiated with different metric learning algorithms provided that they satisfy some constraints. Experimental evidence suggests that our approach outperforms metric learning with an unweighted kernel combination and metric learning with cross-validation based kernel selection.,Metric Learning with Multiple Kernels

Jun Wang

Huyen Do

Adam Woznica

Alexandros Kalousis

{Jun.Wang  Huyen.Do  Adam.Woznica  Alexandros.Kalousis}@unige.ch

AI Lab  Department of Informatics
University of Geneva  Switzerland

Abstract

Metric learning has become a very active research ﬁeld. The most popular
representative–Mahalanobis metric learning–can be seen as learning a linear trans-
formation and then computing the Euclidean metric in the transformed space.
Since a linear transformation might not always be appropriate for a given learning
problem  kernelized versions of various metric learning algorithms exist. How-
ever  the problem then becomes ﬁnding the appropriate kernel function. Multiple
kernel learning addresses this limitation by learning a linear combination of a
number of predeﬁned kernels; this approach can be also readily used in the con-
text of multiple-source learning to fuse different data sources. Surprisingly  and
despite the extensive work on multiple kernel learning for SVMs  there has been
no work in the area of metric learning with multiple kernel learning. In this paper
we ﬁll this gap and present a general approach for metric learning with multiple
kernel learning. Our approach can be instantiated with different metric learning
algorithms provided that they satisfy some constraints. Experimental evidence
suggests that our approach outperforms metric learning with an unweighted ker-
nel combination and metric learning with cross-validation based kernel selection.

1

Introduction

Metric learning (ML)  which aims at learning dissimilarities by determining the importance of dif-
ferent input features and their correlations  has become a very active research ﬁeld over the last
years [23  5  3  14  22  7  12]. The most prominent form of ML is learning the Mahalanobis metric.
Its computation can be seen as a two-step process; in the ﬁrst step we perform a linear projection of
the instances and in the second step we compute their Euclidean metric in the projected space.
Very often a linear projection cannot adequately represent the inherent complexities of a problem
at hand. To address this limitation various works proposed kernelized versions of ML methods in
order to implicitly compute a linear transformation and Euclidean metric in some non-linear feature
space; this computation results in a non-linear projection and distance computation in the original
input space [23  5  3  14  22]. However  we are now faced with a new problem  namely that of
ﬁnding the appropriate kernel function and the associated feature space matching the requirements
of the learning problem.
The simplest approach to address this problem is to select the best kernel from a predeﬁned kernel
set using internal cross-validation. The main drawback of this approach is that only one kernel
is selected which limits the expressiveness of the resulting method. Additionally  this approach
is limited to a small number of kernels–due to computational constraints–and requires the use of
extra data. Multiple Kernel Learning (MKL) [10  17] lifts the above limitations by learning a linear
combination of a number of predeﬁned kernels. The MKL approach can also naturally handle the
multiple-source learning scenarios where instead of combining kernels deﬁned on a single input
data  which depending on the selected kernels could give rise to feature spaces with redundant

1

features  we combine different and complementary data sources. In [11  13] the authors propose a
method that learns a distance metric for multiple-source problems within a multiple-kernel scenario.
The proposed method deﬁnes the distance of two instances as the sum of their distances in the
feature spaces induced by the different kernels. During learning  a set of Mahalanobis metrics  one
for each source  are learned together. However  this approach ignores the potential correlations
between the different kernels. To the best of our knowledge most of the work on MKL has been
conﬁned in the framework of SVMs and despite the recent popularity of ML there exists so far no
work that performs MKL in the ML framework by learning a distance metric in the weighted linear
combination of feature spaces.
In this paper we show how to perform the Mahalanobis ML with MKL. We ﬁrst propose a general
framework of ML with MKL which can be instantiated with virtually any Mahalanobis ML algo-
rithm h provided that the latter satisﬁes some stated conditions. We examine two parametrizations
of the learning problem that give rise to two alternative formulations  denoted by MLh-MKLµ and
MLh-MKLP. Our approach can be seen as the counterpart of MKL with SVMs [10  20  17] for ML.
Since the learned metric matrix has a regularized form (i.e. it has internal structure) we propose a
straightforward non-regularized version of ML with MKL  denoted by NR-MLh-MKL; however  due
to the number of free parameters the non-regularized version can only scale with very small number
of kernels and requires ML methods that are able to cope with large dimensionalities. We performed
a number of experiments for ML with MKL in which  for the needs of this paper  we have cho-
sen the well known Large Margin Nearest Neighbor [22] (LMNN) algorithm as the ML method h.
The experimental results suggest that LMNN-MKLP outperforms LMNN with an unweighted kernel
combination and the single best kernel selected by internal cross-validation.

2 Preliminaries
In the different ﬂavors of metric learning we are given a matrix of learning instances X : n × d  the
i ∈ Rd instance  and a vector of class labels y = (y1  . . .   yn)T   yi ∈
i-th row of which is the xT
{1  . . .   c}. Consider a mapping Φl(x) of instances x to some feature space Hl  i.e. x → Φl(x) ∈
Hl. The corresponding kernel function kl(xi  xj) computes the inner product of two instances
in the Hl feature space  i.e. kl(xi  xj) = (cid:104)Φl(xi)  Φl(xj)(cid:105). We denote dimensionality of Hl
(possibly inﬁnite) as dl. The squared Mahalanobis distance of two instances in the Hl space
(Φl(xi)  Φl(xj)) = (Φl(xi) − Φl(xj))T Ml(Φl(xi) − Φl(xj))  where Ml is
is given by d2
a Positive Semi-Deﬁnite (PSD) metric matrix in the Hl space (Ml (cid:23) 0). For some given ML
method h we optimize (most often minimize) some cost function Fh with respect to the Ml met-
ric matrix1 under the PSD constraint for Ml and an additional set of pairwise distance constraints
Ch({d2
(Φl(xi)  Φl(xj)) | i  j = 1  . . .   n}) that depend on the choice of h  e.g. similarity and
dissimilarity pairwise constraint [3] and relative comparison constraint [22]. In the reminder of this
paper  for simplicity  we denote this set of constraints as Ch(d2
(Φl(xi)  Φl(xj))). The kernelized
ML optimization problem can be now written as:
s.t. Ch(d2

(Φl(xi)  Φl(xj)))  Ml (cid:23) 0

Fh(Ml)

(1)

Ml

Ml

Ml

Ml

min
Ml

Kernelized ML methods do not require to learn the explicit form of the Mahalanobis metric Ml. It
was shown in [9] that the optimal solution of the Mahalanobis metric Ml is in the form of Ml =
ηhI + Φl(X)T AlΦl(X)  where I is the identity matrix of dimensionality dl × dl  Al is a n × n
PSD matrix  Φl(X) is the matrix of learning instances in the Hl space (with instances in rows)  and
ηh is a constant that depends on the ML method h. Since in the vast majority of the existing ML
methods [19  8  18  23  5  14  22] the value of constant ηh is zero  in this paper we only consider the
optimal form of Ml with ηh = 0. Under the optimal parametrization of Ml = Φl(X)T AlΦl(X)
the squared Mahalanobis distance becomes:

(Φl(xi)  Φl(xj)) = (Ki

(2)
d2
Ml
l is the i-th column of kernel matrix Kl  the (i  j) element of which is Klij = kl(xi  xj).

(Φl(xi)  Φl(xj))

l )T Al(Ki

l ) = d2
Al

l − Kj

l − Kj

where Ki
As a result  (1) can be rewritten as:

min
Al

Fh(Φl(X)T AlΦl(X))

s.t. Ch(d2

Al

(Φl(xi)  Φl(xj)))  Al (cid:23) 0

(3)

1The optimization could also be done with respect to other variables of the cost function and not only Ml.

However  to keep the notation uncluttered we parametrize the optimization problem only over Ml.

2

In MKL we are given a set of kernel functions Z = {kl(xi  xj) | l = 1 . . . m} and the goal is to
learn an appropriate kernel function kµ(xi  xj) parametrized by µ under a cost function Q. The cost
function Q is determined by the cost function of the learning method that is coupled with multiple
kernel learning  e.g. it can be the SVM cost function if one is using an SVM as the learning approach.
As in [10  17] we parametrize kµ(xi  xj) by a linear combination of the form:

µlkl(xi  xj)  µl ≥ 0 

kµ(xi  xj) =

(4)
We denote the feature space that is induced by the kµ kernel by Hµ  feature space which is given
by the mapping x → Φµ(x) = (
µmΦm(x)T )T ∈ Hµ. We denote the dimen-
sionality of Hµ by d; it can be inﬁnite. Finally  we denote by H the feature space that we get by the
unweighted concatenation of the m feature spaces  i.e. ∀µi  µi = 1  whose representation is given
by x → Φ(x) = (Φ1(x)T   . . .   Φm(x)T )T .

µ1Φ1(x)T   . . .  

µl = 1

√

√

i=l

l

m(cid:88)

m(cid:88)

3 Metric Learning with Multiple Kernel Learning
The goal is to learn a metric matrix M in the feature space Hµ induced by the mapping Φµ as
well as the kernel weight µ; we denote this metric by d2
M µ. Based on the optimal form of the
Mahalanobis metric M for metric learning method learning with a single kernel function [9]  we
have the following lemma:
Lemma 1. Assume that for a metric learning method h the optimal parameterization of its Maha-
lanobis metric M∗ is Φl(X)T A∗Φl(X)  for some A∗  when learning with a single kernel function
kl(x  x(cid:48)). Then  for h with multiple kernel learning the optimal parametrization of its Mahalanobis
metric M∗∗ is given by Φµ(X)T A∗∗Φµ(X)  for some A∗∗.
The proof of the above Lemma is similar to the proof of Theorem 1 in [9] (it is not presented here
due to the lack of space). Following Lemma 1  we have:
M µ(Φµ(xi)  Φµ(xj)) =
d2

(Φµ(xi) − Φµ(xj))T Φµ(X)T AΦµ(X)(Φµ(xi) − Φµ(xj)) (5)
A µ(Φµ(xi)  Φµ(xj))

= (cid:88)

l )T A(cid:88)

l − Kj

l − Kj

l ) = d2

µl(Ki

µl(Ki

l

l

Based on (5) and the constraints from (4)  the ML optimization problem with MKL can be presented
as:

min
A µ

Fh(Φµ(X)T AΦµ(X))

s.t. Ch(d2

A µ(Φµ(xi)  Φµ(xj)))  A (cid:23) 0  µl ≥ 0 

µl = 1(6)

m(cid:88)

l

We denote the resulting optimization problem and the learning method by MLh-MKLµ; clearly this
is not fully speciﬁed until we choose a speciﬁc ML method h.

 (Ki

(Ki

1 − Kj
. . .
m − Kj

1)T

m)T

. We note that d2

Let B =

as:

A µ(Φµ(xi)  Φµ(xj)) from (5) can also be written

A µ(Φµ(xi)  Φµ(xj)) = µT BABTµ = tr(PBABT) = d2
d2

(7)
where P = µµT and tr(·) is the trace of a matrix. We use ΦP(X) to emphasize the explicit the
dependence of Φµ(X) to P = µµT . As a result  instead of optimizing over µ we can also use the
parametrization over P; the new optimization problem can now be written as:
min
A P

A P(ΦP(xi)  ΦP(xj))

(8)
Pij = 1  Pij ≥ 0  Rank(P) = 1  P = PT

Fh(ΦP(X)T AΦP(X))
Ch(d2

A P(ΦP(xi)  ΦP(xj)))  A (cid:23) 0 

ij Pij = 1  Pij ≥ 0  Rank(P) = 1  and P = PT are added so that
P = µµT . We call the optimization problem and learning method (8) as MLh-MKLP; as before in
order to fully instantiate it we need to choose a speciﬁc metric learning method h.

s.t.

where the constraints(cid:80)

(cid:88)

ij

3

Now  we derive an alternative parametrization of (5). We need two additional matrices: Cµiµj =
µiµjI  where the dimensionality of I is n × n  and Φ
(X) which is an mn × d dimensional matrix:
. . .
. . .
. . . Φm(X)

(cid:34) Φ1(X)

(X) =

0
. . .

. . .
0

(cid:35)

Φ

(cid:48)

(cid:48)

We have:

A µ(Φµ(xi)  Φµ(xj)) = (Φ(xi) − Φ(xj))T M
d2

(cid:48)

(Φ(xi) − Φ(xj))

where:
and A(cid:48) is a mn × mn matrix:

A(cid:48) =

(cid:48)
M

(cid:48)
= Φ

(cid:48)
(X)T A(cid:48)Φ

(cid:34) Cµ1µ1A . . . Cµ1µmA

(X)

. . .

. . .

. . .

Cµmµ1A . . . CµmµmA

(cid:35)

.

(9)

(10)

(11)

From (9) we see that the Mahalanobis metric  parametrized by the M or A matrix  in the feature
space Hµ induced by the kernel kµ  is equivalent to the Mahalanobis metric in the feature space H
which is parametrized by M(cid:48) or A(cid:48). As we can see from (11)  MLh-MKLµ and MLh-MKLP learn
a regularized matrix A(cid:48) (i.e. matrix with internal structure) that corresponds to a parametrization of
the Mahalanobis metric M(cid:48)

in the feature space H.

3.1 Non-Regularized Metric Learning with Multiple Kernel Learning

We present here a more general formulation of the optimization problem (6) in which we lift the
regularization of matrix A(cid:48) from (11)  and learn instead a full PSD matrix A(cid:48)(cid:48):

(cid:34) A11

A(cid:48)(cid:48) =

. . . A1m
. . .
. . .
. . .
A1m . . . Amm

(cid:35)

(12)

where Akl is an n × n matrix. The respective Mahalanobis matrix  which we denote by M(cid:48)(cid:48)  still
(cid:48)
(cid:48)
have the same parametrization form as in (10)  i.e. M(cid:48)(cid:48) = Φ
(X). As a result  by using
A(cid:48)(cid:48) instead of A(cid:48) the squared Mahalanobis distance can be written now as:

(X)T A(cid:48)(cid:48)Φ

A(cid:48)(cid:48)(Φ(xi)  Φ(xj)) = (Φ(xi) − Φ(xj))T M
d2
m)T ]A(cid:48)(cid:48)[(Ki
= [(Ki
= [ΦZ(xi) − ΦZ(xj)]T A(cid:48)(cid:48)(ΦZ(xi) − ΦZ(xj)]

1)T   . . .   (Ki

m − Kj

1 − Kj

(cid:48)(cid:48)

(Φ(xi) − Φ(xj))
1 − Kj
1)T   . . .   (Ki

m − Kj

m)T ]T

(13)

m)T )T ∈ HZ. What we see here is that under the M(cid:48)(cid:48)
where ΦZ(xi) = ((Ki
parametrization computing the Mahalanobis metric in the H is equivalent to computing the Ma-
halanobis metric in the HZ space. Under the parametrization of the Mahalanobis distance given by
(13)  the optimization problem of metric learning with multiple kernel learning is the following:

1)T   . . .   (Ki

(cid:48)

A(cid:48)(cid:48) Fh(Φ
min

(cid:48)
(X)T A(cid:48)(cid:48)Φ

(X))

s.t. Ch(d2

A(cid:48)(cid:48)(Φ(xi)  Φ(xj)))  A(cid:48)(cid:48) (cid:23) 0

(14)

We call this optimization problem NR-MLh-MKL. We should note that this formulation has scaling
problems since it has O(m2n2) parameters that need to be estimated  and it clearly requires a very
efﬁcient ML method h in order to be practical.

4 Optimization

4.1 Analysis

The NR-MLh-MKL optimization problem obviously has the same convexity properties as the metric
(cid:48)
learning algorithm h that will be used  since the parametrization M(cid:48)(cid:48) = Φ
(X) used in
NR-MLh-MKL is linear with A(cid:48)(cid:48)  and the composition of a function with an afﬁne mapping preserves

(cid:48)
(X)T A(cid:48)(cid:48)Φ

4

the convexity property of the original function [1]. This is also valid for the subproblems of learning
matrix A in MLh-MKLµ and MLh-MKLP given the weight vector µ.
Given the PSD matrix A  we have the following two lemmas for optimization problems MLh-
MKL{µ|P}:
Lemma 2. Given the PSD matrix A the MLh-MKLµ optimization problem is convex with µ if
metric learning algorithm h is convex with µ.

Proof. The last two constraints on µ of the optimization problem from (6) are linear  thus this
problem is convex if metric learning algorithm h is convex with µ.

A µ(Φµ(xi)  Φµ(xj)) is convex quadratic of µ  which can be easily proved based on the
Since d2
PSD property of matrix BABT in (7)  many of the well known metric learning algorithms  such as
Pairwise SVM [21]  POLA [19] and Xing’s method [23] satisfy the conditions in Lemma 2.
The MLh-MKLP optimization problem (8) is not convex given a PSD matrix A because the rank
constraint is not convex. However  when the number of kernels m is small  e.g. a few tens of kernels 
there is an equivalent convex formulation.
Lemma 3. Given the PSD matrix A  the MLh-MKLP optimization problem (8) can be formulated
as an equivalent convex problem with respect to P if the ML algorithm h is linear with P and the
number of kernel m is small.

Proof. Given the PSD matrix A  if h is linear with P  we can formulate the rank constraint problem
with the help of the two following convex problems [2]:
Fh(ΦP(X)T AΦP(X)) + w · tr(PT W)
Ch(d2

A P(ΦP(xi)  ΦP(xj)))  A (cid:23) 0  P (cid:23) 0 

Pij = 1  Pij ≥ 0  P = PT

(cid:88)

min
P

(15)

s.t.

where w is a positive scalar just enough to make tr(PT W) vanish  i.e. global convergence deﬁned
in (17)  and the direction matrix W is an optimal solution of the following problem:

ij

min
W

tr(P∗T W)

s.t. 0 (cid:22) W (cid:22) I  tr(W) = m − 1

(16)
where P∗ is an optimal solution of (15) given A and W  and m is the number of kernels. The
problem (16) has a closed form solution W = UUT   where U ∈ Rm×m−1 is the eigenvector matrix
of P∗ whose columns are the eigenvectors which correspond to the m − 1 smallest eigenvalues of
P∗. The two convex problems are iteratively solved until global convergence  deﬁned as:

m(cid:88)

λ(P∗)i = tr(P∗T W∗) = λ(P∗)T λ(W∗) ≡ 0

(17)

i=2

where λ(P∗)i is the i-th largest eigenvalue of P∗. This formulation is not a projection method. At
global convergence the convex problem (15) is not a relaxation of the original problem  instead it is
an equivalent convex problem [2].
We will now prove the convergence of problem (15). Suppose the objective value of (15) is fi at
iteration i. Since both (15) and (16) minimize the objective value of (15)  we have fj < fi for
any iteration j > i. Beacuse the inﬁmum f∗ of the objective value of (15) corresponds to the
optimal objective value of (15) when the second term is removed. Thus the nonincreasing sequence
of objective values is bounded below and as a result converges because any bounded monotonic
sequence in R is convergent. Thus the local convergence of (15) is now established.
Only the local convergence can be established for problem (15) because the objective tr(PT W) is
generally multimodal [2]. However  as indicated in section 7.2 [2]  when the size of m is small  the
global optimal of problem (15) can be often achieved. This can be simply veriﬁed by comparing the
difference between the inﬁmum f∗ and the optimal objective value f of problem (15).

For a number of known metric learning algorithms  such as LMNN [22]  POLA [19]  MLSVM [14]
and Xing’s method [23] linearity with respect to P holds given A (cid:23) 0.

5

Algorithm 1 MLh-MKLµ  MLh-MKLP

Input: X  Y  A0  µ0  and matrices K1  . . .   Km
Output: A and µ
repeat

Kµ(i) =(cid:80)

µ(i)=WeightLearning(A(i−1))
A(i)=MetricLearningh(A(i−1) X Kµ(i))
i := i + 1

kKk

k µi

until convergence

4.2 Optimization Algorithms

The NR-MLh-MKL optimization problem can be directly solved by any metric learning algorithm
h on the space HZ when the optimization problem of the latter only involves the squared pairwise
Mahalanobis distance  e.g. LMNN [22] and MCML [5]. When the metric learning algorithm h
has regularization term on M  e.g.
trace norm [8] and Frobenius norm [14  19]  most often the
NR-MLh-MKL optimization problem can be solved by a slightly modiﬁcation of original algorithm.
We now describe how we can solve the optimization problems of MLh-MKLµ and MLh-MKLP.
Based on Lemmas 2 and 3 we propose for both methods a two-step iterative algorithm  Algorithm 1 
at the ﬁrst step of which we learn the kernel weighting and at the second the metric under the kernel
weighting learned in the ﬁrst step. At the ﬁrst step of the i-th iteration we learn the µ(i) kernel weight
vector under ﬁxed PSD matrices A(i−1)  learned at the preceding iteration (i − 1). For MLh-MKLµ
we solve the weight learning problem using Lemma 2 and for MLh-MKLP using Lemma 3. At
the second step we apply the metric learning algorithm h and we learn the PSD matrices A(i) with
l Ki kernel matrix using as the initial metric matrices the A(i−1). We should
make clear that the optimization problem we are solving is only individually convex with respect to
µ given the PSD matrix A and vice-versa. As a result  the convergence of the two-step algorithm
(possible to a local optima) is guaranteed [6] and checked by the variation of µ and the objective
value of metric learning method h. In our experiments (Section 6) we observed that it most often
converges in less than ten iterations.

the Kµ(i) = (cid:80)

l µ(i)

5 LMNN-Based Instantiation

We have presented two basic approaches to metric learning with multiple kernel learning: MLh-
MKLµ (MLh-MKLP) and NR-MLh-MKL. In order for the approaches to be fully instantiated we have
to specify the ML algorithm h. In this paper we focus on the LMNN state-of-the-art method [22].
Due to the relative comparison constraint  LMNN does not satisfy the condition of Lemma 2. How-
ever  as we already mentioned LMNN satisﬁes the condition of Lemma 3 so we get the MLh-MKLP
variant of the optimization problem for LMNN which we denote by LMNN-MKLP. The resulting
optimization problem is:

A P(ΦP(xi)  ΦP(xj)) + γ

(1 − Yik)ξijk}

(18)

Sij{(1 − γ)d2

(cid:88)
(cid:88)
ij
A P(ΦP(xi)  ΦP(xk)) − d2
d2

min
A P ξ

s.t.

(cid:88)

k

A P(ΦP(xi)  ΦP(xj)) ≥ 1 − ξijk  ξijk > 0  A (cid:23) 0

Pkl = 1  Pkl ≥ 0  Rank(P) = 1  P = PT

kl

where the matrix Y  Yij ∈ {0  1}  indicates if the class labels yi and yj are the same (Yij = 1)
or different (Yij = 0). The matrix S is a binary matrix whose Sij entry is non-zero if instance xj
is one of the k same class nearest neigbors of instance xi. The objective is to minimize the sum of
the distances of all instances to their k same class nearest neighbors while allowing for some errors 
trade of which is controlled by the γ parameter. As the objective function of LMNN only involves
the squared pairwise Mahalanobis distances  the instantiation of NR-MLh-MKL is straightforward
and it consists simply of the application of LMNN on the space HZ in order to learn the metric. We
denote this instantiation by NR-LMNN-MKL.

6

Table 1: Accuracy results. The superscripts +−= next to the accuracies of NR-LMNN-MKLand
LMNN-MKLPindicate the result of the McNemar’s statistical test of their comparison to the accura-
cies of LMNNHand LMNN-MKLCV and denote respectively a signiﬁcant win  loss or no difference.
The number in the parenthesis indicates the score of the respective algorithm for the given dataset
based on the pairwise comparisons of the McNemar’s statistical test.

Datasets
Sonar
Wine
Iris
Ionosphere
Wdbc
CentralNervous
Colon
Leukemia
MaleFemale
Ovarian
Prostate
Stroke
Total Score

NR-LMNN-MKL

88.46+=(3.0)
98.88==(2.0)
93.33==(2.0)
93.73==(2.5)
94.90−=(1.0)
55.00==(2.0)
80.65==(2.0)
95.83+=(2.5)
86.57==(2.5)
95.26+=(3.0)
79.50==(2.0)
69.71==(2.0)

26.5

LMNN-MKLP
85.58==(2.0)
98.88==(2.0)
95.33==(2.0)
94.87=+(3.0)
97.36=+(3.5)
63.33==(2.0)
85.48+=(2.5)
94.44+=(2.5)
88.81+=(3.0)
94.47+=(3.0)
80.43==(2.5)
72.12==(2.0)

30.0

LMNNH
82.21(1.0)
98.31(2.0)
94.67(2.0)
92.59(2.5)
97.36(3.0)
65.00(2.0)
66.13(1.5)
70.83(0.0)
80.60(1.5)
90.51(0.5)
79.19(2.0)
71.15(2.0)

20.0

LMNN-MKLCV

88.46(3.0)
96.07(2.0)
94.00(2.0)
90.88(2.0)
95.96(1.5)
65.00(2.0)
79.03(2.0)
95.83(2.5)
89.55(3.0)
94.47(3.0)
78.88(2.0)
70.19(2.0)

27.0

1-NN

82.21(1.0)
97.19(2.0)
95.33(2.0)
86.89(0.0)
95.43(1.0)
58.33(2.0)
74.19(2.0)
88.89(2.5)
58.96(0.0)
87.35(0.5)
76.71(1.5)
65.38(2.0)

16.5

6 Experiments

In this section we perform a number of experiments on real world datasets in order to compare the
two of the LMNN-based instantiations of our framework  i.e. LMNN-MKLP and NR-LMNN-MKL.
We compare these methods against two baselines: LMNN-MKLCV in which a kernel is selected
from a set of kernels using 2-fold inner cross-validation (CV)  and LMNN with the unweighted
sum of kernels  which induces the H feature space  denoted by LMNNH. Additionally  we report
performance of 1-Nearest-Neighbor  denoted as 1-NN  with no metric learning. The PSD matrix
A and weight vector µ in LMNN-MKLP were respectively initialized by I and equal weighting (1
divided by the number of kernels). The parameter w in the weight learning subproblem of LMNN-
MKLP was selected from {10i | i = 0  1  . . .   8} and was the smallest value enough to achieve
Its direction matrix W was initialized by 0. The number of k same class
global convergence.
nearest neighbors required by LMNN was set to 5 and its γ parameter to 0.5. After learning the
metric and the multiple kernel combination we used 1-NN for classiﬁcation.

6.1 Benchmark Datasets

We ﬁrst experimented with 12 different datasets: ﬁve from the UCI machine learning repository 
i.e. Sonar  Ionosphere  Wine  Iris  and Wdbc; three microarray datasets  i.e. CentralNervous  Colon 
and Leukemia; and four proteomics datasets  i.e. MaleFemale  Stroke  Prostate and Ovarian. The
attributes of all the datasets are standardized in the preprocessing step. The Z set of kernels that we
use consists of the following 20 kernels: 10 polynomial with degree from one to ten  ten Gaussians
with bandwidth σ ∈ {0.5  1  2  5  7  10  12  15  17  20} (the same set of kernels was used in [4]).
Each basic kernel Kk was normalized by the average of its diag(Kk). LMNN-MKLP  LMNNH and
LMNN-MKLCV were tested using the complete Z set. For NR-LMNN-MKL due to its scaling limi-
tations we could only use a small subset of Z consisting of the linear  the second order polynomial 
and the Gaussian kernel with the kernel width of 0.5. We use 10-fold CV to estimate the predictive
performance of the different methods. To test the statistical signiﬁcance of the differences we used
McNemar’s test and we set the p-value to 0.05. To get a better understanding of the relative per-
formance of the different methods for a given dataset we used a ranking schema in which a method
A was assigned one point if its accuracy was signiﬁcantly better than that of another method B  0.5
points if the two methods did not have a signiﬁcantly different performance  and zero points if A
was found to be signiﬁcantly worse than B.
The results are reported in Table 1. First  we observe that by learning the kernel inside LMNN-
MKLP we improve performance over LMNNH that uses the unweighted kernel combination. More
precisely  LMNN-MKLP is signiﬁcantly better than LMNNH in four out of the thirteen datasets. If
we now compare LMNN-MKLP with LMNN-MKLCV   the other baseline method where we select the
best kernel with CV  we can see that LMNN-MKLP also performs better being statistically signiﬁcant

7

Table 2: Accuracy results on the multiple source datasets.

Datasets
Multiple Feature
Oxford Flowers

LMNN-MKLP
98.79++(3.0)
86.01++(3.0)

LMNNH
98.44(1.5)
85.74(2.0)

LMNN-MKLCV

98.44(1.5)
65.46(0.0)

1-NN

97.86(0.0)
67.38(1.0)

better in two dataset. If we now examine NR-LMNN-MKL and LMNNH we see that the former
method  even though learning with only three kernels  is signiﬁcantly better in two datasets  while it
is signiﬁcantly worse in one dataset. Comparing NR-LMNN-MKL and LMNN-MKLCV we observe
that the two methods achieve comparable predictive performances. We should stress here that NR-
LMNN-MKL has a disadvantage since it only uses three kernels  as opposed to other methods that
use 20 kernels; the scalability of NR-LMNN-MKL is left as a future work. In terms of the total score
that the different methods obtain the best one is LMNN-MKLP followed by LMNN-MKLCV and
NR-LMNN-MKL.

6.2 Multiple Source Datasets

To evaluate the proposed method on problems with multiple sources of information we also perform
experiments on the Multiple Features and the Oxford ﬂowers datasets [16]. Multiple Features from
UCI has six different feature representations for 2 000 handwritten digits (0-9); each class has 200
instances. In the preprocessing step all the features are standardized in all the data sources. Oxford
ﬂowers dataset has 17 category ﬂower images; each class has 80 instances. In the experiment seven
distance matrices from the website2 are used; these matrices are precomputed respectively from
seven features  the details of which are described in [16  15]. For both datasets Gaussian kernels are
constructed respectively using the different feature representations of instances with kernel width
σ0  where σ0 is the mean of all pairwise distances. We experiment with 10 random splits where
half of the data is used for training and the other half for testing. We do not experiment here with
NR-LMNN-MKL here due to its scaling limitations.
The accuracy results are reported in Table 2. We can see that by learning a linear combination of
different feature representations LMNN-MKLP achieves the best predictive performance on both
datasets being signiﬁcantly better than the two baselines  LMNNH and LMNN-MKLCV . The bad
performance of LMNN-MKLCV on the Oxford ﬂowers dataset could be explained by the fact that
the different Gaussian kernels are complementary for the given problem  but in LMNN-MKLCV only
one kernel is selected.

7 Conclusions

In this paper we combine two recent developments in the ﬁeld of machine learning  namely metric
learning and multiple kernel learning  and propose a general framework for learning a metric in
a feature space induced by a weighted combination of a number of individual kernels. This is in
contrast with the existing kernelized metric learning techniques which consider only one kernel
function (or possibly an unweighted combination of a number of kernels) and hence are sensitive
to the selection of the associated feature space. The proposed framework is general as it can be
coupled with many existing metric learning techniques. In this work  to practically demonstrate the
effectiveness of the proposed approach  we instantiate it with the well know LMNN metric learning
method. The experimental results conﬁrm that the adaptively induced feature space does bring
an advantage in the terms of predictive performance with respect to feature spaces induced by an
unweighted combination of kernels and the single best kernel selected by internal CV.

Acknowledgments

This work was funded by the Swiss NSF (Grant 200021-122283/1). The support of the European
Commission through EU projects DebugIT (FP7-217139) and e-LICO (FP7-231519) is also grate-
fully acknowledged.

2http://www.robots.ox.ac.uk/∼vgg/data/ﬂowers/index.html

8

References
[1] S.P. Boyd and L. Vandenberghe. Convex optimization. Cambridge Univ Pr  2004.
[2] J. Dattorro. Convex optimization & Euclidean distance geometry. Meboo Publishing USA 

2005.

[3] J.V. Davis  B. Kulis  P. Jain  S. Sra  and I.S. Dhillon. Information-theoretic metric learning. In

ICML  2007.

[4] K. Gai  G. Chen  and C. Zhang. Learning kernels with radiuses of minimum enclosing balls.

NIPS  2010.

[5] A. Globerson and S. Roweis. Metric learning by collapsing classes. In NIPS  2006.
[6] L. Grippo and M. Sciandrone. On the convergence of the block nonlinear gauss-seidel method

under convex constraints* 1. Operations Research Letters  26(3):127–136  2000.

[7] M. Guillaumin  J. Verbeek  and C. Schmid. Is that you? Metric learning approaches for face

identiﬁcation. In ICCV  pages 498–505  2009.

[8] K. Huang  Y. Ying  and C. Campbell. Gsml: A uniﬁed framework for sparse metric learning.
In Data Mining  2009. ICDM’09. Ninth IEEE International Conference on  pages 189–198.
IEEE  2009.

[9] P. Jain  B. Kulis  and I. Dhillon.

2010.

Inductive regularized learning of kernel functions. NIPS 

[10] G.R.G. Lanckriet  N. Cristianini  P. Bartlett  L. El Ghaoui  and M.I. Jordan. Learning the
Kernel Matrix with Semideﬁnite Programming. Journal of Machine Learning Research  5:27–
72  2004.

[11] B. McFee and G. Lanckriet. Partial order embedding with multiple kernels. In Proceedings of
the 26th Annual International Conference on Machine Learning  pages 721–728. ACM  2009.
[12] B. McFee and G. Lanckriet. Metric learning to rank. In ICML. ACM New York  NY  USA 

2010.

[13] B. McFee and G. Lanckriet. Learning multi-modal similarity. The Journal of Machine Learn-

ing Research  12:491–523  2011.

[14] N. Nguyen and Y. Guo. Metric Learning: A Support Vector Approach. In ECML/PKDD  2008.
[15] M.E. Nilsback and A. Zisserman. A visual vocabulary for ﬂower classiﬁcation. In Computer
Vision and Pattern Recognition  2006 IEEE Computer Society Conference on  volume 2  pages
1447–1454. Ieee  2006.

[16] M.E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of
classes. In Computer Vision  Graphics & Image Processing  2008. ICVGIP’08. Sixth Indian
Conference on  pages 722–729. IEEE  2008.

[17] A. Rakotomamonjy  F. Bach  S. Canu  and Y. Grandvalet. SimpleMKL. Journal of Machine

Learning Research  9:2491–2521  2008.

[18] M. Schultz and T. Joachims. Learning a distance metric from relative comparisons. In NIPS 

2003.

[19] S. Shalev-Shwartz  Y. Singer  and A.Y. Ng. Online and batch learning of pseudo-metrics. In

Proceedings of the twenty-ﬁrst international conference on Machine learning. ACM  2004.

[20] S. Sonnenburg  G. Ratsch  and C. Schafer. A general and efﬁcient multiple kernel learning

algorithm. In NIPS  2006.

[21] J.P. Vert  J. Qiu  and W. Noble. A new pairwise kernel for biological network inference with

support vector machines. BMC bioinformatics  8(Suppl 10):S8  2007.

[22] K.Q. Weinberger and L.K. Saul. Distance metric learning for large margin nearest neighbor

classiﬁcation. The Journal of Machine Learning Research  10:207–244  2009.

[23] E.P. Xing  A.Y. Ng  M.I. Jordan  and S. Russell. Distance metric learning with application to

clustering with side-information. In NIPS  2003.

9

,Kewei Tu
Maria Pavlovskaia
Song-Chun Zhu
Anselm Rothe
Brenden Lake
Todd Gureckis