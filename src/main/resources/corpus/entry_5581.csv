2009,Time-rescaling methods for the estimation and assessment of non-Poisson neural encoding models,Recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history. Typically  these models incorporate spike-history dependencies via either: (A) a conditionally-Poisson process with rate dependent on a linear projection of the spike train history (e.g.  generalized linear model); or (B) a modulated non-Poisson renewal process (e.g.  inhomogeneous gamma process). Here we show that the two approaches can be combined  resulting in a {\it conditional renewal} (CR) model for neural spike trains. This model captures both real and rescaled-time effects  and can be fit by maximum likelihood using a simple application of the time-rescaling theorem [1]. We show that for any modulated renewal process model  the log-likelihood is concave in the linear filter parameters only under certain restrictive conditions on the renewal density (ruling out many popular choices  e.g. gamma with $\kappa \neq1$)  suggesting that real-time history effects are easier to estimate than non-Poisson renewal properties. Moreover  we show that goodness-of-fit tests based on the time-rescaling theorem [1] quantify relative-time effects  but do not reliably assess accuracy in spike prediction or stimulus-response modeling. We illustrate the CR model with applications to both real and simulated neural data.,Time-rescaling methods for the estimation and

assessment of non-Poisson neural encoding models

Jonathan W. Pillow

Departments of Psychology and Neurobiology

University of Texas at Austin

pillow@mail.utexas.edu

Abstract

Recent work on the statistical modeling of neural responses has focused on mod-
ulated renewal processes in which the spike rate is a function of the stimulus and
recent spiking history. Typically  these models incorporate spike-history depen-
dencies via either: (A) a conditionally-Poisson process with rate dependent on
a linear projection of the spike train history (e.g.  generalized linear model); or
(B) a modulated non-Poisson renewal process (e.g.  inhomogeneous gamma pro-
cess). Here we show that the two approaches can be combined  resulting in a
conditional renewal (CR) model for neural spike trains. This model captures both
real-time and rescaled-time history effects  and can be ﬁt by maximum likelihood
using a simple application of the time-rescaling theorem [1]. We show that for
any modulated renewal process model  the log-likelihood is concave in the linear
ﬁlter parameters only under certain restrictive conditions on the renewal density
(ruling out many popular choices  e.g. gamma with shape κ (cid:54)= 1)  suggesting that
real-time history effects are easier to estimate than non-Poisson renewal proper-
ties. Moreover  we show that goodness-of-ﬁt tests based on the time-rescaling
theorem [1] quantify relative-time effects  but do not reliably assess accuracy in
spike prediction or stimulus-response modeling. We illustrate the CR model with
applications to both real and simulated neural data.

1 Introduction

A central problem in computational neuroscience is to develop functional models that can accurately
describe the relationship between external variables and neural spike trains. All attempts to measure
information transmission in the nervous system are fundamentally attempts to quantify this relation-
ship  which can be expressed by the conditional probability P ({ti}|X)  where {ti} is a set of spike
times generated in response to an external stimulus X.
Recent work on the neural coding problem has focused on extensions of the Linear-Nonlinear-
Poisson (LNP) “cascade” encoding model  which describes the neural encoding process using a
linear receptive ﬁeld  a point nonlinearity  and an inhomogeneous Poisson spiking process [2  3].
While this model provides a simple  tractable tool for characterizing neural responses  one obvious
shortcoming is the assumption of Poisson spiking. Neural spike trains exhibit spike-history depen-
dencies (e.g.  refractoriness  bursting  adaptation)  violating the Poisson assumption that spikes in
disjoint time intervals are independent. Such dependencies  moreover  have been shown to be es-
sential for extracting complete stimulus information from spike trains in a variety of brain areas
[4  5  6  7  8  9  10  11].
Previous work has considered two basic approaches for incorporating spike-history dependencies
into neural encoding models. One approach is to model spiking as a non-Poisson inhomogeneous
renewal process (e.g.  a modulated gamma process [12  13  14  15]). Under this approach  spike

1

Figure 1: The conditional renewal (CR) model and time-rescaling transform. (A) Stimuli are con-
volved with a ﬁlter k then passed through a nonlinearity f  whose output is the rate λ(t) for an inho-
mogeneous spiking process with renewal density q. The post-spike ﬁlter h provides recurrent additive
input to f for every spike emitted. (B) Illustration of the time-rescaling transform and its inverse. Top:
the intensity λ(t) (here independent of spike history) in response to a one-second stimulus. Bottom
left: interspike intervals (left  intervals between red dots) are drawn i.i.d. in rescaled time from renewal
density q  here set to gamma with shape κ = 20. Samples are mapped to spikes in real time (bot-
tom) via Λ−1(t)  the inverse of the cumulative intensity. Alternatively  Λ(t) maps the true spike times
(bottom) to samples from a homogeneous renewal process in rescaled time (left edge).

times are Markovian  depending on the most recent spike time via a (non-exponential) renewal
density  which may be rescaled in proportion to the instantaneous spike rate. A second approach
is to use a conditionally Poisson process in which the intensity (or spike rate) is a function of the
recent spiking history [4  16  17  18  19  20]. The output of such a model is a conditionally Poisson
process  but not Poisson  since the spike rate itself depends on the spike history.
The time-rescaling theorem  described elegantly for applications to neuroscience in [1]   provides a
powerful tool for connecting these two basic approaches  which is the primary focus of this paper.
We begin by reviewing inhomogeneous renewal models and generalized linear model point process
models for neural spike trains.

2 Point process neural encoding models

2.1 Deﬁnitions and Terminology
Let {ti} be a sequence of spike times on the interval (0  T ]  with 0 < t0 < t1 < . . .   < tn ≤ T  
and let λ(t) denote the intensity (or “spike rate”) for the point process  where λ(t) ≥ 0 ∀t. Gener-
ally  this intensity is a function of some external variable (e.g.  a visual stimulus). The cumulative
intensity function is given by the integrated intensity 

Λ(t) =

λ(s)ds 

(1)

and is also known as the time-rescaling transform [1]. This function rescales the original spike
times into spikes from a (homogeneous) renewal process  that is  a process in which the intervals
are i.i.d. samples from a ﬁxed distribution. Let {ui} denote the inter-spike intervals (ISIs) of the
rescaled process  which are given by the integral of the intensity between successive spikes  i.e. 

(cid:90) t

0

(cid:90) ti

ti−1

ui = Λti−1(ti) =

λ(s)ds.

(2)

Intuitively  this transformation stretches time in proportion to the spike rate λ(t)   so that when the
rate λ(t) is high  ISIs are lengthened and when λ(t) is low  ISIs are compressed. (See ﬁg. 1B for
illustration).

2

Anonlinearityrescaled renewal spiking post-spike filterstimulus filter+01201234567real time (s)rescaled time (unitless)rescaled timep(ISI)...Brenewal density 050100rate (Hz)Let q(u) denote the renewal density  the probability density function from which the rescaled-time
intervals {ui} are drawn. A Poisson process arises if q is exponential  q(u) = e−u; for any other
density  the probability of spiking depends on the most recent spike time. For example  if q(u) is
zero for u ∈ [0  a]  the neuron exhibits a refractory period (whose duration varies with λ(t)).
To sample from this model (illustrated in ﬁg. 1B)  we can draw independent intervals ui from re-
newal density q(u)  then apply the inverse time-rescaling transform to obtain ISIs in real time:

(ti − ti−1) = Λ−1

ti−1(ui) 

(3)

ti−1(t) is the inverse of time-rescaling transform (eq 2).1

where Λ−1
We will generally deﬁne the intensity function (which we will refer to as the base intensity2) in terms
of a linear-nonlinear cascade  with linear dependence on some external covariates of the response
(optionally including spike-history)  followed by a point nonlinearity. The intensity in this case can
be written:

(4)
where xt is a vector representing the stimulus at time t  k is a stimulus ﬁlter  yt is a vector repre-
senting the spike history at t  and h is a spike-history ﬁlter. We assume that the nonlinearity f is
ﬁxed.

λ(t) = f(xt · k + yt · h) 

2.2 The conditional renewal model

We refer to the most general version of this model  in which λ(t) is allowed to depend on both
the stimulus and spike train history  and q(u) is an arbitrary (ﬁnite-mean) density on R+  as a
conditional renewal (CR) model (see ﬁg. 1A). The output of this model forms an inhomogeneous
renewal process conditioned on the process history. Although it is mathematically straightforward
to deﬁne such a model  to our knowledge  no previous work has sought to incorporate both real-time
(via h) and rescaled-time (via q) dependencies in a single model.
Speciﬁc (restricted) cases of the CR model include the generalized linear model (GLM) [17]  and the
modulated renewal model with λ = f(x · k) and q a right-skewed  non-exponential renewal density
[13  15]. (Popular choices for q include gamma  inverse Gaussian  and log-normal distributions).
The conditional probability distribution over spike times {ti} given the external variables X can
be derived using the time-rescaling transformation.
In rescaled time  the CR model speciﬁes a
probability over the ISIs 

P ({ui}|X) =

q(ui).

(5)

n(cid:89)

i=1

n(cid:89)

i=1

A change-of-variables ti = Λ−1
times:

ti−1(ui) + ti−1 (eq. 3) provides the conditional probability over spike

P ({ti}|X) =

λ(ti)q(Λti−1(ti)).

(6)

This probability  considered as a function of the parameters deﬁning λ(t) and q(u)  is the likelihood
function for the CR model  as derived in [13].3 The log-likelihood function can be approximated in
discrete time  with bin-size dt taken small enough to ensure ≤ 1 spike per bin:

n(cid:88)

n(cid:88)

 ti(cid:88)

  

log P ({ti}|X) =

log λ(ti) +

log q

λ(j)dt

(7)

where ti indicates the bin for the ith spike. This approximation becomes exact in the limit as dt → 0.

i=1

i=1

j=ti−1+1

1Note that Λt∗ (t) is invertible for all spike times ti  since necessarily ti ∈ {t; λ(t) > 0}.
2A note on terminology: we follow [13] in deﬁning λ(t) to be the instantaneous rate for an inhomogeneous
renewal process  which is not identical to the hazard function H(t) = P (ti ∈ [t  t + ∆]|ti > ti−1)/∆  also
known as the conditional intensity [1]. We will use “base intensity” for λ(t) to avoid this confusion.

3For simplicity  we have ignored the intervals (0  t0]  the time to the ﬁrst spike  and (tn  T ]  the time after

the last spike  which are simple to compute but contribute only a small fraction to the total likelihood.

3

Figure 2: Time-rescaling and likelihood-based goodness-of-ﬁt tests with simulated data. : Left: Stim-
ulus ﬁlter and renewal density for three point process models (all with nonlinearity f (x) = ex and
history-independent intensity). “True” spikes were generated from (a)  a conditional renewal model
with a gamma renewal density (κ = 10). These responses were ﬁt by: (b)  a Poisson model with the
correct stimulus ﬁlter; and (c)  a modulated renewal process with incorrect stimulus ﬁlter (set to the
negative of the correct ﬁlter)  and renewal density estimated nonparametrically from the transformed
intervals (eq. 10). Middle: Repeated responses from all three models to a novel 1-s stimulus  showing
that spike rate is well predicted by (b) but not by (c). Right: KS plots (above) show time-rescaling
based goodness-of-ﬁt. Here  (b) fails badly  while (c) passes easily  with cdf entirely within within 99%
conﬁdence region (gray lines). Likelihood-based cross-validation tests (below) show that (b) preserves
roughly 1/3 as much information about spike times as (a)  while (c) carries slightly less information
than a homogeneous Poisson process with the correct spike rate.

3 Convexity condition for inhomogeneous renewal models

We now turn to the tractability of estimating the CR model parameters from data. Here  we present
an extension to the results of [21]  which proved a convexity condition for maximum-likelihood
estimation of a conditionally Poisson encoding model (i.e.  generalized linear model). Speciﬁcally 
[21] showed that the log-likelihood for the ﬁlter parameters θ = {k  h} is concave (i.e.  has no non-
global local maxima) if the nonlinear function f is both convex and log-concave (meaning log f is
concave). Under these conditions4  minimizing the negative log-likelihood is a convex optimization
problem.
By extension  we can ask whether the estimation problem remains convex when we relax the Poisson
assumption and allow for a non-exponential renewal density q. Let us write the log-likelihood
function for the linear ﬁlter parameters θ = [kT   hT ]T as

L{D q}(θ) =

log f(X(ti) · θ) +

log q

f(X(t) · θ)dt

 

(8)

n(cid:88)

i

n(cid:88)

(cid:32)(cid:90) ti

i=1

ti−1

(cid:33)

t   yT

t ]T is a vector containing the relevant stimulus and spike history at time t  and

where X(t) = [xT
D = {{ti} {X(t)}} represents the full set of observed data. The condition we obtain is:
Theorem 1. The CR model log-likelihood L{D q}(θ) is concave in the ﬁlter parameters θ  for any
observed data D  if: (1) the nonlinearity f is convex and log-concave; and (2) the renewal density
q is log-concave and non-increasing on (0 ∞].

Proof. It sufﬁces to show that both terms in the equation (8) are concave in θ  since the sum of two
concave functions is concave. The ﬁrst term is obviously concave  since log f is concave. For the

4Allowed nonlinearities must grow monotonically  at least linearly and at most exponentially: e.g.  exp(x);

log(1 + exp(x)); (cid:98)x(cid:99)p  p ≥ 1.

4

stimulus ﬁlternon-parametric1000246052gamma2exponentialISI(rescaled time)50 msrenewal densityrate (Hz)(a)(b)(c)cross-validation 01050time (s)(c)(a/b)rasters(a)(b)(c)01020 bits/s00.5100.51quantilesCDFKS plotsecond term  note that(cid:82) f(X · θ) is a convex function  since it is the integral of a convex function
over a convex region. Then log q[(cid:82) f(X · θ)] is a concave  non-increasing function of a convex

function  since log q is concave and non-increasing; such a function is necessarily concave.5 The
second term is therefore also a sum of concave functions  and thus concave.

Maximum likelihood ﬁlter estimation under the CR model is therefore a convex problem so long as
the renewal density q is both log-concave and non-increasing. This restriction rules out a variety of
renewal densities that are commonly employed to model neural data [13  14  15]. Speciﬁcally  the
log-normal and inverse-Gaussian densities both have increasing regimes on a subset of [0 ∞)  as
does the gamma density q(u) ∝ uκ−1e−uκ when κ > 1. For κ < 1  gamma fails to be log-concave 
meaning that the only gamma density satisfying both conditions is the exponential (κ = 1).
There are nevertheless many densities (besides the exponential) for which these conditions are met 
including

• q(u) ∝ e−up/σ2  for any p ≥ 1
• q(u) = uniform density
• q(u) ∝ (cid:98)f(u)(cid:99)  or q(u) ∝ ef (u)  for any concave  decreasing function f(u)

Unfortunately  no density in this family can exhibit refractory effects  since this would require a q
that is initially zero and then rises. From an estimation standpoint  this suggests that it is easier to
incorporate certain well-known spike-history dependencies using recurrent spike-history ﬁlters (i.e. 
using the GLM framework) than via a non-Poisson renewal density.
An important corollary of this convexity result is that the decoding problem of estimating stimuli
{xt} from a set of observed spike times {ti} using the maximum of the posterior (i.e.  computing
the MAP estimate) is also a convex problem under the same restrictions on f and q  so long as the
prior over stimuli is log-concave.

4 Nonparametric Estimation of the CR model

In practice  we may wish to optimize both the ﬁlter parameters governing the base intensity λ(t) and
the renewal density q  which is not in general a convex problem. We may proceed  however  bearing
in mind that gradient ascent may not achieve the global maximum of the likelihood function.
Here we formulate a slightly different
interval-rescaling function that allows us to non-
parametrically estimate renewal properties using a density on the unit interval. Let us deﬁne the
mapping

(9)
which is the cumulative density function (cdf) for the intervals from a conditionally Poisson process
with cumulative intensity Λ(t). This function maps spikes from a conditionally Poisson process to
i.i.d. samples from U[0  1]. Any discrepancy between the distribution of {vi} and the uniform dis-
tribution represents failures of a Poisson model to correctly describe the renewal statistics. (This is
the central idea underlying time-rescaling based goodness-of-ﬁt test  which we will discuss shortly).
We propose to estimate a density φ(v) for the rescaled intervals {vi} using cubic splines (piecewise
3rd-order polynomials with continuous 2nd derivatives)  with evenly spaced knots on the interval
[0  1].6 This allows us to rewrite the likelihood function (6) as the product of two identiﬁable terms:

vi = 1 − exp(−Λti−1(ti)) 

(cid:32) n(cid:89)

(cid:33)(cid:32) n(cid:89)

(cid:33)

P ({ti}|X) =

λ(ti) e−Λ0(T )

φ(vi)

 

(10)

i=1

i=1

where the ﬁrst term is the likelihood under the conditional Poisson model [17]  and the second is
the probability of the rescaled intervals {vi} under the density φ(v). This formulation allows us to
separate the (real-time) contributions of the intensity function under the assumption of conditionally
5To see this  note that if g is concave (g(cid:48)(cid:48) ≤ 0) and non-increasing (g(cid:48) ≤ 0)  and f is convex (f(cid:48)(cid:48) ≥ 0)  then

dx2 g(f (x)) = g(cid:48)(cid:48)(f (x))f(cid:48)(x)2 + g(cid:48)(f (x))f(cid:48)(cid:48)(x) ≤ 0  implying g(f (x)) is concave.
R 1
6ML estimation of the spline parameters is a convex problem with one linear equality constraint
0 φ(v)dv = 1 and a family of inequality constraints q(v) ≥ 0 ∀v  which can be optimized efﬁciently.

d2

5

1

0

0

1

Figure 3: Left: pairwise dependencies between successive rescaled ISIs from model (“a”  see ﬁg. 2)
when ﬁt by a non-Poisson renewal model “c”. Center: ﬁtted model of the conditional distribution over
rescaled ISIs given the previous ISI  discretized into 7 intervals for the previous ISI. Right: rescaling
the intervals using the cdf (cid:31)  obtained from the conditional (cid:31)(zi+1(cid:124) zi)  produces successive ISIs which
are much more independent. This transformation adds roughly 3 bits/s to the likelihood-based cross-
validation performance of model (c).

Poisson spiking  from the (rescaled-time) contributions of a non-Poisson renewal density. (For a
conditionally Poisson process  (cid:31) is the uniform density on [0(cid:44) 1]  and makes zero contribution to the
total log-likelihood).
We ﬁt this model to simulated data (ﬁg. 2)  and to real neural data using alternating coordinate ascent
of the ﬁlter parameters and the renewal density parameters (ﬁg. 4). In ﬁg. 2  we plot the renewal
distribution (cid:136)q(u) (red trace)  which can be obtained from the estimated (cid:136)(cid:31)(v) via the transformation
(cid:136)q(u) = (cid:136)(cid:31)(1 (cid:31) e(cid:31)u)e(cid:31)u.

4.1

Incorporating dependencies between intervals

The cdf deﬁned by the CR model  (cid:31)(v) =(cid:31) v

so that the
marginal distribution over zi = (cid:31)(vi) is uniform on [0(cid:44) 1]. However  there is no guarantee that the
resulting random variables are independent  as assumed in the likelihood (eq. 10). We can examine
dependencies between successive ISIs by making a scatter plot of pairs (zi(cid:44) zi+1) (see ﬁg. 3). De-
partures from independence can then be modeled by introducing a nonparametric estimator for the
conditional distribution φ(zi(cid:124) zi(cid:31)1). In this case  the likelihood becomes

0 (cid:31)(s)ds  maps the transformed ISIs (cid:123) vi(cid:125)

P ((cid:123) ti(cid:125)

(cid:124) X) =

(cid:29)(ti) e(cid:31)(cid:31)0(T )

(cid:31)(vi)

φ(zi(cid:124) zi(cid:31)1)

(cid:44)

(11)

i=1

i=1

i=2

which now has three terms  corresponding (respectively) to the effects of the base intensity  non-
conditionally Poisson renewal properties  and dependencies between successive intervals.

5 The time-rescaling goodness-of-ﬁt test

If a particular point-process model provides an accurate description of a neuron’s response  then the
cumulative intensity function deﬁnes a mapping from the real time to rescaled-time such that the
rescaled interspike intervals have a common distribution. Time-rescaling can therefore be used as a
tool for assessing the goodness-of-ﬁt of a point process model [1  22]. Speciﬁcally  after remapping
a set of observed spike times according to the (model-deﬁned) cumulative intensity  one can perform
a distributional test (e.g.  Kolmogorov-Smirnov  or KS test) to assess whether the rescaled intervals
have the expected distribution7. For example  for a conditionally Poisson model  the KS test can be
applied to the rescaled intervals (cid:123) vi(cid:125)

(eq. 9) to assess their ﬁt to a uniform distribution.

7Although we have deﬁned the time-rescaling transform using the base intensity instead of the conditional
intensity as in [1]  the resulting tests are equivalent provided the K-S test is applied using the appropriate
distribution.

6

n(cid:29)

n(cid:29)

n(cid:29)

(cid:30)
(cid:28)
(cid:30)
(cid:28)
(cid:30)
(cid:28)
This approach to model validation has grown in popularity in recent years [14  23]  and has in some
instances been used as the only metric for comparing models. We wish to point out that time-
rescaling based tests are sensitive to one kind of error (i.e.  errors in modeling rescaled ISIs)  but
may be insensitive to other kinds of model error (i.e.  errors in modeling the stimulus-dependent
spike rate). Inspection of the CR model likelihood (eq. 10)  makes it clear that time-rescaling based
goodness-of-ﬁt tests are sensitive only to accuracy with which φ(v) (or equivalently  q(u)) models
the rescaled intervals. The test can in fact be independent of the accuracy with which the model
describes the transformation from stimulus to spikes  a point that we illustrate with an (admittedly
contrived) example in ﬁg. 2.
For this example  spikes were genereated from a “true” model (denoted “a”)  a CR model with a
biphasic stimulus ﬁlter and a gamma renewal density (κ = 10). Responses from this model were ﬁt
by two sub-optimal approximate models: “b”  a Poisson (LNP) model  which was speciﬁed to have
the correct stimulus ﬁlter; and “c”  a CR model in which the stimulus ﬁlter was mis-speciﬁed (set
to the negative of the true ﬁlter)  and a renewal density φ(v) was estimated non-parametrically from
the rescaled intervals {vi} (rescaled under the intensity deﬁned by this model).
Although the time-varying spike-rate predictions of model (c) were badly mis-matched to those of
model (a) (ﬁg. 2  middle)  a KS-plot (upper right) shows that (c) exhibits near perfect goodness-of-ﬁt
on a time-rescaling test  which the Poisson model (b) fails badly. We cross-validated these models
by computing the log-likelihood of novel data  which provides a measure of predictive information
about novel spike trains in units of bits/s [24  18]. Using this measure  the “true” model (a) provides
approximately 24 bits/s about the spike response to a novel stimulus. The Poisson model (b) captures
only 8 bits/s  but is still much more accurate than the mis-speciﬁed renewal model (c)  for which
the information is slightly negative (indicating that performance is slightly worse than that of a
homogeneous Poisson process with the correct rate).
Fig. 3 shows that model (c) can be improved by modeling the dependencies between successive
rescaled interspike intervals. We constructed a spline-based non-parametric estimate of the density
π(zi+1|zi)  where zi = Φ(vi). (We discretized zi into 7 bins  based on visual inspection of the pair-
wise dependency structure  and ﬁt a cubic spline with 10 evenly spaced knots on [0 1] to the density
within each bin). Rescaling these intervals using the cdf of the augmented model yields intervals
that are both uniform on [0  1] and approximately independent (ﬁg. 3  right; independence for non-
successive intervals not shown). The augmented model raises the cross-validation score of model (c)
to 1 bit/s  meaning that by incorporating dependencies between intervals  the model carries slightly
more predictive information than a homogeneous Poisson model  despite the mis-speciﬁed stimu-
lus ﬁlter. However  this model—despite passing time-rescaling tests of both marginal distribution
and independence—still carries less information about spike times than the inhomogeneous Poisson
model (b).

6 Application to neural data

Figure 4 shows several speciﬁc cases of the CR model ﬁt to spiking data from an ON parasol cell in
primate retina  which was visually stimulated with binary spatio-temporal white noise (i.e.  ﬂicker-
ing checkerboard  [18]). We ﬁt parameters for the CR model with and without spike-history ﬁlters 
and with and without a non-Poisson renewal density (estimated non-parametrically as described
above).
As expected  a non-parametric renewal density allows for remapping of ISIs to the correct (uniform)
marginal distribution in rescaled time (ﬁg. 4  left)  and leads to near-perfect scores on the time-
rescaling goodness-of-ﬁt test (middle). Even when incorporating spike-history ﬁlters  the model
with conditionally Poisson spiking (red) fails the time-rescaling test at the 95% level  though not so
badly as the the inhomogeneous Poisson model (blue). However  the conditional Poisson model with
spike-history ﬁlter (red) outperforms the non-parametric renewal model without spike-history ﬁlter
(dark gray) on likelihood-based cross-validation  carrying 14% more predictive information. For
this neuron  incorporating non-Poisson renewal properties into a model with spike history dependent
intensity (light gray) provides only a modest (<1%) increase in cross-validation performance. Thus 
in addition to being more tractable for estimation  it appears that the generalized linear modeling
framework captures spike-train dependencies more accurately than a non-Poisson renewal process
(at least for this neuron). We are in the process of applying this analysis to more data.

7

Figure 4: Evaluation of four speciﬁc cases of the conditional renewal model  ﬁt to spike responses
from a retinal ganglion cell stimulated with a time-varying white noise stimulus. Left: marginal dis-
tribution over the interspike intervals {zi}  rescaled according to their cdf deﬁned under four different
models: (a) Inhomogeneous Poisson (i.e.  LNP) model  without spike-history ﬁlter. (b) Conditional
renewal model without spike-history ﬁlter  with non-parametrically estimated renewal density φ. (c)
Conditional Poison model  with spike-history ﬁlter (GLM). (d) Conditional renewal model with spike-
history ﬁlter and non-parametrically estimated renewal density. A uniform distribution indicates good
model ﬁt under the time-rescaling test. Middle: The difference between the empirical cdf of the
rescaled intervals (under all four models) and their quantiles. As expected  (a) fares poorly  (c) per-
forms better but slightly exceeds the 95% conﬁdence interval (black lines)  and (b) and (d) exhibit
near-perfect time-rescaling properties. Right: Likelihood-based cross-validation performance. Adding
a non-parametric renewal density adds 4% to the Poisson model performance  but <1% to the GLM
performance. Overall  a spike-history ﬁlter improves cross-validation performance more than the use
of non-Poisson renewal process.

7 Discussion

We have connected two basic approaches for incorporating spike-history effects into neural encod-
ing models: (1) non-Poisson renewal processes; and (2) conditionally Poisson processes with an
intensity that depends on spike train history. We have shown that both kinds of effects can be re-
garded as special cases of a conditional renewal (CR) process model  and have formulated the model
likelihood in a manner that separates the contributions from these two kinds of mechanisms.
Additionally  we have derived a condition on the CR model renewal density under which the likeli-
hood function over ﬁlter parameters is log-concave  guaranteeing that ML estimation of ﬁlters (and
MAP stimulus decoding) is a convex optimization problem.
We have shown that incorporating a non-parametric estimate of the CR model renewal density en-
sures near-perfect performance on the time-rescaling goodness-of-ﬁt test  even when the model itself
has little predictive accuracy (e.g.  due to a poor model of the base intensity). Thus  we would argue
that K-S tests based on the time-rescaled interspike intervals should not be used in isolation  but
rather in conjunction with other tools for model comparison (e.g.  cross-validated log-likelihood).
Failure under the time-rescaling test indicates that model performance may be improved by incor-
porating a non-Poisson renewal density  which as we have shown  may be estimated directly from
rescaled intervals.
Finally  we have applied the CR model to neural data  and shown that it can capture spike-history
dependencies in both real and rescaled time. In future work  we will examine larger datasets and
explore whether rescaled-time or real-time models provide more accurate descriptions of the depen-
dencies in spike trains from a wider variety of neural datasets.

Acknowledgments

Thanks to E. J. Chichilnisky  A. M. Litke  A. Sher and J. Shlens for retinal data  and to J. Shlens and
L. Paninski for helpful discussions.

8

01stimulus only stimulus + spike-historyconditionalPoissonconditionalrenewalzP(z)bcdabits/smodelabcdcross-validatedlog-likelihood01020CDF - quantileKS statistic010quantileReferences
[1] E. Brown  R. Barbieri  V. Ventura  R. Kass  and L. Frank. The time-rescaling theorem and its application

to neural spike train data analysis. Neural Computation  14:325–346  2002.

[2] E. J. Chichilnisky. A simple white noise analysis of neuronal light responses. Network: Computation in

Neural Systems  12:199–213  2001.

[3] E. P. Simoncelli  L. Paninski  J. W. Pillow  and O. Schwartz. Characterization of neural responses with
stochastic stimuli. In M. Gazzaniga  editor  The Cognitive Neurosciences  III  chapter 23  pages 327–338.
MIT Press  2004.

[4] M. Berry and M. Meister. Refractoriness and neural precision. Journal of Neuroscience  18:2200–2211 

1998.

[5] Daniel S. Reich  Ferenc Mechler  Keith P. Purpura  and Jonathan D. Victor. Interspike intervals  receptive

ﬁelds  and information encoding in primary visual cortex. J. Neurosci.  20(5):1964–1974  2000.

[6] N. Brenner  W. Bialek  and R. de Ruyter van Steveninck. Adaptive rescaling optimizes information

transmission. Neuron  26:695–702  2000.

[7] W. Gerstner. Population dynamics of spiking neurons: Fast transients  asynchronous states  and locking.

Neural Computation  12(1):43–89  2000.

[8] P. Reinagel and R. C. Reid. Temporal coding of visual information in the thalamus. Journal of Neuro-

science  20:5392–5400  2000.

[9] J. W. Pillow  L. Paninski  V. J. Uzzell  E. P. Simoncelli  and E. J. Chichilnisky. Prediction and decod-
ing of retinal ganglion cell responses with a probabilistic spiking model. The Journal of Neuroscience 
25:11003–11013  2005.

[10] M.A. Montemurro  S. Panzeri  M. Maravall  A. Alenda  M.R. Bale  M. Brambilla  and R.S. Petersen.
Role of precise spike timing in coding of dynamic vibrissa stimuli in somatosensory thalamus. Journal of
Neurophysiology  98(4):1871  2007.

[11] A.L. Jacobs  G. Fridman  R.M. Douglas  N.M. Alam  P. Latham  et al. Ruling out and ruling in neural

codes. Proceedings of the National Academy of Sciences  106(14):5936  2009.

[12] M. Berman. Inhomogeneous and modulated gamma processes. Biometrika  68(1):143–152  1981.
[13] R. Barbieri  M.C. Quirk  L.M. Frank  M.A. Wilson  and E.N. Brown. Construction and analysis of
non-poisson stimulus-response models of neural spiking activity. Journal of Neuroscience Methods 
105(1):25–37  2001.

[14] E. Rossoni and J. Feng. A nonparametric approach to extract information from interspike interval data.

Journal of neuroscience methods  150(1):30–40  2006.

[15] K. Koepsell and F.T. Sommer. Information transmission in oscillatory neural activity. Biological Cyber-

netics  99(4):403–416  2008.

[16] R.E. Kass and V. Ventura. A spike-train probability model. Neural computation  13(8):1713–1720  2001.
[17] W. Truccolo  U. T. Eden  M. R. Fellows  J. P. Donoghue  and E. N. Brown. A point process framework
for relating neural spiking activity to spiking history  neural ensemble and extrinsic covariate effects. J.
Neurophysiol  93(2):1074–1089  2005.

[18] J. W. Pillow  J. Shlens  L. Paninski  A. Sher  A. M. Litke  and E. P. Chichilnisky  E. J. Simoncelli. Spatio-
temporal correlations and visual signaling in a complete neuronal population. Nature  454:995–999 
2008.

[19] S. Gerwinn  J.H. Macke  M. Seeger  and M. Bethge. Bayesian inference for spiking neuron models with

a sparsity prior. Advances in Neural Information Processing Systems  2008.

[20] I.H. Stevenson  J.M. Rebesco  L.E. Miller  and K.P. K

”ording. Inferring functional connections between neurons. Current Opinion in Neurobiology  18(6):582–
588  2008.

[21] L. Paninski. Maximum likelihood estimation of cascade point-process neural encoding models. Network:

Computation in Neural Systems  15:243–262  2004.

[22] J. W. Pillow. Likelihood-based approaches to modeling the neural code. In K. Doya  S. Ishii  A. Pouget 
and R. P. Rao  editors  Bayesian Brain: Probabilistic Approaches to Neural Coding  pages 53–70. MIT
Press  2007.

[23] T.P. Coleman and S. Sarma. Using convex optimization for nonparametric statistical analysis of point
processes. In IEEE International Symposium on Information Theory  2007. ISIT 2007  pages 1476–1480 
2007.

[24] L. Paninski  M. Fellows  S. Shoham  N. Hatsopoulos  and J. Donoghue. Superlinear population encoding

of dynamic hand trajectory in primary motor cortex. J. Neurosci.  24:8551–8561  2004.

9

,Ahmed Alaoui
Michael Mahoney