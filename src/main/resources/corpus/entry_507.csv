2018,An Efficient Pruning Algorithm for Robust Isotonic Regression,We study a generalization of the classic isotonic regression problem  where we allow separable nonconvex objective functions  focusing on the case of estimators used in robust regression. A simple dynamic programming approach allows us to solve this problem to within ε-accuracy (of the global minimum) in time linear in 1/ε and the dimension. We can combine techniques from the convex case with branch-and-bound ideas to form a new algorithm for this problem that naturally exploits the shape of the objective function. Our algorithm achieves the best bounds for both the general nonconvex and convex case (linear in log (1/ε))  while performing much faster in practice than a straightforward dynamic programming approach  especially as the desired accuracy increases.,An Efﬁcient Pruning Algorithm for Robust Isotonic

Regression

School of Industrial Systems and Engineering

Cong Han Lim ∗

Georgia Tech

Altanta  GA 30332

clim31@gatech.edu

Abstract

We study a generalization of the classic isotonic regression problem where we allow
separable nonconvex objective functions  focusing on the case where the functions
are estimators used in robust regression. One can solve this problem to within
-accuracy (of the global minimum) in O(n/) using a simple dynamic program 
and the complexity of this approach is independent of the underlying functions. We
introduce an algorithm that combines techniques from the convex case with branch-
and-bound ideas that is able to exploit the shape of the functions. Our algorithm
achieves the best known bounds for both the convex case (O(n log(1/))) and the
general nonconvex case. Experiments show that this algorithm can perform much
faster than the dynamic programming approach on robust estimators  especially as
the desired accuracy increases.

1

Introduction

(cid:88)

i∈[n]

In this paper we study the following optimization problem with monotonicity constraints:

minx∈[0 1]n

fi(xi) where xi ≤ xi+1 for i ∈ [n − 1]

(1)
where the functions f1  f2  . . .   fn : [0  1] → R may be nonconvex and the notation [n] denotes the
set {1  2  . . .   n}. Our goal is to develop an algorithm that achieves an objective -close to the global
optimal value for any  > 0 with a complexity that scales along with the properties of f. In particular 
we present an algorithm that simultaneously achieves the best known bounds when fi are convex and
also for general fi  while scaling much better in practice than the straightforward approach when
considering f used in robust estimation such as Huber Loss  Tukey’s biweight function  and MCP.
Problem (1) is a generalization of the classic isotonic regression problem (Brunk  1955; Ayer et al. 
1955). The goal there to ﬁnd the best isotonic ﬁt in terms of Euclidean distance to a given set of points
y1  y2  . . .   yn. This corresponds to setting each fi(x) to (cid:107)xi − yi(cid:107)2
2. Besides having applications
in domains where such a monotonicity assumption is reasonable  isotonic regression also appears
as a key step in other statistical and optimization problems such as learning generalized linear and
single index models (Kalai and Sastry  2009)  submodular optimization (Bach  2013)  sparse recovery
(Bogdan et al.  2013; Zeng and Figueiredo  2014)  and ranking problems (Gunasekar et al.  2016).
There are several reasons to go beyond Euclidean distance and to consider more general fi functions.
For example  using the appropriate Bregman divergence can lead to better regret bounds for certain
online learning problems over the convex hull of all rankings (Yasutake et al.  2011; Suehiro et al. 
2012)  and allowing general fi functions has applications in computer vision (Hochbaum  2001;

∗Work done while at Wisconsin Institute for Discovery  University of Wisconsin-Madison.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Kolmogorov et al.  2016). In this paper we will focus on the use of quasiconvex distance functions 
the use of which is much more robust to outliers (Bach  2018)2. Figure 1 describes this in more detail.

Figure 1:
Isotonic regression in the presence of outliers. The left image shows the value of the Euclidean
distance and Tukey’s biweight function (a canonical function for robust estimation) from x = −1 to 1  the
middle image demonstrates isotonic regression on a simple linear and noiseless example  and the right image
shows how outliers can adversely affect isotonic regression under Euclidean distance.

For general fi functions we cannot solve Problem (1) exactly (without some strong additional
assumptions)  and instead we focus on the problem

fi(xi) where xi ≤ xi+1 for i ∈ [n − 1]

(2)

(cid:88)

minx∈Gn

k

i∈[n]

where instead of allowing the xi values to lie anywhere in the interval [0  1]  we restrict them to
Gk := {0  1/k  2/k  . . .   1}  a equally-spaced grid of k + 1 points. This discretized version of the
problem will give a feasible solution to the original problem that is close to optimal. The relation
between the granularity of the grid and approximation quality for any optimization problem over a
bounded domain can be described in terms of the Lipschitz constants of the objective function  and
for this particular problem has been described in Bach (2015  2018) — if functions fi are Lipschitz
continuous with constant L  then to obtain a precision of  in terms of the objective value  it sufﬁces to
choose k ≥ 2nL/. One can achieve better bounds using higher-order Lipschitz constants. The main
approach for solving Problem (2) for general nonconvex functions is to use dynamic programming
(see for example Felzenszwalb and Huttenlocher (2012)) that runs in O(nk). When all the fi are
convex  one can instead use the faster O(n log k)) scaling algorithm by Ahuja and Orlin (2001).
Our main contribution is an algorithm that also achieves O(nk) in the general case and O(n log k) in
the convex case by exploiting the following key fact — the dynamic programming method always runs
in time linear in the sum of possible xi values over all xi. Thus  our goal is to limit the range of values
by using properties of the fi functions. This is done by combining ideas from branch-and-bound and
the scaling algorithm by Ahuja and Orlin (2001) with the dynamic programming approach. When
restricted to convex fi functions  our algorithm is very similar to the scaling algorithm.
Our algorithm works by solving the problem over increasingly ﬁner domains  choosing not to include
points that will not get us closer to the global optimum. We use two ways to exclude points  the
ﬁrst of which uses lower bounds over intervals for each fi  and the second requires us to be able to
compute a linear underestimate of fi over an interval efﬁciently. This information is readily available
for a variety of quasiconvex distance functions  and we provide an example of how to compute this
for Tukey’s biweight function. In practice  this leads to an algorithm that can require far less function
evaluations to achieve the same accuracy as dynamic programming  which in turn translates into a
faster running time even considering the additional work needed to process each point.
The paper is organized as follows. For the rest of the introduction  we will survey other methods
for isotonic regression for speciﬁc classes of sets of fi functions and also mention related problems.
Section 2 describes the standard dynamic programming approach. In Section 3  we describe our main
pruning algorithm and the key pruning rules for removing xi values that we need to consider. Section
4 demonstrates the performance of the algorithm on a series of experiments. The longer version of
this paper (provided as supplementary material) includes proofs for the linear underestimation rule
and also brieﬂy discusses a heuristic variant of our main algorithm.

2 Our focus in this paper is on developing algorithms for global optimization. For more on robust estimators 

we refer the reader to textbooks by Huber (2004); Hampel et al. (2011).

2

(cid:80)

Existing methods for isotonic regression. We will ﬁrst discuss the main methods for exactly
solving Problem (1) and the classes of functions the methods can handle. For convex fi functions 
the pool-adjacent-violators (PAV) algorithm (Ayer et al.  1955; Brunk  1955) has been the de facto
method for solving the problem. The algorithm was originally developed for the Euclidean distance
case but in fact works for any set of convex fi  provided that one can exactly solve intermediate
subproblems of the form arg minz∈[0 1]
i∈S fi(z) (Best et al.  2000) over subsets S of [n]. PAV
requires solving up to n such subproblems  and the total cost of solving can be just O(n) for a wide
range of functions  including for many Bregman divergences (Lim and Wright  2016).
There are algorithms for nonconvex functions that are piecewise convex. Let q denote the total number
of pieces over all the fi functions. In the case where the overall functions are convex  piecewise-
linear and -quadratic functions can be handled in O(q log log n) and O(q log n) time respectively
(Kolmogorov et al.  2016; Hochbaum and Lu  2017)  while in the nonconvex case it is O(nq).
In some cases  we cannot solve the problem exactly and instead deal with the discretized problem (2).
For example  this is the case when our knowledge to the functions fi can only be obtained through
function evaluation queries (i.e. xi → fi(xi)). In the convex case  PAV can be used to obtain an
algorithm with O(n2 log k) time to solve the problem over a grid of k points  but a clever recursive
algorithm by Ahuja and Orlin (2001) takes only O(n log k). A general approach that works for
arbitrary functions is dynamic programming  which has a complexity of O(nk).
Bach (2015) recently proposed a framework for optimizing continuous submodular functions that
can be applied to solving such functions over monotonicity constraints. This includes separable
nonconvex functions as a special case. Although the method is a lot more versatile  when specialized
to our setting it results in an algorithm with a complexity of O(n2k2). This and dynamic programming
are the main known methods for general nonconvex functions.

One can also replace the ordering constraints with the pairwise terms(cid:80)

Related Problems. There have been many extensions and variants of the classic isotonic regression
problem  and we will brieﬂy describe two of them. One common extension is to use a partial ordering
instead of a full ordering. This signiﬁcantly increases the difﬁculty of the problem  and this problem
can be solved by recursively solving network ﬂow problems. For a detailed survey of this area  which
considers different types of partial orderings and (cid:96)p functions  we refer the reader to Stout (2014).
i∈[n−1] gi(xi+1 − xi) where
gi : R → R ∪ {∞}. By choosing gi appropriately  we recover many known variants of isotonic
regression  including nearly-isotonic regression (Tibshirani et al.  2011)  smoothed isotonic regression
(Sysoev and Burdakov  2016; Burdakov and Sysoev  2017)  and a variety of problems from computer
vision. The most general recent work (involving piecewise linear functions) is by Hochbaum and Lu
(2017). We note that the works by Bach (2015  2018) also applies in many of these settings.

2 Dynamic Programming

We now provide a DP reformulation of Problem (2). Let AGk
we can deﬁne the following functions:
Gk
i (xi) 

A

Gk
i (xi) := fi(xi) + C
Gk
i (xi) := minxi+1∈Gk A

C

i+1(xi+1) where xi ≤ xi+1.
Gk

n (xn) := fn(xn). For any i ∈ [n − 1] 

(aggregate)
(min-convolution)

Gk
i

Gk
i

functions aggregate the accumulated information from the indices i + 1  i + 2  . . .   n with
functions represent the minimum-convolution of
i+1 function with the indicator function g where g(z) = 0 if z ≤ 0  and g = ∞ otherwise. With
Gk
Gk
1 (x1) has the same objective and x1 value as Problem (2).

The A
the information at the current index i  where the C
the A
this notation  the problem minx1∈Gk A
We can use the above recursion to solve the problem  which we formally describe in Algorithm 1.
This dynamic programming algorithm can be viewed an application of the Viterbi algorithm. The
Gk
algorithm does a backward pass  building up all the A
values from i = n to i = 1. Once A
1
has been computed  we know the minimizer x1. We then work our way forwards  each time picking
i on the grid Gk subject to the condition that xi ≥ xi−1. The total running
Gk
an xi that minimizes A
time of this algorithm is O(nk)  on the order of the number of points in the grid.

Gk
  C
i

Gk
i

3

Algorithm 1 Dynamic Program for ﬁxed grid Gk

input: Functions {fi}  Parameter k
n (z) ← fn(z) for z ∈ Gk
AGk
for i = n − 1  . . .   1 do
i (1) ← A
Gk
Gk
i+1(1)
C
i (1) ← fi(1) + C
Gk
Gk
i (1)
A
for z = k−1/k  k−2/k  . . .   0 do
i (z) ← min(A
Gk
Gk
i+1(z)  C
C
i (z) ← fi(z) + C
Gk
Gk
i (z)
A
x0 ← 0
for i = 1  2 . . .   n do

xi ← arg minz∈Gk z≥xi−1 A

return (x1  x2  . . .   xn)

Gk
i (z)

Gk
i (z + 1/k))

(cid:46) Backwards Pass

(cid:46) Forward Pass

The main drawback of the dynamic programming approach is that it requires us to pick the desired
accuracy a priori via choosing an appropriate k value and then overall running time is then O(nk) 
no matter the properties of the fi functions.

3 A Pruning Algorithm for Robust Isotonic Regression

Instead of solving the full discretized problem (2) directly  we can work over a much smaller set of
points. Let xGk denote an optimal solution to the problem  and for each i ∈ [n] let Si ⊆ Gk denote a
set of points such that x

(cid:88)
i ∈ Si. Then
Gk

minx∈S1×...Sn

i∈[n]

fi(xi) where xi ≤ xi+1 for i ∈ [n − 1] 

has the same solution xGk and it is easy to modify the DP algorithm to work for this problem. All
that is needed is to perform the following replacements:

• z = . . . and z ∈ . . . with the appropriate series of points in Si 
• C
i+1(z) for zmax = arg max(Si)  and
Gk
• C

i (zmax) ← minz≥zmax A
Gk
i (z) ← min(A
Gk
Gk
  A
i

i+1(z(cid:48))) where z(cid:48) ≥ z.
Gk
functions are the same for both problem formulations on xGk.

Gk
The values of the C
i
The modiﬁed operations can be performed efﬁciently by maintaining the appropriate minimum values 
and this results in an algorithm with a complexity of just O(|S1| + . . .|Sn|). Our goal is thus to
restrict the size of Si sets. We perform this by starting from a coarse set of intervals Ii for each
index i that initially contains just [0  1]. This contains all points in Gk. We repeatedly subdivide each
interval into two and keep only the intervals that may contain certain better solutions  which in turn
reduces the number of points in Gk that are contained in some interval.
From here on we assume that k is a power of 2. Algorithm 2 describes the basic framework which
we build on throughout this section.

Algorithm 2 Algorithmic Framework for Faster Robust Isotonic Regression

input: Functions {fi}  Parameter k
k(cid:48) ← 1
Ii ← {[0  1]} for i ∈ [n]
while k(cid:48) < k do

(cid:8)I
(cid:9) using {fi}
x ← run modiﬁed DP on endpoints of(cid:8)IGk

(cid:9) ← Reﬁne(cid:8)IGk(cid:48)

k(cid:48) ← 2k(cid:48)

G2k(cid:48)
i

1

i

return x

(cid:9)

4

At the end of each round of the loop  we want xGk be contained in I1 × . . . × In where Ii is some
interval from Ii. This ensures that we ﬁnd the optimal point in the ﬁnal grid Gk. We also want IGk
to
consist only of intervals of width 1/k(cid:48) with endpoints contained in Gk(cid:48). This ensures that the overall
number of points processed over all iterations is at most O(nk)  and by bounding the number of
intervals in each Ii in each iteration we can achieve signiﬁcantly better performance. In particular 
the scaling algorithm for convex functions by Ahuja and Orlin (2001) can be seen as a particular
realization of this framework where the reﬁnement process keeps the size of each Ii to exactly one.
In the rest of this section  we will describe two efﬁcient rules for reﬁning the sets of intervals {Ii}
and analyze the complexity of the overall algorithm. The ﬁrst rule uses lower and upper bounds (akin
to standard branch-and-bound)  while the second requires one to be able to efﬁciently construct good
linear underestimators of the fi functions within intervals.

i

3.1 Pruning via lower/upper bounds

i

n

min

This pruning rule constructs lower bounds over the current active intervals  then uses upper bounds
(that can be obtained via the aforementioned DP) to decide which intervals can be removed from
consideration in subsequent iterations of the algorithm.
We again modify the dynamic program  this time to compute lower bounds over intervals. Let
ALB Gk

(a) := minxn∈[a a+1/2k] fn(xn) and recursively deﬁne the following:
ALB Gk
C LB Gk

(a) :=
(a) := mina(cid:48)∈Gk ALB Gk
It is straightforward to see that ALB Gk
j=i fj(xj) when xi is contained in
the interval [a  a + 1/2k]. This dynamic program can be computed in O(|I1| + . . .|In|) time using the
same ideas as before  provided that terms of the form minxi∈[a b] fi(xi) can be efﬁciently calculated.
As for which intervals to keep  we remove an interval [a  b] from Ii if there is another interval in Ii
which can be used in place of [a  b] and the upper bound from using the other interval is smaller than
the lower bound corresponding to [a  b]. This concept is formalized in Algorithm 3.

(min-convolution for lower bound)

(a(cid:48)) where a ≤ a(cid:48).

(a) is a lower bound for(cid:80)n

(aggregate for lower bound)

fi(xi) + C LB Gk

xi∈[a a+1/2k]

(a) 

i+1

i

i

i

Algorithm 3 Pruning I via Lower/Upper Bounds

(cid:9) and(cid:8)ALB Gk(cid:48)

Compute(cid:8)A

input: Interval Sets {IGk(cid:48)
Z ← 0
for i = 1  . . .   n do

Gk(cid:48)
i

i

i

}  functions {fi}  Parameter k(cid:48)

(cid:9) using {fi}

z ← ﬁrst element in Z sequence
z(cid:48) ← next element (1 if there are none)
J ← ∅
while z (cid:54)= 1 do

u ← min(cid:8)A
J ← J ∪(cid:8)[a  b] ∈ IGk(cid:48)

(xi) (cid:12)(cid:12) xi ∈ Gk(cid:48) ∩ [z  z(cid:48)](cid:9)

(cid:12)(cid:12) ALB Gk(cid:48)

Gk(cid:48)
i

(a) ≤ u  [a  b] ⊆ [z  z(cid:48)](cid:9)

i

i

z ← z(cid:48)
z(cid:48) ← next element in sequence Z (1 if there are none)
IGk(cid:48)
i ← J
Z ← all endpoints in J
return {IGk(cid:48)
}

i

We can show that this procedure does not remove certain solutions  including the optimal solutions to
Problems (1) and (2). Deﬁnition 3.1 and Proposition 3.2 describes this more precisely.
Deﬁnition 3.1. Given a nondecreasing vector x ∈ Rn  x is S-improvable for some S ⊆ [0  1] if there
i∈[n] fi(xi) and if yi /∈ S it

is a different nondecreasing vector y ∈ Rn such that(cid:80)

i∈[n] fi(yi) <(cid:80)

must be the case that yi = xi.

5

Note that the optimal solution xGk is not Gk(cid:48)-improvable for any k(cid:48) that is a factor of k.
Proposition 3.2. Let x∗ be a nondecreasing vector which is not Gk(cid:48)-improvable. Suppose x∗ is in

(cid:16)(cid:91)(cid:8)[a  b] ∈ IGk(cid:48)

(cid:9)(cid:17)

.

i

(cid:89)

i∈[n]

This remains true after applying Algorithm 3 to the sets {IGk(cid:48)

i

}.

i · (a− b) ≤ fi(z) for a ≤ z < b and fi(b) + gR

3.2 Pruning via linear underestimators
We now describe a rule that uses linear underestimators on intervals in Ii. In the convex case  one
can think of this as using subgradient information. This is what the scaling algorithm of Ahuja and
Orlin (2001) uses to obtain a complexity of O(n log k). We will rely on the following assumption.
Assumption 3.3. Given a  b  c ∈ [0  1] where a < b < c  we can compute in constant time gL
i ∈
i   gR
i · (c− b) ≤ fi(z) for b < z ≤ c.
R such that fi(b) + gL
that satisﬁes the condition  but the tighter the underestimator 
This pruning rule works with any gL
the better our algorithm will perform. In particular  it is ideal to minimize gL
i . For
convex functions  the best possible gR
i
Suppose we have the interval [u  v] ∈ IGk(cid:48)
for i ∈ {s  s + 1  . . .   t}. Our goal is to decide for each i
if we should include the intervals [u  (u+v)/2] and [(u+v)/2  v] in IG2k(cid:48)
. We can do this by taking into
account linear underestimators for fi in each of these two intervals and also by considering which xi
may lie outside of [u  v]. Algorithm 4 describes how this can be done.

is a subgradient of the function.

i and maximize gR

i   gR
i

i

i

Algorithm 4 Pruning Subroutine

i   gR

i (from Assumption 3.3) for i ∈ [n]

input: {fi}  {s  s + 1  . . .   t}  a  b  c ∈ [0  1] where a < b < c  indices l  r
Compute gL
t ← gL
SL
i ← gL
t
SL
i + max(SL
I L ← {i | i ≤ l  SL
s ← gR
SR
i ← gR
s
SR
i + min(SR
I R ← {i | i ≥ r  SR
for i = s  s + 1  . . .   t do

i+1  0) for i ∈ {s  s + 1  . . .   t − 1}
i > 0} ∪ {i | l + 1 ≤ i < k  k is ﬁrst index after l where SL
i−1  0) for i ∈ {s + 1  . . .   t}
i ≤ 0} ∪ {i | k > i ≥ r + 1  k is last index before r where SR

k ≤ 0}

k > 0}

Ii ← ∅
if i ∈ I L  add [a  b] to Ii
if i ∈ I R or Ii is empty  add [b  c] to Ii

return {Ii}

s and x∗

Theorem 3.4. Consider Algorithm 4 and its inputs. Suppose that there is some nondecreasing vector
x∗ ∈ [0  1]n such that x∗ is not {b}-improvable. Let s  t denote the indices where x∗
t are the
ﬁrst and terms of x∗ contained in [a  c] respectively. Suppose l ≥ s − 1 and r ≤ t + 1. For any
i ∈ {s  s + 1  . . .   t}  the term x∗
i is contained in one of the intervals in Ii returned by the algorithm.
We use Algorithm 4 as part of a larger procedure over the entire collection of interval sets I1  . . .  In.
This procedure is detailed in Algorithm 5  and reﬁnes the set of intervals by splitting each interval
into two and running Algorithm 4 on the pair of adjacent intervals.
Proposition 3.5. Suppose the intervals used as inputs to Algorithm 5 are {IGk(cid:48)
} (i.e. all the
endpoints are in Gk(cid:48)). Let x∗ ∈ [0  1]n be a nondecreasing vector that is not G2k(cid:48)-improvable and

is contained in(cid:81)
where(cid:8)IG2k(cid:48)(cid:9) are the intervals returned by the algorithm.

(cid:16)(cid:83)(cid:8)[a  b] ∈ IGk(cid:48)

. Then  x∗ is contained in(cid:81)

(cid:16)(cid:83)(cid:8)[a  b] ∈ IG2k(cid:48)

(cid:9)(cid:17)

(cid:9)(cid:17)

i∈[n]

i∈[n]

i

i

i

3.3 Computing Lower Bounds and Linear Underestimators for Quasiconvex Estimators

For quasiconvex functions  we can compute the lower bound over an interval [a  b] by just evaluating
the function on the endpoints a and b (and by knowing what the minimizer and minimum value are).

6

Algorithm 5 Main Algorithm for Reﬁning via Linear Underestimators

}  functions {fi}

for [u  v] ∈(cid:83)

input: Interval Sets {IGk(cid:48)
i ← ∅ for i ∈ [n]
I(cid:48)
i IGk(cid:48)
for each contiguous block of indices s  s + 1  . . .   t in {i | Ii contains [u  v]} do

do

i

i

l ← max{i | ∃ an interval to the left of [u  v] contained in IGk(cid:48)
}
r ← min{i | ∃ an interval to the right of [u  v] contained in IGk(cid:48)
Update {I(cid:48)

i} with Alg. 4 with inputs {fi} {s  . . .   t}  (a  b  c) = (u  u+v/2  v)  indices l  r

}

i

i

i}
return {I(cid:48)

It is straightforward to compute good linear underestimators for many quasiconvex distance functions
used in robust statistics. We will discuss how this can be done for the Tukey biweight function  and
similar steps can be taken for other popular functions such as the Huber Loss  SCAD  and MCP.

Example: Tukey’s biweight function and how to efﬁciently compute good m values. Tukey’s
biweight function is a classic function used in robust regression. The function is zero at the origin
and the derivative is x(1 − (x/c)2)2 for |x| < c and 0 otherwise for some ﬁxed c.

√
√
Figure 2: Tukey’s biweight function with c = 1. In the plot of the derivative  we mark the region in which the
function is convex in red (−1/
5)  while in the other regions at the sides the function is concave.

5 ≤ x ≤ 1/

We will describe how to choose gL and gR for x < 0  and by symmetry we can use similar methods
for x > 0. We obtain gL from connecting f (x) to the largest value of the function. If x is in the
convex region  we can simply set gR to the gradient. We now add a line with slope −L (where L is
the largest gradient of the function) to the transition point between the concave and convex regions 
and for x in the concave region we obtain gR by connecting f (x) to this line.

3.4 Putting it all together

After stating the pruning and reﬁnement rules for our nonconvex distance functions  we can formally
describe in detail the full process in Algorithm 6. The worse case running time is O(nk)  since the
number of points and intervals processed is on that order and the complexity of the subroutines are
linear in those numbers. On the other hand  when the functions fi are convex 
Theorem 3.6. Algorithm 6 solves Problem (2) in O(nk) time in general  and O(n log k) time for
convex functions if we use subgradient information.

There are two things to note about Algorithm 6. First  it only presents one possible combination of
the pruning rules. Another combination would be to not apply the lower/upper bound pruning rule at
every iteration. We stick to this particular description in our experiments and theorems for simplicity.
Second  we only require the linear underestimator rule for the O(n log k) convex bound  since that
sufﬁces to ensure that sets Si have at most a few points.

7

Algorithm 6 A Pruning Algorithm for Robust Isotonic Regression

(cid:9)

Gk(cid:48)
i

G2k(cid:48)
i

i

input: Functions {fi}  Parameter k
k(cid:48) ← 1
Si ← {0  1} for i ∈ [n]
Ii ← {[0  1]} for i ∈ [n]
while k(cid:48) < k do

(cid:8)I
(cid:9) ← Algorithm 5 to reﬁne and prune(cid:8)I
(cid:8)I
(cid:9) ← Algorithm 3 to prune(cid:8)I
(cid:9)
(cid:9)
x ← run modiﬁed DP on endpoints of(cid:8)IGk

k(cid:48) ← 2k(cid:48)

G2k(cid:48)
i
G2k(cid:48)
i

return x

4 Empirical Observations

We evaluate the efﬁciency of the DP approach and our algorithm on a simple isotonic regression task.
We adopt an experiment setup similar to the one used by Bach (2018). We generate a series of n
points y1  . . .   yn from 0.2 to 0.8 equally spaced out and added Gaussian random noise with standard
deviation of 0.03. We then randomly ﬂipped between 5% to 50% of the points around 0.5  and these
points act as the outliers. Our goal now is to test the computational effort required to solve Problem
(2). where f is the Tukey’s biweight function with c = 0.3. We set n to 1000 and varied k from
27 = 128 to 216 = 65536.

Figure 3: The yi points (pluses) and results of using Euclidean distance (blue  dashed) vs. Tukey’s biweight
function (orange  solid).

We used two metrics to evaluate the computational efﬁciency. The ﬁrst measure we use is the total
number of points in all Si across all iterations  an implementation-independent measure. The second
is the wall-clock time taken. The algorithms were implemented in Python 3.6.7  and the experiments
were ran on an Intel 7th generation core i5-7200U dual-core processor with 8GB of RAM.
The results are summarized in Figure 4  where the results are averaged over 10 independent trials. In
the ﬁrst ﬁgure on the left  we see how the error decreases with an increase in k  reﬂecting the equation
that k ≥ O(1/) is needed to achieve an error of  in the objective.
In the second and third ﬁgures  we compare the performance of the dynamic program against our
method  with different percentages of points ﬂipped/corrupted. Instead of presenting three DP lines
for each percentage  we simply use one line since the number of points evaluated is always the same
and the variation in the timing across all runs is signiﬁcantly less than 5 percent for all values of
k. The fact that our method performs differently for different levels of corruption indicates that the
performance of our method varies with the difﬁculty of the problem  a key design goal.
The difference between the second and third ﬁgures for our method is approximately a constant factor 
indicating that the computational effort for each point is roughly the same. We also see that our
method takes signiﬁcantly more effort per point. Nonetheless our method is signiﬁcantly faster than
the DP across all tested levels of corruption  and the difference gets more signiﬁcant as we increase k.
To more closely investigate how the difﬁculty of the problem can affect the running time performance 
we compare how the speedup is affected by the percent of ﬂipped/corrupted points in Figure 5 at

8

Figure 4: Summary of empirical results. The graph on the left shows how increasing the granularity of the grid
decreases the error  and the next two graphs compare the performance of DP against our method (under different
percent of ﬂipped/corrupted points) in terms of points processed and running time.

k = 216. For low levels of noise  the speedup is extremely high. There is a rapid decrease in
performance between 20 and 30 percent  and at higher levels of noise the performance begins to
stabilize again at about 9-10×.

Figure 5: Speedup as a function of the amount of points that were ﬂipped/corrupted at k = 216.

In addition to the above experiments  we also ran preliminary experiments varying the value of n. As
predicted by the theory  the complexity of both methods scale roughly linearly with n. Tests on a
range of quasiconvex robust estimators shows similar results.

5 Conclusions and Future Work

We propose a pruning algorithm that builds upon the standard DP algorithm for solving the separable
nonconvex isotonic regression problem (1) to any arbitrary accuracy (to the global optimal value). On
the theoretical front  we demonstrate that the pruning rules developed retain the correct points and
intervals required to reach the global optimal value  and in the convex case our algorithm becomes a
variant of the O(n log k) scaling algorithm. In terms of empirical performance  our initial synthetic
experiments show that our algorithm scales signiﬁcantly better as the desired accuracy increases.
Besides developing more pruning rules that can work on a larger range of nonconvex fi functions 
there are two main directions for extensions to this work  mirroring the line of developments for
the classic isotonic regression problem. The ﬁrst is go beyond monotonicity constraints and instead
consider chain functions gi(xi − xi+1) that link together adjacent indices. A particularly interesting
case is the one where gi(xi) incorporates a (cid:96)2-penalty in addition to the monotonicity constraints
in order to promote smoothness. The second is to go from the full ordering we consider here to
general partial orders. Dynamic programming approaches fail in that setting and we would require a
signiﬁcantly different approach. It may be possible to adapt the general submodular-based approach
developed by Bach (2018)  which works in both the above mentioned extensions.

9

Acknowledgements

The author would like to thank Alberto Del Pia and Silvia Di Gregorio for initial discussion that
lead to this work. The author was partially supported by NSF Award CMMI-1634597  NSF Award
IIS-1447449 at UW-Madison. Part of the work was completed while visiting the Simons Institute for
the Theory of Computing (partially supported by the DIMACS/Simons Collaboration on Bridging
Continuous and Discrete Optimization through NSF Award CCF-1740425).

References
Ahuja  R. K. and Orlin  J. B. (2001). A Fast Scaling Algorithm for Minimizing Separable Convex

Functions Subject to Chain Constraints. Operations Research  49(5):784–789.

Ayer  M.  Brunk  H. D.  Ewing  G. M.  Reid  W. T.  and Silverman  E. (1955). An Empirical
Distribution Function for Sampling with Incomplete Information. The Annals of Mathematical
Statistics  26(4):641–647.

Bach  F. (2013). Learning with submodular functions: A convex optimization perspective. Founda-

tions and Trends R(cid:13) in Machine Learning  6(2-3):145–373.

Bach  F. (2015). Submodular Functions: from Discrete to Continous Domains. arXiv:1511.00394

[cs  math]. arXiv: 1511.00394.

Bach  F. (2018). Efﬁcient algorithms for non-convex isotonic regression through submodular opti-
mization. In Bengio  S.  Wallach  H.  Larochelle  H.  Grauman  K.  Cesa-Bianchi  N.  and Garnett 
R.  editors  Advances in Neural Information Processing Systems 31  pages 1–10. Curran Associates 
Inc.

Best  M. J.  Chakravarti  N.  and Ubhaya  V. A. (2000). Minimizing Separable Convex Functions

Subject to Simple Chain Constraints. SIAM Journal on Optimization  10(3):658–672.

Bogdan  M.  van den Berg  E.  Su  W.  and Candes  E. (2013). Statistical estimation and testing via

the sorted L1 norm. arXiv:1310.1969.

Brunk  H. D. (1955). Maximum Likelihood Estimates of Monotone Parameters. The Annals of

Mathematical Statistics  26(4):607–616.

Burdakov  O. and Sysoev  O. (2017). A Dual Active-Set Algorithm for Regularized Monotonic

Regression. Journal of Optimization Theory and Applications  172(3):929–949.

Felzenszwalb  P. F. and Huttenlocher  D. P. (2012). Distance Transforms of Sampled Functions.

Theory of Computing  8:415–428.

Gunasekar  S.  Koyejo  O. O.  and Ghosh  J. (2016). Preference Completion from Partial Rankings.
In Lee  D. D.  Sugiyama  M.  Luxburg  U. V.  Guyon  I.  and Garnett  R.  editors  Advances in
Neural Information Processing Systems 29  pages 1370–1378. Curran Associates  Inc.

Hampel  F. R.  Ronchetti  E. M.  Rousseeuw  P. J.  and Stahel  W. A. (2011). Robust statistics: the

approach based on inﬂuence functions  volume 196. John Wiley & Sons.

Hochbaum  D. and Lu  C. (2017). A Faster Algorithm Solving a Generalization of Isotonic Median
Regression and a Class of Fused Lasso Problems. SIAM Journal on Optimization  27(4):2563–
2596.

Hochbaum  D. S. (2001). An Efﬁcient Algorithm for Image Segmentation  Markov Random Fields

and Related Problems. J. ACM  48(4):686–701.

Huber  P. (2004). Robust Statistics. Wiley Series in Probability and Statistics - Applied Probability

and Statistics Section Series. Wiley.

Kalai  A. and Sastry  R. (2009). The Isotron Algorithm: High-Dimensional Isotonic Regression. In

Conference on Learning Theory.

10

Kolmogorov  V.  Pock  T.  and Rolinek  M. (2016). Total Variation on a Tree. SIAM Journal on

Imaging Sciences  9(2):605–636.

Lim  C. H. and Wright  S. J. (2016). Efﬁcient bregman projections onto the permutahedron and
related polytopes. In Gretton  A. and Robert  C. C.  editors  Proceedings of the 19th International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2016)  page 1205–1213.

Stout  Q. F. (2014). Fastest isotonic regression algorithms.

Suehiro  D.  Hatano  K.  Kijima  S.  Takimoto  E.  and Nagano  K. (2012). Online prediction
under submodular constraints. In Bshouty  N.  Stoltz  G.  Vayatis  N.  and Zeugmann  T.  editors 
Algorithmic Learning Theory  volume 7568 of Lecture Notes in Computer Science  pages 260–274.
Springer Berlin Heidelberg.

Sysoev  O. and Burdakov  O. (2016). A Smoothed Monotonic Regression via L2 Regularization.

Linköping University Electronic Press.

Tibshirani  R. J.  Hoeﬂing  H.  and Tibshirani  R. (2011). Nearly-Isotonic Regression. Technometrics 

53(1):54–61.

Yasutake  S.  Hatano  K.  Kijima  S.  Takimoto  E.  and Takeda  M. (2011). Online linear optimization
over permutations. In Asano  T.  Nakano  S.-i.  Okamoto  Y.  and Watanabe  O.  editors  Algorithms
and Computation  volume 7074 of Lecture Notes in Computer Science  pages 534–543. Springer
Berlin Heidelberg.

Zeng  X. and Figueiredo  M. A. T. (2014). The Ordered Weighted (cid:96)1 Norm: Atomic Formulation 

Projections  and Algorithms. arXiv:1409.4271.

11

,Cong Han Lim