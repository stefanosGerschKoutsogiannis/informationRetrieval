2019,An adaptive nearest neighbor rule for classification,We introduce a variant of the $k$-nearest neighbor classifier in which $k$ is chosen adaptively for each query  rather than supplied as a parameter. The choice of $k$ depends on properties of each neighborhood  and therefore may significantly vary between different points. (For example  the algorithm will use larger $k$ for predicting the labels of points in noisy regions.)  

We provide theory and experiments that demonstrate that the algorithm performs comparably to  and sometimes better than  $k$-NN with an optimal choice of $k$. In particular  we derive bounds on the convergence rates of our classifier that depend on a local quantity we call the ``advantage'' which is significantly weaker than the Lipschitz conditions used in previous convergence rate proofs. These generalization bounds hinge on a variant of the seminal Uniform Convergence Theorem due to Vapnik and Chervonenkis; this variant concerns conditional probabilities and may be of independent interest.,An adaptive nearest neighbor rule for classiﬁcation

Akshay Balsubramani
abalsubr@stanford.edu

Sanjoy Dasgupta

dasgupta@eng.ucsd.edu

Yoav Freund

yfreund@eng.ucsd.edu

Shay Moran

shaym@princeton.edu

Abstract

We introduce a variant of the k-nearest neighbor classiﬁer in which k is chosen
adaptively for each query  rather than being supplied as a parameter. The choice
of k depends on properties of each neighborhood  and therefore may signiﬁcantly
vary between different points. For example  the algorithm will use larger k for
predicting the labels of points in noisy regions.
We provide theory and experiments that demonstrate that the algorithm performs
comparably to  and sometimes better than  k-NN with an optimal choice of k.
In particular  we bound the convergence rate of our classiﬁer in terms of a lo-
cal quantity we call the “advantage”  giving results that are both more general
and more accurate than the smoothness-based bounds of earlier nearest neighbor
work. Our analysis uses a variant of the uniform convergence theorem of Vapnik-
Chervonenkis that is for empirical estimates of conditional probabilities and may
be of independent interest.

Introduction

1
We introduce an adaptive nearest neighbor classiﬁcation rule. Given a training set with labels {±1} 
its prediction at a query point x is based on the training points closest to x  rather like the k-nearest
neighbor rule. However  the value of k that it uses can vary from query to query. Speciﬁcally  if there
are n training points  then for any query x  the smallest k is sought for which the k points closest to x
have labels whose average is either greater than +(n  k)  in which case the prediction is +1  or less
than (n  k)  in which case the prediction is 1; and if no such k exists  then “?” (“don’t know”)
is returned. Here  (n  k) ⇠p(log n)/k corresponds to a conﬁdence interval for the average label
in the region around the query.
We study this rule in the standard statistical framework in which all data are i.i.d. draws from some
unknown underlying distribution P on X⇥Y   where X is the data space and Y is the label space. We
take X to be a separable metric space  with distance function d : X⇥X ! R  and we take Y = {±1}.
We can decompose P into the marginal distribution µ on X and the conditional expectation of the
label at each point x: if (X  Y ) represents a random draw from P   deﬁne ⌘(x) = E(Y |X = x). In
this terminology  the Bayes-optimal classiﬁer is the rule g⇤ : X ! {±1} given by

(1)

g⇤(x) =⇢ sign(⌘(x))

if ⌘(x) 6= 0
either 1 or +1 if ⌘(x) = 0

and its error rate is the Bayes risk  R⇤ = 1
2EX⇠µ [1 | ⌘(X)|]. A variety of nonparametric classi-
ﬁcation schemes are known to have error rates that converge asymptotically to R⇤. These include
k-nearest neighbor (henceforth  k-NN) rules [FH51] in which k grows with the number of training
points n according to a suitable schedule (kn)  under certain technical conditions on the metric
measure space (X   d  µ).
33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

⌘

1

0

1

x

Figure 1: For values of x on the left half of the shown interval  the pointwise bias ⌘(x) is close to
1 or 1  and thus a small value of k will yield an accurate prediction. Larger k will not do as well 
because they may run into neighboring regions with different labels. For values of x on the right half
of the interval  ⌘(x) is close to 0  and thus large k is essential for accurate prediction.

In this paper  we are interested in consistency as well as rates of convergence. In particular  we ﬁnd
that the adaptive nearest neighbor rule is also asymptotically consistent (under the same technical
conditions) while converging at a rate that is about as good as  and sometimes signiﬁcantly better
than  that of k-NN under any schedule (kn).
Intuitively  one of the advantages of k-NN over nonparametric classiﬁers that use a ﬁxed bandwidth
or radius  such as Parzen window or kernel density estimators  is that k-NN automatically adapts to
variation in the marginal distribution µ: in regions with large µ  the k nearest neighbors lie close to
the query point  while in regions with small µ  the k nearest neighbors can be further aﬁeld. The
adaptive NN rule that we propose goes further: it also adapts to variation in ⌘. In certain regions of
the input space  where ⌘ is close to 0  an accurate prediction would need large k. In other regions 
where ⌘ is near 1 or 1  a small k would sufﬁce  and in fact  a larger k might be detrimental because
neighboring regions might be labeled differently. See Figure 1 for one such example. A k-NN
classiﬁer is forced to pick a single value of k that trades off between these two contingencies. Our
adaptive NN rule  however  can pick the right k in each neighborhood separately.
Our estimator allows us to give rates of convergence that are tighter and more transparent than those
customarily obtained in nonparametric statistics. Speciﬁcally  for any point x in the instance space
X   we deﬁne a notion of the advantage at x  denoted adv(x)  which is rather like a local margin.
We show that the prediction at x is very likely to be correct once the number of training points
exceeds ˜O(1/adv(x)). Universal consistency follows by establishing that almost all points have
positive advantage.

1.1 Relation to other work in nonparametric estimation

For linear separators and many other parametric families of classiﬁers  it is possible to give rates
of convergence that hold without any assumptions on the input distribution µ or the conditional
expectation function ⌘. This is not true of nonparametric estimation: although any target function can
in principle be captured  the number of samples needed to achieve a speciﬁc level of accuracy will
inevitably depend upon aspects of this function such as how fast it changes [DGL96  chapter 7]. As a
result  nonparametric statistical theory has focused on (1) asymptotic consistency  ideally without
assumptions  and (2) rates of convergence under a variety of smoothness assumptions.
Asymptotic consistency has been studied in great detail for the k-NN classiﬁer  when k is allowed
to grow with the number of data points n. The risk of the classiﬁer  denoted Rn  is its error rate
on the underlying distribution P ; this is a random variable that depends upon the set of training
points seen. Cover and Hart [CH67] showed that in general metric spaces  under the assumption
that every x in the support of µ is either a continuity point of ⌘ or has µ({x}) > 0  the expected
risk ERn converges to the Bayes-optimal risk R⇤  as long as k ! 1 and k/n ! 0. For points

2

in ﬁnite-dimensional Euclidean space  a series of results starting with Stone [Sto77] established
consistency without any assumptions on µ or ⌘  and showed that Rn ! R⇤ almost surely [DGKL94].
More recent work has extended these universal consistency results—that is  consistency without
assumptions on ⌘—to arbitrary metric measure spaces (X   d  µ) that satisfy a certain differentiation
condition [CG06  CD14].
Rates of convergence have been obtained for k-nearest neighbor classiﬁcation under various smooth-
ness conditions including Holder conditions on ⌘ [KP95  Gyö81] and “Tsybakov margin” condi-
tions [MT99  AT07  CD14]. Such assumptions have become customary in nonparametric statistics 
but they leave a lot to be desired. First  they are uncheckable: it is not possible to empirically
determine the smoothness given samples. Second  they view the underlying distribution P through
the tiny window of two or three parameters  obscuring almost all the remaining structure of the
distribution that also inﬂuences the rate of convergence. Finally  because nonparametric estimation is
often local  there is the intriguing possibility of getting different rates of convergence in different
regions of the input space: a possibility that is immediately defeated by reducing the entire space to
two smoothness constants.
The ﬁrst two of these issues are partially addressed by the work of [CD14]  who analyze the ﬁnite
sample risk of k-NN classiﬁcation without any assumptions on P . Their bounds involve terms that
measure the probability mass of the input space in a carefully deﬁned region around the decision
boundary: that is  bounds that are tailored to the speciﬁc distribution P   rather than reﬂecting worst-
case behavior over some large class to which P belongs. However  the expressions for the risk are
somewhat hard to parse  in large part because of the interaction between n and k.
In the present paper  we obtain ﬁnite-sample rates of convergence that are ﬁne-tuned not just to the
speciﬁc distribution P but also to the speciﬁc query point. This is achieved by deﬁning a margin 
or advantage  at every point in the input space  and giving bounds (Theorem 1) entirely in terms of
this quantity. For parametric classiﬁcation  it has become common to deﬁne a notion of margin that
controls generalization. In the nonparametric setting  it makes sense that the margin would in fact
be a function X! R  and would yield different generalization error bounds in different regions of
space. Our adaptive nearest neighbor classiﬁer allows us to realize this vision in a fairly elementary
manner.
The advantages of setting k locally have been pointed out and quantiﬁed in recent work on non-
parametric regression [DGKL94  CS18]  notably that of [Kpo11]. Although it is common to reduce
classiﬁcation to regression in nonparametric analysis  the right choice of k may be fundamentally
different in the two settings. This is reﬂected in the difference between our setting for k and that
of [Kpo11]; for instance  the physical value of the radius containing k points matters in that work
while playing no role in ours. Moreover  the beneﬁt of local adaptivity may be more pronounced for
classiﬁcation than for regression. Our analysis shows  for instance  that there is a radius rx around
each point x such that prediction based on training points in B(x  rx) will with high probability be
perfect  provided there are enough such points. This is not true of regression  where the target y is a
real value and thus the radius needs to keep shrinking.

Organization. Most proofs are relegated to the appendices.
In Section 2  we introduce the formal model of learning and deﬁne some basic geometric notions  as a
prelude to presenting the adaptive k-NN algorithm in Section 3. In Sections 4 and 5 and Appendix A 
we state and prove consistency and generalization bounds for this classiﬁer  and compare them with
prior work in the k-NN literature. Our bounds exploit a general VC-based uniform convergence
statement which is presented in Section 6 and proved in a self-contained manner in Appendix B.

2 Setup
Take the instance space to be a separable metric space (X   d) and the label space to be Y = {±1}.
All data are assumed to be drawn i.i.d. from a ﬁxed unknown distribution P over X⇥Y .
Let µ denote the marginal distribution on X : if (X  Y ) is a random draw from P   then
for any measurable set S ✓X . For any x 2X   the conditional expectation  or bias  of Y given x  is

µ(S) = Pr(X 2 S)

⌘(x) = E(Y |X = x) 2 [1  1].

3

Similarly  for any measurable set S with µ(S) > 0  the conditional expectation of Y given X 2 S is

⌘(S) = E(Y |X 2 S) =

⌘(x) dµ(x).

1

µ(S)ZS

The risk of a classiﬁer g : X ! {1  +1  ?} is the probability that it is incorrect on pairs (X  Y ) ⇠ P  
(2)
The Bayes-optimal classiﬁer g⇤  as given in (1)  depends only on ⌘  but its risk R⇤ depends on µ. For
a classiﬁer gn based on n training points from P   we will be interested in whether R(gn) converges
to R⇤  and the rate at which this convergence occurs.
The algorithm and analysis in this paper depend heavily on the probability masses and biases of balls
in X . For x 2X and r  0  let B(x  r) denote the closed ball of radius r centered at x 

R(g) = P ({(x  y) : g(x) 6= y}).

B(x  r) = {z 2X : d(x  z)  r}.

For 0  p  1  let rp(x) be the smallest radius r such that B(x  r) has probability mass at least p 
that is 
(3)

rp(x) = inf{r  0 : µ(B(x  r))  p}.

It follows that µ(B(x  rp(x)))  p.
The support of the marginal distribution µ plays an important role in convergence proofs and is
formally deﬁned as

supp(µ) = {x 2X : µ(B(x  r)) > 0 for all r > 0}.

It is a well-known consequence of the separability of X that µ(supp(µ)) = 1 [CH67].
3 The adaptive k-nearest neighbor algorithm
The algorithm is given a labeled training set (x1  y1)  . . .   (xn  yn) 2X⇥Y . Based on these points 
it is able to compute empirical estimates of the probabilities and biases of different balls.
For any set S ✓X   we deﬁne its empirical count and probability mass as

#n(S) = |{i : xi 2 S}|
µn(S) =

#n(S)

.

n

If this is non-zero  we take the empirical bias to be

⌘n(S) = Pi:xi2S yi

#n(S)

.

(4)

(5)

The adaptive k-NN algorithm (AKNN) is shown in Figure 2. It makes a prediction at x by growing a
ball around x until the ball has signiﬁcant bias  and then choosing the corresponding label. In some
cases  a ball of sufﬁcient bias may never be obtained  in which event “?” is returned. In what follows 
let gn : X ! {1  +1  ?} denote the AKNN classiﬁer.
Later  we will also discuss a variant of this algorithm in which a modiﬁed conﬁdence interval 

(n  k  ) = c1r d0 log n + log(1/)
is used  where d0 is the VC dimension of the family of balls in (X   d).
In comparing the algorithm of Figure 2 to standard k-nearest neighbor classiﬁcation  it might at ﬁrst
glance seem that we have merely replaced one parameter (k) with another (). This is not accurate.
Our  is the customary conﬁdence parameter of statistics and learning theory: it provides an upper
bound on the failure probability of the algorithm. It can be set to 0.05  for instance. The algorithm
makes inﬁnitely many parameter choices—it sets k for each query point—and asks for just a single
failure probability that lets it know how aggressively to set its conﬁdence intervals.

(7)

k

4

Given:

• training set (x1  y1)  . . .   (xn  yn) 2 X ⇥ {±1}
• conﬁdence parameter 0 << 1

To predict at x 2X :

exactly k training points. a

• For any integer k  let Bk(x) denote the smallest ball centered at x that contains
• Find the smallest 0 < k  n for which the Bk(x) has a signiﬁcant bias: that

is  |⌘n(Bk(x))| > (n  k  )  where

(n  k  ) = c1r log n + log(1/)
• If there exists such a ball  return label sign(⌘n(Bk(x))).
• If no such ball exists: return “?”

k

.

(6)

aWhen several points have the same distance to x  there might be some values of k for which

Bk(x) is undeﬁned. Our algorithm skips such values of k.

Figure 2: The adaptive k-NN (AKNN) classiﬁer. The absolute constant c1 is from Lemma 7.

4 Pointwise advantage and rates of convergence

We now provide ﬁnite-sample rates of convergence for the adaptive nearest neighbor rule. For
simplicity  we give convergence rates that are speciﬁc to any query point x and that depend on a
suitable notion of the “margin” of distribution P around x.
Pick any p   > 0. Recalling deﬁnition (3)  we say a point x 2X is (p  )-salient if the following
holds for either s = +1 or s = 1:

• s⌘(x) > 0  and s⌘(B(x  r)) > 0 for all r 2 [0  rp(x))  and s⌘(B(x  rp(x)))  .

In words  this means that g⇤(x) = s (recall that g⇤ is the Bayes classiﬁer)  that the biases of all balls
of radius  rp(x) around x have the same sign as s  and that the bias of the ball of radius rp(x)
has absolute value at least . A point x can satisfy this deﬁnition for a variety of pairs (p  ). The
advantage of x is taken to be the largest value of p2 over all such pairs:

adv(x) =⇢ sup{p2 : x is (p  )-salient}

0

if ⌘(x) 6= 0
if ⌘(x) = 0

(8)

We will see (Lemma 3) that under a mild condition on the underlying metric measure space  almost
all x with ⌘(x) 6= 0 have a positive advantage.
4.1 Advantage-based ﬁnite-sample bounds
We now state two generalization bounds for the adaptive nearest neighbor classiﬁer. The ﬁrst holds
pointwise—it bounds the probability of error at a speciﬁc point x—while the second is the type of
uniform convergence bound that is more standard in learning theory.
The following theorem shows that for every point x  if the sample size n satisﬁes n ' 1/adv(x)  then
the label of x is likely to be g⇤(x)  where g⇤ is the Bayes optimal classiﬁer. This provides pointwise
convergence of g(x) to g⇤(x) at a rate that is sensitive to the local geometry of x.
Theorem 1 (Pointwise convergence rate). There is an absolute constant C > 0 for which the
following holds. Let 0 << 1 denote the conﬁdence parameter in the AKNN algorithm (Figure 2) 
and suppose the algorithm is used to deﬁne a classiﬁer gn based on n training points chosen i.i.d.
from P . Then  for every point x 2 supp(µ)  if

C

n 

adv(x)

max✓log

1

adv(x)

  log

1

◆

5

then with probability at least 1   we have that gn(x) = g⇤(x).
If we further assume that the family of all balls in the space has ﬁnite VC dimension d0 then we can
strengthen the guarantee to hold with high probability simultaneously for all x 2 supp(µ). This is
achieved by a modiﬁed version of the algorithm that uses conﬁdence interval (7) instead of (6).
Theorem 2 (Uniform convergence rate). Suppose that the set of balls in (X   d) has ﬁnite VC
dimension d0  and that the algorithm of Figure 2 uses conﬁdence interval (7) instead of (6). Then 
with probability at least 1    the resulting classiﬁer gn satisﬁes the following: for every point x 2
supp(µ)  if

n 

C

adv(x)

max✓log

1

adv(x)

  log

1

◆

then gn(x) = g⇤(x).
A key step towards proving Theorems 1 and 2 is to identify the subset of X that is likely to be
correctly classiﬁed for a given number of training points n. This follows the rough outline of [CD14] 
which gave rates of convergence for k-nearest neighbor  but there are two notable differences. First 
we will see that the likely-correct sets obtained in that earlier work (for k-NN) are  roughly  subsets
of those we obtain for the new adaptive nearest neighbor procedure. Second  the proof for our setting
is considerably more streamlined; for instance  there is no need to devise tie-breaking strategies for
deciding the identities of the k nearest neighbors.

4.2 A comparison with k-nearest neighbor
For a  0  let Xa denote all points with advantage greater than a:

Xa = {x 2 supp(µ) : adv(x) > a}.

(9)

In particular  X0 consists of all points with positive advantage.
By Theorem 1  points in Xa are likely to be correctly classiﬁed when the number of training points
is e⌦(1/a)  where the e⌦(·) notation ignores logarithmic terms. In contrast  the work of [CD14]
showed that with n training points  the k-NN classiﬁer is likely to correctly classify the following set
of points:

X 0n k = {x 2 supp(µ) : ⌘(x) > 0 ⌘ (B(x  r))  k1/2 for all 0  r  rk/n(x)}

[{ x 2 supp(µ) : ⌘(x) < 0 ⌘ (B(x  r))  k1/2 for all 0  r  rk/n(x)}.

Such points are (k/n  k1/2)-salient and thus have advantage at least 1/n. In fact 

[1kn

X 0n k ✓X 1/n.

In this sense  the adaptive nearest neighbor procedure is able to perform roughly as well as all choices
of k simultaneously. This is not a precise statement because of logarithmic factors (the sample
complexity in Theorem 1 is (1/a) log(1/a) rather than 1/a)  and the resulting gap can be seen in our
experiments.

5 Universal consistency

In this section we study the convergence of R(gn) to the Bayes risk R⇤ as the number of points n
grows. An estimator is described as universally consistent in a metric measure space (X   d  µ) if it
has this desired limiting behavior for all conditional expectation functions ⌘.
Earlier work [CD14] established the universal consistency of k-nearest neighbor (for k/n ! 0 and
k/(log n) ! 1) in any metric measure space that satisﬁes the Lebesgue differentiation condition:
that is  for any bounded measurable f : X! R and for almost all (µ-a.e.) x 2X  

1

µ(B(x  r))ZB(x r)

lim
r#0

6

f dµ = f (x).

(10)

This is known to hold  for instance  in any ﬁnite-dimensional normed space or any doubling metric
space [Hei01  Chapter 1].
We will now see that this same condition implies the universal consistency of the adaptive nearest
neighbor rule. To begin with  it implies that almost every point has a positive advantage.
Lemma 3. Suppose metric measure space (X   d  µ) satisﬁes condition (10). Then  for any conditional
expectation ⌘  the set of points

has zero µ-measure.

{x 2X : ⌘(x) 6= 0  adv(x) = 0}

Proof. Let X 0 ✓X consist of all points x 2 supp(µ) for which condition (10) holds true with f = ⌘ 
that is  limr#0 ⌘(B(x  r)) = ⌘(x). Since µ(supp(µ)) = 1  it follows that µ(X 0) = 1.
Pick any x 2X 0 with ⌘(x) 6= 0; without loss of generality  ⌘(x) > 0. By (10)  there exists ro > 0
such that

Thus x is (p  )-salient for p = µ(B(x  ro)) > 0 and  = ⌘(x)/2  and has positive advantage.

⌘(B(x  r))  ⌘(x)/2 for all 0  r  ro.

Universal consistency follows as a consequence; the proof details are deferred to Appendix A.
Theorem 4 (Universal consistency). Suppose the metric measure space (X   d  µ) satisﬁes condi-
tion (10). Let (n) be a sequence in [0  1] with (1)Pn n < 1 and (2) limn!1(log(1/n))/n = 0.
Let the classiﬁer gn n : X ! {1  +1  ?} be the result of applying the AKNN procedure (Figure 2)
with n points chosen i.i.d. from P and with conﬁdence parameter n. Letting Rn = R(gn n) denote
the risk of gn n  we have Rn ! R⇤ almost surely.
6 Uniform convergence of empirical conditional measures

A key piece of our analysis is a uniform convergence bound for empirical estimates of conditional
probabilities. We now discuss this bound in an abstract setting; further details are in Appendix B.
Let P be a distribution over some space X  and let A B be two collections of events. Let x1  . . .   xn
be independent samples from P . We would like to use these to estimate P (A|B) simultaneously for
all A 2A   B 2B . It is natural to consider the empirical estimates:
Pn(A|B) = Pi 1[xi2A\B]
Pi 1[xi2B]

We study the approximation error of these estimates. Note that the case where B = {X} (i.e.  in
which one estimates P (A) using Pn(A) simultaneously for all A 2A ) is handled by the classical
VC theory. Let us assume that both A B have VC dimension upper-bounded by some d0.
To demonstrate the kinds of statements we would like  consider the case where each of A B contains
only one event: A = {A}  and B = {B}  and set #n(B) =Pi 1[xi2B]. A Chernoff bound implies
that conditioned on the event that #n(B) > 0  the following holds with probability at least 1  :
(11)

.

|P (A|B)  Pn(A|B)| s 2 log(1/)

#n(B)

.

This bound depends on #n(B) and is thus data-dependent. To derive it  use that conditioned on
xi 2 B  event xi 2 A has probability P (A|B)  so random variable “#n(B) · pn(A|B)” has a
binomial distribution with parameters #n(B) and P (A|B).
We would want to prove a uniform version of (11)  of the form: with probability at least 1   

(8A 2A ) (8B 2B ) : |P (A|B)  Pn(A|B)| O s d0 log(1/)
#n(B) ! .

But as we explain in the appendix  this is unfortunately false. Instead  we prove the following (slightly
weaker) variant:

7

Theorem 5 (UCECM). Let P be a probability distribution over X  and let A B be two families
of measurable subsets of X such that VC(A)  VC(B)  d0. Let n 2 N  and let x1 . . . xn be n i.i.d
samples from P . Then the following event occurs with probability at least 1  :
(8A 2A ) (8B 2B ) : |P (A|B)  Pn(A|B)| s ko
i=1 1[xi 2 B].

where ko = 1000 (d0 log(8n) + log(4/))  and #n(B) =Pn

#n(B)

 

7 Experiments

Figure 3: Effect of label noise on k-NN and AKNN. Performance on MNIST for different levels of
random label noise p and for different values of k. Each line in the ﬁgure on the left (a) represents the
performance of k-NN as a function of k for a given level of noise. The optimal choice of k increases
with the noise level  and that the performance degrades severely for too-small k. The table (b) shows
that AKNN  with a ﬁxed value of A  performs almost as well as k-NN with the optimal choice of k.

We performed a few experiments using real-world data sets from computer vision and genomics (see
Section C). These were conducted with some practical alterations to the algorithm of Fig. 2.
Multiclass extension: Suppose the set of possible labels is Y. We replace the binary rule “ﬁnd
the smallest k such that |⌘n(Bk(x))| > (n  k  )" with the rule: “ﬁnd the smallest k such that
n(Bk(x))  1
⌘y
|Y|

> (n  k  ) for some y 2Y   where ⌘y

= #n{xi2S and yi=y}
.

n(S)

#n(S)

."

At left: performance of AKNN on notMNIST
for different settings of the conﬁdence param-
eter (A = 1  3  9)  as a function of the neigh-
borhood size. For each conﬁdence level we
show two graphs: an accuracy graph (solid
line) and a coverage line (dashed line). For
each value of k we plot the accuracy and the
coverage of AKNN which is restricted to us-
ing a neighborhood size of at most k.
In-
creasing A generally causes an increase in the
accuracy and a decrease in coverage. Larger
values of A cause AKNN to have coverage
zero for values of k that are too small. For
comparison  we plot the performance of k-
NN as a function of k. The highest accuracy
(⇡ 0.88) is achieved for k = 10 (dotted hori-
zontal line)  and is surpassed by AKNN with
high coverage (100% for A = 1).

Figure 4: Performance of AKNN on notMNIST. See also Figure 5.

8

Figure 5: A visualization of the performance of AKNN on notMNIST. (a) The correct labels  with
prediction errors of AKNN (A = 4) highlighted. (b) The value of k chosen by the algorithm when
predicting each datapoint.

n(Bk(x))/pk is largest.

Parametrization: We replace Equation (6) with = Apk   where A is a conﬁdence parameter
corresponding to the theory’s  (given n).
Resolving multilabel predictions: Our algorithm can output answers that are not a single label. The
output can be “?”  which indicates that no label has sufﬁcient evidence. It can also be a subset of Y
that contains more than one element  indicating that more than one label has signiﬁcant evidence. In
some situations  using subsets of the labels is more informative. However  when we want to compare
head-to-head with k-NN  we need to output a single label. We use a heuristic to predict with a single
label y 2Y on any x: the label for which maxk ⌘y
We brieﬂy discuss our main conclusions from the experiments  with more details in Appendix C.
AKNN is comparable to the best k-NN rule. In Section 4.2 we prove that AKNN compares
favorably to k-NN with any ﬁxed k. We demonstrate this in practice in different situations. With
simulated independent label noise on the MNIST dataset (Fig. 3)  a small value of k is optimal for
noiseless data  but performs very poorly when the noise level is high. On the other hand  AKNN
adapts to the local noise level automatically  as demonstrated without adding noise on the more
challenging notMNIST and single-cell genomics data (Fig. 4  5  6).
Varying the conﬁdence parameter A controls abstaining. The parameter A controls how con-
servative the algorithm is in deciding to abstain  instead of incurring error by predicting. A ! 0
represents the most aggressive setting  in which the algorithm never abstains  essentially predicting
according to a 1-NN rule. Higher settings of A cause the algorithm to abstain on some of these
predicted points  for which there is no sufﬁciently small neighborhood with a sufﬁciently signiﬁcant
label bias (Fig. 7).
Adaptively chosen neighborhood sizes reﬂect local conﬁdence. The number of neighbors chosen
by AKNN is a local quantity that gives a practical pointwise measure of the conﬁdence associated with
label predictions. Small neighborhoods are chosen when one label is measured as signiﬁcant nearly
as soon as statistically possible; by deﬁnition of the AKNN stopping rule  this is not true where large
neighborhoods are necessary. In our experiments  performance on points with signiﬁcantly higher
neighborhood sizes dropped monotonically  with the majority of the data set having performance
signiﬁcantly exceeding the best k-NN rule over a range of settings of A (Fig. 4  6; Appendix C).

9

References
[AT07]

[BBL05]

[C+18]

[CD10]

[CD14]

[CG06]

[CH67]

[CS18]

J.-Y. Audibert and A.B. Tsybakov. Fast learning rates for plug-in classiﬁers. Annals of
Statistics  35(2):608–633  2007.
S. Boucheron  O. Bousquet  and G. Lugosi. Theory of classiﬁcation: A survey of some
recent advances. ESAIM: probability and statistics  9:323–375  2005.
Tabula Muris Consortium et al. Single-cell transcriptomics of 20 mouse organs creates a
tabula muris. Nature  562(7727):367  2018.
K. Chaudhuri and S. Dasgupta. Rates of convergence for the cluster tree. In Advances in
Neural Information Processing Systems  pages 343–351  2010.
K. Chaudhuri and S. Dasgupta. Rates of convergence for nearest neighbor classiﬁcation.
In Advances in Neural Information Processing Systems  pages 3437–3445. 2014.
F. Cerou and A. Guyader. Nearest neighbor classiﬁcation in inﬁnite dimension. ESAIM:
Probability and Statistics  10:340–355  2006.
T. Cover and P.E. Hart. Nearest neighbor pattern classiﬁcation. IEEE Transactions on
Information Theory  13:21–27  1967.
G.H. Chen and D. Shah. Explaining the Success of Nearest Neighbor Methods in
Prediction. Foundations and Trends in Machine Learning. NOW Publishers  2018.

[DCL11] W. Dong  M. Charikar  and K. Li. Efﬁcient k-nearest neighbor graph construction for
generic similarity measures. In Proceedings of the 20th international conference on
World wide web  pages 577–586. ACM  2011.

[DGKL94] L. Devroye  L. Györﬁ  A. Krzyzak  and G. Lugosi. On the strong universal consistency
of nearest neighbor regression function estimates. Annals of Statistics  22:1371–1385 
1994.

[DGL96] L. Devroye  L. Györﬁ  and G. Lugosi. A Probabilistic Theory of Pattern Recognition.

Springer  1996.
R.M. Dudley. Balls in Rk do not cut all subsets of k +2 points. Advances in Mathematics 
31(3):306–308  1979.
E. Fix and J. Hodges. Discriminatory analysis  nonparametric discrimination. USAF
School of Aviation Medicine  Randolph Field  Texas  Project 21-49-004  Report 4  Con-
tract AD41(128)-31  1951.
L. Györﬁ. The rate of convergence of kn-nn regression estimates and classiﬁcation rules.
IEEE Transactions on Information Theory  27(3):362–364  1981.
J. Heinonen. Lectures on Analysis on Metric Spaces. Springer  2001.
S. Kulkarni and S. Posner. Rates of convergence of nearest neighbor estimation under
arbitrary sampling. IEEE Transactions on Information Theory  41(4):1028–1039  1995.
S. Kpotufe. k-nn regression adapts to local intrinsic dimension. In Neural Information
Processing Systems  2011.

[MNI96] MNIST dataset. http://yann.lecun.com/exdb/mnist/  1996.
[Mou18] Mouse cell atlas dataset.

ftp://ngs.sanger.ac.uk/production/teichmann/

BBKNN/MouseAtlas.zip  2018. Accessed: 2019-05-02.
E. Mammen and A.B. Tsybakov. Smooth discrimination analysis. The Annals of
Statistics  27(6):1808–1829  1999.
notMNIST dataset. http://yaroslavb.com/upload/notMNIST/  2011. Accessed:
2019-05-02.
M. Raab and A. Steger. Balls into bins - a simple and tight analysis. In Randomization
and Approximation Techniques in Computer Science  Second International Workshop 
RANDOM’98  Barcelona  Spain  October 8-10  1998  Proceedings  pages 159–170 
1998.
C. Stone. Consistent nonparametric regression. Annals of Statistics  5:595–645  1977.
V.N. Vapnik and A.Y. Chervonenkis. On the uniform convergence of relative frequencies
of events to their probabilities. Theory of Probability & Its Applications  16(2):264–280 
1971.

[Dud79]

[FH51]

[Gyö81]

[Hei01]
[KP95]

[Kpo11]

[MT99]

[not11]

[RS98]

[Sto77]
[VC71]

10

,Akshay Balsubramani
Sanjoy Dasgupta
yoav Freund
Shay Moran