2011,PAC-Bayesian Analysis of Contextual Bandits,We derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits). The scaling of our regret bound with the number of states (contexts) $N$ goes as $\sqrt{N I_{\rho_t}(S;A)}$  where $I_{\rho_t}(S;A)$ is the mutual information between states and actions (the side information) used by the algorithm at round $t$. If the algorithm uses all the side information  the regret bound scales as $\sqrt{N \ln K}$  where $K$ is the number of actions (arms). However  if the side information $I_{\rho_t}(S;A)$ is not fully used  the regret bound is significantly tighter. In the extreme case  when $I_{\rho_t}(S;A) = 0$  the dependence on the number of states reduces from linear to logarithmic. Our analysis allows to provide the algorithm large amount of side information  let the algorithm to decide which side information is relevant for the task  and penalize the algorithm only for the side information that it is using de facto. We also present an algorithm for multiarmed bandits with side information with computational complexity that is a linear in the number of actions.,PAC-Bayesian Analysis of Contextual Bandits

Yevgeny Seldin1 4 Peter Auer2 Franc¸ois Laviolette3 John Shawe-Taylor4 Ronald Ortner2

1Max Planck Institute for Intelligent Systems  T¨ubingen  Germany

2Chair for Information Technology  Montanuniversit¨at Leoben  Austria

3D´epartement d’informatique  Universit´e Laval  Qu´ebec  Canada
4Department of Computer Science  University College London  UK

seldin@tuebingen.mpg.de  {auer ronald.ortner}@unileoben.ac.at 

francois.laviolette@ift.ulaval.ca  jst@cs.ucl.ac.uk

Abstract

We derive an instantaneous (per-round) data-dependent regret bound for stochas-
tic multiarmed bandits with side information (also known as contextual bandits).
The scaling of our regret bound with the number of states (contexts) N goes as

pN I⇢t(S; A)  where I⇢t(S; A) is the mutual information between states and ac-
tions (the side information) used by the algorithm at round t. If the algorithm
uses all the side information  the regret bound scales as pN ln K  where K is
the number of actions (arms). However  if the side information I⇢t(S; A) is not
fully used  the regret bound is signiﬁcantly tighter. In the extreme case  when
I⇢t(S; A) = 0  the dependence on the number of states reduces from linear to
logarithmic. Our analysis allows to provide the algorithm large amount of side
information  let the algorithm to decide which side information is relevant for the
task  and penalize the algorithm only for the side information that it is using de
facto. We also present an algorithm for multiarmed bandits with side information
with O(K) computational complexity per game round.

1

Introduction

Multiarmed bandits with side information are an elegant mathematical model for many real-life
interactive systems  such as personalized online advertising  personalized medical treatment  and so
on. This model is also known as contextual bandits or associative bandits (Kaelbling  1994  Strehl
et al.  2006  Langford and Zhang  2007  Beygelzimer et al.  2011). In multiarmed bandits with side
information the learner repeatedly observes states (side information) {s1  s2  . . .} (for example 
symptoms of a patient) and has to perform actions (for example  prescribe drugs)  such that the
expected regret is minimized. The regret is usually measured by the difference between the reward
that could be achieved by the best (unknown) ﬁxed policy (for example  the number of patients that
would be cured if we knew the best drug for each set of symptoms) and the reward obtained by the
algorithm (the number of patients that were actually cured).
Most of the existing analyses of multiarmed bandits with side information has focused on the ad-
versarial (worst-case) model  where the sequence of rewards associated with each state-action pair
is chosen by an adversary. However  many problems in real-life are not adversarial. We derive data-
dependent analysis for stochastic multiarmed bandits with side information. In the stochastic setting
the rewards for each state-action pair are drawn from a ﬁxed unknown distribution. The sequence of
states is also drawn from a ﬁxed unknown distribution. We restrict ourselves to problems with ﬁnite
number of states N and ﬁnite number of actions K and leave generalization to continuous state and
action spaces to future work. We also do not assume any structure of the state space. Thus  for us
a state is just a number between 1 and N. For example  in online advertising the state can be the
country from which a web page is accessed.

1

The result presented in this paper exhibits adaptive dependency on the side information (state iden-
tity) that is actually used by the algorithm. This allows us to provide the algorithm a large amount
of side information and let the algorithm decide  which of this side information is actually relevant
to the task. For example  in online advertising we can increase the state resolution and provide the
algorithm the town from which the web page was accessed  but if this reﬁned state information is not
used by the algorithm the regret bound will not deteriorate. This can be opposed to existing analysis
of adversarial multiarmed bandits  where the regret bound depends on a predeﬁned complexity of
the underlying expert class (Beygelzimer et al.  2011). Thus  the existing analysis of adversarial
multiarmed bandits would either become looser if we add more side information or a-priori limit the
usage of the side information through its internal structure. (We note that through the relation be-
tween PAC-Bayesian analysis and the analysis of adversarial online learning described in Banerjee
(2006) it might be possible to extend our analysis to adversarial setting  but we leave this research
direction to future work.)
The idea of regularization by relevant mutual information goes back to the Information Bottleneck
principle in supervised and unsupervised learning (Tishby et al.  1999). Tishby and Polani (2010)
further suggested to measure the complexity of a policy in reinforcement learning by the mutual
information between states and actions used by the policy. We note  however  that our starting point
is the regret bound and we derive the regularization term from our analysis without introducing it
a-priori. The analysis also provides time and data dependent weighting of the regularization term.
Our results are based on PAC-Bayesian analysis (Shawe-Taylor and Williamson  1997  Shawe-
Taylor et al.  1998  McAllester  1998  Seeger  2002)  which was developed for supervised learning
within the PAC (Probably Approximately Correct) learning framework (Valiant  1984). In PAC-
Bayesian analysis the complexity of a model is deﬁned by a user-selected prior over a hypothesis
space. Unlike in VC-dimension-based approaches and their successors  where the complexity is
deﬁned for a hypothesis class  in PAC-Bayesian analysis the complexity is deﬁned for individual
hypotheses. The analysis provides an explicit trade-off between individual model complexity and
its empirical performance and a high probability guarantee on the expected performance.
An important distinction between supervised learning and problems with limited feedback  such
as multiarmed bandits and reinforcement learning more generally  is the fact that in supervised
learning the training set is given  whereas in reinforcement learning the training set is generated by
the learner as it plays the game. In supervised learning every hypothesis in a hypothesis class can
be evaluated on all the samples  whereas in reinforcement learning rewards of one action cannot
be used to evaluate another action. Recently  Seldin et al. (2011b a) generalized PAC-Bayesian
analysis to martingales and suggested a way to apply it under limited feedback. Here  we apply this
generalization to multiarmed bandits with side information.
The remainder of the paper is organized as follows. We start with deﬁnitions in Section 2 and provide
our main results in Section 3  which include an instantaneous regret bound and a new algorithm for
stochastic multiarmed bandits with side information. In Section 4 we present an experiment that
illustrates our theoretical results. Then  we dive into the proof of our main results in Section 5 and
discuss the paper in Section 6.

2 Deﬁnitions

In this section we provide all essential deﬁnitions for our main results in the following section. We
start with the deﬁnition of stochastic multiarmed bandits with side information. Let S be a set of
|S| = N states and let A be a set of |A| = K actions  such that any action can be performed in any
state. Let s 2 S denote the states and a 2 A denote the actions. Let R(a  s) be the expected reward
for performing action a in state s. At each round t of the game the learner is presented a state St
drawn i.i.d. according to an unknown distribution p(s). The learner draws an action At according to
his choice of a distribution (policy) ⇡t(a|s) and obtains a stochastic reward Rt with expected value
R(At  St). Let {S1  S2  . . .} denote the sequence of observed states  {⇡1  ⇡2  . . .} the sequence of
policies played  {A1  A2  . . .} the sequence of actions played  and {R1  R2  . . .} the sequence of
observed rewards. Let Tt = {{S1  . . .   St} {⇡1  . . .   ⇡t} {A1  . . .   At} {R1  . . .   Rt}} denote the
history of the game up to time t.

2

Assume that ⇡t(a|s) > 0 for all t  a  and s. For t  1  a 2 {1  . . .   K}  and the sequence of
observed states {S1  . . .   St} deﬁne a set of random variables Ra St
if At = a
otherwise.

⇡t(a|St) Rt 

Ra St

0 

:

1

t

t =⇢

t

(The variables Ra s
are deﬁned only for the observed state s = St.) Note that whenever deﬁned 
E[Ra St
is generally known as importance weighted
sampling (Sutton and Barto  1998). Importance weighted sampling is required for application of
PAC-Bayesian analysis  as will be shown in the technical part of the paper.

|Tt1  St] = R(a  St). The deﬁnition of Ra s

t

t

function). We deﬁne the empirical rewards of state-action pairs as:

⌧ =1 I{S⌧ =s} as the number of times state s appeared up to time t (I is the indicator

Deﬁne nt(s) =Pt

ˆRt(a  s) =( P{⌧ =1 ... t:S⌧ =s} Ra s

nt(s)
0 

⌧

 

if nt(s) > 0
otherwise.

Note that whenever nt(s) > 0 we have E ˆRt(a  s) = R(a  s). For every state s we deﬁne the “best”
action in that state as a⇤ = arg maxa R(a  s) (if there are multiple “best” actions  one of them is
chosen arbitrarily). We then deﬁne the expected and empirical regret for performing any other action
a in state s as:

(a  s) = R(a⇤(s)  s)  R(a  s) 

ˆt(a  s) = ˆRt(a⇤(s)  s)  ˆRt(a  s).

t

Let ˆpt(s) = nt(s)
be the empirical distribution over states observed up to time t. For any pol-
icy ⇢(a|s) we deﬁne the empirical reward  empirical regret  and expected regret of the policy
as: ˆRt(⇢) = Ps ˆpt(s)Pa ⇢(a|s) ˆRt(a  s)  ˆt(⇢) = Ps ˆpt(s)Pa ⇢(a|s) ˆt(a  s)  and (⇢) =
Ps p(s)Pa ⇢(a|s)(a  s).
We deﬁne the marginal distribution over actions that corresponds to a policy ⇢(a|s) and the uniform
NPs ⇢(a|s) and the mutual information between actions and states
distribution over S as ¯⇢(a) = 1
corresponding to the policy ⇢(a|s) and the uniform distribution over S as

I⇢(S; A) =

1

N Xs a

⇢(a|s) ln

⇢(a|s)
¯⇢(a)

.

For the proof of our main result and also in order to explain the experiments we also have to deﬁne
a hypothesis space for our problem. This deﬁnition is not used in the statement of the main result.
Let H be a hypothesis space  such that each member h 2 H is a deterministic mapping from S to
A. Denote by a = h(s) the action assigned by hypothesis h to state s. It is easy to see that the size
of the hypothesis space |H| = KN. Denote by R(h) =Ps2S p(s)R(h(s)  s) the expected reward
of a hypothesis h. Deﬁne:

ˆRt(h) =

Rh(S⌧ ) S⌧

⌧

.

1
t

tX⌧ =1

Note that E ˆRt(h) = R(h).
Let h⇤ = arg maxh2H R(h) be the “best” hypothesis (the one that chooses the “best” action in each
state). (If there are multiple hypotheses achieving maximal reward pick any of them.) Deﬁne:

(h) = R(h⇤)  R(h) 

ˆt(h) = ˆRt(h⇤)  ˆRt(h).

Any policy ⇢(a|s) deﬁnes a distribution over H: we can draw an action a for each state s according
to ⇢(a|s) and thus obtain a hypothesis h 2 H. We use ⇢(h) to denote the respective probability
of drawing h. For a policy ⇢ we deﬁne (⇢) = E⇢(h)[(h)] and ˆt(⇢) = E⇢(h)[ ˆt(h)]. By
marginalization these deﬁnitions are consistent with our preceding deﬁnitions of (⇢) and ˆt(⇢).
s=1 Ih(s)=a be the number of states in which action a is played by the
be the normalized cardinality proﬁle (histogram) over the

Finally  let nh(a) = PN
hypothesis h. Let Ah = n nh(a)

N oa2A

3

nh(a)
N ln nh(a)

actions played by hypothesis h (with respect to the uniform distribution over S). Let H(Ah) =
Pa
N be the entropy of this cardinality proﬁle. In other words  H(Ah) is the entropy
of an action choice of hypothesis h (with respect to the uniform distribution over S). Note  that the
optimal policy ⇢⇤(a|s) (the one  that selects the “best” action in each state) is deterministic and we
have I⇢⇤(S; A) = H(Ah⇤).

3 Main Results

Our main result is a data and complexity dependent regret bound for a general class of prediction
strategies of a smoothed exponential form. Let ⇢t(a) be an arbitrary distribution over actions  let

⇢exp
t

(a|s) =

⇢t(a)et ˆRt(a s)

Z(⇢exp

t

  s)

 

(1)

where Z(⇢exp

t

  s) =Pa ⇢t(a)et ˆRt(a s) is a normalization factor  and let

˜⇢exp
t

t

(a|s) + "t+1

(a|s) = (1  K"t+1)⇢exp

(2)
be a smoothed exponential policy. The following theorem provides a regret bound for playing ˜⇢exp
at round t + 1 of the game. For generality  we assume that rounds 1  . . .   t were played according to
arbitrary policies ⇡1  . . .   ⇡t.
Theorem 1. Assume that in game rounds 1  . . .   t policies {⇡1  . . .   ⇡t} were played and assume
that mina s ⇡t(a|s)  "t for an arbitrary "t that is independent of Tt. Let ⇢t(a) be an arbitrary
distribution over A that can depend on Tt and satisﬁes mina ⇢t(a)  ✏t. Let c > 1 be an arbitrary
number that is independent of Tt. Then  with probability greater than 1   over Tt  simultaneously
for all policies ˜⇢exp

deﬁned by (2) that satisfy

t

t

N I⇢exp

t

(S; A) + K(ln N + ln K) + ln 2mt


2(e  2)t

"t
c2



(3)

we have:

(˜⇢exp

t

)  (1 + c)s 2(e  2)(N I⇢exp

t

where mt = ln⇣q (e2)t

 ⌘ / ln(c)  and for all ⇢exp

ln 2

t

(S; A) + K(ln N + ln K) + ln 2mt
 )

t"t

+ K"t+1 
(4)
that do not satisfy (3)  with the same probability:

+

ln 1
✏t+1
t

2(N I⇢exp

t

(S; A) + K(ln N + ln K) + ln 2mt
 )

(˜⇢exp

t

) 

t"t

+

ln 1
✏t+1
t

+ K"t+1.

.
and not ˜⇢exp
Note that the mutual information in Theorem 1 is calculated with respect to ⇢exp
Theorem 1 allows to tune the learning rate t based on the sample. It also provides an instantaneous
regret bound for any algorithm that plays the policies {˜⇢exp
  . . .} throughout the game. In
order to obtain such a bound we just have to take a decreasing sequence {"1  "2  . . .} and substitute
 in Theorem 1 with t = 
t(t+1). Then  by the union bound  the result holds with probability greater
than 1   for all rounds of the game simultaneously. This leads to Algorithm 1 for stochastic
multiarmed bandits with side information. Note that each round of the algorithm takes O(K) time.
Theorem 1 is based on the following regret decomposition and the subsequent theorem and two
lemmas that bound the three terms in the decomposition.

  ˜⇢exp

1

2

t

t

t

(˜⇢exp

) = [(⇢exp

(5)
Theorem 2. Under the conditions of Theorem 1 on {⇡1  . . .   ⇡t} and c  simultaneously for all
policies ⇢ that satisfy (3) with probability greater than 1  :

)  R(˜⇢exp

)  ˆt(⇢exp

)] + ˆt(⇢exp

) + [R(⇢exp

)].

t

t

t

t

t

(⇢)  ˆt(⇢)  (1 + c)s 2(e  2)(N I⇢(S;A) + K(ln N + ln K) + ln 2mt

t"t

 )

 

(6)

4

Algorithm 1: Algorithm for stochastic contextual bandits. (See text for deﬁnitions of "t and t.)
Input: N  K
ˆR(a  s) 0 for all a  s (These are cumulative [unnormalized] rewards)
K for all a
⇢(a) 1
n(s) 0 for all s
t 1
while not terminated do
if"t  1
⇢(a|St) ⇢(a) for all a

K or (n(St) = 0) then

Observe state St.

else

⇢(a|St) (1  K"t)
⇢(a) N1
N ⇢(a) + 1

⇢(a)et ˆR(a St)/n(St)

Pa0 ⇢(a0)et ˆR(a0 St)/n(St) + "t for all a
N ⇢(a|St) for all a

Draw action At according to ⇢(a|St) and play it.
Observe reward Rt.
n(St) n(St) + 1
ˆR(At  St) ˆR(At  St) + Rt
t t + 1

⇢(At|St)

and for all ⇢ that do not satisfy (3) with the same probability:

(⇢)  ˆt(⇢) 

2(N I⇢(S; A) + K(ln N + ln K) + ln 2mt
 )

t"t

.

Note that Theorem 2 holds for all possible ⇢-s  including those that do not have an exponential form.
Lemma 1. For any distribution ⇢exp

of the form (1)  where ⇢t(a)  ✏ for all a  we have:

t

ˆt(⇢exp

t

) 

ln 1
✏
t

.

Lemma 2. Let ˜⇢ be an "-smoothed version of a policy ⇢  such that ˜⇢(a|s) = (1  K")⇢(a|s) + " 
then

R(⇢)  R(˜⇢)  K".

t

ln 1
✏t+1

depends on the trade-off between its complexity  N I⇢exp

Proof of Theorem 2 is provided in Section 5 and proofs of Lemmas 1 and 2 are provided in the
supplementary material.
Comments on Theorem 1. Theorem 1 exhibits what we were looking for: the regret of a policy
(S; A)  and the empirical regret  which
˜⇢exp
t
. We note that 0  I⇢t(S; A)  ln K  hence  the result is interesting when
is bounded by 1
t
N  K  since otherwise K ln K term in the bound neutralizes the advantage we get from having
small mutual information values. The assumption that N  K is reasonable for many applications.
We believe that the dependence of the ﬁrst term of the regret bound (4) on "t is an artifact of our
crude upper bound on the variance of the sampling process (given in Lemma 3 in the proof of The-
orem 2) and that this term should not be in the bound. This is supported by an empirical study of
stochastic multiarmed bandits (Seldin et al.  2011a). With the current bound the best choice for "t
is "t = (Kt)1/3  which  by integration over the game rounds  yields O(K1/3t2/3) dependence of
the cumulative regret on the number of arms and game rounds. However  if we manage to derive a
tighter analysis and remove "t from the ﬁrst term in (4)  the best choice of "t will be "t = (Kt)1/2
and the dependence of the cumulative regret on the number of arms and time horizon will improve
to O((Kt)1/2). One way to achieve this is to apply EXP3.P-style updates (Auer et al.  2002b)  how-
ever  Seldin et al. (2011a) empirically show that in stochastic environments EXP3 algorithm of Auer
et al. (2002b)  which is closely related to Algorithm 1  has signiﬁcantly better performance. Thus 
it is desirable to derive a better analysis for EXP3 algorithm in stochastic environments. We note

5

(a|s) and thus higher mutual information values I⇢exp

that although UCB algorithm for stochastic multiarmed bandits (Auer et al.  2002a) is asymptoti-
cally better than the EXP3 algorithm  it is not compatible with PAC-Bayesian analysis and we are
not aware of a way to derive a UCB-type algorithm and analysis for multiarmed bandits with side
information  whose dependence on the number of states would be better than O(N ln K). Seldin
et al. (2011a) also demonstrate that empirically it takes a large number of rounds until the asymptotic
advantage of UCB over EXP3 translates into a real advantage in practice.
It is not trivial to minimize (4) with respect to t analytically. Generally  higher values of t decrease
the second term of the bound  but also lead to more concentrated policies (conditional distributions)
(S; A). A simple way to address this
⇢exp
t
trade-off is to set t such that the contribution of the second term is as close to the contribution of
the ﬁrst term as possible. This can be approximated by taking the value of mutual information from
the previous round (or approximation of the value of mutual information from the previous round).
More details on parameter setting for the algorithm are provided in the supplementary material.
Comments on Algorithm 1. By regret decomposition (5) and Theorem 2  regret at round t + 1 is
minimized by a policy ⇢t(a|s) that minimizes a certain trade-off between the mutual information
I⇢(S; A) and the empirical regret ˆRt(⇢). This trade-off is analogical to rate-distortion trade-off in
information theory (Cover and Thomas  1991). Minimization of rate-distortion trade-off is achieved
by iterative updates of the following form  which are known as Blahut-Arimoto (BA) algorithm:

t

⇢BA
t

(a|s) =

(a)et ˆRt(a s)

(a)et ˆRt(a s)

⇢BA
t

Pa ⇢BA

t

 

⇢BA
t

(a) =

1

N Xs

⇢BA
t

(a|s).

Running a similar type of iterations in our case would be prohibitively expensive  since they require
iteration over all states s 2 S at each round of the game. We approximate these iterations by
approximating the marginal distribution over the actions by a running average:

˜⇢exp
t+1(a) =

N  1
N

˜⇢exp
t

(a) +

1
N

˜⇢exp
t

(a|St).

(7)

t

t

(a|s) is bounded from zero by a decreasing sequence "t+1  the same automatically holds
t+1(a) (meaning that in Theorem 1 ✏t = "t). Note that Theorem 1 holds for any choice of

Since ⇢exp
t
for ˜⇢exp
⇢t(a)  including (7).
(a) propagates information between different states  but Theo-
We point out an interesting fact: ⇢exp
K   which corresponds to application of EXP3
rem 1 also holds for the uniform distribution ⇢(a) = 1
algorithm in each state independently. If these independent multiarmed bandits independently con-
verge to similar strategies  we still get a tighter regret bound. This happens because the correspond-
ing subspace of the hypothesis space is signiﬁcantly smaller than the total hypothesis space  which
enables us to put a higher prior on it (Seldin and Tishby  2010). Nevertheless  propagation of infor-
mation between states via the distribution ⇢exp
(a) helps to achieve even faster convergence of the
regret  as we can see from the experiments in the next section.
Comparison with state-of-the-art. We are not aware of algorithms for stochastic multiarmed ban-
dits with side information. The best known to us algorithm for adversarial multiarmed bandits with

side information is EXP4.P by Beygelzimer et al. (2011). EXP4.P has O(pKt ln|H|) regret and
O(K|H|) complexity per game round. In our case |H| = KN  which means that EXP4.P would
have O(pKtN ln K) regret and O(KN +1) computational complexity. For hard problems  where
all side information has to be used  our regret bound is inferior to the regret bound of Beygelzimer
et al. (2011) due to O(t2/3) dependence on the number of game rounds. However  we believe that
this can be improved by a more careful analysis of the existing algorithm. For simple problems
the dependence of our regret bound on the number of states is signiﬁcantly better  up to the point
that when the side information is irrelevant for the task we can get O(pK ln N ) dependence on the
number of states versus O(pN ln K) in EXP4.P. For N  K this leads to tighter regret bounds for

small t even despite the “incorrect” dependence on t of our bound  and if we improve the analysis
it will lead to tighter regret bounds for all t. As we said it already  our algorithm is able to ﬁlter
relevant information from large amounts of side information automatically  whereas in EXP4.P the
usage of side information has to be restricted externally through the construction of the hypothesis
class.

6

4

15x 10

H(Ah*)=0
H(Ah*)=1
H(Ah*)=2
H(Ah*)=3
Baseline

10

)
t
(
∆

5

 

0
0

 

)

p
x
te
~ρ
(
∆
n
o

 

 

d
n
u
o
B

1

2
t

(a) (t)

3

4
6
x 10

2.5

2

1.5

1

 

0.5
0

 

H(Ah*)=0
H(Ah*)=1
H(Ah*)=2
H(Ah*)=3

)

A
S

;

(

t

ρ

I

1

2
t

3

4
6
x 10

(b) bound on (˜⇢exp

t

)

2.5

2

1.5

1

0.5

 

0
0

H(Ah*)=0
H(Ah*)=1
H(Ah*)=2
H(Ah*)=3

 

1

2
t

3

4
6
x 10

(c) I⇢exp

t

(S; A)

) 
Figure 1: Behavior of: (a) cumulative regret (t)  (b) bound on instantaneous regret (˜⇢exp
and (c) the approximation of mutual information I⇢exp
(S; A). “Baseline” in the ﬁrst graph cor-
responds to playing N independent multiarmed bandits  one in each state. Each line in the graphs
corresponds to an average over 10 repetitions of the experiment.

t

t

The second important advantage of our algorithm is the exponential improvement of computational
complexity. This is achieved by switching from the space of experts to the state-action space in all
our calculations.

4 Experiments

(S; A) is approximated by a running average (see supplementary material for details).

experiment for T = 4  000  000 rounds and calculate the cumulative regret (t) =Pt

We present an experiment on synthetic data that illustrates our results. We take N = 100  K = 20  a
uniform distribution over states (p(s) = 0.01)  and consider four settings  with H(Ah⇤) = ln(1) =
0  H(Ah⇤) = ln(3) ⇡ 1  H(Ah⇤) = ln(7) ⇡ 2  and H(Ah⇤) = ln(20) ⇡ 3  respectively. In the
ﬁrst case  the same action is the best in all states (and hence H(Ah⇤) = 0 for the optimal hypothesis
h⇤). In the second case  for the ﬁrst 33 states the best action is number 1  for the next 33 states
the best action is number 2  and for the rest third of the states the best action is number 3 (thus 
depending on the state  one of the three actions is the “best” and H(Ah⇤) = ln(3)). In the third
case  there are seven groups of 14 states and each group has its own best action. In the last case 
there are 20 groups of 5 states and each of K = 20 actions is the best in exactly one of the 20
groups. For all states  the reward of the best action in a state has Bernoulli distribution with bias 0.6
and the rewards of all other actions in that state have Bernoulli distribution with bias 0.5. We run the
⌧ =1 (˜⇢exp
)
and instantaneous regret bound given in (4). For computational efﬁciency  the mutual information
I⇢exp
As we can see from the graphs (see Figure 1)  the algorithm exhibits sublinear cumulative regret
(put attention to the axes’ scales). Furthermore  for simple problems (with small H(Ah⇤)) the
regret grows slower than for complex problems. “Baseline” in Figure 1.a shows the performance
of an algorithm with the same parameter values that runs N multiarmed bandits  one in each state
independently of other states. We see that for all problems except the hardest one our algorithm
performs better than the baseline and for the hardest problem it performs almost as good as the
baseline. The regret bound in Figure 1.b provides meaningful values for the simplest problem after
1 million rounds (which is on average 500 samples per state-action pair) and after 4 million rounds
for all the problems (the graph starts at t = 10  000). Our estimates of the mutual information
(S; A) reﬂect H(Ah⇤) for the corresponding problems (for H(Ah⇤) = 0 it converges to zero 
I⇢exp
for H(Ah⇤) ⇡ 1 it is approximately one  etc.).
5 Proof of Theorem 2

⌧

t

t

The proof of Theorem 2 is based on PAC-Bayes-Bernstein inequality for martingales (Seldin et al. 
2011b). Let KL(⇢kµ) denote the KL-divergence between two distributions (Cover and Thomas 
1991). Let {Z1(h)  . . .   Zn(h) : h 2 H} be martingale difference sequences indexed by h with
respect to the ﬁltration (U1)  . . .   (Un)  where Ui = {Z1(h)  . . .   Zi(h) : h 2 H} is the subset
of martingale difference variables up to index i and (Ui) is the -algebra generated by Ui. This
means that E[Zi(h)|(Ui1)] = 0  where Zi(h) may depend on Zj(h0) for all j < i and h0 2 H.
There might also be interdependence between {Zi(h) : h 2 H}. Let ˆMi(h) = Pi
j=1 Zj(h) be

7

the corresponding martingales. Let Vi(h) =Pi
j=1 E[Zj(h)2|(Uj1)] be cumulative variances of
the martingales ˆMi(h). For a distribution ⇢ over H deﬁne ˆMi(⇢) = E⇢(h)[ ˆMi(h)] and Vt(⇢) =
E⇢(h)[Vt(h)] as weighted averages of the martingales and their cumulative variances according to a
distribution ⇢.
Theorem 3 (PAC-Bayes-Bernstein Inequality). Assume that |Zi(h)|  b for all h with probability
1. Fix a prior distribution µ over H. Pick an arbitrary number c > 1. Then with probability greater
than 1   over Un  simultaneously for all distributions ⇢ over H that satisfy

s KL(⇢kµ) + ln 2m
(e  2)Vn(⇢) 



1
cb

we have

where m = ln⇣q (e2)n

ln 2

| ˆMn(⇢)|  (1 + c)s(e  2)Vn(⇢)✓KL(⇢kµ) + ln
 ⌘ / ln(c)  and for all other ⇢
 ◆ .

| ˆMn(⇢)|  2b✓KL(⇢kµ) + ln

2m

2m

 ◆ 

⌧

Note that Mt(h) = t((h)  ˆt(h)) are martingales and their cumulative variances are Vt(h) =
⌧ =1 E⇥[Rh⇤(S⌧ ) S⌧
Pt
) 1 a prior µ(h) over H  and calculate (or upper bound) the
have to derive an upper bound on Vt(⇢exp
KL-divergence KL(⇢exp
Lemma 3. If {"1  "2  . . .} is a decreasing sequence  such that "t  mina s ⇡t(a|s)  then for all h:

]  [R(h⇤)  R(h)]2T⌧1⇤. In order to apply Theorem 3 we

 Rh(S⌧ ) S⌧
t kµ). This is done in the following three lemmas.

⌧

t

Vt(h) 

2t
"t

.

t

"t

)  2t

The proof of the lemma is provided in the supplementary material. Lemma 3 provides an imme-
diate  but crude  uniform upper bound on Vt(h)  which yields Vt(⇢exp
. Since our algorithm
concentrates on h-s with small (h)  which  in turn  concentrate on the best action in each state  the
variance Vt(h) for the corresponding h-s is expected to be of the order of 2Kt and not 2t
. However 
"t
we were not able to prove yet that the probability ⇢exp
(h) of the remaining hypotheses (those with
large (h)) gets sufﬁciently small (of order K"t)  so that the weighted cumulative variance would
be of order 2Kt. Nevertheless  this seems to hold in practice starting from relatively small values of
t (Seldin et al.  2011a). Improving the upper bound on Vt(⇢exp
) will improve the regret bound  but
for the moment we present the regret bound based on the crude upper bound Vt(⇢exp
The remaining two lemmas  which deﬁne a prior µ over H and bound KL(⇢kµ)  are due to Seldin
and Tishby (2010).
Lemma 4. It is possible to deﬁne a distribution µ over H that satisﬁes:

)  2t

"t

.

t

t

t

Lemma 5. For the distribution µ that satisﬁes (8) and any distribution ⇢(a|s):

KL(⇢kµ)  N I⇢(S; A) + K ln N + K ln K.

µ(h)  eN H(Ah)K ln NK ln K.

(8)

Substitution of the upper bounds on Vt(⇢exp

t

) and KL(⇢exp

t kµ) into Theorem 3 yields Theorem 2.

6 Discussion

We presented PAC-Bayesian analysis of stochastic multiarmed bandits with side information. Our
analysis provides data-dependent algorithm and data-dependent regret analysis for this problem.
The selection of task-relevant side information is delegated from the user to the algorithm. We also
provide a general framework for deriving data-dependent algorithms and analyses for many other
stochastic problems with limited feedback. The analysis of the variance of our algorithm still waits
to be improved and will be addressed in future work.

1Seldin et al. (2011b) show that Vn(⇢) can be replaced by an upper bound everywhere in Theorem 3.

8

Acknowledgments

We would like to thank all the people with whom we discussed this work and  in particular  Nicol`o
Cesa-Bianchi  G´abor Bart´ok  Elad Hazan  Csaba Szepesv´ari  Miroslav Dud´ık  Robert Shapire  John
Langford  and the anonymous reviewers  whose comments helped us to improve the ﬁnal version of
this manuscript. This work was supported in part by the IST Programme of the European Commu-
nity  under the PASCAL2 Network of Excellence  IST-2007-216886  and by the European Commu-
nity’s Seventh Framework Programme (FP7/2007-2013)  under grant agreement N o231495. This
publication only reﬂects the authors’ views.

References
Peter Auer  Nicol`o Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning  47  2002a.

Peter Auer  Nicol`o Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic multiarmed bandit

problem. SIAM Journal of Computing  32(1)  2002b.

Arindam Banerjee. On Bayesian bounds. In Proceedings of the International Conference on Machine Learning

(ICML)  2006.

Alina Beygelzimer  John Langford  Lihong Li  Lev Reyzin  and Robert Schapire. Contextual bandit algo-
rithms with supervised learning guarantees. In Proceedings on the International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS)  2011.

Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. John Wiley & Sons  1991.
Leslie Pack Kaelbling. Associative reinforcement learning: Functions in k-DNF. Machine Learning  15  1994.
John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In Advances

in Neural Information Processing Systems (NIPS)  2007.

David McAllester. Some PAC-Bayesian theorems. In Proceedings of the International Conference on Compu-

tational Learning Theory (COLT)  1998.

Matthias Seeger. PAC-Bayesian generalization error bounds for Gaussian process classiﬁcation. Journal of

Machine Learning Research  2002.

Yevgeny Seldin and Naftali Tishby. PAC-Bayesian analysis of co-clustering and beyond. Journal of Machine

Learning Research  11  2010.

Yevgeny Seldin  Nicol`o Cesa-Bianchi  Peter Auer  Franc¸ois Laviolette  and John Shawe-Taylor. PAC-Bayes-
Bernstein inequality for martingales and its application to multiarmed bandits. 2011a. In review. Preprint
available at http://arxiv.org/abs/1110.6755.

Yevgeny Seldin  Franc¸ois Laviolette  Nicol`o Cesa-Bianchi  John Shawe-Taylor  and Peter Auer. PAC-Bayesian

inequalities for martingales. 2011b. In review. Preprint available at http://arxiv.org/abs/1110.6886.

John Shawe-Taylor and Robert C. Williamson. A PAC analysis of a Bayesian estimator. In Proceedings of the

International Conference on Computational Learning Theory (COLT)  1997.

John Shawe-Taylor  Peter L. Bartlett  Robert C. Williamson  and Martin Anthony. Structural risk minimization

over data-dependent hierarchies. IEEE Transactions on Information Theory  44(5)  1998.

Alexander L. Strehl  Chris Mesterharm  Michael L. Littman  and Haym Hirsh. Experience-efﬁcient learning in
associative bandit problems. In Proceedings of the International Conference on Machine Learning (ICML) 
2006.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press  1998.
Naftali Tishby and Daniel Polani. Information theory of decisions and actions. In Vassilis Cutsuridis  Amir
Hussain  John G. Taylor  and Daniel Polani  editors  Perception-Reason-Action Cycle: Models  Algorithms
and Systems. Springer  2010.

Naftali Tishby  Fernando Pereira  and William Bialek. The information bottleneck method. In Allerton Con-

ference on Communication  Control and Computation  1999.

Leslie G. Valiant. A theory of the learnable. Communications of the Association for Computing Machinery  27

(11)  1984.

9

,David Woodruff
Wataru Kumagai