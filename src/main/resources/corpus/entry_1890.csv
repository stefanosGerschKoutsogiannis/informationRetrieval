2014,Exact Post Model Selection Inference for Marginal Screening,We develop a framework for post model selection inference  via marginal screening  in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response $y$  conditional on the model being selected (``condition on selection framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in high-dimensional statistics  our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix $X$. Furthermore  the computational cost of marginal regression  constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression  thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework  this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit and marginal screening+Lasso.",Exact Post Model Selection Inference for Marginal

Screening

Jason D. Lee

Computational and Mathematical Engineering

Stanford University
Stanford  CA 94305

jdl17@stanford.edu

Jonathan E. Taylor

Department of Statistics

Stanford University
Stanford  CA 94305

jonathan.taylor@stanford.edu

Abstract

We develop a framework for post model selection inference  via marginal screen-
ing  in linear regression. At the core of this framework is a result that charac-
terizes the exact distribution of linear functions of the response y  conditional on
the model being selected (“condition on selection" framework). This allows us
to construct valid conﬁdence intervals and hypothesis tests for regression coef-
ﬁcients that account for the selection procedure.
In contrast to recent work in
high-dimensional statistics  our results are exact (non-asymptotic) and require no
eigenvalue-like assumptions on the design matrix X. Furthermore  the computa-
tional cost of marginal regression  constructing conﬁdence intervals and hypoth-
esis testing is negligible compared to the cost of linear regression  thus making
our methods particularly suitable for extremely large datasets. Although we focus
on marginal screening to illustrate the applicability of the condition on selection
framework  this framework is much more broadly applicable. We show how to
apply the proposed framework to several other selection procedures including or-
thogonal matching pursuit and marginal screening+Lasso.

1

Introduction

Consider the model

yi = µ(xi) + i  i ∼ N (0  σ2I) 

(1)
where µ(x) is an arbitrary function  and xi ∈ Rp. Our goal is to perform inference on
(X T X)−1X T µ  which is the best linear predictor of µ.
In the classical setting of n > p   the
least squares estimator ˆβ = (X T X)−1X T y is a commonly used estimator for (X T X)−1X T µ.
Under the linear model assumption µ = Xβ0  the exact distribution of ˆβ is

j using the z-test.

(2)
j = 0 and form conﬁdence intervals

ˆβ ∼ N (β0  σ2(X T X)−1).
Using the normal distribution  we can test the hypothesis H0 : β0
for β0
However in the high-dimensional p > n setting  the least squares estimator is an underdetermined
problem  and the predominant approach is to perform variable selection or model selection [4].
There are many approaches to variable selection including AIC/BIC  greedy algorithms such as
forward stepwise regression  orthogonal matching pursuit  and regularization methods such as the
Lasso. The focus of this paper will be on the model selection procedure known as marginal screen-
ing  which selects the k most correlated features xj with the response y.
Marginal screening is the simplest and most commonly used of the variable selection procedures
[9  21  16]. Marginal screening requires only O(np) computation and is several orders of magnitude

1

faster than regularization methods such as the Lasso; it is extremely suitable for extremely large
datasets where the Lasso may be computationally intractable to apply. Furthermore  the selection
properties are comparable to the Lasso [8].
Since marginal screening utilizes the response variable y  the conﬁdence intervals and statistical
tests based on the distribution in (2) are not valid; conﬁdence intervals with nominal 1− α coverage
may no longer cover at the advertised level:

Pr(cid:0)β0

j ∈ C1−α(x)(cid:1) < 1 − α.

S XS)−1X T

Several authors have previously noted this problem including recent work in [13  14  15  2]. A major
line of work [13  14  15] has described the difﬁculty of inference post model selection: the distri-
bution of post model selection estimates is complicated and cannot be approximated in a uniform
sense by their asymptotic counterparts.
In this paper  we describe how to form exact conﬁdence intervals for linear regression coefﬁcients
post model selection. We assume the model (1)  and operate under the ﬁxed design matrix X
setting. The linear regression coefﬁcients constrained to a subset of variables S is linear in µ 
S µ = ηT µ for some η. We derive the conditional distribution of ηT y for any
eT
j (X T
vector η  so we are able to form conﬁdence intervals for regression coefﬁcients.
In Section 2 we discuss related work on high-dimensional statistical inference  and Section 3 in-
troduces the marginal screening algorithm and shows how z intervals may fail to have the correct
coverage properties. Section 4 and 5 show how to represent the marginal screening selection event
as constraints on y  and construct pivotal quantities for the truncated Gaussian. Section 6 uses these
tools to develop valid conﬁdence intervals  and Section 7 evaluates the methodology on two real
datasets.
Although the focus of this paper is on marginal screening  the “condition on selection" framework 
ﬁrst proposed for the Lasso in [12]  is much more general; we use marginal screening as a simple and
clean illustration of the applicability of this framework. In Section 8  we discuss several extensions
including how to apply the framework to other variable/model selection procedures and to nonlinear
regression problems. Section 8 covers 1) marginal screening+Lasso  a screen and clean procedure
that ﬁrst uses marginal screening and cleans with the Lasso  and orthogonal matching pursuit (OMP).

2 Related Work

Most of the theoretical work on high-dimensional linear models focuses on consistency. Such results
establish  under restrictive assumptions on X  the Lasso ˆβ is close to the unknown β0 [19] and
selects the correct model [26  23  11]. We refer to the reader to [4] for a comprehensive discussion
about the theoretical properties of the Lasso.
There is also recent work on obtaining conﬁdence intervals and signiﬁcance testing for penalized M-
estimators such as the Lasso. One class of methods uses sample splitting or subsampling to obtain
conﬁdence intervals and p-values [24  18]. In the post model selection literature  the recent work of
[2] proposed the POSI approach  a correction to the usual t-test conﬁdence intervals by controlling
the familywise error rate for all parameters in any possible submodel. The POSI methodology is
extremely computationally intensive and currently only applicable for p ≤ 30.
A separate line of work establishes the asymptotic normality of a corrected estimator obtained by
“inverting” the KKT conditions [22  25  10]. The corrected estimator ˆb has the form ˆb = ˆβ + λ ˆΘˆz 
where ˆz is a subgradient of the penalty at ˆβ and ˆΘ is an approximate inverse to the Gram matrix
X T X. The two main drawbacks to this approach are 1) the conﬁdence intervals are valid only when
the M-estimator is consistent  and thus require restricted eigenvalue conditions on X  2) obtaining
ˆΘ is usually much more expensive than obtaining ˆβ  and 3) the method is speciﬁc to regularized
estimators  and does not extend to marginal screening  forward stepwise  and other variable selection
methods.
Most closely related to our work is the “condition on selection" framework laid out in [12] for the
Lasso. Our work extends this methodology to other variable selection methods such as marginal
screening  marginal screening followed by the Lasso (marginal screening+Lasso) and orthogonal
matching pursuit. The primary contribution of this work is the observation that many model selection

2

methods  including marginal screening and Lasso  lead to “selection events" that can be represented
as a set of constraints on the response variable y. By conditioning on the selection event  we can
characterize the exact distribution of ηT y. This paper focuses on marginal screening  since it is
the simplest of variable selection methods  and thus the applicability of the “condition on selection
event" framework is most transparent. However  this framework is not limited to marginal screening
and can be applied to a wide a class of model selection procedures including greedy algorithms such
as orthogonal matching pursuit. We discuss some of these possible extensions in Section 8  but leave
a thorough investigation to future work.
A remarkable aspect of our work is that we only assume X is in general position  and the test is exact 
meaning the distributional results are true even under ﬁnite samples. By extension  we do not make
any assumptions on n and p  which is unusual in high-dimensional statistics [4]. Furthermore  the
computational requirements of our test are negligible compared to computing the linear regression
coefﬁcients.

3 Marginal Screening
Let X ∈ Rn×p be the design matrix  y ∈ Rn the response variable  and assume the model
yi = µ(xi) + i  i ∼ N (0  σ2I). We will assume that X is in general position and has unit norm
columns. The algorithm estimates ˆβ via Algorithm 1. The marginal screening algorithm chooses

Algorithm 1 Marginal screening algorithm
1: Input: Design matrix X  response y  and model size k.
2: Compute |X T y|.
3: Let ˆS be the index of the k largest entries of |X T y|.
4: Compute ˆβ ˆS = (X T
ˆS

X ˆS)−1X T

y

ˆS

the k variables with highest absolute dot product with y  and then ﬁts a linear model over those k
variables. We will assume k ≤ min(n  p). For any ﬁxed subset of variables S  the distribution of
ˆβS = (X T

S XS)−1(cid:1)

S µ  σ2(X T

(3)
S)j  where j is indexing a variable in the set S. The z-test

S y is

S XS)−1X T

ˆβS ∼ N(cid:0)(X T
(cid:16) ˆβj∈S − σz1−α/2(X T
and each interval has 1 − α coverage  meaning Pr(cid:0)β(cid:63)

We will use the notation β(cid:63)
j∈S := (β(cid:63)
intervals for a regression coefﬁcient are

S XS)−1X T

C(α  j  S) :=

chosen using a model selection procedure that depends on y  the distributional result (3) no longer
< 1− α.
holds and the z-test intervals will not cover at the 1− α level  and Pr

∈ C(α  j  ˆS)

S XS)jj  ˆβj∈S + σz1−α/2(X T

j∈S ∈ C(α  j  S)(cid:1) = 1 − α. However if ˆS is

S XS)jj

(4)

(cid:16)

β(cid:63)
j∈ ˆS

(cid:17)

(cid:17)

3.1 Failure of z-test conﬁdence intervals
We will illustrate empirically that the z-test intervals do not cover at 1 − α when ˆS is chosen by
marginal screening in Algorithm 1. For this experiment we generated X from a standard normal
2 = SNR  y = Xβ0 +   and
with n = 20 and p = 200. The signal vector is 2 sparse with β0
 ∼ N (0  1). The conﬁdence intervals were constructed for the k = 2 variables selected by the
marginal screening algorithm. The z-test intervals were constructed via (4) with α = .1  and the
adjusted intervals were constructed using Algorithm 2. The results are described in Figure 1.

1   β0

4 Representing the selection event

Since Equation (3) does not hold for a selected ˆS when the selection procedure depends on y  the
z-test intervals are not valid. Our strategy will be to understand the conditional distribution of y

3

Figure 1: Plots of the coverage proportion across a range of SNR (log-scale). We see that the
coverage proportion of the z intervals can be far below the nominal level of 1 − α = .9  even at
SNR =5. The adjusted intervals always have coverage proportion .9. Each point represents 500
independent trials.

and contrasts (linear functions of y) ηT y  then construct inference conditional on the selection event
ˆE. We will use ˆE(y) to represent a random variable  and E to represent an element of the range of
ˆE(y). In the case of marginal screening  the selection event ˆE(y) corresponds to the set of selected
variables ˆS and signs s:

(cid:110)

ˆE(y) =

(cid:110)
(cid:110)

=

=

i y)xT

y : sign(xT
i y > ±xT

y : ˆsixT
y : A( ˆS  ˆs)y ≤ b( ˆS  ˆs)

j y and ˆsixT

i y > ±xT
(cid:111)

j y for all i ∈ ˆS and j ∈ ˆSc(cid:111)
i y ≥ 0 for all i ∈ ˆS and j ∈ ˆSc(cid:111)

(5)

for some matrix A( ˆS  ˆs) and vector b( ˆS  ˆs)1. We will use the selection event ˆE and the selected
variables/signs pair ( ˆS  ˆs) interchangeably since they are in bijection.

: A(S  s)y ≤
b(S  s)}2. The vector y can be decomposed with respect to the partition as follows y =

The space Rn is partitioned by the selection events  Rn = (cid:70)
(cid:80)
S s y 1 (A(S  s)y ≤ b(S  s)).
y|{ ˆE(y) = E} d= z(cid:12)(cid:12){A(S  s)z ≤ b}  z ∼ N (µ  σ2I).

Theorem 4.1. The distribution of y conditional on the selection event is a constrained Gaussian 

conditional y(cid:12)(cid:12){A(S  s)y ≤ b(S  s)} is a Gaussian constrained to the set {A(S  s)y ≤ b(S  s)}.

Proof. The event E is in bijection with a pair (S  s)  and y is unconditionally Gaussian. Thus the

(S s){y

5 Truncated Gaussian test

This section summarizes the recent tools developed in [12] for testing contrasts3 ηT y of a con-
strained Gaussian y. The results are stated without proof and the proofs can be found in [12]. The
primary result is a one-dimensional pivotal quantity for ηT µ. This pivot relies on characterizing the
distribution of ηT y as a truncated normal. The key step to deriving this pivot is the following lemma:
Lemma 5.1. The conditioning set can be rewritten in terms of ηT y as follows:

{Ay ≤ b} = {V−(y) ≤ ηT y ≤ V +(y) V 0(y) ≥ 0}

1b can be taken to be 0 for marginal screening  but this extra generality is needed for other model selection

methods.

2It is also possible to use a coarser partition  where each element of the partition only corresponds to a

subset of variables S. See [12] for details.

3A contrast of y is a linear function of the form ηT y.

4

−1010.40.50.60.70.80.91log10 SNRCoverage Proportion AdjustedZ testwhere

α =

AΣη
ηT Ση
V− = V−(y) = max

j: αj <0

bj − (Ay)j + αjηT y

αj

bj − (Ay)j + αjηT y

(6)

(7)

(8)

(9)

V + = V +(y) = min
V 0 = V 0(y) = min
Moreover  (V + V− V 0) are independent of ηT y.
Theorem 5.2. Let Φ(x) denote the CDF of a N (0  1) random variable  and let F [a b]
CDF of T N (µ  σ  a  b)  i.e.:

αj
bj − (Ay)j

j: αj >0

j: αj =0

.

Then F [V− V +]

F [a b]

µ σ2 (x) =

Φ((x − µ)/σ) − Φ((a − µ)/σ)
Φ((b − µ)/σ) − Φ((a − µ)/σ)
ηT µ  ηT Ση(ηT y) is a pivotal quantity  conditional on {Ay ≤ b}:

ηT µ  ηT Ση(ηT y)(cid:12)(cid:12) {Ay ≤ b} ∼ Unif(0  1)

F [V− V +]

where V− and V + are deﬁned in (7) and (8).

µ σ2 denote the

.

(10)

(11)

Figure 2: Histogram and qq plot of F [V− V +]
distribution is very close to Unif(0  1)  which is in agreement with Theorem 5.2.

ηT µ  ηT Ση(ηT y) where y is a constrained Gaussian. The

6

Inference for marginal screening

In this section  we apply the theory summarized in Sections 4 and 5 to marginal screening.
particular  we will construct conﬁdence intervals for the selected variables.
To summarize the developments so far  recall that our model (1) says that y ∼ N (µ  σ2I).
The distribution of interest is y|{ ˆE(y) = E}  and by Theorem 4.1 
this is equivalent to
y|{A(S  s)z ≤ b(S  s)}  where y ∼ N (µ  σ2I). By applying Theorem 5.2  we obtain the pivotal
quantity

In

(ηT y)(cid:12)(cid:12) { ˆE(y) = E} ∼ Unif(0  1)

(12)

for any η  where V− and V + are deﬁned in (7) and (8).
In this section  we describe how to form conﬁdence intervals for the components of β(cid:63)
=
ˆS
  and
(X T
ˆS
ˆβ ˆS = (X T
ˆS

µ. The best linear predictor of µ that uses only the selected variables is β(cid:63)
ˆS

y is an unbiased estimate of β(cid:63)
ˆS

X ˆS)−1X T

X ˆS)−1X T

. If we choose

ˆS

ˆS

ηj = ((X T
ˆS

X ˆS)−1X T

ˆS

ej)T  

(13)

F [V− V +]
ηT µ  σ2||η||2

2

5

00.20.40.60.8105010015020025030000.20.40.60.8100.20.40.60.81 empirical cdfUnif(0 1) cdfj µ = β(cid:63)

then ηT
the model ˆS.

j∈ ˆS

  so the above framework provides a method for inference about the jth variable in

6.1 Conﬁdence intervals for selected variables

(cid:18)

Next  we discuss how to obtain conﬁdence intervals
to obtain an interval

to invert a pivotal quantity [5].

(cid:19)

(cid:12)(cid:12) { ˆE = E}

j∈ ˆS

for β(cid:63)

The standard way
since
= α one can deﬁne a (1 − α) (conditional)

.
In other words 

α

2 ≤ F [V− V +]

Pr
conﬁdence interval for β(cid:63)

j∈ ˆS

β(cid:63)

is
j y) ≤ 1 − α
  σ2||ηj||2 (ηT
as(cid:110)

j  ˆE

2

x :

α
2

≤ F [V− V +]

x  σ2||ηj||2 (ηT

j y) ≤ 1 − α
2

.

(14)

(cid:111)

In fact  F is monotone decreasing in x  so to ﬁnd its endpoints  one need only solve for the root of a
smooth one-dimensional function. The monotonicity is a consequence of the fact that the truncated
Gaussian distribution is a natural exponential family and hence has monotone likelihood ratio in µ
[17].
We now formalize the above observations in the following result  an immediate consequence of
Theorem 5.2.
Corollary 6.1. Let ηj be deﬁned as in (13)  and let Lα = Lα(ηj  ( ˆS  ˆs)) and Uα = Uα(ηj  ( ˆS  ˆs))
be the (unique) values satisfying
F [V− V +]
Lα  σ2||ηj||2(ηT

j y) =

(15)

j y) = 1 − α
2

α
2

F [V− V +]
Uα  σ2||ηj||2 (ηT
  conditional on ˆE:

Then [Lα  Uα] is a (1 − α) conﬁdence interval for β(cid:63)

P(cid:16)

β(cid:63)
j∈ ˆS

∈ [Lα  Uα](cid:12)(cid:12) { ˆE = E}(cid:17)

j∈ ˆS

= 1 − α.

(16)

Proof. The conﬁdence region of β(cid:63)
1− α level. The function F [V− V +]
most extreme values where H0 is still accepted. This gives a 1 − α conﬁdence interval.

is the set of βj such that the test of H0 : β(cid:63)
accepts at the
j y) is monotone in x  so solving for Lα and Uα identify the

x  σ2||ηj||2(ηT

j∈ ˆS

j∈ ˆS

Next  we establish the unconditional coverage of the constructed conﬁdence intervals and the false
coverage rate (FCR) control [1].
Corollary 6.2. For each j ∈ ˆS 

(17)

(cid:16)

(cid:17)

α](cid:9)

∈ [Lj
α  U j

α  U j
α]
j∈ ˆE is α.

= 1 − α.

Pr

β(cid:63)
j∈ ˆS

Furthermore  the FCR of the intervals(cid:8)[Lj
(cid:88)
(cid:88)

∈ [Lj

α  U j
α]

β(cid:63)
j∈ ˆS

(cid:17)

=

E

Proof. By (16)  the conditional coverage of the conﬁdence intervals are 1 − α. The coverage holds
for every element of the partition { ˆE(y) = E}  so
β(cid:63)
j∈ ˆS

∈ [Lα  Uα](cid:12)(cid:12) { ˆE = E}(cid:17)

Pr( ˆE = E)

(cid:16)

(cid:16)

Pr

Pr

=

(1 − α) Pr( ˆE = E) = 1 − α.

E

Remark 6.3. We would like to emphasize that the previous Corollary shows that the constructed
conﬁdence intervals are unconditionally valid. The conditioning on the selection event ˆE(y) = E
was only for mathematical convenience to work out the exact pivot. Unlike standard z-test intervals 
the coverage target  β(cid:63)
  and the interval [Lα  Uα] are random. In a typical conﬁdence interval
only the interval is random; however in the post-selection inference setting  the selected model is
random  so both the interval and the target are necessarily random [2].

j∈ ˆS

We summarize the algorithm for selecting and constructing conﬁdence intervals below.

6

Algorithm 2 Conﬁdence intervals for selected variables
1: Input: Design matrix X  response y  model size k.
2: Use Algorithm 1 to select a subset of variables ˆS and signs ˆs = sign(X T
ˆS
3: Let A = A( ˆS  ˆs) and b = b( ˆS  ˆs) using (5). Let ηj = (X T
ˆS
4: Solve for Lj

)†ej.

α and U j

α using Equation (15) where V− and V + are computed via (7) and (8) using

y).

the A  b  and ηj previously deﬁned.

5: Output: Return the intervals [Lj

α] for j ∈ ˆS.

α  U j

7 Experiments

In Figure 1  we have already seen that the conﬁdence intervals constructed using Algorithm 2 have
exactly 1 − α coverage proportion. In this section  we perform two experiments on real data where
the linear model does not hold  the noise is not Gaussian  and the noise variance is unknown.

7.1 Diabetes dataset

The diabetes dataset contains n = 442 diabetes patients measured on p = 10 baseline variables [6].
The baseline variables are age  sex  body mass index  average blood pressure  and six blood serum
measurements  and the response y is a quantitative measure of disease progression measured one
(cid:107)y−ˆy(cid:107)
year after the baseline. Since the noise variance σ2 is unknown  we estimate it by σ2 =
n−p  

Figure 3: Plot of 1 − α vs the coverage proportion for diabetes dataset. The nominal curve is the
line y = x. The coverage proportion of the adjusted intervals agree with the nominal coverage level 
but the z-test coverage proportion is strictly below the nominal level. The adjusted intervals perform
well  despite the noise being non-Gaussian  and σ2 unknown.

where ˆy = X ˆβ and ˆβ = (X T X)−1X T y. For each trial we generated new responses ˜yi = X ˆβ + ˜ 
and ˜ is bootstrapped from the residuals ri = yi − ˆyi. We used marginal screening to select k = 2
variables  and then ﬁt linear regression on the selected variables. The adjusted conﬁdence intervals
were constructed using Algorithm 2 with the estimated σ2. The nominal coverage level is varied
across 1 − α ∈ {.5  .6  .7  .8  .9  .95  .99}. From Figure 3  we observe that the adjusted intervals
always cover at the nominal level  whereas the z-test is always below. The experiment was repeated
2000 times.

7.2 Riboﬂavin dataset

Our second data example is a high-throughput genomic dataset about riboﬂavin (vitamin B2) pro-
duction rate [3]. There are p = 4088 variables which measure the log expression level of different
genes  a single real-valued response y which measures the logarithm of the riboﬂavin production
rate  and n = 71 samples. We ﬁrst estimate σ2 by using cross-validation [20]  and apply marginal
screening with k = 30  as chosen in [3]. We then use Algorithm 2 to identify genes signiﬁcant at

7

0.60.810.20.40.60.811−αCoverage Proportion Z−testAdjustedNominalα = 10%. The genes identiﬁed as signiﬁcant were YCKE_at  YOAB_at  and YURQ_at. After
using Bonferroni to control for FWER  we found YOAB_at remained signiﬁcant.

8 Extensions

The purpose of this section is to illustrate the broad applicability of the condition on selection frame-
work. For expository purposes  we focused the paper on marginal screening where the framework
is particularly easy to understand. In the rest of this section  we show how to apply the framework
to marginal screening+Lasso  and orthogonal matching pursuit. This is a non-exhaustive list of
selection procedures where the condition on selection framework is applicable  but we hope this in-
complete list emphasizes the ease of constructing tests and conﬁdence intervals post-model selection
via conditioning.

8.1 Marginal screening + Lasso

The marginal screening+Lasso procedure was introduced in [7] as a variable selection method for
the ultra-high dimensional setting of p = O(enk
). Fan et al. [7] recommend applying the marginal
screening algorithm with k = n − 1  followed by the Lasso on the selected variables. This is a
two-stage procedure  so to properly account for the selection we must encode the selection event
of marginal screening followed by Lasso. This can be done by representing the two stage selection
as a single event. Let ( ˆSm  ˆsm) be the variables and signs selected by marginal screening  and the
( ˆSL  ˆzL) be the variables and signs selected by Lasso [12]. In Proposition 2.2 of [12]  it is shown
how to encode the Lasso selection event ( ˆSL  ˆzL) as a set of constraints {ALy ≤ bL} 4  and in
Section 4 we showed how to encode the marginal screening selection event ( ˆSm  ˆsm) as a set of
constraints {Amy ≤ bm}. Thus the selection event of marginal screening+Lasso can be encoded
as {ALy ≤ bL  Amy ≤ bm}. Using these constraints  the hypothesis test and conﬁdence intervals
described in Algorithm 2 are valid for marginal screening+Lasso.

8.2 Orthogonal Matching Pursuit

Orthogonal matching pursuit (OMP) is a commonly used variable selection method. At each itera-
tion  OMP selects the variable most correlated with the residual r  and then recomputes the residual
using the residual of least squares using the selected variables. Similar to Section 4  we can represent
the OMP selection event as a set of linear constraints on y.

ˆE(y) =(cid:8)y : sign(xT

= {y : ˆsixT
ˆsixT
pi

(I − X ˆSi−1

pi

j ri  for all j (cid:54)= pi and all i ∈ [k](cid:9)

ri > ±xT
†
X
ˆSi−1
)y > 0  for all j (cid:54)= pi  and all i ∈ [k] }

j (I − X ˆSi−1

)y > ±xT

†
ˆSi−1

X

)y and

ri)xT
pi

pi

(I − X ˆSi−1

X

†
ˆSi−1

The selection event encodes that OMP selected a certain variable and the sign of the correlation of
that variable with the residual  at steps 1 to k. The primary difference between the OMP selection
event and the marginal screening selection event is that the OMP event also describes the order at
which the variables were chosen.

9 Conclusion

Due to the increasing size of datasets  marginal screening has become an important method for
fast variable selection. However  the standard hypothesis tests and conﬁdence intervals used in
linear regression are invalid after using marginal screening to select important variables. We have
described a method to form conﬁdence intervals after marginal screening. The condition on selection
framework is not restricted to marginal screening  and also applies to OMP and marginal screening
+ Lasso. The supplementary material also discusses the framework applied to non-negative least
squares.

4The Lasso selection event is with respect to the Lasso optimization problem after marginal screening.

8

References
[1] Yoav Benjamini and Daniel Yekutieli. False discovery rate–adjusted multiple conﬁdence intervals for

selected parameters. Journal of the American Statistical Association  100(469):71–81  2005.

[2] Richard Berk  Lawrence Brown  Andreas Buja  Kai Zhang  and Linda Zhao. Valid post-selection infer-

ence. Annals of Statistics  41(2):802–837  2013.

[3] Peter Bühlmann  Markus Kalisch  and Lukas Meier. High-dimensional statistics with a view toward

applications in biology. Statistics  1  2014.

[4] Peter Lukas Bühlmann and Sara A van de Geer. Statistics for High-dimensional Data. Springer  2011.
[5] George Casella and Roger L Berger. Statistical inference  volume 70. Duxbury Press Belmont  CA  1990.
[6] Bradley Efron  Trevor Hastie  Iain Johnstone  and Robert Tibshirani. Least angle regression. The Annals

of statistics  32(2):407–499  2004.

[7] Jianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature space. Journal

of the Royal Statistical Society: Series B (Statistical Methodology)  70(5):849–911  2008.

[8] Christopher R Genovese  Jiashun Jin  Larry Wasserman  and Zhigang Yao. A comparison of the lasso

and marginal regression. The Journal of Machine Learning Research  98888:2107–2143  2012.

[9] Isabelle Guyon and André Elisseeff. An introduction to variable and feature selection. The Journal of

Machine Learning Research  3:1157–1182  2003.

[10] Adel Javanmard and Andrea Montanari. Conﬁdence intervals and hypothesis testing for high-dimensional

regression. arXiv preprint arXiv:1306.3171  2013.

[11] Jason Lee  Yuekai Sun  and Jonathan E Taylor. On model selection consistency of penalized m-estimators:

a geometric theory. In Advances in Neural Information Processing Systems  pages 342–350  2013.

[12] Jason D Lee  Dennis L Sun  Yuekai Sun  and Jonathan E Taylor. Exact inference after model selection

via the lasso. arXiv preprint arXiv:1311.6238  2013.

[13] Hannes Leeb and Benedikt M Pötscher. The ﬁnite-sample distribution of post-model-selection estimators

and uniform versus nonuniform approximations. Econometric Theory  19(1):100–142  2003.

[14] Hannes Leeb and Benedikt M Pötscher. Model selection and inference: Facts and ﬁction. Econometric

Theory  21(1):21–59  2005.

[15] Hannes Leeb and Benedikt M Pötscher. Can one estimate the conditional distribution of post-model-

selection estimators? The Annals of Statistics  pages 2554–2591  2006.

Leek.

[16] Jeff
tors.
prediction-the-lasso-vs-just-using-the-top-10.

predic-
http://simplystatistics.tumblr.com/post/18132467723/

Prediction:

vs

just

lasso

the

using

the

top

10

[17] Erich L. Lehmann and Joseph P. Romano. Testing Statistical Hypotheses. Springer  3 edition  2005.
[18] Nicolai Meinshausen  Lukas Meier  and Peter Bühlmann. P-values for high-dimensional regression. Jour-

nal of the American Statistical Association  104(488)  2009.

[19] Sahand N Negahban  Pradeep Ravikumar  Martin J Wainwright  and Bin Yu. A uniﬁed framework
for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science 
27(4):538–557  2012.

[20] Stephen Reid  Robert Tibshirani  and Jerome Friedman. A study of error variance estimation in lasso

regression. arXiv preprint arXiv:1311.5274  2013.

[21] Virginia Goss Tusher  Robert Tibshirani  and Gilbert Chu. Signiﬁcance analysis of microarrays applied
to the ionizing radiation response. Proceedings of the National Academy of Sciences  98(9):5116–5121 
2001.

[22] Sara van de Geer  Peter Bühlmann  and Ya’acov Ritov. On asymptotically optimal conﬁdence regions and

tests for high-dimensional models. arXiv preprint arXiv:1303.0518  2013.

[23] M.J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using (cid:96)1-constrained

quadratic programming (lasso). 55(5):2183–2202  2009.

[24] Larry Wasserman and Kathryn Roeder. High dimensional variable selection. Annals of statistics 

37(5A):2178  2009.

[25] Cun-Hui Zhang and S Zhang.

Conﬁdence intervals for low-dimensional parameters with high-

dimensional data. arXiv preprint arXiv:1110.2563  2011.

[26] P. Zhao and B. Yu. On model selection consistency of lasso. 7:2541–2563  2006.

9

,Jason Lee
Jonathan Taylor