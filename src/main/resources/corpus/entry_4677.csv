2008,Relative Margin Machines,In classification problems  Support Vector Machines maximize the margin of separation between two classes. While the paradigm has been successful  the solution obtained by SVMs is dominated by the directions with large data spread and biased to separate the classes by cutting along large spread directions. This article proposes a novel formulation to overcome such sensitivity and maximizes the margin relative to the spread of the data. The proposed formulation can be efficiently solved and experiments on digit datasets show drastic performance improvements over SVMs.,Relative Margin Machines

Pannagadatta K Shivaswamy and Tony Jebara

Department of Computer Science  Columbia University  New York  NY

pks2103 jebara@cs.columbia.edu

Abstract

In classiﬁcation problems  Support Vector Machines maximize the margin
of separation between two classes. While the paradigm has been success-
ful  the solution obtained by SVMs is dominated by the directions with
large data spread and biased to separate the classes by cutting along large
spread directions. This article proposes a novel formulation to overcome
such sensitivity and maximizes the margin relative to the spread of the
data. The proposed formulation can be eﬃciently solved and experiments
on digit datasets show drastic performance improvements over SVMs.

1 Introduction

The goal of most machine learning problems is to generalize from a limited number of
training examples. For example  in support vector machines [10] (SVMs) a hyperplane 1
of the form w⊤x + b = 0  w ∈ Rm  x ∈ Rm  b ∈ R is recovered as a decision boundary
after observing a limited number of training examples. The parameters of the hyperplane
(w  b) are estimated by maximizing the margin (the distance between w⊤x + b = 1 and
w⊤x + b = −1) while minimizing a weighted upper bound on the misclassiﬁcation rate on
the training data (the so called slack variables). In practice  the margin is maximized by
minimizing 1

2 w⊤w.

While this works well in practice  we point out that merely changing the scale of the data
can give a diﬀerent solution. On one hand  an adversary can exploit this shortcoming to
transform the data so as to give bad performance. More distressingly  this shortcoming
can naturally lead to a bad performance especially in high dimensional settings. The key
problem is that SVMs simply ﬁnd a large margin solution giving no attention to the spread
of the data. An excellent discriminator lying in a dimension with relatively small data
spread may be easily overlooked by the SVM solution.
In this paper  we propose novel
formulations to overcome such a limitation. The crux here is to ﬁnd the maximum margin
solution with respect to the spread of the data in a relative sense rather than ﬁnding the
absolute large margin solution.

Linear discriminant analysis ﬁnds a projection of the data so that the inter-class separation
is large while within class scatter is small. However  it only makes use of the ﬁrst and
the second order statistics of the data. Feature selection with SVMs [12] remove that have
low discriminative value. Ellipsoidal kernel machines [9] normalize data in feature space
by estimating bounding ellipsoids. While these previous methods showed performance im-
provements  both relied on multiple-step locally optimal algorithms for interleaving spread
information with margin estimation. Recently  additional examples were used to improve
the generalization of the SVMs with so called “Universum” samples [11]. Instead of leverag-
ing additional data or additional model assumptions such as axis-aligned feature selection 

1In this paper we use the dot product w

⊤

x with the understanding that it can be replaced with

an inner product.

1

the proposed method overcomes what seems to be a fundamental limitation of the SVMs
and subsequently yield improvements in the same supervised setting. In addition  the for-
mulations derived in this paper are convex  can be eﬃciently solved and admit some useful
generalization bounds.

Notation Boldface letters indicate vectors/matrices. For two vectors u ∈ Rm and v ∈ Rm 
u ≤ v indicates that ui ≤ vi for all i from 1 to m. 1  0 and I denote the vectors of all ones 
all zeros and the identity matrix respectively. Their dimensions are clear from the context.

4

3

2

1

0

−1

−2

−3

−4
−4

−3

−2

−1

0

1

2

3

4

4

3

2

1

0

−1

−2

−3

−4
−4

−3

−2

−1

0

1

2

3

4

4

3

2

1

0

−1

−2

−3

−4
−4

−3

−2

−1

0

1

2

3

4

Figure 1: Top: As the data is scaled along the x-axis  the SVM solution (red or dark shade)
deviates from the maximum relative margin solution (green or light shade). Bottom: The
projections of the examples in the top row on the real line for the SVM solution (red or
dark shade) and the proposed classiﬁer (green or light shade) in each case.

2 Motivation with a two dimensional example

Let us start with a simple two dimensional toy dataset to illustrate a problem with the
SVM solution. Consider the binary classiﬁcation example shown in the top row of Figure
1 where squares denote examples from one class and triangles denote examples from the
other class. Consider the leftmost plot in the top row of Figure 1. One possible decision
boundary separating the two classes is shown in green (or light shade). The solution shown
in red (or dark shade) is the SVM estimate; it achieves the largest margin possible while
still separating both the classes. Is this necessarily “the best” solution?

Let us now consider the same set of points after scaling the x-axis in the second and the
third plots. With progressive scaling  the SVM increasingly deviates from the green solution 
clearly indicating that the SVM decision boundary is sensitive to aﬃne transformations of
the data and produces a family of diﬀerent solutions as a result. This sensitivity to scaling
and aﬃne transformations is worrisome. If there is a best and a worst solution in the family
of SVM estimates  there is always the possibility that an adversary exploits this scaling such
that the SVM solution we recover is poor. Meanwhile  an algorithm producing the green
decision boundary remains resilient to such adversarial scalings.

In the previous example  a direction with a small spread in the data produced a good
discriminator. Merely ﬁnding a large margin solution  on the other hand  does not recover
the best possible discriminator. This particular weakness in large margin estimation has
only received limited attention in previous work. In the above example  suppose each class is
generated from a one dimensional distribution on a line with the two classes on two parallel
lines. In this case  the green decision boundary should obtain zero test error even if it is
estimated from a ﬁnite number of samples. However  for ﬁnite training data  the SVM
solution will make errors and will do so increasingly as the data is scaled along the x-axis.
Using kernels and nonlinear mappings may help in some cases but might also exacerbate
such problems. Similarly  simple prepossessing of the data (aﬃne “whitening” to make the

2

dataset zero mean and unit covariance or scaling to place the data into a zero-one box) may
fail to resolve such problems.

For more insight  consider the uni-dimensional projections of the data given by the green and
red solutions in the bottom row of Figure 1. In the green solution  all points in the ﬁrst class
are mapped to a single coordinate and all points in the other class are mapped to another
(distinct) coordinate. Meanwhile  the red solution produces more dispersed projections of
the two classes. As the adversarial scaling is increased  the spread of the projection in the
SVM solution increases correspondingly. Large margins are not suﬃcient on their own and
what is needed is a way to also control the spread of the data after projection. Therefore 
rather than just maximizing the margin  a trade-oﬀ regularizer should also be used to
minimize the spread of the projected data.
In other words  we will couple large margin
estimation with regularization which seeks to bound the spread |w⊤x + b| of the data. This
will allow the linear classiﬁer to recover large margin solutions not in the absolute sense but
rather relative to the spread of the data in that projection direction.

3 Formulations

Given (xi  yi)n
i=1 where xi ∈ Rm and yi ∈ {±1} drawn independent and identically dis-
tributed from a distribution Pr(x  y)  the Support Vector Machine primal formulation 2 is
as follows:

min

w b ξ≥0

1
2kwk2 + Cξ⊤1 s.t. yi(w⊤xi + b) ≥ 1 − ξi  ∀1 ≤ i ≤ n.

(1)

The above formulation minimizes an upper bound on the misclassiﬁcation while maximizing
the margin (the two quantities are traded oﬀ by C). In practice  the following dual of the
formulation (1) is solved:

0≤α≤C1 −
max

1
2

n

n

Xi=1

Xj=1

αiαjyiyj x⊤

i xj +

n

Xi=1

αi s.t. α⊤y = 0.

(2)

It is easy to see that the above formulation (2) is rotation invariant; if all the xi are replaced
by Axi where A ∈ Rm×m  A⊤A = I  then the solution remains the same. However  the
solution is not guaranteed to be the same when A is not a rotation matrix. In addition  the
solution is sensitive to translations as well.
Typically  the dot product between the examples is replaced by a kernel function k : Rm ×
Rm → R such that k(xi  xj) = φ(xi)⊤φ(xj )  where φ : Rm → H is a mapping to a Hilbert
space to obtain non-linear decision boundaries in the input space. Thus  in (2)  x⊤
i xj is
replaced by k(xi  xj ) to obtain non-linear solutions.
In rest of this paper  we denote by
K ∈ Rn×n the Gram matrix  whose individual entries are given by Kij = k(xi  xj).
Next  we consider the formulation which corresponds to whitening the data with the covari-
nPn
ance matrix. Denote by Σ = 1
i=1 xi  the
sample covariance and mean respectively. Consider the following formulation which we call
Σ-SVM:
2 wk2 + Cξ⊤1 s.t. yi(w⊤(xi − µ) + b) ≥ 1 − ξi 

nPn
D
2 kΣ

j   and µ = 1

n2 Pn

i=1 xiPn

i=1 xix⊤

i − 1

kwk2 +

j=1 x⊤

(3)

min

w b ξ≥0

1 − D

2

1

where 0 ≤ D ≤ 1 is an additional parameter that trades oﬀ between the two regularization
terms.

The dual of (3) can be shown to be:

max

0≤α≤C1 y⊤α=0

n

Xi=1

αi −

1
2

n

Xi=1

αiyi(xi − µ)⊤((1 − D)I + DΣ)−1

n

Xj=1

αjyj(xj − µ).

(4)

2After this formulation  we stop explicitly writing ∀1 ≤ i ≤ n since it will be obvious from the

context.

3

It is easy to see that the above formulation (4) is translation invariant and tends to an aﬃne
invariant solution when D tends to one. When 0 < D < 1  it can be shown  by using the
Woodbury matrix inversion formula  that the above formulation can be “kernelized” simply
by replacing the dot products x⊤

i xj in (2) by:

1

1 − D  k(xi  xj ) −
1 − D  (cid:18)Ki −

K⊤
i 1
n −
n (cid:19)⊤(cid:18) I
n −

K1

1

K⊤
j 1
n

+

1⊤K1

n2 !
n2 (cid:19)(cid:20) 1 − D

D

11⊤

I + K(cid:18) I
n −

−

−1

11⊤

n2 (cid:19)(cid:21)

(cid:18)Kj −

K1

n (cid:19)!  

where Ki is the ith column of K. For D = 0 and D = 1  it is much easier to obtain the
kernelized formulations. Note that the above formula involves a matrix inversion of size n 
making the kernel computation alone O(n3).

3.1 RMM and its geometrical interpretation

From Section 2  it is clear that large margin in the absolute sense might be deceptive and
could merely be a by product of bad scaling of the data. To overcome this limitation  as
we pointed out earlier  we need to bound the projections of the training examples as well.
As in the two dimensional example  it is necessary to trade oﬀ between the margin and the
spread of the data. We propose a slightly modiﬁed formulation in the next section that
can be solved eﬃciently. For now  we write the following formulation  mainly to show how
it compares with the Σ-SVM. In addition  writing the dual of the following formulation
gives some geometric intuition. Since we trade oﬀ between the projections and the margin 
implicitly  we ﬁnd large relative margin. Thus we call the following formulation the Relative
Margin Machine (RMM):

min

w b ξ≥0

1
2kwk2 + Cξ⊤1 s.t. yi(w⊤xi + b) ≥ 1 − ξi 

1
2

(w⊤xi + b)2 ≤

B2
2

.

(5)

This is a quadratically constrained quadratic problem (QCQP). This formulation has one
extra parameter B in addition to the SVM parameter. Note that B ≥ 1 since having a
B less than one would mean none of the examples would satisfy yi(w⊤xi + b) ≥ 1. Let
wC and bC be the solutions obtained by solving the SVM (1) for a particular value of C 
then B > maxi |w⊤
C xi + bC|  makes the constraint on the second line in the formulation (5)
inactive for each i and the solution obtained is the same as the SVM estimate.

For smaller B values  we start getting diﬀerent solutions. Speciﬁcally  with a smaller B  we
still ﬁnd a large margin solution such that all the projections of the training examples are
bounded by B. Thus by trying out diﬀerent B values  we explore diﬀerent large margin
solutions with respect to the projection and spread of the data.

In the following  we assume that the value of B is smaller than the threshold mentioned
above. The Lagrangian of (5) is given by:

n

n

B2(cid:19)  
1
2kwk2 + Cξ⊤1 −
where α  β  λ ≥ 0 are the Lagrange multipliers corresponding to the constraints. Diﬀer-
entiating with respect to the primal variables and equating them to zero  it can be shown
that:

αi(cid:0)yi(w⊤xi + b) − 1 + ξi(cid:1) − β⊤ξ +

(w⊤xi + b)2 −

λi(cid:18) 1

Xi=1

Xi=1

1
2

2

n

n

n

n

n

(I+

λixi =

λixix⊤

Xi=1

αiyixi  b =

Xi=1
λ⊤1Pn
i − 1
the dual of (5) can be shown to be:

Xi=1
i )w−b
Denoting by Σλ =Pn
αi −

i=1 λixiPn
αiyi(xi − µλ)⊤(I + Σλ)−1

i=1 λixix⊤

0≤α≤C1 λ≥0

max

1
2

(

n

n

Xi=1

1

λ⊤1

Xi=1

Xi=1

n

Xj=1

4

λiw⊤xi)  C1 = α+β.

αiyi−

Xi=1

j=1 λj x⊤

j   and by µλ = 1

j=1 λj xj

λ⊤1Pn

αjyj(xj − µλ) −

1
2

B2λ⊤1 (6)

Note that the above formulation is translation invariant since µλ is subtracted from each xi.
Σλ corresponds to a “shape matrix” (potentially low rank) determined by xi’s that have
non-zero λi. From the KKT conditions of (5)  λi( 1
2 ) = 0. Consequently
λi > 0 implies ( 1

2 (w⊤xi + b)2 − B 2

2 ) = 0.

2 (w⊤xi + b)2 − B 2

Geometrically  in the above formulation (6)  the data is whitened with the matrix (I + Σλ)
while solving SVM. While this is similar to what is done by the Σ-SVM  the matrix (I+ Σλ)
is determined jointly considering both the margin of the data and the spread. In contrast 
in Σ-SVM  whitening is simply a prepossessing step which can be done independently of the
margin. Note that the constraint 1
2 B2 can be relaxed with slack variables at
the expense of one additional parameter however this will not be investigated in this paper.

2 (w⊤xi +b)2 ≤ 1

The proposed formulation is of limited use unless it can be solved eﬃciently. Solving (6)
amounts to solving a semi-deﬁnite program; it cannot scale beyond a few hundred data
points. Thus  for eﬃcient solution  we consider a diﬀerent but equivalent formulation.

2 (w⊤xi + b)2 ≤ 1

Note that the constraint 1
2 B2 can be equivalently posed as two linear
constraints : (w⊤xi + b) ≤ B and −(w⊤xi + b) ≤ B. With these constraints replacing
the quadratic constraint  we have a quadratic program to solve. In the primal  we have 4n
constraints (including ξ ≥ 0 ) instead of the 2n constraints in the SVM. Thus  solving RMM
as a standard QP has the same order of complexity as the SVM. In the next section  we
brieﬂy explain how the RMM can be solved eﬃciently from the dual.

3.2 Fast algorithm

The main idea for the fast algorithm is to have linear constraints bounding the projections
rather than quadratic constraints. The fast algorithm that we developed is based on SVMlight
[5]. We ﬁrst write the equivalent of (5) with linear constraints:

min

w b ξ≥0

1
2kwk2 + Cξ⊤1 s.t. yi(w⊤xi + b) ≥ 1 − ξi  w⊤xi + b ≤ B  − w⊤xi − b ≤ B. (7)

The dual of (7) can be shown to be the following:

1
2

(α ⊗ y − λ + λ∗)⊤ K (α ⊗ y − λ + λ∗) + α⊤1 − Bλ⊤1 − Bλ∗⊤1

α λ λ∗ −
max
s.t. α⊤y − λ⊤1 + λ∗⊤1 = 0  0 ≤ α ≤ C1  λ  λ∗ ≥ 0 

(8)

where  the operator ⊗ denotes the element-wise product of two vectors.
The above QP (8) is solved in an iterative way. In each step  only a subset of the dual
variables are optimized. Let us say  q  r and s (˜q  ˜r and ˜s) are the indices to the free (ﬁxed)
variables in α  λ and λ∗ respectively (such that q ∪ ˜q = {1  2 ··· n} and q ∩ ˜q = ∅  similarly
for the other two indices) in a particular iteration. Then the optimization over the free
variables in that step can be expressed as:

max

αq  λr  λ∗

s −

⊤

⊤

#
#

1

λr
λ∗
s

2" αq ⊗ yq
2" αq ⊗ yq
q yq − λ⊤

λr
λ∗
s

1

−
s.t. α⊤

λr
λ∗
s

Ksq −Ksr Kss #" αq ⊗ yq
" Kqq −Kqr Kqs
−Krq Krr −Krs
" Kq ˜q −Kq ˜r Kq ˜s
Ks ˜q −Ks˜r Ks˜s #" α ˜q ⊗ y ˜q
−Kr ˜q Kr ˜r −Kr ˜s
s 1 = −α⊤

˜r 1 − λ∗⊤

˜q y ˜q + λ⊤

r 1 + λ∗⊤

(9)

#
# + α⊤

λ˜r
λ∗
˜s

q 1 − Bλ⊤
˜s 1  0 ≤ αq ≤ C1  λr  λ∗

s ≥ 0.

r 1 − Bλ∗⊤
s 1

Note that while the ﬁrst term in the objective above is quadratic in the free variables (over
which it is optimized)  the second term is only linear.

The algorithm  solves a small sub-problem like (9) in each step until the KKT conditions
of the formulation (8) are satisﬁed to a given tolerance. In each step  the free variables are
selected using heuristics similar to those in SVMlight but slightly adapted to our formulation.

5

We omit the details due to lack of space. Since only a small subset of the variables is
optimized  book-keeping can be done eﬃciently in each step. Moreover  the algorithm can
be warm-started with a previous solution just like SVMlight.

4 Experiments

Experiments were carried out on three sets of digits - optical digits from the UCI machine
learning repository [1]  USPS digits [6] and MNIST digits [7]. These datasets have diﬀerent
number of features (64 in optical digits  256 in USPS and 784 in MNIST) and training
examples (3823 in optical digits  7291 in USPS and 60000 in MNIST). In all these multi-
class experiments one versus one classiﬁcation strategy was used. We start by noting that 
on the MNIST test set  an improvement of 0.1% is statistically signiﬁcant [3  4]. This
corresponds to 10 or fewer errors by one method over another on the MNIST test set.

All the parameters were tuned by splitting the training data in each case in the ratio 80:20
and using the smaller split for validation and the larger split for training. The process
was repeated ﬁve times over random splits to pick best parameters (C for SVM  C and
D for Σ-SVM and C and B for RMM). A ﬁnal classiﬁer was trained for each of the 45
classiﬁcation problems with the best parameters found from cross validation using all the
training examples in those classes.

In the case of MNIST digits  training Σ-SVM and KLDA are prohibitive since they involve
inverting a matrix. So  to compare all the methods  we conducted an experiment with 1000
examples per training. For the larger experiments we simply excluded Σ-SVM and KLDA.
The larger experiment on MNIST consisted of training with two thirds of the digits (note
that this amounts to training with 8000 examples on an average for each pair of digits) for
each binary classiﬁcation task. In both the experiments  the remaining training data was
used as a validation set. The classiﬁer that performed the best on the validation set was
used for testing.

Once we had 45 classiﬁers for each pair of digits  testing was done on the separate test set
available in each of these three datasets (1797 examples in the case of optical digits  2007
examples in USPS and 10000 examples in MNIST). The ﬁnal prediction given for each test
example was based on the majority of predictions made by the 45 classiﬁers on the test
example with ties broken uniformly at random.

Table 1 shows the result on all the three datasets for polynomial kernel with various degrees
and the RBF kernel. For each dataset  we report the number of misclassiﬁed examples using
the majority voting scheme mentioned above. It can be seen that while Σ-SVM usually
performs much better compared to SVM  RMM performs even better than Σ-SVM in most
cases. Interestingly  with higher degree kernels  Σ-SVM seems to match the performance
of the RMM  but in most of the lower degree kernels  RMM outperforms both SVM and
Σ-SVM convincingly. Since  Σ-SVM is prohibitive to run on large scale datasets  the RMM
was clearly the most competitive method in these experiments.

Training with entire MNIST We used the best parameters found by crossvalidation
in the previous experiments on MNIST and trained 45 classiﬁers for both SVM and RMM
with all the training examples for each class in MNIST for various kernels. The test results
are reported in Table 1; the advantage still carries over to the full MNIST dataset.

4

3.5

3

2.5

2

1.5

1

0.5

0

 
3

SVM
RMM B1
RMM B2
RMM B3

 

3.1

3.2

3.3

3.4

3.5

3.6

3.7

3.8

3.9

4

Figure 2: Log run time versus log number of examples from 1000 to 10000 in steps of 1000.

6

1
SVM
71
Σ-SVM 61
71
KLDA
RMM
71
145
SVM
Σ-SVM 132
132
KLDA
153
RMM
SVM
696
Σ-SVM 671
1663
KLDA
689
RMM
552
SVM
RMM
534
536
SVM
RMM
521

2
57
48
57
36
109
108
119
109
511
470
848
342
237
164
198
146

3
54
41
54
32
109
99
121
94
422
373
591
319
200
148
170
140

4
47
36
47
31
103
94
117
91
380
341
481
301
183
140
156
130

5
40
35
40
33
100
89
114
91
362
322
430
298
178
123
157
119

6
46
31
46
30
95
87
118
90
338
309
419
290
177
129
141
116

7
46
29
46
29
93
90
117
90
332
303
405
296
164
129
136
115

RBF

51
47
45
51
104
97
101
98
670
673
1597
613
166
144
146
129

OPT

USPS

1000-MNIST

2/3-MNIST

Full MNIST

Table 1: Number of digits misclassiﬁed with various kernels by SVM  Σ-SVM and RMM
for three diﬀerent datasets.

Run time comparison We studied the empirical run times using the MNIST digits 3 vs
8 and polynomial kernel with degree two. The tolerance was set to 0.001 in both the cases.
The size of the sub-problem (9) solved was 500 in all the cases. The number of training
examples were increased in steps of 1000 and the training time was noted. C value was
set at 1000. SVM was ﬁrst run on the training examples. The value of maximum absolute
prediction θ was noted. We then tried three diﬀerent values of B for RMM  B1 = 1+(θ−1)/2 
B2 = 1 + (θ − 1)/4 B3 = 1 + (θ − 1)/10. In all the cases  the run time was noted. We show
a log-log plot comparing the number of examples to the run time in Figure 2. Both SVM
and RMM have similar asymptotic behavior. However  in many cases  warm starting RMM
with previous solution signiﬁcantly helped in reducing the run times.

5 Conclusions

We identiﬁed a sensitivity of Support Vector Machines and maximum absolute margin cri-
teria to aﬃne scalings. These classiﬁers are biased towards producing decision boundaries
that separate data along directions with large data spread. The Relative Margin Machine
was proposed to overcome such a problem and optimizes the projection direction such that
the margin is large only relative to the spread of the data. By deriving the dual with
quadratic constraints  a geometrical interpretation was also formulated for RMMs. An im-
plementation for RMMs requiring only additional linear constraints in the SVM quadratic
program leads to a competitively fast implementation. Experiments showed that while aﬃne
transformations can improve over the SVMs  RMM performs even better in practice.

The maximization of relative margin is fairly promising as it is compatible with other popular
problems handled by the SVM framework such as ordinal regression  structured prediction
etc. These are valuable future extensions for the RMM. Furthermore  the constraints that
bound the projection are unsupervised; thus RMMs can readily work in semi-supervised
and transduction problems. We will study these extensions in detail in an extended version
of this paper.

References

[1] A. Asuncion and D.J. Newman. UCI machine learning repository  2007.

[2] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3:463–482  2002.

[3] Y. Bengio  P. Lamblin  D. Popovici  and H. Larochelle. Greedy layer-wise training of deep
In Advances in Neural Information Processing Systems 19  pages 153–160. MIT

networks.
Press  Cambridge  MA  2007.

7

[4] D. Decoste and B. Sch¨olkopf. Training invariant support vector machines. Machine Learning 

pages 161–190  2002.

[5] T. Joachims. Making large-scale support vector machine learning practical. In Advances in

Kernel Methods: Support Vector Machines. MIT Press  Cambridge  MA  1998.

[6] Y. LeCun  B. Boser  J.S. Denker  D. Henderson  R.E. Howard  W. Hubbard  and L. Jackel.
Back-propagation applied to handwritten zip code recognition. Neural Computation  1:541–
551  1989.

[7] Y. LeCun  L. Bottou  Y. Bengio  and P. Haﬀner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[8] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univer-

sity Press  2004.

[9] P. K. Shivaswamy and T. Jebara. Ellipsoidal kernel machines. In Proceedings of the Artiﬁcial

Intelligence and Statistics  2007.

[10] V. Vapnik. The Nature of Statistical Learning Theory. Springer Verlag  New York  1995.

[11] J. Weston  R. Collobert  F. H. Sinz  L. Bottou  and V. Vapnik. Inference with the universum.
In Proceedings of the International Conference on Machine Learning  pages 1009–1016  2006.

[12] J. Weston  S. Mukherjee  O. Chapelle  M. Pontil  T. Poggio  and V. Vapnik. Feature selection

for SVMs. In Neural Information Processing Systems  pages 668–674  2000.

A Generalization Bound

In this section  we give the empirical Rademacher complexity [2  8] for function classes used
by the SVM  and modiﬁed versions of RMM and Σ-SVM which can be plugged into a
generalization bound.

Maximizing the margin can be seen as choosing a function f (x) = w⊤x from a bounded class
of functions FE := {x → w⊤x| 1
2kwk2 ≤ E}. For a technical reason  instead of bounding
the projection on the training examples as in (5)  we consider bounding the projections
on an independent set of examples drawn from Pr(x)  that is  a set U = {u1  u2  . . . unu}.
Note that if we have an iid training set  it can be split into two parts and one part can be
used exclusively to bound the projections and the other part can be used exclusively for
classiﬁcation constraints. Since the labels of the examples used to bound the projections
do not matter  we denote this set by U and the other part of the set by (xi  yi)n
i=1 We
now consider the following function class which is closely related to RMM: HE D := {x →
w⊤x| 1
2 (w⊤ui)2 ≤ E ∀1 ≤ i ≤ nu} where D > 0 trades oﬀ between large margin
and small bound on the projections. Similarly  consider: GE D := {x → w⊤x| 1
2 w⊤w +
2nu Pnu
i=1(w⊤ui)2 ≤ E}  which is closely related to the class of functions considered by
Σ-SVM. The empirical Rademacher complexities of the three classes of functions are as
below:

2 w⊤w + D

D

ˆR(FE) ≤ UFE :=

ˆR(GE D) ≤ UGE D :=

2√2E

n vuut

n

Xi=1

i Σ−1
x⊤

D xi 

n

2√2E

n vuut

ˆR(HE D) ≤ UHE D := min

λ≥0

n

1
n

x⊤

i xi 

i Σ−1
x⊤

Xi=1
Xi=1
i and Σλ D =Pnu

λ Dxi +

2
n

E

λi 

nu

Xi=1

i=1 uiu⊤

i=1 λiuiu⊤

nu Pnu

i=1 λiI + DPnu

where ΣD = I + D
i . Note that the last
upper bound is not a closed form expression  but a semi-deﬁnite optimization. Now  the
upper bounds UFE   UGE D and UHE D can be plugged in the following theorem in place of
ˆR(F ) to obtain Rademacher type generalization bounds.
Theorem 1 Fix γ > 0  let F be the class of functions from Rm × {±1} → R given by
f (x  y) = −yg(x). Let {(x1  y1)  . . .   (xn  yn)} be drawn iid from a probability distribution
D. Then  with probability at least 1− δ over the samples of size n  the following bound holds:
PrD[y 6= sign(g(x))] ≤ ξ⊤1/n + 2 ˆR(F )/γ + 3p(ln(2/δ))/2n  where ξi = max(0  1− yig(xi))
are the so-called slack variables.

8

,Ignacio Rocco
Mircea Cimpoi
Relja Arandjelović
Akihiko Torii
Tomas Pajdla
Josef Sivic