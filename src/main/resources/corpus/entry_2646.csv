2011,Infinite Latent SVM for Classification and Multi-task Learning,Unlike existing nonparametric Bayesian models  which rely solely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations  we study nonparametric Bayesian inference with regularization on the desired posterior distributions. While priors can indirectly affect posterior distributions through Bayes' theorem  imposing posterior regularization is arguably more direct and in some cases can be much easier. We particularly focus on developing infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM)  which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning  respectively. We present efficient inference methods and report empirical studies on several benchmark datasets. Our results appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics.,InﬁniteLatentSVMforClassiﬁcationandMulti-taskLearningJunZhu† NingChen† andEricP.Xing‡†Dept.ofComputerScience&Tech. TNListLab TsinghuaUniversity Beijing100084 China‡MachineLearningDepartment CarnegieMellonUniversity Pittsburgh PA15213 USAdcszj@tsinghua.edu.cn;chenn07@mails.thu.edu.cn;epxing@cs.cmu.eduAbstractUnlikeexistingnonparametricBayesianmodels whichrelysolelyonspeciallyconceivedpriorstoincorporatedomainknowledgefordiscoveringimprovedla-tentrepresentations westudynonparametricBayesianinferencewithregulariza-tiononthedesiredposteriordistributions.Whilepriorscanindirectlyaffectpos-teriordistributionsthroughBayes’theorem imposingposteriorregularizationisarguablymoredirectandinsomecasescanbemucheasier.Weparticularlyfo-cusondevelopinginﬁnitelatentsupportvectormachines(iLSVM)andmulti-taskinﬁnitelatentsupportvectormachines(MT-iLSVM) whichexplorethelarge-marginideaincombinationwithanonparametricBayesianmodelfordiscoveringpredictivelatentfeaturesforclassiﬁcationandmulti-tasklearning respectively.Wepresentefﬁcientinferencemethodsandreportempiricalstudiesonseveralbenchmarkdatasets.Ourresultsappeartodemonstratethemeritsinheritedfrombothlarge-marginlearningandBayesiannonparametrics.1IntroductionNonparametricBayesianlatentvariablemodelshaverecentlygainedremarkablepopularityinstatis-ticsandmachinelearning partlyowningtotheirdesirable“nonparametric”naturewhichallowspractitionersto“sidestep”thedifﬁcultmodelselectionproblem e.g. ﬁguringouttheunknownnum-berofcomponents(orclasses)inamixturemodel[2]ordeterminingtheunknowndimensionalityoflatentfeatures[12] byusinganappropriatepriordistributionwithalargesupport.AmongthemostcommonlyusedpriorsareGaussianprocess(GP)[24] Dirichletprocess(DP)[2]andIndianbuffetprocess(IBP)[12].However standardnonparametricBayesianmodelsarelimitedinthattheyusuallymakeverystrictandunrealisticassumptionsondata suchasthatobservationsbeinghomogeneousorexchangeable.AnumberofrecentdevelopmentsinBayesiannonparametricshaveattemptedtoalleviatesuchlimi-tations.Forexample tohandleheterogenousobservations predictor-dependentprocesses[20]havebeenproposed;andtorelaxtheexchangeabilityassumption variouscorrelationstructures suchashierarchicalstructures[26] temporalorspatialdependencies[5] andstochasticorderingde-pendencies[13 10] havebeenintroduced.However allthesemethodsrelysolelyoncraftinganonparametricBayesianpriorencodingsomespecialstructure whichcanindirectlyinﬂuencetheposteriordistributionofinterestviatrading-offwithlikelihoodmodels.Sinceitistheposteriordistributions whichcapturethelatentstructurestobelearned thatareofourultimateinterest anarguablymoredirectwaytolearnadesirablelatent-variablemodelistoimposeposteriorregular-ization(i.e. regularizationonposteriordistributions) aswewillexploreinthispaper.Anotherreasonforusingposteriorregularizationisthatinsomecasesitismorenaturalandeasiertoincor-poratedomainknowledge suchasthelarge-margin[15 31]ormanifoldconstraints[14] directlyonposteriordistributionsratherthanthroughpriors asshowninthispaper.Posteriorregularization usuallythroughimposingconstraintsontheposteriordistributionsoflatentvariablesorviasomeinformationprojection hasbeenwidelystudiedinlearningaﬁnitelog-linearmodelfrompartiallyobserveddata includinggeneralizedexpectation[21] posteriorregulariza-1tion[11] andalternatingprojection[6] allofwhicharedoingmaximumlikelihoodestimation(MLE)tolearnasinglesetofmodelparametersbyoptimizinganobjective.Recentattemptsto-wardlearningaposteriordistributionofmodelparametersincludethe“learningfrommeasure-ments”[19] maximumentropydiscrimination[15]andMedLDA[31].Butagain allthesemeth-odsarelimitedtoﬁniteparametricmodels.Toourknowledge veryfewattemptshavebeenmadetoimposeposteriorregularizationonnonparametricBayesianlatentvariablemodels.OneexceptionisourrecentworkofinﬁniteSVM(iSVM)[32] aDPmixtureoflarge-marginclassiﬁers.iSVMisalatentclassmodelthatassignseachdataexampletoasinglemixturecomponentforclassiﬁcationandtheunknownnumberofmixturecomponentsisautomaticallyresolvedfromdata.Inthispaper wepresentageneralformulationofperformingnonparametricBayesianinferencesubjecttoappropriateposteriorconstraints.Inparticular weconcentrateondevelopingtheinﬁ-nitelatentsupportvectormachines(iLSVM)andmulti-taskinﬁnitelatentsupportvectormachines(MT-iLSVM) whichexplorethediscriminativelarge-marginideatolearninﬁnitelatentfeaturemodelsforclassiﬁcationandmulti-tasklearning[3 4] respectively.Assuch ourmethodsaswellas[32]representanattempttopushforwardtheinterfacebetweenBayesiannonparametricsandlargemarginlearning whichhavecomplementaryadvantagesbuthavebeenlargelytreatedastwoseparatesubﬁeldsinthemachinelearningcommunity.Technically althoughitisintuitivelynatu-ralforMLE-basedmethodstoincludearegularizationtermontheposteriordistributionsoflatentvariables thisisnotstraightforwardforBayesianinferencebecausewedonothaveanoptimizationobjectivetoberegularized.WebaseourworkontheinterpretationoftheBayes’theorembyZell-ner[29] namely theBayes’theoremcanbereformulatedasaminimizationproblem.Underthisoptimizationframework weincorporateposteriorconstraintstodoregularizedBayesianinference withapenaltytermthatmeasurestheviolationoftheconstraints.BothiLSVMandMT-iLSVMarespecialcasesthatexplorethelarge-marginprincipletoconsidersupervisinginformationforlearn-ingpredictivelatentfeatures whicharegoodforclassiﬁcationormulti-tasklearning.WeusethenonparametricIBPpriortoallowthemodelstohaveanunboundednumberoflatentfeatures.Theregularizedinferenceproblemcanbeefﬁcientlysolvedwithaniterativeprocedure whichleveragesexistinghigh-performanceconvexoptimizationtechniques.RelatedWork:Asstatedabove bothiLSVMandMT-iLSVMgeneralizetheideasofiSVMtoinﬁnitelatentfeaturemodels.Formulti-tasklearning nonparametricBayesianmodelshavebeendevelopedin[28 23]forlearningfeaturessharedbymultipletasks.ButthesemethodsarebasedonstandardBayesianinference withouttheabilitytoconsiderposteriorregularization suchasthelarge-marginconstraintsorthemanifoldconstraints[14].Finally MT-iLSVMisanonparametricBayesiangeneralizationofthepopularmulti-tasklearningmethods[1 16] asexplainedshortly.2RegularizedBayesianInferencewithPosteriorConstraintsInthissection wepresentthegeneralframeworkofregularizedBayesianinferencewithposteriorconstraints.WebeginwithabriefreviewofthebasicresultsduetoZellner[29].2.1BayesianInferenceasaLearningModelLetMbeamodelspace containinganyvariableswhoseposteriordistributionswearetryingtoinfer.Bayesianinferencestartswithapriordistributionπ(M)andalikelihoodfunctionp(x|M)indexedbythemodelM∈M.Then bytheBayes’theorem theposteriordistributionisp(M|x1 ··· xN)=π(M)∏Nn=1p(xn|M)p(x1 ··· xN) (1)wherep(x1 ··· xN)isthemarginallikelihoodorevidenceofobserveddata.Zellner[29]ﬁrstshowedthattheposteriordistributionduetotheBayes’theoremisthesolutionoftheproblemminp(M)KL(p(M)∥π(M))−N∑n=1∫logp(xn|M)p(M)dM(2)s.t.:p(M)∈Pprob whereKL(p(M)∥π(M))istheKullback-Leibler(KL)divergence andPprobisthespaceofvalidprobabilitydistributionswithanappropriatedimension.2.2RegularizedBayesianInferencewithPosteriorConstraintsAscommentedbyE.T.Jaynes[29] “thisfreshinterpretationofBayes’theoremcouldmaketheuseofBayesianmethodsmoreattractiveandwidespread andstimulatenewdevelopmentsin2thegeneraltheoryofinference”.Below westudyhowtoextendthebasicresultstoincorporateposteriorconstraintsinBayesianinference.InthestandardBayesianinference theconstraints(i.e. p(M)∈Pprob)donothaveauxiliaryfreeparameters.Ingeneral regularizedBayesianinferencesolvestheconstrainedoptimizationproblemminp(M) ξKL(p(M)∥π(M))−N∑n=1∫logp(xn|M)p(M)dM+U(ξ)(3)s.t.:p(M)∈Ppost(ξ) wherePpost(ξ)isasubspaceofdistributionsthatsatisfyasetofconstraints.Theauxiliaryparametersξareusuallynonnegativeandinterpretedasslackvariables.U(ξ)isaconvexfunction whichusuallycorrespondstoasurrogateloss(e.g. hingeloss)ofapredictionrule asweshallsee.WecanuseaniterativeproceduretodotheregularizedBayesianinferencebasedonconvexop-timizationtechniques.ThegeneralrecipeisthatweusetheLagrangianmethodbyintroducingLagrangianmultipliersω.Then weiterativelysolveforp(M)withωandξﬁxed;andsolveforωandξwithp(M)given.Fortheﬁrststep wecanusesamplingorvariationalmethods[9]todoapproximateinference;andundercertainconditions suchasusingtheconstraintsbasedonposteriorexpectation[21] thesecondstepcanbeefﬁcientlydoneusinghigh-performanceconvexoptimizationtechniques asweshallsee.3InﬁniteLatentSupportVectorMachinesInthissection weconcretizetheideasofregularizedBayesianinferencebyparticularlyfocusingondevelopinglarge-marginclassiﬁerswithanunboundeddimensionoflatentfeatures whichcanbeusedasarepresentationofexamplesforthesingle-taskclassiﬁcationorasacommonrepresentationthatcapturesrelationshipsamongmultipletasksformulti-tasklearning.Weﬁrstpresentthesingle-taskclassiﬁcationmodel.Thebasicsetupisthatweprojecteachdataexamplex∈X⊂RDtoalatentfeaturevectorz.Here weconsiderbinaryfeatures1.GivenasetofNdataexamples letZbethematrix ofwhicheachrowisabinaryvectorznassociatedwithdatasamplen.Insteadofpre-specifyingaﬁxeddimensionofz weresorttothenonparametricBayesianmethodsandletzhaveaninﬁnitenumberofdimensions.Tomaketheexpectednumberofactivelatentfeaturesﬁnite weputthewell-studiedIBPprioronthebinaryfeaturematrixZ.3.1IndianBuffetProcessIndianbuffetprocess(IBP)wasproposedin[12]andhasbeensuccessfullyappliedinvariousﬁelds suchaslinkprediction[22]andmulti-tasklearning[23].Wefocusonitsstick-breakingconstruction[25] whichisgoodfordevelopingefﬁcientinferencemethods.Letπk∈(0 1)beaparameterassociatedwithcolumnkofthebinarymatrixZ.Givenπk eachznkincolumnkissampledindependentlyfromBernoulli(πk).Theparametersπaregeneratedbyastick-breakingprocessπ1=ν1 andπk=νkπk−1=k∏i=1νi (4)whereνi∼Beta(α 1).Thisprocessresultsinadecreasingsequenceofprobabilitiesπk.Speciﬁ-cally givenaﬁnitedataset theprobabilityofseeingfeaturekdecreasesexponentiallywithk.3.2InﬁniteLatentSupportVectorMachinesWeconsiderthemulti-wayclassiﬁcation whereeachtrainingdataisprovidedwithacategoricallabely wherey∈Ydef={1 ··· L}.Forbinaryclassiﬁcationandregression similarprocedurecanbeappliedtoimposelarge-marginconstraintsonposteriordistributions.Supposethatthelatentfeatureszaregiven thenwecandeﬁnethelatentdiscriminantfunctionasf(y x z;η)def=η⊤g(y x z) (5)whereg(y x z)isavectorstackingofLsubvectors2ofwhichtheythisz⊤andalltheothersarezero.SincewearedoingBayesianinference weneedtomaintaintheentiredistributionproﬁleof1Real-valuedfeaturescanbeeasilyconsideredasin[12].2Wecanconsidertheinputfeaturesxoritscertainstatisticsincombinationwiththelatentfeaturesztodeﬁneaclassiﬁerboundary bysimplyconcatenatingtheminthesubvectors.3thelatentfeaturesZ.However inordertomakeapredictionontheobserveddatax weneedtogetridoftheuncertaintyofZ.Here wedeﬁnetheeffectivediscriminantfunctionasanexpectation3(i.e. aweightedaverageconsideringallpossiblevaluesofZ)ofthelatentdiscriminantfunction.TomakethemodelfullyBayesian wealsotreatηasrandomandaimtoinfertheposteriordistri-butionp(Z η)fromgivendata.Moreformally theeffectivediscriminantfunctionf:X×Y7→Risf(y x;p(Z η))def=Ep(Z η)[f(y x z;η)]=Ep(Z η)[η⊤g(y x z)].(6)Notethatalthoughthenumberoflatentfeaturesisallowedtobeinﬁnite withprobabilityone thenumberofnon-zerofeaturesisﬁnitewhenonlyaﬁnitenumberofdataareobserved undertheIBPprior.Moreover tomakeitcomputationallyfeasible weusuallysetaﬁniteupperboundKtothenumberofpossiblefeatures whereKissufﬁcientlylargeandknownasthetruncationlevel(SeeSec3.4andAppendixA.2fordetails).Asshownin[9] theℓ1-distancetruncationerrorofmarginaldistributionsdecreasesexponentiallyasKincreases.Withtheabovedeﬁnitions wedeﬁnethePpost(ξ)inproblem(3)usinglarge-marginconstraintsasPcpost(ξ)def={p(Z η)∀n∈Itr:f(yn xn;p(Z η))−f(y xn;p(Z η))≥ℓ(y yn)−ξn ∀yξn≥0}(7)anddeﬁnethepenaltyfunctionasUc(ξ)def=C∑n∈Itrξpn wherep≥1.Ifpis1 minimizingUc(ξ)isequivalenttominimizingthehinge-loss(orℓ1-loss)Rchofthepredictionrule(9) whereRch=C∑n∈Itrmaxy(f(y xn;p(Z η))+ℓ(y yn)−f(yn xn;p(Z η)));ifpis2 thesurrogatelossistheℓ2-loss.Forclarity weconsiderthehingeloss.Thenon-negativecostfunctionℓ(y yn)(e.g. 0/1-cost)measuresthecostofpredictingxntobeywhenitstruelabelisyn.Itristheindexsetoftrainingdata.InordertorobustlyestimatethelatentmatrixZ weneedareasonableamountofdata.Therefore wealsorelateZtotheobserveddataxbydeﬁningalikelihoodmodeltoprovideasmuchdataaspossible.Here wedeﬁnethelinear-Gaussianlikelihoodmodelforreal-valueddatap(xn|zn W σ2n0)=N(xn|Wz⊤n σ2n0I) (8)whereWisarandomloadingmatrixandIisanidentitymatrixwithappropriatedimensions.WeassumeWfollowsanindependentGaussianprior i.e. π(W)=∏dN(wd|0 σ20I).Fig.1(a)showsthegraphicalstructureofiLSVM.Thehyperparametersσ20andσ2n0canbesetaprioriorestimatedfromobserveddata(SeeAppendixA.2fordetails).Testing:tomakepredictionontestexamples weputbothtrainingandtestdatatogethertodotheregularizedBayesianinference.Fortrainingdata weimposetheabovelarge-marginconstraintsbecauseoftheawarenessoftheirtruelabels whilefortestdata wedotheinferencewithoutthelarge-marginconstraintssincewedonotknowtheirtruelabels.Afterinference wemakethepredictionviatheruley∗def=argmaxyf(y x;p(Z η)).(9)TheabilitytogeneralizetotestdatareliesonthefactthatallthedataexamplesshareηandtheIBPprior.Wecanalsocasttheproblemasatransductiveinferenceproblembyimposingadditionalconstraintsontestdata[17].However theresultingproblemwillbegenerallyhardertosolve.3.3Multi-TaskInﬁniteLatentSupportVectorMachinesDifferentfromclassiﬁcation whichistypicallyformulatedasasinglelearningtask multi-tasklearningaimstoimproveasetofrelatedtasksthroughsharingstatisticalstrengthbetweenthesetasks whichareperformedjointly.Manydifferentapproacheshavebeendevelopedformulti-tasklearning(See[16]forareview).Inparticular learningacommonlatentrepresentationsharedbyalltherelatedtaskshasproventobeaneffectivewaytocapturetaskrelationships[1 3 23].Below wepresentthemulti-taskinﬁnitelatentSVM(MT-iLSVM)forlearningacommonbinaryprojectionmatrixZtocapturetherelationshipsamongmultipletasks.SimilarasiniLSVM wealsoputtheIBPprioronZtoallowittohaveanunboundednumberofcolumns.3Althoughotherchoicessuchastakingthemodearepossible ourchoicecouldleadtoacomputationallyeasyproblembecauseexpectationisalinearfunctionalofthedistributionunderwhichtheexpectationistaken.Moreover expectationcanbemorerobustthantakingthemode[18] andithasbeenusedin[31 32].4XnYn(cid:740)WZnNIBP((cid:734))(a)ZXmnYmnWmnNM(cid:740)m(cid:771)mIBP((cid:734))(b)Figure1:Graphicalstructuresof(a)inﬁnitela-tentSVM(iLSVM);and(b)multi-taskinﬁnitelatentSVM(MT-iLSVM).ForMT-iLSVM thedashednodes(i.e. ςm)areincludedtoillustratethetaskrelatedness.WehaveomittedthepriorsonWandηfornotationbrevity.SupposewehaveMrelatedtasks.LetDm={(xmn ymn)}n∈Imtrbethetrainingdatafortaskm.Weconsiderbinaryclassiﬁcationtasks whereYm={+1 −1}.Extensiontomulti-wayclassiﬁcationorregressiontaskscanbeeasilydone.IfthelatentmatrixZisgiven wedeﬁnethelatentdiscriminantfunctionfortaskmasfm(x Z;ηm)def=(Zηm)⊤x=η⊤m(Z⊤x).(10)ThisdeﬁnitionprovidestwoviewsofhowtheMtasksgetrelated.Ifweletςm=Zηm thenςmaretheactualparametersoftaskmandallςmindifferenttasksarecoupledbysharingthesamelatentmatrixZ.Anotherviewisthateachtaskmhasitsownparametersηm butallthetaskssharethesamelatentfeaturesZ⊤x whichisaprojectionoftheinputfeaturesxandZisthelatentprojectionmatrix.Assuch ourmethodcanbeviewedasanonparametricBayesiantreatmentofalternatingstructureoptimization(ASO)[1] whichlearnsasingleprojectionmatrixwithapre-speciﬁedlatentdimension.Moreover differentfrom[16] whichlearnsabinaryvectorwithknowndimensionalitytoselectfeaturesorkernelsonx welearnanunboundedprojectionmatrixZusingnonparametricBayesiantechniques.AsiniLSVM wetakethefullyBayeisantreatment(i.e. ηmarealsorandomvariables)anddeﬁnetheeffectivediscriminantfunctionfortaskmastheexpectationfm(x;p(Z η))def=Ep(Z η)[fm(x Z;ηm)]=Ep(Z η)[Zηm]⊤x.(11)Then thepredictionrulefortaskmisnaturallyy∗mdef=signfm(x).Similarly wedoregularizedBayesianinferencebyimposingthefollowingconstraintsanddeﬁningUMT(ξ)def=C∑m n∈ImtrξmnPMTpost(ξ)def={p(Z η)∀m ∀n∈Imtr:ymnEp(Z η)[Zηm]⊤xmn≥1−ξmnξmn≥0}.(12)SimilarasiniLSVM minimizingUMT(ξ)isequivalenttominimizingthehinge-lossRMThofthemultiplebinarypredictionrules whereRMTh=C∑m n∈Imtrmax(0 1−ymnEp(Z η)[Zηm]⊤xmn).Finally toobtainmoredatatoestimatethelatentZ wealsorelateittoobserveddatabydeﬁningthelikelihoodmodelp(xmn|wmn Z λ2mn)=N(xmn|Zwmn λ2mnI) (13)wherewmnisavector.WeassumeWhasanindependentpriorπ(W)=∏mnN(wmn|0 σ2m0I).Fig.1(b)illustratesthegraphicalstructureofMT-iLSVM.Fortesting weusethesamestrategyasiniLSVMtodoBayesianinferenceonbothtrainingandtestdata.Thedifferenceisthattrainingdataaresubjecttolarge-marginconstraints whiletestdataarenot.Similarly thehyper-parametersσ2m0andλ2mncanbesetaprioriorestimatedfromdata(SeeAppendixA.1fordetails).3.4InferencewithTruncatedMean-FieldConstraintsWebrieﬂydiscusshowtodoregularizedBayesianinference(3)withthelarge-marginconstraintsforMT-iLSVM.ForiLSVM similarprocedureapplies.Tomaketheproblemeasiertosolve weusethestick-breakingrepresentationofIBP whichincludestheauxiliaryvariablesν andinfertheposteriorp(ν W Z η).Furthermore weimposethetruncatedmean-ﬁeldconstraintthatp(ν W Z η)=p(η)K∏k=1(p(νk|γk)D∏d=1p(zdk|ψdk))∏mnp(wmn|Φmn σ2mnI) (14)whereKisthetruncationlevel;p(wmn|Φmn σ2mnI)=N(wmn|Φmn σ2mnI);p(zdk|ψdk)=Bernoulli(ψdk);andp(νk|γk)=Beta(γk1 γk2).Weﬁrstturntheconstrainedproblemtoaprob-lemofﬁndingastationarypointusingLagrangianmethodsbyintroducingLagrangemultipliersω oneforeachlarge-marginconstraintasdeﬁnedinEq.(12) anduforthenonnegativityconstraintsofξ.LetL(p ξ ω u)betheLagrangianfunctional.Theinferenceprocedureiterativelysolvesthefollowingtwosteps(WedeferthedetailstoAppendixA.1):5Inferp(ν) p(W) andp(Z):forp(W) sincethepriorisalsonormal wecaneasilyderivetheupdaterulesforΦmnandσ2mn.Forp(ν) wehavethesameupdaterulesasin[9].WedeferthedetailstoAppendixA.1.Now wefocusonp(Z)andprovideinsightsonhowthelarge-marginconstraintsregularizetheprocedureofinferringthelatentmatrixZ.Sincethelarge-marginconstraintsarelinearofp(Z) wecangetthemean-ﬁeldupdateequationasψdk=11+e−ϑdk whereϑdk=k∑j=1Ep[logvj]−Lνk−∑mn12λ2mn((Kσ2mn+(ϕkmn)2)(15)−2xdmnϕkmn+2∑j̸=kϕjmnϕkmnψdj)+∑m n∈ImtrymnEp[ηmk]xdmn whereLνkisanlowerboundofEp[log(1−∏kj=1vj)](SeeAppendixA.1fordetails).Thelasttermofϑdkisduetothelarge-marginposteriorconstraintsasdeﬁnedinEq.(12).Inferp(η)andsolveforωandξ:WeoptimizeLoverp(η)andcangetp(η)=∏mp(ηm) wherep(ηm)∝π(ηm)exp{η⊤mµm} andµm=∑n∈Imtrymnωmn(ψ⊤xmn).Here weassumeπ(ηm)isstandardnormal.Then wehavep(ηm)=N(ηm|µm I).Substitutingthesolutionofp(η)intoL wegetMindependentdualproblemsmaxωm−12µ⊤mµm+∑n∈Imtrωmns.t..:0≤ωmn≤1 ∀n∈Imtr (16)which(oritsprimalform)canbeefﬁcientlysolvedwithabinarySVMsolver suchasSVM-light.4ExperimentsWepresentempiricalresultsforbothclassiﬁcationandmulti-tasklearning.OurresultsdemonstratethemeritsinheritedfrombothBayesiannonparametricsandlarge-marginlearning.4.1Multi-wayClassiﬁcationWeevaluatetheinﬁnitelatentSVM(iLSVM)forclassiﬁcationontherealTRECVID2003andFlickrimagedatasets whichhavebeenextensivelyevaluatedinthecontextoflearningﬁnitelatentfeaturemodels[8].TRECVID2003consistsof1078videokey-frames andeachexamplehastwotypesoffeatures–1894-dimensionbinaryvectoroftextfeaturesand165-dimensionHSVcolorhistogram.TheFlickrimagedatasetconsistsof3411naturalsceneimagesabout13typesofanimals(e.g. tiger catandetc.)downloadedfromtheFlickrwebsite.Also eachexamplehastwotypesoffeatures including500-dimensionSIFTbag-of-wordsand634-dimensionreal-valuedfeatures(e.g. colorhistogram edgedirectionhistogram andblock-wisecolormoments).Here weconsiderthereal-valuedfeaturesonlybyusingnormaldistributionsforx.WecompareiLSVMwiththelarge-marginHarmonium(MMH)[8] whichwasshowntooutperformmanyotherlatentfeaturemodels[8] andtwodecoupledapproaches–EFH+SVMandIBP+SVM.EFH+SVMusestheexponentialfamilyHarmonium(EFH)[27]todiscoverlatentfeaturesandthenlearnsamulti-waySVMclassiﬁer.IBP+SVMissimilar butusesanIBPfactoranalysismodel[12]todiscoverlatentfeatures.Asﬁnitemodels bothMMHandEFH+SVMneedtopre-specifythedimensionalityoflatentfeatures.WereporttheirresultsonclassiﬁcationaccuracyandF1score(i.e. theaverageF1scoreoverallpossibleclasses)[32]achievedwiththebestdimensionalityinTa-ble1.ForiLSVMandIBP+SVM weusethemean-ﬁeldinferencemethodandpresenttheaverageperformancewith5randomlyinitializedruns(SeeAppendixA.2forthealgorithmandinitializa-tiondetails).Weperform5-foldcross-validationontrainingdatatoselecthyperparameters e.g. αandC(weusethesameprocedureforMT-iLSVM).WecanseethatiLSVMcanachievecompa-rableperformancewiththenearlyoptimalMMH withoutneedingtopre-specifythelatentfeaturedimension4 andismuchbetterthanthedecoupledapproaches(i.e. IBP+SVMandEFH+SVM).4.2Multi-taskLearning4.2.1DescriptionoftheDataSceneandYeastData:ThesedatasetsarefromtheUCIrepository andeachdataexamplehasmultiplelabels.Asin[23] wetreatthemulti-labelclassiﬁcationasamulti-tasklearningproblem 4Wesetthetruncationlevelto300 whichislargeenough.6Table1:ClassiﬁcationaccuracyandF1scoresontheTRECVID2003andFlickrimagedatasets.TRECVID2003FlickrModelAccuracyF1scoreAccuracyF1scoreEFH+SVM0.565±0.00.427±0.00.476±0.00.461±0.0MMH0.566±0.00.430±0.00.538±0.00.512±0.0IBP+SVM0.553±0.0130.397±0.0300.500±0.0040.477±0.009iLSVM0.563±0.0100.448±0.0110.533±0.0050.510±0.010Table2:Multi-labelclassiﬁcationperformanceonSceneandYeastdatasets.YeastSceneModelAccF1-MicroF1-MacroAccF1-MicroF1-Macroyaxue[23]0.51060.38970.40220.77650.26690.2816piyushrai-1[23]0.52120.36310.39010.77560.31530.3242piyushrai-2[23]0.54240.39460.41120.79110.32140.3226MT-IBP+SVM0.5475±0.0050.3910±0.0060.4345±0.0070.8590±0.0020.4880±0.0120.5147±0.018MT-iLSVM0.5792±0.0030.4258±0.0050.4742±0.0080.8752±0.0040.5834±0.0260.6148±0.020whereeachlabelassignmentistreatedasabinaryclassiﬁcationtask.TheYeastdatasetconsistsof1500trainingand917testexamples eachhaving103features andthenumberoflabels(ortasks)perexampleis14.TheScenedatasetconsists1211trainingand1196testexamples eachhaving294features andthenumberoflabels(ortasks)perexampleforthisdatasetis6.SchoolData:ThisdatasetcomesfromtheInnerLondonEducationAuthorityandhasbeenusedtostudytheeffectivenessofschools.Itconsistsofexaminationrecordsfrom139secondaryschoolsinyears1985 1986and1987.Itisarandom50%samplewith15362students.Thedatasetispubliclyavailableandhasbeenextensivelyevaluatedinvariousmulti-tasklearningmethods[4 7 30] whereeachtaskisdeﬁnedaspredictingtheexamscoresofstudentsbelongingtoaspeciﬁcschoolbasedonfourstudent-dependentfeatures(yearoftheexam gender VRbandandethnicgroup)andfourschool-dependentfeatures(percentageofstudentseligibleforfreeschoolmeals percentageofstudentsinVRband1 schoolgenderandschooldenomination).Inordertocomparewiththeabovemethods wefollowthesamesetupdescribedin[3 4]andsimilarlywecreatedummyvariablesforthosefeaturesthatarecategoricalformingatotalof19student-dependentfeaturesand8school-dependentfeatures.Weusethesame10randomsplits5ofthedata sothat75%oftheexamplesfromeachschool(task)belongtothetrainingsetand25%tothetestset.Onaverage thetrainingsetincludesabout80studentsperschoolandthetestsetabout30studentsperschool.4.2.2ResultsSceneandYeastData:WecomparewiththecloselyrelatednonparametricBayesianmethods[23 28] whichwereshowntooutperformtheindependentBayesianlogisticregressionandasingle-taskpoolingapproach[23] andadecoupledmethodMT-IBP+SVM6thatusesIBPfactoranalysismodeltoﬁndsharedlatentfeaturesamongmultipletasksandthenbuildsseparateSVMclassiﬁersfordifferenttasks.ForMT-iLSVMandMT-IBP+SVM weusethemean-ﬁeldinferencemethodinSec3.4andreporttheaverageperformancewith5randomlyinitializedruns(SeeAppendixA.1forinitializationdetails).Forcomparisonwith[23 28] weusetheoverallclassiﬁcationaccuracy F1-MacroandF1-Microasperformancemeasures.Table2showstheresults.Wecanseethatthelarge-marginMT-iLSVMperformsmuchbetterthanothernonparametricBayesianmethodsandMT-IBP+SVM whichseparatestheinferenceoflatentfeaturesfromlearningtheclassiﬁers.SchoolData:Weusethepercentageofexplainedvariance[4]asthemeasureoftheregressionperformance whichisdeﬁnedasthetotalvarianceofthedataminusthesum-squarederroronthetestsetasapercentageofthetotalvariance.Sinceweusethesamesettings wecancomparewiththestate-of-the-artresultsofBayesianmulti-tasklearning(BMTL)[4] multi-taskGaussianprocesses(MTGP)[7] convexmulti-taskrelationshiplearning(MTRL)[30] andsingle-tasklearning(STL)asreportedin[7 30].ForMT-iLSVMandMT-IBP+SVM wealsoreporttheresultsachievedbyusingboththelatentfeatures(i.e. Z⊤x)andtheoriginalinputfeaturesxthroughvectorconcatenation andwedenotethecorrespondingmethodsbyMT-iLSVMfandMT-IBP+SVMf respectively.From5Availableat:http://ttic.uchicago.edu/∼argyriou/code/index.html6Thisdecoupledapproachisinfactanone-iterationMT-iLSVM whereweﬁrstinferthesharedlatentmatrixZandthenlearnanSVMclassiﬁerforeachtask.7Table3:PercentageofexplainedvariancebyvariousmodelsontheSchooldataset.STLBMTLMTGPMTRLMT-IBP+SVMMT-iLSVMMT-IBP+SVMfMT-iLSVMf23.5±1.929.5±0.429.2±1.629.9±1.820.0±2.930.9±1.228.5±1.631.7±1.1Table4:PercentageofexplainedvarianceandrunningtimebyMT-iLSVMwithvarioustrainingsizes.50%60%70%80%90%100%explainedvariance(%)25.8±0.427.3±0.729.6±0.430.0±0.530.8±0.430.9±1.2runningtime(s)370.3±32.5455.9±18.6492.6±33.2600.1±50.2777.6±73.4918.9±96.5theresultsinTable3 wecanseethatthemulti-tasklatentSVM(i.e. MT-iLSVM)achievesbetterresultsthantheexistingmethodsthathavebeentestedinpreviousstudies.Again thejointMT-iLSVMperformsmuchbetterthanthedecoupledmethodMT-IBP+SVM whichseparatesthelatentfeatureinferencefromthetrainingoflarge-marginclassiﬁers.Finally usingbothlatentfeaturesandtheoriginalinputfeaturescanboosttheperformanceslightlyforMT-iLSVM whilemuchmoresigniﬁcantlyforthedecoupledMT-IBP+SVM.1234560.5650.570.5750.580.5850.59sqrt of aAccuracy(a)Yeast01234560.5650.570.5750.580.5850.59sqrt of CAccuracy(b)Yeast00.511.522.51520253035CExplained variance (%)(c)SchoolFigure2:SensitivitystudyofMT-iLSVM:(a)classiﬁcationaccuracywithdifferentα;(b)classiﬁcationaccu-racywithdifferentC;and(c)percentageofexplainedvariancewithdifferentC.4.3SensitivityAnalysisFigure2showshowtheperformanceofMT-iLSVMchangesagainstthehyper-parameterαandregularizationconstantConYeastandSchooldatasets.WecanseethatontheYeastdataset MT-iLSVMisinsensitivetoαandC.FortheSchooldataset MT-iLSVMisstablewhenCissetbetween0.3and1.MT-iLSVMisinsensitivetoαontheSchooldatatoo whichisomittedtosavespace.Table4showshowthetrainingsizeaffectstheperformanceandrunningtimeofMT-iLSVMontheSchooldataset.Weusetheﬁrstb%(b=50 60 70 80 90 100)ofthetrainingdataineachofthe10randomsplitsastrainingsetandusethecorrespondingtestdataastestset.Wecanseethatastrainingsizeincreases theperformanceandrunningtimegenerallyincrease;andMT-iLSVMachievesthestate-of-artperformancewhenusingabout70%trainingdata.Fromtherunningtime wecanalsoseethatMT-iLSVMisgenerallyquiteefﬁcientbyusingmean-ﬁeldinference.Finally weinvestigatehowtheperformanceofMT-iLSVMchangesagainstthehyperparametersσ2m0andλ2mn.Weinitiallysetσ2m0=1andcomputeλ2mnfromobserveddata.Ifwefurtherestimatethembymaximizingtheobjectivefunction theperformancedoesnotchangemuch(±0.3%foraverageexplainedvarianceontheSchooldataset).WehavesimilarobservationsforiLSVM.5ConclusionsandFutureWorkWeﬁrstpresentageneralframeworkfordoingregularizedBayesianinferencesubjecttoappro-priateconstraints whichareimposeddirectlyontheposteriordistributions.Then weparticularlyconcentrateondevelopingtwononparametricBayesianmodelstolearnpredictivelatentfeaturesforclassiﬁcationandmulti-tasklearning respectively byexploringthelarge-marginprincipletodeﬁneposteriorconstraints.Bothmodelsallowthelatentdimensiontobeautomaticallyresolvedfromthedata.TheempiricalresultsonseveralrealdatasetsappeartodemonstratethatourmethodsinheritthemeritsfrombothBayesiannonparametricsandlarge-marginlearning.RegularizedBayesianinferenceoffersageneralframeworkforconsideringposteriorregularizationinperformingnonparametricBayesianinference.Forfuturework weplantostudyotherposteriorregularizationbeyondthelarge-marginconstraints suchasposteriorconstraintsdeﬁnedonman-ifoldstructures[14] andinvestigatehowposteriorregularizationcanbeusedinotherinterestingnonparametricBayesianmodels[5 26].8AcknowledgmentsThisworkwasdonewhenJZwasapost-docfellowinCMU.JZissupportedbyNationalKeyProjectforBasicResearchofChina(No.2012CB316300)andtheNationalNaturalScienceFoundationofChina(No.60805023).EXissupportedbyAFOSRFA95501010247 ONRN000140910758 NSFCareerDBI-0546594andAlfredP.SloanResearchFellowship.References[1]R.AndoandT.Zhang.Aframeworkforlearningpredictivestructuresfrommultipletasksandunlabeleddata.JMLR (6):1817–1853 2005.[2]C.E.Antoniak.MixtureofDirichletprocesswithapplicationstoBayesiannonparametricproblems.AnnalsofStats (273):1152–1174 1974.[3]A.Argyriou T.Evgeniou andM.Pontil.Convexmulti-taskfeaturelearning.InNIPS 2007.[4]B.BakkerandT.Heskes.TaskclusteringandgatingforBayesianmultitasklearning.JMLR (4):83–99 2003.[5]M.J.Beal Z.Ghahramani andC.E.Rasmussen.TheinﬁnitehiddenMarkovmodel.InNIPS 2002.[6]K.Bellare G.Druck andA.McCallum.Alternatingprojectionsforlearningwithexpectationconstraints.InUAI 2009.[7]E.Bonilla K.M.A.Chai andC.Williams.Multi-taskGaussianprocessprediction.InNIPS 2008.[8]N.Chen J.Zhu andE.P.Xing.Predictivesubspacelearningformultiviewdata:alargemarginapproach.InNIPS 2010.[9]F.Doshi-Velez K.Miller J.VanGael andY.W.Teh.VariationalinferencefortheIndianbuffetprocess.InAISTATS 2009.[10]D.DunsonandS.Peddada.Bayesiannonparametricinferencesonstochasticordering.ISDSDiscussionPaper 2 2007.[11]K.Ganchev J.Graca J.Gillenwater andB.Taskar.Posteriorregularizationforstructuredlatentvariablemodels.JMLR (11):2001–2094 2010.[12]T.L.GrifﬁthsandZ.Ghahramani.InﬁnitelatentfeaturemodelsandtheIndianbuffetprocess.InNIPS 2006.[13]D.Hoff.Bayesianmethodsforpartialstochasticorderings.Biometrika 90:303–317 2003.[14]S.HuhandS.Fienberg.Discriminativetopicmodelingbasedonmanifoldlearning.InKDD 2010.[15]T.Jaakkola M.Meila andT.Jebara.Maximumentropydiscrimination.InNIPS 1999.[16]T.Jebara.Multitasksparsityviamaximumentropydiscrimination.JMLR (12):75–110 2011.[17]T.Joachims.Transductiveinferencefortextclassiﬁcationusingsupportvectormachines.InICML 1999.[18]M.E.Khan B.Marlin G.Bouchard andK.Murphy.Variationalboundsformixed-datafactoranalysis.InNIPS 2010.[19]P.Liang M.Jordan andD.Klein.Learningfrommeasurementsinexponentialfamilies.InICML 2009.[20]S.N.MacEachern.Dependentnonparametricprocess.IntheSectiononBayesianStatisticalScienceofASA 1999.[21]G.MannandA.McCallum.Generalizedexpectationcriteriaforsemi-supervisedlearningwithweaklylabeleddata.JMLR (11):955–984 2010.[22]K.Miller T.Grifﬁths andM.Jordan.Nonparametriclatentfeaturemodelsforlinkprediction.InNIPS 2009.[23]P.RaiandH.DaumeIII.Inﬁnitepredictorsubspacemodelsformultitasklearning.InAISTATS 2010.[24]C.E.RasmussenandZ.Ghahramani.InﬁnitemixturesofGaussianprocessexperts.InNIPS 2002.[25]Y.W.Teh D.Gorur andZ.Ghahramani.Stick-breakingconstructionoftheIndianbuffetprocess.InAISTATS 2007.[26]Y.W.Teh M.Jordan M.Beal andD.Blei.HierarchicalDirichletprocess.JASA 101(476):1566–1581 2006.[27]M.Welling M.Rosen-Zvi andG.Hinton.Exponentialfamilyharmoniumswithanapplicationtoinformationretrieval.InNIPS 2004.[28]Y.Xue D.Dunson andL.Carin.Thematrixstick-breakingprocessforﬂexiblemulti-tasklearning.InICML 2007.[29]A.Zellner.OptimalinformationprocessingandBayes’theorem.AmericanStatistician 42:278–280 1988.[30]Y.ZhangandD.Y.Yeung.Aconvexformulationforlearningtaskrelationshipsinmulti-tasklearning.InUAI 2010.[31]J.Zhu A.Ahmed andE.P.Xing.MedLDA:Maximummarginsupervisedtopicmodelsforregressionandclassiﬁcation.InICML 2009.[32]J.Zhu N.Chen andE.P.Xing.InﬁniteSVM:aDirichletprocessmixtureoflarge-marginkernelma-chines.InICML 2011.9,Xiangru Huang
Zhenxiao Liang
Chandrajit Bajaj
Qixing Huang
Arash Vahdat
Evgeny Andriyash
William Macready