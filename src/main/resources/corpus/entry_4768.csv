2010,Robust PCA via Outlier Pursuit,Singular Value Decomposition (and Principal Component Analysis)  is one of the most widely used techniques for dimensionality reduction: successful and efficiently computable  it is nevertheless plagued by a well-known  well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet  in applications of SVD or PCA such as robust collaborative filtering or bioinformatics  malicious agents  defective genes  or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted.  We present an efficient convex optimization-based  algorithm we call Outlier Pursuit  that under some mild assumptions on the uncorrupted points (satisfied  e.g.  by the standard generative assumption in PCA problems) recovers the *exact* optimal low-dimensional subspace  and identifies the corrupted points. Such identification of corrupted points that do not conform to the low-dimensional approximation  is of paramount interest in bioinformatics and financial applications  and beyond. Our techniques involve matrix decomposition using nuclear norm minimization  however  our results  setup  and approach  necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition  since we develop an approach to recover the correct *column space* of the uncorrupted matrix  rather than the exact matrix itself.,Robust PCA via Outlier Pursuit

Huan Xu

Electrical and Computer Engineering

University of Texas at Austin

Constantine Caramanis

Electrical and Computer Engineering

University of Texas at Austin

huan.xu@mail.utexas.edu

cmcaram@ece.utexas.edu

Sujay Sanghavi

Electrical and Computer Engineering

University of Texas at Austin

sanghavi@mail.utexas.edu

Abstract

Singular Value Decomposition (and Principal Component Analysis) is one of the
most widely used techniques for dimensionality reduction: successful and efﬁ-
ciently computable  it is nevertheless plagued by a well-known  well-documented
sensitivity to outliers. Recent work has considered the setting where each point
has a few arbitrarily corrupted components. Yet  in applications of SVD or PCA
such as robust collaborative ﬁltering or bioinformatics  malicious agents  defec-
tive genes  or simply corrupted or contaminated experiments may effectively yield
entire points that are completely corrupted.
We present an efﬁcient convex optimization-based algorithm we call Outlier Pur-
suit  that under some mild assumptions on the uncorrupted points (satisﬁed  e.g. 
by the standard generative assumption in PCA problems) recovers the exact opti-
mal low-dimensional subspace  and identiﬁes the corrupted points. Such identi-
ﬁcation of corrupted points that do not conform to the low-dimensional approxi-
mation  is of paramount interest in bioinformatics and ﬁnancial applications  and
beyond. Our techniques involve matrix decomposition using nuclear norm min-
imization  however  our results  setup  and approach  necessarily differ consider-
ably from the existing line of work in matrix completion and matrix decompo-
sition  since we develop an approach to recover the correct column space of the
uncorrupted matrix  rather than the exact matrix itself.

1 Introduction

This paper is about the following problem: suppose we are given a large data matrix M   and we
know it can be decomposed as

M = L0 + C0 

where L0 is a low-rank matrix  and C0 is non-zero in only a fraction of the columns. Aside from
these broad restrictions  both components are arbitrary. In particular we do not know the rank (or
the row/column space) of L0  or the number and positions of the non-zero columns of C0. Can we
recover the column-space of the low-rank matrix L0  and the identities of the non-zero columns of
C0  exactly and efﬁciently?
We are primarily motivated by Principal Component Analysis (PCA)  arguably the most widely
used technique for dimensionality reduction in statistical data analysis. The canonical PCA prob-
lem [1]  seeks to ﬁnd the best (in the least-square-error sense) low-dimensional subspace approx-
imation to high-dimensional points. Using the Singular Value Decomposition (SVD)  PCA ﬁnds
the lower-dimensional approximating subspace by forming a low-rank approximation to the data

1

matrix  formed by considering each point as a column; the output of PCA is the (low-dimensional)
column space of this low-rank approximation.

It is well known (e.g.  [2–4]) that standard PCA is extremely fragile to the presence of outliers: even
a single corrupted point can arbitrarily alter the quality of the approximation. Such non-probabilistic
or persistent data corruption may stem from sensor failures  malicious tampering  or the simple fact
that some of the available data may not conform to the presumed low-dimensional source / model.
In terms of the data matrix  this means that most of the column vectors will lie in a low-dimensional
space – and hence the corresponding matrix L0 will be low-rank – while the remaining columns will
be outliers – corresponding to the column-sparse matrix C. The natural question in this setting is to
ask if we can still (exactly or near-exactly) recover the column space of the uncorrupted points  and
the identities of the outliers. This is precisely our problem.

Recent years have seen a lot of work on both robust PCA [3  5–12]  and on the use of convex
optimization for recovering low-dimensional structure [4  13–15]. Our work lies at the intersection
of these two ﬁelds  but has several signiﬁcant differences from work in either space. We compare
and relate our work to existing literature  and expand on the differences  in Section 3.3.

2 Problem Setup

The precise PCA with outlier problem that we consider is as follows: we are given n points in p-
dimensional space. A fraction 1−γ of the points lie on a r-dimensional true subspace of the ambient
Rp  while the remaining γn points are arbitrarily located – we call these outliers/corrupted points.
We do not have any prior information about the true subspace or its dimension r. Given the set of
points  we would like to learn (a) the true subspace and (b) the identities of the outliers.
As is common practice  we collate the points into a p × n data matrix M   each of whose columns
is one of the points  and each of whose rows is one of the p coordinates. It is then clear that the data
matrix can be decomposed as

M = L0 + C0.

Here L0 is the matrix corresponding to the non-outliers; thus rank(L0) = r. Consider its Singular
Value Decomposition (SVD)

L0 = U0Σ0V ⊤0 .

(1)
Thus it is clear that the columns of U0 form an orthonormal basis for the r-dimensional true sub-
space. Also note that at most (1 − γ)n of the columns of L0 are non-zero (the rest correspond to
the outliers). C0 is the matrix corresponding to the non-outliers; we will denote the set of non-zero
columns of C0 by I0  with |I0| = γn. These non-zero columns are completely arbitrary.
With this notation  out intent is to exactly recover the column space of L0  and the set of outliers I0.
Clearly  this is not always going to be possible (regardless of the algorithm used) and thus we need
to impose a few additional assumptions. We develop these in Section 2.1 below.

We are also interested in the noisy case  where

M = L0 + C0 + N 

and N corresponds to any additional noise. In this case we are interested in approximate identiﬁca-
tion of both the true subspace and the outliers.

2.1 Incoherence: When does exact recovery make sense?

In general  our objective of splitting a low-rank matrix from a column-sparse one is not always a well
deﬁned one. As an extreme example  consider the case where the data matrix M is non-zero in only
one column. Such a matrix is both low-rank and column-sparse  thus the problem is unidentiﬁable.
To make the problem meaningful  we need to impose that the low-rank matrix L0 cannot itself be
column-sparse as well. This is done via the following incoherence condition.
Deﬁnition: A matrix L ∈ Rp×n with SVD as in (1)  and (1 − γ)n of whose columns are non-zero 
is said to be column-incoherent with parameter µ if
i kV ⊤ eik2 ≤
max

µr

(1 − γ)n

2

where {ei} are the coordinate unit vectors.
Thus if V has a column aligned with a coordinate axis  then µ = (1 − γ)n/r. Similarly  if V is
perfectly incoherent (e.g. if r = 1 and every non-zero entry of V has magnitude 1/p(1 − γ)n)
then µ = 1.
In the standard PCA setup  if the points are generated by some low-dimensional isometric Gaussian
distribution  then with high probability  one will have µ = O(max(1  log(n)/r)) [16]. Alternatively 
if the points are generated by a uniform distribution over a bounded set  then µ = Θ(1).
A small incoherence parameter µ essentially enforces that the matrix L0 will have column support
that is spread out. Note that this is quite natural from the application perspective. Indeed  if the left
hand side is as big as 1  it essentially means that one of the directions of the column space which we
wish to recover  is deﬁned by only a single observation. Given the regime of a constant fraction of
arbitrarily chosen and arbitrarily corrupted points  such a setting is not meaningful. Indeed  having
a small incoherence µ is an assumption made in all methods based on nuclear norm minimization
up to date [4  15–17].

We would like to identify the outliers  which can be arbitrary. However  clearly an “outlier” point
that lies in the true subspace is a meaningless concept. Thus  in matrix terms  we require that every
column of C0 does not lie in the column space of L0.
The parameters µ and γ are not required for the execution of the algorithm  and do not need to be
known a priori. They only arise in the analysis of our algorithm’s performance.
Other Notation and Preliminaries: Capital letters such as A are used to represent matrices  and
accordingly  Ai denotes the ith column vector. Letters U   V   I and their variants (complements 
subscripts  etc.) are reserved for column space  row space and column support respectively. There
are four associated projection operators we use throughout. The projection onto the column space 
U   is denoted by PU and given by PU (A) = U U⊤A  and similarly for the row-space PV (A) =
AV V ⊤. The matrix PI(A) is obtained from A by setting column Ai to zero for all i 6∈ I. Finally 
PT is the projection to the space spanned by U and V   and given by PT (·) = PU (·) + PV (·) −
PUPV (·). Note that PT depends on U and V   and we suppress this notation wherever it is clear
which U and V we are using. The complementary operators  PU ⊥  PV ⊥  PT ⊥ and PI c are deﬁned
as usual. The same notation is also used to represent a subspace of matrices: e.g.  we write A ∈ PU
for any matrix A that satisﬁes PU (A) = A. Five matrix norms are used: kAk∗ is the nuclear norm 
kAk is the spectral norm  kAk1 2 is the sum of ℓ2 norm of the columns Ai  kAk∞ 2 is the largest
ℓ2 norm of the columns  and kAkF is the Frobenius norm. The only vector norm used is k · k2  the
ℓ2 norm. Depending on the context  I is either the unit matrix  or the identity operator; ei is the ith
base vector. The SVD of L0 is U0Σ0V ⊤0 . The rank of L0 is denoted as r  and we have γ   |I0|/n 
i.e.  the fraction of outliers.

3 Main Results and Consequences

While we do not recover the matrix L0  we show that the goal of PCA can be attained: even under
our strong corruption model  with a constant fraction of points corrupted  we show that we can –
under very weak assumptions – exactly recover both the column space of L0 (i.e the low-dimensional
space the uncorrupted points lie on) and the column support of C0 (i.e. the identities of the outliers) 
from M . If there is additional noise corrupting the data matrix  i.e. if we have M = L0 + C0 + N  
a natural variant of our approach ﬁnds a good approximation.

3.1 Algorithm

Given data matrix M   our algorithm  called Outlier Pursuit  generates (a) a matrix ˆU   with orthonor-
mal rows  that spans the low-dimensional true subspace we want to recover  and (b) a set of column
indices ˆI corresponding to the outlier points. To ensure success  one choice of the tuning parameter
is λ = 3

7√γn  as Theorem 1 below suggests.

While in the noiseless case there are simple algorithms with similar performance  the beneﬁt of
the algorithm  and of the analysis  is extension to more realistic and interesting situations where in

3

Algorithm 1 Outlier Pursuit
Find ( ˆL  ˆC)  the optimum of the following convex optimization program.

Minimize:
Subject to:

kLk∗ + λkCk1 2
M = L + C

(2)

Compute SVD ˆL = U1Σ1V ⊤1 and output ˆU = U1.
Output the set of non-zero columns of ˆC  i.e. ˆI = {j : ˆcij 6= 0 for some i}.

addition to gross corruption of some samples  there is additional noise. Adapting the Outlier Pursuit
algorithm  we have the following variant for the noisy case.

Noisy Outlier Pursuit:

Minimize:
Subject to:

kLk∗ + λkCk1 2

kM − (L + C)kF ≤ ε

(3)

Outlier Pursuit (and its noisy variant) is a convex surrogate for the following natural (but combina-
torial and intractable) ﬁrst approach to the recovery problem:

Minimize:
Subject to:

rank(L) + λkCk0 c

M = L + C

(4)

where k · k0 c stands for the number of non-zero columns of a matrix.
3.2 Performance

We show that under rather weak assumptions  Outlier Pursuit exactly recovers the column space of
the low-rank matrix L0  and the identities of the non-zero columns of outlier matrix C0. The formal
statement appears below.
Theorem 1 (Noiseless Case). Suppose we observe M = L0 + C0  where L0 has rank r and inco-
herence parameter µ. Suppose further that C0 is supported on at most γn columns. Any output to
Outlier Pursuit recovers the column space exactly  and identiﬁes exactly the indices of columns cor-
responding to outliers not lying in the recovered column space  as long as the fraction of corrupted
points  γ  satisﬁes

where c1 = 9
indeed it holds for any λ in a speciﬁc range which we provide below.

121 . This can be achieved by setting the parameter λ in outlier pursuit to be

For the case where in addition to the corrupted points  we have noisy observations  ˜M = M + W  
we have the following result.
Theorem 2 (Noisy Case). Suppose we observe ˜M = M + N = L0 + C0 + N   where

γ

1 − γ ≤

c2
µr

(6)

with c2 = 9
1024   and kNkF ≤ ε. Let the output of Noisy Outlier Pursuit be L′  C′. Then there exists
˜L  ˜C such that M = ˜L + ˜C  ˜L has the correct column space  and ˜C the correct column support  and

kL′ − ˜LkF ≤ 10√nε;

kC′ − ˜CkF ≤ 9√nε; .

The conditions in this theorem are essentially tight in the following scaling sense (i.e.  up to universal
constants). If there is no additional structure imposed  beyond what we have stated above  then up
to scaling  in the noiseless case  Outlier Pursuit can recover from as many outliers (i.e.  the same
fraction) as any possible algorithm with arbitrary complexity. In particular  it is easy to see that if
the rank of the matrix L0 is r  and the fraction of outliers satisﬁes γ ≥ 1/(r + 1)  then the problem
is not identiﬁable  i.e.  no algorithm can separate authentic and corrupted points.1

1Note that this is no longer true in the presence of stronger assumptions  e.g.  isometric distribution  on the

authentic points [12].

4

γ

1 − γ ≤

c1
µr

 

(5)

3

7√γn –

3.3 Related Work

Robust PCA has a long history (e.g.  [3  5–11]). Each of these algorithms either performs standard
PCA on a robust estimate of the covariance matrix  or ﬁnds directions that maximize a robust es-
timate of the variance of the projected data. These algorithms seek to approximately recover the
column space  and moreover  no existing approach attempts to identify the set of outliers. This out-
lier identiﬁcation  while outside the scope of traditional PCA algorithms  is important in a variety of
applications such as ﬁnance  bio-informatics  and more.

Many existing robust PCA algorithms suffer two pitfalls: performance degradation with dimen-
sion increase  and computational intractability. To wit  [18] shows several robust PCA algorithms
including M-estimator [19]  Convex Peeling [20]  Ellipsoidal Peeling [21]  Classical Outlier Rejec-
tion [22]  Iterative Deletion [23] and Iterative Trimming [24] have breakdown points proportional to
the inverse of dimensionality  and hence are useless in the high dimensional regime we consider.

Algorithms with non-diminishing breakdown point  such as Projection-Pursuit [25] are non-convex
or even combinatorial  and hence computationally intractable (NP-hard) as the size of the problem
scales. In contrast to these  the performance of Outlier Pursuit does not depend on p  and can be
solved in polynomial time.

Algorithms based on nuclear norm minimization to recover low rank matrices are now standard 
since the seminal paper [14]. Recent work [4 15] has taken the nuclear norm minimization approach
to the decomposition of a low-rank matrix and an overall sparse matrix. At a high level  these papers
are close in spirit to ours. However  there are critical differences in the problem setup  the results  and
in key analysis techniques. First  these algorithms fail in our setting as they cannot handle outliers –
entire columns where every entry is corrupted. Second  from a technical and proof perspective  all
the above works investigate exact signal recovery – the intended outcome is known ahead of time 
and one just needs to investigate the conditions needed for success. In our setting however  the
convex optimization cannot recover L0 itself exactly. This requires an auxiliary “oracle problem” as
well as different analysis techniques on which we elaborate below.

4 Proof Outline and Comments

In this section we provide an outline of the proof of Theorem 1. The full proofs of all theorems
appear in a full version available online [26]. The proof follows three main steps

1. Identify the ﬁrst-order necessary and sufﬁcient conditions  for any pair (L′  C′) to be the

optimum of the convex program (2).

2. Consider a candidate pair ( ˆL  ˆC) that is the optimum of an alternate optimization problem 
often called the “oracle problem”. The oracle problem ensures that the pair ( ˆL  ˆC) has the
desired column space and column support  respectively.
3. Show that this ( ˆL  ˆC) is the optimum of Outlier Pursuit.

We remark that the aim of the matrix recovery papers [4  15  16] was exact recovery of the entire
matrix  and thus the optimality conditions required are clear. Since our setup precludes exact recov-
ery of L0 and C0  2 our optimality conditions must imply the optimality for Outlier Pursuit of an
as-of-yet-undetermined pair ( ˆL  ˆC)  the solution to the oracle problem. We now elaborate.
Optimality Conditions: We now specify the conditions a candidate optimum needs to satisfy; these
arise from the standard subgradient conditions for the norms involved. Suppose the pair (L′  C′) is a
feasible point of (2)  i.e. we have that L′ + C′ = M . Let the SVD of L′ be given by L′ = U′Σ′V ′⊤.
For any matrix X  deﬁne PT ′(X) := U′U′⊤X + XV ′V ′⊤ − U′U′⊤XV ′V ′⊤  the projection of X
onto matrices that share the same column space or row space with L′.
Let I′ be the set of non-zero columns of C′  and let H′ be the column-normalized version of C′.
That is  column H′i = C ′
for all i ∈ I′  and H′i = 0 for all i /∈ I′. Finally  for any matrix X let
kC ′
ik2
PI ′(X) denote the matrix with all columns in I′c set to 0  and the columns in I′ left as-is.

i

2The optimum ˆL of (2) will be non-zero in every column of C0 that is not orthogonal to L0’s column space.

5

Proposition 1. With notation as above  L′  C′ is an optimum of the Outlier Pursuit progam (2) if
there exists a Q such that

PT ′ (Q) = U′V ′
PI ′(Q) = λH′

kQ − PT ′(Q)k ≤ 1
kQ − PI ′(Q)k∞ 2 ≤ λ.

(7)

Further  if both inequalities above are strict  dubbed Q strictly satisﬁes (7)  then (L′  C′) is the
unique optimum.

Note that here k · k is the spectral norm (i.e. largest singular value) and k · k∞ 2 is the magnitude –
i.e. ℓ2 norm – of the column with the largest magnitude.
Oracle Problem: We develop our candidate solution ( ˆL  ˆC) by considering the alternate optimiza-
tion problem where we add constraints to (2) based on what we hope its optimum should be. In
particular  recall the SVD of the true L0 = U0Σ0V ⊤0 and deﬁne for any matrix X the projection
onto the space of all matrices with column space contained in U0 as PU0(X) := U0U⊤0 X. Similarly
for the column support I0 of the true C0  deﬁne the projection PI0(X) to be the matrix that results
when all the columns in I c
Note that U0 and I0 above correspond to the truth. Thus  with this notation  we would like the
optimum of (2) to satisfy PU0( ˆL) = ˆL  as this is nothing but the fact that ˆL has recovered the true
subspace. Similarly  having ˆC satisfy PI0( ˆC) = ˆC means that we have succeeded in identifying the
outliers. The oracle problem arises by imposing these as additional constraints in (2). Formally:

0 are set to 0.

Oracle Problem:

Minimize:
Subject to:

kLk∗ + λkCk1 2

M = L + C; PU0 (L) = L; PI0(C) = C.

(8)

Obtaining Dual Certiﬁcates for Outlier Pursuit: We now construct a dual certiﬁcate of ( ˆL  ˆC) to
establish Theorem 1. Let the SVD of ˆL be ˆU ˆΣ ˆV ⊤. It is easy to see that there exists an orthonormal
matrix V ∈ Rr×n such that ˆU ˆV ⊤ = U0V ⊤  where U0 is the column space of L0. Moreover  it is
easy to show that P ˆU (·) = PU0 (·)  P ˆV (·) = PV   and hence the operator P ˆT deﬁned by ˆU and ˆV  
obeys P ˆT (·) = PU0 (·) + PV (·) − PU0PV (·). Let ˆH be the matrix satisfying that PI c
0 ( ˆH) = 0 and
∀i ∈ I0  ˆHi = ˆCi/k ˆCik2.
Deﬁne matrix G ∈ Rr×r as

G   PI0(V ⊤)(PI0 (V ⊤))⊤ = X
i∈I0

[(V ⊤)i][(V ⊤)i]⊤ 

and constant c   kGk. Further deﬁne matrices ∆1   λPU0 ( ˆH)  and
∞
X
∆2   PU ⊥
0PV (cid:2)I +

(PV PI0PV )i(cid:3)PV (λ ˆH) = PI c

0PV (cid:2)I +

0 PI c

∞
X

(PV PI0PV )i(cid:3)PV PU ⊥

0

(λ ˆH).

i=1

i=1

γ

Then we can deﬁne the dual certiﬁcate for strict optimality of the pair ( ˆL  ˆC).

(1−c)√ µr
√n(1−c−√ γ

< λ <

1−γ
1−γ µr)

1−γ < (1−c)2

(3−c)2µr   and

(2−c)√nγ   then Q  

Proposition 2. If c < 1 
1−c
U0V ⊤ + λ ˆH − ∆1 − ∆2 strictly satisﬁes Condition (7)  i.e.  it is the dual certiﬁcate.
Consider the (much) simpler case where the corrupted columns are assumed to be orthogonal to
Indeed  in that setting  where V0 = ˆV =
the column space of L0 which we seek to recover.
V   we automatically satisfy the condition PI0 TPV0 = {0}. In the general case  we require the
condition c < 1 to recover the same property. Moreover  considering that the columns of H are
either zero  or deﬁned as normalizations of the columns of matrix C (i.e.  normalizations of outliers) 
that PU0 (H) = PV0(H) = PT0 (H) = 0  is immediate  as is the condition that PI0(U0V ⊤0 ) = 0.
For the general  non-orthogonal case  however  we require the matrices ∆1 and ∆2 to obtain these
equalities  and the rest of the dual certiﬁcate properties. In the full version [26] we show in detail
how these ideas and the oracle problem  are used to construct the dual certiﬁcate Q. Extending these
ideas  we then quickly obtain the proof for the noisy case.

6

5 Implementation issue and numerical experiments

Solving nuclear-norm minimizations naively requires use of general purpose SDP solvers  which
unfortunately still have questionable scaling capabilities. Instead  we use the proximal gradient al-
gorithms [27]  a.k.a.  Singular Value Thresholding [28] to solve Outlier Pursuit. The algorithm con-
verges with a rate of O(k−2) where k is the number of iterations  and in each iteration  it involves a
singular value decomposition and thresholding  therefore  requiring signiﬁcantly less computational
time than interior point methods.

Our ﬁrst experiment investigates the phase-transition property of Outlier Pursuit  using randomly
generated synthetic data. Fix n = p = 400. For different r and number of outliers γn  we generated
matrices A ∈ Rp×r and B ∈ R(n−γn)×r where each entry is an independent N (0  1) random
variable  and then set L∗ := A × B⊤ (the “clean” part of M ). Outliers  C∗ ∈ Rγn×p are generated
either neutrally  where each entry of C∗ is iid N (0  1)  or adversarial  where every column is an
identical copy of a random Gaussian vector. Outlier Pursuit succeeds if ˆC ∈ PI  and ˆL ∈ PU . Note
that if a lot of outliers span a same direction  it would be difﬁcult to identify whether they are all
outliers  or just a new direction of the true space. Indeed  such a setup is order-wise worst  as we
proved in the full version [26] a matching lower bound is achieved when all outliers are identical.

(a) Random Outlier

(b) Identical Outlier

(c) Noisy Outlier Detection

s=20 

identical outlier

s=20 

random outlier

s=10 

identical outlier

s=10 

random outlier

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

t

e
a
r
 

d
e
e
c
c
u
S

0
0.2

0.3

0.4

0.5

0.7

0.8

0.9

1

0.6

σ/s

Figure 1: Complete Observation: Results averaged over 10 trials.

Figure 1 shows the phase transition property. We represent success in gray scale  with white denoting
success  and black failure. When outliers are random (easier case) Outlier Pursuit succeeds even
when r = 20 with 100 outliers. In the adversarial case  we observe a phase transition: Outlier
Pursuit succeeds when r × γ is small  and fails otherwise  consistent with our theory’s predictions.
We then ﬁx r = γn = 5 and examine the outlier identiﬁcation ability of Outlier Pursuit with noisy
observations. We scale each outlier so that the ℓ2 distance of the outlier to the span of true samples
equals a pre-determined value s. Each true sample is thus corrupted with a Gaussian random vector
with an ℓ2 magnitude σ. We perform (noiseless) Outlier Pursuit on this noisy observation matrix  and
claim that the algorithm successfully identiﬁes outliers if for the resulting ˆC matrix  k ˆCjk2 < k ˆCik2
for all j 6∈ I and i ∈ I  i.e.  there exists a threshold value to separate out outliers. Figure 1 (c) shows
the result: when σ/s ≤ 0.3 for the identical outlier case  and σ/s ≤ 0.7 for the random outlier case 
Outlier Pursuit correctly identiﬁes the outliers.

We further study the case of decomposing M under incomplete observation  which is motivated by
robust collaborative ﬁltering: we generate M as before  but only observe each entry with a given
probability (independently). Letting Ω be the set of observed entries  we solve

Minimize: kLk∗ + λkCk1 2;

Subject to: PΩ(L + C) = PΩ(M ).

(9)

The same success condition is used. Figure 2 shows a very promising result: the successful decom-
position rate under incomplete observation is close to the complete observation case even when only
30% of entries are observed. Given this empirical result  a natural direction of future research is to
understand theoretical guarantee of (9) in the incomplete observation case.

Next we report some experiment results on the USPS digit data-set. The goal of this experiment is
to show that Outlier Pursuit can be used to identify anomalies within the dataset. We use the data
from [29]  and construct the observation matrix M as containing the ﬁrst 220 samples of digit “1”
and the last 11 samples of “7”. The learning objective is to correctly identify all the “7’s”. Note
that throughout the experiment  label information is unavailable to the algorithm  i.e.  there is no
training stage. Since the columns of digit “1” are not exactly low rank  an exact decomposition

7

(a) 30% entries observed

(b) 80% entries observed

(c) Success rate vs Observe ratio

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
t
a
R
 
d
e
e
c
c
u
S

0

0

0.1

0.2

0.3

0.4

Fraction of Observed Entries

0.5

0.6

0.7

0.8

Figure 2: Partial Observation.

is not possible. Hence  we use the ℓ2 norm of each column in the resulting C matrix to identify
the outliers: a larger ℓ2 norm means that the sample is more likely to be an outlier — essentially 
we apply thresholding after C is obtained. Figure 3(a) shows the ℓ2 norm of each column of the
resulting C matrix. We see that all “7’s” are indeed identiﬁed. However  two “1” samples (columns
71 and 137) are also identiﬁed as outliers  due to the fact that these two samples are written in a way
that is different from the rest “1’s” as showed in Figure 4. Under the same setup  we also simulate
the case where only 80% of entries are observed. As Figure 3 (b) and (c) show  similar results as
that of the complete observation case are obtained  i.e.  all true “7’s” and also “1’s” No 71  No 177
are identiﬁed.

(a) Complete Observation

(b) Partial Obs. (one run)

(c) Partial Obs. (average)

6

5

4

3

2

1

i

C

 
f
o
 
m
r
o
n
 

2

l

"1"

"7"

i

C

 
f
o
 
m
r
o
n
 

2

l

6

5

4

3

2

1

"1"

"7"

i

C

 
f
o
 
m
r
o
n
 

2

l

"1"

"7"

6

5

4

3

2

1

0

0

50

100

i

150

200

0

0

50

100

i

150

200

0

0

50

100

i

150

200

Figure 3: Outlyingness: ℓ2 norm of Ci.

“1”

“7”

No 71 No 177

Figure 4: Typical “1”  “7” and abnormal “1”.

6 Conclusion and Future Direction

This paper considers robust PCA from a matrix decomposition approach  and develops the algorithm
Outlier Pursuit. Under some mild conditions  we show that Outlier Pursuit can exactly recover the
column support  and exactly identify outliers. This result is new  differing both from results in
Robust PCA  and also from results using nuclear-norm approaches for matrix completion and matrix
reconstruction. One central innovation we introduce is the use of an oracle problem. Whenever the
recovery concept (in this case  column space) does not uniquely correspond to a single matrix (we
believe many  if not most cases of interest  will fall under this description)  the use of such a tool
will be quite useful. Immediate goals for future work include considering speciﬁc applications  in
particular  robust collaborative ﬁltering (here  the goal is to decompose a partially observed column-
corrupted matrix) and also obtaining tight bounds for outlier identiﬁcation in the noisy case.
Acknowledgements H. Xu would like to acknowledge support from DTRA grant HDTRA1-08-
0029. C. Caramanis would like to acknowledge support from NSF grants EFRI-0735905  CNS-
0721532  CNS- 0831580  and DTRA grant HDTRA1-08-0029. S. Sanghavi would like to acknowl-
edge support from the NSF CAREER program  Grant 0954059.

8

References

[1] I. T. Jolliffe. Principal Component Analysis. Springer Series in Statistics  Berlin: Springer  1986.
[2] P. J. Huber. Robust Statistics. John Wiley & Sons  New York  1981.
[3] L. Xu and A. L. Yuille. Robust principal component analysis by self-organizing rules based on statistical

physics approach. IEEE Tran. on Neural Networks  6(1):131–143  1995.

[4] E. Cand`es  X. Li  Y. Ma  and J. Wright. Robust pricinpal component analysis? ArXiv:0912.3599  2009.
[5] S. J. Devlin  R. Gnanadesikan  and J. R. Kettenring. Robust estimation of dispersion matrices and princi-

pal components. Journal of the American Statistical Association  76(374):354–362  1981.

[6] T. N. Yang and S. D. Wang. Robust algorithms for principal component analysis. Pattern Recognition

Letters  20(9):927–933  1999.

[7] C. Croux and G. Hasebroeck. Principal component analysis based on robust estimators of the covariance

or correlation matrix: Inﬂuence functions and efﬁciencies. Biometrika  87(3):603–618  2000.

[8] F. De la Torre and M. J. Black. Robust principal component analysis for computer vision. In ICCV’01 

pages 362–369  2001.

[9] F. De la Torre and M. J. Black. A framework for robust subspace learning.

Computer Vision  54(1/2/3):117–142  2003.

International Journal of

[10] C. Croux  P. Filzmoser  and M. Oliveira. Algorithms for Projection−Pursuit robust principal component

analysis. Chemometrics and Intelligent Laboratory Systems  87(2):218–225  2007.

[11] S. C. Brubaker. Robust PCA and clustering on noisy mixtures. In SODA’09  pages 1078–1087  2009.
[12] H. Xu  C. Caramanis  and S. Mannor. Principal component analysis with contaminated data: The high

dimensional case. In COLT’10  pages 490–502  2010.

[13] E. J. Cand`es  J. Romberg  and T. Tao. Robust uncertainty principles: Exact signal reconstruction from

highly incomplete frequency information. IEEE Tran. on Information Theory  52(2):489–509  2006.

[14] B. Recht  M. Fazel  and P. Parrilo. Guaranteed minimum rank solutions to linear matrix equations via

nuclear norm minimization. To appear in SIAM Review  2010.

[15] V. Chandrasekaran  S. Sanghavi  P. Parrilo  and A. Willsky. Rank-sparsity incoherence for matrix decom-

position. ArXiv:0906.2220  2009.

[16] E. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of Computational

Mathematics  9:717–772  2009.

[17] E. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Tran. on

Information Theory  56(2053-2080)  2010.

[18] D. L. Donoho. Breakdown properties of multivariate location estimators. Qualifying paper  Harvard

University  1982.

[19] R. Maronna. Robust M-estimators of multivariate location and scatter. The Annals of Statistics  4:51–67 

1976.

[20] V. Barnett. The ordering of multivariate data. Journal of Royal Statistics Society  A  138:318–344  1976.
[21] D. Titterington. Estimation of correlation coefﬁcients by ellipsoidal trimming. Applied Statistics  27:227–

234  1978.

[22] V. Barnett and T. Lewis. Outliers in Statistical Data. Wiley  New York  1978.
[23] A. Dempster and M. Gasko-Green. New tools for residual analysis. The Annals of Statistics  9(5):945–

959  1981.

[24] S. J. Devlin  R. Gnanadesikan  and J. R. Kettenring. Robust estimation and outlier detection with correla-

tion coefﬁcients. Biometrika  62:531–545  1975.

[25] G. Li and Z. Chen. Projection-pursuit approach to robust dispersion matrices and principal components:
Primary theory and monte carlo. Journal of the American Statistical Association  80(391):759–766  1985.
[26] H. Xu  C. Caramanis  and S. Sanghavi. Robust PCA via outlier pursuit. http://arxiv.org/abs/1010.4237 

2010.

[27] Y. Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2). Soviet

Mathematics Doklady  27(372-376)  1983.

[28] J-F. Cai  E. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM

Journal on Optimization  20:1956–1982  2008.

[29] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. the MIT Press  2006.

9

,Stephan Zheng
Yisong Yue
Jennifer Hobbs