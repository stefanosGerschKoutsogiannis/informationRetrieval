2017,Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network,We revisit fuzzy neural network with a cornerstone notion of generalized hamming distance  which provides a novel and theoretically justified framework to re-interpret many useful neural network techniques in terms of fuzzy logic. In particular  we conjecture and empirically illustrate that  the celebrated batch normalization (BN) technique actually adapts the “normalized” bias such that it approximates the rightful bias induced by the generalized hamming distance. Once the due bias is enforced analytically  neither the optimization of bias terms nor the sophisticated batch normalization is needed. Also in the light of generalized hamming distance  the popular rectified linear units (ReLU) can be treated as setting a minimal hamming distance threshold between network inputs and weights. This thresholding scheme  on the one hand  can be improved by introducing double-thresholding on both positive and negative extremes of neuron outputs. On the other hand  ReLUs turn out to be non-essential and can be removed from networks trained for simple tasks like MNIST classification. The proposed generalized hamming network (GHN) as such not only lends itself to rigorous analysis and interpretation within the fuzzy logic theory but also demonstrates fast learning speed  well-controlled behaviour and state-of-the-art performances on a variety of learning tasks.,Revisit Fuzzy Neural Network:

Demystifying Batch Normalization and ReLU with

Generalized Hamming Network

Lixin Fan

lixin.fan@nokia.com

Nokia Technologies
Tampere  Finland

Abstract

We revisit fuzzy neural network with a cornerstone notion of generalized ham-
ming distance  which provides a novel and theoretically justiﬁed framework to
re-interpret many useful neural network techniques in terms of fuzzy logic. In par-
ticular  we conjecture and empirically illustrate that  the celebrated batch normaliza-
tion (BN) technique actually adapts the “normalized” bias such that it approximates
the rightful bias induced by the generalized hamming distance. Once the due bias
is enforced analytically  neither the optimization of bias terms nor the sophisticated
batch normalization is needed. Also in the light of generalized hamming distance 
the popular rectiﬁed linear units (ReLU) can be treated as setting a minimal ham-
ming distance threshold between network inputs and weights. This thresholding
scheme  on the one hand  can be improved by introducing double-thresholding on
both positive and negative extremes of neuron outputs. On the other hand  ReLUs
turn out to be non-essential and can be removed from networks trained for simple
tasks like MNIST classiﬁcation. The proposed generalized hamming network
(GHN) as such not only lends itself to rigorous analysis and interpretation within
the fuzzy logic theory but also demonstrates fast learning speed  well-controlled
behaviour and state-of-the-art performances on a variety of learning tasks.

1

Introduction

Since early 1990s the integration of fuzzy logic and computational neural networks has given birth to
the fuzzy neural networks (FNN) [1]. While the formal fuzzy set theory provides a strict mathematical
framework in which vague conceptual phenomena can be precisely and rigorously studied [2  3  4  5] 
application-oriented fuzzy technologies lag far behind theoretical studies. In particular  fuzzy neural
networks have only demonstrated limited successes on some toy examples such as [6  7]. In order to
catch up with the rapid advances in recent neural network developments  especially those with deep
layered structures  it is the goal of this paper to demonstrate the relevance of FNN  and moreover  to
provide a novel view on its non-fuzzy counterparts.
Our revisiting of FNN is not merely for the fond remembrances of the golden age of “soft computing”
[8]. Instead it provides a novel and theoretically justiﬁed perspective of neural computing  in which
we are able to re-examine and demystify some useful techniques that were proposed to improve
either effectiveness or efﬁciency of neural networks training processes. Among many others  batch
normalization (BN) [9] is probably the most inﬂuential yet mysterious trick  that signiﬁcantly
improved the training efﬁciency by adapting to the change in the distribution of layers’ inputs (coined
as internal covariate shift). Such kind of adaptations  when viewed within the fuzzy neural network
framework  can be interpreted as rectiﬁcations to the deﬁciencies of neuron outputs with respect to the
rightful generalized hamming distance (see deﬁnition 1) between inputs and neuron weights. Once

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

the appropriate rectiﬁcation is applied   the ill effects of internal covariate shift are automatically
eradicated  and consequently  one is able to enjoy the fast training process without resorting to a
sophisticated learning method used by BN.
Another crucial component in neural computing  Rectiﬁed linear unit (ReLU)  has been widely used
due to its strong biological motivations and mathematical justiﬁcations [10  11  12]. We show that
within the generalized hamming group endowed with generalized hamming distance  ReLU can be
regarded as setting a minimal hamming distance threshold between network input and neuron weights.
This novel view immediately leads us to an effective double-thresholding scheme to suppress fuzzy
elements in the generalized hamming group.
The proposed generalized hamming network (GHN) forms its foundation on the cornerstone notion
of generalized hamming distance (GHD)  which is essentially deﬁned as h(x  w) := x + w − 2xw
for any x  w ∈ R (see deﬁnition 1). Its connection with the inferencing rule in neural computing is
obvious: the last term (−2xw) corresponds to element-wise multiplications of neuron inputs and
weights  and since we aim to measure the GHD between inputs x and weights w  the bias term then
should take the value x + w. In this article we deﬁne any network that has its neuron outputs fulﬁlling
this requirement (3) as a generalized hamming network. Since the underlying GHD induces a fuzzy
XOR logic  GHN lends itself to rigorous analysis within the fuzzy logics theory (see deﬁnition 4).
Apart from its theoretical appeals  GHN also demonstrates appealing features in terms of fast learning
speed  well-controlled behaviour and simple parameter settings (see Section 4).

1.1 Related Work

Fuzzy logic and fuzzy neural network: the notion of fuzzy logic is based on the rejection of the
fundamental principle of bivalence of classical logic i.e. any declarative sentence has only two
possible truth values  true and false. Although the earliest connotation of fuzzy logic was attributed
to Aristotle  the founder of classical logic [13]  it was Zadeh’s publication in 1965 that ignited the
enthusiasm about the theory of fuzzy sets [2]. Since then mathematical developments have advanced
to a very high standard and are still forthcoming to day [3  4  5]. Fuzzy neural networks were proposed
to take advantages of the ﬂexible knowledge acquiring capability of neural networks [1  14]. In theory
it was proved that fuzzy systems and certain classes of neural networks are equivalent and convertible
with each other [15  16]. In practice  however  successful applications of FNNs are limited to some
toy examples only [6  7].
Demystifying neural networks: efforts of interpreting neural networks by means of propositional
logic dated back to McCulloch & Pitts’ seminial paper [17]. Recent research along this line include
[18] and the references therein  in which First Order Logic (FOL) rules are encoded using soft logic
on continuous truth values from the interval [0  1]. These interpretations  albeit interesting  seldom
explain effective neural network techniques such as batch normalization or ReLU. Recently [19]
provided an improvement (and explanation) to batch normalization by removing dependencies in
weight normalization between the examples in a minibatch.
Binary-valued neural network: Restricted Boltzmann Machine (RBM) was used to model an “en-
semble of binary vectors” and rose to prominence in the mid-2000s after fast learning algorithms
were demonstrated by Hinton et. al. [20  21]. Recent binarized neural network [22  23] approximated
standard CNNs by binarizing ﬁlter weights and/or inputs  with the aim to reduce computational
complexity and memory consumption. The XNOR operation employed in [23] is limited to binary
hamming distance and not readily applicable to non-binary neuron weights and inputs.
Ensemble of binary patterns: the distributive property of GHD described in (1) provides an intriguing
view on neural computing – even though real-valued pattens are involved in the computation  the
computed GHD is strictly equivalent to the mean of binary hamming distances across two ensembles
of binary patterns! This novel view illuminates the connection between generalized hamming
networks and efﬁcient binary features  that have long been used in various computer vision tasks 
for instance  the celebrated Adaboost face detection[24]  numerous binary features for key-point
matching [25  26] and binary codes for large database hashing [27  28  29  30].

2

(a)

(b)

(c)

(d)

Figure 1: (a) h(a  b) has one fuzzy region near the identity element 0.5 (in white)  two positively
conﬁdent (in red) and two negatively conﬁdent (in blue) regions from above and below  respectively.
(b) Fuzziness F (h(a  b)) = h(a  b) ⊕ h(a  b) has its maxima along a = 0.5 or b = 0.5.
(c)
µ(h(a  b)) : U → I where µ(h) = 1/(1+exp(0.5−h)) is the logistic function to assign membership
to fuzzy set elements (see deﬁnition 4). (d) partial derivative of µ(h(a  b)). Note that magnitudes of
gradient in the fuzzy region is non-negligible.

2 Generalized Hamming Distance
Deﬁnition 1. Let a  b  c ∈ U ⊆ R  and a generalized hamming distance (GHD)  denoted by ⊕  be a
binary operator h : U × U → U ; h(a  b) := a ⊕ b = a + b − 2 · a · b . Then
(i) for U = {0  1} GHD de-generalizes to binary hamming distance with

0 ⊕ 0 = 0; 0 ⊕ 1 = 1; 1 ⊕ 0 = 1; 1 ⊕ 1 = 0;

(ii) for U = [0.0  1.0] the unitary interval I  a ⊕ b ∈ I (closure);

Remark: this case is referred to as the “restricted” hamming distance  in the sense that inverse
of any elements in I are not necessarily contained in I (see below for deﬁnition of inverse).
(iii) for U = R  H := (R ⊕) is a group satisfying ﬁve abelian group axioms  thus is referred to as

the generalized hamming group or hamming group:
• a ⊕ b = (a + b − 2 · a · b) ∈ R (closure);
• a ⊕ b = (a + b − 2 · a · b) = b ⊕ a (commutativity);
• (a ⊕ b) ⊕ c = (a + b − 2 · a · b) + c − 2(a + b − 2 · a · b)c
= a + (b + c − 2 · b · c) − 2 · a · (b + c − 2 · b · c) = a ⊕ (b ⊕ c) (associativity);
• ∃e = 0 ∈ R such that e ⊕ a = a ⊕ e = (0 + a − 2 · 0 · a) = a (identity element);
2·a−1 − 2a ·
• for each a ∈ R \ {0.5}  ∃a−1 := a/(2 · a − 1) s.t. a ⊕ a−1 = (a + a

= 0 = e; and we deﬁne ∞ := (0.5)−1 (inverse element).

a

2·a−1 )

Remark: note that 1 ⊕ a = 1 − a which complements a. “0.5” is a ﬁxed point since ∀a ∈
R  0.5 ⊕ a = 0.5  and 0.5 ⊕ ∞ = 0 according to deﬁnition1.

(iv) GHD naturally leads to a measurement of fuzziness: F (a) := a ⊕ a  R → (−∞  0.5] :
F (a) ≥ 0 ∀a ∈ [0  1]; F (a) < 0 otherwise. Therefore [0  1] is referred to as the fuzzy
region in which F (0.5) = 0.5 has the maximal fuzziness and F (0) = F (1) = 0 are two
boundary points. Outer regions (−∞  0] and [1 ∞) are negative and positive conﬁdent regions
respectively. See Figure 1 (a) for the surface of h(a  b) which has one central fuzzy region  two
positive conﬁdent and two negative conﬁdent regions.

(v) The direct sum of hamming group is still a hamming group HL := ⊕l∈LHl:

let x =
{x1  . . .   xL}  y = {y1  . . .   yL} ∈ HL be two group members  then the generalized ham-
ming distance is deﬁned as the arithmetic mean of element-wise GHD: GL(x ⊕L y) :=
L (x1 ⊕ y1 + . . . + xL ⊕ yL).
1
And let ˜x = (x1 + . . . xL)/L  ˜y = (y1 + . . . yL)/L be arithmetic means of respective elements 
then GL(x ⊕L y) = ˜x + ˜y − 2
L

(x · y)   where x · y =(cid:80)L

l=1 xl · yl is the dot product.

1By this extension  it is R = R ∪ {−∞  +∞} instead of R on which we have all group members.

3

−3−2−101234−3−2−101234−20−1001020h(a b)−3−2−101234−3−2−101234−1200−1000−800−600−400−2000F(h(a b))−3−2−101234−3−2−1012340.20.40.60.8μ(h(a b))−3−2−101234−3−2−101234−0.015−0.010−0.0050.0000.0050.0100.015∂μ∂a(h(a b))(vi) Distributive property:

let ¯XM = (x1 + . . . xM )/M ∈ HL be element-wise arithmetic mean
of a set of members xm ∈ HL  and ¯YN be deﬁned in the same vein. Then GHD is distributive:

GL( ¯XM ⊕L ¯YN ) =

1
L

¯xl ⊕ ¯yl =

1
M

1
N

l ⊕ yn
xm

l

L(cid:88)

l=1

L(cid:88)

M(cid:88)
N(cid:88)
N(cid:88)
M(cid:88)

1
L

m=1

n=1

l=1

(1)

GL(xm ⊕L yn).

=

1

M N

m=1

n=1

l ∈ {0  1} i.e. for two sets of binary patterns  the mean of binary
Remark: in case that xm
hamming distance between two sets can be efﬁciently computed as the GHD between two real-
valued patterns ¯XM   ¯YN . Conversely  a real-valued pattern can be viewed as the element-wise
average of an ensemble of binary patterns.

l   yn

3 Generalized Hamming Network

Despite the recent progresses in deep learning  artiﬁcial neural networks has long been criticized
for its “black box” nature: “they capture hidden relations between inputs and outputs with a highly
accurate approximation  but no deﬁnitive answer is offered for the question of how they work” [16].
In this section we provide an interpretation on neural computing by showing that  if the condition
speciﬁed in (3) is fulﬁlled  outputs of each neuron can be strictly deﬁned as the generalized hamming
distance between inputs and weights. Moreover  the computations of GHD induces fuzzy implication
of XOR connective  and therefore  the inferencing of entire network can be regarded as a logical
calculus in the same vein as described in McCulloch & Pitts’ seminial paper [17].

3.1 New perspective on neural computing

The bearing of generalized hamming distance on neural computing is elucidated by looking at the
negative of generalized hamming distance  (GHD  see deﬁnition 1)  between inputs x ∈ HL and
weights w ∈ HL in which L denotes the length of neuron weights e.g. in convolution kernels:

−GL(w ⊕L x) =

2
L

w · x − 1
L

wl − 1
L

L(cid:88)

l=1

L(cid:88)

l=1

xl

(2)

(3)

Divide (2) by the constant 2

L and let

(cid:0) L(cid:88)

l=1

wl +

(cid:1)

xl

L(cid:88)

l=1

b = − 1
2

negatives of GHD between inputs and weights. Note that  for each layer  the bias term(cid:80)L
averaged over neighbouring neurons in individual input image. The bias term(cid:80)L
the optimization (cid:80)L

then it becomes the familiar form (w · x + b) of neuron outputs save the non-linear activation
function. By enforcing the bias term to take the given value in (3)  standard neuron outputs measure
l=1 xl is
l=1 wl is computed
separately for each ﬁlter in fully connected or convolution layers. When weights are updated during
l=1 wl changes accordingly to keep up with weights and maintain stable neuron

outputs. We discuss below (re-)interpretations of neural computing in terms of GHD.
Fuzzy inference: As illustrated in deﬁnition 4 GHD induces a fuzzy XOR connective. Therefore the
negative of GHD quantiﬁes the degree of equivalence between inputs x and weights w (see deﬁnition
4 of fuzzy XOR)  i.e. the fuzzy truth value of the statement “x ↔ w” where ↔ denotes a fuzzy
equivalence relation. For GHD with multiple layers stacked together  neighbouring neuron outputs
i ↔
from the previous layer are integrated to form composite statements e.g. “(x1
i ) ↔ w2
j ” where superscripts correspond to two layers. Thus stacked layers will form more
w1
complex  and hopefully more powerful  statements as the layer depth increases.

1 ↔ w1

1  . . .   x1

4

Figure 2: Left to right: mean  max and min of neuron outputs  with/without batch normalized (BN 
WO_BN) and generalized hamming distance (XOR). Outputs are averaged over all 64 ﬁlters in the
ﬁrst convolution layer and plotted for 30 epochs training of a MNIST network used in our experiment
(see Section 4).

Batch normalization demystiﬁed: When a mini-batch of training samples X = {x1  . . .   xM} is
involved in the computation  due to the distributive property of GHD  the data-dependent bias term

xl equals the arithmetic mean of corresponding bias terms computed for each sample in the

L(cid:80)

l=1

M(cid:80)

L(cid:80)

l=1

1
M

m=1

mini-batch i.e.
l . It is almost impossible to maintain a constant scalar b that fulﬁls
xm
this requirement when mini-batch changes  especially at deep layers of the network whose inputs
are inﬂuenced by weights of incoming layers. The celebrated batch normalization (BN) technique
therefore proposed a learning method to compensate for the input vector change  with additional
parameters γ  β to be learnt during the training [9]. It is our conjecture that batch normalization is
approximating these rightful bias through optimization  and this connection is empirically revealed
in Figure 2 with very similar neuron outputs obtained by BN and GHD. Indeed they are highly
correlated during the course of training (with Pearson correlation coefﬁcient=0.97)  conﬁrming our
view that BN is attempting to inﬂuence the bias term according to (3).
Once b is enforced to follow (3)  neither the optimization of bias terms nor the sophisticated learning
method of BN is needed. In the following section we will illustrate a rectiﬁed neural network designed
as such.
Rectiﬁed linear units (ReLU) redesigned: Due to its strong biological motivations [10] and mathe-
matical justiﬁcations [11]  rectiﬁed linear unit (ReLu) is the most popular activation function used for
deep neural network [31]. If neuron outputs are rectiﬁed as the generalized hamming distances  the
activation function max(0  0.5 − h(x  w)) then simply sets a minimal hamming distance threshold
of 0.5 (see Figure 1). Astute readers may immediately spot two limitations of this activation function:
a) it only takes into account the negative conﬁdence region while disregards positive conﬁdence
regions; b) it allows elements in the fuzzy regime near 0.5 to misguide the optimization with their
non-negligible gradients.
A straightforward remedy to ReLU is to suppress elements within the fuzzy region by setting outputs
between [0.5 − r  0.5 + r] to 0.5  where r is a parameter to control acceptable fuzziness in neuron
outputs. In particular  we may set thresholds adaptively e.g. [0.5 − r · O  0.5 + r · O] where O
is the maximal magnitude of neuron outputs and the threshold ratio r is adjusted by the optimizer.
This double-thresholding strategy effectively prevents noisy gradients of fuzzy elements  since
0.5 is a ﬁxed point and x ⊕ 0.5 = 0.5 for any x. Empirically we found this scheme  in tandem
with the rectiﬁcation (3)  dramatically boosts the training efﬁciency for challenging tasks such as
CIFAR10/100 image classiﬁcation. It must be noted that  however  the use of non-linear activation as
such is not essential for GHD-based neural computing. When the double-thresholding is switched-off
(by ﬁxing r = 0)  the learning is prolonged for challenging CIFAR10/100 image classiﬁcation but its
inﬂuence on the simple MNIST classiﬁcation is almost negligible (see Section 4 for experimental
results).

3.2 Ganeralized hamming network with induced fuzzy XOR

Deﬁnition 2. A generalized hamming network (GHN) is any networks consisting of neurons  whose
outputs h ∈ HL are related to neuron inputs x ∈ HL and weights w ∈ HL by h = x ⊕L w .

5

051015202530−0.16−0.14−0.12−0.10−0.08−0.06−0.04−0.020.00Y: Mean outputs X:epochs( 100)BNXORWO_BN051015202530012345678Y: Max outputs X:epochs(x100)BNXORWO_BN051015202530−7−6−5−4−3−2−10Y: Min outputs X:epochs(x100)BNXORWO_BNRemark: In case that the bias term is computed directly from (3) such that h = x ⊕L w is fulﬁlled
strictly  the network is called a rectiﬁed GHN or simply a GHN. In other cases where bias terms are
approximating the rightful offsets (e.g. by batch normalization [9])  the trained network is called an
approximated GHN.

Compared with traditional neural networks  the optimization of bias terms is no longer needed in
GHN. Empirically  it is shown that the proposed GHN beneﬁts from a fast and robust learning process
that is on par with that of the batch-normalization approach  yet without resorting to sophisticated
learning process of additional parameters (see Section 4 for experimental results). On the other hand 
GHN also beneﬁts from the rapid developments of neural computing techniques  in particular  those
employing parallel computing on GPUs. Due to this efﬁcient implementation of GHNs  it is the ﬁrst
time that fuzzy neural networks have demonstrated state-of-the-art performances on learning tasks
with large scale datasets.
Often neuron outputs are clamped by a logistic activation function to within the range [0  1]  so
that outputs can be compared with the target labels in supervised learning. As shown below  GHD
followed by such a non-linear activation actually induces a fuzzy XOR connective. We brieﬂy review
basic notion of fuzzy set used in our work and refer readers to [2  32  13] for thorough treatments and
review of the topic.
Deﬁnition 3. Fuzzy Set: Let X be an universal set of elements x ∈ X  then a fuzzy set A is a set

of pairs: A := {(cid:0)x  µA(x)(cid:1)|x ∈ X  µA(x) ∈ I}  in which µA : X → I is called the membership

function (or grade membership).
Remark: In this work we let X be a Cartesian product of two sets X = P × U where P are (2D or
3D) collection of neural nodes and U are real numbers in ⊆ I or ⊆ R. We deﬁne the membership
function µX (x) := µU (xp) ∀x = (p  xp) ∈ X such that it is dependent on xp only. For the sake of
brevity we abuse the notation and use µ(x)  µX (x) and µU (xp) interchangeably.
Deﬁnition 4. Induced fuzzy XOR: let two fuzzy set elements a  b ∈ U be assigned with respective
grade or membership by a membership function µ : U → I : µ(a) = i  µ(b) = j  then the
generalized hamming distance h(a  b) : U × U → U induces a fuzzy XOR connective E : I × I → I
whose membership function is given by

µR(i  j) = µ(h(µ−1(i)  µ−1(j))).

(4)

Remark: For the restricted case U = I the membership function can be trivially deﬁned as the identity
function µ = idI as proved in [4].
Remark: For the generalized case where U = R  the fuzzy membership µ can be deﬁned by a sigmoid
function such as logistic  tanh or any function : U → I. In this work we adopt the logistic function
1+exp(0.5−a) and the resulting fuzzy XOR connective is given by following membership
µ(a) =
function:

1

1 + exp(cid:0)0.5 − µ−1(i) ⊕ µ−1(j)(cid:1)  

1

(5)

µR(i  j) =
a − 1) + 1

where µ−1(a) = − ln( 1
2 is the inverse of µ(a). Following this analysis  it is possible to
rigorously formulate neuron computing of the entire network according to inference rules of fuzzy
logic theory (in the same vein as illustrated in [17]). Nevertheless  research along this line is out of
the scope of the present article and will be reported elsewhere.

4 Performance evaluation

4.1 A case study with MNIST image classiﬁcation

Overall performance: we tested a simple four-layered GHN (cv[1 5 5 16]-pool-cv[16 5 5 64]-pool-
fc[1024]-fc[1024 10]) on the MNIST dataset with 99.0% test accuracy obtained. For this relatively
simple dataset  GHN is able to reach test accuracies above 0.95 with 1000 mini-batches and a
learning rate 0.1. This learning speed is on par with that of the batch normalization (BN)  but without
resorting to the learning of additional parameters in BN. It was also observed a wide range of large
learning rates (from 0.01 to 0.1) all resulted in similar ﬁnal accuracies (see below). We ascribe this
well-controlled robust learning behaviour to rectiﬁed bias terms enforced in GHNs.

6

Figure 3: Test accuracies of MNIST classiﬁcation with Generalized Hamming Network (GHN). Left:
test accuracies without using non-linear activation (by setting r = 0). Middle: with r optimized for
each layer. Right: with r optimized for each ﬁlter. Four learning rates i.e. {0.1  0.05  0.025  0.01} are
used for each case with the ﬁnal accuracy reported in brackets. Note that the number of mini-batch
are in logarithmic scale along x-axis.

Inﬂuence of learning rate: This experiment compares performances with different learning rates
and Figure 3 (middle right) show that a very large learning rate (0.1) leads to much faster learning
without the risk of divergences. A small learning rate (0.01) sufﬁce to guarantee the comparable ﬁnal
test accuracy. Therefore we set the learning rate to a constant 0.1 for all experiments unless stated
otherwise.
Inﬂuence of non-linear double-thresholding: The non-linear double-thresholding can be turned off by
setting the threshold ratio r = 0 (see texts in Section 3.1). Optionally the parameter r is automatically
optimized together with the optimization of neuron weights. Figure 3 (left) shows that the GHN
without non-linear activation (by setting r = 0) performs equally well as compared with the case
where r is optimized (in Figure 3 left  right). There are no signiﬁcant differences between two settings
for this relative simple task.

4.2 CIFAR10/100 image classiﬁcation

In this experiment  we tested a six-layered GHN (cv[3 3 3 64]-cv[64 5 5 256]-pool-cv[256 5 5 256]-
pool-fc[1024]-fc[1024 512]-fc[1024 nclass]) on both CIFAR10 (nclass=10) and CIFAR100
(nclass=100) datasets. Figure 4 shows that the double-thresholding scheme improves the learn-
ing efﬁciency dramatically for these challenging image classiﬁcation tasks: when the parameter r
is optimized for each feature ﬁlter the numbers of iterations required to reach the same level of test
accuracy are reduced by 1 to 2 orders of magnitudes. It must be noted that performances of such a
simple generalized hamming network (89.3% for CIFAR10 and 60.1% for CIFAR100) are on par
with many sophisticated networks reported in [33]. In our view  the rectiﬁed bias enforced by (3) can
be readily applied to these sophisticated networks  although resulting improvements may vary and
remain to be tested.

4.3 Generative modelling with Variational Autoencoder

In this experiment  we tested the effect of rectiﬁcation in GHN applied to a generative modelling
setting. One crucial difference is that the objective is now to minimize reconstruction error instead of
classiﬁcation error. It turns out the double-thresholding scheme is no longer relevant for this setting
and thus not used in the experiment.
The baseline network (784-400-400-20) used in this experiment is an improved implementation [34]
of the inﬂuential paper [35]  trained on the MNIST dataset of images of handwritten digits. We have
rectiﬁed the outputs following (3) and  instead of optimizing the lower bound of the log marginal
likelihood as in [35]  we directly minimize the reconstruction error. Also we did not include weights
regularization terms for the optimization as it is unnecessary for GHN. Figure 5 (left) illustrates
the reconstruction error with respect to number of training steps (mini-batches). It is shown that
the rectiﬁed generalized hamming network converges to a lower minimal reconstruction error as
compared to the baseline network  with about 28% reduction. The rectiﬁcation also leads to a faster
convergence  which is in accordance with our observations in other experiments.

7

3.03.54.04.55.0log(#mini_batch)0.800.850.900.951.00Accuracyrate0.1 (98.97%)rate0.05 (98.86%)rate0.025 (98.96%)rate0.01 (98.69%)3.03.54.04.55.0log(#mini_batch)0.750.800.850.900.951.00Accuracyrate0.1 (98.91%)rate0.05 (99.01%)rate0.025 (98.86%)rate0.01 (98.65%)3.03.54.04.55.0log(#mini_batch)0.750.800.850.900.951.00Accuracyrate0.1 (98.98%)rate0.05 (98.83%)rate0.025 (98.84%)rate0.01 (98.63%)Figure 4: Left: GHN test accuracies of CIFAR10 classiﬁcation (OPT THRES: parameter r op-
timized; WO THRES: without nonlinear activation). Right: GHN test accuracies of CIFAR100
classiﬁcation(OPT THRES: parameter r optimized; WO THRES: without non-linear activation).

Figure 5: Left: Reconstruction errors of convolution VAE with and w/o rectiﬁcation. Right: Evalua-
tion accuracies of Sentence classiﬁcation with GHN rectiﬁcation and w/o rectiﬁcation).

4.4 Sentence classiﬁcation

A simple CNN has been used for sentence-level classiﬁcation tasks and excellent results were
demonstrated on multiple benchmarks [36]. The baseline network used in this experiment is a
re-implementation of [36] made available from [37]. Figure 5 (right) plots accuracy curves from both
networks. It was observed that the rectiﬁed GHN did improve the learning speed  but did not improve
the ﬁnal accuracy as compared with the baseline network: both networks yielded the ﬁnal evaluation
accuracy around 74% despite that the training accuracy were almost 100%. The over-ﬁtting in this
experiment is probably due to the relatively small Movie Review dataset size with 10 662 example
review sentences  half positive and half negative.

5 Conclusion

In summary  we proposed a rectiﬁed generalized hamming network (GHN) architecture which materi-
alizes a re-emerging principle of fuzzy logic inferencing. This principle has been extensively studied
from a theoretic fuzzy logic point of view  but has been largely overlooked in the practical research
of ANN. The rectiﬁed neural network derives fuzzy logic implications with underlying generalized
hamming distances computed in neuron outputs. Bearing this rectiﬁed view in mind  we proposed to
compute bias terms analytically without resorting to sophisticated learning methods such as batch
normalization. Moreover  we have shown that  the rectiﬁed linear units (ReLU) was theoretically
non-essential and could be skipped for some easy tasks. While for challenging classiﬁcation problems 
the double-thresholding scheme did improve the learning efﬁciency signiﬁcantly.
The simple architecture of GHN  on the one hand  lends itself to being analysed rigorously and this
follow up research will be reported elsewhere. On the other hand  GHN is the ﬁrst fuzzy neural
network of its kind that has demonstrated fast learning speed  well-controlled behaviour and state-
of-the-art performances on a variety of learning tasks. By cross-checking existing networks against
GHN  one is able to grasp the most essential ingredient of deep learning. It is our hope that this kind
of comparative study will shed light on future deep learning research and eventually open the “black
box” of artiﬁcial neural networks [16].

8

3.03.54.04.55.05.56.0log(#mini_batch)0.40.50.60.70.80.9AccuracyOPT_THRES (89.26%)WO_THRES (84.63%)3.003.253.503.754.004.254.504.75log(#mini_batch)0.10.20.30.40.50.6AccuracyOPT_THRES (60.05%)WO_THRES (51.71%)020000400006000080000100000#mini_batch15002000250030003500400045005000Reconstruction errorGHNVAE010002000300040005000#mini_batch0.500.550.600.650.700.75accuracyGHNCNNAcknowledgement

I am grateful to anonymous reviewers for their constructive comments to improve the quality of this
paper. I greatly appreciate valuable discussions and supports from colleagues at Nokia Technologies.

References
[1] M M Gupta and D H Rao. Invited Review on the principles of fuzzy neural networks. Fuzzy Sets and

Systems  61:1–18  1994.

[2] L.A. Zadeh. Fuzzy sets. Information Control  8:338–353  1965.

[3] József Tick  János Fodor  and John Von Neumann. Fuzzy Implications and Inference Process. Computing

and Informatics  24:591–602  2005.

[4] Benjamín C Bedregal  Renata H S Reiser  and Graçaliz P Dimuro. Xor-Implications and E-Implications:
Classes of Fuzzy Implications Based on Fuzzy Xor. Electronic Notes in Theoretical Computer Science 
247:5–18  2009.

[5] Krassimir Atanassov. On Zadeh’s intuitionistic fuzzy disjunction and conjunction. NIFS  17(1):1–4  2011.

[6] Abhay B Ulsari. Training Artiﬁcial Neural Networks for Fuzzy Logic. Complex Systems  6:443–457  1992.

[7] Witold Pedrycz and Giancarlo Succi. fXOR fuzzy logic networks. Soft Computing  7  2002.

[8] H.-J Zimmermann. Fuzzy set theory review. Advanced Review  2010.

[9] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Francis R. Bach and David M. Blei  editors  ICML  volume 37  pages 448–456 
2015.

[10] R. Hahnloser  R. Sarpeshkar  M. Mahowald  R.J. Douglas  H.S.Seung. Digital selection and analogue

ampliﬁcation coexist in a cortex-inspired silicon circuit. 405  2000.

[11] H.S. Seung R Hahnloser. Permitted and forbidden sets in symmetric threshold-linear networks. In NIPS 

2001.

[12] Xavier Glorot  Antoine Bordes  and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In Geoffrey
Gordon  David Dunson  and Miroslav Dudík  editors  Proceedings of the Fourteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics  volume 15 of Proceedings of Machine Learning Research 
pages 315–323  11–13 Apr 2011.

[13] R. Belohlavek  J.W. Dauben  and G.J. Klir. Fuzzy Logic and Mathematics: A Historical Perspective.

Oxford University Press  2017.

[14] P. Liu and H.X. Li. Fuzzy Neural Network Theory and Application. Series in machine perception and

artiﬁcial intelligence. World Scientiﬁc  2004.

[15] Jyh-Shing Roger Jang and Chuen-Tsai Sun. Functional equivalence between radial basis function networks

and fuzzy inference systems. IEEE Trans. Neural Networks  4(1):156–159  1993.

[16] José Manuel Benítez  Juan Luis Castro  and Ignacio Requena. Are artiﬁcial neural networks black boxes?

IEEE Trans. Neural Networks  8(5):1156–1164  1997.

[17] Warren Mcculloch and Walter Pitts. A logical calculus of ideas immanent in nervous activity. Bulletin of

Mathematical Biophysics  5:127–147  1943.

[18] Zhiting Hu  Xuezhe Ma  Zhengzhong Liu  Eduard H. Hovy  and Eric P. Xing. Harnessing deep neural
networks with logic rules. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics  ACL 2016  August 7-12  2016  Berlin  Germany  Volume 1: Long Papers  2016.

[19] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate

training of deep neural networks. page 901  2016.

[20] Geoffrey Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural networks.

Science  313(5786):504 – 507  2006.

9

[21] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In
Johannes Fürnkranz and Thorsten Joachims  editors  Proceedings of the 27th International Conference on
Machine Learning (ICML-10)  pages 807–814. Omnipress  2010.

[22] Matthieu Courbariaux and Yoshua Bengio. Binarized neural network: Training deep neural networks with

weights and activations constrained to +1 or -1. CoRR  abs/1602.02830  2016.

[23] Mohammad Rastegari  Vicente Ordonez  Joseph Redmon  and Ali Farhadi. Xnor-net: Imagenet classiﬁca-

tion using binary convolutional neural networks. CoRR  abs/1603.05279  2016.

[24] Paul Viola and Michael J. Jones. Robust real-time face detection. Int. J. Comput. Vision  57(2):137–154 

May 2004.

[25] Michael Calonder  Vincent Lepetit  Christoph Strecha  and Pascal Fua. Brief: Binary robust independent
elementary features. In Proceedings of the 11th European Conference on Computer Vision: Part IV 
ECCV’10  pages 778–792  2010.

[26] Ethan Rublee  Vincent Rabaud  Kurt Konolige  and Gary Bradski. Orb: An efﬁcient alternative to sift
or surf. In Proceedings of the 2011 International Conference on Computer Vision  ICCV ’11  pages
2564–2571  Washington  DC  USA  2011.

[27] Brian Kulis and Trevor Darrell. Learning to hash with binary reconstructive embeddings. In Proceedings of
the 22Nd International Conference on Neural Information Processing Systems  NIPS’09  pages 1042–1050 
2009.

[28] Mohammad Norouzi and David M. Blei. Minimal loss hashing for compact binary codes. In Proceedings
of the 28th International Conference on Machine Learning (ICML-11)  pages 353–360  New York  NY 
USA  2011.

[29] Kevin Lin  Huei-Fang Yang  Jen-Hao Hsiao  and Chu-Song Chen. Deep learning of binary hash codes
for fast image retrieval. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
Workshops  June 2015.

[30] Mohammad Norouzi  David J Fleet  and Ruslan R Salakhutdinov. Hamming distance metric learning. In
F. Pereira  C. J. C. Burges  L. Bottou  and K. Q. Weinberger  editors  Advances in Neural Information
Processing Systems 25  pages 1061–1069. 2012.

[31] Yann Lecun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. Nature  521(7553):436–444  5 2015.

[32] H.-J. Zimmermann. Fuzzy Set Theory — and Its Applications. Kluwer Academic Publishers  Norwell 

MA  USA  2001.

[33] What

is the class of this image?

in objects classiﬁ-
cation. http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_
results.html. Accessed: 2017-07-19.

Discover the current state of the art

[34] A baseline variational auto-encoder based on "auto-encoding variational bayes". https://github.com/

y0ast/VAE-TensorFlow. Accessed: 2017-05-19.

[35] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR  abs/1312.6114  2013.

[36] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. CoRR  abs/1408.5882  2014.

[37] A baseline cnn network for sentence classiﬁcation implemented with tensorﬂow. https://github.com/

dennybritz/cnn-text-classification-tf. Accessed: 2017-05-19.

10

,Ahmed Hefny
Carlton Downey
Geoffrey Gordon
Mohammad Ghavamzadeh
Marek Petrik
Yinlam Chow
Lixin Fan