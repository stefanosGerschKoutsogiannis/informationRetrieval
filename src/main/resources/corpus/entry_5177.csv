2019,SpiderBoost and Momentum: Faster Variance Reduction Algorithms,SARAH and SPIDER are two recently developed stochastic variance-reduced algorithms  and SPIDER has been shown to achieve a near-optimal first-order oracle complexity in smooth nonconvex optimization. However  SPIDER uses an accuracy-dependent stepsize that slows down the convergence in practice  and cannot handle objective functions that involve nonsmooth regularizers. In this paper  we propose SpiderBoost as an improved scheme  which allows to use a much larger constant-level stepsize while maintaining the same near-optimal oracle complexity  and can be extended with proximal mapping to handle composite optimization (which is nonsmooth and nonconvex) with provable convergence guarantee. In particular  we show that proximal SpiderBoost achieves an oracle complexity of  O(min{n^{1/2}\epsilon^{-2} \epsilon^{-3}})  in composite nonconvex optimization  improving the state-of-the-art result by a factor of  O(min{n^{1/6} \epsilon^{-1/3}}). We further develop a novel momentum scheme to accelerate SpiderBoost for composite optimization  which achieves the near-optimal oracle complexity in theory and substantial improvement in experiments.,SpiderBoostandMomentum:FasterStochasticVarianceReductionAlgorithmsZheWangDepartmentofECETheOhioStateUniversitywang.10982@osu.eduKaiyiJiDepartmentofECETheOhioStateUniversityji.367@osu.eduYiZhouDepartmentofECETheUniversityofUtahyi.zhou@utah.eduYingbinLiangDepartmentofECETheOhioStateUniversityliang.889@osu.eduVahidTarokhDepartmentofECEDukeUniversityvahid.tarokh@duke.eduAbstractSARAHandSPIDERaretworecentlydevelopedstochasticvariance-reducedalgorithms andSPIDERhasbeenshowntoachieveanear-optimalﬁrst-orderoraclecomplexityinsmoothnonconvexoptimization.However SPIDERusesanaccuracy-dependentstepsizethatslowsdowntheconvergenceinpractice andcannothandleobjectivefunctionsthatinvolvenonsmoothregularizers.Inthispaper weproposeSpiderBoostasanimprovedscheme whichallowstouseamuchlargerconstant-levelstepsizewhilemaintainingthesamenear-optimaloraclecomplexity andcanbeextendedwithproximalmappingtohandlecompositeoptimization(whichisnonsmoothandnonconvex)withprovableconvergenceguarantee.Inparticular weshowthatproximalSpiderBoostachievesanoraclecomplexityofO(min{n1/2−2 −3})incompositenonconvexoptimization improvingthestate-of-the-artresultbyafactorofO(min{n1/6 −1/3}).WefurtherdevelopanovelmomentumschemetoaccelerateSpiderBoostforcompositeoptimization whichachievesthenear-optimaloraclecomplexityintheoryandsubstantialimprovementinexperiments.1IntroductionWeconsiderthefollowingﬁnite-sumoptimizationproblemminx∈RdΨ(x):=f(x) wheref(x):=1nnXi=1fi(x)(P)wherethefunctionfdenotesthetotallossonthetrainingsamplesandingeneralisnonconvex.Sincelarge-scalemachinelearningproblemscanhaveverylargesamplesizen thefull-batchgradientdescentalgorithmhashighcomputationalcomplexity.Thus variousstochasticgradientdescent(SGD)algorithmshavebeenproposed.Fornonconvexoptimization thebasicSGDalgorithm whichcalculatesthegradientofonedatasampleperiteration hasbeenshowntoyieldanoverallstochasticﬁrst-orderoracle(SFO)complexity i.e. gradientcomplexity ofO(−4)[9]toattainaﬁrst-orderstationarypoint¯xthatsatisﬁesEk∇f(¯x)k≤.SGDalgorithmswithdiminishingstep-size[9 5]orasufﬁcientlylargebatchsize[37 11]werealsoproposedtoguaranteetheirconvergencetoastationarypointratherthanitsneighborhood.Furthermore variousvariancereductionmethodshavebeenproposed whichconstructmoreaccuratestochasticgradientestimatorsthanthatofSGD e.g. SAG[31] SAGA[7]andSVRG[16].In33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.particular SAGAandSVRGhavebeenshowntoyieldanoverallSFOcomplexityofO(n2/3−2)[29 4].Recently [24 25]proposedavariancereductionmethodcalledSARAH wherethegradientestimatorissequentiallyupdatedintheinnerlooptoimprovetheestimationaccuracy.Inparticular SARAHhasbeenshownin[25]toachieveanoverallO(−4)SFOcomplexityfornonconvexoptimization.AnothervariancereductionmethodcalledSPIDERwasproposedin[8] whichusesthesamegradientestimatorasthatofSARAHbutadoptsanormalizedgradientupdatewithastepsizeη=O(/L).[8]showedthatSPIDERachievesanoverallO(min{n1/2−2 −3})SFO whichwasfurthershowntobeoptimalintheregimewithn≤O(−4).ThoughSPIDERistheoreticallyappealing threeimportantissuesstillrequirefurtherattention.First SPIDERrequiresaveryrestrictivestepsizeη=O(/L)toguaranteeitsconvergence whichpreventsSPIDERfrommakingbigprogressevenifitispossible.Relaxingsuchaconditionappearsnoteasyunderitsoriginalconvergenceanalysisframework.•ThispaperproposesamorepracticalSpiderBoostalgorithm whichallowsamuchlargerstepsizeη=O(1/L)thanSPIDERwhileretainingthesamestate-of-the-artcomplexityorderasSPIDER(seeTable2inSuppl).Thisisduetothenewconvergenceanalysisideathatwedevelop whichanalyzestheincrementsofvariablesovereachentireinnerloopratherthanovereachinner-loopiteration andhenceyieldstighterboundandconsequentlymorerelaxedstepsizerequirement.Second theconvergenceanalysisofSPIDERrequiresaverysmallper-iterationincrementkxk+1−xkk=O(/L) whichisdifﬁculttoguaranteeifoneattemptstogeneralizeittoaproximalalgorithmforsolvingthecompositeoptimizationproblem(seeSection3)thatpossiblyinvolvesnonsmoothness.Hence generalizingSPIDERtotheproximalsettingwithprovableconvergenceguaranteeischallenging.•OurSpiderBoosthasanaturalgeneralization i.e. theProx-SpiderBoostalgorithm whichcanbeappliedtosolvecompositeoptimizationproblems.WeshowthatProx-SpiderBoostachievesaSFOcomplexityofO(n1/2−2)andaproximaloracle(PO)complexityofO(−2) whichimprovestheexistingbestresultsbyafactorofO(n1/6)(seeTable1).Third althoughSPIDERachievesthenear-optimaloraclecomplexityinnonconvexoptimization itspracticalperformancehasbeenfound[25 8]tobehardlyadvantageousoverSVRG.Therefore itisofvitalimportancetoexploitotheralgorithmicdimensionstofurtherimprovethepracticalperformanceofSPIDER andmomentumissuchapromisingperspective.However theexistinganalysisofvariance-reducedalgorithmshasbeenexploredforSVRGonlyincertainconvexscenarios[27 1 2 32]andunderalocalgradientdominancegeometryinnonconvexoptimization[19].Therefore itisnotevenclearwhetheracertainmomentumschemecanbeappliedtoSPIDERandyieldtheoptimaloraclegradientcomplexityforgeneralnonconvexoptimization.•ThispaperproposesamomentumschemetoacceleratetheProx-SpiderBoost namedProx-SpiderBoost-M forcompositeoptimization.WeshowthatProx-SpiderBoost-MachievesanoraclecomplexityorderofO(n+√n−2) matchingthecomplexitylowerboundfornonconvexoptimization.Incontrasttotheexistinganalysisforstochasticalgorithmswithmomentum[10]fornonconvexoptimization ourproofexploitsthemartingalestructureofthegradientestimatortoboundthevariancetermanditsaccumulationsovertheentireoptimizationpathinatightwayunderthemomentumscheme.Duetospacelimitation werelegateseveralotherresultstothesupplementarymaterials includinganalysisofProx-SpiderBoostundernon-EuclideangeometryandPolyak-Łojasiewiczcondition andanalysisofbothProx-SpiderBoostandProx-SpiderBoost-Mforonlinenonconvexcompositeoptimization.1.1RelatedWorkStochasticalgorithmsforsmoothnonconvexoptimization:TheconvergenceanalysisforSGDwasstudiedin[11]forsmoothnonconvexoptimization.SGDwithdiminishingstepsizeandsuf-ﬁcientlylargebatchsizewerefurtherstudiedin[11 5 37]toimprovetheperformance.Variousvariance-reducedalgorithmshavebeenproposedandstudied including e.g. SAG[31] SAGA[7] SVRG[16 29 4] SCSG[18] SNVRG[36] SARAH[24 25 26 28] SPIDER[8].Inparticular SPIDERhasbeenshownin[8]toachievetheoraclecomplexitylowerboundforacertainregime.Suchanideahasalsobeenextendedforoptimizationovermanifoldsin[38 35] zeroth-orderopti-mizationin[15] ADMMin[12] zeroth-orderADMMin[13] problemwithnonsmoothnonconvex2Table1:ComparisonofSFOcomplexityandPOcomplexityforcompositeoptimization.AlgorithmsStepsizeηFinite-SumFinite-Sum/Online1SFOPOSFOPOProxGD[11]O(L−1)O(n−2)O(−2)N/AN/AProxSGD[11]O(L−1)N/AN/AO(−4)O(−2)ProxSVRG/SAGA[30]O(L−1)O(n+n2/3−2)O(−2)N/AN/ANatasha1.5[3]O(2/3L−2/3)N/AN/AO(−3+−10/3)O(−10/3)ProxSVRG+[22]O(L−1)O(n+n2/3−2)O(−2)O(−10/3)O(−2)Prox-SpiderBoost(ThisWork)O(L−1)O(n+n1/2−2)O(−2)O(−2+−3)O(−2)1Theonlinesettingreferstothecasewheretheobjectivefunctiontakestheformoftheexpectedvalueofthelossfunctionoverthedatadistribution.Suchamethodcanalsobeappliedtosolvetheﬁnite-sumproblem andhencetheSFOcomplexityinthelastcolumnisapplicabletoboththeﬁnite-sumandonlineproblems.Thus foralgorithmsthathaveSFOboundsavailableinbothofthelasttwocolumns theminimumbetweenthetwoboundsprovidesthebestboundfortheﬁnite-sumproblem.regularizerin[33] stochasticcompositeoptimizationin[34] noisygradientdescentin[21] andanadaptivebatchsizeschemein[14].OurstudyhereproposesaSpiderBoostalgorithm whichsubstantiallyimprovesthestepsizeofSPIDERwhileretainingthesameperformanceguaranteeandperformsmuchfasterthanSPIDERinpractice.Stochasticalgorithmsforcompositenonconvexoptimization:ProximalSGDhasbeenproposedandstudiedby[9 10]tosolvecompositenonconvexoptimizationproblems.Moreover variancereducedalgorithmssuchasProx-SVRGandProx-SAGA[30] Natasha1.5[3] andProxSVRG+[22]havealsobeenproposedtofurtherimprovetheperformance.OurstudyproposesProx-SpiderBoost whichorder-leveloutperformsalltheexistingalgorithmsforcompositenonconvexoptimization.Momentumschemesfornonconvexoptimization:Fornonconvexoptimization [10]establishedconvergenceofSGDwithmomentumtoan-ﬁrst-orderstationarypointwithanoraclecomplexityofO(−4).TheconvergenceguaranteeofSVRGwithmomentumhasbeenexploredunderacertainlocalgradientdominancegeometryinnonconvexoptimization[19].Here weproposeProx-SpiderBoost-Mwhichachievesthecomplexitylowerboundforacertainregime andpracticallysubstantiallyoutperformsexistingvariancereducedalgorithmswithmomentum.2SpiderBoostforNonconvexOptimization2.1SpiderBoostAlgorithmInthissection weintroducetheSpiderBoostalgorithmdesignedfortheproblem(P).In[24] anovelgradientestimatorwasintroducedforreducingthevariance.Morespeciﬁcally consideracertaininnerloop{xk}q−1k=0.Theinitializationoftheestimatorissettobev0=∇f(x0).Then foreachsubsequentiterationk anindexsetSissampledandthecorrespondingestimatorvkisconstructedasvk=1|S|Xi∈S(cid:2)∇fi(xk)−∇fi(xk−1)+vk−1(cid:3).(1)Itcanbeseenthattheestimatorineq.(1)isconstructediterativelybasedontheinformationxk−1andvk−1thatareobtainedfromthepreviousupdate.Asacomparison theSVRGestimator[16]isconstructedbasedontheinformationoftheinitializationofthatloop(i.e. replacexk−1andvk−1ineq.(1)withx0andv0 respectively).Therefore theestimatorineq.(1)utilizesmorefreshinformationandyieldsmoreaccurateestimationofthefullgradient.Theestimatorineq.(1)hasbeenadoptedby[24 25]and[8]forproposingSARAHandSPIDER respectively.Inspeciﬁc SPIDERwasshownin[8]tobeoptimalintheregimewithn≤O(−4).ThoughSPIDERhasdesiredperformanceintheory itcanrunveryslowlyinpracticeduetothechoiceofaconservativestepsize.Inspeciﬁc SPIDERusesaverysmallstepsizeη=O(L)(whereisthedesiredaccuracy)innormalizedgradientdescent whichyieldssmallincrementperiteration i.e. kxk+1−xkk=O().ByfollowingtheanalysisofSPIDER suchastepsizeappearstobenecessaryinordertoachievethedesiredconvergencerate.3Algorithm1SpiderBoostInput:η=12L q K |S|∈N.fork=0 1 ... K−1doifmod(k q)=0thenComputevk=∇f(xk) elseDraw|S|sampleswithreplacement.Computevkaccordingtoeq.(1).endxk+1=xk−ηvk.endOutput:xξ whereξUnif∼{0 ... K−1}.Algorithm2Prox-SpiderBoostInput:η=12L q K |S|∈N.fork=0 1 ... K−1doifmod(k q)=0thenComputevk=∇f(xk) elseDraw|S|sampleswithreplacement.Computevkaccordingtoeq.(1).endxk+1=proxηh(xk−ηvk).endOutput:xξ whereξUnif∼{0 ... K−1}.SuchaconservativestepsizeadoptedbySPIDERmotivatesourdesignofanimprovedalgorithmnamedSpiderBoost(seeAlgorithm1) whichusesthesameestimatoreq.(1)asSARAHandSPIDER butadoptsamuchlargerstepsizeη=12L asopposedtoη=O(L)takenbySPIDER.Also SpiderBoostupdatesthevariableviaagradientdescentstep(sameasSARAH) asopposedtothenormalizedgradientdescentsteptakenbySPIDER.Furthermore SpiderBoostgeneratestheoutputvariableviaarandomstrategywhereasSPIDERoutputsdeterministically.Collectively SpiderBoostcanmakeaconsiderablylargerprogressperiterationthanSPIDER especiallyintheinitialoptimizationphasewheretheestimatedgradientnormkvkkislarge andisstillguaranteedtoachievethesamedesirableconvergencerateasSPIDER asweshowinthenextsubsection.WecomparetheempiricalperformancebetweenSPIDERandSpiderBoostinSection5.1.2.2ConvergenceAnalysisofSpiderBoostInthissubsection westudytheconvergencerateandcomplexityofSpiderBoost.Inparticular weadoptthefollowingstandardassumptions.Assumption1.Theobjectivefunctionintheproblem(P)satisﬁes:1.TheobjectfunctionΨisboundedbelow i.e. Ψ∗:=infx∈RdΨ(x)>−∞;2.Eachgradient∇fi i=1 ... nisL-Lipschitzcontinuous i.e. ∀x y∈Rd k∇fi(x)−∇fi(y)k≤Lkx−yk.Assumption1essentiallyassumesthatthesmoothobjectivefunctionhasanon-trivialminimumanditsgradientisLipschitzcontinuous whicharevalidandstandardconditionsinnonconvexoptimization.Then weobtainthefollowingconvergenceresultforSpiderBoost.Theorem1.LetAssumption1holdandapplySpiderBoostinAlgorithm1tosolvetheproblem(P)withparametersq=|S|=√nandstepsizeη=12L.Then thecorrespondingoutputxξsatisﬁesEk∇f(xξ)k≤providedthatthetotalnumberKofiterationssatisﬁesK≥O(cid:16)L(f(x0)−f∗)2(cid:17).Moreover theoverallSFOcomplexityisO(√n−2+n).Theorem1showsthattheoutputofSpiderBoostachievestheﬁrst-orderstationaryconditionwithinaccuracywithatotalSFOcomplexityO(√n−2+n).Thismatchesthelowerboundthatonecanexpectforﬁrst-orderalgorithmsintheregimen≤O(−4)[8].AsweexplaininSection2.1 SpiderBoostenhancesSPIDERmainlyduetotheutilizationofalargeconstantstepsize whichyieldssigniﬁcantaccelerationoverSPIDERinpracticeasweillustrateintheexperimentsinSection5.1.WenotethattheanalysisofSpiderBoostinTheorem1isverydifferentfromthatofSPIDERthatdependsonan-levelstepsizeandthenormalizedgradientdescentsteptoguaranteeaconstantincrementkxk+1−xkkineveryiteration.Incontrast SpiderBoostexploitsthespecialstructureofgradientestimatorandanalyzesthealgorithmovertheentireinnerloopratherthanovereachiteration andthusyieldsabetterbound.43Prox-SpiderBoostforNonconvexCompositeOptimizationInthissection wegeneralizeSpiderBoosttosolvethefollowingnonconvexcompositeproblem:minx∈XΨ(x):=f(x)+h(x) f(x):=1nnXi=1fi(x)(Q)wherethefunctionfispossiblynonconvex hisasimpleconvexbutpossiblynonsmoothregularizer andXisaconvexconstrainedset.Tohandlethenonsmoothness wenextintroducetheproximalmappingwhichisaneffectivetoolforcompositeoptimization.3.1PreliminariesonProximalMappingConsideraproperandlower-semicontinuousfunctionh(whichcanbenon-differentiable).Wedeﬁneitsproximalmappingatx∈Rdwithparameterη>0asproxηh(x):=argminu∈Xnh(u)+12ηku−xk2o.Suchamappingiswelldeﬁnedandisuniqueparticularlyforconvexfunctions.Furthermore theproximalmappingcanbeusedtogeneralizetheﬁrst-orderstationaryconditionofsmoothoptimizationtononsmoothcompositeoptimizationviathefollowingfact.Fact1.Lethbeaproperandconvexfunction.DeﬁnethefollowingnotionofgeneralizedgradientGη(x):=1η(cid:16)x−proxηh(x−η∇f(x))(cid:17).(2)Then xisacriticalpointofΨ:=f+h(i.e. 0∈∇f(x)+∂h(x))ifandonlyifGη(x)=0.Fact1introducesageneralizednotionofgradientforcompositeoptimization.Toelaborate considerthecaseh≡0sothattheproximalmappingbecomestheidentitymapping.Then thegeneralizedgradientGη(x)reducestothegradient∇f(x)oftheunconstrainedoptimization.Therefore the-ﬁrst-orderstationaryconditionforcompositeoptimizationisnaturallydeﬁnedaskGη(x)k≤.3.2Prox-SpiderBoostandOracleComplexityTogeneralizetocompositeoptimization SpiderBoostadmitsanaturalextensionProx-SpiderBoost whereasSPIDERencounterschallenges.ThemainreasonisbecauseSpiderBoostadmitsaconstantstepsizeanditsconvergenceguaranteedoesnothaveanyrestrictionontheper-iterationincrementofthevariable.However theconvergenceofSPIDERrequirestheper-iterationincrementofthevariabletobeatthe-level whichischallengingtosatisfyunderthenonlinearproximaloperatorincompositeoptimization.ThedetailedstepsofProx-SpiderBoost(whichgeneralizesSpiderBoosttocompositeoptimizationobjectives)aredescribedinAlgorithm2.Inparticular Prox-SpiderBoostupdatesthevariableviaaproximalgradientsteptohandlethepossiblenonsmoothnessincompositeoptimization.WenextcharacterizetheoraclecomplexityofProx-SpiderBoostforachievingthegeneralized-ﬁrst-orderstationarycondition.Theorem2.LetAssumption1holdandconsidertheproblem(Q)withX=Rd.ApplytheProx-SpiderBoostinAlgorithm2withparametersq=|S|=√nandη=12L.Then thecorrespondingoutputxξsatisﬁesEkGη(xξ)k≤providedthatthetotalnumberKofiterationssatisﬁesK≥O(cid:16)L(Ψ(x0)−Ψ∗)2(cid:17).Moreover theSFOcomplexityisO(√n−2+n) andtheproximaloracle(PO)complexityisO(−2).Asacomparison theSFOcomplexityO(√n−2+n)ofProx-SpiderBoostinTheorem2improvestheexistingcomplexityresultbyafactorofn1/6[22].Furthermore thecomplexitylowerboundforachievingthe-ﬁrst-orderstationaryconditioninun-regularizedoptimization[8]alsoservesasalowerboundforcompositeoptimization(byconsideringthespecialcaseh≡0).Therefore the5SFOcomplexityofourProx-SpiderBoostmatchesthecorrespondingcomplexitylowerboundintheregimewithn≤O(−4) andishencenearoptimal.Moreover ourProx-SpiderBooststillachievesthestate-of-the-artconvergenceresultsunderothersettingssuchasonlineoptimization non-EuclideangeometryandPolyak-Łojasiewiczcondition.Duetothespacelimitation werelegatetheseresultstoAppendixD.4AcceleratingProx-SpiderBoostviaMomentumInthissection weproposeaproximalSpiderBoostalgorithmthatincorporatesamomentumscheme(referredtoasProx-SpiderBoost-M)forsolvingthecompositeproblem(Q) andstudyitstheoreticalguaranteeaswellastheoraclecomplexity.4.1AlgorithmDesignWepresentthedetailedupdateruleofProx-SpiderBoost-MinAlgorithm3.Algorithm3Prox-SpiderBoost-MInput:q K∈N {λk}K−1k=1 {βk}K−1k=1>0 y0=x0∈Rd andsetαk=2dk/qe+1.fork=0 1 ... K−1dozk=(1−αk+1)yk+αk+1xk ifmod(k q)=0thensetvk=∇f(zk) elseDrawξksampleswithreplacementandcomputevkaccordingtoeq.(1).endxk+1=proxλkh(cid:0)xk−λkvk(cid:1) yk+1=zk−βkλkxk+βkλkproxλkh(cid:0)xk−λkvk(cid:1).endOutput:zζ whereζUnif∼{0 ... K−1}.Toelaborateonthealgorithmdesign notethatProx-SpiderBoost-Mgeneratesatupleofvariablesequences{xk yk zk}kaccordingtothemomentumscheme.Inspeciﬁc thevariablesxk ykareupdatedviaproximalgradient-likestepsusingthegradientestimatevkproposedforSARAHin[24 25]anddifferentstepsizesλk βk respectively.Then theirconvexcombinationwithmomentumcoefﬁcientαk+1yieldsthevariablezk+1.Here wechooseastandardmomentumcoefﬁcientschedulingthatdiminishesepochwisely(seetheexpressionforαk)forprovingconvergenceguaranteeinnonconvexoptimization.Wealsonotethatthetwoupdatesforxk+1andyk+1donotintroduceextracomputationoverheadascomparedtoasingleupdate sincetheybothdependonthesameproximalterm.WewanttohighlightthedifferencebetweenourmomentumschemeforProx-SpiderBoost-MandtheexistingmomentumschemedesignforproximalSGDin[10]andproximalSVRGin[1].Intheseworks theyusethefollowingproximalgradientstepsforupdatingthevariablesxk+1andyk+1:xk+1=proxλkh(cid:0)xk−λkvk(cid:1) yk+1=proxβkh(cid:0)zk−βkvk(cid:1).(3)Notethateq.(3)usedifferentproximalupdatesthatarebasedonxkandzk respectively.Asacomparison ourmomentumschemeinAlgorithm3appliesthesameproximalgradienttermproxλkh(cid:0)xk−λkvk(cid:1)toupdatebothvariablesxk+1andyk+1 andthereforerequireslesscomputation.Moreover ourupdateforthevariableyk+1isnotasingleproximalgradientupdate(asopposedtoeq.(3)) anditcoupleswiththevariableszkandxk.Themomentumschemeintroducedin[1]wasnotproventohaveaconvergenceguaranteeinnonconvexoptimization.Inthenextsubsection weprovethatourmomentumschemeinAlgorithm3hasaprovableconvergenceguaranteefornonconvexcompositeoptimizationwithconvexregularizers.4.2ConvergenceandComplexityAnalysisInthissubsection westudytheconvergenceguaranteeofProx-SpiderBoost-Mforsolvingtheproblem(Q).Weobtainthefollowingmainresult.6Theorem3.LetAssumption1hold.ApplyProx-SpiderBoost-M(seeAlgorithm3)tosolvetheproblem(Q)withparametersq=|ξk|≡√n βk≡18Landλk∈[βk (1+αk)βk].Then theoutputzζproducedbythealgorithmsatisﬁesEkGλζ(zζ ∇f(zζ))k≤forany>0providedthatthetotalnumberKofiterationssatisﬁesK≥O(cid:18)L(Ψ(x0)−Ψ∗)2(cid:19).(4)Moreover theSFOcomplexityisatmostO(n+√n−2)andthePOcomplexityisatmostO(−2).Theorem3establishestheconvergencerateofProx-SpiderBoost-Mtosatisfythegeneralizedﬁrst-orderstationaryconditionandthecorrespondingoraclecomplexity.Speciﬁcally theiterationcom-plexitytoachievethegeneralized-ﬁrst-orderstationaryconditionisintheorderofO(−2) whichmatchesthatofProx-SpiderBoost.Furthermore thecorrespondingSFOcomplexityO(n+√n−2)matchesthelowerboundfornonconvexoptimization[8].Therefore Prox-SpiderBoost-MenjoysthesameoptimalconvergenceguaranteeasthatfortheProx-SpiderBoostinnonconvexoptimization anditfurtherbeneﬁtsfromthemomentumschemethatcanleadtosigniﬁcantaccelerationinpracticalapplications(aswedemonstrateviaexperimentsinSection5).Fromatechnicalperspective wehighlightthefollowingthreemajornewdevelopmentsintheproofofTheorem3thatisdifferentfromtheproofforthebasicstochasticgradientalgorithmwithmomentum[10]fornonconvexoptimization:1)ourproofexploitsthemartingalestructureoftheSPIDERestimatevkwhichallowstoboundthemean-squareerrortermEk∇f(zk)−vkk2inatightwayunderthemomentumscheme.Intraditionalanalysisofstochasticalgorithmswithmomentum[10] suchanerrortermcorrespondstothevarianceofthestochasticestimatorandisassumedtobeboundedbyauniversalconstant.2):Ourproofrequiresaverycarefulmanipulationoftheboundingstrategytohandletheaccumulationofthemean-squareerrorEk∇f(zk)−vkk2overtheentireoptimizationpath.5Experiments5.1ComparisonbetweenSpiderBoostandSPIDER020406080# of epochs1011109107105103101101Loss(f-f*)SPIDERSpiderBoost(a)Dataset:a9a020406080100120140# of epochs10151012109106103100Loss(f-f*)SPIDERSpiderBoost(b)Dataset:w8a020406080100# of epochs101410121010108106104102100102Loss(f-f*)SPIDERSpiderBoost(c)Dataset:a9a020406080100120# of epochs109107105103101101Loss(f-f*)SPIDERSpiderBoost(d)Dataset:w8aFigure1:(a)and(b):Logisticregressionproblemwithnonconvexregularizer.(c)and(d):Robustlinearregressionproblemwithanl2regularizer.Inthissubsection wecomparetheperformanceofSPIDERandSpiderBoostforsolvingthelogisticregressionproblemwithanonconvexregularizerandthenonconvexrobustlinearregressionproblem(SeeAppendixFfortheformsoftheobjectivefunctions).Foreachproblem weapplytwodifferentdatasetsfromtheLIBSVM[6]:thea9adataset(n=32561 d=123)andthew8adataset(n=49749 d=300).Forbothalgorithms weusethesameparametersettingexceptforthestepsize.Asspeciﬁedin[8]forSPIDER wesetη=0.01(determinedbyaprescribedaccuracytoguaranteeconvergence).Ontheotherhand SpiderBoostallowstosetη=0.05.Figure1showstheconvergenceofthefunctionvaluegapofbothalgorithmsversusthenumberofpassesthataretakenoverthedata.ItcanbeseenthatSpiderBoostenjoysamuchfasterconvergencethanthatofSPIDERduetotheallowanceofalargestepsize.Furthermore SPIDERoscillatesaroundapoint whichistheprescribedaccuracythatdeterminestheadoptedstepsizeη=0.01.ThisimpliesthatsettingalargerstepsizeforSPIDERwouldcauseittosaturateandstarttooscillateatacertainfunctionvalue whichisundesired.5.2ComparisonofSpiderBoostTypeofAlgorithmswithOtherAlgorithmsInthissubsection wecomparetheperformanceofourSpiderBoost(forsmoothproblems) Prox-SpiderBoost(forcompositeproblems) andProx-SpiderBoost-Mwithotherexistingstochastic7variance-reducedalgorithmsincludingSVRGin[16] Katyushansin[1] ASVRGin[32] RSAGin[10].WenotethatallalgorithmsusecertainmomentumschemesexceptforSVRG SpiderBoost andProx-SpiderBoost.Forallalgorithmsconsidered wesettheirlearningratestobe0.05.Foreachexperiment weinitializeallthealgorithmsatthesamepointthatisgeneratedrandomlyfromthenormaldistribution.Also wechooseaﬁxedmini-batchsize256andsettheepochlengthqtobe2n/256suchthatallalgorithmspassovertheentiredatasettwiceineachepoch.Weﬁrstapplythesealgorithmstosolvetwosmoothnonconvexproblems:logisticregressionandrobustlinearregressionproblems eachwithdatasetsofa9aandw8a andreporttheexperimentresultsinFigure2.OnecanseefromFigure2thatourProx-SpiderBoost-Machievesthebestperformanceandsigniﬁcantlyoutperformsotheralgorithms.Also theperformancesofbothKatyushansandASVRGdonotachievemuchaccelerationinsuchanonconvexcase asthesealgorithmsareoriginallydevelopedtoachieveaccelerationforconvexproblems.ThisdemonstratesthatourdesignofProx-SpiderBoost-Mhasastableperformanceinnonconvexoptimizationaswellasprovabletheoreticalguarantee.WenotethatthecurveofSpiderBoostoverlapswiththatofSVRGsimilarlytotheresultsreportedinotherrecentstudies.020406080# of epochs104103102101100101Loss(f-f*)SVRGASVRGKatyusha_nsRSAGSpiderBoostSpiderBoost-M(a)Dataset:a9a0510152025# of epochs101100101Loss(f-f*)SVRGASVRGKatyusha_nsRSAGSpiderBoostSpiderBoost-M(b)Dataset:w8a0510152025303540# of epochs101100Loss(f-f*)SVRGASVRGKatyusha_nsRSAGSpiderBoostSpiderBoost-M(c)Dataset:a9a01020304050# of epochs101100Loss(f-f*)SVRGASVRGKatyusha_nsRSAGSpiderBoostSpiderBoost-M(d)Dataset:w8aFigure2:(a)and(b):Logisticregressionwithnonconvexregularizer (c)and(d):Robustlinearregression..Wefurtheraddan‘1nonsmoothregularizerwithweightcoefﬁcient0.1totheobjectivefunctionsoftheabovetwooptimizationproblems andapplythecorrespondingproximalversionsofthesealgorithmstosolvethenonconvexcompositeoptimizationproblems.AlltheresultsarepresentedinFigures3.OnecanseethatourProx-SpiderBoost-Mstillsigniﬁcantlyoutperformsalltheotheralgorithmsinthesenonsmoothandnonconvexscenarios.Thisdemonstratesthatournoveldesignofthecoupledupdatefor{yk}kinthemomentumschemeisefﬁcientinthenonsmoothandnonconvexsetting.Also itturnsoutthatKatyushansandASVRGaresufferingfromaslowconvergence(theirconvergencesoccurataround40epochs).Togetherwiththeaboveexperimentsforsmoothproblems thisimpliesthattheirperformanceisnotstableandmaynotbegenerallysuitableforsolvingnonconvexproblems.0.02.55.07.510.012.515.017.520.0# of epochs104103102101100101Loss(f-f*)ProxSVRGProxASVRGProxKatyusha_nsProxRSAGProxSpiderBoostProx-SpiderBoost-M(a)Dataset:a9a02468101214# of epochs104103102101100101Loss(f-f*)ProxSVRGProxASVRGProxKatyusha_nsProxRSAGProxSpiderBoostProx-SpiderBoost-M(b)Dataset:w8a0.02.55.07.510.012.515.017.520.0# of epochs104103102101100101Loss(f-f*)ProxSVRGProxASVRGProxKatyusha_nsProxRSAGProxSpiderBoostProx-SpiderBoost-M(c)Dataset:a9a02468101214# of epochs104103102101100101102Loss(f-f*)ProxSVRGProxASVRGProxKatyusha_nsProxRSAGProxSpiderBoostProx-SpiderBoost-M(d)Dataset:w8aFigure3:(a)and(b):Logisticregressionwithan‘1nonsmoothregualarizer.(c)and(d):Robustlinearregressionwithan‘1nonsmoothregualarizer.6ConclusionInthispaper weproposedtheSpiderBoostalgorithm whichachievesthesamenear-optimalcom-plexityperformanceasSPIDER butallowsamuchlargerstepsizeandhencerunsfasterinpracticethanSPIDER.WethenextendtheproposedSpiderBoosttosolvecompositenonconvexoptimization andproposedamomentumschemetofurtheracceleratethealgorithm.Forallthesealgorithms wedevelopnewtechniquestocharacterizetheperformancebounds allofwhichachievethebeststate-of-the-art.WeanticipatethatSpiderBoosthasagreatpotentialtobeappliedtovariousotherlarge-scaleoptimizationproblems.8AcknowledgmentsTheworkofZ.Wang K.Ji andY.LiangwassupportedinpartbytheU.S.NationalScienceFoundationunderthegrantsCCF-1761506 CCF-1909291 andCCF-1900145.References[1]Z.Allen-Zhu.Katyusha:Theﬁrstdirectaccelerationofstochasticgradientmethods.JournalofMachineLearningResearch(JMLR) 18(1):8194–8244 Jan.2017.[2]Z.Allen-Zhu.KatyushaX:Simplemomentummethodforstochasticsum-of-nonconvexoptimization.InProc.InternationalConferenceonMachineLearning(ICML) volume80 pages179–185 10–15Jul2018.[3]Z.Allen-Zhu.Natasha2:Fasternon-convexoptimizationthansgd.InProc.AdvancesinNeuralInformationProcessingSystems(NeurIPS) pages2675–2686.2018.[4]Z.Allen-ZhuandE.Hazan.Variancereductionforfasternon-convexoptimization.InProcInternationalConferenceonMachineLearning(ICML) pages699–707 2016.[5]L.Bottou F.E.Curtis andJ.Nocedal.Optimizationmethodsforlarge-scalemachinelearning.SIAMReview 60(2):223–311 2018.[6]C.ChangandC.Lin.LIBSVM:Alibraryforsupportvectormachines.ACMTransactionsonIntelligentSystemsandTechnology 2(3):1–27 2011.[7]A.Defazio F.Bach andS.Lacoste-Julien.SAGA:Afastincrementalgradientmethodwithsupportfornon-stronglyconvexcompositeobjectives.InProc.AdvancesinNeuralInformationProcessingSystems(NeurIPS) pages1646–1654.2014.[8]C.Fang C.J.Li Z.Lin andT.Zhang.Near-optimalnon-convexoptimizationviastochasticpath-integrateddifferentialestimator.InProc.AdvancesinNeuralInformationProcessingSystems(NeurIPS).2018.[9]S.GhadimiandG.Lan.Stochasticﬁrst-andzeroth-ordermethodsfornonconvexstochasticprogramming.SIAMJournalonOptimization 23(4):2341–2368 2013.[10]S.GhadimiandG.Lan.Acceleratedgradientmethodsfornonconvexnonlinearandstochasticprogramming.MathematicalProgramming 156(1):59–99 Mar2016.[11]S.Ghadimi G.Lan andH.Zhang.Mini-batchstochasticapproximationmethodsfornonconvexstochasticcompositeoptimization.MathematicalProgramming 155(1):267–305 Jan2016.[12]F.Huang S.Chen andH.Huang.Fasterstochasticalternatingdirectionmethodofmultipliersfornonconvexoptimization.InProc.InternationalConferenceonInternationalConferenceonMachineLearning(ICML).2019.[13]F.Huang S.Gao J.Pei andH.Huang.Nonconvexzeroth-orderstochasticADMMmethodswithlowerfunctionquerycomplexity.arXiv:1907.13463 2019.[14]K.Ji Z.Wang Y.Zhou andY.Liang.Fasterstochasticalgorithmsviahistory-gradientaidedbatchsizeadaptation 2019.[15]K.Ji Z.Wang Y.Zhou andY.Liang.Improvedzeroth-ordervariancereducedalgorithmsandanalysisfornonconvexoptimization.InProceedingsofthe36thInternationalConferenceonMachineLearning volume97ofProceedingsofMachineLearningResearch pages3100–3109 09–15Jun2019.[16]R.JohnsonandT.Zhang.Acceleratingstochasticgradientdescentusingpredictivevariancereduction.InProc.AdvancesinNeuralInformationProcessingSystems(NeurIPS) pages315–323.2013.9[17]H.Karimi J.Nutini andM.Schmidt.Linearconvergenceofgradientandproximal-gradientmethodsunderthepolyak-łojasiewiczcondition.InP.Frasconi N.Landwehr G.Manco andJ.Vreeken editors Proc.MachineLearningandKnowledgeDiscoveryinDatabases pages795–811 2016.[18]L.Lei C.Ju J.Chen andM.I.Jordan.Non-convexﬁnite-sumoptimizationviaSCSGmethods.InProc.AdvancesinNeuralInformationProcessingSystems(NeurIPS) pages2348–2358.2017.[19]Q.Li Y.Zhou Y.Liang andP.K.Varshney.Convergenceanalysisofproximalgradientwithmomentumfornonconvexoptimization.InProc.InternationalConferenceonMachineLearning(ICML) volume70 pages2111–2119 2017.[20]X.Li S.Ling T.Strohmer andK.Wei.Rapid robust andreliableblinddeconvolutionvianonconvexoptimization.AppliedandComputationalHarmonicAnalysis 2018.[21]Z.Li.SSRGD:Simplestochasticrecursivegradientdescentforescapingsaddlepoints.arXiv:1904.09265 Apr2019.[22]Z.LiandJ.Li.Asimpleproximalstochasticgradientmethodfornonsmoothnonconvexoptimization.InProc.AdvancesinNeuralInformationProcessingSystem(NeurIPS).2018.[23]Y.Nesterov.Introductorylecturesonconvexoptimization:Abasiccourse.SpringerPublishingCompany Incorporated 2014.[24]L.M.Nguyen J.Liu K.Scheinberg andM.Tak´aˇc.SARAH:Anovelmethodformachinelearningproblemsusingstochasticrecursivegradient.InProc.InternationalConferenceonMachineLearning(ICML) volume70 pages2613–2621 2017.[25]L.M.Nguyen J.Liu K.Scheinberg andM.Tak´aˇc.Stochasticrecursivegradientalgorithmfornonconvexoptimization.ArXiv:1705.07261 May2017.[26]L.M.Nguyen M.vanDijk D.T.Phan P.H.Nguyen T.-W.Weng andJ.R.Kalagnanam.Finite-sumsmoothoptimizationwithSARAH.arXiv:1901.07648 Jan2019.[27]A.Nitanda.Acceleratedstochasticgradientdescentforminimizingﬁnitesums.InProc.InternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS) volume51 pages195–203 May2016.[28]N.H.Pham L.M.Nguyen D.T.Phan andQ.Tran-Dinh.ProxSARAH:Anefﬁcientalgo-rithmicframeworkforstochasticcompositenonconvexoptimization.arXiv:1902.05679 Feb2019.[29]S.J.Reddi A.Hefny S.Sra B.Poczos andA.Smola.Stochasticvariancereductionfornonconvexoptimization.InProc.InternationalConferenceonMachineLearning(ICML) pages314–323 2016.[30]S.J.Reddi S.Sra B.Poczos andA.Smola.Proximalstochasticmethodsfornonsmoothnonconvexﬁnite-sumoptimization.InProc.AdvancesinNeuralInformationProcessingSystems(NeurIPS) pages1145–1153.2016.[31]N.L.Roux M.Schmidt andF.R.Bach.Astochasticgradientmethodwithanexponentialconvergencerateforﬁnitetrainingsets.InProc.AdvancesinNeuralInformationProcessingSystems(NeurIPS) pages2663–2671.2012.[32]F.Shang L.Jiao K.Zhou J.Cheng Y.Ren andY.Jin.ASVRG:AcceleratedproximalSVRG.InProc.AsianConferenceonMachineLearning volume95 pages815–830 2018.[33]Y.Xu R.Jin andT.Yang.Stochasticproximalgradientmethodsfornon-smoothnon-convexregularizedproblems.arXiv:1902.07672 Feb2019.[34]J.ZhangandL.Xiao.Astochasticcompositegradientmethodwithincrementalvariancereduction.arXiv:1906.10186 2019.10[35]J.Zhang H.Zhang andS.Sra.R-SPIDER:AfastRiemannianstochasticoptimizationalgorithmwithcurvatureindependentrate.arXiv:811.04194 2018.[36]D.Zhou P.Xu andQ.Gu.Stochasticnestedvariancereducedgradientdescentfornonconvexoptimization.InProc.AdvancesinNeuralInformationProcessingSystem(NeurIPS).2018.[37]P.Zhou X.Yuan andJ.Feng.Newinsightintohybridstochasticgradientdescent:Beyondwith-replacementsamplingandconvexity.InProc.AdvancesinNeuralInformationProcessingSystems(NeurIPS) pages1242–1251.2018.[38]P.Zhou X.Yuan andJ.Feng.Fasterﬁrst-ordermethodsforstochasticnon-convexoptimizationonRiemannianmanifolds.InProc.InternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS) 2019.[39]Y.ZhouandY.Liang.Characterizationofgradientdominanceandregularityconditionsforneuralnetworks.ArXiv:1710.06910v2 Oct2017.[40]Y.Zhou H.Zhang andY.Liang.Geometricalpropertiesandacceleratedgradientsolversofnon-convexphaseretrieval.Proc.AnnualAllertonConferenceonCommunication Control andComputing(Allerton) pages331–335 2016.11,Zhen Xu
Wen Dong
Sargur Srihari
Zhe Wang
Kaiyi Ji
Yi Zhou
Yingbin Liang
Vahid Tarokh