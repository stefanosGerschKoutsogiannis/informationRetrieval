2013,Rapid Distance-Based Outlier Detection via Sampling,Distance-based approaches to outlier detection are popular in data mining  as they do not require to model the underlying probability distribution  which is particularly challenging for high-dimensional data. We present an empirical comparison of various approaches to distance-based outlier detection across a large number of datasets. We report the surprising observation that a simple  sampling-based scheme outperforms state-of-the-art techniques in terms of both efficiency and effectiveness. To better understand this phenomenon  we provide a theoretical analysis why the sampling-based approach outperforms alternative methods based on k-nearest neighbor search.,Rapid Distance-Based Outlier Detection via Sampling

Mahito Sugiyama1 Karsten M. Borgwardt1;2

1Machine Learning and Computational Biology Research Group  MPIs T¨ubingen  Germany

2Zentrum f¨ur Bioinformatik  Eberhard Karls Universit¨at T¨ubingen  Germany
fmahito.sugiyama karsten.borgwardtg@tuebingen.mpg.de

Abstract

Distance-based approaches to outlier detection are popular in data mining  as they
do not require to model the underlying probability distribution  which is particu-
larly challenging for high-dimensional data. We present an empirical comparison
of various approaches to distance-based outlier detection across a large number
of datasets. We report the surprising observation that a simple  sampling-based
scheme outperforms state-of-the-art techniques in terms of both efﬁciency and ef-
fectiveness. To better understand this phenomenon  we provide a theoretical anal-
ysis why the sampling-based approach outperforms alternative methods based on
k-nearest neighbor search.

1 Introduction

An outlier  which is “an observation which deviates so much from other observations as to arouse
suspicions that it was generated by a different mechanism” (by Hawkins [10])  appears in many real-
life situations. Examples include intrusions in network trafﬁc  credit card frauds  defective products
in industry  and misdiagnosed patients. To discriminate such outliers from normal observations 
machine learning and data mining have deﬁned numerous outlier detection methods  for example 
traditional model-based approaches using statistical tests  convex full layers  or changes of vari-
ances and more recent distance-based approaches using k-nearest neighbors [18]  clusters [23]  or
densities [7] (for reviews  see [1  13]).
We focus in this paper on the latter  the distance-based approaches  which deﬁne outliers as objects
located far away from the remaining objects. More speciﬁcally  given a metric space (M; d)  each
object x 2 M receives a real-valued outlierness score q(x) via a function q : M ! R; q(x)
depends on the distances between x and the other objects in the dataset. Then the top-(cid:20) objects with
maximum outlierness scores are reported to be outliers. To date  this approach has been successfully
applied in various situations due to its ﬂexibility  that is  it does not require to determine or to ﬁt
an underlying probability distribution  which is often difﬁcult  in particular in high-dimensional
settings. For example  LOF (Local Outlier Factor) [7] has become one of the most popular outlier
detection methods  which measures the outlierness of each object by the difference of local densities
between the object and its neighbors.
The main challenge  however  is its scalability since this approach potentially requires computation
of all pairwise distances between objects in a dataset. This quadratic time complexity leads to
runtime problems on massive datasets that emerge across application domains. To avoid this high
computational cost  a number of techniques have already been proposed  which can be roughly
divided into two strategies: indexing of objects such as tree-based structures [5] or projection-based
structures [9] and partial computation of the pairwise distances to compute scores only for the top-(cid:20)
outliers  ﬁrst introduced by Bay and Schwabacher [4] and improved in [6  16]. Unfortunately  both
strategies are nowadays not sufﬁcient  as index structures are often not efﬁcient enough for high-
dimensional data [20] and the number of outliers often increases in direct proportion to the size of
the dataset  which signiﬁcantly deteriorates the efﬁciency of partial computation techniques.

1

Here we show that a surprisingly simple and rapid sampling-based outlier detection method out-
performs state-of-the-art distance-based methods in terms of both efﬁciency and effectiveness by
conducting an extensive empirical analysis. The proposed method behaves as follows: It takes a
small set of samples from a given set of objects  followed by measuring the outlierness of each ob-
ject by the distance from the object to its nearest neighbor in the sample set. Intuitively  the sample
set is employed as a telltale set  that is  it serves as an indicator of outlierness  as outliers should
be signiﬁcantly different from almost all objects by deﬁnition  including the objects in the sample
set. The time complexity is therefore linear in the number of objects  dimensions  and samples. In
addition  this method can be implemented in a one-pass manner with constant space complexity as
we only have to store the sample set  which is ideal for analyzing massive datasets.
This paper is organized as follows: In Section 2  we describe our experimental design for the em-
pirical comparison of different outlier detection strategies. In Section 3  we review a number of
state-of-the-art outlier detection methods which we used in our experiments  including our own
proposal. We present experimental results in Section 4 and theoretically analyze them in Section 5.

2 Experimental Design

We present an extensive empirical analysis of state-of-the-art approaches for distance-based outlier
detection and of our new approach  which are introduced in Section 3. They are evaluated in terms
of both scalability and effectiveness on synthetic and real-world datasets. All parameters are set
by referring the original literature or at popular values  which are also shown in Section 3. Note
that these parameters have to be chosen by heuristics in distance-based approaches  while they still
outperform other approaches such as statistical approaches [3].
Environment. We used Ubuntu version 12.04.3 with a single 2.6 GHz AMD Opteron CPU and 512
GB of memory. All C codes were compiled with gcc 4.6.3. All experiments were performed in the
R environment  version 3.0.1.
Evaluation criterion. To evaluate the effectiveness of each method  we used the area under the
precision-recall curve (AUPRC; equivalent to the average precision)  which is a typical criterion to
measure the success of outlier detection methods [1]. It takes values from 0 to 1 and 1 is the best
score  and quantiﬁes whether the algorithm is able to retrieve outliers correctly. These values were
calculated by the R ROCR package.
Datasets. We collected 14 real-world datasets from the UCI machine learning repository [2]  with
a wide range of sizes and dimensions  whose properties are summarized in Table 1. Most of them
have been intensively used in the outlier detection literature.
In particular  KDD1999 is one of
the most popular benchmark datasets in outlier detection  which was originally used for the KDD
Cup 1999. The task is to detect intrusions from network trafﬁc data  and as in [22]  objects whose
attribute logged in is positive were chosen as outliers. In every dataset  we ﬁrst excluded all
categorical attributes and missing values since some methods cannot handle categorical attributes.
For all datasets except for KDD1999  we assume that objects from the smallest class are outliers  as
they are originally designed for classiﬁcation rather than outlier detection. Three datasets Mfeat 
Isolet  and Optdigits were prepared exactly the same way as [17]  where only two similar
classes were used as inliers. All datasets were normalized beforehand  that is  in each dimension 
the feature values were divided by their standard deviation [1  Chapter 12.10].
In addition  we generated two synthetic datasets (Gaussian) using exactly the same procedure
as [14  17]  of which one is high-dimensional (1000 dimensions) and the other is large (10;000;000
objects). For each dataset  inliers (non-outliers) were generated from a Gaussian mixture model with
ﬁve equally weighted processes  resulting in ﬁve clusters. The mean and the variance of each cluster
was randomly set from the Gaussian distribution N (0; 1)  and 30 outliers were generated from a
uniform distribution in the range from the minimum to the maximum values of inliers.

3 Methods for Outlier Detection

In the following  we will introduce the state-of-the-art methods in distance-based outlier detection 
including our new sampling-based method. Every method is formalized as a scoring function q :
M ! R on a metric space (M; d)  which assigns a real-valued outlierness score to each object x

2

in a given set of objects X . We denote by n the number of objects in X . If X is multivariate  the
number of dimensions is denoted by m. The number of samples (sample size) is denoted by s.

3.1 The kth-nearest neighbor distance

′ 2 X j d(x; x
′

Knorr and Ng [11  12] were the ﬁrst to formalize a distance-based outlier detection scheme  in which
an object x 2 X is said to be a DB((cid:11); (cid:14))-outlier if jfx
) > (cid:14)gj (cid:21) (cid:11)n  where (cid:11) and
(cid:14) with (cid:11); (cid:14) 2 R and 0 (cid:20) (cid:11) (cid:20) 1 are parameters speciﬁed by the user. This means that at least a
fraction (cid:11) of all objects have a distance from x that is larger than (cid:14). This deﬁnition has mainly two
signiﬁcant drawbacks: the difﬁculty of determining the distance threshold (cid:14) in practice and the lack
of a ranking of outliers. To overcome these drawbacks  Ramaswamy et al. [18] proposed to measure
the outlierness by the kth-nearest neighbor (kth-NN) distance. The score qkthNN(x) of an object x
is deﬁned as
where dk(x;X ) is the distance between x and its kth-NN in X . Notice that if we set (cid:11) = (n(cid:0)k)=n 
the set of Knorr and Ng’s DB((cid:11); (cid:14))-outliers coincides with the set fx 2 X j qkthNN(x) (cid:21) (cid:14)g. We
employ qkthNN(x) as a baseline for distance-based methods in our comparison.
Since the na¨ıve computation of scores qkthNN(x) for all x requires quadratic computational cost  a
number of studies investigated speed-up techniques [4  6  16]. We used Bhaduri’s algorithm (called
iORCA) [6] and implemented it in C since it is the latest technique in this branch of research.
It has a parameter k to specify the kth-NN and an additional parameter (cid:20) to retrieve the top-(cid:20)
objects with the largest outlierness scores. We set k = 5  which is a default setting used in the
literature [4  6  15  16]  and set (cid:20) to be twice the number of outliers for each dataset. Note that in
practice we usually do not know the exact number of outliers and have to set (cid:20) large enough.

qkthNN(x) := dk(x;X );

3.2

Iterative sampling

Wu and Jermaine [21] proposed a sampling-based approach to efﬁciently approximate the kth-NN
distance score qkthNN. For each object x 2 X   deﬁne

qkthSp(x) := dk(x; Sx(X ));

where Sx(X ) is a subset of X   which is randomly and iteratively sampled for each object x. In
addition  they introduced a random variable N = jO \ O′j with two sets of top-(cid:20) outliers O and
O′ with respect to qkthNN and qkthSp  and analyzed its expectation E(N ) and the variance Var(N ).
The time complexity is (cid:2)(nms). We implemented this method in C and set k = 5 and the sample
size s = 20 unless stated otherwise.

3.3 One-time sampling (our proposal)

Here we present a new sampling-based method. We randomly and independently sample a subset
S(X ) (cid:26) X only once and deﬁne

qSp(x) := min

x′2S(X )

′

d(x; x

)

for each object x 2 X . Although this deﬁnition is closely related to Wu and Jermaine’s method
qkthSp in the case of k = 1  our method performs sampling only once while their method performs
sampling for each object. We empirically show that this leads to signiﬁcant differences in accuracy
in outlier detection (see Section 4). We also theoretically analyze this phenomenon to get a better
understanding of its cause (see Section 5). The time complexity is (cid:2)(nms) and the space complexity
is (cid:2)(ms) using the number of samples s  as this score can be obtained in a one-pass manner. We
implemented this method in C. We set s = 20 for the comparison with other methods.

3.4

Isolation forest

Liu et al. [15] proposed a random forest-like method  called isolation forest. It uses random recursive
partitions of objects  which are assumed to be m-dimensional vectors  and hence is also based on
the concept of proximity. From a given set X   we construct an iTree in the following manner. First
a sample set S(X ) (cid:26) X is chosen. Then this sample set is partitioned into two non-empty subsets

3

S(X )L and S(X )R such that S(X )L = f x 2 S(X ) j xq < v g and S(X )R = S(X )nS(X )L  where
v and q are randomly chosen. This process is recursively applied to each subset until it becomes a
singleton  resulting in a proper binary tree such that the number of nodes is 2s (cid:0) 1. The outlierness
of an object x is measured by the path length h(x) on the tree  and the score is normalized and
averaged on t iTrees. Finally  the outlierness score qtree(x) is deﬁned as

qtree(x) := 2

(cid:0)h(x)=c(s);

where h(x) is the average of h(x) on t iTrees and c(s) is deﬁned as c(s) := 2H(s(cid:0)1)(cid:0)2(s(cid:0)1)=n 
where H denotes the harmonic number. The overall average and worst case time complexities are
O((s + n)t log s) and O((s + n)ts). We used the ofﬁcial R IsolationForest package1  whose
core process is implemented in C. We set t = 100 and s = 256  which is the same setting as in [15].

3.5 Local outlier factor (LOF)

While LOF [7] is often referred to as not distance-based but density-based  we still include this
method as it is also based on pairwise distances and is known to be a prominent outlier detection
method. Let N k(x) be the set of k-nearest neighbors of x. The local reachability density of x
is deﬁned as (cid:26)(x) := jN k(x)j (
(cid:0)1. Then the local outlier
factor (LOF) qLOF(x) is deﬁned as the ratio of the local reachability density of x and the average
of the local reachability densities of its k-nearest neighbors  that is 

∑
x′2N k(x) maxf dk(x
′

;X ); d(x; x
)

′

)g)

(jN k(x)j(cid:0)1

∑

qLOF(x) :=

′
x′2N k(x) (cid:26)(x

)

(cid:26)(x)

(cid:0)1:

The time complexity is O(n2m)  which is known to be the main disadvantage of this method. We
implemented this method in C and used the commonly used setting k = 10.

3.6 Angle-based outlier factor (ABOF)

′
Kriegel et al. [14] proposed to use angles instead of distances to measure outlierness. Let c(x; x
)
′(cid:0) x)
be the similarity between vectors x and x
′ with respect to the the coordinate origin
should be correlated with the angle of two vectors y and y
x. The insight of Kriegel et al. is that if x is an outlier  the variance of angles between pairs of the
remaining objects becomes small. Formally  for an object x 2 X deﬁne
′ (cid:0) x):

′  for example  the cosine similarity. Then c(y(cid:0) x; y

qABOF(x) := Vary;y′2X c(y (cid:0) x; y

Note that the smaller qABOF(x)  the more likely is x to be an outlier  which is in contrast to the
other methods. This method was originally introduced to overcome the “curse of dimensionality”
in high-dimensional data. However  recently Zimek et al. [24] showed that distance-based methods
such as LOF also work if attributes carry relevant information for outliers. We include several high-
dimensional datasets in experiments and check whether distance-based methods work effectively.
Although this method is attractive as it is parameter-free  the computational cost is cubic in n. Thus
we use its near-linear approximation algorithm proposed by Pham and Pagh [17]. Their algorithm 
′(cid:0)
called FastVOA  estimates the ﬁrst and the second moments of the variance Vary;y′2X c(y(cid:0)x; y
x) independently using two techniques: random projections and AMS sketches. The latter is a
randomized technique to estimate the second frequency moment of a data stream. The resulting time
complexity is O(tn(m+log n+c1c2))  where t is the number of hyperplanes for random projections
and c1  c2 are the number of repetitions for AMS sketches. We implemented this algorithm in C. We
set t = log n  c1 = 1600  and c2 = 10 as they are shown to be empirically sufﬁcient in [17].

3.7 One-class SVM

The One-class SVM  introduced by Sch¨olkopf et al. [19]  classiﬁes objects into inliers and outliers
by introducing a hyperplane between them. This classiﬁcation can be turned into a ranking of
outlierness by considering the signed distance to the separating hyperplane. That is  the further
an object is located in the outlier half space  the more likely it is to be a true outlier. Let X =
fx1; : : : ; xng. Formally  the score of a vector x with a feature map (cid:8) is deﬁned as

qSVM(x) := (cid:26) (cid:0) (w (cid:1) (cid:8)(x));

(1)

1http://sourceforge.net/projects/iforest/

4

Table 1: Summary of datasets. Gaussian is syn-
thetic (marked by *) and the other datasets are
collected from the UCI repository (n = number
of objects  m = number of dimensions).

Ionosphere
Arrhythmia
Wdbc
Mfeat
Isolet
Pima
Gaussian*
Optdigits
Spambase
Statlog
Skin
Pamap2
Covtype
Kdd1999
Record
Gaussian*

n

351
452
569
600
960
768
1000
1688
4601
6435
245057
373161
286048
4898431
5734488
10000000

# of outliers
126
207
212
200
240
268
30
554
1813
626
50859
125953
2747
703067
20887
30

m
34
274
30
649
617
8
1000
64
57
36
3
51
10
6
7
20

Figure 1: Average of area under the precision-
recall curves (AUPRCs) over all datasets with re-
spect to changes in number of samples s for qSp
(one-time sampling; our proposal) and qkthSp (it-
erative sampling by Wu and Jermaine [21]). Note
that the x-axis has logarithmic scale.

n∑

i=1

∥w∥2 +

1
2

1
(cid:23)n

where the weight vector w and the offset (cid:26) are optimized by the following quadratic program:

min

(cid:24)i (cid:0) (cid:26) subject to (w (cid:1) (cid:8)(xi)) (cid:21) (cid:26) (cid:0) (cid:24)i; (cid:24)i (cid:21) 0

n

w2F; (cid:24)2Rn; (cid:26)2R

∑
with a regularization parameter (cid:23). The term w (cid:1) (cid:8)(x) in equation (1) can be replaced with
i=1 (cid:11)ik(xi; x) using a kernel function k  where (cid:11) = ((cid:11)1; : : : ; (cid:11)n) is used in the dual problem.
We tried ten different values of (cid:23) from 0 to 1 and picked up the one maximizing the margin between
negative and positive scores. We used a Gaussian RBF kernel and set its parameter (cid:27) by the popular
heuristics [8]. The R kernlab package was used  whose core process is implemented in C.

4 Experimental Results

4.1 Sensitivity in sampling size and sampling scheme

We ﬁrst analyze the parameter sensitivity of our method qSp with respect to changes in the sample
size s. In addition  for each sample size we compare our qSp (one-time sampling) to Wu and Jer-
maine’s qkthSp (iterative sampling). We set k = 1 in qkthSp  hence the only difference between them
was the sampling scheme. Each method was applied to each dataset listed in Table 1 and the average
of AUPRCs (area under the precision-recall curves) in 10 trials were obtained  and these were again
averaged over all datasets. These scores with varying sample sizes are plotted in Figure 1.
Our method shows robust performance over all sample sizes from 5 to 1000 and the average AUPRC
varies by less than 2%. Interestingly  the score is maximized at a rather small sample size (s = 20)
and monotonically (slightly) decreases with increasing sample size. Moreover  for every sample
size  the one-time sampling qSp signiﬁcantly outperforms the iterative sampling qkthSp (Wilcoxon
signed-rank test  (cid:11) = 0:05). We checked that this behavior is independent from dataset size.

4.2 Scalability and effectiveness

Next we evaluate the scalability and effectiveness of the approaches introduced in Section 3 by
systematically applying them to every dataset. Results of running time and AUPRCs are shown in
Table 2 and Table 3  respectively. As we can see  our method qSp is the fastest among all methods;
it can score more than ﬁve million objects within a few seconds. Although the time complexity of
Wu and Jermaine’s qkthSp is the same as qSp  our method is empirically much faster  especially in
large datasets. The different costs of two processes  sampling once and performing nearest neighbor

5

Number of samplesAUPRC (average)5105020010000.400.450.500.55qSpqkthSpqABOF

(cid:0)1

(cid:0)1

(cid:0)1

(cid:0)2
(cid:0)1
(cid:0)2

(cid:0)2
(cid:0)1
(cid:0)2

(cid:0)4
(cid:0)2
(cid:0)3
(cid:0)2
(cid:0)2
(cid:0)4
(cid:0)1
(cid:0)2
(cid:0)2
(cid:0)2
(cid:0)2

Table 2: Running time (in seconds). Averages in 10 trials are shown in four probabilistic methods
qkthSp  qSp  qtree  and qABOF. “—” means that computation did not completed within 2 months.
qSVM
6.80(cid:2)10
3.88(cid:2)10
9.20(cid:2)10
1.90
3.60
9.60(cid:2)10
7.77
1.14
8.77
1.39(cid:2)101
9.44(cid:2)103
8.37(cid:2)104
1.69(cid:2)104

qkthNN
2.00(cid:2)10
Ionosphere
Arrhythmia 2.56(cid:2)10
7.20(cid:2)10
Wdbc
1.04
Mfeat
Isolet
4.27
4.00(cid:2)10
Pima
4.18
Gaussian
1.04
Optdigits
9.51
Spambase
Statlog
6.99
6.82(cid:2)103
Skin
9.05(cid:2)104
Pamap2
6.87(cid:2)102
Covtype
2.68(cid:2)106
Kdd1999
3.62(cid:2)106
Record
3.37(cid:2)103
Gaussian
Table 3: Area under the precision-recall curve (AUPRC). Averages(cid:6)SEMs in 10 trials are shown
in four probabilistic methods. Best scores are denoted in Bold. Note that the root mean square
deviation (RMSD) rewards methods that are always close to the best result on each dataset.

qkthSp
(cid:0)3
9.60(cid:2)10
(cid:0)2
2.72(cid:2)10
(cid:0)2
1.60(cid:2)10
(cid:0)2
6.00(cid:2)10
(cid:0)2
8.68(cid:2)10
(cid:0)2
2.04(cid:2)10
(cid:0)1
2.13(cid:2)10
(cid:0)2
7.48(cid:2)10
(cid:0)1
7.26(cid:2)10
(cid:0)1
2.03(cid:2)10
2.12(cid:2)101
3.27(cid:2)101
2.16(cid:2)101
4.40(cid:2)102
9.58(cid:2)102
1.73(cid:2)103

qtree
6.25(cid:2)10
2.72
7.32(cid:2)10
8.69
9.71
(cid:0)1
3.14(cid:2)10
2.10(cid:2)101
(cid:0)1
8.65(cid:2)10
1.02
9.35(cid:2)10
3.04
1.20(cid:2)101
6.15
4.78(cid:2)101
8.84(cid:2)101
3.26(cid:2)102

qSp
8.00(cid:2)10
1.52(cid:2)10
2.00(cid:2)10
4.80(cid:2)10
8.37(cid:2)10
4.00(cid:2)10
1.54(cid:2)10
1.48(cid:2)10
3.68(cid:2)10
2.80(cid:2)10
9.72(cid:2)10
2.73
2.83(cid:2)10
3.46
4.11
2.13(cid:2)101

qLOF
2.40(cid:2)10
2.04(cid:2)10
6.80(cid:2)10
1.02
4.61
(cid:0)2
9.20(cid:2)10
2.61(cid:2)101
1.46
1.14(cid:2)101
1.68(cid:2)101
1.38(cid:2)104
1.37(cid:2)105
3.67(cid:2)104

4.72
6.19
7.86
8.26
1.38(cid:2)101
1.07(cid:2)101
1.46(cid:2)101
2.41(cid:2)101
7.75(cid:2)101
1.07(cid:2)102
7.33(cid:2)103
1.71(cid:2)104
1.13(cid:2)104
2.40(cid:2)105
1.07(cid:2)106
1.47(cid:2)106

(cid:0)2
(cid:0)1
(cid:0)2

(cid:0)2

(cid:0)2

(cid:0)1

—
—
—

—
—
—

Ionosphere
Arrhythmia
Wdbc
Mfeat
Isolet
Pima
Gaussian
Optdigits
Spambase
Statlog
Skin
Pamap2
Covtype
Kdd1999
Record
Gaussian
Average
Avg.Rank
RMSD

qkthNN
0.931
0.701
0.607
0.217
0.380
0.519
1.000
0.204
0.395
0.057
0.195
0.249
0.016
0.768
0.002
1.000
0.453
3.750
0.259

qkthSp

0.762(cid:6)0.007
0.674(cid:6)0.008
0.226(cid:6)0.001
0.293(cid:6)0.002
0.175(cid:6)0.001
0.608(cid:6)0.007
1.000(cid:6)0.000
0.319(cid:6)0.001
0.418(cid:6)0.001
0.058(cid:6)0.000
0.146(cid:6)0.000
0.328(cid:6)0.000
0.058(cid:6)0.001
0.081(cid:6)0.000
0.411(cid:6)0.000
0.999(cid:6)0.000

0.410
3.875
0.274

qSp

0.899(cid:6)0.032
0.711(cid:6)0.005
0.667(cid:6)0.036
0.245(cid:6)0.031
0.535(cid:6)0.138
0.512(cid:6)0.010
1.000(cid:6)0.000
0.233(cid:6)0.021
0.422(cid:6)0.011
0.082(cid:6)0.008
0.353(cid:6)0.058
0.268(cid:6)0.009
0.075(cid:6)0.034
0.611(cid:6)0.098
0.933(cid:6)0.013
1.000(cid:6)0.000

0.534
2.188
0.068

qtree

qABOF

qLOF
0.864
0.673
0.428
0.369
0.274
0.406
0.904
0.361
0.354
0.093
0.130
0.338
0.010

0.740(cid:6)0.022
0.697(cid:6)0.005
0.490(cid:6)0.014
0.211(cid:6)0.003
0.520(cid:6)0.034
0.461(cid:6)0.008
0.994(cid:6)0.005
0.255(cid:6)0.006
0.398(cid:6)0.002
0.054(cid:6)0.000
0.258(cid:6)0.006
0.231(cid:6)0.002
0.087(cid:6)0.005

qSVM
0.871(cid:6)0.002
0.794
0.681(cid:6)0.004
0.707
0.595(cid:6)0.018
0.556
0.270(cid:6)0.009
0.257
0.328(cid:6)0.011
0.439
0.441(cid:6)0.003
0.461
0.934(cid:6)0.036
1.000
0.295(cid:6)0.010
0.266
0.419(cid:6)0.011
0.399
0.060(cid:6)0.002
0.056
0.242(cid:6)0.003
0.213
0.252(cid:6)0.001
0.235
0.017(cid:6)0.001
0.095
0.389(cid:6)0.007 — 0.539(cid:6)0.020 —
0.976(cid:6)0.004 — 0.658(cid:6)0.106 —
0.890(cid:6)0.022 — 0.893(cid:6)0.003 —
0.421
4.000
0.094

0.468
4.563
0.140

0.479
3.875
0.133

0.400
4.538
0.152

search versus re-sampling per object and performing kth-NN search  causes this difference. The
baseline qkthNN shows acceptable runtimes for large data only if the number of outliers is small.
In terms of effectiveness  qSp shows the best performance on seven out of sixteen datasets including
the high-dimensional datasets  resulting in the best average AUPRC score  which is signiﬁcantly
higher than every single method except for qLOF (Wilcoxon signed-rank test  (cid:11) = 0:05). The
method qSp also shows the best performance in terms of the average rank and RMSDs (root mean
square deviations) to the best result on each dataset. Moreover  qSp is inferior to the baseline qkthNN
only on three datasets. It is interesting that qtree  which also uses one-time sampling like our method 
shows better performance than exhaustive methods on average. In contrast  qkthSp with iterative
sampling is worst in terms of RMSD among all methods.
Based on these observations we can conclude that (1) small sample sizes lead to the maximum
average precision for qSp; (2) one-time sampling leads to better results than iterative sampling; (3)
one-time sampling leads to better results than exhaustive methods and is also much faster.

6

5 Theoretical Analysis

′ 2 X j d(x; x
′

To understand why our new one-time sampling method qSp shows better performance than the other
methods  we present a theoretical analysis to get answers to the following four questions: (1) What
is the probability that qSp will correctly detect outliers? (2) Why do small sample sizes lead to better
results in qSp? (3) Why is qSp superior to qkthSp? (4) Why is qSp superior to qkthNN? Here we use
the notion of Knorr and Ng’s DB((cid:11); (cid:14))-outliers [11  12] and denote the set of DB((cid:11); (cid:14))-outliers by
X ((cid:11); (cid:14))  that is  an object x 2 X ((cid:11); (cid:14)) if jf x
) > (cid:14) gj (cid:21) (cid:11)n holds. We also deﬁne
X ((cid:11); (cid:14)) = X n X ((cid:11); (cid:14)) and  for simplicity  we call an element in X ((cid:11); (cid:14)) an outlier and that in
X ((cid:11); (cid:14)) an inlier unless otherwise noted. Our method requires as input only the sample size s in
practice  whereas the parameters (cid:14) and (cid:11) are used only in our theoretical analysis. In the following 
we always assume that s ≪ n  hence the sampling process is treated as with replacement.
∪
Probabilistic analysis of qSp. First we introduce a partition of inliers into subsets (clusters) using
the threshold (cid:14). A (cid:14)-partition P (cid:14) of X ((cid:11); (cid:14)) is deﬁned as a set of non-empty disjoint subsets of
X ((cid:11); (cid:14)) such that each element (cluster) C 2 P (cid:14) satisﬁes maxx;x′2C d(x; x
C =
′
C2P (cid:14)
X ((cid:11); (cid:14)). Then if we focus on a cluster C 2 P (cid:14)  the probability of discriminating an outlier from
inliers contained in C can be bounded from below. Remember that s is the number of samples.
Theorem 1 For an outlier x 2 X ((cid:11); (cid:14)) and a cluster C 2 P (cid:14)  we have

) (cid:21) (cid:11)s(1 (cid:0) (cid:12)s) with (cid:12) = (n (cid:0) jCj)=n:

′ 2 C; qSp(x) > qSp(x

(8x

) < (cid:14) and

(2)

Pr

)

′

′

′ 2 C; qSp(x

) < (cid:14) holds for all x

) < (cid:14)) = 1 (cid:0) (cid:12)s. Inequality (2) therefore follows.

Proof. We have the probability Pr(qSp(x) > (cid:14)) = (cid:11)s from the deﬁnition of outliers. Moreover 
if at least one object is sampled from the cluster C  qSp(x
′ 2 C. Thus
′
Pr(8x
For instance  if we assume that 5% of our data are outliers and ﬁx (cid:11) to be 0.95  we have (maximum
(cid:14); mean of (cid:12)) = (10:51; 0:50)  (44:25; 2:23(cid:2) 10
(cid:0)3)  (10:93; 0:67)  (37:10; 0:75)  and (36:37; 0:80)
on our ﬁrst ﬁve datasets from Table 1 to achieve this 5% rate of outliers. These (cid:12) were obtained by
greedily searching each cluster in P (cid:14) under (cid:11) = 0:95 and the respective maximum (cid:14).
Next we consider the task of correctly discriminating an outlier from all inliers. This can be achieved
if for each cluster C 2 P (cid:14) at least one object x 2 C is chosen in the sampling process. Thus the
lower bound can be directly derived using the multinomial distribution as follows.
∑
Theorem 2 Let P (cid:14) = fC1; : : : ;Clg with l clusters and pi = jCij = n for each i 2 f1; : : : ; lg. For
every outlier x 2 X ((cid:11); (cid:14)) and the sample size s (cid:21) l  we have
∏

where f is the probability mass function of the multinomial distribution deﬁned as

) (cid:21) (cid:11)s
∏

′ 2 X ((cid:11); (cid:14)); qSp(x) > qSp(x

f (s1; : : : ; sl; s; p1; : : : ; pl);

(8x

∑

8i;si⪈0

Pr

)

′

f (s1; : : : ; sl; s; p1; : : : ; pl) := (s!=

l
i=1 si!)

l

i=1 psi

i

with

l
i=1 si = s:

Furthermore  let I((cid:11); (cid:14)) be a subset of X ((cid:11); (cid:14)) such that minx′2I((cid:11);(cid:14)) d(x; x
′
) > (cid:14) for every
outlier x 2 X ((cid:11); (cid:14)) and assume that P (cid:14) is a (cid:14)-partition of I((cid:11); (cid:14)) instead of all inliers X ((cid:11); (cid:14)). If
S(X ) (cid:18) I((cid:11); (cid:14)) and at least one object is sampled from each cluster C 2 P (cid:14)  qSp(x) > qSp(x
′
holds for all pairs of an outlier x and an inlier x
∑
Theorem 3 Let P (cid:14) = fC1; : : : ;Clg be a (cid:14)-partition of I((cid:11); (cid:14)) and (cid:13) = jI((cid:11); (cid:14))j = n  and assume
that pi = jCij =jI((cid:11); (cid:14))j for each i 2 f1; : : : ; lg. For every s (cid:21) l 

′.

)

(8x 2 X ((cid:11); (cid:14));8x

Pr

) (cid:21) (cid:13)s

′

)

f (s1; : : : ; sl; s; p1; : : : ; pl):

8i;si⪈0

From the fact that this theorem holds for any (cid:14)-partition  we automatically have the maximum lower
bound over all possible (cid:14)-partitions.

′ 2 X ((cid:11); (cid:14)); qSp(x) > qSp(x
∑

Corollary 1 Let φ(s) =

8i;si⪈0 f (s1; : : : ; sl; s; p1; : : : ; pl) given in Theorem 3. We have

′ 2 X ((cid:11); (cid:14)); qSp(x) > qSp(x
′

)

φ(s):

(3)

(8x 2 X ((cid:11); (cid:14));8x

Pr

) (cid:21) (cid:13)s maxP (cid:14)

7

Let B((cid:13); (cid:14)) be the right-hand side of Inequality (3) above. This bound is maximized for equally
sized clusters when l is ﬁxed and it shows high probability for large (cid:13). For example if (cid:13) = 0:99 
we have (l; optimal s; B((cid:13); (cid:14))) = (2; 7; 0:918)  (3; 12; 0:866)  and (4; 17; 0:818). It is notable that
the bound B((cid:13); (cid:14)) is independent of the actual number of outliers and inliers  which is a desirable
property when analyzing large datasets. Although it is dependent on the number of clusters l  the
best (minimum) l which maximizes B((cid:13); (cid:14)) with the simplest clustering is implicitly chosen in qSp.
Theoretical support for small sample sizes. Let g(s) = (cid:11)s(1 (cid:0) (cid:12)s)  which is the right-hand side
of Inequality (2). From the differentiation dg=ds  we can see that this function is maximized at

)

(

s = log(cid:12)

log (cid:11)=(log (cid:11) + log (cid:12))

;

with the natural assumption 0 < (cid:12) < (cid:11) < 1 and this optimal sample size s is small for large (cid:11)
and small (cid:12)  for example  s = 6 for ((cid:11); (cid:12)) = (0:99; 0:5) and s = 24 for ((cid:11); (cid:12)) = (0:999; 0:8).
Moreover  as we already saw above the bound B((cid:13); (cid:14)) is also maximized at such small sample
sizes for large (cid:13). This could be the reason why qSp works well for small sample sizes  as these are
common values for (cid:11)  (cid:12)  and (cid:13) in outlier detection.
Comparison with qkthSp. Deﬁne Z(x; x
pling method qkthSp. Since we repeat sampling for each object in qkthSp  probability Z(x; x
each x

′ 2 X ((cid:11); (cid:14)) is independent with respect to a ﬁxed x 2 X ((cid:11); (cid:14)). We therefore have

)) for the iterative sam-
) for

) := Pr(qkthSp(x) > qkthSp(x

′

′

′

(8x 2 X ((cid:11); (cid:14));8x

Pr

) (cid:20) min

x2X ((cid:11);(cid:14))

∏

′

Z(x; x

):

x′2X ((cid:11);(cid:14))

′ 2 X ((cid:11); (cid:14)); qkthSp(x) > qkthSp(x

′

)

′
) is typically close to 1 in outlier detection  the overall probability rapidly de-
Although Z(x; x
creases if n is large. Thus the performance suffers on large datasets.
In contrast  our one-time
sampling qSp does not have independence  resulting in our results (Theorem 1  2  3  and Corol-
lary 1) instead of this upper bound  which often lead to higher probability. This fact might be the
reason why qkthSp empirically performs signiﬁcantly worse than qSp and shows the worst RMSD.
Comparison with qkthNN. Finally  let us consider the situation in which there exists the set of “true”
outliers O (cid:26) X given by an oracle. Let (cid:3) = fk 2 N j qkthNN(x) > qkthNN(x
) for all x 2 O and
′
′ 2 X n Og  the set of ks with which we can detect all outliers  and assume that (cid:3) ̸= ∅. Then

x

(8x 2 O;8x

Pr

′ 2 X n O; qSp(x) > qSp(x
′

)

max

k2(cid:3); (cid:14)2∆(k)

B((cid:13); (cid:14))

with ∆(k) = f(cid:14) 2 R j X ((cid:11); (cid:14)) = Og if we set (cid:11) = (n (cid:0) k)=n. Notice that (cid:13) is determined from (cid:11)
(i.e. k) and (cid:14). Thus both k and (cid:14) are implicitly optimized in qSp. In contrast  in qkthNN the number
k is speciﬁed by the user. For example  if (cid:3) is small  it is hardly possible to choose k 2 (cid:3) without
any prior knowledge  resulting in overlooking some outliers  while qSp always has the possibility
to detect them without knowing (cid:3) if I((cid:11); (cid:14)) is non-empty for some (cid:11). This difference in detection
ability could be a reason why qSp signiﬁcantly outperforms qkthNN on average.

) (cid:21)

6 Conclusion

In this study  we have performed an extensive set of experiments to compare current distance-based
outlier detection methods. We have observed that a surprisingly simple sampling-based approach 
which we have newly proposed here  outperforms other state-of-the-art distance-based methods.
Since the approach reached its best performance with small sample sizes  it achieves dramatic speed-
ups compared to exhaustive methods and is faster than all state-of-the-art methods for distance-based
outlier detection. We have also presented a theoretical analysis to understand why such a simple
strategy works well and outperforms the popular approach based on kth-NN distances.
To summarize  our contribution is not only to overcome the scalability issue of the distance-based
approach to outlier detection using the sampling strategy but also  to the best of our knowledge 
to give the ﬁrst thorough experimental comparison of a broad range of recently proposed distance-
based outlier detection methods. We are optimistic that these results will contribute to the further
improvement of outlier detection techniques.
Acknowledgments. M.S. is funded by the Alexander von Humboldt Foundation. The research of
Professor Dr. Karsten Borgwardt was supported by the Alfried Krupp Prize for Young University
Teachers of the Alfried Krupp von Bohlen und Halbach-Stiftung.

8

References
[1] Aggarwal  C. C. Outlier Analysis. Springer  2013.
[2] Bache  K. and Lichman  M. UCI machine learning repository  2013.
[3] Bakar  Z. A.  Mohemad  R.  Ahmad  A.  and Deris  M. M. A comparative study for outlier detection tech-
niques in data mining. In Proceedings of IEEE International Conference on Cybernetics and Intelligent
Systems  1–6  2006.

[4] Bay  S. D. and Schwabacher  M. Mining distance-based outliers in near linear time with randomiza-
tion and a simple pruning rule. In Proceedings of the 9th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining  29–38  2003.

[5] Berchtold  S.  Keim  D. A.  and Kriegel  H.-P. The X-tree: An index structure for high-dimensional data.

In Proceedings of the 22th International Conference on Very Large Data Bases  28–39  1996.

[6] Bhaduri  K.  Matthews  B. L.  and Giannella  C. R. Algorithms for speeding up distance-based outlier
In Proceedings of the 17th ACM SIGKDD Conference on Knowledge Discovery and Data

detection.
Mining  859–867  2011.

[7] Breunig  M. M.  Kriegel  H.-P.  Ng  R. T.  and Sander  J. LOF: Identifying density-based local outliers.
In Proceedings of the ACM SIGMOD International Conference on Management of Data  93–104  2000.
[8] Caputo  B.  Sim  K.  Furesjo  F.  and Smola  A. Appearance-based object recognition using SVMs:
Which kernel should I use? In Proceedings of NIPS Workshop on Statistical Methods for Computational
Experiments in Visual Processing and Computer Vision  2002.

[9] de Vries  T.  Chawla  S.  and Houle  M. E. Density-preserving projections for large-scale local anomaly

detection. Knowledge and Information Systems  32(1):25–52  2012.

[10] Hawkins  D. Identiﬁcation of Outliers. Chapman and Hall  1980.
[11] Knorr  E. M. and Ng  R. T. Algorithms for mining distance-based outliers in large datasets. In Proceedings

of the 24rd International Conference on Very Large Data Bases  392–403  1998.

[12] Knorr  E. M.  Ng  R. T.  and Tucakov  V. Distance-based outliers: algorithms and applications. The VLDB

Journal  8(3):237–253  2000.

[13] Kriegel  H.-P.  Kr¨oger  P.  and Zimak  A. Outlier detection techniques. Tutorial at 16th ACM SIGKDD

Conference on Knowledge Discovery and Data Mining  2010.

[14] Kriegel  H.-P.  Schubert  M.  and Zimek  A. Angle-based outlier detection in high-dimensional data.
In Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining  444–452  2008.

[15] Liu  F. T.  Ting  K. M.  and Zhou  Z. H.

Isolation-based anomaly detection. ACM Transactions on

Knowledge Discovery from Data  6(1):3:1–3:39  2012.

[16] Orair  G. H.  Teixeira  C. H. C.  Wang  Y.  Meira Jr.  W.  and Parthasarathy  S. Distance-based outlier

detection: consolidation and renewed bearing. PVLDB  3(2):1469–1480  2010.

[17] Pham  N. and Pagh  R. A near-linear time approximation algorithm for angle-based outlier detection in
high-dimensional data. In Proceedings of the 18th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining  877–885  2012.

[18] Ramaswamy  S.  Rastogi  R.  and Shim  K. Efﬁcient algorithms for mining outliers from large data sets.
In Proceedings of the ACM SIGMOD International Conference on Management of Data  427–438  2000.
[19] Sch¨olkopf  B.  Platt  J. C.  Shawe-Taylor  J.  Smola  A. J.  and Williamson  R. C. Estimating the support

of a high-dimensional distribution. Neural computation  13(7):1443–1471  2001.

[20] Weber  R.  Schek  H.-J.  and Blott  S. A quantitative analysis and performance study for similarity-search
methods in high-dimensional spaces. In Proceedings of the International Conference on Very Large Data
Bases  194–205  1998.

[21] Wu  M. and Jermaine  C. Outlier detection by sampling with accuracy guarantees. In Proceedings of
the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  767–772 
2006.

[22] Yamanishi  K.  Takeuchi  J.  Williams  G.  and Milne  P. On-line unsupervised outlier detection using
ﬁnite mixtures with discounting learning algorithms. Data Mining and Knowledge Discovery  8(3):275–
300  2004.

[23] Yu  D.  Sheikholeslami  G.  and Zhang  A. FindOut: Finding outliers in very large datasets. Knowledge

and Information Systems  4(4):387–412  2002.

[24] Zimek  A.  Schubert  E.  and Kriegel  H.-P. A survey on unsupervised outlier detection in high-

dimensional numerical data. Statistical Analysis and Data Mining  5(5):363—387  2012.

9

,Mahito Sugiyama
Karsten Borgwardt
Yunzhe Tao
Qi Sun
Qiang Du
Wei Liu