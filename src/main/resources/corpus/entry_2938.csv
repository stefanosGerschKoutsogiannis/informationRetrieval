2013,Learning Kernels Using Local Rademacher Complexity,We use the notion of local Rademacher complexity to design new algorithms for learning kernels. Our algorithms thereby benefit from the sharper learning bounds based on that notion which  under certain general conditions  guarantee a faster convergence rate.  We devise two new learning kernel algorithms: one based on a convex optimization problem for which we give an efficient solution using existing learning kernel techniques  and another one that can be formulated as a DC-programming problem for which we describe a solution in detail. We also report the results of experiments with both algorithms in both binary and multi-class classification tasks.,Learning Kernels Using

Local Rademacher Complexity

Corinna Cortes
Google Research
76 Ninth Avenue

New York  NY 10011

corinna@google.com

Marius Kloft⇤
Courant Institute &

Sloan-Kettering Institute

251 Mercer Street

New York  NY 10012

mkloft@cims.nyu.edu

Mehryar Mohri
Courant Institute &
Google Research
251 Mercer Street

New York  NY 10012

mohri@cims.nyu.edu

Abstract

We use the notion of local Rademacher complexity to design new algorithms for
learning kernels. Our algorithms thereby beneﬁt from the sharper learning bounds
based on that notion which  under certain general conditions  guarantee a faster
convergence rate. We devise two new learning kernel algorithms: one based on
a convex optimization problem for which we give an efﬁcient solution using ex-
isting learning kernel techniques  and another one that can be formulated as a
DC-programming problem for which we describe a solution in detail. We also re-
port the results of experiments with both algorithms in both binary and multi-class
classiﬁcation tasks.

Introduction

1
Kernel-based algorithms are widely used in machine learning and have been shown to often provide
very effective solutions. For such algorithms  the features are provided intrinsically via the choice of
a positive-semi-deﬁnite symmetric kernel function  which can be interpreted as a similarity measure
in a high-dimensional Hilbert space. In the standard setting of these algorithms  the choice of the
kernel is left to the user. That choice is critical since a poor choice  as with a sub-optimal choice of
features  can make learning very challenging. In the last decade or so  a number of algorithms and
theoretical results have been given for a wider setting known as that of learning kernels or multiple
kernel learning (MKL) (e.g.  [1  2  3  4  5  6]). That setting  instead of demanding from the user to
take the risk of specifying a particular kernel function  only requires from him to provide a family
of kernels. Both tasks of selecting the kernel out of that family of kernels and choosing a hypothesis
based on that kernel are then left to the learning algorithm.
One of the most useful data-dependent complexity measures used in the theoretical analysis and
design of learning kernel algorithms is the notion of Rademacher complexity (e.g.  [7  8]). Tight
learning bounds based on this notion were given in [2]  improving earlier results of [4  9  10].
These generalization bounds provide a strong theoretical foundation for a family of learning kernel
algorithms based on a non-negative linear combination of base kernels. Most of these algorithms 
whether for binary classiﬁcation or multi-class classiﬁcation  are based on controlling the trace of
the combined kernel matrix.
This paper seeks to use a ﬁner notion of complexity for the design of algorithms for learning ker-
nels: the notion of local Rademacher complexity [11  12]. One shortcoming of the general notion
of Rademacher complexity is that it does not take into consideration the fact that  typically  the
hypotheses selected by a learning algorithm have a better performance than in the worst case and
belong to a more favorable sub-family of the set of all hypotheses. The notion of local Rademacher
complexity is precisely based on this idea by considering Rademacher averages of smaller subsets
of the hypothesis set. It leads to sharper learning bounds which  under certain general conditions 
guarantee a faster convergence rate.

⇤Alternative address: Memorial Sloan-Kettering Cancer Center  415 E 68th street  New York  NY 10065 

USA. Email: kloft@cbio.mskcc.org.

1

We show how the notion of local Rademacher complexity can be used to guide the design of new
algorithms for learning kernels. For kernel-based hypotheses  the local Rademacher complexity can
be both upper- and lower-bounded in terms of the tail sum of the eigenvalues of the kernel matrix
[13]. This motivates the introduction of two natural families of hypotheses based on non-negative
combinations of base kernels with kernels constrained by a tail sum of the eigenvalues. We study
and compare both families of hypotheses and derive learning kernel algorithms based on both. For
the ﬁrst family of hypotheses  the algorithm is based on a convex optimization problem. We show
how that problem can be solved using optimization solutions for existing learning kernel algorithms.
For the second hypothesis set  we show that the problem can be formulated as a DC-programming
(difference of convex functions programming) problem and describe in detail our solution. We report
empirical results for both algorithms in both binary and multi-class classiﬁcation tasks.
The paper is organized as follows. In Section 2  we present some background on the notion of local
Rademacher complexity by summarizing the main results relevant to our theoretical analysis and the
design of our algorithms. Section 3 describes and analyzes two new kernel learning algorithms  as
just discussed. In Section 4  we give strong theoretical guarantees in support of both algorithms. In
Section 5  we report the results of preliminary experiments  in a series of both binary classiﬁcation
and multi-class classiﬁcation tasks.

2 Background on local Rademacher complexity
In this section  we present an introduction to local Rademacher complexities and related properties.

2.1 Core ideas and deﬁnitions

We consider the standard set-up of supervised learning where the learner receives a samplez1 =
(x1  y1)  . . .   zn = (xn  yn) of size n  1 drawn i.i.d. from a probability distribution P over
Z = X⇥Y . Let F be a set of functions mapping from X to Y  and let l : Y⇥Y! [0  1] be a loss
function. The learning problem is that of selecting a function f 2F with small risk or expected loss
E[l(f (x)  y)]. Let G := l(F ·) denote the loss class  then  this is equivalent to ﬁnding a function
g 2G with small average E[g]. For convenience  in what follows  we assume that the inﬁmum
of E[g] over G is reached and denote by g⇤ 2 argming2G E[g] the most accurate predictor in G.
When the inﬁmum is not reached  in the following results  E[g⇤] can be equivalently replaced by
inf g2G E[g].
Deﬁnition 1. Let 1  . . .   n be an i.i.d. family of Rademacher variables taking values 1 and
+1 with equal probability independent of the sample (z1  . . .   zn). Then  the global Rademacher
complexity of G is deﬁned as

Rn(G) := E sup

g2G

1
n

ig(zi).

nXi=1



.

n

Generalization bounds based on the notion of Rademacher complexity are standard [7]. In particular 
with probability at least 1  :

for the empirical risk minimization (ERM) hypothesisbgn  for any > 0  the following bound holds
(1)
Rn(G) is in the order of O(1/pn) for various classes used in practice  including when F is a
In such cases  the bound (1)
kernel class with bounded trace and when the loss l is Lipschitz.
converges at rate O(1/pn). For some classes G  we may  however  obtain fast rates of up to O(1/n).
The following presentation is based on [12]. Using Talagrand’s inequality  one can show that with
probability at least 1   

g2GE[g] bE[g]  4Rn(G) +s 2 log 2

E[bgn]  E[g⇤]  2 sup

E[bgn]  E[g⇤]  8Rn(G) +⌃( G)s 8 log 2

(2)
Here  ⌃2(G) := supg2G E[g2] is a bound on the variance of the functions in G. The key idea to
obtain fast rates is to choose a much smaller class G?
n ✓G with as small a variance as possible 
n. Since such a small class can also have a substantially smaller
Rademacher complexity Rn(G?
But how can we ﬁnd a small class G?
background on how to construct such a class in the supplementary material section 1. It turns out

while requiring thatbgn still lies in G?

n that is just large enough to containbgn? We give some further

n)  the bound (2) can be sharper than (1).

3 log 2




+

n

.

n

2

Figure 1: Illustration of the bound (3). The volume of the gray shaded area amounts to the term

large  and the center ﬁgure the case corresponding to the appropriate value of ✓.

✓r +Pj>✓ j occurring in (3). The left- and right-most ﬁgures show the cases of ✓ too small or too
that the order of convergence of E[bgn]  E[g⇤] is determined by the order of the ﬁxed point of the
local Rademacher complexity  deﬁned below.
Deﬁnition 2. For any r > 0  the local Rademacher complexity of G is deﬁned as
If the local Rademacher complexity is known  it can be used to comparebgn with g⇤  as E[bgn]E[g⇤]

can be bounded in terms of the ﬁxed point of the Rademacher complexity of F  besides constants and
O(1/n) terms. But  while the global Rademacher complexity is generally of the order of O(1/pn)
at best  its local counterpart can converge at orders up to O(1/n). We give an example of such a
class—particularly relevant for this paper—below.

Rn(G; r) := Rng 2G : E[g2]  r .

2.2 Kernel classes
The local Rademacher complexity for kernel classes can be accurately described and shown to admit
a simple expression in terms of the eigenvalues of the kernel [13] (cf. also Theorem 6.5 in [11]).
Theorem 3. Let k be a Mercer kernel with corresponding feature map k and reproducing kernel

Hilbert space Hk. Let k(x  ˜x) = P1j=1 j'j(x)>'j(˜x) be its eigenvalue decomposition  where
(i)1i=1 is the sequence of eigenvalues arranged in descending order. Let F := {fw = (x 7!
hw  k(x)i) : kwkHk  1}. Then  for every r > 0 

min(r  j).

(3)

1Xj=1

Moreover  there is an absolute constant c such that  if 1  1

n  then for every r  1
n 

n

min

E[R(F; r)]  s 2
✓0⇣✓r +Xj>✓
1Xj=1

c
pn

j⌘ = vuut 2

n

min(r  j)  E[R(F; r)].

We summarize the proof of this result in the supplementary material section 2. In view of (3)  the
local Rademacher complexity for kernel classes is determined by the tail sum of the eigenvalues.
A core idea of the proof is to optimize over the “cut-off point” ✓ of the tail sum of the eigenvalues
in the bound. Solving for the optimal ✓  gives a bound in terms of truncated eigenvalues  which is
illustrated in Figure 1.
Consider  for instance  the special case where r = 1. We can then recover the familiar upper bound
the case of Gaussian kernels [14]  then

on the Rademacher complexity: Rn(F) pTr(k)/n. But  whenPj>✓ j = O(exp(✓))  as in
O⇣ min
✓0✓r + exp(✓)⌘ = O(r log(1/r)).
Therefore  we have R(F; r) = O(p r
by Theorem 8 (shown in the supplemental material)  we have E[bgn]  E[g⇤] = O( log(n)

n log(1/r))  which has the ﬁxed point r⇤ = O( log(n)

3 Algorithms
In this section  we will use the properties of the local Rademacher complexity just discussed to
devise a novel family of algorithms for learning kernels.

yields a much stronger learning guarantee.

n ). Thus 
n )  which

3

Here  ✓ is a free parameter controlling the tail sum. The trace is a linear function and thus the
constraint (4) deﬁnes a half-space  therefore a convex set  in the space of kernels. The function

k 7!Pj>✓ j(k)  however  is concave since it can be expressed as the difference of the trace and
the sum of the ✓ largest eigenvalues  which is a convex function.
Nevertheless  the following upper bound holds  denoting ˜µm := µm/kµk1 

MXm=1

µmXj>✓

j(km) =

MXm=1

˜µmXj>✓

j(kµk1 km)  Xj>✓

where the equality holds by linearity and the inequality by the concavity just discussed. This leads
us to consider alternatively the following class

˜µm kµk1 km

(5)

◆ 

}

=kµ

{z

j✓ MXm=1
|
j(km)  1.

PM

the following hypothesis class:

3.1 Motivation and analysis
Most learning kernel algorithms are based on a family of hypotheses based on a kernel kµ =
m=1 µmkm that is a non-negative linear combination of M base kernels. This is described by

H := fw kµ =x 7! hw  kµ(x)i : kwkHkµ  ⇤  µ ⌫ 0 .

It is known that the Rademacher complexity of H can be upper-bounded in terms of the trace of
the combined kernel. Thus  most existing algorithms for learning kernels [1  4  6] add the following
constraint to restrict H:
(4)
As we saw in the previous section  however  the tail sum of the eigenvalues of the kernel  rather than
its trace  determines the local Rademacher complexity. Since the local Rademacher complexity can
lead to tighter generalization bounds than the global Rademacher complexity  this motivates us to
consider the following hypothesis class for learning kernels:

Tr(kµ)  1.

H1 :=fw kµ 2 H :Xj>✓

j(kµ)  1 .

H2 := ⇢fw kµ 2 H :

MXm=1

µmXj>✓

The class H2 is convex because it is the restriction of the convex class H via a linear inequality
constraint. H2 is thus more convenient to work with. The following proposition helps us compare
these two families.
Proposition 4. The following statements hold for the sets H1 and H2:

1. (a) H1 ✓ H2
2. (b) If ✓ = 0  then H1 = H2.
3. (c) Let ✓> 0. There exist kernels k1  . . .   kM and a probability measure P such that

H1 ( H2.

The proposition shows that  in general  the convex class H2 can be larger than H1. The following
result shows that in general an even stronger result holds.
Proposition 5. Let ✓> 0. There exist kernels k1  . . .   kM and a probability measure P such that
conv(H1) ( H2.
The proofs of these propositions are given in the supplemental material. These results show that in
general H2 could be a richer class than H1 and even its convex hull. This would suggest working
with H1 to further limit the risk of overﬁtting  however  as already pointed out  H2 is more conve-
nient since it is a convex class. Thus  in the next section  we will consider both hypothesis sets and
introduce two distinct learning kernel algorithms  each based on one of these families.
3.2 Convex optimization algorithm
The simpler algorithm performs regularized empirical risk minimization based on the convex
class H2. Note that by a renormalization of the kernels k1  . . .   kM  according to ˜km :=

(Pj>✓ j(km))1km and ˜kµ =PM

H2 = ˜H2 := ⇢fw ˜kµ

= (x 7! hw  ˜kµ

m=1 µm˜km  we can simply rewrite H2 as

(x)i)  kwkH˜kµ  ⇤  µ ⌫ 0  kµk1  1 

(6)

4

which is the commonly studied hypothesis class in multiple kernel learning. Of course  in practice 
we replace the empirical version of the kernel k by the kernel matrix K = (k(xi  xj))n
i j=1  and
consider 1  . . .   n as the eigenvalues of the kernel matrix and not of the kernel itself. Hence  we
can easily exploit existing software solutions:

1. For all m = 1  . . .   M  computePj>✓ j(Km);
(Pj>✓ j(Km))1Km;

over ˜H2.

2. For all m = 1  . . .   M  normalize the kernel matrices according to ˜Km :=

3. Use any of the many existing (`1-norm) MKL solvers to compute the minimizer of ERM

Note that the tail sum can be computed in O(n2✓) for each kernel because it is sufﬁcient to compute

the ✓ largest eigenvalues and the trace:Pj>✓ j(Km) = Tr(Km) P✓

j=1 j(Km).

3.3 DC-programming
In the more challenging case  we perform penalized ERM over the class H1  that is  we aim to solve

min
w

1
2 kwk2

HKµ

+ C

l(yifw Kµ(xi))

nXi=1

s.t. Xj>✓

j(Kµ)  1 .

(7)

This is a convex optimization problem with an additional concave constraintPj>✓ j(Kµ)  1.
This constraint is not differentiable  but it admits a subdifferential at any point µ0 2 RM. Denote the
subdifferential of the function µ 7! j(Kµ) by @µ0j(Kµ0) := {v 2 RM : j(Kµ) j(Kµ0) 
hv  µµ0i 8µ 2 RM}. Moreover  let u1  . . .   un be the eigenvectors of Kµ0 sorted in descending
order. Deﬁning vm :=Pj>✓ u>j Kmuj  one can verify—using the sub-differentiability of the max
operator—that v = (v1  . . .   vM )> is contained in the subdifferential @µ0Pj>✓ j(Kµ0). Thus 
we can linearly approximate the constraint  for any µ0 2 RM  via

Xj>✓

j(Kµ) ⇡ hv  µ  µ0i = Xj>✓

u>j Kµµ0uj.

+ C

min
w µ⌫0

We can thus tackle problem (7) using the DCA algorithm [15]  which in this context reduces to
alternating between the linearization of the concave constraint and solving the resulting convex
problem  that is  for any µ0 2 RM 
1
2 kwk2
s.t. Xj>✓

nXi=1
u>j K(µµ0)uj  1.

Note that µ0 changes in every iteration and so may also do the eigenvectors u1  . . .   un of Kµ0 
until the DCA algorithm converges. The DCA algorithm is proven to converge to a local minimum 
even when the concave function is not differentiable [15]. The algorithm is also close to the CCCP
algorithm of Yuille and Rangarajan [16]  modulo the use of subgradients instead of the gradients.
To solve (8)  we alternate the optimization with respect to µ and w. Note that  for ﬁxed w  we can
compute the optimal µ analytically. Up to normalization the following holds:

l(fw Kµ(xi)  yi)

HKµ

(8)

8m = 1  . . .   M : µm =vuut kwk2

Pj>✓ u>j Kmuj

Hkµ

.

(9)

A very similar optimality expression has been used in the context the group Lasso and `p-norm
multiple kernel learning by [3]. In turn  we need to compute a w that is optimal in (8)  for ﬁxed
µ. We perform this computation in the dual; e.g.  for the hinge loss l(t  y) = max(0  1  ty)  this
reduces to a standard support vector machine (SVM) [17  18] problem 
(↵  y)>Kµ(↵  y) 
(10)

1>↵ 

1
2

max
0↵C

where  denotes the Hadamard product.

5

initialize µm := 1/M for all m = 1  . . .   M

i j=1 and labels y1  . . .   yn 2{ 1  1}  optimization precision "

Algorithm 1 (DC ALGORITHM FOR LEARNING KERNELS BASED ON THE LOCAL RADEMACHER
COMPLEXITY).
1: input: kernel matrix K = (k(xi  xj))n
2:
3: while optimality conditions are not satisﬁed within tolerance ✏ do
4:
5:
6:
7:
8:
9: end while
10: SVM training: solve (10) with respect to ↵
11: output: ✏-accurate ↵ and kernel weights µ

SVM training: compute a new ↵ by solving the SVM problem (10)
eigenvalue computation: compute eigenvalues u1  . . .   un of Kµ
store µ0 := µ
µ update: compute a new µ according to (9) using (11)

normalize µ such thatPj>✓ ujK(µµ0)uj = 1

For the computation of (9)  we can recover the term kwk2
in (10) via

Hkµ

corresponding to the ↵ that is optimal

kwk2

HKµ

= µ2

m(↵  y)>Km(↵  y) 

(11)
which follows from the KKT conditions with respect to (10). In summary  the proposed algorithm 
which is shown in Algorithm Table 1  alternatingly optimizes ↵ and µ  where prior to each µ step
the linear approximation is updated by computing an eigenvalue decomposition of Kµ.
In the discussion that precedes  for the sake of simplicity of the presentation  we restricted ourselves
to the case of an `1-regularization  that is we showed how the standard trace-regularization can be
replaced by a regularization based on the tail-sum of the eigenvalues. It should be clear that in the
same way we can replace the familiar `p-regularization used in learning kernel algorithms [3] for
p  1 with `p-regularization in terms of the tail eigenvalues. In fact  as in the `1 case  in the `p case 
our convex optimization algorithm can be solved using existing MKL optimization solutions. The
results we report in Section 5 will in fact also include those obtained by using the `2 version of our
algorithm.

4 Learning guarantees

An advantage of the algorithms presented is that they beneﬁt from strong theoretical guarantees.
Since H1 ✓ H2  it is sufﬁcient to present these guarantees for H2—any bound that holds for H2 a
fortiori holds for H1. To present the result  recall from Section 3.2 that  by a re-normalization of the
kernels  we may equivalently express H2 by ˜H2  as deﬁned in (6). Thus  the algorithms presented
enjoy the following bound on the local Rademacher complexity  which was shown in [19] (Theorem
5). Similar results were shown in [20  21].
Theorem 6 (Local Rademacher complexity). Assume that the kernels are uniformly bounded (for

bounded as follows:

all m  k˜kmk1 < 1) and uncorrelated. Then  the local Rademacher complexity of eH2 can be

m=1 ... M 1Xj=1

max

min⇣r  e2⇤2 log2(M )j(˜km)⌘! + O✓ 1
n◆ .

R(eH2; r) vuut 16e

n

Note that we show the result under the assumption of uncorrelated kernels only for simplicity of
presentation. More generally  a similar result holds for correlated kernels and arbitrary p  1
(cf. [19]  Theorem 5). Subsequently  we can derive the following bound on the excess risk from
Theorem 6 using a result of [11] (presented as Theorem 8 in the supplemental material 1).
Theorem 7. Let l(t  y) = 1
2 (t  y)2 be the squared loss. Assume that for all m  there exists d such
that j(˜km)  dj for some > 1 (this is a common assumption and  for example  met for ﬁnite
rank kernels and Gaussian kernels [14]). Then  under the assumptions of the previous theorem  for
any > 0  with probability at least 1   over the draw of the sample  the excess loss of the class
˜H2 can be bounded as follows:
+1 + O✓ 1
n◆ .

1  4d⇤2 log2(M ) 1

E[bgn]  E[g⇤]  186r 3  

1
+1 e(M/e)

1
+1 n 

1+ 2

6

0.92

0.9

0.88

C
U
A

0.86

0.84

 
100

250

n

t
h
g
i
e
w

 
l
e
n
r
e
k

3

2

1

0

 

 

l1
l2
unif
conv
dc
1 000

 

n=100
l1
l2
conv
dc

TSS Promo 1st Ex Angle Energ
85.2 80.9 85.8 55.6 72.1

)
θ
(
m
u
s

l
i

a
t
(
g
o

l

2

1

0

−1
0

 

 

n=100

TSS
Promo
1st Ex
Angle
Energ
50
θ

100

Figure 2: Results of the TSS experiment. LEFT: average AUCs of the compared algorithms. CEN-
TER: for each kernel  the average kernel weight and single-kernel AUC. RIGHT: for each kernel

Km  the tail sumPj>✓ j as a function of the eigenvalue cut-off point ✓.
We observe that the above bound converges in O⇣ log2(M )
1+⌘. This can be almost
as slow as O log(M )/pn (when  ⇡ 1) and almost as fast as OM/n (when letting  ! 1).

The latter is the case  for instance  for ﬁnite-rank or Gaussian kernels.

1

1+ M

1
+1 n 

5 Experiments

In this section  we report the results of experiments with the two algorithms we introduced  which
we will denote by conv and dc in short. We will compare our algorithms with the classical `1-norm
MKL (denoted by l1) and the more recent `2-norm MKL [3] (denoted by l2). We also measure
the performance of the uniform kernel combination  denoted by unif  which has frequently been
shown to achieve competitive performances [22]. In all experiments  we use the hinge loss as a loss
function  including a bias term.

5.1 Transcription Start Site Detection
Our ﬁrst experiment aims at detecting transcription start sites (TSS) of RNA Polymerase II binding
genes in genomic DNA sequences. We experiment on the TSS data set  which we downloaded
from http://mldata.org/. This data set  which is a subset of the data used in the larger study
of [23]  comes with 5 kernels  capturing various complementary aspects: a weighted-degree kernel
representing the TSS signal TSS  two spectrum kernels around the promoter region (Promo) and
the 1st exon (1st Ex)  respectively  and two linear kernels based on twisting angles (Angle) and
stacking energies (Energ)  respectively. The SVM based on the uniform combination of these 5
kernels was found to have the highest overall performance among 19 promoter prediction programs
[24]  it therefore constitutes a strong baseline. To be consistent with previous studies [24  3  23]  we
will use the area under the ROC curve (AUC) as an evaluation criterion.
All kernel matrices Km were normalized such that Tr(Km) = n for all m  prior to the experiment.
SVM computations were performed using the SHOGUN toolbox [25]. For both conv and dc  we
experiment with `1- and `2-norms. We randomly drew an n-elemental training set and split the
remaining set into validation and test sets of equal size. The random partitioning was repeated 100
times. We selected the optimal model parameters ✓ 2{ 2i  i = 0  1  . . .   4} and C 2{ 10i  i =
2 1  0  1  2} on the validation set  based on their maximal mean AUC  and report mean AUCs on
the test set as well as standard deviations (the latter are within the interval [1.1  2.5] and are shown in
detail in the supplemental material 4). The experiment was carried out for all n 2{ 100  250  1000}.
Figure 2 (left) shows the mean AUCs on the test sets.
We observe that unif and l2 outperform l1  except when n = 100  in which case the three meth-
ods are on par. This is consistent with the result reported by [3]. For all sample sizes investigated 
conv and dc yield the highest AUCs.
We give a brief explanation for the outcome of the experiment. To further investigate  we compare
the average kernel weights µ output by the compared algorithms (for n = 100). They are shown
in Figure 2 (center)  where we report  below each kernel  also its performance in terms of its AUC
when training an SVM on that single kernel alone. We observe that l1 focuses on the TSS kernel
using the TSS signal  which has the second highest AUC among the kernels (85.2). However  l1
discards the 1st exon kernel  which also has a high predictive performance (AUC of 85.8). A similar
order of kernel importance is determined by l2  but which distributes the weights more broadly 

7

Table 1: The training split (sp) fraction  dataset size (n)  and multi-class accuracies shown with ±1
standard error. The performance results for MKL and conv correspond to the best values obtained
using either `1-norm or `2-norm regularization.

n

940
2732
541
1444
694

unif
91.1 ± 0.8
87.2 ± 1.6
90.5 ± 3.1
90.3 ± 1.8
57.2 ± 2.0

MKL

90.6 ± 0.9
87.7 ± 1.3
90.6 ± 3.4
90.7 ± 1.2
57.2 ± 2.0

conv
91.4 ± 0.7
87.6 ± 0.9
90.8 ± 2.8
91.2 ± 1.3
59.6 ± 2.4

✓

32
4
1
8
8

sp

plant
nonpl
psortPos
psortNeg
protein

0.5
0.5
0.8
0.5
0.5

while still mostly focusing on the TSS kernel. In contrast  conv and dc distribute their weight only
over the TSS  Promoter  and 1st Exon kernels  which are also the kernels that also have the highest
predictive accuracies. The considerably weaker kernels Angle and Energ are discarded.
But why are Angle and Energ discarded? This can be explained by means of Figure 2 (right) 
where we show the tail sum of each kernel as a function of the cut-off point ✓. We observe that
Angle and Energ have only moderately large ﬁrst and second eigenvalues  which is why they
hardly proﬁt when using conv or dc. The Promo and Exon kernels  however  which are discarded
by l1  have a large ﬁrst (and also second) eigenvalues  which is why they are promoted by conv or
dc. Indeed  the model selection determines the optimal cut-off  for both conv and dc  for ✓ = 1.

5.2 Multi-class Experiments
We next carried out a series of experiments with the conv algorithm in the multi-class classiﬁcation
setting  that repeatedly has demonstrated amenable to MKL learning [26  27]. As described in
Section 3.2 the conv problem can be solved by simply re-normalizing the kernels by the tail sum of
the eigenvalues and making use of any `p-norm MKL solver. For our experiments  we used the ufo
algorithm [26] from the DOGMA toolbox http://dogma.sourceforge.net/. For both conv
and ufo we experiment both with `1 and `2 regularization and report the best performance achieved
in each case.
We used the data sets evaluated in [27] (plant  nonpl  psortPos  and psortNeg)  which consist of
either 3 or 4 classes and use 69 biologically motivated sequence kernels.1 Furthermore  we also
considered the proteinFold data set of [28]  which consists of 27 classes and uses 12 biologically
motivated base kernels.2
The results are summarized in Table 1:
they represent mean accuracy values with one standard
deviation as computed over 10 random splits of the data into training and test folds. The fraction of
the data used for training  as well as the total number of examples  is also shown. The optimal value
for the parameter ✓ 2{ 2i  i = 0  1  . . .   8} was determined by cross-validation. For the parameters
↵ and C of the ufo algorithm we followed the methodology of [26]. For plant  psortPos  and
psortNeg  the results show that conv leads to a consistent improvement in a difﬁcult multi-class
setting  although we cannot attest to their signiﬁcance due to the insufﬁcient size of the data sets.
They also demonstrate a signiﬁcant performance improvement over l1 and unif in the proteinFold
data set  a more difﬁcult task where the classiﬁcation accuracies are below 60%.
6 Conclusion
We showed how the notion of local Rademacher complexity can be used to derive new algorithms for
learning kernels by using a regularization based on the tail sum of the eigenvalues of the kernels. We
introduced two natural hypothesis sets based on that regularization  discussed their relationships  and
showed how they can be used to design an algorithm based on a convex optimization and one based
on solving a DC-programming problem. Our algorithms beneﬁt from strong learning guarantees.
Our empirical results show that they can lead to performance improvement in some challenging
tasks. Finally  our analysis based on local Rademacher complexity could be used as the basis for the
design of new learning kernel algorithms.
Acknowledgments
We thank Gunnar R¨atsch for helpful discussions. This work was partly funded by the NSF award
IIS-1117591 and a postdoctoral fellowship funded by the German Research Foundation (DFG).

1Accessible from http://raetschlab.org//projects/protsubloc.
2Accessible from http://mkl.ucsd.edu/dataset/protein-fold-prediction.

8

References
[1] F. R. Bach  G. R. G. Lanckriet  and M. I. Jordan  “Multiple kernel learning  conic duality  and the SMO

algorithm ” in Proc. 21st ICML  ACM  2004.

[2] C. Cortes  M. Mohri  and A. Rostamizadeh  “Generalization bounds for learning kernels ” in Proceedings 

27th ICML  pp. 247–254  2010.

[3] M. Kloft  U. Brefeld  S. Sonnenburg  and A. Zien  “`p-norm multiple kernel learning ” Journal of Machine

Learning Research  vol. 12  pp. 953–997  Mar 2011.

[4] G. Lanckriet  N. Cristianini  L. E. Ghaoui  P. Bartlett  and M. I. Jordan  “Learning the kernel matrix with

semi-deﬁnite programming ” JMLR  vol. 5  pp. 27–72  2004.

[5] A. Rakotomamonjy  F. Bach  S. Canu  and Y. Grandvalet  “SimpleMKL ” J. Mach. Learn. Res.  vol. 9 

pp. 2491–2521  2008.

[6] S. Sonnenburg  G. R¨atsch  C. Sch¨afer  and B. Sch¨olkopf  “Large scale multiple kernel learning ” Journal

of Machine Learning Research  vol. 7  pp. 1531–1565  July 2006.

[7] P. Bartlett and S. Mendelson  “Rademacher and gaussian complexities: Risk bounds and structural re-

sults ” Journal of Machine Learning Research  vol. 3  pp. 463–482  Nov. 2002.

[8] V. Koltchinskii and D. Panchenko  “Empirical margin distributions and bounding the generalization error

of combined classiﬁers ” Annals of Statistics  vol. 30  pp. 1–50  2002.

[9] N. Srebro and S. Ben-David  “Learning bounds for support vector machines with learned kernels ” in

Proc. 19th COLT  pp. 169–183  2006.

[10] Y. Ying and C. Campbell  “Generalization bounds for learning the kernel problem ” in COLT  2009.
[11] P. L. Bartlett  O. Bousquet  and S. Mendelson  “Local Rademacher complexities ” Ann. Stat.  vol. 33 

no. 4  pp. 1497–1537  2005.

[12] V. Koltchinskii  “Local Rademacher complexities and oracle inequalities in risk minimization ” Annals of

Statistics  vol. 34  no. 6  pp. 2593–2656  2006.

[13] S. Mendelson  “On the performance of kernel classes ” J. Mach. Learn. Res.  vol. 4  pp. 759–771  Decem-

ber 2003.

[14] B. Sch¨olkopf and A. Smola  Learning with Kernels. Cambridge  MA: MIT Press  2002.
[15] P. D. Tao and L. T. H. An  “A DC optimization algorithm for solving the trust-region subproblem ” SIAM

Journal on Optimization  vol. 8  no. 2  pp. 476–505  1998.

[16] A. L. Yuille and A. Rangarajan  “The concave-convex procedure ” Neural Computation  vol. 15  pp. 915–

936  Apr. 2003.

[17] C. Cortes and V. Vapnik  “Support vector networks ” Machine Learning  vol. 20  pp. 273–297  1995.
[18] B. Boser  I. Guyon  and V. Vapnik  “A training algorithm for optimal margin classiﬁers ” in Proc. 5th

Annual ACM Workshop on Computational Learning Theory (D. Haussler  ed.)  pp. 144–152  1992.

[19] M. Kloft and G. Blanchard  “On the convergence rate of `p-norm multiple kernel learning ” Journal of

Machine Learning Research  vol. 13  pp. 2465–2502  Aug 2012.

[20] V. Koltchinskii and M. Yuan  “Sparsity in multiple kernel learning ” Ann. Stat.  vol. 38  no. 6  pp. 3660–

3695  2010.

[21] T. Suzuki  “Unifying framework for fast learning rate of non-sparse multiple kernel learning ” in Advances

in Neural Information Processing Systems 24  pp. 1575–1583  2011.

[22] P. Gehler and S. Nowozin  “On feature combination for multiclass object classiﬁcation ” in International

Conference on Computer Vision  pp. 221–228  2009.

[23] S. Sonnenburg  A. Zien  and G. R¨atsch  “Arts: Accurate recognition of transcription starts in human ”

Bioinformatics  vol. 22  no. 14  pp. e472–e480  2006.

[24] T. Abeel  Y. V. de Peer  and Y. Saeys  “Towards a gold standard for promoter prediction evaluation ”

Bioinformatics  2009.

[25] S. Sonnenburg  G. R¨atsch  S. Henschel  C. Widmer  J. Behr  A. Zien  F. de Bona  A. Binder  C. Gehl  and

V. Franc  “The SHOGUN Machine Learning Toolbox ” J. Mach. Learn. Res.  2010.

[26] F. Orabona and L. Jie  “Ultra-fast optimization algorithm for sparse multi kernel learning ” in Proceedings

of the 28th International Conference on Machine Learning  2011.

[27] A. Zien and C. S. Ong  “Multiclass multiple kernel learning ” in ICML 24  pp. 1191–1198  ACM  2007.
[28] T. Damoulas and M. A. Girolami  “Probabilistic multi-class multi-kernel learning: on protein fold recog-

nition and remote homology detection ” Bioinformatics  vol. 24  no. 10  pp. 1264–1270  2008.

[29] P. Bartlett and S. Mendelson  “Empirical minimization ” Probab. Theory Related Fields  vol. 135(3) 

pp. 311–334  2006.

[30] A. B. Tsybakov  “Optimal aggregation of classiﬁers in statistical learning ” Ann. Stat.  vol. 32  pp. 135–

166  2004.

9

,Corinna Cortes
Marius Kloft
Mehryar Mohri