2017,Conservative Contextual Linear Bandits,Safety is a desirable property that can immensely increase the applicability of learning algorithms in real-world decision-making problems. It is much easier for a company to deploy an algorithm that is safe  i.e.  guaranteed to perform at least as well as a baseline. In this paper  we study the issue of safety in contextual linear bandits that have application in many different fields including personalized ad recommendation in online marketing. We formulate a notion of safety for this class of algorithms. We develop a safe contextual linear bandit algorithm  called conservative linear UCB (CLUCB)  that simultaneously minimizes its regret and satisfies the safety constraint  i.e.  maintains its performance above a fixed percentage of the performance of a baseline strategy  uniformly over time. We prove an upper-bound on the regret of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound for the regret of the standard linear UCB algorithm that grows with the time horizon and 2) a constant term that accounts for the loss of being conservative in order to satisfy the safety constraint. We empirically show that our algorithm is safe and validate our theoretical analysis.,Conservative Contextual Linear Bandits

Abbas Kazerouni
Stanford University

Mohammad Ghavamzadeh

DeepMind

abbask@stanford.edu

ghavamza@google.com

Yasin Abbasi-Yadkori

Adobe Research

abbasiya@adobe.com

Benjamin Van Roy
Stanford University
bvr@stanford.edu

Abstract

Safety is a desirable property that can immensely increase the applicability of
learning algorithms in real-world decision-making problems. It is much easier
for a company to deploy an algorithm that is safe  i.e.  guaranteed to perform at
least as well as a baseline. In this paper  we study the issue of safety in contextual
linear bandits that have application in many different ﬁelds including personalized
recommendation. We formulate a notion of safety for this class of algorithms. We
develop a safe contextual linear bandit algorithm  called conservative linear UCB
(CLUCB)  that simultaneously minimizes its regret and satisﬁes the safety con-
straint  i.e.  maintains its performance above a ﬁxed percentage of the performance
of a baseline strategy  uniformly over time. We prove an upper-bound on the regret
of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound
for the regret of the standard linear UCB algorithm that grows with the time horizon
and 2) a constant term that accounts for the loss of being conservative in order to
satisfy the safety constraint. We empirically show that our algorithm is safe and
validate our theoretical analysis.

Introduction

1
Many problems in science and engineering can be formulated as decision-making problems under
uncertainty. Although many learning algorithms have been developed to ﬁnd a good policy/strategy
for these problems  most of them do not provide any guarantee for the performance of their resulting
policy during the initial exploratory phase. This is a major obstacle in using learning algorithms in
many different ﬁelds  such as online marketing  health sciences  ﬁnance  and robotics. Therefore 
developing learning algorithms with safety guarantees can immensely increase the applicability of
learning in solving decision problems. A policy generated by a learning algorithm is considered to be
safe  if it is guaranteed to perform at least as well as a baseline. The baseline can be either a baseline
value or the performance of a baseline strategy. It is important to note that since the policy is learned
from data  it is a random variable  and thus  the safety guarantees are in high probability.
Safety can be studied in both ofﬂine and online scenarios. In the ofﬂine case  the algorithm learns
the policy from a batch of data  usually generated by the current strategy or recent strategies of the
company  and the question is whether the learned policy will perform as well as the current strategy or
no worse than a baseline value  when it is deployed. This scenario has been recently studied heavily
in both model-based (e.g.  Petrik et al. [2016]) and model-free (e.g.  Bottou et al. 2013; Thomas et
al. 2015a b; Swaminathan and Joachims 2015a b) settings. In the model-based approach  we ﬁrst
use the batch of data and build a simulator that mimics the behavior of the dynamical system under
study (hospital’s ER  ﬁnancial market  robot)  and then use this simulator to generate data and learn
the policy. The main challenge here is to have guarantees on the performance of the learned policy 
given the error in the simulator. This line of research is closely related to the area of robust learning
and control. In the model-free approach  we learn the policy directly from the batch of data  without
building a simulator. This line of research is related to off-policy evaluation and control. While the

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

model-free approach is more suitable for problems in which we have access to a large batch of data 
such as in online marketing  the model-based approach works better in problems in which data is
harder to collect  but instead  we have good knowledge about the underlying dynamical system that
allows us to build an accurate simulator.
In the online scenario  the algorithm learns a policy while interacting with the real system. Although
(reasonable) online algorithms will eventually learn a good or an optimal policy  there is no guarantee
for their performance along the way (the performance of their intermediate policies)  especially at
the very beginning  when they perform a large amount of exploration. Thus  in order to guarantee
safety in online algorithms  it is important to control their exploration and make it more conservative.
Consider a manager that allows our learning algorithm runs together with her company’s current
strategy (baseline policy)  as long as it is safe  i.e.  the loss incurred by letting a portion of the trafﬁc
handled by our algorithm (instead of by the baseline policy) does not exceed a certain threshold.
Although we are conﬁdent that our algorithm will eventually perform at least as well as the baseline
strategy  it should be able to remain alive (not terminated by the manager) long enough for this to
happen. Therefore  we should make it more conservative (less exploratory) in a way not to violate the
manager’s safety constraint. This setting has been studied in the multi-armed bandit (MAB) [Wu et
al.  2016]. Wu et al. [2016] considered the baseline policy as a ﬁxed arm in MAB  formulated safety
using a constraint deﬁned based on the performance of the baseline policy (mean of the baseline arm) 
and modiﬁed the UCB algorithm [Auer et al.  2002] to satisfy this constraint.
In this paper  we study the notion of safety in contextual linear bandits  a setting that has application
in many different ﬁelds including personalized recommendation. We ﬁrst formulate safety in this
setting  as a constraint that must hold uniformly in time  in Section 2. Our goal is to design learning
algorithms that minimize regret under the constraint that at any given time  their expected sum of
rewards should be above a ﬁxed percentage of the expected sum of rewards of the baseline policy.
This ﬁxed percentage depends on the amount of risk that the manager is willing to take. In Section 3 
we propose an algorithm  called conservative linear UCB (CLUCB)  that satisﬁes the safety constraint.
At each round  CLUCB plays the action suggested by the standard linear UCB (LUCB) algorithm
(e.g.  Dani et al. 2008; Rusmevichientong and Tsitsiklis 2010; Abbasi-Yadkori et al. 2011; Chu et
al. 2011; Russo and Van Roy 2014)  only if it satisﬁes the safety constraint for the worst choice of
the parameter in the conﬁdence set  and plays the action suggested by the baseline policy  otherwise.
We prove an upper-bound for the regret of CLUCB  which can be decomposed into two terms. The
ﬁrst term is an upper-bound on the regret of LUCB that grows at the rate
T log(T ). The second
term is constant (does not grow with the horizon T ) and accounts for the loss of being conservative in
order to satisfy the safety constraint. This improves over the regret bound derived in Wu et al. [2016]
for the MAB setting  where the regret of being conservative grows with time. In Section 4  we show
how CLUCB can be extended to the case that the reward of the baseline policy is unknown without a
change in its rate of regret. Finally in Section 5  we report experimental results that show CLUCB
behaves as expected in practice and validate our theoretical analysis.

√

2 Problem Formulation
In this section  we ﬁrst review the standard linear bandit setting and then introduce the conservative
linear bandit formulation considered in this paper.

2.1 Linear Bandit
In the linear bandit setting  at any time t  the agent is given a set of (possibly) inﬁnitely many
actions/options At  where each action a ∈ At is associated with a feature vector φt
a ∈ Rd. At each
round t  the agent selects an action at ∈ At and observes a random reward yt generated as

yt = (cid:104)θ∗  φt

at

(cid:105) + ηt 

(1)

= E[yt]  and ηt is a random noise such that

where θ∗ ∈ Rd is the unknown reward parameter  (cid:104)θ∗  φt
at time t  i.e.  rt
at
Assumption 1 Each element ηt of the noise sequence {ηt}∞
i.e.  E[eζηt | a1:t  η1:t−1] ≤ exp(ζ 2σ2/2)  ∀ζ ∈ R.
The sub-Gaussian assumption implies that E[ηt | a1:t  η1:t−1] = 0 and Var[ηt | a1:t  η1:t−1] ≤ σ2.

at is the expected reward of action at

t=1 is conditionally σ-sub-Gaussian 

(cid:105) = rt

at

2

Note that the above formulation contains time-varying action sets and time-dependent feature vectors
for each action  and thus  includes the linear contextual bandit setting. In linear contextual bandit  if
we denote by xt  the state of the system at time t  the time-dependent feature vector φt
a for action a
will be equal to φ(xt  a)  the feature vector of state-action pair (xt  a).
We also make the following standard assumption on the unknown parameter θ∗ and feature vectors:
Assumption 2 There exist constants B  D ≥ 0 such that (cid:107)θ∗(cid:107)2 ≤ B  (cid:107)φt
a(cid:105) ∈
[0  1]  for all t and all a ∈ At.

a(cid:107)2 ≤ D  and (cid:104)θ∗  φt

We deﬁne B =(cid:8)θ ∈ Rd : (cid:107)θ(cid:107)2 ≤ B(cid:9) and F =(cid:8)φ ∈ Rd : (cid:107)φ(cid:107)2 ≤ D  (cid:104)θ∗  φ(cid:105) ∈ [0  1](cid:9) to be the
after T rounds  i.e. (cid:80)T

parameter space and feature space  respectively.
a(cid:105) at
Obviously  if the agent knows θ∗  she will choose the optimal action a∗
each round t. Since θ∗ is unknown  the agent’s goal is to maximize her cumulative expected rewards

(cid:105)  or equivalently  to minimize its (pseudo)-regret  i.e. 

t = arg maxa∈At(cid:104)θ∗  φt

t=1(cid:104)θ∗  φt

at

RT =

(cid:104)θ∗  φt
a∗

t

(cid:104)θ∗  φt

at

(cid:105) 

(2)

T(cid:88)

t=1

(cid:105) − T(cid:88)

t=1

which is the difference between the cumulative expected rewards of the optimal and agent’s strategies.

bt

= (cid:104)θ∗  φt

2.2 Conservative Linear Bandit
The conservative linear bandit setting is exactly the same as the linear bandit  except that there exists
a baseline policy πb (e.g.  the company’s current strategy) that at each round t  selects action bt ∈ At
(cid:105). We assume that the expected rewards of the actions
and incurs the expected reward rt
bt
taken by the baseline policy  rt
  are known (see Remark 1). We relax this assumption in Section 4
bt
and extend our proposed algorithm to the case that the reward function of the baseline policy is not
known in advance. Another difference between the conservative and standard linear bandit settings is
the performance constraint  which is deﬁned as follows:
Deﬁnition 1 (Performance Constraint) At each round t  the difference between the performances
of the baseline and the agent’s policies should remain below a pre-deﬁned fraction α ∈ (0  1) of the
baseline performance. This constraint may be written formally as
∀t ∈ {1  . . .   T} 

− t(cid:88)

or equivalently as

≥ (1−α)

t(cid:88)

t(cid:88)

t(cid:88)

≤ α

ri
bi

ri
ai

ri
bi

ri
ai

i=1

ri
bi

. (3)

i=1

t(cid:88)

i=1

i=1

i=1

The parameter α controls the level of conservatism of the agent. Small values show that only small
losses are tolerated and the agent should be overly conservative  whereas large values indicate that
the manager is willing to take risk and the agent can be more explorative. Here  given the value of
α  the agent should select her actions in a way to both minimize her regret (2) and to satisfy the
performance constraint (3). In the next section  we propose a linear bandit algorithm to achieve this
goal with high probability.
Remark 1. Since the baseline policy is often our company’s strategy  it is reasonable to assume that
a large amount of data generated by this policy is available  and thus  we have an accurate estimate of
its reward function. If in addition to this accurate estimate  we have access to the actual data  we can
use them in our algorithms. The reason we do not use the data generated by the actions suggested by
the baseline policy in constructing the conﬁdence sets of our algorithm in Section 3 is mainly to keep
the analysis simple. However  when dealing with the more general case of unknown baseline reward
in Section 4  we construct the conﬁdence sets using all available data  including those generated by
the baseline policy. It is important to note that having a good estimate of the baseline reward function
does not necessarily mean that we know the unknown parameter θ∗. This is because the data used for
this estimate has been generated by the baseline policy  and thus  may only provide a good estimate
of θ∗ in a limited subspace.
3 A Conservative Linear Bandit Algorithm
In this section  we propose a linear bandit algorithm  called conservative linear upper conﬁdence
bound (CLUCB)  whose pseudocode is shown in Algorithm 1. CLUCB is based on the optimism
in the face of uncertainty principle  and given the value of α  minimizes the regret (2) and satisﬁes
the performance constraint (3) with high probability. At each round t  CLUCB uses the previous

3

Algorithm 1 CLUCB

Input: α B F
Initialize: S0 = ∅  z0 = 0 ∈ Rd  and C1 = B
t (cid:101)θt) ∈ arg max(a θ)∈At×Ct (cid:104)θ  φt
for t = 1  2  3 ··· do
a(cid:105)

Find (a(cid:48)
Compute Lt = minθ∈Ct (cid:104)θ  zt−1 + φt
a(cid:48)
i=1 ri
bi

≥ (1 − α)(cid:80)t

if Lt +(cid:80)

ri
bi

(cid:105)

t

else

end if
end for

i∈Sc
t−1
Play at = a(cid:48)
t and observe reward yt deﬁned by (1)
Set zt = zt−1 + φt
t−1
Given at and yt  construct the conﬁdence set Ct+1 according to (5)
at

then
  St = St−1 ∪ t  Sc

t = Sc

Play at = bt and observe reward yt deﬁned by (1)
Set zt = zt−1  St = St−1  Sc

t−1 ∪ t  Ct+1 = Ct

t = Sc

t ∈ arg maxa∈At maxθ∈Ct(cid:104)θ  φt

observations and builds a conﬁdence set Ct that with high probability contains the unknown parameter
a(cid:105)  which has the best
θ∗. It then selects the optimistic action a(cid:48)
performance among all the actions available in At  within the conﬁdence set Ct. In order to make
sure that the constraint (3) is satisﬁed  the algorithm plays the optimistic action a(cid:48)
t  only if it satisﬁes
the constraint for the worst choice of the parameter θ ∈ Ct. To make this more precise  let St−1 be
the set of rounds i < t at which CLUCB has played the optimistic action  i.e.  ai = a(cid:48)
i. Similarly 
t−1 = {1  2 ···   t − 1} − St−1 is the set of rounds j < t at which CLUCB has followed the
Sc
baseline policy  i.e.  aj = bj.
In order to guarantee that it does not violate constraint (3)  at each round t  CLUCB plays the
optimistic action  i.e.  at = a(cid:48)

t  only if

(cid:104) (cid:88)

i∈Sc

t−1

min
θ∈Ct

(cid:68)

(cid:122)

(cid:125)(cid:124)
(cid:123)(cid:88)

zt−1

(cid:69)

φi
ai

i∈St−1

ri
bi +

θ 

(cid:105)(cid:105) ≥ (1 − α)

t(cid:88)

i=1

ri
bi  

+ (cid:104)θ  φt
a(cid:48)

t

and plays the conservative action  i.e.  at = bt  otherwise. In the following  we describe how CLUCB
constructs and updates its conﬁdence sets Ct.
3.1 Construction of Conﬁdence Sets
CLUCB starts by the most general conﬁdence set C1 = B and updates its conﬁdence set only when it
plays an optimistic action. This is mainly to simplify the analysis and is based on the idea that since
the reward function of the baseline policy is known ahead of time  playing a baseline action does not
provide any new information about the unknown parameter θ∗. However  this can be easily changed
to update the conﬁdence set after each action. In fact  this is what we do in the algorithm proposed in
Section 4. We follow the approach of Abbasi-Yadkori et al. [2011] to build conﬁdence sets for θ∗.
Let St = {i1  . . .   imt} be the set of rounds up to and including round t at which CLUCB has played
the optimistic action. Note that we have deﬁned mt = |St|. For a ﬁxed value of λ > 0  let

be the regularized least square estimate of θ at round t  where Φt = [φi1
ai1
[yi1  . . .   yimt
next round t + 1 as

(cid:98)θt = (ΦtΦ
(cid:110)
θ ∈ Rd : (cid:107)θ −(cid:98)θt(cid:107)Vt ≤ βt+1
Ct+1 =
(cid:17)
(cid:16) 1+(mt+1)D2/λ

(4)
] and Yt =
](cid:62). For a ﬁxed conﬁdence parameter δ ∈ (0  1)  we construct the conﬁdence set for the
(cid:114)

t   and the weighted norm is deﬁned
where βt+1 = σ
x(cid:62)V x for any x ∈ Rd and any positive deﬁnite V ∈ Rd×d. Note that similar to the linear
as (cid:107)x(cid:107)V =
UCB algorithm (LUCB) in Abbasi-Yadkori et al. [2011]  the sub-Gaussian parameter σ and the
regularization parameter λ that appear in the deﬁnitions of βt+1 and Vt should also be given to the
CLUCB algorithm as input. The following proposition (Theorem 2 in Abbasi-Yadkori et al. 2011)
shows that the conﬁdence sets constructed by (5) contain the true parameter θ∗ with high probability.

√
λB  Vt = λI + ΦtΦ(cid:62)

  . . .   φimt
aimt

−1 ΦtYt 

(cid:124)
t + λI)

(cid:111)

d log

(5)

√

+

 

δ

4

Proposition 1 For the conﬁdence set Ct deﬁned by (5)  we have P(cid:2)θ∗ ∈ Ct  ∀t ∈ N(cid:3) ≥ 1 − δ.

As mentioned before  CLUCB ensures that performance constraint (3) holds for all θ ∈ Ct at all
rounds t. As a result  if all the conﬁdence sets hold (i.e.  contain the true parameter θ∗)  CLUCB
is guaranteed to satisfy performance constraint (3). Proposition 1 indicates that this happens with
probability at least 1 − δ. It is worth noting that satisfying constraint (3) implies that CLUCB is at
least as good as the baseline policy at all rounds. In this vein  Proposition 1 guarantees that  with
probability at least 1 − δ  CLUCB performs no worse than the baseline policy at all rounds.
3.2 Regret Analysis of CLUCB
− rt
In this section  we prove a regret bound for the proposed CLUCB algorithm. Let ∆t
bt
bt
be the baseline gap at round t  i.e.  the difference between the expected rewards of the optimal and
baseline actions at round t. This quantity shows how sub-optimal the action suggested by the baseline
policy is at round t. We make the following assumption on the performance of the baseline policy πb.
Assumption 3 There exist 0 ≤ ∆l ≤ ∆h and 0 < rl such that  at each round t 

= rt
a∗

t

∆l ≤ ∆t

bt

≤ ∆h

and

rl ≤ rt

bt

.

(6)

An obvious candidate for both ∆h and rh is 1  as all the mean rewards are conﬁned in [0  1]. The
reward lower-bound rl ensures that the baseline policy maintains a minimum level of performance at
each round. Finally  ∆l = 0 is a reasonable candidate for the lower-bound of the baseline gap.
The following proposition shows that the regret of CLUCB can be decomposed into the regret of
a linear UCB (LUCB) algorithm (e.g.  Abbasi-Yadkori et al. 2011) and a regret caused by being
conservative in order to satisfy the performance constraint (3).

Proposition 2 The regret of CLUCB can be decomposed into two terms as follows:

(7)
T| =
where RST (LUCB) is the cumulative (pseudo)-regret of LUCB at rounds t ∈ ST and nT = |Sc
T − mT is the number of rounds (in T rounds) at which CLUCB has played a conservative action.

RT (CLUCB) ≤ RST (LUCB) + nT ∆h 

Proof: From the deﬁnition of regret (2)  we have

T(cid:88)

− T(cid:88)

(cid:88)

t∈ST

RT (CLUCB) =

rt
a∗

t

rt
at =

t=1

t=1

− rt

at ) +

(rt
a∗

t

(cid:122)

(cid:88)

t∈Sc

T

(rt
a∗

t

(cid:125)(cid:124)

∆t
bt

− rt

(cid:123)
bt ) ≤ (cid:88)

t∈ST

− rt

at ) + nT ∆h. (8)

(rt
a∗

t

The result follows from the fact that for t ∈ ST   CLUCB plays the exact same actions as LUCB  and
(cid:3)
thus  the ﬁrst term in (8) represents LUCB’s regret for these rounds.
The regret bound of LUCB for the conﬁdence set (5) can be derived from the results of Abbasi-
Yadkori et al. [2011]. Let E be the event that θ∗ ∈ Ct  ∀t ∈ N  which according to Proposition 1
holds w.p. at least 1 − δ. The following proposition provides a bound on RST (LUCB). Since this
proposition is a direct application of Thm. 3 in Abbasi-Yadkori et al. [2011]  we omit its proof here.
(cid:19)(cid:21)
Proposition 3 On event E = {θ∗ ∈ Ct  ∀t ∈ N}  for any T ∈ N  we have

(cid:115)

(cid:19)

(cid:18)

(cid:20)

√

mT d log

λ +

B

λ + σ

2 log(

) + d log

1 +

(cid:115)
RST (LUCB) ≤ 4
(cid:18)

1
δ

×

mT D

(cid:19)√

d

(cid:19)

T

.

mT D

λd

(9)

(cid:18)
(cid:18) D

T

λδ

= O

d log

Now in order to bound the regret of CLUCB  we only need to ﬁnd an upper-bound on nT   i.e.  the
number of times that CLUCB deviates from LUCB and selects the action suggested by the baseline
policy. We prove an upper-bound on nT in Theorem 4  which is the main technical result of this
section. Due to space constraint  we only provide a proof sketch for Theorem 4 in the paper and
report its detailed proof in Appendix A. The proof requires several technical lemmas that have been
proved in Appendix C.

5

√

√

(cid:33)(cid:35)2

nT ≤ 1 + 114d2 (B

λ + σ)2
αrl(∆l + αrl)

Theorem 4 Let λ ≥ max(1  D2). Then  on event E  for any horizon T ∈ N  we have

(cid:32)
(cid:34)
Proof Sketch: Let τ = max(cid:8)1 ≤ t ≤ T | at (cid:54)= a(cid:48)
(cid:9) be the last round that CLUCB takes an action
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)V
(cid:88)
τ(cid:88)

suggested by the baseline policy. We ﬁrst show that at round τ  the following holds:

bt ≤ −(mτ−1 + 1)∆l + 2βτ
rt

λ + σ)
δ(∆l + αrl)

(cid:13)(cid:13)φτ

(cid:13)(cid:13)φt

√
62d(B

(cid:13)(cid:13)V

(cid:13)(cid:13)V

t∈Sτ−1

+ 2βτ

φt
at

−1
t

−1
τ

log

+ 2

βt

t=1

a(cid:48)

τ

α

at

.

t

.

−1
τ

Next  using Lemmas 7 and 8 (reported in Appendix C)  and the Cauchy-Schwartz inequality  we
deduce that

τ(cid:88)
≥ rl for all t  and τ = nτ−1 + mτ−1 + 1  it follows that

bt ≤ −(mτ−1 + 1)∆l + 8d(B
rt

√
λ + σ) log

t=1

α

αrlnτ−1 ≤ −(mτ−1 + 1)(∆l + αrl) + 8d(B

λ + σ) log

√

(cid:18) 2(mτ−1 + 1)
(cid:18) 2(mτ−1 + 1)

δ

δ

Since rt
bt

τ

+

a(cid:48)

t∈Sτ−1

(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)φτ
(cid:19)(cid:112)(mτ−1 + 1).
(cid:19)(cid:112)(mτ−1 + 1).

(10)

Note that nτ−1 and mτ−1 appear on the LHS and RHS of (10)  respectively. The key point is that
the RHS is positive only for a ﬁnite number of integers mτ−1  and thus  it has a ﬁnite upper bound.
Using Lemma 9 (reported and proved in Appendix C)  we prove that
√
√
λ + σ)
62d(B
δ(∆l + αrl)

αrlnτ−1 ≤ 114d2 (B

(cid:33)(cid:35)2

∆l + αrl

λ + σ)2

(cid:32)

(cid:34)

√

×

log

.

(cid:3)
Finally  the fact that nT = nτ = nτ−1 + 1 completes the proof.
We now have all the necessary ingredients to derive a regret bound on the performance of the CLUCB
algorithm. We report the regret bound of CLUCB in Theorem 5  whose proof is a direct consequence
of the results of Propositions 2 and 3  and Theorem 4.
Theorem 5 Let λ ≥ max(1  D2). With probability at least 1 − δ  the CLUCB algorithm satisﬁes the
performance constraint (3) for all t ∈ N  and has the regret bound

where K is a constant that only depends on the parameters of the problem as

(cid:19)

T +

K∆h
αrl

(cid:18)
(cid:1)√
d log(cid:0) DT
(cid:32)
(cid:34)

λδ

(cid:33)(cid:35)2

.

√

√

K = 1 + 114d2 (B

λ + σ)2

∆l + αrl

log

√
62d(B

λ + σ)
δ(∆l + αrl)

RT (CLUCB) = O

 

(11)

√
Remark 2. The ﬁrst term in the regret bound (11) is the regret of LUCB  which grows at the rate
T log(T ). The second term accounts for the loss incurred by being conservative in order to satisfy
the performance constraint (3). Our results indicate that this loss does not grow with time (since
CLUCB acts conservatively only in a ﬁnite number of rounds). This is a clear improvement over
the regret bound reported in Wu et al. [2016] for the MAB setting  in which the regret of being
conservative grows with time. Furthermore  the regret bound of Theorem 5 clearly indicates that
CLUCB’s regret is larger for smaller values of α. This perfectly matches the intuition that the agent
must be more conservative  and thus  suffers higher regret for smaller values of α. Theorem 5 also
indicates that CLUCB’s regret is smaller for smaller values of ∆h  because when the baseline policy
πb is close to optimal  the algorithm does not lose much by being conservative.

6

Algorithm 2 CLUCB2

Input: α  rl B F
Initialize: n ← 0  z ← 0  w ← 0  v ← 0 and C1 ← B
for t = 1  2  3 ··· do

t (cid:101)θ) = arg max(a θ)∈At×Ct (cid:104)θ  φt
a(cid:105)

Let bt be the action suggested by πb at round t
Find (a(cid:48)
Find Rt = maxθ∈Ct(cid:104)θ  v + φt
if Lt ≥ (1 − α)Rt then

(cid:105) & Lt = minθ∈Ct(cid:104)θ  z + φt
a(cid:48)

bt

t

Play at = a(cid:48)
Set z ← z + φt
a(cid:48)

t

t and observe yt deﬁned by (1)

and v ← v + φt

bt

else

end for

Play at = bt and observe yt deﬁned by (1)
Set w = w + φt
bt

and n ← n + 1

end if
Given at and yt  construct the conﬁdence set Ct+1 according to (15)

(cid:105) + α max(cid:8) minθ∈Ct(cid:104)θ  w(cid:105)  nrl

(cid:9)

4 Unknown Baseline Reward
In this section  we consider the case where the expected rewards of the actions taken by the baseline
  are unknown at the beginning. We show how the CLUCB algorithm presented in Section 3
policy  rt
bt
should be changed to handle this case  and present a new algorithm  called CLUCB2. We prove a
regret bound for CLUCB2  which is at the same rate as that for CLUCB. This shows that the lack of
knowledge about the reward function of the baseline policy does not hurt our algorithm in terms of
the rate of the regret. The pseudocode of CLUCB2 is shown in Algorithm 2. The main difference
with CLUCB is in the condition that should be checked at each round t to see whether we should
play the optimistic action a(cid:48)
t or the conservative action bt. This condition should be selected in a way
that CLUCB2 satisﬁes constraint (3). We may rewrite (3) as

(cid:88)

i∈St−1

ri
ai

+ rt
a(cid:48)

t

+ α

ri
bi

≥ (1 − α)(cid:0)rt

(cid:88)

+

bt

i∈St−1

If we lower-bound the LHS and upper-bound the RHS of (12)  we obtain
(cid:104)θ 

(cid:104)θ 

(cid:104)θ 

φi
bi

(cid:105) ≥ (1 − α) max
θ∈Ct

(cid:105) + α min
θ∈Ct

φi
ai

+ φt
a(cid:48)

t

min
θ∈Ct

(cid:88)

i∈St−1

(cid:88)
(cid:88)

t−1

i∈Sc

i∈Sc

t−1

ri
bi

(cid:1).
(cid:88)

i∈St−1

(12)

φi
bi

+ φt
bt

(cid:105).

(13)

Since each conﬁdence set Ct is built in a way to contain the true parameter θ∗ with high probability 
it is easy to see that (12) is satisﬁed whenever (13) is true.
CLUCB2 uses both optimistic and conservative actions  and their corresponding rewards in building
]  Yt = [y1  y2 ···   yt]
(cid:124) 
its conﬁdence sets. Speciﬁcally for any t  we let Φt = [φ1
a1
Vt = λI + Φ

(cid:124)
t Φt  and deﬁne the least-square estimate after round t as

 ···   φt

  φ2
a2

at

(cid:124)
t + λI)

(cid:98)θt = (ΦtΦ
−1 ΦtYt.
Given Vt and(cid:98)θt  the conﬁdence set for round t + 1 is constructed as
(cid:110)
(cid:111)
θ ∈ Ct : (cid:107)θ −(cid:98)θt(cid:107)Vt ≤ βt+1
(cid:16) 1+tD2/λ

(cid:114)
i.e.  P(cid:2)θ∗ ∈ Ct  ∀t ∈ N(cid:3) ≥ 1 − δ.

Ct+1 =

(cid:17)

+ B

√

where C1 = B and βt = σ
λ. Similar to Proposition 1  we can easily
prove that the conﬁdence sets built by (15) contain the true parameter θ∗ with high probability 

d log

δ

(14)

(15)

 

Remark 3. Note that unlike the CLUCB algorithm  here we build nested conﬁdence sets  i.e.  ··· ⊆
Ct+1 ⊆ Ct ⊆ Ct−1 ⊆ ···   which is necessary for the proof of the algorithm. This can potentially
increase the computational complexity of CLUCB2  but from a practical point of view  the conﬁdence

7

Figure 1: Average per-step regret (over 1  000 runs) of LUCB and CLUCB for different values of α.

sets become nested automatically after sufﬁcient data has been observed. Therefore  the nested
constraint in building the conﬁdence sets can be relaxed after sufﬁciently large number of rounds.
The following theorem guarantees that CLUCB2 satisﬁes the safety constraint (3) with high probabil-
ity  while its regret has the same rate as that of CLUCB and is worse than that of LUCB only up to an
additive constant.
Theorem 6 Let λ ≥ max(1  D2) and δ ≤ 2/e. Then  with probability at least 1 − δ  CLUCB2
algorithm satisﬁes the performance constraint (3) for all t ∈ N  and has the regret bound

RT (CLUCB2) = O

d log

T +

 

(16)

(cid:18)

(cid:34)

(cid:18) DT
(cid:32)

λδ

(cid:19)√

√

K∆h
α2r2
l

(cid:19)
(cid:33)(cid:35)2

where K is a constant that depends only on the parameters of the problem as

√

K = 256d2(B

λ + σ)2

log

10d(B

λ + σ)

αrl(δ)1/4

+ 1.

We report the proof of Theorem 6 in Appendix B. The proof follows the same steps as that of
Theorem 5  with additional non-trivial technicalities that have been highlighted there.

from N(cid:0)0  I4

(cid:1) such that the mean reward associated to each arm is positive. The observation noise at

5 Simulation Results
In this section  we provide simulation results to illustrate the performance of the proposed CLUCB
algorithm. We considered a time independent action set of 100 arms each having a time independent
feature vector living in R4 space. These feature vectors and the parameter θ∗ are randomly drawn
each time step is also generated independently from N (0  1)  and the mean reward of the baseline
policy at any time is taken to be the reward associated to the 10’th best action. We have taken
λ = 1  δ = 0.001 and the results are averaged over 1 000 realizations.
In Figure 1  we plot per-step regret (i.e.  Rt
t ) of LUCB and CLUCB for different values of α over
a horizon T = 40  000. Figure 1 shows that per-step regret of CLUCB remains constant at the
beginning (the conservative phase). This is because during this phase  CLUCB follows the baseline
policy to make sure that the performance constraint (3) is satisﬁed. As expected  the length of the
conservative phase decreases as α is increased  since the performance constraint is relaxed for larger
values of α  and hence  CLUCB starts playing optimistic actions more quickly. After this initial
conservative phase  CLUCB has learned enough about the optimal action and its performance starts
converging to that of LUCB. On the other hand  Figure 1 shows that per-step regret of CLUCB at the
ﬁrst few periods remains much lower than that of LUCB. This is because LUCB plays agnostic to the
safety constraint  and thus  may select very poor actions in its initial exploration phase. In regard
to this  Figure 2(a) plots the percentage of the rounds  in the ﬁrst 1  000 rounds  at which the safety
constraint (3) is violated by LUCB and CLUCB for different values of α. According to this ﬁgure 

8

(a)

(b)

Figure 2: (a) Percentage of the rounds  in the ﬁrst 1  000 rounds  at which the safety constraint is
violated by LUCB and CLUCB for different values of α  (b) Per-step regret of LUCB and CLUCB
for different values of α  at round t = 40  000.

CLUCB satisﬁes the performance constraint for all values of α  while LUCB fails in a signiﬁcant
number of rounds  specially for small values of α (i.e.  tight constraint).
To better illustrate the effect of the performance constraint (3) on the regret of the algorithms 
Figure 2(b) plots the per-step regret achieved by CLUCB at round t = 40  000 for different values of
α  as well as that for LUCB. As expected from our analysis and is shown in Figure 1  the performance
of CLUCB converges to that of LUCB after an initial conservative phase. Figure 2(b) conﬁrms that
the convergence happens more quickly for larger values of α  where the constraint is more relaxed.

6 Conclusions
In this paper  we studied the concept of safety in contextual linear bandits to address the challenges that
arise in implementing such algorithms in practical situations such as personalized recommendation
systems. Most of the existing linear bandit algorithms  such as LUCB [Abbasi-Yadkori et al.  2011] 
suffer from a large regret at their initial exploratory rounds. This unsafe behavior is not acceptable
in many practical situations  where having a reasonable performance at any time is necessary for a
learning algorithm to be considered reliable and to remain in production.
To guarantee safe learning  we formulated a conservative linear bandit problem  where the per-
formance of the learning algorithm (measured in terms of its cumulative rewards) at any time is
constrained to be at least as good as a fraction of the performance of a baseline policy. We proposed
a conservative version of LUCB algorithm  called CLUCB  to solve this constrained problem  and
showed that it satisﬁes the safety constraint with high probability  while achieving a regret bound
equivalent to that of LUCB up to an additive time-independent constant. We designed two versions of
CLUCB that can be used depending on whether the reward function of the baseline policy is known or
unknown  and showed that in each case  CLUCB acts conservatively (i.e.  plays the action suggested
by the baseline policy) only at a ﬁnite number of rounds  which depends on how suboptimal the
baseline policy is. We reported simulation results that support our analysis and show the performance
of the proposed CLUCB algorithm.

9

References
Y. Abbasi-Yadkori  D. P´al  and C. Szepesv´ari. Improved algorithms for linear stochastic bandits. In

Advances in Neural Information Processing Systems  pages 2312–2320  2011.

P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning Journal  47:235–256  2002.

L. Bottou  J. Peters  J. Quinonero-Candela  D. Charles  D. Chickering  E. Portugaly  D. Ray  P. Simard 
and E. Snelson. Counterfactual reasoning and learning systems: The example of computational
advertising. Journal of Machine Learning Research  14:3207–3260  2013.

W. Chu  L. Li  L. Reyzin  and R. Schapire. Contextual bandits with linear payoff functions. In
Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics 
pages 208–214  2011.

V. Dani  T. Hayes  and S. Kakade. Stochastic linear optimization under bandit feedback. In COLT 

pages 355–366  2008.

M. Petrik  M. Ghavamzadeh  and Y. Chow. Safe policy improvement by minimizing robust baseline

regret. In Advances in Neural Information Processing Systems  pages 2298–2306  2016.

P. Rusmevichientong and J. Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations

Research  35(2):395–411  2010.

D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics of Operations

Research  39(4):1221–1243  2014.

A. Swaminathan and T. Joachims. Batch learning from logged bandit feedback through counterfactual

risk minimization. Journal of Machine Learning Research  16:1731–1755  2015.

A. Swaminathan and T. Joachims. Counterfactual risk minimization: Learning from logged bandit

feedback. In Proceedings of The 32nd International Conference on Machine Learning  2015.

P. Thomas  G. Theocharous  and M. Ghavamzadeh. High conﬁdence off-policy evaluation. In

Proceedings of the Twenty-Ninth Conference on Artiﬁcial Intelligence  2015.

P. Thomas  G. Theocharous  and M. Ghavamzadeh. High conﬁdence policy improvement.

In
Proceedings of the Thirty-Second International Conference on Machine Learning  pages 2380–
2388  2015.

Y. Wu  R. Shariff  T. Lattimore  and C. Szepesv´ari. Conservative bandits. In Proceedings of The 33rd

International Conference on Machine Learning  pages 1254–1262  2016.

10

,Abbas Kazerouni
Mohammad Ghavamzadeh
Yasin Abbasi Yadkori
Benjamin Van Roy