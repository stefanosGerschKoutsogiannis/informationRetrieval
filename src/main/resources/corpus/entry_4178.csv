2019,Outlier-robust estimation of a sparse linear model using $\ell_1$-penalized Huber's $M$-estimator,We study the problem of estimating a $p$-dimensional 
$s$-sparse vector in a linear model with Gaussian design. 
In the case where the labels are contaminated by at most 
$o$ adversarial outliers  we prove that the $\ell_1$-penalized 
Huber's $M$-estimator based on $n$ samples attains the 
optimal rate of convergence $(s/n)^{1/2} + (o/n)$  up to a 
logarithmic factor. For more general design matrices  our results 
highlight the importance of two properties: the transfer principle
and the incoherence property. These properties with suitable 
constants are shown to yield the optimal rates of robust 
estimation with adversarial contamination.,Outlier-robust estimation of a sparse linear model

using (cid:96)1-penalized Huber’s M-estimator

Abstract

We study the problem of estimating a p-dimensional s-sparse vector in a linear
model with Gaussian design and additive noise. In the case where the labels are
contaminated by at most o adversarial outliers  we prove that the (cid:96)1-penalized
Huber’s M-estimator based on n samples attains the optimal rate of convergence
(s/n)1/2 + (o/n)  up to a logarithmic factor. For more general design matrices 
our results highlight the importance of two properties: the transfer principle and
the incoherence property. These properties with suitable constants are shown to
yield the optimal rates  up to log-factors  of robust estimation with adversarial
contamination.

1

Introduction

Is it possible to attain optimal rates of estimation in outlier-robust sparse regression using penalized
empirical risk minimization (PERM) with convex loss and convex penalties? Current state of literature
on robust estimation does not answer this question. Furthermore  it contains some signals that might
suggest that the answer to this question is negative. First  it has been shown in (Chen et al.  2013 
Theorem 1) that in the case of adversarially corrupted samples  no method based on penalized
empirical loss minimization  with convex loss and convex penalty  can lead to consistent support
recovery. The authors then advocate for robustifying the (cid:96)1-penalized least-squares estimators by
replacing usual scalar products by their trimmed counterparts. Second  (Chen et al.  2018) established
that in the multivariate Gaussian model subject to Huber’s contamination  coordinatewise median—
which is the ERM for the (cid:96)1-loss—is sub-optimal. Similar result was proved in (Lai et al.  2016 
Prop. 2.1) for the geometric median  the ERM corresponding to the (cid:96)2-loss. These negative results
prompted researchers to use other techniques  often of higher computational complexity  to solve the
problem of outlier-corrupted sparse linear regression.
In the present work  we prove that the (cid:96)1-penalized empirical risk minimizer based on Huber’s loss is
minimax-rate-optimal  up to possible logarithmic factors. Naturally  this result is not valid in the most
general situation  but we demonstrate its validity under the assumptions that the design matrix satisﬁes
some incoherence condition and only the response is subject to contamination. The incoherence
condition is shown to be satisﬁed by the Gaussian design with a covariance matrix that has bounded
and bounded away from zero diagonal entries. This relatively simple setting is chosen in order to
convey the main message of this work: for properly chosen convex loss and convex penalty functions 
the PERM is minimax-rate-optimal in sparse linear regression with adversarially corrupted labels.
To describe more precisely the aforementioned optimality result  let D◦
i ); i = 1  . . .   n}
be iid feature-label pairs such that X i ∈ Rp are Gaussian with zero mean and covariance matrix Σ
and y◦

i are deﬁned by the linear model

n = {(X i  y◦

i = X(cid:62)
y◦
i β
where the random noise ξi  independent of X i  is Gaussian with zero mean and variance σ2. Instead of
observing the “clean” data D◦
n  we have access to a contaminated version of it  Dn = {(X i  yi); i =
1  . . .   n}  in which a small number o ∈ {1  . . .   n} of labels y◦
i are replaced by an arbitrary value.
Setting θ∗
n  and using the matrix-vector notation  the described model can be written
as

i = (yi − y◦
i )/

i = 1  . . .   n 

+ ξi 

√

∗

√

∗

Y = Xβ

+

n θ

+ ξ 

(1)

∗

Submitted to 33rd Conference on Neural Information Processing Systems (NeurIPS 2019). Do not distribute.

1
2
3
4

5
6
7
8
9

10

11
12
13
14
15
16
17
18
19
20
21
22
23

24
25
26
27
28
29
30
31

32
33
34

35
36
37
38
39

40

41
42
43
44

45

46
47

48
49
50

51
52
53
54
55

56
57
58

59
60

61
62

63

64
65
66

67

68
69
70
71
72
73

= (θ∗

where X = [X(cid:62)
∗
1  . . .   θ∗
θ
estimate the vector β
some small value s ∈ {1  . . .   p}  the vector β
0} ≤ s. In such a setting  it is well-known that if we have access to the clean data D◦

n ] is the n × p design matrix  Y = (y1  . . .   yn)(cid:62) is the response vector 
1 ; . . . ; X(cid:62)
n)(cid:62) is the contamination and ξ = (ξ1  . . .   ξn)(cid:62) is the noise vector. The goal is to
∗ ∈ Rp. The dimension p is assumed to be large  possibly larger than n but  for
∗(cid:107)0 = Card{j : β∗ (cid:54)=
n and measure

the quality of an estimator(cid:98)β by the Mahalanobis norm1 (cid:107)Σ1/2((cid:98)β − β

∗ is assumed to be s-sparse: (cid:107)β

∗

r◦(n  p  s) = σ

(cid:17)1/2

)(cid:107)2  the optimal rate is

(cid:16) s log(p/s)
n is unavailable but one has access to Dn  the

n

.

In the outlier-contaminated setting  i.e.  when D◦
minimax-optimal-rate (Chen et al.  2016) takes the form

(cid:16) s log(p/s)

(cid:17)1/2

n

r(n  p  s  o) = σ

+

σo
n

.

(2)

The ﬁrst estimators proved to attain this rate (Chen et al.  2016; Gao  2017) were computationally
intractable2 for large p  s and o. This motivated several authors to search for polynomial-time
algorithms attaining nearly optimal rate; the most relevant results will be reviewed later in this work.
The assumption that only a small number o of labels are contaminated by outliers implies that the
∗ while ensuring
vector θ
computational tractability of the resulting estimator  a natural approach studied in several papers
(Laska et al.  2009; Nguyen and Tran  2013; Dalalyan and Chen  2012) is to use some version of the
(cid:96)1-penalized ERM. This corresponds to deﬁning

∗ in (1) is o-sparse. In order to take advantage of sparsity of both β

∗ and θ

(cid:107)Y − X(cid:62)β − √

n θ(cid:107)2

2 + λs(cid:107)β(cid:107)1 + λo(cid:107)θ(cid:107)1

(3)

(cid:98)β ∈ arg min

β∈Rp

min
θ∈Rn

(cid:110) 1

2n

(cid:111)

 

√
where λs  λo > 0 are tuning parameters. This estimator is very attractive from a computational
perspective  since it can be seen as the Lasso for the augmented design matrix M = [X 
n In] 
where In is the n × n identity matrix. To date  the best known rate for this type of estimator is

(cid:16) s log p

(cid:17)1/2

σ

n

(cid:16) o

(cid:17)1/2

+ σ

n

 

(4)

obtained in (Nguyen and Tran  2013) under some restrictions on (n  p  s  o). A quick comparison of
(2) and (4) shows that the latter is sub-optimal. Indeed  the ratio of the two rates may be as large as
(n/o)1/2. The main goal of the present paper is to show that this sub-optimality is not an intrinsic
property of the estimator (3)  but rather an artefact of previous proof techniques. By using a reﬁned

argument  we prove that(cid:98)β deﬁned by (3) does attain the optimal rate under very mild assumptions.
In the sequel  we refer to(cid:98)β as (cid:96)1-penalized Huber’s M-estimator. The rationale for this term is that

the minimization with respect to θ in (3) can be done explicitly. It yields (Donoho and Montanari 
2016  Section 6)

(cid:110)

(cid:98)β ∈ arg min

β∈Rp

n(cid:88)

i=1

λ2
o

Φ

(cid:16) yi − X(cid:62)

√

i β
n

λo

(cid:17)

(cid:111)

 

+ λs(cid:107)β(cid:107)1

(5)

where Φ : R → R is Huber’s function deﬁned by Φ(u) = (1/2)u2 ∧ (|u| − 1/2).

To prove the rate-optimality of the estimator(cid:98)β  we ﬁrst establish a risk bound for a general design

matrix X not necessarily formed by Gaussian vectors. This is done in the next section. Then  in
Section 3  we state and discuss the result showing that all the necessary conditions are satisﬁed for the
Gaussian design. Relevant prior work is presented in Section 4  while Section 5 discusses potential
extensions. Section 7 provides a summary of our results and an outlook on future work. The proofs
are deferred to the supplementary material.

1In the sequel  we use notation (cid:107)β(cid:107)q = ((cid:80)

2In the sense that there is no algorithm computing these estimators in time polynomial in (n  p  s  o).

j |βj|q)1/q for any vector β ∈ Rp and any q ≥ 1.

2

74

75

76
77
78
79
80

81
82
83
84

85
86
87

88

89
90

91
92

93
94

95

96

97

98

99

100

101

102
103
104
105

106
107

108
109
110
111

112
113

114
115
116

2 Risk bound for the (cid:96)1-penalized Huber’s M-estimator

This section is devoted to bringing forward sufﬁcient conditions on the design matrix that allow for

rate-optimal risk bounds for the estimator(cid:98)β deﬁned by (3) or  equivalently  by (5). There are two
rate σ(cid:112)(s/n) log(p/s). On the other hand  in the case where n = p and X =

qualitative conditions that can be easily seen to be necessary: we call them restricted invertibility and
incoherence. Indeed  even when there is no contamination  i.e.  the number of outliers is known to
be o = 0  the matrix X has to satisfy a restricted invertibility condition (such as restricted isometry 
restricted eigenvalue or compatibility) in order that the Lasso estimator (3) does achieve the optimal
n In  even in
∗.
+ θ
∗ when the design matrix X is aligned with the

the extremely favorable situation where the noise ξ is zero  the only identiﬁable vector is β
Therefore  it is impossible to consistently estimate β
identity matrix In or close to be so.
The next deﬁnition formalizes what we call restricted invertibility and incoherence by introducing
three notions: the transfer principle  the incoherence property and the augmented transfer principle.
We will show that these notions play a key role in robust estimation by (cid:96)1-penalized least squares.
√
Deﬁnition 1. Let Z ∈ Rn×p be a (random) matrix and Σ ∈ Rp×p. We use notation Z(n) = Z/
n.
(i) We say that Z satisﬁes the transfer principle with a1 ∈ (0  1) and a2 ∈ (0 ∞)  denoted by

√

∗

TPΣ(a1; a2)  if for all v ∈ Rp 

(ii) We say that Z satisﬁes the incoherence property IPΣ(b1; b2; b3) for some positive numbers

(6)

(7)

(cid:13)(cid:13)Z(n)v(cid:13)(cid:13)2 ≥ a1(cid:107)Σ1/2v(cid:107)2 − a2(cid:107)v(cid:107)1.
(cid:13)(cid:13)Σ1/2v(cid:13)(cid:13)2(cid:107)u(cid:107)2 + b2(cid:107)v(cid:107)1(cid:107)u(cid:107)2 + b3

(cid:13)(cid:13)Σ1/2v(cid:13)(cid:13)2(cid:107)u(cid:107)1.

b1  b2 and b3  if for all [v; u] ∈ Rp+n 

|u(cid:62)Z(n)v| ≤ b1

(iii) We say that Z satisﬁes the augmented transfer principle ATPΣ(c1; c2; c3) for some positive

numbers c1  c2 and c3  if for all [v; u] ∈ Rp+n 

(cid:13)(cid:13)[Σ1/2v; u](cid:13)(cid:13)2 − c2(cid:107)v(cid:107)1 − c3(cid:107)u(cid:107)1.

(cid:107)Z(n)v + u(cid:107)2 ≥ c1

These three properties are inter-related and related to extreme singular values of the matrix Z(n).
(P1) If Z satisﬁes ATPΣ(c1; c2; c3) then it also satisﬁes TPΣ(c1; c2).
(P2) If Z satisﬁes TPΣ(a1; a2) and IPΣ(b1; b2; b3) then it also satisﬁes ATPΣ(c1; c2; c3) with

1 − b1 − α2  c2 = a2 + 2b2/α and c3 = 2b3/α for any positive α <(cid:112)a2

1 − b1.

c2
1 = a2

(P3) If Z satisﬁes IPΣ(b1; b2; b3)  then it also satisﬁes IPΣ(0; b2; b1 + b3)
(P4) Any matrix Z satisﬁes TPI(sp(Z(n)); 0)  and IPI(s1(Z(n)); 0; 0)  where sp(Z(n)) and s1(Z(n))

are  respectively  the p-th largest and the largest singular values of Z(n).

Claim (P1) is true  since if we choose u = 0 in (7) we obtain (6). Claim (P2) coincides with Lemma 7 
proved in the supplement. (P3) is a direct consequence of the inequality (cid:107)u(cid:107)2 ≤ (cid:107)u(cid:107)1  valid for
any vector u. (P4) is a well-known characterization of the smallest and the largest singular values
of a matrix. We will show later on that a Gaussian matrix satisﬁes with high probability all these
conditions with constants a1 and c1 independent of (n  p) and a2  b2  b3  c2  c3 of order n−1/2  up to
logarithmic factors.
To state the main theorem of this section  we consider the simpliﬁed setting in which λs = λo = λ.
Remind that in practice it is always recommended to normalize the columns of the matrix X so that
their Euclidean norm is of the order
n. The more precise version of the next result with better
constants is provided in the supplement (see Proposition 1). We recall that a matrix Σ is said to satisfy
the restricted eigenvalue condition RE(s  c0) with some constant κ > 0  if (cid:107)Σ1/2v(cid:107)2 ≥ κ(cid:107)vJ(cid:107)2 for
any vector v ∈ Rp and any set J ⊂ {1  . . .   p} such that Card(J) ≤ s and (cid:107)vJ c(cid:107)1 ≤ c0(cid:107)vJ(cid:107)1.
Theorem 1. Let Σ satisfy the RE(s  5) condition with constant κ > 0. Let b1  b2  b3  c1  c2  c3
be some positive real numbers such that X satisﬁes the IPΣ(0; b2; b3) and the ATPΣ(c1; c2; c3).
Assume that for some δ ∈ (0  1)  the tuning parameter λ satisﬁes
(cid:107)X(n)• j (cid:107)2

n ≥(cid:112)8 log(n/δ)

(cid:1)(cid:112)8 log(p/δ).

(cid:95)(cid:0) max

√
λ

√

j=1 ... p

3

117

118

119

120
121
122

123

124
125

126
127
128
129
130

131
132
133
134

135

136

137

138

139

140
141

142
143

If the sparsity s and the number of outliers o satisfy the condition

then  with probability at least 1 − 2δ  we have

(cid:13)(cid:13)Σ1/2((cid:98)β − β

∗

)(cid:13)(cid:13)2 ≤ 24λ

c2
1

s

κ2 + o ≤

(cid:1)2  

c2
1

400(cid:0)c2 ∨ c3 ∨ 5b2/c1
(cid:17)(cid:16) s
(cid:16) 2c2

(cid:95) b3

c1

c2
1

κ2 + 7o

(cid:17)

√
5λ
s
κ .
6c2
1

+

(8)

(9)

(cid:107)(cid:98)β − β

∗(cid:107)1 + (cid:107)(cid:98)θ − θ

(cid:107)(cid:98)β − β

2 + (cid:107)(cid:98)θ − θ

∗(cid:107)2

Theorem 1 is somewhat hard to parse. At this stage  let us simply mention that in the case of a
Gaussian design considered in the next section  c1 is of order 1 while b2  b3  c2  c3 are of order n−1/2 
up to a factor logarithmic in p  n and 1/δ. Here δ is an upper bound on the probability that the
Gaussian matrix X does not satisfy either IPΣ or ATPΣ. Since Theorem 1 allows us to choose λ
∗  measured in
nκ2 )1/2)  under the assumption that

of the order(cid:112)log{(p + n)/δ}/n  we infer from (9) that the error of estimating β

Euclidean norm  is of order
( s
nκ2 + o
To complete this section  we present a sketch of the proof of Theorem 1. In order to convey the main
ideas without diving too much into technical details  we assume Σ = Ip. This means that the RE
condition is satisﬁed with κ = 1 for any s and c0. From the fact that the ATPΣ holds for X  we infer
that [X
n In] satisﬁes the RE(s + o  5) condition with the constant c1/2. Using the well-known
risk bounds for the Lasso estimator (Bickel et al.  2009)  we get

nκ2 )1/2 = O( o
n ) log(np/δ) is smaller than a universal constant.

nκ2 + o

n + ( s

n + ( s

√

s

∗(cid:107)1 ≤ Cλ(s + o).

∗(cid:107)2
2 ≤ Cλ2(s + o)

.

2n

and

β∈Rp

(cid:111)

(cid:110) 1

(cid:107)Y − Xβ − √

(cid:98)β ∈ arg min

The KKT conditions of this convex optimization problem take the following form

(10)
Note that these are the risk bounds established in3 (Candès and Randall  2008; Dalalyan and Chen 
2012; Nguyen and Tran  2013). These bounds are most likely unimprovable as long as the estimation
∗  considering θ
∗ as a
of θ
nuisance parameter  the following argument leads to a sharper risk bound. First  we note that

∗ is of interest. However  if we focus only on the estimation error of β

for every j ∈ {1  . . .   p}. Multiplying the last displayed equation from left by β

where sgn((cid:98)β) is the subset of Rp containing all the vectors w such that wj(cid:98)βj = |(cid:98)βj| and |wj| ≤ 1

1/nX(cid:62)(Y − X(cid:98)β − √
∗ −(cid:98)β)(cid:62)X(cid:62)(Y − X(cid:98)β − √

n(cid:98)θ(cid:107)2
2 + λ(cid:107)β(cid:107)1
n(cid:98)θ) ∈ λ · sgn((cid:98)β) 
∗ −(cid:98)β  we get
n(cid:98)θ) ≤ λ(cid:0)(cid:107)β
(cid:1).
∗(cid:107)1 − (cid:107)(cid:98)β(cid:107)1
∗ −(cid:98)β and u = θ
∗ −(cid:98)θ. We arrive at
2 = 1/nv(cid:62)X(cid:62)Xv ≤ −v(cid:62)(X(n))(cid:62)u − 1/nv(cid:62)X(cid:62)ξ + λ(cid:0)(cid:107)β
(cid:1).
∗(cid:107)1 − (cid:107)(cid:98)β(cid:107)1
∗(cid:107)1 − (cid:107)(cid:98)β(cid:107)1 ≤
(cid:1).
2 ≤ |v(cid:62)(X(n))(cid:62)u| + λ/2(cid:0)4(cid:107)vS(cid:107)1 − (cid:107)v(cid:107)1

On the one hand  the duality inequality and the lower bound on λ imply that |v(cid:62)X(cid:62)ξ| ≤
(cid:107)v(cid:107)1(cid:107)X(cid:62)ξ(cid:107)∞ ≤ nλ(cid:107)v(cid:107)1/2. On the other hand  well-known arguments yield (cid:107)β
2(cid:107)vS(cid:107)1 − (cid:107)v(cid:107)1. Therefore  we have

Recall now that Y = Xβ

+ ξ and set v = β

1/n(cid:107)Xv(cid:107)2

1/n(cid:107)Xv(cid:107)2

1/n(β

√

n θ

+

∗

∗

(11)
2 ≤ 2/n(cid:107)Xv(cid:107)2

2 +

1(cid:107)v(cid:107)2

1. Combining with (11)  this yields

Since X satisﬁes the ATPI(c1  c2  c3) that implies the TPI(c1  c2)  we get c2
2(cid:107)v(cid:107)2
2c2
1(cid:107)v(cid:107)2
c2

2|v(cid:62)(X(n))(cid:62)u| + λ(cid:0)4(cid:107)vS(cid:107)1 − (cid:107)v(cid:107)1
2b3(cid:107)v(cid:107)2(cid:107)u(cid:107)1 + 2b2(cid:107)v(cid:107)1(cid:107)u(cid:107)2 + λ(cid:0)4(cid:107)vS(cid:107)1 − (cid:107)v(cid:107)1

(cid:1) + 2c2

IPI(0 b2 b3)≤

2(cid:107)v(cid:107)2

≤

2

1

(cid:1) + 2c2

2(cid:107)v(cid:107)2

1

(12)

≤

1 + (cid:107)v(cid:107)1(2b2(cid:107)u(cid:107)2 − λ) + 4λ(cid:107)vS(cid:107)1 + 2c2
3the ﬁrst two references deal with the small dimensional case only  that is where s = p (cid:28) n.

(cid:107)u(cid:107)2

(cid:107)v(cid:107)2

2 +

2b2
3
c2
1

c2
1
2

2(cid:107)v(cid:107)2
1.

4

144
145
146

147
148
149
150

151

152
153
154
155
156
157
158
159
160
161

162
163
164

165
166
167
168
169
170
171

172
173

174
175
176
177
178

Using the ﬁrst inequality in (10) and condition (8)  we upper bound (2b2(cid:107)u(cid:107)2 − λ) by 0. To upper
√
bound the second last term  we use the Cauchy-Schwarz inequality: 4λ(cid:107)vS(cid:107)1 ≤ 4λ
s(cid:107)v(cid:107)2 ≤
(4/c1)2λ2s + (c1/2)2(cid:107)v(cid:107)2
1/4)(cid:107)v(cid:107)2
(c2

2. Combining all these bounds and rearranging the terms  we arrive at
2 ≤ 2{(b3/c1) ∨ c2}2((cid:107)u(cid:107)1 + (cid:107)v(cid:107)1)2 + (4/c1)2λ2s.

Taking the square root of both sides and using the second inequality in (10)  we obtain an inequality
of the same type as (9) but with slightly larger constants. As a concluding remark for this sketch of
proof  let us note that if instead of using the last arguments  we replace all the error terms appearing
in (12) by their upper bounds provided by (10)  we do not get the optimal rate.

3 The case of Gaussian design

Our main result  Theorem 1  shows that if the design matrix satisﬁes the transfer principle and the
incoherence property with suitable constants  then the (cid:96)1-penalized Huber’s M-estimator achieves
the optimal rate under adversarial contamination. As a concrete example of a design matrix for which
the aforementioned conditions are satisﬁed  we consider the case of correlated Gaussian design. As
opposed to most of prior work on robust estimation for linear regression with Gaussian design  we
allow the covariance matrix to have a non degenerate null space. We will simply assume that the
n rows of the matrix X are independently drawn from the Gaussian distribution Np(0  Σ) with a
covariance matrix Σ satisfying the RE(s  5) condition. We will also assume in this section that all the
diagonal entries of Σ are equal to 1: Σjj = 1. The more formal statements of the results  provided in
the supplementary material  do not require this condition.
Theorem 2. Let δ ∈ (0  1/7) be a tolerance level and n ≥ 100. For every positive semi-deﬁnite
matrix Σ with all the diagonal entries bounded by one  with probability at least 1 − 2δ  the matrix X
satisﬁes the TPΣ(a1  a2)  the IPΣ(b1  b2  b3) and the ATPΣ(c1  c2  c3) with constants

√

a1 = 1 − 4.3 +(cid:112)2 log(9/δ)
2 +(cid:112)2 log(81/δ)
− 17.5 + 9.6(cid:112)2 log(2/δ)

√
4.8

b1 =

√

 

 

c1 =

n

n

√

n

3
4

(cid:114)

(cid:114)

2 log p

n

 

2 log n

(cid:114)

n

2 log p

n

a2 = b2 = 1.2

b3 = 1.2

 

c2 = 3.6

(cid:114)

 

c3 = 2.4

2 log n

n

.

The proof of this result is provided in the supplementary material. It relies on by now standard tools
such as Gordon’s comparison inequality  Gaussian concentration inequality and the peeling argument.
Note that the TPΣ and related results have been obtained in Raskutti et al. (2010); Oliveira (2016);
Rudelson and Zhou (2013). The IPΣ is basically a combination of a high probability version of
Chevet’s inequality (Vershynin  2018  Exercises 8.7.3-4) and the peeling argument. A property similar
to the ATPΣ for Gaussian matrices with non degenerate covariance was established in (Nguyen and
Tran  2013  Lemma 1) under further restrictions on n  p  s  o.
Theorem 3. There exist universal positive constants d1  d2  d3 such that if

then  with probability at least 1 − 4δ  (cid:96)1-penalized Huber’s M-estimator with λ2
and λ2

sn = 9σ2 log(p/δ)

s log p

κ2 + o log n ≤ d1n
(cid:13)(cid:13)Σ1/2((cid:98)β − β

)(cid:13)(cid:13)2 ≤ d3σ

on = 8σ2 log(n/δ) satisﬁes
∗

(cid:26)(cid:16) s log(p/δ)

(cid:17)1/2

nκ2

+

o log(n/δ)

n

.

(13)

and

1/7 ≥ δ ≥ 2e−d2n

(cid:27)

Even though the constants appearing in Theorem 2 are reasonably small and smaller than in the
analogous results in prior work  the constants d1  d2 and d3 are large  too large for being of any
practical relevance. Finally  let us note that if s and o are known  it is very likely that following the
techniques developed in (Bellec et al.  2018  Theorem 4.2)  one can replace the terms log(p/δ) and
log(n/δ) in (13) by log(p/sδ) and log(n/oδ)  respectively.

5

179
180
181

182
183
184
185
186
187
188
189
190
191
192
193
194

195

196

197

198
199
200
201
202
203
204
205
206
207
208

209

210
211
212
213
214
215

216
217
218
219
220
221
222
223
224
225
226
227
228

229
230
231
232

Comparing Theorem 3 with (Nguyen and Tran  2013  Theorem 1)  we see that our rate improvement
is not only in terms of its dependence on the proportion of outliers  o/n  but also in terms of the
condition number κ  which is now completely decoupled from o in the risk bound.
While our main focus is on the high dimensional situation in which p can be larger than n  it also
applies to the case of small dimensional dense vectors  i.e.  when s = p is signiﬁcantly smaller than
n. One of the applications of such a setting is the problem of stylized communication considered  for
∗ ∈ Rp to a remote
instance  in (Candès and Randall  2008). The problem is to transmit a signal β
∗ corrupted by small noise
receiver. What the receiver gets is a linearly transformed codeword Xβ
and malicious errors. While all the entries of the received codeword are affected by noise  only a
fraction of them is corrupted by malicious errors  corresponding to outliers. The receiver has access
∗ as well as to the encoding matrix X. Theorem 3.1 from (Candès and
to the corrupted version of Xβ
Randall  2008) establishes that the Dantzig selector (Candès and Tao  2007)  for a properly chosen
tuning parameter proportional to the noise level  achieves the (sub-optimal) rate σ2(s + o)/n  up to
a logarithmic factor. A similar result  with a noise-level-free version of the Dantzig selector  was
proved in (Dalalyan and Chen  2012). Our Theorem 3 implies that the error of the (cid:96)1-penalized
Huber’s estimator goes to zero at the faster rate σ2{(s/n) + (o/n)2}.
Finally  one can deduce from Theorem 3 that as soon as the number of outliers satisﬁes o =

o((cid:112)sn/κ2)  the rate of convergence remains the same as in the outlier-free setting.

4 Prior work

As attested by early references such as (Tukey  1960)  robust estimation has a long history. A
remarkable—by now classic—result by Huber (1964) shows that among all the shift invariant M-
estimators of a location parameter  the one that minimizes the asymptotic variance corresponds to the
loss function φ(x) = 1/2{x2 ∧ (2x − 1)}. This result was proved in the case when the reference
distribution is univariate Gaussian. Apart from some exceptions  such as (Yatracos  1985)  during
several decades the literature on robust estimation was mainly exploring the notions of breakdown
point  inﬂuence function  asymptotic efﬁciency  etc.  see for instance (Donoho and Gasko  1992;
Hampel et al.  2005; Huber and Ronchetti  2009) and the recent survey (Yu and Yao  2017). A more
recent trend in statistics is to focus on ﬁnite sample risk bounds that are minimax-rate-optimal when
the sample size n  the dimension p of the unknown parameter and the number o of outliers tend
jointly to inﬁnity (Chen et al.  2018  2016; Gao  2017).
In the problem of estimating the mean of a multivariate Gaussian distribution  it was shown that
the optimal rate of the estimation error measured in Euclidean norm scales as (p/n)1/2 + (o/n).
Similar results were established for the problem of robust linear regression as well. However  the
estimator that was shown to achieve this rate under fairly general conditions on the design is based
on minimizing regression depths  which is a hard computational problem. Several alternative robust
estimators with polynomial complexity were proposed (Diakonikolas et al.  2016; Lai et al.  2016;
Cheng et al.  2019; Collier and Dalalyan  2017; Diakonikolas et al.  2018).
Many recent papers studied robust linear regression. (Karmalkar and Price  2018) considered (cid:96)1-
constrained minimization of the (cid:96)1-norm of residuals and found a sharp threshold on the proportion
of outliers determining whether the error of estimation tends to zero or not  when the noise level goes
to zero. From a methodological point of view  (cid:96)1-penalized Huber’s estimator has been considered
in (She and Owen  2011; Lee et al.  2012). These papers contain also comprehensive empirical
evaluation and proposals for data-driven choice of tuning parameters. Robust sparse regression with
an emphasis on contaminated design was investigated in (Chen et al.  2013; Balakrishnan et al.  2017;
Diakonikolas et al.  2019; Liu et al.  2018  2019). Iterative and adaptive hard thresholding approaches
were considered in (Bhatia et al.  2017; Suggala et al.  2019). Methods based on penalizing the vector
of outliers were studied by Li (2013); Foygel and Mackey (2014); Adcock et al. (2018)  who adopted
a more signal-processing point of view in which the noise vector is known to have a small (cid:96)2 norm
and nothing else is known about it. We should stress that our proof techniques share many common
features with those in (Foygel and Mackey  2014).
The problem of robust estimation of graphical models  closely related to the present work  was
addressed in (Balmand and Dalalyan  2015; Katiyar et al.  2019; Liu et al.  2019). Quite surprisingly 
at least to us  the minimax rate of robust estimation of the precision matrix in Frobenius norm is not
known yet.

6

5 Extensions

The results presented in previous sections pave the way for some future investigations  that are
discussed below. None of these extensions is carried out in this work  they are listed here as possible
avenues for future research.

240

241
242

i   y◦

239 {(X i  yi); i = 1  . . .   n} such that (X i  yi) = (X◦
i   y◦
i = (yi − X(cid:62)
i β

Contaminated design In addition to labels  the features also might be corrupted by outliers.
This is the case  for instance  in Gaussian graphical models. Formally  this means that instead of
i ); i = 1  . . .   n} satisfying y◦
observing the clean data {(X◦
i = (X◦
i )(cid:62)β∗ + ξi  we observe
i ) for all i except for a fraction of outliers
√
i ∈ O. In such a setting  we can set θ∗
∗ − ξi)/
n and recover exactly the same
model as in (1).
The important difference as compared to the setting investigated in previous section is that it is
not reasonable anymore to assume that the feature vectors {X i : i ∈ O} are iid Gaussian. In the
adversarial setting  they may even be correlated with the noise vector ξ. It is then natural to remove

all the observations for which maxj |X ij| > (cid:112)2 log np/δ and to assume  that the (cid:96)1-penalized
Huber estimator is applied to data for which maxij |X ij| ≤(cid:112)2 log np/δ. This implies that λ can

243
244
245

246

be chosen of the order of4 σ ˜O(n−1/2 + (o/n))  which is an upper bound on (cid:107)X(cid:62)ξ(cid:107)∞/n.
In addition  TPΣ is clearly satisﬁed since it is satisﬁed for the submatrix XOc and (cid:107)Xv(cid:107)2 ≥
249 (cid:107)XOc v(cid:107)2. As for the IPΣ  we know from Theorem 2 that XOc satisﬁes IPΣ with constants b1  b2 
b3 of order ˜O(n−1/2). On the other hand 

OXOv| ≤ (cid:107)X(cid:107)∞(cid:107)uO(cid:107)1(cid:107)v(cid:107)1 ≤(cid:112)2o log(np/δ)(cid:107)uO(cid:107)2(cid:107)v(cid:107)1.
(cid:26)(cid:114) s

This implies that X satisﬁes IPΣ with b1 = ˜O(n−1/2)  b2 = ˜O((o/n)1/2) and b3 = ˜O(n−1/2).
Applying Theorem 1  we obtain that if (so + o2) log(np) ≤ cn for a sufﬁciently small constant c > 0 
then with high probability

(cid:26)(cid:114) s

(cid:114) o

(cid:27)

(cid:27)

|u(cid:62)

(cid:17)

247

248

250

251

o

s

√

(cid:107)Σ1/2((cid:98)β − β

∗

)(cid:107)2 = σ ˜O

(s + o)

= σO

√

n

+

n

+

n

(cid:16) 1√

+

n

o
n

233

234
235
236

237
238

252
253
254

255
256
257

258
259
260
261

262
263

264
265
266
267
268
269

270
271
272
273
274
275
276
277

+

n

o3
n

.

estimator(cid:98)β has the advantage of being independent of the covariance matrix Σ and on the sparsity s.

This rate of convergence appear to be slower than those obtained by methods tailored to deal with
corruption in design  see (Liu et al.  2018  2019) and the references therein. Using more careful
analysis  this rate might be improvable. On the positive side  unlike many of its competitors  the
Furthermore  the upper bound does not depend  even logarithmically  on (cid:107)β
∗(cid:107)2. Finally  if o3 ≤ sn 
our bound yields the minimax-optimal rate. To the best of our knowledge  none of the previously
studied robust estimators has such a property.

Sub-Gaussian design The proof of Theorem 2 makes use of some results  such as Gordon-Sudakov-
Fernique or Gaussian concentration inequality  which are speciﬁc to the Gaussian distribution. A
natural question is whether the rate σ{( s log(p/s)
n} can be obtained for more general design
distributions. In the case of a sub-Gaussian design with the scale- parameter 1  it should be possible
to adapt the methodology developed in this work to show that the TPΣ and the IPΣ are satisﬁed
with high-probability. Indeed  for proving the IPΣ  it is possible to replace Gordon’s comparison
inequality by Talagrand’s sub-Gaussian comparison inequality (Vershynin  2018  Cor. 8.6.2). The
Gaussian concentration inequality can be replaced by generic chaining.

)1/2 + o

n

Heavier tailed noise distributions For simplicity  we assumed in the paper that the random
variables ξi are drawn from a Gaussian distribution. As usual for the Lasso analysis  all the results
extend to the case of sub-Gaussian noise  see (Koltchinskii  2011). Indeed  we only need to control
tail probabilities of the random variable (cid:107)X(cid:62)ξ(cid:107)∞ and (cid:107)ξ(cid:107)∞  which can be done using standard tools.
We believe that it is possible to extend our results beyond sub-Gaussian noise  by assuming some
type of heavy-tailed distributions. The rationale behind this is that any random variable ξ can be
written (in many different ways) as a sum of a sub-Gaussian variable ξnoise and a “sparse” variable
ξout. By “sparse” we mean that ξout takes the value 0 with high probability. The most naive way for

4We use notation an = ˜O(bn) as a shorthand for an ≤ Cbn logc n for some C  c > 0 and for every n.

7

getting such a decomposition is to set ξnoise = ξ1(|ξ| < τ ) and ξout = ξ1(|ξ| ≥ τ ). The random
noise terms ξout
can be merged with θi and considered as outliers. We hope that this approach can
establish a connection between two types of robustness: robustness to outliers considered in this work
and robustness to heavy tails considered in many recent papers (Devroye et al.  2016; Catoni  2012;
Minsker  2018; Lugosi and Mendelson  2019; Lecué and Lerasle  2017).

i

278
279
280
281
282

283

6 Numerical illustration

284
285
286
287
288
289

290

We performed a synthetic experiment to illustrate the obtained theoretical result and to check that it
is in line with numerical results. We chose n = 1000 and p = 100 for 3 different levels of sparsity
∗ was set to have its ﬁrst s non-zero coordinates
s = 5  15  25. The noise variance was set to 1 and β
equal to 10. Each corrupted response coordinate was θ∗
j = 10. The fraction  = o/n of outliers
was ranging between 0 and 0.25 with a step-size of 5 for the number of outliers o is used. The
MSE was computed using 200 independent repetitions. The optimisation problem in (3) was solved

using the glmnet package with the tuning parameters λs = λo =(cid:112)(8/n)(log(p/s) + log(n/o)).

291
292
293

The obtained plots clearly demonstrate that there is a linear dependence on ε of the square-root of the
mean squared error. The R-notebook of this experiment can be found in the supplementary material.

294

7 Conclusion

295
296
297
298
299
300
301
302

303
304
305
306
307
308
309
310
311
312

We provided the ﬁrst proof of the rate-optimality—up to logarithmic terms that can be avoided—
of (cid:96)1-penalized Huber’s M-estimator in the setting of robust linear regression with adversarial
contamination. We established this result under the assumption that the design is Gaussian with a
covariance matrix Σ that need not be invertible. The condition number governing the risk bound is the
ratio of the largest diagonal entry of Σ and its restricted eigenvalue. Thus  in addition to improving
the rate of convergence  we also relaxed the assumptions on the design. Furthermore  we outlined
some possible extensions  namely to corrupted design and/or sub-Gaussian design  which seem to be
fairly easy to carry out building on the current work.
Next on our agenda is the more thorough analysis of the robust estimation by (cid:96)1-penalization in the
case of contaminated design. A possible approach  complementary to the one described in Section 5
above  is to adopt an errors-in-variables point of view similar to that developed in (Belloni et al.  2016).
Another interesting avenue for future research is the development of scale-invariant robust estimators
and their adaptation to the Gaussian graphical models. This can be done using methodology brought
forward in (Sun and Zhang  2013; Balmand and Dalalyan  2015). Finally  we would like to better
understand what is the largest fraction of outliers for which the (cid:96)1-penalized Huber’s M-estimator
has a risk—measured in Euclidean norm—upper bounded by σo/n. Answering this question even
under stringent assumptions of independent standard Gaussian design X ij with (s log p)/n going to
zero as n tends to inﬁnity would be of interest.

8

fractionofoutliers "00.050.10.150.20.25sqrtoftheMSE01020304050s=5s=15s=25313

314
315
316

317
318
319

320
321

322
323

324
325

326
327

328
329
330

331
332

333
334

335
336

337
338

339
340

341
342

343
344

345
346
347

348
349
350

351
352

353
354
355

356
357

References
Adcock  B.  Bao  A.  Jakeman  J.  and Narayan  A. (2018). Compressed sensing with sparse
corruptions: Fault-tolerant sparse collocation approximations. SIAM/ASA Journal on Uncertainty
Quantiﬁcation  6(4):1424–1453.

Balakrishnan  S.  Du  S. S.  Li  J.  and Singh  A. (2017). Computationally efﬁcient robust sparse
estimation in high dimensions. Proceedings of the 2017 Conference on Learning Theory  PMLR 
65:169–212.

Balmand  S. and Dalalyan  A. S. (2015). Convex programming approach to robust estimation of a

multivariate gaussian model. arXiv. 1512.04734.

Bellec  P. C. (2017). Localized Gaussian width of $M$-convex hulls with applications to Lasso and

convex aggregation. arXiv e-prints  page arXiv:1705.10696.

Bellec  P. C.  Lecué  G.  and Tsybakov  A. B. (2018). Slope meets lasso: Improved oracle bounds and

optimality. Ann. Statist.  46(6B):3603–3642.

Belloni  A.  Rosenbaum  M.  and Tsybakov  A. B. (2016). An {(cid:96)1  (cid:96)2  (cid:96)∞}-regularization approach

to high-dimensional errors-in-variables models. Electron. J. Statist.  10(2):1729–1750.

Bhatia  K.  Jain  P.  Kamalaruban  P.  and Kar  P. (2017). Consistent robust regression. In Advances in
Neural Information Processing Systems 30: Annual Conference on Neural Information Processing
Systems 2017  4-9 December 2017  Long Beach  CA  USA  pages 2107–2116.

Bickel  P. J.  Ritov  Y.  and Tsybakov  A. B. (2009). Simultaneous analysis of Lasso and Dantzig

selector. Ann. Statist.  37(4):1705–1732.

Boucheron  S.  Lugosi  G.  and Massart  P. (2013). Concentration inequalities: a nonasymptotic

theory of independence. Oxford University Press.

Candès  E. and Randall  P. A. (2008). Highly robust error correction by convex programming. IEEE

Trans. Inform. Theory  54(7):2829–2840.

Candès  E. and Tao  T. (2007). The Dantzig selector: statistical estimation when p is much larger

than n. Ann. Statist.  35(6):2313–2351.

Catoni  O. (2012). Challenging the empirical mean and empirical variance: a deviation study. Ann.

Inst. Henri Poincaré Probab. Stat.  48(4):1148–1185.

Chen  M.  Gao  C.  and Ren  Z. (2016). A general decision theory for Huber’s -contamination model.

Electron. J. Statist.  10(2):3752–3774.

Chen  M.  Gao  C.  and Ren  Z. (2018). Robust covariance and scatter matrix estimation under

Huber’s contamination model. Ann. Statist.  46(5):1932–1960.

Chen  Y.  Caramanis  C.  and Mannor  S. (2013). Robust sparse regression under adversarial
corruption. In Proceedings of the 30th International Conference on Machine Learning  volume 28
of Proceedings of Machine Learning Research  pages 774–782. PMLR.

Cheng  Y.  Diakonikolas  I.  and Ge  R. (2019). High-dimensional robust mean estimation in nearly-
linear time. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms 
SODA 2019  San Diego  California  USA  January 6-9  2019  pages 2755–2771.

Collier  O. and Dalalyan  A. S. (2017). Minimax estimation of a p-dimensional linear functional in
sparse Gaussian models and robust estimation of the mean. arXiv e-prints  page arXiv:1712.05495.

Dalalyan  A. S. and Chen  Y. (2012). Fused sparsity and robust estimation for linear models with
unknown variance. In Advances in Neural Information Processing Systems 25: NIPS  pages
1268–1276.

Devroye  L.  Lerasle  M.  Lugosi  G.  and Oliveira  R. I. (2016). Sub-Gaussian mean estimators. Ann.

Statist.  44(6):2695–2725.

9

358
359
360

361
362
363
364

365
366
367

368
369

370
371

372
373

374
375

376
377
378

379

380
381

382
383

384
385

386
387
388

389
390
391

392
393
394

395
396

397
398

399
400

401
402

Diakonikolas  I.  Kamath  G.  Kane  D. M.  Li  J.  Moitra  A.  and Stewart  A. (2016). Robust
estimators in high dimensions without the computational intractability. In Foundations of Computer
Science (FOCS)  2016 IEEE 57th Annual Symposium on  pages 655–664. IEEE.

Diakonikolas  I.  Kamath  G.  Kane  D. M.  Li  J.  Moitra  A.  and Stewart  A. (2018). Robustly
learning a gaussian: Getting optimal error  efﬁciently. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms  SODA 2018  New Orleans  LA  USA  January
7-10  2018  pages 2683–2702.

Diakonikolas  I.  Kong  W.  and Stewart  A. (2019). Efﬁcient algorithms and lower bounds for robust
linear regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete
Algorithms  SODA 2019  San Diego  California  USA  January 6-9  2019  pages 2745–2754.

Donoho  D. and Montanari  A. (2016). High dimensional robust m-estimation: asymptotic variance

via approximate message passing. Probability Theory and Related Fields  166(3):935–969.

Donoho  D. L. and Gasko  M. (1992). Breakdown properties of location estimates based on halfspace

depth and projected outlyingness. Ann. Statist.  20(4):1803–1827.

Foygel  R. and Mackey  L. (2014). Corrupted sensing: novel guarantees for separating structured

signals. IEEE Trans. Inform. Theory  60(2):1223–1247.

Gao  C. (2017). Robust Regression via Mutivariate Regression Depth. arXiv e-prints  page

arXiv:1702.04656.

Hampel  F.  Ronchetti  E.  Rousseeuw  P.  and Stahel  W. (2005). Robust statistics: the approach
based on inﬂuence functions. Wiley series in probability and mathematical statistics. Probability
and mathematical statistics. Wiley.

Huber  P. J. (1964). Robust estimation of a location parameter. Ann. Math. Statist.  35(1):73–101.

Huber  P. J. and Ronchetti  E. M. (2009). Robust statistics. Wiley Series in Probability and Statistics.

John Wiley & Sons  Inc.  Hoboken  NJ  second edition.

Karmalkar  S. and Price  E. (2018). Compressed sensing with adversarial sparse noise via l1 regression.

arXiv. 1809.08055.

Katiyar  A.  Hoffmann  J.  and Caramanis  C. (2019). Robust estimation of tree structured Gaussian

Graphical Model. arXiv e-prints  page arXiv:1901.08770.

Koltchinskii  V. (2011). Oracle Inequalities in Empirical Risk Minimization and Sparse Recov-
ery Problems: École d’Été de Probabilités de Saint-Flour XXXVIII-2008. Lecture Notes in
Mathematics. Springer Berlin Heidelberg.

Lai  K. A.  Rao  A. B.  and Vempala  S. (2016). Agnostic estimation of mean and covariance. In
Foundations of Computer Science (FOCS)  2016 IEEE 57th Annual Symposium on  pages 665–674.
IEEE.

Laska  J. N.  Davenport  M. A.  and Baraniuk  R. G. (2009). Exact signal recovery from sparsely
corrupted measurements through the pursuit of justice. In Asilomar Conference on Signals  Systems
and Computers  pages 1556–1560.

Lecué  G. and Lerasle  M. (2017). Robust machine learning by median-of-means : theory and practice.

arXiv e-prints  page arXiv:1711.10306.

Lee  Y.  MacEachern  S. N.  and Jung  Y. (2012). Regularization of case-speciﬁc parameters for

robustness and efﬁciency. Statist. Sci.  27(3):350–372.

Li  X. (2013). Compressed sensing and matrix completion with constant proportion of corruptions.

Constructive Approximation  37(1):73–99.

Liu  L.  Li  T.  and Caramanis  C. (2019). High dimensional robust estimation of sparse models via

trimmed hard thresholding. CoRR  abs/1901.08237.

10

403
404

405
406

407
408

409
410

411
412

413
414

415
416

417
418

419
420

421
422

423
424

425
426

427
428
429

430
431

432
433

Liu  L.  Shen  Y.  Li  T.  and Caramanis  C. (2018). High dimensional robust sparse regression. CoRR 

abs/1805.11643.

Lugosi  G. and Mendelson  S. (2019). Sub-Gaussian estimators of the mean of a random vector. Ann.

Statist.  47(2):783–794.

Minsker  S. (2018). Sub-Gaussian estimators of the mean of a random matrix with heavy-tailed

entries. Ann. Statist.  46(6A):2871–2903.

Nguyen  N. H. and Tran  T. D. (2013). Robust lasso with missing and grossly corrupted observations.

IEEE Trans. Inform. Theory  59(4):2036–2058.

Oliveira  R. (2013). The lower tail of random quadratic forms  with applications to ordinary least

squares and restricted eigenvalue properties. arXiv. 1312.2903.

Oliveira  R. (2016). The lower tail of random quadratic forms with applications to ordinary least

squares. Probability Theory and Related Fields  166(3-4):1175–1194.

Raskutti  G.  Wainwright  M. J.  and Yu  B. (2010). Restricted eigenvalue properties for correlated

Gaussian designs. J. Mach. Learn. Res.  11:2241–2259.

Rudelson  M. and Zhou  S. (2013). Reconstruction from anisotropic random measurements. IEEE

Trans. Inf. Theory  59(6):3434–3447.

She  Y. and Owen  A. B. (2011). Outlier detection using nonconvex penalized regression. Journal of

the American Statistical Association  106(494):626–639.

Suggala  A. S.  Bhatia  K.  Ravikumar  P.  and Jain  P. (2019). Adaptive hard thresholding for

near-optimal consistent robust regression. CoRR  abs/1903.08192.

Sun  T. and Zhang  C.-H. (2013). Sparse matrix inversion with scaled lasso. Journal of Machine

Learning Research  14:3385–3418.

Tukey  J. W. (1960). A survey of sampling from contaminated distributions. Contributions to

Probability and Statistics.

Vershynin  R. (2018). High-dimensional probability  volume 47 of Cambridge Series in Statistical
and Probabilistic Mathematics. Cambridge University Press  Cambridge. An introduction with
applications in data science  With a foreword by Sara van de Geer.

Yatracos  Y. G. (1985). Rates of convergence of minimum distance estimators and kolmogorov’s

entropy. Ann. Statist.  13(2):768–774.

Yu  C. and Yao  W. (2017). Robust linear regression: a review and comparison. Comm. Statist.

Simulation Comput.  46(8):6261–6282.

11

,Arnak Dalalyan
Philip Thompson