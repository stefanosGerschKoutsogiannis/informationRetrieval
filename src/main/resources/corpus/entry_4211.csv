2014,Parallel Direction Method of Multipliers,We consider the problem of minimizing block-separable convex functions subject to linear constraints. While the Alternating Direction Method of Multipliers (ADMM) for two-block linear constraints has been intensively studied both theoretically and empirically  in spite of some preliminary work  effective generalizations of ADMM to multiple blocks is still unclear. In this paper  we propose a parallel randomized block coordinate method named Parallel Direction Method of Multipliers (PDMM) to solve the optimization problems with multi-block linear constraints. PDMM randomly updates some blocks in parallel  behaving like parallel randomized block coordinate descent. We establish the global convergence and the iteration complexity for PDMM with constant step size. We also show that PDMM can do randomized block coordinate descent on overlapping blocks. Experimental results show that PDMM performs better than state-of-the-arts methods in two applications  robust principal component analysis and overlapping group lasso.,Parallel Direction Method of Multipliers

Huahua Wang   Arindam Banerjee   Zhi-Quan Luo

{huwang banerjee}@cs.umn.edu  luozq@umn.edu

University of Minnesota  Twin Cities

Abstract

We consider the problem of minimizing block-separable (non-smooth) convex
functions subject to linear constraints. While the Alternating Direction Method of
Multipliers (ADMM) for two-block linear constraints has been intensively studied
both theoretically and empirically  in spite of some preliminary work  effective
generalizations of ADMM to multiple blocks is still unclear. In this paper  we
propose a parallel randomized block coordinate method named Parallel Direction
Method of Multipliers (PDMM) to solve optimization problems with multi-block
linear constraints. At each iteration  PDMM randomly updates some blocks in
parallel  behaving like parallel randomized block coordinate descent. We establish
the global convergence and the iteration complexity for PDMM with constant step
size. We also show that PDMM can do randomized block coordinate descent on
overlapping blocks. Experimental results show that PDMM performs better than
state-of-the-arts methods in two applications  robust principal component analysis
and overlapping group lasso.

1

Introduction

Ac

jxj = a  

(1)

In this paper  we consider the minimization of block-seperable convex functions subject to linear
constraints  with a canonical form:

J(cid:88)
j ∈ Rm×nj is the j-th column block of A ∈ Rm×n where n =(cid:80)

{xj∈Xj} f (x) =
min

fj(xj)   s.t. Ax =

J(cid:88)

j=1

j=1

where the objective function f (x) is a sum of J block separable (nonsmooth) convex functions 
j nj  xj ∈ Rnj×1 is the j-th
Ac
block coordinate of x  Xj is a local convex constraint of xj and a ∈ Rm×1. The canonical form
can be extended to handle linear inequalities by introducing slack variables  i.e.  writing Ax ≤ a as
Ax + z = a  z ≥ 0.
A variety of machine learning problems can be cast into the linearly-constrained optimization prob-
lem (1) [8  4  24  5  6  21  11]. For example  in robust Principal Component Analysis (RPCA) [5] 
one attempts to recover a low rank matrix L and a sparse matrix S from an observation matrix M 
i.e.  the linear constraint is M = L + S. Further  in the stable version of RPCA [29]  an noisy matrix
Z is taken into consideration  and the linear constraint has three blocks  i.e.  M = L + S + Z. Prob-
lem (1) can also include composite minimization problems which solve a sum of a loss function and
a set of nonsmooth regularization functions. Due to the increasing interest in structural sparsity [1] 
composite regularizers have become widely used  e.g.  overlapping group lasso [28]. As the blocks
are overlapping in this class of problems  it is difﬁcult to apply block coordinate descent methods
for large scale problems [16  18] which assume block-separable. By simply splitting blocks and in-
troducing equality constraints  the composite minimization problem can also formulated as (1) [2].
A classical approach to solving (1) is to relax the linear constraints using the (augmented) La-
grangian  i.e. 

Lρ(x  y) = f (x) + (cid:104)y  Ax − a(cid:105) +

(cid:107)Ax − a(cid:107)2
2  

ρ
2

(2)

1

where ρ ≥ 0 is called the penalty parameter. We call x the primal variable and y the dual variable. (2)
usually leads to primal-dual algorithms which update the primal and dual variables alternatively.
While the dual update is simply dual gradient descent  the primal update is to solve a minimization
problem of (2) given y. If ρ = 0  the primal update can be solved in a parallel block coordinate
fashion [3  19]  leading to the dual ascent method. While the dual ascent method can achieve mas-
sive parallelism  a careful choice of stepsize and some strict conditions are required for convergence 
particularly when f is nonsmooth. To achieve better numerical efﬁciency and convergence behavior
compared to the dual ascent method  it is favorable to set ρ > 0 in the augmented Lagrangian (2)
which we call the method of multipliers. However 
(2) is no longer separable and solving entire
augmented Lagrangian (2) exactly is computationally expensive. In [20]  randomized block coor-
dinate descent (RBCD) [16  18] is used to solve (2) exactly  but leading to a double-loop algorithm
along with the dual step. More recent results show (2) can be solved inexactly by just sweeping the
coordinates once using the alternating direction method of multipliers (ADMM) [12  2]. This paper
attempts to develop a parallel randomized block coordinate variant of ADMM.
When J = 2  ADMM has been widely used to solve the augmented Lagragian (2) in many ap-
plications [2]. Encouraged by the success of ADMM with two blocks  ADMM has also been ex-
tended to solve the problem with multiple blocks [15  14  10  17  13  7]. The variants of ADMM
can be mainly divided into two categories. The ﬁrst category considers Gauss-Seidel ADMM
(GSADMM) [15  14]  which solves (2) in a cyclic block coordinate manner. In [13]  a back sub-
stitution step was added so that the convergence of ADMM for multiple blocks can be proved. In
some cases  it has been shown that ADMM might not converge for multiple blocks [7]. In [14]  a
block successive upper bound minimization method of multipliers (BSUMM) is proposed to solve
the problem (1). The convergence of BSUMM is established under some fairly strict conditions: (i)
certain local error bounds hold; (ii) the step size is either sufﬁciently small or decreasing. However 
in general  Gauss-Seidel ADMM with multiple blocks is not well understood and its iteration com-
plexity is largely open. The second category considers Jacobian variants of ADMM [26  10  17] 
which solves (2) in a parallel block coordinate fashion. In [26  17]  (1) is solved by using two-block
ADMM with splitting variables (sADMM). [10] considers a proximal Jacobian ADMM (PJADMM)
by adding proximal terms. A randomized block coordinate variant of ADMM named RBSUMM
was proposed in [14]. However  RBSUMM can only randomly update one block. Moreover  the
convergence of RBSUMM is established under the same conditions as BSUMM and its iteration
complexity is unknown.
In this paper  we propose a parallel randomized block coordinate method named parallel direction
method of multipliers (PDMM) which randomly picks up any number of blocks to update in parallel 
behaving like randomized block coordinate descent [16  18]. Like the dual ascent method  PDMM
solves the primal update in a parallel block coordinate fashion even with the augmentation term.
Moreover  PDMM inherits the merits of the method of multipliers and can solve a fairly large class
of problems  including nonsmooth functions. Technically  PDMM has three aspects which make it
distinct from such state-of-the-art methods. First  if block coordinates of the primal x is solved ex-
actly  PDMM uses a backward step on the dual update so that the dual variable makes conservative
progress. Second  the sparsity of A and the number of randomized blocks are taken into consider-
ation to determine the step size of the dual update. Third  PDMM can randomly update arbitrary
number of primal blocks in parallel. Moreover  we show that sADMM and PJADMM are the two ex-
treme cases of PDMM. The connection between sADMM and PJADMM through PDMM provides
better understanding of dual backward step. PDMM can also be used to solve overlapping groups in
a randomized block coordinate fashion. Interestingly  the corresponding problem for RBCD [16  18]
with overlapping blocks is still an open problem. We establish the global convergence and O(1/T )
iteration complexity of PDMM with constant step size. We evaluate the performance of PDMM in
two applications: robust principal component analysis and overlapping group lasso.
The rest of the paper is organized as follows: We introduce PDMM in Section 2  and establish
convergence results in Section 3. We evaluate the performance of PDMM in Section 4 and conclude
in Section 5. The technical analysis and detailed proofs are provided in the supplement.
Notations: Assume that A ∈ Rm×n is divided into I × J blocks. Let Ar
i ∈ Rmi×n be the i-th row
j ∈ Rm×nj be the j-th column block of A  and Aij ∈ Rmi×nj be the ij-th block of
block of A  Ac
A. Let yi ∈ Rmi×1 be the i-th block of y ∈ Rm×1. Let N (i) be a set of nonzero blocks Aij in the

2

i and di = |N (i)| be the number of nonzero blocks. Let ˜Ki = min{di  K} where

i-th row block Ar
K is the number of blocks randomly chosen by PDMM and T be the number of iterations.
2 Parallel Direction Method of Multipliers
Consider a direct Jacobi version of ADMM which updates all blocks in parallel:

k(cid:54)=j  yt)  

xt+1
j = argminxj∈Xj Lρ(xj  xt
yt+1 = yt + τ ρ(Axt+1 − a) .

(3)
(4)
where τ is a shrinkage factor for the step size of the dual gradient ascent update. However  empirical
results show that it is almost impossible to make the direct Jacobi updates (3)-(4) to converge even
when τ is extremely small. [15  10] also noticed that the direct Jacobi updates may not converge.
To address the problem in (3) and (4)  we propose a backward step on the dual update. Moreover 
instead of updating all blocks  the blocks xj will be updated in a parallel randomized block coordi-
nate fashion. We call the algorithm Parallel Direction Method of Multipliers (PDMM). PDMM ﬁrst
randomly select K blocks denoted by set Jt at time t  then executes the following iterates:

  ˆyt) + ηjt Bφjt

(xjt  xt
jt

)   jt ∈ Jt 

k(cid:54)=jt

xt+1

jt

Lρ(xjt  xt

= argmin
xjt∈Xjt
i + τiρ(Aixt+1 − ai)  
i − νiρ(Aixt+1 − ai)  

yt+1
i = yt
ˆyt+1
i = yt+1

(5)

(6)
(7)

where τi > 0  0 ≤ νi < 1  ηjt ≥ 0  and Bφjt
) is a Bregman divergence. Note xt+1 =
(xt+1Jt
) in (6) and (7). (6) and (7) update all dual blocks. We show that PDMM can also do
randomized dual block coordinate ascent in an extended work [25]. Let ˜Ki = min{di  K}. τi and
νi can take the following values:

(xjt  xt
jt

k /∈Jt

  xt

K

τi =

˜Ki(2J − K)

  νi = 1 − 1
˜Ki

.

(8)

In the xjt-update (5)  a Bregman divergence is addded so that exact PDMM and its inexact variants
can be analyzed in an uniﬁed framework [23  11]. In particular  if ηjt = 0  (5) is an exact update. If
ηjt > 0  by choosing a suitable Bregman divergence  (5) can be solved by various inexact updates 
often yielding a closed-form for the xjt update (see Section 2.1).
To better understand PDMM  we discuss the following three aspects which play roles in choosing τi
and νi: the dual backward step (7)  the sparsity of A  and the choice of randomized blocks.
Dual Backward Step: We attribute the failure of the Jacobi updates (3)-(4) to the following obser-
vation in (3)  which can be rewritten as:

j = argminxj∈Xj fj(xj) + (cid:104)yt + ρ(Axt − a)  Ac
xt+1

jxj(cid:105) +

ρ
2

(cid:107)Ac

j(xj − xt

j)(cid:107)2
2 .

(9)

In the primal xj update  the quadratic penalty term implicitly adds full gradient ascent step to the
dual variable  i.e.  yt+ρ(Axt−a)  which we call implicit dual ascent. The implicit dual ascent along
with the explicit dual ascent (4) may lead to too aggressive progress on the dual variable  particularly
when the number of blocks is large. Based on this observation  we introduce an intermediate variable
ˆyt to replace yt in (9) so that the implicit dual ascent in (9) makes conservative progress  e.g. 
ˆyt + ρ(Axt − a) = yt + (1 − ν)ρ(Axt − a)   where 0 < ν < 1. ˆyt is the result of a ‘backward
step’ on the dual variable  i.e.  ˆyt = yt − νρ(Axt − a).
Moreover  one can show that τ and ν have also been implicitly used when using two-block ADMM
with splitting variables (sADMM) to solve (1) [17  26]. Section 2.2 shows sADMM is a special case
of PDMM. The connection helps in understanding the role of the two parameters τi  νi in PDMM.
Interestingly  the step sizes τi and νi can be improved by considering the block sparsity of A and
the number of random blocks K to be updated.
Sparsity of A: Assume A is divided into I × J blocks. While xj can be updated in parallel 
the matrix multiplication Ax in the dual update (4) requires synchronization to gather messages
from all block coordinates jt ∈ Jt. For updating the i-th block of the dual yi  we need Aixt+1 =
k which aggregates “messages” from all xjt. If Aijt is a block of

+(cid:80)

Aijtxt+1

(cid:80)

Aikxt

jt∈Jt

k /∈Jt

jt

3

k /∈Jt

Aikxt

jt∈Jt∩N (i) Aijtxt+1
jt

zeros  there is no “message” from xjt to yi. More precisely  Aixt+1 =(cid:80)
(cid:80)

+
k where N (i) denotes a set of nonzero blocks in the i-th row block Ai. N (i) can be
considered as the set of neighbors of the i-th dual block yi and di = |N (i)| is the degree of the i-th
dual block yi. If A is sparse  di could be far smaller than J. According to (8)  a low di will lead to
bigger step sizes τi for the dual update and smaller step sizes for the dual backward step (7). Further 
as shown in Section 2.3  when using PDMM with all blocks to solve composite minimization with
overlapping blocks  PDMM can use τi = 0.5 which is much larger than 1/J in sADMM.
Randomized Blocks: The number of blocks to be randomly chosen also has the effect on τi  νi.
If randomly choosing one block (K = 1)  then νi = 0  τi = 1
2J−1. The dual backward step (7)
vanishes. As K increases  νi increases from 0 to 1 − 1
. If
updating all blocks (K = J)  τi = 1
di
PDMM does not necessarily choose any K combination of J blocks. The J blocks can be randomly
partitioned into J/K groups where each group has K blocks. Then PDMM randomly picks some
groups. A simple way is to permutate the J blocks and choose K blocks cyclically.

and τi increases from 1

  νi = 1 − 1

2J−1 to 1
di

di

di

.

Inexact PDMM

2.1
If ηjt > 0  there is an extra Bregman divergence term in (5)  which can serve two purposes. First 
choosing a suitable Bregman divergence can lead to an efﬁcient solution for (5). Second  if ηjt is
sufﬁciently large  the dual update can use a large step size (τi = 1) and the backward step (7) can be
removed (νi = 0)  leading to the same updates as PJADMM [10] (see Section 2.2).
Given a continuously differentiable and strictly convex function ψjt  its Bregman divergence is
deﬁend as

Bψjt

(xjt  xt
jt

)  xjt − xt

ψjt(xjt) − Bψjt

) = ψjt(xjt) − ψjt(xt

) − (cid:104)∇ψjt(xt
where ∇ψjt denotes the gradient of ψjt. Rearranging the terms yields
) + (cid:104)∇ψjt(xt
(11)
) = ψjt(xt
jt
which is exactly the linearization of ψjt(xjt) at xt
jt. Therefore  if solving (5) exactly becomes
difﬁcult due to some problematic terms  we can use the Bregman divergence to linearize these
problematic terms so that (5) can be solved efﬁciently. More speciﬁcally  in (5)  we can choose
φjt = ϕjt − 1
ψjt assuming ψjt is the problematic term. Using the linearity of Bregman diver-
gence 

)  xjt − xt

(xjt  xt
jt

(10)

(cid:105) 

(cid:105) 

ηjt

jt

jt

jt

jt

jt

Bφjt

(xjt  xt
jt

) = Bϕjt

(xjt  xt
jt

Bψjt

(xjt  xt
jt

) .

(12)

) − 1
ηjt

(cid:88)

2(cid:107)·(cid:107)2
(cid:104)∇fjt(xt

For instance  if fjt is a logistic function  solving (5) exactly requires an iterative algorithm. Setting
ψjt = fjt  ϕjt = 1

2 in (12) and plugging into (5) yield

jt

jt

ρ
2

Ac

k(cid:54)=jt

xt+1

Akxt

(cid:80)

k−a(cid:107)2

= argmin
xjt∈Xjt

(cid:107)Ajtxjt +

)  xjt(cid:105)+(cid:104)ˆyt  Ajtxjt(cid:105)+

which has a closed-form solution.

2 +ηjt(cid:107)xjt−xt
2(cid:107)Ac
xjt +
jt
xjt(cid:107)2
2 
then
)(cid:107)2
2 can be used to linearize the quadratic penalty term.
implies that Bϕjt

if the quadratic penalty term ρ
2(cid:107)Ac

k − a(cid:107)2
2 is a problematic term  we can set ψjt(xjt) = ρ
2(cid:107)Ac
) = ρ

(xjt − xt
Bψjt
jt
In (12)  the nonnegativeness of Bφjt
. This condition can be satisﬁed
as long as ϕjt is more convex than ψjt. Technically  we assume that ϕjt is σ/ηjt-strongly convex
and ψjt has Lipschitz continuous gradient with constant σ  which has been shown in [23].

kxt
(xjt  xt
jt

≥ 1
ηjt

Similarly 

(cid:107)2
2  

Bψjt

k(cid:54)=jt

jt

jt

jt

2.2 Connections to Related Work
Consider the case when all blocks are used in PDMM. There are also two other methods which
update all blocks in parallel. If solving the primal updates exactly  two-block ADMM with splitting
variables (sADMM) is considered in [17  26]. We show that sADMM is a special case of PDMM
If the primal updates are solved
when setting τi = 1
inexactly  [10] considers a proximal Jacobian ADMM (PJADMM) by adding proximal terms where

J and νi = 1 − 1

J (Appendix B in [25]).

4

the converge rate is improved to o(1/T ) given the sufﬁciently large proximal terms. We show that
PJADMM [10] is also a special case of PDMM (Appendix C in [25]). sADMM and PJADMM are
two extreme cases of PDMM. The connection between sADMM and PJADMM through PDMM can
provide better understanding of the three methods and the role of dual backward step. If the primal
update is solved exactly which makes sufﬁcient progress  the dual update should take small step  e.g. 
sADMM. On the other hand  if the primal update takes small progress by adding proximal terms 
the dual update can take full gradient step  e.g. PJADMM. While sADMM is a direct derivation of
ADMM  PJADMM introduces more terms and parameters.
In addition to PDMM  RBUSMM [14] can also randomly update one block. The convergence
of RBSUMM requires certain local error bounds to be hold and decreasing step size. Moreover 
the iteration complexity of RBSUMM is still unknown. In contast  PDMM converges at a rate of
O(1/T ) with the constant step size.

2.3 Randomized Overlapping Block Coordinate Descent
Consider the composite minimization problem of a sum of a loss function (cid:96)(w) and composite
regularizers gj(wj):

min

w

(cid:96)(w) +

gj(wj)  

(13)

which considers L overlapping groups wj ∈ Rb×1. Let J = L + 1  xJ = w. For 1 ≤ j ≤ L 
j xJ  where Uj ∈ Rb×L is the columns of an identity matrix and
denote xj = wj  then xj = UT
extracts the coordinates of xJ. Denote U = [U1 ···   UL] ∈ Rn×(bL) and A = [IbL −UT ] where
bL denotes b × L. By letting fj(xj) = gj(wj) and fJ (xJ ) = (cid:96)(w)  (13) can be written as:

fj(xj)

s.t. Ax = 0.

(14)

K

where x = [x1;··· ; xL; xL+1] ∈ Rb×J.
(14) can be solved by PDMM in a randomized block
coordinate fashion. In A  for b rows block  there are only two nonzero blocks  i.e.  di = 2. There-
fore  τi =
2(2J−K)   νi = 0.5. In particular  if K = J  τi = νi = 0.5. In contrast  sADMM uses
τi = 1/J (cid:28) 0.5  νi = 1 − 1/J > 0.5 if J is larger.
Remark 1 (a) ADMM [2] can solve (14) where the equality constraint is xj = UT
(b) In this setting  Gauss-Seidel ADMM (GSADMM) and BSUMM [14] are the same as ADMM.
BSUMM should converge with constant stepsize ρ (not necessarily sufﬁciently small)  although the
theory of BSUMM does not include this special case.

j xJ.

J(cid:88)

j=1

min

x

L(cid:88)

j=1

5

3 Theoretical Results
We establish the convergence results for PDMM under fairly simple assumptions:
Assumption 1
(1) fj : Rnj (cid:55)→ R ∪ {+∞} are closed  proper  and convex.
(2) A KKT point of the Lagrangian (ρ = 0 in (2)) of Problem (1) exists.
Assumption 1 is the same as that required by ADMM [2  22]. Assume that {x∗
the KKT conditions of the Lagrangian (ρ = 0 in (2))  i.e. 
j y∗ ∈ ∂fj(x∗

j )  

− AT
Ax∗ − a = 0.

j ∈ Xj  y∗

i } satisﬁes

(15)
(16)
) where ∂fj be the

j ∈ Xj  the optimality conditions for the xj update (5) is

j

j

During iterations  (16) is satisﬁed if Axt+1 = a. Let f(cid:48)
subdifferential of fj. For x∗
(cid:104)f(cid:48)
j(xt+1
When Axt+1 = a  yt+1 = yt. If Ac
assuming Bφj (xt+1

j[yt +(1−ν)ρ(Axt−a)+Ac
j(xt+1

j(xt+1
j − xt

j −xt

(15) will be satisﬁed. Note x∗

j) = 0 

)+Ac

  xt

j(xt+1

j

j

)−∇φj(xt

j)]+ηj(∇φj(xt+1

j(cid:105)≤ 0 .
j) = 0  then Axt − a = 0. When ηj ≥ 0  further
j ∈ Xj is always satisﬁed in (5) in

j −x∗

j))  xt+1

j

) ∈ ∂fj(xt+1

i) be generated by PDMM (5)-(7) and h(v∗  vt) be deﬁned in (20).

2

J(cid:88)

PDMM. Overall  the KKT conditions (15)-(16) are satisﬁed if the following optimality conditions
are satisﬁed by the iterates:

j − xt
j(xt+1
  xt
j) = 0 .

j

j) = 0  

i = [zT

Axt+1 = a   Ac
Bφj (xt+1

(17)
(18)
The above optimality conditions are sufﬁcient for the KKT conditions. (17) are the optimality con-
ditions for the exact PDMM. (18) is needed only when ηj > 0.
Let zij = Aijxj ∈ Rmi×1  zr
RJm×1. Deﬁne the residual of optimality conditions (17)-(18) as
i xt+1 − ai(cid:107)2

2 +
. If R(xt+1) → 0  (17)-(18) will be
where Pt is some positive semi-deﬁnite matrix and βi = K
J ˜Ki
satisﬁed and thus PDMM converges to the KKT point {x∗  y∗}. Deﬁne the current iterate vt =
(xt

iJ ]T ∈ RmiJ×1 and z = [(zr

i1 ···   zT
I(cid:88)

1)T  ···   (zr

(cid:107)zt+1 − zt(cid:107)2

I )T ]T ∈

ηjBφj (xt+1

i) and h(v∗  vt) as a distance from vt to a KKT point v∗ = (x∗
(cid:107)z∗ − zt(cid:107)2

j ∈ Xj  y∗
J(cid:88)
i ):
ηjBφj (x∗

h(v∗  vt) =

J(cid:88)

I(cid:88)

R(xt+1) =

βi(cid:107)Ar

j)   (20)

(cid:107)y∗

j  yt

Q +

(19)

j) .

  xt

ρ
2

ρ
2

j=1

i=1

+

Pt

1

j

where Q is a positive semi-deﬁnite matrix and ˜Lρ(xt  yt) with γi = 2(J−K)
˜Ki(2J−K)
(cid:107)Ar

˜Lρ(xt  yt) = f (xt) − f (x∗) +

(γi − τi)ρ

i xt − ai(cid:105) +

(cid:104)yt

ρ
2

i

2 + ˜Lρ(xt  yt) +
i − yt−1
(cid:107)2
(cid:26)
I(cid:88)

i xt − ai(cid:107)2

2

.

(21)

j   xt
− K
J ˜Ki

(cid:27)

K
J

2τiρ

i=1

j=1

+ 1
di

is

i  Ar
The following Lemma shows that h(v∗  vt) ≥ 0.
Lemma 1 Let vt = (xt
Setting νi = 1 − 1
˜Ki

j  yt
and τi =

  we have

i=1

K

I(cid:88)

˜Ki(2J−K)
ζi(cid:107)Ar
− K
J ˜Ki

i=1
+ 1
di

h(v∗  vt) ≥ ρ
2

2 +

(cid:107)z∗ − zt(cid:107)2

i xt − ai(cid:107)2
ηjBφj (x∗
≥ 0. Moreover  if h(v∗  vt) = 0  then Ar

Q +

ρ
2

j=1

j) ≥ 0 .

(22)

j   xt
i xt = ai  zt = z∗ and

j   xt

˜Ki(2J−K)

j) = 0. Thus  (15)-(16) are satisﬁed.

where ζi = J−K
Bφj (x∗
In PDMM  yt+1 depends on xt+1  which in turn depends on Jt. xt and yt are independent of Jt. xt
depends on the observed realizations of the random variable ξt−1 = {J1 ···   Jt−1} .The following
theorem shows that h(v∗  vt) decreases monotonically and thus establishes the global convergence
of PDMM.
Theorem 1 (Global Convergence) Let vt = (xt
(x∗
have

i) be generated by PDMM (5)-(7) and v∗ =
  we

i ) be a KKT point satisfying (15)-(16). Setting νi = 1 − 1

j ∈ Xj  y∗

˜Ki(2J−K)

and τi =

j  yt

˜Ki

K

0 ≤ Eξth(v∗  vt+1) ≤ Eξt−1 h(v∗  vt)   EξtR(xt+1) → 0 .

(23)

The following theorem establishes the iteration complexity of PDMM in an ergodic sense.
Theorem 2 (Iteration Complexity) Let (xt

i) be generated by PDMM (5)-(7). Let ¯xT =

(cid:80)T
t=1 xt. Setting νi = 1 − 1
I(cid:88)

Ef (¯xT ) − f (x∗) ≤

(cid:110)(cid:80)I

J
K

βi(cid:107)Ar

i ¯xT − ai(cid:107)2

2 ≤ 2

E

˜Ki

i=1

i=1

where βi = K
J ˜Ki

ρ h(v∗  v0)

.

T

j  yt
K

  we have
and τi =
˜Ki(2J−K)
2βiρ(cid:107)y∗
i (cid:107)2
2 + ˜Lρ(x1  y1) + ρ
T

1

  Q is a positive semi-deﬁnite matrix  and the expectation is over Jt.

6

Q +(cid:80)J

2(cid:107)z∗ − z1(cid:107)2

j=1 ηjBφj (x∗

j   x1
j )

(cid:111)

 

Figure 1: Comparison of the convergence of PDMM with ADMM methods in RPCA.

Table 1: The best results of PDMM with tuning parameters τi  νi in RPCA.

iteration

residual(×10−5)

objective (log)

8.07
8.07
8.07
8.07
8.07
8.07

time (s)
118.83
PDMM1
137.46
PDMM2
PDMM3
147.82
GSADMM 163.09
RBSUMM 206.96
sADMM1
731.51

40
34
31
28
141
139

3.60
5.51
6.54
6.84
8.55
9.73

1
2

Remark 2 PDMM converges at the same rate as ADMM and its variants. In Theorem 2  PDMM
can achieve the fastest convergence by setting J = K = 1  τi = 1  νi = 0  i.e.  the entire matrix A
is considered as a single block  indicating PDMM reduces to the method of multipliers. In this case 
however  the resulting subproblem may be difﬁcult to solve  as discussed in Section 1. Therefore 
the number of blocks in PDMM depends on the trade-off between the number of subproblems and
how efﬁciently each subproblem can be solved.
4 Experimental Results
In this section  we evaluate the performance of PDMM in solving robust principal component
analysis (RPCA) and overlapping group lasso [28]. We compared PDMM with ADMM [2] or
GSADMM (no theory guarantee)  sADMM [17  26]  and RBSUMM [14]. Note GSADMM in-
cludes BSUMM [14]. All experiments are implemented in Matlab and run sequentially. We run
the experiments 10 times and report the average results. The stopping criterion is either when the
residual is smaller than 10−4 or when the number of iterations exceeds 2000.
RPCA: RPCA is used to obtain a low rank and sparse decomposition of a given matrix A corrupted
by noise [5  17]:

(cid:107)X1(cid:107)2

F + γ2(cid:107)X2(cid:107)1 + γ3(cid:107)X3(cid:107)∗

min

s.t. A = X1 + X2 + X3 .

(24)
where A ∈ Rm×n  X1 is a noise matrix  X2 is a sparse matrix and X3 is a low rank matrix.
A = L + S + V is generated in the same way as [17]1. In this experiment  m = 1000  n = 5000
and the rank is 100. The number appended to PDMM denotes the number of blocks (K) to be chosen
in PDMM  e.g.  PDMM1 randomly updates one block.
Figure 1 compares the convegence results of PDMM with ADMM methods. In PDMM  ρ = 1
3 )} for PDMM1  PDMM2
and τi  νi are chosen according to (8)  i.e.  (τi  νi) = {( 1
and PDMM3 respectively. We choose the ‘best’results for GSADMM (ρ = 1) and RBSUMM
(ρ = 1  α = ρ 11√
) and sADMM (ρ = 1). PDMMs perform better than RBSUMM and sADMM.
Note the public available code of sADMM1 does not have dual update  i.e.  τi = 0. sADMM should
be the same as PDMM3 if τi = 1
3. Since τi = 0  sADMM is the slowest algorithm. Without
tuning the parameters of PDMM  GSADMM converges faster than PDMM. Note PDMM can run
in parallel but GSADMM only runs sequentially. PDMM3 is faster than two randomized version
of PDMM since the costs of extra iterations in PDMM1 and PDMM2 have surpassed the savings
at each iteration. For the two randomized one block coordinate methods  PDMM1 converges faster
than RBSUMM in terms of both the number of iterations and runtime.
The effect of τi  νi: We tuned the parameter τi  νi
in PDMMs. Three randomized meth-
ods (RBSUMM  PDMM1 and PDMM2) choose the blocks cyclically instead of randomly. Ta-
ble 1 compares the ‘best’results of PDMM with other ADMM methods.
In PDMM  (τi  νi) =

5   0)  ( 1

4   1

2 )  ( 1

3   1

t+10

1http://www.stanford.edu/ boyd/papers/prox algs/matrix decomp.html

7

0100200300400500600700800−5−4−3−2−101234time (s)residual (log) PDMM1PDMM2PDMM3GSADMMRBSUMMsADMM050100150200250−5−4−3−2−101234iterationsresidual (log) PDMM1PDMM2PDMM3GSADMMRBSUMMsADMM501001502002503007.87.857.97.9588.058.18.15time (s)objective (log) PDMM1PDMM2PDMM3GSADMMRBSUMMsADMMFigure 2: Comparison of convergence of PDMM and other methods in overlapping group Lasso.

(cid:88)

1

(cid:107)Aw − b(cid:107)2

g∈Gdg(cid:107)wg(cid:107)2 .

2   1

3   1

2 )  ( 1

2   0)  ( 1

{( 1
2 )}. GSADMM converges with the smallest number of iterations  but PDMMs
can converge faster than GSADMM in terms of runtime. The computation per iteration in
GSADMM is slightly higher than PDMM3 because GSADMM updates the sum X1 + X2 + X3 but
PDMM3 can reuse the sum. Therefore  if the numbers of iterations of the two methods are close 
PDMM3 can be faster than GSADMM. PDMM1 and PDMM2 can be faster than PDMM3. By
simply updating one block  PDMM1 is the fastest algorithm and achieves the lowest residual.
Overlapping Group Lasso: We consider solving the overlapping group lasso problem [28]:

w

min

2 +

2Lλ

L   λ = L

J+1   νi = 1 − 1

(25)
where A ∈ Rm×n  w ∈ Rn×1 and wg ∈ Rb×1 is the vector of overlapping group indexed by
g. dg is some positive weight of group g ∈ G. As shown in Section 2.3 
(25) can be rewritten
as the form (14). The data is generated in a same way as [27  9]: the elements of A are sampled
from normal distribution  b = Ax +  with noise  sampled from normal distribution  and xj =
(−1)j exp(−(j − 1)/100). In this experiment  m = 5000  the number of groups is L = 100  and
5 in (25). The size of each group is 100 and the overlap is 10. The total number of
dg = 1
blocks in PDMM and sADMM is J = 101. τi  νi in PDMM are computed according to (8).
In Figure 2  the ﬁrst two ﬁgures plot the convergence of objective in terms of the number of iterations
and time. PDMM uses all 101 blocks and is the fastest algorithm. ADMM is the same as GSADMM
in this problem  but is slower than PDMM. Since sADMM does not consider the sparsity  it uses
J+1  leading to slow convergence. The two accelerated methods  PA-APG [27]
τi = 1
and S-APG [9]  are slower than PDMM and ADMM.
The effect of K: The third ﬁgure shows PDMM with different number of blocks K. Although the
complexity of each iteration is the lowest when K = 1  PDMM takes much more iterations than
other cases and thus takes the longest time. As K increases  PDMM converges faster and faster.
When K = 20  the runtime is already same as using all blocks. When K > 21  PDMM takes less
time to converge than using all blocks. The runtime of PDMM decreases as K increases from 21
to 61. However  the speedup from 61 to 81 is negligable. We tried different set of parameters for
i+t (0 ≤ i ≤ 5  ρ = 0.01  0.1  1) or sufﬁciently small step size  but could not see the
RBSUMM ρ i2+1
convergence of the objective within 5000 iterations. Therefore  the results are not included here.
5 Conclusions
We proposed a randomized block coordinate variant of ADMM named Parallel Direction Method of
Multipliers (PDMM) to solve the class of problem of minimizing block-separable convex functions
subject to linear constraints. PDMM considers the sparsity and the number of blocks to be updated
when setting the step size. We show two existing Jacobian ADMM methods are special cases of
PDMM. We also use PDMM to solve overlapping block problems. The global convergence and
the iteration complexity are established with constant step size. Experiments on robust PCA and
overlapping group lasso show that PDMM is faster than existing methods.
Acknowledgment
H. W. and A. B. acknowledge the support of NSF via IIS-1447566  IIS-1422557  CCF-1451986  CNS-1314560 
IIS-0953274  IIS-1029711  IIS-0916750  and NASA grant NNX12AQ39A. H. W. acknowledges the support
of DDF (2013-2014) from the University of Minnesota. A.B. acknowledges support from IBM and Yahoo.
Z.Q. Luo is supported in part by the US AFOSR via grant number FA9550-12-1-0340 and the National Science
Foundation via grant number DMS-1015346.

8

05010015020000.10.20.30.40.5time (s)objective PA−APGS−APGPDMMADMMsADMM0200400600800100000.10.20.30.40.5iterationobjective PA−APGS−APGPDMMADMMsADMM203040506070−5−4−3−2−10time (s)residual (log) 121416181101References
[1] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Convex Optimization with Sparsity-Inducing Norms.

S. Sra  S. Nowozin  S. J. Wright.  editors  Optimization for Machine Learning  MIT Press  2011.

[2] S. Boyd  E. Chu N. Parikh  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning via
the alternating direction method of multipliers. Foundation and Trends Machine Learning  3(1):1–122 
2011.

[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[4] T. Cai  W. Liu  and X. Luo. A constrained (cid:96)1 minimization approach to sparse precision matrix estimation.

Journal of American Statistical Association  106:594–607  2011.

[5] E. J. Candes  X. Li  Y. Ma  and J. Wright. Robust principal component analysis ?. Journal of the ACM 

58:1–37  2011.

[6] V. Chandrasekaran  P. A. Parrilo  and A. S. Willsky. Latent variable graphical model selection via convex

optimization. Annals of Statistics  40:1935–1967  2012.

[7] C. Chen  B. He  Y. Ye  and X. Yuan. The direct extension of ADMM for multi-block convex minimization

problems is not necessarily convergent. Preprint  2013.

[8] S. Chen  D.L. Donoho  and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM review 

43:129–159  2001.

[9] X. Chen  Q. Lin  S. Kim  J. G. Carbonell  and E. P. Xing. Smoothing proximal gradient method for

general structured sparse regression. The Annals of Applied Statistics  6:719752  2012.

[10] W. Deng  M. Lai  Z. Peng  and W. Yin. Parallel multi-block admm with o(1/k) convergence. ArXiv 

2014.

[11] Q. Fu  H. Wang  and A. Banerjee. Bethe-ADMM for tree decomposition based parallel MAP inference.

In UAI  2013.

[12] D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems via ﬁnite-

element approximations. Computers and Mathematics with Applications  2:17–40  1976.

[13] B. He  M. Tao  and X. Yuan. Alternating direction method with Gaussian back substitution for separable

convex programming. SIAM Journal of Optimization  pages 313–340  2012.

[14] M. Hong  T. Chang  X. Wang  M. Razaviyayn  S. Ma  and Z. Luo. A block successive upper bound

minimization method of multipliers for linearly constrained convex optimization. Preprint  2013.

[15] M. Hong and Z. Luo. On the linear convergence of the alternating direction method of multipliers. ArXiv 

2012.

[16] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization methods. SIAM Journal

on Optimization  22(2):341362  2012.

[17] N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Optimization  1:123–231  2014.
Iteration complexity of randomized block-coordinate descent methods for
[18] P. Richtarik and M. Takac.

minimizing a composite function. Mathematical Programming  2012.

[19] N. Z. Shor. Minimization Methods for Non-Differentiable Functions. Springer-Verlag  1985.
[20] R. Tappenden  P. Richtarik  and B. Buke. Separable approximations and decomposition methods for the

augmented lagrangian. Preprint  2013.

[21] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational inference.

Foundations and Trends in Machine Learning  1:1–305  2008.

[22] H. Wang and A. Banerjee. Online alternating direction method. In ICML  2012.
[23] H. Wang and A. Banerjee. Bregman alternating direction method of multipliers. In NIPS  2014.
[24] H. Wang  A. Banerjee  C. Hsieh  P. Ravikumar  and I. Dhillon. Large scale distributed sparse precesion

estimation. In NIPS  2013.

[25] H. Wang  A. Banerjee  and Z. Luo. Parallel direction method of multipliers. ArXiv  2014.
[26] X. Wang  M. Hong  S. Ma  and Z. Luo. Solving multiple-block separable convex minimization problems

using two-block alternating direction method of multipliers. Preprint  2013.

[27] Y. Yu. Better approximation and faster algorithm using the proximal average. In NIPS  2012.
[28] P. Zhao  G. Rocha  and B. Yu. The composite absolute penalties family for grouped and hierarchical

variable selection. Annals of Statistics  37:34683497  2009.

[29] Z. Zhou  X. Li  J. Wright  E. Candes  and Y. Ma. Stable principal component pursuit. In IEEE Interna-

tional Symposium on Information Theory  2010.

9

,Huahua Wang
Arindam Banerjee
Zhi-Quan Luo