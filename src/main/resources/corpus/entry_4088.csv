2013,Online learning in episodic Markovian decision processes by relative entropy policy search,We study the problem of online learning in finite episodic Markov decision processes where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret defined as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a finite action space $\A$ and the state space $\X$ has a layered structure with $L$ layers  so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after $T$ episodes is $2\sqrt{L\nX\nA T\log(\nX\nA/L)}$ in the bandit setting and $2L\sqrt{T\log(\nX\nA/L)}$ in the full information setting. These guarantees largely improve previously known results under much milder assumptions and cannot be significantly improved under general assumptions.,Online Learning in Episodic Markovian Decision

Processes by Relative Entropy Policy Search

Alexander Zimin

Institute of Science and Technology Austria

alexander.zimin@ist.ac.at

Gergely Neu

INRIA Lille – Nord Europe

gergely.neu@gmail.com

Abstract

We study the problem of online learning in ﬁnite episodic Markov decision pro-
cesses (MDPs) where the loss function is allowed to change between episodes.
The natural performance measure in this learning problem is the regret deﬁned as
the difference between the total loss of the best stationary policy and the total loss
suffered by the learner. We assume that the learner is given access to a ﬁnite action
space A and the state space X has a layered structure with L layers  so that state
transitions are only possible between consecutive layers. We describe a variant of
the recently proposed Relative Entropy Policy Search algorithm and show that its

regret after T episodes is 2(cid:112)L|X||A|T log(|X||A|/L) in the bandit setting and
2L(cid:112)T log(|X||A|/L) in the full information setting  given that the learner has

perfect knowledge of the transition probabilities of the underlying MDP. These
guarantees largely improve previously known results under much milder assump-
tions and cannot be signiﬁcantly improved under general assumptions.

1

Introduction

In this paper  we study the problem of online learning in a class of ﬁnite non-stationary episodic
Markov decision processes. The learning problem that we consider can be formalized as a sequential
interaction between a learner (often called agent) and an environment  where the interaction between
the two entities proceeds in episodes. Every episode consists of multiple time steps: In every time
step of an episode  a learner has to choose one of its available actions after observing some part
of the current state of the environment. The chosen action inﬂuences the observable state of the
environment in a stochastic fashion and imposes some loss on the learner. However  the entire state
(be it observed or not) also inﬂuences the loss. The goal of the learner is to minimize its total
(non-discounted) loss that it suffers. In this work  we assume that the unobserved part of the state
evolves autonomously from the observed part of the state or the actions chosen by the learner  thus
corresponding to a state sequence generated by an oblivious adversary such as nature. Otherwise 
absolutely no statistical assumption is made about the mechanism generating the unobserved state
variables. As usual for such learning problems  we set our goal as minimizing the regret deﬁned as
the difference between the total loss suffered by the learner and the total loss of the best stationary
state-feedback policy. This setting fuses two important paradigms of learning theory: online learning
[5] and reinforcement learning [21  22].
The learning problem outlined above can be formalized as an online learning problem where the
actions of the learner correspond to choosing policies in a known Markovian decision process where
the loss function changes arbitrarily between episodes. This setting is a simpliﬁed version of the

Parts of this work were done while Alexander Zimin was enrolled in the MSc. programme of the Central
European University  Budapest  and Gergely Neu was working on his PhD. thesis at the Budapest University of
Technology and Economics and the MTA SZTAKI Institute for Computer Science and Control  Hungary. Both
authors would like to express their gratitude to L´aszl´o Gy¨orﬁ for making this collaboration possible.

1

learning problem ﬁrst addressed by Even-Dar et al. [8  9]  who consider online learning unichain
MDPs. In their variant of the problem  the learner faces a continuing MDP task where all policies
are assumed to generate a unique stationary distribution over the state space and losses can change
arbitrarily between consecutive time steps. Assuming that the learner observes the complete loss
function after each time step (that is  assuming full information feedback)  they propose an algorithm

called MDP-E and show that its regret is O(τ 2(cid:112)T log |A|)  where τ > 0 is an upper bound on the

mixing time of any policy. The core idea of MDP-E is the observation that the regret of the global
decision problem can be decomposed into regrets of simpler decision problems deﬁned in each state.
Yu et al. [23] consider the same setting and propose an algorithm that guarantees o(T ) regret under
bandit feedback where the learner only observes the losses that it actually suffers  but not the whole
loss function. Based on the results of Even-Dar et al. [9]  Neu et al. [16] propose an algorithm
that is shown to enjoy an O(T 2/3) bound on the regret in the bandit setting  given some further
assumptions concerning the transition structure of the underlying MDP. For the case of continuing
deterministic MDP tasks  Dekel and Hazan [7] describe an algorithm guaranteeing O(T 2/3) regret.
The immediate precursor of the current paper is the work of Neu et al. [14]  who consider online
learning in episodic MDPs where the state space has a layered (or loop-free) structure and every
policy visits every state with a positive probability of at least α > 0. Their analysis is based on a
decomposition similar to the one proposed by Even-Dar et al. [9]  and is sufﬁcient to prove a regret

bound of O(L2(cid:112)T|A| log |A|/α) in the bandit case and O(L2(cid:112)T log |A|) in the full information

case.
In this paper  we present a learning algorithm that directly aims to minimize the global regret of the
algorithm instead of trying to minimize the local regrets in a decomposed problem. Our approach
is motivated by the insightful paper of Peters et al. [17]  who propose an algorithm called Relative
Entropy Policy Search (REPS) for reinforcement learning problems. As Peters et al. [17] and Kakade
[11] point out  good performance of policy search algorithms requires that the information loss
between the consecutive policies selected by the algorithm is bounded  so that policies are only
modiﬁed in small steps. Accordingly  REPS aims to select policies that minimize the expected loss
while guaranteeing that the state-action distributions generated by the policies stay close in terms
of Kullback–Leibler divergence. Further  Daniel et al. [6] point out that REPS is closely related
to a number of previously known probabilistic policy search methods. Our paper is based on the
observation that REPS is closely related to the Proximal Point Algorithm (PPA) ﬁrst proposed by
Martinet [13] (see also [20]).
We propose a variant of REPS called online REPS or O-REPS and analyze it using fundamen-
tal results concerning the PPA family. Our analysis improves all previous results concerning on-
line learning in episodic MDPs: we show that the expected regret of O-REPS is bounded by

2(cid:112)L|X||A|T log(|X||A|/L) in the bandit setting and 2L(cid:112)T log(|X||A|/L) in the full informa-

tion setting. Unlike previous works in the literature  we do not have to make any assumptions about
the transition dynamics apart from the loop-free assumption. The full discussion of our results is
deferred to Section 5.
Before we move to the technical content of the paper  we ﬁrst ﬁx some conventions. Random
variables will be typeset in boldface (e.g.  x  a) and indeﬁnite sums over states and actions are to be
understood as sums over the entire state and action spaces. For clarity  we assume that all actions
are available in all states  however  this assumption is not essential. The indicator of any event A
will be denoted by I{A}.

2 Problem deﬁnition
An episodic loop-free Markov decision process is formally deﬁned by the tuple M = {X  A  P} 
where X is the ﬁnite state space  A is the ﬁnite action space  and P : X × X × A is the transition
function  where P (x(cid:48)|x  a) is the probability that the next state of the Markovian environment will
be x(cid:48)  given that action a is selected in state x. We will assume that M satisﬁes the following
assumptions:

• The state space X can be decomposed into non-intersecting layers  i.e. X = (cid:83)L

k=0 Xk

where Xl ∩ Xk = ∅ for l (cid:54)= k.

• X0 and XL are singletons  i.e. X0 = {x0} and XL = {xL}.

2

• Transitions are possible only between consecutive layers. Formally  if P (x(cid:48)|x  a) > 0  then

x(cid:48) ∈ Xk+1 and x ∈ Xk for some 0 ≤ k ≤ L − 1.

The interaction between the learner and the environment is described on Figure 1. The interaction
of an agent and the Markovian environment proceeds in episodes  where in each episode the agent
starts in state x0 and moves forward across the consecutive layers until it reaches state xL.1 We
assume that the environment selects a sequence of loss functions {(cid:96)t}T
t=1 and the losses only change
between episodes. Furthermore  we assume that the learner only observes the losses that it suffers
in each individual state-action pair that it visits  in other words  we consider bandit feedback.2

Parameters: Markovian environment M = {X  A  P};
For all episodes t = 1  2  . . .   T   repeat

1. The environment chooses the loss function (cid:96)t : X × A → [0  1].
2. The learner starts in state x0(t) = x0.
3. For all time steps l = 0  1  2  . . .   L − 1  repeat

(a) The learner observes xl(t) ∈ Xl.
(b) Based on its previous observations (and randomness)  the learner selects al(t).
(c) The learner suffers and observes loss (cid:96)t(xl(t)  al(t)).
(d) The environment draws the new state xl+1(t) ∼ P (·|xl(t)  al(t)).

Figure 1: The protocol of online learning in episodic MDPs.

For deﬁning our performance measure  we need to specify a set of reference controllers that is made
available to the learner. To this end  we deﬁne the concept of (stochastic stationary) policies: A
policy is deﬁned as a mapping π : A × X → [0  1]  where π(a|x) gives the probability of selecting
action a in state x. The expected total loss of a policy π is deﬁned as

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) P  π

 

LT (π) = E

(cid:96)t(x(cid:48)

k  a(cid:48)
k)

t=1

k=0

(cid:34) T(cid:88)
L−1(cid:88)
(cid:80)L−1
(cid:98)RT =(cid:98)LT − min

k=0

π

LT (π) 

loss suffered by the learner as(cid:98)LT =(cid:80)T

where the notation E [·| P  π] is used to emphasize that the random variables x(cid:48)
k are gener-
ated by executing π in the MDP speciﬁed by the transition function P . Denote the total expected
E [ (cid:96)t(xk(t)  ak(t))| P ]  where the expectation is
taken over the internal randomization of the learner and the random transitions of the Markovian
environment. Using these notations  we deﬁne the learner’s goal as minimizing the (total expected)
regret deﬁned as

k and a(cid:48)

t=1

where the minimum is taken over the complete set of stochastic stationary policies.3
It is beneﬁcial to introduce the concept of occupancy measures on the state-action space X ×A: the
occupancy measure qπ of policy π is deﬁned as the collection of distributions generated by executing
policy π on the episodic MDP described by P :
k(x) = x  a(cid:48)
x(cid:48)
(cid:88)
(cid:88)

measure of any policy π satisﬁes(cid:88)

where k(x) denotes the index of the layer that x belongs to. It is easy to see that the occupancy

qπ(x  a) = P(cid:104)

(cid:12)(cid:12)(cid:12) P  π

k(x) = a

(cid:105)

 

P (x|x(cid:48)  a(cid:48))qπ(x(cid:48)  a(cid:48)) 

(1)

qπ(x  a) =

a

x(cid:48)∈Xk(x)−1

a(cid:48)

1Such MDPs naturally arise in episodic decision tasks where some notion of time is present in the state

description.

feedback  see Audibert et al. [2].

2In the literature of online combinatorial optimization  this feedback scheme is often called semi-bandit

3The existence of this minimum is a standard result of MDP theory  see Puterman [18].

3

for all x ∈ X \{x0  xl}  with qπ(x0  a) = π(a|x0) for all a ∈ A. The set of all occupancy measures
satisfying the above equality in the MDP M will be denoted as ∆(M ). The policy π is said to
generate the occupancy measure q ∈ ∆(M ) if

π(a|x) =

(cid:80)

q(x  a)
b q(x  b)

holds for all (x  a) ∈ X × A. It is clear that there exists a unique generating policy for all measures
in ∆(M ) and vice versa. The policy generating q will be denoted as πq. In what follows  we will
redeﬁne the task of the learner from having to select individual actions ak(t) to having to select
occupancy measures qt ∈ ∆(M ) in each episode t. To see why this notion simpliﬁes the treatment
of the problem  observe that

(cid:34) L−1(cid:88)

k=0

E

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) P  πq

(cid:96)t(x(cid:48)

k  a(cid:48)
k)

(cid:88)

(cid:88)

q(x  a)(cid:96)t(x  a)

a

x∈Xk
q(x  a)(cid:96)t(x  a) def= (cid:104)q  (cid:96)t(cid:105)  

(2)

L−1(cid:88)
(cid:88)

k=0

x a

=

=

(cid:34) T(cid:88)

t=1

where we deﬁned the inner product (cid:104)· ·(cid:105) on X × A in the last line. Using this notation  we can
reformulate our original problem as an instance of online linear optimization with decision space
∆(M ). Assuming that the learner selects occupancy measure qt in episode t  the regret can be
rewritten as

(cid:98)RT = max

q∈∆(M )

E

(cid:35)

(cid:104)qt − q  (cid:96)t(cid:105)

.

3 The algorithm: O-REPS

Using the formalism introduced in the previous section  we now describe our algorithm called Online
Relative Entropy Policy Search (O-REPS). O-REPS is an instance of online linear optimization
methods usually referred to as Follow-the-Regularized-Leader (FTRL)  Online Stochastic Mirror
Descent (OSMD) or the Proximal Point Algorithm (PPA)—see  e.g.  [1]  [19]  [3] and [2] for a
discussion of these methods and their relations. To allow comparisons with the original derivation of
REPS by Peters et al. [17]  we formalize our algorithm as an instance of PPA. Before describing the
algorithm  some more deﬁnitions are in order. First  deﬁne D (q(cid:107)q(cid:48)) as the unnormalized Kullback–
Leibler divergence between two occupancy measures q and q(cid:48):

D (q(cid:107)q(cid:48)) =

q(x  a) log

(q(x  a) − q(cid:48)(x  a)) .

(cid:88)

x a

q(x  a)
q(cid:48)(x  a)

−(cid:88)
q(x  a) log q(x  a) −(cid:88)

x a

(cid:88)

R(q) =

q(x  a).

Furthermore  let R(q) deﬁne the unnormalized negative entropy of the occupancy measure q:

x a

x a

We are now ready to deﬁne O-REPS formally. In the ﬁrst episode  O-REPS chooses the uniform
policy with π1(a|x) = 1/|A| for all x and a  and we let q1 = qπ1.4 Then  the algorithm proceeds
recursively: After observing

ut = (x0(t)  a0(t)  (cid:96)t(x0(t)  a0(t))  . . .   xL−1(t)  aL−1(t)  (cid:96)t(xL−1(t)  aL−1(t))  xL(t))

in episode t  we deﬁne the loss estimates ˆ(cid:96)t as

ˆ(cid:96)t =

(cid:96)t(x  a)
qt(x  a)

I{(x  a) ∈ ut}  

where we used the notation (x  a) ∈ ut to indicate that the state-action pair (x  a) was observed dur-
ing episode t. After episode t  O-REPS selects the occupancy measure that solves the optimization
problem

(cid:110)

(cid:68)

(cid:69)

η

q  ˆ(cid:96)t

+ D(q||qt)

.

(3)

(cid:111)

qt+1 = arg min
q∈∆(M )

4Note that qπ can be simply computed by using (1) recursively.

4

In episode t  our algorithm follows the policy πt = πqt. Deﬁning Ut = (u1  u2  . . .   ut)  we
clearly have that qt(x  a) = P [ (x  a) ∈ ut| Ut−1]  so ˆ(cid:96)t(x  a) is an unbiased estimate of (cid:96)t(x  a)
for all (x  a) such that qt(x  a) > 0:

=

(cid:96)t(x  a)
qt(x  a)

P [ (x  a) ∈ ut| Ut−1] = (cid:96)t(x  a).

(4)

E(cid:104) ˆ(cid:96)t(x  a)

(cid:105)

(cid:12)(cid:12)(cid:12) Ut−1

We now proceed to explain how the policy update step (3) can be implemented efﬁciently. It is
known (see  e.g.  Bart´ok et al. [3  Lemma 8.6]) that performing this optimization can be reformulated
as ﬁrst solving the unconstrained optimization problem

(cid:110)

(cid:68)

(cid:69)

η

q  ˆ(cid:96)t

+ D(q||qt)

(cid:111)

q
and then projecting the result to ∆(M ) as

˜qt+1 = arg min

qt+1 = arg min
q∈∆(M )

D (q(cid:107)˜qt+1) .

The ﬁrst step can be simply carried out by setting ˜qt+1(x  a) = qt(x  a)e−η ˆ(cid:96)t(x a). The projection
step  however  requires more care. To describe the projection procedure  we need to introduce some
more notation. For any function v : X → R and loss function (cid:96) : X × A → [0  1] we deﬁne a
function

δ(x  a|v  (cid:96)) = −η(cid:96)(x  a) − (cid:88)

v(x(cid:48))P (x(cid:48)|x  a) + v(x).

(5)

x(cid:48)∈X

As noted by Peters et al. [17]  the above function can be regarded as the Bellman error corresponding
to the value function v. The next proposition provides a succinct formalization of the optimization
problem (3).
Proposition 1. Let t > 1 and deﬁne the function

(cid:88)

qt(x  a)eδ(x a|v  ˆ(cid:96)t).

Zt(v  k) =

The update step (3) can be performed as

x∈Xk a∈A

qt+1(x  a) =

where

qt(x  a)eδ(x a|ˆvt  ˆ(cid:96)t)

Zt(ˆvt  k(x))

 

L(cid:88)

k=0

ˆvt = arg min

v

ln Zt(v  k).

(6)

Minimizing the expression on the right-hand side of Equation (6) is an unconstrained convex op-
timization problem (see Boyd and Vandenberghe [4] and the comments of Peters et al. [17]) and
can be solved efﬁciently. It is important to note that since q1(x  a) > 0 holds for all (x  a) pairs 
qt(x  a) is also positive for all t > 0 by the multiplicative update rule  so Equation 4 holds for all
state-action pairs (x  a) in all time steps.
The proof follows the steps of Peters et al. [17]  however  their original formalization of REPS is
slightly different  which results in small changes in the analysis as well. For further comments
regarding the differences between O-REPS and REPS  see Section 5.

Proof of Proposition 1. We start with formulating the projection step as a constrained optimization
problem:

subject to (cid:88)
(cid:88)
(cid:88)

a

x∈Xk

a

D (q(cid:107)˜qt+1)

min

q

(cid:88)

q(x  a) =

x(cid:48) a(cid:48)

q(x  a) = 1

P (x|x(cid:48)  a(cid:48))q(x(cid:48)  a(cid:48))

for all x ∈ X \ {x0  xl} 

for all k = 0  1  . . .   L − 1.

5

To solve the problem  consider the Lagrangian:

Lt(q) =D (q(cid:107)˜qt+1) +

x∈Xk a∈A

k=0

λk

 (cid:88)
L−1(cid:88)
 (cid:88)
(cid:88)
(cid:32)
(cid:88)
(cid:32)

x(cid:48)∈Xk−1

q(x0  a)

a(cid:48)

a

λ0 +

(cid:88)

x(cid:48)

+

v(x)

k=1

x∈Xk
=D (q(cid:107)˜qt+1) +

L−1(cid:88)

(cid:88)

(cid:88)

(cid:88)

x(cid:54)=x0

a

+

q(x  a)

λk(x) +



q(x  a) − 1

q(x  a)

q(x(cid:48)  a(cid:48))P (x|x(cid:48)  a(cid:48)) −(cid:88)
(cid:33)
(cid:88)
− L−1(cid:88)
(cid:33)

x(cid:48)
v(x(cid:48))P (x(cid:48)|x  a) − v(x)

v(x(cid:48))P (x(cid:48)|x0  a)

k=0

a

 



λk

where {λk}L−1
v(xL) = 0 for convenience. Differentiating the Lagrangian with respect to any q(x  a)  we get

k=0 and {v(x)}x∈X\{x0 xl} are Lagrange multipliers. In what follows  we set v(x0) =
∂Lt(q)
∂q(x  a)

v(x(cid:48))P (x(cid:48)|x  a) − v(x).

(cid:88)

= ln q(x  a) − ln ˜qt+1(x  a) + λk(x) +
qt+1(x  a) = ˜qt+1(x  a)e−λk(x)−(cid:80)

x(cid:48)

x(cid:48) v(x(cid:48))P (x(cid:48)|x a)+v(x).

Hence  setting the gradient to zero  we obtain the formula for qt+1(x  a):

Substituting the formula for ˜qt+1(x  a)  we get

qt+1(x  a) = qt(x  a)e−λk(x)+δ(x a|v  ˆ(cid:96)t).
Using the second constraint  we have for every k = 0  1  . . .   L − 1 that

qt(x  a)e−λk+δ(x a|v  ˆ(cid:96)t) = 1 

(cid:88)

(cid:88)

x∈Xk

a

yielding e−λk = 1/Zt(v  k)  which leaves us with computing the value of v at the optimum. This
can be done by solving the dual problem of maximizing

(cid:88)
is equivalent to maximizing −(cid:80)L−1

over {λk}L−1

x a

˜qt+1(x  a) − L − L−1(cid:88)

λk

k=0

k=0 . If we drop the constants and express each λk in terms of Zt(v  k)  then the problem

k=0 ln Zt(v  k)  that is  solving the optimization problem (6).

4 Analysis

The next theorem states our main result concerning the regret of O-REPS under bandit feedback. The
proof of the theorem is based on rather common ideas used in the analysis of FTRL/OSMD/PPA-
style algorithms (see  e.g.  [24]  Chapter 11 of [5]  [1]  [12]  [2]). After proving the theorem  we also
present the regret bound for O-REPS when used in a full information setting where the learner gets
to observe (cid:96)t after each episode t.
Theorem 1. Assuming bandit feedback  the total expected regret of O-REPS satisﬁes

(cid:114)

In particular  setting η =

(cid:98)RT ≤ η|X||A|T +

L log

|X||A|

L

η

|X||A|

L log

T|X||A| yields

L

(cid:114)

(cid:98)RT ≤ 2

L|X||A|T log

|X||A|

L

6

.

.

qt(x  a) ˆ(cid:96)2

t (x  a)

qt(x  a)

(cid:96)t(x  a)
qt(x  a)

ˆ(cid:96)t(x  a) ≤ η

T(cid:88)

(cid:88)

t=1

x a

ˆ(cid:96)t(x  a).

Using the exact form of ˜qt+1 and the fact that ex ≥ 1 + x  we get that
˜qt+1(x  a) ≥ qt(x  a) − ηqt(x  a) ˆ(cid:96)t(x  a)

t=1

x a

t=1

(cid:68)

≤ η

and thus

T(cid:88)

(cid:69) ≤ η

qt − ˜qt+1  ˆ(cid:96)t

T(cid:88)
T(cid:88)

Combining this with (7)  we get

(cid:88)
(cid:88)
(cid:69) ≤ η
(cid:34) T(cid:88)
(cid:88)
It also follows from Equation (4) that E(cid:104)(cid:68)

qt − q  ˆ(cid:96)t

T(cid:88)

(cid:68)

t=1

x a

t=1

notice that

E

ˆ(cid:96)t(x  a)

t=1

(cid:69)(cid:105)
D (q(cid:107)q1) ≤R(q) − R(q1) ≤ L−1(cid:88)

x a
q  ˆ(cid:96)t

T(cid:88)

t=1

x a

ˆ(cid:96)t(x  a) +

(cid:88)
(cid:35)
= (cid:104)q  (cid:96)t(cid:105) and E(cid:104)(cid:68)
(cid:88)

≤ |X||A|T.

(cid:88)

Next  we take an expectation on both sides. By Equation (4)  we have

D (q(cid:107)q1)

η

.

(8)

(cid:69)(cid:105)

qt  ˆ(cid:96)t

= E [(cid:104)qt  (cid:96)t(cid:105)]. Finally 

Proof. By standard arguments (see  e.g.  [19  Lemma 12]  [3  Lemma 9.2] or [5  Theorem 11.1]) 
we have

T(cid:88)

(cid:68)

t=1

(cid:69) ≤ T(cid:88)

(cid:68)

t=1

(cid:69)

D (q(cid:107)q1)

η

qt − q  ˆ(cid:96)t

qt − ˜qt+1  ˆ(cid:96)t

+

.

(7)

q1(x  a) log

1

q1(x  a)

k=0

x∈Xk

a

(since R(q) ≤ 0)

≤ L−1(cid:88)

k=0

log |Xk||A| ≤ L log

|X||A|

L

 

where we used the trivial upper bound on the entropy of distributions and Jensen’s inequality in
the last step. Plugging the above upper bound into Equation (8)  we obtain the statement of the
theorem.
Theorem 2. Assuming full feedback  the total expected regret of O-REPS satisﬁes

(cid:113)

log

In particular  setting η =

(cid:98)RT ≤ ηLT +
(cid:114)
(cid:98)RT ≤ 2L

yields

|X||A|

L

T

L log

|X||A|

L

.

η

T log

|X||A|

L

.

The proof of the statement follows directly from the proof of Theorem 1  with the only difference
that we set ˆ(cid:96)t = (cid:96)t and we can use the tighter upper bound

T(cid:88)

t=1

(cid:104)qt − ˜qt+1  (cid:96)t(cid:105) ≤ η

T(cid:88)
T(cid:88)

t=1

(cid:88)
(cid:88)

x a

t=1

x a

≤ η

7

where we used that(cid:80)

x∈Xk

(cid:80)

a qt(x  a) = 1 for all layers k.

qt(x  a)(cid:96)2

t (x  a)

qt(x  a) = ηLT 

5 Conclusions and future work

Comparison with previous results We ﬁrst compare our regret bounds with previous results from
the literature. First  our guarantees for the full information case trade off a factor of L present in

the bounds of Neu et al. [14] to a (usually much smaller) factor of(cid:112)log |X|. More importantly 
our bounds trade off a factor of L3/2/α in the bandit case to a factor of(cid:112)|X|. This improvement

is particularly remarkable considering that we do not need to assume that α > 0  that is  we drop
the rather unnatural assumption that every stationary policy has to visit every state with positive
probability. In particular  dropping this assumption enables our algorithm to work in deterministic
loop-free MDPs  that is  to solve the online shortest path problem (see  e.g.  [10]). In the shortest
path setting  O-REPS provides an alternative implementation to the Component Hedge algorithm
analyzed by Koolen et al. [12]  who prove identical bounds in the full information case. As shown
by Audibert et al. [2]  Component Hedge achieves the analog of our bounds in the bandit case as
well.
O-REPS also bears close resemblance to the algorithms of Even-Dar et al. [9] and Neu et al. [16] who
x(cid:48) P (x(cid:48)|x  a)vt(x(cid:48))).
The most important difference between their algorithm and O-REPS is that their value functions vt
are computed as the solution of the Bellman-equations instead of the solution of the optimization
problem (6). By a simple combination of our analysis and that of Even-Dar et al. [9]  it is possible to
τ T ) in the unichain setting with full information feedback 
improving their bound by a factor of τ 3/2 under the same assumptions. It is an interesting open
problem to ﬁnd out if using the O-REPS value functions is a strictly better idea than solving the
Bellman equations in general. Another important direction of future work is to extend our results to
the case of unichain MDPs with bandit feedback and the setting where the transition probabilities of
the underlying MDP is unknown (see Neu et al. [15]).

also use policy updates of the form πt+1(a|x) ∝ πt(a|x) exp(−η(cid:96)t(x  a)−(cid:80)
show that O-REPS attains a regret of (cid:101)O(

√

Lower bounds Following the proof of Theorem 10 in Audibert et al. [2]  it is straightforward
to construct an MDP consisting of |X|/L chains of L consecutive bandit problems each with |A|

actions such that no algorithm can achieve smaller regret than 0.03L(cid:112)T log(|X||A|) in the full in-
formation case and 0.04(cid:112)L|X||A|T in the bandit case. These results suggest that our bounds can-

not be signiﬁcantly improved in general  however  ﬁnding an appropriate problem-dependent lower
bound remains an interesting open problem in the much broader ﬁeld of online linear optimization.

REPS vs. O-REPS As noted several times above  our algorithm is directly inspired by the work
of Peters et al. [17]. However  there is a slight difference between the original version of REPS and
O-REPS  namely  Peters et al. aim to solve the optimization problem qt+1 = arg minq∈∆(M )(cid:104)q  ˆ(cid:96)t(cid:105)
subject to the constraint D (q(cid:107)qt) ≤ ε for some ε > 0. This is to be contrasted with the following
property of the occupancy measures generated by O-REPS (proved in the supplementary material):
Lemma 1. For any t > 0  D (qt(cid:107)qt+1) ≤ η2

2 (cid:104)qt  ˆ(cid:96)2
t(cid:105).

In particular  if the losses are estimated by bounded sample averages as done by Peters et al. [17] 
this gives D (qt(cid:107)qt+1) ≤ η2/2. While this is not the exact same property as desired by REPS 
both inequalities imply that the occupancy measures stay close to each other in the 1-norm sense by
Pinsker’s inequality. Thus we conjecture that our formulation of O-REPS has similar properties to
the one studied by Peters et al. [17]  while it might be somewhat simpler to implement.

Acknowledgments

Alexander Zimin is an OMV scholar. Gergely Neu’s work was carried out during the tenure of
an ERCIM ”Alain Bensoussan” Fellowship Programme. The research leading to these results has
received funding from INRIA  the European Union Seventh Framework Programme (FP7/2007-
2013) under grant agreements 246016 and 231495 (project CompLACS)  the Ministry of Higher
Education and Research  Nord-Pas-de-Calais Regional Council and FEDER through the “Contrat
de Projets Etat Region (CPER) 2007-2013”.

8

References
[1] Abernethy  J.  Hazan  E.  and Rakhlin  A. (2008). Competing in the dark: An efﬁcient algorithm
for bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory
(COLT)  pages 263–274.

[2] Audibert  J. Y.  Bubeck  S.  and Lugosi  G. (2013). Regret in online combinatorial optimization.

Mathematics of Operations Research. to appear.

[3] Bart´ok  G.  P´al  D.  Szepesv´ari  C.  and Szita  I. (2011). Online learning. Lecture notes  Univer-

sity of Alberta. https://moodle.cs.ualberta.ca/ﬁle.php/354/notes.pdf.

[4] Boyd  S. and Vandenberghe  L. (2004). Convex Optimization. Cambridge University Press.
[5] Cesa-Bianchi  N. and Lugosi  G. (2006). Prediction  Learning  and Games. Cambridge Univer-

sity Press  New York  NY  USA.

[6] Daniel  C.  Neumann  G.  and Peters  J. (2012). Hierarchical relative entropy policy search.
In Proceedings of the Fifteenth International Conference on Artiﬁcial Intelligence and Statistics 
volume 22 of JMLR Workshop and Conference Proceedings  pages 273–281.

[7] Dekel  O. and Hazan  E. (2013). Better rates for any adversarial deterministic mdp. In Dasgupta 
S. and McAllester  D.  editors  Proceedings of the 30th International Conference on Machine
Learning (ICML-13)  volume 28  pages 675–683. JMLR Workshop and Conference Proceedings.
[8] Even-Dar  E.  Kakade  S. M.  and Mansour  Y. (2005). Experts in a Markov decision process.

In NIPS-17  pages 401–408.

[9] Even-Dar  E.  Kakade  S. M.  and Mansour  Y. (2009). Online Markov decision processes.

Mathematics of Operations Research  34(3):726–736.

[10] Gy¨orgy  A.  Linder  T.  Lugosi  G.  and Ottucs´ak  Gy.. (2007). The on-line shortest path prob-

lem under partial monitoring. Journal of Machine Learning Research  8:2369–2403.

[11] Kakade  S. (2001). A natural policy gradient. In Advances in Neural Information Processing

Systems 14 (NIPS)  pages 1531–1538.

[12] Koolen  W. M.  Warmuth  M. K.  and Kivinen  J. (2010). Hedging structured concepts. In

Proceedings of the 23rd Annual Conference on Learning Theory (COLT)  pages 93–105.

[13] Martinet  B. (1970). R´egularisation d’in´equations variationnelles par approximations succes-
sives. ESAIM: Mathematical Modelling and Numerical Analysis - Mod´elisation Math´ematique
et Analyse Num´erique  4(R3):154–158.

[14] Neu  G.  Gy¨orgy  A.  and Szepesv´ari  Cs. (2010a). The online loop-free stochastic shortest-
path problem. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT)  pages
231–243.

[15] Neu  G.  Gy¨orgy  A.  and Szepesv´ari  Cs. (2012). The adversarial stochastic shortest path

problem with unknown transition probabilities. In AISTATS 2012  pages 805–813.

[16] Neu  G.  Gy¨orgy  A.  Szepesv´ari  Cs.  and Antos  A. (2010b). Online Markov decision pro-

cesses under bandit feedback. In NIPS-23  pages 1804–1812. CURRAN.

[17] Peters  J.  M¨ulling  K.  and Altun  Y. (2010). Relative entropy policy search. In AAAI 2010 

pages 1607–1612.

[18] Puterman  M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Program-

ming. Wiley-Interscience.

[19] Rakhlin  A. (2009). Lecture notes on online learning.
[20] Rockafellar  R. T. (1976). Monotone Operators and the Proximal Point Algorithm. SIAM

Journal on Control and Optimization  14(5):877–898.

[21] Sutton  R. and Barto  A. (1998). Reinforcement Learning: An Introduction. MIT Press.
[22] Szepesv´ari  Cs. (2010). Algorithms for Reinforcement Learning. Synthesis Lectures on Artiﬁ-

cial Intelligence and Machine Learning. Morgan & Claypool Publishers.

[23] Yu  J. Y.  Mannor  S.  and Shimkin  N. (2009). Markov decision processes with arbitrary

reward processes. Mathematics of Operations Research  34(3):737–757.

[24] Zinkevich  M. (2003). Online convex programming and generalized inﬁnitesimal gradient
ascent. In Proceedings of the Twentieth International Conference on Machine Learning  pages
928–936.

9

,Alexander Zimin
Gergely Neu
Meisam Razaviyayn
Mingyi Hong
Zhi-Quan Luo
Jie Wang
Jieping Ye
Antti Tarvainen
Harri Valpola
Pei Wang
Nuno Nvasconcelos