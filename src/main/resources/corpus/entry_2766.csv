2009,Kernel Methods for Deep Learning,We introduce a new family of positive-definite kernel functions that mimic the computation in large  multilayer neural nets.  These kernel functions can be used in shallow architectures  such as support vector machines (SVMs)  or in deep kernel-based architectures that we call multilayer kernel machines (MKMs).  We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures.  On several problems  we obtain better results than previous  leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.,Kernel Methods for Deep Learning

Youngmin Cho and Lawrence K. Saul

Department of Computer Science and Engineering

University of California  San Diego
9500 Gilman Drive  Mail Code 0404
{yoc002 saul}@cs.ucsd.edu

La Jolla  CA 92093-0404

Abstract

We introduce a new family of positive-deﬁnite kernel functions that mimic the
computation in large  multilayer neural nets. These kernel functions can be used
in shallow architectures  such as support vector machines (SVMs)  or in deep
kernel-based architectures that we call multilayer kernel machines (MKMs). We
evaluate SVMs and MKMs with these kernel functions on problems designed to
illustrate the advantages of deep architectures. On several problems  we obtain
better results than previous  leading benchmarks from both SVMs with Gaussian
kernels as well as deep belief nets.

1

Introduction

Recent work in machine learning has highlighted the circumstances that appear to favor deep archi-
tectures  such as multilayer neural nets  over shallow architectures  such as support vector machines
(SVMs) [1]. Deep architectures learn complex mappings by transforming their inputs through mul-
tiple layers of nonlinear processing [2]. Researchers have advanced several motivations for deep
architectures: the wide range of functions that can be parameterized by composing weakly non-
linear transformations  the appeal of hierarchical distributed representations  and the potential for
combining unsupervised and supervised methods. Experiments have also shown the beneﬁts of
deep learning in several interesting applications [3  4  5].
Many issues surround the ongoing debate over deep versus shallow architectures [1  6]. Deep ar-
chitectures are generally more difﬁcult to train than shallow ones. They involve difﬁcult nonlinear
optimizations and many heuristics. The challenges of deep learning explain the early and continued
appeal of SVMs  which learn nonlinear classiﬁers via the “kernel trick”. Unlike deep architectures 
SVMs are trained by solving a simple problem in quadratic programming. However  SVMs cannot
seemingly beneﬁt from the advantages of deep learning.
Like many  we are intrigued by the successes of deep architectures yet drawn to the elegance of ker-
nel methods. In this paper  we explore the possibility of deep learning in kernel machines. Though
we share a similar motivation as previous authors [7]  our approach is very different. Our paper
makes two main contributions. First  we develop a new family of kernel functions that mimic the
computation in large neural nets. Second  using these kernel functions  we show how to train multi-
layer kernel machines (MKMs) that beneﬁt from many advantages of deep learning.
The organization of this paper is as follows.
In section 2  we describe a new family of kernel
functions and experiment with their use in SVMs. Our results on SVMs are interesting in their own
right; they also foreshadow certain trends that we observe (and certain choices that we make) for the
MKMs introduced in section 3. In this section  we describe a kernel-based architecture with multiple
layers of nonlinear transformation. The different layers are trained using a simple combination of
supervised and unsupervised methods. Finally  we conclude in section 4 by evaluating the strengths
and weaknesses of our approach.

1

2 Arc-cosine kernels

In this section  we develop a new family of kernel functions for computing the similarity of vector
inputs x  y ∈ (cid:60)d. As shorthand  let Θ(z) = 1
2(1 + sign(z)) denote the Heaviside step function. We
deﬁne the nth order arc-cosine kernel function via the integral representation:

kn(x  y) = 2

dw e− (cid:107)w(cid:107)2
(2π)d/2

2

Θ(w · x) Θ(w · y) (w · x)n (w · y)n

(1)

(cid:90)

The integral representation makes it straightforward to show that these kernel functions are positive-
semideﬁnite. The kernel function in eq. (1) has interesting connections to neural computation [8]
that we explore further in sections 2.2–2.3. However  we begin by elucidating its basic properties.

2.1 Basic properties

We show how to evaluate the integral in eq. (1) analytically in the appendix. The ﬁnal result is most
easily expressed in terms of the angle θ between the inputs:

(cid:19)

(cid:18) x · y

(cid:107)x(cid:107)(cid:107)y(cid:107)

θ = cos−1

.

(2)

The integral in eq. (1) has a simple  trivial dependence on the magnitudes of the inputs x and y  but
a complex  interesting dependence on the angle between them. In particular  we can write:

kn(x  y) =

(cid:107)x(cid:107)n(cid:107)y(cid:107)nJn(θ)

1
π

(3)

where all the angular dependence is captured by the family of functions Jn(θ). Evaluating the
integral in the appendix  we show that this angular dependence is given by:

Jn(θ) = (−1)n(sin θ)2n+1

.

(4)

(cid:18) 1

∂
∂θ

sin θ

(cid:19)n(cid:18) π − θ

(cid:19)

sin θ

For n = 0  this expression reduces to the supplement of the angle between the inputs. However  for
n >0  the angular dependence is more complicated. The ﬁrst few expressions are:

J0(θ) = π − θ
J1(θ) = sin θ + (π − θ) cos θ
J2(θ) = 3 sin θ cos θ + (π − θ)(1 + 2 cos2 θ)

(5)
(6)
(7)

π cos−1 x·y

We describe eq. (3) as an arc-cosine kernel because for n = 0 
it takes the simple form
k0(x  y) = 1− 1
(cid:107)x(cid:107)(cid:107)y(cid:107). In fact  the zeroth and ﬁrst order kernels in this family are strongly
motivated by previous work in neural computation. We explore these connections in the next section.
Arc-cosine kernels have other intriguing properties. From the magnitude dependence in eq. (3) 
we observe the following: (i) the n = 0 arc-cosine kernel maps inputs x to the unit hypersphere
in feature space  with k0(x  x) = 1; (ii) the n = 1 arc-cosine kernel preserves the norm of inputs 
with k1(x  x) = (cid:107)x(cid:107)2; (iii) higher order (n >1) arc-cosine kernels expand the dynamic range of the
inputs  with kn(x  x) ∼ (cid:107)x(cid:107)2n. Properties (i)–(iii) are shared respectively by radial basis function
(RBF)  linear  and polynomial kernels. Interestingly  though  the n = 1 arc-cosine kernel is highly
nonlinear  also satisfying k1(x −x) = 0 for all inputs x. As a practical matter  we note that arc-
cosine kernels do not have any continuous tuning parameters (such as the kernel width in RBF
kernels)  which can be laborious to set by cross-validation.

2.2 Computation in single-layer threshold networks

Consider the single-layer network shown in Fig. 1 (left) whose weights Wij connect the jth input
unit to the ith output unit. The network maps inputs x to outputs f(x) by applying an elementwise
nonlinearity to the matrix-vector product of the inputs and the weight matrix: f(x) = g(Wx). The
nonlinearity is described by the network’s so-called activation function. Here we consider the family
of one-sided polynomial activation functions gn(z) = Θ(z)zn illustrated in the right panel of Fig. 1.

2

Figure 1: Single layer network and activation functions

m(cid:88)

For n = 0  the activation function is a step function  and the network is an array of perceptrons. For
n = 1  the activation function is a ramp function (or rectiﬁcation nonlinearity [9])  and the mapping
f(x) is piecewise linear. More generally  the nonlinear (non-polynomial) behavior of these networks
is induced by thresholding on weighted sums. We refer to networks with these activation functions
as single-layer threshold networks of degree n.
Computation in these networks is closely connected to computation with the arc-cosine kernel func-
tion in eq. (1). To see the connection  consider how inner products are transformed by the mapping
in single-layer threshold networks. As notation  let the vector wi denote ith row of the weight
matrix W. Then we can express the inner product between different outputs of the network as:

f(x) · f(y) =

Θ(wi · x)Θ(wi · y)(wi · x)n(wi · y)n 

(8)

i=1

where m is the number of output units. The connection with the arc-cosine kernel function emerges
in the limit of very large networks [10  8].
Imagine that the network has an inﬁnite number of
output units  and that the weights Wij are Gaussian distributed with zero mean and unit vari-
In this limit  we see that eq. (8) reduces to eq. (1) up to a trivial multiplicative factor:
ance.
m f(x) · f(y) = kn(x  y). Thus the arc-cosine kernel function in eq. (1) can be viewed
limm→∞ 2
as the inner product between feature vectors derived from the mapping of an inﬁnite single-layer
threshold network [8].
Many researchers have noted the general connection between kernel machines and neural networks
with one layer of hidden units [1]. The n = 0 arc-cosine kernel in eq. (1) can also be derived from
an earlier result obtained in the context of Gaussian processes [8]. However  we are unaware of any
previous theoretical or empirical work on the general family of these kernels for degrees n≥0.
Arc-cosine kernels differ from polynomial and RBF kernels in one especially interesting respect.
As highlighted by the integral representation in eq. (1)  arc-cosine kernels induce feature spaces
that mimic the sparse  nonnegative  distributed representations of single-layer threshold networks.
Polynomial and RBF kernels do not encode their inputs in this way. In particular  the feature vector
induced by polynomial kernels is neither sparse nor nonnegative  while the feature vector induced
by RBF kernels resembles the localized output of a soft vector quantizer. Further implications of
this difference are explored in the next section.

2.3 Computation in multilayer threshold networks

A kernel function can be viewed as inducing a nonlinear mapping from inputs x to fea-
ture vectors Φ(x).
in the induced feature space:
k(x  y) = Φ(x)·Φ(y). In this section  we consider how to compose the nonlinear mappings in-
duced by kernel functions. Speciﬁcally  we show how to derive new kernel functions

The kernel computes the inner product

k((cid:96))(x  y) = Φ(Φ(...Φ

(y)))

(9)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:96) times

(cid:123)(cid:122)

(x))) · Φ(Φ(...Φ
(cid:96) times

(cid:125)

(cid:124)

which compute the inner product after (cid:96) successive applications of the nonlinear mapping Φ(·). Our
motivation is the following: intuitively  if the base kernel function k(x  y) = Φ(x) · Φ(y) mimics
the computation in a single-layer network  then the iterated mapping in eq. (9) should mimic the
computation in a multilayer network.

3

f2f3fix1x2xj. . . . . . f1fmxdW. . . . . . −10100.51Step (n=0)−10100.51Ramp (n=1)−10100.51Quarter−pipe (n=2)Figure 2: Left: examples from the rectangles-image data set. Right: classiﬁcation error rates on the
test set. SVMs with arc-cosine kernels have error rates from 22.36–25.64%. Results are shown for
kernels of varying degree (n) and levels of recursion ((cid:96)). The best previous results are 24.04% for
SVMs with RBF kernels and 22.50% for deep belief nets [11]. See text for details.

We ﬁrst examine the results of this procedure for widely used kernels. Here we ﬁnd that the iterated
mapping in eq. (9) does not yield particularly interesting results. Consider the two-fold composition
that maps x to Φ(Φ(x)). For linear kernels k(x  y) = x · y  the composition is trivial: we obtain
the identity map Φ(Φ(x)) = Φ(x) = x. For homogeneous polynomial kernels k(x  y) = (x · y)d 
the composition yields:

Φ(Φ(x)) · Φ(Φ(y)) = (Φ(x) · Φ(y))d = ((x · y)d)d = (x · y)d2

.

(10)

The above result is not especially interesting: the kernel implied by this composition is also polyno-
mial  just of higher degree (d2 versus d) than the one from which it was constructed. Likewise  for
RBF kernels k(x  y) = e−λ(cid:107)x−y(cid:107)2  the composition yields:

Φ(Φ(x)) · Φ(Φ(y)) = e−λ(cid:107)Φ(x)−Φ(y)(cid:107)2 = e−2λ(1−k(x y)).

(11)

Though non-trivial  eq. (11) does not represent a particularly interesting computation. Recall that
RBF kernels mimic the computation of soft vector quantizers  with k(x  y) (cid:28) 1 when (cid:107)x−y(cid:107) is
large compared to the kernel width. It is hard to see how the iterated mapping Φ(Φ(x)) would
generate a qualitatively different representation than the original mapping Φ(x).
Next we consider the (cid:96)-fold composition in eq. (9) for arc-cosine kernel functions. We state the
result in the form of a recursion. The base case is given by eq. (3) for kernels of depth (cid:96) = 1 and
degree n. The inductive step is given by:

k(l+1)
n

(x  y) =

1
π

n (x  x) k(l)
k(l)

n (y  y)

Jn

θ((cid:96))
n

 

(12)

where θ((cid:96))
composition. In particular  we can write:

n is the angle between the images of x and y in the feature space induced by the (cid:96)-fold

n = cos−1
θ((cid:96))

n (x  y)
k((cid:96))

n (x  x) k((cid:96))
k((cid:96))

n (y  y)

.

(13)

The recursion in eq. (12) is simple to compute in practice. The resulting kernels mimic the com-
putations in large multilayer threshold networks. Above  for simplicity  we have assumed that the
arc-cosine kernels have the same degree n at every level (or layer) (cid:96) of the recursion. We can also
use kernels of different degrees at different layers. In the next section  we experiment with SVMs
whose kernel functions are constructed in this way.

2.4 Experiments on binary classiﬁcation
We evaluated SVMs with arc-cosine kernels on two challenging data sets of 28 × 28 grayscale pixel
images. These data sets were speciﬁcally constructed to compare deep architectures and kernel
machines [11]. In the ﬁrst data set  known as rectangles-image  each image contains an occluding
rectangle  and the task is to determine whether the width of the rectangle exceeds its height; ex-
amples are shown in Fig. 2 (left). In the second data set  known as convex  each image contains a
white region  and the task is to determine whether the white region is convex; examples are shown

4

(cid:104)

(cid:18)

(cid:104)

(cid:105)n/2

(cid:16)

(cid:17)

(cid:105)−1/2(cid:19)

222426DBN−3SVM−RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter−pipe (n=2)222426DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure2:Left:examplesfromtherectangles-imagedataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom22.36–25.64%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare24.04%forSVMswithRBFkernelsand22.50%fordeepbeliefnets[2].Seetextfordetails.1718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure3:Left:examplesfromtheconvexdataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom17.15–20.51%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare19.13%forSVMswithRBFkernelsand18.63%fordeepbeliefnets[2].Seetextfordetails.2000trainingexamplesasavalidationsettochoosethemarginpenaltyparameter;afterchoosingthisparameterbycross-validation wethenretrainedeachSVMusingallthetrainingexamples.Forreference wealsoreportthebestresultsobtainedpreviouslyfromthreelayerdeepbeliefnets(DBN-3)andSVMswithRBFkernels(SVM-RBF).Thesereferencesarerepresentativeofthecurrentstate-of-the-artfordeepandshallowarchitecturesonthesedatasets.Therightpanelsofﬁgures2and3showthetestseterrorratesfromarccosinekernelsofvaryingdegree(n)andlevelsofrecursion(!).Weexperimentedwithkernelsofdegreen=0 1and2 correspondingtosinglelayerthresholdnetworkswith“step” “ramp” and“quarter-pipe”activationfunctions.Wealsoexperimentedwiththemultilayerkernelsdescribedinsection2.3 composedfromonetosixlevelsofrecursion.Overall theﬁguresshowthatonthesetwodatasets manydifferentarccosinekernelsoutperformthebestresultspreviouslyreportedforSVMswithRBFkernelsanddeepbeliefnets.Wegivemoredetailsontheseexperimentsbelow.Atahighlevel though wenotethatSVMswitharccosinekernelsareverystraightforwardtotrain;unlikeSVMswithRBFkernels theydonotrequiretuningakernelwidthparameter andunlikedeepbeliefnets theydonotrequiresolvingadifﬁcultnonlinearoptimizationorsearchingoverpossiblearchitectures.Inourexperiments wequicklydiscoveredthatthemultilayerkernelsonlyperformedwellwhenn=1kernelswereusedathigher(!>1)levelsintherecursion.Figs.2and3thereforeshowonlythesesetsofresults;inparticular eachgroupofbarsshowsthetesterrorrateswhenaparticularkernel(ofdegreen=0 1 2)wasusedattheﬁrstlayerofnonlinearity whilethen=1kernelwasusedatsuccessivelayers.Wedonothaveaformalexplanationforthiseffect.However recallthatonlythen=1arccosinekernelpreservesthenormofitsinputs:then=0kernelmapsallinputsontoaunithypersphereinfeaturespace whilehigher-order(n>1)kernelsmayinducefeaturespaceswithseverelydistorteddynamicranges.Therefore wehypothesizethatonlyn=1arccosinekernelspreservesufﬁcientinformationaboutthemagnitudeoftheirinputstoworkeffectivelyincompositionwithotherkernels.Finally theresultsonbothdatasetsrevealaninterestingtrend:themultilayerarccosinekernelsoftenperformbetterthantheirsinglelayercounterparts.ThoughSVMsareshallowarchitectures 5Figure 3: Left: examples from the convex data set. Right: classiﬁcation error rates on the test set.
SVMs with arc-cosine kernels have error rates from 17.15–20.51%. Results are shown for kernels
of varying degree (n) and levels of recursion ((cid:96)). The best previous results are 19.13% for SVMs
with RBF kernels and 18.63% for deep belief nets [11]. See text for details.

in Fig. 3 (left). The rectangles-image data set has 12000 training examples  while the convex data
set has 8000 training examples; both data sets have 50000 test examples. These data sets have
been extensively benchmarked by previous authors [11]. Our experiments in binary classiﬁcation
focused on these data sets because in previously reported benchmarks  they exhibited the biggest
performance gap between deep architectures (e.g.  deep belief nets) and traditional SVMs.
We followed the same experimental methodology as previous authors [11]. SVMs were trained using
libSVM (version 2.88) [12]  a publicly available software package. For each SVM  we used the last
2000 training examples as a validation set to choose the margin penalty parameter; after choosing
this parameter by cross-validation  we then retrained each SVM using all the training examples.
For reference  we also report the best results obtained previously from three-layer deep belief nets
(DBN-3) and SVMs with RBF kernels (SVM-RBF). These references appear to be representative of
the current state-of-the-art for deep and shallow architectures on these data sets.
Figures 2 and 3 show the test set error rates from arc-cosine kernels of varying degree (n) and levels
of recursion ((cid:96)). We experimented with kernels of degree n = 0  1 and 2  corresponding to thresh-
old networks with “step”  “ramp”  and “quarter-pipe” activation functions. We also experimented
with the multilayer kernels described in section 2.3  composed from one to six levels of recursion.
Overall  the ﬁgures show that many SVMs with arc-cosine kernels outperform traditional SVMs 
and a certain number also outperform deep belief nets. In addition to their solid performance  we
note that SVMs with arc-cosine kernels are very straightforward to train; unlike SVMs with RBF
kernels  they do not require tuning a kernel width parameter  and unlike deep belief nets  they do not
require solving a difﬁcult nonlinear optimization or searching over possible architectures.
Our experiments with multilayer kernels revealed that these SVMs only performed well when arc-
cosine kernels of degree n = 1 were used at higher ((cid:96) > 1) levels in the recursion. Figs. 2 and
3 therefore show only these sets of results; in particular  each group of bars shows the test error
rates when a particular kernel (of degree n = 0  1  2) was used at the ﬁrst layer of nonlinearity 
while the n = 1 kernel was used at successive layers. We hypothesize that only n = 1 arc-cosine
kernels preserve sufﬁcient information about the magnitude of their inputs to work effectively in
composition with other kernels. Recall that only the n = 1 arc-cosine kernel preserves the norm of
its inputs: the n = 0 kernel maps all inputs onto a unit hypersphere in feature space  while higher-
order (n >1) kernels induce feature spaces with different dynamic ranges.
Finally  the results on both data sets reveal an interesting trend: the multilayer arc-cosine kernels
often perform better than their single-layer counterparts. Though SVMs are (inherently) shallow
architectures  this trend suggests that for these problems in binary classiﬁcation  arc-cosine kernels
may be yielding some of the advantages typically associated with deep architectures.

3 Deep learning

In this section  we explore how to use kernel methods in deep architectures [7]. We show how to train
deep kernel-based architectures by a simple combination of supervised and unsupervised methods.
Using the arc-cosine kernels in the previous section  these multilayer kernel machines (MKMs)
perform very competitively on multiclass data sets designed to foil shallow architectures [11].

5

222426DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure2:Left:examplesfromtherectangles-imagedataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom22.36–25.64%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare24.04%forSVMswithRBFkernelsand22.50%fordeepbeliefnets[2].Seetextfordetails.1718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure3:Left:examplesfromtheconvexdataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom17.15–20.51%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare19.13%forSVMswithRBFkernelsand18.63%fordeepbeliefnets[2].Seetextfordetails.2000trainingexamplesasavalidationsettochoosethemarginpenaltyparameter;afterchoosingthisparameterbycross-validation wethenretrainedeachSVMusingallthetrainingexamples.Forreference wealsoreportthebestresultsobtainedpreviouslyfromthreelayerdeepbeliefnets(DBN-3)andSVMswithRBFkernels(SVM-RBF).Thesereferencesarerepresentativeofthecurrentstate-of-the-artfordeepandshallowarchitecturesonthesedatasets.Therightpanelsofﬁgures2and3showthetestseterrorratesfromarccosinekernelsofvaryingdegree(n)andlevelsofrecursion(!).Weexperimentedwithkernelsofdegreen=0 1and2 correspondingtosinglelayerthresholdnetworkswith“step” “ramp” and“quarter-pipe”activationfunctions.Wealsoexperimentedwiththemultilayerkernelsdescribedinsection2.3 composedfromonetosixlevelsofrecursion.Overall theﬁguresshowthatonthesetwodatasets manydifferentarccosinekernelsoutperformthebestresultspreviouslyreportedforSVMswithRBFkernelsanddeepbeliefnets.Wegivemoredetailsontheseexperimentsbelow.Atahighlevel though wenotethatSVMswitharccosinekernelsareverystraightforwardtotrain;unlikeSVMswithRBFkernels theydonotrequiretuningakernelwidthparameter andunlikedeepbeliefnets theydonotrequiresolvingadifﬁcultnonlinearoptimizationorsearchingoverpossiblearchitectures.Inourexperiments wequicklydiscoveredthatthemultilayerkernelsonlyperformedwellwhenn=1kernelswereusedathigher(!>1)levelsintherecursion.Figs.2and3thereforeshowonlythesesetsofresults;inparticular eachgroupofbarsshowsthetesterrorrateswhenaparticularkernel(ofdegreen=0 1 2)wasusedattheﬁrstlayerofnonlinearity whilethen=1kernelwasusedatsuccessivelayers.Wedonothaveaformalexplanationforthiseffect.However recallthatonlythen=1arccosinekernelpreservesthenormofitsinputs:then=0kernelmapsallinputsontoaunithypersphereinfeaturespace whilehigher-order(n>1)kernelsmayinducefeaturespaceswithseverelydistorteddynamicranges.Therefore wehypothesizethatonlyn=1arccosinekernelspreservesufﬁcientinformationaboutthemagnitudeoftheirinputstoworkeffectivelyincompositionwithotherkernels.Finally theresultsonbothdatasetsrevealaninterestingtrend:themultilayerarccosinekernelsoftenperformbetterthantheirsinglelayercounterparts.ThoughSVMsareshallowarchitectures 51718192021DBN−3SVM−RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter−pipe (n=2)3.1 Multilayer kernel machines

We explored how to train MKMs in stages that involve kernel PCA [13] and feature selection [14] at
intermediate hidden layers and large-margin nearest neighbor classiﬁcation [15] at the ﬁnal output
layer. Speciﬁcally  for (cid:96)-layer MKMs  we considered the following training procedure:

1. Prune uninformative features from the input space.
2. Repeat (cid:96) times:

(a) Compute principal components in the feature space induced by a nonlinear kernel.
(b) Prune uninformative components from the feature space.

3. Learn a Mahalanobis distance metric for nearest neighbor classiﬁcation.

The individual steps in this procedure are well-established methods; only their combination is new.
While many other approaches are worth investigating  our positive results from the above procedure
provide a ﬁrst proof-of-concept. We discuss each of these steps in greater detail below.
Kernel PCA. Deep learning in MKMs is achieved by iterative applications of kernel PCA [13]. This
use of kernel PCA was suggested over a decade ago [16] and more recently inspired by the pre-
training of deep belief nets by unsupervised methods. In MKMs  the outputs (or features) from
kernel PCA at one layer are the inputs to kernel PCA at the next layer. However  we do not strictly
transmit each layer’s top principal components to the next layer; some components are discarded if
they are deemed uninformative. While any nonlinear kernel can be used for the layerwise PCA in
MKMs  arc-cosine kernels are natural choices to mimic the computations in large neural nets.
Feature selection. The layers in MKMs are trained by interleaving a supervised method for feature
selection with the unsupervised method of kernel PCA. The feature selection is used to prune away
uninformative features at each layer in the MKM (including the zeroth layer which stores the raw
inputs).
Intuitively  this feature selection helps to focus the unsupervised learning in MKMs on
statistics of the inputs that actually contain information about the class labels. We prune features
at each layer by a simple two-step procedure that ﬁrst ranks them by estimates of their mutual
information  then truncates them using cross-validation. More speciﬁcally  in the ﬁrst step  we
discretize each real-valued feature and construct class-conditional and marginal histograms of its
discretized values; then  using these histograms  we estimate each feature’s mutual information with
the class label and sort the features in order of these estimates [14]. In the second step  considering
only the ﬁrst w features in this ordering  we compute the error rates of a basic kNN classiﬁer using
Euclidean distances in feature space. We compute these error rates on a held-out set of validation
examples for many values of k and w and record the optimal values for each layer. The optimal w
determines the number of informative features passed onto the next layer; this is essentially the
width of the layer. In practice  we varied k from 1 to 15 and w from 10 to 300; though exhaustive 
this cross-validation can be done quickly and efﬁciently by careful bookkeeping. Note that this
procedure determines the architecture of the network in a greedy  layer-by-layer fashion.
Distance metric learning. Test examples in MKMs are classiﬁed by a variant of kNN classiﬁcation
on the outputs of the ﬁnal layer. Speciﬁcally  we use large margin nearest neighbor (LMNN) clas-
siﬁcation [15] to learn a Mahalanobis distance metric for these outputs  though other methods are
equally viable [17]. The use of LMNN is inspired by the supervised ﬁne-tuning of weights in the
training of deep architectures [18]. In MKMs  however  this supervised training only occurs at the
ﬁnal layer (which underscores the importance of feature selection in earlier layers). LMNN learns a
distance metric by solving a problem in semideﬁnite programming; one advantage of LMNN is that
the required optimization is convex. Test examples are classiﬁed by the energy-based decision rule
for LMNN [15]  which was itself inspired by earlier work on multilayer neural nets [19].

3.2 Experiments on multiway classiﬁcation

We evaluated MKMs on the two multiclass data sets from previous benchmarks [11] that exhibited
the largest performance gap between deep and shallow architectures. The data sets were created from
the MNIST data set [20] of 28 × 28 grayscale handwritten digits. The mnist-back-rand data set was
generated by ﬁlling the image background by random pixel values  while the mnist-back-image data
set was generated by ﬁlling the image background with random image patches; examples are shown
in Figs. 4 and 5. Each data set contains 12000 and 50000 training and test examples  respectively.

6

Figure 4: Left: examples from the mnist-back-rand data set. Right: classiﬁcation error rates on the
test set for MKMs with different kernels and numbers of layers (cid:96). MKMs with arc-cosine kernel
have error rates from 6.36–7.52%. The best previous results are 14.58% for SVMs with RBF kernels
and 6.73% for deep belief nets [11].

Figure 5: Left: examples from the mnist-back-image data set. Right: classiﬁcation error rates on the
test set for MKMs with different kernels and numbers of layers (cid:96). MKMs with arc-cosine kernel
have error rates from 18.43–29.79%. The best previous results are 22.61% for SVMs with RBF
kernels and 16.31% for deep belief nets [11].

We trained MKMs with arc-cosine kernels and RBF kernels in each layer. For each data set  we
initially withheld the last 2000 training examples as a validation set. Performance on this validation
set was used to determine each MKM’s architecture  as described in the previous section  and also
to set the kernel width in RBF kernels  following the same methodology as earlier studies [11].
Once these parameters were set by cross-validation  we re-inserted the validation examples into the
training set and used all 12000 training examples for feature selection and distance metric learning.
For kernel PCA  we were limited by memory requirements to processing only 6000 out of 12000
training examples. We chose these 6000 examples randomly  but repeated each experiment ﬁve
times to obtain a measure of average performance. The results we report for each MKM are the
average performance over these ﬁve runs.
The right panels of Figs. 4 and 5 show the test set error rates of MKMs with different kernels and
numbers of layers (cid:96). For reference  we also show the best previously reported results [11] using
traditional SVMs (with RBF kernels) and deep belief nets (with three layers). MKMs perform sig-
niﬁcantly better than shallow architectures such as SVMs with RBF kernels or LMNN with feature
selection (reported as the case (cid:96) = 0). Compared to deep belief nets  the leading MKMs obtain
slightly lower error rates on one data set and slightly higher error rates on another.
We can describe the architecture of an MKM by the number of selected features at each layer (in-
cluding the input layer). The number of features essentially corresponds to the number of units in
each layer of a neural net. For the mnist-back-rand data set  the best MKM used an n=1 arc-cosine
kernel and 300-90-105-136-126-240 features at each layer. For the mnist-back-image data set  the
best MKM used an n=0 arc-cosine kernel and 300-50-130-240-160-150 features at each layer.
MKMs worked best with arc-cosine kernels of degree n = 0 and n = 1. The kernel of degree n = 2
performed less well in MKMs  perhaps because multiple iterations of kernel PCA distorted the
dynamic range of the inputs (which in turn seemed to complicate the training for LMNN). MKMs
with RBF kernels were difﬁcult to train due to the sensitive dependence on kernel width parameters.
It was extremely time-consuming to cross-validate the kernel width at each layer of the MKM. We
only obtained meaningful results for one and two-layer MKMs with RBF kernels.

7

5678DBN−3Test error rate (%) 0 1    2    3    4   5      Step (n=0) 1    2    3    4   5     Ramp (n=1)            1    2Quarter−pipe (n=2) 1    2  RBF222426DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure2:Left:examplesfromtherectangles-imagedataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom22.36–25.64%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare24.04%forSVMswithRBFkernelsand22.50%fordeepbeliefnets[2].Seetextfordetails.1718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure3:Left:examplesfromtheconvexdataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom17.15–20.51%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare19.13%forSVMswithRBFkernelsand18.63%fordeepbeliefnets[2].Seetextfordetails.2000trainingexamplesasavalidationsettochoosethemarginpenaltyparameter;afterchoosingthisparameterbycross-validation wethenretrainedeachSVMusingallthetrainingexamples.Forreference wealsoreportthebestresultsobtainedpreviouslyfromthreelayerdeepbeliefnets(DBN-3)andSVMswithRBFkernels(SVM-RBF).Thesereferencesarerepresentativeofthecurrentstate-of-the-artfordeepandshallowarchitecturesonthesedatasets.Therightpanelsofﬁgures2and3showthetestseterrorratesfromarccosinekernelsofvaryingdegree(n)andlevelsofrecursion(!).Weexperimentedwithkernelsofdegreen=0 1and2 correspondingtosinglelayerthresholdnetworkswith“step” “ramp” and“quarter-pipe”activationfunctions.Wealsoexperimentedwiththemultilayerkernelsdescribedinsection2.3 composedfromonetosixlevelsofrecursion.Overall theﬁguresshowthatonthesetwodatasets manydifferentarccosinekernelsoutperformthebestresultspreviouslyreportedforSVMswithRBFkernelsanddeepbeliefnets.Wegivemoredetailsontheseexperimentsbelow.Atahighlevel though wenotethatSVMswitharccosinekernelsareverystraightforwardtotrain;unlikeSVMswithRBFkernels theydonotrequiretuningakernelwidthparameter andunlikedeepbeliefnets theydonotrequiresolvingadifﬁcultnonlinearoptimizationorsearchingoverpossiblearchitectures.Inourexperiments wequicklydiscoveredthatthemultilayerkernelsonlyperformedwellwhenn=1kernelswereusedathigher(!>1)levelsintherecursion.Figs.2and3thereforeshowonlythesesetsofresults;inparticular eachgroupofbarsshowsthetesterrorrateswhenaparticularkernel(ofdegreen=0 1 2)wasusedattheﬁrstlayerofnonlinearity whilethen=1kernelwasusedatsuccessivelayers.Wedonothaveaformalexplanationforthiseffect.However recallthatonlythen=1arccosinekernelpreservesthenormofitsinputs:then=0kernelmapsallinputsontoaunithypersphereinfeaturespace whilehigher-order(n>1)kernelsmayinducefeaturespaceswithseverelydistorteddynamicranges.Therefore wehypothesizethatonlyn=1arccosinekernelspreservesufﬁcientinformationaboutthemagnitudeoftheirinputstoworkeffectivelyincompositionwithotherkernels.Finally theresultsonbothdatasetsrevealaninterestingtrend:themultilayerarccosinekernelsoftenperformbetterthantheirsinglelayercounterparts.ThoughSVMsareshallowarchitectures 51718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)222426DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure2:Left:examplesfromtherectangles-imagedataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom22.36–25.64%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare24.04%forSVMswithRBFkernelsand22.50%fordeepbeliefnets[2].Seetextfordetails.1718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure3:Left:examplesfromtheconvexdataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom17.15–20.51%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare19.13%forSVMswithRBFkernelsand18.63%fordeepbeliefnets[2].Seetextfordetails.2000trainingexamplesasavalidationsettochoosethemarginpenaltyparameter;afterchoosingthisparameterbycross-validation wethenretrainedeachSVMusingallthetrainingexamples.Forreference wealsoreportthebestresultsobtainedpreviouslyfromthreelayerdeepbeliefnets(DBN-3)andSVMswithRBFkernels(SVM-RBF).Thesereferencesarerepresentativeofthecurrentstate-of-the-artfordeepandshallowarchitecturesonthesedatasets.Therightpanelsofﬁgures2and3showthetestseterrorratesfromarccosinekernelsofvaryingdegree(n)andlevelsofrecursion(!).Weexperimentedwithkernelsofdegreen=0 1and2 correspondingtosinglelayerthresholdnetworkswith“step” “ramp” and“quarter-pipe”activationfunctions.Wealsoexperimentedwiththemultilayerkernelsdescribedinsection2.3 composedfromonetosixlevelsofrecursion.Overall theﬁguresshowthatonthesetwodatasets manydifferentarccosinekernelsoutperformthebestresultspreviouslyreportedforSVMswithRBFkernelsanddeepbeliefnets.Wegivemoredetailsontheseexperimentsbelow.Atahighlevel though wenotethatSVMswitharccosinekernelsareverystraightforwardtotrain;unlikeSVMswithRBFkernels theydonotrequiretuningakernelwidthparameter andunlikedeepbeliefnets theydonotrequiresolvingadifﬁcultnonlinearoptimizationorsearchingoverpossiblearchitectures.Inourexperiments wequicklydiscoveredthatthemultilayerkernelsonlyperformedwellwhenn=1kernelswereusedathigher(!>1)levelsintherecursion.Figs.2and3thereforeshowonlythesesetsofresults;inparticular eachgroupofbarsshowsthetesterrorrateswhenaparticularkernel(ofdegreen=0 1 2)wasusedattheﬁrstlayerofnonlinearity whilethen=1kernelwasusedatsuccessivelayers.Wedonothaveaformalexplanationforthiseffect.However recallthatonlythen=1arccosinekernelpreservesthenormofitsinputs:then=0kernelmapsallinputsontoaunithypersphereinfeaturespace whilehigher-order(n>1)kernelsmayinducefeaturespaceswithseverelydistorteddynamicranges.Therefore wehypothesizethatonlyn=1arccosinekernelspreservesufﬁcientinformationaboutthemagnitudeoftheirinputstoworkeffectivelyincompositionwithotherkernels.Finally theresultsonbothdatasetsrevealaninterestingtrend:themultilayerarccosinekernelsoftenperformbetterthantheirsinglelayercounterparts.ThoughSVMsareshallowarchitectures 51718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)15202530DBN−3SVM−RBFTest error rate (%) 0 1    2    3    4   5      Step (n=0) 1    2    3    4   5     Ramp (n=1)            1    2Quarter−pipe (n=2) 1    2  RBFWe brieﬂy summarize many results that we lack space to report in full. We also experimented
on multiclass data sets using SVMs with single and multi-layer arc-cosine kernels  as described in
section 2. For multiclass problems  these SVMs compared poorly to deep architectures (both DBNs
and MKMs)  presumably because they had no unsupervised training that shared information across
examples from all different classes. In further experiments on MKMs  we attempted to evaluate the
individual contributions to performance from feature selection and LMNN classiﬁcation. Feature
selection helped signiﬁcantly on the mnist-back-image data set  but only slightly on the mnist-back-
random data set. Finally  LMNN classiﬁcation in the output layer yielded consistent improvements
over basic kNN classiﬁcation provided that we used the energy-based decision rule [15].

4 Discussion

In this paper  we have developed a new family of kernel functions that mimic the computation in
large  multilayer neural nets. On challenging data sets  we have obtained results that outperform pre-
vious SVMs and compare favorably to deep belief nets. More signiﬁcantly  our experiments validate
the basic intuitions behind deep learning in the altogether different context of kernel-based archi-
tectures. A similar validation was provided by recent work on kernel methods for semi-supervised
embedding [7]. We hope that our results inspire more work on kernel methods for deep learning.
There are many possible directions for future work. For SVMs  we are currently experimenting with
arc-cosine kernel functions of fractional and (even negative) degree n. For MKMs  we are hoping
to explore better schemes for feature selection [21  22] and kernel selection [23]. Also  it would be
desirable to incorporate prior knowledge  such as the invariances modeled by convolutional neural
nets [24  4]  though it is not obvious how to do so. These issues and others are left for future work.

A Derivation of kernel function

(cid:90)

In this appendix  we show how to evaluate the multidimensional integral in eq. (1) for the arc-cosine
kernel. Let θ denote the angle between the inputs x and y. Without loss of generality  we can take x
to lie along the w1 axis and y to lie in the w1w2-plane. Integrating out the orthogonal coordinates
of the weight vector w  we obtain the result in eq. (3) where Jn(θ) is the remaining integral:

dw1 dw2 e− 1

2 (w2

1+w2

Jn(θ) =
1 (w1 cos θ + w2 sin θ)n. (14)
Changing variables to u = w1 and v = w1 cos θ+w2 sin θ  we simplify the domain of integration to
the ﬁrst quadrant of the uv-plane:

2) Θ(w1) Θ(w1 cos θ + w2 sin θ) wn

Jn(θ) =

(15)
The prefactor of (sin θ)−1 in eq. (15) is due to the Jacobian. To simplify the integral further  we
4 ). Then  integrating out the radius
adopt polar coordinates u = r cos( ψ
coordinate r  we obtain:

4 ) and v = r sin( ψ

dv e−(u2+v2−2uv cos θ)/(2 sin2 θ) unvn.

2 + π

2 + π

sin θ

0

0

du

(cid:90) ∞

(cid:90) ∞

1

Jn(θ) = n! (sin θ)2n+1

dψ

cosn ψ

(1 − cos θ cos ψ)n+1 .

(16)

To evaluate eq. (16)  we ﬁrst consider the special case n=0. The following result can be derived by
contour integration in the complex plane [25]:

(cid:90) π

2

0

8

(cid:90) π/2

0

dψ

1 − cos θ cos ψ

= π − θ
sin θ

.

(17)

Substituting eq. (17) into our expression for the angular part of the kernel function in eq. (16)  we
recover our earlier claim that J0(θ) = π− θ. Related integrals for the special case n = 0 can also be
found in earlier work [8].For the case n >0  the integral in eq. (16) can be performed by the method
of differentiating under the integral sign. In particular  we note that:

dψ

cosn ψ

(1 − cos θ cos ψ)n+1 =

1
n!

∂n

∂(cos θ)n

dψ

1 − cos θ cos ψ

.

(18)

Substituting eq. (18) into eq. (16)  then appealing to the previous result in eq. (17)  we recover the
expression for Jn(θ) in eq. (4).

(cid:90) π/2

0

(cid:90) π

2

0

References
[1] Y. Bengio and Y. LeCun. Scaling learning algorithms towards AI. MIT Press  2007.
[2] G.E. Hinton  S. Osindero  and Y.W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-

tation  18(7):1527–1554  2006.

[3] G.E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science 

313(5786):504–507  July 2006.

[4] M.A. Ranzato  F.J. Huang  Y.L. Boureau  and Y. LeCun. Unsupervised learning of invariant feature
In Proceedings of the 2007 IEEE Conference on

hierarchies with applications to object recognition.
Computer Vision and Pattern Recognition (CVPR-07)  pages 1–8  2007.

[5] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: deep neural net-
works with multitask learning. In Proceedings of the 25th International Conference on Machine Learning
(ICML-08)  pages 160–167  2008.

[6] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning  to appear 

2009.

[7] J. Weston  F. Ratle  and R. Collobert. Deep learning via semi-supervised embedding. In Proceedings of

the 25th International Conference on Machine Learning (ICML-08)  pages 1168–1175  2008.

[8] C.K.I. Williams. Computation with inﬁnite neural networks. Neural Computation  10(5):1203–1216 

1998.

[9] R.H.R. Hahnloser  H.S. Seung  and J.J. Slotine. Permitted and forbidden sets in symmetric threshold-

linear networks. Neural Computation  15(3):621–638  2003.

[10] R.M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York  Inc.  1996.
[11] H. Larochelle  D. Erhan  A. Courville  J. Bergstra  and Y. Bengio. An empirical evaluation of deep archi-
tectures on problems with many factors of variation. In Proceedings of the 24th International Conference
on Machine Learning (ICML-07)  pages 473–480  2007.

[12] C.C. Chang and C.J. Lin. LIBSVM: a library for support vector machines  2001. Software available at

http://www.csie.ntu.edu.tw/˜cjlin/libsvm.

[13] B. Sch¨olkopf  A. Smola  and K. M¨uller. Nonlinear component analysis as a kernel eigenvalue problem.

Neural Computation  10(5):1299–1319  1998.

[14] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning

Research  3:1157–1182  2003.

[15] K.Q. Weinberger and L.K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation.

Journal of Machine Learning Research  10:207–244  2009.

[16] B. Sch¨olkopf  A. J. Smola  and K.-R. M¨uller. Nonlinear component analysis as a kernel eigenvalue

problem. Technical Report 44  Max-Planck-Institut f¨ur biologische Kybernetik  1996.

[17] J. Goldberger  S. Roweis  G.E. Hinton  and R. Salakhutdinov. Neighbourhood components analysis. In
L.K. Saul  Y. Weiss  and L. Bottou  editors  Advances in Neural Information Processing Systems 17  pages
513–520. MIT Press  2005.

[18] Y. Bengio  P. Lamblin  D. Popovici  and H. Larochelle. Greedy layer-wise training of deep networks. In
B. Sch¨olkopf  J. Platt  and T. Hoffman  editors  Advances in Neural Information Processing Systems 19 
pages 153–160. MIT Press  2007.

[19] S. Chopra  R. Hadsell  and Y. LeCun. Learning a similarity metric discriminatively  with application to
face veriﬁcation. In Proceedings of the 2005 IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR-05)  pages 539–546  2005.

[20] Y. LeCun and C. Cortes. The MNIST database of handwritten digits. http://yann.lecun.com/

exdb/mnist/.

[21] M. Tipping. Sparse kernel principal component analysis. In Advances in Neural Information Processing

Systems 13. MIT Press  2001.

[22] A.J. Smola  O.L. Mangasarian  and B. Sch¨olkopf. Sparse kernel feature analysis. Technical Report 99-04 

University of Wisconsin  Data Mining Institute  Madison  1999.

[23] G. Lanckriet  N. Cristianini  P. Bartlett  L.E. Ghaoui  and M.I. Jordan. Learning the kernel matrix with

semideﬁnite programming. Journal of Machine Learning Research  5:27–72  2004.

[24] Y. LeCun  B. Boser  J.S. Denker  D. Henderson  R.E. Howard  W. Hubbard  and L.D. Jackel. Backpropa-

gation applied to handwritten zip code recognition. Neural Computation  1(4):541–551  1989.

[25] G.F. Carrier  M. Krook  and C.E. Pearson. Functions of a Complex Variable: Theory and Technique.

Society for Industrial and Applied Mathematics  2005.

9

,Kamalika Chaudhuri
Daniel Hsu
Shuang Song
James McInerney
Rajesh Ranganath
David Blei