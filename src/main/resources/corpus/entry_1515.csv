2016,Direct Feedback Alignment Provides Learning in Deep Neural Networks,Artificial neural networks are most commonly trained with the back-propagation algorithm  where the gradient for learning is provided by back-propagating the error  layer by layer  from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact  random feedback weights work evenly well  because the network learns how to make the feedback useful. In this work  the feedback alignment principle is used for training hidden layers more independently from the rest of the network  and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks  completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local  and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout  the method achieves 1.45% error on the permutation invariant MNIST task.,Direct Feedback Alignment Provides Learning in

Deep Neural Networks

Arild Nøkland

Trondheim  Norway

arild.nokland@gmail.com

Abstract

Artiﬁcial neural networks are most commonly trained with the back-propagation
algorithm  where the gradient for learning is provided by back-propagating the error 
layer by layer  from the output layer to the hidden layers. A recently discovered
method called feedback-alignment shows that the weights used for propagating the
error backward don’t have to be symmetric with the weights used for propagation
the activation forward. In fact  random feedback weights work evenly well  because
the network learns how to make the feedback useful. In this work  the feedback
alignment principle is used for training hidden layers more independently from
the rest of the network  and from a zero initial condition. The error is propagated
through ﬁxed random feedback connections directly from the output layer to each
hidden layer. This simple method is able to achieve zero training error even in
convolutional networks and very deep networks  completely without error back-
propagation. The method is a step towards biologically plausible machine learning
because the error signal is almost local  and no symmetric or reciprocal weights
are required. Experiments show that the test performance on MNIST and CIFAR
is almost as good as those obtained with back-propagation for fully connected
networks. If combined with dropout  the method achieves 1.45% error on the
permutation invariant MNIST task.

1

Introduction

For supervised learning  the back-propagation algorithm (BP)  see [2]  has achieved great success in
training deep neural networks. As today  this method has few real competitors due to its simplicity
and proven performance  although some alternatives do exist.
Boltzmann machine learning in different variants are biologically inspired methods for training neural
networks  see [6]  [10] and [5]. The methods use only local available signals for adjusting the weights.
These methods can be combined with BP ﬁne-tuning to obtain good discriminative performance.
Contrastive Hebbian Learning (CHL)  is similar to Boltzmann Machine learning  but can be used
in deterministic feed-forward networks. In the case of weak symmetric feedback-connections it
resembles BP [16].
Recently  target-propagation (TP) was introduced as an biologically plausible training method  where
each layer is trained to reconstruct the layer below [7]. This method does not require symmetric
weights and propagates target values instead of gradients backward.
A novel training principle called feedback-alignment (FA) was recently introduced [9]. The authors
show that the feedback weights used to back-propagate the gradient do not have to be symmetric with
the feed-forward weights. The network learns how to use ﬁxed random feedback weights in order to
reduce the error. Essentially  the network learns how to learn  and that is a really puzzling result.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Back-propagation with asymmetric weights was also explored in [8]. One of the conclusions from
this work is that the weight symmetry constraint can be signiﬁcantly relaxed while still retaining
strong performance.
The back-propagation algorithm is not biologically plausible for several reasons. First  it requires
symmetric weights. Second  it requires separate phases for inference and learning. Third  the learning
signals are not local  but have to be propagated backward  layer-by-layer  from the output units. This
requires that the error derivative has to be transported as a second signal through the network. To
transport this signal  the derivative of the non-linearities have to be known.
All mentioned methods require the error to travel backward through reciprocal connections. This is
biologically plausible in the sense that cortical areas are known to be reciprocally connected [3]. The
question is how an error signal is relayed through an area to reach more distant areas. For BP and FA
the error signal is represented as a second signal in the neurons participating in the forward pass. For
TP the error is represented as a change in the activation in the same neurons. Consider the possibility
that the error in the relay layer is represented by neurons not participating in the forward pass. For
lower layers  this implies that the feedback path becomes disconnected from the forward path  and
the layer is no longer reciprocally connected to the layer above.
The question arise whether a neuron can receive a teaching signal also through disconnected feedback
paths. This work shows experimentally that directly connected feedback paths from the output layer
to neurons earlier in the pathway is sufﬁcient to enable error-driven learning in a deep network. The
requirements are that the feedback is random and the whole network is adapted. The concept is
quite different from back-propagation  but the result is very similar. Both methods seem to produce
features that makes classiﬁcation easier for the layers above.
Figure 1c) and d) show the novel feedback path conﬁgurations that is further explored in this work.
The methods are based on the feedback alignment principle and is named "direct feedback-alignment"
(DFA) and "indirect feedback-alignment" (IFA).

Figure 1: Overview of different error transportation conﬁgurations. Grey arrows indicate activation
paths and black arrows indicate error paths. Weights that are adapted during learning are denoted as
Wi  and weights that are ﬁxed and random are denoted as Bi. a) Back-propagation. b) Feedback-
alignment. c) Direct feedback-alignment. d) Indirect feedback-alignment.

2 Method

Let (x  y) be mini-batches of input-output vectors that we want the network to learn. For simplicity 
assume that the network has only two hidden layers as in Figure 1  and that the target output y is
scaled between 0 and 1. Let the rows in Wi denote the weights connecting the layer below to a
unit in hidden layer i  and let bi be a column vector with biases for the units in hidden layer i. The
activations in the network are then calculated as

a1 = W1x + b1  h1 = f (a1)
a2 = W2h1 + b2  h2 = f (a2)

2

(1)
(2)

ay = W3h2 + b3  ˆy = fy(ay)

(3)

where f () is the non-linearity used in hidden layers and fy() the non-linearity used in the output
layer. If we choose a logistic activation function in the output layer and a binary cross-entropy loss
function  the loss for a mini-batch with size N and the gradient at the output layer e are calculated as

(cid:88)

J = − 1
N

ymn log ˆymn + (1 − ymn) log(1 − ˆymn)

m n

e = δay =

= ˆy − y

∂J
∂ay

(4)

(5)

where m and n are output unit and mini-batch indexes. For the BP  the gradients for hidden layers are
calculated as

∂J
∂a2

3 e) (cid:12) f(cid:48)(a2)  δa1 =

∂J
∂a1

2 δa2) (cid:12) f(cid:48)(a1)

δa2 =

= (W T

(6)
where (cid:12) is an element-wise multiplication operator and f(cid:48)() is the derivative of the non-linearity.
This gradient is also called steepest descent  because it directly minimizes the loss function given the
linearized version of the network. For FA  the hidden layer update directions are calculated as

= (W T

(7)
where Bi is a ﬁxed random weight matrix with appropriate dimension. For DFA  the hidden layer
update directions are calculated as

δa2 = (B2e) (cid:12) f(cid:48)(a2)  δa1 = (B1δa2) (cid:12) f(cid:48)(a1)

δa2 = (B2e) (cid:12) f(cid:48)(a2)  δa1 = (B1e) (cid:12) f(cid:48)(a1)

(8)
where Bi is a ﬁxed random weight matrix with appropriate dimension. If all hidden layers have the
same number of neurons  Bi can be chosen identical for all hidden layers. For IFA  the hidden layer
update directions are calculated as

δa2 = (W2δa1) (cid:12) f(cid:48)(a2)  δa1 = (B1e) (cid:12) f(cid:48)(a1)

(9)
where B1 is a ﬁxed random weight matrix with appropriate dimension. Ignoring the learning rate  the
weight updates for all methods are calculated as

δW1 = −δa1xT   δW2 = −δa2hT

1   δW3 = −ehT

2

(10)

3 Theoretical results

BP provides a gradient that points in the direction of steepest descent in the loss function landscape.
FA provides a different update direction  but experimental results indicate that the method is able
to reduce the error to zero in networks with non-linear hidden units. This is surprising because the
principle is distinct different from steepest descent. For BP  the feedback weights are the transpose of
the forward weights. For FA the feedback weights are ﬁxed  but if the forward weights are adapted 
they will approximately align with the pseudoinverse of the feedback weights in order to make the
feedback useful [9].
The feedback-alignment paper [9] proves that ﬁxed random feedback asymptotically reduces the
error to zero. The conditions for this to happen are freely restated in the following. 1) The network is
linear with one hidden layer. 2) The input data have zero mean and standard deviation one. 3) The
feedback matrix B satisﬁes B+B = I where B+ is the Moore-Penrose pseudo-inverse of B. 4) The
forward weights are initialized to zero. 5) The output layer weights are adapted to minimize the error.
Let’s call this novel principle the feedback alignment principle.
It is not clear how the feedback alignment principle can be applied to a network with several non-
linear hidden layers. The experiments in [9] show that more layers can be added if the error is
back-propagated layer-by-layer from the output.
The following theorem points at a mechanism that can explain the feedback alignment principle.
The mechanism explains how an asymmetric feedback path can provide learning by aligning the
back-propagated and forward propagated gradients with it’s own  under the assumption of constant
update directions for each data point.

3

Theorem 1. Given 2 hidden layers k and k + 1 in a feed-forward neural network where k connects
to k + 1. Let hk and hk+1 be the hidden layer activations. Let the functional dependency between the
layers be hk+1 = f (ak+1)  where ak+1 = W hk + b. Here W is a weight matrix  b is a bias vector
and f () is a non-linearity. Let the layers be updated according to the non-zero update directions
δhk and δhk+1 where δhk
(cid:107)δhk+1(cid:107) are constant for each data point. The negative update
directions will minimize the following layer-wise criterion

(cid:107)δhk(cid:107) and δhk+1

K = Kk + Kk+1 =

k hk

δhT
(cid:107)δhk(cid:107) +

δhT
k+1hk+1
(cid:107)δhk+1(cid:107)

Minimizing K will maximize the gradient maximizing the alignment criterion

where

L = Lk + Lk+1 =

k ck

δhT
(cid:107)δhk(cid:107) +

δhT
k+1ck+1
(cid:107)δhk+1(cid:107)

ck =

∂hk+1
∂hk

δhk+1 = W T (δhk+1 (cid:12) f(cid:48)(ak+1))

ck+1 =

∂hk+1
∂hT
k

δhk = (W δhk) (cid:12) f(cid:48)(ak+1)

If Lk > 0  then is −δhk a descending direction in order to minimize Kk+1.
Proof. Let i be the any of the layers k or k + 1. The prescribed update −δhi is the steepest descent
direction in order to minimize Ki because by using the product rule and the fact that any partial
derivative of

δhi

(cid:107)δhi(cid:107) is zero we get

(cid:20) δhT

(cid:21)

(cid:20) δhi

(cid:21)

i hi
(cid:107)δhi(cid:107)

− ∂Ki
∂hi

= − ∂
∂hi

= − ∂
∂hi

(cid:107)δhi(cid:107) = −αiδhi
Here αi = 1(cid:107)δhi(cid:107) is a positive scalar because δhi is non-zero. Let δai be deﬁned as δai = ∂hi
δhi =
δhi (cid:12) f(cid:48)(ai) where ai is the input to layer i. Using the product rule again  the gradients maximizing
Lk and Lk+1 are

(cid:107)δhi(cid:107) = −0hi − δhi

hi − ∂hi
∂hi

(cid:107)δhi(cid:107)

(15)

δhi

∂ai

(11)

(12)

(13)

(14)

(16)

(17)

(18)

(cid:20) δhi

(cid:21)

(cid:20) δhT

i ci
(cid:107)δhi(cid:107)
∂Lk+1
∂ck+1
∂ck
∂W T

=

(cid:21)

=

∂ck+1
∂W
∂Lk
∂cT
k

∂Li
∂ci

=

∂
∂ci

∂Lk+1

∂W

=

∂Lk
∂W

ci +

∂ci
∂ci

(cid:107)δhi(cid:107)

∂
∂ci
= αk+1(δhk+1 (cid:12) f(cid:48)(ak+1))δhT

δhi
(cid:107)δhi(cid:107) = 0ci +

δhi
(cid:107)δhi(cid:107) = αiδhi

k = αk+1δak+1δhT
k

= (δhk+1 (cid:12) f(cid:48)(ak+1))αkδhT

k = αkδak+1δhT
k

∂W = ∂Lk+1

∂W = ∂Lk

∂W . If we project hi onto δhi we

Ignoring the magnitude of the gradients we have ∂L
can write hi = hT
δW = −δhk+1

= −(δhk+1(cid:12)f(cid:48)(ak+1))hT

i δhi
(cid:107)δhi(cid:107)2 δhi + hi res = αiKiδhi + hi res. For W   the prescribed update is
∂hk+1
∂W
−αkKkδak+1δhT

k = −δak+1hT

k res = −Kk

k − δak+1hT

− δak+1hT
∂Lk
∂W
We can indirectly maximize Lk and Lk+1 by maximizing the component of ∂Lk
∂W in δW by minimizing
Kk. The gradient to minimize Kk is the prescribed update −δhk.
Lk > 0 implies that the angle β between δhk and the back-propagated gradient ck is within 90◦ of
(cid:107)ck(cid:107)(cid:107)δhk(cid:107) = Lk(cid:107)ck(cid:107) > 0 ⇒ |β| < 90◦. Lk > 0 also implies that ck is
each other because cos(β) = cT
non-zero and thus descending. Then δhk will point in a descending direction because a vector within
90◦ of the steepest descending direction will also point in a descending direction.

k = −δak+1(αkKkδhk+hk res)T =

(19)

k δhk

k res

4

It is important to note that the theorem doesn’t tell that the training will converge or reduce any error
to zero  but if the fake gradient is successful in reducing K  then will this gradient also include a
growing component that tries to increase the alignment criterion L.
The theorem can be applied to the output layer and the last hidden layer in a neural network. To
achieve error-driven learning  we have to close the feedback loop. Then we get the update directions
= e and δhk = Gk(e) where Gk(e) is a feedback path connecting the output to the
δhk+1 = ∂J
∂ay
hidden layer. The prescribed update will directly minimize the loss J given hk. If Lk turns positive 
the feedback will provide a update direction δhk = Gk(e) that reduces the same loss. The theorem
can be applied successively to deeper layers. For each layer i  the weight matrix Wi is updated to
minimize Ki+1 in the layer above  and at the same time indirectly make it’s own update direction
δhi = Gi(e) useful.
Theorem 1 suggests that a large class of asymmetric feedback paths can provide a descending gradient
direction for a hidden layer  as long as on average Li > 0. Choosing feedback paths Gi(e)  visiting
every layer on it’s way backward  with weights ﬁxed and random  gives us the FA method. Choosing
direct feedback paths Gi(e) = Bie  with Bi ﬁxed and random  gives us the DFA method. Choosing
a direct feedback path G1(e) = B1e connecting to the ﬁrst hidden layer  and then visiting every
layer on it’s way forward  gives us the IFA method. The experimental section shows that learning is
possible even with indirect feedback like this.
Direct random feedback δhi = Gi(e) = Bie has the advantage that δhi is non-zero for all non-zero e.
This is because a random matrix Bi will have full rank with a probability very close to 1. A non-zero
δhi is a requirement in order to achieve Li > 0. Keeping the feedback static will ensure that this
property is preserved during training. In addition  a static feedback can make it easier to maximize Li
because the direction of δhi is more constant. If the cross-entropy loss is used  and the output target
values are 0 or 1  then the sign of the error ej for a given sample j will not change. This means that
the quantity Bi sign(ej) will be constant during training because both Bi and sign(ej) are constant.
If the task is to classify  the quantity will in addition be constant for all samples within a class. Direct
random feedback will also provide a update direction δhi with a magnitude that only varies with the
magnitude of the error e.
If the forward weights are initialized to zero  then will Li = 0 because the back-propagated error is
zero. This seems like a good starting point when using asymmetric feedback because the ﬁrst update
steps have the possibility to quickly turn this quantity positive. A zero initial condition is however not
a requirement for asymmetric feedback to work. One of the experiments will show that even when
starting from a bad initial condition  direct random and static feedback is able to turn this quantity
positive and reduce the training error to zero.
For FA and BP  the hidden layer growth is bounded by the layers above. If the layers above saturate 
the hidden layer update δhi becomes zero. For DFA  the hidden layer update δhi will be non-zero as
long as the error e is non-zero. To limit the growth  a squashing non-linearity like hyperbolic tangent
or logistic sigmoid seems appropriate. If we add a tanh non-linearity to the hidden layer  the hidden
activation is bounded within [−1  1]. With zero initial weights  hi will be zero for all data points. The
tanh non-linearity will not limit the initial growth in any direction. The experimental results indicate
that this non-linearity is well suited together with DFA.
If the hyperbolic tangent non-linearity is used in the hidden layer  the forward weights can be
initialized to zero. The rectiﬁed linear activation function (ReLU) will not work with zero initial
weights because the error derivative for such a unit is zero when the bias and incoming weights are
all zero.

4 Experimental results

To investigate if DFA learns useful features in the hidden layers  a 3x400 tanh network was trained
on MNIST with both BP and DFA. The input test images and resulting features were visualized using
t-SNE [15]  see Figure 3. Both methods learns features that makes it easier to discriminate between
the classes. At the third hidden layer  the clusters are well separated  except for some stray points.
The visible improvement in separation from input to ﬁrst hidden layer indicates that error DFA is
able to learn useful features also in deeper hidden layers.

5

Figure 2: Left: Error curves for a network pre-trained with a frozen ﬁrst hidden layer. Right: Error
curves for normal training of a 2x800 tanh network on MNIST.

Figure 3: t-SNE visualization of MNIST input and features. Different colors correspond to different
classes. The top row shows features obtained with BP  the bottom row shows features obtained with
DFA. From left to right: input images  ﬁrst hidden layer features  second hidden layer features and
third hidden layer features.

Furthermore  another experiment was performed to see if error DFA is able to learn useful hidden
representations in deeper layers. A 3x50 tanh network was trained on MNIST. The ﬁrst hidden layer
was ﬁxed to random weights  but the 2 hidden layers above were trained with BP for 50 epochs. At
this point  the training error was about 5%. Then  the ﬁrst hidden layer was unfreezed and training
continued with BP. The training error decreased to 0% in about 50 epochs. The last step was repeated 
but this time the unfreezed layer was trained with DFA. As expected because of different update
directions  the error ﬁrst increased  then decreased to 0% after about 50 epochs. The error curves are
presented in Figure2(Left). Even though the update direction provided by DFA is different from the
back-propagated gradient  the resulting hidden representation reduces the error in a similar way.
Several feed-forward networks were trained on MNIST and CIFAR to compare the performance
of DFA with FA and BP. The experiments were performed with the binary cross-entropy loss and
optimized with RMSprop [14]. For the MNIST dropout experiments  learning rate with decay and
training time was chosen based on a validation set. For all other experiments  the learning rate was
roughly optimized for BP and then used for all methods. The learning rate was constant for each
dataset. Training was stopped when training error reached 0.01% or the number of epochs reached
300. A mini-batch size of 64 was used. No momentum or weight decay was used. The input data
was scaled to be between 0 and 1  but for the convolutional networks  the data was whitened. For
FA and DFA  the weights and biases were initialized to zero  except for the ReLU networks. For BP
and/or ReLU  the initial weights and biases were sampled from a uniform distribution in the range

6

√

√
[−1/
√
√
f anin]. The random feedback weights were sampled from a uniform distribution
f anin  1/
in the range [−1/
f anout  1/

f anout].

MODEL

BP
DFA
2.16 ± 0.13%
2.32 ± 0.15% (0.03%)
7x240 Tanh
3.92 ± 0.09% (0.12%)
100x240 Tanh
1.59 ± 0.04%
1.68 ± 0.05%
1x800 Tanh
1.60 ± 0.06%
1.74 ± 0.08%
2x800 Tanh
1.75 ± 0.05%
1.70 ± 0.04%
3x800 Tanh
1.92 ± 0.11%
1.83 ± 0.07% (0.02%)
4x800 Tanh
1.67 ± 0.03%
1.75 ± 0.04%
2x800 Logistic
1.48 ± 0.06%
1.70 ± 0.06%
2x800 ReLU
1.26 ± 0.03% (0.18%)
1.45 ± 0.07% (0.24%)
2x800 Tanh + DO
2x800 Tanh + ADV 1.01 ± 0.08%
1.02 ± 0.05% (0.12%)
Table 1: MNIST test error for back-propagation (BP)  feedback-alignment (FA) and direct feedback-
alignment (DFA). Training error in brackets when higher than 0.01%. Empty ﬁelds indicate no
convergence.

FA
2.20 ± 0.13% (0.02%)
1.68 ± 0.05%
1.64 ± 0.03%
1.66 ± 0.09%
1.70 ± 0.04%
1.82 ± 0.10%
1.74 ± 0.10%
1.53 ± 0.03% (0.18%)
1.14 ± 0.03%

The results on MNIST are summarized in Table 1. For adversarial regularization (ADV)  the
networks were trained on adversarial examples generated by the "fast-sign-method" [4]. For dropout
regularization (DO) [12]  a dropout probability of 0.1 was used in the input layer and 0.5 elsewhere.
For the 7x240 network  target propagation achieved an error of 1.94% [7]. The results for all
three methods are very similar. Only DFA was able to train the deepest network with the simple
initialization used. The best result for DFA matches the best result for BP.

MODEL

BP
45.1 ± 0.7% (2.5%)
1x1000 Tanh
45.1 ± 0.3% (0.2%)
3x1000 Tanh
3x1000 Tanh + DO 42.2 ± 0.2% (36.7%)
22.5 ± 0.4%
CONV Tanh

FA
46.4 ± 0.4% (3.2%)
47.0 ± 2.2% (0.3%)
46.9 ± 0.3% (48.9%)
27.1 ± 0.8% (0.9%)

DFA
46.4 ± 0.4% (3.2%)
47.4 ± 0.8% (2.3%)
42.9 ± 0.2% (37.6%)
26.9 ± 0.5% (0.2%)

Table 2: CIFAR-10 test error for back-propagation (BP)  feedback-alignment (FA) and direct feedback-
alignment (DFA). Training error in brackets when higher than 0.1%.

The results on CIFAR-10 are summarized in Table 2. For the convolutional network the error was
injected after the max-pooling layers. The model was identical to the one used in the dropout paper
[12]  except for the non-linearity. For the 3x1000 network  target propagation achieved an error of
49.29% [7]. For the dropout experiment  the gap between BP and DFA is only 0.7%. FA does not
seem to improve with dropout. For the convolutional network  DFA and FA are worse than BP.

MODEL

BP
71.7 ± 0.2% (38.7%)
1x1000 Tanh
72.0 ± 0.3% (0.2%)
3x1000 Tanh
3x1000 Tanh + DO 69.8 ± 0.1% (66.8%)
51.7 ± 0.2%
CONV Tanh

FA
73.8 ± 0.3% (37.5%)
75.3 ± 0.1% (0.5%)
75.3 ± 0.2% (77.2%)
60.5 ± 0.3%

DFA
73.8 ± 0.3% (37.5%)
75.9 ± 0.2% (3.1%)
73.1 ± 0.1% (69.8%)
59.0 ± 0.3%

Table 3: CIFAR-100 test error for back-propagation (BP)  feedback-alignment (FA) and direct
feedback-alignment (DFA). Training error in brackets when higher than 0.1%.

The results on CIFAR-100 are summarized in Table 3. DFA improves with dropout  while FA does
not. For the convolutional network  DFA and FA are worse than BP.
The above experiments were performed to verify the DFA method. The feedback loops are the
shortest possible  but other loops can also provide learning. An experiment was performed on MNIST

7

to see if a single feedback loop like in Figure 1d)  was able to train a deep network with 4 hidden
layers of 100 neurons each. The feedback was connected to the ﬁrst hidden layer  and all hidden
layers above were trained with the update direction forward-propagated through this loop. Starting
from a random initialization  the training error reduced to 0%  and the test error reduced to 3.9%.

5 Discussion

The experimental results indicate that DFA is able to ﬁt the training data equally good as BP and FA.
The performance on the test set is similar to FA but lagging a little behind BP. For the convolutional
network  BP is clearly the best performer. Adding regularization seems to help more for DFA than
for FA.
Only DFA was successful in training a network with 100 hidden layers. If proper weight initialization
is used  BP is able to train very deep networks as well [13][11]. The reason why BP fails to converge
is probably the very simple initialization scheme used here. Proper initialization might help FA in a
similar way  but this was not investigated any further.
The DFA training procedure has a lot in common with supervised layer-wise pre-training of a deep
network  but with an important difference. If all layers are trained simultaneously  it is the error at the
top of a deep network that drives the learning  not the error in a shallow pre-training network.
If the network above a target hidden layer is not adapted  FA and DFA will not give an improvement
in the loss. This is in contrast to BP that is able to decrease the error even in this case because the
feedback depends on the weights and layers above.
DFA demonstrates a novel application of the feedback alignment principle. The brain may or may not
implement this kind of feedback  but it is a step towards better better understanding mechanisms that
can provide error-driven learning in the brain. DFA shows that learning is possible in feedback loops
where the forward and feedback paths are disconnected. This introduces a large ﬂexibility in how the
error signal might be transmitted. A neuron might receive it’s error signals via a post-synaptic neuron
(BP CHL)  via a reciprocally connected neuron (FA TP)  directly from a pre-synaptic neuron (DFA) 
or indirectly from an error source located several synapses away earlier in the informational pathway
(IFA).
Disconnected feedback paths can lead to more biologically plausible machine learning. If the feedback
signal is added to the hidden layers before the non-linearity  the derivative of the non-linearity does
not have to be known. The learning rule becomes local because the weight update only depends on
the pre-synaptic activity and the temporal derivative of the post-synaptic activity. Learning is not a
separate phase  but performed at the end of an extended forward pass. The error signal is not a second
signal in the neurons participating in the forward pass  but a separate signal relayed by other neurons.
The local update rule can be linked to Spike-Timing-Dependent Plasticity (STDP) believed to govern
synaptic weight updates in the brain  see [1].
Disconnected feedback paths have great similarities with controllers used in dynamical control loops.
The purpose of the feedback is to provide a change in the state that reduces the output error. For a
dynamical control loop  the change is added to the state and propagated forward to the output. For a
neural network  the change is used to update the weights.

6 Conclusion

A biologically plausible training method based on the feedback alignment principle is presented for
training neural networks with error feedback rather than error back-propagation. In this method 
neither symmetric weights nor reciprocal connections are required. The error paths are short and
enables training of very deep networks. The training signals are local or available at most one synapse
away. No weight initialization is required.
The method was able to ﬁt the training set on all experiments performed on MNIST  Cifar-10 and
Cifar-100. The performance on the test sets lags a little behind back-propagation.
Most importantly  this work suggests that the restriction enforced by back-propagation and feedback-
alignment  that the backward pass have to visit every neuron from the forward pass  can be discarded.
Learning is possible even when the feedback path is disconnected from the forward path.

8

References
[1] Yoshua Bengio  Dong-Hyun Lee  Jörg Bornschein  Thomas Mesnard  and Zhouhan Lin. Towards

biologically plausible deep learning. CoRR  abs/1502.04156  2015.

[2] R. J. Williams D. E. Rumelhart  G. E. Hinton. Learning internal representations by error

propagation. Nature  323:533–536  1986.

[3] Charles D Gilbert and Wu Li. Top-down inﬂuences on visual processing. Nature Reviews

Neuroscience  14(5):350–363  2013.

[4] Ian J. Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adver-

sarial examples. CoRR  abs/1412.6572  2014.

[5] Geoffrey E. Hinton  Simon Osindero  and Yee Whye Teh. A fast learning algorithm for deep

belief nets. Neural Computation  18(7):1527–1554  2006.

[6] Geoffrey E. Hinton and Terrence J. Sejnowski. Optimal Perceptual Inference. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition  1983.

[7] Dong-Hyun Lee  Saizheng Zhang  Asja Fischer  and Yoshua Bengio. Difference target propaga-
tion. In ECML/PKDD (1)  Machine Learning and Knowledge Discovery in Databases  pages
498–515. Springer International Publishing  2015.

[8] Qianli Liao  Joel Z. Leibo  and Tomaso A. Poggio. How important is weight symmetry in

backpropagation? CoRR  abs/1510.05067  2015.

[9] Timothy P. Lillicrap  Daniel Cownden  Douglas B. Tweed  and Colin J. Akerman. Random

feedback weights support learning in deep neural networks. CoRR  abs/1411.0247  2014.

[10] Ruslan Salakhutdinov and Geoffrey E. Hinton. Deep boltzmann machines. In Proceedings of
the Twelfth International Conference on Artiﬁcial Intelligence and Statistics  AISTATS 2009 
volume 5 of JMLR Proceedings  pages 448–455. JMLR.org  2009.

[11] Andrew M. Saxe  James L. McClelland  and Surya Ganguli. Exact solutions to the nonlinear

dynamics of learning in deep linear neural networks. CoRR  abs/1312.6120  2013.

[12] Nitish Srivastava  Geoffrey E. Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdi-
nov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine
Learning Research  15(1):1929–1958  2014.

[13] David Sussillo. Random walks: Training very deep nonlinear feed-forward networks with smart

initialization. CoRR  abs/1412.6558  2014.

[14] T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of

its recent magnitude. COURSERA: Neural Networks for Machine Learning 4  2012.

[15] L.J.P. van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-sne. Journal

of Machine Learning Research  9:2579–2605  2008.

[16] Xiaohui Xie and H. Sebastian Seung. Equivalence of backpropagation and contrastive hebbian

learning in a layered network. Neural Computation  15(2):441–454  2003.

9

,Michael Hughes
Erik Sudderth