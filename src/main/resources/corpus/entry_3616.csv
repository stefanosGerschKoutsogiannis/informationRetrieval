2013,Multiclass Total Variation Clustering,Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks  their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches.,Multiclass Total Variation Clustering

Xavier Bresson

University of Lausanne
Lausanne  Switzerland

xavier.bresson@unil.ch

David Uminsky

University of San Francisco
San Francisco  CA 94117
duminsky@usfca.edu

Thomas Laurent

Loyola Marymount University

Los Angeles  CA 90045
tlaurent@lmu.edu

James H. von Brecht

University of California  Los Angeles

Los Angeles  CA 90095
jub@math.ucla.edu

Abstract

Ideas from the image processing literature have recently motivated a new set of
clustering algorithms that rely on the concept of total variation. While these al-
gorithms perform well for bi-partitioning tasks  their recursive extensions yield
unimpressive results for multiclass clustering tasks. This paper presents a general
framework for multiclass total variation clustering that does not rely on recursion.
The results greatly outperform previous total variation algorithms and compare
well with state-of-the-art NMF approaches.

1

Introduction

Many clustering models rely on the minimization of an energy over possible partitions of the data
set. These discrete optimizations usually pose NP-hard problems  however. A natural resolution
of this issue involves relaxing the discrete minimization space into a continuous one to obtain an
easier minimization procedure. Many current algorithms  such as spectral clustering methods or
non-negative matrix factorization (NMF) methods  follow this relaxation approach.
A fundamental problem arises when using this approach  however; in general the solution of the
relaxed continuous problem and that of the discrete NP-hard problem can differ substantially. In
other words  the relaxation is too loose. A tight relaxation  on the other hand  has a solution that
closely matches the solution of the original discrete NP-hard problem. Ideas from the image pro-
cessing literature have recently motivated a new set of algorithms [17  18  11  12  4  15  3  2  13  10]
that can obtain tighter relaxations than those used by NMF and spectral clustering. These new algo-
rithms all rely on the concept of total variation. Total variation techniques promote the formation of
sharp indicator functions in the continuous relaxation. These functions equal one on a subset of the
graph  zero elsewhere and exhibit a non-smooth jump between these two regions. In contrast to the
relaxations employed by spectral clustering and NMF  total variation techniques therefore lead to
quasi-discrete solutions that closely resemble the discrete solution of the original NP-hard problem.
They provide a promising set of clustering tools for precisely this reason.
Previous total variation algorithms obtain excellent results for two class partitioning problems
[18  11  12  3] . Until now  total variation techniques have relied upon a recursive bi-partitioning
procedure to handle more than two classes. Unfortunately  these recursive extensions have yet to
produce state-of-the-art results. This paper presents a general framework for multiclass total varia-
tion clustering that does not rely on a recursive procedure. Speciﬁcally  we introduce a new discrete
multiclass clustering model  its corresponding continuous relaxation and a new algorithm for opti-
mizing the relaxation. Our approach also easily adapts to handle either unsupervised or transductive

1

clustering tasks. The results signiﬁcantly outperform previous total variation algorithms and com-
pare well against state-of-the-art approaches [19  20  1]. We name our approach Multiclass Total
Variation clustering (MTV clustering).

2 The Multiclass Balanced-Cut Model
Given a weighted graph G = (V  W ) we let V = {x1  . . .   xN} denote the vertex set and W :=
{wij}1≤i j≤N denote the non-negative  symmetric similarity matrix. Each entry wij of W encodes
the similarity  or lack thereof  between a pair of vertices. The classical balanced-cut (or  Cheeger
cut) [7  8] asks for a partition of V = A ∪ Ac into two disjoint sets that minimizes the set energy

Bal(A) :=

Cut(A  Ac)
min{|A| |Ac|} =

xi∈A xj∈Ac wij
min{|A| |Ac|} .

(1)

A simple rationale motivates this model: clusters should exhibit similarity between data points 
which is reﬂected by small values of Cut(A  Ac)  and also form an approximately equal sized parti-
tion of the vertex set. Note that min{|A| |Ac|} attains its maximum when |A| = |Ac| = N/2  so that
for a given value of Cut(A  Ac) the minimum occurs when A and Ac have approximately equal size.
We generalize this model to the multiclass setting by pursuing the same rationale. For a given
number of classes R (that we assume to be known) we formulate our generalized balanced-cut
problem as

(cid:80)

R(cid:88)

r=1

Minimize

Cut(Ar  Ac
r)
min{λ|Ar| |Ac
r|}



(P)

over all disjoint partitions Ar ∩ As = ∅  A1 ∪ ··· ∪ AR = V of the vertex set.

In this model the parameter λ controls the sizes of the sets Ar in the partition. Previous work [4]
has used λ = 1 to obtain a multiclass energy by a straightforward sum of the two-class balanced-cut
terms (1). While this follows the usual practice  it erroneously attempts to enforce that each set in
the partition occupy half of the total number of vertices in the graph. We instead select the parameter
λ to ensure that each of the classes approximately occupy the appropriate fraction 1/R of the total
r| = N − |Ar| 
number of vertices. As the maximum of min{λ|Ar| |Ac
we see that λ = R − 1 is the proper choice.
This general framework also easily incorporates a priori known information  such as a set of labels
for transductive learning. If Lr ⊂ V denotes a set of data points that are a priori known to belong
to class r then we simply enforce Lr ⊂ Ar in the deﬁnition of an allowable partition of the vertex
set. In other words  any allowable disjoint partition Ar ∩ As = ∅  A1 ∪ ··· ∪ AR = V must also
respect the given set of labels.

r|} occurs when λ|Ar| = |Ac

3 Total Variation and a Tight Continuous Relaxation

We derive our continuous optimization by relaxing the set energy (P) to the continuous energy

E(F ) =

(cid:107)fr(cid:107)T V

(cid:107)fr − medλ(fr)(cid:107) 1 λ

.

(2)

Here F := [f1  . . .   fR] ∈ MN×R([0  1]) denotes the N × R matrix that contains in its columns the
relaxed optimization variables associated to the R clusters. A few deﬁnitions will help clarify the
meaning of this formula. The total variation (cid:107)f(cid:107)T V of a vertex function f : V → R is deﬁned by

(cid:107)f(cid:107)T V =

wij|f (xi) − f (xj)|.

(3)

R(cid:88)

r=1

n(cid:88)

n(cid:88)

i=1

j=1

Alternatively  if we view a vertex function f as a vector (f (x1)  . . .   f (xN ))t ∈ RN then we can
write

(4)

(cid:107)f(cid:107)T V := (cid:107)Kf(cid:107)1.

2

Here K ∈ MM×N (R) denotes the gradient matrix of a graph with M edges and N vertices. Each
row of K corresponds to an edge and each column corresponds to a vertex. For any edge (i  j) in
the graph the corresponding row in the matrix K has an entry wij in the column corresponding to
the ith vertex  an entry −wij in the column corresponding to the jth vertex and zeros otherwise.
To make sense of the remainder of (2) we must introduce the asymmetric (cid:96)1-norm. This variant of
the classical (cid:96)1-norm gives different weights to positive and negative values:

(cid:107)f(cid:107)1 λ =

|f (xi)|λ

where

|t|λ =

(5)

(cid:26)λt

−t

if t ≥ 0
if t < 0.

n(cid:88)

i=1

Finally we deﬁne the λ-median (or quantile)  denoted medλ(f )  as:

medλ(f ) = the (k + 1)st largest value in the range of f  where k = (cid:98)N/(λ + 1)(cid:99).

(6)
These deﬁnitions  as well as the relaxation (2) itself  were motivated by the following theorem. Its
proof  in the supplementary material  relies only the three preceding deﬁnitions and some simple
algebra.
Theorem 1. If f = 1A is the indicator function of a subset A ⊂ V then

(cid:107)f(cid:107)T V

(cid:107)f − medλ(f )(cid:107) 1 λ

=

2 Cut(A  Ac)
min{λ|A| |Ac|} .

The preceding theorem allows us to restate the original set optimization problem (P) in the equivalent
discrete form

R(cid:88)

r=1

Minimize

(cid:107)fr(cid:107)T V

(cid:107)fr − medλ(fr)(cid:107) 1 λ

over non-zero functions f1  . . .   fR : V → {0  1} such that f1 + . . . + fR = 1V .
Indeed  since the non-zero functions fr can take only two values  zero or one  they must deﬁne indi-
cator functions of some nonempty set. The simplex constraint f1 + . . . + fR = 1V then guarantees
that the sets Ar := {xi ∈ V : fr(xi) = 1} form a partition of the vertex set. We obtain the relaxed
version (P-rlx) of (P’) in the usual manner by allowing fr ∈ [0  1] to have a continuous range. This
yields



(P’)

(P-rlx)





R(cid:88)

r=1

Minimize

(cid:107)fr(cid:107)T V

(cid:107)fr − medλ(fr)(cid:107) 1 λ

over functions f1  . . .   fR : V → [0  1] such that f1 + . . . + fR = 1V .

The following two points form the foundation on which total variation clustering relies:
1 — As the next subsection details  the total variation terms give rise to quasi-indicator functions.
That is  the relaxed solutions [f1  . . .   fR] of (P-rlx) mostly take values near zero or one and exhibit
a sharp  non-smooth transition between these two regions. Since these quasi-indicator functions es-
sentially take values in the discrete set {0  1} rather than the continuous interval [0  1]  solving (P-rlx)
is almost equivalent to solving either (P) or (P’). In other words  (P-rlx) is a tight relaxation of (P).
2 — Both functions f (cid:55)→ (cid:107)f(cid:107)T V and f (cid:55)→ (cid:107)f − medλ(f )(cid:107)1 λ are convex. The simplex constraint
in (P-rlx) is also convex. Therefore solving (P-rlx) amounts to minimizing a sum of ratios of convex
functions with convex constraints. As the next section details  this fact allows us to use machinery
from convex analysis to develop an efﬁcient  novel algorithm for such problems.

3.1 The Role of Total Variation in the Formation of Quasi-Indicator Functions

To elucidate the precise role that the total variation plays in the formation of quasi-indicator func-
tions  it proves useful to consider a version of (P-rlx) that uses a spectral relaxation in place of the
total variation:

R(cid:88)

r=1

Minimize

(cid:107)fr(cid:107)Lap

(cid:107)fr − medλ(fr)(cid:107) 1 λ

over functions f1  . . .   fR : V → [0  1] such that f1 + . . . + fR = 1V

(P-rlx2)

3

Lap =(cid:80)n

Here (cid:107)f(cid:107)2
i=1 wij|f (xi) − f (xj)|2 denotes the spectral relaxation of Cut(A  Ac); it equals
(cid:104)f  Lf(cid:105) if L denotes the unnormalized graph Laplacian matrix. Thus problem (P-rlx2) relates
to spectral clustering (and therefore NMF [9]) with a positivity constraint. Note that the only
difference between (P-rlx2) and (P-rlx) is that the exponent 2 appears in (cid:107) · (cid:107)Lap while the ex-
ponent 1 appears in the total variation. This simple difference of exponent has an important conse-
quence for the tightness of the relaxations. Figure 1 presents a simple example that illuminates this
difference.
If we bi-partition the depicted graph  i.e. a line with 20 vertices and edge weights
wi i+1 = 1  then the optimal cut lies between vertex 10 and vertex 11 since this gives a per-
fectly balanced cut. Figure 1(a) shows the vertex function f1 generated by (P-rlx) while ﬁgure
1(b) shows the one generated by (P-rlx2). Observe that the solution of the total variation model
coincides with the indicator function of the desired cut whereas the the spectral model prefers its
smoothed version. Note that both functions in ﬁgure 1a) and 1b) have exactly the same total vari-
ation (cid:107)f(cid:107)T V = |f (x1) − f (x2)| + ··· + |f (x19) − f (x20)| = f (x1) − f (x20) = 1 since both
functions are monotonic. The total variation model will therefore prefer the sharp indicator function
since it differs more from its λ-median than the smooth indicator function. Indeed  the denominator
(cid:107)fr − medλ(fr)(cid:107)1 λ is larger for the sharp indicator function than for the smooth one. A differ-
ent scenario occurs when we replace the exponent one in (cid:107) · (cid:107)T V by an exponent two  however. As
(cid:107)f(cid:107)2
Lap = |f (x1)−f (x2)|2 +···+|f (x19)−f (x20)|2 and t2 < t when t < 1 it follows that(cid:107)f(cid:107)Lap
is much smaller for the smooth function than for the sharp one. Thus the spectral model will prefer
the smooth indicator function despite the fact that it differs less from its λ-median. We therefore
recognize the total variation as the driving force behind the formation of sharp indicator functions.

(a)

(b)

Figure 1: Top: The graph used for both relaxations. Bottom left: the solution given by the total
variation relaxation. Bottom right: the solution given by the spectral relaxation. Position along the
x-axis = vertex number  height along the y-axis = value of the vertex function.

This heuristic explanation on a simple  two-class example generalizes to the multiclass case and
to real data sets (see ﬁgure 2). In simple terms  quasi-indicator functions arise due to the fact that
the total variation of a sharp indicator function equals the total variation of a smoothed version of
the same indicator function. The denominator (cid:107)fr − medλ(fr)(cid:107)1 λ then measures the deviation of
these functions from their λ-median. A sharp indicator function deviates more from its median than
does its smoothed version since most of its values concentrate around zero and one. The energy
is therefore much smaller for a sharp indicator function than for a smooth indicator function  and
consequently the total variation clustering energy always prefers sharp indicator functions to smooth
ones. For bi-partitioning problems this fact is well-known. Several previous works have proven that
the relaxation is exact in the two-class case; that is  the total variation solution coincides with the
solution of the original NP-hard problem [8  18  3  5].
Figure 2 illustrates the result of the difference between total variation and NMF relaxations on the
data set OPTDIGITS  which contains 5620 images of handwritten numerical digits. Figure 2(a)
shows the quasi-indicator function f4 obtained  before thresholding  by our MTV algorithm while
2(b) shows the function f4 obtained from the NMF algorithm of [1]. We extract the portion of each
function corresponding to the digits four and nine  then sort and plot the result. The MTV relaxation
leads a sharp transition between the fours and the nines while the NMF relaxation leads to a smooth
transition.

4

Figure 2: Left: Solution f4 from our MTV algorithm (before thresholding) plotted over the fours
and nines. Right: Solution f4 from LSD [1] plotted over the fours and nines.

3.2 Transductive Framework

From a modeling point-of-view  the presence of transductive labels poses no additional difﬁculty. In
addition to the simplex constraint

F ∈ Σ :=

F ∈ MN×R([0  1]) : fr(xi) ≥ 0 

fr(xi) = 1

(7)

(cid:40)

R(cid:88)

r=1

required for unsupervised clustering we also impose the set of labels as a hard constraint.
If
L1  . . .   LR denote the R vertex subsets representing the labeled points  so that xi ∈ Lr means
xi belongs to class r  then we may enforce these labels by restricting F to lie in the subset
F ∈ Λ := {F ∈ MN×R([0  1]) : ∀r  (f1(xi)  . . .   fR(xi)) = er ∀ xi ∈ Lr } .

(8)
Here er denotes the row vector containing a one in the rth location and zeros elsewhere. Our model
for transductive classiﬁcation then aims to solve the problem

(cid:41)

(cid:41)

if f (xi) > medλ(f )
if f (xi) = medλ(f )
if f (xi) < medλ(f )

where

5

R(cid:88)

r=1

Minimize

(cid:107)fr(cid:107)T V

(cid:107)fr − medλ(fr)(cid:107) 1 λ

over matrices F ∈ Σ ∩ Λ.

(P-trans)

Note that Σ ∩ Λ also deﬁnes a convex set  so this minimization remains a sum of ratios of convex
functions subject to a convex constraint. Transductive classiﬁcation therefore poses no additional
algorithmic difﬁculty  either. In particular  we may use the proximal splitting algorithm detailed in
the next section for both unsupervised and transductive classiﬁcation tasks.

4 Proximal Splitting Algorithm

This section details our proximal splitting algorithm for ﬁnding local minimizers of a sum of ratios
of convex functions subject to a convex constraint. We start by showing in the ﬁrst subsection that
the functions

(9)
involved in (P-rlx) or (P-trans) are indeed convex. We also give an explicit formula for a subdiffer-
ential of B since our proximal splitting algorithm requires this in explicit form. We then summarize
a few properties of proximal operators before presenting the algorithm.

and B(f ) := (cid:107)f − medλ(f )1(cid:107)1 λ

T (f ) := (cid:107)f(cid:107)T V

4.1 Convexity  Subgradients and Proximal Operators
Recall that we may view each function f : V → R as a vector in RN with f (xi) as the ith component
of the vector. We may then view T and B as functions from RN to R. The next theorem states that
both B and T deﬁne convex functions on RN and furnishes an element v ∈ ∂B(f ) by means of an
easily computable formula. The formula for the subdifferential generalizes a related result for the
symmetric case [11] to the asymmetric setting. We provide its proof in the supplementary material.
Theorem 2. The functions B and T are convex. Moreover  given f ∈ RN the vector v ∈ RN
deﬁned below belongs to ∂B(f ):

n0 = |{xi ∈ V : f (xi) = medλ(f )}|

n− = |{xi ∈ V : f (xi) < medλ(f )}|
n+ = |{xi ∈ V : f (xi) > medλ(f )}|

λ

n−−λn+
−1

n0

v(xi) =

In the above theorem ∂B(f ) denotes the subdifferential of B at f and v ∈ ∂B(f ) denotes a subgra-
dient. Given a convex function A : RN → R  the proximal operator of A is deﬁned by

proxA(g) := argmin
f∈RN

A(f ) +

||f − g||2
2.

1
2

(10)

If we let δC denote the barrier function of the convex set C  that is

δC(f ) = 0 if f ∈ C and δC(f ) = +∞ if f /∈ C 

then we easily see that proxδC is simply the least-squares projection on C 
proxδC
mapping from RN to RN that generalizes the least-squares projection onto a convex set.

(f ) = projC(f ) := argmin
g∈C

2||f − g||2

1

(11)
in other words 
2. In this manner the proximal operator deﬁnes a

4.2 The Algorithm

We can rewrite the problem (P-rlx) or (P-trans) as

Minimize δC(F ) +

E(fr) over all matrices F = [f1  . . .   fr] ∈ MN×R

(12)

r=1

where E(fr) = T (fr)/B(fr) denotes the energy of the quasi-indicator function of the rth cluster.
The set C = Σ or C = Σ ∩ Λ is the convex subset of MN×R that encodes the simplex constraint
(7) or the simplex constraint with labels. The corresponding function δC(F )  deﬁned in (11)  is
the barrier function of the desired set. Beginning from an initial iterate F 0 ∈ C we propose the
following proximal splitting algorithm:

F k+1 := proxT k+δC

(F k + ∂Bk(F k)).

(13)

R(cid:88)

R(cid:88)

Here T k(F ) and Bk(F ) denote the convex functions

T k(F ) :=

ck
r T (fr)

Bk(F ) :=

the constants (ck

r   dk

r ) are computed using the previous iterate

r=1

R(cid:88)

r=1

dk
r B(fr) 

r = ∆k/B(f k
ck
r )

and

dk
r = ∆kE(f k

r )/B(f k
r )

r ) yields
and ∆k denotes the timestep for the current iteration. This choice of the constants (ck
Bk(F k) = T k(F k)  and this fundamental property allows us to derive (see supplementary material)
the energy descent estimate:
Theorem 3 (Estimate of the energy descent). Each of the F k belongs to C  and if Bk

r   dk

r (cid:54)= 0 then

R(cid:88)

(cid:0)Ek

Bk+1
r
Bk
r

r − Ek+1

r

(cid:1) ≥ (cid:107)F k − F k+1(cid:107)2

∆k

(14)

where Bk

r   Ek

r=1
r stand for B(f k

r )  E(f k

r ).

Inequality (14) states that the energies of the quasi-indicator functions (as a weighted sum) decrease
at every step of the algorithm. It also gives a lower bound for how much these energies decrease. As
the algorithm progress and the iterates stabilize the ratio Bk+1
r converges to 1  in which case
the sum  rather than a weighted sum  of the individual cluster energies decreases.
Our proximal splitting algorithm (13) requires two steps. The ﬁrst step requires computing Gk =
F k + ∂Bk(F k)  and this is straightforward since theorem 2 provides the subdifferential of B  and
therefore of Bk  through an explicit formula. The second step requires computing proxT k+δC
(Gk) 
which seems daunting at a ﬁrst glance. Fortunately  minimization problems of this form play an
important role in the image processing literature. Recent years have therefore produced several fast
and accurate algorithms for computing the proximal operator of the total variation. As T k + δC con-
sists of a weighted sum of total variation terms subject to a convex constraint  we can readily adapt

/Bk

r

6

these algorithms to compute the second step of our algorithm efﬁciently. In this work we use the
primal-dual algorithm of [6] with acceleration. This relies on a proper uniformly convex formulation
of the proximal minimization  which we detail completely in the supplementary material.
(Gk) produces a sequence of approximate
The primal-dual algorithm we use to compute proxT k+δC
solutions by means of an iterative procedure. A stopping criterion is therefore needed to indicate
when the current iterate approximates the actual solution proxT k+δC
(Gk) sufﬁciently. Ideally  we
would like to terminate F k+1 ≈ proxT k+δC
(Gk) in such a manner so that the energy descent
property (14) still holds and F k+1 always satisﬁes the required constraints. In theory we cannot
guarantee that the energy estimate holds for an inexact solution. We may note  however  that a
slightly weaker version of the energy estimate (14)

R(cid:88)

r=1

(cid:0)Ek

Bk+1
r
Bk
r

r − Ek+1

r

(cid:1) ≥ (1 − )

(cid:107)F k − F k+1(cid:107)2

∆k

(15)

holds after a ﬁnite number of iterations of the inner minimization. Moreover  this weaker version
still guarantees that the energies of the quasi-indicator functions decrease as a weighted sum in
exactly the same manner as before. In this way we can terminate the inner loop adaptively: we solve
F k+1 ≈ proxT k+δC
(Gk) less precisely when F k+1 lies far from a minimum and more precisely as
the sequence {F k} progresses. This leads to a substantial increase in efﬁciency of the full algorithm.
Our implementation of the proximal splitting algorithm also guarantees that F k+1 always satisﬁes
the required constraints. We accomplish this task by implementing the primal-dual algorithm in
such a way that each inner iteration always satisﬁes the constraints. This requires computing the
projection projC(F ) exactly at each inner iteration. The overall algorithm remains efﬁcient provided
we can compute this projection quickly. When C = Σ the algorithm [14] performs the required
projection in at most R steps. When C = Σ∩ Λ the computational effort actually decreases  since in
this case the projection consists of a simplex projection on the unlabeled points and straightforward
assignment on the labeled points. Overall  each iteration of the algorithm scales like O(N R2) +
O(M R) + O(RN log(N )) for the simplex projection  application of the gradient matrix and the
computation of the balance terms  respectively.
We may now summarize the full algorithm  including the proximal operator computation. In prac-
R} and any small  work well  so we present the
tice we ﬁnd the choices ∆k = max{Bk
algorithm with these choices. Recall the matrix K in (4) denotes the gradient matrix of the graph.

1   . . .   Bk

(cid:105)

¯F = F

0(τ ∆2L2)−1
B(f1)   . . .   ∆

B(fR)

Algorithm 1 Proximal Splitting Algorithm

Input: F ∈ C  P = 0  L = ||K||2  τ = L−1   = 10−3
while loop not converged do

(cid:104) E(f1)

//Perform outer step Gk = F k + ∂Bk(F k)
∆ = maxr B(fr) ∆0 = minr B(fr)
DE = diag
V = ∆[∂B(f1)  . . .   ∂B(fR)]DE (using theorem 2)
G = F + V
//Perform F k+1 ≈ proxT k+δC
while (15) fails do

B(f1)   . . .   E(fR)

(cid:104) ∆

DB = diag

σ = ∆2

(cid:105)

B(fR)

˜P = P + σK ¯F DB
˜F = F − τ K tP DB F = ( ˜F + τ G)/(1 + τ )
√
θ = 1/
end while

σ = σ/θ

τ = θτ

1 + 2τ

end while

(Gk) until energy estimate holds

P = ˜P / max{| ˜P|  1} (both operations entriwise)

Fold = F

F = projC(F )
¯F = (1 + θ)F − θFold

5 Numerical Experiments

We now demonstrate the MTV algorithm for unsupervised and transductive clustering tasks. We
selected six standard  large-scale data sets as a basis of comparison. We obtained the ﬁrst data set

7

(4MOONS) and its similarity matrix from [4] and the remaining ﬁve data sets and matrices (WE-
BKB4  OPTDIGITS  PENDIGITS  20NEWS  MNIST) from [19]. The 4MOONS data set contains
4K points while the remaining ﬁve contain 4.2K  5.6K  11K  20K and 70K points  respectively.
Our ﬁrst set of experiments compares our MTV algorithm against other unsupervised approaches.
We compare against two previous total variation algorithms [11  3]  which rely on recursive bi-
partitioning  and two top NMF algorithms [1  19]. We use the normalized Cheeger cut versions
of [11] and [3] with default parameters. We used the code available from [19] to test each NMF
algorithm. The non-recursive NMF algorithms (LSD [1]  NMFR [19]) received two types of initial
data: (a) the deterministic data used in [19]; (b) a random procedure leveraging normalized-cut [16].
Procedure (b) ﬁrst selects one data point uniformly at random from each computed NCut cluster 
then sets fr equal to one at the data point drawn from the rth cluster and zero otherwise. We then
propagate this initial stage by replacing each fr with (I +L)−1fr where L denotes the unnormalized
graph Laplacian. Finally  to aid the NMF algorithms  we add a small constant 0.2 to the result (each
performed better than without adding this constant). For MTV we use 30 random trials of (b) then
report the cluster purity (c.f. [19] for a deﬁnition of purity) of the solution with the lowest discrete
energy (P). We then use each NMF with exactly the same initial conditions and report simply the
highest purity achieved over all 31 runs. This biases the results in favor of the NMF algorithms.
Due to the non-convex nature of these algorithms  the random initialization gave the best results and
signiﬁcantly improved upon previously reported results of LSD in particular. For comparison with
[19]  initialization (a) is followed by 10 000 iterations of each NMF algorithm. Each trial of (b) is
followed by 2000 iterations of each non-recursive algorithm. The following table reports the results.
Our next set of experiments demonstrate our algorithm in a transductive setting. For each data set

Alg/Data

4MOONS WEBKB4 OPTDIGITS

NCC-TV [3]
1SPEC [11]

LSD [1]

NMFR [19]

MTV

88.75
73.92
99.40
77.80
99.53

51.76
39.68
54.50
64.32
59.15

95.91
88.65
97.94
97.92
98.29

PENDIGITS

73.25
82.42
88.44
91.21
89.06

20NEWS MNIST
88.80
88.17
95.67
96.99
97.60

23.20
11.49
41.25
63.93
39.40

we randomly sample either one label per class or a percentage of labels per class from the ground
truth. We then run ten trials of initial condition (b) (propagating all labels instead of one) and report
the purity of the lowest energy solution as before along with the average computational time (for
simple MATLAB code running on a standard desktop) of the ten runs. We terminate the algorithm
once the relative change in energy falls below 10−4 between outer steps of algorithm 1. The table
below reports the results. Note that for well-constructed graphs (such as MNIST)  our algorithm
performs remarkably well with only one label per class.

Labels

4MOONS WEBKB4 OPTDIGITS
99.55/ 3.0s
1
1%
99.55/ 3.1s
2.5% 99.55/ 1.9s
99.53/ 1.2s
5%
10% 99.55/ 0.8s

56.58/ 1.8s
58.75/ 2.0s
57.01/ 1.7s
58.34/ 1.3s
62.01/ 1.2s

98.29/ 7s
98.29/ 4s
98.35/ 3s
98.38/ 2s
98.45/ 2s

PENDIGITS
89.17/ 14s
93.73/ 9s
95.83/ 7s
97.98/ 5s
98.22/ 4s

20NEWS
50.07/ 52s
61.70/ 54s
67.61/ 42s
70.51/ 32s
73.97/ 25s

MNIST
97.53/ 98s
97.59/ 54s
97.72/ 39s
97.79/ 31s
98.05/ 25s

Our non-recursive MTV algorithm vastly outperforms the two previous recursive total variation
approaches and also compares well with state-of-the-art NMF approaches. Each of MTV  LSD and
NMFR perform well on manifold data sets such as MNIST  but NMFR tends to perform best on
noisy  non-manifold data sets. This results from the fact that NMFR uses a costly graph smoothing
technique while our algorithm and LSD do not. We plan to incorporate such improvements into the
total variation framework in future work. Lastly  we found procedure (b) can help overcome the
lack of convexity inherent in many clustering approaches. We plan to pursue a more principled and
efﬁcient initialization along these lines in the future as well. Overall  our total variation framework
presents a promising alternative to NMF methods due to its strong mathematical foundation and
tight relaxation.
Acknowledgements: Supported by NSF grant DMS-1109805  AFOSR MURI grant FA9550-10-1-
0569  ONR grant N000141210040  and Swiss National Science Foundation grant SNSF-141283.

8

References
[1] Raman Arora  M Gupta  Amol Kapila  and Maryam Fazel. Clustering by left-stochastic ma-
trix factorization. In International Conference on Machine Learning (ICML)  pages 761–768 
2011.

[2] A. Bertozzi and A. Flenner. Diffuse Interface Models on Graphs for Classiﬁcation of High

Dimensional Data. Multiscale Modeling and Simulation  10(3):1090–1118  2012.

[3] X. Bresson  T. Laurent  D. Uminsky  and J. von Brecht. Convergence and energy landscape for
cheeger cut clustering. In Advances in Neural Information Processing Systems (NIPS)  pages
1394–1402  2012.

[4] X. Bresson  X.-C. Tai  T.F. Chan  and A. Szlam. Multi-Class Transductive Learning based on

(cid:96)1 Relaxations of Cheeger Cut and Mumford-Shah-Potts Model. UCLA CAM Report  2012.

[5] T. B¨uhler and M. Hein. Spectral Clustering Based on the Graph p-Laplacian. In International

Conference on Machine Learning (ICML)  pages 81–88  2009.

[6] A. Chambolle and T. Pock. A First-Order Primal-Dual Algorithm for Convex Problems with
Applications to Imaging. Journal of Mathematical Imaging and Vision  40(1):120–145  2011.
[7] J. Cheeger. A Lower Bound for the Smallest Eigenvalue of the Laplacian. Problems in Analy-

sis  pages 195–199  1970.

[8] F. R. K. Chung. Spectral Graph Theory  volume 92 of CBMS Regional Conference Series in
Mathematics. Published for the Conference Board of the Mathematical Sciences  Washington 
DC  1997.

[9] Chris Ding  Xiaofeng He  and Horst D Simon. On the equivalence of nonnegative matrix
factorization and spectral clustering. In Proc. SIAM Data Mining Conf  number 4  pages 606–
610  2005.

[10] C. Garcia-Cardona  E. Merkurjev  A. L. Bertozzi  A. Flenner  and A. G. Percus. Fast multiclass

segmentation using diffuse interface methods on graphs. Submitted  2013.

[11] M. Hein and T. B¨uhler. An Inverse Power Method for Nonlinear Eigenproblems with Applica-
tions in 1-Spectral Clustering and Sparse PCA. In Advances in Neural Information Processing
Systems (NIPS)  pages 847–855  2010.

[12] M. Hein and S. Setzer. Beyond Spectral Clustering - Tight Relaxations of Balanced Graph

Cuts. In Advances in Neural Information Processing Systems (NIPS)  2011.

[13] E. Merkurjev  T. Kostic  and A. Bertozzi. An mbo scheme on graphs for segmentation and

image processing. UCLA CAM Report 12-46  2012.

[14] C. Michelot. A Finite Algorithm for Finding the Projection of a Point onto the Canonical

Simplex of Rn. Journal of Optimization Theory and Applications  50(1):195–200  1986.

[15] S. Rangapuram and M. Hein. Constrained 1-Spectral Clustering. In International conference

on Artiﬁcial Intelligence and Statistics (AISTATS)  pages 1143–1151  2012.

[16] J. Shi and J. Malik. Normalized Cuts and Image Segmentation. IEEE Transactions on Pattern

Analysis and Machine Intelligence (PAMI)  22(8):888–905  2000.

[17] A. Szlam and X. Bresson. A total variation-based graph clustering algorithm for cheeger ratio

cuts. UCLA CAM Report 09-68  2009.

[18] A. Szlam and X. Bresson. Total variation and cheeger cuts. In International Conference on

Machine Learning (ICML)  pages 1039–1046  2010.

[19] Zhirong Yang  Tele Hao  Onur Dikmen  Xi Chen  and Erkki Oja. Clustering by nonnegative
matrix factorization using graph random walk. In Advances in Neural Information Processing
Systems (NIPS)  pages 1088–1096  2012.

[20] Zhirong Yang and Erkki Oja. Clustering by low-rank doubly stochastic matrix decomposition.

In International Conference on Machine Learning (ICML)  2012.

9

,Xavier Bresson
Thomas Laurent
David Uminsky
James von Brecht