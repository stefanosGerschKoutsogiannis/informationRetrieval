2013,Prior-free and prior-dependent regret bounds for Thompson Sampling,We consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions. We are interested in studying prior-free and prior-dependent regret bounds  very much in the same spirit than the usual distribution-free and distribution-dependent bounds for the non-Bayesian stochastic bandit. We first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by $14 \sqrt{n K}$. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by $\frac{1}{20} \sqrt{n K}$. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time  thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.,Prior-free and prior-dependent regret bounds for

Thompson Sampling

S´ebastien Bubeck  Che-Yu Liu

Department of Operations Research and Financial Engineering 

sbubeck@princeton.edu  cheliu@princeton.edu

Princeton University

Abstract

We consider the stochastic multi-armed bandit problem with a prior distribution
on the reward distributions. We are interested in studying prior-free and prior-
dependent regret bounds  very much in the same spirit than the usual distribution-
free and distribution-dependent bounds for the non-Bayesian stochastic bandit.
We ﬁrst show that Thompson Sampling attains an optimal prior-free bound in the
sense that for any prior distribution its Bayesian regret is bounded from above by
14√nK. This result is unimprovable in the sense that there exists a prior dis-
tribution such that any algorithm has a Bayesian regret bounded from below by
√nK. We also study the case of priors for the setting of Bubeck et al. [2013]
1
20
(where the optimal mean is known as well as a lower bound on the smallest gap)
and we show that in this case the regret of Thompson Sampling is in fact uniformly
bounded over time  thus showing that Thompson Sampling can greatly take advan-
tage of the nice properties of these priors.

1

Introduction

In this paper we are interested in the Bayesian multi-armed bandit problem which can be described
as follows. Let π0 be a known distribution over some set Θ  and let θ be a random variable dis-
tributed according to π0. For i ∈ [K]  let (Xi s)s≥1 be identically distributed random variables
taking values in [0  1] and which are independent conditionally on θ. Denote µi(θ) := E(Xi 1|θ).
Consider now an agent facing K actions (or arms). At each time step t = 1  . . . n  the agent pulls
an arm It ∈ [K]. The agent receives the reward Xi s when he pulls arm i for the sth time. The arm
selection is based only on past observed rewards and potentially on an external source of random-
ness. More formally  let (Us)s≥1 be an i.i.d. sequence of random variables uniformly distributed
on [0  1]  and let Ti(s) = Ps
1It=i  then It is a random variable measurable with respect to
σ(I1  X1 1  . . .   It−1  XIt−1 TIt−1 (t−1)  Ut). We measure the performance of the agent through the
Bayesian regret deﬁned as

t=1

BRn = E

nXt=1(cid:18)max

i∈[K]

µi(θ) − µIt (θ)(cid:19)  

where the expectation is taken with respect to the parameter θ  the rewards (Xi s)s≥1  and the
external source of randomness (Us)s≥1. We will also be interested in the individual regret Rn(θ)
which is deﬁned similarly except that θ is ﬁxed (instead of being integrated over π0). When it is
clear from the context we drop the dependency on θ in the various quantities deﬁned above.

1

Given a prior π0 the problem of ﬁnding an optimal strategy to minimize the Bayesian regret BRn
is a well deﬁned optimization problem and as such it is merely a computational problem. On the
other hand the point of view initially developed in Robbins [1952] leads to a learning problem. In
this latter view the agent’s strategy must have a low regret Rn(θ) for any θ ∈ Θ. Both formulations
of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi
[2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting
a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior
distribution takes a product form an optimal strategy is given by the Gittins indices (which are
relatively easy to compute). The product assumption on the prior means that the reward processes
In the present paper we are precisely interested in the
(Xi s)s≥1 are independent across arms.
situations where this assumption is not satisﬁed. Indeed we believe that one of the strength of the
Bayesian setting is that one can incorporate prior knowledge on the arms in very transparent way.
A prototypical example that we shall consider later on in this paper is when one knows the dis-
tributions of the arms up to a permutation  in which case the reward processes are strongly dependent.

In general without the product assumption on the prior it seems hopeless (from a computational
perspective) to look for the optimal Bayesian strategy. Thus  despite being in a Bayesian setting 
it makes sense to view it as a learning problem and to evaluate the agent’s performance through its
Bayesian regret. In this paper we are particularly interested in studying the Thompson Sampling
strategy which was proposed in the very ﬁrst paper on the multi-armed bandit problem Thompson
[1933]. This strategy can be described very succinctly:
let πt be the posterior distribution on θ
given the history Ht = (I1  X1 1  . . .   It−1  XIt−1 TIt−1 (t−1)) of the algorithm up to the beginning
of round t. Then Thompson Sampling ﬁrst draws a parameter θ(t) from πt (independently from the
past given πt) and it pulls It ∈ argmaxi∈[K] µi(θ(t)).
Recently there has been a surge of interest for this simple policy  mainly because of its ﬂexibility to
incorporate prior knowledge on the arms  see for example Chapelle and Li [2011]. For a long time the
theoretical properties of Thompson Sampling remained elusive. The speciﬁc case of binary rewards
with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a] 
Kaufmann et al. [2012]  Agrawal and Goyal [2012b]. However as we pointed out above here we
are interested in proving regret bounds for the more realistic scenario where one runs Thompson
Sampling with a hand-tuned prior distribution  possibly very different from a Beta prior. The ﬁrst
result in that spirit was obtained very recently by Russo and Roy [2013] who showed that for any

prior distribution π0 Thompson Sampling always satisﬁes BRn ≤ 5√nK log n. A similar bound

was proved in Agrawal and Goyal [2012b] for the speciﬁc case of Beta prior1. Our ﬁrst contribution
is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by
using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009].

Our second contribution is to show that Thompson Sampling can take advantage of the properties of
some non-trivial priors to attain much better regret guarantees. More precisely in Section 2 and 3 we
consider the setting of Bubeck et al. [2013] (which we call the BPR setting) where µ∗ and ε > 0 are
known values such that for any θ ∈ Θ  ﬁrst there is a unique best arm {i∗(θ)} = argmaxi∈[K] µi(θ) 
and furthermore

µi∗(θ)(θ) = µ∗  and ∆i(θ) := µi∗(θ)(θ) − µi(θ) ≥ ε ∀i 6= i∗(θ).

In other words the value of the best arm is known as well as a non-trivial lower bound on the gap
between the values of the best and second best arms. For this problem a new algorithm was proposed
in Bubeck et al. [2013] (which we call the BPR policy)  and it was shown that the BPR policy satisﬁes

Rn(θ) = O Xi6=i∗(θ)

log(∆i(θ)/ε)

∆i(θ)

log log(1/ε)  ∀θ ∈ Θ ∀n ≥ 1.

Thus the BPR policy attains a regret uniformly bounded over time in the BPR setting  a feature that
standard bandit algorithms such as UCB of Auer et al. [2002] cannot achieve. It is natural to view

1Note however that the result of Agrawal and Goyal [2012b] applies to the individual regret Rn(θ) while

the result of Russo and Roy [2013] only applies to the integrated Bayesian regret BRn.

2

the assumptions of the BPR setting as a prior over the reward distributions and to ask what regret
guarantees attain Thompson Sampling in that situation. More precisely we consider Thompson Sam-
pling with Gaussian reward distributions and uniform prior over the possible range of parameters.
We then prove individual regret bounds for any sub-Gaussian distributions (similarly to Bubeck et al.
[2013]). We obtain that Thompson Sampling uses optimally the prior information in the sense that
it also attains uniformly bounded over time regret. Furthermore as an added bonus we remove the
extraneous log-log factor of the BPR policy’s regret bound.

The results presented in Section 2 and 3 can be viewed as a ﬁrst step towards a better understanding
of prior-dependent regret bounds for Thompson Sampling. Generalizing these results to arbitrary
priors is a challenging open problem which is beyond the scope of our current techniques.

2 Optimal prior-free regret bound for Thompson Sampling

In this section we prove the following result.

Theorem 1 For any prior distribution π0 over reward distributions in [0  1]  Thompson Sampling
satisﬁes

BRn ≤ 14√nK.
Remark that the above result is unimprovable in the sense that there exist prior distributions π0 such
√nK (see e.g. [Theorem 3.5  Bubeck and Cesa-Bianchi
that for any algorithm one has Rn ≥ 1
[2012]]). This theorem also implies an optimal rate of identiﬁcation for the best arm  see
Bubeck et al. [2009] for more details on this.

20

Proof We decompose the proof into three steps. We denote i∗(θ) ∈ argmaxi∈[K] µi(θ)  in
particular one has It = i∗(θt).

Step 1: rewriting of the Bayesian regret in terms of upper conﬁdence bounds. This step is given
by [Proposition 1  Russo and Roy [2013]] which we reprove for sake of completeness. Let Bi t be a
random variable measurable with respect to σ(Ht). Note that by deﬁnition θ(t) and θ are identically
distributed conditionally on Ht. This implies by the tower rule:

EBi∗(θ) t = EBi∗(θ(t)) t = EBIt t.

Thus we obtain:

E(cid:0)µi∗(θ)(θ) − µIt (θ)(cid:1) = E(cid:0)µi∗(θ)(θ) − Bi∗(θ) t(cid:1) + E (BIt t − µIt (θ)) .

Inspired by the MOSS strategy of Audibert and Bubeck [2009] we will now take

P(µi − Bi t ≥ u) ≤

4K

nu2 log(cid:18)r n

K

u(cid:19) +

3

1

.

nu2/K − 1

Bi t =bµi Ti(t−1) +vuut log+(cid:16)

n

KTi(t−1)(cid:17)

 

Ti(t − 1)

From now on we work conditionally on θ and thus we drop all the dependency on θ.

t=1 Xi t  and log+(x) = log(x)1x≥1. In the following we denote δ0 = 2q K
sPs
wherebµi s = 1
Step 2: control of E(cid:0)µi∗(θ)(θ) − Bi∗(θ) t|θ(cid:1). By a simple integration of the deviations one has

n .

E (µi∗ − Bi∗ t) ≤ δ0 +Z 1

δ0

P(µi∗ − Bi∗ t ≥ u)du.

Next we extract the following inequality from Audibert and Bubeck [2010] (see p2683–2684)  for
any i ∈ [K] 

 

.

n

n

δ0

δ0

K

1

δ0

K

t=1

E

1

n

1

n

K

δ0

n .

≤

≤

4K

log 3

δ0
and

4K
nu

4K
nδ0

K u + 1

K δ0 + 1

Next we use the following simple inequality:

nu2/K − 1

nu2 log(cid:18)r n

u(cid:19) du =(cid:20)−
du ="−
2r K

log(cid:18)er n
u(cid:19)(cid:21)1
K u − 1!#1
log p n
p n

δ0(cid:19) = 2(1+log 2)r K
log(cid:18)er n
Z 1
K δ0 − 1! =
log p n
2 r K
2r K
Z 1
p n
n ≤ 6q K
2 (cid:17)q K
Thus we proved: E(cid:0)µi∗(θ)(θ) − Bi∗(θ) t|θ(cid:1) ≤(cid:16)2 + 2(1 + log 2) + log 3
Step 3: control ofPn
E (BIt t − µIt (θ)|θ). We start again by integrating the deviations:
(BIt t − µIt ) ≤ δ0n +Z +∞
nXt=1

P(BIt t − µIt ≥ u)du.

nXt=1
1bµi s +s log+(cid:0) n
Ks(cid:1)
Pbµi s +s log+(cid:0) n
Ks(cid:1)
K (cid:17)
3 log(cid:16) nu2

− µi ≥ u
− µi ≥ u .

1{BIt t − µIt ≥ u} ≤

nXs=1
KXi=1
K (cid:17) /u2⌉ where ⌈x⌉ is the smallest integer large than x. Let
Now for u ≥ δ0 let s(u) = ⌈3 log(cid:16) nu2
c = 1 − 1√3
Pbµi s +s log+(cid:0) n
− µi ≥ u ≤
Ks(cid:1)
K (cid:17)
3 log(cid:16) nu2
≤ 3(1 + log(2))r n
Z +∞

nXs=s(u)
K ≤ 5.1r n

P (bµi s − µi ≥ cu) .

Using an integration already done in Step 2 we have

P(BIt t − µIt ≥ u) ≤

nXt=1
nXt=1

. It is is easy to see that one has:

KXi=1
nXs=1

which implies

nXs=1

.

K

s

s

u2

u2

s

δ0

+

 

Next using Hoeffding’s inequality and the fact that the rewards are in [0  1] one has for u ≥ δ0
1u≤1/c.

P (bµi s − µi ≥ cu) ≤

nXs=s(u)
exp(−2sc2u2)1u≤1/c ≤
Now using that 1 − exp(−x) ≥ x − x2/2 for x ≥ 0 one obtains
Z 1/c

1 − exp(−2c2u2)

exp(−12c2 log 2)
1 − exp(−2c2u2)

du +Z 1/c

1/(2c)

δ0

1

1

1

du

1 − exp(−2c2u2)
1

Now an elementary integration gives

δ0

nXs=s(u)
du = Z 1/(2c)
≤ Z 1/(2c)
≤ Z 1/(2c)
≤ 1.9r n

δ0
2
3c2δ0 −

K

=

δ0

.

4

1

1 − exp(−2c2u2)
2c2u2 − 2c4u4 du +
3c2u2 du +
4
3c

+

1

2

2c(1 − exp(−1/2))

2c(1 − exp(−1/2))

2c(1 − exp(−1/2))
1

Putting the pieces together we proved

which concludes the proof together with the results of Step 1 and Step 2.

(BIt t − µIt ) ≤ 7.6√nK 

E

nXt=1

3 Thompson Sampling in the two-armed BPR setting

Following [Section 2  Bubeck et al. [2013]] we consider here the two-armed bandit problem with
sub-Gaussian reward distributions (that is they satisfy Eeλ(X−µ) ≤ eλ2/2 for all λ ∈ R) and such
that one reward distribution has mean µ∗ and the other one has mean µ∗ − ∆ where µ∗ and ∆ are
known values.

In order to derive the Thompson Sampling strategy for this problem we further assume that the
reward distributions are in fact Gaussian with variance 1. In other words let Θ = {θ1  θ2}  π0(θ1) =
π0(θ2) = 1/2  and under θ1 one has X1 s ∼ N (µ∗  1) and X2 s ∼ N (µ∗ − ∆  1) while under θ2
one has X2 s ∼ N (µ∗  1) and X1 s ∼ N (µ∗ − ∆  1). Then a straightforward computation (using
Bayes rule and induction) shows that one has for some normalizing constant c > 0:

πt(θ1) = c exp−
πt(θ2) = c exp−

1
2

1
2

T1(t−1)Xs=1
T1(t−1)Xs=1

(µ∗ − X1 s)2 −

1
2

(µ∗ − ∆ − X1 s)2 −

1
2

T2(t−1)Xs=1

(µ∗ − ∆ − X2 s)2  
(µ∗ − X2 s)2 .
T2(t−1)Xs=1

Recall that Thompson Sampling draws θ(t) from πt and then pulls the best arm for the environment
θ(t). Observe that under θ1 the best arm is arm 1 and under θ2 the best arm is arm 2. In other words
Thompson Sampling draws It at random with the probabilities given by the posterior πt. This leads
to a general algorithm for the two-armed BPR setting with sub-Gaussian reward distributions that
we summarize in Figure 1. The next result shows that it attains optimal performances in this setting
up to a numerical constant (see Bubeck et al. [2013] for lower bounds)  for any sub-Gaussian reward
distribution (not necessarily Gaussian) with largest mean µ∗ and gap ∆.

For rounds t ∈ {1  2}  select arm It = t.
For each round t = 3  4  . . . play It at random from pt where

pt(1) = c exp−
pt(2) = c exp−

1
2

1
2

T1(t−1)Xs=1
T1(t−1)Xs=1

T2(t−1)Xs=1

(µ∗ − ∆ − X2 s)2  
(µ∗ − X2 s)2  
T2(t−1)Xs=1

(µ∗ − X1 s)2 −

1
2

(µ∗ − ∆ − X1 s)2 −

1
2

and c > 0 is such that pt(1) + pt(2) = 1.

Figure 1: Policy inspired by Thompson Sampling for the two-armed BPR setting.

Theorem 2 The policy of Figure 1 has regret bounded as Rn ≤ ∆ + 578

∆   uniformly in n.

5

* = 0  D

 = 0.2

* = 0  D

 = 0.05

n
R

 
 
 
:
t

e
r
g
e
r
 
d
e
a
c
s
e
R

l

5

4

3

2

1

0

Policy 1 from Bubeck et al.[2013]
Policy of Figure 1

n
R

 
 
 
:
t

e
r
g
e
r
 
d
e
a
c
s
e
R

l

5

4

3

2

1

0

Policy 1 from Bubeck et al.[2013]
Policy of Figure 1

0

500

1000

1500

2000

2500

3000

0

5000

10000

15000

20000

Time n

Time n

Figure 2: Empirical comparison of the policy of Figure 1 and Policy 1 of Bubeck et al. [2013] on Gaussian
reward distributions with variance 1.

Note that we did not try to optimize the numerical constant in the above bound. Figure 2 shows
an empirical comparison of the policy of Figure 1 with Policy 1 of Bubeck et al. [2013]. Note in
particular that a regret bound of order 16/∆ was proved for the latter algorithm and the (limited)
numerical simulation presented here suggests that Thompson Sampling outperforms this strategy.
Proof Without loss of generality we assume that arm 1 is the optimal arm  that is µ1 = µ∗ and

sPs
µ2 = µ∗ − ∆. Letbµi s = 1
t=1 Xi t bγ1 s = µ1 −bµ1 s andbγ2 s = bµ2 s − µ2. Note that large
(positive) values ofbγ1 s orbγ2 s might mislead the algorithm into bad decisions  and we will need to

control what happens in various regimes for these γ coefﬁcients. We decompose the proof into three
steps.

Step 1. This ﬁrst step will be useful in the rest of the analysis  it shows how the probability ratio of
a bad pull over a good pull evolves as a function of the γ coefﬁcients introduced above. One has:

1
2

pt(2)
pt(1)

= exp−
= exp(cid:18)−
= exp(cid:18)−
= exp −

t∆2
2

T1(t − 1)

T1(t − 1)

2

1
2

T1 (t−1)Xs=1 (cid:20)(µ2 − X1 s)2 − (µ1 − X1 s)2(cid:21) −
1 − 2(µ2 − µ1)bµ1 T1 (t−1)(cid:21) −

(cid:20)µ2
2 − µ2
(cid:20)∆2 − 2∆(µ1 −bµ1 T1(t−1))(cid:21) −
+ T1(t − 1)∆bγ1 T1(t−1) + T2(t − 1)∆bγ2 T2 (t−1)! .

T2(t − 1)

2

2

T2(t − 1)

T2 (t−1)Xs=1 (cid:20)(µ1 − X2 s)2 − (µ2 − X2 s)2(cid:21)
(cid:20)∆2 − 2∆(bµ2 T2 (t−1) − µ2)(cid:21)(cid:19)

2 − 2(µ1 − µ2)bµ2 T2(t−1)(cid:21)(cid:19)

(cid:20)µ2
1 − µ2

2

Step 2. We decompose the regret Rn as follows:

Rn
∆

= 1 + E

= 1 + E

E

nXt=3

We use Hoeffding’s inequality to control the ﬁrst term:

+E

1{It = 2}

nXt=3
1(cid:26)bγ2 T2(t−1) >
nXt=3
1(cid:26)bγ2 T2(t−1) ≤
nXt=3
1(cid:26)bγ2 T2(t−1) >

∆
4

∆
4

∆
4

∆
4

  It = 2(cid:27) + E
 bγ1 T1(t−1) >
  It = 2(cid:27) ≤ E
nXs=1

1(cid:26)bγ2 T2(t−1) ≤
nXt=3
  It = 2(cid:27) .
1(cid:26)bγ2 s >

4(cid:27) ≤

nXs=1

∆

6

∆
4

 bγ1 T1(t−1) ≤

∆
4

  It = 2(cid:27)

exp(cid:18)−

s∆2

32 (cid:19) ≤

32
∆2 .

D
m
D
m
For the second term  using the rewriting of Step 1 as an upper bound on pt(2)  one obtains:

E

nXt=3

1(cid:26)bγ2 T2(t−1) ≤

∆
4

 bγ1 T1(t−1) ≤

∆
4

  It = 2(cid:27) =

≤

E(cid:18)pt(2)1(cid:26)bγ2 T2(t−1) ≤
exp −

4 ! ≤

4
∆2

t∆2

.

∆
4

 bγ1 T1(t−1) ≤

∆

4(cid:27)(cid:19)

nXt=3
nXt=3

The third term is more difﬁcult to control  and we further decompose the corresponding event as
follows:

The cumulative probability of the ﬁrst event in the above decomposition is easy to control thanks to
Hoeffding’s maximal inequality2 which states that for any m ≥ 1 and x > 0 one has

∆
4

(cid:26)bγ2 T2(t−1) ≤
⊂(cid:26)bγ1 T1(t−1) >

Indeed this implies

P(cid:18)bγ1 T1(t−1) >

and thus

∆
4

∆
4

  It = 2(cid:27)

x2

∆
4

∆
4

 bγ1 T1(t−1) >
  T1(t − 1) > t/4(cid:27) ∪(cid:26)bγ2 T2(t−1) ≤
  It = 2  T1(t − 1) ≤ t/4(cid:27) .
2m(cid:19) .
P(∃ 1 ≤ s ≤ m s.t. sbγ1 s ≥ x) ≤ exp(cid:18)−
  T1(t − 1) > t/4(cid:19) ≤ P(cid:18)∃ 1 ≤ s ≤ t s.t. sbγ1 s >
  T1(t − 1) > t/4(cid:27) ≤
nXt=3
  It = 2  T1(t − 1) ≤ t/4(cid:27) =
nXt=3
nXt=3

E(cid:18)pt(2)1(cid:26)bγ2 T2(t−1) ≤
E exp −

1(cid:26)bγ1 T1(t−1) >

16(cid:19) ≤ exp(cid:18)−

512
∆2 .

+ ∆ max

t∆2
4

∆
4

t∆2

1≤s≤t/4

∆t

∆
4

∆
4

≤

E

  T1(t − 1) ≤ t/4(cid:27)(cid:19)
sbγ1 s!  

512(cid:19)  

It only remains to control the term

E

nXt=3

1(cid:26)bγ2 T2(t−1) ≤

where the last inequality follows from Step 1. The last step is devoted to bounding from above this
last term.

Step 3. By integrating the deviations and using again Hoeffding’s maximal inequality one obtains

1≤s≤t/4

E exp(cid:18)∆ max
exp −

sbγ1 s(cid:19) ≤ 1+Z +∞
4 ! 1 +Z +∞
exp −

Now  straightforward computation gives

t∆2

1

1

1≤s≤ t
4

sbγ1 s ≥
∆2t ! dx! ≤

P max

2(log x)2

nXt=3

log x

1

∆ ! dx ≤ 1+Z +∞
exp(cid:18)−
4 !1 +s π∆2t
exp −
0 s π∆2t
exp −
+Z +∞
∆2 Z +∞
√u exp(−u) du

nXt=3

16√π

4
∆2

t∆2

t∆2

+

2

2

0

2(log x)2

∆2t (cid:19) dx.
8 !
exp t∆2
8 ! dt

≤

≤

≤

4
∆2
30
∆2

.

which concludes the proof by putting this together with the results of the previous step.

2It is an easy exercise to verify that Azuma-Hoeffding holds for martingale differences with sub-Gaussian

increments  which implies Hoeffding’s maximal inequality for sub-Gaussian distributions.

7

4 Optimal strategy for the BPR setting inspired by Thompson Sampling

In this section we consider the general BPR setting. That is the reward distributions are sub-Gaussian
(they satisfy Eeλ(X−µ) ≤ eλ2/2 for all λ ∈ R)  one reward distribution has mean µ∗  and all the other
means are smaller than µ∗ − ε where µ∗ and ε are known values.
Similarly to the previous section we assume that the reward distributions are Gaussian with variance
1 for the derivation of the Thompson Sampling strategy (but we do not make this assumption for the
analysis of the resulting algorithm). Then the set of possible parameters is described as follows:

Θ = ∪K

i=1Θi where Θi = {θ ∈ RK s.t. θi = µ∗ and θj ≤ µ∗ − ε for all j 6= i}.

Assuming a uniform prior over the index of the best arm  and a prior λ over the mean of a suboptimal
arm one obtains by Bayes rule that the probability density function of the posterior is given by:

Now remark that with Thompson Sampling arm i is played at time t if and only if θ(t) ∈ Θi. In other
words It is played at random from probability pt where

1
2

KXj=1

Tj (t−1)Xs=1

dπt(θ) ∝ exp−
(Xj s − θj)2
(Xi s − µ∗)2Yj6=i
Z µ∗−ε
pt(i) = πt(Θi) ∝ exp−
Ti(t−1)Xs=1
(Xi s − µ∗)2(cid:17)
exp(cid:16)− 1
2PTi(t−1)
(Xi s − v)2(cid:17) dλ(v)
−∞ exp(cid:16)− 1
R µ∗−ε
2PTi(t−1)

1
2

s=1

s=1

−∞

∝

KYj=1 j6=i∗(θ)

dλ(θj).

exp−

1
2

Tj (t−1)Xs=1

(Xj s − v)2 dλ(v)

.

Taking inspiration from the above calculation we consider the following policy  where λ is the
Lebesgue measure and we assume a slightly larger value for the variance (this is necessary for the
proof).

For rounds t ∈ [K]  select arm It = t.
For each round t = K + 1  K + 2  . . . play It at random from pt where

pt(i) = c

exp(cid:16)− 1
exp(cid:16)− 1
R µ∗−ε

3PTi(t−1)
3PTi(t−1)

(Xi s − µ∗)2(cid:17)
(Xi s − v)2(cid:17) dv

−∞

s=1

s=1

 

and c > 0 is such thatPK

i=1 pt(i) = 1.

Figure 3: Policy inspired by Thompson Sampling for the BPR setting.

The following theorem shows that this policy attains the best known performance for the BPR setting 
shaving off a log-log term in the regret bound of the BPR policy.

Theorem 3 The policy of Figure 3 has regret bounded as Rn ≤ Pi:∆i>0(cid:16)∆i + 80+log(∆i/ε)

uniformly in n.

∆i

(cid:17) 

The proof of this result is fairly technical and it is deferred to the supplementary material.

8

References
S. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. In

Proceedings of the 25th Annual Conference on Learning Theory (COLT)  2012a.

S. Agrawal and N. Goyal.

arXiv:1209.3353.

Further optimal regret bounds for thompson sampling  2012b.

J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In Proceed-

ings of the 22nd Annual Conference on Learning Theory (COLT)  2009.

J.-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitoring. Journal

of Machine Learning Research  11:2635–2686  2010.

P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning Journal  47(2-3):235–256  2002.

S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit

problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

S. Bubeck  R. Munos  and G. Stoltz. Pure exploration in multi-armed bandits problems. In Proceed-

ings of the 20th International Conference on Algorithmic Learning Theory (ALT)  2009.

S. Bubeck  V. Perchet  and P. Rigollet. Bounded regret in stochastic multi-armed bandits. In Pro-

ceedings of the 26th Annual Conference on Learning Theory (COLT)  2013.

O. Chapelle and L. Li. An empirical evaluation of Thompson sampling.

Information Processing Systems (NIPS)  2011.

In Advances in Neural

J.C. Gittins. Bandit processes and dynamic allocation indices. Journal Royal Statistical Society

Series B  14:148–167  1979.

E. Kaufmann  N. Korda  and R. Munos. Thompson sampling: an asymptotically optimal ﬁnite-time
analysis. In Proceedings of the 23rd International Conference on Algorithmic Learning Theory
(ALT)  2012.

H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathe-

matics Society  58:527–535  1952.

D. Russo and B. Van Roy. Learning to optimize via posterior sampling  2013. arXiv:1301.2609.
W. Thompson. On the likelihood that one unknown probability exceeds another in view of the

evidence of two samples. Bulletin of the American Mathematics Society  25:285–294  1933.

9

,Sebastien Bubeck
Che-Yu Liu
Zhaoran Wang
Han Liu
Kirill Struminsky
Simon Lacoste-Julien
Anton Osokin