2019,The spiked matrix model with generative priors,Using a low-dimensional parametrization of signals is a generic and powerful way to enhance performance in signal processing and statistical inference. A very popular and widely explored type of dimensionality reduction is sparsity; another type is generative modelling of signal distributions. Generative models based on neural networks  such as GANs or variational auto-encoders  are particularly performant and are gaining on applicability. In this paper we study spiked matrix models  where a low-rank matrix is observed through a noisy channel. This problem with sparse structure of the spikes has attracted broad attention in the past literature. Here  we replace the sparsity assumption by generative modelling  and investigate the consequences on statistical and algorithmic properties. We analyze the Bayes-optimal performance under specific generative models for the spike. In contrast with the sparsity assumption  we do not observe regions of parameters where statistical performance is superior to the best known algorithmic performance. We show that in the analyzed cases the approximate message passing algorithm is able to reach optimal performance. We also design enhanced spectral algorithms and analyze their performance and thresholds using random matrix theory  showing their superiority to the classical principal component analysis. We complement our theoretical results by illustrating the performance of the spectral algorithms when the spikes come from real datasets.,The spiked matrix model with generative priors

Benjamin Aubin†  Bruno Loureiro†  Antoine Maillard(cid:63) 

Florent Krzakala(cid:63)  Lenka Zdeborová†

Abstract

Using a low-dimensional parametrization of signals is a generic and powerful
way to enhance performance in signal processing and statistical inference. A very
popular and widely explored type of dimensionality reduction is sparsity; another
type is generative modelling of signal distributions. Generative models based
on neural networks  such as GANs or variational auto-encoders  are particularly
performant and are gaining on applicability. In this paper we study spiked matrix
models  where a low-rank matrix is observed through a noisy channel. This problem
with sparse structure of the spikes has attracted broad attention in the past literature.
Here  we replace the sparsity assumption by generative modelling  and investigate
the consequences on statistical and algorithmic properties. We analyze the Bayes-
optimal performance under speciﬁc generative models for the spike. In contrast
with the sparsity assumption  we do not observe regions of parameters where
statistical performance is superior to the best known algorithmic performance. We
show that in the analyzed cases the approximate message passing algorithm is able
to reach optimal performance. We also design enhanced spectral algorithms and
analyze their performance and thresholds using random matrix theory  showing
their superiority to the classical principal component analysis. We complement our
theoretical results by illustrating the performance of the spectral algorithms when
the spikes come from real datasets.

1

Introduction

A key idea of modern signal processing is to exploit the structure of the signals under investigation.
A traditional and powerful way of doing so is via sparse representations of the signals. Images are
typically sparse in the wavelet domain  sound in the Fourier domain  and sparse coding [1] is designed
to search automatically for dictionaries in which the signal is sparse. This compressed representation
of the signal can be used to enable efﬁcient signal processing under larger noise or with fewer samples
leading to the ideas behind compressed sensing [2] or sparsity enhancing regularizations. Recent years
brought a surge of interest in another powerful and generic way of representing signals – generative
modeling. In particular the generative adversarial networks (GANs) [3] provide an impressively
powerful way to represent classes of signals. A recent series of works on compressed sensing and
other regression-related problems successfully explored the idea of replacing the traditionally used
sparsity by generative models [4–10]. These results and performances conceivably suggest that [11]:

Generative models are the new sparsity.

Next to compressed sensing and regression  another technique in statistical analysis that uses sparsity
in a fruitful way is sparse principal component analysis (PCA) [12]. Compared to the standard PCA 

† Université Paris-Saclay  CNRS  CEA  Institut de physique théorique  91191  Gif-sur-Yvette  France.
(cid:63) Laboratoire de Physique de l’École Normale Supérieure  PSL University & CNRS & Sorbonne Universités 
Paris  France.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

in sparse-PCA the principal components are linear combinations of a few of the input variables 
speciﬁcally k of them. This means (for rank-one) that we aim to decompose the observed data matrix
Y ∈ Rn×p as Y = uv(cid:124)
+ξ where the spike v ∈ Rp is a vector with only k (cid:28) p non-zero components 
and u  ξ are commonly modelled as independent and identically distributed (i.i.d.) Gaussian variables.
The main goal of this paper is to explore the idea of replacing sparsity of the spike v by the
assumption that the spike belongs to the range of a generative model. Sparse-PCA with structured
sparsity inducing priors is well studied  e.g. [13]  in this paper we remove the sparsity entirely and
in a sense replace it by lower dimensionality of the latent space of the generative model. For the
purpose of comparing generative model priors and sparsity we focus on the rich range of properties
in the noisy high-dimensional regime (denoted below  borrowing statistical physics jargon  as the
thermodynamic limit) where the spike v cannot be estimated consistently  but can be estimated better
than by random guessing. In particular we analyze two spiked-matrix models as considered in a
series of existing works on sparse-PCA  e.g. [14–20]  deﬁned as follows:

Spiked Wigner model (vv(cid:124)): Consider an unknown vector (the spike) v(cid:63) ∈ Rp drawn from a
distribution Pv; we observe a matrix Y ∈ Rp×p with a symmetric noise term ξ ∈ Rp×p and ∆ > 0:
(1)

+ √∆ξ  

v(cid:63)v(cid:63)(cid:124)

Y =

1
√p

where ξij∼N (0  1) i.i.d. The aim is to ﬁnd back the hidden spike v(cid:63) from Y (up to a global sign).
Spiked Wishart (or spiked covariance) model (uv(cid:124)): Consider two unknown vectors u(cid:63) ∈ Rn
and v(cid:63) ∈ Rp drawn from distributions Pu and Pv and let ξ ∈ Rn×p with ξµi∼N (0  1) i.i.d. and
∆ > 0  we observe

Y =

1
√p

u(cid:63)v(cid:63)(cid:124)

+ √∆ξ ;

(2)

the goal is to ﬁnd back the hidden spikes u(cid:63) and v(cid:63) from Y ∈ Rn×p.
The noisy high-dimensional limit that we consider in this paper (the thermodynamic limit) is p  n→∞
while β ≡ n/p = Θ(1)  and the noise ξ has a variance ∆ = Θ(1). The prior Pv is representing the
spike v via a k-dimensional parametrization with α≡ p/k = Θ(1). In the sparse case  k is the number
of non-zeros components of v(cid:63)  while in generative models k is the number of latent variables.

1.1 Considered generative models

The simplest non-separable prior Pv that we consider is the Gaussian model with a covariance matrix
Σ  that is Pv(v) = N (v; 0  Σ). This prior is not compressive  yet it captures some structure and can
be simply estimated from data via the empirical covariance. We use this prior later to produce Fig. 4.
To exploit the practically observed power of generative models  it would be desirable to consider
models (e.g. GANs  variational auto-encoders  restricted Boltzmann machines  or others) trained
on datasets of examples of possible spikes. Such training  however  leads to correlations between
the weights of the underlying neural networks for which the theoretical part of the present paper
does not apply readily. To keep tractability in a closed form  and subsequent theoretical insights  we
focus on multi-layer generative models where all the weight matrices W (l) ∈ Rkl+1×kl  l = 1  . . .   L
(cid:18) 1
(with k1 = k  kL+1 = p)  are ﬁxed  layer-wise independent  i.i.d. Gaussian with zero mean and unit
variance. Let v ∈ Rp be the output of such a generative model
√k1

(cid:18) 1

W (L) . . . ϕ(1)

v = ϕ(L)

√kL

W (1)z

(cid:19)

with z ∈ Rk a latent variable drawn from separable distribution Pz  with ρz = EPz
ϕ(l) element-wise activation functions that can be either deterministic or stochastic. In the setting
considered in this paper the ground-truth spike v(cid:63) is generated using a ground-truth value of the
latent variable z(cid:63). The spike is then estimated from the knowledge of the data matrix Y   and the
known form of the spiked-matrix and of the generative model. In particular the matrices W (l) are
known  as are the parameters β  ∆  Pz  Pu  Pv  ϕ(l). Only the spikes v(cid:63)  u(cid:63) and the latent vector z(cid:63)
are unknown  and are to be inferred.

(cid:19)

. . .

.

(3)

(cid:2)z2(cid:3) and

2

For concreteness and simplicity  the generative model that will be analyzed in most examples given
in the present paper is the single-layer case of (3) with L = 1:
⇔ v ∼ Pout

(cid:12)(cid:12)(cid:12) 1

v = ϕ

W z

(4)

W z

.

(cid:18) 1

√k

(cid:19)

(cid:19)

(cid:18)

·

√k

We deﬁne the compression ratio α ≡ p/k. In what follows we will illustrate our results for ϕ being
linear  sign and ReLU functions.

1.2 Summary of main contributions

We analyze how the availability of generative priors  deﬁned in section 1.1  inﬂuences the statistical
and algorithmic properties of the spiked-matrix models (1) and (2). Both sparse-PCA and generative
priors provide statistical advantages when the effective dimensionality k is small  k (cid:28) p. However 
we show that from the algorithmic perspective the two cases are quite different. This is why our main
ﬁndings are best presented in a context of the results known for sparse-PCA. We draw two main
conclusions from the present work:
(i) No algorithmic gap with generative-model priors: Sharp and detailed results are known in the
thermodynamic limit (as deﬁned above) when the spike v(cid:63) is sampled from a separable distribution
Pv. A detailed account of several examples can be found in [21]. The main ﬁnding for sparse priors
Pv is that when the sparsity ρ = k/p = 1/α is large enough then there exist optimal algorithms [15] 
while for ρ small enough there is a striking gap between statistically optimal performance and the
one of best known algorithms [16]. The small-ρ expansion studied in [21] is consistent with the
well-known results for exact recovery of the support of v(cid:63) [22  23]  which is one of the best-known
cases in which gaps between statistical and best-known algorithmic performance were described.
Our analysis of the spiked-matrix models with generative priors reveals that in the investigated cases
the algorithmic gap disappears and known algorithms are able to obtain (asymptotically) optimal
performance even when the dimension is greatly reduced  i.e. α (cid:29) 1. Analogous conclusion about
the lack of algorithmic gaps was reached for the problem of phase retrieval under a deep generative
prior in [9]. This result suggests that plausibly generative priors are better than sparsity as they lead
to algorithmically easier problems and give back the hope that the structure can be exploited not only
information-theoretically but also tractably.
(ii) Spectral algorithms reaching statistical threshold: Arguably the most basic algorithm used to
solve the spiked-matrix model is based on the leading singular vectors of the matrix Y . We will refer
to this as PCA. Previous work on spiked-matrix models [17 21] established that in the thermodynamic
limit and for separable priors of zero mean PCA reaches the best performance of all known efﬁcient
algorithms in terms of the value of noise ∆ below which it is able to provide positive correlation
between its estimator and the ground-truth spike. While for sparse priors positive correlation is
statistically reachable even for larger values of ∆ [17  21]  no efﬁcient algorithm beating the PCA
threshold is known2.
In the case of generative priors we ﬁnd in this paper that other spectral methods improve on the
canonical PCA. We design a spectral method  called LAMP  that (under certain assumptions  e.g.
zero mean of the spikes) reach the statistically optimal threshold  meaning that for larger values
of noise variance no other (even exponential) algorithm is able to reach positive correlation with
the spike. Again this is a striking difference with the sparse separable prior  making the generative
priors algorithmically more attractive. We demonstrate the performance of LAMP on the spiked-
matrix model when the spike is taken to be one of the fashion-MNIST images showing considerable
improvement over canonical PCA.

2 Analysis of information-theoretically optimal estimation

We ﬁrst discuss the information theoretic results on the estimation of the spike  regardless of the
computational cost. A considerable amount of results have been obtained for the spiked-matrix
models with separable priors [14  15  18  19  25–29]. Here  we extend these results to the case where
the spike v(cid:63) ∈ Rp is generated from a generic non-separable prior Pv on Rp.

2This result holds only for sparsity ρ = Θ(1). A line of works shows that when sparsity k scales slower than

linearly with p  algorithms more performant than PCA exist [22  24]

3

2.1 Mutual Information and Minimal Mean Squared Error

We consider the mutual information between the ground-truth spike v(cid:63) and the observation Y  
deﬁned as I(Y ; v(cid:63)) = DKL(P(v(cid:63) Y )(cid:107)Pv(cid:63) PY ). Next  we consider the best possible value of the
mean-squared-error on recovering the spike  commonly called the minimum mean-squared-error
(MMSE). The MMSE estimator is computed from marginal-means of the posterior distribution
P (v|Y ).
Theorem 1. [Mutual information for the spiked Wigner model with structured spike] Informally
(see SM section 3 for details and proof)  assume the spikes v(cid:63) come from a sequence (of growing
dimension p) of generic structured priors Pv on Rp  then
I(Y ; v(cid:63))

p→∞ ip ≡ lim
lim
p→∞

p

= inf

ρv≥qv≥0

with iRS(∆  qv) ≡

(ρv − qv)2

4∆

+ lim
p→∞

I

(cid:16)

iRS(∆  qv) 

(cid:113) ∆

qv

(cid:17)

ξ

v; v +

p

and ξ being a Gaussian vector with zero mean  unit diagonal variance and ρv = lim
p→∞

This theorem connects the asymptotic mutual information of the spiked model with generative prior
Pv to the mutual information between v taken from Pv and its noisy version  I(v; v +
∆/qvξ).
Computing this later mutual information is itself a high-dimensional task  hard in full generality  but it
can be done for a range of models. The simplest tractable case is when the prior Pv is separable  then
it yields back exactly the formula known from [18  19  26]. It can be computed also for the Gaussian
generative model  Pv(v) = N (v; 0  Σ)  leading to I(v; v +
∆/qvξ) = Tr (log (Ip + qvΣ/∆)) /2.
More interestingly  the mutual information associated to the generative prior in eq. (6) can also
be asymptotically computed for the multi-layer generative model with random weights  deﬁned in
eq. (3). Indeed  for the single-layer prior (4) the corresponding formula for mutual information has
been derived and proven in [30]. For the multi-layer case the mutual information formula has been
derived in [6] and proven for the case of two layers in [31]. Theorem 1 together with the results
from [6  30  31] yields the following formula (see SM sec. 3 for details) for the spiked Wigner model
(1) with L-layer generative prior (3):

(cid:112)

(5)

(6)
EPv [v(cid:124)v]/p.
(cid:112)

L(cid:88)
(cid:16)

l=2

Zz

(cid:17)

  qL

(cid:16) qv

∆

(7)

(cid:35)

.

iRS(∆  qv) =

ρ2
v
4∆

1
α

extr
{ˆql ql}l

1
4∆

q2
v+

L(cid:88)

+

1
2

(cid:34)
(cid:16)
(cid:104)
Zz
Z (l)

αl ˆqlql −

αlΨ(l)

out (ˆql  ql−1) − αΨ(L+1)

out

− Ψz (ˆqz)

l=1

(cid:16)

(cid:16)

(cid:17)

x1/2ξ  x

where αl = kl/k (note that in particular α1 = 1) and the functions Ψz  Ψout are deﬁned by

(cid:104)
Ψz(x) ≡ Eξ
out(x  y) ≡ Eξ η
Ψ(l)
ϕ(l)(cid:16) 1√
W (l)h(l)(cid:17)
with ξ  η ∼ N (0  1) i.i.d.  ρl+1 the second moment of the hidden variable h(l+1) =
out are the normalizations of the following denoising scalar
distributions:

∈ Rkl+1 and Zz  Z (l)

x1/2ξ  x  y1/2η  ρl − y

x1/2ξ  x  y1/2η  ρl − y

(cid:17)(cid:17)(cid:105)
(cid:16)

(cid:17)(cid:17)(cid:105)

x1/2ξ  x

Z (l)

out

(cid:17)

(cid:16)

log

(8)

(9)

log

out

kl

 

 

Qγ Λ

z

(z) ≡

Pz(z)
Zz(γ  Λ)

e

− Λ

2 z2+γz   Q(l) B A ω V

out

(v  x) ≡

P (l)
out(v|x)

Z (l)
out(B  A  ω  V )

2 v2+Bv e− (x−ω)2
− A
√2πV
e

2V

.

(10)

Result (7) is remarkable in that it connects the asymptotic mutual information of a high-dimensional
model with a simple scalar formula that can be easily evaluated. In the SM sec. 2 we show how this
formula is obtained using the heuristic replica method from statistical physics and  once we have the
formula in hand  we prove it using the interpolation method in SM sec. 3. In SM sec. 2.2 we also
give the corresponding formula for the spiked Wishart model.

4

Beyond its theoretical interest  the main point of the mutual information formula is that it yields
the optimal value of the mean-squared error (MMSE). It is well-known [32] that the mean-squared
error is minimized by an estimator evaluating the conditional expectation of the signal given the
observations. Following generic theorems on the connection between the mutual information and
the MMSE [33]  one can prove in particular that for the spiked-matrix model [27] the MMSE on the
spike v(cid:63) is asymptotically given by:

where q(cid:63)

v is the optimizer of the function iRS (∆  qv).

MMSEv = ρv − q(cid:63)
v  

(11)

2.2 Examples of phase diagrams

(cid:17)

(cid:16) qv

∆

(cid:17)

(cid:16) qv

∆

Taking the extremization over qv  ˆqz  qz in eq. (7)  we obtain the following ﬁxed point equations:

qv = 2∂qv Ψout

  qz

 

qz = 2∂ˆqz Ψz (ˆqz)  

ˆqz = 2α∂qz Ψout

  qz

.

(12)

Using (11)  analyzing the ﬁxed points of eqs. (12) provides all the informations about the performance
of the Bayes-optimal estimator in the models under consideration.

Phase transition: A ﬁrst question is whether better estimation than random guessing from the
prior is possible. In terms of ﬁxed points of eqs. (12)  this corresponds to the existence of the
non-informative ﬁxed point q(cid:63)
v = 0 (i.e. zero overlap with the spike  or maximum MSEv = ρv).
Evaluating the right-hand side of eqs. (12) at qv = 0  we can see that q(cid:63)

v = 0 is a ﬁxed point if

EPz [z] = 0 and E

Q0

out [v] = 0  

(13)

out

out(v  x) ≡ Q0 0 0 ρz

(v  x) from eq. (10). Note that for a deterministic channel the second

where Q0
condition is equivalent to ϕ being an odd function.
When the condition (13) holds  (qv  ˆqz  qz) = (0  0  0) is a ﬁxed point of eq. (12). The numerical
stability of this ﬁxed point determines a phase transition point ∆c  deﬁned as the noise below which
the ﬁxed point (0  0  0) becomes unstable. This corresponds to the value of ∆ for which the largest
eigenvalue of the Jacobian of the eqs. (12) at (0  0  0)  given by

2d(∂qv Ψout  α∂qz Ψout  ∂ˆqz Ψz)|(0 0 0) =

 1

∆
α
∆

(cid:0)E
(cid:0)E

out v2(cid:1)2
outvx(cid:1)2

Q0

Q0
0

0

(cid:0)EPz z2(cid:1)2

0

α
ρ2
z

(cid:0)E
(cid:0)E

1
ρ2
z

Q0

outvx(cid:1)2

Q0

outx2 − ρz
0

  

(cid:1)2

(14)

becomes greater than one. The details of this calculation can be found in sec. 6 of the SM.
It is instructive to compute ∆c in speciﬁc cases. We therefore ﬁx Pz = N (0  1) and Pout(v|x) =
δ(v − ϕ(x)) and discuss two different choices of (odd) activation function ϕ.
Linear activation: For ϕ(x) = x the leading eigenvalue of the Jacobian becomes one at ∆c = 1+α.
. Note
that in the limit α = 0 we recover the phase transition ∆c = 1 known from the case with
separable prior [21]. For α > 0  we have ∆c > 1 meaning the spike can be estimated more
efﬁciently when its structure is accounted for.

Note that for L > 1 the result is derived in SM sec. 2.3 and reads ∆c = 1 +(cid:80)L

π2 . As above it generalizes for L > 1 as ∆c = 1 +(cid:80)L

Sign activation: For ϕ(x) = sgn(x) the leading eigenvalue of the Jacobian becomes one at ∆c =
. For α = 0 
1 + 4α
Pv = Bern(1/2)  and the transition ∆c = 1 agrees with the one found for a separable prior
distribution [21]. As in the linear case  for α > 0  we can estimate the spike for larger values
of noise than in the separable case.

(cid:1)l α

(cid:0) 4

α
αl

l=1

l=1

π2

αl

In Fig. 1 we solve the ﬁxed point equations (12) and plot the MMSE obtained from the ﬁxed point in
a heat map  for the linear  sign and relu activations. The white dashed line marks the above stated
threshold ∆c. The property that we ﬁnd the most striking is that in these three evaluated cases  for all
values of ∆  α and L that we analyzed  we always found that eq. (12) has a unique stable ﬁxed point.

5

v  and
Figure 1: Spiked Wigner model MMSEv on the spike as a function of noise to signal ratio ∆/ρ2
generative prior (4) with compression ratio α for L = 1 linear (left  ρv = 1)  sign (center  ρv = 1) 
and relu (right  ρv = 1/2) activations. Dashed white lines mark the phase transitions ∆c  matched by
both the AMP and LAMP algorithms. Dotted white line marks the phase transition of canonical PCA.

Figure 2: Spiked Wigner model: MMSEv as a function of noise ∆ - (upper) for a wide range of
compression ratios α = 0  1  10  100  1000  for L = 1 linear (left)  sign (center)  and relu (right)
activations. Unique stable ﬁxed point of (12) is found for all these cases - (lower) for different depths
L = 1  2  3 with constant compressive ratio α1 = α2 = α3 = 1  for linear (left)  sign (center)  and
relu (right) activations. The second moment of the variable v for L = 1  2  3 are ρ(L)
v = 1 for linear
and sign  while for ReLU ρ(L)
v = 1/2L. Similarly a unique stable ﬁxed point is found in these cases.

Thus we have not identiﬁed any ﬁrst order phase transition (in the physics terminology). This is
illustrated in Fig. 2 for larger values of α (upper) and for different depths L (lower)  where we solved
the eq. (12) iteratively from uncorrelated initial condition  and from initial condition corresponding
to the ground truth signal  and found that both lead to the same ﬁxed point. In particular  as a unique
ﬁxed point is found  the Bayes optimal errors are continuous and we did not observe any algorithmic
gap. Details of the expressions equivalent to eq. (12-14) for L ≥ 1 are detailed in SM sec. 2.3.

6

02468100246810∆/ρ2v0246810α0246810∆c∆PCA02468100.00.51.01.52.02.50.10.20.30.40.50.60.70.80.91.0MMSEv(∆/ρ2v α)/MMSEv(∞ α)10−210−11001011021030.00.20.40.60.81.0MMSEv10−210−1100101102103∆0.00.20.40.60.81.0α=0α=1α=10α=100α=100010−210−11001011021030.000.050.100.150.200.250.30123450.00.20.40.60.81.0MMSEv0.51.01.52.02.5∆0.00.20.40.60.81.0L=1L=2L=30.51.01.52.00.000.050.100.150.200.250.300.353 Approximate message passing with generative priors

A straightforward algorithmic evaluation of the Bayes-optimal estimator is exponentially costly.
This section is devoted to the analysis of an approximate message passing (AMP) algorithm that
for the analyzed cases is able to reach the optimal performance (in the thermodynamic limit). For
the purpose of presentation  we focus again on the spiked Wigner model (see SM for the spiked
Wishart model). For separable priors  the AMP for the spiked Wigner model is well known [14–
16].
It can  however  be extended to non-separable priors [6  34  35]. We show in SM sec. 4
how AMP can be generalized to handle the generative model (4). Iterating this derivation leads
naturally to its multi-layer version ML-AMP for L ≥ 1. In particular AMP for L = 1 reads:
Input: Y ∈ Rp×p and W ∈ Rp×k:
Initialize to zero: (g  ˆv  Bv  Av)t=0.
Initialize with: ˆvt=1 = N (0  σ2)  ˆzt=1 = N (0  σ2)  and ˆct=1
repeat
Spiked layer:
(cid:0)1
Y√
p ˆvt − 1
Bt
∆
Generative layer:
(cid:124)
kˆct
V t = 1
z
k
k(cid:107)gt(cid:107)2
Λt = 1
W
2Ik
Update of the estimated marginals:
ˆvt+1 = fv(Bt
and
v  At
ˆzt+1 = fz(γt  Λt)
ˆct+1
z = ∂γfz(γt  Λt) 
t = t + 1.

and At
v = 1
W ˆzt − V tgt−1
(cid:124)gt + Λtˆzt.

(cid:1) Ip  ωt = 1√

v  ωt  V t(cid:1) 

v = 1p  ˆct=1

z = 1k  t = 1.

v  ωt  V t)
and

v = ∂Bfv(Bt
ˆct+1

and gt = fout

∆p(cid:107)ˆvt(cid:107)2

2Ip.

v  At

v  ωt  V t) 

and γt = 1√

(cid:0)Bt

v = 1
∆

(1(cid:124)
pˆct
p

v)

ˆvt−1

v  At

k

k

until Convergence.
Output: ˆv  ˆz.
Algorithm 1: AMP algorithm for the spiked Wigner model with single-layer generative prior.

where Is and 1s denote respectively the identity matrix and vector of ones of size s. The update
functions fout and fv are the means of V −1 (x − ω) and v with respect to Qout  eq. (10)  while the
update function fz is the mean of z with respect to Qz  eq. (10).
The algorithm for the spiked Wishart model is very similar and both derivations are given in SM
(cid:124)v(cid:63)/p−→qt
sec. 4. We deﬁne the overlap of the AMP estimator with the ground truth spike as (ˆvt)
as p → ∞. Perhaps the most important virtue of AMP-type algorithms is that their asymptotic
performance can be tracked exactly via a set of scalar equations called state evolution. This fact has
been proven for a range of models including the spiked matrix models with separable priors in [36] 
and with non-separable priors in [35]. To help the reader understand the state evolution equations we
provide a heuristic derivation in the SM  section 4.4. For L = 1  the state evolution states that the
overlap qt

v evolves under iterations of the AMP algorithm as:

v

qt+1
v = 2∂qv Ψout

 

qt+1
z = 2∂ˆqz Ψz

ˆqt
z = 2α∂qz Ψout

(cid:19)

(cid:18) qt

  qt
z

v
∆

(cid:0)ˆqt

z

(cid:1)  

(cid:18) qt

(cid:19)

  qt
z

v
∆

 

(15)

v = ε  qt=0

with initialization qt=0
z = ε and a small ε > 0. We notice immediately that (15) are the
same equations as the ﬁxed point equations related to the Bayes-optimal estimation (12) with speciﬁc
time-indices and initialization  but crucially the same ﬁxed points. This observation generalizes
naturally to L > 1. Thus the analysis of ﬁxed points in sec. 2.2 applies also to the behaviour of AMP.
In particular in all the scenarios for which we solved the corresponding equations numerically we
found the stable ﬁxed point of (12) to be unique or equivalently the Bayes optimal errors as a function
of the noise to be continuous. Hence under the assumption that the data was created using the model
from eq. (1) and the spike from eq. (3) with i.i.d weight matrices W (l) and i.i.d. Gaussian entries 
it means the AMP algorithm is able to reach asymptotically the optimal performance in all these
cases. This is further illustrated in Fig. 3 where we explicitly compare runs of AMP on ﬁnite size
instances with the results of the asymptotic state evolution  thus also giving an idea of the amplitude
of the ﬁnite size effects. Note that we provide a demonstration notebook in [37] that compares AMP 
LAMP and PCA numerical performances. Finally as has been done in previous works  e.g. [5  8–10]
for compressed sensing and denoising  translating our results to practical situations in designing an
AMP algorithm that takes care of correlated GAN or VAE weights is still under investigation.

7

Figure 3: Comparison between PCA  LAMP and AMP - (upper) for (left) the linear  (center) and
sign activations  for L = 1 and compression ratio α = 2. Lines correspond to the theoretical
asymptotic performance of PCA (red line)  LAMP (green line) and AMP (blue line). Dots correspond
to simulations of PCA (red squares)  LAMP (green crosses) for k = 104 and AMP (blue points)
for k = 5.103  σ2 = 1. (Right) Illustration of the spectral phase transition in the matrix Γvv
p
eq. (18) at α = 2 with an informative leading eigenvector with eigenvalue equal to 1 out of the
bulk for ∆ ≤ 1 + α. We show the bulk spectral density µ(α  ∆). The inset shows the two leading
eigenvalues - (lower) for (left) three layers generative model with (α1  α2  α3) = (1  1  1) using
linear activations (k = 104) (right) two layers generative model with (α1  α2) = (1  1) using sign
activations (k = 2.104). The vertical lines show the PCA and the optimal threshold respectively.

4 Spectral methods for generative priors

Spectral methods are the most common class of algorithms used for spiked matrix estimation. For
instance  canonical PCA estimates the spike from the leading eigenvector of the matrix Y . A classical
result from Baik  Ben Arous and Péché (BBP) [38] shows that this eigenvector is correlated with the
signal if and only if the signal-to-noise ratio ρ2
v = Θ(1)) 
v is also the threshold for AMP and it is conjectured that no polynomial algorithm can
∆PCA = ρ2
improve upon it [21]. In the previous section we show that for the analyzed generative priors AMP
has a better threshold than PCA. Here we design a spectral method  called LAMP  that matches
the AMP threshold and is hence superior over the canonical PCA. In order to do so  we follow the
powerful strategy pioneered in [39] and linearize the AMP around its non-informative ﬁxed point. In
the spiked Wigner model with a single-layer prior (L = 1) the linearized AMP leads to the following
operator:

v/∆ > 1. For sparse separable priors (with ρ2

(cid:18)

(cid:19)

(cid:18) Y

Γvv

p =

(cid:124)
W
√k
where parameters are moments of distributions Pz and Q0
out according to
EPz

(a − b)Ip + b

W W
k

(cid:124)
1p1
k
k

E
Q0

1
∆

+ c

×

−1
z

−3
z

(cid:124)

[vx2]E

[vx]2  

b ≡ ρ

a ≡ ρv  

(17)
We denote the spectral algorithm that takes the leading eigenvectors of (16) as LAMP (for linearized-
AMP). Its derivation is presented in SM sec. 5 together with the one for the spiked Wishart model. For
the speciﬁc case of Gaussian z and prior (4) with the sign activation function we obtain (a  b  c) =
(1  2/π  0). For linear activation we get (a  b  c) = (1  1  0)  leading to

c ≡

[vx] .

Q0

Q0

ρ

out

out

out

√p − aIp

 

(16)

1
2

(cid:19)
(cid:2)z3(cid:3) E

with Kp =

[W W
k

8

(cid:20) Y

(cid:21)

Γvv

p =

1
∆

Kp

√p − Ip

(cid:124)

]

= Σ ≈

1
n

n(cid:88)

α=1

(cid:124)
vα(vα)

 

(18)

12345∆0.000.250.500.751.001.251.501.752.00MSEv12345∆0.000.250.500.751.001.251.501.752.00AMPPCALAMPSEAMPSEPCASELAMP∆c∆PCA−4−201z0.00.51.0dµ(α ∆)/dz13579∆0.91.0λ1λ2∆=1∆=3∆=100123456∆0.000.250.500.751.001.251.501.752.00MSEvPCAlAMPSEAMPSElAMP0.51.01.52.02.53.0∆0.000.250.500.751.001.251.501.752.00MSEvPCAlAMPSEAMPwith Kp = EPv [vv(cid:124)

] .

(cid:104) Y√

Algorithm 2: LAMP spectral algorithm

Figure 4: Illustration of canonical PCA (top line) and the LAMP (bottom line) spectral methods
Alg. 2 on the spiked Wigner model. The covariance Kp is estimated empirically  see (18)  from
the FashionMNIST database [40]. The estimation of the spike is shown for two images from
FashionMNIST  with (from left to right)  noise variance ∆ = 0.01  0.1  1  2  10.
where the last two equalities come from the fact that for the model (4) with linear activation and
Gaussian separable Pz  Kp is asymptotically equal to the covariance matrix between samples of
spikes  Σ. The same observation holds for the sign activation function. In fact  the spectral method
based on the matrix in eq. (18) can also be derived linearizing AMP with a Gaussian prior with
covariance Σ. Interestingly  as the spectral method based on the matrix Kp in eq. (18) can be
empirically estimated directly from n samples of spikes  vα  α = 1  . . .   n  without the knowledge of
the generative model (ϕ  W ) itself  it suggests a simple practical implementation of LAMP Alg. 2
(cid:105)
for any prior Pv.
Input: Observed matrix Y ∈ Rp×p  prior Pv on v ∈ Rp
Take the leading eigenvector ˆv ∈ Rp of Kp
p − Ip
Analogously to the state evolution for AMP  the asymptotic performance of both PCA and LAMP can
be evaluated in a closed-form for the spiked Wigner model with single-layer generative prior with
linear activation (4). The corresponding expressions are derived in SM sec. 5 and plotted in Fig. 3 for
the three considered algorithms that illustrates LAMP spectral method reaches the same threshold
than ML-AMP for different depths L and activations.
For illustration purposes  we display the behaviour of this spectral method on the spiked Wigner
model with spikes coming from the Fashion-MNIST dataset in Fig. 4. A demonstration notebook is
provided in [37]  illustrating PCA and LAMP performances on Fashion-MNIST dataset.
Remarkably  the performance of the spectral method based on matrix (18) can be investigated
independently of AMP using random matrix theory. An analysis of the random matrix (18) shows
that a spectral phase transition for generative prior with linear activations appears at ∆c = 1 + α
(as for AMP). This transition is analogous to the well-known BBP transition [38]  but a non-GOE
random matrix (18) needs to be analyzed. For the spiked Wigner models with linear generative prior
we prove two theorems describing the behavior of the supremum of the bulk spectral density  the
transition of the largest eigenvalue and the correlation of the corresponding eigenvector:
Theorem 2 (Bulk of the spectral density  spiked Wigner  linear activation). Let α  ∆ > 0  then:
p converges almost surely and in the weak sense to a compactly
(i) The spectral measure of Γvv
supported probability measure µ(α  ∆). We denote λmax the supremum of the support of µ(α  ∆).
(ii) For any α > 0  as a function of ∆  λmax has a unique global maximum  reached exactly at the
point ∆ = ∆c(α) = 1 + α. Moreover  λmax(α  ∆c(α)) = 1.
Theorem 3 (Transition of the largest eigenvalue and eigenvector  spiked Wigner  linear activation).
p . If ∆ ≥ ∆c(α)  then as
Let α > 0. We denote λ1 ≥ λ2 the ﬁrst and second eigenvalues of Γvv
p → ∞ we have a.s. λ1→λmax and λ2→λmax. If ∆ ≤ ∆c(α)  then as p → ∞ we have a.s. λ1→1
and λ2→λmax. Further  denoting ˜v a normalized ((cid:107)˜v(cid:107)2 = p ) eigenvector of Γvv
p with eigenvalue λ1 
then |˜v(cid:124)v(cid:63)|2/p2→(∆) a.s.  where (∆) = 0 for all ∆ ≥ ∆c(α)  (∆) > 0 for all ∆ < ∆c(α) and
lim∆→0 (∆) = 1.
Thm. 2 and Thm. 3 are illustrated in Fig. 3. The proof gives the value of (∆)  which turns out to
lead to the same MSE as in Fig. 3 in the linear case. We state the theorems counterparts for the uv(cid:124)
linear case in SM sec. 7. The proofs of the theorems and the precise arguments used to derive the
eigenvalue density  the transition of λ1 and the computation of (∆) are given in SM sec. 7  and a
Mathematica demonstration notebook is also provided in [37]. We also describe in SM the difﬁculties
to circumvent to generalize the analysis to a non-linear activation function with random matrix theory.

9

5 Acknowledgments

This work is supported by the ERC under the European Union’s Horizon 2020 Research and Inno-
vation Program 714608-SMiLe  as well as by the French Agence Nationale de la Recherche under
grant ANR-17-CE23-0023-01 PAIL. We gratefully acknowledge the support of NVIDIA Corporation
with the donation of the Titan Xp GPU used for this research. We thank Google Cloud for providing
us access to their platform through the Research Credits Application program. We would also like to
thank the Kavli Institute for Theoretical Physics (KITP) for welcoming us during part of this research 
with the support of the National Science Foundation under Grant No. NSF PHY-1748958. We thank
Ahmed El Alaoui for insightful discussions about the proof of the Bayes optimal performance  and
Remi Monasson for his insightful lecture series that inspired partly this work. Additional funding is
acknowledged by AM from ‘Chaire de recherche sur les modèles et sciences des données’  Fondation
CFM pour la Recherche-ENS.

10

References
[1] Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy

employed by v1? Vision research  37(23):3311–3325  1997.

[2] David L Donoho. Compressed sensing. IEEE Transactions on information theory  52(4):1289–

1306  2006.

[3] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[4] Eric W Tramel  Andre Manoel  Francesco Caltagirone  Marylou Gabrié  and Florent Krzakala.
Inferring sparsity: Compressed sensing using generalized restricted Boltzmann machines. In
2016 IEEE Information Theory Workshop (ITW)  pages 265–269. IEEE  2016.

[5] Ashish Bora  Ajil Jalal  Eric Price  and Alexandros G Dimakis. Compressed sensing using
generative models. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70  pages 537–546. JMLR. org  2017.

[6] Andre Manoel  Florent Krzakala  Marc Mézard  and Lenka Zdeborová. Multi-layer generalized
linear estimation. In 2017 IEEE International Symposium on Information Theory (ISIT)  pages
2098–2102. IEEE  2017.

[7] Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by

empirical risk. In Conference On Learning Theory  pages 970–978  2018.

[8] Alyson K Fletcher  Sundeep Rangan  and Philip Schniter. Inference in deep networks in high
dimensions. In 2018 IEEE International Symposium on Information Theory (ISIT)  pages
1884–1888. IEEE  2018.

[9] Paul Hand  Oscar Leong  and Vlad Voroninski. Phase retrieval under a generative prior. In

Advances in Neural Information Processing Systems  pages 9136–9146  2018.

[10] Dustin G Mixon and Soledad Villar. Sunlayer: Stable denoising with generative networks.

arXiv preprint arXiv:1803.09319  2018.

[11] Soledad

Villar.

Generative

https://solevillar.github.io/2018/03/28/SUNLayer.html  2018.

models

are

the

new

sparsity?

[12] Hui Zou  Trevor Hastie  and Robert Tibshirani. Sparse principal component analysis. Journal

of computational and graphical statistics  15(2):265–286  2006.

[13] Rodolphe Jenatton  Guillaume Obozinski  and Francis Bach. Structured sparse principal
component analysis. In Proceedings of the Thirteenth International Conference on Artiﬁcial
Intelligence and Statistics  pages 366–373  2010.

[14] Sundeep Rangan and Alyson K Fletcher. Iterative estimation of constrained rank-one matrices
in noise. In Information Theory Proceedings (ISIT)  2012 IEEE International Symposium on 
pages 1246–1250. IEEE  2012.

[15] Yash Deshpande and Andrea Montanari. Information-theoretically optimal sparse PCA. In
2014 IEEE International Symposium on Information Theory  pages 2197–2201. IEEE  2014.
[16] Thibault Lesieur  Florent Krzakala  and Lenka Zdeborová. Phase transitions in sparse PCA. In
2015 IEEE International Symposium on Information Theory (ISIT)  pages 1635–1639. IEEE 
2015.

[17] Amelia Perry  Alexander S. Wein  Afonso S. Bandeira  and Ankur Moitra. Optimality and
sub-optimality of pca i: Spiked random matrix models. Ann. Statist.  46(5):2416–2451  10
2018.

[18] Marc Lelarge and Léo Miolane. Fundamental limits of symmetric low-rank matrix estimation.

Probability Theory and Related Fields  173(3-4):859–929  2019.

[19] Jean Barbier  Mohamad Dia  Nicolas Macris  Florent Krzakala  Thibault Lesieur  and Lenka
Zdeborová. Mutual information for symmetric rank-one matrix estimation: A proof of the
replica formula. In Advances in Neural Information Processing Systems  pages 424–432  2016.
[20] Léo Miolane. Fundamental limits of low-rank matrix estimation: the non-symmetric case. arXiv

preprint arXiv:1702.00473  2017.

11

[21] Thibault Lesieur  Florent Krzakala  and Lenka Zdeborová. Constrained low-rank matrix
estimation: phase transitions  approximate message passing and applications. Journal of
Statistical Mechanics: Theory and Experiment  2017(7):073403  2017.

[22] Arash A Amini and Martin J Wainwright. High-dimensional analysis of semideﬁnite relaxations

for sparse principal components. The Annals of Statistics  pages 2877–2921  2009.

[23] Quentin Berthet and Philippe Rigollet. Computational lower bounds for sparse PCA. arXiv

preprint arXiv:1304.0828  2013.

[24] Yash Deshpande and Andrea Montanari. Sparse PCA via covariance thresholding. In Advances

in Neural Information Processing Systems  pages 334–342  2014.

[25] Yash Deshpande  Emmanuel Abbe  and Andrea Montanari. Asymptotic mutual information for
the balanced binary stochastic block model. Information and Inference: A Journal of the IMA 
6(2):125–170  2016.

[26] Florent Krzakala  Jiaming Xu  and Lenka Zdeborová. Mutual Information in Rank-One Matrix
Estimation. 2016 IEEE Information Theory Workshop (ITW)  pages 71–75  September 2016.
arXiv: 1603.08447.

[27] Ahmed El Alaoui and Florent Krzakala. Estimation in the spiked Wigner model: A short proof
of the replica formula. In 2018 IEEE International Symposium on Information Theory (ISIT) 
pages 1874–1878  June 2018.

[28] Ahmed El Alaoui  Florent Krzakala  and Michael I Jordan. Finite size corrections and likelihood

ratio ﬂuctuations in the spiked Wigner model. arXiv preprint arXiv:1710.02903  2017.

[29] Jean-Christophe Mourrat. Hamilton-Jacobi equations for ﬁnite-rank matrix inference. arXiv

preprint arXiv:1904.05294  2019.

[30] Jean Barbier  Florent Krzakala  Nicolas Macris  Léo Miolane  and Lenka Zdeborová. Optimal
errors and phase transitions in high-dimensional generalized linear models. Proceedings of the
National Academy of Sciences  116(12):5451–5460  2019.

[31] Marylou Gabrié  Andre Manoel  Clément Luneau  Jean Barbier  Nicolas Macris  Florent
Krzakala  and Lenka Zdeborová. Entropy and mutual information in models of deep neural
networks.
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and
R. Garnett  editors  Advances in Neural Information Processing Systems 31  pages 1821–1831.
Curran Associates  Inc.  2018.

[32] Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons 

2012.

[33] Dongning Guo  S. Shamai  and S. Verdú. Mutual information and minimum mean-square error
in gaussian channels. IEEE Transactions on Information Theory  51(4):1261–1282  April 2005.
[34] Christopher A Metzler  Arian Maleki  and Richard G Baraniuk. From denoising to compressed

sensing. IEEE Transactions on Information Theory  62(9):5117–5144  2016.

[35] Raphaël Berthier  Andrea Montanari  and Phan-Minh Nguyen. State evolution for approximate
message passing with non-separable functions. Information and Inference: A Journal of the
IMA  2019.

[36] Adel Javanmard and Andrea Montanari. State evolution for general approximate message
passing algorithms  with applications to spatial coupling. Information and Inference: A Journal
of the IMA  2(2):115–144  2013.

[37] Benjamin Aubin  Bruno Loureiro  Antoine Maillard  Florent Krzakala  and Lenka Zdeborová.
Demonstration codes - the spiked matrix model with generative priors. https://github.com/
benjaminaubin/StructuredPrior_demo.

[38] Jinho Baik  Gérard Ben Arous  Sandrine Péché  et al. Phase transition of the largest eigenvalue
for nonnull complex sample covariance matrices. The Annals of Probability  33(5):1643–1697 
2005.

[39] F. Krzakala  C. Moore  E. Mossel  J. Neeman  A. Sly  L. Zdeborová  and P. Zhang. Spectral
redemption in clustering sparse networks. Proceedings of the National Academy of Sciences 
110(52):20935–20940  December 2013.

[40] Han Xiao  Kashif Rasul  and Roland Vollgraf. Fashion-mnist: a novel image dataset for

benchmarking machine learning algorithms  2017.

12

,Benjamin Aubin
Bruno Loureiro
Antoine Maillard
Florent Krzakala
Lenka Zdeborová