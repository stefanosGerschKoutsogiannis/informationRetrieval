2016,Learning Sparse Gaussian Graphical Models with Overlapping Blocks,We present a novel framework  called GRAB (GRaphical models with overlApping Blocks)  to capture densely connected components in a network estimate. GRAB takes as input a data matrix of p variables and n samples  and jointly learns both a network among p variables and densely connected groups of variables (called `blocks'). GRAB has four major novelties as compared to existing network estimation methods: 1) It does not require the blocks to be given a priori. 2) Blocks can overlap. 3) It can jointly learn a network structure and overlapping blocks. 4) It solves a joint optimization problem with the block coordinate descent method that is convex in each step. We show that GRAB reveals the underlying network structure substantially better than four state-of-the-art competitors on synthetic data. When applied to cancer gene expression data  GRAB outperforms its competitors in revealing known functional gene sets and potentially novel genes that drive cancer.,Learning Sparse Gaussian Graphical Models with

Overlapping Blocks

Mohammad Javad Hosseini1

Su-In Lee1 2

1Department of Computer Science & Engineering  University of Washington  Seattle

2Department of Genome Sciences  University of Washington  Seattle

{hosseini  suinlee}@cs.washington.edu

Abstract

We present a novel framework  called GRAB (GRaphical models with overlApping
Blocks)  to capture densely connected components in a network estimate. GRAB
takes as input a data matrix of p variables and n samples and jointly learns both
a network of the p variables and densely connected groups of variables (called
‘blocks’). GRAB has four major novelties as compared to existing network es-
timation methods: 1) It does not require blocks to be given a priori. 2) Blocks
can overlap. 3) It can jointly learn a network structure and overlapping blocks. 4)
It solves a joint optimization problem with the block coordinate descent method
that is convex in each step. We show that GRAB reveals the underlying network
structure substantially better than four state-of-the-art competitors on synthetic data.
When applied to cancer gene expression data  GRAB outperforms its competitors
in revealing known functional gene sets and potentially novel cancer driver genes.

1

Introduction

Many real-world networks contain subsets of variables densely connected to one another  a property
called modularity (Fig 1A); however  standard network inference methods do not incorporate this
property. As an example  biologists are increasingly interested in understanding how thousands of
genes interact with each other on the basis of gene expression data that measure expression levels of
p genes across n samples. This has stimulated considerable research into the structure estimation
of a network from high-dimensional data (p  n). It is well-known that the network structure
corresponds to the non-zero pattern of the inverse covariance matrix  ⌃1 [1]. Thus  obtaining a
sparse estimate of ⌃1 by using `1 penalty has been a standard approach to inferring a network  a
method called graphical lasso [2]. However  applying an `1 penalty to each edge fails to reﬂect the
fact that genes involved in similar functions are more likely to be connected with each other and that
how genes are organized into functional modules are often not known.
We present a novel structural prior  called GRAB prior  which encourages the network estimate to
be dense within a block (i.e  a subset of variables) and sparse between blocks  where blocks are not
given a priori. Fig 1B illustrates the effectiveness of the GRAB prior (bottom) in a high-dimensional
setting (p = 200 and n = 100)  where it is difﬁcult to reveal the true underlying network by using
the graphical lasso (GLasso) (top). The major novelty of GRAB is four-fold:
First  unlike previous work [3  4  5]  GRAB allows each variable to belong to more than one block 
which is an important property of many real-world networks. For example  genes important in disease
processes are often involved in multiple functional modules [6]  and identifying such genes would be
of great scientiﬁc interest (Section 4.2). Although existing methods to learn non-overlapping blocks
allow edges between different blocks  they use stronger regularization parameters for between-block
edges  which decreases the power to detect variables associated with multiple blocks.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Second  GRAB jointly learns the network structure and the assignment of variables into overlapping
blocks (Fig 2). Existing methods to incorporate blocks in network learning either use blocks given a
priori or use a sequential approach to learn blocks and then learn a network given the blocks held
ﬁxed. Interestingly  the GRAB algorithm can be viewed as a generalization of the joint learning of the
distance metric among p variables and graph-cut clustering of p variables into blocks (Section 3.4)
Third  GRAB solves a joint optimization problem with the block coordinate descent method that is
convex in each step. This is a powerful feature that is difﬁcult to be achieved by existing methods
to cluster variables into blocks. This property guarantees the convergence of the learning algorithm
(Section 3).
Finally  the GRAB framework we presented in this paper uses the Gaussian graphical model as a

baseline model. However  the GRAB prior  formulated as trZZ||⇥| (Section 2.2)  can be used in

any kind of network models such as pairwise Markov random ﬁelds.
In the following sections  we show that GRAB outperforms the graphical lasso [2] and existing
methods to learn blocks and network estimates [3  4] on synthetic data and cancer gene expression
data. We also demonstrate GRAB’s potential to identify novel genes that drive cancer.

(A)$

x2	

x4	

x3	

x1	

(B)$

x5	

x6	

x7	

x8	

x1	

x6	

x7	

x8	

x2	

x3	

x4	

x5	

x1	

x2	

x3	

x4	

x5	

x6	

x7	

x8	

Figure 1: (A) A network with overlapping
blocks (top) and its adjacency matrix (bot-
tom). (B) Network estimates of GLasso (top)
and GRAB (bottom) in a toy example.

2 GGM with Overlapping Blocks

Z:#assignment#matrix#
(A)#

K#

ZT#
p#

K#

Zi#

p#

Zj

T#

(B)#
!
#
p#

(ZZT)ij#

ZZT#
p#

Block#1#

The#similarity#between#ith#
and#jth#variables##

#

p
e
t
s
.
Z

ZT#
p#

Learning#Z##

Z#

K#

(C)#

K#

p#

θ
.
s
t
e
p

#

Learning#θ##

θ#

p#

(D)#
!

#
p#

Sparsity#paBern#
encouraged#by#ZZT#

Figure 2: The GRAB framework – an iterative
algorithm that jointly learns ⇥ and Z.

Block#2#

Block#3#

SBM#

(E)#GRAB#

λ1#

λ2#

λw#
λb#

λ1 #λ2:#based#on#Z#
λw:#within?block#
λb:#between?block#

2.1 Background: High-Dimensional Gaussian Graphical Model (GGM)
We aim to learn a GGM of p variables on the basis of n observations (p  n). That is  suppose that
X(1)  . . .   X(n) are i.i.d. N (µ  ⌃)  where µ 2 Rp and ⌃ is a p ⇥ p positive deﬁnite matrix. It is well
known that the sparsity pattern of ⌃1 determines the conditional independence structure of the p
variables; there is an edge between the ith and jth variables if and only if the (i  j) element of ⌃1 is
non-zero [1]. A number of authors have proposed to estimate ⌃1 using the graphical lasso [2  7  8]:

maximize

⇥⌫0

log det ⇥  tr(S⇥)  k⇥k1 

(1)

where the solution b⇥ is an estimate of ⌃1  S denotes the empirical covariance matrix  and  is a

nonnegative tuning parameter that controls the strength of the `1 penalty applied to the elements of
⇥. This amounts to maximizing a penalized log-likelihood.

2

2.2 GGM with the Overlapping Block Prior

Here  we present the GRAB prior  formulated as trZZ||⇥|  that encourages ⇥ to have overlapping
blocks. Let X = {X1  . . .   Xp} be variables in the network and Z be a real matrix of size p ⇥ K 
where K is the total number of blocks. Each element 1  Zik  1 can be interpreted as a score
representing how likely the ith variable Xi belongs to the kth block Bk. The ith row of Z  denoted
by Zi  can be interpreted as a low-rank embedding for the variable Xi showing its block assignment
scores. Then  the (i  j) element (ZZ|)ij =⌃ K
k=1ZikZjk (the dot product of Zi and Zj) represents
the similarity between variables Xi and Xj in their embeddings.
To more clearly understand the impact of the GRAB prior on the sparsity structure of ⇥  let us
assume a hard assignment model in which we assign variables to blocks. Then  Z becomes a binary
matrix and the sparsity pattern of ZZ| would indicate the region covered by all K blocks (Fig 2A-B).
Then  jointly learning Z and ⇥ to increase ⌃i j(ZZ|)ij|⇥ij| would encourage ⇥ to have a sparsity
structure imposed by (ZZ|). In the continuous case  it would encourage |⇥ij| to be non-zero when
Xi and Xj have similar embeddings (i.e.  a dot product of Zi and Zj is large).
Incorporating the GRAB prior into Eq (1) as a structural prior leads to:

where  is a non-negative tuning parameter. We can re-write Eq (2) as:

maximize
⇥⌫0 Z2D

maximize
⇥⌫0 Z2D

log det ⇥  tr(S⇥)  ⇣k⇥k1  trZZ||⇥|⌘ 
⇣1  (ZZ|)ij⌘|⇥ij|.
log det ⇥  tr(S⇥) Xi j

(2)

(3)

We use the value of the sparsity tuning parameter 1  (ZZ|)ij for each (i  j) element ⇥ij. A
network edge that corresponds to two variables with similar embeddings would be penalized less.
The set D⇢ [1  1]p⇥K contains matrices Z satisfying the following constraints: (a) kZik2  1 
where Zi denotes the ith row of Z. This constraint ensures the regularization parameters of all (i  j)
pairs of variables are non-negative. (b) kZkF  . In addition to the variable speciﬁc constraint
on each Zi in (a)  we need a global constraint on Z to prevent all regularization parameters from
becoming zero (8i  j : (ZZT )ij = 1). (c) kZk2  ⌧  where k.k2 of a matrix is its maximum singular
value. This constraint prevents the case where all variables are assigned to one block.
There are two hyperparameters   and ⌧; however we describe below that we set ⌧ = pK and that
has an effect to guarantee that there are at least K non-empty blocks. In our experiments  we set the
hyper-parameter  =p p
2   which  intuitively  would allow each variable to get on average half of
i where i is the ith singular value
its largest possible squared norm. Given that kZk2
of Z  from the constraint (b) Pp
i  2. We set ⌧ = pK   where ⌧ means the upper bound
of the maximum singular value  given the constraint (c). This means that there would be at least
K non-empty blocks given that the constraint (b) is tight. We show in Section 3 that this choice of
hyperparameters makes our learning algorithm simpler (see Lemma 3.2).

F =Pp

i=1 2

i=1 2

2.3 Probabilistic Interpretation
The joint distribution over X  ⇥ and Z is as: P (X  ⇥  Z) = P (X|⇥)P (⇥|Z)P (Z). The ﬁrst two
terms  log det(⇥)  trace(S⇥)  in Eq (3) correspond to log P (X|⇥)  the log-likelihood of GGM
given a particular parameter ⇥ (i.e.  an estimate of ⌃1)  as described in Section 2.1. For ⇥ ⌫ 0 
P (⇥|Z) =Q P (⇥ij|Z)  where P (⇥ij|Z) represents a conditional probability over ⇥ij given the
block assignment scores of Xi and Xj. We use the Laplacian prior with the sparsity parameter value
DQ(i j) exp(((1  (ZZ|)ij))|⇥ij|)  where D is the
(1  (ZZ|)ij). For ⇥ ⌫ 0  P (⇥|Z) is: 1
normalization constant. The prior probability P (Z) is proportional to D.

2.4 Related Work

To our knowledge  GRAB is the ﬁrst attempt to jointly learn the overlapping blocks and the structure
of a conditional dependence network such as a GGM. Related work consists of 3 categories:

3

1) Learning blocks with a network held ﬁxed: This category includes (a) stochastic block model
(SBM) [9]  (b) spectral clustering [10]  and (c) a screening rule to identify non-overlapping blocks
based on the empirical covariance matrix [11].
2) Learning a network with blocks given a priori and held ﬁxed: This category includes a) a method
to solve graphical lasso with group `1 penalty to encourage group sparsity of edges within pairs of
blocks [12]  and b) an efﬁcient learning algorithm for GGMs given a set of overlapping blocks [13].
3) Learning non-overlapping blocks ﬁrst and then the network given the blocks: (a) Marlin et
al. (2009) extend the prior work [12] to identify non-overlapping blocks which are then used to
learn a network [3]. (b) Another method assigns each variable to one block  and use different
regularization parameters for within-block and between-block edges [14]. (c) Tan et al. (2015)
propose to use hierarchical clustering (complete-linkage and average-linkage) to cluster variables
into non-overlapping blocks  and apply graphical lasso to each block [4].

3 GRAB Learning Algorithm
3.1 Overview
Our learning algorithm jointly learns the block assignment scores Z and the network estimate ⇥ by
solving Eq (2). We adopt the block coordinate descent (BCD) method to iteratively learn Z and ⇥. Our
learning algorithm essentially performs adaptive distance (similarity) metric learning and clustering
of variables into blocks simultaneously (Section 3.4). Given the current assignment of variables into
blocks  Z  we learn a network among variables  ⇥. Then  |⇥| is used as a similarity matrix among
variables to update the assignment of variables to blocks  Z. We iterate until convergence.
Convergence is theoretically guaranteed. Since our objective function is continuous on a compact
level set  based on Theorem 4.1 in [15]  the solution sequence of our method is deﬁned and bounded.
Every coordinate block found by the ⇥-step and Z-step is a stationary point of GRAB. We indeed
observed the value of the objective function monotonically increases until convergence.
In the following  we show that the BCD method will be convex in each step. We ﬁrst re-write Eq (2)
with all the constraints explicitly:

log det ⇥  tr(S⇥)  ⇣k⇥k1  trZZ||⇥|⌘

maximize
⇥⌫0 Z
subject to kZk2  ⌧  kZik2  1  kZkF    (i 2{ 1  . . . p})).

Now  we state the following lemma  the proof of which can be found in the Appendix.
Lemma 3.1 Eq (4) is equivalent to the following:

log det ⇥  tr(S⇥)  ⇣k⇥k1  trW|⇥|⌘

maximize
⇥⌫0 W⌫0
subject to rank(W)  K  W  ⌧ 2I  diag(W)  1  tr(W)  2 

where W is a p ⇥ p matrix  K means the number of blocks  and I is the identity matrix of size p.1
Corollary 3.1.1 Suppose that (⇥⇤  W⇤) is the optimal solution of the optimization problem (5).
Then  ⇥⇤  Z⇤ = UpD is the optimal solution of problem 4  where U 2 Rp⇥K is a matrix with

columns containing K eigenvectors of W corresponding to the largest eigenvalues and D is a
diagonal matrix of the corresponding eigenvalues.

(4)

(5)

3.2 Learning ⇥ (⇥-step)
To estimate ⇥ given Z  based on Eq (3)  we solve the following problem:

maximize

(6)
where ⇤ij = (1 (ZZ|)ij). This is the graphical lasso with edge-speciﬁc regularization parameters
⇤ij. Eq (6) is a convex problem and we solve it by adopting a standard solver for graphical lasso [16].
1 In this paper  we assume diag is an operator that maps a vector to a diagonal matrix with the vector as its

log det ⇥  tr(S⇥) P(i j) ⇤ij|⇥ij| 

⇥⌫0

diagonal  and maps a matrix to a vector containing its diagonal.

4

3.3 Learning Z (Z-step)
Here we describe how to learn Z given ⇥. Instead of solving (4)  we solve (5) because (5) is a
convex optimization problem with respect to W. Interestingly  we can remove the rank constraint 
rank(W)  K; in Lemma 3.2  we show that with the choice of ⌧ = pK   the rank constraint is
automatically satisﬁed. This leads to the following optimization problem:

trW|⇥|

maximize
W⌫0
subject to W  ⌧ 2I  diag(W)  1  tr(W)  2.

This W-step is a semi-deﬁnite programming problem. We solve the dual of Eq (7) that leads to an
efﬁcient optimization problem.2 We introduce three dual variables: 1) a matrix Y ⌫ 0 for the `2
norm constraint  2) a vector v 2 Rp
+ for the constraints on the diagonal and 3) a scalar y  0 for the
constraint on trace. The Lagrangian is:
L(W  Y  v   y) = trW|⇥| + tr(⌧ 2I  W)Y + y(2  tr(W)) + vT (1  diag(W)). (8)

The dual function is as:

sup
W⌫0

trW|⇥| + tr(⌧ 2I  W)Y + y(2  trW)) + v|(1  diag(W))
=⇢⌧ 2tr(Y ) + y + vT 1 if Y ⌫| ⇥| yI  diag(v)

trW(|⇥| Y  yI  diag(v)) + ⌧ 2tr(Y ) + y + v|1

= sup
W⌫0

otherwise

.

consequently  we get the following dual problem for Eq (7):

⌧ 2tr(Y ) + y 2 + v|1

+1
minimize

Y y v

(7)

(9)

(10)

(11)

(13)

Eq (10) has a closed form solution in Y and y given that v is ﬁxed. The dual problem boils down to:

subject to Y ⌫|⇥| yI  diag(v)  Y ⌫ 0  y  0  v  0.

minimize

g(v) = minimize

⌧ 2

v0

v0

KXi=1C+ i + v|1 

where we have replaced 2
assume it has eigenvalues (1  . . . p) in descending order and (C)+ i = max(0  i). We solve
Eq (11) by projected subgradient descent method where the subgradient direction is:

⌧ 2 with K (because ⌧ = /pK). We deﬁne C = (|⇥| diag(v)) and
rvg(v) = ⌧ 2 diagUC1K(DC)U|

(12)
DC is the diagonal matrix of eigenvalues in descending order and UC is the matrix containing
orthonormal eigenvectors of C as its columns. We deﬁne 1K(DC) as a binary vector of size p with
jth element equal to 1 if and only if j  K and j > 0.
After ﬁnding the optimal v⇤  the optimal solution W⇤ can be obtained by:

C + 1.

W⇤ = argmax
W⌫0

trW(|⇥| diag(v⇤))
subject to W  ⌧ 2I  tr(W)  2.

the solution of problem (13) is W ⇤ = ⌧ 2UC⇤12/⌧ 2(DC⇤)U|

One can see that
C⇤ =
⌧ 2UC⇤1K(DC⇤)U|
C⇤  where C⇤  UC⇤  DC⇤ and 1K(DC⇤) are deﬁned similarly to (12). By deﬁni-
tion  1K(.) is a diagonal matrix with at most K nonzeros elements. Therefore  W ⇤ will have rank at
most K  which means that we do not need the rank constraint on W. This leads to the following
lemma.
Lemma 3.2 If we set ⌧ = pK
in (5)  the constraint rank(W)  K will be automatically satisﬁed.
Finally  we construct Z⇤ = UpD as instructed in corollary 3.1.1. Note that in the intermediate
iterations  we do not need to compute Z; we need to construct the matrix Z⇤ to ﬁnd the overlapping
blocks after the learning algorithm will converge3.

2The primal problem has a strictly feasible solution ✏I  where ✏ is a small number and I is the identity matrix;

therefore strong duality holds.

3The source code is available at: http://suinlee.cs.washington.edu/software/grab

5

3.4 A special case: K-way graph cut algorithm

Here  we show that GRAB algorithm generalizes the K-way graph cut algorithm in two ways: 1)
GRAB allows each variable to be in multiple blocks with soft membership; and 2) GRAB updates a
network structure ⇥  used as a similarity matrix  in each iteration. The proof is in the Appendix.

Lemma 3.3 Say that we use a binary matrix Z (hard assignment) with the following constraints: a)
For all variables i  kZik2  1  where Zi denotes the ith row of Z. b) For all blocks k  kZkk2 >= 1 
where Zk denotes the kth column of Z. This means that each variable can belong to only one block
(i.e.  non-overlapping blocks)  and each block has at least one variable. Then GRAB is equivalent to
iterating between K-way graph-cut on |⇥| to ﬁnd Z and solving graphical lasso problem to ﬁnd ⇥.

4 Experimental Results

We present results on synthetically generated data and real data.
Comparison. Three state-of-the-art competitors are considered: UGL1 - unknown group `1 regu-
larization [3]; CGL - cluster graphical lasso [4]; and GLasso - standard graphical lasso [2]. CGL
has two variants depending on the type of hierarchical clustering used: average linkage clustering
(CGL:ALC) and complete linkage clustering (CGL:CLC). Each method selects the regularization
parameter using the standard cross-validation (or held out validation) procedure.
CGL and UGL1 have their own ways of selecting the number of blocks K [4  3]. GRAB selects
K based on the validation-set log-likelihood in initialization. We initialize GRAB by constructing
the Z matrix. We ﬁrst perform spectral clustering on |S|  where S denotes the empirical covariance
matrix  then add overlap by assigning a random subset of variables to clusters with the highest
average correlation. Then  we project the Z matrix into the convex set deﬁned in Section 2.2 and
form W = ZZ|. In the Z-step of the GRAB learning algorithm  we use step size 1/pt  where t is
the iteration number and iterate until the relative change in the objective function is less than 106
(Section 3.3). We use the warm-start technique between the BCD iterations.
Evaluation criteria. In the synthetic data experiments (sectoin 4.1)  we evaluate each method based
on the learned network with the optimal regularization parameter chosen for each method based on
only training-set. For the AML dataset (Section 4.2)  we evaluate the learned blocks for varying
regularization parameters (x-axis) to better illustrate the difference among the methods in terms of
their performances. In all experiments  we standardize the data and show the average results over 10
runs and the standard deviations as error bars.

4.1 Synthetic Data Experiments

Data generation. We ﬁrst generate  overlapping blocks forming a chain  a random tree or a lattice.
In each case  two neighboring blocks overlap each other by o (the ratio of the variables shared between
two overlapping blocks). Then  we randomly generate a true underlying network of p variables with
density of 20%  and convert it to the precision matrix following the procedure of [17]. We generate
100 training samples and 50 validation samples from the multivariate Gaussian distribution with
mean zero and the covariance matrix equal to the inverse of the precision matrix.
We consider a varying number of true blocks  2{ 9  25  49} and overlap ratio o = .25. For
 = 25  we consider o 2{ .1  .25  .4}. We vary the number of variables p 2{ 400  800} for the
lattice-structured blocks. The results on the chain and random tree blocks are similar and so we
provide only the results for p = 400 for these block structures. For all methods  we considered the
regularization parameter  2 [.02  .4] with step size .02.
Results. Fig 3 compares ﬁve methods when a regularization parameter was selected for each method
based on the 50 validation samples. Each of the four plots correspond to different block structure or
number of variables. Each bar group corresponds to a particular (  o  ⌘)  in which we computed the
modularity measure ⌘ as (fraction of edges that fall within groups - expected fraction if edges were
distributed at random)  as was done by [18]. Fig 3A shows how accurately each method recovers
the true network. For each method m  we compared the learned edges (EZ m) and that from the
underlying network (EZ). By comparing EZ m and EZ  we can compute the precision and recall

6

La/ce:'p=400 n=100'

La/ce:'p=800 n=100'

Chain:'p=400 n=100'

Random:'p=400 n=100'

κ=9'
o=.25'
η=0.85'

κ=25'
o=.4'
η=0.88'

κ=25'
o=.25'
η=0.91'
'

κ=25'
o=.1'
η=0.96'

κ=49'
o=.25'
η=0.93'

κ=9'
o=.25'
η=0.85'

κ=25'
o=.4'
η=0.88'

κ=25'
o=.25'
η=0.91'
'

κ=25'
o=.1'
η=0.96'

κ=49'
o=.25'
η=0.93'

κ=10'
o=.25'
η=0.85'

κ=30'
o=.4'
η=0.94'

κ=30'
o=.1'
η=0.96'

κ=50'
o=.25'
η=0.97'

κ=30'
o=.25'
η=0.95'
'

κ=10'
o=.25'
η=0.87'

κ=30'
o=.4'
η=0.93'

κ=30'
o=.25'
η=0.96'
'

κ=30'
o=.1'
η=0.97'

κ=50'
o=.25'
η=0.96'

Figure 3: Comparison based on average network recovery F1 on synthetic data from lattice blocks 
when p = 400 (ﬁrst panel) and p = 800 (second panel)  chain blocks (third panel) and random
blocks (fourth panel) when p = 400. Each bar group corresponds to a particular (number of blocks  
overlap ratio o  modularity ⌘).

pr+rec as an evaluation metric.

of network recovery. Since it is not enough to get only high precision or recall  we use the F1 (or
F-measure) = 2 pr⇤rec
A number of authors have shown that identifying the underlying network structure is very challenging
in the high-dimensional setting  resulting in low accuracies even on synthetic data [14  19  4]. Our
results also show that the F1 scores for network are lower than 0.40. Despite that  GRAB identiﬁes
network edges much more accurately than its competitors.

4.2 Cancer Gene Expression Data

We consider the MILE data [20] that measure the mRNA expression levels of 16 853 genes in 541
patients with acute myeloid leukemia (AML)  an aggressive blood cancer. For a better visualization
of the network in limited space (Fig 5)  we selected 500 genes4  consisting of 488 highest varying
genes in MILE and 12 genes highly associated with AML: FLT3  NPM1  CEBPA  KIT  N-RAS 
MLL  WT1  IDH1/2  TET2  DNMT3A  and ASXL1. These genes are identiﬁed by [21] in a large
study on 1 185 patients with AML to be signiﬁcantly mutated in these AML patients. These genes
are well-known to have signiﬁcant role in driving AML.
Here  we evaluate GRAB and the other methods qualitatively in terms of how useful each method is
for cancer biologists to make discovery from data. For that  we ﬁx the number of blocks to be K = 10
across all methods such that we get average of over 50 variables per block  which is considered close
to the average number of genes in known pathways [22]. We varied K and obtained similar results.
Genes in the same block are likely to share similar functions. Statistical signiﬁcance of the overlap
between gene clusters (here  blocks) and known functional gene sets have been widely used as an
evaluation criteria [23  5]. We show how to obtain blocks from the learned Z.
Obtaining blocks from Z. After the GRAB algorithm converges  we obtain a network estimate ⇥
and a block membership matrix Z. We ﬁnd K overlapping blocks satisfying two constraints: a)
maximum number of assignments is C; and b) each variable is assigned to  1 block. Here  we
used C = 1.3p. We perform the following greedy procedure: 1) We ﬁrst run k-means clustering
algorithm on the p rows of the matrix Z.5. 2) We compute the similarity of variables i to blocks
(ZZ|)ij  where |Bk| is the number of variables in Bk. Then  we add overlap by
Bk as
assigning C  p variables to blocks with highest similarity.
To evaluate the blocks  we used 4 722 curated gene sets from the molecular signature database [24]
and computed a p-value to measure the signiﬁcance of the overlap between each block and each
gene set. We consider the (block  gene set) pairs with false discovery rate (FDR)-corrected p < 0.05
to be signiﬁcantly overlapping pairs. When a block is signiﬁcantly overlapped with a gene set 
we consider the gene set to be revealed by the corresponding block. We compare GRAB with the

|Bk|Pj2Bk

1

4GRAB runs for 0.5-1.5 hours for 500 genes and up to 20 hours for 2 000 genes on a computer with 2.5 GHz

Intel Core i5 processor

5This resembles spectral clustering (equivalently  kmeans on eigenvectors of Laplacian matrix)

7

methods introduced in section 4.1. Since we only need the blocks for this experiment  we added
two more competitors: k-means and spectral clustering methods applied to |S|  where S denotes the
empirical covariance matrix. Fig 4 shows the number of gene sets that are revealed by any block
(FDR-corrected p < 0.05) in each method. GRAB signiﬁcantly outperforms  which indicates the
importance of learning overlapping blocks; GRAB’s overlapping blocks reveal known functional
organization of genes better than other methods. Fig 4 shows the average results of 10 random
initializations.
Fig 5 compares the learned networks ⇥ by GLasso (A) and GRAB (B) when the regularization
parameters are set such that the networks show a similar level of sparsity. For GRAB  we removed
the between-block edges and reordered genes such that the genes in the same blocks tend to appear
next to each other. GRAB shows more interpretable network structure  highlighting the genes that
belong to multiple blocks.
The key innovation of GRAB is to allow for overlap between blocks. Interestingly  the 12 well-known
AML genes are signiﬁcantly enriched for the genes assigned to 3 or more blocks: FLT3  NPM1  TET2
and DNMT3A belong to 3 blocks while there are only 24 such genes out of 500 genes (p-value: 0.001)
(Fig 5B). This supports our claim that variables assigned to multiple blocks are likely important. Out
of the 24 genes assigned to  3 blocks  12 are known to be involved in myeloid differentiation (the
process impaired in AML) or other types of cancer. This can lead to new discovery on the genes that
drive AML.
These genes include CCNA1 that has shown to be signiﬁcantly differentially expressed in AML
patients [25]. TSPAN7 is expressed in acute myelocytic leukemia of some patients6. Several genes
are associated with other types of cancer. For example  CCL20 is associated with pancreatic cancer
[26]. ELOVL7 is involved in prostate cancer growth [27]. SCRN1 is a novel marker for prognosis
in colorectal cancer [28]. These genes assigned to many blocks and have been implicated in other
cancers or leukemias can lead the discovery of novel AML driver genes.

(A)$

(B)$

(C)$

(A)$

(B)$

NPM1$

DNMT3A$
TET2$
FLT3$

Figure 4: Average number of gene sets highly
associated with blocks at a varying regulariza-
tion parameter. The cross-validation results
are consistent with these results.

Figure 5: Learned networks of: (A) GLasso and
(B) GRAB. For GRAB  we have sorted the genes
based on the blocks and highlighted the following
4 genes (out of the 12 highly associated genes with
AML) that belong to many blocks: NPM1  FLT3 
DNMT3A and TET2.

5 Discussion and Future Work

We present a novel general framework  called GRAB  that can explicitly model densely connected
network components that can overlap with each other in a graphical model. The novel GRAB
structural prior encourages the network estimate to be dense within each block (i.e.  a densely
connected group of variables) and sparse between the variables in different blocks. The GRAB
learning algorithm adopts BCD and is convex in each step. We demonstrate the effectiveness of our
framework in synthetic data and cancer gene expression dataset. Our framework is general and can
be applied to other kinds of graphical models  such as pairwise Markov random ﬁelds.
Acknowledgements: We give warm thanks to Reza Eghbali and Amin Jalali for many useful
discussions. This work was supported by the National Science Foundation grant DBI-1355899 and
the American Cancer Society Research Scholar Award 127332-RSG-15-097-01-TBG.

6http://www.genecards.org/cgi-bin/carddisp.pl?gene=TSPAN7

8

References
[1] S. L. Lauritzen. Graphical Models. Oxford Science Publications  1996.
[2] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.

Biostatistics  9:432–441  2007.

[3] B. M. Marlin and K. P. Murphy. Sparse gaussian graphical models with unknown block structure. pages

705–712  2009.

[4] K. M. Tan  D. Witten  and A. Shojaie. The cluster graphical lasso for improved estimation of gaussian

graphical models. Computational statistics & data analysis  85:23–36  2015.

[5] S. Celik  B. A. Logsdon  and S.-I. Lee. Efﬁcient dimensionality reduction for high-dimensional network

estimation. ICML  2014.

[6] A. Lasorella  R. Benezra  and A. Iavarone. The id proteins: master regulators of cancer stem cells and

tumour aggressiveness. Nature Reviews Cancer  14(2):77–91  2014.

[7] M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model. Biometrika 

94(10):19–35  2007.

[8] A. Rothman  E. Levina  and J. Zhu. Sparse permutation invariant covariance estimation. Electronic Journal

of Statistics  2:494–515  2008.

[9] P. W. Holland  K. B. Laskey  and S. Leinhardt. Stochastic blockmodels: Some ﬁrst steps. Social Networks 

5(2):109–137  1983.

[10] J. Shi and J. Malik. Normalized cuts and image segmentation. Pattern Analysis and Machine Intelligence 

IEEE Transactions on  22(8):888–905  2000.

[11] D. M. Witten  J. H. Friedman  and N. Simon. New insights and faster computations for the graphical lasso.

Journal of Computational and Graphical Statistics  20(4):892–900  2011.

[12] J. Duchi  S. Gould  and D. Koller. Projected subgradient methods for learning sparse gaussians. UAI  2008.
[13] M. Grechkin  M. Fazel  D. Witten  and S.-I. Lee. Pathway graphical lasso. 2015.
[14] C. Ambroise  J. Chiquet  and C. Matias. Inferring sparse gaussian graphical models with latent structure.

Electron. J. Statist.  3:205–238  2009.

[15] P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization. Journal

of Optimization Theory and Applications  109(3):475–494  2001.

[16] Cho-Jui Hsieh  Inderjit S Dhillon  Pradeep K Ravikumar  and Mátyás A Sustik. Sparse inverse covariance
matrix estimation using quadratic approximation. In Advances in neural information processing systems 
pages 2330–2338  2011.

[17] Q. Liu and A. T. Ihler. Learning scale free networks by reweighted l1 regularization. In International

Conference on Artiﬁcial Intelligence and Statistics  pages 40–8  2011.

[18] Mark EJ Newman. Modularity and community structure in networks. Proceedings of the National Academy

of Sciences  103(23):8577–8582  2006.

[19] K. Mohan  M. Chung  S. Han  D. Witten  S.-I. Lee  and M. Fazel. Structured learning of gaussian graphical

models. In NIPS  pages 620–628  2012.

[20] T. Haferlach  A. Kohlmann  L. Wieczorek  et al. Clinical utility of microarray-based gene expression
proﬁling in the diagnosis and subclassiﬁcation of leukemia. Journal of Clinical Oncology  28(15):2529–
2537  2010.

[21] Y. Shen  Y.-M. Zhu  X. Fan  et al. Gene mutation patterns and their prognostic impact in a cohort of 1185

patients with acute myeloid leukemia. Blood  118(20):5593–5603  2011.

[22] E. Segal  M. Shapira  A. Regev  D. Pe’er  D. Botstein  D. Koller  and N. Friedman. Module networks:
identifying regulatory modules and their condition-speciﬁc regulators from gene expression data. Nature
genetics  34(2):166–176  2003.

[23] S.-I. Lee and S. Batzoglou. Ica-based clustering of genes from microarray expression data. In Advances in

Neural Information Processing Systems  volume 16  2003.

[24] A. Liberzon  A. Subramanian  R. Pinchback  H. Thorvaldsdóttir  P. Tamayo  and J. P. Mesirov. Molecular

signatures database (msigdb) 3.0. Bioinformatics  27(12):1739–1740  2011.

[25] Y. Fang  L. N. Xie  X. M. Liu  et al. Dysregulated module approach identiﬁes disrupted genes and pathways

associated with acute myelocytic leukemia. Eur Rev Med Pharmacol Sci  19(24):4811–4826  2015.

[26] C. Rubie  V. O. Frick  P. Ghadjar  et al. Research ccl20/ccr6 expression proﬁle in pancreatic cancer. 2010.
[27] K. Tamura  A. Makino  et al. Novel lipogenic enzyme elovl7 is involved in prostate cancer growth through

saturated long-chain fatty acid metabolism. Cancer Research  69(20):8133–8140  2009.

[28] N. Miyoshi  H. Ishii  K. Mimori  et al. Scrn1 is a novel marker for prognosis in colorectal cancer. Journal

of surgical oncology  101(2):156–159  2010.

9

,Mohammad Javad Hosseini
Su-In Lee