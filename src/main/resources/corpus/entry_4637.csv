2015,Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial Monitoring,Partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players. In this game  the learner chooses an action and at the same time the opponent chooses an outcome  then the learner suffers a loss and receives a feedback signal. The goal of the learner is to minimize the total loss. In this paper  we study partial monitoring with finite actions and stochastic outcomes. We derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem. Inspired by the DMED algorithm (Honda and Takemura  2010) for the multi-armed bandit problem  we propose PM-DMED  an algorithm that minimizes the distribution-dependent regret. PM-DMED significantly outperforms state-of-the-art algorithms in numerical experiments. To show the optimality of PM-DMED with respect to the regret bound  we slightly modify the algorithm by introducing a hinge function (PM-DMED-Hinge). Then  we derive an asymptotical optimal regret upper bound of PM-DMED-Hinge that matches the lower bound.,Regret Lower Bound and Optimal Algorithm in

Finite Stochastic Partial Monitoring

Junpei Komiyama

The University of Tokyo

Junya Honda

The University of Tokyo

junpei@komiyama.info

honda@stat.t.u-tokyo.ac.jp

Hiroshi Nakagawa

The University of Tokyo

nakagawa@dl.itc.u-tokyo.ac.jp

Abstract

Partial monitoring is a general model for sequential learning with limited feed-
back formalized as a game between two players. In this game  the learner chooses
an action and at the same time the opponent chooses an outcome  then the learner
suffers a loss and receives a feedback signal. The goal of the learner is to mini-
mize the total loss. In this paper  we study partial monitoring with ﬁnite actions
and stochastic outcomes. We derive a logarithmic distribution-dependent regret
lower bound that deﬁnes the hardness of the problem. Inspired by the DMED
algorithm (Honda and Takemura  2010) for the multi-armed bandit problem  we
propose PM-DMED  an algorithm that minimizes the distribution-dependent re-
gret. PM-DMED signiﬁcantly outperforms state-of-the-art algorithms in numeri-
cal experiments. To show the optimality of PM-DMED with respect to the regret
bound  we slightly modify the algorithm by introducing a hinge function (PM-
DMED-Hinge). Then  we derive an asymptotically optimal regret upper bound of
PM-DMED-Hinge that matches the lower bound.

1 Introduction

Partial monitoring is a general framework for sequential decision making problems with imperfect
feedback. Many classes of problems  including prediction with expert advice [1]  the multi-armed
bandit problem [2]  dynamic pricing [3]  the dark pool problem [4]  label efﬁcient prediction [5] 
and linear and convex optimization with full or bandit feedback [6  7] can be modeled as an instance
of partial monitoring.
Partial monitoring is formalized as a repeated game played by two players called a learner and an
opponent. At each round  the learner chooses an action  and at the same time the opponent chooses
an outcome. Then  the learner observes a feedback signal from a given set of symbols and suffers
some loss  both of which are deterministic functions of the selected action and outcome.
The goal of the learner is to ﬁnd the optimal action that minimizes his/her cumulative loss. Alter-
natively  we can deﬁne the regret as the difference between the cumulative losses of the learner and
the single optimal action  and minimization of the loss is equivalent to minimization of the regret.
A learner with a small regret balances exploration (acquisition of information about the strategy of
the opponent) and exploitation (utilization of information). The rate of regret indicates how fast the
learner adapts to the problem: a linear regret indicates the inability of the learner to ﬁnd the optimal
action  whereas a sublinear regret indicates that the learner can approach the optimal action given
sufﬁciently large time steps.

1

The study of partial monitoring is classiﬁed into two settings with respect to the assumption on the
outcomes. On one hand  in the stochastic setting  the opponent chooses an outcome distribution
before the game starts  and an outcome at each round is an i.i.d. sample from the distribution. On
the other hand  in the adversarial setting  the opponent chooses the outcomes to maximize the regret
of the learner. In this paper  we study the former setting.

1.1 Related work

The paper by Piccolboni and Schindelhauer [8] is one of the ﬁrst to study the regret of the ﬁnite par-
tial monitoring problem. They proposed the FeedExp3 algorithm  which attains O(T 3=4) minimax
regret on some problems. This bound was later improved by Cesa-Bianchi et al. [9] to O(T 2=3) 
who also showed an instance in which the bound is optimal. Since then  most literature on partial
monitoring has dealt with the minimax regret  which is the worst-case regret over all possible op-
p
ponent’s strategies. Bart´ok et al. [10] classiﬁed the partial monitoring problems into four categories
in terms of the minimax regret: a trivial problem with zero regret  an easy problem with ~(cid:2)(
T )
regret1  a hard problem with (cid:2)(T 2=3) regret  and a hopeless problem with (cid:2)(T ) regret. This shows
p
that the class of the partial monitoring problems is not limited to the bandit sort but also includes
larger classes of problems  such as dynamic pricing. Since then  several algorithms with a ~O(
T )
regret bound for easy problems have been proposed [11  12  13]. Among them  the Bayes-update
Partial Monitoring (BPM) algorithm [13] is state-of-the-art in the sense of empirical performance.
Distribution-dependent and minimax regret: we focus on the distribution-dependent regret that
depends on the strategy of the opponent. While the minimax regret in partial monitoring has been ex-
tensively studied  little has been known on distribution-dependent regret in partial monitoring. To the
authors’ knowledge  the only paper focusing on the distribution-dependent regret in ﬁnite discrete
partial monitoring is the one by Bart´ok et al. [11]  which derived O(log T ) distribution-dependent re-
gret for easy problems. In contrast to this situation  much more interest in the distribution-dependent
regret has been shown in the ﬁeld of multi-armed bandit problems. Upper conﬁdence bound (UCB) 
the most well-known algorithm for the multi-armed bandits  has a distribution-dependent regret
bound [2  14]  and algorithms that minimize the distribution-dependent regret (e.g.  KL-UCB) has
been shown to perform better than ones that minimize the minimax regret (e.g.  MOSS)  even in
instances in which the distributions are hard to distinguish (e.g.  Scenario 2 in Garivier et al. [15]).
Therefore  in the ﬁeld of partial monitoring  we can expect that an algorithm that minimizes the
distribution-dependent regret would perform better than the existing ones.
Contribution: the contributions of this paper lie in the following three aspects. First  we derive
the regret lower bound: in some special classes of partial monitoring (e.g.  multi-armed bandits)  an
O(log T ) regret lower bound is known to be achievable. In this paper  we further extend this lower
bound to obtain a regret lower bound for general partial monitoring problems. Second  we propose
an algorithm called Partial Monitoring DMED (PM-DMED). We also introduce a slightly modiﬁed
version of this algorithm (PM-DMED-Hinge) and derive its regret bound. PM-DMED-Hinge is the
ﬁrst algorithm with a logarithmic regret bound for hard problems. Moreover  for both easy and hard
problems  it is the ﬁrst algorithm with the optimal constant factor on the leading logarithmic term.
Third  performances of PM-DMED and existing algorithms are compared in numerical experiments.
Here  the partial monitoring problems consisted of three speciﬁc instances of varying difﬁculty. In
all instances  PM-DMED signiﬁcantly outperformed the existing methods when a number of rounds
is large. The regret of PM-DMED on these problems quickly approached the theoretical lower
bound.

2 Problem Setup

This paper studies the ﬁnite stochastic partial monitoring problem with N actions  M outcomes 
and A symbols. An instance of the partial monitoring game is deﬁned by a loss matrix L = (li;j) 2
RN(cid:2)M and a feedback matrix H = (hi;j) 2 [A]N(cid:2)M   where [A] = f1; 2; : : : ; Ag. At the be-
ginning  the learner is informed of L and H. At each round t = 1; 2; : : : ; T   a learner selects an
action i(t) 2 [N ]  and at the same time an opponent selects an outcome j(t) 2 [M ]. The learner

1Note that ~(cid:2) ignores a polylog factor.

2

suffers loss li(t);j(t)  which he/she cannot observe: the only information the learner receives is the
signal hi(t);j(t) 2 [A]. We consider a stochastic opponent whose strategy for selecting outcomes is
(cid:3) 2 PM   where PM is a set of probability distributions over
governed by the opponent’s strategy p
an M-ary outcome. The outcome j(t) of each round is an i.i.d. sample from p
The goal of the learner is to minimize the cumulative loss over T
rounds. Let the optimal action be the one that minimizes the loss in
⊤
(cid:3)  where Li is the i-th
expectation  that is  i
= arg mini2[N ] L
i p
(cid:3) is unique. Without loss of generality  we
row of L. Assume that i
= 1. Let ∆i = (Li (cid:0) L1)
(cid:3) 2 [0;1) and Ni(t)
⊤
(cid:3)
can assume that i
be the number of rounds before the t-th in which action i is selected.
∑
The performance of the algorithm is measured by the (pseudo) regret 

T∑

(cid:3).

p

(cid:3)

Regret(T ) =

∆i(t) =

t=1

∆iNi(T + 1);

i2[N ]

Figure 1: Cell decomposi-
tion of a partial monitoring
instance with M = 3.

which is the difference between the expected loss of the learner and
the optimal action. It is easy to see that minimizing the loss is equiv-
alent to minimizing the regret. The expectation of the regret measures the performance of an algo-
rithm that the learner uses.
For each action i 2 [N ]  let Ci be the set of opponent strategies for which action i is optimal:

Ci = fq 2 PM : 8j̸=i(Li (cid:0) Lj)
⊤

q (cid:20) 0g:

i = PM n Ci be the set of strategies with which action i is not optimal.

We call Ci the optimality cell of action i. Each optimality cell is a convex closed polytope. Further-
more  we call the set of optimality cells fC1; : : : ;CNg the cell decomposition as shown in Figure 1.
Let Cc
The signal matrix Si 2 f0; 1gA(cid:2)M of action i is deﬁned as (Si)k;j = 11 [hi;j = k]  where 11 [X] = 1
if X is true and 0 otherwise. The signal matrix deﬁned here is slightly different from the one
in the previous papers (e.g.  Bart´ok et al. [10]) in which the number of rows of Si is the number
of the different symbols in the i-th row of H. The advantage in using the deﬁnition here is that 
(cid:3) 2 RA is a probability distribution over symbols that the algorithm observes when it selects
Sip
an action i. Examples of signal matrices are shown in Section 5. An instance of partial monitoring
is globally observable if for all pairs i; j of actions  Li (cid:0) Lj 2 (cid:8)k2[N ]ImS
⊤
k . In this paper  we
exclusively deal with globally observable instances: in view of the minimax regret  this includes
trivial  easy  and hard problems.

3 Regret Lower Bound

A good algorithm should work well against any opponent’s strategy. We extend this idea by intro-
ducing the notion of strong consistency: a partial monitoring algorithm is strongly consistent if it
satisﬁes E[Regret(T )] = o(T a) for any a > 0 and p 2 PM given L and H.
In the context of the multi-armed bandit problem  Lai and Robbins [2] derived the regret lower
bound of a strongly consistent algorithm: an algorithm must select each arm i until its number of
draws Ni(t) satisﬁes log t ≲ Ni(t)d((cid:18)i∥(cid:18)1)  where d((cid:18)i∥(cid:18)1) is the KL divergence between the two
one-parameter distributions from which the rewards of action i and the optimal action are generated.
Analogously  in the partial monitoring problem  we can deﬁne the minimum number of observations.
Lemma 1. For sufﬁciently large T   a strongly consistent algorithm satisﬁes:
∥Siq) (cid:21) log T (cid:0) o(log T );

(cid:3)
E[Ni(T )]D(p
i

∑

8q2Cc

1

i2[N ]
(cid:3) and D(p∥q) =

(cid:3)
i = Sip

∑

i(p)i log ((p)i=(q)i) is the KL divergence between two discrete

where p
distributions  in which we deﬁne 0 log 0=0 = 0.
Lemma 1 can be interpreted as follows: for each round t  consistency requires the algorithm to
make sure that the possible risk that action i ̸= 1 is optimal is smaller than 1=t. Large devia-
(cid:3) is
tion principle [16] states that  the probability that an opponent with strategy q behaves like p

3

p*C1C3C4C2C5||p*-C1c||Mroughly exp ((cid:0)∑
∑

(cid:3)
i

i Ni(t)D(p

∥Siq)). Therefore  we need to continue exploration of the actions

i Ni(t)D(p
∥Siq) (cid:24) log t holds for any q 2 Cc
(cid:3)
i

1 to reduce the risk to exp ((cid:0) log t) = 1=t.

until
The proof of Lemma 1 is in Appendix B in the supplementary material. Based on the technique
used in Lai and Robbins [2]  the proof considers a modiﬁed game in which another action i ̸= 1 is
optimal. The difﬁculty in proving the lower bound in partial monitoring lies in that  the feedback
structure can be quite complex: for example  to conﬁrm the superiority of action 1 over 2  one might
need to use the feedback from action 3 =2 f1; 2g. Still  we can derive the lower bound by utilizing
the consistency of the algorithm in the original and modiﬁed games.
We next derive a lower bound on the regret based on Lemma 1. Note that  the expectation of the
∑
E[Ni(t)](Li (cid:0) L1)
⊤
regret can be expressed as E[Regret(T )] =

∑
{
i̸=1
frigi̸=j 2 [0;1)N(cid:0)1 :

riD(pi∥Siq) (cid:21) 1

Rj(fpig) =

(cid:3). Let

}

p

q2cl(Cc

inf
j ):pj =Sj q

i

where cl((cid:1)) denotes a closure. Moreover  let

C

ri2Rj (fpig)

inf

j (p;fpig) =
(cid:3)
{
frigi̸=j 2 Rj(fpig) :

the optimal solution of which is

R(cid:3)
j (p;fpig) =

∑
∑

i̸=j

i̸=j

ri(Li (cid:0) Lj)

⊤

p ;

}
j (p;fpig)
(cid:3)

ri(Li (cid:0) Lj)

⊤

p = C

;

:

(cid:3)
(cid:3)
1 (p

g) log T is the possible minimum regret for observations such that the mini-
;fp
(cid:3)
The value C
(cid:3) from any q 2 Cc
i
1 is larger than log T . Using Lemma 1 yields the following
mum divergence of p
regret lower bound:
Theorem 2. The regret of a strongly consistent algorithm is lower bounded as:

E[Regret(T )] (cid:21) C

(cid:3)
(cid:3)
1 (p

;fp
(cid:3)
i

g) log T (cid:0) o(log T ):

1

(cid:3)
i

(cid:3)
(cid:3)
1 (p

∑

(cid:3) (cid:0) Cc

∥2
M ): the regret bound has at most quadratic dependence on ∥p
(cid:3) to the boundary of the optimal cell.

g)  whereas
;fp
(cid:3)
From this theorem  we can naturally measure the harshness of the instance by C
i
the past studies (e.g.  Vanchinathan et al. [13]) ambiguously deﬁne the harshness as the closeness to
g) =
;fp
(cid:3)
(cid:3)
the boundary of the cells. Furthermore  we show in Lemma 5 in the Appendix that C
1 (p
∥M   which is
(cid:3) (cid:0) Cc
O(N=∥p
1
deﬁned in Appendix D as the closeness of p
4 PM-DMED Algorithm
In this section  we describe the partial monitoring deterministic minimum empirical divergence (PM-
DMED) algorithm  which is inspired by DMED [17] for solving the multi-armed bandit problem.
Let ^pi(t) 2 [0; 1]A be the empirical distribution of the symbols under the selection of action i.
∑
) = i]). We
Namely  the k-th element of ^pi(t) is (
sometimes omit t from ^pi when it is clear from the context. Let the empirical divergence of q 2 PM
i2[N ] Ni(t)D(^pi(t)∥Siq)  the exponential of which can be considered as a likelihood that q is
be
the opponent’s strategy.
The main routine of PM-DMED is in Algorithm 1. At each loop  the actions in the current list ZC
are selected once. The list for the actions in the next loop ZN is determined by the subroutine in
Algorithm 2. The subroutine checks whether the empirical divergence of each point q 2 Cc
1 is larger
than log t or not (Eq. (3)). If it is large enough  it exploits the current information by selecting ^i(t) 
the optimal action based on the estimation ^p(t) that minimizes the empirical divergence. Otherwise 
it selects the actions with the number of observations below the minimum requirement for making
the empirical divergence of each suboptimal point q 2 Cc
Unlike the N-armed bandit problem in which a reward is associated with an action  in the partial
monitoring problem  actions  outcomes  and feedback signals can be intricately related. Therefore 
we need to solve a non-trivial optimization to run PM-DMED. Later in Section 5  we discuss a
practical implementation of the optimization.

) = i\ hi(t′);j(t′) = k])=(

1 larger than log t.

t(cid:0)1
t′=1 11 [i(t

t(cid:0)1
t′=1 11 [i(t

∑

′

′

4

Algorithm 1 Main routine of PM-DMED and
PM-DMED-Hinge
1: Initialization: select each action once.
2: ZC; ZR [N ]; ZN ∅.
3: while t (cid:20) T do
4:

for i(t) 2 ZC in an arbitrarily ﬁxed order
do
{
Select i(t)  and receive feedback.
ZR ZR n fi(t)g.
Add actions to ZN in accordance with
Algorithm 2 (PM-DMED)
.
Algorithm 3 (PM-DMED-Hinge)
t t + 1.
end for
ZC; ZR ZN   ZN ∅.

8:
9:
10:
11: end while

5:
6:
7:

Algorithm 2 PM-DMED subroutine for adding
actions to ZN (without duplication).
1: Parameter: c > 0.
2: Compute an arbitrary ^p(t) such that

∑

q

i

^p(t) 2 arg min

⊤
and let ^i(t) = arg mini L
i ^p(t).

3: If ^i(t) =2 ZR then put ^i(t) into ZN .
4: If there are actions i =2 ZR such that

Ni(t)D(^pi(t)∥Siq) (1)
√

5: If

(2)

log t

Ni(t) < c
then put them into ZN .
i̸=^i(t) =2 R^i(t)(f^pi(t)g)
fNi(t)= log tg
then compute some
2 R(cid:3)
^i(t)(^p(t);f^pi(t)g)

(4)
and put all actions i such that i =2 ZR and
(cid:3)
i > Ni(t)= log t into ZN .

g
i̸=^i(t)

fr

(3)

(cid:3)
i

r

p
log T exploration: PM-DMED tries to observe each action to some extent (Eq. (2)) 

Necessity of
which is necessary for the following reason: consider a four-state game characterized by

0B@ 0

L =

1
1
0

0
0
10
10
0
11 11 11 11

1
0
1

1CA   H =

0B@ 1

1
1
1

1
2
2
1

1
2
2
2

1
3
3
2

1CA   and

(cid:3)

p

= (0:1; 0:2; 0:3; 0:4)

⊤

:

(cid:3)

(cid:3)
)1  (p

)j is the j-th component of p

(cid:3)
)2 + (p
(cid:3). From this  an algorithm can ﬁnd that (p
(cid:3)

The optimal action here is action 1  which does not yield any useful information. By using action 2 
one receives three kinds of symbols from which one can estimate (p
)4 
)3  and (p
(cid:3)
where (p
)1 is not very
small and thus the expected loss of actions 2 and 3 is larger than that of action 1. Since the feedback
of actions 2 and 3 are the same  one may also use action 3 in the same manner. However  the loss per
observation is 1:2 and 1:3 for actions 2 and 3  respectively  and thus it is better to use action 2. This
(cid:3)
difference comes from the fact that (p
)3. Since an algorithm does not know
(cid:3) beforehand  it needs to observe action 4  the only source for distinguishing (p
(cid:3)
)3.
p
Yet  an optimal algorithm cannot select it more than Ω(log T ) times because it affects the O(log T )
factor in the regret. In fact  O((log T )a) observations of action 4 with some a > 0 are sufﬁcient to
)3 with probability 1 (cid:0) o(1=T poly(a)). For this reason  PM-DMED
(cid:3)
(cid:3)
p
be convinced that (p
)2 < (p
selects each action
log t times.

)2 = 0:2 < 0:3 = (p

)2 from (p

(cid:3)

(cid:3)

(cid:3)

5 Experiment

Following Bart´ok et al. [11]  we compared the performances of algorithms in three different games:
the four-state game (Section 4)  a three-state game and dynamic pricing. Experiments on the N-
armed bandit game was also done  and the result is shown in Appendix C.1 .
The three-state game  which is classiﬁed as easy in terms of the minimax regret  is characterized by:

(

)

(

L =

(

)
(

1 1 0
0 1 1
1 0 1

)

and H =

)

1 2
2 1
2 2

2
2
1

(

:

)

The signal matrices of this game are 

S1 =

1 0 0
0 1 1

; S2 =

0
1

1
0

0
1

; and S3 =

0 0
1 1

1
0

:

5

(a) three-states  benign

(b) three-states  intermediate

(c) three-states  harsh

(d) dynamic pricing  benign

(e) dynamic pricing  intermediate

(f) dynamic pricing  harsh

Figure 2: Regret-round semilog plots of algorithms. The regrets are averaged over 100 runs. LB is
the asymptotic regret lower bound of Theorem 2.

(g) four-states

Dynamic pricing  which is classiﬁed as hard in terms of the minimax regret  is a game that models
a repeated auction between a seller (learner) and a buyer (opponent). At each round  the seller sets
a price for a product  and at the same time  the buyer secretly sets a maximum price he is willing to
pay. The signal is “buy” or “no-buy”  and the seller’s loss is either a given constant (no-buy) or the
difference between the buyer’s and the seller’s prices (buy). The loss and feedback matrices are:

1CCA ;

2
2
...
: : :

: : :
: : :
...
1

2
2

...

2

(cid:3)
(cid:3)
1 (p

;fp
(cid:3)
i

Following Bart´ok et al. [11]  we set N = 5; M = 5  and c = 2.
In our experiments with the three-state game and dynamic pricing  we tested three settings regarding
the harshness of the opponent: at the beginning of a simulation  we sampled 1 000 points uniformly
at random from PM   then sorted them by C
g). We chose the top 10%  50%  and 90%
harshest ones as the opponent’s strategy in the harsh  intermediate  and benign settings  respectively.
We compared Random  FeedExp3 [8]  CBP [11] with (cid:11) = 1:01  BPM-LEAST  BPM-TS [13]  and
PM-DMED with c = 1. Random is a naive algorithm that selects an action uniformly random.
⊤  and thus one cannot apply it to the four-state
FeedExp3 requires a matrix G such that H
game. CBP is an algorithm of logarithmic regret for easy games. The parameters (cid:17) and f (t) of
p
CBP were set in accordance with Theorem 1 in their paper. BPM-LEAST is a Bayesian algorithm
with ~O(
T ) regret for easy games  and BPM-TS is a heuristic of state-of-the-art performance. The
priors of two BPMs were set to be uninformative to avoid a misspeciﬁcation  as recommended in
their paper.

G = L

⊤

6

0BB@ 0

c

...

c

L =

1CCA and H =
}|
z
{
}|

i(cid:0)1

M(cid:0)i+1

1

0BB@ 2
...
{
)

1

: : : N (cid:0) 1
: : : N (cid:0) 2
...
c

0

1
0
...
: : :

...
z

(

Si =

1
0

: : :
: : :

1
0

0
1

: : :
: : :

0
1

:

where signals 1 and 2 correspond to no-buy and buy. The signal matrix of action i is

100101102103104105106t:round020406080100120R(t):regretRandomFeedExp3CBPBPM-LEASTBPM-TSPM-DMEDLB100101102103104105106t:round0100200300400500600R(t):regretRandomFeedExp3CBPBPM-LEASTBPM-TSPM-DMEDLB103104105106t:round050010001500200025003000R(t):regretRandomFeedExp3CBPBPM-LEASTBPM-TSPM-DMEDLB100101102103104105106t:round0200400600800100012001400R(t):regretRandomFeedExp3CBPBPM-LEASTBPM-TSPM-DMEDLB100101102103104105106t:round0100020003000400050006000R(t):regretRandomFeedExp3CBPBPM-LEASTBPM-TSPM-DMEDLB103104105106t:round020000400006000080000100000120000R(t):regretRandomFeedExp3CBPBPM-LEASTBPM-TSPM-DMEDLB100101102103104105106t:round0500100015002000R(t):regretRandomCBPBPM-LEASTBPM-TSPM-DMEDLBAlgorithm 3 PM-DMED-Hinge subroutine for adding actions to ZN (without duplication).
1: Parameters: c > 0  f (n) = bn
2: Compute arbitrary ^p(t) which satisﬁes

(cid:0)1=2 for b > 0  (cid:11)(t) = a=(log log t) for a > 0.

∑

^p(t) 2 arg min

Ni(t)(D(^pi(t)∥Siq) (cid:0) f (Ni(t)))+

q

⊤
and let ^i(t) = arg mini L
i ^p(t).

3: If ^i(t) =2 ZR then put ^i(t) into ZN .
4: If

i

or there exists an action i such that

then put all actions i =2 ZR into ZN .

5: If there are actions i such that

D(^pi(t)∥Si ^p(t)) > f (Ni(t))

^p(t) =2 C^i(t);(cid:11)(t)
√

(5)

(6)

(7)

(8)

(9)

(10)

6: If

log t

then compute some

then put the actions not in ZR into ZN .
fNi(t)= log tg
fr

Ni(t) < c
i̸=^i(t) =2 R^i(t)(f^pi(t); f (Ni(t))g)
2 R(cid:3)
^i(t)(^p(t);f^pi(t); f (Ni(t))g)
and put all actions such that i =2 ZR and r
(cid:3)
i > Ni(t)= log t into ZN . If such r
put all action i =2 ZR into ZN .

g
i̸=^i(t)

(cid:3)
i

(cid:3)
i is infeasible then

(cid:3)
i

(cid:3)
i

(cid:3)

(cid:3)
1 (p

;fp
(cid:3)
i

The computation of ^p(t) in (1) and the evaluation of the condition in (3) involve convex optimiza-
g in (4) is classiﬁed as a linear
tions  which were done with Ipopt [18]. Moreover  obtaining fr
semi-inﬁnite programming (LSIP) problem  a linear programming (LP) with ﬁnitely many variables
and inﬁnitely many constraints. Following the optimization of BPM-LEAST [13]  we resorted to a
ﬁnite sample approximation and used the Gurobi LP solver [19] in computing fr
g: at each round 
we sampled 1 000 points from PM   and relaxed the constraints on the samples. To speed up the
computation  we skipped these optimizations in most rounds with large t and used the result of
g) of the regret lower bound
the last computation. The computation of the coefﬁcient C
(Theorem 2) is also an LSIP  which was approximated by 100 000 sample points from Cc
1.
The experimental results are shown in Figure 2. In the four-state game and the other two games with
an easy or intermediate opponent  PM-DMED outperforms the other algorithms when the number of
rounds is large. In particular  in the dynamic pricing game with an intermediate opponent  the regret
of PM-DMED at T = 106 is ten times smaller than those of the other algorithms. Even in the harsh
setting in which the minimax regret matters  PM-DMED has some advantage over all algorithms
except for BPM-TS. With sufﬁciently large T   the slope of an optimal algorithm should converge to
LB. In all games and settings  the slope of PM-DMED converges to LB  which is empirical evidence
of the optimality of PM-DMED.
6 Theoretical Analysis
Section 5 shows that the empirical performance of PM-DMED is very close to the regret lower
bound in Theorem 2. Although the authors conjecture that PM-DMED is optimal  it is hard to
analyze PM-DMED. The technically hardest part arises from the case in which the divergence of
{
each action is small but not yet fully converged. To circumvent this difﬁculty  we can introduce a
discount factor. Let
frigi̸=j 2 [0;1)N(cid:0)1 :
Rj(fpi; (cid:14)ig)=
; (11)
where (X)+ = max(X; 0). Note that Rj(fpi; (cid:14)ig) in (11) is a natural generalization of Rj(fpig)
in Section 4 in the sense that Rj(fpi; 0g) = Rj(fpig). Event fNi(t)= log tgi̸=1 2 R1(f^pi(t); (cid:14)ig)
means that the number of observations fNi(t)g is enough to ensure that the “f(cid:14)ig-discounted” em-
pirical divergence of each q 2 Cc

}
ri(D(pi∥Siq)(cid:0) (cid:14)i)+ (cid:21) 1

1 is larger than log t. Analogous to Rj(fpi; (cid:14)ig)  we deﬁne

j ):D(pj∥Sj q)(cid:20)(cid:14)j

∑

q2cl(Cc

inf

i

7

and its optimal solution by
R(cid:3)
j (p;fpi; (cid:14)ig) =

frigi̸=j2Rj (fpi;(cid:14)ig))

C

inf

j (p;fpi; (cid:14)ig) =
(cid:3)
{
frigi̸=j 2 Rj(fpi; (cid:14)ig) :

∑

i̸=j

∑

i̸=j

ri(Lj (cid:0) Li)

⊤

p

ri(Lj (cid:0) Li)
⊤

p = C

}
j (p;fpi; (cid:14)ig)
(cid:3)

:

We also deﬁne Ci;(cid:11) = fp 2 PM : L
i p + (cid:11) (cid:20) minj̸=i L
j pg  the optimal region of action i
⊤
⊤
with margin. PM-DMED-Hinge shares the main routine of Algorithm 1 with PM-DMED and lists
the next actions by Algorithm 3. Unlike PM-DMED  it (i) discounts f (Ni(t)) from the empirical
divergence D(^pi(t)∥Siq). Moreover  (ii) when ^p(t) is close to the cell boundary  it encourages more
exploration to identify the cell it belongs to by Eq. (6).
1(p;fpi; (cid:14)ig) is
(cid:3).
Theorem 3. Assume that the following regularity conditions hold for p
∥S1q) (cid:20) (cid:14)g  it holds that
; (cid:14)i = 0. Moreover  (2) for S(cid:14) = fq : D(p
(cid:3)
(cid:3)
(cid:3)
unique at p = p
; pi = Sip
1) \ S(cid:14)) for all (cid:14) (cid:21) 0 in some neighborhood of (cid:14) = 0  where cl((cid:1)) and
cl(int(Cc
1) \ S(cid:14)) = cl(cl(Cc
1
int((cid:1)) denote the closure and the interior  respectively. Then 
E[Regret(T )] (cid:20) C

g) log T + o(log T ) :

(1) R(cid:3)

(cid:3)
(cid:3)
1 (p

;fp
(cid:3)
i

N

(cid:3)

(cid:3)
1 (p

;fp
(cid:3)
i

g) =

1(p;f^pi(t); (cid:14)ig) is the set of optimal solutions
We prove this theorem in Appendix D . Recall that R(cid:3)
of an LSIP. In this problem  KKT conditions and the duality theorem apply as in the case of ﬁnite
(cid:3) (see  e.g.  Ito et al. [20]
constraints; thus  we can check whether Condition 1 holds or not for each p
and references therein). Condition 2 holds in most cases  and an example of an exceptional case is
shown in Appendix A.
Theorem 3 states that PM-DMED-Hinge has a regret upper bound that matches the lower bound of
Theorem 2.
Corollary 4. (Optimality in the N-armed bandit problem) In the N-armed Bernoulli bandit prob-
∑
lem  the regularity conditions in Theorem 3 always hold. Moreover  the coefﬁcient of the lead-
ing logarithmic term in the regret bound of the partial monitoring problem is equal to the bound
i̸=1(∆i=d((cid:22)i∥(cid:22)1))  where d(p∥q) =
given in Lai and Robbins [2]. Namely  C
p log (p=q) + (1 (cid:0) p) log ((1 (cid:0) p)=(1 (cid:0) q)) is the KL-divergence between Bernoulli distributions.
Corollary 4  which is proven in Appendix C  states that PM-DMED-Hinge attains the optimal regret
of the N-armed bandit if we run it on an N-armed bandit game represented as partial monitoring.
Asymptotic analysis: it is Theorem 6 where we lose the ﬁnite-time property. This theorem shows
j (p;fpjg)  which does not mention
the continuity of the optimal solution set R(cid:3)
(cid:3)
∥M ; maxi (cid:14)ig (cid:20) (cid:14)
(cid:3)∥M ; maxi ∥pi(cid:0)p
how close R(cid:3)
(cid:3)
i
for given (cid:14). To obtain an explicit bound  we need sensitivity analysis  the theory of the robustness
of the optimal value and the solution for small deviations of its parameters (see e.g.  Fiacco [21]).
In particular  the optimal solution of partial monitoring involves an inﬁnite number of constraints 
which makes the analysis quite hard. For this reason  we will not perform a ﬁnite-time analysis.
Note that  the N-armed bandit problem is a special instance in which we can avoid solving the
above optimization and a ﬁnite-time optimal bound is known.
Necessity of the discount factor: we are not sure whether discount factor f (n) in PM-DMED-
Hinge is necessary or not. We also empirically tested PM-DMED-Hinge: although it is better than
the other algorithms in many settings  such as dynamic pricing with an intermediate opponent  it
is far worse than PM-DMED. We found that our implementation  which uses the Ipopt nonlinear
optimization solver  was sometimes inaccurate at optimizing (5): there were some cases in which
) (cid:0) f (Ni(t)) = 0  while the solution ^p(t) we obtained
the true p
had non-zero hinge values. In this case  the algorithm lists all actions from (7)  which degrades
performance. Determining whether the discount factor is essential or not is our future work.
Acknowledgements
The authors gratefully acknowledge the advice of Kentaro Minami and sincerely thank the anony-
mous reviewers for their useful comments. This work was supported in part by JSPS KAKENHI
Grant Number 15J09850 and 26106506.

1(p;fpi; (cid:14)ig) of C
i ; 0g) if maxf∥p(cid:0)p
;fp
(cid:3)

(cid:3) satisﬁes 8i2[N ]D(^pi(t)∥Sip
(cid:3)

1(p;fpi; (cid:14)ig) is to R(cid:3)
(cid:3)
1(p

8

References
[1] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Inf. Comput. 

108(2):212–261  February 1994.

[2] T. L. Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics  6(1):4–22  1985.

[3] Robert D. Kleinberg and Frank Thomson Leighton. The value of knowing a demand curve:

Bounds on regret for online posted-price auctions. In FOCS  pages 594–605  2003.

[4] Alekh Agarwal  Peter L. Bartlett  and Max Dama. Optimal allocation strategies for the dark

pool problem. In AISTATS  pages 9–16  2010.

[5] Nicol`o Cesa-Bianchi  G´abor Lugosi  and Gilles Stoltz. Minimizing regret with label efﬁcient

prediction. IEEE Transactions on Information Theory  51(6):2152–2162  2005.

[6] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.

In ICML  pages 928–936  2003.

[7] Varsha Dani  Thomas P. Hayes  and Sham M. Kakade. Stochastic linear optimization under

bandit feedback. In COLT  pages 355–366  2008.

[8] Antonio Piccolboni and Christian Schindelhauer. Discrete prediction games with arbitrary

feedback and loss. In COLT  pages 208–223  2001.

[9] Nicol`o Cesa-Bianchi  G´abor Lugosi  and Gilles Stoltz. Regret minimization under partial

monitoring. Math. Oper. Res.  31(3):562–580  2006.

[10] G´abor Bart´ok  D´avid P´al  and Csaba Szepesv´ari. Minimax regret of ﬁnite partial-monitoring

games in stochastic environments. In COLT  pages 133–154  2011.

[11] G´abor Bart´ok  Navid Zolghadr  and Csaba Szepesv´ari. An adaptive algorithm for ﬁnite stochas-

tic partial monitoring. In ICML  2012.

[12] G´abor Bart´ok. A near-optimal algorithm for ﬁnite partial-monitoring games against adversarial

opponents. In COLT  pages 696–710  2013.

[13] Hastagiri P. Vanchinathan  G´abor Bart´ok  and Andreas Krause. Efﬁcient partial monitoring

with prior information. In NIPS  pages 1691–1699  2014.

[14] Peter Auer  Nicol´o Cesa-bianchi  and Paul Fischer. Finite-time Analysis of the Multiarmed

Bandit Problem. Machine Learning  47:235–256  2002.

[15] Aur´elien Garivier and Olivier Capp´e. The KL-UCB algorithm for bounded stochastic bandits

and beyond. In COLT  pages 359–376  2011.

[16] Amir Dembo and Ofer Zeitouni. Large deviations techniques and applications. Applications

of mathematics. Springer  New York  Berlin  Heidelberg  1998.

[17] Junya Honda and Akimichi Takemura. An Asymptotically Optimal Bandit Algorithm for

Bounded Support Models. In COLT  pages 67–79  2010.

[18] Andreas W¨achter and Carl D. Laird. Interior point optimizer (IPOPT).
[19] Gurobi Optimization Inc. Gurobi optimizer.
[20] S. Ito  Y. Liu  and K. L. Teo. A dual parametrization method for convex semi-inﬁnite program-

ming. Annals of Operations Research  98(1-4):189–213  2000.

[21] Anthony V. Fiacco. Introduction to sensitivity and stability analysis in nonlinear programming.

Academic Press  New York  1983.

9

,Vincent Michalski
Roland Memisevic
Kishore Konda
Junpei Komiyama
Junya Honda
Hiroshi Nakagawa