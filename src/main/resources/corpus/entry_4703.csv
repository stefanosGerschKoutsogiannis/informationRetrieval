2013,Reservoir Boosting : Between Online and Offline Ensemble Learning,We propose to train an ensemble with the help of a reservoir in which the learning algorithm can store a limited number of samples. This novel approach lies in the area between offline and online ensemble approaches and can be seen either as a restriction of the former or an enhancement of the latter.  We identify some basic strategies that can be used to populate this reservoir and present our main contribution  dubbed Greedy Edge Expectation Maximization (GEEM)  that maintains the reservoir content in the case of Boosting by viewing the samples through their projections into the weak classifier response space.  We propose an efficient algorithmic implementation which makes it tractable in practice  and demonstrate its efficiency experimentally on several compute-vision data-sets  on which it outperforms both online and offline methods in a memory constrained setting.,Reservoir Boosting : Between Online and Ofﬂine

Ensemble Learning

Leonidas Lefakis

Idiap Research Institute
Martigny  Switzerland

leonidas.lefakis@idiap.ch

Franc¸ois Fleuret

Idiap Research Institute
Martigny  Switzerland

francois.fleuret@idiap.ch

Abstract

We propose to train an ensemble with the help of a reservoir in which the learning
algorithm can store a limited number of samples.
This novel approach lies in the area between ofﬂine and online ensemble approaches
and can be seen either as a restriction of the former or an enhancement of the latter.
We identify some basic strategies that can be used to populate this reservoir and
present our main contribution  dubbed Greedy Edge Expectation Maximization
(GEEM)  that maintains the reservoir content in the case of Boosting by viewing
the samples through their projections into the weak classiﬁer response space.
We propose an efﬁcient algorithmic implementation which makes it tractable in
practice  and demonstrate its efﬁciency experimentally on several compute-vision
data-sets  on which it outperforms both online and ofﬂine methods in a memory
constrained setting.

Introduction

1
Learning a boosted classiﬁer from a set of samples S = {X  Y }N ∈ RD × {−1  1} is usually
addressed in the context of two main frameworks. In ofﬂine Boosting settings [10] it is assumed that
the learner has full access to the entire dataset S at any given time. At each iteration t  the learning
algorithm calculates a weight wi for each sample i – the derivative of the loss with respect to the
classiﬁer response on that sample – and feeds these weights together with the entire dataset to a
weak learning algorithm  which learns a predictor ht. The coefﬁcient at of the chosen weak learner
ht is then calculated based on its weighted error. There are many variations of this basic model 
too many to mention here  but a common aspect of these is that they do not explicitly address the
issue of limited resources. It is assumed that the dataset can be efﬁciently processed in its entirety at
each iteration. In practice however  memory and computational limitations may make such learning
approaches prohibitive or at least inefﬁcient.
A common approach used in practice to deal with such limitations is that of sub-sampling the data-set
using strategies based on the sample weights W [9  13]. Though these methods address the limits
of the weak learning algorithms resources  they nonetheless assume a) access to the entire data-set
at all times  b) the ability to calculate the weights W of the N samples and to sub-sample K of
these  all in an efﬁcient manner. The issues with such an approach can be seen in tasks such as
computer vision  where samples need not only be loaded sequentially into memory if they do not
all ﬁt which in itself may be computationally prohibitive  but furthermore once loaded they must
be pre-processed  for example by extracting descriptors  making the calculation of the weights
themselves a computationally expensive process.
For large datasets  in order to address such issues  the framework of online learning is frequently
employed. Online Boosting algorithms [15] typically assume access solely to a Filter() function  by
which they mine samples from the data-set typically one at a time. Due to the their online nature

1

such approaches typically treat the weak learning algorithm as a black box  assuming that it can be
trained in an online manner  and concentrate on different approaches to calculating the weak learner
coefﬁcients [15  4]. A notable exception is the works of [11] and [14]  where weak learner selectors
are introduced  one for each weak learner in the ensemble  which are capable of picking a weak
learner from a predetermined pool. All these approaches however are similar in the fact that they are
forced to predetermine the number of weak learners in the boosted strong classiﬁer.
We propose here a middle ground between these two extremes in which the boosted classiﬁer can
store some of the already processed samples in a reservoir  possibly keeping them through multiple
rounds of training. As in online learning we assume access only to a Filter() through which we can
sample Qt samples at each Boosting iteration. This setting is related to the framework proposed
in [2] for dealing with large data-sets  the method proposed there however uses the ﬁlter to obtain
a sample and stochastically accepts or rejects the sample based on its weight. The drawback of
this approach is a) that after each iteration all old samples are discarded  and b) the algorithm must
process an increasing number of samples at each iteration as the weights become increasingly smaller.
We propose to acquire a ﬁxed number of samples at each iteration and to add these to a persistent
reservoir  discarding only a subset. The only other work we know which trains a Boosting classiﬁer
in a similar manner is [12]  where the authors are solely concerned with learning in the presence of
concept drift and do not propose a strategy for ﬁlling this reservoir. Rather they use a simple sliding
window approach and concentrate on the removal and adding of weak learners to tackle this drift.
A related concept to the work presented here is that of learning on a budget [6]  where  as in the
online learning setting  samples are presented one at a time to the learner  a perceptron  which builds
a classiﬁcation model by retaining an active subset of these samples. The main concern in this context
is the complexity of the model itself and its effect via the Gramm matrix computation on both training
and test time. Subsequent works on budget perceptrons has led to tighter budgets [16] (at higher
computational costs)  while [3] proved that such approaches are mistake-bound.
Similar work on Support Vector Machines [1] proposed LaSVM  a SVM solver which was shown
to converge to the SVM QP solution by adopting a scheme composed of two alternating steps 
which consider respectively the expansion and contraction of the support vector set via the SMO
algorithm. SVM budgeted learning was also considered in [8] via an L1-SVM formulation which
allowed users to speciﬁcally set a budget parameter B  and subsequently minimized the loss on the B
worst-classiﬁed examples.
As noted  these approaches are concerned with the complexity of the classiﬁcation model  that is the
budget refers to the number of samples which have none-zero coefﬁcients in the dual representation
of the classiﬁer. In this respect our work is only loosely related to what is often referred to as budget
learning  in that we solve a qualitatively different task  namely addressing the complexity of the
parsing and processing the data during training.

Table 1: Notation

Rt
|Rt|
Qt
ΣAA
µA
yA
wt

Ft(x)

F ilter()

ht
H
◦
T

the contents of the reservoir at iteration t

the size of the reservoir

the fresh batch of samples at iteration t
the covariance matrix of the edges h ◦ y

the expectation of the edges of samples in set A
the vector of labels {−1  1}|A| of samples in A
the vector of Boosting weights at iteration t
the constructed strong classiﬁer at iteration t

a ﬁlter returning samples from S

the weak learner chosen at iteration t

the family of weak learners

component-wise (Hadamard) product

number of weak learners in the strong classiﬁer

2

Table 2: Boosting with a Reservoir

Construct R0 and Q0 with r and q calls to F ilter().
for t = 1  . . .   T do

Discard q samples from Rt−1 ∪ Qt−1 samples to obtain Rt
Select ht using the samples in Rt
Compute at using Rt
Construct Qt with q calls to F ilter()

Return FT =(cid:80)T

end for

t=1 atht

2 Reservoir of samples

In this section we present in more detailed form the framework of learning a boosted classiﬁer with
the help of a reservoir. As mentioned  the batch version of Boosting consists of iteratively selecting a
weak learner ht at each iteration t  based on the loss reduction they induce on the full training set
S. In the reservoir setting  weak learners are selected solely from the information provided by the
samples contained in the reservoir Rt.
Let N be the number of training samples  and S = {1  . . .   N} the set of their indexes. We
consider here one iteration of a Boosting procedure  where each sample is weighted according to its
contribution to the overall loss. Let y ∈ {−1  1}N be the sample labels  and H ⊂ {−1  1}N the set
of weak-learners  each identiﬁed with its vector of responses over the samples. Let ω ∈ RN
+ be the
sample weights at that Boosting iteration.
For any subset of sample indexes B ⊂ {1  . . .   N} let yB ∈ {−1  1}|B| be the “extracted” vector.
We deﬁne similarly ωB  and for any weak learner h ∈ H let hB ∈ {−1  1}|B| stands for the vector
of the |B| responses over the samples in B.
At each iteration t  the learning algorithm is presented with a batch of fresh samples Qt ⊂ S  |Qt| =
q  and must choose r samples from the full set of samples Rt ∪ Qt at its disposal  in order to build
Rt+1 with |Rt+1| = r  which it subsequently uses for training.
Using the samples from Rt  the learner chooses a weak learner ht ∈ H to maximize (cid:104)ht
(cid:105) 
where ◦ stands for the Hadamard component-wise vector product. Maximizing this latter quantity
corresponds to minimizing the weighted error estimated on the samples currently in Rt. The weight
at of the selected weak learner can also be estimated with Rt.
The learner then receives a fresh batch of samples Qt+1 and the process continues iteratively. See
algorithm in Table 2. In the following we will address the issue of which strategy to employ to discard
the q samples at each time step t. To our knowledge  no previous work has been published in this or a
similar framework.

◦yRt  wt

Rt

Rt

3 Reservoir Strategies

Rt∪Qt

In the following we present a number of strategies for populating the reservoir  i.e. for choosing which
q samples from Rt ∪ Qt to discard. We begin by identifying three basic and rather straightforward
approaches. Max Weights (Max) At each iteration t the weight vector wt
is computed for the
r + q samples and the r samples with the largest weights are kept. Weighted Sampling (WSam) As
above wt
is computed  then normalized to 1  and used as a distribution to sample r samples
to keep without replacement. Random Sampling (Rand) The reservoir is constructed by sampling
uniformly r samples from the r + q available  without replacement.
These will serve mainly as benchmark baselines against which we will compare our proposed method 
presented below  which is more sophisticated and  as we show empirically  more efﬁcient. These
baselines are presented to highlight that a more sophisticated reservoir strategy is needed to ensure
competitive performance  rather than to serve as examples of state-of-the-art baselines.
Our objective will be to populate the reservoir with samples that will allow for an optimal selection
of weak learners  as close as possible to the choice we would make if we could keep all samples.

Rt∪Qt

3

The issue at hand is similar to that of feature selection: The selected samples should be jointly
informative for choosing the good weak learners. This forces to ﬁnd a proper balance between the
individual importance of the kept samples (i.e. choosing those with large weights) and maximizing
the heterogeneity of the weak learners responses on them.

3.1 Greedy Edge Expectation Maximization

(cid:104)hB ◦ yB  wB(cid:105)

In that reservoir setting  it makes sense that given a set of samples A from which we must discard
samples and retain only a subset B  what we would like is to retain a training set that is as representa-
tive as possible of the entire set A. Ideally  we would like B to be such that if we pick the optimal
weak-learner according to the samples it contains
h∗ = argmax
h∈H

(1)
it maximizes the same quantity estimated on all the samples in A. i.e. we want (cid:104)h∗
A ◦ yA  wA(cid:105) to be
large.
There may be many weak-learners in H that have the exact same responses as h∗ on the samples
in B  and since we consider a situation where we will not have access to the samples from A \ B
anymore  we model the choice among these weak-learners as a random choice. In which case  a good
h∗ is one maximizing
(2)
that is the average of the scores on the full set A of the weak-learners which coincide with h∗ on the
retained set B.
We propose to model the distribution U(H) with a normal law. If H is picked uniformly in H  under
a reasonable assumption of symmetry  we propose

EH∼U (H) ((cid:104)HA ◦ yA  ωA(cid:105) | HB = h∗

B)  

H ◦ y ∼ N (µ  Σ)

(3)
where µ is the vector of dimension N of the expectations of weak learner edges  and Σ is a covariance
matrix of size N × N. Under this model  if ¯B = A \ B  and with ΣA B denoting an extracted
sub-matrix  we have

B ◦ yB  ωB(cid:105) + EH◦y∼N (µ Σ) ((cid:104)H ¯B ◦ y ¯B  ω ¯B(cid:105) | HB = h∗
B)
B ◦ yB − µB)  w ¯B(cid:105)
B ◦ yB)  wB(cid:105) + (cid:104)µ ¯B + Σ ¯BBΣ−1

EH∼U (H) ((cid:104)HA ◦ yA  ωA(cid:105) | HB = h∗
B)
= EH◦y∼N (µ Σ) ((cid:104)HA ◦ yA  ωA(cid:105) | HB = h∗
B)
= (cid:104)h∗
= (cid:104)(h∗

(4)
(5)
(6)
(7)
Though the modeling of the discrete variables H ◦ y by a continuous distribution may seem awkward 
we point out two important aspects. Firstly the parametric modeling allows for an analytical expression
for the calculation of (2). Given that we seek to maximize this value over the possible subsets B of
A  an analytic approach is necessary for the algorithm to retain tractability. Secondly  for a given
B ◦ yB − µB) is not only the conditional
vector of edges h∗
expectation of h∗
We note that choosing B based on (7) requires estimates of three quantities: the expected weak-learner
edges µA  the co-variance matrix ΣAA  and the weak learner h∗ trained on B. Given these quantities 
we must also develop a tractable optimization scheme to ﬁnd the B maximizing it.

B ◦ yB in B  the vector µ ¯B + Σ ¯BBΣ−1
¯B ◦ y ¯B  but also its optimal linear predictor in a least squares error sense.

BB(h∗

BB(h∗

3.2 Computing Σ and µ

As mentioned  the proposed method requires in particular an estimate of the vector of expected edges
µA of the samples in A  as well as the corresponding covariance matrix ΣAA.
In practice  the estimation of the above depends on the nature of the weak learner family H. In
the case of classiﬁcation stumps  which we use in the experiments below  both these values can be
calculated with small computational cost.
A classiﬁcation stump is a simple classiﬁer hθ α d which for a given threshold θ ∈ R  polarity
α ∈ {−1  1}  and feature index d ∈ {1  . . .   D}  has the following form:

(cid:26) 1 if α xd ≥ α θ

−1 otherwise

∀x ∈ RD  hθ α d(x) =

4

(8)

where xd refers to the value of the dth component of x.
In practice when choosing the optimal stump for a given set of samples A  a learner would sort all the
samples according to each of the D dimensions  and for each dimension d it would consider stumps
with thresholds θ between two consecutive samples in that sorted list.
For this family of stumps H and given that we shall consider both polarities  Eh(hAyA) = 0.
The covariance of the edge of two samples can also be calculated efﬁciently  with O(|A|2D) com-
plexity. For two given samples i j we have

∀h ∈ H  yihiyjhj ∈ {−1  1}.

(9)
Having sorted the samples along a speciﬁc dimension d we have that for α = 1  yihiyjhj (cid:54)= yiyj
for those weak learners which disagree on those samples i.e. with min(xd
j ).
i   xd
i |) such disagreeing
i are the indexes of the samples in the sorted list then there are (|I d
If I d
weak learners for α = 1 (plus the same quantity for α = −1)  given that for each dimension d there
correspond 2(|A| − 1) weak-learners in H  we reach the following update rule ∀d ∀{i  j} :

j ) < θ < max(xd

i   xd
j − I d

j   I d

ΣAA(i  j)+ = yiyj(2 ∗ (|A| − 1) − 4 ∗ |I d

(10)
where ΣAA(i  j) refers to the i  j element of Σ. As can be seen  this leads to a cost of O(|A|2D).
Given that commonly D (cid:29) |A|  this cost should not be much higher than O(D|A| log |A|) the cost
of sorting along the D dimensions.

j − I d
i |)

3.3 Choice of h∗
As stated  the estimation of h∗ for a given B must be computationally efﬁcient. We could further
commit to the Gaussian assumption by deﬁning p(h∗ = h) ∀h ∈ H i.e. the probability that a weak
learner h will be the chosen one given that it will be trained on B and integrating over H  this
however  though consistent with the Gaussian assumption  is computationally prohibitive. Rather  we
present here two cheap alternatives both of which perform well in practice.
The ﬁrst and simplest strategy is to use ∀B  h∗ ◦ yB = (1  . . .   1) which is equivalent to making the
assumption that the training process will results in a weak learner which performs perfectly on the
training data B. This is exactly what the process will strive to achieve  however unlikely it may be.
The second is to generate a number |HLattice| of weak learner edges by sampling on the {−1  1}|B|
lattice using the Gaussian H ◦ y ∼ N (µB  ΣBB) restricted to this lattice and to keep the optimal
h∗ = argmax h ∈ HLattice(cid:104)(hB ◦ yB)  wB(cid:105). We can further simplify this process by considering the
whole set A and the lattice {−1  1}|A| and simply extracting the values h∗
B for the different subsets B.
Though much more complex  this approach can be implemented extremely efﬁciently  experiments
showed however that the simple rule of ∀B  h∗ ◦ yB = (1  . . .   1) works just as well in practice and
is considerably cheaper. In the following experiments we present results solely for this ﬁrst rule.

3.4 Greedy Calculation of argmaxB
Despite the analytical formulation offered by our Gaussian assumption  an exact maximization over
all possible subsets remains computationally intractable. For these reason we propose a greedy
approach to building the reservoir population which is computationally bounded.
We initialize the set B = A  i.e. initially we assume we are keeping all the samples  and calculate
BB. The greedy process then iteratively goes through the |B| samples in B and ﬁnds the sample j
Σ−1
such that for B

(cid:48)

(cid:48) ◦ yB

(cid:48) )  w ¯B

(cid:48)(cid:105) + (cid:104)h∗

B

(cid:48) ◦ yB

(cid:48)(cid:105)

(cid:48)   wB

(11)

= B \ {j} the value
(cid:48) Σ−1
(cid:48)
B

(cid:104)Σ ¯B

(cid:48)
B

B

(cid:48) (h∗

B

is maximized  where  in this context  h∗ refers to the weak learner chosen by training on B
process is repeated q times  i.e. until | ¯B| = q  discarding one sample at each iteration.
In the experiments presented here  we stop the greedy subset selection after these q steps. However in
practice the subset selection can continue by choosing pairs k j to swap between the two steps. In
our experiments however we did not notice any gain from further optimization of the subset B.

. This

(cid:48)

5

A  wA(cid:105)|B)

A  wA(cid:105)|B(cid:48)) for B(cid:48) = B \ {j}.

3.5 Evaluation of E((cid:104)h∗
Each step in the above greedy process requires going through all the samples j in the current B and
calculating E((cid:104)h∗
In order for our method to be computationally tractable we must be able to compute the above value
with a limited computational cost. The naive approach of calculating the value from scratch for each
j would cost O(|B(cid:48)|3 + | ¯B(cid:48)||B|) . The main computational cost here is the ﬁrst factor  incurred
in calculating the inverse of the covariance matrix ΣB
(cid:48) which results from the matrix ΣBB by
removing a single row and column. It is thus important to be able to perform this calculation with a
low computational cost.
3.5.1 Updating Σ−1
(cid:48)
B
For a given matrix M and its inverse M−1 we would like to efﬁciently calculate the inverse of M−j
which is results from M by the deletion of row and column j.
It can be shown that the inverse of the matrix Mej which results from M by the substitution of row
and column j by the basis vector ej is given by the following formula:

(cid:48)
B

B

(cid:48)

M−

ej

1 = M−1 − 1
Mii

M−1

j∗ M−1∗j + eT
j ej

(12)

where M∗j stands for the vector of elements of the jth column of matrix M and Mj∗ stand for the
vector of elements of its jth row. We omit the proof (a relatively straightforward manipulation of the
Sherman-Morrison formulas) due to space constraints. The inverse M−1−j can be recovered by simply
removing the jth row and column of M−1
ej .
(cid:48) in O(|B|2). We further exploit the fact that the matrices
Based on this we can compute Σ−1
(cid:48)
B
(cid:48) . Thus by
B(cid:48) and wT
Σ ¯B
pre-calculating the products Σ−1
¯BΣ ¯BBonce at the beginning of each greedy optimization
step  we can incur a cost of O(|B|) for each sample j and an O(|B|2) cost overall.

(cid:48) enter into the calculations through the products Σ−1
(cid:48)
B

(cid:48) and Σ−1
(cid:48)
B

B and wT

BBh∗

¯BΣ ¯B

(cid:48) h∗

(cid:48)
B

(cid:48)
B

B

B

B

3.6 Weights ˜wB

GEEM provides a method for selecting which samples to keep and which to discard. However in
doing so it creates a biased sample B of the set A  and consequently weights wB are not representative
of the weight distribution wA. It is thus necessary to alter the weights wB to obtain a new weight
vector ˜wB which will takes this bias into account. Based on the assumption (3) and (7)  and the fact
that µA = 0  we set
(13)
The resulting weight vector ˜wB used to pick the weak-learner h∗ correctly reﬂects the entire set
A = Rt ∪ Qt (under the Gaussian assumption)

¯BΣ ¯BBΣ−1

˜wB = wB + wT

BB

3.7 Overall Complexity

The proposed method GEEM comprises  at each boosting iteration  three main steps: (1) The
calculation of ΣAA  (2) The optimization of B  and (3) The training of the weak learner ht
The third step is common to all the reservoir strategies presented here. In the case of classiﬁcation
stumps by presorting the samples along each dimension and exploiting the structure of the hypothesis
space H  we can incur a cost of O(D|B| log |B|) where D is the dimensionality of the input space.
The ﬁrst step  as mentioned  incurs a cost of O(|A|2D) if we go through all dimensions of the
data. However the minimum objective of acquiring an invertible matrix ΣAA by only looking at
|A| dimensions and incurring a cost of O(|A|3). Finally the second step as analyzed in the previous
section  incurs a cost of O(q|A|2).
Thus the overall complexity of the proposed method is O(|A|3 + D|A|log|A|) which in practice
should not be signiﬁcantly larger than O(D|B|log|B|)  the cost of the remaining reservoir strategies.
We note that this analysis ignores the cost of processing incoming samples Qt which is also common
to all strategies  dependent on the task this cost may handily dominate all others.

6

4 Experiments

In order to experimentally validate both the framework of reservoir boosting as well as the proposed
method GEEM  we conducted experiments on four popular computer vision datasets.
In all our experiments we use logitboost for training. It attempts to minimize the logistic loss which
is less aggressive than the exponential loss. Original experiments with the exponential loss in a
reservoir setting showed it to be unstable and to lead to degraded performance for all the reservoir
strategies presented here. In [14] the authors performed extensive comparison in an online setting and
also found logitboost to yield the best results. We set the number of weak learners T in the boosted
classiﬁer to be T = 250 common to all methods. In the case of the online boosting algorithms this
translates to ﬁxing the number of weak learners.
Finally  for the methods that use a reservoir – that is GEEM and the baselines outlined in 3 – we set
r = q. Thus at every iteration  the reservoir is populated with |Rt| = r samples and the algorithm
receives a further |Qt| = r samples from the ﬁlter. The reservoir strategy is then used to discard r of
these samples to build Rt+1.

4.1 Data-sets
We used four standard datasets: CIFAR-10 is a recognition dataset consisting of 32 × 32 images
of 10 distinct classes depicting vehicles and animals. The training data consists of 5000 images
of each class. We pre-process the data as in [5] using code provided by the authors. MNIST is
a well-known optical digit recognition dataset comprising 60000 images of size 28 × 28 of digits
from 0 − 9. We do not preprocess the data in anyway  using the raw pixels as features. INRIA is
a pedestrian detection dataset. The training set consists of 12180 images of size 64 × 128 of both
pedestrians and background images from which we extract HoG features [7]. STL-10 An image
recognition dataset consisting of images of size 96 × 96 belonging to 10 classes  each represented by
500 images in the training set. We pre-process the data as for CIFAR.

4.2 Baselines

The baselines for the reservoir strategy have already been outlined in 3  and we also benchmarked
three online Boosting algorithms: Oza [15]  Chen [4]  and Bisc [11]. The ﬁrst two algorithms treat
weak learners as a black-box but predeﬁne their number. We initiate the weak learners of these
approaches by running Logitboost ofﬂine using a subset of the training set as we found that randomly
sampling the weak learners led to very poor performance; thus though they are online algorithms 
nonetheless in the experiments presented here they are afforded an ofﬂine initialization step. Note
that these approaches are not mutually exclusive with the proposed method  as the weak learners
picked by GEEM can be combined with an online boosting algorithm optimizing their coefﬁcients.
For the ﬁnal method [11]  we initiated the number of selectors to be K = 250 resulting in the same
number of weak learners as the other methods. We also conducted experiments with [14] which is
closely related to [11]  however as it performed consistently worse than [11]  we do not show those
results here.
Finally we compared our method against two sub-sampling methods that have access to the full
dataset and subsample r samples using a weighted sampling routine. At each iteration  these methods
compute the boosting weights of all the samples in the dataset and use weighted sampling to obtain
a subset Rt. The ﬁrst method is a simple weighted sampling method (WSS) while the second is
Madaboost (Mada) which combines weighted sampling with weight adjustment for the sub-sampled
samples. We furthermore show comparison with a ﬁxed reservoir baseline (Fix)  this baseline
subsamples the dataset once prior to learning and then trains the ensemble using ofﬂine Adaboost 
the contents of the reservoir in this case do not change from iteration to iteration.

5 Results and Discussion

Table 3  4  and 5  list respectively the performance of the reservoir baselines  the online Boosting
techniques  and the sub-sampling methods. Each table also presents the performance of our GEEM
approach in the same settings.

7

Max

Rand

WSam

GEEM

r=250

r=100

Dataset
CIFAR 29.59 (0.59) 29.16 (0.71) 46.02 (0.35) 45.88 (0.24) 48.92 (0.34) 50.09 (0.24) 50.96 (0.36) 54.87 (0.28)
STL 30.20 (0.75) 30.72 (0.82) 39.25 (0.32) 39.40 (0.25) 41.60 (0.39) 42.93 (0.30) 42.40 (0.65) 45.70 (0.38)
INRIA 95.57 (0.49) 96.31 (0.37) 91.54 (0.49) 91.72 (0.35) 94.29 (0.23) 94.63 (0.30) 97.21 (0.21) 97.52 (0.13)
MNIST 66.74 (1.45) 68.25 (0.81) 79.97 (0.24) 79.59 (0.22) 83.96 (0.29) 84.07 (0.23) 84.66 (0.30) 84.33 (0.33)

r=250

r=100

r=250

r=250

r=100

r=100

Table 3: Test Accuracy on the four datasets for the different reservoir strategies

Online Boosting

Bisc

Chen

Dataset
CIFAR 39.40 (1.91) 45.03 (0.93) 49.16 (0.40) 54.87 (0.28)
STL 33.09 (1.49) 36.35 (0.49) 39.98 (0.56) 45.70 (0.38)
INRIA 94.23 (0.97) 95.65 (0.38) 95.50 (0.49) 97.53 (0.13)
MNIST 80.99 (1.11) 85.25 (0.82) 84.85 (0.54) 84.33 (0.33)

Oza

GEEM
(r=250)

Table 4: Comparison of GEEM with online boosting algorithms
WSS

Mada

Fix

GEEM

r=250

r=100

Dataset
CIFAR 50.38 (0.38) 51.66 (0.30) 48.87 (0.26) 49.44 (0.33) 48.41 (0.88) 52.40 (0.77) 50.96(0.36) 54.87 (0.28)
STL 42.54 (0.35) 44.07 (0.31) 41.36 (0.32) 42.34 (0.24) 42.04 (0.19) 46.07 (0.41) 42.40 (0.65) 45.70 (0.38)
INRIA 94.24 (0.30) 94.65 (0.16) 94.26 (0.27) 94.65 (0.10) 92.46 (0.67) 93.82 (0.74) 97.21 (0.21) 97.53 (0.13)
MNIST 84.21 (0.27) 84.51 (0.16) 79.00 (0.33) 78.99 (0.31) 85.37 (0.33) 88.02 (0.15) 84.66 (0.30) 84.33 (0.33)

r=1 000

r=2 500

r=100

r=250

r=250

r=100

Table 5: Comparison of GEEM with subsampling algorithms

As can be seen  GEEM outperforms the other reservoir strategies on three of the four datasets and
performs on par with the best on the fourth (MNIST). It also outperforms the on-line Boosting
techniques on three data-sets and on par with the best baselines on MNIST. Finally  GEEM performs
better than all the sub-sampling algorithms. Note that the Fix baseline was provided with ten times
the number of samples to reach a similar level of performance.
These results demonstrate that both the reservoir framework we propose for Boosting  and the speciﬁc
GEEM algorithm  provide performance greater or on par with existing state-of-the-art methods. When
compared with other reservoir strategies  GEEM suffers from larger complexity which translates to
a longer training time. For the INRIA dataset and r = 100 GEEM requires circa 70 seconds for
training as opposed to 50 for the WSam strategy  while for r = 250 GEEM takes approximately 320
seconds to train compared to 70 for WSam. We note however that even when equating training time 
which translates to using r = 100 for GEEM and r = 250 for WSam  GEEM still outperforms the
simpler reservoir strategies. The timing results on the other 3 datasets were similar in this respect.
Many points can still be improved. In our ongoing research we are investigating different approaches
to modeling the process of evaluating h∗  of particular importance is of course that it is both reasonable
and fast to compute  one approach is to consider the maximum a posteriori value of h∗ by drawing on
elements in extreme value theory.
We have further plans to adapt this framework  and the proposed method  to a series of other settings.
It could be applied in the context of parallel processing  where a dataset can be split among CPUs
each training a classiﬁer on a different portion of the data.
Finally  we are also investigating the method’s suitability for active learning tasks and dataset creation.
We note that the proposed method GEEM is not given information concerning the labels of the
samples  but simply the expectation and covariance matrix of the edges.

Acknowledgments

This work was supported by the European Community’s Seventh Framework Programme FP7 -
Challenge 2 - Cognitive Systems  Interaction  Robotics - under grant agreement No 247022 - MASH.

8

References
[1] Antoine Bordes  Seyda Ertekin  Jason Weston  and L´eon Bottou. Fast kernel classiﬁers with

online and active learning. J. Mach. Learn. Res.  6:1579–1619  December 2005.

[2] Joseph K. Bradley and Robert E. Schapire. Filterboost: Regression and classiﬁcation on large

datasets. In NIPS  2007.

[3] Nicol Cesa-Bianchi and Claudio Gentile. Tracking the best hyperplane with a simple budget
perceptron. In In Proc. of Nineteenth Annual Conference on Computational Learning Theory 
pages 483–498. Springer-Verlag  2006.

[4] Shang-Tse Chen  Hsuan-Tien Lin  and Chi-Jen Lu. An online boosting algorithm with theoretical
justiﬁcations. In John Langford and Joelle Pineau  editors  ICML  ICML ’12  pages 1007–1014 
New York  NY  USA  July 2012. Omnipress.

[5] Adam Coates and Andrew Ng. The importance of encoding versus training with sparse coding
and vector quantization. In Lise Getoor and Tobias Scheffer  editors  Proceedings of the 28th
International Conference on Machine Learning (ICML-11)  ICML ’11  pages 921–928  New
York  NY  USA  June 2011. ACM.

[6] Koby Crammer  Jaz S. Kandola  and Yoram Singer. Online classiﬁcation on a budget. In
Sebastian Thrun  Lawrence K. Saul  and Bernhard Schlkopf  editors  NIPS. MIT Press  2003.
[7] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Computer
Vision and Pattern Recognition  2005. CVPR 2005. IEEE Computer Society Conference on 
volume 1  pages 886–893 vol. 1  2005.

[8] Ofer Dekel and Yoram Singer. Support vector machines on a budget. In NIPS  pages 345–352 

2006.

[9] Carlos Domingo and Osamu Watanabe. Madaboost: A modiﬁcation of adaboost. In Proceedings
of the Thirteenth Annual Conference on Computational Learning Theory  COLT ’00  pages
180–189  San Francisco  CA  USA  2000. Morgan Kaufmann Publishers Inc.

[10] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning

and an application to boosting. J. Comput. Syst. Sci.  55(1):119–139  August 1997.

[11] Helmut Grabner and Horst Bischof. On-line boosting and vision. In CVPR (1)  pages 260–267 

2006.

[12] Mihajlo Grbovic and Slobodan Vucetic. Tracking concept change with incremental boosting
by minimization of the evolving exponential loss. In ECML PKDD  ECML PKDD’11  pages
516–532  Berlin  Heidelberg  2011. Springer-Verlag.

[13] Zdenek Kalal  Jiri Matas  and Krystian Mikolajczyk. Weighted sampling for large-scale boosting.

In BMVC  2008.

[14] C. Leistner  A. Saffari  P.M. Roth  and H. Bischof. On robustness of on-line boosting -
In Computer Vision Workshops (ICCV Workshops)  2009 IEEE 12th

a competitive study.
International Conference on  pages 1362 –1369  27 2009-oct. 4 2009.

[15] Nikunj C. Oza and Stuart Russell. Online bagging and boosting. In In Artiﬁcial Intelligence

and Statistics 2001  pages 105–112. Morgan Kaufmann  2001.

[16] Bordes Antoine Weston Jason and L´eon Bottou. Online (and ofﬂine) on an even tighter budget.

In In Artiﬁcial Intelligence and Statistics 2005  2005.

9

,Leonidas Lefakis
François Fleuret