2019,Characterizing the Exact Behaviors of Temporal Difference Learning Algorithms Using Markov Jump Linear System Theory,In this paper  we provide a unified analysis of temporal difference learning algorithms with linear function approximators by exploiting their connections to Markov jump linear systems (MJLS). We tailor the MJLS theory developed in the control community to characterize the exact behaviors of the first and second order moments of a large family of temporal difference learning algorithms. For both the IID and Markov noise cases  we show that the evolution of some augmented versions of the mean and covariance matrix of the TD estimation error exactly follows the trajectory of a deterministic linear time-invariant (LTI) dynamical system. Applying the well-known LTI system theory  we obtain closed-form expressions for the mean and covariance matrix of the TD estimation error at any time step. We provide a tight matrix spectral radius condition to guarantee the convergence of the covariance matrix of the TD estimation error  and perform a perturbation analysis to characterize the dependence of the TD behaviors on learning rate. For the IID case  we provide an exact formula characterizing how the mean and covariance matrix of the TD estimation error converge to the steady state values at a linear rate. For the Markov case  we use our formulas to explain how the behaviors of TD learning algorithms are affected by learning rate and the underlying Markov chain. For both cases  upper and lower bounds for the mean square TD error are provided. The mean square TD error is shown to converge linearly to an exact limit.,Characterizing the Exact Behaviors of

Temporal Difference Learning Algorithms Using

Markov Jump Linear System Theory

Bin Hu 

Usman Ahmed Syed

Department of Electrical and Computer Engineering

Coordinated Science Laboratory

University of Illinois at Urbana-Champaign

Abstract

In this paper  we provide a uniﬁed analysis of temporal difference learning al-
gorithms with linear function approximators by exploiting their connections to
Markov jump linear systems (MJLS). We tailor the MJLS theory developed in the
control community to characterize the exact behaviors of the ﬁrst and second order
moments of a large family of temporal difference learning algorithms. For both
the IID and Markov noise cases  we show that the evolution of some augmented
versions of the mean and covariance matrix of the TD estimation error exactly fol-
lows the trajectory of a deterministic linear time-invariant (LTI) dynamical system.
Applying the well-known LTI system theory  we obtain closed-form expressions
for the mean and covariance matrix of the TD estimation error at any time step. We
provide a tight matrix spectral radius condition to guarantee the convergence of the
covariance matrix of the TD estimation error  and perform a perturbation analysis
to characterize the dependence of the TD behaviors on learning rate. For the IID
case  we provide an exact formula characterizing how the mean and covariance
matrix of the TD estimation error converge to the steady state values at a linear
rate. For the Markov case  we use our formulas to explain how the behaviors of TD
learning algorithms are affected by learning rate and the underlying Markov chain.
For both cases  upper and lower bounds for the mean square TD error are derived.
An exact formula for the steady state mean square TD error is also provided.

1

Introduction

Reinforcement learning (RL) has shown great promise in solving sequential decision making tasks
[5  48]. One important topic for RL is policy evaluation whose objective is to evaluate the value
function of a given policy. A large family of temporal difference (TD) learning methods including
standard TD  GTD  TDC  GTD2  DTD  and ATD [47  50  49  38] have been developed to solve the
policy evaluation problem. These TD learning algorithms have become important building blocks
for RL algorithms. See [17] for a comprehensive survey. Despite the popularity of TD learning 
the behaviors of these algorithms have not been fully understood from a theoretical viewpoint. The
standard ODE technique [51  9  7  36  8] can only be used to prove asymptotic convergence. Finite
sample bounds are challenging to obtain and typically developed in a case-by-case manner. Recently 
there have been intensive research activities focusing on establishing ﬁnite sample bounds for TD
learning methods with linear function approximations under various assumptions. The IID noise
case is covered in [16  37  41]. In [6]  the analysis is extended for a Markov noise model but an
extra projection step in the algorithm is required. Very recently  ﬁnite sample bounds for the TD
method (without the projection step) under the Markov assumption have been obtained in [45].
The bounds in [45] actually work for any TD learning algorithm that can be modeled by a linear

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

stochastic approximation scheme. It remains unclear how tight these bounds are (especially for the
large learning rate region). To complement the existing analysis results and techniques  we propose
a general uniﬁed analysis framework for TD learning algorithms by borrowing the Markov jump
linear system (MJLS) theory [14] from the controls literature. Our approach is inspired by a recent
research trend in applying control theory for analysis of optimization algorithms [39  30  31  29  21 
52  15  46  28  32  22  3  40  26  4  18  43]  and extends the jump system perspective for ﬁnite sum
optimization methods in [31] to TD learning.
Our key insight is that TD learning algorithms with linear function approximations are essentially
just Markov jump linear systems. Notice that a MJLS is described by a linear state space model
whose state/input matrices are functions of a jump parameter sampled from a ﬁnite state Markov
chain. Since the behaviors of MJLS have been well established in the controls ﬁeld [14  23  1  12  13 
33  34  19  20  44]  we can borrow the analysis tools there to analyze TD learning algorithms in a
more uniﬁed manner. Our main contributions are summarized as follows.

1. We present a uniﬁed Markov jump linear system perspective on a large family of TD learning
algorithms including TD  TDC  GTD  GTD2  ATD  and DTD. Speciﬁcally  we make the
key observation that these methods are just MJLS subject to some prescribed input.

2. By tailoring the existing MJLS theory  we show that the evolution of some augmented
versions of the mean and covariance matrix of the estimation error in all above TD learn-
ing methods exactly follows the trajectory of a deterministic linear time-invariant (LTI)
dynamical system for both the IID and Markov noise cases. As a result  we obtain uniﬁed
closed-form formulas for the mean and covariance matrix of the TD estimation error at any
time step.

3. We provide a tight matrix spectral radius condition to guarantee the convergence of the
covariance matrix of the TD estimation error under the general Markov assumption. By
using the matrix perturbation theory [42  35  2  24]  we perform a perturbation analysis to
show the dependence of the behaviors of TD learning on learning rate in a more transparent
manner. For the IID case  we provide an exact formula characterizing how the mean and
covariance matrix of the TD estimation error converge to the steady state values at a linear
rate. For the Markov case  we use our formulas to explain how the behaviors of TD learning
algorithms are affected by learning rate and the underlying Markov chain. For both cases 
we have shown that the mean square error of TD learning converges linearly to a limit whose
exact formula is also provided. In addition  upper and lower bounds for the mean square
error of TD learning are simultaneously obtained.

We view our proposed analysis as a complement rather than a replacement for existing analysis
techniques. The existing analysis focuses on upper bounds for the TD estimation error. Our closed-
form formulas provide both upper and lower bounds for the mean square error of TD learning. Our
analysis also characterizes the exact limit of the steady state TD error and related convergence rates.

2 Background

2.1 Notation
The set of m-dimensional real vectors is denoted as Rm. The Kronecker product of two matrices A
and B is denoted by A ⊗ B. Notice (A ⊗ B)T = AT ⊗ BT and (A ⊗ B)(C ⊗ D) = (AC) ⊗ (BD)
when the matrices have compatible dimensions. Let vec denote the standard vectorization operation
that stacks the columns of a matrix into a vector. We have vec(AXB) = (BT ⊗ A) vec(X). Let sym
denote the symmetrization operation  i.e. sym(A) = AT+A
. Let diag(Hi) denote a matrix whose
(i  i)-th block is Hi and all other blocks are zero. Speciﬁcally  given Hi for i = 1  . . .   n  we have

2

H1

...

0

diag(Hi) =

 .

0

. . .
...
...
. . . Hn

A square matrix is Schur stable if all its eigenvalues have magnitude strictly less than 1. A square
matrix is Hurwitz if all its eigenvalues have strictly negative real parts. The spectral radius of a matrix
H is denoted as σ(H). The eigenvalue with the largest magnitude of H is denoted as λmax(H) and
the eigenvalue with the largest real part of H is denoted as λmax real(H).

2

2.2 Linear time-invariant systems

A linear time-invariant (LTI) system is typically governed by the following state-space model

(1)
where xk ∈ Rnx  uk ∈ Rnu  H ∈ Rnx×nx  and G ∈ Rnx×nu. The LTI system theory has been well
documented in standard control textbooks [27  10]. Here we brieﬂy review several useful results.

xk+1 = Hxk + Guk 

• Closed-form formulas for xk: Given an initial condition x0 and an input sequence {uk} 

the sequence {xk} can be determined using the following closed-form expression

k−1(cid:88)

xk = (H)kx0 +

(H)k−1−tGut 

(2)

t=0

where (H)k stands for the k-th power of the matrix H.
• Necessary and sufﬁcient stability condition: When H is Schur stable  we know
(H)kx0 → 0 for any arbitrary x0. When σ(H) ≥ 1  there always exists x0 such that (H)kx0
does not converge to 0. When σ(H) > 1  there even exists x0 such that (H)kx0 → ∞. See
Section 7.2 in [27] for a detailed discussion. A well-known result in the controls literature is
that the LTI system (1) is stable if and only if H is Schur stable.
• Exact limit for xk: If H is Schur stable and uk converges to a limit u∞  then xk will
converge to an exact limit. This is formalized as follows.
Proposition 1. Consider the LTI system (1). If σ(H) < 1 and limk→∞ uk = u∞  then
limk→∞ xk exists and we have x∞ = limk→∞ xk = (I − H)−1Gu∞.

xk = x∞ + (H)k(x0 − x∞).

• Response for constant input: If uk = u ∀k and σ(H) < 1  then the closed-form expression
for xk can be further simpliﬁed to give the following tight convergence rate result.
Proposition 2. Suppose σ(H) < 1  and xk is determined by (1). If uk = u ∀k  then xk
converges to a limit point x∞ = limk→∞ xk = (I − H)−1Gu. And we can compute xk as
(3)
In addition  (cid:107)xk − x∞(cid:107) ≤ C0(σ(H) + ε)k for some C0 and any arbitrarily small ε > 0.
From the above proposition  we can clearly see that now xk is a sum of a constant steady
state term x∞ and a matrix power term that decays at a linear rate speciﬁed by σ(H)
(see Section 2.2 in [39] for more explanations). The convergence rate characterized by
(σ(H) + ε) is tight. More discussions on the tightness of this convergence rate are provided
in the supplementary material.
• Response for exponentially shrinking input: When uk itself converges at a linear rate
˜ρ and H is Schur stable  xk will converge to its limit point at a linear rate speciﬁed by
max{σ(H) + ε  ˜ρ}. A formal statement is provided as follows.
Proposition 3. Suppose σ(H) < 1  and xk is determined by (1).
If uk converges to
u∞ as (cid:107)uk − u∞(cid:107) ≤ C ˜ρk  then we have x∞ = limk→∞ xk = (I − H)−1Gu∞ and
(cid:107)xk − x∞(cid:107) ≤ C0 (max{σ(H) + ε  ˜ρ})k for some C0 and any arbitrarily small ε > 0.

The results in Propositions 1  2  and 3 are well known in the control community. For completeness 
we will include their proofs in the supplementary material.

2.3 Markov jump linear systems

Another important class of dynamic systems that have been extensively studied in the controls
literature is the so-called Markov jump linear system (MJLS) [14]. Let {zk} be a Markov chain
sampled from a ﬁnite state space S. A MJLS is governed by the following state-space model:

ξk+1 = H(zk)ξk + G(zk)yk 

(4)
where H(zk) and G(zk) are matrix functions of zk. Here  ξk is the state  and yk is the input. There
is a one-to-one mapping from S to the set N := {1  2  . . .   n} where n = |S|. We can assume H(zk)
is sampled from a set of matrices {H1  H2  . . .   Hn} and G(zk) is sampled from {G1  G2  . . .   Gn}.
We have H(zk) = Hi and G(zk) = Gi when zk = i. The MJLS theory has been well developed in
the controls community [14]. We will apply the MJLS theory to analyze TD learning algorithms.

3

3 A general Markov jump system perspective for TD learning

In this section  we provide a general jump system perspective for TD learning with linear function
approximations. Notice that many TD learning algorithms including TD  TDC  GTD  GTD2  A-TD 
and D-TD can be modeled by the following linear stochastic recursion:

ξk+1 = ξk + α(cid:0)A(zk)ξk + b(zk)(cid:1)  

(5)
where {zk} forms a ﬁnite state Markov chain and b(zk) satisﬁes limk→∞ Eb(zk) = 0.1 We have
A(zk) = Ai and b(zk) = bi when zk = i. For simplicity  we mainly focus on analyzing (5). Other
models including two time-scale schemes [25  54] will be discussed in the supplementary material.
Our key observation is that (5) can be rewritten as the following MJLS

ξk+1 = (I + αA(zk))ξk + αb(zk).

(6)
The above model is a special case of (4) if we set H(zk) = I + αA(zk)  G(zk) = αb(zk)  and
yk = 1 ∀k. Consequently  many TD learning algorithms can be analyzed using the MJLS theory.
We will borrow the analysis idea from the standard MJLS theory. Our analysis is built upon the fact
that some augmented versions of the mean and the covariance matrix of {ξk} for the MJLS model (4)
actually follow the dynamics of a deterministic LTI model in the form of (1) [14  Chapter 3]. To see
this  we denote the transition probabilities for the Markov chain {zk} as pij := P(zk+1 = j|zk = i)
and specify the transition matrix P by setting its (i  j)-th entry to be pij. Obviously  we have pij ≥ 0
j=1 pij = 1 for all i. Next  the indicator function 1{zk=i} is deﬁned as 1{zk=i} = 1 if zk = i

and(cid:80)n

and 1{zk=i} = 0 otherwise. Now we deﬁne the following key quantities:

i = E(cid:0)ξk(ξk)T1{zk=i}(cid:1) .

Suppose yk = 1 ∀k. Based on [14  Proposition 3.35]  qk and Qk can be iteratively calculated as

qk

i = E(cid:0)ξk1{zk=i}(cid:1)   Qk
n(cid:88)
n(cid:88)

i + Gipk

pij(Hiqk

i=1

i ) 

where pk

qk+1
j =

1

i=1

pij

(cid:20)

qk+1

qk =

Qk+1

j =

1 Qk
2

i := P(zk = i). If we further augment qk

i and Qk

(cid:0)HiQk
qk
   Qk =(cid:2)Qk
...
(cid:20)H11
(cid:21)
(cid:21)(cid:20)
qk
n
p11H1
p11H1 ⊗ H1
  H22 =
p11(H1 ⊗ G1 + G1 ⊗ H1)
p11G1
pk


p1n(H1 ⊗ G1 + G1 ⊗ H1)
1I
...
pk
nI

vec(Qk+1)
q   and uk
pn1Hn

. . .
...
. . . pnnGn

   uk

0
H21 H22

Q are given by

p1nH1 ⊗ H1

pn1Gn

p1nG1

Q =

=

...

...

...

...

where H11  H21  H22  uk
. . .
...
. . . pnnHn

H11 =

p1nH1

...

...

H21 =

uk
q =

then it is straightforward to rewrite (7) (8) as the following LTI system

i H T

i + 2 sym(Hiqk

i GT

i ) + pk

i GiGT
i

(cid:1)  

i as

(cid:3)  
(cid:20)uk

q
uk
Q

(cid:21)

 

. . . Qk
n

(cid:21)

qk

vec(Qk)

+

  

pn1Hn ⊗ Hn

. . .
...
. . . pnnHn ⊗ Hn

...

  

. . . pn1(Hn ⊗ Gn + Gn ⊗ Hn) 
...
. . .

pnn(Hn ⊗ Gn + Gn ⊗ Hn)

...

p11G1 ⊗ G1

p1nG1 ⊗ G1

...

pn1Gn ⊗ Gn

. . .
...
. . . pnnGn ⊗ Gn

...

(7)

(8)

(9)



pk

1I
...
pk
nI

 .

(10)

1This standard assumption is typically related to the projected Bellman equation and can always be enforced

by a shifting argument. More explanations are provided in Remark 1.

4

theory reviewed in Section 2.2. Obviously  we have Eξk =(cid:80)n
E(cid:107)ξk(cid:107)2 = trace((cid:80)n

A detailed derivation for the above result is presented in the supplementary material. A key implication
here is that qk and vec(Qk) follow the LTI dynamics (9) and can be analyzed using the standard LTI
i   and
n ⊗ vec(Inξ )T) vec(Qk). Hence the mean  covariance  and mean
square norm of ξk can all be calculated using closed-form expressions. We will present a detailed
analysis for (6) and provide related implications for TD learning in the next two sections.
For illustrative purposes  we explain the jump system perspective for the standard TD method.

i   E(cid:0)ξk(ξk)T(cid:1) =(cid:80)n

i ) = (1T

i=1 Qk

i=1 Qk

i=1 qk

Example 1: TD method. The standard TD method (or TD(0)) uses the following update rule:

θk+1 = θk − αφ(sk)(cid:0)(φ(sk) − γφ(sk+1))Tθk − r(sk)(cid:1)  

qk =

i = p∞

(11)
where {sk} is the underlying Markov chain  φ is the feature vector  r is the reward  γ is the discounting
factor  and θk is the weight vector to be estimated. Suppose θ∗ is the vector that solves the projected

Suppose limk→∞ pk
i bi = 0
are actually equivalent  we have naturally enforced limk→∞ Eb(zk) = 0. Therefore  the TD update
can be modeled as (6) with b(zk) satisfying limk→∞ Eb(zk) = 0. See Section 3.1 in [45] for a
similar formulation. Now we can apply the MJLS theory and the LTI model (9) to analyze the

Bellman equation. We can set zk =(cid:2)(sk+1)T
(sk)T(cid:3)T and then rewrite the TD update as
θk+1 − θ∗ =(cid:0)I + αA(zk)(cid:1) (θk − θ∗) + αb(zk) 
where A(zk) = φ(sk)(γφ(sk+1) − φ(sk))T and b(zk) = φ(sk)(cid:0)r(sk) + (φ(sk) − γφ(sk+1))Tθ∗(cid:1).
i . Since the projected Bellman equation and the equation(cid:80)n
covariance E(cid:0)(θk − θ∗)(θk − θ∗)T(cid:1) and the mean square error E(cid:107)θk − θ∗(cid:107)2. In this case  we have
vec(cid:0)E((θk − θ∗)(θk − θ∗)T1{zk=1})(cid:1)
 .
vec(cid:0)E((θk − θ∗)(θk − θ∗)T1{zk=n})(cid:1)
matrix E(cid:0)(θk − θ∗)(θk − θ∗)T(cid:1) and the mean value E(θk − θ∗) do not directly follow an LTI system.
E(cid:107)θk − θ∗(cid:107)2 = trace((cid:80)n

However  when working with the augmented covariance matrix Qk and the augmented mean value
vector qk  we do obtain an LTI model in the form of (1). Once the closed-form expression for Qk
is obtained  the mean square estimation error for the TD update can be immediately calculated as

E(cid:0)(θk − θ∗)1{zk=1}(cid:1)
E(cid:0)(θk − θ∗)1{zk=n}(cid:1)

Then we can easily analyze qk and Qk by applying the LTI model (9). In general  the covariance

   vec(Qk) =

n ⊗ vec(Inθ )T) vec(Qk).

i=1 p∞

i ) = (1T

Here we omit the detailed formulations for other TD learning methods. The key message is that {zk}
can be viewed as a jump parameter and TD learning methods are essentially just MJLS. Notice that all
the TD learning algorithms that can be analyzed using the ODE method are in the form of (6). Jump
system perspectives for other TD learning algorithms are discussed in the supplementary material.
i Ai. In this paper  we will
assume ¯A is Hurwitz. This assumption is standard and even required by the ODE approach. For the
standard TD method  ¯A is Hurwitz when the discount factor is smaller than 1  p∞
is positive for
all i  and the feature matrix is full column rank [51]. It is worth emphasizing that the assumption
i bi (cid:54)= 0. This case can still be handled using
a shifting argument since ¯A is Hurwitz. Notice the iteration ξk+1 = (I + αA(zk))ξk + αb(zk) can be
rewritten as ξk+1 − ˜ξ = ξk − ˜ξ + α
for any ˜ξ. Now we denote
˜ξ + bi and the above iteration just becomes ξk+1 − ˜ξ = (I + αA(zk))(ξk − ˜ξ) + α˜b(zk).
˜bi = Ai
i bi) such
i=1 p∞

Remark 1 (Assumptions). Denote ¯A = limk→∞ EA(zk) = (cid:80)n
limk→∞ Eb(zk) = 0 is also general. Suppose(cid:80)n
When ¯A is Hurwitz (and hence invertible)  we can choose ˜ξ = −((cid:80)n
that(cid:80)n

(cid:17)
i Ai)−1((cid:80)n

A(zk)(ξk − ˜ξ) + A(zk) ˜ξ + b(zk)

˜bi =(cid:80)n

˜ξ + bi) = 0.

i=1 p∞

i=1 p∞

i=1 p∞

i=1 p∞

i=1 p∞

i=1 Qk

i

i (Ai

(cid:16)

...

...

(12)

i

Remark 2 (Generality of (4)). Notice that (4) provides a general jump system model for linear
stochastic schemes that may have more complicated forms than (5). However  (4) can not be directly
used to cover nonlinear stochastic approximation schemes. See [53  11] for recent ﬁnite sample
analysis results on nonlinear stochastic approximation over non-IID data.

5

n(cid:88)
n(cid:88)

i=1

4 Analysis under the IID assumption
For illustrative purposes  we ﬁrst present the analysis for (6) under the IID assumption (P(zk = i) =

pi ∀i). In this case  the analysis is signiﬁcantly simpler  since {Eξk} and {E(cid:0)ξk(ξk)T(cid:1)} directly
form LTI systems with much smaller dimensions. We denote µk := Eξk and Qk := E(cid:0)ξk(ξk)T(cid:1).

Then the following equations hold for the general jump system model (4)

µk+1 =

pi(Hiµk + Gi) = ¯Hµk + ¯G 

vec(Qk+1) = (

piHi ⊗ Hi) vec(Qk) +

i=1

i=1

(cid:32) n(cid:88)

pi(Hi ⊗ Gi + Gi ⊗ Hi)

(cid:33)

µk +

n(cid:88)

i=1

piGi ⊗ Gi.
(13)

There are many ways to derive the above formulas. One way is to ﬁrst show qk
i = piµk and
i = piQk in this case and then apply (7) and (8). Another way is to directly modify the proof
Qk
of Theorem 1 (which is presented in the supplementary material). Now consider the jump system
i=1 pibi = 0. In this case  we have Hi = I + αAi 

i=1 piAi. We can directly obtain the following result.

Theorem 1. Consider the jump system model (6) with Hi = I + αAi  Gi = αbi  and yk = 1.
Suppose {zk} is sampled from N using an IID distribution P(zk = i) = pi. In addition  assume

i=1 pibi = 0. Then µk and vec(Qk) are governed by the following LTI system:

model (6) under the assumption Eb(zk) =(cid:80)n
Gi = αbi  and yk = 1. Denote ¯A :=(cid:80)n
(cid:80)n
(cid:20)H11

(cid:20) µk+1

0
H21 H22
where H11  H21 and H22 are determined as

vec(Qk+1)

(cid:21)

=

0

i=1 pi(bi ⊗ bi)

 

(14)

(cid:21)(cid:20) µk

(cid:21)

vec(Qk)

+

(cid:20)
α2(cid:80)n

(cid:21)

vec(Q∞) = lim
k→0

vec(Qk) = −α

I ⊗ ¯A + ¯A ⊗ I + α

pi(Ai ⊗ Ai)

pi(bi ⊗ bi)

Proof. For completeness  a detailed proof is presented in the supplementary material.

Now we discuss the implications of the above theorem for TD learning. For simplicity  we denote
H =

.

(cid:20)H11

0
H21 H22

(cid:21)

Stability condition for TD learning. From the LTI theory  the system (14) is stable if and only if
H is Schur stable. We can apply Proposition 3.6 in [14] to show that H is Schur stable if and only
if H22 is Schur stable. Hence  a necessary and sufﬁcient stability condition for the LTI system (14)
is that H22 is Schur stable. Under this condition  the ﬁrst term on the right side of (16) converges
to 0 at a linear rate speciﬁed by σ(H)  and the second term on the right side of (16) is a constant

6

H11 = I + α ¯A 
H21 = α2

n(cid:88)

i=1

pi(Ai ⊗ bi + bi ⊗ Ai) 

+ α(I ⊗ ¯A + ¯A ⊗ I) + α2

pi(Ai ⊗ Ai).

In addition  if σ(H22) < 1  we have

ξ

H22 = In2
(cid:21)

(cid:18)(cid:20)H11

(cid:20) µk

(cid:21)(cid:19)k(cid:18)(cid:20) µ0

(cid:21)

vec(Qk)

vec(Q0)
where µ∞ = limk→∞ µk = 0  and vec(Q∞) is given as

=

0
H21 H22

(cid:32)

i=1

n(cid:88)
(cid:20) µ∞
n(cid:88)

−

vec(Q∞)

vec(Q∞)

(cid:21)(cid:19)

+

(cid:20) µ∞
(cid:33)−1(cid:32) n(cid:88)

(15)

(16)

(cid:21)

(cid:33)

i=1

i=1

(17)

Then under mild technical condition2  we can ignore the quadratic term α2(cid:80)n

matrix quantifying the steady state covariance. An important question for TD learning is how to
choose α such that σ(H22) < 1 for some given {Ai}  {bi}  and {pi}. We provide some clue to this
question by applying an eigenvalue perturbation analysis to the matrix H22. We assume α is small.
i=1 pi(Ai ⊗ Ai) in the
expression of H22 and use λmax(In2

+ α(I ⊗ ¯A + ¯A ⊗ I)) to estimate λmax(H22). We have

ξ

λmax(H22) = 1 + 2λmax real( ¯A)α + O(α2).

(18)
Then we immediately obtain σ(H22) ≈ 1 + 2 real(λmax real( ¯A))α + O(α2). Therefore  as long as ¯A
is Hurwitz  there exists sufﬁciently small α such that σ(H22) < 1. More details of the perturbation
analysis are provided in the supplementary material.

− H22)−1 ((cid:80)n

α(cid:80)n

Exact limit for the mean square error of TD learning. Obviously  µk converges to 0 at the rate
speciﬁed by σ(I + α ¯A) due to the relation µk = (I + α ¯A)kµ0. Applying Proposition 3 and making
use of the block structure in H  one can show vec(Q∞) = α2(In2
i=1 pi(bi ⊗ bi)) 
which leads to the result in (17). A key message here is that the covariance matrix converges linearly to
an exact limit under the stability condition σ(H22) < 1. We can clearly see limk→0 vec(Qk) = O(α)
and can be controlled by decreasing α. When α is large  we need to keep the quadratic term
i=1 pi(Ai ⊗ Ai). Therefore  our theory captures the steady-state behavior of TD learning for
both small and large α  and complement the existing ﬁnite sample bounds in literatures. To further
compare our results with existing ﬁnite sample bounds  we obtain the following result for the mean
square error of TD learning.
Corollary 1. Consider the TD update (12) with ¯A being Hurwitz. Suppose σ(H22) < 1 and P(zk =
i) = pi ∀i. Then limk→∞ E(cid:107)θk − θ∗(cid:107)2 exists and is determined as δ∞ := limk→∞ E(cid:107)θk − θ∗(cid:107)2 =
trace(Q∞) where Q∞ is given by (17). In addition  the following mean square TD error bounds
hold for some constant C0 and any arbitrary small positive ε:

ξ

δ∞ − C0(σ(H) + ε)k ≤ E(cid:107)θk − θ∗(cid:107)2 ≤ δ∞ + C0(σ(H) + ε)k.

(19)
Finally  for sufﬁciently small α  one has limk→∞ E(cid:107)θk − θ∗(cid:107)2 = O(α). If λmax(In2
+ α(I ⊗ ¯A +
¯A ⊗ I)) is a semisimple eigenvalue  then σ(H) = σ(H11) = 1 + real(λmax real( ¯A))α for small α.
Proof. Recall that we have E(cid:107)θk − θ∗(cid:107)2 = trace(Qk). Taking limits on both sides leads to the
expression for δ∞. Then we can apply Proposition 2 to obtain a linear convergence bound for Qk
which eventually leads to (19). Notice ¯A is assumed to be Hurwitz. Therefore  we can apply standard
matrix perturbation theory to show δ∞ = O(α) and σ(H) = σ(H11) = 1 + real(λmax real( ¯A))α for
sufﬁciently small α.

ξ

The above corollary gives both upper and lower bounds for the mean square error of TD learning.
From the above result  the ﬁnal TD estimation error is actually exactly on the order of O(α). This
justiﬁes the tightness of the existing upper bounds for the ﬁnal TD error up to a constant factor. From
the above corollary  we can also see that one can obtain a faster convergence rate at the price of
getting a bigger steady state error. This is consistent with the ﬁnite sample bound in the literature
[6  45]. Since H21 = O(α2)  it is possible to tighten the rate as σ(H22) ≈ 1 + 2 real(λmax real( ¯A))α
by allowing some extra error on the order of O(α). We omit the details for such modiﬁcations.

5 Analysis under the Markov assumption
Now we analyze the behaviors of TD learning under the general assumption that {zk} is a Markov
chain. Recall that the augmented mean vector qk and the augmented covariance matrix Qk have been
deﬁned in Section 3. We can directly modify (9) to obtain the following result.
Theorem 2. Consider the jump system model (6) with Hi = I + αAi  Gi = αbi  and
yk = 1. Suppose {zk} is a Markov chain sampled from N using the transition matrix P . In
addition  deﬁne pk

i = P(zk = i) and set the augmented vector pk := (cid:2)pk
Clearly pk = (P T)kp0. Further denote the augmented vectors as b := (cid:2)bT

(cid:3)T.
(cid:3)T 

. . . pk
n
bT
. . .
n

pk
2
bT
2

1

1

2One such condition is that λmax(In2

ξ

+ α(I ⊗ ¯A + ¯A ⊗ I)) is a semisimple eigenvalue.

7

Then qk and vec(Qk) are governed by the following LTI model:

ˆB =(cid:2)(b1 ⊗ b1)T
(cid:20)

qk+1

0
H21 H22
where H11  H21 and H22 are given by

vec(Qk+1)

=

. . .

(cid:21)

H11 = (P T ⊗ Inξ ) diag(Inξ + αAi) 

+

qk

(cid:21)

(cid:35)

(cid:34)

vec(Qk)

α((P T diag(pk
α2((P T diag(pk

i )) ⊗ Inξ )b
i )) ⊗ In2
) ˆB

(bn ⊗ bn)T(cid:3)T  and set S(bi  Ai) := (bi ⊗ (I + αAi) + (I + αAi) ⊗ bi).
(cid:20)H11
(cid:21)(cid:20)
p11S(b1  A1)
k−1(cid:88)

. . .
...
. . . pnnS(bn  An)

) diag((Inξ + αAi) ⊗ (Inξ + αAi)).

  

pn1S(bn  An)

p1nS(b1  A1)

(21)

(20)

...

...

 

ξ

ξ

H22 = (P T ⊗ In2

H21 = α

In addition  the following closed-form solution holds for any k

qk = (H11)kq0 + α

t=0

vec(Qk) = (H22)k vec(Q0) +

(H11)k−1−t((P T diag(pt

i)) ⊗ Inξ )b 

(H22)k−1−t(cid:16)H21qt + α2((P T diag(pt

k−1(cid:88)

(cid:17)

(22)

 

i)) ⊗ In2

ξ

) ˆB

where H11  H21 and H22 are determined by (21).

t=0

Proof. A detailed proof is presented in the supplementary material. We present a proof sketch here.
Notice (20) is a direct consequence of (7) and (8) (which are special cases of Proposition 3.35 in [14]).
Speciﬁcally  it is straightforward to verify the following equations using the Markov assumption

(cid:1)  

i + αpk

i bi

n(cid:88)
n(cid:88)

i=1

(cid:0)(I + αAi)qk
(cid:0)(I + αAi)Qk

pij

pij

qk+1
j =

Qk+1

j =

(cid:1) .

(23)

(24)

i (I + αAi)T + 2α sym((I + αAi)qk

i bT

i ) + α2pk

i bibT
i

i=1

Then we can apply the basic property of the vectorization operation to obtain (20). Applying (2) to
iterate (20) directly leads to (22).

Therefore  the evolutions of qk and Qk can be fully understood via the well-established LTI system
theory. Now we discuss the implications of Theorem 2 for TD learning.

ξ

Stability condition for TD learning. Similar to the IID case  the necessary and sufﬁcient stability
condition is σ(H22) < 1. Now H22 becomes a much larger matrix depending on the transition matrix
P . An important question is how to choose α such that σ(H22) < 1 for some given {Ai}  {bi}  P  
and {p0}. Again  we perform an eigenvalue perturbation analysis for the matrix H22. This case is
quite subtle due to the fact that we are no longer perturbing an identity matrix. We are perturbing the
matrix (P T ⊗ In2
) and the eigenvalues here are not simple. Under the ergodicity assumption  the
largest eigenvalue for (P T ⊗ In2
) (which is 1) is semisimple. Hence we can directly apply the results
in Section II of [35] or Theorem 2.1 in [42] to show

λmax(H22) = 1 + 2λmax real( ¯A)α + o(α) 

(25)
i Ai and p∞ is the unique stationary distribution of the Markov chain under the
ergodicity assumption. Then we still have σ(H22) ≈ 1 + 2 real(λmax real( ¯A))α + o(α). Therefore 
as long as ¯A is Hurwitz  there exists sufﬁciently small α such that σ(H22) < 1. This is consistent
with Assumption 3 in [45]. To understand the details of our perturbation argument  we refer the
readers to the remark placed after Theorem 2.1 in [42]. Notice we have

where ¯A =(cid:80)n

i=1 p∞

ξ

H22 = P T ⊗ In2

+ α(P T ⊗ In2

) diag(Ai ⊗ I + I ⊗ Ai) + O(α2).

The largest eigenvalue of P T ⊗ In2
is semisimple due to the ergodicity assumption. Then the
perturbation result directly follows as a consequence of Theorem 2.1 in [42]. More explanations are
also provided in the supplementary material.

ξ

ξ

ξ

8

Exact limit for the mean square TD error and related convergence rate. Assume the Markov
chain {zk} is aperiodic and irreducible. Then we have pt → p∞ at some linear rate where p∞ is the
stationary distribution. In this case  we can apply Proposition 3 to show that the mean square error of
TD learning converges linearly to an exact limit.
Corollary 2. Consider the TD update (12) with ¯A being Hurwitz. Let {zk} be a Markov chain
sampled from N using the transition matrix P . Suppose σ(H22) < 1. We set N = nn2
ξ. If we assume
pk → p∞ where p∞ is the stationary distribution for {zk}  then we have

vec(Qk) = α2(IN − H22)−1(cid:16)
k→∞ qk = α(I − H11)−1((P T diag(p∞
q∞ = lim
vec(Q∞) = lim
k→0
δ∞ = lim
k→∞

E(cid:107)θk − θ∗(cid:107)2 = (1T

n ⊗ vec(Inθ )T) vec(Q∞).

i )) ⊗ Inξ )b 

α−2H21q∞ + ((P T diag(p∞

i )) ⊗ In2

) ˆB

ξ

 

(26)

(cid:17)

Due to the assumption(cid:80)n

If we further assume the geometric ergodicity  i.e. (cid:107)pk − p∞(cid:107) ≤ C ˜ρk  then we have

ξ

ξ

+ α(P T ⊗ In2

δ∞ − C0 max{σ(H) + ε  ˜ρ}k ≤ E(cid:107)θk − θ∗(cid:107)2 ≤ δ∞ + C0 max{σ(H) + ε  ˜ρ}k 

(27)
where C0 is some constant and ε is an arbitrary small positive number. For sufﬁciently small α 
we have δ∞ = O(α). If λmax(P T ⊗ In2
) diag(Ai ⊗ I + I ⊗ Ai)) is a semisimple
eigenvalue  then we further have σ(H) = 1 + real(λmax real( ¯A))α + o(α) for sufﬁciently small α.
n ⊗ vec(Inξ )T) vec(Qk). We can directly apply Theorem 2 
Proof. Notice E(cid:107)θk − θ∗(cid:107)2 = (1T
Proposition 1  and Proposition 3 to prove (26) and (27). When α is small  we can apply the Laurent
series trick in [2  24] to show that limk→0 vec(Qk) = O(α) and δ∞ = O(α). The difﬁculty here
is that IN − P T ⊗ In2
is a singular matrix and hence (IN − H22)−1 does not have a Taylor series
around α = 0. Therefore  we need to apply some advanced matrix inverse perturbation result to
perform a Laurent expansion of (IN − H22)−1. By using the ergodicity assumption and the matrix
inverse perturbation theory in [2  24]  we can obtain the Laurent expansion of (IN − H22)−1 and
show limk→0 vec(Qk) = O(α). Consequently  we have δ∞ = O(α). By applying Theorem 2.1 in
[42]  we can show σ(H) = 1 + real(λmax real( ¯A))α + o(α).

ξ

i=1 p∞

i bi = 0  we have limk→∞ qk (cid:54)= 0 in general but µ∞ = 0. Again  we
have obtained both upper and lower bounds for the mean square TD error. Our result states that under
mild technical assumptions  the ﬁnal TD error is actually exactly on the order of O(α). This justiﬁes
the tightness of the existing upper bounds for the ﬁnal TD error [6  45] up to a constant factor. From the
above corollary  we can also see the trade-off between the convergence rate and the steady state error.
Clearly  the convergence rate in (27) also depends on the initial distribution p0 and the mixing rate of
the underlying Markov jump parameter {zk} (which is denoted as ˜ρ). If the initial distribution is the
stationary distribution  i.e. p0 = p∞  the input to the LTI dynamical system (20) is just a constant for
all k and then we will be able to obtain an exact formula similar to (16). However  for a general initial
distribution p0  the mixing rate ˜ρ matters more and may affect the overall convergence rate. One
resultant guideline for algorithm design is that increasing α may not increase the convergence rate
when the mixing rate ˜ρ dominates the convergence process. When α becomes smaller and smaller 
eventually σ(H) is going to become the dominating term and the mixing rate does not affect the
convergence rate any more. Similar to the IID case  for sufﬁciently small α  it seems possible to
obtain alternative upper bounds in the form of E(cid:107)θk − θ∗(cid:107)2 ≤ δ∞ + O(α) + C0(σ(H22) + ε)k
where σ(H22) ≈ 1 + 2 real(λmax real( ¯A))α. Such modiﬁcations are not pursued in this paper.
Algorithm design. Here we make a remark on how our proposed MJLS framework can be further
extended to provide clues for designing fast TD learning. When α (or even other hyperparameters
including momentum term) is changing with time  we can still obtain expressions of vec(Qk) and qk
in an iterative form. However  both H and G depend on k now. Then given a ﬁxed time budget T   in
theory it is possible to minimize the mean square estimation error at T subject to some optimization
+ G(k)uk. One
constraints in the form of a time-varying iteration
may use this control-oriented optimization formulation to gain some theoretical insights on how
to choose hyperparameters adaptively for fast TD learning. Clearly  solving such an optimization
problem requires knowing the underlying Markov model. However  this type of theoretical study
may lead to new hyperparameter tuning heuristics that do not require the model information.

= H(k)

vec(Qk+1)

vec(Qk)

qk+1

(cid:20)

(cid:21)

(cid:20)

(cid:21)

qk

9

References
[1] H. Abou-Kandil  G. Freiling  and G. Jank. On the solution of discrete-time Markovian jump linear quadratic

control problems. Automatica  31(5):765–768  1995.

[2] K. Avrachenkov and J. Lasserre. Analytic perturbation of generalized inverses. Linear Algebra and its

Applications  438(4):1793–1813  2013.

[3] N. Aybat  A. Fallah  M. Gurbuzbalaban  and A. Ozdaglar. Robust accelerated gradient method. arXiv

preprint arXiv:1805.10579  2018.

[4] N. Aybat  A. Fallah  M. Gurbuzbalaban  and A. Ozdaglar. A universally optimal multistage accelerated

stochastic gradient method. arXiv preprint arXiv:1901.08022  2019.

[5] D. Bertsekas and J. Tsitsiklis. Neuro-dynamic programming  volume 5. Athena Scientiﬁc Belmont  1996.

[6] J. Bhandari  D. Russo  and R. Singal. A ﬁnite time analysis of temporal difference learning with linear

function approximation. arXiv preprint arXiv:1806.02450  2018.

[7] S. Bhatnagar  H. Prasad  and L. Prashanth. Stochastic recursive algorithms for optimization: simultaneous

perturbation methods  volume 434. Springer  2012.

[8] V. Borkar. Stochastic approximation: a dynamical systems viewpoint  volume 48. Springer  2009.

[9] V. Borkar and S. Meyn. The ODE method for convergence of stochastic approximation and reinforcement

learning. SIAM Journal on Control and Optimization  38(2):447–469  2000.

[10] C. Chen. Linear system theory and design. Oxford University Press  Inc.  1998.

[11] Z. Chen  S. Zhang  T. Doan  S. Maguluri  and J. Clarke. Performance of Q-learning with linear function

approximation: Stability and ﬁnite-time analysis. arXiv preprint arXiv:1905.11425  2019.

[12] H. Chizeck  A. Willsky  and D. Castanon. Discrete-time Markovian-jump linear quadratic optimal control.

International Journal of Control  43(1):213–231  1986.

[13] O. Costa and M. Fragoso. Stability results for discrete-time linear systems with Markovian jumping

parameters. Journal of Mathematical Analysis and Applications  179(1):154–178  1993.

[14] O. Costa  M. Fragoso  and R. Marques. Discrete-time Markov jump linear systems. Springer Science &

Business Media  2006.

[15] S. Cyrus  B. Hu  B. Van Scoy  and L. Lessard. A robust accelerated optimization algorithm for strongly

convex functions. In 2018 Annual American Control Conference (ACC)  pages 1376–1381  2018.

[16] G. Dalal  B. Szörényi  G. Thoppe  and S. Mannor. Finite sample analyses for td (0) with function

approximation. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence  2018.

[17] C. Dann  G. Neumann  and J. Peters. Policy evaluation with temporal differences: A survey and comparison.

The Journal of Machine Learning Research  15(1):809–883  2014.

[18] N. Dhingra  S. Khong  and M. Jovanovic. The proximal augmented lagrangian method for nonsmooth

composite optimization. IEEE Transactions on Automatic Control  2018.

[19] L. El Ghaoui and M. Rami. Robust state-feedback stabilization of jump linear systems via LMIs. Interna-

tional Journal of Robust and Nonlinear Control  6(9-10):1015–1022  1996.

[20] Y. Fang and K. Loparo. Stochastic stability of jump linear systems. IEEE Transactions on Automatic

Control  47(7):1204–1208  2002.

[21] M. Fazlyab  A. Ribeiro  M. Morari  and V. Preciado. Analysis of optimization algorithms via integral

quadratic constraints: Nonstrongly convex problems. arXiv preprint arXiv:1705.03615  2017.

[22] M. Fazlyab  A. Ribeiro  M. Morari  and V. Preciado. A dynamical systems perspective to convergence rate
analysis of proximal algorithms. In 2017 55th Annual Allerton Conference on Communication  Control 
and Computing (Allerton)  pages 354–360  2017.

[23] X. Feng  K. Loparo  Y. Ji  and H. Chizeck. Stochastic stability properties of jump linear systems. IEEE

transactions on Automatic Control  37(1):38–53  1992.

[24] P. Gonzalez-Rodriguez  M. Moscoso  and M. Kindelan. Laurent expansion of the inverse of perturbed 

singular matrices. Journal of Computational Physics  299:307–319  2015.

10

[25] H. Gupta  R Srikant  and L. Ying. Finite-time performance bounds and adaptive learning rate selection for

two time-scale reinforcement learning. In Advances in Neural Information Processing Systems  2019.

[26] S. Han. Systematic design of decentralized algorithms for consensus optimization. arXiv preprint

arXiv:1903.01023  2019.

[27] J. Hespanha. Linear systems theory. Princeton university press  2009.

[28] B. Hu and L. Lessard. Control interpretations for ﬁrst-order optimization methods. In 2017 American

Control Conference (ACC)  pages 3114–3119  2017.

[29] B. Hu and L. Lessard. Dissipativity theory for Nesterov’s accelerated method. In Proceedings of the 34th

International Conference on Machine Learning  2017.

[30] B. Hu  P. Seiler  and L. Lessard. Analysis of approximate stochastic gradient using quadratic constraints

and sequential semideﬁnite programs. arXiv preprint arXiv:1711.00987  2017.

[31] B. Hu  P. Seiler  and A. Rantzer. A uniﬁed analysis of stochastic optimization methods using jump system

theory and quadratic constraints. In Conference on Learning Theory  pages 1157–1189  2017.

[32] B. Hu  S. Wright  and L. Lessard. Dissipativity theory for accelerating stochastic variance reduction: A
uniﬁed analysis of svrg and katyusha using semideﬁnite programs. In International Conference on Machine
Learning  pages 2043–2052  2018.

[33] Y. Ji and H. Chizeck. Controllability  observability and discrete-time Markovian jump linear quadratic

control. International Journal of Control  48(2):481–498  1988.

[34] Y. Ji and H. Chizeck. Jump linear quadratic Gaussian control: Steady-state solution and testable conditions.

Control Theory Adv. Technol.  6:289–319  1990.

[35] T. Kato. Perturbation theory for linear operators  volume 132. Springer Science & Business Media  2013.

[36] H. Kushner and G. Yin. Stochastic approximation and recursive algorithms and applications  volume 35.

Springer Science & Business Media  2003.

[37] C. Lakshminarayanan and C. Szepesvari. Linear stochastic approximation: How far does constant step-size
and iterate averaging go? In International Conference on Artiﬁcial Intelligence and Statistics  pages
1347–1355  2018.

[38] D. Lee and N. He. Target-based temporal difference learning. arXiv preprint arXiv:1904.10945  2019.

[39] L. Lessard  B. Recht  and A. Packard. Analysis and design of optimization algorithms via integral quadratic

constraints. SIAM Journal on Optimization  26(1):57–95  2016.

[40] L. Lessard and P. Seiler. Direct synthesis of iterative algorithms with bounds on achievable worst-case

convergence rate. arXiv preprint arXiv:1904.09046  2019.

[41] B. Liu  J. Liu  M. Ghavamzadeh  S. Mahadevan  and M. Petrik. Finite-sample analysis of proximal gradient

td algorithms. In UAI  pages 504–513  2015.

[42] J. Moro  J. Burke  and M. Overton. On the Lidskii–Vishik–Lyusternik perturbation theory for eigenvalues
of matrices with arbitrary Jordan structure. SIAM Journal on Matrix Analysis and Applications  18(4):793–
817  1997.

[43] Z. Nelson and E. Mallada. An integral quadratic constraint framework for real-time steady-state optimiza-
tion of linear time-invariant systems. In 2018 Annual American Control Conference (ACC)  pages 597–603 
2018.

[44] P. Seiler and R. Sengupta. A bounded real lemma for jump systems. IEEE Transactions on Automatic

Control  48(9):1651–1654  2003.

[45] R. Srikant and L. Ying. Finite-time error bounds for linear stochastic approximation and TD learning.

arXiv preprint arXiv:1902.00923  2019.

[46] A. Sundararajan  B. Hu  and L. Lessard. Robust convergence analysis of distributed optimization algorithms.
In 2017 55th Annual Allerton Conference on Communication  Control  and Computing (Allerton)  pages
1206–1212  2017.

[47] R. Sutton. Learning to predict by the methods of temporal differences. Machine learning  3(1):9–44  1988.

11

[48] R. Sutton and A. Barto. Reinforcement learning: An introduction. MIT press  2018.

[49] R. Sutton  H. Maei  D. Precup  S. Bhatnagar  D. Silver  C. Szepesvári  and E. Wiewiora. Fast gradient-
descent methods for temporal-difference learning with linear function approximation. In Proceedings of
the 26th Annual International Conference on Machine Learning  pages 993–1000  2009.

[50] R. Sutton  C. Szepesvári  and H. Maei. A convergent o (n) algorithm for off-policy temporal-difference
learning with linear function approximation. In Proceedings of the 21st International Conference on
Neural Information Processing Systems  pages 1609–1616  2008.

[51] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation.

IEEE Transactions on Automatic Control  42(5):674–690  1997.

[52] B. Van Scoy  R. Freeman  and K. Lynch. The fastest known globally convergent ﬁrst-order method for

minimizing strongly convex functions. IEEE Control Systems Letters  2(1):49–54  2017.

[53] G. Wang  B. Li  and G. Giannakis. A multistep Lyapunov approach for ﬁnite-time analysis of biased

stochastic approximation. arXiv preprint arXiv:1909.04299  2019.

[54] T. Xu  S. Zou  and Y. Liang. Two time-scale off-policy TD learning: Non-asymptotic analysis over

Markovian samples. In Advances in Neural Information Processing Systems  2019.

12

,Bin Hu
Usman Syed