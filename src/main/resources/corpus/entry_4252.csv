2010,Penalized Principal Component Regression on Graphs for Analysis of Subnetworks,Network models are widely used to capture interactions among component of complex systems  such as social and biological. To understand their behavior  it is often necessary to analyze functionally related components of the system  corresponding to subsystems. Therefore  the analysis of subnetworks may provide additional insight into the behavior of the system  not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efficient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions  and provides a flexible inference framework for analysis of subnetworks  based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method  as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology.,Penalized Principal Component Regression on

Graphs for Analysis of Subnetworks

Ali Shojaie

Department of Statistics
University of Michigan
Ann Arbor  MI 48109

shojaie@umich.edu

George Michailidis

Department of Statistics and EECS

University of Michigan
Ann Arbor  MI 48109

gmichail@umich.edu

Abstract

Network models are widely used to capture interactions among component of
complex systems  such as social and biological. To understand their behavior  it
is often necessary to analyze functionally related components of the system  cor-
responding to subsystems. Therefore  the analysis of subnetworks may provide
additional insight into the behavior of the system  not evident from individual
components. We propose a novel approach for incorporating available network
information into the analysis of arbitrary subnetworks. The proposed method of-
fers an efﬁcient dimension reduction strategy using Laplacian eigenmaps with
Neumann boundary conditions  and provides a ﬂexible inference framework for
analysis of subnetworks  based on a group-penalized principal component regres-
sion model on graphs. Asymptotic properties of the proposed inference method 
as well as the choice of the tuning parameter for control of the false positive rate
are discussed in high dimensional settings. The performance of the proposed
methodology is illustrated using simulated and real data examples from biology.

1

Introduction

Simultaneous analysis of groups of system components with similar functions  or subsystems  has
recently received considerable attention. This problem is of particular interest in high dimensional
biological applications  where changes in individual components may not reveal the underlying
biological phenomenon  whereas the combined effect of functionally related components could im-
prove the efﬁciency and interpretability of results. This idea has motivated the method of gene set
enrichment analysis (GSEA)  along with a number of related methods [1  2]. The main premise
of this method is that by assessing the signiﬁcance of sets rather than individual components (i.e.
genes)  interactions among them can be preserved  and more efﬁcient inference methods can be
developed. A different class of models (see e.g. [3  4] and references therein) has focused on di-
rectly incorporating the network information in order to achieve better efﬁciency in assessing the
signiﬁcance of individual components.
These ideas have been combined in [5  6]  by introducing a model for incorporating the regulatory
gene network  and developing an inference framework for analysis of subnetworks deﬁned by bio-
logical pathways. In this frameworks  called NetGSA  a global model is introduced with parameters

1

for individual genes/proteins  and the parameters are then combined appropriately in order to assess
the signiﬁcance of biological pathways. However  the main challenge in applying NetGSA in real-
world biological applications is the extensive computational time. In addition  the total number of
parameters allowed in the model are limited by the available sample size n (see Section 5).
In this paper  we propose a dimension reduction technique for networks  based on Laplacian eigen-
maps  with the goal of providing an optimal low-dimensional projection for the space of random
variables in each subnetwork. We then propose a general inference framework for analysis of sub-
networks by reformulating the inference problem as a penalized principal regression problem on the
graph. In Section 2  we review the Laplacian eigenmaps and establish their connection to principal
component analysis (PCA) for random variables on a graph. Inference for signiﬁcance of subnet-
works is discussed in Section 3  where we introduce Laplacian eigenmaps with Neumann boundary
conditions and present the group-penalized principal component regression framework for analy-
sis of arbitrary subnetworks. Results of applying the new methodology to simulated and real data
examples are presented in Section 4  and the results are summarized in Section 5.

2 Laplacian Eigenmaps

Consider p random variables Xi i = 1  . . .   p (e.g. expression values of genes) deﬁned on nodes of
an undirected (weighted) graph G = (V E). Here V is the set of nodes of G and E ⊆ V ×V its edge
set. Throughout this paper  we represent the edge set and the strength of associations among nodes
through the adjacency matrix of the graph A. Speciﬁcally  Ai j ≥ 0 and i and j are adjacent if the Ai j
(and hence A ji) is non-zero. In this case we write i ∼ j. Finally  we denote the observed values of
the random variables by the n× p data matrix X.
The subnetworks of interest are deﬁned based on additional knowledge about their attributes and
functions. In biological applications  these subnetworks are deﬁned by common biological function 
co-regulation or chromosomal location. The objective of the current paper is to develop dimension
reduction methods on networks  in order to assess the signiﬁcance of a priori deﬁned subnetworks
(e.g. biological pathways) with minimal information loss.

2.1 Graph Laplacian and Eigenmaps

Laplacian eigenmaps are deﬁned using the eigenfunctions of the graph Laplacian  which is com-
monly used in spectral graph theory  computer science and image processing. Applications based
on Laplacian eigenmaps include image segmentation and the normalized cut algorithm of [7]  spec-
tral clustering [8  9] and collaborative ﬁltering [10].
The Laplacian matrix and its eigenvectors have also been used in biological applications. For exam-
ple  in [11]  the Laplacian matrix has been used to deﬁne a network-penalty for variable selection
on graphs  and the interpretation of Laplacian eigenmaps as a Fourier basis was exploited in [12] to
propose supervised and unsupervised classiﬁcation methods.
Different deﬁnitions and representations have been proposed for the spectrum of a graph  and the
results may vary depending on the deﬁnition of the Laplacian matrix (see [13] for a review). Here 
we follow the notation in [13]  and consider the normalized Laplacian matrix of the graph. To that
end  let D denote the diagonal degree matrix for A  i.e. Dii = ∑ j Ai j ≡ di  and deﬁne the Laplacian
matrix of the graph by L = D−1/2(D− A)D−1/2  or alternatively
j = i d j (cid:54)= 0
j ∼ i
o.w.

1− A j j
d j
− Ai j√
did j
0



Li j =

2

It can be shown that [13] L is positive semideﬁnite with eigenvalues 0 = λ0 ≤ λ1 ≤ . . . ≤ λp−1 ≤ 2.
Its eigenfunctions are known as the spectrum of G   and optimize the Rayleigh quotient

(cid:104)g  L g(cid:105)
(cid:104)g g(cid:105) =

∑i∼ j ( f (i)− f ( j))2

∑ j f ( j)2d j

 

(1)

It can be seen from (1)  that the 0-eigenvalue of L is g = D1/21  corresponding to the average
over the graph G . The ﬁrst non-zero eigenvalue λ1 is the harmonic eigenfunction of L   which
corresponds to the Laplace-Beltrami operator on Reimannian manifolds  and is given by

More generally  denoting by Ck−1 the projection to the subspace of the ﬁrst k− 1 eigenfunctions 

λ1 = inf
f⊥D1

∑ j∼i ( f (i)− f ( j))2

∑ j f ( j)2d j

λk = inf

f⊥DCk−1

∑ j∼i ( f (i)− f ( j))2

∑ j f ( j)2d j

.

2.2 Principal Component Analysis on Graphs

Previous applications of the graph Laplacian and its spectrum often focus on the properties of the
graph; however  the connection to the probability distribution of the random variables on nodes of
the graph has not been strongly emphasized. In graphical models  the undirected graph G among
random variables corresponds naturally to a Markov random ﬁeld [14]. The following result es-
tablishes the relationship between the Laplacian eigenmaps and the principal components of the
random variables deﬁned on the nodes of the graph  in case of Gaussian observations.
Lemma 1. Let X = (X1  . . .  Xp) be random variables deﬁned on the nodes of graph G = (V E)
and denote by L and L + the Laplacian matrix of G and its Moore-Penrose generalized inverse.
If X ∼ N(0 Σ)  then L and L + correspond to Ω and Σ  respectively (Ω ≡ Σ−1). In addition  let
ν0  . . .  νp−1 denote the eigenfunctions corresponding to eigenvalues of L . Then ν0  . . .  νp−1 are
the principal components of X  with ν0 corresponding to the leading principal component.

Proof. For Gaussian random variables  the inverse covariance (or precision) matrix has the same
non-zero pattern as the adjacency matrix of the graph  i.e. for i (cid:54)= j  Ωi j = 0 iff Ai j = 0. Moreover 
Ωii = τ−2
is the partial variance of Xi (see e.g. [15]). However  using the conditional
autoregression (CAR) representation of Gaussian Markov random ﬁelds [16]  we can write

  where τ2
i

i

E(Xi|X−i) = ∑
j∼i

ci jXj

Lemma [16] it follows from (2) that fX (x) ∝ exp(cid:8)−1/2xT(0 T−1(Ip −C))x(cid:9)  where T = diag[τ2

(2)
where −i ≡ {1 . . . p}\i and C = [ci j] has the same non-zero pattern as the adjacency matrix of
the graph A  and amounts to a proper probability distribution for X.
In particular  by Brook’s
i ].
Therefore  Ω = T−1(Ip −C) and hence (Ip −C) should be PD.
However  since L = Ip−D−1/2AD−1/2 is PSD  we can set C = D−1/2AD−1/2−ζ I for any ζ > 0. In
other words  (Ip−C) = L +ζ Ip  which implies that
˜L ≡ L +ζ Ip = TΩ  and hence ˜L −1 = ΣT−1.
Taking limit as ζ → 0  it follows that L and L + correspond to Ω and Σ  respectively.
˜L −1 and Σ. In particular 
The second part follows directly from the above connection between
suppose  without loss of generality  that τ2
i = 1. Then  it is easily seen that the principal components
˜L with
of X are given by eigenfunctions of
the ordering of the eigenvalues reversed. However  since eigenfunctions of L + ζ Ip and L are
equal  the principal components of X are obtained from eigenfunctions of L .

˜L −1  which are in turn equal to the eigenfunctions of

3

Figure 1: Left: A simple subnetwork of interest  marked with the dotted circle. Right: Illustration
of the Neumann random walk  the dotted curve indicates the boundary of the subnetwork.

Remark 2. An alternative justiﬁcation for the above result  for general probability distributions
deﬁned on graphs  can be given by assuming that the graph represents “similarities” among random
variables and using an optimal embedding of graph G in a lower dimensional Euclidean space1.
In the case of one dimensional embedding  the goal is to ﬁnd an embedding v = (v1  . . .  vp)T that
preserves the distances among the nodes of the graph. The objective function of the embedding
problem is then given by Q = ∑i  j (vi − v j)2Ai j  or alternatively Q = 2vT(D− A)v [17]. Thus  the
optimal embedding is found by solving argminvTDv=1 vT(D− A)v. Setting u = D1/2v  this is solved
by ﬁnding the eigenvector corresponding to the smallest eigenvalue of L .

Lemma 1 provides an efﬁcient dimension reduction framework that summarizes the information in
the entire network into few feature vectors. Although the resulting dimension reduction method
can be used efﬁciently in classiﬁcation (as in [12])  the eigenfunctions of G do not provide any
information about signiﬁcance of arbitrary subnetworks  and therefore cannot be used to analyze
the changes in subnetworks. In the next section  we introduce a restricted version of Laplacian
eigenmaps  and discuss the problem of analysis of subnetworks.

3 Analysis of Subnetworks and PCR on Graph (GPCR)

In [5]  the authors argue that to analyze the effect of subnetworks  the test statistic needs to represent
the pure effect of the subnetwork  without being inﬂuenced by external nodes  and propose an
inference procedure based on mixed linear models to achieve this goal. However  in order to achieve
dimension reduction  we need a method that only incorporates local information at the level of each
subnetwork  and possibly its neighbors (see the left panel of Figure 1).
Using the connection of the Laplace operator in Reimannian manifolds to heat ﬂow (see e.g. [17]) 
the problem of analysis of arbitrary subnetworks can be reformulated as a heat equation with bound-
ary conditions. It then follows that in order to assess the “effect” of each subnetwork  the appropriate
boundary conditions should block the ﬂow of heat at the boundary of the set. This corresponds to
insulating the boundary  also known as the Neumann boundary condition. For the general heat
equation τ(v x)  this boundary condition is given by ∂τ
∂ v (x) = 0 at each boundary point x  where v is
the normal direction orthogonal to the tangent hyperplane at x.
In particular  let S
The eigenvalues of subgraphs with boundary conditions are studied in [13].
be any (connected) subnetwork of G   and denote by δ S the boundary of S in G . The Neumann
boundary condition states that for every x ∈ δ S  ∑y:{x y}∈δ S ( f (x)− f (y)) = 0.
The Neumann eigenfunctions of S are then the optimizers of the restricted Rayleigh quotient

∑t∈S ( f (t)− g(t))2 dt
where Ci−1 is the projection to the space of previous eigenfunctions.

∑{t u}∈S∪δ S ( f (t)− f (u))2

λS i = inf
f

sup
g∈Ci−1

1For unweighted graphs  this justiﬁcation was given by [17]  using the unnormlized Laplacian matrix.

4

         ρ1 ρ2 X1 X2 X3 In [13]  a connection between the Neumann boundary conditions and a reﬂected random walk on the
graph is established  and it is shown that the Neumann eigenvectors can be alternatively calculated
from the eigenvectors of the transition probability matrix of this reﬂected random walk  also known
as the Neumann random walk (see [13] for additional details). Here  we generalize this idea to
weighted adjacency matrices.
Let ˜P and P denote the transition probability matrix of the reﬂected random walk  and the original
random walk deﬁned on G   respectively. Noting that P = D−1A  we can extend the results in [13]
as follows. For the general case of weighted graphs  deﬁne the transition probability matrix of the
reﬂected random walk by

 Pi j

Pi j +
0

˜Pi j =

AikAk j
did(cid:48)
k

j ∼ i i  j ∈ S
j ∼ k ∼ i k /∈ S
o.w.

k = ∑i∼k i∈S Aki denotes the degree of the node k in S. Then  the Neumann eigenvalues are

where d(cid:48)
given by λi = 1− κi  where κi is the ith eigenvalue of ˜P.
Remark 3. The connection with the Neumann random walk also sheds light into the effect of the
proposed boundary condition on the joint probability distribution of the random variables on the
graph. To illustrate this  consider the simple graph in the right panel of Figure 1. For the moment 
suppose that the random variables X1 X2 X3 are Gaussian  and the edges from X1 and X2 to X3 are
directed. As discussed in [5]  the joint probability distribution of the random variables on the graph
is then given by linear structural equation models:

X1 = γ1
X2 = γ2
X3 = ρ1X1 + ρ1X2

(cid:33)

(cid:32) 1

0
0
1
ρ1 ρ2

0
0
1

⇒ Y = Λγ 

Λ =

(cid:18) 1 + ρ2

1
ρ1ρ2

(cid:19)

ρ1ρ2
1 + ρ2
2

(3)

(4)

Then  the conditional probability distribution of X1 and X2 given X3  is Gaussian  with the inverse
covariance matrix given by

A comparison between (3) and (4) then reveals that the proposed Neumann random walk corre-
sponds to conditioning on the boundary variables  if the edges going from the set S to its boundary
are directed. The reﬂected random walk  for the original problem  therefore corresponds to ﬁrst
setting all the inﬂuences from other nodes in the graph to nodes in the set S to zero (resulting in
directed edges) and then conditioning on the boundary variables. Therefore  the proposed method
offers a compromise compared to the full model of [5]  based on local information at the level of
each subnetwork.

3.1 Group-Penalized PCR on Graph

Using the Neumann eigenvectors of subnetworks  we now deﬁne a principal component regression
on graphs  which can be used to analyze the signiﬁcance of subnetworks. Let N j denote the |S j|×
m j matrix of the m j smallest Neumann eigenfunctions for subgraph S j. Also  let X ( j) be the n×|S j|
matrix of observations for the j-th subnetwork. An m j-dimensional projection of the original data
matrix X ( j) is then given by ˜X ( j) = X ( j)Nj. Different methods can be used in order to determine
the number of eigenfunctions m j for each subnetwork. A simple procedure determines a predeﬁned
threshold for the proportion of variance explained by each eigenfunction. These proportions can be
determined by considering the reciprocal of Neumann eigenvalues (ignoring the 0-eigenvalue). To
simplify the presentation  here we assume m j = m ∀ j.

5

The signiﬁcance of subnetwork S j is a function of the combined effect of all the nodes  captured
by the transformed data matrix ˜X ( j). This can be evaluated by forming a multivariate ANOVA
(MANOVA) model. Formally  let y be the mn× 1 vector of observations obtained by stacking all
the transformed data matrices ˜X ( j). Also  let X be the mn×Jmr design matrix corresponding to the
experimental settings  where r is the number of parameters used to model experimental conditions 
and β be the vector of regression coefﬁcients. For simplicity  here we focus on the case of a two-
class inference problem (e.g.
treatment vs. control). Extensions to more general experimental
settings follow naturally and are discussed in Section 5.
To evaluate the combined effect of each subnetwork  we impose a group penalty on the coefﬁcient
of the regression of y on the design matrix X . In particular  using the group lasso penalty [18]  we
estimate the signiﬁcance of the subnetwork by solving the following optimization problem2

(cid:41)

(cid:40)
n−1(cid:107)y− J
∑

argmin

β

X ( j)β ( j)(cid:107)2

2 + γ

(cid:107)β ( j)(cid:107)2

J

∑

j=1

j=1

(5)

where J is the total number of subnetworks considered and X ( j) and β ( j) denote the columns of
X   and entries of β corresponding to the subnetwork j  respectively.
In equation (5)  γ is the tuning parameter and is usually determined by performing k-fold cross vali-
dation or evaluation on independent data sets. However  since the goal of our analysis is to determine
the signiﬁcance of subnetworks  γ should be determined so that the probability of false positives is
controlled at a given signiﬁcance level α. Here we adapt the approach in [20] and determine the
optimal value of γ so that the family-wise error rate (FWER) in repeated sampling with replacement
(bootstrap) is controlled at the level α. Speciﬁcally  let qi
γ be the total number of subnetworks con-
sidered signiﬁcant based on the value of γ in the ith bootstrap sample. Let π be the threshold for
selection of variables as signiﬁcant. In other words  if P( j)
is the probability of selecting the coef-
ﬁcients corresponding to subnetwork j in the ith bootstrap sample  the subnetwork j is considered
signiﬁcant if maxγ P( j)
The following result shows that the proposed methodology correctly selects the signiﬁcant subnet-
works  while controlling FWER at level α. We begin by introducing some additional notations and
assumptions. We assume the columns of design matrix X are normalized so that n−1Xi
TXi = 1 
Throughout this paper  we consider the case where the total number of nodes in the graph p  and the
number of design parameters r are allowed to diverge (the p (cid:29) n setting). In addition  let s be the
total number of non-zero elements in the true regression vector β .
Theorem 4. Suppose that m n ≥ 1 and there exists η ≥ 1 and t ≥ s ≥ 1 such that n−1X TXi j ≤
(7ηt)−1 for all i (cid:54)= j. Also suppose that for j (cid:54)= k  the transformed random variables ˜X ( j) and ˜X (k)

are independent. If the tuning parameter γ is selected such that such that qγ =(cid:112)(2π − 1)αrp 

i ≥ π. Using this method  we select γ such that qi

γ =(cid:112)(2π − 1)α p.3

there exists ζ = ζ (n  p) > 0 such that ζ → 0 as n → ∞ and with probability at least 1− ζ the

i

(i)
signiﬁcant subnetworks are correctly selected with high probability 
(ii)

the family-wise error rate is controlled at the level α.

γ ∼(cid:112)log p/(nm3/2)  it follows from the results in [22] that for each bootstrap sample  there exists

Outline of the Proof. First note that the MANOVA model presented above can be reformulated as a
multi-task learning problem [21]. Upon establishing the fact that for the proposed tuning parameter
ε = ε(n) > 0 such that with probability at least 1− (rp)−ε the signiﬁcant subnetworks are correctly
selected. Thus if π ≤ 1−(rp)−ε  the coefﬁcients for signiﬁcant subnetworks are included in the ﬁnal

2The problem in (5) can be solved using the R-package grplasso [19].
3Additional details for this method are given in [20]  but are excluded here due to space limitations.

6

model with hight probability. In particular  it can be shown that ζ = Φ{√

B(1− (rp)−ε − π)/2} 
where B is the number of bootstrap samples and Φ is the cumulative normal distribution. This
proves the ﬁrst claim.
Next  note that the normality assumption  and the fact that the eigenfunctions within each sub-
network are orthogonal  imply that for each j  ˜X ( j)
 i = 1  . . .  m are independent. Moreover  the
assumption of independence of ˜X ( j) and ˜X (k) for j (cid:54)= k implies that the values of y are independent
realizations of i.i.d standard normal random variables. On the other hand  the KarushKuhnTucker
conditions for the optimization problem in (5) imply that β ( j) (cid:54)= 0 iff (nm)(−1)(cid:104)(y− X β )  X ( j)(cid:105) =
sgn ( ˆβ ( j))γ  where (cid:104)x y(cid:105) denotes their inner product. It is hence clear that 1[β ( j)(cid:54)=0] are exchangeable.
Combining this with the ﬁrst part of the theorem  the claim follows from Theorem 1 of [20].

i

Remark 5. The main assumption of Theorem 4 is the independence of the variables in different sub-
networks. Although this is not satisﬁed in general problems  it may be satisﬁed by the conditioning
argument of Remark 3. It is possible to further relax this assumption using an argument similar to
Theorem 2 of [20]  but we do not pursue this here.

4 Experiments

We illustrate the performance of the proposed method using simulated data motivated by biological
applications  as well as a real data application based on gene expression analysis. In the simulation 
we generate a small network of 80 nodes (genes)  with 8 subnetworks. The random variables (ex-
pression levels of genes) are generated according to a normal distribution with mean µ. Under the
null hypothesis  µnull = 1 and the association weight ρ for all edges of the network is set to 0.2. The
setting of parameters under the alternative hypothesis are given in Table 1  where µalt = 3. These
settings are illustrated in the left panel of Figure 2. Table 1 also includes the estimated powers of
the tests for subnetworks based on 200 simulations with n = 50 observations. It can be seen that the
proposed GPCR method offers improvements over GSEA [1]  especially in case of subnetworks 3
and 6. However  it results in a less accurate inference compared to NetGSA [5].
In [5]  the pathways involved in Galactose utilization in yeast were analyzed based on the data from
[23]  and the performances of the NetGSA and GSEA methods were compared. The interactions
among genes  along with signiﬁcance of individual genes (based on single gene analysis) are given
in the right panel of Figure 2  and the results of signiﬁcance analysis based on NetGSA  GSEA
and the proposed GPCR are given in Table 2. As in the simulated example  the results of this
analysis indicate that GPCR results in improved efﬁciency over GSEA  while failing to detect the
signiﬁcance of some of the pathways detected by NetGSA.

5 Conclusion

We proposed a principal component regression method for graphs  called GPCR  using Laplacian
eigenmaps with Neumann boundary conditions. The proposed method offers a systematic approach

Table 1: Parameter settings under the alternative and estimated powers for the simulation study.

Subnet

1
2
3
4

Parameter Setting
% µalt
0.05
0.20
0.50
0.80

ρ
0.2
0.2
0.2
0.2

Estimated Powers

NetGSA

0.02
0.03
1.00
1.00

GPCR
0.08
0.21
0.65
0.81

GSEA
0.01
0.02
0.27
0.90

Subnet

5
6
7
8

Parameter Setting
% µalt
0.05
0.20
0.50
0.80

ρ
0.6
0.6
0.6
0.6

7

Estimated Powers

NetGSA

0.94
1.00
1.00
1.00

GPCR
0.41
0.61
0.99
0.99

GSEA
0.12
0.15
0.97
1.00

Figure 2: Left: Setting of the simulation parameters under the alternative hypothesis. Right: Net-
work of yeast genes involved in Galactose utilization.

for dimension reduction in networks  with a priori deﬁned subnetworks of interest. It can also incor-
porate both weighted and unweighted adjacency matrices and can be easily extended to analyzing
complex experimental conditions through the framework of linear models. This method can also be
used in longitudinal and time-course studies.
Our simulation studies  and the real data example indicate that the proposed GPCR method offers
signiﬁcant improvements over the methods of gene set enrichment analysis (GSEA). However  it
does not achieve optimal powers in comparison to NetGSA. This difference in power may be at-
tributable to the mechanism of incorporating the network information in the two methods: while
NetGSA incorporates the full network information  GPCR only account for local network informa-
tion  at the level of each subnetwork  and restricts the interactions with the rest of the network based
on the Neumann boundary condition. However  the most computationally involved step in Net-
GSA requires O(p3) operation  whereas the computational cost of GPCR is O(m3). It is clear that
since m (cid:28) p in most applications  GPCR could result in signiﬁcant improvement in terms of com-
putational time and memory requirements for analysis of high dimensional networks. In addition 
NetGSA requires that r < n  whilst the dimension reduction and the penalization of the proposed
GPCR removes the need for any such restriction and facilitates the analysis of complex experiments
in the settings with small sample sizes.

Acknowledgments

Funding for this work was provided by NIH grants 1RC1CA145444-0110 and 5R01LM010138-02.

PATHWAY
rProtein Synthesis
Glycolytic Enzymes
RNA Processing
Fatty Acid Oxidation
O2 Stress
Mating  Cell Cycle
Vesicular Transport
Amino Acid Synthesis

GSEA

Size

NetGSA

(cid:88)

(cid:88)

GPCR

NetGSA

Table 2: Signiﬁcance of pathways in Galactose utilization.
Size
28
16
75
7
13
58
19
30

PATHWAY
Sugar Transport
Glycogen Metabolism
Stress
Metal Uptake
Respiration
Gluconeogenesis
Galactose Utilization

2
12
12
4
9
7
12

(cid:88)

GPCR

GSEA

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

8

References
[1] A. Subramanian  P. Tamayo  V.K. Mootha  S. Mukherjee  B.L. Ebert  M.A. Gillette  A. Paulovich  S.L.
Pomeroy  T.R. Golub  E.S. Lander  et al. Gene set enrichment analysis: A knowledge-based approach
for interpreting genome-wide expression proﬁles. Proceedings of the National Academy of Sciences 
102(43):15545–15550  2005.

[2] B. Efron and R. Tibshirani. On testing the signiﬁcance of sets of genes. Annals of Applied Statistics 

1(1):107–129  2007.

[3] T. Ideker  O. Ozier  B. Schwikowski  and A.F. Siegel. Discovering regulatory and signalling circuits in

molecular interaction networks. Bioinformatics  18(1):S233–S240  2002.

[4] Zhi Wei and Li Hongzhe. A markov random ﬁeld model for network-based analysis of genomic data.

Bioinformatics  2007.

[5] A. Shojaie and G. Michailidis. Analysis of gene sets based on the underlying regulatory network. Journal

of Computational Biology  16(3):407–426  2009.

[6] A. Shojaie and G. Michailidis. Network enrichment analysis in complex experiments. Statisitcal Appli-

cations in Genetics and Molecular Biology  9(1)  Article 22  2010.

[7] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis

and machine intelligence  22(8):888–905  2000.

[8] M. Saerens  F. Fouss  L. Yen  and P. Dupont. The principal components analysis of a graph  and its

relationships to spectral clustering. Machine Learning: ECML 2004  pages 371–383  2004.

[9] A.Y. Ng  M.I. Jordan  and Y. Weiss. On spectral clustering: Analysis and an algorithm. Advances in

neural information processing systems  2:849–856  2002.

[10] F. Fouss  A. Pirotte  J.M. Renders  and M. Saerens. A novel way of computing dissimilarities between
nodes of a graph  with application to collaborative ﬁltering and subspace projection of the graph nodes.
In European Conference on Machine Learning Proceedings  ECML  2004.

[11] C. Li and H. Li. Variable Selection and Regression Analysis for Graph-Structured Covariates with an

Application to Genomics. Annals of Applied Statistics  in press  2010.

[12] F. Rapaport  A. Zinovyev  M. Dutreix  E. Barillot  and J.P. Vert. Classiﬁcation of microarray data using

gene networks. BMC bioinformatics  8(1):35  2007.

[13] F.R.K. Chung. Spectral graph theory. American Mathematical Society  1997.
[14] S.L. Lauritzen. Graphical models. Oxford Univ Press  1996.
[15] H. Rue and L. Held. Gaussian Markov random ﬁelds: theory and applications. Chapman & Hall  2005.
[16] J. Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical

Society. Series B (Methodological)  36(2):192–236  1974.

[17] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering.

Advances in neural information processing systems  1:585–592  2002.

[18] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of

Royal Statistical Society. Series B Statistical Methodology  68(1):49  2006.

[19] L. Meier  S. Van de Geer  and P. Buhlmann. The group lasso for logistic regression. Journal of Royal

Statistical Society. Series B Statistical Methodology  70(1):53  2008.

[20] N. Meinshausen and P. B¨uhlmann. Stability selection. Preprint  arXiv  809  2009.
[21] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Machine Learning 

73(3):243–272  2008.

[22] K. Lounici  M. Pontil  A.B. Tsybakov  and S. van de Geer. Taking Advantage of Sparsity in Multi-Task

Learning. Preprint  arXiv  903  2009.

[23] T. Ideker  V. Thorsson  J.A. Ranish  R. Christmas  J. Buhler  J.K. Eng  R. Bumgarner  D.R. Goodlett 
R. Aebersold  and L. Hood. Integrated genomic and proteomic analyses of a systematically perturbed
metabolic network. Science  292(5518):929  2001.

9

,Aijun Bai
Feng Wu
Xiaoping Chen
Hsiao-Yu Tung
Alexander Smola
Jaya Kawale
Hung Bui
Branislav Kveton
Long Tran-Thanh
Sanjay Chawla
Stephan Rabanser
Stephan Günnemann
Zachary Lipton