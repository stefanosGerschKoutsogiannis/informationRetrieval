2019,Towards closing the gap between the theory and practice of SVRG,Amongst the very first variance reduced stochastic methods for solving the empirical risk minimization problem was the SVRG method. SVRG is an inner-outer loop based method  where in the outer loop a reference full gradient is evaluated  after which $m \in \N$ steps of an inner loop are executed where the reference gradient is used to build a variance reduced estimate of the current gradient.
The simplicity of the SVRG method and its analysis have lead to multiple extensions and variants for even non-convex optimization. Yet there is a significant gap between the parameter settings that the analysis suggests and what is known to work well in practice. Our first contribution is that we take several steps towards closing this gap. In particular  the current analysis shows that $m$ should be of the order of the condition number so that the resulting method has a favorable complexity. Yet in practice $m=n$ works well regardless of the condition number  where $n$ is the number of data points. Furthermore  the current analysis shows that the inner iterates have to be reset using averaging after every outer loop. Yet in practice SVRG works best when the inner iterates are updated continuously and not reset. We provide an analysis of these aforementioned practical settings and show that they achieve the same favorable complexity as the original analysis (with slightly better constants). Our second contribution is to  provide a more general analysis than had been previously done by using arbitrary sampling  which allows us to analyze virtually all forms of mini-batching through a single theorem. Since our setup and analysis reflect what is done in practice  we are able to set the parameters such as the mini-batch size and step size using our theory in such a way that produces a more efficient algorithm in practice  as we show in extensive numerical experiments.,Towards closing the gap between the theory and

practice of SVRG

Othmane Sebbouh
LTCI  T´el´ecom Paris

Institut Polytechnique de Paris
othmane.sebbouh@gmail.com

Nidham Gazagnadou
LTCI  T´el´ecom Paris

Institut Polytechnique de Paris

nidham.gazagnadou@telecom-paris.fr

Samy Jelassi

ORFE Department
Princeton University

sjelassi@princeton.edu

Francis Bach

INRIA - Ecole Normale Sup´erieure

PSL Research University
francis.bach@inria.fr

Robert M. Gower
LTCI  T´el´ecom Paris

Institut Polytechnique de Paris

robert.gower@telecom-paris.fr

Abstract

Amongst the very ﬁrst variance reduced stochastic methods for solving the empiri-
cal risk minimization problem was the SVRG method [13]. SVRG is an inner-outer
loop based method  where in the outer loop a reference full gradient is evaluated  af-
ter which m ∈ N steps of an inner loop are executed where the reference gradient is
used to build a variance reduced estimate of the current gradient. The simplicity of
the SVRG method and its analysis have lead to multiple extensions and variants for
even non-convex optimization. Yet there is a signiﬁcant gap between the parameter
settings that the analysis suggests and what is known to work well in practice. Our
ﬁrst contribution is that we take several steps towards closing this gap. In particular 
the current analysis shows that m should be of the order of the condition number so
that the resulting method has a favorable complexity. Yet in practice m = n works
well regardless of the condition number  where n is the number of data points.
Furthermore  the current analysis shows that the inner iterates have to be reset
using averaging after every outer loop. Yet in practice SVRG works best when
the inner iterates are updated continuously and not reset. We provide an analysis
of these aforementioned practical settings and show that they achieve the same
favorable complexity as the original analysis (with slightly better constants). Our
second contribution is to provide a more general analysis than had been previously
done by using arbitrary sampling  which allows us to analyse virtually all forms
of mini-batching through a single theorem. Since our setup and analysis reﬂect
what is done in practice  we are able to set the parameters such as the mini-batch
size and step size using our theory in such a way that produces a more efﬁcient
algorithm in practice  as we show in extensive numerical experiments.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1

Introduction

Consider the problem of minimizing a µ–strongly convex and L–smooth function f where

x∗ = arg min
x∈Rd

1
n

n(cid:88)

i=1

fi(x) =: f (x) 

(1)

and each fi is convex and Li–smooth. Several training problems in machine learning ﬁt this format 
e.g. least-squares  logistic regressions and conditional random ﬁelds. Typically each fi represents a
regularized loss of an ith data point. When n is large  algorithms that rely on full passes over the
data  such as gradient descent  are no longer competitive. Instead  the stochastic version of gradient
descent SGD [26] is often used since it requires only a mini-batch of data to make progress towards
the solution. However  SGD suffers from high variance  which keeps the algorithm from converging
unless a carefully often hand-tuned decreasing sequence of step sizes is chosen. This often results in
a cumbersome parameter tuning and a slow convergence.
To address this issue  many variance reduced methods have been designed in recent years including
SAG [27]  SAGA [6] and SDCA [28] that require only a constant step size to achieve linear conver-
gence. In this paper  we are interested in variance reduced methods with an inner-outer loop structure 
such as S2GD [14]  SARAH [21]  L-SVRG [16] and the orignal SVRG [13] algorithm. Here we
present not only a more general analysis that allows for any mini-batching strategy  but also a more
practical analysis  by analysing methods that are based on what works in practice  and thus providing
an analysis that can inform practice.

2 Background and Contributions

Convergence under arbitrary samplings.
We give the ﬁrst arbitrary sampling conver-
gence results for SVRG type methods in
the convex setting1. That is our analysis in-
cludes all forms of sampling including mini-
batching and importance sampling as a spe-
cial case. To better understand the signiﬁ-
cance of this result  we use mini-batching b
elements without replacement as a running
example throughout the paper. With this sam-
pling the update step of SVRG  starting from
x0 = w0 ∈ Rd  takes the form of
1
b

xt+1 = xt − α

(cid:88)

(cid:32)

Figure 1: Left: the total complexity (3) for random
Gaussian data  right: the step size (4) as b increases.

(cid:33)

(cid:88)

∇fi(xt) − 1
b

∇fi(ws−1) + ∇f (ws−1)

 

(2)

i∈B
where α > 0 is the step size  B ⊆ [n]
= {1  . . .   n} and b = |B|. Here ws−1 is the reference point
which is updated after m ∈ N steps  the xt’s are the inner iterates and m is the loop length. As
a special case of our forthcoming analysis in Corollary 4.1  we show that the total complexity of
the SVRG method based on (2) to reach an  > 0 accurate solution has a simple expression which
depends on n  m  b  µ  L and Lmax

i∈B

def

(cid:16) n

m

(cid:17)

(cid:26) 3

def
= maxi∈[n] Li:
n − b
n − 1

max

b

Cm(b)

def
= 2

+ 2b

Lmax

µ

+

b − 1
n − 1

n
b

L
µ

(cid:27)

(cid:18) 1

(cid:19)



  m

log

 

(3)

so long as the step size is

α =

(4)
By total complexity we mean the total number of individual ∇fi gradients evaluated. This shows that
the total complexity is a simple function of the loop length m and the mini-batch size b. See Figure 1
for an example for how total complexity evolves as we increase the mini-batch size.

3(n − b)Lmax + n(b − 1)L

.

1
2

b(n − 1)

1SVRG has very recently been analysed under arbitrary samplings in the non-convex setting [12].

2

246810mini-batch size150160170180190200210total complexity1n=53500 mini-batch size3.10−61.10−32.10−33.10−3step sizeOptimal mini-batch and step sizes for SVRG. The size of the mini-batch b is often left as a
parameter for the user to choose or set using a rule of thumb. The current analysis in the literature for
mini-batching shows that when increasing the mini-batch size b  while the iteration complexity can
decrease2  the total complexity increases or is invariant. See for instance results in the non-convex
case [22  25]  and for the convex case [10]  [15]  [1] and ﬁnally [18] where one can ﬁnd the iteration
complexity of several variants of SVRG with mini-batching. However  in practice  mini-batching
can often lead to faster algorithms. In contrast our total complexity (3) clearly highlights that when
increasing the mini batch size  the total complexity can decrease and the step size increases  as can
be seen in our plot of (3) and (4) in Figure 1. Furthermore Cm(b) is a convex function in b which
allows us to determine the optimal mini-batch a priori. For m = n – a widely used loop length
in practice – the optimal mini-batch size  depending on the problem setting  is given in Table 1.
Moreover  we can also determine the optimal loop length. The reason we were able to achieve these

n ≤ L

Lmax ≥ nL

3

n

µ
Lmax < nL
3

(cid:106)ˆb
(cid:107)

Lmax ≥ nL

3

(cid:106)˜b
(cid:107)

L

µ < n < 3Lmax

µ
Lmax < nL
3

(cid:106)
min{ˆb  ˜b}(cid:107)

n ≥ 3Lmax

µ

1

Table 1: Optimal mini-batch sizes for Algorithm 1 with m = n. The last line presents the optimal
mini-batch sizes depending on all the possible problem settings  which are presented in the ﬁrst two

lines. Notations: ˆb =

3Lmax−L
nL−3Lmax

  ˜b =

(3Lmax−L)n

n(n−1)µ−nL+3Lmax

.

(cid:113) n

2

new tighter mini-batch complexity bounds was by using the recently introduced concept of expected
smoothness [9] alongside a new constant we introduce in this paper called the expected residual
constant. The expected smoothness and residual constants  which we present later in Lemmas 4.1
and 4.2  show how mini-batching (and arbitrary sampling in general) combined with the smoothness
of our data can determine how smooth in expectation our resulting mini-batched functions are. The
expected smoothness constant has been instrumental in providing a tight mini-batch analysis for
SGD [8]  SAGA [7] and now SVRG.

New practical variants. We took special care so that our analysis allows for practical parameter
settings. In particular  often the loop length is set to m = n or m = n/b in the case of mini-batching3.
And yet  the classical SVRG analysis given in [13] requires m ≥ 20Lmax/µ in order to ensure
a resulting iteration complexity of O((n + Lmax/µ) log(1/)). Furthermore  the standard SVRG
analysis relies on averaging the xt inner iterates after every m iterations of (2)  yet this too is not
what works well in practice4. To remedy this  we propose Free-SVRG  a variant of SVRG where
the inner iterates are not averaged at any point. Furthermore  by developing a new Lyapunov style
convergence for Free-SVRG  our analysis holds for any choice of m  and in particular  for m = n we
show that the resulting complexity is also given by O((n + Lmax/µ) log(1/)).
The only downside of Free-SVRG is that the reference point is set using a weighted averaging based on
the strong convexity parameter. To ﬁx this issue  [11]  and later [16  17]  proposed a loopless version
of SVRG. This loopless variant has no explicit inner-loop structure  it instead updates the reference
point based on a coin toss and lastly requires no knowledge of the strong convexity parameter and no
averaging whatsoever. We introduce L-SVRG-D  an improved variant of Loopless-SVRG that takes
much larger step sizes after the reference point is reset  and gradually smaller step sizes thereafter.

2Note that the total complexity is equal to the iteration complexity times the mini-batch size b.
3See for example the lightning package from scikit-learn [23]: http://contrib.scikit-learn.org/lightning/

and [21] for examples where m = n. See [2] for an example where m = 5n/b.

4Perhaps an exception to the above issues in the literature is the Katyusha method and its analysis [1]  which
is an accelerated variant of SVRG. In [1] the author shows that using a loop length m = 2n and by not averaging

the inner iterates  the Katyusha method achieves the accelerated complexity of O((n+(cid:112)(nLmax)/µ) log(1/)).

Though a remarkable advance in the theory of accelerated methods  the analysis in [1] does not hold for the
unaccelerated case. This is important since  contrary to the name  the accelerated variants of stochastic methods
are not always faster than their non-accelerated counterparts. Indeed  acceleration only helps in the stochastic
setting when Lmax/µ ≥ n  in other words for problems that are sufﬁciently ill-conditioned.

3

Indeed these are unbiased estimators since

n(cid:88)

i=1

fv(x)

def
=

1
n

n(cid:88)

i=1

n(cid:88)

i=1

1
n

n(cid:88)

i=1

(7)

(8)

We provide an complexity analysis of L-SVRG-D that allows for arbitrary sampling and achieves the
same complexity as Free-SVRG  albeit at the cost of introducing more variance into the procedure
due to the coin toss.

3 Assumptions and Sampling

We collect all of the assumptions we use in the following.
Assumption 3.1. There exist L ≥ 0 and µ ≥ 0 such that for all x  y ∈ Rd 
(cid:107)x − y(cid:107)2
2  
(cid:107)x − y(cid:107)2
2 .

f (x) ≤ f (y) + (cid:104)∇f (y)  x − y(cid:105) +
L
2
f (x) ≤ f (y) + (cid:104)∇f (x)  x − y(cid:105) − µ
2

(6)
We say that f is L–smooth (5) and µ–strongly convex (6). Moreover  for all i ∈ [n]  fi is convex and
there exists Li ≥ 0 such that fi is Li–smooth.
So that we can analyse all forms of mini-batching simultaneously through arbitrary sampling we
make use of a sampling vector.
Deﬁnition 3.1 (The sampling vector). We say that the random vector v = [v1  . . .   vn] ∈ Rn with
distribution D is a sampling vector if ED [vi] = 1 for all i ∈ [n].
With a sampling vector we can compute an unbiased estimate of f (x) and ∇f (x) via

(5)

vifi(x)

and ∇fv(w)

def
=

vi∇fi(x).

ED [fv(x)] =

1
n

ED [vi] fi(x) =

1
n

fi(x) = f (x).

Likewise we can show that ED [∇fv(x)] = ∇f (x). Computing ∇fv is cheaper than computing the
full gradient ∇f whenever v is a sparse vector. In particular  this is the case when the support of v is
based on a mini-batch sampling.
Deﬁnition 3.2 (Sampling). A sampling S ⊆ [n] is any random set-valued map which is uniquely
= P(S = B) for all B ⊆ [n]. A sampling S

deﬁned by the probabilities(cid:80)

B⊆[n] pB = 1 where pB
is called proper if for every i ∈ [n]  we have that pi
def

def

= P [i ∈ S] = (cid:80)

pC > 0.

C:i∈C

We can build a sampling vector using sampling as follows.
Lemma 3.1 (Sampling vector). Let S be a proper sampling. Let pi
Diag (p1  . . .   pn). Let v = v(S) be a random vector deﬁned by
= P−1eS.

v(S) = P−1(cid:88)

ei

def

i∈S

def

= P [i ∈ S] and P

def
=

(9)

It follows that v is a sampling vector.
Proof. The i-th coordinate of v(S) is vi(S) = 1(i ∈ S)/pi and thus

E [vi(S)] =

E [1(i ∈ S)]

P [i ∈ S]

pi

=

pi

= 1.

Our forthcoming analysis holds for all samplings. However  we will pay particular attention to b-nice
sampling  otherwise known as mini-batching without replacement  since it is often used in practice.
Deﬁnition 3.3 (b-nice sampling). S is a b-nice sampling if it is sampling such that

P [S = B] =

1(cid:0)n
(cid:1)  

b

∀B ⊆ [n] : |B| = b.

4

(cid:80)

(cid:80)

n for all i ∈ [n] and
i∈S ei according to Lemma 3.1. The resulting subsampled function is

To construct such a sampling vector based on the b–nice sampling  note that pi = b
thus we have that v(S) = n
b
then fv(x) = 1|S|
Using arbitrary sampling also allows us to consider non-uniform samplings  and for completeness 
we present this sampling and several others in Appendix D.

i∈S fi(x)  which is simply the mini-batch average over S.

4 Free-SVRG: freeing up the inner loop size

Similarly to SVRG  Free-SVRG is an inner-outer loop variance reduced algorithm. It differs from the
original SVRG [13] on two major points: how the reference point is reset and how the ﬁrst iterate of
the inner loop is deﬁned  see Algorithm 15.
First  in SVRG  the reference point is the average of the iterates of the inner loop. Thus  old iterates
and recent iterates have equal weights in the average. This is counterintuitive as one would expect
that to reduce the variance of the gradient estimate used in (2)  one needs a reference point which is
closer to the more recent iterates. This is why  inspired by [20]  we use the weighted averaging in
Free-SVRG given in (10)  which gives more importance to recent iterates compared to old ones.
Second  in SVRG  the ﬁrst iterate of the inner loop is reset to the reference point. Thus  the inner
iterates of the algorithm are not updated using a one step recurrence. In contrast  Free-SVRG deﬁnes
the ﬁrst iterate of the inner loop as the last iterate of the previous inner loop  as is also done in practice.
These changes and a new Lyapunov function analysis are what allows us to freely choose the size of
the inner loop6. To declutter the notation  we deﬁne for a given step size α > 0:

Sm

def
=

(1 − αµ)m−1−i

and pt

def
=

(1 − αµ)m−1−t

Sm

for t = 0  . . .   m − 1.

 

(10)

m−1(cid:88)

i=0

Algorithm 1 Free-SVRG

Parameters inner-loop length m  step size α  a sampling vector v ∼ D  and pt deﬁned in (10)
Initialization w0 = xm
for s = 1  2  . . .   S do

0 ∈ Rd

s = xm
x0
for t = 0  1  . . .   m − 1 do

s−1

Sample vt ∼ D
s = ∇fvt(xt
gt
s − αgt
s = xt
xt+1

ws =(cid:80)m−1

t=0 ptxt
s

s

return xm
S

s) − ∇fvt(ws−1) + ∇f (ws−1)

4.1 Convergence analysis

Our analysis relies on two important constants called the expected smoothness constant and the
expected residual constant. Their existence is a result of the smoothness of the function f and that of
the individual functions fi  i ∈ [n].
Lemma 4.1 (Expected smoothness  Theorem 3.6 in [8]). Let v ∼ D be a sampling vector and assume
that Assumption 3.1 holds. There exists L ≥ 0 such that for all x ∈ Rd 

(11)
Lemma 4.2 (Expected residual). Let v ∼ D be a sampling vector and assume that Assumption 3.1
holds. There exists ρ ≥ 0 such that for all x ∈ Rd 

(cid:104)(cid:107)∇fv(x) − ∇fv(x∗)(cid:107)2
(cid:104)(cid:107)∇fv(x) − ∇fv(x∗) − ∇f (x)(cid:107)2

(cid:105) ≤ 2L (f (x) − f (x∗)) .
(cid:105) ≤ 2ρ (f (x) − f (x∗)) .

(12)
5After submitting this work  it has come to our attention that Free-SVRG is a special case of k-SVRG [24]

ED

ED

2

2

when k = 1.

6Hence the name of our method Free-SVRG.

5

For completeness  the proof of Lemma 4.1 is given in Lemma E.1 in the supplementary material. The
proof of Lemma 4.2 is also given in the supplementary material  in Lemma F.1. Indeed  all proofs are
deferred to the supplementary material.
Though Lemma 4.1 establishes the existence of the expected smoothness L  it was only very recently
that a tight estimate of L was conjectured in [7] and proven in [8]. In particular  for our working
example of b–nice sampling  we have that the constants L and ρ have simple closed formulae that
depend on b.
Lemma 4.3 (L and ρ for b-nice sampling). Let v be a sampling vector based on the b–nice sampling.
It follows that.

L = L(b)

ρ = ρ(b)

def
=

def
=

1
b
1
b

n − b
n − 1
n − b
n − 1

Lmax +

n
b

Lmax.

b − 1
n − 1

L 

(13)

(14)

The reason that the expected smoothness and expected residual constants are so useful in obtaining
a tight mini-batch analysis is because  as the mini-batch size b goes from n to 1  L(b) (resp. ρ(b))
gracefully interpolates between the smoothness of the full function L(n) = L (resp. ρ(n) = 0)  and
the smoothness of the individual fi functions L(1) = Lmax (resp ρ(1) = Lmax). Also  we can bound
the second moment of a variance reduced gradient estimate using L and ρ as follows.
Lemma 4.4. Let Assumption 3.1 hold. Let x  w ∈ Rd and v ∼ D be sampling vector. Consider
g(x  w)

= ∇fv(x) − ∇fv(w) + ∇f (w). As a consequence of (11) and (12) we have that

(cid:3) ≤ 4L(f (x) − f (x∗)) + 4ρ(f (w) − f (x∗)).

ED(cid:2)(cid:107)g(x  w)(cid:107)2

(15)

def

2

Next we present a new Lyapunov style convergence analysis through which we will establish the
convergence of the iterates and the function values simultaneously.
Theorem 4.1. Consider the setting of Algorithm 1 and the following Lyapunov function

If Assumption 3.1 holds and if α ≤

def

= (cid:107)xm

s − x∗(cid:107)2

φs

def

= 8α2ρSm(f (ws) − f (x∗)).

2 + ψs where ψs
2(L+2ρ)   then

E [φs] ≤ βsφ0  where β = max(cid:8)(1 − αµ)m  1

1

(cid:9) .

2

(16)

(17)

4.2 Total complexity for b–nice sampling

To gain better insight into the convergence rate stated in Theorem 4.1  we present the total complexity
of Algorithm 1 when v is deﬁned via the b–nice sampling introduced in Deﬁnition 3.3.
Corollary 4.1. Consider the setting of Algorithm 1 and suppose that we use b–nice sampling. Let
2(L(b)+2ρ(b))   where L(b) and ρ(b) are given in (13) and (14). We have that the total complexity
α =
of ﬁnding an  > 0 approximate solution that satisﬁes E

1

(cid:104)(cid:107)xm
(cid:26)L(b) + 2ρ(b)

(cid:27)
s − x∗(cid:107)2

2

(cid:105) ≤  φ0 is
(cid:18) 1
(cid:19)

(cid:17)

Cm(b)

def
= 2

+ 2b

max

(cid:16) n

m

µ

  m

log

.



(18)

Now (3) results from plugging (13) and (14) into (18). As an immediate sanity check  we check the
two extremes b = n and b = 1. When b = n  we would expect to recover the iteration complexity of
gradient descent  as we do in the next corollary7.
Corollary 4.2. Consider the setting of Corollary 4.1 with b = n and m = 1  thus α =
2(L(n)+2ρ(n)) = 1

2L . Hence  the resulting total complexity (18) is given by C1(n) = 6n L

µ log(cid:0) 1

(cid:1) .

1



In practice  the most common setting is choosing b = 1 and the size of the inner loop m = n. Here
we recover a complexity that is common to other non-accelerated algorithms [27]  [6]  [14]  and for a
range of values of m including m = n.

7Though the resulting complexity is 6 times the tightest gradient descent complexity  it is of the same order.

6

Corollary 4.3. Consider the setting of Corollary 4.1 with b = 1 and thus α =
Hence the resulting total complexity (18) is given by Cm(1) = 18

n + Lmax

µ

(cid:16)

(cid:17)

2(L(1)+2ρ(1)) = 1

1

log(cid:0) 1



(cid:1)   so long as

6Lmax

.

(cid:105)

m ∈(cid:104)

min(n  Lmax

µ )  max(n  Lmax
µ )

.

Thus total complexity is essentially invariant for m = n  m = Lmax/µ and everything in between.

5 L-SVRG-D: a decreasing step size approach

Although Free-SVRG solves multiple issues regarding the construction and analysis of SVRG  it still
suffers from an important issue: it requires the knowledge of the strong convexity constant  as is the
case for the original SVRG algorithm [13]. One can of course use an explicit small regularization
parameter as a proxy  but this can result in a slower algorithm.
A loopless variant of SVRG was proposed and analysed in [11  16  17]. At each iteration  their
method makes a coin toss. With (a low) probability p  typically 1/n  the reference point is reset to
the previous iterate  and with probability 1 − p  the reference point remains the same. This method
does not require knowledge of the strong convexity constant.
Our method  L-SVRG-D  uses the same loopless structure as in [11  16  17] but introduces different
step sizes at each iteration  see Algorithm 2. We initialize the step size to a ﬁxed value α > 0. At
each iteration we toss a coin  and if it lands heads (with probability 1 − p) the step size decreases by
1 − p. If it lands tails (with probability p) the reference point is reset to the most recent
a factor
iterate and the step size is reset to its initial value α.
This allows us to take larger steps than L-SVRG when we update the reference point  i.e.  when
the variance of the unbiased estimate of the gradient is low  and smaller steps when this variance
increases.

√

Algorithm 2 L-SVRG-D

Parameters step size α  p ∈ (0  1]  and a sampling vector v ∼ D
Initialization w0 = x0 ∈ Rd  α0 = α
for k = 1  2  . . .   K − 1 do

Sample vk ∼ D
gk = ∇fvk (xk) − ∇fvk (wk) + ∇f (wk)
xk+1 = xk − αkgk
(wk+1  αk+1) =

(cid:26) (xk  α)

√

with probability p

1 − p αk) with probability 1 − p

(wk 

return xK

=(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2

φk def

Theorem 5.1. Consider the iterates of Algorithm 2 and the following Lyapunov function

2 + ψk where ψk def
=

kL
8α2
p(3 − 2p)

(cid:0)f (wk) − f (x∗)(cid:1)  

∀k ∈ N.

(19)

If Assumption 3.1 holds and

α ≤ 1

2ζpL   where

def
=

ζp

then

E(cid:2)φk(cid:3) ≤ βkφ0  where β = max

(cid:26)

(7 − 4p)(1 − (1 − p) 3
2 )
p(2 − p)(3 − 2p)

1 − 2
3

αµ  1 − p
2

 

(cid:27)

.

(20)

(21)

Remark 5.1. To get a sense of the formula of the step size given in (20)  it is easy to show that ζp is
an increasing function of p such that 7/4 ≤ ζp ≤ 3. Since typically p ≈ 0  we often take a step
which is approximately α ≤ 2/(7L).

7

Corollary 5.1. Consider the setting of Algorithm 2 and suppose that we use b–nice sampling. Let
2ζpL(b) . We have that the total complexity of ﬁnding an  > 0 approximate solution that satisﬁes
α = 1

(cid:104)(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2

2

(cid:105) ≤  φ0 is

E

Cp(b)

def
= 2(2b + pn) max

log

.

(22)

(cid:26) 3ζp

2

L(b)
µ

 

1
p

(cid:27)

(cid:18) 1

(cid:19)



6 Optimal parameter settings: loop  mini-batch and step sizes

In this section  we restrict our analysis to b–nice sampling. First  we determine the optimal loop
size for Algorithm 1. Then  we examine the optimal mini-batch and step sizes for particular choices
of the inner loop size m for Algorithm 1 and of the probability p of updating the reference point
in Algorithm 2  that play analogous roles. Note that the steps used in our algorithms depend on
b through the expected smoothness constant L(b) and the expected residual constant ρ(b). Hence 
optimizing the total complexity in the mini-batch size also determines the optimal step size.
Examining the total complexities of Algorithms 1 and 2  given in (18) and (22)  we can see that 
when setting p = 1/m in Algorithm 2  these complexities only differ by constants. Thus  to avoid
redundancy  we present the optimal mini-batch sizes for Algorithm 2 in Appendix C and we only
consider here the complexity of Algorithm 1 given in (18).

6.1 Optimal loop size for Algorithm 1
Here we determine the optimal value of m for a ﬁxed batch size b  denoted by m∗(b)  which minimizes
the total complexity (18).
Proposition 6.1. The loop size that minimizes (18) and the resulting total complexity is given by

m∗(b) =

L(b) + 2ρ(b)

µ

and Cm∗ (b) = 2

n + 2b

L(b) + 2ρ(b)

µ

(cid:18)

(cid:19)

(cid:18) 1

(cid:19)



log

.

(23)

For example when b = 1  we have that m∗(1) = 3Lmax/µ and Cm∗ (1) = O((n+Lmax/µ) log(1/)) 
which is the same complexity as achieved by the range of m values given in Corollary 4.3. Thus 
as we also observed in Corollary 4.3  the total complexity is not very sensitive to the choice of m 
and m = n is a perfectly safe choice as it achieves the same complexity as m∗. We also conﬁrm this
numerically with a series of experiments in Section G.2.2.

6.2 Optimal mini-batch and step sizes

In the following proposition  we determine the optimal mini-batch and step sizes for two practical
choices of the size of the loop m.
Proposition 6.2. Let b∗ def
Cm(b)  where Cm(b) is deﬁned in (18). For the widely used
choice m = n  we have that b∗ is given by Table 1. For another widely used choice m = n/b  which
allows to make a full pass over the data set during each inner loop  we have

= arg min

b∈[n]

µ

if n > 3Lmax
L < n ≤ 3Lmax
if 3Lmax
otherwise  if n ≤ 3Lmax

µ

L

  where ¯b

def
=

n(n − 1)µ − 3n(Lmax − L)

3(nL − Lmax)

.

(24)



(cid:4)¯b(cid:5)

1
n

b∗ =

Previously  theory showed that the total complexity would increase as the mini-batch size increases 
and thus established that single-element sampling was optimal. However  notice that for m = n and
m = n/b  the usual choices for m in practice  the optimal mini-batch size is different than 1 for a
range of problem settings. Since our algorithms are closer to the SVRG variants used in practice  we
argue that our results explain why practitioners experiment that mini-batching works  as we verify in
the next section.

8

7 Experiments

We performed a series of experiments on data sets from LIBSVM [5] and the UCI repository [3]  to
validate our theoretical ﬁndings. We tested l2–regularized logistic regression on ijcnn1 and real-sim 
and ridge regression on slice and YearPredictionMSD. We used two choices for the regularizer:
λ = 10−1 and λ = 10−3. All of our code is implemented in Julia 1.0. Due to lack of space  most
ﬁgures have been relegated to Section G in the supplementary material.

Figure 2: Comparison of theoretical variants of SVRG without mini-batching (b = 1) on the ijcnn1

data set.

Figure 3: Optimality of our mini-batch size b∗ given in Table 1 for Free-SVRG on the slice data set.

Practical theory. Our ﬁrst round of experiments aimed at verifying if our theory does result in
efﬁcient algorithms. Indeed  we found that Free-SVRG and L-SVRG-D with the parameter setting
given by our theory are often faster than SVRG with settings suggested by the theory in [13]  that is
m = 20Lmax/µ and α = 1/10Lmax. See Figure 2  and Section G.1 for more experiments comparing
different theoretical parameter settings.

Optimal mini-batch size. We also conﬁrmed numerically that when using Free-SVRG with m = n 
the optimal mini-batch size b∗ derived in Table 1 was highly competitive as compared to the range
of mini-batch sizes b ∈ {1  100 
n  n}. See Figure 3 and several more such experiments in
Section G.2.1. We also explore the optimality of our m∗ in more experiments in Section G.2.2.

√

9

SVRG (b=1 m=20Lmax/μ)Free-SVRG (b=1 m=n)L-SVRG-D (b=1 p=1/n)05101520epochs10−610−510−410−310−210−1100residual05001000150020002500time10−610−510−410−310−210−1100residual0255075100epochs10−410−310−210−1100residualMini-batch size bb=1 α*(b)=3.03e−06b=100 α*(b)=2.94e−04b=√n=231 α*(b)=6.54e−04b=n=53500 α*(b)=9.39e−03b=b*(n)=31 α*(b)=9.31e−05025005000750010000time10−410−310−210−1100residualMini-batch size bb=1 α*(b)=3.03e−06b=100 α*(b)=2.94e−04b=√n=231 α*(b)=6.54e−04b=n=53500 α*(b)=9.39e−03b=b*(n)=31 α*(b)=9.31e−05Acknowledgments

RMG acknowledges the support by grants from DIM Math Innov R´egion Ile-de-France (ED574 -
FMJH)  reference ANR-11-LABX-0056-LMH  LabEx LMH.

References
[1] Z. Allen-Zhu. “Katyusha: The First Direct Acceleration of Stochastic Gradient Methods”. In:
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing. STOC
2017. 2017  pp. 1200–1205.

[2] Z. Allen-Zhu and E. Hazan. “Variance Reduction for Faster Non-Convex Optimization”.
In: Proceedings of The 33rd International Conference on Machine Learning. Vol. 48. 2016 
pp. 699–707.

[3] A. Asuncion and D. Newman. UCI machine learning repository. 2007.
[4] S. Bubeck et al. “Convex optimization: Algorithms and complexity”. In: Foundations and

Trends R(cid:13) in Machine Learning 8.3-4 (2015)  pp. 231–357.

[5] C.-C. Chang and C.-J. Lin. “LIBSVM: A library for support vector machines”. In: ACM

transactions on intelligent systems and technology (TIST) 2.3 (2011)  p. 27.

[6] A. Defazio  F. Bach  and S. Lacoste-Julien. “SAGA: A Fast Incremental Gradient Method With
Support for Non-Strongly Convex Composite Objectives”. In: Advances in Neural Information
Processing Systems 27. 2014  pp. 1646–1654.

[7] N. Gazagnadou  R. M. Gower  and J. Salmon. “Optimal mini-batch and step sizes for SAGA”.

In: The International Conference on Machine Learning (2019).

[8] R. M. Gower  N. Loizou  X. Qian  A. Sailanbayev  E. Shulgin  and P. Richt´arik. “SGD: general

analysis and improved rates”. In: ().

[9] R. M. Gower  P. Richt´arik  and F. Bach. “Stochastic Quasi-Gradient methods: Variance Reduc-

tion via Jacobian Sketching”. In: arXiv:1805.02632 (2018).

[10] R. Harikandeh  M. O. Ahmed  A. Virani  M. Schmidt  J. Koneˇcn´y  and S. Sallinen. “StopWast-
ing My Gradients: Practical SVRG”. In: Advances in Neural Information Processing Systems
28. 2015  pp. 2251–2259.

[11] T. Hofmann  A. Lucchi  S. Lacoste-Julien  and B. McWilliams. “Variance Reduced Stochastic
Gradient Descent with Neighbors”. In: Advances in Neural Information Processing Systems
28. 2015  pp. 2305–2313.

[12] S. Horv´ath and P. Richt´arik. “Nonconvex Variance Reduced Optimization with Arbitrary

Sampling”. In: ().

[13] R. Johnson and T. Zhang. “Accelerating stochastic gradient descent using predictive variance

[14]

[15]

reduction”. In: Advances in Neural Information Processing Systems. 2013  pp. 315–323.
J. Koneˇcn´y and P. Richt´arik. “Semi-stochastic gradient descent methods”. In: Frontiers in
Applied Mathematics and Statistics 3 (2017)  p. 9.
J. Koneˇcn´y  J. Liu  P. Richt´arik  and M. Tak´aˇc. “Mini-Batch Semi-Stochastic Gradient Descent
in the Proximal Setting”. In: IEEE Journal of Selected Topics in Signal Processing 2 (2016) 
pp. 242–255.

[16] D. Kovalev  S. Horvath  and P. Richt´arik. “Don’t Jump Through Hoops and Remove Those
Loops: SVRG and Katyusha are Better Without the Outer Loop”. In: arXiv:1901.08689 (2019).
[17] A. Kulunchakov and J. Mairal. “Estimate Sequences for Variance-Reduced Stochastic Compos-
ite Optimization”. In: Proceedings of the 36th International Conference on Machine Learning.
Vol. 97. 2019  pp. 3541–3550.

[18] T. Murata and T. Suzuki. “Doubly Accelerated Stochastic Variance Reduced Dual Averaging
Method for Regularized Empirical Risk Minimization”. In: Proceedings of the 31st Inter-
national Conference on Neural Information Processing Systems. NIPS’17. 2017  pp. 608–
617.

[19] Y. Nesterov. Introductory lectures on convex optimization: A basic course. Vol. 87. 2013.

10

[20] Y. Nesterov and J.-P. Vial. “Conﬁdence level solutions for stochastic programming”. In:

Automatica. Vol. 44. 2008  pp. 1559–1568.

[21] L. M. Nguyen  J. Liu  K. Scheinberg  and M. Tak´aˇc. “SARAH: A Novel Method for Machine
Learning Problems Using Stochastic Recursive Gradient”. In: Proceedings of the 34th Interna-
tional Conference on Machine Learning. Vol. 70. Proceedings of Machine Learning Research.
2017  pp. 2613–2621.

[22] A. Nitanda. “Stochastic Proximal Gradient Descent with Acceleration Techniques”. In: Ad-

vances in Neural Information Processing Systems 27. 2014  pp. 1574–1582.

[23] F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel  P.
Prettenhofer  R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher 
M. Perrot  and E. Duchesnay. “Scikit-learn: Machine Learning in Python”. In: Journal of
Machine Learning Research 12 (2011)  pp. 2825–2830.

[24] A. Raj and S. U. Stich. “k-SVRG: Variance Reduction for Large Scale Optimization”. In:

arXiv:1805.00982 ().

[25] S. J. Reddi  A. Hefny  S. Sra  B. P´oczos  and A. J. Smola. “Stochastic Variance Reduction for
Nonconvex Optimization.” In: Proceedings of the 34th International Conference on Machine
Learning. Vol. 48. 2016  pp. 314–323.

[26] H. Robbins and D. Siegmund. “A convergence theorem for non negative almost supermartin-

gales and some applications”. In: Herbert Robbins Selected Papers. 1985  pp. 111–135.

[27] N. L. Roux  M. Schmidt  and F. R. Bach. “A Stochastic Gradient Method with an Exponential
Convergence Rate for Finite Training Sets”. In: Advances in Neural Information Processing
Systems 25. 2012  pp. 2663–2671.

[28] S. Shalev-Shwartz and T. Zhang. “Stochastic dual coordinate ascent methods for regularized
loss minimization”. In: Journal of Machine Learning Research 14.Feb (2013)  pp. 567–599.

11

,Othmane Sebbouh
Nidham Gazagnadou
Samy Jelassi
Francis Bach
Robert Gower