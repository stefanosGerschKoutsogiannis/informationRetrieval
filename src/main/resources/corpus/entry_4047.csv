2017,Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity Auctions,We study the online learning problem of a bidder who participates in repeated auctions. With the goal of maximizing his T-period payoff  the bidder determines the optimal allocation of his budget among his bids for $K$ goods at each period. As a bidding strategy  we propose a polynomial-time algorithm  inspired by the dynamic programming approach to the knapsack problem. The proposed algorithm  referred to as dynamic programming on discrete set (DPDS)  achieves a regret order of $O(\sqrt{T\log{T}})$. By showing that the regret is lower bounded by $\Omega(\sqrt{T})$ for any strategy  we conclude that DPDS is order optimal up to a $\sqrt{\log{T}}$ term. We evaluate the performance of DPDS empirically in the context of virtual trading in wholesale electricity markets by using historical data from the New York market. Empirical results show that DPDS consistently outperforms benchmark heuristic methods that are derived from machine learning and online learning approaches.,Online Learning of Optimal Bidding Strategy

in Repeated Multi-Commodity Auctions

Sevi Baltaoglu
Cornell University
Ithaca  NY 14850

msb372@cornell.edu

Lang Tong

Cornell University
Ithaca  NY 14850

Qing Zhao

Cornell University
Ithaca  NY 14850

lt35@cornell.edu

qz16@cornell.edu

Abstract

We study the online learning problem of a bidder who participates in repeated
auctions. With the goal of maximizing his T-period payoff  the bidder determines
the optimal allocation of his budget among his bids for K goods at each period.
As a bidding strategy  we propose a polynomial-time algorithm  inspired by the
dynamic programming approach to the knapsack problem. The proposed algorithm 
√
referred to as dynamic programming on discrete set (DPDS)  achieves a regret
T ) for
order of O(
any strategy  we conclude that DPDS is order optimal up to a
log T term. We
evaluate the performance of DPDS empirically in the context of virtual trading in
wholesale electricity markets by using historical data from the New York market.
Empirical results show that DPDS consistently outperforms benchmark heuristic
methods that are derived from machine learning and online learning approaches.

√
T log T ). By showing that the regret is lower bounded by Ω(

√

1

Introduction

We consider the problem of optimal bidding in a multi-commodity uniform-price auction (UPA) [1] 
which promotes the law of one price for identical goods. UPA is widely used in practice. Examples
include spectrum auction  the auction of treasury notes  the auction of emission permits (UK)  and
virtual trading in the wholesale electricity market  which we discuss in detail in Sec. 1.1.
A mathematical abstraction of multi-commodity UPA is as follows. A bidder has K goods to bid on
at an auction. With the objective to maximize his T-period expected proﬁt  at each period  the bidder
determines how much to bid for each good subject to a budget constraint.
In the bidding period t  if a bid xt k for good k is greater than or equal to its auction clearing price
λt k  then the bid is cleared  and the bidder pays λt k. His revenue resulting from the cleared bid
will be the good’s spot price (utility) πt k. In particular  the payoff obtained from good k at period
t is (πt k − λt k)1{xt k ≥ λt k} where 1{xt k ≥ λt k} indicates whether the bid is cleared. Let
(cid:124) be the vector of auction clearing and spot market
λt = [λt 1  ...  λt K]
(cid:124) be the vector of bids for period
prices at period t  respectively. Similarly  let xt = [xt 1  ...  xt K]
t. We assume that (πt  λt) are drawn from an unknown joint distribution and  in our analysis 
independent and identically distributed (i.i.d.) over time.1
At the end of each period  the bidder observes the auction clearing and spot prices of all goods.
Therefore  before choosing the bid of period t  all the information the bidder has is a vector It−1
containing his observation and decision history {xi  λi  πi}t−1
i=1. Consequently  a bidding policy µ of
a bidder is deﬁned as a sequence of decision rules  i.e.  µ = (µ0  µ1...  µT−1)  such that  at time t− 1 
1This implies that the auction clearing price is independent of bid xt  which is a reasonable assumption for

(cid:124) and πt = [πt 1  ...  πt K]

any market where an individual’s bid has negligible impact on the market price.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

µt−1 maps the information history It−1 to the bid xt of period t. The performance of any bidding
policy µ is measured by its regret  which is deﬁned by the difference between the total expected
payoff of policy µ and that of the optimal bidding strategy under known distribution of (πt  λt).

1.1 Motivating applications

The mathematical abstraction introduced above applies to virtual trading in the U.S. wholesale
electricity markets that are operated under a two-settlement framework. In the day-ahead (DA)
market  the independent system operator (ISO) receives offers to sell and bids to buy from generators
and retailers for each hour of the next day. To determine the optimal DA dispatch of the next day and
DA electricity prices at each location  ISO solves an economic dispatch problem with the objective of
maximizing social surplus while taking transmission and operational constraints into account. Due
to system congestion and losses  wholesale electricity prices vary from location to location.2 In the
real-time (RT) market  ISO adjusts the DA dispatch according to the RT operating conditions  and the
RT wholesale price compensates deviations in the actual consumption from the DA schedule.
The differences between DA and RT prices occur frequently both as a result of generators and
retailers exercising locational market power [2] and as a result of price spikes in the RT due to
unplanned outages and unpredictable weather conditions [3]. To promote price convergence between
DA and RT markets  in the early 2000s  virtual trading was introduced [4]. Virtual trading is a
ﬁnancial mechanism that allows market participants and external ﬁnancial entities to arbitrage on the
differences between DA and RT prices. Empirical and analytical studies have shown that increased
competition in the market due to virtual trading results in price convergence and increased market
efﬁciency [2  3  5].
Virtual transactions make up a signiﬁcant portion of the wholesale electricity markets. For example 
the total volume of cleared virtual transactions in ﬁve big ISO markets was 13% of the total load in
2013 [4]. In the same year  total payoff resulting from all virtual transactions was around 250 million
dollars in the PJM market [2] and 45 million dollars in NYISO market [6].
A bid in virtual trading is a bid to buy (sell) energy in the DA market at a speciﬁc location with an
obligation to sell (buy) back exactly the same amount in the RT market at the same location if the bid
is cleared (accepted). Speciﬁcally  a bid to buy in the DA market is cleared if the offered bid price is
higher than the DA market price. Similarly  a bid to sell in the DA market is cleared if it is below the
DA market price. In this context  different locations and/or different hours of the day are the set of
goods to bid on. The DA prices are the auction clearing prices  and the RT prices are the spot prices.
The problem studied here may also ﬁnd applications in other types of repeated auctions where the
auction may be of the double  uniform-price  or second-price types. For example  in the case of
online advertising auctions [7]  different goods can correspond to different types of advertising space
an advertiser may consider to bid on.

1.2 Main results and related work

We propose an online learning approach to the algorithmic bidding under budget constraints in
repeated multi-commodity auctions. The proposed approach falls in the category of empirical risk
minimization (ERM) also referred to as the follow the leader approach. The main challenge here is
that optimizing the payoff (risk) amounts to solving a multiple choice knapsack problem (MCKP)
that is known to be NP hard [8]. The proposed approach  referred to as dynamic programming on
discrete set (DPDS)  is inspired by a pseudo-polynomial dynamic programming approach to 0-1
Knapsack problems. DPDS allocates the limited budget of the bidder among K goods in polynomial
time both in terms of the number of goods K and in terms of the time horizon T . We show that the
expected payoff of DPDS converges to that of the optimal strategy under known distribution by a rate
T log T ). By showing that  for
T )  we prove that DPDS is order optimal up
any bidding strategy  the regret is lower bounded by Ω(
to a
log T term. We also evaluate the performance of DPDS empirically in the context of virtual
trading by using historical data from the New York energy market. Our empirical results show that

no slower than(cid:112)log t/t which results in a regret upper bound of O(

√

√

√

2For example  transmission congestion may prevent scheduling the least expensive resources at some

locations.

2

DPDS consistently outperforms benchmark heuristic methods that are derived from standard machine
learning methods.
The problem formulated here can be viewed in multiple machine learning perspectives. We highlight
below several relevant existing approaches. Since the bidder can calculate the reward that could have
been obtained by selecting any given bid value regardless of its own decision  our problem falls into
the category of full-feedback version of multi-armed bandit (MAB) problem  referred to as experts
problem  where the reward of all arms (actions) are observable at the end of each period regardless of
the chosen arm. For the case of ﬁnite number of arms  Kleinberg et al. [9] showed that  for stochastic
setting  constant regret is achievable by choosing the arm with the highest average reward at each
√
period. A special case of the adversarial setting was studied by Cesa-Bianchi et al. [10] who provided
matching upper and lower bounds in the order of Θ(
T ). Later  Freund and Schapire [11] and Auer
et al. [12] showed that the Hedge algorithm  a variation of weighted majority algorithm [13]  achieves
the matching bound for the general setting. These results  however  do not apply to experts problems
with continuous action spaces.
The stochastic experts problem where the set of arms is an uncountable compact metric space (X   d)
rather than ﬁnite was studied by Kleinberg and Slivkins [14] (see [15] for an extended version). Since
there are uncountable number of arms  it is assumed that  in each period  a payoff function drawn from
an i.i.d. distribution is observed rather than the individual payoff of each arm. Under the assumption
√
of Lipschitz expected payoff function  they showed that the instance-speciﬁc regret of any algorithm is
lower bounded by Ω(
T ). They also showed that their algorithm—NaiveExperts—achieves a regret
upper bound of O(T γ) for any γ > (b + 1)/(b + 2) where b is the isometry invariant of the metric
space. However  NaiveExperts is computationally intractable in practice because the computational
complexity of its direct implementation grows exponentially with the dimension (number of goods in
our case). Furthermore  the lower bound in [14] does not imply a lower bound for our problem with
a speciﬁc payoff. Krichene et al. [16] studied the adversarial setting and proposed an extension of
the Hedge algorithm  which achieves O(
T log T ) regret under the assumption of Lipschitz payoff
functions. For our problem  it is reasonable to assume that the expected payoff function is Lipschitz;
yet it is clear that  at each period  the payoff realization is a step function which is not Lipschitz.
Hence  Lipschitz assumption of [16] doesn’t hold in our setting.
Stochastic gradient descent methods  which have low computational complexity  have been extensively
studied in the literature of continuum-armed bandit [17  18  19]. However  either the concavity or
the unimodality of the expected payoff function is required for regret guarantees of these methods to
hold. This may not be the case in our problem depending on the underlying distribution of prices.
A relevant work that takes an online learning perspective for the problem of a bidder engaging in
repeated auctions is Weed et al. [7]. They are motivated by online advertising auctions and studied
the partial information setting of the same problem as ours but without a budget constraint. Under the
margin condition  i.e.  the probability of auction price occurring in close proximity of mean utility is
√
bounded  they showed that their algorithm  inspired by the UCB1 algorithm [20]  achieves regret that
ranges from O(log T ) to O(
T log T ) depending on how tight the margin condition is. They also
provided matching lower bounds up to a logarithmic factor. However  their lower bound does not
imply a bound for the full information setting we study here. Also  the learning algorithm in [7] does
not apply here because the goods are coupled through the budget constraint in our case. Furthermore 
we do not have margin condition  and we allow the utility of the good to depend on the auction price.
Some other examples of literature on online learning in repeated auctions studied the problem of an
advertiser who wants to maximize the number of clicks with a budget constraint [21  22]  or that of
a seller who tries to learn the valuation of its buyer in a posted price auction [23  24]. The settings
considered in those problems are considerably different from that studied here in the implementation
of budget constraints [21  22]  and in the strategic behavior of the bidder [23  24].

√

2 Problem formulation

The total expected payoff at period t given bid xt can be expressed as
(cid:124)1{xt ≥ λt}|xt)  

r(xt) = E ((πt − λt)

where the expectation is taken using the joint distribution of (πt  λt)  and 1{xt ≥ λt} is the vector of
indicator functions with the k-th entry corresponding to 1{xt k ≥ λt k}. We assume that the payoff

3

(cid:33)

(cid:32) T(cid:88)

t=1

E

(πt − λt)
(cid:124)1{xt ≥ λt} obtained at each period is a bounded random variable with support in [l  u] 3
and the auction prices are drawn from a distribution with positive support. Hence  a zero bid for any
good is equivalent to not bidding because it will not get cleared.
The objective is to determine a bidding policy µ that maximizes the expected T-period payoff subject
to a budget constraint for each individual period:

µ

maximize

r(xµ
t )
(cid:107)xµ
t (cid:107)1 ≤ B 
t ≥ 0 
xµ
where B is the auction budget of the bidder  xµ
t k ≥ 0 for all k ∈ {1  2  ...  K}.
is equivalent to xµ

subject to

for all t = 1  ...  T 
for all t = 1  ...  T 

(1)

t denotes the bid determined by policy µ  and xµ

t ≥ 0

2.1 Optimal solution under known distribution

If the joint distribution f (.  .) of πt and λt is known  the optimization problem (1) decouples to
solving for each time instant separately. Since (πt  λt) is i.i.d. over t  an optimal solution under
known model does not depend on t and is given by

x∗ = arg max
xt∈F

r(xt)

(2)

where F = {x ∈ (cid:60)K : x ≥ 0 (cid:107)x(cid:107)1 ≤ B} is the feasible set of bids. Optimal solution x∗ may not
be unique or it may not have a closed form. The following example illustrates a case where there
isn’t a closed form solution and shows that  even in the case of known distribution  the problem is a
combinatorial stochastic optimization  and it is not easy to calculate an optimal solution.

Example. Let λt and πt be independent  λt k be exponentially distributed with mean ¯λk > 0  and
the mean of πt k be ¯πk > 0 for all k ∈ {1  ..  K}. Since not bidding for good k is optimal if ¯πk ≤ 0 
we exclude the case ¯πk ≤ 0 without loss of generality. For this example  we can use the concavity of
(cid:124)  to obtain the unique optimal solution x∗  which
r(x) in the interval [0  ¯π]  where ¯π = [¯π1  ...  ¯πK]
is characterized by

¯πk

x∗
k =

0
xk satisfying (¯πk − xk)e−xk/¯λk /¯λk = γ∗

if(cid:80)K
if(cid:80)K
if(cid:80)K

k=1 ¯πk ≤ B 
k=1 ¯πk > B and ¯πk/¯λk < γ∗ 
k=1 ¯πk > B and ¯πk/¯λk ≥ γ∗ 

where the Lagrange multiplier γ∗ > 0 is chosen such that (cid:107)x∗(cid:107)1 = B is satisﬁed. This solution takes
the form of a "water-ﬁlling" strategy. More speciﬁcally  if the budget constraint is not binding  then
the optimal solution is to bid ¯πk for every good k. However  in the case of a binding budget constraint 
the optimal solution is determined by the bid value at which the marginal expected payoff associated
with each good k is equal to min(γ∗  ¯πk/¯λk)  and this bid value cannot be expressed in closed form.
We measure the performance of a bidding policy µ by its regret4  the difference between the expected
T-period payoff of µ and that of x∗  i.e. 

Rµ
T (f ) =

E(r(x∗) − r(xµ

t )) 

(3)

t=1

where the expectation is taken with respect to the randomness induced by µ. The regret of any policy
is monotonically increasing. Hence  we are interested in policies with sub-linear regret growth.

3This is reasonable in the case of virtual trading because DA and RT prices are bounded due to offer/bid caps.
4The regret deﬁnition used here is the same as in [14]. This deﬁnition is also known as pseudo-regret in the

literature [25].

4

T(cid:88)

3 Online learning approach to optimal bidding

The idea behind our approach is to maximize the sample mean of the expected payoff function  which
is an ERM approach [26]. However  we show that a direct implementation of ERM is NP-hard. Hence 
we propose a polynomial-time algorithm that is based on dynamic programming on a discretized
feasible set. We show that our approach achieves the order optimal regret.

3.1 Approximate expected payoff function and its optimization

Regardless of the bidding policy  one can observe the auction and spot prices of past periods.
Therefore  the average payoff that could have been obtained by bidding x up to the current period can
be calculated for any ﬁxed value of x ∈ F. Speciﬁcally  the average payoff ˆrt k(xk) for a good k as
a function of the bid value xk can be calculated at period t + 1 by using observations up to t  i.e. 

ˆrt k(xk) = (1/t)

(πi k − λi k)1{xk ≥ λi k}.

t(cid:88)

i=1

(cid:26) t−1

For example  at the end of ﬁrst period  ˆrt k(xk) = (π1 k− λ1 k)1{xk ≥ λ1 k} as illustrated in Fig. 1a.
For  t ≥ 2  this can be expressed recursively;

ˆrt k(xk) =

t ˆrt−1 k(xk)
t−1
t ˆrt−1 k(xk) + 1

t (πt k − λt k)

if xk < λt k 
if xk ≥ λt k.

(4)

i=1 and zero be λ(k) =(cid:2)0  λ(1) k  ...  λ(t) k

Since each observation introduces a new breakpoint  and the value of average payoff function is
constant between two consecutive breakpoints  we observe that ˆrt k(xk) is a piece-wise constant
function with at most t breakpoints. Let the vector of order statistics of the observed auction clearing
prices {λi k}t
  and let the vector of associated average
payoffs be r(k)  i.e.  r(k)
e.g.  see Fig. 1b.

. Then  ˆrt k(xk) can be expressed by the pair(cid:0)λ(k)  r(k)(cid:1) 

i = ˆrt k

(cid:3)(cid:124)

λ(k)
i

(cid:16)

(cid:17)

ˆr1 k(xk)

ˆr4 k(xk)

π1 k − λ1 k

0

λ1 k

(a) t = 1

r(k)
5
r(k)
3
r(k)
4
r(k)
2

xk

0

λ(k)
2

λ(k)
λ(k)
4
3
(b) t = 4

λ(k)
5

xk

Figure 1: Piece-wise constant average payoff function of good k

(cid:0)λ(k)  r(k)(cid:1) = (0  0) at the beginning of ﬁrst period. Then  at each period t ≥ 1  the pair(cid:0)λ(k)  r(k)(cid:1)

For a vector y  let ym:n = (ym  ym+1  ...  yn) denote the sequence of entries from m to n. Initialize

can be updated recursively as follows:

(cid:16)

λ(k)  r(k)(cid:17)

(cid:18)(cid:104)

=

λ(k)
1:ik

  λt k  λ(k)

ik+1:t

 

(cid:105)(cid:124)

(cid:20) t − 1

r(k)
1:ik

 

t − 1
t

r(k)
ik:t +

1
t

t

(πt k − λt k)

 

(5)

(cid:21)(cid:124)(cid:19)

i <λt k

i at period t.

where ik = maxi:λ(k)
Consequently  overall average payoff function ˆrt(x) can be expressed as a sum of average payoff
functions of individual goods. Instead of the unknown expected payoff r(x)  let’s consider the
maximization of the average payoff function  which corresponds to the ERM approach  i.e. 

x∈F ˆrt(x) = max
max
x∈F

ˆrt k(xk).

(6)

the same amount to the overall payoff as choosing any xk ∈ (cid:104)

Due to the piece-wise constant structure  choosing xk = λ(k)

k=1

i

for some i ∈ {1  ...  t + 1} contributes
if i < t + 1 and any

  λ(k)
i+1

λ(k)
i

(cid:17)

K(cid:88)

5

xk ≥ λ(k)
an optimal solution to (6) can be obtained by solving the following integer linear program:

if i = t + 1. However  choosing xk = λ(k)

utilizes a smaller portion of the budget. Hence 

i

i

k=1

maximize
{zk}K

K(cid:88)
K(cid:88)
where the bid value xk =(cid:0)λ(k)(cid:1)(cid:124)

subject to

k=1

(cid:16)
(cid:16)

r(k)(cid:17)(cid:124)
λ(k)(cid:17)(cid:124)

zk

zk ≤ 1 

k=1
(cid:124)
1
zk i ∈ {0  1} 

zk for good k.

zk ≤ B 
∀k = 1  ...  K 
∀i = 1  ...  t + 1;∀k = 1  ...  K.

(7)

Observe that (7) is a multiple choice knapsack problem (MCKP) [8]  a generalization of 0-1 knapsack.
Unfortunately  (7) is NP-hard [8]. If we had a polynomial-time algorithm that ﬁnds an optimal
solution x ∈ F to (6)  then we could have obtained the solution of (7) in polynomial-time too by
setting zk i = 1 where i = maxi:λ(k)
i for each k. Therefore  (6) is also NP-hard  and  to the
best of our knowledge  there isn’t any method in the ERM literature [27]  which mostly focuses on
classiﬁcation problems  suitable to implement for the speciﬁc problem at hand.

i ≤xk

3.2 Dynamic programming on discrete set (DPDS) policy

Next  we present an approach that discretizes the feasible set using intervals of equal length and
optimizes the average payoff on this new discrete set via a dynamic program. Although this approach
doesn’t solve (6)  the solution can be arbitrarily close to the optimal depending on the choice of
the interval length under the assumption of the Lipschitz continuous expected payoff function. To
exploit the smoothness of Lipschitz continuity  discretization approach of the continuous feasible set
has been used in the continuous MAB literature previously [17  14]. However  different than MAB
literature  in this paper  discretization approach is utilized to reduce the computational complexity of
an NP-hard problem as well.
Let αt be an integer sequence increasing with t and Dt = {0  B/αt  2B/αt  ...  B} as illustrated in
Fig. 2. Then  the new discrete set is given as Ft = {x ∈ F : xk ∈ Dt ∀k ∈ {1  ...  K}}. Our goal is
to optimize ˆrt(.) on the new set Ft rather than F  i.e. 

max
xt+1∈Ft

ˆrt(xt+1).

(8)

ˆr4 k(xk)

r(k)
5
r(k)
3
r(k)
4
r(k)
2

0

λ(k)
2

B
α4

λ(k)
3

2B
α4

λ(k)
4

3B
α4

λ(k)
5

4B
α4

xk

Figure 2: Example of the discretization of the decision space for good k when t = 4

Now  we use dynamic programming approach that has been used to solve 0-1 Knapsack problems
including MCKP given in (7) [28]. However  direct implementation of this approach results in pseudo-
polynomial computational complexity in the case of 0-1 Knapsack problems. The discretization of
the feasible set with equal interval length reduces the computational complexity to polynomial time.
We deﬁne the maximum payoff one can collect with budget b among goods {1  ...  n} when the bid
value xk is restricted to the set Dt for each good k as

Vn(b) =

k=1:(cid:80)n

{xk}n

max
k=1 xk≤b xk∈Dt∀k

6

n(cid:88)

k=1

ˆrt k(xk).

(cid:40)0

Then  the following recursion can be used to solve for VK(B) which gives the optimal solution to (8):

max
0≤i≤j

Vn(jB/αt) =

(ˆrt n(iB/αt) + Vn−1((j − i)B/αt))

if n = 0  j ∈ {0  1  ...  αt} 
if 1 ≤ n ≤ K  j ∈ {0  1  ...  αt}.
(9)
This is the Bellman equation where Vn(b) is the maximum total payoff one can collect using remaining
budget b and remaining n goods. Its optimality can be shown via a simple induction argument. Recall
that ˆrt n(0) = 0 for all (t  n) pairs due to the assumption of positive day-ahead prices.
Recursion (9) can be solved starting from n = 1 and proceeding to n = K  where  for each n  Vn(b)
is calculated for all b ∈ Dt. Since the computation of Vn(b) requires at most αt + 1 comparison for
any ﬁxed value of n ∈ {1  ...  K} and b ∈ Dt  it has a computational complexity on the order of Kα2
once the average payoff values ˆrt n(xn) for all xn ∈ Dt and n ∈ {1  ...  K} are given. For each
n ∈ {1  ...  K}  computation of ˆrt n(xn) for all xn ∈ Dt introduces an additional computational

complexity of at most on the order of t  which can be observed from the update step of(cid:0)λ(k)  π(k)(cid:1) 

t

given in (5). Hence  total computational complexity of DPDS is O(K max(t  α2

t )) at each period t.

3.3 Convergence and regret of DPDS policy

the value of the optimal policy under known model with a rate faster than or equal to(cid:112)log t/t if the

Under the assumption of Lipschitz continuity  Theorem 1 shows that the value of DPDS converges to
DPDS algorithm parameter αt = (cid:100)tγ(cid:101) with γ ≥ 1/2. Consequently  the regret growth rate of DPDS
T log T ). If γ = 1/2  then the computational complexity of the algorithm
is upper bounded by O(
is bounded by O(Kt) at each period t  and total complexity over the entire horizon is O(KT 2).

√

Theorem 1 Let xDPDS
t+1 denote the bid of DPDS policy for period t + 1. If r(.) is Lipschitz continuous
on F with p-norm and Lipschitz constant L  then  for any γ > 0 and for DPDS parameter choice
αt ≥ 2 
E(r(x∗)− r(xDPDS

+(cid:112)2(γ + 1)K + 1(u− l)

4 min(u − l  LK 1/pB)αK

t+1 )) ≤ LK 1/pB

(cid:114)

log t

+

t

t

t(γ+1)K+1/2

αt

and for αt = max((cid:100)tγ(cid:101)  2) with γ ≥ 1/2 
RDPDS

(f ) ≤ 2(LK 1/pB +4 min(u−l  LK 1/pB))

T

√

T +2(cid:112)2(γ + 1)K + 1(u−l)(cid:112)T log T . (11)

 
(10)

t+1 )) ≤ LK q/p(B/αt)q +(u−l)((cid:112)2(γ + 1)K + 1(cid:112)log t/t+4αK

Actually  we can relax the uniform Lipschitz continuity condition. Under the weaker condition of
|r(x∗) − r(x)| ≤ L(cid:107)x∗ − x(cid:107)q
p for all x ∈ F and for some constant L > 0  the incremental regret
bound that is given in (10) becomes
E(r(x∗)−r(xDPDS
t t−(γ+1)K−1/2).
The proof of Theorem 1 is derived by showing that the value of x∗
t+1 = arg maxx∈Ft r(x) converges
to the value of x∗ due to Lipschitz continuity  and the value of xDPDS
t+1 converges to the value of x∗
t+1
via the use of concentration inequality inspired by [20  17].
Even though the upper bound of regret in Theorem 1 depends on the budget B linearly  this de-
pendence can be avoided in the expense of increase in computational complexity. For example 
in the literature  the reward is generally assumed to be in the unit interval  i.e.  l = 0 and u = 1 
and the expected reward is assumed to be Lipschitz continuous with Euclidean norm and constant
L = 1. In this case  by following the proof of Theorem 1  we observe that assigning γ = 1/2 and
αt = max((cid:100)αtγ(cid:101)  2) for some α > 0 gives a regret upper bound of 2B
√
KT log T +α
for T > α + 1. Consequently  if B = O(K)  then O(K 3/4
KT log T ) regret is achievable
by setting α = K 3/4.

KT /α +12

T +

√

√

√

3.4 Lower bound of regret for any bidding policy

We now show that DPDS in fact achieves the slowest possible regret growth. Speciﬁcally  Theorem 2
states that  for any bidding policy µ and horizon T   there exists a distribution f for which the regret
growth is slower than or equal to the square root of the horizon T .

7

Theorem 2 Consider the case where K = 1  B = 1  and λt and πt are independent random
variables with distributions

fλ(λt) = −11{(1 − )/2 ≤ λt ≤ (1 + )/2}

and fπ(πt) = Bernoulli(¯π)  respectively. Let f (λt  πt) = fλ(λt)fπ(πt) and  = T −1/2/2
for any bidding policy µ 

√

√

either for ¯π = 1/2 +  or for ¯π = 1/2 − .

T (f ) ≥ (1/16
Rµ

5)

T  

√

5. Then 

As seen in Theorem 2  we choose a speciﬁc distribution for the auction clearing and spot prices.
Observe that  for this distribution  the payoff function is Lipschitz continuous with Lipschitz constant
L = 3/2 because the magnitude of the derivative of the payoff function |r(cid:48)(x)| ≤ |¯π − x|/ ≤ 3/2
for (1 − )/2 ≤ x ≤ (1 + )/2 and r(cid:48)(x) = 0 otherwise. So  it satisﬁes the condition given in
Theorem 1.
The proof of Theorem 2 is obtained by showing that  every time the bid is cleared  an incremental
regret greater than /2 is incurred under the distribution with ¯π = (1/2−); otherwise  an incremental
regret greater than /2 is incurred under the distribution with ¯π = (1/2 + ). However  to distinguish
between these two distributions  one needs Ω(T ) samples  which results in a regret lower bound
of Ω(
T ). The bound is obtained by adapting a similar argument used by [29] in the context of
non-stochastic MAB problem.

√

4 Empirical study

New York ISO (NYISO)  which consists of 11 zones  allows virtual transactions at zonal nodes only.
So  we use historical DA and RT prices of these zones from 2011 to 2016 [30]. Since the price for
each hour is different at each zone  there are 11× 24 different locations  i.e.  zone-hour pairs  to bid on
every day. The prices are per unit (MWh) prices. We also consider buy and sell bids simultaneously
for all location. As explained in Sec. 1.1  a sell bid is a bid to sell in the DA market with an obligation
to buy back in the RT market. Hence  the proﬁt of a sell bid at period t is (λt − πt)
(cid:124)1{xt ≤ λt}.
Generally  an upper bound ¯p for the DA prices is known  e.g. ¯p = $1000 for NYISO. We convert
t = ¯p − πt instead of xt  λt 
a sell bid to a buy bid by using xsell
and πt. NYISO DA market for day t closes at 5:00 am on day t − 1. Hence  the RT prices of all
hours of day t − 1 cannot be observed before the bid submission for day t. Therefore  the most recent
information used before the submission for day t was the observations from day t − 2.

t = ¯p − λt  and πsell

t = ¯p − xt  λsell

(a) y = 2012

(b) y = 2013

(c) y = 2014

(d) y = 2015

(e) y = 2016

Figure 3: Cumulative proﬁt trajectory of year y for B = 100000

We compare DPDS with three algorithms. One of them is UCBID-GR  inspired by UCBID [7]. At
each day  UCBID-GR sorts all locations according to their proﬁtabilities  i.e.  their price spread (the
difference between DA and RT price) sample means. Then  starting from the most proﬁtable location 

8

UCBID-GR sets the bid of a location equal to its RT price sample mean until there isn’t any sufﬁcient
budget left.
The second algorithm  referred to as SA  is a variant of Kiefer-Wolfowitz stochastic approximation
method. SA approximates the gradient of the payoff function by using the current observation and
updates the bid of each k as follows;
xt k = xt−1 k + at ((πt−2 k − λt−2 k)(1{xt−1 k + ct ≥ λt−2 k} − 1{xt−1 k ≥ λt−2 k})) /ct.
Then  xt is projected to the feasible set F.
The last algorithm is SVM-GR  which is inspired by the use of support vector machines (SVM) by
Tang et al. [31] to determine if a buy or a sell bid is proﬁtable at a location  i.e.  if the price spread is
positive or negative. Due to possible correlation of the price spread at a location on day t with the
price spreads observed recently at that and also at other locations  the input of SVM for each location
is set as the price spreads of all locations from day t − 7 to day t − 2. To test SVM-GR algorithm
at a particular year  for each location  the data from the previous year is used to train SVM and to
determine the average proﬁt  i.e.  average price spread  and the bid level that will be accepted with
95% conﬁdence in the event that a buy or a sell bid is proﬁtable. For the test year  at each period 
SVM-GR ﬁrst determines if a buy or a sell bid is proﬁtable for each location. Then  SVM-GR sorts
all locations according to their average proﬁts  and  starting from the most proﬁtable location  it sets
the bid of a location equal to the bid level with 95% conﬁdence of acceptance until there isn’t any
sufﬁcient budget left.
To evaluate the performance of a year  DPDS  UCBID-GR  and SA algorithms have also been trained
starting from the beginning of the previous year. The algorithm parameter of DPDS was set as αt = t;
and the step size at and ct of SA were set as 20000/t and 2000/t1/4  respectively.
For B=$100 000  the cumulative proﬁt trajectory of ﬁve consecutive years are given in Fig. 3. We
observe that DPDS obtains a signiﬁcant proﬁt in all cases  and it outperforms other algorithms
consistently except 2015 where SVM-GR makes approximately 25% more proﬁt. However  in three
out of ﬁve years  SVM-GR suffers a considerable amount of loss. In general  UCBID-GR performs
quite well except 2016  and SA algorithm incurs a loss almost every year.

5 Conclusion

√

log T term.

T ) for any bidding

By applying general techniques such as ERM  discretization approach  and dynamic programming  we
derive a practical and efﬁcient algorithm to the algorithmic bidding problem under budget constraint
in repeated multi-commodity auctions. We show that the expected payoff of the proposed algorithm 

DPDS  converges to that of the optimal strategy by a rate no slower than(cid:112)log t/t  which results

√
T log T ) regret. By showing that the regret is lower bounded by Ω(

√
in a O(
strategy  we prove that DPDS is order optimal up to a
For the motivating application of virtual bidding in electricity markets (see Sec. 1.1)  the stochastic
setting  studied in this paper  is natural due to the electricity markets being competitive  which
implies that the existence of an adversary is very unlikely. However  it is also of interest to study the
adversarial setting to extend the results to other applications. For example  the adversarial setting of
our problem is a special case of no-regret learning problem of Simultaneous Second Price Auctions
(SiSPA)  studied by Daskalakis and Syrgkanis [32] and Dudik et al. [33].
In particular  to deal with the adversarial setting  it is possible to use our dynamic programming
approach as the ofﬂine oracle for the Oracle-Based Generalized FTPL algorithm proposed by Dudik
et al. [33] if we ﬁx the discretized action set over the whole time horizon. More speciﬁcally  let the
interval length of discretization be B/m  i.e.  αt = m. Then  it is possible to show that a 1-admissible
translation matrix with K(cid:100)log m(cid:101) columns is implementable with complexity m. Consequently 
no-regret result of Dudik et al. [33] holds with a regret bound of O(K
T log m) if we measure
the performance of the algorithm against the best action in hindsight in the discretized ﬁnite action
set rather than in the original continuous action set considered here. Unfortunately  as shown by
Weed et al. [7]  it is not possible to achieve sublinear regret with a ﬁxed discretization for the speciﬁc
problem considered in this paper. Hence  it requires further work to see if this method can be extended
to obtain no-regret learning for the adversarial setting under the original continuous action set.

√

9

Acknowledgments

We would like to thank Professor Robert Kleinberg for the insightful discussion.
This work was supported in part by the National Science Foundation under Award 1549989 and by
the Army Research Laboratory Network Science CTA under Cooperative Agreement W911NF-09-2-
0053.

References
[1] Paul Milgrom. Putting auction theory to work. Cambridge University Press  2004.

[2] PJM. Virtual transactions in the pjm energy markets. Technical report  Oct 2015. http://

www.pjm.com/~/media/committees-groups/committees/mc/20151019-webinar/
20151019-item-02-virtual-transactions-in-the-pjm-energy-markets-
whitepaper.ashx.

[3] Ruoyang Li  Alva J. Svoboda  and Shmuel S. Oren. Efﬁciency impact of convergence bidding in the

california electricity market. Journal of Regulatory Economics  48(3):245–284  2015.

[4] John E. Parsons  Cathleen Colbert  Jeremy Larrieu  Taylor Martin  and Erin Mastrangelo. Financial
arbitrage and efﬁcient dispatch in wholesale electricity markets  February 2015. https://ssrn.com/
abstract=2574397.

[5] Wenyuan Tang  Ram Rajagopal  Kameshwar Poolla  and Pravin Varaiya. Model and data analysis of
two-settlement electricity market with virtual bidding. In 2016 IEEE 55th Conference on Decision and
Control (CDC)  pages 6645–6650  2016.

[6] David B. Patton  Pallas LeeVanSchaick  and Jie Chen. 2014 state of the market report for the new york iso

markets. Technical report  May 2015. http://www.nyiso.com/public/webdocs/
markets_operations/documents/Studies_and_Reports/Reports/
Market_Monitoring_Unit_Reports/2014/NYISO2014SOMReport__5-13-
2015_Final.pdf.

[7] Jonathan Weed  Vianney Perchet  and Philippe Rigollet. Online learning in repeated auctions. In 29th

Annual Conference on Learning Theory  page 1562–1583  2016.

[8] Hans Kellerer  Ulrich Pferschy  and David Pisinger. The Multiple-Choice Knapsack Problem  pages

317–347. Springer Berlin Heidelberg  2004.

[9] Robert Kleinberg  Alexandru Niculescu-Mizil  and Yogeshwer Sharma. Regret bounds for sleeping experts

and bandits. In 21st Conference on Learning Theory  pages 425–436  2008.

[10] Nicolò Cesa-Bianchi  Yoav Freund  David P. Helmbold  David Haussler  Robert E. Schapire  and Manfred K.
Warmuth. How to use expert advice. In Proceedings of the Twenty-ﬁfth Annual ACM Symposium on Theory
of Computing  pages 382–391. ACM  1993.

[11] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. In Proceedings of the Second European Conference on Computational Learning
Theory  pages 23–37. Springer-Verlag  1995.

[12] Peter Auer  Nicolò Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. Gambling in a rigged casino: The
adversarial multi-armed bandit problem. In Proceedings of IEEE 36th Annual Foundations of Computer
Science  pages 322–331  1995.

[13] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Information and Computa-

tion  108(2):212 – 261  1994.

[14] Robert Kleinberg and Aleksandrs Slivkins. Sharp dichotomies for regret minimization in metric spaces. In
Proceedings of the Twenty-ﬁrst Annual ACM-SIAM Symposium on Discrete Algorithms  pages 827–846.
Society for Industrial and Applied Mathematics  2010.

[15] Robert Kleinberg  Aleksandrs Slivkins  and Eli Upfal. Bandits and experts in metric spaces. arXiv preprint

arXiv:1312.1277v2  2015.

[16] Walid Krichene  Maximilian Balandat  Claire Tomlin  and Alexandre Bayen. The hedge algorithm on a
continuum. In Proceedings of the 32Nd International Conference on International Conference on Machine
Learning - Volume 37  pages 824–832. JMLR.org  2015.

10

[17] Robert D. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In L. K. Saul  Y. Weiss 
and L. Bottou  editors  Advances in Neural Information Processing Systems 17  pages 697–704. MIT Press 
2005.

[18] Abraham D. Flaxman  Adam Tauman Kalai  and H. Brendan McMahan. Online convex optimization in the
bandit setting: Gradient descent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM
Symposium on Discrete Algorithms  pages 385–394. Society for Industrial and Applied Mathematics  2005.

[19] Eric W. Cope. Regret and convergence bounds for a class of continuum-armed bandit problems. IEEE

Transactions on Automatic Control  54(6):1243–1253  2009.

[20] Peter Auer  Nicolò Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning  47(2-3):235–256  2002.

[21] Kareem Amin  Michael Kearns  Peter Key  and Anton Schwaighofer. Budget optimization for sponsored
search: Censored learning in mdps. In Proceedings of the Twenty-Eighth Conference on Uncertainty in
Artiﬁcial Intelligence  pages 54–63. AUAI Press  2012.

[22] Long Tran-Thanh  Lampros Stavrogiannis  Victor Naroditskiy  Valentin Robu  Nicholas R Jennings  and
Peter Key. Efﬁcient regret bounds for online bid optimisation in budget-limited sponsored search auctions.
In Proceedings of the Thirtieth Conference on Uncertainty in Artiﬁcial Intelligence  pages 809–818. AUAI
Press  2014.

[23] Kareem Amin  Afshin Rostamizadeh  and Umar Syed. Learning prices for repeated auctions with strategic
buyers. In C. J. C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K. Q. Weinberger  editors  Advances
in Neural Information Processing Systems 26  pages 1169–1177. Curran Associates  Inc.  2013.

[24] Mehryar Mohri and Andres Munoz. Optimal regret minimization in posted-price auctions with strategic
buyers. In Z. Ghahramani  M. Welling  C. Cortes  N. D. Lawrence  and K. Q. Weinberger  editors  Advances
in Neural Information Processing Systems 27  pages 1871–1879. Curran Associates  Inc.  2014.

[25] Sébastien Bubeck and Nicolò Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

[26] Vladimir Vapnik. Principles of risk minimization for learning theory. In J. E. Moody  S. J. Hanson 
and R. P. Lippmann  editors  Advances in Neural Information Processing Systems 4  pages 831–838.
Morgan-Kaufmann  1992.

[27] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms.

Cambridge University Press  2014.

[28] Krzysztof Dudzi´nski and Stanisław Walukiewicz. Exact methods for the knapsack problem and its

generalizations. European Journal of Operational Research  28(1):3 – 21  1987.

[29] Peter Auer  Nicolò Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic multiarmed

bandit problem. SIAM Journal on Computing  32(1):48–77  2002.

[30] NYISO Website  2017. http://www.nyiso.com/public/markets_operations/

market_data/pricing_data/index.jsp.

[31] Wenyuan Tang  Ram Rajagopal  Kameshwar Poolla  and Pravin Varaiya. Private communications  2017.

[32] Constantinos Daskalakis and Vasilis Syrgkanis. Learning in auctions: Regret is hard  envy is easy. In 2016

IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)  pages 219–228  2016.

[33] Miroslav Dudik  Nika Haghtalab  Haipeng Luo  Robert E. Shapire  Vasilis Syrgkanis  and Jennifer Wortman
Vaughan. Oracle-efﬁcient online learning and auction design. In 2017 IEEE 58th Annual Symposium on
Foundations of Computer Science (FOCS)  pages 528–539  2017.

11

,M. Sevi Baltaoglu
Lang Tong
Qing Zhao