2019,Hamiltonian descent for composite objectives,In optimization the duality gap between the primal and the dual problems is a measure of the suboptimality of any primal-dual point. In classical mechanics the equations of motion of a system can be derived from the Hamiltonian function  which is a quantity that describes the total energy of the system.  In this paper we consider a convex optimization problem consisting of the sum of two convex functions  sometimes referred to as a composite objective  and we identify the duality gap to be the `energy' of the system.  In the Hamiltonian formalism the energy is conserved  so we add a contractive term to the standard equations of motion so that this energy decreases linearly (ie  geometrically) with time.  This yields a continuous-time ordinary differential equation (ODE) in the primal and dual variables which converges to zero duality gap  ie  optimality.  This ODE has several useful properties: it induces a natural operator splitting; at convergence it yields both the primal and dual solutions; and it is invariant to affine transformation despite only using first order information.  We provide several discretizations of this ODE  some of which are new algorithms and others correspond to known techniques  such as the alternating direction method of multipliers (ADMM).  We conclude with some numerical examples that show the promise of our approach. We give an example where our technique can solve a convex quadratic minimization problem orders of magnitude faster than several commonly-used gradient methods  including conjugate gradient  when the conditioning of the problem is poor.  Our framework provides new insights into previously known algorithms in the literature as well as providing a technique to generate new primal-dual algorithms.,Hamiltonian descent for composite objectives

Brendan O’Donoghue

DeepMind

❜♦❞♦♥♦❣❤✉❡❅❣♦♦❣❧❡✳❝♦♠

Chris J. Maddison

DeepMind / University of Oxford

❝♠❛❞❞✐s❅❣♦♦❣❧❡✳❝♦♠

Abstract

In optimization the duality gap between the primal and the dual problems is a
measure of the suboptimality of any primal-dual point. In classical mechanics the
equations of motion of a system can be derived from the Hamiltonian function 
which is a quantity that describes the total energy of the system. In this paper
we consider a convex optimization problem consisting of the sum of two convex
functions  sometimes referred to as a composite objective  and we identify the
duality gap to be the ‘energy’ of the system. In the Hamiltonian formalism the
energy is conserved  so we add a contractive term to the standard equations of
motion so that this energy decreases linearly (i.e.  geometrically) with time. This
yields a continuous-time ordinary differential equation (ODE) in the primal and
dual variables which converges to zero duality gap  i.e.  optimality. This ODE has
several useful properties: it induces a natural operator splitting; at convergence it
yields both the primal and dual solutions; and it is invariant to afﬁne transformation
despite only using ﬁrst order information. We provide several discretizations of
this ODE  some of which are new algorithms and others correspond to known
techniques  such as the alternating direction method of multipliers (ADMM). We
conclude with some numerical examples that show the promise of our approach.
We give an example where our technique can solve a convex quadratic minimization
problem orders of magnitude faster than several commonly-used gradient methods 
including conjugate gradient  when the conditioning of the problem is poor. Our
framework provides new insights into previously known algorithms in the literature
as well as providing a technique to generate new primal-dual algorithms.

1

Introduction and prior work

In physics the Hamiltonian function represents the total energy of a system in some set of coordinates
(loosely speaking). In the most typical case the coordinates are the position x ∈ Rn and momentum
p ∈ Rn  and the Hamiltonian is the sum of the potential energy  a function of the position  and the
kinetic energy  a function of the momentum. The equations of motion for the system can be derived
from the Hamiltonian. Let us denote the Hamiltonian as H : Rn × Rn → R  which we assume is
differentiable  then the equations of motion [1] are given by

˙xt = ∇pH(xt  pt) 

˙pt = −∇xH(xt  pt) 

where we use the notation ˙xt := dxt/dt. For ease of notation we shall sometimes use z := (x  p) ∈
R2n to denote the concatenation of the position and momentum into a single quantity  in which case
we can write the Hamiltonian ﬂow as

˙zt = J∇H(zt) 

J =(cid:20) 0

−I

I

0(cid:21)  

(1)

and note that J T J = I and that J is skew symmetric  that is J = −J T   and so vT Jv = 0 for
any v. It is easy to show that these equations of motion conserve the Hamiltonian since ˙H(zt) =

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

∇zH(zt)T ˙zt = ∇H(zt)T J∇H(zt) = 0. This conservation property is required for anything that
models the energy of a system in the physical universe  but not directly useful in optimization where
the goal is convergence to an optimum. By adding a contractive term to the Hamiltonian ﬂow
we derive an ordinary differential equation (ODE) whose solutions converge to a minimum of the
Hamiltonian. We call the resulting ﬂow “Hamiltonian descent”.

In optimization there has been a lot of recent interest in continuous-time ordinary differential equations
(ODEs) that when discretized yield known or interesting novel algorithms [2  3  4]. In particular Su et
al.[5] derived a simple ODE that corresponds to Nesterov’s accelerated gradient scheme [6]  see also
[7]. That work was extended in [8] where the authors derived a “Bregman Lagrangian” framework
that generates a family of continuous-time ODEs corresponding to several discrete-time algorithms 
including Nesterov’s accelerated gradient. This was extended in [9] to derive a novel acceleration
algorithm. In [10] the authors used Lyapunov functions to analyze the convergence properties of
continuous and discrete-time systems. There is a natural Hamiltonian perspective on the Bregman
Lagrangian  which was exploited in [11] to derive optimization methods from symplectic integrators.

In a similar vein  the authors of [12] used a conformal Hamiltonian system to expand the class
of functions for which linear convergence of ﬁrst-order methods can be obtained by encoding
information about the convex conjugate into a kinetic energy. Follow-up work analyzed the properties
of conformal symplectic integrators for these conformal Hamiltonian systems [13].

Hamiltonian mechanics have previously been applied to several areas outside of classical mechanics
[14]  most notably in Hamiltonian Monte Carlo (HMC)  where the goal is to sample from a target dis-
tribution and Hamiltonian mechanics are used to propose moves in a Metropolis-Hastings algorithm;
see [15] for a good survey. More recently Hamiltonian mechanics has been discussed in the context
of game theory [16]  where a symplectic gradient algorithm was developed that converges to stable
ﬁxed points of general games.

1.1 The convex conjugate

The Hamiltonian as used in physics is derived by taking the Legendre transform (or convex conjugate)
of one of the terms in the Lagrangian describing the system  which for a function f : Rn → R is
deﬁned as

f ∗(p) = sup
x

(xT p − f (x)).

The function f ∗ is always convex  even if f is not. When f is closed  proper  and convex  then
(f ∗)∗ = f   and (∂f )−1 = ∂f ∗  where ∂f denotes the subdifferential of f   which for differentiable
functions is just the gradient  i.e.  ∂f = ∇f (or more precisely ∂f = {∇f }) [17].

2 Hamiltonian descent

A modiﬁcation to the Hamiltonian ﬂow equation (1) yields an ordinary differential equation whose
solutions decrease the Hamiltonian linearly:

˙zt = J∇H(zt) + z⋆ − zt 

(2)

where z⋆ ∈ argminz H(z). This departs from the standard Hamiltonian ﬂow equations by the
addition of the term involving the difference between z⋆ and zt. One can view the Hamiltonian
descent equation as a ﬂow in a ﬁeld consisting of the sum of a standard Hamiltonian ﬁeld and the
negative gradient ﬁeld of function (1/2)kzt − z⋆k2
2. Solutions to this differential equation descend
the level sets of the Hamiltonian and so we refer to (2) as Hamiltonian descent equations. Note
that this ﬂow is different to the dissipative ﬂows using conformal Hamiltonian mechanics studied in
[12  13]  which are also Hamiltonian descent methods but employ a different dissipative force. We
shall show the linear convergence of solutions of (2) to a minimum of the Hamiltonian function; ﬁrst
we will state a necessary assumption:
Assumption 1. The Hamiltonian H together with a point (x⋆  p⋆) = z⋆ ∈ arg minz H(z) satisfy
the following:

• z⋆ = arg minz H(z) is unique 

• H(z) ≥ H(z⋆) = 0 for all z ∈ R2n 

2

• H is proper  closed  convex 

• H is continuously differentiable.

Theorem 1. If zt is following the equations of motion in (2) where z⋆ and the Hamiltonian func-
tion satisfy assumption 1  then the Hamiltonian converges to zero linearly (i.e.  geometrically).
Furthermore  zt converges to z⋆ and ˙zt converges to zero.

Proof. Consider the time derivative of the Hamiltonian:

˙H(zt) = ∇H(zt)T ˙zt = ∇H(zt)T (J∇H(zt) + z⋆ − zt) ≤ −H(zt).

(3)

since J is skew-symmetric  H(z⋆) = 0 and H is convex. Grönwall’s inequality [18] then implies
that 0 ≤ H(zt) ≤ H(z0) exp(−t) and so H(zt) → 0 linearly. Consider M = {z ∈ R2n :
∇H(z)T (z⋆ − z) = 0}. It is not too hard to see that M = {z⋆} and that M is an invariant set  since
⋆) ≥ H(z) by convexity. Because H has a unique minimum  its sublevel set are
∇H(z ′
bounded. Thus  we can apply Theorem 3.4 of [19] (Local Invariant Set Theorem) to argue that all
solutions zt → z⋆. Further  we have ∇H(zt) → 0 by continuity and thus ˙zt → 0.

⋆)T (z⋆ − z ′

In contrast  consider the gradient descent ﬂow ˙zt = −∇H(zt)  which also converges since

˙H(zt) = ∇H(zt)T ˙zt = −k∇H(zt)k2

2 ≤ 0.

In this case  linear convergence is only guaranteed when H has some other property  such as strong
convexity  which Hamiltonian descent does not require.

It may appear that these equations of motion are unrealizable without knowledge of a minimum of
the Hamiltonian z⋆  which would defeat the goal of ﬁnding such a point. However  by a judicious
choice of the Hamiltonian we can cancel the terms involving z⋆  and make the system realizable.
For example  take the problem of minimizing convex f : Rn → R  and consider the following
Hamiltonian

H(x  p) = f (x) + f ∗(p) − pT x⋆ 

where x⋆ is any minimizer of f . Note that (x⋆  0) ∈ argmin(x p) H(x  p). Assuming f and f ∗ are
continuously differentiable and (x⋆  0) is a unique minimum of H  then it is readily veriﬁed that this
Hamiltonian satisﬁes assumption 1. So the solutions of the equations of motion will converge to a
minimum of H linearly. In this case the ﬂow is given by

˙xt = ∇pH(xt  pt) + x⋆ − xt = ∇f ∗(pt) − xt
˙pt = −∇xH(xt  pt) + p⋆ − pt = −∇f (xt) − pt 

since p⋆ = 0  and note that theorem 1 implies that ˙xt → 0  ˙pt → 0 and in the limit these equations
reduce to the optimality condition for the problem  namely ∇f (x) = 0. However  this system requires
the ability to evaluate ∇f ∗  which is as hard as the original problem (since x⋆ = ∇f ∗(0)). In the
sequel we shall exploit the structure of composite optimization problems to avoid this requirement.

2.1 Afﬁne invariance

The Hamiltonian descent equations of motion (2) are invariant to a set of afﬁne transformations. This
property is very useful since it means that the performance of an algorithm based on these equations
will be much less sensitive to the conditioning of the problem than  for example  gradient descent
which does not enjoy afﬁne invariance.

To show this property  consider a non-singular matrix M that satisﬁes M JM T = J and consider the
Hamiltonian in the new coordinate system 

¯H(y) = H(M −1y) 

where clearly y⋆ = M z⋆. At time τ we have the point yτ   and let zτ = M −1yτ . Running Hamiltonian
descent in the transformed coordinates we obtain

˙yτ = J∇ ¯H(yτ ) + y⋆ − yτ

= JM −T ∇H(M −1yτ ) + M z⋆ − M zτ
= M (J∇H(zτ ) + z⋆ − zτ )
= M ˙zτ .

3

Now let z0 = M −1y0  then we have yt = y0 +R t

0 M ˙zτ = M zt for all t  and
therefore ¯H(yt) = H(M −1M zt) = H(zt)  i.e.  the original and transformed Hamiltonians have
exactly the same value for all t and thus the rate of convergence is unchanged by the transformation.
The condition on M is not too onerous; for example any M of the form:

0 ˙yτ = M z0 +R t

for nonsingular R ∈ Rn×n satisﬁes the condition. Contrast this to vanilla gradient ﬂow 

0

M =(cid:20)R

0 R−T(cid:21)

˙yτ = −∇ ¯H(yτ ) = −M −T ∇H(M −1yτ ) = M −T ˙zτ .

Again setting z0 = M −1y0 we obtain yt = y0 +R t

case that M T M = I  i.e.  M is orthogonal.

0 ˙yτ = M z0 +R t

0 M −T ˙zτ 6= M zt except in the

2.2 Discretizations

There are many possible ways to discretize the Hamiltonian descent equations  see  e.g.  [20]. Here
we present two simple approaches and prove their convergence under certain conditions. Later we
shall show that other discretizations correspond to already known algorithms.

2.2.1

Implicit

Consider the following implicit discretization of (2)  for some ǫ > 0 we take

zk+1 = zk + ǫ(J∇H(zk+1) + z⋆ − zk+1).

(4)

Consider the change in Hamiltonian value at iteration k  ∆k = H(zk+1) − H(zk):

∆k ≤ ∇H(zk+1)T (zk+1 − zk) = ǫ∇H(zk+1)T (J∇H(zk+1) + z⋆ − zk+1) ≤ −ǫH(zk+1)

since J is skew-symmetric  H(z⋆) = 0 and H is convex. From this we have H(zk) ≤ (1+ǫ)−kH(z0).
Thus the implicit discretization exhibits linear convergence in discrete-time  without restriction on the
step-size ǫ. However  this scheme is very difﬁcult to implement in practice  since it requires solving a
non-linear equation for zk+1 at every step.

2.2.2 Explicit

Now consider the explicit discretization

zk+1 = zk + ǫ(J∇H(zk) + z⋆ − zk) 

(5)

this differs from the implicit discretization in that the right hand side depends solely on zk rather than
zk+1  and therefore is much more practical to implement. If we assume that the gradient of H is
L-Lipschitz  then we can show that this sequence converges and that the Hamiltonian converges to
zero like O(1/k). The proof of this result is included in the appendix. If  in addition  H is µ > 0
strongly convex  then we can show that the Hamiltonian converges to zero like O(λk) for some λ < 1.
The proof of this result  along the explicit dependence of λ on L and µ is given in the appendix.

We must mention here that both proofs are somewhat lacking. For example  under the assumptions
of L-Lipschitzness of ∇H and µ strong convexity of H  our analysis requires that the step-size ǫ
depend on both L and µ. This is a stronger requirement than the classical gradient descent analysis.
Moreover  the rate λ scales poorly with the condition number L/µ as compared to gradient descent.
This may be due to the fact that both analyses depend strongly on the values of L or µ  which are not
invariant to afﬁne transformation even though the equations of motion are. We suspect that a tighter
analysis is possible under assumptions whose structure mirror the afﬁne invariance structure of the
dynamics.

3 Composite optimization

Now we come to the main problem we investigate in this paper. Consider a convex optimization
problem consisting of the sum of two convex  closed  proper functions h : Rn → R and g : Rm → R:

minimize

f (y) := h(Ay) + g(y)

(6)

4

over variable y ∈ Rm  with data matrix A ∈ Rn×m. This problem is sometimes referred to as a
composite optimization problem  see  e.g.  [21]. The dual problem is given by

maximize d(p) := −h∗(−p) − g∗(AT p) 

(7)

over p ∈ Rn. We assume that h and g∗ are both differentiable  which will help ensure that the
Hamiltonian we derive satisﬁes assumption 1. Weak duality tells us that for any y  p we have
f (y) ≥ d(p)  with equality if and only if y and p are primal-dual optimal  since strong duality always
holds for this problem (under mild technical conditions [22  §5.2.3]). We can rewrite the primal and
dual problems in equality constrained form:

minimize h(x) + g(y)
subject to x = Ay 

maximize −h∗(−p) − g∗(q)
subject to

q = AT p 

and obtain necessary and sufﬁcient optimality conditions in terms of all four variables:

∇g∗(q⋆) − y⋆ = 0
Ay⋆ − x⋆ = 0
−∇h(x⋆) − p⋆ = 0
AT p⋆ − q⋆ = 0 

(8)

(9)

the proof of which is included in the appendix.

3.1 Duality gap as Hamiltonian

In this section we derive a partial duality gap for problem (8) and use it as our Hamiltonian function
to derive equations of motion. Then we shall show that in the limit the equations we derive satisfy the
conditions necessary and sufﬁcient for optimality (9). We start by introducing dual variable p for the
equality constraint in the primal problem (8) to obtain h(x) + g(y) + pT (x − Ay)  and taking the
Legendre transform of g we get the ‘full’ Lagrangian in terms of all four primal and dual variables:

L(x  y  p  q) = h(x) − g∗(q) + yT q + pT (x − Ay) 

which is convex-concave in (x  y) and (p  q). We refer to this as the full Lagrangian  because if
we maximize over (p  q) we recover the primal problem in (8) and if we minimize over (x  y) we
recover the dual problem in (8). Denote by (y⋆  p⋆) any primal-dual optimal point and let x⋆ = Ay⋆ 
q⋆ = AT p⋆  and f⋆ = f (y⋆) = d(p⋆)  then a simple calculation yields

L(x⋆  y⋆  p  q) ≤ max
p q

L(x⋆  y⋆  p  q) = f⋆ = min
x y

L(x  y  p⋆  q⋆) ≤ L(x  y  p⋆  q⋆).

This is due to strong duality holding for this problem.
In other words  if we substitute in the
optimal primal or dual variables into the Lagrangian  then we obtain valid lower and upper bounds
respectively. Then maximizing and minimizing these bounds over the remaining variables yields the
optimal objective value  f⋆. Thus  the difference between these two functions is a partial duality gap
(though uncomputable without knowledge of a primal-dual optimal point) 

gap(x  q) = L(x  y  p⋆  q⋆) − L(x⋆  y⋆  p  q)

= h(x) − h(x⋆) + g∗(q) − g∗(q⋆) + xT p⋆ − qT y⋆
≥ 0 

(10)

with equality only when the Lagrangians are equal  i.e.  are optimal. Note that the gap only depends on
x  q  because the effect of y and p is cancelled out. This gap can also be written in terms of Bregman
divergences  where the Bregman divergence between points u and v induced by a differentiable convex
function h is deﬁned as Dh(u  v) = h(u) − h(v) − ∇h(v)T (u − v)  which is always nonnegative
due the convexity of h. Though not a true distance metric  it does have some useful ‘distance-like’
properties [23  24]. We show in the appendix that our partial duality gap can be rewritten as

gap(x  q) = Dh(x  x⋆) + Dg∗ (q  q⋆).

In other words  the gap also corresponds to a ‘distance’ between the current iterates and their optimal
values  as induced by the functions h and g∗. Furthermore  we show in the appendix that this partial
duality gap is a lower bound on the full duality gap  i.e. 

f (y) − d(p) ≥ gap(Ay  AT p).

5

The gap is not in the form of a Hamiltonian  since the variable x and q are of different dimension. We
can reparameterize q = AT p or x = Ay  which yields two possible Hamiltonians  one in dimension
n and one in dimension m. The ﬁrst of which is

H(x  p) = gap(x  AT p) = h(x) − h(x⋆) + g∗(AT p) − g∗(AT p⋆) + xT p⋆ − pT x⋆.

(11)

Due to the assumptions on h and g∗ we know that H is convex and differentiable  and evidently
H(x  p) ≥ H(x⋆  p⋆) = 0. This Hamiltonian function combined with the equations of motion in
equation (2) yields dynamics

˙xt = ∇pH(xt  pt) + x⋆ − xt = A∇g∗(AT pt) − xt
˙pt = −∇xH(xt  pt) + p⋆ − pt = −∇h(xt) − pt.

(12)

We could rewrite these equations as

∇g∗(qt) − yt = 0
Ayt − xt = ˙xt
−∇h(xt) − pt = ˙pt
AT pt − qt = 0 

If ˙xt → 0 and ˙pt → 0  then the above equations converge to the conditions necessary and sufﬁcient
for optimality  as given in equation (9). This convergence could be guaranteed by theorem 1  when H
has a unique minimum (and thus satisﬁes all of assumption 1). Still  we suspect it is possible to prove
the convergence of the system without this requirement on H’s minima.

The second Hamiltonian is given by

H(y  q) = gap(Ay  q) = h(Ay) − h(Ay⋆) + g∗(q) − g∗(q⋆) + yT q⋆ − qT y⋆

(13)

which yields equations of motion

˙yt = ∇qH(yt  qt) + y⋆ − yt = ∇g∗(qt) − yt
˙qt = −∇yH(yt  qt) + q⋆ − qt = −AT ∇h(Ayt) − qt 

(14)

or equivalently

∇g∗(qt) − yt = ˙yt
Ayt − xt = 0
−∇h(xt) − pt = 0
AT pt − qt = ˙qt.

Again  if ˙yt → 0 and ˙qt → 0  this system will also satisfy the optimality conditions of (9). Finally 
theorem 1 implies that both of these ODEs exhibit linear convergence of the Hamiltonian  i.e.  linear
convergence of the partial duality gap (10)  to zero.

4 Connection to other methods

4.1 ADMM

In this section we show how a particular discretization of our ODE yields the well-known Alternating
direction method of multipliers algorithm (ADMM) [25  26] when applied to problem (6). We
should note that in related work the authors of [27] derive a different ODE that when discretized also
yields ADMM  as well as a related ODE that corresponds to accelerated ADMM [28]. There is no
contradiction here since many ODEs can correspond to the same procedure when discretized.

In order to prove that ADMM is equivalent to a discretization of Hamiltonian descent we will require
the generalized Moreau decomposition  which we present next. In the statement of the lemma
we use the notation (A∂f AT ) to represent the multi-valued operator deﬁned as (A∂f AT )(x) =
A(∂f (AT x)) = {Az | z ∈ ∂f (AT x)}.
Lemma 1. For convex  closed  proper function f : Rm → R and matrix A ∈ Rn×m  any point
x ∈ Rn satisﬁes

x = (I + ρA∂f AT )−1x + ρA(∂f ∗ + ρAT A)−1AT x.

6

We defer the proof to the appendix. To derive ADMM we employ a standard trick in discretizing
differential equations: We add and subtract a term to the dynamics which we shall discretize at
different points  which in the limit of inﬁnitesimal step size will vanish  recovering the original ODE.
Starting from equation (12) and for any ρ > 0 the modiﬁed ODE is

˙pt = −∇h(xt) − pt − ρ(xt − xt)
˙xt = A∇g∗(AT pt) − xt + (1/ρ)(pt − pt).

Now we discretize as follows:

(pk − pk−1)/ǫ = −∇h(xk+1) − pk − ρ(xk+1 − xk)
(xk+1 − xk)/ǫ = A∇g∗(AT pk+1) − xk + (1/ρ)(pk+1 − pk).

Setting ǫ = 1 yields

xk+1 = (ρI + ∇h)−1(ρxk − 2pk + pk−1)
pk+1 = (I + ρA∇g∗AT )−1(pk + ρxk+1)

= pk + ρxk+1 − ρA(∂g + ρAT A)−1AT (pk + ρxk+1)
= pk + ρxk+1 − ρAyk+1

where we used the generalized Moreau decomposition and introduced variable sequence yk ∈ Rm 
and note that from the last equation we have that ρxk − 2pk + pk−1 = ρAyk − pk. Finally this brings
us to ADMM; from any initial y0  p0 iterate

xk+1 = (ρI + ∇h)−1(ρAyk − pk)
yk+1 ∈ (ρAT A + ∂g)−1AT (pk + ρxk+1)
pk+1 = pk + ρ(xk+1 − Ayk+1).

Evidently we have lost the afﬁne invariance property of our ODE. However we might expect
ADMM to be somewhat more robust to conditioning than gradient descent  which appears to be true
empirically [25].

4.2 PDHG

The primal-dual hybrid gradient technique (PDHG)  also called Chambolle-Pock  is another operator
splitting technique with a slightly different form to ADMM. In particular  PDHG only requires
multiplies with A and AT rather than requiring A in the proximal step [29  30  31]. When applied to
problem (6) PDHG yields the following iterates

pk+1 = −(I + ρ∂h∗)−1(ρAyk − pk)
yk+1 = (I + σ∂g)−1(σAT pk+1 + yk).

In the appendix we show that this corresponds to a particular discretization of Hamiltonian descent 
with step size ǫ = 1. Note that the sign of the dual variable pk is different when compared to [31] 
this is due to the fact that the dual problem they consider negates the dual variable when compared to
ours  so this is ﬁxed by rewriting the iterations in terms of −pk.

5 Numerical experiments

In this section we present two numerical examples where we compare the explicit discretization of
Hamiltonian descent ﬂow to gradient descent. Due to the afﬁne invariance property of Hamiltonian
descent we expect our technique to outperform when the conditioning of the problem is poor  so we
generate examples with bad conditioning to test that.

5.1 Regularized least-squares

Consider the following ℓ2-regularized least-squares problem

minimize

(1/2)kAy − bk2

2 + (λ/2)kByk2
2 

(15)

7

2 and g(y) = λkByk2

over variable y ∈ Rm  where A ∈ Rn×m  B ∈ Rm×m  and λ ≥ 0 are data. In the notation of problem
(6) we let h(x) = (1/2)kx − bk2
2  and so ∇g∗(q) = argmaxy(yT q − λkByk2
2)
which we assume is always well-deﬁned (i.e.  BT B is invertible). We apply the explicit discretization
(5) of the dynamics given in equation (14) to this problem. To demonstrate the practical effect of
afﬁne invariance  we randomly generate a nonsingular matrix M and solve a sequence of optimization
problems where A is replaced with ˆAj = AM j and B is replaced with ˆBj = BM j for j =
0  1  . . .   jmax. Note that the optimal objective value of this perturbed problem is unchanged from the
original  and the solution for each perturbed problem can be obtained by (ˆy⋆)j = M −jy⋆  where y⋆
solves the original problem (i.e.  with j = 0). However  the conditioning of the problem is changed
- M is selected so that the conditioning of the data is worsening for increasing j. We compare our
algorithm to vanilla gradient descent  to proximal gradient descent [32] (where the prox-step is on the
g term so it is of a similar cost to our method)  and to restarted accelerated gradient descent [6  33] 
and observe the effect of the worsening conditioning.

j

ˆAj + λ ˆBT

We chose m = n = 1000 and for simplicity we chose B = I  λ = 1  and randomly generated
each entry in A to be IID N (0  1). The best step size was chosen via exhaustive search for all three
algorithms. The matrix M was randomly generated but chosen in such a way so as to be close to
the identity. For j = 0 the condition number of the matrix ˆAT
ˆBj was 4.0 × 103  and for
j
j = jmax = 20 the condition number had grown to 2.2 × 1014  a dramatic increase. Figure (1a)
shows the performance of both our technique and gradient descent on this sequence of problems. The
gradient descent traces are in orange  with a different trace for each j. The fastest converging trace
corresponds to j = 0  the best conditioned problem. As the conditioning deteriorates the convergence
is impacted  getting slower with each increase in j. In the appendix we additionally include Figure 3
which compares our technique to proximal gradient  restarted accelerated gradient  and conjugate
gradient. All three additional techniques display the same deterioration as the conditioning worsens.
By problem j = 20 no variant of gradient descent or conjugate gradient has reduced the primal
objective error  deﬁned as mink(f (yk) − f⋆)  to under O(100). By contrast  our technique is
completely unaffected by the changing data  with every trace essentially identical (up to some
numerical tolerances). Furthermore  we used the exact same step size for every run of our method.
This is because the discretization procedure preserved the afﬁne invariance of the continuous ODE it
is approximating  so the changing conditioning of the data has no effect. In Figure (1b) we plot the
Hamiltonian (13) (i.e.  the partial duality gap) and the full duality gap: f (yk)−d(pk)  for Hamiltonian
descent for each value of j. Once again the traces lie directly on top of each other  until numerical
errors start to have an impact. We note that the Hamiltonian decreases at each iteration  and converges
linearly. The duality gap and the objective values do not necessarily decrease at each iteration  but do
appear to enjoy linear convergence for each j.

(a) Primal objective value.

(b) Hamiltonian value and duality gap for HD.

Figure 1: Comparison of Hamiltonian descent (HD) and Gradient descent (GD) for problem (15).

5.2 Elastic net regularized logistic regression

In logistic regression the goal is to learn a classiﬁer to separate a set of data points based on their
labels  which we take to be either 1 or −1. The elastic net is a type of regularization that promotes
sparsity and small weights in the solution [34]. Given data points ai ∈ Rm with corresponding label

8

050100150200250k103102101100101102f(yk)fHDGD050100150200250k102100102104106HamiltonianDuality gapFigure 2: Comparison of Hamiltonian descent (HD) and Gradient descent (GD) for problem (16).

li ∈ {−1  1} for i = 1  . . .   n  the elastic net regularized logistic regression problem is given by

minimize

(1/n)Pn

(16)
over the variable y ∈ Rm  where λ1 ≥ 0  and λ2 ≥ 0 control the strength of the regularization. In
i=1 log(1 + exp(lixi)) and g(y) = λ1kyk1 +
2. We have a closed form expression for the gradient of g∗ given by the soft-thresholding

i y)) + λ1kyk1 + (λ2/2)kyk2
2

i=1 log(1 + exp(liaT

(λ2/2)kyk2
operator:

the notation of problem (6) we take h(x) = (1/n)Pn
(∇g∗(q))i = (1/λ2)( qi − λ1

0
qi + λ1

qi ≥ λ1
|qi| ≤ λ1
qi ≤ −λ1.

We compare the explicit discretization (5) of Hamiltonian descent in equation (14) to proximal
gradient descent [32]  which in this case has the exact same per-iteration cost since it also relies on
taking the gradient of h and applying the soft-thresholding operator. We chose dimension m = 500
and n = 1000 data points and we set λ1 = λ2 = 0.01. The data were generated randomly  and
then perturbed so as to give a high condition number  which was 1.0 × 108. The best step size for
both algorithms was found using exhaustive search. In Figure 2 we show the primal objective value
error for both algorithms  where the true solution was found using convex cone solver SCS [35  36].
Hamiltonian descent dramatically outperforms gradient descent on this problem  despite having the
same per-iteration cost. This is unsurprising because we would expect Hamiltonian descent to be less
sensitive to the poor conditioning of the data  due to the afﬁne invariance property.

6 Conclusion

Starting from Hamiltonian mechanics in classical physics  we derived a Hamiltonian descent continu-
ous ODE that converges linearly to a minimum of the Hamiltonian function. We applied Hamiltonian
descent to a convex composite optimization problem  and proved linear convergence of the duality
gap  a measure of how far from optimal a primal-dual point is. In some sense applying Hamiltonian
descent to this problem is natural  since we can identify one of the terms in the objective as being
the ‘potential’ energy and the other as the ‘kinetic’ energy. We provided two discretizations that are
guaranteed to converge to the optimum under certain assumptions  and also demonstrated that some
well-known algorithms correspond to other discretizations of our ODE. In particular we show that a
particular discretization yields ADMM. We conclude with two numerical examples that show our
method is much more robust to numerical conditioning than standard gradient methods.

References

[1] Sir William Rowan Hamilton. On a general method in dynamics. Philosophical Transactions of

the Royal Society  2:247–308  1834.

[2] Juan Peypouquet and Sylvain Sorin. Evolution equations for maximal monotone operators:

Asymptotic analysis in continuous and discrete time. arXiv preprint arXiv:0905.1270  2009.

[3] Pascal Bianchi  Walid Hachem  and Adil Salim. A constant step forward-backward algorithm

involving random maximal monotone operators. arXiv preprint arXiv:1702.04144  2017.

9

010002000300040005000600070008000k1010108106104102f(yk)fHDGD[4] Laurent Condat. A primal–dual splitting method for convex optimization involving Lipschitzian 
proximable and linear composite terms. Journal of Optimization Theory and Applications 
158(2):460–479  2013.

[5] Weijie Su  Stephen Boyd  and Emmanuel J Candès. A differential equation for modeling
Nesterov’s accelerated gradient method: theory and insights. Journal of Machine Learning
Research  17(1):5312–5354  2016.

[6] Yurii Nesterov. A method of solving a convex programming problem with convergence rate

o(1/k2). Soviet Mathematics Doklady  27(2):372–376  1983.

[7] Hedy Attouch  Zaki Chbani  and Hassan Riahi. Rate of convergence of the Nesterov accelerated
gradient method in the subcritical case α ≤ 3. ESAIM: Control  Optimisation and Calculus of
Variations  25:2  2019.

[8] Andre Wibisono  Ashia C Wilson  and Michael I Jordan. A variational perspective on accelerated
methods in optimization. Proceedings of the National Academy of Sciences  113(47):E7351–
E7358  2016.

[9] Ashia Wilson  Lester Mackey  and Andre Wibisono. Accelerating rescaled gradient descent.

arXiv preprint arXiv:1902.08825  2019.

[10] Ashia C Wilson  Benjamin Recht  and Michael I Jordan. A Lyapunov analysis of momentum

methods in optimization. arXiv preprint arXiv:1611.02635  2016.

[11] Michael Betancourt  Michael I Jordan  and Ashia C Wilson. On symplectic optimization. arXiv

preprint arXiv:1802.03653  2018.

[12] Chris J Maddison  Daniel Paulin  Yee Whye Teh  Brendan O’Donoghue  and Arnaud Doucet.

Hamiltonian descent methods. arXiv preprint arXiv:1809.05042  2018.

[13] Guilherme França  Jeremias Sulam  Daniel P Robinson  and René Vidal. Conformal symplectic

and relativistic optimization. arXiv preprint arXiv:1903.04100  2019.

[14] RT Rockafellar. Saddle points of Hamiltonian systems in convex problems of lagrange. Journal

of Optimization Theory and Applications  12(4):367–390  1973.

[15] Radford M Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte

Carlo  pages 113–162  2011.

[16] David Balduzzi  Sebastien Racaniere  James Martens  Jakob Foerster  Karl Tuyls  and Thore
Graepel. The mechanics of n-player differentiable games. In International Conference on
Machine Learning  pages 363–372  2018.

[17] Ralph Tyrell Rockafellar. Convex analysis. Princeton university press  1970.

[18] Thomas Hakon Gronwall. Note on the derivatives with respect to a parameter of the solutions

of a system of differential equations. Annals of Mathematics  pages 292–296  1919.

[19] Jean-Jacques E Slotine  Weiping Li  et al. Applied nonlinear control  volume 199. Prentice hall

Englewood Cliffs  NJ  1991.

[20] Ernst Hairer  Christian Lubich  and Gerhard Wanner. Geometric numerical integration:
structure-preserving algorithms for ordinary differential equations  volume 31. Springer
Science & Business Media  2006.

[21] Yu Nesterov. Gradient methods for minimizing composite functions. Mathematical Program-

ming  140(1):125–161  2013.

[22] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press 

2004.

[23] Lev M Bregman. The relaxation method of ﬁnding the common point of convex sets and
its application to the solution of problems in convex programming. USSR computational
mathematics and mathematical physics  7(3):200–217  1967.

10

[24] Heinz H Bauschke and Jonathan M Borwein. Legendre functions and the method of random

bregman projections. Journal of Convex Analysis  4(1):27–67  1997.

[25] Stephen Boyd  Neal Parikh  Eric Chu  Borja Peleato  Jonathan Eckstein  et al. Distributed opti-
mization and statistical learning via the alternating direction method of multipliers. Foundations
and Trends R(cid:13) in Machine learning  3(1):1–122  2011.

[26] Bingsheng He and Xiaoming Yuan. On the o(1/n) convergence rate of the Douglas–Rachford

alternating direction method. SIAM Journal on Numerical Analysis  50(2):700–709  2012.

[27] Guilherme Franca  Daniel Robinson  and Rene Vidal. Admm and accelerated admm as continu-
ous dynamical systems. In International Conference on Machine Learning  pages 1554–1562 
2018.

[28] Tom Goldstein  Brendan O’Donoghue  Simon Setzer  and Richard Baraniuk. Fast alternating

direction optimization methods. SIAM Journal on Imaging Sciences  7(3):1588–1623  2014.

[29] Mingqiang Zhu and Tony Chan. An efﬁcient primal-dual hybrid gradient algorithm for total

variation image restoration. UCLA CAM report  34  2008.

[30] Ernie Esser  Xiaoqun Zhang  and Tony Chan. A general framework for a class of ﬁrst order

primal-dual algorithms for tv minimization. 2009.

[31] Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm for convex problems
with applications to imaging. Journal of mathematical imaging and vision  40(1):120–145 
2011.

[32] Neal Parikh  Stephen Boyd  et al. Proximal algorithms. Foundations and Trends R(cid:13) in Optimiza-

tion  1(3):127–239  2014.

[33] Brendan O’Donoghue and Emmanuel Candes. Adaptive restart for accelerated gradient schemes.

Foundations of computational mathematics  15(3):715–732  2015.

[34] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of

the royal statistical society: series B (statistical methodology)  67(2):301–320  2005.

[35] B. O’Donoghue  E. Chu  N. Parikh  and S. Boyd. Conic optimization via operator splitting
and homogeneous self-dual embedding. Journal of Optimization Theory and Applications 
169(3):1042–1068  June 2016.

[36] B. O’Donoghue  E. Chu  N. Parikh  and S. Boyd. SCS: Splitting conic solver  version 2.1.0.

❤tt♣s✿✴✴❣✐t❤✉❜✳❝♦♠✴❝✈①❣r♣✴s❝s  November 2017.

11

,Brendan O'Donoghue
Chris Maddison