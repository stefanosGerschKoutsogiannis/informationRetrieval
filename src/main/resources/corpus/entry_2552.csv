2018,Implicit Bias of Gradient Descent on Linear Convolutional Networks,We show that gradient descent on full-width linear convolutional networks of depth $L$ converges to a linear predictor related to the $\ell_{2/L}$ bridge penalty in the frequency domain. This is in contrast to linearly fully connected networks  where gradient descent converges to the hard margin linear SVM solution  regardless of depth.,Implicit Bias of Gradient Descent on Linear

Convolutional Networks

Suriya Gunasekar
TTI at Chicago  USA
suriya@ttic.edu

Jason D. Lee

USC Los Angeles  USA

jasonlee@marshall.usc.edu

Daniel Soudry
Technion  Israel

daniel.soudry@gmail.com

Nathan Srebro

TTI at Chicago  USA

nati@ttic.edu

Abstract

We show that gradient descent on full width linear convolutional networks of depth
L converges to a linear predictor related to the (cid:96)2/L bridge penalty in the frequency
domain. This is in contrast to fully connected linear networks  where regardless of
depth  gradient descent converges to the (cid:96)2 maximum margin solution.

1

Introduction

Implicit biases introduced by optimization algorithms play an crucial role in learning deep neural net-
works [Neyshabur et al.  2015b a  Hochreiter and Schmidhuber  1997  Keskar et al.  2016  Chaudhari
et al.  2016  Dinh et al.  2017  Andrychowicz et al.  2016  Neyshabur et al.  2017  Zhang et al.  2017 
Wilson et al.  2017  Hoffer et al.  2017  Smith  2018]. Large scale neural networks used in practice
are highly over-parameterized with far more trainable model parameters compared to the number of
training examples. Consequently  optimization objectives for learning such high capacity models
have many global minima that ﬁt training data perfectly. However  minimizing the training loss
using speciﬁc optimization algorithms take us to not just any global minima  but some special global
minima  e.g.  global minima minimizing some regularizer R(β). In over-parameterized models 
specially deep neural networks  much  if not most  of the inductive bias of the learned model comes
from this implicit regularization from the optimization algorithm. Understanding the implicit bias 
e.g.  via characterizing R(β)  is thus essential for understanding how and what the model learns.
For example  in linear regression we understand how minimizing an under-determined model (with
more parameters than samples) using gradient descent yields the minimum (cid:96)2 norm solution  and for
linear logistic regression trained on linearly separable data  Soudry et al. [2017] recently showed that
gradient descent converges in the direction of the hard margin support vector machine solution  even
though the norm or margin is not explicitly speciﬁed in the optimization problem. Such minimum
norm or maximum margin solutions are of course very special among all solutions or separators
that ﬁt the training data  and in particular can ensure generalization Bartlett and Mendelson [2003] 
Kakade et al. [2009].
Changing the optimization algorithm  even without changing the model  changes this implicit bias 
and consequently also changes generalization properties of the learned models [Neyshabur et al. 
2015a  Keskar et al.  2016  Wilson et al.  2017  Gunasekar et al.  2017  2018]. For example  for linear
logistic regression  using coordinate descent instead of gradient descent return a maximum (cid:96)1 margin
solution instead of the hard margin support vector solution solution—an entirely different inductive
bias Telgarsky [2013]  Gunasekar et al. [2018].

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Similarly  and as we shall see in this paper  changing to a different parameterization of the same model
class can also dramatically change the implicit bias Gunasekar et al. [2017]. In particular  we study
the implicit bias of optimizing multi-layer fully connected linear networks  and linear convolutional
networks (multiple full width convolutional layers followed by a single fully connected layer) using
gradient descent. Both of these types of models ultimately implement linear transformations  and
can implement any linear transformation. The model class deﬁned by these networks is thus simply
the class of all linear predictors  and these models can be seen as mere (over) parameterizations
of the class of linear predictors. Minimizing the training loss on these models is therefore entirely
equivalent to minimizing the training loss for linear classiﬁcation. Nevertheless  as we shall see 
optimizing these networks with gradient descent leads to very different solutions.
In particular  we show that for fully connected networks with single output  optimizing the exponential
loss over linearly separable data using gradient loss again converges to the homogeneous hard margin
support vector machine solution. This holds regardless of the depth of the network  and hence  at least
with a single output  gradient descent on fully connected networks has the same implicit bias as direct
gradient descent on the parameters of the linear predictor. In contrast  training a linear convolutional
network with gradient descent biases us toward linear separators that are sparse in the frequency
domain. Furthermore  this bias changes with the depth of the network  and a network of depth L

(with L − 1 convolutional layers)  implicitly biases towards minimizing the (cid:107)(cid:98)β(cid:107)2/L bridge penalty
with 2/L ≤ 1 of the Fourier transform(cid:98)β of the learned linear predictor β subject to margin constraints
(the gradient descent predictor reaches a stationary point of the (cid:107)(cid:98)β(cid:107)2/L minimization problem). This

is a sparsity inducing regularizer  which induces sparsity more aggressively as the depth increases.
Finally  in this paper we focus on characterizing which global minimum does gradient descent on
over-parameterized linear models converge to  while assuming that for appropriate choice of step sizes
gradient descent iterates asymptotically minimize the optimization objective. A related challenge in
neural networks  not addressed in this paper  is an answer to when does gradient descent minimize
the non-convex empirical loss objective to reach a global minimum. This problem while hard in
worst case  has been studied for linear networks. Recent work have concluded that with sufﬁcient
over-parameterization (as is the case with our settings)  loss landscape of linear models are well
behaved and all local minima are global minima making the problem tractable Burer and Monteiro
[2003]  Journée et al. [2010]  Kawaguchi [2016]  Nguyen and Hein [2017]  Lee et al. [2016].

Notation We typeface vectors with bold characters e.g.  w  β  x. Individual entries of a vector
z ∈ RD are indexed using 0 based indexing as z[d] for d = 0  1  . . .   D − 1. Complex numbers
are represented in the polar form as z = |z|eiφz with |z| ∈ R+ denoting the magnitude of z and
φz ∈ [0  2π) denoting the phase. z∗ = |z|e−iφz denotes the complex conjugate of z. The complex
∗. The Dth complex

inner product between z  β ∈ CD is given by (cid:104)z  β(cid:105) =(cid:80)D
representation of z in the discrete Fourier basis given by (cid:98)z[d] = 1√
D and a  we denote the modulo operator as a mod D = a − D(cid:4) a
D . For integers
networks (formally deﬁned in Section 2)  we will use w ∈ W to denote parameters of the model in
general domain W  and βw or simply β to denote the equivalent linear predictor.

D . For z ∈ RD we use the notation(cid:98)z ∈ CD to denote the
(cid:80)D−1
(cid:5). Finally  for multi-layer linear

root of 1 is denoted by ωD = e− 2πi

∗

[d] = z(cid:62)β

d=1 z[d]β

p=0 z[p]ωpd

D

D

2 Multi-layer Linear Networks
We consider feed forward linear networks that map input features x ∈ RD to a single real valued
output fw(x) ∈ R  where w denote the parameters of the network. Such networks can be thought
of as directed acyclic graphs where each edge is associated with a weight  and the value at each
node/unit is the weighted sum of values from the parent nodes. The input features form source nodes
with no incoming edges and the output is a sink node with no outgoing edge. Every such network
realizes a linear function x → (cid:104)x  βw(cid:105)  where βw ∈ RD denotes the effective linear predictor.
In multi-layer networks  the nodes are arranged in layers  so an L–layer network represents a
composition of L linear maps. We use the convention that  the input x ∈ RD is indexed as the zeroth
layer l = 0  while the output forms the ﬁnal layer with l = L. The outputs of nodes in layer l are
denoted by hl ∈ RDl  where Dl is the number of nodes in layer l. We also use wl to denote the

2

parameters of the linear map between hl−1 and hl  and w = [wl]L
all parameters of the linear network.
Linear fully connected network In a fully connected linear network  the nodes between successive
layers l − 1 and l are densely connected with edge weights wl ∈ RDl−1×Dl  and all the weights are
RDl−1×Dl and
independent parameters. This model class is parameterized by w = [wl]L
the computation for intermediate nodes hl and the composite linear map fw(x) is given by 

l=1 to denote the collective set of

l=1 ∈(cid:81)L

l=1

hl = w(cid:62)

l hl−1

and

fw(x) = hL = w(cid:62)

L w(cid:62)

L−1 . . . w(cid:62)
1 x.

(1)

Linear convolutional network We consider one-dimensional convolutional network architectures
where each non-output layer has exactly D units (same as the input dimensionality) and the linear
transformations from layer l − 1 to layer l are given by the following circular convolutional operation1
parameterized by full width ﬁlters with weights [wl ∈ RD]L−1

l=1 . For l = 1  2  . . .   L − 1 

wl[k] hl−1 [(d + k) mod D] := (hl−1 (cid:63) wl) [d].

(2)

D−1(cid:88)

k=0

hl[d] =

1√
D

l=1 ∈(cid:81)L

(cid:62)

The output layer is fully connected and parameterized by weights wL ∈ RD. The parameters of the
RD 
model class therefor consists of L vectors of size D collectively denoted by w = [wl]L
and the composite linear map fw(x) is given by:

l=1

wL.

fw(x) = ((((x (cid:63) w1) (cid:63) w2) . . .) (cid:63) wL−1)

(3)
√
Remark: We use circular convolution with a scaling of 1/
D to make the analysis cleaner. For
convolutions with zero-padding  we expect a similar behavior. Secondly  since our goal here to study
implicit bias in sufﬁciently over-parameterized models  we only study full dimensional convolutional
ﬁlters. In practice it is common to have ﬁlters of width K smaller than the number of input features 
which can change the implicit bias.
The fully connected and convolutional linear networks described above can both be represented in
terms of a mapping P : W → RD that maps the input parameters w ∈ W to a linear predictor in RD 
such that the output of the network is given by fw(x) = (cid:104)x P(w)(cid:105). For fully connected networks 
the mapping is given by Pf ull(w) = w1w2 . . . wL  and for convolutional networks  Pconv(w) =
  where w↓ denotes the ﬂipped vector corresponding to w 

(cid:1) . . . (cid:63) w1

(cid:16)(cid:0)(w

↓
L (cid:63) wL−1) (cid:63) wL−2

given by w↓[k] = w[D − k − 1] for k = 0  1  . . .   D − 1.
Separable linear classiﬁcation Consider a binary classiﬁcation dataset {(xn  yn)
: n =
1  2  . . . N} with xn ∈ RD and yn ∈ {−1  1}. The empirical risk minimization objective for
training a linear network parameterized as P(w) is given as follows 
(cid:96)((cid:104)xn P(w)(cid:105)  yn) 

(4)
where (cid:96) : R × {−1  1} → R+ is some surrogate loss for classiﬁcation accuracy  e.g.  logistic loss

(cid:96)((cid:98)y  y) = log(1 + exp(−(cid:98)yy)) and exponential loss (cid:96)((cid:98)y  y) = exp(−(cid:98)yy).

w∈W LP (w) :=

N(cid:88)

min

(cid:17)↓

n=1

It is easy to see that both fully connected and convolutional networks of any depth L can realize
any linear predictor β ∈ RD. The model class expressed by both networks is therefore simply
the unconstrained class of linear predictors  and the two architectures are merely different (over)
parameterizations of this class

{Pf ull(w) : w = [wl ∈ RDl−1×Dl ]L

l=1} = {Pconv(w) : w = [wl ∈ RD]L

l=1} = RD.

Thus  the empirical risk minimization problem in (4) is equivalent to the following optimization over
the linear predictors β = P(w):

L(β) :=

min
β∈RD

(cid:96)((cid:104)xn  β(cid:105)  yn).

(5)

N(cid:88)

n=1

1We follow the convention used in neural networks literature that refer to the operation in (2) as convolution 

while in the signal processing terminology  (2) is known as the discrete circular cross-correlation operator

3

Although the optimization problems (4) and (5) are exactly equivalent in terms of the set of global
minima  in this paper  we show that optimizing (4) with different parameterizations leads to very
different classiﬁers compared to optimizing (5) directly.
In particular  consider problems (4)/(5) on a linearly separable dataset {xn  yn}N
n=1 and using the
logistic loss (the two class version of the cross entropy loss typically used in deep learning). The
global inﬁmum of L(β) is 0  but this is not attainable by any ﬁnite β. Instead  the loss can be
minimized by scaling the norm of any linear predictor that separates the data to inﬁnity. Thus  any
sequence of predictors β(t) (say  from an optimization algorithm) that asymptotically minimizes the
loss in eq. (5) necessarily separates the data and diverges in norm  (cid:107)β(t)(cid:107) → ∞. In general there
are many linear separators that correctly label the training data  each corresponding to a direction in
which we can minimize (5). Which of these separators will we converge to when optimizing (4)/(5)?
β(t)
(cid:107)β(t)(cid:107) the iterates of our optimization algorithm
In other words  what is the direction β
∞
will diverge in? If this limit exist we say that β(t) converges in direction to the limit direction β
.
Soudry et al. [2017] studied this implicit bias of gradient descent on (5) over the direct parameteriza-
tion of β. They showed that for any linearly separable dataset and any initialization  gradient descent
w.r.t. β converges in direction to hard margin support vector machine solution:

= lim
t→∞

∞

∞

β

=

β
(cid:107)β

∗
(cid:96)2
∗
(cid:96)2

(cid:107)  where β

∗
(cid:96)2

(cid:107)β(cid:107)2

2 s.t. ∀n  yn(cid:104)β  xn(cid:105) ≥ 1.

= argmin
β∈RD

(6)

In this paper we study the behavior of gradient descent on the problem (4) w.r.t different parameteri-
zations of the model class of linear predictors. For initialization w(0) and sequence of step sizes {ηt} 
gradient descent updates for (4) are given by 

w(t+1) = w(t) − ηt∇wLP (w(t)) = w(t) − ηt∇wP(w(t))∇βL(P(w(t))) 

(7)
where ∇wP(.) denotes the Jacobian of P : W → RD with respect to the parameters w  and ∇βL(.)
is the gradient of the loss function in (5).
For separable datasets  if w(t) minimizes (4) for linear fully connected or convolutional networks 
then we will again have (cid:107)w(t)(cid:107) → ∞  and the question we ask is: what is the limit direction
∞
β

P(w(t))
(cid:107)P(w(t))(cid:107) of the predictors P(w(t)) along the optimization path?

= lim
t→∞

The result in Soudry et al. [2017] holds for any loss function (cid:96)(u  y) that is strictly monotone in uy
with speciﬁc tail behavior  name the tightly exponential tail  which is satisﬁed by popular classiﬁcation
losses like logistic and exponential loss. In the rest of the paper  for simplicity we exclusively focus
on the exponential loss function (cid:96)(u  y) = exp(−uy)  which has the same tail behavior as that of the
logistic loss. Along the lines of Soudry et al. [2017]  our results should also extend for any strictly
monotonic loss function with a tight exponential tail  including logistic loss.

3 Main Results

Our main results characterize the implicit bias of gradient descent for multi-layer fully connected and
convolutional networks with linear activations. For the gradient descent iterates w(t) in eq. (7)  we
henceforth denote the induced linear predictor as β(t) = P(w(t)).
Assumptions. In the following theorems  we characterize the limiting predictor β
under the following assumptions:

= lim
t→∞

β(t)
(cid:107)β(t)(cid:107)

∞

1. w(t) minimize the objective  i.e.  LP (w(t)) → 0.
2. w(t)  and consequently β(t) = P(w(t))  converge in direction to yield a separator β

∞

=

β(t)

(cid:107)β(t)(cid:107) with positive margin  i.e.  minn yn(cid:104)xn  β

∞(cid:105) > 0.

lim
t→∞

3. Gradients with respect to linear predictors ∇βL(β(t)) converge in direction.

These assumptions allow us to focus on the question of which speciﬁc linear predictor do gradient
descent iterates converge to by separating it from the related optimization questions of when gradient
descent iterates minimize the non-convex objective in eq. (5) and nicely converge in direction.

4

(a) Fully connected network of depth L
∞ ∝
β

(cid:107)β(cid:107)2 (independent of L)

argmin

∀n  yn(cid:104)xn β(cid:105)≥1

(b) Convolutional network of depth L

∞ ∝ ﬁrst order stationary point of

argmin

∀n  yn(cid:104)xn β(cid:105)≥1

β

(cid:107)(cid:98)β(cid:107)2/L

(c) Diagonal network of depth L

∞ ∝ ﬁrst order stationary point of

argmin

∀n  yn(cid:104)xn β(cid:105)≥1

β

(cid:107)β(cid:107)2/L

Figure 1: Implicit bias of gradient descent for different linear network architectures.

Theorem 1 (Linear fully connected networks). For any depth L  almost all linearly separable
datasets {xn  yn}N
n=1  almost all initializations w(0)  and any bounded sequence of step sizes {ηt}t 
with exponential loss (cid:96)((cid:98)y  y) = exp(−(cid:98)yy) over L–layer fully connected linear networks.
consider the sequence gradient descent iterates w(t) in eq. (7) for minimizing LPf ull (w) in eq. (4)
If (a) the iterates w(t) minimize the objective  i.e.  LPf ull (w(t)) → 0  (b) w(t)  and consequently
β(t) = Pf ull(w(t))  converge in direction to yield a separator with positive margin  and (c) gradients
with respect to linear predictors ∇βL(β(t)) converge in direction  then the limit direction is given by 

∞

β

= lim
t→∞

Pf ull(w(t))
(cid:107)Pf ull(w(t))(cid:107) =

∗
(cid:96)2
∗
(cid:96)2

β
(cid:107)β

(cid:107)   where β

∗
(cid:96)2

:= argmin

w

(cid:107)β(cid:107)2

2 s.t. ∀n  yn(cid:104)xn  β(cid:105) ≥ 1.

(8)

For fully connected networks with single output  Theorem 1 shows that there is no effect of depth
on the implicit bias of gradient descent. Regardless of the depth of the network  the asymptotic
classiﬁer is always the hard margin support vector machine classiﬁer  which is also the limit direction
of gradient descent for linear logistic regression with the direct parameterization of β = w.
In contrast  next we show that for convolutional networks we get very different biases. Let us ﬁrst look
at a 2–layer linear convolutional network  i.e.  a network with single convolutional layer followed by
a fully connected ﬁnal layer.

5

D−1(cid:80)

(cid:16)− 2πipd

(cid:17)

Recall that(cid:98)β ∈ CD denote the Fourier coefﬁcients of β  i.e. (cid:98)β[d] = 1√

 
and that any non-zero z ∈ C is denoted in polar form as z = |z|eiφz for φz ∈ [0  2π). Linear
predictors induced by gradient descent iterates w(t) for convolutional networks are denoted by β(t) =
(t) converges in direction to(cid:98)β
(cid:98)β
Pconv(w(t)). It is evident that if β(t) converges in direction to β
  then its Fourier transformation
. In the following theorems  in addition to the earlier assumptions 
coordinate-wise. For coordinates d with(cid:98)β
we further assume a technical condition that the phase of the Fourier coefﬁcients eiφ(cid:98)β(t) converge
[d] (cid:54)= 0 this follows from convergence in direction of

β[p] exp

p=0

∞

∞

∞

D

D

iφ(cid:98)β(t)[d] → e

iφ(cid:98)β

∞

[d]. We assume such a φ(cid:98)β

∞

[d]

w(t)  in which case e
Theorem 2 (Linear convolutional networks of depth two). For almost all linearly separable datasets
{xn  yn}N
n=1  almost all initializations w(0)  and any sequence of step sizes {ηt}t with ηt smaller
than the local Lipschitz at w(t)  consider the sequence gradient descent iterates w(t) in eq. (7) for
minimizing LPconv (w) in eq. (4) with exponential loss over 2–layer linear convolutional networks.
If (a) the iterates w(t) minimize the objective  i.e.  LPconv (w(t)) → 0  (b) w(t) converge in direction
(t) of the
to yield a separator β
linear predictors β(t) converge coordinate-wise  i.e.  ∀d  e
[d]  and (d) the gradients
∇βL(β(t)) converge in direction  then the limit direction β

with positive margin  (c) the phase of the Fourier coefﬁcients(cid:98)β

iφ(cid:98)(cid:98)β
is given by 

[d] = 0.

∞

also exists when(cid:98)β

∞

∞

iφ(cid:98)β(t)[d] → e
∞
(cid:107)(cid:98)β(cid:107)1 s.t. ∀n  yn(cid:104)β  xn(cid:105) ≥ 1.

∞

β

=

β
(cid:107)β

∗
F  1
F  1(cid:107)   where β
∗

∗
F  1 := argmin

β

We already see how introducing a single convolutional layer changes the implicit bias of gradient
descent—even without any explicit regularization  gradient descent on the parameters of convolutional
network architecture returns solutions that are biased to have sparsity in the frequency domain.
Furthermore  unlike fully connected networks  for convolutional networks we also see that the implicit
bias changes with the depth of the network as shown by the following theorem.
Theorem 2a (Linear Convolutional Networks of any Depth). For any depth L  under the conditions
Pconv(w(t))
of Theorem 2  the limit direction β
(cid:107)Pconv(w(t))(cid:107) is a scaling of a ﬁrst order stationary
point of the following optimization problem 

= lim
t→∞

∞

(9)

(10)

min

β

(cid:107)(cid:98)β(cid:107)2/L s.t. ∀n  yn(cid:104)β  xn(cid:105) ≥ 1 
(cid:16)(cid:80)D
i=1 |z[i]|p(cid:17)1/p

where the (cid:96)p penalty given by (cid:107)z(cid:107)p =
for p = 1 and a quasi-norm for p < 1.

(also called the bridge penalty) is a norm

When L > 2  and thus p = 2/L < 1  problem (10) is non-convex and intractable Ge et al. [2011].
Hence  we cannot expect to ensure convergence to a global minimum. Instead we show convergence
to a ﬁrst order stationary point of (10) in the sense of sub-stationary points of Rockafellar [1979] for
optimization problems with non-smooth and non-convex objectives. These are solutions where the
local directional derivative along the directions in the tangent cone of the constraints are all zero.
The ﬁrst order stationary points  or sub-stationary points  of (10) are the set of feasible predictors β
such that ∃{αn ≥ 0}N

n=1 satisfying the following: ∀n  yn(cid:104)xn  β(cid:105) > 1 =⇒ αn = 0  and

n

where(cid:98)xn is the Fourier transformation of xn  and ∂◦ denotes the local sub-differential (or Clarke’s
For p = 1 and(cid:98)β represented in polar form as(cid:98)β = |(cid:98)β|eiφ(cid:98)β ∈ CD  (cid:107)(cid:98)β(cid:107)p is convex and the local

sub-differential) operator deﬁned as ∂◦f (β) = conv{v : ∃(zk)k s.t. zk → β and ∇f (zk) → v}.

sub-differential is indeed the global sub-differential given by 

∂◦(cid:107)(cid:98)β(cid:107)1 = {(cid:98)z : ∀d  |(cid:98)z[d]| ≤ 1 and(cid:98)β[d] (cid:54)= 0 =⇒ (cid:98)z[d] = eiφ(cid:98)β[d]}.

(12)

(11)

(cid:88)

αnyn(cid:98)xn ∈ ∂◦(cid:107)(cid:98)β(cid:107)p 

6

For p < 1  the local sub-differential of (cid:107)(cid:98)β(cid:107)p is given by 

∂◦(cid:107)(cid:98)β(cid:107)p = {(cid:98)z :(cid:98)β[d] (cid:54)= 0 =⇒ (cid:98)z[d] = p eiφ(cid:98)β[d] |(cid:98)β[d]|p−1}.

(13)
Figures 1a–1b summarize the implications of the main results in the paper. The proof of this Theorem 
exploits the following representation of Pconv(β) in the Fourier domain.
Lemma 3. For full-dimensional convolutions  β = Pconv(w) is equivalent to

∀p < 1 

where for l = 1  2  . . .   L  (cid:98)w1 ∈ CD are the Fourier coefﬁcients of the parameters wl ∈ RD.

(cid:98)β = diag((cid:98)w1) . . . diag((cid:98)wL−1)(cid:98)wL 

From above lemma (proved in Appendix C)  we can see a connection of convolutional networks to a
special network where the linear transformation between layers is restricted to diagonal entries (see
depiction in Figure 1c)  we refer to such networks as linear diagonal network.
The proof of Theorem 1 and Theorem 2-2a are provided in Appendix B and C  respectively.

4 Understanding Gradient Descent in the Parameter Space

We can decompose the characterization of implicit bias of gradient descent on a parameterization
P(w) into two parts: (a) what is the implicit bias of gradient descent in the space of parameters w? 
and (b) what does this imply in term of the linear predictor β = P(w)  i.e.  how does the bias in
parameter space translate to the linear predictor learned from the model class?
We look at the ﬁrst question for a broad class of linear models  where the linear predictor is given by a
homogeneous polynomial mapping of the parameters: β = P(w)  where w ∈ RP are the parameters
of the model and P : RP → RD satisﬁes deﬁnition below. This class covers the linear convolutional 
fully connected networks  and diagonal networks discussed in Section 3.
Deﬁnition (Homogeneous Polynomial). A multivariate polynomial function P : RP → RD is said
to be homogeneous  if for some ﬁnite integer ν < ∞  ∀α ∈ R  v ∈ RP   P(αv) = ανP(v).
Theorem 4 (Homogeneous Polynomial Parameterization). For any homogeneous polynomial map
P : RP → RD from parameters w ∈ RD to linear predictors  almost all datasets {xn  yn}N
n=1
separable by B := {P(w) : w ∈ RP}  almost all initializations w(0)  and any bounded sequence of
step sizes {ηt}t  consider the sequence of gradient descent updates w(t) from eq. (7) for minimizing
the empirical risk objective LP (w) in (4) with exponential loss (cid:96)(u  y) = exp(−uy).
If (a) the iterates w(t) asymptotically minimize the objective  i.e.  LP (w(t)) = L(P(w(t))) → 0 
(b) w(t)  and consequently β(t) = P(w(t))  converge in direction to yield a separator with positive
margin  and (c) the gradients w.r.t. to the linear predictors  ∇βL(β(t)) converge in direction  then the
limit direction of the parameters w∞ = lim
is a positive scaling of a ﬁrst order stationary
t→∞
point of the following optimization problem 
s.t.

w(t)
(cid:107)w(t)(cid:107)2
∀n  yn(cid:104)xn P(w)(cid:105) ≥ 1.

(14)

(cid:107)w(cid:107)2

2

min
w∈RP

∞

Theorem 4 is proved in Appendix A. The proof of Theorem 4 involves showing that the asymptotic
direction of gradient descent iterates satisﬁes the KKT conditions for ﬁrst order stationary points of
(14). This crucially relies on two properties. First  the sequence of gradients ∇βL(β(t)) converge in
β(t)
direction to a positive span of support vectors of β
(cid:107)β(t)(cid:107) (Lemma 8 in Gunasekar et al.
= lim
t→∞
[2018])  and this result relies on the loss function (cid:96) being exponential tailed. Secondly  if P is not
2 s.t. ∀n (cid:104)xn  yn(cid:105) ≥ γ for different values
homogeneous  then the optimization problems minw(cid:107)w(cid:107)2
of unnormalized margin γ are not equivalent and lead to different separators. Thus  for general
non-homogeneous P  the unnormalized margin of one does not have a signiﬁcance and the necessary
conditions for the ﬁrst order stationarity of (14) are not satisﬁed.
Finally  we also note that in many cases (including linear convolutional networks) the optimization
problem (14) is non-convex and intractable (see e.g.  Ge et al. [2011]). So we cannot expect w∞
to be always be a global minimizer of eq. (14). We however suspect that it is possible to obtain
a stronger result that w∞ reaches a higher order stationary point or even a local minimum of the
explicitly regularized estimator in eq. (14).

7

Implications of the implicit bias in predictor space While eq. (14) characterizes the bias of
gradient descent in the parameter space  what we really care about is the effective bias introduced in
the space of functions learned by the network. In our case  this class of functions is the set of linear
predictors {β ∈ RD}. The (cid:96)2 norm penalized solution in eq. (14)  is equivalently given by 
(cid:107)w(cid:107)2
2.

RP (β) s.t. ∀n  yn(cid:104)β  xn(cid:105) ≥ 1  where RP (β) =

∗
RP = argmin

(15)

inf

β

w:P(w)=β

β

∗

The problems in eq. (14) and eq. (15) have the same global minimizers  i.e.  w∗ is global minimizer
= P(w∗) minimizes eq. (15). However  such an equivalence does not
of eq. (14) if and only if β
extend to the stationary points of the two problems. Speciﬁcally  it is possible that a stationary point
of eq. (14) is merely a feasible point for eq. (15) with no special signiﬁcance. So instead of using
Theorem 4  for the speciﬁc networks in Section 3  we directly show (in Appendix) that gradient
descent updates converge in direction to a ﬁrst order stationary point of the problem in eq. (15).

5 Understanding Gradient Descent in Predictor Space

In the previous section  we saw that the implicit bias of gradient descent on a parameterization
P(w) can be described in terms of the optimization problem (14) and the implied penalty function
RP (β) = minw:P(w)=β(cid:107)w(cid:107)2
2. We now turn to studying this implied penalty RP (β) and obtaining
explicit forms for it  which will reveal the precise form of the implicit bias in terms of the learned
linear predictor. The proofs of the lemmas in this section are provided in the Appendix D.
Lemma 5. For fully connected networks of any depth L > 0 
2 = L(cid:107)β(cid:107)2/L

2 = monotone((cid:107)β(cid:107)2).

RPf ull (β) =

(cid:107)w(cid:107)2

min

w:Pf ull(w)=β

∗
RPf ull

= argminβ RPf ull (β) s.t. ∀n  yn(cid:104)xn  β(cid:105) ≥ 1 in eq. (15) for fully connected
We see that β
networks is independent of the depth of the network L. In Theorem 1  we indeed show that gradient
descent for this class of networks converges in the direction of β
Next  we motivate the characterization of RP (β) for linear convolutional networks by ﬁrst look-
ing at the special linear diagonal network depicted in Figure 1c. The depth–L diagonal net-
work is parameterized by w = [wl ∈ RD]L
l=1 and the mapping to a linear predictor is given
by Pdiag(w) = diag(w1)diag(w2) . . . diag(wL−1)wL.
Lemma 6. For a depth–L diagonal network with parameters w = [wl ∈ RD]L

∗
RPf ull

l−1  we have

.

RPdiag (β) =

min

w:Pdiag(w)=β

(cid:107)w(cid:107)2

2 = L(cid:107)β(cid:107)2/L

2/L = monotone((cid:107)β(cid:107)2/L).

Finally  for full width linear convolutional networks parameterized by w = [wl ∈ RD]L
following representation of β = Pconv(w) in Fourier from Lemma 3.

l=1  recall the

where(cid:98)β (cid:98)wl ∈ CD are Fourier basis representation of β  wl ∈ RD  respectively. Extending the

result of diagonal networks for the complex vector spaces  we get the following characterization of
RPconv (β) for linear convolutional networks.
Lemma 7. For a depth–L convolutional network with parameters w = [wl ∈ RD]L

l−1  we have

(cid:98)β = diag((cid:98)w1) . . . diag((cid:98)wL−1)(cid:98)wL 

RPconv (β) =

min

w:Pconv(w)=β

6 Discussion

2 = L(cid:107)(cid:98)β(cid:107)2/L

2/L = monotone((cid:107)(cid:98)β(cid:107)2/L).

(cid:107)w(cid:107)2

In this paper  we characterized the implicit bias of gradient descent on linear convolutional networks.
We showed that even in the case of linear activations and a full width convolution  wherein the
convolutional network deﬁnes the exact same model class as fully connected networks  merely
changing to a convolutional parameterization introduces radically different  and very interesting  bias

8

when training with gradient descent. Namely  training a convolutional representation with gradient
descent implicitly biases towards sparsity in the frequency domain representation of linear predictor.
For convenience and simplicity of presentation  we studied one dimensional circular convolutions.
Our results can be directly extended to higher dimensional input signals and convolutions  including
the two-dimensional convolutions common in image processing and computer vision. We also expect
similar results for convolutions with zero padding instead of circular convolutions  although this
requires more care with analysis of the edge effects.
A more signiﬁcant way in which our setup differs from usual convolutional networks is that we use
full width convolutions  while in practice it is common to use convolutions with bounded width  much
smaller then the input dimensionality. This setting is within the scope of Theorem 4  as the linear
transformation is still homogeneous. However  understanding the implied bias in the predictor space 
i.e. understanding RP (β) requires additional work. It will be very interesting to see if restricting the
width of the convolutional network gives rise to further interesting behaviors.
Another important direction for future study is understanding the implicit bias for networks with
multiple outputs. For both fully connected and convolutional networks  we looked at networks with a
single output. With C > 1 outputs  the network implements a linear transformation x (cid:55)→ βx where
β ∈ RC×D is now a matrix. Results for matrix sensing in Gunasekar et al. [2018] imply that for
two layer fully connected networks with multiple outputs  the implicit bias is to a maximum margin
solution with respect to the nuclear norm (cid:107)β(cid:107)(cid:63). This is already different from the implicit bias of a
one-layer “network” (i.e. optimizing β directly)  which would be in terms of the Frobenius norm
(cid:107)β(cid:107)F (from the result of Soudry et al. [2017]). We suspect that with multiple outputs  as more layers
are added  even fully connected networks exhibit a shrinking sparsity penalty on the singular values
of the effective linear matrix predictor β ∈ RC×D. Precisely characterizing these biases requires
further study.
When using convolutions as part of a larger network  with multiple parallel ﬁlters  max pooling 
and non-linear activations  the situation is of course more complex  and we do not expect to get the
exact same bias. However  we do expect the bias to be at the very least related to the sparsity-in-
frequency-domain bias that we uncover here  and we hope our work can serve as a basis for further
such study. There are of course many other implicit and explicit sources of inductive bias—here we
show that merely parameterizing transformations via convolutions and using gradient descent for
training already induces sparsity in the frequency domain.
On a technical level  we provided a generic characterization for the bias of gradient descent on linear
models parameterized as β = P(w) for a homogeneous polynomial P. The (cid:96)2 bias (in parameter
space) we obtained is not surprising  but also should not be taken for granted – e.g.  the result
does not hold in general for non-homogeneous P  and even with homogeneous polynomials  the
characterization is not as crisp when other loss functions are used  e.g.  with a squared loss and matrix
factorization (a homogeneous degree two polynomial representation)  the implicit bias is much more
fragile Gunasekar et al. [2017]  Li et al. [2017]. Moreover  Theorem 4 only ensures convergence to
ﬁrst order stationary point in the parameter space  which is not sufﬁcient for convergence to stationary
points of the implied bias in the model space (eq. (15)). It is of interest for future work to strengthen
this result to show either convergence to higher order stationary points or local minima in parameter
space  or to directly show the convergence to stationary points of (15).
It would also be of interest to strengthen other technical aspects of our results: extend the results to
loss functions with tight exponential tails (including logistic loss) and handle all datasets including
the set of measure zero degenerate datasets—these should be possible following the techniques of
Soudry et al. [2017]  Telgarsky [2013]  Ji and Telgarsky [2018]. We can also calculate exact rates of
convergence to the asymptotic separator along the lines of Soudry et al. [2017]  Nacson et al. [2018] 
Ji and Telgarsky [2018] showing how fast the inductive bias from optimization kicks in and why
it might be beneﬁcial to continue optimizing even after the loss value L(β(t)) itself is negligible.
Finally  for logistic regression  Ji and Telgarsky [2018] extend the results of asymptotic convergence
of gradient descent classiﬁer to the cases where the data is not strictly linearly separable. This is an
important relaxation of our assumption on strict linear separability. More generally  for non-separable
data  we would like a more ﬁne grained analysis connecting the iterates β(t) along the optimization

path to the estimates along regularization path  (cid:98)β(c) = argminRP (β)≤c L(β)  where an explicit

regularization is added to the optimization objective.

9

References
Marcin Andrychowicz  Misha Denil  Sergio Gomez  Matthew W Hoffman  David Pfau  Tom Schaul  and Nando
de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information
Processing Systems  2016.

P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results.

Journal of Machine Learning Research  2003.

Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semideﬁnite programs

via low-rank factorization. Mathematical Programming  95(2):329–357  2003.

Pratik Chaudhari  Anna Choromanska  Stefano Soatto  Yann LeCun  Carlo Baldassi  Christian Borgs  Jennifer
Chayes  Levent Sagun  and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys.
arXiv preprint arXiv:1611.01838  2016.

Laurent Dinh  Razvan Pascanu  Samy Bengio  and Yoshua Bengio. Sharp minima can generalize for deep nets.

In International Conference on Machine Learning  2017.

Dongdong Ge  Xiaoye Jiang  and Yinyu Ye. A note on the complexity of lp minimization. Mathematical

programming  2011.

Suriya Gunasekar  Blake E Woodworth  Srinadh Bhojanapalli  Behnam Neyshabur  and Nati Srebro. Implicit

regularization in matrix factorization. In NIPS  2017.

Suriya Gunasekar  Jason D. Lee  Daniel Soudry  and Nathan Srebro. Characterizing implicit bias in terms of

optimization geometry. arXiv preprint  2018.

Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation  1997.

Elad Hoffer  Itay Hubara  and Daniel Soudry. Train longer  generalize better: closing the generalization gap in

large batch training of neural networks. In Advances in Neural Information Processing Systems  2017.

Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint

arXiv:1803.07300  2018.

Michel Journée  Francis Bach  P-A Absil  and Rodolphe Sepulchre. Low-rank optimization on the cone of

positive semideﬁnite matrices. SIAM Journal on Optimization  20(5):2327–2351  2010.

Sham M Kakade  Karthik Sridharan  and Ambuj Tewari. On the complexity of linear prediction: Risk bounds 

margin bounds  and regularization. In Advances in neural information processing systems  2009.

Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information Processing

Systems  2016.

Nitish Shirish Keskar  Dheevatsa Mudigere  Jorge Nocedal  Mikhail Smelyanskiy  and Ping Tak Peter Tang. On
large-batch training for deep learning: Generalization gap and sharp minima. In International Conference on
Learning Representations  2016.

Jason D. Lee  Max Simchowitz  Michael I. Jordan  and Benjamin Recht. Gradient descent only converges to

minimizers. In 29th Annual Conference on Learning Theory  2016.

Yuanzhi Li  Tengyu Ma  and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix recovery.

arXiv preprint arXiv:1712.09203  2017.

Marian Muresan. A concrete approach to classical analysis  volume 14. Springer  2009.

Mor Shpigel Nacson  Jason Lee  Suriya Gunasekar  Nathan Srebro  and Daniel Soudry. Convergence of gradient

descent on separable data. arXiv preprint arXiv:1803.01905  2018.

Behnam Neyshabur  Ruslan R Salakhutdinov  and Nati Srebro. Path-sgd: Path-normalized optimization in deep

neural networks. In Advances in Neural Information Processing Systems  pages 2422–2430  2015a.

Behnam Neyshabur  Ryota Tomioka  and Nathan Srebro. In search of the real inductive bias: On the role of
implicit regularization in deep learning. In International Conference on Learning Representations  2015b.

Behnam Neyshabur  Ryota Tomioka  Ruslan Salakhutdinov  and Nathan Srebro. Geometry of optimization and

implicit regularization in deep learning. arXiv preprint  2017.

10

Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv preprint

arXiv:1704.08045  2017.

R Tyrrell Rockafellar. Directionally lipschitzian functions and subdifferential calculus. Proceedings of the

London Mathematical Society  1979.

Le Smith  Kindermans. Don’t Decay the Learning Rate  Increase the Batch Size. In ICLR  2018.

Daniel Soudry  Elad Hoffer  and Nathan Srebro. The implicit bias of gradient descent on separable data. arXiv

preprint arXiv:1710.10345  2017.

Matus Telgarsky. Margins  shrinkage and boosting. In Proceedings of the 30th International Conference on

International Conference on Machine Learning-Volume 28  pages II–307. JMLR. org  2013.

Ashia C Wilson  Rebecca Roelofs  Mitchell Stern  Nati Srebro  and Benjamin Recht. The marginal value of
adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems  2017.

Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding deep learning

requires rethinking generalization. In International Conference on Learning Representations  2017.

11

,Suriya Gunasekar
Jason Lee
Daniel Soudry
Nati Srebro