2019,Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder,In this work  we consider one challenging training time attack by modifying training data with bounded perturbation  hoping to manipulate the behavior (both targeted or non-targeted) of any corresponding trained classifier during test time when facing clean samples. To achieve this  we proposed to use an auto-encoder-like network to generate such adversarial perturbations on the training data together with one imaginary victim differentiable classifier. The perturbation generator will learn to update its weights so as to produce the most harmful noise  aiming to cause the lowest performance for the victim classifier during test time. This can be formulated into a non-linear equality constrained optimization problem. Unlike GANs  solving such problem is computationally challenging  we then proposed a simple yet effective procedure to decouple the alternating updates for the two networks for stability. By teaching the perturbation generator to hijacking the training trajectory of the victim classifier  the generator can thus learn to move against the victim classifier step by step. The method proposed in this paper can be easily extended to the label specific setting where the attacker can manipulate the predictions of the victim classifier according to some predefined rules rather than only making wrong predictions. Experiments on various datasets including CIFAR-10 and a reduced version of ImageNet confirmed the effectiveness of the proposed method and empirical results showed that  such bounded perturbations have good transferability across different types of victim classifiers.,Learning to Confuse: Generating Training Time

Adversarial Data with Auto-Encoder∗

1National Key Laboratory for Novel Software Technology

Nanjing University  Nanjing 210023  China

Ji Feng1 2  Qi-Zhi Cai2  Zhi-Hua Zhou1

{fengj  zhouzh}@lamda.nju.edu.cn  caiqizhi@chuangxin.com

2Sinovation Ventures AI Institute

Abstract

In this work  we consider one challenging training time attack by modifying training
data with bounded perturbation  hoping to manipulate the behavior (both targeted
or non-targeted) of any corresponding trained classiﬁer during test time when
facing clean samples. To achieve this  we proposed to use an auto-encoder-like
network to generate such adversarial perturbations on the training data together
with one imaginary victim differentiable classiﬁer. The perturbation generator will
learn to update its weights so as to produce the most harmful noise  aiming to
cause the lowest performance for the victim classiﬁer during test time. This can be
formulated into a non-linear equality constrained optimization problem. Unlike
GANs  solving such problem is computationally challenging  we then proposed
a simple yet effective procedure to decouple the alternating updates for the two
networks for stability. By teaching the perturbation generator to hijacking the
training trajectory of the victim classiﬁer  the generator can thus learn to move
against the victim classiﬁer step by step. The method proposed in this paper can
be easily extended to the label speciﬁc setting where the attacker can manipulate
the predictions of the victim classiﬁer according to some predeﬁned rules rather
than only making wrong predictions. Experiments on various datasets including
CIFAR-10 and a reduced version of ImageNet conﬁrmed the effectiveness of the
proposed method and empirical results showed that  such bounded perturbations
have good transferability across different types of victim classiﬁers.

1

Introduction

How to modify the training data with bounded transferable perturbation that can lead to the largest
generalization gap? In other words  we consider the task of adding imperceivable noises to the training
data  hoping to maximally confuse any corresponding classiﬁer so as to make wrong predictions as
much as possible when facing clean test data. In this paper  we refer such perturbed training samples
as training time adversarial training data.
To achieve the above goal  we deﬁned a deep encoder-decoder-like network to generate such pertur-
bations. Meanwhile  we used an imaginary neural network acting as the victim classiﬁer  and the
goal here is to train both networks simultaneously that can cause the lowest accuracy for the victim
classiﬁer on clean test set. We can thus formulate such problem into a non-linear equality constrained
optimization problem. Unlike GANs [9]  such optimization problem is much harder to solve  and a
direct implementation of alternating updates will lead to unstable result. Inspired by some common
techniques in reinforcement learning such as introducing a separate record tracking network like
target-nets to stabilize Q-learning [19]  we proposed a similar approach by decoupling the training

∗The ﬁrst two authors contributed equally to the work.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

procedure for both networks. By doing so  the optimization procedure is much stable in practice. In
other words  the adversarial perturbation generator is trained by hijacking the training procedure of
the victim classiﬁer. By doing so  the noise generator will learn to move against the victim classiﬁer
step by step.
A similar setting is data poisoning [20] proposed in the security community. However  their goal is
quite different compared with this work. The main goal for this work is to reveal some intriguing
properties of neural networks by adding bounded perturbations to the training data  whereas data
poisoning focuses on the restriction that only a few training data is allowed to change. In other
words  in traditional data poisoning tasks  the attackers goal is to add or modify training data as few
as possible  whereas training time adversarial data put the constraint on the perturbation levels (as
human imperceivable as possible). Moreover  having full control of training data (instead of changing
a few) is a realistic assumption. For instance  in some applications an agent may agree to release
some internal data for peer assessment or academic research  but does not like to enable the data
receiver to build a model which performs well on real test data; this can be realized by applying such
adversarial noises before the data release. In addition  when taking this from data privacy aspect 
such procedure is quite different from releasing synthetic data via GANs. Consider a company selling
surveillance cameras and the user will store all the data been taken (these photos cannot be synthetic
for obvious reasons). On the other hand  the user certainly does not want any other unauthorized third
parties to steal the data and train a classiﬁer. Then  our proposed procedure is suitable for this kind of
task since now the user can just make self-perturbations on its own data for protection.
The other contribution of this work is that  such formalization can be easily extended to the label
speciﬁc case  where one wants to speciﬁcally fool the classiﬁer of recognizing one input pattern into a
speciﬁcally predeﬁned class  rather than making a wrong prediction only. Finally  experimental results
showed that  the learned noises is effective and robust to other machine learning models with different
structure or even different types such as Random Forest [4] or Support Vector Machine(SVM) [6].
The rest of the paper is organized as follows: First  we will give a formalization for the proposed
problem and describe the optimization procedure. Experimental results are then presented and ﬁnally
conclusion and future works are discussed.

2 Related Works

One subject which closely relates to our work is data poisoning. The task of data poisoning dates
back to the pre-deep learning times. For instance  there has been some research on poisoning the
classical models  including SVM [2]  Linear Regression [14]  and Naive Bayes [21] which basically
transform the poisoning task into a convex optimization problem.
Poisoning for deep models  however  is a more challenging one. Kon et.al. [16] ﬁrst proposed the
possibility of poisoning deep models via the inﬂuence function to derive adversarial training examples.
Currently  there have been some popular approaches to data poisoning. For instance  sample speciﬁc
poisoning aims to manipulate the model’s behavior on some particular test samples. [24  5  11]. On
the other hand  general poison attacks aiming to reduce the performance on cleaned whole unseen
test set [16  20]. As explained in the previous section  one of the differences with data poisoning is
that the poisoning task mainly focuses on modifying as few samples as possible whereas our work
focus on adding bounded noises as small as possible. In addition  our noise adding scheme can be
scaled to much larger datasets with good transferability.
Another related subject is adversarial examples or testing time attacks  which refers to the case of
presenting malicious testing samples to an already trained classiﬁer. Since the classiﬁer is given and
ﬁxed  there is no two-party game involved. Researches showed deep model is very sensitive to such
adversarial examples due to the high-dimensionality of the input data and the linearity nature inside
deep neural networks [10]. Some recent works showed such adversarial examples also exist in the
physical world [8  1]  making it an important security and safety issue when designing high-stakes
machine learning systems in an open and dynamic environment. Our work can be regarded as a
training time analogy of adversarial examples. There have been some works on explaining the
effectiveness of adversarial examples. The work in [26] proposed that it is the linearity inside neural
networks that makes the decision boundary vulnerable in high dimensional space. Although beyond
the scope of this paper  we tested several hypotheses on explaining the effectiveness of training time
adversarial noises.

2

Figure 1: An overview for learning to confuse: Decoupling the alternating update for fθ and gξ

3 The proposed method

Consider the standard supervised learning procedure for classiﬁcation where one wants to learn
the mapping fθ : X → {0  1}K from data where K is the number of classes being predicted. To
learn the optimal parameters θ∗  a loss function such as cross-entropy for classiﬁcation L(fθ(x)  y) :
Rk × Z+ → R+ on training data is often deﬁned and empirical risk minimization [27] can thus be
applied  that is  one wants to minimize the loss function on training data as:

θ∗ = arg min

θ

[L(fθ(x)  y)]

(1)

(cid:88)

(x y)∼D

When fθ is a differentiable system such as neural networks  stochastic gradient descent (SGD) [3] or
its variants can be applied by updating θ via gradient descent
θ ← θ − α∇θL(fθ(x)  y) 

(2)

where α refers to the learning rate.
The goal for this work is to perturb the training data by adding artiﬁcially imperceivable noise such
that during testing time  the classiﬁer’s behavior will be dramatically different on the clean test-set.
To formulate this  we ﬁrst deﬁne a noise generator gξ : X → X which takes one training sample x in
X and transform it into an imperceivable noise pattern in the same space X . For image data  such
constraint can be formulated as:

∀x (cid:107)gξ(x)(cid:107)∞ ≤ 

(3)

Here  the  controls the perturbation strength which is a common practice in adversarial settings [10].
In this work  we choose the noise generator gξ to be an encoder-decoder neural network and the
activation for the ﬁnal layer is deﬁned to be:  · (tanh(·)) to facilitate the constraint (3).
With the above motivation and notations  we can then formalize the task into the following optimiza-
tion problem as:

(cid:88)

max

ξ

s.t.

(x y)∼D
θ∗(ξ) = arg min

[L(fθ∗(ξ)(x)  y)] 

(cid:88)

θ

(x y)∼D

[L(fθ(x + gξ(x))  y)]

(4)

In other words  every possible conﬁguration ξ is paired with one classiﬁer fθ∗(ξ) trained on the
corresponding modiﬁed data  the goal here is to ﬁnd a noise generator gξ∗ such that the paired
classiﬁer fθ∗(ξ∗) to have the worst performance on the cleaned test set  compared with all the other
possible ξ.
This non-convex optimization problem is challenging  especially due to the nonlinear equality
constraint. Here we propose an alternating update procedure using some commonly accepted tricks
in reinforcement learning for stability [19] which is simple yet effective in practice.
First  since we are assuming fθ and gξ to be neural networks  the equality constraint can be relaxed
into

θi = θi−1 − α · ∇θi−1L(fθi−1(x + gξ(x))  y)

(5)

3

where i is the index for SGD updates.
Second  the basic idea is to alternatively update fθ over adversarial training data via gradient descent
and update gξ over clean data via gradient ascent. The main problem is that  if we directly using
this alternating approach  both networks fθ and gξ won’t converge in practice. To stabilize this
process  we propose to update fθ over the adversarial training data ﬁrst  while collecting the update
trajectories for fθ  then  based on such trajectories  we update the adversarial training data as well as
gξ by calculating the pseudo-update for fθ at each time step. Such whole procedure is repeated T
trials until convergence. The detailed procedure is illustrated in Algorithm 1 and Figure 1.

Algorithm 1: Deep Confuse
Input: Training data D  number of trials T   max iteration for training a classiﬁcation model maxiter 

learning rate of classiﬁcation model αf  learning rate of the Noise Generator αg  batch size b

Output: Learned Noise Generator gξ

1 ξ ← RandomInit()
2 for t = 1 to T do
θ0 ← RandomInit()
3
L ← empty list
4
// Update fθ while keeping gξ ﬁxed
5
for i = 0 to maxiter do
6
7
8
9
10
11
12
13
14
15
16
17
18 end
19 return gξ

end

i

(xi  yi) ∼ D // Sample a mini-batch of training data
L.append((θi  xi  yi))
xadversarial
θi+1 ← θi − αf∇θiL(fθi (xadversarial

← xi + gξ(xi)

i

)  yi) // Update model fθ by SGD

end
// update gξ via a pseudo-update of fθ
for i = 0 to maxiter do
(θi  xi  yi) ← L[i]
θ(cid:48) ← θi − αf∇θiL(fθi (xi + gξ(xi))  yi) // Pseudo-update fθ over the current adversarial data
ξ ← ξ + αg∇ξL(fθ(cid:48) (x)  y) // Update gξ over clean data

Finally  we introduce one more modiﬁcation for efﬁciency. Notice that storing the whole trajectory
of the gradient updates when training fθ is memory inefﬁcient. To avoid directly storing such
information  during each trial of training  we can create a copy of gξ as g(cid:48)
ξ to alternatively
update with fθ  then copy the parameters back to gξ. By doing so  we can merge the two loops within
each trial into a single one and don’t need to store the gradients at all. The detailed procedure is
illustrated in Algorithm 2.

ξ and let g(cid:48)

Algorithm 2: Mem-Efﬁcient Deep Confuse
Input: Training data D  number of trials T   max iteration for training a classiﬁcation model maxiter 

learning rate of classiﬁcation model αf  learning rate of the Noise Generator αg  batch size b

θ0 ← RandomInit()
for i = 0 to maxiter do

Output: Learned Noise Generator gξ
1 ξ ← RandomInit()
ξ ← gξ.copy()
2 g(cid:48)
3 for t = 1 to T do
4
5
6
7
8
9
10
11
12
13 end
14 return gξ

(xi  yi) ∼ D // Sample a mini-batch
θ(cid:48) ← θi − αf∇θiL(fθi (xi + g(cid:48)
ξ(cid:48) ← ξ(cid:48) + αg∇ξ(cid:48)L(fθ(cid:48) (x)  y)
← xi + gξ(xi)
xadversarial
θi+1 ← θi − αf∇θiL(fθi (xadversarial

end
gξ ← g(cid:48)

ξ

i

i

ξ(xi))  yi) // Update g(cid:48)

ξ using current fθ

)  yi) // Update fθ by SGD

4

4 Label Speciﬁc Adversaries

In this section  we give a brief introduction of how to transfer our settings to the label speciﬁc
scenarios. The goal for label speciﬁc adversaries is that the adversary not only wants the classiﬁer to
make the wrong predictions but also want the classiﬁer’s predictions speciﬁcally according to some
pre-deﬁned rules. For instance  the attacker wants the classiﬁer to wrongly recognize the pattern
from class A speciﬁcally to Class B (thus not to Class C). To achieve this  denote η : Z+ → Z+ as a
pre-deﬁned label transformation function which maps one label to another. Here η is pre-deﬁned by
the attacker  and it transforms a label index into another different label index. Such label speciﬁc
adversary can thus be formalized into:

(cid:88)

(cid:88)

θ

(x y)∼D

[L(fθ∗(ξ)(x)  η(y))] 

min

ξ

s.t.

(x y)∼D
θ∗(ξ) = arg min

L(fθ(xi + gξ(xi))  yi)

(6)

It is easy to show that optimizing the above problem is nearly identical with the procedure described in
Algorithm 2. The only thing needed to be changed is to replace the gradient ascent into gradient decent
in line 10 in Algorithm 2 and replace η(y) to y in the same line while keeping others unchanged.

5 Experiment

To validate the effectiveness of our method  we used the classical MNIST [18]  CIFAR-10 [17] for
multi-classiﬁcation and a subset of ImageNet [7] for 2-class classiﬁcation. Concretely  we used a
subset of ImageNet (bulbul v.s. jellyﬁsh) consists of 2 600 colored images with size 224×224×3
for training and 100 colored images for testing. Random samples for the adversarial training data is
illustrated in Figure 2.

(b) MNIST

(a) 2-Class ImageNet

(c) CIFAR-10

Figure 2: First rows: original training samples. Second rows: adversarial training samples.

The classiﬁer fθ during training we used for MNIST is a simple Convolutional Network with 2
convolution layers having 20 and 50 channels respectively  followed by a fully-connected layer
consists of 500 hidden units. For the 2-class ImageNet and CIFAR-10  we used fθ to be a CNN with
5 convolution layers having 32 64 128 128 and 128 channels respectively  each convolution layer is
followed by a 2×2 pooling operations. Both classiﬁers used ReLU as activation and the kernel size is
set to be 3×3. Cross-entropy is used for loss function whereas the learning rate and batch size for the
classiﬁers fθ are set to be 0.01 and 64 for MNIST and CIFAR-10 and 0.1 and 32 for ImageNet. The
number of trials T is set to be 500 for both cases.
The noise generator gξ for MNIST and ImageNet consists of an encoder-decoder structure where
each encoder/decoder has 4 4x4 convolution layers with channel numbers 16 32 64 128 respectively.
For CIFAR-10  we use a U-Net [23] which has larger model capacity. The learning rate for the noise
generator gξ is set to be 10−4 via Adam [15].

5

5.1 Performance Evaluation of Training Time Adversary

Using the model conﬁgurations described above  we trained the noise generator gξ and its corre-
sponding classiﬁer fθ with perturbation constraint  to be 0.3  0.1  0.032  for MNIST  ImageNet and
CIFAR-10  respectively. The classiﬁcation results are summarized in Table 1. Each experiment is
repeated 10 times.

(a) MNIST-Train

(b) ImageNet-Train

(c) CIFAR-Train

(d) MNIST-Test

(e) ImageNet-Test

(f) CIFAR-Test

Figure 3: First row: Deep features of the adversarial training data. Second row: Deep features of the
cleaned test data.

Table 1: Test accuracy (mean±std) when the classiﬁer is trained on the original clean training set and
the adversarial training set respectively.

Clean Data
Adversarial Data

MNIST
99.32 ± 0.05
0.25 ± 0.04

ImageNet
88.5 ± 2.32
54.2 ± 11.19

CIFAR-10
77.28 ± 0.17
28.77 ± 2.80

When trained on the adversarial datasets  the test accuracy dramatically dropped to only 0.25 ± 0.04 
54.2 ± 11.19 and 28.77 ± 2.80  a clear evidence of the effectiveness for the proposed method.
We also visualized the activation of the ﬁnal hidden layers of fθs trained on the adversarial training
sets in Figure 3. Concretely  we ﬁt a PCA [22] model on the ﬁnal hidden layer’s output for each fθ on
the adversarial training data  then using the same projection model  we projected the clean data into
the same space. It can be shown that the classiﬁer trained on the adversarial data cannot differentiate
the clean samples.
It is interesting to know how does the perturbation constraint  affects the performance in terms
of both accuracy and visual appearance. Concretely  on MNIST dataset  we varied  from 0 (no
modiﬁcation) to 0.3  with a step size of 0.05 while keeping other conﬁgurations the same and the
results are illustrated in Figure 4.
The test accuracy in Figure 4 refers to the corresponding model performance trained on the different
adversarial training data with different . From the experimental result  we observed a sudden drop in
performance when  exceeds 0.15. Although beyond the scope of this work  we conjecture this result
is related or somewhat consistent with a similar theoretical guarantee for the robust error bound when
 is 0.10 [28].
Finally  we examined the results when the training data is partially modiﬁed. Concretely  under
different perturbation constraint  we varied the percentage of adversaries in the training data while
keeping other conﬁgurations the same. The results are demonstrated in Figure 5. Random ﬂip refers
to the case when one randomly ﬂip the labels in the training data.

6

Figure 4: Effect of varying .

Figure 5: Varying the ratio of adversaries
under different .

5.2 Evaluation of Transferability

In a more realistic setting  it is important to know the performance when we use a different classiﬁer.
Concretely  denote the original conv-net fθ been used during training as CNNoriginal. After the
adversarial data is obtained  we then train several different classiﬁers on the same adversarial data
and evaluate their performance on the clean test set.
For MNIST  we doubled/halved all the channels/hidden units and denote the model as CNNlarge and
CNNsmall accordingly. In addition  we also trained a standard Random Forest [4] with 300 trees and
a SVM [6] using RBF kernels with kernel coefﬁcient equal to 0.01. The experimental results are
summarized in Figure 6.

Figure 6: Test performance when using different classiﬁers. The horizontal red line indicates random
guess accuracy.

The blue histograms in Figure 6 correspond to the test performance trained on the clean dataset 
whereas orange histograms correspond to the test performance trained on the adversarial dataset.
From the experimental results  it can be shown that the adversarial noises produced by gξ are general
enough such that even non-NN classiﬁers as random forest and SVM are also vulnerable and produce
poor results as expected.

(a) CIFAR-10

(b) Two-class Imagenet

Figure 7: Test performance when using different model architectures.The horizontal red line indicates
random guess accuracy.

7

For CIFAR-10 and ImageNet  we tried a variety of conv-nets including VGG [25]  ResNet [12]
and DenseNet [13] with different layers  and evaluate the performance accordingly. The results are
summarized in Figure 7. Again  good transferability of the adversarial noise is observed.

5.3 The Generalization Gap and Linear Hypothesis

To fully illustrate the generalization gap caused by the adversaries  after we obtained the adversarial
training data  we retrained 3 conv-nets (one for each data-sets) having the same architecture as fθ
and plotted the training curves as illustrated in Figure 8. A clear generalization gap between training
and testing is observed. We conjecture the deep model tends to over-ﬁts towards the training noises
gξ(x).

(a) MNIST.

(b) 2-class ImageNet.

(c) CIFAR-10.

Figure 8: Learning curves for fθ

To validate our conjecture  we measured the predictive accuracy between the true label and the
predictions fθ(gξ(x)) taking only adversarial noises as inputs. The results are summarized in Table 2.
Notice 95.15%  93.00% and 72.98% test accuracy is obtained on the test set.
This interesting result conﬁrmed the conjecture that the model does over-ﬁt to the noises. Here we
give one possible explanation. We hypothesize that it is the linearity inside deep models that make
the adversarial effective. In other words  fθ(gξ(x)) contributes most when minimizing L(fθ(x +
gξ(x))  y). This result is deeply related and consistent with the results from adversarial examples
[10] and the memorization property for DNNs [29].

Table 2: Prediction accuracy taking only
noises as inputs. That is  the accuracy be-
tween the true label and fθ(gξ(x)) where x is
the clean sample.

MNIST
ImageNet
CIFAR-10

Noisetrain Noisetest
95.15
93.00
72.98

95.62
88.87
78.57

Figure 9: Clean samples and their correspond-
ing adversarial noises for MNIST  CIFAR-10
and ImageNet

5.4 Weight Visualizations

Instead of visualizing deep features of the adversarial data  it is also interesting to directly plotting the
trained weights of the victim classiﬁer as a visual interpretation of the effectiveness. Concretely  we
visualized the weights of two linear SVMs trained on clean and adversarial training data  respectively.
Our results are shown in Figure 10.
It can be shown that  compared with image templates (top row) obtained from clean training data 
the victim SVM weights (bottom row) trained on adversarial data went to the opposite direction
and trend to over-ﬁts on image corners. This result is also hinted that  the decision boundary in a
high-dimensional space is indeed easy to manipulate  which in-turn give the attacker the chance of
producing training time adversarial data.

8

Figure 10: LinearSVM weights visualization for MNIST. Top row: Weights trained on clean training
data. Bottom row: Weights trained on adversarial training data.

5.5 Label Speciﬁc Adversaries

To validate the effectiveness in label speciﬁc adversarial setting  without loss of generalizability  here
we shift the predictions by one. For MNIST dataset  we want the classiﬁer trained on the adversarial
data to predict the test samples from class 1 speciﬁcally to class 2  and class 2 to class 3 ... and class
9 to class 0. Using the method described in section 4  we trained the corresponding noise generator
and evaluated the corresponding CNN on the test set  as illustrated in Figure 11.

(a) Clean Training Data

(b) Non-label speciﬁc setting

(c) Label-speciﬁc setting

Figure 11: The confusion matrices on test set under different scenarios for MNIST dataset. They
summarized the test performance of classiﬁer trained on (a) clean training data (b) Non-label speciﬁc
setting and (c) label-speciﬁc setting.
Compared with the test accuracy (0.25 ± 0.04) in the non-label speciﬁc setting  the test accuracy also
dropped to 1.48 ± 0.21  in addition  the success rate for targeting the desired speciﬁc label increased
from 0.00 to 79.7 ± 0.38. Such results gave positive supports for the effectiveness in label speciﬁc
adversarial setting. Notice this is only a side-product of the proposed method to show the formulation
can be easily modiﬁed to achieve some more user-speciﬁc tasks.

6 Conclusion

In this work  we proposed a general framework for generating training time adversarial data by letting
an auto-encoder watch and move against an imaginary victim classiﬁer. We further proposed a simple
yet effective training scheme to train both networks simultaneously by decoupling the alternating
update procedure for stability. Experiments on image data conﬁrmed the effectiveness of the proposed
method  in particular  such adversarial data is still effective even to use a different victim classiﬁer 
making it more useful in a realistic setting.
Theoretical analysis or some more improvements for the optimization procedure is planned as future
works. In addition  it is interesting to design adversarially robustness classiﬁers against this scheme.

Acknowledgments

This research was supported by NSFC (61751306)  the National Key R&D Program of China
(2018YFB1004300)  and the Collaborative Innovation Center of Novel Software Technology and
Industrialization. The ﬁrst two authors would like to thank Beijing Sinnovation Ventures Megvii
International AI Institute Company Limited for the support.

9

References
[1] Athalye  A. and Sutskever  I. Synthesizing robust adversarial examples. In ICML  pp. 284–293  2018.

[2] Biggio  B.  Nelson  B.  and Laskov  P. Poisoning attacks against support vector machines. In ICML  pp.

1467–1474  2012.

[3] Bottou  L. Large-scale machine learning with stochastic gradient descent. In COMPSTAT  pp. 177–186 

2010.

[4] Breiman  L. Random forests. Machine learning  45(1):5–32  2001.

[5] Chen  X.  Liu  C.  Li  B.  Lu  K.  and Song  D. Targeted backdoor attacks on deep learning systems using

data poisoning. arXiv preprint arXiv:1712.05526  2017.

[6] Cortes  C. and Vapnik  V. Support-vector networks. Machine learning  20(3):273–297  1995.

[7] Deng  J.  Dong  W.  Socher  R.  Li  L.-J.  Li  K.  and Fei-Fei  L. Imagenet: A large-scale hierarchical image

database. In CVPR  pp. 248–255  2009.

[8] Eykholt  K.  Evtimov  I.  Fernandes  E.  Li  B.  Rahmati  A.  Xiao  C.  Prakash  A.  Kohno  T.  and Song  D.

Robust physical-world attacks on deep learning visual classiﬁcation. In CVPR  pp. 1625–1634  2018.

[9] Goodfellow  I.  Pouget-Abadie  J.  Mirza  M.  Xu  B.  Warde-Farley  D.  Ozair  S.  Courville  A.  and

Bengio  Y. Generative adversarial nets. In NIPS  pp. 2672–2680  2014.

[10] Goodfellow  I. J.  Shlens  J.  and Szegedy  C. Explaining and harnessing adversarial examples. In ICLR 

2015.

[11] Gu  T.  Dolan-Gavitt  B.  and Garg  S. Badnets: Identifying vulnerabilities in the machine learning model

supply chain. arXiv preprint arXiv:1708.06733  2017.

[12] He  K.  Zhang  X.  Ren  S.  and Sun  J. Deep residual learning for image recognition. In CVPR  pp.

770–778  2016.

[13] Huang  G.  Liu  Z.  van der Maaten  L.  and Weinberger  K. Q. Densely connected convolutional networks.

In Proc. of CVPR  pp. 2261–2269  2017.

[14] Jagielski  M.  Oprea  A.  Biggio  B.  Liu  C.  Nita-Rotaru  C.  and Li  B. Manipulating machine learning:

Poisoning attacks and countermeasures for regression learning. In IEEE S & P  pp. 19–35  2018.

[15] Kingma  D. P. and Ba  J. L. Adam: Amethod for stochastic optimization. In ICLR  2014.

[16] Koh  P. W. and Liang  P. Understanding black-box predictions via inﬂuence functions. In ICML  pp.

1885–1894  2017.

[17] Krizhevsky  A. Learning multiple layers of features from tiny images. 2009.

[18] LeCun  Y.  Bottou  L.  Bengio  Y.  and Haffner  P. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[19] Mnih  V.  Kavukcuoglu  K.  Silver  D.  Rusu  A. A.  Veness  J.  Bellemare  M. G.  Graves  A.  Riedmiller 
M.  Fidjeland  A. K.  Ostrovski  G.  et al. Human-level control through deep reinforcement learning.
Nature  518(7540):529  2015.

[20] Mu˜noz-Gonz´alez  L.  Biggio  B.  Demontis  A.  Paudice  A.  Wongrassamee  V.  Lupu  E. C.  and Roli  F.
Towards poisoning of deep learning algorithms with back-gradient optimization. In ACM Workshop on
Artiﬁcial Intelligence and Security  pp. 27–38  2017.

[21] Nelson  B.  Barreno  M.  Chi  F. J.  Joseph  A. D.  Rubinstein  B. I. P.  Saini  U.  Sutton  C.  Tygar  J. D. 
and Xia  K. Exploiting machine learning to subvert your spam ﬁlter. In Usenix Workshop on Large-Scale
Exploits and Emergent Threats  pp. 7:1–7:9  2008.

[22] Pearson  K. On lines and planes of closest ﬁt to systems of points in space. The London  Edinburgh  and

Dublin Philosophical Magazine and Journal of Science  2(11):559–572  1901.

[23] Ronneberger  O.  Fischer  P.  and Brox  T. U-net: Convolutional networks for biomedical image segmenta-
tion. In International Conference on Medical image computing and computer-assisted intervention  pp.
234–241. Springer  2015.

10

[24] Shafahi  A.  Huang  W. R.  Najibi  M.  Suciu  O.  Studer  C.  Dumitras  T.  and Goldstein  T. Poison frogs!

targeted clean-label poisoning attacks on neural networks. In NIPS  pp. 6106–6116  2018.

[25] Simonyan  K. and Zisserman  A. Very deep convolutional networks for large-scale image recognition.

arXiv preprint arXiv:1409.1556  2014.

[26] Szegedy  C.  Zaremba  W.  Sutskever  I.  Bruna  J.  Erhan  D.  Goodfellow  I.  and Fergus  R. Intriguing

properties of neural networks. arXiv preprint  2013.

[27] Vapnik  V. Principles of risk minimization for learning theory. In NIPS  pp. 831–838  1992.

[28] Wong  E. and Kolter  Z. Provable defenses against adversarial examples via the convex outer adversarial

polytope. In ICML  pp. 5283–5292  2018.

[29] Zhang  C.  Bengio  S.  Hardt  M.  Recht  B.  and Vinyals  O. Understanding deep learning requires

rethinking generalization. In ICLR  2016.

11

,Ji Feng
Qi-Zhi Cai
Zhi-Hua Zhou