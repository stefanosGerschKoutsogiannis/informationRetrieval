2007,Optimal ROC Curve for a Combination of Classifiers,We present a new analysis for the combination of binary classifiers. We propose a theoretical framework based on the Neyman-Pearson lemma to analyze combinations of classifiers. In particular  we give a method for finding the optimal decision rule for a combination of classifiers and prove that it has the optimal ROC curve. We also show how our method generalizes and improves on previous work on combining classifiers and generating ROC curves.,Optimal ROC Curve for a Combination of Classiﬁers

Marco Barreno

Alvaro A. C´ardenas

Computer Science Division

University of California at Berkeley

Berkeley  California 94720

J. D. Tygar

{barreno cardenas tygar}@cs.berkeley.edu

Abstract

We present a new analysis for the combination of binary classiﬁers. Our analysis
makes use of the Neyman-Pearson lemma as a theoretical basis to analyze combi-
nations of classiﬁers. We give a method for ﬁnding the optimal decision rule for a
combination of classiﬁers and prove that it has the optimal ROC curve. We show
how our method generalizes and improves previous work on combining classiﬁers
and generating ROC curves.

1 Introduction

We present an optimal way to combine binary classiﬁers in the Neyman-Pearson sense: for a given
upper bound on false alarms (false positives)  we ﬁnd the set of combination rules maximizing the
detection rate (true positives). This forms the optimal ROC curve of a combination of classiﬁers.

This paper makes the following original contributions: (1) We present a new method for ﬁnding
the meta-classiﬁer with the optimal ROC curve. (2) We show how our framework can be used to
interpret  generalize  and improve previous work by Provost and Fawcett [1] and Flach and Wu [2].
(3) We present experimental results that show our method is practical and performs well  even when
we must estimate the distributions with insufﬁcient data.

In addition  we prove the following results: (1) We show that the optimal ROC curve is composed
in general of 2n + 1 different decision rules and of the interpolation between these rules (over the
space of 22n possible Boolean rules). (2) We prove that our method is optimal in this space. (3) We
prove that the Boolean AND and OR rules are always part of the optimal set for the special case of
independent classiﬁers (though in general we make no independence assumptions). (4) We prove a
sufﬁcient condition for Provost and Fawcett’s method to be optimal.

2 Background

Consider classiﬁcation problems where examples from a space of inputs X are associated with
binary labels {0  1} and there is a ﬁxed but unknown probability distribution P(x  c) over examples
(x  c) ∈ X × {0  1}. H0 and H1 denote the events that c = 0 and c = 1  respectively.
A binary classiﬁer is a function f : X → {0  1} that predicts labels on new inputs. When we use
the term “classiﬁer” in this paper we mean binary classiﬁer. We address the problem of combining
results from n base classiﬁers f1  f2  . . .   fn. Let Yi = fi(X) be a random variable indicating the
output of classiﬁer fi and Y ∈ {0  1}n = (Y1  Y2  . . .   Yn). We can characterize the performance of
classiﬁer fi by its detection rate (also true positives  or power) PDi = Pr[Yi = 1|H1] and its false
alarm rate (also false positives  or test size) PF i = Pr[Yi = 1|H0]. In this paper we are concerned
with proper classiﬁers  that is  classiﬁers where PDi > PF i. We sometimes omit the subscript i.

1

The Receiver Operating Characteristic (ROC) curve plots PF on the x-axis and PD on the y-axis
(ROC space). The point (0  0) represents always classifying as 0  the point (1  1) represents always
classifying as 1  and the point (0  1) represents perfect classiﬁcation. If one classiﬁer’s curve has no
points below another  it weakly dominates the latter. If no points are below and at least one point
is strictly above  it dominates it. The line y = x describes a classiﬁer that is no better than chance 
and every proper classiﬁer dominates this line. When an ROC curve consists of a single point  we
connect it with straight lines to (0  0) and (1  1) in order to compare it with others (see Lemma 1).
In this paper  we focus on base classiﬁers that occupy a single point in ROC space. Many classiﬁers
have tunable parameters and can produce a continuous ROC curve; our analysis can apply to these
cases by choosing representative points and treating each one as a separate classiﬁer.

2.1 The ROC convex hull

Provost and Fawcett [1] give a seminal result on the use of ROC curves for combining classiﬁers.
They suggest taking the convex hull of all points of the ROC curves of the classiﬁers. This ROC
convex hull (ROCCH) combination rule interpolates between base classiﬁers f1  f2  . . .   fn  select-
ing (1) a single best classiﬁer or (2) a randomization between the decisions of two classiﬁers for
every false alarm rate [1]. This approach  however  is not optimal: as pointed out in later work by
Fawcett  the Boolean AND and OR rules over classiﬁers can perform better than the ROCCH [3].

AND and OR are only 2 of 22n possible Boolean rules over the outputs of n base classiﬁers (n
classiﬁers ⇒ 2n possible outcomes ⇒ 22n rules over outcomes). We address ﬁnding optimal rules.

2.2 The Neyman-Pearson lemma

In this section we introduce Neyman-Pearson theory from the framework of statistical hypothesis
testing [4  5]  which forms the basis of our analysis.
We test a null hypothesis H0 against an alternative H1. Let the random variable Y have probability
distributions P (Y|H0) under H0 and P (Y|H1) under H1  and deﬁne the likelihood ratio ℓ(Y) =
P (Y|H1)/P (Y|H0). The Neyman-Pearson lemma states that the likelihood ratio test

D(Y) =( 1

γ
0

if ℓ(Y) > τ
if ℓ(Y) = τ
if ℓ(Y) < τ

 

(1)

for some τ ∈ (0  ∞) and γ ∈ [0  1]  is a most powerful test for its size: no other test has higher
PD = Pr[D(Y) = 1|H1] for the same bound on PF = Pr[D(Y) = 1|H0]. (When ℓ(Y) = τ  
D = 1 with probability γ and 0 otherwise.) Given a test size α  we maximize PD subject to PF ≤ α
by choosing τ and γ as follows. First we ﬁnd the smallest value τ ∗ such that Pr[ℓ(Y) > τ ∗|H0] ≤
α. To maximize PD  which is monotonically nondecreasing with PF   we choose the highest value
γ ∗ that satisﬁes Pr[D(Y) = 1|H0] = Pr[ℓ(Y) > τ ∗|H0] + γ ∗ Pr[ℓ(Y) = τ ∗|H0] ≤ α  ﬁnding
γ ∗ = (α − Pr[ℓ(Y) > τ ∗|H0])/ Pr[ℓ(Y) = τ ∗|H0].

3 The optimal ROC curve for a combination of classiﬁers

We characterize the optimal ROC curve for a decision based on a combination of arbitrary
classiﬁers—for any given bound α on PF   we maximize PD. We frame this problem as a Neyman-
Pearson hypothesis test parameterized by the choice of α. We assume nothing about the classiﬁers
except that each produces an output in {0  1}. In particular  we do not assume the classiﬁers are
independent or related in any way.

Before introducing our method we analyze the one-classiﬁer case (n = 1).

Lemma 1 Let f1 be a classiﬁer with performance probabilities PD1 and PF 1. Its optimal ROC
curve is a piecewise linear function parameterized by a free parameter α bounding PF : for α <
PF 1  PD(α) = (PD1/PF 1)α  and for α > PF 1  PD(α) = [(1 − PD1)/(1 − PF 1)](α − PF 1) + PD1.

Proof. When α < PF 1  we can obtain a likelihood ratio test by setting τ ∗ = ℓ(1) and γ ∗ = α/PF 1 
and for α > PF 1  we set τ ∗ = ℓ(0) and γ ∗ = (α − PF 1)/(1 − PF 1).

2

2

The intuitive interpretation of this result is that to decrease or increase the false alarm rate of the
classiﬁer  we randomize between using its predictions and always choosing 1 or 0. In ROC space 
this forms lines interpolating between (PF 1  PD1) and (1  1) or (0  0)  respectively.
To generalize this result for the combination of n classiﬁers  we require the distributions P (Y|H0)
and P (Y|H1). With this information we then compute and sort the likelihood ratios ℓ(y) for all
outcomes y ∈ {0  1}n. Let L be the list of likelihood ratios ranked from low to high.

Lemma 2 Given any 0 ≤ α ≤ 1  the ordering L determines parameters τ ∗ and γ ∗ for a likelihood
ratio test of size α.

Lemma 2 sets up a classiﬁcation rule for each interval between likelihoods in L and interpolates
between them to create a test with size exactly α. Our meta-classiﬁer does this for any given bound
on its false positive rate  then makes predictions according to Equation 1. To ﬁnd the ROC curve for
our meta-classiﬁer  we plot PD against PF for all 0 ≤ α ≤ 1. In particular  for each y ∈ {0  1}n
we can compute Pr[ℓ(Y) > ℓ(y)|H0]  which gives us one value for τ ∗ and a point in ROC space
(PF and PD follow directly from L and P ). Each τ ∗ will turn out to be the slope of a line segment
between adjacent vertices  and varying γ ∗ interpolates between the vertices. We call the ROC curve
obtained in this way the LR-ROC.

Theorem 1 The LR-ROC weakly dominates the ROC curve of any possible combination of Boolean
functions g : {0  1}n → {0  1} over the outputs of n classiﬁers.

Proof. Let α′ be the probability of false alarm PF for g. Let τ ∗ and γ ∗ be chosen for a test of
size α′. Then our meta-classiﬁer’s decision rule is a likelihood ratio test. By the Neyman-Pearson
lemma  no other test has higher power for any given size. Since ROC space plots power on the
y-axis and size on the x-axis  this means that the PD for g at PF = α′ cannot be higher than that of
the LR-ROC. Since this is true at any α′  the LR-ROC weakly dominates the ROC curve for g. 2

3.1 Practical considerations

To compute all likelihood ratios for the classiﬁer outcomes we need to know the probability distri-
butions P (Y|H0) and P (Y|H1). In practice these distributions need to be estimated. The simplest
method is to run the base classiﬁers on a training set and count occurrences of each outcome. It is
likely that some outcomes will not occur in the training  or will occur only a small number of times.
Our initial approach to deal with small or zero counts when estimating was to use add-one smooth-
ing. In our experiments  however  simple special-case treatment of zero counts always produced
better results than smoothing  both on the training set and on the test set. See Section 5 for details.

Furthermore  the optimal ROC curve may have a different likelihood ratio for each possible outcome
from the n classiﬁers  and therefore a different point in ROC space  so optimal ROC curves in general
have up to 2n points. This implies an exponential (in the number of classiﬁers) lower bound on the
running time of any algorithm to compute the optimal ROC curve for a combination of classiﬁers.
For a handful of classiﬁers  such a bound is not problematic  but it is impractical to compute the
optimal ROC curve for dozens or hundreds of classiﬁers. (However  by computing and sorting the
likelihood ratios we avoid a 22n-time search over all possible classiﬁcation functions.)

4 Analysis

4.1 The independent case

In this section we take an in-depth look at the case of two binary classiﬁers f1 and f2 that are
conditionally independent given the input’s class  so that P (Y1  Y2|Hc) = P (Y1|Hc)P (Y2|Hc) for
c ∈ {0  1} (this section is the only part of the paper in which we make any independence assump-
tions). Since Y1 and Y2 are conditionally independent  we do not need the full joint distribution; we
need only the probabilities PD1  PF 1  PD2  and PF 2 to ﬁnd the combined PD and PF . For example 
ℓ(01) = ((1 − PD1)PD2)/((1 − PF 1)PF 2).
The assumption that f1 and f2 are conditionally independent and proper deﬁnes a partial ordering
on the likelihood ratio: ℓ(00) < ℓ(10) < ℓ(11) and ℓ(00) < ℓ(01) < ℓ(11). Without loss of

3

Table 1: Two probability distributions.

Class 1 (H1)

Class 0 (H0)

Class 1 (H1)

Class 0 (H0)

Y1

1

0.375
0.325

0
0.2
0.1

Y2
0
1

Y2
0
1

0
0.5
0.3

Y1

1
0.1
0.1

(a)

Y2
0
1

0
0.2
0.2

Y1

1
0.1
0.5

Y2
0
1

0
0.1
0.5

Y1

1
0.3
0.1

(b)

generality  we assume ℓ(00) < ℓ(01) < ℓ(10) < ℓ(11). This ordering breaks the likelihood ratio’s
range (0  ∞) into ﬁve regions; choosing τ in each region deﬁnes a different decision rule.
The trivial cases 0 ≤ τ < ℓ(00) and ℓ(11) < τ < ∞ correspond to always classifying as
1 and 0  respectively. PD and PF are therefore both equal to 1 and both equal to 0  respec-
tively. For the case ℓ(00) ≤ τ < ℓ(01)  Pr [ℓ(Y) > τ ] = Pr [Y = 01 ∨ Y = 10 ∨ Y = 11] =
Pr [Y1 = 1 ∨ Y2 = 1] . Thresholds in this range deﬁne an OR rule for the classiﬁers  with PD =
PD1 + PD2 − PD1PD2 and PF = PF 1 + PF 2 − PF 1PF 2. For the case ℓ(01) ≤ τ < ℓ(10)  we
have Pr [ℓ(Y) > τ ] = Pr [Y = 10 ∨ Y = 11] = Pr [Y1 = 1] . Therefore the performance proba-
bilities are simply PD = PD1 and PF = PF 1. Finally  the case ℓ(10) ≤ τ < ℓ(11) implies that
Pr [ℓ(Y) > τ ] = Pr [Y = 11]   and therefore thresholds in this range deﬁne an AND rule  with
PD = PD1PD2 and PF = PF 1PF 2. Figure 1a illustrates this analysis with an example.
The assumption of conditional independence is a sufﬁcient condition for ensuring that the AND and
OR rules improve on the ROCCH for n classiﬁers  as the following result shows.

Theorem 2 If the distributions of the outputs of n proper binary classiﬁers Y1  Y2  . . .   Yn are con-
ditionally independent given the instance class  then the points in ROC space for the rules AND
(Y1 ∧ Y2 ∧ · · · ∧ Yn) and OR (Y1 ∨ Y2 ∨ · · · ∨ Yn) are strictly above the convex hull of the ROC
curves of the base classiﬁers f1  . . .   fn. Furthermore  these Boolean rules belong to the LR-ROC.

Proof.
The likelihood ratio of the case when AND outputs 1 is given by ℓ(11 · · · 1) =
(PD1PD2 · · · PDn)/(PF 1PF 2 · · · PF n). The likelihood ratio of the case when OR does not output 1
is given by ℓ(00 · · · 0) = [(1 − PD1)(1 − PD2) · · · (1 − PDn)]/[(1 − PF 1)(1 − PF 2) · · · (1 − PF n)].
Now recall that for proper classiﬁers fi  PDi > PF i and thus (1 − PDi)/(1 − PF i) < 1 < PDi/PF i.
It is now clear that ℓ(00 · · · 0) is the smallest likelihood ratio and ℓ(11 · · · 1) is the largest likelihood
ratio  since others are obtained only by swapping P(F D)i and (1 − P(F D)i)  and therefore the OR
and AND rules will always be part of the optimal set of decisions for conditionally independent clas-
siﬁers. These rules are strictly above the ROCCH: because ℓ(11 · · · 1) > PD1/PD2  and PD1/PD2
is the slope of the line from (0  0) to the ﬁrst point in the ROCCH (f1)  the AND point must be
above the ROCCH. A similar argument holds for OR since ℓ(00 · · · 0) < (1 − PDn)/(1 − PF n). 2

4.2 Two examples

We return now to the general case with no independence assumptions. We present two example
distributions for the two-classiﬁer case that demonstrate interesting results.

The ﬁrst distribution appears in Table 1a. The likelihood ratio values are ℓ(00) = 0.4  ℓ(10) = 3.75 
ℓ(01) = 1/3  and ℓ(11) = 3.25  giving us ℓ(01) < ℓ(00) < ℓ(11) < ℓ(10). The three non-trivial
rules correspond to the Boolean functions Y1 ∨ ¬Y2  Y1  and Y1 ∧ ¬Y2. Note that Y2 appears only
negatively despite being a proper classiﬁer  and both the AND and OR rules are sub-optimal.

The distribution for the second example appears in Table 1b. The likelihood ratios of the outcomes
are ℓ(00) = 2.0  ℓ(10) = 1/3  ℓ(01) = 0.4  and ℓ(11) = 5  so ℓ(10) < ℓ(01) < ℓ(00) < ℓ(11)
and the three points deﬁning the optimal ROC curve are ¬Y1 ∨ Y2  ¬(Y1 ⊕ Y2)  and Y1 ∧ Y2 (see
Figure 1b). In this case  an XOR rule emerges from the likelihood ratio analysis.

These examples show that for true optimal results it is not sufﬁcient to use weighted voting rules
w1Y1 + w2Y2 + · · · + wnYn ≥ τ   where w ∈ (0  ∞) (like some ensemble methods). Weighted
voting always has AND and OR rules in its ROC curve  so it cannot always express optimal rules.

4

1

0.8

Y1 ∨ Y2

Y2

 

0.6

Y1 ∧ Y2

D
P

0.4

0.2

0
 
0

Y1

ROC of f
1
ROC of f
2
LR−ROC

0.2

0.4

PF

0.6

0.8

1

(a)

D
P

1

0.8

0.6

0.4

0.2

0
 
0

¬Y1 ∨ Y2

 

¬(Y1 ⊕ Y2)

Y1 ∧ Y2

Y2

Y1

ROC of f
1
ROC of f
2
LR−ROC

0.2

0.4

PF

0.6

0.8

1

(b)

D
P

1

0.8

0.6

0.4

0.2

0
 
0

f ′
2

f1

f ′
1

 

f ′
3

f3

f2

Original ROC
LR−ROC

0.2

0.4

PF

0.6

0.8

1

(c)

Figure 1: (a) ROC for two conditionally independent classiﬁers. (b) ROC curve for the distributions
in Table 1b. (c) Original ROC curve and optimal ROC curve for example in Section 4.4.

4.3 Optimality of the ROCCH

We have seen that in some cases  rules exist with points strictly above the ROCCH. As the following
result shows  however  there are conditions under which the ROCCH is optimal.

Theorem 3 Consider n classiﬁers f1  . . .   fn. The convex hull of points (PF i  PDi) with (0  0) and
(1  1) (the ROCCH) is an optimal ROC curve for the combination if (Yi = 1) ⇒ (Yj = 1) for i < j
and the following ordering holds: ℓ(00 · · · 0) < ℓ(00 · · · 01) < ℓ(00 · · · 011) < · · · < ℓ(1 · · · 1).

Proof. The condition (Yi = 1) ⇒ (Yj = 1) for i < j implies that we only need to consider n + 2
points in the ROC space (the two extra points are (0  0) and (1  1)) rather than 2n. It also implies the
following conditions on the joint distribution: Pr[Y1 = 0 ∧ · · · ∧ Yi = 0 ∧ Yi+1 = 1 ∧ · · · ∧ Yn =
1|H0] = PF i+1 − PF i  and Pr[Y1 = 1 ∧ · · · ∧ Yn = 1|H0] = PF 1. With these conditions
and the ordering condition on the likelihood ratios  we have Pr[ℓ(Y) > ℓ(1 · · · 1)|H0] = 0  and
1 · · · 1)|H0] = PF i. Therefore  ﬁnding the optimal threshold of the likelihood
Pr[ℓ(Y) > ℓ(0 · · · 0

ratio test for PF i−1 ≤ α < PF i  we get τ ∗ = ℓ(0 · · · 0
i−1

1 · · · 1)  and for PF i ≤ α < PF i+1 

τ ∗ = ℓ(0 · · · 0

1 · · · 1). This change in τ ∗ implies that the point PF i is part of the LR-ROC. Setting

| {z }

α = PF i (thus τ ∗ = ℓ(0 · · · 0

1 · · · 1) and γ ∗=0) implies Pr[ℓ(Y) > τ ∗|H1] = PDi.

2

| {z }i

| {z }i

| {z }i

The condition Yi = 1 ⇒ Yj = 1 for i < j is the same inclusion condition Flach and Wu use
for repairing an ROC curve [2]. It intuitively represents the performance in ROC space of a single
classiﬁer with different operating points. The next section explores this relationship further.

4.4 Repairing an ROC curve

Flach and Wu give a voting technique to repair concavities in an ROC curve that generates operating
points above the ROCCH [2]. Their intuition is that points underneath the convex hull can be
mirrored to appear above the convex hull in much the same way as an improper classiﬁer can be
negated to obtain a proper classiﬁer. Although their algorithm produces better ROC curves  their
solution will often yield curves with new concavities (see for example Flach and Wu’s Figure 4 [2]).
Their algorithm has a similar purpose to ours  but theirs is a local greedy optimization technique 
while our method performs a global search in order to ﬁnd the best ROC curve.

Figure 1c shows an example comparing their method to ours. Consider the following probabil-
ity distribution on a random variable Y ∈ {0  1}2: P ((00  10  01  11)|H1) = (0.1  0.3  0.0  0.6) 
P ((00  10  01  11)|H0) = (0.5  0.001  0.4  0.099). Flach and Wu’s method assumes the original
ROC curve to be repaired has three models  or operating points: f1 predicts 1 when Y ∈ {11}  f2
predicts 1 when Y ∈ {11  01}  and f3 predicts 1 when Y ∈ {11  01  10}. If we apply Flach and
Wu’s repair algorithm  the point f2 is corrected to the point f ′
2; however  the operating points of f1
and f3 remain the same.

5

d
P

d
P

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

0
1

.

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

d
P

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

Meta (train)
Base (train)
Meta (test)
Base (test)
PART

Meta (train)
Base (train)
Meta (test)
Base (test)
PART

0.00

0.05

0.10

Pfa

(a) adult

0.15

0.20

0.000

0.005

0.010

0.015

Pfa

(b) hypothyroid

d
P

0
1

.

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

Meta (train)
Base (train)
Meta (test)
Base (test)
PART

Meta (train)
Base (train)
Meta (test)
Base (test)
PART

0.00

0.05

0.10

0.15

0.00

0.02

0.04

0.06

0.08

0.10

Pfa

(c) sick-euthyroid

Pfa

(d) sick

Figure 2: Empirical ROC curves for experimental results on four UCI datasets.

Our method improves on this result by ordering the likelihood ratios ℓ(01) < ℓ(00) < ℓ(11) < ℓ(10)
and using that ordering to make three different rules: f ′
2 predicts 1
when Y ∈ {10  11}  and f ′

1 predicts 1 when Y ∈ {10}  f ′

3 predicts 1 when Y ∈ {10  11  00}.

5 Experiments

We ran experiments to test the performance of our combining method on the adult  hypothyroid 
sick-euthyroid  and sick datasets from the UCI machine learning repository [6]. We chose ﬁve base
classiﬁers from the YALE machine learning platform [7]: PART (a decision list algorithm)  SMO
(Sequential Minimal Optimization)  SimpleLogistic  VotedPerceptron  and Y-NaiveBayes. We used
default settings for all classiﬁers. The adult dataset has around 30 000 training points and 15 000
test points and the sick dataset has around 2000 training points and 700 test points. The others each
have around 2000 points that we split randomly into 1000 training and 1000 test.

For each experiment  we estimate the joint distribution by training the base classiﬁers on a training
set and counting the outcomes. We compute likelihood ratios for all outcomes and order them. When
outcomes have no examples  we treat ·/0 as near-inﬁnite and 0/· as near-zero and deﬁne 0/0 = 1.

6

We derive a sequence of decision rules from the likelihood ratios computed on the training set. We
can compute an optimal ROC curve for the combination by counting the number of true positives
and false positives each rule achieves. In the test set we use the rules learned on the training set.

5.1 Results

The ROC graphs for our four experiments appear in Figure 2. The ROC curves in these experiments
all rise very quickly and then ﬂatten out  so we show only the range of PF 1 for which the values
are interesting. We can draw some general conclusions from these graphs. First  PART clearly
outperforms the other base classiﬁers in three out of four experiments  though it seems to overﬁt
on the hypothyroid dataset. The LR-ROC dominates the ROC curves of the base classiﬁers on both
training and test sets. The ROC curves for the base classiﬁers are all strictly below the LR-ROC
in results on the test sets. The results on training sets seem to imply that the LR-ROC is primarily
classifying like PART with a small boost from the other classiﬁers; however  the test set results (in
particular  Figure 2b) demonstrate that the LR-ROC generalizes better than the base classiﬁers.

The robustness of our method to estimation errors is uncertain. In our experiments we found that
smoothing did not improve generalization  but undoubtedly our method would beneﬁt from better
estimation of the outcome distribution and increased robustness.

We ran separate experiments to test how many classiﬁers our method could support in practice.
Estimation of the joint distribution and computation of the ROC curve ﬁnished within one minute
for 20 classiﬁers (not including time to train the individual classiﬁers). Unfortunately  the inherent
exponential structure of the optimal ROC curve means we cannot expect to do signiﬁcantly better
(at the same rate  30 classiﬁers would take over 12 hours and 40 classiﬁers almost a year and a half).

6 Related work

Our work is loosely related to ensemble methods such as bagging [8] and boosting [9] because
it ﬁnds meta-classiﬁcation rules over a set of base classiﬁers. However  bagging and boosting each
take one base classiﬁer and train many times  resampling or reweighting the training data to generate
classiﬁer diversity [10] or increase the classiﬁcation margin [11]. The decision rules applied to
the generated classiﬁers are (weighted) majority voting. In contrast  our method takes any binary
classiﬁers and ﬁnds optimal combination rules from the more general space of all binary functions.

Ranking algorithms  such as RankBoost [12]  approach the problem of ranking points by score or
preference. Although we present our methods in a different light  our decision rule can be interpreted
as a ranking algorithm. RankBoost  however  is a boosting algorithm and therefore fundamentally
different from our approach. Ranking can be used for classiﬁcation by choosing a cutoff or threshold 
and in fact ranking algorithms tend to optimize the common Area Under the ROC Curve (AUC)
metric. Although our method may have the side effect of maximizing the AUC  its formulation is
different in that instead of optimizing a single global metric  it is a constrained optimization problem 
maximizing PD for each PF .
Another more similar method for combining classiﬁers is stacking [13]. Stacking trains a meta-
learner to combine the predictions of several base classiﬁers; in fact  our method might be consid-
ered a stacking method with a particular meta-classiﬁer. It can be difﬁcult to show the improvement
of stacking in general over selecting the best base-level classiﬁer [14]; however  stacking has a use-
ful interpretation as generalized cross-validation that makes it practical. Our analysis shows that our
combination method is the optimal meta-learner in the Neyman-Pearson sense  but incorporating the
model validation aspect of stacking would make an interesting extension to our work.

7 Conclusion

In this paper we introduce a new way to analyze a combination of classiﬁers and their ROC curves.
We give a method for combining classiﬁers and prove that it is optimal in the Neyman-Pearson
sense. This work raises several interesting questions.

Although the algorithm presented in this paper avoids checking the whole doubly exponential num-
ber of rules  the exponential factor in running time limits the number of classiﬁers that can be

7

combined in practice. Can a good approximation algorithm approach optimality while having lower
time complexity? Though in general we make no assumptions about independence  Theorem 2
shows that certain simple rules are optimal when we do know that the classiﬁers are independent.
Theorem 3 proves that the ROCCH can be optimal when only n output combinations are possible.
Perhaps other restrictions on the distribution of outcomes can lead to useful special cases.

Acknowledgments

This work was supported in part by TRUST (Team for Research in Ubiquitous Secure Technology) 
which receives support from the National Science Foundation (NSF award number CCF-0424422)
and the following organizations: AFOSR (#FA9550-06-1-0244)  Cisco  British Telecom  ESCHER 
HP  IBM  iCAST  Intel  Microsoft  ORNL  Pirelli  Qualcomm  Sun  Symantec  Telecom Italia  and
United Technologies; and in part by the UC Berkeley-Taiwan International Collaboration in Ad-
vanced Security Technologies (iCAST) program. The opinions expressed in this paper are solely
those of the authors and do not necessarily reﬂect the opinions of any funding agency or the U.S. or
Taiwanese governments.

References

[1] Foster Provost and Tom Fawcett. Robust classiﬁcation for imprecise environments. Machine Learning

Journal  42(3):203–231  March 2001.

[2] Peter A. Flach and Shaomin Wu. Repairing concavities in ROC curves.

In Proceedings of the 19th

International Joint Conference on Artiﬁcial Intelligence (IJCAI’05)  pages 702–707  August 2005.

[3] Tom Fawcett. ROC graphs: Notes and practical considerations for data mining researchers. Technical

Report HPL-2003-4  HP Laboratories  Palo Alto  CA  January 2003. Updated March 2004.

[4] J. Neyman and E. S. Pearson. On the problem of the most efﬁcient tests of statistical hypotheses. Philo-
sophical Transactions of the Royal Society of London  Series A  Containing Papers of a Mathematical or
Physical Character  231:289–337  1933.

[5] Vincent H. Poor. An Introduction to Signal Detection and Estimation. Springer-Verlag  second edition 

1988.

[6] D. J. Newman  S. Hettich  C. L. Blake  and C. J. Merz. UCI repository of machine learning databases 

1998. http://www.ics.uci.edu/∼mlearn/MLRepository.html.

[7] I. Mierswa  M. Wurst  R. Klinkenberg  M. Scholz  and T. Euler. YALE: Rapid prototyping for com-
plex data mining tasks. In Proceedings of the ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD)  2006.

[8] L. Breiman. Bagging predictors. Machine Learning  24(2):123–140  1996.
[9] Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In Thirteenth International

Conference on Machine Learning  pages 148–156  Bari  Italy  1996. Morgan Kaufmann.

[10] Thomas G. Dietterich. Ensemble methods in machine learning. Lecture Notes in Computer Science 

1857:1–15  2000.

[11] Robert E. Schapire  Yoav Freund  Peter Bartlett  and Wee Sun Lee. Boosting the margin: A new ex-
planation for the effectiveness of voting methods. The Annals of Statistics  26(5):1651–1686  October
1998.

[12] Yoav Freund  Raj Iyer  Robert E. Schapire  and Yoram Singer. An efﬁcient boosting algorithm for com-

bining preferences. Journal of Machine Learning Research (JMLR)  4:933–969  2003.

[13] D. H. Wolpert. Stacked generalization. Neural Networks  5:241–259  1992.
[14] Saso D˘zeroski and Bernard ˘Zenko. Is combining classiﬁers with stacking better than selecting the best

one? Machine Learning  54:255–273  2004.

8

,Daniel Hernández-Lobato
José Miguel Hernández-Lobato
Tomoya Murata
Taiji Suzuki
Shuyang Sun
Jiangmiao Pang
Jianping Shi
Shuai Yi
Wanli Ouyang