2019,On Robustness to Adversarial Examples and Polynomial Optimization,We study the design of computationally efficient algorithms with provable guarantees  that are robust to adversarial (test time) perturbations. While there has been an explosion of recent work on this topic due to its connections to test time robustness of deep networks  there is limited theoretical understanding of several basic questions like (i) when and how can one design provably robust learning algorithms? (ii) what is the price of achieving robustness to adversarial examples in a computationally efficient manner?

The main contribution of this work is to exhibit a strong connection between achieving robustness to adversarial examples  and a rich class of polynomial optimization problems  thereby making progress on the above questions. In particular  we leverage this connection to (a) design computationally efficient robust algorithms with provable guarantees for a large class of hypothesis  namely linear classifiers and degree-2 polynomial threshold functions~(PTFs)  (b) give a precise characterization of the price of achieving robustness in a computationally efficient manner for these classes  (c) design efficient algorithms to certify robustness and generate adversarial attacks in a principled manner for 2-layer neural networks. We empirically demonstrate the effectiveness of these attacks on real data.,On Robustness to Adversarial Examples and

Polynomial Optimization

Department of Computer Science

Pranjal Awasthi
Rutgers University

pranjal.awasthi@rutgers.edu

Abhratanu Dutta

Department of Computer Science

Northwestern University

abhratanudutta2020@u.northwestern.edu

Aravindan Vijayaraghavan
Department of Computer Science

Northwestern University

aravindv@northwestern.edu

Abstract

We study the design of computationally ecient algorithms with provable
guarantees  that are robust to adversarial (test time) perturbations. While
there has been an explosion of recent work on this topic due to its connec-
tions to test time robustness of deep networks  there is limited theoretical
understanding of several basic questions like (i) when and how can one de-
sign provably robust learning algorithms? (ii) what is the price of achieving
robustness to adversarial examples in a computationally ecient manner?
The main contribution of this work is to exhibit a strong connection between
achieving robustness to adversarial examples  and a rich class of polynomial
optimization problems  thereby making progress on the above questions.
In particular  we leverage this connection to (a) design computationally
ecient robust algorithms with provable guarantees for a large class of
hypothesis  namely linear classiﬁers and degree-2 polynomial threshold
functions (PTFs)  (b) give a precise characterization of the price of achieving
robustness in a computationally ecient manner for these classes  (c) design
ecient algorithms to certify robustness and generate adversarial attacks in
a principled manner for 2-layer neural networks. We empirically demonstrate
the eectiveness of these attacks on real data.

1 Introduction
The empirical success of deep learning has led to numerous unexplained phenomena about
which our current theoretical understanding is limited. Examples include the ability of
complex models to generalize well and eectiveness of ﬁrst order methods on optimizing
training loss. The focus of this paper is on the phenomenon of adversarial robustness  that
was ﬁrst pointed out by Szegedy et al. [33]. On many benchmark data sets  deep networks
optimized on the training set can often be fooled into misclassifying a test example by making
a small adversarial perturbation that is imperceptible to a human labeler. This has led to a
proliferation of work on designing robust algorithms that defend against such adversarial
perturbations  as well as attacks that aim to break these defenses.
In this work we choose to focus on perturbation defense  the most widely studied formulation
of adversarial robustness [24]. In the perturbation defense model  given a classiﬁer f  an
adversary can take a test example x generated from the data distribution and perturb it to ˜x

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

such that Îx ≠ ˜xÎ Æ ”. Here ” characterizes the amount of power the adversary has and the
distance is typically measured in the ¸Œ norm (other norms that have been studied include
the ¸2 norm). Given a loss function ¸(·)  the goal is to optimize the robust loss deﬁned as

E(x y)≥DË max

˜x:Îx≠˜xÎŒÆ”

¸(f(˜x)  y)È.

One would expect that when ” is small the label y of an example does not change  thereby
motivating the robust loss objective. Despite a recent surge in eorts to theoretically
understand adversarial robustness [36  37  38  21  30  13  4  10  16  34  25  26  11]  several
central questions remain open. How can one design provable polynomial time algorithms
that are robust to adversarial perturbations? Given a classiﬁer and a test input  how can one
provably construct an adversarial example in polynomial time or certify that none exists?
What computational barriers exist when designing adversarially robust learning algorithms?
In this work we identify and study a natural class of polynomial optimization problems
that are intimately connected to adversarial robustness  and help us shed new light on all
three of the above questions simultaneously! As a result we obtain the ﬁrst polynomial time
learning algorithms for a large class of functions that are optimally robust to adversarial
perturbations. Furthermore  we also provide nearly matching computational intractability
results that  together with our upper bounds give a sharp characterization of the price of
achieving adversarial robustness in a computationally ecient manner. We now summarize
our main results.
Our Contributions Polynomial optimization and Adversarial Robustness. We
identify a natural class of polynomial optimization problems that provide a common and
principled framework for studying various aspects of adversarial robustness. These problems
are also closely related to a rich class of well-studied problems that include the Grothendiëck
problem and its generalizations [2  9  1  22]. Given a classiﬁer of the form sgn(g(x)) with
g : Rn æ R  input x  and budget ”> 0  the optimization problem is

max

zœRn:ÎzÎŒÆ”

g(x + z).

Usually  such problems are NP-hard and one relaxes them to ﬁnd a ˆz such that g(x + ˆz)
comes as close to g(x + zú) in the objective value  where zú is the optimal solution. We
instead require the algorithm to output a ˆz such that g(x + ˆz) Ø g(x + zú) at the cost of
violating the ¸Œ constraint by a factor “ Ø 1. An ecient algorithm for producing such
a ˆz leads to an adversarial attack (in the relaxed ¸Œ neighborhood of radius “”) when an
adversarial example exists. On the other hand  if the algorithm produces no ‚z  then this
guarantees that there is no adversarial example within the ¸Œ neighborhood of radius ”.
We then design such algorithms based on convex programming relaxations to get the ﬁrst
provable polynomial time adversarial attacks when the given classiﬁer is a degree-1 or a
degree-2 polynomial threshold function (PTF).
Algorithms for Learning Adversarially Robust Classiﬁers. Next we use the algorithm
for ﬁnding adversarial examples to design polynomial time algorithms for learning robust
classiﬁers for the class of degree-1 and degree-2 polynomial threshold functions (PTFs).
To incorporate robustness we introduce a parameter “  that helps clarify the tradeo
when computational eciency is desired. We focus on the 0/1 error and say that a
class F of PTFs of VC dimension  is “-approximately robustly learnable if there exists
a (randomized) polynomial time algorithm that  for any given Á  ” > 0  takes as input
poly(  1
Á) examples generated from a distribution and labeled by a function in F that has
zero ”-robust error (realizable case)  outputs a classiﬁer from F that has (”/“)-robust error
upper bounded by Á. See Section 2 for the formal deﬁnition. We design polynomial time
algorithms for degree-1 and degree-2 PTFs with “ = 1 and “ = O(Ôlog n) respectively. Our
next result that we discuss below a nearly matching lower bound. Together this gives nearly
optimal approximately robust polynomial time algorithms for learning PTFs of degree at
most 2.
Computational Hardness. While our algorithm for degree-1 PTFs is optimal  i.e.  has
“ = 1  for degree-2 and higher PTFs  we show that one indeed has to pay a price for
computational robustness. We establish this by proving that robust learning of degree-2
PTFs is computationally hard for “ = o(logc n)  for some constant c > 0 (see Section 5

2

for formal statements). This is in sharp contrast to the non-robust setting (” = 0)  where
there exist polynomial time algorithms for constant degree PTFs (in the literature this is
referred to as proper PAC learning in the realizable setting). More importantly  our lower
bound again leverages the connection to polynomial optimization and in fact shows that
robust learning of degree-2 PTFs for “ = o(Ô÷approx) is NP-hard where ÷approx is precisely
the hardness of approximation factor of a well-studied combinatorial optimization problem
called Quadratic Programming. Hence  any signiﬁcant improvement in the approximation
factor in our upper bound is unlikely. While our hardness result applies to algorithms that
output a classiﬁer of low error  we also prove a more robust hardness result showing that for
learning degree-2 and higher PTFs without any loss in the robustness parameter  i.e  “ = 1 
it is computationally hard to even ﬁnd a classiﬁer of any constant error in the range (0  1
4).
Application to Neural Networks. Finally  we show that the connection to polynomial
optimization also leads to new algorithms for generating adversarial attacks on neural
networks. We focus on 2-layer neural networks with ReLU activations. We show that given
a network and a test input  the problem of ﬁnding an adversarial example can also be
phrased as an optimization problem of the kind studied for PTFs. We design a semi-deﬁnite
programming (SDP) based polynomial time algorithm to generate an adversarial attack for
such networks and compare our attack to the state-of-the-art attack of Madry et al. [24] on
the MNIST data set.
Comparison to Related Work. Among the several recent and concurrent works on
this topic  the most relevant to our result is the work of Bubeck et al. [7  8] that studies
computational complexity of robust learning. We defer other related work to Section C. In
the rest of the paper  we deﬁne our model formally and give an overview of our techniques in
Section 2. We then describe the connection to polynomial optimization in Section 3 and use
it to design robust learning algorithms in Section 4  and derive computational intractability
results in Section 5. In Section 6  we design adversarial attacks for 2 layer neural networks 
followed by conclusions in Section 7.

2 Model and Preliminaries
We focus on binary classiﬁcation  and adversarial perturbations are measured in ¸Œ norm.
For a vector x œ Rn  we have ÎxÎŒ = maxi |xi|. We study robust learning of polynomial
threshold functions (PTFs). These are functions of the form sgn(p(x))  where p(x) is a
polynomial in n variables over the reals. Here sgn(t) equals +1  if t Ø 0 and ≠1 otherwise.
Given y  yÕ œ {≠1  1}  we study the 0/1 loss deﬁned as ¸(y  yÕ) = 1 if y ”= yÕ and 0 otherwise.
Given a binary classiﬁer sgn(g(x))  an input xú  and a budget ”> 0  we say that xú + z is an
adversarial example (for input xú) if sgn(g(xú + z)) ”= sgn(g(xú)) and that ÎzÎŒ Æ ”. One
could similarly deﬁne the notion of adversarial examples for other norms. For a classiﬁer
with multiple outputs  we say that xú + z is an adversarial example i the largest co-ordinate
of g(xú + z) diers from the largest co-ordinate of g(xú). We now deﬁne the notion of robust
error of a classiﬁer.
Deﬁnition 2.1 (”-robust error). Let f(x) be a Boolean function mapping Rn to {≠1  1}.
Let D be a distribution over Rn ◊ {≠1  1}. Given ”> 0  we deﬁne the ”-robust error of
f with respect to D as err” D(f) = E(x y)≥D# supzœBn
Œ(0 ” )
denotes the ¸Œ ball of radius ”  i.e.  Bn
Analogous to empirical error in PAC learning  we denote ˆerr” S(f) to be the ”-robust empirical
error of f  i.e.  the robust error computed on the given sample S. To bound generalization gap 
we will use the notion of adversarial VC dimension as introduced in [10] (See Appendix A).
Next we deﬁne robust learning for PTFs.
Deﬁnition 2.2 (“-approximately robust learning). Let F be the class of degree-d PTFs
from Rn ‘æ {≠1  1} of VC dimension = O(nd). For “ Ø 1  an algorithm A “-approximately
robustly learns F if the following holds for any Á  ”  ÷ > 0: Given m = poly(  1
÷) samples
from a distribution D over Rn◊{≠1  1}  if F contains a function fú such that err” D(fú) = 0 
then with probability at least 1≠ ÷  A runs in time polynomial in m and outputs f œF such
that err”/“ D(f) Æ Á. If F admits such an algorithm then we say that F is “-approximately

(0 ”) ¸(f(x + z)  y)$. Here Bn

Á   1

Œ(0 ” ) = {x œ Rn : ÎxÎŒ Æ ”}.

Œ

3

robustly learnable. Here “ quantiﬁes the price of achieving computationally ecient robust
learning  with “ = 1 implying optimal learnability.

A Note about the Model and the Realizability Assumption Our deﬁnition of an
adversarial example requires that sgn(g(xú + z)) ”= sgn(g(xú))  whereas for robust learning
we require a classiﬁer that satisﬁes sgn(g(xú + z)) ”= y  where y is the given label of xú. This
might create two sources of confusion to the reader: a) In general the two requirements
might be incompatible  and b) It might happen that initially sgn(g(xú)) predicts the true
label incorrectly but there is a perturbation z such that sgn(g(xú + z)) predicts the true
label correctly. In this case one should not count z as an adversarial example. To address
(a) we would like to stress that all our guarantees hold under the realizability assumption 
i.e.  we assume that there is true function cú such that for all examples x in the support of
the distribution and all perturbations of magnitude upto ”  sgn(cú(xú + z)) = sgn(cú(xú)).
Hence  there will indeed be a target concept for which no adversarial example exists and as a
result will have zero robust error. To address (b) we would like to point out that in Section 4
where we use the subroutine for ﬁnding adversarial examples to learn a good classiﬁer sgn(g) 
we always enforce the constraint that on the training set sgn(g(xú)) = sgn(cú(xú)) and g is as
robust as possible. Hence when we ﬁnd an adversarial example for a point xú in our training
set  it will indeed satisfy that sgn(g(xú + z)) ”= sgn(cú(x)) and correctly penalize g for the
mistake. More generally  we could also deﬁne an adversarial example as one where given pair
(xú  y) the goal is to ﬁnd a z such that sgn(g(xú+z)) ”= y. All of our guarantees from Section 3
apply to this deﬁnition as well. Finally  in the non-realizable case  the distinction between
deﬁning adversarial robustness as either sgn(g(xú + z)) ”= sgn(g(xú))  or sgn(g(xú + z)) ”= y 
or even sgn(g(xú + z)) ”= sgn(cú(x)) matters and has dierent computational and statistical
implications [11  18]. Understanding when one can achieve computationally ecient robust
learning in the non-realizable case is an important direction for future work.
The deﬁnition of “-approximately robustly learnability has the realizability assumption built
into it. So  when we prove that a class F is “-approximately robustly learnable  we ﬁnd an
approximate robust learner from F under the realizability assumption on F i.e. for a set
of points from the distribution  the algorithm guarantees to return an approximate robust
learner only if there exists a perfect robust learner in the class F of learners.
3 Finding Adversarial Examples using Polynomial Optimization
In this section we introduce the broad class of polynomial optimization problems which are
useful in designing algorithms with provable guarantees for generating adversarial examples
for large classes like PTFs  and will later be useful for two layer neural networks in Section 6.
These polynomial optimization problems are generalizations of well-studied combinatorial
optimization problems like the Grothëndieck problem and computing operator norms of
matrices [19  2  9]. We then design algorithms with provable guarantees for some of these
classes. The following simple proposition illustrates the connection.
Proposition 3.1. Let “ Ø 1. There is an ecient algorithm that given a classiﬁer sgn(f(x))
Œ(xú “” )  or (b)
and a point xú  guarantees to either (a) ﬁnd an adversarial example in Bn
Œ(xú ” )  given an ecient algorithm
certify the absence of any adversarial example in Bn
that given x and a polynomial g(z) œ{ f(xú + z) ≠f(xú + z)} ﬁnds a ‚z such that g(‚z) Ø
maxÎzÎŒÆ” g(z) with Î‚zÎŒ Æ “”.

When the classiﬁer is a degree-d PTF of the form sgn(f)  we get the following problem:
given as input a degree d polynomial g (potentially dierent from f)  and any ÷  ” > 0  ﬁnd
in time poly(n  log( 1

÷)) and with probability at least 1 ≠ ÷  outputs a point ˆx s.t.

g(ˆx) Ø max
xœBn
Œ

g(x) and‚x œ Bn
This is closely related to the standard approximation variant of polynomial maximization
problem where the goal is to obtain  in polynomial time  an objective value as close to the
optimal one  without violating the Bn
Œ ball constraint. Instead  our problem asks for the
same objective value at the cost of an increase in the radius of the optimization ball (this is

Œ(0 “” ).

(1)

(0 ”)

4

1. Given (A  b  c) that deﬁnes the polynomial g(z) := zT Az + bT z + c.
2. Solve the SDP given by following vector program:

2 Æ ”2 ’i œ [n]  Îu0Î2

max qi j AijÈui  ujÍ +qi biÈui  u0Í + c subject to ÎuiÎ2
2 = 1.
represent the component of ui orthogonal to u0. Draw ’ ≥ N(0  I) a
standard Gaussian vector  and set‚zi := Èui  u0Í+Èu‹i  ’ Í for each i œ{ 0  1  . . .   n}.

3. Let u‹i
4. Repeat rounding O(log(1/÷)) random choices of ’ and pick the best choice.
Figure 1: The SDP-based algorithm for the degree-2 optimization problem.

sometimes called a (1 “ )-bicriteria approximation). This changes the ﬂavor of the problem 
and introduces new challenges particularly when the polynomial g is non-homogenous. We
begin with the following simple claim.
Claim 3.2. There is a deterministic linear-time algorithm that given any linear threshold
function sgn(bT x + c)  a point xú and ”> 0  provably ﬁnds an adversarial example ¸Œ ball
of ” around xú when it exists.
In Section 4  this will be used to give robust learning algorithms for linear classiﬁers. Our
main result of this section is a provable algorithm for degree-2 PTFs.
Theorem 3.3. For any ”  ÷ > 0  there is a polynomial time algorithm that given a degree-2
PTF sgn(f(x)) and a example (xú  sgn(f(xú)))  guarantees at least one of the following holds

with probability at least (1 ≠ ÷): (a) ﬁnds an adversarial example (xú +‚z) i.e.  sgn(f(xú)) ”=
sgn(f(xú +‚z))  with Î‚zÎŒ Æ C”Ôlog n  or (b) certiﬁes that ’z : ÎzÎŒ Æ ”  sgn(f(xú)) =
sgn(f(xú + z)) for some constant C > 0.
To establish the above theorem using Proposition 3.1  we need to design a polynomial time
algorithm that given any degree-2 polynomial g(x) = xT Ax + bT x + c with A œ Rn◊n  b œ
Rn  c œ R  ﬁnds a solution‚x with Î‚xÎŒ Æ O(Ôlog n) · ” such that g(‚x) Ø maxÎxÎŒÆ” g(x).
We design such an algorithm via an semi-deﬁnite programming (SDP) based approach that
is directly inspired by the algorithm for quadratic programming (QP) by [27  9]. However 
further complications arise due to non-homogeneity  and as our goal is to preserve the
objective function while potentially relaxing the constraint. We defer to the appendix for
a detailed discussion. In Fig. 1 we describe the SDP that we use and the corresponding
rounding algorithm to solve the optimization problem. The vector program given in step
2 of Algorithm 1 is an SDP where the variables are Xij = Èui  ujÍ  and can be solved in
polynomial time up to any additive error (using the Ellipsoid algorithm). We defer the the
details Appendix D.

4 From Adversarial Examples to Robust Learning Algorithms
In this section we show how to leverage algorithms for ﬁnding adversarial examples to design
polynomial time robust learning algorithms for degree-1 and degree-2 PTFs. We obtain our
upper bounds by establishing a general algorithmic framework that relates robust learnability
of PTFs to the polynomial maximization problem studied in Section 3.
Deﬁnition 4.1 (“-factor admissibility). For “ Ø 1  we say that a class F of PTFs is “-factor
admissible if F has the following properties:
(1) For any a  b  c œ R s.t. sgn(f(x))  sgn(g(x)) œF   we have sgn(af(x) + bg(x) + c) œF .
Further for any r œ Rn  we have sgn(g(x + r)) œF .
(2) There is a polynomial time algorithm that solves the optimization problem of maximizing
g(x + z) around any point x  i.e.  given a g œF   an x and ”> 0  the algorithm outputs a ˆz
such that g(x + ˆz) Ø maxzœBŒ(0 ”) g(x + z)and ÎzÎŒ Æ “”.
The ﬁrst two conditions above are natural and are satisﬁed by many classes of PTFs. The
third condition in the above deﬁnition concerns the optimization problem studied in Section 3.
The main result of this section  stated below  is the claim that any admissible class of PTFs
is also robustly learnable in polynomial time.
Theorem 4.2. Let F be a class of PTFs that is “-factor admissible for “ Ø 1. Then F is
“-approximate robustly learnable.

5

1. Let S = (x1  y1)  (x2  y2)  . . .   (xm  ym) be the given training set.
2. Find a degree-d polynomial g with sgn(g(x)) œF that satisﬁes:

sup
zœBn
Œ

(0 ”)

’i œ [m] 

(≠yi)g(xi + z) < 0.

d

".

Figure 2: Convex program to ﬁnd a PTF sgn(g(x)) œF with zero robust empirical error.
To learn a g œF we formulate robust empirical risk minimization as a convex program 
shown in Figure 2. Here we use the fact that the value of any polynomial g of degree d at a
given point x can be expressed as the inner product between the co-ecient vector of g and
an appropriate vector Â(x) œ RD where D =!n+d≠1
It is easy to see that the constraints
in the program above are linear in the coecients of g. Furthermore  checking the validity of
each constraint is really asking to check the robustness of g at a given point (xi  yi)  which is
an NP-hard problem [9]. Instead  we will use the fact that F is “-factor admissible to design
an approximate separation oracle for the type of constraints enforced in the program. Below
we give a proof sketch of Theorem 4.2 and defer the full proof to Appendix E.
Proof Sketch of Theorem 4.2. Let B be an algorithm that achieves the “-factor admissibility
for the class F. Given S  we will run the Ellipsoid algorithm on the convex program in
Figure 2. In each iteration  for each i œ [m]  we run B on the polynomial yig(xi + z)  where z
is the variable and xi is ﬁxed to be the ith data point. From the guarantee of B we get that
if there exists an i and z with ÎzÎŒ Æ ”/“  such that (≠yi)g(xi + z) > 0  then with high
probability  B will output a violated constraint of the convex program  i.e.  an index i œ [m]
and ˆz œ Bn
Œ(0 ” ) such that (≠yi)g(xi + ˆz) > 0. This gives us a separating hyperplane of the
form sgn(≠yig(xi + ˆz))  and the algorithm continues. This means that when the algorithm
terminates  we would have the empirical robust error ˆerr”/“ S (sgn(g)) = 0. Using the uniform
convergence bound from Lemma A.1  this would imply that err”/“ D(sgn(g)) Æ Á.
As a result we get the following corollaries about linear classiﬁer and degree-2 PTFs. The
proof for linear classiﬁers just follows from Claim 3.2  and Theorem 3.3 immediately implies
the result for degree-2 PTFs.
Corollary 4.3. The class of linear classiﬁers is optimally robustly learnable. The class of
degree-2 PTFs is O(Ôlog n)-approximately robustly learnable.

5 Computational Intractability of Learning Robust Classiﬁers
In this section  we leverage the connection to polynomial optimization to complement
our upper bound with the following nearly matching lower bound.We give a reduction
from Quadratic Programming (QP) where given a polynomial p(x) = qi<j aijxixj  and
a value s  the goal is to distinguish whether maxxœ{≠1 1}np(x) < s or whether exists an
x such that p(x) > s÷ approx.
It is known that the distinguishing problem is hard for
÷approx = O(logc n) for some constant c > 0 [3]; moreover the state-of-the-art algorithms
give a ÷approx = O(log n) factor approximation [9] and improving upon this factor is a major
open problem. By appropriately scaling the instance  this immediately implies the hardness
of checking whether a given degree-2 PTF is robust around a given point.
However  this does not suce for hardness of learning  since given a distribution supported
at a single point  there is a trivial constant classiﬁer that robustly classiﬁes the instance
correctly. More generally  there could exist a dierent degree-2 PTF that could be easy to
certify for the given point.
Instead  given a degree-2 PTF sgn(p(x))  we carefully construct
a set of O(n2) points such that any classiﬁer that is robust on an instance supported on
the set will have to be close to the given polynomial p. Having established this  we can
distinguish between the two cases of the QP problem by whether the learning algorithm is
able to output a robust classiﬁer or not. This is formalized below.
Theorem 5.1. There exists ”  Á > 0  such that assuming N P ”= RP there is no algorithm
that given a set of N = poly(n  1
Á) samples from a distribution D over Rn ◊ {≠1  +1}  runs
in time poly(N) and distinguishes between the following two cases for any ”Õ = o(Ô÷approx”):

• Yes: There exists a degree-2 PTF that has ”-robust error of 0 w.r.t. D.

6

• No: There exists no degree-2 PTF that has ”Õ-robust error at most Á w.r.t. D.

Here ÷approx is the hardness of approximation factor of the QP problem.
Remark 5.2. The above theorem proves that any polynomial time algorithm that always
outputs a robust classiﬁer (or declares failure if it does not ﬁnd one) will have to incur an
extra factor of (Ô÷approx) in the robustness parameter ”. Our upper bound in Section 4
on the other hand matches this bound. While our lower bound applies to algorithms that
output a classiﬁer of low error  in Appendix (see Theorem G.6) we also prove a more robust
lower bound that rules out the possibility of an ecient robust learner that incurs an error
less than 1/4.

6 Finding Adversarial Examples for Two Layer Neural Networks
Next we use the framework in Section 3 to design new algorithms for ﬁnding adversarial
examples in two layer neural networks with ReLU activations. We describe the setting
for binary classiﬁcation below. A two layer neural network with ReLU gates is given
by parameters (v1  v2  W) and outputs f1(x) = vT1 ‡(W x)  f2(x) = vT2 ‡(W x) where x œ
Rn  v1  v2 œ Rk and W œ Rk◊n. Here ‡ : Rm æ Rm is a co-ordinate wise non-linear operator
‡(yi) = max { 0  yi } for each i œ [m]. The binary classiﬁer corresponding to the network
is sgn(f1(x) ≠ f2(x)) = sgn(vT ‡(W x)) where v = v1 ≠ v2. The optimization problem that
arises is the following: given an instance with A œ Rm1◊n — œ Rm2  B œ Rm2◊n  c1 œ
Rn  c2 œ Rm1  c0 œ R  the goal is to ﬁnd opt(A  B  —  c)  deﬁned as :

opt(A  B  —  c) := max

z:ÎzÎŒÆ”Îc2 + AzÎ1 + cT

1 z ≠ Î— + BzÎ1 + c0

= max

z:ÎzÎŒÆ”

max

y:ÎyÎŒÆ1

yT Az + cT

1 z + cT

2 y ≠

m2ÿj=1|—j + BT

j z|.

(2)

Œ(xú ” ).

Here Bj is the jth row of B. Let c denote (c0  c1  c2)  and let opt(A  B  —  c) be the optimal
value of the above problem. The following proposition holds in a slightly more general setting
where there can be an extra linear term as described below.
Proposition 6.1. Let “ Ø 1. Suppose there is an ecient algorithm that given an instance
of problem (2) ﬁnds a solution ‚z ‚y with Î‚zÎŒ Æ “”  Î‚yÎŒ Æ 1 such that f(‚y ‚z) > 0 when
opt(A  b  —  c) > 0. Then there is an ecient algorithm that given a two layer neural net
sgn(f(x)) where f(x) := vT ‡(W x) + (vÕ)T x and an example xú  guarantees to either (a)
Œ(xú “” ) around xú  or (b) certify the absence of any
ﬁnd an adversarial example in the Bn
adversarial example in Bn
Our algorithm for solving (2) given in Figure 3 is inspired by Algorithm 1 for polynomial
optimization. However  the rounding algorithm diers because the variables yj and variables
zi serve dierent purposes in (2)  and we need to simultaneously satisfy dierent constraints
on them to produce a valid perturbation. Moreover when the SDP is negative  then this
gives a certiﬁcate of robustness around x.
Please see Section F for a simple proof and more details. We remark that one can obtain
provable guarantees similar to Theorem 4.2 for Algorithm 3 under certain regularity conditions
about the SDP solution. However  this is unsatisfactory as this depends on the SDP solution
to the given instance  as opposed to an explicit structural property of the instance. Obtaining
provable guarantees of the latter kind is an interesting open question.
Experiments
Next  we evaluate the performance of the proposed attack in Figure 3 and compare it with
the state of the art projected gradient descent(PGD) based attack of Madry et al. [24]. Our
approach indeed ﬁnds more adversarial examples  although at a higher computational cost
since we need to solve an SDP per example and per pair of classes. We use the MNIST data
set and our 2-layer neural network has d = 784 input units  k = 1024 hidden units and 10
output units. The SDP has d + k + 1 vector variables  and takes about 200s per instance on
a standard desktop. Hence we perform our experiments on randomly chosen subsets of the
MNIST data set. Another optimization we perform for computational reasons is that given

7

rj + c0

nÿi=1

Aj iÈvj  uiÍ +

1. Given instance I = (A  B  —  c) of (2)  solve SDP with parameter ÷ œ (0  1):
m1ÿj=1
sdp = max ÿjœ[m1] iœ[n]
c2(j)Èu0  vjÍ ≠ ÿjœ[m2]
c1(i)Èui  u0Í +
and Îu0Î2 = 1
s.t.’j œ [m1] ÎviÎ2 Æ 1  ’i œ{ 1  . . .   n}Î uiÎ2 Æ ”2 
Bj iÈui  u0Í).
’j œ [k2]

rj Ø (—j +ÿj
2. Let u‹i   v‹j represent the components of ui  vj orthogonal to u0. Let Á œ (0  1)
with Á = (1)/Ôlog m1. Let ’ ≥ N(0  I) be a Gaussian vector; set ’i œ
{ 0  1  . . .   n}   ‚zi := Èui  u0Í + 1
3. Repeat rounding with poly(n) random choices of ’ and pick the best choice.

Bj iÈui  u0Í)  and rj Ø ≠(—j +ÿj
ÁÈu‹i  ’ Í  ‚yj := Èvj  u0Í + ÁÈv‹j  ’ Í.

Figure 3: The SDP-based algorithm for Problem (2).
PGDpass (6 ◊ 50 random samples) PGDfail (8 ◊ 100 random samples)
Mean : 49.5 out of 50  Std : 0.76
Mean 30.6 out of 100  Std : 2.87

297 out of 300 total

244 out of 800 total

” = 0.3

SDP succeeds

” = 0.01

PGDpass (138 samples)

PGDfail (100 ranked)

SDP succeeds
Table 1: For ” = 0.3  we report mean and standard deviation across batches of the number of
adversarial examples found by running our SDPattack algorithm on 6 batches of 50 random examples
from PGDpass and 8 batches of 100 random samples from PGDfail. For ” = 0.01  we run SDPattack
on all 138 examples in PGDpass and ﬁrst 100 sorted examples from PGDfail.

138

45

an example x with predicted class i  we use a greedy heuristic to pick a class j ”= i for the
potential adversarial example x+ z. So the numbers we report below are an underestimate of
the eectiveness of the full SDP based algorithm. See Appendix B for a detailed discussion.
We consider two settings of the parameter ”  the maximum amount by which each pixel can
be perturbed to produce a valid attack example. As in [24] we ﬁrst choose ” = 0.3 and train a
robust 2-layer network using the algorithm in [24]. This network has an accuracy of 82.32%
and adversarial accuracy (allowing for adversarial perturbations) of 31.7% on the test set.
We then run the PGD attack and divide the test set into examples where the PGD attack
succeeds (PGDPass) and examples where the PGD attack fails (PGDfail). We then run our
attack on batches of random subsets chosen from each set. The ﬁrst row of Table 1 shows the
precision and recall of our method  along with the average and the standard deviation across
the chosen batches. As one can see  our method has very high recall  i.e.  whenever the PGD
attack succeeds  our SDP based algorithm also ﬁnds adversarial examples. Furthermore  on
examples where the PGD attack fails  our method is still able to discover new adversarial
examples 30% of the time. See a sample of the perturbed images produced by our method
in Section B. In particular  Figure 4 shows images of some of the examples where the SDP
based attack succeeds  but the PGDattack fails and Figure 5 shows some images where
both the PGDattack and SDP based attack succeed. A visual inspection of both the ﬁgures
reveals that our attack often produces sparse targeted attacks as opposed to PGDattack.
We also run the PGD attack on the network with ” = 0.01. Here we notice that attack
succeeds on only 138 test examples and hence we can aord to run our attack on all of
them. As can be seen from the second row of Table 1 our attack succeeds on all of these
examples. Further  when we run our algorithm on the ﬁrst 100 examples from PGDfail
picked according to a greedy heuristic (see Section B for details)  our method ﬁnds 45 new
adversarial examples. This implies at least a (138 + 45)/138 = 1.33-fold advantage here.
The experiments above suggest that our theoretical claims and algorithms can lead to
improved attacks. We would like to note that the recent work of [29] also studied SDP based
methods for providing adversarial certiﬁcates for 2-layer neural networks. However  our SDP
as outlined in Figure 3 is strictly stronger. The SDP of [29] is in fact independent of the
given example x  so we expect our method to produce better certiﬁcates. We leave as future
work the task of making our theoretical analysis practical for large scale applications.

8

7 Future Directions
Design of polynomial time algorithms that provably achieve adversarial robustness is an
important direction of research. Several open questions remain to be explored further. In
Section 4 we provide a general algorithmic framework for designing polynomial time robust
algorithms. It would be interesting to use our framework to design robust algorithms for
general degree-d PTFs. While there are algorithms to approximately maximize degree-d
polynomials  they focus on the homogeneous case which does not suce for our purposes.
Another important direction for future work is to convert our adversarial attack algorithm
for 2-layer neural networks into a provably robust learning algorithm via the framework
of Section 4. A straightforward invocation of the framework does not lead to a convex
constraint set. It would also be interesting to design provable adversarial attacks for higher
depth networks. Finally  our experimental results suggest that making our SDP based attack
work on a large scale could lead to improved adversarial attacks.

Acknowledgements
The second and third authors were supported by the National Science Foundation (NSF)
under Grant No. CCF-1652491 and CCF-1637585. Additionally  the second author was
funded by the Morrison Fellowship from Northwestern University.

References
[1] Noga Alon  Konstantin Makarychev  Yury Makarychev  and Assaf Naor. Quadratic

forms on graphs. Inventiones mathematicae  163(3):499–522  2006.

[2] Noga Alon and Assaf Naor. Approximating the cut-norm via grothendieck’s inequality.
In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing 
pages 72–80. ACM  2004.

[3] Sanjeev Arora  Eli Berger  Hazan Elad  Guy Kindler  and Muli Safra. On non-
approximability for quadratic programs. In Foundations of Computer Science  2005.
FOCS 2005. 46th Annual IEEE Symposium on  pages 206–215. IEEE  2005.

[4] Idan Attias  Aryeh Kontorovich  and Yishay Mansour. Improved generalization bounds

for robust learning. arXiv preprint arXiv:1810.02180  2018.

[5] Chiranjib Bhattacharyya. Robust classiﬁcation of noisy data using second order cone
In Intelligent Sensing and Information Processing  2004.

programming approach.
Proceedings of International Conference on  pages 433–438. IEEE  2004.

[6] Alberto Bietti  Grégoire Mialon  and Julien Mairal. On regularization and robustness

of deep neural networks. arXiv preprint arXiv:1810.00363  2018.

[7] Sébastien Bubeck  Yin Tat Lee  Eric Price  and Ilya Razenshteyn. Adversarial examples
from cryptographic pseudo-random generators. arXiv preprint arXiv:1811.06418  2018.
[8] Sébastien Bubeck  Eric Price  and Ilya Razenshteyn. Adversarial examples from compu-

tational constraints. arXiv preprint arXiv:1805.10204  2018.

[9] M Charikar and A Wirth. Maximizing quadratic programs: extending grothendieck’s
inequality. In Foundations of Computer Science  2004. Proceedings. 45th Annual IEEE
Symposium on  pages 54–60. IEEE  2004.

[10] Daniel Cullina  Arjun Nitin Bhagoji  and Prateek Mittal. Pac-learning in the presence

of evasion adversaries. arXiv preprint arXiv:1806.01471  2018.

[11] Dimitrios Diochnos  Saeed Mahloujifar  and Mohammad Mahmoody. Adversarial risk
and robustness: General deﬁnitions and implications for the uniform distribution. In
Advances in Neural Information Processing Systems  pages 10380–10389  2018.

9

[12] Alhussein Fawzi  Seyed-Mohsen Moosavi-Dezfooli  and Pascal Frossard. Robustness
of classiﬁers: from adversarial to random noise. In Advances in Neural Information
Processing Systems  pages 1632–1640  2016.

[13] Uriel Feige  Yishay Mansour  and Robert Schapire. Learning and inference in the
presence of corrupted inputs. In Conference on Learning Theory  pages 637–657  2015.
[14] Michael R Garey and David S Johnson. Computers and intractability  volume 29. wh

freeman New York  2002.

[15] Justin Gilmer  Ryan P Adams  Ian Goodfellow  David Andersen  and George E Dahl.
Motivating the rules of the game for adversarial example research. arXiv preprint
arXiv:1807.06732  2018.

[16] Justin Gilmer  Luke Metz  Fartash Faghri  Samuel S Schoenholz  Maithra Raghu  Martin
Wattenberg  and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774 
2018.

[17] Amir Globerson and Sam Roweis. Nightmare at test time: robust learning by feature
deletion. In Proceedings of the 23rd international conference on Machine learning  pages
353–360. ACM  2006.

[18] Pascale Gourdeau  Varun Kanade  Marta Kwiatkowska  and James Worrell. On the

hardness of robust classiﬁcation. arXiv preprint arXiv:1909.05822  2019.

[19] A. Grothendieck and V. Losert. "Résumé de la théorie métrique des produits tensoriels

topologiques". Univ.  1976.

[20] Michael J Kearns  Umesh Virkumar Vazirani  and Umesh Vazirani. An introduction to

computational learning theory. MIT press  1994.

[21] Justin Khim and Po-Ling Loh. Adversarial risk bounds for binary classiﬁcation via

function transformation. arXiv preprint arXiv:1810.09519  2018.

[22] Subhash Khot and Assaf Naor. Linear equations modulo 2 and the l1 diameter of
convex bodies. In Foundations of Computer Science  2007. FOCS’07. 48th Annual
IEEE Symposium on  pages 318–328. IEEE  2007.

[23] Subhash Khot and Ryan O’Donnell. Sdp gaps and ugc-hardness for maxcutgain. In
Foundations of Computer Science  2006. FOCS’06. 47th Annual IEEE Symposium on 
pages 217–226. IEEE  2006.

[24] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083  2017.

[25] Saeed Mahloujifar  Dimitrios I Diochnos  and Mohammad Mahmoody. The curse of
concentration in robust learning: Evasion and poisoning attacks from concentration of
measure. arXiv preprint arXiv:1809.03063  2018.

[26] Saeed Mahloujifar and Mohammad Mahmoody. Can adversarially robust learning

leverage computational hardness? arXiv preprint arXiv:1810.01407  2018.

[27] Yu Nesterov. Semideﬁnite relaxation and nonconvex quadratic optimization. Optimiza-

tion methods and software  9(1-3):141–160  1998.

[28] Ryan O’Donnell. Analysis of Boolean Functions. Cambridge University Press  New

York  NY  USA  2014.

[29] Aditi Raghunathan  Jacob Steinhardt  and Percy Liang. Certiﬁed defenses against

adversarial examples. arXiv preprint arXiv:1801.09344  2018.

[30] Ludwig Schmidt  Shibani Santurkar  Dimitris Tsipras  Kunal Talwar  and Aleksander
arXiv preprint

Madry. Adversarially robust generalization requires more data.
arXiv:1804.11285  2018.

10

[31] Pannagadatta K Shivaswamy  Chiranjib Bhattacharyya  and Alexander J Smola. Second
order cone programming approaches for handling missing and uncertain data. Journal
of Machine Learning Research  7(Jul):1283–1314  2006.

[32] Aman Sinha  Hongseok Namkoong  and John Duchi. Certifying some distributional

robustness with principled adversarial training. 2018.

[33] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian
Goodfellow  and Rob Fergus. Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199  2013.

[34] Dimitris Tsipras  Shibani Santurkar  Logan Engstrom  Alexander Turner  and Aleksander

Madry. Robustness may be at odds with accuracy. 2018.

[35] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the
convex outer adversarial polytope. In International Conference on Machine Learning 
pages 5283–5292  2018.

[36] Huan Xu  Constantine Caramanis  and Shie Mannor. Robustness and regularization of
support vector machines. Journal of Machine Learning Research  10(Jul):1485–1510 
2009.

[37] Huan Xu and Shie Mannor. Robustness and generalization. Machine learning  86(3):391–

423  2012.

[38] Dong Yin  Kannan Ramchandran  and Peter Bartlett. Rademacher complexity for

adversarially robust generalization. arXiv preprint arXiv:1810.11914  2018.

11

,Pranjal Awasthi
Abhratanu Dutta
Aravindan Vijayaraghavan