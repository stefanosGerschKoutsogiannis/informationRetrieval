2014,Robust Tensor Decomposition with Gross Corruption,In this paper  we study the statistical performance of robust tensor decomposition with gross corruption. The observations are noisy realization of the superposition of a low-rank tensor $\mathcal{W}^*$ and an entrywise sparse corruption tensor $\mathcal{V}^*$. Unlike conventional noise with bounded variance in previous convex tensor decomposition analysis  the magnitude of the gross corruption can be arbitrary large. We show that under certain conditions  the true low-rank tensor as well as the sparse corruption tensor can be recovered simultaneously. Our theory yields nonasymptotic Frobenius-norm estimation error bounds for each tensor separately. We show through numerical experiments that our theory can precisely predict the scaling behavior in practice.,Robust Tensor Decomposition with Gross Corruption

Quanquan Gu∗

Department of Operations Research

and Financial Engineering

Princeton University
Princeton  NJ 08544

qgu@princeton.edu

Huan Gui∗ Jiawei Han

Department of Computer Science

University of Illinois
at Urbana-Champaign

Urbana  IL 61801

{huangui2 hanj}@illinois.edu

Abstract

In this paper  we study the statistical performance of robust tensor decomposition
with gross corruption. The observations are noisy realization of the superposition
of a low-rank tensor W∗ and an entrywise sparse corruption tensor V∗. Unlike
conventional noise with bounded variance in previous convex tensor decomposition
analysis  the magnitude of the gross corruption can be arbitrary large. We show
that under certain conditions  the true low-rank tensor as well as the sparse cor-
ruption tensor can be recovered simultaneously. Our theory yields nonasymptotic
Frobenius-norm estimation error bounds for each tensor separately. We show
through numerical experiments that our theory can precisely predict the scaling
behavior in practice.

1

Introduction

Tensor data analysis have witnessed increasing applications in machine learning  data mining and
computer vision. For example  an ensemble of face images can be modeled as a tensor  whose mode
corresponds to pixels  subjects  illumination and viewpoint [23]. Traditional tensor decomposition
methods such as Tucker decomposition and CANDECOMP/PARAFAC(CP) decomposition [14  13]
aim to factorize an input tensor into a number of low-rank factors. However  they are prone to local
optima because they are solving essentially non-convex optimization problems. In order to address
this problem  [15] [20] extended the trace norm of matrices [19] to tensors  and generalized convex
matrix completion [8] [7] and matrix decomposition [6] to convex tensor completion/decomposition.
For example  the goal of tensor decomposition aims to accurately estimate a low-rank tensor W ∈
Rn1×...×nK from the noisy observation tensor Y ∈ Rn1×...×nK that is contaminated by dense
noises  i.e.  Y = W∗ + E  where W∗ ∈ Rn1×...×nK is a low-rank tensor  E ∈ Rn1×...×nK is a
noise tensor whose entries are i.i.d. Gaussian noise with zero mean and bounded variance σ2  i.e. 
Ei1 ... iK ∼ N (0  σ2). [22] [21] analyzed the statistical performance of convex tensor decomposition
under different extensions of trace norm. They showed that  under certain conditions  the estimation
error scales with the rank of the true tensor W∗. Furthermore  they demonstrated that given a noisy
tensor  the true low-rank tensor can be recovered under restricted strong convexity assumption [18].
However  all these algorithms [15] [20] and theoretical results [22] [21] reply on the assumption that
the observation noise has a bounded variance σ2. Without this assumption  we are not able to identify
W∗.
On the other hand  in many practical applications such as face recognition and image/video denoising 
a portion of the observation tensor Y might be contaminated by gross error due to illumination 
occlusion or pepper/salt noise. This scenario is not covered by ﬁnite variance noise assumption 
therefore new mathematical models are demanded to address this problem. This motivates us to study

the rank of W∗  and therefore the estimated low-rank tensor(cid:99)W could be very far from the true tensor

∗Equal Contribution

1

convex tensor decomposition with gross corruption. It is clear that if all the entries of a tensor are
corrupted by large error  there is no hope to recover the underlying low-rank tensor. To overcome
this problem  one common assumption is that the gross corruption is sparse. Under this assumption 
together with previous low-rank assumption  we formalize the noisy linear observation model as
follows:

Y = W∗ + V∗ + E 

(1)
where W∗ ∈ Rn1×...×nK is a low-rank tensor  V∗ ∈ Rn1×...×nK is a sparse corruption tensor  where
the locations of nonzero entries are unknown and the magnitudes of the nonzero entries can be
arbitrarily large  and E ∈ Rn1×...×nK is a noise tensor whose entries are i.i.d. Gaussian noise with
zero mean and bounded variance σ2  and thus dense. Our goal is to recover the low-rank tensor W∗ 
as well as the sparse corruption tensor V∗. Note that in some applications  the corruption tensor is of
independent interest and needs to be recovered.
Given the observation model in (1)  and the low-rank as well as sparse assumptions on W∗ and E∗
respectively  we propose the following convex minimization to estimate the unknown low-rank tensor
W∗ and the sparse corruption tensor E∗ simultaneously:

F + λM |||W|||S1

+ µM |||V|||1  

arg minW V |||Y − W − V|||2

(2)
is tensor Schatten-1 norm [22]  |||·|||1 is entry-wise (cid:96)1 norm of tensors  and λM and µM
where |||·|||S1
are positive regularization parameters. We call this optimization Robust Tensor Decomposition  which
can been seen as a generalization of convex tensor decomposition in [15] [20] [22]. The regularization
associated with the E encourages sparsity on the corruption tensor  where parameter µM controls the
sparsity level. In this paper  we focus on the following questions: under what conditions for the size
of the tensor  the rank of the tensor  and the fraction (sparsity level) of the corruption so that: (i) (2) is
able to recover W∗ and V∗ with small estimator error? (ii) (2) is able to recover the exact rank of
W∗ and the support of V∗? We will present nonasymptotic error bounds to answer these questions.
Experiments on synthetic datasets validate our theoretical results.
The rest of this paper is arranged as follows. Related work is discussed in Section 2. Section 3
introduces the background and notations. Section 4 presents the main results. Section 5 provides
an ADMM algorithm to solve the problem  followed by the numerical experiments in Section 6.
Section 7 concludes this work with remarks.

2 Related Work

The problem of recovering the data under gross error has gained many attentions recently in matrix
decomposition. A large body of work have been proposed and analyzed statistically. For example 
[9] considered the problem of recovering an unknown low-rank and an unknown sparse matrix  given
the sum of the two matrices. [5] proposed a similar problem  namely robust principal component
analysis (RPCA)  which studies the problem of recovering the low-rank and sparse matrices by
solving a convex program. [10] studied multi-task regression which decomposes the coefﬁcient
matrix into two matrices  and imposes different group sparse regularization on two matrices. [25]
considered more general case  where the parameter matrix could be the superposition of more than
two matrices with different structurally constraints. Our paper extends [5] from two perspective: we
extend the problem from matrices to high-order tensors  and consider the additional noise setting.
We notice that [16] extended RPCA to tensors  which aims to recover the low-rank and sparse
tensors by solving a constrained convex program. However  our formulation departs from [16] in
that we consider not only the sparse corruption  but also the dense noise. We also note that low-rank
noisy matrix completion [17] and robust matrix decomposition [1] [12] have been studied in in
the high dimensional setting as well. Our model can be seen as the high-order extension of robust
matrix decomposition. This extension is nontrivial  because the treatment of the tensor trace norm
(Schatten-1 norm) is more complicated. More importantly  for the robust matrix decomposition
problem considered [1]  only the sum of error bound of two matrices (low-rank matrix and the sparse
corruption matrix) can be obtained under the assumption of restricted strongly convexity. In contrast 
under a different condition  our analysis provides error bound for each tensor component (low-rank
tensor and the sparse corruption tensor) separately  making our results more appealing in practice
and of independent theoretical interest. Since the problem in [1] is a special case of our problem  our

2

technical tool can be directly applied to their problem and yields new error bounds on the low-rank
matrix as well as the sparse corruption matrix separately.

3 Notation and Background

Before proceeding  we deﬁne our notation and state assumptions that will appear in various parts of
the analysis. For more details about tensor algebra  please refer to [14].
Scalars are denoted by lower case letters (a  b  . . .)  vectors by bold lower case letters (a  b  . . .) 
matrices by bold upper case letters (A  B  . . .)  and high-order tensors by calligraphic upper case
letters (A B  . . .). A tensor is a higher order generalization of a vector (ﬁrst order tensor) and a matrix
(second order tensor). From a multi-linear algebra view  tensor is a multi-linear mapping over a set of
vector spaces. The order of tensor A ∈ Rn1×...×n2×...×nK is K  where nk is the dimensionality of
the k-th order. Elements of A are denoted as Ai1...ik...in   1 ≤ ik ≤ nk. We denote the number of

elements in A by N =(cid:81)K

The mode-k vectors of a K order tensor A are the nk dimensional vectors obtained from A by
varying index ik while keeping the other indices ﬁxed. The mode-k vectors are the column vectors
of mode-k ﬂattening matrix A(k) ∈ Rnk×(n1...nk−1nk+1...nK ) that results by mode-k ﬂattening the
tensor A. For example  matrix column vectors are referred to as mode-1 vectors and matrix row
vectors are referred to as mode-2 vectors.
The scalar product of

is deﬁned as (cid:104)A B(cid:105) =
Ai1...iKBi1...iK = vec(A)vec(B)  where vec(·) is a vectorization. The Frobenius

. . .(cid:80)
(cid:80)
norm of a tensor A is |||A|||F =(cid:112)(cid:104)A A(cid:105).

two tensors A B ∈ Rn1...n2...nK  

iK

i1

k=1 nk.

There are multiple ways to deﬁne tensor rank. In this paper  following [22]  we deﬁne the rank of
a tensor based on the mode-k rank of a tensor. More speciﬁcally  the mode-k rank of a tensor X  
denoted by rankk(X)  is the rank of the mode-k unfolding X(k) (note that X(k) is a matrix  so its
rank is well-deﬁned). Based on mode-k rank  we deﬁne the rank of tensor X as r(X ) = (r1  . . .   rk)
if the mode-k rank is rk for k = 1  . . .   K. Note that the mode-k rank can be computed in polynomial
time  because it boils down to computing a matrix rank  whereas computing tensor rank [14] is NP
complete.
(cid:80)K
Similarly  we extend the trace norm (a.k.a. nuclear norm) of matrices [19] to tensors. The overlapped
Schatten-1 norm is deﬁned as |||X|||S1
k=1 (cid:107)X(k)(cid:107)S1  where X(k) is the mode-k unfolding
j=1 σj(X)  where σj(X) is the
j-th largest singular value of X. The dual norm of the Schatten-1 norm is Schatten-∞ norm (a.k.a. 
spectral norm) as (cid:107)X(cid:107)S∞ = maxj=1 ... r σj(X).
By H¨older’s inequality  we have |(cid:104)W  X(cid:105)| ≤ (cid:107)W(cid:107)S1(cid:107)X(cid:107)S∞. It is easy to prove a similar result for
the overlapped Schatten-1 norm and its dual norm. We have the following H¨older-like inequality [22]:

of X   and (cid:107) · (cid:107)S1 is the Schatten-1 norm for a matrix  (cid:107)X(cid:107)S1 =(cid:80)r

= 1
K

(cid:80)K
|(cid:104)W X(cid:105)| ≤ |||W|||S1
k=1 (cid:107)X(k)(cid:107)S∞.

|||X|||S∗

1

≤ |||W|||S1

|||X|||mean  

(3)

K

where |||X|||mean := 1

Moreover  we deﬁne (cid:96)1-norm and (cid:96)∞-norm for tensors that |||X|||1 =(cid:80)n1

iK =1 |Xi1 ... iK| 
|||X|||∞ = max1≤i1≤n1 . . . max1≤iK≤nK |Xi1 ... iK|. By H¨older’s inequality  we have |(cid:104)W X(cid:105)| ≤
|||W|||1 |||X|||∞  and the following inequality relates the overlapped Schatten-1 norm with the Frobe-
nius norm 

i1=1 . . .(cid:80)nK

≤ K(cid:88)

|||X|||S1

√

rk |||X|||F .

(4)

k=1

Let W∗ ∈ Rn1×...×nK be the low-rank tensor that we wish to recover. We assume that W∗ is
k   where Uk ∈ Rnk×rk and
of rank (r1  . . .   rK). Thus  for each k  we have W∗
Vk ∈ Rrk×nk are orthogonal matrices  which consist of left and right singular vectors of W∗
(k) 
Sk ∈ Rrk×rk is a diagonal matrix whose diagonal elements are singular values. Let ∆ ∈ Rn1×...×nK

(k) = UkSkV(cid:62)

3

(5)

k )∆(k)(I ¯N\k

k of its mode-k unfolding

be an arbitrary tensor  we deﬁne the mode-k orthogonal complement ∆(cid:48)(cid:48)
∆(k) ∈ Rnk× ¯N\k with respect to the true low-rank tensor W∗ as follows
− VkV(cid:62)
k ).

(k). Note that the decomposition ∆(k) = ∆(cid:48)

k = (Ink − UkU(cid:62)
∆(cid:48)(cid:48)
k = ∆(k) − ∆(cid:48)(cid:48)
In addition ∆(cid:48)
k is the component which has overlapped row/column space with the
unfolding of the true tensor W∗
k is deﬁned for
each mode.
In [18]  the concept of decomposibility and a large class of decomposable norms are discussed
at length. Of particular relevance to us is the decomposability of the Schatten-1 norm and (cid:96)1-
norm. We have the following equality  i.e.  mode-k decomposibility of the Schatten-1 norm that
k(cid:107)S1  k = 1  . . .   K. To note that the decomposibility is deﬁned
(cid:107)W∗
on each mode. It is also easy to check the decomposibility of the (cid:96)1-norm.
(cid:8)(i1  i2  . . .   iK) ∈ [n1] × . . . × [nK]|V∗
Let V∗ ∈ Rn1×...×nK be the gross corruption tensor that we wish to recover. We assume the the
gross corruption is sparse  in that the cardinality s = |supp(V∗)| of its support  S = supp(V∗) =
between the (cid:96)1 norm and the Forbenius norm that |||V∗|||1 ≤ √
s|||V∗|||F . Moreover  we have
|||V∗|||1 = |||V∗

(cid:54)= 0(cid:9). This assumption leads to the inequality

S|||1. For any D ∈ Rn1×...×nK   we have |||D|||1 = |||DS|||1 + |||DSc|||1 .

(k)(cid:107)S1 +(cid:107)∆(cid:48)(cid:48)

k(cid:107)S1 = (cid:107)W∗

(k) + ∆(cid:48)(cid:48)

k + ∆(cid:48)(cid:48)

i1 ... iK

4 Main Results

To get a deep theoretical insight into the recovery property of robust tensor decomposition  we will
now present a set of estimation error bounds. Unlike the analysis in [1]  where only the summation
of the estimation errors on the low-rank matrix and gross corruption matrix are analyzed  we aim
at obtaining the estimation error bounds on each tensor (the low-rank tensor and corrupted tensor)
separately. All the proofs can be found in the longer version of this paper.
Instead of considering the observation model in 1  we consider the following more general observation
model

yi = (cid:104)W∗ Xi(cid:105) + (cid:104)V∗ Xi(cid:105) + i  i = 1  . . .   M 

(6)
where Xi can be seen as an observation operator  and i’s are i.i.d. zero mean Gaussian noise with
variance σ2. Our goal is to estimate an unknown rank (r1  . . .   rk) of tensor W∗ ∈ Rn1×...×nK  
as well as the unknown support of tensor V∗  from observations yi  i = 1  . . .   M. We propose
the following convex minimization to estimate the unknown low-rank tensor W∗ and the sparse
corruption tensor V∗ simultaneously  with composite regularizers on W and V as follows:

((cid:99)W (cid:98)V) = arg minW V

1
2M

(cid:107)y − X(W + V)(cid:107)2

2 + λM |||W|||S1

+ µM |||V|||1  

1  . . .   y∗

M )(cid:62)  where y∗

(7)
where y = (y1  . . .   yM )(cid:62) is the collection of observations  X(W) is the linear observation model
that X(W) = [(cid:104)W X1(cid:105)  . . .  (cid:104)W XM(cid:105)](cid:62). Note that (2) is a special case of (7)  where the linear
operator the identity tensor  we have yi as observation of each element in the summation of tensors
W∗ + V∗.
i = (cid:104)W∗ + V∗ Xi(cid:105)  is the true evaluation. Due to the
We also deﬁne y∗ = (y∗
noise of observation model  we have y = y∗ + . In addition  we deﬁne the adjoint operator of X as

X∗ : RM → Rn1×...×nK that X∗() =(cid:80)M
This section is devoted to obtain the deterministic bound of the residual low-rank tensor ∆ = (cid:99)W−W∗
and residual corruption tensor D = (cid:98)V − V∗ separately  which makes our analysis unique.
We begin with a key technical lemma on residual tensors ∆ = (cid:99)W − W∗ and D = (cid:98)V − V∗  obtained
Lemma 1. Let(cid:99)W and(cid:98)V be the solution of minimization problem (7) with λM ≥ 2|||X∗()|||mean/M 
µM ≥ 2|||X∗()|||∞/M  we have

from the convex problem in (7).

4.1 Deterministic Bounds

i=1 iXi.

4

k) ≤ 2rk.

1. rank(∆(cid:48)

2. There exist β1 ≥ 3 and β2 ≥ 3  such that (cid:80)K

k=1 (cid:107)∆(cid:48)(cid:48)

k(cid:107)S1 ≤ β1

|||DSc|||1 ≤ β2 |||DS|||1.

k(cid:107)S1 and
The lemma can be obtained by utilizing the optimality of(cid:99)W and(cid:98)V  as well as the decomposibility of
Schatten-1 norm and (cid:96)1-norm of tensors.
Theorem 1. Let (cid:99)W and (cid:98)V be the solution of minimization problem (7) with λM ≥
Also  we obtain the key property of the optimal solution of (7)  presented in the following theorem.
K(cid:88)
2|||X∗()|||mean/M  µM ≥ 2|||X∗()|||∞/M  we have

3µM

(cid:80)K
k=1 (cid:107)∆(cid:48)

1
2M

(cid:107)X(∆ + D)(cid:107)2

2 ≤ 3λM
2K

(cid:107)∆(cid:48)

k(cid:107)S1 +

|||DS|||1 .

2

(8)

k=1

Theorem 1 provides a deterministic prediction error bound for model (7). This is a very general
result  and can be applied to any linear operator X  including the robust tensor decomposition case
that we are particularly interested in this paper. It also covers  for example  tensor regression  tensor
compressive sensing  to mention a few.
Furthermore  we impose an assumption on the linear operator and the residual low-rank tensor and
residue sparse corruption tensor  which generalized the restricted eigenvalue assumption [2] [10].
k(cid:107)S1  |||DSc|||1 ≤
β2 |||DS|||1}  we assume there exist positive scalars κ1  κ2 that
> 0  κ2 = min
∆ D∈Ω

Assumption 1. Deﬁning Ω = {(∆ D)|(cid:80)K

(cid:80)K
k=1 (cid:107)∆(cid:48)

k(cid:107)S1 ≤ β1

κ1 = min
∆ D∈Ω

k=1 (cid:107)∆(cid:48)(cid:48)

> 0.

(cid:107)X(∆ + D)(cid:107)2
√
M |||∆|||F

(cid:107)X(∆ + D)(cid:107)2
√
M |||D|||F

Note that Assumption 1 is also related to restricted strong convexity assumption  which is proposed
in [18] to analyze the statistical properties of general M-estimators in the high dimensional setting.
Combing the results in Theorem 1 and Assumption 1  we have the following theorem  which
Theorem 2. Let(cid:99)W (cid:98)V be an optimal solution of (7)  and take the regularization parameters λM ≥
summarizes our main result.
(cid:33)
2|||X∗()|||mean/M  µM ≥ 2|||X∗()|||∞/M. Then the following results hold:
(cid:33)

µM
κ2
√

(cid:32)
(cid:32)

≤ 3
κ1

κ1
√

1
K

2rk

λM

√

√

(9)

+

 

K(cid:88)
K(cid:88)

k=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:99)W − W∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)V − V∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F

s

s

≤ 3
κ2

1
K

λM

2rk

κ1

k=1

+

µM
κ2

.

(10)

Theorem 2 provides us with the error bounds of each tensor separately. Speciﬁcally  these bounds not
only measure how well our decomposition model can approximate the observation model deﬁned
in (6)  but also measure how well the decomposition of the true low-rank tensor and gross corruption
tensor is. When s = 0  our theoretical results reduce to that proposed in [22]  which is a special case
of our problem  i.e.  noisy low-rank tensor decomposition without corruption.
On the other hand  the results obtained in Theorem 2 are very appealing both practically and
theoretically. From the perspective of applications  this result is quite useful as it helps us to better
understand the behavior of each tensor separately. From the theoretical point of view  this result is
novel  and is incomparable with previous results [1][17] or simple generalization of previous results.

Though Theorem 2 has provided estimation error bounds of(cid:99)W and(cid:98)V  it is unclear whether the rank

of W∗ and the support of V∗ can be exactly recovered. We show that under some assumptions about
the true tensors  both of them can be exactly recovered.
Corollary 1. Under the same conditions of Theorem 2  if the following condition holds:

 

(11)

6(1 + β1)(cid:80)K

k=1

κ1M K

(cid:32)

1
K

K(cid:88)

√

λM

2rk

κ1

k=1

(cid:33)

√

s

+

µM
κ2

σrk (W∗

(k)) >

√

2rk

5

where σrk (W∗

(cid:26)

(k)) is the rk-th largest singular value of W∗
2rk

3(1 + β1)(cid:80)K

σr((cid:99)W(k)) >

arg max

√

k=1

r

κ1M K

(k)  then

(cid:32)

K(cid:88)

1
K

(cid:98)rk =

√

2rk

λM

κ1

k=1

(cid:33)(cid:27)

s

√

µM
κ2

+

recovers the rank of W∗
Furthermore  if the following condition holds:

(k) for all k.

|V∗

i1 ... iK

| >

min
i1 ... iK

then

(cid:26)

(cid:98)S =

(i1  i2  . . .   iK) : (cid:98)Vi1 ... iK >

√
6(1 + β2)
κ2M

(cid:32)

s

1
K
√
3(1 + β2)
κ2M

K(cid:88)
(cid:32)

k=1

s

√

2rk

λM

κ1

K(cid:88)

k=1

1
K

(cid:33)

s

 

√

µM
κ2

+

√

2rk

λM

κ1

+

µM
κ2

(12)

(cid:33)(cid:27)

s

√

recovers the true support of V∗.
Corollary 1  basically states that  under the assumption that the singular values of the low-rank tensor
W∗  and the entry values of corruption tensor V∗ are above the noise level (e.g.  (11) and (12))  we
can recover the rank and the support successfully.

4.2 Noisy Tensor Decomposition

Now we are going back to study robust tensor decomposition with corruption in (2)  which is a special
case of (7)  where the linear operator is identity tensor. As the linear operator X is a vectorization such
that M = N  and (cid:107)X(∆ + D)(cid:107)2 = |||∆ + D|||F . In addition  it is easy to show that Assumption 1
√
N ). It remains to bound |||X∗()|||mean and |||X∗()|||∞  as shown in
holds with κ1 = κ2 = O(1/
the following lemma [1] [24].
Lemma 2. Suppose that X : Rn1×···×nK → RN is a vectorization of a tensor. Then we have with
probability at least 1 − 2 exp(−C(nk + ¯N\k)) − 1/N that

(cid:18)√
K(cid:88)
|||X∗()|||∞ ≤ 4σ(cid:112)log N  

|||X∗()|||mean ≤ σ
K

k=1

(cid:113)

(cid:19)

nk +

¯N\k

 

where C is a universal constant.

With Theorem 2 and Lemma 2  we immediately have the following estimation error bounds for robust
tensor decomposition.
Theorem 3. Suppose that X : Rn1×···×nK → RN is a vectorization of a tensor. Then for the
log N /N  with
probability at least 1 − 2 exp(−C(nk + ¯N\k)) − 1/N  any solution of (2) have the following error

/(N K)  µN > 8σ

nk +

√

k=1

(cid:16)√
regularization constants λN ≥ 2σ(cid:80)K
σ(cid:80)K
bound: (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:99)W − W∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F
K(cid:88)
σ(cid:80)K
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)V − V∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F
K(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)V − V∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F

Ks log n(cid:1) and

O(cid:0)σ

rnK−1 + σ

(cid:32)
(cid:32)

≤ 6
κ2

≤ 6
κ1

1
K

1
K

√

√

k=1

k=1

k=1

k=1

(cid:113) ¯N\k
(cid:17)
(cid:113) ¯N\k
(cid:113) ¯N\k

κ1N K

nk +

nk +

(cid:16)√
(cid:16)√

(cid:17)√
(cid:17)√

2rk

2rk

κ1N K

= O(cid:0)σ

√

√

rnK−1 + σ

+

4σ

√

s log N
κ2N
√

(cid:33)
(cid:33)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:99)W − W∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F
Ks log n(cid:1)  which matches

s log N
κ2N

4σ

=

+

 

.

In the special case that n1 = . . . = nK = n and r1 = . . . = rK = r  we have

the error bound of robust matrix decomposition [1] when K = 2.
Note that the high probability support and rank recovery guarantee for the special case of tensor
decomposition follows immediately from Corollary 1. Due to the space limit  we omit the result here.

6

5 Algorithm

In this section  we present an algorithm to solve (2). Since (2) is a special case of (7)  we consider
the more general problem (7). It is easy to show that (7) is equivalent to the following problem with
auxiliary variables Ψ  Φ:

|||Ψk|||S1

+

µM
K

|||Φk|||1  

K(cid:88)

k=1

K(cid:88)

k=1

K

(cid:88)

k

(cid:33)

(cid:33)

1
2M

(cid:107)y − x(cid:62)(w + v)(cid:107)2

minW V Y Z
subject to Pkw = ψk  Pkv = φk 

where x  w  v  ψk  φk are the vectorizations of(cid:80)M

2 +

λM
K

transformation matrix that change the order of rows and columns so that Pkw = ψk.
The augmented Lagrangian (AL) function of the above minimization problem with respect to the
primal variables (W t V t) is given as follows:

i=1 Xi W V  Ψk  Φk respectively  and Pk is the

k=1 {Φk}K

Lη(W V {Ψk}K
(cid:107)y − x(cid:62)(w + v)(cid:107)2
1
2

=

2 +

k=1 {αk}K
K(cid:88)
λM M

k=1 {βk}K
|||Ψk|||S1

+

K

k=1

k=1)

µM M

K(cid:88)

k=1

|||Φk|||1

+η

(α(cid:62)

k (Pkw − ψk) +

(cid:107)Pkw − ψk(cid:107)2

2) +

1
2

(β(cid:62)

k (Pkv − φk) +

(cid:32)(cid:88)

k

(cid:33)

 

(cid:107)Pkv − φk(cid:107)2
2)

1
2

where αt  βt are Lagrangian multiplier vectors  and η > 0 is a penalty parameter.
We then apply the algorithm of Alternating Direction Method of Multipliers (ADMM)
[3  20]
Starting from initial points
(w0  v0 {Ψ0
k=1)  ADMM performs the following updates
iteratively:

above optimization problem.

k=1 {Φ0

k}K

to solve
k}K
(cid:32)
(cid:32)

the
k=1 {α0
k}K
k}K
k=1 {β0
K(cid:88)
K(cid:88)

P(cid:62)
k (ψt

k=1

wt+1 =

vt+1 =

(x(cid:62)y − x(cid:62)xvt) + η

k − αt
k)

/ (1 + ηK)  

(x(cid:62)y − x(cid:62)xwt+1) + η

P(cid:62)
k (φt

k − βt
k)

/ (1 + ηK)  

k=1

λM
ηK

αt+1

Ψt+1

(Pkwt+1 + αt

k = proxtr
k = αt+1

k) 
k + (Pkwt+1 − ψt+1
γ (·) is the soft-thresholding operator for trace norm  and prox(cid:96)1

k = 1  . . .   K 
γ (·) is the soft-thresholding
where proxtr
operator for (cid:96)1 norm [4  11]. The stopping criterion is that all the partial (sub)gradients are (near)
zero  under which condition we obtain the saddle point of the augmented Lagrangian function. Since
(7) is strictly convex  the saddle point is the global optima for the primal problem.

k + (Pkvt+1 − φt+1

k = prox(cid:96)1
Φt+1
βt+1
k = βt+1

(Pkvt+1 + βt
k)

k = 1  . . .   K 

µM
ηK

)

)

k

k

6 Experiments

In this section  we conduct numerical experiments to conﬁrm our analysis in previous sections. The
experiments are conducted under the setting of robust noisy tensor decomposition.
We follow the procedure described in [22] for the experimental part. We randomly generate low-rank
tensors of dimensions n(1) = (50  50  20) ( results are shown in Figure 1(a  b  c)) and n(2) =
(100  100  50)( results are shown in Figure 1(d  e  f)) for various rank (r1  r2  ...  rk). Given a speciﬁc
rank  we ﬁrst generated the ”core tensor” with elements r1 × . . . × rK from the standard normal
distribution  and then multiplied each mode of the core tensor with an orthonormal factor randomly
drawn from the Haar measure. For the gross corruption  we randomly generated the sparsity of
the corruption matrix s  and then randomly selected s elements in which we put values randomly
generated from uniform distribution. The additive independent Gaussian noise with variance σ2

7

(a) |||∆|||F

M against Ns of size n(1). (b) |||∆|||F

M against Nr of size n(1).

(d) |||∆|||F

M against Ns of size n(2). (e) |||∆|||F

M against Nr of size n(2).

(c) κ1 against κ2 of size n(1).

(f) κ1 against κ2 of size n(2).

Figure 1: Results of robust noisy tensor decomposition with corruption  under different sizes.

was added to the observations of elements. We use the alternating direction method of multipliers
(ADMM) to solve the minimization problem (2). The whole experiments were repeated 50 times and
the averaged results are reported.

The results are shown in Figure 1  where Nr =(cid:80)K
Nr at different values  and then draw the value of(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:99)W − W∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F
s. In Figure 1(a  d)  we ﬁrst ﬁx
e)  we ﬁrst ﬁx Ns at different values  and then draw(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:99)W − W∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F
/N against Ns. Similarly  in Figure 1(b 
study the values of κ1 and κ2 at various settings. We can see that (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:99)W − W∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F
/N against Nr. In Figure 1(c  f)  we
with both Ns and Nr. Similar scalings of(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)V − V∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F
/N scales linearly
/N can be observed  hence we omit them due
this ﬁnding is consistent with the fact that (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:99)W − W∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F
to space limitation. We can also observe from Figure 1(c  f) that  under various settings  κ1 ≈ κ2 
/N. All these results are

/N ≈ (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:98)V − V∗(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)F

rk/K  and Ns =

√

√

k=1

consistent with each other  validating our theoretical analysis.

7 Conclusions

In this paper  we analyzed the statistical performance of robust noisy tensor decomposition with
corruption. Our goal is to recover a pair of tensors  based on observing a noisy contaminated version
of their sum. It is based on solving a convex optimization with composite regularizations of Schatten-1
norm and (cid:96)1 norm deﬁned on tensors. We provided a general nonasymptotic estimator error bounds on
the underly low-rank tensor and sparse corruption tensor. Furthermore  the error bound we obtained
in this paper is new  and non-comparable with previous theoretical analysis.

Acknowledgement

We would like to thank the anonymous reviewers for their helpful comments. Research was sponsored
in part by the Army Research Lab  under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA) 
the Army Research Ofﬁce under Cooperative Agreement No. W911NF-13-1-0193  National Science
Foundation IIS-1017362  IIS-1320617  and IIS-1354329  HDTRA1-10-1-0120  and MIAS  a DHS-
IDS Center for Multimodal Information Access and Synthesis at UIUC.

8

101520253035400123456x 10−4Nsmean error of low−rank tensor Nr = 2.9Nr = 4.0Nr = 5.433.544.555.50123456x 10−4Nrmean error of low−rank tensor Ns = 17Ns = 25Ns = 3501234567x 10−6012345678x 10−6κ1κ2101520253035456789101112x 10−5Nsmean error of low−rank tensor Nr = 2.9Nr = 4.0Nr = 4.911.522.533.5423456789x 10−5Nrmean error of low−rank tensor Ns = 15.8Ns = 22.4Ns = 31.60.511.522.533.54x 10−60.511.522.533.54x 10−6κ1κ2References
[1] A. Agarwal  S. Negahban  and M. J. Wainwright. Noisy matrix decomposition via convex relaxation:

Optimal rates in high dimensions. The Annals of Statistics  40(2):1171–1197  04 2012.

[2] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of lasso and dantzig selector. The Annals

of Statistics  pages 1705–1732  2009.

[3] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundations and Trends R(cid:13) in Machine Learning 
3(1):1–122  2011.

[4] J.-F. Cai  E. J. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM

Journal on Optimization  20(4):1956–1982  2010.

[5] E. J. Cand`es  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? J. ACM  58(3):11  2011.
[6] E. J. Cand`es and Y. Plan. Matrix completion with noise. Proceedings of the IEEE  98(6):925–936  2010.
[7] E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Commun. ACM  55(6):111–

119  2012.

[8] E. J. Cand`es and T. Tao. The power of convex relaxation: near-optimal matrix completion.

Transactions on Information Theory  56(5):2053–2080  2010.

IEEE

[9] V. Chandrasekaran  S. Sanghavi  P. A. Parrilo  and A. S. Willsky. Rank-sparsity incoherence for matrix

decomposition. SIAM Journal on Optimization  21(2):572–596  2011.

[10] P. Gong  J. Ye  and C. Zhang. Robust multi-task feature learning. In Proceedings of the 18th ACM SIGKDD

international conference on Knowledge discovery and data mining  pages 895–903. ACM  2012.

[11] E. T. Hale  W. Yin  and Y. Zhang. Fixed-point continuation for \ell 1-minimization: Methodology and

convergence. SIAM Journal on Optimization  19(3):1107–1130  2008.

[12] D. Hsu  S. M. Kakade  and T. Zhang. Robust matrix decomposition with sparse corruptions. IEEE

Transactions on Information Theory  57(11):7221–7234  2011.

[13] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review  51(3):455–500 

2009.

[14] L. D. Lathauwer  B. D. Moor  and J. Vandewalle. On the best rank-1 and rank-(r1 r2 . . . rn) approximation

of higher-order tensors. SIAM J. Matrix Anal. Appl.  21(4):1324–1342  2000.

[15] J. Liu  P. Musialski  P. Wonka  and J. Ye. Tensor completion for estimating missing values in visual data.

IEEE Trans. Pattern Anal. Mach. Intell.  35(1):208–220  2013.

[16] C. Mu  B. Huang  J. Wright  and D. Goldfarb. Square deal: Lower bounds and improved relaxations for

tensor recovery. CoRR  abs/1307.5870  2013.

[17] S. Negahban and M. J. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional

scaling. The Annals of Statistics  39(2):1069–1097  04 2011.

[18] S. N. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers. Statistical Science  27(4):538–557  11 2012.

[19] N. Srebro and A. Shraibman. Rank  trace-norm and max-norm. In COLT  pages 545–560  2005.
[20] R. Tomioka  K. Hayashi  and H. Kashima. Estimation of low-rank tensors via convex optimization. 2010.
[21] R. Tomioka and T. Suzuki. Convex tensor decomposition via structured schatten norm regularization. In

NIPS  pages 1331–1339  2013.

[22] R. Tomioka  T. Suzuki  K. Hayashi  and H. Kashima. Statistical performance of convex tensor decomposi-

tion. In NIPS  pages 972–980  2011.

[23] M. A. O. Vasilescu and D. Terzopoulos. Multilinear analysis of image ensembles: Tensorfaces. In ECCV

(1)  pages 447–460  2002.

[24] R. Vershynin.

Introduction to the non-asymptotic analysis of random matrices.

arXiv:1011.3027  2010.

arXiv preprint

[25] E. Yang and P. D. Ravikumar. Dirty statistical models. In NIPS  pages 611–619  2013.

9

,Quanquan Gu
Huan Gui
Jiawei Han
Kent Quanrud
Daniel Khashabi
ChenHan Jiang
Hang Xu
Xiaodan Liang
Liang Lin
Chen Tessler
Guy Tennenholtz
Shie Mannor