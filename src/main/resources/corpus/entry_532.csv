2010,An analysis on negative curvature induced by singularity in multi-layer neural-network learning,In the neural-network parameter space   an attractive field is likely to be induced by singularities. In such a singularity region  first-order gradient learning typically causes a long plateau with very little change  in the objective function value E (hence  a flat region). Therefore  it may be confused with ``attractive'' local minima. Our analysis shows that the Hessian matrix of E tends to be indefinite in the vicinity of (perturbed) singular points  suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence  we limit the scope to small examples (some of which are found in journal papers)  that allow us to confirm singularities and the eigenvalues of the Hessian matrix  and for which computation using a descent direction of negative curvature encounters no plateau.  Even for those small problems  no efficient methods have been previously developed that avoided plateaus.,An analysis on negative curvature induced by

singularity in multi-layer neural-network learning

Eiji Mizutani

Department of Industrial Management
Taiwan Univ. of Science & Technology

Stuart Dreyfus

Industrial Engineering & Operations Research

University of California  Berkeley

eiji@mail.ntust.edu.tw

dreyfus@ieor.berkeley.edu

Abstract

In the neural-network parameter space  an attractive ﬁeld is likely to be induced
by singularities. In such a singularity region  ﬁrst-order gradient learning typically
causes a long plateau with very little change in the objective function value E
(hence  a ﬂat region). Therefore  it may be confused with “attractive” local min-
ima. Our analysis shows that the Hessian matrix of E tends to be indeﬁnite in
the vicinity of (perturbed) singular points  suggesting a promising strategy that
exploits negative curvature so as to escape from the singularity plateaus. For nu-
merical evidence  we limit the scope to small examples (some of which are found
in journal papers) that allow us to conﬁrm singularities and the eigenvalues of
the Hessian matrix  and for which computation using a descent direction of nega-
tive curvature encounters no plateau. Even for those small problems  no efﬁcient
methods have been previously developed that avoided plateaus.

1 Introduction

Consider a general two-hidden-layer multilayer perceptron (MLP) having a single (terminal) output 
H nodes at the second hidden layer (next to the terminal layer)  I nodes at the ﬁrst hidden layer 
and J nodes at the input layer; hence  a J-I-H-1 MLP. It has totally n parameters  denoted by an
n-vector (cid:18)  including thresholds. Let (cid:30)(:) be some node function; then  the forward pass transforms
the input vector x of length J to the ﬁrst hidden-output vector z of length I  and then to the second
hidden-output vector h of length H  leading to the ﬁnal output y:
j=0 pj(cid:30)(zT

(1)
Here  ﬁctitious outputs x0 = z0 = h0 = 1 are included in the output vectors with subscript “+” for
thresholds p0  v0;j  and w0;k; pj (j = 1; :::; H) is the weight connecting the jth hidden node to the
(ﬁnal) output; vj a vector of “hidden” weights directly connecting to the jth hidden node from the
ﬁrst hidden layer; wk a vector of “hidden” weights to the kth hidden node from the input layer;

+vj)(cid:17) with zk = (cid:30)(xT

+p(cid:1) = (cid:30)(cid:16)PH

j=0 pjhj(cid:17) = (cid:30)(cid:16)PH

y = f ((cid:18); x) = (cid:30)(cid:0)hT

+wk):

hence  (cid:18)T (cid:17)(cid:2)pTjvTjwT(cid:3) =(cid:2)pTjvT

vectors (cid:18)  p  v  w are denoted by n  n3  n2  and n1  respectively  where

1 ; :::; vT

j ; :::; vT

1 ; :::; wT

k ; :::; wT

HjwT

I(cid:3). The length of those weight

(2)

n = n3 +n2 +n1; n3 = (H +1); n2 = H(I +1); n1 = I(J +1):

For parameter optimization  one may attempt to minimize the squared error over m data

E((cid:18)) =

1
2

mX

d=1

ff ((cid:18); xd)(cid:0)tdg2 =

1
2

r2
d((cid:18)) =

1
2

rT r;

(3)

mX

d=1

where td is a desired output on datum d; each residual rd a smooth function from <n to <; and r
an m-vector of residuals. Note here and hereafter that the argument ((cid:18)) for E and r is frequently
suppressed as long as no confusion arises. The gradient and Hessian of E can be expressed as below

rE((cid:18)) =

rdrrd = JT r; and r2E((cid:18)) =

rrdrrT

d +

rdr2rd(cid:17) JT J+S;

(4)

mXd=1

mXd=1

mXd=1

d .
where J(cid:17)rr  an m(cid:2)n Jacobian matrix of r  and the dth row of J is denoted by rr T

1

In the well-known Gauss-Newton method  S  the last matrix of second derivatives of residuals  is
omitted  and its search direction (cid:1)(cid:18) is found by solving J(cid:1)(cid:18)GN =(cid:0)r (or  JT J(cid:1)(cid:18)GN = (cid:0)rE). Under
the normal error assumption  the Fisher information matrix is tantamount to JT J  called the Gauss-
Newton Hessian. This is why natural gradient learning can be viewed as an incremental version of
the Gauss-Newton method (see p.1404 [1]; p.1031 [2]) in the nonlinear least squares sense. Since
JT J is positive (semi)deﬁnite  natural gradient learning has no chance to exploit negative curvature.
It would be of great value to understand the weaknesses of such Gauss-Newton-type methods.

f ((cid:18); x) = p1(cid:30)(v1x)+p2(cid:30)(v2x) = p1ev1x +p2ev2x:

Learning behaviors of layered networks may be attributable to singularities [3  2  4]. Singularities
have been well discussed in the nonlinear least squares literature also: For instance  Jennrich &
Sampson (pp.65–66 [5]) described an overlap-singularity situation involving a redundant model;
speciﬁcally  a classical (linear-output) model of exponentials with hi (cid:17) (cid:30)(vix) and no thresholds in
Eq.(1):
(5)
If the target data follow the path of a single exponential then the two hidden parameters  v1 and v2 
become identical (i.e.  overlap singularity) at the solution point  where J is rank-deﬁcient; hence 
JT J is singular. If the ﬁtted response function nearly follows such a path  then JT J is nearly singu-
lar. This is a typical over-realizable scenario  in which the true teacher lies at the singularity (see [6]
for details about 1-2-1 MLP-learning). In practice  if the solution point (cid:18) (cid:3) is stationary but J((cid:18)(cid:3)) is
rank-deﬁcient  then the search direction (cid:1)(cid:18)GN can be numerically orthogonal to rE at some distant
point from (cid:18)(cid:3); consequently  no progress can be made by searching along the Gauss-Newton direc-
tion (hence  line-search-based algorithms fail); this is ﬁrst pointed out by Powell  who proved in [7]
that the Gauss-Newton iterates converge to a non-stationary limit point at which J is rank-deﬁcient
in solving a particular system of nonlinear equations  for which the merit function is deﬁned as
Eq.(3)  where m = n. Another weak point of the Gauss-Newton-type method is a so-called large-
residual problem (e.g.  see Dennis [8]); this implies that S in r2E is substantial because r is highly
nonlinear  or its norm is large at solution (cid:18) (cid:3). Those drawbacks of the Gauss-Newton-type meth-
ods indicate that negative curvature often arises in MLP-learning when JT J is singular (i.e.  in a
rank-deﬁcient nonlinear least squares problem)  and/or when S is more dominant than JT J. We thus
verify this fact mathematically  and then discuss how exploiting negative curvature is a good way to
escape from singularity plateaus  thereby enhancing the learning capacity.

2 Negative curvature induced by singularity

In rank-deﬁcient nonlinear least squares problems  where J(cid:17)rr is rank deﬁcient  negative curva-
ture often arises. This is true with an arbitrary MLP model  but to make our analysis concrete 
we consider a single terminal linear-output two-hidden-layer MLP: f ((cid:18); x) =PH
j=0 pjhj in Eq. (1).
Then  the n weights separate into linear p and non-linear v and w. In this context  we show that a
4-by-4 indeﬁnite Hessian block can be extracted from the n-by-n Hessian matrix r2E in Eq.(4).
2.1 An existence of the 4 (cid:2) 4 indeﬁnite Hessian block H in r2E
In the posed two-hidden-layer MLP-learning  as indicated after Eq.(1)  the n weights are organized
as (cid:18)T (cid:17)(cid:2)pTjvTjwT(cid:3). Now  we pay attention to two particular hidden nodes j and k at the second
(cid:18)T =(cid:2)p0; p1; :::; pj; :::; pk; :::; pHjv0;1; :::::::; jv0;j; v1;j; :::; vI;jj:::jv0;k; v1;k; :::; vI;kj::::; j wT (cid:3) ;
(6)
where vi;k is a weight from node i at the ﬁrst hidden layer to node k at the second hidden layer.
Given a data pair (x; t)  r(cid:17) f ((cid:18); x)(cid:0)t  a residual element  and uT ; an n-length row vector of the
residual Jacobian matrix J ((cid:17) @r
@(cid:18) ) in Eq.(4)  is given as below using the output vector z+ (including
z0 = 1) at the ﬁrst hidden layer

hidden layer. The weights connecting to those two nodes are pj  pk  vj  and vk; they are arranged
in the following manner:

j(zT

+vj)pj; :::; (cid:30)0

k(zT

uT (cid:17)rrT =(cid:2):::; hj; :::; hk; :::; (cid:30)0

(7)
where only four entries are shown that are associated with four weights: pj  pk  v0;j  and v0;k. The
locations of those four weights in the n-vector (cid:18) are denoted by l1  l2  l3  and l4  respectively  where
(8)
Given J  we interchange columns 1 and l1; then  do columns 2 and l2; then columns 3 and l3; and
ﬁnally columns 4 and l4; this interchanging procedure moves those four columns to the ﬁrst four.

l4(cid:17) (I +1)(k(cid:0)1)+1:

l3(cid:17) (I +1)(j(cid:0)1)+1;

+vk)pk; :::(cid:3) ;

l1(cid:17) j +1;

l2(cid:17) k+1;

2

Suppose that the n(cid:2) n Hessian matrix r2E = uuT+S is evaluated on a given single datum (x; t).
We then apply the above interchanging procedure to both rows and columns of r2E appropriately 
which can be readily accomplished by PTr2E P  where four permutation matrices Pi (i = 1; :::; 4)
are employed as P(cid:17) P1P2P3P4; each Pi satisﬁes PT
i (symmetric);
hence  P is orthogonal. As a result  H  the 4-by-4 Hessian block (at the upper-left corner) of the
ﬁrst four leading rows and columns of PTr2E P has the following structure:
j(:)r
0

i Pi = I (orthogonal) and Pi = PT

j(:)pj
j(:)pj

hjhk
(hk)2

hj(cid:30)0
hk(cid:30)0

(hj)2

0
0

(cid:30)0

(cid:30)0

0

(cid:30)00

j (:)pjr

Symmetric

0
k(:)r
0

(cid:30)00

k (:)pkr

3
75:

(9)

=2
664
H|{z}4(cid:2)4

Symmetric

(cid:8)(cid:30)0
j(:)pj(cid:9)2

hj(cid:30)0
hk(cid:30)0
j(:)(cid:30)0
f(cid:30)0

k(:)pk
k(:)pk
k(:)pjpk
k(:)pkg2

(cid:30)0

3
775+2
64

The posed Hessian block H is associated with a vector of the four weights [pj; pk; v0;j; v0;k]T .

+v); see Eq.(7). Obviously  no matter how many data are accumu-
If vj = vk  then hj = hk = (cid:30)(zT
lated  two columns hj and hk of J in Eq.(4) are identical; therefore  J is rank deﬁcient; hence  JT J
is singular. The posed singularity gives rise to negative curvature because the above 4-by-4 dense
Hessian block is almost always indeﬁnite (so is r2E of size n (cid:2) n) to be proved next.
2.2 Case 1: vj = vk (cid:17) v; hence  hj = hk (cid:17) h = (cid:30)(zT
Given a set of m (training) data  the gradient vector rE and the Hessian matrix r2E in Eq.(4) are
evaluated. We then apply the aforementioned orthogonal matrix P to them as PTrE and PTr2EP 
yielding the gradient vector g of length 4 and the 4-by-4 Hessian block H [see Eq.(9)] associated
with the four weights [pj; pk; v0;j; v0;k]T ; they may be expressed in a compact form as

+v)  and pj 6= pk

(cid:30)0(zT

+d v)rd; d1(cid:17) pjD;

d2(cid:17) pkD:

d=1

g =

rdud =

2
64

mX

(cid:13)
(cid:13)
pje
pke

3
75; H = JT J+S =
where the entries are given below with B(cid:17)Pm
b2(cid:17) pkB;

a(cid:17)

2
64

mXd=1
mXd=1

h2
d; b1(cid:17) pjB;
mXd=1

e(cid:17)

rdhd;

(cid:13)(cid:17)

8>>><
>>>:

a
a
b1
b2

a
a
b1
b2

b1
b1
c11
c12

d=1(cid:30)0(zT

+dv)hd  C (cid:17)Pm
c11(cid:17) p2

j C;

3
75;

0
e
0
d2

0
0
e
0

0
0
0
e

e
0
d1
0

b2
b2
c12
c22

3
2
75+
64
+dv)(cid:17)2
d=1(cid:16)(cid:30)0(zT
  D(cid:17)Pm
c22(cid:17) p2
c12(cid:17) pjpkC;

kC;

d=1(cid:30)00(zT

+dv)rd:

(10)

(11)

Notice here that the subscript d implies datum d (d = 1; :::; m); hence  hd is the hidden-node output
on datum d (but not the dth hidden-node output) common to both nodes j and k due to vj = vk = v.
Theorem 1: When e6= 0  the n-by-n Hessian r2E and its block H in Eq.(10) are always indeﬁnite.
Proof: A similarity transformation with T  a 4-by-4 orthogonal matrix (TT = T(cid:0)1)  obtains

TT HT =2
64

b1 +b2 +e

2a

0

b1(cid:0)b2

b1 +b2 +e

(cid:11)
0
(cid:12)

0

2

0
0
0
e

b1(cid:0)b2

3
75 with T =
6664
2 (c11(cid:0)c22+d1(cid:0)d2)  and (cid:28) (cid:17) 1
2 (c11(cid:0)2c12+c22+d1+d2).

0 (cid:0)
1
p2
1
p2

1
p2
1
p2
0

1
p2
1
p2
0

(12)

1
p2
1
p2

3
;

(cid:12)
e
(cid:28)

7775

0 (cid:0)

0

0

0

where (cid:11)(cid:17) 1
The eigenvalues of the 2-by-2 block at the lower-right corner are obtainable by

2 (c11+2c12+c22+d1+d2)  (cid:12)(cid:17) 1

(cid:12)(cid:12)(cid:12)(cid:21)I (cid:0)h 0 e

e (cid:28) i(cid:12)(cid:12)(cid:12) = (cid:21)((cid:21) (cid:0) (cid:28) ) (cid:0) e2 = (cid:21)2 (cid:0) (cid:28) (cid:21) (cid:0) e2 = 0;

2 ((cid:28) (cid:6) p(cid:28) 2 + 4e2); the “sign-different” eigenvalues as long as e 6= 0 holds. Then  by
which yields 1
Cauchy’s interlace theorem (see Ch.10 of Parlett 1998)  the Hessian r2E is indeﬁnite. (So is H.) 2
2.3 Case 2: vj = vk (cid:17) v (hj = hk (cid:17) h)  and pj = pk (cid:17) p
The result in Case 1 becomes simpler: For a given set of m (training) data 

g =Pm

d=1 rdud =

2
64

(cid:13)
(cid:13)
pe
pe

3
75; H = JT J+S =2
64

a
a
b
b

a
a
b
b

b
b
c
c

b
b
c
c

3
75+2
64

0
0
e
0

0
0
0
e

e
0
d
0

0
e
0
d

3
75 ;

(13)

3

where the entries are readily identiﬁable from Eq.(11). In Eq.(13)  JT J is positive semi-deﬁnite (sin-
gular of rank 2 even when m (cid:21) 2)  and S has an indeﬁnite structure. When e6= 0 (hence  rE 6= 0) 
we can prove below that there always exists negative curvature (i.e.  r2E is always indeﬁnite).
Theorem 2: When e6= 0  the 4(cid:2)4 Hessian block H in Eq.(13) includes the sign-different eigenvalues
2 (d (cid:6) pd2 + 4e2); and the n (cid:2) n Hessian r2E as well as H are always indeﬁnite.
of S; namely  1
Proof: Proceed similarly with the same orthogonal matrix T as deﬁned in Eq.(12)  where b1 = b2 = b 
(cid:12) = 0  and (cid:28) = d  rendering TT HT “block-diagonal.” Its block of size 2(cid:2) 2 at the lower-right corner
has the sign-different eigenvalues determined by (cid:21)2(cid:0)d(cid:21)(cid:0)e2 = 0. 2 QED 2
Now  we investigate stationary points  where the n-length gradient vector rE = 0; hence  g = 0 in
Eq.(13). We thus consider two cases for pe = 0: (a) p = 0 and e 6= 0  and (b) p 6= 0 and e = 0. In
Case (b)  S becomes a diagonal matrix  and the above TT H T shows that H is of (at most) rank 3
(when d 6= 0); hence  H becomes singular.
Theorem 3: If rE((cid:18)(cid:3)) = 0  p = 0  and e6= 0 [i.e.  Case (a)]  then the stationary point (cid:18) (cid:3) is a saddle.
Theorem 4: If rE((cid:18)(cid:3)) = 0  and e = 0  but d < 0 [see Eq.(13)]  then (cid:18) (cid:3) is a saddle point.
Proof of Theorems 3 and 4: From Theorem 2 above  H in Eq.(13) has a negative eigenvalue; hence 
the entire Hessian matrix r2E of size n (cid:2) n is indeﬁnite 2 QED 2
Theorem 4 is a special case of Case (b).
If d = pD > 0  then H becomes positive semi-deﬁnite;
however  we could alter the eigen-spectrum of H by changing linear parameters p in conjunction
with scalar (cid:16) for pj = 2(cid:16)p and pk = 2(1(cid:0)(cid:16))p such that pj +pk = 2p with no change in E and rE = 0
held ﬁxed (to be conﬁrmed in simulation; see Fig.1 later)  leading to the following
Theorem 5: If D6= 0 and C > 0 [see the deﬁnition of C and D for Eq.(11)] and v1 = v2 ((cid:17) v) with
rE = 0  for which p6= 0 and e = 0 (hence  S is diagonal)  then choosing scalar (cid:16) appropriately for
pj = 2(cid:16)p and pk = 2(1(cid:0)(cid:16))p can render H and thus r2E indeﬁnite.
Proof: From Eq.(11)  two on-diagonal (3 3) and (4 4) entries of H are a quadratic function in terms
of (cid:16): The (3 3)-entry of H  H(3; 3) = 2(cid:16)p(2(cid:16)pC + D); has two roots: 0 and (cid:0) D
2pC   whereas the
2pC . Obviously  given p  C 
(4 4)-entry  H(4; 4) = 2(1(cid:0)(cid:16))p[2(1(cid:0)(cid:16))pC+D]; has two roots: 1 and 1 + D
and D  there exists (cid:16) such that the quadratic function value becomes negative (see later Fig.1). This
implies that adjusting (cid:16) can produce a negative diagonal entry of H; hence  indeﬁnite. Then  again
by Cauchy’s interlace theorem  so is r2E. 2 QED 2
Example 1: A two-exponential model in Eq.(5).

Data set 1:

Input x (cid:0)2 (cid:0)1
Target t
3

1

0
2

1
3

2
1

Data set 2:

Input x (cid:0)2 (cid:0)1
Target t
1

3

0
2

1
1

2
3

(14)

2P5

j=1(cid:8)f ((cid:18)0

(cid:3)

Given two sets of ﬁve data pairs (xi; ti) as shown above  for each data set  we ﬁrst ﬁnd
a minimizer (cid:18)0
(cid:3) = [p(cid:3); v(cid:3)]T of a two-weight 1-1-1 MLP  and then expand it with scalar (cid:16) as
(cid:18) = [(cid:16)p(cid:3); (1 (cid:0) (cid:16))p(cid:3); v(cid:3); v(cid:3)]T to construct a four-weight 1-2-1 MLP that produces the same input-to-
(cid:3) = [p(cid:3); v(cid:3)]T using a 1-1-1 MLP  f ((cid:18)0; x) = pevx 
output relations. That is  we ﬁrst ﬁnd the minimizer (cid:18) 0
= 2; and conﬁrm
by solving rE = 0  which yields p(cid:3) = 2; v(cid:3) = 0; E((cid:18)0
(cid:3)) = 1
that the 2 (cid:2) 2 Hessian r2E((cid:18)0
(cid:3)) is positive deﬁnite in both data sets above. Next  we augment (cid:18) 0
as (cid:18) = [p1; p2; v1; v2]T = [(cid:16)p(cid:3); (1 (cid:0) (cid:16))p(cid:3); v(cid:3); v(cid:3)]T to construct a 1-2-1 MLP: f ((cid:18); x) = p1ev1x +p2ev2x 
which realizes the same input-to-output relations as the 1-1-1 MLP. Fig.1 shows how (cid:16) changes the
eigen-spectrum (see solid curve) of the 4 (cid:2) 4 Hessian r2E (supported by Theorem 5).
Conjecture: Suppose that (cid:18) (cid:3) is a local minimum point in two-hidden-layer J-I-H-1 MLP-learning 
and r2E of size n (cid:2) n is positive deﬁnite (so is H) with rE = 0 and E > 0. Then  adding a node at
the second hidden layer can increase learning capacity in the sense that E can be further reduced.
Sketch of Proof: Choose a node j among H hidden nodes  and add a hidden node (call node k) by

(cid:3); xj) (cid:0) tj(cid:9)2

duplicating the hidden weights by vk = vj with pk = 0; hence  totally en(cid:17) n+(I +2) weights. This
certainly renders new JT J of size en (cid:2)en singular  and the (4 4)-entry in H in Eq.(10) becomes zero
(due to pk = 0). Then  by the interlace theorem  new r2E of size en (cid:2)en becomes indeﬁnite. 2
The above proof is not complete since we did not make clear assumptions about how the ﬁrst-order
necessary condition rE = 0 holds [see Cases (a) and (b) just above Theorem 3]. Furthermore  even
if we know in advance the minimum number of hidden nodes  Hmin  for a certain task  we may not
be able to ﬁnd a local-minimum point of an MLP with one less hidden nodes  Hmin(cid:0)1. Consider  for
instance  the well-known (four data) XOR problem. Although it can be solved by a 2-2-1 MLP (nine

4

20

15

10

5

0

−5

−10

−15

−20

−25
−2

min Eig((cid:209) 2E)
min Eig(S)
(cid:209) 2E(3 3)
(cid:209) 2E(4 4)

−1

0

1

2

3

20

15

10

5

0

−5

−10

−15

−20

−25
−2

min Eig((cid:209) 2E)
min Eig(S)
(cid:209) 2E(3 3)
(cid:209) 2E(4 4)

−1

0

1

2

3

Figure 1: The change of the minimum eigenvalue of r2E (solid curve) and of S (dashed) as well as
the (3 3)-entry of r2E (dotted) and the (4 4)-entry of r2E (dash-dot)  both quadratic  according to
value (cid:16) (x-axis) in (cid:18) = [(cid:16)p(cid:3); (1(cid:0)(cid:16))p(cid:3); v(cid:3); v(cid:3)]T   the four weights of a 1-2-1 MLP with exponential
hidden nodes (left) using data set 1  and (right) data set 2 in Eq.(14). Theorem 5 supports this result.

v
 

5

4

3

2

1

0

−1

−2

−3

0

2−D contour plot

Minimizer 

Saddle 

 p

(a)

v
 

20

15

10

5

0

−5

−10

−15

−20
−1

2−D contour plot

Minimizer 

Saddle 

Attractive point 

 p

(b)

4

3

2

1

)
v
 
p
(
E

0
2.5

Attractor 

Minimizer 

Saddle 

2

1.5

1
 p

0.5

0

−0.5

−20

−10

0

 v

10

−1

20

(c)

0.2

0.4

0.6

0.8

1

1.2

−0.5

0

x 
0.5

1

1.5

2

2.5

Figure 2: The 1-1-1 MLP landscape: (a) a magniﬁed view; (b) bird’s-eye views in 2-D  and (c) 3-D.

1+e(cid:0)

(cid:3))(cid:25) 0:4111. The Hessian r2E((cid:18)0

x   and found p(cid:3) (cid:25) 1:0185 and v(cid:3) (cid:25) 0:3571 with krE((cid:18)0

weights)  any local minimum point may not be found by optimizing a 2-1-1 MLP (ﬁve weights) 
since the hidden weights tend to be divergent (or weight-1 attractors). Here is another example:
Example 2: An N-shape curve ﬁtting to four data: Data(x; t) (cid:17) f((cid:0)3; 0); ((cid:0)1; 1); (1; 0); (3; 1)g.
We solved rE = 0 to ﬁnd all stationary points of a two-weight 1-1-1 MLP with a logistic hidden-
node function (cid:30)(x)(cid:17) 1
(cid:3))k = O(10(cid:0)15) 
roughly the order of machine (double) precision  and E((cid:18) 0
(cid:3)) was
positive deﬁnite (eigenvalues: 0.8254 and 1.4824). We also found a saddle point. There was another
type of attractive points  where (cid:30) is driven to saturation due to a large hidden weight v in magnitude
(weight-1 attractors). Fig.2 displays those three types of stationary points. Clearly  for a rigorous
proof of Conjecture  we need to characterize those different types  and clarify their underlying as-
sumptions; yet  it is quite an arduous task because the situation totally depends on data; see also our
Hessian argument for Blum’s line in Sec.3.2.
We continue with Example 2 to verify the above theorems. We set (cid:18) = [(cid:16)p(cid:3); (1(cid:0) (cid:16))p(cid:3); v(cid:3); v(cid:3)] in
a 1-2-1 MLP. When (cid:16) = 0:5  the Hessian r2E was positive semi-deﬁnite. If a small perturbation
is added to v(cid:3)  then r2E becomes indeﬁnite (see Theorem 2). In contrast  when (cid:16) = (cid:0)1:5  r2E
became indeﬁnite (minimum eigenvalue (cid:0)0:2307); this situation was similar to Fig.1(left).
Remarks: The eigen-spectrum (or curvature) variation along a line often arises in separable (i.e. 
mixed linear and nonlinear) optimization problems. As a small non-MLP model  consider  for in-
stance  a separable objective function with (cid:18)(cid:17) [p; v]T   two variables alone: F ((cid:18)) = F (p; v) = pv 2:
Expressed below are the gradient and Hessian of F :

rF =(cid:20) v2

2pv(cid:21) ; r2F =(cid:20) 0

2v

2p(cid:21) :

(15)
Consider a line v = 0  where the Hessian r2F is singular. Then  the eigen-spectrum of r2F changes
as the linear parameter p alters while the ﬁrst-order necessary condition (rF = 0) is maintained with
the objective-function value F = 0 held ﬁxed. Clearly  r2F is positive semi-deﬁnite when p > 0 
whereas it is negative semi-deﬁnite when p < 0. Hence  the line is a collection of degenerate sta-
tionary points. In this way  singularities may be closely related to ﬂat regions  where any updates of

2v

5

z
z
parameters do not change the objective function value. Back to MLP-learning  Blum [10] describes
a different linear manifold of stationary points (see Sec.3.2 for more details)  where the (cid:16)-adjusting
procedure described above fails because D = 0 (see Example 3 below also). Some other types of
linear manifolds (and eigen-spectrum changes) can be found; e.g.  in [11  4  3]; unlike their work 
our paper did not claim anything about local minima  and our approach is totally different.
Example 3: A linear-output ﬁve-weight 1-1-2-1 MLP with (cid:18) = [p1; p2jv1; v2jw1]T (no thresholds) 
having tanh-hidden-node functions. If (cid:18) (cid:3) = [1; 1; 0; 0; 0]T   then rE((cid:18)(cid:3)) = 0 with the indeﬁnite Hes-
sian r2E (hence  (cid:18)(cid:3) a saddle point) below  in which all diagonal entries of S are zero due to D = 0:

r2E((cid:18)(cid:3)) =

2
664

0 0 0 0 0
0 0 0 0 0
0 0 0 0 ?
0 0 0 0 ?
0 0 ? ? 0

3
775

with ? (cid:17)

mXd=1

xdrd:

Here  ? denotes a non-zero entry with input x and residual r over an arbitrary number m of data. 2
The point to note here is that it is important to look at the entire Hessian r2E of size n (cid:2) n. When
H = O  a 4 (cid:2) 4 block of zeros  r2E would be indeﬁnite (again by the interlace theorem) as long
as non-zero off-diagonal entries exist in r2E  as in Example 3 above. Needless to say  however 
the Hessian analysis fails in certain pathological cases (see Sec.3.2). Typical is an aforementioned
weight-1 case  where the sigmoid-shaped hidden-node functions are driven to saturation limits due
to very large hidden weights. Then  only part of JT J associated with linear weights p appear in r2E
since S = O even if residuals are still large. This case is outside the scope of our analysis. It should
be noted that a regularization scheme to penalize large weights is quite orthogonal to our scheme to
exploit negative curvature. If a regularization term (cid:22)(cid:18)T (cid:18) (with non-negative scalar (cid:22)) is added to E 
then the negative-curvature information will be lost due to r2E + (cid:22)I.

3 The 2-2-1 MLP-learning examples found in the literature

In this section  we consider learning with a 2-2-1 MLP having nine weights; then  Eq.(6) reduces to

(cid:18)T (cid:17) [pTjvT ] = [pTjvT

1 jvT

2 ] = [p0; p1; p2jv0;1; v1;1; v2;1jv0;2; v1;2; v2;2];

where vj is a (hidden) weight vector connecting to the jth hidden node. Here  all weights are non-
linear since both hidden and ﬁnal outputs are produced by sigmoidal logistic function (cid:30)(x)(cid:17) 1
x .
3.1 Insensitivity to the initial weights in the singular XOR problem

1+e(cid:0)

The world-renowned XOR problem (involving only four data of binary values: ON and off) with a
standard nine-weight 2-2-1 MLP is inevitably a singular problem because the Gauss-Newton Hes-
sian JT J in Eq.(4) is always singular (at most rank 4)  whereas S tends to be of (nearly) full rank;
so does r2E (cf. rank analysis in [12]). This implies that singularity in terms of JT J is everywhere
in the posed neuro-manifolds. It is well-known (e.g.  see [13]) that the origin (p = 0 and v = 0) is
a singular saddle point  where rE = 0 and r2E = JT J with only one positive eigenvalue and eight
zeros. An interesting observation is that there always exists a descending path to the solution from
any initial point (cid:18)init as long as (cid:18)init is randomly generated in a small range; i.e.  in the vicinity of
the origin. That is  ﬁrst go directly down towards the origin from (cid:18)init  and then move in a descent
direction of negative curvature so as to escape from that singular saddle point. In this way  the 2-2-1
MLP can develop insensitivity to initial weights  always solving the posed XOR problem.

3.2 Blum’s linear manifold of stationary points

L1(cid:17) v0;1 = v0;2; w1(cid:17) v1;1 = v2;2; w2(cid:17) v1;2 = v2;1; w(cid:17) p1 = p2;

In the XOR problem  Blum [10] found a line of stationary points by adding constraints to (cid:18) as
(with L(cid:17) p0);

(16)
leading to a weight-sharing MLP of ﬁve weights: (cid:18)(cid:17) [L; w; L1; w1; w2]T following the notations
in [10]. Using four XOR data: (x1; x2; t) = f(0; 0; off); (0; 1; ON); (1; 0; ON); (1; 1; off)g for E in
Eq.(3)  Blum considered a point with v = 0; hence  (cid:18) (cid:3)(cid:17) [L; w; 0; 0; 0]T   which gives two identical
hidden-node outputs: h1 = h2 = (cid:30)(0) = 1
2 . This is the same situation as in Sec.2.2 and 2.3. By
the constraints given in Eq.(16)  the terminal output is given by y = (cid:30)(L + w). All those node outputs
are independent of input data. Then  for a given target value “off” (e.g.  0.1)  set
ON = 2(cid:30)(L + w) (cid:0) off () (cid:30)(L + w) = (off + ON)=2

so that those target values “off” and “ON” must approximate XOR.

1+e0 = 1

(17)

6

2
66664

4A
4A 2wA
4A
4A 2wA
2wA 2wA w2A
wA wA w2
wA wA w2

wA
wA
w2
2 A

wA
wA
w2
2 A

Blum’s Claim (page 539 [10]): There are many stationary points that are not absolute minima.
They correspond to w and L satisfying Eq.(17). Hence  they lie on a line “L + w = c (constant)”
in the (w; L)-plane. Actually  these points are local minima of E  being 1
A little algebra conﬁrms that rE = 0  and the quantities corresponding to e and D in Eq.(11) are
all zeros; hence  S = O. Consequently  no matter how (cid:16) (see Theorem 5) is changed to update w
and L (along the line)  r2E stays positive semi-deﬁnite  and E in Eq.(3) remains the same 0.5 (ﬂat
region). This is certainly a limitation of the second-order Hessian analysis  and thus more efforts
using higher-order derivatives were needed to disprove Blum’s claim (see [14  15])  and it turned out
that Blum’s line is a collection of singular saddle points. In what follows  we show what conditions
must hold for the Hessian argument to work.
The 5-by-5 Hessian matrix r2E at a stationary point (cid:18) (cid:3) = [L; w; 0; 0; 0] is given by

2 (ON (cid:0) off)2. 2

3
77775

with8><
A(cid:17)f(cid:30)0(L + w)g2
S(cid:17) (cid:30)00(L+w)(cid:16)(cid:30)(L+w)(cid:0) off(cid:17):
>:

8 (3A + S):

(18)

(19)

5(cid:2)5

=

r2E
|{z}
We thus obtain two non-zero eigenvalues of r2E  (cid:21)1 and (cid:21)2  below using k(cid:17) w2

8 (3A+S) w2
8 (3A+S) w2

8 (3A+S)
8 (3A+S)

2 A w2
2 A w2

(cid:21)1; (cid:21)2 = 1

Now  the smaller eigenvalue can be rendered negative when the following condition holds:

2(cid:26)(cid:2)A(w2 + 8) + 2k(cid:3) (cid:6)q[A(w2 + 8) + 2k]2 (cid:0) 2A(w2 + 8)(4k (cid:0) w2A)(cid:27) :
4k (cid:0) w2A < 0 () A + S = f(cid:30)0(L + w)g2 + (cid:30)00(L + w)(cid:16)(cid:30)(L + w) (cid:0) off(cid:17) < 0:

(20)
Choosing L+w = 2 and off = 0:1 accomplishes our goal  yielding sign-different eigenvalues with
ON = 2(cid:30)(2)(cid:0)off(cid:25) 1:6616 by Eq.(17). Because r2E is indeﬁnite  the posed stationary point is a
saddle point with E = 1
2 (ON (cid:0) off)2 ((cid:25) 1:219)  as desired. In other words  the target value for ON
is modiﬁed to break symmetry in data. Such a large target value ON (as 1.6616) is certainly un-
attainable outside the range (0 1) of the sigmoidal logistic function (cid:30)(x)  but notice that ON is often
set equal to 1.0  which is also un-attainable for ﬁnite weight values. It appears that the choice of
such a (ﬁctitiously large) value ON does not violate any Blum’s assumption. When 0 (cid:20) off < ON (cid:20) 1
(with w 6= 0)  the Hessian r2E in Eq.(18) is always positive semi-deﬁnite of rank 2. Hence  it is a
singular saddle point.

3.3 Two-class pattern classiﬁcation problems of Gori and Tesi

We next consider two two-class pattern classiﬁcation problems made by Gori & Tesi: one with
ﬁve binary data (p.80 in [17])  and another with only three data (p.93 in [16]); see Fig.3. Both are
singular problems  because rank(JT J) (cid:20) 5; yet  both S and r2E tend to be of full rank; therefore 
the 9(cid:2)9 Hessian r2E tends to be indeﬁnite (see Theorems 1 and 2). On p.81 in [17]  a conﬁguration
of two separation lines  like two solid lines given by (cid:18)init in Fig.3(left) and (right)  is claimed as a
region attractive to a local-minimum point. Indeed  the batch-mode steepest-descent method fails to
change the orientation of those solid lines. But its failure does not imply that there is no descending
way out of the two-solid-line conﬁguration given by (cid:18)init because the convergence of the steepest-
descent method to a (local) minimizer can be guaranteed by examining negative curvature (e.g.  p.45
in [18]). We shall show a descending negative curvature direction.
In the ﬁve-data case  the steepest-descent method moves (cid:18)init to a point  where the weights become
relatively large; the gradient vector rE (cid:25) 0; the Hessian r2E is positive semi-deﬁnite; and Eq.(3)
with m = 5 is given by E = 1
3 (ON (cid:0) off)2  for which the two residuals at data points (0 0) and (1 1)
are made zeros. We can ﬁnd such a point analytically by a linear-equation solving: Given (cid:18) init in
Fig.3  the solution to the linear system below yields p(cid:3) = [p(cid:3)

2]T (three terminal weights):

0; p(cid:3)

2
4

1 (cid:30)((cid:0)1:5) (cid:30)((cid:0)0:5)
(cid:30)(1:5)
1 (cid:30)(0:5)
1 (cid:30)((cid:0)0:5) (cid:30)(0:5)
0; p(cid:3)

3
2
5
4

p(cid:3)
0
p(cid:3)
1
p(cid:3)
2

3
5 =2
4

1; p(cid:3)
(cid:30)(cid:0)1(off)
(cid:30)(cid:0)1(off)

(cid:30)(cid:0)1(cid:16) 2 ON+off

3

(cid:17)

3
5 :

2; (cid:0)1:5; 1; 1; (cid:0)0:5; 1; 1]T; where the norm of p(cid:3) becomes relatively
The resulting point (cid:18)(cid:3) (cid:17) [p(cid:3)
large O(102)  gives the zero gradient vector  the positive semi-deﬁnite Hessian of rank 5  and
3 (ON(cid:0)off)2  as mentioned above. It is observed  however  that small perturbations on (cid:18) (cid:3) render
E = 1

1; p(cid:3)

7

x 2

1.5

(0  1)

0.5

1.5

x1

0.5

(1  0)

−0.5

0

1

h1

1

0
1

−1.5

0
1

−0.5

1
x 1

−1

h2

1

1

1

2
x2

net = x + x − 1.5

1

2

net = − x + x − 0.5

2

1

x 2

1.5

(0  1)

0.5

(1  1)

1.5

x1

(0  0)

0.5

(1  0)

−0.5

net = − x + x + 0.5

2

1

net = x + x − 0.5

2

1

Figure 3: Gori & Tesi’s two-class pattern classiﬁcation problems (left) three-data case; (right) ﬁve-
data case; and (middle) a 2-2-1 MLP with initial weight values (cid:18)init(cid:17) [0; 1; (cid:0)1; (cid:0)1:5; 1; 1; (cid:0)0:5; 1; 1]T .
Its corresponding initial conﬁguration gives two solid lines of net-inputs (to two hidden nodes) in the
input space  where “(cid:14)” stands for two ON-data (1 0)  (0 1)  whereas “(cid:2)” for one off-data (0.5 0.5) in
left ﬁgure and three off-data (0 0)  (0.5 0.5)  (1 1) in right ﬁgure. A solution to both problems may
be given by the two dotted lines with (cid:18)sol(cid:17) [0; 1; (cid:0)1; (cid:0)0:5; (cid:0)1; 1; 0:5; (cid:0)1; 1]T .
r2E indeﬁnite of full rank (since S is dominant): rank(S) = rank(r2E) = 9 with rank(JT J) = 4; this
suggests a descend direction (other than the steepest descent) to follow from (cid:18)init to a solution (cid:18)sol.
Fig.3(right) presents one of them  an intuitive change of six hidden weights (with the other three
weights held ﬁxed) from two solid lines to two dotted ones  indicated by two thick arrows given by
(cid:1)(cid:18)(cid:17) (cid:18)sol (cid:0) (cid:18)init = [0; 0; 0; 1; (cid:0)2; 0; 1; (cid:0)2; 0]T; is a descent direction of negative curvature down to (cid:18)sol
because (cid:1)(cid:18)Tr2E((cid:18)init)(cid:1)(cid:18) < 0  where r2E((cid:18)init)  the Hessian evaluated at (cid:18)init  was indeﬁnite. In-
triguingly enough  it is easy to conﬁrm for the three-data case that the posed “descent” direction of
negative curvature (cid:1)(cid:18) is orthogonal to (cid:0)rE  the steepest-descent direction.
Claim: Line search from (cid:18)init to (cid:18)sol monotonically decreases the squared error E ((cid:18)init + (cid:17)(cid:1)(cid:18)) as
the step size (cid:17) (scalar) changes from 0 to 1; hence  no plateau.
Proof for the three-data case: (The ﬁve-data case can be proved in a similar fashion.) Using target
values ON=1 and off=0  let q((cid:17))(cid:17) E((cid:18)init +(cid:17)(cid:1)(cid:18))(cid:0)E((cid:18)init). Then  we show below that q 0((cid:17)) < 0 using
a property that (cid:30)((cid:0)x) = 1(cid:0)(cid:30)(x):
2f(cid:30)((cid:0)0:5+(cid:17))(cid:0)(cid:30)(0:5+(cid:17))(cid:0)ONg2(cid:0)f(cid:30)((cid:0)0:5)(cid:0)(cid:30)(0:5)(cid:0)ONg2
q((cid:17)) = 1
2f1(cid:0)(cid:30)(0:5+(cid:17))(cid:0)(cid:30)(0:5(cid:0)(cid:17))(cid:0)ONg2 + 1
2f1(cid:0)(cid:30)(0:5(cid:0)(cid:17))(cid:0)(cid:30)(0:5+(cid:17))(cid:0)ONg2(cid:0)f1(cid:0)(cid:30)(0:5)(cid:0)(cid:30)(0:5)(cid:0)ONg2
= 1
=f1(cid:0)ON(cid:0)(cid:30)(0:5 + (cid:17))(cid:0)(cid:30)(0:5 (cid:0) (cid:17))g2 (cid:0) f1 (cid:0) ON (cid:0) 2(cid:30)(0:5)g2
=f(cid:30)(0:5 + (cid:17)) + (cid:30)(0:5 (cid:0) (cid:17))g2 (cid:0) 4 f(cid:30)(0:5)g2 :
Differentiation leads to q 0((cid:17)) = 2f(cid:30)(0:5+(cid:17))+(cid:30)(0:5(cid:0)(cid:17))g f(cid:30)0(0:5+(cid:17))(cid:0)(cid:30)0(0:5(cid:0)(cid:17))g < 0 because
(cid:30)(0:5+(cid:17)) > 0  (cid:30)(0:5(cid:0)(cid:17)) > 0  and (cid:17) > 0  which guarantees (cid:30)0(0:5+(cid:17)) < (cid:30)0(0:5(cid:0)(cid:17)). 2
4 Summary

2f(cid:30)((cid:0)0:5(cid:0)(cid:17))(cid:0)(cid:30)(0:5(cid:0)(cid:17))(cid:0)ONg2 + 1

In a general setting  we have proved that negative curvature can arise in MLP-learning. To make
it analytically tractable  we intentionally used noise-free small data sets but on “noisy” data  the
conditions for Theorems 1 and 2 most likely hold in the vicinity of singularity regions; it then follows
that the Hessian r2E tends to be indeﬁnite (of nearly full rank). Our numerical results conﬁrm
that the negative-curvature information is of immense value for escaping from singularity plateaus
including some problems where no method was developed to alleviate plateaus. In simulation  we
employed the second-order stagewise backpropagation [12] (that can evaluate r2E and JT J at the
essentially same cost; see proof therein) to obtain r2E explicitly and its eigen-directions so as to
exploit negative curvature. This approach is suitable for up to medium-scale problems  for which
our analysis suggests using existing trust-region globalization strategies whose theory has thrived
on negative curvature including indeﬁnite dogleg [19]. For large-scale problems  one could resort
to matrix-free Krylov subspace methods: Among them  the truncated conjugate-gradient (Krylov-
dogleg) method tends to pick up an arbitrary negative curvature (hence  slowing down learning;
see [20] for numerical evidence); so  other trust-region Krylov subspace methods are of our great
interest such as a Lanczos type [21] and a parameterized eigenvalue approach [22].

Acknowledgments

The work is partially supported by the National Science Council  Taiwan (NSC-99-2221-E-011-097).

8

References

[1] Amari  S.-I.  Park H. & Fukumizu  K. Adaptive Method of Realizing Natural Gradient Learning for Mul-

tilayer Perceptrons. Neural Computation  12:1399-1409  2000.

[2] Amari  S.-I.  Park  H. & Ozeki  T. Singularities affect dynamics of learning in neuro-manifolds. Neural

Computation  18(5):1007-1065  2006.

[3] Wei  H.  Zhang  J.  Cousseau  F.  Ozeki  T.  & Amari  S.-I. Dynamics of Learning Near Singularities in

Layered Networks. Neural Computation  20(3):813-843  2008.

[4] Fukumizu  K. & Amari  S.-I. Local Minima and Plateaus in Hierarchical Structures of Multilayer Percep-

trons. Neural Networks  13(3):317–327  2000.

[5] Jennrich  R.I. & Sampson  P.F. Application of Stepwise Regression to Non-Linear Estimation. Technomet-

rics  10(1):63–72  1968.

[6] Cousseau  F.  Ozeki  T.  & Amari  S.-I. Dynamics of Learning in Multilayer Perceptrons near Singularities

IEEE Trans. on Neural Networks  19(8):1313-1328  2008.

[7] Powell  M.J.D. A hybrid method for nonlinear equations. In Numerical Methods for Nonlinear Algebraic

Equations  Ed. by P.Rabinowitz  Gordon & Breach  London  pp.87–114  1970.

[8] Dennis  J.E.  Jr. Nonlinear least squares and equations. In The state of the art in numerical analysis  Ed.

by D. Jacobs  Academic Press  London  pp.269–312  1977.

[9] Parlett  B.N. The Symmetric Eigenvalue Problem. SIAM  1998.
[10] Blum  E.K. Approximation of Boolean Functions by Sigmoidal Networks: Part I: XOR and other two-

variable functions. Neural Computation  1:532-540  1989.

[11] Sprinkhuizen-Kuyper  I.G. & Boers  E.J.W. A Local Minimum for the 2-3-1 XOR Network.

Transactions on Neural Networks  10(4):968–971  1999.

IEEE

[12] Mizutani  E. & Dreyfus  S.E. Second-order stagewise backpropagation for Hessian-matrix analyses and
investigation of negative curvature. Neural Networks  vol.21 (issues 2–3):193-203  2008. (See its Corri-
gendum in vol.21  issue 9  page 1418).

[13] Sprinkhuizen-Kuyper  I.G. & Boers  E.J.W. The error surface of the 2-2-1 XOR network: The ﬁnite

stationary points. Neural Networks  11:683–690  1998.

[14] Tsaih  R.-H. An Improved Back Propagation Neural Network Learning Algorithm. Ph.D thesis at
the Department of Industrial Engineering and Operations Research  University of California at Berkeley 
pp.67–70  1991.

[15] Sprinkhuizen-Kuyper  I.G. & Boers  E.J.W. A comment on a paper of Blum: Blum’s local minima are
saddle points. Tech. Rep. No. 94-34  Leiden University  Department of Computer Science  Leiden  The
Netherlands  1994.

[16] Gori  M. & Tesi  A. Some examples of local minima during learning with backpropagation. Third
Italian Workshop on Parallel Architectures and Neural Networks. (Ed. by E.R. Caianiello)  World Scientiﬁc
Publishing Co.  pp. 87–94  1990.

[17] Gori  M. & Tesi  A. On the Problem of Local Minima in Backpropagation.

Analysis and Machine Intelligence  14(1):76-86  1992.

IEEE Trans. on Pattern

[18] Nocedal  J & Wright  S.J. Numerical Optimization. Springer Verlag  1999.
[19] Byrd  R.H.  Schnabel  R.B. & Schultz  G.A. Approximate solution of the trust region problems by mini-

mization over two-dimensional subspaces. Mathematical Programming  40:247–263  1988.

[20] Mizutani  E. & Demmel  J.W. Iterative scaled trust-region learning in Krylov subspaces via Pearlmutter’s
In S. Thrun  L. Saul  and B. Sch ¨olkopf  editors  Advances in

implicit sparse Hessian-vector multiply.
Neural Information Processing Systems  MIT Press  16:209–216  2004.

[21] Gould  N.I.M.  Lucidi  S.  Roma  M. & Toint  Ph.L. Solving the trust-region subproblem using the

Lanczos method. SIAM Journal on Optimization  9(2):504–525  1999.

[22] Rojas  M.  Santos  S.A. & Sorensen  D.C. A New Matrix-Free Algorithm for the Large-Scale Trust-

Region Subproblem. SIAM Journal on Optimization  11(3):611–646  2000.

9

,Jisu KIM
Yen-Chi Chen
Sivaraman Balakrishnan
Alessandro Rinaldo
Larry Wasserman