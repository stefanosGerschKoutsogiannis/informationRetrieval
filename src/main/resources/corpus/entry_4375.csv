2017,Predicting User Activity Level In Point Processes With Mass Transport Equation,Point processes are powerful tools to model user activities and have a plethora of applications in social sciences. Predicting user activities based on point processes is a central problem. However  existing works are mostly problem specific  use heuristics  or simplify the stochastic nature of point processes. In this paper  we propose a framework that provides an unbiased estimator of the probability mass function of point processes. In particular  we design a key reformulation of the prediction problem  and further derive a differential-difference equation to compute a conditional probability mass function. Our framework is applicable to general point processes and prediction tasks  and achieves superb predictive and efficiency performance in diverse real-world applications compared to state-of-arts.,Predicting User Activity Level In Point Processes

With Mass Transport Equation

Yichen Wang⇧  Xiaojing Ye⇤  Hongyuan Zha⇧  Le Song⇧†

⇧College of Computing  Georgia Institute of Technology

⇤ School of Mathematics  Georgia State University

† Ant Financial

{yichen.wang}@gatech.edu  xye@gsu.edu

{zha lsong}@cc.gatech.edu

Abstract

Point processes are powerful tools to model user activities and have a plethora of
applications in social sciences. Predicting user activities based on point processes
is a central problem. However  existing works are mostly problem speciﬁc  use
heuristics  or simplify the stochastic nature of point processes. In this paper  we
propose a framework that provides an efﬁcient estimator of the probability mass
function of point processes. In particular  we design a key reformulation of the
prediction problem  and further derive a differential-difference equation to compute
a conditional probability mass function. Our framework is applicable to general
point processes and prediction tasks  and achieves superb predictive and efﬁciency
performance in diverse real-world applications compared to the state of the art.

1

Introduction

Online social platforms  such as Facebook and Twitter  enable users to post opinions  share infor-
mation  and inﬂuence peers. Recently  user-generated event data archived in ﬁne-grained temporal
resolutions are becoming increasingly available  which calls for expressive models and algorithms
to understand  predict and distill knowledge from complex dynamics of these data. Particularly 
temporal point processes are well-suited to model the event pattern of user behaviors and have been
successfully applied in modeling event sequence data [6  10  12  21  23  24  25  26  27  28  33].
A fundamental task in social networks is to predict user activity levels based on learned point process
models. Mathematically  the goal is to compute E[f (N (t))]  where N (·) is a given point process
that is learned from user behaviors  t is a ﬁxed future time  and f is an application-dependent
function. A framework for doing this is critically important. For example  for social networking
services  an accurate inference of the number of reshares of a post enables the network moderator
to detect trending posts and improve its content delivery networks [13  32]; an accurate estimate of
the change of network topology (the number of new followers of a user) facilitates the moderator to
identify inﬂuential users and suppress the spread of terrorist propaganda and cyber-attacks [12]; an
accurate inference of the activity level (number of posts in the network) allows us to gain fundamental
insight into the predictability of collective behaviors [22]. Moreover  for online merchants such as
Amazon  an accurate estimate of the number of future purchases of a product helps optimizing future
advertisement placements [10  25].
Despite the prevalence of prediction problems  an accurate prediction is very challenging for two
reasons. First  the function f is arbitrary. For instance  to evaluate the homogeneity of user activities 
we set f (x) = x log(x) to compute the Shannon entropy; to measure the distance between a predicted
activity level and a target x⇤  we set f (x) = (x  x⇤)2. However  most works [8  9  13  30  31  32]
are problem speciﬁc and only designed for the simple task with f (x) = x; hence these works are

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Simulate point process on 0 #
&'() ={#) #  #-}
#)
#  #-
#
0
&'(  ={#) #  #-}
#)
#
#  #-
0

time

(a) Samples of Hawkes process

/0&1()  
0∈[0 #]
/(0|&1( )

Construct conditional intensity

0

#)

#  #-

#

(b) Intensity functions

0=0

Compute conditional mass function on [0 #]
0=#
67)8 0
67 (8 0)

0

1

1

x

(c) Mass transport from 0 to #	

Average 

(d) Unbiased estimator	

Figure 1: An illustration of HYBRID using Hawkes process (Eq. 1). Our method ﬁrst generates
t} of events; then it constructs intensity functions; with these inputs  it computes
two samples {Hi
conditional probability mass functions ˜i(x  s) := P[N (s) = x|Hi
s] using a mass transport equation.
Panel (c) shows the transport of conditional mass at four different times (the initial probability mass
˜(x  0) is an indicator function I[x = 0]  as there is no event with probability one). Finally  the
average of conditional mass functions yields our estimator of the probability mass.

not generalizable. Second  point process models typically have intertwined stochasticity and can
co-evolve over time [12  25]  e.g.  in the inﬂuence propagation problem  the information diffusion over
networks can change the structure of networks  which adversely inﬂuences the diffusion process [12].
However  previous works often ignore parts of the stochasticity in the intensity function [29] or make
heuristic approximations [13  32]. Hence  there is an urgent need for a method that is applicable to an
arbitrary function f and keeps all the stochasticity in the process  which is largely nonexistent to date.
We propose HYBRID  a generic framework that provides an efﬁcient estimator of the probability mass
of point processes. Figure 1 illustrates our framework. We also make the following contributions:
• Unifying framework. Our framework is applicable to general point processes and does not
depend on speciﬁc parameterization of intensity functions. It incorporates all stochasticity in point
processes and is applicable to prediction tasks with an arbitrary function f.

• Technical challenges. We reformulate the prediction problem and design a random variable with
reduced variance. To derive an analytical form of this random variable  we also propose a mass
transport equation to compute the conditional probability mass of point processes. We further
transform this equation to an Ordinary Differential Equation and provide a scalable algorithm.

• Superior performance. Our framework signiﬁcantly reduces the sample size to estimate the
probability mass function of point processes in real-world applications. For example  to infer
the number of tweeting and retweeting events of users in the co-evolution model of information
diffusion and social link creation [12]  our method needs 103 samples and 14.4 minutes  while
Monte Carlo needs 106 samples and 27.8 hours to achieve the same relative error of 0.1.

2 Background and preliminaries

Point processes. A temporal point process [1] is a random process whose realization consists of a set
of discrete events {tk}  localized in time. It has been successfully applied to model user behaviors
in social networks [16  17  19  23  24  25  28  30]. It can be equivalently represented as a counting
process N (t)  which records the number of events on [0  t]. The counting process is a right continuous
step function  i.e.  if an event happens at t  N (t)  N (t) = 1.
Let Ht = {tk|tk < t} be the history of events happened up to time t. An important way to character-
ize point processes is via the conditional intensity function (t) := (t|Ht)  a stochastic model for
the time of the next event given the history. Formally  (t) is the conditional probability of observing
an event in [t  t + dt) given events on [0  t)  i.e.  P{event in [t  t + dt)|Ht} = E[dN (t)|Ht] :=
(t)dt  where dN (t) 2{ 0  1}.
The intensity function is designed to capture the phenomena of interest. Some useful forms include
(i) Poisson process: the intensity is a deterministic function  and (ii) Hawkes process [15]: it captures
the mutual excitation phenomena between events and its intensity is parameterized as

(t) = ⌘ + ↵Xtk2Ht

2

(t  tk) 

(1)

where ⌘ > 0 is the baseline intensity; the trigging kernel (t) = exp(t) models the decay of past
events’ inﬂuence over time; ↵ > 0 quantiﬁes the strength of inﬂuence from each past event. Here 
the occurrence of each historical event increases the intensity by a certain amount determined by (t)
and ↵  making (t) history-dependent and a stochastic process by itself.
Monte Carlo (MC). To compute the probability mass of a point process  MC simulates n realizations
t} using the thinning algorithm [20]. The number of events in sample i is deﬁned
of history {Hi
as N i(t) = |Hi
t|. Let (x  t) := P[N (t) = x]  where x 2 N  be the probability mass. Then its
estimator ˆmc
n (x  t) =
nPi f (N i(t)). The root mean square error (RMSE) is deﬁned as
nPi I[N i(t) = x] and ˆµmc
1

n (t) for µ(t) := E[f (N (t))] are deﬁned as ˆmc

n (x  t) and the estimator ˆµmc

n (t) = 1

"(ˆµmc

n (t)) =pE[ˆµmc

n (t)  µ(t)]2 =pVAR[f (N (t))]/n.

(2)

3 Solution overview

Given an arbitrary point process N (t) that is learned from data  existing prediction methods for
computing E[f (N (t))] have three major limitations:
• Generalizability. Most methods [8  9  13  30  31  32] only predict E[N (t)] and are not generaliz-
able to an arbitrary function f. Moreover  they typically rely on speciﬁc parameterizations of the
intensity functions  such as the reinforced Poisson process [13] and Hawkes process [5  32]; hence
they are not applicable to general point processes.

• Approximation and heuristics. These works also ignore parts of the stochasticity in the intensity
functions [29] or make heuristic approximations to the point process [13  32]. Hence the accuracy
is limited by the approximations and heuristic corrections.

• Large sample size. The MC method overcomes the above limitations since it has an unbiased
estimator of the probability mass. However  the high stochasticity in point processes leads to a
large value of VAR[f (N (t))]  which requires a large number of samples to achieve a small error.
To address these challenges  we propose a generic framework with a novel estimator of the probability
mass  which has a smaller sample size than MC. Our framework has the following key steps.
I. New random variable. We design a random variable g(Ht)  a conditional expectation given the
history. Its variance is guaranteed to be smaller than that of f (N (t)). For a ﬁxed number of samples 
the error of MC is decided by the variance of the random variable of interest  as shown in (2). Hence 
to achieve the same error  applying MC to estimate the new objective EHt
[g(Ht)] requires smaller
number of samples compared with the procedure that directly estimates E[f (N (t))].
II. Mass transport equation. To compute g(Ht)  we derive a differential-difference equation that
describes the evolutionary dynamics of the conditional probability mass P[N (t) = x|Ht]. We
further formulate this equation as an Ordinary Differential Equation  and provide a scalable algorithm.

4 Hybrid inference machine with probability mass transport

In this section  we present technical details of our framework. We ﬁrst design a new random variable
for prediction; then we propose a mass transport equation to compute this random variable analytically.
Finally  we combine the mass transport equation with the sampling scheme to compute the probability
mass function of general point processes and solve prediction tasks with an arbitrary function f.

4.1 New random variable with reduced variance
We reformulate the problem and design a new random variable g(Ht)  which has a smaller variance
than f (N (t)) and the same expectation. To do this  we express E[f (N (t))] as an iterated expectation
(3)
where EHt is w.r.t. the randomness of the history and EN (t)|Ht
is w.r.t. the randomness of the
point process given the history. We design the random variable as a conditional expectation given the
history: g(Ht) = EN (t)|Ht

E[f (N (t))] = EHthEN (t)|Ht⇥f (N (t))|Ht⇤i = EHthg(Ht)i 

[f (N (t))|Ht]. Theorem 1 shows that it has a smaller variance.

3

Theorem 1. For time t > 0 and an arbitrary function f  we have VAR[g(Ht)] < VAR[f (N (t))].
Theorem 1 extends the Rao-Blackwell (RB) theorem [3] to point processes. RB says that if ˆ✓ is an
estimator of a parameter ✓ and T is a sufﬁcient statistic for ✓; then VAR[E[ˆ✓|T ]] 6 VAR[ˆ✓]  i.e.  the
sufﬁcient statistic reduces uncertainty of ˆ✓. However  RB is not applicable to point processes since it
studies a different problem (improving the estimator of a distribution’s parameter)  while we focus on
the prediction problem for general point processes  which introduces two new technical challenges:
(i) Is there a deﬁnition in point processes whose role is similar to the sufﬁcient statistic in RB?
Our ﬁrst contribution shows that the history Ht contains all the necessary information in a point
process and reduces the uncertainty of N (t). Hence  g(Ht) is an improved variable for prediction.
Moreover  in contrast to the RB theorem  the inequality in Theorem 1 is strict because the counting
process N (t) is right-continuous in time t and not predictable [4] (a predictable process is measurable
w.r.t. Ht  such as the processes that are left-continuous). Appendix C contains details on the proof.
(ii) Is g(Ht) computable for general point processes and an arbitrary function f? An efﬁcient
computation will enable us to estimate EHt
[g(Ht)] using the sampling method. Speciﬁcally  let
t) be the estimator computed from n samples; then from the deﬁnition of RMSE
ˆµn(t) = 1
in (2)  this estimator has smaller error than MC: "(ˆµn(t)) <" (ˆµmc
However  the challenge in our new formulation is that it seems very hard to compute this conditional
expectation  as one typically needs another round of sampling  which is undesirable as it will increase
the variance of the estimator. To address this challenge  next we propose a mass transport equation.

nPi g(Hi

n (t)).

4.2 Transport equation for conditional probability mass function

We present a novel mass transport equation that computes the conditional probability mass ˜(x  t) :=
P[N (t) = x|Ht] of general point processes. With this deﬁnition  we derive an analytical expression
for the conditional expectation: g(Ht) =Px f (x) ˜(x  t). The transport equation is as follows.
Theorem 2 (Mass Transport Equation for Point Processes). Let (t) := (t|Ht) be the conditional
intensity function of the point process N (t) and ˜(x  t) := P[N (t) = x|Ht] be its conditional
probability mass function; then ˜(x  t) satisﬁes the following differential-difference equation:

˜t(x  t)
"

:=

@ ˜(x  t)

@t

rate of change in conditional mass

loss in mass  at rate (t)

=8><>:

(t) ˜(x  t)
(t) ˜(x  t)
}
|

{z

+ (t) ˜(x  1  t)
}
|

gain in mass  at rate (t)

{z

if x = 0
if x = 1  2  3 ···

(4)

Proof sketch. For the simplicity of notation  we set the right-hand-side of (4) to be F[ ˜]  where
F is a functional operator on ˜. We also deﬁne the inner product between functions u : N ! R
and v : N ! R as (u  v) :=Px u(x)v(x). The main idea in our proof is to show that the equality
(v  ˜t) = (v F[ ˜]) holds for any test function v; then ˜t = F[ ˜] follows from the fundamental
lemma of the calculus of variations [14]. Speciﬁcally  the proof contains two parts as follows.
We ﬁrst prove (v  ˜t) = (B[v]  ˜)  where B[v] is a functional operator deﬁned as B[v] = (v(x +
1)  v(x))(t). This equality can be proved by the property of point processes and the deﬁnition of
conditional mass. Second  we show (B[v]  ˜) = (v F[ ˜]) using a variable substitution technique.
Mathematically  this equality means B and F are adjoint operators on the function space. Combining
these two equalities yields the mass transport equation. Appendix A contains details on the proof.
Mass transport dynamics. This differential-difference equation describes the time evolution of the
conditional mass. Speciﬁcally  the differential term ˜t  i.e.  the instantaneous rate of change in the
probability mass  is equal to a ﬁrst order difference equation on the right-hand-side. This difference
equation is a summation of two terms: (i) the negative loss of its own probability mass ˜(x  t) at
rate (t)  and (ii) the positive gain of probability mass ˜(x  1  t) from last state x  1 at rate (t).
Moreover  since initially no event happens with probability one  we have ˜(x  0) = I[x = 0]. Solving
this transport equation on [0  t] essentially transports the initial mass to the mass at time t.

4

k=1  ⌧  set t = tK+1

Algorithm 1: CONDITIONAL MASS FUNCTION
Input: Ht = {tk}K
Output: Conditional probability mass function ˜(t)
for k = 0 ··· K do

Construct (s) and Q(s) on [tk  tk+1] ;
˜(tk+1) = ODE45[ ˜(tk)  Q(s)  ⌧ )] (RK Alg);

end
Set ˜(t) = ˜(tK+1)

Algorithm 2: HYBRID MASS TRANSPORT
Input: Sample size n  time t  ⌧
Output: ˆµn(t)  ˆn(x  t)

Generate n samples of point process:Hi
t n
for i = 1 ···   n do
˜i(x  t) = COND-MASS-FUNC(Hi
end
nPi
ˆn(x  t) = 1

˜i(x  t)  ˆµn(t) =Px f (x) ˆn(x  t)

t   ⌧ );

i=1;

4.3 Mass transport as a banded linear Ordinary Differential Equation (ODE)

To efﬁciently solve the mass transport equation  we reformulate it as a banded linear ODE. Speciﬁcally 
we set the upper bound for x to be M  and set ˜(t) to be a vector that includes the value of ˜(x  t) for
each integer x: ˜(t) = ( ˜(0  t)  ˜(1  t) ···   ˜(M  t))>. With this representation of the conditional
mass  the mass transport equation in (4) can be expressed as a simple banded linear ODE:
(5)
where ˜(t)0 = ( ˜t(0  t) ···   ˜t(M  t))>  and the matrix Q(t) is a sparse bi-diagonal matrix with
Qi i = (t) and Qi1 i = (t). The following equation visualizes the ODE in (5) when M = 2.

˜(t)0 = Q(t) ˜(t) 

˜t(0  t)
˜t(1  t)
˜t(2  t)

0@

1A = (t)

(t) (t) !0@

˜(0  t)
˜(1  t)
˜(2  t)

1A .

(t) (t)

(6)

$

$'
)(*)
$'
!"$'

This dynamic ODE is a compact representation of the transport equation in (4) and M decides
the dimension of the ODE in (5). In theory  M can be unbounded. However  the conditional
probability mass is tends to zero when M becomes large. Hence  in practice we choose a ﬁnite
support {0  1 ···   M} for the conditional probability mass function. To choose a proper M  we
generate samples from the point process. Suppose the largest number of events in the samples
is L  we set M = 2L such that it is reasonably large. Next  with the initial probability mass
˜(t0) = (1  0 ···   0)>  we present an efﬁcient algorithm to solve the ODE.
4.4 Scalable algorithm for solving the ODE

$(

1

)*
$&
!"0

$&

)(*)

$%
)* *∈[$& $%]	
$&
$%
$%
!"$&
!"$%

$

$'

!"$

Figure 2: Illustration of Algorithm 1 using Hawkes
process. The intensity is updated after each event
tk. Within [tk  tk+1]  we use (tk) and the inten-
sity (s) to solve the ODE and obtain (tk+1).

We present the algorithm that transports the ini-
tial mass ˜(t0) to ˜(t) by solving the ODE.
Since the intensity function is history-dependent
and has a discrete jump when an event happens
at time tk  the matrix Q(t) in the ODE is discon-
tinuous at tk. Hence we split [0  t] into intervals
[tk  tk+1]. On each interval  the intensity is con-
tinuous and we can use the classic numerical
Runge-Kutta (RK) method [7] to solve the ODE.
Figure 2 illustrates the overall algorithm.
Our algorithm works as follows. First  with the initial intensity on [0  t1] and ˜(t0) as input  the RK
method solves the ODE on [0  t1] and outputs ˜(t1). Since an event happens at t1  the intensity is
updated on [t1  t2]. Next  with the updated intensity and ˜(t1) as the initial value  the RK method
solves the ODE on [t1  t2] and outputs ˜(t2). This procedure repeats for each [tk  tk+1] until time t.
Now we present the RK method that solves the ODE on each interval [tk  tk+1]. RK divides this
interval into equally-spaced subintervals [⌧i ⌧ i+1]  for i = 0 ···   I and ⌧ = ⌧i+1  ⌧i. It then
conducts linear extrapolation on each subinterval. It starts from ⌧0 = tk and uses ˜(⌧0) and the
approximation of the gradient ˜(⌧0)0 to compute ˜(⌧1). Next  ˜(⌧1) is taken as the initial value and
the process is repeated until ⌧I = tk+1. Appendix D contains details of this method.
The RK method approximates the gradient ˜(t)0 with different levels of accuracy  called states s.
When s = 1  it is the Euler method  which uses the ﬁrst order approximation ˜(⌧i+1)  ˜(⌧i)/⌧.

5

We use the ODE45 solver in MATLAB and choose the stage s = 4 for RK. Moreover  the main
computation in the RK method comes from the matrix-vector product. Since the matrix Q(t) is
sparse and bi-diagonal with O(M ) non-zero elements  the cost for this operation is only O(M ).
4.5 Hybrid inference machine with mass transport equation
With the conditional probability mass  we are now ready to express g(Ht) in closed form and
estimate EHt

[g(Ht)] using the MC sampling method. We present our framework HYBRID:
t} from a point process N (t) with a stochastic intensity (t).

(i) Generate n samples {Hi
(ii) For each sample Hi
s)  for each s 2
(iii) We obtain the estimator of the probability mass function (x  t) and µ(t) by taking the

[0  t]; then we solve (5) to compute the conditional probability mass ˜i(x  t).

t  we compute the value of intensity function (s|Hi
nPn

ˆµn(t) =Px f (x) ˆn(x  t)

average: ˆn(x  t) = 1

˜i(x  t) 

i=1

[ ˜]). Then EHt

[g(Ht)] = (f  EHt

Algorithm 2 summarizes the above procedure. Next  we discuss two properties of HYBRID.
First  our framework efﬁciently uses all event information in each sample. In fact  each event tk
inﬂuences the transport rate of the conditional probability mass (Figure 2). This feature is in sharp
contrast to MC that only uses the information of the total number of events and neglects the differences
in event times. For instance  the two samples in Figure 1(a) both have three events and MC treats them
equally; hence its estimator is an indicator function ˆmc
n (x  t) = I[x = 3]. However  for HYBRID 
these samples have different event information and conditional probability mass functions  and our
estimator in Figure 1(d) is much more informative than an indicator function.
Moreover  our estimator for the probability mass is unbiased if we can solve the mass transport
equation in (4) exactly. To prove this property  we show that the following equality holds for an
[ ˆn] = 
arbitrary function f: (f  ) = E[f (N (t))] = EHt
follows from the fundamental lemma of the calculus of variations [14]. Appendix B contains detailed
derivations. In practice  we choose a reasonable ﬁnite support for the conditional probability mass in
order to solve the mass transport ODE in (5). Hence our estimator is nearly unbiased.
5 Applications and extensions to multi-dimensional point processes
In this section  we present two real world applications  where the point process models have inter-
twined stochasticity and co-evolving intensity functions.
Predicting the activeness and popularity of users in social networks. The co-evolution model [12]
uses a Hawkes process Nus(t) to model information diffusion (tweets/retweets)  and a survival process
Aus(t) to model the dynamics of network topology (link creation process). The intensity of Nus(t)
depends on the network topology Aus(t)  and the intensity of Aus(t) also depends on Nus(t); hence
these processes co-evolve over time. We focus on two tasks in this model: (i) inferring the activeness

of a user by E[Pu Nus(t)]  which is the number of tweets and retweets from user s; and (ii) inferring
the popularity of a user by E[Pu Aus(t)]  which is the number of new links created to the user.

Predicting the popularity of items in recommender systems. Recent works on recommendation
systems [10  25] use a point process Nui(t) to model user u’s sequential interaction with item i.
The intensity function ui(t) denotes user’s interest to the item. As users interact with items over
time  the user latent feature uu(t) and item latent feature iu(t) co-evolve over time  and are mutually
dependent [25]. The intensity is parameterized as ui(t) = ⌘ui +uu(t)>ii(t)  where ⌘ui is a baseline
term representing the long-term preference  and the tendency for u to interact with i depends on the
compatibility of their instantaneous latent features uu(t)>ii(t). With this model  we can infer an

To solve these prediction tasks  we extend the transport equation to the multivariate case. Speciﬁcally 

item’s popularity by evaluating E[Pu Nui(t)]  which is the number of events happened to item i.
we create a new stochastic process x(t) =Pu Nus(t) and compute its conditional mass function.
Theorem 3 (Mass Transport for Multidimensional Point Processes). Let Nus(t) be the point process
with intensity us(t)  x(t) = PU
u=1 Nus(t)  and ˜(x  t) = P[x(t) = x|Ht] be the conditional
probability mass of x(t); then ˜ satisﬁes: ˜t = Pu us(t) ˜(x  t) +Pu us(t) ˜(x  1  t).

To compute the conditional probability mass  we also solve the ODE in (5)  where the diagonal and
off-diagonal of Q(t) is now the negative and positive summation of intensities in all dimensions.

6

0.8

0.6

E
P
A
M

0.4

0.2

1

2
4
Test time (half day)

3

5

(a) MAPE vs. test time

HYBRID
MC-1e6
MC-1e3
SEISMIC
RPP
FPE

E
P
A
M

0.8

0.6

0.4

0.2

HYBRID
MC-1e6
MC-1e3
FPE

HYBRID
MC-1e6
MC-1e3
SEISMIC
RPP
FPE

E
P
A
M

0.8

0.6

0.4

0.2

E
P
A
M

0.8

0.6

0.4

0.2

HYBRID
MC-1e6
MC-1e3
FPE

0.6

0.65

0.7

0.75

0.8

Training data size in proportion
(b) MAPE vs. train size

1

2
4
Test time (half day)

3

5

(c) MAPE vs. test time

0.6

0.65

0.7

0.75

0.8

Training data size in proportion
(d) MAPE vs. train size

Figure 3: Prediction results for user activeness and user popularity. (a b) user activeness: predicting
the number of posts per user; (c d) user popularity: predicting the number of new links per user. Test
times are the relative times after the end of train time. The train data is ﬁxed with 70% of total data.

HYBRID
MC-1e6
MC-1e3
SEISMIC
RPP
FPE

0.8

0.6

E
P
A
M

0.4

0.2

E
P
A
M

0.8

0.6

0.4

0.2

HYBRID
MC-1e6
MC-1e3
SEISMIC
RPP
FPE

0.8

0.6

0.4

0.2

E
P
A
M

E
P
A
M

0.8

0.6

0.4

0.2

1

2

3

Test time (week)

4

5

(a) MAPE vs. test time

0.6

0.65

0.7

0.75

0.8

Training data size in proportion
(b) MAPE vs. train size

2

4

6

Test time (day)

8

10

(c) MAPE vs. test time

0.6

0.65

0.7

0.75

0.8

Training data size in proportion
(d) MAPE vs. train size

Figure 4: Prediction results for item popularity. (a b) predicting the number of watching events per
program on IPTV; (c d) predicting the number of discussions per group on Reddit.

6 Experiments

In this section  we evaluate the predictive performance of HYBRID in two real world applications in
Section 5 and a synthetic dataset. We use the following metrics:
(i) Mean Average Percentage Error (MAPE). Given a prediction time t  we compute the MAPE

|ˆµn(t)  µ(t)|/µ(t) between the estimated value and the ground truth.
(ii) Rank correlation. For all users/items  we obtain two lists of ranks according to the true and
estimated value of user activeness/user popularity/item popularity. The accuracy is evaluated by
the Kendall-⌧ rank correlation [18] between two lists.

6.1 Experiments on real world data

We show HYBRID has both accuracy and efﬁciency improvement in predicting the activeness and
popularity of users in social networks and predicting the popularity of items in recommender systems.
Competitors. We use 103 samples for HYBRID and compare it with the following the state of the art.
• SEISMIC [32]. It deﬁnes a self-exciting process with a post infectiousness factor. It uses the
• RPP [13]. It adds a reinforcement coefﬁcient to Poisson process that depicts the self-excitation
• FPE [29]. It uses a deterministic function to approximate the stochastic intensity function.
• MC-1E3. It is the MC sampling method with 103 samples (same as these for HYBRID)  and

phenomena. It sets dN (t) = (t)dt and solves a deterministic equation for prediction.

branching property of Hawkes process and heuristic corrections for prediction.

MC-1E6 uses 106 samples.

6.1.1 Predicting the activeness and popularity of users in social networks
We use a Twitter dataset [2] that contains 280 000 users with 550 000 tweet  retweet  and link creation
events during Sep. 21 - 30  2012. This data is previously used to validate the network co-evolution
model [12]. The parameters for tweeting/retweeting processes and link creation process are learned
using maximum likelihood estimation [12]. SEISMIC and RPP are not designed for the popularity
prediction task since they do not consider the evolution of network topology. We use p proportion of
total data as the training data to learn parameters of all methods  and the rest as test data. We make
predictions for each user and report the averaged results.

7

)
s
(
 
e
m
T

i

10
8
6
4
2
0
0.5

104
HYBRID
MC

104
HYBRID
MC

3

2

1

)
s
(
 
e
m
T

i

)
s
(
 
e
m
T

i

0.4

0.3
MAPE

0.2

0.1

0
0.5

0.4

0.3
MAPE

0.2

0.1

1000
800
600
400
200
0
0.5

HYBRID

600

HYBRID

)
s
(
 
e
m
T

i

400

200

0.4

0.3
MAPE

0.2

0.1

0
0.5

0.4

0.3
MAPE

0.2

0.1

(a) User activeness

(d) Item popularity  IPTV
Figure 5: Scalability analysis: computation time as a function of error. (a b) comparison between
HYBRID and MC in different problems; (c d) scalability plots for HYBRID.

(b) Item popularity  IPTV

(c) User activeness

(a) User activeness

(c) Item popularity  IPTV (d) Item popularity  Reddit
Figure 6: Rank correlation results in different problems. We vary the proportion p of training data
from 0.6 to 0.8  and the error bar represents the variance over different training sets.

(b) User popularity

Predictive performance. Figure 3(a) shows that MAPE increases as test time increases  since the
model’s stochasticity increases. HYBRID has the smallest error. Figure 3(b) shows that MAPE
decreases as training data increases since model parameters are more accurate. Moreover  HYBRID is
more accurate than SEISMIC and FPE with only 60% of training data  while these works need 80%.
Thus  we make accurate predictions by observing users in the early stage. This feature is important
for network moderators to identify malicious users and suppress the propagation undesired content.
Moreover  the consistent performance improvement shows two messages: (i) considering all the
randomness is important. HYBRID is 2⇥ more accurate than SEISMIC and FPE because HYBRID
naturally considers all the stochasticity  but SEISMIC  FPE  and RPP need heuristics or approximations
that discard parts of the stochasticity; (ii) sampling efﬁciently is important. To consider all the
stochasticity  we need to use the sampling scheme  and HYBRID has a much smaller sample size.
Speciﬁcally  HYBRID uses the same 103 samples  but has 4⇥ error reduction compared with MC-1E3.
MC-1E6 has a similar predictive performance as HYBRID  but needs 103⇥ more samples.
Scalability. How does the reduction in sample size improve the speed? Figure 5(a) shows that as the
error decreases from 0.5 to 0.1  MC has higher computation cost  since it needs much more samples
than HYBRID to achieve the same error. We include the plots of HYBRID in (c). In particular  to
achieve the error of 0.1  MC needs 106 samples in 27.8 hours  but HYBRID only needs 14.4 minutes
with 103 samples. We use the machine with 16 cores  2.4 GHz Intel Core i5 CPU and 64 GB memory.
Rank correlation. We rank all users according to the predicted level of activeness and level of
popularity separately. Figure 6(a b) show that HYBRID performs the best with the accuracy around
80%  and it consistently identiﬁes around 30% items more correctly than FPE on both tasks.

6.1.2 Predicting the popularity of items in recommender systems

In the recommendation system setting  we use two datasets from [25]. The IPTV dataset contains
7 100 users’ watching history of 436 TV programs in 11 months  with around 2M events. The Reddit
dataset contains online discussions of 1 000 users in 1 403 groups  with 10 000 discussion events.
The predictive and scalability performance are consistent with the application in social networks.
Figure 4 shows that HYBRID is 15% more accurate than FPE and 20% than SEISMIC. Figure 5 also
shows that HYBRID needs much smaller amount of computation time than MC-1E6. To achieve the
error of 0.1  it takes 9.8 minutes for HYBRID and 7.5 hours for MC-1E6. Figure 6(c d) show that
HYBRID achieves the rank correlation accuracy of 77%  with 20% improvement over FPE.

8

0.710.690.410.390.210.130.000.250.500.75MethodsRank correlationMethodsHYBRIDMC-1e6FPESEISMICRPPMC-1e30.720.690.440.110.000.250.500.75MethodsRank correlationMethodsHYBRIDMC-1e6FPEMC-1e30.780.760.510.410.210.150.000.250.500.75MethodsRank correlationMethodsHYBRIDMC-1e6FPESEISMICRPPMC-1e30.770.750.580.510.310.210.000.250.500.75MethodsRank correlationMethodsHYBRIDMC-1e6FPESEISMICRPPMC-1e310-1

10-2

10-3

E
P
A
M

HYBRID
MC

100

10-1

10-2

10-3

E
P
A
M

101

103

102
104
number of samples

105

101

Hybrid
MC

100

10-1

10-2

10-3

E
P
A
M

HYBRID
MC

101
100
10-1

10-2

E
P
A
M

HYBRID
MC

(a) f (x) = x

(b) f (x) = x log(x)

103

102
105
number of samples

104

106

101

103

104

102
105
number of samples
(c) f (x) = x2

106

101

103

104

102
105
number of samples
(d) f (x) = exp(x)

106

Figure 7: Error of E[f (N (t))] as a function of sample size (loglog scale). (a-d) different choices of f.

0.025

0.02

0.015

0.01

0.005

y
t
i
l
i

b
a
b
o
r
P

0

0

0.02

0.015

0.01

0.005

y
t
i
l
i

b
a
b
o
r
P

0

85
Counts

160

(a) HYBRID  ˆn(x  t)

160

0

80
Counts
(b) MC  ˆmc
n (x  t)

y
t
i
l
i

b
a
b
o
r
P

0.03
0.025
0.02
0.015
0.01
0.005
0

0

0.05

0.04

0.03

0.02

y
t
i
l
i

b
a
b
o
r
P

0.01

160

0

0

85
Counts

85
Counts

160

(c) HYBRID  1 sample

(d) HYBRID  1 sample

Figure 8: Comparison of estimators of probability mass functions in HYBRID and MC.
estimators with the same 1000 samples. (c d) estimator with one sample in HYBRID.

(a b)

6.2 Experiments on synthetic data
We compare HYBRID with MC in two aspects: (i) the signiﬁcance of the reduction in the error and
sample size  and (ii) estimators of the probability mass function. We study a Hawkes process and set
the parameters of its intensity function as ⌘ = 1.2  and ↵ = 0.5. We ﬁx the prediction time to be
t = 30. The ground truth is computed with 108 samples from MC simulations.
Error vs. number of samples. In four tasks with different f  Figure 7 shows that given the same
number of samples  HYBRID has a smaller error. Moreover  to achieve the same error  HYBRID needs
100⇥ less samples than MC. In particular  to achieve the error of 0.01  (a) shows HYBRID needs 103
and MC needs 105 samples; (b) shows HYBRID needs 104 and MC needs 106 samples.
Probability mass functions. We compare our estimator of the probability mass with MC. Fig-
ure 8(a b) show that our estimator is much smoother than MC  because our estimator is the average of
conditional probability mass functions  which are computed by solving the mass transport equation.
Moreover  our estimator centers around 85  which is the ground truth of E[N (t)]  while that of MC
centers around 80. Hence HYBRID is more accurate. We also plot two conditional mass functions in
(c d). The average of 1000 conditional mass functions yields (a). Thus  this averaging procedure in
HYBRID adjusts the shape of the estimated probability mass. On the contrary  given one sample  the
estimator in MC is just an indicator function and cannot capture the shape of the probability mass.

7 Conclusions

We have proposed HYBRID  a generic framework with a new formulation of the prediction problem
in point processes and a novel mass transport equation. This equation efﬁciently uses the event
information to update the transport rate and compute the conditional mass function. Moreover 
HYBRID is applicable to general point processes and prediction tasks with an arbitrary function f.
Hence it can take any point process models as input  and the predictive performance of our framework
can be further improved with the advancement of point process models. Experiments on real world
and synthetic data demonstrate that HYBRID outperforms the state of the art both in terms of accuracy
and efﬁciency. There are many interesting lines for future research. For example  HYBRID can be
generalized to marked point processes [4]  where a mark is observed along with the timing of each
event.

9

Acknowledgements. This project was supported in part by NSF IIS-1218749  NIH BIGDATA
1R01GM108341  NSF CAREER IIS-1350983  NSF IIS-1639792 EAGER  NSF CNS-1704701  ONR
N00014-15-1-2340  DMS-1620342  CMMI-1745382  IIS-1639792  IIS-1717916  NVIDIA  Intel
ISTC and Amazon AWS.

References
[1] O. Aalen  O. Borgan  and H. Gjessing. Survival and event history analysis: a process point of

view. Springer  2008.

[2] D. Antoniades and C. Dovrolis. Co-evolutionary dynamics in social networks: A case study of

twitter. arXiv preprint arXiv:1309.6001  2013.

[3] D. Blackwell. Conditional expectation and unbiased sequential estimation. The Annals of

Mathematical Statistics  pages 105–110  1947.

[4] P. Brémaud. Point processes and queues. 1981.

[5] J. Da Fonseca and R. Zaatour. Hawkes process: Fast calibration  application to trade clustering 

and diffusive limit. Journal of Futures Markets  34(6):548–579  2014.

[6] H. Dai  Y. Wang  R. Trivedi  and L. Song. Deep coevolutionary network: Embedding user and

item features for recommendation. arXiv preprint arXiv:1609.03675  2016.

[7] J. R. Dormand and P. J. Prince. A family of embedded runge-kutta formulae. Journal of

computational and applied mathematics  6(1):19–26  1980.

[8] N. Du  L. Song  M. Gomez-Rodriguez  and H. Zha. Scalable inﬂuence estimation in continuous-

time diffusion networks. In NIPS  2013.

[9] N. Du  L. Song  A. J. Smola  and M. Yuan. Learning networks of heterogeneous inﬂuence. In

NIPS  2012.

[10] N. Du  Y. Wang  N. He  and L. Song. Time sensitive recommendation from recurrent user

activities. In NIPS  pages 3492–3500  2015.

[11] R. M. Dudley. Real analysis and probability. Cambridge University Press  Cambridge  UK 

2002.

[12] M. Farajtabar  Y. Wang  M. Gomez-Rodriguez  S. Li  H. Zha  and L. Song. Coevolve: A
joint point process model for information diffusion and network co-evolution. In NIPS  pages
1954–1962  2015.

[13] S. Gao  J. Ma  and Z. Chen. Modeling and predicting retweeting dynamics on microblogging

platforms. In WSDM  2015.

[14] I. M. Gelfand  R. A. Silverman  et al. Calculus of variations. Courier Corporation  2000.

[15] A. G. Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika 

58(1):83–90  1971.

[16] N. He  Z. Harchaoui  Y. Wang  and L. Song. Fast and simple optimization for poisson likelihood

models. arXiv preprint arXiv:1608.01264  2016.

[17] X. He  T. Rekatsinas  J. Foulds  L. Getoor  and Y. Liu. Hawkestopic: A joint model for network

inference and topic modeling from text-based cascades. In ICML  pages 871–880  2015.

[18] M. G. Kendall. A new measure of rank correlation. Biometrika  30(1/2):81–93  1938.

[19] W. Lian  R. Henao  V. Rao  J. E. Lucas  and L. Carin. A multitask point process predictive

model. In ICML  pages 2030–2038  2015.

[20] Y. Ogata. On lewis’ simulation method for point processes. IEEE Transactions on Information

Theory  27(1):23–31  1981.

10

[21] J. Pan  V. Rao  P. Agarwal  and A. Gelfand. Markov-modulated marked poisson processes for

check-in data. In ICML  pages 2244–2253  2016.

[22] R. Pastor-Satorras  C. Castellano  P. Van Mieghem  and A. Vespignani. Epidemic processes in

complex networks. Reviews of modern physics  87(3):925  2015.

[23] X. Tan  S. A. Naqvi  A. Y. Qi  K. A. Heller  and V. Rao. Content-based modeling of reciprocal

relationships using hawkes and gaussian processes. In UAI  pages 726–734  2016.

[24] R. Trivedi  H. Dai  Y. Wang  and L. Song. Know-evolve: Deep temporal reasoning for dynamic

knowledge graphs. In ICML  2017.

[25] Y. Wang  N. Du  R. Trivedi  and L. Song. Coevolutionary latent feature processes for continuous-

time user-item interactions. In NIPS  pages 4547–4555  2016.

[26] Y. Wang  E. Theodorou  A. Verma  and L. Song. A stochastic differential equation framework

for guiding online user activities in closed loop. arXiv preprint arXiv:1603.09021  2016.

[27] Y. Wang  G. Williams  E. Theodorou  and L. Song. Variational policy for guiding point processes.

In ICML  2017.

[28] Y. Wang  B. Xie  N. Du  and L. Song. Isotonic hawkes processes. In ICML  pages 2226–2234 

2016.

[29] Y. Wang  X. Ye  H. Zha  and L. Song. Predicting user activity level in point processes with

mass transport equation. In NIPS  2017.

[30] S.-H. Yang and H. Zha. Mixture of mutually exciting processes for viral diffusion. In ICML 

pages 1–9  2013.

[31] L. Yu  P. Cui  F. Wang  C. Song  and S. Yang. From micro to macro: Uncovering and predicting

information cascading process with behavioral dynamics. In ICDM  2015.

[32] Q. Zhao  M. A. Erdogdu  H. Y. He  A. Rajaraman  and J. Leskovec. Seismic: A self-exciting

point process model for predicting tweet popularity. In KDD  2015.

[33] K. Zhou  H. Zha  and L. Song. Learning social infectivity in sparse low-rank networks using

multi-dimensional hawkes processes. In AISTAT  volume 31  pages 641–649  2013.

11

,Yichen Wang
Xiaojing Ye
Hongyuan Zha
Le Song
Kamil Ciosek
Quan Vuong
Robert Loftin
Katja Hofmann