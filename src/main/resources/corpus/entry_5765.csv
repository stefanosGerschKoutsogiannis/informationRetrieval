2019,Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis,Linear dimensionality reduction methods are commonly used to extract low-dimensional structure from high-dimensional data. However  popular methods disregard temporal structure  rendering them prone to extracting noise rather than meaningful dynamics when applied to time series data. At the same time  many successful unsupervised learning methods for temporal  sequential and spatial data extract features which are predictive of their surrounding context. Combining these approaches  we introduce Dynamical Components Analysis (DCA)  a linear dimensionality reduction method which discovers a subspace of high-dimensional time series data with maximal predictive information  defined as the mutual information between the past and future. We test DCA on synthetic examples and demonstrate its superior ability to extract dynamical structure compared to commonly used linear methods. We also apply DCA to several real-world datasets  showing that the dimensions extracted by DCA are more useful than those extracted by other methods for predicting future states and decoding auxiliary variables. Overall  DCA robustly extracts dynamical structure in noisy  high-dimensional data while retaining the computational efficiency and geometric interpretability of linear dimensionality reduction methods.,Unsupervised Discovery of Temporal Structure in
Noisy Data with Dynamical Components Analysis

David G. Clark∗ 1 2

Jesse A. Livezey∗ 2 3 Kristofer E. Bouchard2 3 4

dgc2138@cumc.columbia.edu

kebouchard@lbl.gov

jlivezey@lbl.gov

∗Equal contribution.

1Center for Theoretical Neuroscience  Columbia University

2Biological Systems and Engineering Division  Lawrence Berkeley National Laboratory

3Redwood Center for Theoretical Neuroscience  University of California  Berkeley

4Helen Wills Neuroscience Institute  University of California  Berkeley

Abstract

Linear dimensionality reduction methods are commonly used to extract low-
dimensional structure from high-dimensional data. However  popular methods
disregard temporal structure  rendering them prone to extracting noise rather than
meaningful dynamics when applied to time series data. At the same time  many
successful unsupervised learning methods for temporal  sequential and spatial data
extract features which are predictive of their surrounding context. Combining these
approaches  we introduce Dynamical Components Analysis (DCA)  a linear dimen-
sionality reduction method which discovers a subspace of high-dimensional time
series data with maximal predictive information  deﬁned as the mutual information
between the past and future. We test DCA on synthetic examples and demon-
strate its superior ability to extract dynamical structure compared to commonly
used linear methods. We also apply DCA to several real-world datasets  showing
that the dimensions extracted by DCA are more useful than those extracted by
other methods for predicting future states and decoding auxiliary variables. Over-
all  DCA robustly extracts dynamical structure in noisy  high-dimensional data
while retaining the computational efﬁciency and geometric interpretability of linear
dimensionality reduction methods.

1

Introduction

Extracting meaningful structure from noisy  high-dimensional data in an unsupervised manner
is a fundamental problem in many domains including neuroscience  physics  econometrics and
climatology. In the case of time series data  e.g.  the spiking activity of a network of neurons or
the time-varying prices of many stocks  one often wishes to extract features which capture the
dynamics underlying the system which generated the data. Such dynamics are often expected to
be low-dimensional  reﬂecting the fact that the system has fewer effective degrees of freedom than
observed variables. For instance  in neuroscience  recordings of 100s of neurons during simple stimuli
or behaviors generally contain only ∼10 relevant dimensions [1]. In such cases  dimensionality
reduction methods may be used to uncover the low-dimensional dynamical structure.
Linear dimensionality reduction methods are popular since they are computationally efﬁcient  often
reducing to generalized eigenvalue or simple optimization problems  and geometrically interpretable 
since the high- and low-dimensional variables are related by a simple change of basis [2]. Analyzing
the new basis can provide insight into the relationship between the high- and low-dimensional

DCA code is available at: https://github.com/BouchardLab/DynamicalComponentsAnalysis

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

variables [3]. However  many popular linear methods including Principal Components Analysis 
Factor Analysis and Independent Components Analysis disregard temporal structure  treating data at
different time steps as independent samples from a static distribution. Thus  these methods do not
recover dynamical structure unless it happens to be associated with the static structure targeted by the
chosen method.
On the other hand  several sophisticated unsupervised learning methods for temporal  sequential and
spatial data have recently been proposed  many of them rooted in prediction. These prediction-based
methods extract features which are predictive of the future (or surrounding sequential or spatial
context) [4–9]. Predictive features form useful representations since they are generally linked to the
dynamics  computation or other latent structure of the system which generated the data. Predictive
features are also of interest to organisms  which must make internal estimates of the future of the
world in order to guide behavior and compensate for latencies in sensory processing [10]. These
ideas have been formalized mathematically [11  12] and tested experimentally [13].
We introduce Dynamical Components Analysis (DCA)  a novel method which combines the com-
putational efﬁciency and ease of interpretation of linear dimensionality reduction methods with
the temporal structure-discovery power of prediction-based methods. Speciﬁcally  DCA discov-
ers a subspace of high-dimensional time series data with maximal predictive information  deﬁned
as the mutual information between the past and future [12]. To make the predictive information
differentiable and accurately estimable  we employ a Gaussian approximation of the data  however
we show that maximizing this approximation can yield near-optima of the full information-theoretic
objective. We compare and contrast DCA with several existing methods  including Principal Compo-
nents Analysis and Slow Feature Analysis  and demonstrate the superior ability of DCA to extract
dynamical structure in synthetic data. We apply DCA to several real-world datasets including neural
population activity  multi-city weather data and human kinematics. In all cases  we show that DCA
outperforms commonly used linear dimensionality reduction methods at predicting future states and
decoding auxiliary variables. Altogether  our results establish that DCA is an efﬁcient and robust
linear method for extracting dynamical structure embedded in noisy  high-dimensional time series.

2 Dynamical Components Analysis

2.1 Motivation

Dimensionality reduction methods that do not take time into account will miss dynamical structure
that is not associated with the static structure targeted by the chosen method. We demonstrate this
concretely in the context of Principal Components Analysis (PCA)  whose static structure of interest is
variance [14  15]. Variance arises in time series due to both dynamics and noise  and the dimensions
of greatest variance  found by PCA  contain contributions from both sources in general. Thus  PCA
is prone to extracting spatially structured noise rather than dynamics if the noise variance dominates 
or is comparable to  the dynamics variance (Fig. 1A). We note that for applications in which generic
shared variability due to both dynamics and spatially structured noise is of interest  static methods are
well-suited.
To further illustrate this failure mode of PCA  suppose we embed a low-dimensional dynamical
system  e.g.  a Lorenz attractor  in a higher-dimensional space via a random embedding (Fig. 1B C).
We then add spatially anisotropic Gaussian white noise (Fig. 1D). We deﬁne a signal-to-noise ratio
(SNR) given by the ratio of the variances of the ﬁrst principal components of the dynamics and
noise. When the SNR is small  the noise variance dominates the dynamics variance and PCA
primarily extracts noise  missing the dynamics. Only when the SNR becomes large does PCA extract
dynamical structure (Fig. 1F G  black). Rather than maximizing variance  DCA ﬁnds a projection
which maximizes the mutual information between past and future windows of length T (Fig. 1E).
As we will show  this mutual information is maximized precisely when the projected time series
contains as much dynamical structure  and as little noise  as possible. As a result  DCA extracts
dynamical structure even for small SNR values  and consistently outperforms PCA in terms of
dynamics reconstruction performance as the SNR grows (Fig 1F G  red).

2

Figure 1: DCA ﬁnds dynamics rather than variance. (A) Schematic of unit vectors found by PCA
and DCA for three relative levels of dynamics and noise. The dimension of greatest variance  found
by PCA  contains contributions from both sources while the dimension found by DCA is orthogonal
to the noise. (B) Lorenz attractor in the chaotic regime. (C) Random orthogonal embedding of the
Lorenz attractor into 30-dimensional space. (D) Embedded Lorenz attractor with spatially-structured
white noise. (E) Random three-dimensional projection (top) and DCA projection (bottom) of the
embedded Lorenz attractor. (F) Reconstructions of the Lorenz attractor given the three-dimensional
projections found by DCA and PCA. (G) Lorenz reconstruction performance (R2) as a function of
the SNR for both methods. See Appendix B for details of the noisy Lorenz embedding.

2.2 Predictive information as an objective function

The goal of DCA is to extract a subspace with maximal dynamical structure. One fundamental
characteristic of dynamics is predictability: in a system with dynamics  future uncertainty is reduced
by knowledge of the past. This reduction in future uncertainty may be quantiﬁed using information
theory. In particular  if we equate uncertainty with entropy  this reduction in future uncertainty is the
mutual information between the past and future. This quantity was termed predictive information
by Bialek et al. [12]. Formally  consider a discrete time series X = {xt}  xt ∈ Rn  with a stationary
(time translation-invariant) probability distribution P (X). Let Xpast and Xfuture denote consecu-
tive length-T windows of X  i.e.  Xpast = (x−T +1  . . .   x0) and Xfuture = (x1  . . .   xT ). Then  the
predictive information Ipred
T (X) is deﬁned as
T (X) = H (Xfuture) − H (Xfuture|Xpast)
Ipred

= H (Xpast) + H (Xfuture) − H (Xpast  Xfuture)
= 2HX (T ) − HX (2T )

(1)

where HX (T ) is the entropy of any length-T window of X  which is well-deﬁned by virtue of
the stationarity of X. Unlike entropy and related measures such as Kolmogorov complexity [16] 
predictive information is minimized  not maximized  by serially independent time series (white noise).
This is because predictive information captures the sub-extensive component of the entropy of X.
Speciﬁcally  if the data points that comprise X are mutually independent  then HX (αT ) = αHX (T )
for all α and T   meaning that the entropy is perfectly extensive. On the other hand  if X has temporal
structure  then HX (αT ) < αHX (T ) and the entropy has a sub-extensive component given by
αHX (T ) − HX (αT ) > 0. Upon setting α = 2  this sub-extensive component is the predictive
information.
Beyond simply being able to detect the presence of temporal structure in time series  predic-
tive information discriminates between different types of structure. For example  consider two
discrete-time Gaussian processes with autocovariance functions f1(∆t) = exp (−|∆t/τ|) and
as T → ∞ to c1 log τ

f2(∆t) = exp(cid:0)−∆t2/τ 2(cid:1). For τ (cid:29) 1  the predictive information in these time series saturates

2 and c2τ 4  respectively  where c1 and c2 are constants of order unity (see Ap-

3

PC1DC1PC1DC1xyz1234530···1234530···DCAXpastXfutureTTrandomXpastXfutureSNR = 101SNR = 100SNR = 101ABCDEFGDCAPCAdynamicsnoisePC1DC1101100101SNR0.00.51.0R2pendix D for derivation). The disparity in the predictive information of these time series corresponds
to differences in their underlying dynamics. In particular  f1(∆t) describes Markovian dynamics 
leading to small predictive information  whereas f2(∆t) describes longer-timescale dependencies 
leading to large predictive information. Finally  as discussed by Bialek et al. [12]  the predictive
information of many time series diverges with T . In these cases  different scaling behaviors of the
predictive information correspond to different classes of time series. For one-dimensional time series 
it was demonstrated that the divergent predictive information provides a unique complexity measure
given simple requirements [12].

T (Y ). In certain cases of theoretical interest  P (X) is known and Ipred

2.3 The DCA method
DCA takes as input samples xt ∈ Rn of a discrete time series X  as well as a target dimensionality
d ≤ n  and outputs a projection matrix V ∈ Rn×d such that the projected data yt = V T xt maximize
an empirical estimate of Ipred
T (Y )
may be computed exactly for a given projection V . Systems for which this is possible include linear
dynamical systems with Gaussian noise and Gaussian processes more broadly. In practice  however 
we must estimate of Ipred
T (Y ) from ﬁnitely many samples. Directly estimating mutual information
from multidimensional data with continuous support is possible  and popular nonparametric methods
include those based on binning [17  18]  kernel density estimation [19] and k-nearest neighbor
(kNN) statistics [20]. However  many of these nonparametric methods are not differentiable (e.g. 
kNN-based methods involve counting data points)  complicating optimization. Moreover  these
methods are typically sensitive to the choice of hyperparameters [21] and suffer from the curse of
dimensionality  requiring prohibitively many samples for accurate results [22].
To circumvent these challenges  we assume that X is a stationary (discrete-time) Gaussian process.
It then follows that Y is stationary and Gaussian since Y is a linear projection of X. Under this
assumption  Ipred
T (Y ) may be computed from the second-order statistics of Y   which may in turn
be computed from the second-order statistics of X given V . Crucially  this estimate of Ipred
T (Y ) is
differentiable in V . Toward expressing Ipred
T (Y ) in terms of V   we deﬁne ΣT (X)  the spatiotemporal
covariance matrix of X which encodes all second-order statistics of X across T time steps. Assuming
that (cid:104)xt(cid:105)t = 0  we have

 C0

C1
C T
C0
1
...
...
C T
T−1 C T
T−2

. . . CT−1
. . . CT−2
...
. . .

...
C0

 where C∆t =(cid:10)xtxT

(cid:11)

t+∆t

t .

(2)

ΣT (X) =

Then  the spatiotemporal covariance matrix of Y   ΣT (Y )  is given by sending C∆t → V T C∆tV in
ΣT (X). Finally  Ipred

T (Y ) is given by
T (Y ) = 2HY (T ) − HY (2T ) = log |ΣT (Y )| − 1
Ipred
2

log |Σ2T (Y )|.

(3)

To run DCA on data  we ﬁrst compute the 2T cross-covariance matrices C0  . . .   C2T−1  then
maximize the expression for Ipred
T (Y ) of Eq. 3 with respect to V (see Appendix A for implementation
details). Note that Ipred
T (Y ) is invariant under invertible linear transformations of the columns of V .
Thus  DCA ﬁnds a subspace as opposed to an ordered sequence of one-dimensional projections.
Of course  real data violate the assumptions of both stationarity and Gaussianity. Note that stationarity
is a fundamental conceptual assumption of our method in the sense that predictive information is
deﬁned only for stationary processes  for which the entropy as a function of window length is
well-deﬁned. Nonetheless  extensions of DCA which take nonstationarity into account are possible
(see Discussion). On the other hand  the Gaussian assumption makes optimization tractable  but is
not required in theory. Note  however  that the Gaussian assumption is acceptable so long as the
optima of the Gaussian objective are also near-optima of the full information-theoretic objective.
This is a much weaker condition than agreement between the Gaussian and full objectives over
all possible V . To probe whether the weak condition might hold in practice  we compared the
Gaussian estimate of predictive information to a direct estimate obtained using the nonparametric
kNN estimator of Kraskov et al. [20] for projections of non-Gaussian synthetic data. We refer to

4

these two estimates of predictive information as the “Gaussian” and “full” estimates  respectively.
For random one-dimensional projections of the three-dimensional Lorenz attractor  the Gaussian and
full predictive information estimates are positively correlated  but show a complex  non-monotonic
relationship (Fig. 2A B). However  for one-dimensional projections of the 30-dimensional noisy
Lorenz embedding of Fig. 1  we observe tight agreement between the two estimates for random
projections (Fig. 2C  gray histogram). Running DCA  which by deﬁnition increases the Gaussian
estimate of predictive information  also increases the full estimate (Fig. 2C  red trajectories). When we
consider three-dimensional projections of the same system  random projections no longer efﬁciently
sample the full range of predictive information  but running DCA nevertheless increases both the
Gaussian and full estimates (Fig. 2D  trajectories). These results suggest that DCA ﬁnds good optima
of the full  information-theoretic loss surface in this synthetic system despite only taking second-order
statistics into account.
For a one-dimensional Gaussian time series Y   it is also possible to compute the predictive informa-
tion using the Fourier transform of Y [23]. In particular  when the asymptotic predictive information
k where {bk} are the so-called cepstrum coefﬁ-
Ipred
T→∞(Y ) is ﬁnite  we have Ipred
cients of Y   which are related to the Fourier transform of Y (see Appendix C). When the Fourier
transform of Y is estimated for length-2T windows in conjunction with a window function  this
method computes a regularized estimate of Ipred
T (Y ). We call this the “frequency-domain” method
of computing Gaussian predictive information (in contrast the “time-domain” method of Eq. 3).
Like the time-domain method  the frequency-domain method is differentiable in V . Its primary
advantage lies in leveraging the fast Fourier transform (FFT)  which allows DCA to be run with
much larger T than would be feasible using the time-domain method which requires computing

the log-determinant of a T -by-T matrix  an O(cid:0)T 3(cid:1) operation. By contrast  the FFT is O (T log T ).

However  the frequency-domain method is limited to ﬁnding one-dimensional projections. To ﬁnd
a multidimensional projection  one can greedily ﬁnd one-dimensional projections and iteratively
project them out of of the problem  a technique called deﬂation. However  deﬂation is not guaranteed
to ﬁnd local optima of the DCA objective since correlations between the projected variables are
ignored (Fig. 2E). For this reason  we use the time-domain implementation of DCA unless stated
otherwise.

T→∞(Y ) =(cid:80)∞

k=1 kb2

Figure 2: Comparison of Gaussian vs. full predictive information estimates (A–D) and the
frequency-domain method (E). (A) Predictive information of one-dimensional projections of the
three-dimensional Lorenz attractor as a function of the spherical coordinates (θ  φ) of the projection
using Gaussian and full (kNN) estimates. (A–D) all consider DCA with T = 1. (B) Histogram of the
Gaussian and full estimates of predictive information from (A). (C) Histogram of the Gaussian and
full estimates of predictive information of random one-dimensional projections of the 30-dimensional
noisy Lorenz embedding of Fig. 1. Red trajectories correspond to ﬁve different runs of DCA. (D)
Same as (C) but for three-dimensional projections of the same system. (E) Gaussian predictive
information of subspaces found by different implementations of DCA when run on 109-dimensional
motor cortical data (see Section 4). “DCA” directly optimizes Eq. 3  “deﬂation” optimizes Eq. 3 to
ﬁnd one-dimensional projections in a deﬂational fashion and “FFT deﬂation” uses the frequency-
domain method of computing Gaussian predictive information in a deﬂational fashion. T = 5 is used
in all three cases.

3 Related work

Though less common than static methods  linear dimensionality reduction methods which take time
into account  like DCA  are sometimes used. One popular method is Slow Feature Analysis (SFA) 

5

00full PI0Gaussian PI01full PI01Gaussian PI01full PI01Gaussian PIDCA trajectories01full PI01Gaussian PI020dimension03.3PI (nats)DCAdeflationFFT deflationABCDEwhich we examine in some depth due to its resemblance to DCA [24  25]. Given a discrete time
sereis X  where xt ∈ Rn  SFA ﬁnds projected variables yt = V T xt ∈ Rd that have unit variance 
mutually uncorrelated components and minimal mean-squared time derivatives. For a discrete one-
dimensional time series with unit variance  minimizing the mean-squared time derivative is equivalent
to maximizing the one-time step autocorrelation. Thus  SFA may be formulated as

where V ∈ Rn×d  C0 =(cid:10)xtxT

maximize tr(cid:0)V T Csym
t  C1 =(cid:10)xtxT

1 V(cid:1) subject to V T C0V = I
(cid:0)C1 + C T

t and Csym

(cid:11)

(cid:11)

t+1

1

t

(cid:1). We assume that X

(4)

1

1

Csym

1 C

C1C

−1/2
0

−1/2
0

1 = 1
2
has been temporally oversampled so that the one-time step autocorrelation of any one-dimensional
projection is positive  which is equivalent to assuming that Csym
is positive-deﬁnite (see Appendix
E for explanation). SFA is naturally compared to the T = 1 case of DCA. For one-dimensional
projections (d = 1)  the solutions of SFA and DCA coincide  since mutual information is monoton-
ically related to correlation for Gaussian variables in the positive-correlation regime. For higher-
dimensional projections (d > 1)  the comparison becomes more subtle. SFA is solved by making
the whitening transformation ˜V = C 1/2
0 V and letting ˜V be the top-d orthonormal eigenvectors of
−1/2
. To understand the solution to DCA  it is helpful to consider the relaxed
MSFA = C
0
problem of maximizing I(U T xt; V T xt+1) where U need not equal V . The relaxed problem is
solved by performing Canonical Correlation Analysis (CCA) on xt and xt+1  which entails making
0 U  ˜V = C 1/2
the whitening transformations ˜U = C 1/2
0 V and letting ˜U and ˜V be the top-d left and
−1/2
right singular vectors  respectively  of MCCA = C
[26  27]. If X has time-reversal
0
symmetry  then Csym
1 = C1  so MSFA = MCCA and the projections found by SFA and DCA agree.
(cid:54)= C1  so MSFA (cid:54)= MCCA and the projections found by SFA and
For time-irreversible processes  Csym
DCA disagree. In particular  the SFA objective has no dependence on the off-diagonal elements of
for non-Markovian processes  SFA and DCA yield different subspaces for T > 1 for all d ≥ 1 since
DCA captures longer-timescale dependencies than SFA (Fig. 3A). In summary  DCA is superior
to SFA at capturing past-future mutual information for time-irreversible and/or non-Markovian
processes. Note that most real-world systems including biological networks  stock markets and
out-of-equilibrium physical systems are time-irreversible. Moreover  real-world systems are generally
non-Markovian. Thus  when capturing past-future mutual information is of interest  DCA is superior
to SFA for most realistic applications.
With regard to the relaxed problem solved by CCA  Tegmark [28] has suggested that  for time-

V T C1V   while DCA takes these terms into account to maximize I(cid:0)V T xt; V T xt+1

irreversible processes X  the maximum of I(cid:0)U T xt; V T xt+1

(cid:1) can be signiﬁcantly reduced when

(cid:1). Additionally 

U = V is enforced. This is because  in time-irreversible processes  predictive features are not
necessarily predictable  and vice versa. However  because this work did not compare CCA (the
optimal U (cid:54)= V method) to DCA (the optimal U = V method)  the results are overly pessimistic.
We repeated the analysis of [28] using both the noisy Lorenz embedding of Fig. 1 as well as a
system of coupled oscillators that was used in [28]. For both systems  the single projection found
by DCA captured almost as much past-future mutual information as the pair of projections found
by CCA (Fig. 3B C). This suggests that while predictive and predictable features are different in
general  shared past and future features might sufﬁce for capturing most of the past-future mutual
information in a certain systems. Identifying and characterizing this class of systems could have
important implications for prediction-based unsupervised learning techniques [28  9].
In addition to SFA  other time-based linear dimensionality reduction methods have been proposed.
Maximum Autocorrelation Factors [29] is equivalent to the version of SFA described here. Com-
plexity Pursuit [30] and Forecastable Components Analysis [31] each minimize the entropy of a
nonlinear function of the projected variables. They are similar in spirit to the frequency-domain
implementation of DCA  but do not maximize past-future mutual information. Several algorithms
inspired by Independent Components Analysis that incorporate time have been proposed [32–34] 
but are designed to separate independent dimensions in time series rather than discover a dynamical
subspace with potentially correlated dimensions. Like DCA  Predictable Feature Analysis [35  36] is
a linear dimensionality reduction method with a prediction-based objective. However  Predictable
Feature Analysis requires explicitly specifying a prediction model  whereas DCA does not assume a
particular model. Moreover  Predictable Feature Analysis requires alternating optimization updates
of the prediction model and the projection matrix  whereas DCA is end-to-end differentiable. Finally 
DCA is related to the Past-Future Information Bottleneck [37] (see Appendix F).

6

Figure 3: Comparison of DCA with other methods.
(A) Autocorrelation functions of one-
dimensional DCA projections of motor cortical data (see Section 4) for T = 1  in which case
DCA is equivalent to SFA  and T = 20. While the one-time step autocorrelation is larger for the
T = 1 projection (inset)  the T = 20 projection exhibits stronger oscillations apparent at longer
timescales. (B) Performance of DCA  SFA  PCA and CCA at capturing past-future mutual informa-

(cid:1)  where U = V for DCA  SFA and PCA and U (cid:54)= V for CCA. Following

tion  I(cid:0)U T xt; V T xt+∆t

Tegmark [28]  xt comprises the position and momentum variables of 10 coupled oscillators and
∆t = 10. (C) Same as (B)  but using the 30-dimensional noisy Lorenz embedding of Fig. 1 with
∆t = 2.

We have been made aware of two existing methods which share the name Dynamical Component(s)
Analysis [38–40]. Thematically  they share the goal of uncovering low-dimensional dynamics from
time series data. Thirion and Faugeras [38] perform a two-stage  temporal then kernelized spatial
analysis. Seifert et al. [39] and Korn et al. [40] assume the observed dynamics are formed by low-
dimensional latent variables with linear and nonlinear dynamics. To ﬁt a linear approximation of the
latent variables  they derive a generalized eigenvalue problem which is sensitive to same-time and
one-time step correlations  i.e.  the data and the approximation of its ﬁrst derivative.
An alternative to objective function-based components analysis methods are generative models  which
postulate a low-dimensional latent state that has been embedded in high-dimensional observation
space. Generative models featuring latent states imbued with dynamics  such as the Kalman ﬁlter 
Gaussian Process Factor Analysis and LFADS  have found widespread use in neuroscience (see
Appendix I for comparisons of DCA with the KF and GPFA) [41–43]. The power of these methods
lies in the fact that rich dynamical structure can be encouraged in the latent state through careful
choice of priors and model structure. However  learning and inference in generative models tend to be
computationally expensive  particularly in models featuring dynamics. In the case of deep learning-
based methods such as LFADS  there are often many model and optimization hyperparameters
that need to be tuned.
In terms of computational efﬁciency and simplicity  DCA occupies an
attractive territory between linear methods like PCA and SFA  which are computationally efﬁcient but
extract relatively simple structure  and dynamical generative models like LFADS  which extract rich
dynamical structure but are computationally demanding. As a components analysis method  DCA
makes the desired properties of the learned features explicit through its objective function. Finally 
the ability of DCA to yield a linear subspace in which dynamics unfold may be exploited for many
analyses. For example  the loadings for DCA can be studied to examine the relationship between the
high- and low-dimensional variables (Appendix J).
Lastly  while DCA does not produce an explicit description of the dynamics  this is a potentially
attractive property. In particular  while dynamical generative models such as the KF provide de-
scriptions of the dynamics  they also assume a particular form of dynamics  biasing the extracted
components toward this form. By contrast  DCA is formulated in terms of spatiotemporal correlations
and  as result  can extract broad forms of (stationary) dynamics  be they linear or nonlinear. For
example  the Lorenz attractor of Fig. 1 is a nonlinear dynamical system.

4 Applications to real data

We used DCA to extract dynamical subspaces in four high-dimensional time series datasets: (i) multi-
neuronal spiking activity of 109 single units recorded in monkey primary motor cortex (M1) while
the monkey performed a continuous grid-based reaching task [44]; (ii) multi-neuronal spiking activity
of 55 single units recorded in rat hippocampal CA1 while the rat performed a reward-chasing

7

05101520t (100 ms bins)-0.20.01.0autocorrelationT=1T=2005101520dimensions retained010.5MI (nats)Coupled OscillatorsCCADCASFAPCA05dimensions retained0 3.6MI (nats)Embedded Lorenz Attractor0120.41.0ABCtask [45  46]; (iii) multi-city temperature data from 30 cities over several years [47]; and (iv) 12
variables from an accelerometer  gyroscope  and gravity sensor recording human kinematics [48].
See Appendix B for details. For all results  three bins of projected data were used to predict one bin
of response data. Data were split into ﬁve folds  and reported R2 values are averaged across folds.
To assess the performance of DCA  we noted that subspaces which capture dynamics should be
more predictive of future states than those which capture static structure. Moreover  for the motor
cortical and hippocampal datasets  subspaces which capture dynamics should be more predictive of
behavioral variables (cursor kinematics and rat location  respectively) than subspaces which do not 
since neural dynamics are believed to underlie or encode these variables [49  50]. Thus  we compared
the abilities of subspaces found by DCA  PCA and SFA to decode behavioral variables for the motor
cortical and hippocampal datasets and to forecast future full-dimensional states for the temperature
and accelerometer datasets.
For the motor cortical and hippocampal datasets  DCA outperformed PCA at predicting both current
and future behavioral variables on held-out data (Fig. 4  top row). This reﬂects the existence of
dimensions which have substantial variance  but which do not capture as much dynamical structure
as other  smaller-variance dimensions. Unlike PCA  DCA is not drawn to these noisy  high-variance
dimensions. In addition to demonstrating that DCA captures more dynamical structure than PCA  this
analysis demonstrates the utility of DCA in a common task in neuroscience  namely  extracting low-
dimensional representations of neural dynamics for visualization or further analysis (see Appendix
H for forecasting results on the neural data and Appendix J for example latent trajectories and their
relationship to the original measurement variables) [27  51]. For the temperature dataset  DCA
and PCA performed similarly  and for the accelerometer dataset  DCA outperformed PCA for the
lowest-dimensional projections. The narrower performance gap between DCA and PCA on the
temperature and accelerometer datasets suggests that the alignment between variance and dynamics
is stronger in these datasets than in the neural data.
Assuming Gaussianity  DCA is formally superior to SFA at capturing past-future mutual information
in time series which are time-irreversible and/or non-Markovian (Section 3). All four of our datasets
possess both of these properties  suggesting that subspaces extracted by DCA might offer superior
decoding and forecasting performance to those extracted by SFA. We found this to be the case across
all four datasets (Fig. 4  bottom row). Moreover  the relative performance of DCA often became
stronger as T (the past-future window size of DCA) was increased  highlighting the non-Markovian
nature of the data (see Appendix G for absolute R2 values). This underscores the importance of
leveraging spatiotemporal statistics across long timescales when extracting non-Markovian dynamical
structure from data.

5 Discussion

DCA retains the geometric interpretability of linear dimensionality reduction methods while im-
plementing an information-theoretic objective function that robustly extracts dynamical structure
while minimizing noise. Indeed  the subspace found by DCA may be thought of as the result of a
competition between aligning the subspace with dynamics and making the subspace orthogonal to
noise  as in Fig. 1A. Applied to neural  weather and accelerometer datasets  DCA often outperforms
PCA  indicating that noise variance often dominates or is comparable to dynamics variance in these
datasets. Moreover  DCA often outperforms SFA  particularly when DCA integrates spatiotemporal
statistics over long timescales  highlighting the non-Markovian statistical dependencies present in
these datasets. Overall  our results show that DCA is well-suited for ﬁnding dynamical subspaces in
time series with structural attributes characteristic of real-world data.
Many extensions of DCA are possible. Since real-world data generation processes are generally
non-stationary  extending DCA for non-stationary data is a key direction for future work. For example 
non-stationary data may be segmented into windows such that the data are approximately stationary
within each window [52]. In general  the subspace found by DCA includes contributions from all of
the original variables. For increased interpretability  DCA could be optimized with an (cid:96)1 penalty on
the projection matrix V [53] to identify a small set of relevant features  e.g.  individual neurons or
stocks [3]. Both the time- and frequency-domain implementations of DCA may be made differentiable
in the input data  opening the door to extensions of DCA that learn nonlinear transformations of the
input data  including kernel-like dimensionality expansion  or that use a nonlinear mapping from the

8

Figure 4: DCA for prediction and forecasting. For all panels  color indicates the projected di-
mensionality. For the top row  marker type indicates the lag for prediction. The top row compares
held-out R2 for DCA vs. PCA as a function of projection dimensionality and prediction lag. The
bottom row shows the difference in held-out R2 for DCA vs. SFA as a function of T   the past-future
window size parameter for DCA. (M1) Predicting cursor location from projected motor cortical data.
(Hippocampus) Predicting animal location from projected hippocampal data. (Temperature) Fore-
casting future full-dimensional temperature states from projected temperature states. (Accelerometer)
Forecasting future full-dimensional accelerometer states from projected states.

high- to low-dimensional space  including deep architectures. Since DCA ﬁnds a linear projection 
it can also be kernelized using the kernel trick. The DCA objective could also be used in recurrent
neural networks to encourage rich dynamics. Finally  dimensionality reduction via DCA could serve
as a preprocessing step for time series analysis methods which scale unfavorably in the dimensionality
of the input data  allowing such techniques to be scaled to high-dimensional data.

Acknowledgements

D.G.C. and K.E.B. were funded by LBNL Laboratory Directed Research and Development. We
thank the Neural Systems and Data Science Lab and Laurenz Wiskott for helpful discussion.

References
[1] Peiran Gao and Surya Ganguli. On simplicity and complexity in the brave new world of

large-scale neuroscience. Current opinion in neurobiology  32:148–155  2015.

[2] John P Cunningham and Zoubin Ghahramani. Linear dimensionality reduction: Survey  insights 

and generalizations. The Journal of Machine Learning Research  16(1):2859–2900  2015.

[3] Michael W Mahoney and Petros Drineas. CUR matrix decompositions for improved data

analysis. Proceedings of the National Academy of Sciences  106(3):697–702  2009.

[4] Tomas Mikolov  Kai Chen  Greg Corrado  and Jeffrey Dean. Efﬁcient estimation of word

representations in vector space. arXiv preprint arXiv:1301.3781  2013.

[5] Carl Doersch  Abhinav Gupta  and Alexei A Efros. Unsupervised visual representation learning
by context prediction. In Proceedings of the IEEE International Conference on Computer
Vision  pages 1422–1430  2015.

[6] Yoon Kim  Yacine Jernite  David Sontag  and Alexander M Rush. Character-aware neural

language models. In Thirtieth AAAI Conference on Artiﬁcial Intelligence  2016.

9

00.5PCA R200.5DCA R2T = 5 binsdimlagM151015250 ms250 ms500 ms750 ms00.15PCA R200.15T = 5 binsdimlagHippocampus101525300 ms250 ms500 ms750 ms0.61.0PCA R20.61.0T = 5 binsdimlagTemperature34560 days5 days10 days15 days0.20.9PCA R20.20.9T = 5 binsdimlagAccelerometer34560 ms60 ms120 ms180 ms246810T (50 ms bins)-0.040.08R2 improvementover SFAlag = 5 bins246810T (50 ms bins)-0.020.04lag = 5 bins246810T (1 day bins)-0.0250.05lag = 0 bins246810T (20 ms bins)-0.20.4lag = 3 bins[7] Sarah E Marzen and James P Crutchﬁeld. Nearly maximally predictive features and their

dimensions. Physical Review E  95(5):051301  2017.

[8] David McAllester. Information Theoretic Co-Training. arXiv preprint arXiv:1802.07572  2018.

[9] Aaron van den Oord  Yazhe Li  and Oriol Vinyals. Representation learning with Contrastive

Predictive Coding. arXiv preprint arXiv:1807.03748  2018.

[10] Kristofer E Bouchard and Michael S Brainard. Auditory-induced neural dynamics in sensory-
motor circuitry predict learned temporal and sequential statistics of birdsong. Proceedings of
the National Academy of Sciences  113(34):9641–9646  2016.

[11] Naftali Tishby  Fernando C Pereira  and William Bialek. The information bottleneck method.

arXiv preprint physics/0004057  2000.

[12] William Bialek  Ilya Nemenman  and Naftali Tishby. Predictability  complexity  and learning.

Neural computation  13(11):2409–2463  2001.

[13] Stephanie E Palmer  Olivier Marre  Michael J Berry  and William Bialek. Predictive information
in a sensory population. Proceedings of the National Academy of Sciences  112(22):6908–6913 
2015.

[14] Karl Pearson. On lines and planes of closest ﬁt to systems of points in space. The London 
Edinburgh  and Dublin Philosophical Magazine and Journal of Science  2(11):559–572  1901.

[15] Harold Hotelling. Analysis of a complex of statistical variables into principal components.

Journal of educational psychology  24(6):417  1933.

[16] Ming Li and Paul Vitányi. An introduction to Kolmogorov complexity and its applications.

Springer Science & Business Media  2013.

[17] Steven P Strong  Roland Koberle  Rob R de Ruyter van Steveninck  and William Bialek. Entropy

and information in neural spike trains. Physical review letters  80(1):197  1998.

[18] Liam Paninski. Estimation of entropy and mutual information. Neural computation  15(6):

1191–1253  2003.

[19] Artemy Kolchinsky and Brendan Tracey. Estimating mixture entropy with pairwise distances.

Entropy  19(7):361  2017.

[20] Alexander Kraskov  Harald Stögbauer  and Peter Grassberger. Estimating mutual information.

Physical review E  69(6):066138  2004.

[21] Xianli Zeng  Yingcun Xia  and Howell Tong. Jackknife approach to the estimation of mutual

information. Proceedings of the National Academy of Sciences  115(40):9956–9961  2018.

[22] David McAllester and Karl Statos. Formal limitations on the measurement of mutual information.

arXiv preprint arXiv:1811.04251  2018.

[23] Lei Li and Zhongjie Xie. Model selection and order determination for time series by information

between the past and the future. Journal of time series analysis  17(1):65–84  1996.

[24] Laurenz Wiskott and Terrence J Sejnowski. Slow Feature Analysis: Unsupervised learning of

invariances. Neural computation  14(4):715–770  2002.

[25] Matthias Bethge  Sebastian Gerwinn  and Jakob H Macke. Unsupervised learning of a steerable
basis for invariant image representations. In Human Vision and Electronic Imaging XII  volume
6492  page 64920C. International Society for Optics and Photonics  2007.

[26] Magnus Borga. Canonical correlation: a tutorial. On line tutorial http://people. imt. liu.

se/magnus/cca  4(5)  2001.

[27] John P Cunningham and M Yu Byron. Dimensionality reduction for large-scale neural record-

ings. Nature neuroscience  17(11):1500  2014.

10

[28] Max Tegmark. Optimal latent representations: Distilling mutual information into principal pairs.

arXiv preprint arXiv:1902.03364  2019.

[29] Rasmus Larsen. Decomposition using Maximum Autocorrelation Factors. Journal of Chemo-

metrics: A Journal of the Chemometrics Society  16(8-10):427–435  2002.

[30] Aapo Hyvärinen. Complexity pursuit: separating interesting components from time series.

Neural computation  13(4):883–898  2001.

[31] Georg Goerg. Forecastable component analysis. In International Conference on Machine

Learning  pages 64–72  2013.

[32] Lang Tong  VC Soon  Yih-Fang Huang  and RALR Liu. AMUSE: a new blind identiﬁcation
algorithm. In IEEE international symposium on circuits and systems  pages 1784–1787. IEEE 
1990.

[33] Andreas Ziehe and Klaus-Robert Müller. TDSEP—an efﬁcient algorithm for blind separation
using time structure. In International Conference on Artiﬁcial Neural Networks  pages 675–680.
Springer  1998.

[34] Harald Stögbauer  Alexander Kraskov  Sergey A Astakhov  and Peter Grassberger. Least-
dependent-component analysis based on mutual information. Physical Review E  70(6):066123 
2004.

[35] Stefan Richthofer and Laurenz Wiskott. Predictable Feature Analysis. In 2015 IEEE 14th
International Conference on Machine Learning and Applications (ICMLA)  pages 190–196.
IEEE  2015.

[36] Björn Weghenkel  Asja Fischer  and Laurenz Wiskott. Graph-based Predictable Feature Analysis.

Machine Learning  106(9-10):1359–1380  2017.

[37] Felix Creutzig  Amir Globerson  and Naftali Tishby. Past-future information bottleneck in

dynamical systems. Physical Review E  79(4):041925  2009.

[38] Bertrand Thirion and Olivier Faugeras. Dynamical components analysis of fMRI data through

kernel PCA. NeuroImage  20(1):34–49  2003.

[39] Bastian Seifert  Katharina Korn  Steffen Hartmann  and Christian Uhl. Dynamical Component
Analysis (DyCA): dimensionality reduction for high-dimensional deterministic time-series. In
2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP) 
pages 1–6. IEEE  2018.

[40] Katharina Korn  Bastian Seifert  and Christian Uhl. Dynamical Component Analysis (DyCA)
and its application on epileptic EEG. In ICASSP 2019-2019 IEEE International Conference on
Acoustics  Speech and Signal Processing (ICASSP)  pages 1100–1104. IEEE  2019.

[41] Rudolph Emil Kalman. A new approach to linear ﬁltering and prediction problems. Journal of

basic Engineering  82(1):35–45  1960.

[42] Byron M Yu  John P Cunningham  Gopal Santhanam  Stephen I Ryu  Krishna V Shenoy  and
Maneesh Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of
neural population activity. J Neurophysiol  102:614–635  2009.

[43] Chethan Pandarinath  Daniel J O’Shea  Jasmine Collins  Rafal Jozefowicz  Sergey D Stavisky 
Jonathan C Kao  Eric M Trautmann  Matthew T Kaufman  Stephen I Ryu  Leigh R Hochberg 
et al. Inferring single-trial neural population dynamics using sequential auto-encoders. Nature
methods  page 1  2018.

[44] Joseph E. O’Doherty  Mariana M. B. Cardoso  Joseph G. Makin  and Philip N. Sabes. Nonhuman
Primate Reaching with Multichannel Sensorimotor Cortex Electrophysiology  May 2017. URL
https://doi.org/10.5281/zenodo.583331.

[45] K Mizuseki  A Sirota  E Pastalkova  and G Buzsáki. Multi-unit recordings from the rat

hippocampus made during open ﬁeld foraging. Available online at: CRCNS. org  2009.

11

[46] Joshua I Glaser  Raeed H Chowdhury  Matthew G Perich  Lee E Miller  and Konrad P Kording.

Machine learning for neural decoding. arXiv preprint arXiv:1708.00909  2017.

[47] Selﬁsh Gene. Historical hourly weather data 2012-2017  Dec 2017. URL https://www.

kaggle.com/selfishgene/historical-hourly-weather-data.

[48] Mohammad Malekzadeh  Richard G Clegg  Andrea Cavallaro  and Hamed Haddadi. Protecting
sensory data against sensitive inferences. In Proceedings of the 1st Workshop on Privacy by
Design in Distributed Systems  page 2. ACM  2018.

[49] Mark M Churchland  John P Cunningham  Matthew T Kaufman  Justin D Foster  Paul Nuyu-
jukian  Stephen I Ryu  and Krishna V Shenoy. Neural population dynamics during reaching.
Nature  487(7405):51  2012.

[50] Matthew A Wilson and Bruce L McNaughton. Dynamics of the hippocampal ensemble code

for space. Science  261(5124):1055–1058  1993.

[51] Matthew D Golub  Patrick T Sadtler  Emily R Oby  Kristin M Quick  Stephen I Ryu  Eliza-
beth C Tyler-Kabara  Aaron P Batista  Steven M Chase  and Byron M Yu. Learning by neural
reassociation. Nature neuroscience  21(4):607–616  2018.

[52] Antonio C Costa  Tosif Ahamed  and Greg J Stephens. Adaptive  locally linear models of
complex dynamics. Proceedings of the National Academy of Sciences  116(5):1501–1510 
2019.

[53] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society: Series B (Methodological)  58(1):267–288  1996.

12

,David Clark
Jesse Livezey
Kristofer Bouchard