2018,Regret Bounds for Robust Adaptive Control of the Linear Quadratic Regulator,We consider adaptive control of the Linear Quadratic Regulator (LQR)  where an
unknown linear system is controlled subject to quadratic costs. Leveraging recent
developments in the estimation of linear systems and in robust controller synthesis 
we present the first provably polynomial time algorithm that achieves sub-linear
regret on this problem. We further study the interplay between regret minimization
and parameter estimation by proving a lower bound on the expected regret in
terms of the exploration schedule used by any algorithm. Finally  we conduct a
numerical study comparing our robust adaptive algorithm to other methods from
the adaptive LQR literature  and demonstrate the flexibility of our proposed method
by extending it to a demand forecasting problem subject to state constraints.,Regret Bounds for Robust Adaptive Control of the

Linear Quadratic Regulator

Sarah Dean Horia Mania

Nikolai Matni

Benjamin Recht

Stephen Tu

University of California  Berkeley

Abstract

We consider adaptive control of the Linear Quadratic Regulator (LQR)  where
an unknown linear system is controlled subject to quadratic costs. Leveraging
recent developments in the estimation of linear systems and in robust controller
synthesis  we present the ﬁrst provably polynomial time algorithm that provides
high probability guarantees of sub-linear regret on this problem. We further study
the interplay between regret minimization and parameter estimation by proving a
lower bound on the expected regret in terms of the exploration schedule used by any
algorithm. Finally  we conduct a numerical study comparing our robust adaptive
algorithm to other methods from the adaptive LQR literature  and demonstrate the
ﬂexibility of our proposed method by extending it to a demand forecasting problem
subject to state constraints.

1

Introduction

The problem of adaptively controlling an unknown dynamical system has a rich history  with classical
asymptotic results of convergence and stability dating back decades [12  13]. Of late  there has
been a renewed interest in the study of a particular instance of such problems  namely the adaptive
Linear Quadratic Regulator (LQR)  with an emphasis on non-asymptotic guarantees of stability and
performance. Initiated by Abbasi-Yadkori and Szepesvári [1]  there have since been several works
analyzing the regret suffered by various adaptive algorithms on LQR– here the regret incurred by
an algorithm is thought of as a measure of deviations in performance from optimality over time.
These results can be broadly divided into two categories: those providing high-probability guarantees
for a single execution of the algorithm [1  4  8  11]  and those providing bounds on the expected
Bayesian regret incurred over a family of possible systems [2  16]. As we discuss in more detail 
these methods all suffer from one or several of the following limitations: restrictive and unveriﬁable
assumptions  limited applicability  and computationally intractable subroutines. In this paper  we
provide  to the best of our knowledge  the ﬁrst polynomial-time algorithm for the adaptive LQR
problem that provides high probability guarantees of sub-linear regret  and that does not require
unveriﬁable or unrealistic assumptions.

Related Work. There is a rich body of work on the estimation of linear systems as well as on the
robust and adaptive control of unknown systems. We target our discussion to works on non-asymptotic
guarantees for the LQR control of an unknown system  broadly divided into three categories.
Ofﬂine estimation and control synthesis: In a non-adaptive setting  i.e.  when system identiﬁcation
can be done ofﬂine prior to controller synthesis and implementation  the ﬁrst work to provide end-
to-end guarantees for the LQR optimal control problem is that of Fiechter [10]  who shows that the
discounted LQR problem is PAC-learnable. Dean et al. [6] improve on this result  and provide the
ﬁrst end-to-end sample complexity guarantees for the inﬁnite horizon average cost LQR problem.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Optimism in the Face of Uncertainty (OFU): Abbasi-Yadkori and Szepesvári [1]  Faradonbeh et al.
[8]  and Ibrahimi et al. [11] employ the Optimism in the Face of Uncertainty (OFU) principle [5] 
which optimistically selects model parameters from a conﬁdence set by choosing those that lead to
the best closed-loop (inﬁnite horizon) control performance  and then plays the corresponding optimal
controller  repeating this process online as the conﬁdence set shrinks. While OFU in the LQR setting

has been shown to achieve optimal regret (cid:101)O(√T )  its implementation requires solving a non-convex
optimization problem to precision (cid:101)O(T −1/2)  for which no provably efﬁcient implementation exists.
method achieves (cid:101)O(T 2/3) regret with high-probability for scalar systems. However  their proof
consider expected regret in a Bayesian setting  and provide TS methods which achieve (cid:101)O(√T ) regret.

Thompson Sampling (TS): To circumvent the computational roadblock of OFU  recent works replace
the intractable OFU subroutine with a random draw from the model uncertainty set  resulting in
Thompson Sampling (TS) based policies [2  4  16]. Abeille and Lazaric [4] show that such a

does not extend to the non-scalar setting. Abbasi-Yadkori and Szepesvári [2] and Ouyang et al. [16]

Although not directly comparable to our result  we remark on the computational challenges of these
algorithms. Whereas the proof of Abbasi-Yadkori and Szepesvári [2] was shown to be incorrect [15] 
Ouyang et al. [16] make the restrictive assumption that there exists a (known) initial compact set Θ
describing the uncertainty in the system parameters  such that for any system θ1 ∈ Θ  the optimal
controller K(θ1) is stabilizing when applied to any other system θ2 ∈ Θ. No means of constructing
such a set are provided  and there is no known tractable algorithm to verify if a given set satisﬁes this
property. Also  it is implicitly assumed that projecting onto this set can be done efﬁciently.

programs of size logarithmic in T .

Contributions. To develop the ﬁrst polynomial-time algorithm that provides high probability
guarantees of sub-linear regret  we leverage recent results from the estimation of linear systems [17] 
robust controller synthesis [14  19]  and coarse-ID control [6]. We show that our robust adaptive
control algorithm: (i) guarantees stability and near-optimal performance at all times; (ii) achieves

a regret up to time T bounded by (cid:101)O(T 2/3); and (iii) is based on ﬁnite-dimensional semideﬁnite
Furthermore  our method estimates the system parameters at (cid:101)O(T −1/3) rate in operator norm.

Although system parameter identiﬁcation is not necessary for optimal control performance  an
accurate system model is often desirable in practice. Motivated by this  we study the interplay
between regret minimization and parameter estimation  and identify fundamental limits connecting
the two. We show that the expected regret of our algorithm is lower bounded by Ω(T 2/3)  proving
that our analysis is sharp up to logarithmic factors. Moreover  our lower bound suggests that the
estimation rate achievable by any algorithm with O(T α) regret is Ω(T −α/2).
Finally  we conduct a numerical study of the adaptive LQR problem  in which we implement our
algorithm  and compare its performance to heuristic implementations of OFU and TS based methods.
We show on several examples that the regret incurred by our algorithm is comparable to that of the
OFU and TS based methods. Furthermore  the inﬁnite horizon cost achieved by our algorithm at
any given time on the true system is consistently lower than that attained by OFU and TS based
algorithms. Finally  we use a demand forecasting example to show how our algorithm naturally
generalizes to incorporate environmental uncertainty and safety constraints. The full version of this
paper is [7].

2 Problem Statement and Preliminaries

In this work we consider adaptive control of the following discrete-time linear system

xk+1 = A(cid:63)xk + B(cid:63)uk + wk   wk

(2.1)
where xk ∈ Rn is the state  uk ∈ Rp is the control input  and wk ∈ Rn is the process noise. We
assume that the state variables are observed exactly and  for simplicity  that x0 = 0. We consider the
Linear Quadratic Regulator optimal control problem  given by cost matrices Q (cid:23) 0 and R (cid:31) 0 

wI)  

i.i.d.∼ N (0  σ2
(cid:35)

(cid:34) T(cid:88)

J(cid:63) = min

u

lim
T→∞

E

1
T

x(cid:62)
k Qxk + u(cid:62)

k Ruk

s.t. dynamics (2.1)  

(2.2)

k=1

2

T(cid:88)

k=1

where the minimum is taken over measurable functions u = {uk(·)}k≥1  with each uk adapted to
the history xk  xk−1  . . .   x1  and possibe additional randomness independent of future states. Given
knowledge of (A(cid:63)  B(cid:63))  the optimal policy is a static state-feedback law uk = K(cid:63)xk  where K(cid:63) is
derived from the solution to a discrete algebraic Riccati equation.
We are interested in algorithms which operate without knowledge of the true system transition
matrices (A(cid:63)  B(cid:63)). We measure the performance of such algorithms via their regret  deﬁned as

Regret(T ) :=

(x(cid:62)

k Qxk + u(cid:62)

k Ruk − J(cid:63)) .

(2.3)

The regret of any algorithm is lower-bounded by Ω(√T )  a bound matched by OFU up to logarithmic
factors [8]. However  after each epoch  OFU requires optimizing a non-convex objective to O(T −1/2)
precision. Instead  our method uses a subroutine based on quasi-convex optimization and robust
control.

2.1 Preliminaries: System Level Synthesis

We brieﬂy describe the necessary background on robust control and System Level Synthesis [19]
(SLS). These tools were recently used by Dean et al. [6] to provide non-asymptotic bounds for LQR
in the ofﬂine “estimate-and-then-control” setting. In the appendix of the full version [7] we expand
on these preliminaries.
Consider the dynamics (2.1)  and ﬁx a static state-feedback control policy K  i.e.  let uk = Kxk.
Then  the closed loop map from the disturbance process {w0  w1  . . .} to the state xk and control
input uk at time k is given by

Letting Φx(k) := (A(cid:63) + B(cid:63)K)k−1 and Φu(k) := K(A(cid:63) + B(cid:63)K)k−1  we can rewrite Eq. (2.4) as

xk = (cid:80)k
uk = (cid:80)k
t=1(A(cid:63) + B(cid:63)K)k−twt−1  
t=1 K(A(cid:63) + B(cid:63)K)k−twt−1 .
(cid:20)xk
k(cid:88)

(cid:20)Φx(k − t + 1)

wt−1  

(cid:21)

(cid:21)

=

uk

Φu(k − t + 1)

t=1

(2.4)

(2.5)

where {Φx(k)  Φu(k)} are called the closed loop system response elements induced by the controller
K. The SLS framework shows that for any elements {Φx(k)  Φu(k)} constrained to obey

Φx(k + 1) = A(cid:63)Φx(k) + B(cid:63)Φu(k)   Φx(1) = I   ∀k ≥ 1  

(2.6)
there exists some controller that achieves the desired system responses (2.5). The state-feedback
parameterization result in Theorem 1 of Wang et al. [19] formalizes this observation: the SLS
framework therefore allows for any optimal control problem over linear systems to be cast as an
optimization problem over elements {Φx(k)  Φu(k)}  constrained to satisfy the afﬁne equations (2.6).
Comparing equations (2.4) and (2.5)  we see that the former is non-convex in the controller K 
whereas the latter is afﬁne in the elements {Φx(k)  Φu(k)}  enabling solutions to previously difﬁcult
optimal control problems.
As we work with inﬁnite horizon problems  it is notationally more convenient to work with transfer
function representations of the above objects  which can be obtained by taking a z-transform of
their time-domain representations. The frequency domain variable z can be informally thought
of as the time-shift operator  i.e.  z{xk  xk+1  . . .} = {xk+1  xk+2  . . .}  allowing for a compact
representation of LTI dynamics. We use boldface letters to denote such transfer functions  e.g. 

k=1 Φx(k)z−k. Then  the constraints (2.6) can be rewritten as

Φx(z) =(cid:80)∞

[zI − A(cid:63) −B(cid:63)]

= I  

(2.7)

and the corresponding (not necessarily static) control law u = Kx is given by K = ΦuΦ−1
x .
Although other approaches to optimal controller design exists  we argue now that the SLS parameteri-
zation has some appealing properties when applied to the control of uncertain systems. In particular 

3

(cid:20)Φx

(cid:21)

Φu

= I

if and only if

Φu

(cid:21)

(cid:2)zI − (cid:98)A −(cid:98)B(cid:3)(cid:20)Φx

suppose that rather than having access to the true system transition matrices (A(cid:63)  B(cid:63))  we instead

only have access to estimates ((cid:98)A  (cid:98)B). The SLS framework allows us to characterize the system
responses achieved by a controller  computed using only the estimates ((cid:98)A  (cid:98)B)  on the true system
(A(cid:63)  B(cid:63)). Speciﬁcally  if we denote (cid:98)∆ := ((cid:98)A − A(cid:63))Φx + ((cid:98)B − B(cid:63))Φu  simple algebra shows that
The robust stability result in Theorem 2 of Matni et al. [14] shows that if (I + (cid:98)∆)−1 exists  then the
x   computed using only the estimates ((cid:98)A  (cid:98)B)  achieves the following response
Further  if K stabilizes the system ((cid:98)A  (cid:98)B)  and (I + (cid:98)∆)−1 is stable (simple sufﬁcient conditions can

controller K = ΦuΦ−1
on the true system (A(cid:63)  B(cid:63)):

(I + (cid:98)∆)−1w .

= I + (cid:98)∆ .

[zI − A(cid:63) −B(cid:63)]

(cid:20)Φx

(cid:21)

Φu

(cid:20)Φx

(cid:21)

Φu

(cid:21)

(cid:20)x

u

be derived to ensure this  see [6])  then K is also stabilizing for the true system. It is this transparency
between system uncertainty and controller performance that we exploit in our algorithm.
We end this discussion with the deﬁnition of a function space that we use extensively throughout:

=

.

(2.8)

(cid:40)

(cid:41)
M (k)z−k | (cid:107)M (k)(cid:107) ≤ Cρk   k = 1  2  ...

∞(cid:88)

k=1

S(C  ρ) :=

M =

(cid:112)(cid:80)∞

The space S(C  ρ) consists of (strictly proper) stable transfer functions that satisfy a certain decay
rate in the spectral norm of their impulse response elements. We denote the restriction of S(C  ρ)
to the space of F -length ﬁnite impulse response (FIR) ﬁlters by SF (C  ρ)  i.e.  M ∈ SF (C  ρ) if
M ∈ S(C  ρ)  and M (k) = 0 for all k > F .
We equip S(C  ρ) with the H∞ and H2 norms  which are inﬁnite horizon analogs of the spectral
and Frobenius norms of a matrix  respectively: (cid:107)M(cid:107)H∞ = sup(cid:107)w(cid:107)2=1 (cid:107)Mw(cid:107)2 and (cid:107)M(cid:107)H2 =
F . The H∞ and H2 norm have distinct interpretations. The H∞ norm of a system
M is equal to its (cid:96)2 (cid:55)→ (cid:96)2 operator norm  and can be used to measure the robustness of a system to
unmodelled dynamics [20]. The H2 norm has a direct interpretation as the energy transferred to the
system by a white noise process  and is hence closely related to the LQR optimal control problem.
Unsurprisingly  the H2 norm appears in the objective function of our optimization problem  whereas
the H∞ norm appears in the constraints to ensure robust stability and performance.
3 Algorithm and Guarantees

k=1(cid:107)M (k)(cid:107)2

Our proposed robust adaptive control algorithm for LQR is shown in Algorithm 1. We note that while
Line 8 of Algorithm 1 is written as an inﬁnite-dimensional optimization problem  it can be formulated
in terms of ﬁnite-dimensional decision variables {Φx(k)  Φu(k)}F
k=1 due to the restriction to FIR
ﬁlters. In this formulation  the H2 cost can be written as a Frobenius norm and the H∞ constraint
reduces to a linear matrix inequality. Therefore  the inner optimization can be equivalently written as
a semideﬁnite program over O(Fi(n2 + np)) decision variables. We describe this transformation in
detail in appendix Section G of the full version [7]. We also note that the outer optimization over γ
can be performed efﬁciently by bisection search because the objective is jointly quasi-convex in the
decision variables and is smooth with respect to γ in the feasible domain.

Some remarks on practice are in order. First  in Line 6  only the trajectory data collected during the
i-th epoch is used for the least squares estimate. Second  the epoch lengths we use grow exponentially
in the epoch index. These settings are chosen primarily to simplify the analysis; in practice all the
data collected should be used  and it may be preferable to use a slower growing epoch schedule (such
as Ti = CT (i + 1)). Additionally  for storage considerations  instead of performing a batch least
squares update of the model  a recursive least squares (RLS) estimator rule can be used to update
the parameters in an online manner. Furthermore  many constants in Algorithm 1 depend on the
unknown system to be consistent with our data-independent analysis. In practice  these parameters
can be estimated from collected data.
Finally  we note that the proofs for all results in this section can be found in the full version [7].

4

Algorithm 1 Robust Adaptive Control Algorithm
Require: Stabilizing controller K(0)  failure probability δ ∈ (0  1)  and constants (C(cid:63)  ρ(cid:63) (cid:107)K(cid:63)(cid:107)).
1: Set Cx ←

(cid:16)
O(1)C(cid:63)
(1−ρ(cid:63))3   Cu ← (cid:107)K(cid:63)(cid:107)Cx  and ρ ← .999 + .001ρ(cid:63).

(cid:17)

(n + p) C4

(cid:63) (1+(cid:107)K(cid:63)(cid:107))4
(1−ρ(cid:63))8

.

2: Set CT ← (cid:101)O

3: for i = 0  1  2  ... do
4:
5:

6:

7:

8:

w(Ti/CT )−1/3.

η i ← σ2
k )}Ti

Set Ti ← CT 2i and σ2
Set Di = {(x(i)
k   u(i)
is
obtained from the controller K(i) plus an additional noise term for exploration. More precisely 
u(i) = K(i)x(i) + η(i)  where each entry of η(i) is drawn i.i.d. from N (0  σ2

k=1 ← evolve system forward Ti steps  where each action u(i)
(cid:80)Ti−1
Set ((cid:98)Ai  (cid:98)Bi) ← arg minA B
(cid:113) n+p
(cid:17)
(cid:16) σw(cid:107)K(cid:63)(cid:107)C(cid:63)
Set εi ← (cid:101)O
2(cid:107)x(i)
and Fi ←
(cid:13)(cid:13)(cid:13)(cid:13)(cid:20)Q1/2
(cid:3)(cid:20)Φx
(cid:21)
s.t.(cid:2)zI − (cid:98)Ai −(cid:98)Bi

x   where (Φx  Φu) are the solution to
1
1 − γ

(cid:101)O(1)(i+1)
k+1 − Ax(i)
1−ρ(cid:63)
(cid:21)(cid:20)Φx

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)H∞ ≤ γ  

Set K(i+1) = ΦuΦ−1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:20)Φx

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)H2

k − Bu(i)
k (cid:107)2
2.

minimizeγ∈[0 1)

ση i(1−ρ(cid:63))3

√2εi

η iIp).

1
zFi

= I +

min

Φx Φu V

R1/2

Φu

Φu

Φu

1

k=1

V  

0

0

Ti

.

k

(cid:107)V (cid:107) ≤ CxρFi+1   Φx ∈ SFi(Cx  ρ)   Φu ∈ SFi(Cu  ρ) .

1 − CxρFi+1

9: end for

3.1 Regret Upper Bounds

Our guarantees for Algorithm 1 are stated in terms of certain system speciﬁc constants  which we
deﬁne here. We let K(cid:63) denote the static feedback solution to the LQR problem for (A(cid:63)  B(cid:63)  Q  R).
Next  we deﬁne (C(cid:63)  ρ(cid:63)) such that the closed loop system A(cid:63) + B(cid:63)K(cid:63) belongs to S(C(cid:63)  ρ(cid:63)). Our
main assumption is stated as follows.
Assumption 3.1. We are given a controller K(0) that stabilizes the true system (A(cid:63)  B(cid:63)). Further-
more  letting (Φx  Φu) denote the response of K(0) on (A(cid:63)  B(cid:63))  we assume that Φx ∈ S(Cx  ρ)
and Φu ∈ S(Cu  ρ)  where the constants Cx  Cu  ρ are deﬁned in Algorithm 1.
The requirement of an initial stabilizing controller K(0) is not restrictive; Dean et al. [6] provide an
ofﬂine strategy for ﬁnding such a controller. Furthermore  in practice Algorithm 1 can be initialized
with no controller  with random inputs applied instead to the system in the ﬁrst epoch to estimate
(A(cid:63)  B(cid:63)) within an initial conﬁdence set for which the synthesis problem becomes feasible.
Our ﬁrst guarantee is on the rate of estimation of (A(cid:63)  B(cid:63)) as the algorithm progresses through
time. This result builds on recent progress [17] for estimation along trajectories of a lin-
ear dynamical system.
polylog
1−ρ(cid:63)

the notation (cid:101)O(·) hides absolute constants and
1 − δ the following statement holds. Suppose that T is at an epoch boundary. Let ((cid:98)A(T )  (cid:98)B(T ))

Theorem 3.2. Fix a δ ∈ (0  1) and suppose that Assumption 3.1 holds. With probability at least
denote the current estimate of (A(cid:63)  B(cid:63)) computed by Algorithm 1 at the end of time T . Then  this
estimate satisﬁes the guarantee

  n  p (cid:107)B(cid:63)(cid:107) (cid:107)K(cid:63)(cid:107)

For what follows 

factors.

δ   C(cid:63) 

(cid:16)

(cid:17)

T  1

1

max{(cid:107)(cid:98)A(T ) − A(cid:63)(cid:107) (cid:107)(cid:98)B(T ) − B(cid:63)(cid:107)} ≤ (cid:101)O

(cid:18) C(cid:63)(cid:107)K(cid:63)(cid:107)

(1 − ρ(cid:63))3

(cid:19)

.

√n + p
T 1/3

Theorem 3.2 shows that Algorithm 1 achieves a consistent estimate of the true dynamics (A(cid:63)  B(cid:63)) 

and learns at a rate of (cid:101)O(T −1/3). We note that consistency of parameter estimates is not a guarantee

provided by OFU or TS based approaches.

5

Next  we state an upper bound on the regret incurred by Algorithm 1.
Theorem 3.3. Fix a δ ∈ (0  1) and suppose that Assumption 3.1 holds. With probability at least
(cid:19)
1 − δ the following statement holds. For all T ≥ 0 we have that Algorithm 1 satisﬁes
.

C 4
(cid:63) (1 + (cid:107)K(cid:63)(cid:107))4(1 + (cid:107)B(cid:63)(cid:107))2J(cid:63)

(n + p)

(cid:18)

T 2/3

Regret(T ) ≤ (cid:101)O

Here  the notation (cid:101)O(·) also hides o(T 2/3) terms.

(1 − ρ(cid:63))16

Our proof strategy works as follows. We ﬁrst decompose regret by epochs as follows:

O(log2 T )(cid:88)

Ti(cid:88)

i=0

k=1

Regret(T ) =

((x(i)

k )(cid:62)Qx(i)

k + (u(i)

k )(cid:62)Ru(i)

k − J(cid:63))  

(cid:80)Ti

η i/σ2

k +(u(i)

k )(cid:62)Ru(i)

where x(i)
k denotes the state at the k-th timestep in the i-th epoch (and similarly for u(i)
k ). By
standard concentration of measure arguments  we can upper bound w.h.p. the per-epoch regret
k )(cid:62)Qx(i)
k=1((x(i)
k −J(cid:63)) by its expected value plus a deviation term that involves the
norm of x(i)
0 . Because we constrain the impulse response coefﬁcients of the SLS response {Φx  Φu}
in Algorithm 1  this allows to easily bound (cid:107)x(i)
0 (cid:107)2 w.h.p. again by using standard concentration
arguments. We then use the SLS machinery to quantify the difference between the expected cost
over the horizon Ti minus J(cid:63)  which yields that the regret incurred during epoch i is bounded by
w) contribution is
the additional cost incurred from injecting exploration noise. We then bound our estimation error by
  we have the per-epoch regret
). Choosing α = 1/3 to balance these competing powers of Ti

(cid:101)O(Ti(σ2
εi = (cid:101)O((σw/ση i)T
is bounded by (cid:101)O(T 1−α
and summing over logarithmic number of epochs  we obtain a ﬁnal regret of (cid:101)O(T 2/3).

w + εi−1)J(cid:63))  where εi−1 is the estimation error  and the O(σ2

) using Theorem 3.2. Setting σ2
+ T 1−(1−α)/2

The main difﬁculty in the proof is ensuring that the transient behavior of the resulting controllers is
uniformly bounded when applied to the true system. Prior works sidestep this issue by assuming that
the true dynamics lie within a (known) compact set for which the Heine-Borel theorem asserts the
existence of ﬁnite constants that capture this behavior. We go a step further and work through the
perturbation analysis which allows us to give a regret bound that depends only on simple quantities
of the true system (A(cid:63)  B(cid:63)). The full proof is given in the appendix.
Finally  we remark that the dependence on 1/(1 − ρ(cid:63)) in our results is an artifact of our perturbation
analysis  and we leave sharpening this dependence to future work.

wT −α

η i = σ2

−1/2
i

η i/σ2

i

i

i

3.2 Regret Lower Bounds and Parameter Estimation Rates

We saw that Algorithm 1 achieves (cid:101)O(T 2/3) regret with high probability. Now we provide a matching

algorithmic lower bound on the expected regret  showing that the analysis presented in Section 3.1 is
sharp as a function of T . Moreover  our lower bound characterizes how much regret must be accrued
in order to achieve a speciﬁed estimation rate for the system parameters (A(cid:63)  B(cid:63)).
Theorem 3.4. Let the initial state x0 be distributed according to the steady state distribution
N (0  P∞) of the optimal closed loop system  and let {ut}t≥0 be any sequence of inputs as in
Section 2. Furthermore  let f : R → R be any function such that with probability 1 − δ we have

≥ f (T ) .

(3.1)

(cid:20)xk

uk

(cid:21)(cid:2)x(cid:62)

k

(cid:3)(cid:33)

u(cid:62)

k

λmin

(cid:32)T−1(cid:88)
(cid:3)

k=0

Then  there exist positive values T0 and C0 such that for all T ≥ T0 we have

k Qxk + u(cid:62)

k Ruk − J(cid:63)

≥

1
2

(1 − δ)λmin(R)(1 + σmin(K(cid:63))2)f (T − T0) − C0  

E(cid:2)x(cid:62)

T(cid:88)

k=0

where T0 and C0 are functions of A(cid:63)  B(cid:63)  Q  R  σ2
are given in the proof.

w  and n. We note the speciﬁc form of T0 and C0

6

The proof of the estimation error Theorem 3.2 shows that Algorithm 1 satisﬁes Eq. (3.1) with
η i used by Algorithm 1 during the i-th
wT −i/3)  we obtain the following corollary which demonstrates the

η Θ(log2(T ))). Since the exploration variance σ2

epoch is given by σ2
sharpness of our regret analysis with respect to the scaling of T .
Corollary 3.5. For T > C1(n  δ  σ2

η i = O(σ2

w  A(cid:63)  B(cid:63)  Q  R) the expected regret of Algorithm 1 satisﬁes

k Qxk + u(cid:62)

k Ruk − J(cid:63)

(cid:3)

≥(cid:101)Ω(λmin(R)(1 + σmin(K(cid:63))2)T 2/3) .

f (T ) = (cid:101)O(T σ2
T(cid:88)

E(cid:2)x(cid:62)

k=1

A natural question to ask is how much regret does any algorithm accrue in order to achieve estimation

error (cid:107)(cid:98)A − A(cid:63)(cid:107) ≤ ε and (cid:107)(cid:98)B − B(cid:63)(cid:107) ≤ ε. From Theorem 3.2 we know that Algorithm 1 estimates
(A(cid:63)  B(cid:63)) at rate (cid:101)O(T −1/3). Therefore  in order to achieve ε estimation error  T must be (cid:101)Ω(ε−3).
Hence  Theorem 3.3 implies that the regret of Algorithm 1 to achieve ε estimation error is (cid:101)O(ε−2).
Interestingly  let us consider any other Algorithm achieving O(T α) regret for some 0 < α < 1.
Then  Theorem 3.4 suggests that the best rate achievable by such an algorithm is O(T −α/2)  since
the minimum eigenvalue condition Eq. (3.1) governs the signal-to-noise ratio. In the case of linear-
regression with independent data it is known that the minimax estimation rate is lower bounded by
square root of the inverse of the minimum eigenvalue (3.1). We conjecture that the same results
we note that while Algorithm 1 estimates (A(cid:63)  B(cid:63)) at a rate (cid:101)O(T −1/3)  Theorem 3.4 suggests that
holds in our case. Therefore  to achieve ε estimation error  any Algorithm would likely require
Ω(ε−2) regret  showing that Algorithm 1 is optimal up to logarithmic factors in this sense. Finally 
any algorithm achieving the O(√T ) regret would estimate (A(cid:63)  B(cid:63)) at a rate Ω(T −1/4).
4 Experiments

Regret Comparison. We illustrate the performance of several adaptive schemes empirically. We
compare the proposed robust adaptive method with non-Bayesian Thompson sampling (TS) as
in Abeille and Lazaric [4] and a heuristic projected gradient descent (PGD) implementation of OFU.
As a simple baseline  we use the nominal control method  which synthesizes the optimal inﬁnite-
horizon LQR controller for the estimated system and injects noise with the same schedule as the
robust approach. Computational burden varies across adaptive methods due to differences in both
cost and frequency of controller synthesis; implementation details and computational considerations
for all methods are in Section G of the full version [7].
The comparison experiments are carried out on the following LQR problem:

(cid:34)1.01 0.01

(cid:35)

0

A(cid:63) =

0.01 1.01 0.01
0.01 1.01

0

  B(cid:63) = I  Q = 10I  R = I  σw = 1 .

(4.1)

This system corresponds to a marginally unstable Laplacian system where adjacent nodes are weakly
connected; these dynamics were also studied by [3  6  18]. The cost is such that input size is penalized
relatively less than state. This problem setting is amenable to robust methods due to both the cost
ratio and the marginal instability  which are factors that may hurt optimistic methods. In Section H of
the full version [7]  we show similar results for an unstable system with large transients.
To standardize the initialization of the various adaptive methods  we use a rollout of length T0 = 100
where the input is a stabilizing controller plus Gaussian noise with ﬁxed variance σu = 1. This
trajectory is not counted towards the regret  but the recorded states and inputs are used to initialize
parameter estimates. In each experiment  the system starts from x0 = 0 to reduce variance over runs.

For all methods  the actual errors (cid:98)At − A(cid:63) and (cid:98)Bt − B(cid:63) are used rather than bounds or bootstrapped

estimates. The effect of this choice on regret is small  as examined empirically in Section H of [7].
The performance of the various adaptive methods over time is compared in Figure 1. The median
and 90th percentile cumulative regret over 500 instances is displayed in Figure 1a  which gives an
idea of both typical and worst-case behavior. The regret of the optimal LQR controller for the true
system is displayed as a baseline. Overall  the methods have very similar performance. One beneﬁt
of robustness is the guaranteed stability and bounded inﬁnite-horizon cost at every point during

7

(a) Cumulative Regret

(b) Inﬁnite Horizon LQR Cost

Figure 1: A comparison of different adaptive methods on 500 experiments of the marginally unstable Laplacian
example in 4.1. In (a)  the median and 90th percentile cumulative regret is plotted over time. In (b)  the median
and 90th percentile inﬁnite-horizon LQR cost of the epoch’s controller.

(a) Demand Forecasting

(b) Constraint Satisfaction

Figure 2: The addition of constraints in the robust synthesis problem can guarantee the safe execution of
adaptive systems. We consider an example inspired by demand forecasting  as illustrated in (a)  where the left
hand side of the diagram represents unknown dynamics. The median and maximum values of (cid:107)xt(cid:107)∞ over 500
trials are plotted for both the unconstrained and constrained synthesis problems in (b).

operation. In Figure 1b  this inﬁnite-horizon LQR cost is plotted for the controllers played during
each epoch. This value measures the cost of using each epoch’s controller indeﬁnitely  rather than
continuing to update its parameters. The robust adaptive method performs relatively better than other
adaptive algorithms  indicating that it is more amenable to early stopping  i.e.  to turning off the
adaptive component of the algorithm and playing the current controller indeﬁnitely.

Extension to Uncertain Environment with State Constraints. The proposed robust adaptive
method naturally generalizes beyond the standard LQR problem. We consider a disturbance fore-
casting example which incorporates environmental uncertainty and safety constraints. Consider a
system with known dynamics driven by stochastic disturbances that are now correlated in time. We
model the disturbance process as the output of an unknown autonomous LTI system  as illustrated in
Figure 2a. This setting can be interpreted as a demand forecasting problem  where  for example  the
system is a server farm and the disturbances represent changes in the amount of incoming jobs. If
the dynamics of the correlated disturbance process are known  this knowledge can be used for more
cost-effective temperature control.
We let the system (A(cid:63)  B(cid:63)) with known dynamics be described by the graph Laplacian dynamics as
in Eq. (4.1). The disturbance dynamics are unknown and are governed by a stable system transition
matrix Ad  resulting in the following dynamics for the full system:

(cid:20)xt+1

(cid:21)

dt+1

(cid:20)A(cid:63)

=

I
0 Ad

(cid:21)(cid:20)zt

(cid:21)

dt

(cid:20)B(cid:63)

(cid:21)

0

+

ut +

(cid:21)

(cid:20)0

I

(cid:34)0.5

(cid:35)

.

0.1
0.5
0

0
0.1
0.5

wt   Ad =

0
0

The costs are set to model expensive inputs  with Q = I and R = 1 × 103I. The controller synthesis
problem in Line 8 of Algorithm 1 is modiﬁed to reﬂect the problem structure  and crucially  we add a
constraint on the system response Φx. Further details of the formulation are explained in Section H
of [7]. Figure 2b illustrates the effect. While the unconstrained synthesis results in trajectories with
large state values  the constrained synthesis results in much more moderate behavior.

8

025050075010001250150017502000Iteration0500100015002000RegretOFUTSRobustNominalOptimal025050075010001250150017502000Iteration10−310−2CostSuboptimalityOFUTSRobustNominalLTIfiltertemperaturecontrolstochasticity demand025050075010001250150017502000Iteration02468101214statenormnoconstraintconstrained5 Conclusions and Future Work

We presented a polynomial-time algorithm for the adaptive LQR problem that provides high probabil-
ity guarantees of sub-linear regret. In contrast to other approaches to this problem  our robust adaptive
method guarantees stability  robust performance  and parameter estimation. We also explored the
interplay between regret minimization and parameter estimation  identifying fundamental limits
connecting the two.
Several questions remain to be answered. It is an open question whether a polynomial-time algorithm

can achieve a regret of (cid:101)O(√T ). In our implementation of OFU  we observed that PGD performed

quite effectively. Interesting future work is to see if the techniques of Fazel et al. [9] for policy
gradient optimization on LQR can be applied to prove convergence of PGD on the OFU subroutine 
which would provide an optimal polynomial-time algorithm. Moreover  we observed that OFU
and TS methods in practice gave estimates of system parameters that were comparable with our
method which explicitly adds excitation noise. It seems that the switching of control policies at epoch
boundaries provides more excitation for system identiﬁcation than is currently understood by the
theory. Furthermore  practical issues that remain to be addressed include satisfying safety constraints
and dealing with nonlinear dynamics; in both settings  ﬁnite-sample parameter estimation/system
identiﬁcation and adaptive control remain an open problem.

Acknowledgments

We thank the anonymous reviewers for their feedback  which improved the clarity of our presentation.
SD is supported by an NSF Graduate Research Fellowship under Grant No. DGE 1752814. As part
of the RISE lab  HM is generally supported in part by NSF CISE Expeditions Award CCF-1730628 
DHS Award HSHQDC-16-3-00083  and gifts from Alibaba  Amazon Web Services  Ant Financial 
CapitalOne  Ericsson  GE  Google  Huawei  Intel  IBM  Microsoft  Scotiabank  Splunk and VMware.
BR is generously supported in part by NSF award CCF-1359814  ONR awards N00014-17-1-2191 
N00014-17-1-2401  and N00014-17-1-2502  the DARPA Fundamental Limits of Learning (Fun LoL)
and Lagrange Programs  and an Amazon AWS AI Research Award.

References
[1] Yasin Abbasi-Yadkori and Csaba Szepesvári. Regret Bounds for the Adaptive Control of Linear

Quadratic Systems. In Conference on Learning Theory  2011.

[2] Yasin Abbasi-Yadkori and Csaba Szepesvári. Bayesian Optimal Control of Smoothly Parame-
terized Systems: The Lazy Posterior Sampling Algorithm. In Conference on Uncertainty in
Artiﬁcial Intelligence  2015.

[3] Yasin Abbasi-Yadkori  Nevena Lazic  and Csaba Szepesvári. Model-Free Linear Quadratic

Control via Reduction to Expert Prediction. arXiv:1804.06021  2018.

[4] Marc Abeille and Alessandro Lazaric. Thompson Sampling for Linear-Quadratic Control

Problems. In AISTATS  2017.

[5] S. Bittanti and M. C. Campi. Adaptive control of linear time invariant systems: the “bet on the

best” principle. Communications in Information and Systems  6(4)  2006.

[6] Sarah Dean  Horia Mania  Nikolai Matni  Benjamin Recht  and Stephen Tu. On the Sample

Complexity of the Linear Quadratic Regulator. arXiv:1710.01688  2017.

[7] Sarah Dean  Horia Mania  Nikolai Matni  Benjamin Recht  and Stephen Tu. Regret Bounds for

Robust Adaptive Control of the Linear Quadratic Regulator. arXiv:1805.09388  2018.

[8] Mohamad Kazem Shirani Faradonbeh  Ambuj Tewari  and George Michailidis. Finite Time
Analysis of Optimal Adaptive Policies for Linear-Quadratic Systems. arXiv:1711.07230  2017.

[9] Maryam Fazel  Rong Ge  Sham M. Kakade  and Mehran Mesbahi. Global Convergence of
Policy Gradient Methods for the Linear Quadratic Regulator. In International Conference on
Machine Learning  2018.

9

[10] Claude-Nicolas Fiechter. PAC Adaptive Control of Linear Systems. In Conference on Learning

Theory  1997.

[11] Morteza Ibrahimi  Adel Javanmard  and Benjamin Van Roy. Efﬁcient Reinforcement Learning
for High Dimensional Linear Quadratic Systems. In Neural Information Processing Systems 
2012.

[12] Petros A Ioannou and Jing Sun. Robust adaptive control  volume 1. PTR Prentice-Hall Upper

Saddle River  NJ  1996.

[13] Miroslav Krstic  Ioannis Kanellakopoulos  and Peter V Kokotovic. Nonlinear and adaptive

control design. Wiley  1995.

[14] Nikolai Matni  Yuh-Shyang Wang  and James Anderson. Scalable system level synthesis for

virtually localizable systems. In IEEE Conference on Decision and Control  2017.

[15] Ian Osband and Benjamin Van Roy. Posterior Sampling for Reinforcement Learning Without

Episodes. arXiv:1608.02731  2016.

[16] Yi Ouyang  Mukul Gagrani  and Rahul Jain. Learning-based Control of Unknown Linear

Systems with Thompson Sampling. arXiv:1709.04047  2017.

[17] Max Simchowitz  Horia Mania  Stephen Tu  Michael I. Jordan  and Benjamin Recht. Learning
Without Mixing: Towards A Sharp Analysis of Linear System Identiﬁcation. In Conference on
Learning Theory  2018.

[18] Stephen Tu and Benjamin Recht. Least-Squares Temporal Difference Learning for the Linear

Quadratic Regulator. In International Conference on Machine Learning  2018.

[19] Yuh-Shyang Wang  Nikolai Matni  and John C Doyle. A System Level Approach to Controller

Synthesis. arXiv:1610.04815  2016.

[20] K. Zhou  J. C. Doyle  and K. Glover. Robust and Optimal Control. 1995.

10

,Horia Mania
Nikolai Matni
Benjamin Recht
Stephen Tu