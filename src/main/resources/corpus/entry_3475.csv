2018,MixLasso: Generalized Mixed Regression via Convex Atomic-Norm Regularization,We consider a generalization of mixed regression where the response is an additive combination of several mixture components. Standard mixed regression is a special case where each response is generated from exactly one component. Typical approaches to the mixture regression problem employ local search methods such as Expectation Maximization (EM) that are prone to spurious local optima. On the other hand  a number of recent theoretically-motivated \emph{Tensor-based methods} either have high sample complexity  or require the knowledge of the input distribution  which is not available in most of practical situations. In this work  we study a novel convex estimator \emph{MixLasso} for the estimation of generalized mixed regression  based on an atomic norm specifically constructed to regularize the number of mixture components. Our algorithm gives a risk bound that trades off between prediction accuracy and model sparsity without imposing stringent assumptions on the input/output distribution  and can be easily adapted to the case of non-linear functions. In our numerical experiments on mixtures of linear as well as nonlinear regressions  the proposed method yields high-quality solutions in a wider range of settings than existing approaches.,MixLasso: Generalized Mixed Regression via

Convex Atomic-Norm Regularization

Ian E.H. Yen ∗† Wei-Cheng Lee ‡

Pradeep Ravikumar ∗

Sung-En Chang ‡ Kai Zhong §

Shou-De Lin ‡

∗ Carnegie Mellon University

† Snap Inc.

‡ National Taiwan University

§ Amazon Inc.

Abstract

We consider a generalization of mixed regression where the response is an additive
combination of several mixture components. Standard mixed regression is a special
case where each response is generated from exactly one component. Typical
approaches to the mixture regression problem employ local search methods such
as Expectation Maximization (EM) that are prone to spurious local optima. On the
other hand  a number of recent theoretically-motivated Tensor-based methods either
have high sample complexity  or require the knowledge of the input distribution 
which is not available in most of practical situations. In this work  we study a novel
convex estimator MixLasso for the estimation of generalized mixed regression 
based on an atomic norm speciﬁcally constructed to regularize the number of
mixture components. Our algorithm gives a risk bound that trades off between
prediction accuracy and model sparsity without imposing stringent assumptions on
the input/output distribution  and can be easily adapted to the case of non-linear
functions. In our numerical experiments on mixtures of linear as well as nonlinear
regressions  the proposed method yields high-quality solutions in a wider range of
settings than existing approaches.

1

Introduction

The Mixed Regression (MR) problem considers the estimation of K functions from a collection of
input-output samples  where for each sample  the output is generated by one of the K regression
functions. When ﬁtting linear functions in a noiseless setting  this is equivalent to solving K
linear systems  while at the same time  identifying which system each equation belongs to. The
MR formulation can be employed as an approach to decompose a complicated function into K
simpler ones  by splitting the observations into K classes. Variants of regression families such as
piecewise-linear regression can be viewed as special cases of MR.
However  the MR problem is NP-hard in general [1] due to the simultaneous ﬁtting of the discrete
class labels as well as the regression functions. Standard approaches to the mixture problem employ
local search methods such as Expectation Maximization (EM) [2] and Variational Bayes [3] that are
prone to spurious local optima. There have thus been several lines of recent work studying estimation
of mixed regression models with strong statistical guarantees under additional statistical assumptions.
For the special case of linear function with K=2 components  [4] propose a convex nuclear norm
minimization formulation that is guaranteed to estimate the two functions with minimax-optimal rates
when given a sub-Gaussian design matrix. With the additional conditions of zero noise and isotropic
Gaussian inputs  [1] propose an initialization for the EM algorithm to guarantee exact recovery of
the true parameters. However  in addition to the stringent statistical assumptions  these methods and
results are specialized to the case of two components  and seem non-trivial to generalize.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

For problems with more than two components  most of the existing approaches [5  6  7  8] rely
on the Tensor Methods.
In particular  for a D-dimensional linear MR problem  [6] propose a
convex optimization formulation using a third-order tensor  which results in a computational cost
of O(N D12) and a sample complexity of O(D6/2)  limiting its application to problems of small
dimension. The Tensor Decomposition approach proposed in [5] has a sample complexity of only
O(D3K 4/2) and is computationally efﬁcient. However  it requires the knowledge of the input
probability distribution in order to derive the score function used in their algorithm  which might not
be available  and estimating the density over the D-dimensional input variables could be an even
harder problem than MR itself. Other recent work [7  8] show that in the noiseless setting with
isotropic Gaussian inputs  an Alternating Minimization algorithm initialized with the Tensor Method
leads to exact recovery of the true parameters. These latter methods have sample complexities linear
in D  but with O(K K)  O(K 10) dependencies in K respectively. Finally  [9] observed that  under the
assumption of well-separated data  one can use a guaranteed clustering algorithm to ﬁnd the mixture
assignment of each observation  and thus solves the MR problem as a by-product. However  the data
distribution considered in MR  such as those assumed in [5  6  7  8]  are usually not well-separated
(see our Figure 3 as an example).
In this work  we address a generalized version of Mixed Regression where the output can be an
additive combination of several mixture components. Our approach follows the general meta-approach
emerging in the recent years of addressing latent-variable model estimation from the perspective of
high-dimensional sparse estimation [10  11  12]. We propose a novel convex estimator MixLasso for
the mixed regression problem  which enforces the mixture structure through minimizing a carefully
constructed atomic norm that acts as a surrogate function for the number of mixture components.
We then propose a greedy algorithm that generates a steepest-descent component at each iteration
through solving a sub-problem similar to MAX-CUT. Our analysis of the algorithm gives a risk
bound that trades off prediction accuracy and model sparsity  with a sample complexity that is linear
in both D and K  and without imposing any stringent assumptions on  or assuming knowledge of  the
input/output distribution beyond that of boundedness  and even allowing for model mis-speciﬁcation.
This makes our MixLasso algorithm a theoretically sound method for a wide range of practical
settings. Moreover  we also show how our proposed method can be easily extended to the nonlinear
regression setting  to regression functions lying in a Reproducing Kernel Hilbert Space (RKHS).
Our experiments with both generalized MR and standard MR show that the proposed method ﬁnds
high-quality solutions in a wider range of settings when compared to existing approaches.

K(cid:88)

k=1

2 Generalized Mixed Regression
In Generalized Mixed Regression  the response y ∈ R  given covariates x ∈ X   is speciﬁed as:

y =

zkfk(x) + ω

(1)
where zk ∈ {0  1}  k = 1  . . .   K is a latent binary vector indicating the presence or absence of each
component  and fk(xi) : RD → R is the regression function of k-th component. The standard mixed
regression is a special case of (1) with additional constraint (cid:107)z(cid:107)0 = 1. Here ω ∈ R is a noise term
with both bias and variance. In other words  we consider the very general setting where we allow
for model mis-speciﬁcation  and in general E[ω|x  z] (cid:54)= 0. This makes our problem setting in (1)
very practically plausible  especially when the regression functions {fk(x)}K
k=1 lie in some restricted
family such as linear functions. Our goal is to ﬁnd F := {fk(x)}K

k=1 minimizing the risk

(cid:34)

(cid:35)

(y − K(cid:88)

k=1

r(F) := E

min

z∈{0 1}K

1
2

zkfk(x))2

 

(2)

while keeping the number of components K as small as possible. This yields a trade-off between
r(F) and K. While one can always have a small risk with K → ∞  we would like to ﬁnd the
smallest K that achieves such risk.

3 MixLasso: Convex Estimation via Atomic Norm
In the following  we will ﬁrst focus on the linear case fk(x) := (cid:104)wk  x(cid:105) and consider extension
to nonlinear functions in Section 4.2. Given a collection of i.i.d. samples {(xi  yi)}N
i=1  the (cid:96)2-

2

regularized Empirical Risk Minimization (ERM) problem for our task (2) is
(cid:107)W(cid:107)2
F .

(yi − zT

i W xi)2 +

min

W∈RK×D zi∈{0 1}K

τ
2

N(cid:88)

i=1

1
2N

(3)

N(cid:88)
(cid:80)N
i=1 α∗

i=1

(3) is a hard optimization problem in general due to the simultaneous minimization w.r.t. parameters
W and binary hidden variables {zi}N
i=1 [1]. However  given hidden variables  the problem is convex
w.r.t. W   and thus  from the duality theory (3) is equivalent to

min

Z∈{0 1}N×K

max
α∈RN

−1
N

tr(D(α)XX TD(α)ZZ T)

L∗(yi −αi) − 1
2N 2τ

(4)
2(cid:107)α(cid:107)2
2 (y − ξ)2. The maximizer α∗ of (4) and minimizer
i ) = 1

i=1  D(α) is a diagonal matrix formed by vector α  and L∗(y  α) = yTα + 1

where Z := (zi)N
is the convex conjugate of square loss L(y  ξ) = 1
W ∗ of (3) are related by W ∗ = 1
A key observation for our formulation is that  although (4) is non-convex w.r.t. Z  it is a convex
function of M := ZZ T (since it is a maximum over linear functions of M). Therefore  the
intractability of (4) only lies in the combinatorial constraint M = ZZ T for some Z ∈ {0  1}N×K.
To relax such constraint  we introduce an atomic norm [13] of the form

N τ Z TD(α∗)X.

i (zixT

N τ

(cid:107)M(cid:107)S := min
c≥0

ca s.t. M =

where S := {zzT|z ∈ {0  1}N}. Note if ca takes integer values {0  1}  M =(cid:80)

a∈S caa = ZZ T
for some Z ∈ {0  1}N×K and (cid:107)M(cid:107)S = K. When ca is allowed to be any nonnegative number  (5)
serves as a convex approximation to the number of components K in a sense similar to (cid:96)1-norm
as a convex approximation for the number of non-zero elements in Lasso [14]. Then the MixLasso
estimator minimizes minM∈RN×N

g(M ) + λ(cid:107)M(cid:107)S where

caa.

a∈S

a∈S

(5)

(cid:88)

(cid:88)

+

g(M ) := max
α∈RN

− 1
2N 2τ

tr(D(α)XX TD(α)M ) − 1
N

4 Algorithm

L∗(yi −αi)

(6)

(cid:18) ¯K(cid:88)

(cid:19)

The convex formulation (6) is still a challenging optimization problem since it involves an atomic
norm deﬁned over ¯K := 2N atoms. An equivalent formulation expresses (6) as the minimizatioin of

F (c) := g

ckzkzkT

+ λ(cid:107)c(cid:107)1

(7)

+   where {zk} ¯K

w.r.t. c ∈ R ¯K
k=1 enumerates ∀z ∈ {0  1}N . We introduce a greedy algorithm
(Algorithm 1) for MixLasso  which maintains a sparse set of active components and adds one more
active component zkzkT at each iteration corresponding to the steepest descent direction

k=1

(cid:104)∇g(M )  zzT(cid:105) = − 1
2N 2τ

min

z∈{0 1}N

(8)
where α∗ is the maximizer in (6). As we show in Section 4.1  (8) is equivalent to a MAX-CUT
like problem that can be solved efﬁciently with a constant-ratio approximation guarantee. Then
we minimize (7) w.r.t. coefﬁcients corresponding to the active components through a sequence of
proximal gradient updates:

z∈{0 1}N

max

(cid:104)D(α∗)XX TD(α∗)  zzT(cid:105) 

k ←
cs+1

(9)
for k ∈ A  and s = 1 . . . S  where γ is the Lipschitz-continuous parameter of the coordinate-wise
gradient zkT∇g(M )zk. The evaluation of ∇g(M s) involves ﬁnding the maximizer α∗  which can
be obtained by solving the least-square problem:

γ|A| (zkT∇g(M s)zk + λ)

k − 1
cs

+

(cid:20)

N(cid:88)

i=1

(cid:21)

W ∗ := argmin
W∈R|A|×D

1
2N

(yi − zT

i W xi)2 +

tr(W TD−1(cA)W )

τ
2

(10)

N(cid:88)

i=1

3

Algorithm 1 A Greedy Algorithm for MixLasso (6)

Initialize A = ∅  c = 0.
for t = 1...T do

1. Find a greedy component zzT by solving (8).
2. Add zzT to the active set A.
3. Minimize (7) w.r.t. coordinates cA in the active set A through updates (9).
4. Eliminate {zkzkT|ck = 0} from A.

end for.

i = (yi − zT

i W ∗xi). Let E be the N × (|A|D) design matrix of the least-square
and compute α∗
problem (10). By maintaining E  ETE whenever the active set A changes  solving the least-square
problem (10) costs O(D3|A|3) amortizedly.

4.1 Greedy Generation of Components

Problem (8) for ﬁnding the steepest descent direction is a convex maximization problem with binary-
valued variables and is hard in general. However  we show that it is equivalent to a Boolean Quadratic
Maximization problem similar to MAX-CUT  where constant-ratio approximate algorithm exists
through a Semideﬁnite Relaxation [15]. Furthermore  the Semideﬁnie Relaxation of this type has
scalable solver that requires only complexity linear to the coefﬁcient matrix [16  17].
Let C = D(α∗)XX TD(α∗).
the form
maxz∈{0 1}N(cid:104)C  zzT(cid:105)  which can be reduced to a problem of binary variables v ∈ {−1  1}N
via a transformation v = 2z − 1:

The greedy step (8) solves a problem of

where 1 denotes N-dimensional vector of all 1s. By introducing a dummy variable v0  (11) is
equivalent to

(cid:0)(cid:104)C  vvT(cid:105) + 2(cid:104)C  1vT(cid:105) + (cid:104)C  11T(cid:105)(cid:1) .
(cid:20) v0
(cid:21)(cid:20) v0

(cid:21)T(cid:20) 1T C1 1T C

v

C1

C

v

(cid:21)

max

v∈{−1 1}N

1
4

max

(v0;v)∈{−1 1}N +1

1
4

(11)

.

(12)

Note one can always ﬁnd a solution of v0 = 1 by ﬂipping signs of the solution since this does not
change the objective value. Let the matrix in (12) be ˆC. Problem of form (12) is a Boolean Quadratic
problem similar to MAX-CUT  for which there is Semideﬁnite relaxation of the form

max
V ∈SN
s.t.

(cid:104) ˆC  V (cid:105)
V (cid:23) 0  diag(V ) = 1

(13)

and rounding from which guarantees a solution ˆv to (12) satisfying h − h(ˆv) ≤ ρ(h − h) with
ρ = 2/5 [15]  where h(v) denotes the objective function of (12) and h  h denote the maximum and
minimum of the objective in (12) respectively. Note this result holds for any symmetric matrix ˆC.
Since our problem has a positive-semideﬁnite matrix ˆC  we have h = 0 and therefore the component
zk found this way satisﬁes

−zkT∇g(M )zk = h(ˆv) ≥ µh = µ max
z∈{0 1}N

(14)
with µ = 1 − ρ = 3/5. Semideﬁnite Programming of the form (13) allows specialized solver with
iteration cost linear to the matrix size nnz( ˆC) [16  17]. And it is worth mentioning that  since our
matrix ˆC has low-rank structure (8)  our implementation of the SDP solver [17] can further reduce
the complexity per iteration from nnz( ˆC) to nnz(X).

−zT∇g(M )z

4.2 Nonlinear Extension

A simple way to consider a nonlinear version of the MixLasso estimator is to consider each component
fk(x) lying in a Reproducing Kernel Hilbert Space (RKHS) H with respect to some Mercer kernel

4

K(· ·). In this setting  given {zi}N

(cid:32)
i=1  the minimizer {f∗
N(cid:88)
yi − K(cid:88)

zikfk(xi)

(cid:33)2
k}K

k=1 of

K(cid:88)

+

τ
2

(cid:107)fk(cid:107)2H

min
fk∈H

1
2N

(cid:80)N
satisﬁes the condition of the Representer Theorem that ensures an expression of the form f∗
i=1 αizikK(xi  x)  k ∈ [K]  for the minimizer  and results in a MixLasso estimator (6) with

k=1

k=1

i=1

k (x) =

(15)

N(cid:88)

i=1

g(M ) := max
α∈RN

− 1
2N 2τ

tr(D(α)QD(α)M ) − 1
N

L∗(yi −αi)

(16)

where Q : N × N is the kernel matrix with Qij = K(xi  xj). Then Algorithm 1 can be applied
with the only difference on the evaluation of gradient ∇g(M )  which requires ﬁnding the maximizer
N τ Q ◦ M + I)α = y. where ◦ denotes the
α∗ of (16) by solving the following linear system: ( 1
elementwise product.

4.3 Rounding Procedure for Generalized & Standard Mixed Rregression
While the atomic-norm regularization λ(cid:107)M(cid:107)S is a good convex relaxation of the number of compo-
nents  the number of non-zero components getting from estimator (6) cannot be precisely speciﬁed
apriori by the hyper-parameter λ directly. In practice  it is often useful to obtain a solution c with
exactly (cid:107)c(cid:107)0 = K non-zeros. This can be achieved by setting the K coefﬁcients of largest magnitude
to 1 and all the other coefﬁcients to 0. This results in a N × K matrix of hidden assignments ˆZ as the
output of Algorithm 1. Then  starting from ˆZ  we can perform a number of alternating minimization
steps between model parameters W (or {fk}K
i=1 until
convergence  as in a standard EM algorithm (with MAP hard assignment on zi).
While we have proposed a solution of the generalized version (1)  in some applications  it might be
of interest to solve the special case of standard mixed regression  where each observation belongs
to exactly one mixture component. One approach to convert a generalized mixture solution with K
components to a standard mixture of J components is to ﬁnd the most frequent J patterns z1  z2  ...  zJ
from the estimated hidden assignments {ˆzi}N
i=1  and then force each observation to choose their
j=1 instead of arbitrary 0-1 patterns {0  1}K. This
hidden assignments {zi}N
i=1 from the set {zj}J
results in J functions {fj}J
k=1 zjkfk(x)  j ∈ [J]  being actually used in
the training observation  and thus gives a valid model {fj}J
j=1 of standard mixed regression with
J components. Then as noted previously  one can further reﬁne this rounded solution through EM
iterates of standard mixed regression  initialized with component functions {fj}J

j=1 of the form fj(x) =(cid:80)K

k=1 in general) and hidden assignments {zi}N

j=1.

5 Analysis

5.1 Convergence Analysis
We assume y and x are bounded such that |y| ≤ Ry  (cid:107)x(cid:107)2 ≤ Rx. And without loss of generality  we
assume the data are scaled such that Ry = Rx = 1. Then the following theorem guarantees the rate
of convergence for Algorithm 1 up to a certain precision determined by the approximation ratio given
in (14).
Theorem 1. Let F (c) be the objective (7). The greedy algorithm (Algorithm 1) satisﬁes

F (cT ) − F (c∗) ≤ 2γ(cid:107)c∗(cid:107)2

1

(17)
µ2
for any iterate T satisfying F (cT ) − F (c∗) ≥ 2(1−µ)
µ λ(cid:107)c∗(cid:107)1  where c∗ is any reference solution 
µ = 3/5 is the approximation ratio given by (14) and γ is the Lipschitz-continuous constant of the
coordinate-wise gradient zkT∇g(M )zk  ∀k ∈ [ ¯K].
Then the following lemma shows that  with the additional assumption that F (c) is strongly convex
over a restricted support set A∗  one can get a bound in terms of the (cid:96)0-norm of the reference solution.

T

.

(cid:18) 1

(cid:19)

5

Figure 1: Results for Noiseless Mixture of Linear Regression with N (0  I) input distribution (Top)
and U (−1  1) input distribution (Bottom)  where (Left) D=100  K=3  (Middle) D=20  K=10  and
(Right) Generalized Mixture of Regression with D=20  K=3.

Figure 2: Results for Noisy (σ = 0.1) Mixture of Linear Regression with N (0  I) input distribution
(Top) and U (−1  1) input distribution (Bottom)  where (Left) D=100  K=3  (Middle) D=20  K=10 
and (Right) Generalized Mixture of Regression with D=20  K=3.

Lemma 1. Let A∗ ∈ [ ¯K] be a support set and c∗ := arg minc:supp(c)=A∗ F (c∗). Suppose F (c) is

strongly convex on A∗ with parameter β. We have (cid:107)c∗(cid:107)1 ≤(cid:113) 2(cid:107)c∗(cid:107)0(F (0)−F (c∗))

.

β

(cid:115)

(cid:18) 1

(cid:19)

T

2(1 − µ)λ

µ

+

2(cid:107)c∗(cid:107)0

β

.

(18)

(cid:80)N

Since F (0) − F (c∗) ≤ 1

2N

i=1 y2

i ≤ 1   from (17)  we have

F (cT ) − F (c∗) ≤ 4γ(cid:107)c∗(cid:107)0

βµ2

for any c∗ := arg minc:supp(c)=A∗ F (c).

5.2 Generalization Analysis

In this section  we investigate the performance of output from Algorithm 1 in terms of the risk (2).
√
Given a coefﬁcients c with support A  we can construct the weight matrix by ˆW (c) = D(
cA)W
with W = Z TAD(α∗)X  where ZA = (zk)k∈A and α∗ is the maximizer in (6) as a function of c.
From the duality between (3) and (4)  ˆW satisﬁes
(yi − zT

F + λ(cid:107)cA(cid:107)1.

N(cid:88)

F (cA) =

ˆW xi)2 +

i

(cid:107)W(cid:107)2

(19)

1
2N

τ
2

i=1

6

05001000150020002500300035004000N00.020.040.060.080.10.120.140.160.180.2Parameter-ErrorNormal-D=100-K=3ALT-randomALT-tensorEM-randomEM-tensorMixLasso00.511.52N×10400.050.10.150.20.250.30.350.4Parameter-ErrorNormal-D=20-K=10ALT-randomALT-tensorEM-randomEM-tensorMixLasso050010001500N00.050.10.150.20.250.30.350.40.450.5Parameter-ErrorGeneralized-Normal-D=20-K=3EM-randomMixLasso05001000150020002500300035004000N00.020.040.060.080.10.120.140.160.180.2Parameter-ErrorUniform-D=100-K=3ALT-randomALT-tensorEM-randomEM-tensorMixLasso00.511.52N×10400.050.10.150.20.250.30.350.4Parameter-ErrorUniform-D=20-K=10ALT-randomALT-tensorEM-randomEM-tensorMixLasso050010001500N00.10.20.30.40.50.6Parameter-ErrorGeneralized-Uniform-D=20-K=3EM-randomMixLasso05001000150020002500300035004000N00.020.040.060.080.10.120.140.160.180.2Parameter-ErrorNormal-Noisy-D=100-K=3ALT-randomALT-tensorEM-randomEM-tensorMixLasso00.511.52N×10400.050.10.150.20.250.30.350.4Parameter-ErrorNormal-Noisy-D=20-K=10ALT-randomALT-tensorEM-randomEM-tensorMixLasso050010001500N00.050.10.150.20.250.30.350.40.450.5Parameter-ErrorGeneralized-Normal-Noisy-D=20-K=3EM-randomMixLasso05001000150020002500300035004000N00.020.040.060.080.10.120.140.160.180.2Parameter-ErrorUniform-Noisy-D=100-K=3ALT-randomALT-tensorEM-randomEM-tensorMixLasso00.511.52N×10400.050.10.150.20.250.30.350.4Parameter-ErrorUniform-Noisy-D=20-K=10ALT-randomALT-tensorEM-randomEM-tensorMixLasso050010001500N00.050.10.150.20.250.30.350.40.450.5Parameter-ErrorGeneralized-Uniform-Noisy-D=20-K=3EM-randomMixLassoµ2β ( K

3 log( RK

 ) and N = Ω( DK

The following theorem gives a risk bound for the output weight matrix ˆW (c) from Algorithm 1.
Theorem 2. Let A  ˆc  ˆW be the set of active components  coefﬁcients and corresponding weight
matrix obtained from T iterations of Algorithm 1  and ¯W be the minimizer of the population risk
(2) with K components and (cid:107) ¯W(cid:107)F ≤ R. We have r( ˆW ) ≤ r( ¯W ) +  with probability 1 − ρ for
T ≥ 4γ
Note the output of Algorithm 1 has number of components ˆK ≤ T . Therefore  Theorem 2 gives
a trade-off between the suboptimality of risk r( ˆW ) − r( ¯W ) ≤  and number of components
ˆK = O(K/). Note the result of Theorem (2) is obtained without distributional assumption on
the input/output (except boundedness)  so it is in general not possible to guarantee convergence to
an optimal risk with exactly K components  since ﬁnding such optimal solution is NP-hard even
measured by the empirical risk [1]. It remains open if one can give a tighter result for the estimator (6)
that achieves -suboptimal risk with number of components being a constant multiple of K  or derive
a bound on the parameter estimation error  possibly with additional assumptions on the observations.

ρ )) with λ  τ chosen appropriately as functions of N.

6 Experiments

i=1 and {fk(x)}K

In this section  we compare the proposed MixLasso method with other state-of-the-art approaches
listed as follows. (i) EM-Random: A standard EM algorithm that alternates between minimizing
{zi}N
k=1 until convergence  with random initialized W∼ N (0  I) in the linear case
and random initialized Z∼ M ultinoulli(1/K) in the nonlinear case. Each point in the ﬁgures is
the best result out of 100 random trials. (ii) EM-Tensor: The EM algorithm initialized with Tensor
Method proposed in [8]. The formula of Tensor Method is derived assuming xi ∼ N (0  I). We adopt
implementation provided by the author of [7]. (iii) ALT-Random: An Alternating Minimization
algorithm proposed in [7] with the same initialization strategy and number of trails as EM-Random.
(iv) ALT-Tensor: The Alternating Minimization algorithm initialized with Tensor Method proposed
in [7]. The formula of Tensor Method is derived assuming xi ∼ N (0  I). We adopt implementation
provided by the author of [7]. (v) MixLasso: The proposed estimator with Algorithm 1. We round
our solution to exact K components according to the rounding procedure described in Section 4.3 for
generalized MR and standard MR respectively. The rounded solution is further reﬁned by EM iterates.
For the linear case  we compare methods using the root mean square error on the learned parameters
W compared to the ground-truth parameters W ∗ of size K × D: minS:|S|=K
  where
S denotes a multiset that selects the best matched row in W for each row in W ∗. For the nonlinear
case  we compare methods using RMSE between the predicted value and the ground-truth function
value:

k=1 zikfk(xi) −(cid:80)K

(cid:80)N
i=1((cid:80)K

(cid:107)WS :−W ∗(cid:107)F

(cid:113)

k=1 z∗

ikf∗

k (xi))2.

√

DK

1
N

6.1 Experiments on Synthetic Data

where Syn1∼Syn12 are generated by D-dimensional linear models fk(x) = wT

We generate 14 synthetic data sets according to the model: yi =(cid:80)K
are generated by 1-dimensional polynomial model of degree 6: fk(x) =(cid:80)6

k=1 zikfk(x) + ωi  i ∈ [N ] 
k x and Syn13∼Syn14
j=1 wkjxj. Figure 1 and 2
give experimental results of the linear model in the noiseless and noisy case respectively. We observe
that  in the case of Normal input distribution (Syn1  Syn2  Syn7  Syn8) (top row)  both the Tensor-
initialized methods and MixLasso consistently improve upon random-initialized EM/ALT (even
with 100 trials) in terms of the number of samples required to achieve a good performance  where
ALT performs better than EM in higher dimensional case (D = 100  K = 3) whileEM performs
better for cases of more components (D = 20  K = 10); meanwhile  MixLasso leads to signiﬁcant
improvements in both cases. On the other hand  when the input distribution becomes U(-1 1) (Syn4 
Syn5  Syn10  Syn11)  the tensor-initialized method becomes even worse than the random-initialized
ones  presumably due to the model mis-speciﬁcation  while MixLasso still consistently improve
upon the random initialized EM/ALT. Note we are testing Tensor Method derived based on the
Normal assumption on data with Uniform input on purpose. The fgoal is to see the effect of model
misspeciﬁcation on the Tensor approach  as in practice one would always have model misspeciﬁcation
to some degree. The rightmost columns of Figure 1  2 show the results on data generated from
the generalized mixed regression model (Syn3  Syn6  Syn9  Syn12)  where Tensor-based methods

7

Figure 3: Results on Mixture of 6th-order Polynomial Regression of K=4 components with noise
(Bottom) and without noise (Top). (Left) The best result of EM out of 100 random initialization.
(Middle) Solution from MixLasso followed by ﬁne-tuning EM iterates. (Right) Comparison in terms
of RMSE.

Figure 4: Results of ﬁtting mixture of polynomial regressions on the Stock data set of increasing num-
ber of samples. The top row shows results ﬁtted by EM  and the bottom row shows that from MixLasso.
From left to right we have (left) 100 weeks  (middle) 200 weeks  and (right) 300 weeks. From left to
right  the RMSE of EM=(6.33  6.04  6.27) and the RMSE of MixLasso=(6.29  5.75  5.58).

are not applicable  while MixLasso improves upon EM-Random by a large margin. Figure 3 gives
a comparison of EM-Random and MixLasso on Mixture of Kernel Regression with polynomial
kernel K(xi  xj) = (axT
i xj + b)d (d = 6)  where we generate K=4 random 6th-degree polynomial
functions {f∗
k=1 by uniform sampling their coefﬁcients from U (−4  4). In this setting  we found
EM-Random has a hard time converging to the ground-truth solution even with 100-restarts  while
MixLasso obtains solution close to the ground truth with a small number of samples.

k}K

6.2 Experiments on Real Data

In this section  we compare MixLasso and EM (with 100 restarts) for ﬁtting a mixture of polynomial
regressions on a Stock data set that contains the mixed stock prices of IBM  Facebook  Microsoft and
Nvidia of span 300 weeks till the Feb. of 2018. The task is to automatically recover the company label
of each stock price  while ﬁtting the stock price time series of each company as a polynomial curve.
Both EM and MixLasso use a polynomial kernel of the parameters: K(xi  xj) = (2xT
i xj + 2)8. The
results are shown in Figure 4. We can see that MixLasso almost recovers the pattern when all samples
are given  except for a small number of samples generated by Nvidia’s rapid growth recently. While
MixLasso consistently achieving a lower RMSE over different sample sizes  the RMSE gap between
MixLasso and EM increases as the number of samples grows.

Acknowledgements. P.R. acknowledges the support of NSF via IIS-1149803  IIS-1664720  DMS-
1264033  and ONR via N000141812861.

8

-1-0.8-0.6-0.4-0.200.20.40.60.81-10-5051015-1-0.8-0.6-0.4-0.200.20.40.60.81-10-50510150100200300400500600700800900N00.10.20.30.40.50.60.7RMSEPolyKernel-Deg=6-K=4EM-randomLasso-1-0.8-0.6-0.4-0.200.20.40.60.81-10-505101520-1-0.8-0.6-0.4-0.200.20.40.60.81-10-50510150100200300400500600700800900N00.10.20.30.40.50.60.7RMSEPolyKernel-Noisy-Deg=6-K=4EM-randomMixLasso00.10.20.30.40.50.60.70.80.91050100150200250Normalized week numbersOpen priceStock−Separation−EM (N=400) IBMFBMicrosoftNvidia00.10.20.30.40.50.60.70.80.91050100150200250Normalized week numbersOpen priceStock−Separation−EM (N=800) IBMFBMicrosoftNvidia00.10.20.30.40.50.60.70.80.91050100150200250Normalized week numbersOpen priceStock−Separation−EM (N=1200) IBMFBMicrosoftNvidia00.10.20.30.40.50.60.70.80.91050100150200Normalized week numbersOpen priceStock−Separation−MixLasso (N=400) IBMFBMicrosoftNvidia00.10.20.30.40.50.60.70.80.91050100150200Normalized week numbersOpen priceStock−Separation−MixLasso (N=800) IBMFBMicrosoftNvidia00.10.20.30.40.50.60.70.80.91050100150200Normalized week numbersOpen priceStock−Separation−MixLasso (N=1200) IBMFBMicrosoftNvidiaReferences
[1] Xinyang Yi  Constantine Caramanis  and Sujay Sanghavi. Alternating minimization for mixed

linear regression. In ICML  pages 613–621  2014.

[2] Lei Xu  Michael I Jordan  and Geoffrey E Hinton. An alternative model for mixtures of experts.

In Advances in neural information processing systems  pages 633–640  1995.

[3] Christopher M Bishop and Markus Svenskn. Bayesian hierarchical mixtures of experts. In
Proceedings of the Nineteenth conference on Uncertainty in Artiﬁcial Intelligence  pages 57–64.
Morgan Kaufmann Publishers Inc.  2002.

[4] Yudong Chen  Xinyang Yi  and Constantine Caramanis. A convex formulation for mixed

regression with two components: Minimax optimal rates. In COLT  pages 560–604  2014.

[5] Hanie Sedghi  Majid Janzamin  and Anima Anandkumar. Provable tensor methods for learning
mixtures of generalized linear models. In Artiﬁcial Intelligence and Statistics  pages 1223–1231 
2016.

[6] Arun T Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear regressions.
In Proceedings of the 30th International Conference on Machine Learning (ICML-13)  pages
1040–1048  2013.

[7] Kai Zhong  Prateek Jain  and Inderjit S Dhillon. Mixed linear regression with multiple compo-

nents. In Advances in Neural Information Processing Systems  pages 2190–2198  2016.

[8] Xinyang Yi  Constantine Caramanis  and Sujay Sanghavi. Solving a mixture of many ran-
dom linear equations by tensor decomposition and alternating minimization. arXiv preprint
arXiv:1608.05749  2016.

[9] Paul Hand and Babhru Joshi. A convex program for mixed linear regression with a recovery

guarantee for well-separated data. arXiv preprint arXiv:1612.06067  2016.

[10] Ian En-Hsu Yen  Xin Lin  Kai Zhong  Pradeep Ravikumar  and Inderjit Dhillon. A convex
exemplar-based approach to mad-bayes dirichlet process mixture models. In International
Conference on Machine Learning  pages 2418–2426  2015.

[11] Ian En-Hsu Yen  Xin Lin  Jiong Zhang  Pradeep Ravikumar  and Inderjit Dhillon. A convex
atomic-norm approach to multiple sequence alignment and motif discovery. In International
Conference on Machine Learning  pages 2272–2280  2016.

[12] Ian En-Hsu Yen  Wei-Chen Chang  Sung-En Chang  Arun Sai Suggula  Shou-De Lin  and
Pradeep. Ravikumar. Latent feature lasso. International Conference on Machine Learning 
2017.

[13] Venkat Chandrasekaran  Benjamin Recht  Pablo A Parrilo  and Alan S Willsky. The convex
geometry of linear inverse problems. Foundations of Computational mathematics  12(6):805–
849  2012.

[14] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society. Series B (Methodological)  pages 267–288  1996.

[15] Yurii Nesterov et al. Quality of semideﬁnite relaxation for nonconvex quadratic optimization.
Université Catholique de Louvain. Center for Operations Research and Econometrics [CORE] 
1997.

[16] Nicolas Boumal  Vlad Voroninski  and Afonso Bandeira. The non-convex burer-monteiro ap-
proach works on smooth semideﬁnite programs. In Advances in Neural Information Processing
Systems  pages 2757–2765  2016.

[17] Po-Wei Wang  Wei-Cheng Chang  and J Zico Kolter. The mixing method: coordinate descent

for low-rank semideﬁnite programming. arXiv preprint arXiv:1706.00476  2017.

9

,Ian En-Hsu Yen
Wei-Cheng Lee
Kai Zhong
Sung-En Chang
Pradeep Ravikumar
Shou-De Lin