2018,Breaking the Span Assumption Yields Fast Finite-Sum Minimization,In this paper  we show that SVRG and SARAH can be modified to be fundamentally faster than all of the other standard algorithms that minimize the sum of $n$ smooth functions  such as SAGA  SAG  SDCA  and SDCA without duality. Most finite sum algorithms follow what we call the ``span assumption'': Their updates are in the span of a sequence of component gradients chosen in a random IID fashion. In the big data regime  where the condition number $\kappa=O(n)$  the span assumption prevents algorithms from converging to an approximate solution of accuracy $\epsilon$ in less than $n\ln(1/\epsilon)$ iterations. SVRG and SARAH do not follow the span assumption since they are updated with a hybrid of full-gradient and component-gradient information. We show that because of this  they can be up to $\Omega(1+(\ln(n/\kappa))_+)$ times faster. In particular  to obtain an accuracy $\epsilon = 1/n^\alpha$ for $\kappa=n^\beta$ and $\alpha \beta\in(0 1)$  modified SVRG requires $O(n)$ iterations  whereas algorithms that follow the span assumption require $O(n\ln(n))$ iterations. Moreover  we present lower bound results that show this speedup is optimal  and provide analysis to help explain why this speedup exists. With the understanding that the span assumption is a point of weakness of finite sum algorithms  future work may purposefully exploit this to yield faster algorithms in the big data regime.,Breaking the Span Assumption Yields Fast

Finite-Sum Minimization∗

Robert Hannah†1  Yanli Liu‡1  Daniel O’Connor§2  and Wotao Yin¶1
1Department of Mathematics  University of California  Los Angeles

2Department of Mathematics  University of San Francisco

Abstract

In this paper  we show that SVRG and SARAH can be modiﬁed to be
fundamentally faster than all of the other standard algorithms that minimize
the sum of n smooth functions  such as SAGA  SAG  SDCA  and SDCA
without duality. Most ﬁnite sum algorithms follow what we call the “span
assumption”: Their updates are in the span of a sequence of component
gradients chosen in a random IID fashion. In the big data regime  where the
condition number κ = O(n)  the span assumption prevents algorithms from
converging to an approximate solution of accuracy  in less than n ln(1/)
iterations. SVRG and SARAH do not follow the span assumption since
they are updated with a hybrid of full-gradient and component-gradient
information. We show that because of this  they can be up to Ω(1 +
(ln(n/κ))+) times faster. In particular  to obtain an accuracy  = 1/nα for
κ = nβ and α  β ∈ (0  1)  modiﬁed SVRG requires O(n) iterations  whereas
algorithms that follow the span assumption require O(n ln(n)) iterations.
Moreover  we present lower bound results that show this speedup is optimal 
and provide analysis to help explain why this speedup exists. With the
understanding that the span assumption is a point of weakness of ﬁnite
sum algorithms  future work may purposefully exploit this to yield faster
algorithms in the big data regime.

Introduction

1
Finite sum minimization is an important class of optimization problem that appears in many
applications in machine learning and other areas. We consider the problem of ﬁnding an
approximation ˆx to the minimizer x∗ of functions F : Rd → R of the form:

nX

i=1

F(x) = f(x) + ψ(x) (cid:44) 1

n

fi(x) + ψ(x).

(1.1)

1720237  and ONR N000141712162.

We assume each function fi is smooth6  and possibly nonconvex; ψ is proper  closed  and
convex; and the sum F is strongly convex and smooth. It has become well-known that under
a variety of assumptions  functions of this form can be minimized much faster with variance
∗This work was supported in part by grants: AFOSR MURI FA9550-18-1-0502  NSF DMS-
†Corresponding author: RobertHannah89@gmail.com
‡yanli@math.ucla.edu
§daniel.v.oconnor@gmail.com
¶WotaoYin@math.ucla.edu
6A function f is L-smooth if it has an L-Lipschitz gradient ∇f

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

reduction (VR) algorithms that speciﬁcally exploit the ﬁnite-sum structure. When each fi
is µ-strongly convex and L-smooth  and ψ = 0  SAGA [1]  SAG [2]  Finito/Miso [3]  [4] 
SVRG [5]  SARAH [6]  SDCA [7]  and SDCA without duality [8] can ﬁnd a vector ˆx with
expected suboptimality E(f(ˆx) − f(x∗)) = O() with only O((n + L/µ) ln(1/)) calculations
of component gradients ∇fi(x). This can be up to n times faster than (full) gradient descent 
which takes O(nL/µ ln(1/)) gradients. These algorithms exhibit sublinear convergence for
non-strongly convex problems7. Various results also exist for nonzero convex ψ.
Accelerated VR algorithms have also been proposed. Katyusha [9] is a primal-only Nesterov-
accelerated VR algorithm that uses only component gradients. It is based on SVRG and
has complexity O((n +
nκ) ln(1/))) for condition number κ which is deﬁned as L/µ. In
[10]  the author devises an accelerated SAGA algorithm that attains the same complexity
using component proximal steps. In [11]  the author devises an accelerated primal-dual VR
algorithm. There also exist “catalyst” [12] accelerated methods [13]  [14]. However  catalyst
methods appear to have a logarithmic complexity penalty over Nesterov-accelerated methods 
a defect that researchers have been able to correct.
In [15]  authors show that a class of algorithms that includes SAGA  SAG  Finito (with
replacement)  Miso  SDCA without duality  etc. have complexity K() lower bounded by
nκ) ln(1/)) for problem dimension d ≥ 2K(). More precisely  the lower bound
Ω((n +
applies to algorithms that satisfy what we will call the span condition. That is

√

√

xk+1 ∈ x0 + span(cid:8)∇fi0

(cid:0)x0(cid:1) ∇fi1

(cid:0)x1(cid:1)  . . .  ∇fik

(cid:0)xk(cid:1)(cid:9)

(1.2)
for some ﬁxed IID random variable ik over the indices {1  . . .   n}. Later  [16] and [17] extend
lower bound results to algorithms that do not follow the span assumption: SDCA  SVRG 
SARAH  accelerated SAGA  etc.; but with a weaker lower bound of Ω(n +
nκ ln(1/)).
The diﬀerence in these two expressions was thought to be a proof artifact that would later
be resolved.
However we show a surprising result in Section 2  that SVRG  and SARAH can be funda-
mentally faster than methods that satisfy the span assumption  with the full gradient steps
playing a critical role in their speedup. More precisely  for κ = O(n)  SVRG and SARAH
can be modiﬁed to reach an accuracy of  in O((
1+(ln(n/κ))+ ) ln(1/)) gradient calculations8 
instead of the Θ(n ln(1/)) iterations required for algorithms that follow the span condition.
We also improve the lower bound of [17] to Ω(n + (
nκ) ln(1/)) in Section
2.1. That is  we show that the complexity K() of a very general class of algorithm that
includes all of the above algorithms satisﬁes the lower bound:

1+(ln(n/κ))+

√

√

+

n

n

(Ω(n +

Ω(n +

√

K() =

nκ ln(1/)) 

n

1+(ln(n/κ))+

ln(1/)) 

for n = O(κ) 
for κ = O(n).

(1.3)

Hence when κ = O(n) our modiﬁed SVRG has optimal complexity  and when n = O(κ) 
Katyusha is optimal.
SDCA doesn’t quite follow the span assumption. Also the dimension n of the dual space on
which the algorithm runs is inherently small in comparison to k  the number of iterations.
We complete the picture using diﬀerent arguments  by showing that its complexity is lower
bounded by Ω(n ln(1/)) in Section 2.2. Hence SDCA doesn’t attain this logarithmic speedup.
We leave the analysis of accelerated SAGA  accelerated SDCA  and other algorithms to
future work.
Our results identify a signiﬁcant obstacle to high performance when n (cid:29) κ. The speedup
that SVRG and SARAH can be modiﬁed to attain in this scenario is somewhat accidental
since their original purpose was to minimize memory overhead. However  with the knowledge
that this assumption is a point of weakness for VR algorithms  future work may more
purposefully exploit this to yield better speedups than SVRG and SARAH can currently
attain. Though the complexity of SVRG and SARAH can be made optimal to within a

7SDCA must be modiﬁed however with a dummy regularizer.
8We deﬁne (a)+ as max{a  0} for a ∈ R.

2

√

√

constant factor  this factor is somewhat large  and could potentially be reduced substantially.
Though it is unclear how much of a speedup is possible.
Having n (cid:29) κ  which has been referred to as the “big data condition”  is rather common 
especially in regularized empirical risk minimization (ERM). For instance [2] remarks that
κ =
n is a nearly optimal choice for regularization for empirical risk minimization in the
low dimensional setting. In the high-dimensional setting  the authors of [2] claim there is no
analysis that they are aware that doesn’t imply that we should set the regularization term
to ensure n = O(κ). In [18]  authors consider regularized ERM for minimizing a stochastic
objective. They argue that the optimal choice of regularization parameter λ corresponds to
κ =
Hence our results have wide application.
In the settings described above  we have the
following corollary that implies a complexity improvement from O(n ln(n)) to O(n). This
will follow from Corollary 2 ahead.
Corollary 1. To obtain accuracy  = 1/nα for κ = nβ and some α  β ∈ (0  1)  modiﬁed
SVRG requires O(n) iterations  whereas algorithms that follow the span assumption require
O(n ln(n)) iterations [11] for suﬃciently large problem dimension d.

n. [19] considers regularized SVM with κ = nβ for β < 1.

For large-scale problems  this ln(n) factor can be rather large: For instance in the KDD
Cup 2012 dataset (n = 149  639  105 and ln(n) ≈ 18)  Criteo’s Terabyte Click Logs (n =
4  195  197  692 and ln(n) ≈ 22)  etc. Non-public internal company datasets can be far larger 
with n potentially larger than 1015.
We also analyze Prox-SVRG in the case where fi are smooth and potentially nonconvex  but
the sum F is strongly convex. We build on the work of [20]  which proves state-of-the-art
complexity bounds for this setting  and show that we can attain a similar logarithmic speedup
without modiﬁcation. Lower bounds for this context are lacking  so it is unclear if this result
can be further improved.

2 Optimal Convex SVRG

In this section  we show that the Prox-SVRG algorithm proposed in [21] for problem (1.1)
can be sped up by a factor of Ω(1 + (ln(n/κ))+) when κ = O(n). A similar speedup is clearly
possible for vanilla SVRG and SARAH  which have similar rate expressions. We then reﬁne
the lower bound analysis of [17] to show that the complexity is optimal9 when κ = O(n).
Katyusha is optimal in the other scenario when n = O(κ) by [22].
Assumption 1.

fi is Li−Lipschitz diﬀerentiable for i = 1  2  ...  n. That is 

k∇fi(x) − ∇fi(y)k ≤ Likx − yk

for all x  y ∈ Rd.

f is L−Lipschitz diﬀerentiable. F is µ−strongly convex. That is 

F(y) ≥ F(x) + h ˜∇F(x)  y − xi + µ

2ky − xk2

for all x  y ∈ Rd and ˜∇F(x) ∈ ∂F(x).

Assumption 2.

fi is convex for i = 1  2  ...  n; and ψ is proper  closed  and convex.

9I.e. the complexity cannot be improved among a very broad class of ﬁnite-sum algorithms.

3

i=1 fi(x)  initial vector x0  step size η > 0  number of epochs K 

n

Pn

Algorithm 1 Prox-SVRG(F  x0  η  m)
Input: F(x) = ψ(x) + 1
probability distribution P = {p1  . . .   pn}
Output: vector xK
1: M k ∼ Geom( 1
m);
2: for k ← 0  ...  K − 1 do
w0 ← xk; µ ← ∇f(xk);
3:
for t ← 0  ...  M k do
4:
5:
6:
7:
8:
9:
10: end for

end for
xk+1 ← wM+1;

˜∇t = µ +(cid:0)∇fit(wt) − ∇fit(w0)(cid:1)/(npit);

pick it ∈ {1  2  ...  n} ∼ P randomly;
wt+1 = arg miny∈Rd{ψ(y) + 1

2ηky − wtk2 + h ˜∇t  yi};

We make Assumption 1 throughout the paper  and Assumption 2 in this section. Recall
the Prox-SVRG algorithm of [21]  which we reproduce in Algorithm 1. The algorithm is
organized into a series of K epochs of size M k  where M k is a geometric random variable
with success probability 1/m. Hence epochs have an expected length of m. At the start
of each epoch  a snapshot µ = ∇f(xk) of the gradient is taken. Then for M k steps  a
random component gradient ∇itf(wt) is calculated  for an IID random variable it with ﬁxed
distribution P given by P[it = i] = pi. This component gradient is used to calculate an
unbiased estimate ˜∇t of the true gradient ∇f(wt). Each time  this estimate is then used to
perform a proximal-gradient-like step with step size η. At the end of these M k steps  a new
epoch of size M k+1 is started  and the process continues.
We ﬁrst recall a modiﬁed Theorem 1 from [21]. The diﬀerence is that in [21]  the authors
used an epoch length of m  whereas we use a random epoch length M k with expectation
EM k = m. The proof and theorem statement only require only trivial modiﬁcations to
account for this. This modiﬁcation is only to unify the diﬀerent version of SVRG in [21] and
[20]  and makes no diﬀerence to the result.
It becomes useful to deﬁne the eﬀective Lipschitz constant LQ = maxiLi/(pin)  and the
eﬀective condition number κQ = LQ/µ for this algorithm. These reduce to the standard
Lipschitz constant L  and the standard condition number κ in the standard uniform scenario
where Li = L ∀i  and P is uniform.
Theorem 1. Complexity of Prox-SVRG. Let Assumptions 1 and 2 hold. Then Prox-
SVRG deﬁned in Algorithm 1 satisﬁes

E[F(xk) − F(x∗)] ≤ ρk[F(x0) − F(x∗)]

(2.1)

for ρ = 1 + µη(1 + 4mLQη)
µηm(1 − 4LQη)

.

(2.2)

In previous work  the optimal parameters were not really explored in much detail. In the
original paper [5]  the author suggest η = 0.1/L  which results in linear convergence rate
1/4 ≤ ρ ≤ 1/2 for m ≥ 50κ. In [21]  authors also suggest η = 0.1/L for m = 100κ  which
yields ρ ≈ 5/6. However  they observe that η = 0.01/L works nearly as well. In [6]  authors
obtain a similar rate expression for SARAH and suggest η = 0.5/L and m = 4.5κ which
yields ρ ≈ 7/9. In the following corollary  we propose a choice of η and m that leads to an
optimal complexity to within a constant factor for κ = O(n). This result helps explain why
the optimal step size observed in prior work appears to be much smaller than the “standard”
gradient descent step of 1/L.
Corollary 2. Let the conditions of Theorem 1 hold  and let m = n + 121κQ  and η =
Qm− 1
121+(n/κQ) 
κ
and hence it needs:

2 /(2LQ). The Prox-SVRG in Algorithm 1 has convergence rate ρ ≤q 100

1
2

!

!

ln 1



+ n + κQ

(2.3)

 

K() = O

n

1 + (ln( n
κQ

+ κQ

))+

4

iterations in expectation to obtain a point xK() such that E(cid:2)f(cid:0)xK()(cid:1) − f(x∗)(cid:3) < .
convergence rate ρ ∼pκQ/n → 0  and complexity O(cid:16)

This result is proven in Appendix A. The n + κQ term is needed because we assume that at
least one epoch is completed. For n = O(κQ)  we have a similar convergence rate (ρ ≈ 10
11)
and complexity to algorithms that follow the span assumption. For n (cid:29) κQ  we have a

1+(ln(n/κ)) ln(1/)(cid:17)  which can can

be much better than n ln(1/). See also Corollary 1. The corresponding result and proof for
SARAH is nearly identical  and we do not include this.
In order to obtain this speedup  some estimate of the condition number must be known.
However this is often not a problem. In the case of a regularization term for empirical risk
minimization  the strong convexity modulus is hand-picked based on the number of examples
n. In other cases  we can simply tune an estimate of the parameter κ with the assurance
that this can yield a logarithmic speedup.
Remark 1.
P = {p1  p2  ...  pn} on {1  2  ...  n} is pi =

In Theorem 1 and Corollary 2  the optimal choice of the probability distribution

for i = 1  2  ...  n  and LQ =

Pn

LiPn

i=1 Li
n

.

n

i=1 Lj

2.1 Optimality
The major diﬀerence between SAGA  SAG  Miso/Finito  and SDCA without duality  and
SVRG and SARAH  is that the former satisfy what we call the span condition (1.2). SVRG 
and SARAH  do not  since they also involve full-gradient steps. We refer to SVRG  and
SARAH as hybrid methods  since they use full-gradient and partial gradient information
to calculate their iterations. We assume for simplicity that Li = L  for all i  and that ψ = 0.
We now present a rewording of Corollary 3 from [11].
Corollary 3. For every  and randomized algorithm on (1.1) that follows the span as-
sumption  there are a dimension d  and L-smooth  µ-strongly convex functions fi on Rd
κn) ln(1/)) steps to reach sub-optimality
such that the algorithm takes at least Ω((n +

√

Ef(cid:0)xk(cid:1) − f(x∗) < .

The above algorithms that satisfy the span condition all have known upper complexity
bounds of O((n + κ) ln(1/))  and hence for κ = O(n) we have a sharp convergence rate.
However  it turns out that the span assumption is an obstacle to faster convergence when
n (cid:29) κ (at least for suﬃciently high dimension). In the following theorem  we improve10 the
analysis of [17]  to show that the complexity of SVRG obtained in Corollary 2 is optimal
to within a constant factor without fundamentally diﬀerent assumptions on the class of
algorithms that are allowed. Clearly this also applies to SARAH. The theorem is actually far
more general  and applies to a general class of algorithms called p−CLI oblivious algorithms
introduced in [17]. This class contains all VR algorithms mentioned in this paper.
In
Appendix B  we recall the deﬁnition of p−CLI oblivious algorithms  as well as providing the
proof of a more general version of Theorem 2.
Theorem 2. Lower complexity bound of Prox-SVRG and SARAH. For all µ  L 
there exist L-smooth  and µ-strongly convex functions fi such that at least11

(2.4)

(cid:18)(cid:18)

K() = ˜Ω

n

1 + (ln( n

κ))+

√

+

nκ

ln 1



+ n

(cid:19)

(cid:19)

iterations are needed for SVRG or SARAH to obtain expected suboptimality
E[f(K()) − f(x∗)] < .

2.2 SDCA
To complete the picture  in the following proposition  which we prove in Appendix C  we show
that SDCA has a complexity lower bound of Ω(n ln(1/)). Hence it attains no logarithmic

10Speciﬁcally  we improve the analysis of Theorem 2 from this paper.
11We absorb some smaller low-accuracy terms (high ) as is common practice. Exact lower bound

expressions appear in the proof.

5

It does so with coordinate

nX

speedup. SDCA aims to solve the following problem:

F(x) = 1

fi(x) = 1

n

min
x∈Rd
where each yi ∈ Rd  φi
minimization steps on the corresponding dual problem:
2k 1
i (−αi) + λ
φ∗

: R → R is convex and smooth.

D(α) := 1

nX

min
α∈Rn

i=1

i=1

λn

n

nX

nX

(cid:0)φi(xT yi) + λ

2kxk2(cid:1) 

αiyik2 

n

i=1

i=1

i (u) := maxz

Here φ∗
uniform random variables on {1  ...  n}. SDCA updates a dual point αk  while maintaining a
corresponding primal vector xk. SDCA can be written as:

(cid:0)zu − φi(z)(cid:1) is the convex conjugate of φi. Let ik be an IID sequence of
(cid:26)αk
αk+1
i =
nX
xk+1 = 1

i  
arg minz D(αk1  ...  αk

if i 6= ik 
if i = ik 

n) 
i+1  ...  αk

i−1  z  αk

αk+1

(2.5)

(2.6)

yi 

i

nλ

i=1

Since SDCA doesn’t follow the span assumption  and the number of iterations k is much
greater than the dual problem dimension n  diﬀerent arguments to the ones used in [11]
must be used. Motivated by the analysis in [23]  which only proves a lower bound for dual
suboptimality  we have the following lower complexity bound  which matches the upper
complexity bound given in [7] for κ = O(n).
Proposition 1. Lower complexity bound of SDCA. For all µ  L  n > 2  there exist n
functions fi that are L−smooth  and µ−strongly convex such that

(2.7)
iterations are needed for SDCA to obtain expected suboptimality E[F(xK()) − F(x∗)] ≤ .



K() = Ω(cid:0)n ln 1

(cid:1)

3 Why are hybrid methods faster?
In this section  we explain why SVRG and SARAH  which are a hybrid between full-gradient
and VR methods  are fundamentally faster than other VR algorithms. We consider the
performance of these algorithms on a variation of the adversarial function example from
[11]  [24]. The key insight is that the span condition makes this adversarial example hard to
minimize  but that the full gradient steps of SVRG and SARAH make it easy when n (cid:29) κ.
We conduct the analysis in ‘2  for simplicity12  since the argument readily applies to Rd.
Consider the function introduced in [24] that we introduce for the case n = 1:

(cid:18)1
2hx  Axi − he1  xi

(cid:19)

  for A =

(cid:0)q1  q2

The function φ(x) + 1

φ(x) = L − σ
4
1  . . .(cid:1) for q1 =(cid:0)κ1/2 − 1(cid:1)/(cid:0)κ1/2 + 1(cid:1). We assume that x0 = 0 with no loss in general-
2 σkxk2 is L-smooth and σ-strongly convex. Its minimizer x∗ is given by
ity. Let N(x) be position of the last nonzero in the vector. E.g. N(0  2  3  0  4  0  0  0  . . .) = 5.
(cid:1)
/(cid:0)1 − q2
N(x) is a control on how close x can be to the solution. If N(x) = N  then clearly:

  . . .(cid:1)(cid:13)(cid:13)2 = q2N+2

kx − x∗k2 ≥

...
...

  qN+2

1  q3

min

1

1

1

2 −1
−1
2
...

y s.t. N(y)=N

ky − x∗k2 =(cid:13)(cid:13)(cid:0)0  . . .   0  qN+1
i=1 withP∞

1

12This is the Hilbert space of sequences (xi)∞

i=1 x2

i < ∞



2 −1
−1



6

2 σkxk2(cid:17)(y) =
4 A+σI  the last nonzero N(cid:0)xk(cid:1) of xk can only increase by 1 per iteration by any algorithm
Hence since we have N(cid:0)x0(cid:1) = 0  we have(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2

Because of the tridiagonal pattern of nonzeros in the Hessian ∇2
L−σ
that satisﬁes that span condition (e.g. gradient descent  accelerated gradient descent  etc.).

(cid:16)
/(cid:13)(cid:13)x0 − x∗(cid:13)(cid:13)2 ≥ q2k1 .

φ(x) + 1

x

For the case n > 1  let the solution vector x = (x1  . . .   xn) be split into n coordinate blocks 
and hence deﬁne:

2 σkxk2(cid:1)
(cid:0)φ(xi) + 1
(cid:18)1
(cid:19)
(cid:18) L − σ
2hxi  Axii − he1  xii
(cid:18)(L − σ + σn) − σn

4

+ 1

2(σn)kxik2(cid:19)
(cid:19)

f(x) =

i=1

nX
nX
nX

i=1

i=1

(3.1)

.

(3.2)

/

4

n  q3

=

=

i=1

nq2

+ 1

n/(1 − q2
n)

≥ nX

kx − x∗k2
kx∗k2 =

(cid:13)(cid:13)xi −(cid:0)qn  q2

controls how close x can be to x∗:

n + 1(cid:1)1/2 − 1(cid:17)

(cid:18)1
2hxi  Axii − he1  xii

f is clearly the sum of n convex L-smooth functions φ(xi) + 1
strongly convex.

2 σkxk2  that are σ-
(3.2) shows it is σn-strongly convex and L − σ + σn-smooth with

2(σn)kxik2(cid:19)
respect to coordinate xi. Hence the minimizer is given by xi = (cid:0)qn  q2
qn =(cid:16)(cid:0) κ−1

n  . . .(cid:1) for
(cid:16)(cid:0) κ−1
n + 1(cid:1)1/2 + 1(cid:17) for all i. Similar to before  (N(x1)  . . .   N(xn))
Pn

n  . . .(cid:1)(cid:13)(cid:13)2
satisfy the span assumption  we have N(cid:0)xk
(cid:1) ≤ Ik i. If we assume that ik is uniform  then
nX
nX
/(cid:13)(cid:13)x0 − x∗(cid:13)(cid:13)2 ≥ E
n =(cid:0)1 − n−1(cid:0)1 − q2
(cid:1)(cid:1)k
 
 (cid:18) κ − 1
(cid:19)1/2
≥(cid:0)1 − 2n−1(cid:1)k

Let IK i be the number of times that ik = i for k = 0  1  . . .   K − 1. For algorithms that

IK i is a binomial random variable of probability 1/n and size k. Hence:

for n ≥ κ. the second equality in (3.3) follows from the fact that Ii k is a binomial random

variable. Hence after 1 epoch  E(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2 decreases by a factor of at most ≈ e2  whereas

for SVRG it decreases by at least a factor of ∼ (n/κ)1/2  which is (cid:29) e2 for n (cid:29) κ. To help
understand why  consider trying the above analysis on SVRG for 1 epoch of size n. Because
of the full-gradient step  we actually have N(wn

i ) ≤ 1 + In i  and hence:

E(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2

1 − 4n−1/

!!k

q2N(xi)

n

/n

2Ik i
q
n

/n

/n ≥ E

2N(xk
i )

i=1
2Ik i

= Eq

n

+ 1

i

q
n

(3.3)

≥

n

+ 1

i=1

i=1

Ekwn − x∗k2

/(cid:13)(cid:13)x0 − x∗(cid:13)(cid:13)2 ≥ E

nX

i=1

(cid:0)1 − 2n−1(cid:1)n ≈

q2(In 1+1)

n

≥ q2

n

What it comes down to is that when n (cid:29) κ  we have EPn

Hence attempting the above results in a much weaker lower bound.
2Ii k
n

/n.
The interpretation is that for this objective  the progress towards a solution is limited by
the component function fi that is minimized the least. The full gradient step ensures that
at least some progress is made toward minimizing every fi. For algorithms that follow
the span assumption  there will invariably be many indices i for which no gradient ∇fi
is calculated  and hence xk
i can make no progress towards the minimum. This may be
related to the observation that sampling without replacement can often speed up randomized

2EIi k
n

i=1 q

i=1 q

κ − 1
n

(cid:19)2
(cid:18)1
/n (cid:29) EPn

4

e−2

7

Pn

algorithms. However  on the other hand  it is well known that full gradient methods fail to
achieve a good convergence rate for other objectives with the same parameters µ  L  n (e.g.
2 µkxk2). Hence we conclude that it is because SVRG is a hybrid
f(x) = 1
method  that combines both full-gradient and VR elements  that it is able to outperform
both VR and full-gradient algorithms.

i=1 φ(x) + 1

n

4 Prox-SVRG for strongly convex sum of smooth nonconvex

function

In this section we show that this logarithmic speedup is still possible if we relax Assumption
2: that each fi is convex. By Assumption 1  the functions fi are still smooth  though possibly
nonconvex. The sum F though is strongly convex  and smooth. This is based on the analysis
of Prox-SVRG found in [20]. The proof of Theorem 3 can be found in Appendix D.

Theorem 3. Under Assumption 1  let x∗ = arg minx F(x)  L = (Pn

L2
n2pi

i

) 1
2   κ = L

µ   and

i=1
2}. Then the Prox-SVRG in Algorithm 1 satisﬁes
) 1

η = 1

L   ( 1

2 min{ 1
E[F(xk) − F(x∗)] ≤ O(ρk)[F(x0) − F(x∗)] 

m

L

2

(4.2)
Hence for m = min{n  2}  in order to obtain an -optimal solution in terms of function value 
the SVRG in Algorithm 1 needs at most

for ρ =

(4.1)

.

1
1 + 1
2 mηµ

(cid:1) + 2n

K = O(cid:0)(

√

¯L
µ

) ln 1



n

n

n

(4.3)

+ κ +

2 )1/2)

ln (1 + n

ln(1 + ( nµ2
4L

4κ) +
gradient evaluations in expectation.
K = O(cid:0)(n + κ +
The complexity of nonconvex SVRG using the original analysis of [9] would have been

(4.4)
Hence we have obtained a similar logarithmic speedup as we obtained in Corollary 2. There
are no known nontrivial lower bounds in this regime  and so it is not immediately clear
whether our complexity is optimal.
Remark 2.
{p1  p2  ...  pn} on {1  2  ...  n} is pi =

In Theorem 3  the optimal choice of the probability distribution P =

for i = 1  2  ...  n  and L = (

Pn

) ln 1

iPn

¯L
µ

) 1
2 .

i=1 L2
n

(cid:1)

√

n



i

L2
i=1 L2

j

2.

1
2n

2 + λ

minimize

kAx − bk2

5 Experiments
In this section we compare the performance of SVRG  and SARAH to SAGA to verify our
conclusions. We solve the regularized least squares problem
2kxk2

(5.1)
The matrix A and vector b are generated randomly with entries uniformly distributed between
0 and 1. In this experiment  A has n = 16000 rows and 20 columns. The parameter λ
is chosen to control the condition number κ = L/µ of the problem. Figure 5.1 compares
SAGA  SVRG  and SARAH for three instances of problem (5.1) with conditions numbers
κ = 5  κ = 10  and κ = 20. In order to provide a fair comparison  step sizes were tuned
individually for each algorithm and each problem instance.
There does appear to be a small but noticeable eﬀect when n/κ is large. In all our experiments 
SAGA appears to converge very quickly initially  but slows signiﬁcantly after a few iterations.
To compensate for this  we compare the convergence speed of the three algorithms after the
ﬁrst few iterations in terms of decibels per epoch13. For κ = 5  we obtain convergence speeds
13Decibels are a logarithmic scale. 10 decibels corresponds to a 10-fold increase  100 decibels
corresponds to a 100-fold increase. This is the natural way to compare speeds for linearly converging
error.

8

(a) κ = 5

(b) κ = 10

(c) κ = 20

Figure 5.1: Comparison of SAGA  SVRG  and SARAH for various values of the condition number κ.

of 2.1  4.9  6.3 decibels/epoch for SAGA  SARAH  and SVRG respectively. For κ = 10 these
values are 2.3  5.0  and 6.2 respectively; and for κ = 20 these values are 2.1  4.3  and 6.0. So
SVRG converges above 3× faster than SAGA in the long term  even though SVRG iterations
are twice as expensive as SAGA’s.
It is not yet clear whether this eﬀect will have practical impact. However we see a few
obvious future directions. Firstly  SVRG and SARAH were never intentionally designed to
exploit this logarithmic speedup. It’s possible that designing an algorithm with this in mind
will yield greater speedup. Secondly  it should be investigating whether the initial speed
burst of SAGA can be incorporated into an SVRG-like algorithm. This will make it more
competitive. Thirdly  SVRG and SARAH have iterations that are twice as expensive as
SAGA’s because of the full gradient steps. It should be investigated whether there is a way
of retaining this logarithmic speedup while reducing the iteration cost. Perhaps large batch
gradients instead of full gradient will be suﬃcient to yield this speedup  and avoid the high
cost of full gradient steps.

References
[1] A. Defazio  F. Bach  and S. Lacoste-Julien  “SAGA: A Fast Incremental Gradient
Method With Support for Non-Strongly Convex Composite Objectives ” in Advances
in Neural Information Processing Systems 27  2014  pp. 1646–1654. [Online]. Available:
http://papers.nips.cc/paper/5258- saga- a- fast- incremental- gradient-
method-with-support-for-non-strongly-convex-composite-objectives.pdf.

[2] N. L. Roux  M. Schmidt  and F. R. Bach  “A Stochastic Gradient Method with an
Exponential Convergence _Rate for Finite Training Sets ” in Advances in Neural
Information Processing Systems 25  Curran Associates  Inc.  2012  pp. 2663–2671.
[Online]. Available: http://papers.nips.cc/paper/4633-a-stochastic-gradient-
method-with-an-exponential-convergence-_rate-for-finite-training-sets.
pdf.

[3] A. Defazio  J. Domke  and Caetano  “Finito: A faster  permutable incremental gradient
method for big data problems ” in International Conference on Machine Learning 
Jan. 2014  pp. 1125–1133. [Online]. Available: http://proceedings.mlr.press/v32/
defazio14.html.

[4] J. Mairal  “Optimization with First-Order Surrogate Functions ” in International
Conference on Machine Learning  Feb. 2013  pp. 783–791. [Online]. Available: http:
//proceedings.mlr.press/v28/mairal13.html.

[5] R. Johnson and T. Zhang  “Accelerating stochastic gradient descent using predic-
tive variance reduction ” in Advances in Neural Information Processing Systems 26 
2013  pp. 315–323. [Online]. Available: http : / / papers . nips . cc / paper / 4937 -
accelerating - stochastic - gradient - descent - using - predictive - variance -
reduction.pdf.

[6] L. M. Nguyen  J. Liu  K. Scheinberg  and M. Takáč  “SARAH: A Novel Method for
Machine Learning Problems Using Stochastic Recursive Gradient ” arXiv:1703.00102
[cs  math  stat]  Feb. 2017. [Online]. Available: http://arxiv.org/abs/1703.00102.

9

[7] S. Shalev-Shwartz and T. Zhang  “Stochastic dual coordinate ascent methods for
regularized loss ” J. Mach. Learn. Res.  vol. 14  no. 1  pp. 567–599  Feb. 2013. [Online].
Available: http://dl.acm.org/citation.cfm?id=2502581.2502598.

[8] S. Shalev-Shwartz  “SDCA without Duality ” arXiv:1502.06177  Feb. 2015. [Online].

Available: http://arxiv.org/abs/1502.06177.

[9] Z. Allen-Zhu  “Katyusha: The First Direct Acceleration of Stochastic Gradient Methods ”
in Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing 
ser. STOC 2017  New York  NY  USA: ACM  2017  pp. 1200–1205. [Online]. Available:
http://doi.acm.org/10.1145/3055399.3055448.

[10] A. Defazio  “A simple practical accelerated method for ﬁnite sums ” in Advances in

Neural Information Processing Systems  2016  pp. 676–684.

[11] G. Lan and Y. Zhou  “An optimal randomized incremental gradient method ” Mathemat-
ical Programming  pp. 1–49  Jun. 2017. [Online]. Available: https://link.springer.
com/article/10.1007/s10107-017-1173-0.

[12] H. Lin  J. Mairal  and Z. Harchaoui  “A Universal Catalyst for First-Order Optimiza-
tion ” arXiv:1506.02186  Jun. 2015. [Online]. Available: http://arxiv.org/abs/1506.
02186.

[13] Q. Lin  Z. Lu  and L. Xiao  “An Accelerated Proximal Coordinate Gradient Method
and its Application to Regularized Empirical Risk Minimization ” arXiv:1407.1296 
Jul. 2014. [Online]. Available: http://arxiv.org/abs/1407.1296.

[14] S. Shalev-Shwartz and T. Zhang  “Accelerated proximal stochastic dual coordinate
ascent for regularized loss minimization ” Mathematical Programming  vol. 155  no. 1-2 
pp. 105–145  Jan. 2016. [Online]. Available: http://link.springer.com/article/
10.1007/s10107-014-0839-0.

[15] G. Lan and Y. Zhou  “An optimal randomized incremental gradient method ”
arXiv:1507.02000  Jul. 2015. [Online]. Available: http://arxiv.org/abs/1507.02000.
[16] B. Woodworth and N. Srebro  “Tight Complexity Bounds for Optimizing Composite
Objectives ” arXiv:1605.08003  May 2016. [Online]. Available: http://arxiv.org/
abs/1605.08003.

[17] Y. Arjevani and O. Shamir  “Dimension-Free Iteration Complexity of Finite Sum
Optimization Problems ” arXiv:1606.09333 [cs  math]  Jun. 2016. [Online]. Available:
http://arxiv.org/abs/1606.09333.

[18] K. Sridharan  S. Shalev-shwartz  and N. Srebro  “Fast Rates for Regularized Objectives ”
in Advances in Neural Information Processing Systems 21  Curran Associates  Inc. 
2009  pp. 1545–1552. [Online]. Available: http://papers.nips.cc/paper/3400-fast-
rates-for-regularized-objectives.pdf.

[19] M. Eberts and I. Steinwart  “Optimal learning rates for least squares SVMs using
Gaussian kernels ” in Advances in Neural Information Processing Systems 24  Curran
Associates  Inc.  2011  pp. 1539–1547. [Online]. Available: http://papers.nips.
cc/paper/4216- optimal- learning- rates- for- least- squares- svms- using-
gaussian-kernels.pdf.

[20] Z. Allen-Zhu  “Katyusha X: Practical Momentum Method for Stochastic Sum-of-
Nonconvex Optimization ” arXiv:1802.03866  Feb. 2018. [Online]. Available: http:
//arxiv.org/abs/1802.03866.

[21] L. Xiao and T. Zhang  “A Proximal Stochastic Gradient Method with Progressive
Variance Reduction ” arXiv:1403.4699  Mar. 2014. [Online]. Available: http://arxiv.
org/abs/1403.4699.

[22] Y. Arjevani and O. Shamir  “On the Iteration Complexity of Oblivious First-Order
Optimization Algorithms ” arXiv:1605.03529 [cs  math]  May 2016. [Online]. Available:
http://arxiv.org/abs/1605.03529.

[23] Y. Arjevani  “On Lower and Upper Bounds in Smooth Strongly Convex Optimization
- A Uniﬁed Approach via Linear Iterative Methods ” arXiv:1410.6387  Oct. 2014.
[Online]. Available: http://arxiv.org/abs/1410.6387.

[24] Y. Nesterov  Introductory Lectures on Convex Optimization: A Basic Course. Springer
Science & Business Media  Dec. 2013. [Online]. Available: https://dl.acm.org/
citation.cfm?id=2670022.

10

,Amit Daniely
Roy Frostig
Yoram Singer
Robert Hannah
Yanli Liu
Daniel O'Connor
Wotao Yin