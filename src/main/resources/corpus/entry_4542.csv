2017,Random Permutation Online Isotonic Regression,We revisit isotonic regression on linear orders  the problem of fitting monotonic functions to best explain the data  in an online setting. It was previously shown that online isotonic regression is unlearnable in a fully adversarial model  which lead to its study in the fixed design model. Here  we instead develop the more practical random permutation model. We show that the regret is bounded above by the excess leave-one-out loss for which we develop efficient algorithms and matching lower bounds. We also analyze the class of simple and popular forward algorithms and recommend where to look for  algorithms for online isotonic regression on partial orders.,Random Permutation Online Isotonic Regression

Wojciech Kotłowski

Pozna´n University of Technology

Poland

Wouter M. Koolen

Centrum Wiskunde & Informatica

Amsterdam  The Netherlands

wkotlowski@cs.put.poznan.pl

wmkoolen@cwi.nl

Alan Malek

MIT

Cambridge  MA
amalek@mit.edu

Abstract

We revisit isotonic regression on linear orders  the problem of ﬁtting monotonic
functions to best explain the data  in an online setting. It was previously shown that
online isotonic regression is unlearnable in a fully adversarial model  which lead to
its study in the ﬁxed design model. Here  we instead develop the more practical
random permutation model. We show that the regret is bounded above by the
excess leave-one-out loss for which we develop efﬁcient algorithms and matching
lower bounds. We also analyze the class of simple and popular forward algorithms
and recommend where to look for algorithms for online isotonic regression on
partial orders.

Introduction

1
A function f : R → R is called isotonic (non-decreasing) if x ≤ y implies f (x) ≤ f (y). Isotonic
functions model monotonic relationships between input and output variables  like those between
drug dose and response [25] or lymph node condition and survival time [24]. The problem of
isotonic regression is to ﬁnd the isotonic function that best explains a given data set or population
distribution. The isotonic regression problem has been extensively studied in statistics [1  24]  which
resulted in efﬁcient optimization algorithms for ﬁtting isotonic functions to the data [7  16] and sharp
convergence rates of estimation under various model assumptions [26  29].
In online learning problems  the data arrive sequentially  and the learner is tasked with predicting
each subsequent data point as it arrives [6]. In online isotonic regression  the natural goal is to predict
the incoming data points as well as the best isotonic function in hindsight. Speciﬁcally  for time steps

t = 1  . . .   T   the learner observes an instance xi ∈ R  makes a prediction(cid:98)yi of the true label yi 
evaluate a prediction(cid:98)yi by its squared loss ((cid:98)yi − yi)2. The quality of an algorithm is measured by its
regret (cid:80)T

which is assumed to lie in [0  1]. There is no restriction that the labels or predictions be isotonic. We

T is the loss of the best isotonic function on the entire data

t=1((cid:98)yi − yi)2 − L∗

T   where L∗

sequence.
Isotonic regression is nonparametric: the number of parameters grows linearly with the number of
data points. It is thus natural to ask whether there are efﬁcient  provably low regret algorithms for
online isotonic regression. As of yet  the picture is still very incomplete in the online setting. The
ﬁrst online results were obtained in the recent paper [14] which considered linearly ordered domains
in the adversarial ﬁxed design model  i.e. a model in which all the inputs x1  . . .   xT are given to the
learner before the start of prediction. The authors show that  due to the nonparametric nature of the
problem  many textbook online learning algorithms fail to learn at all (including Online Gradient
Descent  Follow the Leader and Exponential Weights) in the sense that their worst-case regret grows
linearly with the number of data points. They prove a Ω(T 1
3 ) worst case regret lower bound  and
develop a matching algorithm that achieves the optimal ˜O(T 1
3 ) regret. Unfortunately  the ﬁxed design
assumption is often unrealistic. This leads us to our main question: Can we design methods for online
isotonic regression that are practical (do not hinge on ﬁxed design)?

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Our contributions Our long-term goal is to design practical and efﬁcient methods for online
isotonic regression  and in this work we move beyond the ﬁxed design model and study algorithms
that do not depend on future instances. Unfortunately  the completely adversarial design model (in
which the instances are selected by an adaptive adversary) is impossibly hard: every learner can
suffer linear regret in this model [14]. So in order to drop the ﬁxed design assumption  we need to
constrain the adversary in some other way. In this paper we consider the natural random permutation
model  in which all T instances and labels are chosen adversarially before the game begins but then
are presented to the learner in a random order.
This model corresponds with the intuition that the data gathering process (which ﬁxes the order) is
independent of the underlying data generation mechanism (which ﬁxes instances and labels). We
will show that learning is possible in the random permutation model (in fact we present a reduction
showing that it is not harder than adversarial ﬁxed design) by proving an ˜O(T 1
3 ) upper bound on
regret for an online-to-batch conversion of the optimal ﬁxed design algorithm from [14] (Section 3).
Our main tool for analyzing the random permutation model is the leave-one-out loss  drawing
interesting connections with cross-validation and calibration. The leave-one-out loss on a set of t
labeled instances is the error of the learner predicting the i-th label after seeing all remaining t − 1
labels  averaged uniformly over i = 1  . . .   t. We begin by proving a general correspondence between
regret and leave-one-out loss for the random permutation model in Section 2.1  which allows us to
use excess leave-one-out loss as a proxy for regret. We then describe a version of online-to-batch
conversion that relates the ﬁxed design model with the random permutation model  resulting in an
algorithm that attains the optimal ˜O(T 1
Section 4 then turns to the computationally efﬁcient and natural class of forward algorithms that
use an ofﬂine optimization oracle to form their prediction. This class contains most common online
isotonic regression algorithms. We then show a O(T 1
2 ) upper bound on the regret for the entire class 
which improves to O(T 1
3 ) for the well-speciﬁed case where the data are in fact generated from an
isotonic function plus i.i.d. noise (the most common model in the statistics literature).
While forward algorithms match the lower bound for the well-speciﬁed case  there is a factor T 1
6 gap
in the random permutation case. Section 4.6 proposes a new algorithm that calls a weighted ofﬂine
oracle with a large weight on the current instance. This algorithm can be efﬁciently computed via
[16]. We prove necessary bounds on the weight.

3 ) regret.

Related work Ofﬂine isotonic regression has been extensively studied in statistics starting from
work by [1  4]. Applications range across statistics  biology  medicine  psychology  etc. [24  15  25 
22  17]. In statistics  isotonic regression is studied in generative models [26  3  29]. In machine
learning  isotonic regression is used for calibrating class probability estimates [28  21  18  20  27] 
ROC analysis [8]  training Generalized Linear Models and Single Index Models[12  11]  data cleaning
[13]  and ranking [19]. Fast algorithms for partial ordering are developed in [16].
In the online setting  [5] bound the minimax regret for monotone predictors under logarithmic loss
and [23  10] study online nonparametric regression in general. Efﬁcient algorithms and worst-cases
regret bounds for ﬁxed design online isotonic regression are studied in [14]. Finally  the relation
between regret and leave-one-out loss was pioneered by [9] for linear regression.

2 Problem Setup
Given a ﬁnite set of instances {x1  . . .   xt} ⊂ R  a function f : {x1  . . .   xt} → [0  1] is isotonic
(non-decreasing) if xi ≤ xj implies f (xi) ≤ f (xj) for all i  j ∈ {1  . . .   t}. Given a set of labeled
instances D = {(x1  y1)  . . .   (xt  yt)} ⊂ R × [0  1]  let L∗(D) denote the total squared loss of the
best isotonic function on D 

L∗(D) := min
isotonic f

(yi − f (xi))2.

t(cid:88)

i=1

This convex optimization problem can be solved by the celebrated Pool Adjacent Violators Algorithm
(PAVA) in time linear in t [1  7]. The optimal solution  called the isotonic regression function  is
piecewise constant and its value on any of its levels sets equals the average of labels within that set
[24].

2

Online isotonic regression in the random permutation model is deﬁned as follows. At the beginning of
1 and labels y1  . . .   yT . A permutation
the game  the adversary chooses data instances x1 < . . . < xT
σ = (σ1  . . .   σT ) of {1  . . .   T} is then drawn uniformly at random and used to determine the order
predicts(cid:98)yσt. Next  the learner observes the true label yσt and incurs the squared loss ((cid:98)yσt − yσt)2.
in which the data will be revealed. In round t  the instance xσt is revealed to the learner who then
t = L∗({(xσ1  yσ1)  . . .   (xσt  yσt)}) to
For a ﬁxed permutation σ  we use the shorthand notation L∗
denote the optimal isotonic regression loss of the ﬁrst t labeled instances (L∗
t will clearly depend on
σ  except for the case t = T ). The goal of the learner is to minimize the expected regret 

RT := Eσ

− L∗

T =

rt 

(cid:21)

(cid:20) T(cid:88)
(yσt −(cid:98)yσt)2
(cid:104)
(yσt −(cid:98)yσt)2 − L∗

t=1

T(cid:88)
(cid:105)

t=1

 

rt := Eσ

where we have decomposed the regret into its per-round increase 
t + L∗
t−1

(1)
with L∗
0 := 0. To simplify the analysis  let us assume that the prediction strategy does not depend
on the order in which the past data were revealed (which is true for all algorithms considered
in this paper). Fix t and deﬁne D = {(xσ1   yσ1)  . . .   (xσt  yσt)} to be the set of ﬁrst t labeled
instances. Furthermore  let D−i = D\{(xσi  yσi)} denote the set D with the instance from round
i removed. Using this notation  the expression under the expectation in (1) can be written as

(cid:0)yσt −(cid:98)yσt (D−t)(cid:1)2 − L∗(D) + L∗(D−t)  where we made the dependence of(cid:98)yσt on D−t explicit
(cid:104)(cid:0)yσt −(cid:98)yσt(D−t)(cid:1)2(cid:105)
(cid:2)L∗(D−i)(cid:3)  

(and used the fact that it only depends on the set of instances  not on their order). By symmetry of the
expectation over permutations with respect to the indices  we have
Eσ
and Eσ
 
for all i = 1  . . .   t. Thus  (1) can as well be rewritten as:

= Eσ

(cid:104)(cid:0)yσi −(cid:98)yσi(D−i)(cid:1)2(cid:105)
(cid:16)(cid:0)yσi −(cid:98)yσi(D−i)(cid:1)2
t(cid:88)

i=1

(cid:20) 1

t

(cid:2)L∗(D−t)(cid:3) = Eσ
(cid:17) − L∗(D)

(cid:21)

.

rt = Eσ

+ L∗(D−i)

Let us denote the expression inside the expectation by rt(D) to stress its dependence on the set of
instances D  but not on the order in which they were revealed. If we can show that rt(D) ≤ Bt holds

for all t  then its expectation has the same bound  so RT ≤(cid:80)T

t=1 Bt.

2.1 Excess Leave-One-Out Loss and Regret

learner is given D−i  the entire data set except (xi  yi)  and predicts(cid:98)yi (as a function of D−i) on

Our main tool for analyzing the random permutation model is the leave-one-out loss.
In the
leave-one-out model  there is no sequential structure. The adversary picks a data set D =
{(x1  y1)  . . .   (xt  yt)} with x1 < . . . < xt. An index i is sampled uniformly at random  the
instance xi. We call the difference between the expected loss of the learner and L∗(D) the expected
excess leave-one-out loss:

(cid:32)(cid:18) t(cid:88)

i=1

(cid:0)yi −(cid:98)yi(D−i)(cid:1)2(cid:19)

(cid:96)oot(D) :=

1
t

(cid:33)

− L∗(D)

.

(2)

The random permutation model has the important property that the bound on the excess leave-one-out
loss of a prediction algorithm translates into a regret bound. A similar result has been shown by [9]
for expected loss in the i.i.d. setting.
Lemma 2.1. rt(D) ≤ (cid:96)oot(D) for any t and any data set D = {(x1  y1)  . . .   (xt  yt)}.
Proof. As x1 < . . . < xt  let (y∗
L∗(D−i) + (yi − y∗

regression function on D. From the deﬁnition of L∗  we can see that L∗(D) =(cid:80)t
(yi −(cid:98)yi)2 − (yi − y∗

(cid:80)t
i=1(yi − fi)2 be the isotonic
i − yi)2 ≥

(yi −(cid:98)yi)2 + L∗(D−i)

i )2. Thus  the regret increase rt(D) is bounded by

− L∗(D) ≤ t(cid:88)

t ) = argminf1≤...≤ft

1  . . .   y∗

t(cid:88)

i=1(y∗

i )2

= (cid:96)oot(D).

rt(D) =

i=1

t

i=1

t

1 We assume all points xt are distinct as it will signiﬁcantly simplify the presentation. All results in this

paper are also valid for the case x1 ≤ . . . ≤ xT .

3

which the regret bound RT ≤(cid:80)T

However  we note that lower bounds for (cid:96)oot(D) do not imply lower bounds on regret.
In what follows  our strategy will be to derive bounds (cid:96)oot(D) ≤ Bt for various algorithms  from
t=1 Bt can be immediately obtained. From now on  we abbreviate
(cid:96)oot(D) to (cid:96)oot  (as D is clear from the context); we will also consistently assume x1 < . . . < xt.

2.2 Noise free case

As a warm-up  we analyze the noise-free case (when the labels themselves are isotonic) and demon-
strate that analyzing (cid:96)oot easily results in an optimal bound for this setting.

Proposition 2.2. Assume that the labels satisfy y1 ≤ y2 ≤ . . . ≤ yt. The prediction(cid:98)yi that is the
linear interpolation between adjacent labels(cid:98)yi = 1

2 (yi−1 + yi+1)  has

(cid:96)oot ≤ 1
2t

  and thus RT ≤ 1
2

(cid:80)t
Proof. For δi := yi − yi−1  it is easy to check that (cid:96)oot = 1
i=1(δi+1 − δi)2 because the L∗(D)
term is zero. This expression is a convex function of δ1  . . .   δt+1. Note that δi ≥ 0 for each
i=1 δi = 1. Since the maximum of a convex function is at the boundary of
the feasible region  the maximizer is given by δi = 1 for some i ∈ {1  . . .   t + 1}  and δj = 0 for all
j ∈ {1  . . .   t + 1}  j (cid:54)= i. This implies that (cid:96)oot ≤ (2t)−1.

i = 1  . . .   t + 1  and(cid:80)t+1

log(T + 1).

4t

2.3 General Lower Bound

In [14]  a general lower bound was derived showing that the regret of any online isotonic regression
procedure is at least Ω(T 1
3 ) for the adversarial setup (when labels and the index order were chosen
adversarially). This lower bound applies regardless of the order of outcomes  and hence it is also a
lower bound for the random permutation model. This bound translates into (cid:96)oot = Ω(t−2/3).

3 Online-to-batch for ﬁxed design

(cid:98)yfd(cid:0)xσt

(cid:12)(cid:12)yσ1  . . .   yσt−1  {x1  . . .   xTfd}(cid:1) 

Here  we describe an online-to-batch conversion that relates the adversarial ﬁxed design model with
the random permutation model considered in this paper. In the ﬁxed design model with time horizon
Tfd the learner is given the points x1  . . .   xTfd in advance (which is not the case in the random
permutation model)  but the adversary chooses the order σ in which the labels are revealed (as
opposed to σ being drawn at random). We can think of an algorithm for ﬁxed design as a prediction
function
for any order σ  any set {x1  . . .   xTfd} (and hence any time horizon Tfd)  and any time t. This
notation is quite heavy  but makes it explicit that the learner  while predicting at point xσt  knows the
previously revealed labels and the whole set of instances.
In the random permutation model  at trial t  the learner only knows the previously revealed t − 1
labeled instances and predicts on the new instance. Without loss of generality  denote the past
instances by D−i = {(x1  y1)  . . .   (xi−1  yi−1)  (xi+1  yi+1)  . . . (xt  yt)}  and the new instance by

xi  for some i ∈ {1  . . .   t}. Given an algorithm for ﬁxed design(cid:98)yfd  we construct a prediction
(cid:98)yt =(cid:98)yt(D−i  xi) of the algorithm in the random permutation model. The reduction goes through an

online-to-batch conversion. Speciﬁcally  at trial t  given past labeled instances D−i  and a new point
xi  the learner plays the expectation of the prediction of the ﬁxed design algorithm with time horizon
T fd = t and points {x1  . . .   xt} under a uniformly random time from the past j ∈ {1  . . .   t} and a
random permutation σ on {1  . . .   t}  with σt = i  i.e.2

(3)

(cid:20) 1

t(cid:88)

(cid:98)yfd(xi|yσ1   . . .   yσj−1   {x1  . . .   xt}(cid:1)(cid:21)

.

(cid:98)yt = E{σ:σt=i}

t

j=1

2Choosing the prediction as an expectation is elegant but inefﬁcient. However  the proof indicates that we
might as well sample a single j and a single random permutation σ to form the prediction and the reduction
would also work in expectation.

4

Theorem 3.1. Let D = {(x1  y1)  . . .   (xt  yt)} be a set of t labeled instances. Fix any algorithm

Note that this is a valid construction  as the right hand side only depends on D−i and xi  which are
known to the learner in a random permutation model at round t. We prove (in Appendix A) that the

excess leave-one-out loss of(cid:98)y at trial t is upper bounded by the expected regret (over all permutations)
of(cid:98)yfd in trials 1  . . .   t divided by t:
(cid:98)yfd for online adversarial isotonic regression with ﬁxed design  and let Regt((cid:98)yfd | σ) denote its regret
on D when the labels are revealed in order σ. The random permutation learner(cid:98)y from (3) ensures
Eσ[Regt((cid:98)yfd | σ)].
3 ) and hence expected regret RT ≤(cid:80)

This constructions allows immediate transport of the ˜O(T
Theorem 3.2. There is an algorithm for the random-permutation model with excess leave-one-out
loss (cid:96)oot = ˜O(t− 2

fd) ﬁxed design regret result from [14].

(cid:96)oot(D) ≤ 1

3 ) = ˜O(T 1

˜O(t− 2

3 ).

1
3

t

t

4 Forward Algorithms

1  . . .   y∗

For clarity of presentation  we use vector notation in this section: y = (y1  . . .   yt) is the label vector 
y∗ = (y∗
t ) is the isotonic regression function  and y−i = (y1  . . .   yi−1  yi+1  . . .   yt) is y
with i-th label removed. Moreover  keeping in mind that x1 < . . . < xt  we can drop xi’s entirely
from the notation and refer to an instance xi simply by its index i.
Given labels y−i and some index i to predict on  we want a good prediction for yi. Follow the Leader
(FL) algorithms  which predict using the best isotonic function on the data seen so far  are not directly
applicable to online isotonic regression: the best isotonic function is only deﬁned at the observed
data instances and can be arbitrary (up to monotonicity constraint) otherwise. Instead  we analyze
a simple and natural class of algorithms which we dub forward algorithms3. We deﬁne a forward
i ∈ [0  1] (possibly dependent on i and
algorithm  or FA  to be any algorithm that estimates a label y(cid:48)
y−i)  and plays with the FL strategy on the sequence of past data including the new instance with the
estimated label  i.e. performs ofﬂine isotonic regression on y(cid:48) 

(cid:26) t(cid:88)

(cid:98)y = argmin

f1≤...≤ft

(cid:27)

 

j − fj)2
(y(cid:48)

where y(cid:48) = (y1  . . .   yi−1  y(cid:48)

i  yi+1  . . .   yt).

j=1

Then  FA predicts with(cid:98)yi  the value at index i of the ofﬂine function of the augmented data. Note that

if the estimate turned out to be correct (y(cid:48)
loss for that round.
Forward algorithms are practically important: we will show that many popular algorithms can be cast
as FA with a particular estimate. FA automatically inherit any computational advances for ofﬂine
isotonic regression; in particular  they scale efﬁciently to partially ordered data [16]. To our best
knowledge  we are ﬁrst to give bounds on the performance of these algorithms in the online setting.

i = yi)  the forward algorithm would suffer no additional

j=(cid:96) yj

max
(cid:96)≤i

y(cid:96) r = max
(cid:96)≤i

y∗
i = min
r≥i

Alternative formulation We can describe a FA using a minimax representation of the isotonic
regression [see  e.g.  24]: the optimal isotonic regression y∗ satisﬁes
y(cid:96) r 

min
(cid:80)r
r≥i
r−(cid:96)+1 . The “saddle point” ((cid:96)i  ri) for which y∗
where y(cid:96) r =
the level set {j : y∗
It follows from (4) that isotonic regression is monotonic with respect to labels: for any two label
i and(cid:98)y1
i ≤ z∗
sequences y and z such that yi ≤ zi for all i  we have y∗
i for all i. Thus  if we denote the
that any FA has(cid:98)y0
i = 0 and y(cid:48)
predictions for label estimates y(cid:48)
i   respectively  the monotonicity implies
function of y  (which follows from (4))  we can show that for any prediction(cid:98)yi with(cid:98)y0
i ≤(cid:98)yi ≤(cid:98)y1
i . Conversely  using the continuity of isotonic regression y∗ as a
i  
interpret FA as an algorithm which in each trial predicts with some(cid:98)yi in the range [(cid:98)y0
i  (cid:98)y1
t ∈ [0  1] that could generate this prediction. Hence  we can equivalently

i } of the isotonic regression function that contains i.

i = y(cid:96)i ri  speciﬁes the boundaries of

i ≤ (cid:98)yi ≤ (cid:98)y1

there exists an estimate y(cid:48)

i = 1 by(cid:98)y0

j = y∗

(4)

i ].

3The name highlights resemblance to the Forward algorithm introduced by [2] for exponential family models.

5

4.1

Instances

(cid:0)f∗

0 = 0 and f∗

i−1 + f∗
t+1 = 1. To see that this is a FA  note that if we use estimate y(cid:48)
i−1  y(cid:48)
i  f∗

i ]  we
can show that many of the well know isotonic regression algorithms are forward algorithms and
thereby add weight to our next section where we prove regret bounds for the entire class.

Isotonic regression with interpolation (IR-Int)[28] Given y−i and index i  the algorithm ﬁrst
we used f∗
isotonic regression of y(cid:48) = (y1  . . .   yi−1  y(cid:48)

With the above equivalence between forward algorithms and algorithms that predict in [(cid:98)y0
i  (cid:98)y1
(cid:1)  where
computes f∗  the isotonic regression of y−i  and then predicts with(cid:98)yint
i =(cid:98)yint
i  yi+1  . . .   yt) is(cid:98)y = (f∗
This is because: i)(cid:98)y is isotonic by construction; ii) f∗ has the smallest squared error loss for y−t
among isotonic functions; and iii) the loss of (cid:98)y on point y(cid:48)
i is zero  and the loss of (cid:98)y on all other
Direct combination of (cid:98)y0
i and (cid:98)y1
(cid:98)yi = λi(cid:98)y0
i + (1 − λi)(cid:98)y1
It is clear from Section 4  that any algorithm that predicts
λi = 1/2)  or can be chosen depending on(cid:98)y1
i for some λi ∈ [0  1] is a FA. The weight λi can be set to a constant (e.g. 
(cid:98)y1
log-IVAP : (cid:98)ylog
(cid:98)y1
i + 1 −(cid:98)y0
It is straightforward to show that both algorithms satisfy(cid:98)y0

i . Such algorithms were considered by [27]:
i )2

points is equal to the loss of f∗.

i )2 − (1 −(cid:98)y1

i and are thus instances of FA.

i = 1
2
1   . . .   f∗

  the
i+1  . . .   f∗
t ).

1 + ((cid:98)y0

i =

i .

i+1

=

2

 

.

i

i

i

i and(cid:98)y0
Brier-IVAP : (cid:98)yBrier
i ≤(cid:98)yi ≤(cid:98)y1
(cid:110)
(cid:111)
((cid:98)y − yi)2 − L∗(y)

i

 

max
yi∈[0 1]

Last-step minimax (LSM). LSM plays the minimax strategy with one round remaining 

where L∗(y) is the isotonic regression loss on y. Deﬁne L∗
for b ∈ {0  1}  i.e. L∗

b is the loss of isotonic regression function with label estimate y(cid:48)

b = L∗(y1  . . .   yi−1  b  yi+1  . . .   yt)
i = b. In

and it is also an instance of FA.

(cid:98)yi = argmin
(cid:98)y∈[0 1]
Appendix B we show that(cid:98)yi = 1+L∗

0−L∗
2

1

4.2 Bounding the leave-one-out loss

(cid:113) log t

We now give a O(
t ) bound on the leave-one-out loss for forward algorithms. Interestingly  the
bound holds no matter what label estimate the algorithm uses. The proof relies on the stability of
isotonic regression with respect to a change of a single label. While the bound looks suboptimal in
light of Section 2.3  we will argue in Section 4.5 that the bound is actually tight (up to a logarithmic
factor) for one FA and experimentally verify that all other mentioned forward algorithms also have a
tight lower bound of that form for the same sequence of outcomes.

We will bound (cid:96)oot by deﬁning δi =(cid:98)yi − y∗
i − yi)2(cid:17)

((cid:98)yi − yi)2 − (y∗

t(cid:88)

(cid:96)oot =

(cid:16)

1
t

i=1

Theorem 4.1. Any forward algorithm has (cid:96)oot = O

=

i=1

1
t

t(cid:88)

i and using the following simple inequality:
i − 2yi) ≤ 2
t

t(cid:88)
((cid:98)yi − y∗
i )((cid:98)yi + y∗
(cid:16)(cid:113) log t
(cid:17)
i } = {(cid:96)i  . . .   ri}  for some (cid:96)i ≤ i ≤ ri 
i . We need the stronger version of the minimax

j = y∗

|δi|.

i=1

.

t

Proof. Fix some forward algorithm. For any i  let {j : y∗
be the level set of isotonic regression at level y∗
representation  shown in Appendix C:
y∗
i = min
r≥i

We partition the points {1  . . .   t} into K consecutive segments: Sk =

k = 1  . . .   K − 1 and SK =(cid:8)i : y∗

y(cid:96) ri.

y(cid:96)i r = max
(cid:96)≤i

(cid:17)(cid:111)
(cid:9). Due to monotonicity of y∗  Sk are subsets of the

i ∈(cid:104) k−1

K   k

i : y∗

(cid:110)

(5)

for

K

form {(cid:96)k  . . .   rk} (where we use rk = (cid:96)k − 1 if Sk is empty). From the deﬁnition  every level set
of y∗ is contained in Sk for some k  and each (cid:96)k (rk) is a left-end (right-end) of some level set.

K

i ≥ K−1

6

(cid:98)yi = max

(cid:96)≤i
≥ min
r≥i
by (5)≥ y∗

(cid:96)k

min
r≥i

(cid:96) r ≥ min
y(cid:48)
y(cid:48)
(cid:96)k r = min
r≥i
r≥i
i − yi
y(cid:48)
≥ min
r − (cid:96)k + 1
r≥(cid:96)k
−1
≥ y∗
−

(cid:96)k

y(cid:96)k r + min
r≥i

(cid:110)

y(cid:96)k r +

(cid:111)
i − yi
y(cid:48)
r − (cid:96)k + 1
i − yi
y(cid:48)
r − (cid:96)k + 1
i − 1
−
K

y(cid:96)k r + min
r≥i
≥ y∗

1

Now  choose some index i  and let Sk be such that i ∈ Sk. Let y(cid:48)
y(cid:48) = (y1  . . .   yi−1  y(cid:48)

i  yi+1  . . .   yt). The minimax representation (4) and deﬁnition of FA imply

i be the estimate of the FA  and let

1

i

+ 4

1

1

i − (cid:96)k + 1

rk−i+1 . Hence  we can bound |δi| = |(cid:98)yi − y∗
K + 2(cid:0)1 + log |Sk|(cid:1) 
i | ≤

|δi| ≤ |Sk|

i − (cid:96)k + 1

i∈Sk

.

i + 1

rk−i+1

i−(cid:96)k+1  

+ min
r≥i

K + 1

r − (cid:96)k + 1

1
which allows the bound

(cid:9). Summing over i ∈ Sk yields(cid:80)

A symmetric argument gives(cid:98)yi ≤ y∗
K + max(cid:8)
(cid:88)
The theorem follows from setting K = Θ((cid:112)t/ log t).
While the (cid:96)oot upper bound of the previous section yields a regret bound RT ≤(cid:80)

4.3 Forward algorithms for the well-speciﬁed case

(cid:96)oot ≤ 2
t

|δi| ≤ 2
K

(1 + log t).

K
t

t O((cid:112)log t/t) =

2 ) that is a factor O(T 1

˜O(T 1
6 ) gap from the lower bound in Section 2.3  there are two pieces of good
news. First  forward algorithms do get the optimal rate in the well-speciﬁed setting  popular in the
classical statistics literature  where the labels are generated i.i.d. such that E[yi] = µi with isotonic
µ1 ≤ . . . ≤ µt.4 Second  there is a Ω(t− 1
2 ) lower bound for forward algorithms as proven in the next
section. Together  these results imply that the random permutation model in indeed harder than the
well-speciﬁed case: forward algorithms are sufﬁcient for the latter but not the former.
Theorem 4.2. For data generated from the well-speciﬁed setting (monotonic means with i.i.d. noise) 
any FA has (cid:96)oot = ˜O(t− 2
The proof is given in Appendix D. Curiously  the proof makes use of the existence of the seemingly
unrelated optimal algorithm with ˜O(t− 2

3 ) excess leave-one-out loss from Theorem 3.2.

3 )  which translates to a ˜O(T 1

3 ) bound on the regret.

4.4 Entropic loss

We now abandon the squared loss for a moment and analyze how a FA performs when the loss function

is the entropic loss  deﬁned as −y log(cid:98)y − (1 − y) log(1 −(cid:98)y) for y ∈ [0  1]. Entropic loss (precisely:

its binary-label version known as log-loss) is extensively used in the isotonic regression context for
maximum likelihood estimation [14] or for probability calibration [28  21  27]. A surprising fact in
isotonic regression is that minimizing entropic loss5 leads to exactly the same optimal solution as in
the squared loss case  the isotonic regression function y∗ [24].
Not every FA is appropriate for entropic loss  as recklessly choosing the label estimate might result in
an inﬁnite loss in just a single trial (as noted by [27]). Indeed  consider a sequence of outcomes with
y1 = 0 and yi = 1 for i > 1. While predicting on index i = 1  choosing y(cid:48)
which the entropic loss is inﬁnite (as y1 = 0). Does there exists a FA which achieves a meaningful
bound on (cid:96)oot in the entropic loss setup?
We answer this question in the afﬁrmative  showing that the log-IVAP predictor FA gets the same
excess-leave-one-out loss bound as given in Theorem 4.1. As the reduction from the regret to leave-
one-out loss (Lemma 2.1) does not use any properties of the loss function  this immediately implies a
bound on the expected regret. Interestingly  the proof (given in Appendix G) uses as an intermediate
step the bound on |δi| for the worst possible forward algorithm which always produces the estimate
being the opposite of the actual label.
Theorem 4.3. The log-IVAP algorithm has (cid:96)oot = O

1 = 1 results in(cid:98)y1 = 1  for

(cid:16)(cid:113) log t

for the entropic loss.

(cid:17)

t

4The Ω(T 1/3) regret lower bound in [14] uses a mixture of well-speciﬁed distributions and still applies.
5In fact  this statement applies to any Bregman divergence [24].

7

4.5 Lower bound
The last result of this section is that FA can be made to have (cid:96)oot = Ω(t− 1
2 ). We show this by means
of a counterexample. Assume t = n2 for some integer n > 0 and let the labels be binary  yi ∈ {0  1}.
√
We split the set {1  . . .   t} into n consecutive segments  each of size n =
t. The proportion of ones
n  but within each segment all ones precede all zeros. For
(yi = 1) in the k-th segment is equal to k
instance  when t = 25  the label sequence is:
11000

11100

10000

 

(cid:124)(cid:123)(cid:122)(cid:125)

11110

4/5

(cid:124)(cid:123)(cid:122)(cid:125)

11111

5/5

(cid:124)(cid:123)(cid:122)(cid:125)

1/5

(cid:124)(cid:123)(cid:122)(cid:125)

2/5

(cid:124)(cid:123)(cid:122)(cid:125)

3/5

One can use the minimax formulation (4) to verify that the segments will correspond to the level sets
of the isotonic regression and that y∗
n for any i in the k-th segment. This sequence is hard:
Lemma 4.4. The IR-Int algorithm run on the sequence described above has (cid:96)oot = Ω(t− 1
2 ).
We prove the lower bound for IR-Int  since the presentation (in Appendix E) is clearest. Empirical
simulations showing that the other forward algorithms also suffer this regret are in Appendix F.

i = k

4.6 Towards optimal forward algorithms

An attractive feature of forward algorithms is that they generalize to partial orders  for which efﬁcient
ofﬂine optimization algorithms exist. However  in Section 4 we saw that FAs only give a ˜O(t− 1
2 )
rate  while in Section 3 we saw that ˜O(t− 2
3 ) is possible (with an algorithm that is not known to scale
to partial orders). Is there any hope of an algorithm that both generalizes and has the optimal rate?
In this section  we propose the Heavy-γ algorithm  a slight modiﬁcation of the forward algorithm that
i = γ ∈ [0  1] with weight c (with unit weight on all other points)  then plays
plugs in label estimate y(cid:48)
the value of the isotonic regression function. Implementation is straightforward for ofﬂine isotonic
regression algorithms that permit the speciﬁcation of weights (such as [16]). Otherwise  we might
simulate such weighting by plugging in c copies of the estimated label γ at location xi.
What label estimate γ and weight c should we use? We show that the choice of γ is not very sensitive 
but it is crucial to tune the weight to c = Θ(t 1
3 ). Lemmas H.1 and H.2 show that higher and lower c
are necessarily sub-optimal for (cid:96)oot. This leaves only one choice for c  for which we believe
Conjecture 4.5. Heavy-γ with weight c = Θ(t 1

3 ) has (cid:96)oot = ˜O(t− 2
3 ).

We cannot yet prove this conjecture  although numerical experiments strongly suggest it. We do not
believe that picking a constant label γ is special. For example  we might alternatively predict with the
average of the predictions of Heavy-1 and Heavy-0. Yet not any label estimate works. In particular  if
we estimate the label that would be predicted by IR-Int (see 4.1) and the discussion below it)  and
we plug that in with any weight c ≥ 0  then the isotonic regression function will still have that same
label estimate as its value. This means that the Ω(t− 1

2 ) lower bound of Section 4.5 applies.

5 Conclusion

We revisit the problem of online isotonic regression and argue that we need a new perspective to
design practical algorithms. We study the random permutation model as a novel way to bypass the
stringent ﬁxed design requirement of previous work. Our main tool in the design and analysis of
algorithms is the leave-one-out loss  which bounds the expected regret from above. We start by
observing that the adversary from the adversarial ﬁxed design setting also provides a lower bound
here. We then show that this lower bound can be matched by applying online-to-batch conversion to
the optimal algorithm for ﬁxed design. Next we provide an online analysis of the natural  popular and
practical class of Forward Algorithms  which are deﬁned in terms of an ofﬂine optimization oracle.
We show that Forward algorithms achieve a decent regret rate in all cases  and match the optimal rate
in special cases. We conclude by sketching the class of practical Heavy algorithms and conjecture
that a speciﬁc parameter setting might guarantee the correct regret rate.

Open problem The next major challenge is the design and analysis of efﬁcient algorithms for
online isotonic regression on arbitrary partial orders. Heavy-γ is our current best candidate. We pose
deciding if it in fact even guarantees ˜O(T 1

3 ) regret on linear orders as an open problem.

8

Acknowledgments

Wojciech Kotłowski acknowledges support from the Polish National Science Centre (grant no.
2016/22/E/ST6/00299). Wouter Koolen acknowledges support from the Netherlands Organization for
Scientiﬁc Research (NWO) under Veni grant 639.021.439. This work was done in part while Koolen
was visiting the Simons Institute for the Theory of Computing.

References
[1] M. Ayer  H. D. Brunk  G. M. Ewing  W. T. Reid  and E. Silverman. An empirical distribution
function for sampling with incomplete information. Annals of Mathematical Statistics  26(4):
641–647  1955.

[2] K. Azoury and M. Warmuth. Relative loss bounds for on-line density estimation with the

exponential family of distributions. Journal of Machine Learning  43(3):211–246  2001.

[3] Lucien Birgé and Pascal Massart. Rates of convergence for minimum contrast estimators.

Probability Theory and Related Fields  97:113–150  1993.

[4] H. D. Brunk. Maximum likelihood estimates of monotone parameters. Annals of Mathematical

Statistics  26(4):607–616  1955.

[5] Nicolò Cesa-Bianchi and Gábor Lugosi. Worst-case bounds for the logarithmic loss of predictors.

Machine Learning  43(3):247–264  2001.

[6] Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction  learning  and games. Cambridge University

Press  2006.

[7] Jan de Leeuw  Kurt Hornik  and Patrick Mair. Isotone optimization in R: Pool-adjacent-violators

algorithm (PAVA) and active set methods. Journal of Statistical Software  32:1–24  2009.

[8] Tom Fawcett and Alexandru Niculescu-Mizil. PAV and the ROC convex hull. Machine Learning 

68(1):97–106  2007.

[9] Jürgen Forster and Manfred K Warmuth. Relative expected instantaneous loss bounds. Journal

of Computer and System Sciences  64(1):76–102  2002.

[10] Pierre Gaillard and Sébastien Gerchinovitz. A chaining algorithm for online nonparametric

regression. In Conference on Learning Theory (COLT)  pages 764–796  2015.

[11] Sham M Kakade  Varun Kanade  Ohad Shamir  and Adam Kalai. Efﬁcient learning of general-
ized linear and single index models with isotonic regression. In Neural Information Processing
Systems (NIPS)  pages 927–935  2011.

[12] Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic

regression. In COLT  2009.

[13] Wojciech Kotłowski and Roman Słowi´nski. Rule learning with monotonicity constraints. In

International Conference on Machine Learning (ICML)  pages 537–544  2009.

[14] Wojciech Kotłowski  Wouter M. Koolen  and Alan Malek. Online isotonic regression. In
Vitaly Feldman and Alexander Rakhlin  editors  Proceedings of the 29th Annual Conference on
Learning Theory (COLT)  pages 1165–1189  June 2016.

[15] J. B. Kruskal. Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric hypothesis.

Psychometrika  29(1):1–27  1964.

[16] Rasmus Kyng  Anup Rao  and Sushant Sachdeva. Fast  provable algorithms for isotonic

regression in all (cid:96)p-norms. In Neural Information Processing Systems (NIPS)  2015.

[17] Ronny Luss  Saharon Rosset  and Moni Shahar. Efﬁcient regularized isotonic regression with
application to gene–gene interaction search. Annals of Applied Statistics  6(1):253–283  2012.

9

[18] Aditya Krishna Menon  Xiaoqian Jiang  Shankar Vembu  Charles Elkan  and Lucila Ohno-
Machado. Predicting accurate probabilities with a ranking loss. In Interantional Conference on
Machine Learning (ICML)  2012.

[19] T. Moon  A. Smola  Y. Chang  and Z. Zheng. Intervalrank: Isotonic regression with listwise

and pairwise constraint. In WSDM  pages 151–160. ACM  2010.

[20] Harikrishna Narasimhan and Shivani Agarwal. On the relationship between binary classiﬁcation 
bipartite ranking  and binary class probability estimation. In Neural Information Processing
Systems (NIPS)  pages 2913–2921  2013.

[21] Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised

learning. In ICML  volume 119  pages 625–632. ACM  2005.

[22] G. Obozinski  C. E. Grant  G. R. G. Lanckriet  M. I. Jordan  and W. W. Noble. Consistent

probabilistic outputs for protein function prediction. Genome Biology  2008 2008.

[23] Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression. In Conference on

Learning Theory (COLT)  pages 1232–1264  2014.

[24] T. Robertson  F. T. Wright  and R. L. Dykstra. Order Restricted Statistical Inference. John

Wiley & Sons  1998.

[25] Mario Stylianou and Nancy Flournoy. Dose ﬁnding using the biased coin up-and-down design

and isotonic regression. Biometrics  58(1):171–177  2002.

[26] Sara Van de Geer. Estimating a regression function. Annals of Statistics  18:907–924  1990.

[27] Vladimir Vovk  Ivan Petej  and Valentina Fedorova. Large-scale probabilistic predictors with
and without guarantees of validity. In Neural Information Processing Systems (NIPS)  pages
892–900  2015.

[28] Bianca Zadrozny and Charles Elkan. Transforming classiﬁer scores into accurate multiclass
probability estimates. In International Conference on Knowledge Discovery and Data Mining
(KDD)  pages 694–699  2002.

[29] Cun-Hui Zhang. Risk bounds in isotonic regression. The Annals of Statistics  30(2):528–555 

2002.

10

,Wojciech Kotlowski
Wouter Koolen
Alan Malek