2010,Rates of convergence for the cluster tree,For a density f on R^d  a high-density cluster is any connected component of {x: f(x) >= c}  for some c > 0. The set of all high-density clusters form a hierarchy called the cluster tree of f. We present a procedure for estimating the cluster tree given samples from f. We give finite-sample convergence rates for our algorithm  as well as lower bounds on the sample complexity of this estimation problem.,Rates of convergence for the cluster tree

Kamalika Chaudhuri

UC San Diego

kchaudhuri@ucsd.edu

Sanjoy Dasgupta

UC San Diego

dasgupta@cs.ucsd.edu

Abstract

For a density f on Rd  a high-density cluster is any connected component of {x :
f (x) ≥ λ}  for some λ > 0. The set of all high-density clusters form a hierarchy
called the cluster tree of f. We present a procedure for estimating the cluster tree
given samples from f. We give ﬁnite-sample convergence rates for our algorithm 
as well as lower bounds on the sample complexity of this estimation problem.

1

Introduction

A central preoccupation of learning theory is to understand what statistical estimation based on a
ﬁnite data set reveals about the underlying distribution from which the data were sampled. For
classiﬁcation problems  there is now a well-developed theory of generalization. For clustering 
however  this kind of analysis has proved more elusive.

Consider for instance k-means  possibly the most popular clustering procedure in use today.
If
this procedure is run on points X1  . . .   Xn from distribution f  and is told to ﬁnd k clusters  what
do these clusters reveal about f? Pollard [8] proved a basic consistency result: if the algorithm
always ﬁnds the global minimum of the k-means cost function (which is NP-hard  see Theorem 3
of [3])  then as n → ∞  the clustering is the globally optimal k-means solution for f. This result 
however impressive  leaves the fundamental question unanswered: is the best k-means solution to f
an interesting or desirable quantity  in settings outside of vector quantization?

In this paper  we are interested in clustering procedures whose output on a ﬁnite sample converges
to “natural clusters” of the underlying distribution f. There are doubtless many meaningful ways
to deﬁne natural clusters. Here we follow some early work on clustering (for instance  [5]) by
associating clusters with high-density connected regions. Speciﬁcally  a cluster of density f is any
connected component of {x : f (x) ≥ λ}  for any λ > 0. The collection of all such clusters forms
an (inﬁnite) hierarchy called the cluster tree (Figure 1).

Are there hierarchical clustering algorithms which converge to the cluster tree? Previous theory
work [5  7] has provided weak consistency results for the single-linkage clustering algorithm  while
other work [13] has suggested ways to overcome the deﬁciencies of this algorithm by making it
more robust  but without proofs of convergence. In this paper  we propose a novel way to make
single-linkage more robust  while retaining most of its elegance and simplicity (see Figure 3). We
establish its ﬁnite-sample rate of convergence (Theorem 6); the centerpiece of our argument is a
result on continuum percolation (Theorem 11). We also give a lower bound on the problem of
cluster tree estimation (Theorem 12)  which matches our upper bound in its dependence on most of
the parameters of interest.

2 Deﬁnitions and previous work

Let X be a subset of Rd. We exclusively consider Euclidean distance on X   denoted k · k. Let
B(x  r) be the closed ball of radius r around x.

1

f (x)

λ1

λ2

λ3

C1

C2

C3

X

Figure 1: A probability density f on R  and three of its clusters: C1  C2  and C3.

2.1 The cluster tree

We start with notions of connectivity. A path P in S ⊂ X is a continuous 1 − 1 function P :
P
[0  1] → S. If x = P (0) and y = P (1)  we write x
  y  and we say that x and y are connected in
S. This relation – “connected in S” – is an equivalence relation that partitions S into its connected
components. We say S ⊂ X is connected if it has a single connected component.
The cluster tree is a hierarchy each of whose levels is a partition of a subset of X   which we will
occasionally call a subpartition of X . Write Π(X ) = {subpartitions of X}.
Deﬁnition 1 For any f : X → R  the cluster tree of f is a function Cf : R → Π(X ) given by

Cf (λ) = connected components of {x ∈ X : f (x) ≥ λ}.

Any element of Cf (λ)  for any λ  is called a cluster of f.

For any λ  Cf (λ) is a set of disjoint clusters of X . They form a hierarchy in the following sense.
Lemma 2 Pick any λ′ ≤ λ. Then:

1. For any C ∈ Cf (λ)  there exists C ′ ∈ Cf (λ′) such that C ⊆ C ′.
2. For any C ∈ Cf (λ) and C ′ ∈ Cf (λ′)  either C ⊆ C ′ or C ∩ C ′ = ∅.

We will sometimes deal with the restriction of the cluster tree to a ﬁnite set of points x1  . . .   xn.
Formally  the restriction of a subpartition C ∈ Π(X ) to these points is deﬁned to be C[x1  . . .   xn] =
{C ∩ {x1  . . .   xn} : C ∈ C}. Likewise  the restriction of the cluster tree is Cf [x1  . . .   xn] : R →
Π({x1  . . .   xn})  where Cf [x1  . . .   xn](λ) = Cf (λ)[x1  . . .   xn]. See Figure 2 for an example.
2.2 Notion of convergence and previous work

Suppose a sample Xn ⊂ X of size n is used to construct a tree Cn that is an estimate of Cf . Hartigan
[5] provided a very natural notion of consistency for this setting.
Deﬁnition 3 For any sets A  A′ ⊂ X   let An (resp  A′
n) denote the smallest cluster of Cn containing
A ∩ Xn (resp  A′ ∩ Xn). We say Cn is consistent if  whenever A and A′ are different connected
components of {x : f (x) ≥ λ} (for some λ > 0)  P(An is disjoint from A′
It is well known that if Xn is used to build a uniformly consistent density estimate fn (that is 
supx |fn(x) − f (x)| → 0)  then the cluster tree Cfn is consistent; see the appendix for details.
The big problem is that Cfn is not easy to compute for typical density estimates fn: imagine  for
instance  how one might go about trying to ﬁnd level sets of a mixture of Gaussians! Wong and

n) → 1 as n → ∞.

2

f (x)

X

Figure 2: A probability density f  and the restriction of Cf to a ﬁnite set of eight points.

Lane [14] have an efﬁcient procedure that tries to approximate Cfn when fn is a k-nearest neighbor
density estimate  but they have not shown that it preserves the consistency property of Cfn.
There is a simple and elegant algorithm that is a plausible estimator of the cluster tree: single
linkage (or Kruskal’s algorithm); see the appendix for pseudocode. Hartigan [5] has shown that it is
consistent in one dimension (d = 1). But he also demonstrates  by a lovely reduction to continuum
percolation  that this consistency fails in higher dimension d ≥ 2. The problem is the requirement
that A ∩ Xn ⊂ An: by the time the clusters are large enough that one of them contains all of A 
there is a reasonable chance that this cluster will be so big as to also contain part of A′.
With this insight  Hartigan deﬁnes a weaker notion of fractional consistency  under which An (resp 
n) need not contain all of A∩ Xn (resp  A′ ∩ Xn)  but merely a sizeable chunk of it – and ought to
A′
be very close (at distance → 0 as n → ∞) to the remainder. He then shows that single linkage has
this weaker consistency property for any pair A  A′ for which the ratio of inf{f (x) : x ∈ A∪ A′} to
sup{inf{f (x) : x ∈ P} : paths P from A to A′} is sufﬁciently large. More recent work by Penrose
[7] closes the gap and shows fractional consistency whenever this ratio is > 1.
A more robust version of single linkage has been proposed by Wishart [13]: when connecting points
at distance r from each other  only consider points that have at least k neighbors within distance r
(for some k > 2). Thus initially  when r is small  only the regions of highest density are available for
linkage  while the rest of the data set is ignored. As r gets larger  more and more of the data points
become candidates for linkage. This scheme is intuitively sensible  but Wishart does not provide a
proof of convergence. Thus it is unclear how to set k  for instance.
Stuetzle and Nugent [12] have an appealing top-down scheme for estimating the cluster tree  along
with a post-processing step (called runt pruning) that helps identify modes of the distribution. The
consistency of this method has not yet been established.

Several recent papers [6  10  9  11] have considered the problem of recovering the connected com-
ponents of {x : f (x) ≥ λ} for a user-speciﬁed λ: the ﬂat version of our problem. In particular 
the algorithm of [6] is intuitively similar to ours  though they use a single graph in which each point
is connected to its k nearest neighbors  whereas we have a hierarchy of graphs in which each point
is connected to other points at distance ≤ r (for various r). Interestingly  k-nn graphs are valuable
for ﬂat clustering because they can adapt to clusters of different scales (different average interpoint
distances). But they are challenging to analyze and seem to require various regularity assumptions
on the data. A pleasant feature of the hierarchical setting is that different scales appear at different
levels of the tree  rather than being collapsed together. This allows the use of r-neighbor graphs  and
makes possible an analysis that has minimal assumptions on the data.

3 Algorithm and results

In this paper  we consider a generalization of Wishart’s scheme and of single linkage  shown in
Figure 3. It has two free parameters: k and α. For practical reasons  it is of interest to keep these as

3

1. For each xi set rk(xi) = inf{r : B(xi  r) contains k data points}.
2. As r grows from 0 to ∞:

Include edge (xi  xj) if kxi − xjk ≤ αr.

(a) Construct a graph Gr with nodes {xi : rk(xi) ≤ r}.
(b) Let bC(r) be the connected components of Gr.

Figure 3: Algorithm for hierarchical clustering. The input is a sample Xn = {x1  . . .   xn} from
density f on X . Parameters k and α need to be set. Single linkage is (α = 1  k = 2). Wishart
suggested α = 1 and larger k.

small as possible. We provide ﬁnite-sample convergence rates for all 1 ≤ α ≤ 2 and we can achieve
k ∼ d log n  which we conjecture to be the best possible  if α > √2. Our rates for α = 1 force k to
be much larger  exponential in d. It is a fascinating open problem to determine whether the setting
(α = 1  k ∼ d log n) yields consistency.
3.1 A notion of cluster salience

Suppose density f is supported on some subset X of Rd. We will show that the hierarchical cluster-
ing procedure is consistent in the sense of Deﬁnition 3. But the more interesting question is  what
clusters will be identiﬁed from a ﬁnite sample? To answer this  we introduce a notion of salience.

The ﬁrst consideration is that a cluster is hard to identify if it contains a thin “bridge” that would
make it look disconnected in a small sample. To control this  we consider a “buffer zone” of width
σ around the clusters.
Deﬁnition 4 For Z ⊂ Rd and σ > 0  write Zσ = Z + B(0  σ) = {y ∈ Rd : inf z∈Z ky − zk ≤ σ}.
An important technical point is that Zσ is a full-dimensional set  even if Z itself is not.
Second  the ease of distinguishing two clusters A and A′ depends inevitably upon the separation
between them. To keep things simple  we’ll use the same σ as a separation parameter.
Deﬁnition 5 Let f be a density on X ⊂ Rd. We say that A  A′ ⊂ X are (σ  ǫ)-separated if there
exists S ⊂ X (separator set) such that:

• Any path in X from A to A′ intersects S.
• supx∈Sσ f (x) < (1 − ǫ) inf x∈Aσ ∪A′

σ f (x).

Under this deﬁnition  Aσ and A′
is zero. However  Sσ need not be contained in X .
3.2 Consistency and ﬁnite-sample rate of convergence

σ must lie within X   otherwise the right-hand side of the inequality

Here we state the result for α > √2 and k ∼ d log n. The analysis section also has results for
1 ≤ α ≤ 2 and k ∼ (2/α)dd log n.
Theorem 6 There is an absolute constant C such that the following holds. Pick any δ  ǫ > 0  and
run the algorithm on a sample Xn of size n drawn from f  with settings

√2(cid:18)1 +

ǫ2

√d(cid:19) ≤ α ≤ 2 and k = C ·

d log n

ǫ2

· log2 1

δ

.

Then there is a mapping r : [0 ∞) → [0 ∞) such that with probability at least 1 − δ  the following
holds uniformly for all pairs of connected subsets A  A′ ⊂ X : If A  A′ are (σ  ǫ)-separated (for ǫ
and some σ > 0)  and if

f (x) ≥
where vd is the volume of the unit ball in Rd  then:

x∈Aσ ∪A′
σ

λ :=

inf

1

vd(σ/2)d ·

4

k

n ·(cid:16)1 +

ǫ

2(cid:17)

(*)

1. Separation. A ∩ Xn is disconnected from A′ ∩ Xn in Gr(λ).
2. Connectedness. A ∩ Xn and A′ ∩ Xn are each individually connected in Gr(λ).

The two parts of this theorem – separation and connectedness – are proved in Sections 3.3 and 3.4.
We mention in passing that this ﬁnite-sample result implies consistency (Deﬁnition 3): as n → ∞ 
take kn = (d log n)/ǫ2
n with any schedule of (ǫn : n = 1  2  . . .) such that ǫn → 0 and kn/n → 0.
Under mild conditions  any two connected components A  A′ of {f ≥ λ} are (σ  ǫ)-separated for
some σ  ǫ > 0 (see appendix); thus they will get distinguished for sufﬁciently large n.

3.3 Analysis: separation

The cluster tree algorithm depends heavily on the radii rk(x): the distance within which x’s nearest
k neighbors lie (including x itself). Thus the empirical probability mass of B(x  rk(x)) is k/n. To
show that rk(x) is meaningful  we need to establish that the mass of this ball under density f is also 
very approximately  k/n. The uniform convergence of these empirical counts follows from the fact
that balls in Rd have ﬁnite VC dimension  d + 1. Using uniform Bernstein-type bounds  we get a
set of basic inequalities that we use repeatedly.

Cδd log n

Lemma 7 Assume k ≥ d log n  and ﬁx some δ > 0. Then there exists a constant Cδ such that with
probability > 1 − δ  every ball B ⊂ Rd satisﬁes the following conditions:
=⇒ fn(B) > 0
n pkd log n =⇒ fn(B) ≥
k
n
n pkd log n =⇒ fn(B) <
k
n

Here fn(B) = |Xn ∩ B|/n is the empirical mass of B  while f (B) =RB f (x)dx is its true mass.

f (B) ≥
f (B) ≤

PROOF: See appendix. Cδ = 2Co log(2/δ)  where Co is the absolute constant from Lemma 16. (cid:3)

+

f (B) ≥
Cδ
k
n
k
n −

Cδ

n

We will henceforth think of δ as ﬁxed  so that we do not have to repeatedly quantify over it.

Lemma 8 Pick 0 < r < 2σ/(α + 2) such that

vdrdλ ≥
vdrdλ(1 − ǫ) <

+

k
n
k
n −

Cδ

n pkd log n
n pkd log n

Cδ

(recall that vd is the volume of the unit ball in Rd). Then with probability > 1 − δ:

1. Gr contains all points in (Aσ−r ∪ A′
2. A ∩ Xn is disconnected from A′ ∩ Xn in Gr.

σ−r) ∩ Xn and no points in Sσ−r ∩ Xn.

PROOF: For (1)  any point x ∈ (Aσ−r∪A′
σ−r) has f (B(x  r)) ≥ vdrdλ; and thus  by Lemma 7  has
at least k neighbors within radius r. Likewise  any point x ∈ Sσ−r has f (B(x  r)) < vdrdλ(1 − ǫ);
and thus  by Lemma 7  has strictly fewer than k neighbors within distance r.
For (2)  since points in Sσ−r are absent from Gr  any path from A to A′ in that graph must have an
edge across Sσ−r. But any such edge has length at least 2(σ − r) > αr and is thus not in Gr. (cid:3)

Deﬁnition 9 Deﬁne r(λ) to be the value of r for which vdrdλ = k

n + Cδ

n √kd log n.

To satisfy the conditions of Lemma 8  it sufﬁces to take k ≥ 4C 2

δ (d/ǫ2) log n; this is what we use.

5

xi

π(xi)

x′

xi

xi+1

x′

x

x

π(xi)

Figure 4: Left: P is a path from x to x′  and π(xi) is the point furthest along the path that is within
distance r of xi. Right: The next point  xi+1 ∈ Xn  is chosen from a slab of B(π(xi)  r) that is
perpendicular to xi − π(xi) and has width 2ζ/√d.

3.4 Analysis: connectedness

We need to show that points in A (and similarly A′) are connected in Gr(λ). First we state a simple
bound (proved in the appendix) that works if α = 2 and k ∼ d log n; later we consider smaller α.
Lemma 10 Suppose 1 ≤ α ≤ 2. Then with probability ≥ 1 − δ  A ∩ Xn is connected in Gr
whenever r ≤ 2σ/(2 + α) and the conditions of Lemma 8 hold  and

vdrdλ ≥(cid:18) 2

α(cid:19)d Cδd log n

n

.

Comparing this to the deﬁnition of r(λ)  we see that choosing α = 1 would entail k ≥ 2d  which is
undesirable. We can get a more reasonable setting of k ∼ d log n by choosing α = 2  but we’d like
α to be as small as possible. A more reﬁned argument shows that α ≈ √2 is enough.
Theorem 11 Suppose α2 ≥ 2(1 + ζ/√d)  for some 0 < ζ ≤ 1. Then  with probability > 1 − δ 
A ∩ Xn is connected in Gr whenever r ≤ σ/2 and the conditions of Lemma 8 hold  and

vdrdλ ≥

8
ζ ·

Cδd log n

.

n

PROOF: We have already made heavy use of uniform convergence over balls. We now also require
a more complicated class G  each element of which is the intersection of an open ball and a slab
deﬁned by two parallel hyperplanes. Formally  each of these functions is deﬁned by a center µ and
a unit direction u  and is the indicator function of the set

{z ∈ Rd : kz − µk < r |(z − µ) · u| ≤ ζr/√d}.

We will describe any such set as “the slab of B(µ  r) in direction u”. A simple calculation (see
Lemma 4 of [4]) shows that the volume of this slab is at least ζ/4 that of B(x  r). Thus  if the slab lies
entirely in Aσ  its probability mass is at least (ζ/4)vdrdλ. By uniform convergence over G (which
has VC dimension 2d)  we can then conclude (as in Lemma 7) that if (ζ/4)vdrdλ ≥ (2Cδd log n)/n 
then with probability at least 1 − δ  every such slab within A contains at least one data point.
P
Pick any x  x′ ∈ A∩Xn; there is a path P in A with x
  x′. We’ll identify a sequence of data points
x0 = x  x1  x2  . . .  ending in x′  such that for every i  point xi is active in Gr and kxi−xi+1k ≤ αr.
This will conﬁrm that x is connected to x′ in Gr.
To begin with  recall that P is a continuous 1 − 1 function from [0  1] into A. We are also interested
in the inverse P −1  which sends a point on the path back to its parametrization in [0  1]. For any
point y ∈ X   deﬁne N (y) to be the portion of [0  1] whose image under P lies in B(y  r): that is 
N (y) = {0 ≤ z ≤ 1 : P (z) ∈ B(y  r)}. If y is within distance r of P   then N (y) is nonempty.
Deﬁne π(y) = P (sup N (y))  the furthest point along the path within distance r of y (Figure 4  left).
The sequence x0  x1  x2  . . . is deﬁned iteratively; x0 = x  and for i = 0  1  2  . . . :

• If kxi − x′k ≤ αr  set xi+1 = x′ and stop.

6

• By construction  xi is within distance r of path P and hence N (xi) is nonempty.
• Let B be the open ball of radius r around π(xi). The slab of B in direction xi − π(xi)

must contain a data point; this is xi+1 (Figure 4  right).

The process eventually stops because each π(xi+1) is strictly further along path P than π(xi);
formally  P −1(π(xi+1)) > P −1(π(xi)). This is because kxi+1 − π(xi)k < r  so by continuity of
the function P   there are points further along the path (beyond π(xi)) whose distance to xi+1 is still
< r. Thus xi+1 is distinct from x0  x1  . . .   xi. Since there are ﬁnitely many data points  the process
must terminate  so the sequence {xi} does constitute a path from x to x′.
Each xi lies in Ar ⊆ Aσ−r and is thus active in Gr (Lemma 8). Finally  the distance between
successive points is:

kxi − xi+1k2 = kxi − π(xi) + π(xi) − xi+1k2

= kxi − π(xi)k2 + kπ(xi) − xi+1k2 + 2(xi − π(xi)) · (π(xi) − xi+1)
≤ 2r2 +

2ζr2
√d ≤ α2r2 

where the second-last inequality comes from the deﬁnition of slab. (cid:3)

δ (d/ǫ2) log n  which satisﬁes the requirements
To complete the proof of Theorem 6  take k = 4C 2
of Lemma 8 as well as those of Theorem 11  using ζ = 2ǫ2. The relationship that deﬁnes r(λ)
(Deﬁnition 9) then translates into

vdrdλ =

k

n(cid:16)1 +

ǫ

2(cid:17) .

This shows that clusters at density level λ emerge when the growing radius r of the cluster tree
algorithm reaches roughly (k/(λvdn))1/d. In order for (σ  ǫ)-separated clusters to be distinguished 
we need this radius to be at most σ/2; this is what yields the ﬁnal lower bound on λ.

4 Lower bound

We have shown that the algorithm of Figure 3 distinguishes pairs of clusters that are (σ  ǫ)-separated.
The number of samples it requires to capture clusters at density ≥ λ is  by Theorem 6 

O(cid:18)

d

vd(σ/2)dλǫ2 log

vd(σ/2)dλǫ2(cid:19)  

d

We’ll now show that this dependence on σ  λ  and ǫ is optimal. The only room for improvement 
therefore  is in constants involving d.

Theorem 12 Pick any ǫ in (0  1/2)  any d > 1  and any σ  λ > 0 such that λvd−1σd < 1/50. Then
there exist: an input space X ⊂ Rd; a ﬁnite family of densities Θ = {θi} on X ; subsets Ai  A′
i  Si ⊂
θi(x) ≥ λ  with
X such that Ai and A′
the following additional property.
Consider any algorithm that is given n ≥ 100 i.i.d. samples Xn from some θi ∈ Θ and  with
probability at least 1/2  outputs a tree in which the smallest cluster containing Ai ∩ Xn is disjoint
from the smallest cluster containing A′

i are (σ  ǫ)-separated by Si for density θi  and inf x∈Ai σ ∪A′

i σ

i ∩ Xn. Then

n = Ω(cid:18)

1

vdσdλǫ2d1/2 log

1

vdσdλ(cid:19) .

PROOF: We start by constructing the various spaces and densities. X is made up of two disjoint
regions: a cylinder X0  and an additional region X1 whose sole purpose is as a repository for excess
probability mass. Let Bd−1 be the unit ball in Rd−1  and let σBd−1 be this same ball scaled to have
radius σ. The cylinder X0 stretches along the x1-axis; its cross-section is σBd−1 and its length is
4(c + 1)σ for some c > 1 to be speciﬁed: X0 = [0  4(c + 1)σ] × σBd−1. Here is a picture of it:

7

σ

0

4σ

8σ

12σ

4(c + 1)σ

x1 axis

We will construct a family of densities Θ = {θi} on X   and then argue that any cluster tree algorithm
that is able to distinguish (σ  ǫ)-separated clusters must be able  when given samples from some θI 
to determine the identity of I. The sample complexity of this latter task can be lower-bounded using
Fano’s inequality (typically stated as in [2]  but easily rewritten in the convenient form of [15]  see
appendix): it is Ω((log |Θ|)/β)  for β = maxi6=j K(θi  θj)  where K(· ·) is KL divergence.
The family Θ contains c − 1 densities θ1  . . .   θc−1  where θi is deﬁned as follows:

• Density λ on [0  4σi + σ]× σBd−1 and on [4σi + 3σ  4(c + 1)σ]× σBd−1. Since the cross-
sectional area of the cylinder is vd−1σd−1  the total mass here is λvd−1σd(4(c + 1) − 2).
• Density λ(1 − ǫ) on (4σi + σ  4σi + 3σ) × σBd−1.
• Point masses 1/(2c) at locations 4σ  8σ  . . .   4cσ along the x1-axis (use arbitrarily narrow
• The remaining mass  1/2− λvd−1σd(4(c + 1)− 2ǫ)  is placed on X1 in some ﬁxed manner

spikes to avoid discontinuities).

(that does not vary between different densities in Θ).

Here is a sketch of θi. The low-density region of width 2σ is centered at 4σi + 2σ on the x1-axis.

density λ(1 − ǫ)

2σ

density λ

point mass 1/2c

For any i 6= j  the densities θi and θj differ only on the cylindrical sections (4σi + σ  4σi + 3σ) ×
σBd−1 and (4σj + σ  4σj + 3σ)× σBd−1  which are disjoint and each have volume 2vd−1σd. Thus

K(θi  θj) = 2vd−1σd(cid:18)λ log

λ

λ(1 − ǫ)

+ λ(1 − ǫ) log

λ(1 − ǫ)

λ

(cid:19)

= 2vd−1σdλ(−ǫ log(1 − ǫ)) ≤

4
ln 2

vd−1σdλǫ2

(using ln(1 − x) ≥ −2x for 0 < x ≤ 1/2). This is an upper bound on the β in the Fano bound.
Now deﬁne the clusters and separators as follows: for each 1 ≤ i ≤ c − 1 

• Ai is the line segment [σ  4σi] along the x1-axis 
• A′
• Si = {4σi + 2σ} × σBd−1 is the cross-section of the cylinder at location 4σi + 2σ.

i is the line segment [4σ(i + 1)  4(c + 1)σ − σ] along the x1-axis  and

i are one-dimensional sets while Si is a (d − 1)-dimensional set. It can be checked
i are (σ  ǫ)-separated by Si in density θi.

Thus Ai and A′
that Ai and A′
With the various structures deﬁned  what remains is to argue that if an algorithm is given a sample
I ∩ Xn  then it can
Xn from some θI (where I is unknown)  and is able to separate AI ∩ Xn from A′
effectively infer I. This has sample complexity Ω((log c)/β). Details are in the appendix. (cid:3)

There remains a discrepancy of 2d between the upper and lower bounds; it is an interesting open
problem to close this gap. Does the (α = 1  k ∼ d log n) setting (yet to be analyzed) do the job?
Acknowledgments. We thank the anonymous reviewers for their detailed and insightful comments 
and the National Science Foundation for support under grant IIS-0347646.

8

References

[1] O. Bousquet  S. Boucheron  and G. Lugosi. Introduction to statistical learning theory. Lecture

Notes in Artiﬁcial Intelligence  3176:169–207  2004.

[2] T. Cover and J. Thomas. Elements of Information Theory. Wiley  2005.
[3] S. Dasgupta and Y. Freund. Random projection trees for vector quantization. IEEE Transac-

tions on Information Theory  55(7):3229–3242  2009.

[4] S. Dasgupta  A. Kalai  and C. Monteleoni. Analysis of perceptron-based active learning. Jour-

nal of Machine Learning Research  10:281–299  2009.

[5] J.A. Hartigan. Consistency of single linkage for high-density clusters. Journal of the American

Statistical Association  76(374):388–394  1981.

[6] M. Maier  M. Hein  and U. von Luxburg. Optimal construction of k-nearest neighbor graphs

for identifying noisy clusters. Theoretical Computer Science  410:1749–1764  2009.

[7] M. Penrose. Single linkage clustering and continuum percolation. Journal of Multivariate

Analysis  53:94–109  1995.

[8] D. Pollard. Strong consistency of k-means clustering. Annals of Statistics  9(1):135–140  1981.
[9] P. Rigollet and R. Vert. Fast rates for plug-in estimators of density level sets. Bernoulli 

15(4):1154–1178  2009.

[10] A. Rinaldo and L. Wasserman. Generalized density clustering.

38(5):2678–2722  2010.

Annals of Statistics 

[11] A. Singh  C. Scott  and R. Nowak. Adaptive hausdorff estimation of density level sets. Annals

of Statistics  37(5B):2760–2782  2009.

[12] W. Stuetzle and R. Nugent. A generalized single linkage method for estimating the cluster tree

of a density. Journal of Computational and Graphical Statistics  19(2):397–418  2010.

[13] D. Wishart. Mode analysis: a generalization of nearest neighbor which reduces chaining ef-
fects. In Proceedings of the Colloquium on Numerical Taxonomy held in the University of St.
Andrews  pages 282–308  1969.

[14] M.A. Wong and T. Lane. A kth nearest neighbour clustering procedure. Journal of the Royal

Statistical Society Series B  45(3):362–368  1983.

[15] B. Yu. Assouad  Fano and Le Cam. Festschrift for Lucien Le Cam  pages 423–435  1997.

9

,Zi Yin
Yuanyuan Shen