2015,A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA,Independent Component Analysis (ICA) is a popular model for blind signal separation. The ICA model assumes that a number of independent source signals are linearly mixed to form the observed signals. We propose a new algorithm  PEGI (for pseudo-Euclidean Gradient Iteration)  for provable model recovery for ICA with Gaussian noise. The main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-Euclidean (indefinite “inner product”) space. The use of this indefinite “inner product” resolves technical issues common to several existing algorithms for noisy ICA. This leads to an algorithm which is conceptually simple  efficient and accurate in testing.Our second contribution is combining PEGI with the analysis of objectives for optimal recovery in the noisy ICA model. It has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural Signal to Interference plus Noise Ratio (SINR) criterion. There have been several partial solutions proposed in the ICA literature. It turns out that any solution to the mixing matrix reconstruction problem can be used to construct an SINR-optimal ICA demixing  despite the fact that SINR itself cannot be computed from data. That allows us to obtain a practical and provably SINR-optimal recovery method for ICA with arbitrary Gaussian noise.,A Pseudo-Euclidean Iteration for Optimal Recovery

in Noisy ICA

James Voss

The Ohio State University
vossj@cse.ohio-state.edu

Mikhail Belkin

The Ohio State University
mbelkin@cse.ohio-state.edu

Luis Rademacher

The Ohio State University

lrademac@cse.ohio-state.edu

Abstract

Independent Component Analysis (ICA) is a popular model for blind signal sepa-
ration. The ICA model assumes that a number of independent source signals are
linearly mixed to form the observed signals. We propose a new algorithm  PEGI
(for pseudo-Euclidean Gradient Iteration)  for provable model recovery for ICA
with Gaussian noise. The main technical innovation of the algorithm is to use a
ﬁxed point iteration in a pseudo-Euclidean (indeﬁnite “inner product”) space. The
use of this indeﬁnite “inner product” resolves technical issues common to several
existing algorithms for noisy ICA. This leads to an algorithm which is conceptually
simple  efﬁcient and accurate in testing.
Our second contribution is combining PEGI with the analysis of objectives for
optimal recovery in the noisy ICA model. It has been observed that the direct
approach of demixing with the inverse of the mixing matrix is suboptimal for signal
recovery in terms of the natural Signal to Interference plus Noise Ratio (SINR)
criterion. There have been several partial solutions proposed in the ICA literature.
It turns out that any solution to the mixing matrix reconstruction problem can be
used to construct an SINR-optimal ICA demixing  despite the fact that SINR itself
cannot be computed from data. That allows us to obtain a practical and provably
SINR-optimal recovery method for ICA with arbitrary Gaussian noise.

Introduction

X = (cid:80)m

1
Independent Component Analysis refers to a class of methods aiming at recovering statistically
independent signals by observing their unknown linear combination. There is an extensive literature
on this and a number of related problems [7].
In the ICA model  we observe n-dimensional realizations x(1)  . . .   x(N ) of a latent variable model
k=1 SkAk = AS where Ak denotes the kth column of the n × m mixing matrix A and
S = (S1  . . .   Sm)T is the unseen latent random vector of “signals”. It is assumed that S1  . . .   Sm
are independent and non-Gaussian. The source signals and entries of A may be either real- or
complex-valued. For simplicity  we will assume throughout that S has zero mean  as this may be
achieved in practice by centering the observed data.
Many ICA algorithms use the preprocessing “whitening” step whose goal is to orthogonalize the
independent components. In the noiseless  case this is commonly done by computing the square
root of the covariance matrix of X. Consider now the noisy ICA model X = AS + η with additive
0-mean noise η independent of S. It turns out that the introduction of noise makes accurate recovery
of the signals signiﬁcantly more involved. Speciﬁcally  whitening using the covariance matrix does
not work in the noisy ICA model as the covariance matrix combines both signal and noise. For
the case when the noise is Gaussian  matrices constructed from higher order statistics (speciﬁcally 
cumulants) can be used instead of the covariance matrix. However  these matrices are not in general
positive deﬁnite and thus the square root cannot always be extracted. This limits the applicability of

1

several previous methods  such as [1  2  9]. The GI-ICA algorithm proposed in [21] addresses this
issue by using a complicated quasi-orthogonalization step followed by an iterative method.
In this paper (section 2)  we develop a simple and practical one-step algorithm  PEGI (for pseudo-
Euclidean Gradient Iteration) for provably recovering A (up to the unavoidable ambiguities of the
model) in the case when the noise is Gaussian (with an arbitrary  unknown covariance matrix). The
main technical innovation of our approach is to formulate the recovery problem as a ﬁxed point
method in an indeﬁnite (pseudo-Euclidean) “inner product” space.
The second contribution of the paper is combining PEGI with the analysis of objectives for optimal
recovery in the noisy ICA model. In most applications of ICA (e.g.  speech separation [18]  MEG/EEG
artifact removal [20] and others) one cares about recovering the signals s(1)  . . .   s(N ). This is known
as the source recovery problem. This is typically done by ﬁrst recovering the matrix A (up to an
appropriate scaling of the column directions).
At ﬁrst  source recovery and recovering the mixing matrix A appear to be essentially equivalent. In
the noiseless ICA model  if A in invertible1 then s(t) = A−1x(t) recovers the sources. On the other
hand  in the noisy model  the exact recovery of the latent sources s(t) becomes impossible even if A
is known exactly. Part of the “noise” can be incorporated into the “signal” preserving the form of the
model. Even worse  neither A nor S are deﬁned uniquely as there is an inherent ambiguity in the
setting. There could be many equivalent decompositions of the observed signal as X = A(cid:48)S(cid:48) + η(cid:48)
(see the discussion in section 3).
We consider recovered signals of the form ˆS(B) := BX for a choice of m × n demixing matrix B.
Signal recovery is considered optimal if the coordinates of ˆS(B) = ( ˆS1(B)  . . .   ˆSm(B)) maximize
Signal to Interference plus Noise Ratio (SINR) within any ﬁxed model X = AS + η. Note that
the value of SINR depends on the decomposition of the observed data into “noise” and “signal”:
X = A(cid:48)S(cid:48) + η(cid:48).
Surprisingly  the SINR optimal demixing matrix does not depend on the decomposition of data into
signal plus noise. As such  SINR optimal ICA recovery is well deﬁned given access to data despite
the inherent ambiguity in the model. Further  it will be seen that the SINR optimal demixing can be
constructed from cov(X) and the directions of the columns of A (which are also well-deﬁned across
signal/noise decompositions).
Our SINR-optimal demixing approach combined with the PEGI algorithm provides a complete
SINR-optimal recovery algorithm in the ICA model with arbitrary Gaussian noise. We note that the
ICA papers of which we are aware that discuss optimal demixing do not observe that SINR optimal
demixing is invariant to the choice of signal/noise decomposition. Instead  they propose more limited
strategies for improving the demixing quality within a ﬁxed ICA model. For instance  Joho et al.
[14] show how SINR-optimal demixing can be approximated with extra sensors when assuming a
white additive noise  and Koldovsk`y and Tichavsk`y [16] discuss how to achieve asymptotically low
bias ICA demixing assuming white noise within a ﬁxed ICA model. However  the invariance of the
SINR-optimal demixing matrix appears in the array sensor systems literature [6].
Finally  in section 4  we demonstrate experimentally that our proposed algorithm for ICA outperforms
existing practical algorithms at the task of noisy signal recovery  including those speciﬁcally designed
for beamforming  when given sufﬁciently many samples. Moreover  most existing practical algorithms
for noisy source recovery have a bias and cannot recover the optimal demixing matrix even with
inﬁnite samples. We also show that PEGI requires signiﬁcantly fewer samples than GI-ICA [21] to
perform ICA accurately.

1.1 The Indeterminacies of ICA
Notation: We use M∗ to denote the entry-wise complex conjugate of a matrix M  M T to denote its
transpose  M H to denote its conjugate transpose  and M† to denote its Moore-Penrose pseudoinverse.
Before proceeding with our results  we discuss the somewhat subtle issue of indeterminacies in ICA.
These ambiguities arise from the fact that the observed X may have multiple decompositions into
ICA models X = AS + η and X = A(cid:48)S(cid:48) + η(cid:48).

1A−1 can be replaced with A† (A’s pseudoinverse) in the discussion below for over-determined ICA.

2

ICA models X =(cid:80)m

k=1 SkAk and X =(cid:80)m

Noise-free ICA has two natural indeterminacies. For any nonzero constant α  the contribution of
the kth component AkSk to the model can equivalently be obtained by replacing Ak with αAk and
α Sk. To lessen this scaling indeterminacy  we use the convention2 that
Sk with the rescaled signal 1
cov(S) = I throughout this paper. As such  each source Sk (or equivalently each Ak) is deﬁned up
to a choice of sign (a unit modulus factor in the complex case). In addition  there is an ambiguity
in the order of the latent signals. For any permutation π of [m] (where [m] := {1  . . .   m})  the
k=1 Sπ(k)Aπ(k) are indistinguishable. In the noise free
setting  A is said to be recovered if we recover each column of A up to a choice of sign (or up to a unit
modulus factor in the complex case) and an unknown permutation. As the sources S1  . . .   Sm are
only deﬁned up to the same indeterminacies  inverting the recovered matrix ˜A to obtain a demixing
matrix works for signal recovery.
In the noisy ICA setting  there is an additional indeterminacy in the deﬁnition of the sources. Consider
a 0-mean axis-aligned Gaussian random vector ξ. Then  the noisy ICA model X = A(S + ξ) + η in
which ξ is considered part of the latent source signal S(cid:48) = S + ξ  and the model X = AS + (Aξ + η)
in which ξ is part of the noise are indistinguishable. In particular  the latent source S and its covariance
are ill-deﬁned. Due to this extra indeterminacy  the lengths of the columns of A no longer have a fully
deﬁned meaning even when we assume cov(S) = I. In the noisy setting  A is said to be recovered if
we obtain the columns of A up to non-zero scalar multiplicative factors and an arbitrary permutation.
The last indeterminacy is the most troubling as it suggests that the power of each source signal is itself
ill-deﬁned in the noisy setting. Despite this indeterminacy  it is possible to perform an SINR-optimal
demixing without additional assumptions about what portion of the signal is source and what portion
is noise. In section 3  we will see that SINR-optimal source recovery takes on a simple form: Given
any solution ˜A which recovers A up to the inherent ambiguities of noisy ICA  then ˜AH cov(X)† is
an SINR-optimal demixing matrix.

1.2 Related Work and Contributions
Independent Component Analysis is probably the most used model for Blind Signal Separation.
It has seen numerous applications and has generated a vast literature  including in the noisy and
underdetermined settings. We refer the reader to the books [7  13] for a broad overview of the subject.
It was observed early on by Cardoso [4] that ICA algorithms based soley on higher order cumulant
statistics are invariant to additive Gaussian noise. This observation has allowed the creation of many
algorithms for recovering the ICA mixing matrix in the noisy and often underdetermined settings.
Despite the signiﬁcant work on noisy ICA algorithms  they remain less efﬁcient  more specialized  or
less practical than the most popular noise free ICA algorithms.
Research on cumulant-based noisy ICA can largely be split into several lines of work which we only
highlight here. Some algorithms such as FOOBI [4] and BIOME [1] directly use the tensor structure
of higher order cumulants. In another line of work  De Lathauwer et al. [8] and Yeredor [23] have
suggested algorithms which jointly diagonalize cumulant matrices in a manner reminiscent of the
noise-free JADE algorithm [3]. In addition  Yeredor [22] and Goyal et al. [11] have proposed ICA
algorithms based on random directional derivatives of the second characteristic function.
Each line of work has its advantages and disadvantages. The joint diagonalization algorithms and
the tensor based algorithms tend to be practical in the sense that they use redundant cumulant infor-
mation in order to achieve more accurate results. However  they have a higher memory complexity
than popular noise free ICA algorithms such as FastICA [12]. While the tensor methods (FOOBI
and BIOME) can be used when there are more sources than the dimensionality of the space (the
underdetermined ICA setting)  they require all the latent source signals to have positive order 2k
cumulants (k ≥ 2  a predetermined ﬁxed integer) as they rely on taking a matrix square root. Finally 
the methods based on random directional derivatives of the second characteristic function rely heavily
upon randomness in a manner not required by the most popular noise free ICA algorithms.
We continue a line of research started by Arora et al. [2] and Voss et al. [21] on fully determined noisy
ICA which addresses some of these practical issues by using a deﬂationary approach reminiscent
of FastICA. Their algorithms thus have lower memory complexity and are more scalable to high
dimensional data than the joint diagonalization and tensor methods. However  both works require

2Alternatively  one may place the scaling information in the signals by setting (cid:107)Ak(cid:107) = 1 for each k.

3

a preprocessing step (quasi-orthogonalization) to orthogonalize the latent signals which is based
on taking a matrix square root. Arora et al. [2] require each latent signal to have positive fourth
cumulant in order to carry out this preprocessing step. In contrast  Voss et al. [21] are able to
perform quasi-orthogonalization with source signals of mixed sign fourth cumulants; but their quase-
orthogonalization step is more complicated and can run into numerical issues under sampling error.
We demonstrate that quasi-orthogonalization is unnecessary. We introduce the PEGI algorithm to
work within a (not necessarily positive deﬁnite) inner product space instead. Experimentally  this
leads to improved demixing performance. In addition  we handle the case of complex signals.
Finally  another line of work attempts to perform SINR-optimal source recovery in the noisy ICA
setting. It was noted by Koldovsk`y and Tichavsk`y [15] that for noisy ICA  traditional ICA algorithms
such as FastICA and JADE actually outperform algorithms which ﬁrst recover A in the noisy setting
and then use the resulting approximation of A† to perform demixing. It was further observed that
A† is not the optimal demixing matrix for source recovery. Later  Koldovsk`y and Tichavsk`y [17]
proposed an algorithm based on FastICA which performs a low SINR-bias beamforming.

2 Pseudo-Euclidean Gradient Iteration ICA
In this section  we introduce the PEGI algorithm for recovering A in the “fully determined” noisy
ICA setting where m ≤ n. PEGI relies on the idea of Gradient Iteration introduced Voss et al. [21].
However  unlike GI-ICA Voss et al. [21]  PEGI does not require the source signals to be orthogonal-
ized. As such  PEGI does not require the complicated quasi-orthogonalization preprocessing step of
GI-ICA which can be inaccurate to compute in practice. We sketch the Gradient Iteration algorithm
in Section 2.1  and then introduce PEGI in Section 2.2. For simplicity  we limit this discussion to
the case of real-valued signals. A mild variation of our PEGI algorithm works for complex-valued
signals  and its construction is provided in the supplementary material.
In this section we assume a noisy ICA model X = AS + η such that η is arbitrary Gaussian and
independent of S. We also assume that m ≤ n  that m is known  and that the columns of A are
linearly independent.

2.1 Gradient Iteration with Orthogonality
The gradient iteration relies on the properties of cumulants. We will focus on the fourth cumulant 
though similar constructions may be given using other even order cumulants of higher order. For
a zero-mean random variable X  the fourth order cumulant may be deﬁned as κ4(X) := E[X 4] −
3E[X 2]2 [see 7  Chapter 5  Section 1.2]. Higher order cumulants have nice algebraic properties
which make them useful for ICA. In particular  κ4 has the following properties: (1) (Independence) If
X and Y are independent  then κ4(X + Y ) = κ4(X) + κ4(Y ). (2) (Homogeneity) If α is a scalar 
then κ4(αX) = α4κ4(X). (3) (Vanishing Gaussians) If X is normally distributed then κ4(X) = 0.
We consider the following function deﬁned on the unit sphere: f (u) := κ4((cid:104)X  u(cid:105)). Expanding f (u)
using the above properties we obtain:

k=1

k = AD(u)AT

(2)
where D(u) is a diagonal matrix with entries D(u)kk = 12(cid:104)Ak  u(cid:105)2κ4(Sk). We also note that f (u) 
∇f (u)  and Hf (u) have natural sample estimates (see [21]).
Voss et al. [21] introduced GI-ICA as a ﬁxed point algorithm under the assumption that the
columns of A are orthogonal but not necessarily unit vectors. The main idea is that the update
u ← ∇f (u)/(cid:107)∇f (u)(cid:107) is a form of a generalized power iteration. From equation (1)  each Ak may
be considered as a direction in a hidden orthogonal basis of the space. During each iteration  the Ak
coordinate of u is raised to the 3rd power and multiplied by a constant. Treating this iteration as a
ﬁxed point update  it was shown that given a random starting point  this iterative procedure converges
rapidly to one of the columns of A (up to a choice of sign). The rate of convergence is cubic.

4

f (u) = κ4

Taking derivatives we obtain:

(cid:0)(cid:88)m

k=1

(cid:88)m

(cid:104)Ak  u(cid:105)Sk + (cid:104)u  η(cid:105)(cid:1) =
(cid:88)m
(cid:88)m

(cid:104)Ak  u(cid:105)3κ4(Sk)Ak
(cid:104)Ak  u(cid:105)2κ4(Sk)AkAT

k=1

∇f (u) = 4
Hf (u) = 12

k=1

(cid:104)Ak  u(cid:105)4κ4(Sk) .

(1)

However  the GI-ICA algorithm requires a somewhat complicated preprocessing step called
quasi-orthogonalization to linearly transform the data to make columns of A orthogonal. Quasi-
orthogonalization makes use of evaluations of Hessians of the fourth cumulant function to construct
a matrix of the form C = ADAT where D has all positive diagonal entries—a task which is com-
plicated by the possibility that the latent signals Si may have fourth order cumulants of differing
signs—and requires taking the matrix square root of a positive deﬁnite matrix of this form. How-
ever  the algorithm used for constructing C under sampling error is not always positive deﬁnite in
practice  which can make the preprocessing step fail. We will show how our PEGI algorithm makes
quasi-orthogonalization unnecessary  in particular  resolving this issue.

2.2 Gradient Iteration in a Pseudo-Euclidean Space
We now show that the gradient iteration can be performed using in a pseudo-Euclidean space
in which the columns of A are orthogonal. The natural candidate for the “inner product space”
would be to use (cid:104)· ·(cid:105)∗ deﬁned as (cid:104)u  v(cid:105)∗ := uT (AAT )†v. Clearly  (cid:104)Ai  Aj(cid:105)∗ = δij gives the
desired orthogonality property. However  there are two issues with this “inner product space”:
First  it is only an inner product space when A is invertible. This turns out not to be a major
issue  and we move forward largely ignoring this point. The second issue is more fundamen-
tal: We only have access to AAT in the noise free setting where cov(X) = AAT . In the noisy
setting  we have access to matrices of the form Hf (u) = AD(u)AT from equation (2) instead.
We consider a pseudo-Euclidean inner product de-
Algorithm 1 Recovers a column of A up to a
ﬁned as follows: Let C = ADAT where D is a
scaling factor if u0 is generically chosen.
diagonal matrix with non-zero diagonal entries  and
deﬁne (cid:104)· ·(cid:105)C by (cid:104)u  v(cid:105)C = uT C†v. When D con-
tains negative entries  this is not a proper inner prod-
uct since C is not positive deﬁnite.
In particular 
(cid:104)Ak  Ak(cid:105)C = AT
k (ADAT )†Ak = d−1
kk may be neg-
ative. Nevertheless  when k (cid:54)= j  (cid:104)Ak  Aj(cid:105)C =
k (ADAT )†Aj = 0 gives that the columns of A
AT
are orthogonal in this space.
We deﬁne functions αk : Rn → R by αk(u) = (A†u)k such that for any u ∈ span(A1  . . .   Am) 
i=1 αi(u)Ai is the expansion of u in its Ai basis. Continuing from equation (1)  for any
Cκ4(Sk)Ak is the

then u =(cid:80)m
u ∈ Sn−1 we see ∇f (C†u) = 4(cid:80)n

Inputs: Unit vector u0  C  ∇f
k ← 1
repeat

uk ← ∇f (C†uk−1)/(cid:107)∇f (C†uk−1)(cid:107)
k ← k + 1

until Convergence (up to sign)
return uk

k=1(cid:104)Ak  u(cid:105)3
gradient iteration recast in the (cid:104)· ·(cid:105)C space. Expanding u in its Ak basis  we obtain
αk(u)3(d−3

(αk(u)(cid:104)Ak  Ak(cid:105)C)3κ4(Sk)Ak = 4

k=1(cid:104)Ak  C†u(cid:105)3κ4(Sk)Ak = 4(cid:80)n
(cid:88)m

∇f (C†u) = 4

(cid:88)m

kk κ4(Sk))Ak  

(3)

k=1

k=1

k=0 converges to v up to sign if there exists a sequence {ck}∞

which is a power iteration in the unseen Ak coordinate system. As no assumptions are made upon the
κ4(Sk) values  the d−3
kk scalings which were not present in eq. (1) cause no issues. Using this update 
we obtain Alg. 1  a ﬁxed point method for recovering a single column of A up to an unknown scaling.
Before proceeding  we should clarify the notion of ﬁxed point convergence in Algorithm 1. We say
that the sequence {uk}∞
k=0 such that
each ck ∈ {±1} and ckuk → v as k → ∞. We have the following convergence guarantee.
Theorem 1. If u0 is chosen uniformly at random from Sn−1  then with probability 1  there exists
(cid:96) ∈ [m] such that the sequence {uk}∞
k=0 deﬁned as in Algorithm 1 converges to A(cid:96)/(cid:107)A(cid:96)(cid:107) up to sign.
Further  the rate of convergence is cubic.
Due to limited space  we omit the proof of Theorem 1. It is similar to the proof of [21  Theorem 4].
In practice  we test near convergence by checking if we are still making signiﬁcant progress. In
particular  for some predeﬁned  > 0  if there exists a sign value ck ∈ {±1} such that (cid:107)uk −
ckuk−1(cid:107) <   then we declare convergence achieved and return the result. As there are only two
choices for ck  this is easily checked  and we exit the loop if this condition is met.
Full ICA Recovery Via the Pseudo-Euclidean GI-Update. We are able to recover a single column
of A up to its unknown scale. However  for full recovery of A  we would like (given recovered
columns A(cid:96)1   . . .   A(cid:96)j ) to be able to recover a column Ak such that k (cid:54)∈ {(cid:96)1  . . .   (cid:96)j} on demand.
The idea behind the simultaneous recovery of all columns of A is two-fold. First  instead of just
ﬁnding columns of A using Algorithm 1  we simultaneously ﬁnd rows of A†. Then  using the

5

†
k· up to an arbitrary  unknown constant d−1

(cid:80)m

recovered columns of A and rows of A†  we project u onto the orthogonal complement of the
recovered columns of A within the (cid:104)· ·(cid:105)C pseudo-inner product space.
Recovering rows of A†. Suppose we have access to a column Ak (which may be achieved using
†
k· denote the kth row of A†. Then  we note that C†Ak = (ADAT )†Ak =
Algorithm 1). Let A
†
†
d−1
k = d−1
kk . However  the
k·)T recovers A
kk (AT )
kk (A
kk may be recovered by noting that (cid:104)Ak  Ak(cid:105)C = (C†Ak)T Ak = d−1
constant d−1
kk . As such  we may
†
k· as [C†Ak/((C†Ak)T Ak)]T .
estimate A
Enforcing Orthogonality During the GI
Algorithm 2 Full ICA matrix recovery algorithm.
Update. Given access to a vector u =
Returns two matrices: (1) ˜A is the recovered mix-
k=1 αk(u)Ak + PA⊥ u (where PA⊥ is the
ing matrix for the noisy ICA model X = AS + η 
projection onto the orthogonal complements
and (2) ˜B is a running estimate of ˜A†.
of the range of A)  some recovered columns
1: Inputs: C  ∇f
A(cid:96)1   . . .   A(cid:96)r  and corresponding rows of A† 
2: ˜A ← 0  ˜B ← 0
we may zero out the components of u corre-
3: for j ← 1 to m do
sponding to the recovered columns of A. Let-
†
(cid:96)j·u  then u(cid:48) =
4:
5:
k∈[m]\{(cid:96)1 ... (cid:96)r} αk(u)Ak + PA⊥ u. In partic-
ular  u(cid:48) is orthogonal (in the (cid:104)· ·(cid:105)C space) to the
6:
7:
previously recovered columns of A. This allows
8:
the non-orthogonal gradient iteration algorithm
9:
to recover a new column of A.
10:
Using these ideas  we obtain Algorithm 2  which
11: end for
is the PEGI algorithm for recovery of the mix-
12: return ˜A  ˜B
ing matrix A in noisy ICA up to the inherent
(cid:80)n
ambiguities of the problem. Within this Algorithm  step 6 enforces orthogonality with previously
found columns of A  guaranteeing that convergence to a new column of A.
k=1 Hf (ek)  as it can be
Practical Construction of C. In our implementation  we set C = 1
k=1 Hf (ek) = ADAT with dkk = (cid:107)Ak(cid:107)2κ4(Sk). This determinis-
12

shown from equation (2) that(cid:80)n

until Convergence (up to sign)
˜Aj ← u
˜Bj· ← [C†Aj/((C†Aj)T Aj)]T

Draw u uniformly at random from Sn−1.
repeat

u ← u − ˜A ˜Bu
u ← ∇f (C†u)/(cid:107)∇f (C†u)(cid:107).

ting u(cid:48) = u −(cid:80)r
(cid:80)

j=1 A(cid:96)j A

tically guarantees that each latent signal has a signiﬁcant contribution to C.

3 SINR Optimal Recovery in Noisy ICA
In this section  we demonstrate how to perform SINR optimal ICA within the noisy ICA framework
given access to an algorithm (such as PEGI) to recover the directions of the columns of A. To this
end  we ﬁrst discuss the SINR optimal demixing solution within any decomposition of the ICA model
into signal and noise as X = AS + η. We then demonstrate that the SINR optimal demixing matrix
is actually the same across all possible model decompositions  and that it can be recovered. The
results in this section hold in greater generality than in section 2. They hold even if m ≥ n (the
underdetermined setting) and even if the additive noise η is non-Gaussian.
Consider B an m × n demixing matrix  and deﬁne ˆS(B) := BX the resulting approximation to
S. It will also be convenient to estimate the source signal S one coordinate at a time: Given a row
vector b  we deﬁne ˆS(b) := bX. If b = Bk· (the kth row of B)  then ˆS(b) = [ˆS(B)]k = ˆSk(B)
is our estimate to the kth latent signal Sk. Within a speciﬁc ICA model X = AS + η  signal to
intereference-plus-noise ratio (SINR) is deﬁned by the following equation:

SINRk(b) :=

var(bAkSk)

var(bAS − bAkSk) + var(bη)

=

var(bAkSk)

var(bAX) − var(bAkSk)

.

(4)

SINRk is the variance of the contribution of kth source divided by the variance of the noise and
interference contributions within the signal.
Given access to the mixing matrix A  we deﬁne Bopt = AH (AAH + cov(η))†. Since cov(X) =
AAH + cov(η)  it follows that Bopt = AH cov(X)†. Here  cov(X)† may be estimated from data 
but due to the ambiguities of the noisy ICA model  A (and speciﬁcally its column norms) cannot be.
Koldovsk`y and Tichavsk`y [15] observed that when η is a white Gaussian noise  Bopt jointly maxi-
mizes SINRk for each k ∈ [m]  i.e.  SINRk takes on its maximal value at (Bopt)k·. We generalize
this result in Proposition 2 below to include arbitrary non-spherical  potentially non-Gaussian noise.

6

(a) Accuracy under additive Gaussian noise.

(b) Bias under additive Gaussian noise.

Figure 1: SINR performance comparison of ICA algorithms.

It is interesting to note that even after the data is whitened  i.e. cov(X) = I  the optimal SINR
solution is different from the optimal solution in the noiseless case unless A is an orthogonal matrix 
i.e. A† = AH. This is generally not the case  even if η is white Gaussian noise.
Proposition 2. For each k ∈ [m]  (Bopt)k· is a maximizer of SINRk.
The proof of Proposition 2 can be found in the supplementary material.
Since SINR is scale invariant  Proposition 2 implies that any matrix of the form DBopt =
DAH cov(X)† where D is a diagonal scaling matrix (with non-zero diagonal entries) is an SINR-
optimal demixing matrix. More formally  we have the following result.
Theorem 3. Let ˜A be an n × m matrix containing the columns of A up to scale and an arbitrary
permutation. Then  ( ˜AH cov(X)†)π(k)· is a maximizer of SINRk.
By Theorem 3  given access to a matrix ˜A which recovers the directions of the columns of A  then
˜AH cov(X)† is the SINR-optimal demixing matrix. For ICA in the presence of Gaussian noise  the
directions of the columns of A are well deﬁned simply from X  that is  the directions of the columns
of A do not depend on the decomposition of X into signal and noise (see the discussion in section 1.1
on ICA indeterminacies). The problem of SINR optimal demixing is thus well deﬁned for ICA in
the presence of Gaussian noise  and the SINR optimal demixing matrix can be estimated from data
without any additional assumptions on the magnitude of the noise in the data.
Finally  we note that in the noise-free case  the SINR-optimal source recovery simpliﬁes to be ˜A†.
Corollary 4. Suppose that X = AS is a noise free (possibly underdetermined) ICA model. Suppose
that ˜A ∈ Rn×m contains the columns of A up to scale and permutation  i.e.  there exists diagonal
matrix D with non-zero entries and a permutation matrix Π such that ˜A = ADΠ. Then ˜A† is an
SINR-optimal demixing matrix.
Corollary 4 is consistent with known beamforming results. In particular  it is known that A† is optimal
(in terms of minimum mean squared error) for underdetermined ICA [19  section 3B].

4 Experimental Results
We compare the proposed PEGI algorithm with existing ICA algorithms. In addition to qorth+GI-ICA
(i.e.  GI-ICA with quasi-orthogonalization for preprocessing)  we use the following baselines:
JADE [3] is a popular fourth cumulant based ICA algorithm designed for the noise free setting. We
use the implementation of Cardoso and Souloumiac [5].
FastICA [12] is a popular ICA algorithm designed for the noise free setting based on a deﬂationary
approach of recovering one component at a time. We use the implementation of G¨avert et al. [10].
1FICA [16  17] is a variation of FastICA with the tanh contrast function designed to have low bias
for performing SINR-optimal beamforming in the presence of Gaussian noise.
Ainv performs oracle demixing algorithm which uses A† as the demixing matrix.
SINR-opt performs oracle demixing using AH cov(X)† to achieve SINR-optimal demixing.

7

We compare these algorithms on simulated data with n = m. We constructed mixing matrices A
with condition number 3 via a reverse singular value decomposition (A = U ΛV T ). The matrices U
and V were random orthogonal matrices  and Λ was chosen to have 1 as its minimum and 3 as its
maximum singular values  with the intermediate singular values chosen uniformly at random. We
drew data from a noisy ICA model X = AS + η where cov(η) = Σ was chosen to be malaligned
with cov(AS) = AAT . We set Σ = p(10I − AAT ) where p is a constant deﬁning the noise power.
It can be shown that p = maxv var(vT η)
maxv var(vT AS) is the ratio of the maximum directional noise variance to
the maximum directional signal variance. We generated 100 matrices A for our experiments with
100 corresponding ICA data sets for each sample size and noise power. When reporting results  we
apply each algorithm to each of the 100 data sets for the corresponding sample size and noise power
and we report the mean performance. The source distributions used in our ICA experiments were the
Laplace and Bernoulli distribution with parameters 0.05 and 0.5 respectively  the t-distribution with
3 and 5 degrees of freedom respectively  the exponential distribution  and the uniform distribution.
Each distribution was normalized to have unit variance  and the distributions were each used twice to
create 14-dimensional data. We compare the algorithms using either SINR or the SINR loss from the
optimal demixing matrix (deﬁned by SINR Loss = [Optimal SINR − Achieved SINR]).
In Figure 1b  we compare our proprosed ICA algorithm with various ICA algorithms for signal
recovery.
In the PEGI-κ4+SINR algorithm  we use PEGI-κ4 to estimate A  and then perform
demixing using the resulting estimate of AH cov(X)−1  the formula for SINR-optimal demixing. It
is apparent that when given sufﬁcient samples  PEGI-κ4+SINR provides the best SINR demixing.
JADE  FastICA-tanh  and 1FICA each have a bias in the presence of additive Gaussian noise which
keeps them from being SINR-optimal even when given many samples.

Figure 2: Accuracy comparison of PEGI using
pseudo-inner product spaces and GI-ICA using
quasi-orthogonalization.

In Figure 1a  we compare algorithms at vari-
ous sample sizes. The PEGI-κ4+SINR algo-
rithm relies more heavily on accurate estimates
of fourth order statistics than JADE  and the
FastICA-tanh and 1FICA algorithms do not re-
quire the estimation of fourth order statistics.
For this reason  PEGI-κ4+SINR requires more
samples than the other algorithms in order to be
run accurately. However  once sufﬁcient sam-
ples are taken  PEGI-κ4+SINR outperforms the
other algorithms including 1FICA  which is de-
signed to have low SINR bias. We also note
that while not reported in order to avoid clut-
ter  the kurtosis-based FastICA performed very
similarly to FastICA-tanh in our experiments.
In order to avoid clutter  we did not include
qorth+GI-ICA-κ4+SINR (the SINR optimal
demixing estimate constructed using qorth+GI-
ICA-κ4 to estimate A) in the ﬁgures 1b and 1a. It is also assymptotically unbiased in estimating
the directions of the columns of A  and similar conclusions could be drawn using qorth+GI-ICA-κ4
in place of PEGI-κ4. However  in Figure 2  we see that PEGI-κ4+SINR requires fewer samples
than qorth+GI-ICA-κ4+SINR to achieve good performance. This is particularly highlighted in the
medium sample regime.
On the Performance of Traditional ICA Algorithms for Noisy ICA. An interesting observation
[ﬁrst made in 15] is that the popular noise free ICA algorithms JADE and FastICA perform reasonably
well in the noisy setting. In Figures 1b and 1a  they signiﬁcantly outperform demixing using A−1 for
source recovery. It turns out that this may be explained by a shared preprocessing step. Both JADE
and FastICA rely on a whitening preprocessing step in which the data are linearly transformed to
have identity covariance. It can be shown in the noise free setting that after whitening  the mixing
matrix A is a rotation matrix. These algorithms proceed by recovering an orthogonal matrix ˜A to
approximate the true mixing matrix A. Demixing is performed using ˜A−1 = ˜AH. Since the data is
white (has identity covariance)  then the demixing matrix ˜AH = ˜AH cov(X)−1 is an estimate of the
SINR-optimal demixing matrix. Nevertheless  the traditional ICA algorithms give a biased estimate
of A under additive Gaussian noise.

8

References
[1] L. Albera  A. Ferr´eol  P. Comon  and P. Chevalier. Blind identiﬁcation of overcomplete mixtures of sources

(BIOME). Linear algebra and its applications  391:3–30  2004.

[2] S. Arora  R. Ge  A. Moitra  and S. Sachdeva. Provable ICA with unknown Gaussian noise  with implications

for Gaussian mixtures and autoencoders. In NIPS  pages 2384–2392  2012.

[3] J. Cardoso and A. Souloumiac. Blind beamforming for non-Gaussian signals. In Radar and Signal

Processing  IEE Proceedings F  volume 140(6)  pages 362–370. IET  1993.

[4] J.-F. Cardoso. Super-symmetric decomposition of the fourth-order cumulant tensor. Blind identiﬁcation of

more sources than sensors. In ICASSP  pages 3109–3112. IEEE  1991.

[5] J.-F. Cardoso and A. Souloumiac. Matlab JADE for real-valued data v 1.8. http://perso.
telecom-paristech.fr/˜cardoso/Algo/Jade/jadeR.m  2005. [Online; accessed 8-May-
2013].

[6] P. Chevalier. Optimal separation of independent narrow-band sources: Concept and performance 1. Signal

Processing  73(12):27 – 47  1999. ISSN 0165-1684.

[7] P. Comon and C. Jutten  editors. Handbook of Blind Source Separation. Academic Press  2010.

[8] L. De Lathauwer  B. De Moor  and J. Vandewalle. Independent component analysis based on higher-order
statistics only. In Statistical Signal and Array Processing  1996. Proceedings.  8th IEEE Signal Processing
Workshop on  pages 356–359. IEEE  1996.

[9] L. De Lathauwer  J. Castaing  and J. Cardoso. Fourth-order cumulant-based blind identiﬁcation of
underdetermined mixtures. Signal Processing  IEEE Transactions on  55(6):2965–2973  June 2007. ISSN
1053-587X. doi: 10.1109/TSP.2007.893943.

[10] H. G¨avert  J. Hurri  J. S¨arel¨a  and A. Hyv¨arinen. Matlab FastICA v 2.5. http://research.ics.

aalto.fi/ica/fastica/code/dlcode.shtml  2005. [Online; accessed 1-May-2013].

[11] N. Goyal  S. Vempala  and Y. Xiao. Fourier PCA and robust tensor decomposition. In STOC  pages

584–593  2014.

[12] A. Hyv¨arinen and E. Oja. Independent component analysis: Algorithms and applications. Neural Networks 

13(4-5):411–430  2000.

[13] A. Hyv¨arinen  J. Karhunen  and E. Oja. Independent component analysis. John Wiley & Sons  2001.

[14] M. Joho  H. Mathis  and R. H. Lambert. Overdetermined blind source separation: Using more sensors than
source signals in a noisy mixture. In Proc. International Conference on Independent Component Analysis
and Blind Signal Separation. Helsinki  Finland  pages 81–86  2000.

[15] Z. Koldovsk`y and P. Tichavsk`y. Methods of fair comparison of performance of linear ICA techniques in

presence of additive noise. In ICASSP  pages 873–876  2006.

[16] Z. Koldovsk`y and P. Tichavsk`y. Asymptotic analysis of bias of fastica-based algorithms in presence of

additive noise. Technical report  Technical report  2007.

[17] Z. Koldovsk`y and P. Tichavsk`y. Blind instantaneous noisy mixture separation with best interference-plus-
noise rejection. In Independent Component Analysis and Signal Separation  pages 730–737. Springer 
2007.

[18] S. Makino  T.-W. Lee  and H. Sawada. Blind speech separation. Springer  2007.

[19] B. D. Van Veen and K. M. Buckley. Beamforming: A versatile approach to spatial ﬁltering. IEEE assp

magazine  5(2):4–24  1988.

[20] R. Vig´ario  J. Sarela  V. Jousmiki  M. Hamalainen  and E. Oja. Independent component approach to the
analysis of EEG and MEG recordings. Biomedical Engineering  IEEE Transactions on  47(5):589–593 
2000.

[21] J. R. Voss  L. Rademacher  and M. Belkin. Fast algorithms for Gaussian noise invariant independent
component analysis. In Advances in Neural Information Processing Systems 26  pages 2544–2552. 2013.

[22] A. Yeredor. Blind source separation via the second characteristic function. Signal Processing  80(5):

897–902  2000.

[23] A. Yeredor. Non-orthogonal joint diagonalization in the least-squares sense with application in blind source

separation. Signal Processing  IEEE Transactions on  50(7):1545–1553  2002.

9

,Mikhail Belkin
Luis Rademacher