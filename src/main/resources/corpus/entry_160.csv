2012,A Polylog Pivot Steps Simplex Algorithm for Classification,We present a simplex algorithm for linear programming in a linear classification formulation. The paramount complexity parameter in linear classification problems is called the margin. We prove that for margin values of practical interest our simplex variant performs a polylogarithmic number of pivot steps in the worst case  and its overall running time is near linear. This is in contrast to general linear programming  for which no sub-polynomial pivot rule is known.,A provably efﬁcient simplex algorithm for

classiﬁcation

Elad Hazan ∗

Haifa  32000

Technion - Israel Inst. of Tech.

ehazan@ie.technion.ac.il

Zohar Karnin
Yahoo! Research

Haifa

zkarnin@ymail.com

Abstract

We present a simplex algorithm for linear programming in a linear classiﬁcation
formulation. The paramount complexity parameter in linear classiﬁcation prob-
lems is called the margin. We prove that for margin values of practical interest
our simplex variant performs a polylogarithmic number of pivot steps in the worst
case  and its overall running time is near linear. This is in contrast to general linear
programming  for which no sub-polynomial pivot rule is known.

1

Introduction

Linear programming is a fundamental mathematical model with numerous applications in both com-
binatorial and continuous optimization. The simplex algorithm for linear programming is a corner-
stone of operations research. Despite being one of the most useful algorithms ever designed  not
much is known about its theoretical properties.
As of today  it is unknown whether a variant of the simplex algorithm (deﬁned by a pivot rule) exists
which makes it run in strongly polynomial time. Further  the simplex algorithm  being a geomet-
rical algorithm that is applied to polytopes deﬁned by linear programs  relates to deep questions in
geometry. Perhaps the most famous of which is the “polynomial Hirsh conjecture”  that states that
the diameter of a polytope is polynomial in its dimension and the number of its facets.
In this paper we analyze a simplex-based algorithm which is guaranteed to run in worst-case poly-
nomial time for large class of practically-interesting linear programs that arise in machine learn-
ing  namely linear classiﬁcation problems. Further  our simplex algorithm performs only a poly-
logarithmic number of pivot steps and overall near linear running time. The only previously known
poly-time simplex algorithm performs a polynomial number of pivot steps [KS06].

1.1 Related work

The simplex algorithm for linear programming was invented by Danzig [Dan51]. In the sixty years
that have passed  numerous attempts have been made to devise a polynomial time simplex algorithm.
Various authors have proved polynomial bounds on the number of pivot steps required by simplex
variants for inputs that are generated by various distributions  see e.g. [Meg86] as well as articles
referenced therein. However  worst case bounds have eluded researchers for many years.
A breakthrough in the theoretical analysis of the simplex algorithm was obtained by Spielman and
Teng [ST04]  who have shown that its smoothed complexity is polynomial  i.e.
that the expected
running time under a polynomially small perturbation of an arbitrary instance is polynomial. Kelner
and Spielman [KS06] have used similar techniques to provide for a worst-case polynomial time
simplex algorithm.

∗Work conducted at and funded by the Technion-Micorsoft Electronic Commerce Research Center

1

In this paper we take another step at explaining the success of the simplex algorithm - we show that
for one of the most important and widely used classes of linear programs a simplex algorithm runs
in near linear time.
We note that more efﬁcient algorithms for linear classiﬁcation exist  e.g. the optimal algorithm of
[CHW10]. The purpose of this paper is to expand our understanding of the simplex method  rather
than obtain a more efﬁcient algorithm for classiﬁcation.

2 Preliminaries

2.1 Linear classiﬁcation

Linear classiﬁcation is a fundamental primitive of machine learning  and is ubiquitous in applica-
tions. Formally  we are given a set of vectors-labels pairs {Ai  yi|i ∈ [n]}  such that Ai ∈ Rd  yi ∈
{−1  +1} has (cid:96)2 (Euclidean) norm at most one. The goal is to ﬁnd a hyperplane x ∈ Rd that parti-
tions the vectors into two disjoint subsets according to their sign  i.e. sign(Aix) = yi. W.l.o.g we
can assume that all labels are positive by negating the corresponding vectors of negative labels  i.e.
∀iyi = 1.
Linear classiﬁcation can be written as a linear program as follows:

ﬁnd x ∈ Rd s.t. ∀i ∈ [n] (cid:104)Ai  x(cid:105) > 0

(1)

The original linear classiﬁcation problem is then separable  i.e. there exists a separating hyperplane 
if and only if the above linear program has a feasible solution. Further  any linear program in stan-
dard form can be written in linear classiﬁcation form (1) by elementary manipulations and addition
of a single variable (see [DV08] for more details).
Henceforth we refer to a linear program in format (1) by its coefﬁcient matrix A. All vectors are
column vectors  and we denote inner products by (cid:104)x  y(cid:105). A parameter of paramount importance to
linear classiﬁcation is the margin  deﬁned as follows
Deﬁnition 1. The margin of a linear program in format (1)   such that ∀i(cid:107)Ai(cid:107) ≤ 1  is deﬁned as

We say that the instance A is a λ-margin LP.

λ = λ(A) = max
(cid:107)x(cid:107)≤1

min
i∈[n]

(cid:104)Ai  x(cid:105)

Notice that we have restricted x as well as the rows of A to have bounded norm  since otherwise the
margin is ill-deﬁned as it can change by scaling of x. Intuitively  the larger the margin  the easier
the linear program is to solve.
While any linear program can be converted to an equivalent one in form (1)  the margin can be ex-
ponentially small in the representation. However  in practical applications the margin is usually a
constant independent of the problem dimensions; a justiﬁcation is given next. Therefore we hence-
forth treat the margin as a separate parameter of the linear program  and devise efﬁcient algorithms
for solving it when the margin is a constant independent of the problem dimensions.

Support vector machines - why is the margin large ?
separable. This is due to many reasons  most prominently noise and modeling errors.
Hence practitioners settle for approximate linear classiﬁers. Finding a linear classiﬁer that mini-
mizes the number of classiﬁcation errors is NP-hard  and inapproximable [FGKP06]. The relaxation
of choice is to minimize the sum of errors  called “soft-margin SVM” (Support Vector Machine)
[CV95]  and is one of the most widely used algorithms in machine learning. Formally  a soft-margin
SVM instance is given by the following mathematical program:

In real-world problems the data is seldom

(2)

(cid:88)

i

min

ξi
∀i ∈ [n] yi((cid:104)x  Ai(cid:105) + b) + ξi ≥ 0
(cid:107)x(cid:107) ≤ 1

2

The norm constraint on x is usually taken to be the Euclidean norm  but other norms are also com-
mon such as the (cid:96)1 or (cid:96)∞ constraints that give rise to linear programs.
In this paper we discuss the separable case (formulation (1)) alone. The non-separable case turns out
to be much easier when we allow an additive loss of a small constant to the margin. We elaborate
on this point in Section 6.1. We will restrict our attention only to the case where the bounding norm
of x is the (cid:96)2 norm as it is the most common case.

2.2 Linear Programming and Smoothed analysis

Smoothed analysis was introduced in [ST04] to explain the excellent performance of the simplex
algorithm in practice. A σ-smooth LP is an LP where each coefﬁcient is perturbed by a Gaussian
noise of variance σ2.
In their seminal paper  Spielman and Teng proved the existence of a simplex algorithm that solves
σ-smooth LP in polynomial time (polynomial also in σ−1). Consequently  Vershynin [Ver09] pre-
sented a simpler algorithm and signiﬁcantly improved the running time. In the next sections we will
compare our results to the mentioned papers and point out a crucial lemma used in both papers that
will also be used here.

2.3 Statement of our results

For a separable SVM instance of n variables in a space of d dimensions and margin λ  we provide
a simplex algorithm with at most poly(log(n)  λ−1) many pivot steps. Our statement is given for
the (cid:96)2-SVM case  that is the case where the vector w (see Deﬁnition 1) has a bounded (cid:96)2 norm.

The algorithm achieves a solution with margin O((cid:112)log(n)/d) when viewed as a separator in the d
(cid:112)log n/d where c1 is some sufﬁciently large universal constant. Let 0 < ε < λ
in Rd is O((cid:112)log(n)/d). When projecting the data points onto V   the margin of the solution is λ−ε.

dimensional space. However  in an alternative yet (practically) equivalent view  the margin of the
solution is in fact arbitrarily close to λ.
Theorem 1. Let L be a separable (cid:96)2-SVM instance of dimension d with n examples and margin λ.
Assume that λ > c1
be a parameter. The simplex algorithm presented in this paper requires ˜O(nd) preprocessing time
and poly(ε−1  log(n)) pivot steps. The algorithm outputs a subspace V ⊆ Rd of dimension k =
Θ(log(n)/ε2) and a hyperplane within it. The margin of the solution when viewed as a hyperplane

In words  the above theorem states that when viewed as a classiﬁcation problem the obtained margin
is almost optimal. We note that when classifying a new point one does not have to project it to the
subspace V   but rather assign a sign according to the classifying hyperplane in Rd.

Tightness of the Generalization Bound In ﬁrst sight it seems that our result gives a week gener-
alization bound since the margin obtained in the original dimension is low. However  the margin of
the found solution in the reduced dimension (i.e.  within V ) is almost optimal (i.e. λ − ε where λ is
the optimal margin). It follows that the generalization bound essentially the same one obtained by
an exact solution.

LP perspective and the smoothed analysis framework As mentioned earlier  any linear program
can be viewed as a classiﬁcation LP by introducing a single new variable. Furthermore  any solution
with a positive margin translates into an optimal solution to the original LP. Our algorithm solves
the classiﬁcation LP in a sub-optimal manner in the sense that it does not ﬁnd a separator with an
optimal margin. However  in the perspective of a general LP solver1  the solution is optimal as any
positive margin sufﬁces. It stands to reason that in many practical settings the margin of the solution
is constant or polylogarithmically small at worst. In such cases  our simplex algorithm solves the LP
by using at most a polylogarithmic number of pivot steps. We further mention that without the large
margin assumption  in the smoothed analysis framework it is known ([BD02]  Lemma 6.2) that the
margin is w.h.p. polynomially bounded by the parameters. Hence  our algorithm runs in polynomial
time in the smoothed analysis framework as well.

1The statement is true only for feasibility LPs. However  any LP can be transformed into a feasibility LP by

performing a binary search for its solution value.

3

3 Our Techniques

The process involves ﬁve preliminary steps. Reducing the dimension  adding artiﬁcial constraints
to bound the norm of the solution  perturbing the low dimensional LP  ﬁnding a feasible point and
shifting the polytope. The process of reducing the dimension is standard. We use the Johnson and
Lindenstrauss Lemma [JL84] to reduce the dimension of the data points from d to k = O(log(n)/ε2)
where ε is an error parameter that can be considered as a constant. This step reduces the time
complexity by reducing both the number and running time of the pivot steps. In order to bound
the (cid:96)2 norm of the original vector  we bound the (cid:96)∞ norm of the low dimensional vector. This will

eventually result in a multiplicative loss of(cid:112)log(k) to the margin. We note that we could have

avoided this loss by bounding the (cid:96)1 norm of the vector at the cost of a more technically involved
proof. Speciﬁcally  one should bound the (cid:96)1 norm of the embedding of the vector into a space where
the (cid:96)1 and (cid:96)2 norms behave similarly  up to a multiplicative distortion of 1±ε. Such an embedding of
2 in (cid:96)K
1 exists for K = O(k/ε2) [Ind00]. Another side effect is a larger constant in the polynomial
(cid:96)k
dependence of ε in the running time.
The perturbation step involves adding a random Gaussian noise vector to the matrix of constraints 
where the amplitude of each row is determined by the norm of the corresponding constraint vector.
This step ensures the bound on the number of pivot step performed by the simplex algorithm. In
order to ﬁnd a feasible point we exploit the fact that when the margin is allowed to be negative  there
is always a feasible solution. We prove for a ﬁxed set of constraints  one of which is a negative lower
bound on the margin  that the corresponding point v0 is not only feasible but is the unique optimal
solution for ﬁxed direction. The direction is independent of the added noise  which is a necessary
property when bounding the number of pivot steps.
Our ﬁnal step is a shift of the polytope. Since we use the shadow vertex pivot rule we must have
an LP instance for which 0 is an interior point of the polytope. This property is not held for our
polytope as the LP contains inequalities of the form (cid:104)a  x(cid:105) ≥ 0. However  we prove that both 0
and v0 are feasible solution to the LP that do not share a common facet. Hence  their average is
an interior point of the polytope and a shift by −v0/2 would ensure that 0 is an interior point as
required.
Once the preprocessing is done we solve the LP via the shadow vertex method which is guaranteed
to ﬁnish after a polylogarithmic number of pivot steps. Given a sufﬁciently small additive noise and
sufﬁciently large target dimension we are guaranteed that the obtained solution is an almost optimal

solution to the unperturbed low dimensional problem and a ˜O((cid:112)k/d) approximation to the higher

dimensional problem.

4 Tool Set

4.1 Dimension reduction

The Johnson-Lindenstrauss Lemma [JL84] asserts that one can project vectors onto a lower dimen-
sional space and roughly preserve their norms  pairwise distances and inner products. The following
is an immediate consequence of Theorem 2.1 and Lemma 2.2 of [DG03].
Theorem 2. Let ε 0 and let k  d be integers where d > k > 9/ε2. Consider a linear projection
M : Rd (cid:55)→ Rk onto a uniformly chosen subspace2. For any pair of ﬁxed vector u  v ∈ Rd where
(cid:107)u(cid:107) (cid:107)v(cid:107) ≤ 1  it holds that

Pr(cid:2)(cid:12)(cid:12)(cid:107)u(cid:107)2 − (cid:107)M u(cid:107)2(cid:12)(cid:12) > ε(cid:3) < exp(−kε2/9)

Pr [|(cid:104)u  v(cid:105) − (cid:104)M u  M v(cid:105)| > 3ε] < 3 exp(−kε2/9)

4.2 The number of vertices in the shadow of a perturbed polytope

A key lemma in the papers of [ST04  Ver09] is a bound on the expected number of vertices in the
projection of a perturbed polytope onto a plane. The following geometric theorem is will be used in
our paper:

2Alternatively  M can be viewed as the composition of a random rotation U followed by taking the ﬁrst k

coordinates

4

Theorem 3 ([Ver09] Theorem 6.2). Let A1  ...  An be independent Gaussian vectors in Rd with
centers of norm at most 1  and whose varaince satisﬁes:

σ2 ≤

1

36d log n

Let E be a ﬁxed plane in Rd. Then the random polytope P = conv(0  A1  ...  An) satisﬁes

E[| edges(P ∩ E)|] = O(d3σ−4)

4.3 The shadow vertex method

The shadow vertex method is a pivot rule used to solve LPs. In order to apply it  the polytope of the
LP must have 0 as an interior point. Algebraically  all the inequalities must be of the form (cid:104)a  x(cid:105) ≤ 1
(or alternatively (cid:104)a  x(cid:105) ≤ b where b > 0). The input consists of a feasible point v in the polytope
and a direction u in which it is farthest  compared to all other feasible points. In a nutshell  the
method involves gradually turning the vector u towards the direction of the target direction c  while
traversing through the optimal solutions to the temporary direction at every stage. For more on the
shadow vertex method we refer the reader to [ST04]  Section 3.2
The manner in which Theorem 3 is used  both in the above mentioned papers and the current one  is
the following. Consider an LP of the form

max c(cid:62)x
∀i ∈ [n] (cid:104)Ai  x(cid:105) ≤ 1

When solving the LP via the shadow vertex method  the number of pivot steps is upper bounded by
the number of edges in P ∩ E where P = conv(0  A1  ...  An) and E is the plane spanned by the
target direction c and the initial direction u obtained in the phase-1 step.

5 Algorithm and Analysis

Our simplex variant is deﬁned in Algorithm 1 below. It is composed of projecting the polytope
onto a lower dimension  adding noise  ﬁnding an initial vertex (Phase 1)  shifting and applying the
shadow vertex simplex algorithm [GS55].
Theorem 4. Algorithm 1 performs an expected number of O(poly(log n  1
instance A with λ-margin it returns  with probability at least 1 − O( 1
with margin Ω( λ

λ )) pivot steps. Over
n )  a feasible solution ¯x

k + 1

√
k√
d log k ).

Note that the algorithm requires knowledge of λ. This can be overcome with a simple binary search.
To prove Theorem 4  we ﬁrst prove several auxilary lemmas. Due to space restrictions  some of the
proofs are replaced with a brief sketch.
Lemma 5. With probability at least 1 − 1/k there exists a feasible solution to LPbounded  denoted
(ˆx  τ ) that satisﬁes τ ≥ λ − ε and (cid:107)ˆx(cid:107)∞ ≤ 5

log(k)√

√

.

k

Proof Sketch. Since A has margin λ  there exists x∗ ∈ Rd such that ∀i . (cid:104)Ai  x∗(cid:105) ≥ λ and (cid:107)x∗(cid:107)2 =
1. We use Theorem 2 to show that the projection of x∗ has  w.h.p.  both a large margin and a small
(cid:96)∞ norm.

Denote the k + 1 dimensional noise vectors that were added in step 3 by err1  . . .   errn+2k. The
following lemma will provide some basic facts that occur w.h.p. for the noise vectors. The proof
is an easy consequence of the 2-stability of Gaussians  and standard tail bounds of the Chi-Squared
distribution and is thus omitted.
Lemma 6. Let err1  . . .   errn+2k be deﬁned as above:

1. w.p. at least 1 − 1/n  ∀i  (cid:107)erri(cid:107)2 ≤ O(σ(cid:112)k log(n)) ≤ 1

√

20

k

5

1

ε2

  σ2 =

Algorithm 1 large margin simplex
1: Input: a λ-margin LP instance A.
6   k = 9·162 log(n/ε)
2: Let ε = λ
3: (step 1: dimension reduction) Generate M ∈ Rk×d  a projection onto a random k-dimensional

100k log k log n.

subspace. Let ˆA ∈ Rn×(k+1) be given by ˆAi = (

k MAi −1)
√
4: (step 2: bounding (cid:107)x(cid:107)) Add the k constraints (cid:104)ei  x(cid:105) ≥ − 6
log k√

  and one additional constraint τ ≥ −8(cid:112)log(k). Denote the coefﬁcient vectors

√
− 6
log k√
ˆAn+1  . . .   ˆAn+2k and ˆA0 correspondingly. We obtain the following LP denoted by LPbounded 
(3)

  the k constraints (cid:104)−ei  x(cid:105) ≥

max(cid:104)ek+1  (x  τ )(cid:105)

(cid:113) d

k

k

∀i ∈ [0  . . .   n + 2k] .

(cid:68) ˆAi  (x  τ )

(cid:69) ≥ bi

5: (step 3: adding noise) Add a random independently distributed Gaussian noise to every entry of
2). Denote the resulting constraint
6: (step 4: phase-1): Let v0 ∈ Rk+1 be the vertex for which inequalities 0  n + k + 1  . . .   n + 2k

every constraint vector except ˆA0 according to N (0  σ2·(cid:107) ˆAi(cid:107)2
vectors as ˜Ai. Denote the resulting LP by LPnoise.
are held as equalities. Deﬁne u0 ∈ Rk+1 as u0

∆= (1  . . .   1 −1).

7: (step 5: shifting the polytope) For all i ∈ [0  . . .   n + 2k]  change the value of bi to ˆbi

∆=

bi +
8: (step 6 - shadow vertex simplex): Let E = span(u0  ek+1). Apply the shadow ver-
tex simplex algorithm on the polygon which is the projection of conv{V}  where V =
{0  ˆA0/ˆb0  ˜A1/ˆb1  . . .   ˜An+2k/ˆbn+2k}  onto E. Let the solution be ˜x.

(cid:68) ˜Ai  v0/2
(cid:69)

.

9: return

¯x(cid:107)¯x(cid:107)2

where ¯x = M(cid:62)(˜x + v0/2)

2. Fix some I ⊂ [n + 2k] of size |I| = k and deﬁne BI be the (k + 1)× (k + 1) matrix whose
ﬁrst k columns consist of {erri}i∈I and k + 1 column is the 0 vector. W.p. at least 1− 1/n
it holds that the top singular value of BI is at most 1/2. Furthermore  w.p. at least 1− 1/n
the 2-norms of the rows of B are upper bounded by

.

√
4

1
k+1

Lemma 7. Let ˆA  ˜A  ˆx ∈ Rk be as above. Then with probability at least 1 − O(1/k):

1. for τ = λ − 2ε  the point (ˆx  τ ) ∈ Rk+1 is a feasible solution of LPnoise.
2. for every x ∈ Rk  τ ∈ R where (x  τ ) is a feasible solution of the LPnoise it holds that

(cid:112)log(k)√

k

(cid:107)x(cid:107)∞ ≤ 7

 

∀i .

(cid:12)(cid:12)(cid:12)(cid:68) ˜Ai  (x  τ )

(cid:69) −(cid:68) ˆAi  (x  τ )

(cid:69)(cid:12)(cid:12)(cid:12) ≤

√
log k√
k

Proof of this Lemma is deferred to the full version of this paper.
Lemma 8. with probability 1−O(1/k)  the vector v0 is a basic feasible solution (vertex) of LPnoise

proof sketch. The vector v0 is a basic solution as it is deﬁned by k + 1 equalities. To prove that is
feasible we exploit the fact that the last entry corresponding to τ is sufﬁciently small and that all of
the constraints are of the form (cid:104)a  x(cid:105) ≥ τ.

The next lemma provides us with a direction u0 for which v0 is the unique optimal solution w.r.t to
the objective maxx∈P (cid:104)u0  x(cid:105)  where P is the polytope of LPnoise. The vector u0 is independent of
the added noise. This is crucial for the following steps.
Lemma 9. Let u0 = (1  . . .   1 −1). With probability at least 1 − O(1/n)  the point v0 is the
optimal solution w.r.t to the objective maxx∈P (cid:104)u0  x(cid:105)  where P is the polytope of LPnoise.

6

{(cid:80)
u0 =(cid:80)k

Proof Sketch. The set of points u in which v0 is the optimal solution is deﬁned by a (blunt) cone
i αiai| ∀i  αi > 0}  where ai = − ˜An+k+i for i ∈ [k]  ak+1 = − ˆA0. Consider the cone
corresponding to the constraints ˆA; u0 resides in its interior  far away from its boarders. Speciﬁcally 
i=1(− ˆAn+k+i) + (− ˆA0). Since the difference between ˆAi and ˜Ai is small w.h.p.  we get

that u0 resides  w.h.p.  in the cone of points in which v0 is optimal  as required.
Lemma 10. The point v0/2 is a feasible interior point of the polytope with probability at least
1 − O(1/n).

Proof. By Lemma 9  v0 is a feasible point. Also  according to its deﬁnition it is clear that w.p 1  it
lies on k + 1 facets of the polytope  neither of which contains the point 0. In other words  no facet
contains both v0 and 0. Since 0 is clearly a feasible point of the polytope  we get that v0/2 is a
feasible interior point as claimed.

(cid:68) ˆAi  (˜x  τ(cid:48))

(cid:69) ≥(cid:68) ˜Ai  (˜x  τ(cid:48))

(cid:69) −

√
log k√
k

Proof of Theorem 4. We ﬁrst note that in order to use the shadow vertex method  0 must be an
interior point of the polytope. This does not happen in the original polytope  hence the shift of step
5. Indeed according to Lemma 10  v0/2 is an interior point of the polytope  and by shifting it to 0 
the shadow vertex method can indeed be implemented.
We will assume that the statements of the auxiliary lemmas are held. This happens with probability
at least 1 − O( 1
n )  which is the stated success probability of the algorithm. By Lemma 7 
LPnoise has a basic feasible solution with τ ≥ λ − 2ε. The vertex v0  along with the direction u0
which it optimizes  is a feasible starting vector for the shadow vertex simplex algorithm on the plane
E  and hence applying the simplex algorithm with the shadow vertex pivot rule will return a basic
feasible solution in dimension k + 1  denoted (˜x  τ(cid:48))  for which ∀i ∈ [n] .
τ(cid:48) ≥ λ − 2ε. Using Lemma 7 part two  we have that for all i ∈ [n] 

(cid:68) ˜Ai  (˜x  τ(cid:48))

(cid:69) ≥ 0 and

k + 1

≥ −ε ⇒ (cid:104)MAi  ˜x(cid:105) ≥ λ − 3ε.

(4)
i M(cid:62) ˜x = (cid:104)f (Ai)  ˜x(cid:105) ≥

λ − 3ε and this provides a solution to the original LP.
To compute the margin of this solution  note that the rows of M consist of an orthonormal set.

Since ¯x = (cid:112)d/kM(cid:62) ˜x  we get that for all i ∈ [n]  (cid:104)Ai  ¯x(cid:105) = (cid:112)d/kA(cid:62)
Hence  by Lemma 7  (cid:107)M(cid:62) ˜x(cid:107)2 = (cid:107)˜x(cid:107)2 ≤ 7(cid:112)log(k) meaning that (cid:107)¯x(cid:107)2 ≤ 7(cid:112)log(k)d/k. It
k/(7(cid:112)log(k)d)
follows that the margin of the solution is at least ≥ (λ − 3ε) · √
Running time: The number of steps in this simplex step is bounded by the number of vertices in
the polygon which is the projection of the polytope of LPnoise onto the plane E = span{u0  vT}.
Let V = { ˜A}n+2k
i=1 . Since all of the points in V are perturbed  the number of vertices in the polygon
conv(V) ∩ E is bounded w.h.p. as in Theorem 3 by O(k3σ−4) = ˜O(log11(n)/λ14). Since the
points 0  ˆA0 reside in the plane E  the the number of vertices of (conv(V ∪ {0  ˆA0})) ∩ E is at
most the number of vertices in conv(V) ∩ E plus 4  which is asymptotically the same. Each pivot
step in the shadow vertex simplex method can be implemented to run in time O(nk) = ˜O(n/λ14)
for n constraints in dimension k. The dimension reduction step required ˜O(nd) time. All other
operations including adding noise and shifting the polytope are faster than the shadow vertex simplex
procedure  leading to an overall running time of ˜O(nd) (assuming λ is a constant or sub polynomial
in d).

proof of Theorem 1. The statement regarding the margin of the solution  viewed as a point in Rd is
immediate from Theorem 4. To prove the claim regarding the view in the low dimensional space 
consider Equation 4 in the above proof. Put in words  it states the following: Consider the projec-
tion M of the algorithm (or alternatively its image V ) and the classiﬁcation problem of the points
projected onto V . The margin of the solution produced by the algorithm (i.e.  of ˜x) is at least λ− 3ε.

The (cid:96)∞-norm ˜x of is clearly bounded by O((cid:112)log(k)/k). Hence  the margin of the normalized point
˜x/(cid:107)˜x(cid:107)2 is Ω(λ/(cid:112)log(k)). In order to achieve a margin of λ − O(ε)  one should replace the (cid:96)∞

bound in the LP with an approximate (cid:96)2 bound. This can be done via linear constraints by bounding

7

(cid:12)(cid:12)(cid:12)(cid:107)F x(cid:107)1

(cid:107)x(cid:107)2

− 1

the (cid:96)1 norm of F x where F : Rk → RK  K = O(k/ε2) and F has the property that for every
x ∈ Rk 

(cid:12)(cid:12)(cid:12) < ε. A properly scaled matrix of i.i.d. Gaussians has this property [Ind00].
This step would eliminate the need for the extra(cid:112)log(k) factor. The other multiplicative constants

can be reduced to 1 + O(ε)  thus ensuring the norm of ˜x is at most 1 + O(ε)  by assigning a slightly
smaller value for σ; speciﬁcally  σ/ε would do. Once the 2-norm of ˜x is bounded by 1 + O(ε)  the
margin of the normalized point is λ − O(ε).

6 Discussion

The simplex algorithm for linear programming is a cornerstone of operations research whose com-
putational complexity remains elusive despite decades of research. In this paper we examine the
simplex algorithm in the lens of machine learning  and in particular via linear classiﬁcation  which
is equivalent to linear programming. We show that in the cases where the margin parameter is large 
say a small constant  we can construct a simplex algorithm whose worst case complexity is (quasi)
linear. Indeed in many practical problems the margin parameter is a constant unrelated to the other
parameters. For example  in cases where a constant inherent noise exists  the margin must be large
otherwise the problem is simply unsolvable.

6.1

soft margin SVM

In the setting of this paper  the case of soft margin SVM turns out to be algorithmically easier to
solve than the separable case. In a nutshell  the main hardship in the separable case is that a large
number of data points may be problematic. This is since the separating hyperplane must separate
all of the points and not most of them  meaning that every one of the data points must be taken in
consideration. A more formal statement is the following. In our setting we have three parameters.
The number of points n  the dimension d and the ‘sub optimality’ ε. In the soft margin (e.g. hinge
loss) case  the number of points may be reduced to poly(ε−1) by elementary methods. Speciﬁcally 
it is in easy task to prove that if we omit all but a random subset of log(ε−1)/ε2 data points  the
hinge loss corresponding to the obtained separator w.r.t the full set of points will be O(ε). In fact 
it sufﬁces to solve the problem with the reduced number of points  up to an additive loss of ε to the
margin to obtain the same result. As a consequence of the reduced number of points  the dimension
can be reduced  analogously to the separable case to d(cid:48) = O(log(ε−1)/ε2).
The above essentially states that the original problem can be reduced  by performing a single pass
over the input (perhaps even less than that)  to one where all the only parameter is ε. From this
point  the only challenge is to solve the resulting LP  up to an ε additive loss to the optimum  in time
polynomial to its size. There are many methods available for this problem.
To conclude  the soft margin SVM problem is much easier than the separable case hence we do not
analyze it in this paper.

References
[BD02]

A. Blum and J. Dunagan. Smoothed analysis of the perceptron algorithm for linear pro-
gramming. In Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete
algorithms  pages 905–914. Society for Industrial and Applied Mathematics  2002.

[CHW10] Kenneth L. Clarkson  Elad Hazan  and David P. Woodruff. Sublinear optimization for

machine learning. In FOCS  pages 449–457. IEEE Computer Society  2010.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. In Machine Learning 
pages 273–297  1995.

[CV95]

[DG03]

[DV08]

[Dan51] G. B. Dantzig. Maximization of linear function of variables subject to linear inequalities.

Activity Analysis of Production and Allocation  pages 339–347  1951.
Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and
lindenstrauss. Random Struct. Algorithms  22:60–65  January 2003.
John Dunagan and Santosh Vempala. A simple polynomial-time rescaling algorithm for
solving linear programs. Math. Program.  114(1):101–114  2008.

8

[GS55]

[Ind00]

[FGKP06] Vitaly Feldman  Parikshit Gopalan  Subhash Khot  and Ashok Kumar Ponnuswami.
New results for learning noisy parities and halfspaces. In FOCS  pages 563–574. IEEE
Computer Society  2006.
S. Gass and T. Saaty. The computational algorithm for the parameteric objective func-
tion. Naval Research Logistics Quarterly  2:39–45  1955.
P. Indyk. Stable distributions  pseudorandom generators  embeddings and data stream
In Foundations of Computer Science  2000. Proceedings. 41st Annual
computation.
Symposium on  pages 189–197. IEEE  2000.
W. B. Johnson and J. Lindenstrauss. Extensions of lipschitz mapping into hilbert space.
Contemporary Mathematics  26:189–206  1984.
Jonathan A. Kelner and Daniel A. Spielman. A randomized polynomial-time simplex
In Proceedings of the thirty-eighth annual ACM
algorithm for linear programming.
symposium on Theory of computing  STOC ’06  pages 51–60  New York  NY  USA 
2006. ACM.

[JL84]

[KS06]

[Meg86] Nimrod Megiddo. Improved asymptotic analysis of the average number of steps per-

[ST04]

[Ver09]

formed by the self-dual simplex algorithm. Math. Program.  35:140–172  June 1986.
Daniel A. Spielman and Shang-Hua Teng. Smoothed analysis of algorithms: Why the
simplex algorithm usually takes polynomial time. J. ACM  51:385–463  May 2004.
Roman Vershynin. Beyond hirsch conjecture: Walks on random polytopes and smoothed
complexity of the simplex method. SIAM J. Comput.  39(2):646–678  2009.

9

,Mahito Sugiyama
Karsten Borgwardt
Yunzhe Tao
Qi Sun
Qiang Du
Wei Liu