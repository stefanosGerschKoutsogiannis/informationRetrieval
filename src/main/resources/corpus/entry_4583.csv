2013,Reconciling "priors" & "priors" without prejudice?,There are two major routes to address linear inverse problems. Whereas regularization-based approaches build estimators as solutions of penalized regression optimization problems  Bayesian estimators rely on the posterior distribution of the unknown  given some assumed family of priors. While these may seem radically different approaches  recent results have shown that  in the context of additive white Gaussian denoising  the Bayesian conditional mean estimator is always the solution of a penalized regression problem. The contribution of this paper is twofold. First  we extend the additive white Gaussian denoising results to general linear inverse problems with colored Gaussian noise. Second  we characterize conditions under which the penalty function associated to the conditional mean estimator can satisfy certain popular properties such as convexity  separability  and smoothness. This sheds light on some tradeoff between computational efficiency and estimation accuracy in sparse regularization  and draws some connections between Bayesian estimation and proximal optimization.,Reconciling “priors” & “priors” without prejudice?

R´emi Gribonval ∗

Inria

Pierre Machart

Inria

Centre Inria Rennes - Bretagne Atlantique

Centre Inria Rennes - Bretagne Atlantique

remi.gribonval@inria.fr

pierre.machart@inria.fr

Abstract

There are two major routes to address linear inverse problems. Whereas
regularization-based approaches build estimators as solutions of penalized regres-
sion optimization problems  Bayesian estimators rely on the posterior distribution
of the unknown  given some assumed family of priors. While these may seem
radically different approaches  recent results have shown that  in the context of
additive white Gaussian denoising  the Bayesian conditional mean estimator is
always the solution of a penalized regression problem. The contribution of this
paper is twofold. First  we extend the additive white Gaussian denoising results
to general linear inverse problems with colored Gaussian noise. Second  we char-
acterize conditions under which the penalty function associated to the conditional
mean estimator can satisfy certain popular properties such as convexity  separa-
bility  and smoothness. This sheds light on some tradeoff between computational
efﬁciency and estimation accuracy in sparse regularization  and draws some con-
nections between Bayesian estimation and proximal optimization.

1

Introduction

Let us consider a fairly general linear inverse problem  where one wants to estimate a parameter
vector z ∈ RD   from a noisy observation y ∈ Rn  such that y = Az + b  where A ∈ Rn×D
is sometimes referred to as the observation or design matrix  and b ∈ Rn represents an additive
Gaussian noise with a distribution PB ∼ N (0  Σ). When n < D  it turns out to be an ill-posed
problem. However  leveraging some prior knowledge or information  a profusion of schemes have
been developed in order to provide an appropriate estimation of z. In this abundance  we will focus
on two seemingly very different approaches.

1.1 Two families of approaches for linear inverse problems

On the one hand  Bayesian approaches are based on the assumption that z and b are drawn from
probability distributions PZ and PB respectively. From that point  a straightforward way to estimate
z is to build  for instance  the Minimum Mean Squared Estimator (MMSE)  sometimes referred to
as Bayesian Least Squares  conditional expectation or conditional mean estimator  and deﬁned as:

ψMMSE(y) := E(Z|Y = y).

(1)

This estimator has the nice property of being optimal (in a least squares sense) but suffers from
its explicit reliance on the prior distribution  which is usually unknown in practice. Moreover  its
computation involves a tedious integral computation that generally cannot be done explicitly.
On the other hand  regularization-based approaches have been at the centre of a tremendous amount
of work from a wide community of researchers in machine learning  signal processing  and more

∗The authors are with the PANAMA project-team at IRISA  Rennes  France.

1

generally in applied mathematics. These approaches focus on building estimators (also called de-
coders) with no explicit reference to the prior distribution. Instead  these estimators are built as an
optimal trade-off between a data ﬁdelity term and a term promoting some regularity on the solution.
Among these  we will focus on a widely studied family of estimators ψ that write in this form:

(cid:107)y − Az(cid:107)2 + φ(z).

1
2

ψ(y) := argmin
z∈RD
For instance  the speciﬁc choice φ(z) = λ(cid:107)z(cid:107)2
2 gives rise to a method often referred to as the ridge
regression [1] while φ(z) = λ(cid:107)z(cid:107)1 gives rise to the famous Lasso [2].
The (cid:96)1 decoder associated to φ(z) = λ(cid:107)z(cid:107)1 has attracted a particular attention  for the associated
optimization problem is convex  and generalizations to other “mixed” norms are being intensively
studied [3  4]. Several facts explain the popularity of such approaches: a) these penalties have well-
understood geometric interpretations; b) they are known to be sparsity promoting (the minimizer
has many zeroes); c) this can be exploited in active set methods for computational efﬁciency [5]; d)
convexity offers a comfortable framework to ensure both a unique minimum and a rich toolbox of
efﬁcient and provably convergent optimization algorithms [6].

(2)

1.2 Do they really provide different estimators?

Regularization and Bayesian estimation seemingly yield radically different viewpoints on inverse
problems. In fact  they are underpinned by distinct ways of deﬁning signal models or “priors”. The
“regularization prior” is embodied by the penalty function φ(z) which promotes certain solutions 
somehow carving an implicit signal model. In the Bayesian framework  the “Bayesian prior” is
embodied by where the mass of the signal distribution PZ lies.

noise model b ∼ N (0  I) and Bayesian prior PZ(z ∈ E) =(cid:82)

The MAP quid pro quo A quid pro quo between these distinct notions of priors has crystallized
around the notion of maximum a posteriori (MAP) estimation  leading to a long lasting incompre-
hension between two worlds. In fact  a simple application of Bayes rule shows that under a Gaussian
E pZ(z)dz  E ⊂ RN   MAP estima-
tion1 yields the optimization problem (2) with regularization prior φZ(z) := − log pZ(z). By a
trivial identiﬁcation  the optimization problem (2) with regularization prior φ(z) is now routinely
called “MAP with prior exp(−φ(z))”. With the (cid:96)1 penalty  it is often called “MAP with a Laplacian
prior”. As an unfortunate consequence of an erroneous “reverse reading” of this fact  this identiﬁca-
tion has given rise to the erroneous but common myth that the optimization approach is particularly
well adapted when the unknown is distributed as exp(−φ(z)). As a striking counter-example to this
myth  it has recently been proved [7] that when z is drawn i.i.d. Laplacian and A ∈ Rn×D is drawn
from the Gaussian ensemble  the (cid:96)1 decoder – and indeed any sparse decoder – will be outperformed
by the least squares decoder ψLS(y) := pinv(A)y  unless n (cid:38) 0.15D.
In fact  [8] warns us that the MAP estimate is only one of the plural possible Bayesian interpretations
of (2)  even though it is the most straightforward one. Furthermore  to point out that erroneous con-
ception  a deeper connection is dug  showing that in the more restricted context of (white) Gaussian
denoising  for any prior  there exists a regularizer φ such that the MMSE estimator can be expressed
as the solution of problem (2). This result essentially exhibits a regularization-oriented formulation
for which two radically different interpretations can be made. It highlights the important following
fact: the speciﬁc choice of a regularizer φ does not  alone  induce an implicit prior on the supposed
distribution of the unknown; besides a prior PZ  a Bayesian estimator also involves the choice of a
loss function. For certain regularizers φ  there can in fact exist (at least two) different priors PZ for
which the optimization problem (2) yields the optimal Bayesian estimator  associated to (at least)
two different losses (e.g..  the 0/1 loss for the MAP  and the quadratic loss for the MMSE).

1.3 Main contributions

A ﬁrst major contribution of that paper is to extend the aforementioned result [8] to a more general
linear inverse problem setting. Our ﬁrst main results can be introduced as follows:

1which is the Bayesian optimal estimator in a 0/1 loss sense  for discrete signals.

2

Theorem (Flavour of the main result). For any non-degenerate2 prior PZ  any non-degenerate co-
variance matrix Σ and any design matrix A with full rank  there exists a regularizer φA Σ PZ such
that the MMSE estimator of z ∼ PZ given the observation y = Az + b with b ∼ N (0  Σ) 

ψA Σ PZ (y) := E(Z|Y = y) 
Σ + φA Σ PZ (z).

2(cid:107)y − Az(cid:107)2

is a minimizer of z (cid:55)→ 1
Roughly  it states that for the considered inverse problem  for any prior on z  the MMSE estimate
with Gaussian noise is also the solution of a regularization-based problem (the converse is not true).
In addition to this result we further characterize properties of the penalty function φA Σ PZ (z) in
the case where A is invertible  by showing that: a) it is convex if and only if the probability density
function of the observation y  pY (y) (often called the evidence)  is log-concave; b) when A = I 
i=1 φi(zi) where z = (z1  . . .   zn) if  and only if  the evidence is

it is a separable sum φ(z) = (cid:80)n

multiplicatively separable: pY (y) = Πn

i=1pYi(yi).

(3)

1.4 Outline of the paper

In Section 2  we develop the main result of our paper  that we just introduced. To do so  we review an
existing result from the literature and explicit the different steps that make it possible to generalize
it to the linear inverse problem setting. In Section 3  we provide further results that shed some light
on the connections between MMSE and regularization-oriented estimators. Namely  we introduce
some commonly desired properties on the regularizing function such as separability and convexity
and show how they relate to the priors in the Bayesian framework. Finally  in Section 4  we conclude
and offer some perspectives of extension of the present work.

2 Main steps to the main result

We begin by a highlight of some intermediate results that build into steps towards our main theorem.

2.1 An existing result for white Gaussian denoising

As a starting point  let us recall the existing results in [8] (Lemma II.1 and Theorem II.2) dealing
with the additive white Gaussian denoising problem  A = I  Σ = I.
Theorem 1 (Reformulation of the main results of [8]). For any non-degenerate prior PZ  we have:

1. ψI I PZ is one-to-one;
2. ψI I PZ and its inverse are C∞;
3. ∀y ∈ Rn  ψI I PZ (y) is the unique global minimum and unique stationary point of

z (cid:55)→ 1
2

(cid:107)y − Iz(cid:107)2 + φ(z)  with:

(z) − z(cid:107)2

2 − log pY [ψ−1

MMSE(z)];

(cid:26) − 1

2(cid:107)ψ−1
+∞ 

I I PZ

(4)

for z ∈ ImψI I PZ ;
for x /∈ ImψI I PZ ;

φ(z) = φI I PZ (z) :=

4. The penalty function φI I PZ is C∞;
5. Any penalty function φ(z) such that ψI I PZ (y) is a stationary point of (4) satisﬁes φ(z) =

φI I PZ (z) + C for some constant C and all z.

2We only need to assume that Z does not intrinsically live almost surely in a lower dimensional hyperplane.
The results easily generalize to this degenerate situation by considering appropriate projections of y and z.
Similar remarks are in order for the non-degeneracy assumptions on Σ and A.

3

2.2 Non-white noise
Suppose now that B ∈ Rn is a centred non-degenerate normal Gaussian variable with a (positive
deﬁnite) covariance matrix Σ. Using a standard noise whitening technique  Σ−1/2B ∼ N (0  I).
This makes our denoising problem equivalent to yΣ = zΣ+bΣ  with yΣ := Σ−1/2y  zΣ := Σ−1/2z
and bΣ := Σ−1/2b  which is drawn from a Gaussian distribution with an identity covariance matrix.
Finally  let (cid:107).(cid:107)Σ be the norm induced by the scalar product (cid:104)x  y(cid:105)Σ := (cid:104)x  Σ−1y(cid:105).
Corollary 1 (non-white Gaussian noise). For any non-degenerate prior PZ  any non-degenerate Σ 
Y = Z + B  we have:

1. ψI Σ PZ is one-to-one.
2. ψI Σ PZ and its inverse are C∞.
3. ∀y ∈ Rn  ψI Σ PZ (y) is the unique global minimum and stationary point of

(cid:107)y − Iz(cid:107)2

z (cid:55)→ 1
2
(Σ−1/2z)

Σ−1/2Z

Σ + φI Σ PZ (z).

with φI Σ PZ (z) := φI I P

4. φI Σ PZ is C∞.

As with white noise  up to an additive constant  φI Σ PZ is the only penalty with these properties.

Proof. First  we introduce a simple lemma that is pivotal throughout each step of this section.
Lemma 1. For any function f : Rn → R and any M ∈ Rn×p  we have:

M argmin

v∈Rp

f (M v) =

argmin

u∈range(M )⊆Rn

f (u).

Now  the linearity of the (conditional) expectation makes it possible to write

Σ−1/2E(Z|Y = y) = E(Σ−1/2Z|Σ−1/2Y = Σ−1/2y)
⇔ Σ−1/2ψI Σ PZ (y) = ψI I P

(Σ−1/2y).

Σ−1/2Z

Using Theorem 1  it follows that:

ψI Σ PZ (y) = Σ1/2ψI I P

Σ−1/2 Z

(Σ−1/2y)

From this property and Theorem 1  it is clear that ψI Σ PZ is one-to-one  C∞  as well as its inverse.
Now  using Lemma 1 with M = Σ1/2  we get:

(cid:27)

(cid:27)

(cid:26) 1

2

(cid:26) 1
(cid:26) 1

2

2

= argmin

z∈Rn

= argmin

z∈Rn

ψI Σ PZ (y) = Σ1/2 argmin
z(cid:48)∈Rn

(cid:107)Σ−1/2y − z(cid:48)(cid:107)2 + φI I P

Σ−1/2Z

(z(cid:48))

(cid:107)Σ−1/2y − Σ−1/2z(cid:107)2 + φI I P

(Σ−1/2z)

Σ−1/2Z

(cid:27)

(cid:107)y − z(cid:107)2

Σ + φI Σ PZ (z)

 

with φI Σ PZ (z) := φI I P
and that this minimizer is unique (and is the only stationary point).

Σ−1/2Z

(Σ−1/2z). This deﬁnition also makes it clear that φI Σ PZ is C∞ 

4

2.3 A simple under-determined problem

As a step towards handling the more generic linear inverse problem y = Az + b  we will investigate
the particular case where A = [I 0]. For the sake of readability  for any two (column) vectors
u  v  let us denote [u; v] the concatenated (column) vector. First and foremost let us decompose the
MMSE estimator into two parts  composed of the ﬁrst n and last (D − n) components :

ψ[I 0] Σ PZ (y) := [ψ1(y); ψ2(y)]

Corollary 2 (simple under-determined problem). For any non-degenerate prior PZ  any non-
degenerate Σ  we have:

1. ψ1(y) = ψI Σ PZ (y) is one-to-one and C∞. Its inverse and φI Σ PZ are also C∞;
2. ψ2(y) = (pB (cid:63) g)(y)/(pB (cid:63) PZ)(y) (with g(z1) := E(Z2|Z1 = z1)p(z1)) is C∞;
3. ψ[I 0] Σ PZ is injective.

Moreover  let h : R(D−n) × R(D−n) (cid:55)→ R+ be any function such that h(x1  x2) = 0 ⇒ x1 = x2 

3. ∀y ∈ Rn  ψ[I 0] Σ PZ (y) is the unique global minimum and stationary point of

z (cid:55)→ 1
2

(z) := φI Σ PZ (z1) + h(cid:0)z2  ψ2 ◦ ψ−1

(cid:107)y − [I 0]z(cid:107)2

Σ + φh

[I 0] Σ PZ

1 (z1)(cid:1) and z = [z1; z2].

(z)

with φh

[I 0] Σ PZ

4. φ[I 0] Σ PZ is C∞ if and only if h is.

Proof. The expression of ψ2(y) is obtained by Bayes rule in the integral deﬁning the conditional
expectation. The smoothing effect of convolution with the Gaussian pB(b) implies the C∞ nature
of ψ2. Let Z1 = [I 0]Z. Using again the linearity of the expectation  we have:

[I 0]ψ[I 0] Σ PZ (y) = E([I 0]Z|Y = y) = E(Z1|Y = y) = ψI Σ PZ (y).

Hence  ψ1(y) = ψI Σ PZ (y). Given the properties of h  we have:

ψ2(y) = argmin
z2∈R(D−n)

It follows that:

ψ[I 0] Σ PZ (y) = argmin

z=[z1;z2]∈RD

1
2

(cid:107)y − z1(cid:107)2

(cid:0)ψ1(y)(cid:1)(cid:1) .

h(cid:0)z2  ψ2 ◦ ψ−1
Σ + φI Σ PZ (z1) + h(cid:0)z2  ψ2 ◦ ψ−1

1

1 (z1)(cid:1).

From the deﬁnitions of ψ[I 0] Σ PZ and h  it is clear  using Corollary 1 that ψ[I 0] Σ PZ is injective 
is the unique minimizer and stationary point  and that φ[I 0] Σ PZ is C∞ if and only if h is.

2.4

Inverse Problem

We are now equipped to generalize our result to an arbitrary full rank matrix A. Using the Singular
Value Decomposition  A can be factored as:

A = U [∆ 0]V (cid:62) = ˜U [I 0]V (cid:62)  with ˜U = U ∆.

−1

Our problem is now equivalent to y(cid:48) := ˜U
Let ˜Σ = ˜U
Theorem 2 (Main result). Let h : R(D−n) × R(D−n) (cid:55)→ R+ be any function such that h(x1  x2) =
0 ⇒ x1 = x2. For any non-degenerate prior PZ  any non-degenerate Σ and A  we have:

. Note that B(cid:48) ∼ N (0  ˜Σ).

y = [I 0]V (cid:62)z + ˜U

b =: z(cid:48) + b(cid:48).

Σ ˜U

−(cid:62)

−1

−1

1. ψA Σ PZ is injective.

5

2. ∀y ∈ Rn  ψ[I 0] Σ PZ (y) is the unique global minimum and stationary point of
(V (cid:62)z).

(z)  with φh

z (cid:55)→ 1

(z) := φh

2(cid:107)y − Az(cid:107)2

Σ + φh

A Σ PZ

A Σ PZ

[I 0]  ˜Σ PV (cid:62) Z

3. φA Σ PZ is C∞ if and only if h is.

Proof. First note that:

V (cid:62)ψA Σ PZ (y) = V (cid:62)E(Z|Y = y) = E(Z(cid:48)|Y (cid:48) = y(cid:48)) = ψ[I 0]  ˜Σ PZ(cid:48)
(z(cid:48)) 

(cid:107)U(cid:62)y − [I 0]z(cid:48)(cid:107)2

= argmin

˜Σ + φh

[I 0]  ˜Σ PV (cid:62) Z

1
2

z(cid:48)

using Corollary 2. Now  using Lemma 1  we have:

ψA Σ PZ (y) = argmin

z

= argmin

z

1
2
1
2

(cid:107)U(cid:62)(cid:16)

y − U [I 0]V (cid:62)(cid:17)(cid:107)2

(cid:107)y − Az(cid:107)2

Σ + φh

[I 0]  ˜Σ PV (cid:62) Z

(V (cid:62)z)

˜Σ + φh

[I 0]  ˜Σ PV (cid:62)Z

(V (cid:62)z)

The other properties come naturally from those of Corollary 2.
Remark 1. If A is invertible (hence square)  ψA Σ PZ is one-to-one. It is also C∞  as well as its
inverse and φA Σ PZ .

3 Connections between the MMSE and regularization-based estimators

Equipped with the results from the previous sections  we can now have a clearer idea about how
MMSE estimators  and those produced by a regularization-based approach relate to each other. This
is the object of the present section.

3.1 Obvious connections

Some simple observations of the main theorem can already shed some light on those connections.
First  for any prior  and as long as A is invertible  we have shown that there exists a corresponding
regularizing term (which is unique up to an additive constant). This simply means that the set of
MMSE estimators in linear inverse problems with Gaussian noise is a subset of the set of estimators
that can be produced by a regularization approach with a quadratic data-ﬁtting term.
Second  since the corresponding penalty is necessarily smooth  it is in fact only a strict subset of such
regularization estimators. In other words  for some regularizers  there cannot be any interpretation
in terms of an MMSE estimator. For instance  as pinpointed by [8]  all the non-C∞ regularizers
belong to that category. Among them  all the sparsity-inducing regularizers (the (cid:96)1 norm  among
others) fall into this scope. This means that when it comes to solving a linear inverse problem (with
an invertible A) under Gaussian noise  sparsity inducing penalties are necessarily suboptimal (in a
mean squared error sense).

3.2 Relating desired computational properties to the evidence

Let us now focus on the MMSE estimators (which also can be written as regularization-based estima-
tors). As reported in the introduction  one of the reasons explaining the success of the optimization-
based approaches is that one can have a better control on the computational efﬁciency on the algo-
rithms via some appealing properties of the functional to minimize. An interesting question then
is: can we relate these properties of the regularizer to the Bayesian priors  when interpreting the
solution as an MMSE estimate?
For instance  when the regularizer is separable  one may easily rely on coordinate descent algo-
rithms [9]. Here is a more formal deﬁnition:
Deﬁnition 1 (Separability). A vector-valued function f : Rn → Rn is separable if there exists a set
of functions f1  . . .   fn : R → R such that ∀x ∈ Rn  f (x) = (fi(xi))n

i=1.

6

there exists a set of functions g1  . . .   gn : R → R such that ∀x ∈ Rn  g(x) =(cid:80)n
g(x) =(cid:81)n

A scalar-valued function g : Rn → R is additively separable (resp. multiplicatively separable) if
i=1 gi(xi) (resp.

i=1 gi(xi)).

Especially when working with high-dimensional data  coordinate descent algorithms have proven to
be very efﬁcient and have been extensively used for machine learning [10  11].
Even more evidently  when solving optimization problems  dealing with convex functions ensures
that many algorithms will provably converge to the global minimizer [6]. As a consequence  it
would be interesting to be able to characterize the set of priors for which the MMSE estimate can be
expressed as a minimizer of a convex function.
The following lemma precisely addresses these issues. For the sake of simplicity and readability  we
focus on the speciﬁc case where A = I and Σ = I.
Lemma 2 (Convexity and Separability). For any non-degenerate prior PZ  Theorem 1 says that
∀y ∈ Rn  ψI I PZ (y) is the unique global minimum and stationary point of z (cid:55)→ 1
2(cid:107)y − Iz(cid:107)2 +
φI I PZ (z). Moreover  the following results hold:

1. φI I PZ is convex if and only if pY (y) := pB (cid:63) PZ(y) is log-concave 
2. φI I PZ is additively separable if and only if pY (y) is multiplicatively separable.

Proof of Lemma 2. From Lemma II.1 in [8]  the Jacobian matrix J[ψI I PZ ](y) is positive deﬁnite
hence invertible. Derivating φI I PZ [ψI I PZ (y)] from its deﬁnition in Theorem 1  we get:

J[ψI I PZ ](y)∇φI I PZ [ψI I PZ (y)]

(cid:21)

(cid:20)

− 1
2

2 − log pY (y)

(cid:107)y − ψI I PZ (y)(cid:107)2

= ∇
= − (In − J[ψI I PZ ](y)) (y − ψI I PZ (y)) − ∇ log pY (y)
= (In − J[ψI I PZ ](y))∇ log pY (y) − ∇ log pY (y)
= −J[ψI I PZ ](y)∇ log pY (y)
∇φI I PZ [ψI I PZ (y)] = −∇ log pY (y).

Derivating this expression once more  we get:

Then:

Hence:

J[ψI I PZ ](y)∇2φI I PZ [ψI I PZ (y)] = −∇2 log pY (y).
∇2φI I PZ [ψI I PZ (y)] = −J−1[ψI I PZ ](y)∇2 log pY (y).

φI I PZ convex ⇔ ∇2φI I PZ [ψI I PZ (y)] (cid:60) 0

⇔ −J−1[ψI I PZ ](y)∇2 log pY (y) (cid:60) 0

As ψI I PZ is one-to-one  φI I PZ is convex if and only if φI I PZ [ψI I PZ ] is. It also follows that:

As J[ψI I PZ ](y) = In + ∇2 log pY (y) 
J−1[ψI I PZ ](y) are simultaneously diagonalisable.
and ∇2 log pY (y) commute. Now  as J[ψI I PZ ](y) is positive deﬁnite  we have:
−J−1[ψI I PZ ](y)∇2 log pY (y) (cid:60) 0 ⇔ ∇2 log pY (y) (cid:52) 0.

the matrices ∇2 log pY (y)  J[ψI I PZ ](y)  and
It follows that the matrices J−1[ψI I PZ ](y)

It follows that φI I PZ is convex if and only if pY (y) := pB (cid:63) PX (y) is log-concave.
By its deﬁnition (II.3) in [8]  it is clear that:

φI I PZ is additively separable ⇔ ψI I PZ is separable.

Using now equation (II.2) in [8]  we have:

ψI I PZ is separable ⇔ ∇ log pY is separable

⇔ log pY is additively separable
⇔ pY is multiplicatively separable.

7

Remark 2. This lemma focuses on the speciﬁc case where A = I and a white Gaussian noise.
However  considering the transformations induced by a shift to an arbitrary invertible matrix A and
to an arbitrary non-degenerate covariance matrix Σ  which are depicted throughout Section 2  it is
easy to see that the result on convexity carries over. An analogous (but more complicated) result
could be also derived regarding separability. We leave that part to the interested reader.

These results provide a precise characterization of conditions on the Bayesian priors so that the
MMSE estimator can be expressed as minimizer of a convex or separable function. Interestingly 
those conditions are expressed in terms of the probability distribution function (pdf in short) of the
observations pY   which is sometimes referred to as the evidence. The fact that the evidence plays a
key role in Bayesian estimation has been observed in many contexts  see for example [12]. Given
that we assume that the noise is Gaussian  its pdf pB always is log-concave. Thanks to a simple
property of the convolution of log-concave functions  it is sufﬁcient that the prior on the signal pZ
is log-concave to ensure that pY also is. However  it is not a necessary condition. This means that
there are some priors pX that are not log-concave such that the associated MMSE estimator can still
be expressed as the minimizer of a functional with a convex regularizer. For a more detailed analysis
of this last point  in the speciﬁc context of Bernoulli-Gaussian priors (which are not log-concave) 
please refer to the technical report [13].
From this result  one may also draw an interesting negative result. If the distribution of the observa-
tion y is not log-concave  then  the MMSE estimate cannot be expressed as the solution of a convex
regularization-oriented formulation. This means that  with a quadratic data-ﬁtting term  a convex
approach to signal estimation cannot be optimal (in a mean squared error sense).

4 Prospects

In this paper we have extended a result  stating that in the context of linear inverse problems with
Gaussian noise  for any Bayesian prior  there exists a regularizer φ such that the MMSE estimator
can be expressed as the solution of regularized regression problem (2). This result is a generalization
of a result in [8]. However  we think it could be extended with regards to many aspects. For instance 
our proof of the result naturally builds on elementary bricks that combine in a way that is imposed
by the deﬁnition of the linear inverse problem. However  by developing more bricks and combining
them in different ways  it may be possible to derive analogous results for a variety of other problems.
Moreover  in the situation where A is not invertible (i.e. the problem is under-determined)  there is
a large set of admissible regularizers (i.e. up to the choice of a function h in Corollary 2). This addi-
tional degree of freedom might be leveraged in order to provide some additional desirable properties 
from an optimization perspective  for instance.
Also  our result relies heavily on the choice of a quadratic loss for the data-ﬁtting term and on a
Gaussian model for the noise. Naturally  investigating other choices (e.g.
logistic or hinge loss 
Poisson noise  to name a few) is a question of interest. But a careful study of the proofs in [8]
suggests that there is a peculiar connection between the Gaussian noise model on the one hand and
the quadratic loss on the other hand. However  further investigations should be conducted to get a
deeper understanding on how these really interplay on a higher level.
Finally  we have stated a number of negative results regarding the non-optimality of sparse de-
coders or of convex formulations for handling observations drawn from a distribution that is not
log-concave. It would be interesting to develop a metric in the estimators space in order to quantify 
for instance  how “far” one arbitrary estimator is from an optimal one  or  in other words  what is
the intrinsic cost of convex relaxations.

Acknowledgements

This work was supported in part by the European Research Council  PLEASE project (ERC-StG-
2011-277906).

8

References
[1] Arthur E. Hoerl and Robert W. Kennard. Ridge regression: applications to nonorthogonal

problems. Technometrics  12(1):69–82  1970.

[2] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society  58(1):267–288  1996.

[3] Matthieu Kowalski. Sparse regression using mixed norms. Applied and Computational Har-

monic Analysis  27(3):303–324  2009.

[4] Francis Bach  Rodolphe Jenatton  Julien Mairal  and Guillaume Obozinski. Optmization with
sparsity-inducing penalties. Foundations and Trends in Machine Learning  4(1):1–106  2012.
[5] Rodolphe Jenatton  Guillaume Obozinski  and Francis Bach. Active set algorithm for struc-
tured sparsity-inducing norms. In OPT 2009: 2nd NIPS Workshop on Optimization for Ma-
chine Learning  2009.

[6] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2004.
[7] R´emi Gribonval  Volkan Cevher  and Mike Davies  E. Compressible Distributions for High-

dimensional Statistics. IEEE Transactions on Information Theory  2012.

[8] R´emi Gribonval. Should penalized least squares regression be interpreted as maximum a pos-

teriori estimation? IEEE Transactions on Signal Processing  59(5):2405–2410  2011.

[9] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems.
Core discussion papers  Center for Operations Research and Econometrics (CORE)  Catholic
University of Louvain  2010.

[10] C.-J. Hsieh  K.-W. Chang  C.-J. Lin  S. Sathiya Keerthi  and S. Sundararajan. A dual coordinate
descent method for large-scale linear svm. In Proceedings of the 25th International Conference
on Machine Learning  pages 408–415  2008.

[11] Pierre Machart  Thomas Peel  Liva Ralaivola  Sandrine Anthoine  and Herv´e Glotin. Stochas-
In 28th International Conference on Machine

tic low-rank kernel learning for regression.
Learning  2011.

[12] Martin Raphan and Eero P. Simoncelli. Learning to be bayesian without supervision. In in

Adv. Neural Information Processing Systems (NIPS*06. MIT Press  2007.

[13] R´emi Gribonval and Pierre Machart. Reconciling ”priors” & ”priors” without prejudice? Re-

search report RR-8366  INRIA  September 2013.

9

,Remi Gribonval
Pierre Machart
Yanping Huang
Rajesh Rao