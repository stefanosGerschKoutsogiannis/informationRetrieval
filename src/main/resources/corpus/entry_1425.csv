2011,Demixed Principal Component Analysis,In many experiments  the data points collected live in high-dimensional observation spaces  yet can be assigned a set of labels or parameters. In electrophysiological recordings  for instance  the responses of populations of neurons generally depend on mixtures of experimentally controlled parameters. The heterogeneity and diversity of these parameter dependencies can make visualization and interpretation of such data extremely difficult. Standard dimensionality reduction techniques such as principal component analysis (PCA) can provide a succinct and complete description of the data  but the description is constructed independent of the relevant task variables and is often hard to interpret. Here  we start with the assumption that a particularly informative description is one that reveals the dependency of the high-dimensional data on the individual parameters. We show how to modify the loss function of PCA so that the principal components seek to capture both the maximum amount of variance about the data  while also depending on a minimum number of parameters. We call this method demixed principal component analysis (dPCA) as the principal components here segregate the parameter dependencies. We phrase the problem as a probabilistic graphical model  and present a fast Expectation-Maximization (EM) algorithm. We demonstrate the use of this algorithm for electrophysiological data and show that it serves to demix the parameter-dependence of a neural population response.,Demixed Principal Component Analysis

Wieland Brendel

Ranulfo Romo

Ecole Normale Supérieure  Paris  France
Champalimaud Neuroscience Programme

Instituto de Fisiología Celular

Universidad Nacional Autónoma de México

Lisbon  Portugal

Mexico City  Mexico

Christian K. Machens

Ecole Normale Supérieure  Paris  France

Champalimaud Neuroscience Programme  Lisbon  Portugal

Abstract

In many experiments  the data points collected live in high-dimensional observa-
tion spaces  yet can be assigned a set of labels or parameters. In electrophysio-
logical recordings  for instance  the responses of populations of neurons generally
depend on mixtures of experimentally controlled parameters. The heterogene-
ity and diversity of these parameter dependencies can make visualization and in-
terpretation of such data extremely difﬁcult. Standard dimensionality reduction
techniques such as principal component analysis (PCA) can provide a succinct
and complete description of the data  but the description is constructed indepen-
dent of the relevant task variables and is often hard to interpret. Here  we start
with the assumption that a particularly informative description is one that reveals
the dependency of the high-dimensional data on the individual parameters. We
show how to modify the loss function of PCA so that the principal components
seek to capture both the maximum amount of variance about the data  while also
depending on a minimum number of parameters. We call this method demixed
principal component analysis (dPCA) as the principal components here segregate
the parameter dependencies. We phrase the problem as a probabilistic graphical
model  and present a fast Expectation-Maximization (EM) algorithm. We demon-
strate the use of this algorithm for electrophysiological data and show that it serves
to demix the parameter-dependence of a neural population response.

1

Introduction

Samples of multivariate data are often connected with labels or parameters. In fMRI data or elec-
trophysiological data from awake behaving humans and animals  for instance  the multivariate data
may be the voxels of brain activity or the ﬁring rates of a population of neurons  and the parameters
may be sensory stimuli  behavioral choices  or simply the passage of time. In these cases  it is often
of interest to examine how the external parameters or labels are represented in the data set.
Such data sets can be analyzed with principal component analysis (PCA) and related dimensionality
reduction methods [4  2]. While these methods are usually successful in reducing the dimensionality
of the data  they do not take the parameters or labels into account. Not surprisingly  then  they
often fail to represent the data in a way that simpliﬁes the interpretation in terms of the underlying
parameters. On the other hand  dimensionality reduction methods that can take parameters into
account  such as canonical correlation analysis (CCA) or partial least squares (PLS) [1  5]  impose a
speciﬁc model of how the data depend on the parameters (e.g. linearly)  which can be too restrictive.
We illustrate these issues with neural recordings collected from the prefrontal cortex (PFC) of mon-
keys performing a two-frequency discrimination task [9  3  7]. In this experiment a monkey received

1

two mechanical vibrations with frequencies f1 and f2 on its ﬁngertip  delayed by three seconds. The
monkey then had to make a binary decision d depending on whether f1 > f2. In the data set  each
neuron has a unique ﬁring pattern  leading to a large diversity of neural responses. The ﬁring rates of
three neurons (out of a total of 842) are plotted in Fig. 1  top row. The responses of the neurons mix
information about the different task parameters  a common observation for data sets of recordings
in higher-order brain areas  and a problem that exacerbates interpretation of the data.
Here we address this problem by modifying PCA such that the principal components depend on
individual task parameters while still capturing as much variance as possible. Previous work has
addressed the question of how to demix data depending on two [7] or several parameters [8]  but did
not allow components that capture nonlinear mixtures of parameters. Here we extend this previous
work threefold: (1) we show how to systematically split the data into univariate and multivariate
parameter dependencies; (2) we show how this split suggests a simple loss function  capable of
demixing data with arbitrary combinations of parameters  (3) we introduce a probabilistic model for
our method and derive a fast algorithm using expectation-maximization.

2 Principal component analysis and the demixing problem

C =(cid:10)ytsdy(cid:62)

(cid:11)

The ﬁring rates of the neurons in our dataset depend on three external parameters: the time t  the
stimulus s = f1  and the decision d of the monkey. We omit the second frequency f2 since this
parameter is highly correlated with f1 and d (the monkey makes errors in < 10% of the trials). Each
sample of ﬁring rates in the population  yn  is therefore tagged with parameter values (tn  sn  dn).
For notational simplicity  we will assume that each data point is associated with a unique set of
parameter values so that the parameter values themselves can serve as indices for the data points yn.
In turn  we drop the index n  and simply write ytsd.
The main aim of PCA is to ﬁnd a new coordinate system in which the data can be represented in
a more succinct and compact fashion. The covariance matrix of the ﬁring rates summarizes the
second-order statistics of the data set 

(1)
and has size D × D where D is the number of neurons in the data set (we will assume the data are
centered throughout the paper). The angular bracket denotes averaging over all parameter values
(t  s  d) which corresponds to averaging over all data points. Given the covariance matrix  we can
compute the ﬁring rate variance that falls along arbitrary directions in state space. For instance  the
variance captured by a coordinate axis given by a normalized vector w is simply

tsd

tsd

L = w(cid:62)Cw.

(2)
The ﬁrst principal component corresponds to the axis that captures most of the variance of the data 
and thereby maximizes the function L subject to the normalization constraint w(cid:62)w = 1. The
second principal component maximizes variance in the orthogonal subspace and so on [4  2].
PCA succeeds nicely in summarizing the population response for our data set: the ﬁrst ten principal
components capture more than 90% of the variance of the data. However  PCA completely ignores
the causes of ﬁring rate variability. Whether ﬁring rates have changed due to the ﬁrst stimulus
frequency s = f1  due to the passage of time  t  or due to the decision  d  they will enter equally into
the computation of the covariance matrix and therefore do not inﬂuence the choice of the coordinate
system constructed by PCA. To clarify this observation  we will segregate the data ytsd into pieces
capturing the variability caused by different parameters.
Marginalized average. Let us denote the set of parameters by S = {t  s  d}. For every subset of
S we construct a ‘marginalized average’ 

¯yt := (cid:104)ytsd(cid:105)sd  

¯ys := (cid:104)ytsd(cid:105)td  

¯yts := (cid:104)ytsd(cid:105)d − ¯yt − ¯ys 

(3)
(4)
(5)
where (cid:104)ytsd(cid:105)φ denotes the average of the data over the subset φ ⊆ S. In ¯yt = (cid:104)ytsd(cid:105)sd  for instance 
we average over all parameter values (s  d) such that the remaining variation of the averaged data

¯ytsd := ytsd − ¯yts − ¯ytd − ¯ysd − ¯yt − ¯ys − ¯yd 

¯ysd := (cid:104)ytsd(cid:105)t − ¯ys − ¯yd 

¯ytd := (cid:104)ytsd(cid:105)s − ¯yt − ¯yd 

¯yd := (cid:104)ytsd(cid:105)ts

2

Figure 1: (Top row) Firing rates of three (out of D = 842) neurons recorded in the PFC of monkeys
discriminating two vibratory frequencies. The two stimuli were presented during the shaded periods.
The rainbow colors indicate different stimulus frequencies f1  black and gray indicate the decision
of the monkey during the interval [3.5 4.5] sec. (Bottom row) Relative contribution of time (blue) 
stimulus (light blue)  decision (green)  and non-linear mixtures (yellow) to the total variance for a
sample of 14 neurons (left)  the top 14 principal components (middle)  and naive demixing (right).

only comes from t. In ¯yts  we subtract all variation due to t or s individually  leaving only variation
that depends on combined changes of (t  s). These marginalized averages are orthogonal so that

∀φ  φ(cid:48) ⊆ S (cid:104)¯y(cid:62)

φ¯yφ(cid:48)(cid:105) = 0 if φ (cid:54)= φ(cid:48).

(6)

At the same time  their sum reconstructs the original data 

(7)
The latter two properties allow us to segregate the covariance matrix of ytsd into ‘marginalized
covariance matrices’ that capture the variance in a subset of parameters φ ⊆ S 

ytsd = ¯yt + ¯ys + ¯yd + ¯yts + ¯ytd + ¯ysd + ¯ytsd.

C = Ct + Cs + Cd + Cts + Ctd + Csd + Ctsd  with Cφ = (cid:104)¯yφ¯y(cid:62)
φ(cid:105).

Note that here we use the parameters {t  s  d} as labels  whereas they are indices in Eq. (3)-(5) 
and (7). For a given component w  the marginalized covariance matrices allow us to calculate the
variance xφ of w conditioned on φ ⊆ S as
φ = w(cid:62)Cφw 
x2
φ =: (cid:107)x(cid:107)2
2.
φ x2

so that the total variance is given by L =(cid:80)

t /(cid:107)x(cid:107)2

sd + x2

d + x2

td)/(cid:107)x(cid:107)2

s + x2
tsd)/(cid:107)x(cid:107)2

2)  decision (light blue; computed as (x2

Using this segregation  we are able to examine the distribution of variance in the PCA compo-
nents and the original data. In Fig. 1  bottom row  we plot the relative contributions of time (blue;
2)  stimulus (green; com-
computed as x2
ts)/(cid:107)x(cid:107)2
puted as (x2
2)  and nonlinear mixtures of stimulus and decision (yellow; computed as
(x2
2) for a set of sample neurons (left) and for the ﬁrst fourteen components of PCA
(center). The left plot shows that individual neurons carry varying degree of information about the
different task parameters  reafﬁrming the heterogeneity of neural responses. While the situation is
slightly better for the PCA components  we still ﬁnd a strong mixing of the task parameters.
To improve visualization of the data and to facilitate the interpretation of individual components  we
would prefer components that depend on only a single parameter  or  more generally  that depend on
the smallest number of parameters possible. At the same time  we would want to keep the attractive
properties of PCA in which every component captures as much variance as possible about the data.
Naively  we could simply combine eigenvectors from the marginalized covariance matrices. For ex-
ample  consider the ﬁrst Q eigenvectors of each marginalized covariance matrix. Apply symmetric

3

ﬁring rate (Hz)604530150time (s)01234604530150time (s)01234604530150time (s)01234PCAnaive demixingsample neurons1234567891011121314Figure 2: Illustration of the objective functions. The PCA objective function corresponds to the
L2-norm in the space of standard deviations  x. Whether a solution falls into the center or along
the axis does not matter  as long as it captures a maximum of overall variance. The dPCA objective
functions (with parameters λ = 1 and λ = 4) prefer solutions along the axes over solutions in the
center  even if the solutions along the axes capture less overall variance.

orthogonalization to these eigenvectors and choose the Q coordinates that capture the most variance.
The resulting variance distribution is plotted in Fig. 1 (bottom  right). While the parameter depen-
dence of the components is sparser than in PCA  there is a strong bias towards time  and variance
induced by the decision of the monkey is squeezed out. As a further drawback  naive demixing
covers only 84.6% of the total variance compared with 91.7% for PCA. We conclude that we have
to rely on a more systematic approach based speciﬁcally on an objective that promotes demixing.

3 Demixed principal component analysis (dPCA): Loss function

L = w(cid:62)Cw = (cid:80)

φ w(cid:62)Cφw = (cid:80)

φ x2

φ = (cid:107)x(cid:107)2

With respect to the segregated covariances  the PCA objective function  Eq. (2)  can be written as
2. This function is illustrated in Fig 2 (left)  and
shows that PCA will maximize variance  no matter whether this variance comes about through a
single marginalized variance  or through mixtures thereof.
Consequently  we need to modify this objective function such that solutions w that do not mix
variances—thereby falling along one of the axes in x-space—are favored over solutions w that fall
into the center in x-space. Hence  we seek an objective function L = L(x) that grows monotonically
with any xφ such that more variance is better  just as in PCA  and that grows faster along the axes
than in the center so that mixtures of variances get punished. A simple way of imposing this is

(cid:18)(cid:107)x(cid:107)2

(cid:19)λ

LdPCA = (cid:107)x(cid:107)2

2

(cid:107)x(cid:107)1

(8)
where λ ≥ 0 controls the tradeoff. This objective function is illustrated in Fig. 2 (center and right)
for two values of λ. Here  solutions w that lead to mixtures of variances are punished against
solutions that do not mix variances.
Note that the objective function is a function of the coordinate axis w  and the aim is to maximize
LdPCA with respect to w. A generalization to a set of Q components w1  . . .   wQ is straightforward
by maximizing L in steps for every component and ensuring orthonormality by means of symmetric
orthogonalization [6] after each step. We call the resulting algorithm demixed principal component
analysis (dPCA)  since it essentially can be seen as a generalization of standard PCA.

4 Probabilistic principal component analysis with orthogonality constraint

We introduced dPCA by means of a modiﬁcation of the objective function of PCA. It is straight-
forward to build a gradient ascent algorithm to solve Eq. (8). However  we aim for a superior algo-
rithm by framing dPCA in a probabilistic framework. A probabilistic model provides several beneﬁts
that include dealing with missing data and the inclusion of prior knowledge [see 2  p. 570]. Since
the probabilistic treatment of dPCA requires two modiﬁcations over the conventional expectation-
maximization (EM) algorithm for probabilistic PCA (PPCA)  we here review PPCA [11  10]  and
show how to introduce an explicit orthogonality constraint on the mixing matrix.

4

00dPCA (λ=1)x1x220406080100dPCA (λ=4)x1x220406080100PCA or dPCA (λ=0)x1x220406080100204060801000In PPCA  the observed data y are linear combinations of latent variables z

y = Wz + y

(cid:1). The latent variables are assumed to follow a zero-mean 

matrix. In turn  p(y|z) = N(cid:0)y|Wz  σ2ID
of the latent variables. Our aim is to maximize the likelihood of the data  p(Y) =(cid:81)

(9)
where y ∼ N (0  σ2ID) is isotropic Gaussian noise with variance σ2 and W ∈ RD×Q is the mixing
unit-covariance Gaussian prior  p(z) = N (z|0  IQ). These equations completely specify the model
of the data and allow us to compute the marginal distribution p(y).
Let Y = {yn} be the set of data points  with n = 1 . . . N  and Z = {zn} the corresponding values
n p(yn)  with
respect to the parameters W and σ. To this end  we use the EM algorithm  in which we ﬁrst calculate
the statistics (mean and covariance) of the posterior distribution  p(Z|Y)  given ﬁxed values for W
and σ2 (Expectation step). Then  using these statistics  we compute the expected complete-data
likelihood  E[p(Y  Z)]  and maximize it with respect to W and σ2 (Maximization step). We cycle
through the two steps until convergence.
Expectation step. The posterior distribution p(Z|Y) is again Gaussian and given by

N(cid:0)zn

(cid:12)(cid:12)M−1W(cid:62)yn  σ2M−1(cid:1) with M = W(cid:62)W + σ2IQ.

p(Z|Y) =

(10)

Mean and covariance can be read off the arguments  and we note in particular that E[znz(cid:62)
n ] =
σ2M−1 + E[zn]E[zn](cid:62). We can then take the expectation of the complete-data log likelihood with
respect to this posterior distribution  so that

N(cid:89)

n=1

E(cid:2)ln p(cid:0)Y  Z(cid:12)(cid:12)W  σ2(cid:1)(cid:3) = − N(cid:88)

1

2

n=1
1

2σ2 (cid:107)yn(cid:107)2 − 1

(cid:26) D
ln(cid:0)2πσ2(cid:1) +
2σ2 Tr(cid:0)E(cid:2)znz(cid:62)
(cid:110)(cid:107)yn(cid:107)2 − 2E [zn](cid:62) W(cid:62)yn + Tr(cid:0)E(cid:2)znz(cid:62)
N(cid:88)

(cid:3) W(cid:62)W(cid:1) + Q

ln (2π) +

1
2

σ2

+

2

n

n

E [zn](cid:62) W(cid:62)yn

Tr(cid:0)E(cid:2)znz(cid:62)
(cid:3) W(cid:62)W(cid:1)(cid:111)

n

.

(cid:3)(cid:1)(cid:27)

.

(11)

(12)

Maximization step. Next  we need to maximize Eq. (11) with respect to σ and W. For σ  we obtain

(σ∗)2 =

1
N D

n=1

For W  we need to deviate from the conventional PPCA algorithm  since the development of proba-
bilistic dPCA requires an explicit orthogonality constraint on W  which had so far not been included
in PPCA. To impose this constraint  we factorize W into an orthogonal and a diagonal matrix 

(13)
where U ∈ RD×Q has orthogonal columns of unit length and Γ ∈ RQ×Q is diagonal. In order to
maximize Eq. (11) with respect to U and Γ we make use of inﬁnitesimal translations in the respective
restricted space of matrices 

W = UΓ  U(cid:62)U = ID

U → (ID + A) U 

Γ → (IQ +  diag(b)) Γ 

(14)
where A ∈ SkewD is D × D skew-symmetric  b ∈ RQ  and  (cid:28) 1. The set of D × D skew-
symmetric matrices are the generators of rotations in the space of orthogonal matrices. The neces-
sary conditions for a maximum of the likelihood function at U∗  Γ∗ are

E(cid:2)ln p(cid:0)Y  Z(cid:12)(cid:12)(ID + A) U∗Γ  σ2(cid:1)(cid:3) − E(cid:2)ln p(cid:0)Y  Z(cid:12)(cid:12)U∗Γ  σ2(cid:1)(cid:3) = 0 + O(cid:0)2(cid:1) ∀A ∈ SkewD 
E(cid:2)ln p(cid:0)Y  Z(cid:12)(cid:12)U (IQ +  diag(b)) Γ∗  σ2(cid:1)(cid:3) − E(cid:2)ln p(cid:0)Y  Z(cid:12)(cid:12)UΓ∗  σ2(cid:1)(cid:3) = 0 + O(cid:0)2(cid:1) ∀b ∈ RD.
(cid:3) Γ = KΣL(cid:62)  the maximum is
Given the reduced singular value decomposition1 of(cid:80)
(cid:3)(cid:17)
E(cid:2)znz(cid:62)
ynE(cid:2)z(cid:62)

n ynE(cid:2)z(cid:62)
(cid:16)(cid:88)

U∗ = KL(cid:62)
Γ∗ = diag

U(cid:62)(cid:88)

(cid:3)(cid:17)−1

(cid:16)

diag

(16)

(15)

(17)

(18)

n

n

n

1The reduced singular value decomposition factorizes a D×Q matrix A as A = KDL∗  where K is a D×Q

unitary matrix  D is a Q × Q nonnegative  real diagonal matrix  and L∗ is a Q × Q unitary matrix.

n

n

5

Figure 3: (a) Graphical representation of the general idea of dPCA. Here  the data y are projected on
a subspace z of latent variables. Each latent variable zi depends on a set of parameters θj ∈ S. To
ease interpretation of the latent variables zi  we impose a sparse mapping between the parameters
and the latent variables. (b) Full graphical model of dPCA.

where diag(A) returns a square matrix with the same diagonal as A but with all off-diagonal elements
set to zero.

5 Probabilistic demixed principal component analysis

We described a PPCA EM-algorithm with an explicit constraint on the orthogonality of the columns
of W. So far  variance due to different parameters in the data set are completely mixed in the latent
variables z. The essential idea of dPCA is to demix these parameter dependencies by sparsifying
the mapping from parameters to latent variables (see Fig. 3a). Since we do not want to impose
the nature of this mapping (which is to remain non-parametric)  we suggest a model in which each
latent variable zi is segregated into (and replaced by) a set of R latent variables {zφ i}  each of which
depends on a subset φ ⊆ S of parameters. Note that R is the number of all subsets of S  exempting

the empty set. We require zi =(cid:80)

φ⊆S zφ i  so that

Wzφ + y

(19)

y =(cid:88)

φ⊆S

(cid:88)

φ⊆S

with y ∼ N (0  σ2ID)  see also Fig. 3b. The priors over the latent variables are speciﬁed as

(20)
where Λφ is a row in Λ ∈ RR×Q  the matrix of variances for all latent variables. The covariance of
the sum of the latent variables shall again be the identity matrix 

p(zφ) = N (zφ|0  diagΛφ)

diag Λφ = IQ.

(21)

of the latent variables zi = (cid:80)

This completely speciﬁes our model. As before  we will use the EM-algorithm to maximize the
model evidence p(Y) with respect to the parameters Λ  W  σ. However  we additionally impose that
each column Λi of Λ shall be sparse  thereby ensuring that the diversity of parameter dependencies
φ zφ i is reduced. Note that Λi is proportional to the vector x with
elements xφ introduced in section 3. This links the probabilistic model to the loss function in Eq. (8).
Expectation step. Due to the implicit parameter dependencies of the latent variables  the sets of
variables Zφ = {zn
φ} can only depend on the respective marginalized averages of the data. The
posterior distribution over all latent variables Z = {Zφ} therefore factorizes such that

p(Zφ|¯Yφ)

(22)

p(Z|Y) = (cid:89)

φ⊆S

6

bθ1θ2z1z2z3z4y1y2y3y4y5yzσ1WΛσ2NzσL...aAlgorithm 1: demixed Principal Component Analysis (dPCA)

Input: Data Y  # components Q
Algorithm:

U(k=1) ← ﬁrst Q principal components of y 
repeat
φ   U(k)  Γ(k)  σ(k)  Λ(k) → update using (25)  (17)  (18)  (12) and (30)
M(k)
k ← k + 1

I(k=1) ← IQ

until p(Y) converges

Hence  the expectation of the complete-data log-likelihood function is modiﬁed from Eq. (11) 

where

E(cid:2)ln p(cid:0)Y  Z(cid:12)(cid:12)W  σ2(cid:1)(cid:3) = − N(cid:88)

where ¯Yφ = {¯yn
φ} are the marginalized averages over the complete data set. For three parameters 
the marginalized averages were speciﬁed in Eq. (3)-(7). For more than three parameters  we obtain
(23)

(−1)|τ| (cid:104)y(cid:105)n

φ = (cid:104)y(cid:105)n
¯yn

(S\φ)∪τ .

where (cid:104)y(cid:105)n
average to the respective data point.2 In turn  the posterior of Zφ takes the form

ψ denotes averaging of the data over the parameter subset ψ. The index n refers the

p(Zφ|¯Yφ) =

φ W(cid:62)¯yn

φ  σ2M−1

φ

(cid:17)

τ⊆φ

(S\φ) +(cid:88)
N(cid:16)
N(cid:89)

zn
φ

n=1

(cid:12)(cid:12)(cid:12)M−1

Mφ = W(cid:62)W + σ2 diag Λ−1
φ .

ln(cid:0)2πσ2(cid:1) +

(cid:40)
2σ2 Tr(cid:0)E(cid:2)zn

n=1
1

D
2

φzn(cid:62)

φ

ln det diag (Λφ) +

+

+

1
2

1

φ⊆S

ln (2π)

2
W(cid:62)yn

(cid:26) Q
2σ2 (cid:107)yn(cid:107)2 +(cid:88)
(cid:3)(cid:62)
(cid:3) W(cid:62)W(cid:1) − 1
E(cid:2)zn
(cid:3) diag (Λφ)−1(cid:17)(cid:27)(cid:41)
(cid:16)E(cid:2)zn
E [zφ] and E[zz(cid:62)] = (cid:80)

σ2
φzn(cid:62)

1
2

Tr

φ

φ

.

φ zφ  so that E [z] = (cid:80)

of marginalized averages (cid:80)

Maximization Step. Comparison of Eq. (11) and Eq. (26) shows that the maximum-likelihood
estimates of W = UΓ and of σ2 are unchanged (this can be seen by substituting z for the sum
φ ]). The
maximization with respect to Λ is more involved because we have to respect constraints from two
sides. First  Eq. (21) constrains the L1-norm of the columns Λi of Λ. Second  since we aim for
components depending only on a small subset of parameters  we have to introduce another constraint
to promote sparsity of Λi. Though this constraint is rather arbitrary  we found that constraining all
but one entry of Λi to be zero works quite effectively  so that (cid:107)Λi(cid:107)0 = 1. Consequently  for each
column Λi of Λ  the maximization of the expected likelihood  L  Eq. (26)  is given by

E[zφz(cid:62)

φ

φ

Deﬁning Bφi =(cid:80)

Λi → arg max
E[zn

L (Λi) = −(cid:88)

φizn

Λi

n

L (Λi)

s.t.

(cid:0) ln Λφi + BφiΛ−1

(cid:107)Λi(cid:107)1 = 1 and (cid:107)Λi(cid:107)0 = 1.
(cid:1)

φi]  the relevant terms in the likelihood can be written as

= − ln(1 − m) − Bφ(cid:48)i(1 − m)−1 −(cid:88)

φi

φ

(ln  + Bφi−1)

φ∈J

2To see through this notation  notice that the n-th data point yn or yn is tagged with parameter values
θn = (θ1 n  θ2 n  . . .). Any average over a subset ψ = S \ φ of the parameters leaves vectors (cid:104)y(cid:105)ψ that still
depend on some remaining parameters  φ = θrest. We can therefore take their values for the n-th data point 
rest  and assign the respective value of the average to the n-data point as well  writing (cid:104)y(cid:105)n
ψ.
θn

7

(24)

(25)

(26)

(27)

(28)

(29)

Figure 4: On the left we plot the relative variance of the fourteen highest components in dPCA
conditioned on time (blue)  stimulus (light blue)  decision (green) and non-linear mixtures (yellow).
On the right the ﬁring rates of six dPCA components are displayed in three columns separated into
components with the highest variance in time (left)  in decision (middle) and in the stimulus (right).

where φ(cid:48) is the index of the non-zero entry of Λi  and J is the complementing index set (of length
m = R − 1) of all zero-entries which have been set to  (cid:28) 1 for regularization purposes. Since  is
small  its inverse is very large. Accordingly  the likelihood is maximized for the index φ(cid:48) referring
to the largest entry in Bφi  so that

(cid:26) 1

if (cid:80)

φi] ≥(cid:80)

E[zn

φizn

Λφi =

(30)
More generally  it is possible to substitute the sparsity constraint with (cid:107)Λi(cid:107)0 = K for K > 1 and
maximize L (Λi) numerically. The full algorithm for dPCA is summarized in Algorithm 1.

0

n

ψi] for all ψ (cid:54)= φ

ψizn

E[zn
otherwise

n

6 Experimental results

The results of the dPCA algorithm applied to the electrophysiological data from the PFC are shown
in Fig. 4. With 90% of the total variance in the ﬁrst fourteen components  dPCA captures a compa-
rable amount of variance as PCA (91.7%). The distribution of variances in the dPCA components
is shown in Fig. 4  left. Note that  compared with the distribution in the PCA components (Fig. 1 
bottom  center)  the dPCA components clearly separate the different sources of variability. More
speciﬁcally  the neural population is dominated by components that only depend on time (blue)  yet
also features separate components for the monkey’s decision (green) and the perception of the stim-
ulus (light blue). The components of dPCA  of which the six most prominent are displayed in Fig. 4 
right  therefore reﬂect and separate the parameter dependencies of the data  even though these de-
pendencies were completely intermingled on the single neuron level (compare Fig. 1  bottom  left).

7 Conclusions

Dimensionality reduction methods that take labels or parameters into account have recently found a
resurgence in interest. Our study was motivated by the speciﬁc problems related to electrophysiolog-
ical data sets. The main aim of our method—demixing parameter dependencies of high-dimensional
data sets—may be useful in other context as well. Very similar problems arise in fMRI data  for in-
stance  and dPCA could provide a useful alternative to other dimensionality reduction methods such
as CCA  PLS  or Supervised PCA [1  12  5]. Furthermore  the general aim of demixing dependencies
could likely be extended to other methods (such as ICA) as well. Ultimately  we see dPCA as a par-
ticular data visualization technique that will prove useful if a demixing of parameter dependencies
aids in understanding data.
The source code both for Python and Matlab can be found at https://sourceforge.net/projects/dpca/.

8

2501250-125-250ﬁring rate (Hz)ﬁring rate (Hz)time (s)012342501250-125-250time (s)01234time (s)012341234567891011121314dPCAReferences
[1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis.

Technical Report 688  University of California  Berkeley  2005.

[2] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics).

Springer  2006.

[3] C. D. Brody  A. Hernández  A. Zainos  and R. Romo. Timing and neural encoding of so-
matosensory parametric working memory in macaque prefrontal cortex. Cerebral Cortex 
13(11):1196–1207  2003.

[4] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning. Springer 

2001.

[5] A. Krishnan  L. J. Williams  A. R. McIntosh  and H. Abdi. Partial least squares (PLS) methods

for neuroimaging: a tutorial and review. NeuroImage  56:455–475  2011.

[6] P.-O. Lowdin. On the non-orthogonality problem connected with the use of atomic wave
functions in the theory of molecules and crystals. The Journal of Chemical Physics  18(3):365 
1950.

[7] C. K. Machens. Demixing population activity in higher cortical areas. Frontiers in computa-

tional neuroscience  4(October):8  2010.

[8] C. K. Machens  R. Romo  and C. D. Brody. Functional  but not anatomical  separation of

“what” and “when” in prefrontal cortex. Journal of Neuroscience  30(1):350–360  2010.

[9] R. Romo  C. D. Brody  A. Hernandez  and L. Lemus. Neuronal correlates of parametric work-

ing memory in the prefrontal cortex. Nature  399(6735):470–473  1999.

[10] S. Roweis. EM algorithms for PCA and SPCA. Advances in neural information processing

systems  10:626–632  1998.

[11] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the

Royal Statistical Society - Series B: Statistical Methodology  61(3):611–622  1999.

[12] S. Yu  K. Yu  V. Tresp  H. P. Kriegel  and M. Wu. Supervised probabilistic principal component

analysis. Proceedings of 12th ACM SIGKDD International Conf. on KDD  10  2006.

9

,Arturo Deza
Miguel Eckstein