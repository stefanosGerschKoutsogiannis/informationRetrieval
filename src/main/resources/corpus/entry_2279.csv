2018,Generalizing to Unseen Domains via Adversarial Data Augmentation,We are concerned with learning models that generalize well to different unseen
domains. We consider a worst-case formulation over data distributions that are
near the source domain in the feature space. Only using training data from a single
source distribution  we propose an iterative procedure that augments the dataset
with examples from a fictitious target domain that is "hard" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses  we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g.  ridge or lasso). On digit recognition and semantic segmentation tasks  our method learns models improve performance across a range of a priori unknown target domains.,Generalizing to Unseen Domains
via Adversarial Data Augmentation

Riccardo Volpi∗ †

Istituto Italiano di Tecnologia

Hongseok Namkoong∗
Stanford University

Ozan Sener
Intel Labs

John Duchi

Stanford University

Vittorio Murino

Istituto Italiano di Tecnologia

Università di Verona

Silvio Savarese

Stanford University

Abstract

We are concerned with learning models that generalize well to different unseen
domains. We consider a worst-case formulation over data distributions that are
near the source domain in the feature space. Only using training data from a single
source distribution  we propose an iterative procedure that augments the dataset
with examples from a ﬁctitious target domain that is "hard" under the current model.
We show that our iterative scheme is an adaptive data augmentation method where
we append adversarial examples at each iteration. For softmax losses  we show that
our method is a data-dependent regularization scheme that behaves differently from
classical regularizers that regularize towards zero (e.g.  ridge or lasso). On digit
recognition and semantic segmentation tasks  our method learns models improve
performance across a range of a priori unknown target domains.

1

Introduction

In many modern applications of machine learning  we wish to learn a system that can perform
uniformly well across multiple populations. Due to high costs of data acquisition  however  it is
often the case that datasets consist of a limited number of population sources. Standard models that
perform well when evaluated on the validation dataset—usually collected from the same population
as the training dataset—often perform poorly on populations different from that of the training
data [15  3  1  32  38]. In this paper  we are concerned with generalizing to populations different
from the training distribution  in settings where we have no access to any data from the unknown
target distributions. For example  consider a module for self-driving cars that needs to generalize
well across weather conditions and city environments unexplored during training.
A number of authors have proposed domain adaptation methods (for example  see [9  39  36  26  40])
in settings where a fully labeled source dataset and an unlabeled (or partially labeled) set of examples
from ﬁxed target distributions are available. Although such algorithms can successfully learn models
that perform well on known target distributions  the assumption of a priori ﬁxed target distributions
can be restrictive in practical scenarios. For example  consider a semantic segmentation algorithm
used by a robot: every task  robot  environment and camera conﬁguration will result in a different
target distribution  and these diverse scenarios can be identiﬁed only after the model is trained and
deployed  making it difﬁcult to collect samples from them.
In this work  we develop methods that can learn to better generalize to new unknown domains. We
consider the restrictive setting where training data only comes from a single source domain. Inspired

∗Equal contribution.
†Work done while author was a Visiting Student Researcher at Stanford University.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

by recent developments in distributionally robust optimization and adversarial training [34  20  12] 
we consider the following worst-case problem around the (training) source distribution P0

minimize

θ∈Θ

sup

P :D(P P0)≤ρ

EP [(cid:96)(θ; (X  Y ))].

(1)

Here  θ ∈ Θ is the model  (X  Y ) ∈ X × Y is a source data point with its labeling  (cid:96) : X × Y → R
is the loss function  and D(P  Q) is a distance metric on the space of probability distributions.
The solution to worst-case problem (1) guarantees good performance against data distributions that
are distance ρ away from the source domain P0. To allow data distributions that have different support
to that of the source P0  we use Wasserstein distances as our metric D. Our distance will be deﬁned
on the semantic space 3  so that target populations P satisfying D(P  P0) ≤ ρ represent realistic
covariate shifts that preserve the same semantic representation of the source (e.g.  adding color to a
greyscale image). In this regard  we expect the solution to the worst-case problem (1)—the model
that we wish to learn—to have favorable performance across covariate shifts in the semantic space.
We propose an iterative procedure that aims to solve the problem (1) for a small value of ρ at a time 
and does stochastic gradient updates to the model θ with respect to these ﬁctitious worst-case target
distributions (Section 2). Each iteration of our method uses small values of ρ  and we provide a
number of theoretical interpretations of our method. First  we show that our iterative algorithm is an
adaptive data augmentation method where we add adversarially perturbed samples—at the current
model—to the dataset (Section 3). More precisely  our adversarially generated samples roughly
correspond to Tikhonov regularized Newton-steps [21  25] on the loss in the semantic space. Further 
we show that for softmax losses  each iteration of our method can be thought of as a data-dependent
regularization scheme where we regularize towards the parameter vector corresponding to the true
label  instead of regularizing towards zero like classical regularizers such as ridge or lasso.
From a practical viewpoint  a key difﬁculty in applying the worst-case formulation (1) is that the
magnitude of the covariate shift ρ is a priori unknown. We propose to learn an ensemble of models
that correspond to different distances ρ. In other words  our iterative method generates a collection
of datasets  each corresponding to a different inter-dataset distance level ρ  and we learn a model
for each of them. At test time  we use a heuristic method to choose an appropriate model from the
ensemble.
We test our approaches on a simple digit recognition task  and a more realistic semantic segmentation
task across different seasons and weather conditions. In both settings  we observe that our method
allows to learn models that improve performance across a priori unknown target distributions that
have varying distance from the original source domain.

Related work

The literature on adversarial training [10  34  20  12] is closely related to our work  since the main
goal is to devise training procedures that learn models robust to ﬂuctuations in the input. Departing
from imperceptible attacks considered in adversarial training  we aim to learn models that are resistant
to larger perturbations  namely out-of-distribution samples. Sinha et al. [34] proposes a principled
adversarial training procedure  where new images that maximize some risk are generated and the
model parameters are optimized with respect to those adversarial images. Being devised for defense
against imperceptible adversarial attacks  the new images are learned with a loss that penalizes
differences between the original and the new ones. In this work  we rely on a minimax game similar
to the one proposed by Sinha et al. [34]  but we impose the constraint in the semantic space  in order
to allow our adversarial samples from a ﬁctitious distribution to be different at the pixel level  while
sharing the same semantics.
There is a substantial body of work on domain adaptation [15  3  32  9  39  36  26  40]  which aims to
better generalize to a priori ﬁxed target domains whose labels are unknown at training time. This setup
is different from ours in that these algorithms require access to samples from the target distribution
during training. Domain generalization methods [28  22  27  33  24] that propose different ways
to better generalize to unknown domains are also related to our work. These algorithms require

3By semantic space we mean learned representations since recent works [7  16] suggest that distances in the
space of learned representations of high capacity models typically correspond to semantic distances in visual
space.

2

the training samples to be drawn from different domains (while having access to the domain labels
during training)  not a single source  a limitation that our method does not have. In this sense  one
could interpret our problem setting as unsupervised domain generalization. Tobin et al. [37] proposes
domain randomization  which applies to simulated data and creates a variety of random renderings
with the simulator  hoping that the real world will be interpreted as one of them. Our goal is the same 
since we aim at obtaining data distributions more similar to the real world ones  but we accomplish it
by actually learning new data points  and thus making our approach applicable to any data source
and without the need of a simulator.
Hendrycks and Gimpel [13] suggest that a good empirical way to detect whether a test sample is
out-of-distribution for a given model is to evaluate the statistics of the softmax outputs. We adapt this
idea in our setting  learning ensemble of models trained with our method and choosing at test time
the model with the greatest maximum softmax value.

2 Method

The worst-case formulation (1) over domains around the source P0 hinges on the notion of distance
D(P  P0)  that characterizes the set of unknown populations we wish to generalize to. Conventional
notions of Wasserstein distance used for adversarial training [34] are deﬁned with respect to the
original input space X   which for images corresponds to raw pixels. Since our goal is to consider
ﬁctitious target distributions corresponding to realistic covariate shifts  we deﬁne our distance on
the semantic space. Before properly deﬁning our setup  we ﬁrst give a few notations. Letting p the
dimension of output of the last hidden layer  we denote θ = (θc  θf ) where θc ∈ Rp×m is the set of
weights of the ﬁnal layer  and θf is the rest of the weights of the network. We denote by g(θf ; x) the
output of the embedding layer of our neural network. For example  in the classiﬁcation setting  m is
the number of classes and we consider the softmax loss

exp(cid:0)θ(cid:62)
j=1 exp(cid:0)θ(cid:62)
(cid:80)m

c yg(θf ; x)(cid:1)
c jg(θf ; x)(cid:1)

(cid:96)(θ; (x  y)) := − log

(2)

where θc j is the j-th column of the classiﬁcation layer weights θc ∈ Rp×m.
Wasserstein distance on the semantic space On the space Rp × Y  consider the following trans-
portation cost c—cost of moving mass from (z  y) to (z(cid:48)  y(cid:48))

c((z  y)  (z(cid:48)  y(cid:48))) :=

(cid:107)z − z(cid:48)(cid:107)2

2 + ∞ · 1{y (cid:54)= y(cid:48)} .

1
2

The transportation cost takes value ∞ for data points with different labels  since we are only interested
in perturbation to the marginal distribution of Z. We now deﬁne our notion of distance on the semantic
space. For inputs coming from the original space X × Y  we consider the transportation cost cθ
deﬁned with respect to the output of the last hidden layer

cθ((x  y)  (x(cid:48)  y(cid:48))) := c((g(θf ; x)  y)  (g(θf ; x(cid:48))  y(cid:48)))

so that cθ measures distance with respect to the feature mapping g(θf ; x). For probability measures
P and Q both supported on X × Y  let Π(P  Q) denote their couplings  meaning measures M with
M (A X × Y) = P (A) and M (X × Y  A) = Q(A). Then  we deﬁne our notion of distance by

Dθ(P  Q) :=

inf

M∈Π(P Q)

EM [cθ((X  Y )  (X(cid:48)  Y (cid:48)))].

(3)

Armed with this notion of distance on the semantic space  we now consider a variant of the worst-case
problem (1) where we replace the distance with Dθ (3)  our adaptive notion of distance deﬁned on
the semantic space

minimize

θ∈Θ

sup
P

{EP [(cid:96)(θ; (X  Y ))] : Dθ(P  P0) ≤ ρ} .

Computationally  the above supremum over probability distributions is intractable. Hence  we
consider the following Lagrangian relaxation with penalty parameter γ

minimize

θ∈Θ

sup
P

{EP [(cid:96)(θ; (X  Y ))] − γDθ(P  P0)} .

(4)

3

(cid:46) Run the minimax procedure K times

Algorithm 1 Adversarial Data Augmentation

for t = 1  ...  Tmin do

Sample (Xt  Yt) uniformly from dataset
θ ← θ − α∇θ(cid:96)(θ; (Xt  Yt))

Input: original dataset {Xi  Yi}i=1 ... n and initialized weights θ0
Output: learned weights θ
1: Initialize: θ ← θ0
2: for k = 1  ...  K do
3:
4:
5:
6:
7:
8:
9:
i ← X k
X k
10:
Append (X k
11:
12: for t = 1  . . .   T do
13:
14:

i ← Xi
X k
for t = 1  . . .   Tmax do
i + η∇x
i ) to dataset
i   Y k

Sample {Xi  Yi}i=1 ... n uniformly from the dataset
for i = 1  . . .   n do

Sample (X  Y ) uniformly from dataset
θ ← θ − α∇θ(cid:96)(θ; (X  Y ))

(cid:8)(cid:96)(θ; (X k

i   Yi)) − γcθ((X k

i   Yi)  (Xi  Yi))(cid:9)

Taking the dual reformulation of the penalty relaxation (4)  we can obtain an efﬁcient solution
procedure. The following result is a minor adaptation of [2  Theorem 1]; to ease notation  let us
deﬁne the robust surrogate loss

φγ(θ; (x0  y0)) := sup
x∈X

(5)
Lemma 1. Let (cid:96) : Θ× (X ×Y) → R be continuous. For any distribution Q and any γ ≥ 0  we have
(6)

{EP [(cid:96)(θ; (X  Y ))] − γDθ(P  Q)} = EQ[φγ(θ; (X  Y ))].

{(cid:96)(θ; (x  y0)) − γcθ((x  y0)  (x0  y0))} .

sup
P

In order to solve the penalty problem (4)  we can now perform stochastic gradient descent procedures
on the robust surrogate loss φγ. Under suitable conditions [5]  we have

∇θφγ(θ; (x0  y0)) = ∇θ(cid:96)(θ; (x(cid:63)

(7)
γ = arg maxx∈X {(cid:96)(θ; (x  y0)) − γcθ((x  y0)  (x0  y0))} is an adversarial perturbation of
where x(cid:63)
x0 at the current model θ. Hence  computing gradients of the robust surrogate φγ requires solving
the maximization problem (5). Below  we consider an (heuristic) iterative procedure that iteratively
performs stochastic gradient steps on the robust surrogate φγ.

γ  y0)) 

Iterative Procedure We propose an iterative training procedure where two phases are alternated:
a maximization phase where new data points are learned by computing the inner maximization
problem (5) and a minimization phase  where the model parameters are updated according to stochastic
gradients of the loss evaluated on the adversarial examples generated from the maximization phase.
The latter step is equivalent to stochastic gradient steps on the robust surrogate loss φγ  which
motivates its name. The main idea here is to iteratively learn "hard" data points from ﬁctitious target
distributions  while preserving the semantic features of the original data points.
Concretely  in the k-th maximization phase  we compute n adversarially perturbed samples at the
current model θ ∈ Θ

(cid:8)(cid:96)(θ; (x  Yi)) − γcθ((x  Yi)  (X k−1

  Yi))(cid:9)

(8)

i ∈ arg max
X k

x∈X

i

i are the original samples from the source distribution P0. The minimization phase then per-
where X 0
forms repeated stochastic gradient steps on the augmented dataset {X k
i   Yi}0≤k≤K 1≤i≤n}. The max-
imization phase (8) can be efﬁciently computed for smooth losses if x (cid:55)→ cθk−1((x  Yi)  (X k−1
  Yi))
is strongly convex [34  Theorem 2]; for example  this is provably true for any linear network. In
practice  we use gradient ascent steps to solve for worst-case examples (8); see Algorithm 1 for the
full description of our algorithm.

i

4

Ensembles for classiﬁcation The hyperparameter γ—which is inversely proportional to ρ  the
distance between the ﬁctitious target distribution and the source—controls the ability to generalize
outside the source domain. Since target domains are unknown  it is difﬁcult to choose an appropriate

level of γ a priori. We propose a heuristic ensemble approach where we train s models(cid:8)θ0  ...  θs(cid:9).

Each model is associated with a different value of γ  and thus to ﬁctitious target distributions with
varying distances from the source P0. To select the best model at test time—inspired by Hendrycks
and Gimpel [13]—given a sample x  we select the model θu(cid:63)(x) with the greatest softmax score

u(cid:63)(x) := arg max
1≤u≤s

max
1≤j≤k

θu(cid:62)
c j g(θu

f ; x).

(9)

3 Theoretical Motivation

In our iterative algorithm (Algorithm 1)  the maximization phase (8) was a key step that augmented
the dataset with adversarially perturbed data points  which was followed by standard stochastic
gradient updates to the model parameters. In this section  we provide some theoretical understanding
of the augmentation step (8). First  we show that the augmented data points (8) can be interpreted as
Tikhonov regularized Newton-steps [21  25] in the semantic space under the current model. Roughly
speaking  this gives the sense in which Algorithm 1 is an adaptive data augmentation algorithm that
adds data points from ﬁctitious "hard" target distributions. Secondly  recall the robust surrogate
loss (5) whose stochastic gradients were used to update the model parameters θ in the minimization
step (Eq (7)). In the classiﬁcation setting  we show that the robust surrogate (5) roughly corresponds
to a novel data-dependent regularization scheme on the softmax loss (cid:96). Instead of penalizing towards
zero like classical regularizers (e.g.  ridge or lasso)  our data-dependent regularization term penalizes
deviations from the parameter vector corresponding to that of the true label.

3.1 Adaptive Data Augmentation

We now give an interpretation for the augmented data points in the maximization phase (8). Concretely 
we ﬁx θ ∈ Θ  x0 ∈ X   y0 ∈ Y  and consider an -maximizer

 ∈ - arg max
x(cid:63)

{(cid:96)(θ; (x  y0)) − γcθ((x  y0)  (x0  y0))} .

We let z0 := g(θf ; x0) ∈ Rp  and abuse notation by using (cid:96)(θ; (z0  y0)) := (cid:96)(θ; (x0  y0)). In what
follows  we show that the feature mapping g(θf ; x(cid:63)

 ) satisﬁes

x∈X

(cid:18)

+

L2

3(γ − L1)

5

(cid:19)−1 ∇z(cid:96)(θ; (z0  y0))
(cid:125)

(cid:18)(cid:114) 

+O

+

1
γ2

γ

(cid:19)

.

g(θf ; x(cid:63)

 ) = g(θf ; x0) +

(cid:124)

1
γ

I − 1
γ

(cid:123)(cid:122)

∇zz(cid:96)(θ; (z0  y0))
=:(cid:98)gnewton(θf ;x0)

Intuitively  this implies that the adversarially perturbed sample x(cid:63)

distribution where probability mass on z0 = g(θf ; x0) was transported to(cid:98)gnewton(θf ; x0). We note
25] on the loss z (cid:55)→ (cid:96)(θ; (z  y0)) at the current model θ. Noting that computing(cid:98)gnewton(θf ; x0)

that the transported point in the semantic space corresponds to a Tikhonov regularized Newton-step [21 

(10)
 is drawn from a ﬁctitious target

involves backsolves on a large dense matrix  we can interpret our gradient ascent updates in the
maximization phase (8) as an iterative scheme for approximating this quantity.
We assume sufﬁcient smoothness  where we use (cid:107)H(cid:107) to denote the (cid:96)2-operator norm of a matrix H.
Assumption 1. There exists L0  L1 > 0 such that  for all z  z(cid:48) ∈ Rp  we have |(cid:96)(θ; (z  y0)) −
(cid:96)(θ; (z(cid:48)  y0))| ≤ L0 (cid:107)z − z(cid:48)(cid:107)2 and (cid:107)∇z(cid:96)(θ; (z  y0)) − ∇z(cid:96)(θ; (z(cid:48)  y0))(cid:107)2 ≤ L1 (cid:107)z − z(cid:48)(cid:107)2.
for all z  z(cid:48) ∈ Rp  we have
Assumption 2. There exists L2 > 0 such that 
(cid:107)∇zz(cid:96)(θ; (z  y0)) − ∇zz(cid:96)(θ; (z(cid:48)  y0))(cid:107) ≤ L2 (cid:107)z − z(cid:48)(cid:107)2.
Then  we have the following bound (10) whose proof we defer to Appendix A.1.
Theorem 1. Let Assumptions 1  2 hold. If Im(g(θf ;·)) = Rp and γ > L1  then

(cid:40)(cid:18) 5L0

(cid:19)3

(cid:18) L0

(cid:19)3

γ

+

γ − L1

+

γ

2(cid:41)
(cid:19) 3

(cid:18) 2

.

 ) −(cid:98)gnewton(θf ; x0)(cid:107)2

(cid:107)g(θf ; x(cid:63)

2 ≤ 2
γ − L1

3.2 Data-Dependent Regularization

In this section  we argue that under suitable conditions on the loss 
(cid:107)∇z(cid:96)(θ; (z  y))(cid:107)2

φγ(θ; (z  y)) = (cid:96)(θ; (z  y)) +

2 + O

1
γ

(cid:18) 1

(cid:19)

γ2

.

For classiﬁcation problems  we show that the robust surrogate loss (5) corresponds to a particular
data-dependent regularization scheme. Let (cid:96)(θ; (x  y)) be the m-class softmax loss (2) given by

(cid:80)m

exp(θ(cid:62)
l=1 exp(θ(cid:62)

c jg(θ  x))

.

c lg(θf ; x))

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

2

(cid:18) 1

(cid:19)

γ2

pj(θ  x)θc j

+ O

.

(11)

where θc j ∈ Rp is the j-th row of the classiﬁcation layer weight θc ∈ Rp×m. Then  the robust
surrogate φγ is an approximate regularizer on the classiﬁcation layer weights θc

(cid:96)(θ; (x  y)) = − log py(θ  x) where pj(θ  x) :=

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)θc y − m(cid:88)
larization where we minimize the distance between(cid:80)m

φγ(θ; (x  y)) = (cid:96)(θ; (x  y)) +

1
γ

j=1

The expansion (11) shows that the robust surrogate (5) is roughly equivalent to data-dependent regu-
(cid:80)m
j=1 pj(θ  x)θc j  our “average estimated linear
classiﬁer”  to θc y  the linear classiﬁer corresponding to the true label y. Concretely  for any ﬁxed
j=1 (cid:107)θc j(cid:107)2 to
θ ∈ Θ  we have the following result where we use L(θ) := 2 max1≤j(cid:48)≤m (cid:107)θc j(cid:48)(cid:107)2
ease notation. See Appendix A.3 for the proof.
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)θc y − m(cid:88)
Theorem 2. If Im(g(θf ;·)) = Rp and γ > L(θ)  the softmax loss (2) satisﬁes

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)θc y − m(cid:88)

≤ φγ(θ  (x  y))−(cid:96)(θ  (x  y)) ≤

γ − L(θ)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

pj(θ  x)θc j

pj(θ  x)θc j

γ + L(θ)

1

1

j=1

j=1

.

2

2

4 Experiments

We evaluate our method for both classiﬁcation and semantic segmentation settings  following the
evaluation scenarios of domain adaptation techniques [9  39  14]  though in our case the target domains
are unknown at training time. We summarize our experimental setup including implementation details 
evaluation metrics and datasets for each task.

Digit classiﬁcation We train on MNIST [19] dataset and test on MNIST-M [9]  SVHN [30]  SYN
[9] and USPS [6]. We use 10  000 digit samples for training and evaluate our models on the respective
test sets of the different target domains  using accuracy as a metric. In order to work with comparable
datasets  we resized all the images to 32×32  and treated images from MNIST and USPS as RGB. We
use a ConvNet [18] with architecture conv-pool-conv-pool-fc-fc-softmax and set the hyperparameters
α = 0.0001  η = 1.0  Tmin = 100 and Tmax = 15. In the minimization phase  we use Adam [17]
with batch size equal to 324. We compare our method against the Empirical Risk Minimization
(ERM) baseline and different regularization techniques (Dropout [35]  ridge).

Semantic scene segmentation We use the SYTHIA[31] dataset for semantic segmentation. The
dataset contains images from different locations (we use Highway  New York-like City and Old
European Town)  and different weather/time/date conditions (we use Dawn  Fog  Night  Spring and
Winter. We train models on a source domain and test on other domains  using the standard mean
Intersection Over Union (mIoU) metric to evaluate our performance [8]. We arbitrarily chose images
from the left front camera throughout our experiments. For each one  we sample 900 random images
(resized to 192 × 320 pixels) from the training set. We use a Fully Convolutional Network (FCN)
[23]  with a ResNet-50 [11] body and set the hyperparameters α = 0.0001  η = 2.0  Tmin = 500 and
Tmax = 50. For the minimization phase  we use Adam [17] with batch size equal to 8. We compare
our method against the ERM baseline.

4.1 Results on Digit Classiﬁcation

In this section  we present and discuss the results on the digit classiﬁcation experiment. Firstly  we
are interested in analyzing the role of the semantic constraint we impose. Figure 1a (top) shows
4Models were implemented using Tensorﬂow  and training procedures were performed on NVIDIA GPUs.

Code is available at https://github.com/ricvolpi/generalize-unseen-domains

6

Figure 1. Results associated with models trained with 10  000 MNIST samples and tested on SVHN 
MNIST-M  SYN and USPS (1st  2nd  3rd and 4th columns  respectively). Panel (a)  top: comparison
between distances in the pixel space (yellow) and in the semantic space (blue)  with γ = 104 and K = 1.
Panel (a)  bottom: comparison between our method with K = 2 and different γ values (blue bars) and
ERM (red line). Panel (b)  top: comparison between our method with γ = 1.0 and different number
of iterations K (blue)  ERM (red) and Dropout [35] (yellow). Panel (b)  middle: comparison between
models regularized with ridge (green) and with ridge + our method with γ = 1.0 and K = 1 (blue).
Panel (b)  bottom: results related to the ensemble method  using models trained with our methods with
different number of iterations K (blue) and using models trained via ERM (red). The reported results
are obtained by averaging over 10 different runs; black bars indicate the range of accuracy spanned.

performances associated with models trained with Algorithm 1 with K = 1 and γ = 104  with
the constraint in the semantic space (as discussed in Section 2) and in the pixel space [34] (blue
and yellow bars  respectively). Figure 1a (bottom) shows performances of models trained with our
method using different values of the hyperparameter γ (with K = 2) and with ERM (blue bars and
red lines  respectively). These plots show (i) that moving the constraint on the semantic space carries
beneﬁts when models are tested on unseen domains and (ii) that models trained with Algorithm 1
outperform models train with ERM for any value of γ on out-of-sample domains (SVHN  MNIST-M

7

Figure 2. Results obtained with semantic segmentation models trained with ERM (red) and our method
with K = 1 and γ = 1.0 (blue). Leftmost panels are associated with models trained on Highway 
rightmost panels are associated with models trained on New York-like City. Test datasets are Highway 
New York-like City and Old European Town.

and SYN). The latter result is a rather desired achievement  since this hyperparameter cannot be
properly cross-validated. On USPS  our method causes accuracy to drop since MNIST and USPS are
very similar datasets  thus the image domain that USPS belongs to is not explored by our algorithm
during the training procedure  which optimizes for worst case performance.
Figure 1b (top) reports results related to models trained with our method (blue bars)  varying the
number of iterations K and ﬁxing γ = 1.0  and results related to ERM (red bars) and Dropout [35]
(yellow bars). We observe that our method improves performances on SVHN  MNIST-M and SYN 
outperforming both ERM and Dropout [35] statistically signiﬁcantly. In Figure 1b (middle)  we
compare models trained with ridge regularization (green bars) with models trained with Algorithm 1
(with K = 1 and γ = 1.0) and ridge regularization (blue bars); these results show that our method
can potentially beneﬁt from other regularization approaches  as in this case we observed that the
two effects sum up. We further report in Appendix B a comparison between our method and an
unsupervised domain adaptation algorithm (ADDA [39])  and results associated with different values
of the hyperparameters γ and K.
Finally  we report the results obtained by learning an ensemble of models. Since the hyperparameter
γ is nontrivial to set a priori  we use the softmax conﬁdences (9) to choose which model to use
at test time. We learn ensemble of models  each of which is trained by running Algorithm 1 with

different values of the γ as γ = 10−i  with i =(cid:8)0  1  2  3  4  5  6(cid:9). Figure 1b (bottom) shows the

comparison between our method with different numbers of iterations K and ERM (blue and red
bars  respectively). In order to separate the role of ensemble learning  we learn an ensemble of
baseline models each corresponding to a different initialization. We ﬁx the number of models in the
ensemble to be the same for both the baseline (ERM) and our method. Comparing Figure 1b (bottom)
with Figure 1b (top) and Figure 1a (bottom)  our ensemble approach achieves higher accuracy in
different testing scenarios. We observe that our out-of-sample performance improves as the number
of iterations K gets large. Also in the ensemble setting  for the USPS dataset we do not see any
improvement  which we conjecture to be an artifact of the trade-off between good performance on
domains far away from training  and those closer.

4.2 Results on Semantic Scene Segmentation

We report a comparison between models trained with ERM and models trained with our method
(Algorithm 1 with K = 1). We set γ = 1.0 in every experiment  but stress that this is an arbitrary
value; we did not observe a strong correlation between the different values of γ and the general
behavior of the models in this case. Its role was more meaningful in the ensemble setting where each
model is associated with a different level of robustness  as discussed in Section 2. In this setting  we
do not apply the ensemble approach  but only evaluate the performances of the single models. The

8

main reason for this choice is the fact that the heuristics developed to choose the correct model at test
time in effect cannot be applied in a straightforward fashion to a semantic segmentation problem.
Figure 2 reports numerical results obtained. Speciﬁcally  leftmost plots report results associated
with models trained on sequences from the Highway split and tested on the New York-like City and
the Old European Town splits (top-left and bottom-left  respectively); rightmost plots report results
associated with models trained on sequences from the New York-like City split and tested on the
Highway and the Old European Town splits (top-right and bottom-right  respectively). The training
sequences (Dawn  Fog  Night  Spring and Winter) are indicated on the x-axis. Red and blue bars
indicate average mIoUs achieved by models trained with ERM and by models trained with our
method  respectively. These results were calculated by averaging over the mIoUs obtained with each
model on the different conditions of the test set. As can be observed  models trained with our method
mostly better generalize to unknown data distributions. In particular  our method always outperforms
the baseline by a statistically signiﬁcant margin when the training images are from Night scenarios.
This is since the baseline models trained on images from Night are strongly biased towards dark
scenery  while  as a consequence of training over worst-case distributions  our models can overcome
this strong bias and better generalize across different unseen domains.

5 Conclusions and Future Work

We study a new adversarial data augmentation procedure that learns to better generalize across
unseen data distributions  and deﬁne an ensemble method to exploit this technique in a classiﬁcation
framework. This is in contrast to domain adaptation algorithms  which require a sufﬁcient number
of samples from a known  a priori ﬁxed target distribution. Our experimental results show that our
iterative procedure provides broad generalization behavior on digit recognition and cross-season and
cross-weather semantic segmentation tasks.
For future work  we hope to extend the ensemble methods by deﬁning novel decision rules. The
proposed heuristics (9) only apply to classiﬁcation settings  and extending them to a broad realm of
tasks including semantic segmentation is an important direction. Many theoretical questions still
remain. For instance  quantifying the behavior of data-dependent regularization schemes presented in
Section 3 would help us better understand adversarial training methods in general.

References
[1] Shai Ben-David  John Blitzer  Koby Crammer  and Fernando Pereira. Analysis of representations
for domain adaptation. In B. Schölkopf  J. C. Platt  and T. Hoffman  editors  Advances in Neural
Information Processing Systems 19  pages 137–144. MIT Press  2007.

[2] Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport.

arXiv:1604.01446 [math.PR]  2016.

[3] John Blitzer  Ryan McDonald  and Fernando Pereira. Domain adaptation with structural
correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in
Natural Language Processing  EMNLP ’06  pages 120–128  Stroudsburg  PA  USA  2006.
Association for Computational Linguistics.

[4] J Frédéric Bonnans and Alexander Shapiro. Perturbation analysis of optimization problems.

Springer Science & Business Media  2013.

[5] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press 

2004.

[6] J. S. Denker  W. R. Gardner  H. P. Graf  D. Henderson  R. E. Howard  W. Hubbard  L. D. Jackel 
H. S. Baird  and I. Guyon. Advances in neural information processing systems 1. chapter
Neural Network Recognizer for Hand-written Zip Code Digits  pages 323–331. 1989.

[7] Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics
based on deep networks. In Advances in Neural Information Processing Systems  pages 658–666 
2016.

9

[8] M. Everingham  L. Van Gool  C. K. I. Williams  J. Winn  and A. Zisserman. The
PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results. http://www.pascal-
network.org/challenges/VOC/voc2008/workshop/index.html.

[9] Yaroslav Ganin and Victor S. Lempitsky. Unsupervised domain adaptation by backpropagation.
In Proceedings of the 32nd International Conference on Machine Learning  ICML 2015  Lille 
France  6-11 July 2015  pages 1180–1189  2015.

[10] Ian Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversarial

examples. In International Conference on Learning Representations  2015.

[11] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image

recognition. CoRR  abs/1512.03385  2015.

[12] Christina Heinze-Deml and Nicolai Meinshausen. Conditional variance penalties and domain

shift robustness. arXiv preprint arXiv:1710.11469  2017.

[13] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution

examples in neural networks. CoRR  abs/1610.02136  2016.

[14] Judy Hoffman  Dequan Wang  Fisher Yu  and Trevor Darrell. Fcns in the wild: Pixel-level

adversarial and constraint-based adaptation. CoRR  abs/1612.02649  2016.

[15] Hal Daumé III and Daniel Marcu. Domain adaptation for statistical classiﬁers. CoRR 

abs/1109.6341  2011.

[16] Justin Johnson  Alexandre Alahi  and Li Fei-Fei. Perceptual losses for real-time style transfer
and super-resolution. In European Conference on Computer Vision  pages 694–711. Springer 
2016.

[17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR 

abs/1412.6980  2014.

[18] Yann LeCun  Bernhard E. Boser  John S. Denker  Donnie Henderson  Richard E. Howard 
Wayne E. Hubbard  and Lawrence D. Jackel. Backpropagation applied to handwritten zip code
recognition. Neural Computation  1(4):541–551  1989.

[19] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. In Proceedings of the IEEE  pages 2278–2324  1998.

[20] Jaeho Lee and Maxim Raginsky. Minimax statistical learning and domain adaptation with

wasserstein distances. arXiv preprint arXiv:1705.07815  2017.

[21] K Levenberg. A method for the solution of certain problems in least squares. quart. appl. math.

2. 1944.

[22] Da Li  Yongxin Yang  Yi-Zhe Song  and Timothy M. Hospedales. Deeper  broader and artier

domain generalization. CoRR  abs/1710.03077  2017.

[23] Jonathan Long  Evan Shelhamer  and Trevor Darrell. Fully convolutional networks for semantic

segmentation. CoRR  abs/1411.4038  2014.

[24] M. Mancini  S. R. Bulò  B. Caputo  and E. Ricci. Robust place categorization with deep domain

generalization. IEEE Robotics and Automation Letters  3(3):2093–2100  July 2018.

[25] Donald W Marquardt. An algorithm for least-squares estimation of nonlinear parameters.

Journal of the society for Industrial and Applied Mathematics  11(2):431–441  1963.

[26] Pietro Morerio  Jacopo Cavazza  and Vittorio Murino. Minimal-entropy correlation alignment
for unsupervised deep domain adaptation. International Conference on Learning Representa-
tions  2018.

[27] Saeid Motiian  Marco Piccirilli  Donald A. Adjeroh  and Gianfranco Doretto. Uniﬁed deep
supervised domain adaptation and generalization. In The IEEE International Conference on
Computer Vision (ICCV)  Oct 2017.

10

[28] Krikamol Muandet  David Balduzzi  and Bernhard Schölkopf. Domain generalization via
invariant feature representation. In Sanjoy Dasgupta and David McAllester  editors  Proceedings
of the 30th International Conference on Machine Learning  volume 28 of Proceedings of
Machine Learning Research  pages 10–18  Atlanta  Georgia  USA  17–19 Jun 2013. PMLR.

[29] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global

performance. Mathematical Programming  108(1):177–205  2006.

[30] Yuval Netzer  Tao Wang  Adam Coates  Alessandro Bissacco  Bo Wu  and Andrew Y. Ng.
Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on
Deep Learning and Unsupervised Feature Learning 2011  2011.

[31] German Ros  Laura Sellart  Joanna Materzynska  David Vazquez  and Antonio M. Lopez. The
synthia dataset: A large collection of synthetic images for semantic segmentation of urban
scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  June
2016.

[32] Kate Saenko  Brian Kulis  Mario Fritz  and Trevor Darrell. Adapting visual category models to
new domains. In Proceedings of the 11th European Conference on Computer Vision: Part IV 
ECCV’10  pages 213–226  Berlin  Heidelberg  2010. Springer-Verlag.

[33] Shiv Shankar  Vihari Piratla  Soumen Chakrabarti  Siddhartha Chaudhuri  Preethi Jyothi  and
Sunita Sarawagi. Generalizing across domains via cross-gradient training. In International
Conference on Learning Representations  2018.

[34] Aman Sinha  Hongseok Namkoong  and John Duchi. Certiﬁable distributional robustness with
principled adversarial training. In International Conference on Learning Representations  2018.

[35] Nitish Srivastava  Geoffrey Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. J. Mach. Learn. Res.  15(1) 
January 2014.

[36] Baochen Sun and Kate Saenko. Deep CORAL: correlation alignment for deep domain adaptation.

In ECCV Workshops  2016.

[37] Joshua Tobin  Rachel Fong  Alex Ray  Jonas Schneider  Wojciech Zaremba  and Pieter Abbeel.
Domain randomization for transferring deep neural networks from simulation to the real world.
CoRR  abs/1703.06907  2017.

[38] A. Torralba and A. A. Efros. Unbiased look at dataset bias.

In Proceedings of the 2011
IEEE Conference on Computer Vision and Pattern Recognition  CVPR ’11  pages 1521–1528 
Washington  DC  USA  2011. IEEE Computer Society.

[39] Eric Tzeng  Judy Hoffman  Kate Saenko  and Trevor Darrell. Adversarial discriminative domain
adaptation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  July
2017.

[40] Riccardo Volpi  Pietro Morerio  Silvio Savarese  and Vittorio Murino. Adversarial feature
augmentation for unsupervised domain adaptation. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  June 2018.

11

,Danilo Jimenez Rezende
S. M. Ali Eslami
Shakir Mohamed
Peter Battaglia
Max Jaderberg
Nicolas Heess
Riccardo Volpi
Hongseok Namkoong
Ozan Sener
John Duchi
Vittorio Murino
Silvio Savarese