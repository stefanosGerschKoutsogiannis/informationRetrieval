2009,Local Rules for Global MAP: When Do They Work ?,We consider the question of computing Maximum A Posteriori (MAP) assignment in an arbitrary pair-wise Markov Random Field (MRF). We present a randomized iterative algorithm based on simple local updates. The algorithm  starting with an arbitrary initial assignment  updates it in each iteration by first  picking a random node  then selecting an (appropriately chosen) random local neighborhood and optimizing over this local neighborhood. Somewhat surprisingly  we show that this algorithm finds a near optimal assignment within $2n\ln n$ iterations on average and with high probability for {\em any} $n$ node pair-wise MRF with {\em geometry} (i.e. MRF graph with polynomial growth) with the approximation error depending on (in a reasonable manner) the geometric growth rate of the graph and the average radius of the local neighborhood -- this allows for a graceful tradeoff between the complexity of the algorithm and the approximation error. Through extensive simulations  we show that our algorithm finds extremely good approximate solutions for various kinds of MRFs with geometry.,Local Rules for Global MAP: When Do They Work ?

∗

Kyomin Jung

KAIST

Daejeon  Korea

kyomin@kaist.edu

Pushmeet Kohli
Microsoft Research

Cambridge  UK

pkohli@microsoft.com

Devavrat Shah

MIT

Cambridge  MA  USA
devavrat@mit.edu

Abstract

We consider the question of computing Maximum A Posteriori (MAP) assignment
in an arbitrary pair-wise Markov Random Field (MRF). We present a randomized
iterative algorithm based on simple local updates. The algorithm  starting with an
arbitrary initial assignment  updates it in each iteration by ﬁrst  picking a random
node  then selecting an (appropriately chosen) random local neighborhood and
optimizing over this local neighborhood. Somewhat surprisingly  we show that
this algorithm ﬁnds a near optimal assignment within n log 2 n iterations with high
probability for any n node pair-wise MRF with geometry (i.e. MRF graph with
polynomial growth) with the approximation error depending on (in a reasonable
manner) the geometric growth rate of the graph and the average radius of the local
neighborhood – this allows for a graceful tradeoff between the complexity of the
algorithm and the approximation error. Through extensive simulations  we show
that our algorithm ﬁnds extremely good approximate solutions for various kinds
of MRFs with geometry.

1 Introduction

The abstraction of Markov random ﬁeld (MRF) allows one to utilize graphical representation to
capture inter-dependency between large number of random variables in a succinct manner. The MRF
based models have been utilized successfully in the context of coding (e.g. the low density parity
check code [15])  statistical physics (e.g.
the Ising model [5])  natural language processing [13]
and image processing in computer vision [11  12  19]. In most applications  the primary inference
question of interest is that of ﬁnding maximum a posteriori (MAP) solution – e.g. ﬁnding a most
likely transmitted message based on the received signal.

Related Work. Computing the exact MAP solution in general probabilistic models is an NP-hard
problem. This had led researchers to resort of fast approximate algorithms. Various such algorith-
mic approaches have been developed over more than the past three decades. In essence  all such
approaches try to ﬁnd a locally optimal solution of the problem through iterative procedure. These
”local update” algorithms start from an initial solution and proceed by making a series of changes
which lead to solutions having lower energy (or better likelihood)  and hence are also called ”move
making algorithms”. At each step  the algorithms search the space of all possible local changes that
can be made to the current solution (also called move space)  and choose the one which leads to the
solution having the highest probability or lowest energy.

One such algorithm (which has been rediscovered multiple times) is called Iterated Conditional
Modes or ICM for short. Its local update involves selecting (randomly or deterministically) a vari-
able of the problem. Keeping the values of all other variables ﬁxed  the value of the selected variable

∗

This work was partially carried out while the author was visiting Microsoft Research Cambridge  and was

partially supported by NSF CAREER project CNS-0546590.

1

is chosen which results in a solution with the maximum probability. This process is repeated by se-
lecting other variables until the probability cannot be increased further.

The size of the move space is the deﬁning characteristic of any such move making algorithm. A large
move space means that more extensive changes to the current solution can be made. This makes the
algorithm less prone to getting stuck in local minima and also results in a faster rate of convergence.
Expansion and Swap are move making algorithms which search for the optimal move in a move
space of size 2n where n is the number of random variables. For energy functions composed of
metric pairwise potentials  the optimal move can be found in polynomial time by minimizing a
submodular quadratic pseudo-boolean function [3] (or solving an equivalent minimum cost st-cut
problem).

The last few years have seen a lot of interest in st-mincut based move algorithms for energy mini-
mization. Komodakis et al. [9] recently gave an alternative interpretation of the expansion algorithm.
They showed that expansion can be seen as solving the dual of a linear programming relaxation of
the energy minimization problem. Researchers have also proposed a number of novel move en-
coding strategies for solving particular forms of energy functions. Veksler [18] proposed a move
algorithm in which variables can choose any label from a range of labels. They showed that this
move space allowed them to obtain better minima of energy functions with truncated convex pair-
wise terms. Kumar and Torr [10] have since shown that the range move algorithm achieves the same
guarantees as the ones obtained by methods based on the standard linear programming relaxation.

A related popular algorithmic approach is based on max-product belief propagation (cf. [14] and
[22]). In a sense  it can be viewed as an iterative algorithm that makes local updates based optimizing
based on the immediate graphical structure. There is a long list of literature on understanding the
conditions under which max-product belief propagation algorithm ﬁnd correct solution. Speciﬁcally 
in recent years a sequence of results suggest that there is an intimate relation between the max-
product algorithm and a natural linear programming relaxation – for example  see [1  2  8  16  21].

We also note that Swendsen-Wang algorithm (SW) [17]  a local ﬂipping algorithm  has a philosophy
similar to ours in that it repeats a process of randomly partitioning the graph  and computing an
assignment. However  the graph partitioning of SW is fundamentally different from ours and there
is no known guarantee for the error bound of SW.

In summary  all the approaches thus far with provable guarantees for local update based algorithm
are primarily for linear or more generally convex optimization setup.

Our Contribution. As the main result of this paper  we propose a randomized iterative local al-
gorithm that is based on simple local updates. The algorithm  starting with an arbitrary initial as-
signment  updates it in each iteration by ﬁrst picking a random node  then its (appropriate) random
local neighborhood and optimizing over this local neighborhood. Somewhat surprisingly  we show
that this algorithm ﬁnds near optimal assignment within n log 2 n iterations with high probability for
graphs with geometry – i.e. graphs in which the neighborhood of each node within distance r grows
no faster than a polynomial in r. Such graphs can have arbitrarily structure subject to this poly-
nomial growth structure. We show that the approximation error depends gracefully on the average
random radius of the local neighborhood and degree of polynomial growth of the graph. Overall  our
algorithm can provide an ε−approximation MAP with C(ε)n log 2 n total computation with C(ε)
depending only on ε and the degree of polynomial growth. The crucial novel feature of our algo-
rithm is the appropriate selection of random local neighborhood rather than deterministic in order to
achieve provable performance guarantee.

We note that near optimality of our algorithm does not depend on convexity property  or tree-like
structure as many of the previous works; but only relies on geometry of the graphical structure which
is present in many graphical models of interest such as those arising in image processing  wireless
networks  etc.

We use our algorithm to verify its performance in simulation scenario. Speciﬁcally  we apply our
algorithm to two popular setting: (a) a grid graph based pairwise MRF with varying node and edge
interaction strengths  and (b) a grid graph based MRF on the weighted independent set (or hardcore)
model. We ﬁnd that with very small radius (within 3)  we ﬁnd assignment which within 1% (0.99
factor) of the MAP for a large range of parameters and upto graph of 1000 nodes.

2

Organization. We start by formally stating our problem statement and main theorem (Theorem 1)
in Section 2. This is followed by a detailed description of the algorithm in Section 3. We present
the sketch proof of the main result in Section 4. Finally  we provide a detailed simulation results in
Section 5.

2 Main Results

We start with the formal problem description and useful deﬁnitions/notations followed by the state-
ment of the main result about performance of the algorithm. The algorithm will be stated in the next
section.

Deﬁnitions & Problem Statement. Our interest is in a pair-wise MRF deﬁned next. We note that 
formally all (non pair-wise) MRFs are equivalent to pair-wise MRFs – e.g. see [20].
Deﬁnition 1 (Pair-wise MRF). A pair-wise MRF based on graph G = (V  E) with n = |V | vertices
and edge set E is deﬁned by associated a random variable X v with each vertex v ∈ V taking value
in ﬁnite alphabet set Σ; the joint distribution of X = (X v)v∈V deﬁned as

Pr[X = x] ∝

Ψv(xv) ·

Ψuv(xu  xv)

(1)

(cid:2)

v∈V

(cid:2)

(u v)∈E

where Ψv : Σ → R+ and Ψuv : Σ2 → R+ are called node and edge potential functions. 1
In this paper  the question of interest is to ﬁnd the maximum a posteriori (MAP) assignment x ∗ ∈
Σn  i.e.

x∗ ∈ arg max

x∈Σn Pr[X = x].

Equivalently  from the optimization point of view  we wish to ﬁnd an optimal assignment of the
problem

maximize H(x)

x ∈ Σn 

where

H(x) =

ln Ψv(xv) +

ln Ψuv(xu  xv).

(cid:3)

v∈V

over

(cid:3)

(u v)∈E

For completeness and simplicity of exposition  we assume that the function H is ﬁnite valued over
Σn. However  results of this paper extend for hard constrained problems such as the hardcore or
independent set model.

In this paper  we will design algorithms for ﬁnding approximate MAP problem. Speciﬁcally  we call

an assignment(cid:4)x as an ε-approximate MAP if
(1 − ε)H(x∗

) ≤ H((cid:4)x) ≤ H(x∗

).

Graphs with Geometry. We deﬁne notion of graphs with geometry here. To this end  a graph
G = (V  E) induces a natural ‘graph metric’ on vertices V   denoted by d G : V × V → R+ with
dG(v  u) as the length of the shortest path between u and v; with it deﬁned as ∞ if there is no path
between them.

Deﬁnition 2 (Graph with Polynomial Growth). We call a graph G with polynomial growth of degree
(or growth rate) ρ  if for any v ∈ V and r ∈ N 

where C > 0 is a universal constant and BG(v  r) = {w ∈ V |dG(w  v) < r}.

|BG(v  r)| ≤ C · rρ 

A large class of graph model naturally fall into the graphs with polynomial growth. To begin with 
the standard d-dimensional regular grid graphs have polynomial growth rate d – e.g. d = 1 is the
line graph. More generally  in recent years in the context of computational geometry and metric
embedding  the graphs with ﬁnite doubling dimensions have become popular object of study [6  7].

1We assume the positivity of Ψv’s and Ψuv’s for simplicity of analysis.

3

It can be checked that a graph with doubling dimension ρ is also a graph with polynomial growth
rate ρ. Finally  the popular geometric graph model where nodes are placed arbitrarily on a two
dimensional surface with minimum distance separation and two nodes have an edge between them
if they are within certain ﬁnite distance  then it is indeed a graph with ﬁnite polynomial growth rate.

Statement of Main Result. The main result of this paper is a randomized iterative algorithm based
on simple local updates. In essence the algorithm works as follows. It starts with an arbitrary initial
assignment. In each iteration  it picks a node  say v from all n nodes of V   uniformly at random and
picks a random radius Q (as per speciﬁc distribution). The algorithm re-assigns values to all nodes
within distance Q of node v with respect to graph distance d G by ﬁnding the optimal assignment
for this local neighborhood subject to keeping the assignment to all other nodes the same. The
algorithm LOC-ALGO described in Section 3 repeats this process for n log 2 n many times. We show
that LOC-ALGO ﬁnds near optimal solution with high probability as long as the graph has ﬁnite
polynomial growth rate.
Theorem 1. Given MRF based on graph G = (V  E) of n = |V | nodes with polynomial growth rate
iterations produces a solution (cid:4)x such that
ρ and approximation parameter ε ∈ (0  1)  our algorithm LOC-ALGO with O (log(1/δ)n log n)

) − H((cid:4)x) ≤ 2εH(x∗

Pr[H(x∗

)] ≥ 1 − δ −

1

poly(n)

.

And each iteration takes at most ζ(ε  ρ) computation  with

where K(ε  ρ) is deﬁned as

K = K(ε  ρ) =

8ρ
ϕ log

(cid:5)

8ρ
ϕ

ζ(ε  ρ) ≤ |Σ|CK(ε ρ)ρ 
(cid:6)

+

4
ϕ log C +

4
ϕ log

1
ϕ + 2 with ϕ =

ε

5C2ρ

.

In a nutshell  Theorem 1 say that the complexity of the algorithm for obtaining an ε-approximation
scales almost linearly in n  double exponentially in 1/ε and ρ. On one hand  this result establishes
that it is indeed possible to have polynomial (or almost linear) time approximation algorithm for
arbitrary pair-wise MRF with polynomial growth. On the other hand  though theoretical bound on
the pre-constant ζ(ε  ρ) as function of 1/ε and ρ is not very exciting  our simulations suggest (see
Section 5) that even for hard problem setup  the performance is much more optimistic than predicted
by these theoretical bounds. Therefore  as a recommendation for a system designer  we suggest use
of smaller ‘radius’ distribution in algorithm described in Section 3 for obtaining good algorithm.

3 Algorithm Description

noted earlier  the algorithm iteratively updates its estimation of MAP  denoted by (cid:4)x. Initially  the(cid:4)x is
In this section  we provide details of the algorithm intuitively described in the previous section. As
chosen arbitrarily. Iteratively  at each step a vertex v ∈ V is chosen uniformly at random along with
a random radius Q that is chosen independently as per distribution Q. Then  select R ⊂ V   the local
Then while keeping the assignment of all nodes in V \R ﬁxed as per (cid:4)x = (ˆxv)v∈V   ﬁnd MAP
neighborhood (or ball) of radius Q around v as per graph distance d G  i.e. {w ∈ V |dG(u  w) < Q}.
restricted to nodes of R. And  update the assignment of nodes in v ∈ R as per
assignment x∗ R
x∗ R
. A caricature of an iteration is described in Figure 1. The precise description of the algorithm
is given in Figure 2.

In order to have good performance  it is essential to choose appropriate distribution Q for selection
of random radius Q each time. Next  we deﬁne this distribution which is essentially a truncated
Geometric distribution. Speciﬁcally  given parameters ε ∈ (0  1) and the polynomial growth rate ρ
(with constant C) of the graph  deﬁne ϕ = ε

5C2ρ   and

(cid:5)

(cid:6)

Then  the distribution (or random variable) Q is deﬁned over integers from 1 to K(ε  ρ) as

K = K(ε  ρ) =

Pr[Q = i] =

8ρ
ϕ

8ρ
ϕ log
(cid:7)
ϕ(1 − ϕ)i−1
(1 − ϕ)K−1

4

+

4
ϕ log C +

4
ϕ log

1
ϕ + 2.

if 1 ≤ i < K(ε  ρ)
if i = K(ε  ρ)

.

Graph G
Graph G

Q

u

Pr[

iQ

(cid:32)(cid:32)

]

(cid:72)(cid:72)

1(

(cid:16)

i
1)
(cid:16)

for 

i

(cid:32)

3 2 1

(cid:21)

( 

K

(cid:16)

)1

Figure 1: Pictorial description of an iteration of LOC-ALGO.

LOC-ALGO(ε  K)

(0) Input: MRF G = (V  E) with φi(·)  i ∈ V   ψij(· ·)  (i  j) ∈ E.
(1) Initially  select(cid:4)x ∈ Σn arbitrarily.

(2) Do the following for n log 2 n many times :

(a) Choose an element u ∈ V uniformly at random.
(b) Draw a random number Q according to the distribution Q.
(c) Let R ← {w ∈ V |dG(u  w) < Q}.
(d) Through dynamic programming (or exhaustive computation) ﬁnd
of(cid:4)x value outside R.
for R while ﬁxing all the other assignment
an exact MAP x∗ R
(e) Change values of(cid:4)x for R by x∗ R

.

(3) Output(cid:4)x.

Figure 2: Algorithm for approximate MAP computation.

4 Proof of Theorem 1

In this section  we present proof of Theorem 1. To that end  we will prove the following Lemma.
Lemma 1. If we run the LOC-ALGO with (2n ln n) iterations  with probability at least 1− 1/n  we
have

(1 − ε)H(x∗

) ≤ E[H((cid:4)x)] ≤ H(x∗

).

From Lemma 1  we obtain Theorem 1 as follows. Deﬁne T = 2 log(1/δ)  and consider LOC-

ALGO with (2T n ln n) iterations. From the fact that H(x∗) − H((cid:4)x) ≥ 0  and by the Markov
inequality applied to H(x∗) − H((cid:4)x) with Lemma 1  we have that after (2n ln n) iterations 

) − H((cid:4)x) ≤ 2εH(x∗

Pr[H(x∗

)] ≥ 1
2

.

(2)
(2tn ln n) iterations  (2) holds independently with probability 1 − 1/n. Also  note that H((cid:4)x) is
Note that (2) is true for any initial assignment of LOC-ALGO. Hence for each 1 ≤ t ≤ T   after
increasing monotonically. Hence  H(x∗)− H((cid:4)x) > 2εH(x∗) holds after (2T n ln n) iterations only
have Pr[H(x∗) − H((cid:4)x) ≤ 2εH(x∗)] ≥ 1 − δ − 1/poly(n)  which proves the ﬁrst part of Theorem
if the same holds after (2tn ln n) iterations for all 1 ≤ t ≤ T . Hence  after (2T n ln n) iterations  we

1.

For the total computation bound in Theorem 1  note that each iteration of LOC-ALGO involves
dynamic programming over a local neighborhood of radius at most K = K(ε  ρ) around a node.

5

operations as claimed.

This involves  due to the polynomial growth condition  at most CK ρ nodes. Each variable can takes
at most |Σ| different values. Therefore  dynamic programming (or exhaustive search) can take at
most |Σ|CK ρ
Proof of Lemma 1. First observe that by the standard argument in the classical coupon collector
problem with n coupons (e.g. see [4])  it follows that after 2n ln n iterations  with probability at
least 1 − 1/n  all the vertices of V will be chosen as ‘ball centers’ at least once.
answer(cid:4)x generated by LOC-ALGO after 2n ln n iterations  is indeed an ε-approximation on average.
Error bound. Now we prove that if all the vertices of V are chosen as ‘ball centers’ at least once  the
To this end  we construct an imaginarily set of edges as follows. Imagine that the procedure (2) of
LOC-ALGO is done with an iteration parameter t ∈ Z +. Then for each vertex v ∈ V  we assign the
largest iteration number t such that the chosen ball R at the iteration t contains w. That is 
T (v) = max{t ∈ Z+| LOC-ALGO chooses v as a member of R at iteration t}.

Clearly  this is well deﬁned algorithm is run till each node is chosen as the ‘ball center’ at least once.
Now deﬁne an imaginary boundary set of LOC-ALGO as

B = {(u  w) ∈ E|T (u) (cid:10)= T (w)}.

such that

Now consider graph G(cid:4) = (V  E\B) obtained by removing edges B from G. In this graph  nodes
of the same connected component have same T (·) value. Next  we state two Lemmas that will be
crucial to the proof of the Theorem. Proof of Lemmas 2 and 3 are omitted.
Lemma 2. Given two MRFs X1 and X2 on the same graph G = (V E) with identical edge po-
(cid:8)(cid:8) . Finally  for (cid:10) ∈ {1  2} and any x ∈ Σn 
tentials {ψij(· ·)}  (i  j) ∈ E but distinct node potentials {φ1
i (·)}  i ∈ V respectively. For
each i ∈ V  deﬁne φD
(cid:11)
(i j)∈E ψij(xi  xj)  with x∗ (cid:5) being a MAP assignment of MRF
deﬁne H(cid:5)(x) =
i∈V φ(cid:5)
x(cid:5). Then  we have |H1(x∗ 1) − H1(x2 ∗)| ≤ 2
Lemma 3. Given MRF X deﬁned on G (as in (1))  the algorithm LOC-ALGO produces output (cid:4)x

i (·)} {φ2

i = maxσ∈Σ

i(xi) +

i∈V φD
i

(cid:9)

(cid:9)

.

i (σ)

(cid:8)(cid:8)φ1
i (σ) − φ2
(cid:10)(cid:9)
⎛
⎝ (cid:3)

) − H((cid:4)x)| ≤ 5

(cid:10)

ij

(i j)∈B

|H(x∗

ij − ψL
ψU
where B is the (random) imaginary boundary set of LOC-ALGO  ψ U
ij (cid:2) minσ σ(cid:2)∈Σ ψij(σ  σ(cid:4)).
ψL
Now we obtain the following lemma that utilizes the fact that the distribution Q follows a geometric
distribution with rate (1 − ϕ) – its proof is omitted.
Lemma 4. For any edge e ∈ E of G 
From Lemma 4  we obtain that(cid:3)
(cid:10)

(cid:10)

(cid:11)

(cid:11)⎞
⎠  
ij (cid:2) maxσ σ(cid:2)∈Σ ψij(σ  σ(cid:4))  and

.

ij

ij − ψL
ψU
(cid:10)

(i j)∈E

ij − ψL
ψU

ij

(cid:11)

(3)

– its proof is omitted.

+ 1)H(x∗

).

(4)

(cid:11) ≤ ϕ

Pr[e ∈ B] ≤ ϕ.
(cid:3)
(cid:9)

(i j)∈E

ij

ij − ψL
ψU

(i j)∈B

Finally  we establish the following lemma that bounds
Lemma 5. If G has maximum vertex degree d∗
  then
ij − ψL
ψU

(cid:11) ≤ (d∗

(cid:3)

(cid:10)

ij

(i j)∈E

Now recall that the maximum vertex degree d∗
of G is less than 2ρC by the deﬁnition of poly-
nomially growing graph. Therefore  by Lemma 3  (3)  and Lemma 5  the output produced by the
) − H((cid:4)x)| ≤ 5(d∗
LOC-ALGO algorithm is such that

+ 1)ϕH(x∗

) ≤ εH(x∗

) 

|H(x∗
5C2ρ . This completes the proof of Lemma 1.

where recall that ϕ = ε

6

5 Experiments

Our algorithm provides a provable approximation for any MRF on a polynomially growing graph.
In this section  we present experimental evaluations of our algorithm for two popular models: (a)
synthetic Ising model  and (b) hardcore (independent set) model. As a reader will notice  the ex-
perimental results not only conform the qualitatively behavior proved by our theoretical result  but
it also suggest that much tighter approximation guarantees should be expected in practice compared
to what is guaranteed by theoretical results.
⎞
Setup 12 Consider a binary (i.e. Σ = {0  1}) MRF on an n1 × n2 grid G = (V  E):
⎠   for x ∈ {0  1}n1n2 .

⎛
⎝(cid:3)

Pr(x) ∝ exp

θixi +

θij xixj

(cid:3)

i∈V

(i j)∈E

We consider the following scenario for choosing parameters (with the notation U[a  b] for the uni-
form distribution over the interval [a  b]):

1. For each i ∈ V   choose θi independently as per the distribution U[−1  1].
2. For each (i  j) ∈ E  choose θij independently from U[−α  α]. Here the interaction param-

eter α is chose from {0.125  0.25  0.5  1  2  4  8  16  32  64}.

(A)

0.25

0.2

0.15

0 1
0.1

0.05

E
Error

0

0.125 0.25

0.5

1

2

4

8

16

32

64

(cid:68)

(B)

0.3
0.25
0.2
0.15
0.1
0.05
0

Error

0.125 0.25

0.5

1

2

4

8

16

32

64

(cid:68)

r=1
r=2
r 2
r=3

r=1
r=2
r=2
r=3

Figure 3: (A) plots the error of local update algorithm for a random Ising model in the grid graph of
size 10 × 10  and (B) plots the error in the grid of size 100 × 10.

To compare the effectiveness of our algorithm for each size of the local updates  in our simulation 
we ﬁx the square size as a constant instead of choosing it from a distribution. We run the simulation
for the local square size r×r with r = 1  2  3  where r = 1 is the case when each square consists of a
the output(cid:4)x of our local update algorithm for each r  by doing 4n 1n2 log(n1n2) many local updates
single vertex. We computed an exact MAP assignment x∗
by dynamic programming  and computed
for n1 × n2 grid graph. Then compare the error as follows:
H(x∗) − H((cid:16)x∗)

Error =

H(x∗)

.

We run the simulation for 100 trials and compute the average error for each case. The Figure 3(A)
plots the error for the grid of size 10 × 10  while Figure 3(B) plots the error for the grid of size
100 × 10.

2Though this setup has φi  ψij taking negative values  they are equivalent to the setup considered in the

paper  since afﬁne shift will make them non-negative without changing the distribution.

7

Remind that the approximation guarantee of Theorem 1 is an error bound for the worst case. As the
simulation result suggests  for any graph and any range of α  the error of the local update algorithm
decreases dramatically as r increases. Moreover  when r is comparably small as r = 3  the output
of the local update algorithm achieves remarkably good approximation. Hence we observe that our
algorithm performs well not only theoretically  but also practically.

Setup 2. We consider the vertex weighted independent set model deﬁned on a grid graph. To this
end  we start by description of a weighted independent set problem as the MRF model. Speciﬁcally 
consider a binary MRF on an n1 × n2 grid G = (V  E):

⎞
⎠   for x ∈ {0  1}n1n2 .

⎛
⎝(cid:3)

i∈V

(cid:3)

(i j)∈E

Pr(x) ∝ exp

θixi +

Ψ(xixj)

Here  the parameters are chosen as follows.

1. For each i ∈ V   θi is chosen independently as per the distribution U[0  1].
2. The function Ψ(· ·) is deﬁned as
Ψ(σ  σ(cid:4)

(cid:7)−M if (σ  σ(cid:4)) = (1  1)

) =

 

0

otherwise

where M is a large number.

For this model  we did simulations for grid graphs of size 10×10  30×10  and 100×10 respectively.
For each graph  we computed the average error as in the Setup 1  over 100 trials. The result is shown
in the following table. As the result shows  our local update algorithm achieves remarkably good
approximation of the MAP or equivalently in this setup the maximum weight independent set  even
with very small r values !

10 × 10
0.219734
0.016032
0.001539

30 × 10
0.205429
0.019145
0.002616

100 × 10
0.208446
0.019305
0.002445

r=1
r=2
r=3

It is worth nothing that choosing θ i from U[0  α] for any α > 0 will give the same approximation
result  since x∗

and(cid:4)x are both linear on α.

6 Conclusion

We considered the question of designing simple  iterative algorithm with local updates for ﬁnding
MAP in any pair-wise MRF. As the main result of this paper  we presented such a randomized  local
iterative algorithm that can ﬁnd ε-approximate solution of MAP in any pair-wise MRF based on
G within 2n ln n iterations and the computation per iteration is constant C(ε  ρ) dependent on the
accuracy parameter ε as well as the growth rate ρ of the polynomially growing graph G. That is 
ours is a local  iterative randomized PTAS for MAP problem in MRF with geometry. Our results are
somewhat surprising given that thus far the known theoretical justiﬁcation for such local algorithms
strongly dependended on some form of convexity of the ‘energy’ function. In contrast  our results
do not require any such condition  but only the geometry of the underlying MRF. We believe that
our algorithm will be of great practical interest in near future as a large class of problems that utilize
MRF based modeling and inference in practice have the underlying graphical structure possessing
some form of geometry naturally.

8

References

[1] M. Bayati  D. Shah  and M. Sharma. Maximum weight matching via max-product belief

propagation. In IEEE ISIT  2005.

[2] M. Bayati  D. Shah  and M. Sharma. Max-Product for Maximum Weight Matching: Conver-
gence  Correctness  and LP Duality. IEEE Transactions on Information Theory  54(3):1241–
1251  2008.

[3] Y. Boykov  O. Veksler  and R. Zabih. Fast approximate energy minimization via graph cuts.

IEEE Trans. Pattern Anal. Mach. Intell.  23(11):1222–1239  2001.

[4] William Feller. An Introduction to Probability Theory and Its Applications. Wiley  1957.
[5] Hans-Otto Georgii. Gibbs measures and phase transitions. Walter de Gruyter  1988.
[6] A. Gupta  R. Krauthgamer  and J.R. Lee. Bounded geometries  fractals  and low-distortion
embeddings. In In Proceedings of the 44th annual Symposium on the Foundations of Computer
Science  2003.

[7] S. Har-Peled and M. Mendel. Fast construction of nets in low dimensional metrics  and their
applications. In Proceedings of the twenty-ﬁrst annual symposium on Computational geometry 
pages 150–158. ACM New York  NY  USA  2005.

[8] B. Huang and T. Jebara. Loopy belief propagation for bipartite maximum weight b-matching.

Artiﬁcial Intelligence and Statistics (AISTATS)  2007.

[9] N. Komodakis and G. Tziritas. A new framework for approximate labeling via graph cuts. In

International Conference on Computer Vision  pages 1018–1025  2005.

[10] M. Pawan Kumar and Philip H. S. Torr. Improved moves for truncated convex models. In

NIPS  pages 889–896  2008.

[11] Stan Z. Li. Markov Random Field Modeling in Image Analysis. Springer  2001.
[12] M. Malfait and D. Roose. Wavelet-based image denoising using a markov random ﬁeld a priori

model. IEEE Transactions on : Image Processing  6(4):549–565  1997.

[13] Christopher D. Manning and Hinrich Schutze. Foundations of Statistical Natural Language

Processing. The MIT Press  1999.

[14] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. San

Francisco  CA: Morgan Kaufmann  1988.

[15] Thomas Richardson and Ruediger Ubanke. Modern Coding Theory. Cambridge University

Press  2008.

[16] S. Sanghavi  D. Shah  and A. Willsky. Message-passing for Maximum Weight Independent

Set. In Proceedings of NIPS  2007.

[17] R. Swendsen and J. Wang. Nonuniversal critical dynamics in monte carlo simulations. Phys.

Rev. Letter.  58:86–88  1987.

[18] O. Veksler. Graph cut based optimization for mrfs with truncated convex priors. In CVPR 

2007.

[19] Paul Viola and Michael J. Jones. Robust real-time face detection. International Journal of

Computer Vision  57(2):137–154  2004.

[20] M. Wainwright and M. Jordan. Graphical models  exponential families  and variational infer-

ence. UC Berkeley  Dept. of Statistics  Technical Report 649  2003.

[21] M. J. Wainwright  T. Jaakkola  and A. S. Willsky. Map estimation via agreement on (hy-
per)trees: Message-passing and linear-programming approaches. IEEE Transactions on Infor-
mation Theory  2005.

[22] J. Yedidia  W. Freeman  and Y. Weiss. Generalized belief propagation. Mitsubishi Elect. Res.

Lab.  TR-2000-26  2000.

9

,Celestine Dünner
Thomas Parnell
Dimitrios Sarigiannis
Nikolas Ioannou
Andreea Anghel
Gummadi Ravi
Madhusudanan Kandasamy
Haralampos Pozidis