2019,Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks,Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is useful in many applications ranging from robustness certification of classifiers to stability analysis of closed-loop systems with reinforcement learning controllers. Existing methods in the literature for estimating the Lipschitz constant suffer from either lack of accuracy or poor scalability. In this paper  we present a convex optimization framework to compute guaranteed upper bounds on the Lipschitz constant of DNNs both accurately and efficiently. Our main idea is to interpret activation functions as gradients of convex potential functions. Hence  they satisfy certain properties that can be described by quadratic constraints. This particular description allows us to pose the Lipschitz constant estimation problem as a semidefinite program (SDP). The resulting SDP can be adapted to increase either the estimation accuracy (by capturing the interaction between activation functions of different layers) or scalability (by decomposition and parallel implementation).  We illustrate the utility of our approach with a variety of experiments on randomly generated networks and on classifiers trained on the MNIST and Iris datasets. In particular  we experimentally demonstrate that our Lipschitz bounds are the most accurate compared to those in the literature. We also study the impact of adversarial training methods on the Lipschitz bounds of the resulting classifiers and show that our bounds can be used to efficiently provide robustness guarantees.,Efﬁcient and Accurate Estimation of Lipschitz

Constants for Deep Neural Networks

Mahyar Fazlyab
ESE Department

University of Pennsylvania

Philadephia   PA 19104

mahyarfa@seas.upenn.edu

Alexander Robey
ESE Department

University of Pennsylvania

Philadephia   PA 19104

arobey1@seas.upenn.edu

Hamed Hassani
ESE Department

University of Pennsylvania

Philadephia   PA 19104

hassani@seas.upenn.edu

Manfred Morari
ESE Department

University of Pennsylvania

Philadephia   PA 19104

morari@seas.upenn.edu

George J. Pappas
ESE Department

University of Pennsylvania

Philadephia   PA 19104

pappasg@seas.upenn.edu

Abstract

Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is
useful in many applications ranging from robustness certiﬁcation of classiﬁers to
stability analysis of closed-loop systems with reinforcement learning controllers.
Existing methods in the literature for estimating the Lipschitz constant suffer from
either lack of accuracy or poor scalability. In this paper  we present a convex
optimization framework to compute guaranteed upper bounds on the Lipschitz
constant of DNNs both accurately and efﬁciently. Our main idea is to interpret
activation functions as gradients of convex potential functions. Hence  they satisfy
certain properties that can be described by quadratic constraints. This particu-
lar description allows us to pose the Lipschitz constant estimation problem as a
semideﬁnite program (SDP). The resulting SDP can be adapted to increase either
the estimation accuracy (by capturing the interaction between activation functions
of different layers) or scalability (by decomposition and parallel implementation).
We illustrate the utility of our approach with a variety of experiments on randomly
generated networks and on classiﬁers trained on the MNIST and Iris datasets. In
particular  we experimentally demonstrate that our Lipschitz bounds are the most
accurate compared to those in the literature. We also study the impact of adversarial
training methods on the Lipschitz bounds of the resulting classiﬁers and show that
our bounds can be used to efﬁciently provide robustness guarantees.

Introduction

1
A function f : Rn → Rm is globally Lipschitz continuous on X ⊆ Rn if there exists a nonnegative
constant L ≥ 0 such that
(1)
The smallest such L is called the Lipschitz constant of f. The Lipschitz constant is the maximum
ratio between variations in the output space and variations in the input space of f and thus is a
measure of sensitivity of the function with respect to input perturbations.
When a function f is characterized by a deep neural network (DNN)  tight bounds on its Lipschitz
constant can be extremely useful in a variety of applications. In classiﬁcation tasks  for instance 
L can be used as a certiﬁcate of robustness of a neural network classiﬁer to adversarial attacks if

(cid:107)f (x) − f (y)(cid:107) ≤ L(cid:107)x − y(cid:107) for all x  y ∈ X .

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

it is estimated tightly [34]. In deep reinforcement learning  tight bounds on the Lipschitz constant
of a DNN-based controller can be directly used to analyze the stability of the closed-loop system.
Lipschitz regularity can also play a key role in derivation of generalization bounds [6]. In these
applications and many others  it is essential to have tight bounds on the Lipschitz constant of DNNs.
However  as DNNs have highly complex and non-linear structures  estimating the Lipschitz constant
both accurately and efﬁciently has remained a signiﬁcant challenge.
Our contributions. In this paper we propose a novel convex programming framework to derive tight
bounds on the global Lipschitz constant of deep feed-forward neural networks. Our framework yields
signiﬁcantly more accurate bounds compared to the state-of-the-art and lends itself to a distributed
implementation  leading to efﬁcient computation of the bounds for large-scale networks.
Our approach. We use the fact that all common nonlinear activation functions used in neural
networks are gradients of convex functions; hence  as operators  they satisfy certain properties that
can be abstracted as quadratic constraints on their input-output values. This particular abstraction
allows us to pose the Lipschitz estimation problem as a semideﬁnite program (SDP)  which we
call LipSDP. A striking feature of LipSDP is its ﬂexibility to span the trade-off between estimation
accuracy and computational efﬁciency by adding or removing extra decision variables. In particular 
for a neural network with (cid:96) layers and a total of n hidden neurons  the number of decision variables
can vary from (cid:96) (least accurate but most scalable) to O(n2) (most accurate but least scalable). As
such  we derive several distinct yet related formulations of LipSDP that span this trade-off. To scale
each variant of LipSDP to larger networks  we also propose a distributed implementation.
Our results. We illustrate our approach in a variety of experiments on both randomly generated
networks as well as networks trained on the MNIST [23] and Iris [11] datasets. First  we show
empirically that our Lipschitz bounds are the most accurate compared to all other existing methods of
which we are aware. In particular  our experiments on neural networks trained for MNIST show that
our bounds outperform all comparable methods; see Figure 2a for details. Furthermore  we investigate
the effect of two robust training procedures [24  40] on the Lipschitz constant for networks trained
on the MNIST dataset. Our results suggest that robust training procedures signiﬁcantly decrease the
Lipschitz constant of the resulting classiﬁers. Moreover  we use the Lipschitz bound for two robust
training procedures to derive non-vacuous lower bounds on the minimum adversarial perturbation
necessary to change the classiﬁcation of any instance from the test set. For details  see Figure 3.
Related work. The problem of estimating the Lipschitz constant for neural networks has been
studied in several works. In [34]  the authors estimate the global Lipschitz constant of DNNs by the
product of Lipschitz constants of individual layers. This approach is scalable and general but yields
trivial bounds. In [10]  the authors derive bounds on Lipschitz constants by treating the activation
functions as non-expansive averaged operators. The resulting algorithm scales well with the number
of hidden units per layer  but exponentially with the number of layers. In [37]  the authors decompose
the weight matrices of a neural network via singular value decomposition and approximately solve a
convex maximization problem over the unit cube. Notably  estimating the Lipschitz constant using
the method in [37] is intractable even for small networks; indeed  the authors of [37] use a greedy
algorithm to compute a bound  which may underapproximate the Lipschitz constant. In [2]  the
maximum spectral norm of the network Jacobian (taken over the data distribution) is used as an
estimate of the true Lipschitz constant. Again  this approach is not guaranteed to be an upper bound
on the Lipschitz constant. Bounding Lipschitz constants for the speciﬁc case of convolutional neural
networks (CNNs) has also been addressed in [5  44  6].
Using Lipschitz bounds in the context of adversarial robustness and safety veriﬁcation has also been
addressed in several works [39  31  38]. In particular  in [39]  the authors convert the robustness
analysis problem into a local Lipschitz constant estimation problem  where they estimate this local
constant by a set of independently and identically sampled local gradients. This algorithm is scalable
but is not guaranteed to provide upper bounds. In a similar work  the authors of [38] exploit the
piece-wise linear structure of ReLU functions to estimate the local Lipschitz constant of neural
networks. In [13]  the authors use quadratic constraints and semideﬁnite programming to analyze
local (point-wise) robustness of neural networks. In contrast  our Lipschitz bounds can be used as a
global certiﬁcate of robustness and are agnostic to the choice of the test data.

2

1.1 Motivating applications

We now enumerate two applications that highlight the importance of estimating the Lipschitz constant
of DNNs accurately and efﬁciently.
Robustness certiﬁcation of classiﬁers. In response to fragility of DNNs to adversarial attacks 
there has been considerable effort in recent years to improve the robustness of neural networks
against adversarial attacks and input perturbations [16  26  43  22  24  40]. In order to certify and/or
improve the robustness of neural networks  one must be able to bound the possible outputs of the
neural network over a region of input space. This can be done either locally around a speciﬁc
input [7  35  15  33  12  29  30  13  40  21  41  42]  or globally by bounding the sensitivity of the
function to input perturbations  i.e.  the Lipschitz constant [19  34  28  39]. Indeed  tight upper bounds
on the Lipschitz constant can be used to derive non-vacuous lower bounds on the magnitudes of
perturbations necessary to change the decision of neural networks. Finally  an efﬁcient computation
of these bounds can be useful in either assessing robustness after training [29  30  13] or promoting
robustness during training [40  36  17]. In the experiments section  we explore this application in
depth.
Stability analysis of closed-loop systems with learning controllers. A central problem in learning-
based control is to provide stability or safety guarantees for a feedback control loop when a learning-
enabled component  such as a deep neural network  is introduced in the loop [4  8  20]. The Lipschitz
constant of a neural network controller bounds its gain. Therefore a tight estimate can be useful for
certifying the stability of the closed-loop system.
Notation. We denote the set of real n-dimensional vectors by Rn  the set of m × n-dimensional
matrices by Rm×n  and the n-dimensional identity matrix by In. We denote by Sn  Sn
+  and Sn
++
the sets of n-by-n symmetric  positive semideﬁnite  and positive deﬁnite matrices  respectively. The
p-norm (p ≥ 1) is denoted by (cid:107) · (cid:107)p : Rn → R+. The (cid:96)2-norm of a matrix W ∈ Rm×n is the largest
singular value of W . We denote the i-th unit vector in Rn by ei. We write diag(a1  ...  an) for a
diagonal matrix whose diagonal entries starting in the upper left corner are a1 ···   an.
2 LipSDP: Lipschitz certiﬁcates via semideﬁnite programming

x0 = x 

f (x) = W (cid:96)x(cid:96) + b(cid:96).

xk+1 = φ(W kxk + bk) for k = 0 ···   (cid:96) − 1 

2.1 Problem statement
Consider an (cid:96)-layer feed-forward neural network f (x) : Rn0 → Rn(cid:96)+1 described by the following
recursive equations:
(2)
Here x ∈ Rn0 is an input to the network and W k ∈ Rnk+1×nk and bk ∈ Rnk+1 are the weight matrix
and bias vector for the k-th layer. The function φ is the concatenation of activation functions at each
layer  i.e.  it is of the form φ(x) = [ϕ(x1) ··· ϕ(xn)](cid:62). In this paper  our goal is to ﬁnd tight bounds
on the Lipschitz constant of the map x (cid:55)→ f (x) in (cid:96)2-norm. More precisely  we wish to ﬁnd the
smallest constant L2 ≥ 0 such that (cid:107)f (x) − f (y)(cid:107)2 ≤ L2(cid:107)x − y(cid:107)2 for all x  y ∈ Rn0.
The main source of difﬁculty in solving this problem is the presence of the nonlinear activation
functions. To combat this difﬁculty  our main idea is to abstract these activation functions by a set of
constraints that they impose on their input and output values. Then any property (including Lipschitz
continuity) that is satisﬁed by our abstraction will also be satisﬁed by the original network.

2.2 Description of activation functions by quadratic constraints

In this section  we introduce several deﬁnitions and lemmas that characterize our abstraction of
nonlinear activation functions. These results are crucial to the formulation of an SDP that can bound
the Lipschitz constants of networks in Section 2.3.
Deﬁnition 1 (Slope-restricted non-linearity) A function ϕ : R → R is slope-restricted on [α  β]
where 0 ≤ α < β < ∞ if

α ≤

ϕ(y) − ϕ(x)

y − x

≤ β ∀x  y ∈ R.

3

(3)

inequality  ϕ(x) is one-sided Lipschitz with parameter β. Altogether  the preceding inequalities state

The inequality in (3) simply states that the slope of the chord connecting any two points on the curve
of the function x (cid:55)→ ϕ(x) is at least α and at most β (see Figure 1). By multiplying all sides of (3) by
(y−x)2  we can write the slope restriction condition as α(y−x)2 ≤ (ϕ(y)−ϕ(x))(y−x) ≤ β(y−x)2.
By the left inequality  the operator ϕ(x) is strongly monotone with parameter α [32]  or equivalently
the anti-derivative function(cid:82) ϕ(x)dx is strongly convex with parameter α. By the right-hand side
that the anti-derivative function(cid:82) ϕ(x)dx is α-strongly convex and β-smooth.
Note that  except for special cases [2]  all common activation functions used in deep learning satisfy
the slope restriction condition in (3) for some 0 ≤ α < β < ∞. For instance  the ReLU  tanh  and
sigmoid activation functions are all slope restricted with α = 0 and β = 1. More details can be found
in [13].
Deﬁnition 2 (Incremental Quadratic Constraint [1]) A function φ : Rn → Rn satisﬁes the incre-
mental quadratic constraint deﬁned by Q ⊂ S2n if for any Q ∈ Q and x  y ∈ Rn 

(cid:20)
φ(x) − φ(y)(cid:21)(cid:62)

x − y

Q(cid:20)
φ(x) − φ(y)(cid:21) ≥ 0.

x − y

(4)

In the above deﬁnition  Q is the set of all multiplier matrices that characterize φ 
the softmax operator φ(x) =
and is a convex cone by deﬁnition.
((cid:80)n
i=1 exp(xi))−1[exp(x1)··· exp(xn)](cid:62) is the gradient of the convex function ψ(x) =
log((cid:80)n
i=1 exp(xi)). This function is smooth and strongly convex with paramters α = 0 and β = 1
[9]. For this class of functions  it is known that the gradient function φ(x) = ∇ψ(x) satisﬁes the
quadratic inequality [25]

As an example 

x − y

(cid:20)
φ(x) − φ(y)(cid:21)(cid:62)(cid:20) −2αβIn

−2In (cid:21)(cid:20)
Therefore  the softmax operator satisﬁes the incremental quadratic constraint deﬁned by Q = {λM |
λ ≥ 0}  where M the middle matrix in the above inequality.
To see the connection between incremental quadratic constraints and slope-restricted nonlinearities 
note that (3) can be equivalently written as the single inequality

φ(x) − φ(y)(cid:21) ≥ 0.

(α + β)In

(α + β)In

x − y

(5)

(

ϕ(y) − ϕ(x)

y − x

− α)(

ϕ(y) − ϕ(x)

y − x

− β) ≤ 0.

Multiplying through by (y − x)2 and rearranging terms  we can write (6) as
ϕ(x) − ϕ(y)(cid:21) ≥ 0 

ϕ(x) − ϕ(y)(cid:21)(cid:62)(cid:20)−2αβ α + β
(cid:20)
−2 (cid:21)(cid:20)

x − y

x − y

α + β

(6)

(7)

which  in view of Deﬁnition 2  is an incremental quadratic constraint for ϕ. From this perspective 
incremental quadratic constraints generalize the notion of slope-restricted nonlinearities to multi-
variable vector-valued nonlinearities.
Repeated nonlinearities. Now consider the vector-valued function φ(x) = [ϕ(x1)··· ϕ(xn)](cid:62)
obtained by applying a slope-restricted function ϕ component-wise to a vector x ∈ Rn. By exploiting
the fact that the same function ϕ is applied to each component  we can characterize φ(x) by O(n2)
incremental quadratic constraints. In the following lemma  we provide such a characterization.
Lemma 1 Suppose ϕ : R → R is slope-restricted on [α  β]. Deﬁne the set

Tn = {T ∈ Sn | T =

n(cid:88)i=1

λiieie(cid:62)i + (cid:88)1≤i<j≤n

λij(ei − ej)(ei − ej)(cid:62)  λij ≥ 0}.

Then for any T ∈ Tn the vector-valued function φ(x) = [ϕ(x1)··· ϕ(xn)](cid:62) : Rn → Rn satisﬁes

(cid:20)
φ(x) − φ(y)(cid:21)(cid:62)(cid:20) −2αβT

x − y

(α + β)T

(α + β)T

−2T (cid:21)(cid:20)

φ(x) − φ(y)(cid:21) ≥ 0 for all x  y ∈ Rn.

x − y

(8)

(9)

4

Figure 1: An illustrative description of encoding activation functions by quadratic constraints.

Concretely  this lemma captures the coupling between neurons in a neural network by taking advantage
of two particular structures: (a) the same activation function is applied to each hidden neuron and
(b) all activation functions are slope-restricted on the same interval [α  β]. In this way  we can write
the slope restriction condition in (1) for any pair of activation functions in a given neural network.
A conic combination of these constraints would yield (9)  where λij are the coefﬁcients of this
combination. See Figure 1 for an illustrative description.
We will see in the next section that the matrix T that parameterizes the multiplier matrix in (9) appears
as a decision variable in an SDP  in which the objective is to ﬁnd an admissible T that yields the
tightest bound on the Lipschitz constant.

2.3 LipSDP for single-layer neural network

To develop an optimization problem to estimate the Lipschitz constant of a fully-connected feed-
forward neural network  the key insight is that the Lipschitz condition in (1) is in fact equivalent
to an incremental quadratic constraint for the map x (cid:55)→ f (x) characterized by the neural network.
By coupling this to the incremental quadratic constraints satisﬁed by the cascade combination of
the activation functions [14]  we can develop an SDP to minimize an upper bound on the Lipschitz
constant of f. This result is formally stated in the following theorem.
Theorem 1 (Lipshitz certiﬁcates for single-layer neural networks) Consider a single-layer neu-
ral network described by f (x) = W 1φ(W 0x + b0) + b1. Suppose φ(x) : Rn → Rn =
[ϕ(x1)··· ϕ(xn)]  where ϕ is slope-restricted in the sector [α  β]. Deﬁne Tn as in (8). Suppose there
exists a ρ > 0 such that the matrix inequality

M (ρ  T ) :=(cid:34)−2αβW 0(cid:62)T W 0 − ρIn0

−2T + W 1(cid:62)W 1(cid:35) (cid:22) 0 
holds for some T ∈ Tn. Then (cid:107)f (x) − f (y)(cid:107)2 ≤ √ρ(cid:107)x − y(cid:107)2 for all x  y ∈ Rn0.
Theorem 1 provides us with a sufﬁcient condition for L2 = √ρ to be an upper bound on the Lipschitz
constant of f (x) = W 1φ(W 0x + b0) + b1. In particular  we can ﬁnd the tightest bound by solving
the following optimization problem:

(α + β)W 0(cid:62)T

(α + β)T W 0

(10)

minimize ρ

(11)
where the decision variables are (ρ  T ) ∈ R+ × Tn. Note that M (ρ  T ) is linear in ρ and T and the
set Tn is convex. Hence  (11) is an SDP  which can be solved numerically for its global minimum.
2.4 LipSDP for multi-layer neural networks

subject to M (ρ  T ) (cid:22) 0

and T ∈ Tn 

We now consider the multi-layer case. Assuming that all the activation functions are the same  we
can write the neural network model in (2) compactly as

Bx = φ(Ax + b)

and

f (x) = Cx + b(cid:96) 

(12)

5

-505-1-0.500.51(xj '(xj))<latexit sha1_base64="7KVgF9Hepxz+AlzTMESMIJp61E0=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQUpSTe6LLhxWcFeoA1hMp20YyeTMDMp1lJ8FTcuFHHre7jzbZy0WWjrDwMf/zmHc+b3Y0alsu1vI7e2vrG5ld8u7Ozu7R+Yh0ctGSUCkyaOWCQ6PpKEUU6aiipGOrEgKPQZafuj67TeHhMhacTv1CQmbogGnAYUI6UtzzwplR+8+4veGIl4SFOuVEqeWbSr9lzWKjgZFCFTwzO/ev0IJyHhCjMkZdexY+VOkVAUMzIr9BJJYoRHaEC6GjkKiXSn8+tn1rl2+lYQCf24subu74kpCqWchL7uDJEayuVaav5X6yYquHKnlMeJIhwvFgUJs1RkpVFYfSoIVmyiAWFB9a0WHiKBsNKBFXQIzvKXV6FVqzqab2vFej2LIw+ncAZlcOAS6nADDWgChkd4hld4M56MF+Pd+Fi05oxs5hj+yPj8AXzPk+0=</latexit><latexit sha1_base64="7KVgF9Hepxz+AlzTMESMIJp61E0=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQUpSTe6LLhxWcFeoA1hMp20YyeTMDMp1lJ8FTcuFHHre7jzbZy0WWjrDwMf/zmHc+b3Y0alsu1vI7e2vrG5ld8u7Ozu7R+Yh0ctGSUCkyaOWCQ6PpKEUU6aiipGOrEgKPQZafuj67TeHhMhacTv1CQmbogGnAYUI6UtzzwplR+8+4veGIl4SFOuVEqeWbSr9lzWKjgZFCFTwzO/ev0IJyHhCjMkZdexY+VOkVAUMzIr9BJJYoRHaEC6GjkKiXSn8+tn1rl2+lYQCf24subu74kpCqWchL7uDJEayuVaav5X6yYquHKnlMeJIhwvFgUJs1RkpVFYfSoIVmyiAWFB9a0WHiKBsNKBFXQIzvKXV6FVqzqab2vFej2LIw+ncAZlcOAS6nADDWgChkd4hld4M56MF+Pd+Fi05oxs5hj+yPj8AXzPk+0=</latexit><latexit sha1_base64="7KVgF9Hepxz+AlzTMESMIJp61E0=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQUpSTe6LLhxWcFeoA1hMp20YyeTMDMp1lJ8FTcuFHHre7jzbZy0WWjrDwMf/zmHc+b3Y0alsu1vI7e2vrG5ld8u7Ozu7R+Yh0ctGSUCkyaOWCQ6PpKEUU6aiipGOrEgKPQZafuj67TeHhMhacTv1CQmbogGnAYUI6UtzzwplR+8+4veGIl4SFOuVEqeWbSr9lzWKjgZFCFTwzO/ev0IJyHhCjMkZdexY+VOkVAUMzIr9BJJYoRHaEC6GjkKiXSn8+tn1rl2+lYQCf24subu74kpCqWchL7uDJEayuVaav5X6yYquHKnlMeJIhwvFgUJs1RkpVFYfSoIVmyiAWFB9a0WHiKBsNKBFXQIzvKXV6FVqzqab2vFej2LIw+ncAZlcOAS6nADDWgChkd4hld4M56MF+Pd+Fi05oxs5hj+yPj8AXzPk+0=</latexit><latexit sha1_base64="7KVgF9Hepxz+AlzTMESMIJp61E0=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQUpSTe6LLhxWcFeoA1hMp20YyeTMDMp1lJ8FTcuFHHre7jzbZy0WWjrDwMf/zmHc+b3Y0alsu1vI7e2vrG5ld8u7Ozu7R+Yh0ctGSUCkyaOWCQ6PpKEUU6aiipGOrEgKPQZafuj67TeHhMhacTv1CQmbogGnAYUI6UtzzwplR+8+4veGIl4SFOuVEqeWbSr9lzWKjgZFCFTwzO/ev0IJyHhCjMkZdexY+VOkVAUMzIr9BJJYoRHaEC6GjkKiXSn8+tn1rl2+lYQCf24subu74kpCqWchL7uDJEayuVaav5X6yYquHKnlMeJIhwvFgUJs1RkpVFYfSoIVmyiAWFB9a0WHiKBsNKBFXQIzvKXV6FVqzqab2vFej2LIw+ncAZlcOAS6nADDWgChkd4hld4M56MF+Pd+Fi05oxs5hj+yPj8AXzPk+0=</latexit>(xi '(xi))<latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit>↵'(xj)'(xi)xjxi<latexit sha1_base64="bXpVOSwBhezioWcgI+XxN1XSrkk=">AAACLHicbZDLSgMxFIYzXmu9jbp0E6yCLiwz3eiy0I3LCrYVOqWcSc/Y2MzFJFMsQx/Ija8iiAuLuPU5TNsBrz8EvvznHJLz+4ngSjvOxFpYXFpeWS2sFdc3Nre27Z3dpopTybDBYhHLax8UCh5hQ3Mt8DqRCKEvsOUPatN6a4hS8Ti60qMEOyHcRDzgDLSxunbt0AOR9IF6Au+o1wsksMwbgkz6/Pi+e3ty+nXhJ+PMWKeGxnm/jxoOu3bJKTsz0b/g5lAiuepd+9nrxSwNMdJMgFJt10l0JwOpORM4LnqpwgTYAG6wbTCCEFUnmy07pkfG6dEgluZEms7c7xMZhEqNQt90hqD76ndtav5Xa6c6OO9kPEpSjRGbPxSkguqYTpOjPS6RaTEyAExy81fK+mDy0ibfognB/b3yX2hWyq7hy0qpWs3jKJB9ckCOiUvOSJVckDppEEYeyBN5JRPr0Xqx3qz3eeuClc/skR+yPj4Be/+nuQ==</latexit><latexit sha1_base64="bXpVOSwBhezioWcgI+XxN1XSrkk=">AAACLHicbZDLSgMxFIYzXmu9jbp0E6yCLiwz3eiy0I3LCrYVOqWcSc/Y2MzFJFMsQx/Ija8iiAuLuPU5TNsBrz8EvvznHJLz+4ngSjvOxFpYXFpeWS2sFdc3Nre27Z3dpopTybDBYhHLax8UCh5hQ3Mt8DqRCKEvsOUPatN6a4hS8Ti60qMEOyHcRDzgDLSxunbt0AOR9IF6Au+o1wsksMwbgkz6/Pi+e3ty+nXhJ+PMWKeGxnm/jxoOu3bJKTsz0b/g5lAiuepd+9nrxSwNMdJMgFJt10l0JwOpORM4LnqpwgTYAG6wbTCCEFUnmy07pkfG6dEgluZEms7c7xMZhEqNQt90hqD76ndtav5Xa6c6OO9kPEpSjRGbPxSkguqYTpOjPS6RaTEyAExy81fK+mDy0ibfognB/b3yX2hWyq7hy0qpWs3jKJB9ckCOiUvOSJVckDppEEYeyBN5JRPr0Xqx3qz3eeuClc/skR+yPj4Be/+nuQ==</latexit><latexit sha1_base64="bXpVOSwBhezioWcgI+XxN1XSrkk=">AAACLHicbZDLSgMxFIYzXmu9jbp0E6yCLiwz3eiy0I3LCrYVOqWcSc/Y2MzFJFMsQx/Ija8iiAuLuPU5TNsBrz8EvvznHJLz+4ngSjvOxFpYXFpeWS2sFdc3Nre27Z3dpopTybDBYhHLax8UCh5hQ3Mt8DqRCKEvsOUPatN6a4hS8Ti60qMEOyHcRDzgDLSxunbt0AOR9IF6Au+o1wsksMwbgkz6/Pi+e3ty+nXhJ+PMWKeGxnm/jxoOu3bJKTsz0b/g5lAiuepd+9nrxSwNMdJMgFJt10l0JwOpORM4LnqpwgTYAG6wbTCCEFUnmy07pkfG6dEgluZEms7c7xMZhEqNQt90hqD76ndtav5Xa6c6OO9kPEpSjRGbPxSkguqYTpOjPS6RaTEyAExy81fK+mDy0ibfognB/b3yX2hWyq7hy0qpWs3jKJB9ckCOiUvOSJVckDppEEYeyBN5JRPr0Xqx3qz3eeuClc/skR+yPj4Be/+nuQ==</latexit><latexit sha1_base64="bXpVOSwBhezioWcgI+XxN1XSrkk=">AAACLHicbZDLSgMxFIYzXmu9jbp0E6yCLiwz3eiy0I3LCrYVOqWcSc/Y2MzFJFMsQx/Ija8iiAuLuPU5TNsBrz8EvvznHJLz+4ngSjvOxFpYXFpeWS2sFdc3Nre27Z3dpopTybDBYhHLax8UCh5hQ3Mt8DqRCKEvsOUPatN6a4hS8Ti60qMEOyHcRDzgDLSxunbt0AOR9IF6Au+o1wsksMwbgkz6/Pi+e3ty+nXhJ+PMWKeGxnm/jxoOu3bJKTsz0b/g5lAiuepd+9nrxSwNMdJMgFJt10l0JwOpORM4LnqpwgTYAG6wbTCCEFUnmy07pkfG6dEgluZEms7c7xMZhEqNQt90hqD76ndtav5Xa6c6OO9kPEpSjRGbPxSkguqYTpOjPS6RaTEyAExy81fK+mDy0ibfognB/b3yX2hWyq7hy0qpWs3jKJB9ckCOiUvOSJVckDppEEYeyBN5JRPr0Xqx3qz3eeuClc/skR+yPj4Be/+nuQ==</latexit>xi<latexit sha1_base64="pmg+gnVv+HU4HLbjrH6Vq/vhhsQ=">AAAB7HicbZA9TwJBEIbn8AvxC7W02QgmVuSORkoSG0tM5COBC9lb9mDD3t5ld85ILvwGGwuNsfUH2flvXOAKBd9kkyfvzGRn3iCRwqDrfjuFre2d3b3ifung8Oj4pHx61jFxqhlvs1jGuhdQw6VQvI0CJe8lmtMokLwbTG8X9e4j10bE6gFnCfcjOlYiFIyitdrVp6GoDssVt+YuRTbBy6ECuVrD8tdgFLM04gqZpMb0PTdBP6MaBZN8XhqkhieUTemY9y0qGnHjZ8tl5+TKOiMSxto+hWTp/p7IaGTMLApsZ0RxYtZrC/O/Wj/FsOFnQiUpcsVWH4WpJBiTxeVkJDRnKGcWKNPC7krYhGrK0OZTsiF46ydvQqde8yzf1yvNRh5HES7gEq7Bgxtowh20oA0MBDzDK7w5ynlx3p2PVWvByWfO4Y+czx8VwY4o</latexit><latexit sha1_base64="pmg+gnVv+HU4HLbjrH6Vq/vhhsQ=">AAAB7HicbZA9TwJBEIbn8AvxC7W02QgmVuSORkoSG0tM5COBC9lb9mDD3t5ld85ILvwGGwuNsfUH2flvXOAKBd9kkyfvzGRn3iCRwqDrfjuFre2d3b3ifung8Oj4pHx61jFxqhlvs1jGuhdQw6VQvI0CJe8lmtMokLwbTG8X9e4j10bE6gFnCfcjOlYiFIyitdrVp6GoDssVt+YuRTbBy6ECuVrD8tdgFLM04gqZpMb0PTdBP6MaBZN8XhqkhieUTemY9y0qGnHjZ8tl5+TKOiMSxto+hWTp/p7IaGTMLApsZ0RxYtZrC/O/Wj/FsOFnQiUpcsVWH4WpJBiTxeVkJDRnKGcWKNPC7krYhGrK0OZTsiF46ydvQqde8yzf1yvNRh5HES7gEq7Bgxtowh20oA0MBDzDK7w5ynlx3p2PVWvByWfO4Y+czx8VwY4o</latexit><latexit sha1_base64="pmg+gnVv+HU4HLbjrH6Vq/vhhsQ=">AAAB7HicbZA9TwJBEIbn8AvxC7W02QgmVuSORkoSG0tM5COBC9lb9mDD3t5ld85ILvwGGwuNsfUH2flvXOAKBd9kkyfvzGRn3iCRwqDrfjuFre2d3b3ifung8Oj4pHx61jFxqhlvs1jGuhdQw6VQvI0CJe8lmtMokLwbTG8X9e4j10bE6gFnCfcjOlYiFIyitdrVp6GoDssVt+YuRTbBy6ECuVrD8tdgFLM04gqZpMb0PTdBP6MaBZN8XhqkhieUTemY9y0qGnHjZ8tl5+TKOiMSxto+hWTp/p7IaGTMLApsZ0RxYtZrC/O/Wj/FsOFnQiUpcsVWH4WpJBiTxeVkJDRnKGcWKNPC7krYhGrK0OZTsiF46ydvQqde8yzf1yvNRh5HES7gEq7Bgxtowh20oA0MBDzDK7w5ynlx3p2PVWvByWfO4Y+czx8VwY4o</latexit><latexit sha1_base64="pmg+gnVv+HU4HLbjrH6Vq/vhhsQ=">AAAB7HicbZA9TwJBEIbn8AvxC7W02QgmVuSORkoSG0tM5COBC9lb9mDD3t5ld85ILvwGGwuNsfUH2flvXOAKBd9kkyfvzGRn3iCRwqDrfjuFre2d3b3ifung8Oj4pHx61jFxqhlvs1jGuhdQw6VQvI0CJe8lmtMokLwbTG8X9e4j10bE6gFnCfcjOlYiFIyitdrVp6GoDssVt+YuRTbBy6ECuVrD8tdgFLM04gqZpMb0PTdBP6MaBZN8XhqkhieUTemY9y0qGnHjZ8tl5+TKOiMSxto+hWTp/p7IaGTMLApsZ0RxYtZrC/O/Wj/FsOFnQiUpcsVWH4WpJBiTxeVkJDRnKGcWKNPC7krYhGrK0OZTsiF46ydvQqde8yzf1yvNRh5HES7gEq7Bgxtowh20oA0MBDzDK7w5ynlx3p2PVWvByWfO4Y+czx8VwY4o</latexit>'(xi)<latexit sha1_base64="2RSP5OM1rKvlHPSZBvlymYbyN3E=">AAAB9XicbZDLTgIxFIbP4A3xhrp00wgmuCEzbHRJ4sYlJnJJYCSd0oGGTjtpOyiZ8B5uXGiMW9/FnW9jgVko+CdNvvznnJzTP4g508Z1v53cxubW9k5+t7C3f3B4VDw+aWmZKEKbRHKpOgHWlDNBm4YZTjuxojgKOG0H45t5vT2hSjMp7s00pn6Eh4KFjGBjrYdyb4JVPGKVpz67LPeLJbfqLoTWwcugBJka/eJXbyBJElFhCMdadz03Nn6KlWGE01mhl2gaYzLGQ9q1KHBEtZ8urp6hC+sMUCiVfcKghft7IsWR1tMosJ0RNiO9Wpub/9W6iQmv/ZSJODFUkOWiMOHISDSPAA2YosTwqQVMFLO3IjLCChNjgyrYELzVL69Dq1b1LN/VSvV6FkcezuAcKuDBFdThFhrQBAIKnuEV3pxH58V5dz6WrTknmzmFP3I+fwBh5JHB</latexit><latexit sha1_base64="2RSP5OM1rKvlHPSZBvlymYbyN3E=">AAAB9XicbZDLTgIxFIbP4A3xhrp00wgmuCEzbHRJ4sYlJnJJYCSd0oGGTjtpOyiZ8B5uXGiMW9/FnW9jgVko+CdNvvznnJzTP4g508Z1v53cxubW9k5+t7C3f3B4VDw+aWmZKEKbRHKpOgHWlDNBm4YZTjuxojgKOG0H45t5vT2hSjMp7s00pn6Eh4KFjGBjrYdyb4JVPGKVpz67LPeLJbfqLoTWwcugBJka/eJXbyBJElFhCMdadz03Nn6KlWGE01mhl2gaYzLGQ9q1KHBEtZ8urp6hC+sMUCiVfcKghft7IsWR1tMosJ0RNiO9Wpub/9W6iQmv/ZSJODFUkOWiMOHISDSPAA2YosTwqQVMFLO3IjLCChNjgyrYELzVL69Dq1b1LN/VSvV6FkcezuAcKuDBFdThFhrQBAIKnuEV3pxH58V5dz6WrTknmzmFP3I+fwBh5JHB</latexit><latexit sha1_base64="2RSP5OM1rKvlHPSZBvlymYbyN3E=">AAAB9XicbZDLTgIxFIbP4A3xhrp00wgmuCEzbHRJ4sYlJnJJYCSd0oGGTjtpOyiZ8B5uXGiMW9/FnW9jgVko+CdNvvznnJzTP4g508Z1v53cxubW9k5+t7C3f3B4VDw+aWmZKEKbRHKpOgHWlDNBm4YZTjuxojgKOG0H45t5vT2hSjMp7s00pn6Eh4KFjGBjrYdyb4JVPGKVpz67LPeLJbfqLoTWwcugBJka/eJXbyBJElFhCMdadz03Nn6KlWGE01mhl2gaYzLGQ9q1KHBEtZ8urp6hC+sMUCiVfcKghft7IsWR1tMosJ0RNiO9Wpub/9W6iQmv/ZSJODFUkOWiMOHISDSPAA2YosTwqQVMFLO3IjLCChNjgyrYELzVL69Dq1b1LN/VSvV6FkcezuAcKuDBFdThFhrQBAIKnuEV3pxH58V5dz6WrTknmzmFP3I+fwBh5JHB</latexit><latexit sha1_base64="2RSP5OM1rKvlHPSZBvlymYbyN3E=">AAAB9XicbZDLTgIxFIbP4A3xhrp00wgmuCEzbHRJ4sYlJnJJYCSd0oGGTjtpOyiZ8B5uXGiMW9/FnW9jgVko+CdNvvznnJzTP4g508Z1v53cxubW9k5+t7C3f3B4VDw+aWmZKEKbRHKpOgHWlDNBm4YZTjuxojgKOG0H45t5vT2hSjMp7s00pn6Eh4KFjGBjrYdyb4JVPGKVpz67LPeLJbfqLoTWwcugBJka/eJXbyBJElFhCMdadz03Nn6KlWGE01mhl2gaYzLGQ9q1KHBEtZ8urp6hC+sMUCiVfcKghft7IsWR1tMosJ0RNiO9Wpub/9W6iQmv/ZSJODFUkOWiMOHISDSPAA2YosTwqQVMFLO3IjLCChNjgyrYELzVL69Dq1b1LN/VSvV6FkcezuAcKuDBFdThFhrQBAIKnuEV3pxH58V5dz6WrTknmzmFP3I+fwBh5JHB</latexit>'(yi)<latexit sha1_base64="RG1iAtaTSYwavALhRg+orYL4LFU=">AAAB9XicbZDLTgIxFIbPeEW8oS7dNIIJbsgMG12SuHGJiVwSGEmndKCh02naDmYy4T3cuNAYt76LO9/GArNQ8E+afPnPOTmnfyA508Z1v52Nza3tnd3CXnH/4PDouHRy2tZxoghtkZjHqhtgTTkTtGWY4bQrFcVRwGknmNzO650pVZrF4sGkkvoRHgkWMoKNtR4r/SlWcsyq6YBdVQalsltzF0Lr4OVQhlzNQemrP4xJElFhCMda9zxXGj/DyjDC6azYTzSVmEzwiPYsChxR7WeLq2fo0jpDFMbKPmHQwv09keFI6zQKbGeEzViv1ubmf7VeYsIbP2NCJoYKslwUJhyZGM0jQEOmKDE8tYCJYvZWRMZYYWJsUEUbgrf65XVo12ue5ft6udHI4yjAOVxAFTy4hgbcQRNaQEDBM7zCm/PkvDjvzseydcPJZ87gj5zPH2NskcI=</latexit><latexit sha1_base64="RG1iAtaTSYwavALhRg+orYL4LFU=">AAAB9XicbZDLTgIxFIbPeEW8oS7dNIIJbsgMG12SuHGJiVwSGEmndKCh02naDmYy4T3cuNAYt76LO9/GArNQ8E+afPnPOTmnfyA508Z1v52Nza3tnd3CXnH/4PDouHRy2tZxoghtkZjHqhtgTTkTtGWY4bQrFcVRwGknmNzO650pVZrF4sGkkvoRHgkWMoKNtR4r/SlWcsyq6YBdVQalsltzF0Lr4OVQhlzNQemrP4xJElFhCMda9zxXGj/DyjDC6azYTzSVmEzwiPYsChxR7WeLq2fo0jpDFMbKPmHQwv09keFI6zQKbGeEzViv1ubmf7VeYsIbP2NCJoYKslwUJhyZGM0jQEOmKDE8tYCJYvZWRMZYYWJsUEUbgrf65XVo12ue5ft6udHI4yjAOVxAFTy4hgbcQRNaQEDBM7zCm/PkvDjvzseydcPJZ87gj5zPH2NskcI=</latexit><latexit sha1_base64="RG1iAtaTSYwavALhRg+orYL4LFU=">AAAB9XicbZDLTgIxFIbPeEW8oS7dNIIJbsgMG12SuHGJiVwSGEmndKCh02naDmYy4T3cuNAYt76LO9/GArNQ8E+afPnPOTmnfyA508Z1v52Nza3tnd3CXnH/4PDouHRy2tZxoghtkZjHqhtgTTkTtGWY4bQrFcVRwGknmNzO650pVZrF4sGkkvoRHgkWMoKNtR4r/SlWcsyq6YBdVQalsltzF0Lr4OVQhlzNQemrP4xJElFhCMda9zxXGj/DyjDC6azYTzSVmEzwiPYsChxR7WeLq2fo0jpDFMbKPmHQwv09keFI6zQKbGeEzViv1ubmf7VeYsIbP2NCJoYKslwUJhyZGM0jQEOmKDE8tYCJYvZWRMZYYWJsUEUbgrf65XVo12ue5ft6udHI4yjAOVxAFTy4hgbcQRNaQEDBM7zCm/PkvDjvzseydcPJZ87gj5zPH2NskcI=</latexit><latexit sha1_base64="RG1iAtaTSYwavALhRg+orYL4LFU=">AAAB9XicbZDLTgIxFIbPeEW8oS7dNIIJbsgMG12SuHGJiVwSGEmndKCh02naDmYy4T3cuNAYt76LO9/GArNQ8E+afPnPOTmnfyA508Z1v52Nza3tnd3CXnH/4PDouHRy2tZxoghtkZjHqhtgTTkTtGWY4bQrFcVRwGknmNzO650pVZrF4sGkkvoRHgkWMoKNtR4r/SlWcsyq6YBdVQalsltzF0Lr4OVQhlzNQemrP4xJElFhCMda9zxXGj/DyjDC6azYTzSVmEzwiPYsChxR7WeLq2fo0jpDFMbKPmHQwv09keFI6zQKbGeEzViv1ubmf7VeYsIbP2NCJoYKslwUJhyZGM0jQEOmKDE8tYCJYvZWRMZYYWJsUEUbgrf65XVo12ue5ft6udHI4yjAOVxAFTy4hgbcQRNaQEDBM7zCm/PkvDjvzseydcPJZ87gj5zPH2NskcI=</latexit>yi<latexit sha1_base64="WmgSYaT4KZFZh7wSYgzHlBn+laY=">AAAB7HicbZBNT8JAEIan+IX4hXr0shFMPJGWix5JvHjExAIJNGS7bGHDdtvsTk1Iw2/w4kFjvPqDvPlvXKAHBd9kkyfvzGRn3jCVwqDrfjulre2d3b3yfuXg8Oj4pHp61jFJphn3WSIT3Qup4VIo7qNAyXup5jQOJe+G07tFvfvEtRGJesRZyoOYjpWIBKNoLb8+G4r6sFpzG+5SZBO8AmpQqD2sfg1GCctirpBJakzfc1MMcqpRMMnnlUFmeErZlI5536KiMTdBvlx2Tq6sMyJRou1TSJbu74mcxsbM4tB2xhQnZr22MP+r9TOMboNcqDRDrtjqoyiTBBOyuJyMhOYM5cwCZVrYXQmbUE0Z2nwqNgRv/eRN6DQbnuWHZq3VKuIowwVcwjV4cAMtuIc2+MBAwDO8wpujnBfn3flYtZacYuYc/sj5/AEZsI4x</latexit><latexit sha1_base64="WmgSYaT4KZFZh7wSYgzHlBn+laY=">AAAB7HicbZBNT8JAEIan+IX4hXr0shFMPJGWix5JvHjExAIJNGS7bGHDdtvsTk1Iw2/w4kFjvPqDvPlvXKAHBd9kkyfvzGRn3jCVwqDrfjulre2d3b3yfuXg8Oj4pHp61jFJphn3WSIT3Qup4VIo7qNAyXup5jQOJe+G07tFvfvEtRGJesRZyoOYjpWIBKNoLb8+G4r6sFpzG+5SZBO8AmpQqD2sfg1GCctirpBJakzfc1MMcqpRMMnnlUFmeErZlI5536KiMTdBvlx2Tq6sMyJRou1TSJbu74mcxsbM4tB2xhQnZr22MP+r9TOMboNcqDRDrtjqoyiTBBOyuJyMhOYM5cwCZVrYXQmbUE0Z2nwqNgRv/eRN6DQbnuWHZq3VKuIowwVcwjV4cAMtuIc2+MBAwDO8wpujnBfn3flYtZacYuYc/sj5/AEZsI4x</latexit><latexit sha1_base64="WmgSYaT4KZFZh7wSYgzHlBn+laY=">AAAB7HicbZBNT8JAEIan+IX4hXr0shFMPJGWix5JvHjExAIJNGS7bGHDdtvsTk1Iw2/w4kFjvPqDvPlvXKAHBd9kkyfvzGRn3jCVwqDrfjulre2d3b3yfuXg8Oj4pHp61jFJphn3WSIT3Qup4VIo7qNAyXup5jQOJe+G07tFvfvEtRGJesRZyoOYjpWIBKNoLb8+G4r6sFpzG+5SZBO8AmpQqD2sfg1GCctirpBJakzfc1MMcqpRMMnnlUFmeErZlI5536KiMTdBvlx2Tq6sMyJRou1TSJbu74mcxsbM4tB2xhQnZr22MP+r9TOMboNcqDRDrtjqoyiTBBOyuJyMhOYM5cwCZVrYXQmbUE0Z2nwqNgRv/eRN6DQbnuWHZq3VKuIowwVcwjV4cAMtuIc2+MBAwDO8wpujnBfn3flYtZacYuYc/sj5/AEZsI4x</latexit><latexit sha1_base64="WmgSYaT4KZFZh7wSYgzHlBn+laY=">AAAB7HicbZBNT8JAEIan+IX4hXr0shFMPJGWix5JvHjExAIJNGS7bGHDdtvsTk1Iw2/w4kFjvPqDvPlvXKAHBd9kkyfvzGRn3jCVwqDrfjulre2d3b3yfuXg8Oj4pHp61jFJphn3WSIT3Qup4VIo7qNAyXup5jQOJe+G07tFvfvEtRGJesRZyoOYjpWIBKNoLb8+G4r6sFpzG+5SZBO8AmpQqD2sfg1GCctirpBJakzfc1MMcqpRMMnnlUFmeErZlI5536KiMTdBvlx2Tq6sMyJRou1TSJbu74mcxsbM4tB2xhQnZr22MP+r9TOMboNcqDRDrtjqoyiTBBOyuJyMhOYM5cwCZVrYXQmbUE0Z2nwqNgRv/eRN6DQbnuWHZq3VKuIowwVcwjV4cAMtuIc2+MBAwDO8wpujnBfn3flYtZacYuYc/sj5/AEZsI4x</latexit>-505-1-0.500.51(xi '(xi))<latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit><latexit sha1_base64="GllpdvBtNW9LH6e2gGOX+rXlP0k=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdFty4rGAv0IYwmU7aoZNJmJkUayi+ihsXirj1Pdz5Nk7aLLT1h4GP/5zDOfP7MaNS2fa3UVhb39jcKm6Xdnb39g/Mw6O2jBKBSQtHLBJdH0nCKCctRRUj3VgQFPqMdPzxTVbvTIiQNOL3ahoTN0RDTgOKkdKWZ55Uqg8evehPkIhHNONareKZZfvSnstaBSeHMuRqeuZXfxDhJCRcYYak7Dl2rNwUCUUxI7NSP5EkRniMhqSnkaOQSDedXz+zzrUzsIJI6MeVNXd/T6QolHIa+rozRGokl2uZ+V+tl6jg2k0pjxNFOF4sChJmqcjKorAGVBCs2FQDwoLqWy08QgJhpQMr6RCc5S+vQrt+6Wi+q5cbjTyOIpzCGVTBgStowC00oQUYHuEZXuHNeDJejHfjY9FaMPKZY/gj4/MHebWT6w==</latexit>(yi '(yi))<latexit sha1_base64="OwRczwPm0a80lXMROfDF4JnscN8=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdScGNywr2Am0Ik+mkHTqZhJlJIZbiq7hxoYhb38Odb+OkzUJbfxj4+M85nDO/HzMqlW1/G4W19Y3NreJ2aWd3b//APDxqyygRmLRwxCLR9ZEkjHLSUlQx0o0FQaHPSMcf32b1zoQISSP+oNKYuCEachpQjJS2PPOkUk09etGfIBGPaMa1WsUzy/alPZe1Ck4OZcjV9Myv/iDCSUi4wgxJ2XPsWLlTJBTFjMxK/USSGOExGpKeRo5CIt3p/PqZda6dgRVEQj+urLn7e2KKQinT0NedIVIjuVzLzP9qvUQF1+6U8jhRhOPFoiBhloqsLAprQAXBiqUaEBZU32rhERIIKx1YSYfgLH95Fdr1S0fzfb3cuMnjKMIpnEEVHLiCBtxBE1qA4RGe4RXejCfjxXg3PhatBSOfOYY/Mj5/AHw5k+s=</latexit><latexit sha1_base64="OwRczwPm0a80lXMROfDF4JnscN8=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdScGNywr2Am0Ik+mkHTqZhJlJIZbiq7hxoYhb38Odb+OkzUJbfxj4+M85nDO/HzMqlW1/G4W19Y3NreJ2aWd3b//APDxqyygRmLRwxCLR9ZEkjHLSUlQx0o0FQaHPSMcf32b1zoQISSP+oNKYuCEachpQjJS2PPOkUk09etGfIBGPaMa1WsUzy/alPZe1Ck4OZcjV9Myv/iDCSUi4wgxJ2XPsWLlTJBTFjMxK/USSGOExGpKeRo5CIt3p/PqZda6dgRVEQj+urLn7e2KKQinT0NedIVIjuVzLzP9qvUQF1+6U8jhRhOPFoiBhloqsLAprQAXBiqUaEBZU32rhERIIKx1YSYfgLH95Fdr1S0fzfb3cuMnjKMIpnEEVHLiCBtxBE1qA4RGe4RXejCfjxXg3PhatBSOfOYY/Mj5/AHw5k+s=</latexit><latexit sha1_base64="OwRczwPm0a80lXMROfDF4JnscN8=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdScGNywr2Am0Ik+mkHTqZhJlJIZbiq7hxoYhb38Odb+OkzUJbfxj4+M85nDO/HzMqlW1/G4W19Y3NreJ2aWd3b//APDxqyygRmLRwxCLR9ZEkjHLSUlQx0o0FQaHPSMcf32b1zoQISSP+oNKYuCEachpQjJS2PPOkUk09etGfIBGPaMa1WsUzy/alPZe1Ck4OZcjV9Myv/iDCSUi4wgxJ2XPsWLlTJBTFjMxK/USSGOExGpKeRo5CIt3p/PqZda6dgRVEQj+urLn7e2KKQinT0NedIVIjuVzLzP9qvUQF1+6U8jhRhOPFoiBhloqsLAprQAXBiqUaEBZU32rhERIIKx1YSYfgLH95Fdr1S0fzfb3cuMnjKMIpnEEVHLiCBtxBE1qA4RGe4RXejCfjxXg3PhatBSOfOYY/Mj5/AHw5k+s=</latexit><latexit sha1_base64="OwRczwPm0a80lXMROfDF4JnscN8=">AAAB/XicbZDLSsNAFIZP6q3WW7zs3ARboQWRpBtdScGNywr2Am0Ik+mkHTqZhJlJIZbiq7hxoYhb38Odb+OkzUJbfxj4+M85nDO/HzMqlW1/G4W19Y3NreJ2aWd3b//APDxqyygRmLRwxCLR9ZEkjHLSUlQx0o0FQaHPSMcf32b1zoQISSP+oNKYuCEachpQjJS2PPOkUk09etGfIBGPaMa1WsUzy/alPZe1Ck4OZcjV9Myv/iDCSUi4wgxJ2XPsWLlTJBTFjMxK/USSGOExGpKeRo5CIt3p/PqZda6dgRVEQj+urLn7e2KKQinT0NedIVIjuVzLzP9qvUQF1+6U8jhRhOPFoiBhloqsLAprQAXBiqUaEBZU32rhERIIKx1YSYfgLH95Fdr1S0fzfb3cuMnjKMIpnEEVHLiCBtxBE1qA4RGe4RXejCfjxXg3PhatBSOfOYY/Mj5/AHw5k+s=</latexit>↵'(yi)'(xi)yixi<latexit sha1_base64="71vqPKjY+TK9gITBZZq4AVlX2HI=">AAACLHicbZBNS8NAEIY3flu/qh69LFZBD0rSix4LXjxWsLbQlDLZTpqlmw93N8US8oO8+FcE8WARr/4OtzWCtg4sPPPODLPzeongStv22FpYXFpeWV1bL21sbm3vlHf37lScSoYNFotYtjxQKHiEDc21wFYiEUJPYNMbXE3qzSFKxePoVo8S7ITQj7jPGWgjdctXRy6IJADqCrynbs+XwDJ3CDIJ+Mmoy0/PfpIHk+SZkc4M5UW/hxqOuuWKfW5Pg86DU0CFFFHvll/cXszSECPNBCjVduxEdzKQmjOBeclNFSbABtDHtsEIQlSdbHpsTo+N0qN+LM2LNJ2qvycyCJUahZ7pDEEHarY2Ef+rtVPtX3YyHiWpxoh9L/JTQXVMJ87RHpfItBgZACa5+StlARi/tPG3ZExwZk+eh7vquWP4plqp1Qo71sgBOSQnxCEXpEauSZ00CCOP5Jm8kbH1ZL1a79bHd+uCVczskz9hfX4BfAOnuQ==</latexit><latexit sha1_base64="71vqPKjY+TK9gITBZZq4AVlX2HI=">AAACLHicbZBNS8NAEIY3flu/qh69LFZBD0rSix4LXjxWsLbQlDLZTpqlmw93N8US8oO8+FcE8WARr/4OtzWCtg4sPPPODLPzeongStv22FpYXFpeWV1bL21sbm3vlHf37lScSoYNFotYtjxQKHiEDc21wFYiEUJPYNMbXE3qzSFKxePoVo8S7ITQj7jPGWgjdctXRy6IJADqCrynbs+XwDJ3CDIJ+Mmoy0/PfpIHk+SZkc4M5UW/hxqOuuWKfW5Pg86DU0CFFFHvll/cXszSECPNBCjVduxEdzKQmjOBeclNFSbABtDHtsEIQlSdbHpsTo+N0qN+LM2LNJ2qvycyCJUahZ7pDEEHarY2Ef+rtVPtX3YyHiWpxoh9L/JTQXVMJ87RHpfItBgZACa5+StlARi/tPG3ZExwZk+eh7vquWP4plqp1Qo71sgBOSQnxCEXpEauSZ00CCOP5Jm8kbH1ZL1a79bHd+uCVczskz9hfX4BfAOnuQ==</latexit><latexit sha1_base64="71vqPKjY+TK9gITBZZq4AVlX2HI=">AAACLHicbZBNS8NAEIY3flu/qh69LFZBD0rSix4LXjxWsLbQlDLZTpqlmw93N8US8oO8+FcE8WARr/4OtzWCtg4sPPPODLPzeongStv22FpYXFpeWV1bL21sbm3vlHf37lScSoYNFotYtjxQKHiEDc21wFYiEUJPYNMbXE3qzSFKxePoVo8S7ITQj7jPGWgjdctXRy6IJADqCrynbs+XwDJ3CDIJ+Mmoy0/PfpIHk+SZkc4M5UW/hxqOuuWKfW5Pg86DU0CFFFHvll/cXszSECPNBCjVduxEdzKQmjOBeclNFSbABtDHtsEIQlSdbHpsTo+N0qN+LM2LNJ2qvycyCJUahZ7pDEEHarY2Ef+rtVPtX3YyHiWpxoh9L/JTQXVMJ87RHpfItBgZACa5+StlARi/tPG3ZExwZk+eh7vquWP4plqp1Qo71sgBOSQnxCEXpEauSZ00CCOP5Jm8kbH1ZL1a79bHd+uCVczskz9hfX4BfAOnuQ==</latexit><latexit sha1_base64="71vqPKjY+TK9gITBZZq4AVlX2HI=">AAACLHicbZBNS8NAEIY3flu/qh69LFZBD0rSix4LXjxWsLbQlDLZTpqlmw93N8US8oO8+FcE8WARr/4OtzWCtg4sPPPODLPzeongStv22FpYXFpeWV1bL21sbm3vlHf37lScSoYNFotYtjxQKHiEDc21wFYiEUJPYNMbXE3qzSFKxePoVo8S7ITQj7jPGWgjdctXRy6IJADqCrynbs+XwDJ3CDIJ+Mmoy0/PfpIHk+SZkc4M5UW/hxqOuuWKfW5Pg86DU0CFFFHvll/cXszSECPNBCjVduxEdzKQmjOBeclNFSbABtDHtsEIQlSdbHpsTo+N0qN+LM2LNJ2qvycyCJUahZ7pDEEHarY2Ef+rtVPtX3YyHiWpxoh9L/JTQXVMJ87RHpfItBgZACa5+StlARi/tPG3ZExwZk+eh7vquWP4plqp1Qo71sgBOSQnxCEXpEauSZ00CCOP5Jm8kbH1ZL1a79bHd+uCVczskz9hfX4BfAOnuQ==</latexit>xj<latexit sha1_base64="Qmcs1an4x+++o9Cy9ZfnlIj1FDM=">AAAB7HicbZA9TwJBEIbn/ET8Qi1tNoKJFbmjkZLExhITD0iAkL1lDlb29i67e0Zy4TfYWGiMrT/Izn/jAlco+CabPHlnJjvzBong2rjut7OxubW9s1vYK+4fHB4dl05OWzpOFUOfxSJWnYBqFFyib7gR2EkU0igQ2A4mN/N6+xGV5rG8N9ME+xEdSR5yRo21/MrT4KEyKJXdqrsQWQcvhzLkag5KX71hzNIIpWGCat313MT0M6oMZwJnxV6qMaFsQkfYtShphLqfLZadkUvrDEkYK/ukIQv390RGI62nUWA7I2rGerU2N/+rdVMT1vsZl0lqULLlR2EqiInJ/HIy5AqZEVMLlCludyVsTBVlxuZTtCF4qyevQ6tW9Szf1cqNeh5HAc7hAq7Ag2towC00wQcGHJ7hFd4c6bw4787HsnXDyWfO4I+czx8XRo4p</latexit><latexit sha1_base64="Qmcs1an4x+++o9Cy9ZfnlIj1FDM=">AAAB7HicbZA9TwJBEIbn/ET8Qi1tNoKJFbmjkZLExhITD0iAkL1lDlb29i67e0Zy4TfYWGiMrT/Izn/jAlco+CabPHlnJjvzBong2rjut7OxubW9s1vYK+4fHB4dl05OWzpOFUOfxSJWnYBqFFyib7gR2EkU0igQ2A4mN/N6+xGV5rG8N9ME+xEdSR5yRo21/MrT4KEyKJXdqrsQWQcvhzLkag5KX71hzNIIpWGCat313MT0M6oMZwJnxV6qMaFsQkfYtShphLqfLZadkUvrDEkYK/ukIQv390RGI62nUWA7I2rGerU2N/+rdVMT1vsZl0lqULLlR2EqiInJ/HIy5AqZEVMLlCludyVsTBVlxuZTtCF4qyevQ6tW9Szf1cqNeh5HAc7hAq7Ag2towC00wQcGHJ7hFd4c6bw4787HsnXDyWfO4I+czx8XRo4p</latexit><latexit sha1_base64="Qmcs1an4x+++o9Cy9ZfnlIj1FDM=">AAAB7HicbZA9TwJBEIbn/ET8Qi1tNoKJFbmjkZLExhITD0iAkL1lDlb29i67e0Zy4TfYWGiMrT/Izn/jAlco+CabPHlnJjvzBong2rjut7OxubW9s1vYK+4fHB4dl05OWzpOFUOfxSJWnYBqFFyib7gR2EkU0igQ2A4mN/N6+xGV5rG8N9ME+xEdSR5yRo21/MrT4KEyKJXdqrsQWQcvhzLkag5KX71hzNIIpWGCat313MT0M6oMZwJnxV6qMaFsQkfYtShphLqfLZadkUvrDEkYK/ukIQv390RGI62nUWA7I2rGerU2N/+rdVMT1vsZl0lqULLlR2EqiInJ/HIy5AqZEVMLlCludyVsTBVlxuZTtCF4qyevQ6tW9Szf1cqNeh5HAc7hAq7Ag2towC00wQcGHJ7hFd4c6bw4787HsnXDyWfO4I+czx8XRo4p</latexit><latexit sha1_base64="Qmcs1an4x+++o9Cy9ZfnlIj1FDM=">AAAB7HicbZA9TwJBEIbn/ET8Qi1tNoKJFbmjkZLExhITD0iAkL1lDlb29i67e0Zy4TfYWGiMrT/Izn/jAlco+CabPHlnJjvzBong2rjut7OxubW9s1vYK+4fHB4dl05OWzpOFUOfxSJWnYBqFFyib7gR2EkU0igQ2A4mN/N6+xGV5rG8N9ME+xEdSR5yRo21/MrT4KEyKJXdqrsQWQcvhzLkag5KX71hzNIIpWGCat313MT0M6oMZwJnxV6qMaFsQkfYtShphLqfLZadkUvrDEkYK/ukIQv390RGI62nUWA7I2rGerU2N/+rdVMT1vsZl0lqULLlR2EqiInJ/HIy5AqZEVMLlCludyVsTBVlxuZTtCF4qyevQ6tW9Szf1cqNeh5HAc7hAq7Ag2towC00wQcGHJ7hFd4c6bw4787HsnXDyWfO4I+czx8XRo4p</latexit>yj<latexit sha1_base64="GXeTI3hCKBcAvNjSYFW2ns3xRoM=">AAAB7HicbZDNTgIxFIVv8Q/xD3XpphFMXJEZNrIkceMSEwdIYEI6pQOVTmfSdkwmE57BjQuNcesDufNtLDALBU/S5Mu596b3niARXBvH+Ualre2d3b3yfuXg8Oj4pHp61tVxqijzaCxi1Q+IZoJL5hluBOsnipEoEKwXzG4X9d4TU5rH8sFkCfMjMpE85JQYa3n1bPRYH1VrTsNZCm+CW0ANCnVG1a/hOKZpxKShgmg9cJ3E+DlRhlPB5pVhqllC6IxM2MCiJBHTfr5cdo6vrDPGYazskwYv3d8TOYm0zqLAdkbETPV6bWH+VxukJmz5OZdJapikq4/CVGAT48XleMwVo0ZkFghV3O6K6ZQoQo3Np2JDcNdP3oRus+Favm/W2q0ijjJcwCVcgws30IY76IAHFDg8wyu8IYle0Dv6WLWWUDFzDn+EPn8AGM2OKg==</latexit><latexit sha1_base64="GXeTI3hCKBcAvNjSYFW2ns3xRoM=">AAAB7HicbZDNTgIxFIVv8Q/xD3XpphFMXJEZNrIkceMSEwdIYEI6pQOVTmfSdkwmE57BjQuNcesDufNtLDALBU/S5Mu596b3niARXBvH+Ualre2d3b3yfuXg8Oj4pHp61tVxqijzaCxi1Q+IZoJL5hluBOsnipEoEKwXzG4X9d4TU5rH8sFkCfMjMpE85JQYa3n1bPRYH1VrTsNZCm+CW0ANCnVG1a/hOKZpxKShgmg9cJ3E+DlRhlPB5pVhqllC6IxM2MCiJBHTfr5cdo6vrDPGYazskwYv3d8TOYm0zqLAdkbETPV6bWH+VxukJmz5OZdJapikq4/CVGAT48XleMwVo0ZkFghV3O6K6ZQoQo3Np2JDcNdP3oRus+Favm/W2q0ijjJcwCVcgws30IY76IAHFDg8wyu8IYle0Dv6WLWWUDFzDn+EPn8AGM2OKg==</latexit><latexit sha1_base64="GXeTI3hCKBcAvNjSYFW2ns3xRoM=">AAAB7HicbZDNTgIxFIVv8Q/xD3XpphFMXJEZNrIkceMSEwdIYEI6pQOVTmfSdkwmE57BjQuNcesDufNtLDALBU/S5Mu596b3niARXBvH+Ualre2d3b3yfuXg8Oj4pHp61tVxqijzaCxi1Q+IZoJL5hluBOsnipEoEKwXzG4X9d4TU5rH8sFkCfMjMpE85JQYa3n1bPRYH1VrTsNZCm+CW0ANCnVG1a/hOKZpxKShgmg9cJ3E+DlRhlPB5pVhqllC6IxM2MCiJBHTfr5cdo6vrDPGYazskwYv3d8TOYm0zqLAdkbETPV6bWH+VxukJmz5OZdJapikq4/CVGAT48XleMwVo0ZkFghV3O6K6ZQoQo3Np2JDcNdP3oRus+Favm/W2q0ijjJcwCVcgws30IY76IAHFDg8wyu8IYle0Dv6WLWWUDFzDn+EPn8AGM2OKg==</latexit><latexit sha1_base64="GXeTI3hCKBcAvNjSYFW2ns3xRoM=">AAAB7HicbZDNTgIxFIVv8Q/xD3XpphFMXJEZNrIkceMSEwdIYEI6pQOVTmfSdkwmE57BjQuNcesDufNtLDALBU/S5Mu596b3niARXBvH+Ualre2d3b3yfuXg8Oj4pHp61tVxqijzaCxi1Q+IZoJL5hluBOsnipEoEKwXzG4X9d4TU5rH8sFkCfMjMpE85JQYa3n1bPRYH1VrTsNZCm+CW0ANCnVG1a/hOKZpxKShgmg9cJ3E+DlRhlPB5pVhqllC6IxM2MCiJBHTfr5cdo6vrDPGYazskwYv3d8TOYm0zqLAdkbETPV6bWH+VxukJmz5OZdJapikq4/CVGAT48XleMwVo0ZkFghV3O6K6ZQoQo3Np2JDcNdP3oRus+Favm/W2q0ijjJcwCVcgws30IY76IAHFDg8wyu8IYle0Dv6WLWWUDFzDn+EPn8AGM2OKg==</latexit>'(xj)<latexit sha1_base64="oI6G4J0ZwbR7XFQ0Ze3vRM+PxDI=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1mSuHGJiVwSqGQ6TGFkOm1mpihpeA83LjTGre/izrdxCl0o+CeTfPnPOTlnfi/iTGnb/rZyG5tb2zv53cLe/sHhUfH4pK3CWBLaIiEPZdfDinImaEszzWk3khQHHqcdb3Kd1jtTKhULxZ2eRdQN8EgwnxGsjXVf7k+xjMas8jR4uCwPiiW7ai+E1sHJoASZmoPiV38YkjigQhOOleo5dqTdBEvNCKfzQj9WNMJkgke0Z1DggCo3WVw9RxfGGSI/lOYJjRbu74kEB0rNAs90BliP1WotNf+r9WLt192EiSjWVJDlIj/mSIcojQANmaRE85kBTCQztyIyxhITbYIqmBCc1S+vQ7tWdQzf1kqNehZHHs7gHCrgwBU04Aaa0AICEp7hFd6sR+vFerc+lq05K5s5hT+yPn8AYQKRug==</latexit><latexit sha1_base64="oI6G4J0ZwbR7XFQ0Ze3vRM+PxDI=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1mSuHGJiVwSqGQ6TGFkOm1mpihpeA83LjTGre/izrdxCl0o+CeTfPnPOTlnfi/iTGnb/rZyG5tb2zv53cLe/sHhUfH4pK3CWBLaIiEPZdfDinImaEszzWk3khQHHqcdb3Kd1jtTKhULxZ2eRdQN8EgwnxGsjXVf7k+xjMas8jR4uCwPiiW7ai+E1sHJoASZmoPiV38YkjigQhOOleo5dqTdBEvNCKfzQj9WNMJkgke0Z1DggCo3WVw9RxfGGSI/lOYJjRbu74kEB0rNAs90BliP1WotNf+r9WLt192EiSjWVJDlIj/mSIcojQANmaRE85kBTCQztyIyxhITbYIqmBCc1S+vQ7tWdQzf1kqNehZHHs7gHCrgwBU04Aaa0AICEp7hFd6sR+vFerc+lq05K5s5hT+yPn8AYQKRug==</latexit><latexit sha1_base64="oI6G4J0ZwbR7XFQ0Ze3vRM+PxDI=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1mSuHGJiVwSqGQ6TGFkOm1mpihpeA83LjTGre/izrdxCl0o+CeTfPnPOTlnfi/iTGnb/rZyG5tb2zv53cLe/sHhUfH4pK3CWBLaIiEPZdfDinImaEszzWk3khQHHqcdb3Kd1jtTKhULxZ2eRdQN8EgwnxGsjXVf7k+xjMas8jR4uCwPiiW7ai+E1sHJoASZmoPiV38YkjigQhOOleo5dqTdBEvNCKfzQj9WNMJkgke0Z1DggCo3WVw9RxfGGSI/lOYJjRbu74kEB0rNAs90BliP1WotNf+r9WLt192EiSjWVJDlIj/mSIcojQANmaRE85kBTCQztyIyxhITbYIqmBCc1S+vQ7tWdQzf1kqNehZHHs7gHCrgwBU04Aaa0AICEp7hFd6sR+vFerc+lq05K5s5hT+yPn8AYQKRug==</latexit><latexit sha1_base64="oI6G4J0ZwbR7XFQ0Ze3vRM+PxDI=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1mSuHGJiVwSqGQ6TGFkOm1mpihpeA83LjTGre/izrdxCl0o+CeTfPnPOTlnfi/iTGnb/rZyG5tb2zv53cLe/sHhUfH4pK3CWBLaIiEPZdfDinImaEszzWk3khQHHqcdb3Kd1jtTKhULxZ2eRdQN8EgwnxGsjXVf7k+xjMas8jR4uCwPiiW7ai+E1sHJoASZmoPiV38YkjigQhOOleo5dqTdBEvNCKfzQj9WNMJkgke0Z1DggCo3WVw9RxfGGSI/lOYJjRbu74kEB0rNAs90BliP1WotNf+r9WLt192EiSjWVJDlIj/mSIcojQANmaRE85kBTCQztyIyxhITbYIqmBCc1S+vQ7tWdQzf1kqNehZHHs7gHCrgwBU04Aaa0AICEp7hFd6sR+vFerc+lq05K5s5hT+yPn8AYQKRug==</latexit>'(yj)<latexit sha1_base64="I7R4a32uvuL7gDjagUmI6UlCRsU=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1iSuHGJiVwSqGQ6TGFkOm1mppim4T3cuNAYt76LO9/GAbpQ8E8m+fKfc3LO/F7EmdK2/W3ltrZ3dvfy+4WDw6Pjk+LpWUeFsSS0TUIeyp6HFeVM0LZmmtNeJCkOPE673vRmUe/OqFQsFPc6iagb4LFgPiNYG+uhPJhhGU1YJRk+XpeHxZJdtZdCm+BkUIJMrWHxazAKSRxQoQnHSvUdO9JuiqVmhNN5YRArGmEyxWPaNyhwQJWbLq+eoyvjjJAfSvOERkv390SKA6WSwDOdAdYTtV5bmP/V+rH2G27KRBRrKshqkR9zpEO0iACNmKRE88QAJpKZWxGZYImJNkEVTAjO+pc3oVOrOobvaqVmI4sjDxdwCRVwoA5NuIUWtIGAhGd4hTfryXqx3q2PVWvOymbO4Y+szx9iipG7</latexit><latexit sha1_base64="I7R4a32uvuL7gDjagUmI6UlCRsU=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1iSuHGJiVwSqGQ6TGFkOm1mppim4T3cuNAYt76LO9/GAbpQ8E8m+fKfc3LO/F7EmdK2/W3ltrZ3dvfy+4WDw6Pjk+LpWUeFsSS0TUIeyp6HFeVM0LZmmtNeJCkOPE673vRmUe/OqFQsFPc6iagb4LFgPiNYG+uhPJhhGU1YJRk+XpeHxZJdtZdCm+BkUIJMrWHxazAKSRxQoQnHSvUdO9JuiqVmhNN5YRArGmEyxWPaNyhwQJWbLq+eoyvjjJAfSvOERkv390SKA6WSwDOdAdYTtV5bmP/V+rH2G27KRBRrKshqkR9zpEO0iACNmKRE88QAJpKZWxGZYImJNkEVTAjO+pc3oVOrOobvaqVmI4sjDxdwCRVwoA5NuIUWtIGAhGd4hTfryXqx3q2PVWvOymbO4Y+szx9iipG7</latexit><latexit sha1_base64="I7R4a32uvuL7gDjagUmI6UlCRsU=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1iSuHGJiVwSqGQ6TGFkOm1mppim4T3cuNAYt76LO9/GAbpQ8E8m+fKfc3LO/F7EmdK2/W3ltrZ3dvfy+4WDw6Pjk+LpWUeFsSS0TUIeyp6HFeVM0LZmmtNeJCkOPE673vRmUe/OqFQsFPc6iagb4LFgPiNYG+uhPJhhGU1YJRk+XpeHxZJdtZdCm+BkUIJMrWHxazAKSRxQoQnHSvUdO9JuiqVmhNN5YRArGmEyxWPaNyhwQJWbLq+eoyvjjJAfSvOERkv390SKA6WSwDOdAdYTtV5bmP/V+rH2G27KRBRrKshqkR9zpEO0iACNmKRE88QAJpKZWxGZYImJNkEVTAjO+pc3oVOrOobvaqVmI4sjDxdwCRVwoA5NuIUWtIGAhGd4hTfryXqx3q2PVWvOymbO4Y+szx9iipG7</latexit><latexit sha1_base64="I7R4a32uvuL7gDjagUmI6UlCRsU=">AAAB9XicbZDLTsJAFIZP8YZ4Q126mQgmuCEtG1iSuHGJiVwSqGQ6TGFkOm1mppim4T3cuNAYt76LO9/GAbpQ8E8m+fKfc3LO/F7EmdK2/W3ltrZ3dvfy+4WDw6Pjk+LpWUeFsSS0TUIeyp6HFeVM0LZmmtNeJCkOPE673vRmUe/OqFQsFPc6iagb4LFgPiNYG+uhPJhhGU1YJRk+XpeHxZJdtZdCm+BkUIJMrWHxazAKSRxQoQnHSvUdO9JuiqVmhNN5YRArGmEyxWPaNyhwQJWbLq+eoyvjjJAfSvOERkv390SKA6WSwDOdAdYTtV5bmP/V+rH2G27KRBRrKshqkR9zpEO0iACNmKRE88QAJpKZWxGZYImJNkEVTAjO+pc3oVOrOobvaqVmI4sjDxdwCRVwoA5NuIUWtIGAhGd4hTfryXqx3q2PVWvOymbO4Y+szx9iipG7</latexit>nonnegative combination(x)=264'(x1)...'(xn)375<latexit sha1_base64="guxvl0q2bU+xVoPoliEsRkzbL0M=">AAACNXicbZDLSgMxFIYzXmu9VV26CVbBbmSmG7sRCm5cuKhgL9CUksmctsFMZkgypWXoS7nxPVzpwoUibn0F04uirQcCH/9/Djnn92PBtXHdZ2dpeWV1bT2zkd3c2t7Zze3t13SUKAZVFolINXyqQXAJVcONgEasgIa+gLp/dzn2631Qmkfy1gxjaIW0K3mHM2qs1M5dH5O4x08HhQviQ5fL1A+pUXwwIn2qJk7bK2BCMOkHkdFT+nZkgYAMfkaO27m8e+ZOCi+CN4M8mlWlnXskQcSSEKRhgmrd9NzYtFKqDGcCRlmSaIgpu6NdaFqUNATdSidXj/CJVQLciZR90uCJ+nsipaHWw9C3nXbBnp73xuJ/XjMxnVIr5TJODEg2/aiTCGwiPI4QB1wBM2JogTLF7a6Y9aiizNigszYEb/7kRagVzzzLN8V8uTSLI4MO0RE6RR46R2V0hSqoihi6R0/oFb05D86L8+58TFuXnNnMAfpTzucX/a+riw==</latexit><latexit sha1_base64="guxvl0q2bU+xVoPoliEsRkzbL0M=">AAACNXicbZDLSgMxFIYzXmu9VV26CVbBbmSmG7sRCm5cuKhgL9CUksmctsFMZkgypWXoS7nxPVzpwoUibn0F04uirQcCH/9/Djnn92PBtXHdZ2dpeWV1bT2zkd3c2t7Zze3t13SUKAZVFolINXyqQXAJVcONgEasgIa+gLp/dzn2631Qmkfy1gxjaIW0K3mHM2qs1M5dH5O4x08HhQviQ5fL1A+pUXwwIn2qJk7bK2BCMOkHkdFT+nZkgYAMfkaO27m8e+ZOCi+CN4M8mlWlnXskQcSSEKRhgmrd9NzYtFKqDGcCRlmSaIgpu6NdaFqUNATdSidXj/CJVQLciZR90uCJ+nsipaHWw9C3nXbBnp73xuJ/XjMxnVIr5TJODEg2/aiTCGwiPI4QB1wBM2JogTLF7a6Y9aiizNigszYEb/7kRagVzzzLN8V8uTSLI4MO0RE6RR46R2V0hSqoihi6R0/oFb05D86L8+58TFuXnNnMAfpTzucX/a+riw==</latexit><latexit sha1_base64="guxvl0q2bU+xVoPoliEsRkzbL0M=">AAACNXicbZDLSgMxFIYzXmu9VV26CVbBbmSmG7sRCm5cuKhgL9CUksmctsFMZkgypWXoS7nxPVzpwoUibn0F04uirQcCH/9/Djnn92PBtXHdZ2dpeWV1bT2zkd3c2t7Zze3t13SUKAZVFolINXyqQXAJVcONgEasgIa+gLp/dzn2631Qmkfy1gxjaIW0K3mHM2qs1M5dH5O4x08HhQviQ5fL1A+pUXwwIn2qJk7bK2BCMOkHkdFT+nZkgYAMfkaO27m8e+ZOCi+CN4M8mlWlnXskQcSSEKRhgmrd9NzYtFKqDGcCRlmSaIgpu6NdaFqUNATdSidXj/CJVQLciZR90uCJ+nsipaHWw9C3nXbBnp73xuJ/XjMxnVIr5TJODEg2/aiTCGwiPI4QB1wBM2JogTLF7a6Y9aiizNigszYEb/7kRagVzzzLN8V8uTSLI4MO0RE6RR46R2V0hSqoihi6R0/oFb05D86L8+58TFuXnNnMAfpTzucX/a+riw==</latexit><latexit sha1_base64="guxvl0q2bU+xVoPoliEsRkzbL0M=">AAACNXicbZDLSgMxFIYzXmu9VV26CVbBbmSmG7sRCm5cuKhgL9CUksmctsFMZkgypWXoS7nxPVzpwoUibn0F04uirQcCH/9/Djnn92PBtXHdZ2dpeWV1bT2zkd3c2t7Zze3t13SUKAZVFolINXyqQXAJVcONgEasgIa+gLp/dzn2631Qmkfy1gxjaIW0K3mHM2qs1M5dH5O4x08HhQviQ5fL1A+pUXwwIn2qJk7bK2BCMOkHkdFT+nZkgYAMfkaO27m8e+ZOCi+CN4M8mlWlnXskQcSSEKRhgmrd9NzYtFKqDGcCRlmSaIgpu6NdaFqUNATdSidXj/CJVQLciZR90uCJ+nsipaHWw9C3nXbBnp73xuJ/XjMxnVIr5TJODEg2/aiTCGwiPI4QB1wBM2JogTLF7a6Y9aiizNigszYEb/7kRagVzzzLN8V8uTSLI4MO0RE6RR46R2V0hSqoihi6R0/oFb05D86L8+58TFuXnNnMAfpTzucX/a+riw==</latexit>xy(x)(y)>2↵T(↵+)T(↵+)T2Txy(x)(y)0<latexit sha1_base64="xE4TKIfpLj8g4gjosp4+9y8sOeo=">AAACtHiclVHBThsxEPVuaUtDW9Jy7MUiFCWqgnZzaDkicekRpIQgxSGddSaJhdfr2t4qq1W+sLfe+Jt6NxGCwIWRLD+9N288nkm0FNZF0V0Qvtp5/ebt7rvG3vsPH/ebnz5f2Sw3HAc8k5m5TsCiFAoHTjiJ19ogpInEYXJ7XunDP2isyFTfFRrHKcyVmAkOzlOT5t8jluBcqDJJwRmxXC27BWWMMr0Q7WWnW99Fh6Ga3qfcMJdpuuWj3R4DqRdQCQ5onx7T9pr5VjOdflV3mzr2vv6j6i/vh7I5/qbR0aTZik6iOuhTEG9Ai2ziYtL8x6YZz1NUjkuwdhRH2o1LME5wiasGyy1q4Lcwx5GHClK047Ie+op+9cyUzjLjj3K0Zh86SkitLdLEZ/o+F3Zbq8jntFHuZqfjUiidO1R8/dAsl9RltNognQqD3MnCA+BG+F4pX4AB7vyeG34I8faXn4Kr3kns8WWvdXa6Gccu+UIOSZvE5Ac5Iz/JBRkQHsTBMPgVQPg9ZCEPcZ0aBhvPAXkUofoPmdvT4g==</latexit><latexit sha1_base64="xE4TKIfpLj8g4gjosp4+9y8sOeo=">AAACtHiclVHBThsxEPVuaUtDW9Jy7MUiFCWqgnZzaDkicekRpIQgxSGddSaJhdfr2t4qq1W+sLfe+Jt6NxGCwIWRLD+9N288nkm0FNZF0V0Qvtp5/ebt7rvG3vsPH/ebnz5f2Sw3HAc8k5m5TsCiFAoHTjiJ19ogpInEYXJ7XunDP2isyFTfFRrHKcyVmAkOzlOT5t8jluBcqDJJwRmxXC27BWWMMr0Q7WWnW99Fh6Ga3qfcMJdpuuWj3R4DqRdQCQ5onx7T9pr5VjOdflV3mzr2vv6j6i/vh7I5/qbR0aTZik6iOuhTEG9Ai2ziYtL8x6YZz1NUjkuwdhRH2o1LME5wiasGyy1q4Lcwx5GHClK047Ie+op+9cyUzjLjj3K0Zh86SkitLdLEZ/o+F3Zbq8jntFHuZqfjUiidO1R8/dAsl9RltNognQqD3MnCA+BG+F4pX4AB7vyeG34I8faXn4Kr3kns8WWvdXa6Gccu+UIOSZvE5Ac5Iz/JBRkQHsTBMPgVQPg9ZCEPcZ0aBhvPAXkUofoPmdvT4g==</latexit><latexit sha1_base64="xE4TKIfpLj8g4gjosp4+9y8sOeo=">AAACtHiclVHBThsxEPVuaUtDW9Jy7MUiFCWqgnZzaDkicekRpIQgxSGddSaJhdfr2t4qq1W+sLfe+Jt6NxGCwIWRLD+9N288nkm0FNZF0V0Qvtp5/ebt7rvG3vsPH/ebnz5f2Sw3HAc8k5m5TsCiFAoHTjiJ19ogpInEYXJ7XunDP2isyFTfFRrHKcyVmAkOzlOT5t8jluBcqDJJwRmxXC27BWWMMr0Q7WWnW99Fh6Ga3qfcMJdpuuWj3R4DqRdQCQ5onx7T9pr5VjOdflV3mzr2vv6j6i/vh7I5/qbR0aTZik6iOuhTEG9Ai2ziYtL8x6YZz1NUjkuwdhRH2o1LME5wiasGyy1q4Lcwx5GHClK047Ie+op+9cyUzjLjj3K0Zh86SkitLdLEZ/o+F3Zbq8jntFHuZqfjUiidO1R8/dAsl9RltNognQqD3MnCA+BG+F4pX4AB7vyeG34I8faXn4Kr3kns8WWvdXa6Gccu+UIOSZvE5Ac5Iz/JBRkQHsTBMPgVQPg9ZCEPcZ0aBhvPAXkUofoPmdvT4g==</latexit><latexit sha1_base64="xE4TKIfpLj8g4gjosp4+9y8sOeo=">AAACtHiclVHBThsxEPVuaUtDW9Jy7MUiFCWqgnZzaDkicekRpIQgxSGddSaJhdfr2t4qq1W+sLfe+Jt6NxGCwIWRLD+9N288nkm0FNZF0V0Qvtp5/ebt7rvG3vsPH/ebnz5f2Sw3HAc8k5m5TsCiFAoHTjiJ19ogpInEYXJ7XunDP2isyFTfFRrHKcyVmAkOzlOT5t8jluBcqDJJwRmxXC27BWWMMr0Q7WWnW99Fh6Ga3qfcMJdpuuWj3R4DqRdQCQ5onx7T9pr5VjOdflV3mzr2vv6j6i/vh7I5/qbR0aTZik6iOuhTEG9Ai2ziYtL8x6YZz1NUjkuwdhRH2o1LME5wiasGyy1q4Lcwx5GHClK047Ie+op+9cyUzjLjj3K0Zh86SkitLdLEZ/o+F3Zbq8jntFHuZqfjUiidO1R8/dAsl9RltNognQqD3MnCA+BG+F4pX4AB7vyeG34I8faXn4Kr3kns8WWvdXa6Gccu+UIOSZvE5Ac5Iz/JBRkQHsTBMPgVQPg9ZCEPcZ0aBhvPAXkUofoPmdvT4g==</latexit>where x = [x0(cid:62) x1(cid:62) ··· x(cid:96)(cid:62)](cid:62) is the concatenation of the input and the activation values  and the
matrices b  A  B and C are given by [13]

W 0
0
0 W 1
...
...

0

0

. . .

0
In2
...

0

. . .
. . .
...
. . .

0
0

...
In(cid:96)

  

(13)

A =
C =(cid:2)0

...

...

0
0

0
0

0
0

. . .
. . .
...
. . . W (cid:96)−1

   B =
In1
0
...
...
b(cid:96)−1(cid:62)(cid:105)(cid:62) .
0 W (cid:96)(cid:3)   b =(cid:104)b0(cid:62) ···

0

0

0

The particular representation in (12) facilitates the extension of LipSDP to multiple layers  as stated
in the following theorem.
Theorem 2 (Lipschitz certiﬁcates for multi-layer neural networks) Consider an (cid:96)-layer fully
connected neural network described by (2). Let n = (cid:80)(cid:96)
k=1 nk be the total number of hidden
neurons and suppose the activation functions are slope-restricted in the sector [α  β]. Deﬁne Tn as in
(8). Deﬁne A and B as in (13). Consider the matrix inequality
B(cid:21) +
−ρIn0
−2T (cid:21)(cid:20)A
M (ρ  T ) =(cid:20)A
If (14) is satisﬁed for some (ρ  T ) ∈ R+ × Tn  then ||f (x) − f (y)||2 ≤ √ρ||x − y||2  ∀x  y ∈ Rn0.
In a similar way to the single-layer case  we can ﬁnd the best bound on the Lipschitz constant by
solving the SDP in (11) with M (ρ  T ) deﬁned as in (14).
Remark 1 We have only considered the (cid:96)2 norm in our exposition. By using the inequality (cid:107)x(cid:107)p ≤
p− 1

B(cid:21)(cid:62)(cid:20) −2αβT

 (cid:22) 0.

. . .
. . .
...
. . .

(W (cid:96))(cid:62)W (cid:96)

(α + β)T

(α + β)T

0
0
...
0

(14)

0
...
0

0
0
...

q (cid:107)x(cid:107)q  the (cid:96)2-Lipschitz bound implies

n

1

n−( 1

p− 1

2 )(cid:107)f (y) − f (x)(cid:107)p ≤ (cid:107)f (y) − f (x)(cid:107)2 ≤ L2(cid:107)y − x(cid:107)2 ≤ n

1

2− 1

q L2(cid:107)y − x(cid:107)q 

or  equivalently  (cid:107)f (y) − f (x)(cid:107)p ≤ n
q L2 is a Lipschitz constant of f
when (cid:96)q and (cid:96)p norms are used in the input and output spaces  respectively. We can also extend our
framework to accommodate quadratic norms (cid:107)x(cid:107)P = √x(cid:62)P x  where P ∈ Sn
++.

q L2(cid:107)y − x(cid:107)q. Hence  n

1

p− 1

1

p− 1

2.5 Variants of LipSDP: reconciling accuracy and efﬁciency
In LipSDP  there are O(n2) decision variables λij  1 ≤ i  j ≤ n (λij = λji)  where n is the total
number of hidden neurons. For i (cid:54)= j  the variable λij couples the i-th and j-th hidden neuron.
For i = j  the variable λii constrains the input-output of the i-th activation function individually.
Using all these decision variables would provide the tightest convex relaxation in our formulation.
However  solving this SDP with all the decision variables included is impractical for large networks.
Nevertheless  we can consider a hierarchy of relaxations of LipSDP by removing a subset of the
decision variables. Below  we give a brief description of the efﬁciency and accuracy of each variant.
Throughout  we let n be the total number of neurons and (cid:96) the number of hidden layers.

1. LipSDP-Network imposes constraints on all possible pairs of activation functions and has

O(n2) decision variables. It is the least scalable but the most accurate method.

2. LipSDP-Neuron ignores the cross coupling constraints among different neurons and has
O(n) decision variables. It is more scalable and less accurate than LipSDP-Network. For
this case  we have T = diag(λ11 ···   λnn).
ables.
T = blkdiag(λ1In1 ···   λ(cid:96)In(cid:96)).

3. LipSDP-Layer considers only one constraint per layer  resulting in O((cid:96)) decision vari-
It is the most scalable and least accurate method. For this variant  we have

Parallel implementation by splitting. The Lipschitz constant of the composition of two or more
functions can be bounded by the product of the Lipschitz constants of the individual functions. By

6

n
500
1000
1500
2000
2500
3000

LipSDP-
Neuron

5.22
27.91
82.12
200.88
376.07
734.63

LipSDP-

Layer
2.85
17.88
58.61
146.09
245.94
473.25

(cid:96)
5
10
50
100
200
500

LipSDP-
Neuron
20.33
32.18
87.45
135.85
221.2
707.56

LipSDP-

Layer
3.41
7.06
25.88
40.39
64.90
216.49

Table 1: Computation time in seconds for evaluating
Lipschitz bounds of one-hidden-layer neural networks
with a varying number of hidden units. A plot showing
the Lipschitz constant for each network tested in this
table has been provided in the Appendix.

Table 2: Computation time in seconds for comput-
ing Lipschitz bounds of (cid:96)-hidden-layer neural net-
works with 100 activation functions per layer. For
LipSDP-Neuron and LipSDP-Layer  we split each
network up into 5-layer sub-networks.

splitting a neural network up into small sub-networks  one can ﬁrst bound the Lipschitz constant of
each sub-network and then multiply these constants together to obtain a Lipschitz constant for the
entire network. Because sub-networks do not share weights  it is possible to compute the Lipschitz
constants for each sub-network in parallel. This greatly improves the scalability of each variant of
LipSDP with respect to the total number of activation functions in the network. We remark that
this parallelization is not exclusive to our method. However  among all methods that can split the
computations across layers  our method yields more accurate bounds per split.

3 Experiments

In this section we describe several experiments that highlight the key aspects of this work. In
particular  we show empirically that our bounds are much tighter than any comparable method  we
study the impact of robust training on our Lipschitz bounds  and we analyze the scalability of our
methods.
Experimental setup. For our experiments we used MATLAB  the CVX toolbox [18] and MOSEK
[3] on a 9-core CPU with 16GB of RAM to solve the SDPs. All classiﬁers trained on MNIST used
an 80-20 train-test split.
Training procedures. Several training procedures have recently been proposed to improve the
robustness of neural network classiﬁers. Two prominent procedures are the LP-based method in [40]
and projected gradient descent (PGD) based method in [24]. We refer to these training methods as
LP-Train and PGD-Train  respectively. Both procedures take as input a parameter  that deﬁnes the
(cid:96)∞ perturbation of the training data points.
Baselines. Throughout the experiments  we will often show comparisons to the naive upper bound
. We are aware of only two methods
that bound the Lipschitz constant and can scale to fully-connected networks with more than two
hidden layers; these methods are [10]  which we will refer to as CPLip  and [37]  which is called
SeqLip. We compare the Lipschitz bounds obtained by LipSDP-Neuron  LipSDP-Layer  CPLip 
and SeqLip in Figure 2a. It is evident from this ﬁgure that the bounds from LipSDP-Neuron are
tighter than CPLip and SeqLip.
To demonstrate the scalability of the LipSDP formulations  we split a 100-hidden layer neural
network into sub-networks with six hidden layers each and computed the Lipschitz bounds using
LipSDP-Neuron and LipSDP-Layer. The results are shown in Figure 2b. Furthermore  in Tables
1 and 2  we show the computation time for scaling the LipSDP methods in the number of hidden
units per layer and in the number of layers. In particular  the largest network we tested in Table 2 had
50 000 hidden neurons; SDPLip-Neuron took approximately 12 minutes to ﬁnd a Lipschitz bound 
and SDPLip-Layer took approximately 4 minutes.
To evaluate SDPLip-Network  we coupled random pairs of hidden neurons in a one-hidden-layer
network and plotted the computation time and Lipschitz bound found by SDPLip-Network as we
increased the number of paired neurons. Our results show that as the number of coupled neurons

on the Lipschitz constant given by L2  upper =(cid:81)(cid:96)

i=0(cid:12)(cid:12)(cid:12)(cid:12)W i(cid:12)(cid:12)(cid:12)(cid:12)2

7

(a) Comparison of Lipschitz
bounds found by various methods
for ﬁve-hidden-layer networks
trained on MNIST with the Adam
optimizer. Each network had a test
accuracy above 97%.
Figure 2: Comparison of the accuracy LipSDP methods to other methods that compute the Lipschitz constant
and scalability analysis of all three SeqLip methods.

(b) Lipschitz bounds obtained by
splitting a 100-layer network into
sub-networks. Each sub-network
had six layers  and the weights
were generated randomly by sam-
pling from a normal distribution.

(c) LipSDP-Network Lipschitz
bounds and computation time for a
one-hidden-layer network with 100
neurons. The weights for this net-
work were obtained by sampling
from a normal distribution.

(a) Lipschitz bounds for a one-hidden-layer neural net-
works trained on the MNIST dataset with the Adam
optimizer and LP-Train and PGD-Train for two val-
ues of the robustness parameter . Each network
reached an accuracy of 95% or higher.

(b) Histograms showing the local robustness (in (cid:96)∞
norm) around each correctly-classiﬁed test instance
from the MNIST dataset. The neural networks had
three hidden layers with 100  50  20 neurons  respec-
tively. All classiﬁers had a test accuracy of 97%. We
used Remark 1 to convert the norm from (cid:96)2 to (cid:96)∞.

Figure 3: Analysis of impact of robust training on the Lipschitz constant and the distance to misclassiﬁcation
for networks trained on MNIST

increases  the computation time increases quadratically. This shows that while this method is the most
accurate of the three proposed LipSDP methods  it is intractable for even modestly large networks.
Impact of robust training. In Figure 3  we empirically demonstrate that the Lipschitz bound of a
neural network is directly related to the robustness of the corresponding classiﬁer. This ﬁgure shows
that LP-train and PGD-Train networks achieve lower Lipschitz bounds than standard training
procedures. Figure 3a indicates that robust training procedures yield lower Lipschitz constants than
networks trained with standard training procedures such as the Adam optimizer. Figure 3b shows
the utility of sharply estimating the Lipschitz constant; a lower value of L2 guarantees that a neural
network is more locally robust to input perturbations; see Proposition 1 in the Appendix.
In the same vein  Figure 4 shows the impact of varying the robustness parameter  used in LP-Train
and PGD-Train on the test accuracy of networks trained for a ﬁxed number of epochs and the
corresponding Lipschitz constants. In essence  these results quantify how much robustness a ﬁxed
classiﬁer can handle before accuracy plummets. Interestingly  the drops in accuracy as  increases
coincide with corresponding drops in the Lipschitz constant for both LP-Train and PGD-Train.
Robustness for different activation functions. The framework proposed in this work allows us
to examine the impact of using different activation functions on the Lipschitz constant of neural

8

Figure 4: Trade-off between accuracy and Lipschitz
constant for different values of the robustness parame-
ter used for LP-Train and PGD-Train. All networks
had one hidden layer with 50 hidden neurons.

Figure 5: Lipschitz constants for topologically identi-
cal three-hidden-layer networks with ReLU and leaky
ReLU activation functions. All classiﬁers were trained
until they reached 97% test accuracy.

networks. We trained two sets of neural networks on the MNIST dataset. The ﬁrst set used ReLU
activation functions  while the second set used leaky ReLU activations. Figure 5 shows empirically
that the networks with the leaky ReLU activation function have larger Lipschitz constants than
networks of the same architecture with the ReLU activation function.

4 Conclusions and future work

In this paper  we proposed a hierarchy of semideﬁnite programs to derive tight upper bounds on
the Lipschitz constant of feed-forward fully-connected neural networks. Some comments are in
order. First  our framework can be directly used to certify convolutional neural networks (CNNs)
by unrolling them to a large fully-connected neural network. This conversion implicitly handles the
padding and stride hyper parameters. Since the max function is convex  we can describe the max
pooling operation using incremental quadratic constraints without additional assumptions. Therefore 
in principle  LipSDP is applicable to CNNs. A future direction is to exploit the special structure of
CNNs in the resulting SDP. Second  we only considered one application of Lipschitz bounds in depth
(robustness certiﬁcation). Having an accurate upper bound on the Lipschitz constant can be useful
in domains beyond robustness analysis  such as stability analysis of feedback systems with control
policies updated by deep reinforcement learning. Furthermore  Lipschitz bounds can be utilized
during training as a heuristic to promote out-of-sample generalization [36]. We intend to pursue these
applications for future work.

References
[1] Behçet Açıkme¸se and Martin Corless. Observers for systems with nonlinearities satisfying

incremental quadratic constraints. Automatica  47(7):1339–1348  2011.

[2] Cem Anil  James Lucas  and Roger Grosse. Sorting out Lipschitz function approximation. In
Kamalika Chaudhuri and Ruslan Salakhutdinov  editors  Proceedings of the 36th International
Conference on Machine Learning  volume 97 of Proceedings of Machine Learning Research 
pages 291–301  Long Beach  California  USA  09–15 Jun 2019. PMLR.

[3] MOSEK ApS. The MOSEK optimization toolbox for MATLAB manual. Version 8.1.  2017.

[4] Anil Aswani  Humberto Gonzalez  S Shankar Sastry  and Claire Tomlin. Provably safe and

robust learning-based model predictive control. Automatica  49(5):1216–1226  2013.

[5] Radu Balan  Maneesh Singh  and Dongmian Zou. Lipschitz properties for deep convolutional

networks. arXiv preprint arXiv:1701.05217  2017.

9

[6] Peter L Bartlett  Dylan J Foster  and Matus J Telgarsky. Spectrally-normalized margin bounds
for neural networks. In Advances in Neural Information Processing Systems  pages 6240–6249 
2017.

[7] Osbert Bastani  Yani Ioannou  Leonidas Lampropoulos  Dimitrios Vytiniotis  Aditya Nori  and
Antonio Criminisi. Measuring neural net robustness with constraints. In Advances in neural
information processing systems  pages 2613–2621  2016.

[8] Felix Berkenkamp  Matteo Turchetta  Angela Schoellig  and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In Advances in neural information processing
systems  pages 908–918  2017.

[9] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press 

2004.

[10] Patrick L. Combettes and Jean-Christophe Pesquet. Lipschitz certiﬁcates for neural network

structures driven by averaged activation operators. arXiv preprint arXiv:1903.01014  2019.

[11] Dheeru Dua and Casey Graff. UCI machine learning repository  2017.

[12] Souradeep Dutta  Susmit Jha  Sriram Sankaranarayanan  and Ashish Tiwari. Output range
analysis for deep feedforward neural networks. In NASA Formal Methods Symposium  pages
121–138. Springer  2018.

[13] Mahyar Fazlyab  Manfred Morari  and George J Pappas. Safety veriﬁcation and robustness
analysis of neural networks via quadratic constraints and semideﬁnite programming. arXiv
preprint arXiv:1903.01287  2019.

[14] Mahyar Fazlyab  Alejandro Ribeiro  Manfred Morari  and Victor M Preciado. Analysis of
optimization algorithms via integral quadratic constraints: Nonstrongly convex problems. SIAM
Journal on Optimization  28(3):2654–2689  2018.

[15] Timon Gehr  Matthew Mirman  Dana Drachsler-Cohen  Petar Tsankov  Swarat Chaudhuri 
and Martin Vechev. Ai2: Safety and robustness certiﬁcation of neural networks with abstract
interpretation. In 2018 IEEE Symposium on Security and Privacy (SP)  pages 3–18. IEEE  2018.

[16] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversar-

ial examples (2014). arXiv preprint arXiv:1412.6572.

[17] Henry Gouk  Eibe Frank  Bernhard Pfahringer  and Michael Cree. Regularisation of neural

networks by enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368  2018.

[18] Michael Grant  Stephen Boyd  and Yinyu Ye. Cvx: Matlab software for disciplined convex

programming  2008.

[19] Todd Huster  Cho-Yu Jason Chiang  and Ritu Chadha. Limitations of the lipschitz constant as a
defense against adversarial examples. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases  pages 16–29. Springer  2018.

[20] Ming Jin and Javad Lavaei. Stability-certiﬁed reinforcement learning: A control-theoretic

perspective. arXiv preprint arXiv:1810.11505  2018.

[21] Matt Jordan  Justin Lewis  and Alexandros G Dimakis. Provable certiﬁcates for adversarial

examples: Fitting a ball in the union of polytopes. arXiv preprint arXiv:1903.08778  2019.

[22] Alexey Kurakin  Ian Goodfellow  and Samy Bengio. Adversarial machine learning at scale.

arXiv preprint arXiv:1611.01236  2016.

[23] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/ 

1998.

[24] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 
2017.

10

[25] Yurii Nesterov. Introductory lectures on convex optimization: A basic course  volume 87.

Springer Science & Business Media  2013.

[26] Nicolas Papernot  Patrick McDaniel  Xi Wu  Somesh Jha  and Ananthram Swami. Distillation as
a defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium
on Security and Privacy (SP)  pages 582–597. IEEE  2016.

[27] Jonathan Peck  Joris Roels  Bart Goossens  and Yvan Saeys. Lower bounds on the robustness
to adversarial perturbations. In Advances in Neural Information Processing Systems  pages
804–813  2017.

[28] Haifeng Qian and Mark N Wegman. L2-nonexpansive neural networks. arXiv preprint

arXiv:1802.07896  2018.

[29] Aditi Raghunathan  Jacob Steinhardt  and Percy Liang. Certiﬁed defenses against adversarial

examples. arXiv preprint arXiv:1801.09344  2018.

[30] Aditi Raghunathan  Jacob Steinhardt  and Percy S Liang. Semideﬁnite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems 
pages 10900–10910  2018.

[31] Wenjie Ruan  Xiaowei Huang  and Marta Kwiatkowska. Reachability analysis of deep neural

networks with provable guarantees. arXiv preprint arXiv:1805.02242  2018.

[32] Ernest K Ryu and Stephen Boyd. Primer on monotone operator methods. Appl. Comput. Math 

15(1):3–43  2016.

[33] Gagandeep Singh  Timon Gehr  Matthew Mirman  Markus Püschel  and Martin Vechev. Fast
and effective robustness certiﬁcation. In Advances in Neural Information Processing Systems 
pages 10802–10813  2018.

[34] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfel-
low  and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 
2013.

[35] Vincent Tjeng  Kai Xiao  and Russ Tedrake. Evaluating robustness of neural networks with

mixed integer programming. arXiv preprint arXiv:1711.07356  2017.

[36] Yusuke Tsuzuku  Issei Sato  and Masashi Sugiyama. Lipschitz-margin training: Scalable
In Advances in Neural

certiﬁcation of perturbation invariance for deep neural networks.
Information Processing Systems  pages 6541–6550  2018.

[37] Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and
efﬁcient estimation. In Advances in Neural Information Processing Systems  pages 3835–3844 
2018.

[38] Tsui-Wei Weng  Huan Zhang  Hongge Chen  Zhao Song  Cho-Jui Hsieh  Duane Boning 
Inderjit S Dhillon  and Luca Daniel. Towards fast computation of certiﬁed robustness for relu
networks. arXiv preprint arXiv:1804.09699  2018.

[39] Tsui-Wei Weng  Huan Zhang  Pin-Yu Chen  Jinfeng Yi  Dong Su  Yupeng Gao  Cho-Jui Hsieh 
and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory
approach. arXiv preprint arXiv:1801.10578  2018.

[40] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex
outer adversarial polytope. In Jennifer Dy and Andreas Krause  editors  Proceedings of the
35th International Conference on Machine Learning  volume 80 of Proceedings of Machine
Learning Research  pages 5286–5295  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018.
PMLR.

[41] Eric Wong  Frank Schmidt  Jan Hendrik Metzen  and J Zico Kolter. Scaling provable adversarial

defenses. In Advances in Neural Information Processing Systems  pages 8400–8409  2018.

11

[42] Huan Zhang  Tsui-Wei Weng  Pin-Yu Chen  Cho-Jui Hsieh  and Luca Daniel. Efﬁcient neural
In Advances in Neural

network robustness certiﬁcation with general activation functions.
Information Processing Systems  pages 4939–4948  2018.

[43] Stephan Zheng  Yang Song  Thomas Leung  and Ian Goodfellow. Improving the robustness of
deep neural networks via stability training. In Proceedings of the ieee conference on computer
vision and pattern recognition  pages 4480–4488  2016.

[44] Dongmian Zou  Radu Balan  and Maneesh Singh. On lipschitz bounds of general convolutional

neural networks. arXiv preprint arXiv:1808.01415  2018.

12

,Mahyar Fazlyab
Alexander Robey
Hamed Hassani
Manfred Morari
George Pappas