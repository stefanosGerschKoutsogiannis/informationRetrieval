2016,Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization,We consider the problem of jointly inferring the $M$-best diverse labelings for a binary (high-order) submodular energy of a graphical model. Recently  it was shown that this problem can be solved to a global optimum  for many practically interesting diversity measures. It was noted that the labelings are  so-called  nested.  This nestedness property also holds for labelings of a class of parametric submodular minimization problems  where different values of the global parameter $\gamma$ give rise to different solutions. The popular example of the parametric submodular minimization is the monotonic parametric max-flow problem  which is also widely used for computing multiple labelings.  As the main contribution of this work we establish a close relationship between diversity with submodular energies and the parametric submodular minimization. In particular  the joint $M$-best diverse labelings can be obtained by running a non-parametric submodular minimization (in the special case - max-flow) solver for $M$ different values of $\gamma$ in parallel  for certain diversity measures. Importantly  the values for~$\gamma$ can be computed in a closed form in advance  prior to any optimization. These theoretical results suggest two simple yet efficient algorithms for the joint $M$-best diverse problem  which outperform competitors in terms of runtime and quality of results. In particular  as we show in the paper  the new methods compute the exact $M$-best diverse labelings faster than a popular method of Batra et al.  which in some sense only obtains approximate solutions.,Joint M-Best-Diverse Labelings as a Parametric

Submodular Minimization

Alexander Kirillov1 Alexander Shekhovtsov2 Carsten Rother1 Bogdan Savchynskyy1

alexander.kirillov@tu-dresden.de

1 TU Dresden  Dresden  Germany

2 TU Graz  Graz  Austria

Abstract

We consider the problem of jointly inferring the M-best diverse labelings for a
binary (high-order) submodular energy of a graphical model. Recently  it was
shown that this problem can be solved to a global optimum  for many practically
interesting diversity measures. It was noted that the labelings are  so-called  nested.
This nestedness property also holds for labelings of a class of parametric submodu-
lar minimization problems  where different values of the global parameter γ give
rise to different solutions. The popular example of the parametric submodular
minimization is the monotonic parametric max-ﬂow problem  which is also widely
used for computing multiple labelings. As the main contribution of this work
we establish a close relationship between diversity with submodular energies and
the parametric submodular minimization. In particular  the joint M-best diverse
labelings can be obtained by running a non-parametric submodular minimization
(in the special case - max-ﬂow) solver for M different values of γ in parallel  for
certain diversity measures. Importantly  the values for γ can be computed in a
closed form in advance  prior to any optimization. These theoretical results suggest
two simple yet efﬁcient algorithms for the joint M-best diverse problem  which
outperform competitors in terms of runtime and quality of results. In particular  as
we show in the paper  the new methods compute the exact M-best diverse labelings
faster than a popular method of Batra et al.  which in some sense only obtains
approximate solutions.

1

Introduction

A variety of tasks in machine learning  computer vision and other disciplines can be formulated as
energy minimization problems  also known as Maximum-a-Posteriori (MAP) or Maximum Likelihood
(ML) estimation problems in undirected graphical models (Markov or Conditional Random Fields).
The importance of this problem is well-recognized  which can be seen by the many specialized
benchmarks [36  21] and computational challenges [10] for its solvers. This motivates the task of
ﬁnding the most probable solution. Recently  a slightly different task has gained popularity  both from
a practical and theoretical perspective. The task is not only to ﬁnd the most probable solution but
multiple diverse solutions  all with low energy  see e.g.  [4  31  22  23]. The task is referred to as the
“M-best-diverse problem” [4]  and it has been used in a variety of scenarios  such as: (a) Expressing
uncertainty of the computed solutions [33]; (b) Faster training of model parameters [16]; (c) Ranking
of inference results [40]; (d) Empirical risk minimization [32]; (e) Loss-aware optimization [1]; (f)
Using diverse proposals in a cascading framework [39  35].

This project has received funding from the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme (grant agreement No 647769). A. Shekhovtsov was supported
by ERC starting grant agreement 640156.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In this work we build on the recently proposed formulation of [22] for the M-best-diverse problem. In
this formulation all M conﬁgurations are inferred jointly  contrary to the well-known method [4  31] 
where a sequential  greedy procedure is used. Hence  we term it “joint M-best-diverse problem”. As
shown in [22  23]  the joint approach qualitatively outperforms the sequential approach [4  31] in a
number of applications. This is explained by the fact that the sequential method [4] can be considered
as an approximate and greedy optimization technique for solving the joint M-best-diverse problem.
While the joint approach is superior with respect to quality of its results  it is inferior to the sequential
method [4] with respect to runtime. For the case of binary submodular energies  the approximate
solver in [22] and the exact solver in [23] are several times slower than the sequential technique [4]
for a normally sized image. Obviously  this is a major limitation when using it in a practical setting.
Furthermore  the difference in runtime grows with the number M of conﬁgurations.
In this work  we show that in case of binary submodular energies an exact solution to the joint M-
best-diverse problem can be obtained signiﬁcantly faster than the approximate one with the sequential
method [4]. Moreover  the difference in runtime grows with the number M of conﬁgurations.
Related work
The importance of the considered problem can be justiﬁed by the fact that a procedure of computing
M-best solutions to discrete optimization problems was proposed over 40 years ago  in [28]. Later 
more efﬁcient specialized procedures were introduced for MAP-inference on a tree [34]  junction-
trees [30] and general graphical models [41  13  3]. However  such methods are however not suited
for the scenario where diversity of the solutions is required  since they do not enforce it explicitly.
Structural Determinant Point Processes [27] is a tool for modelling probability distributions of
structured models. Unfortunately  an efﬁcient sampling procedure to obtain diverse conﬁgurations is
feasible only for tree-structured graphical models. The recently proposed algorithm [8] to ﬁnd M
best modes of a distribution is also limited to chains and junction chains of bounded width.
Training of M independent graphical models to produce multiple diverse solutions was proposed
in [15]  and was further explored in [17  9]. In contrast  we assume a single ﬁxed model where
conﬁgurations with low energy (hopefully) correspond to the desired output.
The work of [4] deﬁnes the M-best-diverse problem  and proposes a solver for it. However  the
diversity of the solutions is deﬁned sequentially  with respect to already extracted labelings. In
contrast to [4]  the work [22] deﬁned the “joint M-best-diverse problem” as an optimization problem
of a single joint energy. The most related work to ours is [23]  where an efﬁcient method for the
joint M-best-diverse problem was proposed for submodular energies. The method is based on the
fact that for submodular energies and a family of diversity measures (which includes e.g.  Hamming
distance) the set of M diverse solutions can be totally ordered with respect to the partial labeling
order. In the binary labeling case  the M-best-diverse solutions form a nested set. However  although
the method [23] is a considerably more efﬁcient way to solve the problem  compared to the general
algorithm proposed in [22]  it is still considerably slower than the sequential method [4]. Furthermore 
the runtime difference grows with the number M of conﬁgurations.
Interestingly  the above-mentioned “nestedness property” is also fulﬁlled by minimizers of a para-
metric submodular minimization problem [12]. In particular  it holds for the monotonic max-ﬂow
method [25]  which is also widely used for obtaining diverse labelings in practice [7  20  19]. Natu-
rally  we would like to ask questions about the relationship of these two techniques  such as: “Do
the joint M-best-diverse conﬁgurations form a subset of the conﬁgurations returned by a parametric
submodular minimization problem?”  and conversely “Can the parametric submodular minimization
be used to (efﬁciently) produce the M-best-diverse conﬁgurations?” We give positive answers to
both these questions.
Contribution

• For binary submodular energies we provide a relationship between the joint M-best-diverse
and the parametric submodular minimization problems. In case of “concave node-wise diversity
measures” 1 we give a closed-form formula for the parameters values  which corresponds to the joint
M-best-diverse labelings. The values can be computed in advance  prior to any optimization  which
allows to obtain each labeling independently.
• Our theoretical results suggest a number of efﬁcient algorithms for the joint M-best-diverse

1Precise deﬁnition is given in Sections 2 and 3.

2

problem. We describe and experimentally evaluate the two simplest of them  sequential and parallel.
Both are considerably faster than the popular technique [4] and are as easy to implement. We
demonstrate the effectiveness of these algorithms on two publicly available datasets.

LA =(cid:81)

2 Background and Problem Deﬁnition
Energy Minimization Let 2A denote the powerset of a set A. The pair G = (V  F ) is called a
hyper-graph and has V as a ﬁnite set of variable nodes and F ⊆ 2V as a set of factors. Each variable
node v ∈ V is associated with a variable yv taking its values in a ﬁnite set of labels Lv. The set
v∈A Lv denotes the Cartesian product of sets of labels corresponding to a subset A ⊆ V of
variables. Functions θf : Lf → R  associated with factors f ∈ F   are called potentials and deﬁne
local costs on values of variables and their combinations. Potentials θf with |f| = 1 are called
unary  with |f| = 2 pairwise and |f| > 2 high-order. Without loss of generality we will assume
that there is a unary potential θv assigned to each variable v ∈ V. This implies that F = V ∪ F 
where F = {f ∈ F : |f| ≥ 2}. For any non-unary factor f ∈ F the corresponding set of variables
{yv : v ∈ f} will be denoted by yf . The energy minimization problem consists in ﬁnding a labeling
y∗ = (yv : v ∈ V) ∈ LV  which minimizes the total sum of corresponding potentials:

(cid:88)

v∈V

(cid:88)

f∈F

arg min
y∈LV

E(y) = arg min
y∈L

θv(yv) +

θf (yf ) .

(1)

M(cid:88)

Problem (1) is also known as MAP-inference. Labeling y∗ satisfying (1) will be later called a solution
of the energy-minimization or MAP-inference problem  shortly MAP-labeling or MAP-solution.
Unless otherwise speciﬁed  we will assume that Lv = {0  1}  v ∈ V  i.e. each variable may take only
two values. Such energies will be called binary. We also assume that the logical operations ≤ and ≥
are deﬁned in a natural way on the sets Lv. The case  when the energy E decomposes into unary and
pairwise potentials only  we will term as pairwise case or pairwise energy.
In the following  we use brackets to distinguish between upper index and power  i.e. (A)n means
the n-th power of A  whereas n is an upper index in the expression An. We will keep  however  the
standard notation Rn for the n-dimensional vector space and skip the brackets if an upper index does
not make mathematical sense such as in the expression {0  1}|V|.
Joint-DivMBest Problem Instead of searching for a single labeling with the lowest energy  one
might ask for a set of labelings with low energies  yet being signiﬁcantly different from each other. In
[22] it was proposed to infer such M diverse labelings {y1  . . .   yM} ∈ (L)M jointly by minimizing

EM ({y}) =

E(yi) − λ∆M ({y})

i=1

(2)
w.r.t. {y} := y1  . . .   yM for some λ > 0. Following [22] we use the notation {y} and {y}v
as shortcuts for y1  . . .   yM and y1
v correspondingly. Function ∆M deﬁnes diversity of
arbitrary M labelings  i.e. ∆M ({y}) takes a large value if labelings {y} are in a certain sense diverse 
and a small value otherwise.
In the following  we will refer to the problem (1) of minimizing the energy E itself as to the master
problem for (2).
Node-Wise Diversity In what follows we will consider only node-wise diversity measures  i.e. those
which can be represented in the form

v  . . .   yM

for some node diversity measure ∆M
invariant diversity measures. In other words  such measures that ∆M
any permutation π of variables {y}v.

∆M ({y}) =
(3)
v : {0  1}M → R. Moreover  we will stick to permutation
v (π({y}v)) for
m=1(cid:74)ym
v = 0(cid:75)

v =(cid:80)M

Let the expression(cid:74)A(cid:75) be equal to 1 if A is true and 0 otherwise. Let also m0

count the number of 0’s in {y}v. In the binary case Lv = {0  1}  any permutation invariant measure
can be represented as

v ({y}v) = ∆M

v ({y}v)
∆M

(cid:88)

v∈V

v ({y}v) = ¯∆M
∆M
To keep notation simple  we will use ∆M
v for both representations: ∆M

v (m0

v) .

v ({y}v) and ¯∆M

v (m0

v).

(4)

3

Example 1 (Hamming distance diversity). Consider the common node diversity measure  the sum of
Hamming distances between each pair of labels:

v ({y}v) =
∆M

M(cid:88)

i=1

M(cid:88)
j=i+1(cid:74)yi

v(cid:75).
v (cid:54)= yj

(5)

∆M

v (m0

v) = m0

v · (M − m0
v).

This measure is permutation invariant. Therefore  it can be written as a function of the number m0
v:
(6)
Minimization Techniques for Joint-DivMBest Problem Direct minimization of (2) has so far been
considered as a difﬁcult problem even when the master problem (1) is easy to solve. We refer to [22]
for a detailed investigation of using general MAP-inference solvers for (2). In this paragraph we
brieﬂy summarize existing efﬁcient minimization approaches for (2).
As shown in [22] the sequential method DivMBest [4] can be seen as a greedy algorithm for
approximate minimization of (2)  by ﬁnding one solution after another. The sequential method [4]
is used for diversity measures that can be represented by sum of diversity measures between each
m2=m1+1 ∆2(ym1  ym2). For each m = 1  . . .   M

pair of solutions  i.e. ∆M ({y}) =(cid:80)M

(cid:80)M

m1=1

the method sequentially computes

(cid:34)

(cid:35)

ym = arg min

y∈LV

E(y) − λ

∆2(y  yi)

.

(7)

m−1(cid:88)

i=1

In case of a node diversity measure (3)  this algorithm requires sequentially solving M energy
minimization problems (1)  with only modiﬁed unary potentials comparing to the master problem (1).
It typically implies that an efﬁcient solver for the master problem can also be used to obtain its diverse
solutions.
In [23] an efﬁcient approach for (2) was proposed for submodular energies E. An energy E(y) is
called submodular if for any two labelings y  y(cid:48) ∈ LV it holds

E(y ∨ y(cid:48)) + E(y ∧ y(cid:48)) ≤ E(y) + E(y(cid:48))  

(8)
where y ∨ y(cid:48) and y ∧ y(cid:48) denote correspondingly the node-wise maximum and minimum with respect
to the natural order on the label set Lv.
In the following  we will use the term the higher labeling. The labeling y is higher than the labeling
y(cid:48) if yv ≥ y(cid:48)
v for all v ∈ V. So  the labeling y ∨ y(cid:48) is higher than y and y(cid:48). Since the set of all
labelings is a lattice w.r.t. the operation ≥  we will speak also about the highest labeling.
It was shown in [23] that for submodular energies  under certain technical conditions on the diversity
measure ∆M
v (see Lemma 2 in [23])  the problem (2) can be reformulated as a submodular energy
minimization and  therefore  can be solved either exactly or approximately by efﬁcient optimization
techniques (e.g.  by reduction to the min-cut/max-ﬂow in the pairwise case). However  the size of
the reformulated problem grows at least linearly with M (quadratically in the case of the Hamming
distance diversity (5)) and therefore even approximate algorithms require longer time than the
DivMBest (7) method. Moreover  this difference in runtime grows with M.
The mentioned transformation of (2) into a submodular energy minimization problem is based on the
theorem below  which plays a crucial role in obtaining the main results of this work. We ﬁrst give a
deﬁnition of the “nestedness property”  which is also important for the rest of the paper.
Deﬁnition 1. An M-tuple (y1  . . .   yM ) ∈ (LV )M is called nested if for each v ∈ V the inequality
v ≤ yj
yi
Theorem 1. [Special case of Thm. 1 of [23]] For a binary submodular energy and a node-wise
permutation invariant diversity  there exists a nested minimizer to the Joint-DivMBest objective (2).
Parametric submodular minimization Let γ ∈ R|V|  i = {1  . . .   k} be a vector of parameters
with the coordinates indexed by the node index v ∈ V. We deﬁne the parametric energy minimization
as the problem of evaluating the function

v holds for 1 ≤ i ≤ j ≤ M  i.e. for LV = {0  1}  yi

v = 1 implies yj

v = 1 for j > i.

(cid:34)

(cid:35)

min
y∈LV

Eγ(y) := min
y∈L

E(y) +

γvyv

(9)

(cid:88)

v∈V

for all values of the parameter γ ∈ Γ ⊆ R|V|. The most important cases of the parametric energy
minimization are

4

deﬁned as(cid:80)M

Figure 1: Hamming distance (left) and linear (right) diversity measures for M = 5. Value m is

m=1(cid:74)ym

v = 0(cid:75). Both diversity measures are concave.

• the monotonic parametric max-ﬂow problem [14  18]  which corresponds to the case when E is
a binary submodular pairwise energy and Γ = {ν ∈ R|V| : νv = γv(λ)} and functions γv : Λ → R
are non-increasing for Λ ⊆ R.
• a subclass of the parametric submodular minimization [12  2]  where E is submodular and
Γ = {γ1  γ2  . . .   γk ∈ R|V| : γ1 ≥ γ2 ≥ . . . ≥ γk}  where operation ≥ is applied coordinate-wise.
It is known [38] that in these two cases  (i) the highest minimizers y1  . . .   yk ∈ LV of Eγi 
i = {1  . . .   k} are nested and (ii) the parametric problem (9) is solvable efﬁciently by respective
algorithms [14  18  12]. In the following  we will show that for a submodular energy E the Joint-
DivMBest problem (2) reduces to the parametric submodular minimization with the values γ1 ≥
γ2 ≥ . . . ≥ γM ∈ R|V| given in closed form.

3

Joint M-Best-Diverse Problem as a Parametric Submodular Minimization

Our results hold for the following subclass of the permutation invariant node-wise diversity measures:
v (m) is called concave if for any 1 ≤ i ≤ j ≤ M it
Deﬁnition 2. A node-wise diversity measure ∆M
holds
(10)

v (i − 1) ≥ ∆M

v (i) − ∆M
∆M

v (j) − ∆M

v (j − 1).

There are a number of practically relevant concave diversity measures:
Example 2. Hamming distance diversity (6) is concave  see Fig. 1 for illustration.
Example 3. Diversity measures of the form

v) = −(cid:0)|m0

v)|(cid:1)p

= −(cid:0)|2m0

v − M|(cid:1)p

v − (M − m0

∆M

v (m0

(11)
are concave for any p ≥ 1. Here M − m0
v is the number of variables labeled as 1. Hence 
v)| is an absolute value of the difference between the numbers of variables labeled
|m0
as 0 and 1. It expresses the natural fact that a distribution of 0’s and 1’s is more diverse  when their
amounts are similar.

v − (M − m0

For p = 1 we call the measure (11) linear; for p = 2 the measure (11) coincides with the Hamming
distance diversity (6). An illustration of these two cases is given in Fig. 1.
Our main theoretical result is given by the following theorem:
Theorem 2. Let E be binary submodular and ∆M be a node-wise diversity measure with each
v   v ∈ V   being permutation invariant and concave. Then a nested M-tuple (ym)M
component ∆M
m=1
minimizing the Joint-DivMBest objective (2) can be found as the solutions of the following M
problems:

(cid:34)

(cid:35)

E(y) +

γm
v yv

 

(12)

(cid:88)

v∈V

v = λ(cid:0)∆M

where γm
minimizer must be selected.

v (m) − ∆M

ym = arg min

v (m − 1)(cid:1).

yV

In the case of multiple solutions in (12) the highest

We refer to the supplement for the proof of Theorem 2 and discuss its practical consequences below.
First note that the sequence (γm)M
v . Each of the M
optimization problems (12) has the same size as the master problem (1) and differs from it by

m=1 is monotone due to concavity of ∆M

5

012345m0246∆Mv(m)012345m−5−3−1∆Mv(m)unary potentials only. Theorem 2 implies that γm in (12) satisfy the monotonicity condition:
γ1 ≥ γ2 ≥ . . . ≥ γM . Therefore  equations (12) constitute the parametric submodular minimization
problem as deﬁned above  which reduces to the monotonic parametric max-ﬂow problem for pairwise
E. Let (cid:98)·(cid:99) denote the largest integer not exceeding an argument of the operation.
Corollary 1. Let ∆M
1. γm
2. The values γm

v in Theorem 2 be the Hamming distance diversity (6). Then it holds:

v   m = 1  . . .   M are symmetrically distributed around 0:

v = λ(M − 2m + 1).

v

v = 2λ  m = 1  . . .   M.

−γm

v = γM +1−m

v

and γm
− γm

v = 0  if m = (M + 1)/2 .

≥ 0  for m ≤ (cid:98)(M + 1)/2(cid:99)
3. Moreover  this distribution is uniform  that is γm+1
4. When M is odd  the MAP-solution (corresponding to γ(M +1)/2 = 0) is always among the
M-best-diverse labelings minimizing (2).
Corollary 2. Implications 2 and 4 of Corollary 1 hold for any symmetrical concave ∆M
where ∆M
Corollary 3. For linear diversity measure the value γm

v in (12) is equal to λ · sgn(cid:0) M

sgn(x) is a sign function  i.e.  sgn(x) = (cid:74)x > 0(cid:75) −(cid:74)x < 0(cid:75). Since all γm

2 are the
same  this diversity measure can give only up to 3 different diverse labelings. Therefore  this diversity
measure is not useful for M > 3  and can be seen as a limit of useful concave diversity measures.

v (M + 1 − m) for m ≤ (cid:98)(M + 1)/2(cid:99).

2 − m(cid:1)  where

v (m) = ∆M

v   i.e.  those

v for m < M

v

4 Efﬁcient Algorithmic Solutions
Theorem 2 suggests several new computational methods for minimizing the Joint-DivMBest objec-
tive (2). All of them are more efﬁcient than those proposed in [23]. Indeed  as we show experimentally
in Section 5  they outperform even the sequential DivMBest method (7).
The simplest algorithm applies a MAP-inference solver to each of the M problems (12) sequentially
and independently. This algorithm has the same computational cost as DivMBest (7) since it also
sequentially solves M problems of the same size. However  already its slightly improved version 
described below  performs faster than DivMBest (7).
Sequential Algorithm Theorem 2 states that solutions of (12) are nested. Therefore  from ym−1
= 1
v = 1 for labelings ym−1 and ym obtained according to (12). This allows to
it follows that ym
reduce the size and computing time for each subsequent problem in the sequence.2 Reusing the
ﬂow from the previous step gives an additional speedup. In fact  when applying a push relabel or
pseudoﬂow algorithm in this fashion the total work complexity is asymptotically the same as of a
single minimum cut [14  18] of the master problem. In practice  this strategy is efﬁcient with other
min-cut solvers (without theoretical guarantees) as well. In our experiments we evaluated it with the
dynamic augmenting path method [6  24].
Parallel Algorithm The M problems (12) are completely independent  and their highest minimizers
recover the optimal M-tuple (ym)m according to Theorem 2. They can be solved fully in parallel or 
using p < M processors  in parallel groups of M/p problems per processor  incrementally within
each group. The overhead is only in copying data costs and sharing the memory bandwidth.
Alternative approaches One may suggest that for large M it would be more efﬁcient to solve the full
parametric maxﬂow problem [18  14] and then “read out” solutions corresponding to the desired values
γm. However  the known algorithms [18  14] would perform exactly the incremental computation
described in the sequential approach above plus an extra work of identifying all breakpoints. This
is only sensible when M is larger than the number of breakpoints or the diversity measure is not
known in advance (e.g.  is itself parametric). Similarly  parametric submodular function minimization
can be solved in the same worst case complexity [12] as non-parametric  but the algorithm is again
incremental and would just perform less work when the parameters of interest are known in advance.
5 Experimental Evaluation
We base our experiments on two datasets: (i) The interactive foreground/background image segmen-
tation dataset utilized in several papers [4  31  22  23] for comparing diversity techniques; (ii) A new

2By applying “symmetric reasoning” for the label 0  further speed-ups can be achieved. However  we stick to

the ﬁrst variant in our experiments.

6

Table 1: Interactive segmentation. The quality measure is a per-pixel accuracy of the best
segmentation  out of M  averaged over all test images. The runtime is in milliseconds (ms).
The quality for M = 1 is 91.57. Parametric-parallel is the fastest method followed by
Parametric-sequential. Both achieve higher quality than DivMBest  and return the same solu-
tion as Joint-DivMBest.

M=2

M=6

M=10

DivMBest [4]
Joint-DivMBest [23]
Parametric-sequential (1 core)
Parametric-parallel (6 cores)

quality
93.16
95.13
95.13
95.13

time (ms)

2.6
5.5
2.2
1.9

quality
95.02
96.01
96.01
96.01

time (ms)

11.6
17.2
5.5
4.3

quality
95.16
96.19
96.19
96.19

time (ms)

15.4
80.3
8.4
6.2

dataset for foreground/background image segmentation with binary pairwise energies derived from
the well-known PASCAL VOC 2012 dataset [11]. Energies of the master problem (1) in both cases
are binary and pairwise  therefore we use their reduction [26] to the min-cut/max-ﬂow problem to
obtain solutions efﬁciently.
Baselines Our main competitor is the fastest known approach for inferring M diverse solutions  the
DivMBest method [4]. We made its efﬁcient re-implementation using dynamic graph-cut [24]. We
also compare our method with Joint-DivMBest [23]  which provides an exact minimum of (2) as
our method does.
Diversity Measure In all of our experiments we use the Hamming distance diversity measure (5).
Note that in [31] more sophisticated diversity measures were used e.g.  the Hamming Ball. However 
the DivMBest method (7) with this measure requires to run a very time-consuming HOP-MAP [37]
inference technique. Moreover  the experimental evaluation in [23] suggests that the exact minimum
of (2) with Hamming distance diversity (5) outperforms DivMBest with a Hamming Ball distance
diversity.
Our Method We evaluate both algorithms described in Section 4  i.e.  sequential and parallel. We
refer to them as Parametric-sequential and Parametric-parallel respectively. We utilize
the dynamic graph-cut [24] technique for Parametric-sequential  which makes it comparable to
our implementation of DivMBest. The max-ﬂow solver of [6] is used for Parametric-parallel
together with OpenMP directives. For the experiments we use a computer with 6 physical cores (12
virtual cores)  and run Parametric-parallel with M threads.
Parameters λ (from (7) and (2)) were tuned via cross-validation for each algorithm and each experi-
ment separately.
5.1
The basic idea is that after a user interaction  the system provides the user with M diverse seg-
mentations  instead of a single one. The user can then manually select the best one and add more
user scribbles  if necessary. Following [4] we consider only the ﬁrst iteration of such an interactive
procedure  i.e.  we consider user scribbles to be given and compare the sets of segmentations returned
by the system.
The authors of [4] kindly provided us their 50 super-pixel graphical model instances. They are based
on a subset of the PASCAL VOC 2010 [11] segmentation challenge with manually added scribbles.
An instance has on average 3000 nodes. Pairwise potentials are given by contrast-sensitive Potts
terms [5]  which are submodular in the binary case. This implies that Theorem 2 is applicable.
Quantitative comparison and runtime of the different algorithms are presented in Table 1. As
in [4]  our quality measure is a per-pixel accuracy of the best solution for each test image  averaged
over all test images. As expected  Joint-DivMBest and Parametric-* return the same  exact
solution of (2). The measured runtime is also averaged over all test images. Parametric-parallel
is the fastest method followed by Parametric-sequential. Note that on a computer with fewer
cores  Parametric-sequential may even outperform Parametric-parallel because of the
parallelization overheads.
5.2 Foreground/Background Segmentation
The Pascal VOC 2012 [11] segmentation dataset has 21 labels. We selected all those 451 images
from the validation set for which the ground truth labeling has only two labels (background and one

Interactive Segmentation

7

(a) Intersection-over-union (IoU)

(b) Runtime in seconds

Figure 2: Foreground/background segmentation.
(a) Intersection-over-union (IoU) score
Parametric represents a curve  which is the same
for the best segmentation  out of M.
for Parametric-sequential  Parametric-parallel and Joint-DivMBest  since they ex-
actly solve the same Joint-DivMBest problem.
(b) DivMBest uses dynamic graph-cut [24].
Parametric-sequential uses dynamic graph-cut and a reduced size graph for each consecu-
tive labeling problem. Parametric-parallel solves M problems in parallel using OpenMP.

of the 20 object classes) and which were not used for training. As unary potentials we use the output
probabilities of the publicly available fully convolutional neural network FCN-8s [29]  which is
trained for the Pascal VOC 2012 challenge. This CNN gives unary terms for all 21 classes. For each
image we pick only two classes: the background and the class-label that is presented in the ground
truth. As pairwise potentials we use the contrastive-sensitive Potts terms [5] with a 4-connected grid
structure.
Quantitative Comparison and Runtime As quality measure we use the standard Pascal VOC
measure for semantic segmentation – average intersection-over-union (IoU) [11]. The unary potentials
alone  i.e.  output of FCN-8s  give 82.12 IoU. The single best labeling  returned by the MAP-inference
problem  improves it to 83.23 IoU.
The comparisons with respect to runtime and accuracy of results are presented in Fig. 2a and 2b
respectively. The increase in runtime with respect to M for Parametric-parallel is due to
parallelization overhead costs  which grow with M. Parametric-parallel is a clear winner in
this experiment  both in terms of quality and runtime. Parametric-sequential is slower than
Parametric-parallel but faster than DivMBest. The difference in runtime between these three
algorithms grows with M.

6 Conclusion and Outlook
We have shown that the M labelings  which constitute a solution to the Joint-DivMBest problem with
binary submodular energies  and concave node-wise permutation invariant diversity measures can be
computed in parallel  independently from each other  as solutions of the master energy minimization
problem with modiﬁed unary costs. This allows to build solvers which run even faster than the
approximate method of Batra et al. [4]. Furthermore  we have shown that such Joint-DivMBest
problems reduce to the parametric submodular minimization. This shows a clear connection of these
two practical approaches to obtaining diverse solutions to the energy minimization problem.
References
[1] Ahmed  F.  Tarlow  D.  Batra  D.: Optimizing expected Intersection-over-Union with candidate-constrained

CRFs. In: ICCV (2015)

[2] Bach  F.: Learning with submodular functions: A convex optimization perspective. Foundations and

Trends in Machine Learning 6(2-3)  145–373 (2013)

[3] Batra  D.: An efﬁcient message-passing algorithm for the M-best MAP problem. In: UAI (2012)
[4] Batra  D.  Yadollahpour  P.  Guzman-Rivera  A.  Shakhnarovich  G.: Diverse M-best solutions in Markov

random ﬁelds. In: ECCV. Springer Berlin/Heidelberg (2012)

[5] Boykov  Y.  Jolly  M.P.: Interactive graph cuts for optimal boundary & region segmentation of objects in

N-D images. In: ICCV (2001)

[6] Boykov  Y.  Kolmogorov  V.: An experimental comparison of min-cut/max-ﬂow algorithms for energy

minimization in vision. TPAMI 26(9)  1124–1137 (2004)

8

12345678910M838485868788IoUscoreParametricDivMBest12345678910M0.000.050.100.150.200.250.300.350.40time(s)DivMBestParametric-sequentialParametric-parallel[7] Carreira  J.  Sminchisescu  C.: Constrained parametric min-cuts for automatic object segmentation. In:

[8] Chen  C.  Kolmogorov  V.  Zhu  Y.  Metaxas  D.N.  Lampert  C.H.: Computing the M most probable modes

[9] Dey  D.  Ramakrishna  V.  Hebert  M.  Andrew Bagnell  J.: Predicting multiple structured visual interpreta-

CVPR. pp. 3241–3248 (2010)

of a graphical model. In: AISTATS (2013)

tions. In: ICCV (2015)

[10] Elidan  G.  Globerson  A.: The probabilistic inference challenge (PIC2011) (2011)
[11] Everingham  M.  Van Gool  L.  Williams  C.K.I.  Winn  J.  Zisserman  A.: The PASCAL Visual Object

Classes Challenge 2012 (VOC2012) Results (2012)

[12] Fleischer  L.  Iwata  S.: A push-relabel framework for submodular function minimization and applications

to parametric optimization. Discrete Applied Mathematics 131(2)  311–322 (2003)

[13] Fromer  M.  Globerson  A.: An LP view of the M-best MAP problem. In: NIPS 22 (2009)
[14] Gallo  G.  Grigoriadis  M.D.  Tarjan  R.E.: A fast parametric maximum ﬂow algorithm and applications.

SIAM Journal on Computing 18(1)  30–55 (1989)

[15] Guzman-Rivera  A.  Batra  D.  Kohli  P.: Multiple choice learning: Learning to produce multiple structured

[16] Guzman-Rivera  A.  Kohli  P.  Batra  D.: DivMCuts: Faster training of structural SVMs with diverse

[17] Guzman-Rivera  A.  Kohli  P.  Batra  D.  Rutenbar  R.A.: Efﬁciently enforcing diversity in multi-output

[18] Hochbaum  D.S.: The pseudoﬂow algorithm: A new algorithm for the maximum-ﬂow problem. Operations

outputs. In: NIPS 25 (2012)

M-best cutting-planes. In: AISTATS (2013)

structured prediction. In: AISTATS (2014)

research 56(4)  992–1009 (2008)

In: ICCV. pp. 849–856. IEEE (2009)

[19] Hower  D.  Singh  V.  Johnson  S.C.: Label set perturbation for MRF based neuroimaging segmentation.

[20] Jug  F.  Pietzsch  T.  Kainmüller  D.  Funke  J.  Kaiser  M.  van Nimwegen  E.  Rother  C.  Myers  G.:
Optimal joint segmentation and tracking of Escherichia Coli in the mother machine. In: BAMBI (2014)
[21] Kappes  J.H.  Andres  B.  Hamprecht  F.A.  Schnörr  C.  Nowozin  S.  Batra  D.  Kim  S.  Kausler  B.X. 
Kröger  T.  Lellmann  J.  Komodakis  N.  Savchynskyy  B.  Rother  C.: A comparative study of modern
inference techniques for structured discrete energy minimization problems. IJCV pp. 1–30 (2015)

[22] Kirillov  A.  Savchynskyy  B.  Schlesinger  D.  Vetrov  D.  Rother  C.: Inferring M-best diverse labelings in

a single one. In: ICCV (2015)

[23] Kirillov  A.  Schlesinger  D.  Vetrov  D.P.  Rother  C.  Savchynskyy  B.: M-best-diverse labelings for

submodular energies and beyond. In: NIPS (2015)

[24] Kohli  P.  Torr  P.H.: Dynamic graph cuts for efﬁcient inference in Markov random ﬁelds. PAMI (2007)
[25] Kolmogorov  V.  Boykov  Y.  Rother  C.: Applications of parametric maxﬂow in computer vision. In: ICCV.

pp. 1–8. IEEE (2007)

[26] Kolmogorov  V.  Zabih  R.: What energy functions can be minimized via graph cuts? TPAMI (2004)
[27] Kulesza  A.  Taskar  B.: Structured determinantal point processes. In: NIPS 23 (2010)
[28] Lawler  E.L.: A procedure for computing the K best solutions to discrete optimization problems and its

application to the shortest path problem. Management Science 18(7) (1972)

[29] Long  J.  Shelhamer  E.  Darrell  T.: Fully convolutional networks for semantic segmentation. CVPR

(2015)

[30] Nilsson  D.: An efﬁcient algorithm for ﬁnding the M most probable conﬁgurationsin probabilistic expert

systems. Statistics and Computing 8(2)  159–173 (1998)

[31] Prasad  A.  Jegelka  S.  Batra  D.: Submodular meets structured: Finding diverse subsets in exponentially-

large structured item sets. In: NIPS 27 (2014)

[32] Premachandran  V.  Tarlow  D.  Batra  D.: Empirical minimum bayes risk prediction: How to extract an

extra few % performance from vision models with just three more parameters. In: CVPR (2014)

[33] Ramakrishna  V.  Batra  D.: Mode-marginals: Expressing uncertainty via diverse M-best solutions. In:

NIPS Workshop on Perturbations  Optimization  and Statistics (2012)

[34] Schlesinger  M.I.  Hlavac  V.: Ten lectures on statistical and structural pattern recognition (2002)
[35] Sener  O.  Saxena  A.: rCRF: Recursive belief estimation over CRFs in RGB-D activity videos.

In:

Robotics Science and Systems (RSS) (2015)

[36] Szeliski  R.  Zabih  R.  Scharstein  D.  Veksler  O.  Kolmogorov  V.  Agarwala  A.  Tappen  M.F.  Rother  C.:
A comparative study of energy minimization methods for Markov random ﬁelds with smoothness-based
priors. TPAMI 30(6)  1068–1080 (2008)

[37] Tarlow  D.  Givoni  I.E.  Zemel  R.S.: HOP-MAP: Efﬁcient message passing with high order potentials. In:

[38] Topkis  D.M.: Minimizing a submodular function on a lattice. Operations research 26(2)  305–321 (1978)
[39] Wang  S.  Fidler  S.  Urtasun  R.: Lost shopping! monocular localization in large indoor spaces. In: ICCV

[40] Yadollahpour  P.  Batra  D.  Shakhnarovich  G.: Discriminative re-ranking of diverse segmentations. In:

[41] Yanover  C.  Weiss  Y.: Finding the M most probable conﬁgurations using loopy belief propagation. In:

AISTATS (2010)

(2015)

CVPR (2013)

NIPS 17 (2004)

9

,Alexander Kirillov
Alexander Shekhovtsov
Carsten Rother
Bogdan Savchynskyy
Wittawat Jitkrittum
Wenkai Xu
Zoltan Szabo
Kenji Fukumizu
Arthur Gretton