2018,Dimensionality Reduction has Quantifiable Imperfections: Two Geometric Bounds,In this paper  we investigate Dimensionality reduction (DR) maps in an information retrieval setting from a quantitative topology point of view. In particular  we show that no DR maps can achieve perfect precision and perfect recall simultaneously. Thus a continuous DR map must have imperfect precision. We further prove an upper bound on the precision of Lipschitz continuous DR maps. While precision is a natural measure in an information retrieval setting  it does not measure `how' wrong the retrieved data is. We therefore propose a new measure based on Wasserstein distance that comes with similar theoretical guarantee. A key technical step in our proofs is a particular optimization problem of the $L_2$-Wasserstein distance over a constrained set of distributions. We provide a complete solution to this optimization problem  which can be of independent interest on the technical side.,Dimensionality Reduction has Quantiﬁable

Imperfections: Two Geometric Bounds

Kry Yik Chau Lui

Borealis AI

Canada

Gavin Weiguang Ding

Borealis AI

Canada

yikchau.y.lui@borealisai.com

gavin.ding@borealisai.com

Ruitong Huang

Borealis AI

Canada

ruitong.huang@borealisai.com

Robert J. McCann

Department of Mathematics

University of Toronto

Canada

mccann@math.toronto.edu

Abstract

In this paper  we investigate Dimensionality reduction (DR) maps in an information
retrieval setting from a quantitative topology point of view. In particular  we show
that no DR maps can achieve perfect precision and perfect recall simultaneously.
Thus a continuous DR map must have imperfect precision. We further prove an
upper bound on the precision of Lipschitz continuous DR maps. While precision
is a natural measure in an information retrieval setting  it does not measure “how”
wrong the retrieved data is. We therefore propose a new measure based on Wasser-
stein distance that comes with similar theoretical guarantee. A key technical step
in our proofs is a particular optimization problem of the L2-Wasserstein distance
over a constrained set of distributions. We provide a complete solution to this
optimization problem  which can be of independent interest on the technical side.

1

Introduction

Dimensionality reduction (DR) serves as a core problem in machine learning tasks including infor-
mation compression  clustering  manifold learning  feature extraction  logits and other modules in a
neural network and data visualization [16  8  34  19  25]. In many machine learning applications  the
data manifold is reduced to a dimension lower than its intrinsic dimension (e.g. for data visualizations 
output dimension is reduced to 2 or 3; for classiﬁcations  it is the number of classes). In such cases  it
is not possible to have a continuous bijective DR map (i.e. classic algebraic topology result on invari-
ance of dimension [26]). With different motivations  many nonlinear DR maps have been proposed
in the literature  such as Isomap  kernel PCA  and t-SNE  just to name a few [31  33  22]. A common
way to compare the performances of different DR maps is to use a down stream supervised learning
task as the ground truth performance measure. However  when such down stream task is unavailable 
e.g. in an unsupervised learning setting as above  one would have to design a performance measure
based on the particular context. In this paper  we focus on the information retrieval setting  which
falls into this case. An information retrieval system extracts the features f (x) from the raw data x for
future queries. When a new query y0 = f (x0) is submitted  the system returns the most relevant data
with similar features  i.e. all the x such that f (x) is close to y0. For computational efﬁciency and
storage  f is usually a DR map  retaining only the most informative features. Assume that the ground
truth relevant data of x0 is deﬁned as a neighbourhood U of x that is a ball with radius rU centered at

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

x 1  and the system retrieves the data based on relevance in the feature space  i.e. the inverse image 
f−1(V )  of a retrieval neighbourhood V (cid:51) f (x0). Here V is the ball centered at y0 = f (x0) with
radius rV that is determined by the system. It is natural to measure the system’s performance based
on the discrepancy between U and f−1(V ). Many empirical measures of this discrepancy have been
proposed in the literature  among which precision and recall are arguably the most popular ones
[32  23  20  34]. However  theoretical understandings of these measures are still very limited.
In this paper  we start with analyzing the theoretical properties of precision and recall in the informa-
tion retrieval setting. Naively computing precision and recall in the discrete settings gives undesirable
properties  e.g. precision always equals recall when computed by using k nearest neighbors. How
to measure them properly is unclear in the literature (Section 3.2). On the other hand  numerous
experiments have suggested that there exists a tradeoff between the two when dimensionality reduc-
tion happens [34]  yet this tradeoff still remains a conceptual mystery in theory. To theoretically
understand this tradeoff  we look for continuous analogues of precision and recall  and exploit
the geometric and function analytic tools that study dimensionality reduction maps [15]. The ﬁrst
question we ask is what property a DR map should have  so that the information retrieval system can
attain zero false positive error (or false negative error) when the relevant neighbourhood U and the
retrieved neighbourhood V are properly selected. Our analyses show the equivalence between the
achievability of perfect recall (i.e. zero false negative) and the continuity of the DR map. We further
prove that no DR map can achieve both perfect precision and perfect recall simultaneously. Although
it may seem intuitive  to our best knowledge  this is the ﬁrst theoretical guarantee in the literature of
the necessity of the tradeoff between precision and recall in a dimension reduction setting.
Our main results are developed for the class of (Lipschitz) continuous DR maps. The ﬁrst main result
of this paper is an upper bound for the precision of a continuous DR map. We show that given a
continuous DR map  its precision decays exponentially fast with respect to the number of (intrinsic)
dimensions reduced. To our best knowledge  this is the ﬁrst theoretical result in the literature for the
decay rate of the precision of a dimensionality reduction map. The second main result is an alternative
measure for the performance of a continuous DR map  called W2 measure  based on L2-Wasserstein
distance. This new measure is more desirable as it can also detect the distance distortion between
U and f−1(V ). Moreover  we show that our measure also enjoys a theoretical lower bound for
continuous DR maps. Several other distance-based measures have been proposed in the literature
[32  23  20  34]  yet all are proposed heuristically with meagre theoretical understanding. Simulation
results suggest optimizing the Wasserstein measure lower bound corresponds to optimizing a weighted
f-1 score (i.e. f-β score). Thus we may optimize precision and recall without dealing with their
computational difﬁculties in the discrete setting.
Finally  let us make some comments on the technical parts of the paper. The ﬁrst key step is the
Waist Inequality from the ﬁeld of quantitative algebraic topology. At a high level  we need to analyse
f−1(V )  inverse image of an open ball for an arbitrary continuous map f. The waist inequality
guarantees the existence of a ‘large’ ﬁber  which allows us to analyse f−1(V ) and prove our ﬁrst
main result. We further show that in a common setting  a signiﬁcant proportion of ﬁbers are actually
‘large’. For our second main result  a key step in the proof is a complete solution to the following
iterated optimization problem:

inf

W : Voln(W )=M

W2(PBr   PW ) =

inf

W : Voln(W )=M

inf

ξ∈Ξ(PBr  PW )

E(a b)∼ξ[(cid:107)a − b(cid:107)2

2]1/2 

where Br is a ball with radius r  PBr (PW   respectively) is a uniform distribution over Br (W  
respectively)  and W2 is the L2-Wasserstein distance. Unlike a typical optimal transport problem
where the transport function between source and target distributions is optimized  in the above
problem the source distribution is also being optimized at the outer level. This becomes a difﬁcult
constrained iterated optimization problem. To address it  we borrow tools from optimal partial
transport theory [9  11]. Our proof techniques leverage the uniqueness of the solution to the optimal
partial transport problem and the rotational symmetry of Br to deduce W .

1The value of rU is unknown  and it depends on the user and the input data x0. However  we can assume rU
is small compared to the input domain size. For example  the number of relevant items to a particular user is
much fewer than the number of total items.

2

1.1 Notations
We collect our notations in this section. Let m be the embedding dimension  M be an n dimensional
data manifold2 embedded in RN   where N is the ambient dimension. M is typically modelled
as a Riemannian manifold  so it is a metric space with a volume form. Let m < n < N and
f : M ⊂ RN → Rm be a DR map. The pair (x  y) will be the points of interest  where y = f (x).
The inverse image of y under the map f is called ﬁber  denoted f−1(y). We say f is continuous at
point x iff oscf (x) = 0  where oscf (x) = inf U ;Uopen{diam(f (U )); x ∈ U} is the oscillation for f
at x ∈ M. We say f is one-to-one or injective when its ﬁber  f−1(y) is the singleton set {x}.
We let A ⊕  := {x ∈ RN|d(x  A) < } denote the -neighborhood of the nonempty set A. In
RN   we note the -neighborhood of the nonempty set A is the Minkowski sum of A with BN
 (x) 
where the Minkowski sum between two sets A and B is: A ⊕ B = {a + b|a ∈ A  b ∈ B}.
For example  an n dimension open ball with radius r  centered at a point x can be expressed as:
r (x) = x ⊕ Bn
r (0) = x ⊕ r  where the last expression is used to simplify notation. If not speciﬁed 
Bn
the dimension of the ball is n. We also use Br to denote the ball with radius r when its center is
r denotes n-dimensional sphere in Rn+1 with radius r. Let Voln denote
irrelevant. Similarly  Sn
n-dimensional volume.3 When the intrinsic dimension of A is greater than n  we set Voln(A) = ∞.
Throughout the rest of the paper  we use U to denote BrU (x) a ball with radius rU centered at x
and V = BrV (y) a ball with radius rV centered at y. These are metric balls in a metric space. For
example  they are geodesic balls in a Riemannian manifold  whenever they are well deﬁned. In
Euclidean spaces  U is a Euclidean ball with L2 norm. By T#(µ) = ν  we mean a map T pushes
forward a measure µ to ν  i.e. ν(B) = µ(T −1(B)) for any Borel set B. We say a measure µ is
dominated by another measure ν  if for every measurable set A  µ(A) ≤ ν(A).

2 Precision and recall

We present the deﬁnitions of precision and recall in a continuous setting in this section. We then prove
the equivalence between perfect recall and the continuity  followed by a theorem on the necessary
tradeoff between the perfect recall and the perfect precision for a dimension reduction information
retrieval system. The main result of this section is a theoretical upper bound for the precision of a
continuous DR map.

2.1 Precision and recall

While precision and recall are commonly deﬁned based on ﬁnite counts in practice  when analysing
DR maps between spaces  it is natural to extend their deﬁnitions in a continuous setting as follows.
Deﬁnition 1 (Precision and Recall). Let f be a continuous DR map. Fix (x  y = f (x))  rU > 0
(y) ⊂ Rm be the balls with radius rU and rV
and rV > 0  let U = BrU (x) ⊂ RN and V = Bm
respectively. The precision and recall of f at U and V are deﬁned as:

rV

Precisionf (U  V ) =

;

Recallf (U  V ) =

Voln(f−1(V ) ∩ U )

Voln(f−1(V ))

Voln(f−1(V ) ∩ U )

.

Voln(U )

for every rU  

We say f achieves perfect precision at x if
there exists rV such that
P recisionf (U  V ) = 1. Also  f achieves perfect recall at x if for every rV   there exists rU such that
Recallf (U  V ) = 1. Finally  we say f achieves perfect precision (perfect recall  respectively) in an
open set W   if f achieves perfect precision (perfect recall  respectively) at w for any w ∈ W .
Note that perfect precision requires f−1(V ) ⊂ U except a measure zero set. Similarly  perfect
recall requires U ⊂ f−1(V ) except a measure zero set. Figure 1 illustrates the precision and recall
deﬁned above. To measure the performance of the information retrieval system  we would like to
understand how different f−1(V ) is from the ideal response U = BrU (x). Precision and recall
provides two meaningful measures for this difference based on their volumes. Note that f achieves

2There is empirical and theoretical evidence that data distribution lies on low dimensional submanifold in the
ambient space [27].
3 Let A be a set. In Euclidean space  Voln(A) = Ln(A) is the Lebesgue measure. For a general n-rectiﬁable
set  Voln(A) = Hn(A) is the Hausdorff measure. When A is not rectiﬁable  Voln(A) = Mn∗ (A) is the lower
Minkowski content.

3

Figure 1: Illustration of precision and recall.

perfect precision at x implies that no matter how small the relevant radius rU is for the image  the
system would be able to achieve zero false positive by picking proper rV . Similarly perfect recall at
x implies no matter how small rV is  the system would not miss the most relevant images around x.
In fact  the deﬁnitions of perfect precision and perfect recall are closely related to continuity and
injectivity of a function f. Here we only present an informal statement. Rigorous statements are
given in the Appendix B.
Proposition 1. Perfect recall is equivalent to continuity. If f is continuous  then perfect precision is
equivalent to injectivity.

The next result shows that no DR map f  continuous or not  can achieve perfect recall and perfect
precision simultaneously - a widely observed but unproved phenomenon in practice. In other words 
it rigorously justiﬁes the intuition that perfectly maintaining the local neighbourhood structure is
impossible for a DR map.
Theorem 1 (Precision and Recall Tradeoff). Let n > m  M ⊂ RN be a Riemannian n-dimensional
submanifold. Then for any (dimensionality reduction) map f : M → Rm and any open set W ⊂ M 
f cannot achieve both perfect precision and perfect recall on W .

2.2 Upper bound for the precision of a continuous DR map

In this section  we provide a quantitative analysis for the imperfection of f. In particular  we prove
an upper bound for the precision of a continuous DR map f (thus f achieves perfect recall). For
simplicity  we assume the domain of f is an n -ball with radius R embedded in RN   denoted by Bn
R.
Our main tool is the Waist Inequality [29  1] in quantitative topology. See Appendix A for an exact
statement.
Intuitively  the Waist Inequality guarantees the existence of y ∈ Rm such that f−1(y) is a ‘large’
ﬁber. If f is also L-Lipschitz  then for p in a small neighbourhood V of y  f−1(p) is also a ‘large’
ﬁber  thus f−1(V ) has a positive volume in M. Exploiting the lower bound for Voln
to our upper bound in Theorem 2 on the precision of f  Precisionf (U  V ). A rigorous proof is given
in the appendix Appendix C.
R → Rm is a
Theorem 2 (Precision Upper Bound  Worst Case). Assume n > m  and that f : Bn
continuous map with Lipschitz constant L. Let rU and rV > 0 be ﬁxed. Denote

(cid:0)f−1(V )(cid:1) leads

Then there exists y ∈ Rm such that for any x ∈ f−1(y)  we have:

.

m
r
U

R

pm(rV /L)

(1)

(2)

Γ( n−m

D(n  m) =

2 + 1)Γ( m
2 + 1)

Γ( n

2 + 1)

(cid:16) rU

(cid:17)n−m

P recisionf (U  V ) ≤ D(n  m)

4

pm(r)
rm = 1.

R

√

where pm(r) is rm (1 + o(1))  i.e. lim
r→0
Remark 1. Key to the bound is the Waist Inequality. As such  upper bounds on precision for other
spaces (i.e. cube  see Klartag [17] ) can be established  provided there is a Waist Inequality for
the space. The Euclidean norm setting can also be extended to arbitrary norms  exploiting convex
geometry (i.e. Akopyan and Karasev [2]). Rigorous proofs are given in the appendix C.
Remark 2. With m ﬁxed as a constant  note that D(n  m) decays asymptotically at a rate of

(cid:1)n−m decays exponentially. Typically  L can grow at a

(1/n)m/2. Also note that rU < R implies(cid:0) rU

rate of
n. Moreover  while pm(r)’s behaviour is given asymptotically  it is independent of n. Thus
the upper bound decay is dominated by the exponential rate of n − m. For ﬁxed n  m  this upper
bound can be trivial when rU (cid:29) rV . However  this rarely happens in practice in the information
retrieval setting. Note that the number of relevant items  which is indexed by rU   is often smaller than
the number of retrieved items  that depends on rV   while they are both much smaller than number of
total items  indexed by R.
We note however that this bound depends on the intrinsic dimension n. When n (cid:28) N and the ambient
dimension N is used in place  the upper bound could be misleading in practice as it is much smaller
than it should be. To estimate this bound in practice  a good estimate on intrinsic dimension [13] is
needed  which is an active topic in the ﬁeld and beyond the scope of this paper.
Theorem 2 guarantees the existence of a particular point y ∈ Rm where the precision of f on its
neighbourhood is small. It is natural to ask if this is also true in an average sense for every y. In
other words  we know an information retrieval system based on DR maps always has a blindspot 
but is this blindspot behaviour a typical case? In general  when m > 1  this is false  due to a recent
counter-example constructed by Alpert and Guth [3]. However  our next result shows that for a large
number of continuous DR maps in the ﬁeld  such upper bound still holds with high probability.
Theorem 3 (Precision Upper Bound  Average Case). Assume n > m and Bn
probability distribution. Consider the following cases:

R is equiped with uniform

• case 1: m = 1 and f : Bn
• case 2: f : Bn

R → Rm is L Lipschitz continuous  or

R → Rm is a k-layer feedforward neural network map with Lipschitz constant

L  with surjective linear maps in each layer.

Let 0 < δ2 < R2 − r2
case 2  it holds that

U   rU   rV > 0 be ﬁxed  then with probability at least q1 for case 1 or q2 for

(cid:33)n−m

(cid:32)

rU(cid:112)r2

U + δ2

(cid:82)

(cid:82)

P recisionf (U  V ) ≤ D(n  m)

m
r
U

pm(rV /L)

 

(3)

where

(cid:60) =(cid:112)R2 − r2

q1 =

1

2πR

Bm(cid:60)

Voln−m+1Proj−1
Voln(Bn
R)
U − δ2  Proj1 : Sn+1

q2 =
R → Rm and Proj2 : Bn

1 (t)dt

 

maps. Furthermore 

Voln−mProj−1

2 (t)dt

Bm(cid:60)

 

Voln(Bn
R)

R → Rm are arbitrary surjective linear
q2 = 1.

r2
U

r2
U

q1 = 1

lim
R2 →0
+δ2

lim
R2 →0
+δ2
See Appendix D for an explicit characterization of Proj−1
2 (t). Theorem 2 and Theo-
rem 3 together suggest that practioners should be cautious in applying and interpreting DR maps.
One important application of DR maps is in data visualization. Among the many algorithms  t-SNE’s
empirical success made it the de facto standard. While [5] shows t-SNE can recover inter-cluster
structure in some provable settings  the resulted intra-cluster embedding will very likely be subject
to the constraints given in our work4. For example  recall within a cluster will be good  but the
intra-cluster precision won’t be. In more general cases and/or when perplexity is too small  t-SNE

1 (t) and Proj−1

4Strictly speaking  the DR maps induced by t-SNE may not be continuous  and hence our theorems do not
apply directly. However  since we can measure how closely parametric t-SNE (which is continuous) behaves as
t-SNE and there is empirical evidence to their similarity [21]  our theorems may apply again.

5

can create artiﬁcial clusters  separating neighboring datapoints. The resulted visualization embedding
may enjoy higher precision  but its recall suffers. The interested readers are referred to Appendix G.1
for more experimental illustrations. Our work thus sheds light on the inherent tradeoffs in any visual-
ization embedding. It also suggests the companion of a reliability measure to any data visualization
for exploratory data analysis  which measures how a low dimensional visualization represents the
true underlying high dimensional neighborhood structure.5

3 Wasserstein measure

Intuitively we would like to measure how different the original neighbourhood U of x is from the
retrieved neighbourhood f−1(V ) when using the neighbourhood of f (x) in Rm. Precision and Recall
in Section 2.1 provide a semantically meaningful way for this purpose and we gave a non-trivial upper
bound for precision when the feature extraction is a continuous DR map. However  precision and
recall are purely volume-based measures. It would be more desirable if the measure could also reﬂect
the information about the distance distortions between U and f−1(V ). In this section  we propose
an alternative measure to reﬂect such information based on the L2-Wasserstein distance. Efﬁcient
algorithms for computing the empirical Wasserstein distance exists in the literature [4]. Unlike the
measure proposed in Venna et al. [34]  our measure also enjoys a theoretical guarantee similar to
Theorem 2  which provides a non-trivial characterization for the imperfection of dimension reduction
information retrieval.
Let PU (Pf−1(V )  respectively) denote the uniform probability distribution over U (f−1(V )  respec-
tively)  and Ξ(PU   Pf−1(V )) be the set of all the joint distribution over Bn
R  whose marginal
distributions are PU over the ﬁrst Bn
R. We propose to measure the
difference between U and f−1(V ) by the L2-Wasserstein distance between PU and Pf−1(V ):

R and Pf−1(V ) over the second Bn

R × Bn

W2(PU   Pf−1(V )) =

inf
ξ∈Ξ(PU  P

f−1(V ))

E(a b)∼ξ[(cid:107)a − b(cid:107)2

2]1/2.

In practice  it is reasonable to assume that Voln(U ) is small in most retrieval systems. In such cases 
low W2(PU   Pf−1(V )) cost is closely related to high precision retrieval. To see that  when Voln(U ) is
small  achieving high precision retrieval requires small Voln(f−1(V ))  which is a precise quantitative
way of saying f being roughly injective. Moreover  as seen in Section 2.1  f being roughly injective
≈ f giving high precision retrieval. As a result  we can expect high precision retrieval performance
when optimizing W2(PU   Pf−1(V )) measure. Such relation is also empirically conﬁrmed in the
simulation in Section 3.2.
Besides its computational beneﬁts  for a continuous DR map f  the following theorem provides a
lower bound on W2(PU   Pf−1(V )) with a similar ﬂavour to the precision upper bound in Theorem 1.
R → Rm be a L-Lipschitz
Theorem 4 (Wasserstein Measure Lower Bound). Let n > m  f : Bn
R. There exists y ∈ Rm such that for any
continuous map  where R is the radius of the ball Bn
x ∈ f−1(y)  any rU > 0 such that Bn

R  and any rV > 0 such that r ≥ rU   we have:

(x) ⊂ Bn

rU

(cid:16)

where r =

Γ( n

2 +1)

Γ( n−m

2 +1)Γ( m

2 +1)

(r − rU )2

2 (PU   Pf−1(V )) ≥ n
(cid:17) 1
W 2
n + 2
2 (PU   Pf−1(V )) = Ω(cid:0)(R − rU )2(cid:1) .

(pm(rV /L))

W 2

n−m

R

n

n

1

n . In particular  as n → ∞ 

(cid:0)f−1(V )(cid:1) by the topologically ﬂavored waist inequality (Equation (6)). Heuristically

We sketch the proof here. A complete proof can be found in Appendix E. The proof starts with a lower
bound of Voln
Voln(f−1(V )) is much larger than Voln(U ) when n (cid:29) m and R (cid:29) rU . The main component of the
proof is to establish an explicit lower bound for W2(PU   PW ) over all possible W of a ﬁxed volume
V  6 where U is a ball with radius rU   as shown in Theorem 5. In particular  we prove that the shape
5Such attempts existed in literature on visualization of dimensionality reduction (e.g. [34]). However  since
these works are based on heuristics  it is less clear what they measure  nor do they enjoy theoretical guarantee.
6An antecedent of this problem was studied in Section 2.3 of [24]  where the authors optimize over the more
restricted class of ellipses with ﬁxed area. For our purpose  the minimization is over bounded measurable sets.

6

of optimal W ∗ must be rotationally invariant  thus W ∗ must be a union of spheres. This is achieved
by levering the uniqueness of the solution to the optimal partial transport problem [9  11]. We then
prove that the optimal solution for W is the ball that has a common center with U.
Theorem 5. Let U = BrU and V ≥ Vol(U ). Then
inf

W : Voln(W )≥V W2(PU   PW ) =

inf

where BrV is an rV ball with the same center with U such that Voln(BrV ) = V. Moreover  T (x) =
rV x  for x ∈ BrV is the optimal transport map (up to a measure zero set)  so that

rU

W : Voln(W )=V W2(PU   PW ) = W2(PU   PBrV ) 
(cid:90)

|x − T (x)|2 dPBrV (x).

W2(PU   PBrV ) =

BrV

Complementarily  when 0 < V < Voln(U )  the inﬁmum inf W : Voln(W )=V W2(PU   PW ) = 0  is not
attained by any set. On the other hand  inf W : Voln(W )≥V W2(PU   PW ) = 0 by taking W = U.
Remark 3. Our lower bound in Theorem 4 is (asymptotically) tight. Note that by Theorem 4 
2 (PU   Pf−1(V )) has a (maximum) lower bound of scale (R − rU )2. On the other hand  by
W 2
) = Ω((R − rU )2)  where the equality is by standard
Theorem 5  W 2
algebraic calculations.

2 (PU   Pf−1(V )) ≤ W 2

2 (PU   PBn

R

3.1

Iso-Wasserstein inequality

We believe Theorem 5 is of independent interest itself  as it has the same ﬂavor as the isoperimetric
inequality (See Appendix A for an exact statement.) which arguably is the most important inequality
in metric geometry.
In fact  the ﬁrst statement of Theorem 5 can be restated as the following
inequality:
Theorem 6 (Iso-Wasserstein Inequality). Let Br1  Br2 ⊂ Bn
r1 ≤ r2 centered at the origin. For all measurable A ⊂ Bn

R be two concentric n balls with radii

R with Voln(A) = Voln(Br2)  we have

W2(P(A)  P(Br1)) ≥ W2(P(Br2)  P(Br1 ))

where P(S) denotes a uniform probability distribution on S  i.e. P(S) has density

1

Voln(S) .

Recall that an isoperimetric inequality in Euclidean space roughly says balls have the least perimeter
among all equal volume sets. Theorem 6 acts as a transportation cousin of the isoperimetric inequality.
While the isoperimetric inequality compares n − 1 volume between two sets  the iso-Wasserstein
inequality compares their Wasserstein distances to a small ball. The extrema in both inequalities are
attained by Euclidean balls.

3.2 Simulations

In this section  we demonstrate on a synthetic dataset that our lower bound in Theorem 4 can be
a reasonable guidance for selecting the retrieval neighborhood radius rV   which emphasizes on
high precision. The simulation environment is to compute the optimal rV by minimizing the lower
bound in Theorem 4  with a given relevant neighborhood radius rU and embedding dimension m.
Note that minimizing its lower bound instead of the exact cost itself is beneﬁcial as it avoids the
direct computation of the cost. Recall the lower bound of W2(PU   Pf−1(V )) is (asymptotically)
tight (Remark 3) and matches the its upper bound when n − m (cid:29) 0. If the lower bound behaves
roughly like W2(PU   Pf−1(V ))  our simulation result also serves as an empirical evidence that
W2(PU   Pf−1(V )) weighs more on high precision.
Speciﬁcally  we generate 10000 uniformly distributed samples in a 10-dimensional unit (cid:96)2-ball. We
choose rU such that on average each data point has 500 neighbors inside BrU . We then linearly
project these 10 dimensional points into lower dimensional spaces with embedding dimension m
from 1 to 9. For each m  a different rV is used to calculate discrete precision and recall. This
simulates how optimal rV according to Wasserstein measure changes with respect to m. The result is
shown in on the left in Figure 2. Similarly  we can ﬁx m = 5 and track optimal rV ’s behavior when
rU changes. This is shown on the right in Figure 2.
We evalute our measures based on traditional information retrieval metrics such as f-score. To
compute it  we need the discrete/sample-based precision and recall. As discussed in the introduction 

7

Figure 2: Precision and recall results on uniform samples in a 10 dimensional unit ball. The left
ﬁgure contains precision-recall curves for a ﬁxed rU and the optimal rV is chosen according to
m = 1 ···   9. The right ﬁgure plots the curves for m = 5 and the optimal rV ’s is chosen for
different rU   where rU is indexed by k  the average number of neighbors across all points.

a naive sample based calculations of precision and recall makes P recision = Recall at all times.
We compute them alternatively by discretizing Deﬁnition 1  by ﬁxing radii rU and rV . So each U
and f−1(V ) contain different numbers of neighbors.

P recision =

#(points within rU from x and within rV from y)

#(points within rV from y)

Recall =

#(points within rU from x and within rV from y)

#(points within rU from x)

(4)

(5)

The optimal rV according to the lower bound in Theorem 4 (the blue circle-dash-dotted line) aligns
closely with the optimal f-score with β = 0.3 where β weighted f-score  also known as f-βscore  is:

P recision ∗ Recall

(1 + β2)

β2 ∗ P recision + recall
Note that f-score with β < 1 indeed emphasizes on high precision.
In this provable setting  we have demonstrated our bound’s utility. This shows W2 measures’ potential
for evaluating dimension reduction. In general cases  we won’t have such tight lower bounds and
it is natural to optimize according to the sample based W2 measures instead. We performed some
preliminary experiments on this heuristic  shown in Appendix G.

.

4 Relation to metric space embedding and manifold learning
We lastly situate our work in the lines of research on metric space embedding and manifold learning.
One obvious difference between our work and the literature of metric space embedding and manifold
learning is that our work mainly focuses on intrinsic dimensionality reduction maps  i.e. n (cid:29) m 
while in metric space embedding and manifold learning  having n ≤ m < N is common.
Our work also differs from the literature of metric space embedding and manifold learning in its
learning objective. Learning in these ﬁelds aims to preserve the metric structure of the data. Our work
attempts to preserve precision and recall  a weaker structure in the sense of embedding dimension
(Proposition 2). While they typically look for lowest embedding dimension subject to certain loss
(e.g. smoothness  isometry  etc.)  in contrast  our learning goal is to minimize the loss (precision and
recall etc.) subject to a ﬁxed embedding dimension constraint. In these cases  desired structures will
break (Theorem 3) because we cannot choose the embedding dimension m (e.g. for visualizations
m = 2; for classiﬁcations m = number of classes).

8

2

p

We now discuss the technical relations with metric space embedding and manifold learning. Many
datasets can be modelled as a ﬁnite metric space Mk with k points. A natural unsupervised learning
task is to learn an embedding that approximately preserves pairwise distances. The Bourgain
embedding [7] guarantees the metric structure can be preserved with distortion O(log k) in lO(log2 k)
.
When the samples are collected in Euclidean spaces  i.e. Mk ⊂ l2  the Johnson-Lindenstrauss lemma
[10] improves the distortion to (1 + ) in lO(log(k/2))
. These embeddings approximately preserve all
pairwise distances - global metric structure of Mk is compatible to the ambient vector space norms.
Coming back to our work  it is natural to mimic this approach for precision and recall in Mk. The
ﬁrst problem is that the naive sample based precision and recall are always equal (Section 3.2). A
second problem is discrete precision and recall is a non-differentiable objective. In fact  the difﬁculty
of analyzing discrete precision and recall motivates us to look for continuous analogues.
Roughly  our approach is somewhat similar to manifold learning where researchers postulate that the
data Mk are sampled from a continuous manifold M  typically a smooth or Riemannian manifold
M with intrinsic dimension n. In this setting  one is interested in embedding M into l2 locally
isometrically. Then one designs learning algorithms that can combine the local information to learn
some global structure of M. By relaxing to the continuous cases just like our setting  manifold
learning researchers gain access to vast literature in geometry. By the Whitney embedding [25] 
M can be smoothly embedded into R2n. By the Nash embedding [35]  a compact Riemannian
manifold M can be isometrically embedded into Rp(n)  where p(n) is a quadratic polynomial. Hence
the task in manifold learning is wellposed: one seeks an embedding f : M ⊂ RN → Rm with
m ≤ 2n (cid:28) N in the smooth category or m ≤ p(n) (cid:28) N in the Riemannian category. Note that
the embedded manifold metrics (e.g. the Riemannian geodesic distances) are not guaranteed to be
compatible to the ambient vector space’s norm structure with a ﬁxed distortion factor  unlike the
Bourgain embedding or the Johnson-Lindenstrauss lemma in the discrete setting. A continuous
analogue of the norm compatible discrete metric space embeddings is the Kuratowski embedding 
which embeds global-isometrically (preserving pairwise distance) any metric space to an inﬁnite
dimensional Banach space L∞. With  distortion relaxation  it is possible to embed a compact
Riemannian manifold to a ﬁnite dimensional normed space. But this appears to be very hard  in that
the embedding dimension may grow faster than exponentially in n [30].
Like DR in manifold learning and unlike DR in discrete metric space embedding  rather than global
structure we want to preserve local notions such as precision and recall. Unlike DR in manifold
learning  since precision and recall are almost equivalent to continuity and injectivity (Theorem 1) 
we are interested in embeddings in the topological category  instead of the smooth or the Riemannian
category. Thus  our work can be considered as manifold learning from the perspective of information
retrieval  which leads to the following result.
Proposition 2. If m ≥ 2n  where n is the dimension of the data manifold M in domain and m is the
dimension of codomain Rm  then there exists a continuous map f : M → Rm such that f achieves
perfect precision and recall for every point x ∈ M.

Note that the dimension reduction rate is actually much stronger than the case of Riemannian
isometric embedding where the lowest embedding dimension grows polynomially [35]. This is
because preserving precision and recall is weaker than isometric embedding. A practical implication
is that  we can reduce many more dimensions if we only care about precision and recall.

5 Conclusions

We characterized the imperfection of dimensionality reduction mappings from a quantitative topology
perspective. We showed that perfect precision and perfect recall cannot be both achieved by any DR
map. We then proved a non-trivial upper bound for precision for Lipschitz continuous DR maps. To
further quantify the distortion  we proposed a new measure based on L2-Wasserstein distances  and
also proved its lower bound for Lipschitz continuous DR maps. It is also interesting to analyse the
relation between the recall of a continuous DR map and its modulus of continuity. However  the
generality and complexiity of the ﬁbers (inverse images) of these maps so far defy our effort and this
problem remains open. Furthermore  it is interesting to develop a corresponding theory in the discrete
setting.

9

Acknowledgments

We would like to thank Yanshuai Cao  Christopher Srinivasa  and the broader Borealis AI team for
their discussion and support. We also thank Marcus Brubaker  Cathal Smyth  and Matthew E. Taylor
for proofreading the manuscript and their suggestions  as well as April Cooper for creating graphics
for this work.

References
[1] Arseniy Akopyan and Roman Karasev. A tight estimate for the waist of the ball. Bulletin of the

London Mathematical Society  49(4):690–693  2017.

[2] Arseniy Akopyan and Roman Karasev. Waist of balls in hyperbolic and spherical spaces.
International Mathematics Research Notices  page rny037  2018. doi: 10.1093/imrn/rny037.
URL http://dx.doi.org/10.1093/imrn/rny037.

[3] Hannah Alpert and Larry Guth. A family of maps with many small ﬁbers. Journal of Topology

and Analysis  7(01):73–79  2015.

[4] Jason Altschuler  Jonathan Weed  and Philippe Rigollet. Near-linear time approximation
algorithms for optimal transport via sinkhorn iteration. In Advances in Neural Information
Processing Systems  pages 1961–1971  2017.

[5] Sanjeev Arora  Wei Hu  and Pravesh K. Kothari. An analysis of the t-sne algorithm for data
visualization. In Sébastien Bubeck  Vianney Perchet  and Philippe Rigollet  editors  Proceedings
of the 31st Conference On Learning Theory  volume 75 of Proceedings of Machine Learning
Research  pages 1455–1462. PMLR  06–09 Jul 2018. URL http://proceedings.mlr.
press/v75/arora18a.html.

[6] Nicolas Bonneel  Michiel Van De Panne  Sylvain Paris  and Wolfgang Heidrich. Displacement
In ACM Transactions on Graphics (TOG) 

interpolation using lagrangian mass transport.
volume 30  page 158. ACM  2011.

[7] Jean Bourgain. On Lipschitz embedding of ﬁnite metric spaces in Hilbert space. Israel Journal

of Mathematics  52(1):46–52  1985.

[8] Christos Boutsidis  Anastasios Zouzias  and Petros Drineas. Random projections for k-means

clustering. In NIPS  pages 298–306  2010.

[9] Luis A Caffarelli and Robert J McCann. Free boundaries in optimal transport and Monge-

Ampére obstacle problems. Annals of mathematics  171:673–730  2010.

[10] Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of Johnson and

Lindenstrauss. Random Structures & Algorithms  22(1):60–65  2003.

[11] Alessio Figalli. The optimal partial transport problem. Archive for rational mechanics and

analysis  195(2):533–560  2010.

[12] Rémi Flamary and Nicolas Courty. Pot python optimal transport library  2017. URL https:

//github.com/rflamary/POT.

[13] Daniele Granata and Vincenzo Carnevale. Accurate estimation of the intrinsic dimension using
graph distances: Unraveling the geometric complexity of datasets. Scientiﬁc Reports  6  2016.

[14] Victor Guillemin and Alan Pollack. Differential topology  volume 370. American Mathematical

Soc.  2010.

[15] LARRY Guth. The waist inequality in gromov’s work. The Abel Prize 2008  pages 181–195 

2012.

[16] Gísli R. Hjaltason and Hanan Samet. Properties of embedding methods for similarity searching
in metric spaces. IEEE Trans. Pattern Anal. Mach. Intell.  25(5):530–549  May 2003. ISSN
0162-8828. doi: 10.1109/TPAMI.2003.1195989. URL https://doi.org/10.1109/TPAMI.
2003.1195989.

10

[17] Bo’az Klartag. Convex geometry and waist inequalities. Geometric and Functional Analysis 

27(1):130–164  2017.

[18] Jonathan Korman and Robert J McCann. Insights into capacity-constrained optimal transport.

Proceedings of the National Academy of Sciences  110(25):10064–10067  2013.

[19] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. Nature  521(7553):436 

2015.

[20] Sylvain Lespinats and Michaël Aupetit. Checkviz: Sanity check and topological clues for linear
and non-linear mappings. In Computer Graphics Forum  volume 30  pages 113–125. Wiley
Online Library  2011.

[21] Laurens Maaten. Learning a parametric embedding by preserving local structure. In Artiﬁcial

Intelligence and Statistics  pages 384–391  2009.

[22] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine

Learning Research  9(Nov):2579–2605  2008.

[23] Rafael Messias Martins  Danilo Barbosa Coimbra  Rosane Minghim  and Alexandru C Telea.
Visual analysis of dimensionality reduction quality for parameterized projections. Computers &
Graphics  41:26–42  2014.

[24] Robert J McCann and Adam M Oberman. Exact semi-geostrophic ﬂows in an elliptical ocean

basin. Nonlinearity  17(5):1891  2004.

[25] James McQueen  Marina Meila  and Dominique Joncas. Nearly isometric embedding by

relaxation. In NIPS  pages 2631–2639  2016.

[26] Michael Müger. A remark on the invariance of dimension. Mathematische Semesterberichte 

62(1):59–68  2015.

[27] Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis.

In NIPS  pages 1786–1794  2010.

[28] Lawrence E Payne.

453–488  1967.

Isoperimetric inequalities and their applications. SIAM review  9(3):

[29] P Rayón and M Gromov. Isoperimetry of waists and concentration of maps. Geometric &

Functional Analysis GAFA  13(1):178–215  2003.

[30] Malte Roeer. On the ﬁnite dimensional approximation of the Kuratowski-embedding for

compact manifolds. arXiv preprint arXiv:1305.1529  2013.

[31] Bernhard Schölkopf  Alexander Smola  and Klaus-Robert Müller. Kernel principal component
analysis. In International Conference on Artiﬁcial Neural Networks  pages 583–588. Springer 
1997.

[32] Tobias Schreck  Tatiana Von Landesberger  and Sebastian Bremm. Techniques for precision-

based visual analysis of projected data. Information Visualization  9(3):181–193  2010.

[33] Joshua B Tenenbaum  Vin De Silva  and John C Langford. A global geometric framework for

nonlinear dimensionality reduction. Science  290(5500):2319–2323  2000.

[34] Jarkko Venna  Jaakko Peltonen  Kristian Nybo  Helena Aidos  and Samuel Kaski. Information
retrieval perspective to nonlinear dimensionality reduction for data visualization. Journal of
Machine Learning Research  11(Feb):451–490  2010.

[35] Nakul Verma. Distance preserving embeddings for general n-dimensional manifolds. Journal

of Machine Learning Research  14(1):2415–2448  2013.

[36] Xianfu Wang. Volumes of generalized unit balls. Mathematics Magazine  78(5):390–395  2005.

11

,Kry Lui
Gavin Weiguang Ding
Ruitong Huang
Robert McCann