2017,Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data,We present a factorized hierarchical variational autoencoder  which learns disentangled and interpretable representations from sequential data without supervision. Specifically  we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate  qualitatively  its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively  its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35% in mismatched train/test scenarios for automatic speech recognition tasks.,Unsupervised Learning of Disentangled and

Interpretable Representations from Sequential Data

Wei-Ning Hsu  Yu Zhang  and James Glass

Computer Science and Artiﬁcial Intelligence Laboratory

Massachusetts Institute of Technology

Cambridge  MA 02139  USA

{wnhsu yzhang87 glass}@csail.mit.edu

Abstract

We present a factorized hierarchical variational autoencoder  which learns disen-
tangled and interpretable representations from sequential data without supervision.
Speciﬁcally  we exploit the multi-scale nature of information in sequential data by
formulating it explicitly within a factorized hierarchical graphical model that im-
poses sequence-dependent priors and sequence-independent priors to different sets
of latent variables. The model is evaluated on two speech corpora to demonstrate 
qualitatively  its ability to transform speakers or linguistic content by manipulating
different sets of latent variables; and quantitatively  its ability to outperform an
i-vector baseline for speaker veriﬁcation and reduce the word error rate by as much
as 35% in mismatched train/test scenarios for automatic speech recognition tasks.

1

Introduction

Unsupervised learning is a powerful methodology that can leverage vast quantities of unannotated
data in order to learn useful representations that can be incorporated into subsequent applications in
either supervised or unsupervised fashions. One of the principle approaches to unsupervised learning
is probabilistic generative modeling. Recently  there has been signiﬁcant interest in three classes of
deep probabilistic generative models: 1) Variational Autoencoders (VAEs) [23  34  22]  2) Generative
Adversarial Networks (GANs) [11]  and 3) auto-regressive models [30  39]; more recently  there are
also studies combining multiple classes of models [6  27  26]. While GANs bypass any inference of
latent variables  and auto-regressive models abstain from using latent variables  VAEs jointly learn an
inference model and a generative model  allowing them to infer latent variables from observed data.
Despite successes with VAEs  understanding the underlying factors that latent variables associate
with is a major challenge. Some research focuses on the supervised or semi-supervised setting using
VAEs [21  17]. There is also research attempting to develop weakly supervised or unsupervised
methods to learn disentangled representations  such as DC-IGN [25]  InfoGAN [1]  and β-VAE [13].
There is yet another line of research analyzing the latent variables with labeled data after the model
is trained [33  15]. While there has been much research investigating static data  such as the
aforementioned ones  there is relatively little research on learning from sequential data [8  3  2  9  7 
18  36]. Moreover  to the best of our knowledge  there has not been any attempt to learn disentangled
and interpretable representations without supervision from sequential data. The information encoded
in sequential data  such as speech  video  and text  is naturally multi-scaled; in speech for example 
information about the channel  speaker  and linguistic content is encoded in the statistics at the
session  utterance  and segment levels  respectively. By leveraging this source of constraint  we can
learn disentangled and interpretable factors in an unsupervised manner.
In this paper  we propose a novel factorized hierarchical variational autoencoder  which learns
disentangled and interpretable latent representations from sequential data without supervision by

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: FHVAE (α = 0) decoding results of three combinations of latent segment variables z1 and
latent sequence variables z2 from two utterances in Aurora-4: a clean one (top-left) and a noisy one
(bottom-left). FHVAEs learn to encode local attributes  such as linguistic content  into z1  and encode
global attributes  such as noise level  into z2. Therefore  by replacing z2 of a noisy utterance with
z2 of a clean utterance  an FHVAE decodes a denoised utterance (middle-right) that preserves the
linguistic content. Reconstruction results of the clean and noisy utterances are also shown on the
right. Audio samples are available at https://youtu.be/naJZITvCfI4.

explicitly modeling the multi-scaled information with a factorized hierarchical graphical model. The
inference model is designed such that the model can be optimized at the segment level  instead
of at the sequence level  which may cause scalability issues when sequences become too long. A
sequence-to-sequence neural network architecture is applied to better capture temporal relationships.
We evaluate the proposed model on two speech datasets. Qualitatively  the model demonstrates an
ability to factorize sequence-level and segment-level attributes into different sets of latent variables.
Quantitatively  the model achieves 2.38% and 1.34% equal error rate on unsupervised and supervised
speaker veriﬁcation tasks respectively  which outperforms an i-vector baseline. On speech recognition
tasks  it reduces the word error rate in mismatched train/test scenarios by up to 35%.
The rest of the paper is organized as follows. In Section 2  we introduce our proposed model  and
describe the neural network architecture in Section 3. Experimental results are reported in Section 4.
We discuss related work in Section 5  and conclude our work as well as discuss future research plans
in Section 6. We have released the code for the model described in this paper.1

2 Factorized Hierarchical Variational Autoencoder

Generation of sequential data  such as speech  often involves multiple independent factors operating
at different time scales. For instance  the speaker identity affects fundamental frequency (F0) and
volume at the sequence level  while phonetic content only affects spectral contour and durations of
formants at the segmental level. This multi-scale behavior results in the fact that some attributes 
such as F0 and volume  tend to have a smaller amount of variation within an utterance  compared to
between utterances; while other attributes  such as phonetic content  tend to have a similar amount of
variation within and between utterances.
We refer to the ﬁrst type of attributes as sequence-level attributes  and the other as segment-level
attributes. In this work  we achieve disentanglement and interpretability by encoding the two types of
attributes into latent sequence variables and latent segment variables respectively  where the former
is regularized by an sequence-dependent prior and the latter by an sequence-independent prior.
We now formulate a generative process for speech and propose our Factorized Hierarchical Variational
Autoencoder (FHVAE). Consider some dataset D = {X (i)}M
i=1 consisting of M i.i.d. sequences 
where X (i) = {x(i n)}N (i)
n=1 is a sequence of N (i) observed variables. N (i) is referred to as the

1https://github.com/wnhsu/FactorizedHierarchicalVAE

2

(a) Generative Model

(b) Inference Model

Figure 2: Graphical illustration of the proposed generative model and inference model. Grey nodes
denote the observed variables  and white nodes are the hidden variables.

N(cid:89)

number of segments for the i-th sequence  and x(i n) is referred to as the n-th segment of the i-th
sequence. Note that here a “segment” refers to a variable of smaller temporal scale compared to the
“sequence”  which is in fact a sub-sequence. We will drop the index i whenever it is clear that we are
referring to terms associated with a single sequence. We assume that each sequence X is generated
from some random process involving the latent variables Z1 = {z(n)
n=1  and
µ2. The following generation process as illustrated in Figure 2(a) is considered: (1) a s-vector
2 }N
µ2 is drawn from a prior distribution pθ(µ2); (2) N i.i.d.
and latent segment variables {z(n)
n=1 are drawn from a sequence-dependent prior distribution
pθ(z2|µ2) and a sequence-independent prior distribution pθ(z1) respectively; (3) N i.i.d. observed
variables {x(n)}N
n=1 are drawn from a conditional distribution pθ(x|z1  z2). The joint probability
for a sequence is formulated in Eq. 1:

2 }N
latent sequence variables {z(n)

n=1  Z2 = {z(n)

1 }N

1 }N

n=1

pθ(X  Z1  Z2  µ2) = pθ(µ2)

pθ(x(n)|z(n)

1

  z(n)

2

)pθ(z(n)

1

)pθ(z(n)

2

|µ2).

(1)

Speciﬁcally  we formulate each of the RHS term as follows:

n=1

pθ(z1) = N (z1|0  σ2

z1

pθ(x|z1  z2) = N (x|fµx (z1  z2)  diag(fσ2
I) 

I)  pθ(z2|µ2) = N (z2|µ2  σ2

x

z2

(z1  z2)))

pθ(µ2) = N (µ2|0  σ2

I) 

µ2

where the priors over the s-vectors µ2 and the latent segment variables z1 are centered isotropic
multivariate Gaussian distributions. The prior over the latent sequence variable z2 conditioned on µ2
is an isotropic multivariate Gaussian centered at µ2. The conditional distribution of the observed
variable x is the multivariate Gaussian with a diagonal covariance matrix  whose mean and diagonal
variance are parameterized by neural networks fµx (· ·) and fσ2
(· ·) with input z1 and z2. We use θ
to denote the set of parameters in the generative model.
This generative model is factorized in a way such that the latent sequence variables z2 within a
sequence are forced to be close to µ2 as well as to each other in Euclidean distance  and therefore are
encouraged to encode sequence-level attributes that may have larger variance across sequences  but
smaller variance within sequences. The constraint to the latent segment variables z1 is imposed glob-
ally  and therefore encourages encoding of residual attributes whose variation is not distinguishable
inter and intra sequences.
In the variational autoencoder
tractable  an inference model  qφ(Z(i)
pθ(Z(i)
inference model as Figure 2(b):

since the exact posterior
in-
2 |X (i))  that approximates the true posterior 
2 |X (i))  for variational inference [19] is introduced. We consider the following

framework 
1   Z(i)

inference is

1   Z(i)

2   µ(i)

2   µ(i)

x

qφ(Z(i)

qφ(µ(i)

2   µ(i)
1   Z(i)
2 ) = N (µ(i)

2 |X (i)) = qφ(µ(i)
2 )
2 |gµµ2

(i)  σ2
˜µ2

I) 

qφ(z1|x  z2) = N (z1|gµz1

qφ(z(i n)

N (i)(cid:89)
|x(i n)  z(i n)
qφ(z2|x) = N (z2|gµz2
(x  z2)  diag(gσ2
z1

n=1

1

2

(x  z2))) 

)qφ(z(i n)

2

|x(i n))

(x)  diag(gσ2
z2

(x)))

3

Mµ(i)2N(i)x(i n)z(i n)1z(i n)2✓Mµ(i)2N(i)x(i n)z(i n)1z(i n)2where the posteriors over µ2  z1  and z2 are all multivariate diagonal Gaussian distributions. Note that
the mean of the posterior distribution of µ2 is not directly inferred from X  but instead is regarded as
part of inference model parameters  with one for each utterance  which would be optimized during
training. Therefore  gµµ2
(i) to denote
the posterior mean of µ2 for the i-th sequence; we ﬁx the posterior covariance matrix of µ2 for all
(· ·) are also neural
sequences. Similar to the generative model  gµz2
(·) are denoted collectively by φ. The variational lower
networks whose parameters along with gµµ2
bound for this inference model on the marginal likelihood of a sequence X is derived as follows:

(·) can be seen as a lookup table  and we use ˜µ(i)
(· ·)  and gσ2

(·)  gµz1

2 = gµµ2

(·)  gσ2

z1

z2

N(cid:88)

n=1

L(θ  φ; X) =
L(θ  φ; x(n)|˜µ2) =E

L(θ  φ; x(n)|˜µ2) + log pθ(˜µ2) + const

)(cid:3)

(cid:2) log pθ(x(n)|z(n)
(cid:2)DKL(qφ(z(n)

1

|x(n))||pθ(z(n)

2

2

1

  z(n)
|x(n)  z(n)
|˜µ2)).

2

))(cid:3)

)||pθ(z(n)

1

|x(n))

2

1

 z(n)

qφ(z(n)
− E
|x(n))
− DKL(qφ(z(n)

qφ(z(n)

2

2

The detailed derivation can be found in Appendix A. Because the approximated posterior of µ2 does
not depend on the sequence X  the sequence variational lower bound L(θ  φ; X) can be decomposed
into the sum of L(θ  φ; x(n)|˜µ2)  the conditional segment variational lower bounds  over segments 
plus the log prior probability of ˜µ2 and a constant. Therefore  instead of sampling a batch at the
sequence level to maximize the sequence variational lower bound  we can sample a batch at the
segment level to maximize the segment variational lower bound:

L(θ  φ; x(n)) = L(θ  φ; x(n)|˜µ2) +

1
N

log pθ(˜µ2) + const.

(2)

This approach provides better scalability when the sequences are extremely long  such that computing
an entire sequence for a batched update is too computationally expensive.
In this paper we only introduce two scales of attributes; however  one can easily extend this model
to more scales by simply introducing µk for k = 2  3 ··· 2 that constrains the prior distribution of
latent variables at more scales  such as having session-dependent prior or dataset-dependent prior.

2.1 Discriminative Objective

The idea of having sequence-speciﬁc priors for each sequence is to encourage the model to encode
the sequence-level attributes and the segment-level attributes into different sets of latent variables.
However  when µ2 = 0 for all sequences  the prior probability of the s-vector is maximized  and the
KL-divergence of the inferred posterior of z2 is measured from the same conditional prior for all
sequences. This would result in trivial s-vectors µ2  and therefore z1 and z2 would not be factorized
to encode sequence and segment attributes respectively.
To encourage z2 to encode sequence-level attributes  we use z(i n)
infer the sequence index i of x(i n). We formulate the discriminative objective as:

  which is inferred from x(i n)  to

2

log p(i|z(i n)

2

) = log p(z(i n)

2

|i) − log

|j)

(p(i) is assumed uniform)

M(cid:88)
2 ) − log(cid:0) M(cid:88)

j=1

2

p(z(i n)

2 )(cid:1) 

|˜µ(j)

pθ(z(i n)

2

:= log pθ(z(i n)

2

|˜µ(i)

j=1

Combining the discriminative objective using a weighting parameter α with the segment variational
lower bound  the objective function to maximize then becomes:

Ldis(θ  φ; x(i n)) = L(θ  φ; x(i n)) + α log p(i|z(i n)

2
which we refer to as the discriminative segment variational lower bound.

) 

2The index starts from 2 because we do not introduce the hierarchy to z1.

(3)

4

Inferring S-Vectors During Testing

2.2
During testing  we may want to use the s-vector µ2 of an unseen sequence ˜X = {˜x(n)} ˜N
n=1 as
the sequence-level attribute representation for tasks such as speaker veriﬁcation. Since the exact
maximum a posterior estimation of µ2 is intractable  we approximate the estimation using the
conditional segment variational lower bound as follows:

log pθ(µ2| ˜X) = argmax

log pθ( ˜X  µ2)

µ2

log pθ(˜x(n)|µ2)(cid:1) + log pθ(µ2)

µ∗
2 = argmax

µ2

= argmax

µ2

≈ argmax

µ2

(cid:0) ˜N(cid:88)
˜N(cid:88)

n=1

n=1

(cid:80) ˜N

µ∗
2 =

(˜x(n))

n=1 gµz2
˜N + σ2

z2/σ2
µ2

L(θ  φ; ˜x(n)|µ2) + log pθ(µ2).

(4)

The closed form solution of µ∗

2 can be derived by differentiating Eq. 4 w.r.t. µ2 (see Appendix B):

.

(5)

3 Sequence-to-Sequence Autoencoder Model Architecture

In this section  we introduce the detailed neural network architectures for our proposed FHVAE. Let
a segment x = x1:T be a sub-sequence of X that contains T time steps  and xt denotes the t-th time
step of x. We use recurrent network architectures for encoders that capture the temporal relationship
among time steps  and generate a summarized ﬁxed-dimension vector after consuming an entire
sub-sequence. Likewise  we adopt a recurrent network architecture that generates a frame step by
step conditioned on the latent variables z1 and z2. The complete network can be seen as a stochastic
sequence-to-sequence autoencoder that encodes x1:T stochastically into z1  z2  and stochastically
decodes from them back to x1:T .

Figure 3: Sequence-to-sequence factorized hierarchical variational autoencoder. Dashed lines indicate
the sampling process using the reparameterization trick [23]. The encoders for z1 and z2 are pink
and amber  respectively  while the decoder for x is blue. Darker colors denote the recurrent neural
networks  while lighter colors denote the fully-connected layers predicting the mean and log variance.

Figure 3 shows our proposed Seq2Seq-FHVAE architecture.3 Here we show the detailed formulation:

(hz2 t  cz2 t) = LSTM(xt−1  hz2 t−1  cz2 t−1; φLSTM z2)
qφ(z2|x1:T ) = N (z2| MLP(hz2 T ; φMLPµ z2)  diag(exp(MLP(hz2 T ; φMLPσ2  z2 ))))
(hz1 t  cz1 t) = LSTM([xt−1; z2]  hz1 t−1  cz1 t−1; φz1)
qφ(z1|x1:T   z2) = N (z1| MLP(hz1 T ; φMLPµ z1)  diag(exp(MLP(hz1 T ; φMLPσ2  z1 ))))
(hx t  cx t) = LSTM([z1; z2]  hx t−1  cx t−1; φx)
pθ(xt|z1  z2) = N (xt| MLP(hx t; φMLPµ x)  diag(exp(MLP(hx t; φMLPσ2  x)))) 

where LSTM refers to a long short-term memory recurrent neural network [14]  and MLP refers to a
multi-layer perceptron  φ∗ are the related weight matrices. None of the neural network parameters
are shared. We refer to this model as Seq2Seq-FHVAE. Log-likelihood and qualitative comparison
with alternative architectures can be found in Appendix D.

3Best viewed in color.

5

x1p(x1|z1  z2)…z2q(z1|x1:T  z2)…z1EncoderDecoderx2x3xTx1x2p(x2|z1  z2)p(xT|z1  z2)q(z2|x1:T)xT…4 Experiments

We use speech  which inherently contains information at multiple scales  such as channel  speaker 
and linguistic content to test our model. Learning to disentangle the mixed information from the
surface representation is essential for a wide variety of speech applications: for example  noise robust
speech recognition [41  38  37  16]  speaker veriﬁcation [5]  and voice conversion [40  29  24].
The following two corpora are used for our experiments: (1) TIMIT [10]  which contains broadband
16kHz recordings of phonetically-balanced read speech. A total of 6300 utterances (5.4 hours) are
presented with 10 sentences from each of 630 speakers  of which approximately 70% are male and
30% are female. (2) Aurora-4 [32]  a broadband corpus designed for noisy speech recognition tasks
based on the Wall Street Journal corpus (WSJ0) [31]. Two microphone types  CLEAN/CHANNEL
are included  and six noise types are artiﬁcially added to both microphone types  which results in
four conditions: CLEAN  CHANNEL  NOISY  and CHANNEL+NOISY. Two 14 hour training sets are
used  where one is clean and the other is a mix of all four conditions. The same noise types and
microphones are used to generate the development and test sets  which both consist of 330 utterances
from all four conditions  resulting in 4 620 utterances in total for each set.
All speech is represented as a sequence of 80 dimensional Mel-scale ﬁlter bank (FBank) features
or 200 dimensional log-magnitude spectrum (only for audio reconstruction)  computed every 10ms.
Mel-scale features are a popular auditory approximation for many speech applications [28]. We
consider a sample x to be a 200ms sub-sequence  which is on the order of the length of a syllable 
and implies T = 20 for each x. For the Seq2Seq-FHVAE model  all the LSTM and MLP networks
are one-layered  and Adam [20] is used for optimization. More details of the model architecture and
training procedure can be found in Appendix C.

4.1 Qualitative Evaluation of the Disentangled Latent Variables

Figure 4: (left) Examples generated by varying different latent variables. (right) An illustration
of harmonics and formants in ﬁlter bank images. The green block ‘A’ contains four reconstructed
examples. The red block ‘B’ contains ten original sequences on the ﬁrst row with the corresponding
reconstructed examples on the second row. The entry on the i-th row and the j-th column in the blue
block ‘C’ is the reconstructed example using the latent segment variable z1 of the i-th row from block
‘A’ and the latent sequence variable z2 of the j-th column from block ‘B’.

To qualitatively study the factorization of information between the latent segment variable z1 and the
latent sequence variable z2  we generate examples x by varying each of them respectively. Figure 4
shows 40 examples in block ‘C’ of all the combinations of the 4 latent segment variables extracted
from block ‘A’ and the 10 latent sequence variables extracted from block ‘B.’ The top two examples
from block ‘A’ and the ﬁve leftmost examples from block ‘B’ are from male speakers  while the rest
are from female speakers  which show higher fundamental frequencies and harmonics.4

4The harmonics corresponds to horizontal dark stripes in the ﬁgure; the more widely these stripes are spaced

vertically  the higher the fundamental frequency of the speaker is.

6

Figure 5: FHVAE (α = 0) decoding results of three combinations of latent segment variables z1
and latent sequence variables z2 from one male-speaker utterance (top-left) and one female-speaker
utterance (bottom-left) in Aurora-4. By replacing z2 of a male-speaker utterance with z2 of a female-
speaker utterance  an FHVAE decodes a voice-converted utterance (middle-right) that preserves the
linguistic content. Audio samples are available at https://youtu.be/VMX3IZYWYdg.

We can observe that along each row in block ‘C’  the linguistic phonetic-level content  which manifests
itself in the form of the spectral contour and temporal position of formants  as well as the relative
position between formants  is very similar between elements; the speaker identity however changes
(e.g.  harmonic structure). On the other hand  for each column we see that the speaker identity remains
consistent  despite the change of linguistic content. The factorization of the sequence-level attributes
and the segment-level attributes of our proposed Seq2Seq-FHVAE is clearly evident. In addition  we
also show examples of modifying an entire utterance in Figure 1 and 5  which achieves denoising
by replacing the latent sequence variable of a noisy utterance with those of a clean utterance  and
achieves voice conversion by replacing the latent sequence variable of one speaker with that of
another speaker. Details of the operations we applied to modify an entire utterance as well as more
larger-sized examples of different α values can be found in Appendix E. We also show extra latent
space traversal experiments in Appendix H.

4.2 Quantitative Evaluation of S-Vectors – Speaker Veriﬁcation

n=1 gµz1

(˜x(n))/( ˜N + σ2
z1

µ1 = (cid:80) ˜N

To quantify the performance of our model on disentangling the utterance-level attributes from the
segment-level attributes  we present experiments on a speaker veriﬁcation task on the TIMIT corpus
to evaluate how well the estimated µ2 encodes speaker-level information.5 As a sanity check  we
modify Eq. 5 to estimate an alternative s-vector based on latent segment variables z1 as follows:
). We use the i-vector method [5] as the baseline  which is
the representation used in most state-of-the-art speaker veriﬁcation systems. They are in a low
dimensional subspace of the Gaussian mixture model (GMM) mean supervector space  where the
GMM is the universal background model (UBM) that models the generative process of speech.
I-vectors  µ1  and µ2 can all be extracted without supervision; when speaker labels are available
during training  techniques such as linear discriminative analysis (LDA) can be applied to further
improve the linear separability of the representation. For all experiments  we use the fast scoring
approach in [4] that uses cosine similarity as the similarity metric and compute the equal error rate
(EER). More details about the experimental settings can be found in Appendix F.
We compare different dimensions for both features as well as different α’s in Eq.3 for training
FHVAE models. The results in Table 1 show that the 16 dimensional s-vectors µ2 outperform i-vector
baselines in both unsupervised (Raw) and supervised (LDA) settings for all α’s as shown in the fourth
column; the more discriminatively the FHVAE model is trained (i.e.  with larger α)  the better speaker

5TIMIT is not a standard corpus for speaker veriﬁcation  but it is a good corpus to show the utterance-level
attribute we learned via this task  because the main attribute that is consistent within an utterance is speaker
identity  while in Aurora-4 both speaker identity and the background noise are consistent within an utterance.

7

veriﬁcation results it achieves. Moreover  with the appropriately chosen dimension  a 32 dimensional
µ2 reaches an even lower EER at 1.34%. On the other hand  the negative results of using µ1 also
validate the success in disentangling utterance and segment level attributes.

Table 1: Comparison of speaker veriﬁcation equal error rate (EER) on the TIMIT test set

i-vector

Features Dimension α
-
-
-
0
10−1
100
101
101

48
100
200
16
16
16
16
32
16
16
32

µ2

µ1

Raw

LDA (12 dim) LDA (24 dim)

10.12% 6.25%
6.10%
9.52%
9.82%
6.54%
4.02%
5.06%
4.61%
4.91%
3.87%
3.86%
2.38% 2.08%
2.38% 2.08%
22.77% 15.62%
27.68% 22.17%
22.47% 16.82%

5.95%
5.50%
6.10%
-
-
-
-
1.34%
-
-
17.26%

100
101
101

4.3 Quantitative Evaluation of the Latent Segment Variables – Domain Invariant ASR

Speaker adaptation and robust speech recognition in automatic speech recognition (ASR) can often
be seen as domain adaptation problems  where available labeled data is limited and hence the data
distributions during training and testing are mismatched. One approach to reduce the severity of this
issue is to extract speaker/channel invariant features for the tasks.
As demonstrated in Section 4.2  the s-vector contains information about domains. Here we evaluate
if the latent segment variables contains domain invariant linguistic information by evaluating on
an ASR task: (1) train our proposed Seq2Seq-FHVAE using FBank feature on a set that covers
different domains. (2) train an LSTM acoustic model [12  35  42] on the set that only covers partial
domains using mean and log variance of the latent segment variable z1 extracted from the trained
Seq2Seq-FHVAE. (3) test the ASR system on all domains. As a baseline  we also train the same ASR
models but use the FBank features alone. Detailed conﬁgurations are in Appendix G.
For TIMIT we assume that male and female speakers constitute different domains  and show the
results in Table 2. The ﬁrst row of results shows that the ASR model trained on all domains (speakers)
using FBank features as the upper bound. When trained on only male speakers  the phone error rate
(PER) on female speakers increases by 16.1% for FBank features; however  for z1  despite the slight
degradation on male speakers  the PER on the unseen domain  which are female speakers  improves
by 6.6% compared to FBank features.

Table 2: TIMIT test phone error rate of acoustic models trained on different features and sets

Train Set and Conﬁguration

Test PER by Set

ASR

Train All

Train Male

FHVAE

Features Male

Female All

-
-
Train All  α = 10

FBank
FBank
z1

20.1% 16.7% 19.1%
21.0% 32.8% 25.2%
22.0% 26.2% 23.5%

On Aurora-4  four domains are considered  which are clean  noisy  channel  and noisy+channel (NC
for short). We train the FHVAE on the development set for two purposes: (1) the FHVAE can be
considered as a general feature extractor  which can be trained on an arbitrary collection of data that
does not necessarily include the data for subsequent applications. (2) the dev set of Aurora-4 contains
the domain label for each utterance so it is possible to control which domain has been observed by the
FHVAE. Table 3 shows the word error rate (WER) results on Aurora-4  from which we can observe
that the FBank representation suffers from severe domain mismatch problems; speciﬁcally  the WER

8

increases by 53.3% when noise is presented in mismatched microphone recordings (NC). In contrast 
when the FHVAE is trained on data from all domains  using the latent segment variables as features
reduce WER from 16% to 35% compare to baseline on mismatched domains  with less than 2%
WER degradation on the matched domain. In addition  β-VAEs [13] are trained on the same data as
the FHVAE to serve as the baseline feature extractor  from which we extract the latent variables z as
the ASR feature and show the result in the third to the sixth rows. The β-VAE features outperform
FBank in all mismatched domains  but are inferior to the latent segment variable z1 from the FHVAE
in those domains. The results demonstrate the importance of learning not only disentangled  but also
interpretable representations  which can be achieved by our proposed FHVAE models. As a sanity
check  we replace z1 with z2  the latent sequence variable and train an ASR  which results in terrible
WER performance as shown in the eighth row as expected.
Finally  we train another FHVAE on all domains excluding the combinatory NC domain  and shows
the results in the last row in Table 3.
It can be observed that the latent segment variable still
outperforms the baseline feature with 30% lower WER on noise and channel combined data  even
though the FHAVE has only seen noise and channel variation independently.

Table 3: Aurora-4 test word error rate of acoustic models trained on different features and sets

ASR

Train All

Train Set and Conﬁguration
{FH- β-}VAE
-
-
Dev  β = 1
Dev  β = 2
Dev  β = 4
Dev  β = 8
Dev  α = 10
Dev  α = 10
Dev\NC  α = 10

Train Clean

Features

Clean

Noisy

Channel NC

All

Test WER by Set

FBank
FBank
z (β-VAE)
z (β-VAE)
z (β-VAE)
z (β-VAE)
z1 (FHVAE)
z2 (FHVAE)
z1 (FHVAE)

8.24%

7.06%

3.60%
18.49% 11.80%
3.47% 50.97% 36.99% 71.80% 55.51%
4.95%
23.54% 31.12% 46.21% 32.47%
27.24% 30.56% 48.17% 34.75%
3.57%
24.40% 29.80% 47.87% 33.38%
3.89%
34.84% 36.13% 58.02% 42.76%
5.32%
16.42% 20.29% 36.33% 24.41%
5.01%
41.08% 68.73% 61.89% 86.36% 72.53%
16.52% 19.30% 40.59% 26.23%
5.25%

5 Related Work

A number of prior publications have extended VAEs to model structured data by altering the un-
derlying graphical model to dynamic Bayesian networks  such as SRNN [3] and VRNN [9]  or to
hierarchical models  such as neural statistician [7] and SVAE [18]. These models have shown success
in quantitatively increasing the log-likelihood  or qualitatively generating reasonable structured data
by sampling. However  it remains unclear whether independent attributes are disentangled in the
latent space. Moreover  the learned latent variables in these models are not interpretable without
manually inspecting or using labeled data. In contrast  our work presents a VAE framework that
addresses both problems by explicitly modeling the difference in the rate of temporal variation of the
attributes that operate at different scales.
Our work is also related to β-VAE [13] with respect to unsupervised learning of disentangled repre-
sentations with VAEs. The boosted KL-divergence penalty imposed in β-VAE training encourages
disentanglement of independent attributes  but does not provide interpretability without supervision.
We demonstrate in our domain invariant ASR experiments that learning interpretable representations
is important for such applications  and can be achieved by our FHVAE model. In addition  the idea
of boosting KL-divergence regularization is complimentary to our model  which can be potentially
integrated for better disentanglement.

6 Conclusions and Future Work

We introduce the factorized hierarchical variational autoencoder  which learns disentangled and
interpretable representations for sequence-level and segment-level attributes without any supervision.
We verify the disentangling ability both qualitatively and quantitatively on two speech corpora. For
future work  we plan to (1) extend to more levels of hierarchy  (2) investigate adversarial training for
disentanglement  and (3) apply the model to other types of sequential data  such as text and videos.

9

References
[1] Xi Chen  Yan Duan  Rein Houthooft  John Schulman  Ilya Sutskever  and Pieter Abbeel.

Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In Advances
in Neural Information Processing Systems  page 2172–2180  2016.

[2] Junyoung Chung  Sungjin Ahn  and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.

arXiv preprint arXiv:1609.01704  2016.

[3] Junyoung Chung  Kyle Kastner  Laurent Dinh  Kratarth Goel  Aaron C Courville  and Yoshua Bengio. A
recurrent latent variable model for sequential data. In Advances in neural information processing systems 
pages 2980–2988  2015.

[4] Najim Dehak  Reda Dehak  Patrick Kenny  Niko Brümmer  Pierre Ouellet  and Pierre Dumouchel. Support
vector machines versus fast scoring in the low-dimensional total variability space for speaker veriﬁcation.
In Interspeech  volume 9  pages 1559–1562  2009.

[5] Najim Dehak  Patrick J Kenny  Réda Dehak  Pierre Dumouchel  and Pierre Ouellet. Front-end fac-
tor analysis for speaker veriﬁcation. IEEE Transactions on Audio  Speech  and Language Processing 
19(4):788–798  2011.

[6] Vincent Dumoulin  Ishmael Belghazi  Ben Poole  Alex Lamb  Martin Arjovsky  Olivier Mastropietro  and

Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704  2016.

[7] Harrison Edwards and Amos Storkey. Towards a neural statistician. arXiv preprint arXiv:1606.02185 

2016.

[8] Otto Fabius and Joost R van Amersfoort. Variational recurrent auto-encoders.

arXiv:1412.6581  2014.

arXiv preprint

[9] Marco Fraccaro  Søren Kaae Sønderby  Ulrich Paquet  and Ole Winther. Sequential neural models with

stochastic layers. In Advances in Neural Information Processing Systems  pages 2199–2207  2016.

[10] John S Garofolo  Lori F Lamel  William M Fisher  Jonathon G Fiscus  and David S Pallett. DARPA TIMIT
acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1. NASA STI/Recon technical
report n  93  1993.

[11] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair  Aaron
Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems  pages 2672–2680  2014.

[12] Alex Graves  Navdeep Jaitly  and Abdel-rahman Mohamed. Hybrid speech recognition with deep bidirec-
tional LSTM. In Automatic Speech Recognition and Understanding (ASRU)  2013 IEEE Workshop on 
pages 273–278. IEEE  2013.

[13] Irina Higgins  Loic Matthey  Arka Pal  Christopher Burgess  Xavier Glorot  Matthew Botvinick  Shakir
Mohamed  and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational
framework. 2016.

[14] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation  9(8):1735–1780 

1997.

[15] Wei-Ning Hsu  Yu Zhang  and James Glass. Learning latent representations for speech generation and

transformation. In Interspeech  pages 1273–1277  2017.

[16] Wei-Ning Hsu  Yu Zhang  and James Glass. Unsupervised domain adaptation for robust speech recognition
via variational autoencoder-based data augmentation. In Automatic Speech Recognition and Understanding
(ASRU)  2017 IEEE Workshop on. IEEE  2017.

[17] Zhiting Hu  Zichao Yang  Xiaodan Liang  Ruslan Salakhutdinov  and Eric P Xing. Controllable text

generation. arXiv preprint arXiv:1703.00955  2017.

[18] Matthew Johnson  David K Duvenaud  Alex Wiltschko  Ryan P Adams  and Sandeep R Datta. Composing
graphical models with neural networks for structured representations and fast inference. In Advances in
neural information processing systems  pages 2946–2954  2016.

[19] Michael I Jordan  Zoubin Ghahramani  Tommi S Jaakkola  and Lawrence K Saul. An introduction to

variational methods for graphical models. Machine learning  37(2):183–233  1999.

10

[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[21] Diederik P Kingma  Shakir Mohamed  Danilo Jimenez Rezende  and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems  pages
3581–3589  2014.

[22] Diederik P Kingma  Tim Salimans  Rafal Jozefowicz  Xi Chen  Ilya Sutskever  and Max Welling. Improved

variational inference with inverse autoregressive ﬂow. 2016.

[23] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 

2013.

[24] Tomi Kinnunen  Lauri Juvela  Paavo Alku  and Junichi Yamagishi. Non-parallel voice conversion using

i-vector plda: Towards unifying speaker veriﬁcation and transformation. In ICASSP  2017.

[25] Tejas D Kulkarni  William F Whitney  Pushmeet Kohli  and Josh Tenenbaum. Deep convolutional inverse

graphics network. In Advances in Neural Information Processing Systems  pages 2539–2547  2015.

[26] Anders Boesen Lindbo Larsen  Søren Kaae Sønderby  Hugo Larochelle  and Ole Winther. Autoencoding

beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300  2015.

[27] Alireza Makhzani  Jonathon Shlens  Navdeep Jaitly  Ian Goodfellow  and Brendan Frey. Adversarial

autoencoders. arXiv preprint arXiv:1511.05644  2015.

[28] Nelson Mogran  Hervé Bourlard  and Hynek Hermansky. Automatic speech recognition: An auditory

perspective. In Speech processing in the auditory system  pages 309–338. Springer  2004.

[29] Toru Nakashika  Tetsuya Takiguchi  Yasuhiro Minami  Toru Nakashika  Tetsuya Takiguchi  and Yasuhiro
Minami. Non-parallel training in voice conversion using an adaptive restricted boltzmann machine.
IEEE/ACM Trans. Audio  Speech and Lang. Proc.  24(11):2032–2045  November 2016.

[30] Aaron van den Oord  Nal Kalchbrenner  and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv

preprint arXiv:1601.06759  2016.

[31] Douglas B Paul and Janet M Baker. The design for the wall street journal-based csr corpus. In Proceedings
of the workshop on Speech and Natural Language  pages 357–362. Association for Computational
Linguistics  1992.

[32] David Pearce. Aurora working group: DSR front end LVCSR evaluation AU/384/02. PhD thesis  Mississippi

State University  2002.

[33] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with deep

convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[34] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation and approxi-

mate inference in deep generative models. arXiv preprint arXiv:1401.4082  2014.

[35] Hasim Sak  Andrew W Senior  and Françoise Beaufays. Long short-term memory recurrent neural network

architectures for large scale acoustic modeling. In Interspeech  pages 338–342  2014.

[36] Iulian Vlad Serban  Alessandro Sordoni  Ryan Lowe  Laurent Charlin  Joelle Pineau  Aaron Courville 
and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for generating dialogues. In
Thirty-First AAAI Conference on Artiﬁcial Intelligence  2017.

[37] Dmitriy Serdyuk  Kartik Audhkhasi  Philemon Brakel  Bhuvana Ramabhadran  Samuel Thomas  and

Yoshua Bengio. Invariant representations for noisy speech recognition. CoRR  abs/1612.01928  2016.

[38] Yusuke Shunohara. Adversarial multi-task learning of deep neural networks for robust speech recognition.

In Interspeeech  pages 2369–2372  2016.

[39] Aäron van den Oord  Sander Dieleman  Heiga Zen  Karen Simonyan  Oriol Vinyals  Alex Graves  Nal
Kalchbrenner  Andrew Senior  and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.
CoRR abs/1609.03499  2016.

[40] Zhizheng Wu  Eng Siong Chng  and Haizhou Li. Conditional restricted boltzmann machine for voice

conversion. In ChinaSIP  2013.

[41] Dong Yu  Michael Seltzer  Jinyu Li  Jui-Ting Huang  and Frank Seide. Feature learning in deep neural

networks – studies on speech recognition tasks. arXiv preprint arXiv:1301.3605  2013.

11

[42] Yu Zhang  Guoguo Chen  Dong Yu  Kaisheng Yaco  Sanjeev Khudanpur  and James Glass. Highway
long short-term memory RNNs for distant speech recognition. In 2016 IEEE International Conference on
Acoustics  Speech and Signal Processing (ICASSP)  pages 5755–5759. IEEE  2016.

12

,Dylan Festa
Guillaume Hennequin
Mate Lengyel
Laetitia Papaxanthos
Felipe Llinares-López
Dean Bodenham
Karsten Borgwardt
Wei-Ning Hsu
Yu Zhang
James Glass