2018,SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator,In this paper  we propose a new technique named \textit{Stochastic Path-Integrated Differential EstimatoR} (SPIDER)  which can be used to track many deterministic quantities of interests with significantly reduced computational cost. 
Combining SPIDER with the method of normalized gradient descent  we propose SPIDER-SFO that solve non-convex stochastic optimization problems using stochastic gradients only. 
We provide a few error-bound results on its convergence rates.
Specially  we prove that the SPIDER-SFO algorithm achieves a gradient computation cost of $\mathcal{O}\left(  \min( n^{1/2} \epsilon^{-2}  \epsilon^{-3} ) \right)$ to find an $\epsilon$-approximate first-order stationary point. 
In addition  we prove that SPIDER-SFO nearly matches the algorithmic lower bound for finding stationary point under the gradient Lipschitz assumption in the finite-sum setting.
Our SPIDER technique can be further applied to find an $(\epsilon  \mathcal{O}(\ep^{0.5}))$-approximate second-order stationary point at a gradient computation cost of $\tilde{\mathcal{O}}\left(  \min( n^{1/2} \epsilon^{-2}+\epsilon^{-2.5}  \epsilon^{-3} ) \right)$.,SPIDER: Near-Optimal Non-Convex Optimization via

Stochastic Path Integrated Differential Estimator

Cong Fang1 ∗ Chris Junchi Li2 Zhouchen Lin1† Tong Zhang2

1Key Lab. of Machine Intelligence (MoE)  School of EECS  Peking University

2Tencent AI Lab

{fangcong  zlin}@pku.edu.cn

junchi.li.duke@gmail.com tongzhang@tongzhang-ml.org

Abstract

In this paper  we propose a new technique named Stochastic Path-Integrated
Differential EstimatoR (SPIDER)  which can be used to track many deterministic
quantities of interests with signiﬁcantly reduced computational cost. Combining
SPIDER with the method of normalized gradient descent  we propose SPIDER-SFO
that solve non-convex stochastic optimization problems using stochastic gradients
only. We provide a few error-bound results on its convergence rates. Specially 
we prove that the SPIDER-SFO algorithm achieves a gradient computation cost

of O(cid:0)min(n1/2−2  −3)(cid:1) to ﬁnd an -approximate ﬁrst-order stationary point.
cost of ˜O(cid:0)min(n1/2−2 + −2.5  −3)(cid:1).

In addition  we prove that SPIDER-SFO nearly matches the algorithmic lower
bound for ﬁnding stationary point under the gradient Lipschitz assumption in
the ﬁnite-sum setting. Our SPIDER technique can be further applied to ﬁnd an
( O(0.5))-approximate second-order stationary point at a gradient computation

1

Introduction

In this paper  we study the optimization problem

minimize

x∈Rd

f (x) ≡ E [F (x; ζ)]

(1.1)

where the stochastic component F (x; ζ)  indexed by some random vector ζ  is smooth and possibly
non-convex. Non-convex optimization problem of form (1.1) contains many large-scale statistical
learning tasks and is gaining tremendous popularity due to its favorable computational and statistical
efﬁciency [5–7]. Typical examples of form (1.1) include principal component analysis  estimation
of graphical models  as well as training deep neural networks [17]. The expectation-minimization
structure of stochastic optimization problem (1.1) allows us to perform iterative updates and minimize
the objective using its stochastic gradient ∇F (x; ζ) as an estimator of its deterministic counterpart.
A special case of central interest is when the stochastic vector ζ is ﬁnitely sampled. In such
ﬁnite-sum (or ofﬂine) case  we denote each component function as fi(x) and (1.1) can be restated as

minimize

x∈Rd

f (x) =

1
n

fi(x)

(1.2)

n(cid:88)

i=1

∗This work was done while Cong Fang was a Research Intern with Tencent AI Lab.
†Corresponding author.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

where n is the number of individual functions. Another case is when n is reasonably large or even
inﬁnite  running across of the whole dataset is exhaustive or impossible. We refer it as the online (or
streaming) case. For simplicity of notations we will study the optimization problem of form (1.2) in
both ﬁnite-sum and online cases till the rest of this paper.

One important task for non-convex optimization is to search for  given the precision accuracy
 > 0  an -approximate ﬁrst-order stationary point x ∈ Rd or (cid:107)∇f (x)(cid:107) ≤ . In this paper  we aim
to propose a new technique  called the Stochastic Path-Integrated Differential EstimatoR (SPIDER) 
which enables us to construct an estimator that tracks a deterministic quantity with signiﬁcantly
lower sampling costs. As the readers will see  the SPIDER technique further allows us to design an
algorithm with a faster rate of convergence for non-convex problem (1.2)  in which we utilize the
idea of Normalized Gradient Descent (NGD) [18  26]. NGD is a variant of Gradient Descent (GD)
where the stepsize is picked to be inverse-proportional to the norm of the full gradient. Compared to
GD  NGD exempliﬁes faster convergence  especially in the neighborhood of stationary points [25].
However  NGD has been less popular due to its requirement of accessing the full gradient and its
norm at each update. In this paper  we estimate and track the gradient and its norm via the SPIDER
technique and then hybrid it with NGD. Measured by gradient cost which is the total number of
computation of stochastic gradients  our proposed SPIDER-SFO algorithm achieves a faster rate of
convergence in O(min(n1/2−2  −3)) which outperforms the previous best-known results in both
ﬁnite-sum [3][32] and online cases [24] by a factor of O(min(n1/6  −0.333)).

For the task of ﬁnding stationary points for which we already achieved a faster convergence
rate via our proposed SPIDER-SFO algorithm  a follow-up question to ask is: is our proposed
SPIDER-SFO algorithm optimal for an appropriate class of smooth functions? In this paper  we
provide an afﬁrmative answer to this question in the ﬁnite-sum case. To be speciﬁc  inspired by a
counterexample proposed by Carmon et al. [10] we are able to prove that the gradient cost upper
bound of SPIDER-SFO algorithm matches the algorithmic lower bound. To put it differently  the
gradient cost of SPIDER-SFO cannot be further improved for ﬁnding stationary points for some
particular non-convex functions.

1.1 Related Works

In the recent years  there has been a surge of literatures in machine learning community that
analyze the convergence property of non-convex optimization algorithms. Limited by space and our
knowledge  we have listed all literatures that we believe are mostly related to this work. We refer
the readers to the monograph by Jain et al. [19] and the references therein on recent general and
model-speciﬁc convergence rate results on non-convex optimization.

SGD and Variance Reduction For the general problem of ﬁnding approximate stationary points 
under the smoothness condition of f (x)  it is known that vanilla Gradient Descent (GD) and Stochastic
Gradient Descent (SGD)  which can be traced back to Cauchy [11] and Robbins & Monro [33] and
achieve an -approximate stationary point with a gradient cost of O(min(n−2  −4)) [16  26].

Recently  the convergence rate of GD and SGD have been improved by the variance-reduction
type of algorithms [22  34].
In special  the ﬁnite-sum Stochastic Variance-Reduced Gradient
(SVRG) and online Stochastically Controlled Stochastic Gradient (SCSG)  to the gradient cost
of ˜O(min(n2/3−2  −3.333)) [3  24  32].

First-order method for ﬁnding approximate second-order stationary points
It has been shown
that for machine learning methods such as deep learning  approximate stationary points that have
at least one negative Hessian direction  including saddle points and local maximizers  are often not
sufﬁcient and need to be avoided or escaped from [12  15]. Recently  many literature study the problem
of how to avoid or escape saddle points and achieve an (  δ)-approximate second-order stationary
point x at a polynomial gradient cost  i.e. an x ∈ Rd such that (cid:107)∇f (x)(cid:107) ≤   λmin(∇2f (x)) ≥ −δ
[1  2  4  8  15  18  20  21  23  25  30  31  35  38]. Among them  the group of authors Ge et al.
[15]  Jin et al. [20] proposed the noise-perturbed variants of Gradient Descent (PGD) and Stochastic

2

Gradient Descent (SGD) that escape from all saddle points and achieve an -approximate second-
order stationary point in gradient cost of ˜O(min(n−2  poly(d)−4)) stochastic gradients. Levy [25]
proposed the noise-perturbed variant of NGD which yields faster evasion of saddle points than GD.
The breakthrough of gradient cost for ﬁnding second-order stationary points were achieved in
2016/2017  when the two recent lines of literatures  namely FastCubic [1] and CDHS [8] as well as
their stochastic versions [2  35]  achieve a gradient cost of ˜O(min(n−1.5+n3/4−1.75  −3.5)) which
serves as the best-known gradient cost for ﬁnding an ( O(0.5))-approximate second-order stationary
point before the initial submission of this paper.3 4 In particular  Agarwal et al. [1]  Tripuraneni
et al. [35] converted the cubic regularization method for ﬁnding second-order stationary points [27]
to stochastic- gradient based and stochastic-Hessian-vector-product-based methods  and Allen-Zhu
[2]  Carmon et al. [8] used a Negative-Curvature Search method to avoid saddle points. See also
recent works by Reddi et al. [31] for related saddle-point-escaping methods that achieve similar rates
for ﬁnding an approximate second-order stationary point.

Other concurrent works As the current work is carried out in its ﬁnal phase  the authors became
aware that an idea of resemblance was earlier presented in an algorithm named the StochAstic
Recursive grAdient algoritHm (SARAH) [28  29]. Despite the fact that both our SPIDER-SFO and
theirs adopt the recursive stochastic gradient update framework and our SPIDER-SFO can be viewed
as a variant of SARAH with normalization  our work differ from their works in two aspects:

(i) Our analysis techniques are totally different from the version of SARAH proposed by Nguyen
et al. [28  29]. Their version can be seen as a variant of gradient descent  while ours hybrids the
SPIDER technique with normalized gradient descent. Moreover  Nguyen et al. [28  29] adopt a
large stepsize setting (in fact their goal was to design a memory-saving variant of SAGA [13]) 
while our SPIDER-SFO algorithm adopt a small stepsize that is proportional to . All these are
essential elements of our superior achievements in convergence rates;

(ii) Our proposed SPIDER technique is a much more general variance-reduced estimation method
for many quantities (not limited to gradients) and can be ﬂexibly applied to numerous problems 
e.g. stochastic zeroth-order method.

Soon after the initial submission to NIPS and arXiv release of this paper  we became aware
that similar convergence rate results for stochastic ﬁrst-order method were also achieved indepen-
dently by the so-called SNVRG algorithm [39  40]. SNVRG [40] obtains a gradient complexity
of ˜O(min(n1/2−2  −3)) for ﬁnding an -approximate ﬁrst-order stationary point and achieves a
˜O(−3.5) gradient cost for ﬁnding an ( O(0.5))-approximate second-order stationary point [39].
By exploiting the third-order smoothness  an SNVRG variant can also achieve an ( O(0.5))-
approximate second-order stationary point in ˜O(−3) stochastic gradient costs [39].

1.2 Our Contributions

In this work  we propose the Stochastic Path-Integrated Differential Estimator (SPIDER) technique 
which signiﬁcantly avoids excessive access of stochastic oracles and reduces the time complexity.
Such technique can be potential applied in many stochastic estimation problems.

(i) We propose the SPIDER-SFO algorithm (Algorithm 1) for ﬁnding approximate ﬁrst-order
stationary points for non-convex stochastic optimization problem (1.2)  and prove the optimality
of such rate in at least one case. Inspired by recent works Carmon et al. [8  10]  Johnson &
Zhang [22] and independent of Zhou et al. [39  40]  this is the ﬁrst time that the gradient cost of
O(min(n1/2−2  −3)) in both upper and lower (ﬁnite-sum only) bound for ﬁnding ﬁrst-order
stationary points for problem (1.2) were obtained.

3Allen-Zhu [2] also obtains a gradient cost of ˜O(−3.25) to achieve a (modiﬁed and weakened) ( O(0.25))-

approximate second-order stationary point.

4Here and in many places afterwards  the gradient cost also includes the number of stochastic Hessian-vector

product accesses  which has similar running time with computing per-access stochastic gradient.

3

(ii) Following Allen-Zhu & Li [4]  Carmon et al. [8]  Xu et al. [38]  we propose SPIDER-SFO+
algorithm for ﬁnding an approximate second-order stationary point for non-convex stochastic
optimization problem. To best of our knowledge  this is also the ﬁrst time that the gradient cost
of ˜O(min(n1/2−2 + −2.5  −3)) achieved with standard assumptions. We leave the details of
SFO in the long version of our paper: https://arxiv.org/abs/1807.01695

(iii) We propose a new and simpler analysis framework for proving convergence to approximate
stationary points. One can ﬂexibly apply our proof techniques to analyze others algorithms  e.g.
SGD  SVRG [22]  and SAGA [13].

Notation. Throughout this paper  we treat the parameters L  ∆  σ  and ρ  to be speciﬁed later as
global constants. Let (cid:107) · (cid:107) denote the Euclidean norm of a vector or spectral norm of a square
matrix. Denote pn = O(qn) for a sequence of vectors pn and positive scalars qn if there is a global
constant C such that |pn| ≤ Cqn  and pn = ˜O(qn) such C hides a poly-logarithmic factor of the
parameters. Denote pn = Ω(qn) if there is a global constant C such that |pn| ≥ Cqn. Let λmin(A)
denote the least eigenvalue of a real symmetric matrix A. For ﬁxed K ≥ k ≥ 0  let xk:K denote the
sequence {xk  . . .   xK}. Let [n] = {1  . . .   n} and S denote the cardinality of a multi-set S ⊂ [n] of
samples (a generic set that allows elements of multiple instances). For simplicity  we further denote
i∈S Bi and averaged sub-sampled

the averaged sub-sampled stochastic estimator BS := (1/S)(cid:80)
gradient ∇fS := (1/S)(cid:80)

i∈S ∇fi. Other notations are explained at their ﬁrst appearance.

2 Stochastic Path-Integrated Differential Estimator: Core Idea

In this section  we present in detail the underlying idea of our Stochastic Path-Integrated Dif-
ferential Estimator (SPIDER) technique behind the algorithm design. As the readers will see  such
technique signiﬁcantly avoids excessive access of stochastic oracle and reduces complexity  which is
of independent interest and has potential applications in many stochastic estimation problems.

Let us consider an arbitrary deterministic vector quantity Q(x). Assume that we observe a
sequence ˆx0:K  and we want to dynamically track Q(ˆxk) for k = 0  1  . . .   K. Assume further that
we have an initial estimate ˜Q(ˆx0) ≈ Q(ˆx0)  and an unbiased estimate ξk(ˆx0:k) of Q(ˆxk)− Q(ˆxk−1)
such that for each k = 1  . . .   K

E [ξk(ˆx0:k) | ˆx0:k] = Q(ˆxk) − Q(ˆxk−1).

Then we can integrate (in the discrete sense) the stochastic differential estimate as

˜Q(ˆx0:K) := ˜Q(ˆx0) +

ξk(ˆx0:k).

(2.1)

K(cid:88)

k=1

K(cid:88)

We call estimator ˜Q(ˆx0:K) the Stochastic Path-Integrated Differential EstimatoR  or SPIDER for
brevity. We conclude the following proposition which bounds the error of our estimator (cid:107) ˜Q(ˆx0:K) −
Q(ˆxK)(cid:107)  in terms of both expectation and high probability:
Proposition 1. The martingale variance bound has

E(cid:107) ˜Q(ˆx0:K)−Q(ˆxK)(cid:107)2 = E(cid:107) ˜Q(ˆx0)−Q(ˆx0)(cid:107)2 +

E(cid:107)ξk(ˆx0:k)−(Q(ˆxk)−Q(ˆxk−1))(cid:107)2. (2.2)

k=1

Proposition 1 can be easily concluded using the property of square-integrable martingales. Now 
let B map any x ∈ Rd to a random estimate Bi(x) such that  conditioning on the observed sequence
x0:k  we have for each k = 1  . . .   K 

E(cid:2)Bi(xk) − Bi(xk−1) | x0:k

(cid:3) = V k − V k−1.

(2.3)

4

At each step k let S∗ be a subset that samples S∗ elements in [n] with replacement  and let the

stochastic estimator BS∗ = (1/S∗)(cid:80)

i∈S∗ Bi satisfy

and (cid:107)xk − xk−1(cid:107) ≤ 1 for all k = 1  . . .   K. Finally  we set our estimator V k of B(xk) as

E(cid:107)Bi(x) − Bi(y)(cid:107)2 ≤ L2B(cid:107)x − y(cid:107)2 

(2.4)

V k = BS∗ (xk) − BS∗ (xk−1) + V k−1.

Applying Proposition 1 immediately concludes the following lemma  which gives an error bound of
the estimator V k in terms of the second moment of (cid:107)V k − B(xk)(cid:107):
Lemma 1. We have under the condition (2.4) that for all k = 1  . . .   K 
+ E(cid:107)V 0 − B(x0)(cid:107)2.

(2.5)

E(cid:107)V k − B(xk)(cid:107)2 ≤ kL2B2
1S∗

It turns out that one can use SPIDER to track many quantities of interest  such as stochastic
gradient  function values  zero-order estimate gradient  functionals of Hessian matrices  etc. Our
proposed SPIDER-based algorithms in this paper take Bi as the stochastic gradient ∇fi and the
zeroth-order estimate gradient  separately.

3 SPIDER for Stochastic First-Order Method

In this section  we apply SPIDER to the Stochastic First-Order (SFO) method. We introduce the
basic settings and assumptions in §3.1 and propose the main error-bound theorems for ﬁnding an
-approximate ﬁrst-order stationary point in §3.2. We conclude this section with the corresponding
lower-bound result in §3.3.

3.1 Settings and Assumptions

We ﬁrst introduce the formal deﬁnition of an approximate ﬁrst-order stationary point as follows.

Deﬁnition 1. We call x ∈ Rd an -approximate ﬁrst-order stationary point  or simply an FSP  if

(cid:107)∇f (x)(cid:107) ≤ .

(3.1)

For our purpose of analysis  we also pose the following assumption:

Assumption 1. We assume the following
(i) The ∆ := f (x0) − f∗ < ∞ where f∗ = inf x∈Rd f (x) is the global inﬁmum value of f (x);
(ii) The component function fi(x) has an averaged L-Lipschitz gradient  i.e. for all x  y 

E(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ L2(cid:107)x − y(cid:107)2;

(iii) (For online case only) the stochastic gradient has a ﬁnite variance bounded by σ2 < ∞  i.e.

E(cid:107)∇fi(x) − ∇f (x)(cid:107)2 ≤ σ2.

3.2 Upper Bound for Finding First-Order Stationary Points

Recall that NGD has iteration update rule

xk+1 = xk − η

∇f (xk)
(cid:107)∇f (xk)(cid:107)  

(3.2)

where η is a constant step size. The NGD update rule (3.2) ensures (cid:107)xk+1 − xk(cid:107) being constantly
equal to the stepsize η  and might fastly escape from saddle points and converge to a second-order

5

if mod (k  q) = 0 then

Algorithm 1 SPIDER-SFO: Input x0  q  S1  S2  n0    and ˜ (For ﬁnding ﬁrst-order stationary point)
1: for k = 0 to K do
2:
3:
4:
5:
6:

Draw S1 samples (or compute the full gradient for the ﬁnite-sum case)  let vk = ∇fS1(xk)
Draw S2 samples  and let vk = ∇fS2(xk) − ∇fS2(xk−1) + vk−1

end if

else

if (cid:107)vk(cid:107) ≤ 2˜ then

return xk
xk+1 = xk − η · (vk/(cid:107)vk(cid:107)) where

7: OPTION I
8:
9:
10:
11:
12:

end if

else

13: OPTION II

14:

xk+1 = xk − ηkvk where

ηk = min

15: end for

(cid:5) for convergence rates in high probability

η =



Ln0

(cid:18)



Ln0(cid:107)vk(cid:107)  

(cid:19)

(cid:5) for convergence rates in expectation
1

2Ln0

16: OPTION I: Return xK
17: OPTION II: Return ˜x chosen uniformly at random from {xk}K−1

k=0

(cid:5) however  this line is not reached with high probability

stationary point [25]. We propose SPIDER-SFO in Algorithm 1  which resembles a stochastic variant
of NGD with the SPIDER technique applied  so that one can maintain an estimate of ∇f (xk) at a
higher accuracy under limited gradient budgets.

To analyze the convergence rate of SPIDER-SFO  let us ﬁrst consider the online case for Algorithm

1. We let the input parameters be

(cid:18)

(cid:19)

 

2σ2
2  

2σ
n0

 



 



1

σn0

ηk = min

S2 =

S1 =

 

(3.3)
where n0 ∈ [1  2σ/] is a free parameter to choose.5 In this case  vk in Line 5 of Algorithm 1 is a
SPIDER for ∇f (xk). To see this  recall ∇fi(xk−1) is the stochastic gradient drawn at step k and

Ln0(cid:107)vk(cid:107)  

2Ln0

Ln0

η =

q =

E(cid:2)∇fi(xk) − ∇fi(xk−1) | x0:k

(cid:3) = ∇f (xk) − ∇f (xk−1).

(3.4)
Plugging in V k = vk and Bi = ∇fi in Lemma 1 of §2  we can use vk in Algorithm 1 as the SPIDER
and conclude the following lemma that is pivotal to our analysis.
Lemma 2. Set the parameters S1  S2  η  and q as in (3.3)  and k0 = (cid:98)k/q(cid:99) · q. Then under the
Assumption 1  we have

E(cid:2)(cid:107)vk − ∇f (xk)(cid:107)2 | x0:k0

(cid:3) ≤ 2.

Here we compute the conditional expectation over the randomness of x(k0+1):k.

Lemma 2 shows that our SPIDER vk of ∇f (x) maintains an error of O(). Using this lemma 
we are ready to present the following results for Stochastic First-Order (SFO) method for ﬁnding
ﬁrst-order stationary points of (1.2).
Theorem 1 (First-order stationary point  online setting  in expectation). Assume we are in the
online case  let Assumption 1 holds  set the parameters S1  S2  η  and q as in (3.3)  and set

K =(cid:4)(4L∆n0)−2(cid:5) + 1. Then running Algorithm 1 with OPTION II for K iterations outputs a ˜x

5When n0 = 1  the mini-batch size is 2σ/  which is the largest mini-batch size that Algorithm 1 allows to

choose.

6

Turning to the ﬁnite-sum case  analogous to the online case we let

(cid:18)

(cid:19)

satisfying

E [(cid:107)∇f (˜x)(cid:107)] ≤ 5.

The gradient cost is bounded by 24L∆σ· −3 + 2σ2−2 + 4σn−1
Treating ∆  L and σ as positive constants  the stochastic gradient complexity is O(−3).

(3.5)
0 −1 for any choice of n0 ∈ [1  2σ/].

The relatively reduced minibatch size serves as the key ingredient for the superior performance
of SPIDER-SFO. For illustrations  let us compare the sampling efﬁciency among SGD  SCSG and
SPIDER-SFO in their special cases. With some involved analysis of the algorithms above  we can
conclude that to ensure per-iteration sufﬁcient decrease of Ω(2/L)  we have

(i) for SGD the choice of mini-batch size is O(cid:0)σ2 · −2(cid:1);
(ii) for SCSG [24] and Natasha2 [2] the mini-batch size is O(cid:0)σ · −1.333(cid:1);
(iii) for our SPIDER-SFO  only a reduced mini-batch size of O(cid:0)σ · −1(cid:1) is needed.

S2 =

n1/2
n0

 

η =



Ln0

 

ηk = min



Ln0(cid:107)vk(cid:107)  

1

2Ln0

 

q = n0n1/2 

(3.6)

where n0 ∈ [1  n1/2]. In this case  one computes the full gradient vk = ∇fS1 (xk) in Line 3 of
Algorithm 1. We conclude our second upper-bound result:
Theorem 2 (First-order stationary point  ﬁnite-sum setting  in expectation). Assume we are in the
ﬁnite-sum case  let Assumption 1 holds  set the parameters S2  ηk  and q as in (3.6)  set K =

(cid:4)(4L∆n0)−2(cid:5) + 1  and let S1 = [n]  i.e. we obtain the full gradient in Line 3. Then running

Algorithm 1 with OPTION II for K iterations outputs a ˜x satisfying

E(cid:107)∇f (˜x)(cid:107) ≤ 5.

The gradient cost is bounded by n + 12(L∆) · n1/2−2 + 2n−1
0 n1/2 for any choice of n0 ∈ [1  n1/2].
Treating ∆  L and σ as positive constants  the stochastic gradient complexity is O(n + n1/2−2).

3.3 Lower Bound for Finding First-Order Stationary Points

To conclude the optimality of our algorithm we need an algorithmic lower bound result [10  37].
Consider the ﬁnite-sum case and any random algorithm A that maps functions f : Rd → R to a
sequence of iterates in Rd+1  with

[xk; ik] = Ak−1(cid:0)ξ ∇fi0(x0) ∇fi1 (x1)  . . .  ∇fik−1(xk−1)(cid:1) 

(3.7)
where Ak are measure mapping into Rd+1  ik is the individual function chosen by A at iteration k 
and ξ is uniform random vector from [0  1]. And [x0; i0] = A0(ξ)  where A0 is a measure mapping.
The lower-bound result for solving (1.2) is stated as follows:
Theorem 3 (Lower bound for SFO for the ﬁnite-sum setting). For any L > 0  ∆ > 0  and 2 ≤ n ≤

O(cid:0)∆2L2 · −4(cid:1)  for any algorithm A satisfying (3.7)  there exists a dimension d = ˜O(cid:0)∆2L2·n2−4(cid:1) 
which (cid:107)∇f (˜x)(cid:107) ≤   A must cost at least Ω(cid:0)L∆ · n1/2−2(cid:1) stochastic gradient accesses.

and a function f satisﬁes Assumption 1 in the ﬁnite-sum case  such that in order to ﬁnd a point ˜x for

k ≥ 1 

Note the condition n ≤ O(−4) in Theorem 3 ensures that our lower bound Ω(n1/2−2) =
Ω(n + n1/2−2)  and hence our upper bound in Theorem 1 matches the lower bound in Theorem
3 up to a constant factor of relevant parameters  and is hence near-optimal. Inspired by Carmon
et al. [10]  our proof of Theorem 3 utilizes a speciﬁc counterexample function that requires at least
Ω(n1/2−2) stochastic gradient accesses. Note Carmon et al. [10] analyzed such counterexample in
the deterministic case n = 1 and we generalize such analysis to the ﬁnite-sum case n ≥ 1.
Remark 1. Note by setting n = O(−4) the lower bound complexity in Theorem 3 can be as large as
Ω(−4). We emphasize that this does not violate the O(−3) upper bound in the online case [Theorem

7

1]  since the counterexample established in the lower bound depends not on the stochastic gradient
variance σ2 speciﬁed in Assumption 1(iii)  but on the component number n. To obtain the lower
bound result for the online case with the additional Assumption 1(iii)  with more efforts one might be
able to construct a second counterexample that requires Ω(−3) stochastic gradient accesses with
the knowledge of σ instead of n. We leave this as a future work.

4 Further Extensions

Further extensions of our SPIDER technique can be successfully applied to reduce the complexity.
Limited by space  we leave the details of the following important extensions in the long version of
our paper at https://arxiv.org/abs/1807.01695 .

Upper Bound for Finding First-Order Stationary Points  in High-Probability Under more
stringent assumptions on the moments of stochastic gradients  our Algorithm 1 with OPTION I
achieves a gradient cost of ˜O(min(n1/2−2  −3)) (note the additional polylogarithmic factor) with
high probability. We detail the theorems and their proofs in the long version of our paper.

Second-Order Stationary Point To ﬁnd a second-order stationary point with (3.1)  we can fuse
our SPIDER-SFO in Algorithm 1 (OPTION I taken) with a Negative-Curvature-Search (NC-Search)
iteration. In the long version of our paper (and independent of [39])  we proved rigorously that a
gradient cost of ˜O(min(n1/2−2 + −2.5  −3)) can be achieved under standard assumptions:
Theorem 4 (Second-Order Stationary Point  Informal). There exists an algorithm such that under
appropriate assumptions it takes to ﬁnd a ( 
ρ)-second-order stationary point  we have for the
online case  when  ≤ ρσ2 the total number of stochastic gradient computations is ˜O(−3); For the
ﬁnite-sum case  when  ≤ ρn  the total cost of gradient access is ˜O(n−1.5 + n1/2−2 + −2.5).

√

Zeroth-Order Stationary Point After the NIPS submission of this work  we propose a second
application of our SPIDER technique to the stochastic zeroth-order method for problem (1.2) and
achieves individual function accesses of O(min(dn1/2−2  d−3)). To best of our knowledge  this is
also the ﬁrst time a complexity of individual function value accesses for non-convex problems has
been improved to the aforementioned complexity using variance reduction techniques [22  34].

5 Summary and Future Directions

We propose in this work the SPIDER method for non-convex optimization. Our SPIDER-type
algorithms have update rules that are reasonably simple and achieve excellent convergence properties.
However  there are still some important questions are left. For example  the lower bound results for
ﬁnding a second-order stationary point are not complete. Specially  it is not yet clear if our ˜O(−3)
for the online case and ˜O(n1/2−2) for the ﬁnite-sum case gradient cost upper bound for ﬁnding
a second-order stationary point (when n ≥ Ω(−1)) is optimal or the gradient cost can be further
improved  assuming both Lipschitz gradient and Lipschitz Hessian.

Acknowledgement The authors would like to thank Jeffrey Z. HaoChen for his help on the nu-
merical experiments  thank an anonymous reviewer to point out a mistake in the original proof of
Theorem 1 and thank Zeyuan Allen-Zhu and Quanquan Gu for relevant discussions and pointing
out references Zhou et al. [39  40]  also Jianqiao Wangni for pointing out references Nguyen et al.
[28  29]  and Zebang Shen  Ruoyu Sun  Haishan Ye  Pan Zhou for very helpful discussions and com-
ments. Zhouchen Lin is supported by National Basic Research Program of China (973 Program  grant
no. 2015CB352502)  National Natural Science Foundation (NSF) of China (grant nos. 61625301 and
61731018)  and Microsoft Research Asia.

8

References
[1] Agarwal  N.  Allen-Zhu  Z.  Bullins  B.  Hazan  E.  & Ma  T. (2017). Finding approximate local
minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium
on Theory of Computing (pp. 1195–1199).: ACM.

[2] Allen-Zhu  Z. (2018). Natasha 2: Faster non-convex optimization than sgd. In Advances in

Neural Information Processing Systems.

[3] Allen-Zhu  Z. & Hazan  E. (2016). Variance reduction for faster non-convex optimization. In

International Conference on Machine Learning (pp. 699–707).

[4] Allen-Zhu  Z. & Li  Y. (2018). Neon2: Finding local minima via ﬁrst-order oracles. In Advances

in Neural Information Processing Systems.

[5] Bottou  L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings

of COMPSTAT’2010 (pp. 177–186). Springer.

[6] Bottou  L.  Curtis  F. E.  & Nocedal  J. (2018). Optimization methods for large-scale machine

learning. SIAM Review  60(2)  223–311.

[7] Bubeck  S. et al. (2015). Convex optimization: Algorithms and complexity. Foundations and

Trends R(cid:13) in Machine Learning  8(3-4)  231–357.

[8] Carmon  Y.  Duchi  J. C.  Hinder  O.  & Sidford  A. (2016). Accelerated methods for non-convex

optimization. To appear in SIAM Journal on Optimization  accepted.

[9] Carmon  Y.  Duchi  J. C.  Hinder  O.  & Sidford  A. (2017a). “Convex Until Proven Guilty”:
In International

Dimension-free acceleration of gradient descent on non-convex functions.
Conference on Machine Learning (pp. 654–663).

[10] Carmon  Y.  Duchi  J. C.  Hinder  O.  & Sidford  A. (2017b). Lower bounds for ﬁnding stationary

points i. arXiv preprint arXiv:1710.11606.

[11] Cauchy  A. (1847). Méthode générale pour la résolution des systemes déquations simultanées.

Comptes Rendus de l’Academie des Science  25  536–538.

[12] Dauphin  Y. N.  Pascanu  R.  Gulcehre  C.  Cho  K.  Ganguli  S.  & Bengio  Y. (2014). Identi-
fying and attacking the saddle point problem in high-dimensional non-convex optimization. In
Advances in Neural Information Processing Systems (pp. 2933–2941).

[13] Defazio  A.  Bach  F.  & Lacoste-Julien  S. (2014). SAGA: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems (pp. 1646–1654).

[14] Durrett  R. (2010). Probability: Theory and Examples (4th edition). Cambridge University

Press.

[15] Ge  R.  Huang  F.  Jin  C.  & Yuan  Y. (2015). Escaping from saddle points – online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory
(pp. 797–842).

[16] Ghadimi  S. & Lan  G. (2013). Stochastic ﬁrst-and zeroth-order methods for nonconvex

stochastic programming. SIAM Journal on Optimization  23(4)  2341–2368.

[17] Goodfellow  I.  Bengio  Y.  & Courville  A. (2016). Deep Learning. MIT Press. http:

//www.deeplearningbook.org.

[18] Hazan  E.  Levy  K.  & Shalev-Shwartz  S. (2015). Beyond convexity: Stochastic quasi-convex

optimization. In Advances in Neural Information Processing Systems (pp. 1594–1602).

9

[19] Jain  P.  Kar  P.  et al. (2017). Non-convex optimization for machine learning. Foundations and

Trends R(cid:13) in Machine Learning  10(3-4)  142–336.

[20] Jin  C.  Ge  R.  Netrapalli  P.  Kakade  S. M.  & Jordan  M. I. (2017a). How to escape saddle

points efﬁciently. In International Conference on Machine Learning (pp. 1724–1732).

[21] Jin  C.  Netrapalli  P.  & Jordan  M. I. (2017b). Accelerated gradient descent escapes saddle

points faster than gradient descent. arXiv preprint arXiv:1711.10456.

[22] Johnson  R. & Zhang  T. (2013). Accelerating stochastic gradient descent using predictive

variance reduction. In Advances in Neural Information Processing Systems (pp. 315–323).

[23] Lee  J. D.  Simchowitz  M.  Jordan  M. I.  & Recht  B. (2016). Gradient descent only converges

to minimizers. In Proceedings of The 29th Conference on Learning Theory (pp. 1246–1257).

[24] Lei  L.  Ju  C.  Chen  J.  & Jordan  M. I. (2017). Non-convex ﬁnite-sum optimization via scsg

methods. In Advances in Neural Information Processing Systems (pp. 2345–2355).

[25] Levy  K. Y. (2016). The power of normalization: Faster evasion of saddle points. arXiv preprint

arXiv:1611.04831.

[26] Nesterov  Y. (2004). Introductory lectures on convex optimization: A basic course  volume 87.

Springer.

[27] Nesterov  Y. & Polyak  B. T. (2006). Cubic regularization of newton method and its global

performance. Mathematical Programming  108(1)  177–205.

[28] Nguyen  L. M.  Liu  J.  Scheinberg  K.  & Takáˇc  M. (2017a). SARAH: A novel method
for machine learning problems using stochastic recursive gradient. In D. Precup & Y. W. Teh
(Eds.)  Proceedings of the 34th International Conference on Machine Learning  volume 70 of
Proceedings of Machine Learning Research (pp. 2613–2621). International Convention Centre 
Sydney  Australia: PMLR.

[29] Nguyen  L. M.  Liu  J.  Scheinberg  K.  & Takáˇc  M. (2017b). Stochastic recursive gradient

algorithm for nonconvex optimization. arXiv preprint arXiv:1705.07261.

[30] Paquette  C.  Lin  H.  Drusvyatskiy  D.  Mairal  J.  & Harchaoui  Z. (2018). Catalyst for gradient-
based nonconvex optimization. In Proceedings of the Twenty-First International Conference on
Artiﬁcial Intelligence and Statistics (pp. 613–622).

[31] Reddi  S.  Zaheer  M.  Sra  S.  Poczos  B.  Bach  F.  Salakhutdinov  R.  & Smola  A. (2018). A
generic approach for escaping saddle points. In A. Storkey & F. Perez-Cruz (Eds.)  Proceedings of
the Twenty-First International Conference on Artiﬁcial Intelligence and Statistics  volume 84 of
Proceedings of Machine Learning Research (pp. 1233–1242). Playa Blanca  Lanzarote  Canary
Islands: PMLR.

[32] Reddi  S. J.  Hefny  A.  Sra  S.  Poczos  B.  & Smola  A. (2016). Stochastic variance reduction

for nonconvex optimization. In International conference on machine learning (pp. 314–323).

[33] Robbins  H. & Monro  S. (1951). A stochastic approximation method. The annals of mathemat-

ical statistics  (pp. 400–407).

[34] Schmidt  M.  Le Roux  N.  & Bach  F. (2017). Minimizing ﬁnite sums with the stochastic

average gradient. Mathematical Programming  162(1-2)  83–112.

[35] Tripuraneni  N.  Stern  M.  Jin  C.  Regier  J.  & Jordan  M. I. (2018). Stochastic cubic
regularization for fast nonconvex optimization. In Advances in Neural Information Processing
Systems.

10

[36] Woodworth  B. & Srebro  N. (2017). Lower bound for randomized ﬁrst order convex optimiza-

tion. arXiv preprint arXiv:1709.03594.

[37] Woodworth  B. E. & Srebro  N. (2016). Tight complexity bounds for optimizing composite

objectives. In Advances in Neural Information Processing Systems (pp. 3639–3647).

[38] Xu  Y.  Jin  R.  & Yang  T. (2017). First-order stochastic algorithms for escaping from saddle

points in almost linear time. arXiv preprint arXiv:1711.01944.

[39] Zhou  D.  Xu  P.  & Gu  Q. (2018a). Finding local minima via stochastic nested variance

reduction. arXiv preprint arXiv:1806.08782.

[40] Zhou  D.  Xu  P.  & Gu  Q. (2018b). Stochastic nested variance reduction for nonconvex

optimization. arXiv preprint arXiv:1806.07811.

11

,Cong Fang
Chris Junchi Li
Zhouchen Lin
Tong Zhang
Aaron Schein
Scott Linderman
Mingyuan Zhou
David Blei
Hanna Wallach